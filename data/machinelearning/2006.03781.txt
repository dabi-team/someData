1
2
0
2

r
a

M
2

]
L
M

.
t
a
t
s
[

5
v
1
8
7
3
0
.
6
0
0
2
:
v
i
X
r
a

Attribute-Eﬃcient Learning of Halfspaces with Malicious Noise:
Near-Optimal Label Complexity and Noise Tolerance

Jie Shen
Stevens Institute of Technology
jie.shen@stevens.edu

Chicheng Zhang
University of Arizona
chichengz@cs.arizona.edu

March 3, 2021

Abstract
This paper is concerned with computationally eﬃcient learning of homogeneous sparse halfspaces in
Rd under noise. Though recent works have established attribute-eﬃcient learning algorithms under
various types of label noise (e.g. bounded noise), it remains an open question of when and how
s-sparse halfspaces can be eﬃciently learned under the challenging malicious noise model, where
an adversary may corrupt both the unlabeled examples and the labels. We answer this question in
the aﬃrmative by designing a computationally eﬃcient active learning algorithm with near-optimal
ǫ )1 and noise tolerance η = Ω(ǫ), where ǫ
label complexity of ˜O(s log
(0, 1) is the target error
rate, under the assumption that the distribution over (uncorrupted) unlabeled examples is isotropic
log-concave. Our algorithm can be straightforwardly tailored to the passive learning setting, and
we show that its sample complexity is ˜O( 1
d) which also enjoys attribute eﬃciency. Our
main techniques include attribute-eﬃcient paradigms for soft outlier removal and for empirical risk
minimization, and a new analysis of uniform concentration for unbounded instances – all of them
crucially take the sparsity structure of the underlying halfspace into account.

ǫ s2 log

4 d

∈

5

Keywords: halfspaces, malicious noise, passive and active learning, attribute eﬃciency

1 Introduction

This paper investigates the fundamental problem of learning halfspaces under noise [Val84, Val85].
In the absence of noise, this problem is well understood [Ros58, BEHW89]. However, the premise
changes immediately when the unlabeled examples2 or the labels are corrupted by noise. In the last
decades, various types of label noise have been extensively studied, and a plethora of polynomial-
time algorithms have been developed that are resilient to random classiﬁcation noise [BFKV96],
bounded noise [Slo88, Slo92, MN06], and adversarial noise [KSS92, KKMS05]. Signiﬁcant progress
towards optimal noise tolerance is also witnessed in the past few years [Dan15, ABHU15, YZ17,
DGT19, DKTZ20]. In this regard, a surge of recent research interest is concentrated on further
improvement of the performance guarantees by leveraging the structure of the underlying halfspace
into algorithmic design. Of central interest is a property termed attribute eﬃciency, which proves
to be useful when the data lie in a high-dimensional space [Lit87], or even in an inﬁnite-dimensional
space but with bounded number of eﬀective attributes [Blu90]. In the statistics and signal processing
community, it is often referred to as sparsity, dating back to the celebrated Lasso estimator [Tib96,

1We use the notation ˜O(f ) := O(f log f ).
2We will also refer to unlabeled examples as instances in this paper.

1

 
 
 
 
 
 
CDS98, CT05, Don06]. Recently, learning of sparse halfspaces in an attribute-eﬃcient manner was
highlighted as an open problem in [Fel14], and in a series of recent works [PV13b, ABHZ16, Zha18,
ZSA20], this property was carefully explored for label-noise-tolerant learning of halfspaces with
improved or even near-optimal sample complexity, label complexity, or generalization error, where
the key insight is that such structural constraint eﬀectively controls the complexity of the hypothesis
class [Zha02, KST08].

−

η, the adversary returns an instance x drawn from D and the label y = sign (w∗

Compared to the rich set of positive results on attribute-eﬃcient learning of sparse halfspaces
under label noise, less is known when both instances and labels are corrupted. Speciﬁcally, under
the η-malicious noise model [Val85, KL88], there is an unknown hypothesis w∗ and an unknown
instance distribution D selected from a certain family by an adversary. Each time with probability
1
x); with
·
that may
probability η, it instead is allowed to return an arbitrary pair (x, y)
depend on the state of the learning algorithm and the history of its outputs. Since this is a much
more challenging noise model, only recently has an algorithm with near-optimal noise tolerance
been established in [ABL17], although without attribute eﬃciency.
It is worth noting that the
problem of learning sparse halfspaces is also closely related to one-bit compressed sensing [BB08]
where one is allowed to utilize any distribution D over measurements for recovering the target
hypothesis. However, even with such strong condition, existing theory therein can only handle
label noise [PV13a, ABHZ16, BFN+17]. This naturally raises two fundamental questions: 1) can we
design attribute-eﬃcient learning algorithms that are capable of tolerating the malicious noise; and
2) can we still obtain near-optimal performance guarantees on the degree of noise tolerance and on
the sample complexity.

1, 1
}

× {−

Rd

∈

In this paper, we answer the two questions in the aﬃrmative under a mild distributional as-
sumption that D is chosen from the family of isotropic log-concave distributions [LV07, Vem10],
which covers prominent distributions such as normal distributions, exponential distributions, and
logistic distributions. Moreover, we take label complexity into consideration [CAL94], for which we
show that our bound is near-optimal in that aspect. We build our algorithm upon the margin-based
active learning framework [BBZ07], which queries the label of an instance when it has small “margin”
with respect to the currently learned hypothesis.

From a high level, this work can be thought of as extending the best known result of [ABL17]
to the high-dimensional regime. However, even under the low-dimensional setting where s = d, our
bound of label complexity is better than theirs in terms of the dependence on the dimension d: they
have a quadratic dependence whereas we have a linear dependence (up to logarithmic factors).
Moreover, as we will describe in Section 3, obtaining such algorithmic extension is nontrivial both
computationally and statistically. This work can also be viewed as an extension of [Zha18] to the
malicious noise model. In fact, our construction of empirical risk minimization is inspired by that
work. However, they considered only label noise which makes their algorithm and analysis not
applicable to our setting: it turns out that when facing malicious noise, a sophisticated design of
outlier removal paradigm is crucial for optimal noise tolerance [KLS09].

Also in line with this work is learning with nasty noise [DKS18] and robust sparse functional
estimation [BDLS17]. Both works considered more general setting in the following sense: [DKS18]
showed that by properly adapting the techniques in robust mean estimation, some more general
low-degree polynomial threshold functions and intersections of halfspaces, can be
concepts, e.g.
sample complexity; [BDLS17] showed that under proper spar-
eﬃciently learned with poly
can be achieved for many sparse
sity assumptions, a sample complexity bound of poly
estimation problems, such as generalized linear models with Lipschitz mapping functions and co-
variance estimation. However, we remark that neither of them obtained label eﬃciency. In addition,
O(ǫc) for some
when adapted to our setting, Theorem 1.5 of [DKS18] only handles noise rate η

s, log d, 1/ǫ

d, 1/ǫ

(cid:1)

(cid:0)

(cid:0)

(cid:1)

≤

2

constant c that is greater than one, while as to be shown in Section 4, we obtain the near-optimal
noise tolerance η
[BDLS17] achieved near-optimal noise tolerance but their analysis is
restricted to the Gaussian marginal distribution and Lipschitz mapping functions. In addition to
such fundamental diﬀerences, the main techniques we develop are distinct from theirs, which will
be described in more detail in Section 3.3.3.

O(ǫ).

≤

1.1 Main results

We informally present our main results below; readers are referred to Theorem 4 in Section 4 for a
precise statement.

Theorem 1 (Informal). Consider the malicious noise model with noise rate η. If the unlabeled
data distribution is isotropic log-concave and the underlying halfspace w∗ is s-sparse, then there is
(0, 1), PAC learns the underlying halfspace in
an algorithm that for any given target error rate ǫ
and the
polynomial time provided that η
≤
sample complexity is ˜O
ǫ s2 log5 d
.
(cid:1)

First of all, note that the noise tolerance is near-optimal as [KL88] showed that a noise rate
1+ǫ cannot be tolerated by any algorithm regardless of the computational power. The

greater than ǫ
following fact establishes the optimality of our label complexity.

O(ǫ). In addition, the label complexity is ˜O

s log4 d
ǫ

∈

(cid:0)

(cid:0)

(cid:1)

1

Lemma 2. Active learning of s-sparse halfspaces under isotropic log-concave distributions in the
realizable case has an information-theoretic label complexity lower bound of Ω

s(log 1

.

To see this lemma, observe that there exist ǫ-packings of s-sparse halfspaces with sizes ( 1

(cid:0)
s )Ω(s) [RWY11]; applying Theorem 1 of [KMT93] gives the lower bound.

and ( d

ǫ + log d
s )
(cid:1)
ǫ )Ω(s) [Lon95]

1.2 Related works

[KL88] presented a general analysis on eﬃciently learning halfspaces, showing that even without
any distributional assumptions, it is possible to tolerate the malicious noise at a rate of Ω(ǫ/d),
but a noise rate greater than ǫ
1+ǫ cannot be tolerated. The noise model was further studied by
[Sch92, Bsh98, CDF+99], and [KKMS05] obtained a noise tolerance Ω(ǫ/d1/4) when D is the uniform
[KLS09] improved this result to Ω(ǫ2/ log(d/ǫ)) for the uniform distribution, and
distribution.
showed a noise tolerance Ω(ǫ3/ log2(d/ǫ)) for isotropic log-concave distributions. A near-optimal
result of Ω(ǫ) was established in [ABL17] for both uniform and isotropic log-concave distributions.
Achieving attribute eﬃciency has been a long-standing goal in machine learning and statis-
tics [Blu90, BHL95], and has found a variety of applications with strong theoretical backend. A par-
tial list includes online classiﬁcation [Lit87], learning decision lists [Ser99, KS04, LS06], compressed
sensing [Don06, CW08, TW10, SL18], one-bit compressed sensing [BB08, PV16], and variable selec-
tion [FL01, FF08, SL17a, SL17b].

Label-eﬃcient learning has also been broadly studied since gathering high quality labels is often
expensive. The prominent approaches include disagreement-based active learning [Han11, Han14],
margin-based active learning [BBZ07, BL13, YZ17], selective sampling [CCG11, DGS12], and adap-
tive one-bit compressed sensing [ZYJ14, BFN+17]. There are also a number of interesting works that
appeal to extra information to mitigate the labeling cost, such as comparison [XZS+17, KLMZ17]
and search [BH12, BHLZ16].

Recent works such as [DKK+16, LRV16] studied mean estimation under a strong noise model
where in addition to returning dirty instances, the adversary has also the power of eliminating a
few clean instances, similar to the nasty noise model in learning halfspaces [BEK02]. The main

3

technique of robust mean estimation is a novel outlier removal paradigm, which uses the spectral
norm of the covariance matrix to detect dirty instances. This is similar in spirit to the idea of
[KLS09, ABL17] and the current work. However, there is no direct connection between mean
estimation and halfspace learning since the former is an unsupervised problem while the latter is
supervised (although any connection would be very interesting). Very recently, such technique was
extensively investigated in a variety of problems such as clustering and linear regression; we refer
the reader to a comprehensive survey by [DK19] for more information.

Roadmap. We collect useful notations and formally deﬁne the problem in Section 2. In Section 3,
we describe our algorithms, followed by a theoretical analysis in Section 4. We conclude this paper
in Section 5, and defer all proof details to the appendix.

2 Preliminaries

∈

We study the problem of learning sparse halfspaces in Rd under the malicious noise model with noise
[0, 1/2) [Val85, KL88], where an oracle EXη(D, w∗) (i.e. adversary) ﬁrst selects a member
rate η
; during the learning
D from a family of distributions
process, D and w∗ are ﬁxed. Each time the adversary is called, with probability 1
η, a random
x), referred to as a clean sample;
pair (x, y) is returned to the learner with x
·
with probability η, the adversary can return an arbitrary pair (x, y)
, referred to as a
1, 1
}
dirty sample. The adversary is assumed to have unrestricted computational power to search dirty
samples that may depend on, e.g. the states of the learning algorithm and the history of its outputs.
Formally, we make the following distributional assumptions.

and a concept w∗ from a concept class

D and y = sign (w∗

× {−

Rd

−

∼

D

∈

C

Assumption 1. Let
tribution D from which clean instances are drawn is chosen from
during the learning process. The learner is given the knowledge of

be the family of isotropic log-concave distributions. The underlying dis-
by the adversary, and is ﬁxed
but not of D.

D

D
D

Assumption 2. With probability 1
y = sign (w∗

−

x); with probability η, it may return an arbitrary pair (x, y)

η, the adversary returns a pair (x, y) where x
∼
.
1, 1
}

× {−

Rd

∈

D and

·

Since we are interested in obtaining a label-eﬃcient algorithm, we will consider a natural exten-
sion of such passive learning model. In particular, [ABL17] proposed to consider the following: when
a labeled instance (x, y) is generated, the learner only has access to an instance-generation oracle
EXx
η(D, w∗)
to obtain y. We refer to the total number of calls to EXx
η(D, w∗) as the sample complexity of the
learning algorithm, and to that of EXy

η(D, w∗) which returns x, and must make a separate call to a label revealing oracle EXy

η(D, w∗) as the label complexity.

·

C

∈ {

sign (w

= sign (w∗

We will presume that the concept class

consists of homogeneous halfspaces that have unit
is at most s where
. The learning algorithm is given this concept class, that is, the set of homogeneous
1, 2, . . . , d
}
, we deﬁne its error rate on a distribution D as errD(w) =
. The goal of the learner is to ﬁnd a hypothesis w in polynomial
(0, 1) and

ℓ2-norm and are s-sparse, i.e. the number of non-zero elements of any w
s
s-sparse halfspaces. For a hypothesis w
x)
x)
Prx∼D
δ, errD(w)
time such that with probability 1
(cid:1)
(cid:0)
−
(0, 1), with a few calls to EXx
any error rate ǫ

ǫ for any given failure conﬁdence δ
η(D, w∗).

∈
Rd and a positive scalar b, we call the region Xu,b :=

∈
For a reference vector u
x
u
| ≤
·
|
event x
∈
deﬁne the τ -hinge loss ℓτ (w; x, y) = max
deﬁne ℓτ (w; S) = 1
n

Rd :
as band, and we denote by Du,b the distribution obtained by conditioning D on the
}
Xu,b. Given a hypothesis w in Rd, a labeled instance (x, y), and a parameter τ > 0, we
n
i=1, we
(xi, yi)
}
{

≤
η(D, w∗) and EXy

. For a labeled set S =

n
i=1 ℓτ (w; xi, yi).

1
τ y(w

x
{

∈ C

∈ C

0, 1

x)

−

∈

∈

b

·

·

P

(cid:8)

(cid:9)

4

6
−

∈
∈

w
k

u
kp ≤

Rd :
r
. We will be particularly interested in the cases p = 1, 2,
}
Rd, the hard thresholding operation

For p
1, we denote by Bp(u, r) the ℓp-ball centering at the point u with radius r > 0, i.e.
≥
.
Bp(u, r) =
w
∞
{
Hs(u) keeps its s largest (in absolute value)
For a vector u
Rd be two vectors; we write θ(u, v) to denote
elements and sets the remaining to zero. Let u, v
∈
v to denote their inner product. For a matrix H, we denote by
the angle between them, and write u
k∗ its trace norm (also known as the nuclear norm), i.e. the sum of its singular values. We will
H
k
also use
k1 to denote the entrywise ℓ1-norm of H, i.e. the sum of absolute values of its entries.
If H is a symmetric matrix, we use H
Throughout this paper, the subscript variants of the lowercase letter c, e.g. c1 and c2, are
reserved for speciﬁc absolute constants that are uniquely determined by the distribution D. We
also reserve C1 and C2 for speciﬁc constants. We remark that the value of all the constants involved
in the paper does not depend on the underlying distribution D, but rather on the knowledge of
given to the learner. We collect all the deﬁnitions of these constants in Appendix A.

0 to denote that it is positive semideﬁnite.

H

(cid:23)

D

k

·

3 Main Algorithm

We ﬁrst present an overview of our learning algorithm, followed by specifying all the hyper-parameters
used therein. Then we describe in detail the attribute-eﬃcient outlier removal scheme, which is the
core technique in the paper.

3.1 Overview

Our main algorithm, namely Algorithm 1, is based on the celebrated margin-based active learning
framework [BBZ07]. The key observation is that a good classiﬁer can be learned by concentrating on
ﬁtting only the most informative labeled instances, measured by the closeness to the current decision
boundary (i.e. the closer the more informative). In our algorithm, the sampling region is set as
Rd at phase k = 1, and is set as the band Xwk−1,bk =
2.
Once we obtain the instance set ¯T , we perform a pruning step that removes all instances having
large ℓ∞-norm. This is motivated by our analysis that with high probability, all clean instances in ¯T
must have small ℓ∞-norm provided that Assumption 1 is satisﬁed. Since the oracle EXx
η(D, w∗) may
output dirty instances, we design an attribute-eﬃcient soft outlier removal procedure, which aims
to ﬁnd proper weights for all instances in T , such that the clean instances (i.e. those from Dwk−1,bk )
have overwhelming weights compared to dirty instances. Equipped with the learned weights, it is
possible to minimize the reweighted hinge loss to obtain a reﬁned halfspace. However, this would
lead to a suboptimal label complexity since we have to query the label for all instances in T . Our
remedy is to randomly sample a few points from T according to their importance, which is crucial
for us to obtain near-optimal label complexity.

wk−1 ·
|

at phases k

bk}

Rd :

x
{

| ≤

≥

∈

x

When minimizing the hinge loss, we carefully construct the constraint set Wk with three prop-
erties. First, it has an ℓ2-norm constraint. As a useful fact of isotropic log-concave distributions,
the ℓ2-distance to the underlying halfspace w∗ is of the same order as the error rate. Thus, if we
were able to ensure that the target halfspace w∗ stays in Wk, we would show that the error rate
of wk is as small as O(rk), the radius of the ℓ2-ball. Second, Wk has an ℓ1-norm constraint, which
is well-known for its power to promote sparse solutions and to guarantee attribute-eﬃcient sample
complexity [Tib96, CDS98, CT05, PV13b]. Lastly, the ℓ2 and ℓ1 radii of Wk shrinks by a constant
factor in each phase; hence, when Algorithm 1 terminates, the radius of the ℓ2-ball will be as small
as O(ǫ). Notably, [Zha18] also utilizes such constraint for active learning of sparse halfspaces, but
only under the setting of label noise.

5

Algorithm 1 Attribute and Label-Eﬃcient Algorithm Tolerating Malicious Noise
Require: Error rate ǫ, failure probability δ, sparsity parameter s, an instance generation oracle
η(D, w∗).

log

EXx

π
16c1ǫ

η(D, w∗), a label revealing oracle EXy
Ensure: A halfspace wk0 such that errD(wk0)
.
1: k0 ←
2: Initialize w0 as the zero vector in Rd.
(cid:1) (cid:7)
3: for phases k = 1, 2, . . . , k0 do
Clear the working set ¯T .
4:
If k = 1, independently draw nk instances from EXx
draw nk instances from EXx
η(D, w∗) conditioned on

≤

5:

(cid:6)

(cid:0)

ǫ with probability 1

δ.

−

6: Pruning: Remove all instances x in ¯T with
7:

Soft outlier removal: Apply Algorithm 2 to T with u
ξ
probability distribution p over T .
(cid:8)

2C2, and let q =

ξk, C

q(x)

x

x

η(D, w∗) and put them into ¯T ; otherwise,
wk−1 ·
| ≤
|
k∞ > c9 log 48nkd
bkδk
ρk,
wk−1, b
x∈T be the returned function. Normalize q to form a
(cid:9)

bk and put into ¯T .
to form a set T .

rk, ρ

bk, r

←

←

←

←

k

Independently draw mk instances (with replacement) from T
η(D, w∗) for their labels.

←

←
8: Random sampling: Sk ←
according to p and query EXy
Let Wk = B2(wk−1, rk)

9:

B1(wk−1, ρk). Find vk ∈

∩

Wk such that

ℓτk (vk; Sk)

min
w∈Wk

≤

ℓτk (w; Sk) + κ.

.

Hs(vk)
Hs(vk)

k2

10:

wk ←
11: end for
12: return wk0.

k

The last step in Algorithm 1 is to perform hard-thresholding

Hs on the solution vk followed by
ℓ2-normalization. Roughly speaking, these two steps will produce an iterate wk consistent with the
structure of w∗ (i.e. wk is guaranteed to belong to the concept class
), and more importantly, will
be useful to show that w∗ lies in Wk in all phases.

C

3.2 Hyper-parameter setting

−

2t exp(

4 exp

t) + c3π

We elaborate on our hyper-parameter setting that is used in Algorithm 1 and our analysis. Let g(t) =
, where the constants are speciﬁed in Appendix A. Ob-
c2
2−8π, since the continuous
serve that there exists an absolute constant ¯c
(cid:1)
(cid:1)
≥
and all the involved quantities in g(t) are absolute constants. Given
function g(t)
∞
→
2−k−3, τk = c0κ
, δk =
bk, 1/9
such constant ¯c, we set bk = ¯c
}
{

8π/c4 satisfying g(¯c)

δ
(k+1)(k+2) ,

+ 16 exp(

(cid:0)
→

0 as t

min

c4t
4π

−

≤

−

+

t)

(cid:0)

·

·

rk =

(

1,
2−k−3, k

k = 1
2

≥

, and ρk =

√s,
√2s

(

·

2−k−3, k

k = 1
2

≥

.

¯c), and choose ξk = min

We set the constant κ = exp(

−

that all ξk’s are lower bounded by the constant c6 := min
theoretical guarantee holds for any noise rate η
1.
log
We set the total number of phases k0 =
as the size of unlabeled instance set ¯T . We will show
log d + log3 1
δk

in Algorithm 1. Consider any phase k

√C2¯c2 + C2
(cid:1)
(cid:9)
2π ¯cc1c6.
(cid:17)

1 + 4√C2zk/τk
1 + 4
(cid:0)
c5ǫ, where the constant c5 := c8
(cid:16)
π
16c1ǫ

We use nk = ˜O

. Observe
−2

s2 log4 d

. Our

c0κ¯c

≤

≥

o

16

16

1

2 , κ2
2 , κ2
1
(cid:8)
n

−2

(cid:0)

(cid:1) (cid:7)

bk ·

(cid:0)

(cid:0)

(cid:6)
(cid:1)(cid:1)

6

nk/bk

that by making Nk = O
each phase with high probability. We set mk = ˜O
(cid:0)
set Sk, which is also the number of calls to EXy
complexity of Algorithm 1, and m :=

calls to EXx

(cid:1)

η(D, w∗), Algorithm 1 is guaranteed to obtain such ¯T in
as the size of labeled instance
k0
k=1 Nk is the sample

η(D, w∗). Note that N :=
(cid:0)

d
bkδk ·

log d
δk

s log2

(cid:1)

k0
k=1 mk is its label complexity.

P

P
3.3 Attribute and computationally eﬃcient soft outlier removal

Our soft outlier removal procedure is inspired by [ABL17]. We ﬁrst brieﬂy describe their main idea.
Then we introduce a natural extension of their approach to the high-dimensional regime and show
why it fails. Lastly, we present our novel outlier removal scheme.

∈

To ease our discussion, we decompose T = TC ∪

TD where TC is the set of clean instances in
T and TD consists of all dirty instances. Ideally, we would expect to ﬁnd a function q : T
[0, 1]
TC and q(x) = 0 otherwise. Suppose that ξ is the fraction of dirty
such that q(x) = 1 for all x
instances in T . Then one would expect that the total weights
T
|
in order to include such ideal function. On the other hand, we must restrict the weights of dirty
instances; namely, we need to characterize under what conditions TC can be distinguished from TD.
The key observation made in [KLS09] and [ABL17] is that if the dirty instances want to deteriorate
the hinge loss (which is the purpose of the adversary), they must lead to a variance3 of w
x orders of
magnitude larger than Ω(b2 + r2) on the direction of a particular halfspace. Thus, it suﬃces to ﬁnd
a proper weight for each instance, such that the reweighted variance 1
x)2 is as small
|T |
as O(b2 + r2) for all feasible halfspaces w
W . Now it remains to resolve two questions: 1) how
P
many instances do we need to draw in order to guarantee the existence of such function q; and
2) how to ﬁnd a feasible function q in polynomial time.

x∈T q(x) is as large as (1

x∈T q(x)(w

P

→

ξ)

−

∈

|

·

·

If label complexity were our only objective, we could have used the soft outlier removal pro-
cedure of [ABL17] directly, i.e. we set W = B2(u, r), which in conjunction with the ℓ1-norm
constrained hinge loss minimization of [Zha18] would result in an ˜O
sample complexity and a
poly
label complexity. However, as we would also like to optimize for the learner’s
sample complexity by utilizing the sparsity assumption, we need an attribute-eﬃcient outlier removal
(cid:0)
procedure.

s, log d, log(1/ǫ)

d2
ǫ

(cid:1)

(cid:1)

(cid:0)

3.3.1 A natural approach and why it fails

It is well-known that incorporating an ℓ1-norm constraint often leads to a sample complexity sub-
linear in the dimension [Zha02, KST08]. Thus, a natural approach for attribute-eﬃcient outlier
removal is to set W = B2(u, r)
B1(u, ρ) for some carefully chosen radius ρ > 0. With the new lo-
calized concept space, it is possible to show that a sample size of poly (s, log d) suﬃces to guarantee
W . However, on
the existence of a function q such that the reweighted variance is small over all w
W ,
the computational side, for a given q, we will have to check the reweighted variance for all w
which amounts to ﬁnding a global optimum of the following program:

∩

∈

∈

max
w∈Rd

1
T
|

| Xx∈T

q(x)(w

·

x)2, s.t.

w

k

u
k2 ≤

−

r,

w
k

u
k1 ≤

−

ρ.

(3.1)

The above program is closely related to the problem of sparse principal component analysis (PCA) [ZHT06],
and unfortunately it is known that ﬁnding a global optimum is NP-hard [Ste05, TP14].

3We follow [ABL17] and slightly abuse the word “variance” without subtracting the squared mean of w · x.

7

Algorithm 2 Attribute-Eﬃcient Localized Soft Outlier Removal
Require: Reference vector u, band width b, radius r for ℓ2-ball, radius ρ for ℓ1-ball, empirical noise

rate ξ, absolute constant C, a set of unlabeled instances T where for all x

Ensure: A function q : T

[0, 1].

1: Deﬁne the convex set of matrices
2: Find a function q : T

H
k
∈
[0, 1] satisfying the following constraints:

M

H

(cid:23)

=

0,

k∗ ≤

Rd×d : H

→

→

(cid:8)

T ,

x

u

|

·

| ≤

b.

∈

r2,

H
k

k1 ≤

ρ2

.

(cid:9)

1. for all x

T, 0

q(x)

1;

∈
x∈T q(x)

2.

(1

≤

ξ)

≤
;
T
|
|
x∈T q(x)x⊤Hx

−

≥

3. supH∈M

P

1
|T |

3: return q.

P

C(b2 + r2).

≤

3.3.2 Convex relaxation of sparse principal component analysis

∈

Our goal is to ﬁnd a function q such that the objective value in (3.1) is less than O
for all
w
W . To circumvent the computational intractability caused by the non-convexity of the objective
function, we consider an alternative formulation using semideﬁnite programming (SDP), similar to
x)2.
the approach of [dGJL07]. First, let v = w
b2 with probability 1. Thus, we only need
Due to our localized sampling scheme, we have (u
to examine the maximum value of
B1(0, ρ). Now the
technique of [dGJL07] comes in: the rank-one symmetric matrix vv⊤ is replaced by a new variable
Rd×d which is positive semideﬁnite, and the vector ℓ2 and ℓ1-norm constraints are relaxed to
H
the matrix trace and ℓ1-norm constraints respectively as follows:

u. It is not hard to see that (w

≤
x)2 over v

x∈T q(x)(v

x)2 +2(v

B2(0, r)

x)2

x)2

1
|T |

2(u

P

−

≤

∈

∈

∩

(cid:1)

(cid:0)

·

·

·

·

·

b2 + r2

max
H∈Rd×d

1
T
|

| Xx∈T

q(x)x⊤Hx, s.t. H

0,

H

k

k∗ ≤

(cid:23)

r2,

H

k

k1 ≤

ρ2.

(3.2)

The program (3.2) has two salient features: ﬁrst, it is a semideﬁnite program that can be optimized
eﬃciently [BV04]; second, if its objective value is upper bounded by O
, we immediately
obtain that the reweighted variance is well controlled. This is the theme of the following lemma.

b2 + r2

(cid:0)
Lemma 3. Suppose that Assumption 1 and 2 are satisﬁed, and that η
constant C2 > 2 such that the following holds. For any phase k of Algorithm 1 with 1
r2
k,
write
Mk =
draw of TC, we have

. Then with probability 1

Rd×d : H

c5ǫ. There exists a
k0,
k
≤
≤
δk
24 over the

k1 ≤

k∗ ≤

H
k

ρ2
k

H

H

≤

−

(cid:23)

0,

∈

k

(cid:1)

n

o
2C2(b2

k + r2

k),

x⊤Hx

≤

1

sup
H∈Mk

|

TC| Xx∈TC
log d + log2 1
δk

provided that

˜O

TC| ≥
|

s2 log4 d
(cid:16)
Recall that Algorithm 1 sets nk = ˜O

bk ·

(cid:0)

s2 log4 d

log d + log3 1
, which suﬃces to guarantee
δk
holds (see Appendix D.2); therefore, the above concentration bound holds
the condition on
(cid:0)
with high probability. As a result, it is not hard to verify that the function q : T
[0, 1], where
TD, satisﬁes all three constraints in Algorithm 2. In
q(x) = 1 for all x
other words, Lemma 3 establishes the existence of a feasible function q to Algorithm 2. Furthermore,
observe that the optimization problem of ﬁnding a feasible q in Algorithm 2 is a semi-inﬁnite linear

TC|
|
TC and q(x) = 0 for all x

→

(cid:1)(cid:1)

∈

∈

(cid:0)

.
(cid:1)(cid:17)
bk ·

8

it checks if q
program. For a given candidate q, we can construct an eﬃcient oracle as follows:
violates the ﬁrst two constraints; if not, it checks the last constraint by invoking a polynomial-time
SDP solver to ﬁnd the maximum objective value of (3.2). It is well-known that equipped with such
separation oracle, Algorithm 2 will return a desired function q in polynomial time by the ellipsoid
method [GLS12, Chapter 3].

3.3.3 Comparison to prior works

s2
We remark that the setting of nk results in a sample complexity of ˜O
bk
statement in Lemma 6), which implies a total sample complexity of ˜O
(cid:1)
substantially improves upon the sample complexity of ˜O
removal procedure in [ABL17].

for phase k (see a formal
s2
ǫ

d, this
when naively applying the soft outlier

. When s

d2
ǫ

≪

(cid:1)

(cid:0)

(cid:0)

We remark three crucial technical diﬀerences from [DKS18] and [BDLS17]. First, we progres-
sively restrict the variance to identify dirty instances, i.e. the variance upper bound is set as O(1)
at the beginning of Algorithm 1 and progressively decreases to O(ǫ2) (see our setting of bk and rk),
while in [DKS18, BDLS17] and many of their follow-up works it is typically ﬁxed to O(ǫ). Second,
we control the variance locally, i.e. we only require a small variance over a localized instance space
Dwk−1,bk and a localized concept space
Mk. Third, the small variance is used to robustly estimate
the hinge loss in our work, while in [DKS18] it was utilized to approximate the Chow parameters.
All these problem-speciﬁc design of outlier removal are vital for us to obtain the ﬁrst near-optimal
guarantee on attribute eﬃciency and label eﬃciency for learning sparse halfspaces.

(cid:0)

(cid:1)

4 Performance Guarantee

In the following, we always presume that the underlying halfspace is parameterized by w∗, which is
s-sparse and has unit ℓ2-norm. This condition may not be explicitly stated in our analysis.

Our main theorem is as follows. We note that there are two sources of randomness in Algorithm 1:
η(D, w∗), and the random sampling step (i.e. Step 8); the

the random draw of instances from EXx
probability is taken over all the randomness in the algorithm.

Theorem 4. Suppose that Assumptions 1 and 2 are satisﬁed. There exists an absolute constant
c5 such that for any ǫ
δ,
(0, 1), if η
≤
ǫ where wk0 is the output of Algorithm 1. Furthermore, Algorithm 1 has a sample
errD(wk0)
complexity of ˜O
, and
has running time poly

∈
log d + log3 1
δ
.

c5ǫ, then with probability at least 1

, and a label complexity of ˜O

∈
ǫ s2 log4 d

(0, 1) and δ

log d
δ ·

s log2 d

log 1
ǫ

ǫδ ·

≤

−

1

·

(cid:0)

d, 1/ǫ, 1/δ
(cid:0)

(cid:1) (cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Algorithm 1 can be straightforwardly modiﬁed to work in the passive learning setting, where the
learner has direct access to the labeled instance oracle EXη(D, w∗). The modiﬁed algorithm works
as follows: it calls EXη(D, w∗) to obtain a pair of instance and the label whenever Algorithm 1 calls
η(D, w∗). In particular, for the passive learning algorithm, the working set ¯T is always a labeled
EXx
instance set, and there is no need for it to query EXy

η(D, w∗) in the random sampling step.

We have the following simple corollary which is an immediate result from Theorem 4.

Corollary 5. Suppose that Assumptions 1 and 2 are satisﬁed. There exists a polynomial-time
algorithm (that has access to only EXη(D, w∗)) and an absolute constant c5 such that for any ǫ
(0, 1) and δ
with error at most ǫ, using ˜O

∈
δ, the algorithm outputs a hypothesis

c5ǫ, then with probability at least 1

labeled instances.

(0, 1), if η

−

≤

∈

1

ǫ s2 log4 d

log d + log3 1
δ

·

(cid:0)

(cid:0)

(cid:1) (cid:1)

9

We need an ensemble of new results to prove Theorem 4. Speciﬁcally, we propose new techniques
to control the sample and computational complexity of soft outlier removal, and a new analysis of
label complexity by making full use of the localization in the instance and concept spaces. We
elaborate on them in the following, and sketch the proof of Theorem 4 at the end of this section.

4.1 Localized sampling in the instance space

Localized sampling, also known as margin-based active learning, is a useful technique proposed in
[BBZ07]. Interestingly, under isotropic log-concave distributions, [BL13] showed that if the band
, can be safely
width b is large enough, the region outside the band, i.e.
}
“ignored”, in the sense that, if w is close enough to w∗, it is guaranteed to incur a small error rate
therein. Motivated by this elegant ﬁnding, theoretical analyses in the literature are often dedicated
to bounding the error rate within the band, and it is now well understood that a constant error rate
within the band suﬃces to ensure signiﬁcant progress in each phase [ABHU15, ABL17, Zha18]. We
follow this line of reasoning and our technical contribution is to show how to obtain such constant
error rate with near-optimal label complexity and noise tolerance.

Rd :

x
{

w
|

> b

∈

x

|

·

Our analysis will rely on the condition that ¯T has suﬃciently many instances. Speciﬁcally, in
order to collect nk instances to form the working set ¯T , we need to call EXx
η(D, w∗) enough number
of times since our sampling is localized within the band Xk :=
. The following
x
wk−1 ·
|
lemma characterizes the sample complexity at phase k.

| ≤

x :

bk

(cid:8)

(cid:9)

Lemma 6. Suppose that Assumption 1 and 2 are satisﬁed. Further assume η < 1
1
number of Nk = O

δk
4 , we will obtain nk instances that fall into the band Xk =
nk + log 1
δk

calls to the instance generation oracle EXx

wk−1 ·
|

x :
{

1
bk

−

x

bk}
| ≤
η(D, w∗).

2 . With probability
by making a

4.2 Attribute and computationally eﬃcient soft outlier removal

(cid:0)

(cid:0)

(cid:1)(cid:1)

We summarize the performance guarantee of Algorithm 2 in the following proposition.

Proposition 7. Consider phase k of Algorithm 1 for any 1
and 2 are satisﬁed, and that η
¯T , Algorithm 2 will output a function q : T
(1) 1
1
|T |

ξk; (2) for all w

→
Wk, 1
|T |

x∈T q(x)

c5ǫ. With the setting of nk, with probability 1

k0. Suppose that Assumption 1
δk
8 over the draw of
[0, 1] in polynomial time with the following properties:

−
k + r2
b2
k

x∈T q(x)(w

≤

≤

≤

k

.

≥

−

∈

x)2

·

≤

5C2

P

P
Again, we remind that the key diﬀerence between our algorithm and that of [ABL17] is in Con-
x∈T q(x)x⊤Hx of the reweighted
straint 3 of Algorithm 2: we require that the “variance proxy”
instances are small for all positive semideﬁnite H that lies in an intersection of a trace-norm ball
and an ℓ1-norm ball. On the statistical side, this favorable constraint set of H, in conjunction with
Adamczak’s bound in empirical processes literature [Ada08], results in suﬃcient uniform concen-
tration of the variance proxy x⊤Hx with a sample complexity of poly (s, log d). This signiﬁcantly
improves the sample complexity of poly (d) established in [ABL17]. The detailed proof can be found
in Appendix D.3.

P

(cid:1)

(cid:0)

Remark 1. While in some standard settings, a proper ℓ1-norm constraint suﬃces to guarantee a
desired bound of sample complexity in the high-dimensional regime [Wai09, KST08], we note that in
order to establish near-optimal noise tolerance, the ℓ2-norm constraint of w (hence the trace-norm
of H) is vital as well. Though eliminating it eases the search of a feasible function q, this leads to
Ω(ǫ/s). Informally speaking, the per-phase error rate, expected
a suboptimal noise tolerance η
x)2 times ξk, the noise rate within
to be a constant, is inherently proportional to the variance (w
the band. Now without the trace-norm constraint, the variance would be s times larger than before

≤

·

10

(since we now have to use ρ2
k) as a proxy for the constraint set’s radius, measured in trace
norm). This implies that we need to set ξk a factor 1/s of before, which in turn indicates that the
noise tolerance η becomes a factor 1/s of before since η/ǫ
ξk. We refer the reader to Proposition 31
and Lemma 36 for details.

k = O(sr2

≈

Remark 2. The quantity nk has a quadratic dependence on the sparsity parameter s. This can-
not be improved in some sparse PCA related problems [BR13], but it is not clear whether such
dependence is optimal in our case. We leave this investigation to future work.

k + r2

Next, we describe the statistical property of the distribution p (obtained by normalizing q
ξk
returned by Algorithm 2). Observe that the noise rate within the band is at most η/bk ≤
since the probability mass of the band is Θ(bk) – an important property of isotropic log-concave
distributions. Also, it is possible to show that the variance of clean instances on directions H
∈ Mk
is O(b2
k) (see Lemma 16). Therefore, Algorithm 2 is essentially searching for a weighting such
that clean instances have overwhelming weights over dirty instances, and that the variance of the
T is the set of clean
weighted instances is similar to that of the clean instances. Recall that TC ⊂
instances in T . Let ˜TC =
}x∈TC be the unrevealed labeled set where each instance is correctly
annotated by w∗. The following proposition, which is similar to Lemma 4.7 of [ABL17] but with
reﬁnement, states that the reweighted hinge loss ℓτk (w; p) :=
x∈T p(x)ℓτk (w; x, yx), is a good proxy
for the hinge loss evaluated exclusively on clean labeled instances ˜TC.

(x, yx)
{

O(η/ǫ)

≤

P

−

Proposition 8. Suppose Assumption 1 and 2 are satisﬁed, and η
Algorithm 1, with probability 1

≤
4 over the draw of ¯T , we have supw∈Wk
δk
Note that though this proposition is phrased in terms of the hinge loss on pairs (x, yx), it is
only used in the analysis and our algorithm does not require the knowledge of the labels yx – the
algorithm even does not need to exactly identify the set of clean instances TC. As a result, the size of
TC does not count towards our label complexity. Proposition 7 together with Proposition 8 implies
that with high probability, Algorithm 2 produces a desired probability distribution in polynomial
time, which justiﬁes its computational and statistical eﬃciency.
x)

c5ǫ. For any phase k of
ℓτk (w; ˜TC)
κ.
(cid:12)
(cid:12)

In addition, let Lτk (w) := Ex∼Dwk−1,bk

be the expected loss on Dwk−1,bk .

w; x, sign (w∗

ℓτk (w; p)

ℓτk

−

≤

(cid:12)
(cid:12)

The following result links Lτk (w) to the empirical hinge loss on clean instances.

(cid:2)

(cid:0)

Proposition 9. Under Assumption 1 and 2, and η
probability 1

≤
4 over the draw of ¯T , we have supw∈Wk
δk

−

c5ǫ, for any phase k of Algorithm 1, with
Lτk (w)

κ.

·

(cid:1)(cid:3)

−

ℓτk (w; ˜TC)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)

4.3 Attribute and label-eﬃcient empirical risk minimization

In light of Proposition 8, one may want to ﬁnd an iterate by minimizing its reweighted hinge loss
ℓτk (w; p). This requires collecting labels for all instances in T , which leads to a suboptimal label
complexity O
. As a remedy, we perform a random sampling process, which
draws mk instances from T according to the distribution p and then query their labels, resulting
in the labeled instance set Sk. By standard uniform convergence arguments, it is expected that
ℓτk (w; Sk)

ℓτk (w; p) provided that mk is large enough, as is shown in the following proposition.

polylog

d, 1/ǫ

s2

(cid:1)(cid:1)

(cid:0)

(cid:0)

·

≈

δk
4 , we have supw∈Wk

Proposition 10. Suppose that Assumption 1 and 2 are satisﬁed. For any phase k of Algorithm 1,
with probability 1

−
We remark that when establishing the performance guarantee, the ℓ1-norm constraint on the
hypothesis space, together with an ℓ∞-norm upper bound on the localized instance space, leads to
a Rademacher complexity that has a linear dependence on the sparsity (up to a logarithmic factor).

ℓτk (w; Sk)

ℓτk (w; p)

κ.

≤

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

11

Technically speaking, our analysis is more involved than that of [ABL17]: applying their analysis
to the setting of learning sparse halfspaces along with the fact that the VC dimension of the class
of s-sparse halfspaces is O(s log(d/s)) would give a label complexity quadratic in s.

4.4 Uniform concentration for unbounded data

Our analysis involves building uniform concentration bounds. The primary issue of applying stan-
dard concentration results, e.g. Theorem 1 of [KST08], is that the instances are not contained in
a pre-speciﬁed ℓ∞-ball with probability 1 under isotropic log-concave distribution. [ABL17, Zha18]
construct a conditional distribution, on which the data are all bounded from above, and then mea-
sure the diﬀerence between this conditional distribution and the original one. We circumvent such
technical complication by using the Adamczak’s bound [Ada08] in the empirical process literature,
which provides a generic way to analyze concentration inequalities for well-behaved distributions
with unbounded support. See Appendix C for a concrete treatment.

4.5 Proof sketch of Theorem 4

∈

≤

Wk. Therefore, if w∗
Lτk (w∗) + 7κ

Proof. We ﬁrst show that error rate of vk on Dwk−1,bk is a constant, and that of wk follows since hard
thresholding and ℓ2-norm projection can only deviate the error rate by a constant factor. Observe
3κ
that in light of Proposition 8, Proposition 9, and Proposition 10, we have
Wk, by the optimality of vk, we have Lτk (vk)
for all w
≤
ℓτk (w∗; Sk) + 4κ
8κ, where the last inequality is by Lemma 3.7 of [ABL17]. Since
(vk), the constant error rate on Dwk−1,bk
Lτk (vk) always serves as an upper bound of errDwk−1,bk
follows. Next we can use the analysis framework of margin-based active learning to show that such
constant error rate ensures that the angle between wk and w∗ is as small as O(2−k), which in turn
implies w∗
W1; this can be easily seen by the deﬁnition of W1:
∈
k0. Observe that the radius
W1 = B2(0, 1)
of ℓ2-ball of Wk0 is as small as ǫ, which, by a basic property of isotropic log-concave distributions,
implies the error rate of wk0 on D is less than ǫ.

B1(0, √s). Hence, we conclude w∗

Wk+1. It remains to show w∗

Lτk (w)
≤
ℓτk (vk; Sk)+ 3κ

Wk for all 1

ℓτk (w; Sk)

∈
≤

≤

−

≤

≤

∩

∈

∈

(cid:12)
(cid:12)

(cid:12)
(cid:12)

k

The sample and label complexity bounds follow from our setting of Nk and mk, and the fact

that bk ∈

[ǫ, ¯c/16] for all k

≤

k0. See Appendix D.5 for the full proof.

5 Conclusion and Open Questions

We have presented a computationally eﬃcient algorithm for learning sparse halfspaces under the
challenging malicious noise model. Our algorithm leverages the well-established margin-based ac-
tive learning framework, with a particular treatment on attribute eﬃciency, label complexity, and
noise tolerance. We have shown that our theoretical guarantees for label complexity and noise tol-
erance are near-optimal, and the sample complexity of a passive learning variant of our algorithm
is attribute-eﬃcient, thanks to the set of new techniques proposed in this paper.

We raise three open questions for further investigation. First, as we discussed in Section 4.2,
the sample complexity for concentration of x⊤Hx has a quadratic dependence on s. It would be
interesting to study whether this is a fundamental limit of learning under isotropic log-concave
distributions, or it can be improved by a more sophisticated localization scheme in the instance
and the concept spaces. Second, while isotropic log-concave distributions bear favorable properties
that ﬁt perfectly in the margin-based framework, it would be interesting to examine whether the
established results can be extended to heavy-tailed distributions. This may lead to a large error
rate within the band that cannot be controlled at a constant level, and new techniques must be

12

developed. Finally, it would be interesting to design computationally more eﬃcient algorithms,
e.g. stochastic gradient descent-type algorithms similar to [DKM05], with comparable statistical
guarantees.

13

References

[ABHU15] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Eﬃcient
learning of linear separators under bounded noise. In Proceedings of the 28th Annual
Conference on Learning Theory, pages 167–190, 2015.

[ABHZ16] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learn-
ing and 1-bit compressed sensing under asymmetric noise. In Proceedings of the 29th
Annual Conference on Learning Theory, pages 152–192, 2016.

[ABL17]

[Ada08]

[BB08]

Pranjal Awasthi, Maria-Florina Balcan, and Philip M. Long. The power of localization
for eﬃciently learning linear separators with noise. Journal of the ACM, 63(6):50:1–
50:27, 2017.

Radoslaw Adamczak. A tail inequality for suprema of unbounded empirical processes
with applications to Markov chains. Electronic Journal of Probability, 13(34):1000–1034,
2008.

Petros Boufounos and Richard G. Baraniuk. 1-bit compressive sensing. In Proceedings
of the 42nd Annual Conference on Information Sciences and Systems, pages 16–21,
2008.

[BBZ07] Maria-Florina Balcan, Andrei Z. Broder, and Tong Zhang. Margin based active learning.
In Proceedings of the 20th Annual Conference on Learning Theory, pages 35–50, 2007.

[BDLS17]

Sivaraman Balakrishnan, Simon S. Du, Jerry Li, and Aarti Singh. Computationally
eﬃcient robust sparse estimation in high dimensions. In Proceedings of the 30th Annual
Conference on Learning Theory, pages 169–212, 2017.

[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth.
Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–
965, 1989.

[BEK02]

Nader H. Bshouty, Nadav Eiron, and Eyal Kushilevitz. PAC learning with nasty noise.
Theoretical Computer Science, 288(2):255–275, 2002.

[BFKV96] Avrim Blum, Alan M. Frieze, Ravi Kannan, and Santosh S. Vempala. A polynomial-
time algorithm for learning noisy linear threshold functions. In Proceedings of the 37th
Annual IEEE Symposium on Foundations of Computer Science, pages 330–338, 1996.

[BFN+17] Richard G. Baraniuk, Simon Foucart, Deanna Needell, Yaniv Plan, and Mary Wootters.
Exponential decay of reconstruction error from binary measurements of sparse signals.
IEEE Transactions on Information Theory, 63(6):3368–3385, 2017.

[BH12]

Maria Florina Balcan and Steve Hanneke. Robust interactive learning. In Conference
on Learning Theory, pages 20–1, 2012.

[BHL95]

Avrim Blum, Lisa Hellerstein, and Nick Littlestone. Learning in the presence of ﬁnitely
or inﬁnitely many irrelevant attributes. Journal of Computer and System Sciences,
50(1):32–40, 1995.

14

[BHLZ16] Alina Beygelzimer, Daniel J. Hsu, John Langford, and Chicheng Zhang. Search improves
In Proceedings of the 30th Annual Conference on Neural

label for active learning.
Information Processing Systems, pages 3342–3350, 2016.

[BL13]

[Blu90]

[BM02]

[BR13]

[Bsh98]

[BV04]

[CAL94]

Maria-Florina Balcan and Philip M. Long. Active and passive learning of linear sepa-
rators under log-concave distributions. In Proceedings of The 26th Annual Conference
on Learning Theory, pages 288–316, 2013.

Avrim Blum. Learning boolean functions in an inﬁnite attribute space. In Proceedings
of the 22nd Annual ACM Symposium on Theory of Computing, pages 64–72, 1990.

Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk
bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.

Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse
principal component detection. In Proceedings of the 26th Annual Conference on Learn-
ing Theory, pages 1046–1066, 2013.

Nader H. Bshouty. A new composition theorem for learning algorithms. In Proceedings
of the 30th Annual ACM Symposium on the Theory of Computing, pages 583–589, 1998.

Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University
Press, 2004.

David Cohn, Les Atlas, and Richard Ladner.
learning. Machine Learning, 15(2):201–221, 1994.

Improving generalization with active

[CCG11] Giovanni Cavallanti, Nicolò Cesa-Bianchi, and Claudio Gentile. Learning noisy linear
classiﬁers via adaptive and selective sampling. Machine Learning, 83(1):71–102, 2011.

[CDF+99] Nicolò Cesa-Bianchi, Eli Dichterman, Paul Fischer, Eli Shamir, and Hans Ulrich Simon.
Sample-eﬃcient strategies for learning in the presence of noise. Journal of the ACM,
46(5):684–719, 1999.

[CDS98]

Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposi-
tion by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.

[CT05]

[CW08]

[Dan15]

Emmanuel J. Candès and Terence Tao. Decoding by linear programming. IEEE Trans-
actions on Information Theory, 51(12):4203–4215, 2005.

Emmanuel J. Candès and Michael B. Wakin. An introduction to compressive sampling.
IEEE Signal Processing Magazine, 25(2):21–30, 2008.

Amit Daniely. A PTAS for agnostically learning halfspaces. In Proceedings of The 28th
Annual Conference on Learning Theory, volume 40, pages 484–502, 2015.

[dGJL07] Alexandre d’Aspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanck-
riet. A direct formulation for sparse PCA using semideﬁnite programming. SIAM
Review, 49(3):434–448, 2007.

[DGS12]

Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active
learning from single and multiple teachers. Journal of Machine Learning Research,
13:2655–2697, 2012.

15

[DGT19]

Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos. Distribution-independent
PAC learning of halfspaces with Massart noise.
In Proceedings of the 33rd Annual
Conference on Neural Information Processing Systems, pages 4751–4762, 2019.

[DK19]

Ilias Diakonikolas and Daniel M. Kane.
dimensional robust statistics. CoRR, abs/1911.05911, 2019.

Recent advances in algorithmic high-

[DKK+16]

Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Zheng Li, Ankur Moitra,
and Alistair Stewart. Robust estimators in high dimensions without the computational
intractability. CoRR, abs/1604.06443, 2016.

[DKM05]

Sanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-
based active learning. In Proceedings of the 18th Annual Conference on Learning Theory,
pages 249–263, 2005.

[DKS18]

Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Learning geometric concepts
with nasty noise. In Proceedings of the 50th Annual ACM Symposium on Theory of
Computing, pages 1061–1073, 2018.

[DKTZ20]

Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zariﬁs. Learning half-
spaces with Massart noise under structured distributions. In Proceedings of the 33rd
Annual Conference on Learning Theory, volume 125, pages 1486–1513, 2020.

[Don06]

[Dud14]

[Fel14]

[FF08]

[FL01]

David L. Donoho. Compressed sensing. IEEE Transactions on Information Theory,
52(4):1289–1306, 2006.

Richard M. Dudley. Uniform central limit theorems, volume 142. Cambridge University
Press, 2014.

Vitaly Feldman. Open problem: The statistical query complexity of learning sparse half-
spaces. In Proceedings of The 27th Annual Conference on Learning Theory, volume 35,
pages 1283–1289, 2014.

Jianqing Fan and Yingying Fan. High dimensional classiﬁcation using features annealed
independence rules. Annals of Statistics, 36(6):2605–2637, 2008.

Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and
its oracle properties. Journal of the American statistical Association, 96(456):1348–1360,
2001.

[GLS12] Martin Grötschel, László Lovász, and Alexander Schrijver. Geometric algorithms and

combinatorial optimization, volume 2. Springer Science & Business Media, 2012.

[Han11]

[Han14]

Steve Hanneke. Rates of convergence in active learning. The Annals of Statistics,
39(1):333–361, 2011.

Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends
in Machine Learning, 7(2-3):131–309, 2014.

[KKMS05] Adam Tauman Kalai, Adam R. Klivans, Yishay Mansour, and Rocco A. Servedio. Ag-
nostically learning halfspaces. In Proceedings of the 46th Annual IEEE Symposium on
Foundations of Computer Science, pages 11–20, 2005.

16

[KL88]

In
Michael J. Kearns and Ming Li. Learning in the presence of malicious errors.
Proceedings of the 20th Annual ACM Symposium on Theory of Computing, pages 267–
280, 1988.

[KLMZ17] Daniel M. Kane, Shachar Lovett, Shay Moran, and Jiapeng Zhang. Active classiﬁcation
with comparison queries. In Chris Umans, editor, Proceedings of the 58th Annual IEEE
Symposium on Foundations of Computer Science, pages 355–366, 2017.

[KLS09]

Adam R. Klivans, Philip M. Long, and Rocco A. Servedio. Learning halfspaces with
malicious noise. Journal of Machine Learning Research, 10:2715–2740, 2009.

[KMT93]

Sanjeev R. Kulkarni, Sanjoy K. Mitter, and John N. Tsitsiklis. Active learning using
arbitrary binary valued queries. Machine Learning, 11(1):23–35, 1993.

[KS04]

[KSS92]

[KST08]

[Lit87]

[Lon95]

[LRV16]

[LS06]

[LV07]

[MN06]

[PV13a]

[PV13b]

Adam R. Klivans and Rocco A. Servedio. Toward attribute eﬃcient learning of decision
lists and parities. In Proceedings of the 17th Annual Conference on Learning Theory,
pages 224–238, 2004.

Michael J. Kearns, Robert E. Schapire, and Linda Sellie. Toward eﬃcient agnostic
learning. In David Haussler, editor, Proceedings of the 5th Annual Conference on Com-
putational Learning Theory, pages 341–352, 1992.

Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear
prediction: Risk bounds, margin bounds, and regularization. In Proceedings of the 22nd
Annual Conference on Neural Information Processing Systems, pages 793–800, 2008.

Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-
threshold algorithm (extended abstract). In Proceedings of the 28th Annual IEEE Sym-
posium on Foundations of Computer Science, pages 68–77, 1987.

Philip M. Long. On the sample complexity of PAC learning half-spaces against the
uniform distribution. IEEE Transactions on Neural Networks, 6(6):1556–1559, 1995.

Kevin A. Lai, Anup B. Rao, and Santosh S. Vempala. Agnostic estimation of mean
and covariance. In Proceedings of the 57th Annual IEEE Symposium on Foundations of
Computer Science, pages 665–674, 2016.

Philip M. Long and Rocco A. Servedio. Attribute-eﬃcient learning of decision lists
and linear threshold functions under unconcentrated distributions. In Proceedings of
the 20th Annual Conference on Neural Information Processing Systems, pages 921–928,
2006.

László Lovász and Santosh S. Vempala. The geometry of logconcave functions and
sampling algorithms. Random Structures and Algorithms, 30(3):307–358, 2007.

Pascal Massart and Élodie Nédélec. Risk bounds for statistical learning. The Annals
of Statistics, pages 2326–2366, 2006.

Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming.
Communications on Pure and Applied Mathematics, 66(8):1275–1297, 2013.

Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logis-
tic regression: A convex programming approach. IEEE Transactions on Information
Theory, 59(1):482–494, 2013.

17

[PV16]

[Ros58]

Yaniv Plan and Roman Vershynin. The generalized lasso with non-linear observations.
IEEE Transactions on Information Theory, 62(3):1528–1537, 2016.

Frank Rosenblatt. The Perceptron: A probabilistic model for information storage and
organization in the brain. Psychological review, 65(6):386–408, 1958.

[RWY11] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Minimax rates of estimation
for high-dimensional linear regression over ℓq-balls. IEEE Transactions on Information
Theory, 57(10):6976–6994, 2011.

[Sch92]

[Ser99]

[SL17a]

[SL17b]

[SL18]

[Slo88]

[Slo92]

Robert E. Schapire. Design and analysis of eﬃcient learning algorithms. MIT Press,
Cambridge, MA, USA, 1992.

Rocco A. Servedio. Computational sample complexity and attribute-eﬃcient learning.
In Proceedings of the 31st Annual ACM Symposium on Theory of Computing, pages
701–710, 1999.

Jie Shen and Ping Li. On the iteration complexity of support recovery via hard threshold-
ing pursuit. In Proceedings of the 34th International Conference on Machine Learning,
pages 3115–3124, 2017.

Jie Shen and Ping Li. Partial hard thresholding: Towards a principled analysis of
support recovery. In Proceedings of the 31st Annual Conference on Neural Information
Processing Systems, pages 3127–3137, 2017.

Jie Shen and Ping Li. A tight bound of hard thresholding. Journal of Machine Learning
Research, 18(208):1–42, 2018.

Robert H. Sloan. Types of noise in data for concept learning. In Proceedings of the
First Annual Workshop on Computational Learning Theory, pages 91–96, 1988.

Robert H. Sloan. Corrigendum to types of noise in data for concept learning.
In
Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory,
page 450, 1992.

[SSBD14]

Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From
Theory to Algorithms. Cambridge University Press, 2014.

[Ste05]

[Tib96]

[TP14]

[TW10]

[Val84]

Daureen Steinberg. Computation of matrix norms with applications to robust optimiza-
tion. Research thesis, Technion-Israel University of Technology, 2005.

Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the
Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

Andreas M. Tillmann and Marc E. Pfetsch. The computational complexity of the
restricted isometry property, the nullspace property, and related concepts in compressed
sensing. IEEE Transactions on Information Theory, 60(2):1248–1259, 2014.

Joel A. Tropp and Stephen J. Wright. Computational methods for sparse solution of
linear inverse problems. Proceedings of the IEEE, 98(6):948–958, 2010.

Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–
1142, 1984.

18

[Val85]

Leslie G. Valiant. Learning disjunction of conjunctions.
International Joint Conference on Artiﬁcial Intelligence, pages 560–566, 1985.

In Proceedings of the 9th

[vdGL13]

Sara van de Geer and Johannes Lederer. The Bernstein-Orlicz norm and deviation
inequalities. Probability Theory and Related Fields, 157:225–250, 2013.

[VDVW96] Aad W Van Der Vaart and Jon A Wellner. Weak convergence and empirical processes.

Springer, 1996.

[Vem10]

[Wai09]

Santosh S. Vempala. A random-sampling-based algorithm for learning intersections of
halfspaces. Journal of the ACM, 57(6):32:1–32:14, 2010.

Martin J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity re-
covery using ℓ1-constrained quadratic programming (Lasso).
IEEE Transactions on
Information Theory, 55(5):2183–2202, 2009.

[XZS+17] Yichong Xu, Hongyang Zhang, Aarti Singh, Artur Dubrawski, and Kyle Miller. Noise-
tolerant interactive learning using pairwise comparisons.
In Proceedings of the 31st
Annual Conference on Neural Information Processing Systems, pages 2431–2440, 2017.

[YZ17]

[Zha02]

[Zha18]

Songbai Yan and Chicheng Zhang. Revisiting perceptron: Eﬃcient and label-optimal
learning of halfspaces. In Proceedings of the 31st Annual Conference on Neural Infor-
mation Processing Systems, pages 1056–1066, 2017.

Tong Zhang. Covering number bounds of certain regularized linear function classes.
Journal of Machine Learning Research, 2:527–550, 2002.

Chicheng Zhang. Eﬃcient active learning of sparse halfspaces. In Proceedings of the
31st Annual Conference On Learning Theory, pages 1856–1880, 2018.

[ZHT06]

Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis.
Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.

[ZSA20]

[ZYJ14]

Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Eﬃcient active learning of sparse
halfspaces with arbitrary bounded noise. CoRR, abs/2002.04840, 2020.

Lijun Zhang, Jinfeng Yi, and Rong Jin. Eﬃcient algorithms for robust one-bit compres-
sive sensing. In Proceedings of the 31st International Conference on Machine Learning,
pages 820–828, 2014.

19

A Detailed Choices of Reserved Constants and Additional Nota-

tions

Constants. The absolute constants c0, c1 and c2 are speciﬁed in Lemma 12, and c3 and c4 are
speciﬁed in Lemma 13. c5 and c6 were clariﬁed in Section 3.2. The deﬁnition of c7, c8, c9 can be
found in Lemma 14, Lemma 17, and Lemma 18 respectively. The absolute constant C1 acts as an
upper bound of all bk’s, and by our choice in Section 3.2, C1 = ¯c/16. The absolute constant C2 is
deﬁned in Lemma 16. Other absolute constants, such as C3, C4 are not quite crucial to our analysis
or algorithmic design. Therefore, we do not track their deﬁnitions. The subscript variants of K, e.g.
K1 and K2, are also absolute constants but their values may change from appearance to appearance.
We remark that the value of all these constants does not depend on the underlying distribution D
chosen by the adversary, but rather depends on the knowledge of

.

D

x :

Pruning. Consider Algorithm 1. For each phase k, we sample a working set ¯T and remove all
instances that have large ℓ∞-norm to obtain T (Step 6), which is equivalent to intersecting it with
the ℓ∞-ball B∞(0, νk) :=
. This is motivated by Lemma 18,
which states that with high probability, all clean instances in ¯T are in B∞(0, νk). Speciﬁcally, Denote
by ¯TC (respectively ¯TD) the set of clean (respectively dirty) instances in ¯T . Lemma 18 implies that
B∞(0, νk). Therefore, with high probability, all the instances in ¯TC
with probability 1
are kept in this step and only instances in ¯TD may be removed. Denote by TC = ¯TC ∩
B∞(0, νk) and
TD = ¯TD ∩
TD. We ﬁnally denote by
B∞(0, νk); we therefore also have the decomposition T = TC ∪
ˆTC the unrevealed labeled set that corresponds to ¯TC.

(cid:8)
48 , ¯TC ⊂

where νk = c9 log

¯T
d
48
|
|
bkδk

k∞ ≤

x
k

νk

−

(cid:9)

δk

Table 1: Summary of useful notations associated with the working set ¯T at each phase k.

η(D, w∗) conditioned on
x
wk−1 ·
|
η(D, w∗) draws from the distribution D

¯T
instance set obtained by calling EXx
¯TC
set of instances in ¯T that EXx
¯TD set of dirty instances in ¯T , i.e. ¯T
set of instances in ¯T that lie in B∞(0, νk)
T
set of instances in ¯TC that lie in B∞(0, νk)
TC
TD set of instances in ¯TD that lie in B∞(0, νk)
ˆTC
˜TC

unrevealed labeled set of ¯TC
unrevealed labeled set of TC

¯TC
\

bk

| ≤

Regularity condition on Du,b. We will frequently work with the conditional distribution Du,b
. We give the
obtained by conditioning D on the event that x is in the band
}
following regularity condition to ease our terminology.

Rd :

x
{

u
|

| ≤

∈

x

b

·

Deﬁnition 11. A conditional distribution Du,b is said to satisfy the regularity condition if one of
C1; 2) the vector u is the
the following holds: 1) the vector u
zero vector and b = C1.

Rd has unit ℓ2-norm and 0 < b

≤

∈

In particular, at each phase k of Algorithm 1, u is set to wk−1 and b is set to bk. For k = 1,
u = w0 is a zero vector, b = b1 = C1, satisfying the regularity condition. It is worth mentioning that
at phase 1 the conditional distribution Du,b boils down to D. For all k
2, u is a unit vector and
b
1, Dwk−1,bk satisfy the regularity
condition.

(0, C1] in view of our construction of bk. Therefore, for all k

≥

≥

∈

20

B Useful Properties of Isotropic Log-Concave Distributions

We record some useful properties of isotropic log-concave distributions.

Lemma 12. There are absolute constants c0, c1, c2 > 0, such that the following holds for all isotropic
log-concave distributions D

. Let fD be the density function. We have

∈ D

1. Orthogonal projections of D onto subspaces of Rd are isotropic log-concave;

2. If d = 1, then Prx∼D(a

3. If d = 1, then fD(x)

≥

x

b)

b
≤ |

≤
≤
c0 for all x

∈

a

;

|

1/9, 1/9];

−
[
−

4. For any two vectors u, v

Rd,

∈

Prx∼D

sign (u

c1 ·

x)

= sign (v

x)

·

≤

θ(u, v)

c2 ·

≤

·

Prx∼D

sign (u

x)

= sign (v

·

5. Prx∼D

(cid:0)
k2 ≥

x

k

t√d

exp(

−

≤

t + 1).

(cid:1)

(cid:0)

x)

;

·

(cid:1)

(cid:0)

(cid:1)

We remark that Parts 1, 2, 3, and 5 are due to [LV07], and Part 4 is from [Vem10, BL13].
The following lemma is implied by the proof of Theorem 21 of [BL13], which shows that if we
choose a proper band width b > 0, the error outside the band will be small. This observation is
crucial for controlling the error over the distribution D, and has been broadly recognized in the
literature [ABL17, Zha18].

Lemma 13 (Theorem 21 of [BL13]). There are absolute constants c3, c4 > 0 such that the following
. Let u and v be two unit vectors in Rd and
holds for all isotropic log-concave distributions D
assume that θ(u, v) = α < π/2. Then for any b

∈ D
4
c4 α, we have

≥

Prx∼D(sign (u

x)

= sign (v

x) and

·

x

v
|

·

| ≥

b)

≤

c3α exp

·

c4b
2α

.

(cid:19)

−

(cid:18)

Lemma 14 (Lemma 20 of [ABHZ16]). There is an absolute constant c7 > 0 such that the following
. Draw n i.i.d. instances from D to form a
holds for all isotropic log-concave distributions D
set S. Then

∈ D

k∞ ≥
Lemma 15. There is an absolute constant ¯C2 ≥
log-concave distributions D

(cid:18)

PrS∼Dn

max
x∈S k

x

∈ D

c7 log |

d

S
|
δ

δ.

≤

(cid:19)

and all Du,b that satisfy the regularity condition:

1 such that the following holds for all isotropic

sup
w∈B2(u,r)

Ex∼Du,b

x)2

(w

·

(cid:2)

≤

(cid:3)

¯C2(b2 + r2).

Proof. When u is a unit vector, Lemma 3.4 of [ABL17] shows that there exists a constant K1 such
that

sup
w∈B2(u,r)

Ex∼Du,b

x)2

(w

·

≤

K1(b2 + r2).

When u is a zero vector, Du,b reduces to D and the constraint w
Thus we have

B2(u, r) reads as

w

k

k2 ≤

r.

(cid:2)

(cid:3)

∈
r2 < b2 + r2.

=
The proof is complete by choosing ¯C2 = K1 + 1.

(w

(cid:3)

(cid:2)

·

Ex∼Du,b

x)2

w

k

k

2
2 ≤

21

6
6
6
h(cid:0)

d

λi

Xi=1

Lemma 16. There is an absolute constant C2 ≥
log-concave distributions D

and all Du,b that satisfy the regularity condition:

2 such that the following holds for all isotropic

∈ D

Ex∼Du,b

x⊤Hx

sup
H∈M

C2(b2 + r2),

≤

(cid:3)

(cid:2)
r2,

where

:=

H

Rd×d : H

M

∈

0,

H
k

k∗ ≤

(cid:23)

H
k

k1 ≤

ρ2

.

n
is a positive semideﬁnite matrix with trace norm at most r2, it has eigende-
Proof. Since H
∈ M
d
r2, and vi’s
i=1 λiviv⊤
composition H =
0 are the eigenvalues such that
are orthonormal vectors in Rd. Thus,
P

i , where λi ≥

i=1 λi ≤

P

o

d

x⊤Hx =

1
r2

d

Xi=1

λi(rvi ·

x)2

2
r2 ·

≤

d

λi

(rvi + u)

Xi=1

h(cid:0)

2 + (u

x

·

(cid:1)

·

x)2

.

i

Since x is drawn from Du,b, we have (u
of w = rv + u implies that

·

x)2

≤

b2. Moreover, applying Lemma 15 with the setting

sup
v∈B2(0,1)

Therefore,

Ex∼Du,b

(rv + u)

¯C2(b2 + r2).

x

·

2

i

(cid:1)

≤

Ex∼Du,b

x⊤Hx

sup
H∈M

2
r2 ·

≤

(cid:2)
The proof is complete by choosing C2 = 2( ¯C2 + 1).

(cid:3)

¯C2(b2 + r2) + b2
(cid:16)

2( ¯C2 + 1)(b2 + r2).

≤

(cid:17)

Lemma 17. Let c8 = min
and all Du,b satisfying the regularity condition,

2c0, 2c0

9C1 , 1

C1

(cid:9)

x

1. Prx∼D

·
2. Prx∼Du,b(E)

u
|
(cid:0)

b

| ≤

≥

(cid:1)

1
c8b Prx∼D(E) for any event E.

≤

(cid:8)
c8 ·

b;

. Then for all isotropic log-concave distributions D

∈ D

Proof. We ﬁrst consider the case that u is a unit vector.

For the lower bound, Part 3 of Lemma 12 shows that the density function of the random variable
x is lower bounded by c0 when

1/9. Thus

u

x

|

·

| ≤

u

·

Prx∼D

u

x

·

| ≤

b

≥

Prx∼D

x

u
|

·

| ≤

where in the last inequality we use the condition b

(cid:1)

(cid:0)

b, 1/9
min
}
{
(cid:1)
C1.

≥

≤

|
(cid:0)

For any event E, we always have

Prx∼Du,b(E)

≤

Prx∼D(E)
u

x

Prx∼D(
|

·

| ≤

1
c8b

b) ≤

Prx∼D(E).

2c0 min

b, 1/9
{

} ≥

2c0 min

1,

(cid:26)

1
9C1 (cid:27)

b

·

Now we consider the case that u is the zero vector and b = C1. Then Prx∼D
b in view of the choice c8. Thus Part 2 still follows. The proof is complete.

u

x

·

| ≤

b

= 1

≥

(cid:1)

|
(cid:0)

c8 ·

22

Lemma 18. There exists an absolute constant c9 > 0 such that the following holds for all isotropic
log-concave distributions D
and all Du,b that satisfy the regularity condition. Let S be a set of
i.i.d. instances drawn from Du,b. Then

∈ D

PrS∼Dn
u,b

max
x∈S k

x

k∞ ≥

(cid:18)

c9 log |

S
|
bδ

d

δ.

≤

(cid:19)

Proof. Using Lemma 14 we have

(cid:18)
Thus, using Part 2 of Lemma 17 gives

PrS∼Dn

max
x∈S k

x

k∞ ≥

c7 log |

S
|
δ

d

δ.

≤

(cid:19)

PrS∼Dn
u,b

(cid:18)

max
x∈S k

x

k∞ ≥

c7 log |

d

S
|
δ

≤

(cid:19)

δ
c8b

.

The proof is complete by changing δ to δ′ = δ

c8b .

C Orlicz Norm and Concentration Results using Adamczak’s Bound

The following notion of Orlicz norm [vdGL13, Dud14] is useful in handling random variables that
tα) for general α’s beyond α = 2 (subgaussian) and α = 1 (subexpo-
have tails of the form exp(
nential).

−

Deﬁnition 19 (Orlicz norm). For any z
variable Z

R and α > 0, deﬁne

R, let ψα : z

exp(zα)
∈
, the Orlicz norm of Z with respect to ψα, as:

7→

−

1. Furthermore, for a random

∈

Z
k

kψα

We collect some basic facts about Orlicz norms in the following lemma; they can be found in

Z
k

kψα

= inf

t > 0 : EZ

ψα

n

Z
|

/t
|

≤

1

.

(cid:2)

(cid:0)

(cid:1)(cid:3)

o

Section 1.3 of [VDVW96].

Lemma 20. Let Z, Z1, Z2 be real-valued random variables. Consider the Orlicz norm with respect
to ψα. We have the following:

1.

k·kψα

is a norm. For any a

R,

∈

aZ

k

kψα

=

a

|

Z

| · k

;

Z1 + Z2kψα ≤ k
k

Z1kψα

+

Z2kψα

k

.

2.

Z

k

Z

kp ≤ k

kψp ≤

p!

Z
k

3. For any p, α > 0,

Z
k

kψ1
α
ψp
k

=

where

Z

k

Z α
k

kψp/α

kp :=
.

kψα
1/p

.

Z
|

p
|

E

(cid:16)

(cid:2)

(cid:3)(cid:17)

4. If Pr

5. If

Z
k

Z

| ≥

|
(cid:0)
kψα ≤

t

≤

K1 exp (

−

K2tα) for any t

0, then

Z
k

(cid:1)

K, then for all t

0, Pr

≥

Z

| ≥

2 exp

≤

−

≥

t

2(ln K1+1)
K2

1/α

.

(cid:17)

(cid:16)
.

kψα ≤
( t
K )α

(cid:1)
The following auxiliary results, tailored to the localized sampling scheme in Algorithm 1, will

(cid:0)

(cid:1)

|
(cid:0)

also be useful in our analysis.

23

Lemma 21. There exists an absolute constant C3 > 0 such that the following holds for all isotropic
log-concave distributions D
x1, . . . , xn}
{
be a set of n instances drawn from Du,b. Then

and all Du,b that satisfy the regularity condition. Let S =

∈ D

max
x∈S k

x

k∞

ψ1 ≤

C3 log

nd
b

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
ES∼Dn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
x

max
x∈S k

C3 log

nd
b

.

Consequently,

k∞
Proof. Let Z be isotropic log-concave random variable in R. Part 5 of Lemma 12 shows that for all
t > 0,

≤

u,b

i

h

Fix i
and ﬁx j
1, . . . , n
Lemma 12 suggests that x(j)

∈ {

}

i

|

Z
Pr(
|

> t)
exp(
. Denote by x(j)
1, . . . , d
}

−

≤

i

t + 1).

∈ {
is isotropic log-concave. Thus, by Part 2 of Lemma 17,

the j-th coordinate of xi. Part 1 of

Prx∼Du,b

x(j)
i

> t

Prx∼D

x(j)
i

> t

1
c8b

≤

1
c8b

≤

exp(

t + 1).

−

Taking the union bound over i

(cid:16) (cid:12)
(cid:12)

(cid:17)

1, . . . , n

(cid:12)
(cid:12)
∈ {

and j

}

(cid:16) (cid:12)
(cid:12)
∈ {

(cid:17)
(cid:12)
(cid:12)
, we have for all t > 0
1, . . . , d
}

k∞ > t
Now Part 4 of Lemma 20 immediately implies that

max
x∈S k

Prx∼Du,b

(cid:18)

x

nd
c8b

≤

(cid:19)

exp(

−

t + 1).

for some constant C3 > 0. The second inequality of the lemma is an immediate result by combining
the above and Part 2 of Lemma 20.

max
x∈S k

x

k∞

C3 log

nd
b

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ψ1 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

C.1 Adamczak’s bound

In this section, we establish the key concentration results that will be used to analyze the per-
formance of soft outlier removal and random sampling in Algorithm 1. Since we are considering
the isotropic log-concave distribution, any unlabeled instance x is unbounded. This prevents us
from using standard concentration bounds, e.g.
[KST08]. We henceforth appeal to the following
generalization of Talagrand’s inequality, due to [Ada08].

Lemma 22 (Adamczak’s bound). For any α
the following holds. Given any function class
f (x)
i.i.d. instances from D,
(cid:12)
(cid:12)

F
F (x), we have with probability at least 1

≤

∈

(cid:12)
(cid:12)

−

(0, 1], there exists a constant Λα > 0, such that
,
, and a function F such that for any f
∈ F
of

δ over the draw of a set S =

x1, . . . , xn}
{

n

1
n

sup
f ∈F(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (xi)

−

Xi=1

Ex∼D

f (x)

(cid:2)

supf ∈F

+

s

ES∼Dn

Λα 

(f (x))2

(cid:20)

sup
f ∈F(cid:12)
(cid:12)
(cid:12)
(cid:12)
+

ln 1
δ

≤

(cid:12)
(cid:12)
(cid:3)
(cid:12)
(cid:12)
Ex∼D
n
(cid:2)

(cid:3)

24

1
n

n

f (xi)

−

Ex∼D

f (x)

Xi=1
(ln 1
δ )1/α
n

(cid:2)

F (xi)

ψα

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.






max
1≤i≤n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

We ﬁrst establish the following result that upper bounds the expected value of Rademacher

complexity of linear classes by the Orlicz norm of the random instances.

Lemma 23. There exists an absolute constant C5 > 0 such that the following holds for all isotropic
x1, . . . , xn}
log-concave distributions D
{
be a set of n i.i.d. unlabeled instances drawn from Du,b. Denote W = B2(u, r)
B1(u, ρ). Let a
be drawn from a distribution supported on a bounded
sequence of random variables Z =
z1, . . . , zn}
{
interval [
, where the σi’s are i.i.d. Rademacher random
σ1, . . . , σn}
{
variables independent of S and Z. We have:

and all Du,b that satisfy the regularity condition. Let S =

λ, λ] for some λ > 0. Let σ =

∈ D

−

∩

ES,Z,σ

(cid:20)

n

sup
w∈W (cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

σizi(w

·

xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λb√n + C5ρλ

n log d

log

·

nd
b

.

p

≤

(cid:21)

W can be expressed as w = u + v for some

Proof. Let V = B2(0, r)
v

∈
V . First, conditioned on S and Z, we have that

B1(0, ρ) so that any w

∩

∈

Eσ

Thus,

n

Xi=1

(cid:20)

sup
v∈V (cid:12)
(cid:12)
(cid:12)
(cid:12)

σizi(v

ES,Z,σ

≤

(cid:21)

·

xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

(cid:20)

sup
v∈V (cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

ρ

2n log(2d)

max
1≤i≤n k

zixik∞ ≤

·

ρλ

2n log(2d)

max
1≤i≤n k

xik∞ .

·

p

σizi(v

·

xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:21)

≤

p

ES

ρλ

2n log(2d)

·

p
C5ρλ

n log d

log

·

(cid:20)
nd
b

max
1≤i≤n k

xik∞

(cid:21)

,

(C.1)

where the second inequality follows from Lemma 21.

p

On the other side, using the fact that for any random variable A, E[A]

E[A2]

1/2, we have

≤

n

ES,Z,σ

σizi(u

(cid:20) (cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

·

xi)
(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

ES,Z,σ

≤ v
u
u
t

n

Xi=1

"(cid:18)
n

2

(cid:0)

(cid:1)

σizi(u

·

xi)

(cid:19)

#

"
Xi=1
where in the equality we use the observation that ES,Z,σ
= j,
and in the last inequality we used the condition that xi is drawn from Du,b. Combining the above
with (C.1) we obtain the desired result.

= 0 when i

σiσjzizj(u

xi)(u

xj)

(cid:2)

(cid:3)

·

·

=

ES,Z

v
u
u
t

z2
i (u

xi)2

·

# ≤

√nb2λ2,

C.2 Uniform concentration of hinge loss

Proposition 24. There exists an absolute constant C6 > 0 such that the following holds for all
and all Du,b that satisfy the regularity condition. Let
isotropic log-concave distributions D
x1, . . . , xn}
be a set of n i.i.d. unlabeled instances drawn from Du,b which satisﬁes the regularity
S =
{
condition. Let yx = sign (w∗
B1(u, ρ) and let
G(w) = 1
n

Du,b. Denote W = B2(u, r)
. Then with probability 1

n
i=1 ℓτ (w; xi, yxi)

∼
ℓτ (w; x, yx)

x) for any x
·
Ex∼Du,b
−

∩
δ,
−

∈ D

P

sup
w∈W

G(w)

C6

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:2)
b + ρ√log d log nd
b
τ √n

+

(cid:3)
b + r
τ √n r

log

1
δ

+

b + ρ log nd
b
τ n

log

1
δ !

.

25

6
 
t2 s log2 d
b ·

log d
δ

(cid:16)

(cid:17)

In particular, suppose b = O(r), ρ = O(√sr) and τ = Ω(r). Then we have: for any t > 0, a sample
size n = ˜O
t.

suﬃces to guarantee that with probability 1

G(w)

1

δ, supw∈W

−

≤

Proof. We will use Lemma 22 with function class
=
(x, y)
F
τ + ρ
norm with respect to ψ1. We deﬁne F (x, y) = 1 + b
x
τ k
(cid:8)
u)
−
τ

x
·
|
τ ≤

ℓτ (w; x, y)

1 + |

1 +

(w

≤

+

·
τ

w

u

x

·

7→

ℓτ (w; x, y) : w

(cid:12)
and the Orlicz
(cid:12)
W ,
k∞. It can be seen that for every w
x

W

∈

∈

(cid:9)

(cid:12)
(cid:12)

1 +

+

≤

b
τ

ρ
τ k

x

k∞ = F (x, y).

That is, for every f in

(cid:12)
(cid:12)

(cid:12)
(cid:12)

,

f (x, y)

F

≤

F (x, y).

Step 1. We upper bound

max1≤i≤n F (xi, yxi)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:13)
(cid:13)
max
1≤i≤n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

. Since

ψ1

k·kψ1

is a norm, we have

F (xi, yxi)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ψ1 ≤

(cid:13)
(cid:13)
(cid:13)
= 1 +
(cid:13)

+

max
1≤i≤n k

xik∞

(cid:13)
(cid:13)
1 +

b
τ

ρ
τ ·

(cid:13)
(cid:13)
(cid:13)
max
(cid:13)
1≤i≤n k

log

nd
b

,

(cid:13)
(cid:13)
(cid:13)
+
(cid:13)

ψ1
ρ
τ ·

(cid:13)
(cid:13)
C3ρ
(cid:13)
(cid:13)
τ

b
τ

b
τ

1 +

≤

+

ψ1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

xik∞

ψ1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

where we applied Lemma 21 in the last inequality.

Step 2. Next, we upper bound supw∈W

Ex∼Du,b

(ℓτ (w; x, yx))2

. For all w in W , we have

sup
w∈W

Ex∼Du,b

(ℓτ (w; x, yx))2

h

2

sup
w∈W

·

≤

i

(cid:2)
Ex∼Du,b

(w

(cid:3)
x)2

·
τ 2

1 +

(cid:20)

≤

(cid:21)

2 + 2 ¯C2 ·

r2 + b2
τ 2

(C.2)

(C.3)

where the last inequality uses Lemma 15.
Step 3. Finally, we upper bound ES∼Dn
G(w)
an i.i.d. draw from the Rademacher distribution. We have
(cid:12)
(cid:12)
n

supw∈W

(cid:12)
(cid:12)

u,b

. Let σ =

σ1, . . . , σn}
{

where each σi is

(cid:3)

(cid:2)
2
n

ES

sup
w∈W

(cid:20)

G(w)

(cid:12)
(cid:12)

(cid:21)

(cid:12)
(cid:12)

≤

≤

≤

ES,σ

(cid:20)

2
τ n

ES,σ

2b
τ √n

+

sup
w∈W (cid:12)
Xi=1
(cid:12)
n
(cid:12)
(cid:12)
sup
w∈W (cid:12)
(cid:20)
Xi=1
(cid:12)
(cid:12)
2C5ρ
(cid:12)
τ
· r

σiℓτ

w; xi, yxi

(cid:0)
σiyxi(w

·

log d

n ·

log

(cid:21)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

(cid:21)

xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
.

nd
b

(C.4)

In the above, the ﬁrst inequality used standard symmetrization arguments; see, for example, Lemma 26.2
of [SSBD14]. In the second inequality, we used the contraction property of Rademacher complexity
and the fact that ℓτ (w; x, y) can be seen as a 1
applied
on input a = yw
1.

0, 1
x. In the last inequality, we applied Lemma 23 with the fact that

τ -Lipschitz function φ(a) = max

a
τ

−
yxi

(cid:9)
≤

(cid:8)

·

Putting together. The ﬁrst inequality of the proposition follows from combining (C.2), (C.3), and
(C.4), and using Lemma 22 with
and ψ1. Under our choice of (b, r, ρ, τ ), with some calculation
we obtain the bound of n.

F

(cid:12)
(cid:12)

(cid:12)
(cid:12)

26

C.3 Uniform concentration of relaxed sparse PCA

Proposition 25. There exists an absolute constant C7 > 0 such that the following holds for all
isotropic log-concave distributions D
and all Du,b that satisfy the regularity condition. Let
x1, . . . , xn}
S =
be a set of n i.i.d. unlabeled instances drawn from Du,b. Denote G(H) =
{
n
1
i=1 x⊤
i Hxi −
n

. Then with probability 1

Ex∼Du,b

x⊤Hx

∈ D

δ,

−

P

(cid:2)
G(H)

(cid:3)
C7ρ2 log2 nd
b

≤

sup
H∈M

log d
n

+

r

(cid:18)r

log(1/δ)
n

+

log2 1
δ
n

.

(cid:19)

In particular, suppose ρ = O(√sr) and r = O(b). Then we have: for any t > 0, a sample size

(cid:12)
(cid:12)

(cid:12)
(cid:12)

n = ˜O

1

t2 s2b4 log4 d

b ·

log d + log2 1
δ

(cid:18)

(cid:19)!

t.

≤
ρ2

suﬃces to guarantee that with probability 1

δ, supH∈M

G(H)

−

. For any matrix H, we denote
Proof. Recall that
k ≤
by Hij the (i, j)-th entry of the matrix H. For any vector x, we denote by x(i) the i-th coordinate
of x.

M

H

H

(cid:23)

=

0,

∈

n

o

k

Rd×d : H

r2,

(cid:12)
H
(cid:12)
k

(cid:12)
(cid:12)
k1 ≤

We will use Lemma 22 with function class

=

x

and the Orlicz norm

F

7→

∈ M

x⊤Hx : H

with respect to ψ0.5. Consider the function f (x) := x⊤Hx parameterized by H
wish to ﬁnd a function F (x) that upper bounds

o
. It is easy to see that

n
f (x)

. First, we

∈ M

(C.5)

(cid:12)
(cid:12)
≤ k

x

(cid:12)
2
(cid:12)
∞
k

(cid:12)
(cid:12)
(cid:12)

Hij

Xi,j

(cid:12)
(cid:12)

(cid:12)
(cid:12)

ρ2

x

2
∞ .
k

k

≤

x⊤Hx

=

Hijx(i)x(j)

Xi,j
(cid:12)
(cid:12)
(cid:12)
Thus it suﬃces to choose F (x) = ρ2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

x

2
∞.
k

k

Step 1. We ﬁrst bound

max1≤i≤n F (xi)

(cid:13)
(cid:13)
By Part 3 of Lemma 20,
(cid:13)

p

max1≤i≤n F (x)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
F (x)

max
1≤i≤n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ex∼Du,b
is equivalent to taking that over H

Step 2. Next we upper bound supf ∈F
over f

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∈ F

In view of Part 2 of Lemma 20, we have

(f (x))2

≤

=

ρ

max1≤i≤n k

xik∞

·

(cid:13)
equals
(cid:13)

(cid:13)
max1≤i≤n F (x)
(cid:13)

ψ1 ≤
2

ψ1

ψ1

(cid:13)
(cid:13)
(cid:13)
ψ0.5

p

(cid:13)
(cid:13)
(cid:13)
C3ρ log

(cid:18)

2

.

nd
b

(cid:19)

(cid:13)
(cid:13)
(cid:13)

ψ0.5 ≤

C3ρ log nd

b by Lemma 21.

. Thus

(C.6)

(f (x))2

where we remark that taking the superum

(cid:2)
∈ M
(F (x))2

. Since
(cid:3)
ρ4

≤

(cid:12)
4
x
∞ .
(cid:12)
k
k

(cid:12)
(cid:12)

f (x)

≤

F (x), we have

Ex∼Du,b

(cid:18)

h

x

4
∞
k

k

i(cid:19)

1/4

24

≤

x

k

k∞

ψ1 ≤

24C3 log

d
b

,

where the last inequality follows from Lemma 21. Hence,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

sup
f ∈F

Ex∼Du,b

(f (x))2

h

27

K1ρ4 log4 d
b

≤

i

(C.7)

(C.8)

 
for some absolute constant K1 > 0.

Step 3. Finally, we upper bound ES∼Dn
σ1, . . . , σn}
{
symmetrization arguments (see e.g. Lemma 26.2 of [SSBD14]), we have

1
. Let σ =
n
where σi’s are independent draw from the Rademacher distribution. By standard

n
i=1 f (xi)

Ex∼Du,b

supf ∈F

f (x)

(cid:3)(cid:12)
i
(cid:12)
(cid:12)

P

−

(cid:12)
(cid:12)
(cid:12)

h

(cid:2)

ES

sup
f ∈F

G(v, H)

2
n

≤

ES,σ

n

sup
f ∈F(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

σif (xi)
(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

=

ES,σ

2
n

n

sup
H∈M(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

(cid:21)

(cid:20)

(cid:20)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)
We ﬁrst condition on S and consider the expectation over σ. For a matrix H, we use vec(H) to
denote the vector obtained by concatenating all of the columns of H; likewise for xix⊤
i . It is crucial
ρ2. It follows
to observe that with this notation, for any H
that

, we have

vec(H)

k1 ≤

H
k

∈ M

1 =

(cid:20)

σix⊤

i Hxi

.

(C.9)

n

Xi=1

Eσ

sup
" (cid:12)
H∈M
(cid:12)
(cid:12)
(cid:12)

σix⊤

i Hxi

Eσ

"

H:

sup
vec(H)
k

n ln(2d2)

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)

n

(cid:13)
(cid:13)

σi

vec(H), vec(xix⊤
i )

ρ2

≤
= ρ2

p

n ln(2d2)

·

k1≤ρ2(cid:12)
Xi=1
(cid:12)
(cid:12)
max
(cid:12)
1≤i≤n

D
vec(xix⊤
i )
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xik
max
(cid:13)
(cid:13)
1≤i≤n k

2
∞ .

·

∞ ·

#
E(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the second inequality is from Lemma 39, and the equality is from the observation that

p

vec(xix⊤
i )

k∞ =

k

2
∞. Therefore,

xik
k

ES,σ

n

Xi=1

sup
" (cid:12)
H∈M
(cid:12)
(cid:12)
(cid:12)

σix⊤

i Hxi

ρ2

n ln(2d2)

p

p

ρ2

ρ2

2n ln(2d)

2n ln(2d)

·

·

·

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

ES

(cid:20)

max
1≤i≤n k

xik

2
∞

xik∞

2

max
1≤i≤n k
(cid:13)
(cid:13)
3 log2 nd
(cid:13)
C 2
(cid:13)
b

,

(cid:21)
2

ψ1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

where the second inequality follows from Part 2 of Lemma 20, and the last inequality follows from
Lemma 21. In summary,

p

ES,σ

for some constant K2 > 0.

n

"

sup
f ∈F(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)

σix⊤

i Hxi

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K2√n ln d

ρ2 log2 nd
b

·

Combining (C.9) and (C.10), we have

ES

sup
f ∈F

(cid:20)

G(H)

(cid:12)
(cid:12)

(cid:21)

(cid:12)
(cid:12)

K3√log d
√n

ρ2 log2 nd
b

.

·

≤

(C.10)

(C.11)

Putting together. Combining (C.6), (C.8), (C.11), and using Lemma 22 gives the ﬁrst inequality
of the proposition. Under our setting of (b, r, ρ), by some calculation we obtain the bound of n. The
proof is complete.

28

D Performance Guarantee of Algorithm 1

In this section, we leverage all the tools from previous sections to establish the performance guarantee
of Algorithm 1. Our main theorem, Theorem 4, follows from the analysis of each step of the
algorithm, as we describe below.

D.1 Analysis of sample complexity

Recall that we refer to the number of calls to EXx
In order to obtain nk instances residing the band Xk :=
EXx

η(D, w∗) suﬃcient times.

η(D, w∗) as the sample complexity of Algorithm 1.
, we have to call
x

x :
{

wk−1 ·
|

bk}

| ≤

Lemma 26 (Restatement of Lemma 6). Consider phase k of Algorithm 1 for any k
that Assumption 1 and 2 are satisﬁed. Further assume η < 1
calls to the instance generation oracle EXx

1. Suppose
2 . By making a number of Nk =
η(D, w∗), we will obtain nk instances

≥

O

1
bk

nk + log 1
δk
that fall into Xk with probability 1

(cid:16)

(cid:1)(cid:17)

(cid:0)

Proof. By Lemma 17

This implies that

δk
4 .

−

Prx∼D(x

Xk)

∈

≥

c8bk.

Prx∼EXx

= Prx∼EXx

c8bk(1

≥

−

η (D,w∗)(x
η (D,w∗)(x
η).

Xk and x is clean)
Xk |

x is clean)

·

Prx∼EXx

η (D,w∗)(x is clean)

∈

∈

We want to ensure that by drawing Nk instances from EXx
η(D, w∗), with probability at least
δk
4 , nk out of them fall into the band Xk. We apply the second inequality of Lemma 38 by

1
letting Zi = 1

−

{xi∈Xk and xi is clean} and α = 1/2, and obtain

c8bk(1
2

≤

η)

−

Nk

exp

−

(cid:18)

≤

(cid:19)

c8bk(1

η)Nk

−
8

,

(cid:19)

Pr

¯TC

(cid:18)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

where the probability is taken over the event that we make a number of Nk calls to EXx
, we are guaranteed that at least nk samples from EXx
Thus, when Nk ≥
δk
4 . The lemma follows by observing η < 1
(cid:17)
2 .
fall into the band Xk with probability 1

nk + ln 4
δk
(cid:16)

8
c8bk(1−η)

η(D, w∗).
η(D, w∗)

−

D.2 Analysis of pruning and the structure of ¯T
With the instance set ¯T on hand, we estimate the empirical noise rate after applying pruning (Step 6)
in Algorithm 1. Recall that nk =

, i.e. the number of unlabeled instances before pruning.

¯T

Lemma 27. Suppose that Assumption 1 and Assumption 2 are satisﬁed. Further assume η < 1
Du,b satisﬁes the regularity condition, we have

(cid:12)
(cid:12)

(cid:12)
(cid:12)

2 . If

Prx∼EXx

η (D,w∗)

x is dirty

x

|

∈

Xu,b

≤

2η
c8b

where c8 was deﬁned in Lemma 17 and Xu,b :=

x

(cid:0)

n
29

Rd :

∈

(cid:1)

x

u
|

·

| ≤

b

.

o

Proof. For an instance x, we use tagx = 1 to denote that x is drawn from D, and use tagx =
denote that x is adversarially generated.

−

1 to

We ﬁrst calculate the probability that an instance returned by EXx

η(D, w∗) falls into the band

Xu,b as follows:

Prx∼EXx

η (D,w∗)

x

Xu,b

= Prx∼EXx

η (D,w∗)

∈

∈

∈

∈

·

x
(cid:0)

x
(cid:0)

η (D,w∗)

η (D,w∗)
x

x
(cid:0)
Xu,b
(cid:0)

∈

(cid:0)
−

η)

(cid:1)

Prx∼EXx

≥
= Prx∼EXx

= Prx∼D

ζ

≥

≥

(1

c8b
1
2

·
c8b,

(cid:1)

Xu,b and tagx = 1
Xu,b and tagx = 1
(cid:1)
Xu,b |
Prx∼EXx
(cid:1)
Prx∼EXx

tagx = 1
·
η (D,w∗) (tagx = 1)
(cid:1)

+ Prx∼EXx

η (D,w∗)

x

∈

(cid:0)

Xu,b and tagx =

1

−

(cid:1)

η(D,w∗) (tagx = 1)

where in the inequality ζ we applied Part 1 of Lemma 17. It is thus easy to see that

Prx∼EXx

η (D,w∗)

tagx =

x

1

−

|

∈

Xu,b

≤

(cid:0)
which is the desired result.

(cid:1)

Prx∼EXx
Prx∼EXx

η (D,w∗) (tagx =
η (D,w∗)

x

−
Xu,b

1)

∈

2η
c8b

,

≤

(cid:0)

(cid:1)

Lemma 28. Suppose that Assumptions 1 and 2 are satisﬁed. Further assume η
, then with probability 1
1
k0, if nk ≥
hold simultaneously:

c5ǫ. For any
24 over the draw of ¯T , the following results
δk

ln 48
δk

6
ξk

≤

−

≤

≤

k

1. TC = ¯TC and hence ˜TC = ˆTC, i.e. all clean instances in ¯T are intact after pruning;

2.

3.

|TD|
|T | ≤
TC| ≥

|

ξk, i.e. the empirical noise rate after pruning is upper bounded by ξk;

(1

−

ξk)nk.

In particular, with the hyper-parameter setting in Section 3.2,

TC = ¯TC

Proof. Let us write events E1 :=
the two events over the draw of ¯T .
(cid:8)

(cid:8)(cid:12)
(cid:12)
Recall that Lemma 18 implies that with probability 1
¯T
d
48
|
|
bkδk

≤
48 , all instances in ¯TC are in the ℓ∞-ball
δk
δk
48 .
−
x :
We next calculate the noise rate within the band Xk :=
{

, which implies Pr(E1)

by Lemma 27:

(cid:12)
(cid:12)
−
1

. We bound the probability of

B∞(0, νk) for νk = c9 log

, E2 :=

bk}

≥

(cid:9)

(cid:9)

x

|

¯TD

1
2 nk.

TC| ≥
|
ξknk

Prx∼EXx

η (D,w∗)(x is dirty

x

|

∈

Xk)

≤

2η
c8bk

=

c8¯c

2η
2−k−3 ≤

·

wk−1 ·
π
c8¯cc1 ·

| ≤
η
ǫ ≤

πc5
c8¯cc1 ≤

ξk
2

,

where the equality applies our setting on bk, the second inequality uses the condition k
the setting k0 = log
the ﬁrst inequality of Lemma 38 by specifying Zi = 1
(cid:1)

k0 and
, and the last inequality is guaranteed by our choice of c5. Now we apply

{xi is dirty}, α = 1 therein, which gives

π
16c1ǫ

≤

(cid:0)

exp

ξknk
6

,

(cid:19)

−

(cid:18)

Pr

¯TD

ξknk

≥

(cid:0)(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

(cid:1)

30

where the probability is taken over the draw of ¯T . This implies Pr(E2)
nk ≥

ln 48
δk

6
ξk

.

1

−

≥

δk
48 provided that

By union bound, we have Pr(E1 ∩

E2, the second
1
−
¯TD
and third parts of the lemma follow. To see this, we note that it trivially holds that |TD|
|nk
|
since only dirty instances have chance to be removed. This proves the second part. Also, it is easy
¯T
to see that

24 . We show that on the event E1 ∩

, which is exactly the third part.

|T | ≤

E2)

(1

¯T

≥

=

=

¯TD

¯TC

ξk)

δk

TC|
|

−

≥

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
D.3 Analysis of Algorithm 2

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Lemma 29 (Restatement of Lemma 3). Suppose that Assumption 1 and 2 are satisﬁed, and
c5ǫ. There exists a constant C2 > 2 such that the following holds. Consider phase
that η
=
k of Algorithm 1 for any 1
˜O
log d + log2 1
δk

If
δk
24 over the draw of TC, we have

Mk the constraint set of (3.2).
−

k
, then with probability 1

k0. Denote by

TC|
|

bk ·

≤

≤

≤

s2 log4 d
(cid:16)
(cid:0)
1. supH∈Mk

1
|TC|

(cid:1)(cid:17)
x∈TC x⊤Hx

2. supw∈Wk

1
|TC|

P

x∈TC(w

x)2

·

2C2(b2

k + r2

k);

5C2

k + r2
b2
k

.

≤

≤

(cid:1)
Proof. The ﬁrst part is an immediate result by combining Proposition 25 and Lemma 16, and
recognizing our setting of bk and rk.
To see the second part, for any w

Wk, we can upper bound (w

x)2 as follows:

P

(cid:0)

where v = w
wk−1 ∈
indicates that for any w

−

∈
2(wk−1 ·

x)2 + 2(v

x)2

·

≤

·
k + 2x⊤(vv⊤)x,

2b2

B1(0, ρk). Hence it is easy to see that vv⊤ lies in

(w

x)2

≤

·
B2(0, rk)

∩

Wk, there exists an H

∈

x)2

(w

·

2

≤

∈ Mk such that
k + x⊤Hx
b2

.

(cid:2)

(cid:3)

1

Mk. This

(D.1)

Thus,

1

sup
w∈Wk

(w

x)2

2b2

k + 2 sup
H∈Mk

·

|

≤

TC| Xx∈TC
where the last inequality follows from the fact C2 ≥
Proposition 30 (Formal statement of Proposition 7). Consider phase k of Algorithm 1 for any
1
c5ǫ. With probability
≤
[0, 1] with the following
1
properties:

k
8 (over the draw of ¯T ), Algorithm 2 will output a function q : T
δk

k0. Suppose that Assumption 1 and 2 are satisﬁed, and that η

TC| Xx∈TC

≤
−

|
2.

→

≤

≤

x⊤Hx

5C2(b2

k + r2

k),

1. for all x

T, q(x)

2.

1
|T |

∈
x∈T q(x)

[0, 1];

∈

3. for all w

P

∈

1

−

≥
Wk, 1
|T |

ξk;

x∈T q(x)(w

x)2

·

≤

5C2

k + r2
b2
k

.

Furthermore, such function q can be found in polynomial time.

P

(cid:1)

(cid:0)

31

Proof. Our choice on nk satisﬁes the condition nk ≥
(see Section 3.2 for our parameter setting). Thus by Lemma 28, with probability 1
(1

ξk)nk. We henceforth condition on this happening.
On the other side, Lemma 3 and Proposition 25 together implies that with probability 1

since ξk is lower bounded by a constant
TC| ≥
|
δk
24 ,

δk
24 ,

−

−

6
ξk

ln 48
δk

−

for all H

∈ Mk, we have

provided that

1

TC| Xx∈TC

|

x⊤Hx

≤

2C2(b2

k + r2
k)

TC|
Note that (D.3) is satisﬁed in view of the aforementioned event
setting of nk and ξk. By union bound, the events (D.2) and
δk
with probability at least 1
8 .

(cid:16)

(cid:16)

|

(cid:17)(cid:17)
TC| ≥
|
(1
TC| ≥
−
|

s2 log4 d
bk ·

log d + log2 1
δk

= ˜O

−

(D.2)

(D.3)

.

(1
ξk)

−
T
|

ξk)nk along with the
hold simultaneously

|

Now we show that these two events together implies the existence of a feasible function q(x) to
TD and q(x) = 1 for all

Algorithm 2. Consider a particular function q(x) with q(x) = 0 for all x
x

TC. We immediately have

∈

∈

x⊤Hx

≤

2C2(b2

k + r2

k),

(D.4)

In addition, for all H

1
T
|

| Xx∈T

∈ Mk,
q(x)x⊤Hx =

1
T
|

1
T

|

TC|
q(x) = |
T
|
|

1

−

≥

ξk.

| Xx∈T

x⊤Hx

≤

|

| Xx∈TC

1

TC| Xx∈TC
TC|

where the ﬁrst inequality follows from the fact
and the second inequality follows from
T
|
(D.2). Namely, such function q(x) satisﬁes all the constraints in Algorithm 2. Finally, combining
(D.1) and (D.4) gives Part 3.

| ≥ |

It remains to show that for a given candidate function q, a separation oracle for Algorithm 2
can be constructed in polynomial time. First, it is straightforward to check whether the ﬁrst two
are violated. If not, we just need to further
ξ)
constraints q(x)
k + r2
k). To this end, we
check if there exists an H
appeal to solving the following program:

T
|
x∈T q(x)x⊤Hx > 2C2(b2

x∈T q(x)
(1
≥
∈ Mk such that
P

[0, 1] and

−
1
|T |

∈

|

P

max
H∈Mk

1
T
|

| Xx∈T

q(x)x⊤Hx.

This is a semideﬁnite program that can be solved in polynomial time [BV04].
k + r2
objective value is greater than 2C2(b2
would have found a desired function.

If the maximum
k), then we conclude that q is not feasible; otherwise we

The analysis of the following proposition closely follows [ABL17] with a reﬁned treatment. Let
x∈T p(x)ℓτk (w; x, yx) where yx is the unrevealed label of x that the adversary has

ℓτk (w; p) :=
committed to.

P

Proposition 31 (Formal statement of Proposition 8). Consider phase k of Algorithm 1. Suppose
c5ǫ. Set Nk and ξk as in Section 3.2.
that Assumption 1 and 2 are satisﬁed. Assume that η

≤

32

Denote zk :=
Wk
w

∈

q

k + r2
b2

k = √¯c2 + 1

·

2−k−3. With probability 1

ℓτk (w; ˜TC)

ℓτk (w; p)

≤

≤

ℓτk (w; p) + 2ξk

1 +

ℓτk (w; ˜TC) + 2ξk +

(cid:18)

10C2 ·
p
20C2ξk ·

zk
τk (cid:19)
zk
.
τk

−

+

In particular, with our hyper-parameter setting,

p

4 over the draw of ¯T , for all
δk

10C2ξk ·

zk
τk

,

p

Proof. The choice of nk guarantees that Lemma 28 and Proposition 30 hold simultaneously with
probability 1

δk
4 . We thus have for all w

Wk

−

∈

ℓτk (w; ˜TC)
(cid:12)
(cid:12)
(cid:12)

−

ℓτk (w; p)
(cid:12)
(cid:12)
(cid:12)

κ.

≤

1
T
|

q(x)(w

| Xx∈T
1
TC| Xx∈TC

|

(w

5C2z2
k,

5C2z2
k,

ξk.

x)2

x)2

·

·

TD|
|
T
|
|

≤

≤

≤

(D.5)

(D.6)

(D.7)

In the above expression, (D.5) and (D.6) follow from Part 3 and Part 2 of Lemma 29 respectively,
(D.7) follows from Lemma 28. It follows from Eq. (D.7) and ξk ≤
1

1/2 that

T
|
|
TD|
| − |
In the following, we condition on the event that all these inequalities are satisﬁed.
Step 1. First we upper bound ℓτk (w; ˜TC) by ℓτk (w; p).

T
|
|
TC|
|

1
TD|

ξk ≤

T
|

− |

=

≤

−

=

2.

T

/

1

1

|

|

(D.8)

TC| ·
|

ℓτk (w; ˜TC) =

=

ζ1

≤

ζ2

≤

ζ3

≤

ζ4

≤

ζ5

≤

ℓ(w; x, yx)

Xx∈TC

Xx∈T h

q(x)ℓ(w; x, yx) +

1

(cid:0)

q(x)ℓ(w; x, yx) +

Xx∈TC

(1

−

q(x)ℓ(w; x, yx) +

(1

Xx∈TC
T
q(x)ℓ(w; x, yx) + ξk |

+

|

q(x)ℓ(w; x, yx) + ξk |

T

|

+

q(x)ℓ(w; x, yx) + ξk |

T

|

+

Xx∈T

Xx∈T

Xx∈T

Xx∈T

Xx∈T

{x∈TC} −

q(x)

ℓ(w; x, yx)
i

(cid:1)
q(x))ℓ(w; x, yx)

q(x))

−

(cid:18)

1 + |

w
x
·
|
τk (cid:19)

(1

−

q(x))

w

|

x

|

·

1
τk

1

Xx∈TC

τk s Xx∈TC
T
ξk |

1
τk

p

p

33

(1

q(x))2

·

−

5C2 |

| ·

TC| ·

x)2

(w

·

(D.9)

s Xx∈TC
zk,

where ζ1 follows from the simple fact that

1

Xx∈T

(cid:0)

{x∈TC} −

q(x)

ℓ(w; x, yx) =

(cid:1)

Xx∈TC

≤

Xx∈TC

(1

(1

−

−

q(x))ℓ(w; x, yx) +

q(x))ℓ(w; x, yx)

(
−

Xx∈TD

q(x))ℓ(w; x, yx),

ζ2 explores the fact that the hinge loss is always upper bounded by 1+ |w·x|
0, ζ3
τk
follows from Part 2 of Proposition 30, ζ4 applies Cauchy-Schwarz inequality, and ζ5 uses Eq. (D.6).

and that 1

q(x)

−

≥

In view of Eq. (D.8), we have |T |

|TC| ≤

2. Continuing Eq. (D.9), we obtain

ℓτk (w; ˜TC)

≤

=

1

|

TC| Xx∈T
x∈T q(x)
TC| Xx∈T

|

P

q(x)ℓ(w; x, yx) + 2ξk +

10C2ξk ·

zk
τk

p

p(x)ℓ(w; x, yx) + 2ξk +

10C2ξk ·

zk
τk

p

= ℓτk (w; p) +

ℓτk (w; p) +

  P
T
|
|
TC|
|
ℓτk (w; p) + 2ξk

(cid:18)

≤

≤

−

x∈T q(x)
TC|
|
1
(cid:19) Xx∈T

−

Xx∈T

p(x)ℓ(w; x, yx) + 2ξk +

1
!

Xx∈T

p(x)ℓ(w; x, yx) + 2ξk +

p(x)ℓ(w; x, yx) + 2ξk +

p
10C2ξk ·

p

10C2ξk ·
zk
τk

p
10C2ξk ·
zk
τk

,

zk
τk

(D.10)

where in the last inequality we use
have the following result which will be proved later on.

TC| −

T
|

/

|

|

1 = |TD|/|T |

1−|TD|/|T | ≤

2

TD|

|

/

T

|

. On the other hand, we
|

Claim D.1.

x∈T p(x)ℓ(w; x, yx)
Therefore, continuing Eq. (D.10) we have

1 + √10C2 ·

≤

P

zk
τk

.

ℓτk (w; ˜TC)

≤

ℓτk (w; p) + 2ξk

1 +

(cid:18)

10C2 ·

zk
τk (cid:19)

+

p

p

10C2ξk ·

zk
τk

.

which proves the ﬁrst inequality of the proposition.
Step 2. We move on to prove the second inequality of the theorem, i.e. using ℓτk (w; ˜TC) to upper
x∈TD p(x) the probability mass on dirty instances. Then
bound ℓτk (w; p). Let us denote by pD =

pD =

P
x∈TD q(x)
x∈T q(x) ≤

TD|
|
ξk)
−

(1

ξk

T

≤

1

ξk ≤

P
P
where the ﬁrst inequality follows from q(x)
follows from (D.7), and the last inequality is by our choice ξk ≤
Note that by Part 2 of Proposition 30 and the choice ξk ≤
T
ξk)
|

/2. Hence

| ≥ |

−

≤

T

|

|

|

1/2.

2ξk,

(D.11)

1 and Part 2 of Proposition 30, the second inequality

1/2, we have

x∈T q(x)

(1

−

≥

P

p(x)(w

x)2 =

·

Xx∈T

1
x∈T q(x)

Xx∈T

P

q(x)(w

x)2

·

≤

2
T

|

| Xx∈T

q(x)(w

x)2

·

≤

10C2z2
k

(D.12)

34

where the last inequality holds because of (D.5). Thus,

p(x)ℓ(w; x, yx)

Xx∈TD

p(x)

1 + |

w
x
·
|
τk (cid:19)

(cid:18)

≤

Xx∈TD
= pD +

= pD +

1
τk

1
τk

1

p(x)

w

|

x

|

·

Xx∈TD

1
Xx∈T (cid:16)

{x∈TD}

p(x)

p

(cid:17)

p(x)

w
|

·

(cid:16)p

1

{x∈TD}p(x)

p(x)(w

·

·

sXx∈T

x

|

·

(cid:17)
x)2

pD +

≤

τk sXx∈T
pD + √pD ·

(D.12)

≤

10C2 ·

zk
τk

.

p
With the result on hand, we bound ℓτk (w; p) as follows:

ℓτk (w; p) =

p(x)ℓ(w; x, yx) +

p(x)ℓ(w; x, yx)

Xx∈TC

ℓ(w; x, yx) +

Xx∈TD
p(x)ℓ(w; x, yx)

≤

Xx∈TC

= ℓτk (w; ˜TC) +

Xx∈TD
p(x)ℓ(w; x, yx)

Xx∈TD

≤
(D.11)

ℓτk (w; ˜TC) + pD + √pD ·
ℓτk (w; ˜TC) + 2ξk +

≤

10C2 ·

p
20C2ξk ·

zk
τk
zk
τk

,

which proves the second inequality of the proposition.

p

Putting together. We would like to show
κ. Indeed, this is guaranteed
ℓτk (w; p)
by our setting of ξk in Section 3.2 which ensures that ξk simultaneously fulﬁlls the following three
(cid:12)
(cid:12)
constraints:
(cid:12)

ℓτk (w; ˜TC)
(cid:12)
(cid:12)
(cid:12)

−

≤

2ξk

1 +

(cid:18)
2ξk +

p

10C2 ·

20C2ξk ·

+

zk
τk (cid:19)
zk
τk ≤

10C2ξk ·

zk
τk ≤
and ξk ≤

p
κ,

κ,

1
2

.

This completes the proof.

p

35

Proof of Claim D.1. Since ℓ(w; x, yx)

1 + |w·x|
τk

≤

, it follows that

p(x)ℓ(w; x, yx)

Xx∈T

≤

Xx∈T
= 1 +

1 +

≤

p(x)

(cid:18)

1
τk

1

Xx∈T

1 + |

p(x)

w
x
·
|
τk (cid:19)
x
w
|

|

·

p(x)(w

x)2

·

τk sXx∈T

which completes the proof of Claim D.1.

(D.12)

≤

1 +

10C2 ·

zk
τk

,

p

The following result is a simple application of Proposition 24. It shows that the loss evaluated

on clean instances concentrates around the expected loss.

Proposition 32 (Restatement of Proposition 9). Consider phase k of Algorithm 1. Suppose that
δk
4 over the draw
Assumption 1 and 2 are satisﬁed, and assume η
of ¯T , for all w
Wk we have

c5ǫ. Then with probability 1

−

≤

∈

−
ℓτk (w; x, sign (w∗

Lτk (w)
(cid:12)
(cid:12)
(cid:12)

ℓτk (w; ˜TC)
(cid:12)
(cid:12)
(cid:12)

x))

κ.

≤

where Lτk (w) := Ex∼Dwk−1,bk
Proof. The choice of nk, i.e. the size of
ζ log ζ where ζ = K
(cid:12)
(cid:12)
in allusion to Proposition 24 and union bound, immediately gives the desired result.

is at least
for some constant K > 0 in view of Lemma 28. This observation

, ensures that with probability 1

(cid:2)
log d
δk

s log2 d

TC|

δk
8 ,

bk ·

¯T

−

(cid:12)
(cid:12)

(cid:3)

.

·

·

|

D.4 Analysis of random sampling

Proposition 33 (Restatement of Proposition 10). Consider phase k Algorithm 1. Suppose that
c5ǫ. Set nk and mk as in Section 3.2. Then with
Assumption 1 and 2 are satisﬁed, and assume η
δk
Wk we have
4 over the draw of Sk, for all w
probability 1

−

≤
∈
ℓτk (w; Sk)

ℓτk (w; p)

−

κ.

≤

Proof. Since we applied pruning to remove all instances with large ℓ∞-norm, this proposition can
be proved by a standard concentration argument for uniform convergence of linear classes under
distributions with ℓ∞ bounded support. We include the proof for completeness.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Note that the randomness is taken over the i.i.d. draw of mk samples from T according to
Sk, E[ℓτk (w; x, y)] = ℓτk (w; p). Moreover, let
Rk with probability 1. It is

the distribution p over T . Thus, for any (x, y)
x
Rk = maxx∈T k
also easy to verify that

k∞. Any instance x drawn from T satisﬁes

k∞ ≤

x
k

∈

ℓτk (w; x, y)

≤

1 + |

w

x
·
|
τk ≤

1 +

(w

−

x

wk−1)
τk

·

+ |

wk−1 ·
τk

x

|

≤

1 +

ρkRk
τk

+

bk
τk

.

By Theorem 8 of [BM02] along with standard symmetrization arguments, we have that with prob-
ability at least 1

δk
4 ,

−

ℓτk (w; p)

−

ℓτk (w; Sk)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1 +

≤

(cid:18)

ρkRk
τk

+

bk
τk (cid:19) s

ln(4/δk)
2mk

+

(
F
R

; Sk)

(D.13)

36

where
and

(
R
F
:=

; Sk) denotes the Rademacher complexity of function class
ℓτk (w; x, y) : w

. In order to calculate

Wk

F

∈

(cid:8)

ℓτk (w; x, y) is a composition of φ(a) = max
Wk}

. Since φ(a) is 1
τk

(cid:9)

0, 1

−

1
τk

ya

on the labeled set Sk,
; Sk), we observe that each function
:=

(
R
F
and function class

x : w

F

w

G

x
{

7→

·

∈

o
-Lipschitz, by contraction property of Rademacher complexity, we have

n

(
F
R

; Sk)

≤

1
(
τk R
G

; Sk).

(D.14)

where the σi’s are i.i.d. draw from the Rademacher distribution, and let

Let σ =
Vk = B2(0, rk)

σ1, . . . , σmk }
{
∩

B1(0, ρk). We compute

; Sk) as follows:

(
R
G
mk

(
G
R

; Sk) =

=

=

1
mk

1
mk

1
mk

Eσ

Eσ

Eσ

(cid:20)

(cid:20)

(cid:20)

w

sup
w∈Wk

·

wk−1 ·

σixi

(cid:18)
mk

Xi=1
σixi

(cid:19)(cid:21)

+

(cid:18)

Xi=1
mk

(cid:19)(cid:21)

v

sup
v∈Vk

·

σixi

(cid:18)

Xi=1

(cid:19)(cid:21)

1
mk

Eσ

(cid:20)

(w

sup
w∈Wk

−

wk−1)

mk

σixi

(cid:19)(cid:21)

·

(cid:18)

Xi=1

ρkRk

≤

2 log(2d)
mk

,

s

where the ﬁrst equality is by the deﬁnition of Rademacher complexity, the second equality simply
decompose w as a sum of wk−1 and w
wk−1, the third equality is by the fact that every σi has
zero mean, and the inequality applies Lemma 39. We combine the above result with (D.13) and
(D.14), and obtain that with probability 1

−

δk
4 ,

−

ℓτk (w; p)

−

ℓτk (w; Sk)

1 +

≤

(cid:18)

ρkRk
τk

+

bk
τk (cid:19) s

ln(4/δk)
mk

+

ρkRk

τk s

2 log(2d)
mk

.

(D.15)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Recall that we remove all instances with large ℓ∞-norm in the pruning step of Algorithm 1. In
particular, we have

Rk ≤
Plugging this upper bound into (D.15) and using our hyper-parameter setting gives

c9 log

.

48nkd
bkδk

ℓτk (w; p)

−

ℓτk (w; Sk)

K1 ·

≤

√s log

(cid:12)
(cid:12)

for some constant K1 > 0. Hence,

(cid:12)
(cid:12)

nkd
bkδk 

s

log(1/δk)
mk

log d
mk 

+

s





mk = O

s log2 nkd
bkδk ·

log

d
δk (cid:19)

= ˜O

s log2 d

bkδk ·

log

d
δk (cid:19)

(cid:18)

(cid:18)

suﬃces to ensure

ℓτk (w; p)

ℓτk (w; Sk)

−

≤

κ with probability 1

δk
4 .

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

37

D.5 Analysis of Per-Phase Progress
Let Lτk (w) = Ex∼Dwk−1,bk
Lemma 34 (Lemma 3.7 of [ABL17]). Suppose Assumption 1 is satisﬁed. Then

ℓτk (w; x, sign (w∗

x))

(cid:3)

(cid:2)

·

.

In particular, by our choice of τk

Lτk (w∗)

≤

c0 min

τk
bk, 1/9
}
{

.

Lτk (w∗)

κ.

≤

Lemma 35. For any 1

k

≤

≤

k0, if w∗

∈

Wk, then with probability 1

δk, errDwk−1,bk

(vk)

8κ.

≤

−

Proof. Observe that with the setting of Nk, we have with probability 1
δk over all the randomness
in phase k, Lemma 26, Proposition 31, Proposition 32 and Proposition 33 hold simultaneously. Now
we condition on the event that all of these properties are satisﬁed, which implies for all w

−

Wk,

∈

Lτk (w)

−

ℓτk (w; Sk)

3κ.

≤

(D.16)

We have

(cid:12)
(cid:12)

errDwk−1,bk

(vk)

Lτk (vk)

≤

ζ1

≤

ℓτk (vk; Sk) + 3κ

(cid:12)
(cid:12)

ζ2

≤

min
w∈Wk

ℓτk (w; Sk) + 4κ

ζ3

≤

≤

ℓτk (w∗; Sk) + 4κ

Lτk (w∗) + 7κ.

In the above, the ﬁrst inequality follows from the fact that hinge loss upper bounds the 0/1 loss, ζ1
and the last inequality applies (C.1), ζ2 is by the deﬁnition of vk (see Algorithm 1), and ζ3 is by
our assumption that w∗ is feasible. The proof is complete in view of Lemma 34.

Lemma 36. For any 1

k

≤

≤

k0, if w∗

∈

Wk, then with probability 1

δk, θ(vk, w∗)

2−k−8π.

≤

−

Proof. For k = 1, by Lemma 35 and that we actually sample from D, we have

Hence Part 4 of Lemma 12 indicates that

Prx∼D

sign (v1 ·

x)

= sign

w∗

x

·

8κ.

≤

(cid:1)(cid:17)

(cid:0)

(cid:16)

θ(v1, w∗)

≤
k0. Denote Xk =

8c2κ = 16c2κ

2−1.

·

(D.17)

Now we consider 2

x :
.
> bk}
{
We will show that the error of vk on both Xk and ¯Xk is small, hence vk is a good approximation to
w∗.

wk−1 ·
|

wk−1 ·

x :
{

bk}

| ≤

≤

≤

x

x

k

|

|

, and ¯Xk =

First, we consider the error on Xk, which is given by

Prx∼D

= Prx∼D

sign (vk ·
(cid:16)
sign (vk ·
(cid:16)
(vk)

·

x)

= sign

w∗

·

x

, x

∈

x)

= sign

(cid:0)

w∗

·
Xk)
(cid:0)
∈

x

|

∈

(cid:1)

x

(cid:1)

= errDwk−1,bk

Prx∼D(x

2bk
8κ
≤
= 16κbk,

·

38

Xk

(cid:17)
Xk

(cid:17)

Prx∼D(x

Xk)

∈

·

(D.18)

6
6
6
where the inequality is due to Lemma 35 and Lemma 17. Note that the inequality holds with
probability 1

Next we derive the error on ¯Xk. Note that Lemma 10 of [Zha18] states for any unit vector u,

δk.

−

and any general vector v, θ(v, u)

θ(vk, w∗)

π

≤

≤
w∗
vk −

π

v
k

−

2 ≤
(cid:13)
(cid:13)

(cid:13)
(cid:13)

k2. Hence,
u
vk −
π(
k

wk−1k2 +

w∗

−

wk−1

2)

≤

2πrk.

Recall that we set rk = 2−k−3 < 1/4 in our algorithm and choose bk = ¯c
allows us to apply Lemma 13 and obtain

(cid:13)
(cid:13)

(cid:13)
rk where ¯c
(cid:13)

·

≥

8π/c4, which

Prx∼D

sign (vk ·
(cid:16)

x)

= sign

w∗

x

·

, x /
∈

Xk

(cid:0)

(cid:1)

(cid:17)

≤

c3 ·
= 2−k

2πrk ·
c3π
4

·

This in allusion to (D.18) gives

exp

(cid:18)

−

exp

−

(cid:18)

rk
·
2πrk (cid:19)
.

c4¯c
2
·
c4¯c
4π

(cid:19)

errD(vk)

16κ

¯c

·

·

≤

rk + 2−k

c3π
4

·

exp

c4¯c
4π

(cid:19)

−

(cid:18)

=

2κ¯c +

c3π
4

exp

c4¯c
4π

−

(cid:18)

(cid:19)! ·

2−k.

Recall that we set κ = exp(
By Part 4 of Lemma 12

−

¯c) and denote by f (¯c) the coeﬃcient of 2−k in the above expression.

Now let g(¯c) = c2f (¯c) + 16c2 exp(

both (D.17) and (D.19), θ(vk, w∗)

−
2−k−8π for any k

≤

1.

≥

θ(vk, w∗)

c2 errD(vk)

c2f (¯c)

≤
¯c). By our choice of ¯c, g(¯c)

≤

·

2−k.

(D.19)

2−8π. This ensures that for

≤

Lemma 37. For any 1

≤
Proof. We ﬁrst show that
2 sin θ(vk,w∗)

θ(vk, w∗)

2

≤
wk −
k
2−k−8π

≤

≤

k

k0, if θ(vk, w∗)

2−k−8π, then w∗

≤

rk+1. Let ˆvk = vk/

k2 ≤
2−k−6. Now we have

k

w∗

≤

Wk+1.

∈
vkk2. By algebra

ˆvk −

k

w∗

k2 =

w∗

wk −

(cid:13)
(cid:13)

2 =
(cid:13)
(cid:13)

=

Hs(vk)
(cid:13)
Hs(ˆvk)
(cid:13)
w∗
(cid:13)
(cid:13)

2

w∗

w∗

2 −

2 −

2
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)

Hs(vk)/
(cid:13)
(cid:13)
(cid:13)
Hs(ˆvk)/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
Hs(ˆvk)
(cid:13)
(cid:13)
−
(cid:13)
w∗
ˆvk −
4
(cid:13)
(cid:13)
2−k−4
(cid:13)
≤
(cid:13)
= rk+1.

(cid:13)
(cid:13)

≤

≤

By the sparsity of wk and w∗, and our choice ρk+1 = √2srk+1, we always have

w∗

wk −

1 ≤

√2s

w∗

wk −

The proof is complete.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

√2srk+1 = ρk+1.

2 ≤
(cid:13)
(cid:13)

39

6
 
D.6 Proof of Theorem 4

Proof. We will prove the theorem with the following claim.

Claim D.2. For any 1

k

≤

≤

k0, with probability at least 1

−

Based on the claim, we immediately have that with probability at least 1

P

is in Wk0+1. By our construction of Wk0+1, we have

k
i=1 δi, w∗ is in Wk+1.

k0

k=1 δk ≥

1

−

δ, w∗

−

P

w∗

wk0

−

2 ≤

2−k0−4.

This, together with Part 4 of Lemma 12 and the fact that θ(w∗, wk0)
of [Zha18]), implies

(cid:13)
(cid:13)
π
c1 ·
Finally, we derive the sample complexity and label complexity. Recall that nk was involved in

2 (see Lemma 10

2−k0−4 = ǫ.

errD(wk0)

wk0

w∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

−

≤

π

Proposition 30, i.e. the quantity

s2 log4 d
bk ·
It is also involved in Proposition 33, where we need

T
, where we required
|
|
log d + log2 1
δk
(cid:16)

1
δk (cid:19)

nk = ˜O

+ log

(cid:18)

(cid:17)

= ˜O

s2 log4 d
bk ·

(cid:18)

(cid:16)

log d + log2 1
δk

.

(cid:17)(cid:19)

mk = O

s log2 nkd
bkδk ·

log

d
δk (cid:19)

(cid:18)

and nk ≥
choice of nk is given by

mk since Sk is a labeled subset of T . As mk has a cubic dependence on log 1
δk

This in turn gives

nk = ˜O

(cid:18)

s2 log4 d
bk ·

log d + log3 1
δk

(cid:16)
s log2 d

log

bkδk ·

d
δk (cid:19)

.

mk = ˜O

(cid:18)

.

(cid:17)(cid:19)

, our ﬁnal

(D.20)

(D.21)

Therefore, by Lemma 26 we obtain an upper bound of the sample size Nk at phase k as follows:

Nk = ˜O

s2
bk

log4 d
bk ·

(cid:18)

(cid:16)

log d + log3 1
δk

˜O

≤

s2
ǫ

(cid:18)

log4 d

(cid:16)

(cid:17)(cid:19)

log d + log3 1
δ

,

(cid:17)(cid:19)

where the last inequality follows from bk = Ω(ǫ) for all k
the total sample complexity

≤

k0 and our choice of δk. Consequently,

k0

N =

Xk=1

Nk ≤

k0 ·

˜O

s2
ǫ

(cid:18)

log4 d

(cid:16)

log d + log3 1
δ

= ˜O

s2
ǫ

(cid:18)

(cid:17)(cid:19)

log4 d

(cid:16)

log d + log3 1
δ

.

(cid:17)(cid:19)

Likewise, we can show that the total label complexity

k0

m =

Xk=1

mk ≤

k0 ·

˜O

s log2 d
ǫδ ·
(cid:16)

log

d
δ

(cid:17)

= ˜O

(cid:16)

s log2 d
ǫδ ·

log

d
δ ·

log

1
ǫ

.

(cid:17)

∈

It remains to prove Claim D.2 by induction. First, for k = 1, W1 = B2(0, 1)

∩
W1 with probability 1. Now suppose that Claim D.2 holds for some k

Therefore, w∗
is, there is an event Ek−1 that happens with probability 1
By Lemma 36 we know that there is an event Fk that happens with probability 1
θ(vk, w∗)
consider the event Ek−1 ∩
1
δi)(1
(1
−

B1(0, √s).
2, that
Wk.
δk, on which
Wk+1 in view of Lemma 37. Therefore,
Ek−1) =

2−k−8π. This further implies that w∗

≥
δi, and on this event w∗

Wk+1 with probability Pr(Ek−1)

−
Pr(Fk |

Fk, on which w∗

k
i=1 δi.

k−1
i

k−1
i

δk)

P

−

≤

−

≥

−

∈

∈

∈

·

P

P

40

E Miscellaneous Lemmas

Lemma 38 (Chernoﬀ bound). Let Z1, Z2, . . . , Zn be n independent random variables that take value
in

n
i=1 Zi. For each Zi, suppose that Pr(Zi = 1)

η. Then for any α

[0, 1]

. Let Z =
0, 1
}
{

≤

∈

P

Pr

Z

≥

(1 + α)ηn

When Pr(Zi = 1)

η, for any α

≥

(cid:0)
[0, 1]

∈

≤

(cid:1)

Pr

Z

(1

−

≤

α)ηn

≤

2

ηn
3

e− α

2

ηn
2

e− α

.

.

(cid:0)
Lemma 39 (Theorem 1 of [KST08]). Let σ = (σ1, . . . , σn) where σi’s are independent draws from
the Rademacher distribution and let x1, . . . , xn be given instances in Rd. Then

(cid:1)

2n log(2d) max

1≤i≤n k

xik∞ .

Eσ

sup
w∈B1(0,ρ)

(cid:20)

n

Xi=1

σiw

xi

·

ρ

≤

(cid:21)

p

41

