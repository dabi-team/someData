Learning programs with magic values

1

2
2
0
2

t
c
O
1

]

G
L
.
s
c
[

2
v
8
3
2
3
0
.
8
0
2
2
:
v
i
X
r
a

Learning programs with magic values

C´eline Hocquette1* and Andrew Cropper1

1Department of Computer Science, University of Oxford.

*Corresponding author(s). E-mail(s):
celine.hocquette@cs.ox.ac.uk;
Contributing authors: andrew.cropper@cs.ox.ac.uk;

Abstract

A magic value in a program is a constant symbol that is essential for
the execution of the program but has no clear explanation for its choice.
Learning programs with magic values is diﬃcult for existing program
synthesis approaches. To overcome this limitation, we introduce an induc-
tive logic programming approach to eﬃciently learn programs with magic
values. Our experiments on diverse domains, including program syn-
thesis, drug design, and game playing, show that our approach can (i)
outperform existing approaches in terms of predictive accuracies and
learning times, (ii) learn magic values from inﬁnite domains, such as the
value of pi, and (iii) scale to domains with millions of constant symbols.

Keywords: Inductive logic programming, program synthesis

1 Introduction

A magic value in a program is a constant symbol that is essential for the
good execution of the program but has no clear explanation for its choice.
For instance, consider the problem of classifying lists. Figure 1 shows posi-
tive and negative examples. Figure 2 shows a hypothesis which discriminates
between the positive and negative examples. Learning this hypothesis involves
the identiﬁcation of the magic number 7.

Magic values are fundamental to many areas of knowledge,

including
physics and mathematics. For instance, the value of pi is essential to compute
the area of a disk. Likewise, the gravitational constant is essential to identify
whether an object subject to its weight is in mechanical equilibrium. Similarly,

1

 
 
 
 
 
 
2

1 INTRODUCTION

Positive examples

Negative examples

f([a,b,c,7,8,k,f])
f([p,3,9,y,5,r,a,q,7])
f([e,a,a,b,c,7,t,a,b,c,x,z,r,t])

f([x,y,z,8,3,k,f,x,t,8,k,f])
f([a,b,c,p,r,w,q,9])
f([u,k,a,b,c,z,r,t,5,e,t])

Fig. 1 Positive and negative examples

Fig. 2 Target hypothesis

f(A)← head(A,7)
f(A)← tail(A,B),f(B)

f(A)← head(A,B), @magic(B)
f(A)← tail(A,B), f(B)

Fig. 3 Intermediate hypothesis

consider the classical AI task of learning to play games. To play the game con-
nect four 1, a learner must correctly understand the rules of this game, which
implies that they must discover the magic value four, i.e. four tokens in a row.
Although fundamental to AI, learning programs with magic values is dif-
ﬁcult for existing program synthesis approaches. For instance, many recent
inductive logic programming (ILP) [1, 2] approaches ﬁrst enumerate all possi-
ble rules allowed in a program [3–6] and then search for a subset of them. For
example, ASPAL [3] precomputes every possible rule and uses an answer set
solver to ﬁnd a subset of them. Other approaches similarly represent constants
as unary predicate symbols [6, 7]. Both approaches suﬀer from two major lim-
itations. First, they need a ﬁnite and tractable number of constant symbols
to search through, which is clearly infeasible for large and inﬁnite domains,
such as when reasoning about continuous values. Second, they might gener-
ate rules with irrelevant magic values that never appear in the data, and thus
suﬀer from performance issues. Older ILP approaches similarly struggle with
magic values. For instance, for Progol [8] to learn a rule with a constant
symbol, that constant must appear in the bottom clause of an example. Pro-
gol, therefore, struggles to learn recursive programs with constant values. It
can also struggle when the bottom clause grows extremely large due to many
potential magic values.

The goal of this paper, and therefore its main contribution, is to overcome
these limitations by introducing an ILP approach that can eﬃciently learn
programs with magic values, including values from inﬁnite and continuous
domains. The key idea of our approach, which is heavily inspired by Aleph’s
lazy evaluation approach [9], is to not enumerate all possible magic values but
to instead generate hypotheses with variables in place of constant symbols
that are later ﬁlled in by a learner. In other words, the learner ﬁrst builds
a partial general hypothesis and then lazily ﬁlls in the speciﬁc details (the
magic values) by examining the given data. For instance, reconsider the task of
identifying the magic number 7 in a list. The learner ﬁrst constructs a partial

1Connect four is a two-player game in which the players take turns dropping coloured tokens
into a grid. The goal of the game is to be the ﬁrst to form a horizontal, vertical, or diagonal line
of four of one’s own tokens.

3

intermediate hypothesis as the one shown in Figure 3. In the ﬁrst clause,
the ﬁrst-order variable B is marked as a constant with the internal predicate
@magic. However, it is not bound to any particular constant symbol. The value
for this magic variable is lazily identiﬁed by executing this hypothesis on the
examples.

As the example in Figure 3 illustrates, the key advantages of our approach
compared to existing ones are that it (i) does not rely on enumeration of
all constant symbols but only considers candidate constant values which can
be obtained from the examples, (ii) can learn programs with magic values
from large and inﬁnite domains, and (iii) can learn magic values for recursive
programs.

To implement our approach, we build on the learning from failures (LFF)
[7] approach. LFF is a constraint-driven ILP approach where the goal is to
accumulate constraints on the hypothesis space. A LFF learner continually
generates and tests hypotheses, from which it infers constraints. For instance,
if a hypothesis is too general (i.e. entails a negative example), then a generali-
sation constraint prunes generalisations of this hypothesis from the hypothesis
space.

Current LFF approaches [10–12] cannot, however, reason about partial
hypotheses, such as the one shown in Figure 3. They must instead all enu-
merate constant symbols by representing them as unary predicate symbols.
Current approaches, therefore, suﬀer from the same limitations as other recent
ILP approaches, i.e. they struggle to scale to large and inﬁnite domains.
We, therefore, extend the LFF constraints to prune such intermediate partial
hypotheses. Each constraint prunes sets of intermediate hypotheses, each of
which represents the set of its instantiations. We prove that these extended
constraints are optimally sound: they do not prune optimal solutions from the
hypothesis space.

We implement our magic value approach in MagicPopper, which, as it
builds on the LFF learner Popper, supports predicate invention [10] and learn-
ing recursive programs. MagicPopper can learn programs with magic values
from domains with millions of constant symbols and scale to inﬁnite domains.
For instance, we show that MagicPopper can learn (an approximation of)
the value of pi. In addition, in contrast to existing approaches, MagicPop-
per does not need to be told which arguments may be bound to magic values
but instead can automatically identify them if any is needed, although this
fully automatic approach comes with a high cost in terms of performance.
In particular, it can cost additional learning time and can lower predictive
accuracies.

Contributions.

We claim that our approach can improve learning performance when learning
programs with magic values. To support our claim, we make the following
contributions:

4

2 RELATED WORK

1. We introduce a procedure for learning programs in domains with large and

potentially inﬁnite numbers of constant symbols.

2. We extend the LFF hypothesis constraints to additionally prune hypotheses
with constant symbols. We prove the optimal soundness of these constraints.
3. We implement our approach in MagicPopper, which supports learning

recursive programs and predicate invention.

4. We experimentally show on multiple domains (including program synthe-
sis, drug design, and game playing) that our approach can (i) scale to
large search spaces with millions of constant symbols, (ii) learn from inﬁ-
nite domains, and (iii) outperform existing systems in terms of predictive
accuracies and learning times when learning programs with magic values.

2 Related Work

Numeric discovery

Early discovery systems identiﬁed relevant numerical values using a ﬁxed set
of basic operators, such as linear regression, which combined existing numeri-
cal values. The search followed a combinatorial design [13, 14] or was based on
beam search guided by heuristics, such as correlation [15] or qualitative propor-
tionality [16]. These systems could rediscover physical laws with magic values.
However, the class of learnable concepts was limited. BACON [13], for instance,
cannot learn disjunctions representing multiple equations. Conversely, Mag-
icPopper can learn recursive programs and perform predicate invention.
Moreover, MagicPopper can take as input normal logic program background
knowledge and is not restricted to a ﬁxed set of predeﬁned operators.

Symbolic regression

Symbolic regression searches a space of mathematical expressions, using
genetic programming algorithms [17] or formulating the problem as a mixed
integer non-linear program [18]. However, these approaches cannot learn recur-
sive programs nor perform predicate invention and are restricted to learning
mathematical expressions.

Program synthesis

Program synthesis [19] approaches based on the enumeration of the search
space [20, 21] struggle to learn in domains with a large number of constant
symbols. For instance, the Apperception engine [22] disallows constant symbols
in learned hypotheses, apart from the initial conditions represented as ground
facts. To improve the likelihood of identifying relevant constants, Hemberg
et al. [23] manually identify a set of constants from the problem descrip-
tion. Compared to generate-and-test approaches, analytical approaches do not
enumerate all candidate programs and can be faster [24].

Several program synthesis systems consider partial programs in the search.
Neo [25] constructs partial programs, successively ﬁlls their unassigned parts,

5

and prunes partial programs which have no feasible completion. By contrast,
MagicPopper only ﬁlls partial hypotheses with constant symbols. Moreover,
MagicPopper evaluates hypotheses based on logical inference only while Neo
also uses statistical inference. Finally, Neo cannot learn recursive programs.
Perhaps the most similar work is Sketch [26], which uses an SAT solver to
search for suitable constants given a partial program. This approach expects
as input a skeleton of a solution: it is given a partial program and the task
is to ﬁll in the magic values with particular constants symbols. Conversely,
MagicPopper learns both the program and the magic values.

ILP

Bottom clauses.
Early ILP approaches, such as Progol [8] and Aleph [27], use bottom clauses
[8] to identify magic values. The bottom clause is the logically most-speciﬁc
clause that explains an example. By constructing the bottom clause, these
approaches restrict the search space and, in particular, identify a subset of
relevant constant symbols to consider. However, this bottom clause approach
has multiple limitations. First, the bottom clause may grow large which inhibits
scalability. Second, this approach cannot use constants that do not appear in
the bottom clause, which is constructed from a single example. Third, this
approach struggles to learn recursive programs and does not support predicate
invention. Finally, as they rely on mode declarations [8] to build the bottom
clause, they need to be told which argument of which relations should be bound
to a constant.

Lazy evaluation.
The most related work is an extension of Aleph that supports lazy evaluation
[9]. During the construction of the bottom clause, Aleph replaces constant
symbols with existentially quantiﬁed output variables. During the reﬁnement
search of the bottom clause, Aleph ﬁnds substitutions for these variables
by executing the partial hypothesis on the positive and negative examples.
In other words, instead of enumerating all constant symbols, lazy evaluation
only considers constant symbols computable from the examples. Therefore,
lazy evaluation provides better scalability to large domains. This approach can
identify constant symbols not seen in the bottom clause. Moreover, in contrast
with MagicPopper, it also can identify constant symbols whose value arises
from reasoning from multiple examples, such as coeﬃcients in linear regression
or numerical inequalities. It also can predict output numerical variables using
custom loss functions measuring error [28]. However, this approach inherits
some of the limitations of bottom clause approaches aforementioned including
limited learning of recursion and lack of predicate invention. Moreover, the
user needs to provide a deﬁnition capable of computing appropriate constant
symbols from lists of inputs, such as a deﬁnition for computing a threshold

6

2 RELATED WORK

or regression coeﬃcients from data. The user must also provide a list of vari-
ables that should be lazy evaluated or bound to constant symbols in learned
hypotheses.

Regression.

First-order regression [29] and structural regression tree [30] predict continu-
ous numerical values from examples and background knowledge. First-order
regression builds a logic program that can include literals performing linear
regression, whereas MagicPopper cannot perform linear regression. Struc-
tural regression tree builds trees with a numerical value assigned to each leaf.
In contrast with MagicPopper, these two approaches do not learn optimal
programs.

Logical decision and clustering trees.
Tilde [31] and TIC [32] are logical extensions of decision tree learners and
can learn hypotheses with constant values as part of the nodes that split the
examples. These nodes are conjunctions built from the mode declarations.
Tilde and TIC evaluate each candidate node, and select the one which results
in the best split of the examples. Tilde can also use a discretisation procedure
to ﬁnd relevant numerical constants from large, potentially inﬁnite domains,
while making the induction process more eﬃcient [33]. However, this approach
only handles numerical values while MagicPopper can handle magic values
of any type. Moreover, Tilde cannot learn recursive programs and struggles
to learn from small numbers of examples.

Meta-interpretive learning.

Meta-interpretive learning (MIL) [34] uses meta-rules, which are second-order
clauses acting as program templates, to learn programs. A MIL learner induces
programs by searching for substitutions for the variables in meta-rules. These
variables usually denote predicate variables, i.e. variables that can be bound
to a predicate symbol. For instance, the MIL learner Metagol ﬁnds variable
substitutions by constructing a proof of the examples. Metagol can learn
programs with magic values by also allowing some variables in meta-rules to be
bound to constant symbols. With this approach, Metagol, therefore, never
considers constants which do not appear in the proof of at least one positive
example and thus does not enumerate all constants in the search space. Our
magic value approach is similar in that we construct a hypothesis with vari-
ables in it, then ﬁnd substitutions for these variables by testing the hypothesis
on the training examples. However, a key diﬀerence is that Metagol needs
a user-provided set of meta-rules as input to precisely deﬁne the structure
of a hypothesis, which is often diﬃcult to provide, especially when learn-
ing programs with relations of arity greater than two. Moreover, Metagol
does not remember failed hypotheses during the search and might consider
again hypotheses which have already been proved incomplete or inconsistent.
Conversely, MagicPopper can prune the hypothesis space upon failure of

7

completeness or consistency with the examples, which can improve learning
performance.

Meta-level ILP.

To overcome the limitations of older ILP systems, many recent ILP approaches
are meta-level [35] approaches, which predominately formulate the ILP prob-
lem as a declarative search problem. A key advantage of these approaches is
greater ability to learn recursive and optimal programs. Many of these recent
approaches precompute every possible rule in a hypothesis [3–6]. For instance,
ASPAL [3] precomputes every possible rule in a hypothesis space, which means
it needs to ground rules with respect to every allowed constant symbol. This
pure enumeration approach is intractable for domains with large number of
constant symbols and impossible for domains with inﬁnite ones. Moreover, the
variables which should be bound to constants must be provided as part of the
mode declarations by the user [3].

Other recent meta-level ILP systems, such as δ-ILP [6] and Popper [7], do
not directly allow constant symbols in clauses but instead require that constant
symbols are provided as unary predicates. These unary predicates are assumed
to be user-provided. Moreover, since the size of the search space is exponen-
tial into the number of predicate symbols, this approach prevents scalability
and in particular handling domains with inﬁnite number of constant symbols.
Conversely, MagicPopper identiﬁes relevant constant symbols by executing
hypotheses over the positive examples, and can scale to inﬁnite domains. In
addition, it does not express constant symbols with additional predicates and
thus can learn shorter hypotheses.

3 Problem Setting

Logic preliminaries.

We assume familiarity with logic programming [36] but restate some key ter-
minology. A variable is a string of characters starting with an uppercase letter.
A function symbol is a string of characters starting with a lowercase letter.
A predicate symbol is a string of characters starting with a lowercase letter.
The arity n of a function or predicate symbol p is the number of arguments it
takes. A unary or monadic predicate is a predicate with arity one. A constant
symbol is a function symbol with arity zero. A term is a variable or a func-
tion symbol of arity n immediately followed by a tuple of n terms. An atom is
a tuple p (t 1, ..., t n ), where p is a predicate of arity n and t 1, ..., t n are terms,
either variables or constants. An atom is ground if it contains no variables. A
literal is an atom or the negation of an atom. A clause is a set of literals. A
constraint is a clause without a positive literal. A deﬁnite clause is a clause
with exactly one positive literal. A program is a set of deﬁnite clauses. A sub-
stitution θ = {v1/t 1, ..., vn /t n } is the simultaneous replacement of each variable
vi by its corresponding term ti . A clause C1 subsumes a clause C2 if and only
if there exists a substitution θ such that C1θ ⊆ C2. A program H1 subsumes

8

3 PROBLEM SETTING

a program H2, denoted H1 (cid:22) H2, if and only if [C2 ∈ H2, \C1 ∈ H1 such that
C1 subsumes C2. A program H1 is a specialisation of a program H2 if and only
if H2 (cid:22) H1. A program H1 is a generalisation of a program H2 if and only if
H1 (cid:22) H2.

3.1 Learning from Failures

Our problem setting is the learning from failures (LFF) [7] setting, which
in turn is based upon the learning from entailment setting [37]. LFF uses
hypothesis constraints to restrict the hypothesis space. LFF assumes a meta-
language L, which is a language about hypotheses. Hypothesis constraints are
expressed in L. A LFF input is deﬁned as:

Deﬁnition 1 A LFF input is a tuple (E +, E −, B, H , C ) where E + and E − are sets
of ground atoms representing positive and negative examples respectively, B is a
deﬁnite program representing background knowledge, H is a hypothesis space, and
C is a set of hypothesis constraints expressed in the meta-language L.

Given a set of hypotheses constraints C , we say that a hypothesis H is con-
sistent with C if, when written in L, H does not violate any constraint in C .
We call HC the subset of H consistent with C . We deﬁne a LFF solution:

Deﬁnition 2 Given a LFF input (E +, E −, B, H , C ), a LFF solution is a hypothesis
H ∈ HC such that H is complete with respect to E + ([e ∈ E +, B ∪ H |= e) and
consistent with respect to E − ([e ∈ E −, B ∪ H 6|= e).

Conversely, given a LFF input, a hypothesis H is incomplete when \e ∈ E +, H ∪
B 6|= e, and is inconsistent when \e ∈ E −, H ∪ B |= e.

In general, there might be multiple solutions given a LFF input. We asso-
ciate a cost to each hypothesis and prefer the ones with minimal cost. We
deﬁne an optimal solution:

Deﬁnition 3 Given a LFF input (E +, E −, B, H , C ) and a cost function cost : H → Ò,
a LFF optimal solution H1 is a LFF solution such that, for all LFF solution H2,
cost (H1) ≤ cost (H2).

A common bias is to express the cost as the size of a hypothesis. In the follow-
ing, we use this bias, and we measure the size of a hypothesis as the number
of literals in it.

Constraints

A hypothesis that is not a solution is called a failure. A LFF learner identi-
ﬁes constraints from failures to restrict the hypothesis space. We distinguish

9

H : f(A) ← length(A,B), @magic(B)

H1: f(A) ← length(A,B), c1(B)
H2: f(A) ← length(A,B), c2(B)
H3: f(A) ← length(A,B), c3(B)
H4: f(A) ← length(A,B), c4(B)
H5: f(A) ← length(A,B), c5(B)
H6: f(A) ← length(A,B), c6(B)
...

Fig. 4 Some hypotheses considered by Popper (left) and MagicPopper (right). c1, c2, c3,
c4, c5, and c6 are unary predicates that hold when their argument is the number 1, 2, 3, 4,
5, or 6 respectively. These unary predicates are assumed to be user-provided.

several kinds of failures, among which are the following. If a hypothesis is
incomplete, a specialisation constraint prunes its specialisations, as they are
provably also incomplete. If a hypothesis is inconsistent, a generalisation con-
straint prunes its generalisations, as they are provably also inconsistent. A
hypothesis is totally incomplete when [e ∈ E +, H ∪ B 6|= e. If a hypothesis
is totally incomplete, a redundancy constraint prunes hypotheses that con-
tain one of its specialisations as a subset [10]. These constraints are optimally
sound: they do not prune optimal solutions from the hypothesis space [7].

Example 1 (Hypotheses constraints) We call c2 the unary predicate which holds
when its argument is the number 2. Consider the following positive examples E + and
the hypothesis H0:

E + = {f ([b, a]), f ([c, a, e])}

H0: f(A) ← length(A,B), c2(B)
The second example is a list of length 3 while the hypothesis H0 only entails lists of
length 2. Therefore, the hypothesis H0 does not cover the second positive example
and thus is incomplete. We can soundly prune all its specialisations as they also are
incomplete. In particular, we can prune the specialisations H1 and H2:

H1: f(A) ← length(A,B), c2(B), head(A,B)
H2: f(A) ← length(A,B), c2(B), tail(A,C), empty(C)

4 Magic Evaluation

The constraints described in the previous section prune hypotheses. In partic-
ular, they can prune hypotheses with constant symbols as shown in Example 1.
However, hypotheses identical but with diﬀerent constant symbols are treated
independently despite their similarities.

For instance, Popper could consider all of the hypotheses represented
on the left of Figure 4. Each of these hypotheses would be considered
independently. For each of them, Popper learns constraints which prune spe-
cialisations, generalisations, or redundancy of this single hypothesis but do
not apply to other hypotheses. By contrast, as shown on the right of Figure
4, MagicPopper represents all these hypotheses jointly as a single one by

10

4 MAGIC EVALUATION

using variables in place of constant symbols. Thus, MagicPopper reasons
simultaneously about hypotheses with similar program structure but diﬀerent
constant symbols.

MagicPopper extends specialisation, generalisation, and redundancy

constraints to apply to such partial hypotheses.

Moreover, the unary predicate symbols used by Popper must be provided
as bias: it is assumed the user can provide a ﬁnite and tractable number of
them. Conversely, MagicPopper represents the set of hypotheses with similar
structure but with diﬀerent constant symbols as a single one, and therefore
can handle inﬁnite constant domains.

In this section, we introduce MagicPopper’s representation, present these

extended constraints, and prove they are optimally sound.

4.1 Magic Variables

A LFF learner uses a meta-language L to reason about hypotheses. We extend
this meta-language L to represent partial hypotheses with unbound constant
symbols. We deﬁne a magic variable:

Deﬁnition 4 A magic variable is an existentially quantiﬁed ﬁrst-order variable.

A magic variable is a placeholder for a constant symbol. It marks a variable as
a constant but does not require the particular constant symbol to be identiﬁed.
Particular constant symbols can be identiﬁed in a latter stage. We represent
magic variables with the unary predicate symbol @magic. For example, in the
following program H , the variable B marked with the syntax @magic is a magic
variable:

H : f(A) ← length(A,B), @magic(B)

This magic variable is not yet bound to any particular value. The use of
the predicate symbol @magic allows us to concisely represent the set of all
possible substitutions of a variable.

The predicate symbol @magic is an internal predicate. For this reason,
literals with this predicate symbol are not taken into account in the rule
size. For instance, the hypothesis H above has size 2. Therefore, compared to
approaches that use additional unary body literals to identify constant sym-
bols, our representation represents hypotheses with constant symbols with
fewer literals.

4.2 Magic Hypotheses

A magic hypothesis is a hypothesis with at least one magic variable. An instan-
tiated hypothesis, or instantiation, is the result of substituting magic variables
with constant symbols in a magic hypothesis. Magic evaluation is the process
of identifying a relevant subset of substitutions for magic variables in a magic
hypothesis to form instantiations.

4.3 Constraints

11

Example 2 (Magic hypothesis) The magic hypothesis H above may have the following
corresponding instantiated hypotheses, or instantiations, I1 and I2:

I1 : f(A) ← length(A,2)
I2 : f(A) ← length(A,0)

Magic hypotheses allow us to represent the hypothesis space more compactly
and to reason about the set of all instantiations of a magic hypothesis simul-
taneously. For instance, the magic hypothesis H above represents concisely all
its instantiations, including I1 and I2, amongst many other ones. The only
instantiation of a non-magic hypothesis is itself.

In practice, we are not interested in all instantiations of a magic hypothesis,
but only in a subset of relevant instantiations. In the following, we consider a
magic evaluation procedure which only considers instantiations that, together
with the background knowledge, entail at least one positive example. We show
we can ignore other instantiations.

4.3 Constraints

To improve learning performance, we prune the hypothesis space with con-
straints [7]. Given our hypothesis representation, each constraint prunes a set
of magic hypotheses, each of which represents the set of its instantiations. In
other words, for each magic hypothesis pruned, we eliminate all its instanti-
ations. We identify constraints that are optimally sound in that they do not
eliminate optimal solutions from the hypothesis space. Speciﬁcally, we consider
extensions of specialisation, generalisation, and redundancy constraints for
magic hypotheses. We describe them in turn. The proofs are in the appendix.

4.3.1 Extended Specialisation Constraint

We ﬁrst extend specialisation constraints. If all the instantiations of a magic
hypothesis, together with the background knowledge, entail at least one posi-
tive example and are incomplete, then all specialisations of this hypothesis are
incomplete:

Proposition 1 (Extended specialisation constraint) Let (E +, E −, B, H , C ) be a LFF
input, H1 ∈ HC , and H2 ∈ HC be two magic hypotheses such that H1 (cid:22) H2. If
all instantiation I1 of H1 such that \e ∈ E +, B ∪ I1 |= e are incomplete, then all
instantiation of H2 also are incomplete.

We provide an example to illustrate this proposition.

Example 3 (Extended specialisation constraint) Consider the dyadic predicate head
which takes as input a list and returns its ﬁrst element. Consider the following positive
examples E + and the magic hypothesis H0:

E + = {f ([b, a]), f ([c, a, e])}

12

4 MAGIC EVALUATION

H0: f(A) ← head(A,B), @magic(B)

This hypothesis holds for lists whose ﬁrst element is a particular constant symbol
to be determined. This hypothesis H0 has the following two instantiations I0,1 and
I0,2 covering at least one positive example:

I0,1: f(A) ← head(A,b)
I0,2: f(A) ← head(A,c)

The ﬁrst instantiation I0,1 holds for lists whose head is the element b. This instan-
tiation covers the ﬁrst positive example. The second instantiation I0,2 holds for lists
whose head is the element c. It covers the second positive example. However, each of
these instantiations is incomplete and too speciﬁc. Therefore, no instantiation of H0
can entail all the positive examples. As such, all specialisations of H0 can be pruned,
including magic hypotheses such as H1 and H2:

H1: f(A) ← head(A,B), @magic(B), odd(B)
H2: f(A) ← head(A,B), @magic(B), tail(A,C), head(C,D), @magic(D)

4.3.2 Extended Generalisation Constraint

We now extend generalisation constraints. If all the instantiations of a magic
hypothesis together with the background knowledge entail at least one positive
example are inconsistent, then we can prune non-recursive generalisations of
this hypothesis and they are either inconsistent or non-optimal:

Proposition 2 (Extended generalisation constraint) Let (E +, E −, B, H , C ) be a LFF
input, H1 ∈ HC and H2 ∈ HC be two magic hypotheses such that H2 is non-recursive
and H2 (cid:22) H1. If all instantiation I1 of H1 such that \e ∈ E +, B ∪ I1 |= e are
inconsistent, then all instantiations of H2 are inconsistent or non-optimal.

We illustrate generalisation constraints with the following example and give a
counter-example to explain why non-recursive hypotheses cannot be pruned.

Example 4 (Extended generalisation constraint) Consider the following positive
examples E +, the negative examples E − and the magic hypothesis H0:

E + = {f ([b, a]), f ([c, a, e])}
E − = {f ([b]), f ([c])}

H0: f(A) ← head(A,B), @magic(B)
This hypothesis H0 has the following two instantiations I0,1 and I0,2 covering at least
one of these positive examples:

I0,1: f(A) ← head(A,b)
I0,2: f(A) ← head(A,c)
The ﬁrst instantiation I0,1 holds for lists whose head is the element b. This instan-
tiation covers the ﬁrst positive example and the ﬁrst negative example. The second
instantiation I0,2 holds for lists whose head is the element c. It covers the second
positive example and the second negative example. Each of these instantiations is
inconsistent and thus is too general. As such, all non-recursive generalisations of H0
can be pruned. In particular, the magic hypotheses H1 and H2 below are non-recursive

4.3 Constraints

13

generalisations of H0 and can be pruned:

H1 :

H2 :

(cid:26)

(cid:26)

f(A) ← head(A,B), @magic(B)
f(A) ← length(A,B), @magic(B)

f(A) ← head(A,B), @magic(B)
f(A) ← head(A,B), negative(B)

However, there might exist other instantiations of H0 which do not cover any positive
examples but are not inconsistent, such as I0,3:

This instantiation could be used to construct a recursive solution, such as I :

I0,3: f(A) ← head(A,a)

f(A) ← head(A,a)
f(A) ← tail(A,B), f(B)

I :

(cid:26)

The instantiation I holds for list which contain the element a at any position.

4.3.3 Extended Redundancy Constraint

We extend redundancy constraints for magic hypotheses. If a magic hypothesis
has no instantiations which, together with the background knowledge, entail
at least one positive example, we show that it is redundant when included in
any non-recursive hypothesis.

Proposition 3 (Extended redundancy constraint) Let (E +, E −, B, H , C ) be a LFF
input, H1 ∈ HC be a magic hypothesis. If H1 has no instantiation I1 such that
\e ∈ E +, B ∪ I1 |= e, then all non-recursive magic hypotheses H2 which contain a
specialisation of H1 as a subset are non-optimal.

We illustrate this proposition with the following example and provide a
counter-example to explain why non-recursive hypotheses cannot be pruned.

Example 5 (Extended redundancy constraint) Consider the following positive exam-
ples E + and the magic hypothesis H0:

E + = {f ([b, c]), f ([f , g , c])}

H0: f(A) ← head(A,B), @magic(B), tail(A,C),empty(C)
This hypothesis H0 holds for lists which contain a single element which is a particular
constant to be determined. However, both examples have length 2. Therefore, among
the possible instantiations of the hypothesis H0, there are no instantiations which,
together with the background knowledge, cover at least one positive example. H0
cannot entail any of the positive examples and is redundant when included in a non-
recursive hypothesis. As such, all hypotheses which contain a specialisation of H0
as a subset are non-optimal. In particular, the magic hypotheses H1 and H2 below
contain a specialisation of H0 as a subset. They are non-optimal and can be pruned:

H1 :

(cid:26)

f(A) ← head(A,B), @magic(B), odd(B), tail(A,C), empty(C)
f(A) ← length(A,B), @magic(B)

14

4 MAGIC EVALUATION

H2 :

(cid:26)

f(A) ← head(A,B), @magic(B), tail(A,C),empty(C), length(A,D), odd(D)
f(A) ← head(A,B), negative(B)

However, there might exist other instantiations of H0 which are not redundant in
recursive hypotheses. For instance, the following recursive instantiated hypothesis I
includes an instantiation of H0 as a subset but may not be non-optimal:

f(A) ← head(A,c), tail(A,C),empty(C)
f(A) ← tail(A,B), f(B)

I :

(cid:26)

This instantiation holds for lists whose last element is the constant c. It covers both
positive examples but none of the negative examples.

While extended specialisation constraints are sound, extended redundancy
and generalisation constraints are only optimally sound. They might prune
solutions from the hypothesis space but do not prune optimal solutions.

4.3.4 Constraint summary

We summarise our constraint framework as follows. Given a magic hypothesis
H , the learner can infer the following extended constraints under the following
conditions:

1. If all instantiations of H which, together with the background knowledge,
entail at least one positive example are incomplete, according to Proposition
1, we can prune all its specialisations.

2. If all

instantiations of H which, together with the background knowl-
edge, entail at least one positive example are inconsistent, according to
Proposition 2, we can prune all its non-recursive generalisations.

3. If the magic hypothesis H has no instantiation which, together with the
background knowledge, entail at least one positive example, according to
Proposition 3, we can prune all non-recursive hypotheses which contain one
of its specialisations as a subset.

While Proposition 1 can prune recursive hypotheses, Proposition 2 and Propo-
sition 3 do not prune recursive hypotheses. Therefore, pruning is stronger when
recursion is disabled.
We have described our representation of the hypothesis space with magic
hypotheses. We have extended specialisation, generalisation, and redundancy
constraints to prune magic hypotheses and we have demonstrated these
extended constraints are optimally sound. The next section theoretically eval-
uates the gain over the size of the search space of using magic hypotheses
compared to identifying constant symbols with unary predicates.

4.4 Theoretical Analysis

Our representation includes magic hypotheses which contain magic variables.
Each magic variable stands for the set of its substitutions. Therefore, we do
not enumerate constant symbols in the hypothesis space by opposition with
existing approach. Our experiments focus on comparing MagicPopper with

15

approaches which enumerate possible constant symbols with unary body pred-
icates. We focus in this section on theoretically evaluating the reduction over
the hypothesis space size of not enumerating all candidate constant symbols
as unary predicates, and instead using magic variables.

Proposition 4 Let Db be the number of body predicates available in the search
space, m be the maximum number of body literals allowed in a clause, c the num-
ber of constant symbols available, and n the maximum number of clauses allowed in
a hypothesis. Then the maximum number of hypotheses in the hypothesis space can
be multiplied by a factor of ( Db +c
)mn if representing constants with unary predicate
Db
symbols, one per allowed constant symbol, compared to using magic variables.

A proof of Proposition 4 is in the appendix. Proposition 4 shows that allowing
magic variables can reduce the size of the hypothesis space compared to enu-
merating constant symbols through unary predicate symbols. The ratio is a
increasing function of the number of constant symbols available and the com-
plexity of hypotheses, measured as the number of clauses allowed in hypotheses
n and the number of body literals allowed in clauses m. Similar analysis can be
conducted for approaches which enumerate constant symbols in the arguments
of clauses. More generally, Proposition 4 suggests that enumerating constant
symbols can increase the size of the hypothesis space compared to using magic
variables.

5 Implementation

We now describe MagicPopper, which implements our magic evaluation idea.
We ﬁrst describe Popper, on which MagicPopper is based.

5.1 Popper

Popper [7] is a LFF learner. It takes as input a LFF input, which contains
a set of positive (E +) and negative (E −) examples, background knowledge
(B), a bound over the size of hypotheses allowed in H , and a set of hypothe-
ses constraints (C ). Popper represents hypotheses in a meta-language L.
This meta-language L contains literals head literal/4 and body literal/4 rep-
resenting head and body literals respectively. These literals have arguments
(Clause,Pred,Arity,Vars) and denote that there is a head or body literal in
the clause Clause, with the predicate symbol Pred, arity Arity, and variables
Vars. For instance, the following set of literals:

{head literal(0,empty,1,(0)), body literal(0,length,2,(0,1)),
body literal(0,zero,1,(1))}

represents the following clause with id 0:

empty(A) ← length(A,B), zero(B)
To generate hypotheses, Popper uses an ASP program P whose models are
hypothesis solutions represented in the meta-language L. In other words, each

16

5 IMPLEMENTATION

head literal(C,P,A,Vars):-

clause(C),
head pred(P,A),
vars(A,Vars).

0{body literal(C,P,A,Vars): clause(C),body pred(P,A),vars(A,Vars)}N :-
max body(N).

size(N):-

max size(MaxSize),
N = 1..MaxSize,
#sum{K+1,C : body size(C,K)} == N.

var(0..N-1):- max vars(N).
vars(1,(Var1,)):- var(Var1).
vars(2,(Var1,Var2)):- var(Var1),var(Var2).
vars(3,(Var1,Var2,Var3)):- var(Var1),var(Var2),var(Var3).

0 {magic literal(C,Vars): clause(C),vars(1,Vars)} M:- max magic(M).

Fig. 5 Simpliﬁed Popper ASP base program for single clause programs. There is exactly
one head literal per clause. There are at most N body literals per clause, where N is a user-
provided parameter describing the maximum number of body literals allowed in a clause.
Our modiﬁcation is highlighted in bold: we allow at most N variables to be magic variables,
where N is a user-provided parameter.

model (answer set) of P represents a hypothesis. A simpliﬁed version of the
base ASP program (without the predicate declarations which are problem spe-
ciﬁc) is represented in Figure 5. Popper uses a generate, test, and constrain
loop to ﬁnd a solution. First, it generates a hypothesis as a solution to the ASP
program P with the ASP system Clingo [38]. Popper searches for hypothe-
ses by increasing size, the size being evaluated as the number of literals in a
hypothesis. Popper tests this hypothesis against the examples, typically using
Prolog. If the hypothesis is a solution, it is returned. Otherwise, the hypoth-
esis is a failure: Popper identiﬁes the kind of failure and builds constraints
accordingly. For instance, if the hypothesis is inconsistent (entails a negative
example) Popper builds a generalisation constraint. Popper adds these con-
straints to the ASP program P to constrain the subsequent generate steps.
This loop repeats until a hypothesis solution is found or until there are no
more models to the ASP program P .

5.2 MagicPopper

MagicPopper builds on Popper to support magic evaluation. MagicPop-
per likewise follows a generate, test, and constrain loop to ﬁnd a solution. We
describe in turn how each of these steps works.

5.2 MagicPopper

17

Predicate
head pred(f,1).
body pred(cell,4).
body pred(distance,3).

Type
(state)
(state,pos,color,type)
(pos,pos,int)

Directions
(in)
(in,out,out,out)
(in,in,out)

Fig. 6 Predicate declarations for the krk task. The task is to learn a hypothesis to describe
that the white king protects the white rook in the chess endgame king-rook-king.

Generate
Figure 5 shows our modiﬁcation to Popper’s base ASP encoding in bold.
In addition to head literal/4 and body literal/4, MagicPopper can express
magic literal/2. Magic literals have arguments (Clause,Var) and denote that
the variable Var in the clause Clause is a magic variable. There can be at most
M magic literal in a clause, where M is a user deﬁned parameter with default
value 4. This setting expresses the trade-oﬀ between search complexity and
expressivity.

In addition to the standard Popper input, and a maximum number of
magic values per clause, MagicPopper can receive information about which
variables can be magic variables. This information can be provided with three
diﬀerent settings: Arguments, Types, and All. For instance, given the predicate
declarations represented in Figure 6, Figure 7 illustrates how the user can pro-
vide additional bias with each of these settings. A user can specify individually
a list of some arguments of some predicates (Arguments) or a list of variable
types (Types). Otherwise, if no information is given, MagicPopper treats
any variable as a potential magic variable (All ). For any of these settings,
MagicPopper searches for a subset of the variables speciﬁed by the user for
the magic variables. Therefore, All always considers a larger hypothesis space
than Arguments and Types. Arguments is the setting closest to mode decla-
rations [3, 8, 27, 31]. Mode declarations however impose a stricter bias: while
Arguments treats the ﬂagged arguments as potential magic values, mode dec-
larations specify an exact list of arguments which must be constant symbols2.
With the All setting, MagicPopper can automatically identify which vari-
able to treat as magic variables at the expense of more search. In Section 6.7,
we experimentally evaluate the impact on learning performance of these dif-
ferent settings. In Section 6.8, we evaluate the impact on learning performance
of allowing magic values (All setting) when it is unnecessary.

The output of the generate step is a hypothesis which may contain magic
variables, such as the one shown on the right of Figure 4. By contrast, most
ILP approaches [3, 6, 7] cannot generate hypotheses with magic variables but
instead require enumerating constant symbols. Popper and δ-ILP use unary
predicates to represent constant symbols, as shown on the left of Figure 4.
Aspal precomputes all possible rules with some arguments grounded to con-
stant symbols. Conversely, owing to the use of magic variables, MagicPopper
beneﬁts from a more compact representation of the hypothesis space.

2Forcing variables to be magic variables is not a setting currently available in MagicPopper.

18

5 IMPLEMENTATION

Setting

Arguments

Types

Bias
cell, 3
distance, 3
integer
type

Example hypothesis

f(A)← cell(A,B,C,D),cell(A,E,F,D),distance(B,E,H)

f(A)← cell(A,B,C,D),cell(A,E,C,G),distance(B,E,H)

All

f(A)← cell(A,B,C,D),cell(A,E,F,G),distance(B,E,H)
Fig. 7 Example of the diﬀerent bias settings for MagicPopper. Variables that can be magic
variables are represented in bold. Arguments can treat as magic variables some speciﬁed
arguments of speciﬁed predicates. Type can treat as a magic variable any variable of the
speciﬁed types. All expects no additional information and may treat any variable as a magic
variable.

Test

Magic evaluation is executed during the test step. To identify substitutions
for magic variables, we add magic variables as new head arguments. We
execute the resulting program on the positive examples. We save the substitu-
tions for the new head variables. We then bound these substitutions to their
corresponding magic variables and remove the additional head arguments.

Example 6 (Magic evaluation) Consider the magic hypothesis H1 below:

We add magic variables as new head variables. H thus becomes H ′
1.

H1: f(A) ← length(A,B), @magic(B)

H ′

1: f(A,B) ← length(A,B), @magic(B)

We execute H ′
1 on the positive examples to ﬁnd substitutions for the magic variable
B. Assume the single positive example f ([a, b, c]). We transform it into f ([a, b, c], B)
and we ﬁnd the substitution 3 for the variable B. We bind this value to the magic
variable in the hypothesis, which results in the following instantiation:

H ′

1: f(A,B) ← length(A,3)

Example 7 (Magic evaluation of recursive hypothesis) Similarly, the recursive
hypothesis H2 below becomes H ′
2.
f(A) ← length(A,B), @magic(B)
f(A) ← head(A,B), @magic(B), tail(A,C), f(C)

H2 :

(cid:26)

H ′
2 :

f(A,B,D) ← length(A,B), @magic(B)
f(A,B,D) ← head(A,D), @magic(D), tail(A,C), f(C,B,D)

2 on the positive examples to ﬁnd substitutions for the magic variables

(cid:26)
We execute H ′
B and D .

With this procedure, MagicPopper only identiﬁes constants which can be
obtained from the positive examples. In this sense, MagicPopper does not
consider irrelevant constant symbols.

8

examples
instantiations) Given
Example
E + = {f ([a, e]), f ([])}, we consider only the two instantiations I1,1 and I1,2 for the
magic hypothesis H1:

(Relevant

positive

the

H1: f(A) ← length(A,B), @magic(B)

I1,1: f(A) ← length(A,2)
I1,2: f(A) ← length(A,0)

19

We use Prolog to execute programs because of its ability to use lists and
handle large, potentially inﬁnite, domains. As a consequence of using Prolog,
our reasoning to deduce candidate magic values is based on backward chaining,
in contrast to systems that rely on forward chaining [3, 4, 6, 22].

A limitation of the aforementioned approach is the execution time of
learned programs to identify all possible bindings. This approach is especially
expensive when a hypothesis contains multiple magic variables, in which case
one must consider the combinations of their possible bindings.

Example 9 (Execution time complexity) Consider the hypothesis H :

H :

(cid:26)

f(A) ← member(A,B1),@magic(B1)
f(A) ← member(A,B2),@magic(B2)

The hypothesis H is the disjunction of two clauses, each of which contains one magic
value, respectively B1 and B2. Since B1 and B2 can be bound to diﬀerent constant
symbols, this hypothesis is allowed in the search space despite having two clauses
with the exact same literals. More generally, we allow identical clauses with magic
variables.

This hypothesis means that any of two particular elements appears in a list. We
search for substitutions for the magic variables B1 and B2. We call n the size of input
lists. The number of substitutions for the magic variable B1 in the ﬁrst clause is
O (n). Similarly, the number of substitutions for the magic variable B2 in the second
clause is O (n). Therefore, the number of instantiations for H is O (n2).

Constrain
If MagicPopper identiﬁes that a hypothesis has no instantiation, no com-
plete instantiation, or no consistent instantiation, it generates constraints as
explained in Section 4.3.4. Additionally, MagicPopper generates a banish
constraint if no other constraints can be inferred. The banish constraint prunes
this single hypothesis from the hypothesis space. In other words, it ensures
that the same hypothesis will not be generated again in subsequent gener-
ate steps [7]. These constraints prune the hypothesis space and constrain the
following iterations.

6 Experiments

We now evaluate our approach.

6.1 Experimental design

Our main claim is that MagicPopper can improve learning performance
compared to current ILP systems when learning programs with magic values.
Our experiments, therefore, aim to answer the question:

20

6 EXPERIMENTS

Q1 How well does MagicPopper perform compared to other approaches?

To answer Q1, we compare MagicPopper against Metagol, Aleph, and
Popper3. MagicPopper uses diﬀerent biases than Metagol and Aleph.
Therefore a direct comparison is diﬃcult and our results should be interpreted
as indicative only. By contrast, as MagicPopper is based on Popper, the
comparison against Popper is more controlled. The experimental diﬀerence
between the two is the addition of our magic evaluation procedure and the use
of extended constraints.

A key limitation of approaches that enumerate all possible constants
allowed in a rule [3, 6, 7] is diﬃculty learning programs from inﬁnite domains.
By contrast, we claim that MagicPopper can learn in inﬁnite domains.
Therefore, our experiments aim to answer the question:

Q2 Can MagicPopper learn in inﬁnite domains?

To answer Q2, we consider several tasks in inﬁnite and continuous domains
that require magic values as real numbers or integers.

Proposition 4 shows that our magic evaluation procedure can reduce the
search space and thus improve learning performance compared to using unary
body predicates. We thus claim that MagicPopper can improve scalability
compared to Popper. To explore this claim, our experiments aim to answer
the question:

Q3 How well does MagicPopper scale?

To answer Q3, we vary the number of (i) constant symbols in the background
knowledge, (ii) magic values in the target hypotheses, and (iii) training exam-
ples. We use as baseline Popper. We compare our experimental results with
our theoretical analysis from Section 4.4.

Unlike existing approaches, MagicPopper does not need to be told which
variables may be magic variables but can automatically identify this informa-
tion. However, it can use this information if provided by a user. To evaluate
the importance of this additional information, our experiments aim to answer
the question:

Q4 What eﬀect does additional bias about magic variables have on the learning

performance of MagicPopper?

To investigate Q4, we compare diﬀerent settings for MagicPopper, each of
which assumes diﬀerent information regarding which variables may be magic
variables. We use as baseline Popper.

Our approach should improve learning performance when learning pro-
grams with magic values. However, in practical applications, it is unknown
whether magic values are necessary. To evaluate the cost in performance when
magic values are unnecessary, our experiments aim to answer the question:

3We also considered ASPAL [3]. However, as it precomputes every possible rule in a hypothesis,

it does not scale to our experimental domains.

6.1 Experimental design

21

Q5 What eﬀect does allowing magic values have on the learning performance

when magic values are unnecessary?

To answer Q5, we compare the learning performance of MagicPopper
and Popper on problems that should not require magic values. We set
MagicPopper to allow any variable to potentially be a magic value.

6.1.1 Experimental settings

Given p positive and n negative examples, t p true positives and t n true nega-

tives, we deﬁne the predictive accuracy as
. We measure mean predictive
accuracies, mean learning times, and standard errors of the mean over 10
repetitions. We use an 8-Core 3.2 GHz Apple M1 and a single CPU4.

( t p
p + t n
n )
2

6.1.2 Systems settings

Aleph.
Aleph is allowed constant symbols through the mode declarations or lazy
evaluation.

Metagol.
Metagol needs as input second-order clauses called metarules. We provide
Metagol with a set of almost universal metarules for a singleton-free fragment
of monadic and dyadic Datalog [39] and additional curry metarules to identify
constant symbols as existentially quantiﬁed ﬁrst-order variables.

Popper and MagicPopper.
Both systems use Popper 2.0.0 (also known as Popper+) [40]. We provide
Popper with one unary predicate symbol for each constant symbol available
in the background knowledge. We set for both systems the same parameters
bounding the search space (maximum number of variables and maximum num-
ber of literals in the body of clauses). Therefore, since MagicPopper does not
count magic literals in program sizes, it considers a larger search space than
Popper. We provide both systems with types for predicate symbols. In par-
ticular, unary predicates provided to Popper are typed. Therefore, to ensure
a fair comparison, we provide MagicPopper with a list of types to describe
the set of variables which may be magic variables. As explained in Section
5.2, we could have instead provided MagicPopper with a list of arguments
of particular predicate symbols to describe the set of variables which may be
magic variables. Providing a list of predicate arguments would have been a
setting closer to mode declarations, which Aleph uses. However, when speci-
fying types for magic variables, the search space is larger than when specifying
particular arguments of some predicates symbols. Moreover, our setting spec-
iﬁes which variables can be magic variables, and MagicPopper searches for

4The experimental data and code for

reproducing the experiments are available at

https://github.com/celinehocquette/magicpopper.git

22

6 EXPERIMENTS

next val(A,5) ← does(A,player,press button)
next val(A,B) ← does(A,player,noop), true val(A,C), succ(B,C)

Fig. 8 Example solution for the IGGP md next task. This hypothesis states that the value
becomes 5 when the player presses the button, and is the true value minus 1 if the player
does not act. Magic values are represented in bold.

next(A,q)← my true(A,q),does(A,robot,a)
next(A,p)← my true(A,q),does(A,robot,b)
next(A,q)← my true(A,r),does(A,robot,c)
next(A,r)← my true(A,r),does(A,robot,a)
next(A,r)← my true(A,r),does(A,robot,b)
next(A,q)← my true(A,p),does(A,robot,b)
next(A,p)← my true(A,p),does(A,robot,c)
next(A,r)← my true(A,q),does(A,robot,c)
next(A,B)← my true(A,C),my succ(C,B)
next(A,p)← not my true(A,B),does(A,robot,a)

Fig. 9 Example solution for the IGGP buttons-next task. The ﬁrst clause states that the
next value becomes q if the current value is q and the agent presses the button a. Magic
values are represented in bold.

a subset of these variables. Conversely, modes specify which variables must
be constant symbols. In this sense, this setting for MagicPopper considers
a larger hypothesis space than Aleph. In Section 6.7, we evaluate and com-
pare the eﬀect on learning performance of these diﬀerent settings for specifying
magic variables.

6.2 Q1: comparison with other systems

6.2.1 Experimental domains

We compare MagicPopper against state-of-the-art ILP systems. This exper-
iment aims to answer Q1. We consider several domains. Full descriptions of
these domains are in the appendix. We use a timeout of 600s for each task.

IGGP.

In inductive general game playing (IGGP) [41], agents are given game traces
from the general game playing competition [42]. The task is to induce a set of
game rules that could have produced these traces. We use four IGGP games
which contain constant symbols: md (minimal decay), buttons, coins, and gt-
centipede. We learn the next relation in each game, the goal relation for buttons,
coins, gt-centipede and the legal relation for gt-centipede. These tasks involve
the identiﬁcation of respectively 5, 31, 3, 14, 6, 4, 29 and 8 magic values.
Figures 8 and 9 represent examples of some target hypotheses. We measure
balanced accuracies and learning times.

6.2 Q1: comparison with other systems

23

f(A)← cell(A,E,white,rook),cell(A,B,white,king),distance(E,B,1)

Fig. 10 Example solution for the krk task. This hypothesis describes the concept of rook
protected in the chess krk endgame. This hypothesis states that the white king protects the
white rook when the white king and the white rook are at distance 1 of each other. Magic
values are represented in bold.

KRK.

The task is to learn a chess pattern in the king-rook-king (krk ) endgame, which
is the chess ending with white having a king and a rook and black having a
king. We learn the concept of rook protection by its king [43]. An example
target solution is presented in Figure 10. This task involves identifying 4 magic
values.

Program synthesis: list, powerof2 and append.

For list, we learn a hypothesis describing the existence of the magic number ’7’
in a list. Figure 2 in the introduction shows an example solution. For powerof2,
we learn a hypothesis which describes whether a number is of the form 2k ,
with k integer. These two problems involve learning a recursive hypothesis.
For append, we learn that lists must have a particular suﬃx of size 2. For list,
there are 4000 constants in the background knowledge. Examples are lists of
size 500. For powerof2, examples are numbers between 2 and 1000, there are
1000 constants in the background knowledge. For append, examples are lists
of size 10, there are 1000 constants in the background knowledge.

6.2.2 Results

Table 1 shows the learning times. It shows MagicPopper can solve each
of the tasks in at most 100s, often a few seconds. To put these results into
perspective, an approach that precomputes the hypothesis space [3] would
need to precompute at least (#preds#constants)#literals rules. For instance,
for buttons-next, this approach would need to precompute at least (5 ∗ 16)10 =
O (1019) rules, which is infeasible. Conversely, MagicPopper solves this task
in 3 seconds.

Popper is based on enumeration of possible constant symbols: it uses
unary predicate symbols, one for each possible constant symbol. Compared
to Popper, MagicPopper has shorter learning times on seven tasks (md,
buttons-goal, coins-goal, gt-centipede-goal, gt-centipede-legal, gt-centipede-next,
krk, list, powerof2 and append ) and longer learning times on two tasks (buttons-
next and coins-next ). A paired t-test conﬁrms the signiﬁcance of the diﬀerence
for these ten tasks at the p < 0.01 level. For instance, MagicPopper can
solve the krk problem in 6s while Popper requires almost 35s.

There are three main reasons for this improvement. First, MagicPopper
reasons about magic hypotheses while Popper cannot. Each magic hypothe-
sis represents the set of its possible instantiations, which alleviates the need
to enumerate all possible constant symbols. The constraints MagicPopper

24

6 EXPERIMENTS

formulates eliminate magic hypotheses, which prunes more instantiated pro-
grams. Second, compared to Popper, MagicPopper does not need additional
unary predicates to represent constant symbols. This feature allows Mag-
icPopper to learn shorter hypotheses with constant symbols as arguments
instead. For instance, in the krk experiment, MagicPopper typically learns a
hypothesis with 3 body literals while Popper typically needs 6 body literals,
including 3 body literals to represent constant symbols. Popper thus needs to
search up to a larger depth compared to MagicPopper. As demonstrated by
Proposition 4, these two reasons lead to a smaller hypothesis space. Finally,
MagicPopper tests hypotheses against the positive examples and only con-
siders instantiations which, together with the background knowledge, entail
at least one positive example. In this sense, MagicPopper never considers
irrelevant constant symbols. For these three reasons, MagicPopper considers
fewer hypotheses which explains the shorter learning times.

However, given a search bound, MagicPopper searches a larger space
than Popper since it does not count the magic literals in the program size.
MagicPopper considers the same programs as Popper, but also programs
with magic values whose size would exceed the search bound if representing
magic values with unary predicate symbols. Therefore, MagicPopper can
require longer running time than Popper, which is the case for two tasks
(buttons-next and coins-next ).

Aleph restricts the possible constants to constants appearing in the bot-
tom clause, which is the logically most-speciﬁc clause that explains an example.
Aleph also can identify constant symbols through a lazy evaluation proce-
dure [9], which has inspired our magic evaluation procedure. Therefore, Aleph
does not consider irrelevant constant symbols but only symbols that can be
obtained from the examples. Compared to Aleph, MagicPopper has shorter
learning times on four tasks (buttons-next, coins-next, list, append ). A paired t-
test conﬁrms the signiﬁcance of the diﬀerence in learning times for these tasks
at the p < 0.01 level. However, in contrast to Aleph, MagicPopper searches
for optimal solutions. Moreover, MagicPopper is given a weaker bias about
which variables can be magic variables.

Metagol identiﬁes relevant constant symbols by constructing a proof for
the positive examples. Therefore, it also considers only relevant constant sym-
bols that can be obtained from the examples. Compared to MagicPopper,
Metagol has longer learning times on 6 tasks, similar learning times on 3
tasks, and better learning time on three tasks.

Table 2 shows the predictive accuracies. MagicPopper achieves higher or
equal accuracies than Metagol, Aleph, and Popper, apart on gt-centipede-
goal. This improvement can be explained by the fact that MagicPopper can
learn in domains other systems cannot handle. For instance, MagicPopper
supports learning with predicate symbols of arity more than two, which is
necessary for the IGGP games and the krk domain. By contrast, Metagol
cannot learn hypotheses with arity greater than 2 given the set of metarules
provided. Compared to MagicPopper, Aleph struggles to learn recursive

6.3 Q2: inﬁnite domains

25

hypotheses. However, Aleph performs well on the tasks which do not require
recursion, reaching similar or better accuracy than MagicPopper on seven
tasks (md, buttons-goal, gt-centipede-goal, gt-centipede-legal, gt-centipede-next
krk, and append ). Finally, compared to Popper, MagicPopper can achieve
higher accuracies. For instance, on the list problem, MagicPopper reaches
100% accuracy while Popper achieves the default accuracy. Since it does not
enumerate constant symbols, MagicPopper can search a smaller space than
Popper, and thus its learning time can be shorter. Therefore, it is more likely
to ﬁnd a solution before timeout. Also, according to the Blumer bound [44],
given two hypotheses spaces of diﬀerent sizes, searching the smaller space can
result in higher predictive accuracy compared to searching the larger one if a
target hypothesis is in both.

Given these results, we can positively answer Q1 and conﬁrm that Mag-
icPopper can outperform existing approaches in terms of learning times and
predictive accuracies when learning programs with magic values.

Task

Aleph Metagol Popper MagicPopper

md
buttons-next
coins-next
buttons-goal
coins-goal
gt-centipede-goal
gt-centipede-legal
gt-centipede-next
krk
list
powerof2
append

pi
equilibrium
drug design
next
sumk

0 ± 0
32 ± 1
timeout
0 ± 0
0 ± 0
0 ± 0
0 ± 0
0 ± 0
0 ± 0
66 ± 1
0 ± 0
1 ± 0

4 ± 1
0 ± 0
5 ± 1
0 ± 0
0 ± 0

timeout
timeout
0 ± 0
0 ± 0
0 ± 0
0 ± 0
0 ± 0
timeout
541 ± 60
36 ± 8
463 ± 78
0 ± 0

0 ± 0
0 ± 0
timeout
timeout
timeout

1 ± 0
3 ± 0
53 ± 0
1 ± 0
0 ± 0
23 ± 0
4 ± 0
10 ± 0
35 ± 6
timeout
18 ± 0
298 ± 49

timeout
209 ± 7
1 ± 0
1 ± 0
0 ± 0

0 ± 0
4 ± 0
99 ± 1
0 ± 0
0 ± 0
6 ± 0
1 ± 0
0 ± 0
6 ± 0
2 ± 0
0 ± 0
0 ± 0

1 ± 0
72 ± 17
6 ± 3
25 ± 0
99 ± 1

Table 1 Learning times. We round times to the nearest second. The error is standard
deviation. Tasks above the horizontal line have ﬁnite domains while tasks below the
horizontal line have inﬁnite constant domains.

6.3 Q2: learning in inﬁnite domains

We evaluate the performance of MagicPopper in inﬁnite domains and com-
pare it against the performance of Popper, Aleph, and Metagol. This
experiment aims to answer Q1 and Q2. We consider ﬁve tasks. Full descrip-
tions are in the appendix. We use a timeout of 600s for each of these
tasks.

26

6 EXPERIMENTS

Task

Aleph Metagol Popper MagicPopper

md
buttons-next
coins-next
buttons-goal
coins-goal
gt-centipede-goal
gt-centipede-legal
gt-centipede-next
krk
list
powerof2
append

pi
equilibrium
drug design
next
sumk

100 ± 0
81 ± 0
50 ± 0
100 ± 0
50 ± 0
99 ± 0
100 ± 0
100 ± 0
100 ± 0
50 ± 0
86 ± 1
95 ± 1

100 ± 0
100 ± 0
63 ± 7
50 ± 0
50 ± 0

50 ± 0
50 ± 0
50 ± 0
50 ± 0
50 ± 0
50 ± 0
50 ± 0
50 ± 0
54 ± 4
100 ± 0
58 ± 5
99 ± 0

50 ± 0
50 ± 0
50 ± 0
50 ± 0
50 ± 0

100 ± 0
100 ± 0
100 ± 0
98 ± 1
100 ± 0
75 ± 0
100 ± 0
100 ± 0
96 ± 1
49 ± 0
84 ± 1
96 ± 1

50 ± 0
62 ± 1
50 ± 0
49 ± 0
50 ± 0

100 ± 0
100 ± 0
100 ± 0
100 ± 0
100 ± 0
75 ± 0
100 ± 0
100 ± 0
99 ± 0
100 ± 0
100 ± 0
96 ± 1

99 ± 0
86 ± 7
98 ± 0
100 ± 0
100 ± 0

Table 2 Predictive accuracies. We round to the closest integer. The error is standard
deviation. Tasks above the horizontal line have ﬁnite domains while tasks below the
horizontal line have inﬁnite constant domains.

area(A,B) ← square(A,C),mult(C,3.142,B).
Fig. 11 Example solution for the pi task. The magic constant pi is represented in bold.

equilibrium(A) ← mass(A,B),forces(A,C),sum(C,D),mult(B,9.807,D)
Fig. 12 Example solution for the equilibrium task. The gravitational constant g is
represented in bold.

6.3.1 Experimental domains

Learning Pi.

The goal of this task is to learn a mathematical equation over real numbers
expressing the relation between the radius of a disk and its area.

This task involves identifying the magic value pi up to ﬂoating-point
precision. We allow a precision error of 10−3. Figure 11 shows an example
solution.

Equilibrium.

The task is to identify a relation describing mechanical equilibrium for an
object subject to its weight and other forces whose values are known. This task
involves identifying the gravitational constant g up to ﬂoating-point precision.
We allow a precision error of 10−3. Figure 12 shows an example of the target
hypothesis.

6.3 Q2: inﬁnite domains

27

drug(A) ← atom(A,B),atom(A,C),atom type(B,o), atom type(C,h),
distance(B,C,0.513)

Fig. 13 Example solution for the drug design task. Magic values for atom types and an
example of magic value for the distance are represented in bold.

next(A,B) ← head(A,4.543), tail(A,C) head(C,B)
next(A,B) ← tail(A,C),next(C,B)

Fig. 14 Example solution for the next element task. An example of magic constant is
represented in bold.

sumk(A) ← member(A,B), member(A,C), add(B,C,612)

Fig. 15 Example solution for the sumk task. An example of magic constant is represented
in bold.

Drug design.

The goal of this task is to identify molecule properties representing suitable
medicinal activity. An example is a molecule which is represented by the atoms
it contains and the pairwise distance between these atoms. Atoms have varying
types. Figure 13 shows an example solution. This task involves identifying
two magic values representing the particular atom types “o” and “h” and one
magic value representing a speciﬁc distance between two atoms.

Program Synthesis: next and sumk.

For next, we learn a hypothesis for identifying the element following a magic
value in a list. For example, given the magic value 4.543, we may have the pos-
itive example next([1.246, 4.543, 2.156],2.156). Figure 14 shows an example
solution. Examples are lists of size 500 of ﬂoat numbers. For sumk, we learn
a relation describing that two elements of a list have a sum equal to k , where
k is an integer magic value. Examples are lists of size 50 of integer numbers.
Figure 15 shows an example of target hypothesis.

6.3.2 Results

Tables 1 and 2 show the results. They show that, compared to Popper, Mag-
icPopper achieves higher accuracy5. Popper cannot identify hypotheses with
magic values in inﬁnite domains because it cannot represent an inﬁnite number
of constant symbols. Thus, it achieves the default accuracy. Metagol can-
not learn hypotheses with arity greater than 2 given the metarules provided
and therefore struggles on these tasks. It also struggles when the proof length
is large, such as when examples are lists of large size. Aleph, through the
use of lazy evaluation, performs well on the tasks which do not require recur-
sion, especially pi and equilibrium. However, it struggles on next and sumk
which both require recursion. The learning time of MagicPopper is better
than that’s of Aleph on one of the two tasks Aleph can solve, but worse on

5MagicPopper does not always achieve maximal accuracy due to ﬂoating-point precision errors.

28

6 EXPERIMENTS

the other. However, in contrast to Aleph, MagicPopper searches for opti-
mal hypotheses. Moreover, MagicPopper searches a larger search space since
it is given as bias the types of variables which can be magic variables while
Aleph is given the arguments of some predicate symbols through the mode
declarations.

These results demonstrate that MagicPopper can identify magic values
in inﬁnite domains. These results conﬁrm our answer to Q1. Also, we positively
answer Q2.

6.4 Q3: scalability with respect to the number of

constant symbols

We now evaluate how well our approach scales. First, we evaluate how well
our approach scales with the number of constant symbols. To do so, we need
domains in which we can control the number of constant symbols. We consider
two domains: list and md. In the list experiment, described in Section 6.2.1,
we use an increasingly larger set of constant symbols disjoint from {7} in the
background knowledge. In the md experiment, also described in Section 6.2.1,
we vary the number of next values available. We use a timeout of 60s for each
task. Full details are in the appendix.

6.4.1 Results

Figures 16 and 18 show the learning times of Popper, MagicPopper, Aleph,
and Metagol versus the number of constant symbols. These results show
that MagicPopper has a signiﬁcantly shorter learning time than Popper.
Popper needs a unary predicate symbol in the background knowledge for
each constant symbol, thus the search space grows with the number of con-
stant symbols. Moreover, Popper considers individually and exhaustively each
of the candidate constant symbols. Therefore, Popper cannot scale to large
background knowledge including a large number of constant symbols. It is
overwhelmed by 800 constant symbols in the list domain and 200 constant
symbols in the md domain, and it systematically reaches timeout after. By
contrast, MagicPopper does not consider every constant symbol but only
relevant ones which can be identiﬁed from executing the hypotheses on the
examples. Thus, it can scale better and can learn from domains with more
than 3 million constant symbols. This result supports Proposition 4, which
demonstrated that allowing magic variables can reduce the size of the hypoth-
esis space compared to adding unary predicate symbols and that the diﬀerence
in the size of the search spaces increases with the number of constant symbols
available in the background knowledge.

Figures 17 and 19 show the predictive accuracy of Popper, MagicPop-
per, Aleph, and Metagol versus the number of constant symbols. Popper
rapidly converges to the default accuracy (50%) since it reaches timeout.
Conversely, MagicPopper constantly achieves maximal accuracy and out-
performs all other systems. In the md domain, negative examples must be

6.4 Q3: scalability

29

Popper

MagicPopper

Aleph

Metagol

)
s
(

e
m

i
t

i

g
n
n
r
a
e
L

101

100

100

80

60

)

%

(

y
c
a
r
u
c
c
A

102

103

104

105

106

Number of constant symbols

102

103

104

105

106

Number of constant symbols

Fig. 16 List: learning time versus the
number of constant symbols. Axes are log
scaled.

Fig. 17 List: accuracy versus the num-
ber of constant symbols. The horizontal
axis is log scaled.

)
s
(

e
m

i
t

i

g
n
n
r
a
e
L

101

100

100

80

60

)

%

(

y
c
a
r
u
c
c
A

102

103

104

105

106

Number of constant symbols

102

103

104

105

106

Number of constant symbols

Fig. 18 Md:
learning time versus the
number of constant symbols. Axes are log
scaled.

Fig. 19 Md: accuracy versus the number
of constant symbols. The horizontal axis
is log scaled.

sampled from a large number of constant symbols, which also can explain the
drops in accuracy. Aleph struggles to learn recursive programs which explains
its low predictive accuracy in the list domain. Moreover, Aleph is based on
the construction of a bottom clause. The bottom clause can grow very large
in both domains when the number of constant symbols augments, which can
overwhelm the search. Metagol can learn programs with constant symbols
using the curry metarules. It performs well and scales to a large number of
constant symbols in the list experiment. However, the metarules provided are
not expressive enough to support learning with higher-arity predicates, which
in particular prevents Metagol from learning a solution for md for any of
the numbers of constants tested.

These results conﬁrm our answer to Q1. They also show that the answer to
Q3 is that MagicPopper can scale well with the number of constant symbols,
up to millions of constant symbols.

30

6 EXPERIMENTS

f(A) ← member(A,1),member(A,2),member(A,3)

Fig. 20 Example solution. Examples of magic values are represented in bold.

6.5 Q3: scalability with respect to the number of magic

values

To evaluate scalability with respect to the number of magic values, we vary the
number of magic values within the target hypothesis. We vary the number of
magic values along two dimensions (i) the number of magic values within one
clause, and (ii) the number of magic values in diﬀerent independent clauses.

6.5.1 Magic values in one clause

We ﬁrst evaluate scalability with respect to the number of magic values in the
same clause. We learn hypotheses of the form presented in Figure 20, where
the number of body literals varies. There are 100 constants in the background
knowledge. Lists have size 100. We use a timeout of 60s for each task. Full
experimental details are in the appendix.

Results

Figures 21 and 22 show the learning times and predictive accuracies. These
results show that, for a small number of magic values, MagicPopper achieves
shorter learning times than Popper. This results in higher predictive accu-
racies since Popper might not ﬁnd a solution before timeout. From 3 magic
values, both systems reach timeout and their performance is similar. When
increasing the number of magic values, the number of body literals increases
and more search is needed. In particular, Popper requires twice as many body
literals compared to MagicPopper, as it needs unary predicates to repre-
sent constant symbols. MagicPopper evaluates magic values within the same
clause jointly. For each positive example, it considers the cartesian product of
their possible values. The complexity is of the order O (n k ), where n is the size
of lists and k is the number of magic values. The complexity is exponential in
the number of magic values, which limits scalability when increasing the num-
ber of magic values. These results show that MagicPopper can scale as well
as Popper with respect to the number of magic values in the same clause, thus
answering Q3. However, scalability is limited for both systems. More gener-
ally, scalability with respect to the number of magic values is limited for large
inseparable programs, such as programs with several magic values in the same
clause or in recursive clauses with the same head predicate symbol.

6.5.2 Magic values in multiple clauses

We now evaluate scalability with respect to the number of magic values in
diﬀerent independent clauses. We learn hypotheses of the form presented in
Figure 23, where the number of clauses varies. There are 500 constants in the
background knowledge. Lists have size 500. Each clause is independent. We use
a timeout of 60s for each task. Full experimental details are in the appendix.

6.5 Q3: scalability

)
s
(

e
m

i
t

i

g
n
n
r
a
e
L

60

40

20

0

100

80

60

y
c
a
r
u
c
c
A

MagicPopper
Popper

31

MagicPopper
Popper

1

2

3

4

1

2

3

4

Number of magic values

Number of magic values

Fig. 21 Same clause: learning time ver-
sus the number of magic values.

Fig. 22 Same clause: accuracy versus
the number of magic values.

f(A) ← head(A,1).
f(A) ← head(A,2).
f(A) ← head(A,3).
f(A) ← head(A,4).
f(A) ← head(A,5).
f(A) ← head(A,6).

Fig. 23 Example solution. Magic values are represented in bold.

Results

Figure 24 shows the learning times. The accuracy is maximal for both sys-
tems for any of the numbers of magic values tested. This result shows that
MagicPopper and Popper both can handle a large number of magic values
in diﬀerent clauses, up to at least 70. Moreover, MagicPopper signiﬁcantly
outperforms Popper in terms of learning times. For instance, MagicPop-
per can learn a hypothesis with 50 magic values in 50 diﬀerent clauses in
about 2s, while Popper requires 14s. This result shows that MagicPopper
can scale well, in particular better than Popper, with respect to the num-
ber of magic values in diﬀerent clauses, thus answering Q3. As the number of
magic values increases, the target hypothesis has more clauses. Both systems
must consider an increasingly larger number of programs to test. However,
MagicPopper considers magic programs and only considers instantiations
which cover at least one example, which is more eﬃcient than enumerating all
possible instantiations.

We use a version of Popper [40] which learns non-separable programs
independently and then combines them. This strategy is eﬃcient to learn dis-
junctions of independent clauses, which explains the diﬀerence in scale from
the previous experiment. For non-separable hypotheses, MagicPopper must
evaluate magic variables jointly as described in the previous experiment.

32

)
s
(

e
m

i
t

i

g
n
n
r
a
e
L

15

10

5

6 EXPERIMENTS

Popper

MagicPopper

10 20 30 40 50 60 70 80 90

Number of magic values

Fig. 24 Multiple clauses: learning time
versus the number of magic values.

60

50

40

30

20

10

)
s
(

e
m

i
t

i

g
n
n
r
a
e
L

y
c
a
r
u
c
c
A

100

90

80

70

60

50

0

2,000 4,000 6,000 8,000 10,000

0

2,000 4,000 6,000 8,000 10,000

Number of examples

Number of examples

Fig. 25 Learning time versus the num-
ber of examples

Fig. 26 Accuracy versus the number of
examples

6.6 Q3: scalability with respect to the number of

examples

This experiment aims to evaluate how well MagicPopper scales with the
number of examples. We learn the same hypothesis as in Section 6.4. This task
involves learning a recursive hypothesis to identify a magic value in a list. We
compare Popper and MagicPopper. We use the same material and methods
as in Section 6.4. We vary the number of examples: for n between 1 and 3000,
we sample n positive examples and n negative ones. Lists have size at most
50, and there are 200 constant symbols in the background knowledge. We use
a timeout of 60s for each task.

6.6.1 Results

Figures 25 and 26 show the results. They show both MagicPopper and Pop-
per can learn with up to thousands of examples. However, MagicPopper
reaches timeout from 4000 examples while Popper reaches timeout from 9000
examples. Their accuracy consequently drops to the default accuracy from
these points respectively. This result shows that MagicPopper has worse

6.7 Q4: bias

33

scalability than Popper with respect to the number of examples, thus answer-
ing Q3. For both Popper and MagicPopper, we observe a linear increase in
the learning time with the number of examples. When increasing the number
of examples, executing the candidate hypotheses over the examples takes more
time. In particular, MagicPopper searches for substitutions for the magic
variables which cover at least one positive example. Therefore potentially more
bindings for magic variables can be identiﬁed. Then, more bindings are tried
out over the remaining examples as the number of examples increases. Mag-
icPopper eventually needs to consider every constant symbol as a candidate
constant. Moreover, since MagicPopper does not take in account the magic
literals into the program size, it can consider a larger number of programs
with constant symbols than Popper for any given program size bound, which
also explains how its learning time increases faster than the learning time of
Popper. This result highlights one limitation of MagicPopper.

6.7 Q4: eﬀect of the bias about magic variables

In contrast to mode-directed approaches [3, 8, 9], MagicPopper does not
need to be provided as input which variables should be magic variables but
instead can automatically identify them. It can, however, use this additional
information if given as input. We investigate the impact of this additional bias
on learning performance and thus aim to answer Q4.

6.7.1 Material and Methods

We consider the domains presented in Section 6.2.1. We compare three variants
of MagicPopper:

All: we allow any variable to potentially be a magic variable.
Types: we allow any variable of types manually chosen to potentially be a
magic variable. For instance, for md, we allow any variable of type agent, action
and int to potentially be a magic variable.
Arguments: we manually specify a list of arguments of some predicates sym-
bols that can potentially be magic variables. For instance, for md, we ﬂag the
second argument of next and the second and third arguments of does.

Arguments is most closely related to mode declarations approaches, which
expect a speciﬁcation for each argument of each predicate. However, the spec-
iﬁcations of Arguments are more ﬂexible since MagicPopper considers the
ﬂagged variables as potential magic variables and searches for a subset of
these variables to bind to constant symbols. By contrast, mode declarations
are stricter and specify exactly which arguments must be constants. Types is
comparable to Popper, which is provided with types for the unary predicates
in our experiments. Types, Arguments and mode declarations require a user
to specify some information about which variables can be bound to constant
symbols.

34

6 EXPERIMENTS

The variables which may be a magic variable in Arguments are a subset of
those of Types, which themselves are a subset of those of All. In this sense, the
search space is increasingly larger. We compare learning times and predictive
accuracies for each of these systems. We provide learning times of Popper as
a baseline. We use a timeout of 600s per task.

Task

All

Types

Arguments Popper

md
buttons-next
coins-next
buttons-goal
coins-goal
gt-centipede-goal
gt-centipede-legal
gt-centipede-next
krk
list
powerof2
append

0 ± 0
6 ± 0
timeout
0 ± 0
0 ± 0
8 ± 0
2 ± 0
0 ± 0
30 ± 2
timeout
0 ± 0
0 ± 0

0 ± 0
4 ± 0
139 ± 11
0 ± 0
0 ± 0
6 ± 0
1 ± 0
0 ± 0
8 ± 1
2 ± 0
0 ± 0
0 ± 0

0 ± 0
2 ± 0
97 ± 5
0 ± 0
0 ± 0
3 ± 0
0 ± 0
0 ± 0
7 ± 0
1 ± 0
0 ± 0
0 ± 0

1 ± 0
3 ± 0
80 ± 12
1 ± 0
0 ± 0
23 ± 0
4 ± 0
10 ± 0
40 ± 6
timeout
18 ± 0
262 ± 43

Table 3 Learning times. We round times to the nearest second. The error is standard
deviation.

Task

All

Types

Arguments Popper

md
buttons-next
coins-next
buttons-goal
coins-goal
gt-centipede-goal
gt-centipede-legal
gt-centipede-next
krk
list
powerof2
append

100 ± 0
98 ± 0
92 ± 1
100 ± 0
96 ± 0
82 ± 0
100 ± 0
100 ± 0
98 ± 0
50 ± 0
100 ± 0
95 ± 1

100 ± 0
100 ± 0
100 ± 0
100 ± 0
100 ± 0
75 ± 0
100 ± 0
100 ± 0
98 ± 0
100 ± 0
100 ± 0
95 ± 1

100 ± 0
100 ± 0
100 ± 0
100 ± 0
100 ± 0
75 ± 0
100 ± 0
100 ± 0
98 ± 0
100 ± 0
100 ± 0
96 ± 1

100 ± 0
100 ± 0
100 ± 0
96 ± 1
100 ± 0
75 ± 0
100 ± 0
100 ± 0
98 ± 0
50 ± 0
84 ± 1
96 ± 1

Table 4 Predictive Accuracy. We round to the closest integer. The error is standard
deviation.

6.7.2 Results

Table 3 shows the learning times. These results show that in general, All
requires learning times longer than or equal to Types, which in turn requires
learning times longer than or equal to Arguments. For instance, All reaches
timeout on the task list, while Types and Arguments require respectively 2s
and 1s. In some experiments such as krk, Types and Arguments have equiva-
lent bias, because the arguments speciﬁed are the only arguments of the types

6.8 Q5: unnecessary magic values

35

speciﬁed. Popper is provided with types for the unary predicates in these
domains and thus rather is comparable with Types. Yet, Types outperforms
Popper in terms of learning times. This result can be explained by the fact
that Types is a variant of MagicPopper. As such, it considers magic hypothe-
ses which represent the set of their instantiations. Therefore, in contrast to
Popper, Types does not enumerate all possible candidate constants. More-
over, Types only considers instantiations which, together with the background
knowledge, entail at least one positive example, while Popper considers every
possible constant in the search space equally. Also, because Types does not
require additional unary predicates, it can express hypotheses more compactly
and can search up to a smaller depth. Popper can also achieve longer learn-
ing times than All whereas All searches a larger space. For instance, Popper
requires 18s to solve the task powerof2 while All solves it in less than 1s.

Table 4 shows the predictive accuracies. These results show All can achieve
lower predictive accuracies than Types and Arguments. For instance, All
reaches 92% accuracy on coins-next while Types and Arguments reach 100%
accuracy. There are two main reasons explaining this diﬀerence. First, All
has a more expressive language, and in particular can express more speciﬁc
hypotheses through the use of more constant symbols. It is thus more prone
to overﬁtting. Second, All searches a larger search space. It consequently
might not ﬁnd an optimal solution before timeout. Moreover, according to
the Blumer bound [44], searching a larger search space can result in lower
predictive accuracies.

We can conclude that MagicPopper can beneﬁt from additional bias
about which variables should be magic variables, and in particular it can
achieve better learning performance. We thus can positively answer Q4. This
experiment illustrates the impact of more bias. More bias can help reduce
the search space and thus improve learning performance. However, this bias
must be user provided. More generally, choosing an appropriate bias is a key
challenge in ILP [2].

6.8 Q5: eﬀect on learning performance for problems

which do not require magic values

Our approach can improve learning performance for problems which require
magic values. However, magic values are not always required and it is not
always known whether a good solution requires magic values. We investigate in
this experiment the impact on learning performance of unnecessarily allowing
magic values and thus aim to answer Q5.

6.8.1 Material and Methods

To answer Q5, we compare systems which allow constant symbols with systems
which disallow constant symbols. Since it is unknown which variables should be
constant symbols, we allow any variable to be a constant symbol. Therefore, we
use the All setting for MagicPopper. We call Alephc the version of Aleph

36

6 EXPERIMENTS

for which any argument is allowed to be a constant symbol, and Aleph
✄c the
version of Aleph which disallows constant symbols. At the end, we compare
MagicPopper with Popper and Alephc with Aleph
✄c. We use a timeout of
600s per task. We consider two diﬀerent domains.

Michalski trains.

The goal of these tasks is to ﬁnd a hypothesis that distinguishes eastbound and
westbound trains [45]. We use four increasingly complex tasks. There are 1000
examples but the distribution of positive and negative examples is diﬀerent
for each task. We randomly sample the examples and split them into 80/20
train/test partitions.

Program synthesis: evens, last, member, sorted.

We use the same material and methods as [7]. These problems all involve
learning recursive hypotheses.

6.8.2 Results

✄c respectively.

Tables 5 and 6 show the results. They show Popper always outperforms
MagicPopper and Aleph
✄c always outperforms Alephc in terms of learning
times. For instance, MagicPopper takes 7s when solving the trains1 task
while Popper solves it in 3s. Alephc reaches timeout on all trains tasks while
Aleph
✄c solves them in a few seconds. Because it searches a larger space, Mag-
icPopper and Alephc require longer learning times compared to Popper
and Aleph

This increase in learning time can reduce predictive accuracies since Mag-
icPopper or Alephc consequently might not ﬁnd a solution before timeout.
For instance, Alephc reaches timeout and thus achieves the default accu-
racy on the trains tasks while Aleph achieves maximal accuracy. Moreover,
MagicPopper and Alephc are more prone to overﬁtting. For instance, Mag-
icPopper learns overly speciﬁc hypotheses for last. Finally, according to the
Blumer bound [44], searching a larger space can result in lower predictive
accuracies. However, allowing constant symbols in programs provides better
expressivity, since more hypotheses can be formulated compared to disallow-
ing constant symbols. In particular, it might allow the learner to formulate
more accurate hypotheses. For instance, Alephc achieves better accuracies
than Aleph

✄c on member and sorted.

To conclude, these results show that allowing magic values when unnec-
essary can impair learning performance,
in particular learning times and
predictive accuracies, which answers Q5. Future work is needed to automat-
ically identify when magic values are necessary and which variables could be
magic values. More generally, identifying a suitable bias is a major challenge
in ILP [2].

37

Task

trains1
trains2
trains3
trains4
evens
last
member
sorted

Alephc Aleph✄c
1 ± 0
timeout
1 ± 0
timeout
2 ± 0
timeout
5 ± 0
timeout
0 ± 0
14 ± 1
0 ± 0
16 ± 1
0 ± 0
15 ± 1
0 ± 0
5 ± 1

MagicPopper Popper

7 ± 0
4 ± 0
31 ± 0
25 ± 0
5 ± 1
180 ± 91
0 ± 0
86 ± 24

3 ± 0
3 ± 0
24 ± 0
21 ± 0
1 ± 0
1 ± 0
0 ± 0
70 ± 58

Table 5 Learning times. We round times to the nearest second. The error is standard
deviation.

Task

trains1
trains2
trains3
trains4
evens
last
member
sorted

Alephc Aleph✄c
100 ± 0
50 ± 0
98 ± 2
50 ± 0
100 ± 0
50 ± 0
100 ± 0
50 ± 0
54 ± 4
51 ± 0
50 ± 0
50 ± 0
50 ± 0
53 ± 1
72 ± 3
75 ± 2

MagicPopper Popper

100 ± 0
99 ± 0
100 ± 0
100 ± 0
100 ± 0
85 ± 7
100 ± 0
90 ± 5

100 ± 0
99 ± 0
100 ± 0
100 ± 0
100 ± 0
100 ± 0
100 ± 0
97 ± 2

Table 6 Predictive Accuracy. We round to the closest integer. The error is standard
deviation.

7 Conclusion and Limitations

Learning programs with magic values is fundamental to many AI applications.
However, current program synthesis approaches rely on enumerating candi-
date constant symbols, which inhibits scalability and prohibits learning them
from continuous domains. To overcome this limitation, we have introduced
an ILP approach to eﬃciently learn programs with magic values from poten-
tially large or inﬁnite domains. Inspired by Aleph’s lazy evaluation procedure
[9], our approach builds partial hypotheses with variables in place of constant
symbols. Therefore, our approach does not enumerate all candidate constants
when constructing hypotheses. The particular constant symbols are identi-
ﬁed by executing the hypothesis on the examples. Thus, our approach only
considers relevant constant symbols which can be obtained from the exam-
ples. Our approach extends the LFF framework with constraints to prune
partial hypotheses, which each represent a set of instantiated hypotheses. For
these reasons, our approach can eﬃciently learn in large, potentially inﬁnite,
domains. Our experiments on several domains show that our approach can (i)
outperform state-of-the-art approaches and (ii) scale to domains with millions
of constant symbols and even inﬁnite ones, including continuous domains.

38

7 CONCLUSION AND LIMITATIONS

Limitations and Future Work

Noise.
In contrast to other ILP systems [27, 29, 31], MagicPopper cannot identify
magic values from noisy examples. Previous work [46] has extended LFF to
support learning from noisy examples by relaxing the completeness and consis-
tency conditions as well as hypotheses constraints applications. This extension
should be directly applicable to MagicPopper, which we will address as
future work.

Scalability.

To ﬁnd magic values in a same clause, our approach must search through
the cartesian product of each potential magic value. Therefore, its scalability
is limited when increasing the number of magic values in the same clause,
as shown in the experiment presented in Section 6.5.1. Also, MagicPopper
ﬁnds candidate constant symbols from executing hypotheses on the training
examples. As shown in Section 6.6, its scalability is limited when increasing
the number of examples.

Bias.
MagicPopper can be provided as bias which variables can be bound to
constant symbols, if this bias is known, or can automatically identify these
variables at the expense of more search. Our experiments presented in Sections
6.7 and 6.8 have shown that without this additional bias, learning performance
can be degraded, in particular learning times and predictive accuracies. Choos-
ing an appropriate bias more generally is a major issue with ILP systems [2].
As far as we are aware, no system can automatically identify suitable bias
which future work should address.

Numerical values.

Our magic value evaluation procedure identiﬁes bindings by executing the
hypothesis over each example independently. Therefore, it can only ﬁnd magic
values which value arises from single positive examples. In particular, it can-
not identify magic values for which multiple examples are required for their
evaluation. For example, it cannot identify parameters of linear or polynomial
equations in contrast to other ILP systems [9, 29]. Likewise, it cannot identify
values requiring numerical reasoning, such as identifying an optimal threshold
[9, 33]. For the same reason, our method cannot create new constant symbols
which are not part of the domain. To overcome this limitation, we plan to use
SMT solvers to identify magic values from reasoning from multiple examples,
positive and negative.

39

Declarations

This research was funded in whole, or in part, by the EPSRC grant Explainable
Drug Design and the EPSRC fellowship The Automatic Computer Scientist
(EP/V040340/1). For Open Access, the author has applied a CC BY public
copyright licence to any Author Accepted Manuscript version arising from
this submission. All data supporting this study is provided as supplementary
information accompanying this paper.

Acknowledgments

The authors thank H˚akan Kjellerstrand, Rolf Morel, and Oghenejokpeme
Orhobor for valuable feedback.

References

[1] Muggleton, S.H.: Inductive logic programming. New Generation Comput-

ing 8(4), 295–318 (1991). https://doi.org/10.1007/BF03037089

[2] Cropper, A., Dumancic, S.: Inductive logic programming at 30: A new
introduction. J. Artif. Intell. Res. 74, 765–850 (2022). https://doi.org/10.
1613/jair.1.13507

[3] Corapi, D., Russo, A., Lupu, E.: Inductive logic programming in answer
set programming. In: Inductive Logic Programming - 21st International
Conference, pp. 91–97 (2011)

[4] Kaminski, T., Eiter, T., Inoue, K.: Exploiting answer set programming
with external sources for meta-interpretive learning. Theory and Practice
of Logic Programming 18(3-4), 571–588 (2018). https://doi.org/10.1017/
S1471068418000261

[5] Raghothaman, M., Mendelson, J., Zhao, D., Naik, M., Scholz, B.:
Provenance-guided synthesis of datalog programs. Proceedings of the
ACM on Programming Languages 4(POPL), 1–27 (2019)

[6] Evans, R., Grefenstette, E.: Learning explanatory rules from noisy data.

Journal of Artiﬁcial Intelligence Research 61, 1–64 (2018)

[7] Cropper, A., Morel, R.: Learning programs by learning from failures.

Machine Learning 110(4), 801–856 (2021)

[8] Muggleton, S.H.: Inverse Entailment and Progol. New Generation Com-
put. 13(3&4), 245–286 (1995). https://doi.org/10.1007/BF03037227

[9] Srinivasan, A., Camacho, R.: Numerical reasoning with an ILP sys-
tem capable of lazy evaluation and customised search. The Journal

40

7 CONCLUSION AND LIMITATIONS

of Logic Programming 40(2), 185–213 (1999). https://doi.org/10.1016/
S0743-1066(99)00018-7

[10] Cropper, A., Morel, R.: Predicate invention by learning from failures.

arXiv preprint arXiv:2104.14426 (2021)

[11] Cropper, A.: Learning logic programs though divide, constrain, and con-
quer. In: Thirty-Sixth AAAI Conference on Artiﬁcial Intelligence, AAAI
2022, Thirty-Fourth Conference on Innovative Applications of Artiﬁ-
cial Intelligence, IAAI 2022, The Twelveth Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2022 Virtual Event, February
22 - March 1, 2022, pp. 6446–6453. AAAI Press, ??? (2022). https://ojs.
aaai.org/index.php/AAAI/article/view/20596

[12] Purgal, S.J., Cerna, D.M., Kaliszyk, C.: Learning higher-order logic pro-
grams from failures. In: IJCAI 2022, pp. 2726–2733 (2022). https://doi.
org/10.24963/ijcai.2022/378. https://doi.org/10.24963/ijcai.2022/378

[13] Langley, P., Bradshaw, G.L., Simon, H.A.: In: Michalski, R.S., Carbonell,
J.G., Mitchell, T.M. (eds.) Rediscovering Chemistry with the Bacon Sys-
tem, pp. 307–329. Springer, Berlin, Heidelberg (1983). https://doi.org/10.
1007/978-3-662-12405-5 10. https://doi.org/10.1007/978-3-662-12405-5
10

[14] Zytkow, J.M.: Combining many searches in the FAHRENHEIT discovery
system. In: Proceedings of the Fourth International Workshop on Machine
Learning, pp. 281–287 (1987). Elsevier

[15] Nordhausen, B., Langley, P.: A robust approach to numeric dis-
covery.
In: Porter, B., Mooney, R. (eds.) Machine Learning Pro-
ceedings 1990, pp. 411–418. Morgan Kaufmann, San Francisco (CA)
(1990). https://doi.org/10.1016/B978-1-55860-141-3.50052-3. https://
www.sciencedirect.com/science/article/pii/B9781558601413500523

[16] Falkenhainer, B.C., Michalski, R.S.: Integrating quantitative and quali-
tative discovery: the ABACUS system. Machine Learning 1(4), 367–401
(1986)

[17] Augusto, D.A., Barbosa, H.J.: Symbolic regression via genetic program-
ming. In: Proceedings. Vol. 1. Sixth Brazilian Symposium on Neural
Networks, pp. 173–178 (2000). IEEE

[18] Austel, V., Dash, S., Gunluk, O., Horesh, L., Liberti, L., Nannicini, G.,
Schieber, B.: Globally optimal symbolic regression. In: Interpretable ML,
Satellite Workshop of NIPS 2017 (2017)

[19] Shapiro, E.Y.: Algorithmic Program DeBugging, Cambridge, MA, USA

41

(1983)

[20] Si, X., Raghothaman, M., Heo, K., Naik, M.: Synthesizing datalog pro-
grams using numerical relaxation. In: 28th International Joint Conference
on Artiﬁcial Intelligence, IJCAI 2019, pp. 6117–6124 (2019). International
Joint Conferences on Artiﬁcial Intelligence

[21] Ellis, K., Wong, C., Nye, M., Sabl´e-Meyer, M., Morales, L., Hewitt, L.,
Cary, L., Solar-Lezama, A., Tenenbaum, J.B.: DreamCoder: Bootstrap-
ping Inductive Program Synthesis with Wake-Sleep Library Learning, pp.
835–850. Association for Computing Machinery, New York, NY, USA
(2021). https://doi.org/10.1145/3453483.3454080

[22] Evans, R., Hern´andez-Orallo, J., Welbl, J., Kohli, P., Sergot, M.: Making
sense of sensory input. Artiﬁcial Intelligence 293, 103438 (2021). https://
doi.org/10.1016/j.artint.2020.103438

[23] Hemberg, E., Kelly, J., O’Reilly, U.-M.: On domain knowledge and
novelty to improve program synthesis performance with grammatical evo-
lution. In: Proceedings of the Genetic and Evolutionary Computation
Conference, pp. 1039–1046 (2019)

[24] Kitzelmann, E.: Inductive programming: A survey of program synthesis
techniques. In: International Workshop on Approaches and Applications
of Inductive Programming, pp. 50–73 (2009). Springer

[25] Feng, Y., Martins, R., Bastani, O., Dillig, I.: Program synthesis using
conﬂict-driven learning. ACM SIGPLAN Notices 53(4), 420–435 (2018)

[26] Solar-Lezama, A.: The sketching approach to program synthesis. In: Asian
Symposium on Programming Languages and Systems, pp. 4–13 (2009).
Springer

[27] Srinivasan, A.: The ALEPH manual. Machine Learning at the Computing

Laboratory (2001)

[28] Srinivasan, A., Page, D., Camacho, R., King, R.: Quantitative phar-
macophore models with inductive logic programming. Machine learning
64(1), 65–90 (2006)

[29] Karaliˇc, A., Bratko, I.: First order regression. Machine learning 26(2),

147–176 (1997)

[30] Kramer, S.: Structural regression trees. In: AAAI/IAAI, Vol. 1, pp. 812–

819 (1996). Citeseer

[31] Blockeel, H., De Raedt, L.: Top-down induction of ﬁrst-order logical

42

7 CONCLUSION AND LIMITATIONS

decision trees. Artiﬁcial Intelligence 101(1-2), 285–297 (1998)

[32] Blockeel, H., De Raedt, L., Ramon, J.: Top-down induction of clustering

trees. In: ICML (1998)

[33] Blockeel, H., De Raedt, L.: Lookahead and discretization in ILP. In:
Lavraˇc, N., Dˇzeroski, S. (eds.) Inductive Logic Programming, pp. 77–84.
Springer, Berlin, Heidelberg (1997)

[34] Muggleton, S.H., Lin, D., Pahlavi, N., Tamaddoni-Nezhad, A.: Meta-
inference. Machine

interpretive learning: application to grammatical
Learning 94, 25–49 (2014)

[35] Cropper, A., Dumanˇci´c, S., Muggleton, S.H.: Turning 30: New ideas
in Inductive Logic Programming. In: Proceedings of the Twenty-Nineth
International Joint Conference on Artiﬁcial Intelligence, IJCAI, pp.
4833–4839 (2020)

[36] Lloyd, J.W.: Foundations of Logic Programming. Springer, ??? (2012)

[37] Muggleton, S.H., De Raedt, L.: Inductive logic programming: Theory
and methods. The Journal of Logic Programming 19-20, 629–679 (1994).
https://doi.org/10.1016/0743-1066(94)90035-3. Special Issue: Ten Years
of Logic Programming

[38] Gebser, M., Kaminski, R., Kaufmann, B., Schaub, T.: Clingo= ASP+
control: Preliminary report. arXiv preprint arXiv:1405.3694 (2014)

[39] Cropper, A., Tourret, S.: Logical reduction of metarules. Machine
Learning 109(7), 1323–1369 (2020). https://doi.org/10.1007/s10994-019-
05834-x

[40] Cropper, A.: Learning programs by combining programs. arXiv
(2022). https://doi.org/10.48550/ARXIV.2206.01614. https://arxiv.org/
abs/2206.01614

[41] Cropper, A., Evans, R., Law, M.: Inductive general game playing. Machine

Learning 109(7), 1393–1434 (2020)

[42] Genesereth, M., Bj¨ornsson, Y.: The international general game playing

competition. AI Magazine 34(2), 107–107 (2013)

[43] Hocquette, C., Muggleton, S.H.: Complete bottom-up predicate invention
in meta-interpretive learning. In: Proceedings of the 29th International
Joint Conference Artiﬁcial Intelligence, pp. 2312–2318 (2020)

[44] Blumer, A., Ehrenfeucht, A., Haussler, D., Warmuth, M.K.: Learnability
and the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM)

43

36(4), 929–965 (1989)

[45] Larson, J., Michalski, R.S.: Inductive inference of vl decision rules. ACM

SIGART Bulletin (63), 38–44 (1977)

[46] Wahlig, J.: Learning logic programs
abs/2201.03702 (2022) 2201.03702

from noisy failures. CoRR

Appendix A Proofs

A.1 Extended Constraints

We ﬁrst state three lemmas. These lemmas justify why we can restrict the
search for instantiations to instantiations which cover at least one positive
example.

Lemma 1 Let (E +, E −, B, H , C ) be an LFF input and H a magic hypothesis, if all
the instantiations I of H such that \e ∈ E +, B ∪ I |= e are incomplete, then all
instantiations of H are incomplete.

Proof 1 Instantiations I such that ]e ∈ E +, B ∪ I |= e are incomplete.

Lemma 2 Let (E +, E −, B, H , C ) be an LFF input and H a magic hypothesis, if all
the instantiations I of H such that \e ∈ E +, B ∪ I |= e are inconsistent, then all
instantiations of H are totally incomplete or inconsistent.

Proof 2 Instantiations I such that ]e ∈ E +, B ∪ I |= e are totally incomplete.

Lemma 3 Let (E +, E −, B, H , C ) be an LFF input and H a magic hypothesis, if H
has no instantiation I such that \e ∈ E +, B ∪ I |= e, then all instantiations of H are
totally incomplete.

Proof 3 Instantiations I such that ]e ∈ E +, B ∪ I |= e are totally incomplete.

We now introduce extensions of specialisation, generalisation, and redundancy
constraints which prune magic hypotheses. We use the three lemmas above to
prove these constraints are optimally sound.

Proposition 1 Let (E +, E −, B, H , C ) be an LFF input, H1 ∈ HC , and H2 ∈ HC
be two magic hypotheses such that H1 (cid:22) H2. If all instantiation I1 of H1 such that
\e ∈ E +, B ∪ I1 |= e are incomplete, then all instantiation of H2 also are incomplete.

Proof 4 All instantiations I1 of H1 such that \e ∈ E +, B ∪ I1 |= e are incomplete.
Therefore, according to Lemma 1, all instantiations of H1 are incomplete. Let H2 be

44

A PROOFS

a specialisation of H1. Let I2 be an instantiation of H2. I2 is a specialisation of an
instantiation I1 of H1. I1 is incomplete. Since subsumption implies entailment, then
I2 also is incomplete.

Proposition 2 Let (E +, E −, B, H , C ) be an LFF input, H1 ∈ HC and H2 ∈ HC be
two magic hypotheses such that H2 is non-recursive and H2 (cid:22) H1. If all instantiation
I1 of H1 such that \e ∈ E +, B ∪ I1 |= e are inconsistent, then all instantiations of H2
are inconsistent or non-optimal.

Proof 5 Let H2 be a non-recursive generalisation of H1. Let I2 be an instantiation
of H2. I2 is a generalisation of an instantiation I1 of H1. By assumption, all instan-
tiations I1 of H1 such that \e ∈ E +, B ∪ I1 |= e are inconsistent. Therefore, according
to Lemma 2, all instantiations of H1 are (1) inconsistent or (2) totally incomplete.
For (1), if I1 is inconsistent, then I2 also is inconsistent. For (2), assume I1 is
totally incomplete. Since I2 is a generalisation of I1 and is non-recursive, then I1 is
an independent subset of I2 of totally incomplete clauses. Therefore, I1 is a subset of
clauses which is redundant in I2 and thus I2 is non-optimal.

Proposition 3 Let (E +, E −, B, H , C ) be an LFF input, H1 ∈ HC be a magic hypothe-
sis. If H1 has no instantiation I1 such that \e ∈ E +, B ∪ I1 |= e, then all non-recursive
magic hypotheses H2 which contain a specialisation of H1 as a subset are non-optimal.

Proof 6 Let H2 be a non-recursive hypothesis which contains a specialisation of
H1 as a subset. Let I2 be an instantiation of H2. Then I2 contains as a subset a
specialisation of an instantiation I1 of H1. H1 has no instantiation I1 such that
\e ∈ E +, B ∪ I1 |= e, therefore, according to Lemma 3, all instantiations of H1 are
totally incomplete. Therefore, I1 is totally incomplete. Moreover, I2 is non-recursive
by assumption. Then I1 is an independent subset of redundant clauses in I2 and thus
I2 is non-optimal.

A.2 Theoretical Analysis

The following proposition evaluates the reduction over the hypothesis space of
using magic values instead of enumerating all candidate constant symbols as
unary predicates.

Proposition 4 Let Db be the number of body predicates available in the search
space, m be the maximum number of body literals allowed in a clause, c the num-
ber of constant symbols available, and n the maximum number of clauses allowed in
a hypothesis. Then the maximum number of hypotheses in the hypothesis space can
be multiplied by a factor of ( Db +c
)mn if representing constants with unary predicate
Db
symbols, one per allowed constant symbol, compared to using magic variables.

Proof 7 Let Let Dh be the number of head predicates in the search space, a be
the maximum arity of predicates and v be the maximum number of unique variables

45

next cell(A,B,C) ← my true cell(A,B,C),does jump(A,robot,F,D),
diﬀerent(B,F),diﬀerent(D,B)
next cell(A,B,twocoins)← does jump(A,E,F,D),diﬀerent(D,F),
does jump(A,E,F,B)
next cell(A,B,zerocoins) ← does jump(A,F,B,E),does jump(A,F,D,E),
diﬀerent(E,D)

Fig. B1 Example solution for the IGGP coins-next task. Magic values are represented in
bold.

allowed in a clause. From [7], the maximum number of hypotheses in the hypothesis
space is:

n

| Dh | v a

Õj =1 (cid:18)

|Db |v a
i

m
i =1
j
Í

(cid:0)

(cid:1)

(cid:19)

Given that 0 ≤ i <| Db | v a , we have:

| Db | v a
i

(cid:18)

(cid:19)

≤

(| Db | v a )i
i !

Therefore:

| Dh | v a

m

Õi =1 (cid:18)

| Db | v a
i

(cid:19)

<| Dh | v a m (| Db | v a )m

And similarly, we have:

n

| Dh | v a

Õj =1 (cid:18)

|Db |v a
i

m
i =1
j
Í

(cid:0)

(cid:1)

(cid:19)

≤ n (| Dh | v a m (| Db | v a )m )n

If adding one unary predicate symbol per constant symbol, then there are Db + c
body predicates available. Then, the maximum number of hypotheses in the hypothesis
space above becomes:

n

| Dh | v a

Õj =1 (cid:18)

| (Db +c) |v a
i

m
i =1
j

Í

(cid:0)

(cid:1)

(cid:19)

≤ n (| Dh | v a m (| (Db + c) | v a )m )n

Therefore, representing constants with magic variables can reduce the maximum size
of the hypothesis space by a factor of:

(

Db + c
Db

)mn

Appendix B Experiments

B.1 Domains

We describe the domains used in our experiments.

IGGP

Figures B1, B2, B3, B4, B5 and B6 represent some example solutions for these
tasks.

46

B EXPERIMENTS

goal(A,robot,100)← my true step(A,5)
goal(A,robot,0)← my true cell(A,D,onecoin),my true cell(A,D,onecoin)
Fig. B2 Example solution for the IGGP coins-goal task. Magic values are represented in
bold.

goal(A,robot,100) ← my true(A,p),my true(A,r),my true(A,q)
goal(A,robot,0) ← not my true(A,r)
goal(A,robot,0) ← not my true(A,q)
goal(A,robot,0) ← not my true(A,p)

Fig. B3 Example solution for the buttons-goal task. Magic values are represented in bold.

goal(A,B,0) ← true blackPayoﬀ(A,25),role(B).
goal(A,B,0) ← true blackPayoﬀ(A,35),role(B).
goal(A,B,0) ← true blackPayoﬀ(A,45),role(B).
goal(A,B,0) ← true whitePayoﬀ(A,0),role(B),true control(A,black)
goal(A,white,0) ← true control(A,white)
goal(A,black,0) ← true whitePayoﬀ(A,35)
goal(A,black,0) ← true control(A,white),true whitePayoﬀ(A,15)
goal(A,black,15) ← true whitePayoﬀ(A,0),true control(A,white)
goal(A,black,0) ← true whitePayoﬀ(A,5)
goal(A,black,10) ← true control(A,black),true whitePayoﬀ(A,15)
goal(A,white,C) ← true control(A,black),true blackPayoﬀ(A,D),

true whitePayoﬀ(A,C),my succ(D,C)

Fig. B4 Example solution for the gt-centipede-goal task. Magic values are represented in
bold.

KRK

We provide each system with the quadratic predicate cell and the triadic
predicate distance. For MagicPopper, any variable of type type, color or int
is allowed to be a magic value. Training sets contain 10 positive examples and
10 negative ones.

Program synthesis: list
Positive examples are generated by randomly choosing a position for the ele-
ment 7 and sampling the remaining elements from the constants available
in the background knowledge. Negative examples are generated by randomly
sampling elements from the constants in the background knowledge. Lists have
size 500. We generate 10 positive and 10 negative training examples. Therefore,
the default accuracy is 50%. We provide each learning system with the dyadic
predicates head, tail, length, last, geq, and the monadic predicate empty. For
MagicPopper, any variable of type element is allowed to be a magic value.

Program synthesis: powerof2

Figure B7 represents an example of target hypothesis. Positive examples are
powers of 2 between 2 and 210. Negative examples are numbers between 2 and

B.1 Domains

47

legal(A,B,ﬁnish) ← true control(A,B)
legal(A,black,noop) ← true control(A,white)
legal(A,white,noop) ← true control(A,black)
legal(A,B,continue) ← true control(A,B)

Fig. B5 Example solution for the gt-centipede-legal task. Magic values are represented in
bold.

next control(A,white) ← true control(A,black)
next control(A,black) ← true control(A,white)

Fig. B6 Example solution for the gt-centipede-next task. Magic values are represented in
bold.

multiple(1)
multiple(A) ← div(A,2,B),multiple(B)

Fig. B7 Example solution for the powerof2 task. Magic values are represented in bold.

210 which are not a power of 2. We sample 10 positive and 10 negative examples.
We provide each system with the triadic predicate div. For MagicPopper,
any variable is of type number and thus can be a magic variable.

Program synthesis: append

Examples are lists of size 10 of elements from the set of available constants. A
magic suﬃx of size 2 is randomly sampled from the set of constants. Positive
examples are lists ending with the magic suﬃx. Negative examples are lists
that do not end with the magic suﬃx. We sample 10 positive and 10 nega-
tive examples. We provide each system with the triadic predicate append, the
dyadic predicates head and tail. For MagicPopper, any variable is of type
list and thus can be a magic variable.

Learning Pi

An example is a pair radius / area. Radius are real numbers sampled between
0 and 10. We sample 20 training examples, half positive and half negative. We
provide each system with the triadic predicates add, subtract, multiply, divide
and the dyadic predicate square.

Equilibrium

An example is an object, its mass and the forces it is subject to are real
numbers sampled between 0 and 10. There are 7 forces applied to an object.
We sample 10 training examples, half positive and half negative. We provide
each system with the triadic predicates add, subtract, multiply, divide and
the dyadic predicates sum, square, mass and force. For MagicPopper, any
variable of type number is allowed to be a magic value.

48

Drug design

B EXPERIMENTS

Each molecule contains 10 atoms, pairwise distances are ﬂoats sampled
between 0 and 10. There are 10 diﬀerent atom types in the domain. A positive
example contains two atoms, one of type o and one of type h, separated by
the target distance. Target distances are randomly generated for each run. We
sample 20 training examples, half positive and half negative. We provide each
system with the triadic predicate distance and the dyadic predicates atom and
atom type. For MagicPopper, any variable of type type or number is allowed
to be a magic value.

Program Synthesis: next

The target magic value and list elements are real numbers chosen at random.
An example is a pair list/value where the value is an element of the list. We
provide each learning system with the dyadic background predicates head, tail,
length, last, geq and the monadic predicate empty. For MagicPopper, any
variable of type number is allowed to be a magic value. We sample 20 training
examples, half positive and half negative.

Program Synthesis: sumk

The target sum and list elements are integers chosen at random. We provide
each learning system with the dyadic background predicates member and tri-
adic predicate sum. For MagicPopper, any variable of type number is allowed
to be a magic value. We sample 20 training examples, half positive and half
negative.

B.2 Material and methods

We describe our experimental designs.

Minimal Decay
We generate an ordered set of constant symbols S of varying size. An example
is a state described by a true value from S , a next value from S and a player
action from {press button, noop}. True values and player actions are chosen at
random. For each positive example, a set of negative examples are generated
by taking a subset of size at most 1000 of the available constants as next value
for which the target theory does not hold. We generate 10 positive training
examples for each task.

Magic values in one clause
We generate 10 positive and 10 negative examples. For successive values of n,
we sample a set of n magic values. To generate a positive example, we assign
n diﬀerent position in the list, one for each of the magic values. Other values
in the list are chosen at random among the set of other available constant
symbols. To generate a negative example, we choose n − 1 magic values, which
we assign to n − 1 diﬀerent position in the list. Other values are chosen at

B.2 Material and methods

49

random among the set of other available constant symbols. We allow the dyadic
body predicate member in the background knowledge.

Magic values in multiple clauses
We sample a set S of constant symbols, among which we sample a subset M of
varying size representing target magic values. Examples are lists of elements
from S . A positive example is an example which head element is in M . We
generate 200 positive and 200 negative training examples. We check that each
training positive example set contains at least one example for each clause to
ensure the target is identiﬁable.

Program synthesis

We give each system the following dyadic relations head, tail, decrement, geq
and the monadic relations empty, zero, one, even, and odd.

