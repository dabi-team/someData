Neural Lyapunov Differentiable Predictive Control

Sayak Mukherjee, J´an Drgoˇna, Aaron Tuor, Mahantesh Halappanavar, Draguna Vrabie

2
2
0
2

y
a
M
2
2

]

Y
S
.
s
s
e
e
[

1
v
8
2
7
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract— We present a learning-based predictive control
methodology using the differentiable programming framework
with probabilistic Lyapunov-based stability guarantees. The
neural Lyapunov differentiable predictive control (NLDPC)
learns the policy by constructing a computational graph encom-
passing the system dynamics, state and input constraints, and
the necessary Lyapunov certiﬁcation constraints, and thereafter
using the automatic differentiation to update the neural policy
parameters. In conjunction, our approach jointly learns a
Lyapunov function that certiﬁes the regions of state-space with
stable dynamics. We also provide a sampling-based statistical
guarantee for the training of NLDPC from the distribution
of initial conditions. Our ofﬂine training approach provides a
computationally efﬁcient and scalable alternative to classical
explicit model predictive control solutions. We substantiate
the advantages of the proposed approach with simulations to
stabilize the double integrator model and on an example of
controlling an aircraft model.

Index Terms— Differentiable predictive control, model pre-

dictive control, neural policy, Lyapunov guarantee.

I. INTRODUCTION

The optimization-based solution to the model predictive
control (MPC) problems can become restrictive in some
real-life applications due to the computational burden of
computing implicit control policies by solving nonlinear
optimization programs. Explicit MPC (eMPC) provides an
alternative solution at the control inputs are pre-computed
to avoid online optimization for certain prespeciﬁed parame-
ters [1], [2]. However, the multiparametric optimization prob-
lem quickly becomes intractable for larger systems, thereby
limiting its practical usability. Therefore, in the last decade,
researchers have focused on reducing the computational
complexity of the eMPC problems [3], [4]. More recent
trends show promises in developing data-driven predictive
control solutions for the underlying explicit MPC problem.
Learning-based MPC (LBMPC) [5] methods utilizes the
learned system dynamic model from trajectory measure-
ments via adaptive MPC frameworks [6], [7]. Learning-based
approaches are also supplemented with various forms of
robustness conditions and capture state-dependent uncertain-
ties with work such as [8], with guarantees on bounded
uncertainties in [9], to name a few. On the other hand,
approximate MPC [10]–[14] utilizes a supervised learning
framework to imitate the MPC trajectories, which, although
efﬁcient in implementation, is bottlenecked by its inherent
dependency on the original MPC problem solutions. From
the computer science perspective, researchers have focused
on developing new data-driven model architectures inspired

Authors are with the Paciﬁc Northwest National Laboratory (PNNL),
Richland, WA, USA. Emails: { sayak.mukherjee, jan.drgona, aaron.tuor,
mahantesh.halappanavar, draguna.vrabie}@pnnl.gov.

by control theory. Work such as [15] have explored MPC-
inspired neural architectures for learning neural control poli-
cies. The use of convex neural networks [16] for optimal pre-
dictive control problems is investigated in [17]. While [18]
introduced implicit layers to solve constrained optimization
problems within deep neural networks leading to works such
as [19], [20]. Others [21], [22] have proposed the use of
automatic differentiation to compute the sensitivities of the
MPC problems for safe control in the context of imita-
tion learning. To provide guarantees,
[23] leverage linear
matrix inequality (LMI) framework for enforcing Lyapunov
stability conditions while learning neural control policies.
A semideﬁnite programming-based safety veriﬁcation and
robustness of neural network control policies have been
investigated in [24]. [25] presents adaptive safe learning
with Lyapunov provable safety certiﬁcate. [26] presents a
learner-falsiﬁer-based Lyapunov guarantee for learning neu-
ral policies for optimal control of known dynamics. An
alternative approach presented by [27], [28] provides safety
guarantees for reinforcement learning (RL) algorithms by
representing the control policy by an implicit differentiable
MPC. [29] proposes to use neural Lyapunov functions to
tune parameters of online implicit MPC. [30] presents a
system identiﬁcation methodology for nonlinear dynamics
to jointly learn a controller and corresponding stable closed-
loop dynamics.

Inspired by these trends,

in [31] we have introduced
the differentiable predictive control (DPC) framework as
a scalable data-driven alternative to analytical multipara-
metric programming solvers. DPC brings forth the idea of
ofﬂine computation of explicit predictive control policies by
leveraging automatic differentiation (AD) of the constrained
optimization problem for direct computation of the policy
gradients. In our previous work, we have demonstrated the
scalability of the DPC framework on systems,
including
uncertainties and nonlinear constraints [32], [33].
Contributions. This paper brings forth the idea of jointly
learning the Lyapunov function candidates to provide guaran-
tees in the DPC policy learning framework. We parametrize
the Lyapunov function using deep neural network archi-
tecture introduced in [34]. Then we solve the parametric
optimal control problem with such Lyapunov certiﬁcation
conditions as a constraint via the DPC policy optimiza-
tion algorithm [31]. The differentiable programming frame-
work constructs a computational graph of the closed-loop
parametrized system with neural predictive policy and learn-
able Lyapunov functions and then uses AD to backpropagate
resultant
loss using MPC objectives and soft constraint
penalties to update policy and Lyapunov network parameters.

 
 
 
 
 
 
Additionally, we provide a theoretical framework for guaran-
teeing stability and constraint satisfaction during the training,
where we sample initial conditions from a given distribution
and characterize the system trajectories based on such initial
conditions in a probabilistic manner. For such analysis, we
utilize Hoeffding’s inequality adapting from [35] which was
described in the context of unsupervised learning of the
constrained control policies. We perform detailed numerical
experiments on stabilizing the double integrator model and
controlling an aircraft model to bring out various intricacies
of our design.

The paper is organized as follows. The problem formu-
lation and necessary background is discussed in Section
II. The proposed method is described in Section III with
the necessary building blocks and algorithm. Section IV
describes the probabilistic guarantees ensuring stability and
constraint satisfaction with sampled initial conditions. Nu-
merical experiments are presented in detail in Section V, and
we provide concluding remarks in Section VI.

II. PROBLEM FORMULATION AND BACKGROUND

We consider the discrete-time non-linear dynamic system:

,

0

xk+1 = f (xk, uk), k ∈ NN −1
where xk ∈ Rnx is the system state, uk ∈ Rnu is the control
input at time k, and Nb
a = {a, a + 1, . . . , b} is a set of
integers. We assume the dynamics f (.) is locally Lipschitz
continuous and known. The system is also constrained by
state and control input based constraints.

(1)

A. Parametric Optimal Control Problem

We intend to solve the following parametric optimal

control problem:

min
θ

N −1
(cid:88)

k=0

(cid:96)(xk, uk),

,

s.t. xk+1 = f (xk, uk), k ∈ NN −1
0
[u0, . . . , uN −1] = πθ(x0),
h(xk) ≤ 0,
g(uk) ≤ 0,
x0 ∼ Px0 .

(2a)

(2b)

(2c)

(2d)

(2e)

(2f)

where xk ∈ Rnx , uk ∈ Rnu are states and control actions,
respectively, N deﬁnes ﬁnite time prediction horizon, and θ
are the parameters of the control policy (2c) to be optimized
over a distribution of initial conditions (2f).

We consider the objective (2a) to be a differentiable
function. We can consider quadratic objective functions with
state and control energy minimization such as:

(cid:96)(xk, uk, rk) = ||xk||2
Qx
Q = aT Qa being weighted squared 2-norm. We

+ ||uk||2

(3)

Qu

with (cid:107)a(cid:107)2
consider the following assumptions:

Assumption 1: The nominal system dynamics model (1) is
controllable, and known. The dynamics f (.) can be a learned
model or a physics based model.

Assumption 2: The parametric optimal control objective
(cid:96)(xk, uk) and constraints h(xk), and g(uk), respectively, are
at least once differentiable.

B. Neural Predictive Control Policy

Without the speciﬁc parametric policy structure as in (2c),
then the classical MPC solution provides an implicit solution
for the control inputs utilizing nonlinear programming based
solvers. On the contrary, we are interested in generating
explicit model predictive (eMPC) control policies with a
speciﬁc parametric control structure. For the system dynamic
model (1), we want to compute parametric predictive control
policies modeled by deep neural networks πθ(x0) : Rnx →
RN ×nu (4) that minimizes the parametric control objective
function (2a) for prediction window of N steps, while
satisfying the constraints. We consider the policy to be made
of fully connected neural networks such as:

U = πθ(x0) = WLzL + bL,

zl = σ(Wl−1zl−1 + bl−1),
z0 = x0,

(4a)

(4b)

(4c)

where U = [u0, . . . , uN −1] ∈ RN ×nu represents the
generated control sequence over N time steps. Vector x0 rep-
resents full state initial condition feedback, and subsequently
the vector xk provides the instantaneous state vector at time
step k. Considering the architecture of the policy class, zi
represents hidden states, Wi, and bi are the weights and
biases of the i-th layer, respectively. These policy parameters
are compactly represented as θ which we intend to optimize.
The nonlinearity σ : Rnzl → Rnzl
is constructed by
executing differentiable activation function σ : R → R in
an element-wise manner.

C. Stability via Neural Lyapunov Functions

We intend to provide Lyapunov stability certiﬁcate during
the training of the explicit neural predictive control policy
πθ(x0). The closed-loop control at
time step k can be
computed by taking the ﬁrst control action of the predictive
sequence denoted by uk = [πθ(xk)]0, however with slight
abuse of notation we continue to use the notation πθ(xk)
as our control input uk. The Lyapunov function V (xk) can
characterize the stability of the closed-loop dynamics as
follows.

Lemma 1: (Lyapunov Characterization) : Using the neural
predictive control policy uk = πθ(xk) for discrete time steps
k, with the Liptsitz continuous closed-loop f (xk, uk), for the
equilibrium xO = 0, if there exists a set D ⊂ Rn on which
the function V (.) is always positive-deﬁnite, and satisfy,

V (f (xk, uk)) − V (xk) < 0, ∀xk ∈ D − 0,

(5)

then xO = 0 is an asymptotically stable equilibrium, and
V (.) is a control Lyapunov function (CLF) for the closed-
loop dynamics.

We consider a parametrized closed-loop Lyapunov func-
the CLF, with deep neural network parameters φ,
tion,
denoted as Vφ(xk). As proposed in [36], we can consider

the following structure for the Lyapunov function candidate
based on input-convex neural network (ICNN):

z1 = σ0 (W0x + b0) ,
zi+1 = σi (Uizi + Wix + bi) , i = 1, . . . , k − 1,
g(x) ≡ zk,

(6)

(7)

(8)

which makes the function g(x) convex in x. The activations
σk(.) are are monotonically non decreasing convex nonlinear
scalar functions, such as the ReLU, Ui are positive weight
mappings, and other weights and biases are real-valued.
To make the candidate Lyapunov function positive deﬁnite
another compositional operation is performed as:

V (x) = σk+1(g(x) − g(0)) + (cid:15)(cid:107)x(cid:107)2
2,

(9)

where σk(.) is a positive convex non-decreasing function
with σk(0) = 0 and (cid:15) is a small constant. To make
the Lyapunov function continuously differentiable we can
consider smooth ReLU activation as mentioned in [36].
At this point, we state our problem statement as:

P. Learn the neural predictive control policy uk = πθ(xk)
for the parametric optimal control problem (2) along with
simultaneous Lyapunov certiﬁcation using the parametric
control Lyapunov function Vφ(xk) following Lemma 1 such
that the closed-loop f (xk, uk) dynamics remain Lyapunov-
stable along with solving for the desired optimal control
performance.

III. NEURAL LYAPUNOV DIFFERENTIABLE PREDICTIVE
CONTROL (NLDPC) FRAMEWORK

In differentiable predictive control, we train the explicit
predictive control policy by utilizing the system dynamics
f (.) to generate predictive trajectories in a self-supervised
way. We assume that the system’s state dynamics can be ob-
tained by performing system identiﬁcation or mathematical
modeling of the underlying physics. The system dynamics
and the neural predictive control architecture are combined
in a computational graph to create a differentiable closed-
loop representation. The parameters of the control policy are
updated via stochastic gradients descent by backpropagating
the gradients of the optimal control objective function and
constraints penalties. In this work, we expand the DPC
framework to allow for joint learning of neural predictive
control policies and neural Lyapunov functions enforcing
closed-loop stability of the controlled system. We ﬁrst dis-
cuss the penalty-based approach in dealing with the state,
control constraints, and the Lyapunov-based certiﬁcation
criterion.

A. Soft state, control, and Lyapunov constraints

The state and action constraints h(xk), g(xk), and the
control Lyapunov certiﬁcation criterion Vφ(xk) as shown
in Lemma 1 are modeled as soft constraints using the
ReLU activation function represented by px(xk), pu(uk),

and pV (xk+1, xk), respectively as follows:

px(xk) = ||ReLU(h(xk)||2
pu(uk) = ||ReLU(g(uk))||2

pV (xk+1, xk) = ||ReLU(Vφ(xk+1) − Vφ(xk))||2,

(10a)

(10b)

(10c)

with ReLU standing for rectiﬁer linear unit function. This
structural representation will help us to formulate the neural
Lyapunov DPC problem as described in the next sub-section.

B. Neural Lyapunov DPC Problem

To formulate the learning-based predictive control prob-
lem, we utilize the batched training strategy where we
generate multiple batches of randomly sampled initial con-
dition that will be used for forward pass roll outs of the
DPC computational graph. We sample the initial conditions
following user-deﬁned distribution D:

x0 ∼ D, x0 ∈ X ⊆ Rnx .

(11)

Putting all together, we formulate the neural Lyapunov
DPC as following sampled parametric optimal control prob-
lem:

min
θ,φ

1
mN

m
(cid:88)

N −1
(cid:88)

i=1

k=0

(cid:0)(cid:96)(xi

k, ui

k) + QV pV (xi

k+1, xi

k)+

s.t. xi

k)(cid:1),
k), k ∈ NN −1

k) + Qupu(ui
k, ui

Qxpx(xi
k+1 = f (xi
[u0, . . . , uN −1]i = πθ(xi
0 ∈ X ⊆ Rnx ∼ D.
xi

0
k),

,

(12a)

(12b)

(12c)

(12d)

here the index i denotes the batch sample. The DPC loss
function is made of multiple terms, i.e., the optimal control
: Rnx+nu → R, and penalties of
objectibe (cid:96)(xk, uk)
parametric constraints px(h(xk)) : Rnh → R, pu(g(uk)) :
Rng → R, and pV (xk+1, xk) : R → R. QV , Qx and Qu
represent the scalar penalty weights.

To utilize differentiable programming, we keep the follow-

ing assumption.

Assumption 3: The optimal parametric objective function
(cid:96)(xk, uk), and state, input and Lyapunov constraint penal-
ties px(xk), pu(uk), and pV (xk+1, xk) are differentiable at
almost every point in their domain.

C. Neural Lyapunov DPC Computational Graph

The formulation (12) is being implemented in the form of
a differentiable program for optimizing the neural param-
eters θ and φ for neural predictive policy and Lyapunov
function respectively. We construct the computation graph
for the differentiable program which optimizes the neural
parameters by differentiating the neural Lyapunov DPC loss
function (12a) and backpropagating the gradients through
the parametrized closed-loop dynamics given by the system
model (12b) and neural control policy (12c). Fig. 1 shows
the data ﬂow in the computational graph of the presented
neural Lyapunov DPC method. We have developed a generic
framework of output feedback structure where we can use an

Fig. 1: Neural Lyapunov DPC methodology.

estimator parametrized by deep neural networks to estimate
the initial condition from a past output and input trajectories.
However, for the simplicity of the exposition in this paper, we
consider a full state feedback. The policy component reads
the current states and generates predictive control trajectories
over the prediction horizon. Thereafter the initial state and
trajectories are used to generate a N -step ahead
control
rollout of the system dynamics obtaining state trajectories.
The generated trajectories are then passed through penalty-
based constraints including Lyapunov constraint (5).

D. Neural Lyapunov DPC Gradients

Finally, all these individual components described above
lead to the neural NLDPC loss function LNLDPC (12a). One
can use stochastic gradient descent and its variants to opti-
mize the loss function LNLDPC over the sampled distribution
of initial conditions (12d). The differentiable closed-loop
dynamics with the learnable policy, optimal control objective,
and constraints allow us to use automatic differentiation [37]
to directly compute policy gradients. As described before, the
problem (12) is represented as a computational graph, and
therefore, using chain rule, one can compute the gradients
of LNLDPC w.r.t. the policy parameters θ as follows:

∇θLNLDPC =

∂(cid:96)(x, u)
∂θ
∂px(x)
∂θ

+

=

+

+

∂pV (x)
∂θ
∂pu(u)
∂θ
∂x
∂pV (x)
∂u
∂x
∂pu(u)
∂u

(13)

,

+

+

+

∂u
∂θ

∂x
∂u

∂u
∂θ

∂u
∂θ
∂u
∂θ

∂(cid:96)(x, u)
∂x

∂(cid:96)(x, u)
∂u
∂px(x)
∂x

∂u
∂θ
∂x
∂u
where ∂u
∂θ represents partial derivatives of the neural policy
with respect to its weights, computed using backpropaga-
tion. Similarly, we can compute ∇φLNLDPC, i.e., gradients
with respect to neural Lyapunov function parameters. The
parametric optimal control problem (12) now can be solved
by using scalable stochastic gradient optimization algorithms
such as AdamW [38]. This provides us with a way to
optimize the policy ofﬂine. In practice, we use automatic
differentiation frameworks such as Pytorch [39] to compute
these gradients in the NLDPC computational graph.

E. Simultaneous Learning of Constrained Neural Control
Policy and Lyapunov Function

We now describe how to learn both a control policy and
a neural Lyapunov function together, so that the Lyapunov
conditions can be rigorously veriﬁed to ensure the stability
of the system. ∇θLNLDPC, and ∇φLNLDPC help us to train
these policies and neural Lyapunov function simultaneously
with batches of state trajectory data for different samples
of initial conditions. The NLDPC computational graph has
been used along with reverse-mode automatic differentiation
framework to implement the algorithm. Algorithm 1 presents
the steps of the proposed framework.

Algorithm 1 Neural Lyapunov DPC (NLDPC)

1: Input identiﬁed or physics based system dynamics f (.)
2: Input candidate Lyapunov function Vφ(x)
3: Input optimal control loss (cid:96)(x, u) and constraints h(x),

and g(u)

4: Input optimizer O
5: Construct NLDPC computational graph based on (12)
6: for epochs=1:Nepoch do
7:

Sample initial conditions x0 from distribution D
Evaluate forward pass of the NLDPC computational
graph and compute LNLDPC
Differentiate NLDPC computational graph w.r.t.
policy and Lyapunov candidate parameters to obtain
gradients ∇θLNLDPC and ∇φLNLDPC
Update control parameters θ ← θ + α∇θLNLDPC
Update Lyapunov parameters φ ← φ + α∇φLNLDPC

13:
14:
15: end for
16: Return: trained policy πθ and Lyapunov function Vφ

8:
9:
10:
11:
12:

IV. STABILITY GUARANTEES

Probabilistic considerations have been discussed in works
[35], [40], [41] for learning-based MPC setting. We use
sampling-based guarantees for the neural Lyapunov DPC
framework extending the approach of [35]. The parametric
optimization problem (12) has been solved using Algorithm
1 where we created multiple batches with sampled initial
0 ∈ X ⊆ Rnx from a distribution D. The
conditions xi

optimal control problem parametersinitial conditionsvarying referencesand constraintscontrol policysystem modeland constraintsneural Lyapunovfunctionbackward propagationfeedbackforward propagationsimulationstable constrained differentiable closed-loop systemdifferentiable MPC lossreference trackingconstraints penaltiesLyapunovpenaltyvariations in the initial condition are shown with superscript
i, and we create m such scenarios.

We denote the set of closed-loop state rollouts by T i,
and the control trajectories provided by the learned NLDPC
policy by [ui

i ∈ Nm

1 , i.e.,

0), for

0, . . . , ui
N −1] = πθ(xi
(cid:40)

T i :

k}, ∀k ∈ NN −1
{xi
,
0
k + Bui
k+1 = Axi
xi
k.
Considering the constraints, we can create a probabilistic
metric for the sampled roll-outs such as,

Pi = True :






h(xi
g(ui
Vφ(f (xi

k) ≤ 0, ∀k ∈ NN −1
k) ≤ 0, ∀k ∈ NN −1
0
k)) − V (xi

,
,
k) < 0,

k, ui

0

along with the indicator:

I(T i) :=

(cid:40)

1 if Pi = True.
0, otherwise,

(14)

(15)

which indicates if the learning based policy πθ(xi
the required constrained during the sampled roll-out.

0) satisﬁes

Theorem 2: Consider sampled initial conditions for solv-
ing the parametric optimal control problem (12) with as-
sumptions 1−3, and the predictive control policy using Algo-
rithm 1. Selecting constraint violation probability 1 − κ, and
level of conﬁdence parameter δ, if the empirical risk on the
indicator function (15), denoted as ˜σ, with sufﬁciently large
− ln δ
number of sample trajectories m satisfy κ ≤ ˜σ −
2m ,
then the learned NLDPC policy πθ(xk) will constraint
satisfaction and closed-loop stability in probabilistic sense.
Proof: We generate m number of sampled initial condi-
tions which lead to m number of trajectories. The initial
conditions are sampled in an i.i.d. manner, resulting in i.i.d.
T i, and I(T i). The empirical risk for all the trajectories is
given by,

(cid:113)

2

˜σ =

1
m

m
(cid:88)

I(T i).

(16)

i=1
We can guarantee stability and constraint satisfaction if for
0 ∼ D, we have, I(T i) = 1, i.e. h(xi
xi
k) ≤ 0,
Vφ(f (xi

N ∈ XT , ∀i ∈ Nm
1 .

k) ≤ 0, g(xi

k)) − V (xi

k) < 0, xi

Let us denote σ := Pr(I(T i) = 1). Recalling Hoeffding’s

k, ui

Inequality [35] to estimate σ from ˜σ we can write:

Pr(|˜σ − σ| ≥ α) ≤ 2exp(−2rα2) ∀α > 0.

(17)

Thereafter, using δ := 2exp(−2rα2), with conﬁdence 1−δ

one can have,

Pr(I(T i) = 1) = σ ≥ ˜σ − α.

(18)

Therefore with a chosen conﬁdence δ and risk lower bound
κ ≤ Pr(I(T i)) = 1, the empirical risk can be bounded as:

(cid:115)

κ ≤ ˜σ − α = ˜σ −

−

ln δ
2
2m

(19)

In a nutshell, once we ﬁx the conﬁdence δ and risk lower
bound κ, the empirical risk ˜σ and α is evaluated for an

experimental value of m. Therefore when policies trained
via Algorithm 1 satisﬁes (19), then with conﬁdence at least
1 − δ, at least a fraction of κ trajectories T i will satisfy
Pi = True. Therefore, state, action and Lyapunov constraints
are satisﬁed with the conﬁdence 1 − δ. This concludes the
proof.

V. NUMERICAL CASE STUDIES

A. Example 1: Stabilizing Double Integrator

We start with the task of learning a stabilizing neural
feedback policy for an unstable system. The unstable double
integrator model is given as:

xk+1 =

(cid:20)1.2
0.0

(cid:21)

1.0
1.0

xk +

(cid:21)

(cid:20)1.0
0.5

uk.

(20)

The system is subject to state and input constraints given as
xk ∈ X := {x | −10 ≤ x ≤ 10}, and uk ∈ U := {u | −1 ≤
u ≤ 1}, respectively. We consider the following quadratic
objectives,

LMPC =

N −1
(cid:88)

k=0

(cid:0)||xk||2
Qx

+ ||uk||2

Qu

(cid:1).

(21)

For learning neural Lyapunov DPC, we intend to learn a
full state feedback neural policy πθ(xk) using Algorithm 1.
We also impose terminal set constraints xN ∈ Xf
:=
{x | −0.1 ≤ x ≤ 0.1}. We also impose the Lyapunov
constraints given in (12). The weights are considered to
be QV = 2, Qh = 10, Qg = 100, QXf = 1, while for
the control objective (21) we consider prediction horizon
N = 1, and weights Qx = 5, Qu = 0.5. The neural
policy (4) πθ(x) : R2 → R is trained with 4 layers, 20
hidden states, with bias, and ReLU activation functions. For
the Lyapunov function Vφ(x) we use 8 layers with 40 hidden
states and ReLU activations. The training set X train consists
of 3333 normally sampled initial conditions xi
0 fully covering
the admissible set X . The training has been performed for
300 epochs. Fig. 2 shows the phase portrait of the two
states which has been stabilized and the level curves of the
learned Lyapunov function has also been superimposed. The
learned Lyapunov function is shown in Fig. 3 ,and the closed-
loop state trajectories in Fig. 5 which shows the closed-
loop stabilizing performance of NLDPC. Fig. 4 shows the
ﬁnite differences of the learned Lyapunov function for two
consecutive time steps of the closed-loop dynamics in the left
panel, and compared with the quadratic Lyapunov x(cid:62)x in
the right panel. The ﬁgure shows that the quadratic Lyapunov
can give a conservative safe region of operations whereas the
learned version using NLDPC is less conservative and tend
to justify the actual learned closed-loop behaviour.

B. Example 2: Stabilizing PVTOL aircraft model

For the next example, we considered a planar vertical
take-off and landing (PVTOL) aircraft model as appeared
in [42]. We have xk ∈ R6 representing positions (x, y),
velocities ( ˙x, ˙y), and orientation of the center of mass θ,
and its derivative ˙θ. The control inputs u ∈ R2 are the
forces from the aircraft thrusters. We use a discrete-time

Fig. 2: Closed-loop phase portrait with overlaid Lyapunov
contours for double integrator.

Fig. 3: Learned Lyapunov function for double
integrator.

Fig. 4: Stability regions via discrete-time Lyapunov stability
condition (5) for double integrator.

Fig. 5: Closed-loop state and control trajectories for
double integrator.

PVTOL model with sampling time of 0.2 seconds. Here, the
quadratic control objective uses weights Qx = 3, Qu = 0.1
with prediction horizon N = 10, and subject to state and
input constraints xk ∈ X := {x | −5 ≤ x ≤ 5}, and
uk ∈ U := {u | −5 ≤ u ≤ 5}. The penalty weights are
considered to be, QV = 3, Qh = 2, Qg = 2, respectively.
The neural policy (4) πW(x) : R6 → RN ×2 is similarly
considered with 4 layers, 20 hidden states, with bias, and
ReLU activation functions. For the Lyapunov function Vφ(x)
we use 8 layers with 40 hidden states and ReLU activations,
as before. We generate 9000 normally distributed initial
conditions with 3000 each for training, validation and testing
sets. Fig. 6 shows the phase portrait sliced only with the
aircraft velocities, and the overlaid learned Lyapunov level
curves. The learned Lyapunov function with only the above
mentioned dimensions are shown in Fig. 7 showing the
positive deﬁniteness. Fig. 9 shows all the states and control
actions in the closed-loop which satisﬁes all the required
constraints. Fig. 8 shows the ﬁnite differences of the Learned
and the quadratic Lyapunov functions for two time-steps of
the closed-loop where again the learned Lyapunov function
guarantees a larger region of safety than the quadratic one.

VI. CONCLUSIONS

This paper presented a differentiable programming-based
framework for joint learning of explicit neural predictive
control policies and neural Lyapunov functions. We bring
forth the idea of constraining the parametric optimal program
with learnable Lyapunov-based stability certiﬁcations. We
supplement our training framework with initial condition
sampling-based probabilistic guarantees using Hoeffding’s
inequality. Numerical experiments on standard dynamic sys-
tem examples of double integrator and PVTOL aircraft model
substantiate the algorithmic and theoretical results. Future
research will look into more exhaustive experiments with
nonlinear system models and using learned identiﬁcation-
based models in the NLDPC framework including different
architectures of neural Lyapunov function candidates. On
the theoretical side, we intend to expand the framework
to support time-varying Lyapunov functions and dissipative
system conditions used in economic MPC to deal with plant
model mismatch and measurement noise.

ACKNOWLEDGEMENT

This research was supported by the U.S. Department of
Energy, through the Ofﬁce of Advanced Scientiﬁc Comput-

Fig. 6: Closed-loop phase portrait with overlaid Lyapunov
contours for PVTOL aircraft. Visualized on 2D slice of the 6D
state space.

Fig. 7: Learned Lyapunov function for PVTOL
aircraft. Visualized on 2D slice of the 6D state space.

Fig. 8: Stability regions via discrete-time Lyapunov stability
condition (5) for PVTOL aircraft. Visualized on 2D slice of
the 6D state space.

Fig. 9: Closed-loop state and control trajectories for
PVTOL aircraft.

ing Research’s “Data-Driven Decision Control for Complex
Systems (DnC2S)” project. Paciﬁc Northwest National Lab-
oratory is operated by Battelle Memorial Institute for the
U.S. Department of Energy under Contract No. DE-AC05-
76RL01830.

REFERENCES

[1] A. Bemporad, M. M., V. Dua, and E. N. Pistikopoulos, “The ex-
plicit linear quadratic regulator for constrained systems,” Automatica,
vol. 38, no. 1, pp. 3 – 20, 2002.

[2] A. Alessio and A. Bemporad, A Survey on Explicit Model Predictive
Control, in Nonlinear Model Predictive Control: Towards New Chal-
lenging Applications. Berlin, Heidelberg: Springer Berlin Heidelberg,
2009, pp. 345–369.

[3] M. Kvasnica,

J. Hled´ık,

Fikar,
I.
“Complexity reduction of explicit model predictive control via
separation,” Automatica, vol. 49, no. 6, pp. 1776–1781, 2013.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0005109813001076

and M.

Rauov´a,

[4] S. Hovland and J. T. Gravdahl, “Complexity reduction in explicit mpc
reduction,” IFAC Proceedings Volumes, vol. 41,
through model
no. 2, pp. 7711–7716, 2008, 17th IFAC World Congress.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S1474667016401874

[5] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, “Provably
safe and robust learning-based model predictive control,” Automatica,
vol. 49, no. 5, pp. 1216 – 1226, 2013.

[6] A. Aswani, P. Bouffard, X. Zhang, and C. Tomlin, “Practical com-
parison of optimization algorithms for learning-based mpc with linear
models,” 2014.

[7] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger,
“Learning-based model predictive control: Toward safe learning in
control,” Annual Review of Control, Robotics, and Autonomous Sys-
tems, vol. 3, no. 1, p. null, 2020.

[8] R. Soloperto, M. A. M¨uller, S. Trimpe, and F. Allg¨ower, “Learning-
based robust model predictive control with state-dependent uncer-
tainty,” IFAC-PapersOnLine, vol. 51, no. 20, pp. 442 – 447, 2018,
6th IFAC Conference on Nonlinear Model Predictive Control NMPC
2018.

[9] M. Bujarbaruah, X. Zhang, U. Rosolia, and F. Borrelli, “Adaptive mpc
for iterative tasks,” 2018 IEEE Conference on Decision and Control
(CDC), pp. 6322–6327, 2018.

[10] A. Domahidi, M. N. Zeilinger, M. Morari, and C. N. Jones, “Learning
a feasible and stabilizing explicit model predictive control law by
robust optimization,” in 2011 50th IEEE Conference on Decision and
Control and European Control Conference, 2011, pp. 513–519.
[11] T. Zhang, G. Kahn, S. Levine, and P. Abbeel, “Learning deep control
policies for autonomous aerial vehicles with MPC-guided policy
search,” CoRR, vol. abs/1509.06791, 2015.

[12] J. Drgoˇna, D. Picard, M. Kvasnica, and L. Helsen, “Approximate

model predictive building control via machine learning,” Applied
Energy, vol. 218, pp. 199 – 216, 2018.

[13] A. Raha, A. Chakrabarty, V. Raghunathan, and G. T. Buzzard,
“Embedding approximate nonlinear model predictive control at
ultrahigh speed and extremely low power,” IEEE Trans. Control. Syst.
Technol., vol. 28, no. 3, pp. 1092–1099, 2020. [Online]. Available:
https://doi.org/10.1109/TCST.2019.2898835

[14] S. W. Chen, T. Wang, N. Atanasov, V. Kumar, and M. Morari, “Large
scale model predictive control with neural networks and primal active
sets,” 2019.

[15] M. Pereira, D. D. Fan, G. N. An, and E. A. Theodorou,
sequential decision
[Online]. Available:

“Mpc-inspired neural network policies
for
making,” CoRR, vol. abs/1802.05803, 2018.
http://arxiv.org/abs/1802.05803

[16] B. Amos, L. Xu, and J. Z. Kolter, “Input convex neural networks,”

CoRR, vol. abs/1609.07152, 2016.

[17] Y. Chen, Y. Shi, and B. Zhang, “Optimal control via neural networks:

A convex approach,” 2018.

[18] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and
Z. Kolter, “Differentiable convex optimization layers,” ArXiv, vol.
abs/1910.12430, 2019.

[19] B. Chen, Z. Cai, and M. Berg´es, “Gnu-rl: A precocial reinforcement
learning solution for building hvac control using a differentiable
the 6th ACM International
mpc policy,”
Conference on Systems for Energy-Efﬁcient Buildings, Cities, and
Transportation, ser. BuildSys ’19. New York, NY, USA: Association
for Computing Machinery, 2019, p. 316–325. [Online]. Available:
https://doi.org/10.1145/3360322.3360849

in Proceedings of

[20] E. T. Maddalena, C. G. da S. Moraes, G. Waltrich, and C. N. Jones,
“A neural network architecture to learn explicit mpc controllers from
data,” 2019.

[21] B. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter,
“Differentiable MPC for end-to-end planning and control,” CoRR,
vol. abs/1810.13400, 2018. [Online]. Available: http://arxiv.org/abs/
1810.13400

[22] S. East, M. Gallieri, J. Masci, J. Koutnik, and M. Cannon, “Inﬁnite-
horizon differentiable model predictive control,” in International Con-
ference on Learning Representations, 2020.

[34] J. Z. Kolter and G. Manek, “Learning stable deep dynamics
models,” in Advances in Neural Information Processing Systems
32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch-Buc,
Inc., 2019,
E. Fox, and R. Garnett, Eds. Curran Associates,
pp. 11 126–11 134. [Online]. Available: http://papers.nips.cc/paper/
9292-learning-stable-deep-dynamics-models.pdf

[35] M. Hertneck, J. K¨ohler, S. Trimpe, and F. Allg¨ower, “Learning
an approximate model predictive controller with guarantees,” IEEE
Control Systems Letters, vol. 2, no. 3, pp. 543–548, 2018.

[36] J. Z. Kolter and G. Manek, “Learning stable deep dynamics models,”

Advances in neural information processing systems, vol. 32, 2019.

[37] G. Puskorius and L. Feldkamp, “Truncated backpropagation through
time and Kalman ﬁlter training for neurocontrol,” in Proceedings of
1994 IEEE International Conference on Neural Networks (ICNN’94),
vol. 4.

IEEE, 1994, pp. 2488–2493.

[38] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”

arXiv preprint arXiv:1711.05101, 2017.

[39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” in Advances
in Neural Information Processing Systems, 2019, pp. 8024–8035.
[40] U. Rosolia, X. Zhang, and F. Borrelli, “A stochastic mpc approach
with application to iterative learning,” in 2018 IEEE Conference on
Decision and Control (CDC).

IEEE, 2018, pp. 5152–5157.

[41] B. Karg, T. Alamo, and S. Lucia, “Probabilistic performance validation
of deep learning-based robust nmpc controllers,” International Journal
of Robust and Nonlinear Control, vol. 31, no. 18, p. 8855–8876, Jul
2021.

[42] K. J. ˚Astr¨om and R. M. Murray, Feedback Systems: An Introduction
Princeton University Press, 2004.

for Scientists and Engineers.
[Online]. Available: https://fbswiki.org/wiki/index.php/Main Page

[23] P. L. Donti, M. Roderick, M. Fazlyab,

and J. Z. Kolter,
“Enforcing robust control guarantees within neural network policies,”
CoRR, vol.
[Online]. Available: https:
//arxiv.org/abs/2011.08105

abs/2011.08105, 2020.

[24] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints and
semideﬁnite programming,” IEEE Transactions on Automatic Control,
vol. 67, no. 1, pp. 1–15, 2022.

[25] S. M. Richards, F. Berkenkamp, and A. Krause, “The Lyapunov
neural network: Adaptive stability certiﬁcation for safe learning
of dynamic systems,” CoRR, vol. abs/1808.00924, 2018. [Online].
Available: http://arxiv.org/abs/1808.00924
[26] Y.-C. Chang, N. Roohi, and S. Gao, “Neural

lyapunov control,”

Advances in neural information processing systems, vol. 32, 2019.

[27] M. Zanon and S. Gros, “Safe reinforcement learning using robust

MPC,” CoRR, vol. abs/1906.04005, 2019.

[28] S. Gros and M. Zanon, “Reinforcement learning based on mpc and
the stochastic policy gradient method,” in 2021 American Control
Conference (ACC), 2021, pp. 1947–1952.

[29] M. Mittal, M. Gallieri, A. Quaglino, S. S. M. Salehian, and J. Koutn´ık,
“Neural lyapunov model predictive control: Learning safe global con-
trollers from sub-optimal examples,” arXiv preprint arXiv:2002.10451,
2020.

[30] P. Saha, M. Egerstedt, and S. Mukhopadhyay, “Neural identiﬁcation
for control,” IEEE Robotics and Automation Letters, vol. 6, no. 3, pp.
4648–4655, 2021.

[31] J. Drgona, A. Tuor, and D. Vrabie, “Learning constrained adaptive dif-
ferentiable predictive control policies with guarantees,” arXiv preprint
arXiv:2004.11184, 2022.

[32] J. Drgoˇna, S. Mukherjee, A. Tuor, M. Halappanavar, and D. Vrabie,
“Learning stochastic parametric differentiable predictive control poli-
cies,” arXiv preprint arXiv:2203.01447, 2022.

[33] J. Drgoˇna, A. Tuor, E. Skomski, S. Vasisht, and D. Vrabie, “Deep
learning explicit differentiable predictive control laws for buildings,”
IFAC-PapersOnLine, vol. 54, no. 6, pp. 14–19, 2021, 7th IFAC
Conference on Nonlinear Model Predictive Control NMPC 2021.

