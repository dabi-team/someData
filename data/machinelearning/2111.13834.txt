Springer Nature 2021 LATEX template

2
2
0
2

n
a
J

9

]

C
O
.
h
t
a
m

[

3
v
4
3
8
3
1
.
1
1
1
2
:
v
i
X
r
a

Federated Deep Learning in Electricity

Forecasting: An MCDM Approach

Marco Repetto1,2,3, Davide La Torre1* and Muhammad
Tariq4

1*SKEMA Business School, Université Côte d’Azur,
Sophia-Antipolis, France.
2University of Milan-Bicocca, Milan, Italy.
3Siemens S.p.a., Milan, Italy.
4Abu Dhabi School of Management, Abu Dhabi, UAE.

*Corresponding author(s). E-mail(s): davide.latorre@skema.edu;
Contributing authors: marco.repetto@skema.edu;
m.tariq@adsm.ac.ae;

Abstract

Large-scale data analysis is growing at an exponential rate as data pro-
liferates in our societies. This abundance of data has the advantage of
allowing the decision-maker to implement complex models in scenarios
that were prohibitive before. At the same time, such an amount of data
requires a distributed thinking approach. In fact, Deep Learning models
require plenty of resources, and distributed training is needed. This paper
presents a multicriteria approach for distributed learning. Our approach
uses the Weighted Goal Programming approach in its Chebyshev formu-
lation to build an ensemble of decision rules that optimize aprioristically
deﬁned performance metrics. Such a formulation is beneﬁcial because it is
both model and metric agnostic and provides an interpretable output for
the decision-maker. We test our approach by showing a practical applica-
tion in electricity demand forecasting. Our results suggest that when we
allow for dataset split overlapping, the performances of our methodology
are consistently above the baseline model trained on the whole dataset.

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

1 Introduction

It is well know that Artiﬁcial Intelligence (AI) identiﬁes in a broad sense the

ability of a machine to learn from experience, to simulate the human intelli-

gence, to adapt to new scenarios, and to get engaged in human-like activities. AI

identiﬁes an interdisciplinary area which includes computer science, robotics,

engineering, mathematics. Over the years, it has made a rapid progress: it will

contribute to the society transformation through the adoption of innovating

technologies and creative intelligence and the large-scale implementation of

AI in technologies such as IoT, smart speakers, chat-bots, cybersecurity, 3D

printing, drones, face emotions analysis, sentiment analysis, natural language

processing, and their applications to human resources, marketing, ﬁnance,

and many others.

With the term Machine learning (ML), instead, we identify a branch of AI

in which algorithms are used to learn from data to make future decisions or

predictions. ML algorithms are trained on past data in order to make future

predictions or to support the decision making process.

Deep Learning (DL), instead, is a subset of ML and it includes a large family

of ML methods and architectures based on Artiﬁcial Neural Networks (ANNs).

It includes Deep Neural Networks, Deep Belief Networks, Deep Reinforcement

Learning, Recurrent Neural Networks and Convolutional Neural Networks, to

mention a few of them. DL algorithms have been used in several applications

including computer vision, speech recognition, natural language processing,

bioinformatics, medical image analysis, and in most of these areas they have

demonstrated to perform better than humans. In the recent years DL has

disrupted every application domain and it provides a robust, generalized, and

scalable approach to work with diﬀerent data types, including time-series data

[1–4].

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

3

Fostered by the abundance of data, many recent DL architectures require

a considerable amount of training. At the same time, local regulations posed

signiﬁcant constraints in terms of data transmission in distributed systems [5].

Because of this [6] proposed the concept of Federated Learning (FL). FL is a

distributed learning methodology allowing model training on a large corpus

of decentralized data [7].

Recent applications of the FL focus primarily on the medical ﬁeld, as par-

ticular attention has to be placed on patient information. In [8] the authors

proposed an FL system based on memory-aware curriculum learning to solve

the problem of class imbalance in the data of multiple institutions. In [9], the

authors proposed a sound FL methodology with Diﬀerential Privacy with an

acceptable compromise between privacy and performance. It is worth noting

how both techniques may suﬀer from data leakage as by sharing gradients

within nodes, some of the original information may be reconstructed [10].

Aside from the medical ﬁeld, [11] investigated FL applied on the Internet of

Things (IoT) comparing diﬀerent out-of-the-shelf open-source approaches. In

particular, the authors evaluated several aspects ranging from ease of use to

performance in terms of implementation.

Aside from the performance, FL carries diﬀerent concerns in terms of data

leakage depending on the method utilized in the aggregation phase [12].

Moreover, as pointed out by [13] these systems can be subject to adversarial

attacks.

Adversarial training is a fairly recent ﬁeld in ML and DL. It is about the

limitations of existing algorithms in their ability to correctly classify when

they have been trained on adversarially datasets or to react against the eﬀect of

inputs purposely designed to fool a ML model. A critical scenario in adversarial

learning happens when training the same model from diﬀerent datasets that

Springer Nature 2021 LATEX template

4

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

show conﬂicting trends or behaviours. For instance, this is the case in ﬁnancial

forecasting when one tries to predict the future evolution of ﬁnancial markets

based on covid or pre-covid data.

Adversarial training impacts FL based on how the aggregation procedure

is carried out. As demonstrated in [12] in the case of FL in which gradients

are shared, the attacker can leverage models gradients to either reconstruct

datapoints or to inject an adversarial attack. In particular, adversarial attacks

may happen even in federated averaging as these aggregation techniques lack

node accountability [13].

Motivated by the above considerations, in order to avoid the bias given

by one dataset with respect to another, in this paper we propose an new

approach based on Multiple Criteria Decision Making (MCDM). The diﬀerent

loss functions obtained by training the same model on diﬀerent datasets are

combined in a unique framework by means of MCDM techniques. More in

particular, we propose a Goal Programming (GP) model to deal with this

complex scenario. Numerical experiments show the goodness of this approach

and provide strategic managerial insights.

The intersections between MCDM and DL have not been explored yet

and in the literature there are just a few contributions that try to combine

MCDM and forecasting/machine learning techniques. To mention a few of

them, for instance in [14] the authors analyze the relationship between times

series and inﬂuential multiple criteria and they oﬀer long-term electricity

demand predictions in Greece. In [15], instead, the authors propose a novel

fuzzy multiple criteria time series modelling method based on fuzzy linear

programming and sequential interactive techniques and they apply it to urban

transportation planning.

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

5

Our approach aligns with the FL paradigm as our dataset is heterogeneous

and spread across many nodes with some computing capabilities. Moreover,

our approach is aﬀected by the literature on adversarial training as models

compete in the aggregation phase. In our case, the number of models coincides

with the number of nodes, but theoretically, this can be extended by allowing

ensemble in each local node. Our approach distinguishes from the previous

ones as it is not model-dependent [11]. Moreover, our approach is diﬀerent

from model averaging as proposed by [16], [17] and [18] as we allow for nodes

accountability and attribute nodes relevance based on their performance.

The paper is organized as follows. Section 2 is devoted to the literature

review on MCDM. In Section 3 we present our main model. In Section 4 we

present numerical experiments and discuss the results of our approach in the

case of electricity demand forecasting. Section 5 concludes.

2 Multiple Criteria Decision Making and Goal

Programming

MCDM is a branch of Operations Research and Management Science. MCDM

includes the set of methods and processes through which the concern for

several conﬂicting criteria can be explicitly incorporated into the analytical

process [19]. Several strategies, spanning from a priori to interactive ones, have

been developed to address this issue. Furthermore, these methods were widely

used in a variety of ﬁelds, including economics, engineering, ﬁnance, and

management [20]. Historically MCDM’s ﬁrst roots are the ones laid by Pareto

at the end of the 19th century. An MCDM problem is expressed mathematically

as follows:

Springer Nature 2021 LATEX template

6

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

min
xxx

s.t.

(f1(xxx), f2(xxx), . . . , fp(xxx))

xxx ∈ S

(1)

(2)

where fi : IRn → IR is the ith objective and the vector xxx contains the

decionsion variabels that belong to the feasible set S.

Scalarization is a common approach for dealing with Multicriteria optimiza-

tion problems. By scalarization, a vector optimization problem is converted

into a single objective optimization problem by means of a weighted sum
scalarization f (xxx) = www(cid:62)fff (xxx) = (cid:80)p

i=1 wifi(xxx) where www is a vector weights.

Each criterion is included in the scalarized version with diﬀerent weights

which express the relative importance of that criterion for the Decision Maker

(DM). Another method that can be used to solve vector-valued problems is

Goal Programming (or GP approach). The method was initially proposed by

[21] and it aims at minimizing the deviations coming from the over or under

achievement of diﬀerent objectives. The innovative idea behind this model is

the determination of the aspiration levels of an objective function. This model

does not try to ﬁnd an optimal solution but an acceptable one (or satisfying

one), and it tries to achieve the goals set by the DM rather than maximising or

minimising the diﬀerent criteria. Given a set of ideal goals gi , with i = 1, ..., p,

chosen by the DM, a weighted GP problem takes the form:

min
i ,δ−

xxx,δ+

i

p
(cid:88)

i=1

w+

i δ+

i + w−

i δ−

i

s.t.

fi(xxx) + δ−

i − δ+

i = gi,

i = 1 . . . p

(3)

(4)

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

7

xxx ∈ S

i , δ+
δ−

i ≥ 0,

i = 1 . . . p

(5)

(6)

where δ+

i , δ−

i measures respectively the negative and the positive deviations

to the optimal level gi, whereas w+

i , w−

i express the relative importance of the

modeler to these goals.

3 Model Formulation

Before specifying our approach is worth deﬁning some notation that will

be used through the section and in the empirical application. In our setup,

we observe a tuple of the target variable and features over time, that is

{(yt, xt); t = 1...T }. As our empirical application focuses on univariate time

series forecasting, the features are the lagged values of the target variable. How-

ever, our approach is suitable for any use case in which a large dataset prevents

any model estimation in a single machine or whether the dataset is distributed

across many nodes not belonging to the same device. Now let’s assume that
we slice the whole dataset into K partitions such that (cid:80)K

k=1 Sk ≥ T . In other

words, the sum of the lengths of each partition is at least the length of the

dataset itself. This formulation allows for overlapping in between partitions.

This is a recurrent situation in many distributed systems, as data redundancy

is used to enforce the system’s reliability. In our empirical experiment, we con-

sidered the diﬀerent degrees of overlapping ranging from no overlapping at
all, that is, (cid:80)K
the two adjacent ones. In other words (cid:80)K

k=1 Sk = T to full overlapping in which each partition includes

k=1 Sk ≈ 3T

Given these premises, the proposed FL approach can be divided into three

steps: local learning, federated validation-aggregation, and local updating.

Springer Nature 2021 LATEX template

8

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

In step 1, we train a model for each data partition, that is fk(wk, xk,t),

∀k ∈ K, ∀t ∈ Sk, where Sk index the subset of each partition, we can deﬁne

the predictions of such model as ˆyk,t where the prediction depends on both

the model indexed with k and the subset of features provided indexed with t.

In step 2, we solve the following Chebyshev Goal Programming [22] problem

to ﬁnd an ensemble of models that jointly performs better than the sole models

on all the dataset splits:

min
αj, λ

λ

s.t.

δ+
k ≤ λ

∀k ∈ K,

K
(cid:88)

j=1

αjLj(yk,t, ˆyk,t) − δ+

k + δ−

k = Lk(yt, ˆyk,t) ∀k ∈ K, ∀t ∈ Sk,

K
(cid:88)

k=1

αk = 1,

αk, δ+

k , δ−

k ≥ 0

∀k ∈ K

In step 3, with the optimal set of weights α we predict the output as a

convex combination of the 1...K trained models, that is:

ˆyt =

K
(cid:88)

k=1

αkfk(wk, xk,t)

.

The advantage of such an approach is that it is both model and metric

agnostic. More speciﬁcally, the approach is model agnostic in the sense that f (·)

can be any predictive function, ranging from Deep ANNs to Gradient Boosted

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

9

Fig. 1: Total electricity demand for the New England from 1st March 2003 to
30th April 2017 on a hourly basis.

Machines. Moreover, the approach is metric agnostic as L(·) can be substituted

with any, possibly non-diﬀerentiable, performance metric. An example in the

case of classiﬁcation may be to use accuracy or the Area Under the Receiving

Operating Curve (AUROC). However, in such cases, as both the accuracy and

AUROC need to be maximized, δ− should be placed in the ﬁrst soft constraint

of the GP.

4 Numerical Experiments and Discussion

This section describes an application of our approach to the use case of electric-

ity demand forecasting. In particular, we employ a publicly available dataset,

the 2017 Global Energy Forecasting Competition (GEFCom2017) dataset [23].

The dataset consists of information about the electricity load, holiday, and

weather data. Our application in electricity demand forecasting focuses only

on electricity load, particularly on the total electricity load given by the sum of

all the diﬀerent New England zones.

Springer Nature 2021 LATEX template

10

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

The plot in Figure 1 gives a perspective on the size of the time series that

spans from 1st March 2003 to 30th April 2017 with a frequency of one hour. In

total, the time series consists of more than 100 thousand observations.

4.1 Experimental setup

The GEFCom2017 competition required only one month ahead forecast. In our

setup, we extend the forecasting horizon in order to compare our approach

to the approach proposed by other authors, such as the one proposed in [24].

Therefore we split the time series into training and test set. The training set

starts at the beginning of the dataset and ends on the 31st of December 2016.

The test set starts on the 1st of January 2017 and ends on the 30th of April 2017.

The forecasting horizon is then of 2879 steps. To model this time series, we use

the Feedforward Artiﬁcial Neural Network (FANN) in its most straightforward

architecture, the Multilayer Perceptron (MLP). The functional form of the

MLP can be elicited as in [25]:

(cid:32)

f (x) = φ

β0 +

d
(cid:88)

j=1

(cid:32)

βjG

γj0 +

(cid:33)(cid:33)

p
(cid:88)

i=1

γjixi

(7)

where G is the activation function, in our case G(x) =

1

1+e−αx , β and γ represent

weights and biases at each layer, whereas φ(·) is the network output function

that in our case is the identity function as our goal is to regress electricity

demand and not time series classiﬁcation. To test our approach’s robustness,

we used diﬀerent conﬁgurations in terms of split size and splits overlapping.

For the split size, we considered 10, 150, and 200, whereas, for the overlapping

ratio, we considered complete, half, and no overlaps in between the splits.

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

11

4.2 Evaluation metrics

The evaluation metrics we used are: Root Mean Squared Error (RMSE), Mean

Absolute Error (MAE), Mean Absolute Scaled Error (MASE) and Symmetric

Mean Absolute Percentage Error (SMAPE). RMSE is a standard metric used

both in time series contexts and more broadly in any supervised learning

setting. It is deﬁned as:

RM SE =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(yi − ˆyi)2

the RMSE is a scale-dependent metric and, because it uses squared errors, is

more sensitive to higher deviations than other measures such as MAE. MAE,

while being also a scale-dependent measure metric, is formulated as:

M AE =

1
n

n
(cid:88)

i=1

|yi − ˆyi|

Diﬀerently from the ﬁrst two metrics, MASE is a scale-independent metric

that is used chieﬂy in time series contexts, initially proposed by [26], MASE is

deﬁned as follows:

M ASE

1
J

(cid:80)

j |yj − ˆyj|
T
(cid:88)

|yt − yt−1|

.

1
T − 1

t=2

Firstly appeared in [27] SMAPE is also an accuracy metric used in time series.

Contrary to the MASE, the SMAPE is a percentage based metric formulated as:

SM AP E =

1
n

n
(cid:88)

i=1

|yi − ˆyi|
|yi|+|ˆyi|
2

Both SMAPE and MASE are used extensively in time series, and this is the

reason behind their inclusion [28, 29].

Springer Nature 2021 LATEX template

12

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

Table 1: Approach performances based on diﬀerent parametrization of split
size and overlapping ratio.

Split Overlap

RMSE MAE

MASE

SMAPE

10
200
150
200
10
150
200
150
10

1.0
1.0
1.0
0.0
0.5
0.0
0.5
0.5
0.0

1588.22
2979.64
3352.54
4083.87
4598.62
4545.16
4568.74
5105.29
9211.98

1256.06
2474.38
2640.00
3398.31
3548.89
3710.49
3747.89
3962.41
8755.59

2.66
5.25
5.60
7.21
7.53
7.87
7.95
8.40
18.57

0.10
0.18
0.19
0.23
0.24
0.25
0.25
0.26
0.49

4.3 Approach results

To test the performances of our approach, we treated both the number of

splits and the overlapping ratio as hyperparameters. A glimpse of the results

is presented in Table 1.

Figure 2 shows the diﬀerent performances, measured with the SMAPE

metric, of our approach and the sensitivity to hyperparameters change. In

particular, a low number of splits in combination with no overlaps results

in a worse model than the baseline (dashed red line). On the contrary, an

increase in splits and overlapping results in an ensemble model on par with

the benchmark. It is worth noting that although the performance is similar

between the two approaches in this setting, the advantage of our approach is

that it allows for training in diﬀerent machines and does not force the data to

reside on the same device that is doing the model estimation. Last, the more

relevant result is that overlapping aﬀects model performance positively. The

result is far better than the benchmark, with a SMAPE of less than 0.1 in the

case of ten splits. More speciﬁcally, overlapping seems to oﬀset the number of

splits. The potential of overlapping was already signaled by the literature [30].

This diﬀerence in performance is straightforward if we look at Figure 3.

In contrast to the model trained on the whole dataset, the combined forecast

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

13

Fig. 2: Symmetric Mean Absolute Percentage Error on the test set under
diﬀerent split size and overlapping ratios.

Fig. 3: Test set forecasts with respect to the true electricity demand (red line).
The green line depicts the forecasts based on the model trained on the full
dataset. The blue line depicts the forecasts based on the model trained with
our approach with split size 10 and full overlapping.

captures better the decreasing trend of the time series. Moreover, the forecast

is less volatile than the benchmark.

To investigate better the inner workings of our approach is also worth

looking at the weights attributed by our multicriteria decision problem. These

weights are in Figure 4 and are attributed to the model trained on a particular

data split. Therefore, for example, the model trained on the ﬁrst split, the most

recent one, has zero weights and does not contribute to the ﬁnal forecast. On

Springer Nature 2021 LATEX template

14

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

Fig. 4: Weights partitioning of the ensemble model based on split size 10 and
complete overlapping.

the contrary, the models trained on the fourth and sixth split are the only ones

with non-zero weights. This means that these models are the ones that score

best in comparison with other models trained on diﬀerent splits.

These dynamics are particularly highlighted in Figure 5. This ﬁgure repre-

sents the SMAPE of each model applied to a diﬀerent split. The model that

performs worst is the tenth model, and we can see that by looking at the last col-

umn, which is the one with the darkest cells. Another relevant information is

that every model performs poorly on both the ﬁrst split and the last one. What,

instead, is clear is that both M4 and M6 perform better in almost all the splits.

5 Conclusion

The presence of adversarial training, biased datasets, and conﬂicting trends

can generate problems during the machine learning process. This appears to

be very crucial when thinking that splitting a dataset into training, test, and

hold-out subsets is, most of the time, arbitrary and only led by the need to

improve the algorithm performance and accuracy. On the top of this, dealing

with huge amount of information can lead to long execution time of the gradi-

ent descent algorithm. In order to get eﬃcient and prompt decisions it looks

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

15

Fig. 5: Symmetric Mean Absolute Percentage Error on all the training data
splits based on the model.

like of paramount importance to be able to split the original dataset into smaller

chunks, training the same or diﬀerent models on each of them separately, and

ﬁnally recombine all of them in a unique framework. The advantages of this

approach are twofold: from one side it allows to combine diﬀerent models

that are trained on diﬀerent datasets without exchange of information dur-

ing the gradient descent approach. This can be run on parallel architectures.

From another the proposed GP formulation allows to recombine these mod-

els in a unique and eﬃcient framework. Our numerical results conﬁrm the

goodness of this approach in the case of time-series analysis. Future avenues

include the extensions of this framework by including diﬀerent MCDM and

GP approaches.

References

[1] LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature 521(7553), 436–

444 (2015)

[2] Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review

and new perspectives. IEEE transactions on pattern analysis and machine

Springer Nature 2021 LATEX template

16

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

intelligence 35(8), 1798–1828 (2013)

[3] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image

recognition. In: Proceedings of the IEEE Conference on Computer Vision

and Pattern Recognition, pp. 770–778 (2016)

[4] Van Essen, B., Kim, H., Pearce, R., Boakye, K., Chen, B.: Lbann: Livermore

big artiﬁcial neural network hpc toolkit. In: Proceedings of the Workshop

on Machine Learning in High-performance Computing Environments,

pp. 1–6 (2015)

[5] Ahmed, A.S., Abood, M.S., Hamdi, M.M.: Advancement Of Deep

Learning In Big Data And Distributed Systems. In: 2021 3rd Interna-

tional Congress on Human-Computer Interaction, Optimization and

Robotic Applications (HORA), pp. 1–7 (2021). https://doi.org/10.1109/

HORA52670.2021.9461274

[6] Konečný, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh, A.T., Bacon, D.:

Federated Learning: Strategies for Improving Communication Eﬃciency

(2017). http://arxiv.org/abs/1610.05492 Accessed 2021-11-22

[7] Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov,

V., Kiddon, C., Konečný, J., Mazzocchi, S., McMahan, H.B., Van Overveldt,

T., Petrou, D., Ramage, D., Roselander, J.: Towards Federated Learning at

Scale: System Design (2019). http://arxiv.org/abs/1902.01046 Accessed

2021-11-27

[8] Jiménez-Sánchez, A., Tardy, M., Ballester, M.A.G., Mateus, D., Piella, G.:

Memory-aware curriculum federated learning for breast cancer classi-

ﬁcation. arXiv:2107.02504 [cs] (2021) https://arxiv.org/abs/2107.02504

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

17

[cs]

[9] Beguier, C., du Terrail, J.O., Meah, I., Andreux, M., Tramel, E.W.: Diﬀeren-

tially Private Federated Learning for Cancer Prediction. arXiv:2101.02997

[cs, stat] (2021) https://arxiv.org/abs/2101.02997 [cs, stat]

[10] Zhu, L., Han, S.: Deep Leakage from Gradients. In: Yang, Q., Fan, L., Yu,

H. (eds.) Federated Learning: Privacy and Incentive. Lecture Notes in

Computer Science, pp. 17–31. Springer International Publishing, Cham

(2020). https://doi.org/10.1007/978-3-030-63076-8_2

[11] Kholod, I., Yanaki, E., Fomichev, D., Shalugin, E., Novikova, E., Filippov,

E., Nordlund, M.: Open-Source Federated Learning Frameworks for IoT:

A Comparative Review and Analysis. Sensors 21(1), 167 (2021). https:

//doi.org/10.3390/s21010167

[12] Lim, J.Q., Chan, C.S.: From Gradient Leakage To Adversarial Attacks In

Federated Learning. In: 2021 IEEE International Conference on Image Pro-

cessing (ICIP), pp. 3602–3606 (2021). https://doi.org/10.1109/ICIP42928.

2021.9506589

[13] Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., Shmatikov, V.: How To

Backdoor Federated Learning. In: Proceedings of the Twenty Third Inter-

national Conference on Artiﬁcial Intelligence and Statistics, pp. 2938–2948

(2020). https://proceedings.mlr.press/v108/bagdasaryan20a.html

[14] Angelopoulos, D., Siskos, Y., Psarras, J.: Disaggregating time series on

multiple criteria for robust forecasting: The case of long-term electricity

demand in greece. European Journal of Operational Research 275(1),

252–265 (2019)

Springer Nature 2021 LATEX template

18

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

[15] Stoilova, S., Munier, N.: A novel fuzzy simus multicriteria decision-

making method. an application in railway passenger transport planning.

Symmetry 13(3), 483 (2021)

[16] Yager, R.R.: On ordered weighted averaging aggregation operators in

multicriteria decisionmaking. IEEE Transactions on Systems, Man, and

Cybernetics 18(1), 183–190 (1988). https://doi.org/10.1109/21.87068

[17] Warnat-Herresthal, S., Schultze, H., Shastry, K.L., Manamohan, S.,

Mukherjee, S., Garg, V., Sarveswara, R., Händler, K., Pickkers, P., Aziz,

N.A., Ktena, S., Tran, F., Bitzer, M., Ossowski, S., Casadei, N., Herr,

C., Petersheim, D., Behrends, U., Kern, F., Fehlmann, T., Schommers,

P., Lehmann, C., Augustin, M., Rybniker, J., Altmüller, J., Mishra,

N., Bernardes, J.P., Krämer, B., Bonaguro, L., Schulte-Schrepping, J.,

De Domenico, E., Siever, C., Kraut, M., Desai, M., Monnet, B., Saridaki, M.,

Siegel, C.M., Drews, A., Nuesch-Germano, M., Theis, H., Heyckendorf,

J., Schreiber, S., Kim-Hellmuth, S., Nattermann, J., Skowasch, D., Kurth,

I., Keller, A., Bals, R., Nürnberg, P., Rieß, O., Rosenstiel, P., Netea, M.G.,

Theis, F., Mukherjee, S., Backes, M., Aschenbrenner, A.C., Ulas, T., Breteler,

M.M.B., Giamarellos-Bourboulis, E.J., Kox, M., Becker, M., Cheran, S.,

Woodacre, M.S., Goh, E.L., Schultze, J.L.: Swarm Learning for decen-

tralized and conﬁdential clinical machine learning. Nature 594(7862),

265–270 (2021). https://doi.org/10.1038/s41586-021-03583-3

[18] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:

Communication-Eﬃcient Learning of Deep Networks from Decentralized

Data. In: Proceedings of the 20th International Conference on Artiﬁcial

Intelligence and Statistics, pp. 1273–1282 (2017). PMLR

Springer Nature 2021 LATEX template

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

19

[19] Ehrgott, M., Gandibleux, X., Hillier, F.S. (eds.): Multiple Criteria Opti-

mization: State of the Art Annotated Bibliographic Surveys. International

Series in Operations Research & Management Science, vol. 52. Springer

US, Boston, MA (2002). https://doi.org/10.1007/b101915

[20] Colapinto, C., Jayaraman, R., Marsiglio, S.: Multi-criteria decision analysis

with goal programming in engineering, management and social sciences:

A state-of-the art review. Annals of Operations Research 251(1-2), 7–40

(2017)

[21] Charnes, A., Cooper, W.W., Ferguson, R.O.: Optimal estimation of exec-

utive compensation by linear programming. Management science 1(2),

138–151 (1955)

[22] Flavell, R.: A new goal programming formulation. Omega 4(6), 731–732

(1976). https://doi.org/10.1016/0305-0483(76)90099-2

[23] Hong, T., Xie, J., Black, J.: Global energy forecasting competition 2017:

Hierarchical probabilistic load forecasting. International Journal of Fore-

casting 35(4), 1389–1399 (2019). https://doi.org/10.1016/j.ijforecast.2019.

02.006

[24] Wang, X., Kang, Y., Hyndman, R.J., Li, F.: Distributed ARIMA Models for

Ultra-Long Time Series (2020). http://arxiv.org/abs/2007.09577 Accessed

2021-09-16

[25] Arifovic, J., Gencay, R.: Using genetic algorithms to select architecture of

a feedforward artiÿcial neural network. Physica A, 21 (2001)

[26] Hyndman, R.J., Koehler, A.B.: Another look at measures of forecast

accuracy. International Journal of Forecasting 22(4), 679–688 (2006).

Springer Nature 2021 LATEX template

20

Federated Deep Learning in Electricity Forecasting: An MCDM Approach

https://doi.org/10.1016/j.ijforecast.2006.03.001

[27] Armstrong, J.S.: Long-Range Forecasting: From Crystal Ball to Computer,

2. ed edn. Wiley, ??? (1985)

[28] Makridakis, S., Spiliotis, E., Assimakopoulos, V.: The M4 Competition:

100,000 time series and 61 forecasting methods. International Journal of

Forecasting 36(1), 54–74 (2020). https://doi.org/10.1016/j.ijforecast.2019.

04.014

[29] Shankar, S., Ilavarasan, P.V., Punia, S., Singh, S.P.: Forecasting container

throughput with long short-term memory networks. Industrial Manage-

ment & Data Systems 120(3), 425–441 (2020). https://doi.org/10.1108/

IMDS-07-2019-0370

[30] Scott, A., Smith, T.: Analysis of repeated surveys using time series meth-

ods. Journal of the American Statistical Association 69(347), 674–678

(1974)

