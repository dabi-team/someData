IPC: A Benchmark Data Set for Learning with Graph-Structured Data

Patrick Ferber 1 Tengfei Ma 2 Siyu Huo 2 Jie Chen 2 3 Michael Katz 2

9
1
0
2

y
a
M
5
1

]

G
L
.
s
c
[

1
v
3
9
3
6
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

Benchmark data sets are an indispensable ingre-
dient of the evaluation of graph-based machine
learning methods. We release a new data set,
compiled from International Planning Competi-
tions (IPC), for benchmarking graph classiﬁca-
tion, regression, and related tasks. Apart from
the graph construction (based on AI planning
problems) that is interesting in its own right, the
data set possesses distinctly different characteris-
tics from popularly used benchmarks. The data
set, named IPC, consists of two self-contained
versions, grounded and lifted, both including
graphs of large and skewedly distributed sizes,
posing substantial challenges for the computa-
tion of graph models such as graph kernels and
graph neural networks. The graphs in this data
set are directed and the lifted version is acyclic,
offering the opportunity of benchmarking spe-
cialized models for directed (acyclic) structures.
Moreover, the graph generator and the labeling
are computer programmed; thus, the data set
may be extended easily if a larger scale is de-
sired. The data set is accessible from https:
//github.com/IBM/IPC-graph-data.

1. Introduction

Benchmark data sets are indispensable in the evaluation of
machine learning models for graph-structured data. With
the surging interest in graph representation learning, a rich
collection of data sets constructed from real-life applications
becomes important for the validation of effectiveness of any
existing or newly proposed method, and for demonstrating
its widespread applicability. We introduce a new labeled
data set, IPC, compiled from AI plannig tasks described in
the Planning Domain Deﬁnition Language (PDDL) (McDer-
mott, 2000).

1University of Basel 2IBM Research 3MIT-IBM Watson AI Lab.
Correspondence to: Jie Chen <chenjie@us.ibm.com>, Michael
Katz <Michael.Katz1@ibm.com>.

Presented at the ICML 2019 Workshop on Learning and Reasoning
with Graph-Structured Data Copyright 2019 by the author(s).

In this data set, each planning task is represented as a di-
rected graph, which has target values and whose nodes are
equipped with features. Planning tasks described in PDDL
admit a concise representation in transition graphs, how-
ever too big to ﬁt in any conceivable size memory. Recent
advances in planning allow to encode some structural in-
formation of a task in graphs of manageable size. Two
examples are the problem description graph (Pochter et al.,
2011), for a grounded task representation, and the abstract
structure graph (Sievers et al., 2019b), for a lifted representa-
tion. Hence, our data set consists of two versions of graphs
(IPC-grounded and IPC-lifted) for the same set of tasks;
each version may be used independently. There are 2439
planning tasks in total, pre-split for training, validation, and
testing. Moreover, the lifted version is acyclic.

Accompanied with the tasks are performance results for 17
cost-optimal domain-independent planners, each of which
attempts to solve a task under a timeout limit T = 1800
seconds. Hence, the target values for each graph are the CPU
times of these planners, gathered on the same hardware. For
practical reasons, if a planner cannot solve the task before
timeout, the target value is artiﬁcially set as 10000.

The background on AI Planning and graph construction,
including example problem domains, node feature deﬁni-
tion, and how the data set can be extended, are presented in
Section 2. The characteristics of this data set are different in
several aspects from those of the commonly used benchmark
data sets for graph kernels and graph neural networks: The
sizes of our graphs not only are substantially larger but also
vary signiﬁcantly. The imposed challenges and implications
are elaborated in Section 3. We present an example use of
the data set in Section 4 and conclude in Section 5.

2. Data Set Construction

PDDL tasks are deﬁned over a ﬁrst-order language L that
consists of predicates, functions, a set of natural numbers,
variables, and constants. Given L, a normalized (Helmert,
2009) PDDL task is a tuple Π = (cid:104)O, A, I, G(cid:105) of schematic
operators, schematic axioms, initial state speciﬁcation, and
goal speciﬁcation, in so-called lifted representation.

Most tools for solving the planning problems (a.k.a. plan-
ners) perform grounding as a ﬁrst step, followed by trans-

 
 
 
 
 
 
IPC: A Benchmark Data Set for Learning with Graph-Structured Data

(a) Problem (in PDDL)

(b) Grounded graph

(c) Lifted graph

Figure 1. An example planning task (described by using PDDL) and the constructed graphs.

lation into a SAS+ language (B¨ackstr¨om & Nebel, 1995).
In SAS+, a task Π = (cid:104)V, O, A, s0, s(cid:63)(cid:105) consists of ﬁnite-
domain variables, ground operators, ground axioms, initial
state, and the goal.

Deﬁnition 2 (Sievers et al., 2019b) Let S be a set of sym-
bols, where each s ∈ S is associated with a type t(s). The
set of abstract structures over S is inductively deﬁned as
follows:

Our data set consists of two graphical representations per
planning task. These representations losslessly encode the
information in the planning task and are often used for the
computation of structural symmetries (Shleyfman et al.,
2015). The graph obtained from the grounded representa-
tion SAS+ is called the problem description graph (PDG)
(Pochter et al., 2011). We present here the deﬁnition ex-
tended to support conditional effects and axioms and refer
the reader to Sievers et al. (2019a) for further details.

Deﬁnition 1 Let Π = (cid:104)V, Vd, O, A, sd, s0, s(cid:63)(cid:105) be a SAS+
task. The problem description graph of Π is the digraph
(cid:104)N, E(cid:105) with nodes

N = {n0, n(cid:63)} ∪ {nv | v ∈ V} ∪ Nf ∪ NO ∪ {na | a ∈ A},

where Nf = {nd
o ∈ O} ∪ {ne

v | v ∈ V, d ∈ dom(v)} and NO = {no |

o | o ∈ O, e ∈ effs(o)}, and edges

E = E0 ∪ E(cid:63) ∪ Ev ∪ Ea ∪ Eo, where

E0 = {(cid:104)n0, nd
E(cid:63) = {(cid:104)ng, nd
Ev = {(cid:104)nv, nd
Ea = {(cid:104)na, nd
∪ {(cid:104)na, nd
Eo = {(cid:104)no, nd
∪ {(cid:104)no, ne
v, ne
∪ {(cid:104)nd
o, nd
∪ {(cid:104)ne

v(cid:105) | s0[v] = d}
v(cid:105) | v ∈ vars(s(cid:63)), s(cid:63)[v] = d}
v(cid:105) | d ∈ dom(v)}
v(cid:105) | a ∈ A, v ∈ vars(pre(a)), pre(a)[v] = d}
v(cid:105) | a ∈ A, var(a) = v, val(a) = d}
v(cid:105) | o ∈ O, v ∈ vars(pre(o)), pre(o)[v] = d}
o(cid:105) | e ∈ effs(o)}
o(cid:105) | (cid:104)c, ·, ·(cid:105) ∈ effs(o), v ∈ vars(c), c[v] = d}
v(cid:105) | (cid:104)·, v, d(cid:105) ∈ effs(o)}.

The graph obtained from the lifted PDDL representations
is called the abstract structure graph (ASG) (Sievers et al.,
2019b). Planning tasks in PDDL can be naturally modeled
as abstract structures, which, in turn, can be represented
as graphs. In what follows we present the deﬁnitions of
abstract structures and abstract structure graphs, referring
the reader to Sievers et al. (2019b) for further details.

• each symbol s ∈ S is an abstract structure, and

• for

abstract

structures A1, . . . , An,

set
{A1, . . . , An} and the tuple (cid:104)A1, . . . , An(cid:105) are
abstract structures.

the

Using the language L of a PDDL task Π, each part of Π
can inductively be deﬁned as an abstract structure, with the
symbols of L forming the basic abstract structures. Finally,
abstract structures can be naturally turned into a graph.

Deﬁnition 3 (Sievers et al., 2019b) Let A be an abstract
structure over S. The abstract structure graph ASGA is a
digraph (cid:104)N, E(cid:105), deﬁned as follows.

• N contains a node A for the abstract structure A.
If N contains a node for A(cid:48) = {A1, . . . , An} or
A(cid:48) = (cid:104)A1, . . . , An(cid:105), it also contains the nodes for
A1, . . . , An.

• For every set (sub-)structure A(cid:48) = {A1, . . . , An} there

are edges A(cid:48) → Ai for i ∈ {1, . . . , n}.

• For every tuple (sub-)structure A(cid:48) = (cid:104)A1, . . . , An(cid:105),
1 , . . . , nA(cid:48)
n , an
for 1 < i ≤ n.
i → Ai.

the graph contains auxiliary nodes nA(cid:48)
i−1 → nA(cid:48)
1 , and edges nA(cid:48)
edge A(cid:48) → nA(cid:48)
For each component Ai, there is an edge nA(cid:48)

i

Note that the edges in ASGs are from the abstract structures
to their sub-structures, which results in acyclic graphs.

In both PDG and ASG, the node features are one-hot ac-
cording to the self-explanatory node type indicated in the
above deﬁnitions.

The aim in classical planning is to ﬁnd a sequence of ground
operators that, if applied to the initial state, will necessarily
transform it into a goal state. Such a sequence is called a

IPC: A Benchmark Data Set for Learning with Graph-Structured Data

plan. Assigning a quantitative cost to each ground operator,
the cost of a plan is deﬁned as the sum over the costs of
its operators. The goal of cost-optimal classical planning
is to ﬁnd a provably cheapest plan. There exist dozens if
not hundreds of highly parameterized methods for heuristic
guidance computation, giving rise to an enormous possi-
ble number of planners. As even the classical planning is
PSPACE-hard, there cannot be one planner that will work
well on all possible planning problems. Thus, ﬁnding a
planner that works well on a given planning problem is a
challenging task.

While planners are often domain-independent, in the sense
that they depend only on the information encoded in PDDL,
the planning tasks encode computational problems from
various domains. These domains range from puzzles or
one-person games (e.g., towers of Hanoi, 15-puzzle, free-
cell, and sokoban), to real-life domains (e.g., task planning
and automated control of autonomous systems: greenhouse
logistics, rovers, elevators, satellites), as well as emerging
domains (e.g., genome editing distance computation).

Many of the existing domains were introduced through
International Planning Competitions, which were held
regularly since 1998. Each such competition, intended
for comparing the performance of domain-independent
introduced new, previously unseen domains
planners,
In many
on which submitted planners were tested.
cases,
the authors of the domains supplied not only
the planning tasks, but also the generator that allowed
for creating additional tasks. Some of these generators
can be found at, e.g., https://bitbucket.org/
planning-researchers/pddl-generators.
Hence, these generators may be used to extend the current
data set with little effort, although for benchmarking
purpose we did not include any such task in the data set.

3. Statistics

A number of graph statistics, compared with those of com-
monly used datasets (Kersting et al., 2016) for benchmark-
ing graph kernels and graph neural networks, are reported in
Table 1 and Figures 2 and 3 in the supplementary material.
Observations follow.

1. The IPC graphs are signiﬁcantly larger. The graphs
in other data sets under comparison generally have tens
to hundreds of nodes, but 39% of the graphs in IPC-
grounded and 63% in IPC-lifted have over 1,000 nodes.
The largest graph in IPC-grounded has 87,140 nodes,
and the number for IPC-lifted is 238,909.

2. Note that the size of the largest graph is often the
memory bottleneck indicator for graph neural net-
works, because the batch size is at least this number

in stochastic training. Hence, our data set poses sub-
stantial challenges for the computation of many neural
graph models.

3. The sizes of the IPC graphs are highly skewed, com-
pared to those of other data sets. For many machine
learning tasks, especially in the unsupervised setting, the
notion of similarity is key to clustering and categoriza-
tion. When the sizes of two graphs signiﬁcantly differ,
the intuition of similarity is challenged. After all, what
does it mean by saying “a graph with 10 nodes is similar
to another graph with 100,000 nodes?”

4. The lifted graphs are the most sparse, compared to
the grounded ones and graphs in other data sets.

5. Similar to many other data sets, the IPC graphs
are not necessarily connected. However, the main
connected component generally dominates. Hence,
graph neural networks still suffer the memory bottleneck
caused by the exceedingly large graphs.

6. Despite the difference in size and density, the IPC
graphs have a moderate diameter, similar to other
data sets. The number of layers in a graph neural net-
work of neighborhood-aggregation style is often ques-
tioned beyond hyperparameter tuning; and speculation
attributes to the diameter of the graphs. Meanwhile, it
has been widely acknowledged that neighborhood aggre-
gation is a type of Laplacian smoothing and too many
layers lead to oversmoothing (Li et al., 2018; Xu et al.,
2018; Klicpera et al., 2019). The diameter statistics may
be useful for the analysis of the role of small-world struc-
tures handled by graph neural networks.

4. Example Use

For an illustration of the use of the data set, we focus on the
problem of cost-optimal planning, whose goal is to solve as
many tasks by using cost-optimal planners as possible, each
given a time limit T . Hence, for each of the 17 target values,
we convert it to 0 if the value ≤ T and 1 otherwise. For each
target, the problem becomes a binary classiﬁcation and thus
a probability value between 0 and 1 is output. We select
the planner corresponding to the smallest probability and
conﬁrm success if its actual planning time is smaller than the
timeout limit T . Test accuracy (percentage of successfully
solved tasks) is reported.

Three methods for comparison are (a) an image-based CNN
whereby the gray-scale image is converted from the adja-
cency matrix of the graph; (b) a graph convolutional network
(GCN) (Kipf & Welling, 2017) with attention readout; and
(c) a gated graph neural network (GG-NN) (Li et al., 2016).
For details of the CNN architecture, see Katz et al. (2018).

IPC: A Benchmark Data Set for Learning with Graph-Structured Data

Table 1. Statistics of IPC, compared with that of the other commonly used benchmark data sets.

REDDIT-MULTI-12k REDDIT-BINARY

IPC-grounded
directed
2,439
6,233,856
87,140
2,555.9 (6,099.0)
12.3 (131.0)
1.09 (0.61)
8.2 (2.3)

Type
#Graphs
Total #Nodes
Max #Nodes
Mean (Std) #Nodes
Mean (Std) Ave Degree1
Mean (Std) #CC2
Mean (Std) Diam3,4

undirected
11,929
4,669,116
3,782
391.4 (428.7)
4.7 (27.6)
2.81 (2.65)
10.9 (3.1)
1 “Ave Degree” is the average node degree (of the undirected version of the graph).
2 “CC” means connected components (of the undirected version of the graph).
3 “Diam” means diameter. Because a graph may consist of multiple connected components, we deﬁne the diameter

undirected
2,000
859,254
3,782
429.6 (554.1)
4.6 (41.3)
2.48 (2.47)
9.7 (3.1)

IPC-lifted
DAG
2,439
9,816,948
238,909
4,025.0 (14,507.6)
2.9 (35.1)
1.14 (0.49)
17.1 (1.5)

as the maximum of the diameters of each connected component.

4 For large graphs, the diameter is too costly to compute. Hence, for IPC, only the diameters of 94.3% of the graphs

are computed. For other data sets, diameters of all graphs are computed.

COLLAB
undirected
5,000
372,474
492
74.5 (62.3)
Mean (Std) Ave Degree1 132.0 (158.5)

Type
#Graphs
Total #Nodes
Max #Nodes
Mean (Std) #Nodes

Mean (Std) #CC2
Mean (Std) Diam3,4

1 (0)
1.9 (0.3)

NCI1
undirected
4,110
122,747
111
29.9 (13.6)
4.3 (1.6)
1.19 (0.57)
13.3 (5.1)

DD
undirected
1,178
334,925
5,748
106.5 (284.3)
10.1 (3.4)
1.02 (0.18)
19.9 (7.7)

PROTEINS ENZYMES MUTAG
undirected
undirected
undirected
188
600
1,113
3,371
19,580
43,471
28
126
620
18.0 (4.6)
32.6 (15.3)
39.1 (45.8)
4.4 (1.5)
7.6 (2.3)
7.5 (2.3)
1 (0)
1.24 (3.61)
1.08 (0.52)
8.2 (1.8)
10.9 (4.8)
11.6 (7.9)

The data set has been pre-split for training, validation, and
testing. Table 2 reports the test accuracy. Additionally,
we re-split the training/validation combination as a form of
cross validation, whereby we ﬁx the test set because it comes
from the most recent International Planning Competition.
Two forms of random re-splits are possible. One is to pre-
serve the domains of the planning tasks (i.e., tasks from the
same domain cannot appear in both training and validation),
and the other is free from this restriction. We call the former
domain split and the latter random split. For each type of
re-split, we perform ten randomizations. Table 3 reports
the test accuracy together with standard deviation. From
both tables, one sees that the lifted graphs yield much higher
accuracy and GCN outperforms the other two methods.

Table 2. Percentage of solved tasks in the test set.

Method Grounded

CNN
GCN
GG-NN

73.1%
80.7%
77.9%

Lifted
86.9%
87.6%
81.4%

Table 3. Percentage of solved tasks in the test set (lifted graphs).
Multiple training/validation splits.

Method Domain Splits Random Splits
86.1% (5.5%)
CNN
87.2% (3.5%)
GCN
74.4% (2.7%)

82.1% (6.6%)
85.6% (5.5%)
GG-NN 76.6% (5.8%)

5. Conclusions

We have described a new data set, IPC, for benchmarking
graph-based learning models (e.g., graph kernels and graph
neural networks) in classiﬁcation, regression, and related
uses. The graphs are constructed from AI planning tasks
appearing in International Planning Competitions, without
requiring human efforts for labeling, and may be extended
with random instances of planning problems. The data set
has distinctively different statistics from other popularly
used benchmarks: the graphs are much larger and their sizes
vary substantially. Moreover, the lifted version of the data
set is comprised of directed acyclic graphs, enabling the
development of specialized graph models. We anticipate
that the data set is a valuable inclusion to the current col-
lection of commonly used benchmarks for validating the
effectiveness of existing and forthcoming graph methods.

IPC: A Benchmark Data Set for Learning with Graph-Structured Data

References

B¨ackstr¨om, C. and Nebel, B. Complexity results for SAS+

planning. 11(4):625–655, 1995.

Helmert, M. Concise ﬁnite-domain representations for

PDDL planning tasks. AIJ, 173:503–535, 2009.

Katz, M., Sohrabi, S., Samulowitz, H., and Sievers, S. Delﬁ:
Online planner selection for cost-optimal planning. In
IPC-9 planner abstracts, 2018.

Kersting, K., Kriege, N. M., Morris, C., Mutzel, P.,
and Neumann, M. Benchmark data sets for graph
kernels, 2016. URL http://graphkernels.cs.
tu-dortmund.de.

Kipf, T. N. and Welling, M. Semi-supervised classiﬁcation

with graph convolutional networks. In ICLR, 2017.

Klicpera, J., Bojchevski, A., and Gnnemann, S. Predict
then propagate: Graph neural networks meet personalized
pagerank. In ICLR, 2019.

Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph
convolutional networks for semi-supervised learning. In
AAAI, 2018.

Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated

graph sequence neural networks. In ICLR, 2016.

McDermott, D. The 1998 AI Planning Systems competition.

21(2):35–55, 2000.

Pochter, N., Zohar, A., and Rosenschein, J. S. Exploiting
problem symmetries in state-based planners. In AAAI,
2011.

Shleyfman, A., Katz, M., Helmert, M., Sievers, S., and
Wehrle, M. Heuristics and symmetries in classical plan-
ning. In AAAI, 2015.

Sievers, S., Katz, M., Sohrabi, S., Samulowitz, H., and
Ferber, P. Deep learning for cost-optimal planning: Task-
dependent planner selection. In Proc. AAAI 2019, 2019a.

Sievers, S., Rger, G., Wehrle, M., and Katz, M. Theoretical
foundations for structural symmetries of lifted pddl tasks.
In Proc. ICAPS 2019, 2019b.

Xu, K., Li, C., Tian, Y., Sonobe, T., ichi Kawarabayashi, K.,
and Jegelka, S. Representation learning on graphs with
jumping knowledge networks. In ICML, 2018.

A. Additional Information of Graph Statistics

See Figures 2 and 3.

IPC: A Benchmark Data Set for Learning with Graph-Structured Data

(a) IPC-grounded

(b) IPC-lifted

(c) REDDIT-MULTI-12k (d) REDDIT-BINARY

(e) COLLAB

(f) NCI1

(g) DD

(h) PROTEINS

(i) ENZYMES

(j) MUTAG

Figure 2. Box plot of the graph size distribution. The orange bar marks the median; the red cross the mean.

(a) IPC-grounded

(b) IPC-lifted

(c) REDDIT-MULTI-12k (d) REDDIT-BINARY

(e) COLLAB

(f) NCI1

(g) DD

(h) PROTEINS

(i) ENZYMES

(j) MUTAG

Figure 3. Box plot of the diameter distribution. The orange bar marks the median; the red cross the mean.

05000001000002000000200002000200400050100025005000025050005010010201020152001020510151.01.52.0020402550750255002051015