Symbolic Regression in Materials Science:
Discovering Interatomic Potentials from Data

Bogdan Burlacu, Michael Kommenda, Gabriel Kronberger, Stephan Winkler,
Michael Aï¬€enzeller

Abstract Particle-based modeling of materials at atomic scale plays an important
role in the development of new materials and understanding of their properties.
The accuracy of particle simulations is determined by interatomic potentials, which
allow to calculate the potential energy of an atomic system as a function of atomic
coordinates and potentially other properties. First-principles-based ab initio potentials
can reach arbitrary levels of accuracy, however their aplicability is limited by their
high computational cost.

Machine learning (ML) has recently emerged as an eï¬€ective way to oï¬€set the high
computational costs of ab initio atomic potentials by replacing expensive models
with highly eï¬ƒcient surrogates trained on electronic structure data. Among a plethora
of current methods, symbolic regression (SR) is gaining traction as a powerful
â€œwhite-boxâ€ approach for discovering functional forms of interatomic potentials.

This contribution discusses the role of symbolic regression in Materials Science
(MS) and oï¬€ers a comprehensive overview of current methodological challenges
and state-of-the-art results. A genetic programming-based approach for modeling
atomic potentials from raw data (consisting of snapshots of atomic positions and
associated potential energy) is presented and empirically validated on ab initio
electronic structure data.

Heuristic and Evolutionary Algorithms Laboratory
University of Applied Sciences Upper Austria
Softwarepark 11, 4232 Hagenberg, Austria

1

2
2
0
2

l
u
J

1
2

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

2
v
2
2
4
6
0
.

6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

1 Introduction

Burlacu et al.

Materials Science (MS) is a highly interdisciplinary ï¬eld incorporating elements
of physics, chemistry, engineering and more recently, machine learning, in order
to design and discover new materials. The rapid increase in processing power over
the last decades has made computational modeling and simulation the main tool for
studying new materials and determining their properties and behavior. Computational
approaches can deliver accurate quantitative results without the need to set up and
execute highly complex and costly physical experiments.

Potential energy surfaces (PES), describing the relationship between an atomic
systemâ€™s potential energy and the geometry of its atoms, are a central concept in
computational chemistry and play a pivotal role in particle simulations. An example
PES for the water molecule is shown in Figure 1. The mathematical function used to
calculate the potential energy of a system of atoms with given positions in space and
generate the PES is called an interatomic potential function. The form it this function,
itâ€™s physical ï¬delity as well as its complexity and eï¬ƒciency are critical components
in simulations used to predict material properties.

The ability to simulate large particle systems over long timescales depends critically
on the accuracy and computational eï¬ƒciency of the interatomic potential. Broadly
speaking, the more accurate the methods, the lower its computational eï¬ƒciency and
the more limited its applicability. For example, ï¬rst-principle modeling methods such
as density functional theory (DFT) [33] provide highly accurate results by considering
quantum-chemical eï¬€ects but are not eï¬ƒcient enough to simulate large systems
containing thousands of atoms over long timescales of nanoseconds [43].

Molecular dynamics (MD) simulations treat materials as systems consisting
of many microscopic particles (atoms) which interact with each other through
interatomic potentials depending mainly on their positions and are governed by the
laws of statistical thermodynamics. Macroscopic properties of materials are obtained
as time and/or ensemble averages of processes emerging at the microscopic scale [27].
Empirical and semi-empirical methods treat atomic interactions in a more coarse-
grained manner via parameterized analytical functional forms and trade-oï¬€ accuracy
for execution speed in order to enable simulations at a larger scale. Although they are
computationally undemanding, they are only able to provide a qualitatively reasonable
description of chemical interactions [52].

Machine learning (ML) interatomic potentials aim to bridge the gap between
quantum and empirical methods in order to deliver the best of both worlds: functional
forms that are as eï¬ƒcient as empirical potentials and as accurate as quantum-chemical
approaches.

1.1 Materials informatics and data-driven potentials

Building upon the three established paradigms of science that have lead to many
technological advances over time: experimental, theoretical and simulation-based, a

Discovering Interatomic Potentials with Symbolic Regression

3

Fig. 1: PES for water molecule: Shows the energy minimum corresponding to
optimized molecular structure for water- O-H bond length of 0.0958nm and H-O-H
bond angle of 104.5Â°. Image from Wikipedia Â©AimNature

fourth â€œdata-drivenâ€œ paradigm of science is emerging today using machine learning
and the large amounts of experimental and simulation-data available [1]. â€œBig-dataâ€
science uniï¬es the ï¬rst three paradigms and opens up new avenues in materials
science under the umbrella term of materials informatics. The ï¬eld of material
informatics is very new and many unsolved questions still remain open and wait for
proper answers [26].

Machine learning interaction models are generated on the basis of quantum-
chemical reference data consisting of a series of snapshots of atomic coordinates,
associated potential energy of the system and optionally other properties.

In molecular dynamics simulations, the systemâ€™s potential energy is typically
decomposed into a set of independent ğ‘š-body interactions that are a function of each
particleâ€™s position, r. For a two-body or pair potential, it is assumed that the energy
contributions from each pair of interacting particles are independent of other pairs
and therefore:

âˆ‘ï¸

ğ¸ =

ğ‘”(rğ‘–, r ğ‘— )

For a three-body potential, triplets of atoms are also considered:

(cid:104)ğ‘–, ğ‘— (cid:105)

ğ¸ =

âˆ‘ï¸

(cid:104)ğ‘–, ğ‘— (cid:105)

ğ‘”(rğ‘–, r ğ‘— ) +

âˆ‘ï¸

(cid:104)ğ‘–, ğ‘—,ğ‘˜ (cid:105)

â„(rğ‘–, r ğ‘— , rğ‘˜ )

(1)

(2)

Traditionally, the functions ğ‘” and â„ are represented by all kinds of empirical
or semi-empirical analytical functions. With the advent of machine learning and
data-based modeling, it becomes possible to automatically search for these functional
forms with the help of ab initio training data. Substantial eï¬€ort has already been

4

Burlacu et al.

put into this direction and many machine learning models have been successful in
discovering interatomic potentials for a variety of chemical conï¬gurations [41].

1.2 Current challenges

Despite their success in representing atomic interactions, ML-methods are not without
their own challenges. Deriving highly-accurate and tractable analytic functional forms
for high-dimensional PESs is a very active ï¬eld of research. The most important
requirements for ML-based PESs are:

â€¢ general applicability and absence of ad-hoc approximations (transferability)
â€¢ accuracy close to ï¬rst-principles methods (including high-order many-body eï¬€ects)
â€¢ very high eï¬ƒciency to enable large simulations
â€¢
â€¢

the ability to describe chemical reactions and arbitrary atomic conï¬gurations
the ability to be automatically constructed and systematically improved

Currently available potentials are far from satisfying all the needs [6], mainly due

to the following diï¬ƒculties and shortcomings:

Physical plausibility

Closed physical systems are governed by various conservation laws that describe
invariant properties. These fundamental principles of nature provide strong constraints
that can be used to guide the search towards physically-plausible ML models [52].
In molecular systems each conserved quantity is associated with a diï¬€erentiable
symmetry of the action of a physical system.

Typical conserved quantities include temporal and roto-translational invariance
(i.e. total energy, linear and angular momentum). Forces must be the negative gradient
of the potential energy ğ¸ with respect to atomic positions ğ‘Ÿğ‘–:

ğ¹ğ‘– = âˆ’âˆ‡ğ‘Ÿğ‘– ğ¸

When atoms move, they always acquire the same amount of kinetic energy as they
lose in potential energy, and vice versa â€“ the total energy is conserved. The potential
energy of a molecule only depends on the relative positions of atoms and does not
change with rigit rotations or translations.

Another aspect of invariance is permutational invariance resulting from the fact
that from the perspective of the electrons, atoms with the same nuclear charge appear
identical to each other and can thus be exchanged without aï¬€ecting the energy or the
forces. To ensure physically meaninful predictions, ML-based models must exhibit
the same invariant behavior as the true potential energy surface.

Discovering Interatomic Potentials with Symbolic Regression

5

Accuracy

Accuracy is one of the most important requirements of ML potentials. The predicted
energies and forces should be as close as possible to the underlying ab initio data.
Numerical accuracy of the ML models is restricted by the intrinsic limitations of
their functional form and descriptors (input variables) used. For example, conceptual
problems related to incorporating rotational, translational and permutational invari-
ance into descriptors are of primary relevance [6, 21, 45, 46] as well as their optimal
design [20].

Transferability

Ideally, potentials should be generally applicable and should not be restricted to
speciï¬c types of atomic conï¬gurations. Due to their mathematical unbiased form, ML
methods are promising candidates to reach this goal. However in practice, developed
potentials often perform very well in applications they have been designed for, but are
too system-speciï¬c and thus cannot be easily transferred from one system to another.
The issues of extensibility, generality and transferability of the ML potentials need to
be explicitly addressed [6].

Complexity and data requirements

Another issue worth mentioning here is the mathematical complexity of ML potentials.
For example, the most popular ML methods used to represent many-body PESs,
ANNs, require complex architectures with many adjustable parameters (weights
of neural synapses and neuron biases) to yield suï¬ƒciently ï¬‚exible and invariant
PESs representations. For this, large amounts of training data (often dozens or
even hundreds of thousands points) are needed. On the other hand, the number of
training data should be kept as low as possible since they are are calculated via
demanding quantum-chemical methods. It means that as simple as possible analytic
representations of PESs are needed.

Integration of physical knowledge and interpretability

Related to the mathematical complexity issue, it is also important to note that most
of the ML methods (e.g., ANN, SVM) are of a â€œblack-boxâ€ nature and may be less
amenable to including physical information into the functional forms, relying at
least partially on physics-inspired features considered in atomic descriptors. This
often leads to increased mathematical and computational complexity of resulting
interaction models. One of the main directions of the current development in ML-
based computational MS is the shift from â€œblack-boxâ€ methods towards â€œwhite-boxâ€
methods which often oï¬€er better interpretability.

6

2 State of the art

Burlacu et al.

A plethora of machine learning approaches have recently emerged as a powerful
alternative for ï¬nding a functional relation between an atomic conï¬guration and
corresponding energy [6, 17, 23].

Several ML techniques such as polynomial ï¬tting [10], Gaussian processes [5],
spectral neighbor analysis [51], modiï¬ed Shepard interpolation [29], moment tensor
potentials [46], interpolating moving least squares [32], support vector machines [4],
random forests [31], artiï¬cial neural networks (ANNs) [15, 25, 45, 54] or symbolic
regression (SR) [39] have been successfully employed for a variety of systems.

More detailed reviews of current ML potentials can be found for example in [23],
[36], [41] or [52]. Particularly, ANNs have received a considerable attention and
are probably the most popular form of ML potentials used in MS [54]. However,
methods based on symbolic regression are gaining in popularity due to the advantages
they bring in solving aspects of physical knowledge integration, eï¬ƒciency and
interpretability [7, 8, 11, 12, 24, 37, 40, 44, 47].

In the following, we refer to symbolic regression in its canonical incarnation that
employs genetic programming to perform a search over the space of mathematical
expressions. Symbolic regression approaches have succeeded in rediscovering simple
forms of potentials that deliver qualitatively good results in a series of speciï¬c
applications, some of which are described below.

2.1 Directed search

The goal of directed search is to improve search eï¬ƒciency by limiting the hypothesis
space to a functional form known to deliver qualitatively good results, instead of
searching for a brand new potential.

Makarov and Metiu [37] use the Morse potential as a functional template for

modeling diatomic molecules (see Section 5, Eq. 17). They rewrite it in the form:

ğ‘€ (cid:0)ğ· (ğ‘Ÿ), ğ‘…(ğ‘Ÿ)(cid:1) = ğ· (ğ‘Ÿ) (cid:0)1 âˆ’ exp (cid:0)ğ‘…(ğ‘Ÿ)(cid:1)(cid:1) 2

(3)

and use genetic programming to ï¬nd the best ğ· (ğ‘Ÿ) and ğ‘…(ğ‘Ÿ).

The directed search approach is augmented with an error metric that better reï¬‚ects
the physical characteristics of the problem. A standard error metric such as the MSE
has the disadvantage of overemphasizing high-energy points which are rarely used
during simulation. For this reason, the authors found it advantageous to introduce a
scaling factor:

ğ¹ (ğ‘) =

(cid:0)ğ¸ (ğ‘Ÿğ‘–) âˆ’ ğ‘“ (ğ‘Ÿğ‘–; ğ‘)(cid:1) 2
ğ¸ 2(ğ‘Ÿğ‘–) + ğ›¿2

âˆ‘ï¸

ğ‘–

(4)

where the constant ğ›¿ is added to prevent division by zero.

Discovering Interatomic Potentials with Symbolic Regression

7

For each function ğ‘“ğ›¼ in the population of individuals, the ï¬tness function is then

deï¬ned as:

ğ‘ ğ›¼ = exp (cid:0) âˆ’ ğ›½ğ¹ğ›¼(cid:1)
where parameter ğ›½ controls how discriminating the function is and is adaptively
updated during the run. The search starts with a small value for ğ›½ which is gradually
increased as the search improves.

(5)

The authors note the importance of including the derivative of the energy in the

training data:

ğ¹ (cid:48)(ğ‘) =

|âˆ‡ğ¸ (ğ‘Ÿğ‘–) âˆ’ âˆ‡ ğ‘“ (ğ‘Ÿğ‘–; ğ‘)|2
|âˆ‡ğ¸ (ğ‘Ÿğ‘–)|2 + ğ›¿(cid:48)2

âˆ‘ï¸

ğ‘–

Leading to an expanded ï¬tness function ğ‘ ğ›¼:

ğ‘ ğ›¼ = exp (âˆ’ğ›½(ğ¹ + ğ¹ (cid:48)))

(6)

(7)

The recombination pool is ï¬lled using a proportional selection scheme. An
additional â€œnatural selectionâ€ operator employs a â€œbadness listâ€ ğ‘ ğ›¼ = exp(ğ›½ğ¹ğ›¼)
whose elements are the inverse of the ï¬tness. Old individuals are replaced with a
probability proportional with badness.

Results

The directed search approach is is shown to perform better than an undirected
search over the search space, on training data generated using the Lippincott potential
(Section 5, Eq. 19). A population size of 500 individuals is evolved over 150 generations
(75,000 evaluations) using the primitive set P = {+, âˆ’, Ã·, Ã—, exp}. Furthermore, a
search directed by a Lennard-Jones potential gives accuracy comparable to that
directed by a Morse function, suggesting that restricting the hypothesis space with
an appropriate functional template is a powerful and general approach in the search
for interatomic potentials. In the case of the Lennard-Jones potential (Section 5,
Equation 18) the functional template was deï¬ned as

ğ‘“ (ğ‘Ÿ) = 4ğ· (ğ‘Ÿ)

(cid:34) 1
4

+

(cid:16) 1
ğ‘…(ğ‘Ÿ)

(cid:17) 12

âˆ’

(cid:16) 1
ğ‘…(ğ‘Ÿ)

(cid:17) 6(cid:35)

(8)

The authors additionally note that some of the returned models, although accurate,
exhibited unphysical behavior and did not extrapolate well. For example, one of the
returned models based on the Lennard-Jones functional form had very good accuracy
but contained a singularity at ğ‘Ÿ = 12 Ã…, a point outside the interpolation range. The
authors address overï¬tting by ï¬tting the parameters of both the energy function and its
derivative in the local search phase. This reduces the chance of obtaining pathological
curves in the model extrapolation response.

8

Burlacu et al.

Finally, Makarov and Metiu also model the potential of a triatomic molecule on
ab initio data consisting of 60 nuclear conï¬gurations, showing that directed search
maintains high levels of accuracy and scales favorably with dimensionality.

2.2 Directed Search with Parallel Multilevel Genetic Program

Belluci and Coker [7, 8] employ symbolic regression to discover empirical valence
bond (EVB) models using directed search augmented with a multilevel genetic
programming approach: the lower level (LLGP) optimizes co-evolving populations
of models, while the higher level (HLGP) optimizes genetic operator probabilities
of the lower level populations. The approach entitled Parallel Multilevel Genetic
Program (PMLGP) found accurate EVB models for proton transfer in 3-hydroxy-
gramma-pyrone (3-HGP) in gas phase and protic solvent as well as ultrafast enolketo
isomerisation in the lowest singlet excited state of 3-hydroxyï¬‚avone (3-HF).

At the lower level (LLGP), the authors use the same error metric and ï¬tness as
in [37], namely Equations 4 and 5. LLGP individuals represent the ğ‘…(ğ‘Ÿ) functional
part of the Morse potential (see Equation 3). Remarkably, PMLGP does not use
crossover but instead uses six diï¬€erent mutation operators:

â€¢ Point mutation randomly replaces a subtree with a randomly-generated one.
â€¢ Branch mutation replaces a binary operator with one of its arguments at random.
â€¢ Leaf mutation replaces a leaf node with another randomly selected leaf.
â€¢ New tree mutation replaces an entire tree with a newly generated tree.
â€¢ Parameter change replaces each parameter value ğ‘ğ‘– with ğ‘ğ‘– + (ğ‘… âˆ’ 0.5)ğ›¾, where
ğ‘… is a uniform random number on the unit interval and ğ›¾ is a scaling constant.
â€¢ Parameter scaling replaces each parameter value ğ‘ğ‘– with ğ‘ğ‘– ğ‘…ğ›¾, where ğ‘… is a

uniform random number on the unit interval and ğ›¾ is a scaling constant.

Of the last two types of mutation, parameter change is designed to make small local
moves in parameter space, while parameter scaling is designed to make large moves
in parameter space as to escape the basins of attraction of local optima. Selection is
performed using stochastic universal sampling [3].

At the higher level (HLGP) a real vector encoding is used to represent genetic
operator probabilities. The population is initialized with ğ‘˜ random vectors ğ‘ƒğ‘˜ =
(cid:16)
ğ‘ (ğ‘˜)
ğ‘– = 1, where ğ‘˜ ranges from 1 to the total number ğ‘ ğ‘ of
1
processors, such that each vector corresponds to one of the LLGP populations whose
operator probabilities it dynamically adapts.

(cid:17), with (cid:205)ğ‘– ğ‘ (ğ‘˜)

, ..., ğ‘ (ğ‘˜)

6

The ï¬tness of each vector ğ‘ƒğ‘˜ is evaluated based on the maximum ï¬tness delta in

the corresponding LLGP population over a speciï¬ed time interval Î”ğ‘¡:

ğ¹HLGP
ğ‘˜

=

Î”ğ¹LLGP
max
Î”ğ‘¡

(9)

Discovering Interatomic Potentials with Symbolic Regression

9

This is based on the idea that the larger the magnitude of ğ¹HLGP
the set of probabilities ğ‘ƒğ‘˜ at improving the ï¬tness of the population.

ğ‘˜

, the more successful

Two genetic operators are used to modify the probability vectors ğ‘ƒğ‘˜ :

â€¢ Mutation changes each component of the vector by a random amount with the
constraint that all components sum up to one. This operator kicks in when the
ï¬tness of a vector ğ‘ƒğ‘˜ drops below a given threshold.

â€¢ Adaptation attempts to improve the probability distribution given by ğ‘ƒğ‘˜ by using
feedback from the LLGP. Each LLGP builds a histogram of the number of times
each mutation produced the most ï¬t member of the population. Then the success
frequency of the mutation operator is given by:

ğ‘ ğ‘– =

ğ‘¤ğ‘–ğ‘šğ‘–
ğ‘›

, ğ‘¤ğ‘– =

1
ğ‘ğ‘–

, ğ‘› =

âˆ‘ï¸

ğ‘šğ‘–

ğ‘–

Here, ğ‘¤ğ‘– is a weight, ğ‘šğ‘– is the number of successful mutations for the ğ‘–th operator
(component of ğ‘ƒğ‘˜ ) and ğ‘› is the total number of successful mutations (for all
operators). Based on the success frequencies, adaptation shifts a random amount
of probability from the least successful operator to the most successful operator.

The number of LLGP populations (and HLGP individuals, respectively) is set to
the number of available processors. Initially, all LLGP populations are identical but
diverge during evolution as each corresponding ï¬tness function is parameterized with
a diï¬€erent value of ğ›½ evenly sampled over a speciï¬ed range. In eï¬€ect, this applies
diï¬€erent selection pressures on each LLGP population. Migrations are performed
after the last adaptation step in HLGP. At this point, copies of the ï¬ttest individual in
each LLGP population are sent to all the other populations, where they replace the
least ï¬t individual.

Results

Training data for ï¬ve diï¬€erent diatomic molecules (CO, H2, HCl, N2, O2) was
generated using diï¬€erently parameterized Morse functions, Gaussian functions and
double well functions. The corresponding directed search spaces are given by:

ğ¹ğ‘€ = ğ· (cid:0)1 âˆ’ exp(âˆ’ğ‘…(ğ‘Ÿ; ğ‘))(cid:1) 2
ğ¹ğº = ğ´ exp (cid:0)ğ‘…(ğ‘Ÿ; ğ‘)2(cid:1)
ğ¹ğ· = ğ·1 (cid:0)1 âˆ’ exp(âˆ’ğ‘…1 (ğ‘Ÿ; ğ‘))(cid:1) 2

+ ğ‘

Morse (10)

Gaussian (11)

+ ğ·2 (cid:0)1 âˆ’ exp(âˆ’ğ‘…2 (ğ‘Ÿ; ğ‘))(cid:1) 2 Double well

(12)

Parameters ğ·, ğ‘, ğ´, ğ·1 and ğ·2 are optimized by including them as leaves in the trees.
The PMLGP approach was compared against a standard parallel genetic pro-
gramming implementation (SPGP). In both cases, populations of 500 individu-
als were evolved in parallel on 8 processors for 20,000 generations. The func-
tion set F = {+, âˆ’, Ã—, Ã·, exp} was used for internal nodes and the terminal set
T = {ğ‘Ÿ, ğ‘1, ..., ğ‘10} was used for the leaf nodes.

10

Burlacu et al.

PMLGP was shown to converge faster and achieve higher accuracy than SPGP.
The obtained model of the EVB surface accurately reproduced global features of the
ab initio data. The approach provides a basis for high quality many-body potentials
for studying gas and solution phase photon reactions.

2.3 Parallel tempering

Slepoy, Peters and Thompson [47] use a hybrid approach consisting of genetic
programming, Monte Carlo sampling and parallel tempering to discover the functional
form of the Lennard-Jones pair potential.

Parallel tempering is an approach for parallel genetic programming where several
islands (or replicas) evolve at a diï¬€erent eï¬€ective temperature. High eï¬€ective
temperatures favor exploration by accepting new trees even if their ï¬tness is poor,
low eï¬€ective temperatures favor exploitation by being sensitive to small changes
in ï¬tness. By using replicas at diï¬€erent temperatures the approach simultaneously
performs both exploitation and exploration.

The remarkable aspect about this approach is that it marks the ï¬rst large-scale
application of genetic programming in materials science with interesting extensions
to the canonical Koza-style algorithm and without restrictions of the hypothesis space.
The training data used consists of 10 nuclear conï¬gurations of 10 particles placed
in 3-d space. The Lennard-Jones potential describes the interations between pairs of
particles, therefore a nuclear conï¬gurationâ€™s energy is given by the sum of pairwise
potentials:

ğ¸conf =

âˆ‘ï¸

ğ‘‰LJ (ğ‘Ÿğ‘– ğ‘— )

(13)

<ğ‘–, ğ‘—>

where ğ‘Ÿğ‘– ğ‘— = (cid:107)rğ‘–, r ğ‘— (cid:107) is the distance between particles ğ‘– and ğ‘—. Fitness is deï¬ned as
the negative mean squared error.

The evolutionary search is organized as a three-stage process consisting of:

generation, mutation and testing.

Oï¬€spring individuals are tested for acceptance into the new population. A new
tree is unconditionally accepted if its ï¬tness exceeds the old one at the same index.
Otherwise, it is accepted with the Boltzmann probability:

(cid:40)

ğ‘ƒaccept = min

1, exp

(cid:33)(cid:41)

(cid:32) ğ¹new âˆ’ ğ¹old
ğ‘‡

where ğ¹old and ğ¹new are the old and new ï¬tness values, and ğ‘‡ is the eï¬€ective
temperature.

After each generation, each sub-population exchanges one tree with its left
neighbour in temperature space and one tree with its right neighbor. The trees to be
swapped are selected with equal probability from their respective populations. The
tree swap is accepted with a probability based on the relative Boltzmann weights of
the two trees.

Discovering Interatomic Potentials with Symbolic Regression

11

(cid:40)

ğ‘ƒacc = min

1, exp

(cid:20)(cid:18) 1
ğ‘‡ğ‘–

âˆ’

1
ğ‘‡ğ‘–+1

(cid:19) (cid:16)

(cid:17) (cid:21) (cid:41)

ğ¹ğ‘–+1 âˆ’ ğ¹ğ‘–

Results

A large scale experiment was performed on a cluster made of 100 AMD Opteron 2.2
Ghz processors. The trees were restricted to minimum depth 3 and maximum depth 4.
200 replicas with temperatures distributed logarithmically from 0.1 to 10 were used.
The replica size was chosen to be either ğ‘ = 10,000 or ğ‘ = 50,000 individuals. The
primitive set consists of elementary operations P = {+, âˆ’, Ã—, Ã·, exp, | Â· |}.

The proposed approach successfully discovered the Lennard-Jones potential or
arithmetic equivalents within 100 generations. Interestingly, the expended eï¬€ort was
estimated to be somewhere in the range of 109 evaluated trees, which represents only
a small fraction of the possible trees with depth 4 (around 2.9 Ã— 1036) [47].

A number of ideas for improving the physical ï¬delity of the developed functional

forms and their generality and transferability are suggested

Inclusion of additional properties and forces on individual atoms in the training set

â€¢
â€¢ Primitive set extension to include three-body interactions
â€¢

Integration of physical knowledge (inclusion of symmetries, invariances)

2.4 Symbolically-Regressed Table KMC

In order to increase the time scale of simulations, molecular dynamics can be combined
with kinetic (dynamic) Monte Carlo (KMC) techniques [9] that coarse-grain the
state space, for example via discretization (e.g. assign an atom to a lattice site). The
main assumption is that multiscale modeling requires only relevant information at
the appropriate length or time scale.

KMC constructs a look-up table consisting of an a priori list of events such as
atomic jumps or oï¬€-lattice jumps. This yields several order of magnitude increases
in simulated time and allows to directly model many processes unaproachable by
MD alone. However, identifying barrier energies from a list of events is diï¬ƒcult and
restricts the applicability of the method.

Here, symbolic regression is proposed to identify the functional form of the
potential energy surface at barrier energy points from a limited set of ab initio
training data.The method entitled Symbolically-Regressed Table KMC (sr-KMC)
[44] provides a machine learning replacement for the look-up table in KMC, thus
removing the need of explicit calculation of all activation barriers.

Sastry [44] show that symbolic regression allows atomic-scale information (diï¬€u-
sion barriers on the potential energy surface) to be included in a long-time kinetic
simulation without maintaining a detailed description of the all atomistic physics, as
done within molecular dynamics.

12

Burlacu et al.

In this approach, ï¬tness is computed as a weighted mean absolute error between

the predicted and calculated barriers, for ğ‘ random conï¬gurations:

ğ¹ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ‘¤ğ‘–

(cid:12)Î”ğ¸pred (xğ‘–) âˆ’ Î”ğ¸calc (xğ‘–)(cid:12)
(cid:12)
(cid:12)

(14)

Setting ğ‘¤ğ‘– = |Î”ğ¸calc|âˆ’1 gives preference to predicting accurately lower-energy

(most signiï¬cant) events over higher energy events.

The algorithm uses the ramped-half-and-half tree creation method, tournament
selection and Koza-style subtree crossover, subtree mutation and point mutation [34].

Results

sr-KMC is applied to the problem of vacancy-assisted migration on the surface of
phase-separating ğ¶ğ‘¢ ğ‘¥ğ¶ğ‘œ1âˆ’ğ‘¥ at a concentrated allow composition (ğ‘¥ = 0.5). Two
types of potentials (Morse and TB-SMA) are used to generate the training data
via molecular dynamics. The number of active conï¬gurations is limited knowing
that only atoms in the environment locally around vacancy and migrating atoms
signiï¬cantly inï¬‚uence the barrier energies.

The inline barrier function is represented from the primitive set P = F âˆª T , with
F = {+, âˆ’, Ã—, Ã·, pow, exp, sin} and T = {x, R}. Here, x represents the current active
conï¬guration and R is an emphemeral random constant.

The results show that GP predicts all barriers within 0.1% error while using less
than 3% of the active conï¬gurations for training. This leads to a signiï¬cant scale-up
in real simulation time and a signiï¬cant reduction in the CPU time needed for KMC.
sr-KMC is also compared against the basic KMC approach (using a table look-up)
where it was shown to perform orders of magnitude faster.

The authors note that standard basis-set regression methods are generally not
competitive to GP due to the inherent diï¬ƒculty in choosing appropriate basis functions
and show that quadratic and cubic polynomials perform worse in terms of accuracy
(within 2.5% error) while requiring energies for âˆ¼ 6% of the active conï¬gurations.
They also note that GP is robust to changes in the conï¬guration set, the order in
which conï¬gurations are used or the labeling scheme used to convert the conï¬guration
into a vector of inputs.

2.5 Hierarchical Fair Competition

Brown, Thompson and Schultz [11, 12] are able to rediscover the functional forms of
known two- and three-body interatomic potentials using a parallel approach to genetic
programming with extensions towards better generalization. Their implementation is
based on Hierarchical Fair Competition (HFC) by Jianjun et al. [28].

Discovering Interatomic Potentials with Symbolic Regression

13

The HFC framework [28] is designed towards maintaining a continuous supply of
fresh genetic diversity in the population and protecting intermediate individuals who
have not reached their evolutionary potential from being driven to extinction by unfair
competition. It implements these goals with the help of a hierarchical population
structure where individuals only compete with other individuals of similar ï¬tness.

Brown et al. note that a correlation-based ï¬tness measure would increase the
eï¬ƒciency of the search and propose the following formula using the Pearson
correlation coeï¬ƒcient:

ğ¹ =

ğ‘ + 100 âˆ’ 100

ğ‘
ğ‘
âˆ‘ï¸

ğ‘–=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ğ‘¦ğ‘– âˆ’ Â¯ğ‘¦)( Ë†ğ‘¦ğ‘– âˆ’ Â¯Ë†ğ‘¦)
ğ‘ğ‘–ğœğ‘¦ Â· ğ‘ğ‘–ğœË†ğ‘¦

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(15)

Here, ğ‘ is the number of conï¬gurations and ğ‘ğ‘– is the number of terms in the
summation over ğ‘” (see Equation 1). Ordinary least squares is then used to ï¬t the
prediction Ë†ğ‘¦ to the data by introducing scale and intercept terms to the functions ğ‘”
and â„:

âˆ‘ï¸

ğ¸ =

(cid:0)ğ‘ Â· ğ‘”(rğ‘–, r ğ‘— ) + ğ‘(cid:1) +

âˆ‘ï¸

(cid:0)ğ‘ Â· â„(rğ‘–, r ğ‘— , rğ‘˜ ) + ğ‘‘(cid:1)

(16)

(cid:104)ğ‘–, ğ‘— (cid:105)

(cid:104)ğ‘–, ğ‘—,ğ‘˜ (cid:105)

The approach is implemented in pm-dreamer, an open-source software package
developed on top of the Open Beagle library for evolutionary computation [19], using
its available genetic operators. These include several mutations (standard, shrink,
swap, constant), subtree-swapping crossover, tournament selection and elitism:

â€¢ Standard mutation replaces a node in the tree with a randomly-generated subtree.
â€¢ Swap mutation swaps two nodes in the tree.
â€¢ Shrink mutation replaces a subtree with one of its arguments.
â€¢ Swap subtree mutation swaps a subtreeâ€™s arguments.
â€¢ Ephemeral mutation changes the value of a constant in the tree.

Additionally, pm-dreamer implements support for distributed evolution using the
MPI standard and introduces migration operators that exchange individuals between
sub-populations at ï¬xed intervals.

Bloat reduction strategies are implemented to prevent the expression trees from
becoming increasingly large, a tendency observed especially in the case of three-body
modeling. Two strategies are tested:

â€¢ Using a simpliï¬cation operator which replaces subtrees that evaluate to a constant
value with the constant value. This operator is applied generationally at a ï¬xed
interval.

â€¢ Using penalty terms to the ï¬tness function: in this case the ï¬tness is decreased
based on a threshold penalty size value ğ‘ ğ‘ and a maximum penalty size ğ‘ ğ‘’, such
that trees with length < ğ‘ ğ‘ are not penalized at all, and trees with length > ğ‘ ğ‘’ are
penalized fully (ï¬tness is set to zero).

14

Burlacu et al.

Local search. Local search based on the derivative-free Nelder-Mead simplex algo-
rithm is employed with a set probability, optimizing either a single constant or all the
constants in the expression.
HFC Extension. Brown et al. implement HFC in a parallel manner by allowing
populations with diï¬€erent ï¬tness thresholds to evolve in parallel, with periodic
migrations between them. After migrations, populations that grow too large are
â€œdecimatedâ€ by removal of the least ï¬t individuals, while populations that grow too
small are supplemented with new randomly-generated individuals.

The population ï¬tness thresholds are adapted during the search using two strategies:
the ï¬rst strategy uses a percentile parameter ğ‘ which determines the ï¬tness threshold
such that ğ‘ percent of individuals have equal or lower ï¬tness. The second strategy
uses ï¬xed thresholds determined by the ï¬rst non-zero threshold along with a scaling
parameter equal to the ratio between successive thresholds.

Results

Training data for two- and three-body interactions was generated using the Lennard-
Jones and Stillinger-Weber potentials (see Section 5 Eqs. 18, 20). In both cases, ï¬ve
conï¬gurations were used for training and 50 conï¬gurations were used for testing, in
order to realistically represent the problem of obtaining models for condensed phases
from a small training set. The generated data includes pairwise distances between
atoms, the energy and the force on a single atom.

The authors compare a standard parallel island-based evolutionary model against
a parallel HFC evolutionary model, using 32 islands with a population of 10 000
individuals each, evolved over a period of 100 generations.

The primitive set used was P = {+, âˆ’, Ã—, Ã·, pow, exp, log, | Â· |}, tournament selec-
tion was used with a tournament size of 6 and, in the case of the standard evolutionary
model, 500 individuals were migrated between islands every 5 generations. For the
HFC evolutionary model, the migration took place every generation, the ï¬rst ï¬tness
threshold was set to 0.1 and the threshold ratio was set to 1.0. A detailed description
of the other algorithm parameters is given in [12].

After an initial tuning phase, the authors note that the number of interactions per
energy point greatly increases the runtime requirements for optimization. The C++
implementation of pm-dreamer is capable of doing vectorized evaluation of two-
and three-body interatomic potential models using SIMD instructions in a manner
similar to batched tree interpretation more typically used in GP. With vectorization,
the evolutionary algorithm was found to perform roughly four times faster.

Local search was performed with varying probability on all constants in an expres-
sion using a maximum of 6 iterations of the Nelder-Mead algorithm. Simpliï¬cation
is performed every 20 generations.

Overall, the authors show that the HFC strategy consistently outperforms the
standard generational evolutionary strategy and is able to ï¬nd very accurate approxi-
mations for the targeted empirical potentials (Lennard-Jones and Stillinger-Weber).

Discovering Interatomic Potentials with Symbolic Regression

15

2.6 Potential Optimization by Evolutionary Techniques (POET)

POET [24] distinguishes itself from previously described approaches through an
extended primitive set which includes summation symbols that aggregate local energy
values around each atom, smoothing functions meant to exploit the â€œshort-sightednessâ€
of atomic interactions as well as leaf nodes representing the atomic neighborhood
interaction radius.

The primitive set used by the algorithm consists of the function set F =
{(cid:205), ğ‘“ , +, âˆ’, Ã—, Ã·, pow} and terminal set T = {R, ğ‘Ÿ}. Here, (cid:205) are summation symbols,
ğ‘“ are smoothing symbols, R represents an ephemeral constant and, like before, ğ‘Ÿ rep-
resents the distance between atoms. Distances are considered within the neighborhood
of each atom according to inner and outer cutoï¬€ radii ğ‘Ÿin and ğ‘Ÿout.

An exemplary POET-tree including the special symbols (cid:205), ğ‘“ and ğ‘Ÿ is shown in
Figure 2. This tree corresponds to the following function which returns the predicted
value of the local energy ğ¸ğ‘– around the ğ‘–th atom considering the distances ğ‘Ÿğ‘– ğ‘— to its
neighbors:

ğ¸ğ‘– = 7.51 âˆ‘ï¸

ğ‘Ÿ 3.98âˆ’3.93ğ‘Ÿğ‘– ğ‘—
ğ‘– ğ‘—

ğ‘“ (ğ‘Ÿğ‘– ğ‘— )

ğ‘—

(cid:32)

+

28.01 âˆ’ 0.03 âˆ‘ï¸

ğ‘Ÿ 11.73âˆ’2.93ğ‘Ÿğ‘– ğ‘—
ğ‘– ğ‘—

ğ‘“ (ğ‘Ÿğ‘– ğ‘— )

ğ‘—

(cid:33) âˆ’1

ğ‘“ (ğ‘Ÿğ‘– ğ‘— )

(cid:33) (cid:32)

âˆ‘ï¸

ğ‘—

Hernandez et al. employ a parallel version of genetic programming where twelve
populations are evolved simultaneously. The recombination pool in each population
is ï¬lled from three separate sets of models: a set from the current population, a
global set maintained with the overall best (non-dominated) individuals with regard
to ï¬tness, complexity and execution speed, and a set of individuals from the other
populations. These sets are periodically ï¬lled up with individuals at preset intervals.
New individuals are generated by means of crossover and mutation. Crossover
replaces a random subtree in the root parent with either another random subtree from
another parent or with a linear combination of random subtrees from two diï¬€erent
parents. The ï¬rst method was applied with probability 0.9 while the second method
was applied with probability 0.1.

Mutation can replace a subtree with a randomly generated one, swap the argu-
ments of non-commutative symbols or change the symbols of function nodes. Tree
initialization is done using Kozaâ€™s ramped-half-and-half method where the tree depth
is sampled from a Gaussian distribution with ğœ‡ = 5 and ğœ = 1.

Local optimization of model coeï¬ƒcients is performed online during the run with
the help of a covariance matrix adaptation evolution strategy (CMA-ES) optimizer
and a conjugate gradient (CG) optimizer. CMA-ES is used to optimize the coeï¬ƒcients
of models in the global set every 10,000 crossover and mutation operations. The
CG algorithm is used to perform one optimization step for every newly generated
individual.

16

Burlacu et al.

+

Ã—

Ã·

7.51

âˆ’

(cid:205)

28.0

Ã—

ğ‘“ (ğ‘Ÿ )

ğ‘“ (ğ‘Ÿ )

0.03

(cid:205)

Ã—

âˆ’

âˆ§

ğ‘Ÿ

3.98

Ã—

âˆ§

3.93

ğ‘Ÿ

ğ‘Ÿ

(cid:205)

Ã—

âˆ’

ğ‘“ (ğ‘Ÿ )

11.7

Ã—

2.93

ğ‘Ÿ

Fig. 2: Interatomic potential obtained by Hernandez et al. [24], representing local
energy ğ¸ğ‘– around the ğ‘–th atom in electron volts. The expression resembles that of an
embedded atom model with a pairwise repulsive term and a many-body attractive
term formed by a non-linear transformation of pairwise interactions.

Results

The proposed approach is validated using training data from DFT molecular dynamics
simulations containing snapshots of atomic positions, energies, forces and stresses
for an atomic system of 32 Cu atoms. The ï¬tness measure is an aggregation of the
energy, force and stress errors:

ğ¹ = 1000 Â· (cid:0)0.5MSEenergy + 0.4MSEforce + 0.1MSEstress(cid:1)

The authors demonstrate POETâ€™s ability to rediscover Lennard-Jones and Sutton-
Chen potentials. The generated models displayed low overï¬tting and high general-
ization being able to maintain high predictive accuracy for properties on which they
were not trained. The simplicity of the models allows them to predict energies with
speeds in the order of microseconds per atom, about 1-4 orders of magnitude faster
than other ML potentials. The authors also note that such simple models bring the
additional advantage of requiring relatively small amounts of training data.

Discovering Interatomic Potentials with Symbolic Regression

17

In terms of runtime performance of POET itself, the authors report 330 CPU-hours
spent ï¬nding the exact Lennard-Jones potential, 3600 CPU-hours ï¬nding the exact
Sutton-Chen potential and 360 CPU-hours to ï¬nd the three best performing models
reported in [24]. POET code is open-source and available online1.

2.7 Other applications

Makarov and Metiu [38] use directed genetic programming to ï¬nd analytic solutions
to the time-independent SchrÃ¶dinger equation. The training data is generated by
inverting the SchrÃ¶dinger equation such that the potential is a functional depending
on the wave function and the energy.

Kenouï¬ and Kholmurodov [30] used symbolic regression to rediscover the
Lennard-Jones potential and discovered a new potential for an argon dimer, using ab
initio data from DFT simulations.

Mueller et al. [40] used symbolic regression for discovering relevant descriptors in
hydrogenated nanocrystalline silicon with very low crystalline volume fraction, with
applications in improving optical absorption eï¬ƒciency in thin-ï¬lm photovoltaics.

Wang et al. [53] used symbolic regression to discover the Johnson-Mehl-Avrami-
Kolmogorov transformation kinetics law in the recrystallization process of copper; and
the Landau free energy functional form for the displacive tilt transition in perovskite
LaNiO3.

Eldridge et.al [18] used the NSGA-III algorithm to learn interatomic potentials
for carbon. The approach considered training error, individual age and individual
complexity as objectives and was able to ï¬nd simple and accurate potential functions.

2.8 Summary discussion

State-of-the-art SR approaches for modeling interatomic potentials recognize the
need for domain-speciï¬c extensions and hybridizations towards promoting physical
plausibility and achieving high accuracy, while making the most out of the usually
scarce quantities of available ab initio training data.

Several extensions and hybridizations are used to augment the classic (Koza-style)
genetic programming algorithm and increase its search performance. Parallel, island-
based approaches are employed in all of the discussed methods, on the one hand, to
more eï¬ƒciently search the hypothesis space and on the other hand, to achieve higher
throughput and alleviate the high computational costs of summations over two- and
three-body atomic interactions.

Physical plausibility is promoted by restricting the hypothesis space (directed
search), including domain speciï¬c information into the ï¬tness function (e.g. weighing
down high-energy points) or including additional targets (forces and stresses).

1 https://gitlab.com/muellergroup/poet

18

Burlacu et al.

The results achieved so far have demonstrated the ability of symbolic regression to
discover highly accurate and physically-plausible functional forms which can increase,
due to their simplicity and eï¬ƒciency, the performance of particle simulations (allowing
them to run at larger scales or for longer times). At the same time, since the models
are inherently more simple than similar black-box models such as ANNs, they tend
to require a lesser amount of training data, which increases their applicability.

Overall, it can be concluded that symbolic regression represents a very promising
approach for discovering more accurate and eï¬ƒcient potentials. However, designing
evolutionary systems for this application area requires consideration of speciï¬c
challenges as described in Section 1.2. In the following section we discuss several
ideas towards a GP system design which is able to address the domain-speciï¬c
requirements of interatomic potential.

3 Designing GP for modeling interatomic potentials

The problem of modeling interatomic potentials from data has the main particularity
that data comes in the form of atomic conï¬guration snapshots. Each conï¬guration
describes the positions of the atoms, its energy and optionally other properties (forces,
stresses). Canonically, these data snapshots are generated by molecular dynamics
simulation packages such as LAMMPS [42] or VASP [35] and come in speciï¬c
formats, see e.g. POSCAR2.

At the minimum, raw data has the following form:

r(1)
1
...
r( ğ‘ )
1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Â· Â· Â· r(1)
ğ‘€ ğ¸ (1)
...
...
Â· Â· Â· r( ğ‘ )
ğ‘€ ğ¸ ( ğ‘ )

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

where r(ğ‘˜)
potential energy value.

ğ‘–

is the position of the ğ‘–th atom in the snapshot ğ‘˜ and ğ¸ (ğ‘˜) is the associated

Since atomic interactions are computed based on the distances between atoms, the
Cartesian coordinates need to be converted into sets of pairwise distances relative to
each atom. It is then the role of the genetic system to evolve an accurate functional
relationship between distances ğ‘Ÿğ‘– ğ‘— and potential energy. For each training sample ğ‘˜,
the symbolic regression model needs to process a set of pairwise atomic distances
into a prediction for the energy with the help of summation symbol (cid:205).

As it becomes apparent from studying previous approaches described in Section 2,
modeling interatomic potentials is a non-trivial problem which requires substantial
computational resources. Previous implementations employed diï¬€erent strategies
for parallelism as well as other optimization techniques such as vectorized model
evaluation in order to speed up the search. Additionally, most approaches employed
local search in order to improve model coeï¬ƒcients during evolution.

2 https://www.vasp.at/wiki/index.php/POSCAR

Discovering Interatomic Potentials with Symbolic Regression

19

For this reason, we opt to extend the framework Operon [13] with additional
functionality for modeling interatomic potentials. Operon already beneï¬ts from a
ï¬ne-grained parallelism model designed for scalability and was shown to perform well
on a variety of symbolic regression problems [14]. Additionally, it features support
for local optimization using the Levenberg-Marquardt optimization algorithm, where
the gradient is obtained via automatic diï¬€erentiation.

We adopt a multi-objective approach based on the NSGA2 algorithm [16] where
model length is used alongside prediction accuracy in order to promote parsimony,
interpretability and generalization.

3.1 Symbolic regression in Operon

Operon is a C++ framework for symbolic regression that employs logical parallelism
during evolution, such that every new oï¬€spring individual is generated in its own
logical thread. An example evolutionary algorithm implemented in Operon as an
operator graph is shown in Figure 3.

Fig. 3: Taskï¬‚ow describing the NSGA2 algorithm in Operon. Each individual task
within the subï¬‚ows (initialization, evaluation, oï¬€spring generation) executes in
parallel, using a number of logical threads equal to the population size.

Operon uses a linear encoding where each tree is represented as a postï¬x sequence
of nodes. Each node has typical attributes such as length, depth, arity or opcode.
Evaluation eï¬ƒciency is achieved by employing a batched tree interpreter, which
iterates over the tree nodes and executes the corresponding functions on ï¬xed-
size batches of data. As the batch size is known at compile-time, these operations
are vectorized. The entire tree evaluation infrastructure relies on the Eigen C++
library [22] for eï¬ƒcient, vectorized execution.

Taskflow: NSGA2Subflow: initSubflow: main loopinitterminationmain loop0done1initializepopulationevaluatepopulationnon-dominatedsortback0generateoffspringnon-dominatedsortreinsert20

Burlacu et al.

3.1.1 Implementing the (cid:205) symbol

The tree interpreter represents a generic approach to tree evaluation and is agnostic
of the actual primitive set used by the algorithm. Each node is mapped to a callable3
(stateful function object) which deï¬nes the functional transformation. The callables
themselves are required to satisfy a certain function signature and to operate in both
scalar and dual number domains.

This mechanism facilitates the extension of the default primitive set with any kind
of ad hoc functionality â€“ the (cid:205) (summation) symbol in this particular application.
Figure 4 shows the general workï¬‚ow for processing a set of atomic positions into
pairwise distances and using them to estimate the potential energy. The function
ğ¹ represents a symbolic functional form which includes (cid:205) symbols over pairwise
atomic distances. Since the (cid:205) symbol is essentially a reduction operator4, the actual
number of dimensions of the input data is three: ğ‘ snapshots, ğ‘€ atoms, ğ¿ pairwise
distances (where ğ¿ dynamically depends on the cut-oï¬€ radius ğ‘Ÿout).

Snapshot k

Aggregate distances

Estimate energy

ğ· (ğ‘˜)

ğ‘– = {ğ‘Ÿğ‘– ğ‘— |ğ‘Ÿğ‘– ğ‘— < ğ‘Ÿout}

Ë†ğ¸ (ğ‘˜)
ğ‘–

= ğ¹

(cid:17)

(cid:16)

ğ· (ğ‘˜)
ğ‘–

Atom ğ‘–

Fig. 4: Prediction of atom energies using SR. The total energy is then Ë†ğ¸ (ğ‘˜) =

âˆ‘ï¸

Ë†ğ¸ (ğ‘˜)
ğ‘–

.

ğ‘–

Like many other evolutionary frameworks, Operon relies on a dataset object which
holds tabular data in two dimensions: ğ‘‹ features Ã— ğ‘Œ observations. Therefore, it is
not straightforward to accommodate an additional data dimension without signiï¬cant
redesign work. However, it is relatively easy to incorporate an extra, inner dataset
into the function object associated to the (cid:205) symbol, which will contain the values in
the third dimension (interatomic distances).

For this mechanism to work, a convention is necessary: the outer dataset will
contain the target energy values as well as an input variable ğ‘Ÿ whose value is always 1
(this value was chosen arbitrarily as a non-problematic constant which does not cause
discontinuities). The variable ğ‘Ÿ simply acts a placeholder for the pairwise atomic
distance values. The inner dataset will contain the actual pairwise distances under the
same input name ğ‘Ÿ. The distances are computed when the atomic coordinate values
are loaded into the callable. A nested tree interpreter is then used to evaluate the

3 https://en.wikipedia.org/wiki/Callable_object
4 https://en.wikipedia.org/wiki/Reduction_operator

Discovering Interatomic Potentials with Symbolic Regression

21

current (cid:205)-subtree using the inner dataset as input. Similar to Hernandez et al. [24],
the (cid:205) symbol also applies a smoothing function on its output (see Equation (7)
in [24]) with the inner and outer cutoï¬€ radii equal to 3Ã… and 5Ã…, respectively.

Under this set of rules, a leaf node corresponding to the input variable ğ‘Ÿ will
evaluate to 1 when not under a (cid:205) symbol, and to the set of pairwise distance
values corresponding to the current atomic conï¬guration otherwise. Additionally, to
disallow nesting of (cid:205) symbols, the behavior is dynamically switched depending on
the surrounding tree context: if a (cid:205) symbol ï¬nds itself under another (cid:205) symbol, then
it simply acts as the identity function ğ‘“ (ğ‘¥) = ğ‘¥. This convention does not impact the
evolutionary systemâ€™s ability to discover interatomic potential functional forms.

3.2 Empirical validation

We demonstrate the capabilities of the proposed NSGA2-based multi-objective
approach using the ab initio data used by Hernandez et al. [24]. This data consists of
150 snapshots of 32-atom DFT molecular dynamics simulations of copper (Cu): 50
snapshots at 300 K (NVT), 50 snapshots at 1400 K (NVT) and 50 snapshots at 1400
K (NPT at 100 kPa). Although the data also contains components of forces and virial
stress tensors, only the energy was used as a modeling target in this experiment. The
data consisting of 150 conï¬gurations is shuï¬„ed and split equally into training and
test partitions.

3.2.1 Experimental setup

âˆš

ğ‘¥2 + 1.

The experiment used a ï¬xed set of parameters shown in Table 1. The primitive set
was varied and consisted of diï¬€erent symbol combinations, as shown in Table 2:
with and without the power function, and alternating between Ã· and aq, where
aq(ğ‘¥) =

Two input variables are used: ğ‘Ÿ as a placeholder for atomic distances and ğ‘ = 1
ğ‘Ÿ as
a placeholder for the inverse of ğ‘Ÿ, given that some empirical potentials like Lennard-
Jones explicitly use the inverse in their formula. Each experimental conï¬guration
was repeated 50 times and the median values were reported (with the exception of
runtime, which was averaged). Errors are reported as median Â± standard deviation.
Model length was computed as the length of the simpliï¬ed representation returned
by Sympy, using the inï¬x textual representation of the best individual as input.

3.2.2 Results

Results aggregated over 50 runs for each conï¬guration are shown in Table 2, alongside
p-value matrices computed using the Kruskal test. Signiï¬cance is encoded in Tables 3
and 4 using font weight and color: values lower than ğ›¼ = 0.01 are shown in bold

22

Burlacu et al.

population size
tree initialization
max tree length
max tree depth
crossover probability 100%
crossover operator
mutation probability 25%
mutation operator

10,000 individuals
balanced tree creator (BTC) [13]
20
10

subtree crossover

uniformly chosen from:
â€¢ subtree removal/insertion/replacement
â€¢ change function symbol
â€¢ change variable name
â€¢ additive one point leaf mutation (ğ‘£ = ğ‘£ + N (0, 1))
â€¢ discrete point leaf mutation (ğ‘£ â† math constant: ğœ‹, e, ...)
crowded tournament selection, group size = 17
Pearson R2 and model length
108 ï¬tness evaluations

selection operator
objectives
evaluation budget

Table 1: NSGA2 parameters

black font, values lower than ğ›¼ = 0.05 are shown in black font, while all the other
values are shown in gray. The direction of the relationship is determined using a
comparison of median values and shown as â†‘ (worse/higher error) or â†“ (better/lower
error) symbols preï¬xed to the values.

The overall best models from all runs and all conï¬gurations are shown in Table 5.
These models have been selected based on both test accuracy and simplicity of their
functional form. Two other models with better test score have been discarded due to
very complex structure or very large coeï¬ƒcient values. Table 5 illustrates this fact
by displaying the absolute rank of each model (based purely on test accuracy and
disregarding other criteria).

Interestingly, the arithmetic-only conï¬gurations A, B, C generated 4 out of 5 of
the selected best models. Although conï¬guration A produced signiï¬cantly worse
(ğ‘ < 0.01) training accuracies than all other conï¬gurations, it did not produce worse
models in terms of generalization, where it is only worse than E. Nevertheless, the
explicit inclusion of 1/ğ‘Ÿ as an input seems to help the search.

It is also worth noting that conï¬gurations using the analytic quotient instead of
(unprotected) division generally perform better on the training data (ğ‘ < 0.05), but
do not perform better on the test data. For example, conï¬guration K is better than A,
B, C, H, I in terms of training accuracy, but is not better than any of them in terms of
test (on the contrary, it is worse than E at ğ‘ < 0.05). From this we can surmise that in
this particular test setting and for this particular data, AQ does not oï¬€er an advantage
compared to normal division.

Overall, judging from median error values and statistical signiï¬cance p-values,
there is no clear winner among the tested conï¬gurations. However, a pattern emerges
when observing the functional forms of the best models, mostly originating from
conï¬gurations B and C. After simpliï¬cation using Sympy, the models become highly
similar with the same mathematical structure consisting of a sum of three factors in
the numerator (each including the inverse of ğ‘Ÿ) and another sum in the denominator

Discovering Interatomic Potentials with Symbolic Regression

23

(also including the inverse of ğ‘Ÿ). Although these models are remarkably simple,
further testing is required to validate their properties and behavior.

In terms of runtime, the proposed approach is eï¬ƒcient, with the longest run
taking on average 290 seconds to evolve a population of 10,000 individuals for 1000
generations on a single multicore computer. In comparison, Hernandez et al. [24]
report 360 CPU-hours expended on ï¬nding accurate GP models.

ID Primitive set
A (cid:205), +, âˆ’, Ã—, Ã·
B (cid:205), +, âˆ’, Ã—, Ã·
C (cid:205), +, âˆ’, Ã—, Ã·
D (cid:205), +, âˆ’, Ã—, aq
E (cid:205), +, âˆ’, Ã—, aq
(cid:205), +, âˆ’, Ã—, aq
F

Inputs
ğ‘Ÿ
ğ‘
ğ‘Ÿ , ğ‘

ğ‘Ÿ
ğ‘
ğ‘Ÿ , ğ‘

G (cid:205), +, âˆ’, Ã—, Ã·, pow ğ‘Ÿ
H (cid:205), +, âˆ’, Ã—, Ã·, pow ğ‘
I

(cid:205), +, âˆ’, Ã—, Ã·, pow ğ‘Ÿ , ğ‘
(cid:205), +, âˆ’, Ã—, aq, pow ğ‘Ÿ
J
K (cid:205), +, âˆ’, Ã—, aq, pow ğ‘
L (cid:205), +, âˆ’, Ã—, aq, pow ğ‘Ÿ , ğ‘

MAEtrain
0.568 Â± 0.045
0.518 Â± 0.036
0.512 Â± 0.043

0.498 Â± 0.047
0.500 Â± 0.066
0.493 Â± 0.046

0.501 Â± 0.042
0.516 Â± 0.048
0.514 Â± 0.051

0.507 Â± 0.052
0.489 Â± 0.053
0.497 Â± 0.053

MAEtest
0.602 Â± 0.059
0.599 Â± 0.069
0.595 Â± 0.091

0.583 Â± 0.060
0.574 Â± 0.068
0.593 Â± 0.060

0.620 Â± 0.039
0.604 Â± 0.065
0.596 Â± 0.057

0.608 Â± 0.059
0.623 Â± 0.085
0.594 Â± 0.068

Length Runtime (s)

32.0
44.0
42.0

56.0
56.5
60.0

39.0
46.5
47.0

47.0
57.0
57.0

118.52
142.01
143.69

165.49
162.51
169.64

286.95
241.25
290.53

269.26
244.44
281.86

Table 2: Operon NSGA2 Results

A

B

C

D

E

F

G

H

I

J

K

L

A

â†‘4e-07

â†‘8e-07 â†‘1e-09 â†‘2e-07 â†‘1e-09 â†‘1e-08

â†‘3e-06

â†‘8e-06 â†‘3e-07 â†‘9e-10 â†‘9e-09

B â†“4e-07

â†‘6e-01 â†‘2e-02 â†‘1e-01 â†‘2e-02 â†‘1e-01 â†‘1e+00 â†‘8e-01 â†‘3e-01 â†‘6e-03 â†‘5e-02

C â†“8e-07

â†“6e-01

â†‘7e-02 â†‘2e-01 â†‘5e-02 â†‘3e-01

â†“7e-01

â†“9e-01 â†‘5e-01 â†‘2e-02 â†‘1e-01

D â†“1e-09

â†“2e-02

â†“7e-02

â†“4e-01 â†‘9e-01 â†“3e-01

â†“4e-02

â†“6e-02 â†“3e-01 â†‘7e-01 â†‘8e-01

E â†“2e-07

â†“1e-01

â†“2e-01 â†‘4e-01

â†‘5e-01 â†“8e-01

â†“1e-01

â†“3e-01 â†“9e-01 â†‘2e-01 â†‘7e-01

F â†“1e-09

â†“2e-02

â†“5e-02 â†“9e-01 â†“5e-01

â†“4e-01

â†“2e-02

â†“6e-02 â†“3e-01 â†‘6e-01 â†“8e-01

G â†“1e-08

â†“1e-01

â†“3e-01 â†‘3e-01 â†‘8e-01 â†‘4e-01

â†“1e-01

â†“3e-01 â†“8e-01 â†‘2e-01 â†‘6e-01

H â†“3e-06 â†“1e+00 â†‘7e-01 â†‘4e-02 â†‘1e-01 â†‘2e-02 â†‘1e-01

â†‘8e-01 â†‘3e-01 â†‘7e-03 â†‘5e-02

I

J

â†“8e-06

â†“3e-07

â†“8e-01

â†‘9e-01 â†‘6e-02 â†‘3e-01 â†‘6e-02 â†‘3e-01

â†“8e-01

â†‘4e-01 â†‘3e-02 â†‘1e-01

â†“3e-01

â†“5e-01 â†‘3e-01 â†‘9e-01 â†‘3e-01 â†‘8e-01

â†“3e-01

â†“4e-01

â†‘1e-01 â†‘5e-01

K â†“9e-10

â†“6e-03

â†“2e-02 â†“7e-01 â†“2e-01 â†“6e-01 â†“2e-01

â†“7e-03

â†“3e-02 â†“1e-01

â†“4e-01

L â†“9e-09

â†“5e-02

â†“1e-01 â†“8e-01 â†“7e-01 â†‘8e-01 â†“6e-01

â†“5e-02

â†“1e-01 â†“5e-01 â†‘4e-01

Table 3: Training error p-value matrix using the Kruskal statistical test. Signiï¬cance
shown by bold black font (ğ‘ < 0.01), black font (ğ‘ < 0.05) or gray (no signiï¬cance).
Relationship direction given by comparison of medians: â†‘ (worse/higher error), â†“
(better/lower error).

24

A

A

B

C

D

E

F

G

H

I

J

K

L

â†‘3e-01

â†‘4e-01

â†‘2e-01

â†‘7e-03 â†‘8e-02 â†“9e-01 â†“4e-01 â†‘1e-01 â†“5e-01 â†“9e-01 â†‘7e-02

Burlacu et al.

B â†“3e-01

â†‘9e-01

â†‘1e+00 â†‘2e-01 â†‘6e-01 â†“2e-01 â†“7e-01 â†‘6e-01 â†“8e-02 â†“2e-01 â†‘4e-01

C â†“4e-01

â†“9e-01

â†‘1e+00 â†‘2e-01 â†‘5e-01 â†“4e-01 â†“8e-01 â†“8e-01 â†“1e-01 â†“3e-01 â†‘6e-01

D â†“2e-01 â†“1e+00 â†“1e+00

â†‘1e-01 â†“6e-01 â†“3e-01 â†“8e-01 â†“9e-01 â†“4e-02 â†“3e-01 â†“6e-01

E â†“7e-03

â†“2e-01

â†“2e-01

â†“1e-01

â†“4e-01 â†“3e-03 â†“8e-02 â†“3e-01 â†“1e-03 â†“1e-02 â†“4e-01

F â†“8e-02

â†“6e-01

â†“5e-01

â†‘6e-01

â†‘4e-01

â†“5e-02 â†“4e-01 â†“7e-01 â†“1e-02 â†“8e-02 â†“9e-01

G â†‘9e-01

â†‘2e-01

â†‘4e-01

â†‘3e-01

â†‘3e-03 â†‘5e-02

â†‘3e-01 â†‘3e-02 â†‘5e-01 â†“7e-01 â†‘2e-02

H â†‘4e-01

â†‘7e-01

â†‘8e-01

â†‘8e-01

â†‘8e-02 â†‘4e-01 â†“3e-01

â†‘4e-01 â†“1e-01 â†“3e-01 â†‘3e-01

I

J

â†“1e-01

â†“6e-01

â†‘8e-01

â†‘9e-01

â†‘3e-01 â†‘7e-01 â†“3e-02 â†“4e-01

â†“2e-02 â†“1e-01 â†‘7e-01

â†‘5e-01

â†‘8e-02

â†‘1e-01

â†‘4e-02

â†‘1e-03 â†‘1e-02 â†“5e-01 â†‘1e-01 â†‘2e-02

â†“7e-01 â†‘7e-03

K â†‘9e-01

â†‘2e-01

â†‘3e-01

â†‘3e-01

â†‘1e-02 â†‘8e-02 â†‘7e-01 â†‘3e-01 â†‘1e-01 â†‘7e-01

â†‘7e-02

L â†“7e-02

â†“4e-01

â†“6e-01

â†‘6e-01

â†‘4e-01 â†‘9e-01 â†“2e-02 â†“3e-01 â†“7e-01 â†“7e-03 â†“7e-02

Table 4: Test error p-value matrix using the Kruskal statistical test. Signiï¬cance
shown by bold black font (ğ‘ < 0.01), black font (ğ‘ < 0.05) or gray (no signiï¬cance).
Relationship direction given by comparison of medians: â†‘ (worse/higher error), â†“
(better/lower error).

ID

C

E

C

C

B

Model
MAEtrain = 0.579, MAEtest = 0.448, Absolute rank: 1

âˆ’110.531 âˆ’

2929.411 (cid:205) (cid:16) (cid:16)

(cid:17) (cid:16)0.727 âˆ’ 2.888

(cid:17) (cid:16)0.727 âˆ’ 1.747

ğ‘Ÿ

(cid:17) (cid:17)

âˆ’0.974 + 2.68
ğ‘Ÿ
(cid:205) (cid:16)

ğ‘Ÿ

(cid:17)

âˆ’

0.972
ğ‘Ÿ (0.899ğ‘Ÿâˆ’1.815)

MAEtrain = 0.612, MAEtest = 0.454, Absolute rank: 2
âˆ’ (cid:205) (cid:16) 0.211

âˆ’ 2.396(cid:17)

3.037 (cid:16)

(cid:17)

ğ‘Ÿ 2

âˆšï¸‚

(cid:205)2 (cid:16)(cid:16)

âˆ’2.409 + 6.254

ğ‘Ÿ

(cid:17) (cid:16)1.209 âˆ’ 4.99

ğ‘Ÿ

(cid:17) (cid:16)1.209 âˆ’ 2.956

ğ‘Ÿ

(cid:17) (cid:17)

+ 1

âˆ’ 101.086

MAEtrain = 0.585, MAEtest = 0.458, Absolute rank: 3

12327.356 (cid:205) (cid:16) (cid:16)

âˆ’0.817 + 2.014

ğ‘Ÿ

(cid:17) (cid:16)0.318 âˆ’ 1.255

ğ‘Ÿ

(cid:17) (cid:16)0.706 âˆ’ 1.913

ğ‘Ÿ

(cid:17) (cid:17)

âˆ’111.611 +

(cid:18)

(cid:205)

(cid:19)

0.806
ğ‘Ÿ (0.307ğ‘Ÿâˆ’ 1.292

ğ‘Ÿ

)

MAEtrain = 0.550, MAEtest = 0.473, Absolute rank: 6
14618.749 (cid:205) (cid:16) (cid:16)0.555 âˆ’ 1.538

(cid:17) (cid:16)0.707 âˆ’ 1.667

ğ‘Ÿ

(cid:17) (cid:16)0.787 âˆ’ 3.116

ğ‘Ÿ

(cid:17) (cid:17)

ğ‘Ÿ

âˆ’108.409 +

(cid:205) (cid:16)

3.142
ğ‘Ÿ (1.922âˆ’0.953ğ‘Ÿ )

(cid:17)

MAEtrain = 0.549, MAEtest = 0.475, Absolute rank: 7

82734.094 (cid:205) (cid:16) (cid:16)

âˆ’0.361 + 1.414

ğ‘Ÿ

(cid:17) (cid:16)0.527 âˆ’ 1.433

ğ‘Ÿ

(cid:17) (cid:16)0.622 âˆ’ 1.512

ğ‘Ÿ

(cid:17) (cid:17)

âˆ’109.903 âˆ’

(cid:18)

(cid:205)

(cid:19)

0.873
ğ‘Ÿ (âˆ’0.339+ 0.686

ğ‘Ÿ

)

Table 5: Overall best models, where ID identiï¬es the conï¬guration in Table 2.

Discovering Interatomic Potentials with Symbolic Regression

25

4 Conclusion

This work surveyed the main applications of SR in Materials Science, namely for
the discovery of simple and eï¬ƒcient models of interatomic potentials. Both previous
results, as well as results obtained by our own proposed approach and described in
this paper, suggest that SR is capable of ï¬nding accurate models that can further the
capabilities of particle simulations.

Similar to POET [24], our approach does not restrict the search space in any way
(with the exception of tree length and depth limits) and is therefore capable of ï¬nding
models that do not resemble previously known, empirical potential functions. At the
same time, should a directed search be required, the framework is trivial to extend
with this feature.

Empirical testing shows that relatively simple primitive sets are powerful enough
to discover accurate potential functions with good extrapolation behavior. On this
data, no advantage was found in using the analytical quotient over standard division.
More experiments will be required to establish the beneï¬ts of larger primitive sets,
for example ones that include logarithmic, exponential or trigonometric functions.

Several other aspects like a more comprehensive search in the space of hyper-
parameters or an exploration of the eï¬€ects of local search also need to be fully
investigated in the future. Compared to other works described in our survey, our
approach did not diverge from the â€œvanillaâ€ version of GP, using a classical multi-
objective approach (NSGA2) together with a domain speciï¬c primitive set. It will
be also worthwhile to explore various ways to scale up the search using multiple
populations and more sophisticated evolutionary models.

Future development directions include expanding the capabilities of the framework
to include three- or many-body interactions, to consider model derivatives in order
model atomic forces as well, and overall to improve its ability to incorporate and
respect the fundamental laws of this kind of physical systems.

26

5 Appendix

Empirical potentials

Burlacu et al.

For a comprehensive overview of empirical potentials we recommend the work of
AraÃºjo and Ballester [2]. Below we give a casual overview of the most important
empirical potentials mentioned in this contribution.

Morse potential

This is an empirical potential used to model diatomic molecules.

ğ‘‰M (ğ‘Ÿ) = ğ·

(cid:16)1 âˆ’ exp (cid:0) âˆ’ ğ‘(ğ‘Ÿ âˆ’ ğ‘Ÿ0)(cid:1) 2(cid:17)

(17)

where ğ· is the dissociation energy, ğ‘Ÿ is the distance between atoms, ğ‘ is a set of
parameters and ğ‘Ÿ0 is the equilibrium bond distance.

Lennard-Jones potential

The Lennard-Jones potential models soft repulsive and attractive interactions and
can describe electronically neutral atoms or molecules. Interacting particles repel
each other at very close distance, attract each other at moderate distance, and do not
interact at inï¬nite distance.

ğ‘‰LJ (ğ‘Ÿ) = 4ğœ€

(cid:17) 12

(cid:20)(cid:16) ğœ
ğ‘Ÿ

âˆ’

(cid:16) ğœ
ğ‘Ÿ

(cid:17) 6(cid:21)

(18)

where ğ‘Ÿ is the distance between atoms, ğœ€ is the dispersion energy and ğœ is the distance
at which the particle-particle potential energy ğ‘‰ is zero.

Lippincott potential

Lippincott [48] potential involves an exponential of interatomic distances

ğ‘‰LIP (ğ‘Ÿ) = ğ·

(cid:32)

1 âˆ’ exp (cid:16) âˆ’ğ‘›(ğ‘Ÿ âˆ’ ğ‘Ÿ0)2

2ğ‘Ÿ

(cid:33)

(cid:17)

(cid:16)1 + ğ‘ğ¹ (ğ‘Ÿ)

(cid:17)

(19)

where ğ· is the dissociation energy, ğ‘Ÿ is the distance between atoms, ğ‘Ÿ0 is the
equilibrium bond distance and ğ‘ and ğ‘› are parameters. ğ¹ (ğ‘Ÿ) is a function of
internuclear distance such that ğ¹ (ğ‘Ÿ) = 0 when ğ‘Ÿ = âˆ and ğ¹ (ğ‘Ÿ) = âˆ when ğ‘Ÿ = 0.

Discovering Interatomic Potentials with Symbolic Regression

27

Stillinger-Weber potential

The Stillinger-Weber potential [49] models two- and three-body interactions by taking
into account not only the distances between atoms but also the bond angles:

ğ‘‰SW (ğ‘Ÿ) =

ğœ™2(ğ‘Ÿğ‘– ğ‘— ) +

âˆ‘ï¸

(cid:104)ğ‘–, ğ‘— (cid:105)

âˆ‘ï¸

(cid:104)ğ‘–, ğ‘—,ğ‘˜ (cid:105)

ğœ™3 (ğ‘Ÿğ‘– ğ‘— , ğ‘Ÿğ‘–ğ‘˜ , ğœƒğ‘– ğ‘— ğ‘˜ )

(20)

where

ğœ™2 (ğ‘Ÿğ‘– ğ‘— ) = ğ´ğœ€

(cid:19) ğ‘

(cid:20)

ğµ

(cid:18) ğœ
ğ‘Ÿğ‘– ğ‘—

âˆ’

(cid:18) ğœ
ğ‘Ÿğ‘– ğ‘—

(cid:19) ğ‘(cid:21)

exp

ğœ™3 (ğ‘Ÿğ‘– ğ‘— , ğ‘Ÿğ‘–ğ‘˜ , ğœƒğ‘– ğ‘— ğ‘˜ ) = ğœ†ğœ€ (cid:2)cos ğœƒğ‘– ğ‘— ğ‘˜ âˆ’ cos ğœƒ0(cid:3) 2

Ã— exp

Sutton-Chen potential

and

(cid:19)

(cid:18)

ğœ
ğ‘Ÿğ‘– ğ‘— âˆ’ ğ‘ğœ
(cid:18)
ğ›¾ğœ
ğ‘Ÿğ‘– ğ‘— âˆ’ ğ‘ğœ

(cid:19)

exp

(cid:19)

(cid:18)

ğ›¾ğœ
ğ‘Ÿğ‘–ğ‘˜ âˆ’ ğ‘ğœ

(21)

(22)

The Sutton-Chen potential [50] has been used in molecular dynamics and Monte
Carlo simulations of metallic systems. It oï¬€ers a reasonable description of various
bulk properties, with an approximate many-body representation of the delocalized
metallic bonding:

âˆ‘ï¸

ğ‘‰SC =

ğ‘ˆ (ğ‘Ÿğ‘– ğ‘— ) âˆ’

âˆ‘ï¸

âˆš

ğ‘¢

ğœŒğ‘–

(23)

(cid:104)ğ‘–, ğ‘— (cid:105)

ğ‘–

Here, the ï¬rst term represents the repulsion between atomic cores and the second
term models the bonding energy due to the electrons. Both terms are further deï¬ned
in terms of reciprocal power so that the complete expression is:

ğ‘‰SC = ğœ–

(cid:18) ğ‘
ğ‘Ÿğ‘– ğ‘—

âˆ‘ï¸

(cid:104)ğ‘–, ğ‘— (cid:105)

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(cid:19) ğ‘›

âˆ’ ğ¶ âˆ‘ï¸

(cid:118)(cid:116)âˆ‘ï¸

ğ‘–

ğ‘—

(cid:18) ğ‘
ğ‘Ÿğ‘– ğ‘—

(cid:19) ğ‘šï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

(24)

where ğ¶ is a dimensionless parameter, ğœ– is a parameter with dimensions of energy, ğ‘
is the lattice constant, ğ‘š, ğ‘› are positive integers with ğ‘› > ğ‘š and ğ‘Ÿğ‘– ğ‘— is the distance
between the ğ‘–th and ğ‘—th atoms.

References

[1] A. Agrawal and A. Choudhary. Perspective: Materials informatics and big
data: Realization of the â€œfourth paradigmâ€ of science in materials science. APL
Materials, 4(5):053208, 2016.

[2] Judith P. AraÃºjo and Maikel Y. Ballester. A comparative review of 50 analytical
representation of potential energy interaction for diatomic systems: 100 years of
history. International Journal of Quantum Chemistry, 121(24):e26808, 2021.

28

Burlacu et al.

[3] James E. Baker. Reducing bias and ineï¬ƒciency in the selection algorithm. In
Proceedings of the Second International Conference on Genetic Algorithms
on Genetic Algorithms and Their Application, pages 14â€“21, USA, 1987. L.
Erlbaum Associates Inc.

[4] R. M. Balabin and E. I. Lomakina. Support vector machine regression (ls-
svm)â€”an alternative to artiï¬cial neural networks (anns) for the analysis of
quantum chemistry data? Phys. Chem. Chem. Phys., 13:11710â€“11718, 2011.
[5] A. P. BartÃ³k, R. Kondor, and G. CsÃ¡nyi. On representing chemical environments.

[6] J. Behler. Perspective: Machine learning potentials for atomistic simulations. J.

Phys. Rev. B, 87:184115, May 2013.

Chem. Phys., 145(17):170901, 2016.

[7] Michael A. Bellucci and David F. Coker. Empirical valence bond models
for reactive potential energy surfaces: A parallel multilevel genetic program
approach. The Journal of Chemical Physics, 135(4):044115, 2011.

[8] Michael A. Bellucci and David F. Coker. Molecular dynamics of excited state
intramolecular proton transfer: 3-hydroxyï¬‚avone in solution. The Journal of
Chemical Physics, 136(19):194505, 2012.

[9] K. Binder, D. Heermann, Lyle Roelofs, A. John Mallinckrodt, and Susan
McKay. Monte carlo simulation in statistical physics. Computers in Physics,
7(2):156â€“157, 1993.

[10] A. Brown, A. B. McCoy, B. J. Braams, Z. Jin, and J. M. Bowman. Quantum
and classical studies of vibrational motion of ch5+ on a global potential energy
surface obtained from a novel ab initio direct dynamics approach. J. Chem.
Phys., 121(9):4105â€“4116, 2004.

[11] Michael W. Brown, Aidan P. Thompson, Jean-Paul Watson, and Peter A. Schultz.
Bridging scales from ab initio models to predictive empirical models for complex
materials. Technical report, Laboratories, Sandia National, 2008.

[12] W. M. Brown, A. P. Thompson, and P. A. Schultz. Eï¬ƒcient hybrid evolutionary
optimization of interatomic potential models. J. Chem. Phys., 132(2):024108,
2010.

[13] Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon C++:
An eï¬ƒcient genetic programming framework for symbolic regression.
In
Proceedings of the 2020 Genetic and Evolutionary Computation Conference
Companion, GECCO â€™20, pages 1562â€“1570, internet, July 8-12 2020. Associa-
tion for Computing Machinery.

[14] William G. La Cava, Patryk Orzechowski, Bogdan Burlacu, FabrÃ­cio Olivetti
de FranÃ§a, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H. Moore.
Contemporary symbolic regression methods and their relative performance.
CoRR, abs/2107.14351, 2021.

[15] R. Chen, K. Shao, B. Fu, and D. H. Zhang. Fitting potential energy surfaces with
fundamental invariant neural network. ii. generating fundamental invariants for
molecular systems with up to ten atoms. J. Chem. Phys., 152(20):204307, 2020.
[16] Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan. A fast and
elitist multiobjective genetic algorithm: Nsga-ii. IEEE Trans. Evol. Comput,
6(2):182â€“197, 2002.

Discovering Interatomic Potentials with Symbolic Regression

29

[17] P. O. Dral. Quantum chemistry in the age of machine learning. J. Phys. Chem.

Lett., 11(6):2336â€“2347, 2020. PMID: 32125858.

[18] Andrew Eldridge, Alejandro Rodriguez, Ming Hu, and Jianjun Hu. Genetic
programming-based learning of carbon interatomic potential for materials
discovery, 2022.

[19] Christian GagnÃ© and Marc Parizeau. Genericity in evolutionary computation
software tools: Principles and case study. International Journal on Artiï¬cial
Intelligence Tools, 15(2):173â€“194, April 2006.

[20] H. Gao, J. Wang, and J. Sun. Improve the performance of machine-learning
potentials by optimizing descriptors. J. Chem. Phys., 150(24):244110, 2019.

[21] L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl, and M. Scheï¬„er.
Big data of materials science: Critical role of the descriptor. Phys. Rev. Lett.,
114:105503, Mar 2015.

[22] GaÃ«l Guennebaud, BenoÃ®t Jacob, et al. Eigen v3. http://eigen.tuxfamily.org,

2010.

[23] C. M. Handley and J. Behler. Next generation interatomic potentials for
condensed systems. European Physical Journal B, 87(7):152, July 2014.
[24] A. Hernandez, A. Balasubramanian, F. Yuan, S. A. M. Mason, and T. Mueller.
Fast, accurate, and transferable many-body interatomic potentials by symbolic
regression. NPJ Computational Materials, 5(1):112, 2019.

[25] T. Hey, K. Butler, S. Jackson, and J. Thiyagalingam. Machine learning and big
scientiï¬c data. Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 378(2166):20190054, 2020.

[26] Lauri Himanen, Amber Geurts, Adam Stuart Foster, and Patrick Rinke. Data-
driven materials science: Status, challenges, and perspectives. Advanced Science,
6(21):1900808, 2019.

[27] Adam Hospital, Josep Ramon GoÃ±i, Modesto Orozco, and Josep L GelpÃ­.
Molecular dynamics simulations: advances and applications. Advances and
applications in bioinformatics and chemistry: AABC, 8:37, 2015.

[28] Jianjun Hu, Erik Goodman, Kisung Seo, Zhun Fan, and Rondal Rosenberg.
The hierarchical fair competition (hfc) framework for sustainable evolutionary
algorithms. Evolutionary Computation, 13(2):241â€“277, 06 2005.

[29] J. Ischtwan and M. A. Collins. Molecular potential energy surfaces by interpo-

lation. J. Chem. Phys., 100(11):8080â€“8088, 1994.

[30] Abdelouahab Kenouï¬ and Kholmirzo Kholmurodov. Symbolic regression of
interatomic potentials via genetic programming. Biol. Chem. Res, 2:1â€“10, 2015.
[31] Chiho Kim, Ghanshyam Pilania, and Ramamurthy Ramprasad. From organized
high-throughput data to phenomenological theory using machine learning: The
example of dielectric breakdown. Chemistry of Materials, 28(5):1304â€“1311,
2016.

[32] K. H. Kim, Y. S. Lee, T. Ishida, and G-H Jeung. Dynamics calculations for the
lih+h li+h2 reactions using interpolations of accurate ab initio potential energy
surfaces. J. Chem. Phys., 119(9):4689â€“4693, 2003.

[33] W. Kohn and L. J. Sham. Self-consistent equations including exchange and

correlation eï¬€ects. Phys. Rev., 140:A1133â€“A1138, Nov 1965.

30

Burlacu et al.

[34] John R. Koza. Genetic Programming: On the Programming of Computers by
Means of Natural Selection. MIT Press, Cambridge, MA, USA, 1992.
[35] G. Kresse and J. FurthmÃ¼ller. Eï¬ƒcient iterative schemes for ab initio total-energy
calculations using a plane-wave basis set. Phys. Rev. B, 54:11169â€“11186, Oct
1996.

[36] Aaron Kusne, Tim Mueller, and Ramamurthy Ramprasad. Machine learning
in materials science: Recent progress and emerging applications. Reviews in
Computational Chemistry, 2016-05-06 2016.

[37] D. E. Makarov and H. Metiu. Fitting potential-energy surfaces: A search in the
function space by directed genetic programming. J. Chem. Phys., 108(2):590â€“
598, 1998.

[38] Dmitrii E. Makarov and Horia Metiu. Using genetic programming to solve the
schrÃ¶dinger equation. The Journal of Physical Chemistry A, 104(37):8540â€“8545,
2000.

[39] T. Mueller, A. Hernandez, and C. Wang. Machine learning for interatomic

potential models. J. Chem. Phys., 152(5):050902, 2020.

[40] T. Mueller, E. Johlin, and J. C. Grossman. Origins of hole traps in hydrogenated
nanocrystalline and amorphous silicon revealed through machine learning. Phys.
Rev. B, 89:115202, 2014.

[41] Ghanshyam Pilania. Machine learning in materials science: From explain-
able predictions to autonomous design. Computational Materials Science,
193:110360, 2021.

[42] Steve Plimpton. Fast parallel algorithms for short-range molecular dynamics.

Journal of Computational Physics, 117(1):1â€“19, 1995.

[43] T. Rothe, J. Schuster, F. Teichert, and E.E. Lorenz. Machine Learning Potentials
- State of the Research and Potential Applications for Carbon Nanostructures.
Technische UniversitÃ¤t, Faculty of Natural Sciences, Institute of Physics, 2019.
[44] Kumara Narasimha Sastry. Genetic algorithms and genetic programming for
multiscale modeling: Applications in materials science and chemistry and
advances in scalability. PhD thesis, University of Illinois, Urbana-Champaign,
March 2007.

[45] K. Shao, J. Chen, Z. Zhao, and D. H. Zhang. Communication: Fitting potential
energy surfaces with fundamental invariant neural network. J. Chem. Phys.,
145(7):071101, 2016.

[46] A. V. Shapeev. Moment tensor potentials: A class of systematically improvable
interatomic potentials. Multiscale Modeling & Simulation, 14(3):1153â€“1173,
2016.

[47] A. Slepoy, M. D. Peters, and A. P. Thompson. Searching for globally optimal
functional forms for interatomic potentials using genetic programming with
parallel tempering. J. Comput. Chem., 28(15):2465â€“2471, 2007.

[48] Derek Steele, Ellis R. Lippincott, and Joseph T. Vanderslice. Comparative study
of empirical internuclear potential functions. Rev. Mod. Phys., 34:239â€“251,
Apr 1962.

[49] Frank H. Stillinger and Thomas A. Weber. Computer simulation of local order
in condensed phases of silicon. Phys. Rev. B, 31:5262â€“5271, Apr 1985.

Discovering Interatomic Potentials with Symbolic Regression

31

[50] A. P. Sutton and J. Chen. Long-range ï¬nnisâ€“sinclair potentials. Philosophical

Magazine Letters, 61(3):139â€“146, 1990.

[51] A.P. Thompson, L.P. Swiler, C.R. Trott, S.M. Foiles, and G.J. Tucker. Spec-
tral neighbor analysis method for automated generation of quantum-accurate
interatomic potentials. J. Comput. Phys., 285:316â€“330, 2015.

[52] Oliver T. Unke, Stefan Chmiela, Huziel E. Sauceda, Michael Gastegger, Igor
Poltavsky, Kristof T. SchÃ¼tt, Alexandre Tkatchenko, and Klaus-Robert MÃ¼ller.
Machine learning force ï¬elds. Chemical Reviews, 0(0):null, 2021. PMID:
33705118.

[53] Yiqun Wang, Nicholas Wagner, and James M Rondinelli. Symbolic regression

in materials science. MRS Communications, 9(3):793â€“805, 2019.

[54] L. Zhang, J. Han, H. Wang, R. Car, and E. Weinan. Deep potential molecular
dynamics: A scalable model with the accuracy of quantum mechanics. Phys.
Rev. Lett., page 143001, 2018.

