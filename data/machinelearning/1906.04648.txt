1
2
0
2

n
u
J

2
2

]

C
O
.
h
t
a
m

[

4
v
8
4
6
4
0
.
6
0
9
1
:
v
i
X
r
a

Analysis of Optimization Algorithms via
Sum-of-Squares

Sandra S. Y. Tan

Antonios Varvitsiotis

Vincent Y. F. Tan∗

June 23, 2021

Abstract

We introduce a new framework for unifying and systematizing the performance
analysis of ﬁrst-order black-box optimization algorithms for unconstrained convex min-
imization. The low-cost iteration complexity enjoyed by ﬁrst-order algorithms renders
them particularly relevant for applications in machine learning and large-scale data
analysis. Relying on sum-of-squares (SOS) optimization, we introduce a hierarchy of
semideﬁnite programs that give increasingly better convergence bounds for higher levels
of the hierarchy. Alluding to the power of the SOS hierarchy, we show that the (dual of
the) ﬁrst level corresponds to the Performance Estimation Problem (PEP) introduced
by Drori and Teboulle [Math. Program., 145(1):451–482, 2014], a powerful framework
for determining convergence rates of ﬁrst-order optimization algorithms. Consequently,
many results obtained within the PEP framework can be reinterpreted as degree-1 SOS
proofs, and thus, the SOS framework provides a promising new approach for certifying
improved rates of convergence by means of higher-order SOS certiﬁcates. To determine
analytical rate bounds, in this work we use the ﬁrst level of the SOS hierarchy and
derive new results for noisy gradient descent with inexact line search methods (Armijo,
Wolfe, and Goldstein).

1 Introduction

The pervasiveness of machine learning and big-data analytics throughout most academic
ﬁelds and industrial domains has triggered renewed interest in convex optimization, the
subﬁeld of mathematical optimization that is concerned with minimizing a convex objective
function over a convex set of decision variables. Of particular relevance for solving large-scale

∗Sandra S. Y. Tan was with the Department of Electrical and Computer Engineering, National University
of Singapore (sandra_tsy@u.nus.edu). Antonios Varvitsiotis was with the Department of Electrical and
Computer Engineering and Department of Industrial Systems Engineering and Management, National Uni-
versity of Singapore (avarvits@gmail.com). Vincent Tan is with the Department of Electrical and Computer
Engineering and Department of Mathematics, National University of Singapore (vtan@nus.edu.sg).

1

 
 
 
 
 
 
convex optimization problems with low accuracy requirements are ﬁrst-order algorithms,
deﬁned as iterative algorithms that only use (sub)gradient information.

There exists extensive literature on the convergence analysis of ﬁrst-order optimization
algorithms with respect to various performance metrics; see, e.g., [2, 4–6] and the references
therein. However, existing convergence results typically rely on case-by-case analyses and
cannot be understood by a common guiding principle.
In this work we introduce a uni-
ﬁed framework for deriving worst-case upper bounds on the convergence rates of ﬁrst-order
optimization algorithms, through the use of sum-of-squares (SOS) optimization.

SOS optimization is an active research area with important practical applications; see, e.g.,
[3, 21, 22, 29, 30]. The key idea underlying SOS optimization is to use semideﬁnite program-
ming (SDP) relaxations for certifying the nonnegativity of a polynomial over a set deﬁned
by polynomial (in)equalities. This allows to construct hierarchies of SDPs that approximate
the optimal value of arbitrary polynomial optimization problems.

To illustrate the main ingredients of our approach, consider the problem of minimizing
a convex function f : Rn → R over Rn, i.e., minx∈Rn f (x), and let x∗ be a global minimizer.
Any solution strategy entails choosing a black-box algorithm A that generates a sequence
of iterates {xk}k≥1. Our goal is then to estimate the worst-case convergence rate of A
with respect to a ﬁxed family of functions F and an appropriate measure of performance
(e.g., distance to optimality (cid:107) xk − x∗ (cid:107) or objective function accuracy f (xk) − f (x∗)). For
concreteness, using as performance metric the objective function accuracy and a ﬁrst-order
algorithm A that does not increase the objective function value at each step, we seek to solve
the following optimization problem:

t∗ = minimize t

subject to fk+1 − f∗ ≤ t(fk − f∗),

xk+1 = A (x0, . . . , xk; f0, . . . , fk; g0, . . . , gk) ,
for all f ∈ F,

(1)

where we set fk = f (xk) and gk = ∇f (xk) for all k ≥ 1. As the optimization problem (1)
is hard in general, we relax it into a tractable convex program (in fact, an SDP), in two
steps.
In the ﬁrst step, we derive necessary conditions that are expressed as polynomial
inequalities h1(z) ≥ 0, . . . , hm(z) ≥ 0 and equalities v1(z) = 0, . . . , vm(cid:48)(z) = 0, in terms of
the variables in z = (f∗, fk, fk+1, x∗, xk, xk+1, g∗, gk, gk+1), which are dictated by the choice
of the algorithm and the corresponding class of functions. Having identiﬁed these necessary
polynomial constraints, the ﬁrst relaxation of the optimization problem (1) is to ﬁnd the
minimum t ∈ (0, 1) such that the polynomial t(fk − f∗) − (fk+1 − f∗) is nonnegative over the
semi-algebraic set

K = {z : hi(z) ≥ 0, i ∈ [m], vj(z) = 0, j ∈ [m(cid:48)]},

where here and throughout we use the notation [m] = {1, . . . , m}. Nevertheless, as this
second problem is also hard in general, in the second step we further relax this constraint
by demanding that the nonnegativity of the polynomial t(fk − f∗) − (fk+1 − f∗) over K is

2

certiﬁed by an SOS decomposition:

t(fk − f∗) − (fk+1 − f∗) = σ0(z) +

m
(cid:88)

i=1

σi(z)hi(z) +

m(cid:48)
(cid:88)

j=1

θj(z)vj(z),

(2)

where the σi(z)’s are SOS polynomials and the θj(z)’s are arbitrary polynomials. Clearly,
expression (2) certiﬁes that t(fk − f∗) − (fk+1 − f∗) is nonnegative over the semi-algebraic
set K. Furthermore, once the degree of the σi’s and the θj’s has been ﬁxed, the problem of
ﬁnding the least t ∈ (0, 1) such that (2) holds is an instance of an SDP, and thus, it can be
solved eﬃciently.

1.1 Related Work

Performance Estimation Problem. Our work was motivated by the recent framework
introduced by Drori and Teboulle [10] that casts the search for worst-case rate bounds as an
inﬁnite-dimensional optimization problem:

f (xN ) − f (x∗)

maximize
f,x0,...,xN ,x∗
subject to f ∈ F,

xk+1 = A (x0, . . . , xk; f0, . . . , fk; ∇f (x0), . . . , ∇f (xk)) , 0 ≤ k ≤ N − 1,
x∗ is a minimizer of f on Rn, (cid:107) x0 − x∗ (cid:107) ≤ R,
x0, . . . , xN , x∗ ∈ Rn,

(PEP)

called the Performance Estimation Problem (PEP). A series of recent works has highlighted
the PEP as an extremely useful tool in various settings, including the study of worst-case
guarantees for ﬁrst-order optimization algorithms [7,8,10,36–39], the design of optimal meth-
ods [9–11, 19, 20], and the study of worst-case guarantees for solving monotone inclusion
problems [16, 18, 24, 32]. The PEP captures the worst-case objective function accuracy over
all functions within F, after N iterations of the algorithm A from any starting point x0,
which is within distance R from some minimizer x∗.

Although the PEP is inﬁnite-dimensional (as its search space includes all functions in
the class F), it can be transformed into an equivalent ﬁnite-dimensional problem using the
(smooth) convex interpolation approach introduced in [38]. Following [38], the functional
constraint f ∈ F is discretized by introducing 2(N + 2) additional variables capturing the
value and the gradient of the function at the points x0, . . . , xN , x∗. Speciﬁcally, setting
I = {0, 1, . . . , N, ∗}, the ﬁnite-dimensional problem

fN − f∗

maximize
{xi,gi,fi}i∈I
subject to ∃f ∈ F such that fi = f (xi), gi = ∇f (xi) for all i ∈ I,

xk+1 = A (x0, . . . , xk; f0, . . . , fk; g0, . . . , gk) , k = 0, . . . , N − 1,
g∗ = 0, (cid:107) x0 − x∗ (cid:107) ≤ R,

(f-PEP)

3

with decision variables {xi, gi, fi}i∈I, is equivalent to the PEP in the sense that their optimal
values coincide and an optimal solution to the PEP can be transformed to an optimal solution
to the f-PEP (and conversely).

The seemingly simple step of reformulating the PEP into f-PEP by discretizing and
introducing interpolability constraints leads naturally to a powerful approach for evaluating
(or upper bounding) the value of the f-PEP. Speciﬁcally, if interpolability with respect to F
and the iterates generated by A satisfy conditions that are linear in f = (f0, . . . , fN , f∗) and
the entries of the Gram matrix G = X (cid:62)X, where X = (x0 . . . xN x∗ g0 . . . gN g∗) ∈
Rn×2(N +2), the value of the f-PEP is upper bounded by the SDP deﬁned by all necessary
functional and algorithmic constraints, as well as the appropriate reformulations in terms of
f and G of the optimality condition g∗ = 0 and the initialization condition (cid:107) x0 − x∗ (cid:107) ≤ R.
Moreover, in the case where interpolability with respect to F and the ﬁrst-order method
under consideration are both linearly Gram-representable, i.e., exactly characterized by a
ﬁnite number of constraints that are linear in f and in the entries of G, the corresponding
SDP relaxation of f-PEP is tight, for large enough values of n.

Interpolability conditions have been formulated exactly for various function classes, in-
cluding the class of L-smooth and µ-strongly convex functions [38, Theorem 5], indicator
and support functions [37, Section 3.3], smooth and nonconvex functions [37, Section 3.4].
In terms of the tightness of the SDP relaxation of the f-PEP, in the case where F is one of
the aforementioned function classes, and the corresponding algorithm is a ﬁxed-step linear
ﬁrst-order method as deﬁned in [37, Deﬁnition 2.11] the SDP relaxation is tight, as long as
2(N + 1) ≤ n [37, Proposition 2.6].

Integral Quadratic Constraints.
A competing approach that uses SDPs to analyze
iterative optimization algorithms was introduced in [23]. In this setting, the minimizers of the
function of interest are mapped to the ﬁxed points of a discrete-time linear dynamical system
with a nonlinear feedback law, whose convergence is then analyzed using integral quadratic
constraints (IQCs). The IQC approach allows one to derive analytical and numerical upper
bounds on the convergence rates for various algorithms by solving small SDPs. For instance,
in [23], algorithms considered include the gradient method, the heavy-ball method, Nesterov’s
accelerated method (and related variants) applied to smooth and strongly convex functions.
The line of research initiated in [23] has been generalized further in various directions.
Some notable examples include the convergence analysis of the ADMM method [27], the case
of non-strongly convex objective functions [13], the generalization to stochastic algorithms
In addition, an approach
[17], and the design of ﬁrst-order optimization algorithms [12].
drawing upon ideas from both the PEP and IQC frameworks, and comparison between
these, was proposed in [35].

1.2 Summary of Results

In most instances where the PEP framework was applied in the literature, close inspection
of the proofs of the analytic worst-case bounds reveals that they can be reinterpreted as

4

simple, i.e., low-degree SOS certiﬁcates; see, e.g., [39, Appendix A], [38, Section 3.6], and [7,
Section 4.1]. This observation is the point of departure for our work, whose aim is to unify
the aforementioned results, and additionally, to make the search for the underlying SOS
certiﬁcates explicit.

As it turns out, the connection between the PEP and the SOS framework is an instance of
SDP duality. Speciﬁcally, we have mentioned that relaxing the f-PEP into an SDP requires
the functional and algorithmic constraints to imply linear constraints of the form

(cid:104) ci, f (cid:105) +(cid:104)Ci, G(cid:105) ≥ ai

or

(cid:104) dj, f (cid:105) +(cid:104)Dj, G(cid:105) = bi,

(3)

for appropriate vectors ci, dj, matrices Ci, Dj and scalars ai, bj. Nevertheless, notice that
an equivalent way expressing the constraints in (3) is as polynomial constraints in the
variables f∗, fk, fk+1, x∗, xk, xk+1, g∗, gk, gk+1. Speciﬁcally, setting z0 = (f∗, fk, fk+1) and
z(cid:96) = (x∗((cid:96)), xk((cid:96)), xk+1((cid:96)), g∗((cid:96)), gk((cid:96)), gk+1((cid:96))), where we use x∗((cid:96)) to denote the (cid:96)th coordi-
nate of x∗ for (cid:96) ∈ [n], the constraints in (3) may be equivalently expressed as

(cid:104)ci, z0(cid:105) +

n
(cid:88)

(cid:96)=1

z(cid:62)
(cid:96) Ci z(cid:96) ≥ ai

or

(cid:104)dj, z0(cid:105) +

(cid:88)

(cid:96)

z(cid:62)
(cid:96) Dj z(cid:96) = bi,

(4)

i.e., as polynomials in the variables z0, z1, . . . , zn, to which we apply the SOS framework.
Formalizing this connection, in Theorem 3 we show that the dual of the ﬁrst level of the
SOS hierarchy is equivalent to the PEP when the functional and algorithmic constraints are
linearly Gram-representable. This allows to reinterpret existing rate bounds derived within
the PEP framework as degree-1 SOS certiﬁcates.

Nevertheless, despite its many successful applications, the PEP framework does not oﬀer
a systematic way by which the SDP relaxation can be strengthened when the function class
under consideration or the employed algorithm are not linearly Gram-representable. Indeed,
recall that to go from the PEP to an SDP we take two relaxation steps. In the ﬁrst step
we extract necessary (quadratic) conditions that are dictated by the interpolability with
respect to F and the algorithm A. In the second step, we use the identiﬁed conditions to
formulate an SDP, which gives the desired rate bounds. Now, it is clear that if the ﬁrst
relaxation step is loose, then the value of the SDP is not necessarily equal to the value of
the PEP. In such a setting, there is no systematic way to strengthen the PEP-SDP, whereas,
the sum-of-squares approach clearly provides a solution: just consider a higher level of the
hierarchy. This is exactly why we believe that the sum-of-squares approach is an interesting
and complementary approach to the Gram matrix approach of Taylor et al. [38].

On the other hand, the SOS approach provides a systematic framework for ﬁnding better
(i.e., smaller) bounds on the worst-case contraction factor of descent algorithms, by using
higher levels of the SOS hierarchy.
It is worth noting though that this ﬂexibility comes
at a computational cost, in the sense that the SDPs obtained via the SOS hierarchy are
dimension-dependent, i.e., any performance certiﬁcate generated by the model only applies
to functions over a domain with a ﬁxed dimension n.

To overcome this issue, we show in Theorem 2 that in the speciﬁc setting studied in this
work (cf. Section 2.3), a degree-1 certiﬁcate for the univariate case (i.e. n = 1) can be lifted

5

to a degree-1 certiﬁcate for the general case (n > 1). Nevertheless, we have been unable
to extend this lifting procedure for higher-order SOS certiﬁcates. As our goal is to identify
analytic rates, the inability to work with general n has forced us to only consider degree-1
certiﬁcates. We leave the consideration of higher degree certiﬁcates to future work.

In terms of using the SOS hierarchy to derive new convergence results, we focus on gradi-
ent descent applied to L-smooth, µ-strongly convex functions, where the step size is chosen
using inexact line search methods. Speciﬁcally, in Theorem 4, Theorem 5 and Theorem 6
we respectively study the Armijo, Wolfe, and Goldstein conditions with step size selection in
both the noisy and noiseless settings. Denoting by δ ∈ [0, 1) the noise level in the gradient
estimation (see (22)), our main results are the following rate bounds:

Gradient descent with Armijo-terminated line search:

(cid:20)

fk+1 − f∗ ≤

1 −

4µ(cid:15)(1 − δ)2
ηL

(cid:18) 1 − δ
(1 + δ)2 − (cid:15)

(cid:19)(cid:21)

(fk − f∗),

which is valid for any noise level δ ∈ [0, 1), algorithm parameters (cid:15) ∈

(cid:16)

0,

1−δ
(1+δ)2

(cid:17)

and η > 1.

Gradient descent with Goldstein-terminated line search:
(cid:18)

fk+1 − f∗ ≤

1 −

4µ(cid:15)(1 − δ)2
L

(cid:20) 1 − δ
(1 + δ)2 − (1 − (cid:15))

(cid:21)(cid:19)

(fk − f∗),

which is valid for noise levels δ ∈ [0,

√

5 − 2) and algorithm parameter (cid:15) ∈

Gradient descent with Wolfe-terminated line search:

(cid:18)

fk+1 − f∗ ≤

1 −

(cid:19)

2µc1(1 − c2)
L

(fk − f∗),

(cid:16)

1 − 1−δ

(1+δ)2 , 1

2

(cid:17)

.

which is valid for any algorithm parameters 0 < c1 < c2 < 1.

We show that the bound for GD with Armijo-terminated line search rule is an improve-
ment upon two existing bounds in the literature, see [26, Proposition 3.3.5] and [25, Page 239].
On the other hand, our results for GD with Goldstein or Wolfe-terminated line search are,
to the best of our knowledge, new.

The interested reader may ﬁnd the code for numerically and symbolically verifying the

results at https://github.com/sandratsy/SumsOfSquares.

Paper Organization. The paper is organized as follows: Section 2 introduces the SOS
technique, explains how it is applied to derive worst-case bounds and describes the function
class and algorithms we examine within this work. Furthermore, we determine a procedure
for lifting degree-1 certiﬁcates from the univariate to the multivariate case and also prove
the relation between PEP and SOS. In Section 3 we use the SOS framework to determine
new convergence results for noisy gradient descent with inexact line search methods (Armijo,
Wolfe, Goldstein). Lastly, Section 4 contains concluding remarks and suggests avenues for
future work.

6

Note. A preliminary version of this paper was presented at the Signal Processing with
Adaptive Sparse Structured Representations (SPARS) workshop in Toulouse, France in July
2019 [34]. Moreover, several additional convergence results obtained via the SOS approach
including GD with constant step size and exact line search, and proximal gradient with
constant step size and exact line search can be found in the M. Eng.
thesis of the ﬁrst
author [33]. These results have not been included in this manuscript as the exact same rates
have been also derived via the PEP framework, which as already discussed, is equivalent to
degree-1 SOS proofs.

2 Description of our Approach

2.1 Background on Sum-of-Squares

1 . . . zan

n . The degree of the monomial za is deﬁned to be | a | = (cid:80)n

Before we provide the details of our approach, we need to introduce some necessary notation
and deﬁnitions. For any a ∈ Nn, where N is the set of nonnegative integers, we denote by
za the monomial za1
i=1 ai.
Let R[z]n,d denote the set of polynomials in n variables z1, . . . , zn, of degree at most d. Any
polynomial p(z) ∈ R[z]n,d can be written as a linear combination of monomials of degree
at most d, i.e., p(z) = (cid:80)
| a |≤d pa za . An (even-degree) polynomial p(z) is called a sum-of-
squares (SOS) if there exist polynomials q1(z), . . . , qm(z) satisfying p(z) = (cid:80)m
i (z). Note
that if the degree of p(z) is equal to 2d, all polynomials qi(z) will necessarily have degree at
most d. It is instructive to think of the existence of an SOS decomposition as a tractable
certiﬁcate for the global nonnegativity of p(z). Indeed, it is clear that any SOS polynomial
p(z) is also globally nonnegative, i.e., p(z) ≥ 0 for all z ∈ Rn. Furthermore, although less
obvious, it is well-known that checking the existence of an SOS decomposition can be done
eﬃciently using SDPs [30].

i=1 q2

Moving beyond the problem of certifying global nonnegativity, a more general problem
is to certify the nonnegativity of a polynomial p(z) over a (basic) closed semi-algebraic set
K = {z ∈ Rn : hi(z) ≥ 0, i ∈ [m], vj(z) = 0, j ∈ [m(cid:48)]} , i.e., to certify that p(z) ≥ 0 for all
z ∈ K. Analogously to the case of global nonnegativity, we look for certiﬁcates that can be
found eﬃciently using SDPs. One such choice are Putinar-type certiﬁcates [31], given by:

p(z) = σ0(z) +

m
(cid:88)

i=1

σi(z)hi(z) +

m(cid:48)
(cid:88)

j=1

θj(z)vj(z),

(5)

where the σi’s are themselves SOS polynomials and the θj’s are arbitrary (i.e., not necessarily
SOS) polynomials. Clearly, the expression (5) serves as a certiﬁcate that p(z) ≥ 0 for all
z ∈ K and moreover, the existence of such a representation (for a ﬁxed degree d) can be
done using SDPs, e.g., see [30].

7

2.2 Algorithm Analysis Using SOS Certiﬁcates

Fixing a family of functions F and a ﬁrst-order algorithm A—one that uses only gradient
information—our goal is to ﬁnd the best (smallest) contraction factor t ∈ (0, 1) that is
valid over all functions in F and all sequences of iterates that can be generated using the
algorithm A. Concretely, for any ﬁxed k, we want to estimate the minimum t ∈ (0, 1)
satisfying fk+1 − f∗ ≤ t(fk − f∗), for all f ∈ F and xk+1 = A (xk, fk, gk). We address
this question using SOS certiﬁcates. To employ an SOS approach, we ﬁrst need to identify
polynomial inequalities hi(z) ≥ 0 and polynomial equalities vj(z) = 0 in the variables

z := (f∗, fk, fk+1, x∗, xk, xk+1, g∗, gk, gk+1) ∈ R6n+3

(6)

that should be necessarily satisﬁed following the choice of the class of functions F and the
ﬁrst-order algorithm A. Setting K to be the semi-algebraic set deﬁned by the identiﬁed
polynomial equalities and inequalities, i.e.,

K := {z : hi(z) ≥ 0, i ∈ [m],

vj(z) = 0, j ∈ [m(cid:48)]} ,

it follows immediately that if the polynomial

pt(z) := t(fk − f∗) − (fk+1 − f∗)

is nonnegative over the set K for some t ∈ (0, 1), then t also serves as an upper bound on
the worst-case rate t∗, or, in other words, t∗ is upper bounded by the value of the following
optimization problem

tpoly := inf{t : pt(z) ≥ 0 ∀ z ∈ K, t ∈ (0, 1)},

(7)

where the decision variable is the scalar t. As the optimization problem (7) involves a
polynomial nonnegativity constraint (over a semi-algebraic set) it is in general hard—in
fact, strongly NP-hard [1]. To obtain tractable upper bounds, we replace the constraint that
pt(z) is nonnegative over K by asking that pt(z) admits an SOS certiﬁcate of the form (5),
which clearly certiﬁes nonnegativity over K. Concretely, for any d ≥ 0 and n ≥ 1, we get
the SDP:

td := minimize t

subject to pt(z) = s0(z) +

m
(cid:88)

i=1

σi(z)hi(z) +

m(cid:48)
(cid:88)

j=1

θj(z)vj(z),

t ∈ (0, 1),
σ0(z) : SOS polynomial with deg(σ0(z)) ≤ 2d,
σi(z) : SOS polynomial with deg(σi(z)hi(z)) ≤ 2d,
θj(z) : arbitrary polynomial with deg(θj(z)vj(z)) ≤ 2d,

(8)

8

where z ∈ R6n+3. For any ﬁxed integer d ≥ 0 and n ≥ 1, the optimization problem (8)
is an SDP, and consequently, it can be solved in polynomial-time to any desired accuracy.
Furthermore, for a ﬁxed n, it follows immediately from the deﬁnitions that

t∗ ≤ tpoly ≤ ... ≤ td+1 ≤ td, for all d ∈ N.

In other words, as d increases, the SDPs given in (8) give increasingly better—more precisely,
no worse—upper bounds on the worst-case ratio t∗. On the negative side, the sizes of these
SDPs grow as O(nd), so in practice, working with large values of d is computationally
prohibitive. Summarizing, our strategy for estimating the worst-case rate consists of the
following steps:

1. Identify polynomial inequality and equality constraints hi(z) ≥ 0, vj(z) = 0 in the
variable z (recall (6)) that are implied by choosing a function class and an algorithm.

2. Fix a degree d ∈ N for the SOS certiﬁcate, i.e., for the degrees of the polynomials σi’s
and θj’s. Higher degree certiﬁcates allow for tighter bounds but are more diﬃcult to
ﬁnd due to the increase in size of the SDP.

3. Numerically solve the SDP in (8) using degree-d SOS certiﬁcates multiple times, varying
the parameters corresponding to the algorithm and the function class. This allows us
to “guess” the analytic form of the optimal variables for (8).

4. Lastly, we verify that the identiﬁed solution from step 3 is indeed feasible for (8).
Determining feasibility gives an analytic upper bound on the best contraction factor
td that can certiﬁed using degree-d SOS certiﬁcates.

Implementation Details. Throughout this paper, we restrict our attention to degree-1
SOS certiﬁcates, as our main goal is to derive rates symbolically (see Section 2.3). The
derivation of the aﬃne constraints deﬁning the feasible region of the SDP (8) was done by
matching coeﬃcients in (5). The SDP (8) was solved with CVX [14,15], using the supported
SDP solver SDPT3 [40, 41]. Fortunately, there are many SOS optimization toolboxes such
as YALMIP that automate the process of matching coeﬃcients and constructing the SDP.
Finally, veriﬁcation of the identiﬁed solution was done through MATLAB’s Symbolic Math
Toolbox and Mathematica [42]. Mathematica was used to ﬁrst verify that the optimal
matrices are PSD, before we found their corresponding SOS decompositions analytically.
For the interested reader, the codes for implementation of the SDPs and veriﬁcation of the
solutions may be found at https://github.com/sandratsy/SumsOfSquares.

2.3 Choices Speciﬁc to this Work

Function classes of interest. Consider parameters 0 ≤ µ < L < +∞. In this work,
we only consider the class of L-smooth, µ-strongly convex functions—also known as (µ, L)-
smooth functions—with domain Rn, which we denote by F µ,L(Rn). Recall that a proper,

9

closed, convex function f : Rn → R ∪{+∞} is called L-smooth if

(cid:107)g1 − g2(cid:107) ≤ L(cid:107)x1 − x2(cid:107), ∀ x1, x2 ∈ Rn, g1 = ∇f (x1), g2 = ∇f (x2),

and µ-strongly convex if the function f (x) − µ
Euclidean norm.

2 (cid:107)x(cid:107)2 is convex, where (cid:107) · (cid:107) denotes the usual

Throughout this work, we use the following set of necessary and suﬃcient conditions de-
veloped in [38] for the existence of a function in F µ,L(Rn) generating data triples {(xi, fi, gi)}i∈I.

Theorem 1. Given a set {(xi, fi, gi)}i∈I, there exists f ∈ F µ,L(Rn) where fi = f (xi) and
gi = ∇f (xi) for all i ∈ I, if and only if, for all i (cid:54)= j ∈ I:

fi − fj − g(cid:62)

j (xi − xj) ≥

L
2(L − µ)

(cid:18) 1
L

(cid:13)
(cid:13)gi − gj

(cid:13)
2 + µ(cid:107)xi − xj(cid:107)2 − 2
(cid:13)

(gj − gi)(cid:62)(xj − xi)

(cid:19)

.

µ
L

Applying Theorem 1 to the data triples (xk, fk, gk), (xk+1, fk+1, gk+1), and (x∗, f∗, g∗) we
get six polynomial constraints that we denote throughout this paper by h1(z) ≥ 0, . . . , h6(z) ≥
0. Speciﬁcally, setting α :=

2(1−µ/L) , the six F µ,L-interpolability conditions are:

1

fk −fk+1 − g(cid:62)

k+1(xk − xk+1) − α

(cid:18) 1
L

(cid:13)
(cid:13)gk − gk+1

(cid:13)
2 + µ(cid:107)xk − xk+1(cid:107)2 −2
(cid:13)

µ
L

fk − f∗ − g(cid:62)

∗ (xk − x∗) − α

(cid:18) 1
L

fk+1 − fk − g(cid:62)

k (xk+1 − xk) − α

fk+1 − f∗ − g(cid:62)

f∗ − fk − g(cid:62)

∗ (xk+1 − x∗) − α
(cid:18) 1
L

k (x∗ − xk) − α

(cid:13)
(cid:13)gk+1 − gk

(cid:107)gk − g∗(cid:107)2 + µ(cid:107)xk − x∗(cid:107)2 − 2
(cid:18) 1
L
(cid:18) 1
L

(cid:13)
(cid:13)gk+1 − g∗

(cid:13)
2 + µ(cid:107)xk+1 − xk(cid:107)2 − 2
(cid:13)

(cid:13)
2 + µ(cid:107)xk+1 − x∗(cid:107)2 − 2
(cid:13)

(gk+1−gk)(cid:62)(xk+1−xk)
(cid:19)

(cid:19)

≥ 0

(g∗ − gk)(cid:62)(x∗ − xk)

≥ 0

µ
L
µ
L

(gk − gk+1)(cid:62)(xk − xk+1)
(cid:19)

(g∗ − gk+1)(cid:62)(x∗ − xk+1)
(cid:19)

(cid:19)

≥ 0

≥ 0

µ
L

µ
L

(cid:107)g∗ − gk(cid:107)2 + µ(cid:107)x∗ − xk(cid:107)2 − 2

(gk − g∗)(cid:62)(xk − x∗)

≥ 0

f∗ −fk+1 − g(cid:62)

k+1(x∗ − xk+1) − α

(cid:18) 1
L

(cid:13)
(cid:13)g∗ − gk+1

(cid:13)
2 + µ(cid:107)x∗ − xk+1(cid:107)2 −2
(cid:13)

µ
L

(gk+1−g∗)(cid:62)(xk+1−x∗)

(cid:19)

≥ 0.

(9)

2.4 Lifting Univariate Certiﬁcates

As already mentioned, we restrict our attention to degree-1 SOS certiﬁcates (recall (8)). In
this setting, σ0(z) is an SOS of linear polynomials and, since the polynomials hi(z) and vj(z)
we consider are degree-2, the σi(z)’s need to be degree-0 SOS polynomials and the θj(z)’s
degree-0 polynomials. The SOS certiﬁcate can be thus expressed as:

p(z) = σ0(z) +

m
(cid:88)

i=1

σihi(z) +

m(cid:48)
(cid:88)

j=1

θjvj(z),

(10)

10

where σi ∈ R+ and θj ∈ R. We claim that the form of the polynomials pt(z), hi(z)’s and
vj(z)’s, combined with the speciﬁc choice of SOS certiﬁcates under consideration (i.e., degree-
1 certiﬁcates) allow us to only consider the univariate case n = 1. Concretely, we show in the
rest of this section that an SOS certiﬁcate for some contraction factor t ∈ (0, 1) in the univari-
ate case, induces an SOS certiﬁcate for the same contraction factor in the multivariate case
(n > 1). To see this, ﬁrst we rearrange the variable z = (f∗, fk, fk+1, x∗, xk, xk+1, g∗, gk, gk+1)
as z = (z0, z1, . . . , zn), where

z0 = (f∗, fk, fk+1) and z(cid:96) = (x∗((cid:96)), xk((cid:96)), xk+1((cid:96)), g∗((cid:96)), gk((cid:96)), gk+1((cid:96))), (cid:96) ∈ [n]

(11)

where x∗((cid:96)) denotes the (cid:96)th coordinate of x∗ for (cid:96) ∈ [n].

Theorem 2. Assume that the performance measure polynomial and the constraint functions
are separable with respect to the blocks of variables z0, z1, . . . , zn, they are invariant with
respect to permutations of the blocks of variables z1, . . . , zn, and that they have no constant
terms. Then, a degree-1 SOS certiﬁcate for a rate t ∈ (0, 1) in the univariate case (i.e.,
n = 1) can be lifted to degree-1 certiﬁcate for the general case (i.e., n > 1).

Note that the structural assumptions on the performance measure polynomial and the

constraint functions imply that they have the form

pt(z) = p0

t (z0) +

hi(z) = h0

i (z0) +

vj(z) = v0

j (z0) +

n
(cid:88)

(cid:96)=1
n
(cid:88)

(cid:96)=1
n
(cid:88)

(cid:96)=1

p1
t (z(cid:96))

h1
i (z(cid:96))

v1
j (z(cid:96)),

(12)

for some polynomials p0

t , p1

t , h0

i , h1

i , v0

j and v1
j .

Furthermore, note that all the performance measure polynomials (e.g., t(fk −f∗)−(fk+1 −
f∗) and t(cid:107)xk − x∗(cid:107)2 − (cid:107)xk+1 − x∗(cid:107)2) and the constraint functions encountered thus far have
the form (12). Furthermore, (12) is satisﬁed when the polynomial constraints take the form
given in (4), i.e, the constraints are linear in the f ’s and in the inner products of the xi’s
and gi’s.

Proof. To prove the theorem, note that an SOS certiﬁcate {Q, {σi}i, {θj}j}, (i.e., Q is a PSD
matrix, {σi}i ⊆ R+ and {θj}j ⊆ R) for a rate t ∈ (0, 1) in the general case n > 1 has the
following form:








(cid:62)

pt(z) =















Q








1
z0
z1
...
zn

1
z0
z1
...
zn








+

m
(cid:88)

i=1

(cid:32)

σi

h0
i (z0) +

n
(cid:88)

(cid:96)=1

(cid:33)

h1
i (z(cid:96))

+

m(cid:48)
(cid:88)

j=1

(cid:32)

θj

v0
j (z0) +

(cid:33)

v1
j (z(cid:96))

.

n
(cid:88)

(cid:96)=1

(13)

11

As the polynomials have no constant terms, it follows immediately that Q11 = 0. Further-
more, as the polynomials are separable with respect to the blocks of variables (z0, z1, . . . , zn),
Q is block diagonal. Using these two observations, (12) and (13) imply that:

t (z0) = z(cid:62)
p0

0 Q0 z0 +

t (z(cid:96)) = z(cid:62)
p1

(cid:96) Q(cid:96) z(cid:96) +

m
(cid:88)

i=1

m
(cid:88)

i=1

σih0

i (z0) +

m(cid:48)
(cid:88)

θjv0

j (z0),

σih1

i (z(cid:96)) +

j=1

m(cid:48)
(cid:88)

j=1

θjv1

j (z(cid:96)),

(cid:96) ∈ [n].

(14)

(15)

Lastly, assume there exists an SOS certiﬁcate for a rate t ∈ (0, 1) in the univariate case, i.e.,
a PSD matrix ˜Q and scalars {˜σi}i ⊆ R+, {˜θj}j ⊆ R where

pt(z) =



(cid:62)

˜Q







1
z0
z1



 +





1
z0
z1

m
(cid:88)

i=1

˜σi

As before, this may be decomposed into

(cid:0)h0

i (z0) + h1

i (z1)(cid:1) +

m(cid:48)
(cid:88)

j=1

˜θj

(cid:0)v0

j (z0) + v1

j (z1)(cid:1) .

t (z0) = z(cid:62)
p0
0

˜Q0 z0 +

t (z1) = z(cid:62)
p1
1

˜Q1 z1 +

m
(cid:88)

i=1

m
(cid:88)

i=1

˜σih0

i (z0) +

˜σih1

i (z1) +

m(cid:48)
(cid:88)

j=1

m(cid:48)
(cid:88)

j=1

˜θjv0

j (z0)

˜θjv1

j (z1).

(16)

(17)

Comparing equation (14) with (16) and equation (15) with (17), we see that Q0 = ˜Q0,
Q(cid:96) = ˜Q1, (cid:96) ∈ [n], σi = ˜σi, i ∈ [m], θj = ˜θj, j ∈ [m(cid:48)] is a valid certiﬁcate for the multivariate
case.

Lastly, we note that for higher-degree SOS certiﬁcates (beyond degree-1), it is not imme-
diately apparent how to verify that a certiﬁcate for the univariate case induces one for the
multivariate case.

2.5 Dual of the SOS Hierarchy

In this section we determine the exact relationship between the PEP and the SOS hierarchy
introduced in this work. Speciﬁcally, we show that:

Theorem 3. If the functional and algorithmic constraints are linearly Gram-representable
(i.e., (3) holds), the 1-step PEP applied to a contractive algorithm is equivalent to the ﬁrst-
level of the SOS hierarchy.

12

Proof. For concreteness, we consider the 1-step PEP (i.e., where we only take 1 step using
algorithm A) with respect to the performance metric given by the objective function accu-
racy. Similar arguments apply when the performance is measured using the distance from
optimality or the residual gradient norm.

The corresponding optimization problem is given by:

maximize
f,x0,x1,x∗

f (x1) − f (x∗)

subject to f ∈ F,

x1 = A (x0, f0, ∇f (x0)) ,
g∗ = 0, f (x0) − f (x∗) ≤ R,
x0, x1, x∗ ∈ Rn .

If the F-interpolability and algorithmic conditions are of the form given in (3) with ai =
bi = 0, the equivalent (f-PEP) may be relaxed into an SDP of the following form:

f1 − f∗

maximize
{xi,gi,fi}i∈I
subject to (cid:104)ci, f (cid:105) + (cid:104)Ci, G(cid:105) ≥ 0, i = 1, . . . , m,

(cid:104)dj, f (cid:105) + (cid:104)Dj, G(cid:105) = 0, j = 1, . . . , m(cid:48),
f0 − f∗ ≤ R,

(18)

where we recall that f = (f0, f1, f∗), X = (x0 x1 x∗ g0 g1 g∗) ∈ Rn×6, G = X (cid:62)X and
I = {0, 1, ∗}. Note that f1−f∗ may be expressed as (cid:104)(0, 1, −1), f (cid:105) and f0−f∗ as (cid:104)(1, 0, −1), f (cid:105).
Setting σi, θj and t to be the Lagrange multipliers of the three sets of constraints in (18)
respectively, the dual of (18) is

minimize
i=1,{θj }m(cid:48)

t,{σi}m

j=1

tR

subject to







0
1
−1

 − t



 +





1
0
−1

m
(cid:88)

i=1

σici +

m(cid:48)
(cid:88)

j=1

θjdj = 0

(19)

m
(cid:88)

σiCi +

m(cid:48)
(cid:88)

θjDj (cid:22) 0

i=1
t ≥ 0, σi ≥ 0, θj ∈ R .

j=1

On the other hand, using the SOS approach and restricting our attention to degree-1 certiﬁ-

13

cates, the SOS-SDP deﬁned in (8) is given by:

minimize

t,Q,{σi}m

i=1,{θj }m(cid:48)

j=1

t

subject to

t(fk − f∗) − (fk+1 − f∗) =

(cid:19)(cid:62)

(cid:18)1
z

(cid:19)

(cid:18)1
z

Q

+

m
(cid:88)

i=1

σihi(z) +

m(cid:48)
(cid:88)

j=1

θjvj(z),

t ∈ (0, 1),
Q (cid:23) 0, σi ≥ 0, θ ∈ R,

where we deﬁne as before (recall (11)) z = (z0, z1, . . . , zn), with z0 = (fk, fk+1, f∗) and

z(cid:96) = (x∗((cid:96)), xk((cid:96)), xk+1((cid:96)), g∗((cid:96)), gk((cid:96)), gk+1((cid:96))), (cid:96) ∈ [n],

and furthermore, the polynomial constraints hi(z) ≥ 0 and vj(z) = 0 have the form given in
(4) with ai = bi = 0, i.e.,

hi(z) = (cid:104)ci, z0(cid:105) +

n
(cid:88)

(cid:96)=1

z(cid:62)
(cid:96) Ci z(cid:96) and vi(z) = (cid:104)dj, z0(cid:105) +

(cid:88)

(cid:96)

z(cid:62)
(cid:96) Dj z(cid:96) .

(20)

Note that polynomials of the form (20) satisfy the requirement identiﬁed in (12). In partic-
ular, this implies that the matrix Q is block-diagonal. Based on this, the constraint

t(fk − f∗) − (fk+1 − f∗) =

(cid:19)(cid:62)

(cid:18)1
z

(cid:19)

(cid:18)1
z

Q

+

m
(cid:88)

i=1

σihi(z) +

m(cid:48)
(cid:88)

j=1

θjvj(z),

is equivalent to the equality of the following polynomials in the variables z = (z0, z1, . . . , zn):





0 = z(cid:62)
0

t





 −

1
0
−1



 −





0
1
−1

m
(cid:88)

i=1

σici −



θjdj

 ,

m(cid:48)
(cid:88)

j=1

(cid:32)

0 = z(cid:62)
(cid:96)

Q(cid:96) +

m
(cid:88)

i=1

σiCi +

m(cid:48)
(cid:88)

j=1

(cid:33)

θjDj

z(cid:96), ∀(cid:96),

which is in turn equivalent to:





0 = t



 −

1
0
−1



 −





0
1
−1

m
(cid:88)

i=1

σici −

m(cid:48)
(cid:88)

j=1

θjdj,

0 = Q(cid:96) +

m
(cid:88)

i=1

σiCi +

θjDj, ∀(cid:96).

m(cid:48)
(cid:88)

j=1

14

Thus, the SOS-SDP may be expressed as:

minimize
i=1,{θj }m(cid:48)

t,{σi}m

j=1

subject to

t





0
1
−1







 − t



 +

1
0
−1

m
(cid:88)

i=1

σici +

m(cid:48)
(cid:88)

j=1

θjdj = 0,

(21)

m
(cid:88)

σiCi +

m(cid:48)
(cid:88)

θjDj (cid:22) 0,

i=1
j=1
t ∈ (0, 1), σi ≥ 0, θj ∈ R .

Finally, we note that the constraint t < 1 can be dropped if the algorithm is a descent
algorithm. The SDP induced by the 1-PEP (19) and the SDP induced by the degree-1 SOS
problem (21) are hence equivalent problems.

3 Using the SOS Hierarchy to Obtain New Convergence

Bounds

In this section, we consider a few variants of GD with inexact line search under both the
noisy and noiseless settings. In “noisy” GD, the update step is given by xk+1 = xk +γk dk,
where the error (i.e., the diﬀerence between the descent direction dk and negative gradient)
is bounded relative to the gradient:

(cid:107)dk −(− gk)(cid:107) ≤ δ(cid:107)gk(cid:107),

(22)

for some noise level δ ∈ [0, 1). This assumption ensures that the next step taken remains in
a descent direction, i.e., − g(cid:62)

k dk > 0, e.g. see [2, Page 38].

We begin by deriving some inequalities that will be used throughout this section. We

note that

k gk = (dk + gk)(cid:62) gk −(cid:107)gk(cid:107)2
d(cid:62)

≤ (cid:107)dk + gk(cid:107)(cid:107)gk(cid:107) − (cid:107)gk(cid:107)2
≤ (δ − 1)(cid:107)gk(cid:107)2

by Cauchy-Scwartz

where the last inequality follows by (22). By a similar argument, we have

− d(cid:62)

k gk = (− dk − gk)(cid:62) gk +(cid:107)gk(cid:107)2 ≤ (δ + 1)(cid:107)gk(cid:107)2.

Squaring and expanding (22), we have

(cid:107)dk(cid:107)2 ≤ −2 d(cid:62)

k gk −(1 − δ2)(cid:107)gk(cid:107)2,

15

(23)

(24)

(25)

which, combined with (24), implies that

(cid:107)dk(cid:107)2 ≤ (cid:2)2(δ + 1) − (1 − δ2)(cid:3) (cid:107)gk(cid:107)2 = (δ + 1)2(cid:107)gk(cid:107)2.

(26)

Furthermore, by the triangle inequality, we have (cid:107)− gk(cid:107) ≤ (cid:107)dk(cid:107) + (cid:107)− dk − gk(cid:107) and thus

(cid:107)dk(cid:107) ≥ (cid:107)gk(cid:107) − (cid:107)− dk − gk(cid:107) ≥ (cid:107)gk(cid:107) − δ(cid:107)gk(cid:107) = (1 − δ)(cid:107)gk(cid:107).

(27)

3.1 The Armijo Rule

Using Armijo-terminated line search, the step size γk is chosen so that

f (xk +γk dk) ≤ f (xk) + (cid:15)γk d(cid:62)
f (xk +ηγk dk) ≥ f (xk) + (cid:15)ηγk d(cid:62)

k gk
k gk,

(28)

(29)

for some (cid:15) ∈ (0, 1) and η > 1, e.g., see [26, Section 2.4.1] and [2, Page 29]. In the noisy
setting, the gradient is not available. Substituting − dk for gk in (28)-(29) we obtain:

f (xk +γk dk) ≤ f (xk) − (cid:15)γk(cid:107)dk(cid:107)2,
f (xk +ηγk dk) ≥ f (xk) − (cid:15)ηγk(cid:107)dk(cid:107)2.

(30)

(31)

When noisy GD with Armijo-terminated line search is applied to an L-smooth function, we
are able to show the validity of the following inequality:

fk − fk+1 −

2(cid:15)(1 − δ)2
ηL

(cid:19)
(cid:18) 1 − δ
(1 + δ)2 − (cid:15)

(cid:107)gk(cid:107)2 ≥ 0,

for δ ∈ [0, 1), (cid:15) ∈

(cid:16)

0,

1−δ
(1+δ)2

(cid:17)

and η > 1. Indeed, as f is L-smooth we have that

f (y) ≤ f (x) + (y − x)(cid:62)∇f (x) +

L
2

(cid:107)y − x(cid:107)2.

Substituting x = xk and y = xk +ηγk dk we get

f (xk +ηγk dk) − f (xk) ≤ ηγk d(cid:62)

k gk +

L
2

η2γ2

k(cid:107)dk(cid:107)2.

Furthermore, combining (31) and (33), we have

0 ≤ d(cid:62)

k gk +((cid:15) +

L
2

ηγk)(cid:107)dk(cid:107)2

≤ (δ − 1)(cid:107)gk(cid:107)2 + ((cid:15) +

L
2

ηγk)(δ + 1)2(cid:107)gk(cid:107)2

by (23) and (26).

In turn, this implies

γk ≥

2
ηL

(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (cid:15)

,

16

(32)

(33)

(34)

and as we require γk > 0, we need that (cid:15) < 1−δ

(1+δ)2 . Substituting (34) into (30), we have

0 ≤ fk − fk+1 −

≤ fk − fk+1 −

2(cid:15)
ηL
2(cid:15)
ηL

(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (cid:15)
(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (cid:15)

(cid:107)dk(cid:107)2

(1 − δ)2(cid:107)gk(cid:107)2,

where the last inequality follows by (27). We note that for δ = 0, the above inequality
reduces to [26, Equation (3.3.9)].

For the next theorem we use polynomial constraints h1(z) ≥ 0, . . . , h6(z) ≥ 0 given in (9),
and h7(z) ≥ 0 given in (32), and search for a degree-1 SOS certiﬁcate as described in (10).
Constructing and solving the appropriate SDP, we obtain the following result.

and η > 1, given an (µ, L)-smooth function
Theorem 4. For any δ ∈ [0, 1), (cid:15) ∈
f : Rn → R and any sequence of iterates {xk}k≥1 generated using noisy GD with Armijo-
terminated line search, the bound

0,

1−δ
(1+δ)2

(cid:16)

(cid:17)

(cid:20)

fk+1 − f∗ ≤

1 −

4µ(cid:15)(1 − δ)2
ηL

(cid:18) 1 − δ
(1 + δ)2 − (cid:15)

(cid:19)(cid:21)

(fk − f∗)

admits an SOS certiﬁcate of degree-1.

Proof. Deﬁning

σ5 =

4µ(cid:15)(1 − δ)2
ηL

(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (cid:15)

,

σ7 = 1,

t = 1 −

4µ(cid:15)(1 − δ)2
ηL

(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (cid:15)

,

we have that t(fk − f∗) − (fk+1 − f∗) is equal to

2(cid:15)(1 − δ)2 (1 − δ − (cid:15)(1 + δ)2)
η(L − µ)(1 + δ)2

(cid:107)gk +µ(x∗ − xk)(cid:107)2 + σ5h5(z) + σ7h7(z).

(35)

The ﬁrst term in the right-hand-side of equation (35) is strictly positive since (cid:15) < 1−δ
(1+δ)2 .
In addition, as previously discussed, any sequence of iterates {xk}k≥1 generated by noisy
GD with Armijo-terminated line search for minimizing a function f ∈ F µ,L(Rn) satisﬁes
h5(z) ≥ 0 and h7(z) ≥ 0. Since σ5, σ7 ≥ 0, overall the right-hand-side of (35) is positive.
Hence, the left-hand-side of equation (35) is also positive, concluding the proof.

From Theorem 4, we also get a rate bound for GD with Armijo-terminated line search

in the noiseless case (i.e., δ = 0), which is given by

(cid:18)

fk+1 − f∗ ≤

1 −

(cid:19)

4µ(cid:15)(1 − (cid:15))
ηL

(fk − f∗).

(36)

As a consequence of (36), for all N ≥ 1 we have

(cid:18)

fN − f∗ ≤

1 −

(cid:19)N

4(cid:15)(1 − (cid:15))
ηκ

(f0 − f∗).

17

where κ = L/µ is the condition number of f . To the best of our knowledge, the best bounds
for a function f ∈ F µ,L(Rn) minimized by GD with Armijo rule were given by Luenberger
and Ye [25, Page 239] and Nemirovski [26, Proposition 3.3.5]. For any (cid:15) < 0.5 and η > 1,
Luenberger and Ye (LY) showed that

(cid:18)

fN − f∗ ≤

1 −

(cid:19)N

2(cid:15)
ηκ

(f0 − f∗),

while for any (cid:15) ≥ 0.5 and η ≥ 1, Nemirovski showed that

fN − f∗ ≤ κ

(cid:18) κ − (2 − (cid:15)−1)(1 − (cid:15))η−1
κ + ((cid:15)−1 − 1)η−1

(cid:19)N

(f0 − f∗).

To compare these convergence rates, we consider the three contraction factors

tnew = 1 −

4(cid:15)(1 − (cid:15))
ηκ

,

tLY = 1 −

2(cid:15)
ηκ

,

tnemi =

κ − (2 − (cid:15)−1)(1 − (cid:15))η−1
κ + ((cid:15)−1 − 1)η−1

.

Since Luenberger and Ye’s bound only holds for (cid:15) ∈ (0, 0.5) whereas ours hold for (cid:15) ∈ (0, 1),
we compare tnew and tLY within the common range 0 < (cid:15) < 0.5. On the other hand,
Nemirovski’s bound only holds for (cid:15) ∈ [0.5, 1), hence we compare tnew and tnemi within this
range.

Using simple arguments we now show that tnew < tLY and tnew ≤ tnemi within each range
of comparison. Thus, our contraction factor is no larger than those of Luenberger and Ye’s,
and Nemirovski’s. Indeed, to show tnew < tLY note that as (cid:15) < 0.5, we have

1 < 2(1 − (cid:15)) ⇐⇒

2(cid:15)
ηκ

<

4(cid:15)(1 − (cid:15))
ηκ

⇐⇒ 1 −

4(cid:15)(1 − (cid:15))
ηκ

< 1 −

2(cid:15)
ηκ

.

(37)

Next, we show that tnemi ≥ tnew. Since 0.5 ≤ (cid:15), we have

1 − 2(cid:15) ≥ 1 − 4(cid:15)2.

Since η > 1, κ > 1 and 1 − (cid:15) > 0, this implies

ηκ(1 − 2(cid:15))(1 − (cid:15)) ≥ ηκ(1 − 4(cid:15)2)(1 − (cid:15)).

Since 4(cid:15)(1 − (cid:15))2 > 0, we can subtract it from the right-hand side:

ηκ(1 − 2(cid:15))(1 − (cid:15)) ≥ ηκ(1 − 4(cid:15)2)(1 − (cid:15)) − 4(cid:15)(1 − (cid:15))2,

and add (cid:15)(ηκ)2 to each side and factorize:

(cid:15)(ηκ)2 + ηκ(1 − 2(cid:15))(1 − (cid:15)) ≥ (cid:15)(ηκ)2 + ηκ(1 − 4(cid:15)2)(1 − (cid:15)) − 4(cid:15)(1 − (cid:15))2,
ηκ [(cid:15)ηκ + (1 − 2(cid:15))(1 − (cid:15))] ≥ [(cid:15)ηκ + (1 − (cid:15))] [ηκ − 4(cid:15)(1 − (cid:15))] .

18

Figure 1: tnew and tLY for (cid:15) = 0.25.

Figure 2: tnew and tnemi for (cid:15) = 0.5.

Rearranging the equation, we obtain

(cid:15)ηκ + (1 − 2(cid:15))(1 − (cid:15))
(cid:15)ηκ + (1 − (cid:15))

≥

ηκ − 4(cid:15)(1 − (cid:15))
ηκ

,

and the proof that tnemi ≥ tnew is concluded.

Figures 1 and 2 compare tnew with tLY and tnemi respectively for various values of κ and
η. We note that when η → 1+ and κ → 1+ and (cid:15) = 0.5, our contraction factor tnew tends
to 0, whereas Nemirovski’s contraction factor tnemi tends to 0.5. In fact, when κ → 1+, the
function f behaves roughly as a quadratic. Combining this with the fact that η → 1+ and
the updates as shown in (28), it can be veriﬁed that GD with Armijo rule takes only a single
step to attain the optimal solution. Hence, the contraction factor we derived, i.e., tnew = 0,
is tight in this limiting scenario.

Conversely, Nemirovski’s contraction factor tnemi = 0.5 is loose. The looseness of Ne-

mirovski’s analysis can be attributed to the fact that he only applies the condition

f (x) + (y − x)(cid:62)∇f (x) +

µ
2

(cid:107)y − x(cid:107)2 ≤ f (y) ≤ f (x) + (y − x)(cid:62)∇f (x) +

L
2

(cid:107)y − x(cid:107)2

to select iterates (i.e., discretizing the condition), which is not suﬃcient to guarantee F µ,L-
interpolability. On the other hand, we make use of the condition from Theorem 1, which
constitutes a necessary and suﬃcient condition for a function to be F µ,L-interpolable.

3.2 The Goldstein Rule

Using Goldstein-terminated line search, the step size is chosen so that

(1 − (cid:15))γk d(cid:62)

k gk ≤ f (xk +γk dk) − f (xk) ≤ (cid:15)γk d(cid:62)

k gk .

(38)

19

246810Condition number, 0.20.30.40.50.60.70.80.91Contraction factor, ttnew,  = 1.001tLY,  = 1.001tnew,  = 2tLY,  = 2tnew,  = 5tLY,  = 5246810Condition number, 00.20.40.60.81Contraction factor, ttnew,  = 1.001tnemi,  = 1.001tnew,  = 2tnemi,  = 2tnew,  = 5tnemi,  = 5for some (cid:15) ∈ (0, 1/2) [26, Section 2.4.2]. The Goldstein rule was proposed earlier than
the Armijo rule, and encapsulates the same principle of suﬃcient decrease as the Armijo
rule [2, Page 32].

To examine the performance of the Goldstein-terminated line search in the noisy setting,

we again substitute − dk for gk in (38) to get:

−(1 − (cid:15))γk(cid:107)dk(cid:107)2 ≤ f (xk +γk dk) − f (xk) ≤ −(cid:15)γk(cid:107)dk(cid:107)2.

(39)

When noisy GD with the Goldstein rule is applied to an L-smooth function, the following
polynomial constraint:

fk − fk+1 −

2(cid:15)(1 − δ)2
L

(cid:18) 1 − δ
(1 + δ)2 − (1 − (cid:15))

(cid:19)

(cid:107)gk(cid:107)2 ≥ 0

(40)

holds for δ ∈ [0,

√

5 − 2) and (cid:15) ∈

(cid:16)

1 − 1−δ

(1+δ)2 , 1

2

(cid:17)

. Indeed, as f is L-smooth we get

f (xk +γk dk) − f (xk) ≤ γk d(cid:62)

k gk +

L
2

k(cid:107)dk(cid:107)2,
γ2

which combined with the ﬁrst inequality in (39) gives

0 ≤ d(cid:62)

k gk +

(cid:19)

γk + (1 − (cid:15))

(cid:18) L
2

≤ (δ − 1)(cid:107)gk(cid:107) +

(cid:18) L
2

(cid:107)dk(cid:107)2
(cid:19)

γk + 1 − (cid:15)

(1 + δ)2(cid:107)gk(cid:107)2

by (23) and (26).

In turn, this implies that

γk ≥

(cid:18) 1 − δ
(1 + δ)2 − (1 − (cid:15))

2
L

(cid:19)

.

(41)

Since we require γk > 0, we need that (cid:15) > 1 − 1−δ
(cid:15) ∈ (0, 1/2), this implies that the Goldstein rule only works in the case where δ <
and (cid:15) ∈

(1+δ)2 . Together with the condition that
5 − 2

. Substituting (41) into the second inequality in (39), we have

1 − 1−δ

√

(cid:17)

(cid:16)

(1+δ)2 , 1

2

0 ≤ fk − fk+1 −

≤ fk − fk+1 −

2(cid:15)
L
2(cid:15)(1 − δ)2
L

(cid:19)
(cid:18) 1 − δ
(1 + δ)2 − (1 − (cid:15))
(cid:19)
(cid:18) 1 − δ
(1 + δ)2 − (1 − (cid:15))

(cid:107)dk(cid:107)2

(cid:107)gk(cid:107)2

by (27)

The polynomial constraints we use in the search of a degree-1 SOS certiﬁcate (10) are
as follows: h1(z) ≥ 0, . . . , h6(z) ≥ 0 given in (9), as well as (40), denoted by h7(z) ≥ 0. As
before, after constructing and solving the appropriate SDP we derive the following result:

20

, given an (µ, L)-smooth
Theorem 5. For any δ ∈ [0,
function f : Rn → R and any sequence of iterates {xk}k≥1 generated using noisy GD with
the Goldstein rule, the bound

5 − 2) and (cid:15) ∈

1 − 1−δ

(1+δ)2 , 1

2

√

(cid:16)

(cid:17)

(cid:18)

fk+1 − f∗ ≤

1 −

4µ(cid:15)(1 − δ)2
L

(cid:20) 1 − δ
(1 + δ)2 − (1 − (cid:15))

(cid:21)(cid:19)

(fk − f∗)

admits an SOS certiﬁcate of degree-1.

Proof. Deﬁning σ5 = 2µ(cid:15)(1−δ)2

L

(cid:16) 1−δ

(cid:17)
(1+δ)2 − (1 − (cid:15))

, σ7 = 1 and

t = 1 −

2µ(cid:15)(1 − δ)2
L

(cid:18) 1 − δ
(cid:19)
(1 + δ)2 − (1 − (cid:15))

,

we have that t(fk − f∗) − (fk+1 − f∗) is equal to

2(cid:15)(1 − δ)2 ((cid:15)(1 + δ)2 − 3δ − δ2)
(L − µ)(1 + δ)2

(cid:107)gk +µ(x∗ − xk)(cid:107)2 + σ5h5(z) + σ7h7(z).

(42)

The ﬁrst term in the right-hand-side of equation (42) is strictly positive since (cid:15) > 1 − 1−δ
(1+δ)2
implies (cid:15)(1 + δ)2 > δ2 + 3δ. In addition, any sequence of iterates {xk}k≥1 generated by noisy
GD with the Goldstein rule for minimizing a function f ∈ F µ,L(Rn) satisﬁes h5(z) ≥ 0 and
h7(z) ≥ 0. Since σ5, σ7 ≥ 0, the right-hand-side of (42) is positive. Hence, the left-hand-side
of equation (42) is also positive, concluding the proof.

Lastly, from Theorem 5, we recover the bound for GD with the Goldstein rule in the

noiseless case (i.e., δ = 0), which is given by

(cid:18)

fk+1 − f∗ ≤

1 −

(cid:19)

4µ(cid:15)2
L

(fk − f∗).

3.3 The Wolfe Conditions

A step size chosen using the Wolfe conditions must satisfy the following two inequalities:

f (xk +γk dk) ≤ f (xk) + c1γk d(cid:62)

k gk

k+1 dk ≥ c2 d(cid:62)
g(cid:62)

k gk,

(43)

(44)

where dk is a descent direction and 0 < c1 < c2 < 1, e.g. see [28, Page 37].

For the Wolfe conditions, we only have results for noiseless GD. In this setting, dk = − gk

and hence equations (43)-(44) become

f (xk −γk gk) ≤ f (xk) − c1γk(cid:107)gk(cid:107)2,

k+1 gk ≤ c2(cid:107)gk(cid:107)2.
g(cid:62)

(45)

(46)

21

Next, we show that the following polynomial inequality holds when GD with the Wolfe
conditions is applied to an L-smooth function:

fk − fk+1 −

c1(1 − c2)
L

(cid:107)gk(cid:107)2 ≥ 0.

(47)

Indeed, as f is L-smooth we have that

(∇f (x) − ∇f (y))(cid:62) (x − y) ≤ L(cid:107)x − y(cid:107)2,

which for x = xk+1 = xk −γk gk and y = xk gives

(cid:0)gk+1 − gk

(cid:1)(cid:62) (−γk gk) ≤ Lγ2

k(cid:107)gk(cid:107)2.

In turn, this implies that

0 ≤ g(cid:62)

k gk+1 + (Lγk − 1) (cid:107)gk(cid:107)2

≤ (Lγk − 1 + c2) (cid:107)gk(cid:107)2,

by (46)

which shows that

γk ≥

1 − c2
L

.

(48)

Lastly, (48) together with (45), gives us

0 ≤ fk − fk+1 −

c1(1 − c2)
L

(cid:107)gk(cid:107)2.

In the next theorem, the polynomials we use in the search of a degree-1 SOS certiﬁcate
(10) are h1(z) ≥ 0, . . . , h6(z) ≥ 0 given in (9), as well as (47), denoted by h7(z) ≥ 0.
Constructing and solving the appropriate SDP, we obtain the following result.

Theorem 6. For any 0 < c1 < c2 < 1, given an (µ, L)-smooth function f : Rn → R and
any sequence of iterates {xk}k≥1 generated using GD with an inexact line search satisfying
the Wolfe conditions, the bound

(cid:18)

fk+1 − f∗ ≤

1 −

(cid:19)

2µc1(1 − c2)
L

(fk − f∗)

admits an SOS certiﬁcate of degree-1.

Proof. Deﬁning σ5 = 2µc1(1−c2)

L

, σ7 = 1 and t = 1 − 2µc1(1−c2)

L

, we have

t(fk − f∗) − (fk+1 − f∗) =

c1(1 − c2)
(L − µ)

(cid:107)gk +µ(x∗ − xk)(cid:107)2 + σ5h5(z) + σ7h7(z).

(49)

Since σ5 and σ7 are both nonnegative, and since any sequence of iterates {xk}k≥1 generated
by GD with the Wolfe conditions for minimizing a function f ∈ F µ,L(Rn) satisﬁes h5(z) ≥ 0
and h7(z) ≥ 0, the right-hand-side of (49) is nonnegative. The left-hand-side of equation
(49) is also nonnegative, which concludes the proof.

22

4 Conclusions and Future Work

This paper proposes a new technique for bounding the convergence rates for various algorithms—
namely, by searching for SOS certiﬁcates. This leads to a hierarchy of SDPs, for which the
ﬁrst level of the hierarchy is dual to the SDP induced by the one-step PEP as discussed in
Section 2.5. Furthermore, using the ﬁrst level of the SOS hierarchy, we derive new bounds
for gradient descent with three popular inexact line search methods.

However, our technique does not necessarily produce tight bounds, since it entails two
relaxation steps. For one, the constraints characterizing the function class or algorithm may
be relaxed. Secondly, we relax the constraint that p(z) be nonnegative to the constraint that
p(z) is an SOS. Recall that while SOS implies nonnegativity, the converse is not necessarily
true. Proving the tightness of the derived bounds will have to be done via other means.

At present, we have only utilized the ﬁrst level of the proposed hierarchy by searching
for degree-1 certiﬁcates. In future work, we look to apply the SOS framework to broader
function classes, for which exact F-interpolability conditions have not been formulated. In
this setting, the SDP formulation of the f-PEP (dual to our degree-1 SOS-SDP) would, in
general, not be tight, and going higher up the hierarchy may produce tighter contraction
factors (as the degree of the SOS-SDP increases). Finally, in the instances where SOS
certiﬁcates cannot be found, it would be desirable to examine why the technique fails to
better understand the scope for which this approach may be applied.

Acknowledgements

The authors are supported by a Singapore National Research Foundation (NRF) Fellowship
(R-263-000-D02-281).

References

[1] A. A. Ahmadi. Sum of squares (SOS) techniques: An introduction. http://www.
princeton.edu/~amirali/Public/Teaching/ORF523/S16/ORF523_S16_Lec15.pdf.

[2] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, second edition, 1999.

[3] G. Blekherman, P. A. Parrilo, and R. R. Thomas. Semideﬁnite Optimization and Convex

Algebraic Geometry, volume 13. MOS-SIAM Series on Optimization, 2012.

[4] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,

2009.

[5] A. I. Cohen. Rate of convergence of several conjugate gradient algorithms. SIAM

Journal on Numerical Analysis, 9(2):248–259, 1972.

[6] Y.-H. Dai. Nonlinear conjugate gradient methods. Wiley Encyclopedia of Operations

Research and Management Science, 2011.

23

[7] E. de Klerk, F. Glineur, and A. B. Taylor. On the worst-case complexity of the gradient
method with exact line search for smooth strongly convex functions. Optimization
Letters, 11(7):1185–1199, 2017.

[8] E. de Klerk, F. Glineur, and A. B. Taylor. Worst-case convergence analysis of gradi-
ent and Newton methods through semideﬁnite programming performance estimation.
Technical report, https://arxiv.org/abs/1709.05191, 2017.

[9] Y. Drori and A. B. Taylor. Eﬃcient ﬁrst-order methods for convex minimization: a

constructive approach. Mathematical Programming, Jun 2019.

[10] Y. Drori and M. Teboulle. Performance of ﬁrst-order methods for smooth convex min-
imization: A novel approach. Mathematical Programming, 145(1):451–482, 2014.

[11] Y. Drori and M. Teboulle. An optimal variant of Kelley’s cutting-plane method. Math-

ematical Programming, 160(1):321–351, 2016.

[12] M. Fazlyab, M. Morari, and V. M. Preciado. Design of ﬁrst-order optimization algo-
rithms via sum-of-squares programming. In the 18th IEEE Conference on Decision and
Control, pages 4445–4452, 2018.

[13] M. Fazlyab, A. Ribeiro, M. Morari, and V. M. Preciado. Analysis of optimization
algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM
Journal on Optimization, 28(3):2654–2689, 2018.

[14] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In
V. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Con-
trol, Lecture Notes in Control and Information Sciences, pages 95–110. Springer-Verlag
Limited, 2008. http://stanford.edu/~boyd/graph_dcp.html.

[15] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming,

version 2.1. http://cvxr.com/cvx, 2014.

[16] G. Gu and J. Yang. Optimal nonergodic sublinear convergence rate of proximal point
algorithm for maximal monotone inclusion problems. Technical report, https://arxiv.
org/abs/1904.05495, 2019.

[17] B. Hu, P. Seiler, and L. Lessard. Analysis of approximate stochastic gradient using
quadratic constraints and sequential semideﬁnite programs. Technical report, https:
//arxiv.org/pdf/1711.00987.pdf, 2017.

[18] D. Kim. Accelerated proximal point method and forward method for monotone inclu-

sions. Technical report, https://arxiv.org/abs/1905.05149, 2019.

[19] D. Kim and J. A. Fessler. Optimized ﬁrst-order methods for smooth convex minimiza-

tion. Mathematical Programming, 159(1):81–107, 2016.

24

[20] D. Kim and J. A. Fessler. Optimizing the eﬃciency of ﬁrst-order methods for decreasing
the gradient of smooth convex functions. Technical report, https://arxiv.org/abs/
1803.06600, 2018.

[21] J. Lasserre. A sum of squares approximation of nonnegative polynomials. SIAM Review,

49(4):651–669, 2007.

[22] M. Laurent. Sums of Squares, Moment Matrices and Optimization Over Polynomials,

pages 157–270. Springer New York, New York, NY, 2009.

[23] L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms
via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95, 2016.

[24] F. Lieder. On the convergence rate of the halpern-iteration. Technical report, http:

//www.optimization-online.org/DB_FILE/2017/11/6336.pdf, 2019.

[25] D. G. Luenberger and Y. Ye. Linear and Nonlinear Programming. Springer International

Publishing, 4th edition, 2016.

[26] A. Nemirovski. Optimization II: Numerical methods for nonlinear continuous optimiza-

tion. https://www2.isye.gatech.edu/~nemirovs/Lect_OptII.pdf.

[27] R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. I. Jordan. A general analysis
of the convergence of ADMM. In Proceedings of the 32nd International Conference on
Machine Learning, volume 37, 2015.

[28] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, NY, USA,

second edition, 2006.

[29] P. A. Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Math-

ematical Programming, 96(2):293–320, 2003.

[30] P. A. Parrilo. Polynomial Optimization, Sums of Squares, and Applications. Society for

Industrial and Applied Mathematics, 2013.

[31] M. Putinar. Positive polynomials on compact semi-algebraic sets. Indiana University

Mathematics Journal, 42(3):969–984, 1993.

[32] E. K. Ryu, A. B. Taylor, C. Bergeling, and P. Giselsson. Operator splitting performance
estimation: Tight contraction factors and optimal parameter selection. Technical report,
https://arxiv.org/abs/1812.00146, 2018.

[33] S. S. Y. Tan. Performance analysis of optimization algorithms using semideﬁnite pro-
gramming. Master’s thesis, Department of Electrical and Computer Engineering, Na-
tional University of Singapore, https://scholarbank.nus.edu.sg/handle/10635/170801,
2020.

25

[34] S. S. Y. Tan, A. Varvitsiotis, and V. Y. F. Tan. A uniﬁed framework for the convergence
analysis of optimization algorithms via sums-of-squares. In The Signal Processing with
Adaptive Sparse Structured Representations (SPARS) workshop, 2019.

[35] A. Taylor, B. V. Scoy, and L. Lessard. Lyapunov functions for ﬁrst-order methods:
Tight automated convergence guarantees. In 35th International Conference on Machine
Learning Learning, 2018.

[36] A. B. Taylor and F. Bach.

Stochastic ﬁrst-order methods: non-asymptotic and
computer-aided analyses via potential functions. In Proceedings of Machine Learning
Research, volume 99, pages 1–58, 2019.

[37] A. B. Taylor, J. Hendrickx, and F. Glineur. Exact worst-case performance of ﬁrst-
order methods for composite convex optimization. SIAM Journal on Optimization,
27(3):1283–1313, 2017.

[38] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation
and exact worst-case performance of ﬁrst-order methods. Mathematical Programming,
161(1):307–345, 2017.

[39] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case convergence rates of the
proximal gradient method for composite convex minimization. Journal of Optimization
Theory and Applications, 178(2):455–476, 2018.

[40] K. C. Toh, M. J. Todd, and R. H. Tütüncü. SDPT3 — a Matlab software package
for semideﬁnite programming, Version 1.3. Optimization Methods and Software, 11(1-
4):545–581, 1999.

[41] R. H. Tütüncü, K. C. Toh, and M. J. Todd. Solving semideﬁnite-quadratic-linear pro-

grams using SDPT3. Mathematical Programming, 95(2):189–217, 2003.

[42] Wolfram Research, Inc. Mathematica, Version 12.0. Champaign, IL, 2019.

26

