9
1
0
2

p
e
S
9
2

]

C
D
.
s
c
[

2
v
3
5
8
0
1
.
9
0
9
1
:
v
i
X
r
a

IL NUOVO CIMENTO

Vol. ?, N. ?

?

A high-level characterisation and generalisation of communication-
avoiding programming techniques

T. Weinzierl (1) (2)
(1) Department of Computer Science, Durham University, Durham, Great Britain
(2) Institute for Data Science, Durham University, Durham, Great Britain

Summary. — Today’s hardware’s explosion of concurrency plus the explosion
of data we build upon in both machine learning and scientiﬁc simulations have
multifaceted impact on how we write our codes. They have changed our notion
of performance and, hence, of what a good code is: Good code has, ﬁrst of all,
to be able to exploit the unprecedented levels of parallelism. To do so, it has to
manage to move the compute data into the compute facilities on time. As com-
munication and memory bandwidth cannot keep pace with the growth in compute
capabilities and as latency increases—at least relative to what the hardware could
do—communication-avoiding techniques gain importance. We characterise and clas-
sify the ﬁeld of communication-avoiding algorithms. A review of some examples of
communication-avoiding programming by means of our new terminology shows that
we are well-advised to broaden our notion of ‘communication-avoiding” and to look
beyond numerical linear algebra. An abstraction, generalisation and weakening
of the term enriches our toolset of how to tackle the data movement challenges.
Through this, we eventually gain access to a richer set of tools that we can use to
deliver proper code for current and upcoming hardware generations.

1. – Introduction

Advance in computational sciences, engineering, medicine and other computational
disciplines often relies on simulation results to drop in faster, even though researchers
continuously increase the problem size or detail. There are other important driving
factors behind progress such as improved models or numerical techniques. Yet, speed
and size are omnipresent desirable characteristics and requirements. To suit them, the
community relies on the improvement of two tools: their computer hardware and their
software. Improvement here means speed.

Computers are becoming faster. The Top-500 list [42] documents their impressive
evolution. In many manuscripts, the speed growth is attributed towards Moore’s law;
which is not in line with the original statement from [32]. What is however true is that
the growth—though maybe with a reduced constant before the exponential—of Moore’s
law does and continues to hold for computers’ potential capabilities.

c(cid:13) Societ`a Italiana di Fisica

1

 
 
 
 
 
 
2

T. WEINZIERL

Parallelism is key to make computers faster [1]. Starting from around 2004, computers
stagnated in frequency or even reduced it [40]. This is due to physical (quantum) eﬀects
introducing nondeterministic eﬀects, i.e. errors in a deterministic view of the world,
energy envelopes, manufacturing constraints, and so forth. All eﬀectively forbid vendors
to increase the frequency or to shrink the dies aggressively. Shrinking allows us to work at
lower voltages which in turn creates freedom to increase the speed. Once these upscaling
techniques are not an option anymore, we can still increase the number of compute
units. This yields more powerful computers despite the fact that individual compute
components do not speed up.

The seminal, popular science report “The Free Lunch Is Over: A Fundamental Turn
Toward Concurrency in Software” [40] documents this insight as well as its consequences:
software has to be made ready for parallelism, too. A second seminal paper is the SCaLeS
report championed by David E. Keyes [20, pp. 32] which contains content similar to an
algorithmic version of Moore’s Law: The report highlights that the dramatic growth
of performance and capabilities on the hardware side over the last decades is matched
by improvements on the algorithm side. The manuscript’s introductory example from
linear algebra showcases this by the transition from direct Gauss Elimination into full
multigrid [41] which yields a gain in compute eﬃciency equivalent to 35 years of hardware
evolution. More examples along these lines exist.

If we cast statements on algorithmic improvements into statements on performance
without machine context, we run risk to make an error very similar to the misinterpre-
tation of Moore’s law as performance statement. Improvements on the hardware side
and on the algorithm side are not orthogonal: In the present paper, we sketch nuances
of the growth of hardware parallelism and highlight that the sole focus on an increase
of concurrency(1) on the software side falls short, as data movements quickly become a
limiting factor (Section 2). As a consequence, programmers of high-end software have
to familiarise themselves with communication-avoiding techniques.
In Section 3, we
attempt to classify communication-avoiding techniques and claim that the phrase has
to be weakened and generalised to include many more techniques than the traditional
communication-avoiding tool set from linear algebra. Consult [8] for a comprehensive
overview over the latter. Only a generalisation and, hence, an expansion of our toolset
accommodates the pressing needs imposed on us by the hardware evolution. The two
sections 4 and 5 give examples for communication-avoiding techniques in line with this
generalised notion. The examples allow us to conclude (Section 6) which next steps might
be required to tackle the challenges around communication in data-intense computing.

2. – One-way co-design

Reports and roadmaps predict that the exascale wall will be climbed through an in-
crease of parallelism on the hardware side [1, 10]. There are diﬀerent levels of parallelism.
First, vendors increase the parallelism through the widening of vector registers—
AVX512 is an example—or lockstepped hardware threads as we ﬁnd them in GPGPUs.
Parallelism on this level materialises in code which performs basic arithmetic operations

(1) In the tradition of [26], we use concurrency as a theoretical concept that highlights that
two control ﬂows in hardware or in code run independently, can be merged into each other in
any order, or may be executed in parallel, i.e. it is the term parallelism that denotes that things
happen at the same time.

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING

3

on small, fully populated vectors or vector segments. NVIDIA calls its cores streaming
multiprocessors (SMs) which work with many sets (warps) of hardware threads per core
which are swapped quickly. This original SIMT’s (single instruction multiple threads) [28]
concept is conceptionally close to vectorisation. Its swapping idea materialises hardware
concurrency hiding latency and bandwidth constraints.

Second, vendors increase the core count per processor. Intel might have abandoned
their own manycore product line, but we see mainstream supercomputers nevertheless
host already 20 or more cores per processor. Diﬀerent to vector registers, multicores run
independently of each other, i.e. each executes its own instructions. GPGPUs in general
operate with lower physical core/SM counts. As NVIDIA however supports higher thread
divergence, i.e. allows multiple threads ran simultaneously to run diﬀerent code branches
and to synchronise with a subset of other threads, the SIMT paradigm moves closer to
multicore parallelism. Cores and SMs are connected to each other through a shared
memory. One core thus can read data written by another core.

Finally, supercomputers connect multiple of these multicore devices (nodes) with
each other. Nodes do not share data spaces but communicate with each other through
messages. Alternatively, new hardware in the market, such as NVLink, joins distributed
memory logically into large shared spaces and thus pushes multi-node or multi-device
architectures logically into the multicore domain.

Recent hardware generations exhibit a fast growth of concurrency on the vector and
multicore level, while the number of nodes seems to stagnate. One reason for this are data
movements which are subject of the subsequent sections: If it is already expensive and
problematic to transfer data between memory and vector registers as well as in-between
cores, it is even more challenging to transfer data over long distances via messages.

In a co-design fashion, the tremdendous increase in concurrency on the hardware
side forces developers to rewrite their codes such that they are capabable of exploiting
this [40]. The diﬀerent hardware levels here require diﬀerent programming eﬀorts. An
increase in vectorisation parallelism forces programmers to phrase algorithms in terms of
small vectors. These vectors are treated as dense, i.e. vector units on a chip manipulate
vectors agnostic of their content. An increase in core counts requires programmers to
decompose algorithms into ﬁne-granular activities (tasks). This allows a runtime system
to deploy tasks among the cores and, hence, to keep them busy. An increase in node
counts requires programmers to decompose data spaces into larger regions and to design
codes such that the eﬀort to keep these distributed data spaces consistent remains limited.
If we formalise the induced communication as graph, the graph has to be as sparse as
possible. In linear algebra, this corresponds to the need for sparse matrices.

Co-design the other way round is harder to ﬁnd. Algorithms rarely guide the hard-
ware design [35], and special-purpose chips materialising particular algorithms are not
found that often in mainstream HPC. Recently however, we witness an inﬂuence the
other way round: In-hardware security algorithms (encryption) become part of chips,
FPGAs—software-programmable hardware—gain momentum, and GPUs start to be-
come equipped with special-purpose silicon; tensor cores and raycasting/-tracing entities
are the prime examples. The economic success of GPGPUs as a whole has, to some
degree, to be attributed towards machine learning algorithms. One may interpret this
as algorithm-to-hardware co-design. We however prefer a diﬀerent interpretation:

.
2

1. Changing optimality metrics: over-vectorisation and task decomposition. – Many
sophisticated, eﬃcient algorithms reduce the concurrency: In traditional computer sci-
ence, an algorithm is eﬃcient if it requires fewer computations than a na¨ıve, competing

4

T. WEINZIERL

approach. Many computations—preferable of the same type—however tend to yield a
high concurrency, while very few calculations, in the “worst case” with tight data depen-
decies, cannot make use of a parallel machine. Sometimes, simpler is faster.

Some sophisticated codes thus combine the best algorithm on one scale with a simpler
variant with higher concurrency on another scale. Examples for such hybrids that support
our eﬃciency-vs-concurrency claim are block-structured sparse matrix algorithms that
use full matrix algorithms for subblocks [30], linear algebra subroutines which switch
to dense or sparse implementations depending on the number of non-zeros and their
pattern [3], particle dynamics codes that use a grid to administer the cut-oﬀ radius
which is so coarse that it always hosts a reasonable number of molecules [11, 37, 45],
or multigrid codes that rely on algebraic multigrid for coarse resolutions and employ
geometric multigrid with rediscretisation on ﬁner levels [34, 43]. Neural networks often
are an extreme case: Conceptually, they are signiﬁcantly simpler than known approaches
to minimise cost functions describing how well a network represents a training or test
data set, respectively. Their success in ﬁtting strongly overparameterised ansatz functions
stems from massive data plus unprecedented concurrency; typically subject to gradient
descent, i.e. a relatively basic algorithmic approach.

Besides the aforementioned revisit of “less eﬀective” algorithms which allow us to
exploit (vector) concurrency and thus eﬀectively reduce the time-to-solution, the decom-
position of our algorithms into ﬁner and ﬁner tasks is the second important code design
trend in large-scale computing. The latter technique uncovers ﬁne-granular concurrency
and is often followed by a rearrangement of these tasks such that the machine of choice is
exploited better. The rise of task systems—Threading Building Blocks, Cilk, Charm++,
StarPU, and others—documents the popularity of the latter approach. Developers have
to carefully balance task decomposition against vectorisation eﬃciency: Too small tasks
fail to keep the vectorising codes busy. Too big tasks lack the required level of concur-
rency. Both dimensions beneﬁt from homogeneous, non-hierarchical, uniform workload
which can be split relatively straightforwardly [1]. Eventually, both beneﬁt from hybrids.
The paradigm shift to construct hybrids exploiting less eﬃcient algorithms than the-
oretically known results from an update of our cost model, i.e. our understanding what
induces high or low runtimes respectively. New cost models are rarely introduced explic-
itly. Most of the time we alter them implicitly in our minds and support the outcomes
through experimental studies. Yet, the punchline remains the same: The delicate balance
between high concurrency vs. parallelisation overhead in some cases outweights classic
operation count-based cost models. The paradigm shift on the hardware side has recal-
ibrated and changed the correlation of ﬂoating point operation counts (FLOP numbers)
to performance.

.
2

2. Vertical and horizonal communication constraints. – A second challenge rises
in the shadow of hardware concurrency growth: The architectures become ill-balanced
w.r.t. data delivery. While the cores with their vector registers can yield an impressive
number of computations per second and while there are many cores, we struggle to feed
them with data [10, 16, 17, 24, 25, 27].

There is a vertical data movement challenge (Fig. 1): Cores can process data which
is by a magnitude larger than what a standard main memory can deliver. One hardware
workaround here is to introduce caches, i.e. intermediate memories in-between the main
memory and the compute units. They are small yet can hold hot data, i.e. data that is
frequently or soon used. An alternative to caching is the use of massive oversubscription:
Whenever a core runs into a situation where the computations of one thread require

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING

5

Fig. 1. – Schematic illustration of vertical and horizontal data ﬂow in a conventional supercom-
puter consisting of multiple nodes where each node hosts multicore cores which in turn employ
vector registers. The cores access the main memory through a cascade of cache levels, while the
nodes communicate with each other through an interconnect.

data which is not yet brought into the chip’s registers, it “suspends” the execution and
instead progresses another “warp” of (vector) operations from a diﬀerent thread. In the
meantime, the required data can be brought in. The latter technique is used in GPGPUs.
As sketched above, such chips logically can be interpreted as a set of cores where only one
core is active (and thus requires compute resources) at a time, while the others prepare
for their next move.

Intel introduces a new cache level with its Optane technology [2, 5, 22]. Optane
eﬀectively renders the main memory into a further cache—either implicitly managed by
the company or explicitly managed by user codes—and introduces a huge yet relatively
slow main memory. Intel furthermore abandons the idea of inclusive caching with the
Intel R(cid:13) Xeon R(cid:13) Scalable processor (Skylake). Without inclusive caching, the probability
increases that cache misses on a small, close cache induce large runtime penalties: the
missed data is not backed up by the subsequent cache level. If caches become (relatively)
slower, it hurts even more. On the GPGPU side, NVIDIA weakens the strict lockstepping
in warps. This allows users to run codes with a larger thread divergence, yet harms the
GPGPU’s eﬃciency if used heavily. The vertical challenge gains weight.

There is an additional horizontal dimension to the data movement challenge (Fig. 1):
Data movement penalties gain weight as vendors integrate more and more cores into one
chip. Cache levels have to be split or shared between more and more compute units and
to keep their content consistent requires increasing eﬀort manifesting in runtime penalties
(NUMA). On the inter-node level, interconnects remain to be by magnitudes slower than
memory. This also holds for processor-accelerator connections.

Both vertical and horizontal data movement constraints imply that eﬃciency gains
resulting from over-vectorisation and task decomposition run risk to become obsolete.
The fastest compute instruction sequence, i.e. source code, is of no value if the processing
units can not be fed with data. The hardware evolution makes it mandatory that codes

CoresMemory     L3       L2        L1   CoreNodesNodeinterconect6

T. WEINZIERL

avoid communication along all hardware dimensions.

3. – Terminology

Communication-avoiding (CA) algorithms are not new. One of the most important
papers stems from 2013, e.g., [8] and reviews a vast number of CA techniques from linear
algebra. We identify here six dimensions along which we can classify CA techniques:

1. In the tradition of refactoring [12], communication-avoiding modiﬁcations always have

to be semantics-preserving, i.e. the outcome of an algorithm subject to a communication-
avoiding technique has to remain the same. In scientiﬁc computing, “the same” usually
means “reasonably close to machine precision”. This implies that a communication-
avoiding technique may not introduce any numerical instability for the input data of
interest, but we may classify techniques into stability-preserving and stable for
data of interest.

2. We consider both data movements in-between nodes and within a computer, i.e. be-
tween chip and caches, caches and main memory, memory and accelerator, as com-
munication. This is, our notion of communication comprises intra-node and inter-
node communication. The phrase communication-avoiding thus includes cache
optimisation techniques [21]. As novel hardware blurs the distinction between intra-
node and inter-node (through remote memory access or distributed shared memory
for example), we may characterise techniques rather by means whether they address
problems of horizontal or vertical information ﬂow: Horizontal circumscribes
challenges that arise from the growth of concurrency such as increasing node and core
counts but also wider vector registers. Vertical techniques tackle challenges that arise
from the fact that information typically is spread out, i.e. resides on diﬀerent memory
levels, memory segments, devices.

3. Though papers traditionally motivate a communication-avoiding technique through
one application and demonstrate the impact of their ideas by means of one applica-
tion, we can distinguish application-speciﬁc from generic approaches. The latter
techniques apply directly to many diﬀerent areas.

4. There are two extremes of CA code changes: Techniques can preserve the exact
algorithmic steps per datum. If we work in this regime, we are restricted to a
re-orchestration of operations over all data, but the path how this outcome had been
computed is preserved per datum. We are notably not allowed to change the algorithm
that led to a particular result. On the other side of the spectrum are techniques and
algorithms which change the algorithm’s behaviour or even the algorithm itself,
i.e. they change how results are obtained. There are many approaches in-between; in
particular approaches which weakly preserve the behaviour, i.e. yield diﬀerent path-
ways towards the results only in some cases. The in-the-middle regime comprises for
example code modiﬁfcations which materialise in altered iteration counts, required
operations, and so forth but stick to the same algorithmic blueprint.

5. Some CA techniques are strictly monotonous in a sense that they always reduce
the communication. Others pay oﬀ asymptotically for large, characteristic data sets
yet not for each individual choice.

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING

7

6. Finally, many (application) scientists are not motivated by communication-avoiding
per se, but start to investigate CA techniques as they suﬀer from communication penal-
ties. From their point of view, it makes sense to distinguish strong communication-
avoiding techniques from weak communication avoiding techniques. Strong
means that communication is eliminated compared to a na¨ıve baseline implemen-
tation. In the weak sense, a communication-avoiding technique primarily eliminates
its negative impact—it might not reduce the actual communication. Such techniques
are communication-ﬂaw-avoiding.

4. – Communication-avoiding techniques in a strong sense

Table I. – Overview of discussed communication-avoiding techniques. Some labels are subjective
and depend, in practice, very much on the particular application. However, it is fair to assume
that all techniques are strong in a sense that they actually eliminate communication.

l
a
c
i
t
r
e
V
/
l
a
t
n
o
z
i
r
o
H

c
i
r
e
n
e
G

H+V

no

e
l
b
a
t
S

yes

yes
no
no(lossy)/yes
yes
yes

yes
yes/no

yes
yes

yes

H
H+V
H+V
H+V
H+V

H
H+V

V
V

H

no
yes
yes
yes
yes

yes
no

yes
no

yes(no)

s
u
o
n
o
t
o
n
o
M

yes

yes
yes
yes
no
yes

yes
no

yes
yes

yes

e
v
r
e
s
e
r
P

no

yes
yes
yes
yes
yes

yes
no

yes
yes

yes

g
n
o
r
t
S

yes

yes
yes
yes
yes
yes

yes
yes

yes
yes

yes

Temporary variable/storage elim-
ination
Recomputation
Reduced precision
Compression
RLE
Padding

Overlapping (loop unrolling)
Anarchic

Tiling/serialisation
Task fusion

Sparsify collective graph (dynam-
ically)

.
4

1. Volume reduction. – Volume reduction is the most obvious communication-
avoiding technique. A classic example is Gaussian Elimination which reorders entries
such that the number of ﬁll-ins is reduced [8]. Today, such algorithms are integral part of
all major direct solver packages. Algorithmic steps are permuted such that zero entries
in a data structure are not overwritten with non-zero data. Consequently, follow-up later
steps which would otherwise have to eliminate non-zero entries can be skipped. Yet, this
is not the prime goal from a data point of view. The prime target besides a sparsiﬁcation
of the execution/task graph is to avoid a temporary blow-up of the used data structures.
The observation behind this technique is that an intelligent permutation of computations
early in the algorithm’s run implies that we don’t have to store large temporary data.

8

T. WEINZIERL

If this fails, a second technique is the replacement of temporary data/variables with
recomputations. Results are not stored persistently and thus don’t have to be held or
communicated. Instead, we compute results on-the-ﬂy again whenever we need them.
This technique trades memory for computations. It is thus particularly attractive if we
suﬀer from horizontal communication ﬂaws.

If it is not possible to avoid temporary data storage or data usage, precision reduc-
tion becomes popular [9, 23, 24, 27, 29]. Machine learning pushes the introduction of
precision reduction [17], but it is natural to exploit new native hardware formats with
reduced memory footprint in scientiﬁc computations, too. Such a switch yields massive
memory footprint reductions though might proof to be problematic for the stability of
the computations. Therefore, we currently witness massive investments into research
around algorithms which combine diﬀerent precisions without compromising on the out-
put quality [16], or tools which guide the developers through the zoo of novel precisions.
Our own work sticks to standard IEEE precision for all computations, but translates
all data into hierarchical representations prior to their storage [4, 11, 43]. An on-the-ﬂy
analysis of the data yields how many signiﬁcant bits (bits that hold information) are held
per variable. We then cut down the data after a suitable number of bytes without loosing
information before we stream them out of the core. After the compressed, low footprint
data is loaded again, we convert it back into a native precision. Other papers propose
similar storage schemes which store only few bits per datum relative to a (predicted)
reference value [33].

For integer data or sparse records holding lots of zeroes, run-length encoding has
proven to be particularly useful. In its simplest case, it eliminates long sequences of zeroes
for the price of additional header ﬂags in a data stream [38]. In particular for modern
high-level programming languages, it is a CA technique to eliminate the omnipresent
padding and thus to squeeze out bytes without semantics from the data [4].

.
4

2. Synchronisation elimination. – Volume reduction tackles bandwidth penalties.
Modern architectures however suﬀer from limited bandwidth and high latency. Latency
notably causes problems whenever an algorithm synchronises various compute units too
often. It is non-trivial to avoid synchronisation and typically impossible to eliminate it
completely, as algorithms synchronise to keep their distributed states consistent.

Overlapping data spaces and running calculations redundantly tackles the synchroni-
sation challenge. It is a tool classic in stencil-based linear algebra. The idea is to revisit
the data decomposition. Let the data be decomposed into chunks such that each core or
node can run its evaluations independently of the other compute units on its chunk. We
achieve this by augmenting the data chunk with halos, i.e. with copies from the other
chunks such that the calculation itself does not need any data transfer. After the compu-
tation, these halos are updated, as other compute units likely have changed their value.
We can now analyse our data access needs and make the halos bigger such that we can do
two calculations in a row. This typically requires us to run some calculations within the
(larger) halos, too: we “mirror” some calculations done on other chunks. The approach
can be recursively extended. Within the overlaps, we run redundant calculations, but
this redundancy is designed such that the redundant computations allow the algorithm
to immediately run a subsequent calculation without the need to synchronise. We re-
duce the synchronisation frequency; though at the price of an increased data volume.
Increased data overlaps are a valid label for this technique. The CA ﬂavour however is
emphasised once we highlight that the technique involves loop unrolling of the outer al-
gorithm loop and synchronises only once per unrolled loop body. s-step Krylov methods

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING

9

are close to this concept though they give up on some data consistency [14].

A radical alternative to enlarged halos which also reduces the synchronisation is an-
archic programming [47]. We drop the consistency considerations that led to enlarged
data spaces and ignore whether incoming information (data updates) from other entities
have already arrived. Loops on data chunks continue to iterate, knowing that eventu-
ally another code part will feed them with the required input data. Such an anarchic
paradigm requires the underlying algorithm to be robust w.r.t. inconsistent input data.
Elliptic problems seem to suit this requirement.

.
4

3. Single-touch and single-load semantics. – Whenever data volume or latency pose
problems, these problems are ampliﬁed if information has to be communicated multiple
times. We rarely ﬁnd distributed memory codes that exchange messages multiple times.
It happens however frequently that data is moved through the caches multiple times
(capacity misses) or that codes swap data to and from the GPGPU due to their limited
memory capacity.

The optimisation of algorithms towards particular cache architectures is well-under-
stood. Key ingredient here is typically proper tiling (chunkiﬁcation) such that tiles/chunks
ﬁt into the respective caches, and the attempt to increase the spatial and temporal lo-
cality of data accesses [21]. Algorithms or loops, respectively, that read (array) data
multiple times are reordered such that data that are required brieﬂy after each other
are close in the main memory, and such that data that are required at one point are
re-used again brieﬂy thereafter. We adapt the data access pattern of an algorithm and
the data storage to each other. Sophisticated cache blocking techniques [21, 36] exploit
that most memory is organised in pages/blocks/lines, i.e. architectures do not exchange
single entities of data but always transfer whole segments of memory—a built-in message
agglomeration. They make the chunk sizes match particular cache sizes.

Clever data access and data storage reordering reduces multiple loads into caches. An
extreme case of such clever reordering is the total serialisation of an algorithm [44]. Data
are reordered such that the algorithm does not require any indirect memory accesses any-
more, but reads all data in as stream and holds data temporarily only in stacks, streams
or small arrays. As a consequence, an algorithm becomes inherently cache-optimal (cache
oblivious)—the probability that the head of a stack remains in a cache is always very
high as long as the number of stacks remains small—and avoids communication over the
memory interconnects.

Data ordering and operation orchestration go hand in hand. On the orchestration
side, task fusion is the most important single-touch optimisation [7]. Task fusion reorders
operations (tasks) such that two tasks that access the same piece of data are evaluated
directly after another. One might thus (logically) fuse the two pieces of codes into one
big task. If tasks operate on a reasonably small amount of data, we may assume that
all data are held in fast caches after the termination of the ﬁrst task. Task fusion is the
task language’s counterpart to loop fusion on arrays from a CA point of view.

.
4

4. Localisation. – Bandwidth and latency problems require diﬀerent actions. Quan-
titatively, these problems are aﬀected by the number of participating entities: the more
partners involved in communication, the harder the challenge. In many cases, a reason-
ably small number of communication partners makes communication ﬂaws hide behind
runtime noise. If problems however do arise, it is sometimes only few communication
partners that cause delays on all other entities. In such cases, it is reasonable to break
up many-to-many data exchange (collectives). Rather than global data exchange, we lo-

10

T. WEINZIERL

calise data ﬂow. Such a sparsiﬁcation of the data exchange graph can be done a priori by
analysing data dependencies. In multiscale algorithms for example, it might be suﬃcient
to let a computation depend only on a certain subregion of one scale rather than the
whole level of one scale [31]. Sparse collectives have been proposed for MPI [19], though
one can always implement such sparse graphs via tailored MPI (sub-)communicators.

Instead of a static localisation, we have studied algorithms which localise the data
exchange graph dynamically [45]. The concept here is that we query the algorithm for
predictions where collectives actually transfer information. If collectives exchange data
along a sparse communication graph (many entries are zero, e.g.), it is again reasonable
to replace the collective with a set of point-to-point operations.

5. – Communication-avoiding techniques in a weak sense

Table II. – Further communication-avoiding techniques in a weak sense, i.e. they do not actually
eliminate communication but they tackle communication’s negative impact.

Non-blocking exchange
Prefetching

Optimistic
Pipelining

Homogenisation

e
l
b
a
t
S

yes
yes

yes
no

yes

l
a
c
i
t
r
e
V
/
l
a
t
n
o
z
i
r
o
H

H
V

H
H

H+V

c
i
r
e
n
e
G

yes
yes

no
no

yes

e
v
r
e
s
e
r
P

yes
yes

yes
yes

yes

s
u
o
n
o
t
o
n
o
M

yes
yes

no
yes

no

g
n
o
r
t
S

no
no

no
no

yes

.
5

1. Data movement hiding. – The hardware’s organisation into levels (vertical) and
nodes (horizontal) introduces hardware asynchronicity since the entities can move around
data independently and, in particular, while core components compute. This allows us
to hide data movement cost. Hiding is an example of a weaker interpretation of CA, as
the communication is not literally avoided yet negative impact is.

On the distributed memory side, non-blocking data exchange means that we trig-
ger data transfer for a particular memory region, continue to compute, and later check
whether the transfer has completed. This is a rather old feature in MPI, though still
sometimes challenging to use/realise [18, 39, 46]. Within a node, all caches today have
built-in prediction features to transfer data from and to caches. This also holds for new
deep memory [2, 5, 22]. Agnostic of the exact caching strategies, least recently used
(LRU) seems to describe most cache behaviour reasonably. LRU describes reactive cache
behaviour: the “oldest” data is removed from the cache if space is required. Explicit
prefetching can guide the caching as it triggers data movements ahead of time. It hands
a pro-active technique over to developers.

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING 11

.
5

2. Synchronisation hiding. – Optimism is a powerful communication-avoiding tech-
nique to hide synchronisation. Optimistic algorithms synchronise program parts, but
the receiver side does not wait for the synchronisation input. Instead, it optimistically
assumes what information will drop in. If this assumption turns out to be wrong later
during the computation, the receiver side either rolls back and redoes its computations
with the actual values [7] or takes the actual data into account at the earliest next op-
portunity. If rollbacks are required, optimistic codes’ runtime might deteriorate unless
we can immediately stop all calculations once invalidating data drops in.

Pipelining [14, 15] applies the data overlap idea to the control ﬂow. Its idea is that
we bring all the computations forward that feed into computations on another compute
entity. In turn, we receive data early that we need locally to determine results required
in later stages. Data required for the follow-up step is already “in the pipeline” while
we continue to compute. To construct the required level of concurrency, pipelining is
often combined with redundant calculations. Entities exchange data feeding into critical
calculations early, but each rank then computes the results independently from the oth-
ers. Both pipelining and increased overlaps from Section 4 contradict CA’s objective to
keep the memory footprint small. Many approaches relying on redundant computations
furthermore become sensitive to stability issues.

.
5

3. Homogenisation. – Computers tend to be underequipped with memory access
speed and with network speed. Yet, memory controllers and network cards usually are
suﬃciently fast to handle a few cores or nodes, respectively. Service requests from every-
body at once however make them struggle. A weak communication-avoiding technique to
address this ﬂaw is to homogenise bandwidth and network requests over time: we sprin-
In particular, we break up codes
kle communication steps over the actual execution.
written in in terms of compute phases vs. communication phases which take turns and
make many (smaller) communication steps interrupt the computation. On the node, the
same pattern implies that we carefully orchestrate all tasks such that compute-intense
tasks and memory-intense tasks take turns [6, 7]. The goal here is to ensure that some
compute facilities (cores) exploit all available memory bandwidth, while the others spend
the majority of their time in actual computations and do not interfere.

6. – Conclusion

Communication-avoiding techniques have to become omnipresent if the current hard-
ware evolution trend endures. The present collection of CA techniques is a rather arbi-
trary set inspired by the author’s own work and selected examples known to the author.
It is certainly not comprehensive. The present classiﬁcation expresses the author’s per-
sonal opinion. It is likely neither waterproof nor comprehensive either. Yet, the present
collection plus its classiﬁcation can be a starting point to think about CA techniques in
a systematic way covering whole application landscapes. Eventually, a concerted push
by the whole community is required to establish a more complete catalogue of these
techniques similar to design patterns [13]. This way, future generations of scientists will
be able to rely on a well-documented and formalised knowledge base. Without such an
eﬀort, we run risk to be left behind with hardware that we cannot (eﬃciently) use.

An inherent obstacle towards the adoption of communication-avoiding techniques is
the fact that they often require large code refactorings or even rewrites (cmp. preserve
characterisation in Section 3); at a time where researchers advocate to switch to black-
box software building blocks to make software development more mature, eﬃcient and

12

T. WEINZIERL

sustainable. Machine learning serves as prime example for the latter.
It champions
the usage of established software frameworks within multiple research projects. Our
discussion highlights however that a na¨ıve assembly of software blocks representing an
If we
algorithm’s steps (data-ﬂow paradigm) might be an ill-ﬁt to future machines:
complete one computing step, dump or stream the outcome into the next algorithmic
phase, and then continue, we will struggle to use communication-avoiding techniques.

It is almost sarcastic that the discipline that pushed us into petascale, linear algebra,
and the discipline which drove the hardware development towards the pre-exascale era,
i.e. machine learning, are kind of agnostic or even “contraproductive” towards this partic-
ular challenge ﬂavour. One reason for the success of large-scale linear algebra packages is
without a doubt their clear encapsulation. Data transfer is a challenge picked up within
the software, but the API is not designed towards CA(2). One reason for the success of
machine learning frameworks is the fact that we can use them almost as black-box. This
implies that users tend to realise a data pipeline architecture where data transfer is not
a high priority design quality criterion. The visualisation community in contrast is well-
aware of this rising challenge—mainly due to the large IO penalties computers already
face—and pushes in-situ postprocessing and software that is made for in-situ processing.
The computing community either has to follow or establish usable refactoring techniques
[12] such that developers can start from black-box units and then refactor towards CA.
Finally, it is important for the scientiﬁc computing community to continue to push
vendors towards real co-design. At the moment, engineering constraints and economic
interest (machine learning) seem to steer the direction into which our hardware evolves.
While this evolution stimulates the development of communication-avoiding techniques
and thus stimulates an area of research, it is important to challenge vendors over and
over again. Would it not be possible to build better-balanced systems?

∗ ∗ ∗

The author expresses his thanks to all collaborators and students that helped him
to study various ﬂavours of communication-avoiding algorithms. They are enlisted as
co-authors on the cited papers. A lot of the author’s recent communication-avoiding re-
search has been conducted under the umbrella of the ExaHyPE project supported by the
European Unions Horizon 2020 research and innovation programme under grant agree-
ment No 671698 (ExaHyPE). The ideas around the particle handling, i.e. the break up of
collectives, evolved from a research visit of and collaboration with Bart Verleye who had
been funded by Intel and by the Institute for the Promotion of Innovation through Sci-
ence and Technology in Flanders (IWT). Single-touch algorithm rewrites in the context
of additive multigrid has been kicked oﬀ by a collaboration with Bram Reps who had
been supported by the FP7/2007-2013 programme under grant agreement No 610741
(EXA2CT). Volume compression concepts for stencil-based multigrid solvers arose from
a collaboration with Marion Weinzierl who received support from the International Grad-
uate School of Science and Engineering (IGSSE), Technische Universit¨at M¨unchen.

Special thanks are due to Marco Paganoni and Michele Fumagalli for establishing the
contact and eventually inviting me to the Big Data Analytics workshop in Varenna in

(2) The author is well-aware that PETSc, e.g., oﬀers a matrix-free usage mode. Yet, it seems
not to be the most popular way to use the software within complex PDE discretisations; at least
the author is not aware of extended documentation and examples how to integrate it into an
application landscape outside of the PETSc sanctum.

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING 13

June 2019. This manuscript picks up some aspects of my talk there. Special thanks are
also due to Marion Weinzierl who carefully proofread and commented on the present
manuscript.

REFERENCES

[1] L. Asanovic

al. The Landscape

et
from Berkeley.

A
Electrical
View
Engineering and Computer Sciences, University of California at Berkeley, 2006.
http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html.

of Parallel Computing Research:
UCB/EECS-2006-183,

Technical

Report

[2] K Boyandin et

al. Guest post:

Intel

optane

https://blog.selectel.com/guest-post-intel-optane-and-in-memory-databases
29 August 2018), 2018.

and in-memory databases.
(accessed

[3] A. Breuer, A. Heinecke, S. Rettenberger, M. Bader, A.-A. Gabriel, and C. Pelties. Sustained
petascale performance of seismic simulations with seissol on supermuc. In J.M. Kunkel,
T. T. Ludwig, and H.W. Meuer, editors, Supercomputing - 29th International Conference,
ISC 2014, volume 8488 of Lecture Notes in Computer Science, pages 1–18, Heidelberg,
2014. Springer. PRACE ISC Award 2014.

[4] H.-J. Bungartz, W. Eckhardt, T. Weinzierl, and C. Zenger. A precompiler to reduce the
memory footprint of multiscale pde solvers in c++. Future Generation Computer Systems,
26(1):175–182, January 2010.

[5] D. E. Charrier, B. Hazelwood, E. Tutlyaeva, M. Bader, M. Dumbser, A. Kudryavtsev,
A. Moskovsky, and T. Weinzierl. Studies on the energy and deep memory behaviour of
a cache-oblivious, task-based hyperbolic pde solver. The International Journal of High
Performance Computing Applications, 33(5), 2019.

[6] D. E. Charrier, B. Hazelwood, and T. Weinzierl. Enclave Tasking for Discontinuous

Galerkin Methods on Dynamically Adaptive Meshes, 2018. arXiv:1806.07984.

[7] D. E. Charrier and T. Weinzierl. Stop talking to me – a communication-avoiding ADER-

DG realisation, 2019. arXiv:1801.08682.

[8] J. Demmel and K. Yelick. Communication Avoiding (CA) and Other Innovative
Algorithms, chapter 9, pages 243–250. The Berkeley Par Lab: Progress in the Parallel
Computing Landscape. Microsoft Research, 2013.

[9] B. Dickov, M. Perics, P. M. Carpenter, N. Navarro, and E. Ayguad. Analyzing
performance improvements and energy savings in inﬁniband architecture using network
compression. In 2014 IEEE 26th International Symposium on Computer Architecture and
High Performance Computing, pages 73–80, Oct 2014.

[10] J. Dongarra, J. Hittinger, et al. Applied Mathematics Research for Exascale Computing,

DOE ASCR Exascale Mathematics Working Group, 2014.

[11] W. Eckhardt, R. Glas, D. Korzh, S. Wallner, and T. Weinzierl. On-the-ﬂy memory
compression for multibody algorithms. In Adv. in Parallel Comput., volume 27, pages
421–430, 2015.

[12] M. Fowler. Refactoring: Improving the Design of Existing Code. Addison-Wesley, 1999.
[13] E. Gamma, R. Helm, R. E. Johnson, and J. Vlissides. Design Patterns - Elements of

Reusable Object-Oriented Software. Addison-Wesley Longman, 1st edition, 1994.

[14] P. Ghysels, T. Ashby, K. Meerbergen, and W. Vanroose. Hiding global communication
latency in the gmres algorithm on massively parallel machines. SISC, 35(1):C48–C71, 2013.
[15] P. Ghysels and W. Vanroose. Hiding global synchronization latency in the preconditioned

conjugate gradient algorithm. Parallel Computing, 40(7):224–238, 2014.

[16] N. J. Higham. A multiprecision world. SIAM News 3/2017, 2017.
[17] N. J. Higham, S. Pranesh, and M. Zounon. Squeezing a matrix into half precision, with an

application to solving linear systems. SIAM J. Sci. Comput., 41(4):A2536–A2551, 2019.

[18] T. Hoeﬂer and A. Lumsdaine. Message progression in parallel computing - to thread or not
to thread? In 2008 IEEE International Conference on Cluster Computing, pages 213–222,
2008.

14

T. WEINZIERL

[19] T. Hoeﬂer and J. L. Traﬀ. Sparse collective operations for mpi. In 2009 IEEE International

Symposium on Parallel Distributed Processing, pages 1–8, 2009.

[20] D.

Keyes.

E.
Technical
https://www.pnnl.gov/scales/docs/volume1 300dpi.pdf.

case
Science, U.S. Department

science-based
of

report, Oﬃce

for

A

large-scale

of Energy,

simulation.
2003.

[21] M. Kowarschik and C. Weiß. An Overview of Cache Optimization Techniques and Cache-
Aware Numerical Algorithms. In U. Meyer, P. Sanders, and J. F. Sibeyn, editors,
Algorithms for Memory Hierarchies 2002, pages 213–232. Springer-Verlag, 2003.

[22] A. Kudryavtsev. Optane

and intel memory drive

technology,

big

https://itpeernetwork.intel.com/optane-intel-memory-drive-technology
December 2018), 2018.

surprise.
10

(accessed

[23] M. Kuhn, J. Kunkel, and T. Ludwig. Data compression for climate data. Supercomput.

Front. Innov.: Int. J., 3(1):75–94, January 2016.

[24] I. Laguna, P.C. Wood, R. Singh, and S. Bagchi. Gpumixer: Performance-driven ﬂoating-
point tuning for gpu scientiﬁc applications. In M. Weiland, G. Juckeland, C. Trinitis,
and P. Sadayappan, editors, High Performance Computing. ISC High Performance 2019,
volume 11501 of Lecture Notes in Computer Science, pages 227–246, 2019.

[25] M. O. Lam and B. L. Rountree. Floating-point shadow value analysis. In Proceedings of
the 5th Workshop on Extreme-Scale Programming Tools, ESPT ’16, pages 18–25. IEEE
Press, 2016.

[26] L. Lamport. Time, clocks, and the ordering of events in a distributed system.

Communications of the ACM, 21(7):558–565, 1978.

[27] J. Langou, J. Langou, P. Luszczek, J. Kurzak, A. Buttari, and J. J. Dongarra. Tools
and techniques for performance - exploiting the performance of 32 bit ﬂoating point
arithmetic in obtaining 64 bit accuracy (revisiting iterative reﬁnement for linear systems).
In Proceedings of the ACM/IEEE SC2006 Conference on High Performance Networking
and Computing, November 11-17, 2006, Tampa, FL, USA, page 113, 2006.

[28] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym. Nvidia tesla: A uniﬁed graphics

and computing architecture. IEEE Micro, 28(2):39–55, 2008.

[29] P. Lindstrom and M. Isenburg. Fast and eﬃcient compression of ﬂoating-point data. IEEE

Transactions on Visualization and Computer Graphics, 12(5):1245–1250, 2006.

[30] H. Ltaeif, G. Turkiyyah, and D. Keyes. Hierarchical algorithms on hierarchical

architectures. Royal Society Publishing, 2019. (accepted).

[31] W. Mitchell, R. Strzodka, R. Falgout, and S. McCormick. Parallel performance of algebraic

multigrid domain decomposition (amg-dd), 2019.

[32] G. E. Moore. Cramming more components onto integrated circuits. Electronics Magazine,

page 4, 1965.

[33] P. Ratanaworabhan, J. Ke, and M. Burtscher. Fast lossless compression of scientiﬁc
ﬂoating-point data. Data Compression Conference (DCC’06), pages 133–142, 2006.
[34] J. Rudi, A. C. I. Malossi, T. Isaac, G. Stadler, M. Gurnis, P. W. J. Staar, Y. Ineichen,
C. Bekas, A. Curioni, and O. Ghattas. An extreme-scale implicit solver for complex pdes:
Highly heterogeneous ﬂow in earth’s mantle. In Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis, SC ’15, pages 5:1–
5:12, 2015.
[35] J. Russell.

Isc workshop tackles the co-development challenge. HPC Wire, 2016.

https://www.hpcwire.com/2016/07/12/isc-workshop-tackles-co-developments-thorny-
challenges.

[36] G. Saxena, P. K. Jimack, and M. A. Walkley. A quasicacheaware model for optimal domain
partitioning in parallel geometric multigrid. Concurrency and Computation—Practice and
Experience, 30(9), 2017.

[37] M. Schaller, P. Gonnet, A. B. G. Chalk, and P. W. Draper. Swift: Using task-
based parallelism, fully asynchronous communication, and graph partition-based domain
decomposition for strong scaling on more than 100,000 cores. In Proceedings of the Platform
for Advanced Scientiﬁc Computing Conference, PASC ’16, pages 2:1–2:10, 2016.

HIGH-LEVEL CHARACTERISATION AND GENERALISATION OF COMMUNICATION-AVOIDING 15

[38] M. Schreiber, T. Weinzierl, and H.-J. Bungartz. Sfc-based communication metadata
encoding for adaptive mesh. In Michael Bader, editor, Proceedings of the International
Conference on Parallel Computing (ParCo), October 2013. accepted.

[39] M. Sergent, M. Dagrada, P. Carribault, J. Jaeger, M. P´erache, and G. Papaur´e.
Eﬃcient communication/computation overlap with mpi+openmp runtimes collaboration.
In M. Aldinucci, L. Padovani, and M. Torquati, editors, Euro-Par 2018: Parallel
Processing, Lecture Notes in Computer Science, pages 560–572, 2018.

[40] H. Sutter. The free lunch is over: A fundamental turn toward concurrency in software. Dr.

Dobbs Journal, 30(3):202–210, 2005.

[41] U. Trottenberg, C. W. Oosterlee, and A. Sch¨uller. Multigrid, volume 33 of Texts in Applied

Mathematics. Bd. Academic Press, 2001.

[42] PROMETEUS

Professor Meuer Technologieberatung

und

Services GmbH.

www.top500.org.

[43] M. Weinzierl and T. Weinzierl. Algebraic-geometric matrix-free multigrid on dynamically

adaptive Cartesian meshes. ACM Trans. Math. Softw., 44(3):32:1–32:44, 2018.

[44] T. Weinzierl and M. Mehl. Peano – A Traversal and Storage Scheme for Octree-Like
Adaptive Cartesian Multiscale Grids. SIAM Journal on Scientiﬁc Computing, 33(5):2732–
2760, October 2011.

[45] T. Weinzierl, B. Verleye, P. Henri, and D. Roose. Two particle-in-grid realisations on

spacetrees. Parallel Comput., 52:42–64, 2016.

[46] M. Wittmann, G. Hager, T. Zeiser, and G. Wellein. Asynchronous MPI for the Masses.

CoRR, abs/1302.4280, 2013.

[47] J. Wolfson-Pou and E. Chow. Modeling the asynchronous jacobi method without

communication delays. Journal of Parallel and Distributed Computing, 128:84–98, 2019.

