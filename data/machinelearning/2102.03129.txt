1
2
0
2

t
c
O
8

]

G
L
.
s
c
[

3
v
9
2
1
3
0
.
2
0
1
2
:
v
i
X
r
a

Integer Programming for Causal Structure Learning
in the Presence of Latent Variables

Rui Chen
University of Wisconsin-Madison
(rchen234@wisc.edu)

Sanjeeb Dash
IBM Research
(sanjeebd@us.ibm.com)

Tian Gao
IBM Research
(tgao@us.ibm.com)

October 11, 2021

Abstract

The problem of ﬁnding an ancestral acyclic directed mixed graph (ADMG) that represents the
causal relationships between a set of variables is an important area of research on causal inference.
Most existing score-based structure learning methods focus on learning directed acyclic graph
(DAG) models without latent variables. A number of score-based methods have recently been
proposed for the ADMG learning, yet they are heuristic in nature and do not guarantee an optimal
solution. We propose a novel exact score-based method that solves an integer programming (IP)
formulation and returns a score-maximizing ancestral ADMG for a set of continuous variables that
follow a multivariate Gaussian distribution. We generalize the state-of-the-art IP model for DAG
learning problems and derive new classes of valid inequalities to formulate an IP model for ADMG
learning. Empirically, our model can be solved eﬃciently for medium-sized problems and achieves
better accuracy than state-of-the-art score-based methods as well as benchmark constraint-based
methods.

1

Introduction

Causal graphs are graphical models representing dependencies and causal relationships between a set
of variables. One class of the most common causal graphs, often known as Bayesian networks (BNs), is
modeled by directed acyclic graphs (DAGs) in which a direct causal relationship between two nodes is
indicated by a directed edge. However, its structure learning problem is NP-hard [1]. Many exact and
approximate algorithms for learning BN structures from data have been developed, including score-
based and constraint-based approaches. Score-based methods use a score – such as Bayesian scores or
Bayesian information criterion (BIC) – to measure the goodness of ﬁt of diﬀerent graphs over the data,
and then use a search procedure – such as hill-climbing [2, 3, 4], forward-backward search [5], dynamic
programming [6, 7, 8], A* [9] or integer programming [10, 11, 12] – in order to ﬁnd the best graph. On
the other hand, constraint-based structure learning algorithms use (conditional) independence tests to
decide on the existence of edges between all pairs of variables. Popular algorithms include the SGS
algorithm [13], PC algorithm [13], and IC algorithm [14].

Despite their wide application [14], it is known that DAG models are not closed under marginaliza-
tion [15]. This implies that DAGs cannot be used to model structures involving latent variables which
are the most common cases in practice. For example, in healthcare domains, there may be numerous
unobserved factors such as gene expression. Self-reported family history and diets may also leave out
some important information. Ancestral graphs (AGs) were proposed as a generalization of DAGs [16].
AG models include all DAG models and are closed under marginalization and conditioning. AGs cap-
ture the independence relations among observed variables without explicitly including latent variables
in the model. In this work, we assume no selection biases (no undirected edges in AGs). Hence there are

1

 
 
 
 
 
 
two types of edges in the AGs: directed edges represent direct or ancestral causal relationships between
variables and bidirected edges represent latent confounded relationships between variables. A bidirected
edge between two nodes means that there exists at least one latent confounder that causes both nodes,
and neither node causes the other node. AGs considered in this work are also called ancestral acyclic
directed mixed graphs (ADMGs) [17] in the literature.

Causal structure learning algorithms for ancestral ADMGs can also be divided into two main classes:
constraint-based and score-based methods. Constraint-based methods apply conditional independence
tests on the data to infer graph structures. Score-based methods search through possible graph struc-
tures to optimize a criterion for model selection. There are also hybrid methods that use both condi-
tional independent tests and some scoring criterion that measures the likelihood of the data. In the
setting of learning DAGs from observational data, score-based methods often achieve better performance
than constraint-based methods as score-based methods are less sensitive to error propagation [18]. For
ADMG learning, existing methods are mostly constraint-based, including the FCI algorithm [13, 19]
and the conservative FCI (cFCI) [20] to name a couple. Several score-based or hybrid approaches have
been proposed for ancestral ADMG structure learning over continuous Gaussian variables in recent
years [21, 15, 22, 23] but are all greedy or local search algorithms. In this paper, we close the gap
and propose an exact score-based solution method based on an integer programming (IP) formulation
for ancestral ADMG learning. Our method is inspired by existing DAG learning IP formulations, and
we derive new classes of valid inequalities to restrict the learned graph to be a valid ADMG. Empir-
ical evaluation shows that the proposed method outperforms existing score-based DAG and ADMG
structure learning algorithms.

The paper is organized as follows. In Section 2, we deﬁne the ancestral ADMG learning problem. In
Section 3, we propose an integer programming formulation for obtaining the score-maximizing ancestral
ADMG. In Section 4, we present experiments to compare our method with existing baselines.

2 Ancestral ADMG Learning

2.1 Preliminaries

We brieﬂy review some related concepts. DAGs are directed graphs without directed cycles. A directed
mixed graph G = (V, Ed, Eb) consists of a set of nodes V , a set of directed edges (→) Ed ⊆ {(i, j) :
i, j ∈ V, i (cid:54)= j}, and bidirected edges (↔) Eb ⊆ {{i, j} : i, j ∈ V, i (cid:54)= j} between certain pairs of nodes.
Given a directed edge a → b, b is the head node, and a the tail node. We call node a an ancestor of
node b in G if there is a directed path from a to b in G or a = b. We call node a a spouse (resp., parent)
of node b in G if there is a bidirected edge between a and b (resp., a directed edge from a to b) in G.
We denote the set of ancestors, the set of parents, and the set of spouses of node a in G by anG(a),
paG(a), and spG(a), respectively. For i ∈ V and W ⊆ V , W → i denotes that W is the parent set of i.
A directed mixed graph G is called an ancestral ADMG if the following condition holds for all pairs of
nodes a and b in G:

• If a (cid:54)= b and b ∈anG(a)∪spG(a), then a /∈anG(b).

In other words, G is an ancestral ADMG if it contains no directed cycles (a → c → . . . → b → a) or
almost directed cycles. An almost directed cycle is of the form a → c → . . . → b ↔ a; in other words,
{a, b} ∈ Eb is a bidirected edge, and a ∈ anG(b). Given a directed mixed graph G, the districts deﬁne a
set of equivalence classes of nodes in G. The district for node a is deﬁned as the connected component
of a in the subgraph of G induced by all bidirected edges, i.e.,

{b : b ↔ . . . ↔ a in G or a = b}.

Given a district D of G, the directed mixed graph GD is a subgraph of G deﬁned as follows. The node set
of GD consists of all nodes in D and their parents. The bidirected edges in GD are the bidirected edges

2

in G that connect nodes in D. The directed edges in GD are the directed edges in G where the head
nodes are contained in D. We say that GD is the subgraph implied by D. In this paper, we call subgraph
C of G a c-component if C is a subgraph implied by some district of G. Note that if a c-component has
no bidirected edges, then any district must consist of a single node, and the c-component consists of a
number of directed edges all with the same head node. Some authors use c-component as a synonym
for district.

2.2 Calculating Graph Scores

We assume the ground truth data is modeled by the following linear equations:

X = M X + (cid:15).

Here M is a d × d matrix, X = (X1, . . . , Xd)T are variables of the model, and (cid:15) = ((cid:15)1, . . . , (cid:15)d)T is
the error vector which follows a multivariate Gaussian distribution N (0, Σ)1. Neither M nor Σ can be
observed. Our goal is to ﬁnd the best-ﬁtting ancestral ADMG G and an associated parameterization
(M, Σ) satisfying Mij = 0 if i = j or j → i is not in G and Σij = 0 if i ↔ j is not in G. A score of
the graph measures how well the graph structure represents the data. We use BIC [26] scores for all
graphs in this paper. The BIC score for graph G is given by

BICG = 2 ln(LG( ˆΣ)) − ln(N )(2|V | + |E|).

Here ˆΣ is the maximum likelihood estimate of Σ given the graph representation G and ln(LG( ˆΣ)) is
the associated log-likelihood while N denotes the number of samples, |V | and |E| denote the number
of nodes and the number of edges in G, respectively. Given a ﬁxed graph and the empirical covariance
matrix Q, the maximum likelihood estimate of the parameters can be found by applying the residual
iterative conditional ﬁtting algorithm in [27]. According to [28], ln(LG( ˆΣ)) can be decomposed by
c-components in G. Speciﬁcally, let D denote all districts of G. Then

ln(LG( ˆΣ)) =

−

N
2

(cid:88)

(cid:104)

D∈D

|D| ln(2π) + log(

(cid:81)

)+

| ˆΣGD |
j∈paG (D) ˆσ2
N − 1
N

Dj

tr( ˆΣ−1
GD

(cid:105)
QD − |paG(D) \ D|)

.

(1)

Here paG(D) denotes the union of parent sets of nodes in D, ˆΣGD is the maximum log-likelihood for
Dj denotes diagonal entry of ˆΣGD corresponding to node j. Note that the districts
subgraph GD and ˆσ2
partition the nodes of G, and the edges of G are partitioned by the subgraphs GD. The BIC score for
graph G can be expressed as a sum of local scores of its c-components. For example, in Figure 1, the
BIC score of the ancestral ADMG is equal to the sum of local scores of four c-components represented
by diﬀerent colors. Nodes with the same color belong to the same district and directed edges indicate
their parents. When the graph is a DAG, other decomposable scoring functions can also be used [29].
Given the above decomposition property of BIC scores, one can immediately formulate the ancestral
ADMG learning problem as an optimization problem, where one takes as input the set C of all possible
candidate c-components deﬁned on the input random variables, and considers each subset of C where the
union of edges in the c-components in the subset form an ancestral ADMG. This is however impractical
as the size of C grows exponentially with increasing number of nodes/random variables. In a similar
fashion, the DAG learning problem can be solved by considering candidate c-components where each c-
components is implied by a single node district. Even with this restriction, the number of c-components

1We focus on the multivariate Gaussian case in this work as the scoring function can be factorized, although our work

may also apply to discrete cases per diﬀerent factorization rules [24, 25].

3

Figure 1: Decomposition of the BIC score for an ancestral ADMG.

that need to be considered in DAG learning is exponential, and a common approach to deal with this
issue is to only consider c-components with a bounded number of nodes. The model we propose for
ADMG learning is deﬁned over general c-components, although in our computational experiments we
restrict the set of candidate c-components to allow our model to be solved within a reasonable amount
of time. In Section 2.3, we further consider eliminating sub-optimal c-components, which signiﬁcantly
reduces the size of the search space in some cases.

2.3 Pruning the list of candidate c-components

The set of ancestral ADMGs deﬁned over a set of nodes is closed under deletion of directed and bidi-
rected edges. Similarly, the operation of removing all parents of a node and replacing all bidirected
edges incident to the node with outgoing directed edges transforms an ancestral ADMG into another
ancestral ADMG. Both of these operations also result in the transformation of the c-components associ-
ated with an ADMG. If the c-components resulting from applying the above transformation to another
c-component have a higher combined score than the original c-component, then we can assert that the
original c-component need never be considered in an optimal solution of the ADMG learning problem.
We use the above ideas and their hybrids to deﬁne two ways of pruning C in our implementation. Firstly,
if removing one or more edges from a c-component results in a set of c-components with a higher com-
bined score, we prune the original c-component. For example, the c-component {A} → B ↔ C ← {D}
can be pruned if it has a lower score than the sum of the scores of B ← {A} and C ← {D}. Similarly,
we prune a c-component if the second operation above leads to c-components with a higher combined
score. For example, we can prune {A} → B ↔ C ← {D} if it has a score lower than the sum of the
scores of B ← ∅ and C ← {B, D}. A hybrid of the previous two pruning ideas is also implemented. For
example, we can combine both operations to transform {A} → B ↔ C ← {D} into the c-components
B ← ∅ and C ← {B}.

3 An Integer Programming Formulation

Integer programming (IP) is a mathematical optimization tool for modeling and solving optimization
problems involving variables that are restricted to discrete values. For any c-component C implied
by a district, let DC denote the implying district, let EC denote the pairs of nodes in DC that are
connected by a bidirected edge, and for each node i ∈ DC, let WC,i denote the parent set of node i
in c-component C. Given a set of c-components C, ﬁnding a score-maximizing ancestral ADMG with
all its district-implied subgraphs in C can be straightforwardly formulated as an integer program as
follows:

max
z∈{0,1}C

(cid:80)

C∈C sCzC

s.t. (cid:80)

C:i∈DC

zC = 1, i ∈ {1, . . . , d}
G(z) is acyclic and ancestral.

(2)

(3)

(4)

4

A

B

D

C

Figure 2: Depiction of the cluster inequality for S = {A, B, C, D}.

Here zC is a binary variable indicating if the c-component C is chosen, sC is the local score of c-
component C and G(z) is the directed mixed graph whose district-implied subgraphs are exactly the
c-components C with zC = 1. Constraints (3) enforce the condition each node i is contained in a single
district (that implies a chosen c-component). Constraint (4) implies that the resulting directed mixed
graph G(z) contains no directed or almost directed cycles. Standard IP solvers can only deal with linear
inequality constraints, and we next discuss how to represent constraint (4) by a set of linear inequality
constraints.

3.1 Avoiding Directed Cycles

Several (mixed-)integer programming (MIP) formulations exist for constraining a graph to be acyclic.
A description and a comparison of some of these diﬀerent anti-cycle formulations for the DAG learning
problem with continuous data, including the linear ordering formulation [30], the topological ordering
formulation [31], and the cycle elimination formulation, are given in [32]. A new layered network
formulation was also proposed in [32]. One particular class of anti-cycle constraints called cluster
inequalities/constraints was introduced in [10] and often performs better than the other methods above
and results in a very tight integer programming formulation for DAG learning problems. Let I(·) be
a variable indicating whether or not the substructure described between the parentheses is present in
the graph. Speciﬁcally, I(W → i) indicates whether or not paG(z)(i) = W . Cluster constraints have
the following form:

(cid:88)

(cid:88)

I(W → i) ≥ 1, ∀S ⊆ {1, 2, . . . , d}.

(5)

i∈S

W :W ∩S=∅

Inequalities (5) encode the constraint that for a set S of nodes in an acyclic graph, there must exist at
least one node in S whose parent set has no intersection with S. In Figure 2, there is a directed cycle
connecting nodes {A, B, C, D}, and it violates the cluster inequality for S = {A, B, C, D}, which would
be satisﬁed if node D has all its parents outside S. The generation of cluster constraints is essential
for the state-of-the-art IP-based DAG learning solver GOBNILP [33]. The same inequalities can be
applied to ancestral ADMG learning by introducing auxiliary variables I(W → i) deﬁned as

I(W → i) :=

(cid:88)

zC.

C∈C:i∈DC ,WC,i=W

(6)

In our case, inequalities (5) can be further strengthened as some of the original variables zC may be
double counted when they are present in multiple auxiliary variables of the form I(W → i). We propose
the following strengthened cluster constraints for ancestral ADMG learning:

(cid:88)

(cid:88)

i∈S

C∈C:i∈DC ,WC,i∩S=∅

zC ≥ 1, ∀S ⊆ {1, 2, . . . , d}.

(7)

5

A

B

C

Figure 3: Depiction of the bicluster inequality for S = {A, B, C} and (i, j) = (B, C).

Proposition 1. Inequalities (7) are satisﬁed by all solutions of the integer program (2)-(4). If z ∈
{0, 1}d satisﬁes (3) and (7), then z corresponds to an ADMG, i.e., G(z) contains no directed cycles.

Proof. We ﬁrst show the validity of (7). Let ¯z be a feasible solution of the integer program (2)-(4).
Assume that inequality (7) with S set to ¯S is violated by ¯z, i.e.,

(cid:88)

(cid:88)

¯zC = 0.

i∈ ¯S

C∈C:i∈DC ,WC,i∩ ¯S=∅

(8)

By constraint (3), for each i ∈ {1, . . . , d}, there exists Ci satisfying i ∈ DCi such that ¯zCi = 1. By (8),
we have WCi,i ∩ ¯S (cid:54)= ∅ for each i ∈ ¯S. It follows that each node i in ¯S has a parent in ¯S in G(¯z), which
contradicts the feasibility of ¯z since G(¯z) contains a directed cycle with nodes in ¯S.

Next we show that G(z) is an ADMG if z satisﬁes (3) and (7). Constraints (3) imply that G(z) is a
valid directed mixed graph as exactly one c-component having i in its implying district is active. We
only need to show that G(z) contains no directed cycles. Assume for contradiction that G(z) contains
a directed cycle i0 → i1 → . . . → iK → i0. Let S(cid:48) = {i0, i1, . . . , iK}. By constraint (3), for each
i ∈ {1, . . . , d}, there exists exactly one Ci satisfying i ∈ DCi such that zCi = 1. The directed cycle
i0 → i1 → . . . → iK → i0 implies that WCi,i ∩ S(cid:48) (cid:54)= ∅ for each i ∈ S(cid:48). Therefore, by constraint (3),
(cid:80)

C∈C:i∈DC ,WC,i∩S(cid:48)=∅ zC = 0 for each i ∈ S(cid:48), which contradicts inequality (7) with S = S(cid:48).

3.2 Avoiding Almost Directed Cycles

Ancestral ADMGs are ADMGs without almost directed cycles. To encode the constraint that the
graph contains no almost directed cycles, we deﬁne auxiliary variables I(W 1 → i, W 2 → j, i ↔ j) and
I(i ↔ j) in a manner similar to (6), and give the following inequalities for all S ⊆ {1, 2, . . . , d} with
i, j ∈ S and i < j:

(cid:88)

(cid:88)

I(W 1 → i, W 2 → j, i ↔ j)+

W 1:W 1∩S=∅

W 2:W 2∩S=∅

(cid:88)

(cid:88)

v∈S\{i,j}

W :W ∩S=∅

I(W → v) ≥ I(i ↔ j).

(9)

We refer to inequalities (9) as the bicluster inequalities/constraints. Bicluster inequalities encode the
constraint that if a bidirected edge i ↔ j is present in some ancestral ADMG, then, for any set of nodes
S containing i and j, either some node v ∈ S \ {i, j} has all its parents outside of S or both parent
sets of i and j have no intersection with S. As shown in Figure 3, where B and C are connected by a
bidirected edge, either the parents of A must lie outside S = {A, B, C}, or the parents of both B and

6

C must lie outside S. Similar to cluster inequalities, bicluster inequalities can also be strengthened
when written in the original z variable space as some c-component variables are double-counted on
the left-hand side of (9). Also some c-component variables might be contradicting the presence of the
bidirected edge i ↔ j, and therefore cannot be active when I(i ↔ j) is active and should be removed
from the left-hand side.

Proposition 2. The following inequalities are valid for the integer program (2)-(4):

(cid:88)

C∈C(S;{i,j})

zC ≥ I(i ↔ j), ∀S ⊆ {1, 2, . . . , d}

where

C(S; {i, j}) =

with i ∈ S, j ∈ S, i < j

(10)

{C ∈ C : {i, j} ∈ EC, (WC,i ∪ WC,j) ∩ S = ∅}∪
{C ∈ C : |{i, j} ∩ DC| (cid:54)= 1, ∃v ∈ DC ∩ S \ {i, j}

s.t. WC,v ∩ S = ∅}.

Any z ∈ {0, 1}d satisfying (3), (7) and (10) corresponds to an ancestral ADMG.

Proof. We ﬁrst show validity of (10). Let ¯z be a feasible solution of (2)-(4). Assume for contradiction
that inequality (10) with S = ¯S and (i, j) = (¯i, ¯j) is violated by ¯z. Note that the right-hand side value
of (10) can only be binary and (10) cannot be violated if the right hand side is 0 since z is binary.
Then there exists a bidirected edge between ¯i and ¯j in G(¯z). Since z = ¯z violates (10) with S = ¯S and
(i, j) = (¯i, ¯j), ¯zC = 0 for each C ∈ C( ¯S; {¯i, ¯j}). Next we construct an almost directed cycle in G(¯z).
Because variable I(¯i ↔ ¯j) is active, there exists C¯i¯j ∈ C such that {¯i, ¯j} ∈ EC¯i¯j and ¯zC¯i¯j = 1. Since
¯zC = 0 for each C ∈ C( ¯S; {¯i, ¯j}), we have (WC¯i¯j ,¯i ∪ WC¯i¯j ,¯j) ∩ ¯S (cid:54)= ∅. Without loss of generality, assume
WC¯i¯j ,¯i ∩ ¯S (cid:54)= ∅. Let i0 = ¯i. We recursively deﬁne ik as follows:

• Let ik be a parent of ik−1 such that ik ∈ ¯S.

Then this sequence either ﬁnds a directed cycle because of the ﬁniteness of the number of nodes,
which contradicts the feasibility of ¯z, or ends up with some node iK that has no parent in ¯S.
If
it is the second case, we next show that iK can only be j which implies an almost directed cycle
iK → iK−1 → . . . → i0 ↔ iK in G(¯z). First of all, i0 = ¯i has a parent in ¯S since WC¯i¯j ,¯i ∩ ¯S (cid:54)= ∅.
Let ¯C ∈ C be such that iK ∈ D ¯C and z ¯C = 1. Assume for contradiction that iK (cid:54)= j. Then
iK ∈ D ¯C ∩ S \ {i, j} and W ¯C,iK ∩ ¯S = ∅. Also note that either {i, j} ∩ D ¯C = ∅ or {i, j} ⊆ D ¯C since
otherwise it contradicts the activeness of I(i ↔ j). This implies that ¯C ∈ C( ¯S; {¯i, ¯j}) which contradicts
the fact that ¯zC = 0 for each C ∈ C( ¯S; {¯i, ¯j}).

We next show that G(z) is an ancestral ADMG if z satisﬁes (3), (7) and (10). By Proposition
1, G(z) is an ADMG. We only need to show that G(z) contains no almost directed cycles. Assume
for contradiction that G(z) contains an almost directed cycle i0 → i1 → . . . → iK ↔ i0. Let S(cid:48) =
{i0, i1, . . . , iK}. We can then easily verify that zC = 0 for any C ∈ C(S(cid:48); {i0, iK}) which contradicts
(10) with S = S(cid:48), i = min{i0, iK} and j = max{i0, iK}.

We provide a simple example of a strengthened bicluster inequality when the problem contains only

3 variables.

Example 1. Let d = 3 and C be the collection of all possible c-components on three nodes. Consider
S = {1, 2, 3}, i = 2 and j = 3. Then the asociated strengthened bicluster inequality (after simpliﬁcation)
is

z∅→1 ≥ z{1}→2↔3←{1} + z∅→2↔3←{1} + z{1}→2↔3←∅.

(11)

7

Note that at most one of the c-components on the right-hand side of (11) ({1} → 2 ↔ 3 ← {1},
∅ → 2 ↔ 3 ← {1} and {1} → 2 ↔ 3 ← ∅) can be active due to (3). Inequality (11) enforces the
constraint that c-component ∅ → 1 must be active when one of the three c-components on the right-
hand side of (11) is active.

So far we have given an integer programming formulation of the ancestral ADMG learning problem.
However, solving this integer program eﬃciently is nontrivial. The ﬁrst step to solve an integer program
is often solving some polyhedral relaxation of the problem. Directly solving the relaxed problem over the
polyhedron deﬁned by (3), (7) and (10) is computationally infeasible because of the exponential number
of constraints. A common practice for solving such problems with exponentially many constraints is to
further relax the polyhedral relaxation to be the polyhedron deﬁned by only some of these constraints,
ﬁnd an optimal solution, solve a corresponding separation problem, and add violated constraints to
the relaxation if they exist. The separation problem ﬁnds constraints that are violated by the current
solution and this process is repeated until convergence. If the optimal solution of the relaxed problem
is integral, such a separation problem translates to just ﬁnding a directed or almost directed cycle in a
directed mixed graph which can easily be accomplished by depth-ﬁrst search. However, the separation
problem can be much harder to solve when the solution is fractional. We provide separation heuristics
based on variants of Karger’s random contraction algorithm [34] in the supplement.

4 Empirical Evaluation

We conduct a set of experiments to compare our IP model, AGIP, with existing state-of-the-art base-
lines. We use DAGIP to represent existing DAG IP models such as the one in GOBNILP [33], which
is the same IP model as AGIP when all candidate c-components are implied by single-node districts.
The solution obtained from DAGIP is equivalent to any exact score-based method for generating DAG
solutions. We also compare with non-IP-based approaches, namely M3HC [15], FCI [13, 19], and cFCI
[20].

To measure solution quality, we use a few diﬀerent metrics. When comparing against score-based
methods, our optimization model objective is to maximize a score, such as the BIC score [26] for model
selection, and the metric is the solution score. To compare with constraint-based methods which do
not have objective scores, the solution graph is converted to a partial ancestral graph (PAG), which
characterizes a class of Markov equivalent AG solutions, and then compared with the ground truth PAG.
We use structural Hamming distance (SHD) [35], which is the number of edge operations (addition or
deletion of an undirected edge, addition, removal or reversion of the orientation of an edge) between the
predicted graph and the ground truth graph. Finally, precision and recall [36] are also compared. They
are the number of correct edges with correct orientations in the predicted graph divided by the number
of edges in the predicted graph and by the number of edges in the ground truth graph, respectively.

All experiments are run on a Windows laptop with 16GB RAM and an Intel Core i7-7660U processor

running at 2.5GHz. Integer programs are solved using the optimization solver Gurobi 9.0.3.

4.1

Experiment 1: Exact Graph Recovery

3

1

0

4

2

3

1

4

2

3

1

4

2

Figure 4: Ground truth DAG (left), AG (middle), and PAG (right) of the four-node graph.

8

Table 1: Tightness of the AGIP formulation.

(d, l, N )

(18, 2, 1000)
(16, 4, 1000)
(14, 6, 1000)
(18, 2, 10000)
(16, 4, 10000)
(14, 6, 10000)

Avg # bin vars Avg # bin vars Avg pruning Avg root Avg solution
before pruning

after pruning

gap (%)

time (s)

time (s)

59229
39816
20671
59229
39816
20671

4116
3590
1788
9038
7378
3786

19.1
13.6
3.9
33.0
21.4
6.4

0.65
0.43
0.54
0.67
0.53
0.56

60.4
41.0
8.9
323.2
215.4
47.2

We ﬁrst test on a four-node example, where the data (10000 data points) is simulated from a ﬁve-
node DAG model (see Figure 4) with node 0 being unobserved. The purpose of this experiment is
to show that for small graphs, where we can practically enumerate and use all possible c-component
variables, and with enough samples, exact graph recovery is possible with AGIP. We test score-based
methods AGIP, DAGIP and M3HC on this example. In AGIP, we consider all possible c-components
with arbitrary sizes. In DAGIP, we consider all possible c-components implied by single-node districts.

3

1

4

2

3

1

4

2

3

1

4

2

Figure 5: Solutions obtained from AGIP (left), DAGIP (middle) and M3HC (right).

These three methods generate three diﬀerent solutions. Comparing the scores of the solutions, we
observe that score(AGIP) > score(DAGIP) > score(M3HC) in this example. Both AGIP and M3HC
correctly capture the skeleton (presence of edges between each pair of nodes) of the ground truth AG.
Only the AGIP solution is Markov equivalent to the ground truth AG, i.e., the AGIP solution encodes
the same set of conditional independence relationships as the ground truth AG. This result shows that
our method is consistent for large samples and ﬁnds the exact solution.

4.2 Experiment 2: Random Graphs

We further experiment on a set of randomly generated ADMGs following the procedure described in
[21]. For each instance, a random permutation is applied to d + l variables in a DAG as the ordering of
the variables. For each variable i, a set of up to 3 variables that have higher ordering than i is randomly
chosen as the parent set of i. The resulting DAG is then assigned a randomly generated conditional
linear Gaussian parameterization. Within the d+l variables in the DAG, l of them are randomly chosen
as latent variables and marginalized which result in an AG over the observed d variables. For each such
graphical model, a sample of N ∈ {1000, 10000} realizations of the observed variables is simulated to
create the instance. For ﬁxed (d, l, N ), 10 instances with parameters d, l, and N are generated.

9

Table 2: Comparing scores of AGIP, DAGIP, and M3HC.

(d, l, N )

Avg improvement in score
compared with M3HC

# instances where AGIP
improves over DAGIP in score

(18, 2, 1000)
(16, 4, 1000)
(14, 6, 1000)
(18, 2, 10000)
(16, 4, 10000)
(14, 6, 10000)

AGIP

82.75
90.03
34.84
373.44
147.96
150.52

DAGIP

82.32
89.33
34.68
373.44
147.54
150.44

3/10
5/10
3/10
0/10
1/10
1/10

4.2.1 Comparison between AGIP, DAGIP and M3HC solutions

To guarantee eﬃciency, we restrict the sizes of c-components considered in the AGIP and DAGIP
methods. AGIP considers c-components implied by a single-node district with up to 3 parents or a
two-node district with up to 1 parent per node, while DAGIP considers c-components implied by a
single-node district with up to 3 parents.

k

We want to emphasize that ADMG learning can be much harder than DAG learning. In the state-
of-the-art DAG learning IP model [33], assuming n variables and parent sets with maximum size k,
there are at least n(cid:0)n−1
(cid:1) = Θ(nk+1) IP variables in total for ﬁxed k but increasing n (before pruning).
For the ADMG learning problem, assuming a maximum district size of p nodes, the number of IP
variables in our IP model (AGIP) is at least (cid:0)n
(cid:1)(cid:0)n−p
= Θ(np(k+1)) for ﬁxed k, p but increasing n
k
(before pruning). When p = 2 (the minimum required to model bidirected edges), AGIP has the square
of the number of IP variables in DAGIP. With our setting of the experiments, AGIP has roughly double
the IP variables of DAGIP. One possible way to deal with this explosive growth is to add a collection
of potentially good c-components with large-districts to AGIP rather than all possible ones.

(cid:1)p

p

In these experiments, we ﬁx d + l = 20 with varying l ∈ {2, 4, 6}. We ﬁrst show the tightness of our
IP formulation. For each combination of (d, l, N ), we report in Table 1 the average number of binary
variables before and after pruning, average pruning time, average integrality gap at the root node, and
average IP solution time for the AGIP model over the 10 instances. We observe that the solution time
decreases as l increases. On the other hand, the solution time increases when the number of samples
N increases, since fewer c-component variables can be pruned during the pruning phase. For all cases,
the IP formulation has a pretty small gap at the root node which is always below 1% on average.

We next compare the qualities of solutions obtained from AGIP, DAGIP, and M3HC in terms of the
scores. Since the feasible region of the AGIP model is strictly larger than DAGIP, the optimal score
obtained from AGIP is guaranteed to be at least as good as the optimal score obtained from DAGIP. In
Table 2, we report the average diﬀerence (improvement) in score compared with M3HC and the number
of instances where AGIP improves the score over DAGIP for each combination of (d, l, N ). Both AGIP
and DAGIP produce solutions better than the greedy algorithm M3HC, although M3HC in principle
searches over a larger space of graph structures. In fact, M3HC ﬁnds slightly better solutions for only
4 of the 60 instances we tested and performed strictly worse than AGIP on the other 53 instances.
Therefore, we can conclude that the exact methods DAGIP and AGIP are better at obtaining solutions
with higher scores than the greedy method M3HC on randomly generated instances. We also observe
that AGIP improves over DAGIP on 13 of 60 instances. There are two particular explanations for
this. Firstly, the implied ground truth AG might be Markov equivalent to a DAG in which case the
DAG solution can be optimal. Secondly, the “non-DAG” candidate c-components are limited as AGIP
only considers additionally c-components implied by a two-node district with up to 1 parent each node

10

Table 3: Comparison between AGIP, M3HC, FCI, and cFCI for ancestral ADMG Learning.

(d, l, N )

SHD

Precision (%)

Recall (%)

AGIP M3HC FCI

cFCI AGIP M3HC FCI

cFCI AGIP M3HC FCI

cFCI

(18, 2, 1000)
(16, 4, 1000)
(14, 6, 1000)
(18, 2, 10000)
(16, 4, 10000)
(14, 6, 10000)
overall

36.0
23.4
30.1
12.6
24.6
26.5
25.5

32.0
32.1
34.7
26.4
23.9
23.5
28.8

32.9
34.9
34.1
30.8
26.7
27.7
31.2

25.9
29.7
30.6
24.8
23.1
23.3
26.2

41.7
58.0
43.2
80.3
60.6
47.7
55.3

37.6
35.9
36.0
54.1
56.3
54.2
45.7

30.9
27.6
26.3
29.6
43.7
38.6
32.8

49.5
41.0
42.0
50.0
51.1
53.1
47.8

41.9
56.6
37.0
78.7
58.4
44.5
52.8

30.8
29.3
24.8
49.3
51.6
46.5
38.7

24.7
22.8
18.7
26.2
38.5
33.2
27.4

38.5
32.5
30.0
44.1
45.6
44.4
39.2

Figure 6: Performance of diﬀerent methods when the number of latent variables increases.

compared with DAGIP.

4.2.2 Comparison between Score-based Methods and Constraint-based Methods

We compare score-based methods AGIP and M3HC with constraint-based methods FCI and cFCI in
Table 5. All methods have better precision and recall values when the sample size N increases. FCI
seems to perform worse than the other 3 methods, and on average, AGIP has the best SHD, precision,
and recall.

We observe that AGIP has a better recall in 5 out of 6 settings. AGIP has signiﬁcantly better

performance than the other methods when (d, l, N ) = (18, 2, 10000).

To see the impact of latent variables on the performance of these methods, we also regenerate
instances with ﬁxed d = 18 and N = 10000 but varying l ∈ {2, 4, 6}. For each l, we plot on Figure 6
the mean (± standard deviation) of SHD, precision, and recall of estimated graphs from each method
over 10 instances (see the supplement for detailed results). The performance of each method tends to
drop as the number of latent variables increases. The drop is most signiﬁcant for AGIP, presumably
due to the restriction on the c-component size. The ground truth graphs associated with our randomly
generated instances can have c-components with large parental sizes and district sizes, especially in

11

Table 4: Comparison between AGIP and DAGIP on graphs that are not DAG-representable.

Graph index

Avg SHD

Avg precision (%)

Avg recall (%)

# instances where AGIP
improves over DAGIP in score

AGIP DAGIP AGIP

DAGIP

AGIP DAGIP

1
2
3
4
5
overall

6.7
9.2
8.0
29.8
21.7
15.1

6.6
10.5
8.8
29.8
23.0
15.7

63.7
59.4
67.3
27.4
30.0
49.6

59.5
50.5
64.8
29.2
27.6
46.3

64.4
63.0
63.8
17.6
27.3
47.2

60.0
52.0
60.0
19.0
24.7
43.1

10/10
7/10
5/10
4/10
2/10
28/50

cases with more latent variables.

4.3 Experiment 3: Non-DAG ADMGs

Although AGIP generates a solution with the same or better score than DAGIP, it returns a strictly
better score than DAGIP on a small fraction (13/60) of instances in Experiment 2. One possibility is
that all ground-truth graphs are DAGs. We next compare AGIP and DAGIP on graphs that are not
”DAG-representable”, i.e., the ground truth AG is not Markov-equivalent to any DAG. We randomly
generate AGs with (d, l) = (10, 10) following the scheme described in Section 4.2. We pick the ﬁrst
5 AGs that have at least one bidirected edge in the corresponding PAGs (which implies they are not
DAG-representable). All of these 5 AGs are provided in the supplement. For each of the 5 AGs, we
generate 10 instances of 10000 realizations of the model, each with a diﬀerent randomly generated
conditional linear Gaussian parametrization. In addition to the original candidate c-components, we
consider in AGIP c-components that are implied by three-node districts with up to 1 parent for each
node. Table 4 contains SHD, precision, and recall values of DAGIP and AGIP solutions.

AGIP has a strictly better score than DAGIP in 28 out of the 50 instances we considered. Note
that if the scores for optimal AGIP and DAGIP solutions are identical (as in 22 instances), then it is
not possible to predict which solution will perform better on metrics such as SHD, precision, or recall.
AGIP performs better than DAGIP in precision and recall on the 1st, 2nd, 3rd, and 5th AGs, and
performs better in SHD on the 2nd, 3rd, and 5th AGs. AGIP performs slightly worse than DAGIP on
the 4th AG, which has a very large c-component containing 7 nodes in the district. All of the ﬁve AGs
contain at least one c-component that is not covered by AGIP (nor by DAGIP). But considering more
c-components does help improve the solution quality on the average, which illustrates the advantage of
AGIP.

5 Conclusions and Future Work

We presented an integer-programming based approach for learning ancestral ADMGs from observational
data. Our main contributions are: 1) an IP formulation for the ancestral ADMG learning problem;
2) new classes of valid inequalities for eﬃcient solution of the IP model; 3) numerical experiments to
compare our model with existing methods. Empirical evaluation shows that our method has promising
performance and can generate solutions with better accuracy than existing state-of-the-art learning
methods. To our knowledge, this is the ﬁrst exact score-based method for solving such problems in the
presence of latent variables. For future work, extending the current approach to allow eﬃcient solution
with more c-components could further improve the solution quality of the proposed method. Adding

12

other classes of valid inequalities to strengthen the current IP formulation is another direction worth
exploring.

Acknowledgements

We thank Dr. James Luedtke for discussion on the problem formulation.

References

[1] D. M. Chickering, D. Heckerman, and C. Meek, “Large-sample learning of Bayesian networks is

NP-hard,” Journal of Machine Learning Research, vol. 5, pp. 1287–1330, 2004.

[2] D. Heckerman, D. Geiger, and D. M. Chickering, “Learning Bayesian networks: The combination

of knowledge and statistical data,” Machine learning, vol. 20, no. 3, pp. 197–243, 1995.

[3] I. Tsamardinos, L. Brown, and C. Aliferis, “The max-min hill-climbing Bayesian network structure

learning algorithm,” Machine Learning, vol. 65, no. 1, pp. 31–78, 2006.

[4] J. G´amez, J. Mateo, and J. Puerta, “Learning Bayesian networks by hill climbing: eﬃcient methods
based on progressive restriction of the neighborhood,” Data Mining and Knowledge Discovery,
vol. 22, no. 1-2, pp. 106–148, 2011.

[5] D. M. Chickering, “Optimal structure identiﬁcation with greedy search,” Journal of machine learn-

ing research, vol. 3, pp. 507–554, 2002.

[6] A. P. Singh and A. W. Moore, “Finding optimal Bayesian networks by dynamic programming,”

tech. rep., Carnegie Mellon University, 2005.

[7] T. Silander and P. Myllymaki, “A simple approach for ﬁnding the globally optimal Bayesian
network structure,” in Conference on Uncertainty in Artiﬁcial Intelligence, p. 445–452, 2006.

[8] T. Gao and D. Wei, “Parallel bayesian network structure learning,” in International Conference

on Machine Learning, pp. 1671–1680, 2018.

[9] C. Yuan and B. Malone, “Learning optimal bayesian networks: A shortest path perspective,”

Journal of Artiﬁcial Intelligence Research, vol. 48, pp. 23–65, 2013.

[10] T. Jaakkola, D. Sontag, A. Globerson, and M. Meila, “Learning bayesian network structure using
lp relaxations,” in International Conference on Artiﬁcial Intelligence and Statistics, pp. 358–365,
2010.

[11] J. Cussens, “Bayesian network learning with cutting planes,” in Conference on Uncertainty in

Artiﬁcial Intelligence, p. 153–160, 2011.

[12] J. Cussens, D. Haws, and M. Studen`y, “Polyhedral aspects of score equivalence in Bayesian network

structure learning,” Mathematical Programming, pp. 1–40, 2016.

[13] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman, Causation, prediction, and search.

MIT press, 2000.

[14] J. Pearl, Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.

[15] K. Tsirlis, V. Lagani, S. Triantaﬁllou, and I. Tsamardinos, “On scoring maximal ancestral graphs
with the max–min hill climbing algorithm,” International Journal of Approximate Reasoning,
vol. 102, pp. 74–85, 2018.

13

[16] T. Richardson and P. Spirtes, “Ancestral graph markov models,” The Annals of Statistics, vol. 30,

no. 4, pp. 962–1030, 2002.

[17] R. Bhattacharya, T. Nagarajan, D. Malinsky, and I. Shpitser, “Diﬀerentiable causal discovery

under unmeasured confounding,” arXiv preprint arXiv:2010.06978, 2020.

[18] P. Spirtes, “Introduction to causal inference.,” Journal of Machine Learning Research, vol. 11,

no. 5, 2010.

[19] J. Zhang, “On the completeness of orientation rules for causal discovery in the presence of latent

confounders and selection bias,” Artiﬁcial Intelligence, vol. 172, no. 16-17, pp. 1873–1896, 2008.

[20] J. Ramsey, J. Zhang, and P. L. Spirtes, “Adjacency-faithfulness and conservative causal inference,”

arXiv preprint arXiv:1206.6843, 2012.

[21] S. Triantaﬁllou and I. Tsamardinos, “Score-based vs constraint-based causal learning in the pres-
ence of confounders,” in UAI Workshop on Causation: Foundation to Application, pp. 59–67,
2016.

[22] D. Bernstein, B. Saeed, C. Squires, and C. Uhler, “Ordering-based causal structure learning in the
presence of latent variables,” in International Conference on Artiﬁcial Intelligence and Statistics,
pp. 4098–4108, 2020.

[23] K. Chobtham and A. C. Constantinou, “Bayesian network structure learning with causal eﬀects

in the presence of latent variables,” arXiv preprint arXiv:2005.14381, 2020.

[24] T. Richardson, “A factorization criterion for acyclic directed mixed graphs,” in Conference on

Uncertainty in Artiﬁcial Intelligence, p. 462–470, 2009.

[25] R. J. Evans and T. Richardson, “Markovian acyclic directed mixed graphs for discrete data,” The

Annals of Statistics, pp. 1452–1482, 2014.

[26] G. Schwarz, “Estimating the dimension of a model,” The annals of statistics, vol. 6, no. 2, pp. 461–

464, 1978.

[27] M. Drton, M. Eichler, and T. Richardson, “Computing maximum likelihood estimates in recursive
linear models with correlated errors.,” Journal of Machine Learning Research, vol. 10, no. 10,
pp. 2329–2348, 2009.

[28] C. Nowzohour, M. H. Maathuis, R. J. Evans, and P. B¨uhlmann, “Distributional equivalence and
structure learning for bow-free acyclic path diagrams,” Electronic Journal of Statistics, vol. 11,
no. 2, pp. 5342–5374, 2017.

[29] T. Silander, T. Roos, P. Kontkanen, and P. Myllym¨aki, “Factorized normalized maximum like-
lihood criterion for learning bayesian network structures,” in Proceedings of the 4th European
Workshop on Probabilistic Graphical Models, pp. 257–264, 2008.

[30] M. Gr¨otschel, M. J¨unger, and G. Reinelt, “On the acyclic subgraph polytope,” Mathematical

Programming, vol. 33, no. 1, pp. 28–42, 1985.

[31] Y. W. Park and D. Klabjan, “Bayesian network learning via topological order,” The Journal of

Machine Learning Research, vol. 18, no. 1, pp. 3451–3482, 2017.

[32] H. Manzour, S. K¨u¸c¨ukyavuz, H.-H. Wu, and A. Shojaie, “Integer programming for learning directed

acyclic graphs from continuous data,” INFORMS Journal on Optimization, pp. ijoo–2019, 2020.

14

[33] M. Bartlett and J. Cussens, “Integer linear programming for the bayesian network structure learn-

ing problem,” Artiﬁcial Intelligence, vol. 244, pp. 258–271, 2017.

[34] D. R. Karger, “Global min-cuts in rnc, and other ramiﬁcations of a simple min-cut algorithm.,”

in Annual ACM-SIAM Symposium on Discrete Algorithms, vol. 93, pp. 21–30, 1993.

[35] I. Tsamardinos, L. E. Brown, and C. F. Aliferis, “The max-min hill-climbing bayesian network

structure learning algorithm,” Machine learning, vol. 65, no. 1, pp. 31–78, 2006.

[36] R. Tillman and P. Spirtes, “Learning equivalence classes of acyclic models with latent and selec-
tion variables from multiple datasets with overlapping variables,” in International Conference on
Artiﬁcial Intelligence and Statistics, pp. 3–15, 2011.

Supplementary Material

Heuristics for Separation at Fractional Solutions

The starting point of our heuristic is Karger’s random contraction algorithm for ﬁnding near-optimal
min-cuts in edge-weighted undirected graphs (with nonnegative weights). Given a weighted graph with
n nodes and optimal min-cut value t and a positive integer α ≥ 1, Karger’s algorithms runs in time
bounded by a polynomial function of nα and returns all cuts in the graph with weight ≤ αt. A weighted
edge is chosen at random (with probability proportional to the weight of the edge), and the edge is
contracted. When an edge ij is contracted where i and j are (pseudo-)nodes, let i(cid:48) be a new pseudo-
node representing {i, j}. Edges of the form ki or kj in the graph are removed and an edge ki(cid:48) with
weight wki(cid:48) = wki + wkj is added, where wki is the weight of the edge ki in the graph before contraction
and 0 if no such edge exists. This contraction procedure is repeated till there are 2α pseudo-nodes left,
and the min-cut value in the resulting graph is returned. The central idea of the algorithm is that high
weight edges are contracted resulting in the end-nodes of such edges being put in the same ’side’ of the
ﬁnal cut.

We adapt the above idea. We ﬁrst discuss how to ﬁnd violated strengthened cluster inequalities.
Consider a subset S ⊆ V and a solution vector ¯z of the LP relaxation. Let µ(S) equal the left-hand
side of inequality where each z variable is set to the corresponding value in ¯z. If we ﬁnd a subset S ⊂ V
such that µ(S) < 1, then we have found a cluster inequality violated by the point ¯z. However, as
there are exponentially many choices of the set S, it is not realistic to enumerate each S and compute
µ(S). Instead, we initially only consider the sets S = {i} consisting of individual nodes and note that
µ({i}) = 1 for each node i because of equation (cid:80)
zC = 1. Let H0 be the undirected weighted
graph with the same set of nodes as G. We iteratively select and contract “high weight” edges and
create pseudonodes (that consist of the union of nodes associated with the two pseudonodes incident to
the edge), leading to a sequence of graphs H0, H1, . . ., where each graph has one less pseudonode than
the previous one. At the kth iteration we ensure that for each pseudonode i ∈ Hk, we have µk({i})
equal to the value of µ(S) where S is the set of nodes in H0 that correspond to the pseudonode i of
Hk.

C:i∈DC

Let the weight of an edge ij in H0 be calculated as follows. Deﬁne

wij :=

(cid:88)

(cid:88)

(cid:88)

¯zC +

(cid:88)

¯zC +

(cid:88)

¯zC.

W :j∈W

C∈C:i∈DC ,WC,i=W

W :i∈W

C∈C:j∈DC ,WC,j =W

C∈C:{i,j}⊆DC ,i /∈WC,j ,j /∈WC,i

Note that the following relationship holds:

µ({i, j}) = µ({i}) + µ({j}) − wij.

(12)

15

Step 1: If we apply the random contraction step in Karger’s algorithm to the weighted graph H0 to
obtain H1, then with high probability we will contract an edge ij with a high value of wij. This step
leads to an ij such that µ({i, j}) is approximately minimized (as µ({i}) = µ({j}) = 1 for all nodes i, j
of H0).
Step 2: We then create a pseudo-node {i, j} in H1 (labeled, say, by node i if i < j and by j otherwise).
Assuming the new psuedonode in H1 has label i, We let µ1({i}) = µ({i, j}) and µ1({k}) = µ({k}) for
all other nodes.
Step 3: We then recalculate wij values for edges in H1 in such a fashion that for every pair of pseudon-
odes in H1, the relationship in (12) holds. To do this, we ﬁrst remove all c-component variables ¯zC
where i ∈ DC and j ∈ WC,i or j ∈ DC and i ∈ WC,j. Next we replace all occurrences of j by i in the
remaining variables, and then recompute the weights wkl for edges kl.

If we repeat Steps 1-3 for H1 to obtain H2, H3, . . ., then it is not hard to see that we always maintain
the property in (12) with µ replaced by µk, and also the property that for any node i in Hk, the value
µk({i}) is equal to µ(S) where S is the set associated with the pseudonode i. We stop whenever we
ﬁnd a pseudonode i in Hk (and associated S) such that µk({i}) = µ(S) < 1. We repeat this algorithm
multiple times while starting from diﬀerent random seeds. Though this algorithm is not guaranteed to
ﬁnd a set S such that µ(S) < 1, it works well in practice, and does not return spurious sets S.

To adapt the above algorithm to ﬁnd violated strengthened bicluster inequalities, we proceed as
follows. Consider a speciﬁc bidirected edge ij such that ¯I(i ↔ j) > 0 for a given fractional solution
¯z. We ﬁrst contract ij in a special manner to obtain a graph H0. Assume i(cid:48) represents the resulting
pseudonode: for any c-component C such that i, j ∈ DC, we let WC,i and WC,j be replaced by a single
parent set W (cid:48) = WC,i ∪ WC,j of the new pseudonode i(cid:48). We also remove all c-component variables
zC such that DC ∩ {i, j} = 1. We subsequently deﬁne µ({k}) values for nodes k in H0, edge weights
wkl, perform a random contraction step and repeat this process till we ﬁnd a pseudonode i in Hk such
that µk({i}) < I(i ↔ j). We ensure that µk({i}) always represents the left-hand side of strengthened
bicluster inequalities.

Performance of diﬀerent methods when the number of latent
variables increases

We present in the following table the precise numbers (means of SHD, precision and recall) of the
results in Figure 6 of the main paper.

Table 5: Exact numbers for Figure 6

l

2
4
6

SHD

Precision (%)

Recall (%)

AGIP M3HC FCI

cFCI AGIP M3HC FCI

cFCI AGIP M3HC FCI

cFCI

12.6
22.4
28.8

26.4
27.3
32.8

30.8
31.6
35.9

24.8
24.4
31.3

80.3
63.0
57.8

54.1
57.7
49.9

29.6
36.9
33.6

50.0
52.6
46.0

78.7
63.1
53.1

49.3
52.7
44.5

26.2
34.9
29.7

44.1
48.9
39.8

16

Ground Truth AGs for Experiments in Section 4.3 of the main
paper

1

10

9

8

7

6

AG #1

2

5

3

4

9

8

1

10

9

8

7

6

AG #4

2

5

3

4

1

10

6

AG #2

3

4

9

8

7

2

5

1

10

6

AG #3

3

4

7

2

5

2

5

3

4

9

8

1

10

7

6

AG #5

17

