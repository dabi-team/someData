POSET-RL: Phase ordering for Optimizing Size and
Execution Time using Reinforcement Learning

Shalini Jain∗, Yashas Andaluri†, S. VenkataKeerthy‡ and Ramakrishna Upadrasta§
Department of Computer Science and Engineering, IIT Hyderabad
Email: {∗cs15resch11010, †cs17b21m000001, ‡cs17m20p100001}@iith.ac.in, §ramakrishna@cse.iith.ac.in

2
2
0
2

l
u
J

7
2

]
L
P
.
s
c
[

1
v
8
3
2
4
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—The ever increasing memory requirements of several
applications has led to increased demands which might not be
met by embedded devices. Constraining the usage of memory in
such cases is of paramount importance. It is important that such
code size improvements should not have a negative impact on
the runtime. Improving the execution time while optimizing for
code size is a non-trivial but a signiﬁcant task.

The ordering of standard optimization sequences in modern
compilers is ﬁxed, and are heuristically created by the compiler
domain experts based on their expertise. However, this ordering
is sub-optimal, and does not generalize well across all the cases.
We present a reinforcement learning based solution to the
phase ordering problem, where the ordering improves both
the execution time and code size. We propose two different
approaches to model the sequences: one by manual ordering,
and other based on a graph called Oz Dependence Graph (ODG).
Our approach uses minimal data as training set, and is integrated
with LLVM.

We show results on x86 and AArch64 architectures on
the benchmarks from SPEC-CPU 2006, SPEC-CPU 2017 and
MiBench. We observe that the proposed model based on ODG
outperforms the current Oz sequence both in terms of size and
execution time by 6.19% and 11.99% in SPEC 2017 benchmarks,
on an average.

Keywords-Phase Ordering, Compiler Optimization, Reinforce-

ment learning

I. INTRODUCTION

One of the essential components of compiler construction
are optimizations. They involve transforming the input pro-
gram and generating code that is semantically equivalent to
it, but performs better in various metrics like execution time,
code size, power utilization, etc.

Modern compilers have several number of optimizations like
vectorization, dead code elimination implemented as passes. A
collection of these individual passes are statically ordered a
priori and are exposed as off-the-shelf standard optimization
ﬂags like O0, O1, O2, O3, Os, and Oz. Among these
predeﬁned optimization ﬂags, O0 disables all optimizations,
essentially making the compiler a translator of the source-
program to binary. It provides fast compilation of source code,
while also giving good correlation between source and the
generated code. Optimization ﬂags O1, O2, and O3 are
designed to improve performance by reducing the execution
time. While the O1 ﬂag enables some important optimizations,
the O2 ﬂag adds a few more optimizations. The O2 ﬂag also
changes the heuristics for the optimizations when compared to
O1 ﬂag. With respect to execution of the code, the O3 sequence
gives the best performance among all these optimizations.

Fig. 1: O3 vs. Oz: Comparison of runtime and code size

Although these optimizations improve the performance of
program execution, they may also increase code size.

The Os and Oz ﬂags are designed to optimize for code
size. The Os ﬂag is designed to perform nearly similar to O2
ﬂag, while promising an appreciable decrease in code size.
The Oz ﬂag is designed to provide more code size reduction
than Os. Such a reduction in code size by optimizations in
Oz, generally incurs penalty on the execution time, leading
to a trade-off between O3 and Oz in terms of code size and
execution time.

We observe on a set of benchmarks from the standard SPEC
CPU benchmarks that the binaries generated by Oz ﬂag incurs
10% more execution time than that of O3; while resulting in
about 3.5% improvement in terms of code size, on an average.
A chart showing the runtime and code size characteristics of
these benchmarks is shown in Fig. 1. In this work, we try to
ﬁnd if this gap exhibited by Oz passes can be improved by
further reducing the code size while improving the run time.
Although these predeﬁned optimization ﬂags achieve good
performance when compared to the unoptimized case, they
do not guarantee the best performance for every given input
program. Moreover, the ordering of passes in these sequences
is not based on a particular input program but is statically
predeﬁned.

The problem of selecting and ordering the optimization
passes, called the phase ordering problem has been a fertile
ground for compiler researchers for many decades [1]. The
classic paper by Cooper et al. [2] refers to this particular
problem where they propose using AI/ML techniques for
reordering optimizations.

In the literature,

the phase ordering problem has been
recognized to be an important problem for several reasons,

 
 
 
 
 
 
including the following:

• On a particular program, two optimizations applied in
different orders may have an unpredictable impact on
its performance. It is also possible that a predetermined
optimization sequence could result in an adverse effect
on the performance of the program.

• After transformation of a program by an optimization
sequence or ﬂag, there is no guarantee to ensure perfor-
mance improvements. The user, who is interested in the
performance improvement of a particular program, may
also expect some guarantees about the potential improve-
ments on the particular program, or even a assurance that
all (possible) optimizations have been tried to improve the
program.

• The best optimization order designed for one program
may not perform well on another (new) program. The
predetermined optimization sequence could even degrade
the performance for new programs.

Exploiting the undecidable nature of this problem [3],
several machine learning based approaches [4], [5], [6], [7], [8]
have been used to predict the optimization sequences given
an input program. These techniques use the expert-designed
program features to represent programs for training the model
to arrive at the solution.

Several approaches have been proposed in the recent times
to represent programs as vectors, called program embeddings,
analogous to word2vec like approaches of natural language
processing. Such embeddings created by considering syntactic
and semantic information of the program is used as the
input for ML purposes. The most notable embeddings are
AST based embeddings of code2vec [9], and the IR based
embeddings of inst2vec [10], and IR2Vec [11].

To our understanding, the earlier works on phase ordering
have considered code size and execution time optimizations as
independent objectives. However, optimizing for codesize and
execution time are orthogonal - optimization for codesize can
adversely affect the execution time and vice versa. On the other
hand, if the execution time is not considered while optimizing
for codesize, gains in codesize can lead to worse performance.
Hence, we propose a Reinforcement Learning (RL) based
solution for determining better optimization sequences for a
given program to optimize for both code size together and
the execution time in LLVM compiler. Our work is based on
modeling programs as vectors using IR2Vec representations
which is a LLVM-based program encoding.

We use a reinforcement learning approach as the search
space of optimizations is too big to enumerate. For a compiler
with m optimization passes, if the sequence length is ﬁxed as
n, then there can be potentially mn combinations, allowing
repetitions. And, Oz of LLVM has 90 transformation passes,
among which 54 are unique. If one has to replicate such a
setup, there can be potentially 5490 ≈ 10156 combinations. As
it can be seen, creating this many combinations for each ele-
ment of the dataset is highly infeasible, making reinforcement
learning a go to method for this problem.

Also, we use IR2Vec representations as it captures the
syntax and semantics of the code intricately when compared
to the expert-designed feature set. IR2Vec representations of
the program are modelled as the state, and the sequence
of optimization passes as actions for the formulated RL
problem. The agent predicts the optimizations to apply on the
input program. The environment - LLVM optimizer applies
the predicted sequences and recomputes the representation
corresponding to the predicted sequence to update the state
for the next step. This environment is thus responsible for
handling the interaction between the agent and the effect of
the action it takes in the form of rewards.

We propose two different sets of sub-sequences to model
the action space in our RL model. Similar to the earlier works
like [4], [6] that model optimization sequences for execution
time by manual or analytical grouping of passes, we too
design a logical grouping of optimization passes to form the
action space. These groups form the sub-sequences, and are
obtained from the standard -Oz sequence of LLVM. Second,
we create a graph: Oz Dependence Graph (ODG) depicting
the dependencies among the passes in the Oz sequence and
derive the sequences by traversing the graph.

Our method uses Q-learning [12], which involves deter-
mining the optimum Q-function for a state-action pair. We
approximate this by using a neural network called Deep Q-
Network [13],[14], which performs better with complex envi-
ronments than the simple Q-learning algorithm that maintains
a table or a matrix called Q-table. We trained our model
with the single source llvm-test-suite benchmark ﬁles [15].
Our trained model is able to reduce binary sizes for most of
the benchmarks from MiBench [16], SPEC CPU (int and fp)
2017 [17], and SPEC CPU 2006 [18] benchmark-suites.

Contributions: The following are our key contributions:
• We propose a novel Deep Q-Network (DQN) based RL
model that predicts the optimal sequence of optimization
passes for a given input source program. Our framework
is unique as it builds from embeddings given by IR2Vec
encoding and derives its scalability characteristics.

• We propose a Oz Dependence Graph (ODG) to clus-
ter the individual optimization passes into various sub-
sequences that forms an action space of the RL model.
ODG shows all the dependencies among the optimization
passes in terms of the order they are present in the Oz
ﬂag.

• We study the results on two different architectures: x86
and AArch64. We show results on MiBench, SPEC CPU
2006 and 2017 benchmark suites.

• We show that our solution outperforms the standard Oz
sequence by achieving better code size reduction while
improving the runtime on standard compiler benchmarks.
In SPEC 2017, 2006 and MiBench, we achieve a maxi-
mum size reduction of upto 22.94%, 9.93%, 8.68%, while
achieving improvements in runtimes of upto 46%, 6% and
16% respectively.

The rest of the paper is further organized as the following:
Some relevant background of Reinforcement Learning (RL)

2

previous state or its next state depends only on its current
state. This augurs well with respect to our problem where the
agent can choose an optimization pass given its current state
and not have to look in the past.

B. Deep Q-Networks

Q-learning involves quantifying state-action pairs with q-
values that deﬁne how good a given action for a particular state
is. The vanilla Q-learning in practice involves maintaining a
table or matrix of values. Although the Q-learning algorithm
may generalize well to real-world problems, it usually be-
comes infeasible for complex environments as the number of
states go higher. The Deep Q-Network (DQN) was introduced
to solve this problem which involves combining the traditional
Q-learning algorithm with Deep Neural Networks [13]. The
deep neural network here acts as a function approximator for
the Q-function by replacing the need for storing values in a
Q-table. The various ﬂavours of DQN have been proposed and
are widely used [13], [14], [19], [20]. We use Double DQN
for modelling our problem.

A vanilla DQN on training suffers from the overestimation
of Q values [19]. The notion of double DQN was introduced
to meet with this challenge. Instead of using the target network
for predicting the future q-value, double DQN uses an online
network to predict the best action a(cid:48) for a given state s(cid:48). The
q-value, q(s(cid:48), a(cid:48)) for the state-action pair (s(cid:48), a(cid:48)) is given by
the target network.

C. IR2Vec

As described earlier, modern compilers have several nontriv-
ial compiler optimizations implemented in them that are the
result of manual heuristics designed by expert compiler writ-
ers. Various machine learning techniques have been proposed
to solve such tasks on source code by modelling them as Nat-
ural Language Processing problems [21]. Of late, researchers
identiﬁed the issues arising from modelling programs as nat-
ural languages and proposed programming language centric
approaches to model programs. These approaches result in
representing programs as vectors [9], [10], [11], analogous
to that of natural language representations like word2vec and
gloVe.

We use IR2Vec representations [11] to represent programs
in our model. IR2Vec uses the intermediate representation
of LLVM to represent programs as high dimensional vectors
called program embeddings. As it is based on LLVM IR, the
representation is naturally independent of the source languages
and target architectures. It can generate encodings for all the
languages that LLVM can support.

The IR2Vec framework is primarily built from a vocabulary
containing the representation of each fundamental entities of
the IR, viz. opcode, type and operands. These are used to form
the higher-level representations, at the program level, function
level, or instruction level. Also, IR2Vec uses the use-def,
reaching deﬁnitions, and live variable analyses information
to construct ﬂow-analysis based representations. It is shown
that IR2Vec representations have high scalablity, have better

Fig. 2: Reinforcement Learning Process

is explained in Section II. In our solution, we model each
program as states, and optimization passes as actions in accor-
dance with reinforcement learning techniques. We discuss this
design in Section III. In Section IV, we explain the rationale
for the above design. In Section V, we discuss experimentation
details, and the detailed analysis of results. We discuss the
in Section VII, we
related works in Section VI. Finally,
conclude our work.

II. BACKGROUND

In this section we give a brief summary of the reinforcement
learning techniques and the program representation method
that we use.

A. Reinforcement Learning and Markov Decision Process

The overview of a reinforcement learning method using
Markov Decision Process (MDP) is shown in Fig. 2. A rein-
forcement learning problem modelled on the Markov Decision
Process usually consists of the following:

• Agent: An agent is the actor that makes a decision by
choosing an action from an action space for which it
receives reward from the environment. A positive valued
reward represents a good decision and negative value
represents a bad decision. After performing any action,
the agent moves from current state to the new state.
• States: An instance of the environment is deﬁned as a
state. For each action the agent takes, the current state
transitions to a new state.

• Action: Decision taken by the agent by looking into the
current state. Set of all actions that an agent can take
forms an action space.

• Environment: The agent is placed in an environment that
rewards or punishes the agent for the current action. The
agent interacts with the environment to learn better so as
to maximize the reward in future.

An agent usually interacts with the environment through
states and actions to get rewards. The formulation of RL
problems can be exploited by a software agent to explore an
environment and make proﬁtable decisions. This ﬁts nicely
with our need for exploiting a sequence of actions viz opti-
mization passes and taking proﬁtable actions that results in
optimizing code size along with the execution time.

As discussed earlier, reinforcement learning can be formu-
lated using Markov Decision Processes (MDPs). An MDP
provides a mathematical framework that can model decision
making. An agent is said to follow the Markov Property, if
the current state of the agent depends only on its immediate

3

putation which we explain in greater detail in subsequent
Sections, III-A, III-B and III-C respectively.

A. Environment

As our work is based on LLVM-IR, we convert source ﬁles
into LLVM-IR using the existing frontend framework. Since
we consider only C/C++ input ﬁles, clang/clang++ converts
it into IR ﬁles. These IR ﬁles are original non-optimized
versions. We consider this data as an input dataset to the RL
agent.

the environment

As discussed in Section II,

in an RL
problem allows the agent to explore and make decisions on
the basis of the actions it takes. Here our environment serves
the same for the RL agent which interacts with it to perform
actions and to get rewards associated with it. Our environment
further consists of three modules i.e. IR2Vec embeddings,
Rewards and action space, each of these discussed in detail
below.

1) IR2Vec Embeddings: To represent the source programs
we use IR2Vec [11] embeddings which represent each pro-
gram as higher dimensional vectors that encode both program
features as well as ﬂow information. This enables us to
better represent the source programs as the semantics of it is
also encoded in the embeddings. Each source program vector
acts as a state for the environment which changes with the
application of optimization passes. This causes the DQN to
have transitioned to a different state.

2) Rewards: During training, we compile the LLVM-IR
module obtained at each step to its object ﬁle, without linking,
so as to prevent the inclusion of code from linked libraries.
This module computes the total size of the object ﬁle. We use
LLVM-MCA [22] to estimate the throughput of the generated
binary. Higher the throughput, lesser would be the runtime.
Hence we use this throughput as a static measure to model
the runtime of the code. Both the code size and throughput
components form the reward. We describe reward calculation
in section III-C. Initially, for each source program in the
input dataset, this module computes the size of an object
ﬁle and throughput generated by LLVM-MCA without any
optimization and with the -Oz optimization sequence. Then,
for each step of an episode, this module computes the size of
an object ﬁle and throughput generated by LLVM-MCA for
the resultant object ﬁle on applying the predicted action.

3) Action Space: Action space consists of all

the sub-
sequences (set of optimizations passes from LLVM) generated
from the Oz optimization sequence. At each step the RL agent
selects any one sub-sequence from the action space and applies
it to the IR present in the current state. We experiment with
two approaches for forming the action space - sub-sequences
generated by manual grouping and sub-sequence selection by
ODG, discussed in detail in Section IV.

B. RL Agent

As discussed in Section II, we deﬁne our RL agent as a
Double Deep Q-Network (DDQN). Given the IR2Vec rep-
resentation as a state, the DDQN predicts the associated Q-
value for every action in the action space containing different

Fig. 3: Overview of the proposed workﬂow

memory characteristics, and exhibit better Out Of Vocabulary
characteristics than the other approaches.

We chose IR2Vec representations to represent programs, as
it builds on program analysis approaches to form program
centric representations.

III. METHODOLOGY

The overview of our proposed methodology is shown in
Fig. 3. The input source code is initially fed to the environment
where the LLVM IR is generated by the LLVM’s front-end,
which in turn is converted to embeddings by using the IR2Vec
module as shown in Fig. 3. These representations of input
programs form our observation space or states as discussed in
Section II. The RL agent starts learning the information about
the environment through a series of episodes by consuming
these embeddings as input. For each step in an episode, given
the input program representation, the agent interacts with the
environment by predicting the optimization sub-sequence to
apply as an action from the action space. We describe the
action space in detail in Section IV. The LLVM optimizer
or opt is used to apply the optimizations predicted by the
RL agent, which then transitions the current state to the
next state. The new state is formed by updating the IR2Vec
representations corresponding to updated code after applying
the predicted transformations. For each such transition, the RL
agent receives a reward and continues the same till it reaches
the end of an episode. We refer to this ﬂow diagrammatically
in Fig. 3. The rewards are deﬁned as the combination of
reduction in the size of an object ﬁle as well as improvement
in throughput in comparison to an object ﬁle optimized with
the Oz sequence.

We broadly divide our framework into 3 main modules
viz (i) Environment, (ii) RL agent, and (iii) Reward com-

4

optimization sequences. As discussed earlier, the optimization
sequence with the highest Q-value is chosen and is then
applied to LLVM-IR of the source program to obtain the next
state. After gathering enough experiences by interacting with
the environment, the DDQN then starts to train to obtain the
optimal Q-value for every state.

C. Reward Computation

We deﬁne our reward (R) as the combination of improve-
ment in binary size and the throughput corresponding to the
optimized and the original, non optimized versions of the
compiled object ﬁle. It is computed as:

R = α ∗ RBinSize + β ∗ RT hroughput

(1)

Where, reward corresponding to BinSize (RBinSize) is deﬁned
as the ratio of change in size between each interim states of
an episode to binary size of the original object ﬁle, as shown
in Eqn (2).

RBinSize =

BinSizelast − BinSizecurr
BinSizebase

(2)

Here, BinSizelast refers to the size of the object ﬁle of the
previous state, BinSizecurr refers to the total size of the
object ﬁle after performing the action chosen by the model,
and BinSizebase refers to the size of the object ﬁle without
any optimization.

We describe the throughput component of the reward,
RT hroughput as the ratio of change in throughput between each
interim states of an episode to the throughput of the original
object ﬁle, as shown in Eqn (3).

RT hroughput =

T hroughtputcurr − T hroughtputlast
T hroughtputbase

(3)

Here,

T hroughputcurr,
refer

T hroughputbase
LLVM-MCA for current state,
optimization respectively.

T hroughputlast
and
to the throughput generated by
last state and without any

IV. SELECTION OF SUB-SEQUENCES
LLVM provides several optimization sequences (O0, O1,
O2, O3, Os, and Oz) that contribute either or both to
enhancing speedup and reduction of code size. Os and Oz
optimization sequences are well known for code size reduc-
tion. The Oz optimization sequence aims to reduce code size
of the given LLVM IR (even further than Os, at the cost of
possibly greater execution time than O2/O3).

The Oz sequence has the goal of reducing code size and
is designed using the same set of individual optimization
passes as O2. However, the individual optimizations occur with
different frequencies, and in a different order when compared
to O2. And, some passes vary the parameters for optimizations
depending on the optimization level, as different levels have
different goals.

Finding the optimal order of optimization passes with repet-
itive occurrences of optimizations has always been a very hard
problem. One simple way to deﬁne action space is to make
every individual pass as an action for our RL model. Firstly,

5

this has potential to induce a very large search space, as it
grows exponentially (due to various possible combinations of
individual passes, and sequence lengths). Further, this type of
modeling faces the following complications:

• Certain transformation passes cannot be applied individ-
ually. They must appear in a speciﬁc order to maintain
their mutual dependencies.

• Transformation passes can potentially disturb various
properties of the IR: either the control ﬂow of the
program, or its SSA property. These properties must
be restored by other passes before other transformation
passes are applied on the IR.

Due to these reasons, several works on phase ordering
proposed using different set of optimization passes, commonly
known as sub-sequences with respect to improving the exe-
cution time [4], [6], [7], code size reduction [8] and power
utilization [23]. Similarly, in this work we too prefer to use
sub-sequences instead of individual passes as actions. We
generate these sub-sequences by utilizing the Oz sequence,
as Oz is better known for code size reduction. This reduces
the search space of the model and also leads in better code
size reduction.

In this work, we propose two approaches to form the action
space: the ﬁrst is to group the optimization passes manually
to form sub-sequences. The second approach involves forming
a graph out of the optimization passes from Oz sequence,
and traversing it
to form sub-sequences, while preserving
their dependencies. These two approaches are discussed in
Sec. IV-A and Sec. IV-B.

A. Sub-sequences generated by manual grouping

As the number of passes is ﬁnite, obtaining precise sub-
sequences based on their functionality can help the model
make better decisions for selecting a sub-sequence. The
logic behind this is, as for different programs, different sub-
sequences may have different performance effects. So the
model can learn better based on the program characteristics
with the help of IR2Vec embedding and make better decisions.
Table I shows the Oz optimization sequence for llvm-
10. The sub-sequences created manually from LLVM’s Oz
optimization sequence are listed in Table II. This manual
grouping of passes has been done by looking into speciﬁc
functionalities of passes and the Oz sequence. For example,
sequence 2 performs some module level optimizations, while
sequence 4 performs inlining. Similarly, sequence 7 contains
loop related passes like loop rotate, loop invariant code motion,
loop unswitch and also combines redundant instructions, while
sequence 12 performs loop distribution and vectorization.

B. Sub-sequences generated by Oz Directed Graph (ODG)

We utilize the ordering of the passes in Oz to create new
sequences. We propose a directed graph (ODG) that is derived
from the ordering of Oz passes, as shown in Fig. 4.

Each transformation pass in the Oz ﬂag shown in the
Table I forms the node, and the edges depict the ordering
there will be an
of the passes within the sequence. I.e.,

TABLE I: List of transformation passes in Oz optimization sequence (LLVM-10)

List of Transformation Passes in Oz

-ee-instrument -simplifycfg -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -mem2reg -
deadargelim -instcombine -simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-
propagation -simplifycfg -instcombine -tailcallelim -simplifycfg -reassociate -loop-simplify -lcssa -loop-rotate -licm -loop-unswitch -simplifycfg -
instcombine -loop-simplify -lcssa -indvars -loop-idiom -loop-deletion -loop-unroll -mldst-motion -gvn -memcpyopt -sccp -bdce -instcombine -jump-
threading -correlated-propagation -dse -loop-simplify -lcssa -licm -adce -simplifycfg -instcombine -barrier -elim-avail-extern -rpo-functionattrs -globalopt -
globaldce -ﬂoat2int -lower-constant-intrinsics -loop-simplify -lcssa -loop-rotate -loop-distribute -loop-vectorize -loop-simplify -loop-load-elim -instcombine
-simplifycfg -instcombine -loop-simplify -lcssa -loop-unroll -instcombine -loop-simplify -lcssa -licm -alignmentfromassumptions -strip-dead-prototypes
-globaldce -constmerge -loop-simplify -lcssa -loop-sink -instsimplify -div-rem-pairs -simplifycfg

TABLE II: List of manual sub-sequences designed from Oz sequence

S.No. Manual Sub-sequence

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

-ee-instrument -simplifycfg -sroa -early-cse -lower-expect -forceattrs -inferattrs -mem2reg
-ipsccp -called-value-propagation -attributor -globalopt
-deadargelim -instcombine -simplifycfg
-prune-eh -inline -functionattrs -barrier
-sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-propagation
-simplifycfg -instcombine -tailcallelim -simplifycfg -reassociate
-loop-simplify -lcssa -loop-rotate -licm -loop-unswitch -simplifycfg -instcombine
-loop-simplify -lcssa -indvars -loop-idiom -loop-deletion -loop-unroll
-mldst-motion -gvn -memcpyopt -sccp -bdce -instcombine -jump-threading -correlated-propagation -dse
-loop-simplify -lcssa -licm -adce -simplifycfg -instcombine
-barrier -elim-avail-extern -rpo-functionattrs -globalopt -globaldce -ﬂoat2int -lower-constant-intrinsics
-loop-simplify -lcssa -loop-rotate -loop-distribute -loop-vectorize
-loop-simplify -loop-load-elim -instcombine -simplifycfg -instcombine
-loop-simplify -lcssa -loop-unroll -instcombine -loop-simplify -lcssa -licm -alignment-from-assumptions
-strip-dead-prototypes -globaldce -constmerge -loop-simplify -lcssa -loop-sink -instsimplify -div-rem-pairs -simplifycfg

Fig. 4: Oz Dependence Graph

edge between two nodes if they appear consecutively in the
Oz pass sequence. For example, as simplifycfg appears
after instcombine in Oz, we create a directed edge from
simplifycfg to instcombine in the graph.

We pick nodes with a degree greater than an arbitrary value
k as critical nodes. Such critical nodes are the prominent
nodes in the graph, where more than k passes depend on them.
Hence, we start and end the traversal on the ODG with such
nodes. ODG, when traversed from a critical node, would result
in a walk where the dependencies of each pass would appear

before it. We end the traversal when we reach another critical
node. We model the sub-sequences for action space using the
sequence of nodes that are visited during this traversal. This
way of obtaining sequences from the walk on ODG allows us
to get the sub-sequences that are not part of the Oz sequence
while keeping the dependencies of the passes intact.

We choose a degree k ≥ 8 for determining the
critical nodes. Consequently, we have simplifycfg,
instcomibne and, loop-simplify as critical nodes.
They have a degree of 11, 10 and 8 respectively. This results

6

TABLE III: List of sub-sequences designed from ODG of Oz sequence

S.No.

ODG Sub-sequence

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

25

26

27
28
29
30
31

32
33
34

-instcombine -barrier -elim-avail-extern -rpo-functionattrs -globalopt -globaldce -constmerge
-instcombine -barrier -elim-avail-extern -rpo-functionattrs -globalopt -globaldce -ﬂoat2int -lower-constant-intrinsics
-instcombine -barrier -elim-avail-extern -rpo-functionattrs -globalopt -mem2reg -deadargelim
-instcombine -jump-threading -correlated-propagation -dse
-instcombine -jump-threading -correlated-propagation
-instcombine
-instcombine -tailcallelim
-loop-simplify -lcssa -indvars -loop-idiom -loop-deletion -loop-unroll
-loop-simplify -lcssa -indvars -loop-idiom -loop-deletion -loop-unroll -mldst-motion -gvn -memcpyopt -sccp -bdce
-loop-simplify -lcssa -licm -adce
-loop-simplify -lcssa -licm -alignmentfromassumptions -strip-dead-prototypes -globaldce -constmerge
-loop-simplify -lcssa -licm -alignmentfromassumptions -strip-dead-prototypes -globaldce -ﬂoat2int -lower-constant-intrinsics
-loop-simplify -lcssa -licm -loop-unswitch
-loop-simplify -lcssa -loop-rotate -licm -adce
-loop-simplify -lcssa -loop-rotate -licm -alignmentfromassumptions -strip-dead-prototypes -globaldce -constmerge
-loop-simplify -lcssa -loop-rotate -licm -alignmentfromassumptions -strip-dead-prototypes -globaldce -ﬂoat2int -lower-constant-intrinsics
-loop-simplify -lcssa -loop-rotate -licm -loop-unswitch
-loop-simplify -lcssa -loop-rotate -loop-distribute -loop-vectorize
-loop-simplify -lcssa -loop-sink -instsimplify -div-rem-pairs -simplifycfg
-loop-simplify -lcssa -loop-unroll
-loop-simplify -lcssa -loop-unroll -mldst-motion -gvn -memcpyopt -sccp -bdce
-loop-simplify -loop-load-elim
-simplifycfg
-simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor
-globalopt -globaldce -constmerge -barrier
-simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor
-globalopt -globaldce -ﬂoat2int -lower-constant-intrinsics -barrier
-simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor
-globalopt -mem2reg -deadargelim -barrier
-simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-propagation -dse -barrier
-simplifycfg -prune-eh -inline -functionattrs -sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-propagation -barrier
-simplifycfg -reassociate
-simplifycfg -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -globaldce -constmerge
-simplifycfg -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -globaldce -ﬂoat2int
-lower-constant-intrinsics
-simplifycfg -sroa -early-cse -lower-expect -forceattrs -inferattrs -ipsccp -called-value-propagation -attributor -globalopt -mem2reg -deadargelim
-simplifycfg -sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-propagation -dse
-simplifycfg -sroa -early-cse-memssa -speculative-execution -jump-threading -correlated-propagation

in 34 different sub-sequences in our action space is shown in
Table III.

V. EXPERIMENTATION AND RESULT ANALYSIS

In this section we talk about the experimental setup of our

framework along with our results.

A. Experimental Setup

We used LLVM-10 for the experiments on the proposed
framework. The modular structure of LLVM provides us with
easy integration of our proposed action spaces viz Manual
sub-sequences listed in Table II and sub-sequences generated
with ODG listed in Table III. The IR2Vec [11] encoding
and infrastructure which we choose to represent input source
programs also ﬁts nicely with our choice for LLVM-10.

We use 130 source code ﬁles from the single source bench-
marks of LLVM-test-suite [15]. And, we consider entirely
different set of programs/benchmarks from MiBench, LLVM-
test-suite, SPEC-CPU 2006 and SPEC-CPU-2017 benchmark
suite for validation. We use the program level embeddings
of IR2Vec which represents a complete source program as a
vector of 300 dimensions.

Training: The proposed DQN model starts the training
in batches on the input training data. Each batch is a random
sample of states of IR2Vec embeddings which is generated
by the IR2Vec module to form the state space for the base
environment.

The implementation of DQN follows (cid:15)-greedy algorithm
with (cid:15) set
to a high value initially and then annealed in
subsequent steps. When (cid:15) is set to a high value the DQN is
said to be in the exploration state, which involves randomly
selecting actions and remembering the associated rewards and
the next state in a buffer called replay memory. The rewards
for each action is calculated using the reward function R
described in section III-C. We set α to 10 and β to 5 in
the reward function to give more weight to RBinSize than
RT hroughput. With the annealing of (cid:15), the DQN enters the
exploitation phase greedily by choosing the best possible
action next. After each µ step, where µ is a hyperparameter,
a random batch is sampled out of the replay memory to train
the DQN.

Learning rate is set to 10−4. Initial epsilon for exploration
starts at 1.0 and drops to a minimum of 0.01 over 20000
timesteps.

7

We trained our models on Intel Xeon E5-2690 and Intel
Xeon Gold 5122 for both manual and ODF sets of sub-
sequences for x86 and AArch64 processor. The number of
time steps per iteration was set to 1005. It took around 16
hours to train the model on the CPU mentioned. We study
the improvements in code size and runtimes on Intel Xeon
E5-2697 processor. And, we study the improvements in code
size in case of AArch64. This is obtained by cross compiling
LLVM to target Cortex-A72 processor.

B. Results

We evaluated the performance of our trained model by
validating programs from MiBench [16] and SPEC-CPU (2006
and 2017) benchmarks [18], [17]. By choosing these set of
benchmarks we cover both smaller and larger sized datasets.
To check the performance of our trained model, for the above
mentioned benchmarks, we apply the optimization sequence
as suggested by the trained model. Then, we generate the
binaries and compute the binary size for all the programs.
We then compare it with the binary size generated with Oz
optimization for the respective program.

In Table IV, we show the min, avg and max % of size
reduction for both x86 and AArch64 processor when we use
sequences predicted by the model that is trained with manual
and ODG sub-sequences. Negative value indicates that the
binary size obtained by applying the predicted sequence is
more than the binary size obtained using Oz. Min represents
the % of the minimum size improvement. Or, if it is negative,
it represents the maximum size increased. Max represents the
maximum size reduction, while avg represents average/mean
size reduction for the respective benchmark.

TABLE IV: Percentage of min, mean and max size reduction
wrt manual and ODG sequences for x86 and AArch64

Benchmark

Manual sub-sequences

ODG sub-sequences

Min

Avg

Max

Min

Avg

Max

SPEC-2017

SPEC-2006

MiBench

SPEC-2017

SPEC-2006

MiBench

-2.14

-3.69

-4.82

-8.45

-5.16

-9.43

0.12

-0.56

-1.26

0.88

2.47

-2.31

x86

3.74

2.45

0.91

-1.63

-0.02

-1.28

AArch64

4.88

6.64

0.54

-0.99

-0.82

-7.54

6.19

4.38

1.87

5.33

5.04

0.01

22.94

9.93

8.68

20.29

9.58

7.20

Table IV shows the performance, in terms of minimum,
average and maximum reduction in code size, of the model
when using manual sub-sequences and ODG sub-sequences
when compared to Oz. We observe that using manual sub-
sequences can achieve maximum size reduction of 3.74% for
SPEC-CPU 2017 and 2.45% for SPEC-CPU 2006 for x86.
Similarly, for AArch64 we obtain maximum size reduction of
4.88% for SPEC-CPU 2017, and 6.64% for SPEC-CPU 2006.

We also observe that for binary size reduction, the model
with ODG sub-sequences performs better than the model with
manual sub-sequences for all the considered benchmarks on
both x86 and AArch64 processors. With ODG sub-sequences,
the average size reduction is 6.19% and 4.38% with a max-
imum of 22.94% and 9.93% size reduction for SPEC-CPU
2017 and SPEC-CPU 2006 respectively for x86. Likewise,
the average size reduction for SPEC-CPU 2017 is 5.33% with
a maximum of 20.29% size reduction for AArch64. It can
be seen that average size reduction with ODG sub-sequences
is positive for all the benchmarks and a maximum of 6.19%
for SPEC-CPU 2017 for x86 processor is achieved. Although
we achieve good maximum size reduction with manual sub-
sequences, on an average it is not able to reduce binary sizes
when compared to that of ODG sequences. In the sequences
obtained from ODG, there are very few benchmarks where the
binary size has increased in comparison with Oz optimization.
We also run the binaries and measure their execution time
and compare with the execution time of Oz. The detailed
analysis of runtime and code size for the binaries generated
by predicted sequences from ODG for SPEC-CPU 2017 and
SPEC-CPU 2006 wrt x86 processor are shown in Fig. 5.

TABLE V: Percentage of improvement in execution time wrt
manual and ODG sequences for x86

Benchmark

Manual sub-sequences

ODG sub-sequences

SPEC-2017

SPEC-2006

MiBench

7.33

-4.68

4.13

11.99

-4.19

6.00

Table V shows the % decrease in execution time of binaries
obtained by applying the sequences predicted by the model
trained with manual or ODG sub-sequences. We can observe
that using ODG sub-sequences reduces execution time by
11.99% for SPEC-CPU 2017 and 6.00% for MiBench

Fig. 5 shows the execution time improvement in seconds
and binary size reduction in kilobytes for SPEC-CPU 2017 and
SPEC-CPU 2006 benchmarks. With the ODG sub-sequences,
the model predicts a sequence that leads to a reduction in exe-
cution time for 541.leela by 45.91% and 520.omnetpp
by 35.08%. We achieve good performance improvement for
most of the SPEC-CPU 2017 benchmarks, while execution
time is increasing for most of the SPEC-CPU 2006 bench-
marks.

In Fig. 5(c) and 5(d), we show the percentage reduction
in binary size for the sub-sequences generated by applying
ODG sequences when compared to the binaries generated
by applying Oz on SPEC-CPU 2017 and SPEC-CPU 2006
benchmarks. It can be seen that for almost all the benchmarks,
our framework is able to reduce the binary size. However, for
519.lbm and 464.h264 there is a slight increase in binary size.
In Table VI, we show 5 sub-sequences predicted by the
model. The ﬁrst sequence corresponding to x86 508.namd
starts with initial passes from the Oz sequences such
as -simplifycfg, -sroa, -early-cse and then has

8

(a) SPEC Runtimes

(b) SPEC Runtimes

(c) SPEC code size

(d) SPEC code size

Fig. 5: Runtime and Binary size for benchmarks from SPEC-CPU suite (2006 and 2017) for Oz and ODG sub-sequences
(lower is better)

TABLE VI: Some predicted sub-sequences

1

2

3

4

5

9→31→15→6→20→14→25→5→11→27→0→10→30→24→4
15→7→23→21→30→5→14→0→25→31→6→2→27→10→11
24→30→3→31→6→28→5→17→0→18→8→2→25→22→27
14→11→7→32→30→10→23→9→12→18→2→16→13→4→0
14→11→19→10→16→32→30→9→12→23→26→13→7→17→15

intermediate Oz passes such as
loop passes, -mem2reg,
-globalopt, -globaldce and ends with loop passes.
The second sequence was predicted for 525.x264 on x86.
It alternates between initial/intermediate Oz passes and loop
passes. Whereas, the third sequence starts with loop passes,
initial passes from Oz, -mem2reg and then has intermediate
Oz passes and ends with loop passes. This sequence was
predicted for susan benchmark from MiBench in x86. The
fourth sequence has intermediate Oz passes, initial Oz passes,
-mem2reg, loop passes and initial/intermediate Oz passes in
that order. The last two sequences correspond to 508.namd
and 511.povray benchmarks on compiling for AArch64.

As it can be observed, the predicted ODG sequences contain
a mix of passes from initial, intermediate, ending pass sub-
sequences and loop passes of Oz. Such combination of sub-
sequences are not present in the Oz sequence. This shows

that our framework can exploit the performance by applying
the new sub-sequences which otherwise would have not been
applied. It can also be seen that different sub-sequences are
predicted for different sources,
in turn improves the
performance.

that

VI. RELATED WORKS

In recent days, several approaches that use machine learning
to get better heuristics for many compiler optimization prob-
lems like vectorization [24] [25], inlining [26], unrolling [27],
etc. have been proposed and are widely accepted.

Several approaches have been proposed to use machine
learning techniques to solve the phase ordering problem.
These works range from using genetic algorithms [2], [28],
clustering techniques [4], and Bayesian networks [29]. Also,
several such approaches have been proposed to optimize for
either execution time [4], [7] or code size [30], [8]. To our
understanding, ours is the ﬁrst framework that attempts to
predict optimization sequences that
jointly optimizes both
execution time and binary size . There have been other works
including those that optimize compile time [28] and power
consumption [31].

Most of these works are based on expert deﬁned features.
However, the process of deﬁning the complete set of features is
cumbersome and needs domain compiler engineer expertise.

9

Hence, of late, deep learning is gaining popularity with its
ability to process large scale of data, and as it can relieve the
expert to pick the right set of features. The deep learning based
approaches do not require feature engineering by the expert
compiler engineer.

Several approaches have been proposed to represent source
code as input to deep learning methods. Feature based ap-
proaches represent programs as collection of hand picked
features like number of basic blocks, number of branches, etc.
Milepost [5] is one such work that uses program features to
learn the better optimization sequences and the parameters for
the optimization algorithms.

On the other hand, there exist approaches that represent
source codes as distributed vector, where the meaning of the
code is distributed across various components of the high-
dimensional vector. Methods like code2vec [9], inst2vec [10]
and IR2Vec [11] fall under this category. These approaches use
different abstractions of the program for forming representa-
tions. code2vec uses the paths of Abstract Syntax Trees, while
inst2vec and IR2Vec use the Intermediate Representation of
the programs to form language independent representations.
These approaches come up with representations that are inde-
pendent of the underlying applications.

Several approaches that use DNNs on reinforcement learn-
ing setup have been proposed to solve the phase ordering
problem [6], [32]. Modelling phase ordering for compiler
optimizations as a reinforcement learning ﬁts naturally, as
the number of possible combinations of the optimizations are
too high even on ﬁxing the sequence length. For a sequence
of length n and for the number of passes p, a total of
pn combinations are feasible. Enumerating them to create a
dataset to train on is intractable. Hence our framework also
models the phase ordering problem as a reinforcement learning
problem.

the
Different approaches have been proposed to model
output space differently. A seminal work by Cooper et al. [2]
proposed an ordering on 10 transformation passes to optimize
for code size. Another important work is by Kulkarni et al. [28]
that shows that such a space of optimization sequences is too
big to enumerate, and devise an exhaustive search technique
by which they analyze this space to automatically calculate
relationships between different phases.

The recent approaches model

the output space as sub-
sequences, where the sub-sequences are often grouped man-
ually by following certain heuristics. Ashori et al. proposed
a framework, MiCOMP [4] where the passes in O3 sequence
are clustered to form groups. These clusters are then treated
as sub-sequences. A predictive model learns the ordering of
these sub-sequences to achieve better execution time.

Jain et al. [33] analyze the effects of optimizations in Oz
sequence. They divide the Oz sequence into a set of logical
groups and analyze them to study their contributions for the
code size reduction.

Silva et al [34] in a similar spirit designed an approach to
arrive at the reduced set of optimization passes that perform
better than Os. They come up with a covering set of opti-

mization passes by exploring the space of optimizations using
genetic algorithms.

Closest to our work is that of Mammadli et al. [6] They
propose a framework that uses reinforcement learning to come
up with optimization sequences that have better execution
time characteristics when compared to O3. They model action
spaces by dividing the optimizations in O3 into sub-sequences.
These sub-sequences are divided into different levels of ab-
straction, where an action at a higher level has more number
of passes and parameters, while the lowest level has a single
the predicted
pass with parameters. For a given program,
sequence has a ﬁxed length and usually has a repetitions of
the same action. We on the other hand, design a framework
to achieve better code size characteristics while preserving, if
not improving the run time, when compared to Oz by using
a double DQN model. Also, we propose two different output
spaces as actions - a sub-sequence based output space derived
from Oz in the similar spirit of Mammadli et al; sub-sequences
obtained by traversing over the Oz dependence graph (ODG).
Our methodology is also different, we use the novel IR2Vec
embeddings [11] that can use the LLVM-IR representation for
encoding programs.

VII. CONCLUSIONS

We proposed a reinforcement learning based framework
to solve the compiler phase ordering problem, that results
in improving both code size and execution time. We model
an action space using two approaches, one based on manual
sub-sequences, and other by creating a graph out of Oz
optimization sequence and traversing it. Using only minimal
training, we were able to obtain improvements in terms of
code size and execution time over Oz optimization ﬂag of
LLVM-10. Our model uses static rewards calculated at the
compile time relying on LLVM-MCA for the runtime, and the
binary size to model code size characteristics. We show the
code size improvements on x86 and AArch64 architectures,
and execution time on x86 using standard benchmarks.

Our approach can be extened to O3 or other optimizations
by constructing the corresponding pass dependence graphs
for optimizing the performance or size. In future, we plan to
extend this framework to support predicting the parameters of
the optimizations (like unroll factors and vector factors) along
with the sequence. We have integrated our framework with
LLVM 10. The source code and other relevant artifacts are
available at https://compilers.cse.iith.ac.in/projects/posetrl/.

ACKNOWLEDGMENTS

We are grateful to Suresh Purini for insightful discussions
and guidance at the early stage of this work. We thank Anilava
Kundu for the discussions and help in the initial stages of
implementation.

This research is funded by the Department of Electronics &
Information Technology and the Ministry of Communications
& Information Technology, Government of India. This work is
partially supported by a Visvesvaraya PhD Scheme under the
MEITY, GoI (PhD-MLA/04(02)/2015-16), an NSM research

10

[21] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, “A survey
of machine learning for big code and naturalness,” ACM Computing
Surveys (CSUR), vol. 51, no. 4, p. 81, 2018.
“LLVM-MCA.”

https://llvm.org/docs/CommandGuide/

[22] L.

Org.,
llvm-mca.html.

[23] J. Pallister, S. J. Hollis, and J. Bennett, “Identifying compiler options to
minimize energy consumption for embedded platforms,” The Computer
Journal, vol. 58, no. 1, pp. 95–109, 2015.

[24] K. Stock, L.-N. Pouchet, and P. Sadayappan, “Using machine learning
to improve automatic vectorization,” ACM Trans. Archit. Code Optim.,
vol. 8, Jan. 2012.

[25] A. Haj-Ali, N. K. Ahmed, T. Willke, Y. S. Shao, K. Asanovic, and
I. Stoica, “Neurovectorizer: End-to-end vectorization with deep rein-
forcement learning,” in Proceedings of the 18th ACM/IEEE International
Symposium on Code Generation and Optimization, CGO 2020, (New
York, NY, USA), p. 242–255, Association for Computing Machinery,
2020.

[26] D. Simon, J. Cavazos, C. Wimmer, and S. Kulkarni, “Automatic con-
struction of inlining heuristics using machine learning,” in Proceedings
of the 2013 IEEE/ACM International Symposium on Code Generation
and Optimization (CGO), CGO ’13, (USA), p. 1–12, IEEE Computer
Society, 2013.

[27] M. Stephenson and S. Amarasinghe, “Predicting unroll factors using su-
pervised classiﬁcation,” in International Symposium on Code Generation
and Optimization, pp. 123–134, March 2005.

[28] P. A. Kulkarni, D. B. Whalley, G. S. Tyson, and J. W. Davidson,
“Exhaustive optimization phase order space exploration,” in Proceedings
of the International Symposium on Code Generation and Optimization,
CGO ’06, (USA), p. 306–318, IEEE Computer Society, 2006.

[29] A. H. Ashouri, G. Mariani, G. Palermo, E. Park, J. Cavazos, and
C. Silvano, “Cobayn: Compiler autotuning framework using bayesian
networks,” ACM Trans. Archit. Code Optim., vol. 13, June 2016.
[30] K. Cooper, D. Subramanian, and L. Torczon, “Adaptive optimizing
compilers for the 21st century,” The Journal of Supercomputing, vol. 23,
09 2002.

[31] R. Nobre, L. Reis, and J. Cardoso, “Compiler phase ordering as an
orthogonal approach for reducing energy consumption,” 07 2018.
[32] Q. Huang, A. Haj-Ali, W. S. Moses, J. Xiang, I. Stoica, K. Asanovi´c, and
J. Wawrzynek, “Autophase: Compiler phase-ordering for hls with deep
reinforcement learning,” 2019 IEEE 27th Annual International Sympo-
sium on Field-Programmable Custom Computing Machines (FCCM),
pp. 308–308, 2019.

[33] S. Jain, U. Bora, P. Kumar, V. B. Sinha, S. Purini, and R. Upadrasta, “An
analysis of executable size reduction by llvm passes,” CSI Transactions
on ICT, pp. 1–6, 2019.

[34] A. F. d. Silva, B. N. B. de Lima, and F. M. Q. a. Pereira, “Exploring
the space of optimization sequences for code-size reduction: Insights
and tools,” CC 2021, (New York, NY, USA), p. 47–58, Association for
Computing Machinery, 2021.

grant (MeitY/R&D/HPC/2(1)/2014), a Visvesvaraya Young
Faculty Research Fellowship from MeitY, and a Google PhD
Fellowship.

REFERENCES

[1] S. R. Vegdahl, “Phase coupling and constant generation in an optimizing
microcode compiler,” SIGMICRO Newsl., vol. 13, p. 125–133, Oct.
1982.

[2] K. D. Cooper, P. J. Schielke, and D. Subramanian, “Optimizing for
reduced code space using genetic algorithms,” in Proceedings of the
ACM SIGPLAN 1999 Workshop on Languages, Compilers, and Tools
for Embedded Systems, LCTES ’99, (New York, NY, USA), p. 1–9,
Association for Computing Machinery, 1999.

[3] S.-A.-A. Touati and D. Barthou, “On the decidability of phase order-
ing problem in optimizing compilation,” in Proceedings of the 3rd
Conference on Computing Frontiers, CF ’06, (New York, NY, USA),
p. 147–156, Association for Computing Machinery, 2006.

[4] A. H. Ashouri, A. Bignoli, G. Palermo, C. Silvano, S. Kulkarni, and
J. Cavazos, “Micomp: Mitigating the compiler phase-ordering problem
using optimization sub-sequences and machine learning,” vol. 14, no. 3,
2017.

[5] G. Fursin, C. Miranda, O. Temam, M. Namolaru, E. Yom-Tov, A. Zaks,
B. Mendelson, E. V. Bonilla, J. Thomson, H. Leather, C. K. I. Williams,
M. O’Boyle, P. Barnard, E. Ashton, E. Courtois, and F. Bodin, “Milepost
gcc: machine learning based research compiler,” 2008.

[6] R. Mammadli, A. Jannesari, and F. Wolf, “Static neural compiler
optimization via deep reinforcement learning,” pp. 1–11, 11 2020.
[7] S. Purini and L. Jain, “Finding good optimization sequences covering
program space,” ACM Trans. Archit. Code Optim., vol. 9, Jan. 2013.
[8] A. F. Zanella, A. F. da Silva, and F. M. Quint˜ao, “Yacos: A complete
infrastructure to the design and exploration of code optimization se-
quences,” in Proceedings of the 24th Brazilian Symposium on Context-
Oriented Programming and Advanced Modularity, SBLP ’20, (New
York, NY, USA), p. 56–63, Association for Computing Machinery, 2020.
[9] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “Code2vec: Learning
distributed representations of code,” Proc. ACM Program. Lang., vol. 3,
pp. 40:1–40:29, Jan. 2019.

[10] T. Ben-Nun, A. S. Jakobovits, and T. Hoeﬂer, “Neural code compre-
hension: A learnable representation of code semantics,” in Proceedings
of the 32Nd International Conference on Neural Information Processing
Systems, NIPS’18, (USA), pp. 3589–3601, Curran Associates Inc., 2018.
[11] S. VenkataKeerthy, R. Aggarwal, S. Jain, M. S. Desarkar, R. Upadrasta,
and Y. N. Srikant, “Ir2vec: Llvm ir based scalable program embeddings,”
ACM Trans. Archit. Code Optim., vol. 17, Dec. 2020.

[12] C. J. Watkins and P. Dayan, “Q-learning,” vol. 8, pp. 279–292, Springer,

1992.

[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” 2013.

[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control
learning,”
vol. 518, pp. 529–533, Nature Publishing Group, 2015.

through deep reinforcement

[15] L. Org., “LLVM Test Suite.” https://github.com/llvm/llvm-test-suite.

Accessed 2021-08-25.

[16] M. Guthaus, J. Ringenberg, D. Ernst, T. Austin, T. Mudge, and
R. Brown, “Mibench: A free, commercially representative embedded
the Fourth Annual IEEE In-
benchmark suite,” in Proceedings of
ternational Workshop on Workload Characterization. WWC-4 (Cat.
No.01EX538), pp. 3–14, 2001.

[17] J. Bucek, K.-D. Lange, and J. v. Kistowski, “Spec cpu2017: Next-
generation compute benchmark,” in Companion of the 2018 ACM/SPEC
International Conference on Performance Engineering, ICPE ’18, (New
York, NY, USA), pp. 41–42, ACM, 2018.

[18] J. L. Henning, “SPEC CPU2006 benchmark descriptions,” SIGARCH

Comput. Archit. News, vol. 34, no. 4, pp. 1–17, 2006.

[19] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” in Proceedings of the AAAI conference on
artiﬁcial intelligence, vol. 30, 2016.

[20] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
“Dueling network architectures for deep reinforcement learning,” in
International conference on machine learning, pp. 1995–2003, PMLR,
2016.

11

