2
2
0
2

n
u
J

4

]

C
C
.
s
c
[

1
1
v
3
8
7
4
0
.
2
0
0
2
:
v
i
X
r
a

Fixed-Support Wasserstein Barycenters:
Computational Hardness and Fast Algorithm

Tianyi Lin⋄ Nhat Ho⋆ Xi Chen‡ Marco Cuturi⊳,⊲ Michael I. Jordan⋄,†

Department of Electrical Engineering and Computer Sciences⋄
Department of Statistics†
University of California, UC Berkeley
Department of Statistics and Data Sciences, University of Texas, Austin⋆
Stern School of Business, New York University‡
CREST - ENSAE⊳, Google Brain⊲

June 7, 2022

Abstract

≥

≥

3 and n

We study the ﬁxed-support Wasserstein barycenter problem (FS-WBP), which consists
in computing the Wasserstein barycenter of m discrete probability measures supported on
a ﬁnite metric space of size n. We show ﬁrst that the constraint matrix arising from the
standard linear programming (LP) representation of the FS-WBP is not totally unimodular
when m
3. This result resolves an open question pertaining to the relationship
between the FS-WBP and the minimum-cost ﬂow (MCF) problem since it proves that the
3.
FS-WBP in the standard LP form is not an MCF problem when m
≥
We also develop a provably fast deterministic variant of the celebrated iterative Bregman
projection (IBP) algorithm, named FastIBP, with a complexity bound of
O(mn7/3ε−4/3),
where ε
(0, 1) is the desired tolerance. This complexity bound is better than the best
O(mn2ε−2) for the IBP algorithm in terms of ε, and that of
known complexity bound of
O(mn5/2ε−1) from accelerated alternating minimization algorithm or accelerated primal-
dual adaptive gradient algorithm in terms of n. Finally, we conduct extensive experiments
with both synthetic data and real images and demonstrate the favorable performance of
e
the FastIBP algorithm in practice.

3 and n

≥

∈

e

e

1

Introduction

Over the past decade, the Wasserstein barycenter problem [Agueh and Carlier, 2011] (WBP)
has served as a foundation for theoretical analysis in a wide range of ﬁelds, including eco-
nomics [Carlier and Ekeland, 2010, Chiappori et al., 2010] and physics [Buttazzo et al., 2012,
Cotar et al., 2013, Trouv´e and Younes, 2005] to statistics [Munch et al., 2015, Ho et al., 2017,
Srivastava et al., 2018], image and shape analysis [Rabin et al., 2011, Bonneel et al., 2015,
2016] and machine learning [Cuturi and Doucet, 2014]. The WBP problem is related to the
optimal transport (OT) problem, in that both are based on the Wasserstein distance, but the
WBP is signiﬁcantly harder. It requires the minimization of the sum of Wasserstein distances,
and typically considers m > 2 probability measures. Its closest relative is the multimarginal
optimal transport problem [Gangbo and Swiech, 1998], which also compares m measures;
see Villani [2008] for a comprehensive treatment of OT theory and Peyr´e and Cuturi [2019]
for an introduction of OT applications and algorithms.

An ongoing focus of work in both the WBP and the OT problem is the design of fast algo-
rithms for computing the relevant distances and optima and the delineation of lower bounds

1

 
 
 
 
 
 
that capture the computational hardness of these problems [Peyr´e and Cuturi, 2019]. For the
OT problem, Cuturi [2013] introduced the Sinkhorn algorithm which has triggered signiﬁcant
progress [Cuturi and Peyr´e, 2016, Genevay et al., 2016, Altschuler et al., 2017, Dvurechensky et al.,
2018, Blanchet et al., 2018, Lin et al., 2019b, Lahn et al., 2019, Quanrud, 2019, Jambulapati et al.,
2019, Lin et al., 2019c]. Variants of the Sinkhorn and Greenkhorn algorithms [Altschuler et al.,
2017, Dvurechensky et al., 2018, Lin et al., 2019b] continue to serve as the baseline approaches
O(n2ε−1) [Blanchet et al.,
in practice. As for the theoretical complexity, the best bound is
2018, Quanrud, 2019, Lahn et al., 2019, Jambulapati et al., 2019]. Moreover, Lin et al. [2019a]
provided a complexity bound for the multimarginal OT problem.

e

There has been signiﬁcant eﬀort devoted to the development of fast algorithms in the
case of m > 2 discrete probability measures [Rabin et al., 2011, Cuturi and Doucet, 2014,
Carlier et al., 2015, Bonneel et al., 2015, Benamou et al., 2015, Anderes et al., 2016, Staib et al.,
2017, Ye et al., 2017, Borgwardt and Patterson, 2020, Puccetti et al., 2020, Claici et al., 2018,
Uribe et al., 2018, Dvurechenskii et al., 2018, Yang et al., 2018, Le et al., 2019, Kroshnin et al.,
2019, Guminov et al., 2019, Ge et al., 2019, Borgwardt and Patterson, 2019]. This work has
provided the foundation for progress on the WBP. An important step forward was the proposal
of Cuturi and Doucet [2014] to smooth the WBP using an entropic regularization, leading to a
simple gradient-descent scheme that was later improved and generalized under the name of the
iterative Bregman projection (IBP) algorithm [Benamou et al., 2015, Kroshnin et al., 2019].
Further progress includes the semi-dual gradient descent [Cuturi and Peyr´e, 2016, 2018], accel-
erated primal-dual gradient descent (APDAGD) [Dvurechenskii et al., 2018, Kroshnin et al.,
2019], accelerated IBP [Guminov et al., 2019], stochastic gradient descent [Claici et al., 2018],
distributed and parallel gradient descent [Staib et al., 2017, Uribe et al., 2018], alternating di-
rection method of multipliers (ADMM) [Ye et al., 2017, Yang et al., 2018] and interior-point
algorithm [Ge et al., 2019]. Very recently, Kroshnin et al. [2019] and Guminov et al. [2019]
have proposed a novel primal-dual framework that made it possible to derive complexity
bounds for various algorithms, including IBP, accelerated IBP and APDAGD.

Concerning the computational hardness of the WBP with free support, Anderes et al.
[2016] proved that the barycenter of m empirical measures is also an empirical measure with
support whose cardinality is at most the size of the union of the support of the m measures,
minus m
1. When m = 2 and the measures are bound and the support is ﬁxed, the
computation of the barycenter amounts to solving a network ﬂow problem on a directed
graph. Borgwardt and Patterson [2019] proved that ﬁnding a barycenter of sparse support is
NP hard even in the simple setting when m = 3. However, their analysis cannot be extended
to the ﬁxed-support WBP, where the supports of the constituent m probability measures are
prespeciﬁed.

−

Contribution.
In this paper, we revisit the ﬁxed-support Wasserstein barycenter problem
(FS-WBP) between m discrete probability measures supported on a prespeciﬁed set of n
points. Our contributions can be summarized as follows:

1. We prove that the FS-WBP in the standard LP form is not a minimum-cost ﬂow (MCF)
problem in general. In particular, we show that the constraint matrix arising from the
standard LP representation of the FS-WBP is totally unimodular when m
3 and
n = 2 but not totally unimodular when m
3. Our results shed light on
the necessity of problem reformulation—e.g., entropic regularization [Cuturi and Doucet,
2014, Benamou et al., 2015] and block reduction [Ge et al., 2019].

3 and n

≥

≥

≥

2. We propose a fast deterministic variant of the iterative Bregman projection (IBP) al-

2

∈

gorithm, named FastIBP, and provide a theoretical guarantee for the algorithm. Let-
(0, 1) denote the target tolerance, the complexity bound of the algorithm is
ting ε
O(mn2ε−2) of the IBP algo-
O(mn7/3ε−4/3), which improves the complexity bound of
O(mn5/2ε−1)
rithm [Benamou et al., 2015] in terms of ε and the complexity bound of
e
from the accelerated IBP and APDAGD algorithms in terms of n [Kroshnin et al., 2019,
Guminov et al., 2019]. We conduct experiments on synthetic and real datasets and
demonstrate that the FastIBP algorithm achieves the favorable performance in prac-
tice.

e

e

Organization. The remainder of the paper is organized as follows. In Section 2, we provide
the basic setup for the entropic-regularized FS-WBP and the dual problem. In Section 3, we
In
present our computational hardness results for the FS-WBP in the standard LP form.
Sections 4, we propose and analyze the FastIBP algorithm. We present experimental results
on synthetic and real data in Section 5 and conclude in Section 6.

Rn

u
{

and Rn

1, 2, . . . , n
{
+ : 1⊤

. For a smooth function f , we denote
n u = 1
}
∈
∇λf as the full gradient and the gradient with respect to a variable λ. For x
kp for its ℓp-norm. For X = (Xij)

+ be the set of all vectors in Rn with
Notation. We let [n] be the set
}
nonnegative components. 1n and 0n are the n-vectors of ones and zeros. ∆n stands for the
probability simplex: ∆n =
f
∇
Rn
and
Rn×n, the notations
and 1
≤
and det(X) stand for the vector representation and the determinant. The
vec (X)
∈
. The notations r(X) = X1n
notations
Xij|
Xij|
Rn×n, the Frobenius and Kronecker inner product are denoted
and c(X) = X ⊤1n. Let X, Y
Y . Given the dimension n and ε, the notation a = O(b(n, ε)) stands for
by
and X
⊗
i
the upper bound a
O(b(n, ε))
indicates the previous inequality where C depends only the logarithmic factors of n and ε.

b(n, ε) where C > 0 is independent of n and ε, and a =

k∞ = max1≤i,j≤n |
∈

p
Rn2
X
k

, we write

1≤i,j≤n |

X, Y
h

k1 =

≤ ∞

X
k

x
k

and

P

≤

C

∈

∈

·

2 Preliminaries and Technical Background

In this section, we introduce the basic setup of the ﬁxed-support Wasserstein barycenter
problem (FS-WBP), starting with the linear programming (LP) presentation and entropic-
regularized formulation and including a speciﬁcation of an approximate barycenter.

e

2.1 Linear programming formulation

For p
The Wasserstein distance of order p

Pp(Ω) be the set of Borel probability measures on Ω with ﬁnite p-th moment.

1 between µ, ν

1, let

≥

∈ Pp(Ω) is deﬁned by [Villani, 2008]:

≥

Wp(µ, ν) := inf

π∈Π(µ,ν)

(cid:18)ZΩ×Ω

dp(x, y) π(dx, dy)

1/p

,

(cid:19)

(1)

where d(
,
·
weight vector (ω1, ω2, . . . , ωm)
∈
2011] of m probability measures
problem

) is a metric on Ω and Π(µ, ν) is the set of couplings between µ and ν. Given a
·

∆m for m
2, the Wasserstein barycenter [Agueh and Carlier,
m
k=1 is a solution of the following functional minimization
µk}
{

≥

min
µ∈Pp(Ω)

m

Xk=1

ωkW p

p (µ, µk).

3

(2)

Because our goal is to provide computational schemes to approximately solve the WBP, we
need to provide a deﬁnition of an ε-approximate solution to the WBP.

m

m

µ

≤

µ, µk)

k=1 ωkW p

Deﬁnition 2.1. The probability measure

∈ Pp(Ω) is called an ε-approximate barycenter if
k=1 ωkW p
p (µ⋆, µk) + ε where µ⋆ is an optimal solution to problem (2).
p (
There are two main settings: (i) free-support Wasserstein barycenter, namely, when we
P
optimize both the weights and supports of the barycenter in Eq. (2); and (ii) ﬁxed-support
Wasserstein barycenter, namely, when the supports of the barycenter are obtained from those
m
k=1 and we optimize the weights of the barycenter in
from the probability measures
Eq. (2).

µk}
{

P

b

b

−

−

The free-support WBP problem is notoriously diﬃcult to solve. It can either be solved using
a solution to the multimarginal-OT (MOT) problem, as described in detail by Agueh and Carlier
[2011], or approximated using alternative optimization techniques. Assuming that each mea-
sure is supported on n distinct points, the WBP problem can be solved exactly by solving
ﬁrst a MOT, to then compute (n
1)m + 1 barycenters of points in Ω (these barycenters are
exactly the support of the barycentric measure). Solving a MOT is, however, equivalent to
solving an LP with nm variables and (n
1)m + 1 constraints. The other route, alternative
optimization, requires specifying an initial guess for the barycenter, a discrete measure sup-
ported on k weighted points (where k is predeﬁned). One can then proceed by updating the
locations of µ (or even add new ones) to decrease the objective function in Eq. (2), before
changing their weights. In the Euclidean setting with p = 2, the free-support WBP is closely
related to the clustering problem, and equivalent to k-means when m = 1 [Cuturi and Doucet,
2014]. Whereas solving the free-support WBP using MOT results in a convex (yet intractable)
problem, the alternating mimimization approach is not, in very much the same way that the
k-means problem is not, and results in the minimization of a piece-wise quadratic function.
On the other hand, the ﬁxed-support WBP is comparatively easier to solve, and as such has
played a role in real-world applications. For instance, in imaging sciences, pixels and voxels are
supported on a predeﬁned, ﬁnite grid. In these applications, the barycenter and µk measures
share the same support.

In view of this, throughout the remainder of the paper, we let (µk)m
k=1 be discrete probabil-
xk
m
k=1 have the ﬁxed
i }i∈[n] to be ﬁxed. Since
µk}
{
{
m
k=1. Accordingly, the support of the
}
xk
i }i∈[n]. Given this setup, the
{
m
k=1 has the following standard LP representation [Cuturi and Doucet,

ity measures and take the support points
support, they are fully characterized by the weights
barycenter
FS-WBP between
2014, Benamou et al., 2015, Peyr´e and Cuturi, 2019]:

xi}i∈[n] is also ﬁxed and can be prespeciﬁed by
{

µk}
{

uk
{

b

m

+

{Xi}m

ωkh

min
i=1⊆Rn×n

,
Ck, Xki
Rn×...×n
m
k=1 and
where
+
cost matrices and (Ck)ij = dp(xk
xj) for all k
i ,
ter u

Xk=1
m
Ck}
Xk}
k=1 ⊆
{
{
∆n is determined by the weight

s.t.

∈

r(Xk) = uk for all k
c(Xk+1) = c(Xk) for all k

∈

[m],

[m

1],

−

∈

(3)

denote a set of transportation plans and nonnegative
[m]. The ﬁxed-support Wasserstein barycen-

b

∈

m
k=1 ωkc(Xk) and the support (

From Eq. (3), the FS-WBP is an LP with 2mn

x1,
n equality constraints and mn2 vari-
ables. This has inspired work on solving the FS-WBP using classical optimization algo-
b
rithms [Ge et al., 2019, Yang et al., 2018]. Although progress has been made, the understand-
ing of the structure of FS-WBP via this approach has remained limited. Particularly, while
the OT problem [Villani, 2008] is equivalent to a minimum-cost ﬂow (MCF) problem, it re-
mains unknown whether the FS-WBP is a MCF problem even in the simplest setting when
m = 2.

x2, . . . ,

xn).

P

−

b

b

4

2.2 Entropic regularized FS-WBP

Using Cuturi’s entropic approach to the OT problem [Cuturi, 2013], we deﬁne a regularized
version of the FS-WBP in Eq. (3), where an entropic regularization term is added to the
Wasserstein barycenter objective. The resulting formulation is as follows:

min
i=1⊆Rn×n

+

{Xi}m

m

Xk=1

Ck, Xki −
ωk(
h

ηH(Xk)),

s.t.

r(Xk) = uk for all k
c(Xk+1) = c(Xk) for all k

∈

[m],

[m

1],

−

∈

(4)

denotes the entropic
where η > 0 is the parameter and H(X) :=
regularization term. We refer to Eq. (4) as entropic regularized FS-WBP. When η is large,
the optimal value of entropic regularized FS-WBP may yield a poor approximation of the cost
of the FS-WBP. To guarantee a good approximation, we scale the parameter η as a function
of the desired accuracy.

X, log(X)

−h

−

1n1⊤
n i

Deﬁnition 2.2. The probability vector
exists a feasible solution (
m
such that
k=1 ωkc(
u =
2 , . . . , X ⋆
1 , X ⋆
where (X ⋆
b

X2, . . . ,
X1,
Xk) for all k
b
b
b
b

P

u
∈
Xm)
b
∈

∈
[m] and

Rn×n

∆n is called an ε-approximate barycenter if there
for the FS-WBP in Eq. (3)
+ ε
k=1 ωkh
Xki ≤
b

+ × · · · ×
m
k=1 ωkh

Ck, X ⋆
k i

Rn×n
+
Ck,

m

m) is an optimal solution of the FS-WBP in Eq. (3).
P

P

With these deﬁnitions in mind, we develop eﬃcient algorithms for approximately solving
the FS-WBP where the dependence on m, n and ε is competitive to state-of-the-art algo-
rithms [Kroshnin et al., 2019, Guminov et al., 2019].

2.3 Dual entropic regularized FS-WBP

Using the duality theory of convex optimization [Rockafellar, 1970], one dual form of entropic
regularized FS-WBP has been derived before [Cuturi and Doucet, 2014, Kroshnin et al., 2019].
Diﬀering from the usual 2-marginal or multimarginal OT [Cuturi and Peyr´e, 2018, Lin et al.,
2019a], the dual entropic regularized FS-WBP is a convex optimization problem with an aﬃne
constraint set. Formally, we have

min
λ,τ ∈Rmn

ϕold(λ, τ ) :=

m

Xk=1

ωk 


X1≤i,j≤n

eλki+τkj−η−1(Ck)ij

k uk
λ⊤

,



−

m

s.t.

ωkτk = 0n.

(5)

Xk=1



However, the objective function in Eq. (5) is not suﬃciently smooth because of the sum of
exponents. This makes the acceleration very challenging. In order to alleviate this issue, we
turn to derive another smooth dual form of entropic-regularized FS-WBP.
α1, α2, . . . , αm, β1, β2, . . . , βm−1} ⊆
{
Lagrangian function of the entropic regularized FS-WBP in Eq. (4) as follows:

By introducing the dual variables

Rn, we deﬁne the

(X1, . . . , Xm, α1, . . . , αm, β1, . . . , βm−1)
L
m

m

=

Xk=1

Ck, Xki −
ωk(
h

ηH(Xk))

α⊤

k (r(Xk)

uk)

−

−

−

Xk=1

m−1

Xk=1

(6)

β⊤
k (c(Xk+1)

c(Xk)).

−

0 can be neglected. In order to
By the deﬁnition of H(X), the nonnegative constraint X
derive the smooth dual objective function, we consider the following minimization problem:

≥

min
{(X1,...,Xm):kXkk1=1,∀k∈[m]} L

(X1, . . . , Xm, α1, . . . , αm, β1, . . . , βm−1).

5

In the above problem, the objective function is strongly convex. Thus, the optimal solution
is unique. For the simplicity, we let α = (α1, α2, . . . , αm)
∈
R(m−1)n and assume the convention β0 = βm = 0n. After the simple calculations, the optimal
solution ¯X(α, β) = ( ¯X1(α, β), . . . , ¯Xm(α, β)) has the following form:

Rmn and β = (β1, β2, . . . , βm−1)

∈

( ¯Xk(α, β))ij =

k (αki+βk−1,j −βkj)−(Ck)ij )

eη−1(ω−1
1≤i,j≤n eη−1(ω−1

k (αki+βk−1,j−βkj)−(Ck)ij )

for all k

[m],

∈

(7)

Plugging Eq. (7) into Eq. (6) yields that the dual form is:

P

max

α1,...,αm,β1,...,βm−1 


η

−

ωk log



X1≤i,j≤n

m

Xk=1

eη−1(ω−1

k (αki+βk−1,j−βkj)−(Ck)ij )

+



m

Xk=1

.

α⊤
k uk



In order to streamline our subsequent presentation, we perform a change of variables λk =
(ηωk)−1αk and τk = (ηωk)−1(βk−1 −
[m]. Recall that β0 = βm = 0n, we have

m
k=1 ωkτk = 0n. Putting these pieces together, we reformulate the problem as

βk) for all k







∈

P

m

min
λ,τ ∈Rmn

ϕ(λ, τ ) :=

ωk log



eλki+τkj −η−1(Ck)ij



−

m

ωkλ⊤

k uk,

s.t.

Xk=1

X1≤i,j≤n
To further simplify the above formulation, we use the notation Bk(λ, τ )
eλki+τkj −η−1(Ck)ij ) for all (i, j)
[n]
FS-WBP problem deﬁned by

Xk=1





×

∈

∈

ωkτk = 0n.

m

Xk=1

Rn×n by (Bk(λk, τk))ij =

[n]. To this end, we obtain the dual entropic-regularized

min
λ,τ ∈Rmn

m

ϕ(λ, τ ) :=

ωk

Xk=1

(cid:16)

Bk(λk, τk)
log(
k

k1)

−

k uk
λ⊤

,

s.t.

(cid:17)

m

Xk=1

ωkτk = 0n.

(8)

Remark 2.1. The ﬁrst part of the objective function ϕ is in the form of the logarithm of sum
of exponents while the second part is a linear function. This is diﬀerent from the objective
function used in previous dual entropic regularized OT problem in Eq. (5). We also note that
Eq. (8) is a special instance of a softmax minimization problem, and the objective function
ϕ is known to be smooth [Nesterov, 2005]. Finally, we point out that the same problem was
derived in the concurrent work by Guminov et al. [2019] and used for analyzing the accelerated
alternating minimization algorithm.

In the remainder of the paper, we also denote (λ⋆, τ ⋆)
of the dual entropic regularized FS-WBP problem in Eq. (8).

∈

Rmn

Rmn as an optimal solution

×

2.4 Properties of dual entropic regularized FS-WBP

In this section, we present several useful properties of the dual entropic regularized MOT in
Eq. (8). In particular, we show that there exists an optimal solution (λ⋆, τ ⋆) such that it has
an upper bound in terms of the ℓ∞-norm.
Lemma 2.2. For the dual entropic regularized FS-WBP, let ¯C = max1≤k≤m k
Ckk∞ and
¯u = min1≤k≤m,1≤j≤n ukj, there exists an optimal solution (λ⋆, τ ⋆) such that the following
ℓ∞-norm bound holds true:

where Rλ = 9η−1 ¯C + 2 log(n)

−

λ⋆
kk∞ ≤
k

τ ⋆
Rλ,
k k∞ ≤
k
log(¯u) and Rτ = 4η−1 ¯C.

Rτ ,

for all k

[m],

∈

6

Proof. First, we show that there exists m pairs of optimal solutions
each of (λj, τ j) satisﬁes that

(λj, τ j)
{

}j∈[m] such that

max
1≤i≤n

(τ j
k )i ≥

0

min
1≤i≤n

≥

(τ j

k )i,

for all k

= j.

(9)

[m], we let (

τ satisﬁes Eq. (9), the claim holds true for the ﬁxed j

τ ) be an optimal solution of the dual entropic regularized FS-WBP
[m]. Otherwise, we

λ,

∈

Fixing j
in Eq. (8). If
deﬁne m

∈

−

1 shift terms given by

b

b

b

∆

τk =

max1≤i≤n(

τk)i + min1≤i≤n(

τk)i

2

b

∈

b

R for all k

= j,

and let (λj, τ j) with
b

τ j
k =
τ j
j =

τk −
∆
τj + (
b
b

τk1n,
k6=j( ωk
ωj
b
P

λj
k =
τk)1n, λj
j =

)∆

Using this construction, we have (λj
This implies that Bk(

b
τk) = Bk(λk′

λk,

k)i + (τ j
k , τ k′

m

b
ωkτ j
b
k =

Xk=1

m

Xk=1

ωk

τk,

b

for all k

= j,

)∆

τk)1n.

τk)i′ for all i, i′
b

[n] and all k

∈

[m]. Furthermore, we have

[m].

∈

τk1n,
k6=j( ωk
ωj
b
P
λk)i + (

(

λk + ∆
λj −
b
b
k )i′ = (
k ) for all k
b
m
ωk(λj

∈

b
k)⊤uk =

Xk=1

m

Xk=1

ωk

k uk.
λ⊤

b

−

Putting these pieces together yields ϕ(λj, τ j) = ϕ(
and m
solution that satisﬁes Eq. (9) for the ﬁxed j
ﬁnd the desired pairs of optimal solutions
above argument m times.

τ ). Moreover, by the deﬁnition of (λj, τ j)
λ,
1 shift terms, τ j satisﬁes Eq. (9). Therefore, we conclude that (λj, τ j) is an optimal
b
[m]. Since j
[m] is chosen arbitarily, we can
b
∈
(λj, τ j)
}j∈[m] satisfying Eq. (9) by repeating the
{
Furthermore, each of (λj, τ j) must satisfy the optimality condition for Eq. (8) for all
Rn such that
[m]. Fixing j

[m], there exists z

∈

k

∈

∈

ωkτ j

k = 0n

and

m

Xk=1

∈
Bk(λj
Bk(λj
k

k, τ j
k )⊤1n
k, τ j
k1 −
k )

z = 0n

for all k

[m].

∈

(10)

,
By the deﬁnition of Bk(
·
τ j
Bk(λj
k = log(z) + log(
k

), we have
·

k, τ j
k )

k1)1n −

log(e−η−1Ck diag(eλj

k )1n) for all k

This together with the ﬁrst equality in Eq. (10) yields that

m

τ j
k =

ωl log(e−η−1Cldiag(eλj

l )1n)

log(e−η−1Ck diag(eλj

k )1n) for all k

−

For each i

∈

Xl=1
[n] and l

∈

η−1

Clk∞ + log(1⊤
k
Putting these pieces together yields

−

≤

[m], by the nonnegativity of each entry of Cl, we have
n eλj
l ).

[log(e−η−1Cldiag(eλj

n eλj
l )

log(1⊤

l )1n)]i ≤

[m].

∈

[m].

∈

max
1≤i≤n

(τ j
k )i −

min
1≤i≤n

(τ j
k )i ≤

η−1

Ckk∞ +
k

m

Xl=1

ωlη−1

Clk∞ for all k
k

∈

[m].

(11)

7

6
6
6
Combining Eq. (9) and Eq. (11) yields that

τ j
k k∞ ≤
k

η−1

Ckk∞ +
k

m

Xl=1

ωlη−1

Clk∞ for all k
k

= j.

(12)

Since

m

k=1 ωkτ j

k = 0n, we have

P
τ j
j k∞ ≤
k

ω−1
j

Xk6=j

ωkk

τ j
k k∞ ≤

(ηωj)−1

Xk6=j

ωkk

Ckk∞ + (ηωj)−1(1

−

ωj)

m

Xk=1

ωkk

Ckk∞.

Finally, we proceed to the key part and deﬁne the averaging iterate

m

m

λ⋆ =

ωjλj,

τ ⋆ =

ωjτ j.

Xj=1
Since ϕ is convex and (ω1, ω2, . . . , ωm)

Xj=1
∆m, we have ϕ(λ⋆, τ ⋆)

m
k=1 ωkτ ⋆

k = 0n. Since (λj, τ j) are optimal solutions for all j

∈

m
j=1 ωjϕ(λj, τ j) and
[m], we conclude that (λ⋆, τ ⋆)

≤

∈

P

is an optimal solution. Without loss of generality, we assume that (λ⋆, τ ⋆) satisﬁes that
P

max
1≤i≤n

(λ⋆

k)i ≥

0

≥

min
1≤i≤n

(λ⋆

k)i,

for all k

[m].

∈

(13)

Indeed, if λ⋆ does not satisfy the above condition, we deﬁne m shift terms given by

∆λ⋆

k =

and let

max1≤i≤n(λ⋆

k)i + min1≤i≤n(λ⋆

k)i

2

R for all k

m,

∈

∈

k = λ⋆
λ⋆

∆λ⋆

k1n,

for all k

k −

[m].

∈

The above operation will not change the value of ϕ(λ⋆, τ ⋆) such that (λ⋆, τ ⋆) is a desired
optimal solution which satisﬁes Eq. (13).
The remaining step is to show that

[m]. More

Rτ for all k

Rλ and

λ⋆
kk∞ ≤
k

τ ⋆
k k∞ ≤
k

∈

speciﬁcally, we have

m

Xj=1
η−1

η−1

τ ⋆
k k∞ ≤
k

≤

≤

≤

ωjk

τ j
k k∞ = ωkk

τ k
k k∞ +

ωlk

Clk∞ + η−1(1

−

Xl6=k
Ckk∞ + 3η−1
k

m

Xl=1

ωlk

Clk∞

4η−1 ¯C = Rτ .

Xj6=k

ωk)

ωjk

τ j
k k∞

m

Xl=1

ωlk

Clk∞ + η−1(1

Ckk∞ +
ωk)(
k

−

m

Xl=1

ωlk

Clk∞)

Since (λ⋆, τ ⋆) is an optimal solution, it satisﬁes the optimality condition for Eq. (8). Formally,
we have

uk = 0n

for all k

[m].

∈

(14)

,
By the deﬁnition of Bk(
·

k, τ ⋆
Bk(λ⋆
k )1n
k, τ ⋆
Bk(λ⋆
k )
k
), we have
·
Bk(λ⋆
k = log(uk) + log(
λ⋆
k

k, τ ⋆
k )

k1 −

k1)1n −

log(e−η−1Ck diag(eτ ⋆

k )1n) for all k

[m].

∈

8

6
This implies that

max
1≤i≤n
min
1≤i≤n

(λ⋆

(λ⋆

k)i ≤
k)i ≥

η−1

Bk(λ⋆
τ ⋆
Ckk∞ + log(n) +
k k∞ + log(
k
k
k
k, τ ⋆
Bk(λ⋆
τ ⋆
k )
k k∞ + log(
log(n)
k

k, τ ⋆
k )
k1).

− k

−

k1),

log(¯u)

Therefore, we have

max
1≤i≤n

(λ⋆

k)i −

min
1≤i≤n

(λ⋆

k)i ≤

η−1

Ckk∞ + 2 log(n)
k

−

τ ⋆
k k∞.
log(¯u) + 2
k

Together with Eq. (13), we conclude that

λ⋆
kk∞ ≤
k

η−1

Ckk∞ + 2 log(n)
k

−

τ ⋆
k k∞.
log(¯u) + 2
k

This completes the proof.

(cid:3)

Remark 2.3. Lemma 2.2 is analogous to [Lin et al., 2019b, Lemma 3.2] for the OT problem.
However, the dual entropic-regularized FS-WBP is more complex and requires a novel construc-
m
tive iterate, (λ⋆, τ ⋆) =
j=1 ωj(λj, τ j). Moreover, the techniques in Kroshnin et al. [2019] are
not applicable for the analysis of the FastIBP algorithm, and, accordingly, Lemma 2.2 is cru-
cial for the analysis.

P

The upper bound for the ℓ∞-norm of the optimal solution of dual entropic-regularized

FS-WBP in Lemma 2.2 leads to the following straightforward consequence.

Corollary 2.4. For the dual entropic regularized FS-WBP, there exists an optimal solution
(λ⋆, τ ⋆) such that for all k

[m],

∈

λ⋆
kk ≤
k
where Rλ, Rτ > 0 are deﬁned in Lemma 2.2.

√nRλ,

τ ⋆
k k ≤
k

√nRτ ,

for all k

[m],

∈

Finally, we observe that ϕ can be decomposed into the weighted sum of m functions and
prove that each component function ϕk has Lipschitz continuous gradient with the constant
4 in the following lemma.

Lemma 2.5. The following statement holds true, ϕ(λ, τ ) =
k uk for all k
log(1⊤
λ⊤
∈
dient in ℓ2-norm and the Lipschitz constant is upper bounded by 4. Formally,

m
k=1 ϕk(λk, τk) where ϕk(λk, τk) =
[m]. Moreover, each ϕk has Lipschitz continuous gra-

n Bk(λk, τk)1n)

P

−

ϕk(λ, τ )

k∇

− ∇

ϕk(λ′, τ ′)

k ≤

which is equivalent to

λ
τ

(cid:19)

−

(cid:18)

λ′
τ ′

4

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ϕ(λ′, τ ′)

ϕ(λ, τ )

−

⊤

λ′
τ ′

λ
τ

−
−

(cid:19)

∇

≤

(cid:18)

ϕ(λ, τ ) + 2

ωk

Xk=1

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
m

for all k

[m].

∈

λ′
k −
τ ′
k −

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

.

!

λk
τk (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Proof. The ﬁrst statement directly follows from the deﬁnition of ϕ in Eq. (8). For the second
statement, we provide the explicit form of the gradient of ϕk as follows,

ϕk(λ, τ ) =

∇

uk

Bk(λ,τ )1n
1⊤
n Bk(λ,τ )1n −
Bk(λ,τ )⊤1n
1⊤
n Bk(λ,τ )1n

.









9

 
Now we construct the following entropic regularized OT problem,

min
X:kXk1=1h

Ck, X

i −

ηH(X),

s.t.X1n = (1/n)1n, X ⊤1n = (1/n)1n.

Since the function
simplex Q
linearly constrained convex optimization problem:

H(X) is strongly convex with respect to the ℓ1-norm on the probability
, the above entropic regularized OT problem is a special case of the following

Rn2

−

⊆

min
x∈Q

f (x),

s.t. Ax = b,

where f is strongly convex with respect to the ℓ1-norm on the set Q. We use the ℓ2-norm for
the dual space of the Lagrange multipliers. By Nesterov [2005, Theorem 1] and the fact that
k1→2 = 2, the dual objective function ˜ϕk satisﬁes the following inequality:
A
k

˜ϕk(α, β)

k∇

− ∇

˜ϕk(α′, β′)

k ≤

Recall that the function ˜ϕk is given by

α
β

(cid:19)

−

α′
β′

(cid:18)

4
η

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜ϕk(α, β) = η log



(Ck )ij −αi−βj
η

−1

e−



α, u

β, v

+ η.

i

i − h

− h

n

Xi,j=1



,
This together with the deﬁnition of Bk(
·


) implies that
·

Bk(η−1α−(1/2)1n,η−1β−(1/2)1n)1n
1⊤
n Bk(η−1α−(1/2)1n,η−1β−(1/2)1n)1n −
Bk(η−1α−(1/2)1n,η−1β−(1/2)1n)⊤
1⊤
n Bk(η−1α−(1/2)1n,η−1β−(1/2)1n)1n −

1n

u



v

˜ϕk(α, β) = 

∇




Performing the change of variable λ = η−1α
τ ′), we have

(1/2)1n and τ = η−1β

−

−



(1/2)1n (resp. λ′ and

ϕk(λ′, τ ′)
ϕk(λ, τ )
k
˜ϕk(η(λ + (1/2)1n), η(τ + (1/2)1n))

− ∇

k∇

k∇
4

=

≤

λ′
τ ′

λ
τ

−
−

.

(cid:18)
This completes the proof.

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜ϕk(η(λ′ + (1/2)1n), η(τ ′ + (1/2)1n))
k

− ∇

(cid:3)

Remark 2.6. It is worthy noting that Lemma 2.5 exploits the decomposable structure of
the dual function ϕ, and gives the a weighted smoothness inequality. This inequality is
necessary for deriving the complexity bound which depends linearly on the number of probability
measures.

3 Computational Hardness

In this section, we analyze the computational hardness of the FS-WBP in Eq. (3). After
introducing some characterization theorems in combinatorial optimization, we show that the
FS-WBP in Eq. (3) is a minimum-cost ﬂow (MCF) problem when m = 2 and n
3 but not
when m

3 and n

≥

3.

≥

≥

10

3.1 Combinatorial techniques

We present some classical results in combinatorial optimization and graph theory, including
Ghouila-Houri’s celebrated characterization theorem [Ghouila-Houri, 1962].

Deﬁnition 3.1. A totally unimodular (TU) matrix is one for which every square submatrix
has determinant

1, 0 or 1.

−

Proposition 3.1 (Ghouila-Houri). A
if for each I
for j

-valued matrix A
1, 0, 1
}
I2 so that
[m] there is a partition I = I1 ∪

[n].

{−

i∈I1

⊆

Rm×n is TU if and only
1, 0, 1
aij ∈ {−
}

i∈I2

∈
aij −

P

P

∈

The second result [Berge, 2001, Theorem 1, Chapter 15] shows that the incidence matrices

of directed graphs and 2-colorable undirected graphs are TU.

Proposition 3.2. Let A be a
-valued matrix. Then A is TU if each column contains
1, 0, 1
}
at most two nonzero entries and all rows are partitioned into two sets I1 and I2 such that: If
two nonzero entries of a column have the same sign, they are in diﬀerent sets. If these two
entries have diﬀerent signs, they are in the same set.

{−

Finally, we characterize the constraint matrix arising in a MCF problem.

Deﬁnition 3.2. The MCF problem ﬁnds the cheapest possible way of sending a certain
amount of ﬂow through a ﬂow network. Formally,

min
s.t.

f (u, v)
P
≥
f (u, v)
≤
f (u, v) =

(u,v)∈E f (u, v)

·

a(u, v)
0,
E,
for all (u, v)
c(u, v) for all (u, v)

∈

∈
f (v, u) for all (u, v)
(u,w)∈E or (w,u)∈E f (u, w) = 0,
w∈V f (s, w) = d and

−

E,

E,

∈

w∈V f (w, t) = d.

P
P

V , where each edge (u, v)

V and
The ﬂow network G = (V, E) is a directed graph G = (V, E) with a source vertex s
0
a sink vertex t
∈
and cost a(u, v), with most MCF algorithms supporting edges with negative costs. The cost of
sending this ﬂow along an edge (u, v) is f (u, v)
a(u, v). The problem requires an amount of
ﬂow d to be sent from source s to sink t. The deﬁnition of the problem is to minimize the
total cost of the ﬂow over all edges.

E has capacity c(u, v) > 0, ﬂow f (u, v)

≥

∈

∈

·

P

Proposition 3.3. The constraint matrix arising in a MCF problem is TU and its rows are
categorized into a single set using Proposition 3.2.

Proof. The standard LP representation of the MCF problem is

min
x∈R|E|

c⊤x,

s.t. Ax = b, l

x

≤

≤

u.

R|E| with xj being the ﬂow through arc j, b

R|V | with bi being external supply
where x
at node i and 1⊤b = 0, cj is unit cost of ﬂow through arc j, lj and uj are lower and upper
R|V |×|E| is the arc-node incidence matrix with entries
bounds on ﬂow through arc j and A

∈

∈

∈
1 if arc j starts at node i
1 if arc j ends at node i
0 otherwise

−

.

Aij =






11

u11

u12

u13

u14

0

0

0

0

u21

u22

u23

u24

Figure 1: Represent the FS-WBP in Eq. (3) as a MCF problem when (m, n) = (2, 4).

Since each arc has two endpoints, the constraint matrix A is a
which each column contains two nonzero entries 1 and
that A is TU and the rows of A are categorized into a single set.

-valued matrix in
1, 0, 1
}
1. Using Proposition 3.2, we obtain
(cid:3)

{−

−

3.2 Main result

We present our main results on the computational hardness of the FS-WBP in Eq. (3). First,
the FS-WBP in this LP form is an MCF problem when m = 2 and n
2; see Figure 1 for
the graph when (m, n) = (2, 4). Indeed, it is a transportation problem with n warehouses,
n transshipment centers and n retail outlets. Each u1i is the amount of supply provided by
the ith warehouse and each u2j is the amount of demand requested by the jth retail outlet.
(X1)ij is the ﬂow sent from ith warehouse to the jth transshipment center and (X2)ij is the
ﬂow sent from the ith transshipment center to the jth retail outlet. (C1)ij and (C2)ij refer to
the unit cost of the corresponding ﬂow. See [Anderes et al., 2016, Page 400].

≥

Proceed to the setting m

3, it is unclear whether the graph representation of the FS-
WBP carries over. Instead, we turn to algebraic techniques and provide an explicit form as
follows:

≥

m

min

,
Ck, Xki
h
Xk=1

s.t.

E
−
...
...
...
...

G
...
...
























(
−

· · ·
E
. . .
. . .
. . .

G

−

· · ·
. . .
. . .
. . .
. . .
. . .

G G
−
. . .
. . .

· · ·
. . .
. . .
1)m−1E
. . .
. . .
. . .
. . .
1)mG (

(
−

(
−

· · ·
...
...
...
1)mE
...
...
...
1)m+1G
























· · ·

· · ·

· · ·

−

vec (X1)
vec (X2)
...
...
...
...
vec (Xm)

































=

u1
−
u2
...
1)m−1um−1
(
−
1)mum
(
−
0n
...
0n

















,

















(15)

12

where E = In ⊗
arising in Eq. (15) has either 2 or 3 nonzero entries in
we study the structure of the constraint matrix when m

. Each column of the constraint matrix
1, 0, 1
. In the following theorem,
}
3 and n = 2.

In ∈

n ⊗

and G = 1⊤

1⊤
n ∈

Rn×n2

Rn×n2

{−
≥

Theorem 3.4. The constraint matrix arising in Eq. (15) is TU when m

3 and n = 2.

R(4m−2)×4m is a

Proof. When n = 2, the constraint matrix A has E = I2 ⊗
I2. The matrix
-valued matrix with several redundant rows and each column
A
1, 0, 1
}
. Now we simplify the matrix A by removing a
1, 0, 1
has at most three nonzero entries in
}
speciﬁc set of redundant rows. In particular, we observe that

2 ⊗

{−

{−

∈

≥
2 and G = 1⊤
1⊤

i∈{1,2,3,4,2m+1,2m+2}
X
which implies that the (2m + 2)th row is redundant. Similarly, we have

aij = 0,

j

∀

∈

[4m],

i∈{3,4,5,6,2m+3,2m+4}
X

aij = 0,

j

∀

∈

[4m],

which implies that the (2m + 3)th row is redundant. Using this argument, we remove m
rows from the last 2m
∈
such that each column has only two nonzero entries 1 and
m is odd:

1
R(3m−1)×4m has very nice structure
1; see the following matrix when

2 rows. The resulting matrix ¯A

−

−

−

¯A =

E
−
...
...
...
...

e1 −
−

1⊤
2 ⊗
...
...

· · ·

· · ·
E
. . .
. . .
. . .
1⊤
2 ⊗
1⊤
2 ⊗
. . .

· · ·























· · ·
. . .
. . .
. . .
. . .
. . .

e1
e2 1⊤

e2

2 ⊗
. . .

(
−

· · ·
. . .
. . .
1)m−1E
. . .
. . .
. . .
. . .
1)m1⊤

2 ⊗

· · ·
...
...
...
1)mE
...
...
...
1)m+11⊤

(
−

2 ⊗

e2

(
−























.

· · ·

(
−

e2

where e1 and e2 are respectively the ﬁrst and second standard basis (row) vectors in R2. Fur-
thermore, the rows of ¯A are categorized into a single set so that the criterion in Proposition 3.2
holds true (the dashed line in the formulation of ¯A serves as a partition of this single set into
two sets). Using Proposition 3.2, we conclude that ¯A is TU.
(cid:3)

To facilitate the reader, we provide an illustrative counterexample for showing that the

FS-WBP in Eq. (15) is not an MCF problem when m = 3 and n = 3.

Example 3.1. When m = 3 and n = 3, the constraint matrix is

A =









−

1⊤
3

I3 ⊗
03×9
03×9

1⊤
3 ⊗
03×9

I3 −
−

1⊤
3

03×9
I3 ⊗
03×9
1⊤
3 ⊗
1⊤
3 ⊗

I3
I3

13

03×9
03×9
I3 ⊗
03×9
1⊤
3 ⊗

−

1⊤
3

I3

.









1, 4, 7, 10, 11, 13, 15
Setting the set I =
}
{
third standard basis row vectors in Rn, the resulting matrix with the rows in I is

and letting e1, e2 and e3 be the ﬁrst, second and

−

1⊤
3

e1 ⊗
01×9
01×9

1⊤
3 ⊗
1⊤
3 ⊗
01×9
01×9

e1 −
e2 −
−
−

1⊤
3

01×9
e1 ⊗
01×9
1⊤
3 ⊗
1⊤
3 ⊗
1⊤
3 ⊗
1⊤
3 ⊗

e1
e2
e1
e3

R =













01×9
01×9
e1 ⊗
01×9
01×9

−

1⊤
3

1⊤
3 ⊗
1⊤
3 ⊗

e1
e3

.













Instead of considering all columns of R, it suﬃces to show that no partition of I guarantees
for any j

1, 2, 11, 12, 13, 19, 21

that

∈ {

}

Rij −

Xi∈I1

Xi∈I2

Rij ∈ {−

.
1, 0, 1
}

We write the submatrix of R with these columns as

¯R =

1
−
0
0
1
0
0
0













1
−
0
0
0
1
0
0

0
1
0
0
1
−
0
0

0
1
0
0
0
0
1

−

0
0
0
1
−
0
1
−
0

0
0
1
−
0
0
1
0

0
0
1
−
0
0
0
1













First, we claim that rows 1, 2, 4, 5 and 7 are in the same set I1. Indeed, columns 1 and 2
imply that rows 1, 4 and 5 are in the same set. Column 3 and 4 imply that rows 2, 5 and 7
are in the same set. Putting these pieces together yields the desired claim. Then we consider
the set that the row 6 belongs to and claim a contradiction. Indeed, row 6 can not be in I1
since column 5 implies that rows 4 and 6 are not in the same set. However, row 6 must be
in I1 since columns 6 and 7 imply that rows 3, 6 and 7 are in the same set. Putting these
pieces together yields the desired contradiction. Finally, by using Propositions 3.1 and 3.3, we
conclude that A is not TU and problem (15) is not a MCF problem when m = 3 and n = 3.

Finally, we prove that the FS-WBP in Eq. (15) is not a MCF when m
≥
The basic idea is to extend the construction in Example 3.1 to the general case.

3 and n

3.

≥

Theorem 3.5. The FS-WBP in Eq. (15) is not a MCF problem when m

3 and n

3.

≥

≥

Proof. We use the proof by contradiction. In particular, assume that problem (15) is a MCF
3, Proposition 3.3 implies that the constraint matrix A is
3 and n
problem when m
-valued matrix, Proposition 3.1 further implies that for each set
1, 0, 1
TU. Since A is a
}
I

n] there is a partition I1, I2 of I such that

≥
{−

[2mn

≥

⊆

−

aij −

Xi∈I1

Xi∈I2

aij ∈ {−

,
1, 0, 1
}

j

∀

∈

[mn2].

(16)

In what follows, for any given m
3, we construct a set of rows I such that no
≥
partition of I guarantees that Eq. (16) holds true. For the ease of presentation, we rewrite

3 and n

≥

14

the matrix A

∈

R(2mn−n)×mn2

as follows,

−

1⊤
n

In ⊗
...
...
...
...

In −
−

1⊤
n ⊗
...
...

· · ·

1⊤
n

· · ·
In ⊗
. . .
. . .
. . .
1⊤
n ⊗
1⊤
n ⊗
. . .

A =























· · ·
. . .
. . .
. . .
. . .
. . .

(
−

In

1⊤
n

· · ·
. . .
. . .
1)m−1In ⊗
. . .
. . .
. . .
. . .
1)m1⊤

In
In 1⊤

n ⊗
. . .

· · ·
...
...
...
1)mIn ⊗
...
...
...
1)m+11⊤

(
−

1⊤
n

.























· · ·

· · ·

(
−

n ⊗

In

(
−

In

n ⊗

and letting e1, e2 and e3
Setting the set I =
be the ﬁrst, second and third standard basis row vectors in Rn, the resulting matrix with the
rows in I is

1, n + 1, 2n + 1, 3n + 1, 3n + 2, 4n + 1, 4n + 3
}
{

1⊤
n

−

e1 ⊗
01×n2
01×n2
1⊤
n ⊗
1⊤
n ⊗
01×n2
01×n2

e1 −
e2 −
−
−

01×n2
1⊤
e1 ⊗
n
01×n2
1⊤
n ⊗
1⊤
n ⊗
1⊤
n ⊗
1⊤
n ⊗

e1
e2
e1
e3

R =













−

01×n2
01×n2
e1 ⊗
01×n2
01×n2
1⊤
e1
n ⊗
1⊤
e3
n ⊗

01×n2
01×n2
1⊤
n 01×n2
01×n2
01×n2
01×n2
01×n2

· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·

01×n2
01×n2
01×n2
01×n2
01×n2
01×n2
01×n2

.













Instead of considering all columns of R, it suﬃces to show that no partition of I guarantees

Rij −

Rij ∈ {−

,
1, 0, 1
}

Xi∈I1
1, 2, n2 + 2, n2 + 3, n2 + n + 1, 2n2 + 1, 2n2 + 3
. We write the submatrix of R with
}

Xi∈I2

for all j
these columns as

∈ {

¯R =

1
−
0
0
1
0
0
0













1
−
0
0
0
1
0
0

0
1
0
0
1
−
0
0

0
1
0
0
0
0
1

−

0
0
0
1
−
0
1
−
0

0
0
1
−
0
0
1
0

.

0
0

1
−
0


0



0


1



Applying the same argument used in Example 3.1, we obtain from Propositions 3.1 and 3.3
3, which is a contradiction. As a consequence, the
that A is not TU when m
(cid:3)
conclusion of the theorem follows.

3 and n

≥

≥

Remark 3.6. Theorem 3.5 resolves an open question and partially explains why the direct
application of network ﬂow algorithms to the FS-WBP in Eq. (15) is ineﬃcient. However, this
does not eliminate the possibility that the FS-WBP is equivalent to some other LP with good
complexity. For example, Ge et al. [2019] have recently successfully identiﬁed an equivalent

15

Algorithm 1: FastIBP(
Ck, uk
{

}k∈[m], ε)

Initialization: t = 0, θ0 = 1 and ˇλ0 = ˜λ0 = ˇτ 0 = ˜τ 0 = 0mn.
while Et > ε do

Step 1: Compute
−
Step 2: Compute rk = r(Bk(¯λt
k, ¯τ t

= (1

θt)

+ θt
(cid:18)
k)) and ck = c(Bk(¯λt

.
(cid:19)

(cid:19)

(cid:18)

(cid:18)

(cid:19)

ˇλt
ˇτ t

˜λt
˜τ t

¯λt
¯τ t

k, ¯τ t

k)) for all k

[m] and perform

∈

˜λt+1
k

= ˜λt

˜τ t+1 =

rk
1⊤
n rk −
m

k −

1
4θt (cid:18)
argmin
k=1 ωkτk=0n

Pm

ωk

(cid:19)
(τk −

(cid:20)

uk

,

for all k

[m],

∈

k)⊤ ck
¯τ t
1⊤
n ck

+ 2θtk

τk −

2

˜τ t
kk

.

(cid:21)

Step 3: Compute

Step 4: Compute

=

(cid:19)

(cid:18)

¯λt
¯τ t

(cid:19)

+ θt

= argmin

(cid:18)

λt
τ t
´λt
b
´τ t
b

(cid:19)

(cid:18)
ϕ(λ, τ )

−
λ
τ

θt

˜λt
˜τ t

.
(cid:19)

(cid:18)

Xk=1
˜λt+1
˜τ t+1

(cid:18)

(cid:19)

k +

k = ´τ t

(cid:26)
(cid:18)
Step 5a: Compute ck = c(Bk(´λt
k, ´τ t
k)) for all k
m
Step 5b: Compute `τ t
k=1 ωk log(ck)
Step 6a: Compute rk = r(Bk(`λt
k, `τ t
k)) for all k
P
Step 6b: Compute λt
k + log(uk)
Step 7a: Compute ck = c(Bk(λt
Step 7b: Compute ˇτ t+1
k = τ t
k +
θ2
Step 8: Compute θt+1 = θt(
t + 4
P
Step 9: Increment by t = t + 1.
p
2), . . . , Bm(λt

k, τ t
k)) for all k
∈
m
k=1 ωk log(ck)
θt)/2.

1), B2(λt

k = `λt

m, τ t

1, τ t

2, τ t

m)).

end while
Output: (B1(λt

−

−

,

|

ˇλt
ˇτ t

(cid:19)

(cid:26)(cid:18)

λt
τ t
∈
b
[m].
b
log(ck) for all k
[m].

(cid:19)
∈
−
∈
log(rk) for all k

(cid:18)

.
(cid:19)(cid:27)(cid:27)

[m] and `λt+1 = ´λt.

∈

[m] and τ t = `τ t.

∈

[m].

log(ck) for all k

−

∈

[m] and ˇλt+1 = λt.

LP formulation of the FS-WBP which is suitable for the interior-point algorithm. Further-
more, our results support the problem reformulation of the FS-WBP which forms the basis
for various algorithms; e.g., Benamou et al. [2015], Cuturi and Peyr´e [2016], Kroshnin et al.
[2019], Ge et al. [2019], Guminov et al. [2019].

4 Fast Iterative Bregman Projection

In this section, we present a fast deterministic variant of the iterative Bregman projection
(IBP) algorithm, named FastIBP algorithm, and prove that it achieves the complexity bound
O(mn2ε−2) from iterative Bregman projection algo-
of
O(mn5/2ε−1) from the APDAGD and acceler-
rithm [Benamou et al., 2015] in terms of ε and
ated Sinkhorn algorithms [Kroshnin et al., 2019, Guminov et al., 2019] in terms of n.

O(mn7/3ε−4/3). This improves over

e

e

4.1 Algorithmic scheme

e

To facilitate the later discussion, we present the FastIBP algorithm in pseudocode form in
Algorithm 1 and its application to entropic regularized FS-WBP in Algorithm 2. Note that
m)) stand for the primal variables while (λt, τ t) are the dual variables
(B1(λt
for the entropic regularized FS-WBP.

1), . . . , Bm(λt

m, τ t

1, τ t

The FastIBP algorithm is a deterministic variant of the iterative Bregman projection

16

(IBP) algorithm [Benamou et al., 2015]. While the IBP algorithm can be interpreted as a
dual coordinate descent, the acceleration achieved by the FastIBP algorithm mostly depends
on the reﬁned characterization of per-iteration progress using the scheme with momentum;
see Step 1-3 and Step 8. To the best of our knowledge, this scheme has been well stud-
ied by [Nesterov, 2012, 2013, Fercoq and Richt´arik, 2015, Nesterov and Stich, 2017] yet ﬁrst
introduced to accelerate the optimal transport algorithms.

ϕ(ˇλt, ˇτ t)
{
k, τ t

k = `τ t

}t≥0 is monotonically decreasing and Step
Furthermore, Step 4 guarantees that
k) to (ˇλt+1, ˇτ t+1). Step 5 are performed such
7 ensures the suﬃcient large progress from (λt
that τ t
k satisﬁes the bounded diﬀerence property: max1≤i≤n(τ t
k)i ≤
Rτ /2
while Step 6 guarantees that the marginal conditions hold true: r(Bk(λt
k)) = uk for all
[m]. We see from Guminov et al. [2019, Lemma 9] that Step 5-7 refer to the alternating
k
minimization steps for the dual objective function ϕ with respect to two-block variable (λ, τ ).
More speciﬁcally, these steps can be rewritten as follows,

min1≤i≤n(τ t
k, τ t

k)i −

∈

(Step 5) `λt = ´λt,
(Step 6) λt = argmin ϕ(λ, `τ t),
(Step 7) ˇλt+1 = λt,

`τ t = argmin ϕ(´λt, τ ),
τ t = `τ t,
ˇτ t+1 = argmin ϕ(λt, τ ),

s.t.

m
k=1 τk = 0n,

P

s.t.

m
k=1 τk = 0n.

We also remark that Step 4-7 are specialized to the FS-WBP in Eq. (3) and have not been
appeared in the coordinate descent literature before.

P

Finally, the optimality conditions of primal entropic regularized WBP in Eq. (4) and dual

entropic regularized WBP in Eq. (8) are

0n = r(Bk(λk,τk))
kBk(λk,τk)k1 −
0n = c(Bk(λk,τk))
kBk(λk,τk)k1 −
m
0n =
k=1 ωkτk.

uk,
m
i=1 ωi

P

c(Bi(λi,τi))
kBi(λi,τi)k1

,

for all k

for all k

[m],

[m],

∈

∈

P

m
Since the FastIBP algorithm guarantees that
∆n
k=1 ωkτ t
for all k
[m], we can solve simultaneously primal and dual entropic regularized FS-WBP
with an adaptive stopping criterion which does not require to calculate any objective value.
The criterion depends on the following quantity to measure the residue at each iteration:

k = 0n and r(Bk(λt

k)) = uk

k, τ t

P

∈

∈

Et :=

m

Xk=1

c(Bk(λt

k, τ t

k))

ωkk

m

−

Xi=1

ωic(Bi(λt

i, τ t

i ))

k1.

(17)

For the existing algorithms, e.g., accelerated IBP and APDAGD, they are developed based
on the primal-dual optimization framework which allows for achieving better dependence on
1/ε than FastIBP by directly optimizing Et. In contrast, the FastIBP algorithm indirectly
optimizes Et through the dual objective gap and the scheme with momentum (cf. Step 1-3
and Step 8), which can lead to better dependence on n.

Remark 4.1. We provide some comments on the FastIBP algorithm. First, each iteration
updates O(mn2) entries which is similar to the IBP algorithm. Updating ˜λ and ˇλ can be eﬃ-
ciently implemented in distributed manner and each of m machine updates O(n2) entries at
each iteration. Second, the computation of 4m marginals can be performed using implemen-
tation tricks. Indeed, this can be done eﬀectively by using r(e−η−1Ck ) and c(e−η−1Ck ) for all
k

[m], which are computed and stored at the beginning of the algorithm.

∈

Remark 4.2. First, we notice that (
X2, . . . ,
uk
}k∈[m] and an ε-approximate barycenter
transportation plans between m measures
{
b
b

Xm) are one set of approximate optimal
u. These

X1,

b

17

b

Algorithm 2: Finding a Wasserstein barycenter by the FastIBP algorithm

Input: η = ε/(4 log(n)) and ¯ε = ε/(4 max1≤k≤m k
Ckk
Step 1: Compute (˜u1, . . . , ˜um) = (1
Xm) = FastIBP(
Ck, ˜uk
Step 2: Compute (
{
Step 3: Round (
Xm) using Kroshnin et al. [2019, Algorithm 4]
X2, . . . ,
X1,
X2, . . . ,
X1,
e
e
Xm) is feasible to the FS-WBP in Eq. (3).
such that (
X2, . . . ,
e
e
b
Step 4: Compute
u =
Output:

¯ε/4)(u1, . . . , um) + (¯ε/4n)(1n, . . . , 1n).

Xm) to (
e
X ⊤

}k∈[m], ¯ε/2).

m
e
k=1 ωk

X2, . . . ,

k 1n

X1,

X1,

∞).

b
u.

−

b

b

b

b
P

b

b

b

matrices are equivalent to those constructed by using [Altschuler et al., 2017, Algorithm 2]. We
also remark that the approximate barycenter
see [Kroshnin et al., 2019, Section 2.2] for the details.

u can be constructed by only using (

X2, . . . ,

X1,

Xm);

b

e

e

e

4.2 Convergence analysis

We present several technical lemmas which are important to analyzing the FastIBP algorithm.
The ﬁrst lemma provides the inductive formula and the upper bound for θt.

θt}t≥0 be the iterates generated by the FastIBP algorithm. Then we have
{
2/(t + 2) and θ−2
0.

θt+1)θ−2

t+1 for all t

Lemma 4.3. Let
0 < θt ≤
Proof. By the deﬁnition of θt, we have

t = (1

−

≥

2

θt+1
θt (cid:19)

(cid:18)

=

1
4

(cid:18)q

θ2
t + 4

θt

−

2

(cid:19)

= 1 +

θt
2

θt −

(cid:18)

q

θ2
t + 4

= 1

(cid:19)

θt+1,

−

which implies the desired inductive formula and θt > 0 for all t
prove that 0 < θt ≤
holds when t = 0 as θ0 = 1. Assume that the hypothesis holds for t
we have

0. Then we proceed to
≥
0 using the induction. Indeed, the claim trivially
2/(t0 + 2),

t0, i.e., θt0 ≤

2/(t + 2) for all t

≤

≥

θt0+1 =

1 +

2
1 + 4
θ2
t0

2
t0 + 3

.

≤

This completes the proof of the lemma.

q

(cid:3)

The second lemma shows that all the iterates generated by the FastIBP algorithm are

feasible to the dual entropic regularized FS-WBP for all t

1.

≥
}t≥0,

Lemma 4.4. Let
(λt, τ t)
{
m

m

(´λt, ´τ t)
}t≥0,
{
}t≥0 be the iterates generated by the FastIBP algorithm. Then, we have

(ˇλt, ˇτ t)
{

(˜λt, ˜τ t)
{

(¯λt, ¯τ t)
{

λt,
(
{

}t≥0,

}t≥0,

τ t)

}t≥0, and

m

m

m

m

b

b

m

ωk ˇτ t

k =

ωk ˜τ t

k =

ωk ¯τ t

k =

ωk

τ t
k =

ωk ´τ t

k =

ωk `τ t

k =

ωkτ t

k = 0n

for all t

Xk=1
Proof. We ﬁrst verify Lemma 4.4 when t = 0. Indeed,

Xk=1

Xk=1

Xk=1

Xk=1

b

Xk=1

Xk=1

0.

≥

m

Xk=1

m

ωk ˇτ 0

k =

ωk ˜τ 0

k = 0n.

Xk=1

18

By the deﬁnition, ¯τ 0 is a convex combination of ˇτ 0 and ˜τ 0 and
¯τ 0, ˜τ 1 and ˜τ 0. Thus, we have

τ 0 is a linear combination of

m

m

ωk ¯τ 0

k =

ωk

τ 0
k = 0n.

b

This also implies that

Xk=1

Xk=1
k = 0n. Using the update formula for `τ 0, τ 0 and ˇτ 1, we have

b

m
k=1 ωk ´τ 0

P

m

m

m

ωk `τ 0

k =

ωkτ 0

k =

ωk ˇτ 1

k = 0n.

Besides that, the update formula for ˜τ 1 implies
we obtain the desired equality in the conclusion of Lemma 4.4 for all t

k = 0n. Repeating this argument,
(cid:3)
0.

Xk=1

Xk=1

Xk=1
m
k=1 ωk ˜τ 1

The third lemma shows that the iterates

satisﬁes the bounded diﬀerence property: max1≤i≤n(τ t

τ t
{

≥
P
}t≥0 generated by the FastIBP algorithm
k)i ≤

min1≤i≤n(τ t

k)i −

Rτ /2.

Lemma 4.5. Let
following statement holds true:

(λt, τ t)
{

}t≥0 be the iterates generated by the FastIBP algorithm. Then the

max
1≤i≤n

(τ t
k)i −

min
1≤i≤n

(τ t
k)i ≤

Rτ /2,

where Rτ > 0 is deﬁned in Lemma 2.2.

Proof. We observe that τ t

k = `τ t

k for all k

[m]. By the update formula for `τ t

k, we have

∈

m

k = ´τ t
`τ t

k +

ωi log(ci)

m

log(ck) =

Xi=1

Xi=1
After the simple calculation, we have

−

´λt
ωi log(e−η−1Cidiag(e
i )1n)

log(e−η−1Ck diag(e

´λt
k )1n).

−

Ckk∞ + 1⊤
n e
k
Therefore, the following inequality holds true for all k

η−1

≤

−

log(e−η−1Ck diag(e

´λt
k

´λt
k )1n)]j ≤

1⊤
n e

´λt
k .

[m],

∈

max
1≤i≤n

(τ t
k)i −

min
1≤i≤n

(τ t
k)i ≤

η−1

Ckk∞ + η−1
k

m

Xi=1

ωik

Cik∞

!

= 2η−1( max

1≤k≤m k

Ckk∞).

This together with the deﬁnition of Rτ yields the desired inequality.

(cid:3)

The ﬁnal lemma presents a key descent inequality for the FastIBP algorithm.

Lemma 4.6. Let
(λ⋆, τ ⋆) be an optimal solution in Lemma 2.2. Then the following statement holds true:

}t≥0 be the iterates generated by the FastIBP algorithm and let

(ˇλt, ˇτ t)
{

ϕ(ˇλt+1, ˇτ t+1)

−

(1

−

θt)ϕ(ˇλt, ˇτ t)

−

θtϕ(λ⋆, τ ⋆)

2θ2
t

≤

19

ωk

m

Xk=1

λ⋆
k −
τ ⋆
k −

2

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

 (cid:13)
(cid:13)
(cid:13)
(cid:13)

λ⋆
k −
τ ⋆
k −

2

˜λt+1
k
˜τ t+1
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

!!

 
 
Proof. Using Lemma 2.5 with (λ′, τ ′) = (

λt+1,

τ t+1) and (λ, τ ) = (¯λt, ¯τ t), we have

ϕ(

λt+1,

τ t+1)

ϕ(¯λt, ¯τ t) + θt

≤

˜λt+1
˜τ t+1

˜λt
b
˜τ t

−
−

(cid:18)

⊤

(cid:19)

b
∇

ϕ(¯λt, ¯τ t) + 2θ2
t

b

b

After some simple calculations, we ﬁnd that

ωk

m

Xk=1

˜λt+1
k −
˜τ t+1
k −

.

!

2

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ϕ(¯λt, ¯τ t) = (1

˜λt+1
˜τ t+1

⊤

˜λt
˜τ t

−
−

(cid:19)

(cid:18)

∇

ϕ(¯λt, ¯τ t) =

−

θt)ϕ(¯λt, ¯τ t) + θtϕ(¯λt, ¯τ t),
−
˜λt
˜τ t

ϕ(¯λt, ¯τ t) +

¯λt
¯τ t

⊤

∇

˜λt+1
˜τ t+1

−
−

(cid:18)

(cid:19)

(cid:18)

⊤

¯λt
¯τ t

−
−

(cid:19)

∇

ϕ(¯λt, ¯τ t).

(18)



.

!






}

Putting these pieces together yields that

ϕ(

λt+1,

τ t+1)

(1

−

≤

θt)ϕ(¯λt, ¯τ t)

θt

−

⊤

˜λt
˜τ t

¯λt
¯τ t

−
−

(cid:19)

∇

(cid:18)
I

ϕ(¯λt, ¯τ t)

b

b

|



ϕ(¯λt, ¯τ t) +

+θt






|

˜λt+1
˜τ t+1

⊤

¯λt
¯τ t

−
−

(cid:19)

(cid:18)

∇

{z
ϕ(¯λt, ¯τ t) + 2θt

II

m

Xk=1

}

ωk

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜λt+1
˜τ t+1

˜λt
˜τ t

−
−

2

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

For the term I in equation (18), we derive from the deﬁnition of (¯λt, ¯τ t) that

{z

θt

−

˜λt
˜τ t

(cid:18)

¯λt
¯τ t

−
−

(cid:19)

= θt

¯λt
¯τ t

(cid:18)

(cid:19)

+ (1

θt)

−

ˇλt
ˇτ t

(cid:18)

¯λt
¯τ t

(cid:19)

−

(cid:18)

(cid:19)

= (1

θt)

−

ˇλt
ˇτ t

¯λt
¯τ t

.

(cid:19)

−
−

(cid:18)

Using this equality and the convexity of ϕ, we have

I = (1

θt)

−

ϕ(¯λt, ¯τ t) +

⊤

ˇλt
ˇτ t

¯λt
¯τ t

−
−

(cid:18)

(cid:19)

∇

ϕ(¯λt, ¯τ t)

! ≤

θt)ϕ(ˇλt, ˇτ t).

(1

−

(19)

For the term II in equation (18), the deﬁnition of (˜λt+1, ˜τ t+1) implies that



⊤

λ
τ

˜λt+1
˜τ t+1

ϕ(¯λt, ¯τ t) + 4θt

(cid:19)

(cid:18)

∇

−
−










Letting (λ, τ ) = (λ⋆, τ ⋆) and rearranging the resulting inequality yields that































˜τ t
m)

for all (λ, τ )

≥

0,







˜λt
1)

˜λt
m)
˜τ t
1)

ω1(˜λt+1
1 −
...
ωm(˜λt+1
m −
ω1(˜τ t+1
1 −
...
ωm(˜τ t+1
m −

Rmn

.

× P

∈

˜λt+1
˜τ t+1

λ⋆
τ ⋆

−
−

(cid:18)

(cid:18)

−
−
¯λt
¯τ t

⊤

¯λt
¯τ t

(cid:19)
⊤

∇

(cid:19)

≤

ϕ(¯λt, ¯τ t) + 2θt

∇

ϕ(¯λt, ¯τ t) + 2θt

m

ωk

Xk=1
m

ωk

Xk=1

20

(cid:18)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
 (cid:13)
(cid:13)
(cid:13)
(cid:13)

!

2

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
−

˜λt+1
k −
˜τ t+1
k −
˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ⋆
k −
τ ⋆
k −

λ⋆
k −
τ ⋆
k −

2

˜λt+1
k
˜τ t+1
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

!!

 
 
 
 
 
Using the convexity of ϕ again, we have

⊤

λ⋆
τ ⋆

¯λt
¯τ t

−
−

(cid:18)

(cid:19)

∇

ϕ(¯λt, ¯τ t)

ϕ(λ⋆, τ ⋆)

−

≤

ϕ(¯λt, ¯τ t).

Putting these pieces together yields that

I

ϕ(λ⋆, τ ⋆) + 2θt

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
Plugging Eq. (19) and Eq. (20) into Eq. (18) yields that

λ⋆
k −
τ ⋆
k −

 (cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk=1

ωk

≤

(cid:18)

m

2

λ⋆
k −
τ ⋆
k −

2

˜λt+1
k
˜τ t+1
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

!!

(20)

m

b

≤

−

(1

ϕ(

λt+1,

τ t+1)

θt)ϕ(ˇλt, ˇτ t)+θtϕ(λ⋆, τ ⋆)+2θ2
t

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
Since (ˇλt+1, ˇτ t+1) is obtained by an exact coordinate update from (λt, τ t), we have ϕ(λt, τ t)
ϕ(ˇλt+1, ˇτ t+1). Using the similar argument, we have ϕ(´λt, ´τ t)
deﬁnition of (´λt, ´τ t), we have ϕ(
desired inequality.

˜λt+1
˜λt
k
k
˜τ t+1
˜τ t
k (cid:19)(cid:13)
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≥
ϕ(`λt, `τ t)
ϕ(λt, τ t). By the
ϕ(´λt, ´τ t). Putting these pieces together yields the
(cid:3)

λ⋆
k −
τ ⋆
k −

λ⋆
k −
τ ⋆
k −

Xk=1

τ t)

λt,

ωk

−

≥

≥

≥

(cid:18)

(cid:18)

b

2

2

.

!!

4.3 Main result

b

b

We present an upper bound for the iteration numbers required by the FastIBP algorithm.
}t≥0 be the iterates generated by the FastIBP algorithm. Then

Theorem 4.7. Let
the number of iterations required to reach the stopping criterion Et ≤

(λt, τ t)
{

ε satisﬁes

(cid:18)
where Rλ, Rτ > 0 are deﬁned in Lemma 2.2.

1 + 10

t

≤

n(R2

λ + R2
τ )
ε2

1/3

,

(cid:19)

Proof. First, let δt = ϕ(ˇλt, ˇτ t)

ϕ(λ⋆, τ ⋆), we show that

−

8n(R2

λ + R2
τ )

(t + 1)2

.

δt ≤

Indeed, by Lemma 4.3 and 4.6, we have

1

θt+1

−
θ2
t+1 (cid:19)

(cid:18)

δt+1 −

1

θt

−
θ2
t (cid:19)

(cid:18)

2

δt ≤

ωk

m

Xk=1

λ⋆
k −
τ ⋆
k −

(cid:18)

2

−

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ⋆
k −
τ ⋆
k −

(cid:18)

2

˜λt+1
k
˜τ t+1
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

!!

 (cid:13)
(cid:13)
(cid:13)
(cid:13)

By unrolling the recurrence and using θ0 = 1 and ˜λ0 = ˜τ0 = 0mn, we have

1

θt

−
θ2
t (cid:19)

(cid:18)

m

δt + 2

ωk

Xk=1

2

≤

λ⋆
k −
τ ⋆
k −

ωk

(cid:18)
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk=1

2

˜λt
k
˜τ t
k (cid:19)(cid:13)
(cid:13)
(cid:13)
λ⋆
(cid:13)
k
τ ⋆
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

!

≤

21

1

θ0

−
θ2
0 (cid:19)

(cid:18)

Corollary 2.4

! ≤
2

m

δ0 + 2

ωk

Xk=1
λ + R2

τ ).

2n(R2

λ⋆
k −
τ ⋆
k −

˜λ0
k
˜τ 0
k (cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(21)

.

2

!

 
 
 
 
 
 
For t

≥

1, Lemma 4.3 implies that θ−2

t−1 = (1

θt)θ−2
t

. Therefore, we conclude that

−
t−1n(R2

2θ2

λ + R2

τ ).

δt ≤

This together with the fact that 0 < θt−1 ≤

Furthermore, we show that

2/(t + 1) yields the desired inequality.

δt −

δt+1 ≥

E2
t
11

.

(22)

Indeed, by the deﬁnition of ∆t, we have

δt −
By the deﬁnition of ϕ, we have

δt+1 = ϕ(ˇλt, ˇτ t)

−

m

ϕ(ˇλt+1, ˇτ t+1)

ϕ(λt, τ t)

−

≥

ϕ(ˇλt+1, ˇτ t+1).

ϕ(λt, τ t)

−

ϕ(ˇλt+1, ˇτ t+1) =

Bk(λt
ωk(log(
k

k, τ t
k1)
k)

Bk(ˇλt+1
log(
k
k

−

, ˇτ t+1
k

k1)).
)

Xk=1
∆n for all k

Since r(Bk(λt
the update formula of (ˇλt+1, ˇτ t+1) yields that

k)) = uk

k, τ t

∈

∈

[m], we have

Bk(λt
k

k, τ t
k)

k1 = 1. This together with

ϕ(λt, τ t)

−

ϕ(ˇλt+1, ˇτ t+1) =

−

log

n ePm
1⊤
(cid:16)
R, we have

Recall that log(1 + x)

x for all x

k=1 ωk log(c(Bk(λt

k,τ t

k)))

.

(cid:17)

≤
ϕ(λt, τ t)

∈
ϕ(ˇλt+1, ˇτ t+1)

−

n ePm
1⊤

k=1 ωk log(c(Bk(λt

k,τ t

k))).

1

−

≥

Since r(Bk(λt
(ω1, ω2, . . . , ωm)

k, τ t

k)) = uk

∈
∆m. Thus, we have

∆n for all k

[m], we have 1⊤

n c(Bk(λt

k, τ t

k)) = 1. In addition,

∈

∈

−

ϕ(λt, τ t)

ϕ(ˇλt+1, ˇτ t+1)

1⊤
n

≥

m

Xk=1

ωkc(Bk(λt

k, τ t

k))

−

ePm

k=1 ωk log(c(Bk(λt

k,τ t

k)))

.

!

Combining c(Bk(λt

k, τ t

k))

∆n with the arguments in Kroshnin et al. [2019, Lemma 6] yields

∈

ϕ(λt, τ t)

−

ϕ(ˇλt+1, ˇτ t+1)

1
11

≥

m

Xk=1

c(Bk(λt

k, τ t

k))

ωkk

−

ωic(Bi(λt

i, τ t

2
1.
i ))
k

m

Xi=1
m
k=1 ωk = 1, we have

Using the Cauchy-Schwarz inequality together with

m

P
m

E2

t ≤

c(Bk(λt

k, τ t

k))

ωkk

−

2
1.
ωic(Bi(λi, τi))
k

Xk=1
Putting these pieces together yields the desired inequality.

Xi=1

Finally, we derive from Eq. (21) and (22) and the non-negativeness of δt that

+∞

Xi=t

E2

i ≤

11

+∞

Xi=t

(δi −

δi+1)

! ≤

11δt ≤

88n(R2

λ + R2
τ )

(t + 1)2

22

 
 
Let T > 0 satisfy ET ≤
assume T is even. Then the following statement holds true:

ε, we have Et > ε for all t

∈

[T ]. Without loss of generality, we

ε2

≤

704n(R2
λ + R2
τ )
T 3

.

Rearranging the above inequality yields the desired inequality.

(cid:3)

Equipped with the result of Theorem 4.7, we are ready to present the complexity bound

of Algorithm 2 for approximating the FS-WBP in Eq. (3).
Theorem 4.8. The FastIBP algorithm for approximately solving the FS-WBP in Eq. (3)
(Algorithm 2) returns an ε-approximate barycenter

Rn within

u

O

mn7/3





arithmetic operations.

(max1≤k≤m k

∈
Ckk∞)
b
ε

4/3

log(n)

!





p

Xm) be generated by the FastIBP algorithm (cf. Al-
Proof. Consider the iterate (
gorithm 1), the rounding scheme (cf. Kroshnin et al. [2019, Algorithm 4]) returns the feasible
e
e
[m].
solution (
m
Xk) is an ε-approximate barycenter (cf. Deﬁnition 2.2), it
k=1 ωkc(

e
Xm) to the FS-WBP in Eq. (3) and c(

Xk) are the same for all k

X2, . . . ,
u =

X2, . . . ,

X1,

∈

X1,
To show that
b
suﬃces to show that

b

b

b
P

b

b
ωkh

Ck,

m

Xk=1

m

Xk=1

Xki ≤
b

ωkh

Ck, X ⋆
k i

+ ε,

where (X ⋆
2 , . . . , X ⋆
and the barycenter of the FS-WBP.

1 , X ⋆

m) is a set of optimal transportation plan between m measures

First, we derive from the scheme of Kroshnin et al. [2019, Algorithm 4] that the following

inequality holds for all k

[m],

∈

m

Xi=1
This together with the H¨older’s inequality implies that

e

Xk −
k
b

c(
Xkk1 ≤ k
e

Xk)

−

m

Ck,

ωkh

Xk −
Xk=1
b
Furthermore, we have

Xki ≤
e

max
1≤k≤m k

Ckk∞

(cid:18)

m

(cid:19)  

Xk=1

m

Xk=1

Ck,

ωkh

Xk −
e

X ⋆
k i

=

m

Ck,
ωk(
h

Xki −
e
Xk)
ωkηH(

Xk=1
m
+

Since 0
≤
2012] and

Xk=1
2 log(n) for any X

H(X)
m
k=1 ωk = 1, we have

≤

e
Rn×n
+

∈

ηH(

Xk))

−

Xk=1
e
ωkηH(X ⋆
k ).

m

Xk=1

−

ωic(

Xi)

k1.

e

c(
ωkk

Xk)

−

e

m

m

Xi=1

ωic(

Xi)

k1

.

!

(24)

e

Ck, X ⋆
ωk(
h

k i −

ηH(X ⋆

k ))

satisfying that

X
k

k1 = 1 [Cover and Thomas,

m

Xk=1

Ck,

ωkh

P
Xk −
e

X ⋆

k i ≤

2η log(n) +

m

Xk=1

Ck,
ωk(
h

Xki −
e
23

ηH(

Xk))

e

m

−

Xk=1

Ck, X ⋆
ωk(
h

k i −

ηH(X ⋆

k )).

(23)

uk
{

}k∈[m]

 
2 , . . . , X η
Let (X η
FS-WBP in Eq. (4), we have

1 , X η

m) be a set of optimal transportation plans to the entropic regularized

m

m

Ck, X η
ωk(
h
1 , X η

k i −
2 , . . . , X η

Xk=1

ηH(X η

k ))

m), we have

≤

Xk=1

By the optimality of (X η
m

Ck, X ⋆
ωk(
h

k i −

ηH(X ⋆

k )).

Ck, X η
ωk(
h

k i −

ηH(X η

k )) =

η

−

min
λ∈Rmn,τ ∈P

ϕ(λ, τ )

≥ −

Xk=1
X2, . . . ,

(cid:18)
Xm) is generated by the FastIBP algorithm, we have

(cid:19)

ηϕ(λt, τ t).

Xk = Bk(λt

k, τ t

k) for

Since (
all k

∈
m

X1,
[m] where (λt, τ t) are the dual iterates. Then
e
e
Ck,
ωk(
h

Ck, Bk(λt
ωk(
h

Xk)) =

ηH(

m

Xk=1

e
ωk(1⊤

n Bk(λt

Xk=1
k, τ t

k)1n −

(λt

k)⊤uk)

!

e
Xki −
e

=

η

−

−

m

Xk=1

=

ηϕ(λt, τ t) + η

ωk(τ t

k)⊤

c(Bk(λt

k, τ t

k))

ηH(Bk(λt

e
k, τ t

k)))

k, τ t
k)

i −

m

+ η

ωk(τ t

k)⊤c(Bk(λt

k, τ t

k))

Xk=1
m

−

Xi=1

ωic(Bi(λt

i, τ t

i ))

!!

Putting these pieces together yields that

m

ωkh

Xk=1
Since 1⊤

Ck,

Xk −
e
n c(Bk(λt
m

X ⋆

k i ≤

k, τ t

k)) = 1 for all k

Xk=1

[m], we have

m

ωk(τ t

k)⊤

c(Bk(λt

k, τ t

k))

ωic(Bi(λt

i, τ t

i ))

2η log(n) + η

m

ωk(τ t

k)⊤

c(Bk(λt

k, τ t

k))

ωic(Bi(λt

i, τ t

i ))

m

−

Xi=1

.

.

!!

Xk=1
m

ωk

τ t
k −

(cid:18)

=

≤

Xk=1
τ t
k −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Using Lemma 4.5, we have

max1≤i≤n(τ t

k)i + min1≤i≤n(τ t

k)i

!!
⊤

1n

m

(cid:19)

c(Bk(λt

k, τ t

k))

m

−

Xi=1

ωic(Bi(λt

i, τ t

i ))

!!

max1≤i≤n(τ t

k)i + min1≤i≤n(τ t

k)i

2

c(
ωkk

Xk)

1n

∞  

Xk=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
k)i + min1≤i≤n(τ t

k)i

max1≤i≤n(τ t

2

m

−

Xi=1

ωic(

Xi)

k1

.

!

e

e

1n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Rτ
2

.

∞ ≤

τ t
k −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Xk −
e
m
c(
k=1 ωkk

X ⋆

Putting these pieces together yields that

m

Xk=1

Ck,

ωkh

2η log(n) +

k i ≤

Recall that Et =
Eq. (24) and Eq. (25) together imply that

Xk)

−

m
i=1 ωic(

m

m

Xk)

c(
ωkk

ηRτ
2  
e
k1 and Rτ = 4η−1(max1≤k≤m k

Xi=1

Xk=1

ωic(

Xi)

Xi)

−

e

k1

.

!

(25)

Ckk∞). Then

P
m

Xk=1

Ck,

ωkh

P
X ⋆

k i ≤

e
Xk −
b

e

2η log(n) + 3

max
1≤k≤m k

Ckk∞

Et.

(cid:19)

(cid:18)

This together with Et ≤

¯ε/2 and the choice of η and ¯ε implies Eq. (23) as desired.

m

Xk=1

∈

−

2

Xi=1

24

 
 
 
 
 
 
 
 
 
Complexity bound estimation. We ﬁrst bound the number of iterations required by the
FastIBP algorithm (cf. Algorithm 1) to reach Et ≤

¯ε/2. Indeed, Theorem 4.7 implies that

t

1 + 20

≤
(cid:19)
For the simplicity, we let ¯C = max1≤k≤m k
Lemma 2.2, the construction of

(cid:18)

1/3

n(R2

λ + R2
τ )
¯ε2

20 3√n

≤

Rλ + Rτ
¯ε

2/3

.

(cid:18)

(cid:19)
Ckk∞. Using the deﬁnition of Rλ and Rτ in

˜uk
{

}k∈[m] and the choice of η and ¯ε, we have

1+20 3√n

t

≤

4 ¯C
ε

(cid:18)

(cid:18)

52 log(n) ¯C
ε

+ 2 log(n)

16n ¯C
ε

log

−

(cid:18)

(cid:19)(cid:19)(cid:19)

2/3

= O

3√n



4/3

¯C

log(n)
ε

!

p

Recall that each iteration of the FastIBP algorithm requires
(mn2) arithmetic operations,
the total arithmetic operations required by the FastIBP algorithm as the subroutine in
Algorithm 2 is bounded by

O



.





mn7/3

O



¯C

log(n)
ε

!

p

4/3

.




Computing a collection of vectors
}k∈[m] needs
rounding scheme in Kroshnin et al. [2019, Algorithm 4] requires
Putting these pieces together yields that the desired complexity bound of Algorithm 2.

(mn) arithmetic operations while the
(mn2) arithmetic operations.
(cid:3)

˜uk
{



O

O

Remark 4.9. For the simplicity, we assume that all measures have the same support size.
This assumption is not necessary and our analysis is still valid when each measure has ﬁxed
support of diﬀerent size. However, our results can not be generalized to the free-support
Wasserstein barycenter problem in general since the computation of free-support barycenters
requires solving a multimarginal OT problem where the complexity bounds of algorithms become
much worse; see Lin et al. [2019a] for the details.

5 Experiments

In this section, we report the results of extensive numerical experiments to evaluate the
FastIBP algorithm for computing ﬁxed-support Wasserstein barycenters.
In all our ex-
periments, we consider the Wasserstein distance with ℓ2-norm, i.e., 2-Wasserstein distance,
and compare the FastIBP algorithm with Gurobi, iterative Bregman projection (IBP) al-
gorithm [Benamou et al., 2015] and Bregman ADMM (BADMM) [Ye et al., 2017]1. All the
experiments are conducted in MATLAB R2020a on a workstation with an Intel Core i5-9400F
(6 cores and 6 threads) and 32GB memory, equipped with Ubuntu 18.04.

5.1

Implementation details

For the FastIBP algorithm, the regularization parameter η is chosen from
in
our experiments. We follow Benamou et al. [2015, Remark 3] to implement the algorithm and

0.01, 0.001
}
{

1We

implement ADMM [Yang et al., 2018], APDAGD [Kroshnin et al., 2019] and accelerated
IBP [Guminov et al., 2019] and ﬁnd that they perform worse than our algorithm. However, we believe it
is largely due to our own implementation issue since these algorithms require ﬁne hyper-parameter tuning. We
are also unaware of any public codes available online. Thus, we exclude them for a fair comparison.

25

 
 
terminate it when

m

m

k=1 ωkk
k=1 ωkk

1 +

P
P

m
i=1 ωic(Bi(λt

i, τ t
k, τ t
c(Bk(λt
k))
−
m
k, τ t
c(Bk(λt
i=1 ωic(Bi(λt
+
k))
P
k
k
m
k, τ t
r(Bk(λt
k=1 ωkk
P
r(Bk(λt
k, τ t
k))
k
m
i=1 ωic(Bi(λt−1
P

k))
+

i ))
k
i, τ t
i ))
k

uk

k

))
k

k

−

uk
k
m
k=1 ωkk
, τ t−1
))
i
i
k
m
, τ t−1
i=1 ωic(Bi(λt−1
i
i
, τ t−1
Bk(λt−1
)
kF
P
k
m
Bk(λt−1
k=1 ωkk
k
m
λt−1
λt
k=1 ωkk
P
k −
k k
m
m
λt
k=1 ωkk
+
k=1 ωkk
P
kk
τ t−1
τ t
k=1 ωkk
P
k −
k
k
m
m
τ t
k=1 ωkk
+
k=1 ωkk
P
kk

, τ t−1
k

m

λt−1
k k

τ t−1
k

k

1 +
m
i=1 ωic(Bi(λt
P
m
i=1 ωic(Bi(λt
k=1 ωkk
Bk(λt

k
1 +
P
k
P
m
k=1 ωkk
P

m
k=1 ωkk
P
i, τ t
i ))
−
i, τ t
i ))
+
P
k
k
k, τ t
Bk(λt
k)
−
k, τ t
kF +
k)

1 +

m

P

1 +

P

1 +

Tolﬁbp,

Tolﬁbp,

Tolﬁbp,

Tolﬁbp,

Tolﬁbp,

Tolﬁbp.

≤

≤

≤

≤

≤

kF ≤
)

P

P

These inequalities guarantee that (i) the infeasibility violations for marginal constraints, (ii)
the iterative gap between approximate barycenters, and (iii) the iterative gap between dual
variables are relatively small. Computing all the above residuals is expensive. Thus, in
our implementations, we only compute them and check the termination criteria at every 20
iterations when η = 0.01 and every 200 iteration when η = 0.001. We set Tolﬁbp = 10−6 and
MaxIterﬁbp = 10000 on synthetic data and Tolﬁbp = 10−10 on MNIST images.

For IBP and BADMM, we use the Matlab code2 implemented by Ye et al. [2017] and ter-
minate them with the reﬁned stopping criterion provided by Yang et al. [2018]. The regular-
. For synthetic data,
0.01, 0.001
ization parameter η for the IBP algorithm is still chosen from
}
{
we set Tolbadmm = 10−5 and Tolibp = 10−6 with MaxIterbadmm = 5000 and MaxIteribp = 10000.
For MNIST images, we set Tolibp = 10−10.

For the linear programming algorithm, we apply Gurobi 9.0.2 (Gurobi Optimization, 2019)
(with an academic license) to solve the FS-WBP in Eq. (3). Since Gurobi can provide high
quality solutions when the problem of medium size, we use the solution obtained by Gurobi as
a benchmark to evaluate the qualities of solution obtained by diﬀerent algorithms on synthetic
data. In our experiments, we force Gurobi to only run the dual simplex algorithm and use
other parameters in the default settings.

For the evaluation metrics, “normalized obj” stands for the normalized objective value

which is deﬁned by

normalized obj := |

P

m

k=1 ωkh
|

Ck,
m

Xki −
k=1 ωkh
b

m
Ck, X g
P

k=1 ωkh
k i|

Ck, X g

k i|

,

X1, . . . ,

Xm) is the solution obtained by each algorithm and (X g
m) denotes
where (
the solution obtained by Gurobi. “feasibility” denotes the the deviation of the terminating
solution from the feasible set3; see Yang et al. [2018, Section 5.1]. “iteration” denotes the
number of iterations. “time (in seconds)” denotes the computational time.

1 , . . . , X g

b

b

P

2Available in https://github.com/bobye/WBC Matlab
3Since we do not put the iterative gap between dual variables in “feasibility” and the FS-WBP is relatively
easier than general WBP, our results for BADMM and IBP are consistently smaller than that presented
by Ye et al. [2017], Yang et al. [2018], Ge et al. [2019].

26

n=100

n=100

f2

f1

i2

i1

b

f2

f1

i2

i1

b

g

10 0

10 -2

j

b
o

d
e
z

i
l

a
m
r
o
n

)
s
d
n
o
c
e
s

n
i
(

e
m

i
t

10 2

10 0

50

100
numer of marginals (m)

150

200

50
150
100
numer of marginals (m)

200

Figure 2: The average normalized objective value and computational time (in seconds) of
FastIBP, IBP, BADMM, and Gurobi from 10 independent trials.

10 0

n=100

f (prox)

i (prox)

j

b
o
d
e
z

i
l

a
m
r
o
n

10 -1

10 -2

50

100
numer of marginals (m)

150

200

10 3

10 2

10 1

)
s
d
n
o
c
e
s

n
i
(

e
m

i
t

n=100

f (prox)

i (prox)

50

100
numer of marginals (m)

150

200

Figure 3: The average normalized objective value and computational time (in seconds) of the
proximal variants of FastIBP and IBP from 10 independent trials.

In what follows, we present our experimental results.

In Section 5.2, we evaluate all
the candidate algorithms on synthetic data and compare their computational performance
in terms of accuracy and speed.
In Section 5.3, we compare our algorithm with IBP on
the MNIST dataset to visualize the quality of approximate barycenters obtained by each
algorithm. For the simplicity of the presentation, in our ﬁgures “g” stands for Gurobi; “b”
stands for BADMM; “i1” and “i2” stand for the IBP algorithm with η = 0.01 and η = 0.001;
“f1” and “f2” stand for the FastIBP algorithm with η = 0.01 and η = 0.001.

5.2 Experiments on synthetic data

i , xi)
∈
ui, xi)
(
{

m
k=1 with µk =
In this section, we generate a set of discrete probability distributions
(uk
i = 1. The ﬁxed-support Wasserstein barycenter
{
where (x1, x2, . . . , xn) are known. In our experiment, we
µ =
P
set d = 3 and choose diﬀerent values of (m, n). Then, given each tuple (m, n), we randomly
generate a trial as follows.
b

Rd
|
R+ ×

R+ ×
∈

and
[n]
}

[n]
}
i
∈

n
i=1 uk

µk}
{

i
Rd

∈
|

b

First, we generate the support points (xk

1, xk

2, . . . , xk

n) whose entries are drawn from a

Gaussian mixture distribution via the Matlab commands provided by Yang et al. [2018]:

gm num = 5; gm mean = [-20; -10; 0; 10; 20];

27

 
 
 
 
 
 
sigma = zeros(1, 1, gm num); sigma(1, 1, :) = 5*ones(gm num, 1);
gm weights = rand(gm num, 1); gm weights = gm weights/sum(gm weights);
distrib = gmdistribution(gm mean, sigma, gm weights);

n=100

∈

[m], we generate the weight vector (uk

n) whose entries are drawn
For each k
n
i=1 uk
i = 1.
from the uniform distribution on the interval (0, 1), and normalize it such that
xk
k=1, we use the k-means4 method to choose n points from
m
i
After generating all
i |
∈
{
P
[n], k
to be the support points of the barycenter. Finally, we generate the weight vector
[m]
}
(ω1, ω2, . . . , ωm) whose entries are drawn from the uniform distribution on the interval (0, 1),
and normalize it such that

2, . . . , uk

µk}
{

1, uk

∈

m
k=1 ωk = 1.

g

i
t

f2

m

}

f2
g

2000

10 2

10 4

10 6

n
i
(
e
m

∈ {

)
s
d
n
o
c
e
s

normalized obj

20, 50, 100, 200

500
1500
1000
numer of marginals (m)

We present some preliminary numerical results in
P
Figure 2 and 3. Given n = 100, we evaluate the
performance of FastIBP, IBP, BADMM algorithms,
and Gurobi by varying m
and
use the same setup to compare the proximal variants
of FastIBP and IBP. We use the proximal frame-
work [Kroshnin et al., 2019] with the same parame-
ter setting as provided by their paper. As indicated
in Figure 2, the FastIBP algorithm performs bet-
ter than BADMM and IBP algorithms in the sense
that it consistently returns an objective value closer
to that of Gurobi in less computational time. More
speciﬁcally, IBP converges very fast when η = 0.01,
but suﬀers from a crude solution with poor objective
value; BADMM takes much more time with unsatisfac-
tory objective value, and is not provably convergent in
theory; Gurobi is highly optimized and can solve the
problem of relatively small size very eﬃciently. How-
ever, when the problem size becomes larger, Gurobi
would take much more time. As an example, for the
case where (m, n) = (200, 100), we see that Gurobi is
about 10 times slower than the FastIBP algorithm
with η = 0.001 while keeping relatively small normal-
ized objective value. As indicated in Figure 3, the proximal variant of FastIBP algorithm
also outperforms that of IBP algorithm in terms of objective value while not sacriﬁcing the
time eﬃciency. To facilitate the readers, we present the averaged results from 10 independent
trials with FastIBP, IBP, BADMM algorithms, and Gurobi in Table 1. Note that we imple-
ment the rounding scheme after each algorithm (except Gurobi) so the terms in “feasibility”
are zero up to numerical errors for most of medium-size problems.

Figure 4: Preliminary results with Gurobi
and the FastIBP algorithm (η = 0.001).

1.5e+05±2.4e+04
5.0e+05±8.8e+04
1.3e+06±1.5e+05
4.9e+06±1.6e+06

7.4e-07±1.8e-07
7.0e-07±2.8e-07
7.1e-07±2.0e-07
8.7e-07±2.0e-07

3.2e-07±1.8e-07
2.8e-07±5.0e-08
2.1e-07±1.0e-07
2.0e-07±1.3e-07

3.6e-03±3.1e-04
4.4e-03±6.2e-04
4.8e-03±5.4e-04
5.0e-03±3.8e-04

2.4e+03±3.2e+02
3.3e+03±1.4e+03
1.9e+03±3.1e+02
4.5e+03±1.7e+03

200
500
1000
2000

200
500
1000
2000

200
500
1000
2000

feasibility

iteration

-
-
-
-

To further compare the performances of Gurobi and the FastIBP algorithm, we conduct
the experiment with n = 100 and the varying number of marginals m
.
}
We ﬁx Tolﬁbp = 10−6 but without setting the maximum iteration number. Figure 4 shows
the average running time taken by two algorithms over 5 independent trials. We see that
the FastIBP algorithm is competitive with Gurobi in terms of objective value and feasibility
In terms of computational time, the FastIBP algorithm increases linearly with
violation.
respect to the number of marginals, while Gurobi increases much more rapidly. Compared
to the similar results of Gurobi presented before [Yang et al., 2018, Ge et al., 2019], we ﬁnd

200, 500, 1000, 2000

∈ {

4In our experiments, we call the Matlab function kmeans, which is built in machine learning toolbox.

28

 
 
Table 1: Numerical results on synthetic data where each distribution has diﬀerent dense
weights but same support size. The support points of the barycenter is ﬁxed.

m

n

20
20
20
50
50
50
100
100
100
200
200
200

20
20
20
50
50
50
100
100
100
200
200
200

20
20
20
50
50
50
100
100
100
200
200
200

20
20
20
50
50
50
100
100
100
200
200
200

50
100
200
50
100
200
50
100
200
50
100
200

50
100
200
50
100
200
50
100
200
50
100
200

50
100
200
50
100
200
50
100
200
50
100
200

50
100
200
50
100
200
50
100
200
50
100
200

g

-
-
-
-
-
-
-
-
-
-
-
-

b

i1

i2

f1

f2

normalized obj

5.9e-01±1.2e-01
6.7e-01±8.2e-02
7.8e-01±7.4e-02
4.5e-01±4.3e-02
6.4e-01±1.0e-01
8.2e-01±7.8e-02
3.1e-01±3.0e-02
6.1e-01±9.3e-02
8.3e-01±5.0e-02
2.8e-01±4.2e-02
4.4e-01±4.6e-02
8.0e-01±8.7e-02

2.0e-01±8.0e-02
3.2e-01±5.5e-02
4.8e-01±5.9e-02
1.7e-01±3.7e-02
3.7e-01±6.8e-02
5.9e-01±5.7e-02
1.1e-01±2.9e-02
3.8e-01±6.0e-02
6.1e-01±4.0e-02
1.1e-01±3.9e-02
2.8e-01±3.0e-02
6.0e-01±6.8e-02

2.1e-01±1.1e-01
3.6e-01±9.5e-02
6.0e-01±7.6e-02
1.6e-01±4.9e-02
4.3e-01±6.4e-02
6.7e-01±8.5e-02
7.2e-02±2.8e-02
4.6e-01±7.2e-02
7.5e-01±4.2e-02
6.0e-02±3.8e-02
3.7e-01±5.8e-02
7.2e-01±4.6e-02

5.7e-02±1.1e-02
6.7e-02±8.0e-03
6.3e-02±4.7e-03
6.8e-02±1.0e-02
7.6e-02±8.3e-03
6.2e-02±6.9e-03
6.7e-02±1.4e-02
7.7e-02±6.0e-03
5.6e-02±4.7e-03
6.9e-02±1.4e-02
7.9e-02±3.1e-03
5.7e-02±4.5e-03

1.7e-03±9.7e-04
2.1e-03±8.2e-04
2.9e-03±3.8e-04
2.2e-03±8.3e-04
3.0e-03±6.1e-04
4.0e-03±6.4e-04
3.9e-03±2.3e-03
3.6e-03±7.0e-04
4.3e-03±6.9e-04
3.2e-03±1.8e-03
3.7e-03±3.3e-04
5.2e-03±4.7e-04

4.9e-07±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
2.7e-07±1.9e-07

1.9e-07±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
2.2e-07±2.3e-08

3115±532
6415±999
12430±2339
10139±1192
21697±4377
39564±6916
26729±2731
52799±9610
97357±10615
55841±6359
149230±18051
258059±62104

5000±0
3400±94
3580±175
5000±0
3480±103
3740±97
4580±887
3560±84
3780±114
4280±1038
3600±0
3800±94

feasibility

9.0e-07±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
6.2e-07±1.5e-07

9.6e-07±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
9.2e-07±3.9e-08

iteration

1.8e-14±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
7.5e-14±1.9e-13

3.6e-09±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
0.0e+00±0.0e+00
1.0e-07±1.6e-07

440±158
400±0
400±0
360±227
580±63
580±63
380±274
700±287
920±103
300±141
980±537
1440±158

7140±3912
8660±5464
5520±2368
7320±6351
9920±5351
6800±2512
5180±1901
10020±3400
9720±3737
11140±4724
12840±3650
12160±2609

200±0
200±0
200±0
200±0
200±0
220±63
240±84
200±0
200±0
200±0
200±0
200±0

2760±1560
2860±1678
1240±497
3560±1786
1540±1116
1240±833
5940±2406
1360±815
440±280
8000±3749
1880±634
340±165

time (in seconds)

3.6e-01±1.1e-01
1.9e+00±1.2e+00
6.2e+00±1.7e+00
3.1e+00±1.3e+00
1.1e+01±2.1e+00
2.7e+01±5.8e+00
1.3e+01±4.3e+00
3.6e+01±1.1e+01
1.0e+02±2.1e+01
5.4e+01±1.2e+01
2.8e+02±6.7e+01
4.9e+02±2.0e+02

9.4e+00±3.6e+00
2.4e+01±3.9e+00
1.2e+02±5.1e+00
2.3e+01±4.8e+00
6.9e+01±5.0e+00
3.2e+02±1.4e+01
4.3e+01±7.8e+00
1.4e+02±3.9e+00
6.6e+02±2.7e+01
9.3e+01±2.5e+01
3.2e+02±2.7e+01
1.9e+03±9.5e+01

2.5e-01±1.1e-01
1.2e+00±6.2e-01
4.0e+00±5.6e-01
5.4e-01±3.8e-01
3.7e+00±7.1e-01
1.7e+01±5.2e+00
1.1e+00±9.0e-01
7.9e+00±3.5e+00
5.1e+01±6.0e+00
2.0e+00±8.9e-01
3.0e+01±1.9e+01
2.8e+02±3.2e+01

4.1e+00±2.2e+00
2.5e+01±1.5e+01
5.9e+01±2.7e+01
1.2e+01±1.1e+01
7.2e+01±4.1e+01
2.0e+02±7.5e+01
1.7e+01±7.0e+00
1.4e+02±4.9e+01
5.7e+02±2.3e+02
9.0e+01±4.2e+01
4.0e+02±1.3e+02
2.5e+03±5.6e+02

3.1e-01±5.7e-02
1.6e+00±9.0e-01
5.9e+00±7.5e-01
1.1e+00±6.0e-01
3.5e+00±5.3e-01
1.5e+01±4.5e+00
2.7e+00±1.2e+00
7.2e+00±1.0e+00
2.7e+01±3.8e+00
4.8e+00±2.9e+00
1.5e+01±4.5e+00
1.1e+02±8.1e+00

5.2e+00±2.4e+00
2.1e+01±1.3e+01
3.9e+01±1.5e+01
1.8e+01±9.2e+00
3.0e+01±2.2e+01
8.8e+01±5.5e+01
6.1e+01±2.7e+01
5.2e+01±3.0e+01
6.6e+01±4.1e+01
1.8e+02±1.0e+02
1.5e+02±5.1e+01
1.9e+02±9.5e+01

29

that the feasibility violation in our paper is better but the computational time grows much
faster. This makes sense since we run the dual simplex algorithm, which iterates over the
feasible solutions but is more computationally expensive than the interior-point algorithm.
Figure 4 demonstrates that the structure of the FS-WBP is not favorable to the dual simplex
algorithm, conﬁrming our computational hardness results in Section 3.

FastIBP (η = 0.001)

100s

200s

400s

800s

100s

200s

400s

800s

IBP (η = 0.001)

Table 2: Approximate barycenters obtained by running FastIBP and IBP for 100s, 200s, 400s, 800s.

5.3 Experiments on MNIST

To better visualize the quality of approximate barycenters obtained by each algorithm, we
follow Cuturi and Doucet [2014] on the MNIST5 dataset [LeCun et al., 1998]. We randomly
9) and resize each image to ζ times of its original size of
select 50 images for each digit (1
28, where ζ is drawn uniformly at random from [0.5, 2]. We randomly put each resized
28
image in a larger 56
56 blank image and normalize the resulting image so that all pixel
values add up to 1. Each image can be viewed as a discrete distribution supported on grids.
Additionally, we set the weight vector (ω1, ω2, . . . , ωm) such that ωk = 1/m for all k

[m].

∼

×

×

We apply the FastIBP algorithm (η = 0.001) to compute the Wasserstein barycenter of
the resulting images for each digit on the MNIST dataset and compare it to IBP (η = 0.001).
We exclude BADMM since Yang et al. [2018, Figure 3] and Ge et al. [2019, Table 1] have
shown that IBP outperforms BADMM on the MNIST dataset. The size of barycenter is set to
56
56. For a fair comparison, we do not implement convolutional technique [Solomon et al.,
2015] and its stabilized version [Schmitzer, 2019, Section 4.1.2], which can be used to sub-
stantially improve IBP with small η. The approximate barycenters obtained by the FastIBP
and IBP algorithms are presented in Table 2. It can be seen that the FastIBP algorithm

×

∈

5Available in http://yann.lecun.com/exdb/mnist/

30

provides a “sharper” approximate barycenter than IBP when η = 0.001 is set for both. This
demonstrates the good quality of the solution obtained by our algorithm.

6 Conclusions

3 and n

In this paper, we study the computational hardness for solving the ﬁxed-support Wasserstein
barycenter problem (FS-WBP) and proves that the FS-WBP in the standard linear program-
3. Our results
ming form is not a minimum-cost ﬂow (MCF) problem when m
suggest that the direct application of network ﬂow algorithms to the FS-WBP in standard
LP form is ineﬃcient, shedding the light on the practical performance of various existing
algorithms, which are developed based on problem reformulation of the FS-WBP. Moreover,
we propose a deterministic variant of iterative Bregman projection (IBP) algorithm, namely
FastIBP, and prove that the complexity bound is
O(mn7/3ε−4/3). This bound is better
O(mn2ε−2) from the IBP algorithm in terms of ε, and that of
than the complexity bound of
O(mn5/2ε−1) from other accelerated algorithms in terms of n. Experiments on synthetic and
real datasets demonstrate the favorable performance of the FastIBP algorithm in practice.
e
7 Acknowledgments

≥

≥

e

e

We would like to thank Lei Yang for very helpful discussion with the experiments on MNIST
datasets and four anonymous referees for constructive suggestions that improve the quality of
this paper. Xi Chen is supported by National Science Foundation via the Grant IIS-1845444.
This work is supported in part by the Mathematical Data Science program of the Oﬃce of
Naval Research under grant number N00014-18-1-2764.

References

M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathe-

matical Analysis, 43(2):904–924, 2011. (Cited on pages 1, 3, and 4.)

J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In NIPS, pages 1964–1974, 2017. (Cited on pages 2 and 18.)

E. Anderes, S. Borgwardt, and J. Miller. Discrete wasserstein barycenters: Optimal transport
for discrete data. Mathematical Methods of Operations Research, 84(2):389–409, 2016. (Cited
on pages 2 and 12.)

J-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr´e. Iterative Bregman projections
for regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37(2):
A1111–A1138, 2015. (Cited on pages 2, 3, 4, 16, 17, and 25.)

C. Berge. The Theory of Graphs. Courier Corporation, 2001. (Cited on page 11.)

J. Blanchet, A. Jambulapati, C. Kent, and A. Sidford. Towards optimal running times for

optimal transport. ArXiv Preprint: 1810.07717, 2018. (Cited on page 2.)

N. Bonneel, J. Rabin, G. Peyr´e, and H. Pﬁster. Sliced and radon wasserstein barycenters of
measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015. (Cited on pages 1
and 2.)

31

N. Bonneel, G. Peyr´e, and M. Cuturi. Wasserstein barycentric coordinates: histogram re-
gression using optimal transport. ACM Transactions on Graphics, 35(4):71:1–71:10, 2016.
(Cited on page 1.)

S. Borgwardt and S. Patterson. On the computational complexity of ﬁnding a sparse Wasser-

stein barycenter. ArXiv Preprint: 1910.07568, 2019. (Cited on page 2.)

S. Borgwardt and S. Patterson. Improved linear programs for discrete barycenters. Informs

Journal on Optimization, 2(1):14–33, 2020. (Cited on page 2.)

G. Buttazzo, L. De Pascale, and P. Gori-Giorgi. Optimal-transport formulation of electronic

density-functional theory. Physical Review A, 85(6):062502, 2012. (Cited on page 1.)

G. Carlier and I. Ekeland. Matching for teams. Economic theory, 42(2):397–418, 2010. (Cited

on page 1.)

G. Carlier, A. Oberman, and E. Oudet. Numerical methods for matching for teams and
wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):
1621–1642, 2015. (Cited on page 2.)

P-A. Chiappori, R. J. McCann, and L. P. Nesheim. Hedonic price equilibria, stable matching,
and optimal transport: equivalence, topology, and uniqueness. Economic Theory, 42(2):
317–354, 2010. (Cited on page 1.)

S. Claici, E. Chien, and J. Solomon. Stochastic Wasserstein barycenters. In ICML, pages

998–1007, 2018. (Cited on page 2.)

C. Cotar, G. Friesecke, and C. Kl¨uppelberg. Density functional theory and optimal trans-
portation with coulomb cost. Communications on Pure and Applied Mathematics, 66(4):
548–599, 2013. (Cited on page 1.)

T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, 2012.

(Cited on page 23.)

M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS, pages

2292–2300, 2013. (Cited on pages 2 and 5.)

M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In ICML, pages

685–693, 2014. (Cited on pages 1, 2, 4, 5, and 30.)

M. Cuturi and G. Peyr´e. A smoothed dual approach for variational Wasserstein problems.

SIAM Journal on Imaging Sciences, 9(1):320–343, 2016. (Cited on pages 2 and 16.)

M. Cuturi and G. Peyr´e. Semidual regularized optimal transport. SIAM Review, 60(4):

941–965, 2018. (Cited on pages 2 and 5.)

P. Dvurechenskii, D. Dvinskikh, A. Gasnikov, C. Uribe, and A. Nedich. Decentralize and
randomize: faster algorithm for Wasserstein barycenters. In NeurIPS, pages 10783–10793,
2018. (Cited on page 2.)

P. Dvurechensky, A. Gasnikov, and A. Kroshnin. Computational optimal transport: complex-
ity by accelerated gradient descent is better than by Sinkhorn’s algorithm. In ICML, pages
1366–1375, 2018. (Cited on page 2.)

32

O. Fercoq and P. Richt´arik. Accelerated, parallel, and proximal coordinate descent. SIAM

Journal on Optimization, 25(4):1997–2023, 2015. (Cited on page 17.)

W. Gangbo and A. Swiech. Optimal maps for the multidimensional Monge-Kantorovich
problem. Communications on Pure and Applied Mathematics, 51(1):23–45, 1998. (Cited on
page 1.)

D. Ge, H. Wang, Z. Xiong, and Y. Ye.

Interior-point methods strike back: solving the
Wasserstein barycenter problem. In NeurIPS, pages 6891–6902, 2019. (Cited on pages 2, 4, 15,
16, 26, 28, and 30.)

A. Genevay, M. Cuturi, G. Peyr´e, and F. Bach. Stochastic optimization for large-scale optimal

transport. In NIPS, pages 3440–3448, 2016. (Cited on page 2.)

A. Ghouila-Houri. Caract´erisation des matrices totalement unimodulaires. Comptes Redus
Hebdomadaires des S´eances de l’Acad´emie des Sciences (Paris), 254:1192–1194, 1962. (Cited
on page 11.)

S. Guminov, P. Dvurechensky, N. Tupitsa, and A. Gasnikov. Accelerated alternating mini-
mization, accelerated Sinkhorn’s algorithm and accelerated iterative Bregman projections.
ArXiv Preprint: 1906.03622, 2019. (Cited on pages 2, 3, 5, 6, 16, 17, and 25.)

N. Ho, X. L. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel clustering
via Wasserstein means. In ICML, pages 1501–1509. JMLR. org, 2017. (Cited on page 1.)

A. Jambulapati, A. Sidford, and K. Tian. A direct tilde

O
{

(1/epsilon) iteration parallel
}

algorithm for optimal transport. In NeurIPS, pages 11355–11366, 2019. (Cited on page 2.)

A. Kroshnin, N. Tupitsa, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and C. Uribe. On the
complexity of approximating Wasserstein barycenters. In ICML, pages 3530–3540, 2019.
(Cited on pages 2, 3, 5, 9, 16, 18, 22, 23, 25, and 28.)

N. Lahn, D. Mulchandani, and S. Raghvendra. A graph theoretic additive approximation of

optimal transport. In NeurIPS, pages 13836–13846, 2019. (Cited on page 2.)

T. Le, V. Huynh, N. Ho, D. Phung, and M. Yamada. On scalable variant of Wasserstein

barycenter. ArXiv Preprint: 1910.04483, 2019. (Cited on page 2.)

Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. (Cited on page 30.)

T. Lin, N. Ho, M. Cuturi, and M. I. Jordan. On the complexity of approximating multi-
marginal optimal transport. ArXiv Preprint: 1910.00152, 2019a. (Cited on pages 2, 5, and 25.)

T. Lin, N. Ho, and M. Jordan. On eﬃcient optimal transport: an analysis of greedy and
accelerated mirror descent algorithms. In ICML, pages 3982–3991, 2019b. (Cited on pages 2
and 9.)

T. Lin, N. Ho, and M. I. Jordan. On the eﬃciency of the Sinkhorn and Greenkhorn algorithms
and their acceleration for optimal transport. ArXiv Preprint: 1906.01437, 2019c. (Cited on
page 2.)

33

E. Munch, K. Turner, P. Bendich, S. Mukherjee, J. Mattingly, J. Harer, et al. Probabilistic
fr´echet means for time varying persistence diagrams. Electronic Journal of Statistics, 9(1):
1173–1204, 2015. (Cited on page 1.)

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103

(1):127–152, 2005. (Cited on pages 6 and 10.)

Y. Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization, 22(2):341–362, 2012. (Cited on page 17.)

Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming, 140(1):125–161, 2013. (Cited on page 17.)

Y. Nesterov and S. U. Stich. Eﬃciency of the accelerated coordinate descent method on
structured optimization problems. SIAM Journal on Optimization, 27(1):110–123, 2017.
(Cited on page 17.)

G. Peyr´e and M. Cuturi. Computational optimal transport. Foundations and Trends® in

Machine Learning, 11(5-6):355–607, 2019. (Cited on pages 1, 2, and 4.)

G. Puccetti, L. R¨uschendorf, and S. Vanduﬀel. On the computation of Wasserstein barycenters.

Journal of Multivariate Analysis, 176:104581, 2020. (Cited on page 2.)

K. Quanrud. Approximating optimal transport with linear programs.

In SOSA. Schloss

Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019. (Cited on page 2.)

J. Rabin, G. Peyr´e, J. Delon, and M. Bernot. Wasserstein barycenter and its application to
texture mixing. In International Conference on Scale Space and Variational Methods in
Computer Vision, pages 435–446. Springer, 2011. (Cited on pages 1 and 2.)

R. T. Rockafellar. Convex Analysis, volume 28. Princeton University Press, 1970. (Cited on

page 5.)

B. Schmitzer. Stabilized sparse scaling algorithms for entropy regularized transport problems.

SIAM Journal on Scientiﬁc Computing, 41(3):A1443–A1481, 2019. (Cited on page 30.)

J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. Guibas.
Convolutional Wasserstein distances: eﬃcient optimal transportation on geometric domains.
ACM Transactions on Graphics (TOG), 34(4):1–11, 2015. (Cited on page 30.)

S. Srivastava, C. Li, and D. B. Dunson. Scalable Bayes via barycenter in Wasserstein space.

Journal of Machine Learning Research, 19(1):312–346, 2018. (Cited on page 1.)

M. Staib, S. Claici, J. M. Solomon, and S. Jegelka. Parallel streaming Wasserstein barycenters.

In NIPS, pages 2647–2658, 2017. (Cited on page 2.)

A. Trouv´e and L. Younes. Local geometry of deformable templates. SIAM Journal on Math-

ematical Analysis, 37(1):17–59, 2005. (Cited on page 1.)

C. A. Uribe, D. Dvinskikh, P. Dvurechensky, A. Gasnikov, and A. Nedi´c. Distributed com-
putation of Wasserstein barycenters over networks. In CDC, pages 6544–6549. IEEE, 2018.
(Cited on page 2.)

34

C. Villani. Optimal Transport: Old and New, volume 338. Springer Science & Business Media,

2008. (Cited on pages 1, 3, and 4.)

L. Yang, J. Li, D. Sun, and K. Toh. A fast globally linearly convergent algorithm for the
computation of Wasserstein barycenters. Arxiv Preprint: 1809.04249, 2018. (Cited on pages 2,
4, 25, 26, 27, 28, and 30.)

J. Ye, P. Wu, J. Z. Wang, and J. Li. Fast discrete distribution clustering using wasserstein
barycenter with sparse support. IEEE Transactions on Signal Processing, 65(9):2317–2332,
2017. (Cited on pages 2, 25, and 26.)

35

