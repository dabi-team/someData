OnlineBayesianGoalInferenceforBoundedly-RationalPlanningAgentsTanZhi-Xuan,JordynL.Mann,TomSilverJoshuaB.Tenenbaum,VikashK.MansinghkaMassachusettsInstituteofTechnology{xuan,jordynm,tslvr,jbt,vkm}@mit.eduAbstractPeopleroutinelyinferthegoalsofothersbyobservingtheiractionsovertime.Remarkably,wecandosoevenwhenthoseactionsleadtofailure,enablingustoassistotherswhenwedetectthattheymightnotachievetheirgoals.Howmightweendowmachineswithsimilarcapabilities?Herewepresentanarchitecturecapableofinferringanagent’sgoalsonlinefrombothoptimalandnon-optimalsequencesofactions.Ourarchitecturemodelsagentsasboundedly-rationalplannersthatinterleavesearchwithexecutionbyreplanning,therebyaccountingforsub-optimalbehavior.Thesemodelsarespeciﬁedasprobabilisticprograms,allowingustorep-resentandperformefﬁcientBayesianinferenceoveranagent’sgoalsandinternalplanningprocesses.Toperformsuchinference,wedevelopSequentialInversePlanSearch(SIPS),asequentialMonteCarloalgorithmthatexploitstheonlinereplanningassumptionofthesemodels,limitingcomputationbyincrementallyextendinginferredplansasnewactionsareobserved.WepresentexperimentsshowingthatthismodelingandinferencearchitectureoutperformsBayesianinversereinforcementlearningbaselines,accuratelyinferringgoalsfrombothoptimalandnon-optimaltrajectoriesinvolvingfailureandback-tracking,whilegeneralizingacrossdomainswithcompositionalstructureandsparserewards.1IntroductionEverydayexperiencetellsusthatitisimpossibletoplanaheadforeverything.Yet,notonlydohumansstillmanagetoachieveourgoalsbypiecingtogetherpartialandapproximateplans,wealsoappeartoaccountforthiscognitivestrategywheninferringthegoalsofothers,understandingthattheymightplanandactsub-optimally,orevenfailtoachievetheirgoals.Indeed,even18-montholdinfantsseemcapableofsuchinferences,offeringtheirassistancetoadultsafterobservingthemexecutefailedplans[1].Howmightweunderstandthisabilitytoinfergoalsfromsuchplans?Andhowmightweendowmachineswiththiscapacity,sotheymightassistuswhenourplansfail?Whiletherehasbeenconsiderableworkoninferringthegoalsanddesiresofagents,muchofthisworkhasassumedthatagentsactoptimallytoachievetheirgoals.Evenwhenthisassumptionisrelaxed,theformsofsub-optimalityconsideredareoftenhighlysimpliﬁed.Ininversereinforcementlearning,forexample,agentsareassumedtoeitheractoptimally[2]ortoexhibitBoltzmann-rationalactionnoise[3],whileintheplanrecognitionliterature,longerplansareassignedexponentiallydecreasingprobability[4].Noneoftheseapproachesaccountforthedifﬁcultyofplanningitself,whichmayleadagentstoproducesub-optimalorfailedplans.Thisnotonlymakesthemill-equippedtoinfergoalsfromsuchplans,butalsosaddlesthemwithacognitivelyimplausibleburden:Ifinferringanagent’sgoalsrequiresknowingtheoptimalsolutiontoreacheachgoal,thenanobserverwouldneedtocomputetheoptimalplanorpolicyforallofthosegoalsinadvance[5].Outsideofthesimplestproblemsanddomains,thisisdeeplyintractable.Preprint.Underreview.arXiv:2006.07532v2  [cs.AI]  25 Oct 2020Figure1:OurarchitectureperformingonlineBayesiangoalinferenceviaSequentialInversePlanSearch.In(a),anagentexhibitsasub-optimalplantoacquirethebluegem,backtrackingtopickupthekeyrequiredfortheseconddoor.In(b),anagentexhibitsafailedplantoacquirethebluegem,myopicallyusingupitsﬁrstkeytogetclosertothegeminsteadofrealizingthatitneedstocollectthebottomtwokeys.Inbothcases,ourmethodnotonlymanagestoinferthecorrectgoalbytheend,butalsocapturessharphuman-likeshiftsinitsinferencesatkeypoints,suchas(a.ii)whentheagentpicksupakeyunnecessaryfortheredgem,(a.ii)whentheagentstartstobacktrack,(b.iii)whentheagentignoresthedoortotheredgem,or(b.iv)whentheagentunlockstheﬁrstdoortothebluegem.Inthispaper,wepresentauniﬁedmodelingandinferencearchitecture(Figure2)thataddressesbothoftheselimitations.Incontrasttopriorworkthatmodelsagentsasactorsthatarenoisilyrational,wemodelagentsasplannersthatareboundedlyrationalwithrespecttohowmuchtheyplan,interleavingresource-limitedplansearchwithplanexecution.ThisallowsustoperformonlineBayesianinferenceofplansandgoalsevenfromhighlysub-optimaltrajectoriesinvolvingbacktrackingorirreversiblefailure(Figure1).Wedosobymodelingagentsasprobabilisticprograms(Figure3),comprisedofgoalpriorsanddomain-generalplanningalgorithms(Figure2i),andinteractingwithasymbolicenvironmentmodel(Figure2ii).InferenceisthenperformedviaSequentialInversePlanSearch(SIPS),asequentialMonteCarlo(SMC)algorithmthatexploitsthereplanningassumptionofouragentmodels,incrementallyinferringpartialplanswhilelimitingcomputationalcost(Figure2iii).OurarchitecturedeliversbothaccuracyandspeedbybeingbuiltinGen,ageneral-purposeprob-abilisticprogrammingsystemthatsupportscustomizedinferenceusingdata-drivenproposalsandinvolutiverejuvenationkernels[6,7,8],alongsideanembeddingofthePlanningDomainDeﬁnitionLanguage[9,10],enablingtheuseoffastgeneral-purposeplanners[11]asmodelingcomponents.WeevaluateourapproachagainstaBayesianinversereinforcementlearningbaseline[12]onawidevarietyofplanningdomainsthatexhibitcompositionaltaskstructureandsparserewards(e.g.Figure1),achievinghighaccuracyonmanydomains,oftenwithordersofmagnitudelesscomputation.2Sequential Inverse Plan SearchBoundedly RationalAgent ProgramGoal Priorover logical speciﬁcationsor reward functionsOnline Replannere.g. A*, Probabilistic A*, HSPPlanning Algorithme.g. Manhattan, haddPlanning HeuristicDomain-GeneralPDDLEnvironment ModelState Representatione.g. on(A, B), at(Item, X, Y)Logical Predicatese.g. xpos, ypos, fuel-usedNumeric FluentsTransition Operatorse.g. stack(A, B)Precond.: holding(A) & clear(B)  Eﬀect: ¬holding(A) & on(A, B)ActionsOnline Plan Hypothesis Extension[right, down, pickup][right, up][right, up]Goal 2Goal 1[right, down, pickup][right, up, up, left, pickup][right, up, up, right]t = 2t = 3Goal Pruning & Rejuvenationg2g1g1g2g3g3g3g1g3g3g3g3g5g1g4g3g3g3ResamplingRejuvenationInferenceObservations(i)(ii)(iii)Figure2:Ourmodelingandinferencearchitectureiscomprisedof:(i)Aprogrammaticmodelofaboundedlyrationalplanningagent,implementedintheGenprobabilisticprogrammingsystem;(ii)AnenvironmentmodelspeciﬁedinthePlanningDomainDeﬁnitionLanguage(PDDL),facilitatingsupportforawidevarietyofplanningdomainsandstate-of-the-artsymbolicplanners;(iii)SequentialInversePlanSearch(SIPS),anovelSMCalgorithmthatexploitsthereplanningassumptionofouragentmodeltoreducecomputation,extendinghypothesizedplansonlyasnewobservationsarrive.2RelatedWorkInversereinforcementlearning(IRL).Alonglineofworkhasshownhowtolearnrewardfunctionsasexplanationsofgoal-directedagentbehaviorviainversereinforcementlearning[2,13,12,14].However,mostsuchapproachesaretoocostlyforonlinesettingsofcomplexdomains,astheyrequiresolvingtheunderlyingMarkovDecisionProcess(MDP)foreverypositedgoalorrewardfunction,andforallpossibleinitialstates[15,5].Ourapproachinsteadassumesthatagentsareonlinemodel-basedplanners.Thisgreatlyreducescomputationtime,whilealsobetterreﬂectinghumans’intuitiveunderstandingofotheragents.Bayesiantheory-of-mind(BToM).Computationalmodelsofhumans’intuitivetheory-of-mindpositthatweunderstandother’sactionsbyBayesianinferenceoftheirlikelygoalsandbeliefs.Thesemodels,largelybuiltuponthesameMDPformalismusedinIRL,havebeenshowntomakepredictionsthatcorrespondcloselywithhumaninferences[16,17,18,19,20,21,22].Somerecentworkalsomodelsagentsusingprobabilisticprograms[23,24].Ourresearchextendsthislineofworkbyexplicitlymodelinganagent’spartialplans,orintentions[25].Thisallowsourarchitecturetoinferﬁnalgoalsfrominstrumentalsubgoalsproducedaspartofaplan,andtoaccountforsub-optimalityinthoseplans,therebyenrichingtherangeofmentalinferencesthatBToMmodelscanexplain.Planrecognitionasplanning(PRP).Ourworkisrelatedtotheliteratureonplanrecognitionasplanning,whichperformsgoalandplaninferencebyusingclassicalsatisﬁcingplannerstomodelplanlikelihoodsgivenagoal[26,4,27,28,29,30].However,becausetheseapproachesuseaheuristiclikelihoodmodelthatassumesgoalsarealwaysachievable,theyareunabletoinferlikelygoalswhenirreversiblefailuresoccur.Incontrast,wemodelagentsasonlineplannerswhomayoccasionallyexecutepartialplansthatleadtodeadends.Onlinegoalinference.SeveralrecentpapershaveextendedIRLtoanonlinesetting,butthesehaveeitherfocusedonmaximum-likelihoodestimationin1Dstatespaces[31,32],orutilizeanexpensivevalueiterationsubroutinethatisunlikelytoscale[33].Incontrast,wedevelopasequentialMonteCarloalgorithmthatexploitstheonlinenatureoftheagentmodelsinordertoperformincrementalplaninferencewithlimitedcomputationcost.Inferencesfromsub-optimalbehavior.Webuilduponagrowingbodyofresearchoninferringgoalsandpreferenceswhileaccountingforhumansub-optimality[3,24,34,35,36,37],introducingamodelofboundedly-rationalplanningasresource-limitedsearch.Thisreﬂectsanaturalprincipleofresourcerationalityunderwhichagentsarelesslikelytoengageincostlycomputations[38,39].Unlikepriormodelsofmyopicagentswhichassignzerorewardtofuturestatesbeyondsometimehorizon[34,36],ourapproachaccountsformyopicplanningindomainswithinstrumentalsubgoalsandsparserewards.3gGoalp1Plana1Actions2States1StatePlana2Actionp2PlanActionp3a3s3StateActionPlanp4a4s4StatePlanActionp5a5s5Stateo2Obs.o1Obs.o3Obs.o4Obs.o5Obs.η1SearchBudgetη3SearchBudget(a)Onerealizationofouragentandenvironmentmodel.modelUPDATE-PLAN(t,st,pt−1,g)parameters:PLANNER,r,q,γ,hift>LENGTH(pt−1)orst/∈pt−1[t]thenη∼NEGATIVE-BINOMIAL(r,q)˜pt∼PLANNER(st,g,h,γ,η)pt←APPEND(pt−1,˜pt)elsept←pt−1endifreturnptendmodel(i)SamplesfromP(pt|st,pt−1,g)modelSELECT-ACTION(t,st,pt)returnpt[t][st]endmodel(ii)SamplesfromP(at|st,pt)(b)Boundedly-rationalagentprograms.Figure3:Wemodelagentsasboundedlyrationalplannersthatinterleavesearchandexecutionofpartialplansastheyinteractwiththeenvironment.In(a)wedepictonepossiblerealizationofthismodel,wheretheagentinitiallysamplesasearchbudgetη1andsearchesforaplanp1thatistwoactionslong.Att=2,noadditionalplanningneedstobedone,sop2iscopiedfromp1,asdenotedbythedashedlines.Theagentthenreplansatt=3fromstates3,samplinganewsearchbudgetη3andanextendedplanp3withthreemoreactions.Weformallyspecifythisagentmodelusingprobabilisticprograms,withpseudo-codeshownin(b).UPDATE-PLANsamplesextendedplansptgivenpreviousplanspt−1,whileSELECT-ACTIONselectsanactionataccordingthecurrentplanpt.3Boundedly-RationalPlanningAgentsInordertoaccountforsub-optimalbehaviorduetoresource-limitedplanning,observersneedtomodelnotonlyanagent’sgoalsandactions,butalsotheplanstheyformtoachievethosegoals.Assuch,wemodelagentsandtheirenvironmentsasgenerativeprocessesofthefollowingform:Goalprior:g∼P(g)(1)Planupdate:pt∼P(pt|st,pt−1,g)(2)Actionselection:at∼P(at|st,pt)(3)Statetransition:st+1∼P(st+1|st,at)(4)Observationnoise:ot+1∼P(ot+1|st+1)(5)whereg,pt,at,staretheagent’sgoals,theinternalstateoftheagent’splan,theagent’saction,andtheenvironment’sstateattimetrespectively.Forthepurposesofgoalinference,observersalsoassumethateachstatestmightbesubjecttoobservationnoise,producinganobservedstateot.Thisgenerativeprocess,depictedinFigure3a,extendsthestandardmodelofMDPagentsbymodelingplansandplanupdatesexplicitly,allowingustorepresentnotonlyagentsthatactaccordingtosomeprecomputedpolicyat∼π(at|st),butalsoagentsthatcomputeandupdatetheirplanspton-the-ﬂy.Wedescribeeachcomponentofthisprocessingreaterdetailbelow.3.1ModelingGoals,StatesandObservationsTorepresentstates,observations,goals,anddistributionsovergoalsinageneralandﬂexiblemanner,ourarchitectureembedsthePlanningDomainDeﬁnitionLanguage(PDDL)[9,10],representingstatesstandgoalsgintermsofpredicate-basedfacts,relations,andnumericexpressions(Figure2ii).StatetransitionsP(st|st−1,at−1)aremodeledbytransitionoperatorsthatspecifythepreconditionsandeffectsofactions.Whilewefocusondeterministictransitionsinthispaper,wealsosupportstochastictransitions,asinProbabilisticPDDL[40].Giventhisrepresentation,anobserver’spriorovergoalsP(g)canbespeciﬁedasaprobabilisticprogramoverPDDLgoalspeciﬁcations,includingnumericrewardfunctions,aswellassetsofgoalpredicates(e.g.has(gem)),equivalenttoindicatorrewardfunctions.ObservationnoiseP(ot+1|st+1)canalsobemodeledbycorruptingeachBooleanpredicatewithsomeprobability,andaddingcontinuous(e.g.Gaussian)noisetonumericﬂuents.43.2ModelingSub-OptimalPlansandActionsTomodelsub-optimalplans,thebasicinsightwefollowisthatagentslikeourselvesareboundedlyrational:weattempttoplantoachieveourgoalsefﬁciently,butarelimitedbyourcognitiveresources.Theprimarylimitationweconsideristhatfull-horizonplanningisoftencostlyorintractable.Instead,itmayoftenmakesensetoformpartialplanstowardspromisingintermediatestates,executethem,andreplanfromthere.Wemodelthisbyassumingthatagentsonlysearchforaplanuptosomebudgetη,beforeexecutingapartialplantoapromisingstatefoundduringsearch.Weoperationalizeηasthemaximumnumberofnodesexpanded(i.e.,statesexplored),whichwetreatasarandomvariablesampledfromanegativebinomialdistribution:η∼NEGATIVE-BINOMIAL(r,q)(6)Theparametersr(maximumfailurecount)andq(continuationprobability)characterizetheper-sistenceofaplannerwhomaychoosetogiveupafterexpandingeachnode.Whenr>1,thisdistributionpeaksatmediumvaluesofη,thendecreasesexponentially,modelingagentsthatareunlikelytoformextremelylongplans,whicharecostly,orextremelyshortplans,whichareunhelpful.Thismodelalsoassumesaccesstoaplanningalgorithmcapableofproducingpartialplans.Whilewesupportanysuchplannerasasub-component,inthisworkwefocusonA*searchduetoitsabilitytosupportdomain-generalheuristicsthatcanguidesearchinhuman-likeways[11,41].WealsomodifyA*sothatsearchisstochastic,modelingagentsub-optimalityduringsearch.Inparticular,insteadofalwaysexpandingthemostpromisingsuccessorstate,wesamplesuccessorswithprobability:Pexpand(s)∝exp(−f(s,g)/γ)(7)whereγisanoiseparametercontrollingtherandomnessofsearch,andf(s,g)=c(s)+h(s,g)istheestimatedtotalplancost,i.e.thesumofthepathcostc(s)sofarwiththeheuristicgoaldistanceh(s,g).Ontermination,wesimplyreturnthemostrecentlyselectedsuccessorstate,whichislikelytohavelowtotalplancostf(s,g)iftheheuristich(s,g)isinformativeandthenoiseγislow.Weincorporatetheselimitationsintoamodelofhowaboundedlyrationalplanningagentinterleavessearchandexecution,speciﬁedbytheprobabilisticprogramsUPDATE-PLANandSELECT-ACTIONinFigure3b.Ateachtimet,theagentmayreachtheendofitslastmadeplanpt−1orencounterastatestnotanticipatedbytheplan,inwhichcaseitwillcallthebaseplanner(probabilisticA*)witharandomlysamplednodebudgetη.Thepartialplanproducedisthenusedtoextendtheoriginalplan.Otherwise,theagentwillsimplycontinueexecutingitsoriginalplan,performingnoadditionalcomputation.Notethatbyreplanningwhentheunexpectedoccurs,theagentautomaticallyhandlessomeamountofstochasticity,aswellaserrorsinitsenvironmentmodel.4OnlineBayesianGoalInferenceHavingspeciﬁedourmodel,wecannowstatetheproblemofBayesiangoalinference.Weassumethatanobserverreceivesasequenceofpotentiallynoisystateobservationso1:t=(o1,...,ot).GiventheobservationsuptotimesteptandasetofpossiblegoalsG,theobserver’saimistoinfertheagent’sgoalg∈Gbycomputingtheposterior:P(g|o1:t)∝P(g)Ps1:ta1:tp1:tQt−1τ=0P(oτ+1|sτ+1)P(sτ+1|sτ,aτ)P(aτ|sτ,pτ)P(pτ|sτ,pτ−1,g)(8)Computingthisposteriorexactlyisintractable,asitrequiresmarginalizingoveralltherandomlatentvariablessτ,aτ,andpτ.Instead,wedevelopasequentialMonteCarloprocedure,showninAlgorithm1,toperformapproximateinferenceinanonlinemanner,usingsamplesfromtheposteriorP(g|o1:t−1)attimet−1toinformsamplingfromtheposteriorP(g|o1:t)attimet.WecallthisalgorithmSequentialInversePlanSearch(SIPS),becauseitsequentiallyinvertsasearch-basedplanningalgorithm,inferringsequencesofpartialplansthatarelikelygiventheobservations,andconsequentlythelikelygoals.Asinstandardparticleﬁlteringschemes,weﬁrstsampleasetofparticlesorhypothesesi∈[1,k],withcorrespondingweightswi(lines3-5).Eachparticlecorrespondstoaparticularplanpiτandgoalgi.Aseachnewobservationoτarrives,weextendtheparticles(lines12–14)andreweightthembytheirlikelihoodofproducingthatobservation(line15).Thecollectionofweightedparticlesthusapproximatesthefullposteriorovertheunobservedvariablesinourmodel,includingtheagent’splansandgoals.Wedescribeseveralkeyfeaturesofthisalgorithmbelow.5Algorithm1SequentialInversePlanSearch(SIPS)foronlineBayesiangoalinference1:procedureSIPS(s0,o1:t,)2:parameters:k,numberofparticles;c,resamplingthreshold3:wi←1fori∈[1,k].Initializeparticleweights4:si0,pi0,ai0←s0,[],no-opfori∈[1,k].Initializestates,plansandactions5:gi∼GOAL-PRIOR()fori∈[1,k].Samplekparticlesfromgoalprior6:forτ∈[1,t]do7:ifEFFECTIVE-SAMPLE-SIZE(w1,...,wk)/k<cthen.Resampleandrejuvenate8:gi,si1:τ,pi1:τ,ai1:τ∼RESAMPLE([gi,s1:τ,p1:τ,a1:τ]1:k)fori∈[1,k]9:gi,si1:τ,pi1:τ,ai1:τ∼REJUVENATE(gi,o1:τ,si1:τ,pi1:τ,ai1:τ)fori∈[1,k]10:endif11:fori∈[1,k]do.Extendeachparticletotimestepτ12:siτ∼P(sτ|siτ−1,aiτ−1).Samplestatetransition13:piτ∼UPDATE-PLAN(pτ|siτ,piτ−1,gi).Extendplanifnecessary14:aiτ∼SELECT-ACTION(aτ|siτ,piτ).Selectaction15:wi←wi·P(oτ|siτ).Updateparticleweight16:endfor17:endfor18:˜wi←wi/Pkj=1wjfori∈[1,k].Normalizeparticleweights19:return[(g1,w1),...,(gk,wk)].Returnweightedgoalparticles20:endprocedure21:22:procedureREJUVENATE(g,o1:τ,s1:τ,p1:τ,a1:τ).Metropolis-Hastingrejuvenationmove23:parameters:pg,goalrejuvenationprobability24:ifBERNOULLI(pg)then.Heuristic-drivengoalproposal25:g0∼Q(g):=SOFTMAX([h(oτ,g)forg∈G]).Proposeg00basedonest.distancetooτ26:s01:τ,p01:τ,a01:τ∼P(s1:τ,p1:τ,a1:τ|g).Sampletrajectoryundernewgoalg27:α←Q(g)/Q(g0).Computeproposalratio28:else.Error-drivenreplanningproposal29:t∗∼Q(t∗|s1:τ,o1:τ).Sampleatimeclosetowhens1:τdivergesfromo1:τ30:s0t∗:τ,p0t∗:τ,a0t∗:τ∼Q(st∗:τ,pt∗:τ,at∗:τ|ot∗:τ).Proposenewplansequencep0t∗:τ31:α←Q(st∗:τ,pt∗:τ,at∗:τ|ot∗:τ)/Q(s0t∗:τ,p0t∗:τ,a0t∗:τ|ot∗:τ).Computeproposalratio32:α←α·Q(t∗|s01:τ,o1:τ)/Q(t∗|s1:τ,o1:τ).Reweightbyauxiliaryproposalratio33:endif34:α←α·P(o1:τ|s01:τ)/P(o1:τ|s1:τ).Computeacceptanceratio35:returng00,s01:τ,p01:τ,a01:τifBERNOULLI(min(α,1))elseg0,s1:τ,p1:τ,a1:τ.Acceptorrejectproposals36:endprocedure4.1OnlineExtensionofHypothesizedPartialPlansAkeyaspectthatmakesSIPSagenuinelyonlinealgorithmisthemodelingassumptionthatagentsalsoplanonline.Thisobviatestheneedfortheobservertoprecomputeacompleteplanorpolicyforeachoftheagent’spossiblegoalsinadvance,andinsteaddeferssuchcomputationtothepointwheretheagentreachesatimetthattheobserver’shypothesizedplansdonotyetreach.Inparticular,foreachparticlei,thecorrespondingplanhypothesispit−1isextended(Algorithm1,line13)byrunningtheUPDATE-PLANprocedureinFigure3b.i,whichonlyperformsadditionalcomputationifpit−1doesnotalreadycontainaplannedactionfortimetandstatest.Thismeansthatatanygiventimet,onlyasmallnumberofplansrequireextension,limitingthenumberofexpensiveplanningcalls.4.2ManagingHypothesisDiversityviaResamplingandRejuvenationWealsointroduceresamplingandrejuvenationstepsintoSIPSinordertoensureparticlediversity.Whenevertheeffectivesamplesizefallsbelowathresholdc(line7),weresampletheparticles(line8),therebypruninglow-weighthypotheses.Wethenrejuvenatebyapplyingamixtureoftwodata-drivenMetropolis-Hastingskernelstoeachparticle.Theﬁrstkernelusesaheuristic-drivengoalproposal(lines25-27),proposinggoals˜g∈Gwhicharecloseinheuristicdistanceh(oτ,˜g)tothelastobservedstateoτ.ThisallowsSIPStoreintroducegoalsthatwerepruned,butlaterbecomemorelikely.Thesecondkernelusesanerror-drivenreplanningproposal(lines29-32),whichsamplesatimeclosetothedivergencepointbetweenthehypothesizedandobservedtrajectories,andthenproposestoreplanfromthattime,therebyconstructinganewsequenceofhypothesizedpartialplansthatarelesslikelytodivergefromtheobservations.Despitethecomplexityoftheseproposals,acceptanceratiosareautomaticallycalculatedviaGen’ssupportforinvolutivekernels[8].Collectively,thesestepshelptoensurethathypothesesarebothdiverseandlikelygiventheobservations.65ExperimentsWeconductedseveralsetsofexperimentsthatdemonstratethehuman-likeness,accuracy,speed,androbustnessofourapproach.WeﬁrstpresentexperimentsdemonstratingthenovelcapacityofSIPStoinfergoalsfromsub-optimaltrajectoriesinvolvingbacktrackingandfailure(Figure1).ComparingtheseinferencesagainsthumangoalinferencesshowsthatSIPSismorehuman-likethanbaselineapproaches(Figure4).WealsoevaluatetheaccuracyandspeedofSIPSonavarietyofplanningdomains(Table1a),showingthatitoutperformsBayesianIRLbaselines.Finally,wepresentrobustnessexperimentsshowingthatSIPScaninfergoalsevenwhenthedata-generatingmodeldiffersfromthemodelassumedbythealgorithm(Table1b).5.1DomainsWevalidateourapproachondomainswithvaryingdegreesofcomplexity,bothintermsofthesizeofthestatespace|S|andthenumberofpossiblegoals|G|.Alldomainsarecharacterizedbycompositionalstructureandsparserewards,posingachallengeforstandardMDP-basedapproaches.Taxi(|G|=3,|S|=125):Abenchmarkdomainusedinhierarchicalreinforcementlearning[42],whereataxihastotransportapassengerfromonelocationtoanotherinagridworld.Doors,Keys,&Gems(|G|=3,|S|∼105):Adomaininwhichanagentmustnavigateamazewithdoors,keys,andgems(Figure1).Eachkeycanbeusedoncetounlockadoor,allowingtheagenttoacquireitemsbehindthatdoor.Goalscorrespondtoacquiringoneoutofthreecoloredgems.BlockWords(|G|=5,|S|∼105):ABlocksWorldvariantadaptedfrom[4]whereblocksarelabeledwithletters.GoalscorrespondtoblocktowersthatspelloneofasetofﬁveEnglishwords.IntrusionDetection(|G|=20,|S|∼1030):Acybersecurity-inspireddomaindrawnfrom[4],whereanagentmightperformavarietyofattacksonasetofservers.Thereare20possiblegoals,eachcorrespondingtoasetofattacks(e.g.cyber-vandalismordata-theft)onupto10servers.5.2BaselinesWeimplementedBayesianIRL(BIRL)baselinesbyrunningvalueiterationtocomputeaBoltzman-rationalpolicyπ(at|st,g)foreachpossiblegoalg∈G.FollowingthesettingofearlyBayesiantheory-of-mindapproaches[18],wetreatedgoalsasindicatorrewardfunctions,andassumedauniformpriorP(g)overgoals.Inferencewasthenperformedbyexactcomputationoftheposterioroverrewardfunctions,usingthepolicyasthelikelihoodforobservedactions.Unlessotherwisestated,weusedadiscountfactorof0.9,andBoltzmannnoiseparameterα=1.Duetotheexponentiallylargestatespaceofmanyofourdomains,standardvalueiteration(VI)oftenfailedtoconvergeevenafter106iterations.Assuch,weimplementedtwovariantsofBIRLthatuseasynchronousVI,samplingstatesinsteadoffullyenumeratingthem.Theﬁrst,unbiasedBIRL,usesuniformrandomsamplingofthestatespaceupto250,000iterations,sufﬁcientforconvergenceintheBlockWordsdomain.Thesecond,oracleBIRL,assumesoracularaccesstothefullsetofobservedtrajectoriesinadvance,andperformingbiasedsamplingofstatesthatappearinthosetrajectories.Althoughinapplicableinpracticeforonlineuse,thisensuresthatthecomputedpolicyisabletoreachthegoalinallcases,makingitausefulbenchmarkforcomparison.5.3Human-LikeGoalInferencefromSub-optimalandFailedPlansToinvestigatethenovelhuman-likecapabilitiesofourapproach,weperformedasetofqualitativeexperimentsonasetoftrajectoriesdesignedtoexhibitnotablesub-optimalityorfailure.TheexperimentswereperformedontheDoors,Keys&Gemsdomainbecauseitallowsforirreversiblefailures.TwoillustrativeexamplesareshowninFigure1,andmoreareprovidedinthesupplement.InFigure1a,SIPSaccuratelyinfersgoalsfromasub-optimalplanwithbacktracking,initiallyplacingmoreposteriormassontheyellowgemwhentheagentacquirestheﬁrstkey(panelii),butthenswitchingtothebluegemoncetheagentbacktrackstothesecondkey(paneliv).InFigure1b,SIPSremainsuncertainaboutallthreegoalswhentheﬁrstkeyisacquired(panelii),butdiscardstheredgemasapossibilitywhentheagentwalkspastthedoor(paneliii),andﬁnallyconvergesuponthebluegemwhentheagentmyopicallyunlockstheﬁrstdoorrequiredtoaccessthatgem(paneliv).7(c) Bayesian IRL (oracle)(b) Sequential Inverse Plan Search (ours)(a) Human Inferences (cross-subject average)TimeTimeTimer = 0.99r = 0.80(d) Correlation between humans and algorithmsFigure4:(a)Averagehumangoalinferencesovertime(±1std.)forthesub-optimaltrajectoryinFigure1a,comparedto(b)inferencesmadebySIPSand(c)oracleBIRL.WeomitunbiasedBIRLbecauseunbiasedVIfailstoconvergeforthisdomain,producingaﬂatposterior.In(d)weshowascatterplotofmeanhumaninferencesagainstalgorithminferencesacrossalltrajectories.Inadditiontoposteriorconvergence,theinferencesmadebySIPSdisplayhuman-likechangesatkeytimepoints.Wequantiﬁedthishuman-likenessbycollectinghumangoalinferencesontentrajectories(sixsub-optimalorfailed)inapilotstudywithN=8subjects.Humaninferenceswerecollectedeverysixtimesteps,andacomparisonagainstSIPSandtheoracleBIRLbaselineisshowninFigure4.ForthetrajectoryininFigure1a,humaninferences(Figure4a)displayextremelysimilarqualitativetrendsasSIPS(Figure4b,r=0.99).OracleBIRLcorrelateslesswell(Figure4c,r=0.80),assigninghighprobabilitytotheyellowgemevenaftertheagentbacktracksatt≥18.ThisisbecauseBoltzmannactionnoiseassignssigniﬁcantlikelihoodtotheundoingofpastactions.AsFigure4dshows,SIPSalsocorrelatesmorestronglywithmeanhumaninferencesacrossthedataset.InferencesmadeSIPS(yellow)hewcloselytothediagonal,achievingacorrelationofr=0.89,indicatingthattheagentmodelassumedbySIPSissimilartohumans’theory-of-mind.Incontrast,inferencesmadebyBIRL(blue)aremuchmorediffuse,achievingacorrelationofonlyr=0.51.5.4Accuracy,SpeedandRobustnessofInferenceToevaluateaccuracyandspeed,weraneachinferencemethodonadatasetofoptimalandnon-optimalagenttrajectoriesforeachdomain,assumingauniformpriorovergoals.TheoptimaltrajectoriesweregeneratedusingA*searchwithanadmissibleheuristicforeachpossiblegoalinthedomain.Non-optimaltrajectoriesweregeneratedusingthereplanningagentmodelinFigure3b,withparametersr=2,q=0.95,γ=0.1.Wefoundthatwithmatchedmodelparameters,SIPSachievedgoodperformancewith10particlespergoalwithouttheuseofrejuvenationmoves,sowereportthoseresultshere.Furtherexperimentaldetailsandparameterscanbefoundinthesupplement.WesummarizetheresultsoftheseexperimentsinTable1a,withadditionalresultsinthesupplement.OurmethodgreatlyoutperformstheunbiasedBIRLbaselineinbothaccuracyandspeedinthreeoutoffourdomains,withanaverageruntime(AC)oftenseveralordersofmagnitudesmaller.ThisislargelybecauseunbiasedVIfailstoconvergeexceptforthehighlyrestrictedTaxidomain.Incontrast,SIPSrequiresfarlessinitialcomputation,albeitwithhighermarginalcostdueitsonlinegenerationofpartialplans.Infact,itachievescomparableaccuracyandspeedtotheoracleBIRLbaseline,sometimeswithlesscomputation(e.g.inDoors,Keys&Gems).SIPSalsoproduceshigherestimatesofthegoalposteriorP(gtrue|o).Thisisareﬂectionoftheunderlyingagentmodel,whichassumesrandomnessatthelevelofplanninginsteadofacting.Asaresult,evenafewobservationscanprovidesubstantialevidencethataparticularplanandgoalwaschosen.Giventhespeciﬁcassumptionsmadebyouragentmodel,areasonablequestioniswhetherinferenceisrobusttoplansgeneratedbyotheragentmodelsoractualhumans.Toaddressthis,wealsoperformedaseriesofrobustnessexperimentsfortwodomains(Table1b)ondatageneratedbymismatchedmodelparametersr,q,γ,mismatchedplanningheuristicsh,Boltzmann-rationalRLagents,optimalagents,and5pilothumansubjects(30trajectoriespersubject).8AccuracyRuntimeDomainMethodP(gtrue|o)Top-1C0(s)MC(s)AC(s)NQ1Q2Q3Q1Q2Q3Taxi(3Goals)SIPS(ours)0.440.500.620.530.560.6713.01.802.551429BIRL(unbiased)0.340.350.790.330.420.922.220.000.1610000BIRL(oracle)0.370.470.810.420.440.861.630.000.122500Doors,Keys&Gems(3Goals)SIPS(ours)0.370.510.610.740.740.743.300.700.862099BIRL(unbiased)0.330.330.330.330.330.3333260.12154250000BIRL(oracle)0.370.360.420.440.600.801500.127.0110000BlockWords(5Goals)SIPS(ours)0.470.830.900.780.840.9120.82.464.152506BIRL(unbiased)0.200.200.210.420.490.566870.2763.6250000BIRL(oracle)0.200.290.450.730.800.9622.20.052.1210000IntrusionDetection(20Goals)SIPS(ours)0.560.870.870.650.870.873756.6028.013321BIRL(unbiased)0.050.050.050.050.050.05180380.751069250000BIRL(oracle)0.090.240.530.941.001.00980.026.0010000(a)Accuracyandruntimeofgoalinferenceacrossdomainsandinferencemethods.Wequantifyaccuracyatthe1st,2ndand3rdquartiles(Q1–Q3)ofeachobservedtrajectoryviatheposteriorprobabilityofthetruegoalP(gtrue|o),andthefractionofproblemswheregtrueistop-ranked(Top-1).Wemeasureruntimeintermsofthestart-upcost(C0),marginalcostpertimestep(MC),andaveragecostpertimestep(AC)inseconds.Wealsoreportthetotalnumberofstatesvisited(N)duringeithersearchorvalueiterationasaplatform-independentmeasure.Excludingtheoraclebaseline,thebestmetricsarebolded.Persistence(r)Persistence(q)RLOptimalDomain12*40.80.90.95*α=50Doors,Keys,Gems0.600.730.730.530.600.730.580.80BlockWords0.900.870.900.700.830.870.820.80SearchNoise.(γ)Heuristic(h)HumansDomain0.50.1*0.02Mh.*Mz.GC.hadd*n=5Doors,Keys,Gems0.670.730.770.730.90––0.79BlockWords0.830.870.87––0.430.870.73(b)Robustnesstomodelmismatch.Top-1accuracyofSIPSatthethirdtimequartile(Q3),evaluatedondatageneratedbymismatchedparameters,Boltzmann-rationalRLagents,optimalagents,andhumans.WeranSIPSassumingr=2,q=0.95,T=10.ForDoors,Keys,Gems,weassumedaManhattan(Mh.)heuristicagainstamazedistance(Mz.)heuristic.ForBlockWords,weassumedhaddagainstthenaivegoalcount(GC.)heuristic.Matchedparametersarestarred(*).Table1:Accuracy,runtime,androbustnessofinference.AsTable1bshows,SIPSisrelativelyrobusttodatageneratedbytheseothermodelsandparameters.Althoughperformancecandegradewithmismatch,thisispartlyduetothedifﬁcultyofinferencefromhighlyrandombehavior(e.g.q=0.8,h=GC.).Ontheotherhand,whenmismatchedparametersaremoreoptimal,performancecanimprove(e.g.h=Mz.).Importantly,SIPSalsodoeswellonhumandata,showingrobustnessevenwhentheplannerisunknown.Whileourboundedlyrationalagentmodelcannotpossiblycaptureallaspectsofhumanplanning,theseexperimentssuggestthatitisservesasareasonableapproximation,similartoourintuitivetheoriesofotherpeople’sminds.6LimitationsandFutureWorkInthispaper,wedemonstratedanarchitecturecapableofonlineinferenceofgoalsandplans,evenwhenthoseplansmightfail.However,importantlimitationsremain.First,weconsideredonlyﬁnitesetsofgoals,butthespaceofgoalsthathumanspursueiseasilyinﬁnite.Relatedly,weassumethatthesegoalsareﬁnal,insteadofaccountingforthehierarchicalandinstrumentalnatureofgoalsandplans.Apromisingnextstepwouldthusbetoexpresshierarchiesofgoalsandplansasprobabilisticgrammarsorprograms[43,44,45],capturingboththeinﬁnitudeandstructureofthemotivesweattributetoeachother[46,47].Second,unlikethedomainsconsideredhere,theenvironmentsweoperateinofteninvolvestochasticdynamicsandinﬁniteactionspaces[48,49].AnaturalextensionwouldbetointegrateMonteCarloTreeSearchorsample-basedmotionplannersintoourarchitectureasmodelingcomponents[23],potentiallyparameterizedbylearnedheuristics[50].Withhope,ourarchitecturemightthenapproachthefullcomplexityofproblemsthatwefaceeveryday,whetheroneisstackingblocksasakid,ﬁndingtherightkeysfortherightdoors,orwritingaresearchpaper.97BroaderImpactWeembarkeduponthisresearchinthebeliefthat,asincreasinglypowerfulautonomoussystemsbecomeembeddedinoursociety,itmayeventuallybecomenecessaryforthemtoaccuratelyunder-standourgoalsandvalues,soastorobustlyactinourcollectiveinterest.Crucially,thiswillrequiresuchsystemstounderstandthewaysinwhichhumansroutinelyfailtoachieveourgoals,andnottakethatasevidencethatthosegoalswereneverdesired.Duetoourmanifoldcognitivelimitations,gapsemergebetweenourgoalsandourintentions,ourintentionsandouractions,ourbeliefsandourconclusions,andouridealsandourpractices.Totheextentthatwewouldlikemachinestoaidusinactualizingthegoalsandidealswemostvalue,ratherthanthoseweappeartobeactingtowards,itwillbecriticalforthemtounderstandhow,when,andwhythosegapsemerge.Thisaspectofthevaluealignmentproblemhasthusfarbeenunder-explored[51].ByperformingthisresearchattheintersectionofcognitivescienceandAI,wehopetolaysomeoftheconceptualandtechnicalgroundworkthatmaybenecessarytounderstandourboundedly-rationalbehavior.Ofcourse,theabilitytoinferthegoalsofothers,andtodosoonlineanddespitefailures,hasmanymoreimmediateuses,eachofthemwithitsownsetofbeneﬁtsandrisks.Perhapsthemoststraightforwardlybeneﬁcialareassistiveusecases,suchassmartuserinterfaces[52],intelligentpersonalassistants,andcollaborativerobots,whichmayoffertoaidauserifthatuserappearstobepursuingasub-optimalplan.However,eventhoseusecasescomewiththeriskofreducinghumanautonomy,andcareshouldbetakensothatsuchapplicationsensuretheautonomyandwillingconsentofthosebeingaided[53].Moreconcerninghoweveristhepotentialforsuchtechnologytobeabusedformanipulative,offensive,orsurveillancepurposes.Whiletheresearchpresentedinthispaperisnowherenearthelevelofintegrationthatwouldbenecessaryforactivesurveillanceormanipulation,itishighlylikelythatmatureversionsofsimilartechnologywillbeco-optedforsuchpurposesbygovernments,militaries,andthesecurityindustry[54,55].Althoughdetectingandinferring“suspiciousintent”maynotseemharmfulinitsownright,theseusesneedtobeconsideredwithinthebroadercontextofsociety,especiallythewaysinwhichmarginalizedpeoplesareover-policedandincarcerated[56].Giventheserisks,weurgefutureresearchonthistopictoconsiderseriouslythewaysinwhichtechnologyofthissortwillmostlikelybeused,bywhichinstitutions,andwhetherthoseuseswilltendtoleadtojustandbeneﬁcialoutcomesforsocietyasawhole.Theabilitytoinferandunderstandthemotivesofothersisaskillthatcanbewieldedtobothgreatbeneﬁtandgreatharm.Weoughttouseitwisely.8CodeAvailabilityCodeforthearchitectureandexperimentspresentedinthispaperisavailableathttps://github.com/ztangent/Plinf.jl/tree/neurips-2020-experiments,aspartofthePlinf.jlpack-ageforBayesianinverseplanning.9AcknowledgementsThisworkwasfundedinpartbytheDARPAMachineCommonSenseprogram(AwardID:030523-00001);philanthropicgiftsfromtheAphorismFoundationandfromtheSiegelFamilyFoundation;andﬁnancialsupportfromtheMIT-IBMWatsonAILabandtheIntelProbabilisticComputingCenter.TomSilverissupportedbyanNSFGraduateResearchFellowship.References[1]FelixWarnekenandMichaelTomasello.Altruistichelpinginhumaninfantsandyoungchimpanzees.Science,311(5765):1301–1303,2006.[2]AndrewYNgandStuartJRussell.Algorithmsforinversereinforcementlearning.InPro-ceedingsoftheSeventeenthInternationalConferenceonMachineLearning,pages663–670,2000.[3]BrianDZiebart,AndrewLMaas,JAndrewBagnell,andAnindKDey.Maximumentropyinversereinforcementlearning.InAAAI,volume8,pages1433–1438.Chicago,IL,USA,2008.10[4]MiguelRamírezandHectorGeffner.Probabilisticplanrecognitionusingoff-the-shelfclassicalplanners.InTwenty-FourthAAAIConferenceonArtiﬁcialIntelligence,2010.[5]BernardMichiniandJonathanPHow.Improvingtheefﬁciencyofbayesianinversereinforce-mentlearning.In2012IEEEInternationalConferenceonRoboticsandAutomation,pages3651–3656.IEEE,2012.[6]MarcoFCusumano-Towner,FerasASaad,AlexanderKLew,andVikashKMansinghka.Gen:ageneral-purposeprobabilisticprogrammingsystemwithprogrammableinference.InProceedingsofthe40thACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation,pages221–236.ACM,2019.[7]MarcoFCusumano-TownerandVikashKMansinghka.Usingprobabilisticprogramsasproposals.arXivpreprintarXiv:1801.03612,2018.[8]MarcoCusumano-Towner,AlexanderKLew,andVikashKMansinghka.Automatinginvolutivemcmcusingprobabilisticanddifferentiableprogramming.arXivpreprintarXiv:2007.09871,2020.[9]DrewMcDermott,MalikGhallab,AdeleHowe,CraigKnoblock,AshwinRam,ManuelaVeloso,DanielWeld,andDavidWilkins.PDDL-thePlanningDomainDeﬁnitionLanguage,1998.[10]MariaFoxandDerekLong.PDDL2.1:AnextensiontoPDDLforexpressingtemporalplanningdomains.Journalofartiﬁcialintelligenceresearch,20:61–124,2003.[11]BlaiBonetandHéctorGeffner.Planningasheuristicsearch.ArtiﬁcialIntelligence.2001Jun;129(1-2):5-33.,2001.[12]DeepakRamachandranandEyalAmir.Bayesianinversereinforcementlearning.InIJCAI,volume7,pages2586–2591,2007.[13]PieterAbbeelandAndrewYNg.Apprenticeshiplearningviainversereinforcementlearning.InProceedingsofthetwenty-ﬁrstinternationalconferenceonMachinelearning,page1,2004.[14]DylanHadﬁeld-Menell,StuartJRussell,PieterAbbeel,andAncaDragan.Cooperativeinversereinforcementlearning.InAdvancesinneuralinformationprocessingsystems,pages3909–3917,2016.[15]DanielSBrownandScottNiekum.Deepbayesianrewardlearningfrompreferences.arXivpreprintarXiv:1912.04472,2019.[16]NoahDGoodman,ChrisLBaker,ElizabethBaraffBonawitz,VikashKMansinghka,AlisonGopnik,HenryWellman,LauraSchulz,andJoshuaBTenenbaum.Intuitivetheoriesofmind:Arationalapproachtofalsebelief.InProceedingsofthetwenty-eighthannualconferenceofthecognitivesciencesociety,volume6.CognitiveScienceSocietyVancouver,2006.[17]ChrisLBaker,JoshuaBTenenbaum,andRebeccaRSaxe.Goalinferenceasinverseplanning.InProceedingsoftheAnnualMeetingoftheCognitiveScienceSociety,volume29,2007.[18]ChrisLBaker,RebeccaSaxe,andJoshuaBTenenbaum.Actionunderstandingasinverseplanning.Cognition,113(3):329–349,2009.[19]ChrisBaker,RebeccaSaxe,andJoshuaTenenbaum.Bayesiantheoryofmind:Modelingjointbelief-desireattribution.InProceedingsoftheAnnualMeetingoftheCognitiveScienceSociety,33(33),2011.[20]JulianJara-Ettinger,HyowonGweon,LauraESchulz,andJoshuaBTenenbaum.Thenaïveutilitycalculus:Computationalprinciplesunderlyingcommonsensepsychology.Trendsincognitivesciences,20(8):589–604,2016.[21]ChrisLBaker,JulianJara-Ettinger,RebeccaSaxe,andJoshuaBTenenbaum.Rationalquantita-tiveattributionofbeliefs,desiresandperceptsinhumanmentalizing.NatureHumanBehaviour,1(4):1–10,2017.[22]JulianJara-Ettinger,LauraSchulz,andJoshTenenbaum.Thenaiveutilitycalculusasauniﬁed,quantitativeframeworkforactionunderstanding.PsyArXiv,2019.[23]MarcoFCusumano-Towner,AlexeyRadul,DavidWingate,andVikashKMansinghka.Proba-bilisticprogramsforinferringthegoalsofautonomousagents.arXivpreprintarXiv:1704.04977,2017.[24]IrisRSeaman,Jan-WillemvandeMeent,andDavidWingate.Nestedreasoningaboutau-tonomousagentsusingprobabilisticprograms.arXiv,pagesarXiv–1812,2018.11[25]MichaelBratman.Intention,plans,andpracticalreason,volume10.HarvardUniversityPressCambridge,MA,1987.[26]MiquelRamírezandHectorGeffner.Planrecognitionasplanning.InTwenty-FirstInternationalJointConferenceonArtiﬁcialIntelligence,2009.[27]ShirinSohrabi,AntonVRiabov,andOctavianUdrea.Planrecognitionasplanningrevisited.InIJCAI,pages3258–3264,2016.[28]DanielHöller,GregorBehnke,PascalBercher,andSusanneBiundo.Planandgoalrecognitionashtnplanning.In2018IEEE30thInternationalConferenceonToolswithArtiﬁcialIntelligence(ICTAI),pages466–473.IEEE,2018.[29]GalAKaminka,MorVered,andNoaAgmon.Planrecognitionincontinuousdomains.InThirty-SecondAAAIConferenceonArtiﬁcialIntelligence,2018.[30]MorVered,RamonFragaPereira,MauricioCecilioMagnaguagno,FelipeMeneguzzi,andGalAKaminka.Onlinegoalrecognitionasreasoningoverlandmarks.InWorkshopsattheThirty-SecondAAAIConferenceonArtiﬁcialIntelligence,2018.[31]PratikshaThaker,JoshuaBTenenbaum,andSamuelJGershman.Onlinelearningofsymbolicconcepts.JournalofMathematicalPsychology,77:10–20,2017.[32]RyanSelf,MichaelHarlan,andRushikeshKamalapurkar.Onlineinversereinforcementlearningfornonlinearsystems.In2019IEEEConferenceonControlTechnologyandApplications(CCTA),pages296–301.IEEE,2019.[33]NicholasRhinehartandKrisKitani.First-personactivityforecastingfromvideowithonlineinversereinforcementlearning.IEEEtransactionsonpatternanalysisandmachineintelligence,2018.[34]OwainEvansandNoahDGoodman.Learningthepreferencesofboundedagents.InNIPSWorkshoponBoundedOptimality,volume6,2015.[35]OwainEvans,AndreasStuhlmüller,andNoahGoodman.Learningthepreferencesofignorant,inconsistentagents.InThirtiethAAAIConferenceonArtiﬁcialIntelligence,2016.[36]RohinShah,NoahGundotra,PieterAbbeel,andAncaDDragan.Onthefeasibilityoflearning,ratherthanassuming,humanbiasesforrewardinference.arXivpreprintarXiv:1906.09624,2019.[37]StuartArmstrongandSörenMindermann.Occam’srazorisinsufﬁcienttoinferthepreferencesofirrationalagents.InAdvancesinNeuralInformationProcessingSystems,pages5598–5609,2018.[38]ThomasLGrifﬁths,FalkLieder,andNoahDGoodman.Rationaluseofcognitiveresources:Levelsofanalysisbetweenthecomputationalandthealgorithmic.Topicsincognitivescience,7(2):217–229,2015.[39]MarkKHo,DavidAbel,JonathanDCohen,MichaelLLittman,andThomasLGrifﬁths.Theefﬁciencyofhumancognitionreﬂectsplannedinformationprocessing.InProceedingsoftheThirty-FourthAAAIConferenceonArtiﬁcialIntelligence,2020.[40]HåkanLSYounesandMichaelLLittman.Ppddl1.0:Anextensiontopddlforexpressingplanningdomainswithprobabilisticeffects.Techn.Rep.CMU-CS-04-162,2:99,2004.[41]HectorGeffner.Heuristics,planningandcognition.Heuristics,ProbabilityandCausality.ATributetoJudeaPearl.CollegePublications,2010.[42]ThomasGDietterich.Themaxqmethodforhierarchicalreinforcementlearning.InICML,volume98,pages118–126.Citeseer,1998.[43]BrendenMLake,RuslanSalakhutdinov,andJoshuaBTenenbaum.Human-levelconceptlearningthroughprobabilisticprograminduction.Science,350(6266):1332–1338,2015.[44]FerasASaad,MarcoFCusumano-Towner,UlrichSchaechtle,MartinCRinard,andVikashKMansinghka.Bayesiansynthesisofprobabilisticprogramsforautomaticdatamodeling.Pro-ceedingsoftheACMonProgrammingLanguages,3(POPL):1–32,2019.[45]KevinEllis,CatherineWong,MaxwellNye,MathiasSable-Meyer,LucCary,LucasMorales,LukeHewitt,ArmandoSolar-Lezama,andJoshuaBTenenbaum.Dreamcoder:Growinggeneralizable,interpretableknowledgewithwake-sleepbayesianprogramlearning.arXivpreprintarXiv:2006.08381,2020.12[46]ChrisCundyandDanielFilan.Exploringhierarchy-awareinversereinforcementlearning.arXivpreprintarXiv:1807.05037,2018.[47]LesliePackKaelblingandTomásLozano-Pérez.Hierarchicaltaskandmotionplanninginthenow.In2011IEEEInternationalConferenceonRoboticsandAutomation,pages1470–1477.IEEE,2011.[48]CaelanReedGarrett,TomásLozano-Pérez,andLesliePackKaelbling.Stripsplanningininﬁnitedomains.arXivpreprintarXiv:1701.00287,2017.[49]CaelanReedGarrett,TomásLozano-Pérez,andLesliePackKaelbling.PDDLStream:Integrat-ingsymbolicplannersandblackboxsamplersviaoptimisticadaptiveplanning.InProceedingsoftheInternationalConferenceonAutomatedPlanningandScheduling,volume30,pages440–448,2020.[50]DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-che,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal.Masteringthegameofgowithdeepneuralnetworksandtreesearch.nature,529(7587):484,2016.[51]StuartRussell.Humancompatible:Artiﬁcialintelligenceandtheproblemofcontrol.Penguin,2019.[52]EricJHorvitz,JohnSBreese,DavidHeckerman,DavidHovel,andKoosRommelse.Thelumiereproject:Bayesianusermodelingforinferringthegoalsandneedsofsoftwareusers.arXivpreprintarXiv:1301.7385,2013.[53]VasanthSarathy,ThomasArnold,andMatthiasScheutz.Whenexceptionsarethenorm:Exploringtheroleofconsentinhri.ACMTransactionsonHuman-RobotInteraction(THRI),8(3):1–21,2019.[54]ShoshanaZuboff.Bigother:surveillancecapitalismandtheprospectsofaninformationcivilization.JournalofInformationTechnology,30(1):75–89,2015.[55]MilesBrundage,ShaharAvin,JackClark,HelenToner,PeterEckersley,BenGarﬁnkel,AllanDafoe,PaulScharre,ThomasZeitzoff,BobbyFilar,etal.Themalicioususeofartiﬁcialintelligence:Forecasting,prevention,andmitigation.arXivpreprintarXiv:1802.07228,2018.[56]AndrewGuthrieFerguson.Theriseofbigdatapolicing:Surveillance,race,andthefutureoflawenforcement.NYUPress,2019.13SupplementalMaterialOnlineBayesianGoalInferenceforBoundedly-RationalPlanningAgentsTanZhi-Xuan,JordynL.Mann,TomSilverJoshuaB.Tenenbaum,VikashK.MansinghkaMassachusettsInstituteofTechnology{xuan,jordynm,tslvr,jbt,vkm}@mit.eduAExperimentalDetailsBelowweprovideexperimentaldetailsforeachoftheinferencemethodsdescribedinthemaintext.Wehavealsoperformedadditionalexperimentsusingabaselineadaptedfromtheplanrecognitionasplanning(PRP)literature[1],whichweincludebelowasausefulofﬂinebenchmark.A.1SequentialInversePlanSearchWeconductedexperimentsusingtwomainvariantsofSequentialInversePlanSearch(SIPS),theﬁrstusingdata-drivenrejuvenation,asdescribedinthemaintext,andthesecondwithout.RejuvenationisnecessaryfortheresultsshowninFigure1ofthemaintext,andforhighlysub-optimalandfailedplansmoregenerally.However,rejuvenationisalsohardtotune,andcanincreaseruntimeduetotheneedtoreplan.Wethusreportresultswithoutrejuvenationinourquantitativeexperiments.ParametersforqualitativeexperimentsaregivenineachofthecorrespondingﬁguresinsectionB.1.Forthequantitativeexperiments,weusedSIPSwith10particlesperpossiblegoal(e.g.,50particlesfortheBlockWordsdomain),witharesamplingthresholdofc=1/4.Fortheunderlyingagentmodel,weassumedsearchnoiseofγ=0.1andpersistenceparametersofr=2andq=0.95(givinganaveragesearchbudgetof38nodes).Wevariedthesearchheuristichtosuitthetypeofdomain:Forthegridworld-baseddomains(Taxi;Doors,Keys&Gems),weusedaManhattandistanceheuristictothegoal.Fortheotherdomains(BlockWords;IntrusionDetection),weusedthehaddheuristicintroducedbytheHSPalgorithm[2]asageneralizedrelaxed-distanceheuristic.SIPSalsorequiresthespeciﬁcationofanobservationmodelP(o|s),inordertoscorethelikelihoodofahypothesizedstatetrajectoryˆs1,...,ˆstgiventheobservedstateso1,...,ot.Wedeﬁnedthisobservationmodelbyaddingzero-meanGaussiannoisewithσ=0.25foreachnumericvariableinthestate(e.g.,theagent’spositioninagridworld),andBernoullicorruptionnoisewithp=0.05foreachBooleanvariableinthestate(e.g.whetherblockAisontopofblockB).AllSIPSexperimentswereperformedusingPlinf.jl,aJuliaimplementationofourmodelingandinferencearchitecturethatintegratestheGenprobabilisticprogrammingsystemwithPDDL.jl,aJuliainterpreterforthePlanningDomainDeﬁnitionLanguage[3].Experimentswererunona1.9GHzIntelCorei7processorwith16GBRAM.A.2BayesianInverseReinforcementLearningBayesianInverseReinforcementLearning(BIRL)requirescomputinganapproximatevaluefunctionQ(s,a)ofﬂineandaposteriorovergoalsonlineusingthelikelihoodP(a|s,g)=1Zeα·Q(s,a),whereZisthepartitionfunctionandαisanoptimalityparameter.Forthequantitativeexperiments,weusedα=1whichwefoundtoperformwellinpreliminarytrials.Forqualitativecomparisons,however,weusedα=5,asthischoiceproducedresultsinrangemoresimilartohumaninferences.Toapproximatethevaluefunction,weusedvalueiteration(VI)withadiscountfactorof0.9.Preprint.Underreview.arXiv:2006.07532v2  [cs.AI]  25 Oct 2020Asdiscussedinthemaintext,severalofthedomainsconsideredinthisworkhavestatespacesthataretoolargetoenumerate,makingstandardVIintractable.WethereforeusedasynchronousVI,samplingstatesinsteadoffullyenumeratingthem,for250,000iterationsfortheunbiasedbaseline(BIRL-U).Preliminaryexperimentssuggestedthatrunningforupto1,000,000iterationsdidnotappreciablyimproveresults.Taxi,whichhasafarsmallerstatespacethantheotherdomains,wasrunwith10,000iterations,whichwasconsistentlysufﬁcientforconvergence.Fortheoraclebaseline(BIRL-O),2500iterationsweresufﬁcienttoreachconvergencefortheTaxidomain,and10,000iterationsfortheotherdomains.AllBIRLexperimentswerewritteninPythonandrunona2.9GHzIntelCorei9processorwith32GBRAM.WemadeuseofthePDDLGymlibrary[4]forinstantiatingthePDDLplanningproblemsasOpenAIGymenvironments.ToperformasynchronousVIefﬁciently,weimplementedstatesamplersandvalidactiongeneratorsforeachdomain.TheunbiasedversionofBIRL(BIRL-U)usesthesestatesamplerstosamplestateswithinasynchronousVI.Fortheoraclebaseline(BIRL-O),whichhasaccesstothetest-timetrajectories,weinsteadsampledonestateuniformlyatrandomfromthestatesvisitedacrossalltest-timetrajectories.A.3PlanRecognitionasPlanningWeadaptedtheplanrecognitionasplanning(PRP)approachdescribedin[1]asanofﬂinebenchmarkthatachieveshighaccuracyatthecostofsubstantiallymoreruntime(upto30times)thanSIPS.InthePRPapproach,weuseaheuristicapproximationtothelikelihoodofaplanpgivenagoalg:P(p|g)∝e−β(|p|−|pg∗|)(1)wherepg∗isanoptimalplantothegoalg,|p|denotesthelengthoftheplanp,andβisanoiseparameter.Thislikelihoodfunctionmodelagentrationalitybyplacingexponentiallylessprobabilityoncostlierplans,wherelargervaluesofβcorrespondtomoreoptimality.Inordertoperforminferenceusingthislikelihoodmodel,weﬁrstcomputetheoptimalplanpg∗foreachpossiblegoalginadomain.Ateachtimestept,wethenconstructaplanpgttoeachgoalgconsistentwiththeobservationssofar,bycomputinganoptimalpartialplanp+tfromthecurrentobservedstateottog,andthenconcatenatingitwiththeinitialsequenceofactionsp−t:=a1,...,at−1takenbytheagent,givingpgt=[p−t,p+t].Undertheadditionalapproximationthatpgtistheonlyplanconsistentwiththeobservationsequenceo1,...,ot,wecanthencomputethegoalposteriorasP(g|o1,...,ot)’e−β(|pgt|−|pg∗|)Pg0∈Ge−β(|pg0t|−|pg0∗|)(2)Themainlimitationofthisapproachisthatitrequirescomputationofanoptimalpartialplanp+tforeverygoalgateverytimestept,whichscalespoorlywiththenumberofgoalsandtimestepspertrajectory,especiallywhentheobservedtrajectoryleadstheagentfurtherandfurtherawayfrommostofthegoalsunderconsideration.ThisiscontrasttoSIPS,whichperformsincrementalcomputationbyextendingpartialplansfromprevioustimesteps.Inaddition,duetotheassumptionthattherealwaysexistsaplanfromthecurrentobservedstateottoeverygoalg,thePRPapproachisunabletoaccountforirreversiblefailures.Thisisshowninourqualitativecomparisons.Nonetheless,becausePRPstillachieveshighaccuracyonmanysub-optimaltrajectories(attheexpenseofconsiderablymorecomputation,especiallyondomainswithmanygoals),weincludeithereasabenchmarkforaccuracy.AllPRPexperimentswereperformedonthesamemachineastheSIPSexperiments,usingtheimplementationofA*searchprovidedbyPlinf.jl.BAdditionalResultsB.1QualitativeComparisonsforSub-Optimal&FailedPlansHerewepresentdetailedqualitativecomparisonsofthegoalinferencesmadeforsub-optimalandfailedplansintheDoors,Keys&Gemsdomain.FiguresS1andS2showtheinferencesmadefortwosub-optimaltrajectories,whileFiguresS3andS4showtheinferencesmadefortwotrajectorieswithirreversiblefailures.WeomittheunbiasedBayesianIRLbaseline(BIRL-U),becauseitisunabletosolvetheunderlyingMarkovDecisionProcessinanyoftheseexamples,leadingtoauniformposteriorovergoalsovertheentiretrajectory.2FigureS1:GoalinferencesmadebySIPS,BIRL-O,andPRPforthesub-optimaltrajectoryshowninFigure1(a)ofthemaintext.Predictedfuturetrajectoriesinpanels(i)–(iv)aremadebySIPS.ForSIPS,weused30particlespergoal,searchnoiseγ=0.1,persistenceparametersr=2,q=0.95,andaManhattandistanceheuristictothegoal.Rejuvenationmoveswereused,withagoalrejuvenationprobabilityofpg=0.25.ForBIRL-O,weusedα=5.ForPRP,weusedβ=1.FigureS2:GoalinferencesmadebySIPS,BIRL-O,andPRPforanothersub-optimaltrajectory.Predictedfuturetrajectoriesinpanels(i)–(iv)aremadebySIPS.ForSIPS,weused30particlespergoal,searchnoiseγ=0.1,persistenceparametersr=2,q=0.95,andaManhattandistanceheuristictothegoal.Rejuvenationmoveswereused,withagoalrejuvenationprobabilityofpg=0.25.ForBIRL-O,weusedα=5.ForPRP,weusedβ=1.3B.1.1Sub-OptimalPlansFigureS1showshowtheinferencesproducedbySIPSaremorehuman-like,comparedtotheBIRLandPRPbaselines.Inparticular,SIPSadjustsitsinferencesinahuman-likemanner,initiallyremaininguncertainbetweenthe3gems(paneli),placingmoreposteriormassontheyellowgemwhentheagentacquirestheﬁrstkey(panelii),increasingthatposteriormasswhenagentappearstoignorethesecondkeyandunlocktheﬁrstdoor(paneliii),butthenswitchingtothebluegemoncetheagentbacktrackstowardsthesecondkey(paneliv).WhiletheinferencesproducedbyBIRLdisplaysimilartrends,theyaremuchmoregradual,becauseBIRLassumesnoiseatthelevelofactinginsteadofplanning.Inaddition,theagentmodelunderlyingBIRLleadstostrangeartifacts,suchastheriseinprobabilityoftheredgemwhent<9.ThisisbecauseBoltzmannactionnoiseplaceslowerprobabilityP(a|g)onanactionathatleadstoagoalgwhichisfurtheraway,duetothevaluefunctionVgassociatedwiththatgoalgbeingsmallerduetotimediscounting.Asaresult,whent<9,BIRLcomputesthatP(right|red)>P(right|yellow)andP(right|blue),leadingtotheredgembeinginferredasthemostlikelygoal.Finally,PRPexhibitsbothover-conﬁdenceintheyellowgemandslowrecoverytowardsthebluegem.Thisisduetotheassumptionthatthelikelihoodofaplanptosomegoalgisexponentiallydecreasinginitscostdifferencefromtheoptimalplanpg∗.Betweent=10andt=20,allplansconsistentwiththeobservationstothebluegemareconsiderablylongerthantheoptimalplanpblue∗.Asaresult,PRPgivesverylowprobabilitytothebluegem.Thiseffectcontinuesformanytimestepsaftertheagentstartstobacktrack(t=17tot=24),indicatingthatthePRPmodelingassumptionsareinadequateforplanswithsubstantialbacktracking.SimilardynamicscanbeobservedforthetrajectoryinFigureS2.TheBIRLbaselineperformsespeciallypoorly,placinghighprobabilityontheyellowgemevenwhentheagentbacktrackstocollectthesecondkey(t=19tot=22).Thisagainisduetotheassumptionofactionnoiseinsteadofplanningnoise,makingitmuchmorelikelyundertheBIRLmodelthatanagentwouldrandomlywalkbacktowardsthesecondkey.ThePRPbaselineexhibitsthesameissueswithover-conﬁdenceandslowrecoverydescribedearlier,placingsolittleposteriormassonthebluegemfromt=17tot=20thatitevenconsiderstheredgemtobemorelikely.Incontrast,ourmethod,SIPS,immediatelyconvergestothebluegemoncebacktrackingoccursatt=20.B.1.2FailedPlansThedifferencesbetweenSIPSandthebaselinemethodsareevenmorestrikingfortrajectorieswithirreversiblefailures.AsshowninFigureS3,SIPSaccuratelyinfersthatthebluegemisthemostlikelygoalwhentheagentignoresthetwokeysatthebottom,insteadturningtowardstheﬁrstdoorguardingthebluegematt=19.Thisinferencealsoremainsstableaftert=21,whentheagentirreversiblyusesupitskeytounlockthatdoor.SIPSiscapableofsuchinferencesbecausethesearchforpartialplansisbiasedtowardspromisingintermediatestates.Sincetheunderlyingagentmodelassumesarelaxeddistanceheuristicthatconsidersstatesclosertothebluegemaspromising,themodelislikelytoproducepartialplansthatleadspatiallytowardthebluegem,evenifthoseplansmyopicallyuseuptheagent’sonlykey.Incontrast,bothBIRLandPRPfailtoinferthatthebluegemisthegoal.BIRLinitiallyplacesincreasingprobabilityontheredgem,duetoBoltzmannactionnoisefavoringgoalswhichtakelesstimetoreach.Whilethisprobabilitydecreasesslightlyastheagentdetoursfromtheoptimalplantotheredgem,itremainsthehighestprobabilitygoalevenaftertheagentusesupitskeyatt=21.Theposteriorovergoalsstopschangingafterthat,becausetherearenolongeranyanypossiblepathstoagoal.PRPexhibitsadifferentfailuremode.WhileitdoesnotsufferfromtheartifactsduetoBoltzmannactionnoise,itcompletelyfailstoaccountforthepossibilitythatanagentmightmakeafailedplan.Asaresult,theprobabilityofthebluegemdoesnotincreaseevenaftertheagentturnstowardsitatt=19.Furthermore,oncefailureoccursatt=21,PRPendsupdefaultingtoauniformdistributionoverthethreegems,eventhoughithadpreviouslyeliminatedtheredgemasapossibility.TheinferencesinFigureS4displaysimilartrends.Onceagain,SIPSaccuratelyinfersthatthebluegemisthegoal,evenslightlyinadvanceoffailure(paneliii).Incontrast,BIRLwronglyinfersthattheredgemisthemostlikely,whilePRPerroneouslydefaultstoinferringuponfailurethattheonlyremainingacquirablegem(yellow)isthegoal.4FigureS3:GoalinferencesmadebySIPS,BIRL-O,andPRPforthefailedtrajectoryshowninFigure1(b)ofthemaintext.Predictedfuturetrajectoriesinpanels(i)–(iv)aremadebySIPS.ForSIPS,weused30particlespergoal,searchnoiseγ=0.1,persistenceparametersr=2,q=0.95,andamaze-distanceheuristic(i.e.distancetothegoal,ignoringdoors).Rejuvenationmoveswereusedwithpg=0.25.ForBIRL-O,weusedα=5.ForPRP,weusedβ=1.FigureS4:GoalinferencesmadebySIPS,BIRL-O,andPRPforanotherfailedtrajectory.Predictedfuturetrajectoriesinpanels(i)–(iv)aremadebySIPS.ForSIPS,weused30particlespergoal,searchnoiseγ=0.1,persistenceparametersr=2,q=0.95,andaManhattandistanceheuristictothegoal.Rejuvenationmoveswereused,withagoalrejuvenationprobabilityofpg=0.25.ForBIRL-O,weusedα=5.ForPRP,weusedβ=1.5B.2Accuracy&SpeedHerewepresentquantitativecomparisonsoftheaccuracyandspeedofeachinferencemethod.TablesS1andS2showtheaccuracyresultsfortheoptimalandsub-optimaldatasetsrespectively.P(gtrue|o)representstheposteriorprobabilityofthetruegoal,whileTop-1representsthefractionofproblemswheregtrueistop-ranked.Accuracymetricsarereportedattheﬁrst(Q1),second(Q2),andthird(Q3)quartilesofeachobservedtrajectory.Thecorrespondingstandarddeviations(takenacrossthedataset)areshowntotherightofeachaccuracymean.TablesS3andS4showtheruntimeresultsfortheoptimalandsub-optimaldatasetsrespectively.Runtimeisreportedintermsofthestart-upcost(C0),marginalcostpertimestep(MC),andaveragecostpertimestep(AC),allmeasuredinseconds.Thecorrespondingstandarddeviationsareshowntotherightofeachruntimemean.Thetotalnumber(N)ofstatesvisited(duringeitherplansearchorvalueiteration)arealsoreportedasaplatform-independentcostmetric.AccuracyDomainMethodP(gtrue|o)Top-1Q1Q2Q3Q1Q2Q3Taxi(3Goals)SIPS0.45±0.260.48±0.270.64±0.320.67±0.490.67±0.490.67±0.49BIRL-U0.33±0.060.38±0.170.79±0.220.33±0.470.42±0.490.92±0.28BIRL-O0.41±0.330.44±0.400.82±0.230.50±0.500.42±0.491.00±0.00PRP0.33±0.000.36±0.060.44±0.080.33±0.001.00±0.001.00±0.00Doors,Keys&Gems(3Goals)SIPS0.39±0.180.51±0.320.70±0.350.73±0.460.73±0.460.80±0.41BIRL-U0.33±0.000.33±0.000.33±0.000.33±0.000.33±0.000.33±0.00BIRL-O0.41±0.330.37±0.060.41±0.080.50±0.500.67±0.470.87±0.34PRP0.40±0.170.62±0.300.81±0.261.00±0.001.00±0.001.00±0.00BlockWords(5Goals)SIPS0.38±0.270.71±0.410.78±0.410.73±0.460.73±0.460.80±0.41BIRL-U0.20±0.030.21±0.050.23±0.100.53±0.500.53±0.500.60±0.49BIRL-O0.22±0.010.30±0.030.46±0.060.73±0.440.87±0.341.00±0.00PRP0.38±0.180.78±0.280.91±0.180.93±0.260.93±0.261.00±0.00IntrusionDetection(20Goals)SIPS0.65±0.381.00±0.001.00±0.000.80±0.411.00±0.001.00±0.00BIRL-U0.05±0.000.05±0.000.05±0.000.05±0.000.05±0.000.05±0.00BIRL-O0.10±0.010.25±0.020.55±0.031.00±0.001.00±0.001.00±0.00PRP0.35±0.130.96±0.060.99±0.011.00±0.001.00±0.001.00±0.00TableS1:Inferenceaccuracyonthedatasetofoptimaltrajectories.AccuracyDomainMethodP(gtrue|o)Top-1Q1Q2Q3Q1Q2Q3Taxi(3Goals)SIPS0.43±0.320.51±0.380.62±0.420.46±0.510.50±0.510.67±0.48BIRL-U0.34±0.060.33±0.000.79±0.230.33±0.470.42±0.490.92±0.28BIRL-O0.35±0.290.48±0.320.81±0.320.38±0.480.46±0.500.79±0.41PRP0.33±0.000.35±0.060.53±0.230.33±0.001.00±0.001.00±0.00Doors,Keys&Gems(3Goals)SIPS0.35±0.070.51±0.320.54±0.370.75±0.440.75±0.440.70±0.47BIRL-U0.33±0.000.33±0.000.33±0.000.33±0.000.33±0.000.33±0.00BIRL-O0.34±0.020.36±0.040.43±0.070.4±0.490.55±0.500.75±0.43PRP0.35±0.170.38±0.320.64±0.400.90±0.310.70±0.470.83±0.38BlockWords(5Goals)SIPS0.52±0.330.89±0.280.96±0.180.80±0.410.90±0.310.97±0.18BIRL-U0.19±0.030.19±0.030.19±0.040.37±0.480.47±0.500.53±0.50BIRL-O0.19±0.030.29±0.060.45±0.090.73±0.440.77±0.420.93±0.25PRP0.36±0.180.77±0.240.91±0.171.00±0.001.00±0.001.00±0.00IntrusionDetection(20Goals)SIPS0.52±0.430.80±0.410.80±0.410.58±0.500.80±0.410.80±0.41BIRL-U0.05±0.000.05±0.000.05±0.000.05±0.000.05±0.000.05±0.00BIRL-O0.09±0.010.23±0.040.52±0.070.92±0.221.00±0.001.00±0.00PRP0.42±0.010.99±0.0031.00±0.001.00±0.001.00±0.001.00±0.00TableS2:Inferenceaccuracyonthedatasetofsuboptimaltrajectories.6Intermsofaccuracyalone,itcanbeseenthatthePRPbaselinegenerallyachievesthehighestmetrics,withSIPSandBIRL-Operformingcomparably,andwithBIRL-UcompletelyincapableofmakingaccurateinferencesexceptintheTaxidomain.Asdemonstratedbythequalitativecomparisonshowever,thesemetricsalonemaybemisleading,failingtoshowhowinferencesofeachmethodreallyevolveovertime.Inparticular,whilethePRPbaselineisroutinelyabletoachievethehighestTop-1accuracy,thismaynotcorrespondtoasuitablycalibratedposteriorovergoals,normightitcapturethesharphuman-likechangesovertimethatSIPSappearstodisplay.Itshouldalsobenotedthatmostofthedomainsconsidereddonotallowforirreversiblefailures.Assuch,thedistinctivecapabilityofSIPStoinfergoalsdespitefailedplansisnotcapturedbytheresultsinTableS2.RuntimeDomainMethodC0(s)MC(s)AC(s)NTaxi(3Goals)SIPS14.7±6.732.19±0.953.08±1.241220±405BIRL-U2.22±0.060.002±0.00070.17±0.0310000±0BIRL-O0.56±0.020.002±0.00060.04±0.012500±0PRP13.2±2.196.21±1.526.73±1.506830±2090Doors,Keys&Gems(3Goals)SIPS3.17±1.100.72±0.210.84±0.252100±1140BIRL-U3280±1730.13±0.14181±184250000±0BIRL-O142±13.00.13±0.148.00±8.2410000±0PRP5.32±2.213.12±1.583.24±1.675970±3350BlockWords(5Goals)SIPS21.1±4.841.67±0.613.62±0.852380±1110BIRL-U687±2730.15±0.0569.5±31.2250000±0BIRL-O19.5±0.590.12±0.032.11±0.5110000±0PRP25.6±11.326.5±7.9026.3±7.503980±1410IntrusionDetection(20Goals)SIPS325±24.912.0±1.4030.0±3.0014100±343BIRL-U18000±20500.01±0.071130±230250000±0BIRL-O100±11.70.02±0.005.80±0.8610000±0PRP246±5.12381±108374±10275700±20800TableS3:Inferenceruntimeonthedatasetofoptimaltrajectories.RuntimeDomainMethodC0(s)MC(s)AC(s)NTaxi(3Goals)SIPS12.2±7.751.61±0.742.29±1.051530±1110BIRL-U2.22±0.060.003±0.00040.16±0.0410000±0.00BIRL-O2.17±0.050.002±0.00030.15±0.042500±0.00PRP13.3±3.267.33±2.617.74±2.568840±5800Doors,Keys&Gems(3Goals)SIPS3.40±1.180.69±0.240.87±0.312100±1140BIRL-U3360±66.00.11±0.06133±68.7250000±0.00BIRL-O155±3.310.11±0.066.27±3.3110000±0.00PRP4.65±1.583.04±1.563.11±1.566150±3680BlockWords(5Goals)SIPS20.6±5.792.86±1.124.41±1.772570±810BIRL-U687±2730.33±0.1360.6±34.0250000±0.00BIRL-O23.5±1.760.01±0.0012.12±0.8610000±0.00PRP40.5±22.738.9±16.138.9±15.75660±4860IntrusionDetection(20Goals)SIPS400±29.73.90±1.0426.6±2.0612900±3020BIRL-U18000±20501.12±3.831040±163250000±0.00BIRL-O96.9±10.40.02±0.0025.60±0.7710000±0.00PRP281±2.48332±25.8330±24.751900±960TableS4:Inferenceruntimeonthedatasetofsuboptimaltrajectories.Onceruntimeistakenintoaccount,itbecomesclearthatSIPSachievesthebestbalancebetweenspeedandaccuracyduetoitsuseofincrementalcomputation.Incontrast,BIRL-Urequiresordersofmagnitudemoreinitialcomputationwhilestillfailingtoproducemeaningfulinferences,whilePRPrequiresupto30timesmorecomputationpertimestep.ThisisespeciallyapparentontheIntrusionDetectiondomain,whichhasalargenumberofgoals,requiringPRPtocomputealargenumberofoptimalplansateachtimestep.EventheBIRL-Obaseline,whichassumesoracularaccesstothedatasetofobservedtrajectoriesduringvalueiteration,isslowerthanSIPSontheDoors,Keys&Gemsdomainintermsofaverageruntime.Overall,theseresultsimplythatSIPSistheonlymethodsuitableforonlineusageonthefullrangeofdomainsweconsider.7B.3RobustnesstoParameterMismatchTablesS5andS6presentadditionalresultsfortherobustnessexperimentsdescribedinthemaintext,showinghowdifferentsettingsofmodelparametersfareagainsteachother.EachcolumncorrespondstoaparametervalueassumedbySIPS,andeachrowcorrespondstothetrueparameterfortheboundedlyrationalagentmodelusedtogeneratethedata.Withineachsub-table,unspeciﬁedparametersdefaulttoγ=0.1,r=2,q=0.95,h=Manhattan(forDoors,Keys,Gems)andh=hadd(forBlockWords).ItcanbeseenthatSIPSfaresreasonablywellagainstmismatchedparameters,withdegradationpartlydrivenbymismatchitself,butalsopartlybyincreasedrandomnesswhenthedata-generatingparametersleadtolessoptimalagentbehavior.TheeffectofnoisybehaviorisespeciallyapparentinTableS6(d):datageneratedbyagentsusingthehighlyuninformativegoalcountheuristic(whichsimplythecountsthenumberofgoalpredicatesyettobesatisﬁedasadistancemetric)ishighlyrandom.Thisresultsinverypoorinferences(Top-1atQ3=0.37),evenwhenSIPScorrectlyassumesthesameheuristic.Nonetheless,mismatchedheuristicsdoleadtopoorerperformance,raisingtheopenquestionofwhetherobserversneedgoodmodelsofothers’planningheuristicsinordertoaccuratelyinfertheirgoals.Assumedr124True10.800.600.7020.730.730.7740.770.730.87(a)Persistence(r)Assumedq0.800.900.95True0.800.730.670.530.900.770.630.600.950.770.600.73(b)Persistence(q)Assumedγ0.020.100.50True0.020.900.770.830.100.770.730.730.500.730.670.77(c)Searchnoise(γ)AssumedhMh.Mz.TrueMh.0.830.77Mz.0.800.90(d)Heuristic(h)TableS5:RobustnesstoparametermismatchfortheDoors,Keys,Gemsdomain.Themetricshownisthetop-1accuracyofSIPSatthethirdtimequartile(Q3).h=Mh.referstoManhattandistance,whileh=Mz.referstomazedistance.Assumedr124True10.800.900.7720.830.800.9040.870.900.93(a)Persistence(r)Assumedq0.800.900.95True0.800.800.830.700.900.700.800.830.950.830.800.87(b)Persistence(q)Assumedγ0.020.100.50True0.020.800.870.870.100.830.870.830.500.900.830.87(c)Searchnoise(γ)AssumedhGChaddTrueGC.0.370.43hadd0.330.77(d)Heuristic(h)TableS6:RobustnesstoparametermismatchfortheBlocksWorlddomain.Themetricshownisthetop-1accuracyofSIPSatthethirdtimequartile(Q3).h=GC.referstothegoalcountheuristic,whileh=haddreferstotheadditivedelete-relaxationheuristic.CHumanStudiesAsdescribedinthemaintext,weconductedtwosetsofpilotstudieswithhumansubjects,theﬁrsttomeasurehumangoalinferencesforcomparison,andthesecondtocollecthuman-generatedplansforrobustnessexperiments.ThesestudieswereapprovedunderMIT’sIRB(COUHESno.:0812003014).C.1HumanInferencesDatawascollectedfromN=5pilotsubjectsintheMITpopulation.EachsubjectwasgivenaccesstoawebinterfacethatwouldpresenttrajectoriesofanagentintheDoors,Keys&Gemsdomain,andthatwouldaskforgoalinferencejudgementsatevery6thtimestep,aswellastheﬁrstandlasttimestep.Subjectscouldselectwhichgemtheybelievedtobethemostlikelygoaloftheagent,andthenwereallowedtoadjustslidersindicatinghowlikelytheothergoalswereincomparison.Theserelativeprobabilityratingswerenormalized,andrecorded.ExcerptsfromthisinterfaceareshowninFigureS5.Subjectswereshownaseriesof10trajectories,outofwhich4wereoptimaltrajectories,and6exhibitednotablesuboptimalityorfailure.8Next >>Next >>RedYellowBlueAllequallylikelyNext >>Next >>RedYellowBlueAllequallylikelyNot at all1/4Half as likely3/4EquallyNot at all1/4Half as likely3/4EquallyNext >>Next >>RedYellowBlueAllequallylikelyNot at all1/4Half as likely3/4EquallyNot at all1/4Half as likely3/4EquallyFigureS5:Webinterfaceforcollectinghumangoalinferences.Eachpanelshowsonestepinasequenceofjudgementpointspresentedtoaparticipant.C.2HumanPlansDatawascollectedfromN=5pilotsubjectsintheMITpopulation.Eachsubjectreceivedaexper-imentalscripttorun,whichcollecteddataforboththeDoors-Keys-GemsdomainandtheBlocksWorlddomain.FortheBlocksWorlddomain,datawascollectedforallcombinationsof3problemswith5possiblegoals,andfortheDoors-Keys-Gemsdomain,datawascollectedforallcombinationsof5problemswith3possiblegoals.Foreachpairconsistingofaproblemandagoal,thesubjectswerepresentedwithavisualizationoftheinitialstateandatextualdescriptionoftheirgoal.Thesubjectswerethenpresentedwithalistofkeyscorrespondingtotheactionsavailablefromthecurrentstate,andpromptedtopressthekeycorrespondingtotheirselectedaction.Oncethesubjectsenteredtheiractionofchoice,thevisualizationwouldupdatetoshowthestateaftertheactionhadoccurred.Thesubjectswouldthenbepromptedagainforanaction.Thisprocessrepeateduntilthegivengoalwasachieved,orthesubjectterminatedthattask(e.g.ifthegoalwasnolongerachievable).Oncethegoalwasachievedforagivenproblemandgoalpair,thesequenceofactionswasrecorded.References[1]MiguelRamírezandHectorGeffner.Probabilisticplanrecognitionusingoff-the-shelfclassicalplanners.InTwenty-FourthAAAIConferenceonArtiﬁcialIntelligence,2010.[2]BlaiBonetandHéctorGeffner.Planningasheuristicsearch.ArtiﬁcialIntelligence.2001Jun;129(1-2):5-33.,2001.[3]DrewMcDermott,MalikGhallab,AdeleHowe,CraigKnoblock,AshwinRam,ManuelaVeloso,DanielWeld,andDavidWilkins.PDDL-thePlanningDomainDeﬁnitionLanguage,1998.[4]TomSilverandRohanChitnis.PDDLGym:GymenvironmentsfromPDDLproblems,2020.9