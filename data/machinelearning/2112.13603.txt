1

Over-the-Air Federated Multi-Task Learning

Over MIMO Multiple Access Channels

Chenxi Zhong, Huiyuan Yang, and Xiaojun Yuan, Senior Member, IEEE

2
2
0
2

y
a
M
6

]
P
S
.
s
s
e
e
[

2
v
3
0
6
3
1
.
2
1
1
2
:
v
i
X
r
a

Abstract

With the explosive growth of data and wireless devices, federated learning (FL) over wireless

medium has emerged as a promising technology for large-scale distributed intelligent systems. Yet, the

urgent demand for ubiquitous intelligence will generate a large number of concurrent FL tasks, which

may seriously aggravate the scarcity of communication resources. By exploiting the analog superposition

of electromagnetic waves, over-the-air computation (AirComp) is an appealing solution to alleviate the

burden of communication required by FL. However, sharing frequency-time resources in over-the-air

computation inevitably brings about the problem of inter-task interference, which poses a new challenge

that needs to be appropriately addressed. In this paper, we study over-the-air federated multi-task

learning (OA-FMTL) over the multiple-input multiple-output (MIMO) multiple access (MAC) channel.

We propose a novel model aggregation method for the alignment of local gradients of different devices,

which alleviates the straggler problem in over-the-air computation due to the channel heterogeneity.

We establish a communication-learning analysis framework for the proposed OA-FMTL scheme by

considering the spatial correlation between devices, and formulate an optimization problem for the

design of transceiver beamforming and device selection. To solve this problem, we develop an algorithm

by using alternating optimization (AO) and fractional programming (FP), which effectively mitigates

the impact of inter-task interference on the FL learning performance. We show that due to the use of

the new model aggregation method, device selection is no longer essential, thereby avoiding the heavy

computational burden involved in selecting active devices. Numerical results demonstrate the validity

of the analysis and the superb performance of the proposed scheme.

Index Terms

Multi-task federated learning, over-the-air model aggregation, multiple-input multiple-output mul-

tiple access channel, alternating optimization, fractional programming.

C. Zhong, H. Yang and X. Yuan are with the National Key Laboratory of Science and Technology on Communi-
cations, University of Electronic Science and Technology of China, Chengdu, China (e-mail: cxzhong@std.uestc.edu.cn;
hyyang@std.uestc.edu.cn; xjyuan@uestc.edu.cn). The corresponding author is Xiaojun Yuan.

 
 
 
 
 
 
2

I. INTRODUCTION

In recent years, the explosive increase of wireless data has promoted widespread applications of

artiﬁcial intelligence in many ﬁelds, such as computer vision [1] and natural language processing

[2]. To exploit the diversity of wireless data, the conventional centralized machine learning (ML)

paradigm requires edge devices to upload their local data to a central parameter server (PS) for

joint training. However, data uploading brings about huge costs in communication resources

and potentially threatens user privacy. To tackle these challenges, federated learning (FL) has

been proposed as a promising substitute [3]. In FL, the PS ﬁrst broadcasts the global model

parameters to the selected edge devices. Subsequently, each selected device calculates a local

gradient based on its local dataset and upload the gradient to the PS. The global model is then

updated by the PS based on the received local gradients. Clearly, by replacing data sharing with

gradient sharing, FL reduces communication costs and protects user privacy.

Despite the appealing advantages, the huge uplink communication cost is still a critical

bottleneck for FL, especially when uploading through wireless channels. Recently, over-the-

air computation (AirComp) has been adopted to improve the communication efﬁciency of the

FL uplink. [4] In over-the-air FL (OA-FL), edge devices transmit their local gradients by using

shared radio resources, and aggregate them over the air by utilizing the superposition property

of electromagnetic waves. Pioneering work has conﬁrmed that over-the-air FL has strong noise

tolerance [5] and reduces latency substantially compared with the schemes based on conventional

orthogonal multiple access (OMA) protocols [6]–[9].

However, given the above mentioned beneﬁts, introducing AirComp into FL uplink also brings

some tricky problems, such as the so-called straggler1 problem [6], [7]. To aggregate the local

gradients at the PS, the devices with better channel conditions have to lower their transmitting

powers to align themselves with the stragglers (the devices with the worst channel conditions);

see e.g. the state-of-the-art OA-FL literature [7], [10], [11]. Clearly, since this strict-alignment

approach potentially reduces the aggregation signal-to-noise ratio (SNR), it conceivably leads

to a degradation of the FL performance. Refs. [6] and [7] propose to exclude the stragglers

from model aggregation to alleviate this problem. However, this exclusion of devices reduces

1In computer science literature, the word “straggler” usually refers to a device with low computation capacity. But here a

straggler refers to a device with poor channel condition.

3

the available dataset for FL training, thereby deteriorating the FL performance. Thus, developing

more efﬁcient approaches to deal with the straggler problem is highly desirable.

Another problem brought by the over-the-air FL uplink appears in the multi-tasking situ-

ation. Speciﬁcally, when multiple tasks are performed simultaneously at the wireless edge2,

the communication cost of this multi-task FL system will be further exacerbated, which to a

greater extent necessitates the use of AirComp. However, introducing AirComp in this multi-task

situation implies that the FL tasks share time-frequency resources for gradients uploading, which

inevitably introduces inter-task interference. In this paper, we attempt to suppress the inter-task

interference by using the multi-antenna technique.

Moreover, the local gradients of FL exhibit strong correlations temporally (over communication

rounds) and spatially (among devices) [13], [14]. The temporal correlation has been exploited

to improve the uplink efﬁciency of OA-FL in [15], whereas an efﬁcient use of the spatial

correlation has not been well explored. The existing approaches in OA-FL [10], [16] ignore

this correlation and assume spatially independent local gradients in the system optimization,

potentially leading to a substantial performance loss. Developing a spatial-correlation-aware OA-

FL system optimization scheme will be a non-trivial step to exploit the spatial correlation of the

local gradients.

In this paper, we address the above three challenges by developing a novel over-the-air

federated multi-task learning (OA-FMTL) scheme where multiple FL tasks are trained simul-

taneously over a multiple-input multiple-output (MIMO) multiple access (MAC) channel. The

MIMO MAC channel consists of a multi-antenna central PS and a number of multi-antenna

edge devices. These FL tasks share time-frequency resources and thus generally interfere with

each other in model uploading. We establish a communication-learning analysis framework for

the considered OA-FMTL scheme. Based on the framework, we analyse the convergence of the

OA-FMTL scheme and formulate a transceiver beamforming problem for learning performance

enhancement and inter-task interference suppression. We propose a low-complexity algorithm

based on alternating optimization (AO) and fractional programming (FP) to optimize the transmit

and receive beamforming vectors. The main novelties of our approach are listed as follows:

• The OA-FMTL analysis framework: We establish a communication-learning analysis frame-

work for the considered OA-FMTL scheme. Based on the analysis results, we formulate the

2The fast development of intelligent systems spawns a large number of FL tasks to meet various demands of intelligence [12].

4

transceiver beamforming problem and develop a low-complexity solution to the problem.

• Inter-task interference suppression: To the best of our knowledge, this is the ﬁrst attempt to

solve the problem of inter-task interference suppression in OA-FMTL utilizing the MIMO

technique.

• Spatial-correlation-aware design: We establish a probability model to capture the gradient

correlation among the devices, allowing us to design the OA-FMTL scheme with the

awareness of the spatial correlation. We show that the awareness of the spatial correlation

brings substantial improvement in learning performance.

• Soft straggler alignment: We propose a misalignment-tolerant aggregation approach for the

gradient aggregation at the PS. This approach corrects the stereotype that gradients need

to be strictly aligned without going to the other extreme, i.e., to drop out the stragglers,

thereby signiﬁcantly relieving the performance degradation due to the straggler problem.

Extensive numerical results demonstrate that our proposed OA-FMTL scheme achieves sig-

niﬁcant performance improvements than the existing schemes. Furthermore, the test accuracies

of the proposed scheme are very close to those of the ideal error-free benchmarks even in

the presence of severe inter-task interference, demonstrating the effectiveness of our proposed

scheme.

The remainder of this paper is organized as follows. In Section II, the FL model, the MIMO

MAC channel model, the communication system, and the over-the-air model aggregation method

are described. In Section III, we analyse the learning performance of the proposed OA-FMTL

scheme. In Section IV, we formulate the optimization problem that minimizes the total training

loss of the FL tasks and propose algorithms to jointly optimize transmit and receive beamforming

vectors, as well as device selection. In Section V, numerical results are presented to evaluate the

proposed scheme. Finally, we draw the conclusions in Section VI.

Notation: We use R and C to denote the real and complex number sets, respectively. We denote

scalars in italic type, vectors in straight bold small letters and matrices in straight bold capital

letters. (·)†, (·)T, and (·)H are used to denote the conjugate, the transpose, and the conjugate

transpose, respectively. We use s[d] to denote the d-th entry of vector s, s(1 : D) to denote a

sub-vector of s with entries indexed from 1 to D, CN (µ, σ2) to denote the circularly-symmetric
complex normal distribution with mean µ and covariance σ2, E[·] to denote the expectation

operator, and |S| to denote the cardinality of set S. IN , 1N ×M , and 0N ×M are used to denote the

N × N identity matrix, the N × M all-one matrix, and the N × M all-zero matrix, respectively.

5

We use (cid:107) · (cid:107) to denote the l2-norm. [K] denotes the set {k|1 ≤ k ≤ K}.

II. SYSTEM MODEL

In this paper, we consider a K-task OA-FMTL system with one central PS and M wireless

devices, where Mk wireless devices are collaboratively training an identical model for task k3,
k ∈ [K], and M = (cid:80)K

k=1 Mk. A two-task OA-FMTL system is illustrated in Fig. 1. In the
following, we introduce the FL system, the underlying communication system, the over-the-air

model aggregation, and OA-FMTL framework, sequentially.

Fig. 1. FL interference network with two tasks.

A. Federated Learning System

We start with the description of the K-task FL system. Let Ak denote the training dataset of

the k-th FL task (which is stored on the Mk devices). Let Qk be the cardinality of Ak. Each FL

task k has an empirical loss function based on Ak, given by

Fk(wk) (cid:44) 1
Qk

Qk(cid:88)

n=1

fk

(cid:0)wk; ξk,n

(cid:1) , ∀k ∈ [K],

(1)

where wk ∈ RD denotes the parameter vector of task k, 4, ξk,n ∈ Ak denotes the n-th training
(cid:1) denotes the sample-wise loss function with respect to (w.r.t.)
sample in Ak, and fk

(cid:0)wk; ξk,n

ξk,n. Let subscript (cid:104)k, i(cid:105) denote the index of the i-th device in the k-th group, ∀k ∈ [K], i ∈ [Mk].
We assume that the local training datasets of each device (cid:104)k, i(cid:105) in group k are independently

3For simplicity, we assume that each device is assigned to a single task. The extension to the case of one device serving

multiple tasks is straightforward. We omit detailed discussions due to space limitation.

4If the lengths of the parameter vectors vary among the tasks, zero padding is employed to ensure that {wk}K

k=1 have identical

length D.

……and randomly drawn from Ak, k ∈ [K]. Let Q(cid:104)k,i(cid:105) denote the sample size of the (cid:104)k, i(cid:105)-th device.
Naturally, we have Qk = (cid:80)Mk

i=1 Q(cid:104)k,i(cid:105). Then Fk(wk) can be rewritten as

Fk(wk) =

1
Qk

Mk(cid:88)

i=1

Q(cid:104)k,i(cid:105)F(cid:104)k,i(cid:105)(wk), ∀k ∈ [K],

(2)

6

(cid:1) /Q(cid:104)k,i(cid:105) denotes the local loss function of the (cid:104)k, i(cid:105)-
where F(cid:104)k,i(cid:105)(wk) (cid:44) (cid:80)Q(cid:104)k,i(cid:105)
th device with ξ(cid:104)k,i(cid:105),n being the n-th training sample on the (cid:104)k, i(cid:105)-th device. The overall loss

(cid:0)wk; ξ(cid:104)k,i(cid:105),n

n=1 fk

function of the considered OA-FMTL is given by

F(w) (cid:44)

K
(cid:88)

k=1

Fk(wk),

(3)

where w (cid:44) [wT

1 , . . . , wT

K]T.

OA-FMTL aims to minimize F(w) by separately minimize each Fk(wk) using gradient

descent (GD) [3]. We now focus on the training of the k-th task. To be speciﬁc, at the t-th

communication round, the training of each FL task k consists of the following ﬁve steps:

• Device selection: The parameter server (PS) selects a sub-group Mk of group k to participate

in the updating.

• Global model broadcasting: The PS broadcasts global model w(t)
k

to the selected devices

through error-free links.

• Local gradients computing: Each selected device computes its gradient based on the local

training dataset, i.e., to compute the local gradient g(t)

(cid:104)k,i(cid:105) by

g(t)
(cid:104)k,i(cid:105)

(cid:44) ∇F(cid:104)k,i(cid:105)(w(t)

k ) =

1
Q(cid:104)k,i(cid:105)

(cid:88)Q(cid:104)k,i(cid:105)
n=1

(cid:16)

∇fk

w(t)

k ; ξ(cid:104)k,i(cid:105),n

(cid:17)

, ∀i ∈ Mk.

(4)

• Local gradients uploading: Selected devices upload their local gradients to the PS.
• Global model updating: The PS computes the aggregated gradient g(t)

k by

g(t)
k

(cid:44)

(cid:80)

Q(cid:104)k,i(cid:105)g(t)
(cid:104)k,i(cid:105)
Q(cid:104)k,i(cid:105)

i∈Mk
(cid:80)

i∈Mk

The PS then updates the model w(t)

k by

w(t+1)
k

= w(t)

k − ηkg(t)
k ,

where ηk ∈ R is the learning rate of task k.

.

(5)

(6)

7

B. MIMO MAC Channel

We now introduce the wireless channel model of OA-FMTL. Following the common practice

in OA-FL [17]–[19], we assume the global model is broadcast to the devices via error-free

links and the local gradient uploading is synchronized among the devices 5. Recall that OA-

FMTL is composed of a PS (i.e., a base station) and M edge devices. We assume the PS

and each edge device are respectively equipped with NR and NT antennas, which leads to a

multiple-input multiple-output (MIMO) multiple access (MAC) channel. We further assume a

block-fading channel model, where the channel state keeps invariant during the step of local

gradients uploading6.

We next focus on the local gradient uploading process. Let C denote the total number of

channel uses in each communication round. Then, at the t-th communication round, the received

signal matrix at the PS is denoted by

Y(t) =

(cid:88)K

(cid:88)

k=1

i∈Mk

H(t)

(cid:104)k,i(cid:105)X(t)

(cid:104)k,i(cid:105) + N(t),

(7)

where H(t)
(cid:104)k,i(cid:105) ∈ CNR×NT denotes the channel matrix between the (cid:104)k, i(cid:105)-th device and the PS,
X(t)
(cid:104)k,i(cid:105)[c] denotes the signal transmitted by the
(cid:104)k,i(cid:105)
(cid:104)k, i(cid:105)-th device at the c-th channel use, N(t) (cid:44) [n(t)[1], · · · , n(t)[C]] ∈ CNR×C is an additive

(cid:104)k,i(cid:105)[C](cid:3) ∈ CNT×C with x(t)

(cid:104)k,i(cid:105)[1], · · · , x(t)

(cid:44) (cid:2)x(t)

white Gaussian noise (AWGN) matrix whose entries are independently drawn from CN (0, σ2),
and Y(t) (cid:44) (cid:2)y(t)[1], · · · , y(t)[C](cid:3) ∈ CNR×C with y(t)[c] denotes the received signal at the PS at

the c-th channel use.

Note that the two sums in (7) imply that all the edge devices are allowed to participate in

training transmit using shared radio resources. The superposition characteristic of electromagnetic

waves can be utilized for signal aggregation, as suggested by AirComp, while this non-orthogonal

transmission introduces inter-task interference. In the next subsection, we show how to aggregate

local gradients over the air using the underlying communication channel in (7).

C. Over-the-Air Model Aggregation

5The synchronization can be realized by using the existing techniques, e.g., the timing-advance mechanism for uplink

synchronization in 4G Long Term Evolution (LTE) [20].

6We assume that perfect CSI is available at the PS. Studies on CSI acquisition over MIMO MAC channels can be found,

e.g., in [21], [22].

In this subsection, we introduce the approach of over-the-air gradient aggregation based on the

8

(cid:104)k,i(cid:105) | k ∈ [K], i ∈ Mk} are ﬁrst pre-processed to yield signals {X(t)

underlying communication system in Section II-B. As illustrated in Fig. 2, the local gradients
{g(t)

(cid:104)k,i(cid:105) | k ∈ [K], i ∈ Mk},
which are then transmitted to the PS via the underlying communication channel in (7). After the
PS receives Y(t), it separately performs post-processing to obtain {ˆg(t)
of the desired aggregated gradients {g(t)

k=1, which are estimates
k=1. In the following, we introduce the pre-processing

k }K

k }K

and post-processing steps.

Fig. 2. Over-the-air gradient aggregation.

1) Pre-processing: This step performs separately on each participated device, aims to generate

appropriate transmitting signals. Speciﬁcally, we ﬁrst normalize g(t)

(cid:104)k,i(cid:105) to ˜g(t)

(cid:104)k,i(cid:105) ∈ RD by

(cid:104)k,i(cid:105)[d] = (cid:0)g(t)
˜g(t)

(cid:104)k,i(cid:105) − ¯g(t)

(cid:104)k,i(cid:105)1D×1

(cid:1)/

v(t)
(cid:104)k,i(cid:105),

(cid:113)

(8)

(cid:80)D

d=1 g(t)

(cid:104)k,i(cid:105) = 1

where ¯g(t)
D
practice [7], [23], we assume {¯g(t)

(cid:104)k,i(cid:105) = 1
(cid:104)k,i(cid:105)
(cid:104)k,i(cid:105)|∀k, i, t} are transmitted to the PS via error-free links.
To match the complex communication system in (7), we then modulate (analog modulation)

(cid:104)k,i(cid:105)[d] and v(t)
(cid:104)k,i(cid:105), v(t)

. Following the common

d=1

D

(cid:80)D

(cid:12)
(cid:104)k,i(cid:105)[d] − ¯g(t)
(cid:12)g(t)
(cid:12)

2

(cid:12)
(cid:12)
(cid:12)

the normalized gradient ˜g(t)

(cid:104)k,i(cid:105) to a complex data vector r(t)
(cid:19)

(cid:18)

(cid:104)k,i(cid:105), i.e.,

r(t)
(cid:104)k,i(cid:105)

(cid:44) ˜g(t)
(cid:104)k,i(cid:105)

1 :

+ j˜g(t)
(cid:104)k,i(cid:105)

D
2

(cid:18) D + 2
2

(cid:19)

: D

∈ CC,

(9)

where C = D/27. We transmit r(t)

(cid:104)k,i(cid:105) with C times of channel use (one element of r(t)

(cid:104)k,i(cid:105) for one

time of channel use). Speciﬁcally, the transmitted signal is given by

X(t)
(cid:104)k,i(cid:105)

7For simplicity, we assume that D is even.

(cid:44) u(t)

(cid:104)k,i(cid:105)r(t)

(cid:104)k,i(cid:105)

T ∈ CNT×C,

(10)

··················9

where u(cid:104)k,i(cid:105) ∈ CNT is the transmit beamforming vector of the (cid:104)k, i(cid:105)-th device. Recall that
X(t)

(cid:104)k,i(cid:105)[c]u(cid:104)k,i(cid:105) denoting the transmitted
signal of the (cid:104)k, i(cid:105)-th device at the c-th channel use, satisfying the following power constraint:

(cid:104)k,i(cid:105)[C](cid:3). Thus we have x(t)

(cid:104)k,i(cid:105)[1], · · · , x(t)

(cid:104)k,i(cid:105) = (cid:2)x(t)

(cid:104)k,i(cid:105)[c] = r(t)

E

(cid:104)

(cid:107)x(t)

(cid:104)k,i(cid:105)[c](cid:107)2(cid:105)

= 2(cid:107)u(cid:104)k,i(cid:105)(cid:107)2 ≤ P0,

(11)

where the equality follows the normalization in (8) (implying E(cid:2)|r(t)

(cid:104)k,i(cid:105)[c]|2(cid:3) = 2).

2) Post-processing: The received signals from the NT receive antennas are combined sepa-

rately using K receive beamforming vectors to obtain ˆr(t)

k , i.e.,

ˆr(t)
k = ζk

(cid:16)
k Y(t)
f H

k

(cid:17) T= ζk

(cid:16) (cid:88)

r(cid:104)k,i(cid:105)(H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105))T+

(cid:88)

(cid:88)

(cid:104)l,i(cid:105)(H(t)
r(t)

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))T+N(t)T(cid:17)
f †
k,k ∈ [K], (12)

i∈Mk

l(cid:54)=k

i∈Ml

where ζk ∈ R is a weighting factor and fk ∈ CNR is the normalized receive beamforming vector
of task k with (cid:107)fk(cid:107) = 1. Estimates of the desired aggregated gradients {ˆg(t)
from {ˆr(t)

k=1 is reconstructed

k }K

k }K

k=1 by

ˆg(t)
k =

(cid:80)

1

(cid:104)

Q(cid:104)k,i(cid:105)

i∈Mk

Re{ˆr(t)

k }T, Im{ˆr(t)

k }T(cid:105)T

+ ¯g(t)

k 1D×1 ∈ RD, k ∈ [K],

(13)

where ¯g(t)
k

(cid:44) ((cid:80)

i∈Mk

Q(cid:104)k,i(cid:105)¯g(t)

(cid:104)k,i(cid:105))/((cid:80)

i∈Mk

Q(cid:104)k,i(cid:105)).

D. OA-FMTL Framework

We ﬁrst describe the model aggregation error of each task k, and then summarize the OA-

FMTL framework in this subsection. From the channel model in (7), the model aggregations
{ˆg(t)
noise. The global model w(t)

k=1 inevitably suffer from distortions caused by the inter-task interference and the channel

k of task k is updated with the gradient aggregation ˆg(t)
k :

k }K

w(t+1)
k

= w(t)

k − ηkˆg(t)

k = w(t)

k − ηk

(cid:16)

∇Fk(w(t)

k ) − e(t)

k

(cid:17)

,

(14)

where ∇Fk(w(t)
k ) (cid:44) 1
Qk
k , and e(t)
Fk(wk) at wk = w(t)

(cid:80)Qk

k

n=1 ∇fk

(cid:0)wk; ξk,n

(cid:1) is the gradient of the loss function of the k-th task

is the error caused by gradient uploading, which can be divided

into two parts:

k )−g(t)
k = ∇Fk(w(t)
e(t)
(cid:124)
(cid:123)(cid:122)
(cid:125)
e(t)
ds,k

k

+g(t)
(cid:124)

k − ˆg(t)
(cid:125)
(cid:123)(cid:122)
e(t)
com,k

k

,

(15)

ds,k denotes the error caused by device selection, and e(t)

where e(t)
error due to the inter-task interference and the noise. To alleviate the impact of e(t)

com,k denotes the communication
k on the
learning performance, the PS optimizes the device selection, transmit and receive beamforming.

10

We summarize the OA-FMTL framework described above in Algorithm 1. Besides, we deﬁne
k=1, uk (cid:44) {u(cid:104)k,i(cid:105)}i∈Mk,
k=1. In the following section, we establish the connection between the model

the following variables for notational brevity: M (cid:44) {Mk}K
and u (cid:44) {uk}K

k=1, f (cid:44) {fk}K

aggregation errors in (15) and the learning performance of the OA-FMTL.

Algorithm 1 OA-FMTL framework
Input: T , {Q(cid:104)k,i(cid:105)}.
1: Initialization: t = 0, the global models {w(0)
2: for t ∈ [T ] do
3:

The PS estimates the CSI and optimize (M, f , u);
The PS sends the global models {w(t)
for k ∈ [K], i ∈ Mk in parallel do

k } on the PS.

Device (cid:104)k, i(cid:105) computes its local gradient g(t)
Device (cid:104)k, i(cid:105) uploads its gradient g(t)

end for
The PS recovers the aggregated gradient ˆg(t)
The PS updates the global models {w(t+1)

k

10:
11: end for

4:

5:

6:

7:

8:

9:

k } to the devices through orthogonal transmission;

(cid:104)k,i(cid:105) based on the local dataset based on (4);

(cid:104)k,i(cid:105) to the PS via (8)-(11);

k of each task k based on (12) and (13);
} based on (14);

III. PERFORMANCE ANALYSIS

In this section, we analyse the learning performance of the OA-FMTL. We start with some

standard assumptions on the loss functions {Fk(·)}K

k=1,

A. Preliminaries

To conduct convergence analysis, following the stochastic optimization literature [24], [25],

we make the following four assumptions on {Fk(·)}K

k=1:

Assumption 1. For each task k, the loss function Fk is continuously differentiable, and the

gradient ∇Fk(·) is uniformly Lipschitz continuous with parameter ωk, i.e.,

(cid:107)∇Fk(w) − ∇Fk(w(cid:48))(cid:107) ≤ ωk(cid:107)w − w(cid:48)(cid:107), ∀w, w(cid:48) ∈ RD, k ∈ [K].

(16)

11

Assumption 2. For each task k, loss function Fk is strongly convex with positive parameter µk:

Fk(w) ≥ Fk(w(cid:48)) + (w − w(cid:48))T∇Fk(w(cid:48)) +

µk
2

(cid:107)w − w(cid:48)(cid:107)2, ∀w, w(cid:48) ∈ RD, k ∈ [K].

(17)

Assumption 3. Fk(·), k ∈ [K] are twice-continuously differentiable.

Assumption 4. The gradient vector is upper bounded by

(cid:107)∇fk

(cid:0)wk; ξk,n

(cid:1) (cid:107)2 ≤ β1 + β2(cid:107)∇Fk(w(t)

k )(cid:107)2, ∀k ∈ [K],

(18)

for some constants β1 ≥ 0 and β2 ≥ 0. Both β1 and β2 are constants shared by {Fk(·)}K

k=1.

Fig. 3. An illustration of the gradients in the OA-FMTL

Furthermore, unlike the approaches in [10], [16] where the local gradients from a common

task are treated as independent, we ﬁnd that these local gradients are highly correlated in a

typical learning task. As such, it is of critical importance to understand the impact of the spatial

correlation between gradients on the learning performance. To this end, we introduce a probability
(cid:104)k,1(cid:105), · · · , ˜g(t)
model for the gradients as follows. Let ˜G(t)
(cid:104)k,Mk(cid:105)] ∈ RD×Mk be the local
k
gradients from the Mk devices of task k at the t-th round. Let z(t)
k,d ∈ RMk be the d-th dimension
of the local gradients from a common task, and z(t)
k , as shown in Fig. 3.
k,d
To track the correlation of the gradients, we make the following assumption on the distribution

T is the d-th row of ˜G(t)

(cid:44) [˜g(t)

of the gradients elements.

Assumption 5. For the t-th communication round, the gradient matrices { ˜G(t)
independent and non-identically distributed. For the k-th task, the gradients {z(t)

k |k ∈ [K]} are
k,d|d ∈ [D]}, are

……………………independent and identically distributed. That is,

p(t) (cid:16)

{ ˜G(t)

k |k ∈ [K]}

K
(cid:89)

D
(cid:89)

(cid:17)

=

p(t)
k

(cid:17)

(cid:16)
z(t)
k,d

, ∀d,

k=1

d=1

12

(19)

where p(t)(·) denotes the distribution of the elements of { ˜G(t)
distribution of the elements of z(t)
devices have the same degree of variation, i.e., v(t)

k (·) denotes the
k,d. Furthermore, for each task k, the local gradients of the Mk

k |k ∈ [K]}, and p(t)

(cid:104)k,1(cid:105) = · · · = v(t)

(cid:104)k,Mk(cid:105) = v(t)
k .

We now focus on the spatial correlation between the local gradients in a common task, i.e.,

between the entries of z(t)

k,d. The auto-correlation matrix for task k is then deﬁned by

ρ(t)
k

(cid:44) E(cid:2)z(t)

k,d(z(t)

k,d)T(cid:3) ∈ RMk×Mk, ∀d

(20)

where the (i, j)-th entry is denoted as ρ(t)
i and device j of task k. Note that each element of z(t)
Thus, ρ(t)

(cid:104)k,i(cid:105),(cid:104)k,i(cid:105) = 1, and ρ(t)

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105) ∈ [−1, 1], for ∀k, i, j.

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105), measuring the spatial correlation between device
(cid:104)k,i(cid:105)[d], has zero mean and unit variance.

k,d, ˜g(t)

Fig. 4. Experimental results of ρ(t)
k versus communication rounds t. We train three LeNet [26]-based FL models, with M1 =
M2 = M3 = 10 and 20 Monte Carlo trials. The learning rate is set to η1 = η2 = η3 = 0.002, and the momentum is set to
0.9. The loss function is the cross-entropy loss. Each training dataset is assigned 60000 samples and the local training data
is assigned 6000 samples i.i.d. drawn from the dataset. The local updates have 5 times of stochastic gradient descents (SGD),
and the mini-batch size is set to be 1200. The gradients uploading is error-free and all devices are selected. We approximate
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105) by ρ(t)
ρ(t)

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105) ≈ 1

(cid:104)k,i(cid:105)[d]˜g(t)

d=1 ˜g(t)

(cid:104)k,j(cid:105)[d].

(cid:80)D

D

The heatmaps in Fig. 4 illustrate the experimental results of ρ(t)

k versus the communication
round t of three datasets, MNIST [27], Fashion-MNIST [28], and KMNIST [29], where the

gradients are updated ideally without any transmission error. Intuitively, the darker the color of

a pixel, the stronger the spatial correlation between the gradients from the corresponding two

13

devices. From Fig. 4, we see that the correlation grows substantially at the beginning of training,

say, for communication round t ≤ 50. When the training approaches convergence, most of the
elements of ρ(t)
k

reduce to less than 0.3, which indicates that the local gradients have weaker

cross-correlation when the models are close to convergence.

B. Convergence Analysis of OA-FMTL

We start with the analysis of the loss function Fk(·) of task k at each communication round.

From [24, Lemma 2.1], Assumptions 1-4 lead to an upper bound of the loss function Fk(·) at

the t-th round of the recursive updates in (14). We therefore have the following theorem.

Theorem 1. Under Assumptions 1-5, the expected loss function of each task k at the t-th

communication round is bounded by

E[Fk(w(t+1)

k

)] ≤ E[Fk(w(t)

k )]−

1
2ωk

(cid:16)
E[(cid:107)∇Fk(w(t)

k )(cid:107)2]−2E[(cid:107)e(t)

ds,k(cid:107)2]−2E[(cid:107)e(t)

com,k(cid:107)2]

(cid:17)

,

(21)

where the device selection MSE is bounded by

E[(cid:107)e(t)

ds,k(cid:107)2] ≤

(cid:88)

Qk −

(cid:16)

4
Q2
k

i∈Mk

Q(cid:104)k,i(cid:105)

(cid:17)2 (cid:16)

β1 + β2E

(cid:104)

(cid:107)∇Fk(w(t)

k )(cid:107)2(cid:105)(cid:17)

,

(22)

and the communication MSE is given by

E[(cid:107)e(t)

com,k(cid:107)2] =

1

((cid:80)

i∈Mk

Q(cid:104)k,i(cid:105))2

(cid:18)

(cid:88)C

c=1

E

(cid:20)(cid:12)
(cid:12)
(cid:12)

(cid:124)

(cid:88)

i∈Mk

(cid:88)

l(cid:54)=k

+ ζ 2
k
(cid:124)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:88)

E

k H(t)
f H
(cid:123)(cid:122)
the second term: the interference

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105)r(t)

i∈Ml

(cid:16)

(cid:113)

Q(cid:104)k,i(cid:105)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

k H(t)

v(t)
k −ζkf H
(cid:123)(cid:122)
the ﬁrst term: the misalignment error
2(cid:105)

2(cid:105)

E

(cid:104)(cid:12)
+ ζ 2
k n(t)[c]
(cid:12)f H
(cid:12)
k
(cid:125)
(cid:123)(cid:122)
(cid:124)
the third term: the noise

(cid:12)
(cid:12)
(cid:12)

(cid:104)l,i(cid:105)[c]

(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:17)

r(t)
(cid:104)k,i(cid:105)[c]

2(cid:21)

(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:19)

.

(23)

Proof. Please refer to Appendix A.

From Theorem 1, we obtain an upper bound of the loss function Fk(·) w.r.t. the device selection

MSE and the communication MSE. By inspection, the communication MSE in (23) consists of

three terms, where the ﬁrst term represents the misalignment error of the aggregation gradients

from the devices in task k, the second term represents the error caused by the interference from

the devices associated with other tasks, and the third term represents the error caused by the

channel noise. Note that the expression in (23) is convex w.r.t. ζk for any ﬁxed device selection

set Mk and beamforming fk and {u(cid:104)k,i(cid:105)}i∈Mk. Thus, we have the following corollary.

Corollary 1. The optimal ζk is given by

ζ ∗
k =

(cid:113)

(cid:80)

v(t)
k
(cid:16)(cid:80)K

2

i,j∈Mk
(cid:80)

l=1

i,j∈Ml

(cid:16)

Q(cid:104)k,i(cid:105)(f H

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)
ρ(t)
(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

k H(t)

k H(t)

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H + Q(cid:104)k,j(cid:105)f H
k H(t)

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))Hf H

(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2/2

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

(cid:17)

(cid:17) .

14

(24)

Proof. Please refer to Appendix B.

We emphasize that the design strategy of the weighting factors {ζk}K

k=1 in (24) is different
from that of the existing over-the-air FL approaches. For each task k, the ﬁrst component

in (23) represents the misalignment error of the gradient aggregation. In the existing scheme,

k H(t)

Refs. [7], [10], [11] force this component to zero, which leads to the constraints of Q(cid:104)k,i(cid:105)
ζkf H

v(t)
k −
(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105) = 0, ∀i ∈ Mk, and then minimize the rest of the communication MSE to
determine ζk. For all devices in the k-th task, to satisfy the constraints and the transmit power
(cid:104)k,i(cid:105)(cid:107)2).
budgets in (11), the choice of ζk is therefore given by ζ 2
k /(P0(cid:107)f H
(cid:104)k,i(cid:105)(cid:107)2).
Thus, ζk is dominated by the device with the worst channel condition (in terms of (cid:107)f H
That is, the device with the worst channel becomes the bottleneck of the overall scheme, also

k = maxi∈Mk Q2

k H(t)
k H(t)

(cid:104)k,i(cid:105)v(t)

(cid:113)

known as the straggler problem. However, instead of zero-forcing, our approach tolerances the

misalignment error but requires the design of ζk to directly minimize the overall communication

MSE for task k. In this way, ζk is no longer solely determined by the worst channel condition,

which signiﬁcantly relieves the straggler problem. Note that our approach is also different from

the approach in [16] since we aware the spatial correlation between the local gradients, which

brings substantial improvement in learning performance. Numerical results will be presented

later in Section V for veriﬁcation.

In the former of this subsection, we obtain an upper bound of the loss function Fk(·) of task

k at the t-th communication round in Theorem 1. In the following, we analyse the convergence

performance of the entire OA-FMTL model at the t-th round, and obtain an upper bound of the
average difference between the overall loss function at the (t + 1)-th round F (cid:0)w(t+1)(cid:1) and the
optimal F (w∗), i.e., E (cid:2)F (cid:0)w(t+1)(cid:1) − F (w∗)(cid:3). Based on Theorem 1 and Corollary 1, we bring

the device selection MSE and the communication MSE into an analysis framework, and obtain

the following theorem.

Theorem 2. Based on Assumptions 1-4, the overall loss function at the t-th round F(w(t+1))

satisﬁes the following inequality:

E[F(w(t+1))−F(w∗)] ≤ E[F(w(t))−F(w∗)]

(cid:16)

1 −

(cid:17)

µ
ω

+

(cid:16)2µβ2
ω

E[F(w(t))−F(w∗)] +

(cid:17)

β1
ω

E (t)(M, f , u),

(25)

15

where ω (cid:44) maxk ωk, µ (cid:44) mink µk, E (t)(M, f , u) (cid:44) (cid:80)K
is denoted by

k=1 d(t)

k (Mk, fk, uk), and d(t)

k (Mk, fk, uk)

k (Mk, fk, uk) (cid:44) 4
d(t)
Q2
k

(cid:32)

Qk −

(cid:88)

i∈Mk

(cid:33)2

Q(cid:104)k,i(cid:105)

+

1

(cid:32)

(cid:88)

(cid:0)(cid:80)

i∈Mk

(cid:1)2

Q(cid:104)k,i(cid:105)

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,i(cid:105)Q(cid:104)k,j(cid:105)

(cid:16)(cid:80)

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)

(cid:16)

−

Q(cid:104)k,i(cid:105)(f H

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H + Q(cid:104)k,j(cid:105)f H

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

(cid:17)(cid:17)2



(cid:16)(cid:80)K

l=1

4

(cid:80)

i,j∈Ml

ρ(t)
(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))Hf H

k H(t)

(cid:17)
(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2/2

k H(t)
k H(t)


 .

(26)

Proof. Please refer to Appendix C.

Theorem 2 provides a metric for evaluating the learning performance of the OA-FMTL model.

In the next section, we formulate the optimization problem based on this metric, and propose

an efﬁcient algorithm to solve this problem.

IV. SYSTEM OPTIMIZATION
To achieve better learning performance, we aim to minimize the upper bound of E[F (cid:0)w(t+1)(cid:1)−
F (w∗)] in (25), or equivalently, to minimize E (t)(M, f , u) over device selection set M, receive

beamforming f and transmit beamforming u, as detailed below.

A. Problem Formulation

From Theorem 2, the gap E[F(w(t+1))−F(w∗)] has an upper bound in (25). The upper bound

is monotonically increasing w.r.t. E (t)(M, f , u) since 2µβ2
ω > 0. With
ω
the target to minimize the gap E[F(w(t+1))−F(w∗)], we minimize E (t)(M, f , u) at round t, and

E[F(w(t))−F(w∗)] + β1

we formulate the optimization problem P1 as:

(P1) : min
M,f ,u

E (t)(M, f , u) =

(cid:88)K

k=1

d(t)
k (Mk, fk, uk)

s.t. Mk ⊂ [Mk], k ∈ [K],

(cid:107)fk(cid:107) = 1, k ∈ [K],

(cid:13)
(cid:13)u(cid:104)k,i(cid:105)

(cid:13)
2 ≤ P0/2, k ∈ [K], i ∈ [Mk],
(cid:13)

where

d(t)
k (Mk, fk, uk) =

(cid:32)

Qk −

4
Q2
k

(cid:88)

i∈Mk

(cid:33)2

Q(cid:104)k,i(cid:105)

+

(cid:80)

i,j∈Mk

with

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,i(cid:105)Q(cid:104)k,j(cid:105)− a(t)
k (Mk,fk,uk)2
ρ(t)
4b(t)
k (Mk,fk,uk)

(cid:0)(cid:80)

i∈Mk

(cid:1)2

Q(cid:104)k,i(cid:105)

k (Mk,fk,uk) (cid:44)(cid:88)
a(t)
k (Mk,fk,uk) (cid:44)(cid:88)K
b(t)

i,j∈Mk
(cid:88)

l=1

i,j∈Ml

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)

(cid:16)

Q(cid:104)k,i(cid:105)(f H

k H(t)

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H+Q(cid:104)k,j(cid:105)f H

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

(cid:17)

,

ρ(t)
(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

k H(t)

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))Hf H

k H(t)

(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2/2.

16

(27a)

(27b)

(27c)

(27d)

, (28)

(29a)

(29b)

P1 is an optimization problem w.r.t. device selection set M, receive beamforming vectors f and

transmit beamforming vectors u, respectively.

We next design an AO-based algorithm to solve the optimization problem P1. P1 contains

2M +K optimization variables, i.e., M device selection indices, K receive beamforming vectors

at the PS and M transmit beamforming vectors at the devices. P1 is non-convex due to the

coupling of M, f and u. Thus, we adopt the AO framework to solve the problem in a suboptimal

fashion. First, we optimize the beamforming vectors with ﬁxed device selection set M. Second,

with ﬁxed beamforming vectors, we optimize device selection set M with Gibbs sampling [7].

The two steps iterate until convergence. The details are discussed in the following subsections.

B. Optimization of f and u with ﬁxed M

We ﬁrst optimize beamforming vectors f and u with ﬁxed device selection set M. By

inspection of (P1), d(t)

k (Mk, fk, uk) in (28) is invariant to the value of (cid:107)fk(cid:107). Therefore, the
unit-length constraint of fk in (27c) can be ignored without changing the minimum of (P1).

Further, we drop the constant terms in the objective function E (t)(M, f , u) to obtain

17

min
{fk},u

−

K
(cid:88)

k=1

4 (cid:0)(cid:80)

a(t)
k (Mk, fk, uk)2
(cid:1)2 b(t)
Q(cid:104)k,i(cid:105)

i∈Mk

k (Mk, fk, uk)

, s.t.

(27d).

(30)

(30) is still non-convex because both the numerator and the denominator in the k-th summand

contain the optimization variables fk and uk. We adopt the quadratic transform in fractional

programming (FP) [30] to decouple the numerator and the denominator as

min
{fk},u,y

−

K
(cid:88)

k=1

(cid:18) yka(t)
k (Mk, fk, uk)
(cid:80)

Q(cid:104)k,i(cid:105)

i∈Mk

− y2

kb(t)

k (Mk, fk, uk)

(cid:19)

, s.t.

(27d).

(31)

where y = [y1, · · · , yk]T ∈ RK is an auxiliary vector introduced by FP. Note that (31) reduces

to (30) by letting each yk take its optimal form as

yk =

a(t)
k (Mk, fk, uk)
(cid:1) b(t)
Q(cid:104)k,i(cid:105)

k (Mk, fk, uk)

2 (cid:0)(cid:80)

i∈Mk

.

(32)

1) Optimizing u(cid:104)k,i(cid:105) with ﬁxed {u(cid:104)k,i(cid:105)}j(cid:54)=i and {fk}: With ﬁxed {u(cid:104)k,i(cid:105)}j(cid:54)=i and {fk}, when

i /∈ Mk, i.e., the device (cid:104)k, i(cid:105) is not selected, we obtain u(cid:104)k,i(cid:105) = 0NT×1 directly. When i ∈ Mk,
i.e., the device (cid:104)k, i(cid:105) is selected, (31) reduces to

(P2) : min
u(cid:104)k,i(cid:105)

(cid:104)k,i(cid:105)A(t)
uH

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105) − 2 Re

(cid:110)

b(t)
(cid:104)k,i(cid:105)

Hu(cid:104)k,i(cid:105)

(cid:111)

, s.t.

(27d).

where A(t)

(cid:104)k,i(cid:105) ∈ CNT×NT and b(t)

(cid:104)k,i(cid:105) ∈ CNT are deﬁned by

A(t)
(cid:104)k,i(cid:105)

(cid:44) (cid:88)K

l=1

yk

b(t)
(cid:104)k,i(cid:105)

(cid:44)

(cid:80)

j∈Mk

l H(t)

l (H(t)
(cid:104)k,i(cid:105))Hflf H
y2
ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,j(cid:105)(f H
(cid:80)

(cid:104)k,i(cid:105),
k H(t)

Q(cid:104)k,i(cid:105)

i∈Mk

(cid:104)k,i(cid:105))H

K
(cid:88)

−

y2
l

(cid:88)

l=1

j∈Mk,j(cid:54)=i

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)(f H

l H(t)

(cid:104)k,i(cid:105))Hf H

l H(t)

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105).

(33a)

(33b)

Note that A(t)

(cid:104)k,i(cid:105) is a positive semideﬁnite matrix, and that the constraint (27d) is convex w.r.t.
u(cid:104)k,i(cid:105). Thus, P2 is a convex quadratically constrained quadratic programming (QCQP) problem

w.r.t. u(cid:104)k,i(cid:105), which can be solved by standard convex optimization tools.

2) Optimizing fk with ﬁxed u: Similarly to IV-B1, with ﬁxed u, (31) reduces to

(P3) : min

fk

k A(t)
f H

k fk − 2 Re{b(t)

k

Hfk}

where A(t)

k ∈ CNR×NR and b(t)

k ∈ CNR are denoted by

A(t)
k

(cid:44) y2
k

b(t)
k

(cid:44)

(cid:80)

(cid:88)K

(cid:88)

l=1
yk

i,j∈Ml
(cid:88)

i∈Mk

Q(cid:104)k,i(cid:105)

i,j∈Mk

(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)H(t)
ρ(t)

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105)(H(t)

(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105))H + (y2

kσ2/2)INR

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,j(cid:105)H(t)
ρ(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105).

18

(34a)

(34b)

P3 is convex w.r.t. fk by noting the positive semideﬁnite matrix A(t)
k . Note that fk here is obtained
by dropping the unit-length constraint in (27c). Thus, we ﬁnally obtain the optimal fk by scaling

the obtained fk to a unit vector. We summarize the optimization of f and u in Algorithm 2.

Algorithm 2 AO Algorithm to Optimize f and u
Input: M, {ρ(t)
1: Initialization: y, f and u.
2: for τ ∈ [Imax] do
for k ∈ [K] do
3:

k , H(t)

k,(cid:104)l,i(cid:105), Q(cid:104)k,i(cid:105)|k, l ∈ [K], i ∈ [Mk]}, and Imax.

4:

5:

6:

7:

8:

9:

10:

for i ∈ Mk do
Compute A(t)
(cid:104)k,i(cid:105), b(t)
Optimize u(cid:104)k,i(cid:105) by solving (P2);

(cid:104)k,i(cid:105) based on (33) ;

end for
Compute A(t)
Optimize fk by solving (P3), update fk (by scaling to a unit vector)
Updates y based on (32);

k based on (33);

k , b(t)

end for

11:
12: end for
Output: (f , u).

C. Optimizing M with ﬁxed f and u

In this subsection, we optimize device selection set M with ﬁxed beamforming f and u. For

notational convenience, we use a binary indication vector s to represent the device selection
set M, i.e., s (cid:44) [sT
k ]T ∈ {0, 1}M , where sk ∈ {0, 1}Mk with the i-th entry s(cid:104)k,i(cid:105) = 1
meaning that the (cid:104)k, i(cid:105)-th device is selected and s(cid:104)k,i(cid:105) = 0 otherwise. Note that M and s can be

1 , · · · , sT

interchanged. Since s is discrete, we introduce the Gibbs sampling to optimize s by following

the approach in [7]. We denote sold as the sampling device selection solution obtained from

the proceeding sampling round. At the current sampling round, the sampling set S is generated
from sold, given by S (cid:44) {sold} ∪ {sold

(cid:104)k,i(cid:105)|k ∈ [K], i ∈ [Mk]}, where sold

(cid:104)k,i(cid:105) denotes the indication

vector that differs from sold only at the (cid:104)k, i(cid:105)-th element, corresponding to the (cid:104)k, i(cid:105)-th device.

We sample snew according to the following distribution π(snew):

19

π(snew) (cid:44)

(cid:80)K

k=1

(cid:80)Mk

i=1 exp

exp (−φ(snew)/β)
(cid:16)

(cid:17)

−φ(sold

(cid:104)k,i(cid:105))/β

+ exp (−φ(sold)/β)

,

(35)

where φ(sold

(cid:104)k,i(cid:105)) denotes the objective E (t)(M, f , u) with f and u obtained by Algorithm 2 with the
(cid:104)k,i(cid:105), and β > 0 denotes the “temperature parameter”
to accelerate convergence. We summarize the overall algorithm for the optimization of M, f

device selection set M corresponding to sold

and u in Algorithm 3.

(cid:104)l,i(cid:105), Q(cid:104)k,i(cid:105)|k, l ∈ [K], i ∈ [Mk]}, Jmax, β, γ.

Algorithm 3 AO Algorithm plus Gibbs Sampling
k , H(t)
Input: {ρ(t)
1: Initialization: sold = 1M ×1.
2: for j ∈ [Jmax] do
sold = snew;
3:
Generate S;
for every sold

(cid:104)k,i(cid:105) ∈ S do

4:

5:

6:

7:

8:

end for
Sample snew according to (35);
Refresh β = γβ for a certain γ ∈ (0, 1);

9:
10: end for
Output: snew with corresponding (f , u).

Optimize (f , u) by solving P2 with given sold

(cid:104)k,i(cid:105), with Algorithm 2;

D. Complexity Analysis

We now brieﬂy discuss the computational complexity involved in Algorithms 2 and 3. For

Algorithm 2, both P2 and P3 are QCQP problems that can be solved by existing optimization

solvers based on the interior point method. Thus, the worst-case complexity of Algorithm 2
is given by O(Imax(K + M )N 3.5), where N (cid:44) max{NT, NR} denotes the maximum number

of the transmit or receive antennes, Imax is the max iteration times of optimization, and M

is the total number of the devices in the OA-FMTL. Algorithm 3 invokes Algorithm 2 for

JmaxM times to optimize device selection set M. Thus, the complexity of Algorithm 3 is
O(JmaxImax(KM + M 2)N 3.5).

We note that the complexity of Algorithm 3 is quadratic in M , due to the use of Gibbs sampling

in the optimization of device selection. When the number of devices M in the OA-FMTL is

20

large, Gibbs sampling causes a tremendous computation burden. Recall from Section III-B that

the device selection is adopted by the existing works to reduce the impact of stragglers. As

pointed out in Section III-B, our proposed scheme optimises the weighting factor ζk to minimize

the communication MSE, which relieves the straggler problem signiﬁcantly. We observe from

experiments that the improvement of device selection in Algorithm 3 is negligible, as compared

to the performance of Algorithm 2. Therefore, we prefer to use Algorithm 2 in the system

optimization, since Algorithm 2 has a much lower complexity.

A. Simulation Setups

V. NUMERICAL RESULTS

We consider a three-dimensional (3-D) simulation scenario as shown in Fig. 5. The point

locations are represented by cylindrical coordinate triples (δ, θ, χ), where δ, θ and χ denote

the radial distance, the azimuth, and the height, respectively. The locations of the devices are

distributed as follows. All the devices in the OA-FMTL are located in a circle with center

O = (0, 0, 0) and radius ∆. We set the location of the (cid:104)k, i(cid:105)-th device to (δ(cid:104)k,i(cid:105), θ(cid:104)k,i(cid:105), 0), where

δ2
(cid:104)k,i(cid:105) is uniform in [0, ∆], and θ(cid:104)k,i(cid:105) is uniform in [0, 2π). The PS is placed at the center of the
˜H(cid:104)l,i(cid:105),
circle, i.e.,(0, 0, 10). We adopt the channel model in [31], given by H(cid:104)l,i(cid:105) =
where the entries of ˜H(cid:104)l,i(cid:105) are modeled as i.i.d. circularly symmetric complex Gaussian (CSCG)
random variables with zero-mean and unit-variance, GS and GD are the antenna gains at the PS

GSGDκ˜δ−α

(cid:113)

(cid:104)l,i(cid:105)

and the devices, respectively, κ is the path loss at the reference distance δ0 = 1 m [32], α is the
path loss exponent, and ˜δ(cid:104)l,i(cid:105) (cid:44) (cid:113)
δ2
(cid:104)l,i(cid:105) + 102 is the distance between the (cid:104)l, i(cid:105)-th device and the
PS. The simulation settings are given in Table I.

TABLE I
SYSTEM PARAMETERS

Parameter
NT
α
P0
σ2

Value
2
3.8
1 W
−80 dBm

Parameter
NR
κ
β
Qk

Value
8
−60 dB
1
60000

Parameter Value

Parameter Value

Imax
GS
γ

50
5 dBi
0.9

Jmax
GD
∆

50
0 dBi
100 m

We set three image classiﬁcation tasks as FL tasks in the OA-FMTL, with each FL task being

trained on an individual dataset, i.e., MNIST for task 1, Fashion-MNIST for task 2 and KMNIST

for task 3. For each FL task, we train a CNN with two 5×5 convolution layers (the ﬁrst with 16

21

Fig. 5. An illustration of locations for the devices and the PS in the OA-FMTL on the vertical view.

(a)

channels, the second with 32, each followed by 2×2 max pooling), a fully connected layer with

50 units and ReLu activation, and a ﬁnal softmax output layer (D = 39408 total parameters). The

loss function is the cross-entropy loss. We study two ways of partitioning the dataset Ak over

devices: 1) i.i.d., where the data are shufﬂed, and then assigned evenly to the Mk devices; 2)
Non-i.i.d., where each device randomly selects 5 classes, and then randomly draws Qk
5Mk

samples

from each selected class.

B. Comparisons of the Proposed Algorithms Under Various Settings

In this subsection, we study the impact of various approximations of the correlation matrices

{ρ(t)

k }. Speciﬁcally, we consider the following three approximations of the correlation matrix:
• Approximation 1: ρ(t)
• Approximation 2: ρ(t)
k

d=1 z(t)
T.
(cid:104)k,d(cid:105)
is approximated by ρ(t)

k = (cid:15)1Mk +(1−(cid:15))IMk, where (cid:15) is an empirical

(cid:104)k,d(cid:105)z(t)

k = 1

(cid:80)D

D

parameter to represent the degree of correlation.

Approximation 1 estimates the spatial correlation between the gradients of the devices from the
same task k at each round. However, Approximation 1 is impractical since the calculation of ρ(t)
k

requires the knowledge of the local gradients uploaded by the devices. In contrast, Approximation

2 is more practical in implementation, where (cid:15) can be set empirically. Therefore, the proposed

schemes with Approximation 1 are only used as a performance baseline, and the schemes

with Approximation 2 are used in performance comparison with other counterpart schemes.

In addition, we adopt the error-free case as a performance upper bound of each FL task in the

OA-FMTL:

• Error-free bound: Each FL task is trained independently with all the devices being selected,

and the model aggregation is error-free at each communication round.

22

Fig. 6. FL test accuracy of the proposed Algorithm 2 with different approximations of ρ(t)
with K = 2, M1 = M2 = 20. Left: MNIST; right: Fashion-MNIST; top: i.i.d. data; bottom: non-i.i.d. data.

k versus the communication rounds,

We plot the FL test accuracy curve of the proposed Algorithm 2 with the above approximations

of the correlation matrix ρ(t)
k

in Fig. 6. We train two FL tasks, namely, tasks 1 and 2, both on

i.i.d. data and on non-i.i.d. data. We set the numbers of devices M1 = M2 = 20, the learning rates

η1 = η2 = 0.05, and the momentum = 0.5. The local updates consist of 5 times of SGD. The results

are averaged over 20 Monte Carlo trials. In Approximation 2, we set (cid:15) = 0, (cid:15) = 0.5, and (cid:15) = 1. From

Fig. 6, we see that in the case of i.i.d. data, both Approximation 1 and Approximation 2 with

(cid:15) = 1 achieve test accuracies close to the error-free bound on both two tasks, and Approximation

2 with (cid:15) = 0 has the worst learning performance. This is because Approximation 2 with (cid:15) = 0

suffers from a serious aggregation error for ignoring the correlation between the local gradients.

On the other hand, in the case of non-i.i.d. data, the accuracies achieved by Approximation 1

and Approximation 2 with (cid:15) = 0.5 are close to the error-free bound, since (cid:15) = 0.5 approximates

the spatial correlation more precisely in the case of non-i.i.d. data. Thus, Approximation 2 with

(cid:15) = 1 for i.i.d. data and Approximation 2 with (cid:15) = 0.5 for non-i.i.d. data are preferred in the

system design.

We next study the necessity of device selection by comparing the proposed Algorithms 2 and

3. We simulate Algorithms 2 and 3 on tasks 1 and 2, with (cid:15) = 1 on i.i.d. data and (cid:15) = 0.5 on

020406080100Communication round00.250.50.751Test accuracy(a) MNIST, i.i.d. dataError-freeApproximation 1Approximation 2 with  = 1Approximation 2 with  = 0.5Approximation 2 with  = 0020406080100Communication round00.20.40.60.8Test accuracy(b) Fashion-MNIST, i.i.d. data020406080100Communication round00.20.40.60.81Test accuracy(c) MNIST, non-i.i.d. data020406080100Communication round00.20.40.60.8Test accuracy(d) Fashion-MNIST, non-i.i.d. data152025300.50.60.70.75Error-freeApproximation 1Approximation 2 with  = 1Approximation 2 with  = 0.5Approximation 2 with  = 0152025300.40.50.623

non-i.i.d. data. The numbers of devices are set to M1 = M2 = 10. The learning rates are set

to η1 = η2 = 0.05, and the momentum is set to 0.5. The local updates have 10 times of SGD.

The results are averaged over 10 Monte Carlo trials 8. In Fig. 7, we present the test accuracy of

Algorithms 2 and 3 versus communication rounds on i.i.d. and non-i.i.d. data. From Fig. 7, we see

that Algorithms 2 and 3 always perform closely. This implies that device selection is no longer

necessary to our proposed scheme, which avoids the high computational complexity involved

in the implementation of device selection. Thus, we henceforce always employ Algorithm 2

performance comparison when we refer to the proposed AO algorithm.

Fig. 7. Test accuracy versus communication rounds, with K = 2, M1 = M2 = 10. Left: MNIST; right: Fashion-MNIST; top:
i.i.d. data; bottom: non-i.i.d. data.

C. Comparisons With Existing Schemes

In this subsection, we present the performance of system optimization obtained by the proposed

AO algorithm on i.i.d. data. We consider the following baselines for comparison:

• SOCP-based cooperative power control [16]: This method assumes that all the devices

selected, and that the local gradients from devices in the same task are independent with

each other. The phases of transmit beamforming u(cid:104)k,i(cid:105), ∀k, i are given by the zero-forcing.

8Note that we choose a relatively small number of trials due to the high computational complexity of device selection in
Algorithm 3. In simulations, we use a personal computer with an Intel(R) Core(TM) i7-10700 CPU and a GTX 1050Ti GPU.
One Monte Carlo trial of Algorithm 3 takes about 10 hours.

020406080100Communication round00.250.50.751Test accuracy(a) MNIST, i.i.d. dataError-freeAlgorithm 2Algorithm 3020406080100Communication round00.20.40.60.8Test accuracy(b) Fashion-MNIST, i.i.d. data020406080100Communication round00.250.50.751Test accuracy(c) MNIST, non-i.i.d. data020406080100Communication round00.20.40.60.8Test accuracy(d) Fashion-MNIST, non-i.i.d. data24

The design of transmit power (cid:107)u(cid:104)k,i(cid:105)(cid:107)2, ∀k, i are formulated as an second-order cone pro-

gramming (SOCP) problem solved by the bisection method.

• SCA-based optimization and device selection [7]: Mk, fk, and {u(cid:104)k,i(cid:105)} for each FL task are

optimized separately. For task k, the receive beamforming fk is optimized by the successive

convex approximation (SCA)-based optimization algorithm, with given transmit beamform-

ing uk and the weighting factor ζk determined by zero-forcing. With given optimized fk,

uk and ζk, device selection set Mk is optimized via Gibbs sampling.

• Receive beamforming by differential geometry programming [8]: This method optimizes

each task separately, with all the devices selected. fk is optimized on the Grassmann manifold

via differential geometry programming, and uk is given by the zero-forcing.

• Difference-of-convex (DC) programming and device selection [10]: This method optimizes

each task separately. For each task, the method maximizes the number of selected devices

by a two-step framework based on DC programming, with a given threshold of the com-

munication MSE.

Here we simulate the case of 3 FL tasks on i.i.d. data. The numbers of devices for the three

tasks are set to M1 = M2 = M3 = 20. The learning rates are set to η1 = η2 = η3 = 0.05. The local

updates contain 5 mini-batches of SGD. The noise power is σ2 = −60 dBm. We set (cid:15) = 0.5.

Besides, we introduce the normalized mean square error (NMSE) of each task k at round t,
deﬁned as NMSE(t)
k (cid:107)2
k
NMSE over 20 Monte Carlo trials at t = 40 and t = 90. Beneﬁting from interference awareness,

. In Table II, we list the average

(cid:44) 10 log10

k − g(t)

E(cid:2)(cid:107)ˆg(t)

2/(cid:107)g(t)

k (cid:107)2
2

(cid:3)(cid:17)

(cid:16)

the proposed AO algorithm and the method in [16] achieve much better NMSEs on all the three

FL tasks than the other methods. We also see that our algorithm signiﬁcantly outperforms the

method in [16]. This is attributed to the careful optimization of the transceiver beamforming

based on the proposed analytical framework.

TABLE II
COMMUNICATION NMSE

NMSE (dB)

Optimization method

Fashion-MNIST (k = 2) KMNIST (k = 3)
MNIST (k = 1)
t = 40
t = 40
t = 90
t = 40
t = 90
−1.98 −1.75
−1.43 −1.37 −1.81
Proposed AO algorithm
−0.98
−1.11
SOCP-based cooperative power control [16] −0.84 −1.01 −1.10
−0.52
−0.58
−0.40 −0.47 −0.50
SCA & Gibbs [7]
−0.21
−0.22
−0.16 −0.20 −0.21
Differential geometry [8]
−0.13
−0.06
−0.17 −0.20 −0.15
DC and device selection [10]

t = 90
−1.40
−1.01
−0.52
−0.19
−0.21

25

Fig. 8. Proportion of devices versus range of allocated power with tasks 1, 2 and 3. t = 40 for (a)-(c); t = 90 for (d)-(f).

In Fig. 8, we plot the histogram of allocated transmission powers for various optimization

methods. The methods based on zero-forcing [8], [10], [16] only allocate full power to less than

20% of all the devices, due to the stragglers with the worst channel conditions. The SCA & Gibbs

algorithm in [7] excludes several stragglers through Gibbs sampling, improving the number of

full-power-allocated devices, but the percentage is still below 50%. We see that the transmission

powers of most devices are allocated fully in the proposed AO algorithm. This is because our

proposed scheme relaxes the hard requirement for all the devices to align their gradients with

the stragglers, which gives freedom to the devices to fully exploit the power budgets.

In Fig. 9, we present the test accuracies of various optimization algorithms versus commu-

nication rounds. As shown in Fig. 9, the proposed algorithm achieves an accuracy close to the

error-free bound in all the three FL tasks and signiﬁcantly outperforms the other baselines, which

clearly demonstrates the superiority of our proposed scheme.

VI. CONCLUSION

In this paper, we studied a problem of designing an OA-FMTL system over MIMO MAC

channel. We proposed a misalignment-tolerant strategy to align the local gradients at model

aggregation at the PS side to relieve the straggler problem. We further derived a communication-

learning framework to analyze the OA-FMTL performance by characterizing the performance

loss due to device selection, inter-task interference and communication noise. Based on the

analytical framework, we formulated an optimization problem with respect to device selection,

(a)MNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devices(b)Fashion-MNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devices(c)KMNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devices(d)MNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devices(e)Fashion-MNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devices(f)KMNIST[0,0.2)[0.2,0.4)[0.4,0.6)[0.6,0.8)[0.8,1]Range of allocated power (W)00.51Proportion of devicesProposed AO algorithmPower control based on SOCP [16]SCA & Gibbs [7]Differential geometry [8]DC and device selection [10]26

Fig. 9. Test accuracy versus communication rounds on (a) MNIST, (b) Fashion-MNIST, and (c) KMNIST, with K = 3,
M1 = M2 = M3 = 20, i.i.d. data.

transmit beamforming, and receive beamforming. We captured the spatial correlation between

the local gradients to enhance the optimization and proposed a low-complexity algorithm to solve

the communication-learning problem based on AO framework. Finally, we performed extensive

numerical experiments to demonstrate the learning accuracy outstanding improvements of the

proposed algorithm by comparison with the state-of-the-art methods.

APPENDIX A

PROOF OF THEOREM 1

From [24, 3.2], the device selection MSE E[(cid:107)e(t)

ds,k(cid:107)2] is bounded by (22). We next consider

the communication MSE of the k-th task given by

E

(cid:104)

(cid:107)e(t)

com,k(cid:107)2(cid:105)

=

(cid:88)D

d=1

E

(cid:20)(cid:12)
k [d] − ˆg(t)
(cid:12)g(t)
(cid:12)

k [d]

(cid:12)
(cid:12)
(cid:12)

2(cid:21)

.

By plugging (5), (8), (9) and (13) into (36), we obtain

E[(cid:107)e(t)

com,k(cid:107)2]=

1

C
(cid:88)

(cid:1)2

Q(cid:104)k,i(cid:105)

(cid:0) (cid:80)
i∈Mk

c=1

(cid:34)
(cid:12)
(cid:88)
E
(cid:12)
(cid:12)
i∈Mk

(cid:113)

Q(cid:104)k,i(cid:105)

kr(t)
v(t)
(cid:104)k,i(cid:105)[c]−

K
(cid:88)

(cid:88)

l=1

i∈Ml

ζkf H

k H(t)

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105)r(t)

(cid:104)l,i(cid:105)[c]−ζkf H

(cid:12)
k n(t)[c]
(cid:12)
(cid:12)

(36)

(cid:35)
2
.

(37)

Based on Assumption 5, for ∀k (cid:54)= l, we have E[r(cid:104)k,i(cid:105)[c]r(cid:104)l,j(cid:105)[c]†] = 0. Thus, we obtain (23) by

expanding (37). What remains is to prove (21). From [24, Lemma 2.1], Assumptions 1-4 lead

to an upper bound of Fk(·) at the t-th round. Thus, we have the following lemma.

020406080100Communication round00.20.40.60.81Test accuracy(a) MNISTError-freeProposed AO algorithmSOCP-based cooperative power control [16]SCA & Gibbs [7]Differential geometry [8]DC and device selection [10]020406080100Communication round0.10.20.30.40.50.60.70.8Test accuracy(b) Fashion-MNIST020406080100Communication round00.10.20.30.40.50.60.7Test accuracy(c) KMNISTLemma 1. Assume that Fk(·) satisﬁes Assumptions 1-4, at the t-th communication round with

the learning rate ηk is set to 1/ωk. Then

E[Fk(w(t+1)

k

)] ≤ E[Fk(w(t)

k )]−

1
2ωk

E[(cid:107)∇Fk(w(t)

k )(cid:107)2] +

1
2ωk

E[(cid:107)e(t)

k (cid:107)2],

(38)

27

where ωk is the Lipschitz continuity parameter deﬁned in (16), and E[·] is the expectation w.r.t.
{n(t)

(cid:104)k,i(cid:105)[d]|k ∈ [K], z ∈ [NR], c ∈ [C], i ∈ [Mk], d ∈ [D], τ ∈ [t + 1]}.

k,z[c], g(t)

Proof. See [24, Lemma 2.1].

In addition, we obtain

E[(cid:107)e(t)

k (cid:107)2] (a)= E[(cid:107)e(t)

ds,k + e(t)

com,k(cid:107)2]

(cid:16)

(b)
≤ 2

E[(cid:107)e(t)

ds,k(cid:107)2] + E[(cid:107)e(t)

com,k(cid:107)2]

(cid:17)

,

(39)

where step (a) is from the expression of e(t)
k

in (15), and step (b) is from the inequality of

arithmetic and geometric means. By plugging (39) into (38), we obtain (21).

APPENDIX B

PROOF OF COROLLARY 1

From (23), we obtain

E

(cid:104)
(cid:107)e(t)

com,k(cid:107)2(cid:105) (a)=

C

(cid:16)(cid:88)

(cid:0)(cid:80)

i∈Mk

(cid:1)2

Q(cid:104)k,i(cid:105)

i,j∈Mk

2ρ(t)

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,i(cid:105)Q(cid:104)k,j(cid:105)v(t)

k

(cid:16)

2ρ(t)

(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)

Q(cid:104)k,i(cid:105)(f H

k H(t)

2ρ(t)

(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

k H(t)

(cid:104)l,i(cid:105)ul,i)Hf H

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H + Q(cid:104)k,j(cid:105)f H

k H(t)
(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2(cid:17)(cid:17)

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

(cid:17)

(40a)

(cid:1)2

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,i(cid:105)Q(cid:104)k,j(cid:105)

Q(cid:104)k,i(cid:105)(f H

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H + Q(cid:104)k,j(cid:105)f H

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

(cid:16)(cid:80)K

l=1

4

(cid:80)

i,j∈Ml

ρ(t)
(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))Hf H

k H(t)

(cid:17)
(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2/2

k H(t)
k H(t)

(cid:17)(cid:17)2




 ,

(40b)

where step (a) is from E[r(cid:104)k,i(cid:105)[c]r(cid:104)k,j(cid:105)[c]†] = 2ρ(cid:104)k,i(cid:105),(cid:104)k,j(cid:105) based on Assumption 5, and step (b) is
because E[(cid:107)e(t)
com,k(cid:107)2] is a convex quadratic function w.r.t. ζk and the minimizer ζ ∗

k is given by

(24).

−ζk

+ζ 2
k

(b)
≥

(cid:0)(cid:80)

(cid:113)

(cid:88)

v(t)
k
(cid:16)(cid:88)K
l=1
2Cv(t)
k
Q(cid:104)k,i(cid:105)

i∈Mk

i,j∈Mk
(cid:88)

i,j∈Ml
(cid:16)(cid:88)

(cid:16)(cid:80)

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)

(cid:16)

−

28

APPENDIX C

PROOF OF THEOREM 2

We ﬁrst derive an upper bound w.r.t. the communication MSE in (40b). For device (cid:104)k, i(cid:105), ∀k, i,

we have

2Cv(t)

k = 2Cv(t)

(cid:104)k,i(cid:105) =
1
Q(cid:104)k,i(cid:105)

(cid:88)D

(cid:16)

E[|g(t)

(cid:104)k,i(cid:105)[d]]|2(cid:17) (a)
(cid:104)k,i(cid:105)[d]|2] − |E[g(t)
(cid:20)
(cid:21) (c)
1
≤ E
Q(cid:104)k,i(cid:105)

k ; ξ(cid:104)k,i(cid:105),n)(cid:107)2

(

∇fk(w(t)

(cid:20)
(b)= E

(cid:107)

≤ E[(cid:107)g(t)

(cid:104)k,i(cid:105)(cid:107)2]

(cid:88)Q(cid:104)k,i(cid:105)
n=1

(cid:107)∇fk(w(t)

k ; ξ(cid:104)k,i(cid:105),n)(cid:107))2

(cid:21)

(cid:113)

β1 + β2(cid:107)∇Fk(w(t)

k )(cid:107)2

(cid:17)2(cid:21)

= β1 + β2E[(cid:107)∇Fk(w(t)

k )(cid:107)2]

(41)

d=1
(cid:88)Q(cid:104)k,i(cid:105)
n=1

(cid:88)Q(cid:104)k,i(cid:105)
n=1

(cid:20)(cid:16) 1

(d)
≤ E

Q(cid:104)k,i(cid:105)

(cid:12)E[g(t)

(cid:104)k,i(cid:105)[d]](cid:12)
(cid:12)

2 ≥ 0; (b) is from the fact that E[g(t)

where step (a) is from (cid:12)
and with deﬁnition of F(cid:104)k,i(cid:105)(w(t)
from (18). Note that (40b) is obtained by the expression of E[(cid:107)e(t)
plugging (41) into (40b), we obtain an upper bound w.r.t. E[(cid:107)e(t)

(cid:104)k,i(cid:105)] = E[∇F(cid:104)k,i(cid:105)(w(t)
k )]
k ) given below (2); (c) is from the triangle inequality; and (d) is
k . By

com,k(cid:107)2] in (23) as ζk = ζ ∗
com,k(cid:107)2]:

E[(cid:107)e(t)

com,k(cid:107)2] ≤

β1 + β2E[(cid:107)∇Fk(w(t)
k )(cid:107)2]
(cid:1)2

(cid:0)(cid:80)

Q(cid:104)k,i(cid:105)

(cid:32)

(cid:88)

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)Q(cid:104)k,i(cid:105)Q(cid:104)k,j(cid:105)

i∈Mk
(cid:16)

(cid:16)(cid:80)

i,j∈Mk

ρ(t)
(cid:104)k,i(cid:105),(cid:104)k,j(cid:105)

(cid:16)(cid:80)K

l=1

4

(cid:80)

i,j∈Ml

−

Q(cid:104)k,i(cid:105)(f H

(cid:104)k,j(cid:105)u(cid:104)k,j(cid:105))H + Q(cid:104)k,j(cid:105)f H

k H(t)

(cid:104)k,i(cid:105)u(cid:104)k,i(cid:105)

ρ(t)
(cid:104)l,i(cid:105),(cid:104)l,j(cid:105)(f H

(cid:104)l,i(cid:105)u(cid:104)l,i(cid:105))Hf H

k H(t)

(cid:17)
(cid:104)l,j(cid:105)u(cid:104)l,j(cid:105) + σ2(cid:107)fk(cid:107)2/2

k H(t)
k H(t)

(cid:17)(cid:17)2




 ,

(42)

Next, we derive the upper bound of E (cid:2)F (cid:0)w(t+1)(cid:1) − F (w∗)(cid:3). We take summation on both

sides of (21) to obtain the following inequality:

E[F(w(t+1))] ≤ E[F(w(t))]−

1
2ω

(cid:88)K

k=1

(cid:16)
E[(cid:107)∇Fk(w(t)

k )(cid:107)2]−2(E[(cid:107)e(t)

ds,k(cid:107)2] + E[(cid:107)e(t)

(cid:17)
com,k(cid:107)2])

. (43)

where ω = maxkωk, and µ = minkµk. Substituting E[(cid:107)e(t)
com,k(cid:107)2] respectively with
(22) and (42), we have an upper bound of the overall loss function Fk(·) at the round (t+1) as

ds,k(cid:107)2] and E[(cid:107)e(t)

E[F(w(t+1))] ≤ E[F(w(t))]−

1
2ω

K
(cid:88)

k=1

(cid:16)
E[(cid:107)∇Fk(w(t)

k)(cid:107)2](1−2β2d(t)

k(Mk,fk,uk))−2β1d(t)

(cid:17)
k(Mk,fk,uk)

, (44)

k (Mk, fk, uk) is deﬁned by (26). Furthermore, based on Assumption 2, an upper bound

where d(t)
of E[(cid:107)∇Fk(w(t)

k )(cid:107)2] is obtained from [24, eq. (2.4)] as

E[(cid:107)∇Fk(w(t)

k )(cid:107)2] ≥ 2µE

(cid:104)
Fk(w(t)

(cid:105)
k ) − Fk (w∗
k)

.

(45)

29

Plugging (45) into (44) and subtracting F(w∗) on the both sides of (44), we obtain (25).

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.

[2] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep learning based natural language processing,”

IEEE Comput. Intell. Mag., vol. 13, no. 3, pp. 55–75, Aug. 2018.

[3] J. Koneˇcn`y, H. B. McMahan, D. Ramage, and P. Richt´arik, “Federated optimization: Distributed machine learning for

on-device intelligence,” Oct. 2016, [Online] Available: https://arxiv.org/abs/1610.02527.

[4] B. Nazer and M. Gastpar, “Computation over multiple-access channels,” IEEE Trans. Inf. Theory, vol. 53, no. 10, pp.

3498–3516, Oct. 2007.

[5] G. Zhu, Y. Du, D. G¨und¨uz, and K. Huang, “One-bit over-the-air aggregation for communication-efﬁcient federated edge

learning: Design and convergence analysis,” IEEE Trans. Wirel. Commun., vol. 20, no. 3, pp. 2120–2135, Mar. 2020.

[6] G. Zhu, Y. Wang, and K. Huang, “Broadband analog aggregation for low-latency federated edge learning,” IEEE Trans.

Wirel. Commun., vol. 19, no. 1, pp. 491–506, Jan. 2020.

[7] H. Liu, X. Yuan, and Y.-J. A. Zhang, “Reconﬁgurable intelligent surface enabled federated learning: A uniﬁed

communication-learning design approach,” IEEE Trans. Wirel. Commun., vol. 20, no. 11, pp. 7595–7609, Nov. 2021.

[8] G. Zhu, L. Chen, and K. Huang, “MIMO over-the-air computation: Beamforming optimization on the Grassmann manifold,”

in IEEE Global Communications Conference (GLOBECOM), Dec. 2018, pp. 1–6.

[9] H. Liu, X. Yuan, and Y.-J. A. Zhang, “CSIT-free model aggregation for federated edge learning via reconﬁgurable intelligent

surface,” IEEE Wireless Commun. Lett., vol. 10, no. 11, pp. 2440–2444, Nov. 2021.

[10] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air computation,” IEEE Trans. Wirel. Commun.,

vol. 19, no. 3, pp. 2022–2035, Mar. 2020.

[11] Y. Shi, Y. Zhou, and Y. Shi, “Over-the-air decentralized federated learning,” Jun. 2021, [Online] Available: https://arxiv.

org/abs/2106.08011.

[12] Y. Liu, X. Yuan, Z. Xiong, J. Kang, X. Wang, and D. Niyato, “Federated learning for 6G communications: Challenges,

methods, and future directions,” China Commun., vol. 17, no. 9, pp. 105–118, Sept. 2020.

[13] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and connections for efﬁcient neural networks,” Oct.

2015, [Online] Available: https://arxiv.org/abs/1506.02626.

[14] C.-Y. Chen, J. Ni, S. Lu, X. Cui, P.-Y. Chen, X. Sun, N. Wang, S. Venkataramani, V. Srinivasan, and W. Zhang, “Scalecom:

Scalable sparsiﬁed gradient compression for communication-efﬁcient distributed training,” Apr. 2021, [Online] Available:

https://arxiv.org/abs/2104.11125.

[15] D. Fan, X. Yuan, and Y.-J. A. Zhang, “Temporal-structure-assisted gradient aggregation for over-the-air federated edge

learning,” IEEE J. Sel. Areas Commun., vol. 39, no. 12, pp. 3757–3771, Dec. 2021.

[16] X. Cao, G. Zhu, J. Xu, and K. Huang, “Cooperative interference management for over-the-air computation networks,”

IEEE Trans. Wirel. Commun., vol. 20, no. 4, pp. 2634–2651, Apr. 2021.

[17] M. M. Amiri and D. G¨und¨uz, “Federated learning over wireless fading channels,” IEEE Trans. Wirel. Commun., vol. 19,

no. 5, pp. 3546–3557, May 2020.

[18] X. Cao, G. Zhu, J. Xu, and S. Cui, “Transmission power control for over-the-air federated averaging at network edge,”

IEEE J. Sel. Areas Commun., 2022.

30

[19] T. Sery, N. Shlezinger, K. Cohen, and Y. C. Eldar, “Over-the-air federated learning from heterogeneous data,” IEEE Trans.

Signal Process., vol. 69, pp. 3796–3811, June 2021.

[20] G. Zhu, J. Xu, K. Huang, and S. Cui, “Over-the-air computing for wireless data aggregation in massive iot,” IEEE Wirel.

Commun., vol. 28, no. 4, pp. 57–65, Sep. 2021.

[21] S. L. H. Nguyen and A. Ghrayeb, “Compressive sensing-based channel estimation for massive multiuser MIMO systems,”

in 2013 IEEE Wireless Communications and Networking Conference (WCNC).

IEEE, Oct. 2013, pp. 2890–2895.

[22] C.-K. Wen, S. Jin, K.-K. Wong, J.-C. Chen, and P. Ting, “Channel estimation for massive MIMO using Gaussian-mixture

Bayesian learning,” IEEE Trans. Wirel. Commun., vol. 14, no. 3, pp. 1356–1368, Oct. 2014.

[23] Z. Lin, X. Li, V. K. Lau, Y. Gong, and K. Huang, “Deploying federated learning in large-scale cellular networks: Spatial

convergence analysis,” Mar. 2021, [Online] Available: https://arxiv.org/abs/2103.06056.

[24] M. P. Friedlander and M. Schmidt, “Hybrid deterministic-stochastic methods for data ﬁtting,” SIAM J. Sci. Comput., vol. 34,

no. 3, pp. A1380–A1405, Jan. 2012.

[25] D. P. Bertsekas and J. N. Tsitsiklis, “Neuro-dynamic programming: An overview,” in Proc. 1995 34th IEEE conf. Decis.

and Control, vol. 1, 1995, pp. 560–564.

[26] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proc. of the

IEEE, vol. 86, no. 11, pp. 2278–2324, Nov. 1998.

[27] Y. LeCun, C. Cortes, and C. Burges, “The MNIST database of handwritten digits,” 1998, [Online] Available: http://yann.

lecun.com/exdb/mnist.

[28] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,”

Sep. 2017, [Online] Available: https://arxiv.org/abs/1708.07747.

[29] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha, “Deep learning for classical Japanese

literature,” Dec. 2018, [Online] Available: https://arxiv.org/abs/1812.01718.

[30] K. Shen and W. Yu, “Fractional programming for communication systems—Part I: Power control and beamforming,” IEEE

Trans. Signal Process., vol. 66, no. 10, pp. 2616–2630, May 2018.

[31] A. Goldsmith, Wireless Communication. Cambridge Univ. Press, Aug. 2005.

[32] Q. Wu and R. Zhang, “Intelligent reﬂecting surface enhanced wireless network via joint active and passive beamforming,”

IEEE Trans. Wirel. Commun., vol. 18, no. 11, pp. 5394–5409, Nov. 2019.

