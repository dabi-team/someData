1
2
0
2

b
e
F
9

]
L
P
.
s
c
[

2
v
3
0
2
1
1
.
7
0
0
2
:
v
i
X
r
a

Simplifying Dependent Reductions in the Polyhedral Model

CAMBRIDGE YANG, MIT CSAIL, USA
ERIC ATKINSON, MIT CSAIL, USA
MICHAEL CARBIN, MIT CSAIL, USA

A Reduction – an accumulation over a set of values, using an associative and commutative operator – is a
common computation in many numerical computations, including scientific computations, machine learning,
computer vision, and financial analytics.

Contemporary polyhedral-based compilation techniques make it possible to optimize reductions, such as
prefix sums, in which each component of the reduction’s output potentially shares computation with another
component in the reduction. Therefore an optimizing compiler can identify the computation shared between
multiple components and generate code that computes the shared computation only once.

These techniques, however, do not support reductions that – when phrased in the language of the polyhedral
model – span multiple dependent statements. In such cases, existing approaches can generate incorrect code
that violates the data dependences of the original, unoptimized program.

In this work, we identify and formalize the optimization of dependent reductions as an integer bilinear
program. We present a heuristic optimization algorithm that uses an affine sequential schedule of the program
to determine how to simplfy reductions yet still preserve the program’s dependences.

We demonstrate that the algorithm provides optimal complexity for a set of benchmark programs from
the literature on probabilistic inference algorithms, whose performance critically relies on simplifying these
reductions. The complexities for 10 of the 11 programs improve siginifcantly by factors at least of the sizes of
the input data, which are in the range of 104 to 106 for typical real application inputs. We also confirm the
significance of the improvement by showing speedups in wall-clock time that range from 1.1x to over 106x.

CCS Concepts: • Software and its engineering → Compilers; • Theory of computation → Program
analysis; Program semantics; Probabilistic computation.

Additional Key Words and Phrases: reductions, polyhedral model, program dependence

ACM Reference Format:
Cambridge Yang, Eric Atkinson, and Michael Carbin. 2021. Simplifying Dependent Reductions in the Polyhedral
Model. Proc. ACM Program. Lang. 5, POPL, Article 20 (January 2021), 32 pages. https://doi.org/10.1145/3434301

1 INTRODUCTION
A reduction – an accumulation over a set of values, using an associative and commutative operator –
is a common computation in many areas, including scientific computations, machine learning, com-
puter vision, and financial analytics. For example, consider the prefix sum defined mathematically
by Equation (1) and presented by Listing 1 in an imperative language with loops.

The value at each index 𝑖 of the array 𝐵 is the summation of values at indices 𝑗 before and up to
𝑖 of array 𝐴. The complexity of this naïve prefix sum is O (𝑁 2): O (𝑁 ) for iterating over "∀𝑖", and
O (𝑁 ) for the summation over 𝑗.

Authors’ addresses: Cambridge Yang, MIT CSAIL, USA, camyang@csail.mit.edu; Eric Atkinson, MIT CSAIL, USA, eatkinson@
csail.mit.edu; Michael Carbin, MIT CSAIL, USA, mcarbin@csail.mit.edu.

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
© 2021 Copyright held by the owner/author(s).
2475-1421/2021/1-ART20
https://doi.org/10.1145/3434301

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20

 
 
 
 
 
 
20:2

Cambridge Yang, Eric Atkinson, and Michael Carbin

𝐵 [𝑖] =

𝑗 ≤𝑖
∑︁

𝑗=0

𝐴[ 𝑗] ∀𝑖, 0 ≤ 𝑖 < 𝑁

(1)

1
2
3
4

// B is array of ints initialized to all 0
for(i = 0; i < N; i++)
for(j = 0; j<=i; j++)
B[i] += A[j]

Listing 1. Naïve prefix sum

Shared Computation and Reuse. It is possible to implement prefix sum with O (𝑁 ) complexity,
which is a linear speedup over this naïve implementation. The optimization relies on the fact that
consecutive iterations of the loop that computes 𝐵 (indexed by 𝑖) share equivalent computations:
for any pair of consecutive iterations [𝑖] and [𝑖 + 1], the values of the entire set of computations
𝑗 < 𝑖} are the same. Therefore, the latter iteration shares the former iteration’s entire
{𝐴[ 𝑗] |
computation: (cid:205)𝑗 <𝑖
𝐴[ 𝑗]. In principle, that shared computation can be reused: computed once in the
𝑗=0
former iteration, stored, and then reused in the latter iteration.

𝐵 [0] = 𝐴[0]
𝐵 [𝑖] = 𝐵 [𝑖 − 1] + 𝐴[𝑖] ∀𝑖, 1 ≤ 𝑖 < 𝑁

(2a)

(2b)

// B is array of ints initialized to all 0

1
2 B[0] = A[0]
3
4

for(i = 1; i < N; i++)

B[i] = B[i−1] + A[i]

Listing 2. Optimized prefix sum

Optimized Reductions. Equation (2) – and, correspondingly, Listing 2 – presents an O (𝑁 ) im-
plementation of prefix sum that reuses the shared computation between iterations. Instead, an
iteration calculates its result, stores its result into 𝐵 (as normal), and the subsequent iteration reuses
the result of the previous iteration (via 𝐵 [𝑖 − 1]) to compute its own result.

1.1 Simplifying Reductions
Gautam and Rajopadhye [2006] developed a suite of polyhedral compilation techniques for Simpli-
fying Reductions (SR) that can be applied to automatically transform Equation (1) to Equation (2) for
an array equational language that supports reductions as a first class operation [Yuki et al. 2013].
Several of the challenges that these techniques solve are 1) identifying shared computation, 2)
identifying if shared computation is reusable: if it’s possible to transform the program to exploit the
reuse), and 3) identifying if reusing shared computation is profitable: the transformation reduces
the complexity of a program. SR provides a suite of specifications and techniques to identify shared
computation as well as identify when shared computation is profitably reusable.1

Reuse Vector. The core of SR, the Simplification Transformation (ST), codifies the set of profitably
reusable computations as a set of reuse vectors. For Equation (1), the reuse vector [1, 0]⊤ denotes the
shared computation: changing 𝑖 to 𝑖 + 1 and 𝑗 to 𝑗 + 0 (i.e. not changing 𝑗) does not change the value
ofthe reduction body 𝐴[ 𝑗]. Given an equational statement and a reuse vector, ST automatically
transforms the statement into a set of statements that together is semantically equivalent to the
original statement, but reuses shared computation. For example, given Equation (1) and the reuse
vector [1, 0]⊤, ST transforms Equation (1) to Equation (2).

Choosing a Reuse Vector. The space of valid reuse vectors is infinite in general, e.g., any vector
[𝑐, 0]⊤ with constant 𝑐 is a valid choice for the reuse vector for Equation (1), since they all satisfy
that changing from 𝑖 to 𝑖 + 𝑐 and not changing 𝑗 does not change the evaluation of 𝐴[ 𝑗]. Moreover,

1We refer readers to Appendix B.1 for additional explanation of how SR solves these problems. All appendices can be found
in the supplementary materials section of the ACM Digital Library.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:3

different reuse vectors result in different output equations by ST and, ultimately, different programs.
As a concrete example, applying ST to Equation (1) with the reuse vector [−1, 0]⊤ produces
Equation (3).

𝐵 [𝑁 − 1] =

𝑗 <𝑁
∑︁

𝑗=0

𝐴[ 𝑗]

𝐵 [𝑖] = 𝐵 [𝑖 + 1] − 𝐴[𝑖] ∀𝑖, 0 ≤ 𝑖 < 𝑁 − 1

(3a)

(3b)

Instead of initializing 𝐵 [0] and computing 𝐵 [𝑖] from lower indices to higher indices (i.e. left to
right) as in Equation (2), Equation (3) initializes 𝐵 [𝑁 − 1] and computes 𝐵 [𝑖] from higher to lower
indices (i.e. right to left). However, this still has complexity O (𝑁 ), just as does Equation (2).

1.2 Dependent Reductions
The SR framework, including ST, only optimizes reductions that are independent. As a point of
contrast, consider the following dependent reduction:

𝐵 [𝑖] =

𝑗 ≤𝑖
∑︁

𝑗=0

𝐴[ 𝑗] ∀𝑖, 0 ≤ 𝑖 < 𝑁

(4a)

𝐴[𝑖 + 1] = 𝑓 (𝐵 [𝑖]) ∀𝑖, 0 ≤ 𝑖 < 𝑁 − 1
Equation (4) extends Equation (1) (equivalent to Equation (4a)) with an additional statement
(Equation (4b)). The reduction in Equation (4a) is dependent: the value of the reduction 𝐵 [𝑖] depends
on the set of values {𝐴[ 𝑗]| 𝑗 ≤ 𝑖}, while 𝐴[𝑖] depends on the previous value of the reduction 𝐵 [𝑖 − 1].
Dependent reductions pose a challenge to ST because 1) applying ST introduces new dependences,
and 2) the newly introduced dependences together with the program’s existing dependences may
incorrectly form a dependence cycle in the resultant program.

(4b)

For example, applying ST to Equation (4a) with the reuse vector [−1, 0]⊤ produces a program
consisting of three statements: Equations (3a), (3b) and (4b), which, together form a dependence
𝑆
cycle. Specifically, let 𝐸1
−→ 𝐸2 denote a dependence induced by statement 𝑆 between array entries
𝐸1 and 𝐸2. The path 𝐵 [𝑁 − 1]

𝑒𝑞. (3b)
−→ 𝐵 [𝑁 − 1] forms a cycle.

𝑒𝑞. (3a)
−→ 𝐴[𝑁 − 1]

𝑒𝑞. (4b)
−→ 𝐵 [𝑁 − 2]

On the other hand, applying ST to Equation (4a) with
reuse vector [1, 0]⊤ produces a valid program consisting
of three statements: Equations (2a), (2b) and (4b), and
without any dependence cycle. Listing 3 presents a trans-
lation of this program to an imperative language with
loops, which correctly computes array 𝐴 and 𝐵 with com-
plexity O (𝑁 ).

for(i=1; i < N; i++)

1 B[0] = A[0]
2
3
4

B[i] = B[i−1] + A[i]
A[i+1] = f(B[i])

Listing 3. Optimized prefix sum with depen-
dent reductions

In summary, one key challenge for optimizing the de-

pendent reductions is to augment ST to choose reuse vectors such that the augmented transforma-
tion produces programs that have no dependence cycles.

Approach. In this work, present a new technique to automatically optimize dependent reductions
while soundly handling dependences that can automatically generate the code in Listing 3. We
present a heuristic algorithm whose key idea is to use an affine sequential schedule of the program
as a guide to choose among the multiple choices that can be made during the optimization process.
Our results show that even though the algorithm does not consider other viable choices during
optimization, given an affine sequential schedule of the program and all left-hand-side arrays of
reductions, the algorithm is still optimal for reductions with operators that have inverses.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:4

Cambridge Yang, Eric Atkinson, and Michael Carbin

We note that our work relies on the techniques of Gautam and Rajopadhye [2006] to find resuable
shared computation – i.e., reuse vectors – for a single reduction.2 Our work addresses dependent
reductions by further constraining reuse vectors to satisfy (intra/inter)-statement(s) dependences.

Applications. Simplifying Reductions is a classic problem in the compiler optimization litera-
ture that has reemerged as a primary concern for modern applications. In this work, we study a
suite of 11 probabilistic inference algorithms that have been established as widely studied and used
algorithms across data science, artificial intelligence, machine learning, computer vision, physics,
and medicine. We demonstrate that dependent reductions exist in these algorithms’ natural, mathe-
matical specifications. Moreover, delivering efficient implementations of these algorithms by hand
– as is current practice – requires solving the simplifying dependent reduction problem by hand,
which is a tedious and error-prone endeavor. Our approach shows that it is possible to automatically
generate optimized and efficient algorithms from their mathematical specifications alone.

Contributions. In this work, we present the following contributions:

• We identify the problem of simplifying dependent reductions, a problem that was not addressed
by the Simplifying Reductions framework [Gautam and Rajopadhye 2006], which did not consider
dependences. We illustrate the importance of this problem with examples from real applications.
• We formalize the task of optimizing a dependent reduction by combining the insights of the
Simplifying Reductions framework with insights from ILP scheduling [Pouchet et al. 2011]. We
formulate a specification of the problem as a integer bilinear program.
• We propose a heuristic algorithm to solve the above optimization problem.
• We evaluate our proposed method on a benchmark suite consisting of standard probabilistic
inference algorithms and probabilistic models. Our results show that our approach reduces
the complexity of the reductions in our programs to their optimal complexity for all of the
11 programs that we evaluate. In 10 out of the 11 programs, the complexity improves by a
(multiplicative) factor of at least 𝑁 , where 𝑁 is the size of the input data.3 This is significant
because for typical real application inputs of the programs in consideration, 𝑁 is in the range
of 104 to 106 – a factor that subsumes other potential constant-factor improvements. We also
confirm this significance by showing that the speedups in wall-clock time ranges from 1.1x to
over 106x, with a median of 37x. We also outline the limits of the optimality of our approach,
noting that our technique is not optimal if a reduction operator lacks an inverse operation.
In summary, dependent reductions are a key ingredient of probabilistic inference algorithms,
which are driving an emerging class of new programming languages and systems [Bingham
et al. 2019; Cusumano-Towner et al. 2019; Daniel Huang 2017; Gelman et al. 2015; Goodman and
Stuhlmüller 2014; Mansingkha et al. 2018; Narayanan et al. 2016; Tran et al. 2017] designed to
streamline data science and enable new applications. Optimizing these algorithms has historically
either been done by hand or has been baked in as a domain/algorithmic-specific optimization for a
single problem model [Holmes et al. 2012; Liu 1994]. To the best of our knowledge, our results are
the first to identify and formulate the dependent reduction as a general program pattern, detail its
challenges, and propose a technique to optimize its performance.

Road Map. In Section 2, we illustrate a heuristic algorithm to address the simplifying dependent
reduction problem described in Section 1. In addition, to further motivate the problem in the context
of existing well-known algorithms, we present another motivating example which will be used
for evaluation later in the paper. In Sections 3 and 4, we review background on the polyhedral

2We include a description of the techniques in Appendix B
3For programs we consider, for example, this is usually the number of data points or the number of words of a text corpus.
We include a more detailed review of input sizes for each benchmark in Section 8.3.2

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:5

model and SR, respectively. In Section 5 we formalize our problem as a integer bilinear program. In
Section 6 we introduce our heuristic algorithm. In Sections 7 and 8 we discuss the implementation
of our algorithm and its evaluation. In Sections 9 and 10 we summarize related work and end with
concluding remarks, respectively.

2 EXAMPLE
In this section we give two examples. In Section 2.1, we walk through our approach with the
prefix sum example. In Section 2.2, we use a practical application example to further motivate the
importance of the dependent reduction problem.

2.1 Walk Through
In this section, we use the example of Equation (4) to 1)
illustrate the steps of applying ST given a reuse direc-
tion, 2) illustrate an invalid reuse direction that leads ST
(using the algorithm of Gautam and Rajopadhye [2006])
to induce a dependence cycle, and compare it to a valid
reuse direction, and 3) describe the mechanism of our
proposed heuristic algorithm.

Naive Prefix Sum. For ease of comparison and better
visualization, we present the input in Equation (4) with
Figure 1, a visual, polyhedral interpretation of the naive
prefix sum program in Equation (4). In Figure 1, the top
polyhedron with round dots represents the iteration do-
main of the reduction statement, 𝐵 [𝑖] += 𝐴[ 𝑗], with each
round dot denoting an iteration instance of the statement. To the right of the top polyhedron,
we have labeled each round dot in the top polyhedron at coordinate (𝑖, 𝑗) by the array element
𝐴[ 𝑗] that should be accumulated into 𝐵 [𝑖]. The bottom polyhedron with squares represents the
iteration domain for the statement 𝐴[𝑖 + 1] = 𝑓 (𝐵 [𝑖]). The middle polyhedron with diamonds is an
additional polyhedron that our technique inserts into the program’s polyhedral representation to
denote the completion of each reduction 𝐵 [𝑖], which we label as (cid:205) A[j] in the diagram.

Fig. 1. Naive prefix sum (Equation (4))

Data Dependences. Each arrow in Figure 1 represents a data dependence between iteration
instances. An arrow from iteration instance 𝑎 to instance 𝑏 represents a data dependence from 𝑎 to
𝑏. The implication is that 𝑎 needs to execute before 𝑏.

There are three sources of data dependences:

• Reduction. Each point in the middle polyhedron depends on all the points in the respective

column of the top polyhedron. These dependences are those of the reduction.

• Use. Each point in the bottom polyhedron depends on the point in the corresponding column of
the middle polyhedron. These dependences are those from the use of the reduction results.
• Update. Points in each row of the top polyhedron depend on the point in the bottom polyhedron
that is one to the left of the leftmost point of the row. These dependences are those induced by
the update to 𝐴[𝑖 + 1] in Equation (4b) and use by Equation (4a).

Correct Optimization. Figure 2 presents two diagrams, corresponding to an intermediate step

and the result of a correct application of ST, respectively.

Figure 2a presents the intermediate step of ST. In this step, the algorithm chooses a reuse vector
and shifts the reduction statement’s iteration domain along the vector. Figure 2a illustrates the
shift along a correct reuse vector, [1, 0]⊤, which maps iteration instances [𝑖, 𝑗] (round dots in the

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:6

Cambridge Yang, Eric Atkinson, and Michael Carbin

(a) After shift
Fig. 2. Correct optimization of prefix sum with depedent reduction

(b) After transformation

crosshatched polyhedron) to instances [𝑖 + 1, 𝑗] (triangles in the dot-shaded polyhedron). Each
solid arrow represents the mapping from an instance in the polyhedron with round dots to its
counterpart in the polyhedron with triangles. The triangles outlined in round dots (i.e. overlapped
triangles and round dots) are the points corresponding to redundant computations. Specifically,
because the reuse vector has the property that the evaluations of the reduction’s body, 𝐴[ 𝑗], are the
same for any two points in the same row, the evaluation of a reduction over any column col in this
intersection must have the same value as the evaluation of a reduction over the column to the left
of col. ST therefore eliminates this intersection part of the domain by reusing previously computed
reductions (i.e. computes 𝐵 [𝑖] from 𝐵 [𝑖 − 1] by incrementally using points not in the intersection).
Figure 2b presents the result of ST, where ST has eliminated the redundant computations.
Figure 2b corresponds to the resulting polyhedron and dependences after ST eliminates redundant
computations. Specifically, each instance in the intersection of the two polyhedrons has been
eliminated, along with its induced dependences.

To map Figure 2b to Equation (2): the point 𝑎 in the middle polyhedron and the point 𝑏 in the
top polyhedron in Figure 2b correspond to the reduction that initializes 𝐵 [0] in Equation (2a).
All points in the middle polyhedron except 𝑎 then correspond to Equation (2b), i.e., each 𝐵 [𝑖] is
computed by adding the predecessor point 𝐵 [𝑖 − 1] with 𝐴[𝑖].

Dependences in Figure 2b are preserved from Figure 2a for all the non-eliminated instances.

Figure 2b introduces the following new dependences:

• Reuse. Each instance in the middle polyhedron (except the leftmost instance) now depends on

the instance to its left, along the reuse vector. These dependences are those from reusing.

• Increment. Each instance in the middle polyhedron now depends on the corresponding instance
in the top polyhedron in the same column. These dependences are those from incrementalizing.

Incorrect Optimization. The two diagrams in Figure 3 illustrate an incorrect application of ST
using Gautam and Rajopadhye [2006], which ignores the dependences due to dependent reduction.
In this case, instead of using the correct reuse vector [1, 0]⊤, this application of ST uses the vector
[−1, 0]⊤. This vector maps iteration instances [𝑖, 𝑗] to instances [𝑖 − 1, 𝑗].

Figure 3a presents the intermediate step of ST. Same as in the correct optimization’s case, in
this step, the algorithm chooses the a reuse vector and shifts the reduction’s domain along the
vector. Figure 3a illustrates the shift along the incorrect reuse vector, [−1, 0]⊤, which maps iteration
instances [𝑖, 𝑗] (round dots in the crosshatched polyhedron) to instances [𝑖 − 1, 𝑗] (triangles in the

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:7

(a) After shift
Fig. 3. Incorrect optimization of prefix sum with depedent reduction

(b) After transformation

dot-shaded polyhedron). Each solid arrow again represents the maping between the corresponding
instances before and after the shift. The triangles outlined in round dots again are the instances
corresponding to redundant computations to be eliminated by ST.

Figure 3b presents the result of ST, where ST has, again, eliminated the redundant computations.
The top polyhedron now has the round dots at the rightmost column and the triangles along the
hypotenuse of the shifted domain. Note that the hypotenuse is restricted to the domain of projected
domain of the reduction and does not include the point [𝑖, 𝑗] = [−1, 0].

To map the Figure 3b to Equation (3): the point 𝑓 and the round dot column in the top polyhedron
in Figure 3b correspond to the reduction that initializes 𝐵 [𝑁 − 1] in Equation (3a). All points in
the middle polyhedron except 𝑑 then correspond to Equation (3b), i.e., each 𝐵 [𝑖] is computed by
subtracting the successor point 𝐵 [𝑖 + 1] by 𝐴[𝑖].

However, as mentioned in Section 1, Figure 3b’s dependences form cycles (e.g., points 𝑐, 𝑑, 𝑒, 𝑓
form a cycle). Therefore, the transformed program does not have a valid schedule, and consequently
the ST application along the reuse vector with mapping [𝑖, 𝑗] → [𝑖−1, 𝑗] is an incorrect optimization.

Heuristic for Choosing a Valid Direction. As we have seen from the previous illustration, it is
important to choose a valid reuse vector for dependent reductions. In this work, we propose a
heuristic algorithm for choosing a valid reuse vector. Notably, one key difference between Figures 2
and 3 is the dependences drawn on the middle polyhedron. Specifically, in the middle polyhedron
of Figure 2, the drawn dependences on B[𝑖] respect a sequential, scheduled computation order of
B[𝑖] of the original program in Figure 1, whereas that of Figure 3 disobeys that scheduled order.
This observation has inspired the heuristic algorithm, which always chooses the reuse vector that
is consistent with a sequential scheduled computation order of the left hand side of the reduction.
We show that the reuse vector chosen with this algorithm is 1) always sound, and 2) guarantees
optimality if each reduction operator in the target program has an inverse.

2.2 Simplying Dependent Reductions in Practice
As we later show in Section 8 by studying a variety of benchmarks, dependent reductions appear
in the specifications of many problems and algorithms across statistics, artificial intelligence (AI),
and machine learning (ML) with applications to computer vision, physics, and medicine. However,
the common practice is to develop these algorithms by hand. Therefore, our technique offers the
opportunity to automatically translate a specification to an efficient implementation. In this section,
we illustrate our technique on a clustering application used across statistics, AI, and ML.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:8

Cambridge Yang, Eric Atkinson, and Michael Carbin

Specification and Implementation. Consider the following specification of Gibbs Sampling [Geman
and Geman 1984] on a two-cluster Gaussian Mixture Model [see for example, Murphy 2012] (GS-
2GMM). This computation is designed to cluster data points such that similar data points, also
called observations, are assigned to the same cluster. The input to GS-2GMM is a float array
Obs that represents the observations. The two-cluster Gaussian Mixture Model (GMM) assumes
that each single observation belongs to one of the two clusters, and that each cluster follows a
Gaussian distribution. The Gibbs sampling procedure samples the array Z that represents the cluster
membership of all given observations. It does this by iteratively taking in an old cluster assignment
for all observations, and resampling a new assignment by updating the individual assignment of
a single observation. This process will produce a stream of samples of Zs that approach the true
distribution of Z. The mathematical specification of GS-2GMM is given in Equation (5).

𝐶𝑧𝑖 =

𝑆𝑧𝑖 =

∑︁

1 , ∀𝑧, 𝑖

∀𝑗 s.t. 𝑗≠𝑖∧𝑍 𝑗 =𝑧
∑︁

∀𝑗 s.t. 𝑗≠𝑖∧𝑍 𝑗 =𝑧

obs𝑖

, ∀𝑧, 𝑖

𝑃𝑜 (𝑧, 𝑖) = N (cid:0)

𝑆𝑧𝑖
𝐶𝑧𝑖

, (1 + 𝐶𝑧𝑖 )−1 + 1(cid:1) def.

= 𝑃 (obs𝑖 |obs\𝑖, 𝑍\𝑖, 𝑍𝑖 = 𝑧)

𝑃𝑧 (𝑖) =

𝑃𝑜 (0, 𝑖)
𝑃𝑜 (0, 𝑖) + 𝑃𝑜 (1, 𝑖)

def.

= 𝑃 (𝑍𝑖 = 0|𝑍\𝑖, obs)

𝑍𝑖 ∼ 𝑃𝑧 (𝑖)

, ∀𝑖 ∈ {1 . . . 𝑁 }

(5a)

(5b)

(5c)

(5d)

(5e)

In Equations (5a) and (5b), 𝐶0𝑖 and 𝑆0𝑖 represent the counts and sums, respectively, of all the
observations except the one with index 𝑖, for which the current old assignment of cluster mem-
bership is 0 (and similarly for 𝐶1𝑖, 𝑆1𝑖 , with membership of 1). Equation (5c) defines the function
𝑃𝑜 as an abbreviation of a normal distribution that represents the distribution of obs𝑖 given all
values of obs except obs𝑖 and all current assignments of 𝑍 s except fixing 𝑍𝑖 to be 𝑧. We use the
notation \𝑖 to denote the set { 𝑗 | 1 ≤ 𝑗 ≤ 𝑁 ∧ 𝑗 ≠ 𝑖}. Equation (5d) defines the function 𝑃𝑧 as
an abbreviation of a probability representing the chance tha 𝑍𝑖 is equal to 0 given all values of
obs and all current assignments 𝑍 except 𝑍𝑖 . Lastly, Equation (5e) samples each 𝑍𝑖 in order from
its distribution. Note that the exact computations required to perform Gibbs sampling are not
important for understanding the optimization problem.

Listing 4 gives an efficient implementation of the above mathematical specification – notice
that Listing 4 computes the counts and sums incrementally, instead of forming the full reductions
of Equations (5a) and (5b). Deriving Listing 4 from Equation (5) requires manually solving the
simplifying dependent reductions problem which is tedious and error-prone.

Our Approach. Given an array-based representation of Equation (5), our approach automatically
produces Listing 4. For conciseness of presentation, we consider the variable 𝑆𝑧𝑖 with fixed 𝑧 = 0 as
an example. In this case Equation (5b) can be rewritten as sum of two variables 𝑆0𝑖 = S0L[𝑖] +S0R[𝑖],
where S0L, S0R are given by Equations (6a) and (6b), respectively.

𝑗 <𝑖
∑︁

S0L[𝑖] =

(if Z[ 𝑗] == 0 then Obs[ 𝑗] else 0)

S0R[𝑖] =

𝑗=0
𝑗 <𝑁
∑︁

𝑗=𝑖+1

(if Z[ 𝑗] == 0 then Obs[ 𝑗] else 0)

... Other equations...
Z’[𝑖] = sample(S0L[𝑖] + S0R[𝑖], ...)

(6a)

(6b)

(6c)

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:9

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

int[N] C0L, C1L, C0R, C1R = {0...} // Zero initialize
float[N] S0L, S1L, S0R, S1R = {0...} // Zero initialize
for(i = 1; i < N; i++)

C0R[0] += (Z[i] == 0 ? 1 : 0)
C1R[0] += (Z[i] == 1 ? 1 : 0)
S0R[0] += (Z[i] == 0 ? Obs[i] : 0)
S1R[0] += (Z[i] == 1 ? Obs[i] : 0)

for(i = 0; i < N; i++)

// Sample according to Equations (5c) to (5e)
Z'[i] = sample(C0L[i] + C0R[i], C1L[i] + C1R[i], S0L[i] + S0R[i], S1L[i] + S1R[i])
// Incremental updates
C0L[i] = C0L[i−1] + (Z'[i] == 0 ? 1 : 0)
C1L[i] = C1L[i−1] + (Z'[i] == 1 ? 1 : 0)
S0L[i] = S0L[i−1] + (Z'[i] == 0 ? 1 : 0)
S1L[i] = S1L[i−1] + (Z'[i] == 1 ? 1 : 0)
C0R[i] = C0R[i−1] − (Z[i] == 0 ? 1 : 0)
C1R[i] = C1R[i−1] − (Z[i] == 1 ? 1 : 0)
S0R[i] = S0R[i−1] − (Z[i] == 0 ? Obs[i] : 0)
S1R[i] = S1R[i−1] − (Z[i] == 1 ? Obs[i] : 0)

Listing 4. Correct optimized GS-2GMM with dependent reduction; (a ? b : c) denotes if-then-else
expression as in the C language

The step of rewriting in terms of S0L and S0R is standard in polyhedral model compilation: the
original domain with constraint 𝑗 ≠ 𝑖 is non-convex and it is standard to break it into two convex
polyhedrons with constraints 𝑗 < 𝑖 and 𝑗 > 𝑖. Further, we make the non-affine constraint 𝑍 𝑗 = 𝑧
into a simple if-then-else expression guarding the reduction’s body – this is standard approach and
same as the one proposed by Benabderrahmane et al. [2010] to model non-affine constraints as
control predicates.

Equations (6a) and (6c) exactly correspond to Equations (4a) and (4b), respectively, since they
have the same data flow dependences 4. Thus the technique walked through in Section 2.1 also
applies to Equations (6a) and (6c) to produce a specification with efficient complexity. Further,
our technique is general in that it handles any dependent reduction, including Equation (6b) with
constraints 𝑖 + 1 ≤ 𝑗 < 𝑁 , which are the reverse of the constraints in Equation (6a) . Lastly, the
same analysis can be applied to all cases of 𝐶𝑧𝑖 and 𝑆𝑧𝑖 with 𝑧 = 0 or 𝑧 = 1. The analyses in
total produces eight intermedieate variables, namely C0L, C1L, C0R, C1R, S0L, S1L, S0R, S1R, which
produce Listing 4 by applying our technique and compiling to exectuable code.

Results. Our evaluation in Section 8 shows that our technique produces an optimal complexity
algorithm for Gibbs Sampling on the Gaussian Mixture Model, matching that of a manually devel-
oped implementation, and yielding a 7.1x performance improvement over a naive, unoptimized
implementation. These results demonstrate the opportunity to automatically compile high-level
specifications that include dependent reductions to efficient implementations.

3 BACKGROUND: POLYHEDRAL MODEL
In this section, we review the terminology from the polyhedral model that we use in this work. The
polyhedral model represents a program by a set of statements, and for each statement, an associated

4Although Equation (6c) contains sample that is stochastic and Equation (4b) contains f that is deterministic, they still have
the same data flow dependences.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:10

Cambridge Yang, Eric Atkinson, and Michael Carbin

S1 : BTmp[𝑖] += A[ 𝑗] : {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1Fin : B[𝑖] = BTmp[𝑖] : {[𝑖] : 0 ≤ 𝑖 < 𝑁 }
S2 : A[𝑖 + 1] = 𝑓 (B[𝑖]) : {[𝑖] : 0 ≤ 𝑖 < 𝑁 − 1}

Listing 5. Equation (4) in the polyhedral IR

polyhedral set known as the statement’s domain. Each point in a polyhedral set corresponds to one
concrete execution instance of the statement.

Listing 5 gives an example of Equation (4) our polyhedral intermediate representation. Each line
is a polyhedral statement to which we have affixed a label to the left: S1, S1Fin, and S2, respectively.
The polyhedral statement S1 corresponds to Equation (4a), and S2 corresponds to Equation (4b). To
aid understanding, we have added a statement, S1Fin, that does not map directly to an equation in
Equation (4). The statement denotes the completion of the computation (the reduction) for each
element of B and corresponds to the middle polyhedra in Figures 1 to 3.

3.1 Polyhedral Representations

Polyhedral Set. The notation that follows the second colon of each line of Listing 5 denotes the
polyhedral set that defines the statement’s domain. The statement executes once for each point in
the set. We introduce the following definition and notation for a polyhedral set; the notation is
consistent with the Integer Set Library (ISL) [Verdoolaege 2010]’s notation.

Definition 3.1 (Polyhedral set). A polyhedral set P is defined as [ (cid:174)𝑝] → {[ (cid:174)𝑥] : 𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ ≥ (cid:174)0},
which consists of a vector of parameters [ (cid:174)𝑝] , a vector template [ (cid:174)𝑥], and a system of affine inequalities
𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ ≥ (cid:174)0, where 𝑀 is an 𝑚 × (| (cid:174)𝑥 | + | (cid:174)𝑝 | + 1) matrix of constant integers. In addition,
[ (cid:174)𝑝] → {[ (cid:174)𝑥]} is the called the space of P.

A polyhedral set provides an intensional description of a set of tuples, templated by [ (cid:174)𝑥], so
that all tuples in the set satisfy the system of affine inequalities. The set is optionally parametric
in [ (cid:174)𝑝], if [ (cid:174)𝑝] is not empty. For example, the polyhedral set for statement S1Fin in Listing 5 is

[𝑁 ] → {[𝑖] :

(cid:20) 1
0
0
−1 1 −1

(cid:21)

·

𝑖
𝑁
1















≥ (cid:174)0} that denotes the set of integer values of 𝑖 from 0 to 𝑁 − 1.

Specifically, each row of 𝑀 denotes an inequality. Therefore, the inequalities in this example are
(𝑖 ≥ 0) ∧ (𝑖 ≤ 𝑁 − 1) — or simply the shorthand 0 ≤ 𝑖 < 𝑁 . An equality 𝑖 = 0 is shorthand for the
conjunction of two inequalities (𝑖 ≥ 0) ∧ (−𝑖 ≥ 0).

Polyhedral Relation. To give semantics to our polyhedral
intermediate representation (e.g. Listing 5), we also introduce
the following definition for a polyhedral relation, also in ac-
cordance with ISL’s notation.

Definition 3.2 (Polyhedral relation). A polyhedral relation
[ (cid:174)𝑝] → {[ (cid:174)𝑥1] → [ (cid:174)𝑥2] : 𝑀 · [ (cid:174)𝑥1, (cid:174)𝑥2, (cid:174)𝑝, 1]⊤ ≥ (cid:174)0} contains a
vector of parameters [ (cid:174)𝑝], vector templates [ (cid:174)𝑥1], [ (cid:174)𝑥2] , and a
system of affine inequalities 𝑀 · [ (cid:174)𝑥1, (cid:174)𝑥2, (cid:174)𝑝, 1]⊤ ≥ (cid:174)0.

Fig. 4. Plot of example the polyhedral
relation [𝑁 ] → {[𝑖, 𝑗] → [𝑖 + 1, 𝑗] :
0 ≤ 𝑖 < 𝑁 , 0 ≤ 𝑗 < 𝑁 }

A polyhedral relation describes a set of binary relations mapping from [ (cid:174)𝑥1] to [ (cid:174)𝑥2], for every
[ (cid:174)𝑥1]-[ (cid:174)𝑥2] pair that satisfies the system of affine inequalities; a polyhedral relation can also be
parametric in [ (cid:174)𝑝]. For example, [𝑁 ] → {[𝑖, 𝑗] → [𝑖 + 1, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 < 𝑁 } denotes the
relation that maps every integer tuple [𝑖, 𝑗] to [𝑖 + 1, 𝑗] within an 𝑁 -by-𝑁 grid. Figure 4 visualizes
this relation for 𝑁 = 5: the arrows map points corresponding to integer tuples to their right
successors.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:11

For aesthetic reasons, we omit the parameter [ (cid:174)𝑝] when it is clear which identifiers are parameters.

3.2 Polyhedral Representation of a Program

Syntax. Following the formalization by the original SR work
[Gautam and Rajopadhye 2006; Yuki et al. 2013], we use an equation-
based representation of program in this work, presented in gram-
mar by Figure 5. We explain each component in turn:

𝑃 := 𝑆+
𝑆 := 𝑋 (= | ⊕=) 𝐸 : P
𝐸 := 𝐸? ⊕ 𝐸 | 𝑋 | 𝑐
𝑋 := 𝑎 [ (𝐴,)∗ 𝐴 ]
Fig. 5. IR Grammar

• 𝑃 is a program that consists of multiple statements.
• 𝑆 is a statement that consists of a left hand side array access 𝑋 , a middle assignment operator (i.e.
either = or ⊕= ) , a right hand side expression (i.e. 𝐸), and a domain (i.e. P). A statement is called
a normal statement when the middle assignment operator is plain =; it is called a reduction when
the middle assignment operator is ⊕=. The reduction operator ⊕ is associative and commutative,
and has an identity (e.g. 0 is the identity addition, and 1 is the identify for multiplication).

• 𝐸 is an expression that is either an unary or binary operator applied on expression(s), an array

access (i.e. 𝑋 ), or a constant.

• 𝐴 is an affine expression, a kind of expression that applies an affine transformation to variables
and produces a scalar. It references only variables in (cid:174)𝑥 or (cid:174)𝑝, where [ (cid:174)𝑝] → {[ (cid:174)𝑥]} is the space of P.
• 𝑋 is an affine array access that consists of an array identifier (i.e. 𝑎), and a (comma seperated) list of
affine expressions (i.e. 𝐴s). A list of affine expressions of length 𝑛 can be expressed mathematically
as an affine transformation 𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤, where 𝑀 is a constant 𝑛 × (| (cid:174)𝑥 | + | (cid:174)𝑝 | + 1) integer matrix
and (cid:174)𝑥, (cid:174)𝑝 are defined same as those for an affine expression 𝐴.

• P is a polyhedral set representing the statement’s domain. Each point in the domain corresponds
to one concrete execution instance of the statement. If P is [𝑝] → {[𝑡] : 𝑒}, then 𝑝 corresponds
to the set of parameters of the program and 𝑡 corresponds to the loop variables of the statement.

Semantics. We use usual semantics from array languages [Yuki et al. 2013] for our IR. Specifically,
a statement is evaluated under each point of its domain P. An expression is evaluated under a point
by substituting the free variables of the expression with the instantiated values of those variables
under that point. For example, an expression a[N - i + j + 1] with domain [𝑁 ] → {[𝑖, 𝑗] : 0 ≤ 𝑖 <
𝑁 ∧ 0 ≤ 𝑗 < 𝑁 }, evaluates to the value of a[9] at the point [𝑖, 𝑗]⊤ = [1, 0]⊤ and given that 𝑁 = 10.
If the statement is a normal assignment, for each point in P, the right hand side expression is

evaluated and assigned to the left hand side array.

If the statement is a reduction, we first define the projection of a reduction to be a polyhedral
relation that maps points in P to their accessed indices of the left hand side array. For example, for
the reduction S1 in Listing 5, the projection is the polyhedral relation [𝑁 ] → {[𝑖, 𝑗] → [𝑖] : 0 ≤
𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}. Then, for each point 𝑝 ∈ P the right hand side expression is evaluated and
accumulated into the left hand side array at point 𝑝 ′ = 𝑝𝑟𝑜 𝑗 (𝑝) using the operator ⊕, where proj is
the projection of the reduction.

Computability. In general, programs in the equation-based representation (Figure 5) may not be
computable due to cyclic dependences. Saouter and Quinton [1993] showed that detecting cycles
for these programs is undecidable.

In this work, we focus on programs in the equational-based representation (Figure 5) that do not

have cyclic dependences, admit legal schedules, and are always computable.

3.3 Polyhedral Model Scheduling
Scheduling is a step in the polyhedral model where a scheduling function assigns each point
in a statement’s domain a timestamp, denoting the order of all execution instances. The task of

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:12

Cambridge Yang, Eric Atkinson, and Michael Carbin

scheduling a program in the polyhedral model is to find a schedule Θ for the program such that the
schedule timestamps for all statements satisfy the dependence relations of the program.

3.3.1 Dependence Analysis. In scheduling, dependence analysis compute the dependence relations
of the program. Here we recall concepts in dependence analysis and breifly describe how to compute
the dependence relation bewteen two statements in the program.

Access Relation. An access relation is a polyhedral relation mapping from the space of the domain
of a statement to the space of an accessed array. An access relation can either be a write access
relation (when the access is a write to an array on the left hand side of a statement), or a read
access relation (when the access is a read to an array on the right hand side of a statement). Let
array[𝐴1, . . . , 𝐴𝑘 ] be an array reference in a statement with space [ (cid:174)𝑝] → {[ (cid:174)𝑥]}, and the list of
affine expressions 𝐴1, . . . , 𝐴𝑘 is expressed as 𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤. The access relation for this array access
is [𝑝] → {[ (cid:174)𝑥] → [ (cid:174)𝑦] : 𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ = (cid:174)𝑦}.

For example, in Listing 5, since each iteration instance [𝑖, 𝑗] of S1 writes to BTmp[𝑖], the write
access relation of S1 is [𝑁 ] → {[𝑖, 𝑗] → [𝑖] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 < 𝑖}. Also, since each iteration
instance [𝑖, 𝑗] of S1 reads A[ 𝑗], the read access relation of S1 is [𝑁 ] → {[𝑖, 𝑗] → [ 𝑗] : 0 ≤ 𝑖 <
𝑁 ∧ 0 ≤ 𝑗 < 𝑖}.

Array SSA. Following Gautam and Rajopadhye [2006], our IR requires the program to be in array
static-single-assignment (Array SSA) form [Feautrier 1988]; that is, each array element is never
written twice during program execution. This means for each unique left hand side array, and the
statements S0...S𝑘 that write to it, (cid:209)𝑖 WS𝑖 = ∅, where WS𝑖 is the write access relation for S𝑖 .

Dependence Relation. Any two statements 𝑆1, 𝑆2 must satisfy a dependence relation represented
by a polyhedral relation D𝑆1,𝑆2 = [ (cid:174)𝑝] → {[ (cid:174)𝑥𝑆1] → [ (cid:174)𝑥𝑆2] : 𝑀 D𝑆1,𝑆2 · (cid:2) (cid:174)𝑥𝑆1, (cid:174)𝑥𝑆2, (cid:174)𝑝, 1(cid:3) ⊤ ≥ (cid:174)0}, where
𝑀 D𝑆1,𝑆2 is the dependence matrix. The dependence relation D𝑆1,𝑆2
describes the happens before
relation between iterations of 𝑆1 and 𝑆2. For a pair of statements 𝑆1, 𝑆2, let 𝑆1 write to an array 𝑎 and
is equal to R−1 ◦ W, where R and W are the
let 𝑆2 read from 𝑎. The dependence relation D𝑆1,𝑆2
two statements’ read and write access relations w.r.t. 𝑎, respectively. R−1 denotes the inverse of the
polyhedral relation R and ◦ denotes composition. Previous work [Collard et al. 1995; Verdoolaege
et al. 2013] and a textbook [Verdoolaege 2016] contain detailed introductions to dependence analysis
techniques, which we refer the reader to for deeper exposure.

3.3.2

Scheduling Function. Here we recall the definition of a program’s schedule.

Definition 3.3 (Schedule Timestamp). A schedule timestamp is an 𝑚-dimensional vector, where 𝑚
is the upper bound on the dimension of the schedule. For two timestamps 𝜏1 and 𝜏2, 𝜏1 < 𝜏2 (𝜏1
happens before 𝜏2) iff 𝜏1 [𝑖] < 𝜏2 [𝑖] where 𝑖 is the first non-equal index between 𝜏1, 𝜏2.

A schedule Θ for a program is a collection of scheduling functions, one for each statement. A
scheduling function for a statement 𝑆 is an affine transformation represented by the matrix Θ𝑆 ,
which maps statement 𝑆’s domain to its scheduling timestamp. For a statement 𝑆 with domain in
space [ (cid:174)𝑝] → {[ (cid:174)𝑥𝑆 ]}, the schedule function is an 𝑚 × (| (cid:174)𝑥𝑆 | + | (cid:174)𝑝 | + 1) matrix. The statement’s 𝑚
dimensional timestamp 𝜏𝑆 is given by:

𝜏𝑆 = Θ𝑆 ·

(cid:174)𝑥𝑆


(cid:174)𝑝


1










=









Θ1,1
...
Θ𝑚,1

... Θ

1, | (cid:174)𝑥𝑆 |+ | (cid:174)𝑝 |+1

...
...
... Θ𝑚, | (cid:174)𝑥𝑆 |+ | (cid:174)𝑝 |+1

·

(cid:174)𝑥𝑆


(cid:174)𝑝


1










(7)









ILP Formulation Of Scheduling. Early works [Feautrier 1992a,b] gave a greedy algorithm
3.3.3
to the scheduling problem and provided the foundation of the scheduling problem. Pouchet et al.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:13

[2011] formalized the scheduling problem for obtaining an 𝑚-dimensional schedule as the following
single convex problem:

∀D𝑆1,𝑆2

,∀𝑘 ∈ {1...𝑚}, 𝛿 D𝑆1,𝑆2

𝑘

∈ {0, 1}

𝑚
∑︁

∀D𝑆1,𝑆2

,

𝛿 D𝑆1,𝑆2
𝑘

= 1

𝑘=1
,∀𝑘 ∈ {1...𝑚}, ∀[ (cid:174)𝑥𝑆1, (cid:174)𝑥𝑆2, (cid:174)𝑝] ∈ D𝑆1,𝑆2

∀D𝑆1,𝑆2

Θ𝑆2
𝑘 ·

(cid:174)𝑥𝑆2




(cid:174)𝑝




1




denotes the 𝑘–th row of the matrix Θ𝑆 .
In words, Equation (8a) creates a binary variable 𝛿 D𝑆1,𝑆2

(cid:174)𝑥𝑆1


(cid:174)𝑝


1



≥ 𝛿 D𝑆1,𝑆2
𝑘

− Θ𝑆1
𝑘 ·








where Θ𝑆
𝑘

−

(8a)

(8b)

(8c)

𝑘−1
∑︁

𝑖=1

𝛿 D𝑆1,𝑆2
𝑖

(𝐾 (cid:174)𝑝 + 𝐾)

𝑘

for each dependence relation in the
program and for each of the 𝑘 ∈ {1...𝑚} dimensions of the schedule.5 Each variable models
the comparison between the two 𝑚-dimensional timestamps of statements 𝑆1 and 𝑆2 at the 𝑘-th
dimension. Equation (8b) specifies that 𝛿 D𝑆1,𝑆2
is only strongly satisfied once for all 𝑘. Equation (8c)
encodes the constraint that the schedule functions Θ𝑆1 and Θ𝑆2 must satisfy that (cid:174)𝑥𝑆1 is scheduled
before (cid:174)𝑥𝑆2 , if the dependence (cid:174)𝑥𝑆1 → (cid:174)𝑥𝑆2 exists. Specifically, the dependence (cid:174)𝑥𝑆1 → (cid:174)𝑥𝑆2 exists if
[ (cid:174)𝑥𝑆1, (cid:174)𝑥𝑆2, (cid:174)𝑝]⊤ ∈ D𝑆1,𝑆2
. The variable 𝐾 is a known constant obtainable from the original program,
and is an upper bound modeling technique to make the problem convex.

𝑘

Pouchet et al. [2011] show that this problem is equivalent to an ILP thanks to Farkas’ Lemma
given in Defini-
is [ (cid:174)𝑝] → {[ (cid:174)𝑥𝑆1 ] → [ (cid:174)𝑥𝑆2] : 𝑀 D𝑆1,𝑆2 · [ (cid:174)𝑥𝑆1, (cid:174)𝑥𝑆2, (cid:174)𝑝, 1]⊤ ≥ (cid:174)0}). Farkas’ Lemma
, of length 𝑛, where 𝑛 is one
and 𝑘 ∈ {1...𝑚}. It is then possible to expand

[Schrijver 1986]. Specifically, let 𝑀 D𝑆1,𝑆2 denote the constraint matrix of D𝑆1,𝑆2
tion 3.2 (i.e. D𝑆1,𝑆2
makes it possible to introduce a vector of integer variables, ΛD𝑆1,𝑆2
plus the number of rows of 𝑀 D𝑆1,𝑆2 , for all D𝑆1,𝑆2
Equation (8c) to the following constraints.

𝑘

∀D𝑆1,𝑆2
ΛD𝑆1,𝑆2
𝑘

, ∀𝑘 ∈ {1...𝑚},

≥ 0

Θ𝑆2
𝑘 ·

(cid:174)𝑥𝑆2


(cid:174)𝑝


1










− Θ𝑆1
𝑘 ·

(cid:174)𝑥𝑆1


(cid:174)𝑝


1










−𝛿 D𝑆1,𝑆2
𝑘

+

𝑘−1
∑︁

𝑖=1

𝛿 D𝑆1,𝑆2
𝑖

(𝐾 (cid:174)𝑝 +𝐾) =

(cid:16)ΛD𝑆1,𝑆2

𝑘

(cid:17) ⊤

(cid:20)𝑀 D𝑆1,𝑆2
(cid:174)0
1

·

(cid:21)

·

(9a)

(9b)

(cid:174)𝑥𝑆1


(cid:174)𝑥𝑆2


(cid:174)𝑝


1












By equating the left and right side of Equation (9b) for all coefficients of (cid:174)𝑥𝑆1, (cid:174)𝑥𝑆2, (cid:174)𝑝 and the constant
terms produces the desired affine constraints in the ILP formulation of scheduling. Solving the
above formulation produces the desired schedule coefficients Θ in Section 3.3.2.
3.3.4 Example. We give an example of the above ILP formulation of scheduling, using the example
from Section 1 (Equation (4) and its IR form in Listing 5). Let 𝑆1 refer to statement S1 and 𝑆2 refer
to statement S2 in Listing 5. The dependence relations between 𝑆1 and 𝑆2 are:

D𝑆1,𝑆2 = [𝑁 ] → {[𝑖𝑆1, 𝑗𝑆1] → [𝑖𝑆2 ] : 𝑖𝑆1 = 𝑖𝑆2 ∧ 0 ≤ 𝑖𝑆1 < 𝑁 − 1 ∧ 0 ≤ 𝑗𝑆1 ≤ 𝑖𝑆1 }
D𝑆2,𝑆1 = [𝑁 ] → {[𝑖𝑆2] → [𝑖𝑆1, 𝑗𝑆1 ] : 𝑖𝑆1 = 𝑖𝑆2 + 1 ∧ 0 ≤ 𝑖𝑆2 < 𝑁 − 1 ∧ 0 ≤ 𝑗𝑆1 ≤ 𝑖𝑆2 }.
are given in Equation (11).

The corresponding constraint matrices for D𝑆1,𝑆2
5If the dependence relation between 𝑆1, 𝑆2 is a union of polyhedral relations, then we consider each piece of the union as
distinct and set up the constraints in Equation (8) for each piece of the union.

and D𝑆2,𝑆1

(10a)

(10b)

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:14

Cambridge Yang, Eric Atkinson, and Michael Carbin

𝑀 D𝑆1,𝑆2 =

(11a)

𝑀 D𝑆2,𝑆1 =

(11b)

−1
0
1
−1
1

0
1
−1
0
0

0
0
0
1
−1












1 −2


0
0


0
0


0
0


0
0



−1


0


1


−1


1



0
0
0
1
−1

0
1
−1
0
0

1 −2
0
0
0
0
0 −1
1
0












Assuming the schedule timestamp’s dimension is 𝑚 = 3 and 𝐾 is a large enough constant, we

formulate the dependence constraints as in Equations (8c) and (9) by Equation (12).

0 ≤ 𝛿 D𝑆1,𝑆2
𝑘

≤ 1

0 ≤ 𝛿 D𝑆2,𝑆1
𝑘

≤ 1

∀𝑘 ∈ {1...3}

(12a)

3
∑︁

𝑘=1

𝛿 D𝑆1,𝑆2
𝑘

ΛD𝑆1,𝑆2
𝑘

3
∑︁

= 1

𝛿 D𝑆2,𝑆1
𝑘

𝑘=1
ΛD𝑆2,𝑆1
≥ 0
𝑘
= ΛD𝑆1,𝑆2
𝑘
= ΛD𝑆1,𝑆2
𝑘
= ΛD𝑆1,𝑆2
𝑘
= ΛD𝑆1,𝑆2
𝑘
= ΛD𝑆1,𝑆2
𝑘
= ΛD𝑆2,𝑆1
𝑘
= ΛD𝑆2,𝑆1
𝑘
= ΛD𝑆2,𝑆1
𝑘
= ΛD𝑆2,𝑆1
𝑘
= ΛD𝑆2,𝑆1
𝑘

−Θ𝑆1
𝑘,1
−Θ𝑆1
𝑘,2
Θ𝑆2
𝑘,1
𝛿 D𝑆1,𝑆2
𝑖
𝛿 D𝑆1,𝑆2
𝑖
−Θ𝑆2
𝑘,1
Θ𝑆1
𝑘,1
Θ𝑆1
𝑘,2
𝛿 D𝑆2,𝑆1
𝑖
𝛿 D𝑆2,𝑆1
𝑖

· [−1, 0, 1, −1, 1, 0]
· [0, 1, −1, 0, 0, 0]
· [0, 0, 0, 1, −1, 0]
· [1, 0, 0, 0, 0, 0]
· [−2, 0, 0, 0, 0, 1]

· [−1, 0, 1, −1, 1, 0]
· [0, 0, 0, 1, −1, 0]
· [0, 1, −1, 0, 0, 0]
· [1, 0, 0, 0, 0, 0]
· [−2, 0, 0, −1, 1, 1]

𝑖𝑆1 :
𝑗𝑆1 :
𝑖𝑆2 :
𝑁 :
1 : Θ𝑆2
𝑘,3


𝑖𝑆2 :
𝑖𝑆1 :
𝑗𝑆1 :
𝑁 :
1 : Θ𝑆1
𝑘,3








Θ𝑆2
− Θ𝑆1
𝑘,2
𝑘,3
−𝛿 D𝑆1,𝑆2
𝑘

− Θ𝑆1
𝑘,4

+𝐾 · (cid:205)𝑘−1
𝑖=1
+𝐾 · (cid:205)𝑘−1
𝑖=1

Θ𝑆1
− Θ𝑆2
𝑘,3
𝑘,2
−𝛿 D𝑆2,𝑆1
𝑘

− Θ𝑆2
𝑘,4

+𝐾 · (cid:205)𝑘−1
𝑖=1
+𝐾 · (cid:205)𝑘−1
𝑖=1

= 1

(12b)

≥ 0

∀𝑘 ∈ {1...3}

(12c)

∀𝑘 ∈ {1...3}

(12d)

∀𝑘 ∈ {1...3}

(12e)

To briefly summarize, Equations (12a) to (12c) set up the variables related to the dependence con-
straints. Equations (12d) and (12e) handle the case of D𝑆1,𝑆2
respectively for Equation (9).
Specifically, Equations (12d) and (12e) apply Farkas’ Lemma for the coefficients of 𝑖𝑆1, 𝑗𝑆1, 𝑖𝑆2, 𝑁
and for the constant terms. We have labeled each equational constraint in Equations (12d) and (12e)
with the corresponding symbolic term (i.e. one of 𝑖𝑆1, 𝑗𝑆1, 𝑖𝑆2, 𝑁 ) or constant term (i.e. denoted by
1) on the left side of the equation.

and D𝑆2,𝑆1

3.3.5 Non-Sequential Scheduling. We first define a sequential schedule.

Definition 3.4 (Sequential Schedule). A schedule Θ is sequential if for each statement 𝑆 and its

scheduling function Θ𝑆 we have:

Θ𝑆 · [ (cid:174)𝑥𝑆
1

, (cid:174)𝑝, 1]⊤ ≠ Θ𝑆 · [ (cid:174)𝑥𝑆
2

, (cid:174)𝑝, 1]⊤ ∀(cid:174)𝑥𝑆
1

, (cid:174)𝑥𝑆
2

. (cid:174)𝑥𝑆

1 ≠ (cid:174)𝑥𝑆

2

.

(13)

Intuitively, a schedule is sequential if it maps distinct iteration vectors to distinct timestamps.
A valid, dependence-satisfying schedule that satisfies Equation (8) may be non-sequential in that
it permits multiple statement instances to execute in the same timestep. Therefore, it is possible
for a schedule to demand an unbounded number of statement instances to execute at the same
timestamp, which does not directly map to physical machines with finite resources [Gupta et al.
2002; Redon and Feautrier 1994].

In the context of reductions, a reduction may be scheduled to accumulate an unbounded number
of values at the same timestamp [Redon and Feautrier 1994]. For example, the (one-dimensional)
schedule that assigns S1[𝑖, 𝑗] to [𝑖] is not realistic, since it assigns the same timestamp to S1[𝑖, 𝑗]

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:15

S1 : BTmp[𝑖] += A[ 𝑗] : {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1Fin : B[𝑖] = BTmp[𝑖] : {[𝑖] : 0 ≤ 𝑖 < 𝑁 }
S2 : A[𝑖 + 1] = 𝑓 (B[𝑖]) : {[𝑖] : 0 ≤ 𝑖 < 𝑁 − 1}

⇓

S1Add : BTmpAdd[𝑖] += A[ 𝑗] : {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 𝑖 = 𝑗 }
S1AddOnly : BTmp[𝑖] = BTmpAdd[𝑖] : {[𝑖] : 𝑖 = 0}
S1AddReuse : BTmp[𝑖] = BTmp[𝑖 − 1] + BTmpAdd[𝑖] : {[𝑖] : 1 ≤ 𝑖 < 𝑁 }
S1Fin : B[𝑖] = BTmp[𝑖] : {[𝑖] : 0 ≤ 𝑖 < 𝑁 }
S2 : A[𝑖 + 1] = 𝑓 (B[𝑖]) : {[𝑖] : 0 ≤ 𝑖 < 𝑁 − 1}

Listing 6. ST in the polyhedral IR for the example in Section 1 (Equation (4)), given the reuse vector [1, 0]⊤

for all 𝑗, and therefore requires accumulation of a potentially unbounded number of values. Gupta
et al. [2002] demonstrate a scheduling approach that bounds the total accumulations per timestep
to target physical machines.

In this paper, we do not extend Pouchet et al. [2011]’s formalization with physical scheduling
constraints. While this consideration is important for practical scheduling of reductions, we only use
their scheduling formalization to support our formalization of the dependent reduction scheduling
problem in Section 5. Our heuristic algorithm in Section 6.2 does not require a schedule to have
been computed using their scheduling formalization. Our algorithm instead relies on a sequential
schedule that can be computed via any means, including a scheduling algorithm that adopts realistic
scheduling constraints. In Section 6.1 we discuss sequential scheduling in more detail.

4 BACKGROUND: THE SIMPLIFYING REDUCTIONS FRAMEWORK
Previous work introduced a core transformation called the simplification transformation (ST) that
can transform a single reduction specified in in the polyhedral representation (Figure 5) to lower
its complexity [Gautam and Rajopadhye 2006]. The work also introduced a set of enabling transfor-
mations that make available opportunities to simplify reductions.

For the core transformation, we use an example from Section 1 to illustrate the transformation. For
the enabling transformations, we include a brief description of each transformation in Appendix C.
Finally, Gautam and Rajopadhye [2006] combine all the transformations to provide a dynamic
programming algorithm to efficiently choose, from an infinite set of configurations and orders for
the transformations, a sequence of transformations that lead to optimal complexity reduction.

4.1 Simplification Transformation
Here we use the example from Section 2.1 to illustrate how the simplification transformation reduces
the complexity of a reduction. We provide a more complete specification of ST in Appendix B.

Listing 6 illustrates the example of applying ST to Listing 5 (i.e., the IR form of Equation (4)) to
produce the optimized version in Equations (2a), (2b) and (4b). As we mentioned before, core ST
operates on a single statement only and only produces a correct result for a dependent reduction if
provided with a correct reuse vector.

Original Reduction. Above the arrow, Listing 6 presents the reduction in Equation (4a) in the
polyhedral IR as the statement S1 with domain P = [𝑁 ] → {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}. The
right hand side expression is A[ 𝑗], and 𝑖 is not a bound variable – this means given a fixed 𝑗, the
right hand side’s values are the same for different values of 𝑖.

Optimized Reduction. Below the arrow, the optimized prefix sum consists of three distinct com-
putations: seeding the initial values of the reduction, computing the root of the reduction, and
computing the core of the reduction via reuse.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:16

Cambridge Yang, Eric Atkinson, and Michael Carbin

The statement S1Add computes BTmpAdd, which serves to hold the unique seeds of the reduction:
the uniquely computed value introduced into the reduction at each iteration 𝑖. The statement
computes over the full space of 𝑖 and sets BTmpAdd[i] to equal A[i]. The statement S1AddOnly
computes the root of the reduction, the first (by dependence order) value computed into BTmp[0]
and for which the reduction use not reuse any previous computation. Hence, S1AddOnly sets
BTmp[0] to equal BTmpAdd[i], the unique element for this iteration. The statement S1AddReuse
computes the core of the reduction incrementally, summing the previous computation BTmp[i - 1]
to the unique element for this iteration.

To identify this optimization opportunity and generate the optimized code, the Simplification
Transformation identifies a reuse vector by which shifting the original, unoptimized polyhedron
(P) makes plain that consecutive iterations of the polyhedron share the same computation and can
be incrementalized. The reuse vector then drives the transformation of the code. To elaborate ST’s
mechanics, we first present how ST leverages a reuse vector to transform the program before then
explaining how ST identifies appropriate reuse vectors.

4.2 From Reuse Vector to Transformed Program
ST takes a reuse vector and manipulates the domain of the
reduction to compute the subset of iteration instances that can
be transformed to incrementally compute their results using
previously computed results.

Consider the reuse vector (cid:174)𝑟 = [1, 0]⊤, which can also be rep-
resented by the polyhedral relation {[𝑖, 𝑗] → [𝑖 + 1, 𝑗] : ∀𝑖, 𝑗 }.
Conceptually, the reuse vector therefore denotes a shift of all
points [𝑖, 𝑗] to [𝑖 + 1, 𝑗]. Given the reuse vector (cid:174)𝑟 = [1, 0]⊤, ST performs the following steps:

Fig. 6. Visualization of the algo-
rithm on prefix sum example.

• Shift. The transformation first shifts S1’s polyhedron along the direction of the reuse vector,
transforming {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖} into {[𝑖, 𝑗] : 1 ≤ 𝑖 < 𝑁 + 1 ∧ 0 ≤ 𝑗 ≤ 𝑖 − 1}.
Figure 6 illustrates the two polyhedrons, with the pre-transformation polyhedron presented as
the crosshatched polyhedron with round dots and the post-shift polyhedron presented as the
dot-shaded polyhedron with triangles.

• Intersect. The transformation next computes the intersection of the shifted polyhedron with
its original polyhedron, yielding {[𝑖, 𝑗] : 1 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖 − 1} (triangles outlined in
round dots in Figure 6). This polyhedron denotes the subset of points of the original domain P,
whose value can be reused from the predecessor points as indicated by the reuse vector. We note
that the intersection computation itself does not detect the equivalence of expressions across
indices. Instead, ST’s reuse vector selection process (Section 4.3) ensures that instances within
the intersection denote equivalent expressions. i

• Project. Finally, the transformation projects the result onto the space of the polyhedron that
represents the indices of the left hand side of the array BTmp. Concretely, the transformation
applies the projection represented by the polyhedral relation {[𝑖, 𝑗] → [𝑖] : ∀𝑖, 𝑗 }), yielding the
polyhedron {[𝑖] : 1 ≤ 𝑖 < 𝑁 }. This final polyhedron is exactly the domain of elements of BTmp
that exhibits reuse along the reuse vector (cid:174)𝑟 . This polyhedron corresponds to the domain of the
statement S1AddReuse, computes BTmp[𝑖] with BTmp[𝑖] = BTmp[𝑖 − 1] + BTempAdd[𝑖].

The polyhedron {[𝑖] : 1 ≤ 𝑖 < 𝑁 } does not cover the full domain of the original reduction.
Specifically, it is missing BTmp[𝑖] on the domain {[𝑖] : 𝑖 = 0} – that is, exactly when 𝑖 = 0. The value
of BTmp[0] should be equal to A[0] and the transformation generates the statement S1AddOnly to
perform this computation.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:17

4.3 Selecting a Reuse Vector
The simplifying transformation work offers a fully automated technique to identify a reuse vector
for a reduction. The techniques must specifically (1) identify if computation is shared among
iterations of the reduction, (2) identify if the shared computation is reusable: if it is possible to
transform the program reuse the shared computation), and 3) identify if reusing shared computation
is profitable: the transformation reduces the complexity of a program.

Shared. ST includes an algorithm to determine the share space: the set of all vectors such that, for
each vector, the evaluations of right hand side expression of the reduction are the same if shifted
along the reuse vector.

For the prefix sum example in Section 4.2, the right hand side expression (𝐴[ 𝑗])) is the same
along the direction [1, 0]⊤. However, the right hand side expression is also the same along the
direction [−1, 0]⊤ as well as [2, 0]⊤. In fact, any reuse vector [𝑛, 0] for a given integer 𝑛 is in the
share space of this reduction. Therefore, generally, the set of reuse vectors is potentially infinite in
size.

For a statement 𝑆, let S(𝑆) denote the statement’s share space. Gautam and Rajopadhye [2006,

Section 5.2] demonstrated how to compute S(𝑆) from 𝑆.

Inverse. If ⊕ does not have an inverse, we require that applying ST
along a vector (cid:174)𝑟 does not introduce statements that require the inverse
operator of ⊕. For example, if ⊕ is min() or max(), it does not have an
inverse; in such cases, Gautam and Rajopadhye [2006] introduce the
concept of Boundary Constraints – which in short is the set of constraints
of the domain P that are orthogonal to the projection proj – and requires
that (cid:174)𝑟 must be pointing out of (instead of pointing into) the boundaries
of P corresponding to the Boundary Constraints.

For example, consider a modification to the prefix sum example (List-

Fig. 7. Visualization of the
Inverse constraint

ing 1), where the summation is replaced with max. The ST application on this modified example
along the reuse vector [−1, 0]⊤ is invalid, since max does not have an inverse but the transformed
program requires an inverse operation in order to compute 𝐵 [𝑖] from 𝐵 [𝑖 + 1] (i.e. Equation (3b)).
Figure 7 illustrates this constraint: for the boundary of the domain orthogonal to the projection
(the right side of the triangular domain), the inverse constraint requires the reuse vectors to point
out of the domain (i.e. to the dotted arrow’s direction).

For a statement 𝑆, let I (𝑆) denote the set of vectors (cid:174)𝑟 that satisfy the inverse constraint. Gautam

and Rajopadhye [2006, Section 5.4] demonstrated how to compute I (𝑆) for a statement 𝑆.

Profitable. Applying ST with a given reuse vector is profitable only if the transformed program
has lower complexity than the original program. Of note, the complexity of a program will not
increase after applying ST for any (cid:174)𝑟 . However, the complexity can stay the same if (cid:174)𝑟 is chosen along
a direction where the original polyhedral domain P has constant thickness – that is, the extent of
P is bounded by some constant not parameterized by the input parameters of the program.

For example, consider an extreme case of the prefix sum example (Listing 1, Listing 2) but with
the input parameter 𝑁 fixed to some constant – say 𝑁 = 4. The complexities before and after ST
will be the same – O (1) – since both programs will perform a fixed number of computations.

For a reduction statement S with domain P, let L (P) denote the set of reuse vectors that reduce
the complexity of the reduction. Gautam and Rajopadhye [2006, Section 4.2] demonstrated how to
compute L (P) given a domain P.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:18

Cambridge Yang, Eric Atkinson, and Michael Carbin

Valid. In general, the set of valid reuse vectors are those that satisfy the above properties (i.e.,
shared, reusable, and profitable). Formally, if we let the notation S.domain denote the domain of a
statement S, then the set of valid reuse vectors is R (𝑆) = S(𝑆) ∩ I (𝑆) ∩ L (𝑆.domain). Therefore,
any (cid:174)𝑟 ∈ R (𝑆) is a valid reuse vector for applying ST.

4.4 Residual Reduction
Notice in Listing 6 that the statement S1Add still contains a reduction. Although for this example
S1Add does not have further ST opportunities, in general, the residual reduction might still have
available ST opportunities so that ST can be applied recursively to all introduced reductions.6

Even though the space of valid reuse vectors (Section 4.3) is potentially infinite and cannot be
simply enumerated, Gautam and Rajopadhye [2006] presented an dynamic programming algorithm
that partitions this space into equivalence classes. The algorithm then enumerates those equivalence
classes to choose reuse vectors. The algorithm always terminates and produces a resulting program
with optimal complexity (i.e. minimal polynomial degree).

5 SIMPLIFYING DEPENDENT REDUCTIONS PROBLEM
In this section, we state the Simplifying Dependent Reductions (SDR) problem. In particular, we
focus on the core of the Simplifying Reductions approach – the Simplification Transformation
in Section 4.1 – and do not consider the Simplifying Reductions framework’s additional enabling
transformations. These transformations increase available simplification opportunities; we briefly
touch on enabling transformations in Section 5.4.

5.1 Problem Statement
Ideally we would formulate the Simplifying Dependent Reduction (SDR) as Equation (14).

minimize complexity(prog′)
subject to

𝑛
prog0 = prog, prog′ = prog
∀𝑖 ∈ {1...𝑛} : prog

𝑖 = STS𝑖,(cid:174)𝑟𝑖 (prog

𝑖−1)

∃ schedule Θ of prog

(cid:174)𝑟𝑖 ∈ S (S𝑖 ) ∩ I (S𝑖 ) ∩ L (S𝑖 .domain)
𝑛,
s.t. Θ satisfies dependence(prog

𝑛)

given prog, dependence(𝑝𝑟𝑜𝑔)
variables S1, . . . , S𝑛, (cid:174)𝑟1, . . . , (cid:174)𝑟𝑛

(14a)

(14b)
(14c)
(14d)

(14e)

(14f)
(14g)

Equation (14) states that given a program 𝑝𝑟𝑜𝑔, and all pairwise dependences between those
statements, dependence (prog), apply a sequence of 𝑛 ST transformations, STS1,(cid:174)𝑟1
that
minimize the complexity of the resulting program, prog′. Here we use STS,(cid:174)𝑟 (prog) to denote an ST
that is applied on a statement S in prog along the reuse vector (cid:174)𝑟 . Each S𝑖 is a variable that refers
to a reduction in prog𝑖−1. Note that S𝑖 can either be a reduction that is in prog0, the initial input
program, or it can be reduction that is introduced by any of the previous 𝑖 − 1 ST applications (i.e.
recursive ST in Section 4.4). Further, Equation (14d) requires each 𝑟𝑖 to satisfy the constraints (i.e.
complexity, inverse and sharing, denoted by S(.), I (.), L (.) respectively) as stated in Section 4.3.
Unfortunately, there are two issues with Equation (14): 1) it has infinite space for (cid:174)𝑟𝑖 2) it has

, . . . , STS𝑛,(cid:174)𝑟𝑛

impractically large space for S𝑖 .

6In general, ST can also introduce more than one reduction; we include a fuller description of ST in Appendix B.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:19

Firstly, if we assume that 𝑛 is given and bounded, 7 the formulation does not readily translate to
an executable algorithm. Specifically, enumeratively searching all possible (cid:174)𝑟𝑖 combinations is not
feasible: each (cid:174)𝑟𝑖 alone is chosen from an infinite set of vectors, and the entire search space is also
infinite; therefore the search space of reuse vectors cannot be simply enumerated (however it can
be partitioned in to equivalence classes as proposed by Gautam and Rajopadhye [2006]).

Secondly, also assuming 𝑛 is given and bounded, the program relies on a sequence of statements,
(S1, . . . , S𝑛), to specify on which statement in prog𝑖 to perform ST. Although, unlike the case of (cid:174)𝑟𝑖 ,
the number of choices for each S𝑖 is finitely bounded (i.e. by the number of ST-applicable reductions
in the program), the combinations of all possible (S1, . . . , S𝑛) has at least |S1|! possibilities: assuming
the best case scenario where each ST applications removes one reduction and introduces zero
reductions that are potentially applicable for further ST applications, which imples the 𝑖-th ST
application has 𝑛 − 𝑖 + 1 remaining alternative choices of S𝑖 (i.e. |S𝑖 | = 𝑛 − 𝑖 + 1). Therefore the
search space of S𝑖 is also not practical to navigate with enumerative search.

5.2 Per-face ST Application
We will resolve these issues with a correct formalization in the rest of Section 5. Specifically, we
show, for a program, a one-to-one correspondence between all its potential ST applications and all
faces of its reductions’ domains. This correspondance allows a construction of an Integer Bilinear
Programming (IBP) formulation to SDR, which avoids the explicit enumerative searchs in the above
issues of Equation (14).

Definition 5.1 (Face of polyhedral set). Let the polyhedral set P = [ (cid:174)𝑝] → {[ (cid:174)𝑥] : 𝑀 · [ (cid:174)𝑥, (cid:174)𝑝, 1] ≥ (cid:174)0}.
Let 𝑀𝑖 be the 𝑖-th row of matrix 𝑀. A face of P is defined as F = P ∩ B where B = [ (cid:174)𝑝] → {[ (cid:174)𝑥] :
𝐵 · [ (cid:174)𝑥, (cid:174)𝑝, 1] = (cid:174)0} and ∀𝑖∃𝑗, 𝐵𝑖 = 𝑀𝑗 .

In words, a face of P is P with a subset of (potentially empty or all) inequality constraints of P

changed to equality constraints.

We first make the following observation of ST on a single statement S with domain P: if we apply
ST on S, we can then recursively apply ST on the newly introduced reductions, as in Section 4.4, and
this is exactly the root problem of the incorrect formulation Equation (14): this recursion appears
non-terminating. We will solve this issue by stating and proving Lemma 5.3 — to this end, we first
recall Lemma 5.2 from Gautam and Rajopadhye [2006] that we will use in our proof. We then state
Lemma 5.3 and give a proof.

Lemma 5.2 (Local Face Correspondance [Gautam and Rajopadhye 2006, Theorem 3]). Let
P ′ be the translation of an 𝑛-dimensional P along (cid:174)𝑟 , then P − P ′ = ⊎𝑖 P𝑖 . That is, P − P ′ is a finite
union of P𝑖 s. Further, there exists a one-to-one map from 𝑖 to faces of P such that each P𝑖 corresponds
uniquely to a (𝑛 − 1)-dimensional face of P.

Lemma 5.3 (Global Face Correspondance). Each recursive application of ST is on a subset (a

polyhedral set) of P, and all subsets correspond exactly one-to-one to all faces of P.

Proof. Given a statement S with domain P, ST performs a shift of P along a given reuse vector
to P ′; new reduction statements are introduced over domains P − P ′ and P ′ − P. Note that these
two domains are non-convex half shells around the original domain P, and together form a full
shell around P. The two shells are both non-convex, however by Lemma 5.2, they decompose
into convex polyhedral domains, each corresponding to a unique (𝑛 − 1)-dimensional face of the
𝑛-dimensional polyhedron P.

7The total number of ST applications 𝑛 must be a finite given the fact that the complexity of the input program is finite, and
that each ST application reduces the complexity.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:20

Cambridge Yang, Eric Atkinson, and Michael Carbin

ST is applied recursively on these decomposed (𝑛−1)-dimensional faces and then on the sequence
of (𝑛 −𝑖)-dimensional faces until the recursion hits the vertices of P. Therefore, the entire recursion
is a procedure that enumerates through all faces of a statement S’s full domain P, and assigns a
□
reuse vector to each face.

With Lemma 5.3, the recursive ST application always terminates since the number of faces of P
is finite. Further, this introduces a per-face application view of ST. Specifically, if we think about the
algorithm that recursively applies ST to an input program to get the final optimized program as is
done in Gautam and Rajopadhye [2006], the recursion forms a computation tree, where each node
of the tree corresponds to a choice of reuse vector of ST. Lemma 5.3 says that 1) this recursion is
bounded, and 2) each node in this computation tree corresponds one-to-one to a face in the input
program’s statements’ domains. Under the per-face application view, an algorithm may first choose
a reuse vector for each face of each statement in the program up-front (i.e. assigns the choice of
each node in the computation tree up-front), instead of recursively requesting the reuse vectors.

5.3 Integer Bilinear Program Formulation
With the per-face application view of ST in Section 5.2, we are now ready to give the correct
formulation of SDR. The basic idea behind this formulation is to combine previous work on ST for
a single statement [Gautam and Rajopadhye 2006, Section 5] previous work on the integer linear
program formulation of polyhedral model scheduling [Pouchet et al. 2008, 2007, 2011], and the
per-face application view of ST (Section 5.2). We first revisit Equation (14) and give the correct high
level formulation as Equation (15).

This optimization problem minimizes the complexity of prog′ (Equation (15a)), which is a version
of prog transformed by a composition of STs applied to each face (Equation (15b)). The reuse vectors
{𝑟1, . . . , 𝑟𝑛 } that drive each ST must lie in the prescribed set that presents sharing (S(.)), satisfies
the inverse condition (I (.)) and reduces complexity (L (.)), as in Section 4.3 (Equation (15c)). Lastly,
there must exist a schedule Θ that satisfies the dependences in prog′ (Equation (15d)).

minimize complexity(prog′)
subject to

prog′ = (ST𝑓1,(cid:174)𝑟1
◦ . . . ◦ ST𝑓𝑛,(cid:174)𝑟𝑛 )(prog)
∀𝑖 ∈ {1...𝑛} : (cid:174)𝑟𝑖 ∈ S (𝑓𝑖 .stmt) ∩ I (𝑓𝑖 .stmt) ∩ L (𝑓𝑖 )
∃ schedule Θ of prog’,

s.t. Θ satisfy dependence(𝑝𝑟𝑜𝑔′)

given prog, dependence(𝑝𝑟𝑜𝑔)
variables (cid:174)𝑟1, . . . , (cid:174)𝑟𝑛

(15a)

(15b)
(15c)

(15d)

(15e)
(15f)

This high level formulation is similar to Equation (14), except that now 1) each reuse vector (cid:174)𝑟𝑖
is in one-to-one correspondence with a face 𝑓𝑖 — we thus have a bounded number of unknown
variables for reuse vectors, and 2) the variables S𝑖 are eliminated, as the new formulation uses the
per-face ST view, instead of the recursive ST application view. Lastly, each reuse vector is still
constrained to satisfy the validity constraints (i.e. Equation (15c)).

5.3.1 Transformed Program. The program prog′ is a version of prog transformed by the composition
of STs applied to each face (Equation (15b)) following the insight from Section 5.2.

Example. Listing 7 illustrates applications of ST to all the faces of the program for Equation (4).
Specifically, the top of Listing 7 shows the program prog in the polyhedral IR of Equation (4), and
the bottom of Listing 7 shows the program prog′ after the sequence of ST transformations. Each
introduced reduction, by the insight from Section 5.2, corresponds to a face of the reduction S1’s

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:21

S1 : BTmp[𝑖] += A[ 𝑗] : {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1Fin : B[𝑖] = BTmp[𝑖] : {[𝑖] : 0 ≤ 𝑖 < 𝑁 }
S2 : A[𝑖 + 1] = 𝑓 (B[𝑖]) : {[𝑖] : 0 ≤ 𝑖 < 𝑁 − 1}

⇓

S1 : BTmp[𝑖] += 𝐴[ 𝑗] : {[𝑖, 𝑗] : 𝑟𝑖 = 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1AddR1Pos : BTmpAdd[𝑖] += 𝐴[ 𝑗] : {[𝑖, 𝑗] : 𝑟𝑖 > 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑖 < 𝑁 ∧ 𝑖 − 𝑟𝑖 < 𝑗 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1AddR1Neg : BTmpAdd[𝑖] += 𝐴[ 𝑗] : {[𝑖, 𝑗] : 𝑟𝑖 < 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑁 + 𝑟𝑖 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}
S1SubRNeg : BTmpSub[𝑖 + 𝑟𝑖 ] += 𝐴[ 𝑗] : {[𝑖, 𝑗] : 𝑟𝑖 < 0 ∧ 𝑟 𝑗 = 0 ∧ −𝑟𝑖 ≤ 𝑖 < 𝑁 ∧ 𝑖 + 𝑟𝑖 < 𝑗 ≤ 𝑖}
S1AddOnlyR1Pos : BTmp[𝑖] = BTmpAdd[𝑖] : {[𝑖] : 𝑟 𝑗 = 0 ∧ 0 ≤ 𝑖 < 𝑟𝑖 }
S1AddOnlyR1Neg : BTmp[𝑖] = BTmpAdd[𝑖] : {[𝑖] : 𝑟 𝑗 = 0 ∧ 𝑁 + 𝑟𝑖 ≤ 𝑖 < 𝑁 }
S1AddReuse : BTmp[𝑖] = BTmp[𝑖 − 𝑟𝑖 ] + BTmpAdd[𝑖] : {[𝑖] : 𝑟𝑖 > 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑟𝑖 ≤ 𝑖 < 𝑁 }
S1SubRNegReuse : BTmp[𝑖] = BTmp[𝑖 − 𝑟𝑖 ] − BTmpSub[𝑖] : {[𝑖] : 𝑟𝑖 < 0 ∧ 𝑟 𝑗 = 0 ∧ 0 ≤ 𝑖 < 𝑁 + 𝑟𝑖 }
S1Fin : B[𝑖] = BTmp[𝑖] : {[𝑖] : 0 ≤ 𝑖 < 𝑁 }
S2 : A[𝑖 + 1] = 𝑓 (B[𝑖]) : {[𝑖] : 0 ≤ 𝑖 < 𝑁 − 1}

Listing 7. Applying ST to all the faces of the polyhedral IR program of Equation (4), given the reuse vector
(cid:174)𝑟1 = [𝑟𝑖, 𝑟 𝑗 ]⊤consisting of integer variables 𝑟𝑖 and 𝑟 𝑗 . The reuse vectors {(cid:174)𝑟2, . . . , (cid:174)𝑟8} and statements with
empty domains are omitted.

domain. The reduction S1 has domain {[𝑖, 𝑗] : 𝑟𝑖 = 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖}. Equation (16)
gives all the faces 𝑓𝑖 of this domain.

𝑓1 = {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 0 ≤ 𝑗 ≤ 𝑖} (16a)
(16b)
𝑓2 = {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 𝑗 = 0}
𝑓3 = {[𝑖, 𝑗] : 0 ≤ 𝑖 < 𝑁 ∧ 𝑗 = 𝑖}
𝑓4 = {[𝑖, 𝑗] : 𝑖 = 𝑁 − 1 ∧ 0 ≤ 𝑗 ≤ 𝑖}

(16d)

(16c)

𝑓5 = {[𝑖, 𝑗] : 𝑖 = 0 ∧ 𝑗 = 0}
(16f)
𝑓6 = {[𝑖, 𝑗] : 𝑖 = 𝑁 − 1 ∧ 𝑗 = 0}
𝑓7 = {[𝑖, 𝑗] : 𝑖 = 𝑁 − 1 ∧ 𝑗 = 𝑁 − 1} (16g)
(16h)
𝑓8 = {[𝑖, 𝑗] : 1 = 0}

(16e)

We briefly explain the statements in prog′ and associate the introduced reductions to the faces.
(1) S1 in prog′ is the same as S1 in the original prog, but with the extra constraints on its domain
that 𝑟𝑖 = 0 ∧ 𝑟 𝑗 = 0. This corresponds to the case that (cid:174)𝑟1 = [0, 0]⊤, which means that we do not
apply ST at all. This reduction corresponds to the face 𝑓1.

(2) S1AddR1Pos corresponds to 𝑓3 (the top polyhedron in Figure 2b) for the case when 𝑟1 > 0.
S1AddR1Neg corresponds to 𝑓3 (the top polyhedron in Figure 3b) for the case when 𝑟1 < 0.
(3) S1SubRNeg is a residual reduction that is required for incrementally computing B via subtraction.
It is only non-empty for the case when 𝑟1 < 0. This reduction corresponds to the face 𝑓2.
(4) S1AddOnlyR1Pos initializes BTmp from the result of S1AddR1Pos for the case when 𝑟1 > 0.
S1AddOnlyR1Neg initializes BTmp from the result of S1AddR1Neg for the case when 𝑟1 < 0.
(5) S1AddReuse and S1SubRNegReuse compute B incrementally by respectively adding or subtract-

ing from the previous value of B[𝑖 − 𝑟𝑖 ].

(6) SFin and S2 remain the same as in the original prog.

5.3.2 Reuse Constraints. The reuse constraints enforce that each (cid:174)𝑟𝑖 is chosen from S (𝑓𝑖 .stmt) ∩
I (𝑓𝑖 .stmt) ∩ L (𝑓𝑖 ). Gautam and Rajopadhye [2006] demonstrate how to compute each of these sets
from 𝑓𝑖 (Section 4.3). The intersection is a union of polyhedral sets. We use disjunction to constrain
(cid:174)𝑟𝑖 to belong to one of the polyhedral sets. For each polyhedral set, encoding that (cid:174)𝑟𝑖 belongs to the
polyhedral set is then just a simple affine inequality constraint.

Example. We illustrate reuse constraints for the example of Listing 7. First, S(S1) = [𝑁 ] →
{[𝑖, 𝑗] : 𝑗 = 0} since reduction’s right hand side 𝐴[ 𝑗]’s values are the same for different 𝑖 for
any fixed 𝑗, for S(S1). Second, I (S1) is the universe set (i.e. does not impose any constraint),
since the operator of S1 is the addition operator (i.e. +) and it has a well defined inverse operator,

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:22

Cambridge Yang, Eric Atkinson, and Michael Carbin

subtraction (i.e. −). Third, we give L (𝑓𝑖 ) in Equation (1) in Appendix A. Therefore, the intersection
S(S1) ∩I (S1) ∩L (𝑓𝑖 ) is equal to [𝑁 ] → {[𝑖, 𝑗] : 𝑗 = 0} for 𝑓1 and 𝑓2, and is equal to [𝑁 ] → {[𝑖, 𝑗] :
𝑖 = 0 ∧ 𝑗 = 0} for 𝑓3...8. In summary, we impose the constraint that (cid:174)𝑟1, (cid:174)𝑟2 ∈ [𝑁 ] → {[𝑟𝑖, 𝑟 𝑗 ] : 𝑟 𝑗 = 0},
and (cid:174)𝑟𝑖 ∈ [𝑁 ] → {[𝑟𝑖, 𝑟 𝑗 ] : 𝑟𝑖 = 0 ∧ 𝑟 𝑗 = 0} for all 𝑖 ≥ 3.

5.3.3 Dependence Constraints. The dependence constraints enforce that Θ satisfies the dependences
of prog′. Specifically, it requires that for each pair of statements 𝑆1 and 𝑆2 that potentially occur in
prog′, their scheduling functions Θ𝑆1, Θ𝑆2 satisfy the dependence relation D𝑆1,𝑆2
. At a high level,
we set up the dependence constraints just the same as in Equation (8); however, with Equation (15)
to contain entries with (linear) terms with unknowns from
we allow the dependence matrix 𝐷𝑆1,𝑆2
contains these unknown entries is: if we look from
(cid:174)𝑟1...(cid:174)𝑟𝑛. An informative argument for why 𝐷𝑆1,𝑆2
the recursive ST view, each application of ST introduces a reuse vector variable (cid:174)𝑟𝑖 , and the algorithm
recurses down to the residual reductions – for the next recursive application, we can think of it as
taking in a program with both the original program’s parameters and the reuse vectors introduced
by the previous ST application. The residual reductions’ domains then have space extended by
(cid:174)𝑟1...(cid:174)𝑟𝑛.

Example. We formulate the dependence constraints (Equation (15d)) for prog′ in Listing 7. This
step is the same as in Section 3.3.3 except that the dependence relations’ constraint matrices, instead
of being constant matrices, now contain 𝑟1 and 𝑟2 as integer variables. For brevity of presentation,
we will give the example of formulating the dependence constraint (i.e. Equations (8) and (9)) for
statement S1AddReuse to itself. The rest of the dependence constraints between other pairs of
statements can be formulated in the same way.

For

this example, we let statement 𝑆 be
S1AddReuse. For this statement, the dependence re-
lation D𝑆,𝑆 between instances of 𝑆, is [𝑁 ] → {[𝑖] →
[𝑖 ′] : 𝑖 ′ = 𝑖 + 𝑟𝑖 ∧ 𝑟𝑖 > 0 ∧ 𝑟 𝑗 = 0 ∧ 𝑟𝑖 <= 𝑖 < 𝑁 − 𝑟𝑖 }.
Equation (17) presents the constraint matrix 𝑀 D𝑆,𝑆
of this polyhedral relation. Notice that 𝑀 D𝑆,𝑆 con-
tains 𝑟𝑖 and 𝑟 𝑗 as variables in its last column. We can
then formulate the dependence constraint as in Equations (8) and (9) and get Equation (18).

0
0
1 −𝑟𝑖 −1
0
0
0
0

0


1


−1


−1


1


0


0



0
0
0
1
−1
0
0

−𝑟𝑖
𝑟𝑖
𝑟 𝑗
−𝑟 𝑗

𝑟𝑖 −1
−𝑟𝑖

𝑀 D𝑆,𝑆 ·

𝑖
𝑖 ′
𝑁
1

𝑖
𝑖 ′
𝑁
1











































=

·










(17)

0 ≤ 𝛿 D𝑆,𝑆
𝑘

≤ 1 ∀𝑘 ∈ {1...3}

(18a)

3
∑︁

𝑘=1

𝛿 D𝑆,𝑆
𝑘

= 1

(18b)

ΛD𝑆,𝑆
𝑘

≥ 0 ∀𝑘 ∈ {1...3}

(18c)

𝑖 :
𝑖 ′ :
𝑁 :
1 : 𝐾 · (cid:205)𝑘−1
𝑖=1






𝑘,1

−Θ𝑆
Θ𝑆
𝑘,1
𝛿 D𝑆,𝑆
𝑖
−𝛿 D𝑆,𝑆
𝑘

= ΛD𝑆,𝑆
𝑘
= ΛD𝑆,𝑆
𝑘
= ΛD𝑆,𝑆
𝑘
= ΛD𝑆,𝑆
𝑘

· [0, 1, −1, −1, 1, 0, 0, 0]
· [0, 0, 0, 1, −1, 0, 0, 0]
· [0, 0, 1, 0, 0, 0, 0, 0]
· [𝑟𝑖 − 1, −𝑟𝑖, −𝑟𝑖 − 1, −𝑟𝑖, 𝑟𝑖, 𝑟 𝑗 , −𝑟 𝑗 , 1]

∀𝑘 ∈ {1...3}

(18d)

𝐾 · (cid:205)𝑘−1
𝑖=1
𝛿 D𝑆,𝑆
𝑖

Notice that Equation (18d) refers to the integer variables 𝑟𝑖 and 𝑟 𝑗 , multiplying them with ΛD𝑆,𝑆
makes the problem bilinear constrained.

𝑘

5.3.4 Objective: Complexity. Since we would like to minimize the overall complexity, we need to
express our integer bilinear program’s objective as the complexity of the transformed program. We
can compute complexity of each face by counting the cardinality of each face’s domain [Verdoolaege
et al. 2007]. The cardinality of a face is an Ehrhardt polynomial [Ehrhardt 1967] in terms of the
program parameters.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:23

Encoding. If the program only has one parameter, then the degree of the polynomial is a natural

choice of a scalar that represents the complexity of the program.

If the program has multiple parameters, then one needs to be careful about comparing com-
plexities: it is necessary to be able to compare between O (𝑀 2𝑁 ) and O (𝑀𝑁 2) in order to min-
imize complexity. To this end, we assume that a total ordering is given for all possible poly-
nomial terms of global parameters as a sequence of increasing scalars. For example, with two
global parameters 𝑀, 𝑁 , and maximum possible complexity O (𝑀 2𝑁 2), a total ordering such as
O (1) < O (𝑀) < O (𝑁 ) < O (𝑀𝑁 ) < O (𝑀 2𝑁 ) < O (𝑀𝑁 2) < O (𝑀 2𝑁 2) is given, and integers
0...6 are assigned to each big-O term in the previous sequence.

Summing Scalar Encodings. Either the program has a single global parameter or has multiple
global parameters, we have a mapping from complexities, which are polynomials in terms of
global parameters, to their scalar encodings. Since the final objective is the total complexity of the
full transformed program, we need to sum the scalar encoding of complexities for all statements,
without losing the ability to compare the resultants’ degrees. To that end, we propose to use a
simple base-|𝑆 | encoding method where |𝑆 | is the maximum number of statements in the program:
for a complexity encoded as scalar 𝑐, we use |𝑆 |𝑐 as a term in the final objective. As an example,
to sum two complexities represented in scalar 𝑐1 and 𝑐2, we compute |𝑆 |𝑐1 + |𝑆 |𝑐2. We define the
base-|𝑆 | sum of 𝑐𝑖 as (cid:205) |𝑆 |𝑐𝑖 .

Indicator Variable. In the formulation, we require indicator variables to indicate if ST is disabled
along a certain face – in which case no complexity reduction should be applied for the corresponding
domain. We can use the big-M method, a well-known ILP modeling trick [Nemhauser and Wolsey
1988], to encode an indicator variable 𝑦 ∈ {0, 1} for the constraint 𝑥 = 0 so that 𝑦 = 1 iff 𝑥 = 0.

Example. To formulate the objective by encoding the complexity of the prog′ in Listing 7, we first
find the asymptotic complexity of each face by counting its cardinality. For this example, we have
complexity(𝑓1) = O (𝑁 2), complexity(𝑓2) = O (𝑁 ), complexity(𝑓3) = O (𝑁 ) and complexity(𝑓𝑖 ) =
O (1), ∀𝑖 ≥ 4. Next, we assign an indicator variable to indicate if ST is disabled along a certain face
– a face with disabled ST implies that the domain that corresponds to the face is non-empty and
therefore incurs cost in the objective. Take the face 𝑓1 for example, ST is disabled along this face
and incurs cost iff (cid:174)𝑟1 = [0, 0]⊤. We use 1(cid:174)𝑟𝑖 =(cid:174)0 ∈ {0, 1} to denote this indicator variable. Assuming
the ordering O (1) < O (𝑁 ) < O (𝑁 2), we may assign the integers 0, 1, 2 to each complexity term.
The maximum number of statements in the program is |𝑆 | = 9, and we use a base-|𝑆 | encoding for
summing the final objective. The final objective then becomes

∑︁

𝑖

|𝑆 |complexity( 𝑓𝑖 ) · 1(cid:174)𝑟𝑖 =(cid:174)0 = 92 · 1(cid:174)𝑟1=(cid:174)0 + 91 · 1(cid:174)𝑟2=(cid:174)0 + 91 · 1(cid:174)𝑟3=(cid:174)0 + 90 · 1(cid:174)𝑟4=(cid:174)0 + ...90 · 1(cid:174)𝑟8=(cid:174)0

(19)

5.4 Discussion
The above formulation is an integer objective bilinear constrained program. The objective is linear
because it is an affine combination of the indicator variables. The problem is bilinear constrained
because: in the original ILP formulation scheduling, the dependence matrix (defined in Section 3.3.1)
is multiplied by a vector of unknowns to form a linear constraint; however by introducing the
unknown reuse vectors (cid:174)𝑟𝑖 , the dependence matrix contains entries that depends on (cid:174)𝑟𝑖 , thereby
making the constraints bilinear.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:24

Cambridge Yang, Eric Atkinson, and Michael Carbin

6 SDR HEURISTIC ALGORITHM
The problem formulation we present in Section 5.3 is a full characterization of the SDR problem.
In this work we consider this formulation only as a specification instead of a complete solution
— solving an integer linear objective bilinear constrained program is NP-hard. The size of the
formulation (i.e. total number of constraints and number of variables) in Section 5.3 is proportional
to the number of statements, number of faces per statement and the maximal complexity of the
program – either one of which could potentially lead to exponential blow up in the size of the
formulation. Further, our formulation of dependence resolution is based on an ILP formulation of
multidimensional scheduling, which by itself already introduces a tractability challenge as pointed
out in Pouchet et al. [2011].

For these reasons, we propose here a sound heuristic solution to SDR. The key idea behind our
heuristic approach is that for a program that has an affine sequential schedule, 8 we can leverage
the schedule itself to choose a reuse vector for each ST that we apply to the program. Specifically,
for any reuse vector that is valid for a given face (according the constraints in Section 5.3), our
algorithm chooses either the reuse vector itself or its negation as the reuse vector for the ST. This
algorithm – though simple – is still optimal for reductions that have inverses – which spans a broad
class of programs – and always preserves the original dependences of the program.

6.1 Insights
The key insights of our algorithm are that 1) choosing any valid reuse vector for a given ST results in
the same final algorithmic complexity for the program (implied by Gautam and Rajopadhye [2006])
and 2) for any valid reuse vector, the direction itself or its negation adheres to a sequential schedule
of the left hand side of the reduction. We demonstrate these two insights with the following lemmas.

Lemma 6.1. For any application of ST, the complexity decrease is always the same regardless of the

actual choice of reuse vector.

Lemma 6.1 is implied by (though not enunciated in) Gautam and Rajopadhye [2006, Theorem 3
and Section 7]. We state Lemma 6.1 explicitly for the ease of its reference and give a self-contained
proof in Appendix D.

Before introducing the next lemma, we first introduce an extended definition of scheduling
functions. Recall that the scheduling function of a reduction statement is an affine function from
the reduction’s domain to the timestamp. We extend the context of a scheduling function from a
reduction statement to the left hand side of a reduction in a given program as follows. First the
program is augmented by adding to the program a new redirect statement A[ (cid:174)𝑥] = A′[ (cid:174)𝑥] with the
same domain as the domain of A, where A′ is a fresh symbol which replaces the left hand side array
A of the program. Then the scheduling function of the left hand side of the reduction is simply the
scheduling function of the newly introduced redirect statement of the left hand side in the schedule
of the augmented program.

Lemma 6.2. Given a sequential schedule for the augmented program, then for any ST application
on a reduction whose operator has an inverse and for any valid reuse vector (cid:174)𝑟 , either (cid:174)𝑟 or −(cid:174)𝑟 agrees
with the sequential schedule of the original program and does not introduce a dependence cycle.

Proof. Consider a reduction S with projection proj and left hand side array A. Suppose A has
an affine sequential schedule ΘA, then we have that ΘA · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ is the schedule time for A[ (cid:174)𝑥].
Let the vector (cid:174)𝑟 be in the same space as the domain of S, and we shift the domain of S along

8We rely on this for the soundness of our heuristic algorithm. We acknowledge that enforcing sequentiality may limit
available parallelism. In practice, our algorithm works with an affine sequential schedule computed by any algorithm.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:25

(cid:174)𝑟 ; let the projected vector of (cid:174)𝑟 onto the domain of A be (cid:174)𝑟A = proj((cid:174)𝑟 ). Consider (cid:174)𝑥 and (cid:174)𝑥 + (cid:174)𝑟A.
Their scheduled timestamps are ΘA · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ and ΘA · [ (cid:174)𝑥 + (cid:174)𝑟A, (cid:174)𝑝, 1]⊤, respectively. For all (cid:174)𝑥,
ΘA · [ (cid:174)𝑥 + (cid:174)𝑟A, (cid:174)𝑝, 1]⊤ − ΘA · [ (cid:174)𝑥, (cid:174)𝑝, 1]⊤ = ΘA · [ (cid:174)𝑟A, (cid:174)0, 0]⊤ is a constant that does not depend on (cid:174)𝑥. Since
ΘA is sequential, the difference ΘA · [ (cid:174)𝑟A, (cid:174)0, 0]⊤ is non-zero for all non-zero (cid:174)𝑟A. Therefore, A[ (cid:174)𝑥] is
scheduled either always before A[ (cid:174)𝑥 + (cid:174)𝑟A] or always after A[ (cid:174)𝑥 + (cid:174)𝑟A]. We describe these two cases in
detail as follows.

Case 1. When the first non-zero entry (in accordance with the timestamp comparison in Def-
inition 3.3) of ΘA · [ (cid:174)𝑟A, (cid:174)0, 0]⊤ is positive, then A[ (cid:174)𝑥] is always scheduled before A[ (cid:174)𝑥 + (cid:174)𝑟A]. In this
case, applying ST with the reuse vector (cid:174)𝑟 will not introduce any dependence cycle, since the newly
introduced dependence is always consistent with the original schedule.

Case 2. When the first non-zero entry of ΘA · [ (cid:174)𝑟A, (cid:174)0, 0]⊤ is negative, A[ (cid:174)𝑥] is always scheduled
after A[ (cid:174)𝑥 + (cid:174)𝑟A]. In this case, applying ST with the reuse vector −(cid:174)𝑟 will not introduce any dependence
cycle, for the same reason as in Case 1.

Further, since (cid:174)𝑟 chosen this way is always consistent with the original schedule, a previous
application of ST will not affect a later application of ST — intuitively, a previously applied ST
introduces a dependence that can be subsumed by an enforced dependence according to the original
program’s schedule; thus later a application of ST, as long as it is also consistent with original
□
schedule, will not be affected.

6.2 Algorithm
With justification in Section 6.1, we now introduce the heuristic algorithm in Figure 8.

(1) Schedule the augmented program to obtain an initial sequential schedule Θ for all

statements and left hand side of reductions

(2) Apply ST to all faces of all reduction statement’s domains; choose the direction that is

consistent with Θ by:

(a) First pick any valid reuse vector (cid:174)𝑟 from the candidate set.
(b) Test if (cid:174)𝑟 is consistent with Θ, if not consistent, set (cid:174)𝑟 ← −(cid:174)𝑟 , if −(cid:174)𝑟 is also a valid reuse

vector; otherwise, do not apply the current ST.

Fig. 8. SDR heuristic algorithm

To test if (cid:174)𝑟 is consistent with Θ, one can compute ΘA · [ (cid:174)𝑟A, (cid:174)0, 0]⊤, with ΘA and (cid:174)𝑟A defined as in

Lemma 6.2, and then test if the first non-zero entry is positive.

Example. Section 2 shows an example of the above heuristic algorithm. Specifically, the middle
polyhedron in Figure 2a illustrates the augmented redirect statement with the same domain as the
left hand side of the reduction (i.e. [𝑁 ] → {[𝑖] : 0 ≤ 𝑖 < 𝑁 }). A valid two dimensional sequential
schedule for the left hand side of the reduction is 𝜏B = ΘB·[𝑖, 𝑁 , 1]⊤ =
·[𝑖, 𝑁 , 1]⊤ = [1, 𝑖]⊤.
Then, for the reuse vector (cid:174)𝑟 = [1, 0]⊤, we have ΘB · (cid:174)𝑟B = ΘB · [1, 0, 0]⊤ = [0, 1]⊤. Since the first non-
zero entry of [0, 1]⊤ is positive, ST with the reuse vector [1, 0]⊤ will not introduce a dependence
cycle (i.e. consistent with the correct optimization in Section 2).

(cid:20)0
1

1
0

0
0

(cid:21)

6.3 Algorithm Analysis

Heuristic Scheduling. One advantage of the heuristic algorithm in Figure 8 is that the schedule Θ
does not need to be obtained from forming and solving the ILP formulation as in Section 3.3, and

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:26

Cambridge Yang, Eric Atkinson, and Michael Carbin

one is free to choose any scheduling algorithm in the polyhedral literature such as Bondhugula
et al. [2008]; Feautrier [1992a,b]; Gupta et al. [2007]. Most of these algorithms, such as the PLUTO
scheduler [Bondhugula et al. 2008], provide scalable solutions to the polyhedral scheduling problem
and thus the algorithm in Figure 8 does not present bottleneck due to scheduling.

Optimality Guarantee. The algorithm is optimal for the SDR problem if all the reduction operators
have inverses. This is because the algorithm considers a basis direction of reuse, and picks the
direction along that basis that is consistent with the original schedule. As long as all reduction
operators have inverses, the heuristic algorithm will assign a non-zero reuse vector to each face that
has valid reuse opportunities. In other words, the heuristic algorithm maximizes the total number of
ST applications among all faces, if all reduce operators have inverses. For any ST application along a
face, the complexity decrease is always the same regardless of the choice of reuse vector. Therefore,
maximizing the number of ST applications among all faces minimizes the total complexity.

Lastly, if a reduction operator does not have an inverse, thereby restricting the candidate set of
directions, then it is possible for our algorithm to produce a non-optimal solution. Specifically, if an
operator does not have an inverse, the valid reuse vector for that operator will be restricted to a one
sided direction (since ST requires the reuse vector to point out of certain boundaries of the domain
for a reduction if there is no inverse), instead of both directions of the basis. It is possible that the
original program does not have an unique valid schedule. Consider the following scenario: one
schedule is consistent with (cid:174)𝑟 , while another schedule is consistent with −(cid:174)𝑟 ; since the operator does
not have an inverse, only the positive (cid:174)𝑟 is valid. Therefore, the initial schedule will affect whether
this ST is applied or not – which in turn leads to the suboptimality of the algorithm.

7 IMPLEMENTATION
We implemented our IR as in Section 3.2 and the heuristic algorithm as in Section 6 using Python.
We used Integer Set Library (ISL) [Verdoolaege 2010] for manipulation of polyhedral sets and
relations. We use ISL’s scheduler and code generator and generate Cython code [Behnel et al. 2011],
which is then compiled to C++. To obtain an affine sequential schedule for the original augmented
program, we use a PLUTO-like scheduler [Bondhugula et al. 2008] built into ISL.

8 EVALUATION
The algorithm presented in this work is particularly effective on optimizing unoptimized imple-
mentations of probabilistic inference procedures into efficient implementations, where the inference
procedures have mathematical specifications that naturally translate to our IR. The inference pro-
cedures are also iterative, so they contain dependent reductions that are not addressed by previous
work [Gautam and Rajopadhye 2006].

Research Question. The goal of this section is to evaluate how effective the heuristic algorithm is

at improving the performance on benchmarks consisting of algorithms described above.

8.1 Evaluation Metrics
We considered the following two aspects to evaluate the effectiveness of the heuristic algorithm.

8.1.1 Complexity. We first evaluate asymptotic complexity of the algorithms. This is an important
metric because it determines how well these algorithms scale to large data sets, and hence how
widely they can be applied.

Optimality is defined regarding programs realizable through transformations presented in this
work. We evaluated our implementation in Section 7 using unoptimized implementations of prob-
abilistic inference procedures. We present their algorithmic complexities before and after opti-
mization, as well as the optimal complexities achievable with transformations in this work, by

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:27

solving the problem formulation in Section 5.3 exactly. In addition, we also report the complexities
of manual implementations using transformations that are not in this work.

We collected the complexities before and after by counting the cardinality of the resultant
polyhedral domains using library implementations in Verdoolaege et al. [2007]. We collected the
optimal complexities by inspecting the benchmarks and deriving the optimal complexities manually.
We collected complexities of manual implementations by either finding an existing implementation
of the algorithm if one exists in the literature or, otherwise, by manually deriving them.

8.1.2 Runtime. Note that a real performance gain is not necessarily implied by a complexity
improvement because the asymptotic complexity comparison ignores constant factors. The constant
factors can be caused by, for example, auxillary variables overhead or memory/cache effect induced
by ST, and the constant factors could change. Therefore, we further validate the runtime performance
gains due to lowered compliexites, by measuring wall-clock time improvements between the
optimized and unoptimized implementations.

Table 1. Benchmarks: Parameters and Meanings

𝑁
𝐾

𝑊
𝐾

LDA-*

GMM-*

DMM-*

𝑊
𝐾
𝐷
𝐴
𝐿

Benchmark(s)

Param Meaning

# of observations
# of clusters

# of words in corpus
# of topics

# of words in corpus
# of topics
# of documents in corpus
size of alphabet of corpus
max length of document

8.2 Benchmarks
A subset of the benchmark algorithms consid-
ered are identified as “model-algorithm” pairs,
where the model refers to a generative prob-
abilistic model, and the algorithm refers to a
class of algorithm to perform inference on the
model. We considered 3 models and 3 algo-
rithms. For models, we considered the Gaussian
Mixture Model (GMM) [Murphy 2012], Latent
Dirichlet Allocation (LDA) [Blei et al. 2003] and
Dirichlet Multinomial Mixture (DMM) [Holmes
et al. 2012]. For algorithms, we considered
Gibbs Sampling (GS) [Geman and Geman 1984],
Metropolis Hasting (MH) [Hastings 1970; Me-
tropolis et al. 1953] and Likelihood Weighting
(LW) [Fung and Chang 1989]. Thus our benchmarks contain a total of 9 “model-algorithm” pairs.
Models and algorithms above have broad applications in the literature. The models for LDA [Blei
et al. 2003; Griffiths and Steyvers 2004] and DMM [Holmes et al. 2012] are popular for existing data
science problems. The models for GMM [Daniel Huang 2017; Walia et al. 2019], LDA [Daniel Huang
2017; Walia et al. 2019], and DMM [Walia et al. 2019] have been used as benchmarks for probabilistic
inference systems. Gibbs sampling [Geman and Geman 1984], Metropolis-Hastings [Hastings 1970;
Metropolis et al. 1953], and Likelihood Weighting [Fung and Chang 1989] are all widely used
inference algorithms in the literature. LDA and DMM are particularly valuable benchmarks because
there are published Gibbs sampling algorithms that researchers have manually optimized (Griffiths
and Steyvers [2004] and Resnik and Hardisty [2010], respectively).

# of total pixels
max stereo displacement
# of neighbors of a pixel

# of observations
dim of a single observation

LBP-Stereo

CoxPH

𝑁
𝐾
𝐷

𝑁
𝐾

In addition, we included two other benchmarks, namely the Loopy Belief Propagation on a 2D
grid model for the application of Stereo matching (LBP-Stereo [Grauer-Gray and Cavazos 2011;
Jian Sun et al. 2003] ), and the Cox proportional hazards model (CoxPH) [Cox 1972; Therneau
2013]. Loopy Belief Propagation [Bishop 2006] is an iterative approximate inference algorithm, and
its instantiation on the 2D grid model has applications in fields such as vision [Grauer-Gray and
Cavazos 2011; Jian Sun et al. 2003] and physics [Kikuchi 1951]. CoxPH is a well known statistical
model, which is typically combined with Newton’s method, an iterative optimization algorithm, for

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:28

Cambridge Yang, Eric Atkinson, and Michael Carbin

Table 2. Benchmarks: Comparison of Complexities

Benchmark Original

Optimized (Heuristic)

SDR-Optimal

Manual

GMM-GS
O (𝑁 2𝐾 2)
GMM-MH
O (𝑁 (𝑁 + 𝐾))
GMM-LW O (𝑁 (𝑁 + 𝐾))
LDA-GS
LDA-MH
LDA-LW
DMM-GS
DMM-MH
DMM-LW O ((𝑊 𝐴 + 𝐾)𝐷)
LBP-Stereo O (𝑁 𝐾𝐷2)
CoxPH
O (𝐾 2𝑁 2)

O (𝑊 2𝐾 2)
O (𝑊 2𝐾)
O (𝑊 2𝐾)
O (𝑊 𝐴𝐷𝐾 2 + 𝐷2𝐾 2)
O (𝐷2𝐾 2 + 𝐷 (𝑊 + 𝐴)) O ((𝐾 + 𝑊 + 𝐴)𝐷)
O ((𝐾 + 𝑊 + 𝐴)𝐷)
O (𝑁 𝐾𝐷)
O (𝐾 2𝑁 )

O (𝑁 𝐾)
O (𝑁 )
O (𝑁 )
O (𝑊 𝐾)
O (𝑊 𝐾)
O (𝑊 )
O ((𝑊 + 𝐴)𝐾𝐷)

O (𝑁 𝐾)
O (𝑁 )
O (𝑁 )
O (𝑊 𝐾)
O (𝑊 𝐾)
O (𝑊 )
O (𝐴𝐾𝐷)

O (𝑁 𝐾)
O (𝑁 )
O (𝑁 )
O (𝑊 𝐾)
O (𝑊 𝐾)
O (𝑊 )
O ((𝑊 + 𝐴)𝐾𝐷)
O ((𝐾 + 𝑊 + 𝐴)𝐷) O ((𝐾 + 𝐿 + 𝐴)𝐷)
O ((𝐾 + 𝑊 + 𝐴)𝐷) O ((𝐾 + 𝐿 + 𝐴)𝐷)
O (𝑁 𝐾𝐷)
O (𝐾 2𝑁 )

O (𝑁 𝐾𝐷)
O (𝐾 2𝑁 )

inference on the model’s parameters. CoxPH is commonly found in medical applications [Collett
1993; White et al. 2016], and mechanical systems [Susto et al. 2015].

All of the benchmarks have a common feature that they are iterative methods specialized to a

generative probabilistic model. The parameters for these benchmarks are listed in Table 1.

8.3 Results
As described in Section 8.1, we first evaluated our method by analyzing the asymptotic perfor-
mance improvements and reported the results in Section 8.3.1. Then, we validated the runtime
improvements of the benchmarks in Section 8.3.2.

8.3.1 Complexity Results. Table 2 summarizes the results on comparison of complexities, expressed
in terms of the corresponding parameters of each benchmark.

The column “Original” gives the complexity of the original program for the benchmarks. The
column “Optimized (Heuristic)” (later abbreviated as “Optimized”) gives the complexity of the
transformed program using the heuristic implementation in Section 7. The column “SDR-Optimal”
gives the complexity of the transformed program by potentially solving the problem formulation
in Section 5.3 exactly 9; and this is the optimal complexity one can achieve using techniques
presented in this work. The column “Manual” gives the complexity of a potential optimized manual
implementation written by a developer; this means that the complexity reduction potentially comes
from transformations not present in this work.

Comparing the “Original” and “Optimized” columns, our approach reduces the complexities
for all benchmarks. Comparing the “Optimized” and “SDR-Optimal” columns, our heuristic ap-
proach produces complexities same as solving SDR optimally for all benchmarks. Comparing
the “Optimized” and “Manual” columns, our approach produces complexities the same as manual
implementations for 8 out of 11 benchmarks. We identified that the 3 benchmarks related to DMM
require additional data layout modifications which we did not consider in this work, which is a
direction of future research.

8.3.2 Runtime Validation. So far we have evaluated our heuristic algorithm using algorithmic
complexity as the primary factor, which ignores constant factors caused by, for example, auxillary
variables overhead or memory/cache effect induced by ST. In this section, we validate our hypothesis
that asymptotic complexity improvements dominates potential constant factors improvements for

9For all of our benchmarks, it turns out that our heuristic algorithm is able to assign a non-zero reuse vector to each
non-degenerate face (i.e. one whose cardinality is not O (1)). This implies that our heuristic algorithm’s solution is already
the optimal solution to SDR, since it does not have further complexity reduction opportunities. Therefore, through this
argument we obtain the SDR-optimal complexities without having to actually construct and solve the bilinear program.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:29

Table 3. Benchmarks: Parameter Sizes

Table 4. Runtime evaluations

Benchmark(s)

Parameter

Size

GMM-*

LDA-*

DMM-*

LBP-Stereo

CoxPH

𝑁
𝐾

𝑊
𝐾

𝑊
𝐾
𝐷
𝐴
𝐿

𝑁
𝐾
𝐷

𝑁
𝐾

10000
10
466, 000
50
570, 000
4
278
129
3202
110, 592
16
4

424
11

Benchmark Original

Optimized
(Heuristic)

Speedup

GMM-GS
GMM-MH
GMM-LW
LDA-GS
LDA-MH
LDA-LW
DMM-GS
DMM-MH
DMM-LW
LBP-Stereo
CoxPH

29.2 ms
0.94 ms
2.5 s
timeout
timeout
timeout
2.2 s
529 ms
71 s
16.9 s
69.1 ms

4.1 ms
0.72 ms
1.7 s
7.1 ms
0.69 s
41.1 s
0.54 s
14.3 ms
1.23 s
15.1 s
8.4 ms

7.1 x
1.3 x
1.5 x
> 6.1 × 106 x
> 63 × 103 x
> 1.1 × 103 x
4.1 x
37 x
58 x
1.1 x
8.2 x

the parameters of these benchmarks by timing our benchmarks and comparing the runtimes of
the naive implementations with the optimized implementations. We generate C++ code for the IR
programs corresponding to the Original and Optimized (Heuristic) columns in Table 2 . We ran
these implementations and report timeouts for benchmarks that ran for 12+ hours.

Parameter Sizes. We collected the typical instantiated values for global parameters from the
corresponding literature. Specifically, for GMM we use Daniel Huang [2017], for LDA we use
Newman [2008], for DMM we use Turnbaugh et al. [2008], for LBP-Stereo we use Jian Sun et al.
[2003] and for CoxPH we use Therneau [2013, Appendix D2]. Based on these prior works, we
collected the following parameters for each model in Table 3.

Results. In Table 4, all benchmarks have non-trivial speedups. In particular, for LDA benchmarks
all the unoptimized implementations timeout. This is because, in Table 2, the complexities of
LDA benchmarks all improve by a factor of 𝑊 × 𝐾. With our instantiated parameter values this
factor is 466, 000 × 50 = 2.3 × 107 – the largest factor across all benchmarks. This large a factor
unsurprisingly leads to the timeout of the unoptimzied implementations. On the other hand, in
Table 2 LBP-Stereo’s complexity only improves by a factor of 𝐷, the number of neighbors of a pixel,
which is set to 4 (i.e. number immediate neighbors of a pixel) in our parameter setting. Nonetheless,
we observe a speedup of 1.1x for this benchmark. We also note that this speedup scales with the
specification of the LBP model – setting to 8 neighboring pixels (i.e. nearby 8 pixels for a pixel at the
center of a 3 by 3 square) would lead to speed up of of 1.4x. In sum, the observed speedups validates
that for these benchmarks and our technique, complexity dominates constant-factor concerns.

9 RELATED WORK
Reductions Doerfert et al. [2015]; Ginsbach and O’Boyle [2017] introduced techniques to detect
reductions from loop based code; these techniques can be used as front-ends to our technique
for conversion into our reduction based IR. Doerfert et al. [2015]; Ginsbach and O’Boyle [2017];
Rauchwerger and Padua [1999]; Reddy et al. [2016] optimized reductions in the polyhedral model
for considerations such as privatization and parallelization. They do not optimize for complexities;
however, they can be used as optimizing backends for generating efficient code for reductions
after applying our method. Iooss et al. [2014] proposed a semi-algorithm that decides equivalence
between programs with reductions; it can be used to check correctness of a program transformation.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:30

Cambridge Yang, Eric Atkinson, and Michael Carbin

Simplifying Reductions Liu et al. [2005] introduced a loop based transformation algorithm for
reducing complexities on loop programs. The algorithm uses the Omega calculator [Padua 2011]
for analysis on a contributing set. The contributing set based anaylsis is general in the sense that it
works for non-polyhedral sets as well. Liu et al. [2005] used only the direction of loop increment
to decrease the complexity. Gautam and Rajopadhye [2006] generalized the method in Liu et al.
[2005] and introduced the simplifying reductions framework; the framework is more general in
that it uses reuse vectors not limited to just the direction of loop increments.
Scheduling We review related works on polyhedral scheduling in Appendix E.1.
Program Complexity Rubiano [2017] and Alias et al. [2010] studied methods to infer the com-
plexity of programs. These works can reduce the effort for manually analyzing the hand optimized
benchmarks, which is particularly helpful for enlarging the benchmark suite.
Incrementalization in Probabilistic Programming The problem of incrementalization occurs
in probabilistic programming system (PPS), and is known as incrementalized inference. Existing
work [Kiselyov 2016; Nori et al. 2015; Ritchie et al. 2016; Wu et al. 2016; Yang et al. 2014; Zhang and
Xue 2019] attempted to address the problem of incrementalized inference in PPS. However, these
techniques are variants/combinations of 1) tracing JITs, 2) specialization and caching/memoization,
3) dynamic dependence analysis, 4) dynamic program slicing, or 5) runtime symbolic analysis –
in summary, dynamic optimizations. These techniques introduce significant runtime overhead
for storing dependence graph/traces (which is of size proportional to the number of the executed
statement instances) and/or performing analysis on those graphs/traces dynamically. Our technique
can be applied to PPS to solve the incrementalized inference problem; however, our technique is a
static compilation techinique which do not suffer from runtime overhead.
Many existing and ongoing work [Atkinson et al. 2018; Bingham et al. 2019; Daniel Huang 2017;
Goodman et al. 2008; Goodman and Stuhlmüller 2014; Mansingkha et al. 2018; Narayanan et al.
2016; Plummer 2015] allow the user to code in high level DSLs. Though the details on these systems
are out of the scope of the paper, our method can be potentially integrated into these systems for
generating code with efficient complexity, which we discuss more in Appendix E.2.

10 CONCLUSION
In this work, we introduce the simplifying dependent reductions problem and provide a heuristic
algorithm that — given an affine sequential schedule for the program — is optimal for reduc-
tion operators that have inverses. These reductions have otherwise only appeared as domain- or
algorithm-specific optimizations as described in the published description of standard probabilistic
inference algorithms. Our hope is that this work formally outlines a key general-purpose opti-
mization opportunity that can be delegated to the compiler, rather than being a significant piece
of manual implementation that stands between the elaboration of a new probabilistic inference
algorithm and its high-performance implementation. Our results hold the promise that emerging
languages and systems for this increasingly important class of computations could see significant
performance improvements by incorporating our techniques.

ACKNOWLEDGMENTS
We would like to thank Alex Renda, Charith Mandis, Jesse Michel, Jonathan Frankle, Riyadh
Baghdadi, Sanjay Rajopadhye, Sriram Krishnamoorthy, Tian Jin, and anonymous reviewers for
their helpful comments and suggestions. This work was supported in part by the Office of Naval
Research (ONR-N00014-17-1-2699). Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author and do not necessarily reflect the views of the
Office of Naval Research.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:31

REFERENCES
Christophe Alias, Alain Darte, Paul Feautrier, and Laure Gonnord. 2010. Multi-dimensional Rankings, Program Termination,
and Complexity Bounds of Flowchart Programs. In Static Analysis Symposium. https://hal.inria.fr/inria-00523298
Eric Atkinson, Cambridge Yang, and Michael Carbin. 2018. Verifying Handcoded Probabilistic Inference Procedures. In

arXiv e-prints.

Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Dag Sverre Seljebotn, and Kurt Smith. 2011. Cython: The

Best of Both Worlds. Computing in Science and Engg. 13, 2 (March 2011), 31–39.

Mohamed-Walid Benabderrahmane, Louis-Noël Pouchet, Albert Cohen, and Cédric Bastoul. 2010. The Polyhedral Model
is More Widely Applicable Than You Think. In European Conference on Theory and Practice of Software, International
Conference on Compiler Construction.

Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh,
Paul Szerlip, Paul Horsfall, and Noah D. Goodman. 2019. Pyro: Deep Universal Probabilistic Programming. Journal of
Machine Learning Research 20, 28 (2019).

Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag,

Berlin, Heidelberg.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research

3 (Jan. 2003), 993–1022.

Uday Bondhugula, Albert Hartono, J. Ramanujam, and P. Sadayappan. 2008. A Practical Automatic Polyhedral Parallelizer

and Locality Optimizer. In Conference on Programming Language Design and Implementation.

Jean-François Collard, Denis Barthou, and Paul Feautrier. 1995. Fuzzy Array Dataflow Analysis. In Symposium on Principles

and Practice of Parallel Programming.

D Collett. 1993. Modelling Survival Data in Medical Research. Chapman & Hall, New York.
D. R. Cox. 1972. Regression Models and Life-Tables. Journal of the Royal Statistical Society: Series B (Methodological) 34, 2

(1972), 187–202.

Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. 2019. Gen: a General-purpose
Probabilistic Programming System with Programmable Inference. In Conference on Programming Language Design and
Implementation.

Greg Morisett Daniel Huang, Jean-Baptiste Tristan. 2017. Compiling Markov Chain Monte Carlo Algorithms for Probabilistic

Modeling. In Conference on Programming Language Design and Implementation.

Johannes Doerfert, Kevin Streit, Sebastian Hack, and Zino Benaissa. 2015. Polly’s Polyhedral Scheduling in the Presence of

Reductions. In International Workshop on Polyhedral Compilation Techniques.

E. Ehrhardt. 1967. Sur un problème de Géométrie Diophantienne Linéaire. II. Journal für die Reine und Angewandte

Mathematik 1967 (1967), 25–49. Issue 227.

P. Feautrier. 1988. Array Expansion. In International Conference on Supercomputing.
Paul Feautrier. 1992a. Some efficient solutions to the affine scheduling problem. I. One-dimensional time. International

Journal of Parallel Programming 21, 5 (Oct. 1992), 313–347.

Paul Feautrier. 1992b. Some efficient solutions to the affine scheduling problem. Part II. Multidimensional time. International

Journal of Parallel Programming 21, 6 (Dec. 1992), 389–420.

Robert M. Fung and Kuo-Chu Chang. 1989. Weighing and Integrating Evidence for Stochastic Simulation on Bayesian

Networks. In Conference on Uncertainty in Artificial Intelligence.

Gautam and S. Rajopadhye. 2006. Simplifying Reductions. In Symposium on Principles of Programming Languages.
Andrew Gelman, Daniel Lee, and Jiqiang Guo. 2015. Stan: A probabilistic programming language for Bayesian inference

and optimization. Journal of Educational and Behavioral Statistics 40, 5 (2015), 530–543.

Stuart Geman and Donald Geman. 1984. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of images.

Transactions on Pattern Analysis and Machine Intelligence 6 (Nov. 1984), 721–741. Issue 6.

Philip Ginsbach and Michael F. P. O’Boyle. 2017. Discovery and Exploitation of General Reductions: A Constraint Based

Approach. In International Symposium on Code Generation and Optimization.

Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. 2008. Church: A

language for generative models. In Conference on Uncertainty in Artificial Intelligence.

Noah D Goodman and Andreas Stuhlmüller. 2014. The Design and Implementation of Probabilistic Programming Languages.

http://dippl.org. Accessed: 2020-10-30.

Scott Grauer-Gray and John Cavazos. 2011. Optimizing and Auto-tuning Belief Propagation on the GPU. In Workshop on

Languages and Compilers for Parallel Computing.

T. Griffiths and M. Steyvers. 2004. Finding Scientific Topics. Proceedings of the National Academy of Sciences 101, suppl. 1

(April 2004), 5228–5235.

Gautam Gupta, Kim Daegon, and Sanjay Rajopadhye. 2007. Scheduling in the Z-Polyhedral Model. International Parallel

and Distributed Processing Symposium.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

20:32

Cambridge Yang, Eric Atkinson, and Michael Carbin

Gautam Gupta, Sanjay Rajopadhye, and Patrice Quinton. 2002. Scheduling Reductions on Realistic Machines. In Symposium

on Parallel Algorithms and Architectures.

W. K. Hastings. 1970. Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika 57, 1 (April

1970), 97–109.

Ian Holmes, Keith Harris, and Christopher Quince. 2012. Dirichlet Multinomial Mixtures: Generative Models for Microbial

Metagenomics. PLOS ONE 7 (2 2012). Issue 2.

Guillaume Iooss, Christophe Alias, and Sanjay Rajopadhye. 2014. On Program Equivalence with Reductions. In International

Static Analysis Symposium.

Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum. 2003. Stereo matching using belief propagation. Transactions on Pattern

Analysis and Machine Intelligence 25, 7 (July 2003), 787–800.

Ryoichi Kikuchi. 1951. A Theory of Cooperative Phenomena. Physical Review 81 (March 1951), 988–1003. Issue 6.
Oleg Kiselyov. 2016. Probabilistic Programming Language and its Incremental Evaluation. In Asian Symposium on Program-

ming Languages and Systems.

Jun S. Liu. 1994. The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene Regulation Problem.

J. Amer. Statist. Assoc. 89, 427 (Sept. 1994), 958–966.

Yanhong A. Liu, Scott D. Stoller, Ning Li, and Tom Rothamel. 2005. Optimizing Aggregate Array Computations in Loops.

ACM Transactions on Programming Languages and Systems 27, 1 (Jan. 2005), 91–125.

Vikash Mansingkha, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin Rinard. 2018. Probabilistic
Programming with Programmable Inference. In Conference on Programming Language Design and Implementation.
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. 1953. Equation of State Calculations by Fast

Computing Machines. Journal of Chemical Physics 21, 6 (1953), 1087–1092.

Kevin P. Murphy. 2012. Machine Learning: A Probabilistic Perspective. MIT Press, Cambridge, Massachusets.
Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. 2016. Probabilistic Inference
by Program Transformation in Hakaru (System Description). In International Symposium on Functional and Logic
Programming.

George L. Nemhauser and Laurence A. Wolsey. 1988. Integer and Combinatorial Optimization.
David Newman. 2008. Bag of Words Dataset. In UCI Machine Learning Respository.
Aditya V. Nori, Sherjil Ozair, Sriram K. Rajamani, and Deepak Vijaykeerthy. 2015. Efficient Synthesis of Probabilistic

Programs. In Conference on Programming Language Design and Implementation.

David Padua (Ed.). 2011. Omega Calculator. Springer US, Boston, MA, 1355–1355. https://doi.org/10.1007/978-0-387-09766-

4_2303

Martyn Plummer. 2015. JAGS Version 4.0.0 user manual. Addison-Wesley, Reading, Massachusetts.
Louis-Noël Pouchet, Cédric Bastoul, Albert Cohen, and John Cavazos. 2008. Iterative optimization in the polyhedral model:

Part II, multidimensional time. In Conference on Programming Language Design and Implementation.

Louis-Noël Pouchet, Cédric Bastoul, Albert Cohen, and Nicolas Vasilache. 2007. Iterative optimization in the polyhedral

model: Part I, one-dimensional time. In International Symposium on Code Generation and Optimization.

Louis-Noël Pouchet, Uday Bondhugula, Cédric Bastoul, Albert Cohen, J. Ramanujam, P. Sadayappan, and Nicolas Vasi-
lache. 2011. Loop Transformations: Convexity, Pruning and Optimization. In Symposium on Principles of Programming
Languages.

L. Rauchwerger and D. A. Padua. 1999. The LRPD test: speculative run-time parallelization of loops with privatization and

reduction parallelization. Transactions on Parallel and Distributed Systems 10, 2 (Feb. 1999), 160–180.

C. Reddy, M. Kruse, and A. Cohen. 2016. Reduction drawing: Language constructs and polyhedral compilation for reductions

on GPUs. In International Conference on Parallel Architecture and Compilation Techniques.

Xavier Redon and Paul Feautrier. 1994. Scheduling Reductions. In International Conference on Supercomputing.
Philip Resnik and Eric Hardisty. 2010. Gibbs Sampling for the Uninitiated. Technical Report. University of Maryland Institute

of Advanced Computer Studies.

Daniel Ritchie, Andreas Stuhlmüller, and Noah Goodman. 2016. C3: Lightweight Incrementalized MCMC for Probabilistic
Programs using Continuations and Callsite Caching. In International Conference on Artificial Intelligence and Statistics.

Thomas Rubiano. 2017. Implicit Computational Complexity and Compilers. Ph.D. Dissertation.
Yannick Saouter and Patrice Quinton. 1993. Computability of Recurrence Equations. Theoretical Computer Science 116, 2

(Aug. 1993), 317–337.

Alexander Schrijver. 1986. Theory of Linear and Integer Programming. John Wiley & Sons, Inc., New York, NY, USA.
G. A. Susto, A. Schirru, S. Pampuri, S. McLoone, and A. Beghi. 2015. Machine Learning for Predictive Maintenance: A

Multiple Classifier Approach. Transactions on Industrial Informatics 11, 3 (June 2015), 812–820.

Patricia M Therneau, Terry M.;Grambsch. 2013. Modeling Survival Data: Extending the Cox Model. Springer, New York.
Dustin Tran, Matthew D Hoffman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M Blei. 2017. Deep probabilistic

programming. In International Conference on Learning Representations.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

Simplifying Dependent Reductions in the Polyhedral Model

20:33

Peter Turnbaugh, Micah Hamady, Tanya Yatsunenko, Brandi Cantarel, Alexis Duncan, Ruth Ley, Mitchell Sogin, Joe Jones,
Bruce A Roe, Jason Affourtit, Michael Egholm, Bernard Henrissat, Andrew C Heath, Rob Knight, and Jeffrey I Gordon.
2008. A core gut microbiome in obese and lean twins. Nature 457 (12 2008), 480–4.

Sven Verdoolaege. 2010. isl: An Integer Set Library for the Polyhedral Model. In International Congress on Mathematical

Software.

Sven Verdoolaege. 2016. Presburger Formulas and Polyhedral Compilation. https://doi.org/10.13140/RG.2.1.1174.6323
Sven Verdoolaege, Hristo Nikolov, and Todor Stefanov. 2013. On Demand Parametric Array Dataflow Analysis. In Interna-

tional Workshop on Polyhedral Compilation Techniques.

Sven Verdoolaege, Rachid Seghir, Kristof Beyls, Vincent Loechner, and Maurice Bruynooghe. 2007. Counting Integer Points

in Parametric Polytopes Using Barvinok’s Rational Functions. Algorithmica 48, 1 (May 2007), 37–66.

Rajan Walia, Praveen Narayanan, Jacques Carette, Sam Tobin-Hochstadt, and Chung-chieh Shan. 2019. From High-level

Inference Algorithms to Efficient Code. In International Conference on Functional Programming.

Nicola White, Fiona Reid, Adam Harris, Priscilla Harries, and Patrick Stone. 2016. A Systematic Review of Predictions of
Survival in Palliative Care: How Accurate Are Clinicians and Who Are the Experts? PLOS ONE 11, 8 (08 2016), 1–20.

Yi Wu, Lei Li, Stuart Russell, and Rastislav Bodik. 2016. In International Joint Conferences on Artificial Intelligence.
Lingfeng Yang, Patrick Hanrahan, and Noah Goodman. 2014. Generating Efficient MCMC Kernels from Probabilistic

Programs. In International Conference on Artificial Intelligence and Statistics.

Tomofumi Yuki, Gautam Gupta, DaeGon Kim, Tanveer Pathan, and Sanjay Rajopadhye. 2013. AlphaZ: A System for Design

Space Exploration in the Polyhedral Model. In Workshop on Languages and Compilers for Parallel Computing.

Jieyuan Zhang and Jingling Xue. 2019. Incremental Precision-Preserving Symbolic Inference for Probabilistic Programs. In

Conference on Programming Language Design and Implementation.

Proc. ACM Program. Lang., Vol. 5, No. POPL, Article 20. Publication date: January 2021.

