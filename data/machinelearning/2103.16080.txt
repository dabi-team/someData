Geometry of Program Synthesis

James Clift * 1 Daniel Murfet * 2 James Wallbridge * 3

Abstract
We re-evaluate universal computation based on
the synthesis of Turing machines. This leads to a
view of programs as singularities of analytic va-
rieties or, equivalently, as phases of the Bayesian
posterior of a synthesis problem. This new point
of view reveals unexplored directions of research
in program synthesis, of which neural networks
are a subset, for example in relation to phase tran-
sitions, complexity and generalisation. We also
lay the empirical foundations for these new direc-
tions by reporting on our implementation in code
of some simple experiments.

1. Introduction

The challenge to efﬁciently learn arbitrary computable func-
tions, that is, synthesise universal computation, seems a
prerequisite to build truly intelligent systems. There is
strong evidence that neural networks are necessary but not
sufﬁcient for this task (Zador, 2019; Akhlaghpour, 2020).
Indeed, those functions efﬁciently learnt by a neural net-
work are those that are continuous on a compact domain
and that process memory in a rather primitive manner. This
is a rather small subset of the complete set of computable
functions, for example, those computable by a Universal
Turing Machine. This suggests an opportunity to revisit the
basic conceptual framework of modern machine learning by
synthesising Turing machines themselves and re-evaluating
our concept of programs. We will show that this leads to
new unexplored directions for program synthesis.

1
2
0
2

r
a

M
0
3

]

G
L
.
s
c
[

1
v
0
8
0
6
1
.
3
0
1
2
:
v
i
X
r
a

In the classical model of computation a program is a code
for a Universal Turing Machine (UTM). As these codes are
discrete, there is no nontrivial “space” of programs. How-
ever this may be a limitation in the classical model, and not
a fundamental feature of computation. It is worth bearing
in mind that, in reality, there is no such thing as a perfect
logical state: in any implementation of a UTM in matter a
∗ is realised by an equilibrium state of some
code w
0, 1
}
*Author list is alphabetical.

1jamesedwardclift@gmail.com
2Department of Mathematics, University of Melbourne.
d.murfet@unimelb.edu.au 3james.wallbridge@gmail.com.

∈ {

physical system (say a magnetic memory). When we say
the code on the description tape of the physical UTM “is”
w what we actually mean is, adopting the thermodynamic
language, that the system is in a phase (a local minima of
the free energy) including the microstate c we associate to
w. However, when the system is in this phase its microstate
is not equal to c but rather undergoes rapid spontaneous
transitions between many microstates “near” c.

So in any possible physical realisation of a UTM, a program
is realised by a phase of the physical system. Does this have
any computational signiﬁcance?

One might object that this is physics and not computer sci-
ence. But this boundary is not so easily drawn: it is unclear,
given a particular physical realisation of a UTM, which of
the physical characteristics are incidental and which are
essentially related to computation, as such. In his original
paper Turing highlighted locality in space and time, clearly
inspired by the locality of humans operating with eyes, pens
and paper, as essential features of his model of computa-
tion. Other physical characteristics are left out and hence
implicitly regarded as incidental.

In recent years there has been a renewed effort to synthe-
sise programs by gradient descent (Neelakantan et al., 2016;
Kaiser & Sutskever, 2016; Bunel et al., 2016; Gaunt et al.,
2016; Evans & Grefenstette, 2018; Chen et al., 2018). How-
ever, there appear to be serious obstacles to this effort stem-
ming from the singular nature of the loss landscape. When
taken seriously at a mathematical level these efforts necessar-
ily come up against this question of programs versus phases.
When placed within the correct mathematical setting of
singular learning theory (Watanabe, 2009), a clear picture
arises of programs as singularities which complements the
physical interpretation of phases. The loss function is given
as the KL-divergence, an analytic function on the manifold
of learnable weights, which corresponds to the microscopic
Hamiltonian of the physical system. The Bayesian poste-
rior corresponds to the Boltzmann distribution and we see
emerging a dictionary between singular learning theory and
statistical mechanics.

The purpose of this article is to (a) explain a formal math-
ematical setting in which it can be explained why this is
the case (b) show how to explore this setting experimen-
tally in code and (c) sketch the new avenues of research in

 
 
 
 
 
 
Geometry of Program Synthesis

program synthesis that this point of view opens up. The
conclusion is that computation should be understood within
the framework of singular learning theory with its associ-
ated statistical mechanical interpretation which provides a
new range of tools to study the learning process, including
generalisation and complexity.

1.1. Contributions

−→

Any approach to program synthesis by gradient descent can
be formulated mathematically using some kind of mani-
fold W containing a discrete subset W code of codes, and
R whose set
equipped with a smooth function K : W
W code are precisely the
of zeros W0 are such that W0 ∩
solutions of the synthesis problem. The synthesis process
consists of gradient descent with respect to
K to ﬁnd a
point of W0, followed perhaps by some secondary process
W code. We refer to points of W0 as
to ﬁnd a point of W0 ∩
W code as classical solutions.
solutions and points of W0 ∩
We construct a prototypical example of such a setup, where
W code is a set of codes for a speciﬁc UTM with a ﬁxed
length, W is a space of sequences of probability distribu-
tions over code symbols, and K is the Kullback-Leibler
divergence. The construction of this prototypical example is
interesting in its own right, since it puts program synthesis
formally in the setting of singular learning theory.

∇

In more detail:

• We deﬁne a smooth relaxation of a UTM (Appendix G)
based on (Clift & Murfet, 2018) which is well-suited
to experiments for conﬁrming theoretical statements in
singular learning theory. Propagating uncertainty about
the code through this UTM deﬁnes a triple (p, q, ϕ) in
the sense of singular learning theory associated to a
synthesis problem. This formally embeds program
synthesis within singular learning theory.

• We realise this embedding in code by providing an
implementation in PyTorch of this propagation of un-
certainty through a UTM. Using the No-U-Turn variant
of MCMC (Hoffman & Gelman, 2014) we can approx-
imate the Bayesian posterior of any program synthesis
problem (computational constraints permitting).

• We explain how the real log canonical threshold
(RLCT), the key geometric invariant from singular
learning theory, is related to Kolmogorov complexity
(Section 4) and provide a novel thermodynamic inter-
pretation (Section 6).

• We show that a MCMC-based approach to program
synthesis will ﬁnd, with high probability, a solution
that is of low complexity (if it ﬁnds a solution at all)
and sketch a novel point of view on the problem of

“bad local minima” (Gaunt et al., 2016) based on these
ideas (Appendix A).

• We give a simple example (Appendix E) in which W0
contains the set of classical solutions as a proper subset
and every point of W0 is a degenerate critical point of
K, demonstrating the singular nature of the synthesis
problem.

• For two simple synthesis problems detectA and
parityCheck we demonstrate all of the above, us-
ing MCMC to approximate the Bayesian posterior and
theorems from (Watanabe, 2013) to estimate the RLCT
(Section 7). We discuss how W0 is an extended object
and how the RLCT relates to the local dimension of
W0 near a classical solution.

2. Preliminaries

We use Turing machines, but mutatis mutandis everything
applies to other programming languages. Let T be a Turing
machine with tape alphabet Σ and set of states Q and assume
Σ∗ the machine eventually halts with
that on any input x
Σ∗. Then to the machine T we may associate
output T (x)
Σ∗. Program synthesis is
the set
⊆
the study of the inverse problem: given a subset of Σ∗
Σ∗
we would like to determine (if possible) a Turing machine
which computes the given outputs on the given inputs.

∈
(x, T (x))
{

}x∈Σ∗

Σ∗

×

×

∈

×

If we presume given a probability distribution q(x) on Σ∗
then we can formulate this as a problem of statistical infer-
Σ∗
ence: given a probability distribution q(x, y) on Σ∗
determine the most likely machine producing the observed
distribution q(x, y) = q(y
x)q(x). If we ﬁx a universal Tur-
|
then Turing machines can be parametrised
ing machine
U
Σ∗.
W code with
by codes w
U
∈
We let p(y
x, w) denote the probability of
(x, w) = y
(which is either zero or one) so that solutions to the synthe-
sis problem are in bijection with the zeros of the Kullback-
Leibler divergence K(w) between the true distribution and
the model. So far this is just a trivial rephrasing of the com-
binatorial optimisation problem of ﬁnding a Turing machine
T with T (x) = y for all (x, y) with q(x, y) > 0.

(x, w) = T (x) for all x

∈

U

|

⊇

→

Smooth relaxation. One approach is to seek a smooth
relaxation of the synthesis problem consisting of an analytic
W code and an extension of K to an analytic
manifold W
R so that we can search for the zeros
function K : W
of K using gradient descent. Perhaps the most natural
way to construct such a smooth relaxation is to take W
to be a space of probability distributions over W code and
prescribe a model p(y
x, w) for propagating uncertainty
|
about codes to uncertainty about outputs (Gaunt et al., 2016;
Evans & Grefenstette, 2018). Supposing that such a smooth
relaxation has been chosen together with a prior ϕ(w) over
W , smooth program synthesis becomes the study of the

Geometry of Program Synthesis

statistical learning theory of the triple (p, q, ϕ).

There are at least two reasons to consider the smooth re-
laxation. Firstly, one might hope that stochastic gradient
descent or techniques like Markov chain Monte Carlo will
be effective means of solving the original combinatorial
optimisation problem. This is not a new idea (Gulwani et al.,
2017, §6) but so far its effectiveness for large programs has
not been proven. Independently, one might hope to ﬁnd
powerful new mathematical ideas that apply to the relaxed
problem and shed light on the nature of program synthesis.

Singular learning theory. All known approaches to pro-
gram synthesis can be formulated in terms of a singular
learning problem. Singular learning theory is the extension
of statistical learning theory to account for the fact that the
set of true parameters has in general the structure of an ana-
lytic space as opposed to an analytic manifold (Watanabe,
2007; 2009). It is organised around triples (p, q, ϕ) con-
sisting of a class of models
, a true
distribution q(y

x, w) : w

p(y
{

W

∈

}

|

x) and a prior ϕ on W .
|

Given a triple arising as above from a smooth relaxation
(see Section 3 for the key example) the Kullback-Leibler
divergence between the true distribution and the model is

K(w) := DKL(q

p) =

(cid:107)

q(y

x)q(x) log
|

(cid:90) (cid:90)

We call points of W0 =
|
the synthesis problem and note that

W

w

∈

{

K(w) = 0
}

p(y

q(y

dxdy

x)
|
x, w)
|
solutions of

W code

(1)

⊆

W

W0 ∩

W0 ⊆
W code is the discrete set of solutions to the
where W0 ∩
original synthesis problem. We refer to these as the classical
solutions. As the vanishing locus of an analytic function,
W0 is an analytic space over R (Hironaka, 1964, §0.1),
(Grifﬁth & Harris, 1978) and except in trivial cases it is not
an analytic manifold.

∈

W is a critical point of K if

The set of solutions fails to be a manifold due to singularities.
We say w
K(w) = 0 and
a singularity of the function K if it is a critical point where
K(w) = 0. Since K is a Kullback-Leibler divergence it
is non-negative and so it not only vanishes on W0 but
K
also vanishes, hence every point of W0 is a singular point.
Thus programs to be synthesised are singularities of analytic
functions.

∇

∇

The geometry of W0 depends on the particular model
x, w) that has been chosen, but some aspects are uni-
p(y
|
versal: the nature of program synthesis means that typically
W0 is an extended object (i.e. it contains points other than
the classical solutions) and the Hessian matrix of second
order partial derivatives of K at a classical solution is not
invertible - that is, the classical solutions are degenerate
critical points of K. This means that singularity theory is

the appropriate branch of mathematics for studying the ge-
ometry of W0 near a classical solution. It also means that
the Fisher information matrix is degenerate at a classical
solution, so that the appropriate branch of statistical learning
theory is singular learning theory (Watanabe, 2007; 2009).
For an introduction to singular learning theory in the context
of deep learning see (Murfet et al., 2020).

|

(cid:107)

x, Dn) = (cid:82) p(y

x, w)p(w
|
|
p∗) is given in expectation by E[B∗

This geometry has important consequences for all synthesis
n
tasks. In particular, given a dataset Dn =
(xi, yi)
i=1
}
{
of input-output pairs from q(x, y) and a predictive distribu-
tion p∗(y
Dn)dw, the asymp-
totic form of the Bayes generalization error Bg(n) :=
g ] = λ where λ
DKL(q
is a rational number called the real log canonical threshold
(RLCT) (Watanabe, 2009). Therefore, the RLCT is the key
geometric invariant which should be estimated to determine
the generalization capacity of program synthesis. We also
show how this number measures the model complexity of
program synthesis by relating it to the Kolmogorov com-
plexity. One may interpret the RLCT as a reﬁned measure
of complexity.

The synthesis process. Synthesis is a problem because
we do not assume that the true distribution is known: for
x) is deterministic and the associated func-
example, if q(y
|
tion is f : Σ∗
Q, we assume that some example pairs
→
(x, f (x)) are known but no general algorithm for comput-
ing f is known (if it were, synthesis would have already
been performed). In practice synthesis starts with a sample
n
i=1 from q(x, y) with associated empirical
Dn =
(xi, yi)
}
{
Kullback-Leibler distance

Kn(w) =

1
n

n
(cid:88)

i=1

log

q(yi|
p(yi|

xi)
xi, w)

.

(2)

W code
If the synthesis problem is deterministic and u
then Kn(u) = 0 if and only if u explains the data in the
sense that stept(xi, u) = yi for 1
n. We now review
two natural ways of ﬁnding such solutions in the context of
machine learning.

≤

≤

∈

i

Synthesis by stochastic gradient descent (SGD). The ﬁrst
approach is to view the process of program synthesis as
R.
stochastic gradient descent for the function K : W
We view Dn as a large training set and further sample sub-
n and compute
sets Dm with m
Km to take gradient
Km(wi) for some learning
descent steps wi+1 = wi −
η
∇
rate η. Stochastic gradient descent has the advantage (in
principle) of scaling to high-dimensional parameter spaces
W , but in practice it is challenging to use gradient descent
to ﬁnd points of W0 (Gaunt et al., 2016).

(cid:28)

→

∇

Synthesis by sampling. The second approach is to consider
the Bayesian posterior associated to the synthesis problem,
which can be viewed as an update on the prior distribution

Geometry of Program Synthesis

ϕ after seeing Dn

p(w

Dn) =
|

p(Dn|

w)p(w)

p(Dn)

=

1
Zn

ϕ(w)

n
(cid:89)

i=1

p(yi|

xi, w)

}

=

(3)

exp

nKn(w) + log ϕ(w)

1
Z 0
{−
n
n = (cid:82) ϕ(w) exp(
where Z 0
nKn(w))dw. If n is large the
−
posterior distribution concentrates around solutions w
W0
and so sampling from the posterior will tend to produce
machines that are (nearly) solutions. The gold standard
sampling is Markov Chain Monte Carlo (MCMC). Scal-
ing MCMC to where W is high-dimensional is a challeng-
ing task with many attempts to bridge the gap with SGD
(Welling & Teh, 2011; Chen et al., 2014; Ding et al., 2014;
Zhang et al., 2020). Nonetheless in simple cases we demon-
strate experimentally in Section 7 that machines may be
synthesised by using MCMC to sample from the posterior.

∈

3. Program Synthesis as Singular Learning

To demonstrate how program synthesis may be formalised
in the context of singular learning theory we construct a very
general synthesis model that can in principle synthesise ar-
bitrary computable functions from input-output examples.
This is done by propagating uncertaintly through the weights
of a smooth relaxation of a universal Turing machine (UTM)
(Clift & Murfet, 2018). Related approaches to program syn-
thesis paying particular attention to model complexity have
been given in (Schmidhuber, 1997; Hutter, 2004; Gaunt
et al., 2016; Freer et al., 2014).

U

We ﬁx a Universal Turing Machine (UTM), denoted
, with
a description tape (which speciﬁes the code of the Turing
machine to be executed), a work tape (simulating the tape
of that Turing machine during its operation) and a state tape
(simulating the state of that Turing machine). The general
statistical learning problem that can be formulated using
U
is the following: given some initial string x on the work tape,
predict the state of the simulated machine and the contents
of the work tape after some speciﬁed number of steps.

For simplicity, in this paper we consider models that only
predict the ﬁnal state; the necessary modiﬁcations in the gen-
eral case are routine. We also assume that W parametrises
Turing machines whose tape alphabet Σ and set of states Q
have been encoded by individual symbols in the tape alpha-
is actually what we call a pseudo-UTM
bet of
(see Appendix G). Again, treating the general case is routine
and for the present purposes only introduces uninteresting
complexity.

. Hence

U

U

Let Σ denote the tape alphabet of the simulated machine,
Q the set of states and let L, S, R stand for left, stay and
right, the possible motions of the Turing machine head. We
> 1 since otherwise the synthesis problem
assume that

Q
|

|

is trivial. The set of ordinary codes W code for a Turing ma-
chine sits inside a compact space of probability distributions
W over codes

W code :=

(cid:89)

Q

Σ

×

× {

L, S, R

}

σ,q
(cid:89)

⊆

σ,q

∆Σ

∆Q

∆

L, S, R
{

}

×

×

=: W

(4)

×

(σ(cid:48), q(cid:48), d)
{

Q.1 For example the point

where ∆X denotes the set of probability distributions over
a set X, see (13), and the product is over pairs (σ, q)
∈
W code
Σ
encodes the machine which when it reads σ under the head
in state q writes σ(cid:48), transitions into state q(cid:48) and moves in
Q denote
direction d. Given w
∈
the contents of the state tape of
after t timesteps (of the
simulated machine) when the work tape is initialised with
x and the description tape with w. There is a principled
extension of this operation of

W code let stept(x, w)

to a smooth function

}σ,q ∈

∈

U

U

∆ stept : Σ∗

W

∆Q

(5)

→

×
which propagates uncertainty about the symbols on the de-
scription tape to uncertainty about the ﬁnal state and we
refer to this extension as the smooth relaxation of
. The
details are given in Appendix H but at an informal level the
idea behind the relaxation is easy to understand: to sample
from ∆ stept(x, w) we run
to simulate t timesteps in such
a way that whenever the UTM needs to “look at” an entry
on the description tape we sample from the corresponding
distribution speciﬁed by w.2

U

U

The class of models that we consider is

p(y

x, w) = ∆ stept(x, w)
|

(6)

consists of a probability distribution q(x, y) over Σ∗

where t is ﬁxed for simplicity in this paper. More generally
we could also view x as consisting of a sequence and a
timeout, as is done in (Clift & Murfet, 2018, §7.1). The
construction of this model is summarised in Figure 1.
Deﬁnition 3.1 (Synthesis problem). A synthesis problem
for
×
Q. We say that the synthesis problem is deterministic if
there is f : Σ∗
x) = 1 for all
Q such that q(y = f (x)
|
Σ∗.
x
Deﬁnition 3.2. The triple (p, q, ϕ) associated to a synthe-
sis problem is the model p of (6) together with the true
distribution q and uniform prior ϕ on W .

→

∈

U

1The space W of parameters is clearly semi-analytic, that
is, it is cut out of Rd for some d by the vanishing f1(x) =
· · · = fr(x) = 0 of ﬁnitely many analytic functions on open
subsets of Rd together with ﬁnitely many inequalities g1(x) ≥
0, . . . , gs(x) ≥ 0 where the gj(x) are analytic. In fact W is semi-
algebraic, since the fi and gj may all be chosen to be polynomial
functions.

2Noting that this sampling procedure is repeated every time the

UTM looks at a given entry.

Geometry of Program Synthesis

This important quantity will be estimated in Section 5. Intu-
itively, the more singular the analytic space W0 of solutions
is, the smaller the RLCT. One way to think of the RLCT is
as a count of the effective number of parameters near W0
(Murfet et al., 2020, §4). A thermodynamic interpretation
is provided in Section 6. In Section 4 we relate the RLCT
to Kolmogorov complexity and in Section 7 we estimate
the RLCT of the synthesis problem detectA using our
estimation method.

Example 3.5 (detectA). The deterministic synthesis prob-
(cid:3), A, B
lem detectA has Σ =
reject, accept
}
{
{
and q(y
x) is determined by the function taking in a string
|
x of A’s and B’s and returning the state accept if the string
contains an A and state reject otherwise. The conditional
true distribution q(y
x) is realisable because this function is
|
computed by a Turing machine.

, Q =

}

W0 ∩

Two solutions are shown in Figure 2. On the left is a param-
W code.
W code and on the right is wr ∈
eter wl ∈
W0 \
Varying the distributions in wl that have nonzero entropy we
obtain a submanifold V
W0 containing wl of dimension
14. This leads by (Watanabe, 2009, Remark 7.3) to a bound
14) = 8 which is consistent
on the RLCT of λ
−
with the experimental results in Table 1. This highlights
that solutions need not lie at vertices of the probability sim-
plex, and W0 may contain a high-dimensional submanifold
around a given classical solution.

1
2 (30

⊆

≤

Figure 1. The state of U is represented by the state of the work tape,
state tape and description (code) tape. The work tape is initialised
with a sequence x ∈ Σ∗, the code tape with w ∈ W and the state
tape with some standard initial state, the smooth relaxation ∆ step
of the pseudo-UTM is run for t steps and the ﬁnal probability
distribution over states is y.

W with q(y

As ∆ stept is a polynomial function, K is analytic and so
W0 is a semi-analytic space (it is cut out of the semi-analytic
space W by the vanishing of K). If the synthesis problem
is deterministic and q(x) is uniform on some ﬁnite subset
of Σ∗ then W0 is semi-algebraic (it is cut out of W by poly-
nomial equations) and all solutions lie at the boundary of
the parameter space W (Appendix B). However in general
W0 is only semi-analytic and intersects the interior of W
x) is realisable that is,
(Example E.2). We assume that q(y
|
there exists w0 ∈
x) = p(y
|
A triple (p, q, ϕ) is regular if the model is identiﬁable, ie.
Rn, the map sending w to the condi-
for all inputs x
∈
tional probability distribution p(y
x, w) is one-to-one, and
the Fisher information matrix is non-degenerate. Otherwise,
the learning machine is strictly singular (Watanabe, 2009,
§1.2.1). Triples arising from synthesis problems are typi-
cally singular: in Example 3.5 below we show an explicit
example where multiple parameters w determine the same
model, and in Example E.2 we give an example where the
Hessian of K is degenerate everywhere on W0 (Watanabe,
2009, §1.1.3).

x, w0).
|

|

Remark 3.3. Non-deterministic synthesis problems arise
naturally in various contexts, for example in the ﬁtting of
algorithms to the behaviour of deep reinforcement learning
agents. Suppose an agent is acting in an environment with
Σ∗ and possible episode
starting states encoded by x
Q. Even if the optimal policy is known
end states by y
to determine a computable function Σ∗
Q the statistics
of the observed behaviour after ﬁnite training time will
only provide a function Σ∗
∆Q and if we wish to ﬁt
→
algorithms to behaviour it makes sense to deal with this
uncertainty directly.

→

∈

∈

Figure 2. Visualisation of two solutions for the synthesis problem
detectA .

4. Kolmogorov Complexity

Deﬁnition 3.4. Let (p, q, ϕ) be the triple associated to a
synthesis problem. The Real Log Canonical Threshold
(RLCT) λ of the synthesis problem is deﬁned so that
λ
is the largest pole of the meromorphic extension (Atiyah,
1970) of the zeta function ζ(z) = (cid:82) K(w)zϕ(w)dw.

−

Every Turing machine is the solution of a deterministic
synthesis problem so Section 3 associates to any Turing
machine a singularity of a semi-analytic space W0. To
indicate that this connection is not vacuous, we sketch how
the length of a program is related to the real log canonical

work<latexit sha1_base64="TJWz1OU75YFDdX7wl0/SZiBZv6w=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMA9IljA7mSRD5rHMzCphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KYs6M9f1vr7CxubW9U9wt7e0fHB6Vj0/aRiWa0BZRXOluhA3lTNKWZZbTbqwpFhGnnWh6m/mdR6oNU/LBzmIaCjyWbMQItpn0pPR0UK74VX8BtE6CnFQgR3NQ/uoPFUkElZZwbEwv8GMbplhbRjidl/qJoTEmUzymPUclFtSE6eLWObpwyhCNlHYlLVqovydSLIyZich1CmwnZtXLxP+8XmJHN2HKZJxYKsly0SjhyCqUPY6GTFNi+cwRTDRztyIywRoT6+IpuRCC1ZfXSbtWDerVq/tapVHP4yjCGZzDJQRwDQ24gya0gMAEnuEV3jzhvXjv3seyteDlM6fwB97nD0s7jl0=</latexit>state<latexit sha1_base64="SC7T0UNtxJZeOTT+l5BLbIY5Owg=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKRY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKvkGKfFCtuXV3AbJOvILUoEB7UP3qDxOWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPni2Bm5sMqQRIm2pZAs1N8TOY2Nmcah7Ywpjs2qNxf/83oZRjdBLlSaIVdsuSjKJMGEzD8nQ6E5Qzm1hDIt7K2EjammDG0+FRuCt/ryOuk06l6zfnXfqLWaRRxlOINzuAQPrqEFd9AGHxgIeIZXeHOU8+K8Ox/L1pJTzJzCHzifPwBVjsU=</latexit>code<latexit sha1_base64="om3Rg/XzVobOPXTCvLtEyjMGzbs=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMImQLGF2dpIMmccyMyuEJb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vdLG5tb2Tnm3srd/cHhUPT7pGpVqQjtEcaUfI2woZ5J2LLOcPiaaYhFx2oumt7nfe6LaMCUf7CyhocBjyUaMYJtLRMV0WK35dX8BtE6CgtSgQHtY/RrEiqSCSks4NqYf+IkNM6wtI5zOK4PU0ASTKR7TvqMSC2rCbHHrHF04JUYjpV1Jixbq74kMC2NmInKdAtuJWfVy8T+vn9rRTZgxmaSWSrJcNEo5sgrlj6OYaUosnzmCiWbuVkQmWGNiXTwVF0Kw+vI66TbqQbN+dd+otZpFHGU4g3O4hACuoQV30IYOEJjAM7zCmye8F+/d+1i2lrxi5hT+wPv8AQ5RjjU=</latexit>w<latexit sha1_base64="R7WKE1UfW8UlgiOq9cSzJJOgiHo=">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHYJRo8kXjxCIo8ENmR2aGBkdnYzM6shG77AiweN8eonefNvHGAPClbSSaWqO91dQSy4Nq777eQ2Nre2d/K7hb39g8Oj4vFJS0eJYthkkYhUJ6AaBZfYNNwI7MQKaRgIbAeT27nffkSleSTvzTRGP6QjyYecUWOlxlO/WHLL7gJknXgZKUGGer/41RtELAlRGiao1l3PjY2fUmU4Ezgr9BKNMWUTOsKupZKGqP10ceiMXFhlQIaRsiUNWai/J1Iaaj0NA9sZUjPWq95c/M/rJmZ446dcxolByZaLhokgJiLzr8mAK2RGTC2hTHF7K2FjqigzNpuCDcFbfXmdtCplr1q+alRKtWoWRx7O4BwuwYNrqMEd1KEJDBCe4RXenAfnxXl3PpatOSebOYU/cD5/AOJ3jPM=</latexit>x<latexit sha1_base64="tfWAW1SjmyNhrA0capfB+UlJ15k=">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHYJRo8kXjxCIo8ENmR2aGBkdnYzM2skG77AiweN8eonefNvHGAPClbSSaWqO91dQSy4Nq777eQ2Nre2d/K7hb39g8Oj4vFJS0eJYthkkYhUJ6AaBZfYNNwI7MQKaRgIbAeT27nffkSleSTvzTRGP6QjyYecUWOlxlO/WHLL7gJknXgZKUGGer/41RtELAlRGiao1l3PjY2fUmU4Ezgr9BKNMWUTOsKupZKGqP10ceiMXFhlQIaRsiUNWai/J1Iaaj0NA9sZUjPWq95c/M/rJmZ446dcxolByZaLhokgJiLzr8mAK2RGTC2hTHF7K2FjqigzNpuCDcFbfXmdtCplr1q+alRKtWoWRx7O4BwuwYNrqMEd1KEJDBCe4RXenAfnxXl3PpatOSebOYU/cD5/AOP7jPQ=</latexit>y<latexit sha1_base64="GM5ZmXW1PJCOVOCvnUffSh1cn3U=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Kkmp6LHgxWML9gPaUDbbSbt2swm7GyGU/gIvHhTx6k/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8Hkbu53nlBpHssHkyXoR3QkecgZNVZqZoNS2a24C5B14uWkDDkag9JXfxizNEJpmKBa9zw3Mf6UKsOZwFmxn2pMKJvQEfYslTRC7U8Xh87IpVWGJIyVLWnIQv09MaWR1lkU2M6ImrFe9ebif14vNeGtP+UySQ1KtlwUpoKYmMy/JkOukBmRWUKZ4vZWwsZUUWZsNkUbgrf68jppVyterXLdrJbrtTyOApzDBVyBBzdQh3toQAsYIDzDK7w5j86L8+58LFs3nHzmDP7A+fwB5X+M9Q==</latexit> step<latexit sha1_base64="NI9QQ7XStFY2oO5uCpCo3hbaGCA=">AAACBXicbVA9SwNBEN3zM8avqKUWi4lgFe5CRMuAFpYRzAckIextJsmSvdtjd04IRxob/4qNhSK2/gc7/417SQpNfDDweG+GmXl+JIVB1/12VlbX1jc2M1vZ7Z3dvf3cwWHdqFhzqHEllW76zIAUIdRQoIRmpIEFvoSGP7pO/cYDaCNUeI/jCDoBG4SiLzhDK3VzJ4X2DUhktK0i0AyVDlkAiUGIJoVuLu8W3SnoMvHmJE/mqHZzX+2e4nEAIXLJjGl5boSdhGkUXMIk244NRIyP2ABalqarTCeZfjGhZ1bp0b7StkKkU/X3RMICY8aBbzsDhkOz6KXif14rxv5VJxFhFCOEfLaoH0uKiqaR0J7QwFGOLWFcC3sr5UOmGUcbXNaG4C2+vEzqpaJXLl7clfKV8jyODDkmp+SceOSSVMgtqZIa4eSRPJNX8uY8OS/Ou/Mxa11x5jNH5A+czx92LpiG</latexit>init<latexit sha1_base64="eVVGLMwSrwOU7JIqXWoswyQdy7Y=">AAAB/nicbVBNSwMxEM36WevXqnjyEmwFT2W3VPRY8OKxgv2AdinZNG1Ds8mSzAplKfhXvHhQxKu/w5v/xmy7B219MPB4b4aZeWEsuAHP+3bW1jc2t7YLO8Xdvf2DQ/fouGVUoilrUiWU7oTEMMElawIHwTqxZiQKBWuHk9vMbz8ybbiSDzCNWRCRkeRDTglYqe+elnsqZpqA0pJELOWSw6zcd0texZsDrxI/JyWUo9F3v3oDRZOISaCCGNP1vRiClGjgVLBZsZcYFhM6ISPWtTRbZYJ0fv4MX1hlgIdK25KA5+rviZRExkyj0HZGBMZm2cvE/7xuAsObwL4UJ8AkXSwaJgKDwlkWeMA1oyCmlhCqub0V0zHRhIJNrGhD8JdfXiWtasWvVa7uq6V6LY+jgM7QObpEPrpGdXSHGqiJKErRM3pFb86T8+K8Ox+L1jUnnzlBf+B8/gCKDZXS</latexit>work<latexit sha1_base64="TJWz1OU75YFDdX7wl0/SZiBZv6w=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMA9IljA7mSRD5rHMzCphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KYs6M9f1vr7CxubW9U9wt7e0fHB6Vj0/aRiWa0BZRXOluhA3lTNKWZZbTbqwpFhGnnWh6m/mdR6oNU/LBzmIaCjyWbMQItpn0pPR0UK74VX8BtE6CnFQgR3NQ/uoPFUkElZZwbEwv8GMbplhbRjidl/qJoTEmUzymPUclFtSE6eLWObpwyhCNlHYlLVqovydSLIyZich1CmwnZtXLxP+8XmJHN2HKZJxYKsly0SjhyCqUPY6GTFNi+cwRTDRztyIywRoT6+IpuRCC1ZfXSbtWDerVq/tapVHP4yjCGZzDJQRwDQ24gya0gMAEnuEV3jzhvXjv3seyteDlM6fwB97nD0s7jl0=</latexit>state<latexit sha1_base64="SC7T0UNtxJZeOTT+l5BLbIY5Owg=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKRY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKvkGKfFCtuXV3AbJOvILUoEB7UP3qDxOWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPni2Bm5sMqQRIm2pZAs1N8TOY2Nmcah7Ywpjs2qNxf/83oZRjdBLlSaIVdsuSjKJMGEzD8nQ6E5Qzm1hDIt7K2EjammDG0+FRuCt/ryOuk06l6zfnXfqLWaRRxlOINzuAQPrqEFd9AGHxgIeIZXeHOU8+K8Ox/L1pJTzJzCHzifPwBVjsU=</latexit>code<latexit sha1_base64="om3Rg/XzVobOPXTCvLtEyjMGzbs=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMImQLGF2dpIMmccyMyuEJb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vdLG5tb2Tnm3srd/cHhUPT7pGpVqQjtEcaUfI2woZ5J2LLOcPiaaYhFx2oumt7nfe6LaMCUf7CyhocBjyUaMYJtLRMV0WK35dX8BtE6CgtSgQHtY/RrEiqSCSks4NqYf+IkNM6wtI5zOK4PU0ASTKR7TvqMSC2rCbHHrHF04JUYjpV1Jixbq74kMC2NmInKdAtuJWfVy8T+vn9rRTZgxmaSWSrJcNEo5sgrlj6OYaUosnzmCiWbuVkQmWGNiXTwVF0Kw+vI66TbqQbN+dd+otZpFHGU4g3O4hACuoQV30IYOEJjAM7zCmye8F+/d+1i2lrxi5hT+wPv8AQ5RjjU=</latexit>work<latexit sha1_base64="TJWz1OU75YFDdX7wl0/SZiBZv6w=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMA9IljA7mSRD5rHMzCphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KYs6M9f1vr7CxubW9U9wt7e0fHB6Vj0/aRiWa0BZRXOluhA3lTNKWZZbTbqwpFhGnnWh6m/mdR6oNU/LBzmIaCjyWbMQItpn0pPR0UK74VX8BtE6CnFQgR3NQ/uoPFUkElZZwbEwv8GMbplhbRjidl/qJoTEmUzymPUclFtSE6eLWObpwyhCNlHYlLVqovydSLIyZich1CmwnZtXLxP+8XmJHN2HKZJxYKsly0SjhyCqUPY6GTFNi+cwRTDRztyIywRoT6+IpuRCC1ZfXSbtWDerVq/tapVHP4yjCGZzDJQRwDQ24gya0gMAEnuEV3jzhvXjv3seyteDlM6fwB97nD0s7jl0=</latexit>state<latexit sha1_base64="SC7T0UNtxJZeOTT+l5BLbIY5Owg=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKRY8FLx4rmFZoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSMUmmGfdZIhP9GFLDpVDcR4GSP6aa0ziUvBtObud+94lrIxL1gNOUBzEdKREJRtFKvkGKfFCtuXV3AbJOvILUoEB7UP3qDxOWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPni2Bm5sMqQRIm2pZAs1N8TOY2Nmcah7Ywpjs2qNxf/83oZRjdBLlSaIVdsuSjKJMGEzD8nQ6E5Qzm1hDIt7K2EjammDG0+FRuCt/ryOuk06l6zfnXfqLWaRRxlOINzuAQPrqEFd9AGHxgIeIZXeHOU8+K8Ox/L1pJTzJzCHzifPwBVjsU=</latexit>code<latexit sha1_base64="om3Rg/XzVobOPXTCvLtEyjMGzbs=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKeyGiB4DXjxGMImQLGF2dpIMmccyMyuEJb/gxYMiXv0hb/6Ns8keNLGgoajqprsrSjgz1ve/vdLG5tb2Tnm3srd/cHhUPT7pGpVqQjtEcaUfI2woZ5J2LLOcPiaaYhFx2oumt7nfe6LaMCUf7CyhocBjyUaMYJtLRMV0WK35dX8BtE6CgtSgQHtY/RrEiqSCSks4NqYf+IkNM6wtI5zOK4PU0ASTKR7TvqMSC2rCbHHrHF04JUYjpV1Jixbq74kMC2NmInKdAtuJWfVy8T+vn9rRTZgxmaSWSrJcNEo5sgrlj6OYaUosnzmCiWbuVkQmWGNiXTwVF0Kw+vI66TbqQbN+dd+otZpFHGU4g3O4hACuoQV30IYOEJjAM7zCmye8F+/d+1i2lrxi5hT+wPv8AQ5RjjU=</latexit>···<latexit sha1_base64="0bCcXyz6HriuD2v71FSD+L8Alno=">AAAB73icbVBNSwMxEJ2tX7V+VT16CbaCp7JbKnosePFYwX5Au5RsNtuGZpM1yQpl6Z/w4kERr/4db/4b03YP2vpg4PHeDDPzgoQzbVz32ylsbG5t7xR3S3v7B4dH5eOTjpapIrRNJJeqF2BNORO0bZjhtJcoiuOA024wuZ373SeqNJPiwUwT6sd4JFjECDZW6lUHJJRGV4fliltzF0DrxMtJBXK0huWvQShJGlNhCMda9z03MX6GlWGE01lpkGqaYDLBI9q3VOCYaj9b3DtDF1YJUSSVLWHQQv09keFY62kc2M4Ym7Fe9ebif14/NdGNnzGRpIYKslwUpRwZiebPo5ApSgyfWoKJYvZWRMZYYWJsRCUbgrf68jrp1Gteo3Z1X680G3kcRTiDc7gED66hCXfQgjYQ4PAMr/DmPDovzrvzsWwtOPnMKfyB8/kDZpuPgw==</latexit> step<latexit sha1_base64="NI9QQ7XStFY2oO5uCpCo3hbaGCA=">AAACBXicbVA9SwNBEN3zM8avqKUWi4lgFe5CRMuAFpYRzAckIextJsmSvdtjd04IRxob/4qNhSK2/gc7/417SQpNfDDweG+GmXl+JIVB1/12VlbX1jc2M1vZ7Z3dvf3cwWHdqFhzqHEllW76zIAUIdRQoIRmpIEFvoSGP7pO/cYDaCNUeI/jCDoBG4SiLzhDK3VzJ4X2DUhktK0i0AyVDlkAiUGIJoVuLu8W3SnoMvHmJE/mqHZzX+2e4nEAIXLJjGl5boSdhGkUXMIk244NRIyP2ABalqarTCeZfjGhZ1bp0b7StkKkU/X3RMICY8aBbzsDhkOz6KXif14rxv5VJxFhFCOEfLaoH0uKiqaR0J7QwFGOLWFcC3sr5UOmGUcbXNaG4C2+vEzqpaJXLl7clfKV8jyODDkmp+SceOSSVMgtqZIa4eSRPJNX8uY8OS/Ou/Mxa11x5jNH5A+czx92LpiG</latexit> step<latexit sha1_base64="NI9QQ7XStFY2oO5uCpCo3hbaGCA=">AAACBXicbVA9SwNBEN3zM8avqKUWi4lgFe5CRMuAFpYRzAckIextJsmSvdtjd04IRxob/4qNhSK2/gc7/417SQpNfDDweG+GmXl+JIVB1/12VlbX1jc2M1vZ7Z3dvf3cwWHdqFhzqHEllW76zIAUIdRQoIRmpIEFvoSGP7pO/cYDaCNUeI/jCDoBG4SiLzhDK3VzJ4X2DUhktK0i0AyVDlkAiUGIJoVuLu8W3SnoMvHmJE/mqHZzX+2e4nEAIXLJjGl5boSdhGkUXMIk244NRIyP2ABalqarTCeZfjGhZ1bp0b7StkKkU/X3RMICY8aBbzsDhkOz6KXif14rxv5VJxFhFCOEfLaoH0uKiqaR0J7QwFGOLWFcC3sr5UOmGUcbXNaG4C2+vEzqpaJXLl7clfKV8jyODDkmp+SceOSSVMgtqZIa4eSRPJNX8uY8OS/Ou/Mxa11x5jNH5A+czx92LpiG</latexit>blankABrejectacceptblankABrejectacceptleftrightstaytuple_1tuple_2tuple_3tuple_4tuple_5tuple_6blankABrejectacceptblankABrejectacceptleftrightstaytuple_1tuple_2tuple_3tuple_4tuple_5tuple_6Geometry of Program Synthesis

threshold of a singularity.

U

Let q(x, y) be a deterministic synthesis problem for
which
only involves input sequences in some restricted alphabet
(Σinput)∗. Let Dn be sam-
Σinput, that is, q(x) = 0 if x /
∈
W code
W0 be two explana-
pled from q(x, y) and let u, v
∈
tions for the sample in the sense that Kn(u) = Kn(v) = 0.
Which explanation for the data should we prefer? The clas-
sical answer based on Occam’s razor (Solomonoff, 1964) is
that we should prefer the shorter program, that is, the one
using the fewest states and symbols.

∩

|

|

Σ

≤

and M =

. Any Turing machine T using
Set N =
Q
|
|
N (cid:48)
N symbols and M (cid:48)
of
≤
U
length cM (cid:48)N (cid:48) where c is a constant. We assume that Σinput
is included in the tape alphabet of T so that N (cid:48)
Σinput|
and deﬁne the Kolmogorov complexity of q with respect to
to be the inﬁmum c(q) of M (cid:48)N (cid:48) over Turing machines T

M states has a code for

≥ |

U
that give classical solutions for q.

Let λ be the RLCT of the triple (p, q, ϕ) associated to the
synthesis problem (Deﬁnition 3.4).
1
2 (M + N )c(q).
Theorem 4.1. λ

≤
The proof is deferred to Appendix C.
Remark 4.2. The Kolmogorov complexity depends only on
the number of symbols and states used. The RLCT is a more
reﬁned invariant since it also depends on how each symbol
and state is used (Clift & Murfet, 2018, Remark 7.8) as this
affects the polynomials deﬁning W0 (see Appendix B).
(Σinput)∗ and consider the determin-
Remark 4.3. Let s
denotes
) = 1 where
istic synthesis problem q with q(
∅
∅
the empty string, and q(y = s
) = 1. Here we con-
x =
∅
|
sider the modiﬁcation of the statistical model described in
Section 3 which predicts the contents of the work tape af-
ter t steps rather than the state, and condition on the state
being halt. Then a classical solution to this synthesis prob-
lem is a Turing machine which halts within t steps with
s on the work tape. If we make t sufﬁciently large then
the Kolmogorov complexity of q in the above sense is the
Kolmogorov complexity of s in the usual sense, up to a
constant multiplicative factor depending only on
. Hence
the terminology used above is reasonable.

∈

U

5. Algorithm for Estimating RLCTs

We have stated that the RLCT λ is the most important geo-
metric invariant in singular learning theory (Section 2) and
shown how it is related to computation by relating it to the
Kolmogorov complexity (Section 4). Now we explain how
to estimate it in practice.

Given a sample Dn =

Ln(w) :=

1
n

−

n
i=1 from q(x, y) let
(xi, yi)
}
{
n
(cid:88)

xi, w) = Kn(w) + Sn

log p(yi|

i=1

be the negative log likelihood, where Sn is the empirical
entropy of the true distribution. We would like to estimate

Eβ

w[nLn(w)] :=

(cid:90)

1
Z β
n

nLn(w)ϕ(w)

n
(cid:89)

i=1

xi, w)βdw

p(yi|

n = (cid:82) ϕ(w) (cid:81)n
where Z β
i=1 p(yi|
verse temperature β. If β = β0
then by Theorem 4 of (Watanabe, 2013),

xi, w)βdw for some in-
log n for some constant β0,

Eβ

w[nLn(w)] = nLn(w0)+

λ log n
β0

+Un

(cid:115)

λ log n
2β0

+Op(1)

Un}
{

(7)
where
is a sequence of random variables satisfying
E[Un] = 0 and λ is the RLCT. In practice, the last two
terms often vary negligibly with 1/β and so Eβ
w[nLn(w)]
approximates a linear function of 1/β with slope λ (Watan-
abe, 2013, Corollary 3). This is the foundation of the RLCT
estimation procedure found in Algorithm 1 which is used in
our experiments.

Algorithm 1 RLCT estimation

Input: range of β’s, set of training sets T each of size n, approx-
imate samples {w1, . . . , wR} from pβ(w|Dn) for each training
set Dn and each β
for training set Dn ∈ T do
for β in range of β’s do
Approximate Eβ
w1, . . . , wR are approximate samples from pβ(w|Dn)

w[nLn(w)] with 1
R

i=1 nLn(wr) where

(cid:80)R

end for
Perform generalised least squares to ﬁt λ in Equation (7), call
result ˆλ(Dn)

end for
Output:

1
|T |

(cid:80)

Dn∈T

ˆλ(Dn)

Each RLCT estimate ˆλ(
Dn) in Algorithm 1 was performed
5
by linear regression on the pairs
w [nLn(w)])
i=1
}
where the ﬁve inverse temperatures βi are centered on the in-
verse temperature 1/T where T is the temperature reported
for each experiment in Table 1 and Table 3.

(1/βi, Eβi
{

6. Thermodynamics of program synthesis

Now that we have formulated program synthesis in the set-
ting of singular learning theory, we introduce the thermody-
namic dictionary that this makes available. Given a triple
(p, q, ϕ) associated to a synthesis problem, the physical sys-
tem consists of the space W of microstates and the random
Hamiltonian

Hn(w) = nKn(w)

1
β

−

log ϕ(w)

(8)

which depends on a sample Dn from the true distribution
of size n and the inverse temperature β. The corresponding

Boltzmann distribution is the tempered Bayesian posterior
(3) for the synthesis problem

determine phases, and in general there will be phases not
associated to any solution.

Geometry of Program Synthesis

1
Z

exp(

−

βHn(w))dw

pβ(w

Dn)dw =
|
W exp(

−

where Z = (cid:82)
βHn(w))dw is called the partition
function. In thermodynamics we think of functions f (w) of
microstates as fundamentally unobservable; what we can
observe are averages
(cid:90)

Eβ

w [f (w)] =

1
Z

f (w)ϕ(w) exp(

−

W

nβKn(w))dw

possibly restricted to subsets W (cid:48)
W (microstates com-
patible with some speciﬁed macroscopic condition). Tak-
ing program synthesis seriously leads us to systematically
re-evaluate aspects of the theory of computation with this
restriction (that only integrals are observable) in mind.

⊆

To give one important example: the energy of the Boltzmann
distribution (tempered Bayesian posterior) at a ﬁxed inverse
temperature β and value of n is by deﬁnition

U (n, β) =

=

(cid:90)

(cid:90)

dw pβ(w

dw pβ(w

Dn)Hn(w)
|
Dn)(cid:2)nKn(w)
|

= Eβ
w

(cid:2)nKn(w)] +

(cid:2)

Eβ
w

1
β

= Eβ
w

(cid:2)nLn(w)]

nSn +

−

−
1
β

log ϕ(w)(cid:3)

1
β
−
log ϕ(w)(cid:3)

(cid:2)

Eβ
w

log ϕ(w)(cid:3)

−
(cid:2)

log ϕ(w)(cid:3)
which is a random variable. The quantity 1
β
is the contribution to the energy from interaction with the
prior, and by (7) the most important contribution to the ﬁrst
summand is λT where T = 1
β . We see the fundamental role
that the RLCT plays in the thermodynamic picture.

Eβ
w

−

6.1. Programs as phases

In physics a phase is deﬁned to be a local minima of some
thermodynamic potential with respect to an extensive ther-
modynamic coordinate; for example the Gibbs potential as
a function of volume V . The role of the volume may in
R
principle be played by any analytic function V : W
and in the following we assume such a function has been
chosen and deﬁne the free energy to be
(cid:90)

−→

F (n, β, v) = log

{w|V (w)=v}

dwϕ(w) exp(

−

nβLn(w)) .

(9)
Deﬁnition 6.1. A phase of the synthesis problem q is a local
minima of the free energy F (n, β, v) as a function of v, at
ﬁxed values of n, β.

Any classical solution w0 determines a phase, that is, a
local minimum of F at V0 = V (w0). Other solutions may

Different phases have different “physical” properties, such
as energy or entropy, calculated in the ﬁrst case by restrict-
ing the integral deﬁning U (n, β) to a range of V values
near V0. The methods of asymptotic analysis introduced
by Watanabe show that these properties of phases are de-
termined by geometric invariants, such as the RLCT. For
example the arguments of Section 4 show that the entropy of
the phase associated to a classical solution may be bounded
above (up to multiplicative constants depending only on
)
by a function of n and the Kolmogorov complexity.

U

6.2. A ﬁrst-order phase transition

To demonstrate the thermodynamic language in the context
of program synthesis we give a qualitative treatment in this
section of a simple example of a ﬁrst-order phase transition.

Let M1, . . . , Mr be Turing machines (not necessarily clas-
sical solutions) and [M1], . . . , [Mr] the associated points of
W and set Vi = V ([Mi]). We may approximate the free
energy Fi = F (Vi) for sufﬁciently large n by (Watanabe,
2009, Section 7.6) as follows:

Fi ≈

nβLn([Mi]) + λi log n

(10)

|

w
{

is the RLCT of the level set
where λi
K(w) =
at [Mi] and the free energies are deﬁned up to an
K([Mi])
}
additive constant that is the same for each i (and which we
may ignore if, as we do, we only want to consider ordinality
of the free energies).
By the same argument as Section 4 we have λi ≤
c l(Mi)
where c is a constant depending only on
and l(Mi) is the
“length” of the code for Mi, taken to be the product of the
number of used states and symbols. This demonstrates the
following inequalities for 1

U

r

i

≤

≤

≤

nβLn([Mi])

nβLn([Mi]) + c l(Mi) log(n) .

Fi ≤
(11)
Ln([Mi]) to
For n sufﬁciently large we may take Li ≈
be constant, at least for the purposes of comparing free
energies. Suppose that Li > Lj but l(Mj) > l(Mi), that
is, the Turing machine Mj produces outputs closer to the
samples than Mi, but its code is longer. It is possible for
small n that the free energy Fi is nonetheless lower than
Fj (and thus samples from the posterior are more likely to
come from V = Vi than V = Vj) since the penalty for
longer programs is relatively harsher when n is small.

→ ∞

However as n
it is easy to see using the inequalities
(11) that eventually Fi > Fj whenever Li > Lj. Hence
as the system is exposed to more input-output examples it
undergoes a series of ﬁrst-order phase transitions, where
the phases of low entropy but high energy are eventually
supplanted as the global minima by phases with low energy.

7. Experiments

Geometry of Program Synthesis

We estimate the RLCT for the triples (p, q, ϕ) associated
to the synthesis problems detectA (Example 3.5) and
parityCheck (Appendix F). Hyperparameters of the var-
ious machines are contained in Table 2 of Appendix D. The
true distribution q(x) is deﬁned as follows: we ﬁx a mini-
mum and maximum sequence length a
b and to sample
q(x) we ﬁrst sample a length l uniformly from [a, b]
x
and then uniformly sample x from

∼

≤

l.

A, B
{

}

∈

W

p(y
{

x, w) : w
|

We perform MCMC on the weight vector for the model
class
where w is represented in
}
our PyTorch implementation by three tensors of shape
}1≤i≤3 where L is the number of tuples in the de-
[L, ni]
{
scription tape of the TM being simulated and
are the
number of symbols, states and directions respectively. A
direct simulation of the UTM is used for all experiments
to improve computational efﬁciency (Appendix I). We gen-
erate, for each inverse temperature β and dataset Dn, a
Markov chain via the No-U-Turn sampler from (Hoffman &
Gelman, 2014). We use the standard uniform distribution as
our prior ϕ.

ni}

{

Max-length

Temperature

RLCT

Std

R squared

7
7
8
8
9
9
10
10

log(500)
log(1000)
log(500)
log(1000)
log(500)
log(1000)
log(500)
log(1000)

8.089205
6.533362
4.601800
4.431683
5.302598
4.027324
3.224910
3.433624

3.524719
2.094278
1.156325
1.069020
2.415647
1.866802
1.169699
0.999967

0.965384
0.966856
0.974569
0.967847
0.973016
0.958805
0.963358
0.949972

Table 1. RLCT estimates for detectA.

For the problem detectA given in Example 3.5 the dimen-
sion of parameter space is dim W = 30. We use generalised
least squares to ﬁt the RLCT λ (with goodness-of-ﬁt mea-
sured by R2), the algorithm of which is given in Section 5.
Our results are displayed in Table 1 and Figure 3. Our
purpose in these experiments is not to provide high accu-
racy estimates of the RLCT, as these would require much
longer Markov chains. Instead we demonstrate how rough
estimates consistent with the theory can be obtained at low
computational cost. If this model were regular the RLCT
would be dim W/2 = 15.

8. Discussion

Figure 3. Plot of RLCT estimates for detectA. Shaded region
shows one standard deviation.

but this is a false intuition based on regular models.

Since neural networks, Bayesian networks, smooth relax-
ations of UTMs and all other extant approaches to smooth
program synthesis are strictly singular models (the map
from parameters to functions is not injective) the set W0 of
parameters w with K(w) = 0 is a complex extended object,
whose geometry is shown by Watanabe’s singular learning
theory to be deeply related to the learning process. We
have examined this geometry in several speciﬁc examples
and shown how to think about programs from a geometric
and thermodynamic perspective. It is our hope that this
approach can assist in developing the next generation of
synthesis machines.

References

Akhlaghpour, H. A theory of natural universal computation
through RNA. arXiv preprint arXiv:2008.08814, 2020.

Atiyah, M. F. Resolution of singularities and division of dis-
tributions. Communications on Pure and Applied Mathe-
matics, 23(2):145–150, 1970.

Bunel, R. R., Desmaison, A., Mudigonda, P. K., Kohli, P.,
and Torr, P. Adaptive neural compilation. In Advances in
Neural Information Processing Systems, pp. 1444–1452,
2016.

We have developed a theoretical framework in which all
programs can in principle be synthesised from input-output
examples. This is done by propagating uncertainty through
a smooth relaxation of a universal Turing machine. In ap-
proaches to program synthesis based on gradient descent
there is a tendency to think of solutions to the synthesis
problem as isolated critical points of the loss function K,

Chen, T., Fox, E., and Guestrin, C. Stochastic gradient
Hamiltonian Monte Carlo. In International Conference
on Machine Learning, pp. 1683–1691, 2014.

Chen, X., Liu, C., and Song, D. Execution-guided neu-
ral program synthesis. In International Conference on
Learning Representations, 2018.

Clift, J. and Murfet, D. Derivatives of Turing machines in
linear logic. arXiv preprint arXiv:1805.11813, 2018.

Solomonoff, R. J. A formal theory of inductive inference.

Part I. Information and control, 7(1):1–22, 1964.

Geometry of Program Synthesis

Watanabe, S. Almost all learning machines are singular. In
2007 IEEE Symposium on Foundations of Computational
Intelligence, pp. 383–388. IEEE, 2007.

Watanabe, S. Algebraic Geometry and Statistical Learning
Theory, volume 25. Cambridge University Press, 2009.

Watanabe, S. A widely applicable Bayesian information
criterion. Journal of Machine Learning Research, 14:
867–897, 2013.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic
gradient Langevin dynamics. In Proceedings of the 28th
International Conference on Machine Learning, pp. 681–
688, 2011.

Zador, A. M. A critique of pure learning and what artiﬁcial
neural networks can learn from animal brains. Nature
communications, 10(1):1–7, 2019.

Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G.
Cyclical stochastic gradient MCMC for Bayesian deep
learning. In International Conference on Learning Rep-
resentations, 2020.

Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D.,
and Neven, H. Bayesian sampling using stochastic gra-
dient thermostats. In Advances in Neural Information
Processing Systems, pp. 3203–3211, 2014.

Evans, R. and Grefenstette, E. Learning explanatory rules
from noisy data. Journal of Artiﬁcial Intelligence Re-
search, 61:1–64, 2018.

Freer, C. E., Roy, D. M., and Tenenbaum, J. B. To-
wards common-sense reasoning via conditional simula-
tion: legacies of Turing in artiﬁcial intelligence. Turing’s
Legacy, 42:195–252, 2014.

Gaunt, A. L., Brockschmidt, M., Singh, R., Kushman, N.,
Kohli, P., Taylor, J., and Tarlow, D. Terpret: A probabilis-
tic programming language for program induction. arXiv
preprint arXiv:1608.04428, 2016.

Grifﬁth, P. and Harris, J. Principles of Algebraic Geometry.

Wiley-Interscience, 1978.

Gulwani, S., Polozov, O., and Singh, R. Program synthesis.
Foundations and Trends in Programming Languages, 4
(1-2):1–119, 2017.

Hironaka, H. Resolution of singularities of an algebraic
variety over a ﬁeld of characteristic zero: I. Annals of
Mathematics, 79(1):109–203, 1964.

Hoffman, M. D. and Gelman, A. The No-U-Turn sampler:
adaptively setting path lengths in Hamiltonian Monte
Carlo. J. Mach. Learn. Res., 15(1):1593–1623, 2014.

Hutter, M. Universal artiﬁcial intelligence: Sequential
decisions based on algorithmic probability. Springer
Science & Business Media, 2004.

Kaiser, Ł. and Sutskever, I. Neural GPUs learn algorithms.
In International Conference on Learning Representations,
2016.

Murfet, D., Wei, S., Gong, M., Li, H., Gell-Redman, J.,
and Quella, T. Deep learning is singular, and that’s good.
arXiv preprint arXiv:2010.11560, 2020.

Neelakantan, A., Le, Q. V., and Sutskever, I. Neural pro-
grammer: Inducing latent programs with gradient descent.
In International Conference on Learning Representations,
ICLR 2016, 2016.

Schmidhuber, J. Discovering neural nets with low Kol-
mogorov complexity and high generalization capability.
Neural Networks, 10(5):857–873, 1997.

Geometry of Program Synthesis

A. Practical implications

Using singular learning theory we have explained how pro-
grams to be synthesised are singularities of analytic func-
tions, and how the Kolmogorov complexity of a program
bounds the RLCT of the associated singularity. We now
sketch some practical insights that follow from this point of
view.

Why synthesis gets stuck: the kind of local minimum of
the free energy that we want the synthesis process to ﬁnd
W0 where λα is minimal. By Section 4
are solutions wα ∈
one may think of these points as the “lowest complexity”
solutions. However it is possible that there are other local
Indeed, there may be local
minima of the free energy.
minima where the free energy is lower than the free energy
at any solution since at ﬁnite n it is possible to tradeoff
an increase in Kα against a decrease in the RLCT λα. In
practice, the existence of such “siren minima” of the free
energy may manifest itself as regions where the synthesis
process gets stuck and fails to converge to a solution. In such
a region Kαn + λα log(n) < λ log(n) where λ is the RLCT
of the synthesis problem. In practice it has been observed
that program synthesis by gradient descent often fails for
complex problems in the sense that it fails to converge to
a solution (Gaunt et al., 2016). While synthesis by SGD
and sampling are different, it is a reasonable hypothesis that
these siren minima are a signiﬁcant contributing factor in
both cases.

Can we avoid siren minima? If we let λc denote the RLCT
of the level set Wc then siren minima of the free energy will
be impossible at a given value of n and c as long as λc ≥
c n
log(n) . Recall that the more singular Wc is the lower
λ
−
the RLCT, so this lower bound says that the level sets should
not become too singular too quickly as c increases. At any
given value of n there is a “siren free” region in the range
since the RLCT is non-negative (Figure 4).
c
Thus the learning process will be more reliable the smaller
λ log(n)
is. This can be arranged either by increasing n
n

λ log(n)
n

≥

(providing more examples) or decreasing λ.

U

U

. As we have deﬁned it

While the RLCT is determined by the synthesis problem, it
is possible to change its value by changing the structure of
is a “simulation type”
the UTM
UTM, but one could for example add special states such
that if a code speciﬁes a transition into that state a series
of steps is executed by the UTM (i.e. a subroutine). This
amounts to specifying codes in a higher level programming
language. Hence one of the practical insights that can be de-
rived from the geometric point of view on program synthesis
is that varying this language is a natural way to engineer
the singularities of the level sets of K, which according
to singular learning theory has direct implications for the
learning process.

Figure 4. Level sets above the cutoff cannot contain siren local
minima of the free energy.

B. Deterministic synthesis problems

In this section we consider the case of a deterministic synthe-
sis problem q(x, y) which is ﬁnitely supported in the sense
Σ∗ such that q(x) = c
that there exists a ﬁnite set
for all x
. We ﬁrst need
to discuss the coordinates on the parameter space W of
(4). To specify a point on W is to specify for each pair
Q (that is, for each tuple on the description
(σ, q)
tape) a triple of probability distributions

and q(x) = 0 for all x /

X ⊆

∈ X

∈ X

×

Σ

∈

(cid:88)

σ(cid:48)∈Q
(cid:88)

q(cid:48)∈Q

xσ,q
σ(cid:48)

yσ,q
q(cid:48)

σ(cid:48)

q(cid:48)

·

·

∈

∈

∆Σ ,

∆Q ,

(cid:88)

d∈{L,S,R}

zσ,q
d

d

·

∈

∆

L, S, R
{

}

.

The space W of distributions is therefore contained in the
afﬁne space with coordinate ring
σ,q,σ(cid:48), (cid:8)yσ,q

RW = R(cid:2)(cid:8)xσ,q
σ(cid:48)

σ,q,q(cid:48), (cid:8)zσ,q
(cid:9)

(cid:3) .

σ,q,d

(cid:9)

(cid:9)

q(cid:48)

d

−

→

) : W

Q by F x

The function F x = ∆ stept(x,
∆Q is polyno-
mial (Clift & Murfet, 2018, Proposition 4.2) and we denote
for s
RW the polynomial computing the
associated component of the function F x. Let ∂W denote
the boundary of the manifold with corners W , that is, the
set of all points on W where at least one of the coordinate
functions given above vanishes

s ∈

∈

∂W = V(cid:0) (cid:89)

(cid:104) (cid:89)

xσ,q
σ(cid:48)

(cid:89)

yσ,q
q(cid:48)

(cid:89)

(cid:105)(cid:1)

zσ,q
d

σ,q

σ(cid:48)∈Q

q(cid:48)∈Q

d∈{L,S,R}

where V(h) denotes the vanishing locus of h.
= W .
Lemma B.1. W0 (cid:54)
Proof. Choose x
that q(y

x) = 1. Let w

∈ X

with q(x) > 0 and let y be such
W code be the code for the

|

∈

 <latexit sha1_base64="87p802AjPb7Cs/EP/Opj7RaGDGU=">AAAB7nicbVDLSsNAFL2pr1pfVZduBovgqiSi6LLoxmUF+4A2lJvJpB06mYSZiVBCP8KNC0Xc+j3u/BunbRbaemDgcM65zL0nSAXXxnW/ndLa+sbmVnm7srO7t39QPTxq6yRTlLVoIhLVDVAzwSVrGW4E66aKYRwI1gnGdzO/88SU5ol8NJOU+TEOJY84RWOlTl/YaIiDas2tu3OQVeIVpAYFmoPqVz9MaBYzaahArXuemxo/R2U4FWxa6WeapUjHOGQ9SyXGTPv5fN0pObNKSKJE2ScNmau/J3KMtZ7EgU3GaEZ62ZuJ/3m9zEQ3fs5lmhkm6eKjKBPEJGR2Owm5YtSIiSVIFbe7EjpChdTYhiq2BG/55FXSvqh7l/Wrh8ta47aoowwncArn4ME1NOAemtACCmN4hld4c1LnxXl3PhbRklPMHMMfOJ8/P12PhQ==</latexit> log(n)n<latexit sha1_base64="lsQVynnXbJ6Gfqi4JWsS+3V0pmU=">AAACAnicbVDLSgMxFM3UV62vUVfiJliEuikzUtFl0Y3LCvYBnaFkMpk2NJMMSUYow+DGX3HjQhG3foU7/8a0nYW2HggczrmHm3uChFGlHefbKq2srq1vlDcrW9s7u3v2/kFHiVRi0saCCdkLkCKMctLWVDPSSyRBccBINxjfTP3uA5GKCn6vJwnxYzTkNKIYaSMN7CMvkghnHjOREEGPiWGNn+UZzwd21ak7M8Bl4hakCgq0BvaXFwqcxoRrzJBSfddJtJ8hqSlmJK94qSIJwmM0JH1DOYqJ8rPZCTk8NUoIIyHN4xrO1N+JDMVKTeLATMZIj9SiNxX/8/qpjq78jPIk1YTj+aIoZVALOO0DhlQSrNnEEIQlNX+FeIRMJ9q0VjEluIsnL5POed1t1C/uGtXmdVFHGRyDE1ADLrgETXALWqANMHgEz+AVvFlP1ov1bn3MR0tWkTkEf2B9/gAV3pc9</latexit>c<latexit sha1_base64="XEi9uEsjWQ+XS0FVxzpFQZ5h7eQ=">AAAB7HicbVBNSwMxEJ31s9avqkcvwVbwIGVXFPVkwYvHCm5baJeSTbNtaJJdkqxQlv4GLx4U8erv8Dd489+YbnvQ1gcDj/dmmJkXJpxp47rfztLyyuraemGjuLm1vbNb2ttv6DhVhPok5rFqhVhTziT1DTOcthJFsQg5bYbD24nffKRKs1g+mFFCA4H7kkWMYGMlv9I5JZVuqexW3RxokXgzUr75hBz1bumr04tJKqg0hGOt256bmCDDyjDC6bjYSTVNMBniPm1bKrGgOsjyY8fo2Co9FMXKljQoV39PZFhoPRKh7RTYDPS8NxH/89qpia6CjMkkNVSS6aIo5cjEaPI56jFFieEjSzBRzN6KyAArTIzNp2hD8OZfXiSNs6p3Xr24d8u162kaUIBDOIIT8OASanAHdfCBAIMneIFXRzrPzpvzPm1dcmYzB/AHzscP8FSO2Q==</latexit>sirenfree<latexit sha1_base64="A9isavlJv74fj8B+wrJ3O/Bkfds=">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4kJIURY8FLx4r2A9oQtlsJ+3SzSbsboQS+je8eFDEq3/Gm//GbZuDtj4YeLw3w8y8MBVcG9f9dtbWNza3tks75d29/YPDytFxWyeZYthiiUhUN6QaBZfYMtwI7KYKaRwK7ITju5nfeUKleSIfzSTFIKZDySPOqLGS719qrlCSSCH2K1W35s5BVolXkCoUaPYrX/4gYVmM0jBBte55bmqCnCrDmcBp2c80ppSN6RB7lkoaow7y+c1Tcm6VAYkSZUsaMld/T+Q01noSh7Yzpmakl72Z+J/Xy0x0G+RcpplByRaLokwQk5BZAGRgH2ZGTCyhTHF7K2EjqigzNqayDcFbfnmVtOs176p2/VCvNupFHCU4hTO4AA9uoAH30IQWMEjhGV7hzcmcF+fd+Vi0rjnFzAn8gfP5A437kVM=</latexit>slope nlog(n)<latexit sha1_base64="PMhVvaslfSLTYIx1aQVXJ67JT24=">AAACCHicbVDLSgMxFM3UV62vUZcuDLZChVpmiqLLghuXFewDOkPJpJk2NJMMSUYoQ5du/BU3LhRx6ye4829M21lo64ELh3Pu5d57gphRpR3n28qtrK6tb+Q3C1vbO7t79v5BS4lEYtLEggnZCZAijHLS1FQz0oklQVHASDsY3Uz99gORigp+r8cx8SM04DSkGGkj9exjr+JVFBMxgaVzT4cS4ZRPUo+JQZmfTUo9u+hUnRngMnEzUgQZGj37y+sLnESEa8yQUl3XibWfIqkpZmRS8BJFYoRHaEC6hnIUEeWns0cm8NQofRgKaYprOFN/T6QoUmocBaYzQnqoFr2p+J/XTXR47aeUx4kmHM8XhQmDWsBpKrBPJcGajQ1BWFJzK8RDZMLQJruCCcFdfHmZtGpV96J6eVcr1mtZHHlwBE5AGbjgCtTBLWiAJsDgETyDV/BmPVkv1rv1MW/NWdnMIfgD6/MHMdWYxg==</latexit>Geometry of Program Synthesis

W0.

Turing machine which ignores the symbol under the head
= y and
and current state, transitions to some ﬁxed state s
stays. Then w /
∈
Lemma B.2. The set W0 is semi-algebraic and W0 ⊆
Proof. Given x
∈
for the unique state with q(x, y)
Kullback-Leibler divergence is

Σ∗ with q(x) > 0 we write y = y(x)
= 0. In this notation the

∂W .

K(w) =

(cid:88)

x∈X

c DKL(y

F x(w))

(cid:107)

=

c
−

(cid:88)

x∈X

log F x

y (w) =

c log

−

(cid:89)

x∈X

F x

y (w) .

Hence

W0 = W

(cid:92)

∩

x∈X

V(1

−

F x

y (w))

is semi-algebraic.

Recall that the function ∆ stept is associated to an encoding
of the UTM in linear logic by the Sweedler semantics (Clift
& Murfet, 2018) and the particular polynomials involved
have a form that is determined by the details of that encoding
(Clift & Murfet, 2018, Proposition 4.3). From the design
of our UTM we obtain positive integers lσ, mq, nd for σ
Σ, q

and a function π : Θ

∈
Q where

L, S, R

Q, d

∈

∈ {

Θ =

(cid:89)

σ,q

}
Σlσ

Qmq

L, S, R

}

× {

×

→
nd .

We represent elements of Θ by tuples (µ, ζ, ξ)
∈
µ(σ, q, i)
i
≤
≤
similarly ζ(σ, q, j)
L, S, R
polynomial F x

∈
∈
Q and ξ(σ, q, k)

Q and 1

Σ for σ

Σ, q

∈ {

Θ where
lσ and
. The
}

∈

∈

s is
(cid:88)

F x

s =

δ(s = π(µ, ζ, ξ))

×

xσ,q
µ(σ,q,i)

mq
(cid:89)

j=1

yσ,q
ζ(σ,q,j)

(cid:105)

zσ,q
ξ(σ,q,k)

nd(cid:89)

k=1

(µ,ζ,ξ)∈Θ
(cid:104) lσ(cid:89)

(cid:89)

σ,q

i=1

where δ is a Kronecker delta. With this in hand we may
compute

W0 = W

= W

∩

∩

(cid:92)

V(1

x∈X
(cid:92)

(cid:92)

x∈X

s(cid:54)=y

F x

y (w))

−

V(F x

s (w)) .

But F x
s is a polynomial with non-negative integer coefﬁ-
W . Hence
cients, which takes values in [0, 1] for w
it vanishes on w if and only if for each triple µ, ζ, ξ with
s = π(µ, ζ, ξ) one or more of the coordinate functions
µ(σ,q,i), yσ,q
xσ,q

ξ(σ,q,k) vanishes on w.

ζ(σ,q,j), zσ,q

∈

and
∈ X
Θ we have π(µ, ζ, ξ) = y so that F x
s = 0 for all
= y. But in this case case W0 = W which contradicts

The desired conclusion follows unless for every x
(µ, ζ, ξ)
s
Lemma B.1.

∈

C. Kolmogorov complexity: Proofs

Let λ be the RLCT of the triple (p, q, ϕ) associated to the
synthesis problem (Deﬁnition 3.4).

Theorem C.1. λ

1
2 (M + N )c(q).

≤

|

∩

∈

W code

Q(cid:48)

Proof. Let u
W0 be the code of a Turing ma-
chine realising the inﬁmum in the deﬁnition of the Kol-
mogorov complexity and suppose that this machine only
uses symbols in Σ(cid:48) and states in Q(cid:48) with N (cid:48) =
and
M (cid:48) =
. The time evolution of the staged pseudo-UTM
|
input is independent of the entries
simulating u on x
U
on the description tape that belong to tuples of the form
Σ(cid:48)
(σ, q, ?, ?, ?) with (σ, q) /
W be the
∈
⊆
submanifold of points which agree with u on all tuples with
Q(cid:48) and are otherwise free. Then u
(σ, q)
W0
and codim(V ) = M (cid:48)N (cid:48)(M + N ) and by (Watanabe, 2009,
1
2 codim(V ).
Theorem 7.3) we have λ

Q(cid:48). Let V

Σ(cid:48)
|

Σ∗

Σ(cid:48)

⊆

×

×

∈

∈

∈

V

|

≤

D. Hyperparameters

|T |

The hyperparameters for the various synthesis tasks are
contained in Table 2. The number of samples is R in Al-
gorithm 1 and the number of datasets is
. Samples are
taken according to the Dirichlet distribution, a probability
distribution over the simplex, which is controlled by the
concentration. When the concentration is a constant across
all dimensions, as is assumed here, this corresponds to a
density which is symmetric about the uniform probability
mass function occurring in the centre of the simplex. The
value α = 1.0 corresponds to the uniform distribution over
the simplex. Finally, the chain temperature controls the de-
fault β value, ie. all inverse temperature values are centered
around 1/T where T is the chain temperature.

Hyperparameter

detectA

parityCheck

Dataset size (n)
Minimum sequence length (a)
Maximum sequence length (b)
Number of samples (R)
Number of burn-in steps
Number of datasets (|T |)
Target accept probability
Concentration (α)
Chain temperature (T )
Number of timesteps (t)

200
4
7/8/9/10
20,000
1,000
4
0.8
1.0
log(500)/log(1000)
10

100
1
5/6/7
2,000
500
3
0.8
1.0
log(300)
42

Table 2. Hyperparameters for Datasets and MCMC.

(cid:54)
(cid:54)
(cid:54)
Geometry of Program Synthesis

E. The Shift Machine

The pseudo-UTM
is a complicated Turing machine,
U
and the models p(y
x, w) of Section 3 are therefore not
|
easy to analyse by hand. To illustrate the kind of geom-
etry that appears, we study the simple Turing machine
shiftMachine of (Clift & Murfet, 2018) and formulate
an associated statistical learning problem. The tape alphabet
is Σ =
and the input to the machine will
{
be a string of the form (cid:3)na1a2a3(cid:3) where n is called the
counter and ai ∈ {
. The transition function, given in
loc.cit., will move the string of A’s and B’s leftwards by n
steps and ﬁll the right hand end of the string with A’s, keep-
ing the string length invariant. For example, if (cid:3)2BAB(cid:3)
is the input to M , the output will be (cid:3)0BAA(cid:3).

(cid:3), A, B, 0, 1, 2
}

A, B

}

Set W = ∆
∆
0, 2
{
as representing a probability distribution (1
for the counter and (1

W
2
h)
A for a1. The model is

and view w = (h, k)

A, B
{

∈
0 + h

B + k

} ×

k)

−

}

·

·

·
x = (a2, a3), w(cid:1) = (1

−

·
h)2k

p(cid:0)y

|

h)2(1

(1

−

k)

·

−

B +

·
(cid:19)

A+

hi−1(1

h)3−i

ai.

·

−

1

3
(cid:88)

i=2

−
(cid:18) 2
i

−

This model is derived by propagating uncertainty through
shiftMachine in the same way that p(y
x, w) is de-
rived from ∆ stept in Section 3 by propagating uncertainty
through
. We assume that some distribution q(x) over
A, B

|

U
2 is given.
}

{
Example E.1. Suppose q(y
(1, 1). It is easy to see that

K(w) =

−

=

−

1
4

1
2

(cid:88)

a2,a3

log p(cid:0)y = a3|

x = (a2, a3), w(cid:1)

log[g(h, k)]

where g(h, k) = (cid:0)(1
is a polynomial in w. Hence

−

h)2k + h2(cid:1)(cid:0)(1

h)2(1

−

−

k) + h2(cid:1)

W0 =

(h, k)
{

W : g(h, k) = 1
}

∈

= V(g

1)

∩

−

[0, 1]2

is a semi-algebraic variety, that is, it is deﬁned by poly-
nomial equations and inequalities. Here V(h) denotes the
vanishing locus of a function h.

|

1
2 log(4f (1

Example E.2. Suppose q(AB) = 1 and q(y
2 A + 1
1
K(h, k) =
−
K = (f
2h(1
has no critical points, and so
if and only if f (h, k) = 1
w

x = AB) =
2 B. Then the Kullback-Leibler divergence is
h)2k +
f . Note that f
(0, 1)2
2 . Since K is non-negative, any

f )) where f = (1
1
1
2 )
f (1−f ) ∇
K = 0 at (h, k)

K(w) = 0 and so

−
h). Hence

W0 satisﬁes

−
∇

∇

−

−

∈

∈
W0 = [0, 1]2

∇
V(4f (1

∩

f )

−

−

1) = [0, 1]2

V(f

1
2 )

−

∩

−

is semi-algebraic. Note that the curve f = 1
2 is regular while
f ) = 1 is singular and it is the geometry of
the curve 4f (1
the singular curve that is related to the behaviour of K. This
curve is shown in Figure 5. It is straightforward to check
that the determinant of the Hessian of K is identically zero
on W0, so that every point on W0 is a degenerate critical
point of K.

Figure 5. Values of K(h, k) on [0, 1]2 are shown by colour, rang-
ing from blue (zero) to red (0.01). The singular analytic space
K = 0 (white) and the regular analytic level set K = 0.001
(black).

F. Parity Check

The deterministic synthesis problem parityCheck has

(cid:3), A, B, X
{

,

}

reject, accept, getNextAB,

Q =

{

getNextA, getNextB, gotoStart
}

.

|

The distribution q(x) is as discussed in Section 7 and q(y
x)
is determined by the function taking in a string of A’s and
B’s, and terminating in state accept if the string contains
the same number of A’s as B’s, and terminating in state
reject otherwise. The string is assumed to contain no blank
symbols. The true distribution is realisable because there
is a Turing machine using Σ and Q which computes this
function: the machine works by repeatedly overwriting pairs
consisting of a single A and B with X’s; if there are any A’s
without a matching B left over (or vice versa), we reject,
otherwise we accept.

In more detail, the starting state getNextAB moves right
on the tape until the ﬁrst A or B is found, and overwrites it
with an X. If it’s an A (resp. B) we enter state getNextB
(resp. getNextA). If no A or B is found, we enter the
state accept. The state getNextA (resp. getNextB) moves
right until an A (resp. B) is found, overwrites it with an X
and enters state gotoStart which moves left until a blank
symbol is found (resetting the machine to the left end of
the tape). If no A’s (resp. B’s) were left on the tape, we

x) = p(y

x, w0) where w0 =
|

|

Σ =

Geometry of Program Synthesis

enter state reject. The dimension of the parameter space is
dim W = 240. If this model were regular, the RLCT would
be dim W/2 = 120. Our RLCT estimates are contained in
Table 3 and Figure 6.

The UTM has four tapes numbered from 0 to 3, which
we refer to as the description tape, the staging tape, the
state tape and the working tape respectively. Initially the
description tape contains a string of the form

Max-length

Temperature

RLCT

Std

R squared

5
6
7

log(300)
log(300)
log(300)

4.411732
4.005667
3.887679

0.252458
0.365855
0.276337

0.969500
0.971619
0.973716

Table 3. RLCT estimates for parityCheck.

Figure 6. Plot of RLCT estimates for parityCheck. Shaded
region shows one standard deviation.

G. Staged Pseudo-UTM

Simulating a Turing machine M with tape alphabet Σ and
set of states Q on a standard UTM requires the speciﬁca-
tion of an encoding of Σ and Q in the tape alphabet of the
UTM. From the point of view of exploring the geometry of
program synthesis, this additional complexity is uninterest-
ing and so here we consider a staged pseudo-UTM whose
alphabet is

X, (cid:3)
}

ΣUTM = Σ

Q

L, R, S

∪

} ∪ {

∪ {
where the union is disjoint where (cid:3) is the blank symbol
(which is distinct from the blank symbol of M ). Such a
machine is capable of simulating any machine with tape
alphabet Σ and set of states Q but cannot simulate arbitrary
machines and is not a UTM in the standard sense. The
adjective staged refers to the design of the UTM, which we
now explain. The set of states is

QUTM =

compSymbol, compState, copySymbol,
{
copyState, copyDir,

compState,

copySymbol,

¬

¬

copyState,

copyDir, updateSymbol,

¬
updateState, updateDir, resetDescr
.
}

¬

Xs0q0s(cid:48)

0q(cid:48)

0d0s1q1s(cid:48)

1q(cid:48)

1d1 . . . sN qN s(cid:48)

N q(cid:48)

N dN X,

corresponding to the tuples which deﬁne M , with the tape
head initially on s0. The staging tape is initially a string
XXX with the tape head over the second X. The state
tape has a single square containing some distribution in ∆Q,
corresponding to the initial state of the simulated machine
M , with the tape head over that square. Each square on
the the working tape is some distribution in ∆Σ with only
ﬁnitely many distributions different from (cid:3). The UTM is
initialised in state compSymbol.

Figure 7. The UTM. Each of the rectangles are states, and an arrow
q → q(cid:48) has the following interpretation: if the UTM is in state q
and sees the tape symbols (on the four tapes) as indicated by the
source of the arrow, then the UTM transitions to state q(cid:48), writes
the indicated symbols (or if there is no write instruction, simply
rewrites the same symbols back onto the tapes), and performs
the indicated movements of each of the tape heads. The symbols
a, b, c, d stand for generic symbols which are not X.

The operation of the UTM is outlined in Figure 7. It consists
of two phases; the scan phase (middle and right path), and
the update phase (left path). During the scan phase, the
description tape is scanned from left to right, and the ﬁrst
two squares of each tuple are compared to the contents of

Geometry of Program Synthesis

the working tape and state tape respectively. If both agree,
then the last three symbols of the tuple are written to the
staging tape (middle path), otherwise the tuple is ignored
(right path). Once the X at the end of the description tape
is reached, the UTM begins the update phase, wherein the
three symbols on the staging tape are then used to print the
new symbol on the working tape, to update the simulated
state on the state tape, and to move the working tape head in
the appropriate direction. The tape head on the description
tape is then reset to the initial X.

Remark G.1. One could imagine a variant of the UTM
which did not include a staging tape, instead performing
the actions on the work and state tape directly upon reading
the appropriate tuple on the description tape. However, this
is problematic when the contents of the state or working
tape are distributions, as the exact time-step of the simulated
machine can become unsynchronised, increasing entropy.
As a simple example, suppose that the contents of the state
tape were 0.5q + 0.5p, and the symbol under the working
tape head was s. Upon encountering the tuple sqs(cid:48)q(cid:48)R, the
machine would enter a superposition of states corresponding
to the tape head having both moved right and not moved,
complicating the future behaviour.

We deﬁne the period of the UTM to be the smallest nonzero
time interval taken for the tape head on the description tape
to return to the initial X, and the machine to reenter the state
compSymbol. If the number of tuples on the description
tape is N , then the period of the UTM is T = 10N + 5.
Moreover, other than the working tape, the position of the
tape heads are T -periodic.

H. Smooth Turing Machines

U

be the staged pseudo-UTM of Appendix G. In deﬁn-
Let
ing the model p(y
x, w) associated to a synthesis problem
|
in Section 3 we use a smooth relaxation ∆ stept of the step
. In this appendix we deﬁne the smooth re-
function of
laxation of any Turing machine following (Clift & Murfet,
2018).

U

Let M = (Σ, Q, δ) be a Turing machine with a ﬁnite set of
symbols Σ, a ﬁnite set of states Q and transition function
. We write δi = proji ◦
δ
δ : Σ
1, 0, 1
Σ
}
. For (cid:3)
Σ, let
for the ith component of δ for i
1, 2, 3
}
∈ {

× {−

→

Q

Q

×

×

∈

is the step function deﬁned by

step(σ, q) =

(cid:16)

αδ3(σ0,q)(cid:0) . . . , σ−2, σ−1, δ1(σ0, q),
(cid:17)
σ1, σ2, . . . (cid:1), δ2(σ0, q)

.

(12)

with shift map αδ3(σ0,q)(σ)u = σu+δ3(σ0,q).
Let X be a ﬁnite set. The standard X-simplex is deﬁned as

∆X =

(cid:88)

{

x∈X

λxx

∈

RX

(cid:88)

|

x

λx = 1 and

λx ≥

0 for all x

X

}

∈

(13)

} ⊂

→
0, 1

∆(
{

0, 1
}

where RX is the free vector space on X. We often identify
X with the vertices of ∆X under the canonical inclusion i :
x(cid:48)∈X δx=x(cid:48)x(cid:48). For example
X

∆X given by i(x) = (cid:80)
)

{
Z if
A tape square is said to be at relative position u
it is labelled u after enumerating all squares in increasing
order from left to right such that the square currently under
the head is assigned zero. Consider the following random
variables at times t

[0, 1].

0:

(cid:39)

∈

≥

• Yu,t ∈
position u at time t.

Σ: the content of the tape square at relative

Q: the internal state at time t.

• St ∈
• W rt ∈
from time t to t + 1.

Σ: the symbol to be written, in the transition

• M vt ∈ {
L, S, R
tion from time t to t + 1.

: the direction to move, in the transi-
}

We call a smooth dynamical system a pair (A, φ) consisting
of a smooth manifold A with corners together with a smooth
transformation φ : A

A.

Deﬁnition H.1. Let M = (Σ, Q, δ) be a Turing machine.
The smooth relaxation of M is the smooth dynamical system
((∆Σ)Z,(cid:3)

∆Q, ∆step) where

→

×

∆step : (∆Σ)

Z,(cid:3)

∆Q

Z,(cid:3)

(∆Σ)

∆Q

a
is
(
P (Yu,t)
{
determined by the equations

smooth
}u∈Z, P (St))

×

→
transformation
to (
P (Yu,t+1)
{

×
sending

a
state
}u∈Z, P (St+1))

Z,(cid:3)

Σ

=

f : Z
{

Σ

f (i) = (cid:3) except for ﬁnitely many i
|

.
}

→

• P (M vt = d
C) =
|
(cid:80)

σ,q δδ3(σ,q)=dP (Y0,t = σ

C)P (St = q
|

|

C),

We can associate to M a discrete dynamical system (cid:99)M =
(ΣZ,(cid:3)

Q, step) where

×

step : Σ

Z,(cid:3)

Q

×

→

Z,(cid:3)

Σ

Q

×

• P (W rt = σ
(cid:80)

C) =
|

σ(cid:48),q δδ1(σ(cid:48),q)=σP (Y0,t = σ(cid:48)

C)P (St = q

|

C),

|

• P (St+1 = q
(cid:80)

|

C) =

σ,q(cid:48) δδ2(σ,q(cid:48))=qP (Y0,t = σ

C)P (St = q(cid:48)
|

|

C),

Geometry of Program Synthesis

• P (Yu,t+1 = σ
C) =
|
(cid:16)
P (M vt = L
|

C)

δu(cid:54)=1P (Yu−1,t = σ

C) +
|

δu=1P (W rt = σ

P (M vt = S

(cid:16)

C)

|

δu(cid:54)=0P (Yu,t = σ

δu=0P (W rt = σ

P (M vt = R

C)
|

(cid:16)

δu(cid:54)=−1P (Yu+1,t = σ

(cid:17)

C)
|
C) +
|

(cid:17)

C)
|
C) +
|

U

where the ﬁrst factor is the conﬁguration of the staging tape.
is periodic of period T = 10N + 5 (Appendix G)
Since
the iterated function (∆ stepU )T takes an input with staging
tape in its default state XXX and UTM state compSymbol
and returns a conﬁguration with the same staging tape and
state, but with the conﬁguration of the work tape, description
tape and state tape updated by one complete simulation step.
That is,

+

+

δu=−1P (W rt = σ

C)
|

(cid:17)

,

(∆ stepU )T (cid:0)x, w, q,XXX, compSymbol) =

(F (x, w, q), XXX, compSymbol(cid:1)

where C

∈

(∆Σ)Z,(cid:3)

×

∆Q is an initial state.

We will call the smooth relaxation of a Turing machine a
smooth Turing machine. A smooth Turing machine encodes
uncertainty in the initial conﬁguration of a Turing machine
together with an update rule for how to propagate this un-
certainty over time. We interpret the smooth step function
as updating the state of belief of a “naive” Bayesian ob-
server. This nomenclature comes from the assumption of
conditional independence between random variables in our
probability functions.

Remark H.2. Propagating uncertainty using standard prob-
ability leads to a smooth dynamical system which encodes
the state evolution of an “ordinary” Bayesian observer of
the Turing machine. This requires the calculation of various
joint distributions which makes such an extension computa-
tionally difﬁcult to work with. Computation aside, the naive
probabilistic extension is justiﬁed from the point of view
of derivatives of algorithms according to the denotational
semantics of differential linear logic. See (Clift & Murfet,
2018) for further details.

We call the smooth extension of a universal Turing machine
a smooth universal Turing machine. Recall that the staged
pseudo-UTM
has four tapes: the description tape, the
staging tape, the state tape and working tape. The smooth
relaxation of

is a smooth dynamical system

U

U

∆stepU : [(∆ΣUTM)

Z,(cid:3)

]4

×

∆QUTM
Z,(cid:3)

]4

[(∆ΣUTM)

→
∆QUTM .

×

If we use the staged pseudo-UTM to simulate a Turing
machine with tape alphabet Σ
⊆
ΣUTM then with some determined initial state the function
∆step restricts to

ΣUTM and states Q

⊆

∆stepU : (∆Σ)

Z,(cid:3)

W

×
(∆Σ)

×
Z,(cid:3)

∆Q

× X →
∆Q
W

×

×

× X

where the ﬁrst factor is the conﬁguration of the work tape,
W is as in (4) and

for some smooth function

F : (∆Σ)

Z,(cid:3)

W

∆Q

Z,(cid:3)

(∆Σ)

W

∆Q . (14)

×

×

→
Finally we can deﬁne the function ∆ stept of (5). We as-
sume all Turing machines are initialised in some common
state init

×

×

Q.

∈

Deﬁnition H.3. Given t

0 we deﬁne

≥

∆ stept : Σ∗

W

×

→

∆Q

by

∆ stept(x, w) = ΠQF t(x, w, init)

where ΠQ is the projection onto ∆Q.

I. Direct Simulation

For computational efﬁciency in our PyTorch implementation
of the staged pseudo-UTM we implement F of (14) rather
than ∆ stepU . We refer to this as direction simulation since
it means that we update in one step the state and working
tape of the UTM for a full cycle where a cycle consists of
T = 10N + 5 steps of the UTM.

Let S(t) and Yu(t) be random variables describing the con-
tents of state tape and working tape in relative positions 0, u
0 time steps of the UTM. We deﬁne
respectively after t
(cid:101)S(t) := S(4 + T t) and (cid:101)Yu(t) := Yu(4 + T t) where t
0
Z. The task then is to deﬁne functions f, g such
and u
that

≥

≥

∈

(cid:101)S(t + 1) = f ( (cid:101)S(t))

(cid:101)Yu(t + 1) = g( (cid:101)Yu(t)).

The functional relationship is given as follows: for 1
≤
N indexing tuples on the description tape, while pro-
i
≤
cessing that tuple, the UTM is in a state distribution λi ·
¯q +
.
copySymbol, copyState, copyDir
(1
}
−
Given the initial state of the description tape, we assume
uncertainty about s(cid:48), q(cid:48), d only. This determines a map

¯q where ¯q

∈ {

λi)

·¬

= [(∆ΣUTM)

Z,(cid:3)

X

∆QUTM

]

×

θ :

1, . . . , N
{

} →

Q

Σ

×

Geometry of Program Synthesis

where the description tape at tuple number i is given by
θ(i)1θ(i)2P (s(cid:48)
i)P (q(cid:48)
i)P (di). We deﬁne the conditionally
by
independent joint distribution between

(cid:101)Y0,t−1, (cid:101)St−1}
{
δθ(i)2=qP ( (cid:101)St−1 = q)

λi =

(cid:88)

σ∈Σ

δθ(i)1=σP ( (cid:101)Y0,t−1 = σ)

·

(cid:88)

q∈Q

= P ( (cid:101)Y0,t−1 = θ(i)1)

P ( (cid:101)St−1 = θ(i)2).

·

We then calculate a recursive set of equations for 0
≤
N describing distributions P (ˆsj), P (ˆqj) and P ( ˆdj) on the
staging tape after processing all tuples up to and including
tuple j. These are given by P (ˆs0) = P (ˆq0) = P ( ˆd0) =
1

X and

≤

j

·
P (ˆsi) =

(cid:88)

λi ·

{
σ∈Σ
P (ˆsi−1 = σ)

P (s(cid:48)

i = σ) + (1

λi)
·

−

σ + (1

λi)

·

−

P (ˆsi−1 = X)

X,

·

} ·

P (ˆqi) =

(cid:88)

{
q∈Q

λi ·

P (q(cid:48)

i = q) + (1

λi)

·

−

P (ˆqi−1 = q)

} ·

q + (1

λi)

·

−

P (ˆqi−1 = X)

X,

·

P (ˆsN = σ) =

N
(cid:88)

j=1

λj ·

P (s(cid:48)

j = σ)

N
(cid:89)

(1

l=j+1

λl),

−

P (ˆqN = X) =

N
(cid:89)

(1

j=1

λj),

−

P (ˆqN = q) =

N
(cid:88)

j=1

λj ·

P (q(cid:48)

j = q)

N
(cid:89)

(1

l=j+1

λl),

−

P ( ˆdN = X) =

N
(cid:89)

(1

j=1

λj),

−

P ( ˆdN = a) =

N
(cid:88)

j=1

λj ·

P (dj = a)

N
(cid:89)

(1

l=j+1

λl).

−

To enable efﬁcient computation, we can express these equa-
RN .
tions using tensor calculus. Let λ = (λ1, . . . , λN )
We view

∈

as a tensor and so

θ : RN

RΣ

RQ

→

⊗

θ =

N
(cid:88)

i=1

i

θ(i)1 ⊗

θ(i)2 ∈

⊗

RN

RΣ

⊗

⊗

RQ.

P ( ˆdi) =

(cid:88)

a∈{L,R,S}
P ( ˆdi−1 = a)

P (di = a) + (1

λi)

·

−

Then
θ(cid:121)

(cid:16)

P ( (cid:101)Y0,t−1)

⊗

P ( (cid:101)St−1)

(cid:17)

=

λi ·
{

a + (1

λi)

·

−

P ( ˆdi−1 = X)

X.

·

N
(cid:88)

i

} ·

P ( (cid:101)Y0,t−1 = θ(i)1)

Let Aσ = P (ˆsN = X)
terms of the above distributions

·

P ( (cid:101)Y0,t−1 = σ) + P (ˆsN = σ). In

P ( (cid:101)St) =

(cid:88)

(cid:16)

q∈Q

P (ˆqN = X)

·

P ( (cid:101)St−1 = q)

+ P (ˆqN = q)

(cid:17)

q

·

= λ

and

P ( (cid:101)Yu,t = σ) =
P ( ˆdN = L)

P ( ˆdN = R)

P ( ˆdN = S)

(cid:16)

(cid:16)

P ( ˆdN = X)

δu(cid:54)=1P ( (cid:101)Yu−1,t−1 = σ) + δu=1Aσ
(cid:16)

δu(cid:54)=−1P ( (cid:101)Yu+1,t−1 = σ) + δu=−1Aσ

+
(cid:17)

+

δu(cid:54)=0P ( (cid:101)Yu,t−1 = σ) + δu=0Aσ
(cid:16)

δu(cid:54)=0P ( (cid:101)Yu,t−1 = σ) + δu=0Aσ

(cid:17)

(cid:17)

+

(cid:17)

.

·

i=1
If we view P (s(cid:48)

∗ =

P (ˆsN ) =

N
(cid:88)

j=1

)
•

∈

RN



⊗

P (s(cid:48)

j =

λj

)

•

·

P ( (cid:101)St−1 = θ(i)2) = λ.

·

RΣ as a tensor, then

N
(cid:89)

(1

l=j+1



λl)



−

(cid:32) N
(cid:89)

(1

·

−

N
(cid:89)

(1

λl),

λl), . . . , (1

−

−

(cid:33)

λN ), 1

l=2

l=3
can be expressed in terms on the vector λ only. Similarly,
P (q(cid:48)

RN

)

∗ =

RQ with


⊗

•

∈
N
(cid:88)

j=1

P (ˆqN ) =

P (q(cid:48)

j =

λj

)
•

·

(cid:32) N
(cid:89)

(1

l=2

−

= λ

·

and P (d∗ =

RN

)

•

∈

⊗

N
(cid:89)

(1

λl),

l=3
R3 with


N
(cid:89)

(1

l=j+1



λl)



−

(cid:33)

λl), . . . , (1

λN ), 1

−

−

Using these equations, we can state efﬁcient update rules
for the staging tape. We have

P ( ˆdN ) =

N
(cid:88)

j=1

P (dj =

λj

)
•

·

N
(cid:89)

(1

l=j+1



λl)



−

P (ˆsN = X) =

N
(cid:89)

(1

j=1

λj),

−

(cid:32) N
(cid:89)

(1

l=2

λl),

−

N
(cid:89)

(1

l=3

−

= λ

·

(cid:33)

λl), . . . , (1

λN ), 1

.

−

