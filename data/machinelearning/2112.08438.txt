Programmatic Reward Design by Example

Weichao Zhou and Wenchao Li

Boston University
{zwc662,wenchao}@bu.edu

2
2
0
2

n
a
J

7

]

G
L
.
s
c
[

2
v
8
3
4
8
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

Reward design is a fundamental problem in reinforce-
ment learning (RL). A misspeciﬁed or poorly designed
reward can result in low sample efﬁciency and un-
desired behaviors. In this paper, we propose the idea
of programmatic reward design, i.e. using programs
to specify the reward functions in RL environments.
Programs allow human engineers to express sub-goals
and complex task scenarios in a structured and inter-
pretable way. The challenge of programmatic reward
design, however, is that while humans can provide the
high-level structures, properly setting the low-level de-
tails, such as the right amount of reward for a spe-
ciﬁc sub-task, remains difﬁcult. A major contribution
of this paper is a probabilistic framework that can infer
the best candidate programmatic reward function from
expert demonstrations. Inspired by recent generative-
adversarial approaches, our framework searches for the
most likely programmatic reward function under which
the optimally generated trajectories cannot be differen-
tiated from the demonstrated trajectories. Experimen-
tal results show that programmatic reward functions
learned using this framework can signiﬁcantly outper-
form those learned using existing reward learning algo-
rithms, and enable RL agents to achieve state-of-the-art
performance on highly complex tasks.

Introduction
Reward signals are an integral part of reinforcement learning
(RL). Most conventional reward functions are goal-driven –
they reward the agent only at the end of each episode. The
sparsity of such reward signals, however, can make RL al-
gorithms highly inefﬁcient. As the complexity of the task
increases, it becomes difﬁcult for the agent to grasp the intri-
cacies of the task solely from a goal-driven reward (Amodei
et al. 2016).

Inverse Reinforcement Learning (IRL)

is a general
paradigm that aims at recovering the intrinsic reward func-
tion of human experts from their demonstrations (Ng and
Russell 2000; Ziebart et al. 2008). Earlier works of IRL re-
quire the provision of multiple feature functions to construct
the reward function. More recent attempts use function ap-
proximation by means of deep neural networks to alleviate
this limitation and have considerable success (Fu, Luo, and
Levine 2018; Finn, Levine, and Abbeel 2016; Finn et al.

2016). However, due to the lack of interpretability of the
approximated reward functions, it is difﬁcult to enforce spe-
ciﬁc correctness constraints in the reward learning process.
Recent works proposed logic-based reward functions (Li,
Vasile, and Belta 2017; Camacho et al. 2019) to endow an
RL agent with high-level knowledge of the task via logi-
cal speciﬁcations. The logic-constrained reward (Hasanbeig,
Abate, and Kroening 2019) and reward machines (Icarte
et al. 2018) explicitly represents reward functions as au-
tomata. However, it is still cumbersome to design the au-
tomata and they can be difﬁcult to understand when the
size of the automata are large.
In this paper, we propose
programmatic reward functions, i.e. reward functions rep-
resented as programs expressed in human-readable domain
speciﬁc language (DSL). There are several beneﬁts of using
programmatic reward functions for RL policy training. First,
programs allow human engineers to express sub-goals, com-
plex task scenarios in a structural and interpretable way. The
inclusion of such domain knowledge forms inductive biases
that help improve the sample efﬁciency and performance of
RL agents. Second, engineers can take advantage of the rich
semantics of DSLs to explicitly memorize, manipulate and
leverage hindsight experiences of the RL agent. Lastly, pro-
grams are amenable to the speciﬁcation of symbolic con-
straints over the holes. In a typical design routine of a pro-
grammatic reward function, we assume that an engineer can
provide human insights in the form of a partial program,
or a sketch (Solar-Lezama 2008), analogous to human pro-
viding feature functions in IRL and logical speciﬁcations in
logic-based reward designs, to express the high-level struc-
tures of the task speciﬁcation or reward function. The sketch
in essence deﬁnes the set of events and certain interactions
among them that the human engineer deem relevant to the
task. The low-level details, such as the right amount of re-
ward for a speciﬁc event or sub-task, are left as holes.

Similar to Programming by Example (PBE) (Menon et al.
2013), we propose to infer the holes in a programmatic re-
ward sketch from expert demonstrated trajectories. A key
difference of our approach from PBE is that the demon-
strated trajectories do not directly correspond to inputs or
(intermediate) outputs of the program, but instead are as-
sumed to be generated by an unknown expert policy that is
optimal under some realization of the programmatic reward
function. A major contribution of this paper is a probabilistic

 
 
 
 
 
 
learning algorithm that can complete a given programmatic
reward sketch based on expert demonstrations. Our overall
framework, called Programmatic Reward Design by Exam-
ple (PRDBE), consists of three components: a set of exam-
ple trajectories demonstrated by a human expert, a program
sketch and a symbolic constraint that the complete program
should satisfy. Directly searching in the program space is a
combinatorial problem and can easily become intractable.
Our approach is to search for the most likely program that
matches the expert’s intrinsic reward function. Our solu-
tion is inspired by generative adversarial approaches (Finn
et al. 2016; Jeon, Seo, and Kim 2018) which introduce a dis-
criminator to distinguish agent trajectories from the expert’s.
However, instead of formulating an agent’s policy as a gen-
erative model, we sample trajectories that are optimal under
a candidate programmatic reward function and iteratively
improve the candidate program to maximize the chance of
the discriminator making false predictions on those trajecto-
ries. To circumvent the issue of non-differentiability of pro-
grams, we employ a sampler to sample candidate programs
from the space of valid programs. In particular, we use self-
normalized importance sampling to sample trajectories from
an agent’s policy. We summarize our contributions below.

• We propose Programmatic Reward Design by Example
(PRDBE), a novel paradigm to design and learn program-
like reward functions for RL problems.

• We develop a probabilistic learning framework that can
infer the most likely candidate reward program from ex-
pert demonstrations.

• Our approach enables RL agents to achieve state-of-the-
art performance on highly complex environments with
only a few demonstrations. In addition, we show that pro-
grammatic reward functions generalize across different
environment conﬁgurations of the same task.

Related Work

Inverse Reinforcement Learning. IRL (Ng and Russell
2000; Abbeel and Ng 2004) instantiates a learning-from-
demonstrations (LfD) framework to effectively infer the in-
trinsic reward functions of human experts. It is also no-
table for having inﬁnite number of solutions. To resolve the
ambiguity of IRL, max-entropy (Ziebart et al. 2008), max-
margin (Abbeel and Ng 2004; Ratliff, Bagnell, and Zinke-
vich 2006) and Bayesian (Ramachandran and Amir 2007)
methods have been proposed. However, those IRL methods
assume linear rewards on the basis of non-linear state fea-
tures, and also call RL in a loop to repeatedly solve the en-
tire environments. The recent works (Fu, Luo, and Levine
2018; Ho and Ermon 2016; Jeon, Seo, and Kim 2018; Finn
et al. 2016), drawing a connection between IRL and Genera-
tive Adversarial Networks (GANs) (Goodfellow et al. 2014),
have achieved substantially improved the scalablity of IRL
by using deep neural networks and data-driven approaches.
Our work, while embracing a data-driven ideology, repre-
sents the reward function in an interpretable way.
Reward Design. Reward shaping (Ng, Harada, and Russell
1999) highly speeds up RL training by modifying a sparse

reward functions with state-based potential functions. Intrin-
sic reward generation (Bellemare et al. 2016; Pathak et al.
2017; Alshiekh et al. 2017) and adversarially-guided (Flet-
Berliac et al. 2021) techniques aims at motivating the agent
to exhaustively explore the environments. Unlike these ap-
proaches, our work aims at capturing human knowledge in
the reward function and does not generate uninterpretable
reward signals densely ranging over the entire state space.
Logic-based reward designs (Li, Vasile, and Belta 2017;
Hasanbeig, Abate, and Kroening 2019; Camacho et al. 2019)
present human knowledge in reward functions with speciﬁ-
cation languages such as linear temporal logic (LTL)(Baier
and Katoen 2008). Reward machine theories (Icarte et al.
2020) further directly represent the reward functions as ﬁ-
nite state automata which can be translated into logic formu-
las. Our work distinguishes itself by 1) using programming
languages instead of logics to expressively represent human
insights in the reward functions; 2) adopting LfD to imple-
ment low-level details in the reward functions. Regarding
LfD, Inverse reward design (IRD) (Hadﬁeld-Menell et al.
2017) design reward functions in a manner similar to IRL.
Safety-aware apprenticeship learning (Zhou and Li 2018) in-
corporate formal speciﬁcation and formal veriﬁcation with
IRL. However, those works restrict the reward function to
be a linear combination of state features. Our work does not
have such limitations.
Interpretable Reinforcement Learning. Learning inter-
pretable RL policies has drawn continual attention (An-
dre and Russell 2001, 2002; Verma et al. 2018; Zhu et al.
2019; Yang et al. 2021; Tian et al. 2020). Our work fo-
cuses on programmatic reward functions instead of poli-
cies because well-designed reward functions can beneﬁt
diverse RL training methods. There have been a variety
of works on learning-based program synthesis (Ellis et al.
2021; Parisotto et al. 2016; Itzhaky et al. 2010; Gulwani and
Jain 2017; Ellis, Solar-Lezama, and Tenenbaum 2015).Our
work is inspired by the concept of sketch synthesis (Solar-
Lezama 2008). Realizing sketch synthesis from example for
reward function is our main contribution in this paper.

Background
Reinforcement Learning. RL problems model an envi-
ronment as a Markov Decision Process (MDP) M :=
(cid:104)S, A, P, d0(cid:105) where S is a state space, A is an action space,
P(s(cid:48)|s, a) is the probability of reaching a state s(cid:48) by per-
forming an action a at a state s, d0 is an initial state dis-
tribution. A trajectory τ = s(0)a(0)s(1)a(1) . . . s(T )a(T ) is
produced by sequentially performing actions for T steps af-
ter starting from an initial state s(0) ∼ d0. An RL agent can
select actions according to a control policy π(a|s) which de-
termines the probability of performing action a at any state
s. A reward function is a mapping f : S × A → R from
state-action pairs to the real space. With a slight abuse of no-
tations, we also represent the total reward and the joint prob-
ability along a trajectory τ as f (τ ) = (cid:80)T
t=0 f (s(t), a(t)) and
π(τ ) = (cid:81)T
t=0 π(a(t)|s(t)) respectively. Let H(π) be the en-
tropy of π. The objective of entropy-regularized RL (Levine
2018) is to minimize JRL(π) = Eτ ∼π[f (τ )] − H(π).

Learning from Demonstrations. When the reward func-
tion is not given but a set E of expert trajectories τE’s
is demonstrated by some unknown expert policy πE,
GAIL(Ho and Ermon 2016) trains an agent policy πA to
imitate πE via a generative adversarial objective as in (2)
where a discriminator D : S × A → [0, 1] is trained to max-
imize (2) so that it can identify whether an (s, a) is sampled
from τE’s; πA as the generator is optimized to minimize
(2) so that its trajectories τA’s are indistinguishable from
τE’s. Bayesian GAIL (Jeon, Seo, and Kim 2018) labels any
expert trajectory τE with 1E and 0E to respectively indicate
classifying τE as being sampled from E or not. Likewise,
1A and 0A indicate classifying any agent trajectory τA
as from E or not. Bayesian GAIL learns the most likely
D that makes the correct classiﬁcations 0A, 1E in terms
of p(D|0A, 1E; πA; E) ∝ p(D)p(0A, 1E|πA, D; E) ∝
(cid:80)
where p(1|τ ; D) = (cid:81)T
t=0 D(s(t), a(t)). Its logarithm (1)
is lower-bounded by (2) due to Jensen’s inequality. It is
shown in (Fu, Luo, and Levine 2018) that by representing
D(s, a) :=
exp(f (s,a))+πA(a|s) with a neural network f , (2)
is maximized when f ≡ log πE, implying that f is a reward
function under which πE is optimal. Hence, by representing
D in (1) and (2) with f , an IRL objective of solving the
most likely expert reward function f is obtained.

p(τA|πA)p(0A|τA; D) (cid:80)

p(τE|E)p(1E|τE; D)

exp(f (s,a))

τA

τE

log (cid:80)
τE

p(τE|E)p(1E|τE; D) (cid:80)
τA

p(τA|πA)p(0A|τA; D)(1)

≥

E
τE ∼E

(cid:104)

log

T
(cid:81)
t=0

D(s(t)

(cid:105)
E , a(t)
E )

+

E
τA∼πA

(cid:104)

log

T
(cid:81)
t=0

1 − D(s(t)

(cid:105)
A , a(t)
A )

(2)

Program Synthesis by Sketching. In the sketch synthesis
problem, a human designer provides a sketch e, i.e., an in-
complete program wherein certain details are left empty.
Each unknown detail is called a hole and denoted as ?id,
indexed by id in order of its appearance in e. The formal-
ism of the sketches follows a general grammar Λ as in (3)
and (4) where G is a family of functions customized with
domain-speciﬁc knowledge for the tasks; x represents the in-
put argument of the program. The grammar Λ induces a set
E of syntactically allowable sketches. When a sketch e ∈ E
is given by the designer, ?e = {?1, ?2, . . .} denotes an or-
dered set of the holes appearing in e. Let H be a set of
possible assignments to ?e. The sketch e and H induce a
set L = {l (cid:44) e[h/?e]|h ∈ H} of complete programs where
e[h/?e] means substituting ?e in e with an assignment h. Be-
sides the syntax, the program l is required to be a function
with a list type [(S, A)] argument. A valid assignment to the
argument can be a list-represented trajectory, i.e., by writ-
ing τ = s(0)a(0) . . . s(t)a(t) as [(s(0)a(0)), . . . , (s(t), a(t))].
Hereinafter we refer to τ either as a sequence of state-action
pairs or as a list of state-action tuples depending on the con-
texts. The rules in (5) and (6) deﬁne the semantics of Λ
wherein (6) replaces x with τ in g(e1, . . . , en). The result
of directly applying input τ to a sketch e is written as [[e]](τ )
which induces a partial program with ?e as free variables.

(a)

(b)

(c)

(d)

Figure 1: (a) MiniGrid 8x8 Door-Key; (b) A programmatic
reward sketch for (a);(c) A reward function as a ﬁnite state
automaton; (d) a programmatic reward sketch for (c)

For any complete program l, given a trajectory input τ , the
output [[l]](τ ) is required to be a |τ |-length real-valued list.

Sketch e := u | g(e1, . . . , en)
T erm u := const | ?id | x

g ∈ G

(3)
(4)

[[const]](τ ) := const [[?id]](τ ) := ?id [[x]](τ ) := τ (5)
(6)

[[g(e1, . . . , en)]](τ ) := g(e1, . . . , en)[τ /x]

Motivating Example
In this section, we motivate the problem of programmatic
reward design with the pseudo-code of two programmatic
reward function sketches, one for a navigation task in a grid-
world and the other one for representing a reward function
that has been formulated as a ﬁnite state automaton (FSA).
For those two tasks, we assume that the domain expert pro-
vides a set of tokens such as reach goal, unlock door rep-
resenting the behavior of the agent, and A, B, C representing
the FSA states. A predicate function pred(·) ∈ G of trajec-
tory can output those tokens according to the last state-action
in the input trajectory.
Example 1. (Door-Key Task) Fig.1a is an 8 × 8 Door-
Key task in a Mini-Grid environment (Chevalier-Boisvert,
Willems, and Pal 2018). An agent needs to pick up a key, un-
lock the yellow door on the grey wall and reach the green
goal tile. In every step, the agent can observe at most 7 × 7
area in front if the area is not blocked by walls and doors.
By default, the environment only returns a reward when the
agent reaches the goal tile.

A sketch reward fn of programmatic reward function for
Example 1 is shown in Figure1b. The holes {?id}5
id=1 are
unknown numerical values. The input argument is written

as traj instead of x for readability. The output is a real-
valued list concatenating the results of each recursive call to
reward fn. The elements in the output list are the very re-
wards for the state-action pairs in the input trajectory in suc-
cessive steps. This sketch responds to events such as reach-
ing the goal and unlocking the door. We note that this pro-
grammatic reward function, whereas sparse in appearance
considering the large state space, differs from the default
goal-driven reward by informing the agent the stage-wise
completion of the task. Line 7, 10 and 13 explicitly lever-
age hindsight experience to determine reward for the current
step. Critically, ?id’s in Figure1b ought not to be arbitrarily
assigned. Suppose that a penalty ?5 ≤ 0 for dropping the key
is less than a award ?4 ≥ 0 for picking up the key. The agent
would repeatedly pick up and drop the key to obtain high
net gain ?5+?4 ≥ 0 instead of reaching for the goal state.
Besides, we observe that the penalties for redundant actions
such as closing the door ought not to be overwhelmingly
high. Otherwise an under-trained agent may be incentivized
to reside away from the door for good.
Example 2. (Reward Function as A Finite State Automa-
ton (FSA)) In Fig.1c, a reward function for some RL prob-
lem is represented as an FSA comprised of at least 3 states
A, B, C among which A indicates task initialization, B in-
dicates task completion, C indicates the occurrence of some
other event of interests. Each directed edge represents a
state-transition triggered by a step in the environment. Other
states and transitions are omitted. Each transition is an-
notated with (pid, ?id) where pid is an unknown transition
probability dependent on both the environment and the RL
policy; ?id is the reward sent to the agent at the moment of
the transition. An RL agent is supposed to accomplish the
task with minimal amount of steps in the environment. Note
that the states in Fig.1c are not to be confused with the MDP
states in the environment; one step in the environment does
not necessarily trigger the transitions drawn in Fig.1c either.

FSAs such as the one in Example 2 are explicitly or im-
plicitly adopted in several logic-based reward designs such
as reward machines (Icarte et al. 2018). Such an FSA can
be represented by the sketch in Fig.1d by introducing a lo-
cal variable q as the FSA state. The key problem again is to
determine appropriate values for the ?id’s. We discuss the
challenges facing this problem in the next section.

Problem Formulation
In this section we augment the concept of sketching. Then
we characterize the problem of Programmatic Reward De-
sign (PRD) and discuss the challenges in PRD.

Sketching with Symbolic Constraints
Given a sketch e, a designer can put constraints on ?e. A
symbolic constraint c is a combination of multiple predi-
cates c := µ | ¬µ | c1 ∧ c2 | c1 ∨ c2 where the atomic pred-
icate µ : H → {(cid:62), ⊥} is the basic building block. Under
Boolean operations, the predicates follow the semantics (7)
where the expression µ[h/?e] substitutes ?e in µ with h to
output Boolean values. A satisfying implementation of the
mapping eval : {(cid:62), ⊥} →R is eval(·) := 2I(·) − 1 where

I : {(cid:62), ⊥} → {0, 1} is an indicator function. A symbolic
constraint c is satisﬁed if [[c]](h) ≥ 0. Table 1 instantiates
two predicates c1, c2 for Example 1. Then c = c1 ∧ c2 can
be a symbolic constraint for Example 1. For simplicity, we
only consider conjunctions of atomic predicates in this pa-
per. Now we are now ready to state the Programmatic Re-
ward Design problem as in Deﬁnition 1.

Properties
[c1]Reward reaching the goal
[c2]Penalize dropping unused key

Predicates
∧5
?5+?4 ≤ 0

id=1(?id ≤ ?1)

Table 1: The correspondence between two desired properties
and the predicates for the sketch in Fig.1b.

[[µ]](h) := eval(µ[h/?e])

[[¬µ]](h) := −[[µ]](h)

[[c1 ∧ c2]](h) := min([[c1]](h), [[c2]](h))
[[c1 ∨ c2]](h) := max([[c1]](h), [[c2]](h))

(7)

Deﬁnition 1 (Programmatic Reward Design (PRD)). For
an RL task, a PRD problem is a tuple (cid:104)e, ?e, H, c(cid:105) where
e is a sketch with holes ?e that takes values from set H;
c is a symbolic constraint. The solution to a PRD prob-
lem (cid:104)e, ?e, H, c(cid:105) is any valid program l(cid:44) e[h/?e] subject
to h ∈ H ∧ [[c]](h) ≥ 0.

Challenges in PRD
We note that solving the PRD problem does not guaran-
tee that the resulting reward function will be effective. The
challenge of assigning proper values to the holes is faced
not only by PRD but also by other symbolic reward design
approaches. We use Example 2 to illustrate two aspects of
this challenge. A) Goal-driven rewards. The reward func-
tion speciﬁed via logically guided approaches (Hasanbeig,
Abate, and Kroening 2019) can usually be translated into
transition systems similar to the one in Fig.1c. Many of those
approaches end up only assigning non-trivial values to ?1
and ?4 while equalizing the rewards for all the other tran-
sitions. However, when p1 and p4 are extremely small, e.g.
p1, p4 (cid:28) p2, p3, such goal-driven reward assignment barely
provide any guidance when the agent explores the environ-
ment. B) Unknown dynamics. The reward shaping approach
from (Camacho et al. 2019) adopts reward structures similar
to the one in Fig.1c but allocates rewards to all transitions
while ignoring the dynamics of the environment. This ap-
proach may result in inefﬁciency in large environments. For
instance, if p2p4 < p1 (cid:28) p2, a global optimal policy would
only aim at triggering one transition A → B. However, the
shaped reward may assign a non-trivial reward to ?2, causing
the RL algorithm to spend excessive training episodes on a
local-optimal policy that lingers over A → C → B.

Given a sketch such as the one in Fig.1d, a PRD designer
would also face the those challenges. Thus we assume that
a PRD problem is accompanied by a set of demonstrations
that show how an expert can accomplish the task, similar to
the setting of IRL. These demonstrations will help narrow
down the solutions to the PRD problem. We hereby propose
Programmatic Reward Design by Example (PRDBE).

Programmatic Reward Design by Example
Similar to Bayesian GAIL introduced in the Background
section, we consider a probabilistic inference perspective for
formulating PRDBE. We ﬁrst introduce a term to correlate
programmatic reward functions with trajectory distributions.

Deﬁnition 2 (Nominal Trajectory Distribution). Given a
programmatic reward function l, a nominal trajectory dis-
tribution of l is ˆp(τ |l) = p(τ ) exp(l(τ )) where p(τ ) is
the probability of sampling τ under the passive dynamics
of the environment; l(τ ) is short for (cid:80)
t[[l]](τ )[t]. Further-
more, a normalized nominal trajectory distribution of l is
p(τ |l) = p(τ ) exp(l(τ ))/Zl where Zl = (cid:80)

τ p(τ )ˆp(τ |l).

The nominal

trajectory distribution ˆp(τ |l) can be
viewed as a proxy of a possibly non-Markov policy
πl(a(t)|s(t)) ∝ exp([[l]](τ )[t]). Such a policy trivially mini-
mizes JRL(πl)(Levine 2018), the RL loss described in the
Background section. Intuitively, we search for an l∗ such
that πl∗ matches πE. Given this intuition, we formally deﬁne
the problem of Programmatic Reward Design by Example in
Deﬁnition 3.

Deﬁnition 3 (Programmatic Reward Design by Example
(PRDBE)). Given a set of expert demonstrated trajectories
E and a PRD problem (cid:104)e, ?e, H, c(cid:105), the PRDBE problem
(cid:104)e, ?e, H, c, E(cid:105) is to ﬁnd a solution l∗ to the PRD problem
such that for any τ the nominal trajectory distribution satis-
ﬁes ˆp(τ |l∗) = p(τ |l∗) = pE(τ ) where pE is the probability
of sampling τ from E.

However, solving the PRDBE problem requires address-
ing the following challenges: a) the set of solutions to the
PRD problem may not contain a satisfying solution l∗ for
PRDBE, and b) the sketch may not be differentiable w.r.t
the holes. In other words, there may not exist a perfect so-
lution to the PRDBE problem and gradient-based optimiza-
tions may not be readily applicable to PRDBE. To overcome
these issues, we propose a learning framework with a re-
laxed objective.

A Generative Adversarial Learning Framework
Our learning framework realizes the probability match-
ing between ˆp(τ |l), p(τ |l) and pE(τ ) in a generative-
adversarial fashion. It searches for an l such that even the
best discriminator represented with a reward function f as
mentioned in the Background section cannot distinguish
trajectories sampled by pE from those by p(τ |l). Given an
intermediate, learned reward function f , while Bayesian
GAIL trains a πA to minimize the log-likelihood (1) of cor-
rect classiﬁcations between agent trajectory τA and expert
trajectory τE, we learn an l to maximize the log-likelihood
log p(1A, 0E|l, f ; E) =
of
i.e.,
log (cid:80)
pE(τE)p(0E|τE; l, f )
A ,a(t)
with p(1A|τA; l, f ) := (cid:81)T
and p(0E|τE; l, f ) := (cid:81)T
E ))+exp([[l]](τE )[t])
being the discriminators; p(τA|l) being the generator of
τA’s. Since the l’s are non-differentiable, we do not directly
optimize l. Recalling that L is the program space induced by

exp(f (s(t)
A ,a(t)
exp([[l]](τE )[t])
E ,a(t)

A ))
A ))+exp([[l]](τA)[t])

p(τA|l)p(1A|τA; l, f ) (cid:80)

false classiﬁcations,

exp(f (s(t)

exp(f (s(t)

t=0

t=0

τA

τE

H, we optimize a sampler q : L → [0, 1] to concentrate the
distribution density on those candidate programs l’s that in-
cur high values of log p(1A, 0E|l, f ; E). Due to the introduc-
tion of q, the log-likelihood of false classiﬁcation becomes
log p(1A, 0E|q, f ; E) ≥ (cid:80)
l∈L q(l) log p(1A, 0E|l, f ; E)
which is further lower-bounded by (8). We then introduce
an agent policy πA for importance sampling of p(τA|l)
in (9). Thus q and πA together constitute the generator.
After cancelling out the passive-dynamics induced p(τA)
in the nominator and denominator as in (10), we handle the
normalization constant Zl via self-normalized importance
sampling (Owen 2013) as in (11) by i.i.d. sampling a set
{τA,i}m
i=1 of trajectories with πA. We refer to (11) as the
generative objective Jgen(q).

(cid:88)

q(l) log p(1A, 0E|πl; l, f ; E)

l∈L
≥ E
l∼q

(cid:110)

E
τA∼p(τA|l)

(cid:2) log p(1A|τA; l, f )(cid:3) +

(cid:2) log p(0E|τE; l, f )(cid:3)(cid:111)

(8)

E
τE ∼pE

= E
l∼q

(cid:110)

E
τA∼πA

≥ E
l∼q

(cid:110)

E
τA∼πA

(cid:2) p(τA|l)
p(τA|πA)
E
τE ∼pE
(cid:104) exp(l(τA))
ZlπA(τA)

log p(1A|τA; l, f )(cid:3) +
(cid:2) log p(0E|τE; l, f )(cid:3)(cid:111)

(9)

(cid:105)
log p(1A|τA; l, f )
+
(cid:2) log p(0E|τE; l, f )(cid:3)(cid:111)

(10)

E
τE ∼pE

m
(cid:80)
i=1

(cid:110)

≈ E
l∼q

(cid:0) exp(l(τA,i))
πA(τA,i)

(cid:1) log p(1A|τA,i; l, f )

m
(cid:80)
i=1

exp(l(τA,i))
πA(τA,i)

+

(cid:2) log p(0E|τE; l, f )(cid:3)(cid:111)

(11)

E
τE ∼pE

(cid:44) Jgen(q)
Next, we note that

the existence of symbolic con-
straint c imposes a prior p(l|c) over the search space
of l. We let p(l|c) be a uniform distribution among
i.e., {l (cid:44)
that do not violate c,
the programs
e[h/?e]|[[c]](h) ≥ 0} ⊆ L while being zero anywhere else.
Then the objective of our learning framework for q be-
(cid:0)q(l)||p(l|1A, 0E, f ; E, c)(cid:1) where
comes minimizing DKL
p(l|1A, 0E, f ; E, c) = p(1A,0E |l,f ;E)p(l|c)
. We minimize
this KL-divergence by maximizing its evidence lower-bound
(ELBO) as in (12) which equals the sum of Jgen(q), an en-
tropy term H(q) and a Jc(q) (cid:44) E
l∼q

[log p(l|c)].

p(1A,0E |f ;E)

ELBO(q) = Jgen(q) − DKL

(cid:0)q(l)||p(l|c)(cid:1)

= H(q) + Jc(q) + Jgen(q)

(12)

In our implementation, the holes in the designed sketches
are all unknown reals, i.e., H = R|?e|. Rather than sampling
from L, we can use a neural network qϕ to specify the mean

Algorithm 1: Generative Adversarial PRDBE
Input: PRDBE tuple (cid:104)e, ?e, H, c, E(cid:105), iteration number N ,
sample number m, K, optimization parameters α, β
Output: l∗, π∗
1: initialization: program space L = {e[h/?e]|h ∈ H};
an agent policy πφ0; neural reward function fθ0; sam-
pler qϕ0 : L → [0, 1]

5:

qϕn (l)

n]](τA,i)}m

n = arg max

2: while iteration number n = 0, 1, . . . , N do
3:
4:

i=1 by using policy πφn
i=1 with the most

Sample trajectory set {τA,i}m
Calculate rewards {[[l∗
likely program l∗
l
Update φn to φn+1 using policy learning algorithm,
e.g. PPO (Schulman et al. 2017)
Sample K programs {lk}K
i=1 by using qϕn
i=1}K
Calculate rewards {{[[lk]](τA,i)}m
Update θn+1 ← θn + α∇θJadv(fθn )
Update ϕn+1 ← ϕn + β∇ϕELBO(qϕn)

6:
7:
8:
9:
10: end while
11: return l∗ := arg max

qϕN (l) and π∗ := πφN

k=1

l

Figure 2: Flowchart for our learning framework

(cid:80)K

El∼qϕn

[·] ≈ 1
K

and variance of a |?e|-dimensional multivariate Gaussian
distribution in H. We use qϕ(l) to denote the probability of
this Gaussian distribution producing an h ∈ H such that l =
e[h/?e]. The mean of the Gaussian corresponds to the most
likely l∗ which we use to train a neural network policy πφ as
the agent policy πA in (11). To calculate the gradient of Jgen,
we use the logarithmic trick (Peters and Schaal 2008), i.e.,
k=1 ∇ϕn log qϕn (lk)[·]. We note that
∇ϕn
Jc(q) is −∞ once the support of q contains an l that violates
c. Since the support of the Gaussian speciﬁed by qϕ is the
real space, Jc(qϕ) can always be −∞. Hence, we relax Jc by
using a ReLU function and a cross-entropy loss to only pe-
nalize qϕ if l∗ violates c. We also train a neural reward func-
tion fθ as the f in (11) by maximize an adversarial objective
Jadv(fθ) that differs from the generative objective Jgen by
changing 1A, 0E into 0A, 1E such that p(0A|τA; l, fθ) :=
(cid:81)T
and p(1E|τE; l, fθ) :=

exp([[l]](τA)[t])
A ,a(t)

A ))+exp([[l]](τA)[t])

exp(fθ(s(t)

t=0

(cid:81)T

t=0

fθ(s(t)

E ,a(t)
E )

exp(fθ(s(t)

E ,a(t)

E ))+exp([[l]](τE )[t])

. The algorithm is sum-

marized in Algorithm 1 and depicted in Fig. 2. In a nutshell,
this algorithm iteratively samples trajectories with πφ, train
πφ with l∗, then update fθ and qϕ respectively.

Experiments
In this section, we experimentally investigate: A. Perfor-
mance: whether Algorithm 1 can efﬁciently train an agent
policy to attain high performance; B. Example Efﬁciency:
whether Algorithm 1 can work with only a few demon-
strated trajectories; C. Generalization: whether the pro-
grams learned via Algorithm 1 in one environment can gen-
eralize across different environments of the same task.
Benchmark. We select
from the MiniGrid environ-
ments (Chevalier-Boisvert, Willems, and Pal 2018) three
challenging RL tasks in ascending order of difﬁculty: the
basic setup and the default reward function for the Door-
Key task have been described in Example 1; KeyCorridor
shown in Fig.3e and ObstructedMaze in Fig.3i both require
the agent to travel from room to room to ﬁnd the key(s) to
pick up a colored ball in some locked room. In KeyCor-
ridor, all but one room are unlocked. In ObstructedMaze,
most of the rooms are locked and obstructed by green balls
and the keys are hidden in grey boxes which the agent must
open ﬁrst. Note that the agent can only carry one object at a
time, which makes picking up and dropping the same ob-
jects multiple times almost inevitable. The environments
can vary in size by changing the number of rooms and tiles
(e.g. DoorKey-8x8 vs. DoorKey-16x16). The placements of
the objects and doors are randomized in each instance of
an environment (e.g. ObstructedMaze-Full). By default, the
environment only returns a reward when the agent reaches
the goal tile or picks up the targeted ball, making explo-
ration in the environment particularly challenging. Besides,
since only the 7 × 7 tiles in front of the agent are ob-
servable, memorization also turns out to be challenging.
Those environments have been extensively used to bench-
mark exploration-driven, curiosity-driven and intrinsic re-
ward driven RL agents. In this paper, we use programmatic
reward function to train RL agents to accomplish those tasks.
Baselines. We compare Algorithm 1 with IRL algorithms,
GAN-GCL (Fu, Luo, and Levine 2018) and GAIL (Ho and
Ermon 2016) to answer question A. We use PPO (Schul-
man et al. 2017), and AGAC (Flet-Berliac et al. 2021) for
RL training in line 5 of Algorithm 1. We answer ques-
tion B by varying the number of demonstrated trajectories
when running Algorithm 1. We answer question C by using
the programmatic reward functions learned via Algorithm
1 in small environments to train RL agents in larger envi-
ronments (the results are annotated with AGAC/PPO prog).
In all three tasks, we provide the results of running the
aforementioned RL algorithms as well as an intrinsic-reward
augmented RL algorithm, RIDE (Raileanu and Rockt¨aschel
2020), with the default rewards. The sketches and symbolic
constraints are in a similar form to those described in Ex-
ample 1. We implement an LSTM reward function fθ for
Algorithm 1 and IRL baselines; an MLP qϕ; a CNN ver-
sion and an LSTM version of the actor-critic RL agent πφ
respectively but only report the version with the higher per-
formance.
DoorKey. We use 10 example trajectories demonstrated in
a DoorKey-8x8 environment.The PRDBE problem and its
solution are both annotated as prog. In Fig.3b, running Al-
gorithm 1 by using PPO and AGAC in line 5 respectively

(a) DoorKey-16x16

(b) DoorKey-8x8-v0

(c) DoorKey-8x8-v0

(d) DoorKey-16x16-v0

(e) KeyCorridorS6R3

(f) KeyCorridorS3R3

(g) KeyCorridorS3R3

(h) KeyCorridorS4/6R3

(i) ObstructedMaze-Full

(j) ObstructedMaze-2Dlhb

(k) ObstructedMaze-2Dlhb

(l) ObstructedMaze-Full

Figure 3: Frames are number of interactions with the environment. Average return is the average default reward achieved over a
series of episodes and is no larger than 1. Alg1 w/ AGAC/PPO indicates using AGAC or PPO as the policy learning algorithm
in line 5 of Algorithm 1. AGAC/PPO prog indicates training an AGAC or PPO agent with reward provided by a programmatic
reward function prog. CNN and LSTM indicate the structures of the actor-critic netowrks.

produces a high-performance policy with fewer frames than
by training PPO or AGAC with the default reward. On the
other hand, RIDE, GAN-GCL and GAIL all fail with close-
to-zero returns. In Fig.3c we reduce the number of examples
from 10 to 1 and it does not affect the number of frames that
Algorithm 1 needs to produce a policy with average return of
at least 0.8, regardless of whether PPO or AGAC is used in
line 5. This shows that Algorithm 1 is example efﬁcient. In
Fig.3h, we use the learned program to train PPO and AGAC
agents in a larger DoorKey environment and achieve high
performances with fewer frames than training PPO, AGAC
or RIDE with the default reward.This shows that the learned
programmatic reward generalizes well across environments.

KeyCorridor. We use 10 example trajectories demonstrated
in a 6 × 6 KeyCorridorS3R3 environment. In Fig.3f, by
using PPO and AGAC in line 5 of Algorithm 1, we re-
spectively obtain high performance with signiﬁcantly fewer
frames than by training AGAC with the default reward.
We omit GAIL and GAN-GCL because both fail in this
task. As shown in Fig.3g, reducing the number of examples
(to 1) does not affect the performance of Algorithm 1. In
Fig.3h, we use the learned program to train AGAC agents
in two larger environments, 10 × 10 KeyCorridorS4R3 and

16 × 16 KeyCorridorS6R3, and achieve high performances
with fewer frames than training AGAC with the default re-
ward. Note that when using the default reward, AGAC has
the prior SOTA performance for this task.

ObstructedMaze. We use 10 example trajectories demon-
strated in a two-room ObstructedMaze-2Dhlb environment.
When training the PPO agent, we discount PPO by episodic
state visitation counts as in many exploration-driven ap-
proaches including AGAC. In Fig.3j we show that Algo-
rithm 1 can produce high-performance policies with fewer
frames than AGAC trained with the default reward. Since
AGAC(CNN) is the SOTA for this task, we do not show the
results of other methods. In Fig.3h, we use the learned pro-
gram to train AGAC agents in a larger environment with as
many as 9 rooms, ObstructedMaze-Full, and achieve higher
performances with fewer frames than other methods trained
with the default reward. Regarding the number of demon-
strations, as shown in Fig.3k, the performance of Algorithm
1 does not change much when the number is decreased to 6,
but drops drastically when the number is further decreased
possibly due to the higher complexity of this task.

(a) DoorKey-8x8

(b) KeyCorridorS3R6

Figure 4: Alg1 w/ AGAC/PPO indicates using AGAC or PPO as the policy learning algorithm in line 5 of Algorithm 1; Alg1(no
c) w/ +PPO(CNN) indicates that running Algorithm 1 without considering its symbolic constraint while using PPO(CNN) in
line 5; Alg1(sign only) w/ + AGAC(LSTM) indicates that running Algorithm 1 with only non-relational constraint while using
AGAC(LSTM) in line 5; CNN and LSTM indicate the structures of the actor-critic networks.

(a) ObstructedMaze-2Dhlb

(b) ObstructedMaze-2Dhlb

Figure 5: Alg1(prog/prog1/prog2) w/ AGAC/PPO indicate using AGAC or PPO as the policy learning algorithm in line 5 of
Algorithm 1 while using different program sketches which are annotated with prog, prog1 and prog2. CNN and LSTM indicate
the structures of the actor-critic networks.

(a) DoorKey-16x16

(b) KeyCorridorS4R3

(c) ObstructedMaze-Full

Figure 6: In each task, given a sketch and a symbolic constraint, assign the holes with different values and use the induced
programmatic reward functions to train agent policies. AGAC/PPO prog indicates that the hole assignments are learned via
Algorithm 1; AGAC/PPO rand# with an index # indicates that the holes are randomly assigned with some values that satisfy
the symbolic constraint for that task. CNN and LSTM indicate the structures of the actor-critic networks.

Ablation Studies
In addition to the questions investigated in the experimental
section of the paper, we have performed ablation studies to
investigate the following three questions.

• E. Is solving a PRD problem with random assignments to
the holes sufﬁcient to obtain an effective programmatic
reward function for RL training?

• F. Can Algorithm 1 still work with weaker symbolic con-

straints or even without any symbolic constraint?
• G. Can Algorithm 1 work with different sketches?

For question E, we solve PRD problems for all three tasks
by randomly generating hole assignments that satisfy the
symbolic constraints. Those assignments are generated by
only optimizing the supervised objective Jc which is men-
tioned in the PRDBE section.

• DoorKey-16x16 . In Fig.6a, we test three 3 randomly
generated hole assignments for the programmatic reward
function, each annotated by PPO(LSTM) rand# . The
trained PPO(LSTM) agents achieve higher performance
with less amount of frames than the PPO(LSTM) agent
that is trained with the default reward function. However,
the programmatic reward function with a learned hole
assignment, annotated by PPO(LSTM) prog, enables the
agent to attain high performance with the lowest amount
of frames.

• KeyCorridorS4R4 . we test 3 randomly generated hole
assignments for the programmatic reward functions, each
the
annotated by AGAC(CNN) rand#. As in Fig.6b,
agents trained with the programmatic reward functions
with random assignments used even more frames than
that trained with the default reward. In contrast, the agent
trained with the programmatic reward function with a
learned hole assignment achieves the best efﬁciency in
achieving high performance.

.

each

annotated

• Obstructed-Full
reward

In Fig.5b, we test 3 randomly
assigned
by
functions,
PPO(LSTM) rand#. The training efﬁciency is competi-
tive in the early stage of the training process. However,
as the number of frames increases, the performance of
the trained policies tend to decrease, which is unlike
that of the programmatic reward function with learned
hole assignment, as annotated by PPO(LSTM) prog.
Note that the rewards sent to the PPO(LSTM) agents in
this environment are all discounted by state-visitation
count (Flet-Berliac et al. 2021).

For question F, we conduct experiments on two of the

tasks.

• DoorKey-8x8 . We test whether Algorithm 1 can prop-
erly train a policy even if no symbolic constraint
is provided. That means, Algorithm 1 has to infer
proper assignment for the holes solely by learning from
demonstrations. As the plot annotated by Alg1(no c) w/
PPO(CNN) in Fig.4a shows, Algorithm 1 succeeded in
training a PPO(CNN) policy to achieve the same level
of performance with almost the same amount of frames
as that PPO(CNN) policy trained with the default reward

function, although both are inferior compared to running
Algorithm 1 with the symbolic constraint included, this
result is a strong evidence that validates Algorithm 1.
• KeyCorridorS3R3. We modify the symbolic constraint
by replacing the predicates concerning the relations be-
tween holes, such as the ones in Table.1, with non-
relational ones such as ?id ≤ 0 (detail will be shown
in Table.3-4 and explained after that). Basically, the
non-relational constraint is weaker than its relational
counterpart. The training result obtained with the non-
relational constraint is shown by the curve annotated by
Alg1(sign only) w/ AGAC(LSTM) in Fig.6c. By includ-
ing the relational predicates, we obtain the curve anno-
tated by Alg1 w/ AGAC(LSTM). Observe that by only
adopting the non-relational predicates, the learning al-
gorithm can still produce a policy with less frames than
the one trained with the default reward, as annotated by
AGAC(LSTM). However, the training process is less sta-
ble compared with its relational counterpart.

For question G, beside the sketch that leads to the re-
sults in the Experiment section in the main text, we de-
sign two more sketches for the ObstructedMaze task. Those
two sketches are annotated as prog1 and prog2 while the
sketch involved in the Experiment section is annotated as
prog. We run Algorithm 1 with these two sketches in the
ObstructedMaze-2Dhlb environment and compare the re-
sults with the sketch annotated by prog in the Experiments
section. Fig.5a shows the results of using PPO(LSTM) in
line 5 of Algorithm 1 and Fig.5b shows the result of us-
ing AGAC(LSTM). We will describe the difference between
these sketches in the appendix.

Conclusion
We propose a novel paradigm for using programs to spec-
ify the reward functions in RL environments. We have de-
veloped a framework to complete a reward program sketch
by learning from expert demonstrations. We experimentally
validate our approach on challenging benchmarks and by
comparing with SOTA baselines. Our future work will focus
on reducing human efforts in the sketch creation process.

References
Abbeel, P.; and Ng, A. Y. 2004. Apprenticeship Learning
via Inverse Reinforcement Learning. In Proceedings of the
Twenty-ﬁrst International Conference on Machine Learning,
ICML ’04, 1–. New York, NY, USA: ACM. ISBN 1-58113-
838-5.
Alshiekh, M.; Bloem, R.; Ehlers, R.; K¨onighofer, B.;
Niekum, S.; and Topcu, U. 2017. Safe Reinforcement Learn-
ing via Shielding. CoRR, abs/1708.08611.
Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schul-
man, J.; and Man´e, D. 2016. Concrete Problems in AI
Safety. CoRR, abs/1606.06565.
Andre, D.; and Russell, S. J. 2001. Programmable reinforce-
In Advances in neural information
ment learning agents.
processing systems, 1019–1025.

Andre, D.; and Russell, S. J. 2002. State abstraction for pro-
In AAAI/IAAI,
grammable reinforcement learning agents.
119–125.
Baier, C.; and Katoen, J.-P. 2008. Principles of Model
Checking (Representation and Mind Series). The MIT Press.
ISBN 026202649X.
Bellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.;
Saxton, D.; and Munos, R. 2016. Unifying count-based ex-
ploration and intrinsic motivation. Advances in neural infor-
mation processing systems, 29: 1471–1479.
Camacho, A.; Toro Icarte, R.; Klassen, T. Q.; Valenzano, R.;
and McIlraith, S. A. 2019. LTL and Beyond: Formal Lan-
guages for Reward Function Speciﬁcation in Reinforcement
Learning. In Proceedings of the Twenty-Eighth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-19, 6065–
6073. International Joint Conferences on Artiﬁcial Intelli-
gence Organization.
Chevalier-Boisvert, M.; Willems, L.; and Pal, S. 2018. Min-
imalistic Gridworld Environment for OpenAI Gym. https:
//github.com/maximecb/gym-minigrid.
Ellis, K.; Solar-Lezama, A.; and Tenenbaum, J. 2015. Un-
supervised Learning by Program Synthesis. In Cortes, C.;
Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and Garnett,
R., eds., Advances in Neural Information Processing Sys-
tems 28, 973–981. Curran Associates, Inc.
Ellis, K.; Wong, C.; Nye, M.; Sabl´e-Meyer, M.; Morales,
L.; Hewitt, L.; Cary, L.; Solar-Lezama, A.; and Tenenbaum,
J. B. 2021. DreamCoder: bootstrapping inductive program
synthesis with wake-sleep library learning. In Proceedings
of the 42nd ACM SIGPLAN International Conference on
Programming Language Design and Implementation, 835–
850.
Finn, C.; Christiano, P.; Abbeel, P.; and Levine, S. 2016. A
Connection between Generative Adversarial Networks, In-
verse Reinforcement Learning, and Energy-Based Models.
CoRR, abs/1611.03852.
Finn, C.; Levine, S.; and Abbeel, P. 2016. Guided cost
learning: Deep inverse optimal control via policy optimiza-
tion. In International conference on machine learning, 49–
58. PMLR.
Flet-Berliac, Y.; Ferret, J.; Pietquin, O.; Preux, P.; and Geist,
In Interna-
M. 2021. Adversarially Guided Actor-Critic.
tional Conference on Learning Representations.
Fu, J.; Luo, K.; and Levine, S. 2018. Learning Robust Re-
wards with Adverserial Inverse Reinforcement Learning. In
International Conference on Learning Representations.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
In Advances in neural
2014. Generative adversarial nets.
information processing systems, 2672–2680.
Gulwani, S.; and Jain, P. 2017. Programming by Examples:
PL meets ML. Springer.
Hadﬁeld-Menell, D.; Milli, S.; Abbeel, P.; Russell, S. J.; and
In Guyon, I.;
Dragan, A. 2017.
Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vish-
wanathan, S.; and Garnett, R., eds., Advances in Neural

Inverse Reward Design.

Information Processing Systems, volume 30. Curran Asso-
ciates, Inc.
Hasanbeig, M.; Abate, A.; and Kroening, D. 2019.
Logically-constrained neural ﬁtted q-iteration. In Proceed-
ings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, 2012–2014. International
Foundation for Autonomous Agents and Multiagent Sys-
tems.
Ho, J.; and Ermon, S. 2016. Generative adversarial imita-
tion learning. In Advances in Neural Information Processing
Systems, 4565–4573.
Icarte, R. T.; Klassen, T.; Valenzano, R.; and McIlraith, S.
2018. Using Reward Machines for High-Level Task Speciﬁ-
cation and Decomposition in Reinforcement Learning.
In
Dy, J.; and Krause, A., eds., Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, 2107–2116.
PMLR.
Icarte, R. T.; Klassen, T. Q.; Valenzano, R. A.; and McIl-
raith, S. A. 2020. Reward Machines: Exploiting Reward
Function Structure in Reinforcement Learning. CoRR,
abs/2010.03950.
Itzhaky, S.; Gulwani, S.; Immerman, N.; and Sagiv, M. 2010.
A Simple Inductive Synthesis Methodology and its Appli-
In OOPSLA/SPLASH’10, October 17-21, 2010,
cations.
Reno/Tahoe, Nevada, USA.
Jeon, W.; Seo, S.; and Kim, K.-E. 2018. A Bayesian Ap-
proach to Generative Adversarial Imitation Learning.
In
Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-
Bianchi, N.; and Garnett, R., eds., Advances in Neural Infor-
mation Processing Systems, volume 31. Curran Associates,
Inc.
Levine, S. 2018. Reinforcement learning and control as
probabilistic inference: Tutorial and review. arXiv preprint
arXiv:1805.00909.
Li, X.; Vasile, C.-I.; and Belta, C. 2017. Reinforcement
In 2017 IEEE/RSJ
learning with temporal logic rewards.
International Conference on Intelligent Robots and Systems
(IROS), 3834–3839. IEEE.
Menon, A. K.; Tamuz, O.; Gulwani, S.; Lampson, B.; and
Kalai, A. T. 2013. A Machine Learning Framework for Pro-
gramming by Example. In Proceedings of the 30th Interna-
tional Conference on International Conference on Machine
Learning - Volume 28, ICML’13, I–187–I–195. JMLR.org.
Ng, A. Y.; Harada, D.; and Russell, S. J. 1999. Policy Invari-
ance Under Reward Transformations: Theory and Applica-
In Proceedings of the Sixteenth
tion to Reward Shaping.
International Conference on Machine Learning, ICML ’99,
278–287. San Francisco, CA, USA: Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-612-2.
Ng, A. Y.; and Russell, S. J. 2000. Algorithms for Inverse
Reinforcement Learning. In Proceedings of the Seventeenth
International Conference on Machine Learning, ICML ’00,
663–670. San Francisco, CA, USA: Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-707-2.
Owen, A. B. 2013. Monte Carlo theory, methods and exam-
ples.

Parisotto, E.; Mohamed, A.-r.; Singh, R.; Li, L.; Zhou, D.;
and Kohli, P. 2016. Neuro-symbolic program synthesis.
arXiv preprint arXiv:1611.01855.
Pathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017.
Curiosity-driven exploration by self-supervised prediction.
In International conference on machine learning, 2778–
2787. PMLR.
Peters, J.; and Schaal, S. 2008. Reinforcement learning of
motor skills with policy gradients. Neural networks, 21(4):
682–697.
Raileanu, R.; and Rockt¨aschel, T. 2020. RIDE: Rewarding
Impact-Driven Exploration for Procedurally-Generated En-
vironments. In International Conference on Learning Rep-
resentations.
Ramachandran, D.; and Amir, E. 2007. Bayesian inverse
reinforcement learning. Urbana, 51(61801): 1–4.
Ratliff, N. D.; Bagnell, J. A.; and Zinkevich, M. A. 2006.
In Proceedings of the 23rd
Maximum Margin Planning.
International Conference on Machine Learning, ICML ’06,
729–736. New York, NY, USA: ACM. ISBN 1-59593-383-
2.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal Policy Optimization Algorithms.
CoRR, abs/1707.06347.
Solar-Lezama, A. 2008. Program synthesis by sketching.
University of California, Berkeley.
Tian, L.; Ellis, K.; Kryven, M.; and Tenenbaum, J. 2020.
Learning abstract structure for drawing by efﬁcient motor
program induction. Advances in Neural Information Pro-
cessing Systems, 33.
Verma, A.; Murali, V.; Singh, R.; Kohli, P.; and Chaud-
huri, S. 2018. Programmatically interpretable reinforcement
learning. In International Conference on Machine Learning,
5045–5054. PMLR.
Yang, Y.; Inala, J. P.; Bastani, O.; Pu, Y.; Solar-Lezama, A.;
and Rinard, M. 2021. Program Synthesis Guided Reinforce-
ment Learning. CoRR, abs/2102.11137.
Zhou, W.; and Li, W. 2018. Safety-aware apprenticeship
In International Conference on Computer Aided
learning.
Veriﬁcation, 662–680. Springer.
Zhu, H.; Xiong, Z.; Magill, S.; and Jagannathan, S. 2019.
An inductive synthesis framework for veriﬁable reinforce-
ment learning. In Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Imple-
mentation, 686–701.
Ziebart, B. D.; Maas, A.; Bagnell, J. A.; and Dey, A. K.
2008. Maximum Entropy Inverse Reinforcement Learning.
In Proceedings of the 23rd National Conference on Artiﬁcial
Intelligence - Volume 3, AAAI’08, 1433–1438. AAAI Press.
ISBN 978-1-57735-368-3.

Appendix
In this appendix, we will present the sketches and symbolic
constraints used in the experiments, experimental setup, and
the alternative sampling scheme mentioned in the paper.

Sketches and Symbolic Constraints
In this section, we provide pseudo-code of the sketches as
well as details on the symbolic constraints for the three tasks
considered in the Experiments section. For readability pur-
poses, we use the notation traj instead of the term x speci-
ﬁed in the syntax rule (4) to refer to the input trajectory.
DoorKey. We show the DoorKey environments of differ-
ent sizes in Fig.7a and 7b. The sketch is shown in Fig.7c.
Most statements in the Fig.7c are self-explanatory. We use
the function pred to identify the token of any given state-
action tuple, then use match to compare the token of the
given state-action tuple with the tokens listed in line 4, 5,
6, 9 and 12 to determine the reward. Line 10 and 13 both
scan past states in the trajectory with function filter and
use len to check whether the agent has unlocked the door
before. In line 7, the conditional statement speciﬁes that the
agent will be penalized with ?3 for closing the door, which
is a redundant behavior for the task, unless the accumu-
lated penalty will exceed the reward ?2 for unlocking the
door. This is a heuristic that prevents an under-trained agent
from being excessively penalized for its actions related to
the door. Note that by allowing putting holes in the guard
of the conditional we make the programmatic reward func-
tions non-linear in the holes (instead of being linear in the
holes with one-hot coefﬁcients). This is a major difference
between programmatic reward functions and the linear func-
tions used in the generic IRL methods.

The symbolic constraint for this task is deﬁned as the con-
junction of the predicates listed in Table.2. In c1 and c3 we
specify that the reward for unlocking the door is larger than
any other behavior except for the reward for reaching the
goal. In c2, we specify that if the agent has not used the
key to unlock the door, the total reward for picking up and
dropping the key must be non-positive. In c4 we specify that
closing door should be penalized with non-positive reward
while in c5 we ensure that such penalty is limited.
KeyCorridor. We show the KeyCorridor environments of
different sizes in Fig.8a-8c. We show the pseudo-code for
this task in Fig.8d.

This

sketch maintains a set of door coordinates,

Properties

[c1]Reward reaching the goal

[c2]Penalize dropping unused key

[c3]Reward unlocking door

[c4]Penalty for closing door
[c5]Mildly penalize door toggling

(?id ≤ ?1)

Predicates
5(cid:86)
id=1
?5+?4 ≤ 0
5(cid:86)
id=2
?3 ≤ 0
?3 + ?2 ≤ 0

(?id ≤ ?2)

Table 2: The correspondence between properties and predi-
cates for the DoorKey sketch in Fig.7c

(a)

(b)

(c)

Figure 7: (a)DoorKey-8x8; (b)DoorKey-16x16; (c)Our pro-
grammatic reward sketch for DoorKey task

door set. When the agent opens a door, the sketch checks
door set as in line 16 whether this door has been opened
before. The function get front coord scans the past states
in the trajectory and measures the relative position of the
door w.r.t the agent’s initial position. If the door has not
been opened, the relative coordinate of the door is added to
door set as in line 18. The sketch determines the reward for
an opening-door behavior depending on whether the agent
has found the key as in line 19. We identify this condition
for the consideration that whereas the agent may have to
search from door to door to ﬁnd the key, it may not have
to do another round of exhaustive search for the locked door
afterwards, since it may have spotted the locked door before
it ﬁnds the key. Especially, we implement a reward scheme
in line 20 and 23 such that, if the accumulated rewards for
opening doors have exceeded the rewards for ﬁnding the key
or unlocking the door, which are both crucial milestones for
ﬁnishing the task, the sketch outputs a reward 0 instead of
?5. Note that in line 20 and line 23 we subtract ?8 to make
sure that, given any trajectory, even if all the opening-door
actions were replaced with some meaningless action, such as
closing door, the agent can still gain even higher total reward
by picking up a key or unlocking a door. This design heuris-
tic aims at rewarding the agent to explore the environment
while preventing the agent from excessive exploration.

A symbolic constraint c for the sketch is deﬁned as the
conjunction of the predicates listed in Table.3. Note that
when an assignment for the holes ?id=1:8 is given, the max-

(a)

(b)

(c)

Figure 8: (a)KeyCorridorS3R3; (b)KeyCorridorS4R3; (c) KeyCorridorS6R3; (d) The pseudo-code of our program sketch for
KeyCorridor task

(d)

Properties

[c1]Reward picking up ball

[c2]Reward 1st time picking up key
[c3]Reward dropping used key
[c4]Reward unlocking door
[c5]Encourage opening door
[c7]Penalize meaningless move
[c8]Moderately reward opening door
[c9]Penalize dropping unused key
[c10]Penalize picking up used key

(?id ≤ ?1)

Predicates
8(cid:86)
id=2
?2 ≥ 0
?3 ≥ 0
?4 ≥ 0
?5 ≥ 0
?8 ≤ 0
?5 − ?8 ≤ ?2
?2 + ?6 ≤ 0
?3 + ?7 ≤ 0

Table 3: The correspondence between properties and predi-
cates for the reward sketch of KeyCorridor in Fig.8d

Properties
[c1]Reward picking up ball
[c2]Reward 1st time picking up key
[c3]Reward dropping used key
[c4]Reward unlocking door
[c5]Encourage opening door
[c7]Penalize meaningless move
[c8] Penalize dropping unused key
[c10]Penalize picking up used key

Predicates
?1 ≥ 0
?2 ≥ 0
?3 ≥ 0
?4 ≥ 0
?5 ≥ 0
?8 ≤ 0
?6 ≤ 0
?7 ≤ 0

Table 4: The non-relational predicates for the reward sketch
of KeyCorridor in Fig.8d

imum number of doors that the agent is encouraged to open
is ﬁxed due to the conditionals in line 19 and line 22 in
Fig.8d. This feature allows the learned program to gener-
alize to larger environments if the number of doors does not
change too much. As mentioned earlier in the ablation study,
we also tested a weaker symbolic constraint that only con-
tains non-relational predicates. This symbolic constraint is
shown in Table.4. It only speciﬁes the signs of the holes and
ignores the relational order between the holes.
ObstructedMaze. We show the environments of different
sizes in Fig.9a and 9b. We designed three sketches for this
task each implementing a different reward scheme. We show
the pseudo-code for the ﬁrst sketch, with which we obtain
the results in the Experiments section as well as the ablation
study for question E. We will also brieﬂy illustrate the two
other sketches.

From line 3 to 8 in Fig.9c, we specify that if the agent has
spotted the targeted blue ball, the program will only respond
to three behaviors of the agent: picking up the target in line
5, picking a key in line 6 and dropping the key in line 7. For
all other behaviors the program will return a 0 reward as in
line 8. If the agent has not spotted the target, the program
will bypass line 3-8 and execute line 10. We use the con-
ditional statements from line 28-40 to implement a similar
heuristic as that in line 19-24 of the sketch in Fig.8d, that is,
to conditionally encourage exploration.

From line 14-27, we implement a new heuristic for this
task. If the program detects in line 16 that agent manages to
unlock a door, the program checks the last time step when
the agent unlocked a door as in line 16 and resets all the re-

Properties

[c1]Reward picking up target

[c2]Reward ﬁnding target
[c3]Reward opening door

(?id ≤ ?1)

Predicates
12(cid:86)
id=2
?2 ≥?4+?5 − 2?3
?4 ≥ 0

[c4]Reward unlocking door

?5 ≥

[c5]Penalize meaningless move
[c6]Penalize picking up used key
[c7]Reward opening box
[c8]Reward picking up ball
[c9]Reward picking up key
[c10]Reward dropping ball
[c11]Reward dropping used key

10(cid:80)
id=6

?id

?3 ≤ 0
?11+?12 ≤ 0
?6 ≥ 0
?7 ≥ 0
?8 ≥ 0
?8 ≥ 0
?12 ≥ 0

Table 5: The correspondence between properties and predi-
cates for the reward sketch of ObstructedMaze task in Fig.9c

wards from then till the contemporary step to 0 as in line
21. Then from line 17-25 the program selectively re-assign
rewards to some signiﬁcant time steps. The functions called
in line 15-20 all check the hindsight trajectory in the sim-
ilar way as the function get front coord in Fig.8d does.
We omit the details of implementations in those functions
here. The underlying idea behind line 15-25 is that once the
agent manages to unlock a door, the program is able to rec-
ognize which past behaviors directly contribute to this door
unlocking outcome, e.g. by opening which box the agent
found the key for this door, which green ball was obstruct-
ing this door, where the agent put the ball so that it would
not longer obstruct the door. We adopt such a heuristic be-
cause it is cumbersome to judge every behavior of the agent
in this task before observing any meaningful outcome. By
only recognizing the milestone behaviors instead of carrying
out a detailed motion planning, we make a trade-off between
effectiveness and complexity in the in the programmatic re-
ward function design. We remark that through line 21-25,
the sketch modiﬁes hindsight rewards based on the current
information. Similar ideas have been proposed in hindsight
experience replay. However, the existing works deﬁne the
rewards as the Euclidean distances between the agent and
some targets, which do not suit our task. By conducting the
programmatic reward function design procedures, we have
the ﬂexibility to adapt to different tasks.

The symbolic constraint c is deﬁned as the conjunction of
the predicates listed in Table.5. Especially, c4 together with
the conditional from line 28-33 we make sure that the total
reward gained from exploration is no larger than that gained
from unlocking doors.

For the other two sketches, we make some modiﬁcation
on the sketch in Fig.9c. The ﬁrst sketch, which we annotate
as prog1, is different from the one in Fig.9c by removing
lines 15-25. The second sketch, which we annotate as prog2,
is different from the one in Fig.9c by additional treating
opening door and unlocking door as exploration behaviors
before ﬁnding the goal. Basically, when the agent is opening
or unlocking a door as in line 13 or 14 in Fig.9c, prog2 sub-
tracts ?2 with the total rewards that the agent has gained from

(a)

(b)

Figure 9: (a) ObstructedMaze-2Dhlb; (b) ObstructedMaze-Full; (c) Program sketch for ObstructedMaze task

(c)

opening and unlocking doors in the trajectory. If this sub-
traction ends up non-positive, prog2 no longer rewards the
agent for opening or unlocking doors in the future. Further-
more, the behaviors listed from lines 35-39 in Fig.9c will no
longer be rewarded either, which is equivalent to letting ?5
equal 0 in lines 29, thus having the conditional always points
to line 41 henceforth. This modiﬁcation basically makes sure
that the total reward of the trajectory is upper-bounded by a
value dependent on the reward ?2.

Training details
• Training Overhead. We note that the sketches in Fig.7c,
8d and 9c all require checking hindsight experiences,
or maintaining memory or other expensive procedures.
However, line 7 of Algorithm 1 requires running all K
candidate programs on all m sampled trajectories, which
may incur a substantial overhead during training. Our so-
lution is that, before sampling any program as in line 6 of
Algorithm 1, we evaluate the result of [[e]](τA,i) for all the
m trajectories. As mentioned earlier, each [[e]](τA,i) is a
partial program with the holes ?e being the free variable.
By doing this, we only need to execute once for certain
expensive procedures that do not involve the holes, such
as running the function len(f ilter()) in all sketches, the
function get front coord in Fig.8d and all the func-
tions called in line 15-20 of Fig.9c. Then we use qϕ to
sample K hole assignments {hk}K
k=1 from H and feed
them to {[[e]](τA,i)}m
k=1.
By replacing line 6 and line 7 with those two steps in
Algorithm 1, we signiﬁcantly reduce the overhead.

i=1 to obtain {{[[lk]](τA,i}m

i=1}K

• Supervised Learning Loss. In Algorithm 1, a super-
vised learning objective Jc is used to penalize any sam-
pled hole assignment for not satisfying the symbolic con-
straint. In practice, since our sampler qϕ directly outputs
the mean and log-variance of a multivariate Gaussian dis-
tribution for the candidate hole assignments, we directly
evaluate the satisfaction of the mean. Besides, as men-
tioned earlier, in our experiments we only consider sym-
bolic constraint as a conjunction of atomic predicates,
e.g. c = ∧n
i=1µi and each µi only concerns linear relation
between the holes, we reformulated each µi into a form
λh.ui(h) ≤ 0 where ui is some linear function of the
holes. We make sure that (ui(h) ≤ 0) ↔ (µi(h) == (cid:62)).
Given a hole assignment h output by a qϕ, we ﬁrst cal-
culate each ui(h), which is now a real number, then
we let Jc(qϕ) be a negative binary cross-entropy loss
for Sigmoid(ReLU ([ui(h), . . . , un(h)]T ))) with 0 be-
ing the ground truth. This loss penalizes any h that makes
ui(h) > 0. In this way Jc(qϕ) is differentiable w.r.t ϕ.
Thus, we do not implement the logarithmic trick when
optimizing Jc.
Network Architectures. Algorithm 1 involves an agent
policy πϕ, a neural reward function fθ and a sampler qφ.
Each of the three is composed of one or more neural net-
works.
– Agent policy πϕ. Depending on the tasks, we pre-
pare two versions of actor-critic networks, a CNN ver-
sion and an LSTM version. For the CNN version, we

directly adopt the actor-critic network from the off-
the-shelf implementation of AGAC (Flet-Berliac et al.
2021). The CNN version has 3 convolutional layers
each with 32 ﬁlters, 3×3 kernel size, and a stride of
2. A diagram of the CNN layers can be found in (Flet-
Berliac et al. 2021). For the LSTM version, we sim-
ply concatenate 3 convolutional layers, which are the
same as those in the CNN version, with a LSTM cell
of which the state vector has a size of 32. The LSTM
cell is then followed by multiple fully connected lay-
ers each to simulate the policy, value and advantage
functions. The AGAC and PPO policies always share
the identically structured actor-critic networks, in both
the CNN and LSTM versions. While AGAC contains
other components (Flet-Berliac et al. 2021), the PPO
agent solely consists of the actor-critic networks.
– Neural reward function fθ. For all the tasks, we use
identical networks. Each network has 3 convolutional
layers each with 16, 32 and 64 ﬁlters, 2×2 kernel size
and a stride of 1. The last convolutional layer is con-
catenated with an LSTM cell of which the state vector
has a size of 128. The LSTM cell is then followed by
a 3-layer fully connected network where each hidden
layer is of size 64. Between each hidden layer we use
two tanh functions and one Sigmoid function as the
activation functions. The output of the Sigmoid func-
tion is the logit for each action in the action space A.
Finally, given an action in a state, we use softmax and
a Categorical distribution to output the log-likelihood
for the given action as the reward.

– Sampler qφ. Since the holes in our sketches all take
numerical values. We implement for each sketch a
sampler that outputs the mean and log-variance of
a multivariate Gaussian distribution of which the di-
mension is determined by the number of holes in the
sketch. The network structures, on the other hand, are
identical across all tasks. The input to each sampler
is a constant [1, . . . , 1]T of size 20. Each sampler is a
fully-connected network with 2 hidden layers of size
64. The activation functions are both tanh. Suppose
that there are |?e| holes in the sketch. Then the output
of the sampler qϕ is a vector of size no less than 2|?e|.
The |?e| most signiﬁcant elements in the output vec-
tor will be used as the mean of the Gaussian, and the
next |?e| most signiﬁcant elements constitute a diago-
nal log-variance matrix.

• Normalization. Besides outputting the mean and log-
variance for the hole assignment, the sampler qϕ addi-
tionally outputs a value log ˆzl to normalize exp(l(τ ))
in Jgen. Speciﬁcally, we introduce such normalization
term because our formulated learning objective aims at
having ˆp(τ |l) = p(τ ) exp(l(τ )) match the probabilities
pE(τ ) ≈ p(τ )πE(τ ), which implies that l(τ ) has to be
negative such that exp(l(τ )) ≡ πE(τ ). However, neg-
ative l(τ ) in our sketch design indicates penalization.
During implementation, we replace every [[l]](τ )[t] with
[[l]](τ )[t] − log ˆzl when calculating Jgen and let l(τ ) :=
(cid:80)
t([[l]](τ )[t] − log ˆzl) in ˆp(τ |l) = p(τ ) exp(l(τ )). Then

Parameter
# Epochs
# minibatches (πϕ)
# batch size (fθ)
# frames stacked (CNN πϕ)
# reccurence (LSTM πϕ)
# recurrence (fθ)
Discount factor γ
GAE parameter λ
PPO clipping parameter (cid:15)
K
α
β
η

Value
4
8
32
4
1
8
0.99
0.95
0.2
16
0.001
0.0003
1.e8

Table 6: Hyperparameters used in the training processes

by maximizing the ELBO in (12), we on one hand search
for the proper hole assignment for the sketch, and on the
other hand search for a ˆzl such that ˆp(τ |l) ≡ p(τ |l) ≡
pE(τ ) can be possibly realized. Since ˆzl is constant, we
still use [[l∗]](τ ) for the policy training in line 5 of Al-
gorithm 1. Note that given such ˆzl, the normalization
term Zl in (10) still has to be introduced in case that the
intermediately learned ˆzl does not accurately normalize
ˆp(τ |l).

• Hyperparameters. Most of the hyperparameters that ap-
pear in Algorithm 1 are summarized as in Table.6. All
hyperparameters relevant to AGAC are identical as those
in (Flet-Berliac et al. 2021) although we do not present
all of them in Table.6 in order to avoid confusion. The
hyperparameter η is made large to heavily penalize qϕ
when its output violates the symbolic constraint c.

Alternative Sampling Scheme
For any term in the form of Jvl = Eτ ∼p(τ |l)[vl(τ )] such
as the EτA∼p(τA|l)[·] part in (8), we can estimate it with
ˆJvl as in (13) with two batches of i.i.d trajectories {τi}m
i=1
and {τj}m
j=1 of πA. This scheme is equivalent to indepen-
dently estimating Zl that appears in (10). Assuming that
vl ∈ max
|vl(τ )| is an upper-bound of vl, we show
in Theorem 1 that the chance of ˆJvl falling in a bounded
neighborhood of Jvl increases with m. For the theorem to
hold, we require that πA(τ ) is positively lower-bounded if
p(τ ) > 0, which in practice can be realized by assuming
that A is bounded and letting πA(a|s) > (cid:15) for any s, a with
some (cid:15) > 0. .

τ :p(τ )>0

ˆJvl

:=

m
(cid:80)
i=1

exp(l(τi))
πA(τi) vl(τi)

m
(cid:80)
j=1

exp(l(τj ))
πA(τj )

(13)

ability of ˆJvl − Jvl ∈ (cid:2) ˆZlJvl −γ
ˆZl+γ/vl
(cid:17)4
(cid:16)
1 − exp( −2mγ2πA

2/v2
exp(2l(τ )) )
l

.

max
τ :p(τ )>0

,

ˆZlJvl +γ
ˆZl−γ/vl

(cid:3) is no less than

Proof. We ﬁrst show in (14) and (15) that the numerator and
denominator of ˆJc,l are respectively unbiased estimates of
ZlJvl and Zl.

E
τj ∼πA

(cid:104) exp(l(τj))
πA(τj)

(cid:105)

(cid:88)

=
τj :πA(τi)>0

πA(τj)p(τj) exp(l(τj))
πA(τj)

(cid:88)

=

p(τj) exp(l(τj)) = Zl

τj :πA(τi)>0

E
τi∼πA

(cid:104) exp(l(τi))vl(τi)
πA(τi)

(cid:105)

= ZlJvl

(14)

(15)

By Hoeffding’s inequality, for arbitrary γ > 0 we have the
conﬁdence (16) and (17) respectively on the two batches of
(τ )2
m i.i.d sampled trajectories. The term max

vl exp (l)
πA

is an abbreviation of max

τ :p(τ )>0

(cid:0) vl(τ ) exp (l(τ ))
πA(τ )

τ :p(τ )>0
(cid:1)2

.

P

(cid:16) 1
m

exp(l(τi))vl(τi)
πA(τi)

− ZlJvl ≥ −γ

(cid:17)

m
(cid:88)

i=1
(cid:16)

≥ 1 − exp

−2mγ2
vl exp (l)
πA

(τ )2

max
τ

(cid:17)

(cid:17)

≥ 1 − exp

(cid:16) −2mγ2/v2
l

max
τ :p(τ )>0

exp(2l(τ ))
πA(τ )2

P

(cid:16) 1
m

m
(cid:88)

j=1

exp(l(τj))
πA(τj)

− Zl ≤

(cid:17)

γ
vl

≥ 1 − exp

(cid:16) −2mγ2/v2
l

(cid:17)

max
τ :p(τ )>0

exp(2l(τ ))
πA(τ )2

(16)

(17)

By conjoining (16) and (17), the conﬁdence on the lower-
bound of ˆJvl is lower-bounded by (18) for any γ > 0.

(cid:16) ˆJvl ≥ ZlJvl −γ
P

Zl+γ/vl

(cid:17)

= P

(cid:16) ˆJvl ≥ Jvl − γvl+γJvl

Zlvl+γ
(cid:17)2
)

(cid:17)

(18)

(cid:16)

≥

1 − exp(

−2mγ2/v2
l
exp(2l(τ ))
πA (τ )2

max
τ :p(τ )>0

Theorem 1. Given a program l, a bounded function
vl(τ ) ∈ [−vl, vl] and a lower-bounded agent policy πA, i.e.
∀τ.p(τ ) > 0 ⇒ πA(τ ) ≥ πA, for any γ > 0, the prob-

Again, by Hoeffding’s inequality, for arbitrary γ > 0 we
have the conﬁdence (19) and (20) on those two batches of m

i.i.d sampled trajectories.

P

(cid:16) 1
m

m
(cid:88)

j=1

exp(l(τj))vl(τj)
πA(τj)

− ZlJvl ≤ γ

(cid:17)

≥ 1 − exp

(cid:16) −2mγ2/v2
l

(cid:17)

max
τ :p(τ )>0

exp(2l(τ ))
πA(τ )2

P

(cid:16) 1
m

m
(cid:88)

i=1

exp(l(τi))
πA(τi)

− Zl ≥ −

(cid:17)

γ
vl

≥ 1 − exp

(cid:16) −2mγ2/v2
l

(cid:17)

max
τ :p(τ )>0

exp(2l(τ ))
πA(τ )2

(19)

(20)

By conjoining (19) and (20), the conﬁdence on the upper-
bound of ˆJvl is upper-bounded by (21) for any γ > 0.
(cid:17)
(cid:16) ˆJvl ≤ Jvl + γvl+γJvl

(cid:16) ˆJvl ≤ ZlJvl +γ
P

= P

(cid:17)

Zl−γ/vl

(cid:16)

≥

1 − exp(

−2mγ2/v2
l
exp(2l(τ ))
πA (τ )2

max
τ :p(τ )>0

Zlvl−γ
(cid:17)2
)

(21)

Then by conjoining (18) and (21), we further obtain a conﬁ-
dence on ˆJvl being in the interval as shown in (22).
(cid:16) ˆJvl − Jvl ∈ (cid:2) ZlJvl − γ
ZlJvl + γ
P
Zl + γ/vl
Zl − γ/vl
−2mγ2/v2
l

(cid:3)(cid:17)

(cid:16)

,

≥

1 − exp(

(cid:17)4
)

max
τ :p(τ )>0

exp(2l(τ ))
πA(τ )2

(cid:16)

≥

1 − exp(

−2mγ2πA
max
τ :p(τ )>0

2/v2
l
exp(2l(τ ))

(cid:17)4
)

(22)

