9
1
0
2

p
e
S
9
1

]
E
N
.
s
c
[

2
v
3
3
2
5
0
.
9
0
9
1
:
v
i
X
r
a

THE NEURAL STATE PUSHDOWN AUTOMATA

Ankur Mali1
aam35@psu.edu,

Alexander Ororbia2
ago@cs.rit.edu

C. Lee Giles 1
clg20@psu.edu

1Penn State University
State College, PA 16801
2Rochester Institute of Technology
Rochester, NY 14623

ABSTRACT

In order to learn complex grammars, recurrent neural networks (RNNs) require sufﬁcient computa-
tional resources to ensure correct grammar recognition. A widely-used approach to expand model
capacity would be to couple an RNN to an external memory stack. Here, we introduce a “neural
state” pushdown automaton (NSPDA), which consists of a digital stack, instead of an analog one, that
is coupled to a neural network state machine. We empirically show its effectiveness in recognizing
various context-free grammars (CFGs). First, we develop the underlying mechanics of the proposed
higher order recurrent network and its manipulation of a stack as well as how to stably program its
underlying pushdown automaton (PDA) to achieve desired ﬁnite-state network dynamics. Next, we
introduce a noise regularization scheme for higher-order (tensor) networks, to our knowledge the
ﬁrst of its kind, and design an algorithm for improved incremental learning. Finally, we design a
method for inserting grammar rules into a NSPDA and empirically show that this prior knowledge
improves its training convergence time by an order of magnitude and, in some cases, leads to better
generalization. The NSPDA is also compared to a classical analog stack neural network pushdown
automaton (NNPDA) as well as a wide array of ﬁrst and second-order RNNs with and without external
memory, trained using different learning algorithms. Our results show that, for Dyck(2) languages,
prior rule-based knowledge is critical for optimization convergence and for ensuring generalization to
longer sequences at test time. We observe that many RNNs with and without memory, but no prior
knowledge, fail to converge and generalize poorly on CFGs.

Introduction

Despite their success, artiﬁcial neural networks (ANNs), especially recurrent neural networks (RNNs), have repeatedly
been shown to struggle with generalizing in a sophisticated, systematic manner, often uncovering misleading statistical
associations instead of true casual relations. Verifying what is learned by these black-box models remains an open
challenge, centering around one central issue – the lack of interpretability and modularity. The fact that successful ANN
optimization depends heavily on large quantities of data only serves to further worsen the problem.

One research direction towards developing more interpretable ANNs focuses on rule extraction from and assimilation
of rules into RNNs [1, 2]. To solve difﬁcult grammatical inference problems, various types of specialized RNNs have
been designed [3, 4, 5, 6, 7, 8] However, it has been shown that RNNs augmented with external memory structures,
such as the neural network pushdown automaton (NNPDA), are more powerful than RNNs without, both historically
[9, 10, 11] and recently, using differentiable memory [12, 13, 14, 15, 11, 16, 17, 18]. Yet most of these models often
lack interpretability and how they learn any given grammar is still debatable. In the past, rule integration methods
have been proposed to tackle the interpretability issue [9, 19] and offer a promising path towards the design of ANNs
with an underlying knowledge structure that is bit more understandable and transparent. However, to the best of our
knowledge, there exists no method for inserting rules into the states of the far more powerful class of higher order,
memory-augmented RNNs.

In working towards interpretable, memory-based neural models, in this work, our contributions are the following:

 
 
 
 
 
 
• We propose the neural state pushdown automaton and its incremental training method, which exploits the

concept of iterative reﬁnement

• We develop a novel regularization method that empirically yields better generalization in complex, memory-
based RNNs. To our knowledge, we are the ﬁrst to propose a weight regularizer that works with higher-order
RNNs.

• We propose a method for programming states into a neural state machine with binary second and third-order

weights .

• We develop a method for inserting rules into stack-based recurrent networks.
• We compare our model with the NNPDA and other RNNs, trained using different learning algorithms.

Motivation & Related Work

Research related to integrating knowledge into ANNs has existed for quite some time, such as through the design of
state machines [20, 19]. Recent efforts in the domain of natural language processing have shown the effectiveness of
using state machines for tasks such as visual question answering, which allow an agent to directly use higher-level
semantic concepts to represent visual and linguistic modalities [21]. With respect to rule-insertion itself, there exists a
great deal of work showcasing its effectiveness when used with ANNs[22] as well as with RNNs [9, 19]. Notably, [19]
showed how deterministic ﬁnite automaton rules could be encoded into second order RNNs.

One important, classical model that we draw inspiration from is the neural network pushdown automaton (NNPDA)
[23]. The structure of our proposed model is similar to the NNPDA, but, as we will discuss, the major difference is that
the model works with a digital stack as opposed to an analog one. Interestingly enough, prior work has also shown how
to “hints” into the NNPDA, where knowledge of “dead states” can be used to guide its learning process [23]. In the
spirit of this hint-based methodology, we will develop a method for encoding useful rules related to target CFGs into
our neural state pushdown automaton (NSPDA). This, to our knowledge, is the ﬁrst approach of its kind, since no rule
methodology has been previously proposed for complex state-based models. Creating such a procedure allows us to
both exploit the far greater representational capabilities of memory-augmented RNNs while offering an intuitive way
for understanding the knowledge contained and acquired by RNNs.

In this work, we will focus on RNNs that control a discrete stack, particularly our proposed NSPDA. We will empirically
determine if the inductive biases we encode into its synaptic weights speed up the parameter optimization process
and, furthermore, improve model generalization over longer sequences at test time. Furthermore, the results of our
experiments, which compare a wide variety of RNNs (of varying order, with and without memory), will strongly
contradict the claim presented in recent work [24], which speciﬁcally claims that ﬁrst order RNNs, like the popular
gated recurrent unit RNN [25], are as powerful as a PDA. In essence, our work demonstrates that for an RNN to
recognize a complex CFG, it will, at least, require external memory. Our results also demonstrate the value of encoding
even partial PDA information which positively impacts convergence time and model generalization.

The Neural State Pushdown Automaton

Neural Architecture

The model we propose, the NSPDA with iterative reﬁnement is shown in ﬁgure 1. The NSPDA consists of fully
connected recurrent neurons which we will label as state neurons, primarily to distinguish them from the neurons
that function as output neurons. Introducing the concept of state neurons is important when considering the notion
of higher-order networks, i.e., second or third order RNNs, which allows us to map state representations directly to
outputs. In this model, at each time step t, the state neuron receives signals from the input neurons, its previous state,
and the stack-read neurons. The input neurons process a string, one character at a time, while non-recurrent neurons,
also labeled as “action neurons”, represent an operation to be performed on a stack data structure, i.e., Push/Pop/No-op.
The action neurons are also designated as the controller which can either be recurrent or linear (recurrent controllers
usually perform better in practice, so we focus on these in this paper). Furthermore, “read” neurons are used to keep
track of the symbols present at the top of the stack.

To make concrete the above high-level description, consider a single hidden-layer NSPDA. A full symbol sequence
sample (y, X) is deﬁned as X = {x1, x2, · · · , xT } where the binary label y indicates whether the sequence is valid
(1) or not (0). When processing a (binary) symbol/token xt ∈ {0, 1}L×1 at the discrete time step t, the NSPDA is
engaged with computing a new state variable vector zt ∈ RJ×1, where L is the total number of input/sensory neurons
(or dimensionality of the input space, sometimes classically refered to as alphabet size) and J is the total number of

2

state neurons. The action neuron vector is deﬁned as a ∈ RL×1 and the read neuron vector is deﬁned as r ∈ RL×1, i.e.,
the action and read spaces are of the same dimensionality of the input or |x| = |r| = |a|. Taken together, the above sets
of input, state, and read neurons represent a full NSPDA model with parameters Θ = {W s, W a, W o}. Crucially, W s
and W a are both 4-dimensional (4D) synpatic weight tensor, i.e., the binary “to-state” tensor W s ∈ {0, 1}J×L×L×L
and the 4D tenary to-action tensor W a ∈ {−1, 0, 1}J×L×L×L (note that: −1 is “pop”, 0 is “no-op”, and 1 is “push”).
At t, inference (for a third order NSPDA) is conducted as follows:
t , rk
t , rk

t) + bi
s)
t) + bi
a)

t , xl
t , xl

(2)

(1)

zi
t+1 = g(Σj,k,lW s
t+1 = f (Σj,k,lW a
ai

if ai

if ai
if ai

ijkl(zj
ijkl(zj
t = 0
t = 1
t = −1

ri
t+1 =

α1
α2
α3



(3)

where α1 ∼ U (0.0001, 0.008), α2 ∼ U (0.901, 0.992), and α3 ∼ U (0.025, 0.110), are threshold values that determine
what the next state of the discrete read unit ri
t will be (sampled uniformly from a special interval to create continuous
value for backprop to work with). Note that zt+1 is the next hidden state, at+1 is the next stack action, and rt+1 is the
next value of the neuron that reads the content at the top of the stack. g(v) and f (v) are non-linear activation functions,
speciﬁcally, quantized sigmoidal functions, deﬁned as:

ˆg(v) =

g(v) =

f (v) =

,

ˆf (v) = 2g(v) − 1

1
(1 + e−v)
(cid:26) 1
0

if ˆg(v) > 0.5
otherwise.






1
0
−1

if ˆf (v) > 0.13
if − 0.09 ≤ ˆf (v) ≤ 0.13
otherwise.

(4)

(5)

(6)

As the NSPDA processes a string, a prediction ˆyt of its validity is made at each step. Speciﬁcally, the output weights
W o ∈ RJ×1 (and bias scalar bo) are used to map the state vector zt to the output space. The output model is deﬁned as
ˆyt = σ(W o · zt + bo), where σ(v) is the logistic link function.
The actual external stack itself is manipulated by discrete-valued action neurons that trigger a discrete push or pop
action (as given by Equation 2). Take, for example, a 2-letter alphabet, i.e., {a, b}. The dimensions of the action and
read spaces would then, in this case, be |a| = |r| = 2. When using a digital stack, the following actions can be taken:

• PUSH: This means that the current input is pushed to the top of the stack. Example: To push the symbol “a”,

use at =< 1, 0 > and rt =< 0.955, 0.008 >.

• POP: This means that the element is removed from the top of the stack. Example: To remove the symbol “b”,

use at =< 0, −1 > and rt =< 0.008, 0.065 >.

• NO-OP: This simply means “no operation, or, in other words, nothing is to be done with the stack. Example:

use at =< 0, 0 > and rt =< 0.008, 0.008 >.

In the case of the vector rt, we are reading the symbol currently located at the top of the stack (at each time step)
(corresponding read vectors are shown above in the action vector examples0. Our goal is to make sure the RNNs choose
the correct action during training and yet still maintain stable binary read states zt.

Learning and Optimization

First, we deﬁne the loss function used to both measure the performance of the network as well as optimize its parameters.
Classically, state neural models such as the NNPDA exclusively made use of a binary loss function that only considered
if a string was valid or invalid [26]. Furthermore, these models only made a prediction/classiﬁcation at the very end of
the sequence. In contrast, the NSPDA is an iterative, step-by-step predictive model. Thus, we consider using a sequence
loss based on binary cross entropy.1. The instantaneous loss, for a single sequence (y, X), is:

L(y, X, Θ) =

T
(cid:88)

t=1

−y log(ˆyt)) − (1 − y) log(1 − ˆyt).

(7)

1In preliminary experiments, models using a squared error loss, with and without regularization penalties, had great difﬁculty in

converging. We found using cross entropy was far more effective.

3

Figure 1: The NSPDA shown making predictions over K = 2 steps of iterative reﬁnement.

where ˆyt is the t-th prediction/output from the ﬁnal state neuron. Note that y is copied each step in time, which injects
an extra error signal throughout the sequence length, improving the optimization process (as opposed to relying on only
a single output error signal to be effectively propagated backwards through the underlying computation graph).

To compute updates for the NSPDA’s parameters, we employed several gradient-based approaches, including the
popular and common back-propagation through time (BPTT) procedure as well as online algorithms such as real-time
recurrent learning (RTRL) [27] and unbiased online recurrent optimization (UORO) [28]. In short, all of these algorithms
compute gradients of the loss function (Equation 7) with respect to NSPDA weights. The primary difference between
the algorithms is that BPTT is based on reverse-mode differentiation routine while RTRL is based on forward-mode
differentiation (and UORO is a faster, higher variance approximation of RTRL). In further detail, we describe UORO
and RTRL in the appendix. While UORO and RTRL are not commonly used to train modern-day RNNs, they offer faster
ways to train them without requiring graph unfolding. Thus, we compare the results of using each in our experiments.

Iterative Reﬁnement

One important element we introduced into the training protocol of the NSPDA is that of iterative reﬁnement, an
algorithm proposed in the signal processing literature for incorporating partial iterative inference into a next-step
predictive RNN [29]. At a high-level, this means that, during training, at step t, the NSPDA is forced to predict the
same target (yt) K times (except for the state transitions that are provided as “hints”, of which we will describe in a
later section). Crucially, the state vector is still carried over these K steps, meaning the recurrent synapses relating
the state of the model at time t to t + 1 .To adapt iterative reﬁnement to a next-step sequence model like the NSPDA,
iterative reﬁnement can cleanly introduced by manipulating the sequence loss of Equation 7 as follows:

D(ˆy, y) = −y log(ˆy)) − (1 − y) log(1 − ˆy)

L(y, X, S, Θ) =

K=S(t)
(cid:88)

(cid:88)

t

k=1

D(ˆyt,k, y)

(8)

(9)

noting that we have introduced the variable S to augment the sample (y, X). S is an integer sequence computed
as follows: S = K(1 − H) + H where H is a binary “hint” vector (automatically generated) of the form H =
{h0, h1, · · · , hT } (ht = 1 signals a hint is used, while ht == 0 is “no hint”). Empirically, we found K = 4 worked
well. In [29], using an RNN’s recurrent weights as a lateral processing mechanism [30] was related to an RNN acting
as a deep feedforward network with tied weights across K hidden layers (a “prediction episode”). This means that
additional nonlinearity (via depth) is being efﬁciently exploited without incurring the memory cost of storing extra
weights. We found that iterative reﬁnement introduces greater stability into learning process primarily when gradient
noise is used. Note that, even in this case, while we work with full precision weights for gradient computation, before
evaluation is conducted, the weights are converted to discrete values.

Two Stage Incremental learning

Incremental learning, or, in other words, training procedures that sort data samples based on their inherent difﬁculty
and progressively present them to a neural agent progressively, has been shown to quite effective when training RNNs

4

State NeuronAt Time tRecurrent StepAction NeuronInput NeuronReading from top of StackHigher Order ConnectionsHigher Order Weights UpdateNSPDA   NSPDAK=2 (Iterative Refinement Steps)Discrete StackUpdate Stack at the end of episodePrevious statek=1k=2yyPrediction  yㅅPrediction  yㅅon input data that is known to have some structure [31, 26]. Based on this prior ﬁnding, we developed a two-stage
incremental learning approach for improving a higher-order RNN’s ability to generalize to longer sequences. Formally,
Algorithm 1 depicts the overall process. We found that using a stochastic learning rate [29] worked better in the ﬁrst
stage while a ﬁxed learning rate combined with stochastic noise process applied to the weights (similar to gradient noise)
worked better during second stage. As we will see later experimentally, whenever the data has some exploitable structure

Algorithm 1 Two Stage Incremental Learning

Input: Θ (model weights), training set D, validation set V, NT r (midpoint length threshold), λ (learning rate)
// ————————— Stage #1 —————————
Nmax = maxLen(D)
// Sequential Curriculum Update Phase
Dl = ∅
for Nl = 1 to NT r do

(cid:46) Calculate longest string length

Dl = Extract from D all strings lengths ≤ Nl
TRAIN(Model(Θ), λ, Dl)
// Random Curriculum Phase
while Model(Θ) not converged on V or en < 200 do

TRAIN(Model(Θ), λ, Dl),

en = en + 1

// ————————— Stage #2 —————————
// Sequential Curriculum Update Phase
Dl = ∅
for Nl = 1 to Nmax do

Dl = Extract from D all strings lengths ≤ Nl
TRAIN(Model(Θ), λ, Dl)
// Random Curriculum Phrase
while Model(Θ) not converged on V or en < 350 do

TRAIN(Model(Θ), λ, Dl),

en = en + 1

return Θ

(cid:46) Single pass through Dl

(cid:46) Single pass through Dl

(cid:46) Return ﬁnal trained model weights

that allows for an automatic sorting of samples by increasing complexity, incremental learning is highly effective in
training higher-order RNNs. In the case of CFGs, we can sort samples based on string length and progressively build a
model that can learn to generalize to increasingly longer string sequences. Algorithm 1 depicts the full process (note
that we set NT = 14 in this paper and en is a variable that marks the number of epochs so far).

Regularizing Higher Order RNNs:

When training any RNN for long periods of time, the model tends to memorize the input training data which damages
its ability to generalize to unseen sequence data, i.e., overﬁtting. Higher order RNNs are also susceptible to overﬁtting
given their high-capacity and complexity, and yet, no regularization has ever been proposed to help these kinds of RNNs
to combat overﬁtting. In this work we extend an adaptive (layer-dependent) noise scheme that was originally proposed
for training neurobiologically-plausible ANNs [32], which showed strong positive results for simple feedforward
classiﬁcation tasks, to RNNs. Notably, our noise-based regularizer applies to higher-dimensional tensors, which are
fundamental to implementing any n-th order RNN. We are also motivated by the fact that injecting noise to gradients
can encourage exploration of an RNN’s error optimization landscape [33] in one of two ways: 1) at the input, i.e., data
augmentation [33], or 2) at the recurrence [34]. Our regularizer falls under the second case.2

The key details of our noise-based regularizer are depicted in Algorithm 2. Based on preliminary experiments, we
found that a noise level less than 30% and more than 8% helps the network to converge faster and, more importantly,
generalize better on unseen sequences, longer that than those found in the training set. Experimentally, later we will see
that this regularizer improves generalization even when prior knowledge is not integrated into the RNN.

2We implemented a data augmentation approach but found it yielded poor results when learning context-free grammars.

5

Algorithm 2 Adaptive Noise Regularizer

Input: Tensor W ∈ RA×B×C×D, e.g., W s or W a
// Np=Percentage of Noise, “· · · ” means k = k + 1
function CREATEPARTITIONS(W, K,Np)

(cid:46) Partition sub-routine for noise regularization function

// len(W ) = A ∗ B (calculate length by multiplying 1st two tensor dimensions)
// s(Pi, Np) randomly selects Np matrices in Pi
// Divide W into 3 partitions {P1, P2, P3}
// Pi = {M1, M2, · · · , Mlen(W )}, Mi ∈ RC×D
if len(W ) is odd

P1 = W [k = 1, · · · , K/3]
P2 = W [k = k/3 + 1, · · · , 2K/3]
P3 = W [k = (2k/3) + 1, · · · , K]

else len(W ) is even

P1 = W [k = 1, · · · , (K − 1)/3]
P2 = W [k = (k − 1)/3 + 1, · · · , (2K − 1)/3]
P3 = W [k = (2k − 1)/3 + 1, · · · , K]

// Create set Q of Np random matrices from each Pi
Q = {s(P1, Np), s(P2, Np), s(P3, Np)}
Return Q

function ADAPTIVE NOISE(Q)
(cid:98)p ∼ N (µ = 0, σ = 1)
for each M in Q do

M = ((cid:98)p ∗ β)M , (cid:98)p = (cid:98)p/2

// Remap matrices M in Q to tensor shaped like W
W ← remap(Q)
Return W
// Use updated weight matrix for gradient computation

Integrating Prior Knowledge

Programming and Inserting Rules

(cid:46) Draw Gaussian scalar sample
(cid:46) for each matrix in Q

We start by deﬁning the data generating process that any RNN is to learn from, i.e., a PDA that generates a set of
positive and negative strings. Formally, the M -state PDA is deﬁned as a 7-tuple (Q, Σ, Γ, δ, q0, ⊥, F ) where:

• Σ = {a1, · · · , al, · · · , aL} is the input alphabet
• Q = {s1, · · · , sm, · · · , sM } is the ﬁnite set of states
• Γ is known as stack alphabet (a ﬁnite set of tokens)
• q0 is the start state
• ⊥ is the initial stack symbol
• F ⊆ Q is the set of accepting states
• δ ⊆ Q × (Σ∪)| × Γ → Q × Γ∗) is the state transition.

To insert rules related to known state transitions into the (N -state) NSPDA, one needs to program its recurrent weights
(which could be second or third order). Since the number of states in PDA is not known before hand, we assume that
J > M and that the network has enough capacity to learn an unknown context-free grammar.

In order to program and insert rules, we propose adapting methodology originally developed for second-order RNNs
and deterministic ﬁnite state automata (DFA) [19] to the case of PDA-based RNNs. Speciﬁcally, we will exploit the
similarity between the state transitions of the target PDA and the underlying dynamics of a stack-driven RNN. Consider
a known transition δ(sj, al, Ts) = (si, γ), where Ts is the top of the stack and γ is the sequence of symbols replacing
Ts. We then identify PDA states sj and si, which correspond to state neurons zj and zi, respectively. Recall that each
symbol has speciﬁc stack operations associated with it, which provide prior knowledge as to when to push and when to
pop from the stack. It is desirable that the state neuron zi has a high output close to 1 and zj has a low output close to 0
after reading an input symbol al using input neuron xm and the top of the stack Ts using read neuron rl (remember
that a read depends on an action neuron, as depicted in model Equation 3). This condition can be achieved by doing the

6

following: 1) set the (third order) weights W s
zt+1
i
large negative value, which would make the output of the state neuron zt+1

ijkl to a large positive value, which helps to ensure that the state neuron
jjkl to a

at the next time step t + 1 will be high (and since ˆg(v) is sigmoidal, this tends towards 1), and 2) set W s

low (tending ˆg(v) towards 0).

j

The next item to consider are the (ternary) action weights stored in W a
ijkl, which drive the action neurons that yield the
stack operations (recall that [-1,0,1] maps to [pop,no-op,push]). First, we must assume that the total contribution of the
weighted output of all state neurons can be neglected – this can be achieved by setting all other state neurons to the
lowest value. In addition, we assume that each state neuron can only be assigned to one known state of the PDA. If we
have prior knowledge of accepting and non-accepting states related to a particular neuron, we may then bias its output
zi
t+1. We start from i = 1 (the leftmost neuron in the vector zt) and work towards i = J, programming each one by one.
Armed with these assumptions, we can then stably encode rules into the NSPDA by programming the weight W s
ijkl to
be large positive value if the PDA’s state si is an accepting state. Otherwise, we set W s
ijkl to be a large negative value if
the state is non-accepting. If no such knowledge of the PDA is available, W s

ijkl remains unchanged.

Though described for a third order NSPDA, the above approach for programming weights also applies to a second
ijk and W a
order model as well. In a lower order NSPDA, with 3D weight tensors W s
ijk, state updates and transitions
are conducted by concatenating a read neuron ri
t with an input neuron xk
t to create a single vector. However, when
programming a second order model, we are now working with a DFA [19] instead of a PDA, which limits the capabilities
of the NSPDA (as well as restricts its capacity) since we do not possess any knowledge about what to push or pop.
However, when combined with our proposed learning procedure that incorporates iterative reﬁnement, we believe that
the second order NSPDA can still learn what action to perform. However, the issue of dimensionality arises – the state
space of a lower order model is very large when compared to that of a third order NSPDA. In the case of a PDA-based
model, pushing multiple symbols might lead to reaching same accepting state, however, in case of a DFA-based model
(the second order NSPDA), we create separate sets of accepting states for each symbol. We found that this splitting
mechanism was crucial in getting our network to work perfectly with a digital stack.

While the above rule insertion scheme seems simple enough, determining the actual values for the weights that are to
be programmed can be quite problematic. In the case of third order synaptic connections (with binary weights), with
just 4 neurons, there are 2256 different combinations, which would quickly render our method impractical and near
useless. However, we can sidestep this computational infeasibility by making use of “hints” [19] within the framework
of “orthogonal state encoding”. By assuming that the PDA starts generating a valid grammar at its initial state, we can
then randomly choose a single state and make the output of one state neuron equal to 1. The outputs of all the other
neurons are set to be equal to 0. Following this, we set the values of weights (according to known state transitions)
according to the approach described above. Notably, these weights, though initially programmed, are still adaptable,
making them amenable to tuning to a target grammar underlying a data sample. Programming the weights of second or
third order networks jointly impacts the behavior of the state neurons zt, the read neurons rt and the input neurons xt.
Following the scheme we described above yields sparse NSPDA representations of PDA states.

It is difﬁcult to program an NSPDA with a minimal number of states, despite the fact that we have a theoretical guarantee
that the third order model is equivalent to PDA dynamics [23].

We will observe in our results, the proposed methodology signiﬁcantly reduces the NSPDA’s convergence time during
optimization (leading to roughly comparable training time characteristic of ﬁrst order RNNs), which is particularly
important given the fact that its inference process entails 4D tensor products (which are far more expensive than the
matrix computations of modern-day RNNs).

Experimental Details

We focused on ﬁve context-free grammars, some labeled as Dyck(2) languages, which are some of the more difﬁcult
CFGs to recognize. For each grammatical inference task, we create a dataset that contains 1987 positive and 2021
negative (string) samples. Each sequence was of length T which was sampled via T ∼ U (1, 21), where U (a, b) is the
uniform distribution deﬁned over the interval [a, b]. From the samples generated, we randomly sampled a subset from
the total number of tokens generated.

The number of state neurons for a second order NSPDA is set according to the following formula: J = M + ∼ U (12, 29).
For a third order NSDPA, the number of state neurons was set according to: J = M + ∼ U (2, 6).

All models made use of the iterative reﬁnement loss (Equation 9, with K = 4), weight updates were computed using
whichever algorithm, i.e., BPTT, truncated BPTT (TBPTT) (50 steps back in time), RTRL, or UORO, yielded best
performance for a given model. For higher order networks, UORO performed better and we use this to optimize all

7

Rule Method
NNPDA w/o hints
NNPDA w/ dead neuron hints
NSPDA w/o hints
NSPDA w/ Hint #1
NSPDA w/ Hint #2

anbn

anbncbmam an+mbncm
P alindrome
W1 W2 W1 W2 W1 W2 W1 W2
215 N A N A N A N A
100
195
145
192
92
293
159
192
91
160
140
170
80
148
134
138
70

280
212
221
190
150

485
488
410
389

250
339
240
222

81
83
79
75
72

Table 1: Comparison between NSPDAs trained w/ and w/o hints using either 2nd order weights (W1) or 3rd order
weights (W2).

Train Method M1
Standard
5699
IL
2678
2-IL (ours)
2001

anbn

P alindrome
M2
5912 > 200000 > 210000 > 240000 > 233000 > 320000 > 315000
222144
192001
2552
177190
130192
2199

108200
9899

222171
177189

104556
10001

192551
129998

M1

M2

M1

M2

M1

M2

an+mbncm

anbncbmam

Table 2: Incremental learning NSPDA (without hints) performance results. Each value is a measurement of the average
number of characters required to reach convergence (M1 = 2nd Order NSPDA, M2 = 3rd order NSDPA).

an+mbncm
Regularization Method M1 M2 M1 M2 M1 M2 M1 M2
w/o reg
2.00
w reg
0.00

P alindrome

anbncbmam

2.99
0.00

1.28
0.06

2.18
0.09

4.19
0.00

4.55
0.00

1.55
0.01

5.51
0.99

anbn

Table 3: Mean classiﬁcation error for an NSPDA w/ & w/o adaptive noise (tested on string length up to T = 60).

RNN Type
RNN
LSTM
LSTM-p
GRU
Stack RNN 40+10
Stack RNN 40+10+ rounding
listRNN 40+5
2nd Order RNN
2nd Order RNN reg (ours)
NNPDA
NNPDA reg (ours)
NSPDA, M1 (ours)
NSPDA, M2 (ours)

P alindrome
Test
Train
78.2
0.00
12.58
0.00
8.69
0.00
14.99
0.00
4.99
0.00
0.09
0.00
0.39
0.00
9.26
0.00
1.88
0.00
7.00
0.00
4.28
0.00
0.00
0.00
0.00
0.00

anbn

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Test
74.11
13.26
11.25
14.89
3.01
0.89
2.29
8.51
2.09
15.25
14.20
0.06
0.01

anbncbmam
Test
83.33
14.22
13.99
19.22
34.19
1.01
19.63
17.52
2.19
17.49
13.00
0.99
0.00

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

an+mbncm
Test
73.69
10.56
12.88
14.00
58.66
0.79
1.27
11.17
0.99
55.28
41.01
0.09
0.00

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

P arenthesis
Test
Train
99.96
30.72
97.88
48.92
99.00
49.68
98.70
43.21
9.38
10.25
3.968
4.03
7.45
4.89
37.42
27.89
27.59
21.69
29.21
5.96
27.09
5.62
2.58
0.58
0.88
0.01

Table 4: Mean classiﬁcation error for various recurrent architectures when tested on strings of length up to T = 60.

RNNs of this type in this study3 (in the appendix, we offer a comparison of the various weight update rules when
training an NSPDA). Gradients were hard clipped to 13. Parameters were updated using stochastic gradient descent
(SGD) which made use of the stochastic learning rate annealing scheme proposed in [29] with initial learning rate of
0.1005000321. All models were trained for a maximum of 500 epochs (or until convergence was reached, which was
marked as 100% training accuracy). Experiments for each and every model was repeated 5 times.

All of our models used our proposed rule encoding scheme and all of the RNNs were trained using our proposed
two-stage incremental learning procedure. In Table 2, to demonstrate the value of our proposed two stage incremental
training procedure (2-IL), we compare an NSPDA trained without any incremental learning, one with ours, and one
with the incremental learning approach (IL) proposed in [26] and ﬁnd that the our approach yields the best results
across all grammars. All higher-order RNNs made use of our proposed adaptive noise regularizer, though in Table
3, we examine how the NSPDA performs with and without the proposed regularizer. With respect to the hints used,
for all tables presented in the main paper, whenever hint usage is indicated, we mean Hint #2 (which worked the best
empirically). In the appendix, we provide a detailed breakdown and ablation for all of the models investigated in this
paper. Speciﬁcally, we present results for models that were trained with and without our regularizer as well as under
various hint insertion conditions (no hints, Hint #1, and Hint #2).

3For all ﬁrst order RNNs, we found BPTT worked best and use that to train all RNNs of this type in our experiments.

8

Baseline Algorithms: In order to provide the proper context do demonstrate the effectiveness of our proposed NSPDA,
we conduct a thorough comparison of our model to as many baseline RNN models as possible. These models include
a plethora of ﬁrst order RNNs such as variations of the stack-RNN [12] (depth k = 2, all other metaparameters set
according to original source) including the two variant models as well as the linked-list model (using the same model
labels as the original paper), the Long Short Term Memory RNN [35] with (LSTM) and without peepholes (LSTM-p),
the Gated Recurrent Unit (GRU) RNN [25], and a simple Elman RNN. We also compared to gated ﬁrst order RNNs
with multiplicative units, but due to space constraints, we report these results in the appendix. We furthermore compare
against second order RNNs with (2nd Order RNN) and without regularization (2nd Order RNN reg), as well as the
classical NNPDA with and without regularization (NNPDA reg). All baselines RNNs had a single layer of ≤ 50 neurons
and individual hyperparameters for each was optimized based on validation set performance.

Results and Discussion

To the best of our knowledge, we are the ﬁrst to conduct a comparison across such a wide variety of RNN models of
both ﬁrst, second, and third order, with and without external (stack-based) memory. For simple algorithmic patterns
(non-Dyck(2) CFGs), ﬁrst order RNNs like the LSTM and GRU perform reasonably well, primarily because they
utilize dynamic counting [3, 7] but yet do not learn any state transitions. This is evidenced when considering their
performance on on the complex Dyck(2) CFG where the majority of RNNs exhibit great difﬁculty in generalizing to
longer sequences. These results do corroborate those of prior work, speciﬁcally those that demonstrate that the LSTM
essentially performs a form of dynamic counting, making it ill-suited to recognizing complex grammars [36].

As pointed out by [36] there is a strong need for neural architectures with external memory, i.e., a stack, to solve
complex CFGs but, in this study, we furthermore argue that prior knowledge is also needed as well. This makes sense
given that is known that prior information often leads to greatly improved reasoning and better generalization [21].
The stack and list RNNs do make use of (continuous) external memory (in fact, multiple stack/lists) but, theoretically,
only one stack should be sufﬁcient to recognize a PDA of any arbitrary length while a 2-stack PDA is as powerful as a
Turning machine [37]. However, quite surprisingly, a stack-RNN with even 10 stacks has difﬁculty in generalizing
to a complex grammar. This lines up with the theory – [37] has proven that adding any more than 2 stacks to a PDA
does not provide any further computational advantage.

Finally, it is impressive to see that high order RNNs coupled with external memory, particularly with a discrete stack
structure (as opposed to a continuous stack like that of the stack-RNN), perform so well across all CFGs. It is important
to note that even the way our state-based RNN operates is markedly different than the way those of the past did – the
NSPDA works as a next-step prediction model, which allows us to use the powerful iterative reﬁnement procedure
as a way to aggressively error correct its states when predicting string validity (at least during training time). Table
4 shows that our NSPDA model generalizes very well when trained on sequences of length T ≤ 21 but tested on
sequences on length up to T = 60. Finally, our results demonstrate the value of rule insertion, which, as we see
empirically, in some cases, improved convergence speed by a wide margin.

Conclusions

In this work, we proposed the neural state pushdown automate (NSPDA) and its learning process, which utilizes an
iterative reﬁnement-based loss function, a two-stage incremental training procedure, an adaptive noise regularization
scheme (which works with any higher order network), and a method for stably encoding rules into the model itself.

Our experimental results, which focused on context-free grammars (CFGs), demonstrate that prior knowledge is essential
to learning memory-augmented that recognize complex CFGs well. Notably, we have empirically demonstrated the
expressvity and ﬂexibility of a high order temporal neural model that learns how to manipulate an external discrete stack.
While our proposed neural model works with a discrete stack, our model’s underlying framework could be extended to
manipulate other kinds of data structures, a subject of future work. When training on various CFGs, the state-based
neural models we optimize converge faster and are more expressive than even powerful classical models such as
the neural network pushdown automaton. Furthermore, we have shown that modern-day, popular recurrent network
structures (all of which are ﬁrst order) struggle greatly to recognize complex grammars.These discovered limitations of
ﬁrst order RNNs indicates that ANN research should consider the exploration of more expressive, memory-augmented
models that offer ways to better integrate prior knowledge.

9

References

[1] Dana Angluin and Carl H Smith. Inductive inference: Theory and methods. ACM Computing Surveys (CSUR), 15(3):237–269,

1983.

[2] King Sun Fu, editor. Syntactic Pattern Recognition, Applications. Springer Berlin Heidelberg, 1977.

[3] F. A. Gers and E. Schmidhuber. Lstm recurrent networks learn simple context-free and context-sensitive languages. IEEE

Transactions on Neural Networks, 12(6):1333–1340, Nov 2001.

[4] Mikael Bodén and Janet Wiles. Context-free and context-sensitive dynamics in recurrent neural networks. Connection Science,

12(3-4):197–210, 2000.

[5] Whitney Tabor. Fractal encoding of context-free grammars in connectionist networks. Expert Systems, 17(1):41–56, 2000.

[6] Janet Wiles and Jeff Elman. Learning to count without a counter: A case study of dynamics and activation landscapes in
recurrent networks. In Proceedings of the seventeenth annual conference of the cognitive science society, number s 482, page
487. Erlbaum Hillsdale, NJ, 1995.

[7] Luzi Sennhauser and Robert C Berwick. Evaluating the ability of lstms to learn context-free grammars. arXiv preprint

arXiv:1811.02611, 2018.

[8] Hyoungwook Nam, Segwang Kim, and Kyomin Jung. Number sequence prediction problems for evaluating computational
powers of neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4626–4633,
2019.

[9] C Lee Giles, Clifford B Miller, Dong Chen, Hsing-Hen Chen, Guo-Zheng Sun, and Yee-Chun Lee. Learning and extracting

ﬁnite state automata with second-order recurrent neural networks. Neural Computation, 4(3):393–405, 1992.

[10] Jordan B Pollack. Recursive distributed representations. Artiﬁcial Intelligence, 46(1-2):77–105, 1990.

[11] Zheng Zeng, Rodney M Goodman, and Padhraic Smyth. Discrete recurrent neural networks for grammatical inference. IEEE

Transactions on Neural Networks, 5(2):320–330, 1994.

[12] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural

information processing systems, pages 190–198, 2015.

[13] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded

memory. In Advances in neural information processing systems, pages 1828–1836, 2015.

[14] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.

[15] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. arXiv preprint arXiv:1511.06392,

2015.

[16] Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, and Simon Mendelsohn. Context-free

transductions with neural stacks. arXiv preprint arXiv:1809.02836, 2018.

[17] Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. Memory
architectures in recurrent neural network language models. In International Conference on Learning Representations, 2018.

[18] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi´nska, Sergio Gómez
Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with
dynamic external memory. Nature, 538(7626):471, 2016.

[19] Christian W Omlin and C Lee Giles. Constructing deterministic ﬁnite-state automata in recurrent neural networks. Journal of

the ACM (JACM), 43(6):937–972, 1996.

[20] Peter Tiˇno, Bill G Horne, C Lee Giles, and Pete C Collingwood. Finite state machines and recurrent neural networks—automata

and dynamical systems approaches. In Neural networks and pattern recognition, pages 171–219. Elsevier, 1998.

[21] Drew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine. CoRR, abs/1907.03950,

2019.

[22] Y. S. Abu-Mostafa. Learning from hints in neural networks. J. Complex., 6(2):192–198, June 1990.

[23] G. Z. Sun, C. L. Giles, and H. H. Chen. The neural network pushdown automaton: Architecture, dynamics and training, pages

296–345. Springer Berlin Heidelberg, Berlin, Heidelberg, 1998.

[24] Samuel A. Korsky and Robert C. Berwick. On the computational power of rnns. CoRR, abs/1906.06349, 2019.

[25] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural

networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

[26] Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. Using prior knowledge in a nnpda to learn context-free languages. In

Advances in neural information processing systems, pages 65–72, 1993.

10

[27] R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural Computation,

1(2):270–280, June 1989.

[28] Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. CoRR, abs/1702.05043, 2017.

[29] Alexander G Ororbia, Ankur Mali, Jian Wu, Scott O’Connell, William Dreese, David Miller, and C Lee Giles. Learned neural
iterative decoding for lossy image compression systems. In 2019 Data Compression Conference (DCC), pages 3–12. IEEE,
2019.

[30] Alexander Ororbia, Ankur Mali, Daniel Kifer, and C Lee Giles. Lifelong neural predictive coding: Sparsity yields less forgetting

when learning cumulatively. arXiv preprint arXiv:1905.10696, 2019.

[31] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99,

1993.

[32] Alexander G Ororbia and Ankur Mali. Biologically motivated algorithms for propagating local target representations. In

Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4651–4658, 2019.

[33] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

Deep Learning. MIT Press, 2016.

http://www.

deeplearningbook.org.

[34] David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal,
Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv
preprint arXiv:1606.01305, 2016.

[35] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[36] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M. Shieber. LSTM networks can perform dynamic counting.

CoRR, abs/1906.03648, 2019.

[37] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to Automata Theory, Languages, and Computation

(3rd Edition). Pearson, 2006.

[38] Ronald J Williams and David Zipser. Gradient-based learning algorithms for recurrent networks and their computational

complexity. Backpropagation: Theory, architectures, and applications, 433, 1995.

[39] Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without backtracking. arXiv

preprint arXiv:1507.07680, 2015.

11

Appendix

Additional Results

In Table 7, we report an expansion of the model performance table that appears in the main paper. In it, we report the
performance of 3 modern gated RNNs with multiplicative gating units, i.e., MI-RNN, MI-LSTM, MI-GRU. Interestingly
enough, one could consider the multiplicative units to be a crude approximation of second order state neurons.

Table 5 shows results for stably programming the weights of the NSPDA which, in effect, demonstrates that a
programmed NSPDA (without learning) is equivalent to complex grammar PDA.

In the other table (Table 6), we highlight how various learning algorithms affect the generalization ability of higher
order recurrent networks. Here, we compare back-propagation through time (BPTT) to other online learning algorithms
such as real time recurrent learning (RTRL) and unbiased online recurrent optimization (UORO). We describe these
procedures in further detail in the next section.

Notably, in our experiments, we observed that UORO boosts performance for higher order recurrent networks, while
being faster than RTRL, the original algorithm-of-choice when training higher order, state-based models. Furthermore,
we remark that truncated BPTT (TBPTT), for some CFGs, can actually slightly improve model performance over BPTT
(but in ohers, such as is the case for the palindrome CFG, lead to worse generalization).

On Training Algorithms

For all of the RNNs we study, we compared their (validation) performance when using various online and ofﬂine based
learning algorithms. As mentioned in the last section, we found that UORO worked best for the NSPDA, which is
advantageous in that UORO is faster than RTRL (even largely in terms of complexity) and does not require model
unfolding like the popular and standard BPTT/TBPTT algorithms do. These results, again, are summarized in Table 6.

Below we brieﬂy describe the non-standard approaches to learning RNNs, speciﬁcally RTRL and UORO. Notably, we
are the ﬁrst to implement and adapt UORO in calculating the updates to the weights of higher order networks.

Real-Time Recurrent Learning

Real-time recurrent learning (RTRL) is a classical online learning procedure for training RNNs [27]. The aim is to
optimize the parameters Θ of a state-based model in order to minimize a total (sequence) loss. The state model is
abstract to the following function:

zt+1 = Fstate(xt+1, zt, Θ).

(10)

RTRL computes the derivative of the model’s states and outputs with respect to the synaptic weights during the model’s
forward computation, as data points in the sequence are processed iteratively, i.e., without any unfolding as in BPTT.
When the task is next step prediction (predict xt given a history x<t), the loss L to optimize, using RTRL, is deﬁned as
follows:

∂Lt+1
∂Θ

=

∂Lt+1(yt+1, y∗

t+1)

∂y

⊗

(cid:18) ∂Fout(xt+1, zt, Θ)
∂zt

∂zt
∂Θ

+

∂Fout(xt+1, zt, Θ)
∂Θ

(cid:19)

.

(11)

Once we differentiate Equation 10 with respect to Θ, we obtain:

∂zt + 1
∂Θ

=

∂Fstate(xt+1, zt, Θ)
∂Θ

+

∂Fstate(xt+1, zt, Θ)
∂zt

⊗

∂zt
∂Θ

.

(12)

Where at each time we compute ∂zt

∂Θ based on ∂zt−1

∂Θ . These values are then used to directly compute ∂zt+1
∂Θ .

The above is, in short, how RTRL calculates its gradients without resorting to backward transfer or computation graph
unfolding (as in reverse-mode differentiation). Since the shape of ∂zt
∂Θ is the same as |z| × |Θ|, for standard RNNs with n
hidden units, this calculation scales as n4 (time complexity [38]). This high complexity makes RTRL highly impractical
for training very wide and very deep recurrent models. However, in the case of a third order model like NSPDA (or an
NNPDA), the number of states need for learning a target grammar are generally far fewer than those required of second
or ﬁrst order models (as we mentioned in the main paper). This means that a procedure such as RTRL is still applicable
and useful at least for training RNNs to recognize context free grammars (of low input dimensionality).

12

Model
2nd Order NSPDA
3rd Order NSPDA

P alindrome
n=480
0.0
0.0

n=960
0.0
0.0

n=60
0.0
0.0

n=60
0.0
0.0

anbn
n=480
0.0
0.0

n=960
0.0
0.0

n=60
0.0
0.0

anbncbmam
n=480
0.0
0.0

n=960
0.0
0.0

an+mbncm
n=480
0.0
0.0

n=960
0.0
0.0

n=60
0.0
0.0

Table 5: Mean classiﬁcation error results when using a programmed NSPDA (lower is better).

anbn

P alindrome

an+mbncm
Learning Algorithm M1 M2 M1 M2 M1 M2 M1 M2
BPTT
1.59
2.23
TBPTT
1.11
1.05
RTRL
0.01
0.09
UORO
0.00
0.06

2.02
2.59
0.02
0.00

2.79
2.02
0.07
0.00

2.55
1.29
0.10
0.01

2.99
2.97
1.85
0.99

2.99
1.58
0.09
0.09

1.99
2.81
0.19
0.00

anbncbmam

Table 6: Mean classiﬁcation error for the NSPDA trained via various learning algorithms (tested on string length up to
T = 60).

RNN Type
RNN
LSTM
LSTM-p
GRU
Stack RNN 40+10
Stack RNN 40+10+ rounding
listRNN 40+5
MI-RNN
MI-LSTM
MI-GRU
2nd Order RNN
2nd Order RNN reg (ours)
NNPDA
NNPDA reg (ours)
NSPDA, M1 (ours)
NSPDA, M2 (ours)

P alindrome
Test
Train
78.2
0.00
12.58
0.00
8.69
0.00
14.99
0.00
4.99
0.00
0.09
0.00
0.39
0.00
75.69
0.00
9.99
0.00
16.22
0.00
9.26
0.00
1.88
0.00
7.00
0.00
4.28
0.00
0.00
0.00
0.00
0.00

anbn

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Test
74.11
13.26
11.25
14.89
3.01
0.89
2.29
70.26
10.86
13.29
8.51
2.09
15.25
14.20
0.06
0.01

anbncbmam
Test
83.33
14.22
13.99
19.22
34.19
1.01
19.63
76.69
13.55
20.02
17.52
2.19
17.49
13.00
0.99
0.00

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

an+mbncm
Test
73.69
10.56
12.88
14.00
58.66
0.79
1.27
73.01
14.22
14.83
11.17
0.99
55.28
41.01
0.09
0.00

Train
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

P arenthesis
Test
Train
99.96
30.72
97.88
48.92
99.00
49.68
98.70
43.21
9.38
10.25
3.968
4.03
7.45
4.89
99.92
29.58
99.80
47.83
99.20
42.88
37.42
27.89
27.59
21.69
29.21
5.96
27.09
5.62
2.58
0.58
0.88
0.01

Table 7: Mean classiﬁcation error for various recurrent architectures when tested on strings of length up to T = 60.

Unbiased Online Recurrent Optimization

Unbiased Online Recurrent Optimization (UORO) [28] uses a rank-one trick to approximate the operations need to
make RTRL’s gradient computation work. This trick helps to reduce the overall complexity of the at the price of
increasing variance of its gradient estimates.
When designing an optimizer like UORO, we start from the idea that for any given unbiased estimation of ∂zt
∂Θ , we can
form a stochastic matrix ˜Zt such that E( ˜Zt) = ∂zt
∂θ , the “unbiasedness” (of
gradient estimates) is preserved due to the linearlity of the expectation. Next, we compute the value of ˜Zt and plug
it into 11 and 12 to calculate the value for ∂Lt+1
∂Θ . In a rank-one, unbiased approximation, at time step t,
˜Zt = ˜zt ⊗ ˜Θt. To calculate ˆZt + 1 at t + 1, we plug in ˜Zt into 12. Nonetheless, mathematically, the above is still not
yet a rank-one approximation of RTRL.

∂Θ . Since Equation 11 and 12 are afﬁne in ∂zt

∂Θ and ∂zt+1

In order to ﬁnally obtain a proper rank-one approximation, one must use an additional, efﬁcient approximation technique,
proposed in [39], to rewrite the above equation as:

˜Zt+1

=

(cid:18)

ρ0

∂Fstate(xt+1, zt, θ)
∂z

˜zt + ρ1ν

(cid:19)

⊗

(cid:18) ˜θt
ρ0

+

(ν)T
ρ1

∂Fstate(xt+1, zt, θ)
∂θ

(cid:19)

.

(13)

13

Note that ν is a vector of independent, random signs and ρ contains k positive numbers. Thus, the rank-one trick
can be applied for any ρ. In UORO, ρ0 and ρ1 are factors meant to control the variance of the estimator’s computed
approximate derivatives. In practice, we deﬁne ρ0 as:
(cid:118)
(cid:117)
(cid:117)
(cid:116)

ρ0 =

(14)

(cid:107)˜θt(cid:107)
(cid:107) ∂Fstate(xt+1,zt,θ)
∂z

˜z(cid:107)

and ρ1 is deﬁned to be:

(cid:115)

ρ1 =

(cid:107)(ν)T ∂Fstate(xt+1,zt,θ)
(cid:107)ν(cid:107)

θ

(cid:107)

.

(15)

Initially, ˜z0 = 0 and ˜Θ0 = 0, which yields unbiased estimates at time t = 0. Given the construction of the UORO
procedure, by induction, all subsequent estimates can be shown to be unbiased as well.

14

