1
2
0
2

t
c
O
1

]
E
N
.
s
c
[

1
v
8
6
4
0
0
.
0
1
1
2
:
v
i
X
r
a

New Evolutionary Computation Models and
their Applications to Machine Learning
PhD thesis

Mihai Oltean

Department of Computer Science
Faculty of Mathematics and Computer Science
Babe¸s-Bolyai University, Kog˘alniceanu 1
Cluj-Napoca, 3400, Romania.
mihai.oltean@gmail.com

December 2004

 
 
 
 
 
 
Contents

1 Introduction

13
1.1 Machine Learning and Genetic Programming . . . . . . . . . . 13
1.2 Thesis structure and achievements . . . . . . . . . . . . . . . . 15
1.3 Other ML results not included in this Thesis . . . . . . . . . . 17

2 Genetic Programming and related techniques

18
2.1 Genetic Programming
. . . . . . . . . . . . . . . . . . . . . . 18
2.2 Cartesian Genetic Programming . . . . . . . . . . . . . . . . . 19
2.3 Gene Expression Programming
. . . . . . . . . . . . . . . . . 21
2.4 Linear Genetic Programming . . . . . . . . . . . . . . . . . . . 22
2.5 Grammatical Evolution . . . . . . . . . . . . . . . . . . . . . . 23

3 Multi Expression Programming

27
3.1 MEP basic ideas
. . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2 MEP algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.3 MEP representation . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4 MEP phenotypic transcription. Fitness assignment
. . . . . . 30
3.5 MEP representation revisited . . . . . . . . . . . . . . . . . . 32
. . . . . . . . . . . . . . . . . . . . . . . . . 34
3.6 Search operators
3.6.1 Crossover
. . . . . . . . . . . . . . . . . . . . . . . . . 34
3.6.2 Mutation . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.7 Handling exceptions within MEP . . . . . . . . . . . . . . . . 37
3.8 MEP complexity . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.9 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

4 MEP for Data Mining

39
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2 Symbolic regression . . . . . . . . . . . . . . . . . . . . . . . . 39

1

4.2.1 Problem statement . . . . . . . . . . . . . . . . . . . . 40
4.2.2 Numerical experiments . . . . . . . . . . . . . . . . . . 40
4.2.3 MEP vs. GEP . . . . . . . . . . . . . . . . . . . . . . . 45
4.2.4 MEP vs. CGP . . . . . . . . . . . . . . . . . . . . . . 46
4.2.5 MEP vs. GP . . . . . . . . . . . . . . . . . . . . . . . 47
4.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

5 Designing Digital Circuits with MEP

5.3 Designing digital circuits for arithmetic functions

50
5.1 Even-parity problem . . . . . . . . . . . . . . . . . . . . . . . 50
5.1.1 Problem statement . . . . . . . . . . . . . . . . . . . . 50
5.1.2 Numerical experiments . . . . . . . . . . . . . . . . . . 51
5.2 Multiplexer problem . . . . . . . . . . . . . . . . . . . . . . . 52
5.2.1 Problem statement . . . . . . . . . . . . . . . . . . . . 52
5.2.2 Numerical experiments . . . . . . . . . . . . . . . . . . 53
. . . . . . . 56
5.3.1 Problem statement . . . . . . . . . . . . . . . . . . . . 56
5.3.2 CGP for evolving digital circuits . . . . . . . . . . . . . 58
. . . . . . . . . . . . 59
5.3.3 MEP for evolving digital circuits
5.3.4 Numerical experiments . . . . . . . . . . . . . . . . . . 60
5.4 Designing digital circuits for NP-Complete problems . . . . . . 72
5.4.1 Evolving circuits for the knapsack problem . . . . . . . 72
5.4.2 Numerical experiments . . . . . . . . . . . . . . . . . . 73
5.5 Conclusions and Further Work . . . . . . . . . . . . . . . . . . 75

6 MEP for Evolving Algorithms and Game Strategies

6.1 Discovering game strategies

76
. . . . . . . . . . . . . . . . . . . 76
6.1.1 TTT game description . . . . . . . . . . . . . . . . . . 76
6.1.2 Chellapilla’s approach of TTT . . . . . . . . . . . . . . 77
6.1.3 MEP approach of TTT . . . . . . . . . . . . . . . . . . 78
6.2 Evolving winning strategies for Nim-like games . . . . . . . . . 82
Introduction . . . . . . . . . . . . . . . . . . . . . . . . 82
6.2.1
. . . . . . . . . . . . . . . . . . . 83
6.2.2 Basics on Nim game
6.2.3 Fitness assignment process . . . . . . . . . . . . . . . . 84
6.2.4 Numerical experiments . . . . . . . . . . . . . . . . . . 85
6.3 Evolving heuristics for NP-Complete problems . . . . . . . . . 91
6.3.1 MEP for TSP . . . . . . . . . . . . . . . . . . . . . . . 91
6.3.2 TSP problem with triangle inequality . . . . . . . . . . 92
6.3.3 Terminals and functions for evolving heuristic function f 93

2

6.3.4 Fitness assignment . . . . . . . . . . . . . . . . . . . . 94
6.3.5 A numerical experiment
. . . . . . . . . . . . . . . . . 95
6.3.6 Assessing the performance of the evolved MEP heuristic 96
6.4 Conclusions and further work . . . . . . . . . . . . . . . . . . 97
99
6.4.1 Applying MEP for generating complex game strategies

7 Inﬁx Form Genetic Programming

101
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7.1
7.2 Prerequisite . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
7.3
Individual representation . . . . . . . . . . . . . . . . . . . . . 102
7.4 Decoding IFGP individuals . . . . . . . . . . . . . . . . . . . . 103
7.5 Using constants within the IFGP model
. . . . . . . . . . . . 105
7.6 Fitness assignment process . . . . . . . . . . . . . . . . . . . . 106
7.7 Search operators
. . . . . . . . . . . . . . . . . . . . . . . . . 108
. . . . . . . . . . . . . . . . . . . . . . . . . 108
7.7.1 Crossover
7.7.2 Mutation . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.8 Handling exceptions within IFGP . . . . . . . . . . . . . . . . 108
IFGP algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.9
7.10 Solving classiﬁcation problems using IFGP . . . . . . . . . . . 109
7.10.1 Data sets
. . . . . . . . . . . . . . . . . . . . . . . . . 109
7.10.2 Numerical experiments . . . . . . . . . . . . . . . . . . 110
7.11 Conclusion and further work . . . . . . . . . . . . . . . . . . . 112

8 Multi Solution Linear Genetic Programming

116
8.1 MS-LGP representation and ﬁtness assignment process . . . . 116
8.2 Numerical experiments . . . . . . . . . . . . . . . . . . . . . . 118
8.3 Conclusions and further work . . . . . . . . . . . . . . . . . . 121

9 Evolving Evolutionary Algorithms

123
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

9.1
9.2 Evolving evolutionary algorithms using Multi Expression Pro-

gramming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
9.2.1 Evolutionary model . . . . . . . . . . . . . . . . . . . . 125
9.2.2 Fitness assignment . . . . . . . . . . . . . . . . . . . . 126
9.2.3 Numerical experiments . . . . . . . . . . . . . . . . . . 128

9.3 Evolving evolutionary algorithms with Linear Genetic Pro-

gramming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
Individual representation for evolving EAs . . . . . . . 130
9.3.1

3

9.3.2 Fitness assignment . . . . . . . . . . . . . . . . . . . . 133
9.3.3 The model used for evolving EAs . . . . . . . . . . . . 133
9.3.4 Evolving EAs for function optimization . . . . . . . . . 134
9.3.5 Evolving EAs for TSP . . . . . . . . . . . . . . . . . . 142
9.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

10 Searching for a Practical Evidence of the No Free Lunch

150
Theorems
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
10.2 Basics on No Free Lunch Theorems . . . . . . . . . . . . . . . 152
10.3 A NFL-style algorithm . . . . . . . . . . . . . . . . . . . . . . 152
10.4 Evolutionary Model and the Fitness Assignment Process
. . . 154
10.5 Algorithms used for comparison . . . . . . . . . . . . . . . . . 155
10.6 Numerical experiments . . . . . . . . . . . . . . . . . . . . . . 155
10.7 Conclusions and further work . . . . . . . . . . . . . . . . . . 157

11 Conclusions and further work

158

4

List of Figures

2.1 A mathematical expression in inﬁx form (a), Polish form (c)

and the corresponding program tree (b).

. . . . . . . . . . . . 19

2.2 A CGP program with 5 inputs, 2 outputs and 3 functions (0,
1, 2 inside square nodes). The grey squares represent uncon-
nected nodes.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 20

4.1 The relationship between the success rate and the number
of symbols in a MEP chromosome. The number of symbols
in chromosome varies between 5 and 100. The results are
summed over 100 runs. . . . . . . . . . . . . . . . . . . . . . . 41

4.2 The relationship between the success rate and the number
of symbols in a MEP chromosome. The number of symbols
in chromosome varies between 5 and 300. The results are
summed over 100 runs. . . . . . . . . . . . . . . . . . . . . . . 42

4.3 Success rate of the MEP algorithm. Population size varies

between 5 and 100. The results are summed over 100 runs.

. . 44

4.4 Relationship between the success rate of MEP algorithm and
the number of generations used in the search process. The
number of generations varies between 10 and 300. The results
are summed over 100 runs. . . . . . . . . . . . . . . . . . . . . 45

4.5 The relationship between the number of genes in a chromo-
some and the MEP success rate. The number of genes in a
chromosome varies between 10 and 100. The results are aver-
aged over 100 runs.

. . . . . . . . . . . . . . . . . . . . . . . . 49

5.1 An evolved circuit for the even-3-parity problem.
5.2 An evolved circuit for the even-4-parity problem.

. . . . . . . 52
. . . . . . . 53

5

5.3 The success rate of the MEP algorithm for solving the 6-
multiplexer problem. (a) The relationship between the suc-
cess rate and the chromosome length.
(b) The relationship
between the success rate and the population size. Results are
summed over 100 runs. . . . . . . . . . . . . . . . . . . . . . . 55

5.4 The symbols used to represent some of the logical gates in
Table 5.3 (OR is function 12, AND is function 6, XOR is
function 10 and MUX is function 16). In some pictures a small
circle may appear on these symbols indicating the negation
(inversion) of the respective results.

. . . . . . . . . . . . . . . 57

5.5 A Cartesian Genetic Programming program for 1-bit adder

problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

5.6 The relationship between the success rate of the MEP algo-
rithm and (a) number of genes in a chromosome, (b) the num-
ber of individuals in population. Results are averaged over
100 runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

5.7 The relationship between the success rate of the MEP algo-
rithm and (a) number of genes in a chromosome, (b) the num-
ber of individuals in population. Results are averaged over
100 runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

5.8 The relationship between the success rate of the MEP algo-
rithm and (a) number of genes in chromosome, (b) the number
of individuals in population. Results are averaged over 100 runs. 68

5.9 The cumulative probability of success and the number of in-
dividuals to be processed in order to obtained a solution with
99% probability. Results are averaged over 100 runs.

. . . . . 70

5.10 The relationship between the success rate of the MEP algo-
rithm and (a) number of genes in a chromosome, (b) the num-
ber of individuals in population. Results are averaged over
100 runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

6.1 Game board linearized representation.
6.2 Fitness of the best individual in the best runs and the average
ﬁtness of the best individuals over all runs. The results are
taken over 30 runs.

. . . . . . . . . . . . . . . . . . . . . . . . 80

. . . . . . . . . . . . . 78

6

6.3 The game tree for a Nim game that starts with the conﬁgu-
ration (2, 1). At the right side of each game conﬁguration is
printed the conﬁguration’ state (P -position or N -position) as
computed by the formula E = a1 − a1*a2. The conﬁgurations
that violate one of the three rules described above are encircled. 86

6.4 The relationship between the population size and the rate of
success. The results are averaged over 50 runs. The population
size varies between 20 and 200.

. . . . . . . . . . . . . . . . . 87

6.5 The relationship between the number of generations and the
rate of success. The results are averaged over 50 runs. The
number of generations varies between 20 and 200.

. . . . . . . 89

6.6 The relationship between the chromosome length and the suc-
cess rate. The results are averaged over 50 runs. The chromo-
some length varies between 5 and 50.

. . . . . . . . . . . . . . 90

6.7 The ﬁtness evolution of the best individual in the best run and
the average ﬁtness of the best individuals over 30 runs.

. . . . 96

7.1 The expression tree of E = b / (a + a).

. . . . . . . . . . . . . 105

8.1 The relationship between the success rate and the number of
instructions in a chromosome. Results are averaged over 100
runs.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

8.2 The relationship between the population size and the success
rate. Population size varies between 10 and 100. Results are
averaged over 100 runs. . . . . . . . . . . . . . . . . . . . . . . 122

9.1 The ﬁtness of the best individual in the best run and the
average (over 10 runs) of the ﬁtness of the best individual
over all runs.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 130

9.2 The ﬁtness of the best individual in the best run and the
average (over 10 runs) of the ﬁtness of the best individual
over all runs.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 131

9.3 The relationship between the ﬁtness of the best individual in
each generation and the number of generations. Results are
averaged over 10 runs.

. . . . . . . . . . . . . . . . . . . . . . 139

7

9.4 The relationship between the number of generations and the
quality of the solutions obtained by the Evolved EA (EEA)
and by the Standard GA (SGA) for the unimodal test func-
tions f1 − f6. The number of generations varies between 20
and 600. Results are averaged over 30 runs.

. . . . . . . . . . 143

9.5 The relationship between the number of generations and the
quality of the solutions obtained by the Evolved EA (EEA)
and by the Standard GA (SGA) for the multimodal test func-
tions f7 − f10. The number of generations varies between 20
and 600. Results are averaged over 30 runs.

. . . . . . . . . . 144

8

List of Tables

3.1 MEP one-point recombination. . . . . . . . . . . . . . . . . . . 35
. . . . . . . . . . . . . . . . . 36
3.2 MEP two-point recombination.
3.3 MEP uniform recombination.
. . . . . . . . . . . . . . . . . . 36
. . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.4 MEP mutation.

. . . . . . . . . . 41
4.1 Algorithm parameters for the Experiment 1.
. . . . . . . . . . . . 43
4.2 Algorithm parameters for Experiment 3.
4.3 Algorithm parameters for Experiment 4.
. . . . . . . . . . . . 44
4.4 Algorithm parameters for the MEP vs. CGP experiment. . . . 47
. . . . . 48
4.5 GP parameters used for solving the quartic problem.

5.1 The MEP algorithm parameters for the numerical experiments

with even-parity problems. . . . . . . . . . . . . . . . . . . . . 51

5.2 MEP algorithm parameters for the numerical experiments with

6-multiplexer problem.

. . . . . . . . . . . . . . . . . . . . . . 54

5.3 Function set (gates) used in numerical experiments. Some
functions are independent on the input (functions 0 and 1),
other depend on only one of the input variables (functions 2-
5), other functions depend on two input variables (functions
6-15) and the other functions depends on three input variables
(functions 16-19). These functions are taken from [58].

. . . . 57
5.4 Representation of some functions given in Table 5.3. . . . . . . 58
. . . . . . . . . . . . . . . 62
5.5 Parameters of the CGP algorithm.
5.6 Parameters of the MEP algorithm.
. . . . . . . . . . . . . . . 62
5.7 Computation eﬀort spent for evolving two-bit multipliers for
diﬀerent population sizes. CGP results are taken from [58].
The diﬀerences ∆ in percent considering the values of MEP as
a baseline are given in the last column. Results are averaged
over 100 runs.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 63

9

5.8 Parameters of the MEP algorithm for evolving digital circuits.
5.9 Parameters of the MEP algorithm for solving the 3-Bit Adder

67

problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

5.10 General parameters of the MEP algorithm for evolving digital

circuits.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

5.11 Particular parameters of the MEP algorithm for diﬀerent in-
stances of the knapsack problem. In the second column the
base set of numbers is given for each instance. In the third
column the target sum is given.

. . . . . . . . . . . . . . . . . 74

5.12 Results obtained by MEP for the considered test problems.

100 independent runs have been performed for all problems.

. 74

6.1 Algorithm parameters for TTT game. . . . . . . . . . . . . . . 79
. . . . . . . . . 87
6.2 MEP algorithm parameters for Experiment 1.
. . . . . . . . . 88
6.3 MEP algorithm parameters for Experiment 2.
. . . . . . . . . 89
6.4 MEP algorithm parameters for Experiment 3.
6.5 MEP algorithm parameters for Experiment 4.
. . . . . . . . . 91
6.6 MEP algorithm parameters for evolving a heuristic for TSP

with triangle inequality . . . . . . . . . . . . . . . . . . . . . . 95

6.7 Evolved MEP heuristic vs. NN, MST. For each graph class we
present the number of graphs for which evolved MEP heuris-
tic generates a cycle shorter than the cycle obtained by the
algorithm MST and NN.

. . . . . . . . . . . . . . . . . . . . . 97

6.8 The performance of evolved MEP heuristic, NN and MST on
some problems in TSPLIB. Length is the length of the TSP
path obtained with one of the considered heuristics. Error is
calculated as (Length - Shortest Length)/ Shortest Length *
100. Each node of the graph has been considered as the ﬁrst
node of the path . . . . . . . . . . . . . . . . . . . . . . . . . 98

7.1 Summarized attributes of several classiﬁcation problems from

PROBEN1.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

7.2 Linear GP parameters used for solving classiﬁcation tasks from

7.3

PROBEN1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
IFGP algorithm parameters for solving classiﬁcation problems
from PROBEN1.

. . . . . . . . . . . . . . . . . . . . . . . . . 112

10

7.4 Classiﬁcation error rates of IFGP, LGP and ANN for some
date sets from PROBEN1. LGP results are taken from [9].
ANNs results are taken from [86]. The cases where IFGP is
better than LGP have been written on a grey background.
The cases where IFGP is better than ANNs have been bolded
and italicized. Results are averaged over 30 runs . . . . . . . . 113

7.5 Classiﬁcation error rates of IFGP (on the test set) using dif-
ferent number of constants. The cases where IFGP is better
than LGP have been written on a grey background. The cases
where IFGP is better than ANNs have been bolded and itali-
cized. Results are averaged over 30 runs.

. . . . . . . . . . . . 114

8.1 The parameters of the LGP algorithm for symbolic regression

problems.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

9.1 Evolutionary Algorithms encoded in the MEP chromosome C. 127
9.2 The parameters of the MEP algorithm for Experiment 1.
. . . 129
9.3 Test functions used in our experimental study. The parameter
n is the space dimension (n = 5 in our numerical experiments)
and fmin is the minimum value of the function.

. . . . . . . . 135
9.4 The parameters of a standard GA for Experiment 1. . . . . . . 138
9.5 The parameters of the LGP algorithm used for Experiment 1. 138
9.6 The parameters of the evolved EA for function optimization.
. 139
9.7 The results of applying the Evolved EA and the Standard
GA for the considered test functions. StdDev stands for the
standard deviation. The results are averaged over 30 runs.

. . 140

9.8 The results of applying the Evolved EA and the Standard
GA for the considered test functions. StdDev stands for the
standard deviation. Results are averaged over 30 runs.

. . . . 141
9.9 The parameters of the LGP algorithm used for Experiment 5. 145
9.10 The parameters of the evolved EA for TSP.
. . . . . . . . . . 145
9.11 The results of the standard GA and Evolved EA for 27 in-
stances from TSPLIB. Mean stands for the mean over all runs
and StdDev stands for the standard deviation. The diﬀerence
∆ is in percent and it is computed considering the values of
the Evolved EA as a baseline. Results are averaged over 30
runs.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

11

9.12 The results of the Evolved EA for function optimization (see
Appendix 1) and Evolved EA for TSP (see Appendix 2) for
27 instances from TSPLIB. Mean stands for the mean over
all runs and StdDev stands for the standard deviation. The
diﬀerence ∆ is in percent and it is computed considering the
values of the Evolved EA for TSP as a baseline. Results are
averaged over 30 runs.

. . . . . . . . . . . . . . . . . . . . . . 148

10.1 The variables used by the NFL algorithm.
10.2 The parameters of the GP algorithm used for numerical ex-

. . . . . . . . . . . 153

periments

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.3 The evolved test functions. . . . . . . . . . . . . . . . . . . . . 156

12

Chapter 1

Introduction

This chapter serves as an introduction in the ﬁeld of Machine Learning and
Genetic Programming. The last section presents the goals and the achieve-
ments of the thesis.

1.1 Machine Learning and Genetic Program-

ming

Automatic Programming is one of the most important areas of computer
science research today. Hardware speed and capability has increased expo-
nentially, but the software is years behind. The demand for software has also
increased signiﬁcantly, but it is still written in old-fashion: by using humans.
There are multiple problems when the work is done by humans: cost,
time, quality. It is costly to pay humans, it is hard to keep them satisﬁed for
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).

The real advances in the human civilization appeared during the indus-
trial revolutions. Before the ﬁrst revolution most people worked in agricul-
ture. Today, very few percent of people work in this ﬁeld.

A similar revolution must appear in the computer programming ﬁeld.
Otherwise we will have so many people working in this ﬁeld as we had in the
past working in agriculture.

How people know how to write computer programs? Very simple: by
learning. Can we do the same for software? Can we put the software to
learn how to write software?

13

It seems that is possible (to some degree) and the term is called Machine
Learning. It was ﬁrst coined in 1959 by the ﬁrst person who made a computer
perform a serious learning task, namely Arthur Samuel [89].

However, things are not so easy as in humans (well, truth to be said -
for some humans it is impossible to learn how to write software). So far
we do not have a software which can learn perfectly to write software. We
have some particular cases where some programs do better than humans, but
the examples are sporadic at best. Learning from experience is diﬃcult for
computer programs.

Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature. What is the advantage of na-
ture when solving problems? Answer: Nature is huge and can easily solve
problems by brute force. It can try a huge number of possibilities and choose
the best one (by a mechanism called survival).

Genetic algorithms are inspired by nature. They have random generation
of solutions, they have crossover, mutation, selection and so on. However,
genetic algorithms cannot simulate the nature perfectly because they run on
computers which have limited resources.

Can genetic algorithms learn programming? Yes. The subﬁeld is called

Genetic Programming [42] and is quite popular today.

In Genetic Programming, we evolve computer programs by using ideas

inspired from nature.

Because Genetic Programming belongs to Machine learning, it should be
able to learn. It can do that if it has a set of so called training data which
is nothing but a set of pairs (input;output). The most important aspect
of learning in Genetic Programming is how the ﬁtness is computed. That
measurement tells us how great is learning so far.

After training we usually use a so called test set to see if the obtained
program is able to generalize on unseen data. If we still get minimum errors
on test set, it means that we have obtained an intelligent program.

However, one should not imagine that things are so easy. Currently Ge-
netic Programming has generated some good results, but in general it cannot
generate large computer programs. It cannot generate an operating system.
Not even an word editor, not even a game. The programmer of such system
should know its limitations and should help the system to reduce the search
space. It is very important to ask Genetic Programming to do what humans
cannot do.

There are several questions to be answered when solving a problem using

14

a GP/ML technique. Some of the questions are:

How are solutions represented in the algorithm?

What search operators does the algorithm use to move in the solution

space?

What type of search is conducted?

Is the learning supervised or unsupervised?

Shortly speaking, Genetic Programming (GP) [41, 42] is a special sub-
domain of the ML. GP individuals are computer programs evolved by using
speciﬁc genetic operators. The GP search is guided by a ﬁtness function and
the learning process is usually unsupervised.

1.2 Thesis structure and achievements

This thesis describes several evolutionary models developed by the author
during his PhD program.

Chapter 2 provides an introduction to the ﬁeld of Evolutionary Code Gen-
eration. The most important evolutionary technique addressing the problem
of code generation is Genetic Programming (GP) [42]. Several linear vari-
ants of the GP technique namely Gene Expression Programming (GEP) [23],
Grammatical Evolution (GE) [82], Linear Genetic Programming (LGP) [9]
and Cartesian Genetic Programming (CGP) [55], are brieﬂy described.

Chapter 3 describes Multi Expression Programming (MEP), an original
evolutionary paradigm intended for solving computationally diﬃcult prob-
lems. MEP individuals are linear entities that encode complex computer
programs. MEP chromosomes are represented in the way C or Pascal compil-
ers translate mathematical expressions into machine code. A unique feature
of MEP is its ability of storing multiple solutions in a chromosome. This
ability proved to be crucial for improving the search process. The chapter is
entirely original and is based on the paper [64].

In chapters 4, 5 MEP technique is used for solving certain diﬃcult prob-
lems such as symbolic regression, classiﬁcation, multiplexer, designing digital
circuits. MEP is compared to Genetic Programming, Gene Expression Pro-

15

gramming, Linear Genetic Programming and Cartesian Genetic Program-
ming by using several well-known test problems. This chapter is entirely
original and it is based on the papers [64, 66, 71, 79, 80].

Chapter 6 describes the way in which MEP may be used for evolving more
complex computer programs such as heuristics for the Traveling Salesman
Problem and winning strategies for the Nim-like games and Tic-Tac-Toe.
This chapter is entirely original and it is based on the papers [70, 76].

Chapter 7 describes a new evolutionary technique called Inﬁx Form Ge-
netic Programming (IFGP). IFGP individuals encodes mathematical expres-
sions in inﬁx form. Each IFGP individual is an array of integer numbers
that are translated into mathematical expressions. IFGP is used for solving
several well-known symbolic regression and classiﬁcation problems. IFGP is
compared to standard Genetic Programming, Linear Genetic Programming
and Neural Networks approaches. This chapter is entirely original and it is
based on the paper [67].

Chapter 8 describes Multi Solution Linear Genetic Programming (MSLGP),

an improved technique based on Linear Genetic Programming. Each MSLGP
individual stores multiple solutions of the problem being solved in a single
chromosome. MSLGP is used for solving symbolic regression problems. Nu-
merical experiments show that Multi Solution Linear Genetic Programming
performs signiﬁcantly better than the standard Linear Genetic Programming
for all the considered test problems. This chapter is entirely original and it
is based on the paper [74, 78].

Chapter 9 describes two evolutionary models used for evolving evolution-
ary algorithms. The models are based on Multi Expression Programming and
Linear Genetic Programming. The output of the programs implementing the
proposed models are some full-featured evolutionary algorithms able to solve
a given problem. Several evolutionary algorithms for function optimization,
Traveling Salesman Problem and for the Quadratic Assignment Problem are
evolved using the proposed models. This chapter is entirely original and it
is based on the papers [63, 65].

Chapter 10 describes an attempt to provide a practical evidence of the
No Free Lunch (NFL) theorems. NFL theorems state that all black box al-
gorithms perform equally well over the space of all optimization problems,
when averaged over all the optimization problems. An important question
related to NFL is ﬁnding problems for which a given algorithm A is better
than another speciﬁed algorithm B. The problem of ﬁnding mathematical
functions for which an evolutionary algorithm is better than another evo-

16

lutionary algorithm is addressed in this chapter. This chapter is entirely
original and it is based on the papers [68, 69].

1.3 Other ML results not included in this

Thesis

During the PhD program I participated to the development of other evolu-
tionary paradigms and algorithms:

Traceless Genetic Programming (TGP) [73, 77] - a Genetic Programming
variant that does not store the evolved computer programs. Only the outputs
of the program are stored and recombined using speciﬁc operators.

Evolutionary Cutting Algorithm (ECA) [72] - an algorithm for the 2-

dimensional cutting stock problem.

DNA Theorem Proving [96] - a technique for proving theorems using DNA

computers.

Genetic Programming Theorem Prover [20] - an algorithm for proving

theorems using Genetic Programming.

Adaptive Representation Evolutionary Algorithm (AREA) [35] - an evolu-
tionary model that allow dynamic representation, i.e. encodes solutions over
several alphabets. The encoding alphabet is adaptive and it may be changes
during the search process in order to accomodate to the ﬁtness landscape.

Evolving Continuous Pareto Regions [21] - a technique for detecting con-

tinuous Pareto regions (when they exist).

17

Chapter 2

Genetic Programming and
related techniques

This chapter describes Genetic Programming (GP) and several related tech-
niques. The chapter is organized as follows: Genetic Programming is de-
scribed in section 2.1. Cartesian Genetic Programming is presented in sec-
tion 2.2. Gene Expression Programming is described in section 2.3. Linear
Genetic Programming is described in section 2.4. Grammatical Evolution is
described in section 2.5.

2.1 Genetic Programming

Genetic Programming (GP) [42, 43, 44] is an evolutionary technique used
for breeding a population of computer programs. Whereas the evolution-
ary schemes employed by GP are similar to those used by other techniques
(such as Genetic Algorithms [39], Evolutionary Programming [102], Evo-
lution Strategies [88]), the individual representation and the corresponding
genetic operators are speciﬁc only to GP. Due to its nonlinear individual rep-
resentation (GP individuals are usually represented as trees) GP is widely
known as a technique that creates computer programs.

Each GP individual is usually represented as a tree encoding a complex
computer program. The genetic operators used with GP are crossover and
mutation. The crossover operator takes two parents and generates two oﬀ-
spring by exchanging sub-trees between the parents. The mutation operator
generates an oﬀspring by changing a sub-tree of a parent into another sub-

18

tree.

For eﬃciency reasons, each GP program tree is stored as a vector using
the Polish form (see [44] chapter 63). A mathematical expression in Inﬁx
and Polish notations and the corresponding GP program tree are depicted
in Figure 2.1.

Figure 2.1: A mathematical expression in inﬁx form (a), Polish form (c) and
the corresponding program tree (b).

Each element in this vector contains a function or a terminal symbol.
Since each function has a unique arity we can clearly interpret each vector
that stores an expression in Polish notation.
In this notation, a sub-tree
of a program tree corresponds to a particular contiguous subsequence of the
vector. When applying the crossover or the mutation operator, the exchanged
or changed subsequences can easily be identiﬁed and manipulated.

2.2 Cartesian Genetic Programming

Cartesian Genetic Programming (CGP) [55] is a GP technique that encodes
chromosomes in graph structures rather than standard GP trees. The mo-
tivation for this representation is that the graphs are more general than the
tree structures, thus allowing the construction of more complex computer
programs [55].

19

CGP is Cartesian in the sense that the graph nodes are represented in
a Cartesian coordinate system. This representation was chosen due to the
node connection mechanism, which is similar to GP mechanism. A CGP
node contains a function symbol and pointers towards nodes representing
function parameters. Each CGP node has an output that may be used as
input for another node.

An example of CGP program is depicted in Figure 2.2.

Figure 2.2: A CGP program with 5 inputs, 2 outputs and 3 functions (0, 1,
2 inside square nodes). The grey squares represent unconnected nodes.

Each CGP program (graph) is deﬁned by several parameters: number of
rows (nr), number of columns (nc), number of inputs, number of outputs,
and number of functions. The nodes interconnectivity is deﬁned as being
the number (l) of previous columns of cells that may have their outputs
connected to a node in the current column (the primary inputs are treated
as node outputs).

CGP chromosomes are encoded as strings by reading the graph columns
top down and printing the input nodes and the function symbol for each
node. The CGP chromosome depicted in Figure 2.2 is encoded as:

C = 0 1 3 0 1 0 4 1 3 5 2 2 8 5 0 1 6 8 3 2 4 2 6 0 6 10 7 2 9 11 10

20

1 8 8 6 2 11 13

Standard string genetic operators (crossover and mutation) are used within
CGP system. Crossover may be applied without any restrictions. Mutation
operator requires that some conditions are met. Nodes supplying the outputs
are not ﬁxed as they are also subject to crossover and mutation.

2.3 Gene Expression Programming

Gene Expression Programming (GEP) [23] uses linear chromosomes. A chro-
mosome is composed of genes containing terminal and function symbols.
Chromosomes are modiﬁed by mutation, transposition, root transposition,
gene transposition, gene recombination, one-point and two-point recombina-
tion.

GEP genes are composed of a head and a tail. The head contains both

functions and terminals symbols. The tail contains only terminal symbols.

For each problem the head length (h) is chosen by the user. The tail

length (denoted t) is calculated using the formula:

t = (n − 1) ∗ h + 1,

where n is the number of arguments of the function with more arguments.

A tree-program is translated into a GEP gene is by means of breadth-ﬁrst

parsing.

Let us consider a gene made up of symbols in the set S:

S = {*, /, +, -, a, b}.

In this case we have n = 2. If we choose h = 10 we get t = 11, and the

length of the gene is 10 + 11 = 21. Such a gene is given below:

C = +*ab/+aab+ababbbababb.

The expression encoded by the gene C is:

E = (a + b)/a ∗ b + a.

21

The expression E represents the phenotypic transcription of a chromo-

some having C as its unique gene.

Usually, a GEP gene is not entirely used for phenotypic transcription.
If the ﬁrst symbol in the gene is a terminal symbol, the expression tree
consists of a single node. If all symbols in the head are function symbols, the
expression tree uses all the symbols of the gene.

GEP genes may be linked by a function symbol in order to obtain a fully
functional chromosome. In the current version of GEP, the linking functions
for algebraic expressions are addition and multiplication. A single type of
function is used for linking multiple genes.

This seems to be enough in some situation [23]. But, generally, it is not
a good idea to assume that the genes may be linked either by addition or
by multiplication. If the functions {+, -, *, /} are used as linking operators
then, the complexity of the problem grows substantially (since the problem
of determining how to mixed these operators with the genes is as hard as the
initial problem).

When solving computationally diﬃcult problems (like automated code
generation) one should not assume that a unique kind of function symbol
(like for, while or if instructions) is necessary for inter-connecting diﬀerent
program parts.

Moreover, the success rate of GEP increases with the number of genes
in the chromosome [23]. However, after a certain value, the success rate
decreases if the number of genes in the chromosome increases. This is because
one can not force a complex chromosome to encode a less complex expression.
Thus, when using GEP one must be careful with the number of genes
that form the chromosome. The number of genes in the chromosome must
be somehow related to the complexity of the expression that one wants to
discover.

According to [23], GEP performs better than standard GP for several

particular problems.

2.4 Linear Genetic Programming

Linear Genetic Programming (LGP) [9, 10, 11] uses a speciﬁc linear repre-
sentation of computer programs. Instead of the tree-based GP expressions of
a functional programming language (like LISP), programs of an imperative
language (like C ) are evolved.

22

A LGP individual is represented by a variable-length sequence of simple C
language instructions. Instructions operate on one or two indexed variables
(registers) r or on constants c from predeﬁned sets. The result is assigned to
a destination register, e.g. ri = rj * c.

An example of the LGP program is the following one:
void LGP(double v[8])
{
v[0] = v[5] + 73;
v[7] = v[3] - 59;
if (v[1] > 0)
if (v[5] > 21)
v[4] = v[2] * v[1];
v[2] = v[5] + v[4];
v[6] = v[7] * 25;
v[6] = v[4] - 4;
v[1] = sin(v[6]);
if (v[0] > v[1])
v[3] = v[5] * v[5];
v[7] = v[6] * 2;
v[5] = [7] + 115;
if (v[1] <= v[6])
v[1] = sin(v[7]);
}
A linear genetic program can be turned into a functional representation
by successive replacements of variables starting with the last eﬀective in-
struction. The variation operators used here are crossover and mutation. By
crossover, continuous sequences of instructions are selected and exchanged
between parents. Two types of mutations are used: micro mutation and
macro mutation. By micro mutation an operand or an operator of an instruc-
tion is changed. Macro mutation inserts or deletes a random instruction.

2.5 Grammatical Evolution

Grammatical Evolution (GE) [82] uses Backus - Naur Form (BNF) in order
to express computer programs. BNF is a notation that allows a computer
program to be expressed as a grammar.

23

A BNF grammar consists of terminal and non-terminal symbols. Gram-
mar symbols may be re-written in other terminal and non-terminal symbols.
Each GE individual is a variable length binary string that contains the
necessary information for selecting a production rule from a BNF grammar
in its codons (groups of 8 bits).

An example from a BNF grammar is given by the following production

rules:

S ::= expr (0)
|if -stmt (1)
|loop (2)

These production rules state that the start symbol S can be replaced

(re-written) either by one of the non-terminals expr, if -stmt, or by loop.

The grammar is used in a generative process to construct a program by
applying production rules, selected by the genome, beginning with the start
symbol of the grammar.

In order to select a GE production rule, the next codon value on the

genome is generated and placed in the following formula:

Rule = Codon Value MOD Num Rules.

If the next Codon integer value is 4, knowing that we have 3 rules to

select from, as in the example above, we get 4 MOD 3 = 1.

Therefore, S will be replaced with the non-terminal if-stmt, corresponding

to the second production rule.

Beginning from the left side of the genome codon, integer values are
generated and used for selecting rules from the BNF grammar, until one of
the following situations arises:

(i) A complete program is generated. This occurs when all the non-
terminals in the expression being mapped, are turned into elements
from the terminal set of the BNF grammar.

(i) The end of the genome is reached, in which case the wrapping operator
is invoked. This results in the return of the genome reading frame
to the left side of the genome once again. The reading of the codons
will then continue unless a higher threshold representing the maximum

24

number of wrapping events has occurred during this individual mapping
process.

In the case that a threshold on the number of wrapping events is ex-
ceeded and the individual is still incompletely mapped, the mapping process
is halted, and the individual is assigned the lowest possible ﬁtness value.

Example

Consider the grammar:

G = {N , T , S, P },

where the terminal set is:

T = {+, -, *, /, sin, exp, var, (, )},

and the nonterminal symbols are:

N = {expr, op, pre op}.

The start symbol is S = <expr >.

The production rules P are:

<expr > :: <expr > <op> <expr > | (0)
(<expr > <op> <expr >) | (1)
<pre op> (<expr >) | (2)
<var >. (3)
<op> ::= + | (0)
- | (1)
* | (2)
/. (3)
<pre op> ::= sin | (0)
exp. (1)

25

An example of a GE chromosome is the following:

CGE = 000000000000001000000001000000110000001000000011.

Translated into GE codons, the chromosome is:

CGE = 0, 2, 1, 3, 2, 3.

This chromosome is translated into the expression:

E = exp(x) * x.

Using the BNF grammars for specifying a chromosome provides a natural
way of evolving programs written in programming languages whose instruc-
tions may be expressed as BNF rules.

The wrapping operator provides a very original way of translating short
chromosomes into very long expressions. Wrapping also provides an eﬃcient
way to avoid the obtaining of invalid expressions.

The GE mapping process also has some disadvantages. Wrapping may
never end in some situations. For instance consider the GGE grammar de-
ﬁned above. In these conditions the chromosome

C (cid:48)

GE = 0, 0, 0, 0, 0

cannot be translated into a valid expression as it does not contain operands.
To prevent inﬁnite cycling a ﬁxed number of wrapping occurrences is allowed.
If this threshold is exceeded the obtained expression is incorrect and the
corresponding individual is considered to be invalid.

Since the debate regarding the supremacy of binary encoding over integer
encoding has not ﬁnished yet we cannot say which one is better. However,
as the translation from binary representations to integer/real representations
takes some time we suspect that the GE system is a little slower than other
GP techniques that use integer representation. GE uses a steady-state [95]
algorithm.

26

Chapter 3

Multi Expression Programming

The chapter is organized as follows: MEP algorithm is given in section 3.2.
Individual representation is described in section 3.3. The way in which MEP
individuals are translated in computer programs is presented in section 3.4.
The search operators used in conjunction with MEP are given in section
3.6. The way in which MEP handles exceptions raised during the ﬁtness
assignment process is presented in section 3.7. MEP complexity is computed
in section 3.8.

The chapter is entirely original and it is based on the papers [64, 66, 65,

71, 79, 78].

3.1 MEP basic ideas

Multi Expression Programming (MEP) [64] is a GP variant that uses a lin-
ear representation of chromosomes. MEP individuals are strings of genes
encoding complex computer programs.

When MEP individuals encode expressions, their representation is sim-
ilar to the way in which compilers translate C or Pascal expressions into
machine code [2]. This may lead to very eﬃcient implementation into as-
sembler languages. The ability of evolving machine code (leading to very
important speedups) has been considered by others researchers, too. For in-
stance Nordin [62] evolves programs represented in machine code. Poli and
Langdon [85] proposed Sub-machine code GP, which exploits the processor
ability to perform some operations simultaneously. Compared to these ap-
proaches, MEP has the advantage that it uses a representation that is more

27

compact, simpler, and independent of any programming language.

A salient MEP feature is the ability of storing multiple solutions of a
problem in a single chromosome. Usually, the best solution is chosen for
ﬁtness assignment. When solving symbolic regression or classiﬁcation prob-
lems (or any other problems for which the training set is known before the
problem is solved) MEP has the same complexity as other techniques storing
a single solution in a chromosome (such as GP, CGP, GEP or GE).

Evaluation of the expressions encoded into a MEP individual can be per-

formed by a single parsing of the chromosome.

Oﬀspring obtained by crossover and mutation are always syntactically
correct MEP individuals (computer programs). Thus, no extra processing
for repairing newly obtained individuals is needed.

3.2 MEP algorithm

Standard MEP algorithm uses steady-state evolutionary model [95] as its
underlying mechanism.

The MEP algorithm starts by creating a random population of individu-
als. The following stpng are repeated until a given number of generations is
reached: Two parents are selected using a standard selection procedure. The
parents are recombined in order to obtain two oﬀspring. The oﬀspring are
considered for mutation. The best oﬀspring O replaces the worst individual
W in the current population if O is better than W .

The variation operators ensure that the chromosome length is a constant
of the search process. The algorithm returns as its answer the best expression
evolved along a ﬁxed number of generations.

The standard MEP algorithm is outlined below:

Standard MEP Algorithm

S1. Randomly create the initial population P(0)
S2. for t = 1 to Max Generations do
S3. for k = 1 to |P(t)| / 2 do
S4. p1 = Select(P(t)); // select one individual from the current
// population
S5. p2 = Select(P(t)); // select the second individual
S6. Crossover (p1, p2, o1, o2); // crossover the parents p1 and p2

28

// the oﬀspring o1 and o2 are obtained
S7. Mutation (o1); // mutate the oﬀspring o1
S8. Mutation (o2); // mutate the oﬀspring o2
S9. if Fitness(o1) < Fitness(o2)
S10. then if Fitness(o1) < the ﬁtness of the worst individual
in the current population
S11. then Replace the worst individual with o1;
S12. else if Fitness(o2) < the ﬁtness of the worst individual
in the current population
S13. then Replace the worst individual with o2;
S14. endfor
S15. endfor

3.3 MEP representation

MEP genes are (represented by) substrings of a variable length. The number
of genes per chromosome is constant. This number deﬁnes the length of the
chromosome. Each gene encodes a terminal or a function symbol. A gene
that encodes a function includes pointers towards the function arguments.
Function arguments always have indices of lower values than the position of
the function itself in the chromosome.

The proposed representation ensures that no cycle arises while the chro-
mosome is decoded (phenotypically transcripted). According to the proposed
representation scheme, the ﬁrst symbol of the chromosome must be a terminal
symbol. In this way, only syntactically correct programs (MEP individuals)
are obtained.

Example

Consider a representation where the numbers on the left positions stand
for gene labels. Labels do not belong to the chromosome, as they are provided
only for explanation purposes.

For this example we use the set of functions:

F = {+, *},

29

and the set of terminals

T = {a, b, c, d}.

An example of chromosome using the sets F and T is given below:

1: a
2: b
3: + 1, 2
4: c
5: d
6: + 4, 5
7: * 3, 6

The maximum number of symbols in MEP chromosome is given by the

formula:

Number of Symbols = (n+1) * (Number of Genes – 1) + 1,

where n is the number of arguments of the function with the greatest number
of arguments.

The maximum number of eﬀective symbols is achieved when each gene
(excepting the ﬁrst one) encodes a function symbol with the highest number
of arguments. The minimum number of eﬀective symbols is equal to the
number of genes and it is achieved when all genes encode terminal symbols
only.

3.4 MEP phenotypic transcription. Fitness

assignment

Now we are ready to describe how MEP individuals are translated into com-
puter programs. This translation represents the phenotypic transcription of
the MEP chromosomes.

Phenotypic translation is obtained by parsing the chromosome top-down.
A terminal symbol speciﬁes a simple expression. A function symbol speciﬁes
a complex expression obtained by connecting the operands speciﬁed by the

30

argument positions with the current function symbol.

For instance, genes 1, 2, 4 and 5 in the previous example encode simple

expressions formed by a single terminal symbol. These expressions are:

E1 = a,
E2 = b,
E4 = c,
E5 = d,

Gene 3 indicates the operation + on the operands located at positions 1

and 2 of the chromosome. Therefore gene 3 encodes the expression:

E3 = a + b.

Gene 6 indicates the operation + on the operands located at positions 4

and 5. Therefore gene 6 encodes the expression:

E6 = c + d.

Gene 7 indicates the operation * on the operands located at position 3

and 6. Therefore gene 7 encodes the expression:

E7 = (a + b) ∗ (c + d).

E7 is the expression encoded by the whole chromosome.
There is neither practical nor theoretical evidence that one of these ex-
pressions is better than the others. Moreover, Wolpert and McReady [100,
101] proved that we cannot use the search algorithm’s behavior so far for a
particular test function to predict its future behavior on that function. This
is why each MEP chromosome is allowed to encode a number of expressions
equal to the chromosome length (number of genes). The chromosome de-
scribed above encodes the following expressions:

E1 = a,
E2 = b,
E3 = a + b,
E4 = c,
E5 = d,

31

E6 = c + d,
E7 = (a + b) * (c + d).

The value of these expressions may be computed by reading the chromo-
some top down. Partial results are computed by dynamic programming [7]
and are stored in a conventional manner.

Due to its multi expression representation, each MEP chromosome may
be viewed as a forest of trees rather than as a single tree, which is the case
of Genetic Programming.

As MEP chromosome encodes more than one problem solution, it is in-

teresting to see how the ﬁtness is assigned.

The chromosome ﬁtness is usually deﬁned as the ﬁtness of the best ex-

pression encoded by that chromosome.

For instance, if we want to solve symbolic regression problems, the ﬁtness

of each sub-expression Ei may be computed using the formula:

f (Ei) =

n
(cid:88)

k=1

|ok,i − wk|,

(3.1)

where ok,i is the result obtained by the expression Ei for the ﬁtness case
k and wk is the targeted result for the ﬁtness case k. In this case the ﬁtness
needs to be minimized.

The ﬁtness of an individual is set to be equal to the lowest ﬁtness of the

expressions encoded in the chromosome:

f (C) = min

f (Ei).

(3.2)

i
When we have to deal with other problems, we compute the ﬁtness of
each sub-expression encoded in the MEP chromosome. Thus, the ﬁtness of
the entire individual is supplied by the ﬁtness of the best expression encoded
in that chromosome.

3.5 MEP representation revisited

Generally a GP chromosome encodes a single expression (computer program).
This is also the case for GEP and GE chromosomes. By contrast, a MEP
chromosome encodes several expressions (as it allows a multi-expression rep-
resentation). Each of the encoded expressions may be chosen to represent

32

the chromosome, i.e. to provide the phenotypic transcription of the chromo-
some. Usually, the best expression that the chromosome encodes supplies its
phenotypic transcription (represents the chromosome).

Therefore, the MEP technique is based on a special kind of implicit paral-
lelism. A chromosome usually encodes several well-deﬁned expressions. The
ability of MEP chromosome to encode several syntactically correct expres-
sions in a chromosome is called strong implicit parallelism (SIP).

Although, the ability of storing multiple solutions in a single chromosome
has been suggested by others authors, too (see for instance [49]), and several
attempts have been made for implementing this ability in GP technique. For
instance Handley [37] stored the entire population of GP trees in a single
graph. In this way a lot of memory is saved. Also, if partial solutions are
eﬃciently stored, we can get a considerable speed up.

Linear GP [9] is also very suitable for storing multiple solutions in a
single chromosome. In that case the multi expression ability is given by the
possibility of choosing any variable as the program output.

It can be seen that the eﬀective length of the expression may increases
exponentially with the length of the chromosome. This is happening because
some sub-expressions may be used more than once to build a more complex
(or a bigger) expression. Consider, for instance, that we want to obtain a
chromosome that encodes the expressiona2n, and only the operators {+, -,
*, /} are allowed. If we use a GEP representation the chromosome has to
contain at least (2n+1– 1) symbols since we need to store 2n terminal sym-
bols and (2n – 1) function operators. A GEP chromosome that encodes the
expression E = a8 is given below:

C = *******aaaaaaaa.

A MEP chromosome uses only (3n + 1) symbols for encoding the expres-
sion a2n. A MEP chromosome that encodes expression E = a8 is given below:

1: a
2: * 1, 1
3: * 2, 2
4: * 3, 3

As a further comparison, when n = 20, a GEP chromosome has to have

2097151 symbols, while MEP needs only 61 symbols.

33

MEP representation is similar to GP and CGP, in the sense that each
function symbol provides pointers towards its parameters. Whereas both
GP and CGP have complicated representations (trees and graphs), MEP
provides an easy and eﬀective way to connect (sub) parts of a computer
program. Moreover, the motivation for MEP was to provide an individual
representation close to the way in which C or Pascal compilers interpret
mathematical expressions [2]. That code is also called three addresses code
or intermediary code.

Some GP techniques, like Linear GP, remove non-coding sequences of
chromosome during the search process. As already noted [9] this strategy
does not give the best results. The reason is that sometimes, a part of
the useless genetic material has to be kept in the chromosome in order to
maintain population diversity.

3.6 Search operators

The search operators used within MEP algorithm are crossover and mutation.
These search operators preserve the chromosome structure. All oﬀspring are
syntactically correct expressions.

3.6.1 Crossover

By crossover two parents are selected and are recombined.

Three variants of recombination have been considered and tested within
our MEP implementation: one-point recombination, two-point recombina-
tion and uniform recombination.

One-point recombination

One-point recombination operator in MEP representation is similar to
the corresponding binary representation operator [19]. One crossover point
is randomly chosen and the parent chromosomes exchange the sequences at
the right side of the crossover point.

Example

34

Consider the parents C1 and C2 given below. Choosing the crossover point
after position 3 two oﬀspring, O1 and O2 are obtained as given in Table 3.6.1.

Table 3.1: MEP one-point recombination.

Parents
C1
1: b
2: * 1, 1
3: + 2, 1
4: a
5: * 3, 2
6: a
7: - 1, 4

C2
1: a
2: b
3: + 1, 2
4: c
5: d
6: + 4, 5
7: * 3, 6

Oﬀspring
O1
1: b
2: * 1, 1
3: + 2, 1
4: c
5: d
6: + 4, 5
7: * 3, 6

O2
1: a
2: b
3: + 1, 2
4: a
5: * 3, 2
6: a
7: - 1, 4

Two-point recombination

Two crossover points are randomly chosen and the chromosomes exchange

genetic material between the crossover points.

Example

Let us consider the parents C1 and C2 given below. Suppose that the
crossover points were chosen after positions 2 and 5. In this case the oﬀspring
O1 and O2 are obtained as given in Table 3.6.1.

Uniform recombination

During the process of uniform recombination, oﬀspring genes are taken

randomly from one parent or another.

Example

Let us consider the two parents C1 and C2 given below. The two oﬀspring

O1 and O2 are obtained by uniform recombination as given in Table 3.3.

It is easy to derive - by analogy with standard GA several recombination

operators.

35

Table 3.2: MEP two-point recombination.

Parents
C1
1: b
2: * 1, 1
3: + 2, 1
4: a
5: * 3, 2
6: a
7: - 1, 4

C2
1: a
2: b
3: + 1, 2
4: c
5: d
6: + 4, 5
7: * 3, 6

Oﬀspring
O1
1: b
2: * 1, 1
3: + 1, 2
4: c
5: d
6: a
7: - 1, 4

O2
1: a
2: b
3: + 2, 1
4: a
5: * 3, 2
6: + 4, 5
7: * 3, 6

Table 3.3: MEP uniform recombination.

Parents
C1
1: b
2: * 1, 1
3: + 2, 1
4: a
5: * 3, 2
6: a
7: - 1, 4

C2
1: a
2: b
3: + 1, 2
4: c
5: d
6: + 4, 5
7: * 3, 6

Oﬀspring
O1
1: a
2: * 1, 1
3: + 2, 1
4: c
5: * 3, 2
6: + 4, 5
7: - 1, 4

O2
1: b
2: b
3: + 1, 2
4: a
5: d
6: a
7: * 3, 6

36

3.6.2 Mutation

Each symbol (terminal, function of function pointer) in the chromosome may
be the target of the mutation operator. Some symbols in the chromosome
are changed by mutation. To preserve the consistency of the chromosome,
its ﬁrst gene must encode a terminal symbol.

We may say that the crossover operator occurs between genes and the

mutation operator occurs inside genes.

If the current gene encodes a terminal symbol, it may be changed into
another terminal symbol or into a function symbol. In the later case, the
positions indicating the function arguments are randomly generated. If the
current gene encodes a function, the gene may be mutated into a terminal
symbol or into another function (function symbol and pointers towards ar-
guments).

Example

Consider the chromosome C given below. If the boldfaced symbols are

selected for mutation an oﬀspring O is obtained as given in Table 3.4.

Table 3.4: MEP mutation.

C
1: a
2: * 1, 1
3: b
4: * 2, 2
5: b
6: + 3, 5
7: a

O
1: a
2: * 1, 1
3: + 1, 2
4: * 2, 2
5: b
6: + 1, 5
7: a

3.7 Handling exceptions within MEP

Exceptions are special situations that interrupt the normal ﬂow of expression
evaluation (program execution). An example of exception is division by zero,
which is raised when the divisor is equal to zero.

37

Exception handling is a mechanism that performs special processing when

an exception is thrown.

Usually, GP techniques use a protected exception handling mechanism
[42]. For instance, if a division by zero exception is encountered, a predeﬁned
value (for instance 1 or the numerator) is returned.

GEP uses a diﬀerent mechanism: if an individual contains an expression
that generates an error, this individual receives the lowest ﬁtness possible
[23].

MEP uses a new and speciﬁc mechanism for handling exceptions. When
an exception is encountered (which is always generated by a gene containing
a function symbol), the gene that generated the exception is mutated into a
terminal symbol. Thus, no infertile individual appears in a population.

3.8 MEP complexity

Let NG be the number of genes in a chromosome.

When solving symbolic regression, classiﬁcation or any other problems for
which the training set is known in advance (before computing the ﬁtness),
the ﬁtness of an individual can be computed in O(NG) stpng by dynamic
programming [7].
In fact, a MEP chromosome needs to be read once for
computing the ﬁtness.

Thus, MEP decoding process does not have a higher complexity than
other GP - techniques that encode a single computer program in each chro-
mosome.

3.9 Conclusions

Multi Expression Programming has been described in this chapter. A de-
tailed description of the representation and of the ﬁtness assignment has
been given.

A distinct feature of MEP is its ability to encode multiple solutions in
the same chromosome. It has been shown that the complexity of decoding
process is the same as in the case of other GP techniques encoding a single
solution in the same chromosome.

38

Chapter 4

MEP for Data Mining

4.1

Introduction

Multi Expression Programming is used for solving several symbolic regression
and classiﬁcation problems. A comparison of MEP with standard GP, GEP
and CGP is also provided.

The chapter is organized as follows: Symbolic regression problems are
addressed in section 4.2. For this problem MEP is compared to GP, GEP and
CGP. Even Parity problems are addressed in section 5.1. The Multiplexer is
addressed in section 5.2. The way in which MEP may be used for designing
digital circuits is described in sections 5.3 and 5.4. In sections 6.1 and 6.2
MEP is used to evolve winning strategies for the Tic-Tac-Toe and Nim-like
games. Another interesting application of MEP is the discovery of heuristics
for NP-Complete problems.
In section 6.3 MEP is used for evolving an
heuristic for the Traveling Salesman Problem.

The chapter is based on the author’s papers [64].

4.2 Symbolic regression

In this section, MEP technique is used for solving symbolic regression prob-
lems. Results are reported in the papers [64].

39

4.2.1 Problem statement

The aim of symbolic regression is to discover a function that satisﬁes a set
of ﬁtness cases.

Two well-known problems are used for testing the MEP ability of solving

symbolic regression problems. The problems are:

The quartic polynomial [42]. Find a mathematical expression that satis-

ﬁes best a set of ﬁtness cases generated by the function:

f (x) = x4 + x3 + x2 + x.

The sextic polynomial [43]. Find a mathematical expression that satisﬁes

best a set of ﬁtness cases generated by the function:

f (x) = x6 − 2x4 + x2.

A set of 20 ﬁtness cases was randomly generated over the interval [-1.0,

1.0] and used in the experiments performed.

4.2.2 Numerical experiments

In this section several numerical experiments with Multi Expression Pro-
gramming for solving symbolic regression problems are performed.

Experiment 1

The success rate of the MEP algorithm is analyzed in this experiment.
Success rate is computed as the number of successful runs over the total num-
ber of runs. The chromosome length is gradually increased. MEP algorithm
parameters are given in Table 4.1.

The success rate of the MEP algorithm depending on the number of

symbols in the chromosome is depicted in Figure 4.1.

The success rate of the MEP algorithm increases with the chromosome
length and never decreases towards very low values. When the search space
(chromosome length) increases, an increased number of expressions are en-
coded by MEP chromosomes. Very large search spaces (very long chromo-
somes) are extremely beneﬁcial for MEP technique due to its multi expression
representation. This behavior is diﬀerent from those obtained with the GP

40

Table 4.1: Algorithm parameters for the Experiment 1.

Parameter
Population size
Number of generations
Mutation
Crossover type
Crossover probability
Selection
Terminal Set
Function Set

Value
30
50
2 symbols / chromosome
Uniform-crossover
0.9
Binary tournament
T = {x}
F = {+, -, *, /}

Figure 4.1: The relationship between the success rate and the number of
symbols in a MEP chromosome. The number of symbols in chromosome
varies between 5 and 100. The results are summed over 100 runs.

41

variants that encode a single solution in a chromosome (such as GEP). Fig-
ure 4.1 also shows that the sextic polynomial is more diﬃcult to solve with
MEP (with the parameters given in Table 4.1) than the quatic polynomial.

Experiment 2

From Experiment 1 we may infer that for the considered problem, the
MEP success rate never decreases to very low values as the number of genes
increases. To obtain an experimental evidence for this assertion longer chro-
mosomes are considered. We extend chromosome length up to 300 genes (898
symbols).

The success rate of MEP is depicted in Figure 4.2.

Figure 4.2: The relationship between the success rate and the number of
symbols in a MEP chromosome. The number of symbols in chromosome
varies between 5 and 300. The results are summed over 100 runs.

Figure 4.2 shows that, when solving the quartic (sextic) polynomial prob-
lem, the MEP success rate, lies in the interval [90, 100] ([60, 80]) for the
chromosome length larger than 20.

One may note that after that the chromosome length becomes 10, the
success rate never decrease more than 90% (for the quartic polynomial) and
It also seems
never decrease more than 60% (for the sextic polynomial).

42

that, after a certain value of the chromosome length, the success rate does
not improve signiﬁcantly.

Experiment 3

In this experiment the relationship between the success rate and the pop-
ulation size is analyzed. Algorithm parameters for this experiment are given
in Table 4.2.

Table 4.2: Algorithm parameters for Experiment 3.

Parameter
Number of generations
Chromosome length
Mutation
Crossover type
Crossover probability
Selection
Terminal Set
Function Set

Value
50
10 genes
2 symbols / chromosome
Uniform-crossover
0.9
Binary tournament
T = {x}
F = {+, -, *, /}

Experiment results are given in Figure 4.3.
For the quartic problem and for the MEP algorithm parameters given in
Table 4.2, the optimal population size is 70 (see Figure 4.3). The correspond-
ing success rate is 99%. A population of 100 individuals yields a success rate
of 88% for the sextic polynomial. This result suggests that even small MEP
populations may supply very good results.

Experiment 4

In this experiment the relationship between the MEP success rate and

the number of generations used in the search process is analyzed.

MEP algorithm parameters are given in Table 4.3.
Experiment results are given in Figure 4.4.
Figure 4.4 shows that the success rate of the MEP algorithm rapidly
increases from 34%, respectively 8% (when the number of generations is 10)
up to 95%, and 74% respectively (when the number of generations is 300).

43

Figure 4.3: Success rate of the MEP algorithm. Population size varies be-
tween 5 and 100. The results are summed over 100 runs.

Table 4.3: Algorithm parameters for Experiment 4.

Parameter
Population size
Chromosome length
Mutation
Crossover type
Crossover probability
Selection
Terminal Set
Function Set

Value
20
12 genes
2 genes / chromosome
Uniform-crossover
0.9
Binary tournament
T = {x}
F = {+, -, *, /}

44

Figure 4.4: Relationship between the success rate of MEP algorithm and the
number of generations used in the search process. The number of generations
varies between 10 and 300. The results are summed over 100 runs.

4.2.3 MEP vs. GEP

In [23] GEP has been used for solving the quartic polynomial based on a
set of 10 ﬁtness cases randomly generated over the interval [0, 20]. Several
numerical experiments analyzing the relationship between the success rate
and the main parameters of the GEP algorithm have been performed in [23].
In what follows we will perform similar experiments with MEP.

The ﬁrst experiment performed in [23] analyses the relationship between
the GEP chromosome length and the success rate. GEP success rate increases
up to 80% (obtained when the GEP chromosome length is 30) and then
decreases. This indicates that very long GEP chromosomes cannot encode
short expressions eﬃciently. The length of the GEP chromosome must be
somehow related to the length of the expression that must be discovered.

Using the same parameter setting (i.e. Population Size = 30, Number
of Generations = 50; Crossover probability = 0.7, Mutations = 2 / chromo-
some), the MEP obtained a success rate of 98% when the chromosome length
was set to 20 genes (58 symbols).

The second experiment performed in [23] analyses the relationship be-

45

tween the population size used by the GEP algorithm and the success rate.
For a population of size 30, the GEP success rate reaches 80%, and for a
population of 50 individuals the GEP success rate reaches 90%.

Using the same parameter setting (i.e. Number of Symbols in Chromo-
some = 49 (17 MEP genes), Number of Generations = 50; Crossover prob-
ability = 0.7, Mutations = 2 / chromosome), MEP obtained a success rate
of 99% (in 99 runs out of 100 MEP has found the correct solution) using a
population of only 30 individuals.

In another experiment performed in [23], the relationship between the
number of generations used by GEP and the rate of success is analysed. The
success rate of 69% was obtained by GEP when the number of generations
was 70 and a success rate of 90% was obtained only when the number of
generations reached 500. For the considered generation range GEP success
rate never reached 100%.

Using the same parameter setting (i.e. Number of Symbols in Chromo-
some = 80 (27 MEP genes), Population Size = 30; Crossover probability =
0.7, Mutations = 2 / chromosome), the MEP obtained a success rate of 97%
(in 97 runs out of 100 MEP has found the correct solution) using 30 gener-
ations only. This is an improvement (regarding the number of generations
used to obtain the same success rate) with more than one order of magnitude.
We may conclude that for the quartic polynomial, the MEP has a higher

success rate than GEP using the previously given parameter settings.

4.2.4 MEP vs. CGP

CGP has been used [55] for symbolic regression of the sextic polynomial
problem.

In this section, the MEP technique is used to solve the same problem using
parameters settings similar to those of CGP. To provide a fair comparison,
all experimental conditions described in [55] are carefully reproduced for the
MEP technique.

CGP chromosomes were characterized by the following parameters: nr
= 1, nc = 10, l = 10. MEP chromosomes are set to contain 12 genes (in
addition MEP uses two supplementary genes for the terminal symbols {1.0,
x}).

MEP parameters (similar to those used by CGP) are given in Table 4.4.
In the experiment with CGP a population of 10 individuals and a number
of 8000 generations have been used. We performed two experiments. In the

46

Table 4.4: Algorithm parameters for the MEP vs. CGP experiment.

Parameter
Chromosome length
Mutation
Crossover type
Crossover probability
Selection
Elitism size
Terminal set
Function set

Value
12 genes
2 genes / chromosome
One point crossover
0.7
Binary tournament
1
T = {x, 1.0}
F = {+, -, *, /}

ﬁrst experiment, the MEP population size is set to 10 individuals and we
compute the number of generations needed to obtain the success rate (61 %)
reported in [55] for CGP.

When the MEP run for 800 generations, the success rate was 60% (in
60 runs (out of 100) MEP found the correct solution). Thus MEP requires
10 times less generations than CGP to solve the same problem (the sextic
polynomial problem in our case). This represents an improvement of one
order of magnitude.

In the second experiment, the number of generations is kept unchanged
(8000) and a small MEP population is used. We are interested to see which
is the optimal population size required by MEP to solve this problem.

After several trials, we found that MEP has a success rate of 70% when
a population of 3 individuals is used and a success rate of 46% when a
population of 2 individuals is used. This means that MEP requires 3 times
less individuals than CGP for solving the sextic polynomial problem.

4.2.5 MEP vs. GP

In [42] GP was used for symbolic regression of the quartic polynomial func-
tion.

GP parameters are given in Table 4.5.
It is diﬃcult to compare MEP with GP since the experimental conditions
were not the same. The main diﬃculty is related to the number of symbols
in chromosome. While GP individuals may increase, MEP chromosomes

47

Table 4.5: GP parameters used for solving the quartic problem.

Parameter
Population Size
Number of generations
Crossover probability
Mutation probability
Maximum tree depth
Maximum initial
depth
Terminal set
Function set

tree

Value
500
51
0.9
0
17
6

T = {x}
F = {+, - , *, %, Sin, Cos, Exp, RLog}

have ﬁxed length. Individuals in the initial GP population are trees having
a maximum depth of 6 levels. The number of nodes in the largest tree
containing symbols from F ∪ T and having 6 levels is 26 – 1 = 63 nodes.
The number of nodes in the largest tree containing symbols from F ∪ T
and having 17 levels (maximum depth allowed for a GP tree) is 217 – 1 =
131071 nodes.

Due to this reason we cannot compare MEP and GP relying on the num-
ber of genes in a chromosome. Instead, we analyse diﬀerent values for the
MEP chromosome length.

MEP algorithm parameters are similar to those used by GP [42]. The

results are depicted in Figure 4.5.

For this problem, the GP cumulative probability of success is 35% (see
[42]). Figure 4.5 shows that the lowest success rate for MEP is 65%, while the
highest success rate is 100% (for the considered chromosome length domain).
Thus, MEP outperforms GP on the quartic polynomial problem (when the
parameters given in Table 4.1 are used).

4.3 Conclusions

In this chapter, MEP has been used for solving various symbolic regres-
sion problems. MEP has been compared with other Genetic Programming
techniques. Numerical results indicate that MEP performs better than the

48

Figure 4.5: The relationship between the number of genes in a chromosome
and the MEP success rate. The number of genes in a chromosome varies
between 10 and 100. The results are averaged over 100 runs.

compared methods.

49

Chapter 5

Designing Digital Circuits with
MEP

MEP is used for designing digital circuits based on the truth table. Four
problems are addressed: even-parity, multiplexer, arithmetic circuits and
circuits for NP-complete problems. This chapter is entirely original and it is
based on the papers [66, 79, 80].

5.1 Even-parity problem

5.1.1 Problem statement

The Boolean even-parity function of k Boolean arguments returns T (True)
if an even number of its arguments are T. Otherwise the even-parity function
returns NIL (False) [42].

In applying MEP to the even-parity function of k arguments, the terminal
set T consists of the k Boolean arguments d0, d1, d2, ... dk−1. The function
set F consists of four two-argument primitive Boolean functions: AND, OR,
NAND, NOR. According to [42] the Boolean even-parity functions appear to
be the most diﬃcult Boolean functions to be detected via a blind random
search.

The set of ﬁtness cases for this problem consists of the 2k combinations
of the k Boolean arguments. The ﬁtness of an MEP chromosome is the
sum, over these 2k ﬁtness cases, of the Hamming distance (error) between
the returned value by the MEP chromosome and the correct value of the

50

Boolean function. Since the standardized ﬁtness ranges between 0 and 2k, a
value closer to zero is better (since the ﬁtness is to be minimized).

5.1.2 Numerical experiments

The parameters for the numerical experiments with MEP for even-parity
problems are given in Table 5.1.

Table 5.1: The MEP algorithm parameters for the numerical experiments
with even-parity problems.

Parameter
Number of generations
Crossover type
Crossover probability
Mutation probability
Terminal set

Function set

Value
51
Uniform
0.9
0.2
T3 = {D0, D1, D2} for even-3-parity
T4 = {D0, D1, D2, D3} for even-4-parity
F = {AND, OR, NAND, NOR}

In order to reduce the length of the chromosome all the terminals are
kept on the ﬁrst positions of the MEP chromosomes. The selection pressure
is also increased by using higher values (usually 10% of the population size)
for the q-tournament size.

Several numerical experiments with MEP have been performed for solv-
ing the even-3-parity and the even-4-parity problems. After several trials
we have found that a population of 100 individuals having 300 genes was
enough to yield a success rate of 100% for the even-3-parity problem and a
population of 400 individuals with 200 genes yielded a success rate of 43%
for the even-4-parity problem. GP without Automatically Deﬁned Functions
has been used for solving the even-3 and even-4 parity problems using a pop-
ulation of 4000 individuals [42]. The cumulative probability of success was
100% for the even-3-parity problem and 42% for the even-4-parity problem
[42]. Thus, MEP outperforms GP for the even-3 and even-4 parity problems
with more than one order of magnitude. However, we already mentioned,
a perfect comparison between MEP and GP cannot be drawn due to the
incompatibility of the respective representations.

51

One of the evolved circuits for the even-3-parity problem is given in Figure
5.1 and one of the evolved circuits for the even-4-parity is given in Figure
5.2.

Figure 5.1: An evolved circuit for the even-3-parity problem.

5.2 Multiplexer problem

In this section, the MEP technique is used for solving the 6-multiplexer
and the 11-multiplexer problems [42]. Numerical experiments obtained by
applying MEP to multiplexer problem are reported in [64].

5.2.1 Problem statement

The input to the Boolean N -multiplexer function consists of k address bits
ai and 2k data bits di, where

N = k + 2k. That is, the input consists of the k+2k bits ak−1, ...

,
a1, a0, d2k−1, ...
, d1, d0. The value of the Boolean multiplexer function
is the Boolean value (0 or 1) of the particular data bit that is singled out
by the k address bits of the multiplexer. Another way to look at the search
space is that the Boolean multiplexer function with k+2k arguments is a
particular function of 2k+2k possible Boolean functions of k+2k arguments.
For example, when k=3, then k+2k = 11 and this search space is of size 2211.
That is, the search space is of size 22048, which is approximately 10616.

The terminal set for the 6-multiplexer problem consists of the 6 Boolean
inputs, and for the 11-multiplexer problem consists of the 11 Boolean inputs.

52

Figure 5.2: An evolved circuit for the even-4-parity problem.

Thus, the terminal set T for the 6-multiplexer is of T = {a0, a1, d0, d1, ... ,
d4} and for the 11-multiplexer is of T = {a0, a1, a2, d0, d1, ... , d7}.

The function set F for this problem is F = {AND, OR, NOT, IF} taking
2, 2, 1, and 3 arguments, respectively [42]. The function IF returns its 3rd
argument if its ﬁrst argument is set to 0. Otherwise it returns its second
argument.

There are 211 = 2,048 possible combinations of the 11 arguments a0a1a2d0d1d2d3d4d5d6d7

along with the associated correct value of the 11-multiplexer function. For
this particular problem, we use the entire set of 2048 combinations of argu-
ments as the ﬁtness cases for evaluating ﬁtness.

5.2.2 Numerical experiments

Several numerical experiments with the 6-multiplexer and 11-multiplexer are
performed in this section.

Experiments with 6-multiplexer

Two main statistics are of high interest: the relationship between the success
rate and the number of genes in a MEP chromosome and the relationship

53

between the success rate and the size of the population used by the MEP
algorithm. For these experiments the parameters are given in Table 5.2.

Table 5.2: MEP algorithm parameters for the numerical experiments with
6-multiplexer problem.

Parameter
Number of generations
Crossover type
Crossover probability
Mutation probability
Terminal set
Function set

Value
51
Uniform
0.9
0.1
T = {a0, a1, d0, d1, ... , d4}
F = {AND, OR, NOT, IF}

A population of 100 individuals is used when the inﬂuence of the number
of genes is analysed and a code length of 100 genes is used when the inﬂuence
of the population size is analysed. For reducing the chromosome length we
keep all the terminals on the ﬁrst positions of the MEP chromosomes. We
also increased the selection pressure by using larger values (usually 10% of
the population size) for the tournament sample.

The results of these experiments are given in Figure 5.3.
Figure 5.3 shows that MEP is able to solve the 6-multiplexer problem very
well. A population of 500 individuals yields a success rate of 84%. A similar
experiment using the GP technique with a population of 500 individuals has
been reported in [83]. The reported probability of success is a little less
(79,5%) than the one obtained with MEP (84%).

Experiments with 11-multiplexer

We also performed several experiments with the 11-multiplexer problem. We
have used a population of 500 individuals and three values (100, 200 and 300)
for the number of genes in a MEP chromosome. In all these experiments,
MEP was able to ﬁnd a perfect solution (out of 30 runs), thus yielding a
success rate of 3.33%. When the number of genes was set to 300, the average
of the best ﬁtness of each run taken as a percentage of the perfect ﬁtness
was 91.13%, with a standard deviation of 4.04. As a comparison, GP was
not able to obtain a perfect solution by using a population of 500 individuals

54

Figure 5.3: The success rate of the MEP algorithm for solving the 6-
multiplexer problem. (a) The relationship between the success rate and the
chromosome length. (b) The relationship between the success rate and the
population size. Results are summed over 100 runs.

55

and the average of the best ﬁtness of each run taken as a percentage of the
perfect ﬁtness was 79.2% (as reported in [83]).

5.3 Designing digital circuits for arithmetic

functions

The problem of evolving digital circuits has been intensely analyzed in the
recent past [54, 56, 57, 58, 91]. A considerable eﬀort has been spent on
evolving very eﬃcient (regarding the number of gates) digital circuits. J.
Miller, one of the pioneers in the ﬁeld of the evolvable digital circuits, used
a special technique called Cartesian Genetic Programming (CGP) [55] for
evolving digital circuits. CGP architecture consists of a network of gates
(placed in a grid structure) and a set of wires connecting them. For instance
this structure has been used for evolving digital circuits for the multiplier
problem [58]. The results [58] shown that CGP was able to evolve digital
circuits better than those designed by human experts.

In this section, we use Multi Expression Programming for evolving digital
circuits with multiple outputs. We present the way in which MEP may be
eﬃciently applied for evolving digital circuits. We show the way in which
multiple digital circuits may be stored in a single MEP chromosome and the
way in which the ﬁtness of this chromosome may be computed by traversing
the MEP chromosome only once.

Several numerical experiments are performed with MEP for evolving
arithmetic circuits. The results show that MEP signiﬁcantly outperforms
CGP for the considered test problems.

Numerical results are reported in the papers [79].

5.3.1 Problem statement

The problem that we are trying to solve here may be brieﬂy stated as follows:

Find a digital circuit that implements a function given by its truth table.

The gates that are usually used in the design of digital circuits along with

their description are given in Table 5.3.

The symbols used to represent some of the logical gates are given in Figure

5.4.

56

Table 5.3: Function set (gates) used in numerical experiments. Some func-
tions are independent on the input (functions 0 and 1), other depend on only
one of the input variables (functions 2-5), other functions depend on two
input variables (functions 6-15) and the other functions depends on three
input variables (functions 16-19). These functions are taken from [58].

#
0
1
2
3
4
5
6
7
8
9

Function
0
1
a
b
¯a
¯b
a · b
a · ¯b
¯a · b
¯a · ¯b

#
10
11
12
13
14
15
16
17
18
19

Function
a ⊕ b
a ⊕ ¯b
a + b
a + ¯b
¯a + b
¯a + ¯b
a · ¯c + b · c
a · ¯c + ¯b · c
¯a · ¯c + b · c
¯a · ¯c + ¯b · c

Figure 5.4: The symbols used to represent some of the logical gates in Table
5.3 (OR is function 12, AND is function 6, XOR is function 10 and MUX is
function 16). In some pictures a small circle may appear on these symbols
indicating the negation (inversion) of the respective results.

57

The MUX gate may be also represented using 2 ANDs and 1 OR [58].
However some modern devices use the MUX gate as an atomic device in that
all other gates are synthesized using this one.

Gates may also be represented using the symbols given in Table 5.4.

Table 5.4: Representation of some functions given in Table 5.3.

Gate
AND
OR
XOR
NOT

Representation
·
+
⊕
-

5.3.2 CGP for evolving digital circuits

An example of CGP program encoding a digital circuit is depicted in Figure
5.5.

Figure 5.5: A Cartesian Genetic Programming program for 1-bit adder prob-
lem.

In Figure 5.5, a gate array representation of a one-bit adder is given. A,
B, and Cin are the binary inputs. The outputs Sum and Cout are the binary
outputs. Sum represents the sum bit of the addition of A+B+Cin, and Cout
the carry bit. The chromosome representation of the circuit in Figure 2 is
the following (function symbols are given in bold):

58

0 1 0 10 0 0 2 6 3 2 1 10 0 2 3 16 6 5.
The evolutionary algorithm used in [58] to evolve digital circuits is a
simple form of (1+λ)-ES [19, 58], where λ was set to 4. This algorithm
seems to perform very well in conjunction to CGP representation. However,
a Genetic Algorithm (GA) [34] may also be used as underlying mechanism
for CGP.

5.3.3 MEP for evolving digital circuits

In this section we describe the way in which Multi Expression Programming
may be eﬃciently used for evolving digital circuits.

Each circuit has one or more inputs (denoted by NI ) and one or more
outputs (denoted NO). In section 3.4 we presented the way in which is the
ﬁtness of a chromosome with a single output is computed. When multiple
outputs are required for a problem, we have to choose NO genes which will
provide the desired output (it is obvious that the genes must be distinct
unless the outputs are redundant).

In CGP, the genes providing the program’s output are evolved just like all
other genes. In MEP, the best genes in a chromosome are chosen to provide
the program’s outputs. When a single value is expected for output we simply
choose the best gene (see section 3.4). When multiple genes are required as
outputs we have to select those genes which minimize the diﬀerence between
the obtained result and the expected output.

We have to compute ﬁrst the quality of a gene (sub-expression) for a given

output:

f (Ei, q) =

n
(cid:88)

|ok,i − wk,q|,

(5.1)

k=1
where ok,i is the obtained result by the expression (gene) Ei for the ﬁtness
case k and wk,q is the targeted result for the ﬁtness case k and for the output
q. The values f (Ei, q) are stored in a matrix (by using dynamic programming
[7] for latter use (see formula (5.2)).

Since the ﬁtness needs to be minimized, the quality of a MEP chromosome

is computed by using the formula:

f (C) = min

i1,i2,...,iN O

N O
(cid:88)

q=1

f (Eiq , q).

(5.2)

59

In equation (5.2) we have to choose numbers i1, i2, . . . , iN O in such way
to minimize the program’s output. For this we shall use a simple heuristic
which does not increase the complexity of the MEP decoding process: for
each output q (1 ≤ q ≤ NO) we choose the gene i that minimize the
quantity f (Ei, q). Thus, to an output is assigned the best gene (which has
not been assigned before to another output). The selected gene will provide
the value of the qth output.

Remark

Formulas (5.1) and (5.2) are the generalization of formulas (3.1) and (3.2)
for the case of multiple outputs of a MEP chromosome.

The complexity of the heuristic used for assigning outputs to genes is

O(NG · NO)

where NG is the number of genes and NO is the number of outputs.
We may use another procedure for selecting the genes that will provide the
problem’s outputs. This procedure selects, at each step, the minimal value in
the matrix f (Ei, q) and assign the corresponding gene i to its paired output
q. Again, the genes already used will be excluded from the search. This
procedure will be repeated until all outputs have been assigned to a gene.
However, we did not used this procedure because it has a higher complexity
– O(NO·log 2(NO)·NG) - than the previously described procedure which has
the complexity O(NO·NG).

5.3.4 Numerical experiments

In this section, several numerical experiments with MEP for evolving digital
circuits are performed. For this purpose several well-known test problems
[58] are used.

For reducing the chromosome length and for preventing input redundancy

we keep all the terminals on the ﬁrst positions of the MEP chromosomes.

For assessing the performance of the MEP algorithm three statistics are

of high interest:

(i) The relationship between the success rate and the number of genes in

a MEP chromosome.

60

(ii) The relationship between the success rate and the size of the population

used by the MEP algorithm.

(iii) The computation eﬀort.

The success rate is computed using the equation (5.3).

Success rate =

T he number of successf ul runs
T he total number of runs

.

(5.3)

The method used to assess the eﬀectiveness of an algorithm has been
suggested by Koza [42]. It consists of calculating the number of chromosomes,
which would have to be processed to give a certain probability of success.
To calculate this ﬁgure one must ﬁrst calculate the cumulative probability of
success P (M, i), where M represents the population size, and ithe generation
number. The value R(z) represents the number of independent runs required
for a probability of success (given by z) at generation i. The quantity I(M, z,
i) represents the minimum number of chromosomes which must be processed
to give a probability of success z, at generation i. Ns(i) represents the number
of successful runs at generation i, and Ntotal, represents the total number of
runs:

The formulae are given below:

P (M, i) =

N s(i)
Ntotal

.

R(z) = ceil

(cid:40) log(1 − z)

log(1 − P (M, i)

(cid:41)

.

I(M, i, z) = M · R(z) · i.

(5.4)

(5.5)

(5.6)

Note that when z = 1.0 the formulae are invalid (all runs successful). In

the tables and graphs of this section z takes the value 0.99.

In the numerical experiments performed in this section the number of
symbols in a MEP chromosome is usually larger than the number of symbols
in a CGP chromosome because in a MEP the problem’s inputs are also
treated as a normal gene and in a CGP the inputs are treated as being
isolated from the main CGP chromosome. Thus, the number of genes in a
MEP chromosome is equal to the number of genes in CGP chromosome +
the number of problem’s inputs.

61

Two-bit multiplier: a MEP vs. CGP experiment

The two-bit multiplier [54] implements the binary multiplication of two two-
bit numbers to produce a possible four-bit number. The training set for
this problem consist of 16 ﬁtness cases, each of them having 4 inputs and 4
outputs.

Several experiments for evolving a circuit that implements the two-bit
multiplier are performed. In the ﬁrst experiment we want to compare the
computation eﬀort spent by CGP and MEP for solving this problem. Gates
6, 7 and 10 (see Table 5.3) are used in this experiment.

The parameters of CGP are given in Table 5.5 and the parameters of the

MEP algorithm are given in Table 5.6.

Table 5.5: Parameters of the CGP algorithm.

Parameter
Number of rows
Number of columns
Levels back
Mutation
Evolutionary scheme

Value
1
10
10
3 symbols / chromosome
(1+4) ES

Table 5.6: Parameters of the MEP algorithm.

Parameter
Code length
Crossover
Crossover probability
Mutation
Selection

Value
14 (10 gates + 4 inputs)
Uniform
0.9
3 symbols / chromosome
Binary Tournament

One hundred runs of 150000 generations are performed for each popula-

tion size. Results are given in Table 5.7.

MEP outperforms CGP for all considered population sizes as shown in
Table 5.7. The diﬀerences range from 3.24% (for 3 individuals in the popu-

62

Table 5.7: Computation eﬀort spent for evolving two-bit multipliers for dif-
ferent population sizes. CGP results are taken from [58]. The diﬀerences ∆
in percent considering the values of MEP as a baseline are given in the last
column. Results are averaged over 100 runs.

Population
size
2
3
4
5
6
7
8
9
10
12
14
16
18
20
25
30
40
50

Cartesian Genetic
Programming
148808
115224
81608
126015
100824
100821
96032
108036
108090
115248
117698
120080
145854
120100
180075
162180
216360
225250

Multi Expression
Programming
53352
111600
54300
59000
68850
39424
44160
70272
28910
25536
26544
21216
17820
21120
23500
19440
16000
13250

∆

178.91
3.24
50.29
113.58
46.44
155.73
117.46
53.73
273.88
351.31
343.40
465.98
718.48
468.65
666.27
734.25
1252.25
1600.00

63

lation) up to 1600% (for 50 individuals in the population). From this exper-
iment we also may infer that large populations are better for MEP than for
CGP. The computational eﬀort decrease for MEP as the population size is
increased.

We are also interested in computing the relationship between the success

rate and the chromosome length and the population size.

The number of genes in each MEP chromosome is set to 20 genes when
the relationship between the success rate and the population size is analyzed.
When the relationship between the success rate and the population size is
analyzed a population consisting of 20 MEP chromosomes is used. Gates 6,
7 and 10 are used in this experiment. Other MEP parameters are given in
Table 5.6.

Results are depicted in Figure 5.6.

Figure 5.6: The relationship between the success rate of the MEP algorithm
and (a) number of genes in a chromosome, (b) the number of individuals in
population. Results are averaged over 100 runs.

Figure 5.6 shows that MEP is able to ﬁnd a correct digital circuit in
multiple runs. A population consisting of 90 individuals with 20 genes yields a
success rate of 100% (see Figure 5.6(b)) and a population with 20 individuals

64

with 85 genes yields a success rate of 92% (see Figure 5.6(a)).

From Figure 5.6(a) we may infer that larger MEP chromosomes are better
than the shorter ones. The minimum number of gates for this circuit is 7.
This number has been achieved by Miller during his numerical experiments
(see [58]). A MEP chromosome implementing Miller’s digital circuit has 11
genes (the actual digital circuit + 4 input genes). From Figure 5.6(a) we can
see that, for a MEP chromosome with 11 genes, only 6 correct solutions have
been evolved. As the chromosome length increases the number of correct
solutions evolved by also increases.
If the chromosome has more than 21
genes the success rate never decreases below than 70%.

Even if the chromosome length is larger than the minimum required (11
genes) the evolved solutions usually have no more than 14 genes. This is
due to the multi expression ability of MEP which acts like a provider of
variable length chromosomes [64]. The length of the obtained circuits could
be reduced by adding another feature to our MEP algorithm. This feature
has been suggested by C. Coello in [13] and it consists of a multiobjective
ﬁtness function. The ﬁrst objective is to minimize the diﬀerences between the
expected output and the actual output (see formulas (5.1) and (5.2)). The
second objective is to minimize the number of gates used by the digital circuit.
Note that he ﬁrst objective is more important than the second one. We also
have to modify the algorithm. Instead of stopping the MEP algorithm when
an optimal solution (regarding the ﬁrst objective) is found we continue to
run the program until a ﬁxed number of generations have been elapsed. In
this way we hope that also the number of gates (the second objective) will
be minimized.

Two-bit adder with carry

A more complex situation is the Two Bit Adder with Carry problem [58].
The circuit implementing this problem adds 5 bits (two numbers represented
using 2 bits each and a carry bit) and gives a three-bit number representing
the output.

The training set consists of 32 ﬁtness cases with 5 inputs and 3 outputs.
The relationship between the success rate and the chromosome length

and the population size is analyzed for this problem.

When the relationship between the success rate and the population size
is analyzed the number of genes in each MEP chromosome is set to 20 genes.
When the relationship between the success rate and the population size is

65

analyzed a population consisting of 20 MEP chromosomes is used. Gates
10 and 16 (see Table 5.3) are used in this experiment (as indicated in [58]).
Other MEP parameters are given in Table 5.4.

Results are depicted in Figure 5.7.

Figure 5.7: The relationship between the success rate of the MEP algorithm
and (a) number of genes in a chromosome, (b) the number of individuals in
population. Results are averaged over 100 runs.

Figure 5.7 shows that MEP is able solve this problem very well. When
the number of genes in a MEP chromosome is larger than 30 in more than
80 cases (out of 100) MEP was able to ﬁnd a perfect solution (see Figure
5.7(a)). After this value, the success rate does not increase signiﬁcantly. A
population with 270 individuals yields over 90 (out of 100) successful runs
(see Figure 5.7(b)).

This problem is more diﬃcult than the two-bit multiplier even if we used a
smaller function set (functions 10 and 16) that the set used for the multiplier
(function 6, 7 and 10).

66

Two-bit adder

The circuit implementing the N-Bit Adder problem adds two numbers rep-
resented using N bits each and gives a (N + 1)-bit number representing the
output.

The training set for this problem consists of 16 ﬁtness cases with 4 inputs

and 3 outputs.

For this problem the relationship between the success rate and the chro-

mosome length and the population size is analyzed.

When the relationship between the success rate and the population size
is analyzed the number of genes in each MEP chromosome is set to 12 genes.
When the relationship between the success rate and the chromosome length
is analyzed a population consisting of 100 MEP chromosomes is used.

Gates 0 to 9 (see Table 5.3) are used in this experiment. Other MEP

parameters are given in Table 5.8.

Table 5.8: Parameters of the MEP algorithm for evolving digital circuits.

Parameter
Crossover type
Crossover probability
Mutation
Selection

Value
Uniform
0.9
2 symbols / chromosome
Binary Tournament

For reducing the chromosome length and for preventing input redundancy

we keep all terminals on the ﬁrst positions of the MEP chromosomes.

Results are depicted in Figure 5.8
Figure 5.8 shows that MEP is able solve this problem very well. When
the number of genes in a MEP chromosome is 27 in more than 98 cases (out
of 100) MEP was able to ﬁnd a perfect solution (see Figure 5.8(a)). After
this value, the success rate does not increase signiﬁcantly. A population with
500 individuals yields over 69 (out of 100) successful runs (see Figure 5.8(b)).
The success rate for this problem may be increased by reducing the set of
function symbols to an optimal set.

From Figure 5.8(a) we may infer that larger MEP chromosomes are better
than the shorter ones. The minimum number of gates for this circuit is 7. A
MEP chromosome implementing the optimal digital circuit has 11 genes (the

67

Figure 5.8: The relationship between the success rate of the MEP algorithm
and (a) number of genes in chromosome, (b) the number of individuals in
population. Results are averaged over 100 runs.

68

actual digital circuit + 4 genes storing the inputs). From Figure 5.8(a) we can
see that, for a MEP chromosome with 11 genes, only 2 correct solutions have
been evolved. As the chromosome length increases the number of correct
If the chromosome has more than 21
solutions evolved by also increases.
genes the success rate never decreases below than 89%.

Three-bit adder

The training set for this problem consists of 64 ﬁtness cases with 6 inputs
and 4 outputs.

Due to the increased size of the training set we analyze in this section
only the cumulative probability of success and the computation eﬀort over
100 independent runs.

Gates 0 to 9 (see Table 5.3) are used in this experiment. Other MEP

parameters are given in Table 5.9.

Table 5.9: Parameters of the MEP algorithm for solving the 3-Bit Adder
problem.

Parameter
Population size
Code Length
Crossover type
Crossover probability
Mutation

Selection

Value
2000
30 genes
Uniform
0.9
2 symbols / chromo-
some
Binary Tournament

Results are depicted in Figure 5.9.
Figure 5.9 shows that MEP is able solve this problem very well. In 62
cases (out of 100) MEP was able to produce a perfect solution. The minimum
number of individuals required to be processed in order to obtain a solution
with a 99% probability is 2030000. This number was obtained at generation
145. The shortest evolved circuit for this problem contains 12 gates.

69

Figure 5.9: The cumulative probability of success and the number of indi-
viduals to be processed in order to obtained a solution with 99% probability.
Results are averaged over 100 runs.

Four-bit adder: preliminary results

The training set for this problem consists of 256 ﬁtness cases, each of them
having 8 inputs and 5 outputs. Due to the increased complexity of this
problem we performed only 30 independent runs using a population of 5000
individuals having 60 genes each. The number of generations was set to 1000.
In 24 (out of 30) runs MEP was able to ﬁnd a perfect solution. The short-
est evolved digital circuit contains 19 gates. Further numerical experiments
will be focused on evolving more eﬃcient circuits for this problem.

Two-bit multiplier

The two-bit multiplier circuit implements the binary multiplication of two
N -bit numbers to produce a possible 2 * N -bit number.

The training set for this problem consists of 16 ﬁtness cases, each of them

having 4 inputs and 4 outputs.

Several experiments for evolving a circuit that implements the two-bit
multiplier are performed. Since the problem has a reduced computational

70

complexity we perform a detailed analysis by computing the relationship
between the success rate and the code length and the population size.

The number of genes in each MEP chromosome is set to 14 genes when
the relationship between the success rate and the population size is analyzed.
When the relationship between the success rate and the chromosome length
is analyzed a population consisting of 50 MEP chromosomes is used. Gates
0 to 9 (see Table 5.3) are used in this experiment. Other MEP parameters
are given in Table 5.4.

Results are depicted in Figure 5.10.

Figure 5.10: The relationship between the success rate of the MEP algorithm
and (a) number of genes in a chromosome, (b) the number of individuals in
population. Results are averaged over 100 runs.

Figure 5.10 shows that MEP is able to ﬁnd a correct digital circuit in
many runs. A population consisting of 120 individuals with 14 genes yields a
success rate of 94% (see Figure 5.10(b)) and a population with 50 individuals
with 47 genes yields a success rate of 90% (see Figure 5.10(a)). We can see
that the success rate increase with more than 60% from MEP chromosomes
with 11 genes to MEP chromosomes with 15 genes. The minimum number
of gates required by this circuit is 7 and it has been obtained in most of the

71

runs.

3-Bit multiplier

The training set for this problem consists of 64 ﬁtness cases, each of them
having 6 inputs and 6 outputs.

This problem turned out to be more diﬃcult than the corresponding
3-Bit Adder (see section 5.3.4). Using a population of 10000 individuals
each having 100 genes we have obtained only 20 perfect solutions (out of 50
independent runs). The shortest evolved digital circuit contains 35 gates.
Further research will be focused on evolving more eﬃcient circuits for this
problem.

5.4 Designing digital circuits for NP-Complete

problems

MEP is used for evolving digital circuits for a well-known NP-Complete [31]
problem: the knapsack (subset sum) problem. Numerical results are reported
in [71].

Since this problem is NP-Complete we cannot realistically expect to ﬁnd
a polynomial-time algorithm for it. Instead, we have to speed-up the existing
techniques in order to reduce the time needed to obtain a solution. A possi-
bility for speeding-up the algorithms for this problem is to implement them
in assembly language. This could lead sometimes to improvements of over
two orders of magnitude. Another possibility is to design and build a special
hardware dedicated to that problem. This approach could lead to signiﬁ-
cantly improvements of the running time. Due to this reason we have chosen
to design, by the means of evolution, digital circuits for several instances of
the knapsack problem.

5.4.1 Evolving circuits for the knapsack problem

The knapsack problem may also be used as benchmarking problem for the
evolutionary techniques which design electronic circuits. The main advantage
increasing the number of inputs
of the knapsack problem is its scalability:
leads to more and more complicated circuits. The results show that MEP
performs very well for all the considered test problems.

72

The knapsack problem is a well-known NP-Complete problem [31]. No

polynomial-time algorithm is known for this problem.

Instead of designing a heuristic for this problem we will try to evolve

digital circuits which will provide the answer for a given input.

In the experiments performed in this section the set M consists of several
integer numbers from the set of consecutive integers starting with 1. For
instance if the base set is {1, 2, 3, 4, 5, 6, 7} then M may be {2, 5, 6}. We
will try to evolve a digital circuit that is able t provide the correct answer
for all subsets M of the base set.

The input for this problem is a sequence of bits. A value of 1 in position k
means that the integer number k belongs to the set M , otherwise the number
k does not belong to the set M .

For instance consider the consecutive integer numbers starting with 1
and ending with 7. The string 0100110 encodes the set M = {2, 5, 6}. The
number 1, 3, 4 and 7 do not belong to M since the corresponding positions
are 0. The possible subsets of M instance have the sum 2, 5, 6, 7, 8, 11 or
13. In our approach, the target sum is ﬁxed and we are asking if is there a
subset of given sum.

The number of training instances for this problem depends on the number
of consecutive integers used as base for M . If we use numbers 1, 2 and 3,
we have 23 = 8 training instances. If we use number 1, 2, 3, 4, 5, 6 and 7,
we have 27 = 128 training instances. In this case, whichever subset M of
{1,. . . ,7} will be presented to the evolved circuit we have to obtain a binary
answer whether the target sum k may or not be obtained from a subset of
M .

5.4.2 Numerical experiments

In this section several numerical experiments for evolving digital circuits for
the knapsack problem are performed. The general parameters of the MEP
algorithm are given in Table 5.10. Since diﬀerent instances of the problem
being solved will have diﬀerent degrees of diﬃculty we will use diﬀerent pop-
ulation sizes, number of genes in a chromosome and number of generations
for each instance. Particular parameters are given in Table 5.11.

Experimental results are given in Table 5.12. We are interested in com-
puting the number of successful runs and the number of gates in the shortest
evolved circuit.

73

Table 5.10: General parameters of the MEP algorithm for evolving digital
circuits.

Parameter
Crossover probability
Crossover type
Mutations
Function set
Terminal set
Selection

Value
0.9
Uniform
5 / chromosome
Gates 0 to 9
Problem inputs
Binary Tournament

Table 5.11: Particular parameters of the MEP algorithm for diﬀerent in-
stances of the knapsack problem.
In the second column the base set of
numbers is given for each instance. In the third column the target sum is
given.

# Set

of

num-
bers
{1. . . 4}
{1. . . 5}
{1. . . 6}
{1. . . 7}

1
2
3
4

ﬁtness

Sum Number
of
cases
16
32
64
128

5
7
10
14

Population
size

Number
of genes

Number of
generations

20
100
500
1000

10
30
50
100

51
101
101
201

Table 5.12: Results obtained by MEP for the considered test problems. 100
independent runs have been performed for all problems.

# Set

of

Sum

Successful runs

numbers
{1. . . 4}
{1. . . 5}
{1. . . 6}
{1. . . 7}

1
2
3
4

5
7
10
14

39 out of 100
31 out of 100
10 out of 100
7 out of 100

74

Number of gates in
the shortest circuit
3
5
11
21

Table 5.12 shows that MEP successfully found at least a solution for
the considered test problems. The diﬃculty of evolving a digital circuit for
this problem increases with the number of inputs of the problem. Only
20 individuals are required to obtain 39 solutions (out of 100 runs) for the
instance with 4 inputs.
In return, 1000 individuals (50 times more) are
required to obtain 10 perfect solutions (out of 100 independent runs) for the
instance with 7 inputs. Also the size of the evolved circuits increases with
the number of problem inputs. However, due to the reduced number of runs
we cannot be sure that we have obtained the optimal circuits. Additional
experiments are required in this respect.

Due to the NP-Completeness of the problem it is expected that the num-
ber of gates in the shortest circuit to increase exponentially with the number
of inputs.

5.5 Conclusions and Further Work

In this section, Multi Expression Programming has been used for evolving
digital circuits.
It has been shown the way in which multiple digital cir-
cuits may be encoded in the same chromosome and the way in which MEP
chromosomes are read only once for computing their quality. There was no
human input about how the circuits should be designed, just a measurement
of the degree to which a given circuit achieves the desired response.

Several numerical experiments for evolving digital circuits have been per-
formed. The circuits evolved during the numerical experiments are for the
2 and 3-bit Multiplier, the 2, 3 and 4-bit Adder problems, even-parity, NP-
complete problems and multiplexers.

These problems are well-known benchmark instances used for assessing

the performance of the algorithms evolving digital circuits.

Further numerical experiments with Multi Expression Programming will

be focused on evolving digital circuits for other interesting problems.

75

Chapter 6

MEP for Evolving Algorithms
and Game Strategies

6.1 Discovering game strategies

In this section we investigate the application of MEP technique for discover-
ing game strategies. This chapter is entirely original and it is based on the
papers [64, 70, 76].

Koza [42] suggested that GP can be applied to discover game strategy.
The game-playing strategy may be viewed as a computer program that takes
the information about the game as its input and produces a move as output.
The available information may be an explicit history of previous moves or
an implicit history of previous moves in the form of a current state of game
(e.g. the position of each piece on the chess board) [42].

Tic-tac-toe (TTT, or naughts and crosses) is a game with simple rules, but
complex enough to illustrate the ability of MEP to discover game strategy.

6.1.1 TTT game description

In Tic-Tac-Toe there are two players and a 3 × 3 grid. Initially the grid is
empty. Each player moves in turn by placing a marker in an open square. By
convention, the ﬁrst player’s marker is ”X” and the second player’s marker
is ”0”.

The player that put three markers of his type (”X” for the ﬁrst player

and ”0” for the second player) in a row is declared the winner.

76

The game is over when one of the players wins or all squares are marked
and no player wins. In the second case, the game ends with draw (none of
the players win). Enumerating the game tree shows that the second player
can obtain at least a draw.

A well-known evolutionary algorithm that evolves game strategy has been

proposed in [12]. This algorithm will be reviewed in the next section.

6.1.2 Chellapilla’s approach of TTT

In [12] Evolutionary Programming has been used in order to obtain a good
strategy (that never loses) for the Tic-Tac-Toe game. A strategy is encoded
in a neural network. A population of strategies encoded by neural networks
is evolved.

Each network receives a board pattern as input and yields a move as
output. The aim is to store in a neural network the function that gives the
quality of a conﬁguration. When a conﬁguration is presented to the network,
the network output (supplies) the next move.

Each neural network has an input layer with 9 nodes, an output layer

with 9 nodes, and a hidden layer with a variable number of nodes.

Fogel’s algorithm starts with a random population of 50 neural networks.
For each network the number of nodes from the hidden layer is randomly
chosen with a uniform distribution over integers 1...10. The initial weighted
connection strengths and bias terms are randomly distributed according to
a uniform distribution ranging over [-0.5, 0.5].

From each parent a single oﬀspring is obtained by mutation. Mutation op-
erator aﬀects the hidden layer structure, weight connections and bias terms.
Each strategy encoded in a neural network was played 32 times against

a heuristic rule base procedure.

The payoﬀ function has several values corresponding to winning, loosing

and draw.

The best individuals from a generation are retained to form the next

generation.

The process is evolved for 800 generations. According to [12], the best
obtained neural network is able to play to win or draw with a perfect play
strategy.

77

6.1.3 MEP approach of TTT

In this section we illustrate the use of MEP to discover an unbeatable play
strategy for Tic-Tac-Toe.

We are searching for a mathematical function F that gives the quality of
each game conﬁguration. Using this function the best conﬁgurations that can
be reached in one move from the current conﬁguration, is selected. Therefore
function F supplies the move to be performed for each game conﬁguration.
Function F evaluating each game conﬁguration is represented as a MEP
chromosome. The best expression encoded by a chromosome is chosen to be
the game strategy of that chromosome.

Without any loose of generality we may allow MEP strategy to be the

ﬁrst player in each game.

All expressions in the chromosome are considered in the ﬁtness assignment
process. Each expression is evaluated using an ”all-possibilities” procedure.
This procedure executes all moves that are possible for the second player. The
ﬁtness of an expression E is the number of games that the strategy encoded
by the expression E loses. Obviously the ﬁtness has to be minimized.

Let us denote by

P = (p0, p1. . . p8)

a game conﬁguration.
Each pk describes the states ”X”, ”0” or an empty square. In our experi-
ments the set {5, -5, 2} has been used for representing the symbols ”X”, ”0”
and the empty square.

The game board has been linearized by scanning board squares from up
to down and from left to right. Thus the squares in the ﬁrst line have indices
0, 1 and 2, etc. (see Figure 6.1).

Figure 6.1: Game board linearized representation.

Algorithm parameters are given in Table 6.1.

78

Table 6.1: Algorithm parameters for TTT game.

Parameter
Population size
Chromosome length
Mutation probability
Crossover type
Selection
Elitism size
Terminal set
Function set

Value
50
50 genes
0.05
Two-point-crossover
Binary tournament
1
T = {p0, p1,. . . , p8}
F = {+, -, *, /}

The MEP algorithm is able to evolve a perfect, non-loosing, game strategy
in 11 generations. This process requires less than 10 seconds when an Intel
Pentium 3 processor at 1GHz is used.

In Figure 6.2, the ﬁtness of the best individual in the best run and average

ﬁtness of the best individuals over all runs are depicted.

Figure 6.2 shows that an individual representing a non-loosing strategy

appears in the population at generation 14.

Some functions evolved by the MEP algorithm are given below:

F1(P ) = ((p4 − p5-(p6 + p5))*p8 + p4*p3)*(p4 − p7),

F2(P ) = p2-(p8*p7 − p4) − p7-(p2*p5),

F3(P ) = (p4*p1 + p2)*p7-(p1 − p2 + p7*p5)-(p8-(p3*p5)).

These functions do not force the win when it is possible, but they never
lose. This is a consequence of ﬁtness assignment process. However, the
proposed technique can also generate a function that forces the win whenever
it is possible.

It is not easy to compare this result with the result obtained by Chel-
lapilla and Fogel [12] as the experiment conditions were not the same. In [12]
evolved strategies play against a heuristic procedure, but here MEP formulas
play against an all-moves procedure. Population size was the same (50 indi-
viduals). Individual sizes are diﬃcult to be compared. All MEP individual

79

Figure 6.2: Fitness of the best individual in the best runs and the average
ﬁtness of the best individuals over all runs. The results are taken over 30
runs.

80

have the same size: 148 symbols. Neural network’s sizes used in [12] are
variable since the hidden layer contains a variable number of nodes. If the
number of nodes in the hidden layer is 9, then the size of the neural network
(biases + connection weights) is 9 * 9 + 9 * 9 * 9 + 9 * 9 = 324.

MEP approach seems to be faster as MEP was able to discover a non-
losing strategy in no more than 17 generations. As noted in [12] neural
network approach requires 800 generations.

A TTT heuristic vs. MEP approach

A good heuristic for Tic-Tac-Toe is described in what follows:

S1. If a winning move is available make that move, else
S2. If a winning move is available for the opponent, move to block it, else
S3. If a move of the opponent that leads to two winning ways is available,

block that move, else

S4. If the board center is available, move in the board center,
S5. If one ore more corners of the table are available, move in one of

them, else

S6. Move randomly in an available square.

This heuristic performs well on most of the game positions. However, by
applying one of the formulas evolved by the MEP algorithm some beneﬁts
are obtained:

• easy implementation in programming languages,

• MEP evolved formula is a faster algorithm than the previously shown

heuristic.

Applying MEP for generating complex game strategies

Using the all-possibilities technique (a backtracking procedure that plays all
the moves for the second player) allows us to compute the absolute quality
of a game position.

For complex games a diﬀerent ﬁtness assignment technique is needed since
the moves for the second player can not be simulated by an all-possibilities
procedure (as the number of moves that needs to be simulated is too large).

81

One ﬁtness assignment possibility is to use a heuristic procedure that
acts as the second player. But there are several diﬃculties related to this
approach. If the heuristic is very good it is possible that none of evolved
strategy could ever beat the heuristic. If the heuristic procedure plays as a
novice then many evolved strategy could beat the heuristic from the earlier
stages of the search process. In the last case ﬁtness is not correctly assigned
to population members and thus we can not perform a correct selection.

A good heuristic must play on several levels of complexity. At the be-
ginning of the search process the heuristic procedure must play at easier
levels. As the search process advances, the level of diﬃculty of the heuristic
procedure must increases.

However, for complex games such a procedure is diﬃcult to implement.
Another possibility is to search for a game strategy using a coevolutionary
algorithm [12]. This approach seems to oﬀer the most spectacular results. In
this case, MEP population must develop intelligent behavior based only on
internal competition.

6.2 Evolving winning strategies for Nim-like

games

In this section, we propose an evolutionary approach for computing the win-
ning strategy for Nim-like games. The proposed approach is is entirely orig-
inal and it is reported in the paper [76].

6.2.1 Introduction

Nim is one of the older two-person games known today. Whereas the stan-
dard approaches for determining winning strategies for Nim are based on the
Grundy-Sprague theory [8, 14], this problem can be solved using other tech-
niques. For instance, the ﬁrst winning strategy for this game was proposed
in 1901 by L.C. Bouton from the Harvard University. The Bouton’s solution
is based on computing the xor sum of the numbers of objects in each heap.
In other words Bouton computed a relation between the current state of the
game and the player which has a winning strategy if it is his/her turn to
move.

In this section, we propose an evolutionary approach for computing the
winning strategy for Nim-like games. The proposed approach is based on

82

Multi Expression Programming. The idea is to ﬁnd a mathematical relation
(an expression) between the current game state and the winner of the game
(assuming that both players do not make wrong moves). The searched ex-
pression should contain some mathematical operators (such as +, -, *, div ,
mod , and , or , not, xor ) and some operands (encoding the current game
state).

It is widely known [8, 30] that a winning strategy is based on separation
of the game’s states in two types of positions: P -positions (advantage to the
previous player) and N -positions (advantage to the next player). Our aim
is to ﬁnd a formula that is able to detect whether a given game position
belongs to P -positions or to N -positions. Our formula has to return 0 if the
given position is a P -position and a nonzero value otherwise. That could
be easily assimilated to a symbolic regression [42] or a classiﬁcation task.
It is well-known that machine learning techniques (such as Neural Networks
or Evolutionary Algorithms [34] are very suitable for solving this kind of
problems. However, the proposed approach is diﬀerent from the classical
approaches mainly because the P and N -positions are usually diﬃcult to be
identiﬁed for a new game. Instead we propose an approach that checks P
and N -position during the traversing of the game tree.

This theory can be easily extended for other games that share several
properties with the Nim game (i.e. games for which the winning strategy is
based on P and N -positions).

The problem of ﬁnding N and P -positions could be also viewed as a
classiﬁcation task with two classes. However, we do not use this approach
because in this case it is required to know the class (P or N ) for each game
position.

The results presented in this section enter in the class of human-competitive

results produced by an artiﬁcial machine. According to [42, 44] a result pro-
duced by an artiﬁcial machine is considered intelligent if it is equal or better
than a result that was accepted as a new scientiﬁc result at the time when
it was published in a peer-reviewed scientiﬁc. A list with other human-
competitive results produced by the Genetic Programming can be found in
[44].

6.2.2 Basics on Nim game

Nim is one of the oldest and most engaging of all two-person mathemati-
cal games known today [8, 14]. The name and the complete theory of the

83

game were invented by the professor Charles Leonard Bouton from Harvard
University about 100 years ago.

Players take turns removing objects (counters, pebbles, coins, pieces of
paper) from heaps (piles, rows, boxes), but only from one heap at a time. In
the normal convention the player who removes the last object wins.

The usual practice in impartial games is to call a hot position (N -position
- advantage to the next player, i.e. the one who is about to make a move)
and a cold one (P -position - advantage to the previous player, i.e. the one
who has just made a move).

In 1930, R. P. Sprague and P. M. Grundy developed a theory of impar-
tial games in which Nim played a most important role. According to the
Sprague-Grundy theory every position in an impartial game can be assigned
a Grundy number which makes it equivalent to a Nim heap of that size. The
Grundy number of a position is variously known as its Nim-heap or nimber
for short [8, 14].

A P-position for the Nim game is given by the equation:
x1 xor x2 xor . . . xor xn = 0,

where n is the number of heaps, xi is the number of objects in the ith heap
and xor acts as the modulo 2 operator.

A variant of the Nim game, also analyzed in this section, is the one
in which players may remove no more than k objects from a heap. In this
variant a P -position is characterized by the equation:

(x1 mod k) xor (x2 mod k) xor . . . xor (xn mod k) = 0,
where the equation parameters have been previously explained.

Due to the way of computing P -position we shall call this game Nim-

ModK .

6.2.3 Fitness assignment process

The procedure used for computing the quality of a chromosome is described
in this section.

Even if this problem could be easily handled as a classiﬁcation problem
(based on a set of ﬁtness cases), we do not use this approach since for the
new games it is diﬃcult to ﬁnd which the P -positions and N -positions are.
Instead we employ an approach based on the traversing the game tree. Each
nod in this tree is a game conﬁguration (state).

There are three theorems that run the winning strategy for the Nim

game [8]:

84

(i) Any move applied to a P -position turns the game into a N -position.

(ii) There is at least one move that turns the game from a N -position into

a P -position.

(iii) The ﬁnal position (when the game is over) is a P -position.

The value of the expression encoded into a MEP chromosome is computed
for each game state. If the obtained value is 0, the corresponding game state
is considered as being a P -position, otherwise the conﬁguration is considered
as being a N -position.

The ﬁtness of a chromosome is equal to the number of violations of the
above described rule that arises in a game tree. Thus, if the current formula
(chromosome) indicates that the game state encoded into a node of the game
tree is a P -position and (the same current formula indicates that) all the
game states encoded in the oﬀspring nodes are also P -positions means that
we have a violation of the rule b).

Since we do not want to have violations of the previously described rule,
our chromosome must have the ﬁtness equal to zero. This means that the
ﬁtness has to be minimized.

For a better understanding of the ﬁtness assignment process we provide

an example where we shall compute by hand the ﬁtness of a chromosome.

Consider the game state (2,1), and a MEP chromosome encoding the
expression E = a1 − a2*a1. The game tree of the Nim game is given in
Figure 6.3.

Figure 6.3 shows that the ﬁtness of a MEP chromosome encoding the
formula E = a1 − a2*a1 is four (there are four violations of the winning
strategy rules).

6.2.4 Numerical experiments

Several numerical for evolving winning strategies for Nim-like games are per-
formed in this section.

The purpose of these experiments is to evolve a formula capable to dis-
tinguish between a N -position and a P -position for the Nim game. We
shall analyze the relationships between the success rate and the population
size, the chromosome length and the number of generations used during the
search process.

85

Figure 6.3: The game tree for a Nim game that starts with the conﬁgu-
ration (2, 1). At the right side of each game conﬁguration is printed the
conﬁguration’ state (P -position or N -position) as computed by the formula
E = a1 − a1*a2. The conﬁgurations that violate one of the three rules de-
scribed above are encircled.

In all the experiments it is considered the following conﬁguration for the
Nim game: (4, 4, 4, 4). This conﬁguration has been chosen in order to have
a small computational time. However, this conﬁguration has proved to be
enough for evolving a winning strategy.

The total number of game conﬁgurations is 70 (which can be obtained
either by counting nodes in the game tree or by using the formula of combi-
nations with repetitions). Two permutations of the same conﬁguration are
not considered diﬀerent.

Remark

The success rate is computed by using the formula:

Success rate =

the number of successful runs
the total number of runs

.

Experiment 1

In the ﬁrst experiment the relationship between the population size and
the success rate is analyzed. MEP algorithm parameters are given in Table
6.2.

The results of this experiment are depicted in Figure 6.4.
Figure 6.4 shows that the success rate increases as the population size

86

Table 6.2: MEP algorithm parameters for Experiment 1.

Parameter
Chromosome length
Number of generations
Crossover probability
Mutations
Selection strategy
Terminal set
Function set

Value
15 genes
100
0.9
2 mutations / chromosome
binary tournament
TN im = {n, a1, a2, . . . , an}.
F = {+, -, *, div , mod , and , not, xor ,
or }

Figure 6.4: The relationship between the population size and the rate of
success. The results are averaged over 50 runs. The population size varies
between 20 and 200.

87

increases. The highest value - 37 successful runs (out of 50) - is obtained
with a population containing 140 individuals. Even a population with 20
individuals is able to yield 6 successful runs (out of 50).

Experiment 2

In the second experiment the relationship between the number of genera-
tions and the success rate is analyzed. MEP algorithm parameters are given
in Table 6.3.

Table 6.3: MEP algorithm parameters for Experiment 2.

Parameter
Chromosome length
Population Size
Crossover probability
Mutations
Selection strategy
Terminal set
Function set

Value
15 genes
100 individuals
0.9
2 mutations / chromosome
binary tournament
TN im = {n, a1, a2, . . . , an}.
F = {+, -, *, div, mod, and, not, xor,
or}

The results of this experiment are depicted in Figure 6.5.
Figure 6.5 shows that MEP is able to ﬁnd a winning strategy for the
Nim game in most of the runs. In 41 runs (out of 50) a perfect solutions
was obtained after 100 generations. 9 successful runs were obtained when
the algorithm is run for 20 generations.

Experiment 3

In the third experiment the relationship between the chromosome length
and the success rate is analyzed. MEP algorithm parameters are given in
Table 6.4.

The results of this experiment are depicted in Figure 6.6.
Figure 6.6 shows that the optimal number of genes of a MEP chromosome

is 35. With this value 25 runs (out of 50) were successful.

88

Figure 6.5: The relationship between the number of generations and the rate
of success. The results are averaged over 50 runs. The number of generations
varies between 20 and 200.

Table 6.4: MEP algorithm parameters for Experiment 3.

Parameter
Number Of Generations
Population Size
Crossover probability
Mutations
Selection strategy
Terminal set
Function set

Value
50
100 individuals
0.9
2 mutations / chromosome
binary tournament
TN im = {n, a1, a2, . . . , an}.
F = {+, -, *, div, mod, and, not, xor,
or}

89

Figure 6.6: The relationship between the chromosome length and the success
rate. The results are averaged over 50 runs. The chromosome length varies
between 5 and 50.

It is interesting to note that the formulas evolved by MEP are sometimes
diﬀerent from the classical a1 xor a2 xor a3 xor a4. For instance a correct
formula evolved by MEP is:

F = a1 xor a2 xor a3 − a4.

This formula is also correct due to the properties of the xor operator.

Experiment 4

In this experiment a formula for the NimModK game is evolved. The
initial conﬁguration was the same (4, 4, 4, 4) and k was set to 2. The
parameters for the MEP algorithm are given in Table 6.5.

This problem turned out to be more diﬃcult than the previous one. In
only 3 runs (out of 50) MEP was able to ﬁnd a perfect formula (i.e. a formula
that has the ﬁtness equal to 0).

90

Table 6.5: MEP algorithm parameters for Experiment 4.

Parameter
Chromosome length
Number of generations
Population size
Crossover probability
Mutations
Selection strategy
Terminal set
Function set

Value
35 genes
100
1000 individuals
0.9
2 mutations / chromosome
binary tournament
TN imM odK = {n, k, a1, a2, . . . , an}.
F = {+, -, *, div , mod , and , not, xor ,
or }

6.3 Evolving heuristics for NP-Complete prob-

lems

MEP technique is used for discovering TSP heuristics for graphs satisfying
triangle inequality (TI graphs). This option was chosen due to the existence
of a big number of real-world applications implying TI graphs (e.g. plains,
trains and vehicles routes). MEP technique is used to learn a path function
f that is used for evaluating the reachable nodes. This function serves as a
heuristic for detecting the optimum path.

This section is entire original and it is based on the paper [70].

6.3.1 MEP for TSP

In the proposed approach the TSP path starts with a randomly selected node
of the graph. Each node reachable from the current node in one step is eval-
uated using the function (computer program) f evolved by MEP algorithm.
The best node is added to the already detected path. The algorithm stops
when the path contains all graph nodes.

MEP learning process for TSP has a remarkable quality: the evolved
(learned) heuristic works very well for data sets much larger than the train-
ing set. For MEP training stage graphs having 3 to 50 nodes are considered.
Evolved MEP function was tested and performs well for graphs having max-

91

imum 1000 nodes.

Evolved function f is compared with some well known heuristics. Nu-
merical experiments emphasize that (for considered examples) MEP function
outperforms dedicated heuristics.

6.3.2 TSP problem with triangle inequality

TSP problem for TI graphs (i.e. satisfying triangle inequality) is stated as
follows.

Consider a set C = {c0, c1,. . . , cN–1} of cities, and a distance d(ci, cj) ∈
∈ C, d(ci, cj) = d(cj, ci), and for each three cities
∈ C, d(ci, cj) ≤ d(ci, ck) + d(ck, cj). The tour <cπ(0), cπ(1),

Z+ for each pair ci, cj
ci, cj, ck
. . . , cπ(N–1) > of all cities in C having minimum length is needed [1, 31]

TSP problem with triangle inequality is an NP-complete problem [31].

No polynomial time algorithm for solving TSP problem is known.

Several heuristics for solving TSP problem have been proposed. The most

important are Nearest Neighbor and the Minimum Spanning Tree [15, 31].

In this section we address the problem of discovering heuristics that can

solve TSP rather than solving a particular instance of the problem.

MEP technique is used for evolving a path function f that gives a way to
choose graph vertices in order to obtain a Hamiltonian cycle. The ﬁtness is
assigned to a function f in the current population by applying f on several
randomly chosen graphs (training set) and evaluating the results.

Evolved path function may be used for solving particular instances of
TSP. For each problem the graph nodes are evaluated using the path function
f and are added one by one to the already build path.

The algorithm for TSP using evolved path function f may be described

as follows:

S1. Let cπ(0) = c0 {the path starts with the node c0}
S2. k = 1;
S3. while k < N – 1 do
S4.
S5.
S6.
S7. endwhile

Using function f select cπ(k+1) – the next node of the path
Add cπ(k+1) to the already built path.
k = k + 1;

92

S4 is the key step of this algorithm. The procedure that selects the next
node of the path in an optimal way uses the function f evolved by the MEP
technique as described in the next sections.

6.3.3 Terminals and functions for evolving heuristic

function f

Path function f has to use (as input) some information about already build
path and some information about unvisited nodes. We consider a special
terminal set which is independent with respect to the number of graph nodes.
Let us denote by y1 the last visited node (current node). We have to
select the next node to be added to the path. In this respect all unvisited
nodes are considered. Let us denote by y2 the next node to be visited.

For evolving path function f we consider a set T of terminals involving

the following elements:

d y 1 y 2 – distance between the graph nodes y1 and y2,
min g y 1 (min g y 2) – the minimum distance from the nodes y1 (y2) to

unvisited nodes,

sum g y 1 (sum g y 2) – the sum of all distances between nodes y1 (y2) and

unvisited nodes,

prod g y 1 (prod g y 2) – the product of all distances between nodes y1 (y2)

and unvisited nodes,

max g y 1 (max g y 2) – the maximum distance from the nodes y1 (y2) to

unvisited nodes,

length – the length of the already built path.

The set T of terminals (function variables) is thus:

T = {d y 1 y 2, min g y 1, min g y 2, max g y 1, max g y 2, sum g y 1, sum g y 2,

prod g y 1, prod g y 2, length}.

Let us remark that members of T are not actual terminals (in the standard
acceptation). For this reason we may call members of T as instantiated (or
intermediate) nonterminals.

Set T of terminals is chosen in such way to be independent of the number
of graph nodes. This choice confers ﬂexibility and robustness to the evolved

93

heuristic.

For evolving a MEP function for TSP problem we may consider the fol-

lowing set of function symbols: F = {+, -, /, *, cos, sin, min, max }.

The node y2 that generates the lowest output of evolved function f is
chosen to be the next node of the path. Ties are solved arbitrarily. For
instance we may consider the node with the lowest index is selected.

Example

Consider the MEP linear structure:

1: d y1 y2
2: min g y 1
3: + 1, 2
4: sum g y 2
5: * 2, 4

This MEP individual encodes the path functions f1, f2, f3, f4, f5 given by:

f1 = d y1 y2,
f2 = min g y 1,
f3 = d y1 y2 + min g y 1,
f4 = sum g y 2,
f5 = min g y 1* sum g y 2.

6.3.4 Fitness assignment

In order to obtain a good heuristic we have to train the path function f
using several graphs. The training graphs are randomly generated at the
beginning of the search process and remain unchanged during the search
process. To avoid overﬁtting (see [86]), another set of randomly generated
graphs (validation set) is considered. After each generation the quality of the
best-so-far individual is calculated using the validation set in order to check
its generalization ability during training. At the end of the search process,
the function with the highest quality is supplied as the program output.

The ﬁtness (quality) of a detected path function f is deﬁned as the sum
of the TSP path length of graphs in the training set. Thus the ﬁtness is to
be minimized.

94

6.3.5 A numerical experiment

In this experiment we evolve a heuristic for solving TSP problem.

Let us denote by Gk the set of class of TI graphs having maximum k

nodes.

MEP algorithm considers the class G50 (i.e. graphs having 3 to 50 nodes)
for training and the class G100 for validation. Evolved path function was
tested for graphs in the class G1000 (i.e. graphs having maxim 1000 nodes).
MEP algorithm parameters are given in Table 6.3.5.

Table 6.6: MEP algorithm parameters for evolving a heuristic for TSP with
triangle inequality

Parameter
Population size
Number of generations
Chromosome length
Mutation probability
Crossover type
Crossover probability
Training set size
Maximum number of nodes in training set
Validation set size
Maximum number of nodes in validation set

Value
300
100
40 genes
0.1
One-Crossover-Point
0.9
30
50
20
100

The evolution of the best individual ﬁtness and the average ﬁtness of the

best individuals over 30 runs are depicted in Table 6.7.
A path function evolved by the MEP algorithm is:

f = (sum g(y2)) * (d y 1 y 2 - (max (d y 1 y 2, max g(y1))) + d y 1 y 2).

Heuristic function f that is evolved by MEP technique is directly used
for building the optimum path. The corresponding learning process has a
remarkable quality: the evolved (learned) heuristic works very well on data
sets signiﬁcantly larger than the training set. In our example the training
set G50 is signiﬁcantly smaller than the set G1000 used for testing.

95

Figure 6.7: The ﬁtness evolution of the best individual in the best run and
the average ﬁtness of the best individuals over 30 runs.

6.3.6 Assessing the performance of the evolved MEP

heuristic

In this section the performance of evolved MEP heuristic, NN and MST are
compared. In the ﬁrst experiment we compare the considered algorithms on
some randomly generated graphs. In the second experiment the heuristics
are compared against several diﬃcult problems in TSPLIB [87].

Experiment 1

In this experiment we provide a direct comparison of the evolved MEP
heuristic, NN and MST. The considered heuristics are tested for randomly
generated graphs satisfying triangle inequality.

Evolved heuristic was tested for diﬀerent graphs from the classes G200,
G500 and G1000. For each graph class 1000 graphs satisfying triangle inequal-
ity have been randomly generated. These graphs have been considered for
experiments with evolved MEP heuristic, NN and MST.

Performance of evolved MEP heuristic, NN and MST are depicted in

96

Table 6.7.

Table 6.7: Evolved MEP heuristic vs. NN, MST. For each graph class we
present the number of graphs for which evolved MEP heuristic generates a
cycle shorter than the cycle obtained by the algorithm MST and NN.

Graphs types
G200
G500
G1000

MST
953
974
990

NN
800
906
948

Results obtained emphasizes that evolved MEP heuristic outperforms NN

and MST algorithms on random graphs.

Experiment 2

To obtain a stronger evidence of the results above we test the performance
of the considered heuristics against some diﬃcult problems in TSPLIB. The
results are presented in Table 6.8.

From Table 6.8 we can see that evolved MEP heuristic performs better
than NN and MST on most of the considered problems. Only for ﬁve prob-
lems (bier127, ch150, d198, d493, ﬂ417) NN performs better than evolved
MEP heuristic. MST does not perform better than evolved MEP heuristic
for no problem. The highest error obtained by the evolved MEP heuristic
is 23.05 (the problem d493) while the highest error obtained by NN is 23.45
(the problem rd400). The lowest error obtained with MEP is 1.72 (problem
berlin52) while the lowest error obtained by NN is 8.17 (problem bier127).
The mean of errors for all considered problems is 10.61 (for evolved MEP
heuristic) 16.33 (for NN heuristic) and 35.77 (for MST heuristic).

6.4 Conclusions and further work

Three approaches have been proposed in this section:

• an evolutionary approach for the Nim game. The underlying evolu-
tionary technique is Multi Expression Programming - a very fast and

97

Table 6.8: The performance of evolved MEP heuristic, NN and MST on
some problems in TSPLIB. Length is the length of the TSP path obtained
with one of the considered heuristics. Error is calculated as (Length - Short-
est Length)/ Shortest Length * 100. Each node of the graph has been con-
sidered as the ﬁrst node of the path

Problem MEP
Length

a280
att48
berlin52
bier127
ch130
ch150
d198
d493
d657
eil101
eil51
eil76
ﬂ417
gil262
kroA150
kroA200
kroB100
kroB200
lin105
lin318
pcb442
pr226
pr264
rat195
rat575
rat783
rd400
ts225
u574
u724

2858.86
37188.2
7672.1
134945
6558.03
7104.03
17780.7
43071.3
56965.6
685.013
441.969
564.179
13933.8
2659.17
28376.3
32040.3
24801
33267.4
15133.2
46203.4
56948.3
84937.8
55827.1
2473.49
7573.6
9982.96
16973.3
136069
43095.6
46545.7

Error
(%)
10.85
10.93
1.72
14.08
7.33
8.82
12.67
23.05
16.46
8.9
3.74
4.86
17.47
11.82
6.98
9.09
12.01
13.01
5.24
9.93
12.15
5.68
13.61
6.47
11.82
13.36
11.07
7.44
16.77
11.06

NN
Length

3084.22
39236.9
8182.19
127954
7198.74
7078.44
17575.1
41167
60398.7
753.044
505.298
612.656
13828.2
2799.49
31482
34547.7
25883
35592.4
16939.4
49215.6
57856.3
92905.1
54124.5
2560.62
7914.2
10836.6
18303.3
140485
44605.1
98
50731.4

MST

Error (%) Length

19.58976
17.04227
8.488332
8.177068
17.81899
8.431985
11.37579
17.61328
23.48442
19.72083
18.61455
13.87658
16.58545
17.72456
18.6925
17.63722
16.90077
20.91042
17.80652
17.09915
13.9397
15.59818
10.15468
10.22901
16.84925
23.05928
19.77816
10.92994
20.86465
21.04844

3475.23
43955.8
10403.9
152747
8276.51
9142.99
17957.6
41846.6
63044.2
846.116
605.049
739.229
16113.2
3340.84
38754.8
40204.1
28803.5
40619.9
18855.6
60964.8
73580.1
111998
65486.5
2979.64
9423.4
11990.5
20962
187246
50066
60098.9

Error
(%)
34.75
31.11
37.94
29.13
35.45
40.05
13.79
19.55
28.89
34.51
42.03
37.4
35.85
40.48
46.11
36.89
30.09
37.98
31.13
45.05
44.9
39.35
33.27
28.26
39.13
36.16
37.17
47.85
35.66
43.39

eﬃcient Genetic Programming variant. Numerical experiments have
shown that MEP is able to discover a winning strategy in most of the
runs.

The proposed method can be easily applied for games whose winning
strategy is based on P and N -positions. The idea is to read the game
tree and to count the number of conﬁgurations that violates the rules
of the winning strategy.

• an evolutionary approach for the Tic-Tac-Toe game. Evolved strat-
egy is very fast. About 400.000 games/second can be played without
loss.

• an evolutionary approach for evolving heuristics for the TSP. Numerical
experiments have shown that the evolved heuristic performs better than
the NN and MST heuristics.

6.4.1 Applying MEP for generating complex game strate-

gies

Using the all-possibilities technique (a backtracking procedure that plays all
the moves for the second player) allows us to compute the absolute quality of
a game position. For complex games a diﬀerent ﬁtness assignment technique
is needed since the moves for the second player can not be simulated by an
all-possibilities procedure (as the number of moves that needs to be simulated
is too large). One ﬁtness assignment possibility is to use a heuristic procedure
that acts as the second player. But there are several diﬃculties related to
this approach. If the heuristic is very good it is possible that none of evolved
strategy could ever beat the heuristic. If the heuristic procedure plays as a
novice then many evolved strategy could beat the heuristic from the earlier
stages of the search process. In the last case ﬁtness is not correctly assigned
to population members and thus we can not perform a correct selection. A
good heuristic must play on several levels of complexity. At the beginning
of the search process the heuristic procedure must play at easier levels. As
the search process advances, the level of diﬃculty of the heuristic procedure
must increases. However, for complex games such a procedure is diﬃcult
to implement. Another possibility is to search for a game strategy using a
coevolutionary algorithm. This approach seems to oﬀer the most spectacular

99

results. In this case, MEP population must develop intelligent behavior based
only on internal competition.

100

Chapter 7

Inﬁx Form Genetic
Programming

A new GP variant called Inﬁx Form Genetic Programming (IFGP) is de-
scribed. IFGP individuals are arrays of integer values encoding mathematical
expressions in inﬁx form. IFGP is used for solving real-world classiﬁcation
problems.

The chapter is entirely original and it is based on the paper [67].

7.1 Introduction

Classiﬁcation is the task of assigning inputs to a number of discrete categories
or classes [38]. Examples include classifying a handwritten letter as one from
A-Z, classifying a speech pattern to the corresponding word, etc.

Machine learning techniques have been extensively used for solving clas-
siﬁcation problems. In particular Artiﬁcial Neural Networks (ANNs) [38, 86]
have been originally designed for classifying a set of points in two distinct
classes. Genetic Programming (GP) techniques [42] have also been used for
classiﬁcation purposes. For instance, LGP [9] has been used for solving sev-
eral classiﬁcation problems in PROBEN1. The conclusion was that LGP is
able to solve the classiﬁcation problems with the same error rate as a neural
network.

Inﬁx Form Genetic Programming (IFGP), chromosomes are strings en-
coding complex mathematical expressions using inﬁx form. An interesting
feature of IFGP is its ability of storing multiple solutions of a problem in a

101

chromosome.

In what follows IFGP is described and used for solving several real-world

classiﬁcation problems taken from PROBEN1 [86].

7.2 Prerequisite

We denote by F the set of function symbols (or operators) that may appear
in a mathematical expression. F usually contains the binary operators {+,
−, *, /}. Number of Operators denotes the number of elements in F . A
correct mathematical expression also contains some terminal symbols. The
set of terminal symbols is denoted by T . The number of terminal symbols is
denoted by Number of Variables.

The symbols that may appear in a mathematical expression encoded by
the IFGP are from the set T ∪ F ∪ {’(’, ’)’}. The total number of sym-
bols that may appear in a valid mathematical expression is denoted by Num-
ber of Symbols.

By Ci we denote the value on the ith gene in a IFGP chromosome and
by Gi the symbol in the ith position in the mathematical expression encoded
into an IFGP chromosome.

7.3 Individual representation

In this section we describe how IFGP individuals are represented and how
they are decoded in order to obtain a valid mathematical expression.

Each IFGP individual is a ﬁxed size string of genes. Each gene is an inte-
ger number in the interval [0 .. Number Of Symbols - 1]. An IFGP individual
can be transformed into a functional mathematical expression by replacing
each gene with an eﬀective symbol (a variable, an operator or a parenthesis).

Example

If we use the set of functions symbols F = {+, *, -, /}, and the set of

terminals T = {a, b}, the following chromosome

C = 7, 3, 2, 2, 5

is a valid chromosome in IFGP system.

102

7.4 Decoding IFGP individuals

We will begin to decode this chromosome into a valid mathematical expres-
sion. In the ﬁrst position (in a valid mathematical expression) we may have
either a variable, or an open parenthesis. That means that we have Num-
ber Of Variables + 1 possibilities to choose a correct symbol on the ﬁrst
position. We put these possibilities in order: the ﬁrst possibility is to choose
the variable x1, the second possibility is to choose the variable x2 . . . the last
possibility is to choose the closed parenthesis ’)’. The actual value is given
by the value of the ﬁrst gene of the chromosome. Because the number stored
in a chromosome gene may be larger than the number of possible correct
symbols for the ﬁrst position we take only the value of the ﬁrst gene modulo
number of possibilities for the ﬁrst gene.

Generally, when we compute the symbol stored in the ith position in
expression we have to compute ﬁrst how many symbols may be placed in that
position. The number of possible symbols that may be placed in the current
position depends on the symbol placed in the previous position. Thus:

(i) if the previous position contains a variable (xi), then for the current
position we may have either an operator or a closed parenthesis. The
closed parenthesis is considered only if the number of open parentheses
so far is larger than the number of closed parentheses so far.

(ii) if the previous position contains an operator, then for the current po-

sition we may have either a variable or an open parenthesis.

(iii) if the previous position contains an open parenthesis, then for the cur-
rent position we may have either a variable or another open parenthesis.

(iv) if the previous position contains a closed parenthesis, then for the cur-
rent position we may have either an operator or another closed paren-
thesis. The closed parenthesis is considered only if the number of open
parentheses so far is larger than the number of closed parentheses.

Once we have computed the number of possibilities for the current posi-
tion it is easy to determine the symbol that will be placed in that position:
ﬁrst we take the value of the corresponding gene modulo the number of pos-
sibilities for that position. Let p be that value

103

(p = Ci mod Number Of Possibilities).

The pth symbol from the permitted symbols for the current is placed in
(Symbols that may
the current position in the mathematical expression.
appear into a mathematical expression are ordered arbitrarily. For instance
we may use the following order: x1, x2, . . . , +, -, *, /, ’(’, ’)’. )

All chromosome genes are translated but the last one. The last gene is

used by the correction mechanism (see below).

The obtained expression usually is syntactically correct. However, in
some situations the obtained expression needs to be repaired. There are two
cases when the expression needs to be corrected:

The last symbol is an operator (+, -, *, /) or an open parenthesis. In that
case a terminal symbol (a variable) is added to the end of the expression.
The added symbol is given by the last gene of the chromosome.

The number of open parentheses is greater than the number of closed
parentheses. In that case several closed parentheses are automatically added
to the end in order to obtain a syntactically correct expression.

Remark

If the correction mechanism is not used, the last gene of the chromosome will
not be used.

Example

Consider the chromosome

C = 7, 3, 2, 0, 5, 2

and the set of terminal and function symbols previously deﬁned

T = {a, b},

F = {+, -, *, /}.

For the ﬁrst position we have 3 possible symbols (a, b and ‘(‘). Thus,
the symbol in the position C0 mod 3 = 1 in the array of possible symbols is
placed in the current position in expression. The chosen symbol is b, because
the index of the ﬁrst symbol is considered to be 0.

104

For the second position we have 4 possibilities (+, -, *, /). The possibility
of placing a closed parenthesis is ignored since the diﬀerence between the
number of open parentheses and the number of closed parentheses is zero.
Thus, the symbol ’/’ is placed in position 2.

For the third position we have 3 possibilities (a, b and ’(’). The symbol

placed on that position is an open parenthesis ’(’.

In the fourth position we have 3 possibilities again (a, b and ’(’). The

symbol placed on that position is the variable a.

For the last position we have 5 possibilities (+, -, *, /) and the closed
parenthesis ’)’. We choose the symbol on the position 5 mod 5 = 0 in the
array of possible symbols. Thus the symbol ‘+’ is placed in that position.

The obtained expression is E = b / (a+.
It can be seen that the expression E it is not syntactically correct. For
repairing it we add a terminal symbol to the end and then we add a closed
parenthesis. Now we have obtained a correct expression:

E = b / (a + a).

The expression tree of E is depicted in Figure 7.1.

Figure 7.1: The expression tree of E = b / (a + a).

7.5 Using constants within the IFGP model

An important issue when designing a new GP technique is the way in which
the constants are embaded into the proposed model.

Fixed or ephemeral random constants have been extensively tested within
GP systems [42]. The interval over which the constants are initially generated

105

is usually problem-dependent. For a good functionality of the program a
priori knowledge about the required constants is usually needed.

By contrast, IFGP involves a problem-independent system of constants.
It is known that each real number may be written as a sum of powers of

2.

Within the IFGP model each constant is a power of 2. The total number
of constants is also problem-independent. For instance, if we want to solve
problems using double precision constants, we need 127 constants:

2−63, 2−62,. . . , 2−1, 20, 21,. . . , 263.

Particular instances of the classiﬁcation problems may require fewer con-
stants. As can be seen in section 7.4 a good solution for some classiﬁcation
problems can be obtained without using constants. However, if we do not
know what kind of constants are required by the problems being solved it is
better the use the double precision system of constants (as described above).
Within the IFGP chromosome each constant is represented by its expo-
nent. For instance the constant 217 is stored as the number 17, the constant
2−4 is stored as the number -4 and the constant 1 = 20 is stored as the num-
ber 0. These constants will act as terminal symbols. Thus the extended set
of terminal symbols is

T = {x1, x2,. . . , -63, -62, .., -1, 0, 1, . . . , 62, 63}.

7.6 Fitness assignment process

In this section we describe how IFGP may be eﬃciently used for solving
classiﬁcation problems.

A GP chromosome usually stores a single solution of a problem and the

ﬁtness is normally computed using a set of ﬁtness cases.

Instead of encoding a single solution, an IFGP individual is allowed to
store multiple solutions of a problem. The ﬁtness of each solution is computed
in a conventional manner and the solution having the best ﬁtness is chosen
to represent the chromosome.

In the IFGP representation each sub-tree (sub-expression) is considered

as a potential solution of a problem.

106

Example

The previously obtained expression (see section 7.4) contains 4 distinct

solutions (sub-expressions):

E1 = a,
E2 = b,
E3 = a + a,
E4 = b / (a + a).

Now we will explain how the ﬁtness of a (sub)expression is computed.
Each class has associated a numerical value: the ﬁrst class has the value
0, the second class has the value 1 and the mth class has associated the
numerical value m−1. Any other system of distinct numbers may be used.
We denote by ok the number associated to the kth class.

The value vj(Ei) of each expression Ei (in an IFGP) chromosome for
each row (example) j in the training set is computed. Then, each row in
the training set will be classiﬁed to the nearest class (the class k for which
the diﬀerence |vj(Ei) − ok| is minimal). The ﬁtness of a (sub)expression is
equal to the number of incorrectly classiﬁed examples in the training set.
The ﬁtness of an IFGP chromosome will be equal to the ﬁtness of the best
expression encoded in that chromosome.

Remarks

(i) Since the set of numbers associated with the problem classes was arbi-
trarily chosen it is expected that diﬀerent systems of number to gener-
ate diﬀerent solutions.

(ii) When solving symbolic regression or classiﬁcation problems IFGP chro-
mosomes need to be traversed twice for computing the ﬁtness. That
means that the complexity of the IFGP decoding process it is not higher
than the complexity of other methods that store a single solution in a
chromosome.

107

7.7 Search operators

Search operators used within the IFGP model are recombination and muta-
tion. These operators are similar to the genetic operators used in conjunction
with binary encoding [19].

7.7.1 Crossover

By recombination two parents exchange genetic material in order to obtain
two oﬀspring. In our numerical experiments only two-point recombination is
used.

7.7.2 Mutation

Mutation operator is applied with a ﬁxed mutation probability (pm). By mu-
tation a randomly generated value over the interval [0, Number of Symbols-1]
is assigned to the target gene.

7.8 Handling exceptions within IFGP

Exceptions are special situations that interrupt the normal ﬂow of expression
evaluation (program execution). An example of exception is division by zero
which is raised when the divisor is equal to zero.

GP techniques usually use a protected exception handling mechanism [42].
For instance if a division by zero exception is encountered, a predeﬁned value
(for instance 1 or the numerator) is returned. This kind of handling mecha-
nism is speciﬁc for Linear GP [9], standard GP [42] and GE [82].

IFGP uses a new and speciﬁc mechanism for handling exceptions. When
an exception is encountered (which is always generated by a gene containing
a function symbol), the entire (sub) tree which has generated the exception is
mutated (changed) into a terminal symbol. Exception handling is performed
during the ﬁtness assignment process.

7.9 IFGP algorithm

A steady-state [95] variant of IFGP is employed in this section. The algorithm
starts with a randomly chosen population of individuals. The following stpng

108

are repeated until a termination condition is reached. Two parents are chosen
at each step using binary tournament selection [19]. The selected individuals
are recombined with a ﬁxed crossover probability pc. By recombining two
parents, two oﬀspring are obtained. The oﬀspring are mutated and the best
of them replaces the worst individual in the current population (only if the
oﬀspring is better than the worst individual in population).

The algorithm returns as its answer the best expression evolved for a ﬁxed

number of generations.

7.10 Solving classiﬁcation problems using IFGP

IFGP technique is applied for solving diﬃcult learning problems. Real-world
data sets are considered for the training process.

7.10.1 Data sets

Numerical experiments performed in this section are based on several bench-
mark problems taken from PROBEN1 [86]. These datasets were created
based on the datasets from the UCI Machine Learning Repository [103].

Used problems are brieﬂy described in what follows.

Cancer

Diagnosis of breast cancer. Try to classify a tumor as either benignant
or malignant based on cell descriptions gathered by microscopic examination.

Diabetes

Diagnosis diabetes of Pima Indians. Based on personal data and the re-
sults of medical examinations try to decide whether a Pima Indian individual
is diabetes positive or not.

Heartc

Predicts heart disease. Decides whether at least one of four major vessels
is reduced in diameter by more than 50%. The binary decision is made
based on personal data such as age, sex, smoking habits, subjective patient

109

pain descriptions and results of various medical examinations such as blood
pressure and electro cardiogram results.

This data set was originally created by Robert Detrano from V.A. Medi-

cal Center Long Beach and Cleveland Clinic Foundation.

Horse

Predicts the fate of a horse that has colic. The results of a veterinary
examination of a horse having colic are used to predict whether the horse
will survive will die or should be euthanized.

The number of inputs, of classes and of available examples, for each test

problem, are summarized in Table 7.1.

Table 7.1: Summarized attributes of several classiﬁcation problems from
PROBEN1.

Problem Number of

in-

puts
cancer
9
diabetes 8
35
heartc
58
horse

Number
classes
2
2
2
3

of

Number of ex-
amples
699
768
303
364

7.10.2 Numerical experiments

The results of several numerical experiments with ANNs, LGP and IFGP are
presented in this section.

Each data set is divided in three sub-sets (training set -50%, validation

set - 25 %, and test set - 25%) (see [86]).

The test set performance is computed for that chromosome which had
minim validation error during the search process. This method, called early
stopping, is a good way to avoid overﬁtting [86] of the population individuals
to the particular training examples used.
In that case the generalization
performance will be reduced.

110

In [9] Linear GP was used to solve several classiﬁcation problems from

PROBEN1. The parameters used by Linear GP are given in Table 7.2.

Table 7.2: Linear GP parameters used for solving classiﬁcation tasks from
PROBEN1

Parameter
Population size
Number of demes
Migration rate
Classiﬁcation error weight in ﬁtness
Maximum number of generations
Crossover probability
Mutation probability
Maximum mutation step size for con-
stants
Maximum program size
Initial maximum program size
Function set
Terminal set

Value
5000
10
0.05
1.0
250
0.9
0.9
±5

256 instructions
25 instructions
{+, -, *, /, sin, exp, if >, if ≤}
{0,..,256} ∪ {input variables}

IFGP algorithm parameters are given in Table 7.3.
The results of the numerical experiments are presented in Table 7.4.
Table 7.4 shows that IFGP is able to obtain similar performances as those
obtained by LGP even if the population size and the chromosome length used
by IFGP are smaller than those used by LGP. When compared to ANNs we
can see that IFGP is better only in 3 cases (out of 12).

We are also interested in analysing the relationship between the classiﬁ-
cation error and the number of constants used by the IFGP chromosomes.
For this purpose we will use a small population made up of only 50 indi-
viduals. Note that this is two magnitude orders smaller than those used by
LGP. Other IFGP parameters are given in Table 7.3. Experimental results
are given in Table 7.5.

Table 7.5 shows that the best results are obtained when the constants
are not used in our IFGP system. For 8 (out of 12) cases the best result
obtained by IFGP outperform the best result obtained by LGP. That does

111

Table 7.3: IFGP algorithm parameters for solving classiﬁcation problems
from PROBEN1.

Parameter
Population Size
Chromosome length
Number of generations
Crossover probability
Crossover type
Mutation
Number of Constants
Function set
Terminal set

Value
250
30
250
0.9
Two-point Crossover
2 mutations per chromosome
41 {2−20, . . . , 20, 220}
{+, -, *, /, sin, exp}
{input variables} ∪ The set of con-
stants.

not mean that the constants are useless in our model and for the considered
test problems. An explanation for this behaviour can be found if we take
a look at the parameters used by IFGP. The population size (of only 50
individuals) and the chromosome length (of only 30 genes) could not be
enough to obtain a perfect convergence knowing that some problems have
many parameters (input variables). For instance the horse problem has 58
attributes and a chromosome of only 30 genes could not be enough to evolve
a complex expression that contains suﬃcient problem’s variables and some
of the considered constants.
It is expected that longer chromosomes will
increase the performances of the IFGP technique.

7.11 Conclusion and further work

An evolutionary technique, Inﬁx Form Genetic Programming (IFGP) has
been described in this chapter. The IFGP technique has been used for solving
several classiﬁcation problems. Numerical experiments show that the error
rates obtained by using IFGP are similar and sometimes even better than
those obtained by Linear Genetic Programming.

Further numerical experiments will try to analyse the relationship be-

112

Table 7.4: Classiﬁcation error rates of IFGP, LGP and ANN for some date
sets from PROBEN1. LGP results are taken from [9]. ANNs results are taken
from [86]. The cases where IFGP is better than LGP have been written on
a grey background. The cases where IFGP is better than ANNs have been
bolded and italicized. Results are averaged over 30 runs

Problem IFGP–test set

best mean
1.14
cancer1
4.59
cancer2
cancer3
3.44
diabetes1 22.39
diabetes2 25.52
diabetes3 21.35
16.00
heart1
1.33
heart2
12.00
heart3
23.07
horse1
30.76
horse2
30.76
horse3

2.45
6.16
4.92
25.64
28.92
25.31
23.06
4.40
13.64
31.11
35.05
35.01

LGP–test set
stddev best mean
0.57
0.69
4.02
0.45
3.45
1.23
21.35
1.61
25.00
1.71
19.27
2.20
18.67
3.72
1.33
2.35
10.67
2.34
23.08
2.68
31.87
2.33
31.87
2.82

2.18
5.72
4.93
23.96
27.85
23.09
21.12
7.31
13.98
30.55
36.12
35.44

NN–test set

stddev mean
0.59
0.66
0.65
1.42
1.49
1.27
2.02
3.31
2.03
2.24
1.95
1.77

1.38
4.77
3.70
24.10
26.42
22.59
20.82
5.13
15.40
29.19
35.86
34.16

stddev
0.49
0.94
0.52
1.91
2.26
2.23
1.47
1.63
3.20
2.62
2.46
2.32

113

Table 7.5: Classiﬁcation error rates of IFGP (on the test set) using diﬀerent
number of constants. The cases where IFGP is better than LGP have been
written on a grey background. The cases where IFGP is better than ANNs
have been bolded and italicized. Results are averaged over 30 runs.

Problem IFGP–41

con-

IFGP–0
stants

con-

IFGP–81 con-
stants

stants
best mean stddevbest mean stddev best mean stddev
1.14
cancer1
2.45
0.60
4.02
cancer2
6.16
0.91
cancer3
3.44
5.17
1.10
diabetes1 22.39
25.74
2.19
diabetes2 26.04
29.91
1.65
diabetes3 21.87
25.34
1.76
17.33
heart1
24.44
3.76
4.07
6.97
1.33
heart2
12.00 14.00 2.51
heart3
26.37
horse1
3.12
32.16
30.76 35.64 2.37
horse2
28.57 34.13 3.14
horse3

1.06
3.18
1.14
0.81
6.14
4.02
1.50
5.07
2.87
1.76
26.04
21.87
2.21
29.21
25.00
1.91
24.79
19.79
3.33
23.28
18.67
3.16
4.97
1.33
3.40
15.42
10.67
25.27
2.49
30.69
30.76 35.49 2.81
3.90
35.67
28.57

2.41
0.77
1.14
6.24
0.99
4.59
5.15
1.07
2.87
25.34
2.08
21.87
29.82
1.30
25.52
25.88
3.60
22.91
25.28
3.64
18.66
4.60
7.06
1.33
15.11 4.25
9.33
27.47
1.91
31.57
31.86 35.71 2.23
3.53
34.90
28.57

114

tween the parameters of the IFGP algorithm and the classiﬁcation error for
the considered test problems.

115

Chapter 8

Multi Solution Linear Genetic
Programming

A new Linear Genetic Programming [9] variant called Multi-Solution Linear
Genetic Programming (MS-LGP) is described. Each MS-LGP chromosome
encodes multiple solutions of the problem being solved. The best of these
solutions is used for ﬁtness assignment purposes.

This chapter is entirely original and it is based on the papers [74, 78].

8.1 MS-LGP representation and ﬁtness as-

signment process

MS-LGP enrichs LGP structure in two ways:

• Each destination variable is allowed to represent the output of the
program. In the standard LGP only one variable is chosen to provide
the output.

• The program output is checked after each instruction in chromosome.
Note that within the standard LGP the output is checked after the
execution of all instructions in a chromosome.

After each instruction, the value stored in the destination variable is
considered as a potential solution of the problem. The best value stored in
one of the destination variables is considered for ﬁtness assignment purposes.

116

Example

Consider the chromosome C given below:

void LGP(double r [8])
{

r[5] = r[3] * r[2];
r[3] = r[1] + 6;
r[0] = r[4] * r[7];
r[6] = r[4] – r[1];
r[1] = r[6] * 7;
r[0] = r[0] + r[4];
r[2] = r[3] / r[4];

}

Instead of encoding the output of the problem in a single variable (as in
SS-LGP) we allow that each of the destination variables (r[5], r[3], r[0], r[6],
r[1] or r[2]) to store the program output. The best output stored in these
variables will provide the ﬁtness of the chromosome.

For instance, if we want to solve symbolic regression problems, the ﬁtness

of each destination variable r[i] may be computed using the formula:

f (r[i]) =

n
(cid:88)

k=1

|ok,i − wk|,

where ok,i is the result obtained in variable r[i] for the ﬁtness case k, wk is
the targeted result for the ﬁtness case k and n is the number of ﬁtness cases.
For this problem the ﬁtness needs to be minimized.

The ﬁtness of an individual is set to be equal to the lowest ﬁtness of the

destination variables encoded in the chromosome:

f (C) = min

i

f (r[i]).

Thus, we have a Multi-Solution program at two levels:

• First level is given by the possibility that each variable to represent the

output of the program.

• Second level is given by the possibility of checking for the output at

each instruction in the chromosome.

117

Our choice was mainly motivated by the No Free Lunch Theorems for
Search [100, 101]. There is neither practical nor theoretical evidence that
one of the variables employed by the LGP is better than the others. More
than that, Wolpert and McReady [100] proved that we cannot use the search
algorithm’s behavior so far for a particular test function to predict its future
behavior on that function.

The Multi-Solution ability has been tested within other evolutionary
model such as Multi Expression Programming [64] or Inﬁx Form Genetic
Programming [67]. For these methods it has been shown [64] that encoding
multiple solutions in a single chromosome leads to signiﬁcant improvements.

8.2 Numerical experiments

In this section several experiments with SS-LGP and MS-LGP are performed.
For this purpose we use several well-known symbolic regression problems.
The problems used for assessing the performance of the compared algorithms
are:

f1(x) = x4 + x3 + x2 + x.

f2(x) = x6 – 2x4 + x2.

f3(x) = sin(x4 + x2).

f4(x) = sin(x4) + sin(x2).

For each function 20 ﬁtness cases have been randomly generated with a

uniform distribution over the [0, 1] interval.

The general parameters of the LGP algorithms are given in Table 8.1.
The same settings are used for Multi Solution LGP and for Single-Solution
LGP.

For all problems the relationship between the success rate and the chro-
mosome length and the population size is analyzed. The success rate is
computed as the number of successful runs over the total number of runs.

Experiment 1

118

Table 8.1: The parameters of the LGP algorithm for symbolic regression
problems.

Parameter
Number of generations
Crossover probability
Crossover type
Mutations
Function set
Terminal set
Constants
Selection
Algorithm

Value
51
0.9
Uniform
2 / chromosome
F = {+, -, *, /, sin}
Problem inputs + 4 supplementary registers
Not used
Binary Tournament
Steady State

In this experiment the relationship between the success rate and the chro-
mosome length is analyzed. For this experiment the population size was set
to 50 individuals. Other parameters of the LGP algorithms are given in Table
8.1. Results are depicted in Figure 8.1.

Figure 8.1 shows that Multi-Solution LGP signiﬁcantly outperforms Single-
Solution LGP for all the considered test problems and for all the considered
parameter setting. More than that, large chromosomes are better for MS-
LGP than short chromosomes. This is due to the multi-solution ability:
increasing the chromosome length leads to more solutions encoded in the
same individual.

The easiest problem is f1. MS-LGP success rate for this problem is over
90% when the number of instructions in a chromosome is larger than 12. The
most diﬃcult problem is f4. For this problem and with the parameters given
in Table 8.1, the success rate of the MS-LGP algorithm never increases over
47%. However, these results are very good compared to those obtained by
SS-LGP (the success rate never increases over 5%).

Experiment 2

In this experiment the relationship between the success rate and the pop-
ulation size is analyzed. For this experiment the number of instructions in a
LGP chromosome was set to 12. Other parameters for the LGP algorithms

119

Figure 8.1: The relationship between the success rate and the number of
instructions in a chromosome. Results are averaged over 100 runs.

120

are given in Table 8.1. Results are depicted in Figure 8.2.

Figure 8.2 also shows that Multi-Solution LGP performs better than
Single-Solution LGP. Problem f1 is the easiest one and problem f4 is the
most diﬃcult one.

8.3 Conclusions and further work

The ability of encoding multiple solutions in a single chromosome has been
analyzed in this chapter for Linear Genetic Programming. It has been shown
how to eﬃciently decode the considered chromosomes by traversing them only
once.

Numerical experiments have shown that Multi-Solution LGP signiﬁcantly
improves the evolutionary search for all the considered test problems. There
are several reasons for which Multi Solution Programming performs better
than Single Solution Programming:

• MS-LGP chromosomes act like variable-length chromosomes even if
they are stored as ﬁxed-length chromosomes. The variable-length chro-
mosomes are better than ﬁxed-length chromosomes because they can
easily store expressions of various complexities,

• MS-LGP algorithms perform more function evaluations than their SS-
LGP counterparts. However the complexity of decoding individuals is
the same for both MS-LGP and SS-LGP techniques.

The multi-solution ability will be investigated within other evolutionary

models.

121

Figure 8.2: The relationship between the population size and the success
rate. Population size varies between 10 and 100. Results are averaged over
100 runs.

122

Chapter 9

Evolving Evolutionary
Algorithms

Two new models for evolving Evolutionary Algorithms are described in this
chapter. The models are based on Multi Expression Programming and Linear
Genetic Programming. Several Evolutionary Algorithms for function opti-
mization and Traveling Salesman Problem are evolved using the proposed
models.

This chapter is entirely original and it is based on the papers [63, 65].

9.1 Introduction

Evolutionary Algorithms (EAs) [39, 34] are nonconventional tools for solving
diﬃcult real-world problems. They were developed under the pressure gener-
ated by the inability of classical (mathematical) methods to solve some com-
plex real-world problems. Many of these unsolved problems are (or could be
turned into) optimization problems. Solving an optimization problem means
ﬁnding of solutions that maximize or minimize a criteria function [19, 39, 34].
Many EAs were proposed for dealing with optimization problems. Many
solution representations and search operators were proposed and tested within
a wide range of evolutionary models. There are several natural questions that
are to be answered in all of these evolutionary models:

What is the optimal population size?
What is the optimal individual representation?
What are the optimal probabilities for applying speciﬁc genetic operators?

123

What is the optimal number of generations before halting the evolution?
A breakthrough arose in 1995 when Wolpert and McReady unveiled their
work on the No Free Lunch (NFL) theorems [100, 101]. The NFL theorems
state that all of the black-box algorithms perform equally well over the entire
set of optimization problems. A black-box algorithm does not take into
account any information about the problem or the particular instance being
solved.

The magnitudes of the NFL results stroke all of the eﬀorts for develop-
ing a universal black-box optimization algorithm able to solve best all the
optimization problems.

In their attempt to solve problems, men delegated computers to develop
algorithms able to perform certain tasks. The most prominent eﬀort in this
direction is Genetic Programming (GP) [42]. Instead of evolving solutions
for a particular problem instance, GP is mainly intended for discovering com-
puter programs able to solve particular classes of problems. (This statement
is only partially true, since the discovery of computer programs may be also
viewed as a technique for solving a particular problem instance. The fol-
lowing could be an example of a problem: ”Find a computer program that
calculates the sum of the elements of an array of integers.”)

There are many such approaches so far in the GP literature [42, 43, 44].
The evolving of deterministic computer programs able to solve speciﬁc prob-
lems requires a lot of eﬀort.

Instead of evolving deterministic computer programs we evolve a full-
featured evolutionary algorithm (i.e. the output of the main program will be
an EA able to perform a given task). Proposed approach works with EAs at
two levels:

• The ﬁrst (macro) level consists of a steady-state EA [95] which uses
a ﬁxed population size, a ﬁxed mutation probability, a ﬁxed crossover
probability etc.

• The second (micro) level consists of the solution encoded in a chromo-

some from the GA on the ﬁrst level.

We propose two evolutionary models similar to Multi Expression Pro-
gramming (MEP) [64] and Linear Genetic Programming [9]. These models
are very suitable for evolving computer programs that may be easily trans-
lated into an imperative language (like C or Pascal ).

124

9.2 Evolving evolutionary algorithms using Multi

Expression Programming

In this section, Multi Expression Programming is used for evolving Evolu-
tionary Algorithms.

9.2.1 Evolutionary model

In order to use MEP for evolving EAs we have to deﬁne a set of terminal
symbols and a set of function symbols. When we deﬁne these sets we have
to keep in mind that the value stored by a terminal symbol is independent of
other symbols in the chromosome and a function symbol changes the solution
stored in another gene.

An EA usually involves 4 types of genetic operators:

• Initialize - randomly initializes a solution,

• Select - selects the best solution among several already existing solu-

tions

• Crossover - recombines two already existing solutions,

• Mutate - varies an already existing solution.

These operators act as symbols that may appear into an MEP chromo-
some. The only operator that generates a solution independent of the already
existing solutions is the Initialize operator. This operator will constitute the
terminal set. The other operators will be considered function symbols. Thus,
we have:

T = {Initialize},

F = {Select, Crossover, Mutate}.

A MEP chromosome C, storing an evolutionary algorithm is:

1: Initialize
2: Initialize
3: Mutate 1

{Randomly generates a solution.}
{Randomly generates another solution.}
{Mutates the solution stored on position 1}

125

4: Select 1, 3

{Selects the best solution from those}

5: Crossover 2, 4
4}
6: Mutate 4
7: Mutate 5
8: Crossover 2, 6
6}

{stored on positions 1 and 3}

{Recombines the solutions on positions 2 and

{Mutates the solution stored on position 4}
{Mutates the solution stored on position 5}
{Recombines the solutions on positions 2 and

This MEP chromosome encodes multiple evolutionary algorithms. Each
EA is obtained by reading the chromosome bottom up, starting with the
current gene and following the links provided by the function pointers. Thus
we deal with EAs at two diﬀerent levels: a micro level representing the evo-
lutionary algorithm encoded in a MEP chromosome and a macro level GA,
which evolves MEP individuals. The number of genetic operators (initializa-
tions, crossovers, mutations, selections) is not ﬁxed and it may vary between
1 and the MEP chromosome length. These values are automatically discov-
ered by the evolution. The macro level GA execution is bound by the known
rules for GAs (see [34]).

For instance, the chromosome deﬁned above encodes 8 EAs. They are

given in Table 9.1.
Remarks

(i) In our model the Crossover operator always generates a single oﬀspring
from two parents. The crossover operators generating two oﬀspring may
also be designed to ﬁt our evolutionary model.

(ii) The Select operator acts as a binary tournament selection. The best
out of two individuals is always accepted as the selection result.

(iii) The Initialize, Crossover and Mutate operators are problem dependent.

9.2.2 Fitness assignment

We have to compute the quality of each EA encoded in the chromosome in
order to establish the ﬁtness of a MEP individual. For this purpose each
EA encoded in a MEP chromosome is run on the particular problem being
solved.

126

Table 9.1: Evolutionary Algorithms encoded in the MEP chromosome C.

EA1
i1=Initialize

EA3
i1=Initialize
i2=Mutate (i1)

EA5
i1=Initialize
i2=Initialize
i3=Mutate (i1)
i4=Select (i1, i3)
i5=Crossover (i2, i4)
EA7
i1=Initialize
i2=Initialize
i3=Mutate (i1)
i4=Select (i1, i3)
i5=Crossover (i2, i4)
i6 =Mutate (i5)

EA2
i1=Initialize

EA4
i1=Initialize
i2=Mutate (i1)
i3=Select (i1, i2)
EA6
i1=Initialize
i2=Mutate (i1)
i3=Select (i1, i2)
i4=Mutate (i3)

EA8
i1=Initialize
i2=Initialize
i3=Mutate (i1)
i4=Select (i1, i3)
i5=Mutate (i4)
i6=Crossover (i2, i5)

127

Roughly speaking the ﬁtness of a MEP individual is equal to the ﬁtness of
the best solution generated by one of the evolutionary algorithms encoded in
that MEP chromosome. But, since the EAs encoded in a MEP chromosome
use pseudo-random numbers it is likely that successive runs of the same EA
generate completely diﬀerent solutions. This stability problem is handled in
the following manner: each EA encoded in a MEP chromosome is executed
(run) more times and the ﬁtness of a MEP chromosome is the average of
the ﬁtness of the best EA encoded in that chromosome over all runs. In all
of the experiments performed in this section each EA encoded into a MEP
chromosome was run 200 times.

9.2.3 Numerical experiments

In this section, we evolve an EA for function optimization. For training
purposes we use the Griewangk’s function [102].

Griewangk’s test function is deﬁned by the equation 9.1.

f (x) =

1
4000

n
(cid:88)

i=1

x2
i −

n
(cid:89)

i=1

cos

(cid:33)

(cid:32) xi√
i

+ 1.

(9.1)

The domain of deﬁnition is [−500, 500]n. We use n = 5 in this study. The
optimal solution is x0 = (0,. . . ,0) and f (x0) = 0. Griewangk’s test function
has many widespread local minima which are regularly distributed.

An important issue concerns the representation of the solutions evolved
by the EAs encoded in an MEP chromosome and the speciﬁc genetic opera-
tors used for this purpose. The solutions evolved by the EAs encoded in MEP
chromosomes are represented by using real values [34] (i.e. a chromosome of
the second level EA is an array of real values). By initialization, a random
point within the deﬁnition domain is generated. The convex crossover with
α = 1

2 and the Gaussian mutation with σ = 0.5 are used.

Experiment 1

In this experiment we are interested in seeing the way in which the qual-
ity of the best evolved EA improves as the search process advances. MEP
algorithm parameters are given in Table 9.2.

The results of this experiment are depicted in Figure 9.1.
Figure 9.1 clearly shows the eﬀectiveness of our approach. The MEP
technique is able to evolve an EA for solving optimization problems. The

128

Table 9.2: The parameters of the MEP algorithm for Experiment 1.

Parameter
Population size
Code Length
Number of generations
Crossover probability
Crossover type
Mutation
Terminal set
Function set

Value
100
3000 genes
100
0.7
Uniform Crossover
5 mutations per chromosome
F = {Initialize}
F = {Select, Crossover, Mutate}

quality of the best evolved EA is 8.5 at generation 0. That means that the
ﬁtness of the best solution obtained by the best evolved EA is 8.5 (averaged
over 200 runs). This is a good result, knowing that the worst solution over
the deﬁnition domain is about 313. After 100 generations the quality of the
best evolved EA is 3.36.

Experiment 2

We are also interested in seeing how the structure of the best evolved EA

changed during the search process.

The evolution of the number of the genetic operators used by the best

evolved EA is depicted in Figure 9.2.

Figure 9.2 shows that the number of the genetic operators used by the
best EA increases as the search process advances. For instance the averaged
number of Initializations in the best EA from generation 0 is 27, while the
averaged number of Initializations in the best evolved EA (after 100 genera-
tions) is 43. The averaged number of Mutations is small (less than 18) when
compared to the number of occurrences of other genetic operators.

129

Figure 9.1: The ﬁtness of the best individual in the best run and the average
(over 10 runs) of the ﬁtness of the best individual over all runs.

9.3 Evolving evolutionary algorithms with Lin-

ear Genetic Programming

In order to use LGP for evolving EAs we have to modify the structure of an
LGP chromosome and to deﬁne a set of function symbols. This model was
proposed in [63].

9.3.1

Individual representation for evolving EAs

Instead of working with registers, our LGP program will modify an array of
individuals (the population). In what follows we denote by Pop the array of
individuals (the population) which will be modiﬁed by an LGP program.

The set of function symbols will consist of genetic operators that may
appear into an evolutionary algorithm. Usually, there are 3 types of genetic
operators that may appear into an EA. These genetic operators are:

Select - that selects the best solution among several already existing so-

lutions,

Crossover - that recombine two existing solutions,
Mutate - that varies an existing solution.
These operators will act like possible function symbols that may appear
into a LGP chromosome. Thus, each simple C instruction that has appeared
into a standard LGP chromosome will be replaced by a more complex instruc-
tion containing genetic operators. More speciﬁc, in the modiﬁed LGP chro-

130

Figure 9.2: The ﬁtness of the best individual in the best run and the average
(over 10 runs) of the ﬁtness of the best individual over all runs.

mosomes we may have three major types of instructions. These instructions
are:

Pop[k] = Select (Pop[i], Pop[j]); // Select the best individual from those

stored in

// Pop[i] and Pop[j] and keep the result in position k.
Pop[k] = Crossover (Pop[i], Pop[j]); // Crossover the individuals stored in

Pop[i] and Pop[j]

// and keep the result in position k.
Pop[k] = Mutate (Pop[i]); // Mutate the individual stored in
// position i and keep the result in position k.

An LGP chromosome C, storing an evolutionary algorithm is the follow-

ing.

void LGP Program(Chromosome Pop[8]) // a population with 8 individuals
{
...
Pop[0] = Mutate(Pop[5]);
Pop[7] = Select(Pop[3], Pop[6]);
Pop[4] = Mutate(Pop[2]);
Pop[2] = Crossover(Pop[0], Pop[2]);
Pop[6] = Mutate(Pop[1]);
Pop[2] = Select(Pop[4], Pop[3]);

131

Pop[1] = Mutate(Pop[6]);
Pop[3] = Crossover(Pop[5], Pop[1]);
...
}

These statements will be considered as genetic operations that are exe-
cuted during an EA generation. Since our purpose is to evolve a generational
EA we have to add a wrapper loop around the genetic operations that are
executed during an EA generation. More than that, each EA starts with
a random population of individuals. Thus, the LGP program must contain
some instructions that initialize the initial population.
The obtained LGP chromosome is given below:

void LGP Program(Chromosome Pop[8]) // a population consisting of 8

individuals
{
Randomly initialize the population();
for (int k = 0; k ¡ MaxGenerations; k++){ // repeat for a ﬁxed
// number of generations
Pop[0] = Mutate(Pop[5]);
Pop[7] = Select(Pop[3], Pop[6]);
Pop[4] = Mutate(Pop[2]);
Pop[2] = Crossover(Pop[0], Pop[2]);
Pop[6] = Mutate(Pop[1]);
Pop[2] = Select(Pop[4], Pop[3]);
Pop[1] = Mutate(Pop[6]);
Pop[3] = Crossover(Pop[5], Pop[1]);
}
}

Remark

The initialization function and the for cycle will not be aﬀected by the
genetic operators. These parts are kept unchanged during the search process.

132

9.3.2 Fitness assignment

We deal with EAs at two diﬀerent levels: a micro level representing the
evolutionary algorithm encoded into a LGP chromosome and a macro level
GA, which evolves LGP individuals. Macro level GA execution is bounded
by known rules for GAs [9].

For computing the ﬁtness of a LGP individual we have to compute the
quality of the EA encoded in that chromosome. For this purpose the EA
encoded into a LGP chromosome is run on the particular problem being
solved.

Roughly speaking the ﬁtness of a LGP individual is equal to the ﬁtness of
the best solution generated by the evolutionary algorithm encoded into that
LGP chromosome. But, since the EA encoded into a LGP chromosome use
pseudo-random numbers it is very possible as successive runs of the same EA
to generate completely diﬀerent solutions. This stability problem is handled
in a standard manner: the EA encoded into a LGP chromosome is executed
(run) more times (in fact 200 runs are executed in all the experiments per-
formed for evolving EAs for function optimization and 15 runs for evolving
EAs for the TSP) and the ﬁtness of a LGP chromosome is the average of the
ﬁtness of the EA encoded in that chromosome over all runs.

Remark

In the standard LGP one of the registers is chosen as the program output.
In our approach
This register is not changed during the search process.
the register storing the best value (best ﬁtness) is chosen to represent the
chromosome. Thus, each LGP chromosome stores multiple solutions of a
problem in the same manner as Multi Expression Programming [64].

9.3.3 The model used for evolving EAs

For evolving EAs we use the steady state algorithm [95]. For increasing the
generalization ability (e.g. the ability of the evolved EA to yield good results
on new test problems), the problem set has been divided into three sets,
suggestively called training set, validation set and test set (see [86]). In our
experiments the training set consists of a diﬃcult test problem. Validation is
performed using another diﬃcult test problem. The test set consists of other
well-known benchmarking problems.

133

A method called early stopping is used to avoid overﬁtting of the popula-
tion individuals to the particular training examples used [86]. This method
consists of computing the test set performance for that chromosome which
had the minimum validation error during the search process. Using the early
stopping technique will increase the generalization performance [86].

The test set consists of several well-known benchmarking problems [87,

102] used for assessing the performances of the evolutionary algorithms.

9.3.4 Evolving EAs for function optimization

Test functions

Ten test problems f1 − f10 (given in Table 9.3) are used to asses the per-
formance of the evolved EA. Functions f1 − f6 are unimodal test function.
Functions f7 − f10 are highly multimodal (the number of local minima in-
creases exponentially with the problem dimension [102]).

Experimental results

In this section we evolve an EA for function optimization and then we asses
the performance of the evolved EA. A comparison with a standard GA is
performed later in this section.

For evolving an EA we use f2 as the training problem and the function

f3 as the validation problem.

An important issue regards the solutions evolved by the EAs encoded into
a LGP chromosome and the speciﬁc genetic operators used for this purpose.
Solutions evolved by the EA encoded into LGP chromosomes are represented
using real values [34]. By initialization, a point within the deﬁnition domain
is randomly generated. Convex crossover with α = 1/2 and Gaussian mutation
with σ = 0.5 are used (for more information on real encoding and speciﬁc
operators see [19]).

Experiment 1

In this experiment, an EA for function optimization is evolved.
There is a wide range of EAs that can be evolved using the technique
described above. Since, the evolved EA has to be compared with another

134

Table 9.3: Test functions used in our experimental study. The parameter n
is the space dimension (n = 5 in our numerical experiments) and fmin is the
minimum value of the function.

Test function

f1(x) =

n
(cid:80)
i=1

(i · x2

f2(x) = 10 · n +

i ).
n
(cid:80)
i=1

(cid:115) n(cid:80)

(x2

i − 10 · cos(2 · π · xi))

[-5, 5]n

f3(x) = −a · e−b

x2
i

i=1

n − e

(cid:80) cos(c·xi)

n

+ a + e.

Domain
[-5, 5]n

fmin
0

0

0

0

-n •
418.9829

0

0

0

0

0

[-32, 32]n
a = 20, b =
0.2, c = 2π.
[-500, 500]n

[-500, 500]n

[-100, 100]n

[-10, 10]n

[-100, 100]n

[-100, 100]n

[-30, 30]n

f4(x) = 1
4000 ·
n
(cid:80)
i=1

f5(x) =

n
(cid:80)
i=1

x2
i −

n
(cid:81)
i=1
(cid:113)

(−xi · sin(

|xi|))

cos( xi√
i

) + 1.

n
(cid:80)
i=1
n
(cid:80)
i=1
n
(cid:80)
i=1

f6(x) =

x2
i .

n
(cid:81)
i=1
(cid:33)
.

|xi|.

f8(x) =

f7(x) =

|xi| +
(cid:32) i
(cid:80)
j=1
f9(x) = maxi{xi, 1 ≤ i ≤ n}.
100 · (xi+1 − x2

f10(x) =

x2
j

n−1
(cid:80)
i=1

i )2 + (1 − xi)2.

135

algorithm (such as a standard GA or an ES), the parameters of the evolved
EA have to be similar to the parameters of the algorithm used for comparison.
For instance, a standard GA uses a primary population of N individuals
and an additional population (the new population) that stores the oﬀspring
obtained by crossover and mutation. Thus, the memory requirements for a
standard GA is O(2*N ). In each generation there will be 2 * N Selections,
N Crossovers and N Mutations (we assume here that only one oﬀspring is
obtained by crossover of two parents). Thus, the number of genetic opera-
tors (Crossovers, Mutations and Selections) in a standard GA is 4 * N . We
do not take into account the complexity of the genetic operators, since in
most of the cases this complexity is diﬀerent from operator to operator. The
standard GA algorithm is given below:

Standard GA algorithm

S1. Randomly create the initial population P(0)
S2. for t = 1 to Max Generations do
S3. P’(t) = φ;
S4. for k = 1 to |P(t)| do
S5. p1 = Select(P(t)); // select an individual from the mating pool
S6. p2 = Select(P(t)); // select the second individual
S7. Crossover (p1, p2, oﬀsp); // crossover the parents p1 and p2
// the oﬀspring oﬀspr is obtained
S8. Mutation (oﬀspr ); // mutate the oﬀspring oﬀspr
S9. Add oﬀspf to P’(t);
S10. endfor
S11. P(t+1) = P’(t);
S12. endfor

Rewritten as an LGP program, the Standard GA is given below. The
individuals of the standard (main) population are indexed from 0 to PopSize
– 1 and the individuals of the new population are indexed from PopSize up
to 2 * PopSize – 1.

void LGP Program(Chromosome Pop[2 * PopSize])
//an array containing of 2 * PopSize individuals
{

136

Randomly initialize the population();
for (int k = 0; k ¡ MaxGenerations; k++){ // repeat for a ﬁxed
// number of generations
// create the new population
p1 = Select(Pop[3], Pop[6]);
p2 = Select(Pop[7], Pop[7]);
o = Crossover (p1, p2);
Pop[PopSize] = Mutate(o);
p1 = Select(Pop[3], Pop[6]);
p2 = Select(Pop[7], Pop[7]);
o = Crossover (p1, p2);
Pop[PopSize + 1] = Mutate(o);
p1 = Select(Pop[3], Pop[6]);
p2 = Select(Pop[7], Pop[7]);
o = Crossover (p1, p2);
Pop[PopSize + 2] = Mutate(o);
...
p1 = Select(Pop[3], Pop[6]);
p2 = Select(Pop[7], Pop[7]);
o = Crossover (p1, p2);
Pop[2 * PopSize - 1] = Mutate(o);
// pop(t + 1) = new pop (t)
// copy the individuals from new pop to the next population
Pop[0] = Pop[PopSize];
Pop[1] = Pop[PopSize];
Pop[2] = Pop[PopSize];
...
Pop[PopSize - 1] = Pop[2 * PopSize - 1];
}
}

The parameters of the standard GA are given in Table 9.4.
We will evolve an EA that uses the same memory requirements and the

same number of genetic operations as the standard GA described above.

Remark

137

Table 9.4: The parameters of a standard GA for Experiment 1.

Parameter
Population size

Individual encoding
Number of generations
Crossover probability
Crossover type
Mutation
Selection

Value
20 (+ 20 individuals in the new
pop)
Real
100
1
Convex Crossover with α = 0.5
Gaussian mutation with σ = 0.01
Binary Tournament

Our comparison is based on the memory requirements (i.e. the population
size) and the number of genetic operators used during the search process. A
better comparison could be made if we take into account only the number
of function evaluations performed during the search. Unfortunately, this
comparison cannot be realized in our model since we cannot control the
number of function evaluations (this number is decided by the evolution).
The total number of genetic operators (crossovers + mutations + selections)
is the only parameter that can be controlled in our model.

The parameters of the LGP algorithm are given in Table 9.5.

Table 9.5: The parameters of the LGP algorithm used for Experiment 1.

Parameter
Population size
Code Length
Number of generations
Crossover probability
Crossover type
Mutation
Function set

Value
500
80 instructions
100
0.7
Uniform Crossover
5 mutations per chromosome
F = {Select, Crossover, Mutate}

The parameters of the evolved EA are given in Table 9.6.
The results of this experiment are depicted in Figure 9.3.

138

Table 9.6: The parameters of the evolved EA for function optimization.

Parameter
Population size
Number of generations
Crossover probability
Crossover type
Mutation
Selection

Value
40
100
1
Convex Crossover with α = 0.5
Gaussian mutation with σ = 0.01
Binary Tournament

Figure 9.3: The relationship between the ﬁtness of the best individual in
each generation and the number of generations. Results are averaged over
10 runs.

139

Figure 9.3 shows the eﬀectiveness of our approach. LGP technique is
able to evolve an EA for solving optimization problems. The quality of the
evolved EA improves as the search process advances.

Experiment 2

In this experiment we compare the evolved EA to the standard Genetic
Algorithm described in Experiment 1. The parameters used by the evolved
EA are given in Table 9.6 and the parameters used by the standard GA are
given in Table 9.4. The results of the comparison are given in Table 9.7.

Table 9.7: The results of applying the Evolved EA and the Standard GA
for the considered test functions. StdDev stands for the standard deviation.
The results are averaged over 30 runs.

Test function

Evolved EA
40 individuals

f1
f2
f3
f4
f5
f6
f7
f8
f9
f10

Mean
0.6152
2.6016
7.5945
2.4639
-552.0043
273.7000
2.0521
340.2770
10.3317
10123.3083

StdDev
0.8406
1.7073
2.5006
1.6389
218.8526
235.7794
1.1694
348.3748
4.2009
18645.7247

Standard GA
20 individuals
in the
standard population +
in the
20 individuals
new population
Mean
3.1636
5.8268
10.8979
6.0176
-288.3484
817.1237
4.8836
639.2252
20.6574
208900.5717 444827.6967

StdDev
3.7997
3.9453
2.7603
4.4822
200.5584
699.2686
2.4269
486.7850
8.8268

From Table 9.7 it can be seen that the Evolved EA signiﬁcantly outper-

forms the standard GA on all of the considered test problems.

To avoid any suspicion regarding the Evolved EA we will compare it with
a GA that uses the same standard population size as the Evolved EA. Thus,
the standard GA will use a double population (a standard population and a

140

new population) vis-`a-vis the population employed by the Evolved EA. Note
that this will provide a signiﬁcant advantage of the standard GA over the
Evolved EA. However, we use this larger population because, in this case,
both algorithms (the Standard GA and the Evolved EA) have one important
parameter in common: they perform the same number of initializations.
Results are presented in Table 9.8.

Table 9.8: The results of applying the Evolved EA and the Standard GA
for the considered test functions. StdDev stands for the standard deviation.
Results are averaged over 30 runs.

Test function

Evolved EA
40 individuals

f1
f2
f3
f4
f5
f6
f7
f8
f9
f10

Mean
0.6152
2.6016
7.5945
2.4639
-552.0043
273.7000
2.0521
340.2770
10.3317
10123.3083

StdDev
0.8406
1.7073
2.5006
1.6389
218.8526
235.7794
1.1694
348.3748
4.2009
18645.7247

Standard GA
40 individuals in
the standard pop-
ulation + 40 indi-
viduals in the new
population
Mean
0.2978
3.4031
6.2529
2.4669
-287.0752
263.6049
2.0366
285.9284
10.3776
9102.8337

StdDev
0.5221
2.7188
2.8255
1.5651
156.5294
239.6022
1.5072
254.9170
5.9560
23981.1050

Form Table 9.8 it can be seen that the Evolved EA is better than the
standard GA in 4 cases (out of 10). However, in this case the standard GA
has a considerable advantage over the Evolved EA.

Experiment 3

We are also interested to analyze the relationship between the number of
generations of the evolved EA and the quality of the solutions obtained by

141

applying the evolved EA for the considered test functions. The parameters
of the Evolved EA (EEA) are given in Table 9.6 and the parameters of the
Standard GA (SGA) are given in Table 9.4.

The results of this experiment are depicted in Figure 9.4 (for the unimodal

test functions) and in Figure 9.5 (for the multimodal test functions).

Figures 9.4 and 9.5 show that the Evolved EA is scalable in comparison
with the number of generations. It also can be sent that the Evolved EA
outperforms the standard GA for all the considered number of generations.
For the unimodal test functions (f1−f6) we can see a continuous improvement
tendency during the search process. For the multimodal test functions we
can see a similar behavior only for the test function f10.

9.3.5 Evolving EAs for TSP

In this section, an Evolutionary Algorithm for solving the Traveling Salesman
Problem [31, 53, 46] is evolved. First of all, an EA is evolved and its perfor-
mance is assessed by running it on several well-known instances in TSPLIB
[87].

This section is entirely original and it is based on the papers [70].
Experiment 5

In this experiment, an EA for the TSP problem is evolved.
A TSP path will be represented as a permutation of cities [51] and it is
initialized by using the Nearest Neighbor heuristic [15, 31]. Genetic operators
used by the Evolved EA are DPX as crossover [52] and 2-Exchange [46] as
mutation. These operators are brieﬂy described in what follows.

DPX recombination operator copies into oﬀspring all the common edges
of the parents. Then it completes the oﬀspring to achieve a valid tour with
links that do not belong to the parents, in such way that the distance between
parents in the newly created oﬀspring is preserved. This completion may be
done by using nearest neighbor information [52].

Mutation is done by applying 2-Exchange operator. The 2-Exchange
operator breaks the tour by 2 edges and then rebuilds the path by adding 2
new edges (see [46]).

The parameters used by the LGP algorithm are given in Table 9.9.
The parameters of the Evolved EA are given in Table 9.10.

142

Figure 9.4: The relationship between the number of generations and the
143
quality of the solutions obtained by the Evolved EA (EEA) and by the Stan-
dard GA (SGA) for the unimodal test functions f1 − f6. The number of
generations varies between 20 and 600. Results are averaged over 30 runs.

Figure 9.5: The relationship between the number of generations and the
quality of the solutions obtained by the Evolved EA (EEA) and by the Stan-
dard GA (SGA) for the multimodal test functions f7 − f10. The number of
generations varies between 20 and 600. Results are averaged over 30 runs.

144

Table 9.9: The parameters of the LGP algorithm used for Experiment 5.

Parameter
Population size
Code Length
Number of generations
Crossover probability
Crossover type
Mutation
Function set

Value
500
80 instructions
50
0.7
Uniform Crossover
5 mutations per chromosome
F = {Select, Crossover, Mutate}

Table 9.10: The parameters of the evolved EA for TSP.

Parameter
Population size
Number of generations
Crossover probability
Crossover type
Mutation
Selection

Value
40
100
1
DPX
2-Exchange
Binary Tournament

145

For training and testing stages of our algorithm we use several problems
from the TSPLIB [86]. The att48 problem (containing 48 nodes) is used for
training purposes and the berlin52 problem (containing 52 nodes) is used for
validation purposes. Other 25 well-known TSP instances are used as the test
set.

Five runs for evolving EAs were performed. A run took about one day
on a PIII -600 MHz computer. In each run an EA yielding very good per-
formance has been evolved. One of these EAs has been tested against other
25 diﬃcult instances from TSPLIB. The results of the Evolved EA along
with the results obtained with the standard GA described in section 9.3.4
are given in Table 9.11. The standard GA uses a standard population of
40 individuals and an additional population (the new population) with 40
individuals.

Table 9.11 shows that the Evolved EA performs better than the standard
GA for all the considered test problems. The diﬀerence ∆ ranges from 0.97
% (for the problem bier127 ) up to 20.67 % (for the problem lin105 ).

One can see that the standard GA performs very poor compared to other
implementations found in literature [52, 46]. This is due to the weak (non-
elitist) evolutionary scheme employed in this experiment. The performance
of the GA can be improved by preserving the best individual found so far.
However, this is beyond the purpose of this research. Our main aim was to
evolve an Evolutionary Algorithm and then to compare it with some similar
(in terms of number of genetic operations performed) EA structures.

Experiment 6

In this experiment we use the EA (evolved for function optimization) to
solve TSP instances. This transmutation is always possible since the evolved
EA does not store any information about the problem being solved. Results
are given in Table 9.12.

From Tables 9.11 and 9.12 it can be seen that the EA evolved for function
optimization performs better than the standard GA but worse than the EA
evolved for TSP. These results suggest that the structure of an evolutionary
algorithm might depend on the problem being solved. This observation is in
full concordance with the NFL theorems which tell us that we cannot obtain
”the best” EA unless we embed some information about the problem being
solved.

146

Table 9.11: The results of the standard GA and Evolved EA for 27 instances
from TSPLIB. Mean stands for the mean over all runs and StdDev stands
for the standard deviation. The diﬀerence ∆ is in percent and it is computed
considering the values of the Evolved EA as a baseline. Results are averaged
over 30 runs.

Problem

a280
att48
berlin52
bier127
ch130
ch150
d198
d493
d657
eil101
eil51
eil76
ﬂ417
gil262
kroA100
kroA150
kroA200
kroB100
kroB150
kroC100
kroD100
kroE100
lin105
lin318
p654
pcb442
pr107

Standard GA
40 individuals in the
standard
population
+ 40 individuals in
the new population
Mean
3291.18
39512.65
8872.41
129859.76
7531.64
8087.08
18592.67
42846.82
62348.86
806.03
510.81
677.55
15287.26
2952.11
25938.18
33510.69
35896.96
27259.50
32602.75
25990.92
26454.58
27126.75
19998.93
53525.55
45830.71
60528.60
48438.22

StdDev
39.25
883.47
145.00
854.03
116.92
181.14
291.75
686.30
364.76
17.61
4.41
26.34
159.86
67.68
650.96
445.14
295.57
1295.30
590.64
453.61
864.43
667.92
339.39
976.88
384.92
294.78
476.81

147

Evolved EA
40 individuals

∆

Mean
3066.72
36464.63
8054.99
128603.46
6818.78
7019.56
17171.83
40184.86
58421.90
734.62
470.64
599.71
14444.20
2746.59
23916.58
30650.92
34150.88
23912.50
29811.95
22263.00
24454.33
24295.64
16573.09
49778.67
41697.25
59188.30
46158.41

StdDev
51.67
780.86
128.36
1058.51
142.05
140.01
254.37
544.35
740.38
8.60
12.19
11.46
268.73
53.71
529.01
558.66
814.83
346.77
519.03
585.83
383.18
517.73
528.26
768.35
1356.95
677.04
268.34

7.31
8.35
10.14
0.97
10.45
15.20
8.27
6.62
6.72
9.72
8.53
12.97
5.83
7.48
8.45
9.33
5.11
13.99
9.36
16.74
8.17
11.65
20.67
7.52
9.91
2.26
4.93

Table 9.12: The results of the Evolved EA for function optimization (see
Appendix 1) and Evolved EA for TSP (see Appendix 2) for 27 instances
from TSPLIB. Mean stands for the mean over all runs and StdDev stands
for the standard deviation. The diﬀerence ∆ is in percent and it is computed
considering the values of the Evolved EA for TSP as a baseline. Results are
averaged over 30 runs.

Problem

a280
att48
berlin52
bier127
ch130
ch150
d198
d493
d657
eil101
eil51
eil76
ﬂ417
gil262
kroA100
kroA150
kroA200
kroB100
kroB150
kroC100
kroD100
kroE100
lin105
lin318
p654
pcb442
pr107

Evolved EA for func-
tion optimization
40 individuals
Mean
3156.39
39286.76
8389.56
131069.06
7221.11
7094.49
18001.13
41837.06
60844.49
742.77
506.59
615.68
15284.92
2923.88
25273.13
31938.69
35015.95
25919.43
31822.17
23588.58
25028.15
25061.34
16977.1
50008.23
43689.11
59825.62
47718.90

StdDev
8.54
192.38
316.86
1959.86
30.51
13.57
81.42
492.34
336.61
5.92
2.95
6.69
64.54
27.38
550.62
384.85
377.26
90.71
403.77
107.12
171.43
200.77
49.02
413.72
228.15
292.70
113.56

148

Evolved EA for TSP
40 individuals

∆

Mean
3066.72
36464.63
8054.99
128603.46
6818.78
7019.56
17171.83
40184.86
58421.90
734.62
470.64
599.71
14444.20
2746.59
23916.58
30650.92
34150.88
23912.50
29811.95
22263.00
24454.33
24295.64
16573.09
49778.67
41697.25
59188.30
46158.41

StdDev
51.67
780.86
128.36
1058.51
142.05
140.01
254.37
544.35
740.38
8.60
12.19
11.46
268.73
53.71
529.01
558.66
814.83
346.77
519.03
585.83
383.18
517.73
528.26
768.35
1356.95
677.04
268.34

2.92
7.73
4.15
1.91
5.90
1.06
4.82
4.11
4.14
1.10
7.63
2.66
5.82
6.45
5.67
4.20
2.53
8.39
6.74
5.95
2.34
3.15
2.43
0.46
4.77
1.07
3.38

9.4 Conclusions

In this chapter, LGP and MEP have been used for evolving Evolutionary
Algorithms. A detailed description of the proposed approaches has been
given allowing researchers to apply the method for evolving Evolutionary
Algorithms that could be used for solving problems in their ﬁelds of interest.
The proposed model has been used for evolving Evolutionary Algorithms
for function optimization and the Traveling Salesman Problem. Numerical
experiments emphasize the robustness and the eﬃcacy of this approach. The
evolved Evolutionary Algorithms perform similar and sometimes even better
than some standard approaches in the literature.

149

Chapter 10

Searching for a Practical
Evidence of the No Free Lunch
Theorems

A framework for constructing test functions that match a given algorithm
is developed in this chapter. More speciﬁc, given two algorithms A and B,
the question is which the functions for which A performs better than B (and
vice-versa) are. For obtaining such functions we will use an evolutionary
approach: the functions matched to a given algorithm are evolved by using
the Genetic Programming (GP) [42] technique.

This chapter is entirely original and it is based on the paper [68, 69].

10.1

Introduction

Since the advent of No Free Lunch (NFL) theorems in 1995 [100], the trends
of Evolutionary Computation (EC) [34] have not changed at all, although
these breakthrough theories should have produced dramatic changes. Most
researchers chose to ignore NFL theorems: they developed new algorithms
that work better than the old ones on some particular test problems. The
researchers have eventually added: ”The algorithm X performs better than
another algorithm on the considered test functions”. That is somehow useless
since the proposed algorithms cannot be the best on all the considered test
functions. Moreover, most of the functions employed for testing algorithms
are artiﬁcially constructed.

150

Consider for instance, the ﬁeld of evolutionary single-criteria optimization
where most of the algorithms were tested and compared on some artiﬁcially
constructed test functions (most of them being known as De’Jong test prob-
lems) [34, 102]. These test problems were used for comparison purposes
before the birth of the NFL theorems and they are used even today (8 years
later after the birth of the NFL theorems). Evolutionary multi-criteria op-
timization was treated in a similar manner: most of the recent algorithms
in this ﬁeld were tested on several artiﬁcially constructed test functions pro-
posed by K. Deb in [17].

Roughly speaking, the NFL theorems state that all the black-box opti-
mization algorithms perform equally well over the entire set of optimization
problems. Thus, if an algorithm A is better than another algorithm B on
some classes of functions, the algorithm B is better than A on the rest of the
functions.

As a consequence of the NFL theories, even a computer program (imple-
menting an Evolutionary Algorithm (EA)) containing programming errors
can perform better than some other highly tuned algorithms for some test
functions.

Random search (RS) being a black box search / optimization algorithm
should perform better than all of the other algorithms for some classes of test
functions. Even if this statement is true, there is no result reported in the
specialized literature of a test function for which RS performs better than all
the other algorithms (taking into account the NFL restriction concerning the
number of distinct solutions visited during the search). However, a function
which is hard for all Evolutionary Algorithms is presented in [18].

Three questions (on how we match problems to algorithms) are of high

interest:

For a given class of problems, which is (are) the algorithm(s) that per-

forms (perform) better than all other algorithms?

For a given algorithm which is (are) the class(es) of problems for which

the algorithm performs best?

Given two algorithms A and B, which is (are) the class (es) of problems

for which A performs better than B?

Answering these questions is not an easy task. All these problems are
still open questions and they probably lie in the class of the NP-Complete
problems. If this assumption is true it means that we do not know if we are
able to construct a polynomial algorithm that takes a function as input and
outputs the best optimization algorithm for that function (and vice versa).

151

Fortunately, we can try to develop a heuristic algorithm able to handle this
problem.

10.2 Basics on No Free Lunch Theorems

The results provided by the No Free Lunch Theorems are divided in two
main classes: No Free Lunch Theorems for Oprimization and No Free Lunch
Theorems for Search.

Roughly speaking, the NFL theorems for Optimization [101] state that all
the black-box optimization algorithms perform equally well over the entire
set of optimization problems.

Thus, if an algorithm A is better than another algorithm B on some
classes of functions, the algorithm B is better than A on the rest of the
functions.

The NFL theorems for Search [100] state that we cannot use the informa-
tion about the algorithm’ behaviour so far to predict it’s future behaviour.

10.3 A NFL-style algorithm

Firstly, we deﬁne a black-box optimization algorithm as indicated by Wolpert
and McReady in [100].

The evolutionary model (the NFL-style algorithm) employed in this study
uses a population consisting of a single individual. This considerably simplify
the description and the implementation of a NFL-style algorithm. No archive
for storing the best solutions found so far (see for instance Pareto Archived
Evolution Strategy [45]) is maintained. However, we implicitly maintain an
archive containing all the distinct solutions explored until the current state.
We do so because only the number of distinct solutions is counted in the
NFL theories. This kind of archive is also employed by Tabu Search [32, 33].
The variables and the parameters used by a NFL algorithm are given in

Table 10.1.

The NFL-style algorithm is outlined below:

NFL Algorithm

S 1. Archive = ∅;
S 2. Randomly initializes the current solution (curr sol)

152

Table 10.1: The variables used by the NFL algorithm.

Variable
Archive

curr sol

new sol

MAX STpng

t

Meaning
the archive storing all distinct solutions
visited by algorithm
the current solution (point in the search
space)
a new solution (obtained either by mu-
tation or by initialization)
the number of generations (the number
of distinct points in the search space
visited by the algorithm).
the number of distinct solutions ex-
plored so far

// add the current solution to the archive
S 3. Archive = Archive + {curr sol};
S 4. t = 1;
S 5. while t < MAX STpng do
S 6. Select a new solution (new sol) in
the neighborhood of the curr sol
S 7. Archive = Archive + {new sol};
S 8. curr sol = new sol;
S 9. t = t + 1;
S 10. endwhile

An important issue concerning the NFL algorithm described above is
related to the step S6 which selects a new solution that does not belong
to the Archive. This is usually done by mutating the current solution and
keeping the oﬀspring if the latter does not already belong to the Archive
(The actual acceptance mechanism is minutely described in section 10.5). If
the oﬀspring belongs to the Archive for a ﬁxed number of mutations (stpng)
it means that the neighborhood of the current solutions could be exhausted
(completely explored). In this case, a new random solution is generated and
the search process moves to another region of the search space.

It is sometimes possible the generated solution to already belong to the

153

Archive. In this case, another random solution is generated over the search
space. We assume that the search space is large enough and after a ﬁnite
number of re-initializations the generated solution will not belong to the
Archive.

The algorithm for selecting a new solution which does not belong to the

Archive (the step S6) is given below:

SS 1. nr mut = 0; // the number of mutations is set to 0
SS 2. Repeat
SS 3. new sol = Mutate (curr sol);
SS 4. nr mut = nr mut + 1;
SS 5. until (nr mut = MAX MUTATIONS) and (new sol /∈Archive) and

Accepted(new sol);

SS 6. while new sol /∈Archive do
SS 7.

Initialize(new sol); //we jump into another randomly chosen point of

the search space

SS 8. endwhile

10.4 Evolutionary Model and the Fitness As-

signment Process

Our aim is to ﬁnd a test function for which a given algorithm A performs
better than another given algorithm B. The test function that is being
searched for will be evolved by using Genetic Programming [42] with steady
state [95].

The quality of the test function encoded in a GP chromosome is computed
in a standard manner. The given algorithms A and B are applied to the test
function. These algorithms will try to optimize (ﬁnd the minimal value of)
that test function. To avoid the lucky guesses of the optimal point, each
algorithm is run 500 times and the results are averaged. Then, the ﬁtness of
a GP chromosome is computed as the diﬀerence between the averaged results
of the algorithm A and the averaged results of the algorithm B. In the case
of function minimization, a negative ﬁtness of a GP chromosome means that
the algorithm A performs better than the algorithm B (the values obtained
by A are smaller (on average) than those obtained by B).

154

10.5 Algorithms used for comparison

We describe several evolutionary algorithms used for comparison purposes.
All the algorithms described in this section are embedded in the NFL algo-
rithm described in section 10.3. More precisely, the considered algorithms
particularize the solution representation, the mutation operator, and the
acceptance mechanism (the procedure Accepted ) of the NFL algorithm de-
scribed in section 10.3. The mutation operator is the only search operator
used for exploring the neighborhood of a point in the search space.

A1 - real encoding (the individuals are represented as real numbers using
32 bits), Gaussian mutation with σ1 = 0.001, the parent and the oﬀspring
compete for survival.

A2 - real encoding (the individuals are represented as real numbers using
32 bits), Gaussian mutation with σ2 = 0.01, the parent and the oﬀspring
compete for survival.

A3 - binary encoding (the individuals are represented as binary strings of
32 bits), point mutation with pm = 0.3, the parent and the oﬀspring compete
for survival.

A4 - binary encoding (the individuals are represented as binary strings of
32 bits), point mutation with pm = 0.1, the parent and the oﬀspring compete
for survival.

10.6 Numerical experiments

Several numerical experiments for evolving functions matched to a given
algorithm are performed in this section. The algorithms used for comparison
have been described in section 10.5.

The number of dimensions of the space is set to 1 (i.e. one-dimensional

functions) and the deﬁnition domain of the evolved test functions is [0, 1].

The parameters of the GP algorithm are given in Table 10.2.
The small number of generations (only 10) has been proved to be suﬃcient

for the experiments performed in this study.

Evolved functions are given in Table 10.3. For each pair (Ak, Aj) is given

155

Table 10.2: The parameters of the GP algorithm used for numerical experi-
ments

Parameter
Population size
Number of generations
Maximal GP tree depth
Function set
Terminal set
Crossover probability
Mutation
Runs

Value
50
10
6
F = {+, -, *, sin, exp}
T = {x}
0.9
1 mutation / chromosome
30

the evolved test function for which the algorithm Ak performs better than
the algorithm Aj. The mean of the ﬁtness of the best GP individual over 30
runs is also reported.

Table 10.3: The evolved test functions.

Algorithms Evolved Test Func-

Averaged ﬁtness

(A1, A2)
(A2, A1)
(A3, A4)
(A4, A3)
(A2, A4)
(A4, A2)

tion
f1(x) = 0.
f2(x) = −6x3 − x.
f3(x) = x − 2x5.
f4(x) = −4x8.
f5(x) = 0.
f6(x) = −6x3 − x.

0
-806.03
-58.22
-34.09
0
-1601.36

Table 10.3 shows that the proposed approach made possible the evolving
of test functions matched to the most of the given algorithms. The results of
these experiments give a ﬁrst impression of how diﬃcult the problems are.
Several interesting observations can be made:

The GP algorithm was able to evolve a function for which the algorithm
A2 (real encoding with σ = 0.01) was better then the algorithm A1 (real
encoding with σ = 0.001) in all the runs (30). However, the GP algorithm

156

was not able to evolve a test function for which the algorithm A1 is better
that the algorithm A2.
In this case the function f (x) = 0 (where both
algorithms perform the same) was the only one to be found. It seems to be
easier to ﬁnd a function for which an algorithm with larger ”jumps” is better
than an algorithm with smaller ”jumps” than to ﬁnd a function for which
an algorithm with smaller ”jumps” is better than an algorithm with larger
”jumps”.

A test function for which the algorithm A4 (binary encoding) is better
than the algorithm A2 (real encoding) was easy to ﬁnd. The reverse (i.e.
a test function for which the real encoding algorithm A2 is better than the
binary encoded algorithm A4) has not been found by using the GP parameters
considered in Table 10.2.

10.7 Conclusions and further work

In this paper, a framework for evolving test functions that are matched to
a given algorithm has been proposed. The proposed framework is intended
to provide a practical evidence for the NFL theories. Numerical experiments
have shown the eﬃcacy of the proposed approach.

Further research will be focused on the following directions:

(i) extending the function set (F ) and the number of space dimensions.

(ii) comparing other evolutionary algorithms for single and multiobjective
optimization. Several test functions matched to some classical algo-
rithms (such as standard GAs or ES) for function optimization will
be evolved. In this case the problem is much more diﬃcult since the
number of distinct solutions visited during the search process could be
diﬀerent for each algorithm.

(iii) evolving test instances for algorithms used for solving other real-world
problems (such as TSP, symbolic regression, classiﬁcation etc).

(iv) ﬁnding test functions for which random search is better than other

algorithms.

(v) ﬁnding the set of test functions for which an algorithm is better than

the other.

157

Chapter 11

Conclusions and further work

Three new techniques have been proposed during the course of this the-
sis: Multi Expression Programming, Inﬁx Form Genetic Programming, and
Traceless Genetic Programming.

Proposed techniques have been used in the following applications: sym-
bolic regression, classiﬁcation, designing digital circuits, evolving play strate-
gies, evolving heuristics for NP-complete problems, evolving evolutionary al-
gorithms, and evolving test problems.

Further work will be focused on developing new GP techniques, Designing
digital circuits for reversible computers, evolving fast winning strategies for
the end of the chess and othello games, evolving EAs with patterns, evolving
heuristics for other NP-complete problems, applying the existing GP tech-
niques to real-world prediction data, etc.

158

Bibliography

[1] Aarts, E.H.L., Lenstra, J. K. (Editors), Local Search in Combinatorial

Optimization, John Wiley and Sons, London, 1997.

[2] Aho, A., Sethi R., Ullman J., Compilers: Principles, Techniques, and

Tools, Addison Wesley, 1986.

[3] Angeline, P.J., Two Self-Adaptive Crossover Operators for Genetic
Programming, In Advances in Genetic Programming II, Edited by An-
geline, P.J. and Kinnear, K.E., pp. 89-110, MIT Press, 1996.

[4] Banzhaf, W., Nordin, P., Keller, E. R., Francone, F. D., Genetic Pro-
gramming - An Introduction, Morgan Kaufmann, San Francisco, CA,
1998.

[5] Back, T., Evolutionary Algorithms in Theory and Practice, Oxford

University Press, Oxford, 1996.

[6] Back, T., Fogel, D.B., Michalewicz, Z. (Eds.), Handbook of Evolution-
ary Computation, Institute of Physics Publishing, Bristol, and Oxford
University Press, New York, 1997.

[7] Bellman, R., Dynamic Programming, Princeton, Princeton University

Press, New Jersey, 1957.

[8] Berlekamp, E. R., Conway J. H., Guy R. K., Winning Ways for Your

Mathematical Plays, Academic Press, London, 1982.

[9] Brameier, M., Banzhaf, W., A Comparison of Linear Genetic Program-
ming and Neural Networks in Medical Data Mining, IEEE Transactions
on Evolutionary Computation, Vol. 5, pp. 17-26, IEEE Press, NY, 2001.

159

[10] Brameier, M., Banzhaf, W., Explicit Control of Diversity and Eﬀec-
tive Variation Distance in Linear Genetic Programming, In Proceed-
ings of the 4th European Conference on Genetic Programming, Edited
by Lutton, E., Foster, J., Miller, J., Ryan, C., and Tettamanzi, A.,
Springer-Berlin, pp. 38-50, 2002.

[11] Brameier, M., Banzhaf, W., Evolving Teams of Predictors with Linear
Genetic Programming, Genetic Programming and Evolvable Machines,
Vol. 2, pp. 381-407, 2001.

[12] Chellapilla, K., Fogel, D. B., Evolution, Neural Networks, Games and

Intelligence, Proceedings of the IEEE, pp. 1471-1496, 2000.

[13] Coello, C., Alba, E., Luque, G., Aguire A., Comparing diﬀerent Se-
rial and Parallel Heuristics to Design Combinational Logic Circuits, In
Proceedings of 2003 NASA/DoD Conference on Evolvable Hardware,
J. Lohn, R. Zebulum, J. Steincamp, D. Keymeulen, A. Stoica, M.I.
Ferguson, pp. 3-12, 2003.

[14] Conway, J. H., On Numbers and Games, Academic Press, London,

1976.

[15] Cormen, T.H., Leiserson, C.E. Rivest, R. R., Introduction to Algo-

rithms, MIT Press, Cambridge, MA, 1990.

[16] Cramer, N.L., A representation for the adaptive generation of sequen-
cial programs, In Proceedings of the Workshop on Genetic Algorithms:
From Theory to Real-World Applications, pp. 183-187, 1985.

[17] Deb, K., Multi-Objective Genetic Algorithms: Problem Diﬃculties and
Construction of Test Functions, Evolutionary Computation, Vol. 7, pp.
205-230 , MIT Press, Cambridge, MA, 1999.

[18] Droste, S., Jansen, T., Wegener, I., A Natural and Simple Function
which is Hard for All Evolutionary Algorithms, technical report, Univ.
Dortmund, Germany, 2000.

[19] Dumitrescu, D., Lazzerini, B., Jain, L., Dumitrescu, A., Evolutionary

Computation. CRC Press, Boca Raton, FL, 2000.

160

[20] Dumitrescu, D., Oltean, M., Proving Theorems using Genetic Pro-
gramming, Studia, Ser. Informatica, Babe¸s-Bolyai University, 1999.

[21] Dumitrescu, D., Gro¸san, C., Oltean, M., Evolving Pareto Continuous
Regions, Evolutionary Computation Based Multi-Criteria Optimiza-
tion: Theoretical Advances and Applications, A. Abraham, L. Jain
and R. Goldberg, (editors), Springer-Verlag, London, 2004.

[22] Edmonds, B., Meta-Genetic Programming: Co-evolving the Operators

of Variation. Electrik, Vol. 9, pp. 13-29, 2001.

[23] Ferreira, C., Gene Expression Programming: a New Adaptive Algo-
rithm for Solving Problems, Complex Systems, Vol. 13, Nr. 1, pp. 87-
129, 2001.

[24] Ferreira, C., Mutation, Transposition, and Recombination: An Analy-
sis of the Evolutionary Dynamics. In H. J. Caulﬁeld, S.-H. Chen, H.-D.
Cheng, R. Duro, V. Honavar, E. E. Kerre, M. Lu, M. G. Romay, T.
K. Shih, D. Ventura, P. P. Wang, Y. Yang, eds., Proceedings of the
6th Joint Conference on Information Sciences, 4th International Work-
shop on Frontiers in Evolutionary Algorithms, pp. 614-617, Research
Triangle Park, North Carolina, USA, 2002.

[25] Ferreira, C., Discovery of the Boolean Functions to the Best Density-
Classiﬁcation Rules Using Gene Expression Programming. In E. Lut-
ton, J. A. Foster, J. Miller, C. Ryan, and A. G. B. Tettamanzi, eds.,
Proceedings of the 4th European Conference on Genetic Programming,
EuroGP 2002, LNCS 2278, pp. 51-60, Springer-Verlag, Berlin, Ger-
many, 2002.

[26] Ferreira, C., 2002. Genetic Representation and Genetic Neutrality in
Gene Expression Programming. Advances in Complex Systems, Vol. 5
Nr. 4 pp. 389-408, 2002.

[27] Fogel, D.B., Evolutionary Computation, IEEE Press, New-York, 1995.

[28] Fraenkel, S., Scenic Trails Ascending from Sea-Level Nim to Alpine
Chess, Games of No Chance, MSRI Publications, Vol. 29, 1996.

161

[29] Freisleben, B. Merz, P., A Genetic Local Search Algorithm for Solv-
ing Symmetric and Asymmetric Traveling Salesman Problems, In Pro-
ceedings of the 1996 IEEE International Conference on Evolutionary
Computation, pp. 616-621, 1996.

[30] Gardner, M., Hexaﬂexagons and Other Mathematical Diversions, The

University of Chicago Press, 1988.

[31] Garey, M.R., Johnson D.S., Computers and Intractability: A Guide to

NP-completeness, Freeman & Co, San Francisco, 1979.

[32] Glover, F., Tabu search - Part I, ORSA Journal of Computing, 1,

(1989), 190-206

[33] Glover, F., Tabu search - Part II, ORSA Journal of Computing, 2,

(1990), 4-32

[34] Goldberg, D.E., Genetic Algorithms in Search, Optimization, and Ma-

chine Learning, Addison-Wesley, Reading, MA, 1989.

[35] Gro¸san, C., Oltean, M., Adaptive Representation for Single Objective
Optimization, First Balkan Conference on Informatics, 17-20 November
2003, Tesalonik, Greece, Y. Manoulopulus (editor), pp. 345-355, 2003.

[36] Gro¸san C., Oltean M., Adaptive Representation for Single Objective
Optimization, Soft Computing, Springer-Verlag, 2004 (Accepted).

[37] Handley, S., On the Use of a Directed Graph to Represent a Population
of Computer Programs. Proceeding of the IEEE World Congress on
Computational Intelligence, pp. 154-159, Orlando, Florida, 1994.

[38] Haykin, S., Neural Networks, a Comprehensive Foundation, second

edition, Prentice-Hall, Englewood Cliﬀs, 1999.

[39] Holland, J.H., Adaptation in Natural and Artiﬁcial Systems, University

of Michigan Press, Ann Arbor, 1975.

[40] Kantschik, W., Banzhaf, W., Linear-Tree GP and its comparison with
other GP structures, In J. Miller, M. Tomassini, P.-L. Lanzi, C. Ryan,
A. Tettamanzi and W. B. Langdon editors, Proceedings of 4th EuroGP
Conference, Springer, pp. 302-312, 2001.

162

[41] Joza, J.R. Hierarchical genetic algorithms operating on populations of
computer programs. In Proceedings of the Eleventh International Joint
Conference on Artiﬁcial Intelligence IJCAI-89, pp. 768-774, Morgan-
Kaufmann, San-Francisco, CA, 1989.

[42] Koza, J. R., Genetic Programming: On the Programming of Comput-
ers by Means of Natural Selection, MIT Press, Cambridge, MA, 1992.

[43] Koza, J. R., Genetic Programming II: Automatic Discovery of Reusable

Subprograms, MIT Press, Cambridge, MA, 1994.

[44] Koza, J. R. et al., Genetic Programming III: Darwinian Invention and
Problem Solving, Morgan Kaufmann, San Francisco, CA, 1999.

[45] Knowles, J. D., Corne, D.W.: Approximating the Nondominated Front
using the Pareto Archived Evolution Strategies, Evolutionary Compu-
tation, Vol. 8, pp. 149-172, MIT Press, Cambridge, MA, 2000.

[46] Krasnogor, N., Studies on the Theory and Design Space of Memetic
Algorithms, PhD Thesis, University of the West of England, Bristol,
2002.

[47] Krasnogor, N., Smith, J.E., A Memetic Algorithm with Self-Adaptive
Local Search: TSP a Case Study, In Proceedings of 2000 Genetic and
Evolutionary Computation Conference, Morgan Kaufmann, 2000.

[48] Langdon, W. B., Poli, R., Genetic Programming Bloat with Dynamic
Fitness. W. Banzhaf, R. Poli, M. Schoenauer, and T. C. Fogarty (ed-
itors), Proceedings of the First European Workshop on Genetic Pro-
gramming, pp. 96-112, Springer-Verlag, Berlin, 1998.

[49] Lones, M.A., Tyrrell, A.M., Biomimetic Representation in Genetic
Programming, Proceedings of the Workshop on Computation in Gene
Expression at the Genetic and Evolutionary Computation Conference
2001 (GECCO2001), July 2001.

[50] McPhee, N. F., Hopper N. J., Analysis of Genetic Diversity through
Population History, Proceedings of the Genetic and Evolutionary Com-
putation Conference, W. Banzhaf, J. Daida, A. E. Eiben, M. H. Gar-
zon, V. Honavar, M. Jakiela, and R. E. Smith, (editor) pp. 1112-1120,
Morgan Kaufmann, 1999.

163

[51] Merz, P., Freisleben, B., A Genetic Local Search Approach to the
Quadratic Assignment Problem, in Proceedings of the 7th International
Conference on Genetic Algorithms, Back T. (Editors), pp. 465–472,
Morgan Kaufmann, 1997.

[52] Merz, P., Freisleben, B., Genetic Local Search for the TSP: New Re-
sults, in Proceedings of the IEEE International Conference on Evolu-
tionary Computation, Back T., Michalewicz Z. and Yao X. (Editors),
pp. 159-164, IEEE Press, 1997.

[53] Merz, P., Freisleben, B., Memetic Algorithms for the Traveling Sales-

man Problem, Complex-Systems, Vol 13, 2003.

[54] Miller, J. F., Thomson, P., Aspects of Digital Evolution: Evolvability
and Architecture. In Proceedings of the Parallel Problem Solving from
Nature V, A. E. Eiben, B¨ack T., Schoenauer M. and Schwefel H-P.
(Editors), pp. 927-936, Springer, 1998.

[55] Miller, J.F., Thomson, P., Cartesian Genetic Programming. In Pro-
ceedings of the 3rd International Conference on Genetic Programming
(EuroGP2000), R. Poli, J.F. Miller, W. Banzhaf, W.B. Langdon, J.F.
Miller, P. Nordin, T.C. Fogarty (Editors), LNCS 1802, Springer-Verlag,
Berlin, pp. 15-17, 2000.

[56] Miller, J. F., Thomson, P., Fogarty, T., Designing Electronic Circuits
using Evolutionary Algorithms. Arithmetic Circuits: A Case Study.
In Genetic Algorithms and Evolution Strategies in Engineering and
Computer Science, Quagliarella D., Periaux J., Poloni C. and G. Winter
(Editors), pp. 105–131, Chechester, UK-Wiley, 1997.

[57] Miller, J. F., An Empirical Study of the Eﬃciency of Learning Boolean
Functions using a Cartesian Genetic Programming Approach. In Pro-
ceedings of the 1st Genetic and Evolutionary Computation Conference,
Banzhaf W., Daida J., Eiben A. E., Garzon M. H., Honavar V., Jakiela
M., and Smith R. E., (editors), Vol. 2, pp. 1135–1142, Morgan Kauf-
mann, San Francisco, CA, 1999.

[58] Miller, J. F., Job, D., Vassilev, V.K., Principles in the Evolutionary
Design of Digital Circuits - Part I, Genetic Programming and Evolvable
Machines, Vol. 1(1), pp. 7–35, Kluwer Academic Publishers, 2000.

164

[59] Mitchell, T., Machine Learning, McGraw-Hill, New-York, 1996.

[60] Moscato, P., On Evolution, Search, Optimization, Genetic Algorithms
and Martial Arts: Towards Memetic Algorithms, Tech. Rep. Caltech
Concurrent Computation Program, Report. 826, California Institute of
Technology, Pasadena, California, USA, 1989.

[61] Moscato, P. Norman, M. G., A Memetic Approach for the Travel-
ing Salesman Problem Implementation of a Computational Ecology
for Combinatorial Optimization on Message-Passing Systems, in Par-
allel Computing and Transputer Applications, Valero M., Onate E.,
Jane M., Larriba J. L., and Suarez B. (editor), pp. 177–186, IOS Press,
Amsterdam, 1992.

[62] Nordin, P., A Compiling Genetic Programming System that Directly
Manipulates the Machine Code, K. E. Kinnear, Jr. (editor), Advances
in Genetic Programming, pp. 311-331, MIT Press, 1994.

[63] Oltean, M., Evolving Evolutionary Algorithms for Function Opti-
mization, in Proceedings of the 8th International Conference on In-
formation Sciences, 26-30 September 2003, North Carolina, K. Chen
(editor), pp. 295-298, 2003.

[64] Oltean, M., Dumitrescu, D., Multi Expression Program-
(available at

report, Babes-Bolyai University,

technical

ming,
www.mep.cs.ubbcluj.ro), 2002.

[65] Oltean, M., Gro¸san C., Evolving Evolutionary Algorithms using
Multi Expression Programming, The 7th European Conference on Ar-
tiﬁcial Life, Dortmund, 14-17 September, W. Banzhaf (editor), LNAI
2801, pp. 651-658, Springer-Verlag, Berlin, 2003.

[66] Oltean, M., Solving Even-parity problems with Multi Expression Pro-
gramming, Te 8th International Conference on Computation Sciences,
26-30 September 2003, North Carolina, K. Chen (editor), pp. 315-318,
2003.

[67] Oltean, M., Gro¸san, C., Solving Classiﬁcation Problems using In-
ﬁx Form Genetic Programming, The 5th International Symposium on
Intelligent Data Analysis, August 28-30, 2003, Berlin, M. Berthold (ed-
itor), LNCS 2810, pp. 242-252, Springer, Berlin, 2003.

165

[68] Oltean, M., A Practical Evidence for the No Free Lunch Theorems,
BioInspired Approaches to Advanced Information Technology, BioA-
DIT’04, Lausanne, Switzerland, 29-31 January, A. Ijspeert et al. (edi-
tors), pp. 482-488, 2004.

[69] Oltean, M., Searching for a Practical Evidence for the No Free Lunch
Theorems, BioInspired Approaches to Advanced Information Technol-
ogy, BioADIT’04, Lausanne, Switzerland, 29-31 January, A. Ijspeert
(editor), LNCS 3141, Springer-Verlag, Berlin, pp. 395-406, 2004.

[70] Oltean, M., Dumitrescu, D., Evolving TSP Heuristics using Multi
Expression Programming, International Conference on Computational
Sciences, ICCS’04, M. Bubak, G. D. van Albada, P. Sloot, and J.
Dongarra (editors), Vol II, pp. 670-673, 6-9 June, Krakow, Poland,
2004.

[71] Oltean, M., Evolving Digital Circuits for the Knapsack Problem, In-
ternational Conference on Computational Sciences, M. Bubak, G. D.
van Albada, P. Sloot, and J. Dongarra (editors), Vol. III, pp. 1257-1264,
6-9 June, Krakow, Poland, 2004.

[72] Oltean, M., Dumitrescu, D., A Permutation based Approach for the
2-D Cutting Stock Problem, First International Industrial Conference
Bionik 2004, Edited by Boblan, I. and Bannasch, R., 22-23 April, Han-
nover, Germany, pp. 73-80, 2004.

[73] Oltean, M., Solving Even-Parity Problems using Traceless Genetic
Programming, IEEE Congress on Evolutionary Computation, Port-
land, 19-23 June, G. Greenwood (editor), pages 1813-1819, IEEE Press,
2004.

[74] Oltean, M., Encoding Multiple Solutions in a Linear GP Chromo-
some, International Conference on Computational Sciences, M. Bubak,
G. D. van Albada, P. Sloot, and J. Dongarra (editors), Vol III, pp.
1281-1288, 6-9 June, Krakow, Poland, 2004.

[75] Oltean, M., Gro¸san, A Comparison of Several Linear Genetic Pro-
gramming Techniques, Complex-Systems, Vol. 14, Nr. 4, pp. 282-311,
2003.

166

[76] Oltean, M., Evolving Winning Strategies for Nim-like Games, World
Computer Congress, Student Forum, 26-29 August, Toulouse, France,
edited by Mohamed Kaaniche, pp. 353-364, Kluwer Academic Pub-
lisher, 2004.

[77] Oltean, M., Solving Classiﬁcation Problems using Traceless Genetic
Programming, World Computer Congress, The Symposium on Profe-
sional Practice in AI, 26-29 August, Toulouse, France, edited by E.
Mercier-Laurent, J. Debenham, pp. 403-412, 2004.

[78] Oltean M., Improving the Search by Encoding Multiple Solutions in
a Chromosome, contributed chapter 15, Evolutionary Machine Design,
Nova Science Publisher, New-York, N. Nedjah, L. de Mourrelo (edi-
tors), 2004.

[79] Oltean M., Grosan C., Evolving Digital Circuits using Multi Ex-
pression Programming, NASA/DoD Conference on Evolvable Hard-
ware, 24-25 June, Seatle, R. Zebulum, D. Gwaltney, G. Horbny, D.
Keymeulen, J. Lohn, A. Stoica (editors), pages 87-90, IEEE Press,
NJ,2004.

[80] Oltean M., Improving Multi Expression Programming: an Ascending
Trail from Sea-Level Even-3-Parity to Alpine Even-18-Parity, chapter
10, Evolutionary Machine Design: Theory and Applications, Springer-
Verlag, N. Nedjah and L. de Mourrelo (editors), pp. 270-306, 2004.

[81] Oltean, M., Evolving Evolutionary Algorithms using Linear Genetic
Programming, Evolutionary Computation, MIT Press, MA, USA, 2005
(accepted).

[82] O’Neill, M. Ryan, C., Grammatical Evolution, IEEE Transaction on

Evolutionary Computation, Vol 5, 2001.

[83] O’Reilly, U.M., Oppacher, F., A Comparative Analysis of Genetic Pro-
gramming, Advances in Genetic Programming 2, Peter J. Angeline and
Kenneth E. Kinnear (editors), pp. 23-44, MIT Press, 1996.

[84] Patterson, N.R., Genetic Programming with Context-Sensitive Gram-

mars, PhD thesis, University of St Andrews, Scotland, 2003.

167

[85] Poli, R., Langdon, W. B., Sub-machine Code Genetic Programming,
Technical report CSRP-98-18, School of Computer Science, University
of Birmingham, 1998.

[86] Prechelt, L., PROBEN1 – A Set of Neural Network Problems and
Benchmarking Rules, Technical Report 21, University of Karlsruhe,
1994.

[87] Reinelt, G., TSPLIB – A Traveling Salesman Problem Library, ORSA,

Journal of Computing, Vol. 3(4), pp. 376-384, 1991.

[88] Rechenberg, I., Evolutionsstrategie’94, Frommann-Holzboog, 1994.

[89] Samuel, A., Some studies in machine learning using the game of check-
ers, In Computers and Thought, Feigenbaum, E., and Feldman, J.,
(editors), McGraw-Hill, New-York, 1963.

[90] Spector, L., Robinson, A., Genetic Programming and Autoconstructive
Evolution with the Push Programming Language, Genetic Program-
ming and Evolvable Machines, Vol.3, Nr.1 pp. 7-40, 2002.

[91] Stoica, A., Zebulum, R., Guo, X., Keymeulen, D., Ferguson, M. Duong,
V., Silicon Validation of Evolution Designed Circuits, In Proceedings
of the 2003 NASA/DoD Conference on Evolvable Hardware, pp. 21-26,
2003.

[92] Stoica, A., Zebulum, R. S., Xin Guo, Keymeulen, D., Ferguson, M. I.,
Duong, V., Taking Evolutionary Circuit Design from Experimentation
to Implementation: some useful Techniques and a Silicon Demonstra-
tion, In IEE Proceedings Computers and Digital Techniques (Special
Issue on Evolvable Hardware), 2003.

[93] Stoica, A., Zebulum, R. S., Xin Guo, Keymeulen, D., Ferguson, M.
I., Scalability Issues in Evolutionary Synthesis of Electronic Circuits
Lessons Learned and Challenges ahead Computational Synthesis Work-
shop, AAAI Spring Symposium Series, Stanford University, CA, pp.
233-238, 2003.

[94] Streeter M.J. Two Broad Classes of functions for Which a No Free
Lunch Result does not Hold, In Proceedings of the GECCO 2003,
edited by Erick Cantu-Paz (et al), 1418-1430, Springer, 2003.

168

[95] Syswerda, G., Uniform Crossover in Genetic Algorithms, Schaﬀer, J.D.,
(editor), Proceedings of the 3rd International Conference on Genetic
Algorithms, pp. 2-9, Morgan Kaufmann Publishers, San Mateo, CA,
1989.

[96] Tatar, D., Oltean, M., Proving Theorems using DNA, Studia, Ser.

Informatica, Babe¸s-Bolyai University, 1999.

[97] Teller, A., Evolving Programmers: the Co-evolution of Intelligent Re-
combination Operators, in Advances in Genetic Programming II, An-
geline P. and Kinnear K.E., (editors), MIT Press, pp. 45-68, 1996.

[98] Whitley, L.D., A Free Lunch Proof for Gray versus Binary Encodings,
in Proceedings of the Genetic and Evolutionary Computation Confer-
ence, W. Banzhaf J., Daida, A. E. Eiben M. H. Garzon, V. Honavar M.
Jakiela, R. E. Smith (editors) Vol. 1, Morgan Kaufmann, pp. 726-733,
1999.

[99] Whitley, L.D., Functions as Permutations: Implications for No Free
Lunch, Walsh Analysis and Statistics, in Proc. of the Sixth Interna-
tional Conference on Parallel Problem Solving from Nature (PPSN VI),
Schoenauer, M., Deb, K., Rudolph, G., Yao, X., Lutton, E., Merelo,
J.J. and Schwefel, H.-P. (editors) , Springer Verlag, Berlin, pp. 169-178,
2000.

[100] Wolpert, D.H., McReady, W.G., No Free Lunch Theorems for Search,

technical report SFI-TR-05-010, Santa Fe Institute, 1995.

[101] Wolpert, D.H., McReady, W.G., No Free Lunch Theorems for Opti-
misation, IEEE Transaction on Evolutionary Computation, Vol. 1, pp.
67-82, 1997.

[102] Yao, X., Liu, Y., Lin, G., Evolutionary Programming Made Faster,
IEEE Transaction on Evolutionary Computation, Vol. 3(2), pp. 82-
102, 1999.

[103] UCI

Repository,
www.ics.uci.edu/∼mlearn/MLRepository.html

Learning

Machine

Available

from

169

