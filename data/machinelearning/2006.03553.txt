Logical Team Q-learning: An approach towards factored policies in
cooperative MARL

1
2
0
2

r
a

M
8
2

]

G
L
.
s
c
[

2
v
3
5
5
3
0
.
6
0
0
2
:
v
i
X
r
a

Lucas Cassano
´Ecole Polytechnique F´ed´erale de Lausanne

Ali H. Sayed
´Ecole Polytechnique F´ed´erale de Lausanne

Abstract

We address the challenge of learning factored
policies in cooperative MARL scenarios. In
particular, we consider the situation in which
a team of agents collaborates to optimize a
common cost. The goal is to obtain fac-
tored policies that determine the individual
behavior of each agent so that the resulting
joint policy is optimal. The main contribu-
tion of this work is the introduction of Logi-
cal Team Q-learning (LTQL). LTQL does not
rely on assumptions about the environment
and hence is generally applicable to any col-
laborative MARL scenario. We derive LTQL
as a stochastic approximation to a dynamic
programming method we introduce in this
work. We conclude the paper by providing
experiments (both in the tabular and deep
settings) that illustrate the claims.

1

INTRODUCTION

Reinforcement Learning (RL) has had considerable
In particular, Q-learning
success in many domains.
[Watkins and Dayan, 1992] and its deep learning ex-
tension DQN [Mnih et al., 2013] have shown great per-
formance in challenging domains such as the Atari
Learning Environment [Bellemare et al., 2013]. At the
core of DQN lie two important features: the ability to
use expressive function approximators (in particular,
neural networks (NN)) that allow it to estimate com-
plex Q-functions; and the ability to learn oﬀ-policy
and use replay buﬀers [Lin, 1992], which allows DQN
to be very sample eﬃcient. Traditional RL focuses on
the interaction between one agent and an environment.
However, in many cases of interest, a multiplicity of
agents will need to interact with a unique environment
and with each other. This is the object of study of
Multi-agent RL (MARL), which goes back to the early

work of [Tan, 1993] and has seen renewed interest of
late (for an updated survey see [Zhang et al., 2019]).
In this paper we consider the particular case of cooper-
ative MARL in which the agents form a team and have
a shared unique goal. We are interested in tasks where
collaboration is fundamental and a high degree of co-
ordination is necessary to achieve good performance.
In particular, we consider two scenarios.

In the ﬁrst scenario, the global state and all actions
are visible to all agents. One example of this situ-
ation could be a team of robots that collaborate to
move a big and heavy object. It is well known that
in this scenario the team can be regarded as one sin-
gle agent where the aggregate action consists of the
joint actions by all agents [Littman, 2001]. The fun-
damental drawback of this approach is that the joint
action space grows exponentially in the number of
agents and the problem quickly becomes intractable
[Kok and Vlassis, 2004, Guestrin et al., 2002b]. One
well-known and popular approach to solve these issues,
is to consider each agent as an independent learner (IL)
[Tan, 1993]. However, this approach has a number of
problems. First, from the point of view of each IL, the
environment is non-stationary (due to the changing
policies of the other agents), which jeopardizes conver-
gence. And second, replay buﬀers cannot be used due
to the changing nature of the environment and there-
fore even in cases where this approach might work,
the data eﬃciency of the algorithm is negatively af-
fected. Ideally, it is desirable to derive an algorithm
with the following features: i) it learns individual poli-
cies (and is therefore scalable), ii) local actions chosen
greedily with respect to these individual policies re-
sult in an optimal team action, iii) can be combined
with NN’s, iv) works oﬀ-policy and can leverage replay
buﬀers (for data eﬃciency), v) and enjoys theoretical
guarantees to team optimal policies at least in the dy-
namic programming scenario. Indeed, the main contri-
bution of this work is the introduction of Logical Team
Q-learning (LTQL), an algorithm that has all these
properties. We start in the dynamic programing set-
ting and derive equations that characterize the desired

 
 
 
 
 
 
Logical Team Q-learning: An approach towards factored policies in cooperative MARL

solution. We use these equations to deﬁne the Factored
Team Optimality Bellman Operator and provide a the-
orem that characterizes the convergence properties of
this operator. A stochastic approximation of the dy-
namic programming setting is used to obtain the tab-
ular and deep versions of our algorithm. For the single
agent setting, these steps reduce to: the Bellman opti-
mality equation, the Bellman optimality operator and
Q-learning (in its tabular form and DQN).

In the second scenario, we consider the centralized
training and decentralized execution paradigm under
partial observability. Under this scheme, training is
done in a centralized manner and therefore we as-
sume global information to be available during train-
ing. During execution, agents only have access to
their own observations. Therefore, even though dur-
ing training global information is available, the learned
policies must only rely on local observations. An ex-
ample of this case would be a soccer team that during
training can rely on a centralized server where data is
aggregated but has to play games in a fully decentral-
ized manner without the aid of such server.

1.1 Relation to Prior Work

Some of the earliest works on MARL are [Tan, 1993,
Claus and Boutilier, 1998].
[Tan, 1993] studied In-
dependent Q-learning (IQL) and identiﬁed that IQL
learners in a MARL setting may fail to converge
due to the non-stationarity of the perceived envi-
ronment.
[Claus and Boutilier, 1998] compared the
performance of IQL and joint action learners (JAL)
where all agents learn the Q-values for all the joint
actions, and identiﬁed the problem of coordination
during decentralized execution when multiple opti-
mal policies are available.
[Littman, 2001] later pro-
vided a proof of convergence for JALs. Recently,
[Tampuu et al., 2017] did an experimental study of
ILs using DQNS in the Atari game Pong. All these
mentioned approaches cannot use experience replay
due to the non-stationarity of the preceived environ-
ment. Following Hyper Q-learning [Tesauro, 2004],
[Foerster et al., 2017] addressed this issue to some
extent using ﬁngerprints as proxys to model other
agents’ strategies.

introduced Distributed
[Lauer and Riedmiller, 2000]
Q-learning (DistQ), which in the tabular setting has
guaranteed convergence to an optimal policy for de-
terministic MDPs. However, this algorithm performs
very poorly in stochastic scenarios and becomes di-
vergent when combined with function approximation.
Later Hysteretic Q-learning (HystQ) was introduced
in [Matignon et al., 2007] to improve these two lim-
itations. HystQ is based on a heuristic and can be
thought of as a generalization of DistQ. These works

also consider the scenario where agents cannot per-
ceive the actions of other agents. They are related to
LTQL (from this work) in that they can be consid-
ered approximations to our algorithm in the scenario
where agents do not have information about other
agents’ actions. Recently [Omidshaﬁei et al., 2017] in-
troduced Dec-HDRQNs for multi-task MARL, which
combines HystQ with Recurrent NNs and experience
replay (which they recognize is important to achieve
high sample eﬃciency) through the use of Concurrent
Experience Replay Trajectories.

[Wang and Sandholm, 2003] introduced OAB, the ﬁrst
algorithm that converges to an optimal Nash equilib-
rium with probability one in any team Markov game.
OAB considers the team scenario where agents observe
the full state and joint actions. The main disadvan-
tage of this algorithm is that it requires estimation
of the transition kernel and rewards for the joint ac-
tion state space and also relies on keeping count of
state-action visitation, which makes it impractical for
MDPs of even moderate size and cannot be combined
with function approximators.

[Guestrin et al., 2002a,
Guestrin et al., 2002b,
Kok and Vlassis, 2004] introduced the idea of factor-
ing the joint Q-function to handle the scalability issue.
These papers have the disadvantage that they require
coordination graphs that specify how agents aﬀect
each other (the graphs require signiﬁcant domain
knowledge). The main shortcoming of these papers is
the factoring model they use, in particular they model
the optimal Q-function (which depends on the joint
actions) as a sum of K local Q-functions (where K is
the number of agents, and each Q-function considers
only the action of its corresponding agent). The main
issue with this factorization model is that the optimal
Q-function cannot always be factored in this way, in
fact, the tasks for which this model does not hold are
typically the ones that require a high degree of coor-
dination, which happen to be the tasks where one is
most interested in applying speciﬁc MARL approaches
as opposed to ILs. The approach we introduce in this
paper also considers learning factored Q-functions.
However, the fundamental diﬀerence is that the fac-
tored relations we estimate always exist and the joint
action that results from maximizing these individual
Q-functions is optimal. VDN [Sunehag et al., 2018]
and Qmix [Rashid et al., 2018] are two recent deep
methods that also factorize the optimal Q-function
assuming additivity and monotonicity, respectively.
This factoring is their main limitation since many
MARL problems of interest do not satisfy any of these
two assumptions.
[Son et al., 2019] showed
that these methods are unable to solve a simple
matrix game. Furthermore, the individual policies

Indeed,

Lucas Cassano, Ali H. Sayed

[Son et al., 2019]

cannot be used for prediction, since the individual Q
values are not estimates of the return. To improve
on the representation limitation due to the factoring
assumption,
introduced QTRAN
which factors the Q-function in a more general
manner and therefore allows for a wider applicability.
The main issue with QTRAN is that although it
can approximate a wider class of Q-functions than
VDN and Qmix,
the algorithm resorts to other
approximations, which degrade its performance in
complex environments (see [Rashid et al., 2020]).

it

the

that

performs

disadvantage

Recently, actor-critic strategies have been explored.
The algorithm introduced in [Zhang et al., 2018]
has
poor
credit assignment and as a consequence can eas-
ily converge to highly suboptimal strategies (see
intro-
[Cassano et al., 2019]).
duces policy gradient schemes that also have the
credit assignment issue. The algorithm presented by
[Foerster et al., 2018] addresses this issue, but does so
by learning the team’s joint q-function and hence this
approach does not address the exponential scalability
issue. These methods have the added inconvenience
that they are on-policy and hence do not enjoy the
data eﬃciency that oﬀ-policy methods can achieve.

[Gupta et al., 2017]

2 PROBLEM FORMULATION

We consider a situation where multiple agents form
a team and interact with an environment and with
each other. We model this interaction as a decen-
tralized partially observable Markov decision process
(Dec-POMDP)[Oliehoek et al., 2016], which is deﬁned
by the tuple (S,K,ok,Ak,P,r), where, S is a set of
global states shared by all agents; K is the set of
K of agents; ok : S → Ok is the observation func-
tion for agent k, whose output lies in some set of
observations Ok; Ak is the set of actions available
to agent k; P(s(cid:48)|s, a1, · · · , aK) speciﬁes the proba-
bility of transitioning to state s(cid:48) ∈ S from state
s ∈ S having taken joint actions ak ∈ Ak; and
r : S × A1 × · · · × AK × S → R is a global reward
function. Speciﬁcally, r(s, a1, · · · , aK, s(cid:48)) is the reward
when the team transitions to state s(cid:48) ∈ S from state
s ∈ S having taken actions a1, · · · , aK. The reward
r(s, a1, · · · , aK, s(cid:48)) can be a random variable follow-
ing some distribution fs,a1,··· ,aK ,s(cid:48)(r). We clarify that
from now on we will refer to the collection of all in-
dividual actions as the team’s action, denoted as ¯a.
Furthermore we will use a−k to refer to the actions of
all agents except for action ak. Therefore we can write
P(s(cid:48)|s, a1, · · · , aK) = P(s(cid:48)|s, ak, a−k) = P(s(cid:48)|s, ¯a).
The goal of the team is to maximize the team’s re-

turn:

J(π) =

∞
(cid:88)

t=0

γtE π,P,d,f [r(st, ¯at, st+1)]

(1)

where st and ¯at are the state and actions at time t,
respectively, π(¯a|s) is the team’s policy, d is the distri-
bution of initial states, and γ ∈ [0, 1) is the discount
factor. We clarify that we use bold font to denote ran-
dom variables and the notation E (cid:96) makes explicit that
the expectation is taken with respect to distribution
(cid:96). From now on, we will only make the distributions
explicit in cases where doing so makes the equations
more clear. Accordingly, the team’s optimal state-
action value function (q†) and optimal policy (π†) are
given by [Sutton and Barto, 1998]:

π†(¯a|s) = arg max

π(¯a|s)

E π,P

(cid:2)r(s, ¯a)+γ max
¯a(cid:48)

q†(s(cid:48), ¯a(cid:48))(cid:3) (2a)

q†(s, ¯a) = E P

(cid:2)r(s, ¯a) + γ max
¯a(cid:48)

q†(s(cid:48), ¯a(cid:48))(cid:3)

(2b)

where r(s, ¯a) = E P,f r(s, ¯a, s(cid:48)). As already mentioned,
a team problem of this form can be addressed with
any single-agent algorithm. The fundamental incon-
venience with this approach is that the joint action
space scales exponentially with the number of agents,
more speciﬁcally | ¯A| = (cid:81)K
k=1 |Ak| (where ¯A is the joint
action space). Another problem with this approach is
that the learned Q-function cannot be executed in a
decentralized manner using the agents’ observations.
For these reasons, in the next sections we concern our-
selves with learning factored quantities.

3 DYNAMIC PROGRAMMING

Similarly to the way that relation (2b) is used to derive
Q-learning in the single agent setting, the goal of this
section is to derive relations in the dynamic program-
ming setting from which we can derive a cooperative
MARL algorithm. The following two propositions take
the ﬁrst steps in this direction.

Proposition 1. For each deterministic team optimal
policy, there exist K factored functions qk,(cid:63) : S ×Ak →
R such that:

qk,(cid:63)(s, ak),

∀1 ≤ k ≤ K (3a)

q†(s, ¯a) = max
ak
q†(s, ¯a) = q†(cid:16)

max
¯a

max
¯a

s, arg max
a1

q1,(cid:63)(s, a1), · · ·

qK,(cid:63)(s, aK)

(cid:17)

, arg max
aK

qk,(cid:63)(s, ak) = BEqk,(cid:63)(s, ak)
BEqk(s, ak) = r(s, ak, a−k)

(3b)

(3c)

+γE max

a(cid:48)

qk(s(cid:48), a(cid:48))(cid:12)

(cid:12)an=arg max

qn(s,an) ∀n(cid:54)=k (3d)

an

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

where operator BE is deﬁned such that if there are mul-
tiple arguments that maximize arg maxan qn(s, an) the
actions that jointly maximize (3d) are chosen.

Proposition 3. Sub-optimal Nash ﬁxed points:
There may exist K functions qk such that (3c) is sat-
isﬁed but (3b) is not.

Proof. Assume that we have some deterministic team
optimal policy π†(¯a|s). We deﬁne qk,(cid:63)(s, ak) as follows:

Proof. See Appendix 6.1.

qk,(cid:63)(s, ak) = q†(s, ak, a−k)|arg maxa−k π†(ak,a−k|s)

(4)

Note that by construction, qk,(cid:63)(s, ak) satisﬁes relations
(3a) and (3b) and also:

arg max
ak

qk,(cid:63)(s, ak) = ak ∼ π†(ak, a−k|s) ∀k

(5)

Relation (3c) is obtained by combining relations (2b),
(4) and (5).

A simple interpretation of equation (3c) is that
qk,(cid:63)(s, ak) is the expected return starting from state
s when agent k takes action ak while the rest of the
team acts in an optimal manner.

Proposition 2. Each deterministic team optimal pol-
icy that can be factored into K deterministic policies
πk,(cid:63)(ak|s). Such factored deterministic policies can be
obtained as follows:

πk,(cid:63)(ak|s) = I(cid:0)ak =arg maxak qk,(cid:63)(s, ak)(cid:1)

(6)

where I is the indicator function.

Proof. The proof follows from equations (3a)-(3b).

Propositions 1 and 2 are useful because they show
that if the agents learn factored functions that sat-
isfy (3) and act greedily with respect to their cor-
responding qk,(cid:63), then the resulting team policy is
guaranteed to be optimal and hence they are not
subject to the coordination problem identiﬁed in
[Lauer and Riedmiller, 2000]1 (we show this in section
5.1). Therefore, an algorithm that learns qk,(cid:63) would
satisfy the ﬁrst two of the ﬁve desired properties that
were enumerated in the introduction. As a sanity
check, note that for the case where there is only one
agent, equation (3c) simpliﬁes to the Bellman opti-
mality equation. Although in the single agent case the
Bellman optimality operator can be used to obtain q†
(by repeated application of the operator), we cannot
do the same with BE. The fundamental reason for this
is stated in proposition 3.

1This problem arises in situations in which the en-
vironment has multiple deterministic team optimal poli-
cies and the agents learn factored functions of the form
maxa−kq†(s,ak,a−k) (we remark that these are not the same
as qk,(cid:63)(s, ak)).

Note that proposition 3 implies that relation (3c) is
not suﬃcient to derive a learning algorithm capable
of obtaining a team optimal policy because it can con-
verge to sub-optimal team strategies instead. To avoid
this inconvenience, it is necessary to ﬁnd another re-
lation that is only satisﬁed by q(cid:63). We can obtain one
such relation combining (3a) and (3c):

max
ak

qk,(cid:63)(s,ak) = max

¯a

(cid:2)r(s,¯a)+γEmax
a(cid:48),k

qk,(cid:63)(s(cid:48), a(cid:48),k)(cid:3) (7)

The sub-optimal Nash ﬁxed points mentioned in
proposition 3 do not satisfy relation (7) since by def-
inition the right hand side is equal to max¯a q†(s, ¯a).
Intuitively, equation (7) is not satisﬁed by these sub-
optimal strategies because the max¯a considers all pos-
sible team actions (while Nash equilibria only consider
unilateral deviations).

Deﬁnition 1. Combining equations (3c) and (7), we
deﬁne the Factored Team Optimality Bellman operator
Bp as follows:

(cid:40)

Bpqk(s,ak)=

BEqk(s,ak) with probability p>0
BI qk(s,ak) else
BI qk(s, ak) = max (cid:8)qk(s, ak),

(8)

max
a−k

(cid:0)r(s, ak, a−k) + γE max
a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)(cid:9)

(9)

LTQL is based on operator Bp, the reason we use re-
lations (3c) and (7) to deﬁne this operator instead of
just (7), is that using only relation (7) we would derive
DistQ that has the shortcomings we discussed in sec-
tion 1.1. A simple interpretation of operator Bp is the
following. Consider a basketball game, in which player
α has the ball and passes the ball to teammate β. If
β gets distracted, misses the ball and the opposing
team ends up scoring, should α learn from this expe-
rience and modify its policy to not pass the ball? The
answer is no, since the poor outcome was player β’s
fault. In plain English, from the point of view of some
player k, what the operator BE means is “I will only
learn from experiences in which my teammates acted
according to what I think is the optimal team strategy”.
It is easy to see why this kind of stubborn rationale
cannot escape Nash equilibria (i.e., agents do not learn
when the team deviates from its current best strategy,
which obviously is a necessary condition to learn bet-
ter strategies). The interpretation of the full operator
Bp is “I will learn from experiences in which: a) my

Lucas Cassano, Ali H. Sayed

teammates acted according to what I think is the opti-
mal team strategy; or b) my teammates deviated from
what I believe is the optimal strategy and the outcome
of such deviation was better than I expected if they had
acted according to what I thought was optimal”, which
arguably is what a logical player would do (this is the
origin of the algorithm’s name). We now proceed to
describe the convergence properties of operator Bp.
Lemma 1. For any δ1 > 0, after N applications of
operator Bp to any set of K qk(s, ak) functions it holds:

P(cid:0)BN

p qk(s, ak) ∈ CU
δ1

(cid:1) ≥ 1 −

no(cid:88)

n=0

(cid:18)N
n

(cid:19)

pn(1 − p)N −n

(10)

CU
δ1

no =

q†(s, ak, a−k) + δ1

= (cid:8)qk|qk(s, ak) ≤ max
a−k
∀(k, s, ak)∈(K, S, Ak)(cid:9) (11)
(cid:19)(cid:23)
δ1
qU − mins max¯a q†(s, ¯a)

(cid:22)
logγ

(12)

(cid:18)

qU = max{rmax(1 − γ)−1, max
k,s,ak

qk(s, ak)}

(13)

where rmax = maxs,¯a r(s, ¯a). For the special case
where N > no/p we can lower bound equation (10)
as follows:

P(cid:0)BN

p qk(s, ak) ∈ CU
δ1

(cid:1) ≥ 1 − e−2N(p− no

N )2

(14)

Proof. See Appendix 6.2.

Lemma 2. After N ≥ L applications of operator Bp
to any set of K qk(s, ak) ∈ CU

0 functions it holds:
(cid:1)≥1−βN,L+(1−p)LβN−L,L (15)

P(cid:0)BN

p qk(s, ak) ∈ Cδ2
(cid:98)N/(L+1)(cid:99)
(cid:88)

βN,L =

j=0

(−1)j

(cid:19)

(cid:18)N − jL
j

(cid:0)p(1 − p)L(cid:1)j

(16)

Lemma 1 indicates that if the initial functions qk(s, ak)
have values that are larger than maxa−k q†(s, ak, a−k),
after suﬃcient applications of operator Bp all over-
estimations will be reduced such that qk(s, ak) ≤
maxa−k q†(s, ak, a−k) + δ1 with high probability.
Lemma 2 show that if the operator Bp is applied suf-
ﬁcient times to functions that do not overestimate
maxa−k q†(s, ak, a−k), then the obtained function lie
in a small neighborhood of the desired solution with
high probability. These results give rise to the follow-
ing important theorem.

Theorem 1. Repeated application of the operator Bp
to any initial set of K qk-functions followed by an ap-
plication of operator BE converge to the δ-neighborhood
(δ > 0) of some set qk,(cid:63) with high probability. For the
particular case, where p > 0/5 and N > L + no/p it
holds:

P(cid:0)|BEBN

p qk(s, ak)−qk,(cid:63)(s, ak)|<δ(cid:1)≥1−O(θN )

(20)

for any δ > 0, where 0 ≤ θ < 1 is a constant that
depends on δ, γ, p, r(s, ¯a), P and the initial functions
qk(s, ak).

Proof. Combining the results from lemmas 1 and 2
and setting δ = δ1 = δ2 we get that after N1 + N2 >
L + no/p > 0 applications of operator Bp to any set of
K qk(s, ak) functions it holds:

P(cid:0)BN

p qk(s, ak) ∈ Cδ

(cid:1) ≥ max
N1> no
p
N2≥L

(cid:18)

1 − e−2N1

(cid:17)2(cid:19)

(cid:16)

p− no
N1

(cid:18)

·

1 −

1 − (1 − p)ξ1
pξ1(1 + L − Lξ1)

ξ−N2
1

−

L
p

(cid:19)

(1 − p)N2+2

= 1 − O(θN )

(21)

where 0 ≤ θ < 1. Now we proceed to analyze
p qk(s, ak). If qk(s, ak) ∈ Cδ and δ satisﬁes:
BEBN

(cid:32)

(cid:38)
logγ

L =

max
s

δ2
(cid:12)
qk(s, ak) − max
(cid:12) max
ak
Cδ2 = (cid:8)qk|qk,(cid:63)(s, ak) − δ2 ≤ qk(s, ak)
≤max
a−k

¯a

(cid:33)(cid:39)

q†(s, ¯a)(cid:12)
(cid:12)

q†(s, ak, a−k)+δ2∀(k, s, ak)∈(K, S, Ak)(cid:9) (18)

for any δ2 > 0.
bounded by:

If p > 0.5, probability (15) can be

P(cid:0)BN

p qk(s, ak) ∈ Cδ2

(cid:1) ≥ 1 −

−

1 − (1 − p)ξ1
pξ1(1 + L − Lξ1)
L
p

(1 − p)N +2

ξ−N
1

(19)

where 1 < ξ1 < 1 + L−1.

Proof. See Appendix 6.3.

(17)

δ <

1
2

min
s

(cid:0) max

¯a

q†(s, ¯a) −

max
¯a(cid:54)=arg max¯a q†(s,¯a)

q†(s, ¯a)(cid:1) (22)

we get:

BEqk(s, ak) = (cid:0)r(s, ak, a−k)
+ γE max

qk(s(cid:48), a(cid:48))(cid:1)(cid:12)

a(cid:48)

(cid:12)an=arg maxan qn(s,an) ∀n(cid:54)=k (23)

Using the fact that qk(s, ak) ∈ Cδ it follows:

max
ak

qk,(cid:63)(s, ¯a) − δ ≤ max
ak

qk(s, ak)

qk(s, ak,•) ≤ max
a−k

q†(s, ak,•, a−k) + δ

(a)
< max

¯a

q†(s, ¯a) − δ

(b)
= max

qk,(cid:63)(s, ¯a) − δ2

ak,• =

arg max
ak(cid:54)=arg maxak qk(s,ak)

ak
qk(s, ak)

(24)

(25)

(26)

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

where in (a) we used condition (22) and in (b) we used
equation (3a). Combining equations (23) through (26)
we get:

BEqk(s, ak) = (cid:0)r(s, ak, a−k)

+ γE max

a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

qn,(cid:63)(s,an) ∀n(cid:54)=k

an
qk,(cid:63)(s(cid:48), a(cid:48)) + max

a(cid:48)

qk(s(cid:48), a(cid:48))

= (cid:0)r(s, ak, a−k) + γE (cid:0) max
a(cid:48)

qk,(cid:63)(s(cid:48), a(cid:48))(cid:1)(cid:1)(cid:12)

− max
a(cid:48)

= qk,(cid:63)(s, ak) + γE (cid:0) max
a(cid:48)

qk,(cid:63)(s(cid:48), a(cid:48))(cid:1)(cid:12)

− max
a(cid:48)

(cid:12)an=arg max

qn,(cid:63)(s,an) ∀n(cid:54)=k

an
qk(s(cid:48), a(cid:48))

(cid:12)an=arg max

qn,(cid:63)(s,an) ∀n(cid:54)=k

(27)

an

Combining equation (27) with the fact that qk(s, ak) ∈
Cδ we get:

|BEqk(s, ak) − qk,(cid:63)(s, ak)| ≤ γδ

(28)

Combining (28) and (21) completes the proof.

Relation (28) shows why we include an application of
BE at the end in equation (20). The reason is that
if we do not, the q-value for suboptimal actions os-
cillates between qk,(cid:63)(s, ak) and maxa−k q†(s, ak, a−k),
we illustrate this eﬀect in appendix 6.5. We reiter-
ate that qk,(cid:63)(s, ak) and maxa−k q†(s, ak, a−k) are only
equal when optimal actions are chosen (equation (3a)).
qk,(cid:63)(s, ak) is the expected return if agent k chooses ac-
tion ak and the rest of the team follows an optimal
policy, while maxa−k q†(s, ak, a−k) is the best return
that can be achieved if agent k chooses action ak.

4 REINFORCEMENT LEARNING

In this section we present LTQL (see algorithm 1),
which we obtain as a stochastic approximation to the
procedure described in theorem 1. Note that the al-
gorithm utilizes two q estimates for each agent k, a
biased one parameterized by θk (which we denote qθk )
and an unbiased one parameterized by ωk (which we
denote qωk ). We clarify that in the listing of LTQL +=
is the accumulate and add operator and that we used
a constant step-size, however this can be replaced with
decaying step-sizes or other schemes such as AdaGrad
[Duchi et al., 2011] or Adam [Kingma and Ba, 2014].
Note that the target of the unbiased network is used
to calculate the target values for both functions; this
prevents the bias in the estimates qθk (which arises
due to the c2 condition)2 from propagating through
bootstrapping. The target parameters of the biased

2We refer to the condition of the ﬁrst if statement (i.e.,
(s, an) ∀n (cid:54)= k) as c1, and the condition

an = arg maxan qθk
corresponding to the second if statement as c2.

T

Algorithm 1 Logical Team Q-Learning

Initialize: an empty replay buﬀer R, parameters
θk and ωk and their corresponding targets θk
T and
ωk
for iterations e = 0, . . . , E do

T for all agents k ∈ K.

Sample T transitions (s, ¯a, r, s(cid:48)) by following some
behavior policy which guarantees all joint actions
are sampled with non-zero probability and store
them in R.
for iterations i = 0, . . . , I do

Sample a mini-batch of B transitions (s, ¯a, r, s(cid:48))
from R.
Set ∆θk = 0 and ∆ωk = 0 for all agents k.
for each transition of the mini-batch b =
1, · · · , B and each agent k = 1, · · · , K do

if an = arg maxan qθk
(cid:0)r+max

∆θk + = ∇θk

T

(s, an) ∀n (cid:54)= k then

(s(cid:48), a)−qθk (s, ak)(cid:1)

qωk

T

a

∆ωk+ = ∇ωk
else if (cid:0)r+max
a
∆θk+ = α∇θk

T

a

qωk

(cid:0)r+max
qθk
(cid:0)r+max

(s(cid:48), a)−qωk (s, ak)(cid:1)
(s(cid:48), a) > qθk (s, ak)(cid:1) then
(s(cid:48), a)−qθk (s, ak)(cid:1)

qωk

T

a

T

end if
end for
θk+ = µ∆θk

ωk+ = µ∆ωk

end for
Update targets θk

end for

T = θk and ωk

T = ωk.

estimates (θk
T ) are used solely to evaluate condition
c1. We have found that this stabilizes the training of
the networks, as opposed to just using θk. Hyperpa-
rameter α weights samples that satisfy condition c2
diﬀerently from those who satisfy c1. As we remarked
in the introduction, LTQL reduces to DQN for the
case where there is a unique agent. In appendix 6.4
we include the tabular version of the algorithm along
with a brief discussion.

Note that LTQL works oﬀ-policy and there is no
necessity of synchronization for exploration. There-
fore in applications where agents have access to the
global state and can perceive the actions of all other
agents (so that they can evaluate c1), it can be im-
plemented in a fully decentralized manner. Interest-
ingly, if condition c1 was omitted (to eliminate the
requirement that agents have access to all this in-
formation), the resulting algorithm is exactly DistQ
[Lauer and Riedmiller, 2000]. However, as the proof of
lemma 1 indicates, the resulting algorithm would only
converge in situations where it could be guaranteed
that during learning, overestimation of the q values is
not possible (i.e., the tabular setting applied to deter-
ministic environments; this remark was already made
In the case where
in [Lauer and Riedmiller, 2000]).

Lucas Cassano, Ali H. Sayed

this condition could not be guaranteed (i.e., when us-
ing function approximation and/or stochastic environ-
ments), some mechanism to decrease overestimated q
values would be necessary, as this is the main task of
the updates due to c1. One possible way to do this
would be to use all transitions to update the q esti-
mates but use a smaller step-size for the ones that
do not satisfy c2. Notice that the resulting algorithm
would be exactly HystQ [Matignon et al., 2007].

Notice that the listing of LTQL relies on global states
s as opposed to local agent observations. Therefore
in its current form the algorithm is only applicable
to the ﬁrst scenario described in the introduction, in
which agents have access to the global state both dur-
ing training and execution. For the second scenario, in
which during execution agents rely on their local obser-
vation we make the usual approximation qk(Hk, ak) ≈
qk(s, ak) where Hk is the action-observation history of
agent k. Hence, to adapt algorithm 1 to this second
scenario all that is necessary is to replace qθk (s, ak)
for qθk (Hk, ak) (and similarly for ωk, θk
T ),
and the observation histories need to be stored in
the replay buﬀer as well. In practice recurrent archi-
tectures (like the Long Short Term Memory (LSTM)
[Hochreiter and Schmidhuber, 1997]) can be used to
parameterize qθk (Hk, ak) as is done in Deep recurrent
Q-Network (DRQN) [Hausknecht and Stone, 2015].

T and ωk

5 EXPERIMENTS

5.1 Matrix Game

The ﬁrst experiment is a simple matrix game (ﬁgure 1a
shows the payoﬀ structure) with multiple team opti-
mum policies to evaluate the resilience of the algorithm
to the coordination issue mentioned in section 3. In
this case, we implemented IQL, DistQ, LTQL, Qmix
and Qtran (we do not include a curve labeled HystQ
because in deterministic environments with tabular
representation the optimum value for the small step-
size is 0, in which case it becomes exactly equivalent
to DistQ). All algorithms are implemented in tabular
form.3 In all cases we used uniform exploratory poli-
cies ((cid:15) = 1) and we did not use replay buﬀers. IQL
fails at this task and oscillates due to the perceived
time-varying environment (see ﬁgure 2 in appendix
6.5). DistQ converges to (29)-(30), which clearly shows
why DistQ has a coordination issue. However, LTQL
converges to either of the two possible solutions (31)
or (32) (depending on the seed) for which individual
greedy policies result in team optimal policies. Qmix
fails at identifying an optimum team policy and the

3In the case of Qmix we used tabular representations
for the individual q functions and a NN for the mixing
network.

resulting joint q-function obtained using the mixing
network also fails at predicting the rewards. Qmix
converges to (33). The joint q-function is shown in ap-
pendix 6.5. And Qtran also oscillates due to the fact
that in this matrix game there are two jointly opti-
mal actions. In appendix 6.5 we include the learning
curves of all algorithms for the readers reference along
with a brief discussion.

q1(a1) = max
a2
q2(a2) = max
a1

q†(a1, a2) = [2, 2]

q†(a1, a2) = [0, 2, 2]

q1,(cid:63)(a1) = [2, 1]
q1,(cid:63)(a1) = [0, 2]
q1(a1) = [−0.7, 1.1]

q2,(cid:63)(a2) = [0, 2, 0]
q2,(cid:63)(a2) = [0, 1, 2]

q2(a2) = [−3.5, 1.8, 0.6]

(29)

(30)

(31)

(32)

(33)

5.2 Stochastic Finite Environment

In this experiment we use a tabular representation in
an environment that is stochastic and episodic. The
environment is a linear grid with 4 positions and 2
agents. At the beginning of the episode, the agents are
initialized in the far right. Agent 1 cannot move and
has 2 actions (push button or not push), while agent 2
has 3 actions (stay, move left or move right). If agent 2
is located in the far left and chooses to stay while agent
2 chooses push, the team receives a +10 reward. If the
button is pushed while agent 2 is moving left the team
receives a −30 reward. This negative reward is also
obtained if agent 2 stays still in the leftmost position
and agent 1 does not push the button. All rewards are
subject to additive Gaussian noise with mean 0 and
standard deviation equal to 1. Furthermore if agent 2
tries to move beyond an edge (left or right), it stays in
place and the team receives a Gaussian reward with 0
mean and standard deviation equal to 3. The episode
ﬁnishes after 5 timesteps or if the team gets the +10
reward (whichever happens ﬁrst). We ran the simula-
tion 20 times with diﬀerent seeds. Figure 1b shows the
average test return4 (without the added noise) of IQL,
LTQL, HystQ, DistQ, Qmix and Qtran. As can be
seen, LTQL and Qtran are the only algorithms capa-
ble of learning optimal team policies. In appendix 6.6
we specify the hyperparameters and include the learn-
ing curves of the Q-functions along with a discussion
on the performance of each algorithm.

5.3 Cowboy Bull Game

In this experiment we use a more complex environ-
ment, a challenging predator-prey type game with par-
tial observability, in which 4 cowboys try to catch a

4The average test return is the return following a greedy

policy averaged over 50 games.

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

Agent 2
a2
2
1

a3
0
2

a1
0
0

A
g
e
n
t

1

b1
b2

(a) Experiment 1

(b) Experiment 2

(c) Experiment 3

(d) Experiment 3

(e) Experiment 3

Figure 1: The dark curves show the mean over all seeds while the shaded region show the minimum and maximum
values. We clarify that in ﬁgure 1b the curves corresponding to HystQ and DistQ, which are partially occluded,
converge to 0.

bull (see ﬁgure 1c). The position of all players is a
continuous variable (and hence the state space is con-
tinuous). The space is unbounded and the bull can
move 20% faster than the cowboys. The bull follows a
ﬁxed stochastic policy, which is handcrafted to mimic
natural behavior and evade capture. Due to the un-
bounded space and the fact that the bull moves faster
than the cowboys, it cannot be captured unless all
agents develop a coordinated strategy (the bull can
only be caught if the agents ﬁrst surround it and then
evenly close in). The task is episodic and ends af-
ter 75 timesteps or when the bull is caught. Each
agent has 5 actions (the four moves plus stay). When
the bull is caught a +1 reward is obtained and the
team also receives a small penalty (−1/(4 × 75)) for
every agent that moves. Note that due to the reward
structure there is a very easily attainable Nash equi-
librium, which is for every agent to stay still (since in
this way they do not incur in the penalties associated
with movement). Figure 1d shows the test win per-
centage5 and ﬁgure 1e shows the average test return

for IQL, LTQL, HystQ, Qmix and Qtran. The best
performing algorithm is LTQL. HystQ learns a pol-
icy that catches the bull 80% of the times, although
it fails at obtaining returns higher than zero.
IQL
fails because the agents quickly converge to the pol-
icy of never moving (to avoid incurring in the nega-
tive rewards associated with movement). We believe
that the poor performance of Qmix in this task is
a consequence of its limited representation capacity
due to its monotonic factoring model. Qtran fails in
this complex scenario, which is in agreement with re-
sults reported in [Rashid et al., 2020] where Qtran also
shows poor performance in the Starcraft multi-agent
challenge (SMAC) [Samvelyan et al., 2019]. In the ap-
pendix we provide all hyperparameters and implemen-
tation details, we detail the bull’s policy and the ob-
servation function. All code6, a pre-trained model and
a video of the policy learned by LTQL are included as
supplementary material.

5Percentage of games, out of 50, in which the team suc-

ceeds to catch the bull following a greedy policy.

6Code

is

also

available

at

https://github.com/lcassano/Logical-Team-Q-Learning-
paper.

Lucas Cassano, Ali H. Sayed

6 CONCLUDING REMARKS

In this article we have introduced Logical Team Q-
Learning, which has the 5 desirable properties men-
tioned in the introduction. LTQL does not impose
constraints on the learned individual Q-functions and
hence it can solve environments where state of the art
algorithms like Qmix and Qtran fail. The algorithm
ﬁts in the centralized training and decentralized exe-
cution paradigm. It can also be implemented in a fully
distributed manner in situations where all agents have
access to each others’ observations and actions.

References

[Bellemare et al., 2013] Bellemare, M. G., Naddaf, Y.,
Veness, J., and Bowling, M. (2013). The arcade
learning environment: An evaluation platform for
general agents. Journal of Artiﬁcial Intelligence Re-
search, 47:253–279.

[Cassano et al., 2019] Cassano, L., Alghunaim, S. A.,
and Sayed, A. H. (2019). Team policy learning for
multi-agent reinforcement learning. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 3062–3066.

[Claus and Boutilier, 1998] Claus, C. and Boutilier,
C. (1998). The dynamics of reinforcement learn-
ing in cooperative multiagent systems. AAAI/IAAI,
1998(746-752):2.

[Duchi et al., 2011] Duchi, J., Hazan, E., and Singer,
Y. (2011). Adaptive subgradient methods for on-
line learning and stochastic optimization. Journal
of Machine Learning Research, pages 2121–2159.

[Foerster et al., 2017] Foerster, J., Nardelli, N., Far-
quhar, G., Afouras, T., Torr, P. H., Kohli, P.,
and Whiteson, S. (2017). Stabilising experience re-
play for deep multi-agent reinforcement learning. In
Proceedings International Conference on Machine
Learning, pages 1146–1155.

[Foerster et al., 2018] Foerster, J. N., Farquhar, G.,
Afouras, T., Nardelli, N., and Whiteson, S. (2018).
Counterfactual multi-agent policy gradients.
In
AAAI Conference on Artiﬁcial Intelligence.

[Guestrin et al., 2002a] Guestrin, C., Koller, D., and
Parr, R. (2002a). Multiagent planning with factored
MDPs. In Advances in neural information process-
ing systems, pages 1523–1530.

[Guestrin et al., 2002b] Guestrin, C., Lagoudakis, M.,
and Parr, R. (2002b). Coordinated reinforcement
learning. In ICML, volume 2, pages 227–234.

[Gupta et al., 2017] Gupta, J. K., Egorov, M., and
Kochenderfer, M. (2017). Cooperative multi-agent
control using deep reinforcement learning.
In In-
ternational Conference on Autonomous Agents and
Multiagent Systems, pages 66–83, Sao Paulo, Brazil.

[Hausknecht and Stone, 2015] Hausknecht, M. and
Stone, P. (2015). Deep recurrent Q-learning for par-
tially observable mdps. In AAAI Fall Symposium on
Sequential Decision Making for Intelligent Agents,
Virginia, USA.

[Hochreiter and Schmidhuber, 1997] Hochreiter,

and Schmidhuber, J. (1997).
memory. Neural computation, 9(8):1735–1780.

S.
Long short-term

[Kingma and Ba, 2014] Kingma, D. P. and Ba, J.
(2014). Adam: A method for stochastic optimiza-
tion. arXiv:1412.6980.

[Kok and Vlassis, 2004] Kok, J. R. and Vlassis, N.
(2004). Sparse cooperative Q-learning. In Proceed-
ings International Conference on Machine Learning,
page 61.

[Lauer and Riedmiller, 2000] Lauer, M. and Ried-
miller, M. (2000). An algorithm for distributed re-
inforcement learning in cooperative multi-agent sys-
tems. In Proc. International Conference on Machine
Learning (ICML), pages 535–542, Palo Alto, USA.

[Lin, 1992] Lin, L.-J. (1992). Self-improving reactive
agents based on reinforcement learning, planning
and teaching. Machine Learning, 8(3-4):293–321.

[Littman, 2001] Littman, M. L.

Value-
function reinforcement learning in markov games.
Cognitive Systems Research, 2(1):55–66.

(2001).

[Matignon et al., 2007] Matignon, L., Laurent, G.,
and Le Fort-Piat, N. (2007). Hysteretic Q-learning:
an algorithm for decentralized reinforcement learn-
In Proc.
ing in cooperative multi-agent teams.
IEEE/RSJ International Conference on Intelligent
Robots and Systems, pages 64–69, San Diego, USA.

[Mnih et al., 2013] Mnih, V., Kavukcuoglu, K., Silver,
D., Graves, A., Antonoglou, I., Wierstra, D., and
Riedmiller, M. (2013). Playing Atari with deep re-
inforcement learning. arXiv:1312.5602.

[Oliehoek et al., 2016] Oliehoek, F. A., Amato, C.,
et al. (2016). A Concise Introduction to Decentral-
ized POMDPs, volume 1. Springer.

[Omidshaﬁei et al., 2017] Omidshaﬁei, S., Pazis, J.,
Amato, C., How, J. P., and Vian, J. (2017). Deep
decentralized multi-task multi-agent reinforcement
learning under partial observability.
In Proceed-
ings International Conference on Machine Learning,
pages 2681–2690.

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

[Rashid et al., 2020] Rashid, T., Samvelyan, M.,
De Witt, C. S., Farquhar, G., Foerster, J., and
Whiteson, S. (2020). Monotonic value function
factorisation for deep multi-agent reinforcement
learning. Journal of Machine Learning Research,
21(178):1–51.

[Rashid et al., 2018] Rashid, T., Samvelyan, M.,
Schroeder, C., Farquhar, G., Foerster, J., and
Whiteson, S. (2018). Qmix: Monotonic value func-
tion factorisation for deep multi-agent reinforcement
learning. In Proceedings of the International Con-
ference on Machine Learning, pages 4295–4304.

[Samvelyan et al., 2019] Samvelyan, M., Rashid, T.,
Schroeder de Witt, C., Farquhar, G., Nardelli, N.,
Rudner, T. G. J., Hung, C.-M., Torr, P. H. S., Fo-
erster, J., and Whiteson, S. (2019). The StarCraft
In Proceedings of the In-
Multi-Agent Challenge.
ternational Conference on Autonomous Agents and
MultiAgent Systems, pages 2186–2188.

[Son et al., 2019] Son, K., Kim, D., Kang, W. J.,
Hostallero, D. E., and Yi, Y. (2019). Qtran: Learn-
ing to factorize with transformation for cooperative
multi-agent reinforcement learning. In International
Conference on Machine Learning, pages 5887–5896.

[Wang and Sandholm, 2003] Wang, X. and Sand-
holm, T. (2003). Reinforcement learning to play
an optimal nash equilibrium in team markov games.
In Proc. Advances in Neural Information Processing
Systems, pages 1603–1610, Vancouver, Canada.

[Watkins and Dayan, 1992] Watkins, C. and Dayan,
P. (1992). Q-learning. Machine Learning, 8(3-
4):279–292.

[Zhang et al., 2019] Zhang, K., Yang, Z., and Ba¸sar,
T. (2019). Multi-agent reinforcement learning:
A selective overview of theories and algorithms.
arXiv:1911.10635.

[Zhang et al., 2018] Zhang, K., Yang, Z., Liu, H.,
Zhang, T., and Ba¸sar, T. (2018). Fully decentralized
multi-agent reinforcement learning with networked
agents. In Proceedings of the International Confer-
ence on Machine Learning, pages 5872–5881.

P.,

[Sunehag et al., 2018] Sunehag,

Lever, G.,
Gruslys, A., Czarnecki, W. M., Zambaldi, V.,
Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo,
J. Z., Tuyls, K., et al. (2018). Value-decomposition
networks for cooperative multi-agent learning based
on team reward.
In Proceedings International
Conference on Autonomous Agents and Multiagent
Systems, pages 2085–2087.

[Sutton and Barto, 1998] Sutton, R. S. and Barto,
A. G. (1998). Reinforcement Learning: An Intro-
duction. MIT Press.

[Tampuu et al., 2017] Tampuu, A., Matiisen, T.,
Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,
J., and Vicente, R. (2017). Multiagent cooperation
and competition with deep reinforcement learning.
PloS one, 12(4).

[Tan, 1993] Tan, M. (1993). Multi-agent reinforce-
ment learning: Independent vs. cooperative agents.
In Proceedings International Conference on Ma-
chine Learning, pages 330–337.

[Tesauro, 2004] Tesauro, G. (2004). Extending Q-
learning to general adaptive multi-agent systems.
In Proc. Advances in Neural Information Process-
ing Systems, pages 871–878, Vancouver, Canada.

[Uspensky, 1937] Uspensky, J. V. (1937).

Introduc-
tion to mathematical probability. McGraw-Hill, New
York.

Lucas Cassano, Ali H. Sayed

Appendix

6.1 Proof of proposition 3

Consider the matrix game with two agents, each of which has two actions (A = {α; β}) and the following reward
structure:

Reward structure

Agent 2
β
α
−1
α
0
1
β −1

Agent 1

For this case q†, π†, q1,(cid:63), q2,(cid:63), π1,(cid:63) and π2,(cid:63) are given by:

q†(a1, a2)

α
α
0
β −1

β
−1
1

π†(a1, a2)

α β
0
0
1
0

α
β

q1,(cid:63)(a)

α
−1

β
1

q2,(cid:63)(a)

α
−1

β
1

π1,(cid:63)(a)

π2,(cid:63)(a)

α β
1
0

α β
1
0

Notice that as expected, q(cid:63) satisﬁes (3c). However, note that (3c) is also satisﬁed by the following q function
which is diﬀerent from q(cid:63).

Notice further that the team policy obtained by choosing actions in a greedy fashion with respect to q constitutes
a sub-optimal Nash equilibrium.

q(a = α) = 0,

q(a = β) = −1

(34)

6.2 Proof of Lemma 1

We start deﬁning:

qU = max{rmax(1 − γ)−1, max
k,s,ak

qk(s, ak)}

U (s, ak) = qU
qk

(35)

(36)

where rmax = maxs,¯a r(s, ¯a). We recall that to simplify notation we deﬁned E P,f r(s, ak, a−k, s(cid:48)) = r(s, ak, a−k).
The ﬁrst part of the proof consists in upper bounding any sequence of the form BK(cid:96)
U (s, ak),
where K(cid:96) ∈ N for all (cid:96). Applying operator BI to qk

I BKn−1

E BK0

· · · BK1

U (s, ak) we get:

I qk

E

BI qk

U (s, ak) = max (cid:8)qU , max

a−k

r(s, ak, a−k) + γqU

(cid:9) = qU

Therefore, BK0

I qk
BEqk

U (s, ak) = qk
U (s, ak) = E

U (s, ak) for any K0 ∈ N. Applying operator BE we get:
(cid:1)(cid:12)
(cid:12)an=arg max

(cid:0)r(s, ak, a−k) + γqU

qU ∀n(cid:54)=k ≤ max
a−k

s(cid:48)∼P

Eqk
B2

U (s, ak) ≤ E

s(cid:48)∼P

(cid:0)r(s, ak, a−k)+γ max
¯a(cid:48)

E

≤ max
a−k

s(cid:48)∼P

(cid:16)

r(s, ak, a−k) + γ max

BK1
E qk

U (s, ak) ≤

max
a−k
0 ,¯a1,··· ,¯aK1−1

(cid:32)K1−1
(cid:88)

E

i=0

an
r(s(cid:48), ¯a(cid:48))+γ2qU

(cid:1)(cid:12)
(cid:12)an=arg max

an
(cid:17)

¯a(cid:48)

r(s(cid:48), ¯a(cid:48)) + γ2qU

(cid:33)

γir(si, ak

i , a−k
i

)|s0 = s

+ γK1 qU

r(s, ak, a−k) + γqU

BE qn

U (s,an) ∀n(cid:54)=k

(37)

(38)

(39)

(40)

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

Further application of BI we get:

BI BK1

E qk

U (s, ak) ≤ max

(cid:26)

max
a−k
0 ,¯a1,··· ,¯aK1−1

(cid:32)K1−1
(cid:88)

E

i=0

γir(si, ak

i , a−k
i

)|s0 = s

+ γK1 qU ,

(cid:33)

max
a−k
0 ,¯a1,··· ,¯aK1

(cid:32) K1(cid:88)

E

i=0

γir(si, ak

i , a−k
i

)|s0 = s

+ γK1+1qU

(cid:33)

(cid:27)

=

max
a−k
0 ,¯a1,··· ,¯aK1−1

(cid:32)K1−1
(cid:88)

E

i=0

γir(si, ak

i , a−k
i

)|s0 = s

+ γK1qU

(41)

(cid:33)

Therefore, we conclude that BK2

I BK1

E BK0

I qk

U (s, ak) = BK1

E qk

U (s, ak). More generally, we can write:

BK(cid:96)
I

· · · BK1

E BK0

I qk

U (s, ak) = Bn

Eqk

U (s, ak) ≤

max
ak
0 ,¯a1,··· ,¯an−1

E

(cid:32)n−1
(cid:88)

i=0

γir(si, ak

i , a−k
i

)|s0 = s

+ γnqU

(cid:33)

≤ max
a−k

q†(s, ak, a−k) + γn(cid:0)qU − min

s

q†(s, ¯a)(cid:1)

max
¯a

(42)

where we deﬁne n to be the total number of times that operator BE is applied. Notice that if operator Bp is
applied N times, n is a random variable that follows a binomial distribution with total samples N and probability
p. Therefore, we get:

p qk
BN

U (s, ak) ≤ max
a−k

q†(s, ak, a−k) + γn(cid:0)qU − min

s

q†(s, ¯a)(cid:1)

max
¯a

To ensure that BN

p qk

U (s, ak) ∈ CU
δ1

we need:

δ1 ≥ γn(cid:0)qU − min

s

max
¯a

q†(s, ¯a)(cid:1) → n ≥ logγ

(cid:18)

δ1
qU − mins max¯a q†(s, ¯a)

(cid:19)

Since n follows a binomial distribution we get:

(43)

(44)

(cid:32)

P

n ≥

(cid:18)

(cid:22)
logγ

(cid:124)

δ1
qU − mins max¯a q†(s, ¯a)

(cid:123)(cid:122)
∆=no

(cid:33)

(cid:19)(cid:23)

(cid:125)

= 1 −

(cid:19)

no(cid:88)

n=0

(cid:18)N
n

pn(1 − p)N −n

(45)

Therefore we can conclude that:

P(cid:0)BN

p qk

U (s, ak) ∈ CU
δ1

(cid:1) ≥ 1 −

(cid:19)

no(cid:88)

n=0

(cid:18)N
n

pn(1 − p)N −n

CU
δ1

= (cid:8)qk|qk(s, ak) ≤ max
a−k

q†(s, ak, a−k) + δ1

∀(k, s, ak)∈(K, S, Ak)(cid:9)

Noting that by construction BN

p qk

U (s, ak) ≥ BN

p qk(s, ak) for all N ≥ 1, we get that:

P(cid:0)BN

p qk(s, ak) ∈ CU
δ1

(cid:1) ≥ 1 −

(cid:19)

no(cid:88)

n=0

(cid:18)N
n

pn(1 − p)N −n

(46)

(47)

(48)

For the special case where no < pN we can bound the cumulative distribution function of the binomial distribu-
tion using Hoeﬀding’s bound:

P(cid:0)BN

p qk(s, ak) ∈ CU
δ1

(cid:1) ≥ 1 − e−2N(p− no

N )2

(49)

which concludes the proof.

6.3 Proof of Lemma 2

Lucas Cassano, Ali H. Sayed

We start stating the following auxiliary lemma.
Lemma 3. If qk(s, ak) ∈ CU

0 then it holds that BN

p qk(s, ak) ∈ CU

0 for all N ≥ 0.

Proof. We start noting that if qk(s, ak) ∈ CU
BI qk(s, ak) = max (cid:8)qk(s, ak), max

0 then BEqk(s, ak) ∈ CU
E (r(s, ak, a−k, s(cid:48)) + γ max
a(cid:48),k

0 and BI qk(s, ak) ∈ CU
0 .
qk(s(cid:48), a(cid:48),k))(cid:9)

a−k

(a)

≤ max (cid:8) max
q†(s, ak), max
a−k
a−k
BEqk(s, ak) = E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max

E (r(s, ak, a−k, s(cid:48)) + γ max

q†(s(cid:48), ¯a(cid:48)))(cid:9) = max
a−k

q†(s, ak)

(50)

¯a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

an

qn(s,an) ∀n(cid:54)=k

a(cid:48)

≤ max
a−k

E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max
a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)

(b)
≤ max
a−k

E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max
¯a(cid:48)

q†(s(cid:48), ¯a(cid:48))(cid:1) = max
a−k

q†(s, ak)

(51)

where in (a) and (b) we used the fact that qk(s, ak) ∈ CU
0 it immediately follows that BN
CU

p qk(s, ak) ∈ CU

0 for all N ≥ 0.

0 . Since it holds that BEqk(s, ak) ∈ CU

0 and BI qk(s, ak) ∈

We follow by noting that applying operator BI L times to qk(s, ak) < maxa−k q†(s, ak, a−k) we get:

BI qk(s, ak) = max (cid:8)qk(s, ak), max

a−k
E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max
a(cid:48),k

≥ max
a−k

qk(s(cid:48), a(cid:48),k)(cid:1)

E (r(s, ak, a−k, s(cid:48)) + γ max
a(cid:48),k

BL
I qk(s, ak) = max

E
a−k
0 ,¯a1,··· ,¯aL−1

(cid:32)L−1
(cid:88)

i=0

γir(si, ak

i , a−k
i

) + γL max

ak
L

qk(s(cid:48), a(cid:48),k))(cid:9)

(cid:33)

qk(sL, ak

L)|s0 = s, ak

0 = ak

(cid:12)
(cid:12)BL

I qk(s, ak)− max
a−k

q†(s, ak, a−k)(cid:12)
(cid:12)

(cid:12)
(cid:12) max
ak
I qk(s, ak) < maxa−k q†(s, ak, a−k) from lemma 3. If (cid:15)(L) < δ2, we get:

qk(s, ak) − max

q†(s, ¯a)(cid:12)
(cid:12)

∆= (cid:15)(L)

¯a

s

(c)
≤ γL max

where in (c) we used BL

BL
I qk(s, ak) ∈ Cδ2
Cδ2 = (cid:8)qk|qk,(cid:63)(s, ak) − δ2 ≤ qk(s, ak) ≤ max

a−k

q†(s, ak, a−k) + δ2∀(k, s, ak)∈(K, S, Ak)(cid:9)

Lemma 4. If qk(s, ak) ∈ Cδ2 and δ2 is small enough, then it holds that BN

p qk(s, ak) ∈ CγN δ2 for all N > 0.

Proof. Assume δ2 satisﬁes the following relation:
(cid:0) max

δ2 < 2−1 min

s

¯a

q†(s, ¯a) −

max
¯a(cid:54)=arg max

q†(s,¯a)

¯a

q†(s, ¯a)(cid:1)

We clarify that the term in between parenthesis in the r.h.s. of relation (56) is the diﬀerence between the
optimal q-value and the second highest q-value for state s. Note that if qk(s, ak) ∈ Cδ2 then it trivially follows
that BI qk(s, ak) ∈ Cδ2 , for the case of BE we get:

BEqk(s, ak) = E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max

a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

an

qn(s,an) ∀n(cid:54)=k

Using equation (3a) and the fact that qk(s, ak) ∈ Cδ2 it follows:
qk,(cid:63)(s, ¯a) − δ2 ≤ max
ak

qk(s, ak)

max
ak

q†(s, ak,•, a−k) + δ2

(d)
< max

¯a

q†(s, ¯a) − δ2 = max
ak

qk,(cid:63)(s, ¯a) − δ2

qk(s, ak,•) ≤ max
a−k
arg max

ak,• =

ak(cid:54)=arg max

qk(s,ak)

ak

qk(s, ak)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

where in (d) we used condition (56). Combining equations (57) through (60) we get:

BEqk(s, ak) = E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max

a(cid:48)

qk(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

an

qn,(cid:63)(s,an) ∀n(cid:54)=k

= E (cid:0)r(s, ak, a−k, s(cid:48)) + γ max
a(cid:48)

qk(s(cid:48), a(cid:48)) + γ max

qk,(cid:63)(s(cid:48), a(cid:48)) − γ max

qk,(cid:63)(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

an

a(cid:48)

= qk,(cid:63)(s, ak) + γE (cid:0) max
a(cid:48)

qk(s(cid:48), a(cid:48)) − max

a(cid:48)

a(cid:48)
qk,(cid:63)(s(cid:48), a(cid:48))(cid:1)(cid:12)

(cid:12)an=arg max

an

qn,(cid:63)(s,an) ∀n(cid:54)=k

Combining equation (61) with the fact that qk(s, ak) ∈ Cδ2 we get:

qk,(cid:63)(s, ak) − γδ2 ≤ BEqk(s, ak) ≤ max
a−k

q†(s, ak, a−k) − γδ2

which completes the proof.

qn,(cid:63)(s,an) ∀n(cid:54)=k

(61)

(62)

Lemma 5. For any qk(s, ak) ∈ CU
N operators Bp includes at least L consecutive BI ’s. Where L is given by:

0 , δ2 > 0 and N ≥, it holds that BN

p qk(s, ak) ∈ Cδ2 as long as the sequence of

(cid:38)

(cid:32)

L =

logγ

maxs

δ2

(cid:12) maxak qk(s, ak) − max¯a q†(s, ¯a)(cid:12)
(cid:12)
(cid:12)

(cid:33)(cid:39)

(63)

Proof. The statement is an immediate consequence of lemmas 3 and 4 and relation (53). Relation (63) follows
from combining equation (53) and (cid:15)(L) < δ2.

Notice that the probability of applying operator BI at least L consecutive times when operator Bp is applied
N ≥ L times, is the same as the probability of obtaining at least L consecutive heads when a biased coin (with
probability of head 1−p) is tossed N times. This problem has been extensively studied and the result is available
in the literature. We state the following useful result from [Uspensky, 1937]:

Lemma 6. [Uspensky, 1937]: If a biased coin (with probability of head being 1 − p) is tossed N ≥ L times, the
probability of having a sequence of at least L consecutive heads is given by:

P(L) = 1 − βN,L + (1 − p)LβN −L,L
(cid:19)

(cid:98)N/(L+1)(cid:99)
(cid:88)

(−1)j

(cid:18)N − jL
j

βN,L =

j=0

(cid:0)p(1 − p)L(cid:1)j

Furthermore, if p > 0.5 the probability can be lower bounded as follows:

P(L) ≥ 1 −

1 − (1 − p)ξ1
pξ1(1 + L − Lξ1)

ξ−N
1 −

L
p

(1 − p)N +2

where 1 < ξ1 < 1 + L−1.

(64)

(65)

(66)

Combining lemmas 6 and 5 we can conclude that after N ≥ L > 0 applications of operator Bp to any set of K
qk(s, ak) ∈ CU

0 functions it holds:
P(cid:0)BN
p qk(s, ak) ∈ Cδ2
(cid:32)
(cid:38)

(cid:1) ≥ 1 − βN,L + (1 − p)LβN −L,L

L =

logγ

maxs

(cid:12)
(cid:12) maxak qk(s, ak) − max¯a q†(s, ¯a)(cid:12)
(cid:12)

δ2

(cid:33)(cid:39)

Cδ2 = (cid:8)qk|qk,(cid:63)(s, ak) − δ2 ≤ qk(s, ak) ≤ max

a−k

q†(s, ak, a−k) + δ2∀(k, s, ak)∈(K, S, Ak)(cid:9)

If p > 0.5 we can lower bound probability (67) by:

P(cid:0)BN

p qk(s, ak) ∈ Cδ2

(cid:1) ≥ 1 −

1 − (1 − p)ξ1
pξ1(1 + L − Lξ1)

ξ−N
1 −

L
p

(1 − p)N +2

(67)

(68)

(69)

(70)

Lucas Cassano, Ali H. Sayed

6.4 Tabular Logical Team Q-Learning

In the particular case where the MDP is deterministic (and hence E (r(s, ¯a, s(cid:48)) + γ maxa(cid:48) qk(s(cid:48), a(cid:48))) = r(s, ¯a, s(cid:48)) +
γ maxa(cid:48) qk(s(cid:48), a(cid:48))) the tabular version of Logical Team Q-learning is given by algorithm 2.

Algorithm 2 Tabular Logical Team Q-Learning for deterministic MDPs

Initialize: an empty replay buﬀer R and estimates (cid:98)qk
for iterations e = 0, . . . , E do

B and (cid:98)qk
U .

Sample T transitions (s, ¯a, r, s(cid:48)) by following some behavior policy which guarantees all joint actions are
sampled with non-zero probability and store them in R.
for iterations i = 0, . . . , I do

Sample a transition (s, ¯a, r, s(cid:48)) from R.
for agent k = 1, · · · , K do

if (cid:0)an = arg maxan (cid:98)qn(s, an) ∀n (cid:54)= k(cid:1) then
(cid:98)qk(s, ak) = (cid:98)qk(s, ak) + µ(cid:0)r + max

else if (cid:0)r + max

a (cid:98)qk(s(cid:48), a) > (cid:98)qk(s, ak)(cid:1) then

a (cid:98)qk(s(cid:48), a) − (cid:98)qk(s, ak)(cid:1)

(cid:98)qk(s, ak) = (cid:98)qk(s, ak) + µα(cid:0)r + max

a (cid:98)qk(s(cid:48), a) − (cid:98)qk(s, ak)(cid:1)

end if
end for

end for

end for

Algorithm 3 Tabular Logical Team Q-Learning

Initialize: an empty replay buﬀer R and estimates (cid:98)qk
for iterations e = 0, . . . , E do

B and (cid:98)qk
U .

Sample T transitions (s, ¯a, r, s(cid:48)) by following some behavior policy and store them in R.
for iterations i = 0, . . . , I do

Sample a transition (s, ¯a, r, s(cid:48)) from R.
for agent k = 1, · · · , K do
if an = arg maxan (cid:98)qn
B(s, ak) = (cid:98)qk
(cid:98)qk
U (s, ak) = (cid:98)qk
(cid:98)qk
end if
if (cid:0)r + max

B(s, an) ∀n (cid:54)= k then
a (cid:98)qk
a (cid:98)qk
B(s, ak)(cid:1) then
B(s, ak) + µα(cid:0)r + max
a (cid:98)qk

B(s, ak) + µ(cid:0)r + max
U (s, ak) + µ(cid:0)r + max

U (s(cid:48), a) > (cid:98)qk

a (cid:98)qk
B(s, ak) = (cid:98)qk
(cid:98)qk
end if
end for

U (s(cid:48), a) − (cid:98)qk
U (s(cid:48), a) − (cid:98)qk

B(s, ak)(cid:1)
U (s, ak)(cid:1)

U (s(cid:48), a) − (cid:98)qk

B(s, ak)(cid:1)

end for

end for

(cid:0)r + maxa (cid:98)qk(s(cid:48), a) > (cid:98)qk(s, ak)(cid:1), it would
If algorithm 2 were applied to a stochastic MDP, due to condition c2
be subject to bias, which would propagate through bootstrapping and hence could compromise its performance.
This can be solved by having a second unbiased estimate qU that is updated only when c1 is satisﬁed and use
this unbiased estimate to bootstrap. The resulting algorithm is shown in algorithm 3.

6.5 Matrix game additional results

We start specifying the hyperparameters. For IQL, DistQ, LTQL and Qtran we used a step-size equal to 0.1.
The α parameter for LTQL is equal to 1. The mixing network in Qmix has 2 hidden layers with 5 units each,
the nonlinearity used was the ELu and the step-size used was 0.05 (we had to make it smaller than the others

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

to make the SGD optimizer converge). We ﬁnally remark that due to the use of a NN in Qmix we had to train
this algorithm with 100 times more games (notice the x-axis in ﬁgure 2).

In ﬁgure 2 we show the convergence curves for IQL 2a, DistQ 2b, LTQL 2c-2f, Qmix 2g and Qtran 2h. Figures
2c and 2d correspond to the deterministic (algorithm 2) and general version (algorithm 3) of LTQL, respectively.
Figures 2e and 2f also show curves for LTQL but use diﬀerent seeds and show that this algorithm can converge
to either of the two following set of factored q-functions:

q1,(cid:63)(a1) = [2, 1]

q2,(cid:63)(a2) = [0, 2, 0]

or

q1,(cid:63)(a1) = [0, 2]

q2,(cid:63)(a2) = [0, 1, 2]

(71)

One interesting fact to note is that the suboptimal values of qk
B (in ﬁgures 2c and 2e) do not converge while the
same values do converge in the case of qk
U . The reason for this is that there is a parallel between including
the unbiased estimate qk
U in the RL algorithm and including an application of operator BE at the end of
the dynamic programming procedure described in theorem 1. The proof of the theorem shows that if such
operator is not included, only the optimal values of the estimates generated by Logical Team Q-learning converge
to qk,(cid:63), while the values corresponding to suboptimal actions oscillate in the region between qk,(cid:63)(s, ak) and
maxa−k q†(s, ak, a−k) (which is what happens when the unbiased estimate qk
U is not used in algorithm 2). Note
U is included all values qk converges to qk,(cid:63) for
that in the case of algorithm 3 where the unbiased estimate qk
all actions, not just the optimal ones (this is equivalent to including the operator BE after BN
p in the dynamic
programming setting). Below we show the joint q values generated by Qmix’s mixing network.

a1 (−3.49)
b1 (−0.74) −4.78 × 10−2
1.51 × 10−3
b2 (1.09)

A
g
e
n
t

1

Agent 2
a2 (1.83)
1.17
1.57

a3 (0.62)
6.86 × 10−1
1.09

Table 1: Qmix full results

Code is available at https://github.com/lcassano/Logical-Team-Q-Learning-paper.

6.6 Stochastic TMDP additional results

In this environment agents rely on the following observations. The observation corresponding to agent 1 is a
vector with two binary elements: the ﬁrst one indicates whether or not agent 2 is in the leftmost position, and
the second element indicates whether or not there is enough time for agent 2 to reach the leftmost position. The
observation corresponding to agent 2 is a vector with two elements: the ﬁrst one is the number of the position it
occupies and the second one is the same as agent 1 (whether there is enough time to reach the leftmost position).

All algorithms are implemented in an on-line manner with no replay buﬀer. (cid:15)-greedy exploration with a decaying
schedule is used in all cases ((cid:15) = max[0.05, 1 − epoch/2 × 105]). The step-size used is µ = 0.025 and the smaller
step-size for HystQ is µsmall = 10−2, in the case of Qmix we used µ = 10−3 to guarantee stability. The α
parameter for LTQL is equal to 1.

Figures 3 show the estimated q-values for LTQL corresponding to 4 diﬀerent observations at the 4 positions. Note
that in all ﬁgures the optimum action has the highest value and correctly estimates the return corresponding to
the optimal team policy (+10).

Figures 4 show the learning curves for IQL. Note that IQL fails at this environment because it has no mechanism
to discard the −30 penalty incurred due to moving to the left when agent 1 presses the button due to exploration.

Figures 5 show the learning curves for DistQ. The reason that this algorithm cannot solve this environment is
that it severely overestimates the value of choosing to move to the right whilst on the rightmost position. It is
well known that this is a consequence of the fact that DistQ only performs updates that increase the estimates
of the Q-values combined with the stochastic reward received when agent 2 “stumbles” against the right edge.

Figures 6 show the learning curves for HystQ. This algorithm cannot solve this environment because it has two
issues and the way to solve one makes the other worse. More speciﬁcally, one can be solved by increasing the
smaller step-size, while the other needs to decrease it. The ﬁrst issue is the same one that aﬀects DistQ, i.e.,

Lucas Cassano, Ali H. Sayed

(a) IQL

(b) DistQ

(c) qB (seed=0)

(d) qU (seed=0)

(e) qB (seed=1)

(f) qU (seed=1)

(g) Qmix

(h) Qtran

Figure 2: Matrix game. In all ﬁgures the red curves correspond to the three actions of agent 2, while the two
blue curves correspond to the two actions from agent 1.

the overestimation of the move right action in the rightmost position. Note that this can be ameliorated by
increasing the small step-size. The second issue is the penalty incurred due to moving to the left when agent
1 presses the button. This can be ameliorated by decreasing the small step-size. The fact that there is no
intermediate value for the small step-size to solve both issues is the reason that this algorithm cannot solve this
environment.

Figures 7 show the learning curves for Qmix. Qmix fails at this task due to the fact that its monotonic factoring
assumption is not satisﬁed at this task. The architecture used is as follows: we used tabular representation
for the individual q functions, and for the mixing and hypernetworks we used the architecture speciﬁed in
[Rashid et al., 2020]. More speciﬁcally, the mixing network is composed of two hidden layers (with 10 units
each) with ELu nonlinearities in the ﬁrst layer while the second layer is linear. The hypernetworks that output
the weights of the mixing network consist of two layers with ReLU nonlinearities followed by an activation
function that takes the absolute value to ensure that the mixing network weights are non-negative. The bias of
the ﬁrst mixing layer is produced by a network with a unique linear layer and the other bias is produced by a
two layer hypernetwork with a ReLU nonlinearity. All hypernetwork layers are fully connected and have 5 units.

Figures 8 show the learning curves for Qtran. Qtran succeeds at this task. However, it is important to remark

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 3: Learning curves for agent 2 of Logical Team Q-learning for a random seed.

that this is a tabular implementation of Qtran (an algorithm designed to be used in conjuntion with NNs in
complex environment), where the algorithm estimates the full joint q-function in tabular form, which is not
scalable and defeats the purpose of learning factored q-functions.

Lucas Cassano, Ali H. Sayed

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 4: Learning curves for agent 2 of IQL for a random seed.

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 8: Learning curves for agent 2 of Qtran for a random seed.

All code is available at https://github.com/lcassano/Logical-Team-Q-Learning-paper.

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 5: Learning curves for agent 2 of DistQ for a random seed.

6.7 Cowboy bull game additional results

The bull’s policy is given by the pseudocode shown in algorithm 4.

Algorithm 4 Bull’s policy.

if distance to all predators > 10 (this circumference is depicted by the blue line in ﬁgure 1c) then

Natural foraging behavior: Stay still with 90% probability, otherwise make a small move in a random
direction.

else

if the maximum angle formed by two predators is > 108o then

There’s a hole to escape: Escape through the direction in between these two predators.

else if distance to farthest predator - distance to closest predator > 5 then

There’s no hole, but one predator is much closer than the others so run in the direction opposite to this
predator.

else

No way out (scared): Stay still with 70% probability, otherwise make a fast move in a random direction.

end if

end if

We now specify the hyperparameters for Logical Team Q-learning. All NN’s have two hidden layers with 50
units and ReLu nonlinearities. However, for each Q-network, instead of having one network with 5 outputs, we
have 5 networks each with 1 output (one for each action). At every epoch the agent collects data by playing
32 full games and then performs 50 gradient backpropagation steps. Half of the 32 games are played greedily
and the other half use a Boltzmann policy with temperature bT that decays according to the following schedule
bT = max[0.05, 0.5 × (1 − epoch/15 × 103)]. We use this behavior policy to ensure that there are suﬃcient
transitions that satisfy condition c1 and that also there are transitions that satisfy c2. The target networks are
updated every 50 backprop steps. The capacity of the replay buﬀer is 2.105 transitions, the mini-batch size is

Lucas Cassano, Ali H. Sayed

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 6: Learning curves for agent 2 of HystQ for a random seed.

1024, α = 1, we use a discount factor equal to 0.99 and optimize the networks using the Adam optimizer with
initial step-size 10−5.

The hyperparameters of the HystQ implementation are the same as those of LTQL, the ratio of the two step-sizes
used by HystQ is 0.1. To run IQL we used the implementation of HystQ with the ratio of the two step-sizes set
to 0.

The architecture used by Qmix is the one suggested in [Rashid et al., 2020] with the exception that, for fairness,
the individual Q-networks used the same architecture as the ones used by the other algorithms (i.e., 5 networks
with a unique output as opposed to 1 network with 5 outputs). All hidden layers of the hypernetworks as well
the mixing network have 10 units. In this case we did 5 backprop iterations per epoch and the target network
update period is 15. We use a batch size of 256, a discount factor equal to 0.98 and optimize the networks with
the Adam optimizer with initial step-size 10−6. In this case the behavior policy is always Boltzmann with the
following annealing schedule for the temperature parameter bT = max[0.005, 0.05 × (1 − epoch/25 × 103)].

The Qtran-base variant was implemented. The individual Q-networks have the same architecture used by the
other algorithms. The joint Q-network has two hidden layers with 60 units each, the input of this network is
the global state concatenated with the agents’ actions with one hot encoding. The value function network has
only one hidden layer with 25 units and its input is the global state. The target networks are updated every
50 backprop steps. The capacity of the replay buﬀer is 2.105 transitions, the mini-batch size is 1024, we use a
discount factor equal to 0.99 and optimize the networks using the Adam optimizer with initial step-size 10−5.

The batch size, Boltzmann temperature value, learning step-size and target update period were chosen by grid
search.

All implementations use TensorFlow 2. The code is available at https://github.com/lcassano/Logical-Team-Q-
Learning-paper. Running one seed for one agent takes approximately 12 hours in our hardware (2017 iMac with
3.8 GHz Intel Core i5 and 16GB of RAM).

Logical Team Q-learning: An approach towards factored policies in cooperative MARL

(a) Leftmost position and t = 3

(b) Slot adjacent to leftmost and t = 2

(c) Slot adjacent to rightmost and t = 1

(d) Rightmost position and t = 0

Figure 7: Learning curves for agent 2 of Qmix for a random seed.

