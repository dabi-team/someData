0
2
0
2

n
u
J

4
2

]
L
C
.
s
c
[

2
v
1
4
6
2
1
.
6
0
0
2
:
v
i
X
r
a

Exploring Software Naturalness through
Neural Language Models

Luca Buratti∗
IBM Research
luca.buratti1@ibm.com

Saurabh Pujar∗
IBM Research
saurabh.pujar@ibm.com

Mihaela Bornea∗
IBM Research
mabornea@us.ibm.com

Scott McCarley∗
IBM Research
jsmc@us.ibm.com

Yunhui Zheng
IBM Research
zhengyu@us.ibm.com

Gaetano Rossiello
IBM Research
gaetano.rossiello@ibm.com

Alessandro Morari
IBM Research
amorari@us.ibm.com

Jim Laredo
IBM Research
laredoj@us.ibm.com

Veronika Thost
IBM Research
veronika.thost@ibm.com

Yufan Zhuang
IBM Research
yufan.zhuang@ibm.com

Giacomo Domeniconi
IBM Research
giacomo.domeniconi1@ibm.com

Abstract

The Software Naturalness hypothesis argues that programming languages can be
understood through the same techniques used in natural language processing. We
explore this hypothesis through the use of a pre-trained transformer-based lan-
guage model to perform code analysis tasks. Present approaches to code analysis
depend heavily on features derived from the Abstract Syntax Tree (AST) while
our transformer-based language models work on raw source code. This work is
the ﬁrst to investigate whether such language models can discover AST features
automatically. To achieve this, we introduce a sequence labeling task that directly
probes the language model’s understanding of AST. Our results show that trans-
former based language models achieve high accuracy in the AST tagging task.
Furthermore, we evaluate our model on a software vulnerability identiﬁcation task.
Importantly, we show that our approach obtains vulnerability identiﬁcation results
comparable to graph based approaches that rely heavily on compilers for feature
extraction.

1 Introduction

The ﬁelds of Programming Languages (PL) and Natural Language Processing (NLP) have long re-
lied on separate communities, approaches and techniques. Researchers in the Software Engineering
community have proposed the Software Naturalness hypothesis [12] which argues that programming
languages can be understood and manipulated with the same approaches as natural languages.

The idea of transferring representations, models and techniques from natural languages to program-
ming languages has inspired interesting research. However, it raises the question of whether lan-
guage model approaches based purely on source code can compensate for the lack of structure and

∗Equal Contribution. In no particular order.

Preprint. Under review.

 
 
 
 
 
 
semantics available to graph-based approaches which incorporate compiler-produced features. The
application of language models to represent the source code has shown convincing results on code
completion and bug detection [11, 18]. The use of structured information, such as the Abstract Syn-
tax Tree (AST), the Control Flow Graph (CFG) and the Data Flow Graph (DFG), has proven to be
beneﬁcial for vulnerability identiﬁcation in source code [45]. However, the extraction of structured
information can be very costly, and requires the complete project. In the case of the C and C++ lan-
guages, this can only be done through complete pre-processing and compilation that includes all the
libraries and source ﬁles. Because of these requirements, approaches based on structural informa-
tion are not only computationally expensive but also inapplicable to incomplete code, for instance a
pull request.

This work explores the software naturalness hypothesis, employing the pre-training/ﬁne-tuning
paradigm widely used with transformer-based [37] language models (LMs) [5, 38] to address tasks
involving the analysis of both syntax and semantics of the source code in the C language. To in-
vestigate syntax, we ﬁrst introduce a novel sequence labeling task that directly probes the language
model’s understanding of AST, as produced by Clang [48]. Furthermore, we investigate the capa-
bilities of LMs in handling complex semantics in the source code through the task of vulnerability
identiﬁcation (VI). All of our experiments involved LMs pre-trained from scratch on the C language
source code of 100 open source repositories. The use of language models with source code rather
than natural language leads to multiple challenges. Data sparsity is a major problem when building
language models, leading to out-of-vocabulary (OOV) terms and poor representations for rare words.
These issues are particularly severe for PL because variable and function names can be of almost
arbitrary length and complexity.

There is a tradeoff between the granularity of tokenization and availability of long-range context for
the LM. Fine-grained tokenizations break identiﬁers into many small common tokens, alleviating
issues with rare tokens, but at the risk of spreading important context across too many tokens. We ad-
dress the OOV and rare words problems by investigating three choices for tokenization, which span
the context/vocabulary size tradeoff from the extreme of character-based tokenization to subword
tokenization styles familiar to the NLP community. We indicate that the choice of the pre-training
objective is closely connected to the choice of the vocabulary. In particular, with character based to-
kenization the pre-training task seems too easy. We introduce a more difﬁcult, whole word masking
(WWM) pre-training objective.

In our experiments, we show that our language model is able to effectively learn how to extract
AST features from source code. Moreover, we obtain compelling results compared to graph-based
methods in the vulnerability identiﬁcation task. While current approaches to code analysis depend
heavily on features derived from the AST [45, 43, 43], our approach works using raw source code
without leveraging any kind of external features. This a major advantage, since it avoids a full
compilation phase to avoid the extraction of structural information. Indeed, our model can identify
vulnerabilities during the software development stage or in artifacts with incomplete code, which is
a valuable feature to increase productivity. We show the merits of simple approaches to tokenization,
obtaining the best results using the character based tokenization with WWM pre-training.

The contributions of this work are summarized as follows: 1) we investigate the application of
transformer-based language models for complex source code analysis tasks; 2) we demonstrate that
character-based tokenization and pre-training with WWM eliminate the OOV and rare words; 3) this
work is the ﬁrst to investigate whether such language models can discover AST features automat-
ically; 4) our language model outperforms graph-based methods that use the compiler to generate
features.

2 Model: C-BERT

We investigate the software naturalness hypothesis by pre-training from scratch a transformer-based
LM, based on BERT [5], on a collection of repositories written in C language. Then, we ﬁne-tune it
on the AST node tagging (Section 3) and vulnerability identiﬁcation (Section 4) tasks.

However, the application of BERT on source code requires re-thinking the tokenization strategies.
Indeed, tokenizers based on language grammar are unsuitable for use with transformer language
models because of the nearly unlimited resulting vocabulary size. (There are more than 4 million
unique C tokens in our pre-training corpus.) This greatly increases the training time of the language

2

Table 1: Average number of tokens per ﬁle in pre-training dataset. For comparison there were 3,272
C tokens per ﬁle in the pre-training data. Column 4 refers to the VI task (Section 4)

Tokenizer
Char
KeyChar
SP E

Vocab Size
(tokens)
103
135
5,000

Pre-training data
(tokens/ﬁle)
15,594
15,011
5,500

VI data
(tokens/ﬁle)
1789
1469
558

model, and introduces unacceptably many OOV items in held-out data if truncated to typical vocab-
ulary sizes. Encouraged by recent work on subword tokenization for source code [18] we explore
three subword tokenizer choices that reduce the vocabulary size and the impact of OOV and rare
words.

2.1 Tokenizers

Character (Char) We investigate the extreme of subword tokenization: an ASCII vocabulary, an
alternative dismissed by [18]. Since our datasets are almost entirely ASCII characters, we are able to
reduce the total vocabulary size to 103 (including [CLS], [SEP], [MASK] and [UNK].) This choice
minimizes vocabulary size, at the cost of limiting the size of context window available to the model.

Character + Keyword (KeyChar) Most programming languages have a relatively small number
of keywords. By augmenting the Char vocabulary with 32 C language keywords [19], we are able
to treat each keyword as a token, rather than breaking it up into individual characters or subwords.
This reduces the number of tokens per document (and increases the available context window), as
can be seen in Table 1.

Sentencepiece (SP E) Neural NLP models almost always use a subword tokenizer such as Senten-
cepiece [20] or Byte-Pair Encoding (BPE) [35]. We follow other transformer based models and use
a sentencepiece tokenizer, with vocabulary size chosen to be 5000. This includes all C keywords in
the vocabulary, along with many common library functions and variable names. We ﬁnd that this
leaves an almost-negligible number of tokens (< 0.001%) out-of-vocabulary in held-out data. Table
1 shows that SP E tokenization reduces the number of tokens per document by about a factor of 3
compared to Char and KeyChar.

2.2 Transformer Based Language Models.

Our model architecture is a multi-layer bidirectional transformer [37] based on BERT [5], which
we refer to as C-BERT. As in [5], the language model component of our model is ﬁrst pre-trained
on a large unlabeled corpus. Task-speciﬁc training ("ﬁne-tuning") is then continued with different
task-speciﬁc objective functions on additional training data labeled for the tasks. In this section, we
brieﬂy review the four objective functions and associated classiﬁer layers involved in training our
models. In all cases, our model is trained on ﬁxed-width windows of tokens, to which we prepend
a [CLS] token to indicate the beginning of the sequence and appennd [SEP ] token to indicate the
end of the sequence.
For every input sequence X = [x1 = CLS, x2, . . . , xT −1, xT = SEP ], the language model outputs
a sequence of contextualized token representations H = [h1 = hCLS, h2, . . . , hT = hSEP ] ∈
RT ×768 where ht ∈ R768.
Masked Language Model (MLM) Pre-training Objective A small percentage of tokens are se-
lected for "masking", as described in [5]. A linear layer WLM ∈ R768×|V | (V denotes the C-BERT
vocabulary associated with a tokenization) followed by softmax is added on top of C-BERT represen-
tation to compute the probability distribution over V for each masked token. The objective function
is maximum likelihood of the true labels as computed from

pLM = sof tmax(HWLM) ∈ R768×|V |

(1)

and evaluated over only the masked tokens. The model trained with this objective is used to initialize
the task-speciﬁc models.

3

Whole Word Masked (WWM) Pre-training Objective The masked language model task used
to pretrain C-BERT depends heavily on the tokenization. Particularly with Char tokenization, many
masked tokens are ASCII characters within a variable name that is repeated elsewhere in the context.
We suspect in this case that the MLM task is too easy to adequately pretrain C-BERT, and conjecture
that making the pre-training task more difﬁcult by masking longer spans of source code could result
in stronger models on downstream tasks. Indeed, there is evidence that such techniques are beneﬁcial
in NLP. [9, 16]

As an alternative, we choose types of strings (for example, text strings that are legal variable or
function names) to mask from the pre-training dataset, and write regular expressions to extract them.
These matches are stored in a trie to create a dictionary. During pre-training, when a token is selected
for masking, we identify which dictionary strings contain the token and then add the entire span of
tokens from that string to the set of tokens masked in the MLM objective. We change the masking
rate from 15% in MLM, to 3% for Char and KeyChar tokenizers and 9% for SP E. The new
probabilities ensure that the average number of masked tokens remains roughly the same as MLM.
In this sequence labeling task, we add a linear layer WA ∈
AST Fine-tuning Objective.
R768×|VAST | followed by softmax. Here VAST represents the set of AST labels. We ﬁne tune the
model using the cross entropy between the gold AST labels and
pAST = sof tmax(HWAST)

(2)

across all tokens.

VI Fine-tuning Objective. The VI is a binary classiﬁcation task and depends only on the hCLS
embedding. We add a single linear layer w ∈ R768. We ﬁne-tune using the cross entropy between
the true labels and

pV I = sigmoid(h⊤

CLSw)

(3)

evaluated across all context windows.

2.3 Pre-training Corpus

For the pre-training dataset we chose the top-100 starred (at least 5500 stars) GitHub C language
repositories. Forks and clones were excluded to avoid duplication. These include well-maintained,
widely-used repositories such as linux, git, postgres, and openssl. The total size of the pre-training
corpus is about 5.8 GB, much less than the corpus used to train BERT, or source code versions of
BERT such as [17] and [7]. Comments were removed to keep only tokens related to code. We
implemented the de-duplication strategy in [1] and found only about 40 ﬁles out of 60,000+ to be
duplicates.

2.4 Pre-training Details

All of our models are built using the Huggingface Pytorch Transformers library [41]. Regardless
of the type of tokenizer used, our model consists of 12 layers of transformers, 768-dimensional
embeddings, 12 attention heads/layers, the same architectural size of BERTBASE [5]. We divide
the training data into train, dev and test sets. Training is stopped based on dev set accuracy and
models are selected for the VI task based on test set accuracy. We train on 8 nodes, with 32 GPUs,
512 maximum sequence length and batch size of 8, per GPU, for a total batch size of 256. Learning
rate and number of epochs vary based on the tokenizer. We found that Char and KeyChar models
learn faster compared to the SP E models. In total we selected 6 pretrained models, based on a
combination of 3 tokenizers and 2 masking strategies. We experiment with different learning rates
(LR). Char models with both MLM and WWM were trained with LR 10−4 and reach peak accuracy
relatively quickly compared to the respective KeyChar and SP E models. For KeyChar we used
LR of 2 × 10−5 for MLM and 10−4 for WWM. For SP E we used LR of 2 × 10−5 for MLM and
2 × 10−6 for WWM.

3 AST Node Tagging Task

The Abstract Syntax Tree (AST) is an important structure produced by compilers and it has been
used in prior works to improve the performance of code analysis tasks, such as vulnerability de-
tection [45]. As a step toward full language model understanding of the AST, in this section, we

4

Table 2: Example with ∗ used as multiplication (on the left); Example with ∗ used as pointer deref-
erence (on the right)

token
if
(
lensum
∗
9
/
10
>
maxpos

cursor_kind
IF_STMT
IF_STMT
DECL_REF_EXPR
BINARY_OPERATOR
INTEGER_LITERAL
BINARY_OPERATOR
INTEGER_LITERAL
BINARY_OPERATOR
DECL_REF_EXPR

token_kind
KEYWORD
PUNCTUATION
IDENTIFIER
PUNCTUATION
LITERAL
PUNCTUATION
LITERAL
PUNCTUATION
IDENTIFIER

cursor_kind
token
CXX_UNARY_EXPR
sizeof
PAREN_EXPR
(
∗
UNARY_OPERATOR
∗
UNARY_OPERATOR
DECL_REF_EXPR
s
->
MEMBER_REF_EXPR
coarse MEMBER_REF_EXPR
PAREN_EXPR
)

token_kind
KEYWORD
PUNCTUATION
PUNCTUATION
PUNCTUATION
IDENTIFIER
PUNCTUATION
IDENTIFIER
PUNCTUATION

describe a sequence labeling problem where the goal is to capture the syntactic role of each compo-
nent of a linearized AST. Part-Of-Speech (POS) tagging is an analogous problem in NLP [25].

In detail, we propose two tasks to show that our models can discover the AST structure. For each task
we represent the source code as a sequence of programming tokens, where each token is labeled with
attributes token_kind and a cursor_kind . Gold labels for token_kind are produced by the compiler’s
tokenizer component, while gold labels for the cursor_kind are produced by the compiler’s parser
component which maps each token to its cursor node in the AST. The role of the language model
is to predict the correct token_kind and cursor_kind for every programming token by examining the
source code only.

To generate the gold data for our AST node tagging task we used Clang [48], an open-source C/C++
compiler with a modular design. Clang2 has 5 token_kind labels and 209 cursor_kind labels. The
compiler handles the tokenization of the input source ﬁle and assigns the attribute token_kind to each
source code token.

The AST is represented as a directed acyclic graph of cursor nodes. The cursor_kind is an attribute
of the cursor to indicate the type of the node in the AST. During parsing, every token in the source
code is mapped to the corresponding cursor node in the AST according the rules of the C language
grammar.

The BERT-based language models and the baseline model solved the token_kind task with ease.
Both accuracy and F 1 were greater than 99% across all three tokenizations on our data sets. On
this basis we concluded that small differences in performance on token_kind tasks were unlikely to
provide useful information about the strengths and weakness of our model. Further discussion will
focus entirely on the cursor_kind task.

Predicting Clang’s cursor_kind label is similar to the task of word sense disambiguation in NLP. Just
as the English word ’bank’ may refer to a ﬁnancial institution, or to the edge of a river, many tokens
produced by Clang have ambiguous meanings. For example the * operator is used as a binary
operator, for multiplication, and as a unary operator, for dereferencing a pointer variable (among
other meanings.) We show examples of Clang output for two C expressions containing * in Table 2.
In

the * is used for multiplication, and is labeled BINARY_OPERATOR by Clang. In

if ( lensum * 9 / 10 > maxpos )

sizeof(**s->coarse)

(4)

(5)

both instances of the * are used for pointer dereference, and are labeled UNARY _OPERATOR. The
parentheses also have different labels in the two examples, and two meaning of the character > are
implicitly distinguished by its incorporation in the -> token.

Datasets We created two annotated datasets using the code from the FFmpeg [46] and QEMU [47]
GitHub repositories. FFmpeg is a collection of libraries and tools to process multimedia content
such as audio, video, subtitles and related metadata. It contains 1, 751 ﬁles with over 6 million
Clang tokens. QEMU is a generic machine and user space emulator and virtualizer containing

2We use libclang python bindings atop Clang 11.0

5

2, 152 source ﬁles and over 7 million Clang tokens. Both projects are open source. We split each
dataset by randomly assigning the ﬁles in a 70/10/20 ratio between train, dev and test sets.

We use the libclang python bindings atop Clang 11.0 to parse the C/C++ source ﬁles and traverse
the AST. In order to get the precise context information (e.g. cursor_kind for tokens), we make
sure the source ﬁles can be correctly compiled by intercepting the build process to obtain necessary
compiler options and headers. We apply the LM tokenizer to the Clang output on each dataset and
retain a unique, deterministic alignment between the Clang tokens and the LM’s tokens to produce
predictions using C-BERT, as explained in Section 2.2.

Fine-tuning Details We initialize from pre-trained C-BERT models for the three different tok-
enization strategies described in Section 2 after validating that these models yielded acceptable
performance on the VI task. The sequence classiﬁer head in Equation 2 was randomly initialized.

Selected experiments were repeated with 5 random number generator seeds in order to gauge ﬂuc-
tuations. Since BERT is conﬁgured to handle a ﬁnite context window, the training ﬁle was divided
into non-overlapping windows of maximum 250 tokens. For the dev and test data a sliding window
approach is used to create chunks of 250 tokens with 32 tokens overlap. When a token appears in
multiple segments, its prediction is determined through voting. Models were trained using a batch
size of 16, and for a maximum of 10 epochs or 24 hours, with ﬁnal models selected on the basis of
dev-set F 1. We investigated learning rates from the set of {5 × 10−5, 2 × 10−5, 10−4} For most
experiments the learning rate of 2 × 10−5 was used - we selected 10−4 if it yielded notably better
accuracy during the learning rate exploration. Most experiments were run on NVidia K-80s, with
learning rate exploration on NVidia V-100s.

Experimental Results Test set results for the cursor_kind tasks are shown in Table 3. We compare
our transformer LMs with a BiLSTM baseline that uses the same tokenization as C-BERT. Both
systems produce cursor_kind predictions for every LM token in the input. We use the alignment
between C tokens and LM tokens to report C token level F1 and accuracy for each dataset.

Two trends are immediately apparent. First, BERT-based language models out-performed BiLSTM-
based language models across all three tokenizations, and on both the FFmpeg and QEMU data sets.
Second, performance on the FFmpeg set was considerably higher than for the QEMU dataset, for
both the BERT and the BiLSTM-based models.

Differences in tokenization style led to only small changes in performance for C-BERT. These dif-
ferences were not statistically signiﬁcant - the absolute differences between the min and max F 1
across ﬁve initialization seeds ranged from 1% to 3% for the different tokenizations of QEMU, and
ranged from 0.2% to 0.4% for the different tokenizations of FFmpeg. It appears that neural language
models are able to infer enough higher-level structures of programming languages to perform these
tasks without additional assistance from the tokenizer. For the BiLSTM results the tokenization
styles show no impact on the FFmpeg dataset where both the accuracy and the F1 score are high.
The tokenization has an impact on the QEMU dataset where the SP E tokenizer performs the best.

We analyzed the confusion matrix for both FFmpeg and QEMU dev sets. In both, the top 2 errors
were system predictions of COMPOUND_STATEMENT or DECL_REF_EXPR when the reference
indicated O (for outside of a cursor_kind ). Clang typically indicates O for #include statements,
macro deﬁnitions, and architecture-dependent conditional compilations that are skipped. In particu-
lar, we note that both repositories contain long macro deﬁnitions which superﬁcially resemble active
code, but which require fairly long context to identify as a macro deﬁnition. The most common error
not involving ’O’ was predicting DECL_REF_EXPR for COMPOUND_STMT.

We also experimented with BERT models pre-trained with the whole-word masking objective. These
models hurt performance here, whereas (we will see in Section 4) they improve VI performance.
For example, on the QEMU dev-set, F 1 dropped from 93.3% to 82.4% for Char tokenization,
from 93.3% to 88.2% for SP E, and from 94.1% to 90.8% for KeyChar tokenization. Further
investigation is needed to determine whether this difference is due to the syntax/semantics focus of
the two tasks or whether the WWM is simply more beneﬁcial to a task such as VI with a lower
baseline performance.

6

Table 3: Test set accuracy and F1 for the AST cursor_kind tasks

Model

Tokenizer
Char
BiLSTM SP E

C-BERT

KeyChar
Char
SP E
KeyChar

FFmpeg

QEMU

Acc
94.96
94.69
95.68
97.10
97.72
97.73

F1
95.71
95.52
96.58
97.72
98.29
98.31

Acc
71.68
74.12
66.20
81.06
81.11
80.78

F1
80.53
81.58
76.19
87.43
87.79
87.49

4 Vulnerability Identiﬁcation Task (VI)

While VI has been investigated previously (e.g. Draper[23], Juliet[28], Devign[45]) there is no
satisfactory comparable baseline. The Draper dataset [23] has many false positives, and the Juliet
dataset [28] consists of synthetic data. Devign [45] released appropriate natural data, but there are
limitations which make it impossible to compare with their results. Only 2 out of 4 projects from
the dataset were released, and the training/test split was not speciﬁed. Also, unspeciﬁed data was
omitted from their published results because of computational complexity and limitations of Joern
[42] preprocessing, a key step in their pipeline. Furthermore, lack of details about a key feature of
their GGNN [21] network, the pooling layer, prevent exact replication of the "Devign composite"
model.

Data Description Our dataset consists of functions from the FFmpeg and QEMU projects with
non-vulnerable/vulnerable labels as released by Devign, with the duplicates removed. We also use
these two datasets to create a combined dataset. The original FFmpeg dataset contains 9,683 exam-
ples and QEMU contains 17,515 examples. We call these three datasets f ull FFmpeg, f ull QEMU
and f ull combined datasets. In order to implement the GGNN baseline, we use Joern to compute
multiple graph features, including AST, just like Devign. We skip some examples that yield a com-
pilation error with Joern. With the exclusion of problematic samples, we get reduced versions of the
full datasets, and we call the resulting datasets FFmpeg reduced, QEMU reduced and combined
reduced. The reduced dataset contains 6169 FFmpeg and 14,896 QEMU examples, which is a total
reduction of 6133 relatively large functions.

Fine-tuning Details We initialize with each of six pre-trained models described in Section 2.4.
The Huggingface implementation of BERT truncates function tokens beyond the count of 512. We
call this default approach T runcate. We ﬁne-tune according to Eq. 3. As can be seen in Table 1,
column 4, using Char and KeyChar tokenizers often pushes the context token count beyond 512.
With Char tokenizer, about 80% of FFmpeg functions have more than 510 tokens. We aggregate
such functions, similar to [29], by ﬁrst breaking the input example into N different segments of
maximum size 510 and then using a BiLSTM layer to combine the [CLS] output. This output is
passed to the linear layer for classiﬁcation. We train for 10 Epochs, with learning rate of 2 × 10−5
for KeyChar, SP E and 8 × 10−6 for Char, max sequence length of 512 and batch size of 4.

Baselines The Naive baseline shows the accuracy if all instances are labeled vulnerable. Just like
Devign, we use BiLSTM and CNN as baselines. Our strongest baseline, GGNN, is implemented as
described in the Devign paper (GGRN composite) using the same graph features. Our implementa-
tion of the Devign composite model, which is GGNN with pooling, did not improve upon GGNN
because we lacked adequate information on how to implement the pooling layer. We did not include
Devign composite into our results. We train BiLSTM and CNN baselines on all the six datasets.
We train GGNN only on the reduced datasets due to Joern compilation errors. Both BiLSTM and
GGNN baselines are initialized with Word2Vec [27] embeddings trained on source code from the
FFmpeg and QEMU projects.

Experiment Results We report results with accuracy as the evaluation metric rather than F1 score
because our datasets are well-balanced, with 40%-55% labeled as vulnerable. The experiment re-
sults are in Table 4. C-BERT models, with aggregation outperform the strongest GGNN baseline by

7

Table 4: Test set accuracy for the VI task on the f ull and reduced ("red") datasets; C-BERT model id
indicates tokenization (C|K|S), LM masking objective (M|W) and aggregation(T|B); Aggr indicates
the aggregation method

LM + Tokenizer

Masking Aggr.

Model
Naive
BiLSTM
CNN
GGNN
CMT
KMT
SMT
CWB
KWB
SMB

C-BERT Char
MLM
C-BERT KeyChar MLM
C-BERT SP E
MLM
C-BERT Char
WWM
C-BERT KeyChar WWM
C-BERT SP E
MLM

QEMU

full
42.4
61.6
60.5

red
41.1
64.0
63.3

Combined
FFmpeg
red
full
red
full
42.7
45.5
46.5
51.1
61.5
57.6
58.3
59.5
57.3
59.9
56.9
58.7
NA 61.1 NA 65.8 NA 63.2
57.8
52.7
Truncate
58.1
55.5
Truncate
58.0
Truncate
57.7
66.2
BiLSTM 62.2
64.3
BiLSTM 58.0
65.4
BiLSTM 60.7

58.7
59.5
60.5
68.1
67.7
66.4

54.7
57.0
54.8
65.5
61.9
62.8

55.5
56.2
57.4
63.5
61.5
63.6

57.3
57.5
59.3
65.8
64.1
66.1

a reasonable margin of 3-4 points across all datasets. They also perform better than BiLSTM and
CNN baselines on both the full and reduced datasets.

The three lines CMT, KMT and SMT show the effect of varying the tokenization strategy when
trained with the MLM objective. KMT performs better than CMT across all datasets and SMT
has the best overall performance. As expected, tokenizations which enable the model to see longer
context windows (KeyChar and then SP E) achieve generally better results. All of our C-BERT
MLM models improve signiﬁcantly upon the naive baseline.

CWB, KWB and SMB are the best results for each of the three tokenizers. The best model is CWB,
showing the highest accuracy on most datasets. On the QEMU full and combined full, the CWB
accuracy is comparable to SMB. BiLSTM aggregation technique improves results across all datasets,
for all models. WWM improves the performance of Char and KeyChar tokenizer, as expected.
It does not improve the SP E tokenization. We found that WWM pre-training improves Char the
most, and mostly eliminates systematic differences in tokenization. Indeed, a Char model, CWB,
has the best results on 4 of the 6 datasets, while an SP E model with MLM pre-training, SMB, is
best on the other two.

5 Related Work

In recent years, research at the intersection of NLP, Machine Learning and Software Engineering
has signiﬁcantly increased. Topics in this ﬁeld include code completion [33], [11], [13], program
repair [4], [34], [36], bug detection [31] and type inference [32], [10]. [24], [30]. A common factor
among all these techniques is the use of an n-gram or RNN based LM with the tokenization deﬁned
by the programming language. Thus they are severely affected by the OOV and rare words problems.
Sub-word tokenization (e.g. BPE) was ﬁrst proposed by [18] as a suitable alternative for source code
and shows compelling results for code completion and bug detection. In our work, we show that ﬁner
grained tokenization techniques are beneﬁcial for source code LMs compared to subword tokenizers.

CuBert [17] is a recently introduced LM for modeling code. In contrast to our approach, this work
does not consider character based tokenization and uses a subword tokenizer. A Github corpus in
Python is used for pre-training a BERT-like model which is ﬁne-tuned for tasks like variable misuse
classiﬁcation, wrong binary operator detection, swapped operands, function-docstring mismatch,
and prediction of exception type. CodeBert [7] is another transformer LM specialized for code
generation from textual descriptions. Their specialized pre-training uses bimodal examples of source
code paired with natural language documentation from Github repositories.

Prior to the use of machine learning techniques, static code analysis was used to understand code
structures and apply rules handcrafted by programmers to identify potential vulnerabilities. Tools
like RATS [14], Flawﬁnder [40], and Infer [6] are of this type. However, they produce many false
positives, a problem identiﬁed early on by [2, 8, 15], making these tools difﬁcult to use effectively
as part of the developer tool chain.

8

Current machine learning approaches to code analysis depend heavily on features derived from
the compiler, such as the AST, or other derived structures that require the compilation of source
code [45]. These features are paired with complex GGNN [26] models. In addition to the VI task
[45], combining the AST and GGNN have shown good results in type inference [39], code clone
detection [3, 44] and bug detection [22]. This evidence motivated our AST tagging task.

6 Conclusions

This work explores the software naturalness hypothesis by using language models on multiple source
code tasks. Unlike graph based approaches that use structural features like the AST, CFG and
DFG, our model is built on raw source code. Furthermore, we show that the AST token_kind and
cursor_kind can be learned with the LM. LMs can work even better than graph based approaches for
VI because they avoid the computational cost and requirements of full compilation. This makes our
approach suitable for a wider range of scenarios, such as pull requests. We propose two character
based tokenization approaches that solve the OOV problem while having very small vocabularies.
We suggest ways to improve them with aggregation and WWM. These approaches work just as well
and sometimes even better than a subword tokenizer like sentencepiece that has been previously
explored for source code. In future work we propose joint learning of the AST and VI tasks on top of
LM to further improve code analysis, without using the compiler to extract structured information.

Broader Impact

Our research supports the software naturalness hypothesis. This means that it may be possible to
transfer many recent advances in natural language understanding into the software domain and im-
prove source code understanding. The LM created by pre-training on source code can thus be used
for different tasks such as VI, code completion [33], code repair [36], as well as multi-modal tasks
involving both natural language and source code [7]. These tasks have applications in the ﬁelds of
software security, developer tools and automation of software development and maintenance. Here
we focus on software security through the task of VI. Development and improvement of end-to-
end systems that can identify software vulnerabilities will make it easier to do the same in open
source software. As these tools become more available to everyone, it becomes imperative that
developers protect their code from malicious agents by incorporating these tools in Continuous Inte-
gration/Continuous Delivery pipelines to identify vulnerabilities before code is exposed to others.

References

[1] ALLAMANIS, M. The adverse effects of code duplication in machine learning models of
In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas,

code.
New Paradigms, and Reﬂections on Programming and Software (2019), pp. 143–153.

[2] CHEIRDARI, F., AND KARABATIS, G. Analyzing false positive source code vulnerabilities
using static analysis tools. In 2018 IEEE International Conference on Big Data (Big Data)
(2018).

[3] CHEN, L., YE, W., AND ZHANG, S. Capturing source code semantics via tree-based con-
In Proceedings of the 16th ACM International Conference
volution over api-enhanced ast.
on Computing Frontiers (New York, NY, USA, 2019), CF ’19, Association for Computing
Machinery, p. 174–182.

[4] CHEN, Z., KOMMRUSCH, S. J., TUFANO, M., POUCHET, L., POSHYVANYK, D., AND
MONPERRUS, M. Sequencer: Sequence-to-sequence learning for end-to-end program repair.
IEEE Transactions on Software Engineering (2019).

[5] DEVLIN, J., CHANG, M.-W., LEE, K., AND TOUTANOVA, K. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) (Minneapolis, Minnesota, June
2019), Association for Computational Linguistics, pp. 4171–4186.

[6] FACEBOOK. Infer. https://fbinfer.com/, 2016.

9

[7] FENG, Z., GUO, D., TANG, D., DUAN, N., FENG, X., GONG, M., SHOU, L., QIN, B., LIU,
T., JIANG, D., ET AL. Codebert: A pre-trained model for programming and natural languages.
arXiv preprint arXiv:2002.08155 (2020).

[8] GADELHA, M. R., STEFFINLONGO, E., CORDEIRO, L. C., FISCHER, B., AND NICOLE,
D. A. Smt-based refutation of spurious bug reports in the clang static analyzer. In Proceed-
ings of the 41st International Conference on Software Engineering: Companion Proceedings
(2019), ICSE ’19, p. 11–14.

[9] GOOGLE. Bert. github.com/google-research/bert/blob/master/README.md, 2018.

[10] HELLENDOORN, V. J., BIRD, C., BARR, E. T., AND ALLAMANIS, M. Deep learning type
inference. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engi-
neering Conference and Symposium on the Foundations of Software Engineering (New York,
NY, USA, 2018), ESEC/FSE 2018, Association for Computing Machinery, p. 152–162.

[11] HELLENDOORN, V. J., AND DEVANBU, P. Are deep neural networks the best choice for
modeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Soft-
ware Engineering (New York, NY, USA, 2017), ESEC/FSE 2017, Association for Computing
Machinery, p. 763–773.

[12] HINDLE, A., BARR, E. T., SU, Z., GABEL, M., AND DEVANBU, P. On the naturalness
of software. In Proceedings of the 34th International Conference on Software Engineering
(2012), ICSE ’12, IEEE Press, p. 837–847.

[13] HUSSAIN, Y., HUANG, Z., ZHOU, Y., AND WANG, S. Deep transfer learning for source code

modeling. arXiv preprint arXiv:1910.05493 (2019).

[14] INC., S. S. Rough Audit Tool for Security. https://github.com/stgnet/rats, 2013.

[15] JOHNSON, B., SONG, Y., MURPHY-HILL, E., AND BOWDIDGE, R. Why don’t software
developers use static analysis tools to ﬁnd bugs? In Proceedings of the 2013 International
Conference on Software Engineering (2013), ICSE ’13, p. 672–681.

[16] JOSHI, M., CHEN, D., LIU, Y., WELD, D. S., ZETTLEMOYER, L., AND LEVY, O. Spanbert:
Improving pre-training by representing and predicting spans. Transactions of the Association
for Computational Linguistics 8 (2020), 64–77.

[17] KANADE, A., MANIATIS, P., BALAKRISHNAN, G., AND SHI, K. Pre-trained contextual

embedding of source code. arXiv preprint arXiv:2001.00059 (2019).

[18] KARAMPATSIS, R.-M., BABII, H., ROBBES, R., SUTTON, C., AND JANES, A. Big code !=
big vocabulary: Open-vocabulary models for source code. arXiv preprint arXiv:2003.07914
(2020).

[19] KERNIGHAN, B. W., AND RITCHIE, D. M. The C Programming Language, 2nd ed. Prentice

Hall Professional Technical Reference, 1988.

[20] KUDO, T., AND RICHARDSON, J. SentencePiece: A simple and language independent sub-
word tokenizer and detokenizer for neural text processing. In "Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language Processing: System Demonstrations" ("Brus-
sels, Belgium", nov 2018), "Association for Computational Linguistics", pp. "66–71".

[21] LI, Y., TARLOW, D., BROCKSCHMIDT, M., AND ZEMEL, R. Gated graph sequence neural

networks. ICLR (2016).

[22] LIANG, H., SUN, L., WANG, M., AND XING YANG, Y. Deep learning with customized

abstract syntax tree for bug localization. IEEE Access 7 (2019), 116309–116320.

[23] LOUIS KIM, REBECCA RUSSELL. Draper VDISC Dataset - Vulnerability Detection in Source

Code. https://osf.io/d45bw/, 2020.

[24] MALIK, R. S., PATRA, J., AND PRADEL, M. Nl2type: Inferring javascript function types
In 2019 IEEE/ACM 41st International Conference on

from natural language information.
Software Engineering (ICSE) (2019), pp. 304–315.

[25] MANNING, C. D., AND SCHÜTZE, H. Foundations of Statistical Natural Language Process-

ing. MIT Press, Cambridge, MA, USA, 1999.

[26] MICROSOFT. github.com/microsoft/gated-graph-neural-network-samples, 2015.

10

[27] MIKOLOV, T., SUTSKEVER, I., CHEN, K., CORRADO, G. S., AND DEAN, J. Distributed
representations of words and phrases and their compositionality. In Advances in Neural Infor-
mation Processing Systems 26, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 3111–3119.

[28] NIST. Juliet test suite v1.3. https://samate.nist.gov/SRD/testsuite.php, 2017.
[29] PAPPAGARI, R., ˙ZELASKO, P., VILLALBA, J., CARMIEL, Y., AND DEHAK, N. Hierarchical
transformers for long document classiﬁcation. arXiv preprint arXiv:1910.10781 (2019).

[30] PRADEL, M., GOUSIOS, G., LIU, J., AND CHANDRA, S. Typewriter: Neural type prediction

with search-based validation. arXiv preprint arXiv:1912.03768 (2019).

[31] RAY, B., HELLENDOORN, V., GODHANE, S., TU, Z., BACCHELLI, A., AND DEVANBU, P.
On the "naturalness" of buggy code. In 2016 IEEE/ACM 38th International Conference on
Software Engineering (ICSE) (2016), pp. 428–439.

[32] RAYCHEV, V., VECHEV, M., AND KRAUSE, A. Predicting program properties from “big
code”. In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages (New York, NY, USA, 2015), POPL ’15, Association for Comput-
ing Machinery, p. 111–124.

[33] RAYCHEV, V., VECHEV, M., AND YAHAV, E. Code completion with statistical language
models. In In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language
Design and Implementation (2014), pp. 419–428.

[34] SANTOS, E. A., CAMPBELL, J. C., PATEL, D., HINDLE, A., AND AMARAL, J. N. Syntax
and sensibility: Using language models to detect and correct syntax errors. In 2018 IEEE 25th
International Conference on Software Analysis, Evolution and Reengineering (SANER) (2018),
pp. 311–322.

[35] "SENNRICH, R., HADDOW, B., AND BIRCH, A. "neural machine translation of rare words
with subword units". In "Proceedings of the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers)" ("Berlin, Germany", aug 2016), "Association
for Computational Linguistics", pp. "1715–1725".

[36] VASIC, M., KANADE, A., MANIATIS, P., BIEBER, D., AND SINGH, R. Neural program
repair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720 (2019).

[37] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N.,
In Advances in Neural
KAISER, L. U., AND POLOSUKHIN, I. Attention is all you need.
Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 5998–6008.

[38] WANG, A., PRUKSACHATKUN, Y., NANGIA, N., SINGH, A., MICHAEL, J., HILL, F., LEVY,
O., AND BOWMAN, S. R. Superglue: A stickier benchmark for general-purpose language
understanding systems. In NeurIPS (2019), pp. 3261–3275.

[39] WEI, J., GOYAL, M., DURRETT, G., AND DILLIG, I. Lambdanet: Probabilistic type infer-
ence using graph neural networks. In International Conference on Learning Representations
(2020).

[40] WHEELER, D.A. Flawﬁnder. http://www.dwheeler.com/flawfinder, 2018.

[41] WOLF, T., DEBUT, L., SANH, V., CHAUMOND, J., DELANGUE, C., MOI, A., CISTAC, P.,
RAULT, T., LOUF, R., FUNTOWICZ, M., AND BREW, J. Huggingface’s transformers: State-
of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).

[42] YAMAGUCHI, F., GOLDE, N., ARP, D., AND RIECK, K. Modeling and discovering vulnera-
bilities with code property graphs. In Proc. of IEEE Symposium on Security and Privacy (S&P)
(2014).

[43] ZHANG, J., WANG, X., ZHANG, H., SUN, H., WANG, K., AND LIU, X. A novel neural
source code representation based on abstract syntax tree. In Proceedings of the 41st Interna-
tional Conference on Software Engineering (2019), ICSE ’19, IEEE Press, p. 783–794.

[44] ZHANG, J., WANG, X., ZHANG, H., SUN, H., WANG, K., AND LIU, X. A novel neural
source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International
Conference on Software Engineering (ICSE) (2019), pp. 783–794.

11

[45] ZHOU, Y., LIU, S., SIOW, J., DU, X., AND LIU, Y. Devign: Effective vulnerability identiﬁca-
tion by learning comprehensive program semantics via graph neural networks. In Advances in
Neural Information Processing Systems 32 (2019), Curran Associates, Inc., pp. 10197–10207.

[46] FFMPeg. https://github.com/FFmpeg/FFmpeg, 2020.
[47] QEMU. https://github.com/quemu/qemu, 2020.
[48] Clang Static Analyzer. https://clang-analyzer.llvm.org, 2020.

12

