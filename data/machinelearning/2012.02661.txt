1
2
0
2

y
a
M
1

]

G
L
.
s
c
[

2
v
1
6
6
2
0
.
2
1
0
2
:
v
i
X
r
a

Efﬁcient semideﬁnite-programming-based inference
for binary and multi-class MRFs

Chirag Pabbaraju
Carnegie Mellon University
cpabbara@cs.cmu.edu

Po-Wei Wang
Carnegie Mellon University
poweiw@cs.cmu.edu

J. Zico Kolter
Carnegie Mellon University
Bosch Center for AI
zkolter@cs.cmu.edu

Abstract

Probabilistic inference in pairwise Markov Random Fields (MRFs), i.e. computing
the partition function or computing a MAP estimate of the variables, is a foun-
dational problem in probabilistic graphical models. Semideﬁnite programming
relaxations have long been a theoretically powerful tool for analyzing properties of
probabilistic inference, but have not been practical owing to the high computational
cost of typical solvers for solving the resulting SDPs. In this paper, we propose
an efﬁcient method for computing the partition function or MAP estimate in a
pairwise MRF by instead exploiting a recently proposed coordinate-descent-based
fast semideﬁnite solver. We also extend semideﬁnite relaxations from the typical
binary MRF to the full multi-class setting, and develop a compact semideﬁnite
relaxation that can again be solved efﬁciently using the solver. We show that the
method substantially outperforms (both in terms of solution quality and speed) the
existing state of the art in approximate inference, on benchmark problems drawn
from previous work. We also show that our approach can scale to large MRF
domains such as fully-connected pairwise CRF models used in computer vision.

1

Introduction

Undirected graphical models or Markov Random Fields (MRFs) are used in various real-world
applications like computer vision, computational biology, etc. because of their ability to concisely
represent associations amongst variables of interest. A general pairwise MRF over binary random
variables x ∈ {−1, 1}n may be characterized by the following joint distribution

p(x) ∝ exp (cid:0)xT Ax + hT x(cid:1) ,
where A ∈ Rn×n denotes the “coupling matrix” and encodes symmetric pairwise correlations, while
h ∈ Rn consists of the biases for the variables. In this model, there are three fundamental problems
of interest: (a) estimating the mode of the distribution, otherwise termed as maximum a posteriori
(MAP) inference (b) estimating p(x) for a conﬁguration x or generating samples from the distribution,
and (c) learning the parameters (A, h) given samples from the joint distribution. Since there are an
exponential number of conﬁgurations in the support of the MRF, the problem of ﬁnding the true
mode of the distribution is in general a hard problem. Similarly, to compute the probability p(x) of
any particular conﬁguration, one has to compute the constant of proportionality in (1) which ensures
that the distribution sums up to 1. This constant, denoted as Z, is called the partition function, where

(1)

Z =

(cid:88)

exp (cid:0)xT Ax + hT x(cid:1) .

x∈{−1,1}n

Computing Z exactly also involves summing up an exponential number of terms and is #P hard
[17, 13] in general. The problem becomes harder still when we go beyond binary random variables
and consider the case of a general multi-class MRF (also termed as a Potts model), where each
variable can take on values from a ﬁnite set. Since problem (b) above requires computing Z accurately,

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
several approximations have been proposed in the literature. These methods have typically suffered
in the quality of their approximations in the case of problem instances where the entries of A have
large magnitude; this is referred to as the low-temperature setting [31, 29, 12, 26].

Recently, Park et al. [26] proposed a novel spectral algorithm that provably computes an approximate
estimate of the partition function in time polynomial in the dimension n and spectral properties of
A. They show that their algorithm is fast, and signiﬁcantly outperforms popular techniques used
in approximate inference, particularly in the low-temperature setting. However, their experimental
results suggest that there is still room for improvement in this setting. Furthermore, it is unclear how
their method could be conveniently generalized to the richer domain of multi-class MRFs.

Another well-studied approach to compute the mode, i.e. the maximizer of the RHS in (1), is to relax
the discrete optimization problem to a semideﬁnite program (SDP) [35, 1] and solve the SDP instead.
Rounding techniques like randomized rounding [15] are then used to round the SDP solution to the
original discrete space. In particular, Wang et al. [35] employ this approach in the case of a binary
RBM and demonstrate impressive results. Frieze et al. [10] draw parallels between mode estimation
in a general k-class Potts model and the MAX k-CUT problem, and suggest an SDP relaxation for
the same. However, their relaxation has a quadratic number of constraints in the number of variables
in the MRF. Therefore, using traditional convex program solvers employing the primal dual-interior
point method [2] to solve the SDP would be computationally very expensive for large MRFs.

In this work, we propose solving a fundamentally different SDP relaxation for performing inference
in a general k-class Potts model, that can be solved efﬁciently via a recently proposed low-rank
SDP solver [32], and show that our method performs accurately and efﬁciently in practice, scaling
successfully to large MRFs. Our SDP relaxation has only a linear number of constraints in the
number of variables in the MRF. This allows us to exploit a low-rank solver based on coordinate
descent, called the “Mixing method” [32], which converges extremely fast to a global solution of the
proposed relaxation. We further propose a simple importance sampling-based method to estimate the
partition function. Once we have solved the SDP, we state a rounding procedure to obtain samples
in the discrete space. Since the rounding is applied to the optimal solution, the samples returned
are closely clustered around the true mode in function value. Then, to ensure additional exploration
in the space of the samples, we obtain a fraction of samples from the uniform distribution on the
discrete hypercube. The combination results in an accurate estimate of the partition function.

Our experimental results show that our technique excels in both mode and partition function estima-
tion, when compared to state-of-the-art methods like Spectral Approximate Inference [26], as well as
specialized Markov Chain Monte Carlo (MCMC) techniques like Annealed Importance Sampling
(AIS) [23], especially in the low temperature setting. Not only does our method outperform these
methods in terms of accuracy, but it also runs signiﬁcantly faster, particularly compared to AIS.
We display these results on synthetic binary MRF settings drawn from Park et al. [26], as well as
synthetic multi-class MRFs. Finally, we demonstrate that, owing to the efﬁciency of the fast SDP
solver, our method is able to scale to large real-world MRFs used in image segmentation tasks.

2 Background and Related Work

Variational methods and Continuous relaxations One popular class of approaches in approxi-
mate inference consists of framing a relevant optimization problem whose solution can be treated as a
reasonable approximation to the true mode/partition function. This includes techniques employing the
Gibbs variational principle [3] that solve an optimization problem over the set of all possible distribu-
tions on the random variables (generally intractable). Amongst these, the mean-ﬁeld approximation
[25], which makes the simplifying (and possibly inaccurate) assumption of a product distribution
amongst the variables, is extremely popular. Belief propagation [37] is another popular algorithm used
for inference that has connections to variational inference, but has strong theoretical guarantees only
when the underlying MRFs are loop-free. In addition, several LP-based and SDP-based continuous
relaxations [18, 30] to the discrete optimization problem have been proposed. In particular, Frieze
et al. [10] model the problem of estimating the mode in a multi-class MRF as an instance of the
MAX k-CUT problem and propose an SDP relaxation as well as rounding mechanism for the same.
Generally, such SDP-based approaches are theoretically attractive to analyze [11, 27, 5, 24], but
practically infeasible for large MRFs with many constraints due to their high computational cost.

MCMC and Sampling-based methods Another class of approaches involves running MCMC
chains whose stationary distribution is the one speciﬁed by (1). These methods run a particular

2

number of MCMC steps and then do some sort of averaging over the samples at the end of the
chain. Popular methods include Gibbs sampling [14] and Metropolis-Hastings [16]. A signiﬁcant
development in these methods was the introduction of annealing over a range of temperatures by Neal
[23], which computes an unbiased estimate of the true partition function. Thereafter, several other
methods in this line that employ some form of annealing and importance sampling have emerged
[4, 21, 6]. However, it is typically difﬁcult to determine the number of steps that the MCMC chain
requires to converge to the stationary distribution (denoted mixing time). Further, as mentioned above,
both variational methods (due to their propensity to converge to suboptimal solutions) and MCMC
methods (due to large mixing times) are known to underperform in the low-temperature setting.

Other methods Some other popular techniques for inference include variable elimination methods
like bucket elimination [7, 8] that typically use a form of dynamic programming (DP) to approximately
marginalize the variables in the model one-by-one. A signiﬁcant recent development in this line,
which is also based on DP, is the spectral approach by Park et al. [26]. By viewing all possible
conﬁgurations of the random variables in the function space, Park et al. [26] build a bottom-up
approximate DP tree which yields a fully-polynomial time approximation scheme for estimating Z,
and markedly outperforms other standard techniques. However, as mentioned above, it is a priori
unclear how their method could be extended to the multi-class Potts model, since their bottom-up
dynamic programming chain depends on the variables being binary-valued. Other approximate
algorithms for estimating the partition function with theoretical guarantees include those that employ
discrete integration by hashing [9] as well as quadrature-based methods [28]. While our method does
not purely belong to either of the two categories mentioned above (since it involves both an SDP
relaxation and importance sampling), it successfully generalizes to multi-class MRFs.

3 Estimation of the mode in a general k-class MRF

In this section, we formulate the SDP relaxation which we propose to solve for mode estimation in a
k-class Potts model. First, we state the optimization problem for mode estimation in a binary MRF:

max
x∈{−1,1}n

xT Ax + hT x.

(2)

We observe that the above problem can be equivalently stated as below:

max
x∈{−1,1}n

n
(cid:88)

n
(cid:88)

i=1

j=1

Aij

ˆδ(xi, xj) +

n
(cid:88)

(cid:88)

i=1

l∈{−1,1}

ˆh(l)
i

ˆδ(xi, l); where ˆδ(a, b) =

(cid:26)1

−1

if a = b
otherwise.

(3)

The equivalence in (2) and (3) is readily achieved by setting ˆh(l)
i − ˆh(−1)
.
i
However, viewing the optimization problem thus helps us naturally extend the problem to general
k-class MRFs where the random variables xi can take values in a discrete domain {1, . . . , k} (denoted
[k]). For the general case, we can frame a discrete optimization problem as follows:

such that hi = ˆh(1)

i

max
x∈[k]n

n
(cid:88)

n
(cid:88)

i=1

j=1

Aij

ˆδ(xi, xj) +

n
(cid:88)

k
(cid:88)

i=1

l=1

ˆh(l)
i

ˆδ(xi, l).

(4)

where we are now provided with bias vectors ˆh(l) for each of the k classes.

Efﬁcient SDP relaxation We now derive an SDP-based relaxation to (4) that grows only linearly
in n. To motivate this approach, we note that for the case of (4) (without the bias terms), Frieze et
al. [10] ﬁrst state an equivalent optimization problem deﬁned over a simplex in Rk−1, and go on to
derive the following relaxation for which theoretical guarantees hold:

max
vi∈Rn, (cid:107)vi(cid:107)2=1 ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj

subject to

vT
i vj ≥ −

1
k − 1

∀i (cid:54)= j.

(5)

The above problem (5) can be equivalently posed as a convex problem over PSD matrices Y , albeit
with ∼ n2 entry-wise constraints Yij ≥ −1/k − 1 corresponding to each pairwise constraint in vi, vj.

3

Thus, for large n, solving (5) via traditional convex program solvers would be very expensive. Note
further that, unlike the binary case (where the pairwise constraints hold trivially), it would also be
challenging to solve this problem with low-rank methods, due to the quadratic number of constraints.

Towards this, we propose an alternate relaxation to (4) that reduces the number of constraints to be
linear in n. Observe that the pairwise constraints in (5) are controlling the separation between vi, vj
and trying to keep them roughly aligned with the vertices of a simplex. With this insight, we try to
incorporate the functionality of these constraints within the criterion by plugging them in as part of
the bias terms. Speciﬁcally, let us ﬁx r1, . . . , rk ∈ Rn on the vertices of a simplex, so that
(cid:26)1

rT
l rl(cid:48) =

− 1
k−1

if l = l(cid:48)
if l (cid:54)= l(cid:48).

Then, we can observe that the following holds:

2
k
so that solving the following discrete optimization problem is identical to solving (4):

rxj + 1(cid:1) − 1,

(cid:0)(k − 1)rT
xi

ˆδ(xi, xj) =

max
vi∈{r1,...,rk} ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

k
(cid:88)

i=1

l=1

ˆh(l)
i vT

i rl.

(6)

(7)

The motivation here is that we are trying to mimic the ˆδ operator in (4) via inner products, but in such
a way that the bias coefﬁcients ˆh(l)
i determine the degree to which vi is aligned with a particular rl.
Thus, intuitively at least, we have incorporated the pairwise constraints in (5) within the criterion. As
the next step, we simply relax the domain of optimization in (7) from the discrete set {ri}k
i=1 to unit
vectors in Rn so as to derive the following relaxation:

max
vi∈Rn, (cid:107)vi(cid:107)2=1 ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl.

(8)

Now, let H ∈ Rn×k such that Hij = ˆh(j)

i

. Deﬁne the block matrix C ∈ R(k+n)×(k+n) such that:
(cid:20) 0
1
2 · H

2 · H T
A

(cid:21)

1

.

C =

Then, the following convex program is equivalent to (8):

max
Y (cid:23)0

Y · C

subject to Yii = 1 ∀i ∈ [k + n];

Yij = −

1
k − 1

∀i ∈ [k], i < j ≤ k.

(9)

We defer the proof of the equivalence between (8) and (9) to Appendix A. Note that the number of
constraints in (9) is now only n + k + k(k−1)
2 = n + k(k+1)
i.e. linear in n as opposed to the quadratic
number of constraints in (5). We can then use the results by Barvinok [1] and Pataki [27], which state

2

that there indeed exists a low-rank solution to (9) with rank d at most

we can instead work in the space Rd, leading to the following optimization problem:

max
vi∈Rd, (cid:107)vi(cid:107)2=1 ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl.

. Thus,

(10)

(cid:24)(cid:114)

(cid:16)

2

n + k(k+1)

2

(cid:17)(cid:25)

This low-rank relaxation can then directly be solved in its existing non-convex form by using the
method for solving norm-constrained SDPs by Wang et al. [32] called the “mixing method”, which we
refer to as M 4 (“Mixing Method for Multi-class MRFs”). M 4 derives closed-form coordinate-descent
updates for the maximization in (10), and has been shown [34, 33] to reach accurate solutions in just
a few iterations. Pseudocode for solving (10) via M 4 is given in the block for Algorithm 1.

Once we have a solution v1, . . . , vn to (10), we still require a technique to round back these vectors
to conﬁgurations in the discrete space [k]n. For this purpose, Frieze et al. [10] propose a natural

4

i=1, {ˆh(l)}k

l=1, {rl}k

l=1

Initialize num iters
for iter = 1, 2 . . . , num iters do

Algorithm 1 Solving (10) via M 4

Input: A, {vi}n
1: procedure M 4:
2:
3:
4:
5:
6:
7:
8:
9:
10: end procedure

end for

end for
return v1, . . . , vn

for i = 1, 2 . . . , n do
gi ← 2 (cid:80)
vi ← gi
(cid:107)gi(cid:107)2

j(cid:54)=i Aijvj + (cid:80)k

l=1

ˆh(l)
i rl

Algorithm 2 Rounding in the multi-class case
i=1, {rl}k

Input: {vi}n

l=1 ∼ Unif(S d)

xi ← arg maxl∈[k] vT

i ml

l=1
1: procedure ROUNDING:
Sample {ml}k
2:
for i = 1, 2 . . . , n do
3:
4:
5:
6:
7:
8:
9:
10: end procedure

end for
for i = 1, 2 . . . , n do

end for
return x

xi ← arg maxl∈[k] mT
xi

rl

extension to the technique of randomized rounding suggested by Goemans et al. [15], which involves
rounding the vis to k randomly drawn unit vectors. We further extend their approach as described
in Algorithm 2 for the purposes of rounding our SDP relaxation (10). In the ﬁrst step in Algorithm
l=1 uniformly on the unit sphere S d and perform rounding as in
2, we sample k unit vectors {ml}k
Frieze et al. [10]. However, we need to reconcile this rounding with the truth vectors on the simplex.
Thus, in the second step, we reassign each rounded value to a truth vector: if vi was mapped to ml in
the ﬁrst step, we now map it to rl(cid:48) such that ml is closest to rl(cid:48). In this way, we can obtain a bunch of
rounded conﬁgurations, and output as the mode the one that has the maximum criterion value in (4).

Alternate relaxation to (7) M 4 provably solves the optimization problem in (10), but the rounded
solution after applying Algorithm 2 lacks any approximation guarantees. This is because we simply
discarded the pairwise constraints which existed in the original formulation (5) of Frieze et al. [10].
In order to remedy this, we further propose an alternate relaxation, so as to obtain an approximation
ratio for the rounded solution.

To ensure that vi, vj satisfy the pairwise constraints in 5, we adopt an alternate parameterization of
the vis in terms of auxiliary variables zi ∈ Rd for d = m · k, m ∈ Z (we want to be able to segment
k (1k×k ⊗ Im)(cid:1) where 1k×k is ﬁlled with 1s, and let
each zi into k blocks). Let C = k
k−1
i ∈ Rm denote the bth block in zi.
C = ST S denote the Cholesky decomposition of C. Further, let zb
Then, we frame the following optimization problem:

(cid:0)Id − 1

max
zi∈Rd ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl

subject to

zi ≥ 0,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

b=1

zb
i

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

= 1, vi = Szi ∀i ∈ [n]

(11)

discrete, where f (cid:63)

We defer the detailed derivation for formulating (11) and solving it by Algorithm 3 (denoted as M 4+)
to Appendix B. Here, we simply describe the motivation for such a parameterization of the vis. The
approximation guarantees of Frieze et al. [10] hold for any set of vis lying in the feasible set of
i vj ≥ − 1/k − 1, we have that E[f (Rounding(V ))] ≥ α · f (V ),
(5). Concretely, for (cid:107)vi(cid:107)2 = 1 and vT
where we denote the objective by f and V = {v1, . . . , vn}. Thus, if we ﬁnd a set of feasible vis such
that f (V ) ≥ f (cid:63)
discrete refers to the solution to (7), we have the required guarantee
on the expected value of the rounding. In the above, the parameterization of the vis via the zis and S,
together with the positivity constraints on zi, ensure that vi, vj satisfy the pairwise constraints. In
addition, step 10 in Algorithm 3 ensures that after each round of updates, (cid:107)zi(cid:107)2 = 1 and consequently,
(cid:107)vi(cid:107)2 = 1 for all i. Thus, we have that the vis after each round of updates in M 4+ lie in the required
feasible set. Further, we empirically observe that at convergence of Algorithm 3, f (V ) > f (cid:63)
discrete
always (in fact, the solution is within 5% of the true solution to (11)). To summarize, provided
that Algorithm 3 converges to a set of vis such that f (V ) > f (cid:63)
discrete, we have the approximation
guarantees of Frieze et al. [10] for the rounded solution.

5

Algorithm 3 Solving (11) via M 4+
i=1, {ˆh(l), rl}k

Input: A, {zi}n
1: procedure M 4+:
2:
3:
4:

Initialize num iters
for iter = 1, 2 . . . , num iters do

for i = 1, 2 . . . , n do

l=1, C = ST S

5:

6:
7:

8:

g ← 2

n
(cid:80)
j(cid:54)=i

AijCzj + ST

k
(cid:80)
l=1

ˆh(l)
i rl

for j = 1, 2 . . . , m do

Pick any b(j) ∈ arg maxb gb
j
j )+ if b = b(j)
otherwise

(cid:26)(gb
0

gb
j ←

end for
zi ← g
(cid:107)g(cid:107)2

9:
10:
11:
12:
13:
14: end procedure

end for

end for
return Sz1, . . . , Szn

Algorithm 4 Estimation of Z
i=1, {rl}k

Input: k, {vi}n

l=1

Sample x ∼ pv using Algorithm 2
If x not in Xpv , add x to Xpv

Initialize R ∈ Z, Xpv = {}, XΩ = [ ]
for i = 1, 2 . . . , R do

1: procedure PARTITIONFUNCTION:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end for
q ← 1
kn−|Xpv |
for i = 1, 2 . . . , R do

Sample x ∼ U nif ([k]n \ Xpv )
Append x to XΩ

ef (x) + 1
R

ef (x)
q

end for
ˆZ ← (cid:80)
x∈Xpv

(cid:80)
x∈XΩ

return ˆZ

13:
14: end procedure

4 Estimation of the partition function

In this section, we deal with the other fundamental problem in inference: estimating the partition
function. Following Section 3 above, the joint distribution in a k-class MRF can be expressed as:

p(x) ∝ exp





n
(cid:88)

n
(cid:88)

Aij

ˆδ(xi, xj) +

i=1

j=1

(cid:124)

(cid:123)(cid:122)
f (x)



ˆh(l)
i

ˆδ(xi, l)



.

n
(cid:88)

k
(cid:88)

i=1

l=1

(cid:125)

(12)

As stated previously, the central aspect in computing the probability of a conﬁguration x in this model
is being able to calculate the partition function Z. The expression for the partition function in (12) is:

(cid:88)

Z =



exp



n
(cid:88)

n
(cid:88)

x∈[k]n

i=1

j=1

Aij

ˆδ(xi, xj) +



ˆh(l)
i

ˆδ(xi, l)

 .

n
(cid:88)

k
(cid:88)

i=1

l=1

(13)

We begin with the intuition that the solution to (4) can be useful in computing the partition function.
This intuition indeed holds if the terms in the summation are dominated by a few terms with large
magnitude, which happens to be the case when A has entries with large magnitude (low temperature
setting). The hope then is that the rounding procedure described above more often than not rounds to
conﬁgurations that have large f values (ideally close to the mode). In that case, a few iterations of
rounding would essentially yield all the conﬁgurations that make up the entire probability mass (a
phenomenon which we empirically conﬁrm ahead).

With this motivation, we describe a simple algorithm to estimate Z that exploits the solution to our
proposed relaxations. The rounding procedure described in Algorithm 2 induces a distribution on x
in the original space. Let us denote this distribution as pv. For the case when d = 2, Wang et al. [35]
propose a geometric technique for exactly calculating pv, and derive an importance sampling estimate
of Z based on the empirical expectation ˆE [ exp(f (x))/pv(x)]. However, this approach does not scale to
higher dimensions. Further, note that for small values of d, pv does not have a full support of [k]n.
Thus, an importance sampling estimate computed solely on pv would not be theoretically unbiased.
Consequently, we propose using Algorithm 4 to estimate Z. First, we do a round of sampling from
pv, and store all the unique x’s seen in Xpv . At this point, the hope is that Xpv stores all the x’s that
constitute a bulk of the probability mass. Thereafter, to encourage exploration and ensure that our
sampling process has a full support of [k]n, we do a round of importance sampling from the uniform
distribution on [k]n \ Xpv and combine the result with the samples stored in Xpv . For the estimate of
Z thus obtained, the following guarantee can be easily shown (refer to Appendix C for proof):
Theorem 1. The estimate ˆZ given by Algorithm 4 is unbiased i.e. E[ ˆZ] = Z.

6

(a) Complete graph, k = 5, n = 7

(b) Complete graph k = 5, n = 7

(c) Mass sampled k = 5, n = 7

Figure 1: (a) Mode estimation - comparison with AIS (x-axis on log-scale) (b) Mode estimate
comparison w/ max-product BP and decimation (c) Randomized rounding samples most of the mass

5 Experimental Results

In this section, we validate our formulations on a variety of MRF settings, both synthetic and real-
world. Following its usage in Park et al. [26], we ﬁrst state the notion of “coupling strength” of a
matrix A ∈ Rn×n, which determines the temperature of the problem instance:

CS(A) =

1
n(n − 1)

(cid:88)

i(cid:54)=j

|Aij|.

(14)

As in the setup in Park et al. [26], the coupling matrices are generated as follows: for a coupling
strength c, the entries on edges in A are sampled uniformly from [−c(cid:48), c(cid:48)], where c(cid:48) is appropriately
scaled so that CS(A) ≈ c. The biases are sampled uniformly from [−1, 1]. We generate random
complete graphs and Erd¨os-Renyi (ER) graphs. While generating ER graphs, we sample an edge
in A with probability 0.5. We perform experiments on estimating the mode (Section 3) as well as
Z (Section 4). The algorithms we mainly compare to in our experiments are AIS [23], Spectral
Approximate Inference (Spectral AI) [26] and the method suggested by Wang et al. [35]. We note that
for the binary MRFs considered in the partition function task, Park et al. [26] demonstrate that they
signiﬁcantly outperform popular algorithms like belief propagation, mean-ﬁeld approximation and
mini-bucket variable elimination (Figures 3(a), 3(c) in Park et al. [26]). Hence, we simply compare
to Spectral AI. For AIS, we have 3 main parameters: (K, num cycles, num samples). We provide
a description of these parameters along with complete pseudocode of our implementation of AIS in
Appendix D. All the results in any synthetic setting are averaged over 100 random problem instances.1

5.1 Mode estimation
We compare the quality of the mode estimate over progress of our methods (rounding applied to M 4
and M 4+) and AIS. On the x-axis, we plot the time elapsed for either method, and on the y-axis, we
plot the relative error f − ˆf
f . Figure 1a shows the comparison for k = 5 and CS(A) = 2.5. In the
legend, the number in parentheses following our methods is the number of rounding iterations, while
those following AIS are (K, num cycles, num samples) respectively. We observe from the plots
that our methods are able to achieve a near optimal mode much quicker than AIS, underlining the
efﬁcacy of our method. Next, we compare the quality of the mode estimates given by both of our
relaxations with the max-product belief propagation and decimation (DecMAP) algorithms provided
in libDAI [22]. Across a range of coupling strengths, we plot the relative error of the mode estimates
given by each method, for k = 5. We can observe (Figure 1b) that both our relaxations provide mode
estimates that have very small relative error (∼ 0.018 at worst), whilst also being faster. Additional
plots comparing our methods as well as timing experiments are provided in Appendix E.

5.2 Partition function estimation
We now evaluate the accuracy of the partition function estimate given by our Algorithm 4 applied
on the M 4 solution. First, we empirically verify our intuition about randomized rounding returning
conﬁgurations that account for most of the probability mass in (12). For a 5-class MRF with
CS(A) = 2.5, we bucket the attainable f values over different conﬁgurations of x, and compute
the probability mass in each bucket. The buckets are then arranged in an increasing order of the
probability mass, and the bars in Figure 1c show the mass in this order. Then, we obtain 1000 samples

1Source code for our experiments is available at https://github.com/locuslab/sdp_mrf.

7

104103102101100Time elapsed0.00.20.40.60.81.0Relative error (fff)AIS (3, 1, 500)M4+ (500)M4 (500)0.00.51.01.52.02.53.03.5Coupling strength0.000.010.020.030.040.05Relative error (fff)M4+ (500) (~0.003s)M4 (500) (~0.002s)DecMAP (~0.036s)Max-product (~0.007s)Ranked order0.000.050.100.150.200.250.300.35Probability Mass (1Zexp(f))All massSampled by rounding(a) Complete graph k = 2, n = 20 (b) Complete graph k = 2, n = 20 (c) Complete graph k = 3, n = 10

(d) ER graph k = 2, n = 20

(e) ER graph k = 2, n = 20

(f) Complete graph k = 4, n = 8

Figure 2: Estimation of Z

via randomized rounding, and ﬁll in with blue the probability mass corresponding to the observed
samples. We can observe that with just 1000 iterations of rounding, the obtained samples constitute
most of the probability mass. Next, we consider coupling matrices over a range of coupling strengths
and plot the error | log Z − log ˆZ| against the coupling strength. We note here that for k > 2, there
is no straightforward way in which we could extend the formulation in Spectral AI [26] to multiple
classes; hence we only provide comparisons with AIS in this case. In the binary case (Figures 2a, 2d),
we can observe that our estimates are more accurate than both Spectral AI [26] and Wang et al. [35]
almost everywhere. Importantly, in the high-coupling strength setting, where the performance of
Spectral AI [26] becomes pretty inaccurate, we are still able to maintain high accuracy. We also note
that with just 500 rounding iterations, the running time of our algorithm is faster than Spectral AI
[26]. We also provide comprehensive comparisons with AIS in Figures 2b, 2e,2c, 2f,3a over a range
of parameter settings of K and num cycles. We can see in the plots that on increasing the number
of temperatures (K), the AIS estimates become more accurate, but suffer a lot with respect to time.
Finally, we also analyze the performance of Algorithm 4 applied to the M 4+ solution in Figures 3b,
3c,3d. We observe that the M 4+ estimates for Z are slightly worse when compared to M 4 for larger
k, but still much more accurate and efﬁcient when compared to AIS.

Image segmentation

5.3
In this section, we demonstrate that our method of inference is able to scale up to large fully connected
CRFs used in image segmentation tasks. Here, we consider the setting as in DenseCRF [19] where
the task is to compute the conﬁguration of labels x ∈ [k]n for the pixels in an image that maximizes:

max
x∈[k]n

(cid:88)

i<j

µ(xi, xj) ¯K(fi, fj) +

(cid:88)

i

ψu(xi).

The ﬁrst term provides pairwise potentials where ¯K is modelled as a Gaussian kernel that measures
similarity between pixel-feature vectors fi, fj and µ is the label compatibility function. The second
term corresponds to unary potentials for individual pixels. As in the SDP relaxation described above,
we relax each pixel to a unit vector in Rd. We model µ via an inner product, and base the unary
potentials φu on rough annotations provided with the images to derive the following objective:

max
vi∈Rd, (cid:107)vi(cid:107)2=1 ∀i∈[n]

(cid:88)

i<j

¯K(fi, fj)vT

i vj + θ

n
(cid:88)

k
(cid:88)

i=1

l=1

log pi,l · vT

i rl.

(15)

In the second term above, log pi,l plugs in our prior belief based on annotations for the ith pixel
being assigned the lth label. The coefﬁcient θ helps control the relative weight on pairwise and

8

012345Coupling strength0246810121416|logZlogZ|Spectral AI [~0.3s]Wang et al. d=2 (500) [~0.2s]M4 (500) [~0.2s]012345Coupling strength0.00.51.01.52.02.5|logZlogZ|AIS (25, 1, 500) [~2.1s]AIS (25, 5, 500) [~9.8s]AIS (50, 1, 500) [~4.0s]AIS (50, 5, 500) [~17.4s]M4 (500) [~0.2s]0.00.51.01.52.02.53.03.5Coupling strength01234567|logZlogZ|AIS (3, 1, 500) (~3.6s)AIS (3, 5, 500) (~17.3s)AIS (5, 1, 500) (~7.3s)AIS (5, 5, 500) (~34.7s)M4 (5000) (~0.9s)012345Coupling strength05101520|logZlogZ|Spectral AI [~0.3s]Wang et al. d=2 (500) [~0.2s]M4 (500) [~0.2s]012345Coupling strength012345|logZlogZ|AIS (25, 1, 500) [~2.0s]AIS (25, 5, 500) [~9.5s]AIS (50, 1, 500) [~4.0s]AIS (50, 5, 500) [~17.4s]M4 (500) [~0.2s]0.00.51.01.52.02.53.03.5Coupling strength01234|logZlogZ|AIS (3, 1, 500) (~3.8s)AIS (3, 5, 500) (~18.5s)AIS (5, 1, 500) (~7.6s)AIS (5, 5, 500) (~36.8s)M4 (5000) (~0.9s)(a) Complete graph k = 5, n = 7

(b) Complete graph k = 3, n = 10

(c) Complete graph k = 4, n = 8

(d) Complete graph k = 5, n = 7

(e) Original image, Annotated image, Segmented image

(f) Pairs of Original and Segmented images

Figure 3: (a), (b), (c), (d) Estimation of Z (e) For the tree, we show the original image, annotations
and segmented image (f) Segmentations computed on other images based on similar annotations

unary potentials. We note here that running MCMC-based methods on MRFs with as many nodes
as pixels in standard images is generally infeasible. However, we can solve (15) efﬁciently via the
mixing method. At convergence, using the rounding scheme described in Algorithm 2, we are able to
obtain accurate segmentations of images (Figures 3e, 3f), competitive with the quality presented in
DenseCRF [19]. More details regarding the setting here are described in Appendix G.

6 Conclusion and Future Work

In this paper, we presented a novel relaxation to estimate the mode in a general k-class Potts model
that can be written as a low-rank SDP and solved efﬁciently by a recently proposed low-rank solver
based on coordinate descent. We further introduced a relaxation that allows for approximation
guarantees. We also proposed a simple and intuitive algorithm based on importance sampling which
guarantees an unbiased estimate of the partition function. We set up experiments to empirically study
the performance of our method as compared to relevant state-of-the-art methods in approximate
inference, and veriﬁed that our relaxation provides an accurate estimate of the mode, while our
algorithm for computing the partition function also gives fast and accurate estimates. We also
demonstrated that our method is able to scale up to very large MRFs in an efﬁcient manner.

The simplicity of our algorithm also lends itself to certain areas for improvement. Speciﬁcally, in
the case of MRFs that have many well-separated modes, an accurate estimate of Z should require
sampling around each of the modes. Although we did empirically observe that randomized rounding
samples most of the probability mass, the next steps involve studying other structured sampling
mechanisms that indeed guarantee adequate sampling around each of the modes.

9

0.00.51.01.52.02.53.03.5Coupling strength0.00.51.01.52.02.53.03.5|logZlogZ|AIS (3, 1, 500) (~4s)AIS (3, 5, 500) (~19.6s)AIS (5, 1, 500) (~8s)AIS (5, 5, 500) (~39.4s)M4 (5000) (~0.9s)0.00.51.01.52.02.53.03.5Coupling strength0.00.51.01.52.02.5|logZlogZ|AIS (5, 5, 500) (~34.7s)M4+ (5000) (~0.9s)M4 (5000) (~0.9s)0.00.51.01.52.02.53.03.5Coupling strength0.000.250.500.751.001.251.501.75|logZlogZ|AIS (5, 5, 500) (~36.8s)M4+ (5000) (~0.9s)M4 (5000) (~0.9s)0.00.51.01.52.02.53.03.5Coupling strength0.00.20.40.60.81.01.2|logZlogZ|AIS (5, 5, 500) (~36.8s)M4+ (5000) (~0.9s)M4 (5000) (~0.9s)Broader Impact

Probabilistic inference has been used in a number of domains including, e.g. the image segmentation
domains highlighted in our ﬁnal experimental results section. However, the methods have also been
applied extensively to biological applications, such as a protein side chain prediction or protein
design [36]. Such applications all have the ability to be directly affected by upstream algorithmic
improvements to approximate inference methods. This also, however, applies to potentially question-
able applications of machine learning, such as those used by automated surveillance systems. While
it may be difﬁcult to assess the precise impact of this work in such domains (especially since the
vast majority of deployed systems are based upon deep learning methods rather than probabilistic
inference at this point), these are applications that should be considered in the further development of
probabilistic approaches.

From a more algorithmic perspective, many applications of approximate inference in recent years
have become dominated by end-to-end deep learning approaches, forgoing application of probabilistic
inference altogether. One potential advantage of our approach, which we have not explored in this
current work, is that because it is based upon a continuous relaxation, the probabilistic inference
method we present here can itself be made differentiable, and used within an end-to-end pipeline.
This has potentially potentially positive effects (it could help in the interpretability of deep networks,
for example), but also negative effects, such as the possibility that the inference procedure itself
actually becomes less intuitively understandable if it’s trained solely in an end-to-end fashion. We
hope that both these perspectives, as well as potential enabled applications, are considered in the
possible extension of this work to these settings.

Acknowledgments

Po-Wei Wang is supported by a grant from the Bosch Center for Artiﬁcial Intelligence.

References

[1] Alexander Barvinok. A remark on the rank of positive semideﬁnite matrices subject to afﬁne

constraints. Discrete & Computational Geometry, 25(1):23–31, 2001.

[2] Steven J. Benson and Yinyu Ye. DSDP5: Software for semideﬁnite programming. Technical Re-
port ANL/MCS-P1289-0905, Mathematics and Computer Science Division, Argonne National
Laboratory, Argonne, IL, September 2005. Submitted to ACM Transactions on Mathematical
Software.

[3] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for

statisticians. Journal of the American statistical Association, 112(518):859–877, 2017.

[4] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates
of mrf log-likelihood using reverse annealing. In Artiﬁcial Intelligence and Statistics, pages
102–110, 2015.

[5] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming, 95(2):329–357,
2003.

[6] David Carlson, Patrick Stinson, Ari Pakman, and Liam Paninski. Partition functions from
rao-blackwellized tempered sampling. In International Conference on Machine Learning, pages
2896–2905, 2016.

[7] Rina Dechter. Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelligence,

113(1-2):41–85, 1999.

[8] Rina Dechter and Irina Rish. Mini-buckets: A general scheme for bounded inference. Journal

of the ACM (JACM), 50(2):107–153, 2003.

[9] Stefano Ermon, Carla Gomes, Ashish Sabharwal, and Bart Selman. Taming the curse of
dimensionality: Discrete integration by hashing and optimization. volume 28 of Proceedings of
Machine Learning Research, pages 334–342, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.

10

[10] Alan Frieze and Mark Jerrum. Improved approximation algorithms for max k-cut and max bi-
section. In International Conference on Integer Programming and Combinatorial Optimization,
pages 1–13. Springer, 1995.

[11] Roy Frostig, Sida Wang, Percy S Liang, and Christopher D Manning. Simple map inference via
low-rank relaxations. In Advances in Neural Information Processing Systems, pages 3077–3085,
2014.

[12] Andreas Galanis, Daniel ˇStefankoviˇc, and Eric Vigoda. Inapproximability for antiferromagnetic
spin systems in the tree nonuniqueness region. Journal of the ACM (JACM), 62(6):1–60, 2015.

[13] Michael R Garey and David S Johnson. Computers and intractability, volume 174. freeman

San Francisco, 1979.

[14] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on pattern analysis and machine intelligence, (6):721–
741, 1984.

[15] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM),
42(6):1115–1145, 1995.

[16] W Keith Hastings. Monte carlo sampling methods using markov chains and their applications.

1970.

[17] Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the ising

model. SIAM Journal on computing, 22(5):1087–1116, 1993.

[18] Hariprasad Kannan, Nikos Komodakis, and Nikos Paragios. Tighter continuous relaxations
for map inference in discrete mrfs: A survey. In Handbook of Numerical Analysis, volume 20,
pages 351–400. Elsevier, 2019.

[19] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference in fully connected crfs with gaussian
edge potentials. In Advances in neural information processing systems, pages 109–117, 2011.

[20] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised
convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 3159–3167, 2016.

[21] Qiang Liu, Jian Peng, Alexander Ihler, and John Fisher III. Estimating the partition function
by discriminance sampling. In Proceedings of the Thirty-First Conference on Uncertainty in
Artiﬁcial Intelligence, pages 514–522, 2015.

[22] Joris M. Mooij. libDAI: A free and open source C++ library for discrete approximate inference
in graphical models. Journal of Machine Learning Research, 11:2169–2173, August 2010.

[23] Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125–139,

2001.

[24] Alantha Newman. Complex semideﬁnite programming and max-k-cut. arXiv preprint

arXiv:1812.10770, 2018.

[25] Giorgio Parisi. Statistical ﬁeld theory. Addison-Wesley, 1988.

[26] Sejun Park, Eunho Yang, Se-Young Yun, and Jinwoo Shin. Spectral approximate inference. In
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pages 5052–5061, Long Beach, California, USA, 09–15 Jun 2019. PMLR.

[27] G´abor Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of

optimal eigenvalues. Mathematics of operations research, 23(2):339–358, 1998.

[28] Nico Piatkowski and Katharina Morik. Stochastic discrete clenshaw-curtis quadrature. vol-
ume 48 of Proceedings of Machine Learning Research, pages 3000–3009, New York, New
York, USA, 20–22 Jun 2016. PMLR.

11

[29] Allan Sly and Nike Sun. The computational hardness of counting in two-spin models on
d-regular graphs. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science,
pages 361–369. IEEE, 2012.

[30] David Sontag, Talya Meltzer, Amir Globerson, Tommi S Jaakkola, and Yair Weiss. Tightening

lp relaxations for map using message passing. arXiv preprint arXiv:1206.3288, 2012.

[31] MF Sykes, JW Essam, and DS Gaunt. Derivation of low-temperature expansions for the ising
model of a ferromagnet and an antiferromagnet. Journal of Mathematical Physics, 6(2):283–298,
1965.

[32] Po-Wei Wang, Wei-Cheng Chang, and J Zico Kolter. The mixing method:

low-rank co-
ordinate descent for semideﬁnite programming with diagonal constraints. arXiv preprint
arXiv:1706.00476, 2017.

[33] Po-Wei Wang, Priya L Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and
logical reasoning using a differentiable satisﬁability solver. arXiv preprint arXiv:1905.12149,
2019.

[34] Po-Wei Wang and J Zico Kolter. Low-rank semideﬁnite programming for the max2sat problem.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 1641–1649,
2019.

[35] Sida I Wang, Roy Frostig, Percy Liang, and Christopher D Manning. Relaxations for inference

in restricted boltzmann machines. arXiv preprint arXiv:1312.6205, 2013.

[36] Chen Yanover, Talya Meltzer, and Yair Weiss. Linear programming relaxations and belief
propagation–an empirical study. Journal of Machine Learning Research, 7(Sep):1887–1907,
2006.

[37] Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding belief propagation and
its generalizations. Exploring artiﬁcial intelligence in the new millennium, 8:236–239, 2003.

12

A Proof of Equivalence of (8) and (9)

We state (8) and (9) again. Let the vectors r1, . . . , rk ∈ Rn be ﬁxed on the simplex. Then, (8) is
stated as follows:

max
vi∈Rn, (cid:107)vi(cid:107)2=1 ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl.

Let H ∈ Rn×k such that Hij = ˆh(j)

i

. Deﬁne the block matrix C ∈ R(k+n)×(k+n) such that:

C =

(cid:20) 0
1
2 · H

1

2 · H T
A

(cid:21)

.

Then, (9) is stated as follows:

max
Y (cid:23)0

Y · C

subject to Yii = 1 ∀i ∈ [n + k]

Yij = −

1
k − 1

∀i ∈ [k], i < j ≤ k.

(8)

(9)

We will show that the optimal solutions to both these optimization problems are equal. Consider
any v1, . . . , vn ∈ Rn in the feasible set of (8). Then, corresponding to these v1, . . . , vn, consider the
matrix Y ∈ R(k+n)×(k+n) deﬁned as follows:












[r1

. . .

rk

v1

. . .

vn] .

Y =












rT
1
...
rT
k
vT
1
...
vn

Clearly, Y (cid:23) 0. Further, since r1, . . . , rk are on the simplex and v1, . . . , vn are in the feasible set of
(8), this matrix Y satisﬁes the constraints in (9). Thus, Y lies in the feasible set of (9). Also, because
of the way in which the block matrix C is deﬁned, we can verify that:

Y · C =

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl.

Thus, for any v1, . . . , vn in the feasible set of (8), we have a corresponding Y in the feasible set of
(9) such that the criterion values match.

k and the last n columns of U as v(cid:48)

Now, consider any Y in the feasible set of (9). Since Y (cid:23) 0, we can compute its Cholesky
decomposition as Y = U T U for some U ∈ R(k+n)×(k+n). Denote the ﬁrst k columns in U as
r(cid:48)
1, . . . , r(cid:48)
n. Then, since Y satisﬁes the constraints in (9),
we have that (cid:107)v(cid:48)
j = 1 if i = j and r
otherwise. Thus, the vectors r(cid:48)
exists a rotation matrix ¯R ∈ Rn×n such that ¯Rr(cid:48)
the vectors vi = ¯Rv(cid:48)

j = − 1
k−1
k correspond to the vertices of a simplex in Rn. Then, there
l = rl for all i ∈ [k] and ¯RT ¯R = I. Then, consider
i for i ∈ [n]. Since rotation matrices preserve norm, we have that (cid:107)vi(cid:107)2 = 1 for

i(cid:107)2 = 1 for all i ∈ [n]. Also, we have that r

1, . . . , v(cid:48)

1, . . . , r(cid:48)

(cid:48)T
i r(cid:48)

(cid:48)T
i r(cid:48)

13

all i ∈ [n]. Thus, v1, . . . , vn lie in the feasible set of (8). Also, we have that:

Y · C = U T U · C

=

=

=

=

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

Aijv

(cid:48)T
i vj +

n
(cid:88)

v

(cid:48)T
i

k
(cid:88)

i=1

l=1

ˆh(l)
i r(cid:48)
l

Aijv

(cid:48)T
i

¯RT ¯Rvj +

n
(cid:88)

i=1

k
(cid:88)

v

(cid:48)T
i

¯RT

ˆh(l)
i

¯Rr(cid:48)
l

(since ¯RT ¯R = I)

l=1

k
(cid:88)

l=1

ˆh(l)
i rl

(since ¯Rr(cid:48)

i = ri)

Aij( ¯Rv(cid:48)

i)T ¯Rvj +

n
(cid:88)

( ¯Rv(cid:48)

i)T

AijvT

i vj +

n
(cid:88)

vT
i

i=1

k
(cid:88)

ˆh(l)
i rl.

i=1

l=1

Thus, corresponding to any Y in the feasible set of (9), we have found vectors v1, . . . , vn in the
feasible set of (8) such that criterion values match.

Consequently, we have shown the range of criterion values in both optimization problems is the same,
and hence the optimization problems have equivalent optimal solutions.

14

B Derivation of (11)

k (1k×k ⊗ Im)(cid:1) where
Let z1, . . . , zn ∈ Rd such that d = m · k, m ∈ Z, and let C = k
k−1
1k×k is a matrix ﬁlled with 1s. Let C = ST S denote the Cholesky decomposition of C. Further, let
i ∈ Rm denotes the bth block. Then, we state (11) again:
us segment each zi into k blocks such that zb

(cid:0)Id − 1

max
zi∈Rd ∀i∈[n]

n
(cid:88)

n
(cid:88)

i=1

j=1

AijvT

i vj +

n
(cid:88)

vT
i

k
(cid:88)

i=1

l=1

ˆh(l)
i rl

subject to

zi ≥ 0,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

b=1

zb
i

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

= 1, vi = Szi ∀i ∈ [n].

(11)

i vj ≥ −1
First, we show that with the parameterization above, 1 ≥ vT
constraints. Note that with the structure of C as deﬁned, we have that

k−1 , i.e. vi, vj satisfy the pairwise

i vj = zT
vT
= zT

i ST Szj
i Czj

=

k
k − 1


zT

i zj −

1
k

(cid:32) k

(cid:88)

zb
i

(cid:33)T (cid:32) k
(cid:88)

(cid:33)
 .

zb
j

b=1

b=1

Now, since zi ≥ 0 and since

(cid:13)
(cid:13)
(cid:13)

(cid:80)k

b=1 zb
i

(cid:13)
2
(cid:13)
(cid:13)
2

Schwartz inequality, we have that 0 ≤ zT
i vj ≥ k
Thus, we have vT
k−1

(cid:0)0 − 1

(cid:1) = − 1

i zj ≤ 1, and also that, 0 ≤
k−1 . Further, note that

k

= 1, we have that (cid:107)zi(cid:107)2 ≤ 1. Thus, by the Cauchy-
(cid:16)(cid:80)k

(cid:17)T (cid:16)(cid:80)k

(cid:17)

b=1 zb
i

b=1 zb
j

≤ 1.

(cid:107)vi(cid:107)2

2 =

k
k − 1


(cid:107)zi(cid:107)2

2 −

=

k
k − 1

(cid:18)

(cid:107)zi(cid:107)2

2 −

1
k

1
k





2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

zb
i

k
(cid:88)

b=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:19)

≤ 1.

i in each zi to e1 ∈ Rm (where e1 is the ﬁrst basis vector), and set all the other zb(cid:48)

Thus, we have that vT
i vj ≤ (cid:107)vi(cid:107)2(cid:107)vj(cid:107)2 ≤ 1, establishing both bounds. Next, note that we can set the
appropriate zb
to 0,
and this allows vi = Szi to be the required vector rl on the simplex corresponding to the optimal
solution to the discrete problem (7). Thus, we have that the optimal solution to (11) is at least as large
as f (cid:63)

i

discrete.

Our goal then is to obtain candidates z1, . . . , zn, such that the objective is at least as large as f (cid:63)
Additionally, if we can further guarantee that (cid:107)zi(cid:107)2 = 1 for all i, we are done.

discrete.

Let us write the objective in (11) in terms of zi. This is

n
(cid:88)

n
(cid:88)

i=1

j=1

AijzT

i Czj +

n
(cid:88)

i=1

i ST
zT

k
(cid:88)

l=1

ˆh(l)
i rl.

Let us consider the terms in the objective involving a particular zi. These are



ˆh(l)
i rl

.



k
(cid:88)

l=1

(cid:125)



zT
i

2

n
(cid:88)

j(cid:54)=i

(cid:124)

AijCzj + ST

(cid:123)(cid:122)
gi

15

Note that for every index j within a block in gi, across the k blocks, there will deﬁnitely be at least
one positive entry. This is because

2

n
(cid:88)

j(cid:54)=i

AijCzj + ST

k
(cid:88)

l=1

ˆh(l)
i rl = 2

= 2

n
(cid:88)

j(cid:54)=i

n
(cid:88)

j(cid:54)=i


AijCzj + ST

k
(cid:88)

l=1

ˆh(l)
i Sel

AijCzj + C

n
(cid:88)

j(cid:54)=i

Aijzj +

(cid:123)(cid:122)
p

k
(cid:88)

l=1

k
(cid:88)

l=1

ˆh(l)
i el



ˆh(l)
i el



(cid:125)

= C

2

(cid:124)

= Cp.

and because of the nature of the matrix C, the entries at a particular index j across the k blocks in
Cp will each be of the form x − avg(x). This fact will be useful later on.

We now consider updating each zi in a sequential manner as a block-coordinate update, just as in
the original mixing method. In the following, we drop the subscript i in zi and gi for convenience.
Concretely, we aim to solve the problem

min − gT z

subject to

z ≥ 0;

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

b=1

zb

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

= 1.

(16)

Let us write the Lagrangian L(z, α, λ) for the above constrained optimization problem, for dual
variables α ≥ 0, λ:

L(z, α, λ) = −gT z +

λ
2

The KKT conditions are





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

b=1


 − αT z.

− 1

zb

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

Stationarity:

i + αb
gb

i = λ

k
(cid:88)

zb
i ∀b ∈ [k], i ∈ [m]

b=1
i = 0 ∀b ∈ [k], i ∈ [m]

Primal feasibility:

i zb
Complementary slackness: αb
zb
i ≥ 0 ∀b ∈ [k], i ∈ [m]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Dual feasibility: αb

b=1
i ≥ 0 ∀b ∈ [k], i ∈ [m].

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

k
(cid:88)

= 1

zb

Note that now, zb
i refers to the ith entry in the bth block in z. Since the KKT conditions are always
sufﬁcient, if we are able to construct z and α, λ that satisfy all the conditions above, z and α, λ would
be optimal primal and dual solutions to (16) respectively.

Towards this, let (·)+ denote the operation that thresholds the argument at 0, i.e.

(x)+ =

(cid:26)x if x ≥ 0

0

otherwise.

16

For any ﬁxed index i ∈ [m], let b(i) = arg maxb gb
following assignment:

i (if there are multiple, pick any). Consider the

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

(gb(i)
i

)2
+

λ =

i=1
(gb(i)
i
λ

zb(i)
i =

)+

, αb(i)

i =

(cid:40)
0
−gb(i)
i

i > 0

if gb(i)
otherwise

i = 0, αb
zb

i + λzb(i)
Note that λ > 0, since we argued above that there will be at least one entry that will be positive
across the blocks. We will now verify that this assignment satisﬁes all the KKT conditions. First,
note that (cid:80)k

. Consider stationarity: for b(i), if gb(i)

for b (cid:54)= b(i).

i = −gb

i

b=1 zb

i = zb(i)

i

i + αb(i)
gb(i)

i = gb(i)

i = (gb(i)

i

i > 0,
)+ = λzb(i)

.

i

otherwise if gb(i)

i ≤ 0, zb(i)

i = 0 and so
i + αb(i)
gb(i)

i = gb(i)

i − gb(i)

i = 0 = λzb(i)

i

.

For b (cid:54)= b(i), by construction

i + αb
gb

i = λzb(i)

i

.

Next, we can observe that complementary slackness holds, since either one of zb
Next, we verify primal feasibility. We can observe that zb
i ≥ 0 for all b. Further,

i or αb

i is always 0.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

b=1

zb

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

=

m
(cid:88)

i=1

zb(i)2
i

=

1
λ2

m
(cid:88)

i=1

(gb(i)
i

)2
+ = 1.

Finally, we verify dual feasibility. For b(i), we have that

αb(i)

i =

(cid:40)
0
−gb(i)
i

if gb(i)
i > 0
otherwise.

.

Either way, αb(i)

i ≥ 0. For b (cid:54)= b(i),

i + (gb(i)

i + λzb(i)

i = −gb
αb

i = −gb
Thus, we observe that the constructed z and α, λ satisfy all the KKT conditions. Hence, z (as
constructed as above) is the optimal solution to (16). Algorithm 3 precisely updates each zi based on
this constructed solution. The hope at the convergence of this routine is that we will have ended up
with a solution v1, . . . , vn such that f (v1, . . . , vn) > f (cid:63)
discrete. Empirically, we always observe that
this is the case. In fact, the solution at convergence is within 5% of the true optimal solution of (11)
itself. Thus, the approximation guarantees of Frieze et al. [10] go through for the rounded solution
on v1, . . . , vn at convergence, assuming that the entries in A are positive.

)+ ≥ 0.

i

17

C Proof of Theorem 1

We have that

(cid:105)

[ ˆZ]


(cid:104)
E·|Xpv

E·|Xpv

E[ ˆZ] = EXpv

= EXpv

= EXpv

= EXpv

= EXpv

= EXpv

= EXpv

= EXpv

= EXpv





(cid:88)

x∈Xpv





(cid:88)

x∈Xpv





(cid:88)

x∈Xpv





(cid:88)

x∈Xpv





(cid:88)

x∈Xpv





(cid:88)

x∈Xpv





(cid:88)

x∈[k]n

(cid:88)



x∈Xpv

exp(f (x)) +

exp(f (x)) +

1
R

E·|Xpv

1
R

(cid:34)

(cid:88)

x∈XΩ

exp(f (x))
q









(cid:88)

x∈XΩ

exp(f (x))
q

(cid:35)


exp(f (x)) +

exp(f (x)) +

exp(f (x)) +

exp(f (x)) +

1
Rq

1
Rq

(cid:88)

x∈XΩ

E·|Xpv

[exp(f (x))]





(cid:88)

(cid:88)

q · exp(f (y))

x∈XΩ

y∈{[k]n\Xpv }





1
R

1
R

(cid:88)

(cid:88)

x∈XΩ

y∈{[k]n\Xpv }





exp(f (y))

· R ·

(cid:88)

exp(f (y))

y∈{[k]n\Xpv }





exp(f (x)) +

(cid:88)

exp(f (y))

y∈{[k]n\Xpv }





exp(f (x))





[Z]

= EXpv
= Z.

Thus, the estimate ˆZ given by Algorithm 4 is unbiased.

18

D Pseudocode for AIS

Our implementation of AIS has 3 main parameters: the number of temperatures in the annealing
chain (denoted K), the number of cycles of Gibbs sampling while transitioning from one temperature
to another (denoted num cycles), and the number of samples used (denoted num samples). First,
we deﬁne K + 1 coefﬁcients 0 = β0 < β1 < · · · < βK = 1 . Then, given a general k-class MRF
problem instance as deﬁned in Sections 3, 4, let

f (x) =

n
(cid:88)

n
(cid:88)

i=1

j=1

Aij

ˆδ(xi, xj) +

n
(cid:88)

k
(cid:88)

i=1

l=1

ˆh(l)
i

ˆδ(xi, l).

Further, deﬁne functions fk as follows:

fk(x) =

(cid:19)1−βk

(cid:18) 1
kn

(exp(f (x)))βk .

Also, let p0 denote the uniform distribution on the discrete hypercube [k]n. The complete pseudocode
for our implementation of AIS is then provided below:

Let p(x) ∝ (exp(f (x)))βk
for cycle = 1, 2 . . . , num cycles do

Algorithm 5 Annealed Importance Sampling
1: procedure GIBBSSAMPLING(x, βk, num cycles)
2:
3:
4:
5:
6:
7:
8:
9: end procedure

xi ← Sample p(xi|x−i)

for i = 1, 2, . . . , n do

end for
return x

end for

for i = 1, 2 . . . , num samples do

10: procedure AIS(K, num cycles, num samples)
11:
12:
13:
14:
15:

Sample x ∼ p0
w(i) ← 1
for k = 1, 2, . . . , K do
fk(x)
w(i) ← w(i) ·
fk−1(x)
x ← GIBBSSAMPLING(x, βk, num cycles)

end for

16:
17:
end for
18:
return Z =
19:
20: end procedure

1
num samples

(cid:80)num samples

i=1

w(i)

19

E Mode estimation comparisons
Here, we compare the mode estimates given by M 4 and M 4+ with max-product belief propagation
and decimation algorithm given in libDAI [22] over complete graphs across a range of coupling
strengths for k = 2, 3, 4, 5.

(a) k = 2, n = 20

(b) k = 3, n = 10

(c) k = 4, n = 8

(d) k = 5, n = 7

Figure 4: Mode estimation comparison with max-product BP and decimation

We can observe that for both methods, the relative errors are very small (∼ 0.018 at worst) compared
to the other methods, but M 4+ suffers a little for larger k.

Next, we show the results for the mode estimation task (timing comparison versus AIS) on complete
graphs for k = 2, 3, 4, 5. The coupling matrices are ﬁxed to have a coupling strength CS(A) = 2.5.

(a) k = 2, n = 20

(b) k = 3, n = 10

(c) k = 4, n = 8

(d) k = 5, n = 7

Figure 5: Mode estimation comparison with AIS

We can observe that both M 4 and M 4+ are able to achieve an accurate estimate of the mode much
quicker than AIS across different values of k.

20

012345Coupling strength0.000.010.020.030.04Relative error (fff)M4+ (500) (~0.003s)M4 (500) (~0.002s)DecMAP (~0.594s)Max-product (~0.037s)0.00.51.01.52.02.53.03.5Coupling strength0.0000.0050.0100.0150.0200.0250.0300.035Relative error (fff)M4+ (500) (~0.003s)M4 (500) (~0.002s)DecMAP (~0.077s)Max-product (~0.009s)0.00.51.01.52.02.53.03.5Coupling strength0.000.010.020.030.040.05Relative error (fff)M4+ (500) (~0.003s)M4 (500) (~0.002s)DecMAP (~0.047s)Max-product (~0.007s)0.00.51.01.52.02.53.03.5Coupling strength0.000.010.020.030.040.05Relative error (fff)M4+ (500) (~0.003s)M4 (500) (~0.002s)DecMAP (~0.036s)Max-product (~0.007s)104103102101100Time elapsed0.00.20.40.60.81.0Relative error (fff)AIS (25, 1, 500)M4+ (500)M4 (500)104103102101100Time elapsed0.00.20.40.60.81.0Relative error (fff)AIS (3, 1, 500)M4+ (500)M4 (500)104103102101100Time elapsed0.00.20.40.60.81.0Relative error (fff)AIS (3, 1, 500)M4+ (500)M4 (500)104103102101100Time elapsed0.00.20.40.60.81.0Relative error (fff)AIS (3, 1, 500)M4+ (500)M4 (500)F Performance of AIS with varying parameters

Here, we demonstrate how the performance of AIS is affected on separately varying the parameters K
and num cycles (Algorithm 5) in the partition function task. We consider similar problem instances
described in Section 5 in the paper:

1. We ﬁx num cycles = 1 and vary K. Figure 6 shows the results. We can observe that
increasing K helps increase the accuracy of the estimate of Z, but also becomes very
expensive w.r.t. time.

(a) Complete graph k = 2, n = 20

(b) ER graph k = 2, n = 20

(c) Complete graph k = 3, n = 10

Figure 6: Variation of K in AIS

2. Next, we ﬁx K and vary num cycles in the Gibbs sampling step. Figure 7 shows the results.
We can observe that increasing num cycles helps increase the accuracy of the estimate of
Z (although the effect is much less pronounced when compared to increasing K), but also
becomes very expensive w.r.t. time.

(a) Complete graph k = 2, n = 20

(b) ER graph k = 2, n = 20

(c) Complete graph k = 3, n = 10

Figure 7: Variation of num cycles in AIS

21

012345Coupling strength0.00.51.01.52.02.5|logZlogZ|AIS (25, 1, 500) [~2.1s]AIS (50, 1, 500) [~4.0s]AIS (75, 1, 500) [~6.2s]AIS (100, 1, 500) [~7.9s]M4 (500) [~0.2s]012345Coupling strength012345|logZlogZ|AIS (25, 1, 500) [~2.0s]AIS (50, 1, 500) [~4.0s]AIS (75, 1, 500) [~6.5s]AIS (100, 1, 500) [~8.0s]M4 (500) [~0.2s]0.00.51.01.52.02.53.03.5Coupling strength01234567|logZlogZ|AIS (3, 1, 500) (~3.6s)AIS (5, 1, 500) (~7.3s)AIS (7, 1, 500) (~10.8s)AIS (10, 1, 500) (~16.2s)M4 (5000) (~0.9s)012345Coupling strength0.00.51.01.52.02.5|logZlogZ|AIS (25, 1, 500) [~2.1s]AIS (25, 3, 500) [~5.6s]AIS (25, 5, 500) [~9.8s]AIS (25, 7, 500) [~12.2s]M4 (500) [~0.2s]012345Coupling strength012345|logZlogZ|AIS (25, 1, 500) [~2.0s]AIS (25, 3, 500) [~5.4s]AIS (25, 5, 500) [~9.5s]AIS (25, 7, 500) [~12.0s]M4 (500) [~0.2s]0.00.51.01.52.02.53.03.5Coupling strength01234567|logZlogZ|AIS (3, 1, 500) (~3.6s)AIS (3, 3, 500) (~10.6s)AIS (3, 5, 500) (~17.3s)AIS (3, 7, 500) (~24.0s)M4 (5000) (~0.9s)G Image Segmentation - more results

We describe in more detail the setting in DenseCRF [19]. Let fi denote the feature vector associated
with the ith pixel in an image e.g. position, RGB values, etc. Then, the image segmentation task is to
compute the conﬁguration of labels x ∈ [k]n for the pixels in an image that maximizes:

max
x∈[k]n

(cid:88)

i<j

µ(xi, xj) ¯K(fi, fj) +

(cid:88)

i

ψu(xi).

The ﬁrst term provides pairwise potentials where ¯K(fi, fj) is modelled as a Gaussian kernel consist-
ing of smoothness and appearance kernels and the coefﬁcient µ is the label compatibility function.
The second term corresponds to unary potentials for the individual pixels. In keeping with the SDP
relaxation described above, we relax each pixel to Rd to derive the following optimization problem:

max
vi∈Rd, (cid:107)vi(cid:107)2=1 ∀i∈[n]

(cid:88)

i<j

¯K(fi, fj)vT

i vj + θ

n
(cid:88)

k
(cid:88)

i=1

l=1

log pi,l · vT

i rl.

(15)

In the ﬁrst term above, the term vT
i vj models the label compatibility function µ, and we can observe
that if ¯K(fi, fj) is large i.e. the pixels are similar, it encourages the vectors vi and vj to be aligned.
The second term models unary potentials φu from available rough annotations, so that we have a bias
vector rl for each label, and the term log pi,l plugs in our prior belief based on annotations of the ith
pixel being assigned the lth label. The coefﬁcient θ helps control the relative weight on the pairwise
and unary potentials. The mixing method update for the above objective is:

vi ← normalize





(cid:124)

(cid:88)

j(cid:54)=i

¯K(fi, fj)vj + θ

L
(cid:88)

l=1



log pi,l · rl

.



(17)

(cid:123)(cid:122)
Gi

(cid:125)

We note here that computing the pairwise kernels ¯K(fi, fj) naively has a quadratic time complexity
in n, and for standard images, the number of pixels is pretty large, making this computation very slow.
Here, we use the high-dimensional ﬁltering method as in DenseCRF [19] which provides a linear
time approximation for simultaneously updating all the vis as given by the update in (17). However,
because of the simultaneous nature of the updates, we are no longer employing true coordinate
descent. Hence, we instead propose to use a form of gradient descent with a small learning rate α to
update each of the vis as follows. Here, the Gis are those that are simultaneously given for all i at
once by the high-dimensional ﬁltering method:

vi ← normalize(vi + α · Gi).

At convergence, we use the same rounding scheme described in Algorithm 2 above to obtain a
conﬁguration of labels for each pixel. Figures 8, 9 below show the results of using our method for
performing image segmentation on some benchmark images obtained from the works of DenseCRF
[19], Lin et al. [20]. We can see that our method produces accurate segmentations, competitive with
the quality of segmentations demonstrated in DenseCRF[19].

The naive runtime for segmenting a standard (say 400x400) image by our method (without any
GPU parallelization) is roughly ∼ 2 minutes. We remark here that performing each segmentation
constitutes randomly initializing the vi vectors and solving (15) via the mixing method, and also
performing a few rounds of rounding. However, with parallelization and several optimizations, we
believe that there is massive scope for signiﬁcantly reducing this runtime.

22

Figure 8: Original image, annotated image, segmented image

23

Figure 9: Original image, annotated image, segmented image

24

