PyXtal FF: a Python Library for Automated Force Field Generation

Howard Yanxona, David Zagacetaa, Binh Tangb, David Mattesonb, Qiang Zhua,∗

aDepartment of Physics and Astronomy, University of Nevada Las Vegas, Las Vegas, NV 89154, USA
bDepartment of Statistics and Data Science, Cornell University, Ithaca, NY 14853, USA

0
2
0
2

l
u
J

5
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
2
1
0
3
1
.
7
0
0
2
:
v
i
X
r
a

Abstract

We present PyXtal FF—a package based on Python programming language—for developing machine learning potentials (MLPs).
The aim of PyXtal FF is to promote the application of atomistic simulations by providing several choices of structural descrip-
tors and machine learning regressions in one platform. Based on the given choice of structural descriptors (including the atom-
centered symmetry functions, embedded atom density, SO4 bispectrum, and smooth SO3 power spectrum), PyXtal FF can train
the MLPs with either the generalized linear regression or neural networks model, by simultaneously minimizing the errors of en-
ergy/forces/stress tensors in comparison with the data from the ab-initio simulation. The trained MLP model from PyXtal FF is
interfaced with the Atomic Simulation Environment (ASE) package, which allows diﬀerent types of light-weight simulations such
as geometry optimization, molecular dynamics simulation, and physical properties prediction. Finally, we will illustrate the perfor-
mance of PyXtal FF by applying it to investigate several material systems, including the bulk SiO2, high entropy alloy NbMoTaW,
and elemental Pt for general purposes. Full documentation of PyXtal FF is available at https://pyxtal-ﬀ.readthedocs.io.

Keywords: Machine learning potential; Linear regression; Neural networks; Atom-centered descriptors; Atomistic simulation.

PROGRAM SUMMARY
Program Title: PyXtal-FF
Licensing provisions: MIT [1]
Programming language: Python 3
Nature of problem:
the potential energy
In materials modelling,
surface of a system is often computed by either the ab initio method
or an approximated classical force ﬁeld. The former is proven to be
the most accurate but computationally demanding, while the latter is
much cheaper but suﬀers from insuﬃcient accuracy. As such, a new
approach to resolve the dilemma in comprising between accuracy and
cost is needed.
Solution method: By representing the atomic structures to a set of
atom-centered descriptors, one can employ a variety of machine
learning models (e.g., linear regression, artiﬁcial neural networks) to
eﬀectively learn the relationship between the descriptors and energy.
Due to the ﬂexible nature of machine learning, these approaches often
yield better accuracy while maintaining lower computational cost in
comparison to the ab initio method.

References

[1] https://opensource.org/licenses/MIT

1. Introduction

Molecular dynamics (MD) simulations have been used rou-
tinely to model the physical behaviors of many complex sys-
tems [1, 2, 3]. The accuracy of the simulations is highly de-
pendent on the underlying potential energy surface (PES) of

∗Corresponding author.

E-mail address: qiang.zhu@unlv.edu

the system. In principle, MD simulations can be based on ab
initio quantum-mechanical [4] or classical force ﬁeld methods.
The ab initio MD (AIMD) simulations usually employ density
functional theory (DFT) approximation [5], which can provide
a reliable representation of the system. Despite the accuracy,
DFT simulation can be extendable only to a few hundreds of
atoms at a few picoseconds. This is due to solving the Kohn-
Sham equation requires thousands of quantum-mechanical cal-
culations that are scaled at O(N3) with respect to the number of
atoms N. In consequence, simulating the structural evolution
of many of complicated systems in DFT remains demanding
in spite of the remarkable progress in computational facilities
and eﬃcient algorithms. This bottleneck in the DFT method is
likely to persist in the foreseeable future. On the other hand,
the classical MD method can model large systems at long-time
scale, countering the unwavering issue of the DFT method. A
great amount of eﬀorts has been dedicated in developing PES
using the classical method [6, 7, 8, 9, 10]. The reconstruction
of the PES is usually based on simple analytical functions re-
lated to the scalar properties of the system. The class force
ﬁelds can be applied to comprehend the qualitative behavior of
the system. However, they are often inadequate to describe the
quantitative properties of the system.

Recently, machine learning methods have been widely ap-
plied to resolve the dilemma in comprising between accuracy
and cost [11]. The machine learning potential (MLP) are
trained by minimizing the cost function to attune the model
to deliberately describe the ab initio data. The cost of atom-
istic simulation is orders of magnitude lower than the quantum
mechanical simulation, allowing the system to be scaled up to
105–106 atoms [12, 13].

Preprint submitted to Computer Physics Communications

July 28, 2020

 
 
 
 
 
 
Among many diﬀerent ML models, two regression tech-
niques are becoming increasingly popular in the materials mod-
elling community. They include the neural networks and Gaus-
sian process regressions. The neural networks approach has an
unbiased mathematical form that can adapt to any set of refer-
ence points through an iterative ﬁtting process given “enough”
training data. The ﬁrst well accepted neural networks potential
(NNP) was originally applied to elemental silicon system by
Behler and Parrinello [14], which demonstrated that the NNP
was able to reproduce the energetic sequences of many silicon
phases, as well as the radial distribution function of a silicon
melt at 3000 K from DFT simulation. To gain a better pre-
dictive power, they also proposed to use a series of symmetry
functions (see section 2.1.1), instead of the Cartesian coordi-
nates, as the descriptors to represent the atomic environment.
Since then, many attempts have been undertaken to improve
the capability of neural networks approach [15]. The accom-
plishments of neural networks approach have been extended
to multi-component [16, 17] and organic [18] systems. In ad-
dition, Gaussian Approximation Potential (GAP), in conjunc-
tion with the bispectrum coeﬃcients of atomic neighbour den-
sity (see section 2.1.3)), was ﬁrst introduced to model the car-
bon, silicon, germanium, iron, and gallium nitride [19]. GAP
was further enhanced by replacing bispectrum coeﬃcients with
smooth overlapping power spectrum coeﬃcients with explicit
radial basis [20]. Similar to GAP, Thompson et al. [21] devel-
oped (quadratic) Spectral Neighbor Analysis Potential (SNAP)
method based on the Taylor expansion of bispectrum coef-
ﬁcients.
In addition, linear regression model based on the
moment tensor—comparable to atomic environments inertia
tensors—as the descriptor [22] was also demonstrated to be
a competitive approach. Many applications based on diﬀer-
ent MLP models have shown that machine learning potentials
work remarkably well in diﬀerent types of atomistic simula-
tions [23, 24, 25, 26, 27, 28].

In the recent years, several software packages [16, 29, 30,
31, 32, 33] were developed to train the MLPs. Among these,
the RuNNer [16] is a closed source software for developing
NNP, and ænet [34] is mainly written in FORTRAN/C and uti-
lizes atom-centered symmetry functions (see section 2.1.1) as
the descriptor. Similar codes, such as the n2p2 package [29] in
C++, SIMPLE-NN package [30] in Python/C, and AMP pack-
age [31] in Python/FORTRAN, have similar feature as ænet
package. SIMPLE-NN leverages the capability of Tensorﬂow
platform—a deep learning GPU-accelerated library, and AMP
provides several other descriptors such as Zernike and bispec-
trum components. Our recent works also suggested that NNP
can be developed using bispectrum and power spectrum com-
ponents as the descriptor while training on energy, forces, and
stress simultaneously [35, 36]. Moreover, DeepPot-SE [37] and
SchNetPack [33] packages introduce additional ﬁlters to the de-
scriptor such as distance-chemical-species-dependent ﬁlter and
continuous convolutional ﬁlter, respectively, prior to the deep
learning model.

In this paper, we present PyXtal FF—an open-source pack-
age in Python scripting language—for developing MLP such
as NNP and generalized linear potential (GLP). The objective

of PyXtal FF is to provide handy user-interface in developing
MLP with training of energy, force, and stress contributions si-
multaneously. PyXtal FF creates MLP based on atom-centered
descriptors such as (weighted) atom-centered symmetry func-
tions [11], embedded atom density [38], SO(4) bispectrum co-
eﬃcients [19], and smooth SO(3) power spectrum [20]. Finally,
we will demonstrate the usage of the current features of the
package with SiO2 [30], high entropy alloy [39], and elemental
Pt [37] as examples.

2. Theory

In this section, we will provide in-depth discussions of the
two main ingredients in creating MLP: atom-centered descrip-
tor and regression technique. The construction of the total en-
ergy of a crystal structure can be written as the collections of
atomic energy contributions, in which is a functional (E ) of the
atom-centered descriptor (Xi):

Etotal =

N(cid:88)

i=1

Ei =

N(cid:88)

i=1

Ei(Xi)

(1)

Speciﬁcally,
such as neural networks or generalized linear regressions.

the functional represents regression techniques

Since neural networks and generalized linear regressions
have well-deﬁned functional forms, the analytic derivatives can
be derived by applying the chain rule to obtain the force at each
atomic coordinate, rm:

Fm = −

N(cid:88)

i=1

∂Ei(Xi)
∂Xi

·

∂Xi
∂rm

(2)

Force is an important property to accurately describe the local
atomic environment especially in geometry optimization and
MD simulation. Finally, the stress tensor is acquired through
the virial stress relation:

S = −

N(cid:88)

m=1

rm ⊗

N(cid:88)

i=1

∂Ei(Xi)
∂Xi

·

∂Xi
∂rm

,

(3)

where ⊗ is the outer product.

According to Eqs. 2 and 3, one needs to compute the energy
derivative ∂E
∂X and the derivatives of descriptor X with respect to
the atomic positions. For a structure with N atoms and L de-
scriptors per atom, the energy derivative is a 2D array of [N, L].
The force related derivative (dxdr) can be best organized as a 4D
array with the dimension of [N, N, L, 3]. Note that dxdr[i, j, :, :]
is zero when the i- j atomic pair has a distance larger than the
cutoﬀ distance. Thus, it may become a sparse array when the
structure has a large number of atoms. Correspondingly, one
can easily derive the 5D rdxdr array by multiplying r to each
dxdr according to the outer product. In Python, one can simply
compute the forces and stresses based on the following Einstein
summation.

import numpy as np

"""

2

Einstein summation to compute force and stress .
dedx : 2 D array [N , L ]
dxdr : 4 D array [N , N , L , 3]
rdxdr : 5 D array [N , N , L , 3 , 3]
force : 2 D array [N , 3]
stress : 2 D array [3 , 3]
"""
force = - np . einsum ( " ik , ijkl - > jl " , dedx , dxdr )
stress = - np . einsum ( " ik , ijklm - > lm " , dedx , rdxdr )
Listing 1: Force and stress computation in Python.

2.1. Atom-centered Descriptors

Descriptor—a representation of a crystal structure—plays a
critical role in constructing reliable MLP. If the MLP is di-
rectly mapped from the atomic positions or the Cartesian co-
ordinates, it can only describe systems with the same number
of atoms due to the ﬁxed length of the regression input. In ad-
dition, Cartesian coordinates are poor descriptors in describing
the structural environment of the system, restricted by the peri-
odic boundary conditions. While the total energy of the struc-
ture remains the same by translation, rotational, or permutation
operations, the atomic positions will change. Several types of
descriptors have been developed in the past few years [40]. For
example, Coulomb matrix has been widely used due to its sim-
plicity. Coulomb matrix encompasses self interaction based on
the nuclear charge and Coulomb repulsion between two nuclei
[41, 42]. Logically, the Coulomb matrix can be upgraded for
periodic crystals through Ewald summation that includes long
range interaction calculated in reciprocal space.
In addition,
many-body tensor representation—derives from Coulomb ma-
trix while related to bag of bonds which corresponds to diﬀerent
types of bonding in molecular systems—can be used for both
ﬁnite and periodic systems when interpretability/visualization
is desirable [43]. These descriptors have been widely used to
model the molecules.

In the atom-centered descriptors, one usually needs to con-
sider the neighboring environment for the centered atom within
a cutoﬀ radius of Rc. To ensure the descriptor mapping from the
atomic positions smoothly approaching zero at the Rc, a cutoﬀ
function ( fc) is included to every mapping scheme:

fc(R) =

(cid:16)

π R
Rc


1

2 cos

0

(cid:17) + 1

2 R ≤ Rc
R > Rc

(4)

where R is distance. The cutoﬀ function is zero at Rc and the in-
tensity decreases as R approaches Rc. Consequently, the deriva-
tive of the cutoﬀ function is:

∂ fc
∂R

=





− π
2Rc
0

sin

(cid:17)

(cid:16)
π R
Rc

R ≤ Rc
R > Rc

(5)

One should heed of the importance of the vanishing derivative
of cutoﬀ function at Rc, which is important in describing the
force. By deﬁnition, there is no discontinuity as the slope de-
cays to zero at Rc.

In the following, we will introduce four types of atom-
centered descriptors in details. The corresponding derivative
terms can be found in Appendix A, Appendix B and our re-
cent work [36].

2.1.1. (Weighted) Atom-centered Symmetry Functions (G)

The atom-centered symmetry functions (ACSFs) are the very
ﬁrst types of descriptors used in the MLP development [14]. In
general, there are two classes of ACSFs: radial and angular
symmetry functions [11]. The radial symmetry function or G(2)
describes the radial distribution of the atomic environment, and
the angular symmetry functions, G(4) and G(5), account for the
three-body angular distribution of atoms in the neighborhood.
The G(2) is expressed as the sum of the radial distances between
the center atom i and the neighbor atoms j as follow:
(cid:88)

G(2)
i

=

e−η(Ri j−Rs)2 · fc(Ri j)

j(cid:44)i

(6)

Here, G(2) value is controlled by the width (η) and the shift (Rs).
G(4) and G(5) symmetry functions are a few of many ways
to capture the angular information via three-body interactions
(θi jk). As the structures are constraint by the periodic boundary
condition, a three-body periodic description such as cos(θi jk) is
used. The explicit form of G(4) and G(5) are:

G(4)
i

=21−ζ (cid:88)

(cid:88)

[(1 + λ cos θi jk)ζ · e−η(R2

i j

+R2
ik

+R2

jk)·

j(cid:44)i

k(cid:44)i, j

fc(Ri j) · fc(Rik) · fc(R jk)]

G(5)
i

=21−ζ (cid:88)

(cid:88)

j(cid:44)i

k(cid:44)i, j

[(1 + λ cos θi jk)ζ · e−η(R2

i j

+R2

ik)·

fc(Ri j) · fc(Rik)]

(7)

(8)

ζ determines the strength of angular information. The degree
of ζ is normalized by 21−ζ for unvarying the values of G(4) and
G(5) symmetry functions due to ranges of ζ. λ values are set to
+1 and -1, for inverting the shape of the cosine function. The
diﬀerence between G(4) and G(5) symmetry functions is in the
interactions between the neighbors j and k. The modiﬁcation
in G(5) symmetry function yields in dampening value of G(5),
which can be beneﬁcial in representing larger atomic separation
between the two neighbors.

Clearly, the number of ACSFs will grow depending on chem-
ical species as the separations of chemical species are needed.
For instance, in a binary AB system, the number of G(2) ACSFs
on specie A need to double to distinguish A-A and A-B pair
interactions. For G(4), three diﬀerent triplets A-A-A, A-A-B,
B-A-B (where the middle position denotes the center atom)
will be needed. To avoid this unpleasant growth, one can ap-
ply a weighting parameter based on the chemical species when
counting these atomic pairs and triplets. One popular choice is
simply to use the atomic number as the weighting parameters.
Hence, Gastegger and coauthors proposed the weighted version
of ACSF [44], in which each component of the radial and an-
gular symmetry functions in Eqs. (6, 7, 8) can be multiplied by
the followings:

the weighted ACSF:





Z j
Z jZk

radial
angular

3

where Z j, Zk represents the atomic number of neighboring atom
j and k.

To obtain a satisfactory MLP model, one has to choose a set
of parameters to construct the (w)ACSF descriptors, which may
require some demanding human intervention [44, 37, 33]. As
mentioned, the choice of λ is straightforward.
In general, ζ
takes the value of 1. Increasing ζ focuses on the strength of the
angular information in region close to 0◦ and 180◦, and decreas-
ing it will weaken the contribution of angular information at
around 90◦. Since the exponential term has larger eﬀect on the
symmetry functions, the selection of η and Rs can be more elab-
orate. Several routines are available in the literature [44, 38] by
ﬁxing η while varying Rs or vice versa.

2.1.2. Embedded Atom Density (ρ)

Embedded atom density (EAD) descriptor [38] is inspired by
embedded atom method (EAM)—description of atomic bond-
ing by assuming each atom is embedded in the uniform electron
cloud of the neighboring atoms [6, 45]. In EAD, the electron
density is modiﬁed by including the square of the linear combi-
nation the atomic orbital components:

ρi(Ri j) =

lx+ly+lz=Lmax(cid:88)

lx,ly,lz

Lmax!
lx!ly!lz!

(cid:18) N(cid:88)

j(cid:44)i

(cid:19)2

Z jΦ(Ri j)

(9)

where Z j represents the atomic number of neighbor atom j.
Lmax is the quantized angular momentum, and lx,y,z are the quan-
tized directional-dependent angular momentum. For example,
Lmax = 2 corresponds to the d orbital. Lastly, the explicit form
of Φ is:

Where wi is a species dependent weight factor and fc is a cutoﬀ
function. The cutoﬀ function is fc is introduced to ensure that
the atomic neighbor density function goes smoothly to zero at
the cutoﬀ.

Then we map the atomic neighbor density function from 3-
D euclidean space to another 3-D space, the surface of a four
dimensional hypersphere:

s1 = r0 cos ω
s2 = r0 sin ω cos θ
s3 = r0 sin ω sin θ cos φ
s4 = r0 sin ω sin θ sin φ,

where r0 is a parameter and the polar angles are deﬁned by:

(cid:19)

(cid:19)

(cid:18) z
r
(cid:18) y
x

θ = arccos

φ = arctan
ω = πr
r0

(12)

The Winger-D matrix elements (D j

m(cid:48),m) are the harmonic
functions on the 3-sphere, therefore an arbitrary function de-
ﬁned on the 3-sphere can be expanded in terms of Wigner-D
matrix elements. Here we expand the atomic neighbor density
function on the 3-sphere in terms of Wigner-D matrices.

ρ(r) =

+∞(cid:88)

+ j(cid:88)

j=0

m(cid:48),m=− j

m(cid:48),mD j
c j

m(cid:48),m (ω; θ, φ)

Φ(Ri j) =

i jyly
i jzlz
xlx
Rlx+ly+lz

i j

c

· e−η(Ri j−Rs)2 · fc(Ri j)

(10)

lowing inner product

Where the expansion coeﬃcients c j

m(cid:48),m are given by the fol-

According to quantum mechanics, ρ follows the similar proce-
dure in determining the probability density of the states, i.e. the
Born rule.

EAD can be regarded as an alternative version of ACSF with-
out classiﬁcation between the radial and angular term. The
angular or three-body term is implicitly incorporated in when
Lmax > 0 [38]. By deﬁnition, the computation cost for cal-
culating EAD is cheaper than angular symmetry functions by
avoiding the extra sum of the k neighbors. In term of usage,
the parameters η and Rs are similar to the strategy used in the
Gaussian symmetry functions, and the maximum value for Lmax
is 3, i.e. up to f orbital.

2.1.3. SO(4) Bispectrum (B)

The SO(4) bispectrum components [19, 20, 21] are another
type of atom-centered descriptor based on the harmonic anal-
ysis of the atomic neighbor density function on the 3-sphere.
The atomic neighbor density function is given by [20]:

ρ(r) = δ(r) +

Rc(cid:88)

i

wi fc(ri)δ(r − ri)

(11)

4

c j
m(cid:48),m

= (cid:68)

D j

m(cid:48),m|ρ(r)

(cid:69) = D∗ j

m(cid:48),m(0) +

ri≤Rc(cid:88)

fc(ri)D∗ j

m(cid:48),m(ωi; θi, φi)

(13)
Finally, the SO(4) bispectrum components can then be calcu-
lated using third order products of the expansion coeﬃcients:

i

B j1, j2, j =

j(cid:88)

m(cid:48),m=− j

c∗ j
m(cid:48),m

j1(cid:88)

1,m1=− j1
m(cid:48)

c j1
m(cid:48)

1,m1

×

j2(cid:88)

2,m2=− j2
m(cid:48)

c j2
m(cid:48)

2,m2

mm1m2C j j1 j2
C j j1 j2
m(cid:48)m(cid:48)

1m(cid:48)

2

,

(14)

where C is a Clebsch-Gordan coeﬃcient.

2.1.4. Smooth SO(3) Power Spectrum (P)

The Smooth SO(3) Power Spectrum components were been
proposed to describe the atomic local environment [20]. In con-
trast to the SO(4) bispectrum components, the Smooth SO(3)
power spectrum is based on an alternative atomic neighbor den-
sity while also expanded on the 2-sphere and a radial basis. The

alternative atomic neighbor density is deﬁned in terms of Gaus-
sians as follows:

we will explanation the regression models for the single-species
system.

ρ(cid:48)(r) =

ri≤Rc(cid:88)

i

wie−α|r−ri|2

,

(15)

Then the atomic neighbor density function is then expanded in
terms of spherical harmonics and a radial basis gn(r) as shown
in Eq. 15:

ρ(cid:48)(r) =

+∞(cid:88)

+l(cid:88)

l=0

m=−(cid:48)l

cnlmgn(r)Ylm(ˆr)

Where the expansion coeﬃcients cnlm are given by

cnlm = (cid:10)Ylmgn(r)|ρ(cid:48)(cid:11) =

ri≤Rc(cid:88)

4π

wie−αr2

i Y ∗

lm(ˆri)×

i
(cid:90) Rc

0

r2gn(r)Il(2αrri)e−αr2

dr

where Il is a modiﬁed spherical Bessel function of the ﬁrst kind.
A convenient radial basis for this purpose, gn(r), consisting of
cubic and higher order polynomials, orthonormalized on the in-
terval (0, Rc) has been suggested by Bartok [20].

gn(r) =

(cid:88)

α

Wn,αφα(r)

(16)

where Wn,α are the orthonormalization coeﬃcients given by

the relation to the overlap matrix S by W = S−1/2 and

φα(r) = (Rc − r)α+2/Nα

(cid:115)

Nα =

2r(2α+7)
cut
(2α + 5)(2α + 6)(2α + 7)

And the elements of the overlap matrix S are given by

(cid:90) rcut

0

r2φα(r)φβ(r)dr

S αβ =
(cid:112)

=

(2α + 5)(2α + 6)(2α + 7)(2β + 5)(2β + 6)(2β + 7)
(5 + α + β)(6 + α + β)(7 + α + β)

In any regression model, the objective is to minimize a loss
function which describes the discrepencies between the pre-
diction and true reference values (including energy, force, and
stress tensors) for each atomic conﬁguration in the training data
set.

∆ = 1
2M

M(cid:88)

i=1

(cid:34)(cid:18) Ei − ERef
Ni

i

atom

(cid:19)2

+

β f
3Ni

atom

3Ni
atom(cid:88)

(Fi, j − FRef

i, j )2

j=1

(cid:35)

(19)

+ βs
6

2(cid:88)

p(cid:88)

p=0

q=0

(S pq − S Ref

pq )2

where M is the total number of structures in the training pool,
and Natom
is the total number of atoms in the i-th structure. The
i
superscript Ref corresponds to the target property. β f and βs
are the force and stress coeﬃcients respectively. They scale
the importance between energy, force, and stress contribution
as the force and stress information can overwhelm the energy
information due to their sizes. Additionally, a regularization
term can be added to induce penalty on the entire parameters
preventing overﬁtting:

∆p = α
2M

m(cid:88)

i=1

(wi)2

(20)

where α is a dimensionless number that controls the degree of
regularization.

Clearly, one has to choose diﬀerentiable functional as well
as its derivative due to the existence of force (F) and stress (S )
contribution along with the energy (E) in the loss function. In
the following sections, generalized linear regression and neural
network regression will be introduced.

2.2.1. Generalized Linear Regression

This regression methodology is a type of polynomial regres-
sion. Essentially, the quantum-mechanical energy, forces, and
stress can be expanded via Taylor series with atom-centered de-
scriptors as the independent variables:

(17)

Etotal = γ0 + γ ·

N(cid:88)

i=1

Xi + 1
2

N(cid:88)

i=1

XT
i

· Γ · Xi + · · ·

(21)

and ﬁnally, the Smooth SO(3) power spectrum is given by

pn1n2l =

+l(cid:88)

m=−l

cn1lmc∗

n2lm

(18)

2.2. Regression Models

Here, we discuss the regression model, i.e., the functional
form (E ) presented in Eq. 1. Each regression model is species-
dependent, i.e. as the the number of species increases, the re-
gression parameters will increase. For the sake of simplicity,

where N is the total atoms in a structure. γ0 and γ are the
weights presented in scalar and vector forms. Γ is the sym-
metric weight matrix (i.e. Γ12 = Γ21) describing the quadratic
terms. In this equation, we only restricted the expansion up to
polynomial 2 due to to enormous increase in the weight param-
eters.

In consequence, the force on atom j and the stress matrix can

be derived according to Eqs. (2, 3), respectively:

Fm = −

N(cid:88)

(cid:18)
γ ·

i=1

∂Xi
∂rm

+ 1
2

(cid:20) ∂XT
i
∂rm

· Γ · Xi + XT
i

· Γ ·

(cid:21)(cid:19)

∂Xi
∂rm

(22)

5

S = −

N(cid:88)

m=1

N(cid:88)

(cid:18)
γ·

rm ⊗

i=1

∂Xi
∂rm

+ 1
2

(cid:20) ∂XT
i
∂rm

·Γ· Xi + XT

i ·Γ·

(cid:21)(cid:19)

∂Xi
∂rm

(23)

Note that the energy, force, and stress share the weights
parameters {γ0, γ1, ..., γN, Γ11, Γ12, ..., ΓNN}. Once the energy,
force and stress tensors are known, the derivative of the loss
function can be evaluated. Finding the zero derivative of loss
function (Eq. 19) in linear regression is equivalent to solve a
set of linear equations of Ax = b. In PyXtal FF, we construct
such A matrix and use the numpy.linalg.lstsq solver to obtain
the least-squares solution.

2.2.2. Neural Network Regression

Figure 1: (a) A schematic diagram of high-dimensional neural networks. (b) A
zoom-in version of the color-coded part in (a).

Compared to the linear regression, neural networks provides
more ﬂexible functionals to ﬁt a large data sets. Figure 1 shows
a schematic diagram based on neural networks training. Prior to
the neural networks architecture, the atom-centered descriptors
are mapped based on the atomic environment of a structural
conﬁguration as discussed in the previous section. These de-
scriptors serve as the input to the neural networks architecture
and are arranged in the ﬁrst layer as shown in Figure 1b. The
next layers are the hidden layers. Neural networks can sim-
ply cast more weights parameters as needed through increasing
number of hidden layers and/or hidden layers nodes without the
increasing number of descriptors. The nodes in the hidden lay-
ers carry no physical meaning.

For each of the hidden nodes, activation functions such as
Tanh and Sigmoid functions are frequently used in our NNP
implementation. While ReLU as an activation function is ex-
tremely popular in image processing, we believe ReLU is not
an appropriate choice in constructing MLP, due to the function

6

carries discontinuity at zero. These nodes are connected via the
weights and biases and propagate in forward direction only. In
the end, the output node represents the atomic energy. A math-
ematical form to determine any node value can be written as:

Xl
ni

= al
ni

(cid:18)
bl−1
ni

+

(cid:19)

W l−1,l
n j,ni

· Xl−1
n j

N(cid:88)

n j=1

(24)

n j,ni ), the bias (bl−1
n j ). W l−1,l

The value of a neuron (Xl
ni) at layer l can determined by the
relationships between the weights (W l−1,l
ni ), and
all neurons from the previous layer (Xl−1
n j,ni speciﬁes the
connectivity of neuron n j at layer l − 1 to the neuron ni at layer
l. bl−1
represents the bias of the previous layer that belongs to
ni
the neuron ni. These connectivity are summed based on the total
number of neurons (N) at layer l−1. Finally, an activation func-
tion (al
ni) is applied to the summation to induce non-linearity
to the neuron (Xl
ni).Xni at the output layer is equivalent to an
atomic energy, and it represents an atom-centered descriptor at
the input layer. The collection of atomic energy contributions
are summed to obtain the total energy of the structure. At the
end, the total energy, forces ans stress tensors are compared to
the reference values (see Eq. 19). This process is called forward
propagation.

Similar to the linear regression, one needs to obtain a set of
In NN ar-
weight parameters to minimize the loss function.
chitecture, the gradient of loss with respect to the weight pa-
rameters can be conveniently done by the backpropagation al-
gorithm. Hence, a number of optimization algorithms can be
applied here to update the weights iteratively, until the optimal
solution is found.

3. PyXtal FF Workﬂow

In this section, we discuss about development of PyXtal FF
and its philosophy. PyXtal FF is written in Python. Presently,
the package is equipped with two regression models and four
types atom-centered descriptor, as explained in Section 2.
These regression models and atom-centered descriptors are eas-
ily extendable without changing the core user-interface fea-
tures. Figure 2 represents the workﬂow of PyXtal FF.

First, PyXtal FF utilizes the Atomic Simulation Environment
(ASE) package [46] to parse the DFT data assembled in several
formats, including ASE database, JSON, extended XYZ and
the VASP OUTCAR formats. Further, ASE is also employed
to compile the atomic neighborhood of each atom in the unit
cell based on the periodic boundary conditions within a cutoﬀ
radius. After the neighboring data are gathered, it will compute
the user-deﬁned type of descriptor. The computation follows
the theory described in Section 2.1 utilizing NumPy—a Python
library for scientiﬁc computing [47]. For every structure, the
descriptor calculator will return the descriptors and the force
and stress related derivatives. Eventually, the descriptors rep-
resent as the independent variables in the regression models to
obtain the energy, and the derivative terms are needed to com-
pute the force and stress values.

…RNR1R2X1X2XN…NN2E1ENN1NNNE2EN……𝑤""#"(a)(b)Biasx1x2Eat𝑎%"𝑎&"𝑎""𝑎%%𝑎&%𝑎"%𝑤""%&𝑏"#𝑏"%𝑏&#𝑤"""%vides the supports to utilize the trained models for several types
of atomistic simulations, including geometry optimization, MD
simulation, physical properties prediction, and phonon calcula-
tion. These features are managed by ASE calculator, in which
the MLP potential passes the energy, forces, and stress tensors
to the calculator and ASE performs the relevant atomistic sim-
ulations. Since these simulation will be powered by Python, we
only recommend to use them for light weight simulations. In
the near future, we are going to work on interfacing the trained
MLP with LAMMPS [53] to enable the truly large scale atom-
istic simulation.

4. Example Usage

PyXtal FF can be used as stand-alone library in Python
scripts. A PyXtal FF example code to train Pt model is shown
in the following listing

from pyxtal_ff import PyXtal_FF

# define the path of train / test data
train = ’ train . json ’
test = ’ test . json ’

# define the descriptor
descriptor = { ’ type ’: ’ Bispectrum ’ ,

’ parameters ’: { ’ lmax ’: 3} ,
’ Rc ’: 4.9}

# define the regression model
model = { ’ system ’ : [ ’ Pt ’] ,

’ hiddenlayers ’: [16 , 16] ,
’ epoch ’: 1000 ,
’ path ’: ’Pt - Bispectrum / ’
’ optimizer ’: { ’ method ’: ’ lbfgs ’ }}

# define the pyxtal_FF model and train it
mlp = PyXtal_FF ( descriptor , model )
mlp . run ( TrainData = train , TestData = test )

Listing 2: PyXtal FF script for force ﬁeld training

The atom-centered descriptors and the model are described in
dictionary. The dictionary keys determine the necessary com-
mand for the code and are made as intuitive as possible. Most
of keys follow the hyperparameters in the section 2. By default,
PyXtal FF will use neural networks as the regression algorithm.
Here, PyXtal FF will look for train.json and test.json ﬁles as the
training and test data set, respectively.

After the training is complete, the trained model is saved
in the result folder (Pt-Bispectrum) with a name of 16-16-
checkpoint.pth, in which 16-16 denotes two hidden layers with
16 nodes each. PyXtal FF provides a built-in interface with
the ASE code [46], in which one can use the model to perform
diﬀerent types of calculations through ASE. Below is a sim-
ple example to perform the geometry optimization on a Pt bulk
crystal (Pt bulk.cif ) based on the trained model from the listing
2.

from pyxtal_ff import PyXtal_FF
from pyxtal_ff . calculator import
PyXtalFFCalculator , optimize

from ase . io import read

Figure 2: Schematic diagram of PyXtal FF workﬂow for NNP/GLP training.
In the neural network, data parser includes normalization of the calculated de-
scriptors.

For the regression, the Pyxtal FF supports two models, lin-
ear (quadratic) regression and neural networks. Here we focus
on the latter since it is a more popular workforce for MLP de-
velopment. The neural network regression is powered by Py-
Torch [48]—an open-source deep learning framework based on
automatic diﬀerentiation [49]. Currently, we support three op-
timization algorithms for training: Limited Broyden-Fletcher-
Goldfarb-Shanno (L-BFGS) [50], adaptive Moment Estimation
(Adam) [51], and stochastic gradient descent (SGD) with mo-
mentum [52]. The L-BFGS method with approximated line
search is the recommended optimizer when the training data is
relatively small, as the quasi-Newton method is generally more
stable and ﬁnds local optima more eﬃciently. With larger train-
ing datasets, however, the L-BFGS method is memory demand-
ing, and one can seek to use ﬁrst-order methods such as Adam
or SGD with momentum. Both SGD and Adam algorithms are
usually done in mini-batches, where the the gradients for each
weight update are calculated based on a subset of the entire
training data set. Training in mini batches can reduce the vari-
ance of the parameter updates leading to stable convergence. If
needed, the training can also be done in graphical processing
units (GPU) mode.

In addition to the force ﬁeld generation, PyXtal FF also pro-

7

DFT dataPyXtal_FFCalculate1.wACSF2.EAD3.SO(4) Bispectrum4.SO(3) Power SpectrumStructuresNeural networks/Generalizedlinear regressionEnergy, forces, and stressData parserConverged?NoSave potentialASE calculatorYesAtomistic simulations# load the trained model
mliap = " Pt - Bispectrum /16 -16 - Pt_cluster . pth "
ff = PyXtal_FF ( model ={ ’ system ’: [ " Pt " ]} ,
logo = False )

ff . run ( mode = ’ predict ’ , mliap = mliap )
calc = P y X t a l F F C a l cu l at or ( calc )

# read the structure
pt_bulk = read ( ’ Pt_bulk . cif ’)

# perform the relaxation
pt_bulk . set_calculator ( calc , box = True )
pt_bulk = optimize ( pt_bulk )
print ( ’ energy : ’ , pt_bulk . g e t _ p o t e n t i a l _ e n e r g y () )

Listing 3: PyXtal FF script to perform geometry optimization

In addition to geometry optimization and MD simulation,
PyXtal FF also provides several utility functions to simulate the
elastic and phonon properties, which are based on several exter-
nal Python libraries including Phonopy [54], seekpath [55] and
matscipy [56]. More detailed examples can be found in the on-
line documentation https://pyxtal-ﬀ.readthedocs.io.

5. Applications

In this section, we choose three diﬀerent examples to illus-
trate the power of PyXtal FF and benchmark the performances
of diﬀerent descriptors. While the linear regression scheme is
also supporte by PyXtal FF, we will focus on the NNP model
as it provides more ﬂexibility. The examples to be investi-
gated mainly diﬀer by the source of datasets, including (1) sin-
gle SiO2 from pure MD simulation; (2) collective data set of
NbMoTaW from various approaches; (3) elemental Pt consist-
ing of bulk, surfaces and clusters from diﬀerent runs of MD
simulations.

5.1. Binary System

The SiO2 data set [30] was generated by the DFT method
within the framework of VASP [57], using the general-
ized gradient approximation Perdew-Burke-Ernzerhof (PBE)
exchange-correlation functional [58]. The kinetic energy cut-
oﬀ was set to 500 eV, and the energy convergence criterion is
within 10 meV/atom. The MD trajectories are taken at diﬀerent
temperatures including liquid, amorphous and crystalline (α-
quartz, α-cristobalite, and tridymite) conﬁgurations. The orig-
inal data set contain 3,048 SiO2 conﬁgurations (60 atoms per
structure). For simplicity, we considered a subset that consists
of 1,316 structures. with the goal of to gaining an overview of
performances and computation costs for each descriptor. Below
gives the parameters to deﬁne each descriptor.

# ACSF (70)
para = { ’ G2 ’:

{ ’ eta ’: [0.003214 , 0.035711 ,
0.071421 , 0.124987 ,
0.214264 , 0.357106 ,
0.714213 , 1.428426] ,

’ Rs ’: [0]} ,

’ G4 ’:
{ ’ lambda ’: [ -1 , 1] ,
’ zeta ’ :[1 , 2 , 4] ,
’ eta ’: [0.000357 , 0.028569 , 0.089277]}

}

descriptor = { ’ type ’: ’ ACSF ’ ,

’ Rc ’: 4.9 ,
’ parameters ’: para ,

}

# wACSF (26)
descriptor = { ’ type ’: ’ wACSF ’ ,

’ Rc ’: 4.9 ,
’ parameters ’: para ,

}

# EAD (30)
para = { ’ eta ’: [0.003214 , 0.035711 ,

0.071421 , 0.124987 , 0.214264] ,

’ Rs ’: [0 , 1.50] ,
’ lmax ’: 2 ,

}

descriptor = { ’ type ’: ’ EAD ’ ,

’ Rc ’: 4.9 ,
’ parameters ’: para ,

}

# SO3 (40)
descriptor = { ’ type ’: ’ SO3 ’ ,

’ Rc ’: 4.9 ,
’ parameters ’: { ’ nmax ’: 4 ,

’ lmax ’: 3} ,

}

# SO4 (30)
descriptor = { ’ type ’: ’ SO4 ’ ,

’ Rc ’: 4.9 ,
’ parameters ’: { ’ lmax ’: 3} ,

}

Listing 4: PyXtal FF script to deﬁne the descriptors.

In short, we choose a universal cutoﬀ value of 4.9 Å for all
descriptors. Each descriptors requires some manual selection
of hyperparameters in the real (e.g., η, λ, ζ, Rs) or integer (lmax,
nmax) space. The ACSF parameters were taken from Ref. [30]
which lead to 70 descriptors. In its wACSF version, the number
is reduced to 26. For EAD, we chose a similar set of parameters
for η and Rs, which make 30 descriptors when Lmax = 2. For
SO3 and SO4, only the integer type hypeparameters need to be
provided. In this work, we set 40 SO3 descriptors with nmax =
4 and lmax =3, and 30 SO4 descriptors with lmax =4. The neural
network regression will be used with two hidden layers with 30
nodes each.

Table 1 summarizes the performances of each training af-
ter 12000 steps. First, the ACSF-70 set yields the best accu-
racy in both energy (1.3 meV/atom) and forces (81.2 meV/Å),
while the errors in its corresponding wACSF-26 set rise by 60-
70% in both energy (2.1 meV/atom) and forces (141.8 meV/Å).
On the other hand, the weighted EAD-30 descriptor, supposed
to mimic G2 and G4 ACSFs, gives the highest errors (4.0
meV/atom for energy and 300 meV/Å for forces). This may
be due to lack of optimization on the hyperparameters. How-
ever, it should be noted that the computation of EAD is much
faster than ACSF. Therefore, it is worth exploring a systematic
approach to obtain the optimum set for EAD. For the two spec-
tral descriptors, SO3-40 seems to outperform SO4-30 while it
cost about a similar level of CPU time. In terms of accuracy,

8

SO3-40 (1.4 meV/atom in energy MAE and 115.1 meV/Å in
force MAE) is in the middle of ACSF-70 and wACSF-26. An-
other remarkable advantage of the spectral descriptors is that
tuning the hyperparameters is much easier. If one does not want
to spend too much time on choosing the hyperparameters, SO3
seems to be a better choice than ACSF. We note that all de-
scriptor computations are based on Python. It is expected that
the speed will be much faster when they are implemented in
FORTRAN or C languages.

Table 1: The MAE values of the predicted energy and forces of 1316 SiO2
data set from the 30-30 neural network models with diﬀerent descriptors within
12000 L-BFGS steps of training. For each type of descriptors, the average CPU
time for descriptor computation per structure is also given.
Energy
(meV/atom)
1.3
2.1
4.8
1.4
3.3

CPU time
(secs/60 atoms)
4.374
4.372
0.584
1.028
1.078

Force
(meV/Å)
81.2
141.8
259.0
115.1
204.2

ACSF (70)
wACSF (26)
EAD (30)
SO3 (40)
SO4 (30)

5.2. High Entropy Alloy

High entropy alloys (HEAs) are systems that encompass four
or more equimolar/near-equimolar alloying elements.
It has
been shown that HEAs carry many interesting properties such
as high hardness and corrosion resistance [59, 60]. Due to the
high computational cost of DFT method, HEA serves as a great
example in MLP development with PyXtal FF. Here, we will
use NbMoTaW HEA as an example [39], which are comprised
of elemental, binary, ternary, and quaternary systems. Each of
the elemental systems has their ground state, strain-distorted,
surface, and AIMD conﬁgurations. The binary alloys are com-
posed of solid solution structures with the size of 2 × 2 × 2
supercell. Lastly, 300 K, 1000 K, and 3000 K AIMD conﬁgu-
rations along with special quasi-random structures establish the
ternary and quaternary data points. The total structures used
in developing the MLP are 5529 conﬁgurations for training set
and 376 conﬁgurations for test set.

In the original work [39], SNAP based on the linear re-
gression predicted that the MAE values for energies are 4.3
meV/atom and 5.1 meV/atom for the training and test sets, and
the MAE values in forces are 0.13 eV/Å and 0.14 eV/Å. The
results demonstrated a quite satisfactory accuracy comparable
to the quantum calculation. However, it needs to be noted that
the energy training in Ref. [39] was based on the comparison of
formation energy relative to the elemental solids, which spans
from -0.193 to 0.934 eV/atom for the entire dataset, whereas the
atomic energy spans from -12.960 to -9.502 eV/atom. Training
with the energy in a normalized range can surely reduce the er-
ror of ﬁtting. However, this ﬁtting method does not fully solve
the force ﬁeld prediction problem since it relies on some DFT
reference data. We attempted to employ the linear regression
model to ﬁt only the absolute DFT energy based on the same
descriptor as used in Ref. [39], the resulting MAE values are

Figure 3: The correlation plots between NNP and DFT for (a) energy and (b)
forces in the HEA system. The energy MAE values are 6.69 meV/atom and
7.57 meV/atom for training and test sets, while the force MAE values are 0.14
eV/Å and 0.17 eV/Å.

944 meV/atom and 6.329 eV/Å for energy and force when the
force coeﬃcient is 10−4. The MAE values of formation en-
ergy ﬁtting yield remarkable improvement of 22 meV/atom and
0.243 eV/Å for energy and force, respectively. Despite this im-
provement, the accuracy is insuﬃcient. Perhaps, it is due to
lack of ﬁne tuning of hyperparameters, such as atomic weights
and cutoﬀ radii for each species.

To obtain a better accuracy, we decided to ﬁt the absolute
DFT energy and forces based on the NNP model. We employed
the smooth SO(3) power spectrum as the descriptor, which are
formed by nmax=4 and lmax=3 with 40 components in total up to
the cutoﬀ radius of 5.0 Å. The NNP training is executed with 2
hidden layers with 20 nodes for each layer while energy, force,
and stress contributions are trained simultaneously. The impor-
tance coeﬃcients of force and stress are set to 10−3 and 10−4,
respectively. The results of the NNP training is illustrated in
Figure 3. The NNP energy MAE values for the training and
test sets are 6.69 meV/atom and 7.57 meV/atom, and the NNP
force MAE values are 0.14 eV/Å and 0.17 eV/Å. In addition,
the MAE value of stress for training set is 0.078 GPa. Our re-
sults of energy and force yield worse performance compared to
the previous report. Nevertheless, our NNP model oﬀers a more
general representation of the DFT PES since it does not rely on

9

any prior reference values.

Furthermore, we calculated physical properties such as elas-
tic constants, bulk and shear moduli, and the Poisson’s ratio of
the cubic elemental crystals (see Table 2). From the table, the
overall performances of NNP in predicting the physical proper-
ties are reasonable, except that the C44 value of Nb is negative.
However, this is consistent with the fact that the DFT’s C44 is
also signiﬁcantly lower than other terms. Hence, the negative
C44 is acceptable if one considers the noise of stress data in
training. Meanwhile, the C44 value may be remedied by provid-
ing additional training data set focusing on the shearing eﬀect
of Nb, increasing the importance of the stress coeﬃcient, or in-
creasing the hidden layer size in the NNP training. In addition,
SO(3) descriptor can be conveniently expanded in terms of both
radial basis (nmax) and angular momentum (lmax) for achieving
better overall accuracy.

Table 2: Comparison of physical properties predicted with SO(3)-NNP. The
DFT and experiment values are obtained from Ref [39]. B and G denote the
empirical Voigt-Reuss-Hill average bulk and shear moduli. ν is the Poisson’s
ratio. The DFT results were taken from open database of Materials Project [61].

C11
(GPa)

C12
(GPa)

C44
(GPa)

B
(GPa)

G
(GPa)

ν

Mo
DFT
Ref[39]
NNP
Nb
DFT
Ref[39]
NNP
Ta
DFT
Ref[39]
NNP
W
DFT
Ref[39]
NNP

472
435
453

233
266
255

265
257
280

510
560
527

158
169
161

145
142
130

158
161
165

201
218
196

106
96
107

11
20
-3

69
67
72

143
154
143

262
258
259

174
183
171

194
193
203

304
332
306

127
110
121

24
32
N/A

63
59
66

147
160
151

0.30
0.31
0.30

0.45
0.42
0.47

0.35
0.36
0.35

0.29
0.29
0.29

5.3. Pt MLP for General Purposes

Compared to crystalline systems, surfaces and nanoparticles
generally represent the more challenging cases in MLP training
as the nanoparticle contain more versatile atomic environments
and more complex PES is expected. Here, we applied the NNP
model to a Pt data set [37], which consists of three data types:
Pt surface, Pt bulk, and Pt cluster. There are 927 clusters of
15 atoms, and the Pt bulk type consists of 1717 conﬁgurations
which are composed of 256 atoms. Pt surface are constructed
from (001), (110), and (111) surfaces. Respectively, there are
949, 819, and 700 structures which consist of 320, 160, and
320 atoms. In our NNP development, we chose 90% of the to-
tal structures randomly as the training set, and the remaining
10% as the test set. The SO(3) power pectrum descriptor with
lmax = 3 and nmax = 4 at radius cutoﬀ of 4.9 Å was used to

construct the MLP in the NNP model with two hidden layers
with 30 nodes each. Unlike the previous examples, the mini-
batch scheme with the Adam optimizer was employed. In each
iteration, the training process was updated in a batch size of 25
conﬁgurations.

Table 3: The trained RMSE values of the predicted energy and forces of Pt
data set from the 30-40-40-1 NNP model. For reference, the results from the
DeepPot-SE model [37] is also reported. It should be noted that that DeepPot-
SE results were based on training the entire MoS2/Pt dataset.
SO(3)-NNP

Energy
(meV)
1.64
5.91
7.63

Force
(meV/Å)
64
87
247

Bulk
Surface
Cluster

DeepPot-SE [37]
Energy
Force
(meV/Å)
(meV)
84
2.00
105
6.77
201
30.6

Figure 4: The correlation plots between NNP and DFT for (a) energy and (b)
forces in the Pt system.

In the original literature [37], DeepPot-SE includes MoS2
slab and Pt clusters on MoS2 substrate (MoS2/Pt). The perfor-
mance of DeepPot-SE yields satisfactory results. Meanwhile,
embedded atom neural networks method can achieves outstand-
ing results using a fraction of the same data set [38]. Both of the
methods exploit a large number of neural networks parameters,
in the order of 104–105, while the current study only adopts

10

2191 weight parameters for exemplary purpose. As shown in
Table 3, the accuracy from our small neural network model is
comparable to that of DeepPot-SE results. Not surprisingly, the
group of Pt bulk has the lowest errors with only 1.64 meV/atom
for the RMSE in energy and 67 meV/Å in force. On the con-
trary, the errors on Pt clusters are about 2-4 time higher for both
energy and forces. This is expected since the local atomic en-
vironments in the clusters are more diverse and thus learning
the relation is harder. Nevertheless, the values from this ex-
ploratory study is comparable to the results from deep learning
models. This example also suggests that a small NNP model
with the properly constructed features can be a complementary
solution for MLP development.

6. Conclusion

In conclusion, we introduced PyXtal FF a versatile package
for developing MLPs that can perform at the DFT level. Cur-
rently, the code allows one to construct the MLPs from four
diﬀerent types of atom-centered descriptors: (w)ACSFs, EAD,
SO4 bispectrum, or SO3 power spectrum. Two regression mod-
els, the generalized linear regression and neurual networks, are
supported to train the MLP by simultaneously ﬁtting the data of
energy, forces and stresses from the ab-initio simulation. In par-
ticular, we focus on the neural networks potential development.
Our software package utilizes PyTorch as the main machinery,
which is equipped with neural network models, automatic dif-
ferentiation, as well as various optimization algorithms. We
demonstrated the features of the current PyXtal FF version by
three examples on SiO2, NbMoTaW HEA, and elemental Pt,
respectively. In general, the mean absolute error values of each
trained MLPs fall into the range of several meVs/atom in en-
ergy and several hundred meV/Å in forces. While training on
stress is optional, it is helpful to improve the general accuracy
of the model. More importantly, this is crucial to yield better
prediction on materials’ elastic properties. As such, the MLPs
can be applied to investigate the materials properties in greater
accuracy than the classical potentials built from the empirical
model. Finally, the PyXtal FF is an open source code. We wel-
come anyone who is interested in MLP development to con-
tribute to this project.

Acknowledgments

We acknowledge the NSF (I-DIRSE-IL: 1940272) and
supports.
for
NASA (80NSSC19M0152)
The computing resources are provided by XSEDE (TG-
DMR180040).

their ﬁnancial

Appendix A. The Derivatives of ACSF

Following the Eq. 6 the derivative with respect to an atom m

can be written in the following form:

∂G(2)
i
∂rm

(cid:88)

=

j(cid:44)i

e−η(Ri j−Rs)2 (cid:18) ∂ fc
∂Ri j

− 2η(Ri j − Rs) fc

(cid:19) ∂Ri j
∂rm

(A.1)

For the periodic system, the computation of ∂Ri j
∂rm

is straight-
forward except that one needs to consider one additional case.
When i = j, the derivative is always zero.

∂Ri j
∂rm

=

m (cid:60) [i, j]
m = i = j


0
0
− ri j
Ri j m = i (when i (cid:44) j)
ri j
m = j (when i (cid:44) j)
Ri j




In Eqs. (7, 8), the cosine function can be deﬁned as:

cos θi jk =

ri j · rik
Ri jRik

(A.2)

(A.3)

where ri j is the relative position between atom j and atom i.

In the following, the expressions for the derivative with re-

spect to an interacting atom m are:

(cid:88)

= 21−ζ (cid:88)

∂G(4)
i
∂rm
(cid:34)
λζ(1 + λ cos θi jk)ζ−1 ∂ cos θi jk

e−η(R2

k(cid:44)i, j

j(cid:44)i

+R2
ik

i j

+R2

jk)

− 2η(1 + λ cos θi jk)ζ

(cid:18)
Ri j

∂rm
∂Ri j
∂rm
(cid:18) ∂ fc(Ri j)
∂Ri j

∂Ri j
∂rm

+ (1 + λ cos θi jk)ζ

fc(Rik) fc(R jk)

+ fc(Ri j)

∂ fc(Rik)
∂Rik

∂Rik
∂rm

fc(R jk) + fc(Ri j) fc(Rik)

fc(Ri j) fc(Rik) fc(R jk)

+ Rik

∂Rik
∂rm

+ R jk

(cid:19)

∂R jk
∂rm

fc(Ri j) fc(Rik) fc(R jk)

(cid:19)(cid:35)

∂ fc(R jk)
∂R jk

∂R jk
∂rm

(A.4)

(cid:88)

= 21−ζ (cid:88)

∂G(5)
i
∂rm
(cid:34)
λζ(1 + λ cos θi jk)ζ−1 ∂ cos θi jk

e−η(R2

k(cid:44)i, j

j(cid:44)i

i j

+R2
ik)

− 2η(1 + λ cos θi jk)ζ

(cid:18)
Ri j

∂rm
∂Ri j
∂rm
(cid:18) ∂ fc(Ri j)
∂Ri j

fc(Ri j) fc(Rik)

+ Rik

∂Rik
∂rm

+ R jk

(cid:19)

∂R jk
∂rm

fc(Ri j) fc(Rik)
(cid:19)(cid:35)

∂Ri j
∂rm

fc(Rik) + fc(Ri j)

+ (1 + λ cos θi jk)ζ

∂Rik
∂rm
(A.5)
The derivatives of atomic distances, Ri j and Rik, carry the
same meaning as in Eq. A.2. The expression of the cosine of
triple-atom angle is

∂ fc(Rik)
∂Rik

∂ cos θi jk
∂rm

= rik
Ri jRik

·

∂ri j
∂rm

+

ri j
Ri jRik

·

∂rik
∂rm

−

ri j · rik
R2
i jRik

∂Ri j
∂rm

−

∂ri j
∂rm

=





δm j − δmi
0
0

0
δm j − δmi
0

0
0
δm j − δmi





ri j · rik
∂Rik
Ri jR2
∂rm
ik
(A.6)

(A.7)

where δm j is the Kronecker delta between atom m and j.

11

Appendix B. The Derivatives of EAD

The expression of the derivative with respect to an interacting

atom m is shown in the following:

∂ρi
∂rm

=

lx+ly+lz=L(cid:88)

lx,ly,lz=0

2Lmax!
lx!ly!lz!

(cid:20) N(cid:88)

(cid:21)(cid:20) N(cid:88)

Z jΦ

j(cid:44)i

j(cid:44)i

Z j

(cid:21)

∂Φ
∂rm

(B.1)

where the derivative of Φ with respect to an interacting atom m
is

∂Φ
∂rm

= e−η(Ri j−Rs)2
Rlx+ly+lz

c

(cid:20)(cid:18) ∂xlx
i j
∂rm

yly
i jzlz

i j

+ xlx
i j

∂yly
i j
∂rm

zlz
i j

+ xlx

i jyly

i j

+xlx

i jyly

i jzlz

i j

(cid:18) ∂ fc
∂Ri j

− 2 fcη(Ri j − Rs)

(cid:19)

∂zlz
i j
∂rm
(cid:19) ∂Ri j
∂rm

fc

(cid:19)(cid:21)

(B.2)

References

[1] V. Yamakov, D. Wolf, S. R. Phillpot, A. K. Mukherjee, H. Gleiter, Dis-
location processes in the deformation of nanocrystalline aluminium by
molecular-dynamics simulation, Nat. Mater. 1 (1) (2002) 45–49. doi:
https://doi.org/10.1038/nmat700.

[2] M. Terrones, F. Banhart, N. Grobert, J.-C. Charlier, H. Terrones,
P. Ajayan, Molecular junctions by joining single-walled carbon nan-
otubes, Phys. Rev. Lett. 89 (7) (2002) 075505. doi:https://doi.org/
10.1103/PhysRevLett.89.075505.

[3] X. Li, Y. Wei, L. Lu, K. Lu, H. Gao, Dislocation nucleation governed soft-
ening and maximum strength in nano-twinned metals, Nature 464 (7290)
(2010) 877–880. doi:https://doi.org/10.1038/nature08929.
[4] G. Kresse, J. Hafner, Ab initio molecular dynamics for liquid metals,
Phys. Rev. B 47 (1) (1993) 558. doi:https://doi.org/10.1103/
PhysRevB.47.558.

[5] W. Kohn, L. J. Sham, Self-consistent equations including exchange and
correlation eﬀects, Phys. Rev. 140 (4A) (1965) A1133. doi:https://
doi.org/10.1103/PhysRev.140.A1133.

[6] M. S. Daw, M. I. Baskes, Embedded-atom method: Derivation and ap-
plication to impurities, surfaces, and other defects in metals, Phys. Rev.
B 29 (12) (1984) 6443. doi:https://doi.org/10.1103/PhysRevB.
29.6443.

[7] M. S. Daw, S. M. Foiles, M. I. Baskes, The embedded-atom method:
a review of theory and applications, Materials Science Reports 9 (7-8)
(1993) 251–310. doi:https://doi.org/10.1016/0920-2307(93)
90001-U.

[8] J. Tersoﬀ, New empirical model for the structural properties of silicon,
Phys. Rev. Lett. 56 (6) (1986) 632. doi:https://doi.org/10.1103/
PhysRevLett.56.632.

[9] F. H. Stillinger, T. A. Weber, Computer simulation of local order in con-
densed phases of silicon, Phys. Rev. B 31 (8) (1985) 5262. doi:https:
//doi.org/10.1103/PhysRevB.31.5262.

[10] A. D. MacKerell Jr, D. Bashford, M. Bellott, R. L. Dunbrack Jr, J. D.
Evanseck, M. J. Field, S. Fischer, J. Gao, H. Guo, S. Ha, et al., All-
atom empirical potential for molecular modeling and dynamics studies
of proteins, J. Phys. Chem. B 102 (18) (1998) 3586–3616. doi:https:
//doi.org/10.1021/jp973084f.

[11] J. Behler, Constructing high-dimensional neural network potentials: A
tutorial review, International Journal of Quantum Chemistry 115 (16)
(2015) 1032–1050. doi:https://doi.org/10.1002/qua.24890.

[12] N. Artrith, J. Behler, High-dimensional neural network potentials for
metal surfaces: A prototype study for copper, Phys. Rev. B 85 (4) (2012)
045439.

[13] W. Li, Y. Ando, E. Minamitani, S. Watanabe, Study of Li atom dif-
fusion in amorphous Li3PO4 with neural network potential, J. Chem.
Phys. 147 (21) (2017) 214106. doi:https://doi.org/10.1063/1.
4997242.

12

[14] J. Behler, M. Parrinello, Generalized neural-network representation of
high-dimensional potential-energy surfaces, Phys. Rev. Lett. 98 (14)
(2007) 146401. doi:https://doi.org/10.1103/PhysRevLett.98.
146401.

[15] J. Behler, Atom-centered symmetry functions for constructing high-
dimensional neural network potentials, J. Chem. Phys. 134 (7) (2011)
074106. doi:https://doi.org/10.1063/1.3553717.

[16] N. Artrith, T. Morawietz, J. Behler, High-dimensional neural-network po-
tentials for multicomponent systems: Applications to zinc oxide, Phys.
Rev. B 83 (15) (2011) 153101. doi:https://doi.org/10.1103/
PhysRevB.83.153101.

[17] S. Hajinazar, J. Shao, A. N. Kolmogorov, Stratiﬁed construction of neu-
ral network based interatomic models for multicomponent materials,
Phys. Rev. B 95 (1) (2017) 014114. doi:https://doi.org/10.1103/
PhysRevB.95.014114.

[18] M. Gastegger, P. Marquetand, High-dimensional neural network poten-
tials for organic reactions and an improved training algorithm, Jour-
nal of Chemical Theory and Computation 11 (5) (2015) 2187–2198.
doi:https://doi.org/10.1021/acs.jctc.5b00211.

[19] A. P. Bart´ok, M. C. Payne, R. Kondor, G. Cs´anyi, Gaussian approximation
potentials: The accuracy of quantum mechanics, without the electrons,
Phys. Rev. Lett. 104 (13) (2010) 136403. doi:https://doi.org/10.
1103/PhysRevLett.104.136403.

[20] A. P. Bart´ok, R. Kondor, G. Cs´anyi, On representing chemical environ-
ments, Phys. Rev. B 87 (18) (2013) 184115. doi:https://doi.org/
10.1103/PhysRevB.87.184115.

[21] A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles, G. J. Tucker,
Spectral neighbor analysis method for automated generation of quantum-
accurate interatomic potentials, J. Comput. Phys. 285 (2015) 316–330.
doi:https://doi.org/10.1016/j.jcp.2014.12.018.

[22] A. V. Shapeev, Moment tensor potentials: A class of systematically im-
provable interatomic potentials, Multiscale Modeling & Simulation 14 (3)
(2016) 1153–1173. doi:https://doi.org/10.1137/15M1054183.

[23] C. Chen, Z. Deng, R. Tran, H. Tang, I.-H. Chu, S. P. Ong, Accurate force
ﬁeld for molybdenum by machine learning large materials data, Phys.
Rev. Mater. 1 (4) (2017) 043603. doi:https://doi.org/10.1103/
PhysRevMaterials.1.043603.

[24] X.-G. Li, C. Hu, C. Chen, Z. Deng, J. Luo, S. P. Ong, Quantum-accurate
spectral neighbor analysis potential models for ni-mo binary alloys and
fcc metals, Phys. Rev. B 98 (9) (2018) 094104. doi:https://doi.
org/10.1103/PhysRevB.98.094104.

[25] W. J. Szlachta, A. P. Bart´ok, G. Cs´anyi, Accuracy and transferability
of gaussian approximation potential models for tungsten, Phys. Rev. B
90 (10) (2014) 104108. doi:https://doi.org/10.1103/PhysRevB.
90.104108.

[26] V. L. Deringer, D. M. Proserpio, G. Cs´anyi, C. J. Pickard, Data-
driven learning and prediction of inorganic crystal structures, Faraday
discussions 211 (2018) 45–59.
doi:https://doi.org/10.1039/
C8FD00034D.

[27] V. L. Deringer, C. J. Pickard, G. Cs´anyi, Data-driven learning of to-
tal and local energies in elemental boron, Phys. Rev. Lett. 120 (15)
(2018) 156001.
doi:https://doi.org/10.1103/PhysRevLett.
120.156001.

[28] E. V. Podryabinkin, E. V. Tikhonov, A. V. Shapeev, A. R. Oganov, Ac-
celerating crystal structure prediction by machine-learning interatomic
potentials with active learning, Phys. Rev. B 99 (6) (2019) 064114.
doi:10.1103/PhysRevB.99.064114.

[29] A. Singraber, T. Morawietz, J. Behler, C. Dellago, Parallel multistream
training of high-dimensional neural network potentials, Journal of chem-
ical theory and computation 15 (5) (2019) 3075–3092. doi:https:
//doi.org/10.1021/acs.jctc.8b01092.

[30] K. Lee, D. Yoo, W. Jeong, S. Han, Simple-nn: An eﬃcient package for
training and executing neural-network interatomic potentials, Computer
Physics Communications 242 (2019) 95–103. doi:https://doi.org/
10.1016/j.cpc.2019.04.014.

[31] A. Khorshidi, A. A. Peterson, Amp: A modular approach to ma-
chine learning in atomistic simulations, Computer Physics Communica-
tions 207 (2016) 310–324. doi:https://doi.org/10.1016/j.cpc.
2016.05.010.

[32] Y. Shao, M. Hellstr¨om, P. D. Mitev, L. Knijﬀ, C. Zhang, Pinn: A python
library for building atomic neural networks of molecules and materials,

[51] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv
preprint arXiv:1412.6980doi:https://arxiv.org/abs/1412.6980.
[52] N. Qian, On the momentum term in gradient descent learning algorithms,
Neural networks 12 (1) (1999) 145–151. doi:https://doi.org/10.
1016/S0893-6080(98)00116-6.

[53] S. Plimpton, Fast parallel algorithms for short-range molecular dynamics,
J. Comp. Phys. 117 (1) (1995) 1 – 19. doi:https://doi.org/10.
1006/jcph.1995.1039.

[54] A. Togo, I. Tanaka, First principles phonon calculations in materials sci-

ence, Scr. Mater. 108 (2015) 1–5.

[55] Y. Hinuma, G. Pizzi, Y. Kumagai, F. Oba, I. Tanaka, Band structure di-
agram paths based on crystallography, Comput. Mater. Sci. 128 (2017)
140 – 184. doi:https://doi.org/10.1016/j.commatsci.2016.
10.015.

[56] Matscipy, https://gitlab.com/libAtoms/matscipy.
[57] G. Kresse, J. Furthm¨uller, Eﬃcient iterative schemes for ab initio total-
energy calculations using a plane-wave basis set, Phys. Rev. B 54 (1996)
11169–11186. doi:10.1103/PhysRevB.54.11169.

[58] J. P. Perdew, K. Burke, M. Ernzerhof, Generalized gradient approxi-
mation made simple, Phys. Rev. Lett. 77 (1996) 3865–3868.
doi:
10.1103/PhysRevLett.77.3865.

[59] J.-W. Yeh, S.-K. Chen, S.-J. Lin, J.-Y. Gan, T.-S. Chin, T.-T. Shun, C.-
H. Tsau, S.-Y. Chang, Nanostructured high-entropy alloys with multiple
principal elements: novel alloy design concepts and outcomes, Advanced
Engineering Materials 6 (5) (2004) 299–303. doi:https://doi.org/
10.1002/adem.200300567.

[60] O. N. Senkov, G. Wilks, J. Scott, D. B. Miracle, Mechanical properties of
Nb25Mo25Ta25W25 and V20Nb20Mo20Ta20W20 refractory high entropy
alloys, Intermetallics 19 (5) (2011) 698–706. doi:https://doi.org/
10.1016/j.intermet.2011.01.004.

[61] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cho-
lia, D. Gunter, D. Skinner, G. Ceder, et al., Commentary: The materials
project: A materials genome approach to accelerating materials innova-
tion, Apl Materials 1 (1) (2013) 011002. doi:10.1063/1.4812323.

Journal of Chemical Information and Modeling 60 (3) (2020) 1184–1193,
pMID: 31935100. arXiv:https://doi.org/10.1021/acs.jcim.
9b00994, doi:10.1021/acs.jcim.9b00994.
URL https://doi.org/10.1021/acs.jcim.9b00994

[33] K. T. Sch¨utt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, K.-R.
M¨uller, Schnet–a deep learning architecture for molecules and materials,
J. Chem. Phys. 148 (24) (2018) 241722. doi:https://doi.org/10.
1063/1.5019779.

[34] N. Artrith, A. Urban, An implementation of artiﬁcial neural-network po-
tentials for atomistic materials simulations: Performance for tio2, Com-
putational Materials Science 114 (2016) 135–150. doi:https://doi.
org/10.1016/j.commatsci.2015.11.047.

[35] H. Yanxon, D. Zagaceta, B. C. Wood, Q. Zhu, Neural networks poten-
tial from the bispectrum component: A case study on crystalline silicon,
arXiv preprint arXiv:2001.00972.

[36] D. Zagaceta, H. Yanxon, Q. Zhu, Spectral neural network potentials for

binary alloys, arXiv: Computational Physics.

[37] L. Zhang, J. Han, H. Wang, W. Saidi, R. Car, E. Weinan, End-to-end
symmetry preserving inter-atomic potential energy model for ﬁnite and
extended systems, in: Advances in Neural Information Processing Sys-
tems, 2018, pp. 4436–4446.

[38] Y. Zhang, C. Hu, B. Jiang, Embedded atom neural network potentials:
Eﬃcient and accurate machine learning with a physically inspired repre-
sentation, J. Phys. Chem. Lett. 10 (17) (2019) 4962–4967. doi:https:
//doi.org/10.1021/acs.jpclett.9b02037.

[39] X.-G. Li, C. Chen, H. Zheng, S. P. Ong, Unravelling complex strengthen-
ing mechanisms in the NbMoTaW multi-principal element alloy with ma-
chine learning potentials, arXiv preprint arXiv:1912.01789doi:https:
//arxiv.org/abs/1912.01789.

[40] Dscribe: Library of descriptors for machine learning in materials science,
Computer Physics Communications 247 (2020) 106949. doi:https:
//doi.org/10.1016/j.cpc.2019.106949.

[41] M. Rupp, A. Tkatchenko, K.-R. M¨uller, O. A. Von Lilienfeld, Fast
and accurate modeling of molecular atomization energies with machine
learning, Physical review letters 108 (5) (2012) 058301. doi:https:
//doi.org/10.1103/PhysRevLett.108.058301.

[42] F. Faber, A. Lindmaa, O. A. von Lilienfeld, R. Armiento, Crystal struc-
ture representations for machine learning models of formation energies,
International Journal of Quantum Chemistry 115 (16) (2015) 1094–1101.
doi:https://doi.org/10.1002/qua.24917.

[43] H. Huo, M. Rupp, Uniﬁed representation of molecules and crystals for

machine learning, arXiv preprint arXiv:1704.06439.

[44] M. Gastegger, L. Schwiedrzik, M. Bittermann, F. Berzsenyi, P. Marque-
tand, wacsf—weighted atom-centered symmetry functions as descriptors
in machine learning potentials, J. Chem. Phys. 148 (24) (2018) 241709.
doi:https://doi.org/10.1063/1.5019667.

[45] M. S. Daw, M. I. Baskes, Semiempirical, quantum mechanical calcula-
tion of hydrogen embrittlement in metals, Phys. Rev. Lett. 50 (17) (1983)
1285. doi:https://doi.org/10.1103/PhysRevLett.50.1285.
[46] A. H. Larsen, J. J. Mortensen, J. Blomqvist, I. E. Castelli, R. Chris-
tensen, M. Dułak, J. Friis, M. N. Groves, B. Hammer, C. Hargus, et al.,
The atomic simulation environment—a python library for working with
atoms, Journal of Physics: Condensed Matter 29 (27) (2017) 273002.
doi:https://doi.org/10.1088/1361-648X/aa680e.

[47] S. v. d. Walt, S. C. Colbert, G. Varoquaux, The numpy array: a struc-
ture for eﬃcient numerical computation, Computing in Science & Engi-
neering 13 (2) (2011) 22–30. doi:https://doi.org/10.1109/MCSE.
2011.37.

[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high-
performance deep learning library,
in: H. Wallach, H. Larochelle,
A. Beygelzimer, F. dAlche-Buc, E. Fox, R. Garnett (Eds.), Advances
in Neural Information Processing Systems 32, Curran Associates, Inc.,
2019, pp. 8024–8035.

[49] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, A. Lerer, Automatic diﬀerentiation in pytorch.
[50] D. C. Liu, J. Nocedal, On the limited memory BFGS method for large
scale optimization, Mathematical programming 45 (1-3) (1989) 503–528.
doi:https://doi.org/10.1007/BF01589116.

13

