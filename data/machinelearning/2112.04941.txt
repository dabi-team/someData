1
2
0
2
c
e
D
9

]
S
D
.
s
c
[

1
v
1
4
9
4
0
.
2
1
1
2
:
v
i
X
r
a

Testing Probabilistic Circuits ∗†

Yash Pote

r(cid:13) Kuldeep S. Meel

School of Computing, National University of Singapore

Abstract

Probabilistic circuits (PCs) are a powerful modeling framework for representing
tractable probability distributions over combinatorial spaces. In machine learning
and probabilistic programming, one is often interested in understanding whether
the distributions learned using PCs are close to the desired distribution. Thus,
given two probabilistic circuits, a fundamental problem of interest is to determine
whether their distributions are close to each other.
The primary contribution of this paper is a closeness test for PCs with respect
to the total variation distance metric. Our algorithm utilizes two common PC
queries, counting and sampling. In particular, we provide a poly-time probabilistic
algorithm to check the closeness of two PCs, when the PCs support tractable ap-
proximate counting and sampling. We demonstrate the practical efﬁciency of our
algorithmic framework via a detailed experimental evaluation of a prototype im-
plementation against a set of 475 PC benchmarks. We ﬁnd that our test correctly
decides the closeness of all 475 PCs within 3600 seconds.

1 Introduction

Probabilistic modeling is at the heart of modern computer science, with applications ranging from
image recognition and image generation [29, 30] to weather forecasting [3]. Probabilistic models
have a multitude of representations, such as probabilistic circuits (PCs) [9], graphical models [19],
generative networks [16], and determinantal point processes [20]. Of particular interest to us are PCs,
which are known to support guaranteed inference and thus have applications in safety-critical ﬁelds
such as healthcare [2, 25]. In this work, we will focus on PCs that are fragments of the Negation
Normal Form (NNF), speciﬁcally DNNFs, d-DNNFs, SDNNFs, and PIs [13]. We refer to the survey
by Choi et al. [9] for more details regarding PCs.

Given two distributions P and Q, a fundamental problem is to determine whether they are close.
Closeness between distributions is frequently quantiﬁed using the total variation (TV) distance,
is the ℓ1 norm [21, 6]. Thus, stated formally, closeness test-
dT V (P, Q) = (1/2)
Q
−
k
ing is the problem of deciding whether dT V (P, Q)
1. De-
termining the closeness of models has applications in AI planning [13], bioinformatics [31, 33, 35]
and probabilistic program veriﬁcation [15, 23].

ε or dT V (P, Q)

k1, where

η for 0

ε < η

k·k

≤

≥

≤

≤

P

Equivalence testing is a special case of closeness testing, where one tests if dT V (P, Q) = 0. Dar-
wiche and Huang [13] initiated the study of equivalence testing of PCs by designing an equivalence
test for d-DNNFs. An equivalence test is, however, of little use in contexts where the PCs under test

∗The accompanying tool, available open source, can be found at https://github.com/meelgroup/teq. The

Appendix is available in the accompanying supplementary material.

†The authors decided to forgo the old convention of alphabetical ordering of authors in favor of a
randomized ordering, denoted by r(cid:13). The publicly veriﬁable record of the randomization is available
at https://www.aeaweb.org/journals/policies/random-author-order/search with conﬁrmation
code: icoxrj0sNB2L. For citation of the work, authors request that the citation guidelines by AEA for random
author ordering be followed.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
encode non-identical distributions that are nonetheless close enough for practical purposes. Such
situations may arise due to the use of approximate PC compilation [10] and sampling-based learn-
ing of PCs [26, 27]. As a concrete example, consider PCs that are learned via approximate methods
such as stochastic gradient descent [27]. In such a case, PCs are likely to converge to close but non-
identical distributions. Given two such PCs, we would like to know whether they have converged to
distributions close to each other. Thus, we raise the question: Does there exist an efﬁcient algorithm
to test the closeness of two PC distributions?

In this work, we design the ﬁrst closeness test for PCs with respect to TV distance, called Teq.
Assuming the tested PCs allow poly-time approximate weighted model counting and sampling, Teq
runs in polynomial time. Formally, given two PC distributions P and Q, and three parameters (ε,η,δ),
ε and Reject if
for closeness(ε), farness(η), and tolerance(δ), Teq returns Accept if dT V (P, Q)
ε)−2 log(δ−1)) calls to
dT V (P, Q)
−
the sampler and exactly 2 calls to the counter.

η with probability at least 1

δ. Teq makes atmost O((η

≥

−

≤

Teq builds on a general distance estimation technique of Canonne and Rubinfeld [4] that estimates
the distance between two distributions with a small number of samples. In the context of PCs, the
algorithm requires access to an exact sampler and an exact counter. Since not all PCs support exact
sampling and counting, we modify the technique presented in [4] to allow for approximate samples
and counts. Furthermore, we implement and test Teq on a dataset of publicly available PCs arising
from applications in circuit testing. Our results show that closeness testing can be accurate and
scalable in practice.

For some NNF fragments, such as DNNF, no sampling algorithm is known, and for fragments such
as PI, sampling is known to be NP-hard [32]. Since Teq requires access to approximate weighted
counters and samplers to achieve tractability, the question of determining the closeness of the PCs
mentioned above remains unanswered. Thus, we investigate further and characterize the complex-
ity of closeness testing for a broad range of PCs. Our characterization reveals that PCs from the
fragments d-DNNFs and SDNNFs can be tested for closeness in poly-time via Teq, owing to the
algorithms of Darwiche [11] and Arenas et al. [1]. We show that the SDNNF approximate counting
algorithm of Arenas et al. [1] can be extended to log-linear SDNNFs using chain formulas [8]. Then,
using previously known results, we also ﬁnd that there are no poly-time equivalence tests for PCs
from PI and DNNF, conditional on widely believed complexity-theoretic conjectures. Our charac-
terization also reveals some open questions regarding the complexity of closeness and equivalence
testing of PCs.

The rest of the paper is organized in the following way. We deﬁne the notation and discuss related
work in Section 2. We then present the main contribution of the paper, the closeness test Teq, and
the associated proof of correctness in Section 3. We present our experimental ﬁndings in Section 4,
and then discuss the complexity landscape of closeness testing in Section 5. We conclude the paper
and discuss some open problems in Section 6. Due to space constraints, we defer some proofs to the
supplementary Section A.

2 Background

|

n

}

0, 1

0, 1

→ {

be a circuit over n Boolean variables. An assignment σ

}
> 0, then ϕ is said to be satisﬁable and if

n to the
Let ϕ :
{
variables of ϕ is a satisfying assignment if ϕ(σ) = 1. The set of all satisfying assignments of ϕ is
= 2n, then ϕ is said to be valid. We
Rϕ. If
Rϕ|
|
to denote the size of circuit ϕ, where the size is the total number of vertices and edges in the
use
ϕ
|
circuit DAG.
The polynomial hierarchy (PH) contains the classes ΣP
1 (co-NP) along with general-
i+1 = co-NPΠP
i and ΠP
izations of the form ΣP
i [34]. The classes ΣP
i
and ΠP
i are said to be at level i. If it is shown that two classes on the same or consecutive levels are
equal, the hierarchy collapses to that level. Such a collapse is considered unlikely, and hence is used
as the basic assumption for showing hardness results, including the ones we present in the paper.

1 (NP) and ΠP
i and ΣP

i+1 = NPΣP

i where ΠP

Rϕ|

∈ {

0, 1

}

|

2

2.1 Probability distributions

A weight function w :
→
extend the deﬁnition of w to also allow circuits as input: w(ϕ) =

Q+ assigns a positive rational weight to each assignment σ. We
w(σ). For weight function w

0, 1

{

}

n

and circuit ϕ, w(ϕ) is the weighted model count (WMC) of ϕ w.r.t. w.
In this paper, we focus on log-linear weight functions as they capture a wide class of distributions, in-
cluding those arising from graphical models, conditional random ﬁelds, and skip-gram models [24].
Log-linear models are represented as literal-weighted functions, deﬁned as:
Deﬁnition 1. For a set X of n variables, a weight function w is called literal-weighted if there is a
poly-time computable map w : X

(0, 1) such that for any assignment σ

0, 1

n :

Q

Pσ∈Rϕ

∈ {

}

∩

→
w(σ) =

Yx∈σ

w(x)
1
(cid:26)

w(x)

if x = 1
if x = 0

−
For all circuits ϕ, and log-linear weight functions w, w(ϕ) can be represented in size polynomial in
the input.

Probabilistic circuits: A probabilistic circuit is a satisﬁable circuit ϕ along with a weight function
w. ϕ and w together deﬁne a discrete probability distribution on the set
n that is supported over

0, 1

Rϕ. We denote the p.m.f. of this distribution as: P (ϕ, w)(σ) =

(cid:26)

{

}

0
ϕ(σ) = 0
w(σ)/w(ϕ) ϕ(σ) = 1

¬

∧

∨

or

v; and each internal node is labeled with a

In this paper, we study circuits that are fragments of the Negation Normal Form (NNF). A circuit ϕ
in NNF is a rooted, directed acyclic graph (DAG), where each leaf node is labeled with true, false,
v or
and can have arbitrarily many children.
We focus on four fragments of NNF, namely, Decomposable NNF(DNNF), deterministic-DNNF(d-
DNNF), Structured DNNF(SDNNF), and Prime Implicates(PI). For further information regarding
circuits in NNF, refer to the survey [14] and the paper [28].
The TV distance of two probability distributions P (ϕ1, w1) and P (ϕ2, w2) over
as: dT V (P (ϕ1, w1), P (ϕ2, w2)) = 1
P (ϕ2, w2)(σ)
.
2
|
|
P (ϕ1, w1) and P (ϕ2, w2) are said to be (1) equivalent if dT V (P (ϕ1, w1), P (ϕ2, w2)) = 0, (2) ε-close
if dT V (P (ϕ1, w1), P (ϕ2, w2))
Our closeness testing algorithm Teq, assumes access to an approximate weighted counter
Awct(α, β, ϕ, w), and an approximate weighted sampler Samp(α, β, ϕ, w). We deﬁne their behav-
ior as follows:
Deﬁnition 2. Awct(α, β, ϕ, w) takes a circuit ϕ, a weight function w, a tolerance parameter α > 0
and a conﬁdence parameter β > 0 as input and returns the approximate weighted model count of ϕ
w.r.t. w such that

ε, and (3) η-far if dT V (P (ϕ1, w1), P (ϕ2, w2))

P (ϕ1, w1)(σ)

n is deﬁned

σ∈{0,1}n

0, 1

P

η.

≤

−

≥

{

}

Pr

w(ϕ)
1 + α ≤

(cid:20)

Awct(α, β, ϕ, w)

(1 + α)w(ϕ)

(cid:21) ≥

β

1

−

≤

Tractable approximate counting algorithms for PCs are known as Fully Polynomial Randomised
Approximation Schemes (FPRAS). The running time of an FPRAS is given by T (α, β, ϕ) =
poly(α−1, log(β−1),
Deﬁnition 3. Samp(α, β, ϕ, w) takes a circuit ϕ, a weight function w, a tolerance parameter α > 0
and a conﬁdence parameter β > 0 as input and returns either (1) a satisfying assignment σ sampled
approximately w.r.t. weight function w with probability
indicating failure
with probability < β. In other words, whenever Samp samples σ:

β or (2) a symbol

ϕ
|

≥

⊥

−

).

1

|

P (ϕ, w)(σ)

1 + α ≤

Pr[Samp(α, β, ϕ, w) = σ]

(1 + α)P (ϕ, w)(σ)

≤

Tractable approximate sampling algorithms for PCs are known as Fully Polynomial Almost Uniform
Samplers (FPAUS). The running time of an FPAUS for a single sample is given by T (α, β, ϕ) =
poly(α−1, log(β−1),

).

ϕ
|

|
In the rest of the paper [m] denotes the set
event e, and E(v) represents the expectation of random variable v.

1, 2, . . . m

{

}

, 1(e) represents the indicator variable for

3

2.2 Related work

Closeness testing: Viewing circuit equivalence testing through the lens of distribution testing, we
see that the d-DNNF equivalence test of Darwiche and Huang [13] can be interpreted as an equiv-
alence test for uniform distribution on the satisfying assignments of d-DNNFs. This relationship
between circuit equivalence testing and closeness testing lets us rule out the existence of distribu-
tional equivalence tests for all those circuits for which circuit equivalence is already known to be
hard under complexity-theoretic assumptions. We will explore this further in Section 5.2.

Distribution testing: Discrete probability distributions are typically deﬁned over an exponentially
large number of points; hence a lot of recent algorithms research has focused on devising tests that
require access to only a sublinear or even constant number of points in the distribution [5]. In this
n, and thus we aim to devise algorithms with running
work, we work with distributions over
time at most polynomial in n. Previous work in testing distributions over Boolean functions has
focused on the setting where the distributions offer pair-conditional sampling access [7, 22]. Using
pair-conditional sampling access, Meel r
et al. [22] were able to test distributions for closeness
(cid:13)
using ˜O(tilt(ϕ)2/(η
6ε)2η) queries, where tilt is the ratio of the probabilities of the most and
least probable element in the support.

0, 1

−

}

{

3 Teq: a tractable algorithm for closeness testing

In this section, we present the main contribution of the paper: a closeness test for PCs, Teq. The
pseudocode of Teq is given in Algorithm 1.
Given satisﬁable circuits ϕ1, ϕ2 and weight functions w1, w2 along with parameters (ε, η, δ), Teq
decides whether the TV distance between P (ϕ1, w1) and P (ϕ2, w2) is lesser than ε or greater
δ. Teq assumes access to an approximate weighted counter
than η with conﬁdence at least 1
Awct(α, β, ϕ, w), and an approximate weighted sampler Samp(α, β, ϕ, w). We deﬁne their behavior
in the following two deﬁnitions.

−

−

−

⊥

p

1 + γ/4

. Teq only samples from one of the two PCs.

The algorithm Teq starts by computing constants γ and m. Then it queries the Awct routine
with circuit ϕ1 and weight function w1 to obtain a
1 approximation of w1(ϕ1) with
δ/8. A similar query is made for ϕ2 and w2 to obtain an approximate value
conﬁdence at least 1
for w2(ϕ2). These values are stored in k1 and k2, respectively. Teq maintains a m-sized array Γ, to
store the estimates for r(σi). Teq now iterates m times. In each iteration, it generates one sample σi
through the Samp call on line 7. There is a small probability of at most δ/4m that this call fails and
returns
The algorithm then proceeds to compute the weight of assignment σi w.r.t. the weight functions w1
and w2 and stores it in s1 and s2, respectively. Using the weights and approximate weighted counts
stored in k1, k2 the algorithm computes the value r(σi) on line 10, where r(σi) is an approximation
of the ratio of the probability of σi in the distribution P (ϕ2, w2) to its probability in P (ϕ1, w1). Since
σi was sampled from P (ϕ1, w1), its probability in P (ϕ1, w1) cannot be 0, ensuring that there is no
division by 0. If the ratio r(σi) is less than 1, then Γ[i] is updated with the value 1
r(σi) otherwise
the value of Γ[i] remains 0. After the m iterations, Teq sums up the values in the array Γ. If the sum
is found to be less than threshold m(ε + γ), Teq returns Accept and otherwise returns Reject.

−

The following theorem asserts the correctness of Teq.
Theorem 1. Given two satisﬁable probabilistic circuits ϕ1, ϕ2 and weight functions w1, w2, along
with parameters ε < η < 1 and δ < 1,

A. If dT V (P (ϕ1, w1), P (ϕ2, w2))

probability at least (1

δ).

B. If dT V (P (ϕ1, w1), P (ϕ2, w2))

probability at least (1

δ).

−

ε, then Teq(ϕ1, w1, ϕ2, w2, ε, η, δ) returns Accept with

η, then Teq(ϕ1, w1, ϕ2, w2, ε, η, δ) returns Reject with

≤

≥

−
The following theorem states the running time of the algorithm,
Theorem 2. Let
TAwct(γ, δ, max(
O
|

then
=
−
)) + TSamp(γ, δ, max(
ϕ2|
|

the
,
ϕ1|

γ
ϕ1|

ϕ2|

ε,

η

,

|

|

(cid:16)

time
)) log(δ−1)

complexity
If
.

γ2

(cid:17)

of Teq
the

is
in
underlying

4

1, δ/8, ϕ1, w1)
1, δ/8, ϕ2, w2)

−
−
2γ), δ/4m, ϕ1, w1)

⌉

(η

ε)/2
−
2 log(4/δ)/γ2

1 + γ/4
1 + γ/4
do

Algorithm 1 Teq(ϕ1, w1, ϕ2, w2, ε, η, δ)
1: γ
←
2: m
← ⌈
[0]
m
3: Γ
∗
←
Awct(
4: k1 ←
Awct(
5: k2 ←
6: for i
7:
8:
9:
10:
11:
Γ[i]
12:
←
13: if
i∈[m] Γ[i]
≤
Return Accept
14:
P
15: else
16:

p
1, 2 . . . , m
p
∈ {
}
Samp(γ/(4η
σi ←
−
then
if σi 6
=
⊥
w1(σi), s2 ←
s1 ←
s2
r(σi)
k2 ·
←
if r(σi) < 1 then

−
m(ε + γ) then

Return Reject

w2(σi)

r(σi)

k1
s1

1

PCs support approximate counting and sampling in polynomial time, then the running time of Teq
is also polynomial in terms of γ, log(δ−1) and max(
|

ϕ1|

ϕ2|

).

|

,

To improve readability, we use P1 to refer to the distribution P (ϕ1, w1) and P2 to refer to P (ϕ2, w2).

3.1 Proving the correctness of Teq

−

p

1 + γ/4

1 + γ/4

In this subsection, we present the theoretical analysis of Teq, and the proof of Theorem 1(A). We
will defer the proofs of Theorem 1(B) and Theorem 2 to the supplementary Section A.4.2 and Sec-
tion A.4.3, respectively.
For the purpose of the proof, we will ﬁrst deﬁne events Pass1, Pass2 and Good. Events
1, δ/8, ϕ1, w1) and
Pass1, Pass2 are deﬁned w.r.t.
the function calls Awct(
1, δ/8, ϕ2, w2), respectively (as on lines 4, 5 of Algorithm 1). Pass1 and Pass2
Awct(
1 + γ/4 approximations of the weighted
represent the events that the two calls correctly return
1 + γ/4)w1(ϕ1),
model counts of ϕ1 and ϕ2 i.e.
(
p

Awct(
and
−
Awct, we have Pr[Pass1], Pr[Pass2]
≥
Let Faili denote the event that Samp (Algorithm 1, line 7) returns the symbol
⊥
∀i∈[m] Pr[Faili] < δ/4m.
of the loop. By the deﬁnition of Samp we know that
The analysis of Teq requires that all m Samp calls and both Awct calls return correctly. We denote
Pass2. Applying the union bound we see that
this super-event as Good =
the probability of all calls to Awct and Samp returning without error is at least 1

−
≤
1 + γ/4)w2(ϕ2). From the deﬁnition of

p
1, δ/8, ϕ2, w2)

w2(ϕ2)
√1+γ/4 ≤

w1(ϕ1)
√1+γ/4 ≤

1, δ/8, ϕ1, w1)

in the ith iteration

1 + γ/4

1 + γ/4

(
p

Faili

Pass1

Awct(

δ/2:

δ/8.

i∈[m]

p

p

p

−

−

≤

∩

∩

1

T

Pr[Good] = 1

Pr[

−

[i∈[m]

Faili

Pass1

Pass2]

∪

∪

1

m

·

−

≥

δ/4m

2

·

−

−
δ/8 = 1

δ/2

(1)

−

We will now state a lemma, which we will prove in the supplementary Section A.4.

Lemma 1. Good

r(σ)

P2(σ)
P1(σ)

γ/4

P2(σ)
P1(σ)

→ (cid:12)
(cid:12)
(cid:12)
We now prove the lemma critical for our proof of correctness of Teq.
Lemma 2. Assuming the event Good, let A =

−

≤

(cid:12)
(cid:12)
(cid:12)

·

σ∈{0,1}n 1 (r(σ) < 1) (1

1. If dT V (P1, P2)

ε , then A

≤

≤

P
ε + γ/4

5

r(σ)) P1(σ) , then

−

2. If dT V (P1, P2)

η , then A

η

γ/4

≥

≥
P2(x)) = 0, then 1
2

−

Proof. If

x (P1(x)

−

P

P2(x)). Using this fact we see that,

P1(x)

P2(x)
|

−

=

x |

P

(P1(x)

x:P1(x)−P2(x)>0
P

−

dT V (P1, P2) =

Xσ:P2(σ)<P1(σ)

P1(σ)

−

P2(σ) =

Xσ:
P2 (σ)
P1 (σ) <1

1
(cid:18)

−

P2(σ)
P1(σ) (cid:19)

P1(σ)

=

Xσ∈{0,1}n

= A +

Xσ∈{0,1}n

P2(σ)
P1(σ)

1 (cid:18)

< 1

1
(cid:19) (cid:18)

−

P2(σ)
P1(σ) (cid:19)

P1(σ)

P2(σ)
P1(σ)

1 (cid:18)

< 1

1

−

(cid:19) (cid:18)

P2(σ)
P1(σ) (cid:19)

P1(σ)

A

−

−

B
Thus we have that dT V (P1, P2)
|
{z
three disjoint partition S1, S2 and S3 as following: S1 =
P1(σ) < 1) > 1(r(σ) < 1)
}

A = B. We now divide the set of assignments σ
}
P1(σ) < 1) = 1(r(σ) < 1)
}
. The
; S3 =
S2 =
deﬁnition implies that the indicator 1(r(σ) < 1) is 0 for all assignments in the set S2, and is 1 for
all assignments in S3. Similarly 1( P2(σ)
P1(σ) < 1) takes value 1 and 0 for all elements in S2 and S3,
respectively.

P1(σ) < 1) < 1(r(σ) < 1)
}

σ : 1( P2(σ)

σ : 1( P2(σ)

σ : 1( P2(σ)

n into
;

∈ {

0, 1

{

}

{

{

Now we bound the magnitude of B,

|

|

B

1
Xσ∈{0,1}n (cid:20)(cid:18)
For bj > 0, we have that

= (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B

|

| ≤ Xσ∈{0,1}n (cid:12)
(cid:12)
(cid:12)
(cid:12)

P2(σ)
P1(σ) (cid:19) 1 (cid:18)

P2(σ)
P1(σ)

−

< 1

(1

−

(cid:19) −

r(σ)) 1 (r(σ) < 1)

(cid:21)

|

P
1
(cid:20)(cid:18)

−

j |

j ajbj| ≤
P2(σ)
P1(σ) (cid:19) 1 (cid:18)

P

aj|
P2(σ)
P1(σ)

bj, and thus:

< 1

(1

−

(cid:19) −

r(σ)) 1 (r(σ) < 1)

P1(σ)

P1(σ)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

We can split the summation into three terms based on the sets in which the assignments lie. Some
summands take the value 0 in a particular set, so we don’t include them in the term.
P2(σ)
P1(σ) (cid:12)
(cid:12)
(cid:12)
(cid:12)
r(σ)) P1(σ)

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
1 (r(σ) < 1) (1

| ≤ Xσ∈S1
+

P2(σ)
P1(σ) (cid:19)

P2(σ)
P1(σ)

P2(σ)
P1(σ)

P1(σ) +

1
(cid:19) (cid:18)

P1(σ)

Xσ∈S2

r(σ)

< 1

< 1

1 (cid:18)

1 (cid:18)

−

−

B

|

Xσ∈S3

−

S3, P2(σ)

P1(σ) > 1, we can alter the second and third

|

∀

σ

∈

∈

B

1 (cid:18)

< 1

| ≤ Xσ∈S1

P2(σ)
P1(σ)

S2, r(σ) > 1 and
Since we know that
σ
∀
terms of the inequality in the following way:
P2(σ)
P1(σ) (cid:12)
(cid:12)
(cid:12)
(cid:12)
P1(σ)

r(σ)
(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
P2(σ)
P1(σ) −

(cid:12)
(cid:12)
(cid:12)
Using our assumption of the event Good and Lemma 1,
(cid:12)
Since dT V (P1, P2)
dT V (P1, P2)

−
ε + γ/4 and if dT V (P1, P2)

r(σ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
dT V (P1, P2)

A = B, we get

1 (r(σ) < 1)

−
ε, then A

P1(σ) +

Xσ∈S3

−

+

|

Xσ∈S2

≤

≤

≤ Xσ∈S1∪S2∪S3

P2(σ)
P1(σ)

P2(σ)
P1(σ) (cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1

r(σ)

P1(σ)

−

1 (cid:18)

r(σ)

(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
P2(σ)
(cid:12)
P1(σ) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γ/4
σ∈{0,1}n γ/4
·
γ/4. We can now deduce that if
P
| ≤
η, then A

P1(σ)

P1(σ)

γ/4.

| ≤

≤

−

η

B

≥

−

|
A

≥

Using Teq to test PCs in general. Exact weighted model counting(WMC) is a commonly sup-
ported query on PCs. In the language of PC queries, a WMC query is known as the marginal (MAR)
query. Conditional inference (CON) is another well studied PC query. Using CON and MAR, one
can sample from the distribution encoded by a given PC. It is known that if a PC has the structural
properties of smoothness and decomposability, then the CON and MAR queries can be computed
tractably. For the deﬁnitions of the above terms and further details, please refer to the survey [9].

6

4 Evaluation

To evaluate the performance of Teq, we implemented a prototype in Python. The prototype uses
WAPS3 [17] as a weighted sampler to sample over the input d-DNNF circuits. The primary objective
of our experimental evaluation was to seek an answer to the following question: Is Teq able to
determine the closeness of a pair of probabilistic circuits by returning Accept if the circuits are
ε-close and Reject if they are η-far? We test our tool Teq in the following two settings:

A. The pair of PCs represent small randomly generated circuits and weight functions.
B. The pair of PCs are from the set of publicly available benchmarks arising from sampling

and counting tasks.

Our experiments were conducted on a high performance compute cluster with Intel Xeon(R) E5-
2690 v3@2.60GHz CPU cores. For each benchmark, we use a single core with a timeout of 7200
seconds.

4.1 Setting A - Synthetic benchmarks

Dataset Our dataset for experiments conducted in setting A consisted of randomly generated 3-
CNFs and with random literal weights. Our dataset consisted of 3-CNFs with
14, 15, 16, 17, 18
}
variables. Since the circuits are small, we validate the results by computing the actual total variation
distance using brute-force.

{

dT V

η Actual Result Expected Result

Benchmark

15_3

14_2

17_4

14_1

ǫ

≤
0.75

0.8

0.75

≥
0.94

0.9

0.9

0.804

0.764

0.941

0.9

0.99

0.740

R

A

R

A

A/R

A

R

A

0.75

18_2

0.918
Table 1: Runtime performance of Teq. We experiment with 375 random PCs with known dT V , and
out of the 375 benchmarks we display 5 in the table and the rest in the supplementary Section B. In
the table ‘A’ represents Accept and ‘R’ represents Reject. In the last column ‘A/R’ represents that
both Accept and Reject are acceptable outputs for Teq.

0.9

R

R

Results Our tests terminated with the correct result in less than 10 seconds on all the randomly
generated PCs we experimented with. We present the empirical results in Table 1. The ﬁrst column
indicates the benchmark’s name, the second and third indicate the parameters ε and η on which we
executed Teq. The fourth column indicates the actual dT V distance between the two benchmark
PCs. The ﬁfth column indicates the output of Teq, and the sixth indicates the expected result. The
full detailed results are presented in the appendix Section B.

4.2 Setting B - Real-world benchmarks

Dataset We conducted experiments on a range of publicly available benchmarks arising from
sampling and counting tasks4. Our dataset contained 100 d-DNNF circuits with weights. We have
assigned random weights to literals wherever weights were not readily available. For the empirical
evaluation of Teq, we needed pairs of weighted d-DNNFs with known dT V distance. To generate
such a dataset, we ﬁrst chose a circuit and a weight function, and then we synthesized new weight
functions using the technique of one variable perturbation, described in the appendix Section B.1.

3https://github.com/meelgroup/WAPS
4https://zenodo.org/record/3793090

7

Benchmark

or-70-10-8-UC-10
s641_15_7
or-50-5-4
ProjectService3
s713_15_7
or-100-10-2-UC-30
s1423a_3_2
s1423a_7_4
or-50-5-10
or-60-20-6-UC-20

dT V ≤

ε

dT V ≥

η

Result Teq(s) Result Teq(s)

A
A
A
A
A
A
A
A
A
A

23.2
33.66
414.17
356.15
24.86
31.04
153.13
104.93
283.05
363.32

R
R
R
R
R
R
R
R
R
R

22.82
33.51
408.59
356.14
24.41
31.0
152.81
103.51
282.97
362.8

Table 2: Runtime performance of Teq. We experiment with 100 PCs with known dT V , and out of the
100 benchmarks we display 10 in the table and the rest in the appendix B. In the table ‘A’ represents
Accept and ‘R’ represents Reject. The value of the closeness parameter is ε = 0.01 and the farness
parameter is η = 0.2.

Results We set the closeness parameter ε, farness parameter η and conﬁdence δ for Teq to be
0.01, 0.2 and 0.01, respectively. The chosen parameters imply that if the input pair of probabilistic
0.01 close in dT V , then Teq returns Accept with probability atleast 0.99, otherwise
circuits are
0.2 far in dT V , the algorithm returns Reject with probability at least 0.99.
if the circuits are
The number of samples required for Teq (indicated by the variable m as on line 2 of Algorithm 1)
depends only on ε, η, δ and for the values we have chosen, we ﬁnd that we require m = 294 samples.

≥

≤

Our tests terminated with the correct result in less than 3600 seconds on all the PCs we experimented
with. We present the empirical results in Table 2. The ﬁrst column indicates the benchmark’s name,
the second and third indicate the result and runtime of Teq when presented with a pair of ε-close
PCs as input. Similarly, the fourth and ﬁfth columns indicate the result and observed runtime of Teq
when the input PCs are η-far . The full set of results are presented in the supplementary Section B.

5 A characterization of the complexity of testing

In this section, we characterize PCs according to the complexity of closeness and equivalence testing.
We present the characterization in Table 3. The results presented in the table can be separated into
(1) hardness results, and (2) upper bounds. The hardness results, presented in Section 5.2, are largely
derived from known complexity-theoretic results. The upper bounds, presented in Section 5.1, are
derived from a combination of established results, our algorithm Teq and the exact equivalence test
of Darwiche and Huang [13](presented in supplementary Section A.1 for completeness).

5.1 Upper bounds

In Table 3 we label the pair of classes of PCs that admit a poly-time closeness and equivalence test
with green symbols C and E respectively. Darwiche and Huang [13] provided an equivalence test
for d-DNNF s. From Theorem 1, we know that PCs that supports the Awct and Samp queries in
poly-time must also admit a poly-time approximate equivalence test. A weighted model counting
algorithms for d-DNNFs was ﬁrst provided by Darwiche [11], and a weighted sampler was provided
by [17]. Arenas et al. [1] provided the ﬁrst approximate counting and uniform sampling algorithm
for SDNNFs. Using the following lemma, we show that with the use of chain formulas, the uniform
sampling and counting algorithms extend to log-linear SDNNF distributions as well.
Lemma 3. Given a SDNNF formula ϕ (with a v-tree T ), and a weight function w, Samp(ϕ, w)
requires polynomial time in the size of ϕ.

The proof is provided in the supplementary Section A.5.

8

NNF
NNF
EC
PI
EC
DNNF
EC
SDNNF
EC
d-DNNF EC

PI

DNNF

SDNNF

d-DNNF

U U
EU
EU
U U

EU
EU
EU

EC
EC

EC

Table 3: Summary of results. C (resp. E) indicates that a poly-time closeness (resp. equivalence) test
exists. C (resp. E) indicates that a poly-time closeness (equivalence) test exists only if PH collapses.
‘U ’ indicates that the existence of a poly-time test is not known. The table is best viewed in color.

5.2 Hardness

In Table 3, we claim that the pairs of classes of PCs labeled with symbols C and E , cannot be
tested in poly-time for closeness equivalence, respectively. Our claim assumes that the polynomial
hierarchy (PH) does not collapse. To prove the hardness of testing the labeled pairs, we combine
previously known facts about PCs and a few new arguments. Summarizing for brevity,

• We start off by observing that PC families are in a hierarchy, with CNF

NNF and DNF

⊆

SDNNF

DNNF [14].

⊆

⊆

• We then reduce the problem of satisﬁability testing of CNFs (NP-hard) and validity testing
of DNFs (co-NP-hard) into the problem of equivalence and closeness testing of PCs, in
Propositions 1, 2 and 3. These propositions and their proofs can be found in the supplemen-
tary Section A.5.

• We then connect the existence of poly-time algorithms for equivalence to the collapse of

PH via a complexity result due to Karp and Lipton [18].

The NP-hardness of deciding the equivalence of pairs of DNNFs and pairs of SDNNFs was ﬁrst
shown by Pipatsrisawat and Darwiche [28]. We recast their proofs in the language of distribution
testing for the sake of completeness in the supplementary Section A.5.

6 Conclusion and future work

In this paper, we studied the problem of closeness testing of PCs. Before our work, poly-time al-
gorithms were known only for the special case of equivalence testing of PCs; and, no poly-time
closeness test was known for any PC. We provided the ﬁrst such test, called Teq, that used ideas
from the ﬁeld of distribution testing to design a novel algorithm for testing the closeness of PCs. We
then implemented a prototype for Teq, and tested it on publicly available benchmarks to determine
the runtime performance. Experimental results demonstrate the effectiveness of Teq in practice.

We also characterized PCs with respect to the complexity of deciding equivalence and closeness.
We combined known hardness results, reductions, and our proposed algorithm Teq to classify pairs
of PCs according to closeness and equivalence testing complexity. Since the characterization is
incomplete, as seen in Table 3, there are questions left open regarding the existence of tests for
certain PCs, which we leave for future work.

Broader Impact

Recent advances in probabilistic modeling techniques have led to increased adoption of the said
techniques in safety-critical domains, thus creating a need for appropriate veriﬁcation and testing
methodologies. This paper seeks to take a step in this direction and focuses on testing properties of
probabilistic models likely to ﬁnd use in safety-critical domains. Since our guarantees are probabilis-
tic, practical adoption of such techniques still requires careful design to handle failures.

9

Acknowledgments and Disclosure of Funding

We are grateful to the anonymous reviewers of UAI 2021 and NeurIPS 2021 for their constructive
feedback that greatly improved the paper. We would also like to thank Suwei Yang and Lawqueen
Kanesh for their useful comments on the earlier drafts of the paper. This work was supported in part
by National Research Foundation Singapore under its NRF Fellowship Programme[NRF-NRFFAI1-
2019-0004 ] and AI Singapore Programme [AISG-RP-2018-005], and NUS ODPRT Grant [R-252-
000-685-13]. The computational work for this article was performed on resources of the National
Supercomputing Centre, Singapore (https://www.nscc.sg).

10

References

[1] Marcelo Arenas, Luis Alberto Croquevielle, Rajesh Jayaram, and Cristian Riveros. When is

Approximate Counting for Conjunctive Queries Tractable? In STOC, 2021.

[2] Dominik Aronsky and Peter J Haug. Diagnosing community-acquired pneumonia with a

bayesian network. In Proceedings of the AMIA Symposium, 1998.

[3] Rafael Cano, Carmen Sordo, and José M Gutiérrez. Applications of bayesian networks in

meteorology. In Advances in Bayesian networks. 2004.

[4] Clément Canonne and Ronitt Rubinfeld. Testing probability distributions underlying aggre-

gated data. In Automata, Languages, and Programming, 2014.

[5] Clément L Canonne. A survey on distribution testing: Your data is big. but is it blue? Theory

of Computing, 2020.

[6] Clément L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing

bayesian networks. IEEE Transactions on Information Theory, 2020.

[7] Sourav Chakraborty and Kuldeep S. Meel. On testing of uniform samplers. In AAAI, 2019.

[8] Supratik Chakraborty, Dror Fried, Kuldeep S Meel, and Moshe Y Vardi. From weighted to

unweighted model counting. In IJCAI, 2015.

[9] YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying

framework for tractable probabilistic models. 2020.

[10] Karine Chubarian and György Turán. Interpretability of bayesian network classiﬁers: Obdd

approximation and polynomial threshold functions. In ISAIM, 2020.

[11] Adnan Darwiche. On the tractable counting of theory models and its application to truth

maintenance and belief revision. Journal of Applied Non-Classical Logics, 2001.

[12] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the

ACM (JACM), 2003.

[13] Adnan Darwiche and Jinbo Huang. Testing equivalence probabilistically. In Technical Report,

2002.

[14] Adnan Darwiche and Pierre Marquis. A knowledge compilation map. JAIR, 2002.

[15] Saikat Dutta, Owolabi Legunsen, Zixin Huang, and Sasa Misailovic. Testing probabilistic

programming systems. In Proc. of Joint Meeting on ESE and FSE, 2018.

[16] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil

Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.

[17] Rahul Gupta, Shubham Sharma, Subhajit Roy, and Kuldeep S Meel. Waps: Weighted and

projected sampling. In TACAS, 2019.

[18] Richard M. Karp and Richard J. Lipton. Some connections between nonuniform and uniform

complexity classes. In STOC, 1980.

[19] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

MIT press, 2009.

[20] Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. arXiv

preprint arXiv:1207.6083, 2012.

[21] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples

in generative adversarial networks. NeurIPS, 2018.

[22] Kuldeep S. Meel r
(cid:13)
NeurIPS, 2020.

, Yash Pote r
(cid:13)

, and Sourav Chakraborty. On Testing of Samplers.

In

11

[23] Andrzej S. Murawski and Joël Ouaknine. On probabilistic program equivalence and reﬁnement.

In Martín Abadi and Luca de Alfaro, editors, CONCUR, 2005.

[24] K.P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.

[25] Agnieszka Oni´sko, Marek J Druzdzel, and Hanna Wasyluk. Extension of the hepar ii model to

multiple-disorder diagnosis. In Intelligent Information Systems. 2000.

[26] Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp,
Guy Van den Broeck, Kristian Kersting, and Zoubin Ghahramani. Einsum networks: Fast and
scalable learning of tractable probabilistic circuits. In ICML, 2020.

[27] Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin
Trapp, Kristian Kersting, and Zoubin Ghahramani. Random sum-product networks: A sim-
ple and effective approach to probabilistic deep learning. In UAI. PMLR, 2020.

[28] Knot Pipatsrisawat and Adnan Darwiche. New compilation languages based on structured

decomposability. In AAAI, 2008.

[29] Arthur R Pope and David G Lowe. Probabilistic models of appearance for 3-d object recogni-

tion. International Journal of Computer Vision, 2000.

[30] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[31] Yasir Rahmatallah, Frank Emmert-Streib, and Galina Glazko. Gene sets net correlations anal-

ysis (gsnca): a multivariate differential coexpression test for gene sets. Bioinformatics, 2014.

[32] Dan Roth. On the hardness of approximate reasoning. Artiﬁcial Intelligence, 82(1-2), 1996.

[33] Nicolas Städler and Sach Mukherjee. Multivariate gene-set testing based on graphical models.

Biostatistics, 2015.

[34] Larry J. Stockmeyer. The polynomial-time hierarchy. Theoretical Computer Science, 1976.

[35] Weiwei Yin, Swetha Garimalla, Alberto Moreno, Mary R Galinski, and Mark P Styczynski. A
tree-like bayesian structure learning algorithm for small-sample datasets from complex biolog-
ical model systems. BMC systems biology, 2015.

12

A Proofs omitted from the paper

A.1 A test for equivalence

For the sake of completeness we recast the d-DNNF circuit equivalence test of Darwiche and Huang
[13] into an equivalence test for log-linear probability distributions.

⌉

n/δ

[m]n

Algorithm 2 Peq(ϕ1, w1, ϕ2, w2, δ)
1: m
2: θ
3: if π(ϕ1, w1)(θ) = π(ϕ2, w2)(θ) then
4:
5: else
6:

← ⌈
∼
Return Accept

Return Reject

∈

The algorithm: The pseudocode for Peq is shown in Algorithm 2. Peq takes as input two sat-
isﬁable circuits ϕ1, ϕ2 deﬁned over n Boolean variables, a pair of weight functions w1, w2 and a
(0, 1). Recall that a circuit ϕ and a weight function w together deﬁne
tolerance parameter δ
the probability distribution P (ϕ, w). Peq returns Accept with conﬁdence 1 if the two probabil-
ity distributions P (ϕ1, w1) and P (ϕ2, w2) are equivalent, i.e. dT V (P (ϕ1, w1), P (ϕ2, w2)) = 0. If
dT V (P (ϕ1, w1), P (ϕ2, w2)) > 0, then it returns Reject with conﬁdence at least 1
The algorithm starts by drawing a uniform random assignment θ from [m]n, where m =
.
⌉
Using the procedure given in Proposition 2 (in Section A.2), Peq computes the values π(ϕ1, w1)(θ)
and π(ϕ2, w2)(θ), where π(ϕ, w) is the network polynomial [12]. π(ϕ, w) deﬁned as:

n/δ

−

δ.

⌈

π(ϕ, w) =

w(σ)
w(ϕ)





Xσ∈Rϕ

xi

Yxi|=σ

Y¬xj|=σ

(1

−

xj )



The two values are then compared on line 3, and if they are equal the algorithm returns Accept and
otherwise returns Reject. The central idea of the test is that whenever the two distributions P (ϕ1, w1)
and P (ϕ2, w2) are equivalent, the polynomials π(ϕ1, w1) and π(ϕ2, w2) are also equivalent, however
when they are not equivalent, the polynomials disagree on atleast 1
δ fraction of assignments from
the set [m]n.

−

We formally claim and prove the correctness of Peq in Lemma 4 in the Section A.2.

A.2 An analysis for Algorithm 2

In this section, we present the theoretical analysis of Algorithm 2 (Peq) and the proof of the follow-
ing lemma.
Lemma 4. Given two satisﬁable probabilistic circuits ϕ1, ϕ2 and weight functions w1, w2, along
with conﬁdence parameter δ

(0, 1).

A. If dT V (P (ϕ1, w1), P (ϕ2, w2)) = 0, then Peq(ϕ1, w1, ϕ2, w2, δ) returns Accept with proba-

B. If dT V (P (ϕ1, w1), P (ϕ2, w2)) > 0, then Peq(ϕ1, w1, ϕ2, w2, δ) returns Reject with proba-

∈

bility 1.

bility at least (1

δ).

−

π(ϕ1, w1)
Peq returns Accept if π(ϕ1, w1)(σ) = π(ϕ2, w2)(σ). Since P (ϕ1, w1)
≡
π(ϕ2, w2), it follows that Peq always returns Accept for two equivalent probabilistic distributions.
For the proof of Lemma 4(B) we will ﬁrst deﬁne some notation, and then we show (in Lemma 5) that
a random assignment over [m]n is likely to be a witness for non-equivalence with probability > 1
δ.
= π(ϕ2, w2)(σ).
The proof immediately follows as we know that Peq returns Reject if π(ϕ1, w1)(σ)
Deﬁnition 4. π
1 variables, obtained by setting the variable xi
to 1. Similarly π

|xi=1(ϕ, w) is a polynomial over n
|xi=0(ϕ, w) is obtained by setting the variable xi to 0, thus:
|xi=1(ϕ, w)
π(ϕ, w) = (1

|xi=0(ϕ, w) + xiπ

P (ϕ2, w2)

xi)π

→

−

≡

−

−

13

6
From the deﬁnition, we can immediately infer the following proposition.
Proposition 1. If π(ϕ1, w1)

π(ϕ2, w2) then for all xi, at least one of the following must be true:

• π

• π

|xi=1(ϕ1, w1)
|xi=0(ϕ1, w1)

= π

= π

6≡
|xi=1(ϕ2, w2)
|xi=0(ϕ2, w2)

For the proofs in this section, we will use the following notation. For a circuit ϕ deﬁned over the
variables

, we deﬁne a polynomial P (ϕ, w) :

[0, 1]:

0, 1

n

x1, . . . , xn}

{

{

}

→

P (ϕ, w) =

w(σ)
w(ϕ)

Xσ∈Rϕ

xi



Yxi|=σ



Y¬xj|=σ

(1

−

xj)



.

}

We deﬁne another polynomial π(ϕ, w) which is P (ϕ, w) but deﬁned from [m]n
1 . . . , m
{
To show that the polynomial π(ϕ, w) can be computed in time polynomial in the size of the repre-
sentation, we will adapt the procedure given by [13].
Proposition 2. Let ϕ be a circuit over the set X =
time WMC. Let w : X
variables in X and θ(x) be the assignment to variable x
deﬁne a function S() recursively as follows:

of n variables , that admits poly-
[m]n be an assignment to the
X in θ. For each node η in the circuit,

x1, . . . , xn}
Q+ be a weight function and let θ
∈

Q where [m] =

→

→

∈

{

• S(η) =

• S(η) =

i S(ni), where η is an or-node with children ni.
i S(ni), where η is an and-node with children ni.

P

• S(η) =

0,
1,
w(x)θ(x),
(1

−

w(x))(1

if η is a leaf node false
if η is a leaf node true
if η is a leaf node x
if η is a leaf node

X
∈
x, x

¬

X

∈

θ(x)),

−

• π(ϕ, w) = S(η)/w(ϕ), where η is the root node

Q





We can compute the quantity w(ϕ) in linear time due to our assumption of poly-time WMC, hence
we can ﬁnd π(ϕ, w)(θ) in time linear in the size of the d-DNNF.

Lemma 5. For a random assignment σ
P (ϕ2, w2)] > 1
δ

−

∼

[m]n, Pr[π(ϕ1, w1)(σ)

= π(ϕ2, w2)(σ)

P (ϕ1, w1)

|

6≡

Proof. For n = 1, σ is an assignment to a single variable x. The polynomial on the single variable
x can be parameterised as π(ϕ, w)(x) = αx + (1
w(θ) .
Let polynomials π(ϕ1, w1), π(ϕ2, w2) be parameterised with α1, α2, respectively. Our assumption
that P (ϕ1, w1)
π(ϕ2, w2) which in turn
6≡
= α2.
implies that α1 6
The the set of inputs x for which two non-equivalent polynomials agree is given by,

P (ϕ2, w2) immediately leads to the fact that π(ϕ1, w1)

x) where parameter α =

w(x)
Pθ∈Rϕ

α)(1

−

−

6≡

π(ϕ1, w1)(x) = π(ϕ2, w2)(x)

α1x + (1

α1)(1
−
2(α1 −

x) = α2x + (1
α2

−
α2)x = α1 −
x = 1/2

α2)(1

x)

−

−

From the initial assumption we know that x can only take integer values, hence there are no inputs in
the set [m] for which π(ϕ1, w1)(σ)
=
π(ϕ2, w2)(σ)
We now assume that the hypothesis holds for n
π(ϕ2, w2) over n variables. From Prop 1 we know that at least one of the following holds:

= π(ϕ2, w2)(σ). Thus, for n = 1, and any σ, Pr[π(ϕ1, w1)(σ)

1 variables. Consider polynomials π(ϕ1, w1)

P (ϕ2, w2)] = 0

P (ϕ1, w1)

−

6≡

6≡

|

14

6
6
6
6
6
• π

• π

|xi=1(ϕ1, w1)
|xi=0(ϕ1, w1)

= π

= π

|xi=1(ϕ2, w2)
|xi=0(ϕ2, w2)

Without any loss of generality we assume the latter. Then we know that there exists a set Σ
[m]n−1,

(m

Σ

⊆

|

| ≥

−

1)n−1, such that
∀σ∈Σ, π

|xn=0(ϕ1, w1)(σ)

= π

|xn=0(ϕ2, w2)(σ)

The set of assignments σ for which π(ϕ1, w1)(σ) = π(ϕ2, w2)(σ) is given by,

π(ϕ1, w1)(σ) = π(ϕ2, w2)(σ)

|xn=1(ϕ1, w1)(σ) = (1

−

xn)π

|xn=0(ϕ2, w2)(σ) + xnπ

|xn=1(ϕ2, w2)(σ)

(1

−

xn)π

|xn=0(ϕ1, w1)(σ) + xnπ
|xn=1(ϕ1, w1)(σ)

π

−

xn(π

π
−
= π

|xn=0(ϕ1, w1)(σ)

|xn=0(ϕ2, w2)(σ))
|xn=1(ϕ2, w2)(σ) + π
|xn=0(ϕ1, w1)(σ)
|xn=0(ϕ2, w2)(σ)
π
−
1)n−1 assignments σ s.t.
there are at
least (m
From the assumptions we know that
−
|xn=0(ϕ1, w1)(σ)
|xn=0(ϕ2, w2)(σ)
= 0, from which we can conclude that the RHS is non-
π
zero. Thus for all such σ there can be at most one value of xn for which the equality holds, which
1)n
leaves m
assignments to n variables for which π(ϕ1, w1)(σ)
Since the total number of assignments for n variables is mn, out of which (m
1)n witness the non-
equivalence of the two probability distributions, we know that for a randomly chosen assignment
σ

1 values which xn cannot take. Thus there are at least (m
= π(ϕ2, w2)(σ).

[m]n, we have

1)n−1 = (m

(m

1)

×

−

−

−

−

−

−

π

∼

(m

1)n

≥
> 1

−
mn
δ

−

n

1
≥ (cid:18)

δ
n (cid:19)
(using m from Algorithm 2)

−

Pr[π(ϕ1, w1)(σ)

= π(ϕ2, w2)(σ)

P (ϕ1, w1)

|

6≡

P (ϕ2, w2)]

15

6
6
6
6
6
6
A.3 Omitted proof from the analysis of Algorithm 1

In this subsection, we present the proof of Theorem 1(B), and Theorem 2. Recall that we use P1 and
P2 to refer to P (ϕ1, w1) and P (ϕ2, w2), respectively.

A.4 Proof of Lemma 1

Proof. The quantity r(σ) (line 10 from Algorithm 1) conditioned on the event Faili

Good:

w2(σ)

1, δ/8, ϕ2, w2) ·

Awct(

p

1 + γ/4

−
w1(σ)

⊂
1, δ/8, ϕ1, w1)

r(σ) =

Awct(
Conditioned on the events Pass1, Pass2

1 + γ/4

p

−

⊂

Good, we know that with probability 1:
w2(σ)w1(ϕ1)
w2(ϕ2)w1(σ)

1 + γ/4)2

1 + γ/4)−2 < r(σ) < (

w2(σ)w1(ϕ1)
w2(ϕ2)w1(σ)
P1(σ) (1 + γ/4)−1 < r(σ) < (1 + γ/4) P2(σ)

(
p

p
P1(σ) and therefore,

γ/4, 1

1
1+γ/4

−

(cid:17) ≤

P2(σ)
P1(σ) ·

γ/4

Which gives us: P2(σ)
P2(σ)
P1(σ)

r(σ)

P2(σ)
P1(σ) ·

≤

−

(cid:12)
(cid:12)
(cid:12)
A.4.1 Proof of Theorem 1(A)

(cid:12)
(cid:12)
(cid:12)

max
0<γ<1 (cid:16)

−
−
Pr[Samp(γ/(4η

Proof. We assume the event Good. Let σi be the sample returned by the sampler Samp in the ith
r(σi). Thus Γ[i] is a r.v. which takes on a
iteration. If r(σi) > 1, Γ[i] takes value 0, else Γ[i] = 1
value from [0, 1]. We can write Γ[i] = 1 (r(σi) < 1) (1
r(σi)) The expectation of Γ[i] is:

E[Γ[i]] =

1 (r(σ) < 1) (1

Xσ∈{0,1}n

r(σ))

·

−

2γ), δ/4m, ϕ1, w1) = σ]

(2)

−

According to deﬁnition 3, and our assumption of Faili
Pr[Samp(γ/(4η

2γ), δ/4m, ϕ1, w1) = σ]

⊂
(1 + γ/(4η

Good, we know that with probability 1

2γ))P1(σ). Thus we have,

−
E[Γ[i]]

≤ Xσ∈{0,1}n

1 (r(σ) < 1) (1

r(σ))

·

−

≤

−
(1 + γ/(4η

2γ))P1(σ)

−

Recall that in Lemma 2, we deﬁne A =
can simplify the above expression as: E[Γ[i]] = (1 + γ/(4η
assumption of ε-closeness and the result of Lemma 2-1 to ﬁnd a bound on the expectation,

r(σ)) P1(σ). Therefore, we
A. We can then use the

σ∈{0,1}n 1 (r(σ) < 1) (1

−
2γ))

P

−

·

E[Γ[i]]

(1 + γ/(4η

2γ)) (ε + γ/4)

ε + γ/2

≤

Using the linearity of expectation we get: E

−
< m(ε + γ/2). Teq returns Reject when
i∈[m] Γ[i]
i∈[m] Γ[i] > m(ε + γ) on line 13. Since the Γ[i]’s are i.i.d random variables taking values in [0,

hP
1], we apply the Chernoff bound to ﬁnd the probability of Accept, assuming the event Good:
P

≤

i

Teq returns Accept

Pr

(cid:20)

Good

(cid:21)

= 1

−

Pr 

Γ[i] > m(ε + γ)

Xi∈[m]





2e−γ2m/2

1

−

≥

1

−

≥

δ/2

The value for m is taken from line 2 of Algorithm 1. Using (1), we see that the probability of
Good] Pr[Good] =
Teq returning Accept is: Pr[Teq returns Accept]
1
(1

Pr[Teq returns Accept

δ/2)(1

δ/2)

≥

δ

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

≥

−

A.4.2 Proof of Theorem 1(B)

Proof. First we assume the event Good. Then according to deﬁnition 3, we know that with proba-
bility 1 (since we assume event Faili

Good)

⊂

Pr[Samp(γ/(4η

−

2γ), δ/4m, ϕ1, w1) = σ]

P1(σ)
(1 + γ/(4η

−

≥

2γ))

16

Thus substituting into (2), we get

E[Γ[i]]

≥ Xσ∈{0,1}n

1 (r(σ) < 1) (1

r(σ))

−

P1(σi)

1 + γ/(4η

2γ)

−

Then we use the η-farness assumption and Lemma 2-2
γ/4

The algorithm returns Accept when
linearity of expectation.

P

E[Γ[i]]

≥

−

η
1 + γ/(4η
i∈[m] Γ[i]

≤

= η

γ/2

2γ)

−

−
m(ε + γ) (on line 13). Then using (4) and the

(3)

(4)


Since the Γ[i]’s are i.i.d random variables taking values in [0, 1], we apply the Chernoff bound to
ﬁnd the probability of Reject, given the assumption of the event Good:



E 

Xi∈[m]

Γ[i]]

m(η

−

≥

γ/2)

Pr [Teq returns Reject

Good] = 1

|

−

Pr 

Xi∈[m]



Γ[i]

≤

m(ε + γ)



1

−

≥

Pr 

m(η

γ/2)

−



− Xi∈[m]

Γ[i]

m(η

γ/2

ε

−

−

γ)

−

≥



1

1

−

−

≥

≥

Pr 

| Xi∈[m]

2e−γ2m/2

Γ[i]

−

m(η

−

γ/2)

| ≥

mγ/2



δ/2 (Substituting m as in line 2)

1

−

≥

Hence, the probability that Algorithm 1 returns Reject is

Pr[Teq returns Reject]

A.4.3 Proof of Theorem 2

≥
= (1

Pr [Teq returns Reject

δ/2)(1

δ/2)

−

≥

−

Good] Pr [Good]
|
1

(Using (1))

δ

−

|

ϕ
|
√1 + x for x

Proof. Teq makes two calls to Awct on line 4 and 5 of Algorithm 1. According to deﬁni-
tion 2, the runtime of the Awct(
1, δ/8, ϕ) =
poly((

1, δ/8, ϕ, w) query is T (

1 + γ/4
).

1)−1, log(δ−1),
p

1 + γ/4

1 + γ/4

p

−

−

p

Using the identity 1 + x

−

x2
2 ≤

2 −

≥

0 and the fact that γ
11
γ

(0, 1)

∈

|

⌈

−

<

−

p

)))

γ/8

1 ≤

1 + γ/4

1
γ2/32

the runtime of the Samp(γ/(4η

1
1 + γ/4
p
1)−1) algorithm also runs in poly(γ−1). Thus the Awct queries run
Hence any poly((
−
in O(poly(γ−1, log(δ−1), max(
ϕ1|
,
|
log(2/δ)/2γ2

ϕ2|
calls to Samp on lines 7 of Algorithm 1. According

ϕ1|
−
thus the algorithm remains in poly(γ−1). We then see that

Teq makes m =
to deﬁnition 3,
2γ), δ/4m,
ϕ1|
4
γ ,
log(δ−1). Since log(m)
∈
O(poly(γ−1, log(δ−1), max(
|
Since each Samp call and each Awct call requires atmost polynomial time in terms of γ−1, log(δ−1)
) we know that the algorithm itself runs in time polynomial in γ−1, log(δ−1) and
and max(
|
,
ϕ1|
max(
|

−
<
log(4m/δ) = log(4m) +
poly(log(γ−1), log log(δ−1)), we know that Samp queries run in
ϕ1|

2γ), δ/4m, ϕ1, w1) query is T (γ/(4η
). First we see that 4η−2γ

2γ))−1, log((δ/4m)−1),

) = poly((γ/(4η

ϕ1|
,
ϕ2|
|

ϕ2|
|
).

ϕ2|

))).

−

⌉

γ

,

|

|

|

17

A.5 Proofs omitted from Section 5

For the following proofs, we assume a uniform weight function.
Proposition 1. If there exists a poly-time randomised algorithm for deciding the equivalence of a
pair of PCs with at least one PC in CNF, then NP=RP.

Proof. For CNFs, testing satisﬁability is known to be NP-hard. Consider a CNF ϕ deﬁned over
variables

and a circuit ψ s.t. ψ

x1, . . . , xn}

{

ˆϕ = (

≡
xn+1 →
¬

V
ϕ)

i∈[n+1] xi. Deﬁne
(xn+1 → ^i∈[n]

∧

xi)

We see that the size of the new CNF is
speciﬁcally the assignment
only if
dT V (P ( ˆϕ, w), P (ψ, w)) = 0 would imply NP

+ n). ˆϕ has at least one satisfying assignment,
∀i∈[n+1]xi = 1. We notice that dT V (P ( ˆϕ, w), P (ψ, w)) = 0 if and
= 0. Thus the existence of a poly-time randomised algorithm for deciding whether

Rϕ|

O(
|

ϕ
|

| ∈

ˆϕ

|

|

RP and hence NP=RP.

⊆

Proposition 2. If there exists a poly-time randomised algorithm for deciding the closeness of a pair
of PCs with at least one PC in CNF, then NP=RP.

Proof. dT V (P ( ˆϕ, w), P (ψ, w))
time randomised algorithm which returns Reject if dT V (P ( ˆϕ, w), P (ψ, w))
dT V (P ( ˆϕ, w), P (ψ, w))
and hence NP=RP.

Rϕ|
0.1 with probability > 2/3. Such an algorithm would imply BPP

> 0. Assume there exists a poly-
0.4 and Accept if
NP,

0.5 if and only if

⊆

≥

≤

≥

|

Proposition 3. If there exists a poly-time randomised algorithm for deciding the equivalence of a
pair of PCs with at least one PC in DNF, then NP=RP.

Proof. For DNFs, deciding validity is known to be co-NP-hard. Given DNF ϕ and a circuit ψ =
T rue, the existence of a poly-time randomized algorithm for checking the equivalence of ψ and ϕ
would imply that co-NP

co-RP and hence co-NP = co-RP.

⊆

Using Corollary 6.3 from [18] we see that PH collapses as a result of either of the above implications.

From the set inclusions DNF
NNF, we obtain all hardness results.
From the fact that d-DNNFs support weighted counting and sampling we have the existence results.

DNNF and CNF

SDNNF

⊆

⊆

⊆

The following lemma supports our claim in table 3.
Lemma 6. Given a SDNNF formula ϕ (with a v-tree T ), and a weight function w, Samp(ϕ, w)
requires polynomial time in the size of ϕ.

Proof. Here we will assume that the weights are in the dyadic form i.e. they can be repre-
Z+. Then using the weighted to unweighted construction
sented as the fraction d/2p for d, p
from [8], the problem of approximate weighted sampling over SDNNF can be reduced to approx-
imate uniform sampling. Given a SDNNF ϕ, and a weight function w, we generate a SDNNF
C0
is chain formula having exactly
xi ∨
ϕw ≡
¬
2p = d
d satisfying assignments, and C1
xi)
w(
i
−
¬
satisfying assignments.

is a chain formula with w(xi)

i∈[n](
∧
2p = 2p
V

i ). Here, C0
i

i∈[n](xi ∨

C1
i )

V

×

×

∈

∧

ϕ

∧

The property of decomposability on the
of variables disjoint from the set of variables in ϕ and and also from all Cj , such that j

nodes of ϕ is preserved as each Ci introduces a new set
= i. The
nodes in the chain formula are also trivially decomposable and structured as each chain formula

∧
variable appears exactly once in the formula.
If σ is an assignment to the set of variables of S and if S′
S, then let σ↓S′ denote the projec-
tion of σ on the variables in S′. The weighted formula ϕ is deﬁned over variable set var(ϕ). The
σ′
formula ϕw deﬁned above has the property that if ϕ(σ) = 1, then
↓var(ϕ) =
= w(σ). Thus a uniform distribution on Rϕw , when projected on var(ϕ) induces the
σ
weighted distribution P (ϕ, w). This property allows weighted sampling and counting on ϕ with the
help of a uniform sampler for the generated formula ϕw.

ϕw(σ′) = 1

Rϕw |

σ′

}|

|{

⊆

∧

/

|

|

18

6
B Experimental evaluation

In this section we will ﬁrst discuss the method for generating the synthetic dataset, and then we
present the extended table of results.

B.1 One variable perturbation

Consider two weight functions w1 and w2 that differ only in the weight assigned to the literals v0 and
v1. Then, from the deﬁnition of dT V :

dT V (P (ϕ, w1), P (ϕ, w2)) =

1
2 Xσ∈{0,1}n (cid:12)
(cid:12)
(cid:12)
(cid:12)

w1(σ)
w1(ϕ) >

w1(σ)
w1(ϕ) −

w2(σ)
w2(ϕ) (cid:12)
(cid:12)
(cid:12)
(cid:12)

w2(σ)
w2(ϕ) . Thus,
w2(σ)
w2(ϕ) (cid:19)

w1(σ)
w1(ϕ) −

Let S

0, 1

}

⊆ {

n be the set of assignments for which

dT V (P (ϕ, w1), P (ϕ, w2)) =

(cid:18)

Xσ∈S

∧
dT V (P (ϕ, w1), P (ϕ, w2)) =

≡

v1. Thus,
w1(ϕ

Lets assume wlog that w1 assigns a larger weight to v1 than w2 does. Then, S contains all and only
those assignments that have literal v1, i.e. S

ϕ

w2(ϕ

∧

v1)

v1)
w1(ϕ) −
1 is w1 with the weight of v1 set to 1. Using a

∧
w2(ϕ)

We can rewrite w1(ϕ
v1) = w′
similar transformation on w2(ϕ

∧

1(ϕ)

w1(v1), where w′

×

v1) we get

∧
dT V (P (ϕ, w1), P (ϕ, w2)) =

w′

1(ϕ)

w1(v1)

×
w1(ϕ)

w′

2(ϕ)

w2(v1)

×
w2(ϕ)

−

We know that w′

1(ϕ) = w′

2(ϕ) as w1 and w2 differed only on the one variable v1.

dT V (P (ϕ, w1), P (ϕ, w2)) = w′

1(ϕ)

w1(v1)
w1(ϕ) −

w2(v1)
w2(ϕ) (cid:19)

× (cid:18)

All quantities in the above expression are either known constants or they are deﬁned w.r.t the already
compiled d-DNNF, thus guaranteeing that dT V (P (ϕ, w1), P (ϕ, w2)) can be computed in poly-time.

B.2 Extended table of results

The timeout for all our experiments was set to 7200 seconds.

B.2.1 Synthetic PCs

In the following table, the ﬁrst column indicates the benchmark, the second and the third indicate
the closeness parameter ε and η used in the test. The fourth column indicates actual dT V distance
between the two benchmark PCs . The ﬁfth column indicates the test outcome and the sixth repre-
sents the expected outcome. ‘A’ represents Accept and ‘R’ represents Reject and ‘A/R’ represents
that both ‘A’ and ‘R’ are acceptable outputs.

Table 4: The Extended Table

Benchmark

14_4

17_2

14_2

ε

0.9

0.75

0.9

η

0.99

0.99

0.99

Actual dT V
0.773

0.998

0.764

Result

Expected Result

A

R

A

A

R

A

19

18_3

17_4

18_3

17_1

17_0

14_2

14_4

15_4

16_2

14_0

16_3

17_2

15_0

18_0

17_4

18_4

18_2

14_1

16_1

17_0

15_4

17_0

17_0

14_0

16_4

15_1

14_4

14_3

14_2

16_4

17_3

15_1

16_4

14_3

16_1

16_2

15_3

14_3

16_4

18_0

0.75

0.75

0.8

0.8

0.85

0.85

0.85

0.9

0.9

0.75

0.8

0.75

0.8

0.75

0.75

0.75

0.75

0.75

0.85

0.85

0.85

0.8

0.85

0.9

0.75

0.85

0.9

0.8

0.9

0.75

0.9

0.85

0.9

0.8

0.8

0.85

0.75

0.75

0.85

0.8

0.96

0.99

0.99

0.96

0.94

0.94

0.94

0.94

0.99

0.96

0.9

0.96

0.9

0.94

0.96

0.99

0.99

0.99

0.9

0.96

0.94

0.94

0.99

0.94

0.96

0.96

0.96

0.96

0.96

0.9

0.94

0.9

0.96

0.94

0.99

0.96

0.99

0.94

0.96

0.9

0.930

0.941

0.930

0.874

0.968

0.764

0.773

0.941

0.987

0.771

0.879

0.998

0.984

0.994

0.941

0.907

0.918

0.740

0.918

0.968

0.941

0.968

0.968

0.771

0.833

0.927

0.773

0.852

0.764

0.833

0.914

0.927

0.833

0.852

0.918

0.987

0.804

0.852

0.833

0.994

20

R

R

R

R

A

A

A

R

A

A

A

R

R

R

R

R

R

A

R

A

R

A

A

A

A

R

A

A

A

A

A

R

A

A

R

A

R

A

A

R

A/R

A/R

A/R

A/R

A/R

A

A

R

A/R

A/R

A/R

R

R

R

A/R

A/R

A/R

A

R

A/R

R

A/R

A/R

A

A/R

A/R

A

A/R

A

A/R

A/R

R

A

A/R

A/R

A/R

A/R

A/R

A

R

18_4

18_4

18_0

15_1

14_3

16_2

15_0

18_4

17_0

18_1

18_0

17_3

18_3

17_2

15_1

14_3

15_3

17_3

14_3

17_3

14_3

17_0

18_2

17_0

17_1

16_3

14_1

16_3

18_0

15_3

16_4

14_1

16_2

17_1

15_1

15_4

18_2

18_4

18_4

16_3

0.8

0.85

0.9

0.9

0.85

0.75

0.9

0.8

0.75

0.9

0.9

0.8

0.85

0.85

0.75

0.75

0.8

0.85

0.8

0.75

0.9

0.75

0.8

0.8

0.85

0.8

0.85

0.85

0.85

0.9

0.8

0.75

0.8

0.75

0.8

0.8

0.9

0.85

0.85

0.9

0.94

0.99

0.99

0.99

0.96

0.94

0.96

0.96

0.9

0.96

0.96

0.99

0.9

0.94

0.94

0.9

0.99

0.94

0.9

0.99

0.99

0.94

0.99

0.96

0.94

0.94

0.94

0.99

0.96

0.94

0.9

0.96

0.9

0.96

0.9

0.96

0.94

0.94

0.96

0.99

0.907

0.907

0.994

0.927

0.852

0.987

0.984

0.907

0.968

0.993

0.994

0.914

0.930

0.998

0.927

0.852

0.804

0.914

0.852

0.914

0.852

0.968

0.918

0.968

0.874

0.879

0.740

0.879

0.994

0.804

0.833

0.740

0.987

0.874

0.927

0.941

0.918

0.907

0.907

0.879

21

R

A

R

R

A

R

R

R

R

R

R

A

R

R

R

R

R

A

R

R

A

R

R

A

A

A

A

A

R

A

A

A

R

R

R

R

R

R

R

A

A/R

A/R

R

A/R

A/R

R

R

A/R

R

R

R

A/R

R

R

A/R

A/R

A/R

A/R

A/R

A/R

A

R

A/R

A/R

A/R

A/R

A

A/R

R

A

A/R

A

R

A/R

R

A/R

A/R

A/R

A/R

A

14_0

16_0

14_4

16_1

17_1

17_1

16_4

14_1

17_0

14_2

15_0

14_0

14_4

16_3

17_2

15_2

14_4

14_2

16_0

17_2

16_3

14_2

18_1

18_1

18_1

16_4

15_0

15_1

16_0

17_1

15_0

17_4

18_2

14_2

15_0

14_2

14_4

14_1

17_0

14_0

0.75

0.85

0.85

0.8

0.8

0.85

0.9

0.9

0.8

0.75

0.85

0.8

0.75

0.75

0.9

0.85

0.8

0.8

0.8

0.85

0.85

0.75

0.8

0.85

0.9

0.75

0.9

0.9

0.9

0.75

0.8

0.8

0.85

0.75

0.8

0.8

0.8

0.85

0.75

0.85

0.99

0.9

0.99

0.9

0.94

0.99

0.99

0.94

0.9

0.96

0.96

0.96

0.96

0.9

0.94

0.9

0.94

0.94

0.99

0.9

0.96

0.94

0.94

0.99

0.99

0.99

0.99

0.96

0.94

0.9

0.94

0.99

0.9

0.9

0.99

0.9

0.9

0.9

0.99

0.9

0.771

0.954

0.773

0.918

0.874

0.874

0.833

0.740

0.968

0.764

0.984

0.771

0.773

0.879

0.998

0.905

0.773

0.764

0.954

0.998

0.879

0.764

0.993

0.993

0.993

0.833

0.984

0.927

0.954

0.874

0.984

0.941

0.918

0.764

0.984

0.764

0.773

0.740

0.968

0.771

22

A

R

A

R

R

A

A

A

R

A

R

A

A

R

R

A

A

A

R

R

A

A

R

R

R

A

R

R

A

R

R

R

R

A

R

A

A

A

A

A

A/R

R

A

R

A/R

A/R

A

A

R

A/R

R

A

A/R

A/R

R

A/R

A

A

A/R

R

A/R

A/R

R

R

R

A/R

A/R

A/R

A/R

A/R

R

A/R

R

A/R

A/R

A

A

A

A/R

A

17_1

18_1

18_1

18_1

17_3

18_3

16_2

14_0

16_2

16_4

18_1

16_4

15_2

15_0

16_0

15_4

17_0

15_3

18_3

18_3

16_0

17_4

14_3

17_2

17_4

16_2

17_4

16_3

17_1

16_2

15_3

17_2

14_1

14_0

16_2

15_2

14_3

15_3

14_2

16_3

0.75

0.85

0.8

0.75

0.8

0.9

0.8

0.85

0.85

0.8

0.85

0.85

0.9

0.75

0.9

0.8

0.75

0.8

0.9

0.85

0.8

0.85

0.75

0.85

0.8

0.8

0.85

0.8

0.8

0.75

0.85

0.8

0.8

0.85

0.75

0.85

0.85

0.85

0.85

0.8

0.94

0.94

0.99

0.9

0.96

0.96

0.94

0.94

0.99

0.94

0.96

0.99

0.94

0.9

0.99

0.9

0.96

0.96

0.94

0.94

0.96

0.96

0.99

0.96

0.94

0.96

0.99

0.96

0.9

0.96

0.96

0.94

0.96

0.99

0.9

0.94

0.99

0.9

0.99

0.99

0.874

0.993

0.993

0.993

0.914

0.930

0.987

0.771

0.987

0.833

0.993

0.833

0.905

0.984

0.954

0.941

0.968

0.804

0.930

0.930

0.954

0.941

0.852

0.998

0.941

0.987

0.941

0.879

0.874

0.987

0.804

0.998

0.740

0.771

0.987

0.905

0.852

0.804

0.764

0.879

23

R

R

R

R

R

R

A

A

A

A

R

A

A

R

A

R

R

R

R

R

A

R

A

R

R

A

R

A

R

R

A

R

A

A

R

A

A

R

A

A

A/R

R

R

R

A/R

A/R

A/R

A

A/R

A/R

R

A

A/R

R

A/R

R

R

A/R

A/R

A/R

A/R

A/R

A/R

R

R

A/R

A/R

A/R

A/R

R

A

R

A

A

R

A/R

A/R

A/R

A

A/R

17_3

16_0

14_1

18_4

18_0

14_3

18_0

18_2

16_3

15_3

16_4

15_2

16_1

18_2

17_2

15_1

17_3

15_1

15_1

14_1

17_4

18_0

17_1

17_2

18_0

18_0

17_2

18_2

16_1

16_1

18_2

15_1

15_1

16_1

14_3

18_3

15_2

18_0

18_1

18_2

0.85

0.85

0.75

0.8

0.8

0.8

0.85

0.9

0.75

0.9

0.75

0.9

0.9

0.8

0.9

0.8

0.9

0.75

0.8

0.8

0.75

0.75

0.75

0.75

0.8

0.75

0.8

0.9

0.8

0.85

0.85

0.9

0.75

0.9

0.75

0.75

0.8

0.9

0.75

0.85

0.9

0.96

0.94

0.9

0.94

0.99

0.99

0.99

0.99

0.99

0.94

0.96

0.94

0.96

0.96

0.94

0.99

0.96

0.99

0.9

0.94

0.96

0.99

0.94

0.99

0.9

0.96

0.96

0.94

0.99

0.96

0.94

0.9

0.99

0.96

0.94

0.96

0.94

0.99

0.94

0.914

0.954

0.740

0.907

0.994

0.852

0.994

0.918

0.879

0.804

0.833

0.905

0.918

0.918

0.998

0.927

0.914

0.927

0.927

0.740

0.941

0.994

0.874

0.998

0.994

0.994

0.998

0.918

0.918

0.918

0.918

0.927

0.927

0.918

0.852

0.930

0.905

0.994

0.993

0.918

24

A

A

A

R

R

A

R

R

A

A

A

A

R

R

R

R

A

R

R

A

R

R

R

R

R

R

R

R

R

R

R

R

R

R

A

R

A

R

R

R

A/R

A/R

A

R

R

A/R

R

A/R

A/R

A

A/R

A/R

A/R

A/R

R

A/R

A/R

A/R

A/R

A

R

R

A/R

R

R

R

R

A/R

A/R

A/R

A/R

A/R

R

A/R

A/R

A/R

A/R

R

R

A/R

17_3

14_2

15_3

17_2

16_3

14_2

15_2

14_1

16_1

17_4

15_4

16_4

15_0

15_0

16_2

17_0

16_1

14_0

16_2

18_3

18_3

14_2

16_1

18_3

14_4

16_0

18_3

16_2

14_0

15_2

16_1

16_4

15_3

16_2

18_3

17_3

15_2

17_0

15_2

18_4

0.85

0.75

0.85

0.8

0.75

0.85

0.85

0.9

0.75

0.9

0.85

0.8

0.75

0.85

0.8

0.85

0.85

0.75

0.9

0.8

0.8

0.8

0.9

0.85

0.8

0.9

0.9

0.75

0.85

0.9

0.75

0.9

0.9

0.9

0.8

0.9

0.8

0.8

0.75

0.85

0.96

0.99

0.94

0.9

0.96

0.9

0.96

0.96

0.9

0.94

0.9

0.99

0.99

0.9

0.99

0.9

0.96

0.94

0.96

0.9

0.94

0.99

0.96

0.99

0.99

0.96

0.99

0.99

0.96

0.99

0.94

0.94

0.96

0.94

0.96

0.96

0.94

0.99

0.94

0.9

0.914

0.764

0.804

0.998

0.879

0.764

0.905

0.740

0.918

0.941

0.941

0.833

0.984

0.984

0.987

0.968

0.918

0.771

0.987

0.930

0.930

0.764

0.918

0.930

0.773

0.954

0.930

0.987

0.771

0.905

0.918

0.833

0.804

0.987

0.930

0.914

0.905

0.968

0.905

0.907

25

A

A

A

R

A

A

A

A

R

R

R

A

R

R

A

A

R

A

A

R

R

A

R

R

A

A

R

A

A

A

R

A

A

A

R

A

A

A

R

R

A/R

A/R

A

R

A/R

A

A/R

A

R

R

R

A/R

A/R

R

A/R

A/R

A/R

A/R

A/R

R

A/R

A

A/R

A/R

A

A/R

A/R

A/R

A

A/R

A/R

A

A

A/R

A/R

A/R

A/R

A/R

A/R

R

15_4

15_4

14_4

14_0

14_0

18_1

17_3

18_3

17_4

16_0

15_0

16_0

15_4

15_1

18_3

15_0

15_2

15_2

15_2

15_3

18_2

18_4

15_1

18_1

18_0

14_3

16_3

16_1

14_1

15_0

17_2

14_2

14_4

17_3

16_0

14_0

17_1

16_0

14_1

15_1

0.8

0.75

0.75

0.8

0.9

0.75

0.75

0.75

0.85

0.8

0.85

0.85

0.75

0.85

0.85

0.9

0.75

0.85

0.8

0.85

0.75

0.75

0.8

0.9

0.75

0.85

0.85

0.8

0.85

0.85

0.85

0.9

0.9

0.8

0.75

0.9

0.9

0.75

0.8

0.75

0.99

0.94

0.9

0.9

0.99

0.96

0.94

0.9

0.94

0.94

0.99

0.99

0.9

0.99

0.96

0.94

0.9

0.99

0.9

0.99

0.94

0.94

0.96

0.94

0.99

0.9

0.9

0.96

0.99

0.94

0.99

0.94

0.94

0.9

0.96

0.96

0.94

0.9

0.94

0.99

0.941

0.941

0.773

0.771

0.771

0.993

0.914

0.930

0.941

0.954

0.984

0.954

0.941

0.927

0.930

0.984

0.905

0.905

0.905

0.804

0.918

0.907

0.927

0.993

0.994

0.852

0.879

0.918

0.740

0.984

0.998

0.764

0.773

0.914

0.954

0.771

0.874

0.954

0.740

0.927

26

R

R

A

A

A

R

R

R

R

A

R

A

R

R

R

R

R

A

A

A

R

R

R

R

R

A

A

R

A

R

R

A

A

R

R

A

A

R

A

R

A/R

R

A/R

A

A

R

A/R

R

R

A/R

A/R

A/R

R

A/R

A/R

R

R

A/R

A/R

A

A/R

A/R

A/R

R

R

A/R

A/R

A/R

A

R

R

A

A

R

A/R

A

A

R

A

A/R

17_1

18_2

18_2

14_1

18_2

18_4

16_1

14_1

16_0

15_4

17_2

16_3

18_0

17_4

15_3

17_1

18_1

15_3

14_1

17_1

15_3

18_4

14_1

18_2

18_4

18_4

18_2

17_4

14_3

18_4

17_3

17_4

15_3

16_0

17_3

15_3

18_1

16_1

16_3

18_4

0.85

0.8

0.8

0.8

0.85

0.9

0.75

0.85

0.75

0.9

0.75

0.9

0.8

0.75

0.8

0.8

0.85

0.75

0.75

0.9

0.75

0.75

0.9

0.75

0.8

0.75

0.75

0.8

0.85

0.9

0.75

0.9

0.75

0.8

0.75

0.8

0.75

0.85

0.85

0.9

0.9

0.9

0.94

0.99

0.99

0.99

0.99

0.96

0.94

0.96

0.9

0.94

0.96

0.9

0.94

0.99

0.9

0.94

0.9

0.99

0.96

0.96

0.99

0.96

0.99

0.9

0.9

0.96

0.94

0.96

0.9

0.96

0.9

0.9

0.96

0.9

0.94

0.94

0.94

0.94

0.874

0.918

0.918

0.740

0.918

0.907

0.918

0.740

0.954

0.941

0.998

0.879

0.994

0.941

0.804

0.874

0.993

0.804

0.740

0.874

0.804

0.907

0.740

0.918

0.907

0.907

0.918

0.941

0.852

0.907

0.914

0.941

0.804

0.954

0.914

0.804

0.993

0.918

0.879

0.907

27

R

R

R

A

R

A

R

A

R

R

R

A

R

R

R

R

R

R

A

A

R

R

A

R

R

R

R

R

A

A

R

R

R

R

R

R

R

R

A

A

A/R

R

A/R

A

A/R

A/R

A/R

A

R

A/R

R

A

R

R

A/R

A/R

R

A/R

A

A

A/R

A/R

A

A/R

A/R

R

R

A/R

A/R

A/R

R

A/R

A/R

R

A/R

A/R

R

A/R

A/R

A/R

15_0

16_0

14_4

18_3

17_0

18_0

17_1

16_4

16_2

17_1

14_4

16_4

17_3

15_1

17_3

14_3

17_4

16_1

15_4

14_2

14_3

17_0

14_4

16_3

15_4

14_0

17_4

15_2

14_4

17_4

18_1

15_4

14_0

17_2

14_0

14_4

16_0

14_2

15_4

16_3

0.8

0.85

0.85

0.75

0.9

0.85

0.9

0.8

0.85

0.85

0.75

0.85

0.8

0.85

0.85

0.9

0.8

0.75

0.85

0.8

0.9

0.9

0.8

0.9

0.75

0.8

0.85

0.75

0.75

0.9

0.8

0.85

0.8

0.9

0.75

0.85

0.75

0.85

0.9

0.75

0.96

0.94

0.9

0.99

0.96

0.94

0.96

0.96

0.9

0.96

0.99

0.9

0.94

0.94

0.99

0.94

0.9

0.96

0.96

0.96

0.96

0.94

0.96

0.96

0.99

0.94

0.9

0.99

0.94

0.99

0.9

0.99

0.99

0.99

0.9

0.96

0.99

0.96

0.99

0.94

0.984

0.954

0.773

0.930

0.968

0.994

0.874

0.833

0.987

0.874

0.773

0.833

0.914

0.927

0.914

0.852

0.941

0.918

0.941

0.764

0.852

0.968

0.773

0.879

0.941

0.771

0.941

0.905

0.773

0.941

0.993

0.941

0.771

0.998

0.771

0.773

0.954

0.764

0.941

0.879

28

R

A

A

R

A

R

A

A

A

A

A

A

R

R

A

A

R

R

R

A

A

A

A

A

R

A

R

A

A

A

R

R

A

R

A

A

R

A

A

R

R

A/R

A

A/R

A/R

R

A

A/R

A/R

A/R

A/R

A

A/R

A/R

A/R

A

R

A/R

A/R

A

A

A/R

A

A

A/R

A

R

A/R

A/R

A/R

R

A/R

A

R

A/R

A

A/R

A

A/R

A/R

16_4

18_1

15_0

16_2

15_4

17_2

18_0

15_0

15_4

17_0

15_2

15_2

0.85

0.8

0.75

0.85

0.8

0.8

0.85

0.75

0.75

0.9

0.8

0.75

0.94

0.96

0.96

0.94

0.94

0.99

0.9

0.94

0.96

0.99

0.99

0.96

0.833

0.993

0.984

0.987

0.941

0.998

0.994

0.984

0.941

0.968

0.905

0.905

A

R

R

A

R

R

R

R

R

A

A

R

A

R

R

A/R

R

R

R

R

A/R

A/R

A/R

A/R

B.2.2 Real-world PCs

In the following table, the ﬁrst column indicates the benchmark, the second indicates the time re-
quired for the test, and the third column indicates the test outcome. ‘A’ represents Accept and ‘R’
represents Reject.

Table 5: The Extended Table

Benchmark

or-70-10-8-UC-10_0
or-70-10-8-UC-10_1
or-70-10-8-UC-10_2
or-70-10-8-UC-10_3
or-70-10-8-UC-10_4
or-70-10-8-UC-10_5
or-70-10-8-UC-10_6
or-70-10-8-UC-10_7
or-70-10-8-UC-10_8
or-70-10-8-UC-10_9
s641_15_7_0
s641_15_7_1
s641_15_7_2
s641_15_7_3
s641_15_7_4
s641_15_7_5
s641_15_7_6
s641_15_7_7
s641_15_7_8
s641_15_7_9
or-50-5-4_0
or-50-5-4_1
or-50-5-4_2
or-50-5-4_3
or-50-5-4_4
or-50-5-4_5
or-50-5-4_6
or-50-5-4_7
or-50-5-4_8
or-50-5-4_9
ProjectService3.sk_12_55_0

Teq(s)

23.2
22.72
22.92
22.87
22.78
23.06
22.99
22.93
22.82
22.82
33.66
33.4
33.45
33.32
33.51
33.21
33.46
33.23
33.61
33.51
414.17
414.84
410.16
414.15
410.07
412.27
414.77
415.19
416.84
408.59
356.58

29

Result

A
R
R
R
R
R
R
R
R
R
A
R
R
R
R
R
R
R
R
R
A
R
R
R
R
R
R
R
R
R
A

ProjectService3.sk_12_55_1
ProjectService3.sk_12_55_2
ProjectService3.sk_12_55_3
ProjectService3.sk_12_55_4
ProjectService3.sk_12_55_5
ProjectService3.sk_12_55_6
ProjectService3.sk_12_55_7
ProjectService3.sk_12_55_8
ProjectService3.sk_12_55_9
s713_15_7_0
s713_15_7_1
s713_15_7_2
s713_15_7_3
s713_15_7_4
s713_15_7_5
s713_15_7_6
s713_15_7_7
s713_15_7_8
s713_15_7_9
or-100-10-2-UC-30_0
or-100-10-2-UC-30_1
or-100-10-2-UC-30_2
or-100-10-2-UC-30_3
or-100-10-2-UC-30_4
or-100-10-2-UC-30_5
or-100-10-2-UC-30_6
or-100-10-2-UC-30_7
or-100-10-2-UC-30_8
or-100-10-2-UC-30_9
s1423a_3_2_0
s1423a_3_2_1
s1423a_3_2_2
s1423a_3_2_3
s1423a_3_2_4
s1423a_3_2_5
s1423a_3_2_6
s1423a_3_2_7
s1423a_3_2_8
s1423a_3_2_9
s1423a_7_4_0
s1423a_7_4_1
s1423a_7_4_2
s1423a_7_4_3
s1423a_7_4_4
s1423a_7_4_5
s1423a_7_4_6
s1423a_7_4_7
s1423a_7_4_8
s1423a_7_4_9
or-50-5-10_0
or-50-5-10_1
or-50-5-10_2
or-50-5-10_3
or-50-5-10_4
or-50-5-10_5
or-50-5-10_6
or-50-5-10_7
or-50-5-10_8
or-50-5-10_9

353.77
355.93
356.11
356.15
355.64
357.89
356.69
353.36
356.14
24.56
24.68
24.28
24.47
24.65
24.32
24.4
24.39
24.86
24.41
31.11
31.16
31.04
31.13
31.14
31.04
31.03
31.13
31.17
31.0
153.8
152.37
152.01
150.96
152.64
153.13
151.52
152.53
152.4
152.81
104.28
103.4
103.82
104.18
103.95
103.59
104.31
104.93
104.93
103.51
282.09
282.49
279.63
281.8
280.69
279.91
283.05
282.69
279.65
282.97

30

R
R
R
A
R
R
R
R
R
R
R
R
R
R
R
R
R
A
R
R
R
R
R
R
A
R
R
R
R
R
R
R
R
R
A
R
R
R
R
R
R
R
R
R
R
R
R
A
R
R
R
R
R
R
R
A
R
R
R

or-60-20-6-UC-20_0
or-60-20-6-UC-20_1
or-60-20-6-UC-20_2
or-60-20-6-UC-20_3
or-60-20-6-UC-20_4
or-60-20-6-UC-20_5
or-60-20-6-UC-20_6
or-60-20-6-UC-20_7
or-60-20-6-UC-20_8
or-60-20-6-UC-20_9

359.89
362.3
363.1
363.11
362.76
358.76
363.32
358.41
358.8
362.8

R
R
R
R
R
R
A
R
R
R

31

