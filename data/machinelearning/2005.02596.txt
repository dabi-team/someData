What You Must Remember When Transforming
Datawords
M. Praveen
Chennai Mathematical Institute, India
UMI ReLaX, Indo-French joint research unit

Abstract

Streaming Data String Transducers (SDSTs) were introduced to model a class of imperative and a
class of functional programs, manipulating lists of data items. These can be used to write commonly
used routines such as insert, delete and reverse. SDSTs can handle data values from a potentially
inﬁnite data domain. The model of Streaming String Transducers (SSTs) is the fragment of SDSTs
where the inﬁnite data domain is dropped and only ﬁnite alphabets are considered. SSTs have been
much studied from a language theoretical point of view. We introduce data back into SSTs, just
like data was introduced to ﬁnite state automata to get register automata. The result is Streaming
String Register Transducers (SSRTs), which is a subclass of SDSTs.

We use origin semantics for SSRTs and give a machine independent characterization, along
the lines of Myhill-Nerode theorem. Machine independent characterizations for similar models
are the basis of learning algorithms and enable us to understand fragments of the models. Origin
semantics of transducers track which positions of the output originate from which positions of the
input. Although a restriction, using origin semantics is well justiﬁed and is known to simplify many
problems related to transducers. We use origin semantics as a technical building block, in addition
to characterizations of deterministic register automata. However, we need to build more on top of
these to overcome some challenges unique to SSRTs.

2012 ACM Subject Classiﬁcation Theory of computation → Transducers; Theory of computation
→ Automata over inﬁnite objects

Keywords and phrases Streaming String Transducers, Data words, Machine independent character-
ization

Funding M. Praveen: Partially supported by a grant from the Infosys foundation.

Acknowledgements The author thanks C. Aiswarya, Kamal Lodaya, K. Narayan Kumar and
anonymous reviewers for suggestions to improve the presentation and pointers to related works.

1

Introduction

Transductions are in general relations among words. Transducers are theoretical models that
implement transductions. Transducers are used in a variety of applications, such as analysis
of web sanitization frameworks, host based intrusion detection, natural language processing,
modeling some classes of programming languages and constructing programming language
tools like evaluators, type checkers and translators. Streaming Data String Transducers
(SDSTs) were introduced in [2] to model a class of imperative and a class of functional
programs, manipulating lists of data items. Transducers have been used in [16] to infer
semantic interfaces of data structures such as stacks. Such applications use Angluin style
learning, which involves constructing transducers by looking at example operations of the
object under study. Since the transducer is still under construction, we need to make
inferences about the transduction without having access to a transducer which implements it.
Theoretical bases for doing this are machine independent characterizations, which identify
what kind of transductions can be implemented by what kind of transducers and give a
template for constructing transducers. Indeed the seminal Myhill-Nerode theorem gives a
machine independent characterization for regular languages over ﬁnite alphabets, which form

0
2
0
2
c
e
D
4
1

]
L
F
.
s
c
[

2
v
6
9
5
2
0
.
5
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

What You Must Remember When Transforming Datawords

the basis of Angluin style learning of regular languages [3]. A similar characterization for a
fragment of SDSTs is given in [5] and is used as a basis to design a learning algorithm.

Programs deal with data from an inﬁnite domain and transducers modeling the programs
should also treat data as such. For example in [16], the state space reduced from 109 to 800
and the number of learning queries reduced from billions to 4000 by switching to a transducer
model that can deal with data from an inﬁnite domain. We give a machine independent
characterization for a fragment of SDSTs more powerful than those in [16, 5]. The additional
power comes from signiﬁcant conceptual diﬀerences. The transducers used in [16] produce
the output in a linear fashion without remembering what was output before. For example,
they cannot output the reverse of the input strings, which can be done by our model. The
model studied in [5] are called Streaming String Transducers (SSTs), the fragment obtained
from SDSTs by dropping the ability to deal with data values from an inﬁnite domain. We
retain this ability in our model, called Streaming String Register Transducers (SSRTs). It is
obtained from SDSTs by dropping the ability to deal with linear orders in the data domain.
Apart from Angluin style learning algorithms, machine independent characterizations are
also useful for studying fragments of transducer models. E.g. in [5], machine independent
characterization of SSTs is used to study fragments such as non-deterministic automata with
output and transductions deﬁnable in First Order logic.

We use origin semantics of transducers, which are used in [5] to take into account how
positions of the output originate from the positions of the input. Using origin semantics is
known to ease some of the problems related to transducers, e.g., [7]. Origin semantics is a
restriction, but a reasonable one and is used extensively in this paper.

Contributions

Machine independent characterizations are known for automata over data values from an
inﬁnite domain [15, 4] and for streaming transducers over ﬁnite alphabets [5], but not for
streaming transducers over data values, which is what we develop here. This involves both
conceptual and technical challenges. In [15, 4], data values that must be remembered by an
automaton while reading a word from left to right are identiﬁed using a machine independent
deﬁnition. We lift this to transducers and identify that the concept of factored outputs
from [5] is necessary for this. Factored outputs can let us ignore some parts of transduction
outputs, which is necessary to deﬁne when two words behave similarly. However, [5] does
not deal with data values from an inﬁnite domain and it takes quite a bit of manipulation
with permutations on data values to make ideas from there work here. In transductions,
suﬃxes can inﬂuence how preﬁxes are transformed. This is elegantly handled in [5] using two
way transducer models known to be equivalent to SSTs. There are no such models known
when data values are present. To handle it in a one way transducer model, we introduce
data structures based on trees that keep track of all possible suﬃxes. This does raise the
question of whether there are interesting two way transducer models with data values. Recent
work [6] has made progress in this direction, which we discuss at the end of this article.
We concentrate here on SDSTs and its fragments, which are known to be equivalent to
classes of imperative and functional programming languages. In [2], it is explained in detail
which features of programming languages correspond to which features of the transducer.
Over ﬁnite alphabets, streaming string transducers are expressively equivalent to regular
transductions, which are also deﬁned by two way deterministic ﬁnite-state transducers and
by monadic second order logic [1].

M. Praveen

Related Works

3

Studying transducer models capable of handling data values from an inﬁnite domain is an
active area of research [13, 14]. Streaming transducers like SDSTs have the distinctive feature
of using variables to store intermediate values while computing transductions; this idea
appears in an earlier work [11] that introduced simple programs on strings, which implement
the same class of transductions as those implemented by SSTs. An Angluin style learning
algorithm for deterministic automata with memory is given in [17]. A machine independent
characterization of automata with ﬁnite memory is given in [8], which is further extended
to data domains with arbitrary binary relations in [9]. The learning algorithm of [17] is
extended to Mealy machines with data in [16]. However, Mealy machines are not as powerful
as SSRTs that we consider here. Using a more abstract approach of nominal automata, [19]
presents a learning algorithm for automata over inﬁnite alphabets. Logical characterizations
of transducers that can handle data are considered in [12]. However, the transducers in
that paper cannot use data values to make decisions, although they are part of the output.
Register automata with linear arithmetic introduced in [10] shares some of the features of the
transducer model used here. Here, data words stored in variables can be concatenated, while
in register automata with linear arithmetic, numbers stored in variables can be operated
upon by linear operators.

Proofs of some of the results in this paper are tedious and are moved to the appendix
to maintain ﬂow of ideas in the main paper. Proofs of results stated in the main part of
the paper are in Sections B, C and D. Section A states and proves some basic properties
of transductions and transducers that are only invoked in Sections B, C and D. Section E
contains proofs that are especially long. They consist of lengthy case analyses to rigorously
verify facts that are intuitively clear.

2

Preliminaries

Let I be the set of integers, N be the set of non-negative integers and D be an inﬁnite set of
data values. We will refer to D as the data domain. For i, j ∈ I, we denote by [i, j] the set
{k | i ≤ k ≤ j}. For any set S, S∗ denotes the set of all ﬁnite sequences of elements from S.
The empty sequence is denoted by (cid:15). Given u, v ∈ S∗, v is a preﬁx (resp. suﬃx) of u if there
exists w ∈ S∗ such that u = vw (resp. u = wv). The sequence v is an inﬁx of u if there are
sequences w1, w2 such that u = w1vw2.

Let Σ, Γ be ﬁnite alphabets. We will use Σ for input alphabet and Γ for output alphabet. A
data word over Σ is a word in (Σ × D)∗. A data word with origin information over Γ is a word
in (Γ × D × N)∗. Suppose Σ = {title, ﬁrstName, lastName} and Γ = {givenName, surName}.
An example data word over Σ is (title, Mr.)(ﬁrstName, Harry)(lastName, Tom). If we were to
give this as input to a device that reverses the order of names, the output would be the data
word with origin information (surName, Tom, 3)(givenName, Harry, 2), over Γ. In the triple
(givenName, Harry, 2), the third component 2 indicates that the pair (givenName, Harry)
originates from the second position of the input data word. We call the third component
origin and it indicates the position in the input that is responsible for producing the output
triple. If a transduction is being implemented by a transducer, the origin of an output
position is the position of the input that the transducer was reading when it produced the
output. The data value at some position of the output may come from any position (not
necessarily the origin) of the input data word. We write transduction for any function from
data words over Σ to data words with origin information over Γ.

For a data word w, |w| is its length. For a position i ∈ [1, |w|], we denote by data(w, i)

4

What You Must Remember When Transforming Datawords

(resp. letter(w, i)) the data value (resp. the letter from the ﬁnite alphabet) at the ith
position of w. We denote by data(w, ∗) the set of all data values that appear in w. For
positions i ≤ j, we denote by w[i, j] the inﬁx of w starting at position i and ending at
position j. Note that w[1, |w|] = w. Two data words w1, w2 are isomorphic (denoted by
w1 ’ w2) if |w1| = |w2|, letter(w1, i) = letter(w2, i) and data(w1, i) = data(w1, j) iﬀ
data(w2, i) = data(w2, j) for all positions i, j ∈ [1, |w1|]. For data values d, d0, we denote
by w[d/d0] the data word obtained from w by replacing all occurrences of d by d0. We say
that d0 is a safe replacement for d in w if w[d/d0] ’ w. Intuitively, replacing d by d0 doesn’t
introduce new equalities/inequalities among the positions of w. For example, d1 is a safe
replacement for d2 in (a, d3)(b, d2), but not in (a, d1)(b, d2).

A permutation on data values is any bijection π : D → D. For a data word u, π(u)
is obtained from u by replacing all its data values by their respective images under π. A
transduction f is invariant under permutations if for every data word u and every permutation
π, f (π(u)) = π(f (u)) (permutation can be applied before or after the transduction).

Suppose a transduction f has the property that for any triple (γ, d, o) in any output
f (w), there is a position i ≤ o in w such that data(w, i) = d. If the data value d is output
from the origin o, then d should have already occurred in the input on or before o. Such
transductions are said to be without data peeking. We say that a transduction has linear
blow up if there is a constant K such that for any position o of any input, there are at most
K positions in the output whose origin is o.

Streaming String Register Transducers

We present an extension of SSTs to handle data values, just like ﬁnite state automata were
extended to ﬁnite memory automata [18]. Our model is a subclass of SDSTs, which can store
intermediate values (which can be long words) in variables. E.g., reversing an input word can
be achieved as follows: as each input symbol is read, concatenate it to the back of a variable
maintained for this purpose. At the end, the variable will have the reverse of the input.
There are also registers in these models, which can store single data values. Transitions can
be enabled/disabled based on whether the currently read data value is equal/unequal to the
one stored in one of the registers.

(cid:73) Deﬁnition 1. A Streaming String Register Transducer (SSRT) is an eight tuple S =
(Σ, Γ, Q, q0, R, X, O, ∆), where

the ﬁnite alphabets Σ, Γ are used for input, output respectively,
Q is a ﬁnite set of states, q0 is the initial state,
R is a ﬁnite set of registers and X is a ﬁnite set of data word variables,
O : Q * ((Γ × ˆR) ∪ X)∗ is a partial output function, where ˆR = R ∪ {curr}, with curr
being a special symbol used to denote the current data value being read and
∆ ⊆ (Q × Σ × Φ × Q × 2R × U ) is a ﬁnite set of transitions. The set Φ consists of all
Boolean combinations of atomic constraints of the form r= or r6= for r ∈ R. The set U is
the set of all functions from the set X of data word variables to ((Γ × ˆR) ∪ X)∗.

It is required that

For every q ∈ Q and x ∈ X, there is at most one occurrence of x in O(q) and
for every transition (q, σ, φ, q0, R0, ud) and for every x ∈ X, x appears at most once in
the set {ud(y) | y ∈ X}.

We say that the last two conditions above enforce a SSRT to be copyless, since it prevents
multiple copies of contents being made.

M. Praveen

5

A valuation val for a transducer S is a partial function over registers and data word
variables such that for every register r ∈ R, either val(r) is undeﬁned or is a data value in
D, and for every data word variable x ∈ X, val(x) is a data word with origin information
over Γ. The valuation val and data value d satisﬁes the atomic constraint r= (resp. r6=) if
val(r) is deﬁned and d = val(r) (resp. undeﬁned or d 6= val(r)). Satisfaction is extended to
Boolean combinations in the standard way. We say that a SSRT is deterministic if for every
two transitions (q, σ, φ, q0, R0, u) and (q, σ, φ0, q00, R00, u0) with the same source state q and
input symbol σ, the formulas φ and φ0 are mutually exclusive (i.e., φ ∧ φ0 is unsatisﬁable).
We consider only deterministic SSRTs here.

A conﬁguration is a triple (q, val, i) where q ∈ Q is a state, val is a valuation and i is the
number of symbols read so far. The transducer starts in the conﬁguration (q0, val (cid:15), 0) where
q0 is the initial state and val (cid:15) is the valuation such that val (cid:15)(r) is undeﬁned for every register
r ∈ R and val (cid:15)(x) = (cid:15) for every data word variable x ∈ X. From a conﬁguration (q, val, i),
the transducer can read a pair (σ, d) ∈ Σ × D and go to the conﬁguration (q0, val 0, i + 1) if
there is a transition (q, σ, φ, q0, R0, ud) and 1) d and val satisﬁes φ and 2) val 0 is obtained
from val by assigning d to all the registers in R0 and for every x ∈ X, setting val 0(x) to
ud(x)[y 7→ val(y), (γ, curr) 7→ (γ, d, i + 1), (γ, r) 7→ (γ, val(r), i + 1)] (in ud(x), replace every
occurrence of y by val(y) for every data word variable y ∈ X, replace every occurrence of
(γ, curr) by (γ, d, i + 1) for every output letter γ ∈ Γ and replace every occurrence of (γ, r) by
(γ, val(r), i + 1) for every output letter γ ∈ Γ and every register r ∈ R). After reading a data
word w, if the transducer reaches some conﬁguration (q, val, n) and O(q) is not deﬁned, then
(w) is undeﬁned for the input w. Otherwise, the transducer’s
the transducer’s output
(cid:74)
output is deﬁned as
(w) = O(q)[y 7→ val(y), (γ, curr) 7→ (γ, d, n), (γ, r) 7→ (γ, val(r), n)],
S
where d is the last data value in w.

S

(cid:75)

(cid:75)

(cid:74)

Intuitively, the transition (q, σ, φ, q0, R0, ud) checks that the current valuation val and the
data value d being read satisﬁes φ, goes to the state q0, stores d into the registers in R0 and
updates data word variables according to the update function ud. The condition that x
appears at most once in the set {ud(y) | y ∈ X} ensures that the contents of any data word
variable are not duplicated into more than one variable. This ensures, among other things,
that the length of the output is linear in the length of the input. The condition that for every
two transitions (q, σ, φ, q0, R0, ud) and (q, σ, φ0, q00, R00, ud 0) with the same source state and
input symbol, the formulas φ and φ0 are mutually exclusive ensures that the transducer cannot
reach multiple conﬁgurations after reading a data word (i.e., the transducer is deterministic).

(cid:73) Example 2. Consider the transduction that is the identity on inputs in which the ﬁrst
and last data values are equal. On the remaining inputs, the output is the reverse of the
input. This can be implemented by a SSRT using two data word variables. As each input
symbol is read, it is appended to the front of the ﬁrst variable and to the back of the second
variable. The ﬁrst variable stores the input and the second one stores the reverse. At the
end, either the ﬁrst or the second variable is output, depending on whether the last data
value is equal or unequal to the ﬁrst data value (which is stored in a register).

In Section 3, we deﬁne an equivalence relation on data words and state our main result in
terms of the ﬁniteness of the index of the equivalence relation and a few other properties. In
Section 4, we prove that transductions satisfying certain properties can be implemented by
SSRTs (the backward direction of the main result) and we prove the converse in Section 5.

6

What You Must Remember When Transforming Datawords

3 How Preﬁxes and Suﬃxes Inﬂuence Each Other

As is usual in many machine independent characterizations (like the classic Myhill-Nerode
theorem for regular languages), we deﬁne an equivalence relation on the set of data words to
identify similar ones. If the equivalence relation has ﬁnite index, it can be used to construct
ﬁnite state models. We start by looking at what “similar data words” mean in the context of
transductions.

Suppose L is the set of all even length words over some ﬁnite alphabet. The words a
and aaa do the same thing to any suﬃx v: a · v ∈ L iﬀ aaa · v ∈ L. So, a and aaa are
identiﬁed to be similar with respect to L in the classic machine independent characterization.
Instead of a language L, suppose we have a transduction f and we are trying to identify
words u1, u2 that do the same thing to any suﬃx v. The naive approach would be to check
if f (u1 · v) = f (u2 · v), but this does not work. Suppose a transduction f is such that
f (a · b) = (a, 1)(b, 2), f (aaa · b) = (a, 1)(a, 2)(a, 3) · (b, 4) and f (c · b) = (c, 1)(b, 2)(b, 2) (we
have ignored data values in this transduction). The words a and aaa do the same thing to
the suﬃx b (the suﬃx is copied as it is to the output), as opposed to c (which copies the
suﬃx twice to the output). But f (a · b) 6= f (aaa · b). The problem is that we are not only
comparing what a and aaa do to the suﬃx b, but also comparing what they do to themselves.
We want to indicate in some way that we want to ignore the parts of the output that come
from a or aaa: f (a | v) = left · (b, 2) and f (aaa | b) = left · (b, 4). We have underlined
a and aaa on the input side to indicate that we want to ignore them; we have replaced
a and aaa in the output by left to indicate that they are coming from ignored parts of
the input. This has been formalized as factored outputs in [5]. This is still not enough
for our purpose, since the outputs (b, 2) and (b, 4) indicate that a and aaa have diﬀerent
lengths. This can be resolved by oﬀsetting one of the outputs by the diﬀerence in the lengths:
f (a | v) = left · (b, 2) = f−2(aaa | b). The subscript −2 in f−2(aaa | b) indicates that we
want to oﬀset the origins by −2. We have formalized this in the deﬁnition below, in which
we have borrowed the basic deﬁnition from [5] and added data values and oﬀsets.

(cid:73) Deﬁnition 3 (Oﬀset factored outputs). Suppose f is a transduction and uvw is a data word
over Σ. For a triple (γ, d, o) in f (uvw), the abstract origin abs(o) of o is left (resp. middle,
right) if o is in u (resp. v, w). The factored output f (u | v | w) is obtained from f (uvw) by
ﬁrst replacing every triple (γ, d, o) by (∗, ∗, abs(o)) if abs(o) = left (the other triples are
retained without change). Then all consecutive occurrences of (∗, ∗, left) are replaced by a
single triple (∗, ∗, left) to get f (u | v | w). Similarly we get f (u | v | w) and f (u | v | w)
by using (∗, ∗, middle) and (∗, ∗, right) respectively. We get f (u | v) and f (u | v) similarly,
except that there is no middle part. For an integer z, we obtain fz(u | v) by replacing every
triple (γ, d, o) by (γ, d, o + z) (triples (∗, ∗, left) are retained without change).

Let w = (a, d1)(a, d2)(b, d3)(c, d4) and f be the transduction in Example 2. Then f (w) =
(c, d4, 4)(b, d3, 3)(a, d2, 2)(a, d1, 1) (assuming d4 6= d1). The factored output f ((a, d1)(a, d2) |
(b, d3) | (c, d4)) is (c, d4, 4)(b, d3, 3)(∗, ∗, left).

It is tempting to say that two data words u1 and u2 are equivalent if for all v, f (u1 | v) =
fz(u2 | v), where z = |u1| − |u2|. But this does not work; continuing with the transduction
f from Example 2, no two data words from the inﬁnite set {(a, di) | i ≥ 1} would be
equivalent: f ((a, di) | (a, di)) 6= f ((a, dj) | (a, di)) for i 6= j. To get an equivalence relation
with ﬁnite index, we need to realize that the important thing is not the ﬁrst data value, but
its (dis)equality with the last one. So we can say that for every i, there is a permutation πi
on data values mapping di to d1 such that f (πi(a, di) | v) = f ((a, d1) | v). This will get us
an equivalence relation with ﬁnite index but it is not enough, since the transducer model we

M. Praveen

7

build must satisfy another property: it must use only ﬁnitely many registers to remember
data values. Next we examine which data values must be remembered.

Suppose L is the set of all data words in which the ﬁrst and last data values are equal.
Suppose a device is reading the word d1d2d3d1 from left to right and trying to determine
whether the word belongs to L (we are ignoring letters from the ﬁnite alphabet here). The
device must remember d1 when it is read ﬁrst, so that it can be compared to the last data
value. A machine independent characterization of what must be remembered is given in
[4, Deﬁnition 2]; it says that the ﬁrst occurrence of d1 in d1d2d3d1 is L-memorable because
replacing it with some fresh data value d4 (which doesn’t occur in the word) makes a
diﬀerence: d1d2d3d1 ∈ L but d4d2d3d1 /∈ L. We adapt this concept to transductions, by
suitably modifying the deﬁnition of “making a diﬀerence”.

(cid:73) Deﬁnition 4 (memorable values). Suppose f is a transduction. A data value d is f -
memorable in a data word u if there exists a data word v and a safe replacement d0 for d in
u such that f (u[d/d0] | v) 6= f (u | v).

Let f be the transduction of Example 2 and d1, d2, d3, d0
f (d1d2d3 | d1) = (∗, ∗, left)(d1, 4) and f (d0
f -memorable in d1d2d3.

1 be distinct data values. We have
1d2d3 | d1) = (d1, 4)(∗, ∗, left). Hence, d1 is

1 be distinct data values. We have f (d1d2d3d4 | v) = (cid:15) = f (d0

We have to consider one more phenomenon in transductions. Consider the transduction
f whose output is (cid:15) for inputs of length less than ﬁve. For other inputs, the output is
the third (resp. fourth) data value if the ﬁrst and ﬁfth are equal (resp. unequal). Let
d1, d2, d3, d4, d5, d0
1d2d3d4 | v)
if v = (cid:15) and f (d1d2d3d4 | v) = (∗, ∗, left) = f (d0
1d2d3d4 | v) otherwise. Hence, d1 is not
f -memorable in d1d2d3d4. However, any device implementing f must remember d1 after
reading d1d2d3d4, so that it can be compared to the ﬁfth data value. Replacing d1 by d0
1 does
make a diﬀerence but we cannot detect it by comparing f (d1d2d3d4 | v) and f (d0
1d2d3d4 | v).
We can detect it as follows: f (d1d2d3d4 | d1) = (d3, 3) 6= (d4, 4) = f (d1d2d3d4 | d5). Changing
the suﬃx from d1 to d5 inﬂuences how the preﬁx d1d2d3d4 is transformed (in transductions,
preﬁxes are vulnerable to the inﬂuence of suﬃxes). The value d1 is also contained in the
preﬁx d1d2, but f (d1d2 | v) = f (d1d2 | v[d1/d5]) for all v. To detect that d1d2 is vulnerable,
we ﬁrst need to append d3d4 to d1d2 and then have a suﬃx in which we substitute d1 with
something else. We formalize this in the deﬁnition below; it can be related to the example
above by setting u = d1d2, u0 = d3d4 and v = d1.
(cid:73) Deﬁnition 5 (vulnerable values). A data value d is f -vulnerable in a data word u if there
exist data words u0, v and a data value d0 such that d does not occur in u0, d0 is a safe
replacement for d in u · u0 · v and f (u · u0 | v[d/d0]) 6= f (u · u0 | v).

Consider the transduction f deﬁned as f (u) = f1(u) · f2(u); for i ∈ [1, 2], fi reverses its
input if the ith and last data values are distinct. On other inputs, fi is the identity (f1 is
the transduction given in Example 2). In the two words d1d2d3d1d2d3 and d1d2d3d2d1d3, d1
and d2 are f -memorable. For every data word v, f (d1d2d3d1d2d3 | v) = f (d1d2d3d2d1d3 | v),
so it is tempting to say that the two words are equivalent. But after reading d1d2d3d1d2d3, a
transducer would remember that d2 is the latest f -memorable value it has seen. After reading
d1d2d3d2d1d3, the transducer would remember that d1 is the latest f -memorable value it has
seen. Diﬀerent f -memorable values play diﬀerent roles and one way to distinguish which
is which is to remember the order in which they occurred last. So we distinguish between
d1d2d3d1d2d3 and d1d2d3d2d1d3. Suppose d2, d1 are two data values in some data word u.
We say that d1 is fresher than d2 in u if the last occurrence of d1 in u is to the right of the
last occurrence of d2 in u.

8

What You Must Remember When Transforming Datawords

(cid:73) Deﬁnition 6. Suppose f is a transduction and u is a data word. We say that a data value
d is f -inﬂuencing in u if it is either f -memorable or f -vulnerable in u. We denote by iflf (u)
the sequence dm · · · d1, where {dm, . . . , d1} is the set of all f -inﬂuencing values in u and for
all i ∈ [1, m − 1], di is fresher than di+1 in u. We call di the ith f -inﬂuencing data value in u.
If a data value d is both f -vulnerable and f -memorable in u, we say that d is of type vm. If
d is f -memorable but not f -vulnerable (resp. f -vulnerable but not f -memorable) in u, we say
that d is of type m (resp. v). We denote by aiflf (u) the sequence (dm, t(dm)) · · · (d1, t(d1)),
where t(di) is the type of di for all i ∈ [1, m].

To consider two data words u1 and u2 to be equivalent, we can insist that aiflf (u1) =
aiflf (u2). But as before, this may result in some inﬁnite set of pairwise non-equivalent data
words. We will relax the condition by saying that there must be a permutation π on data
values such that aiflf (π(u2)) = aiflf (u1). This is still not enough; we have overlooked one
more thing that must be considered in such an equivalence. Recall that in transductions,
preﬁxes are vulnerable to the inﬂuence of suﬃxes. So if u1 is vulnerable to changing the
suﬃx from v1 to v2, then π(u2) must also have the same vulnerability. This is covered by
the third condition in the deﬁnition below.

(cid:73) Deﬁnition 7. For a transduction f , we deﬁne the relation ≡f on data words as u1 ≡f u2
if there exists a permutation π on data values satisfying the following conditions:

λv.fz(π(u2) | v) = λv.f (u1 | v), where z = |u1| − |u2|,
aiflf (π(u2)) = aiflf (u1) and
for all u, v1, v2, f (u1 · u | v1) = f (u1 · u | v2) iﬀ f (π(u2) · u | v1) = f (π(u2) · u | v2).
As in the standard lambda calculus notation, λv.fz(u | v) denotes the function that maps
each input v to fz(u | v). It is routine to verify that for any data word u and permutation π,
π(u) ≡f u, since π itself satisﬁes all the conditions above.
(cid:73) Lemma 8. If f is invariant under permutations, then ≡f is an equivalence relation.

We denote by [u]f the equivalence class of ≡f containing u. Following is the main result

of this paper.

(cid:73) Theorem 9. A transduction f is implemented by a SSRT iﬀ f satisﬁes the following
properties: 1)f is invariant under permutations, 2) f is without data peeking, 3) f has linear
blow up and 4) ≡f has ﬁnite index.

4

Constructing a SSRT from a Transduction

In this section, we prove the reverse direction of Theorem 9, by showing how to construct
a SSRT that implements a transduction, if it satisﬁes the four conditions in the theorem.
SSRTs read their input from left to right. Our ﬁrst task is to get SSRTs to identify inﬂuencing
data values as they are read one by one. Suppose a transducer that is intended to implement
a transduction f has read a data word u and has stored in its registers the data values that
are f -inﬂuencing in u. Suppose the transducer reads the next symbol (σ, e). To identify the
data values that are f -inﬂuencing in u · (σ, e), will the transducer need to read the whole
data word u · (σ, e) again? The answer turns out to be no, as the following result shows. The
only data values that can possibly be f -inﬂuencing in u · (σ, e) are e and the data values that
are f -inﬂuencing in u.

(cid:73) Lemma 10. Let f be a transduction, u be a data word, σ ∈ Σ and d, e be distinct
data values. If d is not f -memorable (resp. f -vulnerable) in u, then d is not f -memorable
(resp. f -vulnerable) in u · (σ, e).

M. Praveen

9

Next, suppose that d is f -inﬂuencing in u. How will we get the transducer to detect
whether d continues to be f -inﬂuencing in u · (σ, e)? The following result provides a partial
answer. If u1 ≡f u2 and the ith f -inﬂuencing value in u1 continues to be f -inﬂuencing in
u1 · (σ, e), then the ith f -inﬂuencing value in u2 continues to be f -inﬂuencing in u2 · (σ, e).
The following result combines many such similar results into a single one.

(cid:73) Lemma 11. Suppose f is a transduction that is invariant under permutations and without
1 dm−1
data peeking. Suppose u1, u2 are data words such that u1 ≡f u2, iflf (u1) = dm
· · · d1
1
1
and iflf (u2) = dm
1 ∈ D is not f -inﬂuencing in u1, d0
2. Suppose d0
2 ∈ D is not
f -inﬂuencing in u2 and σ ∈ Σ. For all i, j ∈ [0, m], the following are true:
1. di

2 is f -memorable (resp. f -

2 dm−1
2

1) iﬀ di

· · · d1

1 is f -memorable (resp. f -vulnerable) in u1 · (σ, dj
vulnerable) in u2 · (σ, dj
1) ≡f u2 · (σ, dj

2).
2).

2. u1 · (σ, dj

If u1 ≡f u2, there exists a permutation π such that aiflf (u1) = aiflf (π(u2)). Hence,
all data words in the same equivalence class of ≡f have the same number of f -inﬂuencing
values. If ≡f has ﬁnite index, then there is a bound (say I) such that any data word has at
most I f -inﬂuencing data values. We are going to construct a SSRT to identify f -inﬂuencing
data values. The construction is technically involved, so we motivate it by stating the end
result ﬁrst. Consider a SSRT Sifl
f with the set of registers R = {r1, . . . , rI }. The states are
of the form ([u]f , ptr), where u is some data word and ptr : [1, |iflf (u)|] → R is a pointer
function. Let ptr ⊥ be the trivial function from ∅ to R. The transitions can be designed to
satisfy the following.

(cid:73) Lemma 12. Suppose the SSRT Sifl
starts in the conﬁguration (([(cid:15)]f , ptr ⊥), val (cid:15), 0) and
reads some data word u. It reaches the conﬁguration (([u]f , ptr), val, |u|) such that val(ptr(i))
is the ith f -inﬂuencing value in u for all i ∈ [1, |iflf (u)|].

f

In short, the idea is that we can hard code rules such as “if the data value just read is the ith
f -inﬂuencing value in u, it continues to be f -inﬂuencing in the new data word”. Lemma 11
implies that the validity of such rules depend only on the equivalence class [u]f containing
u and does not depend on u itself. So the SSRT need not remember the entire word u; it
just remembers the equivalence class [u]f in its control state. The SSRT can check whether
the new data value is the ith f -inﬂuencing value in u, by comparing it with the register
ptr(i). To give the full details of constructing Sifl
, we need another concept explained in
the following paragraph.

f

Recall the transduction f from Example 2 and the inﬁnite set of data words {(a, di) | i ≥ 1}.
For any i 6= j, f ((a, di) | (a, di)) 6= f ((a, dj) | (a, di)) for i 6= j. But for every i, there is a
permutation πi on data values mapping di to d1 so that f (πi(a, di) | v) = f ((a, d1) | v) for
any data word v. We have revealed that all data words in {(a, di) | i ≥ 1} are equivalent by
applying a permutation to each data word, so that they all have the same f -inﬂuencing data
values. We formalize this idea below.

(cid:73) Deﬁnition 13. Let f be a transduction and Π be the set of all permutations on the set of
data values D. An equalizing scheme for f is a function E : (Σ × D)∗ → Π such that there
exists a sequence δ1δ2 · · · of data values satisfying the following condition: for every data
word u and every i ∈ [1, |iflf (u)|], the ith f -inﬂuencing data value of E(u)(u) is δi.

Note that E(u)(u) denotes the application of the permutation E(u) to the data word u.
We will write E(u)(u) as uq for short (intended to be read as “equalized u”). Note that
E(u)−1(uq) = u.

Now we give the full details of constructing Sifl

f

.

10

What You Must Remember When Transforming Datawords

(cid:73) Construction 14. Suppose f is a transduction that is invariant under permutations, ≡f has
ﬁnite index and E is an equalizing scheme. Let I be the maximum number of f -inﬂuencing
data values in any data word and δ1 · · · δI ∈ D∗ be such that for any data word u, δi is the ith
f -inﬂuencing value in uq. Consider a SSRT Sifl
f with the set of registers R = {r1, . . . , rI }.
The states are of the form ([u]f , ptr), where u is some data word and ptr : [1, |iflf (u)|] → R
is a pointer function. If |iflf (u)| = 0, then ptr = ptr ⊥, the trivial function from ∅ to R.
We let the set X of data word variables to be empty. Let ud ⊥ be the trivial update function
for the empty set X. The initial state is ([(cid:15)]f , ptr ⊥). Let δ0 be an arbitrary data value in
D \ {δ1, . . . , δI }. From a state ([u]f , ptr), for every σ ∈ Σ and i ∈ [0, |iflf (u)|], there is a
transition (([u]f , ptr), σ, φ, ([uq · (σ, δi)]f , ptr 0), R0, ud ⊥). The condition φ is as follows, where
m = |iflf (u)|:

φ =

(Vm

j=1 ptr(j)6=
φ = ptr(i)= ∧ V

i = 0
j∈[1,m]\{i} ptr(j)6= i 6= 0

For every j ∈ [1, |iflf (uq · (σ, δi))|], ptr 0(j) is as follows: if the jth f -inﬂuencing value of
uq · (σ, δi) is the kth f -inﬂuencing value of uq for some k, then ptr 0(j) = ptr(k). Otherwise,
ptr 0(j) = rreuse = min(R \{ptr(k) | 1 ≤ k ≤ m, δk is f -inﬂuencing in uq ·(σ, δi)}) (minimum
is based on the order r1 < r2 < · · · < rI ). The set R0 is {rreuse} if i = 0 and δ0 is f -inﬂuencing
in uq · (σ, δ0); R0 is ∅ otherwise.

It is routine to verify that the SSRT constructed above is deterministic. The deﬁnition of the
next pointer function ptr 0 ensures that the register ptr(j) always stores the jth f -inﬂuencing
value in the data word read so far. This is shown in the proof of Lemma 12, which can be
found in Section C.1.

Next we will extend the transducer to compute the output of a transduction. Suppose
the transducer has read the data word u so far. The transducer doesn’t know what is the
suﬃx that is going to come, so whatever computation it does has to cover all possibilities.
The idea is to compute {f (u | v) | v ∈ (Σ × D)∗} and store them in data word variables, so
that when it has to output f (u) at the end, it can output f (u | (cid:15)). However, this set can be
inﬁnite. If ≡f has ﬁnite index, we can reduce it to a ﬁnite set.

Left parts that have been equalized by an equalizing scheme will not have arbitrary
inﬂuencing data values — they will be from the sequence δ1δ2 · · · . For the transduction in
Example 2, the ﬁrst data value is the only inﬂuencing value in any data word. An equalizing
scheme will map the ﬁrst data value of all data words to δ1.

The relation ≡f identiﬁes two preﬁxes when they behave similarly. We now deﬁne a

relation that serves a similar propose, but for suﬃxes.

(cid:73) Deﬁnition 15. For a transduction f and equalizing scheme E, we deﬁne the relation ≡E
f
on data words as v1 ≡E

f v2 if for every data word u, f (uq | v1) = f (uq | v2).

It is routine to verify that ≡E
f is an equivalence relation. Saying that v1 and v2 are
similar suﬃxes if f (u | v1) = f (u | v2) for all u doesn’t work; this may result in inﬁnitely
many pairwise unequivalent suﬃxes (just like ≡f may have inﬁnite index if we don’t apply
permutations to preﬁxes). So we “equalize” the preﬁxes so that they have the same f -
inﬂuencing data values, before checking how suﬃxes inﬂuence them.

(cid:73) Lemma 16. Suppose f is a transduction satisfying all the conditions of Theorem 9. If E
is an equalizing scheme for f , then ≡E
f has ﬁnite index.

M. Praveen

11

Suppose we are trying to design a SSRT to implement a transduction f , which has the
property that ≡E
f has ﬁnite index. The SSRT can compute the set {f (uq | v) | v ∈ (Σ × D)∗},
which is ﬁnite (it is enough to consider one representative v from every equivalence class of
≡E
f ). At the end when the SSRT has to output f (u), it can output E(u)−1(f (uq | (cid:15))) = f (u).
The SSRT never knows what is the next suﬃx; at any point of time, the next suﬃx could
be (cid:15). So the SSRT has to apply the permutation E(u)−1 at each step. Letting V be
a ﬁnite set of representatives from every equivalence class of ≡E
f , the SSRT computes
{f (u | E(u)−1(v)) | v ∈ V } at every step.

Now suppose the SSRT has computed {f (u | E(u)−1(v)) | v ∈ V }, stored them in data
word variables and it reads the next symbol (σ, d). The SSRT has to compute {f (u · (σ, d) |
E(u · (σ, d))−1(v)) | v ∈ V } from whatever it had computed for u.

To explain how the above computation is done, we use some terminology. In factored
outputs of the form f (u | v), f (u | v), f (u | v | w) or f (u | v | w), a triple is said to
come from u if it has origin in u or it is the triple (∗, ∗, left). A left block in such a
factored output is a maximal inﬁx of triples, all coming from the left part u. Similarly,
a non-right block is a maximal inﬁx of triples, none coming from the right part. Middle
blocks are deﬁned similarly. For the transduction f in Example 2, f ((a, d1)(b, d2)(c, d3))
In f ((a, d1)(b, d2) | (c, d3)), (b, d2, 2)(a, d1, 1) is a left block.
is (c, d3, 3)(b, d2, 2)(a, d1, 1).
In f ((a, d1) | (b, d2) | (c, d3)), (b, d2, 2) is a middle block. In f ((a, d1) | (b, d2) | (c, d3)),
(∗, ∗, middle)(∗, ∗, left) is a non-right block, consisting of one middle and one left block.

The concretization of the ith left block (resp. middle block) in f (u | v | w) is deﬁned to be
the ith left block in f (u | vw) (resp. the ith middle block in f (u | v | w)). The concretization
of the ith non-right block in f (u | v | w) is obtained by concatenating the concretizations of
the left and middle blocks that occur in the ith non-right block. The following is a direct
consequence of the deﬁnitions.

(cid:73) Proposition 17. The ith left block of f (u·(σ, d) | v) is the concretization of the ith non-right
block of f (u | (σ, d) | v).

For the transduction f from Example 2, the ﬁrst left block of f ((a, d1)(b, d2) | (c, d3)) is
(b, d2, 2)(a, d1, 1), which is the concretization of (∗, ∗, middle)(∗, ∗, left), the ﬁrst non-right
block of f ((a, d1) | (b, d2) | (c, d3)).

From Proposition 17, we deduce that the ith left block of f (u · (σ, d) | E(u · (σ, d))−1(v))
is the concretization of the ith non-right block of f (u | (σ, d) | E(u · (σ, d))−1(v)). The
concretizations come from the left blocks of f (u | (σ, d) · E(u · (σ, d))−1(v)) and the middle
blocks of f (u | (σ, d) | E(u · (σ, d))−1(v)). In the absence of data values, the above two
statements would be as follows: The ith left block of f (u · σ | v) is the concretization of the ith
non-right block of f (u | σ | v). The concretizations come from the left blocks of f (u | σ · v)
and the middle blocks of f (u | σ | v). This technique of incrementally computing factored
outputs was introduced in [5] for SSTs. In SSTs, f (u | σ · v) would have been computed as
f (u | v0) when u was read, where v0 is some word that inﬂuences preﬁxes in the same way as
σ · v. But in SSRTs, only f (u | E(u)−1(v0)) would have been computed for various v0; what
we need is f (u | (σ, d) · E(u · (σ, d))−1(v)). We work around this by proving that a v0 can be
computed such that f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u | E(u)−1(v0)). This needs some
technical work, which follows next.

SSRTs will keep left blocks in variables, so we need a bound on the number of blocks.

(cid:73) Lemma 18. Suppose f is a transduction that is invariant under permutations and has
linear blow up and E is an equalizing scheme such that ≡E
f has ﬁnite index. There is a bound
B ∈ N such that for all data words u, v, the number of left blocks in f (u | v) is at most B.

12

What You Must Remember When Transforming Datawords

(cid:73) Deﬁnition 19. Suppose iflf (uq) = δm · · · δ1, δ0 ∈ D \ {δm, . . . , δ1}, η ∈ {δ0, . . . , δm} and
σ ∈ Σ. We say that a permutation π tracks inﬂuencing values on uq · (σ, η) if π(δi) is the ith
f -inﬂuencing value in uq · (σ, η) for all i ∈ [1, |iflf (uq · (σ, η))|].

Lemma 10 implies that for i ≥ 2 in the above deﬁnition, π(δi) ∈ {δm, . . . , δ1} and π(δ1) ∈
{δm, . . . , δ0}. We can infer from Lemma 11 that if u ≡f u0 and π tracks inﬂuencing values
on E(u0)(u0) · (σ, η), then it also tracks inﬂuencing values on uq · (σ, η).

(cid:73) Lemma 20. Suppose f is a transduction that is invariant under permutations and without
data peeking, u, u0, v are data words, σ ∈ Σ, iflf (u) = dm · · · d1, d0 ∈ D \ {dm, . . . , d1},
δ0 ∈ D \ {δm, . . . , δ1}, (d, η) ∈ {(di, δi) | i ∈ [0, m]}, π tracks inﬂuencing values on uq · (σ, η)
and u ≡f u0. Then f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u | E(u)−1((σ, η) · π(v))).
If
(d, η) ∈ {(di, δi) | i ∈ [1, m]}, then f (u | (σ, d) | E(u · (σ, d))−1(v)) = E(u)−1(fz(u0
q | (σ, η) |
π(v))), where z = |u| − |u0|. If (d, η) = (d0, δ0), then f (u | (σ, d) | E(u · (σ, d))−1(v)) =
q | (σ, η) | π(v))), where π0 is the permutation that interchanges δ0 and
E(u)−1 (cid:12) π0(fz(u0
E(u)(d0) and doesn’t change any other data value ((cid:12) denotes composition of permutations).

The left blocks of f (u | (σ, d) · E(u · (σ, d))−1(v)) are hence equal to those of the factored
output f (u | E(u)−1((σ, η) · π(v))), which would have been be stored as f (u | E(u)−1(v0)) in
one of the data word variables when u was read, where v0 ≡E

f (σ, η) · π(v).

f (σ, η) · π(v1) ≡E

Suppose v1, v2 ∈ V and v0 ≡E

f (σ, η) · π(v2). The computation of f (u ·
(σ, d) | E(u · (σ, d))−1(v1)) requires the left blocks of f (u | E(u)−1(v0)) and the computation
of f (u · (σ, d) | E(u · (σ, d))−1(v2)) also requires the left blocks of f (u | E(u)−1(v0)). The
SSRT would have stored f (u | E(u)−1(v0)) in a data word variable and now it is needed for
two computations. But in SSRTs, the contents of one data word variable cannot be used in
two computations, since SSRTs are copyless. This problem is solved in [5] for SSTs using
a two way transducer model equivalent to SSTs. In this two way model, the suﬃx can be
read and there is no need to perform computations for multiple suﬃxes. We cannot use that
technique here, since there are no known two way models equivalent to SSRTs.

We solve this problem by not performing the two computations immediately. Instead, we
remember the fact that there is a multiple dependency on a single data word variable. The
actual computation is delayed until the SSRT reads more symbols from the input and gathers
enough information about the suﬃx to discard all but one of the dependencies. Suppose
we have delayed computing f (u · (σ, d) | E(u · (σ, d))−1(v1)) due to some dependency. After
reading the next symbol, f (u · (σ, d) | E(u · (σ, d))−1(v1)) itself might be needed for multiple
computations. We keep track of such nested dependencies in a tree data structure called
dependency tree. Dependency trees can grow unboundedly, but if ≡E
f has ﬁnite index, it can
be shown that some parts can be discarded from time to time to keep their size bounded. We
store such reduced dependency trees as part of the control states of the SSRT. The details of
this construction constitute the rest of this section.

For a transduction f , let B be the maximum of the bounds on the number of left
blocks shown in Lemma 18 and the number of middle blocks in factored outputs of the
form f (u | (σ, d) | v). Let (Σ × D)∗/ ≡E
f , let
ˆX = {hθ, ii | θ ∈ ((Σ × D)∗/ ≡E
f )∗, let
Xθ = {hθ, ii | 1 ≤ i ≤ B2 + B}. We denote by θ (cid:56) the sequence obtained from θ by removing
the right most equivalence class. We use a set P = {P1, . . . , PB} of parent references in the
following deﬁnition. We use a ﬁnite subset of ˆX as data word variables to construct SSRTs.

f )∗, 1 ≤ i ≤ B2 + B} and for θ ∈ ((Σ × D)∗/ ≡E

f be the set of equivalence classes of ≡E

(cid:73) Deﬁnition 21. Suppose f is a transduction and E is an equalizing scheme for f . A
dependency tree T is a tuple (Θ, pref , bl), where the set of nodes Θ is a preﬁx closed

M. Praveen

13

ﬁnite subset of ((Σ × D)∗/ ≡E
f )∗ and pref , bl are labeling functions. The root is (cid:15) and if
θ ∈ Θ \ {(cid:15)}, its parent is θ (cid:56). The labeling functions are pref : Θ → (Σ × D)∗/ ≡f and
bl : Θ × [1, B] → ( ˆX ∪ P)∗. We call bl(θ, i) a block description. The dependency tree is said
to be reduced if the following conditions are satisﬁed:

every sequence θ in Θ has length that is bounded by |(Σ × D)∗/ ≡E
pref labels all the leaves with a single equivalence class of ≡f ,
for every equivalence class [v]E
class in θ is [v]E
f ,
bl(θ, i) ∈ (Xθ ∪ P)∗ and is of length at most 2B + 1 for all θ ∈ Θ and i ∈ [1, B] and
for all θ ∈ Θ, each element of Xθ ∪ P occurs at most once in {bl(θ, i) | 1 ≤ i ≤ B}.

f , there is exactly one leaf θ such that the last equivalence

f | + 1,

If ≡f and ≡E

f have ﬁnite indices, there are ﬁnitely many possible reduced dependency
trees. Suppose θ = θ0 · [v]E
f is in Θ, pref (θ) = [u]f and bl(θ, 1) = P1hθ, 1iP2. The intended
meaning is that there is a data word u0 that has been read by a SSRT and u0 ≡f u. The
block description bl(θ, 1) = P1hθ, 1iP2 is a template for assembling the ﬁrst left block of
f (u0 | E(u0)−1(v)) from smaller blocks: take the ﬁrst left block in the parent node θ0 (P1 refers
to the ﬁrst left block of the factored output assembled in the parent node), append to it the
contents of the data word variable hθ, 1i, then append the second left block in the parent node
θ0. Intuitively, if u0 = u00 · (σ, d), then the ﬁrst non-right block of f (u00 | (σ, d) | E(u0)−1(v)) is
(∗, ∗, left)(∗, ∗, middle)(∗, ∗, left) and P1 refers to the concretization of the ﬁrst left block
(∗, ∗, left), hθ, 1i contains the concretization of the ﬁrst middle block (∗, ∗, middle) and so
on. The ﬁrst left block in the parent node θ0 itself may consist of some parent references and
the contents of some other data word variables. This “unrolling” is formalized below.

(cid:73) Deﬁnition 22. Suppose T is a dependency tree with set of nodes Θ. The function ur :
Θ × ( ˆX ∪ P)∗ → ˆX ∗ is deﬁned as follows. For θ ∈ Θ and µ ∈ ( ˆX ∪ P)∗, ur(θ, µ) is obtained
from µ by replacing every occurrence of a parent reference Pi by ur(θ (cid:56), bl(θ (cid:56), i)) (replace
by (cid:15) if θ = (cid:15)) for all i.

Intuitively, an occurrence of Pi in µ refers to the ith left block in the parent node. If the
current node is θ, the parent node is θ (cid:56), so we unroll µ by inductively unrolling the ith left
block of θ’s parent, which is given by ur(θ (cid:56), bl(θ (cid:56), i)). We are interested in dependency
trees that allow to compute all factored outputs of the form f (u | E(u)−1(v)) by unrolling
appropriate leaves. For convenience, we assume that f ((cid:15)) = (cid:15). Let T⊥ = ({(cid:15)}, pref (cid:15), bl (cid:15)),
where pref (cid:15)((cid:15)) = [(cid:15)]f and bl (cid:15)((cid:15), i) = (cid:15) for all i ∈ [1, B].

(cid:73) Deﬁnition 23. Suppose f is a transduction, val is a valuation assigning a data word to
every element of ˆX and T is a dependency tree. The pair (T, val) is complete for a data
word u if u = (cid:15) and T = T⊥, or u 6= (cid:15) and the following conditions are satisﬁed: for every
equivalence class [v]E
f such that pref (θ) = [u]f and for
every i, the ith left block of f (u | E(u)−1(v)) is val(ur(θ, bl(θ, i))).

f , there exists a leaf node θ = θ0 · [v]E

We construct SSRTs that will have dependency trees in its states, which will be complete for
the data word read so far. As more symbols of the input data word are read, the dependency
tree and the valuation for ˆX are updated as deﬁned next.

(cid:73) Deﬁnition 24. Suppose f is a transduction, E is an equalizing scheme and T is either
T⊥ or a reduced dependency tree in which pref
labels all the leaves with [u]f for some
data word u. Suppose iflf (u) = dm · · · d1, d0 ∈ D \ {dm, . . . , d1}, δ0 ∈ D \ {δm, . . . , δ1},
(d, η) ∈ {(di, δi) | i ∈ [0, m]} and σ ∈ Σ. Let π be a permutation tracking inﬂuencing values
on uq · (σ, η) as deﬁned in Deﬁnition 19. For every equivalence class [v]E
f , there is a leaf

14

What You Must Remember When Transforming Datawords

f (with θv as parent) and set pref (θ) = [u0

node θv = θ0 · [(σ, η) · π(v)]E
f (or θv = (cid:15), the root of the trivial dependency tree in case
u = (cid:15)). Let u0 be an arbitrary data word in the equivalence class [u]f . The (σ, η) extension
of T is deﬁned to be the tree obtained from T as follows: for every equivalence class [v]E
f ,
create a new leaf θ = θv · [v]E
q · (σ, η)]f . For every
i ∈ [1, B], let z be the ith non-right block in f (u0
q | (σ, η) | π(v)) (z is a sequence of left and
middle blocks). Let z0 be obtained from z by replacing jth left block with Pj and kth middle
block with hθ, ki for all j, k. Set bl(θ, i) to be z0. If there are internal nodes (nodes that are
neither leaves nor the root) of this extended tree which do not have any of the newly added
leaves as descendants, remove such nodes. The resulting tree T 0 is the (σ, η) extension of T .
Suppose val is a valuation for ˆX such that (T, val) is complete for u. The (σ, d) extension
val 0 of val is deﬁned to be the valuation obtained from val by setting val 0(hθ, ki) to be the kth
middle block of f (u | (σ, d) | E(u · (σ, d))−1(v)) for every newly added leaf θ = θv · [v]E
f and
every k ∈ [1, B]. For all other variables, val 0 coincides with val. We call (T 0, val 0) the (σ, d)
extension of (T, val).

If some internal nodes are removed as described in Deﬁnition 24, it means that some
dependencies have vanished due to the extension. For a newly added leaf θ, every element of
Xθ ∪ P occurs at most once in {bl(θ, i) | 1 ≤ i ≤ B}.

(cid:73) Lemma 25. If (T, val) is complete for some data word u and (T 0, val 0) is the (σ, d) extension
of (T, val), then (T 0, val 0) is complete for u · (σ, d).

If (T, val) is complete for u and (T 0, val 0) is the (σ, d) extension of (T, val), then the data
word val 0(hθ, ki) is the kth middle block of f (u | (σ, d) | E(u · (σ, d))−1(v)). We call hθ, ki a
new middle block variable and refer to it later for deﬁning variable updates in transitions of
SSRTs. The tree T 0 may not be reduced since it may contain branches that are too long.
Next we see how to eliminate long branches.

(cid:73) Deﬁnition 26. Suppose T is a dependency tree. A shortening of T is obtained from T
as follows: let θ be an internal node that has only one child. Make the child of θ a child
of θ’s parent, bypassing and removing the original node θ. Any descendant θ · θ0 of θ in T
is now identiﬁed by θ (cid:56) ·θ0. Set pref (θ (cid:56) ·θ0) to be pref (θ · θ0), the label given by pref for
the original descendant θ · θ0 in T . Suppose θ · [v]E
f is the only child of θ in T . For every
i ∈ [1, B], set bl(θ (cid:56) ·[v]E
f , i) by replacing every
occurrence of Pj by bl(θ, j). For strict descendants θ (cid:56) ·[v]E
f and for every
i ∈ [1, B], set bl(θ (cid:56) ·[v]E

f , i) = µ, where µ is obtained from bl(θ · [v]E

f · θ0 of θ (cid:56) ·[v]E

f · θ0, i) = bl(θ · [v]E

f · θ0, i).

Intuitively, θ has only one child, so only one factored output is dependent on the factored
output stored in θ (all but one of the dependencies have vanished). Therefore, we can
remove θ and pass on the information stored there to its only child. This is accomplished by
replacing any occurrence of Pj in a block description of the child by bl(θ, j). Figure 1 shows
an example, where θ1 is the only child of θ. So θ is removed, θ1 becomes θ2 and a child of
θ (cid:56).

(cid:73) Lemma 27. If (T, val) is complete for a data word u and T 0 is a shortening of T , then
(T 0, val) is also complete for u.

Note that the valuation val need not be changed to maintain completeness of (T 0, val).
Hence, any new middle block variable will continue to store some middle block as before.
Shortening will reduce the lengths of paths in the tree; still the resulting tree may not be
reduced, since some node θ may have a block description bl(θ, i) that is too long and/or
contains variables not in Xθ. Next we explain how to resolve this.

M. Praveen

15

θ (cid:56)

θ

bl(θ, 1) = P1hθ, 1i
bl(θ, 2) = P2hθ, 2i

θ1 = θ · [v]E
f

bl(θ1, 1) =
P1 hθ1, 1i P2

θ (cid:56)

θ2 = θ (cid:56) ·[v]E
f

bl(θ2, 1) =
P1hθ, 1i hθ1, 1i P2hθ, 2i

Figure 1 A dependency tree (left) and its shortening (right)

In a block description bl(θ, i), a non-parent block is any inﬁx bl(θ, i)[j, k] such that 1)j = 1
or the (j − 1)th element of bl(θ, i) is a parent reference, 2)k = |bl(θ, i)| or the (k + 1)th element
of bl(θ, i) is a parent reference and 3) for every k0 ∈ [j, k], the k0th element of bl(θ, i) is not a
parent reference. Intuitively, a non-parent block of bl(θ, i) is a maximal inﬁx consisting of
elements of ˆX only.

(cid:73) Deﬁnition 28. Suppose T is a dependency tree and val is a valuation for X. The trimming
of T is obtained from T by performing the following for every node θ: enumerate the
set {z | z is a non-parent block in bl(θ, i), 1 ≤ i ≤ B} as z1, z2, . . . , zr, choosing the order
If bl(θ, i) for some i contains zj for some j, replace zj by hθ, ji. Perform
arbitrarily.
such replacements for all i and j. The trimming val 0 of val is obtained from val by setting
val 0(hθ, ji) = val(zj) for all j and val 0(hθ0, ki) = (cid:15) for all hθ0, ki occurring in any zj. For
elements of ˆX that neither occur in any zj nor replace any zj, val and val 0 coincide.

For example, bl(θ2, 1) = P1hθ, 1ihθ1, 1iP2hθ, 2i in Figure 1 is replaced by P1hθ2, 1iP2hθ2, 2i.
In the new valuation, we have val 0(hθ2, 1i) = val(hθ, 1i) · val(hθ1, 1i), val 0(hθ2, 2i) = val(θ, 2)
and val 0(hθ, 1i) = val 0(hθ1, 1i) = val 0(hθ, 2i) = (cid:15). The following result follows directly from
deﬁnitions.

(cid:73) Proposition 29. If (T, val) is complete for a data word u, then so is the trimming (T 0, val 0).

States of the SSRT we construct will have reduced dependency trees. The following result
is helpful in deﬁning the SSRT transitions, where we have to say how to obtain a new tree
from an old one.

(cid:73) Lemma 30. Suppose T is a reduced dependency tree or T⊥, T1 is the (σ, η) extension of
T for some (σ, η) ∈ Σ × {δ0, δ1, . . .}, T2 is obtained from T1 by shortening it as much as
possible and T3 is the trimming of T2. Then T3 is a reduced dependency tree.

We will now extend the SSRT constructed in Construction 14 to transform input data
words to output data words with origin information. For any data word with origin information
w, let (cid:22)2 (w) be the data word obtained from w by discarding the third component in every
triple.

(cid:73) Construction 31. Suppose f is a transduction satisfying all the conditions in Theorem 9.
Let I be the maximum number of f -inﬂuencing values in any data word and let B be the
maximum number of blocks in any factored output of the form f (u| | v) or f (u | v | w).
Consider a SSRT with set of registers R = {R1, . . . , RI } and data word variables X = {hθ, ii |
f | + 1, i ∈ [1, B2 + B]}. Every state is a triple
θ ∈ ((Σ × D)∗/ ≡E

f )∗, |θ| ≤ |(Σ × D)∗/ ≡E

16

What You Must Remember When Transforming Datawords

([u]f , ptr, T ) where u is some data word, T is a reduced dependency tree or T⊥ such that pref
labels every leaf in T with [u]f and ptr : [1, |iflf (u)|] → R is a pointer function. The initial
state is ([(cid:15)]f , ptr ⊥, T⊥). Let δ0 /∈ {δ|iflf (u)|, . . . , δ1} be an arbitrary data value. For every T
and for every transition (([u]f , ptr), σ, φ, ([uq·(σ, δi)]f , ptr 0), R0, ud ⊥) given in Construction 14,
we will have the following transition: (([u]f , ptr, T ), σ, φ, ([uq · (σ, δi)]f , ptr 0, T 0), R0, ud). Let
T1 be the (σ, δi) extension of T and let T2 be obtained from T1 by shortening it as much as
possible. T 0 is deﬁned to be the trimming of T2. We deﬁne the update function ud using an
intermediate function ud 1 and an arbitrary data word u0 ∈ [u]f . For every data word variable
hθ, ii that is not a new middle block variable in T1, set ud 1(hθ, ii) = hθ, ii. For every new
f . Set ud 1(hθ, ki) =(cid:22)2 (z), where z is obtained
middle block variable hθ, ki, say θ = θv · [v]E
from the kth middle block of f (E(u0)(u0) | (σ, δi) | π(v)) by replacing every occurrence of
δj by ptr(j) for all j ∈ [1, |iflf (u)|] and replacing every occurrence of δ0 by curr. Here, π
is a permutation tracking inﬂuencing values in E(u0)(u0) · (σ, δi) as given in Deﬁnition 19.
Next we deﬁne the function ud. While trimming T2, suppose a non-parent block zj in a node
θ was replaced by a data word variable hθ, ji. Deﬁne ud(hθ, ji) = ud 1(zj). For every data
word variable hθ1, ki occurring in zj, deﬁne ud(hθ1, ki) = (cid:15). For all other data word variables
hθ2, k0i, deﬁne ud(hθ2, k0i) = ud 1(hθ2, k0i). The output function O is deﬁned as follows: for
every state ([u]f , ptr, T ), O(([u]f , ptr, T )) = ur(θ, bl(θ, 1)) · · · · · ur(θ, bl(θ, B)) where θ is the
f ends in the equivalence class [(cid:15)]E
leaf of T such that θ = θ0 · [(cid:15)]E
f .

Lemma 30 implies that if T is T⊥ or a reduced dependency tree, then so is T 0. It is routine
to verify that this SSRT is deterministic and copyless.

(cid:73) Lemma 32. Let the SSRT constructed in Construction 31 be S. After reading a data word
u, S reaches the conﬁguration (([u]f , ptr, T ), val, |u|) such that ptr(i) is the ith f -inﬂuencing
value in u and (T, val) is complete for u.

Proof of reverse direction of Theorem 9. Let f be a transduction that satisﬁes all the
properties stated in Theorem 9. We infer from Lemma 32 that the SSRT S constructed in
Construction 31 satisﬁes the following property. After reading a data word u, S reaches
the conﬁguration (([u]f , ptr, T ), val, |u|) such that ptr(i) is the ith f -inﬂuencing value in
u and (T, val) is complete for u. We deﬁne the output function of the SSRT such that
(u) = val(ur(θ, bl(θ, 1)) · · · · · ur(θ, bl(θ, B))), where θ = θ0 · [(cid:15)]E
f is the leaf of T ending
S
(cid:74)
(cid:75)
with [(cid:15)]E
f . Since (T, val) is complete for u, we infer that val(ur(θ, bl(θ, 1)) · · · · · ur(θ, bl(θ, B)))
is the concatenation of the left blocks of f (u | E(u)−1((cid:15))) = f (u). Hence, the SSRT S
(cid:74)
implements the transduction f .

5

Properties of Transductions Implemented by SSRTs

In this section, we prove the forward direction of our main result (Theorem 9).

For a valuation val and permutation π, we denote by π(val) the valuation that assigns
π(val(r)) to every register r and π(val(x)) to every data word variable x. The following two
results easily follow from deﬁnitions.

(cid:73) Proposition 33. Suppose a SSRT S reaches a conﬁguration (q, val, n) after reading a data
word u. If π is any permutation, then S reaches the conﬁguration (q, π(val), n) after reading
π(u).

(cid:73) Proposition 34. If a SSRT S implements a transduction f , then f is invariant under
permutations and is without data peeking.

M. Praveen

17

After a SSRT reads a data word, data values that are not stored in any of the registers

will not inﬂuence the rest of the operations.

(cid:73) Lemma 35. Suppose a SSRT S implements the transduction f . Any data value d that is
f -inﬂuencing in some data word u will be stored in one of the registers of S after reading u.

Now we identify data words after reading which, a SSRT reaches similar coﬁgurations.

(cid:73) Deﬁnition 36. For a SSRT S, we deﬁne a binary relation ≡S on data words as follows:
u1 ≡S u2 if they satisfy the following conditions. Suppose f is the transduction implemented
by S, which reaches the conﬁguration (q1, val 1, |u1|) after reading u1 and reaches (q2, val 2, |u2|)
after reading u2.
1. q1 = q2,
2. for any two registers r1, r2, we have val 1(r1) = val 1(r2) iﬀ val 2(r1) = val 2(r2),
3. for any register r, val 1(r) is the ith f -suﬃx inﬂuencing value (resp. f -preﬁx inﬂuencing
value) in u1 iﬀ val 2(r) is the ith f -suﬃx inﬂuencing value (resp. f -preﬁx inﬂuencing
value) in u2,

4. for any data word variable x, we have val 1(x) = (cid:15) iﬀ val 2(x) = (cid:15) and
5. for any two subsets X1, X2 ⊆ X and any arrangements χ1, χ2 of X1, X2 respectively,

val 1(χ1) = val 1(χ2) iﬀ val 2(χ1) = val 2(χ2).

An arrangement of a ﬁnite set X1 is a sequence in X ∗
exactly once. It is routine to verify that ≡S is an equivalence relation of ﬁnite index.

1 in which every element of X1 occurs

Suppose a SSRT S reads a data word u, reaches the conﬁguration (q, val, |u|) and from
there, continues to read a data word v. For some data word variable x ∈ X, if val(x) is some
data word z, then none of the transitions executed while reading v will split z — it might
be appended or prepended with other data words and may be moved to other variables
but never split. Suppose X = {x1, . . . , xm}. The transitions executed while reading v can
arrange val(x1), . . . , val(xm) in various ways, possibly inserting other data words (whose
origin is in v) in between. Hence, any left block of
(u | v) is val(χ), where χ is some
(cid:75)
arrangement of some subset X 0 ⊆ X. The following result is shown by proving that ≡S
reﬁnes ≡f . The most diﬃcult part of this proof is to prove that if u1 ≡S u2, then there
exists a permutation π such that for all data words u, v1, v2, f (u1 · u | v1) = f (u1 · u | v2) iﬀ
f (π(u2) · u | v1) = f (π(u2) · u | v2). The idea is to show that if f (u1 · u | v1) 6= f (u1 · u | v2),
then for some arrangements χ1, χ2 of some subsets X1, X2 ⊆ X, val 1(χ1) 6= val 1(χ2) (val 1
(resp. val 2) is the valuation reached by S after reading u1 (resp. u2)). Since u1 ≡S u2, this
implies that val 2(χ1) 6= val 2(χ2), which in turn implies that f (π(u2)·u | v1) 6= f (π(u2)·u | v2).

S
(cid:74)

(cid:73) Lemma 37. If a SSRT S implements a transduction f , then ≡f has ﬁnite index.

Proof of forward direction of Theorem 9. Suppose f is the transduction implemented by
a SSRT S. Lemma 37 implies that ≡f has ﬁnite index. Proposition 34 implies that f is
invariant under permutations and is without data peeking. The output of S on any input
is the concatenation of the data words stored in some variables in S and constantly many
symbols coming from the output ﬁnction of S. The contents of data word variables are
generated by transitions when reading input symbols and each transition can write only
constantly many symbols into any data word variable after reading one input symbol. After
some content is written into a data word variable, it is never duplicated into multiple copies
since the transitions of S are copyless. Hence, any input position can be the origin of only
(cid:74)
constantly many output positions. Hence, f has linear blow up.

18

What You Must Remember When Transforming Datawords

6

Future Work

One direction to explore is whether there is a notion of minimal canonical SSRT and if a
given SSRT can be reduced to an equivalent minimal one. Adding a linear order on the data
domain, logical characterization of SSRTs and studying two way transducer models with
data are some more interesting studies.

Using nominal automata, techniques for ﬁnite alphabets can often be elegantly carried
over to inﬁnite alphabets, as done in [19], for example. It would be interesting to see if the
same can be done for streaming transducers over inﬁnite alphabets. Using concepts from the
theory of nominal automata, recent work [6] has shown that an atom extension of streaming
string transducers is equivalent to a certain class of two way transducers. This model of
transducers is a restriction of SSRTs and is robust like regular languages over ﬁnite alphabets.
It would also be interesting to see how can techniques in this extended abstract be simpliﬁed
to work on the transducer model presented in [6].

References

1 R. Alur and P. Černý. Expressiveness of streaming string transducers. In FSTTCS 2010,
volume 8 of LIPIcs, pages 1–12. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2010.
doi:10.4230/LIPIcs.FSTTCS.2010.1.

2 R. Alur and P. Černý. Streaming transducers for algorithmic veriﬁcation of single-pass

list-processing programs. In POPL 2011, POPL, pages 1–12. ACM, 2011.

3 D. Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87–

106, 1987.

4 M. Benedikt, C. Ley, and G. Puppis. What you must remember when processing data words.
In Proceedings of the 4th Alberto Mendelzon International Workshop on Foundations of Data
Management, Argentina, volume 619 of CEUR Workshop Proceedings, 2010.

5 M. Bojańczyk. Transducers with origin information. In ICALP, volume 8573 of LNCS, pages

26–37, Berlin, Heidelberg, 2014. Springer.

7

6 M. Bojańczyk and R. Stefański. Single-Use Automata and Transducers for Inﬁnite Al-
phabets.
In ICALP 2020, volume 168 of Leibniz International Proceedings in Inform-
atics (LIPIcs), pages 113:1–113:14, Dagstuhl, Germany, 2020. Schloss Dagstuhl–Leibniz-
Zentrum für Informatik. URL: https://drops.dagstuhl.de/opus/volltexte/2020/12520,
doi:10.4230/LIPIcs.ICALP.2020.113.
Sougata Bose, Anca Muscholl, Vincent Penelle, and Gabriele Puppis. Origin-equivalence
of two-way word transducers is in PSPACE. In Sumit Ganguly and Paritosh K. Pandya,
editors, 38th IARCS Annual Conference on Foundations of Software Technology and Theoretical
Computer Science, FSTTCS 2018, December 11-13, 2018, Ahmedabad, India, volume 122
of LIPIcs, pages 22:1–22:18. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018. doi:
10.4230/LIPIcs.FSTTCS.2018.22.
S. Cassel, F. Howar, B. Jonsson, M. Merten, and B. Steﬀen. A succinct canonical register
automaton model. Journal of Logical and Algebraic Methods in Programming, 84(1):54 – 66,
2015. Special Issue: The 23rd Nordic Workshop on Programming Theory (NWPT 2011) Special
Issue: Domains X, International workshop on Domain Theory and applications, Swansea, 5-7
September, 2011.
S. Cassel, B. Jonsson, F. Howar, and B. Steﬀen. A succinct canonical register automaton
model for data domains with binary relations. In Automated Technology for Veriﬁcation
and Analysis - 10th International Symposium, 2012, Proceedings, pages 57–71, 2012. doi:
10.1007/978-3-642-33386-6\_6.

9

8

10 Y-F Chen, O. Lengál, T. Tan, and Z. Wu. Register automata with linear arithmetic. In 32nd
Annual ACM/IEEE Symposium on Logic in Computer Science, LICS 2017, Reykjavik, Iceland,
June 20-23, 2017, pages 1–12. IEEE Computer Society, 2017.

M. Praveen

19

11 M. Chytil and V. Jákl. Serial composition of 2-way ﬁnite-state transducers and simple programs
on strings. In Automata, Languages and Programming, Fourth Colloquium, University of
Turku, Finland, July 18-22, 1977, Proceedings, pages 135–147. Springer Berlin Heidelberg,
1977.

13

12 A. Durand-Gasselin and P. Habermehl. Regular transformations of data words through origin
information.
In B. Jacobs and C. Löding, editors, Foundations of Software Science and
Computation Structures, pages 285–300, Berlin, Heidelberg, 2016. Springer Berlin Heidelberg.
Léo Exibard, Emmanuel Filiot, and Pierre-Alain Reynier. Synthesis of data word transducers. In
Wan J. Fokkink and Rob van Glabbeek, editors, 30th International Conference on Concurrency
Theory, CONCUR 2019, August 27-30, 2019, Amsterdam, the Netherlands, volume 140 of
LIPIcs, pages 24:1–24:15. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2019. doi:
10.4230/LIPIcs.CONCUR.2019.24.
Léo Exibard, Emmanuel Filiot, and Pierre-Alain Reynier. On computability of data word
functions deﬁned by transducers.
In Jean Goubault-Larrecq and Barbara König, edit-
ors, Foundations of Software Science and Computation Structures - 23rd International
Conference, FOSSACS 2020, Held as Part of the European Joint Conferences on The-
ory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25-30, 2020, Proceed-
ings, volume 12077 of Lecture Notes in Computer Science, pages 217–236. Springer, 2020.
doi:10.1007/978-3-030-45231-5\_12.

14

15 N. Francez and M. Kaminski. An algebraic characterization of deterministic regular languages

16

17

over inﬁnite alphabets. Theoretical Computer Science, 306:155–175, 2003.
F. Howar, M. Isberner, B. Steﬀen, O. Bauer, and B. Jonsson. Inferring semantic interfaces of
data structures. In T. Margaria and B. Steﬀen, editors, Leveraging Applications of Formal
Methods, Veriﬁcation and Validation. Technologies for Mastering Change, pages 554–571,
Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
F. Howar, B. Steﬀen, B. Jonsson, and S. Cassel.
Inferring canonical register automata.
In V. Kuncak and A. Rybalchenko, editors, Veriﬁcation, Model Checking, and Abstract
Interpretation, pages 251–266, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.

18 M. Kaminski and N. Francez. Finite-memory automata. Theoretical Computer Science,

19

134(2):329 – 363, 1994.
J. Moerman, M. Sammartino, A. Silva, B. Klin, and M. Szynwelski. Learning nominal
automata. In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming
Languages, POPL 2017, Paris, France, January 18-20, 2017, pages 613–625, 2017.

20

What You Must Remember When Transforming Datawords

A

Fundamental Properties of Transductions

The following result says that if a transduction is invariant under permutations, then so are
all its factored outputs.

(cid:73) Lemma 38. Suppose f is a transduction that is invariant under permutations, u, v, w are
data words, π is any permutation and z is any integer. Then π(fz(u | v)) = fz(π(u) | π(v)),
π(fz(u | v)) = fz(π(u) | π(v)) and π(fz(u | v | w)) = fz(π(u) | π(v) | π(w)).

Proof. From the invariance of f under permutations, we have f (π(u) · π(v)) = π(f (u · v)).
Adding z to every triple on both sides, we get

fz(π(u) · π(v)) = π(fz(u · v)) .

For every i ∈ [1, |fz(π(u) · π(v))|], we perform the following on the LHS of the above equation:
let (γ, d, o) be the ith triple in the LHS; if o − z ∈ [1, |u|], replace the triple by (∗, ∗, left).
After performing this change for every i, merge consecutive occurrences of (∗, ∗, left) into a
single triple (∗, ∗, left). At the end, we get fz(π(u) | π(v)).

Now perform exactly the same operations not on the RHS π(fz(u · v)), but on fz(u · v).
The ith triple will be (γ, π−1(d), o) and it changes to (∗, ∗, left) iﬀ the ith triple (γ, d, o) in
the LHS changed to (∗, ∗, left). Now, if we merge consecutive occurrences of (∗, ∗, left)
into a single triple (∗, ∗, left), we get fz(u | v). If we now apply the permutation π to this,
we get π(fz(u | v)), but we also get exactly the same sequence of triples we got from LHS
after the changes, which is fz(π(u) | π(v)). Hence, fz(π(u) | π(v)) = π(fz(u | v)). The proofs
(cid:74)
of the other two equalities are similar.

The following result says that the inﬂuencing values of a data word are aﬀected by a

permutation as expected.

(cid:73) Lemma 39. If f is a transduction that is invariant under permutations and u is a data
word, then for any permutation π, aiflf (π(u)) = π(aiflf (u)).

Proof. It is suﬃcient to prove that for any position j of u, the data value in the jth position
of u is a f -memorable value in u iﬀ the data value in the jth position of π(u) is a f -memorable
value in π(u) and similarly for f -vulnerable values. Indeed, suppose d is the data value in the
jth position of u and it is a f -memorable value in u. By Deﬁnition 4, there exists a data word
v and a data value d0 that is a safe replacement for d in u such that f (u[d/d0] | v) 6= f (u | v).
The data value at jth position of π(u) is π(d) and the word π(v) and the data value π(d0)
witnesses that π(d) is a f -memorable in π(u). Indeed, if f (u[d/d0] | v) 6= f (u | v), then
Lemma 38 implies that f (π(u)[π(d)/π(d0)] | π(v)) 6= f (π(u) | π(v)). The converse direction
of the proof is symmetric, using the permutation π−1.

Suppose d is the data value in the jth position of u and it is a f -vulnerable value in u. By
Deﬁnition 4, there exist data words u0, v and a data value d0 that is a safe replacement for d
in u · u0 · v such that d doesn’t occur in u0 and f (u · u0 | v) 6= f (u · u0 | v[d/d0]). The data value
at jth position of π(u) is π(d) and the words π(u0), π(v) and the data value π(d0) witnesses
that π(d) is a f -vulnerable in π(u). Indeed, since f (u · u0 | v) 6= f (u · u0 | v[d/d0]), Lemma 38
implies that f (π(u) · π(u0) | π(v)) 6= f (π(u) · π(u0) | π(v)[π(d)/π(d0)]). The converse direction
(cid:74)
of the proof is symmetric, using the permutation π−1.

A data value that does not occur in a data word can not inﬂuence how it is transformed.

(cid:73) Lemma 40. Suppose f is a transduction that is invariant under permutations and without
data peeking and a data value d is f -vulnerable in a data word u. Then d occurs in u.

M. Praveen

21

Proof. Suppose d does not occur in u. We will prove that d is not f -vulnerable in u. Let
u0, v be any data words such that d does not occur in u0. Suppose d0 is a safe replacement
for d in u · u0 · v. Let π be the permutation that interchanges d and d0 and does not change
any other value. Neither d nor d0 occurs in u · u0, so π(u · u0) = u · u0. The data value
d0 does not occur in v, so π(v) = v[d/d0]. Since f is without data peeking, only data
values in occurring in u · u0 occur f (u · u0 | v), so neither d nor d0 occur in f (u · u0 | v), so
π(f (u·u0 | v)) = f (u·u0 | v). Since f is invariant under permutations, we infer from Lemma 38
that π(f (u · u0 | v)) = f (π(u · u0) | π(v)). This implies that f (u · u0 | v) = f (u · u0 | v[d/d0]).
(cid:74)
Hence, d is not f -vulnerable in u.

Data values in a preﬁx can be permuted without changing the way it aﬀects suﬃxes, as

long as we don’t change the inﬂuencing values.

(cid:73) Lemma 41. Suppose f is a transduction that is invariant under permutations, u, v are
data words and π is any permutation that is identity on the set of data values that are
f -inﬂuencing in u. Then f (π(u) | v) = f (u | v) and aiflf (u) = aiflf (π(u)).

1, . . . , d0

1]. Hence, f (u[d1/d0

1][d2/d0
1 is not f -inﬂuencing in u[d1/d0

n be safe replacements for d1, . . . , dn respectively in u, such that {d0

Proof. Let {d1, . . . , dn} be the set of all data values occurring in u that are not f -inﬂuencing in
1, . . . , d0
u. Let d0
n}∩
({d1, . . . , dn}∪{π(d1), . . . , π(dn)}) = ∅. Since d1 is not f -memorable in u, we have f (u[d1/d0
1] |
v) = f (u | v). Since d2 is not f -inﬂuencing in u, we infer from Lemma 51 that d2 is not f -
inﬂuencing in u[d1/d0
1] | v) = f (u | v). Also from
Lemma 51, we infer that d0
1 in Lemma 51 to see
this). Similarly, neither d0
2 are f -inﬂuencing in u[d1/d0
2]. On the other hand, we
infer from Lemma 51 that all the data values that are f -memorable (resp. f -vulnerable) in u
are also f -memorable (resp. f -vulnerable) in u[d1/d0
2]. This reasoning can be routinely
extended to an induction on i to infer that f (u[d1/d0
1, . . . , d0
i
are not f -inﬂuencing in u[d1/d0
n] | v) = f (u | v).
In addition, all the data values that are f -memorable (resp. f -vulnerable) in u are also
f -memorable (resp. f -vulnerable) in u[d1/d0

1][d2/d0
1, . . . , di/d0
i]. Hence, f (u[d1/d0

i] | v) = f (u | v) and d0
1, . . . , dn/d0

1] (put e = d0
1][d2/d0

2] | v) = f (u[d1/d0

1, . . . , di/d0

1 nor d0

1, . . . , dn/d0

n].

Now we prove that π(d1), . . . , π(dn) are safe replacements for d0
1, . . . , dn/d0

1, . . . , dn/d0
We know that data(u[d1/d0
n} ∪ {d | d is f -inﬂuencing in u}.
We have {π(d1), . . . , π(dn)} ∩ {d0
n} = ∅ by choice. Since π is identity on {d |
d is f -inﬂuencing in u} and d1, . . . , dn are not f -inﬂuencing in u, we have {π(d1), . . . , π(dn)}∩
{d | d is f -inﬂuencing in u} = ∅. This proves that π(d1), . . . , π(dn) are safe replacements for
d0
1, . . . , d0

n], ∗) = {d0

n in u[d1/d0

1, . . . , dn/d0

n in u[d1/d0

1, . . . , d0

1, . . . , d0

1, . . . , d0

n].

n].

1, . . . , dn/d0

n][d0
As we did in the ﬁrst paragraph of this proof, we conclude that f (u[d1/d0
n] = f (u | v). Since u[d1/d0
v) = f (u[d1/d0
n/π(dn)] =
u[d1/π(d1), . . . , dn/π(dn)] = π(u), we infer that f (π(u) | v) = f (u | v).
In addition,
π(d1), . . . , π(dn) are not f -inﬂuencing in π(u) and all the values that are f -memorable (resp. f -
vulnerable) in u are also f -memorable (resp. f -vulnerable) in π(u). Hence, aifl(π(u)) =
(cid:74)
aifl(u).

1/π(d1), . . . , d0

1, . . . , dn/d0

1, . . . , dn/d0

n][d0

1/π(d1), . . . , d0

n/π(dn)] |

Data values in a suﬃx can be permuted without changing the way it aﬀects preﬁxes, as

long as we don’t change the preﬁx inﬂuencing values.

(cid:73) Lemma 42. Suppose f is a transduction that is invariant under permutations and without
data peeking, u, v are data data words and π is any permutation that is identity on the set of
data values that are f -vulnerable in u. Then f (u | π(v)) = f (u | v).

22

What You Must Remember When Transforming Datawords

1, d2/d0

1, d2/d0

1, . . . , d0

2, . . . , dn/d0

2, . . . , dn/d0

2]) = f (u | v[d1/d0

n]. We have data(v[d1/d0

Proof. Let {d1, . . . , dn} be the set of all data values occurring in v that are not f -vulnerable
in u. Let d0
n be safe replacements for d1, . . . , dn respectively in u · v, such that
1, . . . , d0
{d0
n} ∩ ({d1, . . . , dn} ∪ {π(d1), . . . , π(dn)}) = ∅. Since d1 is not f -vulnerable in u,
we have f (u | v[d1/d0
1]) = f (u | v). Since d2 is not f -vulnerable in u, we have f (u |
v[d1/d0
1][d2/d0
1]) = f (u | v). The same reasoning can be used in an
induction to conclude that f (u | v[d1/d0

n]) = f (u | v).
Now we will prove that π(d1), . . . , π(dn) are safe replacements for d0

n respectively
n], ∗) = {d0
1, . . . , d0
in v[d1/d0
n} ∪ {d |
d is f -vulnerable in u}. We have {π(d1), . . . , π(dn)} ∩ {d0
n} = ∅ by choice. Since π
is identity on {d | d is f -vulnerable in u} and d1, . . . , dn are not f -vulnerable in u, we have
{π(d1), . . . , π(dn)} ∩ {d | d is f -vulnerable in u} = ∅. This proves that π(d1), . . . , π(dn) are
1, . . . , dn/d0
safe replacements for d0
2, . . . , dn/d0
2, . . . , dn/d0
1, d2/d0
n][d0
Suppose not, i.e., f (u | v[d1/d0
This can be written equivalently as f (u | v[d1/d0
1/π(d1)]) 6= f (u |
v[d1/d0
n][d0
1]). Then we infer from Deﬁnition 4 that π(d1)
is f -vulnerable in u, which contradicts the hypothesis that π is identity on all values
that are f -vulnerable in u. Hence, f (u | v[d1/d0
1/π(d1)]) = f (u |
v[d1/d0
n]).

n].
n][d0
1/π(d1)]) 6= f (u | v[d1/d0
n][d0
2, . . . , dn/d0
1, d2/d0

Now we claim that f (u | v[d1/d0

1, . . . , dn/d0
1, . . . , d0

1/π(d1)]) = f (u | v[d1/d0

n in v[d1/d0
1, d2/d0

1/π(d1)][π(d1)/d0

2, . . . , dn/d0

2, . . . , dn/d0

2, . . . , dn/d0

2, . . . , dn/d0

1, . . . , d0

1, . . . , d0

1, d2/d0

1, d2/d0

1, d2/d0

1, d2/d0

1, d2/d0

n][d0

2, . . . , dn/d0

n]).

n]).

Similar reasoning can then be used to infer that f (u | v[d1/d0

1, d2/d0

2, . . . , dn/d0

n][d0

f (u | v[d1/d0

1, d2/d0

2, . . . , dn/d0

n]) = f (u | v). Hence, f (u | π(v)) = f (u | v).

1/π(d1), . . . , d0
(cid:74)

n/π(dn)]) =

If two factored outputs are equal, factoring out the same word from the same positions of

the inputs will not destroy the equality.

(cid:73) Lemma 43. Suppose f is a transduction, u, u1, u2, v, v1, v2 are data words, σ ∈ Σ, d is a
data value and z = |u1| − |u2|.
1. If f (u1 | u · v) = fz(u2 | u · v), then f (u1 | u | v) = fz(u2 | u | v).
2. If f (u1 | u · v) = fz(u2 | u · v), then f (u1 · u | v) = fz(u2 · u | v).
3. If f (u · v | v1) = f (u · v | v2), then f (u | v · v1) = f (u | v · v2).
4. If f (u · v | v1) = f (u · v | v2), then f (u | v | v1) = f (u | v | v2).

Proof. We prove the ﬁrst statement. Others are similar. We have the following equality
from the hypothesis.

f (u1 | u · v) = fz(u2 | u · v)

For every i ∈ [1, |f (u1 | u · v)|], we perform the following on the LHS of the above equation:
let (γ, d, o) be the ith triple in the LHS; if o > |u1| + |u|, replace the triple by (∗, ∗, right)
(the origin of such a triple is in v). Otherwise, don’t change the triple. After performing
this change for every i, merge consecutive occurrences of (∗, ∗, right) into a single triple
(∗, ∗, right). At the end, we get f (u1 | u | v).

Now perform exactly the same operations on the RHS fz(u2 | u · v). The ith triple
(γ, d, o) will change to (∗, ∗, right) (resp. will not change) iﬀ the ith triple (γ, d, o) in the LHS
changed to (∗, ∗, right) (resp. did not change). Note that if o > |u1| + |u|, o − z > |u2| + |u|.
Hence, the triples that change to (∗, ∗, right) in the RHS are precisely the triples whose
origin is in v. Now, if we merge consecutive occurrences of (∗, ∗, right) into a single triple
(∗, ∗, right), we get fz(u2 | u | v). This is also the same sequence of triples we got from LHS
(cid:74)
after the changes, which is f (u1 | u | v). Hence, f (u1 | u | v) = fz(u2 | u | v).

M. Praveen

23

(cid:73) Lemma 44. Suppose f is a transduction that is invariant under permutations, u, v, w are
data words and π, π0 ∈ Π are permutations on the data domain D. If π and π0 coincide on
those data values that are f -inﬂuencing in u · v, then π(f (u | v | w)) = f (π(u) | π(v) | π0(w)).

Proof. Since π and π0 coincide on those data values that are f -inﬂuencing in u · v, we infer
from Lemma 42 that f (π(u · v) | π(w)) = f (π(u · v) | π0(w)). From point 4 of Lemma 43,
we conclude that f (π(u) | π(v) | π(w)) = f (π(u) | π(v) | π0(w)). We have from Lemma 38
that π(f (u | v | w)) = f (π(u) | π(v) | π(w)). Combining the last two equalities, we get the
(cid:74)
result.

The following result is in some sense the converse of points (3) and (4) in Lemma 43.

(cid:73) Lemma 45. Let f be a transduction and u, v, w1, w2 be data words. If f (u | v | w1) =
f (u | v | w2) and f (u | vw1) = f (u | vw2), then f (uv | w1) = f (uv | w2).

Proof. The number of occurrences of the triple (∗, ∗, right) is the same in f (u | v | w1) and
f (uv | w1). The number of occurrences of the triple (∗, ∗, right) is the same in f (u | v | w2)
and f (uv | w2). Suppose f (uv | w1) 6= f (uv | w2). If the number of occurrences of the triple
(∗, ∗, right) are diﬀerent in f (uv | w1) and f (uv | w2), then the number of occurrences of
the triple (∗, ∗, right) are diﬀerent in f (u | v | w1) and f (u | v | w2) and we are done. So
assume that the number of occurrences of the triple (∗, ∗, right) is the same in f (uv | w1)
and f (uv | w2). Let i be the ﬁrst position where f (uv | w1) and f (uv | w2) diﬀer.

Case 1: at position i, f (uv | w1) contains (∗, ∗, right) and f (uv | w2) contains a triple
whose origin is in u or v. If the ith triple in f (uv | w2) has origin in u, there will be a position
in f (u | vw2) that will have a triple whose origin is in u and the same position in f (u | vw1)
will have (∗, ∗, right) and we are done. If the ith triple in f (uv | w2) has origin in v, there
will be a position in f (u | v | w2) that will have a triple whose origin is in v and the same
position in f (u | v | w1) will have (∗, ∗, right) and we are done.

Case 2: at position i, f (uv | w2) contains (∗, ∗, right) and f (uv | w1) contains a triple
whose origin is in u or v. This can be handled similarly as above, with the role of w1 and w2
interchanged.

Case 3: at position i, f (uv | w1) contains a triple whose origin is in u and f (uv | w2)
contains a triple whose origin is in v. In this case, f (u | v | w1) will have a position with the
triple (∗, ∗, left) and the same position in f (u | v | w2) will have a triple whose origin is in
v and we are done.

Case 4: at position i, f (uv | w1) contains a triple whose origin is in v and f (uv | w2)

contains a triple whose origin is in u. This case can be handled similarly as above.

Case 5: at position i, both f (uv | w1) and f (uv | w2) has triples whose origin is in u
but the contents are diﬀerent. In this case, there will be a position where f (u | vw1) and
f (u | vw2) diﬀer and we are done.

Case 6: at position i, both f (uv | w1) and f (uv | w2) has triples whose origin is in v
but the contents are diﬀerent. In this case, there will be a position where f (u | v | w1) and
(cid:74)
f (u | v | w2) diﬀer and we are done.

The following result makes it easier to compute certain factored outputs.

(cid:73) Lemma 46. Suppose f is a transduction without data peeking, u, v are data words, σ ∈ Σ
and d ∈ D. The data values occurring in f (u | (σ, d) | v) are either d or those that are
f -memorable in u.

Proof. From the hypothesis that f is without data peeking, we infer that the data values
occurring in f (u | (σ, d) | v) are either d or those that occur in u. Suppose a data value

24

What You Must Remember When Transforming Datawords

e 6= d occurs in f (u | (σ, d) | v). Let e0 be a safe replacement for e in u. We have
f (u[e/e0] | (σ, d) | v) 6= f (u | (σ, d) | v), since e cannot occur in f (u[e/e0] | (σ, d) | v) but
it does occur in f (u | (σ, d) | v). Applying the contrapositive of point 1 in Lemma 43 to
the above inequality, we infer that f (u[e/e0] | (σ, d) · v) 6= f (u | (σ, d) · v). According to
(cid:74)
Deﬁnition 4, this certiﬁes that e is f -memorable in u.

The following result uses the binary relation ≡f from Deﬁnition 7 and equalizing schemes

from Deﬁnition 13.

(cid:73) Lemma 47. Suppose f is a transduction that is invariant under permutations, E is an
equalizing scheme for f and u, u0, v, w are data words. If u ≡f u0, then f (E(u)(u) | v | w) =
fz(E(u0)(u0) | v | w), where z = |u| − |u0|.

Proof. Since E(u)(u) ’ u, we have E(u)(u) ≡f u. So we infer that E(u)(u) ≡f u ≡f
u0 ≡f E(u0)(u0). Since ≡f is transitive, E(u)(u) ≡f E(u0)(u0). So we infer from Deﬁnition 7
that there exists a permutation π such that π(aiflf (E(u0)(u0))) = aiflf (E(u)(u)) and
f (E(u)(u) | v · w) = fz(π(E(u0)(u0)) | v · w). Since u ≡f u0, we infer from Deﬁnition 7 and
Deﬁnition 13 that aiflf (E(u0)(u0)) = aiflf (E(u)(u)), so π (and hence π−1) is identity
on those data values that are f -inﬂuencing in E(u0)(u0). Hence we infer from Lemma 41
that fz(π(E(u0)(u0)) | v · w) = fz(π−1 (cid:12) π(E(u0)(u0)) | v · w) = fz(E(u0)(u0) | v · w). Hence,
f (E(u)(u) | v · w) = fz(E(u0)(u0) | v · w). We infer from point 1 of Lemma 43 that
(cid:74)
f (E(u)(u) | v | w) = fz(E(u0)(u0) | v | w).

Suppose a SSRT is at a conﬁguration and reads a data word running a sequence of
transitions. If a permutation is applied to the conﬁguration and the data word, then the
new data word is read by the SSRT starting from the new conﬁguration running the same
sequence of transitions. This is formalized in the following result.

(cid:73) Lemma 48. Suppose S is a SSRT, the set of registers R is partitioned into two parts
R1, R2 and (q, val 1, n1), (q, val 2, n2) are conﬁgurations satisfying the following properties:

val 1 and val 2 coincide on R1,
for every r1, r2 ∈ R, val 1(r1) = val 1(r2) iﬀ val 2(r1) = val 2(r2) and
{val 1(r) | r ∈ R1} ∩ {val 1(r) | r ∈ R2} = ∅ = {val 2(r) | r ∈ R1} ∩ {val 2(r) | r ∈ R2}.
There exists a permutation π that is identity on {val 1(r) | r ∈ R1} such that for any data
word v, the sequence of transitions executed when reading v from (q, val 1) is same as the
sequence executed when reading π(v) from (q, val 2).

Proof. Let π be a permutation that is identity on {val 1(r) | r ∈ R1} such that for every
r2 ∈ R2, π(val 1(r2)) = val 2(r2). For every register r and every position i of v, val 1(r) =
(cid:74)
data(v, i) iﬀ val 2(r) = data(π(v), i). The result follows by a routine induction on |v|.

The next result says that if two strings belong to the same equivalence class of ≡f , then
they can be equalized by an equalizing scheme after which both will be transformed similarly
by any suﬃx. It uses the binary relation ≡S and the concept of arrangements of elements of
a set from Section 5.

(cid:73) Lemma 49. Suppose S is a SSRT implementing a transduction f , u1 ≡S u2, S reaches the
conﬁguration (q1, val 1, |u1|) after reading E(u1)(u1) and reaches (q2, val 2, |u2|) after reading
E(u2)(u2). For any data word v and any i, if the ith left block of f (E(u1)(u1) | v) is
val 1(χ) where χ is some arrangement of some subset X 0 ⊆ X, then the ith left block of
f (E(u2)(u2) | v) is val 2(χ).

M. Praveen

25

Proof. Since u1 ≡S u2, E(u1)(u1) ≡S E(u2)(u2), so q1 = q2, say q1 = q2 = q. For any
i, the ith f -inﬂuencing value is δi in both E(u1)(u1) and E(u2)(u2). From condition 3 of
Deﬁnition 36, we infer that val 1 and val 2 coincide on all the registers that store f -inﬂuencing
values. Suppose for the sake of contradiction that for some data word v and some i, the
ith left block of f (E(u1)(u1) | v) is val 1(χ) and the ith left block of f (E(u2)(u2) | v) is
val 2(χ0) 6= val 2(χ). This means that while reading v from (q, val 2), the sequence of transitions
is diﬀerent from the sequence when reading v from (q, val 1). This diﬀerence is due to the
diﬀerence between val 1 and val 2 in registers that don’t store f -inﬂuencing values. Hence,
we infer from Lemma 48 that there exists a permutation π that is identity on f -inﬂuencing
values such that the sequence of transitions executed when reading v from (q, val 1) is the
same sequence executed when reading π(v) from (q, val 2). Hence, the ith left block of
f (E(u2)(u2) | π(v)) is val 2(χ), which is diﬀerent from the ith left block of f (E(u2)(u2) | v),
which is val 2(χ0). Since f is invariant under permutations and without data peeking (from
(cid:74)
Proposition 34), this contradicts Lemma 42.

B

Proofs of Results in Section 3

Proof of Lemma 8. We have u ≡f u for all u, since the identity permutation satisﬁes all
the conditions of Deﬁnition 7. Hence, ≡f is reﬂexive.

Suppose u1 ≡f u2 and there exists a permutation π satisfying all the conditions of
Deﬁnition 7. We have aiflf (π(u2)) = aiflf (u1) and applying the permutation π−1 on both
sides gives us π−1(aiflf (π(u2))) = π−1(aiflf (u1)). Since f is invariant under permutations,
we infer from Lemma 39 that aiflf (u2) = aiflf (π−1(u1)). For any v, we have fz(π(u2) |
π(v)) = f (u1 | π(v)), where z = |u1| − |u2|. Applying π−1 on both sides and using Lemma 38,
we get fz(u2 | v) = f (π−1(u1) | v) for any v. Hence, λv.f (u2 | v) = λv.f−z(π−1(u1) | v). For
all data words u, v1, v2, we have f (u1 · π(u) | π(v1)) = f (u1 · π(u) | π(v2)) iﬀ f (π(u2) · π(u) |
π(v1)) = f (π(u2) · π(u) | π(v2)). Applying π−1 on both sides of both the equalities and using
Lemma 38, we get f (π−1(u1) · u | v1) = f (π−1(u1) · u | v2) iﬀ f (u2 · u | v1) = f (u2 · u | v2).
Hence, π−1 satisﬁes all the conditions of Deﬁnition 7, so u2 ≡f u1, so ≡f is symmetric.

Suppose u1 ≡f u2 and there exists a permutation π satisfying all the conditions of
Deﬁnition 7. Suppose u2 ≡f u3 and there exists a permutation π0 satisfying all the conditions
of Deﬁnition 7. Let π (cid:12) π0 be the composition of π and π0 (π (cid:12) π0(u) = π(π0(u)) for all u). It
is routine verify the following equalities: iflf (π (cid:12) π0(u3)) = iflf (u1), λv.fz+z0(π (cid:12) π0(u3) |
v) = f (u1 | v) where z = |u1| − |u2| and z0 = |u2| − |u3| and for all data words u, v1, v2,
f (u1 · u | v1) = f (u1 · u | v2) iﬀ f (π (cid:12) π0(u3) · u | v1) = f (π (cid:12) π0(u3) · u | v2). Hence ≡f is
(cid:74)
transitive.

C

Technical Details and Proofs of Results in Section 4

C.1 Recognizing Inﬂuencing Values

Proof of Lemma 10. Suppose d is f -memorable in u · (σ, e). There exists a data value d0
that is a safe replacement for d in u · (σ, e) and a data word v such that the next inequality
is true.

f ((u · (σ, e))[d/d0] | v) 6= f (u · (σ, e) | v)

f (u[d/d0] · (σ, e) | v) 6= f (u · (σ, e) | v)

[d 6= e]

f (u[d/d0] | (σ, e) · v) 6= f (u | (σ, e) · v)

[contrapositive of Lemma 43, point 2]

26

What You Must Remember When Transforming Datawords

The last inequality above shows that d is f -memorable in u.

Suppose d is f -vulnerable in u · (σ, e). Then there exist data words u0, v and a data
value d0 such that d doesn’t occur in u0, d0 is a safe replacement for d in u · (σ, e) · u0 · v
and f (u · (σ, e) · u0 | v[d/d0]) 6= f (u · (σ, e) · u0 | v). Since d doesn’t occur in u0 and d 6= e, d
doesn’t occur in (σ, e) · u0. We observe that f (u · ((σ, e) · u0) | v[d/d0]) 6= f (u · ((σ, e) · u0) | v)
(cid:74)
to conclude that d is f -vulnerable in u.

Proof of Lemma 12. By induction on |u|. The base case with |u| = 0 is trivial. As induction
hypothesis, suppose that after reading a data word u, the SSRT reaches the conﬁguration
(([u]f , ptr), val, |u|) such that val(ptr(i)) is the ith f -inﬂuencing value in u for all i ∈ [1, m],
where m = |iflf (u)|. Suppose the SSRT reads (σ, d) ∈ Σ × D next. We give the proof for
the case where d is not f -inﬂuencing in u and it is f -inﬂuencing in u · (σ, d). The other
cases are similar. Let m0 be the number of f -inﬂuencing values in E(u)(u) · (σ, δ0). We infer
from Lemma 11 that δ0 is f -inﬂuencing in E(u)(u) · (σ, δ0). We prove that the transition
from ([u]f , ptr) corresponding to i = 0 in Construction 14 can be executed. We infer from
Lemma 11 that u · (σ, d) ≡f E(u)(u) · (σ, δ0) so [u · (σ, d)]f = [E(u)(u) · (σ, δ0)]f , the next
state of the SSRT. The condition φ = Vj=m
j=1 ptr(j)6= is satisﬁed since d is not f -inﬂuencing
in u and for all j ∈ [1, m], val(ptr(j)) is the jth f -inﬂuencing value in u, which is not
equal to d. We infer from Lemma 11 that u · (σ, d) has m0 f -inﬂuencing values. For every
j ∈ [1, m], δj is f -inﬂuencing in E(u)(u) · (σ, δ0) iﬀ the jth f -inﬂuencing value in u (which
is assigned to ptr(j) by val) is f -inﬂuencing in u · (σ, d). Since δ0 is the 1st f -inﬂuencing
value in E(u)(u) · (σ, δ0), ptr 0(1) = rreuse as given in Construction 14. Since rreuse is the
ﬁrst register in the set R \ {ptr(l) | 1 ≤ l ≤ m, δl is f -inﬂuencing in E(u)(u) · (σ, δ0)}, rreuse
is the ﬁrst register that is not holding a data value that is f -inﬂuencing in u and in u · (σ, d).
Since R0 = {rreuse}, the transition of the SSRT changes the valuation to val 0 such that
val 0(rreuse) = d. So val 0(ptr 0(1)) = val 0(rreuse) = d, the ﬁrst f -inﬂuencing value in u · (σ, d).
Suppose j ∈ [2, m0] and the jth f -inﬂuencing value in E(u)(u) · (σ, δ0) is δk, the kth f -
inﬂuencing value in E(u)(u) (this will be true for some k, by Lemma 10). Since R = {rreuse},
val and val 0 coincide on all registers except rreuse. Since rreuse is the ﬁrst register in the set
R \ {ptr(l) | 1 ≤ l ≤ m, δl is f -inﬂuencing in E(u)(u) · (σ, δ0)}, rreuse 6= ptr(k) and val and
val 0 coincide on ptr(k). Hence, val 0(ptr(k)) = val(ptr(k)). Since the jth f -inﬂuencing value
in E(u)(u) · (σ, δ0) is δk, the kth f -inﬂuencing value in E(u)(u), we infer from Lemma 11
that the jth f -inﬂuencing value in u · (σ, d) is the kth f -inﬂuencing value in u. Hence,
val 0(ptr 0(j)) = val 0(ptr(k)) = val(ptr(k)), which is the kth f -inﬂuencing value in u and the
jth f -inﬂuencing value in u · (σ, d). The ﬁrst equality above follows since ptr 0(j) = ptr(k) as
(cid:74)
given in Construction 14.

C.2 Computing Transduction Outputs

(cid:73) Lemma 50. Suppose f is a transduction that is invariant under permutations and without
data peeking and E1, E2 are equalizing schemes. Suppose a set V = {v1, v2, . . .} has the follow-
ing property: for any i 6= j, there exists ui,j such that f (E1(ui,j)(ui,j) | vi) 6= f (E1(ui,j)(ui,j) |
vj). Then there exists a set V 0 = {v0
2, . . .} of the same cardinality as V such that for any
i 6= j, f (E2(ui,j)(ui,j) | v0
j). For any i 6= j, the same ui,j works for
both V and V 0; we use the equalizing scheme E1 for V and E2 for V 0.

1, v0
i) 6= f (E2(ui,j)(ui,j) | v0

Proof. Let δ1δ2 · · · be the sequence of data values such that for every data word u and
every i ∈ [1, |iflf (u)|], the ith f -inﬂuencing data value of E1(u)(u) is δi. Let η1η2 · · · be
the sequence of data values such that for every data word u and every i ∈ [1, |iflf (u)|],
the ith f -inﬂuencing data value of E2(u)(u) is ηi. Let π be a permutation such that

M. Praveen

27

π(δ1δ2 · · · ) = η1η2 · · · . Let V 0 = {π(vi) | vi ∈ V }. We will show that V 0 satisﬁes the
condition of the lemma.

For any i 6= j, let ui,j be a data word such that f (E1(ui,j)(ui,j) | vi) 6= f (E1(ui,j)(ui,j) |
vj). Applying the permutation E2(ui,j)·E−1
1 (ui,j) to both sides and using Lemma 38, we infer
that f (E2(ui,j)(ui,j) | E2(ui,j) · E−1
1 (ui,j)(vj)).
Suppose iflf (E2(ui,j)(ui,j)) = η1 · · · ηr. We will prove that there exist permutations
πi, πj such that they are identity on η1 · · · ηr, πi (cid:12) E2(ui,j) (cid:12) E−1
1 (ui,j)(vi) = π(vi) and
πj (cid:12) E2(ui,j) (cid:12) E−1

1 (ui,j)(vi)) 6= f (E2(ui,j)(ui,j) | E2(ui,j) · E−1

1 (ui,j)(vj) = π(vj). Then, using Lemma 42, we get

f (E2(ui,j)(ui,j) | π(vi)) = f (E2(ui,j)(ui,j) | πi · E2(ui,j) · E−1

1 (ui,j)(vi))

= f (E2(ui,j)(ui,j) | E2(ui,j) · E−1
6= f (E2(ui,j)(ui,j) | E2(ui,j) · E−1
= f (E2(ui,j)(ui,j) | πj · E2(ui,j) · E−1

1 (ui,j)(vi))
1 (ui,j)(vj))

1 (ui,j)(vj))

= f (E2(ui,j)(ui,j) | π(vj)) .

Now we will prove that there exists a permutation πi such that it is identity on η1 · · · ηr
1 (ui,j)(vi) = π(vi). Let iflf (ui,j) = d1 · · · dr. For all i ∈ {1, . . . , r},
1 (ui,j) : δi 7→ di, E2(ui,j)(ui,j) : di 7→ ηi. Deﬁne πi such that πi : ηi 7→ ηi.
1 (ui,j) : δ 7→ d and E2(ui,j) : d 7→ η0. Deﬁne πi
1 (ui,j)(vi) = π(vi).
(cid:74)

and πi · E2(ui,j) · E−1
π : δi 7→ ηi and E−1
For δ /∈ {δ1, . . . , δr}, suppose π : δ 7→ η, E−1
such that πi : η0 7→ η. Now πi is identity on η1 · · · ηr and πi · E2(ui,j) · E−1
The existence of πj can be proved similarly.

Proof of Lemma 16. Suppose for the sake of contradiction that ≡E
f has inﬁnite index. Then,
there is an inﬁnite set {vi}i≥1 of data words such that for any j 6= k, there exists a data word
uk,j such that f (E(uk,j)(uk,j) | vk) 6= f (E(uk,j)(uk,j) | vj). Let us say that a set U of data
words covers a set V ⊆ {vi}i≥1 using E if for every v, v0 ∈ V , there exists E(u)(u) ∈ U such
that f (E(u)(u) | v) 6= f (E(u)(u) | v0). Since ≡f has ﬁnite index, at least one equivalence
class of ≡f (say U ) covers an inﬁnite subset (say V ) of {vi}i≥1.

We claim that for any v0

Now we build another equalizing scheme E0 as follows. Fix an arbitrary data word u ∈ U .
We infer from Deﬁnition 7 that for every v ∈ U \ {u}, there exists a permutation πv such
that aiflf (π(v)) = aiflf (u). Let E0 be an equalizing scheme such that E0(u) is the identity
permutation and for all v ∈ U \ {u}, E0(v) = πv. From Lemma 50, we infer that there exists
an inﬁnite set V 0 that is covered by U using E0.
j ∈ V 0, f (u | v0

we infer that there exists a data word E0(ui,j)(ui,j) ∈ U such that f (E0(ui,j)(ui,j) | v0
f (E0(ui,j)(ui,j) | v0
Deﬁnition 7 that there exists a permutation πi,j such that f (πi,j(ui,j) | v0
iﬀ f (u | v0
and f (E0(ui,j)(ui,j) | v0
proving the claim.

j). Since U covers V 0 using E0,
i) 6=
j). Since u ≡f ui,j ≡f E0(ui,j)(ui,j), we infer from the third condition of
i) 6= f (πi,j(ui,j) | v0
j)
j). Since we chose the equalizing scheme E0 such that πi,j = E0(ui,j)
j),

j), we conclude that f (u | v0

i) 6= f (E0(ui,j)(ui,j) | v0

i) 6= f (u | v0

i) 6= f (u | v0

i) 6= f (u | v0

i 6= v0

Now, {f (u | v0) | v0 ∈ V 0} is an inﬁnite set. Since there is no data peeking in f , f (u | v0)
contains data values only from u for any v0 ∈ V 0. Hence, the only way {f (u | v0) | v0 ∈ V 0}
can be inﬁnite is that there is no bound on the length of the factored outputs in that set.
Since there are a ﬁxed number of positions in u, this contradicts the fact that f has linear
(cid:74)
blow up. Hence, ≡E

f has ﬁnite index.

Proof of Lemma 18. Suppose for the sake of contradiction that there is no such bound
B. Then there is an inﬁnite family of pairs of data words (u1, v1), (u2, v2), . . . such that

28

What You Must Remember When Transforming Datawords

for all i ≥ 1, f (ui | vi) has at least i left blocks. Applying any permutation to f (ui | vi)
will not change the number of left blocks. From Lemma 38, we infer that for all i ≥ 1,
f (E(ui)(ui) | E(ui)(vi)) has at least i left blocks. Since ≡E
f has ﬁnite index, there is at least
one equivalence class of ≡E
f that contains E(ui)(vi) for inﬁnitely many i. Let v be a data
word from this equivalence class. From the deﬁnition of ≡E
f (Deﬁnition 15), we infer that
for inﬁnitely many i, f (E(ui)(ui) | v) has at least i left blocks. Hence, for inﬁnitely many i,
f (E(ui)(ui) | v) has at least (i − 1) right blocks. Triples in the right blocks have origin in v.
Since the number of positions in v is bounded, this contradicts the hypothesis that f has
(cid:74)
linear blow up.

Proof of Lemma 20. Since E(u · (σ, d))−1(v) and E(u)−1(π(v)) are obtained from applying
diﬀerent permutations to v, they are isomorphic. We will prove that for all j and i ≥ 2, if
the jth position of E(u · (σ, d))−1(v) contains the ith f -inﬂuencing value of u · (σ, d), then
the same is contained in the jth position of E(u)−1(π(v)).
1. The jth position of E(u · (σ, d))−1(v) contains the ith f -inﬂuencing value of u · (σ, d).
2. Hence, the jth position of v contains δi, by deﬁnition of equalizing schemes (Deﬁnition 13).
3. For i ≥ 2, the ith f -inﬂuencing value of u · (σ, d) is among {dm, . . . , d1}, the f -inﬂuencing

values in u, by Lemma 10.

4. Say dk is the ith f -inﬂuencing value of u · (σ, d). Then δk is the ith f -inﬂuencing value of

E(u0)(u0) · (σ, η), by Lemma 11.

5. The permutation π maps δi to δk, by Deﬁnition 19.
6. The permutation E(u)−1 maps δk to dk, by deﬁnition of equalizing schemes (Deﬁnition 13).
7. The jth position of E(u)−1(π(v)) contains dk, by points (2), (5) and (6) above.
8. By point (4) above, dk is the ith f -inﬂuencing value of u · (σ, d), so the jth position of

E(u)−1(π(v)) contains the ith f -inﬂuencing value of u · (σ, d).

Suppose (d, η) ∈ {(di, δi) | i ∈ {1, . . . , m}} or (d, η) = (d0, δ0) and the ﬁrst f -inﬂuencing
value in u · (σ, d) is among {dm, . . . , d1}. Then we can put i ≥ 1 in the above reasoning
to infer that for all j and i ≥ 1, if the jth position of E(u · (σ, d))−1(v) contains the ith
f -inﬂuencing value of u · (σ, d), then the same is contained in the jth position of E(u)−1(π(v)).
Hence, we get the following equality.

f (u · (σ, d) | E(u · (σ, d))−1(v)) = f (u · (σ, d) | E(u)−1(π(v)))

[Lemma 42]

f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u | (σ, d) · E(u)−1(π(v)))

[Lemma 43, point 2]

f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u | E(u)−1((σ, η) · π(v)))

[E(u)(d) = η]

Suppose (d, η) = (d0, δ0) and the ﬁrst f -inﬂuencing value in u · (σ, d) is d. Then π maps
δ1 to η = δ0. Let π0 be the permutation that interchanges E(u)−1(η) and d and doesn’t
change any other value. For all j and i ≥ 1, if the jth position of E(u · (σ, d))−1(v) contains
the ith f -inﬂuencing value of u · (σ, d), then the same is contained in the jth position of
π0 (cid:12) E(u)−1(π(v)). Hence, we get the following equality.

f (u · (σ, d) | E(u · (σ, d))−1(v)) = f (u · (σ, d) | π0 (cid:12) E(u)−1(π(v)))

[Lemma 41]

f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u | (σ, d) · π0 (cid:12) E(u)−1(π(v)))

[Lemma 43, point 2]

Since η = δ0 does not occur in {δm, . . . , δ1}, E(u)−1(η) does not occur in {dm, . . . , d1},
the f -inﬂuencing data values in u. Since d also does not occur in {dm, . . . , d1}, π0 only
interchanges two data values that are not f -inﬂuencing in u and doesn’t change any
other value. So we infer from Lemma 42 that f (u | (σ, d) · π0 (cid:12) E(u)−1(π(v))) = f (u |

M. Praveen

29

π0((σ, d)) · π0 (cid:12) π0 (cid:12) E(u)−1(π(v))) = f (u | E(u)−1((σ, η)) · E(u)−1(π(v))) = f (u | E(u)−1((σ, η) · π(v))).
Combining this with the equality above, we get f (u | (σ, d) · E(u · (σ, d))−1(v)) = f (u |
E(u)−1((σ, η) · π(v))).

Now we will prove the statements about f (u | (σ, d) | E(u · (σ, d))−1(v)). Let g be a

function such that for i ≥ 2, the ith f -inﬂuencing value in E(u)(u) · (σ, η) is δg(i).

Case 1: (d, η) ∈ {(di, δi) | i ∈ [1, m]}. Let iflf (E(u · (σ, d))(u · (σ, d))) = δr · · · δ1.
We will ﬁrst prove that E(u) (cid:12) E(u · (σ, d))−1 coincides with π on δr, . . . , δ1. For i ≥ 2,
E(u · (σ, d))−1(δi) is the ith f -inﬂuencing value in u · (σ, d) and we infer from Lemma 11
that the ith f -inﬂuencing value in u · (σ, d) is dg(i), the g(i)th f -inﬂuencing value in u
(since the ith f -inﬂuencing value in E(u)(u) · (σ, η) is δg(i), the g(i)th f -inﬂuencing value in
E(u)(u)). By Deﬁnition 13, E(u) maps dg(i) to δg(i). Hence, for i ≥ 2, E(u) (cid:12) E(u · (σ, d))−1
maps δi to δg(i), which is exactly what π does to δi. Say the ﬁrst f -inﬂuencing value in
E(u0)(u0) · (σ, η) is δj. We infer from Lemma 11 that the ﬁrst f -inﬂuencing value in u · (σ, d)
is dj. Hence, E(u) (cid:12) E(u · (σ, d))−1 maps δ1 to δj, which is exactly what π does to δ1.
Hence, E(u) (cid:12) E(u · (σ, d))−1 coincides with π on δr, . . . , δ1, the f -inﬂuencing values of
E(u · (σ, d))(u · (σ, d)).

E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v)))

= E(u) (cid:12) E(u · (σ, d))−1 (cid:12) E(u · (σ, d))(f (u | (σ, d) | E(u · (σ, d))−1(v)))

= E(u) (cid:12) E(u · (σ, d))−1(f (E(u · (σ, d))(u) | E(u · (σ, d))(σ, d)) | v)

= f (E(u)(u) | E(u)(σ, d) | π(v))
= fz(E(u0)(u0) | (σ, η) | π(v))

[Lemma 38]

[Lemma 44]

[Lemma 47]

In the last inequality above, apart from Lemma 47, we also use the fact that E(u)(d) =
E(u)(di) = δi = η. So we get E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v))) = fz(E(u0)(u0) | (σ, η) |
π(v)), concluding the proof for this case.

Case 2: (d, η) = (d0, δ0). Let π1 be any permutation satisfying the following conditions:

For i ≥ 2, π1(δi) = π(δi),

if the ﬁrst f -inﬂuencing value in E(u0)(u0) · (σ, η) is δj for some j ≥ 1, then π1(δ1) = π(δ1)
and

if the ﬁrst f -inﬂuencing value in E(u0)(u0) · (σ, η) is η = δ0, then π1(δ1) = E(u)(d) =
E(u)(d0).

As seen in case 1, E(u)·E(u·(σ, d))−1 coincides with π1 on δr, . . . , δ2. If the ﬁrst f -inﬂuencing
value in E(u)(u) · (σ, η) is δj for some j ≥ 1, then again as in case 1, E(u) · E(u · (σ, d))−1
coincides with π1 on δ1. If the ﬁrst f -inﬂuencing value in E(u)(u) · (σ, η) is η, we infer
from Lemma 11 that the ﬁrst f -inﬂuencing value in u · (σ, d) is d, so E(u · (σ, d))−1 maps
δ1 to d. In this case, π1(δ1) = E(u)(d), so E(u) · E(u · (σ, d))−1 coincides with π1 on δ1.
So E(u) · E(u · (σ, d))−1 coincides with π1 on δr, . . . , δ1. Hence, similar to case 1, we get
E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v))) = fz(E(u0)(u0) | E(u)(σ, d) | π1(v)).

Recall that δ0 is a data value that is not f -inﬂuencing in E(u0)(u0) and does not occur in
{δm, . . . , δ1}. Let π0 be the permutation that interchanges δ0 and E(u)(d) and doesn’t change
any other value. Since d is not f -inﬂuencing in u, E(u)(d) does not occur in {δm, . . . , δ1}.
Since the f -inﬂuencing values of E(u0)(u0) are δm, . . . , δ1 and neither δ0 nor E(u)(d) occur

30

What You Must Remember When Transforming Datawords

in {δm, . . . , δ1}, we get the following:

f (E(u0)(u0) | (σ, δ0) · π0 (cid:12) π1(v)) = f (π0 (cid:12) E(u0)(u0) | (σ, δ0) · π0 (cid:12) π1(v))
f (E(u0)(u0) | (σ, δ0) | π0 (cid:12) π1(v)) = f (π0 (cid:12) E(u0)(u0) | (σ, δ0) | π0 (cid:12) π1(v))

[Lemma 41]

[Lemma 43, point 1]

E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v))) = fz(E(u0)(u0) | E(u)(σ, d) | π1(v))
π0 (cid:12) E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v))) = π0(fz(E(u0)(u0) | E(u)(σ, d) | π1(v)))

[apply π0 on both sides]

= fz(π0 (cid:12) E(u0)(u0) | (σ, δ0) | π0 (cid:12) π1(v))
= fz(E(u0)(u0) | (σ, δ0) | π0 (cid:12) π1(v))

[Lemma 38]

[second equality above]

For i ∈ {1, . . . , r}, π maps δi to the ith f -inﬂuencing value in E(u0)(u0) · (σ, η) by deﬁnition.
We will prove that π0 (cid:12) π1 does exactly the same on δ1, . . . , δr. For i ≥ 2, π maps δi to the
ith f -inﬂuencing value in E(u0)(u0) · (σ, η), which is among δm, . . . , δ1. By deﬁnition, π1 also
maps δi to the ith f -inﬂuencing value in E(u0)(u0) · (σ, η), and π0 doesn’t change this value,
since neither E(u)(d) nor δ0 are among δm, . . . , δ1. The permutation π maps δ1 to the ﬁrst
f -inﬂuencing value in E(u0)(u0) · (σ, η). If this ﬁrst f -inﬂuencing value is δj for some j ≥ 1,
then, by deﬁnition, π1 also maps δ1 to the ﬁrst f -inﬂuencing value in E(u0)(u0) · (σ, η), and
π0 doesn’t change this value, since neither E(u)(d) nor δ0 are among δm, . . . , δ1. If the ﬁrst
f -inﬂuencing value in E(u0)(u0) · (σ, η) is η = δ0, then π maps δ1 to δ0. By deﬁnition, π1
maps δ1 to E(u)(d) and π0 maps E(u)(d) to δ0. Hence, π0 (cid:12) π1 maps δ1 to δ0. Therefore, for
i ∈ {1, . . . , r}, both π and π0 (cid:12) π1 map δi to the ith f -inﬂuencing value in E(u0)(u0) · (σ, η).
Hence, we can apply Lemma 42 to get the next equality.

f (E(u0)(u0) · (σ, δ0) | π0 (cid:12) π1(v)) = f (E(u0)(u0) · (σ, δ0) | π(v))
f (E(u0)(u0) | (σ, δ0) | π0 (cid:12) π1(v)) = f (E(u0)(u0) | (σ, δ0) | π(v))

[Lemma 43, point 4]

Hence π0 (cid:12) E(u)(f (u | (σ, d) | E(u · (σ, d))−1(v))) = fz(E(u0)(u0) | (σ, δ0) | π(v)), concluding
(cid:74)
the proof for this case.

C.3 Dependency Trees

Proof of Lemma 25. Suppose [v]E
f is an equivalence class and θv, θ are as explained in
Deﬁnition 24. If d is the ith f -inﬂuencing value in u for some i ≥ 1, let η = δi and let
η = δ0 otherwise. Let u0 be an arbitrary data word in [u]f . We have from Lemma 11
that u · (σ, d) ≡f E(u0)(u0) · (σ, η), so pref (θ) = [E(u0)(u0) · (σ, η)]f = [u · (σ, d)]f as
required. We have from Lemma 20 that f (u | (σ, d) | E(u · (σ, d))−1(v)) is equal to either
E(u)−1(fz(E(u0)(u0) | (σ, η) | π(v))) or E(u)−1 (cid:12) π0(fz(E(u0)(u0) | (σ, η) | π(v))). Hence,
f (u | (σ, d) | E(u · (σ, d))−1(v)) and fz(E(u0)(u0) | (σ, η) | π(v)) are isomorphic. Hence,
the ith left block of f (u · (σ, d) | E(u · (σ, d))−1(v)) is the concretization of z, the ith non-
right block of f (E(u0)(u0) | (σ, η) | π(v)), as deﬁned in Deﬁnition 24. We will prove that
val 0(ur(θ, bl(θ, i))) is the concretization of z, which is suﬃcient to complete the proof.

Indeed, val 0(ur(θ, bl(θ, i))) = val 0(ur(θ, z0)), where z0 is obtained from z by replacing
jth left block by Pj and kth middle block by hθ, ki. Since we set val 0(hθ, ki) to be the kth
middle block of f (u | (σ, d) | E(u · (σ, d))−1(v)), val 0(ur(θ, bl(θ, i))) correctly concretizes the
middle blocks. Since ur(θ, Pj) = ur(θv, bl(θv, j)) and θv is a node in the original tree T ,
we infer that val(ur(θv, bl(θv, j))) is the jth left block of f (u | E(u)−1((σ, η) · π(v))). Since
val and val 0 diﬀer only in the variables hθ, ki where θ is newly introduced, we infer that
val 0(ur(θv, bl(θv, j))) = val(ur(θv, bl(θv, j))) is the jth left block of f (u | E(u)−1((σ, η) · π(v))).
From Lemma 20, we infer that the jth left block of f (u | E(u)−1((σ, η) · π(v))) is equal to

M. Praveen

31

the jth left block of f (u | (σ, d) · E(u · (σ, d))−1(v)). Hence, val 0(ur(θ, bl(θ, j))) correctly
(cid:74)
concretizes the left blocks.

f , bl(θ · [v]E

We get ur(θ · [v]E

f , bl(θ (cid:56) ·[v]E

f , i)) = ur(θ · [v]E

Proof of Lemma 27. Suppose T 0 is obtained from T by removing a node θ and making the
only child of θ a child of θ’s parent. If the only child of θ is θ · [v]E
f , we will prove that for all
i ∈ [1, B], ur(θ (cid:56) ·[v]E
f , i)). This will imply that the
unrolling of any block description in any leaf remains unchanged due to the shortening, so the
lemma will be proved. First we will prove that ur(θ (cid:56) ·[v]E
f , bl(θ, j)) = ur(θ, bl(θ, j)). Indeed,
both are obtained from bl(θ, j) by replacing every occurrence of Pk by ur(θ (cid:56), bl(θ (cid:56), k)).
f , i) by replacing every occurrence of Pj by
ur(θ, bl(θ, j)). We will prove that we also get ur(θ (cid:56) ·[v]E
f , i)
by replacing every occurrence of Pj by ur(θ, bl(θ, j)), which is suﬃcient to prove the lemma.
f , i) by replacing every occurrence
f , i))
f , i) by ﬁrst replacing every occurrence of Pj by bl(θ, j), which is then replaced
f , bl(θ (cid:56)
(cid:74)

of Pj by bl(θ, j), as given in Deﬁnition 26. Hence, we get ur(θ (cid:56) ·[v]E
from bl(θ · [v]E
by ur(θ (cid:56) ·[v]E
·[v]E

f , bl(θ, j)) = ur(θ, bl(θ, j)). Hence, for all i ∈ [1, B], ur(θ (cid:56) ·[v]E

f , i) is obtained from bl(θ · [v]E

Recall that bl(θ (cid:56) ·[v]E

f , i)) from bl(θ · [v]E

f , i)) from bl(θ · [v]E

f , bl(θ (cid:56) ·[v]E

f , bl(θ (cid:56) ·[v]E

f , bl(θ · [v]E

f , i)) = ur(θ · [v]E

f , bl(θ · [v]E

f , i)).

Proof of Lemma 30. Suppose all leaves in T are labeled with [u]f by pref . Then all leaves
in T1 (and hence in T2 and T3) are labeled by [u · (σ, η)]f . All paths in T2 (and hence in T3)
are of length at most |(Σ × D)∗/ ≡E
f | + 1: if there are longer paths, there will be at least
|(Σ × D)∗/ ≡E
f | + 1 leaves since each internal node has at least two children. However, this
is not possible since T2 has only one leaf for every equivalence class of ≡E
f . In T3, for any
node θ and any i ∈ [1, B], bl(θ, i) will only contain elements from Xθ and P, as ensured in
the trimming process in Deﬁnition 28. There are at most B parent references, each of which
occurs at most once in bl(θ, i) for at most one i ∈ [1, B]. Since every non-parent block is
replaced by a data word variable in the trimming process, each bl(θ, i) is of length at most
2B + 1. Each bl(θ, i) has at most (B + 1) data word variables and i ∈ [1, B], so at most
(B2 + B) data word variables are suﬃcient for the block descriptions in θ. Hence, T3 is
(cid:74)
reduced.

Proof of Lemma 32. Since S is an extension of the SSRT constructed in Construction 14,
the claim about the pointer function ptr comes from Lemma 12. For the We will prove that
(T, val) is complete for u by induction on |u|. For the base case, |u| = 0 and we infer that
(([(cid:15)]f , ptr ⊥, T⊥), val (cid:15)) is complete for u = (cid:15) by deﬁnition. We inductively assume that after
reading u, S reaches the conﬁguration (([u]f , ptr, T ), val, |u|) such that val(ptr(i)) is the ith
f -inﬂuencing value in u and (T, val) is complete for u. Suppose the next symbol read by the
SSRT is (σ, d) and m = |iflf (u)|.

If d is the ith f -inﬂuencing value in u for some i ≥ 1, let η = δi and let η = δ0 otherwise.
Let π be a permutation tracking inﬂuencing values on E(u0)(u0)·(σ, η) as given in Deﬁnition 19.
Suppose T1 is the (σ, η) extension of T , T2 is obtained from T1 by shortening it as much as
possible and T 0 is the trimming of T2. Let ud 1 be the function as deﬁned in Construction 31. If
S had the transition (([u]f , ptr, T ), σ, φ, ([E(u0)(u0) · (σ, η)]f , ptr 0, T1), R0, ud 1), S would read
(σ, d) and reach the conﬁguration (([E(u0)(u0) · (σ, η)]f , ptr 0, T1), val 1, |u| + 1). We will prove
that (T1, val 1) is complete for u·(σ, d). This can be inferred from Lemma 25 if val 1 is the (σ, d)
extension of (T, val). This can be inferred if val 1 is obtained from val by setting val 1(hθ, ki) to
the kth middle block of f (u | (σ, d) | E(u · (σ, d))−1(v)) for every leaf θ = θv ·[v]E
f that is newly
added while extending T to T1. This can be inferred from Lemma 20 if val 1(hθ, ki) is set to
z1, the kth middle block of E(u)−1(fz(E(u0)(u0) | (σ, η) | π(v))) if η = δi for some i ∈ [1, m]
and val 1(hθ, ki) is set to z2, the kth middle block of E(u)−1 (cid:12) π0(fz(E(u0)(u0) | (σ, η) | π(v)))

32

What You Must Remember When Transforming Datawords

if η = δ0, where z = |u| − |u0| and π0 is the permutation that interchanges δ0 and E(u)(d)
and doesn’t change any other value. From the semantics of SSRTs, we infer that the third
component in every triple of val 1(hθ, ki) is |u| + 1, as required. Hence, it remains to prove
that (cid:22)2 (val 1(hθ, ki)) =(cid:22)2 (z1) if η = δi for some i ∈ [1, m] and (cid:22)2 (val 1(hθ, ki)) =(cid:22)2 (z2) if
η = δ0.

From Lemma 46, we infer that all data values in fz(E(u0)(u0) | (σ, η) | π(v)) are among
{δ0, . . . , δm}. Hence, we get z1 and z2 from the kth middle block of fz(E(u0)(u0) | (σ, η) | π(v))
by replacing every occurrence of δj for j ∈ [1, m] by E(u)−1(δj) (which is the jth f -inﬂuencing
value in u) and replacing every occurrence of δ0 by E(u)−1 (cid:12) π0(δ0) (which is d). This
exactly what the update function ud 1 does to hθ, ki: it is set to the kth middle block of
fz(E(u0)(u0) | (σ, η) | π(v)) and every occurrence of δj is replaced by ptr(j) (the transition of
S then replaces this with val(ptr(j)), the jth f -inﬂuencing value in u) and every occurrence
of δ0 is replaced by curr (the transition of S then replaces this with d, the current data value
being read). Hence, (T1, val 1) is complete for u · (σ, d).

Since T2 is obtained from T1 by shortening it as much as possible, we infer from Lemma 27
that (T2, val 1) is complete for u·(σ, d). The actual transition in S is (([u]f , ptr, T ), σ, φ, ([E(u0)(u0)·
(σ, η)]f , ptr 0, T 0), R0, ud). After reading (σ, d), S goes to the conﬁguration (([u·(σ, η)]f , ptr 0, T 0), val 0, |u|+
1) where val 0 is the trimming of val 1 (due to the way ud is deﬁned from ud 1). Since T 0 is the
trimming of T2, we conclude from Proposition 29 that (T 0, val 0) is complete for u · (σ, d). (cid:74)

D Technical Details and Proofs of Results in Section 5

Proof of Lemma 35. Suppose a data value d is not stored in any of the registers after
reading u. We will prove that d is neither f -memorable nor f -vulnerable in u. To prove that
d is not f -memorable in u, we will show that for any data word v and any safe replacement
d0 for d in u, f (u[d/d0] | v) = f (u | v). Indeed, let π be the permutation that interchanges
d and d0 and that doesn’t change any other value. We have u[d/d0] = π(u). Suppose S
reaches the conﬁguration (q, val) after reading u. We infer from Lemma 33 that S reaches
the conﬁguration (q, π(val)) after reading π(u). Since d is not stored in any of the registers
under the valuation val, π(val) coincides with val on all registers. Hence, if S executes
a sequence of transitions reading a data word v from the conﬁguration (q, val), the same
sequence of transitions are executed reading v from (q, π(val)). Since f (u[d/d0] | v) and
f (u | v) depends only on the sequence of transitions that are executed while reading v, we
infer that f (u[d/d0] | v) = f (u | v).

Next we will prove that if a data value d is not stored in any of the registers after reading
u, then d is not f -vulnerable in u. Let u0, v be data words and d0 be a data value such that d
doesn’t occur in u0. Since d is not stored in any of the registers after reading u and d doesn’t
occur in u0, d is not stored in any of the registers after reading u · u0. Suppose d0 is a safe
replacement for d in u · u0 · v. Then d0 doesn’t occur in u · u0 so neither d0 nor d is stored in
any of the registers after reading u · u0. Since d0 doesn’t occur in v, v ’ v[d/d0]. Hence the
SSRT executes the same sequence of transitions for reading u · u0 · v and for u · u0 · v[d/d0].
Hence, the only diﬀerence between f (u · u0 · v) and f (u · u0 · v[d/d0]) is that at some positions
whose origin is not in u · u0, the ﬁrst one may contain d and the second one may contain
d0. Since such positions are abstracted out, f (u · u0 | v[d/d0]) = f (u · u0 | v). Hence, d is not
(cid:74)
f -vulnerable in u.

Proof of Lemma 37. We will prove that ≡S reﬁnes ≡f . Suppose u1, u2 are data words
such that u1 ≡S u2 and S reaches the conﬁgurations (q, val 1), (q, val 2) after reading u1,u2
respectively. Let π be a permutation such that for every register r, π(val 2(r)) = val 1(r). We

M. Praveen

33

can verify by a routine induction on |u2| that after reading π(u2), S reaches the conﬁguration
(q, π(val 2)). We infer from Lemma 35 that all f -inﬂuencing values of u1 are stored in registers
in the conﬁguration (q, val 1) and all f -inﬂuencing values of π(u2) are stored in registers in
the conﬁguration (q, π(val 2)). The valuations π(val 2) and val 1 coincide on all the registers.
Hence, we can infer from condition 3 of Deﬁnition 36 that aiflf (π(u2)) = aiflf (u1).

Since π(val 2) and val 1 coincide on all the registers, for any data word v, the sequence of
transitions executed when reading v from the conﬁguration (q, val 1) and from (q, π(val 2))
are the same. Hence, fz(π(u2) | v) = f (u1 | v), where z = |u1| − |u2|.

Let u, v1, v2 be data words. To ﬁnish the proof, we have to show that f (u1 · u | v1) =
f (u1 · u | v2) iﬀ f (π(u2) · u | v1) = f (π(u2) · u | v2). Any left factor of f (u1 | u · v1) is of the
form val 1(χ), where χ1 is some arrangement of some subset X1 ⊆ X. Since val 1 and π(val 2)
coincide on all the registers and val 1(x) = (cid:15) iﬀ π(val 2)(x) = (cid:15) for all data word variables
x ∈ X (by condition 4 of Deﬁnition 36), it can be routinely veriﬁed that f (u1 | u · v1) and
f (π(u2) | u · v1) have the same number of left blocks and right blocks. If the ith left block
of f (u1 | u · v1) is val 1(χ), then the ith left block of f (π(u2) | u · v1) is π(val 2)(χ). We will
assume that f (u1 · u | v1) 6= f (u1 · u | v2) and show that f (π(u2) · u | v1) 6= f (π(u2) · u | v2).
The proof of the converse direction is symmetric.
It is suﬃcient to prove that either
f (π(u2) | u · v1) 6= f (π(u2) | u · v2) or f (π(u2) | u | v1) 6= f (π(u2) | u | v2); we can infer from
the contrapositive of point 3 or point 4 of Lemma 43 respectively that f (π(u2) · u | v1) 6=
f (π(u2) · u | v2). Since f (u1 · u | v1) 6= f (u1 · u | v2), we infer from the contrapositive of
Lemma 45 that either f (u1 | u · v1) 6= f (u1 | u · v2) or f (u1 | u | v1) 6= f (u1 | u | v2).

Case 1: f (u1 | u · v1) 6= f (u1 | u · v2). If the number of left blocks in f (u1 | u · v1) is
diﬀerent from the number of left blocks in f (u1 | u · v2), then the number of left blocks in
f (π(u2) | u · v1) is diﬀerent from the number of left blocks in f (π(u2) | u · v2) and we are
done. Suppose f (u1 | u · v1) and f (u1 | u · v2) have the same number of left blocks but the
ith left blocks are diﬀerent. Suppose the ith left block of f (u1 | u · v1) is val 1(χ1) and the ith
left block of f (u1 | u · v2) is val 1(χ2), where χ1, χ2 are some arrangements of some subsets
X1, X2 ⊆ X respectively. The ith left block of f (π(u2) | u · v1) is π(val 2)(χ1) and the ith left
block of f (π(u2) | u · v2) is π(val 2)(χ2). Since val 1(χ1) 6= val 1(χ2), we infer from condition 5
of Deﬁnition 36 that π(val 2)(χ1) 6= π(val 2)(χ2). Hence, the ith left blocks of f (π(u2) | u · v1)
and f (π(u2) | u · v2) are diﬀerent and we are done.

Case 2: f (u1 | u | v1) 6= f (u1 | u | v2). As we have seen in the second paragraph of this
proof, fz(π(u2) | u · v1) = f (u1 | u · v1) and fz(π(u2) | u · v2) = f (u1 | u · v2). We infer from
point 1 of Lemma 43 that fz(π(u2) | u | v1) = f (u1 | u | v1) and fz(π(u2) | u | v2) = f (u1 |
u | v2). Since f (u1 | u | v1) 6= f (u1 | u | v2), fz(π(u2) | u | v1) 6= fz(π(u2) | u | v2), hence
(cid:74)
f (π(u2) | u | v1) 6= f (π(u2) | u | v2) and we are done.

E

Proofs with Lengthy Case Analyses

(cid:73) Lemma 51. Suppose f is a transduction that is invariant under permutations and without
If d is a data value that is not
data peeking, u is a data word and e is a data value.
f -inﬂuencing in u and d0 is a safe replacement for d in u, then e is f -memorable (resp. f -
vulnerable) in u iﬀ e is f -memorable (resp. f -vulnerable) in u[d/d0].

Proof. The idea for the proof is the following. If a data value e0 and data word v certify
that e is f -memorable in u, then some permutations can be applied on e0 and v to certify
that e is f -memorable in u[d/d0]. Similar strategies work for the converse direction and for
f -vulnerable values.

34

What You Must Remember When Transforming Datawords

Suppose e = d. We have to prove that d is not f -inﬂuencing in u[d/d0]. Since d
doesn’t occur in u[d/d0], we get u[d/d0][d/d00] = u[d/d0] for any data value d00. Hence,
f (u[d/d0][d/d00] | v) = f (u[d/d0] | v) for all data words v, so d is not f -memorable in u[d/d0].
Since d doesn’t occur in u[d/d0], d is not f -vulnerable in u[d/d0], as proved in Lemma 40.

Suppose e 6= d. First we will prove the statement about f -memorable data values. First
we will assume that e is f -memorable in u and prove that e is f -memorable in u[d/d0]. There
exists a safe replacement e0 for e in u and a data word v such that

f (u[e/e0] | v) 6= f (u | v)

(1)

Let e1 6∈ data(u · v, ∗) ∪ {d, d0, e, e0} be a fresh data value and π1 be the permutation that
interchanges e0 and e1 and doesn’t change any other data value. We apply π1 to both sides
of (1) to get π1(f (u[e/e0] | v)) 6= π1(f (u | v)). From Lemma 38, we then infer that

f (π1(u[e/e0]) | π1(v)) 6= f (π1(u) | π1(v)) .

(2)

Since, e0 is a safe replacement for e in u, e0 doesn’t occur in u. Hence, π1(u[e/e0]) = u[e/e1]
and π1(u) = u. Using these in (2), we get

f (u[e/e1] | π1(v)) 6= f (u | π1(v)) .

(3)

Let π2 be the permutation that interchanges d and d0 and doesn’t change any other data
value. We apply π2 to both sides of (3) to get π2(f (u[e/e1] | π1(v))) 6= π2(f (u | π1(v))).
From Lemma 38, we then infer that f (π2(u[e/e1]) | π2(π1(v))) 6= f (π2(u) | π2(π1(v))). Since
d0 is a safe replacement for d in u, d0 doesn’t occur in u. By choice, d0
6= e1. Hence,
π2(u[e/e1]) = u[e/e1][d/d0] = u[d/d0][e/e1] and π2(u) = u[d/d0]. Using these in the last
inequality, we get f (u[d/d0][e/e1] | π2(π1(v))) 6= f (u[d/d0] | π2(π1(v))). This implies that e is
f -memorable in u[d/d0].

For the converse direction, we will ﬁrst prove that d0 is not f -memorable in u[d/d0].
Suppose for the sake of contradiction that d0 is f -memorable in u[d/d0]. Then there exists
a data word v and a data value d00 that is a safe replacement for d0 in u[d/d0] such that
f (u[d/d0][d0/d00] | v) 6= f (u[d/d0] | v), so f (u[d/d00] | v) 6= f (u[d/d0] | v). Now we apply the
permutation π3 that interchanges d and d00 on both sides of this inequality and Lemma 38
implies that f (u | π3(v)) 6= f (u[d/d0] | π3(v)). This shows that d is f -memorable in u,
a contradiction. Hence, d0 is not f -memorable in u[d/d0]. Now, we have that d0 is not
f -memorable in u[d/d0] and d is a safe replacement for d0 in u[d/d0] and we have to prove that
if e is f -memorable in u[d/d0], then e is f -memorable in u, which is same as u[d/d0][d0/d].
This is similar to proving that if e is f -memorable in u, then e is f -memorable in u[d/d0],
which we have already proved.

Next we will prove the statement about f -vulnerable data values. We have already proved
the statement for e = d, so assume that e 6= d. First assume that e doesn’t occur in u. Then
e is not f -vulnerable in u. The value e is also not f -vulnerable in u[d/d0] in the case where
d0 6= e, since e doesn’t occur in u[d/d0]. We will prove that e is not f -vulnerable in u[d/e].
Suppose for the sake of contradiction that e is f -vulnerable in u[d/e]. There exist data words
u0, v such that e does not occur in u0 and there exists a data value e0 that is a safe replacement
for e in u[d/e] · u0 · v such that f (u[d/e] · u0 | v) 6= f (u[d/e] · u0 | v[e/e0]). Now we apply the
permutation π that interchanges d and e on both sides of this inequality and Lemma 38
implies that f (u · π(u0) | π(v)) 6= f (u · π(u0) | π(v[e/e0])). We have π(v[e/e0]) = π(v)[d/e0],
so f (u · π(u0) | π(v)) 6= f (u · π(u0) | π(v)[d/e0]). Since e doesn’t occur in u0, d doesn’t occur
in π(u0). This implies that d is f -vulnerable in u, a contradiction. So e is not f -vulnerable
in u[d/e].

M. Praveen

35

Next we will assume that e occurs in u. First we will assume that e is f -vulnerable
in u and prove that e is f -vulnerable in u[d/d0]. Suppose that e is f -vulnerable in u. So
there exist data words u0, v such that e doesn’t occur in u0 and there exists a data value e0
that is a safe replacement for e in u · u0 · v such that f (u · u0 | v) 6= f (u · u0 | v[e/e0]). Let
e1 6∈ data(u · u0 · v, ∗) ∪ {d, d0, e, e0} be a fresh data value. The values e0, e1 don’t occur in
u · u0 · v, so we can apply the permutation that interchanges e0 and e1 to both sides of the
last inequality and Lemma 38 implies that f (u · u0 | v) 6= f (u · u0 | v[e/e1]). Now we apply
the permutation π that interchanges d and d0 to both sides of the last inequality and from
Lemma 38, we get that f (u[d/d0] · π(u0) | π(v)) 6= f (u[d/d0] · π(u0) | π(v[e/e1])). The value
d0 doesn’t occur in u (since d0 is a safe replacement for d in u) but e does, so e 6= d0. We
also have d 6= e, d 6= e1 and d0 6= e1, so {d, d0} ∩ {e, e1} = ∅. Hence, π(v[e/e1]) = π(v)[e/e1].
So we get f (u[d/d0] · π(u0) | π(v)) 6= f (u[d/d0] · π(u0) | π(v)[e/e1]), demonstrating that e is a
f -vulnerable value in u[d/d0] (note that since e doesn’t occur in u0, it doesn’t occur in π(u0)
also). Hence we have shown that when e 6= d, if e is f -inﬂuencing in u, then e is f -inﬂuencing
in u[d/d0].

For the converse direction, we will ﬁrst prove that d0 is not f -vulnerable in u[d/d0]. We
have already proved that if e doesn’t occur in u, then e is not f -vulnerable in u[d/e]. Since
d0 doesn’t occur in u, we can put e = d0 to conclude that d0 is not f -vulnerable in u[d/d0].
Now, we have that d0 is not f -vulnerable in u[d/d0] and d is a safe replacement for d0 in
u[d/d0] and we have to prove that if e is f -vulnerable in u[d/d0], then e is f -vulnerable in u,
which is same as u[d/d0][d0/d]. This is similar to proving that if e is f -vulnerable in u, then
(cid:74)
e is f -vulnerable in u[d/d0]. Hence the proof is complete.

(cid:73) Lemma 52. Suppose f is a transduction that is invariant under permutations, σ ∈ Σ is a
letter and u is a data string. If d, e are data values, neither of which are f -inﬂuencing in
u, then d is f -memorable in u · (σ, d) iﬀ e is f -memorable in u · (σ, e). In addition, for any
data value δ /∈ {d, e}, δ is f -memorable in u · (σ, d) iﬀ δ is f -memorable in u · (σ, e).

Proof. We will assume that d is f -memorable in u · (σ, d) and prove that e is f -memorable
in u · (σ, e). The proof of the other direction is similar. Let π be the permutation that
interchanges d and e and doesn’t change any other value. Since d is f -memorable in u · (σ, d),
there exist a data word v and a data value d0 that is a safe replacement for d in u · (σ, d)
satisfying the next inequality. Let π0 be the permutation that interchanges d0 and e and
doesn’t change any other value.

f ((u · (σ, d))[d/d0] | v) 6= f (u · (σ, d) | v)

[Deﬁnition 4]

π(f ((u · (σ, d))[d/d0] | v)) 6= π(f (u · (σ, d) | v))

[apply π to both sides]

f (π((u · (σ, d))[d/d0]) | π(v)) 6= f (π(u · (σ, d)) | π(v))

f (π(u) | (σ, e) · π(v)) = f (u | (σ, e) · π(v))

[Lemma 38]

[Lemma 41]

f (π(u) · (σ, e) | π(v)) = f (u · (σ, e) | π(v))

[Lemma 43, point 2]

f (π(u · (σ, d)) | π(v)) = f (u · (σ, e) | π(v))

f (u | (σ, d0) · π(v)) = f (π(u) | (σ, d0) · π(v))

[Lemma 41]

f (π0(u) | (σ, d0) · π(v)) = f (π0 (cid:12) π(u) | (σ, d0) · π(v))

f (u[e/d0] | (σ, d0) · π(v)) = f (π(u[d/d0]) | (σ, d0) · π(v))

f ((u · (σ, e))[e/d0] | π(v)) = f (π((u · (σ, d))[d/d0]) | π(v))

f ((u · (σ, e))[e/d0] | π(v)) 6= f (u · (σ, e) | π(v))

[Lemma 41]
[d0 /∈ data(u, ∗)]
[d0 /∈ data(u, ∗)]

[(4),(5), (6)]

(4)

(5)

(6)

36

What You Must Remember When Transforming Datawords

From the last inequality above, we conclude that e is f -memorable in u · (σ, e).

Next we will assume that δ is f -memorable in u · (σ, d) and prove that δ is f -memorable
in u · (σ, e). The proof of the other direction is similar. Since δ is f -memorable in u · (σ, d),
there exists a data value δ0 that is safe for replacing δ in u · (σ, d) and a data word v such that
f ((u · (σ, d))[δ/δ0] | v) 6= f (u · (σ, d) | v). Let δ00 be a data value that is a safe replacement
for δ in u · (σ, d) · (σ, e). Let π1 be the permutation that interchanges δ0 and δ00 and doesn’t
change any other value. Let π2 be the permutation that interchanges δ and δ00 and doesn’t
change any other value.

f ((u · (σ, d))[δ/δ0] | v) 6= f (u · (σ, d) | v)
π1(f ((u · (σ, d))[δ/δ0] | v)) 6= π1(f (u · (σ, d) | v))
f (π1((u · (σ, d))[δ/δ0]) | π1(v)) 6= f (π1(u · (σ, d)) | π1(v))
f ((u · (σ, d))[δ/δ00] | π1(v)) 6= f (u · (σ, d) | π1(v))
π(f ((u · (σ, d))[δ/δ00] | π1(v))) 6= π(f (u · (σ, d) | π1(v)))
f (π(u[δ/δ00]) · (σ, e) | π (cid:12) π1(v)) 6= f (π(u) · (σ, e) | π (cid:12) π1(v))

[apply π1 on both sides]

[Lemma 38]
[δ0, δ00 /∈ data(u · (σ, d), ∗)]

[apply π on both sides]

[Lemma 38]

(7)

f (u | (σ, d) · π1(v)) = f (π(u) | (σ, d) · (π1(v)))

[Lemma 41]

π(f (u | (σ, d) · π1(v))) = π(f (π(u) | (σ, d) · π1(v)))

[apply π on both sides]

f (π(u) | (σ, e) · π (cid:12) π1(v)) = f (u | (σ, e) · π (cid:12) π1(v))

[Lemma 38]

f (π(u) · (σ, e) | π (cid:12) π1(v)) = f (u · (σ, e) | π (cid:12) π1(v))

[Lemma 43, point 2]
(8)

d, e /∈ data(iflf (π2(u)), ∗)

[{d, e} ∩ {δ, δ00} = ∅, Lemma 39]

f (π2(u) | (σ, d) · π1(v)) = f (π (cid:12) π2(u) | (σ, d) · π1(v))

[Lemma 41]

π(f (π2(u) | (σ, d) · π1(v))) = π(f (π (cid:12) π2(u) | (σ, d) · π1(v)))

[apply π on both sides]

f (π (cid:12) π2(u) | (σ, e) · π (cid:12) π1(v)) = f (π2(u) | (σ, e) · π (cid:12) π1(v))
f (π(u[δ/δ00]) | (σ, e) · π (cid:12) π1(v)) = f (u[δ/δ00] | (σ, e) · π (cid:12) π1(v))
f (π(u[δ/δ00]) · (σ, e) | π (cid:12) π1(v)) = f (u[δ/δ00] · (σ, e) | π (cid:12) π1(v))

[Lemma 38]
[δ00 /∈ data(u, ∗)]

[Lemma 43, point 2]
(9)

f (u[δ/δ00] · (σ, e) | π (cid:12) π1(v)) 6= f (u · (σ, e) | π (cid:12) π1(v))
f ((u · (σ, e))[δ/δ00] | π (cid:12) π1(v)) 6= f (u · (σ, e) | π (cid:12) π1(v))

[(7), (8), (9)]

[δ 6= e]

The last inequality above certiﬁes that δ is f -memorable in u · (σ, e).

(cid:74)

(cid:73) Lemma 53. Suppose f is a transduction that is invariant under permutations, σ ∈ Σ is a
letter and u is a data string. If d, e are data values, neither of which are f -inﬂuencing in u,
then d is f -vulnerable in u · (σ, d) iﬀ e is f -vulnerable in u · (σ, e). In addition, for any data
value δ /∈ {d, e}, δ is f -vulnerable in u · (σ, d) iﬀ δ is f -vulnerable in u · (σ, e).

Proof. We will assume that d is f -vulnerable in u · (σ, d) and prove that e is f -vulnerable
in u · (σ, e). The proof of the other direction is similar. Let π be the permutation that
interchanges d and e and doesn’t change any other value. Since d is f -vulnerable in
u · (σ, d), we infer from Deﬁnition 4 that there exist data words u0, v and a data value
d0 such that d doesn’t occur in u0, d0 is a safe replacement for d in u · (σ, d) · u0 · v and
f (u · (σ, d) · u0 | v[d/d0]) 6= f (u · (σ, d) · u0 | v). Applying the contrapositive of Lemma 45 to

M. Praveen

37

the above inequality, we infer that at least one of the following inequalities are true.

f (u | (σ, d) · u0 | v[d/d0]) 6= f (u | (σ, d) · u0 | v)

f (u | (σ, d) · u0 · v[d/d0]) 6= f (u | (σ, d) · u0 · v)

Each of the above inequalities is taken up in one of the following cases. Let π be the
permutation that interchanges d and e and doesn’t change any other value. Let d00 be a
data value such that d00 /∈ data(u · u0 · v, ∗) ∪ {d, e, d0, π(d), π(d0), π(e), π(e0)}. Let π0 be the
permutation that interchanges d0 and d00 and doesn’t change any other value.

Case 1:

f (u | (σ, d) · u0 | v[d/d0]) 6= f (u | (σ, d) · u0 | v)

π0(f (u | (σ, d) · u0 | v[d/d0])) 6= π0(f (u | (σ, d) · u0 | v))

f (u | (σ, d) · u0 | v[d/d00]) 6= f (u | (σ, d) · u0 | v)

[apply π0 to both sides]
[Lemma 38, d0, d00 /∈ data(u · u0 · (σ, d) · v, ∗)]
(10)

f (u | (σ, d) · u0 · v[d/d00]) = f (π(u) | (σ, d) · u0 · v[d/d00])

[Lemma 41]

f (u | (σ, d) · u0 | v[d/d00]) = f (π(u) | (σ, d) · u0 | v[d/d00])

[point 1 of Lemma 43]

(11)

f (u | (σ, d) · u0 · v) = f (π(u) | (σ, d) · u0 · v)

[Lemma 41]

f (u | (σ, d) · u0 | v) = f (π(u) | (σ, d) · u0 | v)

[point 1 of Lemma 43]

(12)

f (π(u) | (σ, d) · u0 | v[d/d00]) 6= f (π(u) | (σ, d) · u0 | v)

[(10), (11), (12)]

π(f (π(u) | (σ, d) · u0 | v[d/d00])) 6= π(f (π(u) | (σ, d) · u0 | v))

[apply π on both sides]

f (u | (σ, e) · π(u0) | π(v)[e/d00]) 6= f (u | (σ, e) · π(u0) | π(v))

[Lemma 38, π(π(u)) = u, π(v[d/d00]) = π(v)[e/d00]]

f (u · (σ, e) · π(u0) | π(v)[e/d00]) 6= f (u · (σ, e) · π(u0) | π(v))

[contrapositive of Lemma 43, point 4]

Case 2:

f (u | (σ, d) · u0 · v[d/d0]) 6= f (u | (σ, d) · u0 · v)

π0(f (u | (σ, d) · u0 · v[d/d0])) 6= π0(f (u | (σ, d) · u0 · v))

f (u | (σ, d) · u0 · v[d/d00]) 6= f (u | (σ, d) · u0 · v)

[apply π0 on both sides]
[Lemma 38, d0, d00 /∈ data(u · u0 · (σ, d) · v, ∗)]

f (u | π((σ, d) · u0 · v[d/d00])) 6= f (u | π((σ, d) · u0 · v))

[Lemma 42]

f (u | (σ, e) · π(u0) · π(v)[e/d00]) 6= f (u | (σ, e) · π(u0) · π(v))

f (u · (σ, e) · π(u0) | π(v)[e/d00]) 6= f (u · (σ, e) · π(u0) | π(v))

[contrapositive of Lemma 43, point 3]

Since d doesn’t occur in u0, e doesn’t occur in π(u0). The last inequalities in each of the

above cases certify that e is f -vulnerable in u · (σ, e).

Next we will assume that δ is f -vulnerable in u · (σ, d) and prove that δ is f -vulnerable in
u · (σ, e). The proof of the other direction is similar. Since δ is f -vulnerable in u · (σ, d), we
infer from Deﬁnition 4 that there exist data words u0, v and a data value δ0 such that δ doesn’t
occur in u0, δ0 is a safe replacement for δ in u · (σ, d) · u0 · v and f (u · (σ, d) · u0 | v[δ/δ0]) 6=
f (u · (σ, d) · u0 | v). Applying the contrapositive of Lemma 45 to the above inequality, we
infer that at least one of the following inequalities are true.

f (u | (σ, d) · u0 | v[δ/δ0]) 6= f (u | (σ, d) · u0 | v)

f (u | (σ, d) · u0 · v[δ/δ0]) 6= f (u | (σ, d) · u0 · v)

38

What You Must Remember When Transforming Datawords

Each of the above inequalities is taken up in one of the following cases. Let π be the
permutation that interchanges d and e and doesn’t change any other value. Let δ00 be a data
value such that δ00 /∈ data(u · u0 · v, ∗) ∪ {d, e, δ0}. Let π0 be the permutation that interchanges
δ0 and δ00 and doesn’t change any other value.

Case 1:

f (u | (σ, d) · u0 | v[δ/δ0]) 6= f (u | (σ, d) · u0 | v)

π0(f (u | (σ, d) · u0 | v[δ/δ0])) 6= π0(f (u | (σ, d) · u0 | v))

f (u | (σ, d) · u0 | v[δ/δ00]) 6= f (u | (σ, d) · u0 | v)

[apply π0 to both sides]
[Lemma 38, δ0, δ00 /∈ data(u · u0 · (σ, d) · v, ∗)]
(13)

f (u | (σ, d) · u0 · v[δ/δ00]) = f (π(u) | (σ, d) · u0 · v[δ/δ00])

[Lemma 41]

f (u | (σ, d) · u0 | v[δ/δ00]) = f (π(u) | (σ, d) · u0 | v[δ/δ00])

[point 1 of Lemma 43]

(14)

f (u | (σ, d) · u0 · v) = f (π(u) | (σ, d) · u0 · v)

[Lemma 41]

f (u | (σ, d) · u0 | v) = f (π(u) | (σ, d) · u0 | v)

[point 1 of Lemma 43]

(15)

f (π(u) | (σ, d) · u0 | v[δ/δ00]) 6= f (π(u) | (σ, d) · u0 | v)

[(13), (14), (15)]

π(f (π(u) | (σ, d) · u0 | v[δ/δ00])) 6= π(f (π(u) | (σ, d) · u0 | v))

[apply π on both sides]

f (u | (σ, e) · π(u0) | π(v)[δ/δ00]) 6= f (u | (σ, e) · π(u0) | π(v))

[Lemma 38, π(π(u)) = u, {d, e} ∩ {δ, δ00} = ∅]

f (u · (σ, e) · π(u0) | π(v)[δ/δ00]) 6= f (u · (σ, e) · π(u0) | π(v))

[contrapositive of Lemma 43, point 4]

Case 2:

f (u | (σ, d) · u0 · v[δ/δ0]) 6= f (u | (σ, d) · u0 · v)

π0(f (u | (σ, d) · u0 · v[δ/δ0])) 6= π0(f (u | (σ, d) · u0 · v))

f (u | (σ, d) · u0 · v[δ/δ00]) 6= f (u | (σ, d) · u0 · v)

[apply π0 on both sides]
[Lemma 38, δ0, δ00 /∈ data(u · u0 · (σ, d) · v, ∗)]

f (u | π((σ, d) · u0 · v[δ/δ00])) 6= f (u | π((σ, d) · u0 · v))

[Lemma 42]

f (u | (σ, e) · π(u0) · π(v)[δ/δ00]) 6= f (u | (σ, e) · π(u0) · π(v))

f (u · (σ, e) · π(u0) | π(v)[δ/δ00]) 6= f (u · (σ, e) · π(u0) | π(v))

[contrapositive of Lemma 43, point 3]

Since δ doesn’t occur in u0, δ doesn’t occur in π(u0). The last inequalities in each of the
(cid:74)
above cases certify that δ is f -vulnerable in u · (σ, e).

Proof of Lemma 11. Since u1 ≡f u2, there exists a permutation π satisfying the conditions
of Deﬁnition 7. Let z = |u1| − |u2|.

Proof of 1. Suppose di

1 is f -memorable in u1·(σ, dj

1). There exist a data word v and a safe
1 in u1 · (σ, dj
replacement d0 for di
1) | v).
Let d00 be a data value that is a safe replacement for di
1) · v · π(u2). Let π1 be
the permutation that interchanges d0 and d00 and doesn’t change any other value. Let π2 be

1))[di
1 in u1 · (σ, dj

1) such that f ((u1 · (σ, dj

1/d0] | v) 6= f (u1 · (σ, dj

M. Praveen

39

the permutation that interchanges di

1 and d00 and doesn’t change any other value.

f ((u1 · (σ, dj
π1(f ((u1 · (σ, dj
1))[di
1))[di

f (π1((u1 · (σ, dj
f ((u1 · (σ, dj

1/d0] | v) 6= f (u1 · (σ, dj
1))[di
1/d0] | v)) 6= π1(f (u1 · (σ, dj
1))[di
1/d0]) | π1(v)) 6= f (π1(u1 · (σ, dj
1/d00] | π1(v)) 6= f (u1 · (σ, dj

1) | v)

1) | π1(v))

1) | v))
1)) | π1(v))

f (u1 | (σ, dj
f (u1 · (σ, dj

1) · π1(v)) = fz(π(u2) | (σ, dj
1) | π1(v)) = fz(π(u2) · (σ, dj

1) · π1(v))
1) | π1(v))

f (u1 | (σ, dj
π2(f (u1 | (σ, dj
f (π2(u1) | π2((σ, dj
1/d00] | (σ, dj
f (u1[di
f ((u1 · (σ, dj

1) · π−1
1) · π−1
1) · π−1
1)[di
1))[di

2 (cid:12) π1(v)) = fz(π(u2) | (σ, dj
2 (cid:12) π1(v))) = π2(fz(π(u2) | (σ, dj
2 (cid:12) π1(v))) = fz(π2(π(u2)) | π2((σ, dj
1/d00] | (σ, dj
1/d00] · π1(v)) = fz(π(u2)[di
1/d00] | π1(v)) = fz((π(u2) · (σ, dj
1))[di

1) · π−1

1) · π−1

2 (cid:12) π1(v)))
1) · π−1
1)[di

2 (cid:12) π1(v)))
1/d00] · π1(v))

1/d00] | π1(v))

2 (cid:12) π1(v))

fz((π(u2) · (σ, dj
f ((π(u2) · (σ, dj

1))[di
1))[di

1/d00] | π1(v)) 6= fz(π(u2) · (σ, dj
1/d00] | π1(v)) 6= f (π(u2) · (σ, dj

1) | π1(v))
1) | π1(v))

[Deﬁnition 4]

[apply π1 on both sides]

[Lemma 38]
[{d0, d00} /∈ data(u1 · (σ, dj

1), ∗)]

(16)

[Deﬁnition 7]

[Lemma 43, point 2]

(17)

[Deﬁnition 7]

[apply π2 on both sides]

[Lemma 38]
[d00 /∈ data(u1 · (σ, dj

1) · π(u2), ∗)]

[Lemma 43, point 2]

(18)

[(16), (17), (18)]

1 is f -memorable in π(u2 · (σ, π−1(dj

Since d00 is a safe replacement for di
1 is f -memorable in π(u2) · (σ, dj
di
that di
f -memorable in u2 · (σ, π−1(dj
1, dj
1, dk
2). Since di

Case 1: (dj

2) ∈ {(dk

f -memorable in u2 · (σ, dj
that di
1, di
(di

1 is f -memorable in u1 or di

2) ∈ {(dk
Case 2: (dj

1, dk
1, dj

2) | 1 ≤ k ≤ m}. Hence π−1(di
1 = d0
2) = (d0

2). Since dj

1, d0

1 in π(u2) · (σ, dj
1). Since π(u2) · (σ, dj

1), the last inequality above certiﬁes that
1))), we infer
1) is

1) = π(u2 · (σ, π−1(dj

1))). From Lemma 39, we infer that π−1(di

1)).
2) | 1 ≤ k ≤ m}. In this case, π−1(dj
1 is f -memorable in u1 · (σ, dj
1 = dj

1. Either way, di

1) = dj

2. So π−1(di

1) is
1), we infer from Lemma 10
1 | 1 ≤ k ≤ m}, so
2).

2 is f -memorable in u2 · (σ, dj

1 ∈ {dk

2, so di

1) = di
1 is not f -inﬂuencing in u1, π−1(dj
2 = d0

1, dk

f -inﬂuencing in u2. From the hypothesis of this lemma, dj
1, di
u2. If (di
u2 · (σ, π−1(dj
other possibility is that (di
in u1, π−1(di
u2 · (σ, π−1(dj
2 is f -memorable in u2 · (σ, dj
di
similar proof.

2) | 1 ≤ k ≤ m}, then π−1(di
2) ∈ {(dk
1)). From Lemma 52, we conclude that di
2) = (d0
1) is not f -inﬂuencing in u2. Since π−1(di
2 = dj
1)), from Lemma 52, we conclude that di
1 is f -memorable in u1 · (σ, dj
2), we can prove that di

1 = d0
1) = π−1(dj
2 is f -memorable in u2 · (σ, dj

1) is not
2 is not f -inﬂuencing in
2 is f -memorable in
2). The
1 is not f -inﬂuencing
1) is f -memorable in
2). If
1) with a

1) = di
2 is f -memorable in u2 · (σ, dj

2). Since di

2) = (dj

2. So di

1, dj

1, d0

1, di

Suppose di

1 is f -vulnerable in u1 · (σ, dj

words u0, v and a data value d0 such that di
1) · u0 · v and f (u1 · (σ, dj
for di
be a data value that is a safe replacement for di
permutation that interchanges d0 and d00 and doesn’t change any other value.

1). We infer from Deﬁnition 4 that there exist data
1 doesn’t occur in u0, d0 is a safe replacement
1) · u0 | v). Let d00
1) · u0 · v · π(u2). Let π1 be the

1/d0]) 6= f (u1 · (σ, dj

1 in u1 · (σ, dj

1 in u1 · (σ, dj

1) · u0 | v[di

40

What You Must Remember When Transforming Datawords

f (u1 · (σ, dj
π1(f (u1 · (σ, dj
f (u1 · (σ, dj
f (π(u2) · (σ, dj

1) · u0 | v[di
1) · u0 | v[di
1) · u0 | v[di
1) · u0 | v[di

1/d0]) 6= f (u1 · (σ, dj
1/d0])) 6= π1(f (u1 · (σ, dj
1/d00]) 6= f (u1 · (σ, dj
1/d00]) 6= f (π(u2) · (σ, dj

1) · u0 | v))

1) · u0 | v)

1) · u0 | v)

1) · u0 | v)

[apply π1 on both sides]
[Lemma 38, d0, d00 /∈ data(u1 · (σ, dj

1) · u0 · v, ∗)]

[last condition on π in Deﬁnition 7]

1 is f -vulnerable in π(u2) · (σ, dj

1 is f -vulnerable in π(u2 · (σ, π−1(dj

1). Since, π(u2) ·
1))). From Lemma 39, we

1))), di

1) = π(u2 · (σ, π−1(dj

1) is f -vulnerable in u2 · (σ, π−1(dj
2) ∈ {(dk

The last inequality above implies that di
(σ, dj
infer that π−1(di
1, dj
Case 1: (dj
iflf (u2) to iflf (u1)), so π−1(di
u1 · (σ, dj
di
1 ∈ {dk
f -vulnerable in u2 · (σ, dj

1), we infer from Lemma 10 that di
1, dk
1 | 1 ≤ k ≤ m}, so (di

2) ∈ {(dk

1, dk

1, di

1)).

2) | 1 ≤ k ≤ m}. In this case, π−1(dj

1) = dj

1) is f -vulnerable in u2 · (σ, dj

2). Since di
1 is f -vulnerable in u1 or di
2) | 1 ≤ k ≤ m}. Hence, π−1(di

2 (since π maps
1 is f -vulnerable in
1 = dj
1. Either way,
2, so di
2 is

1) = di

2).

1, di

1, d0

1, d0

1, dk

1) = di

1 is not f -inﬂuencing in u1, π−1(di

1) is f -vulnerable in u2 · (σ, π−1(dj

Case 2: (dj

In this case, dj

2) | 1 ≤ k ≤ m}, then π−1(di

1, dj
1) = π−1(d0
2) ∈ {(dk

2).
2 = d0
2) = (d0
1) is not f -inﬂuencing in u2 (since d0
2. So di

π−1(dj
(di
1, di
From Lemma 53, we infer that di
1, dj
that (di
not f -inﬂuencing in u2. Since π−1(di
2 = dj
Lemma 53, we conclude that di
u2 · (σ, dj
1 is f -vulnerable in u1 · (σ, dj
2), we can prove that di

2 is f -vulnerable in u2 · (σ, dj
1 = d0
2). Since di

1) = π−1(dj
2 is f -vulnerable in u2 · (σ, dj

2) = (dj

2) = (d0

2 is not f -inﬂuencing in u2, and
If
1 is not f -inﬂuencing in u1).
2 is f -vulnerable in u2 ·(σ, π−1(dj
1)).
2). The other possibility is
1) is
1)), from
2 if f -vulnerable in

2). If di

Proof of 2. Let π0 be the permutation that interchanges d0
1) ≡f u2 · (σ, dj

change any other value. To prove that u1 · (σ, dj
permutation π0 (cid:12)π satisﬁes all the conditions of Deﬁnition 7. Note that π0 (cid:12)π(dj
Lemma 10, we infer that f -inﬂuencing values in u1 ·(σ, dj
and that f -inﬂuencing values in u2 · (σ, dj
from point 1 of this lemma that dj
2 is f -memorable (resp. f -vulnerable) in u2 · (σ, dj
dj
lemma that for (di
2) ∈ {(dk
u1 · (σ, dj
and π0 (cid:12) π(dj

1) with a similar proof.
1 and π(d0
2) and doesn’t
2), we will prove that the
2) = dj
1. From
1 | 1 ≤ k ≤ m}∪{dj
1}
2}. We infer
1) iﬀ
2). We also infer from point 1 of this
1 is f -memorable (resp. f -vulnerable) in
2) = di
1

2) | 1 ≤ k ≤ m}, di
2 is f -memorable (resp. f -vulnerable) in u2 · (σ, dj

1 is f -memorable (resp. f -vulnerable) in u1 · (σ, dj

1, we infer that aiflf (π0 (cid:12) π(u2 · (σ, dj

2))) = aiflf (u1 · (σ, dj

2 | 1 ≤ k ≤ m} ∪ {dj

2). Since, π0 (cid:12) π(di

2) are among {dk

1) are among {dk

2) = dj

1) iﬀ di

1, dk

1, di

1)).
2 is not f -inﬂuencing in u2, π(d0

2) is not

Let v be an arbitrary data word. Since d0

f -inﬂuencing in u1.

fz(π(u2) | (σ, dj
fz(π0 (cid:12) π(u2) | (σ, dj
fz(π0 (cid:12) π(u2) · (σ, dj
fz(π0 (cid:12) π(u2 · (σ, dj

1) · v) = f (u1 | (σ, dj
1) · v) = f (u1 | (σ, dj
1) | v) = f (u1 · (σ, dj
2)) | v) = f (u1 · (σ, dj

1) · v)
1) · v)
1) | v)
1) | v)

[ﬁrst condition on π in Deﬁnition 7]

[Lemma 41]

[Lemma 43, point 2]

Since the last inequality above holds for any data word v, it proves the ﬁrst condition of
Deﬁnition 7.

For the last condition of Deﬁnition 7, suppose u, v1, v2 are arbitrary data values and
1) · u | v2). Since, u1 ≡f u2 and π satisﬁes all the conditions

1) · u | v1) = f (u1 · (σ, dj

f (u1 · (σ, dj

M. Praveen

41

of Deﬁnition 7, we infer that f (π(u2) · (σ, dj

1) · u | v1) = f (π(u2) · (σ, dj

1) · u | v2).

f (π(u2) · (σ, dj
f (π(u2) | (σ, dj
f (π(u2) | π0((σ, dj
π0(f (π(u2) | π0((σ, dj

f (π0 (cid:12) π(u2) | (σ, dj
f (π(u2) · (σ, dj
f (π(u2) | (σ, dj

1) · u | v2)
1) · u · v2)

1) · u | v1) = f (π(u2) · (σ, dj
1) · u · v1) = f (π(u2) | (σ, dj
1) · u · v1)) = f (π(u2) | π0((σ, dj
1) · u · v1))) = π0(f (π(u2) | π0((σ, dj
1) · u · v1) = f (π0 (cid:12) π(u2) | (σ, dj
1) · u | v1) = f (π(u2) · (σ, dj
1) · u | v1) = f (π(u2) | (σ, dj

1) · u | v2)
1) · u | v2)

1) · u · v2))

1) · u · v2)))
1) · u · v2)

f (π0 (cid:12) π(u2) | (σ, dj
f (π0 (cid:12) π(u2) | (σ, dj

1) · u · v1) = f (π(u2) | (σ, dj
1) · u | v1) = f (π(u2) | (σ, dj

1) · u · v1)
1) · u | v1)

f (π0 (cid:12) π(u2) | (σ, dj
f (π0 (cid:12) π(u2) | (σ, dj

1) · u · v2) = f (π(u2) | (σ, dj
1) · u | v2) = f (π(u2) | (σ, dj

1) · u · v2)
1) · u | v2)

[Lemma 43, point 3]
2), d0

[Lemma 42, π(d0

1 /∈ data(aiflf (π(u2), ∗))]

[apply π0 on both sides]

[Lemma 38]

(19)

[Lemma 41, π(d0

[Lemma 43, point 4]
(20)
2), d0
[Lemma 43, point 1]
(21)
2), d0
[Lemma 43, point 1]
(22)

[Lemma 41, π(d0

1 /∈ data(aiflf (π(u2), ∗))]

1 /∈ data(aiflf (π(u2), ∗))]

f (π0 (cid:12) π(u2) | (σ, dj
f (π0 (cid:12) π(u2) · (σ, dj
f (π0 (cid:12) π(u2 · (σ, dj

1) · u | v1) = f (π0 (cid:12) π(u2) | (σ, dj
1) · u | v1) = f (π0 (cid:12) π(u2) · (σ, dj
2)) · u | v1) = f (π0 (cid:12) π(u2 · (σ, dj

1) · u | v2)
1) · u | v2)
2)) · u | v2)

[(20),(21),(22)] (23)

[(19),(23), Lemma 45]

Hence, if f (u1 · (σ, dj
f (π0 (cid:12) π(u2 · (σ, dj

2)) · u | v2).

1) · u | v1) = f (u1 · (σ, dj

1) · u | v2), then f (π0 (cid:12) π(u2 · (σ, dj

2)) · u | v1) =

Conversely, suppose f (π0 (cid:12) π(u2 · (σ, dj

2)) · u | v1) = f (π0 (cid:12) π(u2 · (σ, dj

we have f (π0 (cid:12) π(u2) · (σ, dj

1) · u | v1) = f (π0 (cid:12) π(u2) · (σ, dj

1) · u | v2). Recall that π(d0

2)) · u | v2). Then
2) and

42

What You Must Remember When Transforming Datawords

d0
1 are not f -inﬂuencing in π(u2).

f (π0 (cid:12) π(u2) · (σ, dj
f (π0 (cid:12) π(u2) | (σ, dj
f (π0 (cid:12) π(u2) | π0((σ, dj

1) · u | v1) = f (π0 (cid:12) π(u2) · (σ, dj
1) · u · v1) = f (π0 (cid:12) π(u2) | (σ, dj
1) · u · v1)) = f (π0 (cid:12) π(u2) | π0((σ, dj

1) · u | v2)
1) · u · v2)

1) · u · v2))

π0(f (π0 (cid:12) π(u2) | π0((σ, dj

1) · u · v1))) = π0(f (π0 (cid:12) π(u2) | π0((σ, dj

1) · u · v2)))

f (π(u2) | (σ, dj

1) · u · v1) = f (π(u2) | (σ, dj

1) · u · v2)

f (π0 (cid:12) π(u2) · (σ, dj
f (π0 (cid:12) π(u2) | (σ, dj

1) · u | v1) = f (π0 (cid:12) π(u2) · (σ, dj
1) · u | v1) = f (π0 (cid:12) π(u2) | (σ, dj

1) · u | v2)
1) · u | v2)

f (π0 (cid:12) π(u2) | (σ, dj

1) · u · v1) = f (π(u2) | (σ, dj

1) · u · v1)

f (π0 (cid:12) π(u2) | (σ, dj

1) · u | v1) = f (π(u2) | (σ, dj

1) · u | v1)

f (π0 (cid:12) π(u2) | (σ, dj

1) · u · v2) = f (π(u2) | (σ, dj

1) · u · v2)

f (π0 (cid:12) π(u2) | (σ, dj

1) · u | v2) = f (π(u2) | (σ, dj

1) · u | v2)

f (π(u2) | (σ, dj

1) · u | v1) = f (π(u2) | (σ, dj

1) · u | v2)

f (π(u2) · (σ, dj

1) · u | v1) = f (π(u2) · (σ, dj

1) · u | v2)

[Lemma 43, point 3]
2), d0

[Lemma 42, π(d0
data(aiflf (π0 · π(u2), ∗))]
[apply π0 on both sides]

1 /∈

[Lemma 38]

(24)

[Lemma 43, point 4]

(25)
[Lemma 41, π(d0

1 /∈
data(aiflf (π(u2), ∗))]

2), d0

[Lemma 43, point 1]

(26)
[Lemma 41, π(d0

1 /∈
data(aiflf (π(u2), ∗))]

2), d0

[Lemma 43, point 1]

(27)

[(25),(26),(27)]
(28)

[(24),(28), Lemma 45]

Since, u1 ≡f u2 and π satisﬁes all the conditions of Deﬁnition 7, we infer from the last equality
above that f (u1 ·(σ, dj
2))·u | v1) =
f (π0 (cid:12) π(u2 · (σ, dj
1) · u | v2). Therefore, the
1) ≡f u2 · (σ, dj
permutation π0 (cid:12) π satisﬁes all the conditions of Deﬁnition 7, so u1 · (σ, dj
2). (cid:74)

1)·u | v2). Hence, if f (π0 (cid:12)π(u2 ·(σ, dj
1) · u | v1) = f (u1 · (σ, dj

1)·u | v1) = f (u1 ·(σ, dj
2)) · u | v2), then f (u1 · (σ, dj

