Markov Decision Process modeled with Bandits for Sequential
Decision Making in Linear-flow

Wenjun Zeng
zengwenj@amazon.com
Amazon.com
Seattle, Washington, United States

Yi Liu
yiam@amazon.com
Amazon.com
Seattle, Washington, United States

2
2
0
2

r
a

M
6
1

]

G
L
.
s
c
[

2
v
4
0
2
0
0
.
7
0
1
2
:
v
i
X
r
a

ABSTRACT
For marketing, we sometimes need to recommend content for mul-
tiple pages in sequence. Different from general sequential decision
making process, the use cases have a simpler flow where customers
per seeing recommended content on each page can only return feed-
back as moving forward in the process or dropping from it until a
termination state. We refer to this type of problems as sequential
decision making in linearâ€“flow. We propose to formulate the prob-
lem as an MDP with Bandits where Bandits are employed to model
the transition probability matrix. At recommendation time, we use
Thompson sampling (TS) to sample the transition probabilities and
allocate the best series of actions with analytical solution through
exact dynamic programming. The way that we formulate the prob-
lem allows us to leverage TSâ€™s efficiency in balancing exploration
and exploitation and Banditâ€™s convenience in modeling actionsâ€™
incompatibility. In the simulation study, we observe the proposed
MDP with Bandits algorithm outperforms Q-learning with ğœ–-greedy
and decreasing ğœ–, independent Bandits, and interaction Bandits. We
also find the proposed algorithmâ€™s performance is the most robust
to changes in the across-page interdependence strength.

KEYWORDS
Bandit, Reinforcement Learning, posteriror sampling, MDP, recom-
mendation

ACM Reference Format:
Wenjun Zeng and Yi Liu. 2021. Markov Decision Process modeled with Ban-
dits for Sequential Decision Making in Linear-flow. In Proceedings of Marble-
KDD 21. Singapore, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
For marketing, we sometimes need to recommend content for mul-
tiple pages in sequence. At each page, users can choose to continue
the process or leave. Use cases like these require sequential decision
making process since each page has a set of content candidates and
the algorithm needs to make recommendation considering inter-
dependence between content on successive pages. Different from
general sequential decision making process, it is a simpler flow
where customers per seeing recommended content on each page
can only move forward in the process or drop from it. We define
problems like this as sequential decision making in linearâ€“flow.
We observe this type of problems in customer engagement beyond
content recommendation. For instance, it is common to take a series
of nudges, such as promotion email and limited-time discount offer,
to increase customerâ€™s activity. We can consider customers enter

Marble-KDD 21, August 16, 2021, Singapore
2021. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

the workflow when their interactions drop below a threshold, and
leave the flow with reward as +1 if their interactions increase to a
healthy level or âˆ’1 if customers become inactive.

In this paper, we formulate sequential decision making process
in linear-flow as a Markov Decision Process (MDP). We model the
transition probability matrix with contextual Bayesian Bandits [3],
use Thompson Sampling (TS) as the exploration strategy, and apply
exact Dynamic Programming (DP) to solve the MDP. Modeling
transition probability matrix with contextual Bandits makes it con-
venient to specify actionsâ€™ incompatibility, when certain action
pairs at different steps are ineligible. It also makes it straightfor-
ward to interpret interdependence strength between actions in two
successive situations. Our algorithm is a realization of the Posterior
Sampling for Reinforcement Learning (PSRL) framework [4, 5] with
one difference. PSRL framework assumes each (state, action) pair is
modeled separately while we model the pairs at a step using one
Bayesian Bandit model. We present our algorithm in section 2. We
use simulator to evaluate the proposed algorithm comparing to
Q-learning with ğœ–-greedy and decreasing ğœ–, independent Bandits,
and interaction Bandits. The evaluation results are documented in
section 3. We then conclude the paper in section 4. In the remainder
of this paper, we relate writing to content recommendation appli-
cations while the algorithm works for any linear-flow problems.

2 METHODS
2.1 Problem Formulation
We model sequential decision making in linear flow as an MDP.
Let ğ· be number of the pages that content recommendation is
needed. For each page ğ‘–, there are ğ‘ğ‘– content candidates that we
can choose from. The selected content can be denoted as ğ‘ğ‘–,ğ‘›ğ‘– where
ğ‘›ğ‘– âˆˆ {1, 2, Â· Â· Â· , ğ‘ğ‘– }. For simplicity, we will denote the content shown
on page ğ‘– as ğ‘ğ‘– unless the notation ğ‘ğ‘–,ğ‘›ğ‘– is needed to distinguish
between different content for page ğ‘–. The flow starts with state
ğ‘†1,ğ‘‹ where ğ‘‹ is customer feature vector. After action ğ‘ğ‘– is selected
to show to customer with features ğ‘‹ on page ğ‘–, there are two
possible states: ğ‘†ğ‘–+1,ğ‘‹ ,ğ‘ğ‘– and ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ . The only exception is for the
last page, page ğ·, the two possible states after action ğ‘ğ· are: ğ‘†ğ¸ğ‘›ğ‘‘
and ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ . ğ‘†ğ‘–+1,ğ‘‹ ,ğ‘ğ‘– means customers move forward in the process
to the next page after seeing ğ‘ğ‘– . ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ means customers drop from
the process without reaching the end and ğ‘†ğ¸ğ‘›ğ‘‘ means customers
reached the end. We denote ğ‘…ğ‘– and ğºğ‘– as shortâ€“term and longâ€“term
rewards after action ğ‘ğ‘– respectively. The goal is to optimize ğº1.
Customers are at ğ‘†1,ğ‘‹ after they trigger the flow. ğ‘‹ may include
customer features, the upstream channel that customers land from,
etc. Customers get to ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ if they drop from the process at any
page leading to ğ‘…ğ‘– = 1. Customers are in ğ‘†ğ‘–+1,ğ‘‹ ,ğ‘ğ‘– if they move on

 
 
 
 
 
 
Marble-KDD 21, August 16, 2021, Singapore

in the process after marketing content ğ‘ğ‘– . The short-term reward is
0 from entering ğ‘†ğ‘–+1,ğ‘‹ ,ğ‘ğ‘– . ğ‘†ğ¸ğ‘›ğ‘‘ is another termination state other
than ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ . Customers reach this state if they decide not to take the
offer after seeing content on the last page. This leads to ğ‘…ğ· = 0.

2.2 Transition Probability Matrix Modeling
We use contextual Bandit to model and update transition probabil-
ity matrix. As our rewards are binary and there can be only two
outcomes per action, we select BLIP [2] as the Bandit algorithm. The
transition probability is formulated through a Probit link function.
After showing page ğ‘–, the probability of customers moving forward
in the process and dropping from the process is formulated as in
Equations 1 and 2 respectively. By using this formulation, we model
only interdependence between two successive pages. For page 1,
ğ‘ğ‘–âˆ’1 is dropped from the subscripts but the rest of the equations
remain the same.

ğ‘ƒ (ğ‘†ğ‘¡ +1 = ğ‘†ğ‘–+1,ğ‘‹ ,ğ‘ğ‘– |ğ‘†ğ‘¡ = ğ‘†ğ‘–,ğ‘‹ ,ğ‘ğ‘–âˆ’1

, ğ‘ğ‘¡ = ğ‘ğ‘– ) = Î¦(âˆ’1 âˆ—

ğµğ‘‡
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–
ğ›½

ğ‘Šğ‘–

)

(1)

ğ‘ƒ (ğ‘†ğ‘¡ +1 = ğ‘†ğ¸ğ‘¥ğ‘–ğ‘¡ |ğ‘†ğ‘¡ = ğ‘†ğ‘–,ğ‘‹ ,ğ‘ğ‘–âˆ’1

, ğ‘ğ‘¡ = ğ‘ğ‘– ) = Î¦(

ğ‘Šğ‘–

ğµğ‘‡
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–
ğ›½

)

(2)

In the formula, Î¦ is cumulative distribution function for stan-
dard normal distribution. ğ›½ is a scaling hyperparameter. ğ‘Šğ‘– are
the weights quantifying feature contributions to the utility func-
tion. ğµğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘– is the final vector that we will learn weights for
and formed by combining ğ‘‹ , ğ‘ğ‘–âˆ’1 and ğ‘ğ‘– (usually with interaction
terms). ğµğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘– is the reason why we can easily model content in-
compatibility. To explain this, letâ€™s first assume we have one feature
ğ‘¥ for ğ‘‹ , ğ‘ğ‘–âˆ’1 has two candidates: ğ‘ğ‘–âˆ’1,1 and ğ‘ğ‘–âˆ’1,2, and ğ‘ğ‘– has three
candidates: ğ‘ğ‘–,1, ğ‘ğ‘–,2 and ğ‘ğ‘–,3. We assume ğ‘ğ‘–,3 cannot be shown if
ğ‘ğ‘–âˆ’1,1 is shown to customers. We may set ğµğ‘ğ‘–âˆ’1,ğ‘ğ‘– as in Equation 3
where we have 1 for intercept and contextual feature interacting
with action terms. We recognize content incompatibility between
ğ‘ğ‘–âˆ’1,1 and ğ‘ğ‘–,3 by simply omitting interaction term ğ‘¥ Â· ğ‘ğ‘–âˆ’1,1 Â· ğ‘ğ‘–,3 in
the equation. In addition, we can interpret interdependence strength
between pages by checking the weights for the interaction terms
across pages.

ğµğ‘‡
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘– = (1, ğ‘¥ Â· ğ‘ğ‘–,1, ğ‘¥ Â· ğ‘ğ‘–,2, ğ‘¥ Â· ğ‘ğ‘–,3,
ğ‘ğ‘–âˆ’1,1 Â· ğ‘ğ‘–,1, ğ‘ğ‘–âˆ’1,1 Â· ğ‘ğ‘–,2,
ğ‘ğ‘–âˆ’1,2 Â· ğ‘ğ‘–,1, ğ‘ğ‘–âˆ’1,2 Â· ğ‘ğ‘–,2, ğ‘ğ‘–âˆ’1,2 Â· ğ‘ğ‘–,3)

(3)

We assume the weights ğ‘Šğ‘– follow Normal distribution parame-
terized by a mean vector ğğ‘Šğ‘– and variance vector ğ‚ğ‘Šğ‘– . We update
the distribution from prior to posterior by enforcing Normal dis-
tribution as the distribution type and obtaining the distribution
parameters by minimizing KL-divergence from the approximate
normal distribution and the exact distribution.

2.3 RL Policy and Policy Updating
We propose Algorithm 1 MDP with Bandits to solve the formulated
MDP. Dğ‘‘ denotes the dataset for updating contextual Bandit for
Page ğ‘‘. ğ‘˜ denotes the ğ‘˜ğ‘¡â„ impression. ğ‘âˆ—
ğ‘– (ğ‘ğ‘–âˆ’1) denotes the optimal

action for page ğ‘– given content on page ğ‘– âˆ’ 1. The complexity of the
algorithm is ğ‘‚ (ğ·ğ‘ 2ğ¾) assuming number of contents on each page
is ğ‘ for discussion simplicity. For each observation, we sample
weights from their posterior distribution using TS for calculating
transition probability matrix. We then employ exact DP to allocate
the optimal series of content to show on each page given the weights.
Per reward signal availability, we update the weights distributions
and the loop repeats. If we get the feedback signal in batches, the
algorithm alters on Lines 11 and 12 where we would have the
updates at batch level instead of observation level.

3 SIMULATION
Using simulation, we compare the proposed MDP with Bandits algo-
rithm to three other algorithms: independent Bandits, interaction
Bandits and Q-learning. Independent Bandits algorithm assumes
no interdependence across pages. TS for each Bandit model is con-
ducted separately for each page. Interaction Bandits algorithm mod-
els interdependence in two successive pages by using the content
shown in a previous page to construct features in the Bandit for-
mulation. Other than the feature construction, TS for each Bandit
model in the interaction Bandits algorithm is also conducted sepa-
rately. As shown in Table 1, in MDP with Bandits, we use short-term
reward ğ‘…ğ‘– as the feedback signal for Bandits while independent and
interaction Bandits use the long-term reward ğºğ‘– as the feedback
signal. For notation simplicity, we use the tide sign âˆ¼ to introduce
the linear model form used for Equations 1 and 2 with Probit link
function assumed as default. The Bandits forms only involve con-
tent features to highlight the action interaction. MDP with Bandits
and interaction Bandits have the same linear model form. Lift in
performance from MDP with Bandits (if any) directly quantifies the
benefit of modeling the process as an MDP instead of a series of
Bandits. For Q-learning, we follow the vanilla setup: we model the
content shown in the previous page as the state and update the
Q-Table based on Bellman Equation. We set the learning rate to be
0.05 and discount factor to be 1. The policy is based on ğœ–-greedy
with ğœ– linearly decreases from 0.05 (2nd batch) to 0.01 (last batch).

3.1 Simulated Data
We generate simulation data assuming Probit link function between
short-term reward ğ‘…ğ‘– and ğµğ‘‡
ğ‘Šğ‘– based on equations 1 and
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–
2, with ğ›½ set as 1 + ğ›¼1 + ğ›¼ğ‘ + ğ›¼2 and the linear model forms for
ğµğ‘‡
ğ‘Šğ‘– specified as in equation 4 for first page and equation
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–
5 for the other pages. Similar to [3], we use the ğ›¼1, ğ›¼ğ‘ and ğ›¼2 to
separately control the importance of content to recommend on
the current page, content already shown on the previous page,
and the interaction between them. For each simulation run, the
ground truth weights ğ‘Šğ‘– are sampled independently from a normal
ğ‘– . Mean for ğ‘¤ 0
distribution with mean 0 and variance 1 except for ğ‘¤ 0
ğ‘–
is set as Î¦âˆ’1 (ğ‘¥)ğ›½ where ğ‘¥ reflects the average success rate. With a
larger value for ğ›¼2, interdependence across pages is set stronger.

ğµğ‘‡
ğ‘‹ ,ğ‘1

ğ‘Š1 = ğ‘¤ 0

1 + ğ›¼1

ğ‘1âˆ‘ï¸
ğ‘›1=1

ğ‘¤ 1
ğ‘›1

ğ‘1,ğ‘›1 + ğ›¼1

ğ‘¤ 1
ğ‘¥ ğ‘¥

âˆ‘ï¸

ğ‘¥

(4)

Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow

Marble-KDD 21, August 16, 2021, Singapore

Algorithm 1 MDP with Bandits
1: Init D1 = D2 = Â· Â· Â· = Dğ· = âˆ…
2: for ğ‘˜ = 1, Â· Â· Â· , ğ¾ do
3:

Receive context ğ‘¥ğ‘˜ and Initiate E[ğº |ğ‘ğ· ] = 0
Sample ğ‘Š1, Â· Â· Â· ,ğ‘Šğ· from the posterior ğ‘ƒ (ğ‘Š1|D1), Â· Â· Â· , ğ‘ƒ (ğ‘Šğ· |Dğ· )
for ğ‘– = ğ·, ğ· âˆ’ 1, Â· Â· Â· , 2 do

for ğ‘ğ‘–âˆ’1 in content pool of page ğ‘– âˆ’ 1 do:

ğ‘âˆ—
ğ‘– (ğ‘ğ‘–âˆ’1) = argmax
E[ğº |ğ‘ğ‘–âˆ’1] = E[ğ‘…|ğ‘âˆ—

ğ‘ğ‘–

(cid:8) E[ğ‘…|ğ‘ğ‘–, ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘˜ ;ğ‘Šğ‘– ] + (1 âˆ’ E[ğ‘…|ğ‘ğ‘–, ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘˜ ;ğ‘Šğ‘– ]) âˆ— E[ğº |ğ‘ğ‘– ](cid:9)
ğ‘– (ğ‘ğ‘–âˆ’1), ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘˜ ;ğ‘Šğ‘– ] + (1 âˆ’ E[ğ‘…|ğ‘âˆ—

ğ‘– (ğ‘ğ‘–âˆ’1), ğ‘ğ‘–âˆ’1, ğ‘¥ğ‘˜ ;ğ‘Šğ‘– ]) âˆ— E[ğº |ğ‘âˆ—

ğ‘– (ğ‘ğ‘–âˆ’1)]

end for

(cid:8) E[ğ‘…|ğ‘1, ğ‘¥ğ‘˜ ;ğ‘Š1] + (1 âˆ’ ğ¸ [ğ‘…|ğ‘1, ğ‘¥ğ‘˜ ;ğ‘Š1]) E[ğº |ğ‘1](cid:9)

end for
ğ‘âˆ—
1 = argmax
ğ‘1
Display layout {ğ‘âˆ—
1
Update D1 = D1 âˆª (ğ‘¥ğ‘˜, ğ‘1ğ‘˜, ğ‘…1ğ‘˜ ) and update ğğ‘Š1 , ğ‚ğ‘Š1
Update Dğ‘– = Dğ‘– âˆª (ğ‘¥ğ‘˜, ğ‘ğ‘–ğ‘˜, ğ‘ğ‘–âˆ’1,ğ‘˜, ğ‘…ğ‘–,ğ‘˜ ) and update ğğ‘Šğ‘– , ğ‚ğ‘Šğ‘– for ğ‘– âˆˆ {2, 3, Â· Â· Â· , ğ· } if ğ‘ğ‘–ğ‘˜ was presented

)} and observe reward ğ‘…1,ğ‘˜, Â· Â· Â· , ğ‘…ğ·,ğ‘˜

1), Â· Â· Â· , ğ‘âˆ—

ğ· (ğ‘âˆ—

2 (ğ‘âˆ—

ğ·âˆ’1

, ğ‘âˆ—

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:
15: end for

âŠ² Thompson Sampling
âŠ² Line 5 - 11: Exact Dynamic Programming

âŠ² Take actions and collect feedbacks
âŠ² BLIP Updating. Formula (7, 8) in [2]

Table 1: Algorithms in Comparison

Algorithm
MDP with Bandits
Interaction Bandits
Independent Bandit

Linear model form
Page 1: ğ‘…1 âˆ¼ ğ‘1; Page ğ‘–: ğ‘…ğ‘– âˆ¼ ğ‘ğ‘– + ğ‘ğ‘–âˆ’1 + ğ‘ğ‘–âˆ’1 : ğ‘ğ‘– for ğ‘– âˆˆ 2, Â· Â· Â· , ğ· ğ‘‚ (ğ·ğ‘ 2)
ğ‘‚ (ğ·ğ‘ 2)
Page 1: ğº1 âˆ¼ ğ‘1; Page ğ‘–: ğºğ‘– âˆ¼ ğ‘ğ‘– + ğ‘ğ‘–âˆ’1 + ğ‘ğ‘–âˆ’1 : ğ‘ğ‘–
ğ‘‚ (ğ·ğ‘ )
Page 1: ğº1 âˆ¼ ğ‘1; Page ğ‘–: ğºğ‘– âˆ¼ ğ‘ğ‘–

# parameters

using batch size normalized cumulative regret:

ğµğ‘‡
ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–

ğ‘Šğ‘– = ğ‘¤ 0

ğ‘– + ğ›¼1

ğ‘ğ‘–
âˆ‘ï¸

ğ‘›ğ‘– =1

ğ‘¤ 1
ğ‘›ğ‘–

ğ‘ğ‘–,ğ‘›ğ‘– + ğ›¼1

ğ‘¤ 1

ğ‘¥ ğ‘¥+

âˆ‘ï¸

ğ‘¥

ğ›¼ğ‘

ğ›¼2

ğ›¼2

ğ‘ğ‘–âˆ’1âˆ‘ï¸
ğ‘›ğ‘–âˆ’1=1
ğ‘ğ‘–
âˆ‘ï¸

ğ‘›ğ‘– =1
ğ‘ğ‘–
âˆ‘ï¸

ğ‘ğ‘–âˆ’1âˆ‘ï¸
ğ‘›ğ‘–âˆ’1=1

âˆ‘ï¸

ğ‘›ğ‘– =1

ğ‘¥

ğ‘¤ğ‘

ğ‘›ğ‘–âˆ’1

ğ‘ğ‘–âˆ’1,ğ‘›ğ‘–âˆ’1 +

ğ‘¤ 2

ğ‘›ğ‘–,ğ‘›ğ‘–âˆ’1

ğ‘ğ‘–,ğ‘›ğ‘– ğ‘ğ‘–âˆ’1,ğ‘›ğ‘–âˆ’1 +

(5)

ğ‘¤ 2

ğ‘›ğ‘–,ğ‘¥ğ‘ğ‘–,ğ‘›ğ‘– ğ‘¥

for

ğ‘– âˆˆ 2, Â· Â· Â· , ğ·

For each simulation, we sample the actual weights in equations
4 and 5 once and we calculate the long-term reward of each combi-
nation (ğ‘1, Â· Â· Â· , ğ‘ğ· ) as:

E[ğº |ğ‘Š ,ğ‘1, Â· Â· Â· , ğ‘ğ· ] =

E[ğ‘…1 + (1 âˆ’ ğ‘…1)ğ‘…2 + Â· Â· Â· + Î ğ·âˆ’1

ğ‘–=1 (1 âˆ’ ğ‘…ğ‘– )ğ‘…ğ· ]

(6)

where ğ‘…ğ‘– = Î¦(

ğµğ‘‡

ğ‘‹ ,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–
ğ›½

ğ‘Šğ‘–

)

for

ğ‘– âˆˆ 2, Â· Â· Â· , ğ· and ğ‘…1 = Î¦(

ğ‘Š1

ğµğ‘‡

ğ‘‹ ,ğ‘1
ğ›½

).

The series of actions for each algorithm are selected by itself.
Given sampled weights and selected actions, empirical rewards (ğ‘…)
are sampled from the data generation model. Actual reward ğºğ‘˜ is 0
if any ğ‘…ğ‘˜ is 0. Each simulation was run for ğ¾ = 14ğ‘˜ time steps with
updating batch size 1ğ‘˜ to mimic a 2-week experiment. We ran 100
simulations and report the averaged performance of each algorithm

(E[ğº |ğ‘Š , ğ‘âˆ—

1,ğ‘— , Â· Â· Â· , ğ‘âˆ—

ğ·,ğ‘— ] âˆ’ ğº ğ‘—,ğ‘˜ )

(7)

1
#ğ‘Ÿğ‘¢ğ‘›ğ‘ 

#ğ‘Ÿğ‘¢ğ‘›ğ‘ 
âˆ‘ï¸

ğ¾
âˆ‘ï¸

1
ğ‘ğ‘ğ‘¡ğ‘â„ğ‘†ğ‘–ğ‘§ğ‘’

ğ‘—=1
Where E[ğº |ğ‘Š , ğ‘âˆ—

ğ‘˜=1
1,ğ‘— , Â· Â· Â· , ğ‘âˆ—

ğ·,ğ‘— ] is the best expected long-term
reward for ğ‘—ğ‘¡â„ run and ğº ğ‘—,ğ‘˜ is the long-term reward for ğ‘—ğ‘¡â„ run and
ğ‘˜ğ‘¡â„ time step.

3.2 Simulation Results
We first compare the performance with ğ›¼1 = 1, ğ›¼ğ‘ = 1, ğ›¼2 = 2 in
both non-contextual and contextual use case. In the contextual case,
we include 1 categorical feature with 3 equally-likely categories. We
assume 3 pages and each page has 3 content candidates. Results are
shown in figures 1 and 2. The proposed algorithm converges faster
than all the other three. Also, it has the lowest cumulative regret
followed by interaction Bandits. This means when there is consid-
erable interdependence across pages, modeling interactions in the
transition probability reduces regrets and modeling the process as
an MDP instead of separate Bandits further lifts the performance.
On the other hand, bandit algorithms with TS generally outperform
Q-learning with ğœ–-greedy, given its efficiency in exploration.

We then take a look at the impact of number of pages on per-
formance, shown in figure 3. The number of pages varies from 2
to 6 and that of content combinations ğ‘ ğ· varies from 9 to 729.
MDP with Bandits consistently outperforms the other three algo-
rithms. The performance advantage over Q-learning shrinks with
increasing page number.

To test the modelâ€™s sensitivity to interdependence strength, we
conduct a set of simulation runs with ğ›¼2 increasing from 0 to 3 while

Marble-KDD 21, August 16, 2021, Singapore

Figure 1: Example run of algorithms on simulated data with
ğ›¼1 = 1, ğ›¼ğ‘ = 1, ğ›¼2 = 2

Figure 3: Algorithm performance as number of pages vary.
ğ›¼1 = 1, ğ›¼ğ‘ = 1, ğ›¼2 = 2

Figure 2: Example run of algorithms on simulated data with
ğ›¼1 = 1, ğ›¼ğ‘ = 1, ğ›¼2 = 2 and 1 categorical feature with 3 cate-
gories

Figure 4: Algorithm performance as ğ›¼2 is varied. ğ›¼1 = 1, ğ›¼ğ‘ = 1

fixing ğ›¼1 and ğ›¼ğ‘ at 1. Results are shown in figure 4. When ğ›¼2 is 0,
independent bandits algorithm outperforms the other two bandit al-
gorithms as both interaction Bandits and MDP with Bandits require
traffic to learn the weights for interaction terms should actually be
set as 0. As ğ›¼2 increases, MDP with Bandits starts to outperform. As
Q-learning directly learns the Q function by Bellman equation, in-
teraction strength has minor influence to its performance. Overall,
the performance for MDP with Bandits is consistently better than
all other algorithms when interaction strength is greater than 1.

When scanning through figures 1, 2, 3, 4, we find interaction
Bandits is the 2nd best when MDP with Bandits is the best and
its performance is comparable to that of MDP with Bandits. When
interaction strength is small, it can perform better than MDP with
Bandits, as at the left bottom of figure 4. In addition, similar to
MDP with Bandits, the cumulative regrets of interaction Bandits
always level out as in figures 1, 2. These mean interactive Bandit,
where separate contextual Bandits with long-term reward and ac-
tion shown on the previous page as feature are built to model each
page, is a good surrogate modeling strategy to a multi-step RL and
always converges to the optimal actions in the case of stationary
reward. This is because that interactive Bandit samples the best

action for each page under the expected rewards of all the trajec-
tories after the action. With improving performance estimation
regarding each trajectory along training, the algorithm is able to
learn the trajectory with the highest long-term reward. We do not
though consistently observe convergence for independent Bandits
and Q-learning. We observe independent Bandits got stuck in local
optimal actions that maximize short-term but not long-term reward
for some simulation runs, even if it is using the long-term reward.
For Q-learning, theoretically it is guaranteed to converge, but with
the conditions that the learning rate needs to be diverged and each
state-action pair must be visited infinitely often [7]. In practice, itâ€™s
not trivial to tune the parameters and the convergence may not be
obtained.

4 CONCLUSIONS
We proposed a RL algorithm with posterior sampling for recom-
mendation in multi-step linearâ€“flow. We set the problem up as an
MDP with Bandits to model transition probability matrix. At policy
inference time, we use TS to sample the transition probabilities and
allocate the best series of actions with exact dynamic programming.
The algorithm is able to leverage TSâ€™s efficiency in balancing ex-
ploration and exploitation and Banditâ€™s convenience in modeling
actionsâ€™ incompatibility. In the simulation study, we observe the

Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow

Marble-KDD 21, August 16, 2021, Singapore

proposed algorithm outperforms Q-learning with ğœ–-greedy, inde-
pendent Bandits, and interaction Bandits, under different scenarios
as long as there is reasonable interdependence strength across pages.
We also see the proposed algorithm is the most robust to changes in
the interdependence strength. We often face dynamic environments
where actions and interdependence between sequential actions are
subject to change. Robustness to changes is thus a good to have.
On the other hand, we find interactive Bandit, where separate con-
textual Bandits with long-term reward and action shown on the
previous page as feature are built to model each page, is a good
surrogate modeling strategy to a multi-step. Just like the proposed
algorithm, it always converges to the optimal actions in the case of
stationary reward. This is evidence that when the long-term impact
of actions is significant, Bandits if set up properly may not be the
best formulation, but can still be a good baseline.

Our work is a start in using Bandit (1-step RL) to solve stateful RL
(multi-step RL). More efforts are in need to generalize the applica-
tion. For instance, one desired algorithm extension is to use Bandit
to model a multi-stage flow where customers can move forward
with more than two options after seeing an action. In terms of algo-
rithm evaluation, we seek to leverage logged data for counterfactual
evaluation in addition to simulation analysis [1, 6].

ACKNOWLEDGMENTS
We would like to thank Lihong Li and Shipra Agrawal for their
valuable comments and helpful discussions.

REFERENCES
[1] Miroslav DudÃ­k, John Langford, and Lihong Li. 2011. Doubly robust policy evalua-

tion and learning. arXiv preprint arXiv:1103.4601 (2011).

[2] Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich.
2010. Web-scale bayesian click-through rate prediction for sponsored search
advertising in microsoftâ€™s bing search engine. Omnipress.

[3] Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. 2017.
An efficient bandit algorithm for realtime multivariate optimization. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 1813â€“1821.

[4] Ian Osband, Daniel Russo, and Benjamin Van Roy. 2013. (More) efficient reinforce-
ment learning via posterior sampling. In Advances in Neural Information Processing
Systems. 3003â€“3011.

[5] Ian Osband and Benjamin Van Roy. 2017. Why is posterior sampling better than
optimism for reinforcement learning?. In International Conference on Machine
Learning. PMLR, 2701â€“2710.

[6] Alex Strehl, John Langford, Sham Kakade, and Lihong Li. 2010. Learning from

logged implicit exploration data. arXiv preprint arXiv:1003.0120 (2010).

[7] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning 8,

3-4 (1992), 279â€“292.

