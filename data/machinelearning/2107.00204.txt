Markov Decision Process modeled with Bandits for Sequential
Decision Making in Linear-flow

Wenjun Zeng
zengwenj@amazon.com
Amazon.com
Seattle, Washington, United States

Yi Liu
yiam@amazon.com
Amazon.com
Seattle, Washington, United States

2
2
0
2

r
a

M
6
1

]

G
L
.
s
c
[

2
v
4
0
2
0
0
.
7
0
1
2
:
v
i
X
r
a

ABSTRACT
For marketing, we sometimes need to recommend content for mul-
tiple pages in sequence. Different from general sequential decision
making process, the use cases have a simpler flow where customers
per seeing recommended content on each page can only return feed-
back as moving forward in the process or dropping from it until a
termination state. We refer to this type of problems as sequential
decision making in linear–flow. We propose to formulate the prob-
lem as an MDP with Bandits where Bandits are employed to model
the transition probability matrix. At recommendation time, we use
Thompson sampling (TS) to sample the transition probabilities and
allocate the best series of actions with analytical solution through
exact dynamic programming. The way that we formulate the prob-
lem allows us to leverage TS’s efficiency in balancing exploration
and exploitation and Bandit’s convenience in modeling actions’
incompatibility. In the simulation study, we observe the proposed
MDP with Bandits algorithm outperforms Q-learning with 𝜖-greedy
and decreasing 𝜖, independent Bandits, and interaction Bandits. We
also find the proposed algorithm’s performance is the most robust
to changes in the across-page interdependence strength.

KEYWORDS
Bandit, Reinforcement Learning, posteriror sampling, MDP, recom-
mendation

ACM Reference Format:
Wenjun Zeng and Yi Liu. 2021. Markov Decision Process modeled with Ban-
dits for Sequential Decision Making in Linear-flow. In Proceedings of Marble-
KDD 21. Singapore, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
For marketing, we sometimes need to recommend content for mul-
tiple pages in sequence. At each page, users can choose to continue
the process or leave. Use cases like these require sequential decision
making process since each page has a set of content candidates and
the algorithm needs to make recommendation considering inter-
dependence between content on successive pages. Different from
general sequential decision making process, it is a simpler flow
where customers per seeing recommended content on each page
can only move forward in the process or drop from it. We define
problems like this as sequential decision making in linear–flow.
We observe this type of problems in customer engagement beyond
content recommendation. For instance, it is common to take a series
of nudges, such as promotion email and limited-time discount offer,
to increase customer’s activity. We can consider customers enter

Marble-KDD 21, August 16, 2021, Singapore
2021. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

the workflow when their interactions drop below a threshold, and
leave the flow with reward as +1 if their interactions increase to a
healthy level or −1 if customers become inactive.

In this paper, we formulate sequential decision making process
in linear-flow as a Markov Decision Process (MDP). We model the
transition probability matrix with contextual Bayesian Bandits [3],
use Thompson Sampling (TS) as the exploration strategy, and apply
exact Dynamic Programming (DP) to solve the MDP. Modeling
transition probability matrix with contextual Bandits makes it con-
venient to specify actions’ incompatibility, when certain action
pairs at different steps are ineligible. It also makes it straightfor-
ward to interpret interdependence strength between actions in two
successive situations. Our algorithm is a realization of the Posterior
Sampling for Reinforcement Learning (PSRL) framework [4, 5] with
one difference. PSRL framework assumes each (state, action) pair is
modeled separately while we model the pairs at a step using one
Bayesian Bandit model. We present our algorithm in section 2. We
use simulator to evaluate the proposed algorithm comparing to
Q-learning with 𝜖-greedy and decreasing 𝜖, independent Bandits,
and interaction Bandits. The evaluation results are documented in
section 3. We then conclude the paper in section 4. In the remainder
of this paper, we relate writing to content recommendation appli-
cations while the algorithm works for any linear-flow problems.

2 METHODS
2.1 Problem Formulation
We model sequential decision making in linear flow as an MDP.
Let 𝐷 be number of the pages that content recommendation is
needed. For each page 𝑖, there are 𝑁𝑖 content candidates that we
can choose from. The selected content can be denoted as 𝑎𝑖,𝑛𝑖 where
𝑛𝑖 ∈ {1, 2, · · · , 𝑁𝑖 }. For simplicity, we will denote the content shown
on page 𝑖 as 𝑎𝑖 unless the notation 𝑎𝑖,𝑛𝑖 is needed to distinguish
between different content for page 𝑖. The flow starts with state
𝑆1,𝑋 where 𝑋 is customer feature vector. After action 𝑎𝑖 is selected
to show to customer with features 𝑋 on page 𝑖, there are two
possible states: 𝑆𝑖+1,𝑋 ,𝑎𝑖 and 𝑆𝐸𝑥𝑖𝑡 . The only exception is for the
last page, page 𝐷, the two possible states after action 𝑎𝐷 are: 𝑆𝐸𝑛𝑑
and 𝑆𝐸𝑥𝑖𝑡 . 𝑆𝑖+1,𝑋 ,𝑎𝑖 means customers move forward in the process
to the next page after seeing 𝑎𝑖 . 𝑆𝐸𝑥𝑖𝑡 means customers drop from
the process without reaching the end and 𝑆𝐸𝑛𝑑 means customers
reached the end. We denote 𝑅𝑖 and 𝐺𝑖 as short–term and long–term
rewards after action 𝑎𝑖 respectively. The goal is to optimize 𝐺1.
Customers are at 𝑆1,𝑋 after they trigger the flow. 𝑋 may include
customer features, the upstream channel that customers land from,
etc. Customers get to 𝑆𝐸𝑥𝑖𝑡 if they drop from the process at any
page leading to 𝑅𝑖 = 1. Customers are in 𝑆𝑖+1,𝑋 ,𝑎𝑖 if they move on

 
 
 
 
 
 
Marble-KDD 21, August 16, 2021, Singapore

in the process after marketing content 𝑎𝑖 . The short-term reward is
0 from entering 𝑆𝑖+1,𝑋 ,𝑎𝑖 . 𝑆𝐸𝑛𝑑 is another termination state other
than 𝑆𝐸𝑥𝑖𝑡 . Customers reach this state if they decide not to take the
offer after seeing content on the last page. This leads to 𝑅𝐷 = 0.

2.2 Transition Probability Matrix Modeling
We use contextual Bandit to model and update transition probabil-
ity matrix. As our rewards are binary and there can be only two
outcomes per action, we select BLIP [2] as the Bandit algorithm. The
transition probability is formulated through a Probit link function.
After showing page 𝑖, the probability of customers moving forward
in the process and dropping from the process is formulated as in
Equations 1 and 2 respectively. By using this formulation, we model
only interdependence between two successive pages. For page 1,
𝑎𝑖−1 is dropped from the subscripts but the rest of the equations
remain the same.

𝑃 (𝑆𝑡 +1 = 𝑆𝑖+1,𝑋 ,𝑎𝑖 |𝑆𝑡 = 𝑆𝑖,𝑋 ,𝑎𝑖−1

, 𝑎𝑡 = 𝑎𝑖 ) = Φ(−1 ∗

𝐵𝑇
𝑋 ,𝑎𝑖−1,𝑎𝑖
𝛽

𝑊𝑖

)

(1)

𝑃 (𝑆𝑡 +1 = 𝑆𝐸𝑥𝑖𝑡 |𝑆𝑡 = 𝑆𝑖,𝑋 ,𝑎𝑖−1

, 𝑎𝑡 = 𝑎𝑖 ) = Φ(

𝑊𝑖

𝐵𝑇
𝑋 ,𝑎𝑖−1,𝑎𝑖
𝛽

)

(2)

In the formula, Φ is cumulative distribution function for stan-
dard normal distribution. 𝛽 is a scaling hyperparameter. 𝑊𝑖 are
the weights quantifying feature contributions to the utility func-
tion. 𝐵𝑋 ,𝑎𝑖−1,𝑎𝑖 is the final vector that we will learn weights for
and formed by combining 𝑋 , 𝑎𝑖−1 and 𝑎𝑖 (usually with interaction
terms). 𝐵𝑋 ,𝑎𝑖−1,𝑎𝑖 is the reason why we can easily model content in-
compatibility. To explain this, let’s first assume we have one feature
𝑥 for 𝑋 , 𝑎𝑖−1 has two candidates: 𝑎𝑖−1,1 and 𝑎𝑖−1,2, and 𝑎𝑖 has three
candidates: 𝑎𝑖,1, 𝑎𝑖,2 and 𝑎𝑖,3. We assume 𝑎𝑖,3 cannot be shown if
𝑎𝑖−1,1 is shown to customers. We may set 𝐵𝑎𝑖−1,𝑎𝑖 as in Equation 3
where we have 1 for intercept and contextual feature interacting
with action terms. We recognize content incompatibility between
𝑎𝑖−1,1 and 𝑎𝑖,3 by simply omitting interaction term 𝑥 · 𝑎𝑖−1,1 · 𝑎𝑖,3 in
the equation. In addition, we can interpret interdependence strength
between pages by checking the weights for the interaction terms
across pages.

𝐵𝑇
𝑋 ,𝑎𝑖−1,𝑎𝑖 = (1, 𝑥 · 𝑎𝑖,1, 𝑥 · 𝑎𝑖,2, 𝑥 · 𝑎𝑖,3,
𝑎𝑖−1,1 · 𝑎𝑖,1, 𝑎𝑖−1,1 · 𝑎𝑖,2,
𝑎𝑖−1,2 · 𝑎𝑖,1, 𝑎𝑖−1,2 · 𝑎𝑖,2, 𝑎𝑖−1,2 · 𝑎𝑖,3)

(3)

We assume the weights 𝑊𝑖 follow Normal distribution parame-
terized by a mean vector 𝝁𝑊𝑖 and variance vector 𝝂𝑊𝑖 . We update
the distribution from prior to posterior by enforcing Normal dis-
tribution as the distribution type and obtaining the distribution
parameters by minimizing KL-divergence from the approximate
normal distribution and the exact distribution.

2.3 RL Policy and Policy Updating
We propose Algorithm 1 MDP with Bandits to solve the formulated
MDP. D𝑑 denotes the dataset for updating contextual Bandit for
Page 𝑑. 𝑘 denotes the 𝑘𝑡ℎ impression. 𝑎∗
𝑖 (𝑎𝑖−1) denotes the optimal

action for page 𝑖 given content on page 𝑖 − 1. The complexity of the
algorithm is 𝑂 (𝐷𝑁 2𝐾) assuming number of contents on each page
is 𝑁 for discussion simplicity. For each observation, we sample
weights from their posterior distribution using TS for calculating
transition probability matrix. We then employ exact DP to allocate
the optimal series of content to show on each page given the weights.
Per reward signal availability, we update the weights distributions
and the loop repeats. If we get the feedback signal in batches, the
algorithm alters on Lines 11 and 12 where we would have the
updates at batch level instead of observation level.

3 SIMULATION
Using simulation, we compare the proposed MDP with Bandits algo-
rithm to three other algorithms: independent Bandits, interaction
Bandits and Q-learning. Independent Bandits algorithm assumes
no interdependence across pages. TS for each Bandit model is con-
ducted separately for each page. Interaction Bandits algorithm mod-
els interdependence in two successive pages by using the content
shown in a previous page to construct features in the Bandit for-
mulation. Other than the feature construction, TS for each Bandit
model in the interaction Bandits algorithm is also conducted sepa-
rately. As shown in Table 1, in MDP with Bandits, we use short-term
reward 𝑅𝑖 as the feedback signal for Bandits while independent and
interaction Bandits use the long-term reward 𝐺𝑖 as the feedback
signal. For notation simplicity, we use the tide sign ∼ to introduce
the linear model form used for Equations 1 and 2 with Probit link
function assumed as default. The Bandits forms only involve con-
tent features to highlight the action interaction. MDP with Bandits
and interaction Bandits have the same linear model form. Lift in
performance from MDP with Bandits (if any) directly quantifies the
benefit of modeling the process as an MDP instead of a series of
Bandits. For Q-learning, we follow the vanilla setup: we model the
content shown in the previous page as the state and update the
Q-Table based on Bellman Equation. We set the learning rate to be
0.05 and discount factor to be 1. The policy is based on 𝜖-greedy
with 𝜖 linearly decreases from 0.05 (2nd batch) to 0.01 (last batch).

3.1 Simulated Data
We generate simulation data assuming Probit link function between
short-term reward 𝑅𝑖 and 𝐵𝑇
𝑊𝑖 based on equations 1 and
𝑋 ,𝑎𝑖−1,𝑎𝑖
2, with 𝛽 set as 1 + 𝛼1 + 𝛼𝑐 + 𝛼2 and the linear model forms for
𝐵𝑇
𝑊𝑖 specified as in equation 4 for first page and equation
𝑋 ,𝑎𝑖−1,𝑎𝑖
5 for the other pages. Similar to [3], we use the 𝛼1, 𝛼𝑐 and 𝛼2 to
separately control the importance of content to recommend on
the current page, content already shown on the previous page,
and the interaction between them. For each simulation run, the
ground truth weights 𝑊𝑖 are sampled independently from a normal
𝑖 . Mean for 𝑤 0
distribution with mean 0 and variance 1 except for 𝑤 0
𝑖
is set as Φ−1 (𝑥)𝛽 where 𝑥 reflects the average success rate. With a
larger value for 𝛼2, interdependence across pages is set stronger.

𝐵𝑇
𝑋 ,𝑎1

𝑊1 = 𝑤 0

1 + 𝛼1

𝑁1∑︁
𝑛1=1

𝑤 1
𝑛1

𝑎1,𝑛1 + 𝛼1

𝑤 1
𝑥 𝑥

∑︁

𝑥

(4)

Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow

Marble-KDD 21, August 16, 2021, Singapore

Algorithm 1 MDP with Bandits
1: Init D1 = D2 = · · · = D𝐷 = ∅
2: for 𝑘 = 1, · · · , 𝐾 do
3:

Receive context 𝑥𝑘 and Initiate E[𝐺 |𝑎𝐷 ] = 0
Sample 𝑊1, · · · ,𝑊𝐷 from the posterior 𝑃 (𝑊1|D1), · · · , 𝑃 (𝑊𝐷 |D𝐷 )
for 𝑖 = 𝐷, 𝐷 − 1, · · · , 2 do

for 𝑎𝑖−1 in content pool of page 𝑖 − 1 do:

𝑎∗
𝑖 (𝑎𝑖−1) = argmax
E[𝐺 |𝑎𝑖−1] = E[𝑅|𝑎∗

𝑎𝑖

(cid:8) E[𝑅|𝑎𝑖, 𝑎𝑖−1, 𝑥𝑘 ;𝑊𝑖 ] + (1 − E[𝑅|𝑎𝑖, 𝑎𝑖−1, 𝑥𝑘 ;𝑊𝑖 ]) ∗ E[𝐺 |𝑎𝑖 ](cid:9)
𝑖 (𝑎𝑖−1), 𝑎𝑖−1, 𝑥𝑘 ;𝑊𝑖 ] + (1 − E[𝑅|𝑎∗

𝑖 (𝑎𝑖−1), 𝑎𝑖−1, 𝑥𝑘 ;𝑊𝑖 ]) ∗ E[𝐺 |𝑎∗

𝑖 (𝑎𝑖−1)]

end for

(cid:8) E[𝑅|𝑎1, 𝑥𝑘 ;𝑊1] + (1 − 𝐸 [𝑅|𝑎1, 𝑥𝑘 ;𝑊1]) E[𝐺 |𝑎1](cid:9)

end for
𝑎∗
1 = argmax
𝑎1
Display layout {𝑎∗
1
Update D1 = D1 ∪ (𝑥𝑘, 𝑎1𝑘, 𝑅1𝑘 ) and update 𝝁𝑊1 , 𝝂𝑊1
Update D𝑖 = D𝑖 ∪ (𝑥𝑘, 𝑎𝑖𝑘, 𝑎𝑖−1,𝑘, 𝑅𝑖,𝑘 ) and update 𝝁𝑊𝑖 , 𝝂𝑊𝑖 for 𝑖 ∈ {2, 3, · · · , 𝐷 } if 𝑎𝑖𝑘 was presented

)} and observe reward 𝑅1,𝑘, · · · , 𝑅𝐷,𝑘

1), · · · , 𝑎∗

𝐷 (𝑎∗

2 (𝑎∗

𝐷−1

, 𝑎∗

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:
15: end for

⊲ Thompson Sampling
⊲ Line 5 - 11: Exact Dynamic Programming

⊲ Take actions and collect feedbacks
⊲ BLIP Updating. Formula (7, 8) in [2]

Table 1: Algorithms in Comparison

Algorithm
MDP with Bandits
Interaction Bandits
Independent Bandit

Linear model form
Page 1: 𝑅1 ∼ 𝑎1; Page 𝑖: 𝑅𝑖 ∼ 𝑎𝑖 + 𝑎𝑖−1 + 𝑎𝑖−1 : 𝑎𝑖 for 𝑖 ∈ 2, · · · , 𝐷 𝑂 (𝐷𝑁 2)
𝑂 (𝐷𝑁 2)
Page 1: 𝐺1 ∼ 𝑎1; Page 𝑖: 𝐺𝑖 ∼ 𝑎𝑖 + 𝑎𝑖−1 + 𝑎𝑖−1 : 𝑎𝑖
𝑂 (𝐷𝑁 )
Page 1: 𝐺1 ∼ 𝑎1; Page 𝑖: 𝐺𝑖 ∼ 𝑎𝑖

# parameters

using batch size normalized cumulative regret:

𝐵𝑇
𝑋 ,𝑎𝑖−1,𝑎𝑖

𝑊𝑖 = 𝑤 0

𝑖 + 𝛼1

𝑁𝑖
∑︁

𝑛𝑖 =1

𝑤 1
𝑛𝑖

𝑎𝑖,𝑛𝑖 + 𝛼1

𝑤 1

𝑥 𝑥+

∑︁

𝑥

𝛼𝑐

𝛼2

𝛼2

𝑁𝑖−1∑︁
𝑛𝑖−1=1
𝑁𝑖
∑︁

𝑛𝑖 =1
𝑁𝑖
∑︁

𝑁𝑖−1∑︁
𝑛𝑖−1=1

∑︁

𝑛𝑖 =1

𝑥

𝑤𝑐

𝑛𝑖−1

𝑎𝑖−1,𝑛𝑖−1 +

𝑤 2

𝑛𝑖,𝑛𝑖−1

𝑎𝑖,𝑛𝑖 𝑎𝑖−1,𝑛𝑖−1 +

(5)

𝑤 2

𝑛𝑖,𝑥𝑎𝑖,𝑛𝑖 𝑥

for

𝑖 ∈ 2, · · · , 𝐷

For each simulation, we sample the actual weights in equations
4 and 5 once and we calculate the long-term reward of each combi-
nation (𝑎1, · · · , 𝑎𝐷 ) as:

E[𝐺 |𝑊 ,𝑎1, · · · , 𝑎𝐷 ] =

E[𝑅1 + (1 − 𝑅1)𝑅2 + · · · + Π𝐷−1

𝑖=1 (1 − 𝑅𝑖 )𝑅𝐷 ]

(6)

where 𝑅𝑖 = Φ(

𝐵𝑇

𝑋 ,𝑎𝑖−1,𝑎𝑖
𝛽

𝑊𝑖

)

for

𝑖 ∈ 2, · · · , 𝐷 and 𝑅1 = Φ(

𝑊1

𝐵𝑇

𝑋 ,𝑎1
𝛽

).

The series of actions for each algorithm are selected by itself.
Given sampled weights and selected actions, empirical rewards (𝑅)
are sampled from the data generation model. Actual reward 𝐺𝑘 is 0
if any 𝑅𝑘 is 0. Each simulation was run for 𝐾 = 14𝑘 time steps with
updating batch size 1𝑘 to mimic a 2-week experiment. We ran 100
simulations and report the averaged performance of each algorithm

(E[𝐺 |𝑊 , 𝑎∗

1,𝑗 , · · · , 𝑎∗

𝐷,𝑗 ] − 𝐺 𝑗,𝑘 )

(7)

1
#𝑟𝑢𝑛𝑠

#𝑟𝑢𝑛𝑠
∑︁

𝐾
∑︁

1
𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒

𝑗=1
Where E[𝐺 |𝑊 , 𝑎∗

𝑘=1
1,𝑗 , · · · , 𝑎∗

𝐷,𝑗 ] is the best expected long-term
reward for 𝑗𝑡ℎ run and 𝐺 𝑗,𝑘 is the long-term reward for 𝑗𝑡ℎ run and
𝑘𝑡ℎ time step.

3.2 Simulation Results
We first compare the performance with 𝛼1 = 1, 𝛼𝑐 = 1, 𝛼2 = 2 in
both non-contextual and contextual use case. In the contextual case,
we include 1 categorical feature with 3 equally-likely categories. We
assume 3 pages and each page has 3 content candidates. Results are
shown in figures 1 and 2. The proposed algorithm converges faster
than all the other three. Also, it has the lowest cumulative regret
followed by interaction Bandits. This means when there is consid-
erable interdependence across pages, modeling interactions in the
transition probability reduces regrets and modeling the process as
an MDP instead of separate Bandits further lifts the performance.
On the other hand, bandit algorithms with TS generally outperform
Q-learning with 𝜖-greedy, given its efficiency in exploration.

We then take a look at the impact of number of pages on per-
formance, shown in figure 3. The number of pages varies from 2
to 6 and that of content combinations 𝑁 𝐷 varies from 9 to 729.
MDP with Bandits consistently outperforms the other three algo-
rithms. The performance advantage over Q-learning shrinks with
increasing page number.

To test the model’s sensitivity to interdependence strength, we
conduct a set of simulation runs with 𝛼2 increasing from 0 to 3 while

Marble-KDD 21, August 16, 2021, Singapore

Figure 1: Example run of algorithms on simulated data with
𝛼1 = 1, 𝛼𝑐 = 1, 𝛼2 = 2

Figure 3: Algorithm performance as number of pages vary.
𝛼1 = 1, 𝛼𝑐 = 1, 𝛼2 = 2

Figure 2: Example run of algorithms on simulated data with
𝛼1 = 1, 𝛼𝑐 = 1, 𝛼2 = 2 and 1 categorical feature with 3 cate-
gories

Figure 4: Algorithm performance as 𝛼2 is varied. 𝛼1 = 1, 𝛼𝑐 = 1

fixing 𝛼1 and 𝛼𝑐 at 1. Results are shown in figure 4. When 𝛼2 is 0,
independent bandits algorithm outperforms the other two bandit al-
gorithms as both interaction Bandits and MDP with Bandits require
traffic to learn the weights for interaction terms should actually be
set as 0. As 𝛼2 increases, MDP with Bandits starts to outperform. As
Q-learning directly learns the Q function by Bellman equation, in-
teraction strength has minor influence to its performance. Overall,
the performance for MDP with Bandits is consistently better than
all other algorithms when interaction strength is greater than 1.

When scanning through figures 1, 2, 3, 4, we find interaction
Bandits is the 2nd best when MDP with Bandits is the best and
its performance is comparable to that of MDP with Bandits. When
interaction strength is small, it can perform better than MDP with
Bandits, as at the left bottom of figure 4. In addition, similar to
MDP with Bandits, the cumulative regrets of interaction Bandits
always level out as in figures 1, 2. These mean interactive Bandit,
where separate contextual Bandits with long-term reward and ac-
tion shown on the previous page as feature are built to model each
page, is a good surrogate modeling strategy to a multi-step RL and
always converges to the optimal actions in the case of stationary
reward. This is because that interactive Bandit samples the best

action for each page under the expected rewards of all the trajec-
tories after the action. With improving performance estimation
regarding each trajectory along training, the algorithm is able to
learn the trajectory with the highest long-term reward. We do not
though consistently observe convergence for independent Bandits
and Q-learning. We observe independent Bandits got stuck in local
optimal actions that maximize short-term but not long-term reward
for some simulation runs, even if it is using the long-term reward.
For Q-learning, theoretically it is guaranteed to converge, but with
the conditions that the learning rate needs to be diverged and each
state-action pair must be visited infinitely often [7]. In practice, it’s
not trivial to tune the parameters and the convergence may not be
obtained.

4 CONCLUSIONS
We proposed a RL algorithm with posterior sampling for recom-
mendation in multi-step linear–flow. We set the problem up as an
MDP with Bandits to model transition probability matrix. At policy
inference time, we use TS to sample the transition probabilities and
allocate the best series of actions with exact dynamic programming.
The algorithm is able to leverage TS’s efficiency in balancing ex-
ploration and exploitation and Bandit’s convenience in modeling
actions’ incompatibility. In the simulation study, we observe the

Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow

Marble-KDD 21, August 16, 2021, Singapore

proposed algorithm outperforms Q-learning with 𝜖-greedy, inde-
pendent Bandits, and interaction Bandits, under different scenarios
as long as there is reasonable interdependence strength across pages.
We also see the proposed algorithm is the most robust to changes in
the interdependence strength. We often face dynamic environments
where actions and interdependence between sequential actions are
subject to change. Robustness to changes is thus a good to have.
On the other hand, we find interactive Bandit, where separate con-
textual Bandits with long-term reward and action shown on the
previous page as feature are built to model each page, is a good
surrogate modeling strategy to a multi-step. Just like the proposed
algorithm, it always converges to the optimal actions in the case of
stationary reward. This is evidence that when the long-term impact
of actions is significant, Bandits if set up properly may not be the
best formulation, but can still be a good baseline.

Our work is a start in using Bandit (1-step RL) to solve stateful RL
(multi-step RL). More efforts are in need to generalize the applica-
tion. For instance, one desired algorithm extension is to use Bandit
to model a multi-stage flow where customers can move forward
with more than two options after seeing an action. In terms of algo-
rithm evaluation, we seek to leverage logged data for counterfactual
evaluation in addition to simulation analysis [1, 6].

ACKNOWLEDGMENTS
We would like to thank Lihong Li and Shipra Agrawal for their
valuable comments and helpful discussions.

REFERENCES
[1] Miroslav Dudík, John Langford, and Lihong Li. 2011. Doubly robust policy evalua-

tion and learning. arXiv preprint arXiv:1103.4601 (2011).

[2] Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich.
2010. Web-scale bayesian click-through rate prediction for sponsored search
advertising in microsoft’s bing search engine. Omnipress.

[3] Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. 2017.
An efficient bandit algorithm for realtime multivariate optimization. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 1813–1821.

[4] Ian Osband, Daniel Russo, and Benjamin Van Roy. 2013. (More) efficient reinforce-
ment learning via posterior sampling. In Advances in Neural Information Processing
Systems. 3003–3011.

[5] Ian Osband and Benjamin Van Roy. 2017. Why is posterior sampling better than
optimism for reinforcement learning?. In International Conference on Machine
Learning. PMLR, 2701–2710.

[6] Alex Strehl, John Langford, Sham Kakade, and Lihong Li. 2010. Learning from

logged implicit exploration data. arXiv preprint arXiv:1003.0120 (2010).

[7] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning 8,

3-4 (1992), 279–292.

