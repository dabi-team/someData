Diﬀerentiable Programming `a la Moreau

Vincent Roulet, Zaid Harchaoui
Department of Statistics, University of Washington, Seattle, USA

0
2
0
2

c
e
D
1
3

]

C
O
.
h
t
a
m

[

1
v
8
5
4
5
1
.
2
1
0
2
:
v
i
X
r
a

Abstract

The notion of a Moreau envelope is central to the analysis of ﬁrst-order optimization algorithms for
machine learning. Yet, it has not been developed and extended to be applied to a deep network and, more
broadly, to a machine learning system with a diﬀerentiable programming implementation. We deﬁne a
compositional calculus adapted to Moreau envelopes and show how to integrate it within diﬀerentiable
programming. The proposed framework casts in a mathematical optimization framework several variants
of gradient back-propagation related to the idea of the propagation of virtual targets.

Introduction

We look into diﬀerentiable programming through the lens of the black-box model of ﬁrst-order optimiza-
tion (Guzm´an and Nemirovski, 2015; Bubeck and Lee, 2016). Here a ﬁrst-order oracle returns ﬁrst-order
information on a chain of compositions of the form

f (w, x) = φτ (wτ , φτ −1(wτ −1, . . . , φ1(w1, x))).

First-order oracles of this kind arise in gradient-based training of deep networks (Lecun, 1988; Goodfellow
et al., 2016), and more broadly end-to-end training of modern machine learning systems (Goodfellow et al.,
2016), as well as in various nonlinear control or reinforcement learning problems (Bertsekas, 2005).

The computation structure departs from the one considered in convex optimization for linear predic-
tion (Sra et al., 2012), in that the function involved is a chain of compositions f of elementary functions
(φt). The simplest instance of a ﬁrst-order oracle is the one returning the evaluation and the gradient, which
allows to build a local linear surrogate of the objective then used for gradient-based optimization (Guzm´an
and Nemirovski, 2015; Bubeck and Lee, 2016). Obtaining the gradient then amounts to applying the chain-
rule, which is nowadays usually implemented using automatic diﬀerentiation for deep neural networks and
other complex models (Pearlmutter, 1994). The broad outreach of automatic diﬀerentiation gives rise to
the folk notion of a diﬀerentiable program, that is, a numerical program that not only outputs a numerical
value (or a data structure ﬁlled with numerical values) but also stores intermediate gradients to allow an
automatic diﬀerentiation library to produce Jacobian-vector products. Modern machine learning libraries
are written in a diﬀerentiable programming spirit (Paszke et al., 2017; Abadi et al., 2015).

As diﬀerentiable programming stands out as a computational framework tailored for training models
using ﬁrst-order optimization, one may ask how the notion of Moreau envelope could ﬁt into it and expand
its scope. Indeed, the notion of Moreau envelope has arisen as a central notion in the analysis of ﬁrst-order
optimization algorithms for machine learning (Duchi and Ruan, 2018; Lin et al., 2018; Drusvyatskiy and
Paquette, 2019). As we shall see this inquiry opens up interesting venues. First, to blend Moreau envelopes
into diﬀerentiable programming, one needs to deﬁne a calculus adapted to Moreau envelopes. We construct
a framework to deﬁne such a calculus and show how to integrate it within diﬀerentiable programming. Along
the way, we encounter computational building blocks similar to the ones considered in variants of gradient
back-propagation which can be traced back to the so-called virtual target propagation (Lecun, 1988; Rohwer,
1990; Mirowski and LeCun, 2009; Bengio, 2014). We present convergence guarantees for template algorithms
and numerical results in nonlinear control and deep learning.

1

 
 
 
 
 
 
1 Diﬀerentiable Program for the Moreau Envelope

Goal. We seek to build and study a machinery akin to automatic diﬀerentiation to both evaluate the value
of a chain of compositions of modules, i.e., numerical programs, and enable the approximate computation
of the gradient of the Moreau envelope of this chain using the ones of each module.

Formally we seek to deﬁne a numerical program M, which implements
(cid:40)

M :

Rd → Rm × (Rm → Rd)
x

(cid:55)→ (f (x), λ (cid:55)→ ∇ env(λ(cid:62)f )(x)),

where env refers to the Moreau envelope and ∇ env refers to its gradient which we shall refer to as the
Moreau gradient.

We shall ﬁrst deﬁne a chain of computations and then the Moreau envelope and the Moreau gradient. In
full generality, automatic diﬀerentiation is deﬁned for graphs of computations. However, we restrict ourselves
to chains of computations, for simplicity.

Chain of computations. With diﬀerentiable programming for machine learning in mind, we deﬁne here
a chain of computation as follows.

Deﬁnition 1.1. A function f : Rp × Rd → Rm is a parameterized chain of τ computations, if it is deﬁned
by τ functions φt : Rpt × Rqt−1 → Rqt for t = 1, . . . , τ such that for x0 ∈ Rd, w = (w1; . . . ; wτ ) ∈ Rp, with
wt ∈ Rpt, the output of f is given by

f (w, x0) = xτ , with xt = φt(wt, xt−1) for t = 1, ..., τ,

(1)

where p= (cid:80)τ

t=1 pt, d=q0, m=qτ .

This formulation describes a deep network where φt is the composition of a bilinear function and a
non-linear one. For example, AlexNet is described as Conv → ReLU → MaxPool → . . . → Conv → ReLU
→ MaxPool, where Conv is a convolution (bilinear operation) between the parameters of the network and
ReLU and max-pooling are non-linear functions (Krizhevsky et al., 2012). The general formulation of a deep
network as a chain of computations is given in Appendix B. The above formulation allows us to encompass
many popular deep architectures.

Automatic diﬀerentiation. Key to the minimization of parameterized chains of computations is the
availability of ﬁrst order information via automatic diﬀerentiation. Automatic diﬀerentiation relies on the
implementation of a function as a diﬀerentiable program such that one can easily obtain gradients. Formally,
a diﬀerentiable program P implements the evaluation of a function f and enables the computation of any
gradient vector product of this function, i.e.,

(cid:40)

P :

Rd → Rm × (Rm → Rd)
x

(cid:55)→ (f (x), λ (cid:55)→ ∇f (x)λ)

.

The implementation of a function as a diﬀerentiable program relies on the expression of the computation of
the gradient as a sub-problem, i.e.,

∇f (x)λ = − argmin

y∈Rd

(cid:96)λ(cid:62)f (x + y; x) +

(cid:26)

(cid:27)

,

(cid:107)y(cid:107)2
2

1
2

(2)

where (cid:96)λ(cid:62)f (x + y; x) = λ(cid:62)f (x) + ∇(λ(cid:62)f )(x)(cid:62)(y − x) is a linear approximation of the function. For chains
of computations, a diﬀerentiable program instantiates problem (2) by expressing (cid:96)λ(cid:62)f (x + y; x) using the
linearizations of the computations stored in memory. The gradient is then computed by calling a sub-routine
that solves (2) using the decomposition of (cid:96)λ(cid:62)f (x + y; x). Namely, it can be seen as a dynamic program
applied to (2) as shown in the Appendix D.

2

Moreau envelope. The linear approximation (2) deﬁning the gradient can be replaced by any surrogate of
the function; see e.g. (Lan, 2020; Nesterov, 2018). Yet as soon as the approximation is based on an analytic
approximation of the objective, the information provided by the oracle is limited into a region deﬁned by the
smoothness properties of the function (Guzm´an and Nemirovski, 2015). The Moreau envelope overcomes
this issue by deﬁning an oracle through the minimization of the function itself (Moreau, 1962; Bauschke and
Combettes, 2017). Formally, for a real function f : Rd → R and α > 0 such that x (cid:55)→ f (x) + (cid:107)x(cid:107)2
2/(2α) is
strongly convex1, the Moreau envelope is deﬁned as follows
(cid:27)

(cid:26)

(cid:27)

(cid:26)

envα(f )(x) = inf
y∈Rd

f (y) +

1
2α

(cid:107)x − y(cid:107)2
2

= inf
y∈Rd

f (x + y) +

1
2α

(cid:107)y(cid:107)2
2

,

(3)

and its gradient, called in the following a Moreau gradient, is deﬁned by

α∇ envα(f )(x) = ∇ env(αf )(x) = − argmin

αf (x + y) +

y∈Rd

(cid:26)

(cid:27)

,

(cid:107)y(cid:107)2
2

1
2

where env(αf ) = env1(αf ). The parameter α acts as a step-size for the oracle, note that α∇ env(f )(x) (cid:54)=
∇ env(αf )(x), i.e., the step-size is part of the deﬁnition of the oracle. The Moreau envelope and its gradient
yield smooth surrogates of the function and the gradient of the function. For example, if a function is convex,
its Moreau envelope is well-deﬁned for any step-size α > 0 and 1/α smooth even if the function was not
smooth. In general, the Moreau gradient may provide information on the function on larger regions than
the usual gradient, at a cost of solving (3), see Appendix C for more details.

In practice, one usually approximates the Moreau envelope using an optimiza-
Approximate envelope.
tion algorithm to solve the problem deﬁning the envelope; see e.g. (Lin et al., 2018). Namely, for f diﬀer-
entiable and α > 0 chosen such that x (cid:55)→ f (x) + (cid:107)x(cid:107)2
2/(2α) is strongly convex, the Moreau gradient can be
approximately computed by an iterative procedure such as gradient descent, i.e.

∇ env(αf )(x)=− lim

Ak

k→+∞

(cid:18)

αf (x + ·) +

(cid:19)

,

(cid:107) · (cid:107)2
2

1
2

(4)

where Ak (h) is the kth output of an algorithm A applied to minimize a function h.

Moreau gradient for multivariate functions. For a multivariate function f : Rd → Rm, a classical
gradient encodes the linear form λ → ∇(λ(cid:62)f )(x). Similarly, we deﬁne the Moreau gradient of f as the
non-linear form λ → ∇ env(λ(cid:62)f )(x).

2 Moreau Envelope of Compositions

Single composition. We ﬁrst consider the case of a single composition of the form f ◦g, with g and f non-
linear. The following proposition shows how the computation of the Moreau envelope can be decomposed
under suitable assumptions. Appendix C provides additional perspectives. For a diﬀerentiable function
f : Rd → Rm, we denote

(cid:107)f (x)−f (y)(cid:107)2
(cid:107)x − y(cid:107)2

(cid:96)f = sup
x,y∈Rd
x(cid:54)=y

, Lf = sup
x,y∈Rd
x(cid:54)=y

(cid:107)∇f (x)−∇f (y)(cid:107)2,2
(cid:107)x − y(cid:107)2

,

the Lipschitz continuity parameter and the smoothness parameter (Lipschitz continuity parameter of the
gradient) of f (here (cid:107)A(cid:107)2,2 is the matrix norm induced by the Euclidean norm).

Proposition 2.1. Consider g : Rd → Rk and f : Rk → R Lipschitz-continuous and smooth with f convex.

1The Moreau envelope is guaranteed to exist under weaker assumptions, see Appendix C for more details.

3

w

f

x0
h ◦ f

f (w, x0)
∇ env(h)(f (w, x0))

h

x0

(∇ env(αh ◦ fw,t)(wt))τ

t=1

h(f (w))

w

w2

φ2

Φx
1

Φw
2

w1

φ1

Φx
1

Φw
1

x1

λ1

x2

λ2

w3

φ3

Φx
3

Φw
3

f (w, x)

µ

Fig. 1: Black-box representation Moreau
gradient computation.

∇ env(µ(cid:62)fw,1)(w1)
f

∇ env(µ(cid:62)fw,2)(w2)

∇ env(µ(cid:62)fw,3)(w3)

Fig. 2: Detailed computation process.

The gradient of the Moreau envelope at a point x exists for 0 ≤ α ≤ 1/(2(cid:96)f Lg) and is given by
(cid:26)

(cid:27)

∇ env(αf ◦ g)(x) = − argmin

y∈Rd

µ∗(cid:62)g(x + y) +

= ∇ env(µ∗(cid:62)g)(x),

1
2

(cid:107)y(cid:107)2
2

where µ∗ = argmax

−(αf )∗(µ) + env(µ(cid:62)g)(x),

µ∈Rk

where (αf )∗ is the convex conjugate of αf . Problem (6) can be solved to ε accuracy in (cid:101)O
gradients of f and g up to logarithmic factors.

Compare (5) to a classical gradient that computes
(cid:26)

∇(αf ◦ g)(x)=− argmin

y∈Rd

µ(cid:62)∇g(x)(cid:62)(x+y)+

(cid:27)

,

(cid:107)y(cid:107)2
2

1
2

(5)

(6)

(cid:17)

(cid:16) (cid:96)2

gLf
(cid:96)f Lg

calls to

where µ=α∇f (g(x)),

(7)

we retrieve the same structure, except that (i) for the Moreau gradient the dual direction µ∗ is given by
solving an optimization problem, (ii) the classical gradient minimizes a linearized approximation of the inner
function along this direction, while for the Moreau gradient the inner function itself is used.

We also observe that the step-size α for the Moreau gradient to be computed by Prop. 2.1 is upper
bounded by 1/(2(cid:96)f Lg). In comparison, the maximum step-size for a gradient descent is a priori bounded2
by 1/(Lf (cid:96)2
g + Lg(cid:96)f ). Adding the computational cost of computing the Moreau gradients, we see that, up
to logarithmic factors, a gradient descent using Moreau gradients requires at most ˜O((cid:96)2
gLf /ε2) calls to the
In comparison, a gradient descent
gradients of f and g to get a point x such that (cid:107)∇(f ◦ g)(x)(cid:107)2 ≤ ε.
requires a priori O(((cid:96)2
gLf + Lg(cid:96)f )/ε2) calls to the gradients of f and g for the same task. Overall, given the
assumptions of Prop. 2.1, in terms of total calls to the gradients of f and g, a Moreau gradient descent is
faster than a gradient descent.

The computation of the dual direction µ∗ in (6) is central to the computation of the Moreau gradient.
A simple approximation consists in taking µ∗ ≈ ∇f (g(x)), which can be seen as an approximate solution for
problem (6). Precisely, denote for x ﬁxed, c(µ) = env(µ(cid:62)g)(x), such that problem (6) reads

c(µ) − (αf )∗(µ).

max
µ∈Rk

(8)

Approximating c by its linearization around 0 (note that ∇c(0) = g(x)) gives then

µ∗ ≈ argmax

∇c(0)(cid:62)µ − (αf )∗(µ) = α∇f (g(x)).

µ∈Rk

2We use that Lf ◦g ≤ Lf (cid:96)2

g + Lg(cid:96)f is an upper bound on the smoothness parameter Lf ◦g of the composition.

4

This approach is missing a regularization term to account for the approximation error of c by its linearization.
By adding a regularization term, which amounts to make a proximal gradient step on c − (αf )∗ from 0, we
get

µ∗≈ argmax

∇c(0)(cid:62)µ−(αf )∗(µ)−

µ∈Rk

1
2β

(cid:107)µ(cid:107)2
2

=β∇ env((α/β)f )(g(x)),

where, e.g., 0 < β ≤ 1/(2(cid:96)2
g) ensures a decrease of the objective in (8) for α ≤ 1/(2(cid:96)f Lg) (see proof of
Prop. 2.1 in Appendix C). A solution of the computation of (6) may then be obtained by iterating proximal
gradient steps, see proof of Prop. 2.1 in Appendix C.

If f is not convex, the optimization problem maxµ∈Rk −(αf )∗(µ)+ env(µ(cid:62)g)(x) is a relaxation of the
problem deﬁning the computation of the Moreau envelope. We use this relaxation and its approximation by
a proximal gradient step to deﬁne an approximate Moreau gradient as

∇ env(αf ◦ g)(x) ≈ ∇ env(αˆµ(cid:62)g)(x)

where

ˆµ = (β/α)∇ env((α/β)f )(g(x)),

for some β > 0.

Chain of compositions. We consider now multiple compositions of the form ψ = ϕτ ◦. . . ◦ϕ1 : Rd0 → Rdτ
and h : Rdτ → R. We approximate the computation of the Moreau gradient of the composition by
∇ env(αh ◦ ψ)(x) ≈ ∇ env(αµ(cid:62)

1 ϕ1)(x),

where for t ∈ {τ − 1, . . . , 1}, denoting µτ = γ−1

µt = γ−1

t ∇ env(γtµ(cid:62)

τ ∇ env(γτ h)(ϕτ ◦ . . . ◦ ϕ1(x)),
t+1ϕt+1)(ϕt ◦ . . . ϕ1(x)),

(9)

for some γt > 0. This approximation can be interpreted as one pass of a proximal point method applied
to the Lagrangian in order to perform the minimization of the composite objective as formulated in the
following Proposition detailed in Appendix D.

Proposition 2.2. For ψ = ϕτ ◦ . . . ◦ ϕ1 : Rd0 → Rdτ and h : Rdτ → R, consider the Lagrangian formulation
of the minimization of h ◦ ψ, i.e.,

min
x0

h ◦ ψ(x0) = min
x0,...,xτ

L(x0:τ , µ1:τ )

sup
µ1,...,µτ
τ
(cid:88)

L(x0:τ , µ1:τ ) = h(xτ ) +

µ(cid:62)

t (ϕt(xt−1) − xt).

For x0 ∈ Rd0 , denoting xt = ϕt(xt−1) for t ∈ {1, . . . , τ }, a pass of a block coordinate proximal point method
on the Lagrangian on x0:τ , µ1:τ = 0 from t = τ to t = 0 is given as

t=1

x+
τ = xτ − ∇ env(γτ h)(xτ ),
τ = βτ (xτ − x+
µ+

τ ) = βτ ∇ env(γτ h)(xτ ),

and for t ∈ {τ − 1, . . . , 1},

x+
t = xt − ∇ env(γtµ+
t = βt(xt − x+
µ+

ϕt+1)(xt)
t ) = βt∇ env(γtµ+

t+1

t+1

(cid:62)

(cid:62)

ϕt+1)(xt),

until x+

0 = x0 − ∇ env(γ0µ+
1

(cid:62)

ϕ1)(x0), where γt, βt are given step-sizes of the proximal point steps.

Taking βt = γ−1

and γ0 = α in Prop. 2.2, we retrieve the approximation deﬁned in Eq. (9). The following
proposition gives a theoretical choice for the step-sizes γt such that the Moreau gradients of the intermediate
compositions, ∇ env(γtµ+

φt)(xt), can be computed using, e.g., a ﬁrst order method.

(cid:62)

t

t+1

5

Algorithm 1 Forward pass
1: Inputs: Chain f , input x0, parameters w1, . . . , wτ , optimization algorithm A
2: for t = 1, . . . , τ do
3:
4:

Compute xt = φt(wt, xt−1)
Deﬁne and store the non-linear forms

Φw
: λ (cid:55)→ ∇ env(λ(cid:62)φt(·, xt−1))(wt)
t
t : λ (cid:55)→ ∇ env(λ(cid:62)φt(wt, ·))(xt−1)
Φx

where for a function f , ∇ env(f ) is either given in closed form or computed as

with A(h) is the result of an optimization algorithm applied to minimize h.

∇ env(f )(x) = −A(f (x + ·) + (cid:107) · (cid:107)2

2/2)

5: end for
6: Output: Last state xτ
7: Store: Non-linear forms (Φx

t , Φw

t )τ

t=1

Proposition 2.3. Consider the setting of Prop. 2.2, assume h, ϕt to be Lipschitz continuous and smooth
and βt = 1/γt. Assume that the minimization problems deﬁning ∇ env(γτ h) and ∇ env(γtµ+
ϕt+1) to be
t+1
performed exactly. For t ∈ {0, . . . , τ − 1}, if γt ≤ 1/ct+1 with ct = (cid:96)hLϕt
s=t+1 (cid:96)ϕt the problems deﬁning
∇ env(γtµ+
ϕt+1) are strongly convex, hence they can be solved up to any accuracy by a ﬁrst-order method.

(cid:81)τ

(cid:62)

(cid:62)

t+1

3 Approximate Diﬀerentiable Program `a la Moreau

3.1 Composition of Moreau gradients

Algorithm. We consider now compositions of the form h(f (w, x0)) as illustrated in Fig. 5 and 6, where
f : Rp × Rd → Rm is a chain of computations parameterized by φt as deﬁned in Def. 1.1 such as a multi-layer
perceptron network and an objective h : Rm → R. We focus on deriving the Moreau gradient w.r.t. each
component, i.e., computing for t ∈ {1, . . . , τ }

∇ env(αh ◦ fw,t)(wt)= − argmin
vt∈Rpt
where fw,t(wt + vt) = f (w + Etvt, x0) and Et = (0pt×p1, . . . , Idpt, . . . , 0pt×pτ )(cid:62) ∈ Rp×pt is such that E(cid:62)
t w =
wt. Block-decomposition optimization over (w1, . . . , wτ ) using ∇ env(αh ◦ fw,t) amounts then to applying a
block-coordinate proximal point method, see Appendix D.

αh ◦ f (w+Etvt, x0)+

(10)

1
2

(cid:107)vt(cid:107)2
2,

Computing (10) amounts to ﬁxing the parameters wi of the chain of computations for i (cid:54)= t and compute
the Moreau gradient of the resulting composition. Formally, for w = (w1; . . . ; wτ ) ﬁxed with corresponding
states x1, . . . , xτ output by the chain of composition f , computing (10) amounts to considering the com-
position started by v → φt(wt + v, xt−1) and followed by ϕi(y) = φi(wi, y) for i ∈ {t + 1, . . . , τ }, that
is,

∇ env(αh ◦ fw,t)(wt)
=∇ env(αh ◦ ϕτ ◦ . . . ◦ ϕt+1 ◦ φt(·, xt−1))(wt)

=− argmin
vt∈Rpt

αh◦ϕτ ◦ . . . ◦ϕt+1◦φt(wt+vt, xt−1)+

1
2

(cid:107)vt(cid:107)2
2.

(11)

We consider the approximate computation of the Moreau gradient of the composition described in Eq. (9).

6

τ ∇ env(γτ h)(xτ )

Algorithm 2 Backward pass
1: Inputs: Objective h, non-linear forms (Φx
2: Initialize λτ = γ−1
3: for t = τ, . . . , 1 do
λt−1=γ−1
4:
gt=Φw
5:
6: end for
7: Output: g1, . . . , gτ .

t (γt−1λt)=γ−1
t (αtλt)=∇ env(αtλ(cid:62)

t−1∇ env(γt−1λ(cid:62)
t φt(·, xt−1))(wt)

t−1Φx

t , Φw

t )τ

t=1, output xτ , stepsizes αt, γt,

t φt(wt, ·))(xt−1)

Namely, we approximate (10) as

∇ env(αh ◦ fw,t)(wt) ≈ ∇ env(αλ(cid:62)

t φt(·, xt−1))(wt),

where λt is computed by back-propagating the Moreau gradients of the composition, i.e., starting from
λτ = ∇ env(αh)(xτ ), we compute for s ∈ {τ, . . . , t},

λs−1 = γ−1
= γ−1

s−1∇ env(γs−1λ(cid:62)
s−1∇ env(γs−1λ(cid:62)

s ϕs)(xs−1)
s φs(ws, ·))(xs−1),

for γs > 0 and xs = ϕs(xs−1) = φs(ws, xs−1) for s ∈ {t, . . . , τ }. These computations deﬁne a backward
pass on the chain of computations from which the gradients ∇ env(αh ◦ fw,t)(wt) are computed for each
t ∈ {1, . . . , τ − 1}.

The overall algorithm is presented in Algo. 1, Algo. 2. It follows the same principle as back-propagation,
except that, rather than storing the gradients of the computations ∇wtφt(wt, xt−1) and ∇xt−1φt(wt, xt−1)
(see Algo. 3 and Algo. 4 for a detailed implementation of gradient back-propagation), Algo. 1 stores the
non-linear forms λ (cid:55)→ ∇ env(λ(cid:62)φt(·, xt−1))(wt) and λ (cid:55)→ ∇ env(λ(cid:62)φt(wt, ·))(xt−1). The updates on the
parameters are then given as

w+

t = wt − argmin
vt∈Rpt

αλ(cid:62)

t φt(wt + vt, xt−1) +

1
2

(cid:107)vt(cid:107)2
2

≈ wt − ∇ env(αh ◦ fw,t)(wt).

(12)

Theoretical guarantees. The following proposition shows that the exact computation of (10) takes the
form (12).

Proposition 3.1. Consider h : Rm → R and f a chain of computations parameterized by φt with h and
φt Lipschitz-continuous and smooth. Let α ≤ 1/ct with ct = (cid:96)hLφt
s=t+1 (cid:96)φt. For w = (w1; . . . ; wτ ) ﬁxed
consider the computation of (10) and denote λ∗

t:τ regular solutions of

t:τ and x∗

(cid:81)τ

min
vt

min
xt:τ

sup
λt:τ

h(xτ ) +

τ
(cid:88)

s=t+1

s (φs(ws, xs−1) − xs) + λ(cid:62)
λ(cid:62)

t (φt(wt + vt, xt−1) − xt) +

1
2α

(cid:107)vt(cid:107)2
2,

(13)

for xt−1 deﬁned by xs = φt(wt, xs−1) for s ∈ {1, . . . , t − 1}. The Moreau gradients ∇ env(αh ◦ fw,t)(wt) as
deﬁned in (10) are given by

argmin
vt∈Rpt

λ∗
t

(cid:62)φt(wt+vt, xt−1)+(cid:107)vt(cid:107)2

2/(2α).

(14)

The above proposition emphasizes that the dual directions λ∗

t that deﬁne the Moreau gradient ∇ env(αh◦
(cid:62)φt(·, xt−1))(wt) are a priori given as the solution of an optimization problem for each
fw,t)(wt)=∇ env(αλ∗
t
t. However, it is unclear how to link the solution at time t, λ∗
t , to the solution at time t + 1, i.e., a new
optimization problem should be solved at each step of the backward pass to compute ∇ env(αh ◦ fw,t)(wt)
deﬁned in (10). Our approximation takes advantage of the previous computations. However, several other
routines can be used to compute proximal point steps on the objective h ◦ f as shown below.

7

3.2 A fresh look on target propagation and proximal back-propagation

Numerous alternatives to back-propagation have been proposed. See e.g. (Lee et al., 2015) for a recent
account of target propagation. We show here that the folk target propagation algorithm and a proximal back-
propagation algorithm can both be framed using the proposed framework. We circumscribe the components
of these algorithms that boil down to computing an approximate Moreau envelope, or that could be recast as
such a computation. Moreover, the theoretical convergence guarantee we develop can be used to understand
their convergence behavior.

Target propagation. Virtual target propagation can be described as using approximate inverses of layers
when computing the gradient of a deep network (Lee et al., 2015). The virtual targets that minimize the
overall objective are back-propagated via approximate layer inverses. The layer weights are then updated
by minimizing the distance between the output of the layer and the given virtual target. These algorithms
were found to be eﬀective in some settings and were, for the most part, motivated by empirical observations.
We provide here a theoretical grounding to these empirical ideas.

Recall that for a function f : Rk → Rm and z ∈ f (Rk), the inverse of f is given as f −1(z) =
argminy∈Rd (cid:107)f (y) − z(cid:107)2
2. Now, consider the computation of the Moreau envelope of a composition f ◦ g
as in Prop. 2.1 with step-size 1. Rather than using the conjugate of f , one can decompose the problem of
computing the Moreau envelope in the intermediate space g(Rd), i.e.,

min
z∈Rk

f (z) + πg(z),

(15)

where πg(z) = miny∈Rd,g(y)=z
then to computing

1

2 (cid:107)x − y(cid:107)2

2. A split-decomposition proximal point method on (15), amounts

z1 = z − ∇ env(f )(z),

z2 = z1 − ∇ env(πg)(z1),

where z1 − ∇ env(πg)(z1) is the solution of
1
2

(cid:107)x − y(cid:107)2

min
z∈Rk

2 +

1
2

min
y∈Rd
g(y)=z

(cid:107)z − z1(cid:107)2

2 = min
y∈Rd

1
2

(cid:107)x − y(cid:107)2

2 +

1
2

(cid:107)g(y) − z1(cid:107)2
2,

(16)

(17)

which is a regularized inverse of g. See Appendix D for details.

Now consider f = h ◦ ϕτ ◦ . . . ◦ ϕt+1 and g = φt(·, xt−1) as in the computation of ∇ env(αh ◦ fw,t)(wt)
in (10). The above approach can be unfolded through the compositions deﬁning f . Namely, the proximal
point step on f in (16) can be further approximated using that f is a composition. Unfolding the process
until the outer function h, we get an algorithm similar to Algo. 2, that, given a set of parameters w with
corresponding states xt, starts with zτ = xτ − ∇ env(h)(xτ ) and iterates for s = τ, . . . , t + 1,

zs−1 = xs−1 − ∇ env(πϕs)(zs),
which are regularized inverses of the functions ϕt = φt(wt, ·) until the update of the parameters of the layers
deﬁned by

(18)

argmin
vt∈Rpt

1
2

(cid:107)vt − wt(cid:107)2

2 +

1
2

(cid:107)φt(vt, xt−1) − zt(cid:107)2
2.

We retrieve then the rationale of target propagation in the sense that we back-propagate through approximate
inverses until we minimize the distance between the target at layer t and the output of the layer t. Compared
to our approach depicted in Fig. 6, in target propagation, state variables xt are back-propagated and not
dual directions λt.

The viewpoint of Moreau envelope introduces a regularization to the computation of the inverse, which
can stabilize the procedure. Moreover, it shows that target propagation can be viewed as an approximate
solution to the computation of a Moreau envelope. Precisely, given the approximation error of (18) and
the approximation error of an alternating proximal point method on (15), we can establish, under appro-
priate assumptions, the convergence of target propagation by using tools from proximal point methods. See
Appendix D.

8

Proximal back-propagation. Frerix et al. (2018) shows that the classical back-propagation algorithm
can be seen as a coordinate gradient descent on a penalized formulation of the problem (see Eq. (20) below).
This insight is used to modify back-propagation by replacing the gradient step on the parameters by a
so-called proximal step that reads

argmin
vt

1
2α

(cid:107)vt−wt(cid:107)2

2+

1
2

(cid:107)φt(vt, xt−1)−zt(cid:107)2
2,

(19)

for α a given stepsize and zt+1 deﬁned by a coordinate gradient descent on the penalized formulation.

This approach can be seen as a proximal gradient method on the problem (15) using the gradient of
f = h ◦ ϕτ ◦ . . . ◦ ϕt+1 and the proximal operator of πg for g = φt(·, xt−1). We retrieve then again another
way to access Moreau gradients of the composition. The convergence of this procedure can be analyzed using
tools from proximal point methods.

Lagrangian and penalized formulations. We consider the penalized formulation of the problem

min
w1,...,wτ

min
x1,...,xτ

h(xτ )+

τ
(cid:88)

t=1

ρ
2

(cid:107)φt(wt, xt−1)−xt(cid:107)2
2.

(20)

This was considered in several papers (Carreira-Perpinan and Wang, 2014; Gotmare et al., 2018). This formu-
lation can be used to decouple the optimization of the weights of the layers in a distributed way (Carreira-
Perpinan and Wang, 2014). However, the distributed approach dismisses the dynamical structure of the
problem. On the other hand, one pass of a block-coordinate method can be related to back-propagation or
its cousin procedures.

For example, a block-coordinate gradient descent on the penalized formulation amounts to back propa-
gation as mentioned by Frerix et al. (2018). Alternatively, a block-coordinate minimization on the penalized
formulation amounts to target propagation as presented above; see Appendix D. Rather than taking a pe-
nalized formulation, we can consider the Lagrangian as in Prop. 2.2. A block-coordinate proximal point
method on the Lagrangian amounts to our approach. Overall proximal-based methods on the penalized or
the Lagrangian formulation are approximate proximal point methods that can be analyzed from the lens of
the Moreau envelope in our framework.

Other works. Taylor et al. (2016) reformulate the minimization problem using a penalized formulation
and add a Lagrange multiplier for the last layer. They use one pass of a block-coordinate minimization
method on the penalized problem. The resulting algorithm can be interpreted as a constrained problem in
the last state and is then similar to the rationale of target propagation. Gotmare et al. (2018) develop a
similar approach to the one from Frerix et al. (2018). They consider the penalized formulation to compute
the back-propagation steps. However, in the end, they use a gradient step on the weights rather than a
proximal step done in our work. Their approach goes somewhat in the opposite direction compared to
the method from Frerix et al. (2018). Making only a gradient step on the parameters after the additional
burden of computations to derive the minimum of the penalized formulation outweighs the computational
beneﬁts of the penalized formulation. Jaderberg et al. (2017), the authors propose to learn approximations
of the gradients that can be used without the need for back-propagation through the entire network. In
our framework, this amounts to have access to a procedure that approximates (6) in order to minimize a
linearization of the layer as in (7).

4 Numerical Illustrations

We present illustrations in a deterministic setting, the control of pendulum to swing it up, and in a stochastic
setting, the optimization of deep networks to classify images. Additional results and additional details can
be found in Appendix E.

9

Fig. 3: Gradient (GD) vs Moreau gradient (MorGD) descent on the control of a pendulum.

Left: horizon τ = 50, Right: horizon τ = 100

Remarks on implementation. The approximate back-propagation of the Moreau envelope outlined in
Algo. 1, 2, involves an inexact minimization to deﬁne Φx
t at line 4 by some algorithm A. We
may use any algorithm A enjoying at least linear convergence for strongly convex problems for this inexact
minimization, such as gradient descent or conjugate gradient (Bonnans et al., 2006). To illustrate the
potential of the proposed approach, we report the results when this inexact minimization is performed with
a quasi-Newton algorithm. In Algo. 2, we choose γt = γτ −t+1 and αt = αγt+1 such that the updates of
our algorithm can be rewritten ˆλt−1=∇ env(γˆλ(cid:62)
t φt(·, xt−1))(wt). This
choice of step-size is motivated by Prop. 2.3 that shows that the step-sizes γt required for the subproblems
to be strongly convex need to decrease geometrically as t goes from τ to 1.

t φt(wt, ·))(xt−1) and gt = ∇ env(αˆλ(cid:62)

t and Φw

Nonlinear control of a swinging pendulum. Discrete-time control problems are deﬁned by iterative
compositions of discretized dynamics controlled by some parameters w1, . . . , wτ . The dynamics deﬁne a chain
of computations f (w, x0) such that xt+1 = φt(wt, xt−1). Formally, we consider the control of a pendulum to
make it swing-up after a ﬁnite time, which can be written as

min
w1,...,wτ

h(xτ )

s.t. xt+1 = φt(wt, xt−1) for t ∈ {1, . . . , τ },

for x0 ﬁxed, where the formulations of φt and h are given in Appendix E. The horizon τ is usually large to
ensure that the discretization scheme is accurate enough. As many compositions are then involved, we are
interested in the beneﬁts of using approximate Moreau gradients rather than gradients.

We compare a gradient descent to our approximation of a Moreau gradient descent on the control of a
pendulum in Fig. 3 for various horizons τ . We ﬁx γ = 0.5 for the step-sizes γt = γτ −t+1 in Algo. 2 and
perform a grid-search on the step-sizes of the gradient descent and the step-size α of the Moreau gradient
descent such that αt = αγt+1. We observe that our approach provides smoother and faster optimization.

Supervised classiﬁcation with deep networks. For supervised classiﬁcation with deep networks, we
consider a mini-batch stochastic counterpart to the proposed algorithm. Namely, we compute approximate
Moreau gradients for mini-batches written as

hm(fm(w, x0)) =

1
m

m
(cid:88)

i=1

L(yi, ψ(w, x0,i)),

where L is a smooth loss function, x0 = (x0,1; . . . ; x0,m) is a mini-batch of m samples, h(ˆy) = (cid:80)m
i=1 L(yi, ˆyi)/m
for ˆy = (ˆy1; . . . ; ˆym). Here f is the concatenation of a chain of computations applied to the mini-batch of
inputs, i.e., it reads f (w, x0) = (ψ(w, x0,1); . . . ; ψ(w, x0,m)).

In Fig. 4 we compare plain mini-batch stochastic gradient descent against a mini-batch approximate
stochastic Moreau gradient descent to train a deep network with the squared loss on CIFAR10 (Krizhevsky
et al., 2009). We consider a fully connected multi-layer neural network with hidden layer sizes (4000, 1000, 4000)

10

02040Iteration0510CostGDMorGD02040Iteration0510CostGDMorGDFig. 4: Stochastic Gradient Descent (SGD) versus Stochastic Moreau Gradient Descent (MorSGD) on deep
learning problems. Top: MLP, Bottom: Convnet

and a convolutional neural network architecture speciﬁed as

Conv[16×32×32]→ReLU→Pool[16×16×16]
→Conv[20×16×16]→ReLU→ Pool[20×8×8]
→Conv[20×8×8]→ReLU→Pool[20×4×4]→FC

with a logistic loss as done by Frerix et al. (2018). The plots present the minimum of the loss or maximum
of the accuracy obtained so far, i.e., on the y-axis we plot yk = mini=0,...,k hn(fn(w(i), x0)) for the train
loss where n is the total number of samples in the train and w(i) the current set of parameters. Detailed
hyper-parameter settings can be found in the Appendix.

We observe that the mini-batch stochastic counterpart of the proposed algorithm compares favorably
with a stochastic gradient descent. Similar results were obtained for the variants described in Sec. 3.2
by e.g. Frerix et al. (2018). The next sections provide the detailed proofs and additional results.

Acknowledgments. This work was supported by NSF CCF-1740551, NSF DMS-1839371, the CIFAR
program “Learning in Machines and Brains”, and faculty research awards.

11

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R.,
Kaiser, L., Kudlur, M., Levenberg, J., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster,
M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´egas,
F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y. and Zheng, X. (2015), ‘TensorFlow:
Large-scale machine learning on heterogeneous systems’.
URL: http://tensorﬂow.org/

Baur, W. and Strassen, V. (1983), ‘The complexity of partial derivatives’, Theoretical computer science

22(3), 317–330.

Bauschke, H. H. and Combettes, P. L. (2017), Convex analysis and monotone operator theory in Hilbert

spaces, Vol. 408, 2nd edn, Springer.

Beck, A. and Teboulle, M. (2012), ‘Smoothing and ﬁrst order methods: A uniﬁed framework’, SIAM Journal

on Optimization 22(2), 557–580.

Bengio, Y. (2014), ‘How auto-encoders could provide credit assignment in deep networks via target propa-

gation’, arXiv preprint arXiv:1407.7906 .

Bertsekas, D. P. (2005), Dynamic programming and optimal control, 3rd edn, Athena Scientiﬁc.

Bonnans, J.-F., Gilbert, J. C., Lemar´echal, C. and Sagastiz´abal, C. A. (2006), Numerical optimization:

theoretical and practical aspects, Springer Science & Business Media.

Bubeck, S. and Lee, Y. T. (2016), Black-box optimization with a politician, in ‘Proceedings of the 33nd

International Conference on Machine Learning,’, Vol. 48, pp. 1624–1631.

Carreira-Perpinan, M. and Wang, W. (2014), Distributed optimization of deeply nested systems, in ‘Pro-

ceedings of the 17th International Conference on Artiﬁcial Intelligence and Statistics’.

Devolder, O., Glineur, F. and Nesterov, Y. (2014), ‘First-order methods of smooth convex optimization with

inexact oracle’, Mathematical Programming 146(1-2), 37–75.

Drusvyatskiy, D. and Paquette, C. (2019), ‘Eﬃciency of minimizing compositions of convex functions and

smooth maps’, Mathematical Programming 178(1-2), 503–558.

Duchi, J. C. and Ruan, F. (2018), ‘Stochastic methods for composite and weakly convex optimization

problems’, SIAM Journal on Optimization 28(4), 3229–3259.

Fong, R., Patrick, M. and Vedaldi, A. (2019), Understanding deep networks via extremal perturbations and
smooth masks, in ‘Proceedings of the IEEE International Conference on Computer Vision’, pp. 2950–2958.

Frerix, T., M¨ollenhoﬀ, T., Moeller, M. and Cremers, D. (2018), Proximal backpropagation, in ‘International

Conference on Learning Representations’.

Goodfellow, I., Bengio, Y. and Courville, A. (2016), Deep Learning, The MIT Press.

Gotmare, A., Thomas, V., Brea, J. and Jaggi, M. (2018), Decoupling backpropagation using constrained
optimization methods, in ‘Credit Assignment in Deep Learning and Reinforcement Learning Workshop
(ICML 2018 ECA)’.

Guzm´an, C. and Nemirovski, A. (2015), ‘On lower complexity bounds for large-scale smooth convex opti-

mization’, Journal of Complexity 31(1), 1 – 14.

12

Hoheisel, T., Laborde, M. and Oberman, A. (2020), ‘A regularization interpretation of the proximal point

method for weakly convex functions’, Journal of Dynamics and Games 7(2164-6066 2020 1 79), 79.

Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals, O., Graves, A., Silver, D. and Kavukcuoglu, K.
(2017), Decoupled neural interfaces using synthetic gradients, in ‘Proceedings of the 34th International
Conference on Machine Learning’.

Kim, K. V., Nesterov, Y. E. and Cherkasskii, B. (1984), ‘An estimate of the eﬀort in computing the gradient’,

Doklady Akademii Nauk 275, 1306–1309.

Krizhevsky, A., Sutskever, I. and Hinton, G. E. (2012), ImageNet classiﬁcation with deep convolutional

neural networks, in ‘Advances in Neural Information Processing Systems 25’.

Krizhevsky, A. et al. (2009), Learning multiple layers of features from tiny images, Technical report, Uni-

versity of Toronto.

Lan, G. (2020), First-order and Stochastic Optimization Methods for Machine Learning, Springer.

Lecun, Y. (1988), A theoretical framework for back-propagation, in ‘1988 Connectionist Models Summer

School, CMU, Pittsburg, PA’.

Lee, D.-H., Zhang, S., Fischer, A. and Bengio, Y. (2015), Diﬀerence target propagation, in ‘Joint european

conference on machine learning and knowledge discovery in databases’, Springer, pp. 498–515.

Lemar´echal, C. and Sagastiz´abal, C. (1997), ‘Practical aspects of the Moreau–Yosida regularization: Theo-

retical preliminaries’, SIAM Journal on Optimization 7(2), 367–385.

Lin, H., Mairal, J. and Harchaoui, Z. (2018), ‘Catalyst acceleration for ﬁrst-order convex optimization: from

theory to practice’, Journal of Machine Learning Research 18(212), 1–54.

Manchev, N. and Spratling, M. (2020), ‘Target propagation in recurrent neural networks’, Journal of Machine

Learning Research 21(7), 1–33.

Mirowski, P. and LeCun, Y. (2009), Dynamic factor graphs for time series modeling, in ‘Joint European

Conference on Machine Learning and Knowledge Discovery in Databases’, Springer, pp. 128–143.

Moreau, J. J. (1962), ‘Fonctions convexes duales et points proximaux dans un espace hilbertien’, Comptes

Rendus de l’Acad´emie des Sciences 255.

Nesterov, Y. (2013), ‘Gradient methods for minimizing composite functions’, Mathematical Programming

140(1), 125–161.

Nesterov, Y. (2018), Lectures on convex optimization, 2nd edn, Springer.

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L.

and Lerer, A. (2017), ‘Automatic diﬀerentiation in PyTorch’.

Pearlmutter, B. A. (1994), ‘Fast exact multiplication by the Hessian’, Neural Computation 6(1), 147–160.

Rockafellar, R. T. and Wets, R. J.-B. (2009), Variational analysis, Vol. 317, Springer Science & Business

Media.

Rohwer, R. (1990), The “moving targets” training algorithm, in ‘Advances in neural information processing

systems’, pp. 558–565.

Roulet, V. and Harchaoui, Z. (2019), An elementary approach to convergence guarantees of optimization
algorithms for deep networks, in ‘Proceedings of the 57th Annual Allerton Conference on Communication,
Control, and Computing (Allerton)’. Long version CoRR abs/2002.09051.

13

Simonyan, K. and Zisserman, A. (2015), Very deep convolutional networks for large-scale image recognition,

in ‘International Conference on Learning Representations’.

Sra, S., Nowozin, S. and Wright, S. J. (2012), Optimization for machine learning, MIT Press.

Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A. and Goldstein, T. (2016), Training neural networks
without gradients: A scalable admm approach, in ‘Proceedings of the 33rd International Conference on
Machine Learning’.

14

Appendix

First, in Appendix A, we provide the notations used in the main text and in the appendix. In Appendix B,
we explain how a deep network can be represented by a chain of computations. We deﬁne generic models
of network layers and show common layers can be cast in this framework. In Appendix C, we recall the
basic properties of the Moreau envelope and state a convergence guarantee for an approximate gradient
descent algorithm on the Moreau envelope. Moreover, we present chain rules to compute the gradient of the
Moreau envelope of a chain of compositions under various assumptions. In Appendix D, we present how our
algorithm and variants of back-propagation can be understood using alternative formulations of the problem
and give the proofs of Prop 2.2, Prop 2.3 and Prop. 3.1. Finally, in Appendix E, we provide the details on
our experiments and additional results.

A Notations

For a multivariate function f : Rd (cid:55)→ Rn, composed of f (j) real functions with j ∈ {1, . . . n}, we de-
note ∇f (x) = (∇f (1)(x), . . . , ∇f (n)(x)) ∈ Rd×n, that is the transpose of its Jacobian on x, ∇f (x) =
( ∂f (j)
∂xi

(x))1≤i≤d,1≤j≤n ∈ Rd×n.

For a real function, f : Rd ×Rp (cid:55)→ R, whose value is denoted f (x, y), we decompose its gradient ∇f (x, y) ∈

Rd+p on (x, y) ∈ Rd × Rp as

∇f (x, y) =

(cid:19)
(cid:18)∇xf (x, y)
∇yf (x, y)

with

∇xf (x, y) ∈ Rd, ∇yf (x, y) ∈ Rp.

Given a function f : Rd × Rp (cid:55)→ Rn and (x, y), we denote ∇xf (x, y) = (∇xf (1)(x, y), . . . , ∇xf (n)(x, y)) ∈

Rd×n and we deﬁne similarly ∇yf (x, y) ∈ Rp×n.
We denote by ⊗ the Kronecker product.

B Deep Networks

We present here a mathematical framework for the diﬀerent modules of a deep network. We focus here
on common modules of common deep networks. A comprehensive framework is beyond the scope of this
conference paper. This said, our framework can actually encompass all modules we are aware of. In Ap-
pendix E, we consider multi-layer perceptrons and convolutional neural networks. Both types fall within
our framework. We also consider a nonlinear control problem. This type of problem also falls within our
framework. See (Roulet and Harchaoui, 2019) for a discussion of the relationship between nonlinear control
and deep learning.

B.1 Deep learning

Deep networks learn mappings from a given set of input-output pairs (x0,i, yi)n
objective of the form

i=1 by minimizing a training

min
w∈Rp

1
n

n
(cid:88)

i=1

L(yi, f (w, x0,i) + r(w),

(21)

where L is a given loss function and r is a regularization term. From a functional viewpoint, a deep network
is a function f , taking an input x0 onto xτ = f (w, x0), which unfolds into a chain of compositions of length
or depth τ , deﬁned by τ layers each of the form

xt = φt(wt, xt−1),

(22)

15

where the layer parameters are concatenated into w = (w1; . . . ; wτ )3 ∈ Rp. Each layer function φt is either
a basic function or a composition of basic layers typical in deep networks.

B.2 Network layers

The layer t of a deep network can be described by the following components,

(i) a bi-aﬃne operation such as a matrix multiplication or a convolution, denoted bt : Rpt × Rqt−1 → Rkt

and decomposed as

where βt is bilinear, βw

bt(wt, xt−1) = βt(wt, xt−1) + βw
t are linear and β0

t (wt) + βx
t is a constant vector,

t and βx

t (xt−1) + β0
t ,

(23)

(ii) an activation function, such as the element-wise application of a non-linear function, denoted αt :

Rkt → Rkt,

(iii) a reduction of dimension, such as a pooling operation, denoted πt : Rkt → Rqt,

(iv) a normalization of the output, such as batch-normalization, denoted νt : Rqt → Rqt.

By concatenating the last three operations, i.e., deﬁning at = νt ◦ πt ◦ αt, a layer can be written as

φt(wt, xt−1) = at(bt(wt, xt−1)).

We detail how aﬃne fully connected layers and convolutional layers can be described in this formalism. For
more details about the other components see (Roulet and Harchaoui, 2019).

B.3 Linear operations

In the following, we drop the dependency w.r.t. the layer t and denote by ˜· the quantities characterizing the
output. We denote by semi-columns the concatenations of matrices by rows, i.e., for A ∈ Rd×n, B ∈ Rq×n,
(A; B) = (A(cid:62), B(cid:62))(cid:62) ∈ R(d+q)×n.

Fully connected layer. A fully connected layer taking a batch of m inputs of dimension d is written
˜X = W (cid:62)X + ω0 1(cid:62)
m,
where X ∈ Rd×m is the batch of inputs, W ∈ Rd× ˜d are the weights of the layer and ω0 ∈ R ˜d deﬁne the
oﬀsets. By vectorizing the parameters and the inputs, a fully connected layer can be written as

where

˜x = β(w, x) + βw(w),
β(w, x) = Vec(W (cid:62)X) ∈ Rm ˜d, βw(w) = Vec(ω0 1(cid:62)
x = Vec(X) ∈ Rmd, ˜x = Vec( ˜X) ∈ Rm ˜d, w = Vec(W ; ω0) ∈ R

m),

˜d(d+1).

Convolutional layer. A convolutional layer convolves a batch of m inputs (images or signals) of dimension
d stacked as X = (χ1, . . . , χm) ∈ Rd×m with nf aﬃne ﬁlters of size sf deﬁned by weights W = (ω1, . . . , ωnf ) ∈
Rsf ×nf
through np patches. The kth output of the convolution of the
ith input by the jth ﬁlter reads

and oﬀsets ω0 = (ω0

nf ) ∈ Rnf

1, . . . , ω0

(24)
where Πk ∈ Rsf ×d extracts a patch of size sf at a given position of the input χi. The output ˜X = ( ˜χ1, . . . , ˜χm)
is then given by the concatenation ˜χi,k+np(j−1) = Ξi,j,k. By vectorizing the inputs and the outputs, the

Ξi,j,k = ω(cid:62)

j Πkχi + ω0
j ,

3We denote by semi-colons the concatenation of variables.

16

convolution operation is deﬁned by a set of matrices (Πk)np
˜x = β(w, x) + βw(w),

k=1 such that

where

j Πkχi)i=1,...m;j=1,...,nf ;k=1,...,np ∈ Rmnf np

β(w, x) = (ω(cid:62)
x = Vec(X) ∈ Rmd, w = Vec(W ; ω0) ∈ R(sf +1)nf
X = (χ1, . . . , χm), W = (ω1, . . . , ωnf ),
where β(w, x) is deﬁned by concatenations of the output.

,

, βw(w) = 1m ⊗ω0 ⊗ 1np ,

C Moreau Envelopes

C.1 Properties of the Moreau envelope

We start by recalling the basic properties of a Moreau envelope, see e.g. (Lemar´echal and Sagastiz´abal, 1997;
Bauschke and Combettes, 2017; Beck and Teboulle, 2012; Rockafellar and Wets, 2009) for more details.

Lemma C.1. If there exists ¯α > 0 such that inf x∈Rd f (x) + (2¯α)−1(cid:107)x(cid:107)2
2 is ﬁnite, then envα(f ) is deﬁned
for 0 < α < ¯α, it is Lipschitz continuous and ﬁnite everywhere. Moreover if f is (cid:96)-Lipschitz continuous and
α < ¯α, then the Moreau envelope of f approximates f up to α(cid:96)2 accuracy, i.e., |f (x) − envα(f )(x)| ≤ α(cid:96)2
for all x ∈ dom f .

The smoothing eﬀect of the Moreau envelope is valid for a broad class of functions called weakly convex.

A function f : Rd → R is ν-weakly convex on C ⊂ R for ν ∈ R if

x → f (x) +

ν
2

(cid:107)x(cid:107)2
2

is convex on C.

If we omit the dependency w.r.t. C, the above deﬁnition must be valid on the domain dom f of f . Convex
functions are naturally 0-weakly convex. A µ-strongly convex functions is −µ-weakly. A L-smooth function
is also L-weakly convex as recalled below.

Fact C.2. If f : Rd → R is L-smooth, then for any τ ≥ 1, f + Lτ

2 (cid:107) · (cid:107)2

2 is (τ − 1)L strongly convex.

Proof. We have by smoothness of f , for any x, y ∈ Rd, f (y) ≥ f (x) + ∇f (x)(cid:62)(y − x) − L

2 (cid:107)x − y(cid:107)2

2. Hence,

f (y) +

Lτ
2

(cid:107)y(cid:107)2

2 ≥ f (x) +

Lτ
2

(cid:107)x(cid:107)2

2 + (∇f (x) + Lτ x)(cid:62)(y − x) + (τ − 1)

L
2

(cid:107)y − x(cid:107)2
2,

where we used that (cid:107)y(cid:107)2
(Nesterov, 2018).

2 = (cid:107)x(cid:107)2

2 + 2x(cid:62)(y − x) + (cid:107)y − x(cid:107)2

2. Therefore f + Lτ

2 (cid:107) · (cid:107)2

2 is (τ − 1)L strongly convex

The smoothing eﬀect of the Moreau-envelope is recalled here, see (Hoheisel et al., 2020, Corollary 3.4)

for a proof.

Lemma C.3. If f is ν-weakly convex, then envα(f ) for 0 < α < 1/ν is Lα-smooth with Lα = max{ν/(1 −
να), 1/α}. In particular for α < 1/(2ν), the envelope is 1/α-smooth.

C.2 Chain rules for the gradient of the Moreau envelope

We introduce here composition rules for the gradient of the Moreau envelope of simple compositions under
various assumptions on the functions. These composition rules are original and new. The ﬁrst chain rule is
the basis of the reasoning presented in Section 1.

17

Convex outer function.

Proposition 2.1. Consider g : Rd → Rk and f : Rk → R Lipschitz-continuous and smooth with f convex.
The gradient of the Moreau envelope at a point x exists for 0 ≤ α ≤ 1/(2(cid:96)f Lg) and is given by
(cid:26)

(cid:27)

∇ env(αf ◦ g)(x) = − argmin

y∈Rd

µ∗(cid:62)g(x + y) +

= ∇ env(µ∗(cid:62)g)(x),

1
2

(cid:107)y(cid:107)2
2

where µ∗ = argmax

−(αf )∗(µ) + env(µ(cid:62)g)(x),

µ∈Rk

where (αf )∗ is the convex conjugate of αf . Problem (6) can be solved to ε accuracy in (cid:101)O
gradients of f and g up to logarithmic factors.

Proof. The computation of the Moreau envelope amounts to solve

(5)

(6)

(cid:17)

(cid:16) (cid:96)2

gLf
(cid:96)f Lg

calls to

min
y∈Rd

αf ◦ g(x + y) +

1
2

(cid:107)y(cid:107)2

2 = min
y∈Rd

sup
µ∈dom(αf )∗

µ(cid:62)g(x + y) − (αf )∗(µ) +

1
2

(cid:107)y(cid:107)2
2.

(25)

2 (cid:107)y(cid:107)2

For µ ∈ Rk have that x → µ(cid:62)g(x) is (cid:107)µ(cid:107)Lg smooth. Therefore for µ such that Lg(cid:107)µ(cid:107) ≤ 1
y) + 1
Therefore, for α(cid:96)f Lg ≤ 1
Hence we can swap min and max such that the problem reads

2 , y → µ(cid:62)g(x +
2 is 1/2-strongly convex. Since f is (cid:96)f -Lipschitz continuous, dom(αf )∗ ⊂ {µ ∈ Rk : (cid:107)µ(cid:107)2 ≤ α(cid:96)f }.
2 , problem (25) is strongly convex in y and concave in µ with dom(αf )∗ compact.

(cid:26)

(cid:26)

sup
µ∈dom(αf )∗

min
y∈Rd

µ(cid:62)g(x + y) +

(cid:27)

1
2

(cid:107)y(cid:107)2
2

(cid:27)

− (αf )∗(µ)

= sup
µ∈Rk

env(µ(cid:62)g)(x) − (αf )∗(µ).

Note that µ → env(µ(cid:62)g)(x) is concave as an inﬁmum of linear functions in µ. For f Lf smooth, (αf )∗ is
1/(αLf ) strongly convex. Therefore the above problem is strongly concave. For µ∗ solution of the above
problem, the primal solution is given by

(cid:26)

argmin
y∈Rd

µ∗(cid:62)g(x+y)+

(cid:27)

.

(cid:107)y(cid:107)2
2

1
2

Denoting c(µ) = env(µ(cid:62)g)(x), consider for example a proximal gradient ascent to solve (6), i.e., starting
from e.g. µ(0) = 0,

µ(k+1) = argmax
ν∈dom(αf )∗

c(µ(k)) + ∇c(µ(k))(cid:62)(ν − µ(k)) − (αf )∗(ν) −

1
2β

(cid:107)ν − µ(k)(cid:107)2
2.

These iterations are well-deﬁned and converge for β suﬃciently small provided c is smooth, i.e., diﬀerentiable
with Lipschitz continuous gradients. For µ ∈ (dom(αf )∗), we have that y → µ(cid:62)g(x + y) + 1
2 is 1/2-
strongly convex, hence ∇c(µ) is well deﬁned and is given by

2 (cid:107)y(cid:107)2

∇c(µ) = g(x + y(µ))

for

y(µ) = argmin

µ(cid:62)g(x + y) +

y∈Rd

= −∇g(x + y(µ))µ.

1
2

(cid:107)y(cid:107)2
2

For µ, ν ∈ dom(αf )∗, such that (cid:107)µ(cid:107) ≤ α(cid:96)f , we have

(cid:107)y(µ) − y(ν)(cid:107)2 ≤ (cid:96)g(cid:107)µ − ν(cid:107)2 + (cid:107)µ(cid:107)2Lg(cid:107)y(µ) − y(ν)(cid:107)2 ≤ (cid:96)g(cid:107)µ − ν(cid:107)2 + α(cid:96)f Lg(cid:107)y(µ) − y(ν)(cid:107)2.

Hence we have

(cid:107)∇c(µ) − ∇c(ν)(cid:107)2 ≤ (cid:96)g(cid:107)y(µ) − y(ν)(cid:107)2 ≤

(cid:96)2
g
1 − α(cid:96)f Lg
g smooth. Since (αf )∗ is 1/(αLf ) strongly convex, a proximal gradient descent (4) with
gLf
iterations to a point ε close to the
(cid:96)f Lg

g) converges in O((cid:96)2

gαLf log(ε)) ≤ O

(cid:107)µ − ν(cid:107)2 ≤ 2(cid:96)2

g(cid:107)µ − ν(cid:107)2.

log(ε)

(cid:16) (cid:96)2

(cid:17)

Therefore c is 2(cid:96)2

step-size β = 1/(4(cid:96)2

18

solution (Nesterov, 2013). An accelerated gradient descent converges in O((cid:96)g
Each step requires to compute

(cid:112)Lf /(cid:96)f Lg log(ε)) iterations.

(αf )∗(ν) +

argmin
ν∈Rk

1
2β

(cid:107)λ − ν(cid:107)2

2 = λ − β argmin

(cid:26)

αf (z) +

(cid:27)

(cid:107)z − λ/β(cid:107)2
2

β
2

z∈Rk
= β∇ env((α/β)f )(λ/β)

where λ = µ(k) + β∇h(µ(k))

= µ(k) + βg(x + ∇ env(µ(k)(cid:62)

g)(x)).

Computing ∇ env((α/β)f )(λ) up to ε accuracy requires

O((α/β)Lf log(ε)) ≤ O

(cid:32)

(cid:96)2
gLf
(cid:96)f Lg

(cid:33)

log(ε)

iterations of a gradient descent and

(cid:32)

(cid:115)

O

(cid:96)g

Lf
(cid:96)f Lg

(cid:33)

log(ε)

iterations of an accelerated gradient descent. Computing ∇ env(µ(k)(cid:62)
requires O (α(cid:96)f Lg log(ε)) ≤ (cid:101)O(log(ε)) iterations of a gradient descent or an accelerated gradient descent.

g)(x) for (cid:107)µ(cid:107) ≤ α(cid:96)f up to ε accuracy

An approximate accelerated proximal gradient descent with increasing accuracy of the inner computations
of the oracles lead up to logarithmic factors to the same rates as an accelerated proximal gradient method,
as demonstrated for example by Lin et al. (2018) in the case of approximate accelerated proximal point
methods. Overall computing the gradient of the Moreau envelope using accelerated gradient descent in the
outer and inner loops costs up to logarithmic factors O((cid:96)2

gLf /(cid:96)f Lg).

Convex non-smooth composition. The previous proposition is a particular case of the following propo-
sition. Namely, the smoothness properties used in Prop. 2.1 are used to characterize the range of possible
stepsizes and the complexity of solving the sub-problems. Yet, in general, these smoothness assumptions are
not necessary as long as the outer function is convex and its gradient take values in the range of variables λ
such that y → λ(cid:62)g(y) + 1

2 (cid:107)y(cid:107)2

2 is convex.

Proposition C.4. Consider g : Rd → Rk and f : Rk → R. Assume f to be convex and for α ≥ 0,

dom(αf )∗ ⊂ Λg := {λ ∈ Rk : y → λ(cid:62)g(y) +

1
2

(cid:107)y(cid:107)2

2 is strongly convex}

with dom(αf )∗ compact, then

∇ env(αf ◦ g)(x)=− argmin

(cid:26)

µ∗(cid:62)g(x+y)+

where µ∗∈ argmax

y∈Rd
−(αf )∗(µ)+ env(µ(cid:62)g)(x).

1
2

(cid:107)y(cid:107)2
2

(cid:27)

,

(26)

µ∈Rk

Proof. The computation of the Moreau envelope amounts to solve

min
y∈Rd

αf ◦ g(x + y) +

1
2

(cid:107)y(cid:107)2

2 = min
y∈Rd

sup
µ∈dom(αf )∗

µ(cid:62)g(x + y) − (αf )∗(µ) +

1
2

(cid:107)y(cid:107)2
2.

By assumption the min and sup can be swapped, which gives the result.

The deﬁnition of Λg can be derived for simple functions:
• for g(x) = Ax, we have Λg = Rk,
• for g(x) = (σi(x))k

{1, . . . , k}}, the positive orthant,

• for g Lg-smooth, Λg ⊃ {λ ∈ Rk : (cid:107)λ(cid:107)2 < 1/Lg}.

19

i=1 with σi : Rd → R convex for all i, we have Λg ⊃ Rk

+ = {λ ∈ Rk : λi ≥ 0, ∀i ∈

In general, we always have 0 ∈ Λg

As an example, for f strictly convex and continuously diﬀerentiable and g : x → Ax linear the Moreau

envelope of the composition can be expressed as

which is a reformulation of a known result (Bauschke and Combettes, 2017, Proposition 23.25).

∇ env(f ◦ A) = A(cid:62) ◦ (cid:0)AA(cid:62) + ∇f −1(cid:1)−1

◦ A,

Non-convex case. For a non-convex function the problem of computing the Moreau envelope of a com-
position can be written as follows. By relaxing the problem (27) by swapping min and sup we get back the
formulation of Prop. 2.1. In other words, the formulation of the Moreau envelope in Prop. 2.1 is in general
a relaxation of the computation of the Moreau envelope of the composition. The advantage of this relaxation
is that it can always be computed with ﬁrst-order methods.

Proposition C.5. Consider g : Rd → Rk and f : Rk → R smooth and Lipschitz continuous then the Moreau
envelope of the composition is given for α ≤ 1/(2(cid:96)f Lg) as

∇ env(αf ◦ g)(x) = − argmin

{α∇f (z∗)(cid:62)g(x + y) +

for

y∈Rd

(cid:40)

1
2

(cid:107)y(cid:107)2
2}

(cid:41)

z∗ ∈ argmin

αf (z) + sup

−z(cid:62)µ + env(µ(cid:62)g)(x)

z∈Rk

(cid:107)µ(cid:107)≤α(cid:96)f

(27)

Proof. Since f is (cid:96)f Lipschitz continuous, f (x) = minz∈Rk f (z) + (cid:96)f (cid:107)x − z(cid:107)2. Therefore

min
y∈Rd

αf (g(x + y)) +

1
2

(cid:107)y(cid:107)2

2 = min
z∈Rk

min
y∈Rd

αf (z) + α(cid:96)f (cid:107)g(x + y) − z(cid:107)2 +

1
2

(cid:107)y(cid:107)2
2

= min
z∈Rk

αf (z) + min
y∈Rk

sup
(cid:107)µ(cid:107)2≤α(cid:96)f

µ(cid:62)(g(x + y) − z) +

1
2

(cid:107)y(cid:107)2
2

= min
z∈Rk

αf (z) + sup

−z(cid:62)µ + env(µ(cid:62)g)(x)

(cid:107)µ(cid:107)2≤α(cid:96)f

The ﬁrst order optimality conditions give that α∇f (z∗) = µ∗ for µ∗ ∈ argmax(cid:107)µ(cid:107)2≤α(cid:96)f
The primal solution in terms of y follows.

−z(cid:62)µ + env(µ(cid:62)g)(x).

D Lagrangian and Penalized Formulations of Back-propagation

Procedures

We are interested in ﬁrst-order oracles for an objective of the form

h(f (w, x0)),
where h : Rm → R is a diﬀerentiable objective and f : Rp × Rd → Rm is a chain of mappings (each
diﬀerentiable) as deﬁned in Def. 1.1. We shall refer to this canonical form of an objective as the form (A).
Table 1 gives an overview of the various methods we consider here. All algorithms are collected in
Section D.9 for reference. We ﬁrst provide examples of objectives that read in form (A). Then, in Sec. D.2,
we recall the classical back-propagation algorithm (Back-propagation in Table 1) and revisit the key properties
that serve as a basis for our developments. We express the approximate diﬀerentiable program `a la Moreau
presented in Sec. 2 as a block-coordinate proximal point method on the Lagrangian formulation of the
problem. We present in Sec. D.8 convergence guarantees for a gradient method with Moreau gradients on a
classical formulation (Prop. D.8; Moreau gradients in Table 1) and a gradient method with Moreau gradients
on an augmented Lagrangian formulation (Prop. D.10; Augmented Moreau gradients in Table 1). We shall

(A)

20

discuss in Sec. D.5 the relationships to virtual target propagation algorithms (Manchev and Spratling, 2020;
Bengio, 2014) and to the proximal back-propagation algorithm (Frerix et al., 2018).

D.1 Examples of objectives

Nonlinear control. Discrete-time nonlinear control problems are deﬁned by iterated compositions of
dynamics controlled by some parameters w1, . . . , wτ . Nonlinear control problems with a ﬁnal cost can be
written as

min
w1,...,wτ

h(xτ )

s.t. xt = φt(wt, xt−1) for t ∈ {1, . . . , τ },

(28)

(29)

for x0 ﬁxed. The dynamics deﬁne a chain of composition f (w, x0) such that xt = φt(wt, xt−1) for w =
(w1; . . . ; wτ ). The problem can be written in (A) form. See Sec. E for an example with the control of a
pendulum.

Deep learning. A deep network learns a mapping from a given set of input-output pairs (x0,i, yi)n
minimizing a training objective of the form

i=1 by

1
n

n
(cid:88)

i=1

L(yi, ψ(w, x0,i)),

(30)

where L is a given loss function and f is a deep network as explained in Sec. B. By deﬁning for z = (z1; . . . ; zn),
n
(cid:88)

h(z) =

L(yi, zi),

1
n

i=1

and for x0 = (x0,1, . . . , x0,n),

we recover that (30) can be expressed as h(f (w, x0)) as in (A) form.

f (w, x0) = (ψ(w, x0,1); . . . ; ψ(w, x0,n)),

Training and optimization. For objectives of the form (30), we consider ﬁrst-order oracles that operate
on mini-batches of examples. Namely, the stochastic counterparts of the algorithms proceed as follows,
starting from a given set of parameters w(0).

1. Pick at random a subset S ⊂ {1, . . . n} of size m.

2. Compute an oracle g for the mini-batch objective
1
m

hm(fm(w(k), x0)) =

L(yi, ψ(w(k), x0,i)),

(cid:88)

i∈S

using Algo. 1-2, Algo. 3-4, Algo. 8-9, Algo. 10-11 or Algo. 12-13 where hm(z) = 1/m (cid:80)
i∈S L(yi, zi)
and fm is the concatenation of a chain of computations applied to the mini-batch of inputs, i.e., it
reads fm(w, x0) = (ψ(w, x0,i))i∈S.

3. Deﬁne

w(k+1) =

(cid:40)

w(k) − g
g

for Algo. 1-2 or Algo. 3-4
for Algo. 8-9 or Algo. 10-11 or Algo. 12-13

.

D.2 Gradient back-propagation

Principle. The classical back-propagation procedure consists in computing the gradient as (for a ﬁxed x0)
(cid:26)

(cid:27)

1
2

(cid:107)v(cid:107)2
2

,

(31)

∇w(h ◦ f )(w, x0) = − argmin

v∈Rp

(cid:96)h◦f (·,x0)(w + v; w) +

21

Oracles
Back-Propagation
Moreau gradients
Target Propagation
Regularized Target Propagation
Proximal Propagation

Problem formulation
Penalized
Lagrangian
Penalized
Penalized
Penalized

Augmented Moreau Gradients

Augmented Lagrangian

Optimization method
Block-coordinate gradient descent
Block-coordinate proximal point method
Block-coordinate minimization
Block-coordinate proximal point method

Block-coordinate gradient descent on xt
Block-coordinate proximal point on wt

Block-coordinate proximal point method

Table 1: Interpretations of optimization oracles as optimization methods on reformulations of the problem.

w

f

x0
h ◦ f

f (w, x0)
∇h(f (w, x0))

h

x0

∇w(h ◦ f )(w, x0)

h ◦ f (w)

Fig. 5: First order black-box oracle for
chain of computations.

w

w2

φ2

Φx
1

Φw
2

w1

φ1

Φx
1

Φw
1

x1

λ1

x2

λ2

w3

φ3

Φx
3

Φw
3

f (w, x)

µ

∇w1f (w, x0)µ
f

∇w2f (w, x0)µ

∇w3f (w, x0)µ

∇wf (w, x0)µ

Fig. 6: Detailed ﬁrst-order black-box oracle.

where for a function g, (cid:96)g(x + y; x) = g(x) + ∇g(x)(cid:62)(y − x) is a linear approximation of the function. By
decomposing the linearization of the chain f into the computations φt, the above problem amounts to solve

min
v1,...,vτ
y1,...,yτ

∇h(f (w, x0))(cid:62)yτ +

1
2α

τ
(cid:88)

t=1

(cid:107)vt(cid:107)2
2

subject to yt = ∇xφt(wt, xt−1)(cid:62)yt−1 + ∇wφt(wt, xt−1)(cid:62)vt

for t ∈ {1, . . . , τ }

(32)

such that the solution is (v∗

τ ) = −∇w(h ◦ f )(w, x0).

y0 = 0,
1; . . . ; v∗

Algorithm. We recall in Algo. 3 the forward pass deﬁning the problem (32). This allows computing a
gradient by solving (32), which amounts to Algo. 4. Overall, a diﬀerentiable program giving access to a
ﬁrst-order oracle is presented in Algo. 5. The procedure is also illustrated in Fig. 5 and Fig. 6.

Interpretation. Frerix et al. (2018) showed that back-propagation could be seen as block coordinate
gradient descent on the penalized formulation. See Prop. 1 in there where parameters from layer t = τ to
t = 0 are updated. We consider alternative methods to compute ﬁrst-order oracles and show how they can
be understood using Lagrangian, penalized, or augmented Lagrangian formulations of the problem.

Arithmetic complexity. Given that the cost of computing the gradient of the computations φt is the same
as the cost of computing the functions themselves (or at most a constant times more), the cost of computing

22

the gradient of a chain of computations is the same as the cost of computing the chain of computations (Baur
and Strassen, 1983; Kim et al., 1984).

Lipschitz continuity estimates of the gradients of a chain of computations. Gradients are there-
fore easy to compute, however, for long chain of computations, the error of approximations of the compu-
tations by their linear approximations a priori accumulate through the computations. To understand the
limitations of the gradient, we can compute its Lipschitz continuity parameter that controls the error of
approximation of a function by its linear approximation, i.e.
Lf
2

|f (y) − (cid:96)f (y; x)| ≤

(cid:107)x − y(cid:107)2
2

for all x, y in the domain of f with Lf the smoothness parameter of the function. The smaller Lf , the larger
is the region around x such that the linear approximation of f is (cid:15) close to the function. We use Prop. 4.3
from (Roulet and Harchaoui, 2019) to obtain an estimate of the Lipschitz continuity of the gradient (i.e.,
smoothness property of the function) w.r.t. the length of the chain. Under more stringent assumptions,
better estimates could be obtained; the tightness of such estimates in speciﬁc situations is not the focus of
our paper where general results are sought. We use elementary estimates for the simplicity of the exposition.

This proposition can be adapted to deep networks (Roulet and Harchaoui, 2019, Prop. 4.4).

Proposition D.1. (Roulet and Harchaoui, 2019, Prop. 4.3) Consider a chain f of τ computations as
deﬁned in Def. 1.1, by smooth and Lipschitz-continuous layers φt. Consider x0 ﬁxed.
(i) An upper-bound on the Lipschitz-continuity of the chain f is given by (cid:96)f = (cid:96)τ , where for t ∈ {1, . . . , τ },

(cid:96)t = (cid:96)φt + (cid:96)t−1(cid:96)φt,
(ii) An upper-bound on the smoothness of the chain f is given by Lf = Lτ , where for t ∈ {1, . . . , τ },
Lt = Lt−1(cid:96)φt + Lφt(1 + (cid:96)t−1)2,

L0 = 0.

(cid:96)0 = 0.

D.3 Lagrangian formulation

In Sec. 2 we presented how the Moreau gradients can be approximated by the composition of the Moreau
gradients of the composition. We present here how this approach can be understood as a block coordinate
proximal point method on the Lagrangian.

We present an inexact block-coordinate proximal point method on the Lagrangian formulation of the
objective, this provides a proof of Prop. 2.2 and a justiﬁcation for Algo. 1 and Algo. 2 by simplifying
Eq. (33), (34), (35). A gradient descent using Moreau gradients is presented in Algo. 6.

Fact D.2. Consider the Lagrangian formulation of the minimization of h◦f with f a chain of computations,

L(w1:τ , x1:τ , λ1:τ ) = h(xτ ) +

τ
(cid:88)

t=1

λ(cid:62)
t (φt(wt, xt−1)−xt)

with x0 ﬁxed. Consider w1:τ = (w1; . . . ; wτ ), λ1:τ = (0; . . . ; 0) and x1:τ = (x1; . . . ; xτ ) deﬁned by xt =
φt(wt, xt−1) for t ∈ {1, . . . , τ }. Consider a block coordinate proximal point method that updates the parame-
ters in the order xt → λt → wt for t = τ, . . . , 1 and denote by a superscript + the updates.

At layer τ , a proximal point step on xτ with step-size γτ , is given as (using λτ = 0, xτ = φτ (wτ , xτ −1))

x+
τ = argmin
yτ ∈Rqτ

h(yτ ) +

1
2γτ

(cid:107)yτ − xτ (cid:107)2
2

= xτ − ∇ env(γτ h)(xτ ).

(33)

At each layer t ∈ {τ, . . . , 1},
1. a proximal point step on λt = 0 with step-size βt is given as (for x+

t , xt−1 ﬁxed using that xt = φt(wt, xt−1))

λ+
t = argmax
λt∈Rqt

t (φt(wt, xt−1) − x+
λ(cid:62)

t )−

1
2βt

(cid:107)λt(cid:107)2 = βt(xt − x+

t ),

(34)

23

2. a proximal point step on wt with step-size αt reads (for λ+

w+

t = argmin
vt∈Rpt

(cid:62)

λ+
t

φt(vt, xt−1) +

t , xt−1 ﬁxed)
1
2αt

(cid:107)vt − wt(cid:107)2
2

3. if t ∈ {τ . . . , 2}, a proximal point step on xt−1 with step-size γt−1 reads (for λ+

t , wt, λt−1 = 0 ﬁxed)

= wt − ∇ env(αtλ+
t

(cid:62)

φt(·, xt−1))(wt),

(cid:62)

λ+
t

x+
t−1= argmin
yt−1∈Rqt−1
= xt−1 − ∇ env(γt−1λ+
t

φt(wt, yt−1) +

1
2γt−1

(cid:107)yt−1 − xt−1(cid:107)2

(cid:62)

φt(wt, ·))(xt−1).

(35)

For the ﬁrst layer x0 is ﬁxed to x0.

Proposition 2.2. For ψ = ϕτ ◦ . . . ◦ ϕ1 : Rd0 → Rdτ and h : Rdτ → R, consider the Lagrangian formulation
of the minimization of h ◦ ψ, i.e.,

min
x0

h ◦ ψ(x0) = min
x0,...,xτ

L(x0:τ , µ1:τ )

sup
µ1,...,µτ
τ
(cid:88)

L(x0:τ , µ1:τ ) = h(xτ ) +

µ(cid:62)

t (ϕt(xt−1) − xt).

For x0 ∈ Rd0 , denoting xt = ϕt(xt−1) for t ∈ {1, . . . , τ }, a pass of a block coordinate proximal point method
on the Lagrangian on x0:τ , µ1:τ = 0 from t = τ to t = 0 is given as

t=1

x+
τ = xτ − ∇ env(γτ h)(xτ ),
τ = βτ (xτ − x+
µ+

τ ) = βτ ∇ env(γτ h)(xτ ),

and for t ∈ {τ − 1, . . . , 1},

x+
t = xt − ∇ env(γtµ+
t = βt(xt − x+
µ+

ϕt+1)(xt)
t ) = βt∇ env(γtµ+

t+1

t+1

(cid:62)

(cid:62)

ϕt+1)(xt),

until x+

0 = x0 − ∇ env(γ0µ+
1

(cid:62)

ϕ1)(x0), where γt, βt are given step-sizes of the proximal point steps.

Proof. The proof follows from the same reasoning as Fact D.2.

The choice of the step-sizes for each Moreau gradient to be computed by a ﬁrst-order method is given in

the following proposition.

Proposition 2.3. Consider the setting of Prop. 2.2, assume h, ϕt to be Lipschitz continuous and smooth
and βt = 1/γt. Assume that the minimization problems deﬁning ∇ env(γτ h) and ∇ env(γtµ+
ϕt+1) to be
t+1
s=t+1 (cid:96)ϕt the problems deﬁning
performed exactly. For t ∈ {0, . . . , τ − 1}, if γt ≤ 1/ct+1 with ct = (cid:96)hLϕt
∇ env(γtµ+
ϕt+1) are strongly convex, hence they can be solved up to any accuracy by a ﬁrst-order method.

(cid:81)τ

(cid:62)

(cid:62)

t+1

Proof. For t ∈ {1, . . . , τ }, denote ˜L
The computation of the Moreau gradient ∇ env(γt−1µ+
t

(cid:62)ϕt

µ+
t

(cid:62)

ϕt) amounts to solve

an upper bound on the smoothness constant L

of µ+
t

(cid:62)

ϕt.

(cid:62)ϕt

µ+
t

(cid:62)

µ+
t

min
yt−1

ϕt(xt−1 + yt−1) +

1
2γt−1

(cid:107)yt−1(cid:107)2
2.

The above problem is strongly convex if γt−1 < 1/ ˜L

. We have

(cid:62)ϕt
µ+
t
≤ (cid:107)µ+

t (cid:107)2Lϕt,

L

µ+
t

(cid:62)ϕt

so it remains to bound (cid:107)µ+

t (cid:107)2. If the minimizations are exact, we have, using the notations of Prop. 2.2, for

24

t ∈ {2, . . . , τ }

So since µ+

t are given in closed form, for t ∈ {2, . . . , τ },

xτ − x+

τ = γτ ∇h(x+
τ )
t−1 = γt−1∇xϕt(x+

xt−1 − x+

t−1)µ+
t .

(cid:107)µ+
τ (cid:107)2 = (cid:107)βτ (xτ − x+
t−1(cid:107)2 = (cid:107)βt−1(xt−1 − x+

(cid:107)µ+

τ )(cid:107)2 ≤ γτ βτ (cid:96)h

t−1)(cid:107)2 ≤ γt−1βt−1(cid:96)ϕt(cid:107)µ+

t (cid:107).

Therefore for t ∈ {1, . . . , τ }, using γt = 1/βt, we get

(cid:107)µ+

t (cid:107)2 ≤ (cid:96)h

τ
(cid:89)

s=t+1

(cid:96)ϕt.

So a bound on the smoothness constant of µ+
t

(cid:62)

ϕt is given by ˜L

= ct = (cid:96)hLϕt

(cid:81)τ

s=t+1 (cid:96)ϕt.

µ+
t

(cid:62)ϕt

Finally we add a justiﬁcation of the form of the updates.

Proposition 3.1. Consider h : Rm → R and f a chain of computations parameterized by φt with h and
φt Lipschitz-continuous and smooth. Let α ≤ 1/ct with ct = (cid:96)hLφt
s=t+1 (cid:96)φt. For w = (w1; . . . ; wτ ) ﬁxed
consider the computation of (10) and denote λ∗

t:τ regular solutions of

t:τ and x∗

(cid:81)τ

min
vt

min
xt:τ

sup
λt:τ

h(xτ ) +

τ
(cid:88)

s=t+1

s (φs(ws, xs−1) − xs) + λ(cid:62)
λ(cid:62)

t (φt(wt + vt, xt−1) − xt) +

1
2α

(cid:107)vt(cid:107)2
2,

(13)

for xt−1 deﬁned by xs = φt(wt, xs−1) for s ∈ {1, . . . , t − 1}. The Moreau gradients ∇ env(αh ◦ fw,t)(wt) as
deﬁned in (10) are given by

argmin
vt∈Rpt

λ∗
t

(cid:62)φt(wt+vt, xt−1)+(cid:107)vt(cid:107)2

2/(2α).

(14)

Proof. If v∗
for s ∈ {t + 1, . . . , τ },

t:τ , x∗

t , λ∗

t:τ are regular solutions of (13), they satisfy the KKT conditions. In particular, we have

τ = ∇h(x∗
λ∗
τ )
s−1 = ∇xφs(w∗
λ∗
t , xt−1)λ∗
t = v∗
t .
j=s+1 (cid:96)φj . Therefore if α < c−1

s−1)λ∗
s

s , x∗

t

α∇wφt(wt + v∗
(cid:81)τ

(36)

, the function vt → λ∗
t

(cid:62)φt(wt +

We then have for s ∈ {t, . . . , τ }, (cid:107)λ∗
vt) + (cid:107)vt(cid:107)2

2/(2α) is strongly convex. Therefore v∗

s(cid:107)2 ≤ (cid:96)h

t is also given as the solution of (14).

D.4 Augmented Lagrangian Formulation

We can go further than the Lagrangian formulation and consider an augmented Lagrangian one instead.
The augmented Lagrangian formulation approach blends the approximate Moreau gradient descent and the
regularized virtual target propagation approach presented below. Namely, taking κ = 0 in Algo.
8 and
Algo. 9 we recover Algo. 1 and Algo. 2. Alternatively taking βt = 0 in Algo. 8 and Algo. 9 we recover
Algo. 12 and Algo. 13.

The augmented Lagrangian formulation can appear related to the one from Taylor et al. (2016), where
an ADMM method for deep learning was considered. The diﬀerence is that they use a Lagrange multiplier
only at the last layer; see Eq. 5 in there. Without this Lagrange multiplier, the approach of Taylor et al.
(2016) amounts to a block-coordinate descent on the penalized formulation, i.e., virtual target propagation
as presented below.

Note that, moreover, the approach of Taylor et al. (2016) is not, strictly speaking, a classical ADMM
approach actually, since the constraints are non-linear (some computations of the layers are non-linear). Our
approach does not correspond to an ADMM method for the same reason.

25

We present an inexact block-coordinate proximal point method on the augmented Lagrangian formulation
of the objective. The overall algorithm based on an augmented Lagrangian decomposition of the objective
is presented in Algo. 8 and Algo. 9.

Fact D.3. Consider the augmented Lagrangian formulation of h ◦ f with f a chain of computations, i.e.,

Laug,κ(w, x, λ) = h(xτ ) +

τ
(cid:88)

t=1

λ(cid:62)
t (φt(wt, xt−1)−xt) +

κ
2

τ
(cid:88)

t=1

(cid:107)φt(wt, xt−1) − xt(cid:107)2
2

(37)

Consider w = (w1; . . . ; wτ ), λ = (0; . . . ; 0) and x = (x0; . . . ; xτ ) deﬁned by xt = φt(wt, xt−1) for t ∈
{1, . . . , τ }. Consider a block coordinate proximal point method that updates the parameters in the order
xt → λt → wt for t = τ, . . . , 1 and denote by a superscript + the updates.

At layer τ , a proximal point step on xτ with step-size γτ , is given as (using λτ = 0, xτ = φτ (wτ , xτ −1))

x+
τ = argmin
yτ ∈Rqτ

h(yτ ) +

γ−1
τ + κ
2

(cid:107)yτ − xτ (cid:107)2
2.

At each layer t ∈ {τ, . . . , 1},
1. a proximal point step on λt = 0 with step-size βt is given as (for x+
t , xt−1 ﬁxed)
1
2βt

t (φt(wt, xt−1) − x+
λ(cid:62)

λ+
t = argmax
λt∈Rqt

(cid:107)λt(cid:107)2

t )−

=βt(φt(wt, xt−1) − x+

t ),

2. a proximal point step on wt with step-size αt reads (for λ+

t , x+

w+

t = argmin
vt∈Rpt

(cid:62)

λ+
t

φt(vt, xt−1) +

1
2αt

t , xt−1 ﬁxed)
κ
2

2 +

(cid:107)vt − wt(cid:107)2

(cid:107)φt(vt, xt−1) − x+

t (cid:107)2
2

3. if t ∈ {τ, . . . , 2}, an approximate proximal point step on xt−1 with step-size γt−1 reads (for λ+

t , x+

t , wt, λt−1 =

0 ﬁxed and using that xt−1 = φt−1(wt−1, xt−2))
γ−1
t−1 + κ
2

x+
t−1= argmin
yt−1∈Rqt−1

φt(wt, yt−1) +

λ+
t

(cid:62)

(cid:107)yt−1 − xt−1(cid:107)2 +

κ
2

(cid:107)φt(wt, yt−1) − x+

t (cid:107)2
2

For the ﬁrst layer x0 is ﬁxed to x0.

D.5 Penalized Formulation

As shown by Frerix et al. (2018), a block coordinate gradient descent on the penalized formulation of the
problem amounts to gradient descent. We show here how alternative methods such as block coordinate min-
imization or block-coordinate proximal point methods amount to virtual target propagation and regularized
virtual target propagation.

We present an inexact block-minimization method on the penalized formulation of the objective. The
overall algorithm based on this inexact block-minimization is presented in Algo. 10 and Algo. 11. We
recover then exactly the rationale of virtual target propagation, where targets are back-propagated using
approximate inverses of the layers.

Fact D.4. Consider the penalized formulation of the minimization of h ◦ f with f a chain of computations

Lpen,κ(w1:τ , x1:τ ) = h(xτ ) +

κ
2

τ
(cid:88)

t=1

(cid:107)φt(wt, xt−1) − xt(cid:107)2
2,

for x0 ﬁxed. Consider w1:τ = (w1; . . . ; wτ ) and x1:τ = (x1; . . . ; xτ ) deﬁned by xt = φt(wt, xt−1) for t ∈
{1, . . . , τ }. Consider a block coordinate minimization method that updates the parameters xt, wt for t =
τ, . . . , 1 and denote by a superscript + the updates.

26

At layer τ , a minimization on xτ amounts to computing (using that xτ = φτ (wτ , xτ −1))

x+
τ = argmin
yτ ∈Rqτ

h(yτ ) +

κ
2

(cid:107)yτ − xτ (cid:107)2
2

= xτ − ∇ env(κ−1h)(xτ ).

At each layer t ∈ {τ, . . . , 1},
1. a minimization on wt amounts to computing (for x+

w+

t = argmin
vt∈Rpt

κ
2

t , xt−1 ﬁxed )
(cid:107)φt(vt, xt−1) − x+

t (cid:107)2
2,

2. if t ∈ {τ, . . . , 2}, a minimization on xt−1 amounts to computing (for x+

t , wt ﬁxed )

x+
t−1= argmin
yt−1∈Rqt

κ
2

(cid:107)φt(wt, yt−1) − x+

t (cid:107)2
2.

For the ﬁrst layer x0 is ﬁxed to x0.

(38)

(39)

(40)

Comparison to target propagation. Eq. (40) amounts to compute an approximate inverse of the layers:
In that paper, an
this is exactly the reasoning explained for example in Eq. 8 from Lee et al. (2015).
autoencoder is used to learn the inverse mapping. Precisely, given a layer φt(wt, xt−1), the algorithm in (Lee
et al., 2015) seeks to compute an inverse layer parameterized as ψt(xt, vt) such that vt minimizes

(cid:107)ψt(φt(wt, xt−1), vt) − xt−1(cid:107)2
2

See Eq. 12 from that paper. A stochastic gradient descent is used there to optimize vt.

On the other hand equation (39) amounts to minimize the distance between the mapping of the layer
and the target: this is exactly the procedure presented in the last loop of Algo.1 of (Lee et al., 2015). See
e.g. (Manchev and Spratling, 2020; Bengio, 2014) for other presentations of target propagation.

In Algo 11 we use a proximal step on the objective h to deﬁne the ﬁrst target of the algorithm. In (Lee
et al., 2015) the ﬁrst target is given by a gradient step, namely Eq. (41) is replaced by x+
τ = xτ − γ∇h(xτ )
for a given step-size γ. The proximal step we use follow more closely the idea of a target than a simple
gradient step.

Lee et al. (2015) do not relate targets to approximate inverses. Instead a heuristic formula referred to as
diﬀerence target propagation is used. This formula is claimed by the authors to stabilize the procedure. This
corresponds to x+
t , vt) − ψt(xt, vt) where ψt(·, vt) is an approximate inverse of φt(wt, ·).
We consider a direct and transparent approach to stabilize the procedure by regularizing the computation
of the approximate inverses.

t−1 = xt−1 + ψt(x+

Regularized Target Propagation. We present an inexact proximal point method on the penalized
formulation of the objective. The overall algorithm based on this inexact block-minimization is presented in
Algo. 12 and Algo. 13. We recover then a regularized alternative to virtual target propagation as presented
in Sec. 3.2.

Fact D.5. Consider the penalized formulation of the minimization of h ◦ f with f a chain of computations

Lpen,κ(w1:τ , x1:τ ) = h(xτ ) +

κ
2

τ
(cid:88)

t=1

(cid:107)φt(wt, xt−1) − xt(cid:107)2
2,

fro x0 ﬁxed. Consider w1:τ = (w1; . . . ; wτ ) and x1:τ = (x1; . . . ; xτ ) deﬁned by xt = φt(wt, xt−1) for t ∈
{1, . . . , τ }. Consider a block coordinate proximal point method that updates parameters xt → wt for t =
τ, . . . , 1 and denote by a superscript + the updates.

At layer τ , a proximal point step on xτ with stepsize γ−1

τ

amounts to computing (using that xτ =

27

φτ (wτ , xτ −1))

x+
τ = argmin
yτ ∈Rqτ

κ + γ−1
2
τ )−1h)(xτ ).
= xτ − ∇ env((κ + γ−1

h(yτ ) +

τ

(cid:107)yτ − xτ (cid:107)2
2

(41)

At each layer t ∈ {τ, . . . , 1},
1. a proximal point step on wt with step-size αt amounts to computing (for x+

t , xt−1 ﬁxed )

w+

t = argmin
vt∈Rpt

κ
2

(cid:107)φt(vt, xt−1) − x+

t (cid:107)2

2 +

1
2αt

(cid:107)vt − wt(cid:107)2
2,

2. if t ∈ {τ, . . . , 2}, a proximal point step on xt−1 with step-size γt−1 amounts to computing (for x+

t , wt ﬁxed

using that xt−1 = φt−1(wt−1, xt−2) )

x+
t−1= argmin
yt−1∈Rqt

κ
2

For the ﬁrst layer x0 is ﬁxed to x0.

(cid:107)φt(wt, yt−1) − x+

t (cid:107)2

2 +

κ + γ−1
t−1
2

(cid:107)yt−1 − xt−1(cid:107)2
2.

(42)

Proximal back-propagation. We detail here the algorithm of Frerix et al. (2018) for an objective of
the form (A). A forward pass as in Algo 3 stores the intermediate gradients of the computations for
given parameters w1, . . . , wτ with corresponding states x1, . . . , xτ . Then for t = τ, . . . , 1, starting from
λτ = ∇h(xτ ),

1. Compute λt−1 = ∇φt(wt, xt−1)λt
2. Compute zt = xt − λt
3. Update the parameters as the solutions of

min
vt

1
2α

(cid:107)vt−wt(cid:107)2

2+

1
2

(cid:107)φt(vt, xt−1)−zt(cid:107)2
2,

(43)

for α a given stepsize.

This approach can be seen as a proximal gradient method on the problem (15) using the gradient of
f = h ◦ ϕτ ◦ . . . ◦ ϕt+1 and the proximal operator of πg for g = φt(·, xt−1). We retrieve then again another
way to access Moreau gradients of the composition.

D.6 Proximal point with a regularization

If a decomposable regularization is added to the objective, the augmented Lagrangian formulation reads

Laug,κ(w, x, λ) = h(xτ ) +

τ
(cid:88)

t=1

λ(cid:62)
t (φt(wt, xt−1)−xt) +

κ
2

τ
(cid:88)

t=1

(cid:107)φt(wt, xt−1) − xt(cid:107)2

2 +

τ
(cid:88)

t=1

rt(wt).

(44)

The penalized or the Lagrangian formulation can be modiﬁed analogously. In that case, Fact. D.3 is modiﬁed
by considering the following updates on the parameters

w+

t ≈ argmin
vt∈Rpt

(cid:62)

λ+
t

φt(vt, xt−1) +

1
2αt

(cid:107)vt − wt(cid:107)2

2 +

κ
2

D.7 Complexities

(cid:107)φt(vt, xt−1) − x+

t (cid:107)2

2 + rt(vt).

The complexity of the alternative of back-propagation in Algo 1-2 can be stated in a form similar to Baur-
Strassen’s theorem.

Proposition D.6. The space complexity S and time complexity T of the diﬀerentiable program a la Moreau

28

detailed in Algo. 1, Algo. 2 with A being a gradient descent stopped after KGD iterations is of the order of
τ
(cid:88)

τ
(cid:88)

S =

(pt + qt + S(φt)) ,

T =

T (φt)+KGD(T (∇φt)+(qt−1qt+ptqt)),

t=1
where S(φt) denotes the space complexity to retain the expression of the layer, T (φt) and T (∇φt) are the
complexities of computing the layer and its gradient respectively.

t=1

D.8 Convergence guarantees

The following convergence guarantees for ideal versions of our algorithms follow from the proof of an ap-
proximate gradient descent on the Moreau envelope recalled below.

Proposition D.7. Consider a ν-weakly convex objective f , that is such that f +ν(cid:107)·(cid:107)2
an approximate gradient descent on the Moreau envelope of f , that is, x(k+1) = x(k) − δg(k), where

2/2 is convex. Consider

for α ≤ 1/(2ν) and 2δ ≤ 1/α. Then after k iterations, this method outputs a point ˆx(k) such that

(cid:107)g(k) − ∇ envα(f )(x(k))(cid:107)2 ≤ ε,

(cid:107)∇ envα(f )(ˆx(k))(cid:107)2

2 ≤

8
5δk

(f (x(0)) − f ∗) + 8ε2.

Moreover ˆx(k) is close to a point x∗ of f , that satisﬁes

(cid:107)ˆx(k) − x∗(cid:107)2

2 ≤

8α
5δk

(f (x(0)) − f ∗) + 8αε2,

(cid:107)∇f (x∗)(cid:107)2

2 ≤

8
5δk

(f (x(0)) − f ∗) + 8ε2.

Proof. By Prop. C.3, the Moreau envelope of a ν-weakly convex function with α ≤ 1/(2ν) is 1/α smooth.
One can then apply Prop. D.8, such that after k iterations of an approximate gradient method on the Moreau
envelope, we get a point ˆx(k) that satisﬁes

(cid:107)∇ envα(f )(ˆx(k))(cid:107)2

2 ≤

8
5δk

(envα(f )(x(0)) − min
x∈Rd

envα(f )(x)) + 8ε2.

By deﬁnition of the Moreau envelope, envα(f )(x) ≤ f (x) and envα(f )(x) ≥ minx∈Rd f (x) := f ∗, so last
result simpliﬁes as

(cid:107)∇ envα(f )(ˆx(k))(cid:107)2

2 ≤

(f (x(0)) − f ∗) + 8ε2.

8
5δk

Finally if a point x satisfy (cid:107)∇ envα(f )(x)(cid:107) ≤ ε, then denoting x∗ = argminy∈Rd f (y) + 1
2, we have
(cid:107)x∗ − x(cid:107)2 = α(cid:107)∇ envα(f )(x)(cid:107)2 ≤ αε and (cid:107)x − x∗(cid:107)2 = α(cid:107)∇g(x∗)(cid:107)2. Therefore x is αε-near to an ε-stationary
point. This gives the claim.

2α (cid:107)x − y(cid:107)2

We give here a convergence guarantee for an approximate gradient descent on a smooth function with

appropriate step-sizes below. The proof technique follows the one of Devolder et al. (2014).

Proposition D.8. Let f : Rd → R be a L-smooth function. Consider an ε-approximate gradient descent on
f with step-size δ ≤ 1/(2L), i.e.,

x(k+1) = x(k) − δ (cid:98)∇f (x(k)),

where (cid:107) (cid:98)∇f (x(k)) − ∇f (x(k))(cid:107)2 ≤ ε. After k iterations, this method returns a point ˆx(k) that satisﬁes
8
5δk

(f (x(0)) − f ∗) + 8ε2,

(cid:107)∇f (ˆx(k))(cid:107)2

2 ≤

where f ∗ = minx∈Rd f (x).

Proof. Denote ξ(k) = (cid:98)∇f (x(k)) − ∇f (x(k)) for all k ≥ 0. By L-smoothness of the objective, the iterations of

29

the approximate gradient descent satisfy

f (x(k+1)) ≤ f (x(k)) + ∇f (x(k))(cid:62)(x(k+1) − x(k)) +

L
2

(cid:107)x(k+1) − x(k)(cid:107)2
2

2 − δ∇f (x(k))(cid:62)ξ(k) +

Lδ2
2

(cid:107)∇f (x(k)) + ξ(k)(cid:107)2
2

= f (x(k)) − δ(cid:107)∇f (x(k))(cid:107)2
(cid:19)

(cid:18)

= f (x(k)) − δ

≤ f (x(k)) − δ

1 −

(cid:18)

1 −

Lδ
2
Lδ
2

(cid:107)∇f (x(k))(cid:107)2

2 +

(cid:19)

(cid:107)∇f (x(k))(cid:107)2

2 +

Lδ2
2
Lδ2
2

(cid:107)ξ(k)(cid:107)2

2 + δ(Lδ − 1)∇f (x(k))(cid:62)ξ(k)

(cid:107)ξ(k)(cid:107)2

2 + δ(1 − Lδ)(cid:107)∇f (x(k))(cid:107)2(cid:107)ξ(k)(cid:107)2,

where in the last inequality we bounded the absolute value of the last term and used that δL ≤ 1. Now we
use that for any a, b ∈ R and θ > 0, 2ab ≤ θa2 + θ−1b2, which gives for θ > 0, a = (cid:112)δ(1 − Lδ)/2(cid:107)∇f (x(k))(cid:107)2
and b = (cid:112)δ(1 − Lδ)/2(cid:107)ξ(k)(cid:107)2,

f (x(k+1)) ≤ f (x(k)) − δ

(cid:18)

1 −

(cid:107)∇f (x(k))(cid:107)2

2 +

Lδ2 + θ−1δ(1 − Lδ)
2

(cid:107)ξ(k)(cid:107)2
2.

Using 0 ≤ Lδ ≤ 1/2, θ = 1/4 and (cid:107)ξ(k)(cid:107)2

(cid:19)

Lδ + θ(1 − Lδ)
2
2 ≤ ε2, we get
5
8

f (x(k+1)) ≤ f (x(k)) −

δ(cid:107)∇f (x(k))(cid:107)2

2 +

9
2

δε2.

Rearranging the terms, summing from i = 0, . . . , k − 1 we get

k−1
(cid:88)

i=0

(cid:107)∇f (x(i))(cid:107)2

2 ≤

8
5δ

(f (x(0)) − f (x(k))) +

36
5

kε2.

Taking the minimum and dividing by k we get

min
i∈{0,...,k−1}

(cid:107)∇f (x(i))(cid:107)2

2 ≤

8
5δk

(f (x(0)) − f (x(k))) + 8ε2.

Taking ˆx(k) the minimum iterate of the left hand side we get the claim.

The following proposition presents how a gradient descent using approximate Moreau gradients converges

to a stationary point up to the error of approximation.

Proposition D.9. Consider a gradient descent on the Moreau envelope applied to an objective of the form
h(f (w, x0)) with f a chain of computations, that is

w(k+1) = w(k)+ argmin

v∈Rp

min
x∈Rq

h(xτ ) +

1
2α

τ
(cid:88)

t=1

(cid:107)vt(cid:107)2
2

subject to xt = φt(w(k)

t + vt, xt−1),

(45)

(cid:81)τ

s=t+1 (cid:96)φt

(cid:1).

with x0 = x0. Assume h and φt to be Lispchitz continuous and convex and α ≤ 1/ (cid:0)maxt∈{1,...,τ } (cid:96)hLφt
Assume moreover w → h(f (w, x0)) to be ν-weakly convex and α ≤ ν−1/2.

Denote x∗ = (x∗
1; . . . ; λ∗

τ )a regular solution4 to (45) with associated Lagrange multipliers
λ∗ = (λ∗
the gradient descent with approximate
Moreau gradients presented in Algo. 1 and Algo. 2 , Lagrange multipliers λ(k) are computed such that for

1; . . . ; v∗
τ ). Assume that at each iteration of Algo. 6, i.e.

τ ), v∗ = (v∗

1; . . . ; x∗

4Denote c(w, x) = 0 the constraints deﬁning (45) with c : Rp+q → Rτ , a regular solution is a pair (w∗, x∗) such that

∇c(w∗, x∗) is of rank τ .

30

any t = 1, . . . , τ ,

with

(cid:107)v(k)

t − v∗

t (cid:107) ≤ αε
λ(k)
t

v(k)
t = argmin
vt∈Rpt

(cid:62)

φt(w(k)

t + vt, x(k)

t−1) +

1
2α

(cid:107)vt(cid:107)2
2

v∗
t = argmin
vt∈Rpt

λ∗
t

(cid:62)φt(w(k)

t + vt, x∗

t−1) +

1
2α

(cid:107)vt(cid:107)2
2,

(46)

(47)

t

where x(k)
are given by a forward pass on the chain of computations. Consider then iterations w(k+1) =
w(k) + v(k). After K iterations, this method outputs a point w close to a stationary point w(cid:63), that is, such
that

(cid:107)w − w(cid:63)(cid:107)2

2 ≤

(cid:107)∇w(h ◦ f )(w(cid:63), x0)(cid:107)2

2 ≤

8
5K
8
5αK

(h ◦ f (w(0), x0) − h ◦ f ∗) + 8αε2,

(h ◦ f (w(0), x0) − h ◦ f ∗) + 8ε2,

with h ◦ f ∗ = minw h ◦ f (w, x0).
Proof. Since α ≤ 1/ (cid:0)maxt∈{1,...,τ } (cid:96)hLφt
(cid:1), the problems (47) are convex. Therefore the deviations
v∗ computed using the Lagrange multipliers λ∗ and optimal state variables x∗ form an exact gradient step on
the Moreau envelope.. Then assumption (46) amounts to have access to an ε approximation of the gradient
of the Moreau envelope. Prop. D.7 gives then the result.

s=t+1 (cid:96)φt

(cid:81)τ

The previous proposition can now be adapted as follows to the augmented Lagrangian formulation.

Proposition D.10. Consider a gradient descent on the Moreau envelope applied to an objective of the form
h(f (w, x0)) with f a chain of computations, that is

w(k+1) = w(k)+ argmin

v∈Rp

min
x∈Rq

h(xτ ) +

1
2α

τ
(cid:88)

t=1

(cid:107)vt(cid:107)2
2

subject to xt = φt(w(k)

t + vt, xt−1),

(48)

with x0 = x0. Assume moreover w → h(f (w, x0)) to be ν-weakly convex and α ≤ ν−1/2.

τ ), v∗ = (v∗

1; . . . ; v∗

τ ), λ∗ = (λ∗

1; . . . ; λ∗

τ ) the solutions of the augmented Lagrangian

Denote x∗ = (x∗

1; . . . ; x∗
formulation of the problem

h(xτ ) +

min
x1,...,xτ ,
v1,...,vτ ,
λ1,...,λτ

τ
(cid:88)

t=1

t (φt(w(k)
λ(cid:62)

t + vt, xt−1)−xt) +

κ
2

τ
(cid:88)

t=1

(cid:107)φt(w(k)

t + vt, xt−1) − xt(cid:107)2

2 +

1
2α

τ
(cid:88)

t=1

(cid:107)vt(cid:107)2
2.

Assume that at each iteration of Algo. 7, i.e. the approximate Moreau gradient descent based on an augmented
Lagrangian formulation of the problem, Lagrange multipliers λ(k) and target variables z(k) are computed such
that for any t = 1, . . . , τ ,
(cid:107)v(k)

(49)

t − v∗

t (cid:107) ≤ αε
λ(k)
t

(cid:62)

with

v(k)
t = argmin
vt∈Rpt

φt(wt + vt, x(k)

t−1) +

(cid:107)vt(cid:107)2

2 +

(cid:107)φt(vt, x(k)

t−1) − z(k)

t (cid:107)2
2

1
2α
1
2α

κ
2
κ
2

v∗
t = argmin
vt∈Rpt

λ∗
t

(cid:62)φt(w(k)

t + vt, x∗

t−1) +

(cid:107)vt(cid:107)2

2 +

(cid:107)φt(vt, x∗

t−1) − x∗

t (cid:107)2
2,

(50)

(51)

where x(k)
vt, x∗

t
t−1) + 1

are given by a forward pass on the chain of computations and we assume vt → λ∗
t
2α (cid:107)vt(cid:107)2

t +
2 convex. Consider then iterations w(k+1) = w(k) + v(k). After K

2 (cid:107)φt(vt, x∗

t−1) − x∗

2 + κ

t (cid:107)2

(cid:62)φt(w(k)

31

iterations, this method outputs a point w close to a stationary point w(cid:63), that is, such that

(cid:107)w − w(cid:63)(cid:107)2

2 ≤

(cid:107)∇w(h ◦ f )(w(cid:63), x0)(cid:107)2

2 ≤

with h ◦ f ∗ = minw h ◦ f (w, x0).

8
5K
8
5αK

(h ◦ f (w(0), x0) − h ◦ f ∗) + 8αε2,

(h ◦ f (w(0), x0) − h ◦ f ∗) + 8ε2,

Proof. The proof is the same as Prop. D.9. Here we directly assumed vt → λ∗
t
κ
2 (cid:107)φt(vt, x∗
the solution of the computations in the inner problems.

2 +
2 to be convex such that the solution of the augmented Lagrangian formulation is also

t +vt, x∗

t−1) − x∗

2α (cid:107)vt(cid:107)2

t−1)+ 1

t (cid:107)2

(cid:62)φt(w(k)

32

D.9 Algorithms

Algorithm 3 Forward pass for gradient-backpropagation

1: Inputs: Chain of computations f deﬁned by (φt)t=1,...,τ as in Def. 1.1,

input x0, variable w =

(w1; . . . ; wτ )
2: Initialize x0 = x0
3: for t = 1, . . . , τ do
4:
5:
6: end for
7: Output: xτ
8: Store: (Φx

t , Φw

t )τ

t=1

Compute xt = φt(wt, xt−1)
Store Φx

t = ∇xφt(wt, xt−1)(cid:62), Φw

t = ∇wφt(wt, xt−1)(cid:62)

Algorithm 4 Backward pass for gradient-backpropagation
1: Inputs: Input µ, intermediate gradients (Φx
2: Initialize λτ = µ
3: for t = τ, . . . , 1 do
4:
5:
6: end for
7: Output: (g1; . . . ; gτ ; λ0) = ∇f (w, x0)µ

Compute λt−1 = Φx
t
Store gt = Φw
t

t , Φw

(cid:62)λt

(cid:62)λt

t )τ

t=1

Algorithm 5 Diﬀerentiable program for computing ﬁrst-order oracles

1: Inputs: Chain of computations f deﬁned by (φt)t=1,...,τ as in Def. 1.1,

input x0, variable w =

(w1; . . . ; wτ )

2: Compute using Algo. 3

which gives f (w, x0) = xτ .
3: Deﬁne µ → ∇f (w, x0)µ as

(xτ , (Φx

t , Φw

t )τ

t=1) = Forward(f, w)

µ → Backward(µ, (Φx

t , Φw

t )τ

t=1)

according to Algo. 4.

4: Output: f (w, x0), µ → ∇f (w, x0)µ

Algorithm 6 Gradient descent with approximate Moreau gradients

Inputs: Chain of computations f , objective h, initial point x0 initial variables w(0) = (w(0)
total number of iterations K, stepsizes (αt, γt)τ
t=1
for k = 0, . . . , K − 1 do

1 , . . . , w(0)
τ ),

Compute xτ = f (w(k), x0), (Φx
Compute g1, . . . , gτ from Algo. 2 with inputs xτ , h, (Φx
, . . . w(k+1)
Update the parameters as w(k+1) = (w(k+1)
τ

t , Φw

t )τ

t , Φw
t )τ
) with

1

t=1 from Algo. 1 applied to f with inputs x0 and parameters w(k)

t=1 and stepsizes (αt, γt)τ

t=1

end for

w(k+1)
t

= w(k)

t − gt

for

t ∈ {1, . . . τ }

33

Algorithm 7 Gradient descent with approximate Moreau gradients using the Augmented Lagrangian for-
mulation

Inputs: Chain of computations f , objective h, initial point x0 initial variables w(0) = (w(0)
total number of iterations K, stepsizes (αt, βt, γt)τ
for k = 0, . . . , K − 1 do

t=1,penalty parameter κ

1 , . . . , w(0)
τ ),

Compute xτ = f (w(k), x0), (Φx

t,κ, Φw

t,κ)τ

t=1 from Algo. 8 applied to f with inputs x0, parameters w(k)

and penalty parameter κ

Update the parameters as w(k+1) = (w+
t,κ)τ

1 ; . . . ; w+
t=1 and stepsizes (αt, βt, γt)τ

t,κ, Φw

τ ) where w+
t=1

inputs xτ , h, (Φx
end for

1 , . . . , w+

τ are the outputs from Algo. 9 with

Algorithm 8 Forward pass using the augmented Lagrangian formulation (37)

Inputs: Input x0, chain of computations f deﬁned by computations φt, variable w = (w1; . . . ; wτ ), penalty
parameter κ
for t = 1, . . . , τ do

Compute xt = φt(wt, xt−1)
Store

Φx

t,κ : x+

t , βt, γt−1 → A

(cid:34)
βt(φt(wt, xt−1) − x+

t )(cid:62)φt(wt, ·) +

γ−1
t−1 + κ
2

(cid:107) · −xt−1(cid:107)2 +

κ
2

(cid:35)

(cid:107)φt(wt, ·) − x+

t (cid:107)2
2

Φw

t,κ : x+

t , βt, αt → A

(cid:20)
βt(φt(wt, xt−1) − x+

t )(cid:62)φt(·, xt−1) +

1
2αt

(cid:107) · −wt−1(cid:107)2 +

κ
2

(cid:107)φt(·, xt−1) − x+

t (cid:107)2
2

(cid:21)

where A(h) is the result of the minimization of h either in closed form or by an algorithm A that returns
an approximate solution.
end for
Output: Last state xτ
Store: Non-linear forms (Φx

t,κ, Φw

t,κ)τ

t=1

Algorithm 9 Backward pass using the augmented Lagrangian formulation (37)

t,κ)τ

t=1 computed in Algo. 8, output xτ , stepsizes αt, βt, γt

τ +κ
2

τ = A

(cid:104)
h(·) + γ−1

t,κ, Φw
Inputs: Objective h, non-linear forms (Φx
(cid:105)
Initialize x+
(cid:107) · −xτ (cid:107)2
2.
for t = τ, . . . , 1 do
t,κ(x+
x+
t−1 = Φx
t,κ(x+
w+
t = Φw
end for
Output: w+

t , βt, γt−1)
t , βt, αt, )

1 , . . . , w+
τ .

34

Algorithm 10 Forward pass using a block coordinate minimization on the penalized formulation (20)

Inputs: Input x0, chain of computations f deﬁned by computations φt, variable w = (w1; . . . ; wτ )
for t = 1, . . . , τ do

Compute xt = φt(wt, xt−1)
Store

Φx

t : x+

t → A

(cid:107)φt(wt, ·) − x+

t (cid:107)2
2

(cid:21)

(cid:20) 1
2

Φw
t

: x+

t → A

(cid:107)φt(·, xt−1) − x+

t (cid:107)2
2

(cid:21)

(cid:20) 1
2

where A(h) is the result of the minimization of h either in closed form or by an algorithm A that returns
an approximate solution.
end for
Output: Last state xτ
Store: Non-linear forms (Φx

t , Φw

t )τ

t=1

Algorithm 11 Backward pass using a block coordinate minimization on the penalized formulation (20)
t )τ

t=1 computed in Algo.

10, output xτ , penalization

t , Φw

τ = A (cid:2)h(·) + κ

Inputs: Objective h, non-linear forms (Φx
parameter κ
Initialize x+
for t = τ, . . . , 1 do
t (x+
x+
t−1 = Φx
t )
t (x+
gt = Φw
t )

2 (cid:107) · −xτ (cid:107)2

(cid:3)

2

end for
Output: g1, . . . , gτ .

Algorithm 12 Forward pass using a proximal point method on the penalized formulation (20)

Inputs: Input x0, chain of computations f deﬁned by computations φt, variable w = (w1; . . . ; wτ )
for t = 1, . . . , τ do

Compute xt = φt(wt, xt−1)
Store

Φx

t,γt−1

: x+

t → A

Φw

t,αt

: x+

t → A

(cid:34)

κ
2
(cid:20) κ
2

(cid:107)φt(wt, ·) − x+

t (cid:107)2

2 +

κ + γ−1
t−1
2

(cid:35)

(cid:107) · −xt−1(cid:107)2
2

(cid:107)φt(·, xt−1) − x+

t (cid:107)2

2 +

(cid:21)

(cid:107) · −wt(cid:107)2
2

1
2αt

where A(h) is the result of the minimization of h either in closed form or by an algorithm A that returns
an approximate solution.
end for
Output: Last state xτ
Store: Non-linear forms (Φx

, Φw

t,γt−1

t,αt

)τ
t=1

35

Algorithm 13 Backward pass using a proximal point method on the penalized formulation (20)
t , Φw

t=1 computed in Algo.

t )τ

12, output xτ , penalization

Inputs: Objective h, non-linear forms (Φx
parameter κ, stepsizes αt, γt
Initialize x+
τ = A
for t = τ, . . . , 1 do
x+
t−1 = Φx
gt = Φw

(cid:104)
h(·) + κ+γ−1

(cid:107) · −xτ (cid:107)2
2

t,γt−1
(x+
t )

(x+
t )

(cid:105)

2

τ

t,αt

end for
Output: g1, . . . , gτ .

36

Fig. 7: Pendulum.

E Additional Details on Numerical Results

E.1 Additional details related to the numerical results

E.1.1 Motion dynamics for the swinging pendulum

In Sec. 4 we presented comparisons of a gradient descent against a gradient descent with approximate
Moreau gradients on a pendulum. We detail here the formulation of the control of a pendulum. A pendulum
is described by the angle of the rod θ with the vertical axis see Fig. 7, and its dynamics are described in
continuous time as

¨θ(t) = −

sin θ(t) −

˙θ(t) +

µ
ml2

1
ml2 w(t),

g
l

where m = 1 denotes the mass of the bob, l = 1 denotes the length of the rod, µ = 0.01 is the friction
coeﬃcient, g = 9.81 is the g-force and w(t) is a torque applied to the pendulum. The state of the pendulum
is given by the angle θ and its speed ω = ˙θ concatenated in x = (θ; ω) which after discretization follow the
dynamics φ(xt−1, ut) = xt = (θt; ωt) s.t.

θt = θt−1 + δωt−1

(cid:18)

ωt = ωt−1 + δ

−

g
l

sin θt−1 −

µ
ml2 ωt−1 +

1
ml2 wt

(cid:19)

,

where δ is the discretization step, xt−1 = (θt−1, ωt−1) us the current state and wt is a control parameter.
The objective is then given by for x = (θ, ω)

h(x) = (θ − π)2 + ρω2,

where ρ = 0.1 is a penalty parameter. The objective h enforces the pendulum to swing up and be close
to equilibrium (low speed) after τ steps. We take a discretization step δ = 0.1 and an horizon τ = 50 or
τ = 100.

E.1.2 Training

For the optimization of deep networks we consider objectives of the form

min
w

1
n

n
(cid:88)

i=1

L(yi, f (w, xi)) + r(w),

where (x1, y1), . . . , (xn, yn) are the images-labels samples of the dataset MNIST, L is the squared loss, r is
a layer-wise (cid:96)2
2/2 with µ = 10−6. The approximate Moreau gradient
computations are modiﬁed as explained in Sec. D.6. We consider

2-regularization, r(w) = (cid:80)τ

t=1 µ(cid:107)wt(cid:107)2

37

• a multi-layer perceptron with hidden layers of size (4000, 1000, 4000), where each layer is an aﬃne

layer followed by a ReLU activation, trained with a squared-loss,

• a convolutional neural network, whose architecture is outlined in Sec. 4, trained with a logistic loss

(after the ﬁnal fully connected layer FC).

E.2 Experimental setting

E.2.1 Numerical implementation

The approximate back-propagation of the Moreau envelope outlined in Algo. 1, 2 involve an inexact mini-
mization to deﬁne Φx
t at line 4 by some algorithm A. To illustrate the potential of the proposed
approach, we report the results when this inexact minimization is performed with 2 steps of a quasi-Newton
algorithm. Namely we use one step of a gradient descent with a Goldstein line-search followed by one step
of gradient descent using the Barzilai-Borwein step-size computation (Bonnans et al., 2006).

t and Φw

t φt(wt, ·))(xt−1) and gt = ∇ env(αˆλ(cid:62)

In Algo. 2, we choose γt = γτ −t+1 and αt = αγt+1 such that the updates of our algorithm can be rewritten
ˆλt−1=∇ env(γˆλ(cid:62)
t φt(·, xt−1))(wt). This choice of step-size is motivated
by Prop. 2.3 that shows that the step-sizes γt required for the subproblems to be strongly convex need to
decrease geometrically as t goes from τ to 1. Alternatively it can be seen as ﬁxing β = 1 in Prop. 2.2 and
γt = γ.

E.2.2 Parameter setting

For the approximate Moreau gradient descent we use updates of the form

ˆλt−1=∇ env(γˆλ(cid:62)

t φt(wt, ·))(xt−1)

gt = ∇ env(αˆλ(cid:62)

t φt(·, xt−1))(wt),

(52)

as explained above.

For the nonlinear control example, we set γ = 0.5 and perform a grid-search on powers of 2 for α which
gives α = 27 for both τ = 50 or τ = 100. In comparison, a grid-search on the stepsize, denoted γ, of a
gradient descent on powers of 2 gives γ = 1 for τ = 50 and γ = 0.25 for τ = 100.

For the deep learning example, we found γ = 1, α = 2 in (52) after a grid-search on these parameters
and γ = 0.5, α = 0.5 for the ConvNet experiment. The stepsize of SGD was optimized on a grid of powers
of 2 and gave γ = 1 for the MLP experiment and γ = 0.125 for the ConvNet.

We explore in more detail the choice of the parameters of our approach by considering the general

augmented Lagrangian formulation presented in Algo. 8 and Algo. 9.

F Additional comparisons

F.1 Inversion of a deep network

Given a deep network with learned parameters, the information contained in the representation can be
analyzed by inverting the deep network as proposed by Fong et al. (2019). Denoting fw : Rd → Rm the
mapping of a deep network with ﬁxed learned parameters w = (w1; . . . ; wτ ), the objective is to ﬁnd a
perturbation that maximizes the response of the network, i.e., it amounts to solve

max
m∈Sm

fw(m ⊗ x) + r(m),

(53)

where Sm is a set of smooth masks applied to an image x through an operation m ⊗ x and r(m) is a
regularization on the size of the mask, see (Fong et al., 2019) for more details. Gradient ascent has been
used to maximize this objective. However an adequate step-size for the inversion might be too small due
to the potentially large smoothness constant of the mapping. To tackle this problem we can rather use an
approach `a la Moreau that approximately inverses the layers in lines 4-5 of Algo. 2.

38

Fig. 8: Comparison of the inversion of the VGG network using gradient descent (orange) on the
minimization formulation of problem (53) or an approximation of the Moreau envelope. (blue)

We use the code from Fong et al. (2019) and we consider the VGG network from Simonyan and Zisserman
(2015) with 16 layers. We used a blur perturbation for the mask. The objective is to preserve the maximum
information that encodes the label. The area of the mask is constrained to be 5% of the original image area.
To implement our algorithm, we used the diﬀerentiable programming `a la Moreau to back-propagate
the inverses of each layer of the VGG network. Once a candidate inverse has been computed, the mask
is updated by using a ﬁnal minimization step between the image computed in the back-propagation `a la
Moreau and the output of the perturbation done by the mask plus a regularization. In the left of Fig. 8, we
plot the average performance of our method compared to a gradient descent on 10 images.

Hyper-parameter tuning. For the inversion example, we take the largest step-size of gradient descent
that did not lead to divergence on a scale of 10k. We then took 0.1 as the step-size for the gradient descent.
We consider the augmented Lagrangian approach (Algo. 7), we searched for γ and β on a scale of 10k and
took γ = 102 and β = 10−2. We used κ = 10−6 for our method.

F.2 Optimization of multi-layer perceptrons of diﬀerent sizes using an aug-

mented Lagrangian approach

We consider the optimization of deep networks using the augmented Lagrangian formulation presented in
Algo. 8 and Algo. 9. We denote in the following Algo. 7 as AL-MGD and its stochastic variant as AL-MSGD.
In Fig. 9, we present comparisons of our algorithm versus a gradient descent for diﬀerent values of κ.
The task is to classify the handwritten digits of the dataset MNIST using a multi-layer perceptron with
hidden layers (500, 300, 100). We observe that κ = 10−5 provides an optimal behavior both in the batch and
stochastic setting, while a non-augmented Lagrangian formulation (κ = 0) performs better in the stochastic
setting and a more regularized Lagrangian formulation (κ = 10−3) performs better in the batch setting.

In Fig 10, we present additional comparisons of our approach on mini-batches compared to stochastic

gradient descent. The mini-batches are of size 256.

For all plots we present on the y-axis the minimum of the objective obtained so far, i.e, yk = mini=1,...,k F (w(i))

where F is the objective (train or test losses) and w(i) are the parameters along the iterations i.

Parameter setting. For the step-size on variables λt, we perform a search for β ∈ {10i : i ∈ {−4, . . . 0}}.
We observed that for high values of β the augmented Lagrangian version of our algorithm diverges. The

39

050100Iterations0100200ObjectiveSoftGDGDGDAL-MGDSoft Gradient DescentGradient DescentOriginal imageApproximate Moreau Gradient DescentGradient DescentOriginal ImageHidden Layers
(500)
(500, 300, 100)
(500, 500, 500)
(500, 400, 300, 200, 100)
(500, 500, 500, 500, 500)

SGD
step-size
1
4
8
8
8

AL -MSGD
γ
256
32
256
2
16

α
64
8
64
16
64

β
0.01
0.1
0.01
1
0.1

Table 2: Parameter settings considered for mini-batch stochastic optimization.

main parameter is the step-size used to update the parameters α for which we perform a grid-search on {2i :
i ∈ {−10, . . . , 9}} (same as the search used for the classical gradient descent or stochastic gradient descent).
Finally, the step-size used to update the states is coupled with the parameter used for the parameters i.e.
γ = ρα where we make ρ varies in {2i : i ∈ {−4, . . . , 3}}. We choose κ = 10−5 for the regularization of the
augmented Lagrangian approach as explained below.

The parameters with the smallest area under the optimization curve after 50 iterations or epochs are
selected. Table 2 presents the results of this grid-search for diﬀerent MLP architectures for the stochastic
optimization with mini batches of size 256. On the left, we vary the number of hidden layers and units per
hidden layers in the multi-layer perceptrons architectures. On the SGD column we present the best learning
rate found by SGD on this problem for mini-batches of size 256. On the AL-MSGD columns we present the
best parameters α, γ, β, found for our proposed algorithm presented in Algo. 8, Algo. 9, Algo. 7 using again
mini-batches of size 256. These parameters are used in the plots of Fig. 10.

40

Fig. 9: Left: batch comparison of GD and AL-MGD on a MLP with hidden layers (500, 300, 100) on
MNIST for diﬀerent values of κ. Right: same experiment but with mini-batches of size 256. For all plots
we present on the y-axis the minimum of the objective obtained so far, i.e, yk = mini=1,...,k F (w(i)) where
F is the objective (train or test losses) and w(i) are the parameters along the iterations i.

41

020406080100Iterations1234Train1e2AL-MGD =0.00e+00AL-MGD =1.00e-05AL-MGD =1.00e-03GD020406080100Iterations0.51.0TrainAL-MSGD =0.00e+00AL-MSGD =1.00e-05AL-MSGD =1.00e-03SGD020406080100Iterations1234Test1e2AL-MGD =0.00e+00AL-MGD =1.00e-05AL-MGD =1.00e-03GD020406080100Iterations234Test1e3AL-MSGD =0.00e+00AL-MSGD =1.00e-05AL-MSGD =1.00e-03SGDFig. 10: Comparisons of stochastic gradient descent and approximate Moreau gradient descent on MNIST
for various architecture sizes. From top to bottom the number of hidden layers is (i) (500), (ii) (500, 300,
100), (iii) (500, 500, 500), (iv) (500, 400, 300, 200, 100), (v) (500, 500, 500, 500, 500) For all plots we
present on the y-axis the minimum of the objective obtained so far, i.e, yk = mini=1,...,k F (w(i)) where F is
the objective (train or test losses) and w(i) are the parameters along the iterations i.
42

01020304050Iterations0.51.01.52.0TrainAL-MSGDSGD01020304050Iterations45678Test1e3AL-MSGDSGD01020304050Iterations0.40.60.81.01.2TrainAL-MSGDSGD01020304050Iterations234Test1e3AL-MSGDSGD01020304050Iterations0.250.500.751.001.25TrainAL-MSGDSGD01020304050Iterations234Test1e3AL-MSGDSGD01020304050Iterations0.250.500.751.001.251.50TrainAL-MSGDSGD01020304050Iterations234Test1e3AL-MSGDSGD01020304050Iterations0.51.01.52.0TrainAL-MSGDSGD01020304050Iterations234Test1e3AL-MSGDSGD