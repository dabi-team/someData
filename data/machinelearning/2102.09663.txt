Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility
Pump

Meng Qi * 1 Mengxin Wang * 1 Zuo-Jun (Max) Shen 1

1
2
0
2

l
u
J

6
1

]

G
L
.
s
c
[

2
v
3
6
6
9
0
.
2
0
1
2
:
v
i
X
r
a

Abstract

Mixed integer programming (MIP) is a general
optimization technique with various real-world
applications. Finding feasible solutions for MIP
problems is critical because many successful
heuristics rely on a known initial feasible solution.
However, it is in general NP-hard. In this work,
we propose a deep reinforcement learning (DRL)
model that efﬁciently ﬁnds a feasible solution for
a general type of MIPs. In particular, we develop
a smart feasibility pump (SFP) method empow-
ered by DRL, inspired by Feasibility Pump (FP), a
popular heuristic for searching feasible MIP solu-
tions. Numerical experiments on various problem
instances show that SFP signiﬁcantly outperforms
the classic FP in terms of the number of steps
required to reach the ﬁrst feasible solution. We
consider two different structures for the policy
network. The classic perception (MLP) and a
novel convolution neural network (CNN) struc-
ture. The CNN captures the hidden information
of the constraint matrix of the MIP problem and
relieves the burden of calculating the projection
of the current solution as the input at each time
step. This highlights the representational power
of the CNN structure.

1. Introduction

Integer programming (IP) and mixed integer programming
(MIP) are mathematical optimization problems where all or
some of the decision variables are restricted to be integers.
IPs and MIPs are critical for solving discrete and combinato-
rial optimization problems in various applications, including
production planning, scheduling, and vehicle routing (we

*Equal contribution

1Department of Industrial Engineer-
ing and Operations Research, UC Berkeley.
Correspon-
dence to: Meng Qi <meng qi@berkeley.edu>, Mengxin Wang
<mengxin wang@berkeley.edu>.

Reinforcement Learning for Real Life (RL4RealLife) Workshop in
the 38 th International Conference on Machine Learning, 2021.
Copyright 2021 by the author(s).

refer to (Pochet and Wolsey, 2006), (Sawik, 2011), and (Ma-
landraki and Daskin, 1992) for more details). The study
of theory and computation methods dates back to several
decades ago ((Dantzig et al., 1954), (Land and Doig, 2010),
and (Padberg, 1973)).

Despite the importance of solving IPs and MIPs in appli-
cation, they are generally very difﬁcult to solve in theory
(NP-hard) and in practice. There is no polynomial time algo-
rithm that can guarantee to solve a general IP/MIP. Besides
exact methods (Branch-and-bound, branch-and-cut, etc.),
heuristic algorithms are widely adopted for their simplicity
and efﬁciency. Many successful heuristics including local
branching (Fischetti and Lodi, 2003) and RINS (Danna et
al., 2005), require an initial feasible solution to start with.
Finding a feasible solution is often the ﬁrst step and one of
the most critical steps of solving IP/MIPs. However, it is
also NP-hard in general; even well-developed commercial
solvers, such as CPLEX, may struggle or even fail.

In this study, our goal is to provide a deep reinforcement
learning (DRL) model that efﬁciently ﬁnds a feasible solu-
tion for a general type of IP/MIPs. In particular, we consider
IP/MIP problems with a linear objective, linear constraints,
and integral constraints. Such formulation is quite general
since any combinatorial optimization problem with ﬁnite
feasible regions can be reformulated as an IP/MIP in this
form (Tang et al., 2020). We aim at developing a DRL agent
that smartly perform the idea of Feasibility Pump, which is
one of the most well-known algorithms for ﬁnding a feasible
solution for MIPs in optimization theory.

The main idea of FP is to decompose the original prob-
lem into two problems: one focuses on the feasibility of a
continuous relaxation and the other retains integrality. The
original work of FP was introduced by (Fischetti et al.,
2005) for 0 − 1 MIPs. The authors demonstrated that FP is
very effective in ﬁnding feasible solutions. Later, (Bertacco
et al., 2007) extended this algorithm to general MIPs and
introduced a random component in the rounding function.
Further extensions of FP include the following. (Fischetti
and Salvagnin, 2009) adjusted the rounding method so that
a feasible solution can be found in fewer steps. (Baena and
Castro, 2011) and (Boland et al., 2014) investigated the idea
of using integral reference points to further improve the

 
 
 
 
 
 
Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

rounding steps. (Achterberg and Berthold, 2007) proposed
the objective feasibility pump, which includes the original
objective in the objective function of FP to improve the qual-
ity of the feasible solution. (Boland et al., 2012) proposed
a penalty system for non-integrality to prevent the FP from
cycling. (Bonami et al., 2009) and (Bonami and Gonc¸alves,
2012) extended the FP algorithm to nonlinear MIPs.

Although this heuristic is widely adopted, it requires solving
an optimization problem within each iteration, which is
especially inefﬁcient when extending to nonlinear cases.
The efﬁciency may also be weakened because FP may loop
between infeasible points.

Recently, several studies have been conducted exploring
the topic of using machine learning methods to solve
IP/MIPs. The main drawback of adopting supervised learn-
ing schemes is that they require known (near) optimal so-
lutions in the training set, which is difﬁcult to achieve for
IP/MIPs. One of the most closely related work is (Fischetti
and Jo, 2017), where they trained deep neural networks to
ﬁnd a feasible solution for 0-1 MIPs.

Some other works have aimed to use machine learning meth-
ods for branch-and-bound algorithms. (He et al., 2014) used
imitation learning to learn an adaptive node searching order
to achieve better solutions faster for MIPs. (Khalil et al.,
2016) learned an easy-to-evaluate surrogate function that
mimics the strong branching strategy for branch-and-bound.
(Nair et al., 2020) proposed the methods of neural diving
and neural branching, to improve the high-quality joint vari-
able assignment and bounding the gap, respectively.

Other related works have studied combinatorial problems,
which are a subset of IPs. The most related works include
(Khalil et al., 2017), where the authors proposed a method
based on RL to solve various combinatorial problems on
graphs. (Bello et al., 2016) proposed a framework based
on recurrent neural network (RNN) and deep reinforcement
learning (DRL) to general combinatorial problems. (Nazari
et al., 2018) proposed a simpliﬁed version of RNN and DRL
to tackle the vehicle routing problem (VRP). (Li et al., 2018)
combined deep learning techniques with a graph convolu-
tional network to solve combinatorial problems based on
graphs. (Delarue et al., 2020) investigated VRP and de-
veloped a framework for a value-function-based RL model
where the action selection problem is formulated as a mixed-
integer optimization problem. (Tang et al., 2020) designed
RL methods to enhance the cutting plane method for solving
IPs.

So far to our knowledge, there is no existing work that fo-
cuses on feasibility study for general MIPs using RL models.
We believe that RL models can be utilized to learn feasible
solutions for a general type of MIPs.

Our contributions. In this work, we propose a general

framework, the smart feasibility pump (SFP) model, which
utilizes the RL frameworks based on the spirit of FP. More
speciﬁcally, we propose two different SFP models: SFP
based on multi-layer perception (SFP-MLP) and SFP based
on convolutional neural network (SFP-CNN), both ﬁnd fea-
sible solutions for IP/MIPs more efﬁciently, that is, with
fewer search steps. Moreover, to better capture the structure
of the constraint matrix, SFP-CNN adopts a novel CNN
structure for policy learning networks that signiﬁcantly im-
proves the performance of SFP-CNN compared to SFP-
MLP and the original FP. We summarize the contribution of
our work as follows:

• A RL model for feasible solutions of MIPs. Differ-
ent from existing literature which mostly focus on one
speciﬁc application and aim for the optimal solution,
our work is the ﬁrst attempt to use (deep) RL methods
for seeking feasible solutions for a class of general
MIPs.

• The spirit of a successful heuristic. Our model is
inspired by FP, one of the most popular heuristic algo-
rithms for ﬁnding feasible solutions for IP/MIP. The
effectiveness of decomposing the continuous relaxation
and integrality has been veriﬁed by the success of FP.
Adopting a similar idea, we propose the smart feasibil-
ity pump model because it is empowered by deep RL
models.

• A novel CNN for constraint matrix. Besides a regu-
lar MLP, we innovatively adopt a convolutional struc-
ture for the policy network to capture the structure of
constraint matrix of MIPs. The CNN provides us with
a stronger representational power to capture the hidden
structure of the constraint matrix.

• Empirical evaluation. We conduct numerical experi-
ments with different problem dimensions and number
of constraints. The results demonstrate the signiﬁcant
advantages of the SFP models compared to the original
FP.

2. Background

Mixed Integer Programming. In this work, we aim at
ﬁnding a feasible solution of a generic Mixed Integer Pro-
gramming (MIP) problem. A generic MIP problem can be
written as follows:

min cT x
s.t. Ax ≤ b

xi ∈ Z, ∀i ∈ S

(1a)

(1b)

(1c)

where A ∈ Rm×n, c, b ∈ Rn, and x ∈ Rn denote the
decision variables. A subset of the decision variables is

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

integral, and I denotes the index set of the integer variables.
When I = {1, 2, ..., n}, it becomes an IP problem. The
MIP problem aims to minimize a linear objective function
(1c) while satisfying a set of linear constraints (1b) and the
integral constraints (1c). A feasible solution is an element in
the feasible region P = {x ∈ R|Ax ≤ b, xi ∈ Z, ∀i ∈ S}.
To avoid triviality, we focus on the case when P is bounded
and excludes the origin.

Solving an MIP problem is generally known to be NP-
hard. Even well-developed commercial solvers (for in-
stance, Cplex, Gurobi, and Mosek) may struggle or fail.
Many successful heuristics for solving MIPs, such as local
branching (Fischetti and Lodi, 2003) and RINS (Danna et
al., 2005), only work with a known initial feasible solution.
It is of great importance to ﬁnd a good feasible solution for
MIPs. However, ﬁnding a feasible solution for a MIP is
also NP-hard, owing to the well-known equivalence of the
optimization problem and the feasibility problem in terms
of computational complexity.

Figure 1: Illustration of the Feasibility Pump

Feasibility Pump. The feasibility pump is one of the most
popular approaches for ﬁnding the initial feasible solution
of MIPs. The basic idea of the FP algorithm is to iteratively
ﬁnd and round a continuous relaxation solution for the MIP.
Let PR = {x ∈ R|Ax ≤ b} denote the continuous relax-
ation of P. The overall procedure is shown in Algorithm
1 and illustrated in Figure 1. The FP algorithm starts with
the rounded optimal continuous relaxation solution of the
MIP and then searches for the nearest points in the relaxed
feasible region. It continues perturbing and rounding the
new point found at each step until a feasible solution is dis-
covered or the limit of maximum number of steps is reached.
Despite being a powerful heuristic, it requires solving an
optimization problem within each iteration, which becomes
especially inefﬁcient when the problem size increases or
extends to nonlinear constraint cases. In addition, the FP
algorithm tends to converge to and loop between infeasible
points with non-binary integer variables (Fischetti et al.,
2005).

Algorithm 1 Feasibility Pump. With a slight abuse of nota-
tion, [x] denotes the rounding of x, i.e. [x]i = [xi], ∀i ∈ I
and [x]i = xi∀i /∈ I. The algorithm terminates if reaching
the maximum number of iterations with no feasible solution
found.
1: Initialization: x0 ← arg minx∈PR cT x; ¯x0 ← [x0];

k ← 0

xk+1 ← arg minx∈PR (cid:107)x − ¯xk(cid:107)
¯xk+1 ← [xk+1]
if ¯xk+1 = ¯xk then

2: while ¯xk is not feasible do
3:
4:
5:
6:
7:
8:
9:
10: end while
11: Return ¯xk

random perturbation of ¯xk

k ← k + 1

end if

else

j , ∀j ∈ I.

3. Smart Feasibility Pump: a RL Formulation

Here, we present our formulation of the SFP method as an
RL problem. We aim to learn an RL policy that can ﬁnd a
feasible solution for any randomly generated IP/MIPs with
the form of (1).

3.1. RL Formulation

In this section, we describe two slightly different formula-
tions for the SFP-MLP and the SFP-CNN. For each of the
formulations, we start by specifying the Markov decision
process. At each time step t, we denote the state as st ∈ S,
the action as at ∈ A, and the instant reward as rt ∈ R. A
policy π : S → P(A) provides a mapping from any state
to a distribution over actions π(·|st). Our goal is to learn
a policy that maximizes the expected cumulative rewards
over a horizon T , that is, maxπ J(π) := Eπ[(cid:80)T −1
t=0 γtrt],
where γ ∈ (0, 1] is a discount factor. In the remaining of
this section, we specify the state space S, action space A,
reward rt, and the transition from st to st+1.

State Space S for SFP-MLP. In the case of MIP, we let
the state at each time t to be st = (Flat(A), b, xt, ˜xt, I),
˜x denotes the projection of the current solution to the fea-
sible region deﬁned by constraints (1b). The idea of in-
cluding a projection ˜xt is borrowed from the original FP
algorithm, which provides useful information for the agent
about the direction to move to achieve feasibility. It im-
proves the learning ability of the agent yet with the cost
of large computational efforts at each time step. Moreover,
Flat(A) is a vector that equals the ﬂattened constraint ma-
trix A in the original problem (1a)-(1c), and I is a binary
vector that indicates whether the ith variable is an integer
(whether i ∈ I). For IPs, as we know that all decision
variables in (1a)-(1c) are integrals, we set the state vector as

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

st = (Flat(A), b, ˜x, x).

Policy network structure.

As explained later in Section 3.2, if we use our proposed
network structure with CNN, we can avoid calculating the
projection of xt at each time step t and instead use the initial
solution x0 as ˜xt for all time steps.

State Space S for SFP-CNN. When we adopt our proposed
CNN as the policy network, because of the ability of the
CNN to capture the structure of the constraint matrix, the
learning ability of the RL agent is brought up to a level
that we can get rid of the projection of xt at each time step
t. Instead, it only requires the projection to be performed
once for the initial point x0 and use it as xt for all t > 0.
Therefore, for MIPs, we deﬁned the state vector as st =
([A, b], xt, ˜x0, I), where [A, b] denotes the matrix deﬁned as
matrix A concatenated with column vector b. Similarly, for
IPs, we have st = ([A, b], xt, ˜x0).

Action Space and State Transition. In each time step t,
the agent draws an action at as a movement of the cur-
rent solution xt, that is, at ∈ A = Rn. Then, xt is
updated by rounding xt + at, that is, xt+1 = [xt + at]
and xt+1 is integral but not guaranteed to satisfy the
constraints (1b). The transition of state is deterministic
and st+1 = (Flat(A), b, ˜xt+1, xt+1, I) (for IP, st+1 =
(Flat(A), b, ˜xt+1, xt+1)), where ˜xt+1 is the projection of
xt+1 to the feasible region deﬁned by (1b).

Reward. As our goal is to ﬁnd a feasible solution, we use
the total violation of constraints (1b) as the reward, that is,
rt = −(cid:107)Axt − b(cid:107). As in each step, xt is guaranteed to be
integral, it is reasonable to only consider the violation of
constraints (1b) rather than constraints (1c).

3.2. Policy learning

In this part, we demonstrate our RL model. To train the
policy network, we use actor-critic algorithms with proximal
policy optimization (PPO) as proposed in (Schulman et al.,
2017). Moreover, two different policy network structures:
the classic MLP and a novel CNN are adopted for the policy
network of SFP-MLP and SFP-CNN, respectively.

Actor-Critic with PPO. For policy gradient methods, we
use PPO for actor-critic. PPO is a family of policy gradient
methods that use different surrogate objective functions
compared to standard policy gradient methods. In particular,
we use the clipped surrogate objective method that proposed
in (Schulman et al., 2017), that is, we use the following
objective function while training the policy network:

LCLIP (θ) = ˆEt[min(rt(θ) ˆAt, clip(rt(θ), 1−(cid:15), 1+(cid:15)) ˆAt)],
(2)
where ˆAt is an estimator of the advantage function at time
step t. We refer to (Schulman et al., 2017) for more details
about the PPO methods.

• SFP-MLP. As illustrated in Figure 2, in the SFP-MLP
model, the policy network is an MLP that takes the
state vector as input. The state vector consists of a ﬂat-
tened constraint matrix A, constraint vector b, current
solution xt, and its projection ˜xt, as well as the integral
indicator vector I.

• SFP-CNN. To better characterize the constraint ma-
trix (A, b) in IP/MIP deﬁned in (1), we use a CNN
to capture the policy πθ(a|s), instead of an MLP. The
network structure is illustrated in Figure 3. The in-
puts of the neural network include two parts, where
Constr Mat represents the constraint matrix [A, b]
and Current Sols represents the current solution x
and the initial solution x0. In the case of MIPs, there
is an additional input I that indicates the index of the
integral variables. Unlike using MLP to ﬁt the policy,
we keep using the initial solution (which is the rounded
optimal solution of a continuous relaxation) instead
of the projection of the current solution. Therefore,
there is no need to calculate the projection at each time
step, which makes our method more computationally
efﬁcient compared to using MLP for policy ﬁtting.

Figure 2: Policy Network Structure of SFP-MLP.

Figure 3: Policy Network Structure of SFP-CNN.

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

(a) 5-dim, 6 constraints

(b) 7-dim, 9 constraints

(c) 9-dim, 18 constraints

Figure 4: Comparison of SFP-MLP and SFP-CNN. In each of the subﬁgures (a)–(c), we demonstrate the performance comparison of
SFP-MLP and SFP-CNN in the average number of steps (denoted by EpLenMean) and the standard deviation of the number of steps
(denoted by EpLenStd) in each iteration. Note that the agent stops if a feasible solution is found or if it reaches the maximum number of
steps (100). Therefore, in our setting, it is not always better to have a lower variance in the number of steps. Since the agent interacts with
the environment for at most 100 steps, a poorly performed model might have EpLenStd = 0 if it fails to ﬁnd any feasible solution. An
ideal model has both a low EpLenMean and a low EpLenStd. A model is dominated by another model if it has a higher EpLenMean and a
lower EpLenStd.

4. Evaluation

4.1. Experiment Design

Simulator Design and Experiment Setting. We randomly
generate a set of IP and MIP problem instances of form
(1) based on the parameters listed in Table 1. The prob-
lem instances are small, moderate and large in terms of the
dimension of the decision variable n and the number of con-
straints m, including a 5-dimensional decision variable with
six constraints; 7-dimensional decision variable with nine
constraints; and a 9-dimensional decision variable with 18
constraints. We implemented a simulation environment for
the SFP agent. In each training iteration, the agent interacts
with one of the generated problems starting at the optimal
continuous relaxation. For each problem size, the agent is
trained for 50 iterations. The maximum number of train-
ing steps is 100. The agent either ﬁnds a feasible solution
within 100 steps or stops at the 100-th step. Hence, step
< 100 means that a feasible solution has been found, and
step = 100 means that the agent has failed for this sample.
The same problem instances are tested on the original FP
algorithm for comparison.

Performance Evaluation Metric. The reward per iteration
measures that the violation of the constraints is not a direct
metric of the model performance in the context of our appli-

Table 1: Problem Parameters

Parameter Distribution
Aij
b

randint[-10,10]
Aξ + (cid:15), where ξj ∼ randint[1,10] and
(cid:15)j ∼ randint[1,10] for all j = 1, ..., n
randint[0,1]

I

cation since our primary goal is to ﬁnd a feasible solution
within a small number of steps. We consider the number of
steps to reach a feasible solution as our performance evalua-
tion metric instead of the reward per iteration. In particular,
we evaluated the empirical mean and standard deviation of
the number of steps, which we denote as EpLenMean and
EpLenStd, respectively. Note that in this setting, it is not al-
ways better to have a lower variance of the number of steps.
Because our agent interacts with the environment for at most
100 steps, a poorly performed model may have EpLenMean
= 100 but EpLenStd = 0 if it fails to ﬁnd any feasible so-
lution. An ideal model has both a low EpLenMean and a
low EpLenStd. A model outperforms model with higher
EpLenMean and lower EpLenStd because the latter one is
stable with more steps to reach a feasible solution.

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

Table 2: Comparison with Feasibility Pump (IP).

n = 5, m = 6
MLP
15.0
34.6
100.0
100.0
1.0

CNN
11.1
29.0
100.0
29.5
1.0

FP
43.2
48.6
100.0
100.0
1.0

n = 7, m = 9
MLP
32.8
46.4
100.0
100.0
1.0

CNN
28.2
44.2
100.0
100.0
1.0

FP
65.5
46.8
100.0
100.0
1.0

n = 9, m = 18
MLP
90.3
30.9
100.0
100.0
51.5

CNN
54.0
49.6
100.0
100.0
1.0

FP
90.0
29.7
100.0
100.0
61.2

Table 3: Comparison with Feasibility Pump (MIP)

n = 5, m = 6
MLP
13.1
32.5
100.0
100.0
1.0

CNN
12.7
31.3
100.0
100.0
1.0

FP
31.9
45.5
100.0
100.0
1.0

n = 7, m = 9
MLP
25.0
42.6
100.0
100.0
1.0

CNN
27.2
42.6
100.0
100.0
1.0

FP
57.0
48.2
100.0
100.0
1.0

n = 9, m = 18
MLP
70.5
46.0
100.0
100.0
1.0

CNN
60.8
48.3
100.0
100.0
1.0

FP
82.7
36.8
100.0
100.0
2.0

EpLenMean
EpLenStd
EpLenMax
90 Quant
10 Quant

EpLenMean
EpLenStd
EpLenMax
90 Quant
10 Quant

4.2. Empirical Results

Experiment 1: Comparison with FP In this section, we
compare our proposed SFP agents with the classic FP algo-
rithm (Algorithm 1. The results are summarized in Table
2 for the IP problems and Table 3 for the MIP problems.
EpLenMax, 90 Quant and 10 Quant denote the max-
imum, 90 quantile and 10 quantile of the number of steps,
respectively. The result shows that the SFP agent ﬁnds a
feasible solution to IP/MIPs faster than the FP algorithm. In
addition, SFP-CNN dominates the FP algorithm and SFP-
MLP, especially when the problem size increases. Therefore,
the SFP agent is much more computationally efﬁcient than
the classic FP algorithm.

Experiment 2: MLP and CNN. In this section, we present
the empirical evaluation results for the SFP-MLP and SFP-
CNN methods. Figure 4 shows the training curves of SFP-
MLP and SFP-CNN with different problem scales. The
results show that SFP-MLP and SFP-CNN are comparable
when the problem size is small (Figure 4a). Both EpLen-
Mean and EpLenStd decreases when the training iteration
increases, which is how an ideal model would perform. SFP-
CNN outperforms SFP-MLP in the sense that it converges
faster to a lower EpLenMean with comparable EpLenStd
when the problem size becomes larger (Figures 4b and 4c).
Figure 4c provides an additional comparison of the SFP-
MLP without the projection of the previous point. Inter-
estingly, we observe that the performance of SFP-MLP is
largely dampened without the projection information, while
the performance of SFP-CNN without projection is better
than that of SFP-MLP with projection. Thus, SFP-CNN can
be more computationally efﬁcient than SFP-MLP with larger
problem scales. This also highlights the representational

power of the CNN structure to capture hidden information
in the constraint matrices.

In summary, the SFP agents ﬁnd a feasible solution for
IP/MIPs efﬁciently and outperforms the classic FP algo-
rithm. SFP-CNN dominates the classic FP algorithm and
SFP-MLP. In addition, SFP-CNN works without the projec-
tion of the current point. As noted in Section 2, it requires
solving an optimization problem at each step to obtain the
projection. Therefore, SFP-CNN achieves a signiﬁcant im-
provement over the original FP method. This highlights the
representational power of our proposed CNN structure to
capture hidden information in the constraint matrix.

5. Conclusion

In this work, we propose an SFP, a reinforcement learning-
based model for ﬁnding feasible solutions to generic IP/MIP
problems. Unlike the supervised learning scheme, our
method does not require the knowledge of a training set
that includes feasible solutions. Our model is constructed
based on the spirit of a well-known heuristic, the FP. Numer-
ical experiments on different problem scales show that our
proposed SFP agents can efﬁciently ﬁnd a feasible solution
for general IP/MIPs and improve the performance compared
to the original FP algorithm.

To capture the inherent structure of the constraint matrix,
we propose a novel CNN structure for the policy network in
addition to the MLP policy network. SFP-CNN outperforms
SFP-MLP and is more computationally efﬁcient because it
does not require ﬁnding the projection of the current point at
each step. The CNN structure exhibits great representation
power for optimization problems.

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

We suggest several opportunities for future research in this
direction. One interesting topic would be to leverage the
CNN structure, which can transfer useful information be-
tween different problem settings with different numbers of
constraints. The other potential direction would be to take
the objective value into account and trying to ﬁnd a feasible
solution with better quality.

References

Tobias Achterberg and Timo Berthold. Improving the feasi-
bility pump. Discrete Optimization, 4(1):77–86, 2007.

Daniel Baena and Jordi Castro. Using the analytic center
in the feasibility pump. Operations Research Letters,
39(5):310–317, 2011.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi,
and Samy Bengio. Neural combinatorial optimiza-
arXiv preprint
tion with reinforcement
arXiv:1611.09940, 2016.

learning.

Livio Bertacco, Matteo Fischetti, and Andrea Lodi. A feasi-
bility pump heuristic for general mixed-integer problems.
Discrete Optimization, 4(1):63–76, 2007.

Matteo Fischetti and Jason Jo. Deep neural networks as 0-1
mixed integer linear programs: A feasibility study. arXiv
preprint arXiv:1712.06174, 2017.

Matteo Fischetti and Andrea Lodi. Local branching. Mathe-

matical programming, 98(1-3):23–47, 2003.

Matteo Fischetti and Domenico Salvagnin. Feasibility
pump 2.0. Mathematical Programming Computation,
1(2-3):201–222, 2009.

Matteo Fischetti, Fred Glover, and Andrea Lodi. The feasi-
bility pump. Mathematical Programming, 104(1):91–104,
2005.

He He, Hal Daume III, and Jason M Eisner. Learning to
search in branch and bound algorithms. Advances in
neural information processing systems, 27:3293–3301,
2014.

Elias Boutros Khalil, Pierre Le Bodic, Le Song, George L
Nemhauser, and Bistra N Dilkina. Learning to branch in
mixed integer programming. In AAAI, pages 724–731,
2016.

Natashia L Boland, Andrew C Eberhard, F Engineer, and
Angelos Tsoukalas. A new approach to the feasibility
pump in mixed integer programming. SIAM Journal on
Optimization, 22(3):831–861, 2012.

Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and
Le Song. Learning combinatorial optimization algorithms
over graphs. In Advances in neural information process-
ing systems, pages 6348–6358, 2017.

Natashia L Boland, Andrew C Eberhard, Faramroze G En-
gineer, Matteo Fischetti, Martin WP Savelsbergh, and
Angelos Tsoukalas. Boosting the feasibility pump. Mathe-
matical Programming Computation, 6(3):255–279, 2014.

Pierre Bonami and Jo˜ao PM Gonc¸alves. Heuristics for
convex mixed integer nonlinear programs. Computational
Optimization and Applications, 51(2):729–747, 2012.

Pierre Bonami, G´erard Cornu´ejols, Andrea Lodi, and
Franc¸ois Margot. A feasibility pump for mixed inte-
ger nonlinear programs. Mathematical Programming,
119(2):331–352, 2009.

Emilie Danna, Edward Rothberg, and Claude Le Pape. Ex-
ploring relaxation induced neighborhoods to improve mip
solutions. Mathematical Programming, 102(1):71–90,
2005.

George Dantzig, Ray Fulkerson, and Selmer Johnson. Solu-
tion of a large-scale traveling-salesman problem. Journal
of the operations research society of America, 2(4):393–
410, 1954.

Ailsa H Land and Alison G Doig. An automatic method for
solving discrete programming problems. In 50 Years of In-
teger Programming 1958-2008, pages 105–132. Springer,
2010.

Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinato-
rial optimization with graph convolutional networks and
guided tree search. In Advances in Neural Information
Processing Systems, pages 539–548, 2018.

Chryssi Malandraki and Mark S Daskin. Time dependent
vehicle routing problems: Formulations, properties and
heuristic algorithms. Transportation science, 26(3):185–
200, 1992.

Vinod Nair, Sergey Bartunov, Felix Gimeno,

Ingrid
von Glehn, Pawel Lichocki,
Ivan Lobov, Brendan
O’Donoghue, Nicolas Sonnerat, Christian Tjandraat-
madja, Pengming Wang, et al. Solving mixed inte-
ger programs using neural networks. arXiv preprint
arXiv:2012.13349, 2020.

Arthur Delarue, Ross Anderson, and Christian Tjandraat-
madja. Reinforcement learning with combinatorial ac-
tions: An application to vehicle routing. arXiv preprint
arXiv:2010.12001, 2020.

MohammadReza Nazari, Afshin Oroojlooy, Lawrence Sny-
der, and Martin Takac. Reinforcement learning for solv-
ing the vehicle routing problem. In Advances in Neural
Information Processing Systems, pages 9860–9870, 2018.

Reinforcement Learning for (Mixed) Integer Programming: Smart Feasibility Pump

Manfred W Padberg. On the facial structure of set packing
polyhedra. Mathematical programming, 5(1):199–215,
1973.

Yves Pochet and Laurence A Wolsey. Production plan-
ning by mixed integer programming. Springer Science &
Business Media, 2006.

Tadeusz Sawik. Scheduling in supply chains using mixed

integer programming. Wiley Online Library, 2011.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.

Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforce-
ment learning for integer programming: Learning to cut.
In International Conference on Machine Learning, pages
9367–9376. PMLR, 2020.

