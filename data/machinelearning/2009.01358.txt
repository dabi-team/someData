0
2
0
2

p
e
S
2

]

G
L
.
s
c
[

1
v
8
5
3
1
0
.
9
0
0
2
:
v
i
X
r
a

Change Point Detection by Cross-Entropy
Maximization

Aur´elien Serre

Didier Ch´etelat

Andrea Lodi

CERC, Polytechnique Montr´eal
{aurelien.serre, didier.chetelat,
andrea.lodi}@polymtl.ca

September 4, 2020

Abstract

Many oﬄine unsupervised change point detection algorithms rely on
minimizing a penalized sum of segment-wise costs. We extend this frame-
work by proposing to minimize a sum of discrepancies between segments.
In particular, we propose to select the change points so as to maximize
the cross-entropy between successive segments, balanced by a penalty for
introducing new change points. We propose a dynamic programming al-
gorithm to solve this problem and analyze its complexity. Experiments
on two challenging datasets demonstrate the advantages of our method
compared to three state-of-the-art approaches.

1 Introduction

Change point detection is the problem of dividing a time series into statisti-
cally homogeneous segments. It has numerous applications in ﬁelds as varied
as manufacturing [Page, 1954], speech processing [Chowdhury et al., 2012], cli-
mate science [Reeves et al., 2007], bioinformatics [Oudre et al., 2011; Haynes
et al., 2017b; Schr¨oder and Ombao, 2019; Bosc et al., 2003; Hocking et al., 2013;
Keogh et al., 2001] and ﬁnance [Killick et al., 2012b]. The problem can be of-
ﬂine, where segmentation can be performed after seeing all the data, or online,
where the time series must be segmented as data streams in. The problem can
also be unsupervised or supervised, depending on whether expert segmentations
are available for training. In this article we focus on the oﬄine unsupervised
problem.

Oﬄine unsupervised change point detection is usually based on statistical
testing or ﬁtting a piecewise model. As there is a wide variety of approaches,
it is diﬃcult to give a general summary. However, importantly, most methods
can be reinterpreted as ﬁnding an optimal segmentation T with change points

1

 
 
 
 
 
 
1 = τ0 < τ1 < · · · < τm < τm+1 = T of a time-series x = x1:T by solving an
optimization problem

min
T

m+1

Xi=1

c(xτi−1:τi) + β pen(T ),

(1)

where c(x) is a cost function on each segment and pen(T ) is a segmentation-
speciﬁc penalty [Truong et al., 2019]. The costs usually come from negative
log-likelihoods (for piecewise models) or test statistics (in hypothesis testing).
An advantage of problems of the form (1) is that algorithms have been
developed that can solve them for any cost function, exactly or approximately,
and very eﬃciently. Moreover, these algorithms tend to scale well: for modern
large, high-dimensional time series, they are often the only realistic option.
Nevertheless, the framework is also limiting: for example, when costs represent
negative log-likelihoods, we are ﬁtting a piecewise model where segments are
statistically independent, an assumption that is often untenable. Ideally, one
would like to retain the computational advantages of (1) while moving towards
more expressive (but less scalable) methods that do not solve (1), such as hidden
semi-Markov models [Baum and Petrie, 1966]. A simple option would be to
consider a generalized sum-of-costs problem, say

min
T

m

Xi=1

c(xτi−1:τi, xτi:τi+1) + β pen(T ),

(2)

where c(x, y) would now be a cost function depending on pairs of consecutive
segments. This would increase the expressive power of the methods while re-
maining eﬃciently solvable by appropriate modiﬁcations of standard algorithms.
Unfortunately, it does not appear obvious how to automatically generalize stan-
dard single-segment costs to this case.

In this work, we propose that in the common case of negative log-likelihood
costs, the negative empirical cross-entropy c(x, y) = nce(y || x) could be an ap-
propriate generalization. We combine it with standard choices: we penalize the
number of change points |T |, select the penalty hyperparameter β by a BIC
criterion, and adapt Jackson et al. [2005]’s Optimal Partitioning algorithm to
the pairwise cost case, a variant we call OTAWA. Results from the experimen-
tal section show that this leads to improvements of performance on two diﬃcult
real-world, labeled datasets against three standard methods solving (1): Opti-
mal Partitioning, Window Sliding [Truong et al., 2019, Algorithm 3] and Binary
Segmentation [Scott and Knott, 1974].

The OTAWA algorithm we develop in this work is in fact completely general
and can solve exactly any problem of the form (2) with pen(T ) = |T |. Thus
the approach is in principle not restricted to cross-entropy costs, if alternatives
could be derived in future work (e.g. analogues of test statistic costs).

This article is divided as follows. Section 2 summarizes the literature leading
to this work. Section 3 details the OTAWA algorithm we propose. Section 4
analyses the computational complexity of the resulting algorithm. Section 5

2

proposes constraints that can be added to the segmentation search so as to
reduce computational cost. Finally, Section 6 details the experimental results.

2 Previous Work

Many excellent surveys of oﬄine unsupervised change point detection have been
published recently [Tartakovsky et al., 2014; Aminikhanghahi and Cook, 2017;
Truong et al., 2019]. We thus only give a brief overview of methods solving
Equation (1), which includes most methods in the literature. We can charac-
terize them as a combination of a segment cost function, a penalty function for
controlling the number of change points and an algorithm to solve the optimiza-
tion problem.

2.1 Cost function

The costs used in the literature are often negative log-likelihoods of time se-
ries models. The earliest example of such a choice is the work of Page [1954],
which assumes a normal distribution with ﬁxed variance, with corresponding
2. Since then, many other models have
cost c(xτi:τi+1) =
been proposed, such as i.i.d. Poisson [Ko et al., 2015] and autoregressive models
[Chakar et al., 2017]. Alternatively, methods based on hypothesis testing are
also popular, such as Zou et al. [2014], where the costs derive from the test
statistic. These methods are often non-parametric.

τi+1
t=τikxt − ¯xτi:τi+1k2

P

2.2 Penalty

Directly minimizing the sum of segment costs generally leads to overﬁtting,
meaning that the number of change points tends to be overestimated [Haynes
et al., 2017a]. A penalty term pen(T ) helps avoid overﬁtting by penalizing
segmentations with too many change points. A very common choice is a penalty
term linear in the number of change points, of the form pen(T ) = |T | [Killick
et al., 2012a].

2.3 Optimization Algorithm

In oﬄine unsupervised change point detection, the problem (1) is often solved
to optimality. Many approaches are based on dynamic programming, such as
the Optimal Partitioning (OP) algorithm proposed by Jackson et al. [2005]
for penalty terms linear in the number of change points pen(T ) = |T |, or the
Segment Neighborhood (SN) algorithm proposed by Auger and Lawrence [1989].
The complexity of these dynamic programming algorithms can be improved
under certain conditions by using pruning rules, such as in the work of Haynes
et al. [2017a], Rigaill [2015] and Maidstone et al. [2017].

In cases where segmentations need to be computed in a very short amount
of time, approximate algorithms have also been developed that converge faster

3

at the expense of accuracy. Binary Segmentation [Scott and Knott, 1974] is
an example of such an algorithm.
It sequentially adds a single change point
to the current segmentation greedily until a stopping criterion is met. This
approximate algorithm is usually faster than its exact counterpart because it
only requires estimating the location of a single change point at a time, which is a
much simpler problem than the global multiple change point detection problem.
A related algorithm, Bottom Up [Keogh et al., 2001], works similarly but starts
with many candidate change points and then sequentially removes them greedily.
Finally, the Window Sliding algorithm [Truong et al., 2019, Algorithm 3] is
another popular approximate algorithm for change point detection. It computes
the score

d(t) = c(xt−L:t+L) − [c(xt−L:t) + c(xt:t+L)]

(3)

for every time-index t such that L ≤ t ≤ T − L, which aims to capture the
discrepancy between the statistical properties of two adjacent windows of length
L located on each side of t. The locations of the change points is then obtained
using a peak detection algorithm on those scores.

3 Methodology

We propose to extend upon previous work by solving problem (2) with the em-
pirical cross-entropy as cost measure, the number of change points as penalty, a
dynamic programming algorithm as exact solving method and the BIC criterion
for selecting the penalty hyperparameter. We detail each choice in turn in this
section.

3.1 Cost Function

The cross-entropy is a measure of discrepancy between two distributions with
densities fx and fy, and is deﬁned by

CE(fy k fx) = −

Z

fy(t) log fx(t) dt.

We propose to use the negative of the empirical analogue of this measure as cost
function. Namely, given two successive segments xτi−1:τi and xτi:τi+1, one can ﬁt
a statistical time series model fθ, such as an i.i.d. Gaussian or an autoregressive
model, by maximum likelihood on the prior segment xτi−1:τi. Denote by ˆθi,ML
the resulting maximum likelihood estimate of the parameters of the model ﬁtted
on the ith segment: we will from now on refer to the f ˆθi,ML as the “segment
models”. Then our cost functions are the average log-likelihood of the prior
segment model on the subsequent segment,

c(xτi−1:τi, xτi:τi+1) = nce(xτi−1:τi, xτi:τi+1)
τi+1

≡

1
τi+1 − τi

log f ˆθi,ML

xj |xτi:j
(cid:0)

.
(cid:1)

Xj=τi

(4)

4

Minimizing this measure has then the eﬀect of looking for points τi such that
the cross-entropy of the distribution of xτi−1:τi and xτi:τi+1 is maximized, that
is, such that the distributions are as diﬀerent as possible. Those are presumably
points where there are abrupt changes in the underlying statistical distributions.

3.2 Penalty

Fitting the optimization problem (2) without any penalty will tend to produce
solutions that vastly overestimate the number of change points: that is, we need
to prevent overﬁtting. We follow the classical choice

pen(T ) = |T |,

(5)

the number of change points.

3.3 Optimization Algorithm

With the choices of c(·, ·) and pen(·) as in Equations (4)– (5), the optimization
problem (2) becomes

min
T

V (T , x) ≡

m

Xi=1

nce(xτi−1:τi, xτi:τi+1) + β|T |.

(6)

We propose to solve this problem exactly using dynamic programming. This is
possible because we can regard it as an optimal control problem, where we must
successively decide on the location of each change point τi+1 in order, incurring
the cost c(xi−1:i, xi:i+1) + β after each action.

From this point of view we can derive an algorithm, which we call Optimal
Two Adjacent Windows Algorithm (OTAWA), that will eﬃciently ﬁnd the op-
timal solution to optimization problem (6) for a given hyperparameter β. A
pseudocode description is given as Algorithm 1. A proof of correctness as well
as a complexity analysis will be given in Section 4.

3.4 Hyperparameter Selection

OTAWA will solve optimization problem (6) for a given penalty hyperparam-
eter β, which controls the ﬁnal number of change points. Since we are in an
unsupervised context, however, it is not obvious how to select the number of
change points. We propose a model selection procedure based on the following
argument.

We can regard solving our procedure as ﬁtting a piecewise model on the time
series x. Namely, each segment is modeled by its segment model f ˆθi,ML ﬁtted by
maximum likelihood, while the change points are ﬁtted to maximize the cross-
entropy between the successive segments.
If the segmentation is reasonable,
then this piecewise model should generalize well, in the sense that given a new
time series realization y from the same distribution as our training time series x,

5

Algorithm 1 OTAWA
Input: Time-series x1:T , penalty hyperparameter β
Declare G a real-valued (T − 1) × (T − 1) array
Declare S a set-valued (T − 1) × (T − 1) array
for u = 2, . . . , T − 1 do

Init G[1, u] = 0
Init S[1, u] = {1, u}

end for
for s = 2, . . . , T − 1 do

for r = 1, . . . , s − 1 do
Fit model fθ on xr:s,

end for

end for
for s = 2, . . . , T − 1 do

for t = s + 1, . . . , T do

for r = 1, . . . , s − 1 do

Compute c(xr:s, xs:t) following (4)

end for
r∗ = arg min1≤r<s{G[r, s] + c(xr:s, xs:t) − β}
G[s, t] = G[r∗, s] + c(xr∗:s, xs:t) − β
S[s, t] = S[r∗, s] ∪ t

end for

end for
t∗ = arg min1<t<T G[t, T ]

Output: T = S[t∗, T ]

our piecewise model ﬁtted on x should give high likelihood to y. So a reasonable
criterion for selecting β is to choose it so as to minimize generalization error.

A classic procedure for selecting hyperparameters that minimize generaliza-
tion error is the Bayesian Information Criterion (BIC) [Schwarz, 1978]. Let
T ∗(β) be an optimal segmentation for the optimization problem (6) with hy-
perparameter β. The BIC of the corresponding piecewise model is

BIC(β) = − 2

m

τ ∗
i

Xi=1

Xj=τ ∗
i−1

log f ˆθi,ML(xj |xτ ∗

i−1:j−1)

+ log(T )

m

Xi=1

pi

where pi is the number of parameters of the segment model f ˆθi,ML . For ex-
ample, univariate i.i.d. Gaussian models have two parameters and univariate
autoregressive models of order p have p parameters.

One approach to minimize the BIC is to compute it on a grid of β and se-
lecting the β leading to the smallest BIC. Alternatively, one can use the CROPS
algorithm [Haynes et al., 2017a] to eﬃciently ﬁnd the best segmentation for all
β’s in a desired range [βmin, βmax]. If we denote by mmin and mmax the number

6

of change points of the optimal segmentation corresponding to βmin and βmax,
then CROPS will ﬁnd the optimal segmentation in the range in worst-case time
O(mmin − mmax).

4 Analysis

In this section we perform a theoretical analysis of our proposed change point
detection algorithm. We ﬁrst show that OTAWA indeed solves the desired
optimization problem.

Proposition 1. Let T (x, β) be the output of OTAWA for a given time series
x and penalty hyperparameter β. Then T (x, β) solves optimization problem (6)
exactly.

Proof. For any integers 1 < i < j, let Segment(i, j) = {(τ0, . . . , τm, i, j) | 0 ≤
m < i, 1 = τ0 < τ1 < · · · < τm < i} the set of all segmentations of x1:j whose
second-to-last change point is i, and let

T ∗
i,j ∈

V

arg min
T ∈Segment(i,j)
T ∗
i,j, x1:j
(cid:0)

T , x1:j
(cid:0)
.
(cid:1)

,
(cid:1)

G(i, j) = V

Moreover, for any 1 < j let G(1, j) = 0. Now take any integers 1 < s < t and
write T ∗

s,t = (τ0, . . . , τm, s, t). If m = 0, then

G(s, t) = c(x1:s, xs:t) + β = G(1, s) + c(x1:s, xs:t) + β

≥ min
1≤r<s

{G(r, s) + c(xr:s, xs:t) + β}

(7)

as G(1, s) = 0. Otherwise, if m ≥ 1, let T = (τ0, . . . , τm, s) ∈ Segment(τm, s).
Then for any other S = (σ0, . . . , σn, τm, s) ∈ Segment(τm, s) we have

V (T , x1:s) + c(xτm:s, xs:t) + β = V (T ∗

s,t, x1:t)

≤ V
(σ0, . . . , σn, τm, s, t), x1:t
(cid:0)
= V (S, x1:s) + c(xτm:s, xs:t) + β,

(cid:1)

so V (T , x1:s) ≤ V (S, x1:s) for any S ∈ Segment(τm, s), that is, G(τm, s) =
V (T , x1:s). Thus

G(s, t) = G(τm, s) + c(xτm:s, xs:t) + β

≥ min
1≤r<s

{G(r, s) + c(xr:s, xs:t) + β}.

On the other hand, take any integers 1 ≤ r < s. If r = 1, then

G(1, s) + c(x1:s, xs:t) + β = c(x1:s, xs:t) + β

= V

(1, s, t), x1:t
(cid:0)

(cid:1)

≥ G(s, t)

7

(8)

(9)

since G(1, s) = 0. Else if r > 1, take any T ∗
and write it as T ∗

r,s = (τ0, . . . , τm, r, s). Then observe that

r,s ∈ arg minT ∈Segment(r,s)V

G(r, s) + c(xr:s, xs:t) + β

= V

(τ0, . . . , τm, r, s, t), x1:t
(cid:0)

(cid:1)

≥ G(s, t).

T , x1:s
(cid:0)

(cid:1)

(10)

Thus by combining the cases of Equations (9)–(10) and taking a minimum we
ﬁnd

min
1≤r<s

{G(r, s) + c(xr:s, xs:t) + β} ≥ G(s, t).

(11)

Then combining Equation (11) with Equations (7)–(8) yields that for any 1 <
s < t,

G(s, t) = min
1≤r<s

{G(r, s) + c(xr:s, xs:t) + β},

(12)

a Bellman-type equation. But this means we can compute T ∗
i,j and G(i, j)
recursively from Equation (12), starting from the initial condition G(1, t) = 0,
and once those are computed the optimal segmentation can be found by T ∗ =
arg minT ∗

i,T , x1:T ). This is Algorithm 1.

i,T V (T ∗

Next, we show that OTAWA has at least quartic complexity in the total

number of timesteps of the time series.

Proposition 2. Let h(t) denote the worst-case time complexity of ﬁtting a
segment model fθ on a time series of t timesteps, and assume h is monotone
increasing. Then OTAWA has O(T 2h(T ) + T 4) worst-case time complexity.

Proof. The initialization loop takes O(T ) iterations, with inner operations of
constant time complexity. The model ﬁtting loop takes O(T 2) iterations, and the
model ﬁtting itself takes at worst h(T ) time. Computation of a cost c(xr:s, xs:t)
following Equation (4) takes O(T ) worst-case time, and there are O(T 3) costs
to be computed. Computation of r∗ takes O(T ) worst-case time while the other
operations in the loop are constant, and there are O(T 2) iterations. Finally,
computation of the ﬁnal argmin takes O(T ) time. This yields a total worst-case
time complexity of O(T 2h(T ) + T 4).

Since in practice one must ﬁnd a good β, the complexity is actually higher.
For example, using the CROPS algorithm as mentioned in Section 3.4 to search
for a segmentation with no more than M change points lead to an overall
O(M T 2h(T ) + M T 4) complexity.

5 Constrained Segmentations

For many time series and choices of local models, optimization problem (6) can
be solved exactly, but for other large time series this is prohibitive. A typical

8

solution is to restrict ourselves to searching among a subset of segmentations,
since often one has a certain tolerance as to where a change point might lie.
For example, one can impose a minimal distance between consecutive change
points, or require that change points lie on a grid, e.g. as implemented in the
change point detection library ruptures [Truong et al., 2018].

Formally, the ﬁrst constraint imposes that τi+1 − τi ≥ S ∀i = 0, . . . , m for
some S ∈ N. This is reasonable as knowledge regarding how close changes can
be is often available, and also as a minimal amount of observations is required
within each segment in order for the local models to be trained eﬃciently, hence
for the cross-entropy estimate to be reasonable. The second constraint imposes
that τi ∈ R N ∀i = 0, . . . , m for some R ∈ N. This is reasonable again when
change points must be found only within a certain precision.

Many change point detection algorithms can be accommodated to search
only for segmentations satisfying these constraints, and OTAWA is no exception.
Indeed, in our case, all that is needed is to restrict the range of loops over r, s,
and t in Algorithm 1 appropriately. In such a case, following the same reasoning
as in Proposition 2 yields that the resulting worst-case time complexity of the
. In practice the
algorithm is reduced to O
(cid:1)
computational gains are signiﬁcant.

T (T − S)h(T )/R2 + T 2(T − S)2/R3

(cid:0)

6 Experiments

In this section, the performance of OTAWA is compared to three other methods
from the literature on two real-world datasets.

6.1 Setup

We compare OTAWA against the Optimal Partitioning (OP) algorithm [Jack-
son et al., 2005], a state-of-the-art exact method solving optimization problem
(1). We use as single-segment cost the negative log-likelihood of a model ﬁt-
ted on the segment, and we use the number of change points as penalty. The
optimization problem is solved using the PELT [Killick et al., 2012a] implemen-
tation in the ruptures library [Truong et al., 2018]. We also compare against
two approximate algorithms : the Window Sliding (WS) algorithm detailed in
Truong et al. [2019, Algorithm 3], and the Binary Segmentation (BS) algorithm
[Scott and Knott, 1974], implemented in ruptures. Both algorithms are based
on the same single-segment cost as OP.

In order to evaluate the performance of the diﬀerent methods, we use two
real-world datasets for which the positions of the change points have been la-
beled manually. The four methods that we compare are unsupervised. The
knowledge of the positions of the change points is only used a posteriori in or-
der to evaluate the accuracy of the estimated segmentations. All four methods
also rely on segment-wise statistical models, and for a given dataset the same
model is used with all methods, for comparison purposes.

9

Time domain

0

0

0 : 0

0 : 0

0

0

1

1

0 : 0

0 : 0

0

0

2

2

0 : 0

0 : 0

0

0

3

0 : 0

4

0 : 0

0
0
Frequency domain

0

0 : 0

3

0 : 0

0

4

0 : 0

0

0 : 0

0

5

5

6

6

0 : 0

0 : 0

0

0

Figure 1: The Human Activities dataset. Top: acceleration measurements
across time. Bottom: same signal after a STFT, only 3 of the 23 variables
are displayed for visibility.

b

b

T , T ∗) = |m∗ −

T ) and Precision(T ∗,

m and the true number m∗, AnnotationError(

We evaluate performance using the following ﬁve metrics. The Annota-
tion Error is the absolute diﬀerence between the estimated number of change
points
m|.
F1-Score(T ∗,
T ) are the F1-score and precision de-
b
b
rived from deﬁning a detection radius r > 0, and considering a true change
point as detected if a change point has been estimated within r time-indices
of its location. Hausdorff(T ∗,
T ) is the Hausdorﬀ distance between the sub-
sets of {1, . . . , T } corresponding to the true segmentation T ∗ and the estimated
segmentation
T , seen as sets of integers. The Mean Distance metric is the
mean over every true change points of the distance to the closest estimated
change point, MeanDistance(T ∗,
T |ˆt − t∗|. Finally,
T ) = 1
|T ∗|
P
RandIndex(T ∗,
T ), initially introduced by Rand [1971] for evaluating cluster-
ing methods, is the proportion of pairs of time-indices that are either in the
same segment according to both T ∗ and
T or in diﬀerent segments according
to both T ∗ and

t∗∈T ∗ minˆt∈

T .

b

b

b

b

b

b

b

As OP and BS come from the production-quality library rutpures while
our implementation of OTAWA and WS are proofs of concept, we do not report
running times. Nonetheless, even with our development implementations, all
algorithms took under 3 minutes to run on each dataset. OTAWA was the
slowest in both cases, as expected from the complexity analysis of Section 4.

b

6.2 Human Activities Dataset

The Human Activities dataset [Kawaguchi et al., 2011] contains measurements
acquired by a device ﬁxed on the waist of a person while performing diﬀerent

10

OTAWA

OP

WS

BS

OTAWA

OP

WS

BS

randindex

f1

precision

annotationerror

18

mean_dist

hausdorff

0.960

0.955

0.950

0.945

0.940

0.935

0.930

0.925

0.920

0.65

0.60

0.55

0.50

1.00

0.95

0.90

0.85

0.80

0.75

0.70

16

14

12

10

8

12

10

8

6

4

50

40

30

20

10

the RandIndex,
(a) Results
F1-Score and Precision metrics.
(Higher is better.)

for

for

the Annotation-
(b) Results
Error, MeanDistance and Haus-
dorff metrics. (Lower is better.)

Figure 2: Results for the OTAWA, Optimal Partitioning (OP), Window Slid-
ing (WS) and Binary Segmentation (BS) algorithms on the Human Activities
dataset. The center value and the error bars represent the mean and standard
deviation over the six time series of the dataset.

activities such as walking or going up a staircase. The task is to segment the time
series into the diﬀerent activities in a unsupervised manner, which is a change
point detection problem. Using accelerometer and gyroscope measurements
along the three spacial axes yields 6 real-valued time series.

We preprocess the data by applying a short-time Fourier transform (STFT)
to the time-series and clipping to the range [0.5Hz – 5Hz] similarly to Oudre
et al. [2011]. The STFT is performed using a window size of approximately 5s
(512samples), and an overlapping between windows of 75%. This yields a total
of 23 frequency bins, so that in total we have six 23-dimensional real-valued
time series with 308 timesteps each. We normalize each variable separately so
that its range lies in [0, 1] using min-max scaling.

Within one type of activity, it is reasonable to assume that the repetitive
motion is stationary, meaning that the spectral information is stationary as well
within each segment. For this reason, we assume that the observations in the
time-series are i.i.d. and follow a Gaussian distribution with constant unknown
variance, and piecewise constant mean.

With all four algorithms, we use a resolution parameter of R = 2 samples and
a minimum segment length of S = 8 samples. The Window Sliding algorithm
has its window length set to L = 10 samples. The F1-Score is computed with
a detection radius of r = 6.

6.2.1 Results

Figure 2 shows the mean and standard deviation of the performance obtained by
each of the four methods on the six time series. We can observe that OTAWA

11

OTAWA

OP

WS

BS

OTAWA

OP

WS

BS

randindex

f1

precision

annotationerror

mean_dist

0.92

0.90

0.88

0.86

0.84

0.82

0.80

0.35

0.30

0.25

0.20

0.15

0.300

0.275

0.250

0.225

0.200

0.175

0.150

0.125

5

4

3

2

1

0

70

60

50

40

30

20

hausdorff

250

225

200

175

150

125

100

75

50

the RandIndex,
(a) Results
F1-Score and Precision metrics.
(Higher is better.)

for

for

the Annotation-
(b) Results
Error, MeanDistance and Haus-
dorff metrics. (Lower is better.)

Figure 3: Results for the OTAWA, Optimal Partitioning (OP), Window Sliding
(WS) and Binary Segmentation (BS) algorithms on the Industrial Equipment
dataset.

achieves the best average performance among the four methods according to
every metric except for RandIndex.

6.3

Industrial Equipment Dataset

This dataset contains measurements of two sensors acquired on an industrial
equipment over a period of 716 days, to be made public upon publication of this
article. The task is to retrieve in an unsupervised manner the dates of main-
tenance events that occurred during the recorded period. The actual dates of
these events have been labeled manually, in order to evaluate the performance
of the four algorithms a posteriori. According to expert knowledge of the indus-
trial equipment, we should expect the behavior of the equipment to slowly drift
between maintenance events due to normal degradations.
In contrast, main-
tenance events are expected to correspond to abrupt changes of the behavior,
from one observation to the next. Thus, this task can be interpreted as a change
point detection problem.

Stationary models such as i.i.d. models are not suitable here, since they
would not be able to capture the drift, and we propose instead to use a vector
autoregressive (VAR) model. We train it with an L1 penalty to avoid overﬁtting.
The time-series initially contains 10399 observations, that we reduce down to
716 by sub-sampling at a rate of one observation per day. We also normalize
each variable between 0 and 1 using min-max scaling.

For all four methods, we use a VAR model of order p = 3, estimated with
L1 regularization parameter α = 10−2. We will also use a minimum spacing of
S = 50 samples between change points, and a resolution parameter of R = 5.
The Window Sliding algorithm is used with windows of size L = 20 samples.

12

The F1-Score and Precision metrics are computed with a detection radius
of r = 10.

6.3.1 Results

Figure 3 reports the performance achieved by the four algorithms on the time
series. As can be seen, OTAWA outperforms the other two methods according
to every metric except for AnnotationError.

7 Conclusion

In this article, we proposed an extension of a standard framework for oﬄine
unsupervised change point detection. We propose to select change points so
as to maximize the empirical cross-entropy between successive segments, while
balancing the introduction of new change points with a penalty on the number
of segments. We proposed a dynamic programming algorithm to solve this
problem exactly, as well as variants on a reduced search space, and detailed
experimental evidence of the improvements provided by our approach against
state-of-the-art methods on two challenging datasets.

Our approach can be regarded as a extension of the standard sum-of-costs
formulation to costs depending on pairs of segments, in the case where costs
are derived from negative log-likelihoods. A promising line of work would be
to generalize this approach to arbitrary cost functions, such as derived from
parametric and non-parametric hypothesis testing.

References

Aminikhanghahi, S. and Cook, D. J. (2017). A survey of methods for time series
change point detection. Knowledge and Information Systems, 51:339–367.

Auger, I. E. and Lawrence, C. E. (1989). Algorithms for the optimal identiﬁca-
tion of segment neighborhoods. Bulletin of Mathematical Biology, 51(1):39–
54.

Baum, L. E. and Petrie, T. (1966). Statistical inference for probabilistic func-
tions of ﬁnite state markov chains. The Annals of Mathematical Statistics,
37(6):1554–1563.

Bosc, M., Heitz, F., Armspach, J.-P., Namer, I., Gounot, D., and Rumbach, L.
(2003). Automatic change detection in multimodal serial MRI: application to
multiple sclerosis lesion evolution. NeuroImage, 20(2):643–656.

Chakar, S., Lebarbier, E., L´evy-Leduc, C., and Robin, S. (2017). A robust
approach for estimating change-points in the mean of an AR(1) process.
Bernouilli Society for Mathematical Statistics and Probability, 23(2):1408–
1447.

13

Chowdhury, M. F. R., Selouani, S.-A., and O’Shaughnessy, D. (2012). Bayesian
on-line spectral change point detection: a soft computing approach for on-line
ASR. International Journal of Speech Technology, 15(1):5–23.

Haynes, K., Eckley, I. A., and Fearnhead, P. (2017a). Computationally eﬃcient
changepoint detection for a range of penalties. Journal of Computational and
Graphical Statistics, 26(1):134–143.

Haynes, K., Fearnhead, P., and Eckley, I. A. (2017b). A computationally eﬃcient
nonparametric approach for changepoint detection. Statistics and Computing,
27(5):1293–1305.

Hocking, T. D., Schleiermacher, G., Janoueix-Lerosey, I., Boeva, V., Cappo, J.,
Delattre, O., Bach, F., and Vert, J.-P. (2013). Learning smoothing models
of copy number proﬁles using breakpoint annotations. BMC Bioinformatics,
14:164.

Jackson, B., Scargle, J. D., Barnes, D., Arabhi, S., Alt, A., Gioumousis, P.,
Gwin, E., Sangtrakulcharoen, P., Tan, L., and Tsai, T. T. (2005). An algo-
rithm for optimal partitioning of data on an interval. IEEE Signal Processing
Letters, 12(2):105–108.

Kawaguchi, N., Ogawa, N., Iwasaki, Y., Kaji, K., Terada, T., Murao, K., Inoue,
S., Kawahara, Y., Sumi, Y., and Nishio, N. (2011). HASC Challenge: Gather-
ing large scale human activity corpus for the real-world activity understand-
ings. In Proceedings of the 2nd Augmented Human International Conference,
AH 11, New York, NY, USA. Association for Computing Machinery.

Keogh, E., Chu, S., Hart, D., and Pazzani, M. (2001). An online algorithm
In Proceedings of the 2001 IEEE International

for segmenting time series.
Conference on Data Mining, pages 289–296. IEEE.

Killick, R., Fearnhead, P., and Eckley, I. A. (2012a). Optimal detection of
changepoints with a linear computational cost. Journal of the American Sta-
tistical Association, 107(500):1590–1598.

Killick, R., Fearnhead, P., and Eckley, I. A. (2012b). Supplemental material:
Optimal detection of changepoints with a linear computational cost. Journal
of the American Statistical Association, 107(500).

Ko, S. I., Chong, T. T., and Ghosh, P. (2015). Dirichlet process hidden Markov

multiple change-point model. Bayesian Analysis, 10(2):275–296.

Maidstone, R., Hocking, T., Rigaill, G., and Fearnhead, P. (2017). On optimal
multiple changepoint algorithms for large data. Statistics and Computing,
27(2):519–533.

Oudre, L., Lung-Yut-Fong, A., and Bianchi, P. (2011). Segmentation automa-
tique de signaux issus dun acc´el´erom`etre triaxial en p´eriode de marche. Pro-
ceedings of the Groupe de Recherche et dEtudes en Traitement du Signal et
des Images (GRETSI), Bordeaux, France.

14

Page, E. S. (1954). Continuous inspection schemes. Biometrika, 41(1-2):100–

115.

Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods.

Journal of the American Statistical Association, 66(336):846.

Reeves, J., Chen, J., Wang, X. L., Lund, R., and Lu, Q. Q. (2007). A review
and comparison of changepoint detection techniques for climate data. Journal
of Applied Meteorology and Climatology, 46(6):900–915.

Rigaill, G. (2015). A pruned dynamic programming algorithm to recover the best
segmentations with 1 to Kmax change-points. Journal de la Soci´et´e Fran¸caise
de Statistique, 156(4):180–205.

Schr¨oder, A. L. and Ombao, H. (2019). FreSpeD: Frequency-speciﬁc change-
point detection in epileptic seizure multi-channel EEG data. Journal of the
American Statistical Association, 114(525):115–128.

Schwarz, G. (1978). Estimating the dimension of a model. The Annals of

Statistics, 6(2):461–464.

Scott, A. J. and Knott, M. (1974). A cluster analysis method for grouping

means in the analysis of variance. Biometrics, 30(3):507.

Tartakovsky, A., Nikiforov, I., and Basseville, M. (2014). Sequential analysis:
Hypothesis testing and changepoint detection. Chapman and Hall/CRC.

Truong, C., Oudre, L., and Vayatis, N. (2018). ruptures: change point detection

in python.

Truong, C., Oudre, L., and Vayatis, N. (2019). Selective review of oﬄine change

point detection methods. Signal Processing, 107299.

Zou, C., Yin, G., Feng, L., and Wang, Z. (2014). Nonparametric maximum like-
lihood approach to multiple change-point problems. The Annals of Statistics,
42(3):970–1002.

15

