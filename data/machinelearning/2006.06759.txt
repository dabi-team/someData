0
2
0
2

t
c
O
6
2

]

C
O
.
h
t
a
m

[

2
v
9
5
7
6
0
.
6
0
0
2
:
v
i
X
r
a

On the Tightness of Semideﬁnite Relaxations for
Certifying Robustness to Adversarial Examples

Richard Y. Zhang
Department of Electrical and Computer Engineering,
University of Illinois at Urbana-Champaign,
Urbana, 61801 IL, USA.
ryz@illinois.edu

Abstract

The robustness of a neural network to adversarial examples can be provably certiﬁed
by solving a convex relaxation.
If the relaxation is loose, however, then the
resulting certiﬁcate can be too conservative to be practically useful. Recently,
a less conservative robustness certiﬁcate was proposed, based on a semideﬁnite
programming (SDP) relaxation of the ReLU activation function. In this paper,
we describe a geometric technique that determines whether this SDP certiﬁcate is
exact, meaning whether it provides both a lower-bound on the size of the smallest
adversarial perturbation, as well as a globally optimal perturbation that attains
the lower-bound. Concretely, we show, for a least-squares restriction of the usual
adversarial attack problem, that the SDP relaxation amounts to the nonconvex
projection of a point onto a hyperbola. The resulting SDP certiﬁcate is exact if
and only if the projection of the point lies on the major axis of the hyperbola.
Using this geometric technique, we prove that the certiﬁcate is exact over a single
hidden layer under mild assumptions, and explain why it is usually conservative
for several hidden layers. We experimentally conﬁrm our theoretical insights
using a general-purpose interior-point method and a custom rank-2 Burer-Monteiro
algorithm.

1

Introduction

It is now well-known that neural networks are vulnerable to adversarial examples: imperceptibly
small changes to the input that result in large, possibly targeted change to the output [1–3]. Adversarial
examples are particularly concerning for safety-critical applications like self-driving cars and smart
grids, because they present a mechanism for erratic behavior and a vector for malicious attacks.

Methods for analyzing robustness to adversarial examples work by formulating the problem of ﬁnding
the smallest perturbation needed to result in an adversarial outcome. For example, this could be
the smallest change to an image of the digit “3” for a given model to misclassify it as an “8”. The
size of this smallest change serves as a robustness margin: the model is robust if even the smallest
adversarial change is still easily detectable.

Computing the robustness margin is a nonconvex optimization problem. In fact, methods that attack
a model work by locally solving this optimization, usually using a variant of gradient descent [3–6].
A successful attack demonstrates vulnerability by explicitly stating a small—but not necessarily the
smallest—adversarial perturbation. Of course, failed attacks do not prove robustness, as there is
always the risk of being defeated by stronger attacks in the future. Instead, robustness can be certiﬁed
by proving lower-bounds on the robustness margin [7–18]. Training against a robustness certiﬁcate (as
an adversary) in turn produces models that are certiﬁably robust to adversarial examples [10, 19, 20].

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
The most useful robustness certiﬁcates are exact, meaning that they also explicitly state an adversarial
perturbation whose size matches their lower-bound on the robustness margin, thereby proving global
optimality [7–9]. Unfortunately, the robustness certiﬁcation problem is NP-hard in general, so most
existing methods for exact certiﬁcation require worst-case time that scales exponentially with respect
to the number of neurons. In contrast, conservative certiﬁcates are more scalable because the have
polynomial worst-case time complexity [10–18]. Their usefulness is derived from their level of
conservatism. The issue is that a pessimistic assessement for a model that is ostensibly robust can
be attributed to either undue conservatism in the certiﬁcate, or an undiscovered vulnerability in the
model. Also, training against an overly conservative certiﬁcate will result in an overly cautious model
that is too willing to sacriﬁce performance for perceived safety.

Recently, Raghunathan et al. [21] proposed a less conservative certiﬁcate based on a semideﬁnite
programming (SDP) relaxation of the rectiﬁed linear unit (ReLU) activation function. Their em-
pirical results found it to be signiﬁcantly less conservative than competing approaches, based on
linear programming or propagating Lipschitz constants. In other domains, ranging from integer
programming [22, 23], polynomial optimization [24, 25], matrix completion [26, 27], to matrix
sensing [28], the SDP relaxation is often tight, in the sense that it is formally equivalent to the
original combinatorially hard problem. Within our context, tightness corresponds to exactness in
the robustness certiﬁcate. Hence, the SDP relaxation is a good candidate for exact certiﬁcation in
polynomial time, possibly over some restricted class of models or datasets.

This paper aims to understand when the SDP relaxation of the ReLU becomes tight, with the goal
of characterizing conditions for exact robustness certiﬁcation. Our main contribution is a geometric
technique for analyzing tightness, based on splitting a least-squares restriction of the adversarial
attack problem into a sequence of projection problems. The ﬁnal problem projects a point onto a
nonconvex hyperboloid (i.e. a high-dimensional hyperbola), and the SDP relaxation is tight if and
only if this projection lies on the major axis of the hyperboloid. Using this geometric technique, we
prove that the SDP certiﬁcate is generally exact for a single hidden layer. The certiﬁcate is usually
conservative for several hidden layers; we use the same geometric technique to offer an explanation
for why this is the case.

Notations. Denote vectors in boldface lower-case x, matrices in boldface upper-case X, and scalars
in non-boldface x, X. The bracket denotes indexing x[i] starting from 1, and also concatenation,
which is row-wise via the comma [a, b] and column-wise via the semicolon [a; b]. The i-th canonical
basis vector ei satisﬁes ei[i] = 1 and ei[j] = 0 for all j (cid:54)= i. The usual inner product is (cid:104)a, b(cid:105) =
(cid:80)

ia[i]b[i], and the usual rectiﬁed linear unit activation function is ReLU(x) ≡ max{x, 0}.

2 Main results

Let f : Rn → Rm be a feedforward ReLU neural network classiﬁer with (cid:96) hidden layers

f (x0) = x(cid:96) where

xk+1 = ReLU(Wkxk + bk)

(2.1)
that takes an input ˆx ∈ Rn (say, an image of a hand-written single digit) labeled as belonging to the
i-th of m classes (say, the 5-th of 10 possible classes of single digits), and outputs a prediction vector
s = W(cid:96)f (ˆx) + b(cid:96) ∈ Rm whose i-th element is the largest, as in s[i] > s[j] for all j (cid:54)= i. Then, the
problem of ﬁnding an adversarial example x that is similar to ˆx but causes an incorrect j-th class to
be ranked over the i-th class can be posed

for all k ∈ {0, 1, . . . , (cid:96) − 1},

(cid:107)x − ˆx(cid:107)

subject to

(2.1),

(cid:104)w, f (x)(cid:105) + b ≤ 0,

(A)

where w = WT
ˆx over all incorrect classes gives a robustness margin d(cid:63) = minj(cid:54)=i dj for the neural network.
In practice, the SDP relaxation for problem (A) is often loose. To understand the underlying
mechanism, we study a slight modiﬁcation that we call its least-squares restriction

(cid:96) (ei − ej). In turn, the adversarial example x(cid:63) most similar to

L(cid:63) = min
x∈Rn

(cid:107)x − ˆx(cid:107)

subject to

(2.1),

(cid:107)f (x) − ˆz(cid:107) ≤ ρ,

(B)

where ˆz ∈ Rm is the targeted output, and ρ > 0 is a radius parameter. Problem (A) is equivalent to
problem (B) taken at the limit ρ → ∞, because a half-space is just an inﬁnite-sized ball
(cid:107)z+w (cid:0)b/(cid:107)w(cid:107)2 + ρ/(cid:107)w(cid:107)(cid:1) (cid:107)2 ≤ ρ2 ⇐⇒ (cid:107)w(cid:107)

2ρ (cid:107)z+w (cid:0)b/(cid:107)w(cid:107)2(cid:1) (cid:107)2 +[(cid:104)w, z(cid:105)+b] ≤ 0 (2.2)

2

d(cid:63)
j = min
x∈Rn
(cid:96) (ei − ej) and b = bT

with a center ˆz = −w (cid:0)b/(cid:107)w(cid:107)2 + ρ/(cid:107)w(cid:107)(cid:1) that tends to inﬁnity alongside the ball radius ρ. The SDP
relaxation for problem (B) is often tight for ﬁnite values of the radius ρ. The resulting solution x is a
strictly feasible (but suboptimal) attack for problem (A) that causes misclassiﬁcation (cid:104)w, f (x)(cid:105) + b <
0. The corresponding optimal value L(cid:63) gives an upper-bound dub ≡ L(cid:63) ≥ d(cid:63) that converges to an
equality as ρ → ∞. (See Appendix E for details.)

In Section 5, we completely characterize the SDP relaxation for problem (B) over a single hidden
neuron, by appealing to the underlying geometry of the relaxation. In Section 6, we extend these
insights to partially characterize the case of a single hidden layer.
Theorem 2.1 (One hidden neuron). Consider the one-neuron version of problem (B), explicitly
written

L(cid:63) = min

x

|x − ˆx|

subject to |ReLU(x) − ˆz| ≤ ρ.

(2.3)

The SDP relaxation of (2.3) yields a tight lower-bound Llb = L(cid:63) and a globally optimal x(cid:63)
satisfying |x(cid:63) − ˆx| = Llb if and only if one of the two conditions hold: (i) ρ ≥ |ˆz|; or (ii)
ρ < ˆz/(1 − min{0, ˆx/ˆz}).
Theorem 2.2 (One hidden layer). Consider the one-layer version of problem (B), explicitly written

L(cid:63) = min
x∈Rn

(cid:107)x − ˆx(cid:107)

s.t.

(cid:107)ReLU(Wx) − ˆz(cid:107) ≤ ρ

(2.4)

The SDP relaxation of (2.3) yields a tight lower-bound Llb = L(cid:63) and a globally optimal x(cid:63) satisfying
(cid:107)x(cid:63) − ˆx(cid:107) = Llb if one of the two conditions hold: (i) ρ ≥ (cid:107)ReLU(Wˆx) − ˆz(cid:107); or (ii) ρ < ˆzmin/2(1 +
κ) and (cid:107)Wˆx − ˆz(cid:107)∞ < ˆz2

min/(2ρκ) where ˆzmin = mini ˆzi and κ = (cid:107)W(cid:107)2(cid:107)(WWT )−1(cid:107)∞.

The lack of a weight term in (2.3) and a bias term in (2.3) and (2.4) is without loss of generality, as
these can always be accommodated by shifting and scaling x and ˆx. Intuitively, Theorem 2.1 and
Theorem 2.2 say that the SDP relaxation tends to be tight if the output target ˆz is feasible, meaning
that there exists some choice of u such that ˆz = f (u). (The condition ρ < ˆzmin/2(1 + κ) is sufﬁcient
for feasibility.) Conversely, the SDP relaxation tends to be loose if the radius ρ > 0 lies within an
intermediate band of “bad” values. For example, over a single neuron with a feasible ˆz = 1, the
relaxation is loose if and only if ˆx ≤ 0 and 1/(1 + |ˆx|) ≤ ρ < 1. These two general trends are
experimentally veriﬁed in Section 8.

In the case of multiple layers, the SDP relaxation is usually loose, with a notable exception being the
trivial case with L(cid:63) = 0.
Corollary 2.3 (Multiple layers). If ρ ≥ (cid:107)f (ˆx) − ˆz(cid:107), then the SDP relaxation of problem (B) yields
the tight lower-bound Llb = L(cid:63) = 0 and the globally optimal x(cid:63) = ˆx satisfying (cid:107)x(cid:63) − ˆx(cid:107) = 0.

The proof is given in Appendix E. In Section 7, we explain the looseness of the relaxation for multiple
layers using the geometric insight developed for the single layer. As mentioned above, the general
looseness of the SDP relaxation for problem (B) then immediately implies the general looseness for
problem (A).

3 Related work

Adversarial attacks, robustness certiﬁcates, and certiﬁably robust models. Adversarial examples
are usually found by using projected gradient descent to solve problem (A) with its objective and
constraint swapped [3–6]. Training a model against these empirical attacks generally yield very
resilient models [4–6]. It is possible to certify robustness exactly despite the NP-hardness of the
problem [7–9, 29]. Nevertheless, conservative certiﬁcates show greater promise for scalability
because they are polynomial-time algorithms. From the perspective of tightness, the next most
promising techniques after the SDP relaxation are relaxations based on linear programming (LP) [10–
13], though techniques based on propagating bounds and/or Lipschitz constants tend to be much
faster in practice [14–18]. Aside from training a model against a robustness certiﬁcate [10, 19, 20],
certiﬁably robust models can also be constructed by randomized smoothing [30–32].

Tightness of SDP relaxations. The geometric techniques used in our analysis are grounded in the
classic paper of Goemans and Williamson [33] (see also [34–36]), but our focuses are different: they
prove general bounds valid over entire classes of SDP relaxations, whereas we identify speciﬁc SDP
relaxations that are exactly tight. In the sense of tight relaxations, our results are reminescent of

3

the guarantees by Candès and Recht [26], Candès and Tao [27] (see also [37, 38]) on the matrix
completion problem, but our approaches are very different: their arguments are based on using the
dual to imply tightness in the primal, whereas our proof analyzes the primal directly.

After this paper was submitted, we became aware of two parallel work [39, 40] that also study the
tightness of SDP relaxations for robustness to adversarial examples. The ﬁrst, due to Fazlyab et al.
[39], uses similar techniques like the S-procedure to study a different SDP relaxation constructed
from robust control techniques. The second, due to Dvijotham et al. [40], studies the same SDP
relaxation within the context of a different attack problem, namely the version of Problem (A) with
the objective replaced by the inﬁnity norm distance (cid:107)x − ˆx(cid:107)∞.

Efﬁcient algorithms for SDPs. While SDPs are computationally expensive to solve using off-the-
shelf algorithms, efﬁcient formulation-speciﬁc solvers were eventually developed once their use case
became sufﬁciently justiﬁed. In fact, most state-of-the-art algorithms for phase retrieval [41–44] and
collaborative ﬁltering [45–49] can be viewed as highly optimized algorithms to solve an underlying
SDP relaxation. Our rank-2 Burer-Monteiro algorithm in Section 8 is inspired by Burer et al. [50]. It
takes strides towards an efﬁcient algorithm, but the primary focus of this paper is to understand the
use case for robustness certiﬁcation.

4 Preliminary: Geometry of the SDP relaxation

The SDP relaxation of Raghunathan et al. [21] is based on the observation that the rectiﬁed linear
unit (ReLU) activation function z = ReLU(x) ≡ max{0, x} is equivalent to the inequalities z ≥ 0,
z ≥ x, and z(z − x) ≤ 0. Viewing these as quadratics, we apply a standard technique (see Shor
[51] and also [24, 25, 33]) to rewrite them as linear inequalities over a positive semideﬁnite matrix
variable,

z ≥ 0,

z ≥ x, Z ≤ Y, G =

(cid:23) 0,

rank(G) = 1.

(4.1)

(cid:35)

(cid:34)1
x
z
x X Y
z Y Z

In essence, the reformulation collects the inherent nonconvexity of ReLU(·) into the constraint
rank(G) = 1, which can then be deleted to yield a convex relaxation. If the relaxation has a unique
solution G(cid:63) satisfying rank(G(cid:63)) = 1, then we say that it is tight.1 In this case, the globally optimal
solution x(cid:63), z(cid:63) to the original nonconvex problem can be found by solving the SDP relaxation in
polynomial time and factorizing the solution G(cid:63) = ggT where g = [1; x(cid:63); z(cid:63)]T .
It is helpful to view G as the Gram matrix associated with the vectors e, x, z ∈ Rp in an ambient p-
dimensional space, where p is the order of G (here p = 3). The individual elements of G correspond
to the inner products terms associated with e, x, z, as in

(cid:104)e, z(cid:105) ≥ max{0, (cid:104)e, x(cid:105)},

(cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105),

(cid:107)e(cid:107)2 = 1, G =

(cid:34)(cid:104)e, e(cid:105)
(cid:104)e, x(cid:105)
(cid:104)e, z(cid:105)

(cid:104)e, x(cid:105)
(cid:104)x, x(cid:105)
(cid:104)x, z(cid:105)

(cid:35)

(cid:104)e, z(cid:105)
(cid:104)x, z(cid:105)
(cid:104)z, z(cid:105)

, (4.2)

and rank(G) = 1 corresponds to collinearity between x, z, and e, as in (cid:107)e(cid:107)(cid:107)x(cid:107) = |(cid:104)e, x(cid:105)| and
(cid:107)e(cid:107)(cid:107)z(cid:107) = |(cid:104)e, z(cid:105)|. From the Gram matrix perspective, the SDP relaxation works by allowing the
underlying vectors x, z, and e to take on arbitrary directions; the relaxation is tight if and only if all
possible solutions e(cid:63), x(cid:63), z(cid:63) are collinear.

Figure 1 shows the underlying geometry the ReLU constraints (4.2) as
noted by Raghunathan et al. [21]. Take z as the variable and ﬁx e, x. Since
e is a unit vector, we may view (cid:104)e, x(cid:105) and (cid:104)e, z(cid:105) as the “e-axis coordinates”
for the vectors x and z. The constraint (cid:104)e, z(cid:105) ≥ max{0, (cid:104)e, x(cid:105)} is then
a half-space that restricts the “e-coordinate” of z to be nonnegative
and greater than that of x. The constraint (cid:104)z, z − x(cid:105) ≤ 0 is rewritten
as (cid:107)z − x/2(cid:107)2 ≤ (cid:107)x/2(cid:107)2 by completing the square; this is a sphere
that restricts z to lie within a distance of (cid:107)x/2(cid:107) from the center x/2.
Combined, the ReLU constraints (4.2) constrain z to lie within a spherical
cap—a portion of a sphere cut off by a plane.

x

e

x/2

Fig. 1 – The ReLU con-
straints (4.2) describe a
spherical cap.

1If a rank-1 solution exists but is nonunique, then we do not consider the SDP relaxation tight because the
rank-1 solution cannot usually be found in polynomial time. Indeed, an interior-point method converges onto a
maximum rank solution, but this can be rank-1 only if it is unique.

4

5 Tightness for one hidden neuron

Now, consider the SDP relaxation of the one-neuron problem (2.3), explicitly written as

L2

lb = min
G

X − 2xˆx + ˆx2

s.t.

z ≥ max{0, x}, Z ≤ Y,
Z − 2z ˆz + ˆz2 ≤ ρ2,

G =

(cid:35)

(cid:34)1
x
z
x X Y
z Y Z

(cid:23) 0.

(5.1)

Viewing the matrix variable G (cid:23) 0 as the Gram matrix associated with the vectors e, x, z ∈ Rp
where p = 3 rewrites (5.1) as the following

Llb = min

x,z,e∈Rp

(cid:107)x − ˆx e(cid:107)

s.t. (cid:104)z, e(cid:105) ≥ max{(cid:104)x, e(cid:105), 0}, (cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105), (cid:107)z − ˆz e(cid:107) ≤ ρ.

(5.2)

The SDP relaxation (5.1) has a unique rank-1 solution if and only if its nonconvex vector interpretation
(5.2) has a unique solution that aligns with e. The proof for the following is given in Appendix A.
Lemma 5.1 (Collinearity and rank-1). Fix e ∈ Rp. Then, problem (5.2) has a unique solution
x(cid:63) satisfying (cid:107)x(cid:63)(cid:107) = |(cid:104)x(cid:63), e(cid:105)| if and only if problem (5.1) has a unique solution G(cid:63) satisfying
rank(G(cid:63)) = 1.

We proceed to solve problem (5.2) by rewriting it as the composition of a convex projection over z
with a nonconvex projection over x, as in:

(cid:107)z − ˆze(cid:107)

subject to

(cid:104)e, z(cid:105) ≥ max{(cid:104)e, x(cid:105), 0},

(cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105),

(5.3)

φ(x, ˆz) = min
z∈Rp
Llb = min
x∈Rp

(cid:107)x − ˆxe(cid:107)

subject to

φ(x, ˆz) ≤ ρ.

Problem (5.3) is clearly the projection of the point ˆze onto the spherical
cap shown in Figure 1.
In a remarkable symmetry, it turns out that
problem (5.4) is the projection of the point ˆxe onto a hyperboloidal cap—
a portion of a high-dimensional hyperbola cut off by a plane—with the
optimal x(cid:63) in problem (5.2) being the resulting projection. In turn, our
goal of verifying collinearity between x(cid:63) and e amounts to checking
whether ˆxe projects onto the major axis of the hyperboloid.

ˆz1e

(5.4)

x

ˆz2e

x/2

To turn this intuitive sketch into a rigorous proof, we begin by solving
the convex projection (5.3) onto the spherical cap. Figure 2 shows the
corresponding geometry. There are two distinct scenarios: i) For ˆz1e that
is above the spherical cap, the projection must intersect the upper, round
portion of the spherical cap along the line from ˆz1e to x/2. This yields
a distance of φ(x, ˆz1) = (cid:107)ˆz1e − x/2(cid:107) − (cid:107)x/2(cid:107). ii) For ˆz2e that is below the spherical cap, the
projection is simply the closest point directly above, at a distance of φ(x, ˆz2) = max{0, (cid:104)e, x(cid:105)} − ˆz2.

Fig. 2 – Problem (5.3) is the
projection of a point onto a
spherical cap.

It turns out that the conditional statements are unnecessary; the distance
φ(x, ˆz) simply takes on the larger of the two values derived above. In
Appendix B, we prove this claim algebraically, thereby establishing the
following.
Lemma 5.2 (Projection onto spherical cap). The function φ : Rp × R →
R deﬁned in (5.3) satisﬁes

φ(x, ˆz) = max{max{0, (cid:104)e, x(cid:105)} − ˆz,

(cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107)}.

(5.5)

Taking x as the variable, we see from Lemma 5.2 that each level set
φ(x, ˆz) = ρ is either: 1) a hyperplane normal to e at the intercept ˆz + ρ;
or 2) a two-sheet hyperboloid centered at ˆze, with semi-major axis ρ and
focal distance |ˆz| in the direction of e. Hence, the sublevel set φ(x, ˆz) ≤ ρ
is a hyperboloidal cap as claimed.

ρ
ρ

ˆx1e

ˆze

ˆx2e
ˆx3e

We proceed to solve the nonconvex projection (5.4) onto the hyperboloidal
cap. This shape degenerates into a half-space if the semi-major axis ρ
is longer than the focal distance, as in ρ ≥ |ˆz|, and becomes empty
altogether with a negative center, as in ˆz < −ρ. Figure 3 shows the geometry of projecting onto a
nondegenerate hyperboloidal cap with ˆz > ρ. There are three distinct scenarios: i) For ˆx1e that is

Fig. 3 – Problem (5.4) is the
projection of a point onto a
hyperbolidal cap.

5

either above or interior to the hyperboloidal cap, the projection is either the closest point directly
below or the point itself, as in x(cid:63) = min{ˆz + ρ, ˆx1}e; ii) For ˆx2e that is below and sufﬁciently close
to the hyperboloidal cap, the projection lies at the top of the hyperbolid sheet at x(cid:63) = (ˆz − ρ)e;
iii) For ˆx3e that is below and far away from the hyperboloidal cap, the projection lies somewhere
along the side of the hyperboloid.
Evidently, the ﬁrst two scenarios correspond to choices of x(cid:63) that are collinear to e, while the third
scenario does not. To resolve the boundary between the second and third scenarios, we solve the
projection onto a hyperbolidal cap in closed-form.
Lemma 5.3 (Projection onto nondegenerate hyperboloidal cap). Given e ∈ Rp, ˆx ∈ R, and
ˆz > ρ > 0, deﬁne x(cid:63) as the solution to the following projection

min
x∈Rp

(cid:107)x − ˆxe(cid:107)2

s.t.

(cid:104)e, x(cid:105) − ˆz ≤ ρ,

(cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107) ≤ ρ.

Then, x(cid:63) is unique and satisﬁes (cid:107)x(cid:63)(cid:107) = |(cid:104)e, x(cid:63)(cid:105)| if and only if (ˆz − ˆx) < ˆz2/ρ.

We defer the proof of Lemma 5.3 to Appendix C, but note that the main idea is to use the S-lemma
(see e.g. [52, p. 655] or [53]) to solve the minimization of a quadratic (the distance) subject to a
quadratic constraint (the nondegenerate hyperboloid). Resolving the degenerate cases and applying
Lemma 5.3 to the nondegenerate case yields a proof of our main result.

Proof of Theorem 2.1. If ˆz < −ρ, then the hyperbolidal cap φ(x, ˆz) ≤ ρ is empty as ρ < −ˆz ≤
φ(x, ˆz). In this case, problem (5.4) is infeasible. If |ˆz| ≤ ρ, then the hyperbolidal cap φ(x, ˆz) ≤ ρ
degenerates into a half-space (cid:104)e, x(cid:105) ≤ ˆz+ρ, because (cid:107)ˆze−x/2(cid:107)−(cid:107)x/2(cid:107) ≤ (cid:107)ˆze(cid:107)+(cid:107)x/2(cid:107)−(cid:107)x/2(cid:107) =
|ˆz| ≤ ρ. In this case, the projection x(cid:63) = min{ˆz + ρ, ˆx}e is clearly collinear to e, so |ˆz| ≤ ρ is the
ﬁrst condition of Theorem 2.1. Finally, if ˆz > ρ, Lemma 5.3 says that x(cid:63) is collinear with e whenever
(ˆz−ˆx) < ˆz2/ρ, which is rewritten ˆx > ˆz(1−ˆz/ρ). Under ˆz > ρ, this is equivalent to ρ < ˆz/(1−ˆx/ˆz).
Finally, taking the intersection of these two constraints yields ρ < ˆz/(1 − min{0, ˆx/ˆz}), which is
the second condition of Theorem 2.1.

6 Tightness for one layer

Our analysis of the one-hidden-neuron case extends to the one-hidden-layer case without signiﬁcant
modiﬁcation. Here, the semideﬁnite relaxation reads

L2

lb = min
G

tr(X) − 2(cid:104)x, ˆx(cid:105) + (cid:107)ˆx(cid:107)2

s.t.

z ≥ max{0, Wx}, diag(WZ) ≤ diag(WY),
tr(Z) − 2(cid:104)z, ˆz(cid:105) + (cid:107)ˆz(cid:107)2 ≤ ρ2,

G =

(6.1)



 (cid:23) 0.





z
x
1
x X Y
z YT Z

Viewing the matrix variable G (cid:23) 0 in the corresponding SDP relaxation (6.1) as the Gram matrix
associated a set of length-p vectors (where p = m + n + 1 is the order of the matrix G) yields the
following2

L2

lb =

min
xj ,zi∈Rp

s.t.

(cid:80)

j(cid:107)xj − ˆxj e(cid:107)2
(cid:110)
0, (cid:104)e, (cid:80)

(cid:104)e, zi(cid:105) ≥ max

(cid:107)zi(cid:107)2 ≤ (cid:104)zi, (cid:80)

jWi,jxj(cid:105),

jWi,jxj(cid:105)

(cid:111)

(6.2)

,

(cid:80)

i(cid:107)zi − ˆzi e(cid:107)2 ≤ ρ2 for all i,

with indices i ∈ {1, 2, . . . , m} and j ∈ {1, 2, . . . , n}. We will derive conditions for the SDP
relaxation (6.1) to have a unique, rank-1 solution by ﬁxing e in problem (6.2) and verifying that every
optimal x(cid:63)
j is collinear with e for all j. The proof for the following is given in Appendix A.
Lemma 6.1 (Collinearity and rank-1). Fix e ∈ Rp. Then, problem (6.2) has a unique solution
x(cid:63)
j , e(cid:105)| if and only if problem (6.1) has a unique solution G(cid:63)
1, x(cid:63)
satisfying rank(G(cid:63)) = 1.

n satisfying (cid:107)x(cid:63)

j (cid:107) = |(cid:104)x(cid:63)

2, . . . , x(cid:63)

2To avoid visual clutter we will abbreviate (cid:80)n
whenever the ranges of indices are clear from context.

j=1 xj and “for all i ∈ {1, 2, . . . , n}” as (cid:80)

j xj and “for all i”

6

Problem (6.2) can be rewritten as the composition of a series of projections over zi, followed by a
sequence of nonconvex projections over xj, as in

L2

lb = min

xj ∈Rp,ai≥0

(cid:80)

j(cid:107)xj − ˆxj e(cid:107)2

s.t. φ((cid:80)

jWi,jxj, ˆzi) ≤ ρi for all i, (cid:80)

iρ2

i ≤ ρ2, (6.3)

where φ was previously deﬁned in the one-neuron convex projection (5.3). Whereas in the one-neuron
case we are projecting a single point onto a single hyperboloidal cap, the one-layer case requires us
to project n points onto the intersection of n hyperboloidal caps. This has a closed-form solution
only when all the hyperboloids are nondegenerate.
Lemma 6.2 (Projection onto several hyperboloidal caps). Given W = [Wi,j] ∈ Rm×n, ˆx = [ˆxj] ∈
Rn, ˆz = [ˆzi] ∈ Rm, e ∈ Rp, and ρi satisfying ˆzi > ρ > 0, deﬁne x(cid:63)
j as the solution to the following
projection

min
xj ∈Rp

(cid:80)

j(cid:107)xj − ˆxje(cid:107)2

s.t.

(cid:107)ˆzie − (cid:80)

jWi,jxj/2(cid:107) − (cid:107)(cid:80)

(cid:104)e, (cid:80)

jWi,jxj(cid:105) − ˆzi ≤ ρi for all i,
jWi,jxj/2(cid:107) ≤ ρi for all i.

If ρmax(cid:107)W(cid:107)2(cid:107)(WWT )−1(Wˆx − ˆz)(cid:107)∞ + ρ2
ρmax = maxi ρi and ˆzmin = mini ˆzi, then x(cid:63)

max(1 + (cid:107)W(cid:107)2(cid:107)(WWT )−1(cid:107)∞) < ˆz2
j (cid:107) = |(cid:104)e, x(cid:63)

j is unique and satisﬁes (cid:107)x(cid:63)

min holds with

j (cid:105)| for all j.

We defer the proof of Lemma 6.2 to Appendix D, but note that the main idea is to use the lossy
S-lemma to solve the minimization of one quadratic (the distance) over several quadratic constraints
(the hyperboloids). Theorem 2.2 then follows immediately from Lemma 6.2 and Corollary 2.3.

7 Looseness for multiple layers

Unfortunately, the SDP relaxation is not usually tight for more than a single layer. Let f (x) =
ReLU(ReLU(x)) denote a two-layer neural network with a single neuron per layer. The correspond-
ing instance of problem (B) is essentially the same as problem (2.3) from Section 5 for the one-layer
one-neuron network, because ReLU(ReLU(x)) = ReLU(x) holds for all x. However, constructing
the SDP relaxation and taking the Gram matrix interpretation reveals the following (with p = 4)

Llb = min

x,z,e∈Rp

(cid:107)x − ˆx e(cid:107)

s.t.

(cid:104)z, e(cid:105) ≥ max{(cid:104)x, e(cid:105), 0}, (cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105),
(cid:104)z, e(cid:105) − ˆz ≤ ρ, (cid:107)ˆze − z/2(cid:107) − (cid:107)z/2(cid:107) ≤ ρ,

(7.1)

which is almost the same as problem (5.2) from Section 5, except that the convex ball constraint
(cid:107)ˆze − z(cid:107) ≤ ρ has been replaced by a nonconvex hyperboloid. As we will see, it is this hyperbolic
geometry that makes it harder for the SDP relaxation to be tight.

Denote x(cid:63) as the solution to both instances of problem (B). The point
u = x(cid:63)e must be the unique solution to (7.1) and (5.2) in order for their
respective SDP relaxations to be tight. Now, suppose that ˆx < 0 and
ˆz > ρ > 0, so that both instances of problem (B) have x(cid:63) = ˆz − ρ > 0.
Both (5.2) and (7.1) are convex over x; ﬁxing z and optimizing over x in
each case yields (cid:107)x(cid:63) − ˆxe(cid:107) = (cid:107)z(cid:107) − ˆx cos θ where cos θ = (cid:104)e, z(cid:105)/(cid:107)z(cid:107).
In order for u to be the unique solution, we need (cid:107)z(cid:107) − ˆx cos θ to be
globally minimized at z = u. As shown in Figure 4, (cid:107)z(cid:107) is clearly
minimized at z(cid:63) = u over the ball constraint (cid:107)ˆze − z(cid:107) ≤ ρ, but the same
is not obviously true for the hyperbolid (cid:107)ˆze − z/2(cid:107) − (cid:107)z/2(cid:107) ≤ ρ. Some
detailed computation readily conﬁrm the geometric intuition that u is
generally a local minimum over the circle, but not over the hyperbola.

zc

u

zh

Fig. 4 – Any point zc on
the circle clearly satisﬁes
(cid:107)zc(cid:107) > (cid:107)u(cid:107), but a point zh
on the hyperbola may have
(cid:107)zh(cid:107) ≈ (cid:107)u(cid:107).

8 Numerical experiments

Dataset and setup. We use the MNIST dataset of 28 × 28 images of
handwritten digits, consisting of 60,000 training images and 10,000 testing images. We remove and
set aside the ﬁnal 1,000 images from the training set as the veriﬁcation set. All our experiments are
performed on an Intel Xeon E3-1230 CPU (4-core, 8-thread, 3.2-3.6 GHz) with 32 GB of RAM.

Architecture. We train two small fully-connected neural network (“dense-1” and “dense-3”) whose
SDP relaxations can be quickly solved using MOSEK [54], and a larger convolutional network

7

(“CNN”) whose SDP relaxation must be solved using a custom algorithm described below. The
“dense-1” and “dense-3” models respectively have one and three fully-connected layer(s) of 50
neurons, and are trained on a 4 × 4 maxpooled version of the training set (each image is downsampled
to 7 × 7). The “CNN” model has two convolutional layers (stride 2) with 16 and 32 ﬁlters (size 4 × 4)
respectively, followed by a fully-connected layer with 100 neurons, and is trained on the original
dataset of 28 × 28 images. All models are implemented in tensorﬂow and trained over 50 epochs
using the SGD optimizer (learning rate 0.01, momentum 0.9, “Nesterov” true).

Rank-2 Burer-Monteiro algorithm (“BM2”). We use a rank-2 Burer–Monteiro algorithm to solve
instances of the SDP relaxation on the “CNN” model, by applying a local optimization algorithm to
the following (see Appendix F for a detailed derivation and implementation details)

min
uk,vk∈Rn

(cid:107)u0 − ˆx(cid:107)2 + (cid:107)v0(cid:107)2

(BM2)

s.t. diag(uk+1uT

k+1 + vk+1vT

k+1) ≤ diag((Wkuk + bk)uT

k+1 + Wkvk+1vT

k+1)

uk+1 ≥ max {0, Wkuk + bk} ,

(cid:107)u(cid:96) − ˆz(cid:107)2 + (cid:107)v(cid:96)(cid:107)2 ≤ ρ2

for all k.

k, v(cid:63)

0 = 0, then by induction u(cid:63)

Let {u(cid:63)
k} be a locally optimal solution satisfying the ﬁrst- and second-order optimality conditions
(see e.g. [55, Chapter 12]). If v(cid:63)
k = 0
for all k. It then follows from a well-known result of Burer and Monteiro [56] (see also [57, 58] and
in particular [59, Lemma 1]) that {u(cid:63)
k} corresponds to a rank-1 solution of the SDP relaxation,
and is therefore globally optimal. Of course, such a solution must not exist if the relaxation is loose;
even when it does exist, the algorithm might still fail to ﬁnd it if it gets stuck in a spurious local
minimum with v(cid:63)
0 (cid:54)= 0. Our experience is that the algorithm consistently succeeds whenever the
relaxation is tight, but admittedly this is not guaranteed.

k+1 = ReLU(Wku(cid:63)

k + bk) and v(cid:63)

k, v(cid:63)

Tightness for problem (B). Our theoretical results suggest that the SDP relaxation for problem (B)
should be tight for one layer and loose for multiple layers. To verify, we consider the ﬁrst k layers
of the “dense-3” and “CNN” models over a range of radii ρ. In each case, we solve 1000 instances
of the SDP relaxation, setting ˆx to be a new image from the veriﬁcation set, and ˆz = f (u) where
u is the previous image used as ˆx. MOSEK solved each instance of “dense-3” in 5-20 minutes
and BM2 solved each instance of “CNN” in 15-60 minutes. We mark G(cid:63) as numerically rank-1 if
λ1(G(cid:63))/λ2(G(cid:63)) > 103, and plot the success rates in Figure 5a. Consistent with Theorem 2.2, the
relaxation over one layer is most likely to be loose for intermediate values of ρ. Consistent with
Corollary 2.3, the relaxation eventually becomes tight once ρ is large enough to yield a trivial solution.
The results for CNN are dramatic, with an 100% success rate over a single layer, and a 0% success
rate for two (and more) layers. BM2 is less successful for very large and very small ρ in part due to
numerical issues associated with the factor-of-two exponent in (cid:107)z − ˆz(cid:107)2 ≤ ρ2.

CNN ((cid:96) = 1)

dense-3 ((cid:96) = 1)

dense-3 ((cid:96) = 2)

a

dense-3
((cid:96) = 3)

CNN ((cid:96) = 2)

b

infeas

tight

loose

tight

c

Fig. 5 – a. The SDP relaxation for problem (B) is generally tight over a single layer, and loose over multiple
layers. b. Viewing problem (A) as (B) taken at the limit ρ → ∞, the resulting SDP relaxation can be close to,
but not exactly, tight, for ﬁnite values of ρ. c. The SDP relaxation of (B) can produce a near-optimal attack x
satisfying (cid:104)w, f (x)(cid:105) + b < 0 for problem (A), even when relaxation itself is not actually tight.

8

10-210-1100101102103-1.5-1-0.5010-210-11001011021030.60.811.2ActualRelax10-310-210-1100101102020406080100Application to problem (A). Viewing problem (A) as problem (B) in the limit ρ → ∞, we consider
a ﬁnite range of values for ρ, and solve the corresponding SDP relaxation with ˆz = −w(b/(cid:107)w(cid:107)2 +
ρ/(cid:107)w(cid:107)). Here, the SDP relaxation is generally loose, so BM2 does not usually succeed, and we must
resort to using MOSEK to solve it on the small “dense-1” model. Figure 5b compares the relaxation
objective (cid:112)tr(X) − 2(cid:104)ˆx, x(cid:105) + (cid:107)ˆx(cid:107)2 with the actual distance (cid:107)x − ˆx(cid:107), while Figure 5c compares
the feasibility predicted by the relaxation (cid:104)w, z(cid:105) + b with the actual feasibility (cid:104)w, f (x)(cid:105) + b. The
relaxation is tight for 0.07 ≤ ρ ≤ 0.4 and ρ ≥ 60 so the plots coincide. The relaxation is loose
for 0.4 ≤ ρ ≤ 60, and the relaxation objective is strictly greater than the actual distance because
X (cid:31) xxT . The resulting attack x must fail to satisfy (cid:107)f (x) − ˆz(cid:107) ≤ ρ, but in this case it is still always
feasible for problem (A). For ρ < 0.07, the SDP relaxation is infeasible, so we deduce that the output
target ˆz is not actually feasible.

9 Conclusions

This paper presented a geometric study of the SDP relaxation of the ReLU. We split the a modiﬁcation
of the robustness certiﬁcation problem into the composition of a convex projection onto a spherical
cap, and a nonconvex projection onto a hyperboloidal cap, so that the relaxation is tight if and only if
the latter projection lies on the major axis of the hyperboloid. This insight allowed us to completely
characterize the tightness of the SDP relaxation over a single neuron, and partially characterize
the case for the single layer. The multilayer case is usually loose due to the underlying hyperbolic
geometry, and this implies looseness in the SDP relaxation of the original certiﬁcation problem.
Our rank-2 Burer-Monteiro algorithm was able to solve the SDP relaxation on a convolution neural
network, but better algorithms are still needed before models of realistic scales can be certiﬁed.

Broader Impact

This work contributes towards making neural networks more robust to adversarial examples. This is a
crucial roadblock before neural networks can be widely adopted in safety-critical applications like self-
driving cars and smart grids. The ultimate, overarching goal is to take the high performance of neural
networks—already enjoyed by applications in computer vision and natural language processing—and
extend towards applications in societal infrastructure.

Towards this direction, SDP relaxations allow us to make mathematical guarantees on the robustness
of a given neural network model. However, a blind reliance on mathematical guarantees leads to a
false sense of security. While this work contributes towards robustness of neural networks, much
more work is needed to understand the appropriateness of neural networks for societal applications in
the ﬁrst place.

Acknowledgments

The author is grateful to Salar Fattahi, Cedric Josz, and Yi Ouyang for early discussions and detailed
feedback on several versions of the draft. Partial ﬁnancial support was provided by the National
Science Foundation under award ECCS-1808859.

References

[1] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
Joint European conference on machine learning and knowledge discovery in databases, pages
387–402. Springer, 2013. 1

[2] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference
on Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.

[3] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In

2017 ieee symposium on security and privacy (sp), pages 39–57. IEEE, 2017. 1, 3

9

[4] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
In International Conference on Learning Representations, 2015. URL http:

examples.
//arxiv.org/abs/1412.6572. 3

[5] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint arXiv:1611.01236, 2016.

[6] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018. 1, 3

[7] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex:
An efﬁcient SMT solver for verifying deep neural networks. In International Conference on
Computer Aided Veriﬁcation, pages 97–117. Springer, 2017. 1, 3

[8] Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis, pages 269–
286. Springer, 2017.

[9] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep
neural networks. In International Conference on Computer Aided Veriﬁcation, pages 3–29.
Springer, 2017. 1, 3

[10] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pages 5286–5295,
2018. 1, 3

[11] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pages 8400–8409, 2018.

[12] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet
Kohli. A dual approach to scalable veriﬁcation of deep networks. In UAI, volume 1, page 2,
2018.

[13] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned
veriﬁers. arXiv preprint arXiv:1805.10265, 2018. 3

[14] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for
provably robust neural networks. In International Conference on Machine Learning, pages
3578–3586, 2018. 3

[15] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems,
pages 10802–10813, 2018.

[16] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715,
2018.

Ef-
[17] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
ﬁcient neural network robustness certiﬁcation with general activation functions.
In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31, pages 4939–
4948. Curran Associates,
URL http://papers.nips.cc/paper/
7742-efficient-neural-network-robustness-certification-with-general-activation-functions.
pdf.

2018.

Inc.,

[18] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for ReLU networks. In
International Conference on Machine Learning, pages 5276–5285, 2018. 1, 3

10

[19] Aman Sinha, Hongseok Namkoong, and John Duchi. Certiﬁable distributional robustness with

principled adversarial training. arXiv preprint arXiv:1710.10571, 2, 2018. 1, 3

[20] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial

examples. In International Conference on Learning Representations, 2018. 1, 3

[21] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems
31, pages 10900–10910. Curran Associates, Inc., 2018. 1, 4, 4

[22] László Lovász and Alexander Schrijver. Cones of matrices and set-functions and 0–1 optimiza-

tion. SIAM journal on optimization, 1(2):166–190, 1991. 1

[23] Jean B Lasserre. An explicit exact sdp relaxation for nonlinear 0-1 programs. In International
Conference on Integer Programming and Combinatorial Optimization, pages 293–303. Springer,
2001. 1

[24] Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM

Journal on optimization, 11(3):796–817, 2001. 1, 4

[25] Pablo A Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Mathemat-

ical programming, 96(2):293–320, 2003. 1, 4

[26] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics, 9(6):717, 2009. 1, 3

[27] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010. 1, 3

[28] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010. 1

[29] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference on Learning Representations, 2019. 3

[30] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on
Security and Privacy (SP), pages 656–672. IEEE, 2019. 3

[31] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pages 1310–1320, 2019.

[32] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classiﬁers. In
Advances in Neural Information Processing Systems, pages 11289–11300, 2019. 3

[33] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM),
42(6):1115–1145, 1995. 3, 4

[34] Alan Frieze and Mark Jerrum. Improved approximation algorithms for maxk-cut and max

bisection. Algorithmica, 18(1):67–81, 1997. 3

[35] Yurii Nesterov. Quality of semideﬁnite relaxation for nonconvex quadratic optimization. Techni-
cal report, Université catholique de Louvain, Center for Operations Research and Econometrics,
1997.

[36] Anthony Man-Cho So, Jiawei Zhang, and Yinyu Ye. On approximating complex quadratic
optimization problems via semideﬁnite programming relaxations. Mathematical Programming,
110(1):93–110, 2007. 3

[37] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE,

98(6):925–936, 2010. 3

11

[38] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Re-
search, 12(104):3413–3430, 2011. URL http://jmlr.org/papers/v12/recht11a.html.
3

[39] Mahyar Fazlyab, Manfred Morari, and George J Pappas. Safety veriﬁcation and robustness
analysis of neural networks via quadratic constraints and semideﬁnite programming. arXiv
preprint arXiv:1903.01287, 2019. 3

[40] Krishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and
Pushmeet Kohli. Efﬁcient neural network veriﬁcation with exactness characterization. In
Uncertainty in Artiﬁcial Intelligence, pages 497–507. PMLR, 2020. 3

[41] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating min-
imization. In Advances in Neural Information Processing Systems, pages 2796–2804, 2013.
3

[42] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly
as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages
739–747, 2015.

[43] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search
for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages
3873–3881, 2016.

[44] Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. IEEE

Transactions on Information Theory, 64(4):2675–2689, 2018. 3

[45] Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen. A singular value thresholding algorithm

for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010. 3

[46] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy

entries. Journal of Machine Learning Research, 11(Jul):2057–2078, 2010.

[47] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a

few entries. IEEE transactions on information theory, 56(6):2980–2998, 2010.

[48] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.

IEEE Transactions on Information Theory, 62(11):6535–6579, 2016.

[49] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In

Advances in Neural Information Processing Systems, pages 2973–2981, 2016. 3

[50] Samuel Burer, Renato DC Monteiro, and Yin Zhang. Rank-two relaxation heuristics for max-cut
and other binary quadratic programs. SIAM Journal on Optimization, 12(2):503–521, 2002. 3

[51] N.Z. Shor. Quadratic optimization problems. Soviet journal of computer and systems sciences,
25(6):1–11, 1987. URL https://www.scopus.com/inward/record.uri?eid=2-s2.
0-0023454146&partnerID=40&md5=04f73c1bb9b80f64beacc99d77e7588a.
cited By
120. 4

[52] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge

university press, 2004. 5, C

[53] Imre Pólik and Tamás Terlaky. A survey of the s-lemma. SIAM review, 49(3):371–418, 2007. 5,

C

[54] ApS MOSEK. The MOSEK optimization toolbox for MATLAB manual, 2019. URL https:

//docs.mosek.com/9.0/toolbox.pdf. 8

[55] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business

Media, 2006. 8

[56] Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semideﬁnite

programming. Mathematical Programming, 103(3):427–444, 2005. 8, F

12

[57] Nicolas Boumal, Vladislav Voroninski, and Afonso S Bandeira. Deterministic guarantees for
Burer-Monteiro factorizations of smooth semideﬁnite programs. Communications on Pure and
Applied Mathematics, 73(3):581–608, 2020. 8, F

[58] Michel Journée, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization, 20(5):2327–2351,
2010. 8, F

[59] Diego Cifuentes. Burer-monteiro guarantees for general semideﬁnite programs. arXiv preprint

arXiv:1904.07147, 2019. 8, F

[60] Alex Lemon, Anthony Man-Cho So, Yinyu Ye, et al. Low-rank semideﬁnite programming:

Theory and applications. Foundations and Trends® in Optimization, 2(1-2):1–156, 2016. A

[61] Mituhiro Fukuda, Masakazu Kojima, Kazuo Murota, and Kazuhide Nakata. Exploiting sparsity
in semideﬁnite programming via matrix completion I: General framework. SIAM Journal on
Optimization, 11(3):647–674, 2001. E

[62] Lieven Vandenberghe, Martin S Andersen, et al. Chordal graphs and semideﬁnite optimization.

Foundations and Trends® in Optimization, 1(4):241–433, 2015. E

13

A Uniqueness of a Rank-1 Solution

Consider the rank-constrained semideﬁnite program

minimize
x∈Rn, X∈Rn×n
subject to

(cid:104)D, X(cid:105) + (cid:104)f , x(cid:105)

(A.1)

(cid:104)Ai, X(cid:105) + (cid:104)bi, x(cid:105) ≤ ci
(cid:21)

(cid:23) 0,

rank(X) ≤ p,

(cid:20)1 xT
x X

for all i ∈ {1, 2, . . . , m},

and its corresponding nonconvex optimization interpretation

minimize
v1,v2,...,vn∈Rp

subject to

n
(cid:88)

n
(cid:88)

(cid:104)vj, vk(cid:105)(cid:104)D, ekeT

j (cid:105) +

n
(cid:88)

(cid:104)e, vj(cid:105)(cid:104)f , ej(cid:105)

(A.2)

j=1

k=1
n
(cid:88)

n
(cid:88)

(cid:104)vj, vk(cid:105)(cid:104)Ai, ekeT

j (cid:105) +

j=1
n
(cid:88)

(cid:104)e, vj(cid:105)(cid:104)bi, ej(cid:105) ≤ ci

for all i ∈ {1, 2, . . . , m},

k=1

j=1

j=1

where e is an arbitrary, ﬁxed unit vector satisfying (cid:107)e(cid:107) = 1. Our main result in this section is that we
can guarantee a rank-1 solution to (A.1) to be unique, and hence computable via an interior-point
method, by verifying that every solution to (A.2) is collinear with the unit vector e.
Deﬁnition A.1. Fix e ∈ Rp with (cid:107)e(cid:107) = 1. We say that v ∈ Rp is collinear or that it satisﬁes
collinearity if |(cid:104)e, v(cid:105)| = (cid:107)v(cid:107).
Theorem A.2 (Unique rank-1). Fix e ∈ Rp with (cid:107)e(cid:107) = 1, and write V (cid:63) as the resulting solution set
associated with (A.2). Then, problem (A.1) has a unique solution satisfying X(cid:63) = x(cid:63)(x(cid:63))T if and
only if v(cid:63)

n are collinear for all (v(cid:63)

2, . . . , v(cid:63)

2, . . . , v(cid:63)

n) ∈ V (cid:63).

1, v(cid:63)

1, v(cid:63)

We brieﬂy defer the proof of Theorem A.2 to discuss its consequences.
constraints, if the input vectors xj are collinear, then the output vectors zi are also collinear.
Lemma A.3 (Propagation of collinearity). Fix e ∈ Rp with (cid:107)e(cid:107) = 1. Under the ReLU constraints
z, (cid:80)
(cid:104)e, z(cid:105) ≥ max{0, (cid:104)e, (cid:80)
, if xj is collinear for all j, then z is
also collinear.

jwjxj(cid:105)(cid:105) and (cid:104)z, z(cid:105) ≤

In the case of ReLU

jwjxj

(cid:68)

(cid:69)

Proof. Let xj = xje for all j, and write α = (cid:80)
(cid:68)
z, (cid:80)

jwjxj for clarity. Observe that (cid:104)z, z(cid:105) ≤
jwjxj (cid:104)z, e(cid:105) = α (cid:104)z, e(cid:105). If α < 0, then (cid:104)z, e(cid:105) = 0 and (cid:104)z, e(cid:105) = 0 as claimed.

jwjxj

= (cid:80)

(cid:69)

(cid:68)
e, (cid:80)

(cid:69)

= (cid:80)

If α ≥ 0, then (cid:104)e, z(cid:105) ≥
jwjxj = α. Combined with the above, this yields
(cid:104)z, z(cid:105) ≤ α (cid:104)z, e(cid:105) ≤ (cid:104)z, e(cid:105)2. We actually have (cid:104)z, z(cid:105) = (cid:104)z, e(cid:105)2 as claimed, because (cid:104)z, z(cid:105) ≥ (cid:104)z, e(cid:105)2
already holds by the Cauchy–Schwarz inequality.

jwjxj

Hence, the conditions for the uniquess of the rank-1 solution throughout the main body of the paper
are simply special cases of Theorem A.2.

Proof of Lemma 5.1. Observe that the semideﬁnite program (5.1) is a special instance of (A.1), and
that its nonconvex interpretation (5.2) is the corresponding instance of (A.2). In the one-neuron
case, Lemma A.3 says that if x(cid:63) satisﬁes collinearity, then z(cid:63) also satisﬁes collinearity. Or put in
another way, x(cid:63) and z(cid:63) satisfy collinearity if and only if x(cid:63) satisﬁes collinearity. Using the latter
as an equivalent condition for the former and substituting into Theorem A.2 yields Lemma 5.1 as
desired.

Proof of Lemma 6.1. We repeat the proof of Lemma 5.1, but note that x(cid:63)
for all i and j if and only if x(cid:63)
condition for the former and substituting into Theorem A.2 yields Lemma 6.1 as desired.

i satisfy collinearity
j satisﬁes collinearity for all j. Using the latter as an equivalent

j and z(cid:63)

The main intuition behind the proof of Theorem A.2 is that a non-collinear solution to (A.2) cor-
responds to a high rank solution to (A.1) with rank(X(cid:63)) > 1. In turn, a rank-1 solution is unique

14

if and only if there exists no high-rank solutions; see [60, Theorem 2.4]. To make these ideas
rigorous, we begin by reviewing some preliminaries. First, without loss of generality, we can ﬁx
e = e1, that is, the ﬁrst canonical basis vector. If we wish to solve (A.2) with a different e = e(cid:48),
then we simply need to ﬁnd an orthonormal matrix U for which e(cid:48) = Ue, for example, using the
Gram-Schmidt process. Given a solution v1, v2, . . . , vn (A.2) with e = e1, setting v(cid:48)
j = Uvj yields
k, v(cid:48)
a solution v(cid:48)
j(cid:105) and
(cid:104)e1, vj(cid:105) = (cid:104)Ue1, Uvj(cid:105) = (cid:104)e(cid:48), v(cid:48)
The equivalence between (A.1) and (A.2) is established by using the solution to one problem to
construct a corresponding solution satisfying the following relationship

n to (A.2) with e = e(cid:48), because (cid:104)vk, vj(cid:105) = (cid:104)Uvk, Uvj(cid:105) = (cid:104)v(cid:48)

2, . . . , v(cid:48)

1, v(cid:48)

j(cid:105).

(cid:104)x, ej(cid:105) = (cid:104)e, vj(cid:105),

(cid:104)X, ejeT

k (cid:105) = (cid:104)vj, vk(cid:105),

for the other problem. In one direction, given a solution v1, v2, . . . , vn ∈ Rp to (A.2), the corre-
sponding solution to (A.1) is simply

x = [(cid:104)e, vj(cid:105)]n

j=1 = VT e,

X = [(cid:104)vj, vk(cid:105)]n

j,k=1 = VT V,

· · · vn] ∈ Rp×n. In the other direction, given a solution x and X to (A.1),

where V = [v1 v2
we factorize X − xxT = ˜VT ˜V so that
(cid:21)

(cid:20)1 xT
x X

= [e1 V]T [e1 V] , V =

(cid:21)

(cid:20)xT
˜V

= [v1 v2

· · · vn] ∈ Rp×n.

Then, v1, v2, . . . , vn is a corresponding solution to (A.2) with e = e1.

1, v(cid:63)

2, . . . , v(cid:63)

s(cid:105)| = (cid:107)v(cid:63)
s − v(cid:48)

s(cid:107), so we can have either (cid:104)X(cid:63) − X(cid:48), eseT

j = (cid:104)ej, x(cid:63)(cid:105)e to obtain a corresponding solution v(cid:63)

Proof of Theorem A.2. (⇒) Given a rank-1 solution X(cid:63) = x(cid:63)(x(cid:63))T of the relaxation (A.1), we set
x(cid:63)
j = (cid:104)ej, x(cid:63)(cid:105) and v(cid:63)
n to (A.2) that
satisﬁes collinearity. By contradiction, suppose that there exists another solution v(cid:48)
1, v(cid:48)
2, . . . , v(cid:48)
n to
(A.2) that does not satisfy collinearity, meaning that there exists some s such that |(cid:104)e, v(cid:48)
s(cid:105)| (cid:54)= (cid:107)v(cid:48)
s(cid:107).
Then, its corresponding solution x(cid:48), X(cid:48) is distinct from x(cid:63), X(cid:63), because |(cid:104)e, v(cid:48)
s(cid:107) but
s(cid:107)2 = 0 or (cid:104)x(cid:63) − x(cid:48), es(cid:105) =
|(cid:104)e, v(cid:63)
(cid:104)e, v(cid:63)
s(cid:105) = 0 but not both at the same time. This contradicts the hypothesis that X(cid:63) is a unique
solution.
(⇐) Without loss of generality, we assume that e = e1. Given a solution v(cid:63)
n to (A.2)
satisfying collinearity, we set x(cid:63)
j=1, and X(cid:63) = x(cid:63)(x(cid:63))T , in order to obtain a
corresponding rank-1 solution to (A.1). By contradiction, suppose that there exists another solution
x(cid:48), X(cid:48) to (A.1) that is distinct from x(cid:63), X(cid:63), with corresponding solution v(cid:48)
n to (A.2).
This solution v(cid:48)
1, v(cid:48)
n must satisfy collinearity, or else our hypothesis is immediately violated.
j=1 and X(cid:48) = x(cid:48)(x(cid:48))T . Then, the
Under collinearity, we again set x(cid:48)
following

j(cid:105) such that x(cid:48) = [x(cid:48)

j (cid:105), x(cid:63) = [x(cid:63)

s(cid:105)| (cid:54)= (cid:107)v(cid:48)

j = (cid:104)e, v(cid:63)

j = (cid:104)e, v(cid:48)

s(cid:107)2 − (cid:107)v(cid:48)

s (cid:105) = (cid:107)v(cid:63)

2, . . . , v(cid:63)

2, . . . , v(cid:48)

2, . . . , v(cid:48)

1, v(cid:63)

1, v(cid:48)

j ]n

j]n

yields another solution, since

vj =

e1(x(cid:63)

j + x(cid:48)

j) +

e2(x(cid:63)

j − x(cid:48)
j)

1
2

(cid:104)e1, vj(cid:105) =

(cid:104)vj, vk(cid:105) =

=

1
2
1
4
1
2

(x(cid:63)

j + x(cid:48)

j) =

((cid:104)e1, v(cid:63)

j(cid:105))

j (cid:105) + (cid:104)e1, v(cid:48)
1
4

j − x(cid:48)

(x(cid:63)

(x(cid:63)

j + x(cid:48)

j)(x(cid:63)

k) +

j)(x(cid:63)

k − x(cid:48)

k) =

1
2

(x(cid:63)

j x(cid:63)

k + x(cid:48)

jx(cid:48)
k)

((cid:104)v(cid:63)

j , v(cid:63)

k(cid:105) + (cid:104)v(cid:48)

j, v(cid:48)

k(cid:105))

In order for x(cid:48), X(cid:48) is distinct from x(cid:63), X(cid:63), there must be some choice of s such that x(cid:63)
this means that vs does not satisfy collinearity, since (cid:104)e2, vs(cid:105) = 1
s − x(cid:48)
the hypothesis that all solutions v1, v2, . . . , vn to (A.2) satisfy collinearity.

s, but
s) (cid:54)= 0. This contradicts

s (cid:54)= x(cid:48)

2 (x(cid:63)

B Projection onto ReLU Feasbility Set

Fix e, x ∈ Rp and ˆz ∈ R. Let α = max{(cid:104)e, x(cid:105), 0}, and deﬁne φ as the projection distance onto the
spherical cap deﬁned by the “ReLU feasible set” (5.4), restated here as

φ = min
z∈Rp

(cid:107)z − ˆze(cid:107)

s.t.

(cid:104)e, z(cid:105) ≥ α,

(cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105).

(B.1)

15

1
2

1
2
k + x(cid:48)

In the main text, we used intuitive, geometric arguments to prove that

φ =

(cid:26)α − ˆz

(cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107)

ˆz ≤ α,
ˆz > α.

(B.2)

In this section, we will rigorously verify (B.2) and then prove that the conditional statements are
unnecessary, in that φ simply takes on the larger of the two values, as in

φ = max{α − ˆz,

(cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107)}.

(B.3)

This was stated in the main text as Lemma 5.2.

We ﬁrst rigorously verify (B.2) by: 1) relaxing a constraint for a speciﬁed case; 2) solving the
relaxation in closed-form; 3) verifying that the closed-form solution satisﬁes the original constraints,
and must therefore be optimal for the original problem. In the case of ˆz ≤ α, the following relaxation

φlb1 = min
z∈Rp

{(cid:107)z − ˆze(cid:107)

:

(cid:104)e, z(cid:105) ≥ α}

has solution z(cid:63) = αe that is clearly feasible for (B.1) since (cid:107)z(cid:63)(cid:107)2 = (cid:104)z(cid:63), x(cid:105) = α2. Hence, this z(cid:63)
must be optimal; its objective (cid:107)z(cid:63) − ˆze(cid:107) = α − ˆz yields the desired value of φ.

In the case of ˆz > α, the following relaxation

φlb2 = min
z∈Rp

{(cid:107)z − ˆze(cid:107)2

:

(cid:107)z(cid:107)2 ≤ (cid:104)z, x(cid:105)},

must have an active constraint at optimality. Otherwise, the solution would be z = ˆze, but this cannot
be feasible as ˆz2 = (cid:107)z(cid:107) ≤ (cid:104)z, x(cid:105) = ˆz(cid:104)e, x(cid:105) ≤ ˆzα would contradict ˆz > α ≥ 0. Applying Lagrange
multipliers, the solution reads z(cid:63) = t · ˆze + (1 − t) · x/2 where t = (cid:107)x/2(cid:107)/(cid:107)ˆze − x/2(cid:107) is chosen to
make the constraint active. We will need the following lemma to verify that (cid:104)e, z(cid:63)(cid:105) ≥ α.

Lemma B.1. Let |v| ≤ R. If u >

R2 − v2, then Ru/

u2 + v2 ≥

R2 − v2.

√

√

√

Proof. We will prove that if u2 + v2 > R2 then R2u2/(u2 + v2) + v2 ≥ R2. By contradiction,
suppose that R2u2/(u2 + v2) + v2 < R.
If u2 + v2 = 0, then the premise is already false.
Otherwise, we multiply by u2 + v2 > 0 to yield R2u2 + v2(u2 + v2) < R2(u2 + v2), or equivalently
v2(u2 + v2 − R2) < 0. This last condition is only possible if v (cid:54)= 0 and u2 + v2 < R2, but this
again contradicts the premise.

For u = 2ˆz − (cid:104)e, x(cid:105), v = (cid:112)(cid:107)x(cid:107)2 − (cid:104)e, x(cid:105)2, and R = (cid:107)x(cid:107), observe that

t =

(cid:107)x/2(cid:107)
(cid:107)ˆze − x/2(cid:107)

=

√

R
u2 + v2

,

α = max{(cid:104)e, x(cid:105), 0} =

(cid:104)e, x(cid:105)
2

+

|(cid:104)e, x(cid:105)|
2

.

Then, z(cid:63) = t · ˆze + (1 − t) · x/2 is feasible for (B.1), because substituting u, v, R into Lemma B.1
yields

ˆz > α ⇐⇒ ˆz −

(cid:104)e, x(cid:105)
2

>

|(cid:104)e, x(cid:105)|
2

(cid:18)

=⇒ t ·

ˆz −

(cid:19)

(cid:104)e, x(cid:105)
2

≥

|(cid:104)e, x(cid:105)|
2

,

and this in turn implies that

(cid:104)e, z(cid:63)(cid:105) =

(cid:104)e, x(cid:105)
2

(cid:18)

+ t ·

ˆz −

(cid:19)

(cid:104)e, x(cid:105)
2

≥

(cid:104)e, x(cid:105)
2

+

|(cid:104)e, x(cid:105)|
2

= α.

Hence, this z(cid:63) must be optimal; its objective (cid:107)z(cid:63) − ˆze(cid:107) = (1 − t)(cid:107)ˆze − x/2(cid:107) yields the desired
value of φ.

Finally, we prove (5.5) by showing that the conditional statements in (B.2) are unnecessary.

Proof of Lemma 5.2. If ˆz > α, then clearly φ = φlb2 ≥ 0 by construction, but α − ˆz < 0, so
φ = max{α− ˆz, φlb2} as desired. For ˆz ≤ α, we will proceed by examining two cases. First, suppose
that ˆz ≥ 0 and hence α = (cid:104)e, x(cid:105) and ˆz ≤ (cid:104)e, x(cid:105). Then, (cid:107)ˆze − x/2(cid:107)2 − (cid:107)x/2(cid:107)2 = ˆz(ˆz − (cid:104)e, x(cid:105)) ≤ 0,
and (cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107) ≤ 0, so φ = max{φlb1, (cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107)} as desired. In the case

16

of ˆz ≤ 0, Lemma C.1 shows that (cid:104)e, x(cid:105) − ˆz ≤ ρ implies (cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107) ≤ ρ, since with
u = (cid:104)e, x(cid:105), v = (cid:112)(cid:107)x(cid:107)2 − (cid:104)e, x(cid:105)2, c = |ˆz|, and a = ρ, we have

(cid:107)x/2 + |ˆz|e(cid:107) − (cid:107)x/2(cid:107) ≤ ρ ⇐⇒

(cid:104)e, x(cid:105) + |ˆz|
ρ

(cid:115)

≤

1 +

(cid:107)x(cid:107)2 − (cid:104)e, x(cid:105)2
ˆz2 − ρ2

but (cid:104)e, x(cid:105) − ˆz ≤ ρ already implies 1
implies (cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107) ≤ φlb1 shows that we have φ = max{φlb1, (cid:107)ˆze − x/2(cid:107) − (cid:107)x/2(cid:107)}.

ρ [(cid:104)e, x(cid:105) + |ˆz|] ≤ 1. In particular, the fact that (cid:104)e, x(cid:105) − ˆz ≤ φlb1

C Projection onto a hyperbola

Fix e, x ∈ Rp and ˆx, ˆz, ρ ∈ R such that ˆz > ρ > 0. Deﬁne ψ as the projection distance onto the
hyperboloidal cap (5.4), restated here

ψ = min
x∈Rp

(cid:107)x − ˆxe(cid:107)

s.t.

(cid:104)e, x(cid:105) − ˆz ≤ ρ,

(cid:107)2ˆze − x(cid:107) − (cid:107)x(cid:107) ≤ 2ρ.

(C.1)

Without loss of generality, we can ﬁx e = e1 (see Appendix A), and split the coordinates of x as in
u = x[1] and v[j] = x[1 + j] for j ∈ {1, 2, . . . , p − 1} to rewrite (C.1) as the following

ψ2 =

min
(u,v)∈Rp

(u − ˆx)2 + (cid:107)v(cid:107)2

(C.2)

s.t. u − ˆz ≤ ρ, (cid:112)(u − 2ˆz)2 + (cid:107)v(cid:107)2 − (cid:112)u2 + (cid:107)v(cid:107)2 ≤ 2ρ.

Observe that the variable v ∈ Rp−1 only appears in (C.2) via its norm (cid:107)v(cid:107). Hence, (C.2) is equivalent
to the following problem

ψ2 = min
u,v∈R

(u − ˆx)2 + v2

(C.3)

s.t. u − ˆz ≤ ρ, (cid:112)(u − 2ˆz)2 + v2 −

(cid:112)

u2 + v2 ≤ 2ρ,

and a solution v(cid:63) to (C.2) can be recovered from a solution v(cid:63) to (C.3) by picking any unit vector
s ∈ Rp−1 with (cid:107)s(cid:107) = 1 and setting v(cid:63) = v(cid:63)s. We have reduced the projection over a hyperboloid
(C.1) into a projection onto a hyperbola (C.3) by taking a quotient over the minor-axis directions. To
proceed, we will need the following technical lemma, which is mechancially derived by completing
the square and collecting terms.

Lemma C.1. Given semi-major axis a > 0, semi-minor axis b > 0, and focus c =
following hold

(cid:112)(u − 2c)2 + v2 −

(cid:112)

u2 + v2 ≤ 2a ⇐⇒

(cid:112)(u + 2c)2 + v2 −

(cid:112)

u2 + v2 ≤ 2a ⇐⇒

u − c
a
u + c
a

(cid:114)

≥

1 +

(cid:114)

≤

1 +

v2
b2 ,
v2
b2 .

√

a2 + b2, the

(C.4a)

(C.4b)

We use Lemma C.1 to rewrite the hyperbolic constraint in (C.3) in quadratic form, as in

ψ2 = min
u,v∈R

(u − ˆx)2 + v2

s.t.

u − ˆz
ρ

≤ 1,

(u − ˆz)2
ρ2

−

v2

ˆz2 − ρ2 ≤ 1.

(C.5)

We will need the following to solve (C.5). This is the main result of this section.
Theorem C.2 (Axial projection onto a hyperbola). The problem data a, x ∈ Rm, c ∈ Rm and b ∈ R
satisfy

a, c (cid:54)= 0,
if and only if the following projection

|(cid:104)a, x(cid:105) − b| − 1 < (cid:107)a(cid:107)2/(cid:107)c(cid:107)2

(u(cid:63), v(cid:63)) = arg min
u,v

(cid:8)(cid:107)u − x(cid:107)2 + (cid:107)v(cid:107)2 : ((cid:104)a, u(cid:105) − b)2 − (cid:104)c, v(cid:105)2 ≤ 1(cid:9)

has a unique solution

((cid:104)a, x(cid:105) − b)
(cid:107)a(cid:107)2

(cid:18)

1 −

1
|(cid:104)a, x(cid:105) − b|

(cid:19)

,

u(cid:63) = x − a

v(cid:63) = 0.

17

The proof of Theorem C.2 will span the remainder of this section. Lemma 5.3 is clearly a special
instance as applied to (C.5).

Proof of Lemma 5.3. If ˆx ≥ ˆz − ρ, then relaxing the hyperbolic constraint in (C.5) yields a unique
solution of u(cid:63) = min{ˆx, ˆz + ρ} and v(cid:63) = 0. Indeed, this solution also satisﬁes the hyperbolic
constraint, and is therefore optimal for (C.5). Otherwise, if ˆx < ˆz − ρ, then we will use relax the
linear constraint in (C.5) and apply Theorem C.2. Here, a = 1/ρ, b = ˆz/ρ, c = 1/(cid:112)ˆz2 − ρ2, and
x = ˆx, and the condition for (C.5) to have a unique condition u(cid:63) and v(cid:63) with v(cid:63) = 0 is

|ˆx − ˆz|/ρ − 1 < (ˆz2 − ρ2)/ρ2

⇐⇒

|ˆx − ˆz| < ˆz2/ρ.

(C.6)

It is easy to verify that the resulting solution is feasible for (C.5), and hence optimal. Under the
premise ˆx − ˆz < −ρ < 0, the condition (C.6) is just ˆx > ˆz − ˆz2/ρ, which also implies ˆx ≥ ˆz − ρ
because ˆz > ρ. Hence, we have covered both cases; the condition (ˆz − ˆx) < ˆz2/ρ guarantees a
unique u(cid:63) and v(cid:63) = 0 as claimed.

We will now prove Theorem C.2. The Euclidean projection onto a hyperbola is the minimization of
one quadratic function subject to another quadratic function. This is well-known to be a tractable
problem via the S-procedure (see e.g. [52, p. 655] or [53]). In its original form, it states that for two
quadratics f (x) and g(x) for which there exists x0 satisfying g(x0) < 0, that

f (x) ≥ 0

holds for all x satisfying g(x) ≤ 0

if and only if there exists λ ≥ 0 such that

Clearly, a corollary of the S-procedure is strong duality, as in

f (x) + λg(x) ≥ 0

holds for all x.

min
x

{f (x) : g(x) ≤ 0) = max
λ≥0

min
x

{f (x) + λg(x)},

and so the Karush–Kuhn–Tucker conditions allow us to solve the primal by solving the dual, assuming
the existence of a strictly feasible point x0 with g(x0) < 0. To proceed, we will need the following
technical lemma, which is mechancially derived by applying the Sherman-Morrison identity.
Lemma C.3 (Rank-1 update). Given a, x ∈ Rm, b ∈ R, and λ > −1/(cid:107)a(cid:107)2, the following projection

u(cid:63) = arg min
u∈Rn

{(cid:107)u − x(cid:107)2 + λ((cid:104)a, u(cid:105) − b)2}

has a unique solution u(cid:63) satisfying

u(cid:63) = x − λa

(cid:18) (cid:104)a, x(cid:105) − b
1 + λ(cid:107)a(cid:107)2

(cid:19)

,

(cid:104)a, u(cid:63)(cid:105) − b =

(cid:104)a, x(cid:105) − b
1 + λ(cid:107)a(cid:107)2 .

(cid:107)u(cid:63) − x(cid:107)2 + λ((cid:104)a, u(cid:63)(cid:105) − b)2 =

λ((cid:104)a, x(cid:105) − b)2
1 + λ(cid:107)a(cid:107)2

We will actually solve the most general form of the projection problem.
Lemma C.4 (General projection onto a single hyperbola). Let a, x ∈ Rm, c, y ∈ Rm and b, d ∈ R
satisfy a, c (cid:54)= 0. Let u(cid:63) ∈ Rm, v(cid:63) ∈ Rm be solutions to the projection

φ = min
u,v

(cid:8)(cid:107)u − x(cid:107)2 + (cid:107)v − y(cid:107)2 : ((cid:104)a, u(cid:105) − b)2 − ((cid:104)c, v(cid:105) − d)2 ≤ 1(cid:9) ,

and let λ(cid:63) be the unique solution to the Lagrangian dual
(cid:20) ((cid:104)a, x(cid:105) − b)2

(cid:26)

φlb = max

0≤λ≤1/(cid:107)c(cid:107)2

λ

1 + λ(cid:107)a(cid:107)2 −

((cid:104)c, y(cid:105) − d)2
1 − λ(cid:107)c(cid:107)2 − 1

(cid:21)(cid:27)

.

Then, φ = φlb. Moreover the primal solutions are unique if and only if λ(cid:63) < 1/(cid:107)c(cid:107)2, with values

u(cid:63) = x − λ(cid:63)a

(cid:18) (cid:104)a, x(cid:105) − b
1 + λ(cid:63)(cid:107)a(cid:107)2

(cid:19)

,

v(cid:63) = y + λ(cid:63)c

(cid:18) (cid:104)c, y(cid:105) − d
1 − λ(cid:63)(cid:107)c(cid:107)2

(cid:19)

.

18

Proof. We deﬁne the following two quadratics and corresponding Lagrangian

f (u, v) = (cid:107)u − x(cid:107)2 + (cid:107)v − y(cid:107)2,
g(u, v) = ((cid:104)a, u(cid:105) − b)2 − ((cid:104)c, v(cid:105) − d)2 − 1,
L(u, v, λ) = f (u, v) + λg(u, v).
Note that u0 = ba/(cid:107)a(cid:107) and v0 = dc/(cid:107)c(cid:107) satisﬁes g(u0, v0) < 0, so strong duality holds via the
S-procedure. Next, we apply Lemma C.3 to yield the Lagrangian dual φlb via

min
u,v

L(u, v, λ) =

(cid:40)

λ

(cid:104) ((cid:104)a,x(cid:105)−b)2

1+λ(cid:107)a(cid:107)2 − ((cid:104)c,y(cid:105)−d)2

(cid:105)
1−λ(cid:107)c(cid:107)2 − 1

−∞

λ ≤ 1/(cid:107)c(cid:107)2,
λ > 1/(cid:107)c(cid:107)2.

It is easy to verify that the dual function above is strongly concave over λ, so the solution λ(cid:63) is unique.
Finally, if λ(cid:63) < 1/(cid:107)c(cid:107)2, then the Lagrangian L(u, v, λ(cid:63)) is strongly convex, and the primal solutions
u(cid:63) and v(cid:63) are both uniquely determined by minimizing L(u, v, λ(cid:63)). Otherwise, if λ(cid:63) = 1/(cid:107)c(cid:107)2, then
L(u, v, λ(cid:63)) is weakly convex over v. Here, u(cid:63) is uniquely determined by minimizing L(u, v, λ(cid:63)),
but v(cid:63) can be any choice that satisﬁes primal feasibility ((cid:104)a, u(cid:63)(cid:105) − b)2 − ((cid:104)c, v(cid:63)(cid:105) − d)2 = 1, and is
therefore nonunique.

Finally, we prove Theorem C.2 using Lemma C.4.

Proof of Theorem C.2. The axial projection problem of Theorem C.2 is an instance of the more
general projection problem in Lemma C.4 with y = 0 and d = 0. The intended claim holds so long
as λ(cid:63) < 1/(cid:107)c(cid:107)2. Now, ﬁrst order optimality in the Lagrangian dual reads
((cid:104)a, x(cid:105) − b)2
(1 + λ(cid:63)(cid:107)a(cid:107)2)2 −

((cid:104)c, y(cid:105) − d)2
(1 − λ(cid:63)(cid:107)c(cid:107)2)2 − 1 = 0,

and this implies 1+λ(cid:63)(cid:107)a(cid:107)2 = |(cid:104)a, x(cid:105)−b| and hence λ(cid:63) = (|(cid:104)a, x(cid:105)−b|−1)/(cid:107)a(cid:107)2. Finally, imposing
the bound λ(cid:63) < 1/(cid:107)c(cid:107)2 on this value yields our desired claim.

D Projection onto several hyperbolas

Given W = [Wi,j] ∈ Rm×n, ˆx = [ˆxj] ∈ Rn, ˆz = [ˆzi] ∈ Rm, e ∈ Rp, and ρi satisfying ˆzi > ρ > 0,
we will partially solve

min
xj ∈Rp

(cid:80)

j(cid:107)xj − ˆxje(cid:107)2

s.t.

(cid:107)ˆzie − (cid:80)

jWi,jxj/2(cid:107) − (cid:107)(cid:80)

jWi,jxj(cid:105) − ˆzi ≤ ρi for all i,
jWi,jxj/2(cid:107) ≤ ρi for all i.

(D.1)

(cid:104)e, (cid:80)

Without loss of generality, we can ﬁx e = e1 and split the coordinates of xj as in u[j] = xj[1] for all
j and vk[j] = xj[1 + k] for all j, k to rewrite (D.1) as the following

(cid:107)u − ˆx(cid:107)2 + (cid:80)

k(cid:107)vk(cid:107)2

(D.2)

min
u,vj ∈Rn
s.t.

(cid:104)wi, u(cid:105) − ˆzi ≤ ρi,
(cid:113)

(cid:113)

((cid:104)wi, u(cid:105) − 2ˆzi)2 + (cid:80)

k(cid:104)wi, vk(cid:105)2 −
for all i, where wi[j] = W[i, j] is the i-th row of W. Applying Lemma C.1 then rewrites (D.2) as
the following.

k(cid:104)wi, vk(cid:105)2 ≤ 2ρi,

(cid:104)wi, u(cid:105)2 + (cid:80)

min
u,vj ∈Rn

(cid:107)u − ˆx(cid:107)2 + (cid:80)

k(cid:107)vk(cid:107)2

s.t.

1 +

(cid:115)

(cid:80)

k(cid:104)wi, vk(cid:105)2
i − ρ2
ˆz2
i

≤

(cid:104)wi, u(cid:105) − ˆz
ρi

≤ 1,

(D.3)

We will need the following to solve (D.3). This is the main result of this section.
Theorem D.1 (Axial projection onto several hyperbolas). If the problem data x ∈ Rm, ai ∈ Rm,
bi ∈ R, ci ∈ Rn for i ∈ {1, 2, . . . , (cid:96)} satisfy

(cid:107)C(cid:107)2 · ((cid:107)(AAT )−1(Ax − b)(cid:107)∞ + (cid:107)(AAT )−1(cid:107)∞) < 1

where A[i, j] = ai[j], b[i] = bi, and C[i, j] = ci[j] for all i, j, then the following projection

(u(cid:63), v(cid:63)) = arg min
u,v
has a unique solution (u(cid:63), v(cid:63)) with v(cid:63)

(cid:110)
(cid:107)u − x(cid:107)2 + (cid:80)

j = 0.

j(cid:107)vj(cid:107)2 : ((cid:104)ai, u(cid:105) − bi)2 − (cid:80)

j(cid:104)ci, vj(cid:105)2 ≤ 1

(cid:111)

for all i

19

The proof of Theorem D.1 will span the remainder of this section. Lemma 6.2 is clearly a special
instance as applied to (D.3).

Proof of Lemma 6.2. Write D1 = diag(ρi) and D2 = diag((cid:112)ˆz2
rem D.1 with x = ˆx, A = D−1

1 ˆz, and C = D−1
2 W(cid:107)2 ≤ (cid:107)W(cid:107)2/(ˆz2
(cid:107)(AAT )−1(Ax − b)(cid:107)∞ = (cid:107)D1(WWT )−1(Wˆx − ˆz)(cid:107)∞ ≤ ρmax(cid:107)(WWT )−1(Wˆx − ˆz)(cid:107)∞

i − ρ2
2 W. Clearly
min − ρ2

1 W, b = D−1

i ). Then, we apply Theo-

(cid:107)C(cid:107)2 = (cid:107)D−1

max)

(cid:107)D1(WWT )−1D1(cid:107)∞ ≤ ρ2

max(cid:107)(WWT )−1(cid:107)∞

and hence the condition in Theorem D.1 is the following

ρmax(cid:107)W(cid:107)2(cid:107)(WWT )−1(Wˆx − ˆz)(cid:107)∞ + ρ2

max(cid:107)W(cid:107)2(cid:107)(WWT )−1(cid:107)∞ < ˆz2

min − ρ2

max

which is the same condition stated in Lemma 6.2.

Our proof of Theorem D.1 is based on a SDP relaxation.

Proof of Theorem D.1. The problem is nonconvex over v, but a convex relaxation is easily con-
structed by representing the quadratic outer product (cid:80)

k by V (cid:23) 0, as in

kvkvT

minimize
u∈Rm,v∈Rn

1
2

(cid:107)u − x(cid:107)2 +

1
2

tr(V)

subject to − 1 ≤ (cid:104)ai, u(cid:105) − bi ≤

(cid:113)

1 + (cid:104)cicT

i , V(cid:105)

for all i ∈ {1, 2, . . . , (cid:96)}

with the relaxation being exact whenever V(cid:63) = 0. The corresponding Lagrangian is

L(u, V, ξ, µ) =

1
2

(cid:107)u − x(cid:107)2 +

1
2

tr(V) + (ξ − µ)T (Au − b) −

(cid:96)
(cid:88)

(cid:20)

(cid:113)

ξi

i=1

1 + (cid:104)cicT

i , V(cid:105) + µi

(cid:21)

,

over Lagrange multipliers ξ, µ ≥ 0. Assuming that AT A (cid:54)= 0, this problem has strictly feasible
primal point u = (AT A)−1b and V = I, and strictly feasible dual point ξ = µ = (cid:15)1 for (cid:15) > 0.
Hence, strong duality is attained as in

min
V(cid:23)0,u

max
λ,µ≥0

L(u, V, ξ, µ) = max
λ,µ≥0

min
V(cid:23)0,u

L(u, V, ξ, µ).

Examining the inner minimization over V (cid:23) 0, note that the associated optimiality conditions read

(cid:32)

q
(cid:88)

1
2

∇VL(u, V(cid:63), ξ, µ) = S =

ξicicT
i
(cid:112)1 + (cid:104)cicT
i , V(cid:63)(cid:105)
Hence, the minimum is attained at V(cid:63) = 0 if and only if (cid:80)
the dual for the optimal Lagrange multiplier ξ(cid:63) and verify that (cid:80)
In the case that V(cid:63) = 0, the corresponding u(cid:63) is unique

I −

i=1

(cid:33)

(cid:23) 0,

SV(cid:63) = 0.

i ξicicT
i ≺ I. We will proceed to solve
i cicT
i ξ(cid:63)

i ≺ I is satisﬁed.

u(cid:63) = arg min
u

L(u, 0, λ, µ) = arg min

u

1
2

(cid:107)u − x(cid:107)2 + yT (Au − b) = x − AT y

where y = ξ − µ, and the dual problem is written
(cid:26) 1
2

L(u, 0, ξ, µ) = − min

max
ξ,µ≥0

min
u

y

(cid:107)AT y(cid:107)2 − yT (Ax − b) + (cid:107)y(cid:107)1

(cid:27)

.

whose optimal conditions read

AAT y(cid:63) − (Ax − b) ∈ sign(y(cid:63))

where sign(α) =






+1
α > 0,
[−1, +1] α = 0,
α < 0.
−1

20

We wish to impose conditions on the data A, b, x to ensure that λmax(max{0, y(cid:63)
i ) < 1 holds
at dual optimality. A conservative condition is to use the enclosure sign(α) ⊂ [−1, +1] to solve a
relaxation

i }cicT

(cid:40)

max
y

λmax

(cid:32)

(cid:88)

(cid:33)

(cid:41)

max{0, eT

i y}cicT
i

: y = (AAT )−1(Ax − b − s),

s ∈ sign(y)

cicT
i

cicT
i

i
(cid:33)

(cid:33)

(cid:33)

· max
y

· max
y

(cid:110)

(cid:110)

max
i

max
i

≤λmax

≤λmax

=λmax

(cid:32)

(cid:88)

i

(cid:32)

(cid:88)

i

(cid:32)

(cid:88)

i

{0, eT

i y} : y = (AAT )−1(Ax − b − s),

{0, eT

i y} : y = (AAT )−1(Ax − b − s),

s ∈ sign(y)

(cid:111)

(cid:111)

(cid:107)s(cid:107)∞ ≤ 1

cicT
i

· max
i

(cid:8)0, eT

i (AAT )−1(Ax − b − s)(cid:9) .

Hence, if the following holds



(cid:107)C(cid:107)2 · max

i



i (AAT )−1(Ax − b) +
eT

n
(cid:88)

j=1

|eT

i (AAT )−1ej|






< 1

or more conservatively, if the following holds

(cid:107)C(cid:107)2 · ((cid:107)(AAT )−1(Ax − b)(cid:107)∞ + (cid:107)(AAT )−1(cid:107)∞) < 1

then V(cid:63) = 0 and u(cid:63) is unique as desired.

E Further details for multiple layers

In the general (cid:96)-layer case, the SDP relaxation for problem (A) reads
d2
lb = min
Gk(cid:23)0

tr(X0) − 2(cid:104)ˆx, x0(cid:105) + (cid:107)ˆx(cid:107)2

s.t.

xk+1 ≥ 0, xk+1 ≥ Wkxk + bk,
diag(Xk+1) ≤ diag(WkYT
(cid:104)w, x(cid:96)(cid:105) + b ≤ 0,

k ),

Gk =





xT

xT
1
k
k+1
xk Xk Yk
xk+1 YT
k Xk+1

(A-lb)



 (cid:23) 0 for all k,

over layer indices k ∈ {0, 1, . . . , (cid:96) − 1}, while the SDP relaxation for problem (B) is almost identical,
except the constraint on x(cid:96):
L2

tr(X0) − 2(cid:104)ˆx, x0(cid:105) + (cid:107)ˆx(cid:107)2

(B-lb)

lb = min
Gk(cid:23)0

s.t.

xk+1 ≥ 0, xk+1 ≥ Wkxk + bk,
diag(Xk+1) ≤ diag(WkYT
k ),
tr(X(cid:96)) − 2(cid:104)ˆz, x(cid:96)(cid:105) + (cid:107)ˆz(cid:107)2 ≤ ρ2,

Gk =





xT

xT
1
k
k+1
xk Xk Yk
xk+1 YT
k Xk+1



 (cid:23) 0 for all k,

over layer indices k ∈ {0, 1, . . . , (cid:96) − 1}. Note that both (A-lb) and (B-lb) are SDPs over (cid:96) smaller
semideﬁnite variables, each of order 1 + nk + nk+1, rather than over a single large semideﬁnite
variable of order 1 + (cid:80)(cid:96)
k=1 nk. This reduction is from an application of the chordal graph matrix
completion of Fukuda et al. [61]; see also [62, Chapter 10].
Now, for the choice ˆz = u − ρw/(cid:107)w(cid:107) where u = −bw/(cid:107)w(cid:107)2, the optimal value L(cid:63) to problem (B)
gives an upper-bound to the optimal value L(cid:63) ≥ d(cid:63) of problem (A) that converges to an equality
at ρ → ∞. At the same time, Llb ≥ dlb holds for all ρ > 0 because problem (A-lb) is always a
relaxation of problem (B-lb). To show this, we observe that for this choice of ˆz, we have

tr(X(cid:96)) − 2(cid:104)ˆz, x(cid:96)(cid:105) + (cid:107)ˆz(cid:107)2 − ρ2

=tr(X(cid:96)) − 2(cid:104)u, x(cid:96)(cid:105) + (2ρ/(cid:107)w(cid:107))(cid:104)w, x(cid:96)(cid:105) + (cid:107)u(cid:107)2 − (2ρ/(cid:107)w(cid:107))(cid:104)w, u(cid:105) + ρ2 − ρ2
= tr(X(cid:96)) − 2(cid:104)u, x(cid:96)(cid:105) + (cid:107)u(cid:107)2
(cid:123)(cid:122)
(cid:125)
≥0

+(2ρ/(cid:107)w(cid:107))[(cid:104)w, z(cid:105) + b].

(cid:124)

21

The nonnegativity of this ﬁrst term follows because

tr(X(cid:96)) − 2(cid:104)u, x(cid:96)(cid:105) + (cid:107)u(cid:107)2 = tr(X(cid:96) − uxT
= tr(X(cid:96) − x(cid:96)xT
= tr(X(cid:96) − x(cid:96)xT

(cid:96) − x(cid:96)uT + uuT )
(cid:96) + (x(cid:96) − u)(x(cid:96) − u)T )
(cid:96) ) + (cid:107)x(cid:96) − u(cid:107)2

and that

(cid:21)

(cid:20) 1
xT
(cid:96)
x(cid:96) X(cid:96)

(cid:23) 0 implies X(cid:96) − x(cid:96)xT

(cid:96) (cid:23) 0 by the Schur complement lemma and therefore

tr(X(cid:96) − x(cid:96)xT
(cid:96) ) ≥ 0. Hence, a feasible point X(cid:96), x(cid:96) for the relaxation (B-lb) satisfying tr(X(cid:96)) −
2(cid:104)ˆz, x(cid:96)(cid:105) + (cid:107)ˆz(cid:107)2 ≤ ρ2 must immediately satisfy (cid:104)w, z(cid:105) + b ≤ 0 and therefore be feasible for the
relaxation (A-lb).
If the relaxation (A-lb) is tight, meaning that d(cid:63) = dlb, then the the relaxation (B-lb) must automat-
ically be tight at ρ → ∞, because d(cid:63) = L(cid:63) ≥ Llb ≥ dlb = d(cid:63). But the converse need not hold:
the relaxation (A-lb) can still be loose even though (B-lb) is tight, because even with L(cid:63) = Llb at
ρ → ∞, it is still possible to have dlb < d(cid:63).
The nonlinear interpretation of (B-lb) reads

min
x(k)
i ∈Rp

(cid:80)

j(cid:107)x0,j − ˆxje(cid:107)2

s.t.

(cid:104)e, x(k+1)
i
(cid:107)x(k+1)
i

(cid:105) ≥ max
(cid:107)2 ≤ (cid:104)x(k+1)

(cid:110)
0, (cid:104)e, (cid:80)
i,j x(k)
, (cid:80)
i,j x(k)
j(cid:107)x(cid:96),j − ˆzje(cid:107)2 ≤ ρ2,

jW (k)
jW (k)

(cid:80)

i

j + b(k)
j + b(k)

i e(cid:105)
i e(cid:105),

(cid:111)

,

for all i, k

(E.1)

over layer indices k ∈ {0, 1, . . . , (cid:96) − 1} and neuron indices i ∈ {1, 2, . . . , n} at each k-th layer.
Suppose that problem (B) has a trivial solution x(cid:63) = ˆx with objective zero. Then, it follows that
every solution to (E.1) must be collinear and satisfy x(cid:63)
0,j = ˆxje, so the relaxation (B-lb) has a unique
rank-1 solution via Theorem A.2.

0,j = ˆxje for all j at the input layer. This choice of x(cid:63)

Proof of Corollary 2.3. If (cid:107)f (ˆx) − ˆz(cid:107) ≤ ρ, then problem (E.1) has a minimum of zero, obtained by
setting x(cid:63)
0,j −
ˆxje(cid:107)2 = 0 holds if and only if x(cid:63)
0,j = ˆxje, so the input layer must be collinear at optimality,
meaning that (cid:107)x(cid:63)
0,j(cid:105)| for all j is guaranteed to hold. Then, applying Lemma A.3 shows
1,i − b(1)
1,i − b(1)
that (cid:107)x(cid:63)
i e(cid:105)| and therefore (cid:107)x(cid:63)
1,i(cid:105)| for all i, so the ﬁrst
hidden layer is also collinear. Inductively repeating this argument, if the k-th layer is collinear, as in
(cid:107)x(cid:63)
k,j(cid:105)| for all j, then Lemma A.3 shows that the (k + 1)-th layer is also collinear, as in
(cid:107)x(cid:63)
k,j(cid:105)|
for all j, k. Evoking Theorem A.2 then yields our desired claim.

k+1,i(cid:105)| for all i. Hence, all solutions to (E.1) are collinear, as in (cid:107)x(cid:63)

0,j(cid:107) = |(cid:104)e, x(cid:63)
i e(cid:107) = |(cid:104)e, x(cid:63)

k,j(cid:107) = |(cid:104)e, x(cid:63)
k+1,i(cid:107) = |(cid:104)e, x(cid:63)

0,j is unique, because (cid:80)

1,i(cid:107) = |(cid:104)e, x(cid:63)

k,j(cid:107) = |(cid:104)e, x(cid:63)

j(cid:107)x(cid:63)

F The Rank-2 Burer–Monteiro Algorithm

The Burer-Monteiro algorithm is obtained by using a local optimization algorithm to solve the
nonconvex interpretation of (B-lb) stated in (E.1). In particular, ﬁx p = 2, deﬁne at the k-th layer
uk[j] = (cid:104)e, x(k)
(cid:105)2 yields the rank-2 Burer–Monteiro problem
(cid:105) and vk[j] =
j
k, v(cid:63)
k} to (BM2) satisfying v(cid:63)
(BM2) as desired. In turn, given a solution {u(cid:63)
k = 0, we recover a rank
deﬁcient rank-2 solution to (E.1) with x(k)
j = u(cid:63)
k[j]e. This rank-deﬁcient solution is guaranteed to be
globally optimal if it satisﬁes ﬁrst- and second-order optimality; see Burer and Monteiro [56] and
also [57, 58] and in particular [59, Lemma 1].

j (cid:107)2 − (cid:104)e, x(k)

(cid:107)x(k)

(cid:113)

j

solves

implementation

problem (BM2)

fmincon with
Our MATLAB
algorithm=’interior-point’, starting from an initial point selected i.i.d.
from the unit
Gaussian, and terminating at relative tolerances of 10−8. If the algorithm gets stuck at a spurious
local minimum with v(cid:63)
0 (cid:54)= 0, or if more than 300 iterations or function calls are made, then we restart
with a new random initial point; we give up after 5 failed attempts. Empirically, we observed that
whenever the SDP relaxation is tight, fmincon would consistently converge to a globally optimal
solution satisfying v(cid:63)
0 ≈ 0 within 80 iterations of the ﬁrst attempt; this suggests an underlying “no
spurious local minima” result like that of Boumal et al. [57].

using

22

