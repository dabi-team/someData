2
2
0
2

g
u
A
1
1

]

G
L
.
s
c
[

1
v
0
4
7
5
0
.
8
0
2
2
:
v
i
X
r
a

General Cutting Planes for Bound-Propagation-Based
Neural Network Veriﬁcation

Huan Zhang*,1

Shiqi Wang*,2 Kaidi Xu*,3

Linyi Li4 Bo Li4
1CMU 2Columbia University

Suman Jana2 Cho-Jui Hsieh5
3Drexel University

J. Zico Kolter1,6
4UIUC 5UCLA 6Bosch Center for AI

huan@huan-zhang.com
linyi2@illinois.edu lbo@illinois.edu

sw3215@columbia.edu kx46@drexel.edu
suman@cs.columbia.edu

chohsieh@cs.ucla.edu

zkolter@cs.cmu.edu

* Equal Contribution

Abstract

Bound propagation methods, when combined with branch and bound, are among
the most effective methods to formally verify properties of deep neural networks
such as correctness, robustness, and safety. However, existing works cannot handle
the general form of cutting plane constraints widely accepted in traditional solvers,
which are crucial for strengthening veriﬁers with tightened convex relaxations. In
this paper, we generalize the bound propagation procedure to allow the addition
of arbitrary cutting plane constraints, including those involving relaxed integer
variables that do not appear in existing bound propagation formulations. Our
generalized bound propagation method, GCP-CROWN, opens up the opportunity
to apply general cutting plane methods for neural network veriﬁcation while ben-
eﬁting from the efﬁciency and GPU acceleration of bound propagation methods.
As a case study, we investigate the use of cutting planes generated by off-the-shelf
mixed integer programming (MIP) solver. We ﬁnd that MIP solvers can generate
high-quality cutting planes for strengthening bound-propagation-based veriﬁers
using our new formulation. Since the branching-focused bound propagation proce-
dure and the cutting-plane-focused MIP solver can run in parallel utilizing different
types of hardware (GPUs and CPUs), their combination can quickly explore a
large number of branches with strong cutting planes, leading to strong veriﬁcation
performance. Experiments demonstrate that our method is the ﬁrst veriﬁer that can
completely solve the oval20 benchmark and verify twice as many instances on
the oval21 benchmark compared to the best tool in VNN-COMP 2021, and also
noticeably outperforms state-of-the-art veriﬁers on a wide range of benchmarks.
GCP-CROWN is part of the α,β-CROWN veriﬁer, the VNN-COMP 2022 winner.
Code is available at http://PaperCode.cc/GCP-CROWN.

1

Introduction

Neural network (NN) veriﬁcation aims to formally prove or disprove certain properties (e.g., cor-
rectness and safety properties) of a NN under a certain set of inputs. These methods can provide
worst-case performance guarantees of a NN, and have been applied to mission-critical applications that
involve neural networks, such as automatic aircraft control [31, 4], learning-enabled cyber-physical
systems [54], and NN based algorithms in an operating system [51].

The NN veriﬁcation problem is generally NP-complete [30]. For piece-wise linear networks, it can
be encoded as a mixed integer programming (MIP) [53] problem with the non-linear ReLU neurons

Preprint. Under review.

 
 
 
 
 
 
described by binary variables. Thus, fundamentally, the NN veriﬁcation problem can be solved using
the branch and bound (BaB) [10] method similar to generic MIP solvers, by branching some binary
variables and relaxing the rest into a convex problem such as linear programming (LP) to obtain
bounds on the objective. Although early neural network veriﬁers relied on off-the-shelf CPU-based
LP solvers [36, 9] for bounding in BaB, LP solvers do not scale well to large NNs. Thus, many
recent veriﬁers are instead based on efﬁcient and GPU-accelerated algorithms customized to NN
veriﬁcation, such as bound propagation methods [60, 57], Lagrangian decomposition methods [8, 17]
and others [16, 11]. Bound propagation methods, presented in a few different formulations [58,
18, 56, 61, 50, 25], empower state-of-the-art NN veriﬁers such as α,β-CROWN [61, 60, 57] and
VeriNet [3], and can achieve two to three orders of magnitudes speedup compared to solving the NN
veriﬁcation problem using an off-the-shelf solver directly [57], especially on large networks.

Despite the success of existing NN veriﬁers, we experimentally ﬁnd that state-of-the-art NN veriﬁers
may timeout on certain hard instances which a generic MIP solver can solve relatively quickly,
sometimes even without branching. Compared to an MIP solver, a crucial factor missing in most
scalable NN veriﬁers is the ability to efﬁciently generate and solve general cutting planes (or “cuts”).
In generic MIP solvers, cutting planes are essential to strengthen the convex relaxation, so that
much less branching is required. Advanced cutting planes are among the most important factors
in modern MIP solvers [6]; they can strengthen the convex relaxation without removing any valid
integer solution from the MIP formulation. In the setting of NN veriﬁcation, cutting planes reﬂects
complex intra-layer and inter-layer dependencies between multiple neurons, which cannot be easily
captured by existing bound propagation methods with single neuron relaxations [45]. This motivates
us to seek the combination of efﬁcient bound propagation method with effective cutting planes to
further increase the power of NN veriﬁers.

A few key factors make the inclusion of general cutting planes in NN veriﬁers quite challenging. First,
existing efﬁcient bound propagation frameworks such as CROWN [61] and β-CROWN [57] cannot
solve general cutting plane constraints that may involve variables across different layers in the MIP
formulation. Particularly, these frameworks do not explicitly include the integer variables in the MIP
formulation that are crucial when encoding many classical strong cutting planes, such as Gomory
cuts and mixed integer rounding (MIR) cuts. Furthermore, although some existing works [47, 52, 40]
enhanced the basic convex relaxation used in NN veriﬁcation (such as the Planet relaxation [19]),
these enhanced relaxations involve only one or a few neurons in a single layer or two adjacent layers,
and are not general enough. In addition, an LP solver is often required to handle these additional
cutting plane constraints [40], for which the efﬁcient and GPU-accelerated bound propagation cannot
be used, so the use of these tighter relaxations may not always bring improvements.

In this paper, we achieve major progress in using general cutting planes in bound propagation based
NN veriﬁers. To mitigate the challenge of efﬁciently solving general cuts, our ﬁrst contribution is
to generalize existing bound propagation methods to their most general form, enabling constraints
involving variables from neurons of any layer as well as integer variables that encode the status of a
ReLU neuron. This allows us to consider any cuts during bound propagation without relying on a slow
LP solver, and opens up the opportunity for using advanced cutting plane techniques efﬁciently for
the NN veriﬁcation problem. Our second contribution involves combining a cutting-plane-focused,
off-the-shelf MIP solver with our GPU-accelerated, branching-focused bound propagation method
capable of handling general cuts. We entirely disable branching in the MIP solver and use it only for
generating high quality cutting planes not restricting to neurons within adjacent layers. Although an
MIP solver often cannot verify large neural networks, we ﬁnd that they can generate high quality
cutting planes within a short time, signiﬁcantly helping bound propagation to achieve better bounds.

Our experiments show that general cutting planes can bring signiﬁcant improvements to NN veriﬁers:
we are the ﬁrst veriﬁer that completely solves all instances in the oval20 benchmark, with an
average time of only 13 seconds per instance; on the even harder oval21 benchmark in VNN-
COMP 2021 [3], we can verify twice as many instances compared to the competition winner. We
also outperform existing state-of-the-art bound propagation based methods including those using
multi-neuron relaxations [20] (a limited form of cutting planes).

2 Background
The NN veriﬁcation problem We consider the veriﬁcation problem for an L-layer ReLU-based
Neural Network (NN) with inputs ˆx(0) := x ∈ Rd0 , weights W(i) ∈ Rdi×di−1, and biases

2

b(i) ∈ Rdi (i ∈ {1, · · · , L}). We can get the NN outputs f (x) = x(L) ∈ RdL by sequentially
propagating the input x through afﬁne layers with x(i) = W(i) ˆx(i−1) + b(i) and ReLU layer with
ˆx(i) = ReLU(x(i)). We also let scalars ˆx(i)
j denote the post-activation and pre-activation,
respectively, of j-th ReLU neuron in i-th layer. Throughout the paper, we use bold symbols to denote
vectors (e.g., x(i)) and regular symbols to denote scalars (e.g., x(i)
is the j-th element of x(i)). We
j
use the shorthand [N ] to denote {1, · · · , N }, and W(i)

:,j is the j-th column of W(i).

j and x(i)

Commonly, the input x is bounded within a perturbation set C (such as an (cid:96)p norm ball) and the
veriﬁcation speciﬁcation deﬁnes a property of the output f (x) that should hold for any x ∈ C,
e.g., whether the true label’s logit fy(x) will be always larger than another label’s logit fj(x), (i.e.,
checking if fy(x)−fj(x) is always positive). Since we can append the veriﬁcation speciﬁcation (such
as a linear function on neural network output) as an additional layer of the network, canonically, the
NN veriﬁcation problem requires one to solve the following one-dimensional (dL = 1) optimization
objective on f (x):

(1)
with the relevant property deﬁned to be proven if the optimal solution f ∗ ≥ 0. Throughout this work,
we consider the (cid:96)∞ norm ball C := {x : (cid:107)x − x0(cid:107)∞ ≤ (cid:15)} where x0 is a predeﬁned constant (e.g., a
clean input image), although it is possible to extend to other norms or speciﬁcations [43, 59].

f (x), ∀x ∈ C

f ∗ = min
x

The MIP and LP formulation for NN veriﬁcation The mixed integer programming (MIP) for-
mulation is the root of many NN veriﬁcation algorithms. This formulation uses binary variables z
to encode the non-linear ReLU neurons to make the non-convex optimization problem (1) tractable.
Additionally, we assume that we know sound pre-activation bounds l(i) ≤ x(i) ≤ u(i) for x ∈ C
which can be obtained via cheap bound propagation methods such as IBP [23] or CROWN [61].
Then ReLU neurons for each layer i can be classiﬁed into three classes [58], namely “active” (I +(i)),
“inactive” (I −(i)) and “unstable” (I (i)) neurons, respectively:

I +(i) := {j : l(i)

j ≥ 0};

I −(i) := {j : u(i)

j ≤ 0};

I (i) := {j : l(i)

j ≤ 0, u(i)

j ≥ 0}

Based on the deﬁnition of ReLU, activate and inactive neurons are linear functions, so only unstable
neurons require binary encoding. The MIP formulation of (1) is:
f (x)

ˆx(0) = x; x ∈ C;

s.t. f (x) = x(L);

(2)

f ∗ = min
x, ˆx,z

i ∈ [L],

j ∈ I (i), i ∈ [L−1]

j ∈ I (i), i ∈ [L−1]

x(i) = W(i) ˆx(i−1) + b(i);
ˆx(i)
j ≥ 0;
j ≥ x(i)
ˆx(i)
j ;
j z(i)
ˆx(i)
j ≤ u(i)
;
j (1 − z(i)
j − l(i)
j ≤ x(i)
ˆx(i)
j );
z(i)
j ∈ I −(i), i ∈ [L−1]
j ∈ {0, 1};
ˆx(i)
j = x(i)
j ;
ˆx(i)
j = 0;

j ∈ I −(i), i ∈ [L−1]

j ∈ I +(i), i ∈ [L−1]

j ∈ I (i), i ∈ [L−1]

j

j ∈ I (i), i ∈ [L−1]

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)
Since a MIP problem is slow or intractable to solve, it is commonly relaxed as a When the integer
variables are relaxed to continuous ones, we obtain the LP relaxation of NN veriﬁcation problem:

f ∗
LP = min
x, ˆx,z

f (x)

s.t. (3), (2), (4), (5), (6), (7), (9),(10),

(11)
The ReLU constraints involving z is often projected out, leading to the well-known Planet relaxation
used in many NN veriﬁers, replacing (6), (7) and (8) with a single constraint to get an equivalent LP:

j ≤ 1;

j ∈ I (i), i ∈ [L−1]

0 ≤ z(i)

s.t. (3), (2), (4), (5), (9), (10),

f ∗
LP = min
x, ˆx,z

f (x)

ˆx(i)
j ≤

u(i)
j
j − l(i)
u(i)

j

3

(x(i)

j − l(i)

j ); j ∈ I (i), i ∈ [L−1]

(12)

LP ≤ f ∗. A veriﬁer using this formulation is incomplete: if f ∗

Due to the relaxations, the objective of the LP formulation is always a lower bound of the MIP
formulation: f ∗
LP ≥ 0, then f ∗ ≥ 0
and the property is veriﬁed; otherwise, we cannot conclude the sign of f ∗ so the veriﬁer must return
“unknown”. Branch and bound can be used to improve the lower bound and achieve completeness [10,
57]. However, in this paper, we work on an orthogonal direction of strengthening the LP formulation
by adding cutting planes to obtain larger bounds.

Bound propagation methods
Instead of solving the LP formulation directly using a LP solver,
bound propagation methods aims to quickly give a lower bound for f ∗
LP. For example, CROWN [61]
and β-CROWN [57] propagate a sound linear lower bound backwards for fL(x) with respect to each
intermediate layer. For example, suppose we know

min
x∈C

fL(x) ≥ min
x∈C

a(i)(cid:62)

ˆx(i) + c(i)

(13)

With i = L − 1 the above is trivially hold with a(L−1) = W(L), c(L−1) = b(L).
In bound
propagation methods, a propagation rule propagates an inequality (13) through a previous layer
ˆx(i) := ReLU(W(i) ˆx(i−1) + b(i)) to obtain a sound inequality with respect to ˆx(i−1):

min
x∈C

fL(x) ≥ min
x∈C

a(i−1)(cid:62)

ˆx(i−1) + c(i−1)

Here a(i−1), c(i−1) can be calculated in close-form via a(i), c(i), W(i), b(i), l(i) and u(i) such that
the bound still holds (see Lemma 1 in [57]). Applying the procedure repeatedly will eventually reach
the input layer:

a(0)(cid:62)

x + c(0)

min
x∈C

fL(x) ≥ min
x∈C
The minimization on linear layer can be solved easily when C is a (cid:96)p norm ball to obtain a valid lower
bound of f ∗. Since the bounds propagate layer-by-layer, this process can be implemented efﬁciently
on GPUs [59] without relying on a slow LP solver, which greatly improves the scalability and solving
time. Additionally, it is often used to obtain intermediate layer bounds l(i) and and u(i) required for
the MIP formulation (6)(7), by treating each x(i)
j as the output neuron. The bound propagation rule
can either be derived in primal space [61], dual space [58] or abstract interpretations [50]. In Sec.3.1,
we will discuss the our bound propagation procedure with general cutting plane constraints.

(14)

3 Neural Network Veriﬁcation with General Cutting Planes

3.1 GCP-CROWN: General Cutting Planes in Bound Propagation

In this section, we generalize existing bound propagation method to handle general cutting plane
constraints. Our goal is to derive a bound propagation rule similar to CROWN and β-CROWN
discussed in Section 2, however considering additional constraints among any variables within the
LP relaxation. To achieve this, we ﬁrst derive the dual problem of the LP; inspired by the dual
formulation, we derive the bound prorogation rule in a layer by layer manner that takes all cutting
plane constraints into consideration. The derivation process is inspired by [58, 45] and [57].

LP relaxation with cutting planes.
In this section, we derive the bound propagation procedure
under the presence of general cutting plane constraints. A cutting plane is a constraint involving any
variables x(i) (pre-activation), ˆx(i) (post-activation), z(i) (ReLU indicators) from any layer i:

h(i)(cid:62)

x(i) + g(i)(cid:62)

ˆx(i) + q(i)(cid:62)

z(i)(cid:17)

≤ d

L−1
(cid:88)

(cid:16)

i=1

Here h(i), g(i) and q(i) are coefﬁcients for this cut constraint. The difference between a valid
cutting plane and an arbitrary constraint is that a valid cutting plane should not remove any valid
integer solution from the MIP formulation. Our new bound propagation procedure can work for any
constraints, although in this work we focus on studying the impacts of cutting planes. When there are
N cutting planes, we write them in a matrix form:

(cid:16)
H (i)x(i) + G(i) ˆx(i) + Q(i)z(i)(cid:17)

≤ d

(15)

L−1
(cid:88)

i=1

4

where H (i), G(i), Q(i) ∈ RN ×di . The LP relaxation with all cutting planes is:

f ∗
LP-cut = min
x, ˆx,z

f (x)

s.t. (3), (2), (4), (5), (6), (7),(9), (10),

j ∈ I (i), i ∈ [L−1]

0 ≤ z(i)

j ≤ 1;
H (i)x(i) + G(i) ˆx(i) + Q(i)z(i)(cid:17)

≤ d

L−1
(cid:88)

(cid:16)

i=1

(16)

Since an additional constraint is added, f ∗
LP and we get closer to f ∗. Unlike the original LP
where each constraint only contains variables from two consecutive layers, our general cutting plane
constraint may involve any variable from any layer in a single constraint.

LP-cut ≥ f ∗

The dual problem with cutting planes We ﬁrst show the dual problem for the above LP. The dual
problem we consider here is different from existing works in two ways: ﬁrst, we have constraints
with integer variables to support potential cutting planes on z. Additionally, we have cutting planes
constraints that may involve variables in any layer, so the dual in previous works such as [58] cannot
be directly reused. Our dual problem is given below (derivation details in Appendix A):

f ∗
LP-cut =

max
ν,µ≥0,τ ≥0
γ≥0,π≥0,β≥0

−(cid:15)(cid:107)ν(1)(cid:62)W(1)x0(cid:107)1 − β(cid:62)d −

L
(cid:88)

i=1

ν(i)(cid:62)b(i)

L−1
(cid:88)

(cid:88)

(cid:104)

+

π(i)
j

i=1

j∈I(i)

j − ReLU(u(i)
l(i)

j γ(i)

j + l(i)

j π(i)

j − β(cid:62)Q(i)
:,j )

(cid:105)

s.t. ν(L) = −1; and for each i ∈ [L − 1] :

j = ν(i+1)(cid:62)W(i+1)
ν(i)
j = −β(cid:62)H (i)
ν(i)

:,j ; j ∈ I −(i)

:,j − β(cid:62)(H (i)

:,j + G(i)

:,j ); j ∈ I +(i)

j

(cid:16)

(cid:17)

(cid:17)

(cid:16)

−

and for each j ∈ I (i) the two equalities below hold:
j + τ (i)
µ(i)

j − τ (i)

j + γ(i)
π(i)

j = π(i)
ν(i)

j − β(cid:62)H (i)
:,j ;

= ν(i+1)(cid:62)W(i+1)

:,j − β(cid:62)G(i)
Instead of solving the dual problem exactly, we use it to obtain a lower bound of f ∗
LP-cut. Intuitively,
due to the deﬁnition of due problem, any valid setting of dual variables leads to a lower bound of
f ∗
LP-cut. Informally, starting from ν(L) = −1, by applying the constraints in this dual formulation,
we can compute ν(L−1), ν(L−2), · · · until ν(1). The ﬁnal objective is a function of ν(i), i ∈ [N ] and
other dual variables. Precisely, our GCP-CROWN bound propagation procedure with general cutting
plane constraint is presented in the theorem below (proof in Appendix A):
Theorem 3.1 (Bound propagation with general cutting planes). Given any optimizable parameters
0 ≤ α(i)

LP-cut is lower bounded by the following objective function:

j ≤ 1 and β ≥ 0, f ∗

:,j

j

g(α, β) = −(cid:15)(cid:107)ν(1)(cid:62)W(1)x0(cid:107)1 −

L
(cid:88)

i=1

ν(i)(cid:62)b(i) − β(cid:62)d +

L−1
(cid:88)

(cid:88)

i=1

j∈I(i)

h(i)
j (β)

where variables ν(i) are obtained by propagating ν(L) = −1 throughout all i ∈ [L−1]:

Here ˆν(i)

j

, π(i)
j

∗

and h(i)

j

j

∗

j [ˆν(i)

:,j + G(i)

]− − β(cid:62)H (i)

:,j , j ∈ I −(i)

:,j ), j ∈ I +(i)

:,j − β(cid:62)(H (i)

ν(i)
j = ν(i+1)(cid:62)W(i+1)
j = −β(cid:62)H (i)
ν(i)
− α(i)
j = π(i)
ν(i)
j (β) are deﬁned for each unstable neuron j ∈ I (i) as:
:,j − β(cid:62)G(i)
ˆν(i)
:= ν(i+1)(cid:62)W(i+1)
j
(cid:32) u(i)
(cid:32)
]+ + β(cid:62)Q(i)
j [ˆν(i)
j
:,j
j − l(i)
u(i)

:,j , j ∈ I (i)

, [ˆν(i)
j

= max

min

]+

, 0

(cid:33)

:,j

∗

j

π(i)
j

(cid:33)

5

h(i)
j (β) =






∗

j

j π(i)
l(i)
0
β(cid:62)Q(i)
:,j

j

j [ˆν(i)
l(i)
if
if β(cid:62)Q(i)
if β(cid:62)Q(i)

:,j ≤ u(i)
]+ ≤ β(cid:62)Q(i)
j [ˆν(i)
]+
j [ˆν(i)
]+

:,j ≥ u(i)
:,j ≤ l(i)

j

j

j [ˆν(i)

j

]+

Based on Theorem 3.1, to obtain an lower bound of f ∗
LP-cut, we start with any valid setting of
0 ≤ α ≤ 1 and β ≥ 0 and ν(L) = −1. According to the bound propagation rule, we can compute
each ν(i), i ∈ [L − 1], in a layer by layer manner. Then objective g(α, β) can be evaluated based on
all ν(i) to give an lower bound of f ∗
LP-cut. Since any valid setting of α and β lead to a valid lower
bound, we can optimize α and β using gradient ascent in a similar manner as in [60, 57] to tighten
this lower bound. The entire procedure can also run on GPU for great acceleration.

Connection to Convex Outer Adversarial Polytope
In convex outer adversarial polytope [58], a
bound propagation rule was developed in a similar manner in the dual space without considering
cutting plane constraints, and is a special case of ours. We denote their bound propagation objective
function as gWK which also contains optimizable parameters αWK.
Proposition 3.2. Given the same input x, perturbation set C, network weights, and N cutting plane
constraints,

max
α,β

g(α, β) ≥ max
αWK

gWK(αWK)

Proof. In Theorem 3.1, when all β are set to 0, then π(i)
j

l(i)
j ,
we recover exactly the same bound propagation equations as in [58]. However, since we allow the
addition of cutting plane methods and we can maximize over the additional parameter β, the objective
given by our bound propagation is always at least as good as gKW.

j (β) = π(i)

and h(i)

=

j

j

j

j [ˆν(i)
u(i)
]+
j −l(i)
u(i)

∗

∗

Connection to CROWN-like bound propagation methods CROWN [61] and α-CROWN [60]
use the same bound propagation rule as [58] so Proposition 3.2 also applies, although they were
derived from primal space without explicitly formulating the problem as a LP. Salman et al. [45]
showed that many other bound propagation methods [50, 55] are equivalent to or weaker than [58].
Recently, Wang et al. [57] extends CROWN to β-CROWN to handle split constraints (e.g., x(i)
j ≥ 0).
It can be seen a special case as GCP-CROWN where all H, G and Q matrices are zeros except:

H (i)

j,j = 1,j ∈ I −(i)

for x(i)

j ≤ 0 split;

H (i)

j,j = −1,j ∈ I +(i)

for x(i)

j ≥ 0 split

In addition, [20] encodes multi-neuron relaxations using sparse H (i) and G(i) and each cut contains
a small number of neurons involving x(i) and ˆx(i) for the same layer i. Wang et al. [57] derived
bound propagation rules from both the dual LP and the primal space with a Lagrangian without LP.
However, in our case, it is not intuitive to derive bound propagation without LP due to the potential
cutting planes on relaxed integer variables z, which do not appear without the explicit LP formulation.
Furthermore, although we derived cutting planes for bound propagation methods, it is technically
also possible to derive them using other bounding frameworks such as Lagrangian decomposition [8].

3.2 Branch-and-bound with GCP-CROWN and MIP Solver Generated Cuts

To build a complete NN veriﬁer, we follow the popular branch-and-bound (BaB) procedure [10, 9] in
state-of-the-art NN veriﬁers with GPU accelerated bound propagation method [8, 60, 16, 57], and
our GCP-CROWN is used as the bounding procedure in BaB. We refer the readers to Appendix B for
a more detailed background on branch-and-bound. Having the efﬁcient bound propagation procedure
with general cutting plane constraints, we now need to ﬁnd a good set of general cutting planes
H (i), G(i), Q(i) to accelerate NN veriﬁcation. Since GCP-CROWN can adopt any cutting planes, to
fully exploit its power, we propose to use off-the-shelf MIP solvers to generate cutting planes and
create an NN veriﬁer combining GPU-accelerated bound propagation with strong cuts generated
by a MIP solver. We make the bound propagation on GPU and the MIP solver on CPU run in
parallel with cuts added on-the-ﬂy, so the original strong performance of bound-propagation-based
NN veriﬁer will never be affected by a potentially slow MIP solver. This allows us to make full use
of available computing resource (GPU + CPU). The architecture of our veriﬁer is shown in Fig 1.

6

MIP solvers for cutting plane generation Generic
MIP solvers such as cplex[28] and gurobi[24] also ap-
ply a branch-and-bound strategy, conceptually similar to
state-of-the-art NN veriﬁers. They often tend to be slower
than specialized NN veriﬁers because MIP solvers rely
on slower bounding procedures (e.g., Simplex or barrier
method) and cannot apply an GPU-accelerated method
such as bound propagation or Lagrangian decomposition.
However, we still ﬁnd that MIP solvers are a strong base-
line when combined with tight intermediate layer bounds.
For example, in the oval21 benchmark in Table 2, α-
CROWN+MIP (MIP solver combined with tight interme-
diate layer bound computed by α-CROWN) is able to solve
4 more instances compared to all other tools in the com-
petition. Our investigation found that α-CROWN+MIP
explores much less branches than other state-of-the-art
branch-and-bound based NN veriﬁers, however before
branching starts, the MIP solver produces very effective
cutting planes that can often verify an instance with little branching. MIP solvers is able to discover
sophisticated cutting planes involving several NN layers reﬂecting complex correlations among
neurons, while existing NN veriﬁers only exploit speciﬁc forms of constraints within same layer
or between adjacent layers [52, 47, 40]. This motivates us to combine the strong cutting planes
generated by MIP solvers with GPU-accelerated bound-propagation-based BaB.

Figure 1: Overview of our cutting-plane-
enhanced, fully-parallelized NN veriﬁer.

In this work, we use cplex as the MIP solver for cutting plane generation since gurobi cannot
export cuts. We entirely disable all branching features in cplex and make it focus on ﬁnding cutting
planes only. These cutting planes are generated only for the root node of the BaB search tree so they
are sound for any subdomains with neuron splits in BaB. We conduct branching using our generalized
bound propagation procedure in Section (3.1) with the cutting planes generated by cplex.

Fully-parallelized NN veriﬁer design We design our veriﬁer as shown in Figure 1. After parsing
the NN under veriﬁcation, we launch a separate process to encode the NN veriﬁcation problem as
MIP problem and start multiple MIP solvers, one for each target label to be veriﬁed. At the same time,
the main NN veriﬁer process executes branch-and-bound without waiting for the MIP solver process.
In each iteration of branch and bound, we query the MIP solving processes and fetch any newly
generated cutting planes. If any cutting planes are produced, they are added as H (i), G(i), Q(i) in
GCP-CROWN and tighten the bounds for subsequent branching and bounding. If no cutting planes
are produced, GCP-CROWN reduces to β-CROWN [57]. Since our veriﬁer is based on strengthening
the bounds in β-CROWN with sound cutting planes, it is also sound and complete.

Adjustments to existing branch and bound method. We implement GCP-CROWN into the α,β-
CROWN veriﬁer [61, 59, 57], the winning veriﬁer in VNN-COMP 2021 [3], as the backbone for
BaB with bound propagation. To better exploit the power of cutting planes under a fully parallel
and asynchronous design, we made a key changes to the BaB procedure. When the number of
BaB subdomains are greater than batch size, we rank the subdomains by their lower bounds and
choose the easiest domains with largest lower bounds ﬁrst to verify with GCP-CROWN, unlike most
existing veriﬁers which solve the worst domains ﬁrst. We use such a order because the MIP solver
generates cutting planes incrementally. Solving these easier subdomains tend to require no or fewer
cutting planes, so we solve them at earlier stages where cutting planes have not been generated or are
relatively weak. On the other hand, if we split worst subdomains ﬁrst, the number of subdomains will
grow quickly, and it can take a long time to verify these domains when stronger cuts become available
later. Under a similar rationale, when verifying a multi-class model and BaB needs to verify each
target class label one by one, we start BaB from the easiest label ﬁrst (α-CROWN bound closest to
0), allowing the MIP solver to run longer for harder labels, generating stronger cuts for harder labels.

7

MIPSolverCoordinationProcessGenericNNParserMainProcessNNVerifierParallelizedMIPSolver on CPUforDifferentLabelsLaunchsolversBranchingPicksub-domainsGPU-AcceleratedGCP-CROWNFetching General CuttingPlanesLooptill no domain remainsMIP Solver ProcessesMIPEncoderTerminateReal-timecuttingplanefetchingupongenerationTerminateTable 1: Average runtime and average number of branches on oval20 benchmarks with 100 properties per
model. Timeout is set to 3,600 seconds (consistent with other literature results).
CIFAR-10 Base

CIFAR-10 Wide

CIFAR-10 Deep

Method

time(s)

branches %timeout

time(s)

branches %timeout

time(s)

branches %timeout

MIPplanet [19]
BaBSR [9]
GNN-online [36]
BDD+ BaBSR [8]
Fast-and-Complete [60]
OVAL (BDD+ GNN)∗[8, 36]
A.set BaBSR [16]
BigM+A.set BaBSR [16]
BaDNB (BDD+ FSB)[17]
ERAN∗[47, 48, 50, 49]
β-CROWN [57]
α-CROWN+MIP†
GCP-CROWN with MIP cuts

2849.69
2367.78
1794.85
807.91
695.01
662.17
381.78
390.44
309.29
805.94
118.23
335.50
13.40

-
1020.55
565.13
195480.14
119522.65
67938.38
12004.60
11938.75
38239.04
-
208018.21
8523.37
25436.28

68.00
36.00
33.00
20.00
17.00
16.00
7.00
7.00
7.00
5.00
3.00
3.00
0.00

2417.53
2871.14
1367.38
505.65
495.88
280.38
165.91
172.65
165.53
632.20
78.32
203.87
3.02

-
812.65
372.74
74203.11
80519.85
17895.94
2233.10
4050.59
11214.44
-
116912.57
2029.60
2095.18

46.00
49.00
15.00
10.00
9.00
6.00
3.00
3.00
4.00
9.00
2.00
0.00
0.00

2302.25
2750.75
1055.33
266.28
105.64
94.69
190.28
177.22
10.50
545.72
5.69
76.90
3.70

-
401.28
131.85
12722.74
2455.11
1990.34
2491.55
3275.25
368.16
-
41.12
1364.24
110.92

40.00
39.00
4.00
4.00
1.00
1.00
2.00
2.00
0.00
0.00
0.00
0.00
0.00

* Results from VNN-COMP 2020 report [34].

† A new baseline proposed and evaluated in this work, not presented in previous papers.

Figure 2: Percentage of solved properties on the oval20 benchmark vs. running time (timeout 1hr).

4 Experiments

We now evaluate our veriﬁer, GCP-CROWN with MIP cuts, on a few popular veriﬁcation benchmarks.
Since our veriﬁer uses a MIP solver in our pipeline, we also include a new baseline, α-CROWN+MIP,
which uses gurobi as the MIP solver with the tightest possible intermediate layer bounds from
α-CROWN [59]. We use the same branch and bound algorithm as in β-CROWN and we use ﬁltered
smart branching (FSB) [16] as the branching heuristic in all experiments. Without cutting planes,
GCP-CROWN becomes vanilla β-CROWN as we share the same code base as β-CROWN. We
include model information and detailed setup for our experiments in Appendix C. Our code is
available at http://PaperCode.cc/GCP-CROWN.

Results on the oval20 benchmark in VNN-COMP 2020. oval20 is a popular benchmark con-
sistently used in huge amount of NN veriﬁers and it perfectly reﬂects the progress of NN veriﬁers. We
include literature results for many baselines in Table 1. We are the only veriﬁer that can completely
solve all three models without any timeout. Our average runtime is signiﬁcantly lower compared to
the time of baselines because we have no timeout (counted as 3600s), and our slowest instance only
takes about a few minutes while easy ones only take a few seconds, as shown in Figure 2. Additionally,
we often use less number of branches compared to the state-of-the-art veriﬁer, β-CROWN, since our
strong cutting planes help us to eliminate many hard to solve subdomains in BaB. Furthermore, we
highlight that α-CROWN+MIP also achieves a low timeout rate, although it is much slower than our
bound propagation based approach combined with cuts from a MIP solver.

Results on VNN-COMP 2021 benchmarks. Among the eight scoring benchmarks in VNN-
COMP 2021 [3], only two (oval21 and cifar10-resnet) are most suitable for the evaluation
of this work. Among other benchmarks, acasxu and nn4sys have low input dimensionality and
require input space branching rather than ReLU branching; verivital, mnistfc, and eran bench-
marks consist of small MLP networks that can be solved directly by MIP solvers; marabou contains
mostly adversarial examples, making it a good benchmark for falsiﬁers rather than veriﬁers. We
present our results in Table 2. Besides results from 6 VNN-COMP 2021 participants, we also include
two additional baselines, α-CROWN+MIP (same as in Table 1), and MN-BaB [20], a recently
proposed branch and bound framework with multi-neuron relaxations [39], which can be viewed as a
restricted form of cutting planes. On the oval21 benchmark, OVAL, VeriNet and α,β-CROWN are
the best performing tools, veriﬁed 11 out of 27 instances, while we can verify twice more instances
(22 out of 27) on this benchmark. On the cifar10-resnet benchmark, our veriﬁer also solves the

8

1101001000CIFAR-10 Base: Running time (in s)0%20%40%60%80%100%GNN-OnlineBDD+ BaBSRFast-and-CompleteOVAL (VNN-Comp 20')A.Set BaBSRBig-M+A.Set BaBSRBaDNB(BDD+ FSB)ERAN (VNN-Comp 20')-CROWN-CROWN+MIPGCP-CROWN+MIP cuts1101001000CIFAR-10 Wide: Running time (in s)0%20%40%60%80%100%1101001000CIFAR-10 Deep: Running time (in s)0%20%40%60%80%100%Table 2: VNN-COMP 2021 benchmarks: oval21 and cifar10-resnet. Results marked with VNN-COMP
are from publicly available benchmark data on VNN-COMP 2021 Github. “-” indicates unsupported model.
oval21 (30 properties; PGD upper bound 27)

cifar10-resnet (72 properties)

Method

time(s)

# veriﬁed

%timeout

time(s)

# veriﬁed

%timeout

nnenum∗ [2, 4]
Marabou∗ [31]
ERAN∗[40, 38]
OVAL∗ [17, 16]
VeriNet∗ [25, 26]
α,β-CROWN∗ [61, 60, 57]
MN-BaB [20]
Venus2† [7, 33]
α-CROWN+MIP
GCP-CROWN with MIP cuts

630.06
429.13
233.84
393.14
414.61
395.32
435.46
386.71
301.23
187.91

2
5
6
11
11
11
10
17
15
22

86.66
73.33
70.00
53.33
53.33
53.33
56.66
33.33
40.00
16.66

-
157.70
129.48
-
105.91
99.87
-
-
125.48
77.72

-
39
43
-
48
58
-
-
46
61

-
45.83
40.28
-
33.33
19.44
-
-
36.11
15.28

* Results from VNN-COMP 2021 report [3].

† We use the latest code of Venus2 in the vnncomp branch, committed on

Jul 18, 2022. Older versions cannot run these convolutional networks.

Table 3: Veriﬁed accuracy (%) and avg. per-example veriﬁcation time (s) on 7 models from SDP-FO [15].
α-CROWN+MIP GCP-CROWN Upper
Dataset
(cid:15) = 0.3 and (cid:15) = 2/255
bound

Veriﬁed% Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s)

SDP-FO [15]∗

β-CROWN [57]

Venus2 [7, 33]

MN-BaB [20]

PRIMA [40]

Model

MNIST

CIFAR

CNN-A-Adv
CNN-B-Adv
CNN-B-Adv-4
CNN-A-Adv
CNN-A-Adv-4
CNN-A-Mix
CNN-A-Mix-4

43.4
32.8
46.0
39.6
40.0
39.6
47.8

>20h
>25h
>25h
>25h
>25h
>25h
>25h

44.5
38.0
53.5
41.5
45.0
37.5
48.5

135.9
343.6
43.8
4.8
4.9
34.3
7.0

70.5
46.5
54.0
44.0
46.0
41.5
50.5

21.1
32.2
11.6
5.8
5.6
49.6
5.9

-
-
-
42.5
46.0
35.0
49.0

-
-
-
68.3
37.7
140.3
70.9

35.5
-
-
47.5
47.5
33.5
49.0

148.4
-
-
26.0
13.1
72.4
37.3

56.5
27.0
52.5
46.0
48.5
32.5
52.5

224.3
360.6
129.5
63.1
16.4
231.3
77.7

71.0
48.5
58.0
48.0
48.5
46.0
54.0

20.75
59.4
21.9
14.4
5.7
39.1
13.4

76.5
65.0
63.5
50.0
49.5
53.0
57.5

*

We run α-CROWN+MIP and MN-BaB with 600s timeout threshold for all models. “-” indicates that we could not run a model due to unsupported model structure or other errors. We run
our GCP-CROWN with MIP cuts with a shorter 200s timeout for all models and it achieves better veriﬁed accuracy than all other baselines. Other results are reported from [57].

most number of instances and achieves the lowest average time. In fact, α-CROWN+MIP is also a
strong baseline, solving 4 more instances than all competition participants, showing the importance
of strong cutting planes. Our GCP-CROWN with MIP cuts combines the beneﬁts of fast bound
propagation on GPU with the strong cutting planes generated by a MIP solver and achieves the best
performance. We present a more detailed analysis on the cutting planes used in this benchmark in
Appendix C.2.

The oval21 benchmark was also included as part of VNN-COMP 2022, concluded in July 2022,
with a different sample of 30 instances. GCP-CROWN is part of the winning tool in VNN-COMP
2022, α,β-CROWN, which veriﬁed 25 out of 30 instances in this benchmark, outperforming the
second place tool (MN-BaB [20] with multi-neuron relaxations) with 19 veriﬁed instances by a large
margin. More results on VNN-COMP 2022 can be found in these slides1.

Results on SDP-FO benchmarks. We further evaluate our method on the SDP-FO benchmarks
in [15, 57]. This benchmark contains 7 mostly adversarially trained MNIST and CIFAR models with
200 instances each, which are hard for many existing veriﬁers. Beyond the baselines reported in [57],
we also include two additional baselines, α-CROWN+MIP (same as in Table 1) and MN-BaB [20]
(same as in Table 2). Table 3 shows that our method improves the percentage of veriﬁed images
(“veriﬁed accuracy”) on all models compared to state-of-the-art veriﬁers, further closing the gap
between veriﬁed accuracy and empirical robust accuracy obtained by PGD attack (reported as “upper
bound” in Table 3).

5 Related Work
Cutting plane method is a classic technique to strengthen the convex relaxation of an integer pro-
gramming problem. Generic cutting planes such as Gomory’s cut [21, 22], Chvátal–Gomory cut [12],
implied bound cut [27], lift-and-project [35], reformulation-linearization techniques [46] and mixed
integer rounding cuts [41, 37] can be applied to almost any LP relaxed problems, and problem speciﬁc
cutting planes such as Knapsack cut [14], Flow-cover cut [42] and Clique cut [29] require speciﬁc
problem structures. Modern MIP solvers typically uses a branch-and-cut strategy, which tends to
generate a large number of cuts before starting the next iteration of branching, and solve the LP
relaxation of the MIP problem with cutting planes with an exact method such as the Simplex method.
Our GCP-CROWN is a specialized solver for the NN veriﬁcation problem, which can quickly obtain
a lower bound of the LP relaxation with cutting planes specially for the NN veriﬁcation problem.

1A formal competition report is under preparation by VNN-COMP organizers; scores were presented in

FLoC 2022: https://drive.google.com/file/d/1nnRWSq3plsPvOT3V-drAF5D8zWGu02VF/view.

9

The veriﬁcation of piece-wise linear NNs can be formulated as a MIP problem, so early works [30, 31,
53] solve an integer or combinatorial formulation directly. For efﬁciency reasons, most recent works
use a convex relaxation such as linear relaxation [19, 58] or semideﬁnite relaxation [44, 15]. Salman
et al. [45] discussed the limitation of many convex relaxation based NN veriﬁcation algorithms
and coined the term “convex relaxation barrier”, speciﬁcally for the popular single-neuron “Planet”
relaxation [19]. Several works developed novel techniques to break this barrier. [47] added constraints
that depends on the aggregation of multiple neurons, and these constraints were passed to a LP solver.
[40] enhanced the multi-neuron formulation of [47] to obtain tighter relaxations. [1] studied stronger
convex relaxations for a ReLU neuron after an afﬁne layer, and [52] constructed efﬁcient algorithms
based on this relaxation for incomplete veriﬁcation. [16] extended the formulation in [1] to a dual
form and combined it with branch and bound to achieve completeness. [7] proposed specialized cuts
considering neuron dependencies and solve them using a MIP solver. [20] combined the multi-neuron
relaxation [40] with branch and bound. Although these works can be seen as a special form of cutting
planes, they mostly focused on enhancing the relaxation for several neurons within a single layer
or two adjacent layers. GCP-CROWN can efﬁciently handle general cutting plane constraints with
neurons from any layers in a bound propagation manner, and the cutting planes we ﬁnd from a MIP
solver can be seen as tighter convex relaxations encoding multi-neuron and multi-layer correlations.

6 Conclusion

In this paper, we propose GCP-CROWN, an efﬁcient and GPU-accelerated bound propagation method
for NN veriﬁcation capable of handling any cutting plane constraints. We combine GCP-CROWN
with branch and bound and high quality cutting planes generated by a MIP solver to tighten the
convex relaxation for NN veriﬁcation. The combination of fast bound propagation and strong cutting
planes lead to state-of-the-art veriﬁcation performance on multiple benchmarks. Our work opens up
a great opportunity for studying more efﬁcient and powerful cutting planes for NN veriﬁcation.

Limitations of this work Our work generalizes existing bound propagation methods that can
handle only simple constraints (such as neuron split constraints in β-CROWN [57]) to general
constraints, and we share a few common limitations as in previous works [10, 60, 16]: the branch-
and-bound procces and bound propagation procedure are developed on ReLU networks, and it can
be non-trivial to extend it to neural networks with non-piecewise-linear operations. In addition, we
currently directly use cutting planes generated by a generic MIP solver, and there might exist stronger
and faster cutting plane methods that can exploit the structure of the neural network veriﬁcation
problem. We hope these limitations can be addressed in future works.

Potential Negative Societal Impact Our work focuses on formally proving desired properties of a
neural network under investigation such as safety and robustness, which is an important direction of
trustworthy machine learning and has overall positive societal impact. Since our veriﬁer is a complete
veriﬁer, it might be possible to use it to ﬁnd weakness of a neural network and guide adversarial
attacks. However, we believe that formally characterizing a model’s behavior and potential weakness
is important for building robust models and preventing real-world malicious attack.

References

[1] Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong
convex relaxations and mixed-integer programming formulations for trained neural networks.
In Mathematical Programming, 2020.

[2] Stanley Bak. nnenum: Veriﬁcation of relu neural networks with optimized abstraction reﬁnement.

In NASA Formal Methods Symposium, pages 19–36. Springer, 2021.

[3] Stanley Bak, Changliu Liu, and Taylor Johnson. The second international veriﬁcation
of neural networks competition (vnn-comp 2021): Summary and results. arXiv preprint
arXiv:2109.00498, 2021.

[4] Stanley Bak, Hoang-Dung Tran, Kerianne Hobbs, and Taylor T. Johnson. Improved geometric
path enumeration for verifying relu neural networks. In Proceedings of the 32nd International
Conference on Computer Aided Veriﬁcation. Springer, 2020.

10

[5] Dimitris Bertsimas and Robert Weismantel. Optimization over integers. Athena Scientiﬁc,

2005.

[6] Robert Bixby and Edward Rothberg. Progress in computational mixed integer programming—a
look back from the other side of the tipping point. Annals of Operations Research, 149(1):37–41,
2007.

[7] Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener.
Efﬁcient veriﬁcation of relu-based neural networks via dependency analysis. In AAAI Conference
on Artiﬁcial Intelligence (AAAI), 2020.

[8] Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet
Kohli, Philip H. S. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network
veriﬁcation. Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020.

[9] Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, and P Mudigonda. Branch and
bound for piecewise linear neural network veriﬁcation. Journal of Machine Learning Research
(JMLR), 2020.

[10] Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A
uniﬁed view of piecewise linear neural network veriﬁcation. In Advances in Neural Information
Processing Systems (NeurIPS), 2018.

[11] Shaoru Chen, Eric Wong, J Zico Kolter, and Mahyar Fazlyab. Deepsplit: Scalable veriﬁcation
of deep neural networks via operator splitting. arXiv preprint arXiv:2106.09117, 2021.

[12] Vasek Chvátal. Edmonds polytopes and a hierarchy of combinatorial problems. Discrete

mathematics, 4(4):305–337, 1973.

[13] Michele Conforti, Gérard Cornuéjols, and Giacomo Zambelli. Integer programming / Michele
Conforti, Gérard Cornuéjols, Giacomo Zambelli. Graduate texts in mathematics, . 271. Springer,
Cham, 2014.

[14] Harlan Crowder, Ellis L Johnson, and Manfred Padberg. Solving large-scale zero-one linear

programming problems. Operations Research, 31(5):803–834, 1983.

[15] Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan
Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang,
et al. Enabling certiﬁcation of veriﬁcation-agnostic networks via memory-efﬁcient semideﬁnite
programming. Advances in Neural Information Processing Systems (NeurIPS), 2020.

[16] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan
Kumar. Scaling the convex barrier with active sets. International Conference on Learning
Representations (ICLR), 2021.

[17] Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet
Kohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network
veriﬁcation via lagrangian decomposition. arXiv preprint arXiv:2104.06718, 2021.

[18] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli.
A dual approach to scalable veriﬁcation of deep networks. Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), 2018.

[19] Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis (ATVA), 2017.

[20] Claudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete veriﬁca-
tion via multi-neuron relaxation guided branch-and-bound. arXiv preprint arXiv:2205.00263,
2022.

[21] Paul C Gilmore and Ralph E Gomory. A linear programming approach to the cutting-stock

problem. Operations research, 9(6):849–859, 1961.

[22] Paul C Gilmore and Ralph E Gomory. A linear programming approach to the cutting stock

problem—part ii. Operations research, 11(6):863–888, 1963.

11

[23] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation
for training veriﬁably robust models. Proceedings of the IEEE International Conference on
Computer Vision (ICCV), 2019.

[24] LLC Gurobi Optimization. Gurobi optimizer - gurobi, 2022.

[25] Patrick Henriksen and Alessio Lomuscio. Efﬁcient neural network veriﬁcation via adaptive

reﬁnement and adversarial search. In ECAI 2020, pages 2513–2520. IOS Press, 2020.

[26] Patrick Henriksen and Alessio Lomuscio. Deepsplit: An efﬁcient splitting method for neural
network veriﬁcation via indirect effect analysis. In Proceedings of the 30th international joint
conference on artiﬁcial intelligence (IJCAI21), pages 2549–2555, 2021.

[27] Karla L Hoffman and Manfred Padberg. Solving airline crew scheduling problems by branch-

and-cut. Management science, 39(6):657–682, 1993.

[28] IBM. Ibm ilog cplex optimizer, 2022.

[29] Ellis L Johnson and Manfred W Padberg. Degree-two inequalities, clique facets, and biperfect

graphs. In North-Holland Mathematics Studies, volume 66, pages 169–187. Elsevier, 1982.

[30] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex:
An efﬁcient smt solver for verifying deep neural networks. In International Conference on
Computer Aided Veriﬁcation (CAV), 2017.

[31] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim,
Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji´c, et al. The marabou framework
for veriﬁcation and analysis of deep neural networks. In International Conference on Computer
Aided Veriﬁcation (CAV), 2019.

[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International

Conference on Learning Representations (ICLR), 2015.

[33] Panagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete veriﬁcation of relu

neural networks via dependency-based branching. In IJCAI, pages 2643–2650, 2021.

[34] Changliu Liu and Taylor Johnson. Vnn comp 2020.

[35] László Lovász and Alexander Schrijver. Cones of matrices and set-functions and 0–1 optimiza-

tion. SIAM journal on optimization, 1(2):166–190, 1991.

[36] Jingyue Lu and M Pawan Kumar. Neural network branching for neural network veriﬁcation.

International Conference on Learning Representation (ICLR), 2020.

[37] Hugues Marchand and Laurence A Wolsey. Aggregation and mixed integer rounding to solve

mips. Operations research, 49(3):363–371, 2001.

[38] Christoph Müller, Francois Serre, Gagandeep Singh, Markus Püschel, and Martin Vechev.
Scaling polyhedral neural network veriﬁcation on gpus. Proceedings of Machine Learning and
Systems, 3:733–746, 2021.

[39] Christoph Müller, Gagandeep Singh, Markus Püschel, and Martin Vechev. Neural network

robustness veriﬁcation on gpus. arXiv preprint arXiv:2007.10868, 2020.

[40] Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin
Vechev. Precise multi-neuron abstractions for neural network certiﬁcation. arXiv preprint
arXiv:2103.03638, 2021.

[41] George L Nemhauser and Laurence A Wolsey. A recursive procedure to generate all cuts for

0–1 mixed integer programs. Mathematical Programming, 46(1):379–390, 1990.

[42] Manfred W Padberg, Tony J Van Roy, and Laurence A Wolsey. Valid linear inequalities for

ﬁxed charge problems. Operations Research, 33(4):842–861, 1985.

12

[43] Chongli Qin, Brendan O’Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan
Uesato, Grzegorz Swirszcz, Pushmeet Kohli, et al. Veriﬁcation of non-linear speciﬁcations for
neural networks. arXiv preprint arXiv:1902.09592, 2019.

[44] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[45] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex
relaxation barrier to tight robustness veriﬁcation of neural networks. In Advances in Neural
Information Processing Systems (NeurIPS), 2019.

[46] Hanif D Sherali and Warren P Adams. A reformulation-linearization technique for solving
discrete and continuous nonconvex problems, volume 31. Springer Science & Business Media,
2013.

[47] Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin Vechev. Beyond the single
neuron convex barrier for neural network certiﬁcation. In Advances in Neural Information
Processing Systems (NeurIPS), 2019.

[48] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[49] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. Boosting robustness
certiﬁcation of neural networks. In International Conference on Learning Representations,
2018.

[50] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for
certifying neural networks. Proceedings of the ACM on Programming Languages (POPL),
2019.

[51] Cheng Tan, Yibo Zhu, and Chuanxiong Guo. Building veriﬁed neural networks with speciﬁca-
tions for systems. In Proceedings of the 12th ACM SIGOPS Asia-Paciﬁc Workshop on Systems,
pages 42–47, 2021.

[52] Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo
Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural
network veriﬁcation. Advances in Neural Information Processing Systems (NeurIPS), 2020.

[53] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. International Conference on Learning Representations (ICLR),
2019.

[54] Hoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen,
Weiming Xiang, Stanley Bak, and Taylor T Johnson. Nnv: The neural network veriﬁcation
tool for deep neural networks and learning-enabled cyber-physical systems. In International
Conference on Computer Aided Veriﬁcation, pages 3–17. Springer, 2020.

[55] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efﬁcient formal
safety analysis of neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[56] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security
analysis of neural networks using symbolic intervals. In USENIX Security Symposium, 2018.

[57] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-crown: Efﬁcient bound propagation with per-neuron split constraints for neural network
robustness veriﬁcation. Advances in Neural Information Processing Systems, 34, 2021.

[58] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

13

[59] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certiﬁed
robustness and beyond. Advances in Neural Information Processing Systems (NeurIPS), 2020.

[60] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh.
Fast and complete: Enabling complete neural network veriﬁcation with rapid and massively
parallel incomplete veriﬁers. International Conference on Learning Representations (ICLR),
2021.

[61] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efﬁcient neural
In Advances in Neural

network robustness certiﬁcation with general activation functions.
Information Processing Systems (NeurIPS), 2018.

14

In Section A we derive the bound propagation procedure of GCP-CROWN and give a proof for
Theorem 3.1 (GCP-CROWN bound propagation). In Section B we give additional background on
branch and bound. In section C we give more details of experiments, more results as well as a case
study for cutting planes used in GCP-CROWN with MIP cuts.

A The dual problem with cutting planes

In this section we derive the dual formulation for neural network veriﬁcation problem with general
cutting planes (or arbitrarily added linear constraints across any layers). Our derivation is based on
the linearly relaxed MIP formulation with original binary variables z intact, to allow us to add cuts
also to the integer variable (the mostly commonly used triangle relaxation does not have z). We
ﬁrst write the full LP relaxation with arbitrary cutting planes, as well as their corresponding dual
variables:

f ∗
LP-cut = min
x, ˆx,z

f (x)

s.t.

f (x) = x(L); x0 − (cid:15) ≤ x ≤ x0 + (cid:15);
x(i) = W(i) ˆx(i−1) + b(i);
i ∈ [L],
ˆx(i)
j ≥ 0; j ∈ I (i)
ˆx(i)
j ≥ x(i)
ˆx(i)
j ≤ u(i)
j ≤ x(i)
ˆx(i)
ˆx(i)
j = x(i)
ˆx(i)
j = 0; j ∈ I −(i)
0 ≤ z(i)
j ≤ 1;

j ; j ∈ I (i)
j z(i)
j − l(i)
j ; j ∈ I +(i)

; j ∈ I (i)
j (1 − z(i)

j ∈ I (i), i ∈ [L]

j ); j ∈ I (i)

j

⇒ ν(i) ∈ Rdi
⇒ µ(i)
j ∈ R
⇒ τ (i)
j ∈ R
⇒ γ(i)
j ∈ R
⇒ π(i)
j ∈ R

(cid:16)

H (i)x(i) + G(i) ˆx(i) + Q(i)z(i)(cid:17)

≤ d

⇒ β ∈ RN

L−1
(cid:88)

i=1

The Lagrangian function can be constructed as:

f ∗
LP-cut = min
x, ˆx,z

max
ν,µ,τ ,γ,π,β

x(L) +

L
(cid:88)

i=1

ν(i)(cid:62) (cid:16)

x(i) − W(i) ˆx(i−1) + b(i)(cid:17)

(2)

(3)

(4)

(5)

(6)

(7)

(9)

(10)

(11)

(15)

L−1
(cid:88)

(cid:88)

(cid:104)

+

j (−ˆx(i)
µ(i)

j ) + τ (i)

j (x(i)

j − ˆx(i)

j ) + γ(i)

j (ˆx(i)

j − u(i)

j z(i)

j ) + π(i)

j (ˆx(i)

j − x(i)

j + l(i)

j − l(i)

j z(i)
j )

(cid:105)

i=1

+ β(cid:62)

j∈I(i)
(cid:34)L−1
(cid:88)

(cid:16)

i=1

H (i)x(i) + G(i) ˆx(i) + Q(i)z(i)(cid:17)

− d

(cid:35)

s.t. ˆx(i)

j = 0, j ∈ I −(i);
µ ≥ 0;

j = x(i)
ˆx(i)
τ ≥ 0; γ ≥ 0; π ≥ 0; β ≥ 0

j , j ∈ I +(i); x0 − (cid:15) ≤ x ≤ x0 + (cid:15);

0 ≤ z(i)

j ≤ 1, j ∈ I (i)

Here µ, τ , γ, π are shorthands for all dual variables in each layer and each neuron. Note that for
some constraints, their dual variables are not created because they are trivial to handle in the step
steps. Rearrange the equation and swap the min and max (strong duality) gives us:

15

f ∗
LP-cut = max

ν,µ,τ ,γ,π,β

min
x, ˆx,z

(ν(L) + 1)x(L) − ν(1)(cid:62)W(1) ˆx(0)

L−1
(cid:88)

+

(cid:88)

(cid:16)

i=1

j∈I+(i)

j + β(cid:62)H (i)
ν(i)

:,j − ν(i+1)(cid:62)W(i+1)

:,j + β(cid:62)G(i)

:,j

(cid:17)

x(i)
j

L−1
(cid:88)

+

(cid:88)

(cid:16)

j + β(cid:62)H (i)
ν(i)

:,j

(cid:17)

x(i)
j

i=1

j∈I−(i)

L−1
(cid:88)

(cid:88)

(cid:104) (cid:16)

j + β(cid:62)H (i)
ν(i)

:,j + τ (i)

j − π(i)

j

(cid:17)

x(i)
j

i=1
(cid:16)

j∈I(i)
−ν(i)(cid:62)W(i)

:,j − µ(i)
j π(i)

j − τ (i)
j + β(cid:62)Q(i)

j + π(i)
j + γ(i)
(cid:105)
(cid:17)
z(i)
j

:,j

−u(i)

j γ(i)

j − l(i)

ν(i)(cid:62)b(i) +

L−1
(cid:88)

(cid:88)

π(i)
j

l(i)
j − β(cid:62)d

+

+

+

−

(cid:16)

L
(cid:88)

i=1

j + β(cid:62)G(i)

:,j

(cid:17)

ˆx(i)
j

s.t. x0 − (cid:15) ≤ x ≤ x0 + (cid:15);
µ ≥ 0;

τ ≥ 0; γ ≥ 0; π ≥ 0; β ≥ 0

i=1
0 ≤ z(i)

j∈I(i)
j ≤ 1, j ∈ I (i)

:,j

Here W(i+1)
have replaced ˆx(i) with x(i) to obtain the above equation. For the the term x(i)
0 so does not appear.

denotes the j-th column of W(i+1). Note that for the term involving j ∈ I +(i) we
j , j ∈ I +(i) it is always

Solving the inner minimization gives us the dual formulation:

f ∗
LP-cut = max

ν,µ,τ ,γ,π,β

− (cid:15)(cid:107)ν(1)(cid:62)W(1)x0(cid:107)1 −

L
(cid:88)

i=1

ν(i)(cid:62)b(i) − β(cid:62)d

L−1
(cid:88)

(cid:88)

(cid:104)

+

π(i)
j

i=1

j∈I(i)

j − ReLU(u(i)
l(i)

j γ(i)

j + l(i)

j π(i)

j − β(cid:62)Q(i)
:,j )

s.t. ν(L) = −1
j = ν(i+1)(cid:62)W(i+1)
ν(i)

:,j − β(cid:62)(H (i)

:,j + G(i)
:,j ),

for j ∈ I +(i), i ∈ [L − 1]

j = −β(cid:62)H (i)
ν(i)
:,j ,
j − τ (i)
j = π(i)
ν(i)
j − β(cid:62)H (i)
(cid:17)
= ν(i+1)(cid:62)W(i+1)
j + τ (i)
µ(i)

:,j

j

:,j − β(cid:62)G(i)
:,j ,

for j ∈ I −(i), i ∈ [L − 1]

for j ∈ I (i), i ∈ [L − 1]

for j ∈ I (i), i ∈ [L − 1]

(cid:16)
j + γ(i)
π(i)

j

(cid:17)

−

(cid:16)

s.t. µ ≥ 0;

τ ≥ 0; γ ≥ 0; π ≥ 0; β ≥ 0

The (cid:107) · (cid:107)1 comes from minimizing over x0 with the constraint x0 − (cid:15) ≤ x ≤ x0 + (cid:15), and the ReLU(·)
j with the constraint 0 ≤ z(i)
term comes from minimizing over z(i)
Before we give the bound propagation procedure, we ﬁrst give a technical lemma:

j ≤ 1.

Lemma A.1. Given u ≥ 0, l ≤ 0, π ≥ 0, γ ≥ 0, and π + γ = C, and deﬁne the function:

g(π, γ) = −ReLU(uγ + lπ + q) + lπ

16

(cid:105)

(17)

(18)

(19)

(20)

(21)

(22)

Then

max
π≥0,γ≥0

g(π, γ) =






lπ∗,
0,
−q,

if −uC ≤ q ≤ −lC
if q < −uC
if q > −lC

where the optimal values for π and γ are:
(cid:18)
(cid:19)

π∗ = max

min

(cid:18) uC + q
u − l

(cid:19)

, C

, 0

,

γ∗ = max

(cid:18)

min

(cid:18) −lC − q
u − l

(cid:19)

(cid:19)

, C

, 0

Proof. Case 1: when uγ + lπ + q ≥ 0, the objective becomes:

max
π≥0,γ≥0

g(π, γ) = −uγ − q

s.t. π + γ = C
uγ + lπ + q ≥ 0

Since q is a constant and the object only involves a non-negative variable γ with non-positive
coefﬁcient −u, γ needs to be as smaller as possible as long as the constraint is satisﬁed. Substitute
π = C − γ into the constraint uγ + lπ + q ≥ 0,

uγ + l(C − γ) + q ≥ 0 ⇒ γ ≥

−lC − q
u − l

Considering γ is within [0, C], the optimal γ and π are:

γ∗ = max

(cid:18)

min

(cid:18) −lC − q
u − l

, C

(cid:19)(cid:19)

,

π∗ = max

(cid:18)

min

(cid:18) uC + q
u − l

, C

(cid:19)(cid:19)

Case 2: when uγ + lπ + q ≤ 0, the objective becomes:

max
π≥0,γ≥0

g(π, γ) = lπ

s.t. π + γ = C
uγ + lπ + q ≤ 0

Since q is a constant and the object only involves a non-negative variable π with non-positive
coefﬁcient l, π needs to be as smaller as possible as long as the constraint is satisﬁed. Substitute
γ = C − π into the constraint uγ + lπ + q ≤ 0,

u(C − π) + lπ + q ≤ 0 ⇒ π ≥

uC + q
u − l

The optimal π∗ and γ∗ are the same as in case 1. The optimal objective can be obtained by substitute
π∗ and γ∗ into g(π, γ). The objective depends on q because when q is too large (q ≥ −lC) or too
small (q ≤ −uC), π∗ and γ∗ are ﬁxed at either 0 or C.

With this lemma, we are now ready to prove Theorem 3.1, the bound propagation rule with general
cutting planes (GCP-CROWN):
Theorem 3.1 (Bound propagation with general cutting planes). Given the following bound propaga-
tion rule on ν with optimizable parameter 0 ≤ α(i)

j ≤ 1 and β ≥ 0:

:,j − β(cid:62)(H (i)

:,j + G(i)
:,j ),

ν(L) = −1
ν(i)
j = ν(i+1)(cid:62)W(i+1)
ν(i)
j = −β(cid:62)H (i)
:,j ,
ˆν(i)
j

:,j − β(cid:62)G(i)
:= ν(i+1)(cid:62)W(i+1)
(cid:32) u(i)
(cid:32)
]+ + β(cid:62)Q(i)
j [ˆν(i)
j
:,j
j − l(i)
u(i)

:= max

min

:,j

j

ν(i)
j

− α(i)

j [ˆν(i)

j

(cid:33)

(cid:33)

, [ˆν(i)
j

]+

, 0

17

j ∈ I +(i), i ∈ [L − 1]

j ∈ I −(i), i ∈ [L − 1]

j ∈ I (i), i ∈ [L − 1]

]− − β(cid:62)H (i)
:,j

j ∈ I (i), i ∈ [L − 1]

Then f ∗

LP-cut is lower bounded by the following objective with any valid 0 ≤ α ≤ 1 and β ≥ 0:

g(α, β) = −(cid:15)(cid:107)ν(1)(cid:62)W(1)x0(cid:107)1 −

L
(cid:88)

i=1

ν(i)(cid:62)b(i) − β(cid:62)d +

L−1
(cid:88)

(cid:88)

i=1

j∈I(i)

h(i)
j (β)

where h(i)

j (β) is deﬁned as:

h(i)
j (β) =






j l(i)
u(i)

]++l(i)
j [ˆν(i)
j
j −l(i)
u(i)

j

j β(cid:62)Q(i)

:,j

0
β(cid:62)Q(i)
:,j

j

if

j [ˆν(i)
l(i)
if β(cid:62)Q(i)
if β(cid:62)Q(i)

]+ ≤ β(cid:62)Q(i)
:,j ≤ u(i)
j [ˆν(i)
]+
j [ˆν(i)
]+

:,j ≥ u(i)
:,j ≤ l(i)

j

j

j [ˆν(i)

j

]+

Proof. Given (22), for j ∈ I (i), observing that the upper and lower bounds of ReLU relaxations
for a single neuron cannot be tight simultaneously [58], π(i)
cannot be both
non-zero2. Thus, we have

j + γ(i)

j + τ (i)

and µ(i)

j

j

j + γ(i)
π(i)

j =

j + τ (i)
µ(i)

j =

(cid:104)

(cid:104)

ν(i+1)(cid:62)W(i+1)

:,j − β(cid:62)G(i)

:,j

ν(i+1)(cid:62)W(i+1)

:,j − β(cid:62)G(i)

:,j

(cid:105)

(cid:105)

+

−

:=

:=

(cid:104)

ˆν(i)
j
(cid:104)
ˆν(i)
j

(cid:105)

+
(cid:105)

−

,

.

(23)

(24)

:,j − β(cid:62)G(i)

:= ν(i+1)(cid:62)W(i+1)

To avoid clutter, we deﬁne ˆν(i)
j
[ · ]− := min(0, ·).
To derived the proposed bound propagation rule, in (17), we must eliminate variable γ(i)
j
Ignoring terms related to ν, we observe that we can optimize the term π(i)
j
j π(i)
l(i)
optimization problem on function h:

and π(i)
.
j
j γ(i)
j +
:,j ) for each j for I (i) individually. We seek the optimal solution for the follow

:,j , and we deﬁne [ · ]+ := max(0, ·) and

j − ReLU(u(i)
l(i)

j − β(cid:62)Q(i)

max
j ≥0,γ(i)
π(i)

j ≥0

j (π(i)
h(i)

j

, γ(i)
j

; β) : = π(i)
j

j − ReLU(u(i)
l(i)

j γ(i)

j + l(i)

j π(i)

j − β(cid:62)Q(i)
:,j )

(25)

s.t. π(i)

(cid:104)

ˆν(i)
j

j =

j + γ(i)
π(i)
j ≥ 0;

(cid:105)

+
γ(i)
j ≥ 0

Note that we optimize over π(i)
j
π = π(i)
j
for (25):

, γ = γ(i)
j

, u = u(i)

j , l = l(i)

and γ(i)

j here and treat β as a constant. Applying Lemma A.1 with
]+ we obtain the optimal objective

:,j , C = [ˆν(i)

j , q = −β(cid:62)Q(i)

j

h(i)
j (β) =






j

j π(i)
l(i)
0
β(cid:62)Q(i)
:,j

j

j [ˆν(i)
l(i)
if
if β(cid:62)Q(i)
if β(cid:62)Q(i)

:,j ≤ u(i)
]+ ≤ β(cid:62)Q(i)
j [ˆν(i)
]+
j [ˆν(i)
]+

:,j ≥ u(i)
:,j ≤ l(i)

j

j

j [ˆν(i)

j

]+

(cid:32)

,

π(i)
j = max

min

(cid:32) u(i)

]+ + β(cid:62)Q(i)
j [ˆν(i)
j
:,j
j − l(i)
u(i)

j

(cid:33)

(cid:33)

, C

, 0

Substitute this solution of π(i)
j
, so we can rewrite (21) as:

0 and

(cid:104)
ˆν(i)
j

(cid:105)

−

into (21), and also observe that due to (24), τ (i)

is a variable between

j

(cid:32)

ν(i)
j

:= max

min

(cid:32) u(i)

j [ˆν(i)
]+ + β(cid:62)Q(i)
:,j
j
j − l(i)
u(i)

j

(cid:33)

(cid:33)

, [ˆν(i)
j

]+

, 0

− α(i)

j [ˆν(i)

j

]− − β(cid:62)H (i)
:,j

j

(cid:54)= 0, γ(i)

2In theorey, it is possible that τ (i)
This situation may happen when u(i)
lower and upper linear equatlities. Practically, this can be avoided by adding a small δ to u(i)
scenarios u(i)
to [58].

j = 0.
is at the intersection of one
j , and in most
are loose bounds so this situation will not occur. The same argument is also applicable

j
is exactly tight and the solution x(i)
j

j = 0 or π(i)

j = γ(i)

j are l(i)

j = µ(i)

(cid:54)= 0, µ(i)
j

j or l(i)

j or l(i)

(cid:54)= 0, π(i)

(cid:54)= 0, τ (i)

j

j

j

18

Figure 3: Branch and Bound (BaB) for neural network veriﬁcation. A unstable ReLU neuron (e.g.,
x1 may receive either negative or positive inputs for some input x in perturbation set C. On the other
hand, a stable ReLU neuron is always positive or negative for all x ∈ C, so it is a linear operation and
branching is not needed.

Here 0 ≤ α(i)
j ≤ 1 is an optimizable parameter that can be updated via its gradient during bound
propagation, and any 0 ≤ α(i)
j ≤ 1 produces valid lower bounds. All above substitutions replace each
dual variable π, γ and µ to a valid setting in the dual formulation, so the ﬁnal objective g(α, β) is
always a sound lower bound of f ∗

LP-cut. This theorem establishes the soundness of GCP-CROWN.

Note that an optimal setting of α and β do not necessarily lead to the optimal primal value f ∗
LP-cut.
The main reason is that (25) gives a closed-form solution for π and γ to eliminate these variables
without considering other dual variables like ν to simplify the bound propagation process. To achieve
theoretical optimality, π and γ can also be optimized.

B More technical details and background of GCP-CROWN with MIP cuts

Branch-and-bound Our work is based on branch-and-bound (BaB), a powerful framework for
neural network veriﬁcation [10] which many state-of-the-art veriﬁers are based on [57, 16, 26, 20].
Here we give a brief introduction for the concept of branch-and-bound for readers who are not
familiar with this ﬁeld.

We illustrate the branch-and-bound process in Figure 3. Neural network veriﬁcation seeks to minimize
the objective (1): f ∗ = minx f (x), ∀x ∈ C, where f (x) is a ReLU network under our setting. A
positive f ∗ indicates that the network can be veriﬁed. However, since ReLU neurons are non-linear,
this optimization problem is non-convex, and it usually requires us to relax ReLU neurons with
linear constraints (e.g., the LP relaxation in (11)) to obtain a lower bound for objective f ∗. This
lower bound (−3.0 at the root node in Figure 3) might become negative, even if f ∗ (the unknown
ground-truth) is positive. Branch-and-bound is a systematic method to tighten this lower bound. First,
we split an unstable ReLU neuron, such as x3, into two cases: x3 ≥ 0 and x3 ≤ 0 (the branching
step), producing two subproblems each with an additional constraint x3 ≥ 0 or x3 ≤ 0. In each
subproblem, neuron x3 does not need to be relaxed anymore, so the lower bound of f ∗ becomes
tighter in each of the two subdomains in the subsequent bounding step (−3.0 becomes −2.0 and
0.5) in Figure 3. Any subproblem with a positive lower bound is successfully veriﬁed and no further
split is needed. Then, we repeat the branching and bounding steps on subproblems that still have
a negative lower bound. We terminate when all unstable neurons are split or all subproblems are
veriﬁed. In Figure 3, all leaf subproblems have positive lower bounds so the network is veriﬁed.
On the other hand, if we have split all unstable neurons and there still exist domains with negative
bounds, a counter-example can be constructed.

The contribution of this work is to improve the bounding step using cutting planes. As one can observe
in Figure 3, if we can make the bounds tighter, they approach to zero faster and less branching is
needed. Since the number of the subproblems may be exponential to the number of unstable neurons

19

-3.0-2.00.5-1.0-0.5-0.50.20.30.11.00.7All subproblems verified!Branch and Bound improves the lower boundLower bound = -3.0Bounds can be computed by bound propagation methodsLower bound = min(-2.0, 0.5) = -2.0Lower bound = min(-1.0, -0.5) = -1.0Lower bound = -0.5Lower bound = 0.1BoundBranchBranchBoundBranchBranchBoundBound𝑥3≥0𝑥3≤0𝑥4≥0𝑥7≥0𝑥6≥0𝑥4≤0𝑥7≤0𝑥7≥0𝑥7≤0𝑥6≤0𝑥𝑖unstableneuron𝑥2𝑥1𝑥3𝑥4𝑥5𝑥6𝑥7𝑥8𝑥9always activealways inactivein the worst case, it is crucial to improve the bounds quickly. Informally, cutting planes are additional
valid constraints for the relaxed subproblems, and adding these additional constraints as in (16) makes
the lower bounds tighter. We refer the readers to integer programming literature [5, 13] for more
details on cutting plane methods. Existing neural network veriﬁers mostly use bound propagation
method [61, 50, 60, 57, 16] for the bounding step thanks to their efﬁciency and scalability, but none of
them can handle general cutting planes. Our GCP-CROWN is the ﬁrst bound propagation method that
supports general cutting planes, and we demonstrated its effectiveness in Section 4 when combined
with cutting planes generated by a MIP solver.

Soundness and Completeness GCP-CROWN is sound because Theorem 3.1 guarantees that we
always obtain a sound lower bound of the veriﬁcation problem as long as valid cutting planes
(generated by a MIP solver in our case) are added. Our method, when combined with branch-and-
bound, is also complete because we provide a strict improvement in the bounding step over existing
methods such as [57], and the arguments for completeness in existing veriﬁers using branch-and-
bound on splitting ReLU neurons such as [9, 57] are still valid for this work.

C More details on experiments

C.1 Experimental Setup

Our experiments are conducted on a desktop with an AMD Ryzen 9 5950X CPU, one NVIDIA RTX
3090 GPU (24GB GPU memory), and 64GB CPU memory. Our implementation is based on the
open-source α,β-CROWN veriﬁer3 with cutting plane related code added. Both α-CROWN+MIP and
GCP-CROWN with MIP cuts use all 16 CPU cores and 1 GPU. gurobi is used as the MIP solver
in α-CROWN+MIP. Although gurobi usually outperforms other MIP solvers for NN veriﬁcation
problems, it cannot export cutting planes, so our cutting planes are acquired by the cplex [28] solver
(version 22.1.0.0). We use the Adam optimizer [32] to solve both α and β with 20 iterations. The
learning rates are set as 0.1 and 0.02 (0.01 for oval20) for optimizing α and β respectively. We
decay the learning rates with a factor of 0.9 (0.8 for oval20) per iteration. Timeout for properties in
oval20 are set as 1 hour follow the original benchmark. Timeout for oval21 and cifar10-resnet
are set as 720s and 300s respectively, the same as in VNN-COMP 2021. Timeout for SDP-FO
models are set as 600s for α-CROWN+MIP and MN-BaB, and a shorter 200s timeout is used for
GCP-CROWN with MIP cuts.

We summarize the model structures and batch size used in our experiments in Table 4. The CIFAR-10
Base, Wide and Deep models are used in oval20 and oval21 benchmarks.

Table 4: Model structures used in our experiments. The notation Conv(a, b, c) stands for a conven-
tional layer with a input channel, b output channels and a kernel size of c × c. Linear(a, b) stands
for a fully connected layer with a input features and b output features. ResBlock(a, b) stands for a
residual block that has a input channels and b output channels. We have ReLU activation functions
between two consecutive linear or convolutional layers.

Model structure

Batch size

Model name

Base (CIFAR-10)
Wide (CIFAR-10)
Deep (CIFAR-10)

Conv(3, 8, 4) - Conv(8, 16, 4) - Linear(1024, 100) - Linear(100, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)
Conv(3, 8, 4) - Conv(8, 8, 3) - Conv(8, 8, 3) - Conv(8, 8, 4) - Linear(412, 100) - Linear(100, 10)

cifar10-resnet2b
cifar10-resnet4b

Conv(3, 8, 3) - ResBlock(8, 16) - ResBlock(16, 16)) - Linear(1024, 100) - Linear(100, 10)
Conv(3, 16, 3) - ResBlock(16, 32) - ResBlock(32, 32)) - Linear(512, 100) - Linear(100, 10)

CNN-A-Adv (MNIST)
CNN-A-Adv/-4 (CIFAR-10)
CNN-B-Adv/-4 (CIFAR-10)
CNN-A-Mix/-4 (CIFAR-10)

Conv(1, 16, 4) - Conv(16, 32, 4) - Linear(1568, 100) - Linear(100, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)
Conv(3, 32, 5) - Conv(32, 128, 4) - Linear(8192, 250) - Linear(250, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)

1024
1024
1024

2048
2048

4096
4096
1024
4096

C.2 A case study on cutting planes

The effectiveness of GCP-CROWN with MIP cuts motivates us to take a more careful look at the
cutting planes generated by off-the-shelf MIP solvers, and understand how well it can contribute to

3https://github.com/huanzhang12/alpha-beta-CROWN

20

tighten the lower bounds and strengthen veriﬁcation performance. We use the oval21 as a sample
case study because GCP-CROWN with MIP cuts is very effective on this benchmark.

The oval21 benchmark has 30 instances, each with 9 target labels (properties) to verify. Among the
total of 270 properties, we ﬁlter out the easy cases where fast incomplete veriﬁers like α-CROWN
can verify directly, and 39 hard properties remain which must be solved using branch and bound
and/or cutting planes. Cutting planes are generated on these hard properties and they greatly help
GCP-CROWN.

Number of cuts used to verify each property. The maximal number of cuts applied per property
is 4,162 and the minimal number is 318. On average, we have 1,683 cuts applied to our GCP-CROWN
to solve these hard properties.

Improvements on lower bounds.
In branch-and-bound, we lower bound the objective of Eq. 1
with ReLU split constraints [57]. A tighter lower bound can reduce the number of branches used and
usually leads to stronger veriﬁcation. We measure how well the cuts generated by off-the-shelf solvers
in improving the tightness of the lower bound for veriﬁcation. Without branching and cutting planes,
the average α-CROWN lower bound is -2.54. With generated cutting planes, we can improve the
lower bound by 0.51 on average and can directly verify 4 out of 39 hard properties without branching.
The lower bound without branching can be maximally improved by 1.54 and minimally improved by
0.04.

Structure of Generated Cuts. Finally, we investigate the variables involved in each generated
cutting plane. In total, the MIP solver generates 65,647 cutting planes in total for the 39 hard
properties. 65,301 of the cuts involve variables across multiple layers. It indicates that single layer
cutting planes (e.g., constraints involving pre- and post-activation variables of a single ReLU layer
only) commonly used in previous works [47, 40] are not optimal in general. Additionally, all of
65,647 cuts have at least one ReLU integer variable z(i) used, which was not supported in existing
bound propagation methods. 65,197 of all cuts involve at least one variable of input neurons. 23,600
of all cuts have at least one pre-ReLU variables and 51,617 of them have at least one post-ReLU
variables.

21

