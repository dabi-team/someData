1
2
0
2

c
e
D
8
2

]

G
L
.
s
c
[

1
v
4
3
4
5
1
.
2
1
1
2
:
v
i
X
r
a

Adversarial Learning for Incentive Optimization in Mobile
Payment Marketing

Xuanying Chen*
Ant Financial Services Group
xuanying.cxy@antgroup.com

Sen Li
Alibaba Group
lisen.lisen@alibaba-inc.com

Zhining Liu*
Ant Financial Services Group
eason.lzn@antgroup.com

Lihong Gu
Ant Financial Services Group
lihong.glh@antgroup.com

Li Yu*
Ant Financial Services Group
jinli.yl@antgroup.com

Xiaodong Zeng
Ant Financial Services Group
xiaodong.zxd@antgroup.com

Yize Tan
Ant Financial Services Group
yize.tyz@antgroup.com

Jinjie Gu
Ant Financial Services Group
jinjie.gujj@antgroup.com

ABSTRACT
Many payment platforms hold large-scale marketing campaigns,
which allocate incentives to encourage users to pay through their
applications. To maximize the return on investment, incentive al-
locations are commonly solved in a two-stage procedure. After
training a response estimation model to estimate the usersâ€™ mo-
bile payment probabilities (MPP), a linear programming process
is applied to obtain the optimal incentive allocation. However, the
large amount of biased data in the training set, generated by the
previous biased allocation policy, causes a biased estimation. This
bias deteriorates the performance of the response model and mis-
leads the linear programming process, dramatically degrading the
performance of the resulting allocation policy. To overcome this
obstacle, we propose a bias correction adversarial network. Our
method leverages the small set of unbiased data obtained under a
full-randomized allocation policy to train an unbiased model and
then uses it to reduce the bias with adversarial learning. Offline
and online experimental results demonstrate that our method out-
performs state-of-the-art approaches and significantly improves
the performance of the resulting allocation policy in a real-world
marketing campaign.

CCS CONCEPTS
â€¢ Applied computing â†’ Marketing.
KEYWORDS
mobile payment; adversarial network; bias correction

*These authors contributed equally to this work.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CIKM â€™21, November 1â€“5, 2021, Virtual Event, QLD, Australia
Â© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00
https://doi.org/10.1145/3459637.3482052

ACM Reference Format:
Xuanying Chen*, Zhining Liu*, Li Yu*, Sen Li, Lihong Gu, Xiaodong Zeng,
Yize Tan, and Jinjie Gu. 2021. Adversarial Learning for Incentive Opti-
mization in Mobile Payment Marketing. In Proceedings of the 30th ACM
International Conference on Information and Knowledge Management (CIKM
â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY,
USA, 5 pages. https://doi.org/10.1145/3459637.3482052

1 INTRODUCTION
Mobile payments such as Alipay, WeChat Pay, Apple Pay are now
experiencing rapid growth. To maximize the return on investment,
how to allocate the user-specific incentive under budget constraints
is playing the central role in marketing budget allocation.

Chu et al. [5] [10] divided the budget allocation into two stages:
adapting a response model to estimate the userâ€™s response score, i.e.,
MPP in our paper, and applying a linear programming to obtain the
optimal incentive allocation under budget constraints. In this paper,
we will focus on the first step to learn a better response model and
not elaborate the whole literature of the second step.

It is non-trivial to estimate the response score of each user to
incentives, since the training payment data is typically generated by
previous biased allocation policy. Figure 1a shows the relationship
between MMP and the amount of incentives under random data and
biased data. Under random data, the MPP increases as the incentive
increases, which describes real user behavior. On the contrary, the
previous bias allocation policy generates a large amount of biased
data, e.g., active users have a high probability of being allocated a
small amount of incentive, while low-active users are the opposite.
As a result, the MPP-incentives curve shows a downward trend.
Specifically, due to the naturally high MPP of active users, even
if a small amount of incentives are allocated, they are more likely
to pay through the applications. On the contrary, even if a large
amount of incentives are allocated to low-active users, their interest
in paying through the applications is also low. The response model
trained with these data will over-estimate the response score on
a small amount of the incentive and under-estimate the response
score on a large amount of the incentive, shown in Figure 1b, which
cannot accurately estimate the userâ€™s response score. As a result,
the linear programming algorithm will tend to allocate a relative
small amount of incentive to users. One solution to get unbiased

 
 
 
 
 
 
(a)

(b)

Figure 1: The curve of the mobile payment probability and
the scale of incentives on payment data. (a) Pure random
data versus previous biased data; (b) Response model trained
with random data versus biased data.

estimation is to adapt a full-randomized allocation policy [3] to
collect a large amount of unbiased data, which is impractical due to
the limited budget. In this paper, we propose a price-bias correction
adversarial network (PCAN), which leverages the small set of un-
biased data obtained under a full-randomized allocation policy to
train an unbiased network and then uses the unbiased network to
reduce the price-bias in the biased network with adversarial learn-
ing. Specifically, PCAN first learns to distinguish the distribution
difference between the biased and unbiased data representation
and then teaches the biased network to generate a representation
close to the unbiased network, which can alleviate the problem of
price-bias.

Our paper is organized as follows. Related works are reviewed
in Section 2, followed by our proposed method in Section 3. Exper-
imental results are reported in Section 4 before we conclude the
paper in the last section.

2 RELATED WORK
Existing incentives allocation methods usually are divided into two
stages: (1) the response model estimates the response score; (2) the
response score is served as an input of the optimization model,
which tries to maximize the MPP under the budget constraint.
Response Model Estimation. [1, 7] introduce a dynamic market-
ing allocation budget. Through strong fitting ability, neural net-
works can achieve a high prediction accuracy, widely used in many
scenarios [12, 17, 18]. [6, 19] use machine learning techniques to es-
timate future demand for new products and provide recommended
prices for Airbnb hosts. However, as a black-box model, there are
some gaps between the prediction and decision-making of deep
neural networks [2]. Therefore, [13, 22] propose a semi-black box
model that extends the logarithmic demand curve through neural
network and graph learning to solve this problem.
Allocation Optimization. [11] proposes a fast approximation us-
ing semi-definite programming relaxation. [6] optimizes pricing
decisions by using demand predictions from the regression trees as
inputs of the price optimization model. [4, 16] through markov de-
cision process value function and the connection with continuous
sub-module function to solve the allocation optimization problem.

3 PROPOSED METHOD
From the online deployed system, we can collect the logged data
ğ· = (ğ‘‹,ğ‘‡ , ğ‘Œ ), where ğ‘¥ âˆˆ ğ‘‹ denotes a feature vector that encodes
the information of usersâ€™ demographic profiles and online behaviors.
ğ‘¡ âˆˆ ğ‘‡ represents the amount of incentives allocated to the users,
which are usually several preset numbers. ğ‘¦ âˆˆ ğ‘Œ is the label, which
denotes whether users make mobile payments.

To allocate the budget in an optimal way, a two-stage solu-
tion [13, 22] is a common choice. First, a response model ğ‘¦ = ğ‘“ (ğ‘¥, ğ‘¡)
estimates the usersâ€™ MPPs for each user described by ğ‘¥ and each
price ğ‘¡. Based on the response model, the best allocation under
the constraint of the budget ğµ can be obtained by solving a linear
programming problem. In the following subsections, we illustrate
our proposed method from the two perspectives in detail.

3.1 Response Model Estimation
With the collected data ğ·, the response model ğ‘“ can be estimated
through minimizing the following loss function:

âˆ‘ï¸

L =

(ğ‘¦,ğ‘¥,ğ‘¡ ) âˆˆğ·

ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘¦, ğ‘“ (ğ‘¥, ğ‘¡))

(1)

where ğ‘“ is usually defined as:

ğ‘“ (ğ‘¥ğ‘–, ğ‘¡ğ‘– ) = ğœ (ğ‘¤ğ‘” (â„(ğ‘¥ğ‘– )) Â· ğ‘¡ğ‘– + ğ‘¤ğ‘ (â„(ğ‘¥ğ‘– )))
(2)
where â„(ğ‘¥ğ‘– ) denotes the latent representation of the user. ğœ (Â·) de-
notes the sigmoid function. ğ‘¤ğ‘” and ğ‘¤ğ‘ are two trainable functions.
To further meet the monotonicity constraint, a semi-black-box
model with Softplus (SBBM-Softplus) [13] is introduced to guaran-
tee the positive effect of the treatment:

ğ‘“ (ğ‘¥ğ‘–, ğ‘¡ğ‘– ) = ğœ (SOFTPLUS[ğ‘¤ğ‘” (â„(ğ‘¥ğ‘– ))] Â· ğ‘¡ğ‘– + ğ‘¤ğ‘ (â„(ğ‘¥ğ‘– )))

(3)

where SOFTPLUS(ğ‘¥) = ln(1 + ğ‘’ğ‘¥ ).

3.2 Price-Bias Correction Adversarial Network
However, due to the budget constraints, we would not deploy a
random allocation policy online. Training based on logged data
from a biased allocation policy,ğ‘“ (ğ‘¥ğ‘–, ğ‘¡ğ‘– ) is easily biased. As pointed
by [15, 21], the observed accurate response score is estimated using
Inverse Propensity Scoring (IPS)-based methods:

ğ‘“ğ‘¡ğ‘Ÿğ‘¢ğ‘’ (ğ‘¥ğ‘–, ğ‘¡ğ‘– ) =

ğ‘“ (ğ‘¥ğ‘–, ğ‘¡ğ‘– )
ğœ‹ (ğ‘¡ğ‘– |ğ‘¥ğ‘– )

(4)

where ğœ‹ (ğ‘¡ğ‘– |ğ‘¥ğ‘– ) denotes the probability for the user ğ‘¥ğ‘– to be allocated
with the incentive ğ‘¡ğ‘– .

But IPS-based estimators cannot handle well significant shifts in
exposure probability between treatment and control policies under
biased exposure. Therefore, to correct this bias, in our learning
setup, we assume that we have access to a large sample ğ·ğ‘ from the
online biased allocation policy and a small sample ğ·ğ‘¢ from the ran-
domized allocation policy, i.e., ğ· = ğ·ğ‘ âˆª ğ·ğ‘¢ . Specifically, as shown
in Figure 2, our entire model includes two subnets named BiasedNet
ğ‘“ğ‘ and UnbiasedNet ğ‘“ğ‘¢ . Two subnets are trained to optimize Lğ‘
and Lğ‘¢ , respectively, where Lğ‘ and Lğ‘¢ are defined as:

Lğ‘ =

Lğ‘¢ =

âˆ‘ï¸

(ğ‘¦,ğ‘¥,ğ‘¡ ) âˆˆğ·ğ‘
âˆ‘ï¸

(ğ‘¦,ğ‘¥,ğ‘¡ ) âˆˆğ·ğ‘¢

ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘¦, ğ‘“ (ğ‘¥, ğ‘¡))

ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘¦, ğ‘“ (ğ‘¥, ğ‘¡))

(5)

406080100120140160The amount of incentives0.40.50.60.70.80.9MMPrandom databiased data406080100120140160The amount of incentives0.500.550.600.650.700.75MMPunbias modelbias modelFigure 2: An illustration of the proposed framework.

Although the distributions of ğ·ğ‘ and ğ·ğ‘¢ are different, we as-
sume that the same set of users should have a fixed distribution
representation. Therefore, we would like to enforce â„ğ‘ (ğ‘¥) to follow
the same distribution of â„ğ‘¢ (ğ‘¥) to alleviate the bias. Specifically, we
realize the goal of debiasing using adversarial learning through
optimize the following objective function:

min
â„ğ‘

max
ğ‘‘

E
ğ‘¥âˆ¼ğ·ğ‘¢

[log ğ‘‘ (â„ğ‘¢ (ğ‘¥))] + E

ğ‘¥âˆ¼ğ·ğ‘

[log(1 âˆ’ ğ‘‘ (â„ğ‘ (ğ‘¥))]

(6)

As shown in the equation, two components play the central role
of representation generator and bias discriminator. We describe
two parts in detail in the following subsections.

3.2.1 Representation Generator. BiasedNet, which also acts as the
representation generator to generate a biased latent representation
in the training process of the adversarial network. In contrast, a
user described by â„ğ‘¢ (ğ‘¥) in UnbiasedNet represents an unbiased
latent representation, which serves as the supervision of â„ğ‘ (ğ‘¥). The
representation generator will align â„ğ‘ to â„ğ‘¢ to eliminate deviation.

3.2.2 Bias Discriminator. We introduce the bias discriminator ğ‘‘ to
determine which dataset it comes from, i.e., ğ·ğ‘ and ğ·ğ‘¢ . Specifically,
one batch of training data is mixed with unbiased/biased samples,
and ğ‘‘ is trained to maximize the probability of correctly identifying
which dataset â„ğ‘ (ğ‘¥) is generated from. In an adversarial way, â„ğ‘ is
trained to maximize the probability of ğ‘‘ (ğ‘¥) incorrectly identifying.
Since â„ğ‘¢ is trained by unbiased data, the learned representation
â„ğ‘¢ (ğ‘¥) is assumed to be unbiased. Under the guidance of the ad-
versarial network, BiasNet can gradually learn the distribution of
â„ğ‘¢ (ğ‘¥), and finally achieve bias correction.

3.2.3 Optimization Algorithm. To train the response model, we
need to alternately optimize Eq. 5 and Eq. 6, which requires carefully
update the discriminator and generator. Algorithm 1 summarizes
the training details. The algorithm includes two phases of training.
The first phase is the warm-up [20], which is designed to ensure
the training of adversarial components to start from a relatively
good situation. Here the warm-up step ğ‘›ğ‘¤ is set to 100 in this paper.
Then in the second phase, we begin to update the representation
generator and bias discriminator under the framework of adversar-
ial learning. Moreover, to enhance the stability of training [8], we
alternate between five steps of optimizing ğ‘‘ (ğ‘¥) and one step of op-
timizing â„ğ‘” (ğ‘¥). The algorithm finally returns the learned response
model ğ‘“ âˆ— after convergence.

Algorithm 1 Training Process of PCAN
Input:

Unbiased dataset ğ·ğ‘¢ , biased dataset ğ·ğ‘ and warm-up step ğ‘›ğ‘¤

Output:

Response Model ğ‘“ âˆ—

step â† 0

1: Random initialization of the model
2:
3: while not converged do
4: while step <= ğ‘›ğ‘¤ do
5:

Sample one batch of data from ğ·ğ‘¢ and ğ·ğ‘ , respectively
Update BiasedNet and UnbiasedNet by optimizing Eq.5
step â† step + 1

end while

8:
9: while step <= ğ‘›ğ‘¤ + 6 do
10:

Sample one batch of data from ğ·ğ‘¢ and ğ·ğ‘ , respectively
Update alternately BiasedNet and ğ‘‘ by optimizing Eq. 6
step â† step + 1

6:

7:

11:

12:

13:

end while
step â† 0

14:
15: end while
16: Return BiasedNet.

3.3 Allocation Optimization as Linear

Programming

Based on the estimated response model, we can get the mobile
payment probability ğ‘“ (ğ‘¥ğ‘–, ğ‘¡ ğ‘— ) for each user with the short form
ğ‘“ğ‘– ğ‘— . Assuming that ğ‘‡ is the treatment list, which is defined as
ğ‘‡ = (ğ‘¡1, ..., ğ‘¡ |ğ‘‡ |). Then we formalize the allocation as a linear pro-
gramming problem given the payment probability ğ‘“ğ‘– ğ‘— and the bud-
get ğµ, and the objective is to maximize the sum of mobile payment
probability over the user set ğ‘€:

ğ‘€
âˆ‘ï¸

ğ‘˜ğ‘–

|ğ‘‡ |
âˆ‘ï¸

max

ğ‘“ğ‘– ğ‘— âˆ— ğ‘ğ‘– ğ‘—

ğ‘–=1
ğ‘—=1
ğ‘˜ğ‘– (cid:205)|ğ‘‡ |
ğ‘—=1
(cid:205)ğ‘€
ğ‘˜ğ‘–
ğ‘–=1
where ğ‘˜ğ‘– is the number of incentives. ğ‘ğ‘– ğ‘— is the indicator of whether
choosing the incentive ğ‘¡ ğ‘— for user ğ‘¥ğ‘– . The optimal solution is :

(cid:205)ğ‘€
ğ‘–=1

ğ‘ğ‘– ğ‘— âˆ— ğ‘¡ ğ‘—

<= ğµ

ğ‘ .ğ‘¡ .

(7)

ğ‘ğ‘Ÿğ‘” min

ğ‘“ğ‘– ğ‘— âˆ’ ğœ†(ğ‘¡ ğ‘— âˆ’ ğµ)

ğ‘—
where ğœ† denote the dual optimal.

ğ‘“ ğ‘œğ‘Ÿ

ğ‘— = 1, ..., |ğ‘‡ |

(8)

â€¦â€¦BiasedNetMLPBiasedSampleâ€¦â€¦MLPUnbiasedSampleUnbiasedNetMLPSharedEmbeddingOrBiasness DiscriminatorIsbiased?MLPMLPBiasedResponseUnbiasedResponseMerchant profilefeatureHistory tradefeatureContextfeatureRepresentationGenerator4 EXPERIMENTS
In this section, we conduct offline and online experiments on the
proposed method to demonstrate its effectiveness. Before diving
into experimental results, we first introduce experiment settings.

4.1 Experimental Settings
In this section, we first introduce our dataset for training the model
and experimental settings.

4.1.1 Dataset. We collected two experimental datasets separately
from two real-world mobile marketing campaigns.
Dataset A: Contains more than 50 million samples which include
11 kinds of incentives. 5% of the data is collected by a random
strategy. The remaining 95% is collected by the biased strategy.
Dataset B: Contains millions of samples which include 16 kinds of
incentives, and the sample ratio is the same as dataset A.
Test Set: All the test set in this paper is the fully random dataset.

4.1.2 Comparison Methods. We compare our method with several
baselines:

â€¢ SBBM-U [22]: A baseline model training based on ğ·ğ‘¢ .
â€¢ SBBM-B [22]: Similar with SBBM-U, which train the re-

sponse model on the dataset ğ· = ğ·ğ‘¢ âˆª ğ·ğ‘ .

â€¢ SBBM-Sp [13]: A response model with the constraint of the

monotonicity trained based on ğ· = ğ·ğ‘¢ âˆª ğ·ğ‘ .

â€¢ IPW [14]: The method which weights each sample with the

inverse of its propensity score.

â€¢ CausE [3]: The approach uses the data from ğ· = ğ·ğ‘¢ âˆª ğ·ğ‘ ,
introducing a regularizer to minimize the difference between
the weights of the two models.

4.1.3 Metrics. We introduce Area-Under-Curve (AUC) to quantita-
tively measure the performance of different methods. Besides, price
calibration error (PCE) is also introduced according to expected
calibration error (ECE) [9], which is defined as:
ğ‘€
âˆ‘ï¸

PCE =

ğ‘–=1

1
ğ‘€

| E
ğ‘– âˆˆğ‘Ÿğ‘’ğ‘ğ‘™

[ğ‘¦ğ‘– ] âˆ’ E

[ğ‘¦ğ‘– ]|

ğ‘– âˆˆğ‘ğ‘Ÿğ‘’ğ‘‘

(9)

where ğ‘€ represents the number of amounts of the incentive. PCE is
designed to measure the difference between the average prediction
scores and labels. Due to the confidentiality of data, all metrics are
presented the relative improvement over the baseline.

Table 1 shows the experiment result on the dataset. We can
observe that PCAN performs the best on the two metrics overall
baseline methods across all the two datasets. To further demon-
strate the effectiveness of the proposed method in correcting the
bias, we show average response scores over a group of users with
randomly allocated amount of incentives which shows the aver-
age estimated MPP of different models for different amounts in
Figure 3a. Comparing these models trained on the biased dataset,
PCAN successfully captures the monotonicity. Besides, PCAN is
the most similar to the average score of an actual label in most
cases.

4.2 Offline Results
In addition, all the above results are tested with the ratio of |ğ·ğ‘¢ | :
|ğ·ğ‘ | = 19 : 1. To prove the robustness of our model, we change the

Table 1: Performance comparison of different methods over
SBBM-U.

Model
Data SBBM-U SBBM-B SBBM-Sp

IPW CausE

PCAN

AUC

PCE

A
B

A
B

-
-

-
-

-83.16% +5.67% +3.87% +18.32% +61.92%
+7.74% +6.16% +8.76% +9.78%
-1.54%

-38.38% +36.58% +41.80% +39.30% +44.35%
-9.58% +15.01% +14.12% +17.36% +19.04%

(a)

(b)

Figure 3: (a) The response curve of different methods; (b) The
performance of difference methods over different ratio of
random data.

proportion of |ğ·ğ‘¢ | to view the performance of each model. It can
be seen in Figure 3b that 1) As the proportion of |ğ·ğ‘¢ | increases, the
performance of all models gradually improves. 2) As |ğ·ğ‘¢ | shrinks,
the performance of all methods gradually deteriorates. However,
our proposed framework, PCAN, still performs better than other
frameworks. In particular, for other methods, the AUC value is
strongly affected by the size of |ğ·ğ‘¢ |, but our method is still stable.
Since the cost of an unbiased sample |ğ·ğ‘¢ | is relatively high in actual
marketing activities, PCAN can achieve good results at a relatively
low |ğ·ğ‘¢ | sample ratio, save a lot of costs, and has strong robustness.

4.3 Online Results
To verify the proposed methodâ€™s effectiveness, we further conducted
an A/B test against a baseline (SBBM-Softplus). In the A/B test,
we first randomly split all candidates into two buckets. Under the
same budget, the baseline and our approach allocate the incentives
to users in the two buckets based on estimated response models,
respectively. Over two marketing campaigns, we both observe an
over 3% increase in the number of mobile payments.

5 CONCLUSION
In this paper, we propose an adversarial learning method for incen-
tive optimization in mobile payment marketing. We identify the bias
in the response model estimated from the biased data by analyzing
the response-incentive curve. We further introduce the mechanism
of adversarial learning to build an unbiased response model. Com-
paring with other state-of-the-arts methods, The online experiment
results verify that our proposed method can significantly increase
mobile payment usage under a limited budget. Future work will
design a more effective method with approximate the accurate
response model with less randomized allocation data.

406080100120140160the amount of incentives0.30.40.50.60.70.8mobile payment probabilityREALSBBM-USBBM-SoftplusIPWPCANCausESBBM-B0.625%1.25%2.5%5%10%the ratio of unbias data0.500.550.600.650.700.750.80AUCSBBM-USBBM-BIPWCausEPCANREFERENCES
[1] Shahriar Akter and Samuel Fosso Wamba. 2016. Big data analytics in E-commerce:
a systematic review and agenda for future research. Electronic Markets 26, 2
(2016), 173â€“194.

[2] Susan Athey. 2017. Beyond prediction: Using big data for policy problems. Science
(New York, N.Y.) 355, 6324 (February 2017), 483â€”485. https://doi.org/10.1126/
science.aal4321

[3] Stephen Bonner and Flavian Vasile. 2018. Causal embeddings for recommendation.
In Proceedings of the 12th ACM Conference on Recommender Systems. 104â€“112.
[4] Craig Boutilier and Tyler Lu. 2016. Budget allocation using weakly coupled,

constrained Markov decision processes. (2016).

[5] Kyounghee Chu, Soyeon Kim, and Changhui Choi. 2013. A Study on the impact
of weather on sales and optimal budget allocation of weather marketing. Journal
of the Korean Operations Research and Management Science Society 38, 1 (2013),
153â€“181.

[6] Kris Johnson Ferreira, Bin Hong Alex Lee, and David Simchi-Levi. 2016. Analytics
for an online retailer: Demand forecasting and price optimization. Manufacturing
& Service Operations Management 18, 1 (2016), 69â€“88.

[7] Marc Fischer, SÃ¶nke Albers, Nils Wagner, and Monika Frie. 2011. Practice Prize
Winnerâ€”Dynamic Marketing Budget Allocation Across Countries, Products, and
Marketing Activities. Marketing Science 30, 4 (July 2011), 568â€“585.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672â€“2680.

[9] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration

of modern neural networks. arXiv preprint arXiv:1706.04599 (2017).

[10] Sunil Gupta and Thomas Steenburgh. 2008. Allocating marketing resources.
Marketing Mix Decisions: New Perspectives and Practices, Roger A. Kerin and Rob
Oâ€™Regan, eds., American Marketing Association, Chicago, IL (2008), 90â€“105.
[11] Shinji Ito and Ryohei Fujimaki. 2017. Optimization beyond prediction: Prescrip-
tive price optimization. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1833â€“1841.

[12] Binbing Liao, Jingqing Zhang, Chao Wu, Douglas McIlwraith, Tong Chen, Sheng-
wen Yang, Yike Guo, and Fei Wu. 2018. Deep sequence learning with auxiliary
information for traffic prediction. In Proceedings of the 24th ACM SIGKDD Inter-
national Conference on Knowledge Discovery & Data Mining. 537â€“546.

[13] Ziqi Liu, Dong Wang, Qianyu Yu, Zhiqiang Zhang, Yue Shen, Jian Ma, Wenliang
Zhong, Jinjie Gu, Jun Zhou, Shuang Yang, et al. 2019. Graph Representation
Learning for Merchant Incentive Optimization in Mobile Payment Marketing.
In Proceedings of the 28th ACM International Conference on Information and
Knowledge Management. 2577â€“2584.

[14] Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity
score in observational studies for causal effects. Biometrika 70, 1 (1983), 41â€“55.
[15] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and
Thorsten Joachims. 2016. Recommendations as treatments: Debiasing learning
and evaluation. arXiv preprint arXiv:1602.05352 (2016).

[16] Matthew Staib and Stefanie Jegelka. 2017. Robust budget allocation via continuous
submodular functions. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70. JMLR. org, 3230â€“3240.

[17] Souhaib Ben Taieb, Jiafan Yu, Mateus Neves Barreto, and Ram Rajagopal. 2017.
Regularization in Hierarchical Time Series Forecasting with Application to Elec-
tricity Smart Meter Data.. In Aaai. 4474â€“4480.

[18] Yongxin Tong, Yuqiang Chen, Zimu Zhou, Lei Chen, Jie Wang, Qiang Yang,
Jieping Ye, and Weifeng Lv. 2017. The simpler the better: a unified approach to
predicting original taxi demands based on large-scale online platforms. In Pro-
ceedings of the 23rd ACM SIGKDD international conference on knowledge discovery
and data mining. 1653â€“1662.

[19] Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, Spencer
De Mars, Frank Yang, and Li Zhang. 2018. Customized regression model for
airbnb dynamic pricing. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 932â€“940.

[20] Yang You, Igor Gitman, and Boris Ginsburg. 2017. Large batch training of convo-

lutional networks. arXiv preprint arXiv:1708.03888 (2017).

[21] Bowen Yuan, Jui-Yang Hsia, Meng-Yuan Yang, Hong Zhu, Chih-Yao Chang, Zhen-
hua Dong, and Chih-Jen Lin. 2019. Improving ad click prediction by considering
non-displayed events. In Proceedings of the 28th ACM International Conference on
Information and Knowledge Management. 329â€“338.

[22] Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang.
2019. A Unified Framework for Marketing Budget Allocation. arXiv preprint
arXiv:1902.01128 (2019).

