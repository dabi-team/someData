Improving Fault Localization by Integrating Value
and Predicate Based Causal Inference Techniques

Yi˘git K¨uc¸ ¨uk,
Department of Computer and Data Sciences
Case Western Reserve University
Cleveland, OH, USA
yxk368@case.edu

Tim A. D. Henderson,
Google Inc.
Mountain View, CA, USA
tadh@google.com

Andy Podgurski
Department of Computer and Data Sciences
Case Western Reserve University
Cleveland, OH, USA
podgurski@case.edu

1
2
0
2

b
e
F
1
1

]
E
S
.
s
c
[

1
v
2
9
2
6
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—Statistical fault localization (SFL) techniques use
execution proﬁles and success/failure information from software
executions, in conjunction with statistical inference, to automati-
cally score program elements based on how likely they are to be
faulty. SFL techniques typically employ one type of proﬁle data:
either coverage data, predicate outcomes, or variable values.
Most SFL techniques actually measure correlation, not causation,
between proﬁle values and success/failure, and so they are subject
to confounding bias that distorts the scores they produce. This
paper presents a new SFL technique, named UniVal, that uses
causal inference techniques and machine learning to integrate
information about both predicate outcomes and variable values
to more accurately estimate the true failure-causing effect of
program statements. UniVal was empirically compared to several
coverage-based, predicate-based, and value-based SFL techniques
on 800 program versions with real faults.

I. INTRODUCTION
There has been a vast amount of research on automated soft-
ware fault localization (AFL) [1]–[4], which seeks to automate
all or part of the process of locating the software faults that
are responsible for observed software failures. Statistical fault
localization (SFL) or spectrum-based fault localization [1],
[2], [4], [5] comprises a large body of AFL techniques that ap-
ply statistical measures of association — some computed with
machine learning or data mining techniques — to execution
data (execution proﬁles or spectra) and to failure data (e.g.,
pass/fail labels) in order to compute putative measures, called
suspiciouness scores, of the likelihood that individual program
statements or other program elements are responsible for
observed failures. These scores are used to guide developers to
faults, e.g., by using them to highlight suspicious statements
in graphical displays of code [1] or to rank statements for
inspection [4]. Statistical fault localization is also the ﬁrst step
in a number of automated program repair techniques [6], [7].
SFL techniques are applicable when the data they require
are readily and cheaply available in sufﬁcient quantity. In
they generally require data from a diverse and
particular,
penetrating set of tests or operational executions that includes
signiﬁcant numbers of both failures and successes. Such data
might be available, for instance, from deployed software that is
instrumented to record proﬁle data and that is equipped with a
mechanism by which users may report failures they encounter.
It seems fair to say that statistical fault localization research
stands at a crossroads. Although a large number of SFL

techniques have been proposed, to our knowledge none of
them consistently locates faults with enough precision to
justify its widespread use in industry. In part, this is because
most such techniques rely on just one source of information
about internal program behavior: code coverage proﬁles, or,
similarly, recorded outcomes of program predicates such as
those that control the execution of conditional branches and
loops. Recent work has sought to overcome this limitation,
e.g., by employing value proﬁles/spectra (proﬁles of vari-
able values) [8]–[10] or by combining different kinds of
information pertinent to fault localization, such as coverage-
based SFL scores,
textual similarity measures, and fault-
proneness predictions based on static program analysis [11]–
[14]. Another problem with existing SFL techniques is that
many of them suffer from confounding bias, which can cause
a correct statement to appear suspicious because of another,
faulty statement that inﬂuences its execution. Recent work has
also sought to overcome this problem by employing causal
inference techniques [15]–[19].

Existing SFL techniques are based on analysis of code
coverage proﬁles or predicate proﬁles, on one hand, or of value
proﬁles, on the other hand. To our knowledge, no previous
technique is both coverage-based (or predicate-based) and
value-based. While two techniques, one of each kind, can be
combined simply by taking the average or maximum of the
scores they produce for each potential fault location, this will
not in itself properly address confounding bias.

This paper proposes a new approach to statistical fault
localization that integrates information about both predicate
outcomes and variable values, and that does so in a principled
way that controls for confounding bias. The key to this
approach, which we call UniVal, is to transform the program
under analysis so that branch and loop predicate outcomes
become variable values, so that one causally sound value-based
SFL technique can be applied to both variable assignments
and predicates. This paper reports on an large-scale empirical
evaluation of UniVal involving the latest version (2.0.0) of
the widely used Defects4J evaluation framework [20], which
contains seventeen software projects and 835 program versions
containing actual faults. In this study, UniVal was compared to
several coverage-based, predicate-based, and value-based SFL
techniques. UniVal substantially out-performed each of these

 
 
 
 
 
 
techniques. To the best our knowledge this is the only fault
localization study that has used all the programs in the latest
version of Defects4J, and it uses the largest number of real
programs and faults of any study. We also report empirical
results characterizing the relationship between the cost of fault
localization and an important property of data in a causal
inference study called covariate balance. These results help
explain the effectiveness of UniVal.

II. MOTIVATING EXAMPLE
To illustrate our technique, we now apply UniVal to locate
a real software fault. This example is from the 62nd faulty
version of Google’s Closure Compiler (Closure 62) from
the widely used Defects4J [20] repository (v2.0). This fault
exists in the format method, which is located between lines
66-111 in the LightweightMessageFormatter class.
The faulty segment of the code is depicted in Listing 1 with
some elements omitted for brevity, and with the lines of code
renumbered.

The method format is intended to compose an error
message for a script written in Javascript. As input parameters,
format takes a JSError object named error, which
contains information about
the nature of the error, and a
boolean value named warning, which indicates whether the
error message should be formatted as a warning message.
However, this method has a faulty predicate at line 5 (“<” is

1
2
3
4
5

6
7
8
9
10
11
12
13
14
15
16

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

private String format(JSError error, boolean warning) {

...
int charno = error.getCharno(); // Column number within line
// Fix: && charno <= sourceExcerpt.length()
if (excerpt.equals(LINE) && 0 <= charno && charno <

sourceExcerpt.length()) {

for (int i = 0; i < charno; i++) {

char c = sourceExcerpt.charAt(i);
if (Character.isWhitespace(c)) {

b.append(c);

} else {

b.append(’ ’);

}

}
b.append("ˆ\n");

}
...

Listing 1: Original code for the motivating example

private String format(JSError error, boolean warning) {

...
int charno = error.getCharno();
boolean P3_0 = false;
boolean P3_1 = false;
boolean P3_2 = false;
if ((P3_2 = (excerpt.equals(LINE)))
&& (P3_1 = (0 <= charno))
&& (P3_0 = (charno < sourceExcerpt.length()))) {

boolean P4_0 = false;
for (int i = 0; (P4_0 = (i < charno)); i++) {

char c = sourceExcerpt.charAt(i);
boolean P5_0 = false;
if ((P5_0 = (Character.isWhitespace(c)))) {

b.append(c);

} else {

b.append(’ ’);

}

}
b.append("ˆ\n");

}
...

used instead of “≤”) that prevents the error column number,
charno, from being displayed correctly for some inputs to the
method. The ﬁx for the faulty predicate expression is shown
in a comment on line 4.

The code in Listing 1 illustrates how confounding bias
[21] may arise in fault localization: the outcome of the faulty
predicate at line 5 confounds the causal effects on program
failure of the statements at
lines 6-14. This implies that
unless we adjust for confounding, the suspiciousness scores
calculated for the variables and predicates located at those
lines are likely to be biased and distorted.

In order to apply UniVal and other fault localization tech-
niques to the code in Listing 1, we ﬁrst used a tool we devel-
oped named PredicateTransformer to transform the predicates
in branch and loop conditions into assignment statements that
assign the results of evaluating the predicates to new boolean
variables. Note that each predicate in a compound boolean
expression is transformed in this way. The transformed code
is depicted in Listing 2.

Next, we used our instrumentation tool GSA Gen on the
predicate-transformed program to generate our own variant of
the gated static single-assignment (GSA) form [22] representa-
tion of the program. This tool collects information about data
dependencies between variables, and it also instruments the
program to record the runtime values of assignment targets in
a dictionary data structure that is maintained by a Java class we
created named CollectOut. The variable values are recorded by
inserting calls to a static method CollectOut.record() (referred
to as record() in Listing 3), which takes the following

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38

private String format(JSError error, boolean warning) {

...
int charno = error.getCharno();
record("Formatter", "format", 88, 2, "charno_0", charno, 0);
boolean P3_0 = false;
record("Formatter", "format", 92, 2, "P3_0_0", P3_0, 0);
boolean P3_1 = false;
record("Formatter", "format", 93, 2, "P3_1_0", P3_1, 0);
boolean P3_2 = false;
record("Formatter", "format", 94, 2, "P3_2_0", P3_2, 0);
if ((P3_2 = (excerpt.equals(LINE)))
&& (P3_1 = (0 <= charno))
&& (P3_0 = (charno < sourceExcerpt.length()))) {

record("Formatter", "format", 109, 2, "charno_2", charno, 2);
record("Formatter", "format", 109, 2, "charno_3", charno, 3);
record("Formatter", "format", 109, 2, "P3_2_1", P3_2, 1);
record("Formatter", "format", 109, 2, "P3_1_1", P3_1, 1);
record("Formatter", "format", 109, 2, "P3_0_1", P3_0, 1);
boolean P4_0 = false;
record("Formatter", "format", 98, 3, "P4_0_0", P4_0, 0);
for (int i = 0; (P4_0 = (i < charno)); i++) {

record("Formatter", "format", 99, 3, "P4_0_1", P4_0, 1);
record("Formatter", "format", 99, 3, "charno_1", charno, 1);
char c = sourceExcerpt.charAt(i);
record("Formatter", "format", 100, 4, "c_0", c, 0);
boolean P5_0 = false;
record("Formatter", "format", 101, 4, "P5_0_0", P5_0, 0);
if ((P5_0 = (Character.isWhitespace(c)))) {

record("Formatter", "format", 106, 4, "P5_0_1", P5_0, 1);
record("Formatter", "format", 106, 4, "c_1", c, 1);
b.append(c);

} else {

b.append(’ ’);

}

}
b.append("ˆ\n");

}
...

Listing 2: Predicates in Listing 1 extracted to boolean vari-
ables.

Listing 3: Instrumented version of the code that belongs to the
format method from Listing 1

parameters in the given order: package name, class name,
method name, line number encountered in the original code,
code block number, name in GSA form, value to be recorded,
and the version of the program variable. The GSA version of
the code in Listing 1 is shown in Listing 3.

Consider what happens when GSA Gen encounters the
assignment statement P3_1 = (0 <= charno) at line 8
of Listing 2. The tool then: inserts the proper GSA variable
version increments; inserts a call to CollectOut.record() to
record the value of P3_1 and the other parameters mentioned
above; and adds a key-value pair [P3_1_i,charno_j]
to the dictionary, where i and j are the respective variable
versions in the assignment statement. The value(s) for key
P3_1_i are later used for confounding adjustment (see Sec-
tion IV).

We tested the GSA transformed program with the developer-
written tests in the Defects4J suite, using the test command
to run these tests. Out of the 6050 tests, two failed with
assertion errors. We applied two of the coverage-based sta-
tistical fault localization metrics that performed the best in
recent studies [23]–[26], namely, Ochiai [5] and DStar [25],
to the code coverage and failure data for all of the tests.
We then ranked the variables and predicates of the faulty
class, LightweightMessageFormatter, in nonincreas-
ing order of their suspiciousness scores (with ties receiving
the average rank for all tied variables). The Ochiai and DStar
metrics produced identical rankings. The faulty predicate
P3_0 at line 13 in Listing 3 (line 5 in Listing 1) received
the same score as 23 other variables and ranked in 17th place
in the resulting suspiciousness list, which included all of the
variables and predicates within the if branch in the faulty
method. The low rank for the faulty predicate is likely due to
confounding, which the Ochiai and DStar metrics don’t adjust
for.

We next applied UniVal to the same transformed program.
We input the data recorded by our run-time library CollectOut
together with the values of a binary outcome variable Y ,
which indicates whether the program executions failed (1) or
succeeded (0), to our method for calculating suspiciousness
scores. This code executes the analysis phase of UniVal, which
is described in Section IV. Finally, we mapped these scored
statements back to the original program.

The suspiciousness list produced with UniVal contained
unique scores for each variable and predicate. The faulty
predicate P3_0, at line 13 in Listing 3 (line 5 in Listing 1),
was ranked highest among all the predicates with a score of
0.086, and it ranked 5th among all assignment targets. The
variables charno_2 and charno_3 recorded at lines 14
and 15 of Listing 3, which are recorded for possible use
in simulating a GSA-form φif function (see Section III-B),
represent the value of the variable charno in lines 12 and 13.
They would both be ranked 1st overall, with a score of 0.49,
if we included them in the ranking.1 The non-faulty predicate

1In our empirical evaluation, we assign scores to only the predicates in

branch conditions.

P4_0, at line 21 in Listing 3 (line 6 in Listing 1), was ranked
18th with a score of 0.001.

We performed an extra step to the experiment to illustrate
that the adjustment for confounding in UniVal indeed makes
a difference. We replicated the steps described previously for
using UniVal with a minor change for the correct predicate
P4_0, which is located at line 21 in Listing 3. We removed
P3_0 (at line 13 in Listing 3) from the set of adjustment
variables (covariates) in the random forest model for P4_0.
Consequently, the score for P4_0 increased to 0.01, which
changed its rank to 14th. The new score and rank of P4_0
were higher than those of the faulty predicate P3_0, whose
score and rank declined to 0.005 and 16th. Therefore, the
suspiciousness scores for the variables were in fact distorted
by the lack of adjustment for confounding bias.

III. BACKGROUND

A. Causal Inference

In this section we provide background on statistical causal
inference required to understand our technique, UniVal. Sta-
tistical causal inference is concerned with estimating without
bias the average causal effect of a treatment variable upon an
outcome variable. A treatment variable or exposure variable
T is a variable that an investigator could, at least in principle,
intervene upon to change its value. For example, in SFL T
might represent the outcome of a branch predicate, which a
developer could change in a debugger. An outcome variable
Y is an observable variable of interest to an investigator, such
as an indicator of whether a program execution fails or not. In
statistical causal inference, one is concerned with outcomes for
individuals or units in a population, and it is often convenient
to denote the outcome for a unit i by Yi. In SFL, the units are
typically program executions.

A key concept in causal inference is that of a counterfactual
outcome, which is a value that the outcome variable Y could
take on if, counter to the facts, the treatment variable T took
on a value t(cid:48) different from the value t that it actually took on.
A very similar concept is that of a potential outcome, which is
a value that the outcome variable could take on if the treatment
variable were assigned a particular value. As is common in the
causal inference literature, we shall use the terms “counterfac-
tual outcome” and “potential outcome” interchangeably. In the
inﬂuential Neyman-Rubin causal modeling framework [27], a
distinct potential outcome random variable Y T =t exists for
each possible value t of the outcome variable.

i

and Y T =t(cid:48)
i

In principle, for each unit i and each pair of treatment values
i − yt(cid:48)
t and t(cid:48) there is an individual causal effect δi = yt
i ,
i and yt(cid:48)
where yt
i are values of the potential outcome random
variables Y T =t
, respectively. Conceptually, the
average causal effect (ACE) of T on Y over a population
is the expected value E[δ] of the individual causal effects δi
over that population. However, in general it is not possible to
compute the δi, because at most one of Y T =t
is
observed for each i, namely, the potential outcome under the
actual treatment. This is known as the Fundamental Problem
of Causal Inference [28]. In this sense, causal inference is

and Y T =t(cid:48)
i

i

conditioning on the value of C during the analysis, e.g., by
computing causal effect estimates separately for each level or
stratum of C and then combining them via a weighted average
to obtain an estimate of the population ACE. Causal inference
theory provides a number of results that characterize the sets
of variables that may be used for confounding adjustment,
in terms of the structure of a causal DAG. For example,
the Backdoor Adjustment Theorem [21] states that a set Z
of variables that blocks every backdoor path between the
treatment variable and the outcome variable is sufﬁcient for
confounding adjustment.

Another way to estimate the average causal effect, which we
employ in this paper, is to interpolate or predict the missing
counterfactual outcome Y T =t
for each unit based
on the data for all the units in the study sample and then to
plug in the predicted value in the formula for the individual
causal effect δi.

or Y T =t(cid:48)
i

i

B. Gated Static Single Assignment Form

Gated single assignment form (GSA form) [22] is an exten-
sion of static single assignment form (SSA form) [30]. SSA
form is a specialized intermediate program representation that
makes the data ﬂow of a program explicit by ensuring that each
variable is deﬁned in exactly one location (hence the name
static single assignment form). When two or more deﬁnitions
for a variable reach a single use a new deﬁnition is created
to merge the reaching deﬁnitions by using a special “pseudo-
function” called φ which “picks” the correct deﬁnition to use
at runtime.

GSA replaces the φ function of SSA form with three gating
functions φif , φentry , and φexit (alternatively, γ, µ, and η,
respectively). φif represents the merging of control ﬂow after
an if-statement and takes as an argument the predicate in the
controlling if-statement, allowing φif
to choose the correct
value. φentry is similar to φif except it merges loop-carried
variables at the top of iterative control structures. Finally, φexit
merges variables that are both live at the exit of a loop and are
modiﬁed by that loop. Taken together these three new gating
functions effectively embed the control dependence graph [31]
into the intermediate representation by linking the choice of
the variable deﬁnition to use to the predicate which controls
the computation.

The instrumentation used by UniVal is inspired by (and
its placement is guided by) GSA form. If GSA form was
directly converted into program instrumentation it would be
expensive due to the extra runtime overhead implied by the
gating functions φif , φentry , and φexit for choosing the right
deﬁnitions. Instead instrumentation is inserted to record values
(and their controlling predicates) at the locations where the
gating functions would have appeared in GSA form. (See
Section II.) This permits us to determine causal parents needed
to control for confounding in UniVal.

IV. METHOD

Fig. 1: Example Confounding DAG
a missing data problem. In statistical causal inference, this
problem is circumvented by estimating average causal effects
from a study sample of units and by “borrowing information”
from each unit in the sample.

] = E[Y T =t]−E[Y T =t(cid:48)

In a randomized experiment, which is an interventional
study in which the the investigator assigns treatments ran-
domly to units, the missing potential outcomes are missing
completely at random, and the average causal effect E[δ] =
E[Y T =t −Y T =t(cid:48)
] can be estimated by
computing the average outcomes in the treatment groups with
T = t and T = t(cid:48) and taking their difference [29]. However,
in observational studies generally and in SFL applications in
particular, treatment assignment is usually not randomized, and
hence the difference of the treatment group averages is often
a biased estimate of the average causal effect. In SFL, for
example, whether a particular statement is executed during a
program run or whether a given variable is assigned a speciﬁc
value depends on the behavior of other statements, which may
also affect whether the program fails.

Different types of bias can affect a causal effect estimate
[29]. The best known form of systematic bias and the one
that will be addressed in this paper is confounding bias
(or simply confounding). Confounding is bias due to the
presence of a variable that is a common cause of the treatment
variable and the outcome variable. Confounding bias is best
explained in terms of a causal graph, and such graphs are
also used to identify confounders, which are variables that
can be statistically adjusted for during causal effect estimation
in order to reduce or eliminate confounding bias. A causal
directed acyclic graph or causal DAG is a DAG in which
the vertices represent causal variables and in which there is a
directed edge (A, B) or A → B between two variables A and
B only if A is known or assumed to be a cause of B.

Figure 1 is a very simple example of a DAG involving three
variables, T , C, and Y . The edge T → Y represents the direct
causal effect of T on Y . The DAG indicates that C confounds
this effect, because C is a common cause of T and Y . The
noncausal path T ← C → Y , which is called a backdoor path
because it begins with an arrow into the treatment variable,
represents a biasing ﬂow of statistical association from T to
Y via C. As a result of this ﬂow, a naive, unadjusted estimate
of the ACE would mix the causal effect of T on Y with the
association “carried” by the backdoor path. This implies that
to obtain an unbiased estimate of the ACE, C must be adjusted
for.

One way to obtain an unbiased estimate of the ACE is to
block (d-separate [21]) the backdoor path T ← C → Y by

In this section, we describe the UniVal fault localization
technique in detail. UniVal is based on predicting the values

Fig. 2: Causal fault localization method UniVal

of individual counterfactual outcomes, using machine learning
models that are trained on data from a sample consisting
(ideally) of signiﬁcant numbers of both passing and failing
executions. There is a separate model for each assignment to
a program variable, including assignments inserted into branch
predicates (e.g., lines 11-13 in Listing 3). The data required for
each execution and each assignment includes the values of the
treatment variable T , the outcome variable Y , and the set of
covariates X. The treatment variable is the assignment target,
the outcome variable indicates whether the execution passed
or failed, and the covariates are the parents of the treatment
variable (if any exist) in the causal DAG for the program
being debugged. For example, the recorded data for line 12 of
Listing 3 includes the values of P3_1 and charno as well as
the program outcome (pass or fail). Note that each backdoor
path (see Section III) in the causal DAG begins with an arrow
T ← P , where P is a parent of T . Thus, by the Backdoor
Adjustment Theorem (see Section III) the set X of parents of
T blocks all backdoor paths from T to Y , and therefore X is
sufﬁcient for confounding adjustment.

Figure 2 depicts the operation of UniVal. The ﬁrst two
steps in UniVal, which together we call the instrumentation

phase, are source-to-source transformations of the program
P to be debugged. First, each predicate in a branch con-
ditions of P is transformed into an assignment statement
that assigns the value of the predicate to a new boolean
variable. (See for example lines 7-9 of Listing 2, which
correspond to line 5 of Listing 1.) We created a prototype
tool named the PredicateTransformer for this task. This tool
also records information about each predicate, including the
type of control statement it belongs to (e.g. while, for, if-
else, if), the predicate expression, and the line number where
it was encountered in the original Java ﬁle. Second,
the
resulting program is transformed by another prototype tool
we created, named GSA Gen, into the specialized version of
Gated Single Assignment form described in Section III-B. This
entails inserting calls to a function that records the values of
treatment variables and covariates as well as other information
needed by our implementation. At the same time, the causal
parents of assignment targets (the variables whose values are
used in the assignment) are determined.

The next phase of UniVal, which we call the proﬁling phase,
involves executing the instrumented GSA version GSA(P) of
the program on a set I of test cases or operational inputs

in order to record the variable values and other information
mentioned above. Note that for a treatment variable T assigned
to in a loop, only the last value assigned to T is recorded,
along with the corresponding covariate values. It is assumed
that the program outcomes (pass or fail) for these executions
are already known or are determined prior to analysis.

In UniVal, assignment targets will have missing values in
particular program runs (which we call NAs) if their assign-
ment statements are not executed. If the treatment variable
has no recorded values or just one unique value, it is omitted
from UniVal’s consideration. Note that parts of a compound
predicate expression might have missing values due to short-
circuited evaluation.

The third phase of UniVal, which we call the analysis phase,
involves ﬁtting a counterfactual prediction model for each
assignment target in GSA(P). We adopted the counterfactual
prediction approach to analysis described in [32], although,
unlike UniVal, that work does not address predicates or strings.
In the current implementation of UniVal, we employ random
forest learners [33] as prediction models, because they are
ﬂexible enough to non-parametrically model a wide variety of
relationships between the treatment variable and covariates, on
one hand, and the outcome variable, on the other hand. Specif-
ically, we used the Ranger random forest package [34]. The
model for a given variable assignment includes the treatment
variable (the assigned variable) and the covariates (the used
variables) as predictors, and the model is used to predict the
counterfactual program outcomes (pass or fail) under different
values of the treatment variable.

For each treatment variable T , a set RepT of representative
treatment values is chosen, and counterfactual outcomes are
predicted for these treatments. RepT is chosen differently
based on the type of T . For a boolean or categorical treatment
variable T , RepT contains all recorded values of T . For a
string variable, a clustering algorithm is ﬁrst used to cluster
the recorded treatment values, and then RepT becomes the set
of cluster IDs, which are treated like categorical values. We
use the stringdist [35] package to obtain a matrix of distances
between values. This matrix is input to the distance-based
clustering algorithm DBSCAN [36] (with MinPts = 2 ∗ dim,
where dim is the dimension of the dataframe). For a numeric
treatment variable T , RepT consists of the 0.05, 0.15, . . . , 0.95
quantiles of the empirical distribution of the recorded values
of T . UniVal does not currently handle other data types.

For each representative treatment value t ∈ RepT and each
complete input i to P, UniVal predicts the counterfactual
outcome yt
i . This is done by plugging t into the model together
with the covariate values xi recorded at the assignment to
T during execution of P on i. Note that UniVal predicts
counterfactual outcomes even for actual, recorded treatment-
covariate combinations, that is, even when the true counterfac-
tual outcome is known. Given the set of predicted counterfac-
tual outcomes for T , UniVal computes for each t ∈ RepT
an estimate ˆE[Y T =t] of the counterfactual mean E[Y T =t],
by averaging the predictions {ˆyt
i }. The suspiciousness score
for T is set to the maximum, over all pairs t, t(cid:48) ∈ RepT

of ˆE[Y T =t] − ˆE[Y T =t(cid:48)
]. That is, the score is the maximum,
over all pairs of representative treatment values for T , of the
average failure-causing effect of assigning t instead of t(cid:48) to T .
The ﬁnal phase of UniVal, which we call the localization
phase, involves employing the suspiciousness scores to assist
developers in ﬁnding the cause or causes of failures observed
when P was executed on the set of inputs I. Traditionally,
this is done by ranking statements in non-increasing order
of their suspiciousness scores and then having developers
inspect statements in that order [4]. Although this is convenient
is used for that
for evaluating SFL techniques — and it
purpose in this paper — it has been argued that
this is
a simplistic approach to fault localization [37], [38], which
programmers are in many cases unlikely to follow (e.g., when
many statements in a program get very high scores). We
envision UniVal being used in combination with other sources
of information, including developer knowledge and intuition,
to effectively localize faults.

V. EMPIRICAL EVALUATION

A. Study Setup

We empirically evaluated the fault localization performance
of UniVal in a substantial empirical study involving subject
programs from the latest, expanded version (2.0.0) of the
popular Defects4J evaluation framework [20]. We compared
the fault localization costs of UniVal and several compet-
ing techniques: the non-interventional value-based techniques
Elastic Predicates (ESP) [10] and NUMFL (speciﬁcally the
two variants NUMFL-DLRM and NUMFL-QRM) [19]; the
non-interventional coverage-based technique of Baah et al.
[15], which employs linear regression for causal inference; the
interventional technique Predicate Switching [39], which alters
conditional branching; and ﬁnally two well-known coverage-
based SFL (CB-SFL) metrics that performed well in recent
comparative studies [5], [24], [26], namely, Ochiai [5] and
D-Star (with star = 2) [25].

There is one important note concerning the implementation
of Baah et al.’s original technique based on linear regression
[15]. With that technique, the only covariate in the regression
model was a coverage indicator for the forward control de-
pendence predecessor of the target statement. For this study,
we have modiﬁed Baah et al.’s technique by including the
variables used at the target statement as covariates. We believe
this is a notable improvement on the original technique. The
modiﬁed version almost always performs second best among
the studied methods.

Defects4J (Version 2.0.0) [20] is a collection of 17 programs
and 835 faulty program versions containing a wide spectrum of
real software faults. The number of subject programs and the
total number of faulty program versions have nearly doubled
in this release of Defects4J. Although the very low failure
rates of many Defects4J programs make them non-ideal for
statistical fault localization [40], its user friendly scripts and
continuous support and evolution makes it an invaluable source
for empirical studies.

Defects4J Subject Programs

Program ID
Chart
Cli
Csv
Math
Time
Lang
Closure
Mockito
Codec
JxPath
Gson
Collections
Compress
Jsoup
JacksonCore
JacksonXml
JacksonDatabind

KLOC
96
4
2
85
28
22
90
23
10
21
12
46
11
14
31
6
4

Average # of tests
220
147
153
172
2525
94
3448
671
132
272
566
180
158
344
248
143
1343

# of faulty versions
24
37
16
103
25
60
170
35
16
20
16
3
45
90
25
6
109

TABLE I: Summary of Subject Programs

the fault was not directly related to an assignment statement
or predicate (e.g. a fault of omission) we determined a set
of fault localization candidates with an approach similar to
the one described by Pearson et al. [24]. Finally, We used a
linux Ubuntu 20.04 LTS machine that runs on an Intel i5-930H
quad core CPU at 2.4-4.1 GHz and that has 8GB of RAM
for our experiments. The time measurements are calculated
by inserting a timestamp at the beginning and at the end of
each step of the pipeline script and reporting the averages for
all programs. The EXAM and Hit@N metrics involve simple
average or count calculations; hence, we used Microsoft Excel
to compute them. We ran all the program versions in our
comparison 10 times and averaged the results over all runs
in case techniques displayed random variation.

B. Results

Table II provides an summary of the results of the empirical
evaluation showing the average performance of each technique
on each program using the three evaluation methods: EXAM,
Hit@10, and Hit@5. Lower EXAM scores are better than high
ones, while higher Hit@10 and Hit@5 scores are better than
low ones. Average runtimes for the methods in our empirical
evaluation were: 55.6 seconds for UniVal, 78.6 seconds for
NUMFL (cumulative time to run both models), 660 seconds
for Predicate Switching, 44.8 seconds for Baah2010, 13.2
seconds for ESP, and 4.4 seconds for the coverage based
techniques. The average overhead of our instrumentation is
about 25% (e.g. Math-1 takes 8 seconds to execute without
the instrumentation and 10 seconds with the instrumentation).
We make two observations about UniVal’s overall perfor-
mance as shown in Table II. First, UniVal usually had the best
score for a given program/cost metric combination. Second, as
can be seen in the EXAM Score table, UniVal’s scores display
less variation than other methods. This indicates not only that
UniVal is capable of relatively precise localization (as shown
in the Hit@5 table) but also that it provides more consistent
results. In contrast, the well known Ochiai method provides
precise localization somewhat frequently, but it exhibits much
more variation.

Fig. 3: Process of Empirical Evaluation

In our study, the techniques we compare assign suspicious-
ness scores to different program elements. UniVal assigns
scores to numeric, string, and categorical assignment state-
ments and to predicates, ESP assigns scores to numeric assign-
ment statements and to predicates, and NUMFL assigns scores
to each subexpression of numeric assignment statements. Pred-
icate Switching assigns scores only to predicates. Baah et al’s
linear regression technique [15] and the other coverage-based
SFL techniques assign scores to all the statements, although
each statement within a control dependence region receives the
same score. To enable a fair comparison between techniques,
we conﬁned the comparison to only predicates and numeric
assignment statements. (Note that to reduce overhead in this
study only the faulty classes are instrumented.) With Predicate
Switching, however, we reported the cost of ﬁnding the nearest
predicate to the fault, since the technique does not assign
scores to non-predicates. As a result of these restrictions,
our study did not evaluate UniVal’s ability to handle string
variables. We intend to do so in a future study.

For a few program versions NUMFL failed to assign a
suspiciousness score to any assignment statement and the
suspiciousness list only contained zero values. We suspect this
is caused by using a binary outcome variable rather than the
absolute difference between the actual and expected output as
NUMFL is intended to do. We did not include these versions
in our comparison. We also did not include a program version
if it had fewer than 20 “relevant” test cases, which cover a
faulty class [20]. The remaining subject program versions are
summarized in Table I.

We used the EXAM score measure [41] and Hit@N measure
(sometimes referred to as Recall@N or Top-N) [42] to report
the cost of fault
localization for each technique. EXAM
score is the percentage of program statements a developer
must examine, in non-increasing order of their suspiciousness
scores, before ﬁnding the fault. If there are ties in the scoring
of statements, we assumed that half of the tied statements will
have to be examined before a programmer localizes any fault
among them. Hit@N is the number of program versions for
which a fault was found within the top N ranked statements
in non-increasing order of suspiciousness scores. Because our
comparison involves assignment statements and predicates, if

C. The Effect of Covariate Balance

In causal

inference, covariates are variables other than
the treatment variable or the outcome variable that may be
associated with either variable. They are used for confounding
adjustment or for reducing the variance of estimates. In the
absence of confounding, as in a randomized experiment, the
joint distributions of the covariates should be similar in both
(or in all) treatment groups [29], [44]. This condition is called
covariate balance. It typically does not occur in observational
studies. There are techniques, such as matching [16], [45], that
attempt to achieve covariate balance in the analysis sample
by removing certain units from the original study sample.
However, UniVal statistically adjusts for covariates rather than
modifying the study sample.

To better understand the effect of covariate balance and
imbalance on the performance of UniVal and coverage-based
fault
localization, we conducted a sub-study in which we
measured the degree of covariate imbalance in the data sets
for faulty Defects4J [46] versions and we related it to the
cost of fault localization, as measured by EXAM score [41],
for UniVal and the coverage-based SFL metric Ochiai [5].
To simplify the comparison, we considered only program
versions that contain a faulty predicate in a branch condition.
Thus, for both the UniVal and Ochiai techniques the treatment
value corresponded to the outcome of that predicate (true
or false). Additionally, we considered only predicates that
have covariates with numerical values. To measure covariate
imbalance, we calculated the mean, over all covariates, of the
difference in the mean values of individual covariates for the
two treatment groups.

We located the program versions in release 1.4 of the
Defects4J repository that contained faults in branch predicates
by consulting a recent study that details the fault types in that
release [46]. For release 2.0 of Defects4J, we searched among
the fault-ﬁx patches included with the release. We found a
total of 228 program versions ﬁtting our criteria.

The results of our study of the effect of covariate imbalance
on fault localization are depicted in Figure 5. The ﬁgure shows
a scatter plot in which the X-axis represents the mean, over all
covariates, of the difference in the mean values of individual
covariates for the two treatment groups and in which the Y-
axis represents the EXAM scores for UniVal and Ochiai for
data sets with given levels of covariate imbalance. Note that
along the X-axis, larger values represent greater imbalance,
and along the Y-axis, larger values represent higher costs for
fault localization.

The results indicate that higher fault localization costs are
associated with greater imbalance. This is consistent with
previous results indicating that covariate balance is associated
with lower estimation bias in observational studies [47]. How-
ever, it is evident in Figure 5 that even when the covariates
are imbalanced, the cost of fault localization with UniVal is
usually lower than with Ochiai. This is due to the fact that
UniVal adjusts for confounding and therefore mitigates the
effects of covariate imbalance.

TABLE II: Comparison of UniVal and competitive metrics for
all Defects4J programs

To evaluate the statistical signiﬁcance of our results, we
conducted Wilcoxon signed-rank tests [43] for the difference
in performance between UniVal and other techniques, for
each part of Table II. The resulting p-values are reported in
the bottom row of each part of the table. Each difference
in performance between UniVal and another technique is
signiﬁcant at the 0.05 level.

Figure 4 visually compares UniVal’s performance versus
the other techniques using the EXAM score. There are only
seven instances in which UniVal does worse than a competing
technique (out of 119 total). In general, this study indicates
that of all the techniques considered, UniVal has the best
performance on the the Defects4J dataset.

Fig. 4: EXAM score reductions achieved by UniVal over other techniques, for all Defects4J programs

Fig. 5: Relationship Between Covariate Imbalance and EXAM Score

D. Threats to Validity

1) Internal Validity: As previously noted in the literature
the EXAM score and Hit@N metrics are imperfect evaluation
methods for automatic fault localization [37], [38]. In partic-
ular they assume a programmer will always start at the top of
the ranked list of program elements and move monotonically
down the list ignoring the surrounding program structure. They
also assume that the programmer is equally likely to examine
program elements with the same suspiciousness score. But,
programmers use their intuition, background knowledge of the
program, and other methods of debugging when using auto-
matic fault localization tools. Hence, this evaluation method
does not fully account for programmer behavior.

The EXAM and Hit@N cost metrics handle program el-
ements with the same suspiciousness scores by giving each
such element the same (average) rank score (see Standard
Rank Score in [38]). This creates evaluation bias in favor
of fault
localization techniques that are more likely than
others to assign different elements the same suspiciousness
score (such as the coverage-based metrics Ochiai and DStar).
However, some of the techniques we studied — including the
proposed technique UniVal — don’t tend give the same scores
to different program elements.

2) External Validity: The Defects4J dataset [20] is a very
useful collection of programs, bugs, and programmer-written
tests. However, no collection can be all encompassing, and

this collection is still only a small sample of real programs
and their bugs. A technique performing well on these 17
programs might not perform particularly well on any other
program (and vice versa). Second, the test cases are provided
by the developers and are generally in the form of unit tests.
Arguably, end-to-end system tests would be a better choice for
evaluating fault localization techniques [40]. In particular the
tests fail at very low rates on the faulty program versions in the
dataset. Ideally, there would be more balance between failing
and passing tests. Finally, because the tests are generally unit
tests they do not simulate operational input by end-users the
programs.

the
In general, despite the weaknesses outlined above,
authors feel this study is a state-of-the-art empirical evaluation
of fault localization techniques. The study employs only real
bugs, real tests, and substantially sized programs. It does not
use the often criticized “injected faults” or “generated test
cases.” Nor was it conducted on toy programs constructed for
the purpose. Finally, every effort was made to put the previous
work in the best light: from improvements to Baah’s method
to ﬁltering out faults which NUMFL could not localize.

VI. RELATED WORK

Cleve and Zeller [48] proposed an interventional approach
to fault localization based on cause transitions, points in time
where a variable starts to become the cause of failure, and
they showed how delta debugging [3] can be used to ﬁnd
them. Jeffrey et al. [9] presented a value-based fault local-
ization technique called value replacement, which searches
for program statements whose execution can be intervened
upon to change incorrect program output to correct output.
Similarly, earlier work by Zhang et al. introduced predicate
switching [39], which searches predicates that are executed by
failing tests and whose outcomes can be altered to make the
program succeed. Johnson et al. [49] present an approach to
“causal testing”, in which input fuzzing is used to generate
passing and failing tests that are similar to an original failing
test. The generated tests are then used to pinpoint failures.
The aforementioned techniques each require a complete or
partial test oracle. Furthermore, they also entail possibly costly
repeated runs of subject programs and searches among them,
while UniVal does not require any oracle or repeated runs of
subject programs.

Fariha et al. [50] propose a technique called Automated
Interventional Debugging (AID) that seeks to pinpoint the root
cause of an application’s intermittent failures and to generate
an explanation of how the root cause triggers them. AID
approximates causal relationships between events in terms of
their occurrence times and intervenes to change the value of
predicates in order to prune a causal DAG and extract a causal
path from the root cause to a failure. Although AID makes use
of counterfactuals, it does not adjust for confounding bias.

Baah et al. [15] pointed out that the conventional SFL
techniques are susceptible to confounding bias, and they
employed causal inference methodology to localize faults. A
linear regression model was ﬁtted for test outcomes with a

coverage indicator for the target statement as the treatment
and a coverage indicator for its forward control dependence
predecessor as a covariate. Bai et al. presented two variants
of a value-based causal statistical fault localization technique
called NUMFL [19]. This technique makes use of generalized
propensity scores, which are used to achieve covariate balance.
Although these causal inference based techniques adjust for
confounding like UniVal, they employ parametric regression
and thus lack the modeling ﬂexbility of random forests. Elastic
predicates (ESP) [10], which were presented by Gore et
al., involve measuring how different, in standard deviations,
an assigned variable’s value is from its average value. This
difference is used as a suspiciousness score. In contrast to
UniVal, ESP does not control for confounding bias.

Feyzi et al. proposed an approach [51] to reducing the num-
ber of statements that must be considered in fault localization.
They use backward slicing techniques and information theory
to ﬁnd candidate cause-effect chains (introduced by Zeller et
al. [3]), before applying causal inference based techniques
(such as Baah2010) on these candidates. Their method is still
subjected to the limitations of the techniques it combines,
which are mentioned throughout this section.

Recent studies have investigated combinations of SFL met-
rics or sources of information for use in fault localization.
Xuan et al. [12] presented a technique that uses learning-to-
rank methods [52] to combine multiple SFL metrics. Sohn and
Yoo [13] combined SFL results with source code metrics from
static analysis. Zou et al. [14] found that combining a variety
of SFL techniques was beneﬁcial. Li et al. [11] uses deep
learning with neural networks to integrate multiple sources of
information that vary from SFL metrics to textual similarity
measures. Although these approaches to combining different
sources of information in fault localization are promising, they
do not adjust for confounding or other biases and therefore
their results are prone to bias. [53] Mutation-based SFL
techniques [54]–[56] measure the suspiciousness of a program
statement by how much a mutation to it can change the number
of program failures that occur on a test set. These techniques
often generate a very large number of mutations, hence they
may be very costly to apply. Unlike UniVal, they do not
employ established causal inference methodology.

Model-based debugging (MBD) techniques apply model-
based diagnosis to software fault localization [23], [57]–[59].
In MBD a logical model of a program is derived automatically
from its source code and a search algorithm ﬁnds minimal
sets of statements whose faultiness can explain incorrect
behavior observed on test cases. Abreu et al. [23] presented a
new model-based approach, named BARINEL, to diagnosing
multiple intermittent faults, which uses a Bayesian method
for estimating the probability that a faulty component exhibits
correct behavior. Recent studies [60]–[62] have applied model-
based debugging to spreadsheet programs, which are a special
type of numerical program. We believe UniVal might be an
efﬁcient algorithm for spreadsheet programs. None of the
aforementioned MBD techniques addresses confounding bias.

VII. CONCLUSION

This paper has presented UniVal, which is a novel approach
to statistical fault localization that is based on statistical causal
inference methodology and that integrates value-based and
predicate-based fault localization by transforming predicates
into assignment statements. It uses a machine learning model
to estimate, with minimal bias, the average causal effect of
counterfactual assignments to program variables, without ac-
tually changing the program or its executions. UniVal currently
handles the values of numeric, boolean, categorical, and string
values. We reported the results of an extensive empirical
evaluation of UniVal, in which it outperformed a variety of
competing techniques. In future work, we intend to expand
the range of program elements and data types to which UniVal
applies.

DATA AVAILABILITY

Our implementation for the prototype tools and experiments

presented in this paper is publicly available [63].

ACKNOWLEDGEMENT

This work was partially supported by NSF award CCF-
1525178 to Case Western Reserve University. The authors
would also like to thank Zhoufu Bai for providing scripts we
used for including NUMFL into our evaluation.

REFERENCES

[1] J. A. Jones, M. J. Harrold, and J. Stasko, “Visualization of test informa-
tion to assist fault localization,” in Proceedings of the 24rd International
Conference on Software Engineering, 2002. ICSE 2002.
IEEE, 2002,
pp. 467–477.

[2] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan, “Scalable
statistical bug isolation,” Acm Sigplan Notices, vol. 40, no. 6, pp. 15–26,
2005.

[3] A. Zeller and R. Hildebrandt, “Simplifying and isolating failure-inducing
input,” IEEE Transactions on Software Engineering, vol. 28, no. 2, pp.
183–200, 2002.

[4] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, “A survey on
software fault localization,” IEEE Transactions on Software Engineering,
vol. 42, no. 8, pp. 707–740, 2016.

[5] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. Van Gemund, “A practical
evaluation of spectrum-based fault localization,” Journal of Systems and
Software, vol. 82, no. 11, pp. 1780–1792, 2009.

[6] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semﬁx:
Program repair via semantic analysis,” in 2013 35th International
Conference on Software Engineering (ICSE).
IEEE, 2013, pp. 772–
781.

[7] F. Y. Assiri and J. M. Bieman, “Fault localization for automated program
repair: effectiveness, performance, repair correctness,” Software Quality
Journal, vol. 25, no. 1, pp. 171–199, 2017.

[8] T. Xie and D. Notkin, “Checking inside the black box: Regression
testing by comparing value spectra,” IEEE Transactions on software
Engineering, vol. 31, no. 10, pp. 869–883, 2005.

[9] D. Jeffrey, N. Gupta, and R. Gupta, “Fault localization using value
replacement,” in Proceedings of the 2008 international symposium on
software testing and analysis. ACM, 2008, pp. 167–178.

[10] R. Gore, P. F. Reynolds, and D. Kamensky, “Statistical debugging
with elastic predicates,” in Proceedings of the 2011 26th IEEE/ACM
International Conference on Automated Software Engineering.
IEEE
Computer Society, 2011, pp. 492–495.

[11] X. Li, W. Li, Y. Zhang, and L. Zhang, “Deepﬂ: integrating multiple fault
diagnosis dimensions for deep fault localization,” in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing and
Analysis. ACM, 2019, pp. 169–180.

[12] J. Xuan and M. Monperrus, “Learning to combine multiple ranking
metrics for fault localization,” in 2014 IEEE International Conference
on Software Maintenance and Evolution.

IEEE, 2014, pp. 191–200.

[13] J. Sohn and S. Yoo, “Fluccs: Using code and change metrics to
improve fault localization,” in Proceedings of the 26th ACM SIGSOFT
International Symposium on Software Testing and Analysis.
ACM,
2017, pp. 273–283.

[14] D. Zou, J. Liang, Y. Xiong, M. D. Ernst, and L. Zhang, “An empirical
localization families and their combinations,” IEEE

study of fault
Transactions on Software Engineering, 2019.

[15] G. K. Baah, A. Podgurski, and M. J. Harrold, “Causal inference for
statistical fault localization,” in Proceedings of the 19th international
symposium on Software testing and analysis. ACM, 2010, pp. 73–84.
[16] ——, “Mitigating the confounding effects of program dependences for
effective fault localization,” in Proceedings of the 19th ACM SIGSOFT
symposium and the 13th European conference on foundations of software
engineering. ACM, 2011, pp. 146–156.

[17] R. Gore and P. F. Reynolds, “Reducing confounding bias in predicate-
level statistical debugging metrics,” in 34th International Conference on
Software Engineering (ICSE), 2012.

IEEE, 2012, pp. 463–473.

[18] G. Shu, B. Sun, A. Podgurski, and F. Cao, “Mﬂ: Method-level fault
localization with causal inference,” in 2013 IEEE Sixth International
Conference on Software Testing, Veriﬁcation and Validation.
IEEE,
2013, pp. 124–133.

[19] Z. Bai, G. Shu, and A. Podgurski, “Causal inference based fault localiza-
tion for numerical software with numﬂ,” Software Testing, Veriﬁcation
and Reliability, vol. 27, no. 6, p. e1613, 2017.

[20] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: A database of existing
faults to enable controlled testing studies for java programs,” in Pro-
ceedings of the 2014 International Symposium on Software Testing and
Analysis. ACM, 2014, pp. 437–440.

[21] J. Pearl, Causality. Cambridge University Press, 2009.
[22] K. J. Ottenstein, R. A. Ballance, and A. B. MacCabe, “The program
dependence web: a representation supporting control-, data-, and
demand-driven interpretation of imperative languages,” in Proceedings
the ACM SIGPLAN 1990 conference on Programming language
of
design and implementation - PLDI ’90, vol. 20-22-June. New York,
New York, USA: ACM Press, 1990, pp. 257–271. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=93542.93578

[23] R. Abreu, P. Zoeteweij, and A. J. Van Gemund, “A new bayesian
approach to multiple intermittent fault diagnosis,” in Twenty-First In-
ternational Joint Conference on Artiﬁcial Intelligence, 2009.

[24] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst,
D. Pang, and B. Keller, “Evaluating and improving fault localization,”
in Proceedings of the 39th International Conference on Software Engi-
neering.

IEEE Press, 2017, pp. 609–620.

[25] W. E. Wong, V. Debroy, R. Gao, and Y. Li, “The dstar method for
effective software fault localization,” IEEE Transactions on Reliability,
vol. 63, no. 1, pp. 290–308, 2014.

[26] L. Lucia, D. Lo, L. Jiang, F. Thung, and A. Budi, “Extended compre-
hensive study of association measures for fault localization,” Journal of
software: Evolution and Process, vol. 26, no. 2, pp. 172–219, 2014.

[27] G. W. Imbens and D. B. Rubin, Causal inference in statistics, social,

and biomedical sciences. Cambridge University Press, 2015.

[28] P. W. Holland, “Statistics and causal inference,” Journal of the American

statistical Association, vol. 81, no. 396, pp. 945–960, 1986.

[29] M. Hern´an and J. Robins, Causal Inference.

Chapman Hall/CRC,

forthcoming, 2018.

[30] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck,
“Efﬁciently computing static single assignment form and the control
dependence graph,” ACM Transactions on Programming Languages
and Systems, vol. 13, no. 4, pp. 451–490, oct 1991. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=115372.115320

[31] J. Ferrante, K. J. Ottenstein, and J. D. Warren, “The program dependence
graph and its use in optimization,” ACM Transactions on Programming
Languages and Systems (TOPLAS), vol. 9, no. 3, pp. 319–349, 1987.

[32] A. Podgurski and Y. K¨uc¸ ¨uk, “Counterfault: Value-based fault local-
ization by modeling and predicting counterfactual outcomes,” in 2020
IEEE International Conference on Software Maintenance and Evolution
(ICSME).

IEEE, 2020, pp. 382–393.

[33] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.

5–32, 2001.

[58] W. Mayer, R. Abreu, M. Stumptner, A. J. Van Gemund et al., “Pri-
oritising model-based debugging diagnostic reports,” in Proceedings of
the 19th International Workshop on Principles of Diagnosis. Citeseer,
2008, pp. 127–134.

[59] F. Wotawa, M. Nica, and I. Moraru, “Automated debugging based on a
constraint model of the program and a test case,” The journal of logic
and algebraic programming, vol. 81, no. 4, pp. 390–407, 2012.
[60] D. Jannach, T. Schmitz, B. Hofer, K. Schekotihin, P. Koch, and
F. Wotawa, “Fragment-based spreadsheet debugging,” Automated soft-
ware engineering, vol. 26, no. 1, pp. 203–239, 2019.

[61] R. Abreu, B. Hofer, A. Perez, and F. Wotawa, “Using constraints to
diagnose faulty spreadsheets,” Software Quality Journal, vol. 23, no. 2,
pp. 297–322, 2015.

[62] R. Abreu, A. Riboira, and F. Wotawa, “Debugging spreadsheets: A
csp-based approach,” in 2012 IEEE 23rd International Symposium on
Software Reliability Engineering Workshops.
IEEE, 2012, pp. 159–164.
[63] Y. Kucuk, T. Henderson, and A. Podgurski, “Improving Fault
Localization by Integrating Value and Predicate Based Causal Inference
Techniques,” Jan. 2021. [Online]. Available: https://doi.org/10.5281/
zenodo.4441439

[34] M. N. Wright and A. Ziegler, “Ranger: a fast

implementation of
random forests for high dimensional data in c++ and r,” arXiv preprint
arXiv:1508.04409, 2015.

[35] M. van der Loo, J. van der Laan, R. C. Team, N. Logan, and C. Muir,

“Package ‘stringdist’,” CRAN, June, vol. 6, 2019.

[36] M. Hahsler, M. Piekenbrock, and D. Doran, “dbscan: Fast density-based
clustering with r,” Journal of Statistical Software, vol. 91, no. 1, pp. 1–
30, 2019.

[37] C. Parnin and A. Orso, “Are automated debugging techniques actually
helping programmers?” in Proceedings of the 2011 international sym-
posium on software testing and analysis. ACM, 2011, pp. 199–209.

[38] T. A. Henderson, A. Podgurski, and Y. Kucuk, “Evaluating automatic
localization using markov processes,” in 2019 19th Interna-
fault
tional Working Conference on Source Code Analysis and Manipulation
(SCAM).

IEEE, 2019, pp. 115–126.

[39] X. Zhang, N. Gupta, and R. Gupta, “Locating faults through automated
predicate switching,” in Proceedings of the 28th international conference
on Software engineering, 2006, pp. 272–281.

[40] Y. K¨uc¸ ¨uk, T. A. Henderson, and A. Podgurski, “The impact of rare
failures on statistical fault localization: the case of the defects4j suite,”
in 2019 IEEE International Conference on Software Maintenance and
Evolution (ICSME).

IEEE, pp. 24–28.

[41] E. Wong, T. Wei, Y. Qi, and L. Zhao, “A crosstab-based statistical
method for effective fault localization,” in Software Testing, Veriﬁcation,
and Validation, 2008 1st International Conference on.
IEEE, 2008, pp.
42–51.

[42] S. Wang and D. Lo, “Version history, similar report, and structure:
Putting them together for improved bug localization,” in Proceedings of
the 22nd International Conference on Program Comprehension. ACM,
2014, pp. 53–63.

[43] F. Wilcoxon, “Individual comparisons by ranking methods,” in Break-

throughs in statistics. Springer, 1992, pp. 196–202.

[44] B. B. Hansen and J. Bowers, “Covariate balance in simple, stratiﬁed and
clustered comparative studies,” Statistical Science, pp. 219–236, 2008.
[45] P. C. Austin, “Balance diagnostics for comparing the distribution of base-
line covariates between treatment groups in propensity-score matched
samples,” Statistics in medicine, vol. 28, no. 25, pp. 3083–3107, 2009.
[46] V. Sobreira, T. Durieux, F. Madeiral, M. Monperrus, and M. A. Maia,
“Dissection of a bug dataset: Anatomy of 395 patches from defects4j,”
in Proceedings of SANER, 2018.

[47] J. J. Sauppe and S. H. Jacobson, “The role of covariate balance in
observational studies,” Naval Research Logistics (NRL), vol. 64, no. 4,
pp. 323–344, 2017.

[48] H. Cleve and A. Zeller, “Locating causes of program failures,” in Pro-
ceedings of the 27th international conference on Software engineering.
ACM, 2005, pp. 342–351.

[49] B. Johnson, Y. Brun, and A. Meliou, “Causal testing: Understanding de-
fects’ root causes,” in Proceedings of the 2020 International Conference
on Software Engineering, 2020.

[50] A. Fariha, S. Nath, and A. Meliou, “Causality-guided adaptive in-
terventional debugging,” in Proceedings of the 2020 ACM SIGMOD
International Conference on Management of Data, 2020, pp. 431–446.
[51] F. Feyzi and S. Parsa, “Inforence: effective fault localization based on
information-theoretic analysis and statistical causal inference,” Frontiers
of Computer Science, vol. 13, no. 4, pp. 735–759, 2019.

[52] T.-Y. Liu et al., “Learning to rank for information retrieval,” Foundations
and Trends® in Information Retrieval, vol. 3, no. 3, pp. 225–331, 2009.
[53] C. Liu, C. Gao, X. Xia, D. Lo, J. Grundy, and X. Yang, “On the repli-
cability and reproducibility of deep learning in software engineering,”
arXiv preprint arXiv:2006.14244, 2020.

[54] M. Papadakis and Y. Le Traon, “Metallaxis-ﬂ: mutation-based fault
localization,” Software Testing, Veriﬁcation and Reliability, vol. 25, no.
5-7, pp. 605–628, 2015.

[55] L. Zhang, L. Zhang, and S. Khurshid, “Injecting mechanical faults
to localize developer faults for evolving software,” in ACM SIGPLAN
Notices, vol. 48, no. 10. ACM, 2013, pp. 765–784.

[56] S. Moon, Y. Kim, M. Kim, and S. Yoo, “Ask the mutants: Mutating
faulty programs for fault localization,” in 2014 IEEE Seventh Inter-
national Conference on Software Testing, Veriﬁcation and Validation.
IEEE, 2014, pp. 153–162.

[57] W. Mayer and M. Stumptner, “Evaluating models for model-based
debugging,” in Proceedings of the 2008 23rd IEEE/ACM International
Conference on Automated Software Engineering.
IEEE Computer
Society, 2008, pp. 128–137.

