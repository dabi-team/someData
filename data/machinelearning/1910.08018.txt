0
2
0
2

b
e
F
2

]
L
M

.
t
a
t
s
[

2
v
8
1
0
8
0
.
0
1
9
1
:
v
i
X
r
a

A Uniﬁed Framework for Tuning Hyperparameters in Clustering
Problems

Xinjie Fan1, Yuguang Yue1, Purnamrita Sarkar1, and Y. X. Rachel Wang2

1Department of Statistics and Data Science, University of Texas at Austin
2School of Mathematics and Statistics, University of Sydney
xfan@utexas.edu, yuguang@utexas.edu, purna.sarkar@austin.utexas.edu, rachel.wang@sydney.edu.au

February 4, 2020

Abstract

Selecting hyperparameters for unsupervised learning problems is challenging in general due to the lack
of ground truth for validation. Despite the prevalence of this issue in statistics and machine learning,
especially in clustering problems, there are not many methods for tuning these hyperparameters with
theoretical guarantees. In this paper, we provide a framework with provable guarantees for selecting
hyperparameters in a number of distinct models. We consider both the subgaussian mixture model and
network models to serve as examples of i.i.d. and non-i.i.d. data. We demonstrate that the same framework
can be used to choose the Lagrange multipliers of penalty terms in semideﬁnite programming (SDP)
relaxations for community detection, and the bandwidth parameter for constructing kernel similarity
matrices for spectral clustering. By incorporating a cross-validation procedure, we show the framework
can also do consistent model selection for network models. Using a variety of simulated and real data
examples, we show that our framework outperforms other widely used tuning procedures in a broad range
of parameter settings.

1

Introduction

A standard statistical model has parameters, which characterize the underlying data distribution; an inference
algorithm to learn these parameters typically involve hyperparameters (or tuning parameters). Popular
examples include the penalty parameter in regularized regression models, the number of clusters in clustering
analysis, the bandwidth parameter in kernel based clustering, nonparameteric density estimation or regression
methods (Wasserman (2006); Tibshirani et al. (2015)), to name but a few. It is well-known that selecting
these hyperparameters may require repeated training to search through diﬀerent combinations of plausible
hyperparameter values and often has to rely on good heuristics and domain knowledge from the user.

A classical method to do automated hyperparameter tuning is the nonparametric procedure Cross
Validation (CV) (Stone (1974); Zhang (1993)) which has been used extensively in machine learning and
statistics (Hastie et al. (2005)).CV has been studied extensively in supervised learning settings, particularly
in low dimensional linear models (Shao (1993); Yang et al.
(2007)) and penalized regression in high
dimension (Wasserman & Roeder (2009)). Other notable stability based methods for model selection in
similar supervised settings include Breiman et al. (1996); Bach (2008); Meinshausen & Bühlmann (2010);
Lim & Yu (2016). Finally, a large number of empirical methods exist in the machine learning literature for
tuning hyperparameters in various training algorithms (Bergstra & Bengio (2012); Bengio (2000); Snoek et al.
(2012); Bergstra et al. (2011)), most of which do not provide theoretical guarantees.

In contrast to the supervised setting with i.i.d. data used in many of the above methods, in this paper,
we consider unsupervised clustering problems with possible dependence structure in the datapoints. We
propose an overarching framework for hyperparameter tuning and model selection for a variety of probabilistic
clustering models. Here the challenge is two-fold. Since labels are not available, choosing a criterion for
evaluation and in general a method for selecting hyperparameters is not easy. One may consider splitting the
data in diﬀerent folds and selecting the model or hyperparameter with the most stable solution. However, for

1

 
 
 
 
 
 
multiple splits of the data, the inference algorithm may get stuck at the same local optima, and thus stability
alone can lead to a suboptimal solution (Von Luxburg et al. (2010)). In Wang (2010); Fang & Wang (2012),
the authors overcome this by redeﬁning the number of clusters as one that gives the most stable clustering for
a given algorithm. In Meila (2018), a semi-deﬁnite program (SDP) maximizing an inner product criterion is
performed for each clustering solution, and the value of the objective function is used to evaluate the stability
of the clustering. The analysis is done without any model assumptions. The second diﬃculty arises if there
is dependence structure in the datapoints, which necessitates careful splitting procedures in a CV-based
procedure.

To illustrate the generality of our framework, we focus on subgaussian mixtures and the statistical network
models like the Stochastic Blockmodel (SBM) and the Mixed Membership Stochastic Blockmodel (MMSB)
as two representative models for i.i.d. data and non i.i.d. data, where clustering is a natural problem. We
propose a uniﬁed framework with provable guarantees to do hyperparameter tuning and model selection in
these models. More speciﬁcally, our contributions can be summarized as below:
1. Our framework can provably tune the following hyperparameters:

(a) Lagrange multiplier of the penalty term in a type of semideﬁnite relaxation for community detection

problems in SBM;

(b) Bandwidth parameter used in kernel spectral clustering for subgaussian mixture models.

2. We have consistent model selection, i.e. determining number of clusters:

(a) When the model selection problem is embedded in the choice of the Lagrange multiplier in another type

of SDP relaxation for community detection in SBM;

(b) General model selection for the Mixed Membership Stochastic Blockmodel (MMSB), which includes the

SBM as a sub-model.

We choose to focus on model selection for network-structured data, because there already is an extensive
repertoire of empirical and provable methods including the gap statistic (Tibshirani et al. , 2001), silhouette
index (Rousseeuw, 1987), the slope criterion (Birgé & Massart, 2001), eigen-gap Von Luxburg (2007), penalized
maximum likelihood (Leroux, 1992), information theoretic approaches (AIC (Bozdogan, 1987), BIC (Keribin,
2000; Drton & Plummer, 2017), minimum message length (Figueiredo & Jain, 2002)), spectral clustering and
diﬀusion based methods (Maggioni & Murphy, 2018; Little et al. , 2017) for i.i.d mixture models. We discuss
the related work on the other models in the following subsection.

1.1 Related Work

Hyperparameters and model selection in network models: In network analysis, while a number of
methods exist for selecting the true number of communities (denoted by r) with consistency guarantees
including Lei et al. (2016); Wang & Bickel (2017); Le & Levina (2015); Bickel & Sarkar (2016) for SBM,
and Fan et al. (2019) and Han et al. (2019) for more general models such as the degree-corrected mixed
membership blockmodel, these methods have not been generalized to other hyperparameter selection problems.
For CV-based methods, existing strategies involve node splitting (Chen & Lei (2018)), or edge splitting (Li
et al. (2016)). In the former, it is established that CV prevents underﬁtting for model selection in SBM.
In the latter, a similar one-sided consistency result for Random Dot Product Models (RDPG) (Young &
Scheinerman (2007), which includes SBM as a special case) is shown. This method has also been empirically
applied to tune other hyperparameters, though no provable guarantee was provided.

In terms of algorithms for community detection or clustering, SDP methods have gained a lot of attention
(Abbe et al. (2015); Amini et al. (2018); Guédon & Vershynin (2016); Cai et al. (2015); Hajek et al.
(2016)) due to their strong theoretical guarantees. Typically, SDP based methods can be divided into two
broad categories. The ﬁrst one maximizes a penalized trace of the product of the adjacency matrix and an
unnormalized clustering matrix (see deﬁnition in Section 2.2). Here the hyperparameter is the Lagrange
multiplier of the penalty term Amini et al. (2018); Cai et al. (2015); Chen & Lei (2018); Guédon & Vershynin
(2016). In this formulation, the optimization problem does not need to know the number of clusters. However,
it is implicitly required in the ﬁnal step which obtains the memberships from the clustering matrix.

2

The other class of SDP methods uses a trace criterion with a normalized clustering matrix (deﬁnition in
Section 2.2) (Peng & Wei, 2007; Yan & Sarkar, 2019; Mixon et al. , 2017). Here the constraints directly use the
number of clusters. (Yan et al. , 2017) use a penalized alternative of this SDP to do provable model selection
for SBMs. However, most of these methods require appropriate tuning of the Lagrange multipliers, which
are themselves hyperparameters. Usually the theoretical upper and lower bounds on these hyperparameters
involve unknown model parameters, which are nontrivial to estimate. The proposed method in Abbe &
Sandon (2015) is agnostic of model parameters, but it involves a highly-tuned and hard to implement spectral
clustering step (also noted by Perry & Wein (2017)).

In this paper, we use a SDP from the ﬁrst class (SDP-1) to demonstrate our provable tuning procedure,
and another SDP from the second class (SDP-2) to establish consistency guarantee for our model selection
method.

Spectral clustering with mixture model: In statistical machine learning literature, analysis of
spectral clustering typically is done in terms of the Laplacian matrix built from an appropriately constructed
similarity matrix of the datapoints. There has been much work (Hein et al. , 2005; Hein, 2006; von Luxburg,
2007; Belkin & Niyogi, 2003; Giné & Koltchinskii, 2006) on establishing diﬀerent forms of asymptotic
convergence of the Laplacian. Recently Löﬄer et al.
(2019) have established error bounds for spectral
clustering that uses the gram matrix as the similarity matrix. In Srivastava et al. (2019) error bounds are
obtained for a variant of spectral clustering for the Gaussian kernel in presence of outliers. Most of the
existing tuning procedures for the bandwidth parameter of the Gaussian kernel are heuristic and do not have
provable guarantees. Notable methods include von Luxburg (2007), who choose an analogous parameter,
namely the radius (cid:15) in an (cid:15)-neighborhood graph “as the length of the longest edge in a minimal spanning tree
of the fully connected graph on the data points.” Other discussions on selecting the bandwidth can be found
in (Hein et al. , 2005; Coifman et al. , 2008) and (Schiebinger et al. , 2015). Shi et al. (2008) propose a
data dependent way to set the bandwidth parameter by suitably normalizing the 95% quantile of a vector
containing 5% quantiles of distances from each point.

We now present our problem setup in Section 2. Section 3 proposes and analyzes our hyperparameter
tuning method MATR for networks and subgaussian mixtures. Next, in Section 4, we present MATR-CV
and the related consistency guarantees for model selection for SBM and MMSB models. Finally, Section 5
contains detailed simulated and real data experiments and we conclude with paper with a discussion in
Section 6.

2 Preliminaries and Notations
2.1 Notations
Let (C1, ..., Cr) denote a partition of n data points into r clusters; mi = |Ci| denote the size of Ci. Denote
πmin = mini mi/n. The cluster membership of each node is represented by a n × r matrix Z, with Zij = 1 if
data point i belongs to cluster j, and 0 otherwise. Since r is the true number of clusters, Z T Z is full rank.
Given Z, the corresponding unnormalized clustering matrix is ZZ T , and the normalized clustering matrix is
Z(Z T Z)−1Z T . X can be either a normalized or unnormalized clustering matrix, and will be made clear. We
use ˜X to denote the matrix returned by SDP algorithms, which may not be a clustering matrix. Denote
Xr as the set of all possible normalized clustering matrices with cluster number r. Let Z0 and X0 be the
membership and normalized clustering matrix from the ground truth. λ is a general hyperparameter; although
with a slight abuse of notation, we also use λ to denote the Lagrange multiplier in SDP methods. For any
matrix X ∈ Rn×n, let XCk,C(cid:96) be a matrix such that XCk,C(cid:96)(i, j) = X(i, j) if i ∈ Ck, j ∈ C(cid:96), and 0 otherwise.
En is the n × n all ones matrix. We write (cid:104)A, B(cid:105) = trace(AT B). Standard notations of o, O, oP , OP , Θ, Ω
will be used. By “with high probability”, we mean with probability tending to one.

2.2 Problem setup and motivation

We consider a general clustering setting where the data D gives rise to a n × n observed similarity matrix
ˆS, where ˆS is symmetric. Denote A as a clustering algorithm which operates on the data D with a
hyperparameter λ and outputs a clustering result in the form of ˆZ or ˆX. Here note that A may or may not
perform clustering on ˆS, and A , ˆZ and ˆX could all depend on λ. In this paper we assume that ˆS has the
form ˆS = S + R, where R is a matrix of arbitrary noise, and S is the “population similarity matrix”. As we

3

consider diﬀerent clustering models for network-structured data and iid mixture data, it will be made clear
what ˆS and S are in each context.

Assortativity (weak and strong): In some cases, we require weak assortativity on the similarity
matrix S deﬁned as follows. Suppose for i, j ∈ Ck, Sij = akk. Deﬁne the minimal diﬀerence between diagonal
term and oﬀ-diagonal terms in the same row cluster as



pgap = min

k

akk − max
i∈Ck,j∈C(cid:96)
(cid:96)(cid:54)=k



Sij

 .

(1)

Weak assortativity requires pgap > 0. This condition is similar to weak assortativity deﬁned for blockmodels
(e.g. Amini et al. (2018)). It is mild compared to strong assortativity requiring mink akk − maxi∈Ck,j∈C(cid:96)
Sij >
0.

(cid:96)(cid:54)=k

Stochastic Blockmodel (SBM): The SBM is a generative model of networks with community structure
on n nodes. By ﬁrst partitioning the nodes into r classes which leads to a membership matrix Z, the n × n
binary adjacency matrix A is sampled from probability matrix P = ZiBZ T
j 1(i (cid:54)= j). where Zi and Zj are the
ith and jth row of matrix Z, B is the r × r block probability matrix. The aim is to estimate node memberships
given A. We assume the elements of B have order Θ(ρ) with ρ → 0 at some rate.

Mixed Membership Stochastic Blockmodel (MMSB): The SBM can be restrictive when it comes
to modeling real world networks. As a result, various extensions have been proposed. The mixed membership
stochastic blockmodel (MMSB, (Airoldi et al. , 2008)) relaxes the requirement on the membership vector Zi
being binary and allows the entries to be in [0, 1]r, such that they sum up to 1 for all i. We will denote this
soft membership matrix by Θ.

Under the MMSB model, the n × n adjacency matrix A is sampled from the probability matrix P with
Pij = ΘiBΘT
j 1(i (cid:54)= j). We use an analogous deﬁnition for normalized clustering matrix: X = Θ(ΘT Θ)−1Θ.
Note that this reduces to the usual normalized clustering matrix when Θ is a binary cluster membership
matrix.

Mixture of sub-gaussian random variables: Let Y = [Y1, . . . , Yn]T be a n × d data matrix. We

consider a setting where Yi are generated from a mixture model with r clusters,

Yi = µa + Wi, E(Wi) = 0, Cov(Wi) = σ2

aI,

a = 1, . . . , r,

(2)

where Wi’s are independent sub-gaussian vectors.

Trace criterion: Our framework is centered around the trace (cid:104) ˆS, Xλ(cid:105), where Xλ is the normalized
clustering matrix associated with hyperparameter λ. This criterion is often used in relaxations of the k-means
objective (Mixon et al. , 2017; Peng & Wei, 2007; Yan et al. , 2017) in the context of SDP methods. The
idea is that the criterion is large when datapoints within the same cluster are more similar. This criterion
is also used by Meila (2018) for evaluating stability of a clustering solution, where the author uses SDP to
maximize this criterion for each clustering solution. Of course, this makes the implicit assumption that ˆS
(and S) is assortative, i.e. datapoints within the same cluster have high similarity based on ˆS. While this is
reasonable for iid mixture models, not all community structures in network models are assortative if we use
the adjacency matrix A as ˆS. If all the communities in a network are dis-assortative, then one can just use
−A as ˆS. However, for the SBM or MMSB models, one may have a mixture of assortative and dis-assortative
structure. In what follows, we begin our discussion of hyperparameter tuning and model selection for SBM
by assuming weak assortativity, both for ease of demonstration and the fact that our algorithms of interest,
SDP methods, operate on weakly assortative networks. For MMSB, which includes SBM as a sub-model, we
show the same criterion still works without assortativity if we choose ˆS to be A2 with the diagonal removed.

3 Hyperparameter tuning with known r

In this section, we consider tuning hyperparameters when the true number of clusters r is known. First, we
provide two simulation studies to motivate this section. The detailed parameter settings for generating the
data can be found in the Appendix Section C.

4

As mentioned in Section 1.1, SDP is an important class of methods for community detection in SBM, but
its performance can depend on the choice of the Lagrange multiplier parameter. We ﬁrst consider a SDP
formulation (Li et al. , 2018), which has been widely used with slight variations in the literature (Amini et al.
, 2018; Perry & Wein, 2017; Guédon & Vershynin, 2016; Cai et al. , 2015; Chen & Lei, 2018),

max trace(AX) − λtrace(XEn)
s.t. X (cid:23) 0, X ≥ 0, Xii = 1 for 1 ≤ i ≤ n,

(SDP-1)

where λ is a hyperparameter. Typically, one then performs spectral clustering (that is, k-means on the top r
eigenvectors) on the output of the SDP to get the clustering result. In Figure 1 (a), we generate an adjacency
matrix from the probability matrix described in Appendix Section C and use SDP-1 with tuning parameter λ
from 0 to 1. The accuracy of the clustering result is measured by the normalized mutual information (NMI)
and shown in Figure 1 (a). We can see that diﬀerent λ values lead to widely varying clustering performance.
As a second example, we consider a four-component Gaussian mixture model generated as described
in Appendix Section C. We perform spectral clustering (k-means on the top r eigenvectors) on the widely
used Gaussian kernel matrix (denoted K) with bandwidth parameter θ. Figure 1(b) shows the clustering
performance using NMI as θ varies, and the ﬂat region of suboptimal θ corresponds to cases when the two
adjacent clusters cannot be separated well.

(a) NMI v.s. λ

(b) NMI v.s. θ

Figure 1: Tuning parameters in SDP and Spectral clustering; accuracy measured by normalized mutual
information (NMI).

We show that in the case where the true cluster number r is known, an ideal hyperparameter λ can be
chosen by simply maximizing the trace criterion introduced in Section 2.2. The tuning algorithm (MATR)
is presented in Algorithm 1. It takes a general clustering algorithm A , data D and similarity matrix ˆS as
inputs, and outputs a clustering result ˆZλ∗ with λ∗ chosen by maximizing the trace criterion.

Algorithm 1: MAx-TRace (MATR) based tuning algorithm for known number of clusters.

Input: clustering algorithm A , data D, similarity matrix ˆS, a set of candidates {λ1, · · · , λT }, number
of clusters r;
Procedure:
for t = 1 : T do

run clustering on D: ˆZt = A (D, λt, r);
compute normalized clustering matrix: ˆXt = ˆZt( ˆZ T
t
compute inner product: lt = (cid:104) ˆS, ˆXt(cid:105);

ˆZt)−1 ˆZ T
t ;

end for
t∗ = argmax(l1, ..., lT );
Output: ˆZt∗

We have the following theoretical guarantee for Algorithm 1.

5

Theorem 1. Consider a clustering algorithm A with inputs D, λ, r and output ˆZλ. The similarity matrix ˆS
used for Algorithm 1(MATR) can be written as ˆS = S + R. We further assume S is weakly assortative with
pgap deﬁned in Eq (1), and X0 is the normalized clustering matrix for the true binary membership matrix Z0.
Let πmin be the smallest cluster proportion, and τ := nπminpgap. As long as there exists λ0 ∈ {λ1, . . . , λT },
such that (cid:104) ˆXλ0, ˆS(cid:105) ≥ (cid:104)X0, S(cid:105) − (cid:15), Algorithm 1 will output a ˆZλ∗ , such that

(cid:13)
ˆXλ∗ − X0
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

≤

2
τ

((cid:15) + sup
X∈Xr

|(cid:104)X, R(cid:105)|),

where ˆXλ∗ is the normalized clustering matrix associated with ˆZλ∗ .

In other words, as long as the range of λ we consider covers some optimal λ value that leads to a suﬃciently
large trace criterion (compared with the true underlying X0 and the population similarity matrix S), the
theorem guarantees Algorithm 1 will lead to a normalized clustering matrix with small error. The deviation (cid:15)
depends both on the noise matrix R and how close the estimated ˆXλ0 is to the ground truth X0, i.e. the
performance of the algorithm. If both (cid:15) and supX∈Xr |(cid:104)X, R(cid:105)| are oP (τ ), then MATR will yield a clustering
matrix which is weakly consistent. The proof is in the Appendix Section A.

In the following subsections, we apply MATR to more speciﬁc settings, namely to select the Lagrange
multiplier parameter in SDP-1 for SBM and the bandwidth parameter in spectral clustering for sub-gaussian
mixtures.

3.1 Hyperparameter tuning for SBM

We consider the problem of choosing λ in SDP-1 for community detection in SBM. Here, the input to
Algorithm 1 – the data D and similarity matrix ˆS – are both the adjacency matrix A. A natural choice
of a weakly assortative S is the conditional expectation of A, i.e. P up to diagonal entries: let ˜Pij = Pij
for i (cid:54)= j and ˜Pii = Bkk for i ∈ Ck. Note that ˜P is blockwise constant, and assortativity condition on ˜P
translates naturally to the usual assortativity condition on B. As the output matrix ˜X from SDP-1 may not
necessarily be a clustering matrix, we use spectral clustering on ˜X to get the membership matrix ˆZ required
in Algorithm 1. SDP-1 together with spectral clustering is used as A .

In Proposition 13 of the Appendix, we show that SDP-1 is strongly consistent, when applied to a general

strongly assortative SBM with known r, as long as λ satisﬁes:
Bk,l + Ω((cid:112)ρ log n/nπmin) ≤ λ ≤ min

max
k(cid:54)=l

Bkk + O((cid:112)ρ log n/nπ2

max)

k

(3)

An empirical way of choosing λ was provided in Cai et al. (2015), which we will compare with in Section
5. We ﬁrst show a result complementary to Eq 3 under a SBM model with weakly assortative B, that for a
speciﬁc region of λ, the normalized clustering matrix from SDP-1 will merge two clusters with high probability.
This highlights the importance of selecting an appropriate λ since diﬀerent values can lead to drastically
diﬀerent clustering result. The detailed statement and proof can be found in Proposition 12 of the Appendix
Section A.2.

When we use Algorithm 1 to tune λ for A , we have the following theoretical guarantee.

Corollary 2. Consider A ∼ SBM (B, Z0) with weakly assortative B and r number of communities. Denote
τ := nπmin mink(Bkk − max(cid:96)(cid:54)=k Bk(cid:96)). If we have (cid:15) = oP (τ ), r
nρ = o(τ ), nρ ≥ c log n, for some constant
c > 0, then as long as there exists λ0 ∈ {λ1, . . . , λT }, such that (cid:104) ˆXλ0 , A(cid:105) ≥ (cid:104)X0, P (cid:105) − (cid:15) , with A Algorithm
1(MATR) will output a ˆZλ∗ , such that (cid:107) ˆXλ∗ − X0(cid:107)2
F = oP (1), where ˆXλ∗ , X0 are the normalized clustering
matrices for ˆZλ∗ , Z0 respectively.

√

Remark 3.

1. Since λ ∈ [0, 1], to ensure the range of λ considered overlaps with the optimal range in (3),
it suﬃces to consider λ choices from [0, 1]. Then for λ satisfying Eq 3, SDP-1 produces ˜X = X0
nρ), and the
w.h.p. if B is strongly assortative. Since (cid:104)X0, R(cid:105) = OP (r
conditions in this corollary imply
→ 0. Suppose all the communities are of comparable sizes,
i.e. πmin = Θ(1/r), then the conditions only require r = O(

nρ), we can take (cid:15) = O(r

n) since nρ → ∞.

nρπmin

r√

√

√

√

6

2. Since the proofs of Theorem 1 and Corollary 2 are general, the conclusion is not limited to SDP-1 and
applies to more general community detection algorithms for SBM when r is known. It is easy to see that
a suﬃcient condition for the consistency of ˆXλ∗ to hold is that there exists λ0 in the range considered,
such that |(cid:104) ˆXλ0 − X0, P (cid:105)| = oP (τ ).

3. We note that the speciﬁc application of Corollary 2 to SDP-1 leads to weak consistency of ˆXλ∗ instead
of strong consistency as originally proved for SDP-1. This is partly due to the generality of theorem
(including the relaxation of strong assortativity on B to weak assortativity) as discussed above, and the
fact that we are estimating λ.

3.2 Hyperparameter tuning for mixtures of subgaussians

In this case, the data D is Y deﬁned in Eq (2), the clustering algorithm A is spectral clustering (see
. Note that one could
motivating example in Section 3) on the Gaussian kernel K(i, j) = exp
use the similarity matrix as the kernel itself. However, this makes the trace criterion a function of the
hyperparameter we are trying to tune, which compounds the diﬃculty of the problem. For simplicity, we use
the negative squared distance matrix as ˆS, i.e. ˆSij = −(cid:107)Yi − Yj(cid:107)2
2. The natural choice for S would be the
conditional expectation of ˆS given the cluster memberships, which is blockwise constant, as in the case for
SBM’s. However, in this case, the convergence behavior is diﬀerent from that of blockmodels. In addition,
this choice leads to a suboptimal error rate. Therefore we use a slightly corrected variant of the matrix as S
(also see (Mixon et al. , 2017)), called the reference matrix:

− (cid:107)Yi−Yj (cid:107)2
2θ2

(cid:16)

(cid:17)

2

Sij = −

d2
ab
2

(cid:26)

− max

0,

d2
ab
2

+ 2(Wi − Wj)T (µa − µb)

(cid:27)

1(i ∈ Ca, j ∈ Cb),

(4)

where dab := (cid:107)µa − µb(cid:107), Wi is deﬁned in Eq 2. Note that for i, j in the same cluster Sij = 0. Interestingly
this reference matrix is random itself, which is a deviation from the S used for network models. For MATR
applied to select θ, we have the following theoretical guarantee.

Corollary 4. Let ˆS be the negative squared distance matrix, and let S be deﬁned as in Eq 4. Let δsep
sep/2 and
denote the minimum distance between cluster centers, i.e. mink(cid:54)=(cid:96) (cid:107)µk − µ(cid:96)(cid:107). Denote τ := nπminδ2
α = πmax/πmin. As long as there exists θ0 ∈ {θ1, . . . , θT }, such that (cid:104) ˆXθ0 , ˆS(cid:105) ≥ (cid:104)X0, S(cid:105) − nπmin(cid:15) , Algorithm
1(MATR) will output a ˆZθ∗ , such that w.h.p.

(cid:107) ˆXθ∗ − X0(cid:107)2

F ≤ C

(cid:15) + rασ2

max(α + min{r, d})

δ2
sep

where σmax is the largest operator norm of the covariance matrices of the mixture components, ˆXθ∗ is the
normalized clustering matrix for ˆZθ∗ and C is an universal constant.

Remark 5. Note that, similar to SBMs, in this setting, (cid:15) has to be much smaller than δ2
in order to
guarantee small error. This will happen if the spectral clustering algorithm is supplied with an appropriate
bandwidth parameter that leads to small error in estimating X0 (see for example (Srivastava et al. , 2019)).
This is satisﬁed by the condition θ0 ∈ {θ1, . . . , θT } in Corollary 4.

sep

4 Hyperparameter tuning with unknown r

In this section, we adapt MATR to situations where the number of clusters is unknown to perform model
selection. Similar to Section 3, we ﬁrst explain the general tuning algorithm and state a general theorem to
guarantee its performance. Then applications to speciﬁc models will be discussed in the following subsections.
Since the applications we focus on are network models, we will present our algorithm with the data D being
A for clarity.

We show that MATR can be extended to model selection if we incorporate a cross-validation (CV)
procedure. In Algorithm 2, we present the general MATR-CV algorithm which takes clustering algorithm

7

A , adjacency matrix A, and similarity matrix ˆS as inputs. Compared with MATR, MATR-CV has two
additional parts.

The ﬁrst part (Algorithm 3) is to split nodes into two subsets for training and testing. This in turn
partitions the adjacency matrix A into four submatrices A11, A22, A21 and its transpose, and similarly
for ˆS. MATR-CV makes use of all the submatrices: A11 for training, A22 for testing, A11 and A21 for
estimating the clustering result for nodes in A22 as shown in Algorithm 4, which is the second additional
part. Algorithm 4 clusters testing nodes based on the training nodes cluster membership estimated from A11,
and the connections between training nodes and testing nodes A21.

Algorithm 2: MATR-CV.

Input: clustering algorithm A , adjacency matrix A,
similarity matrix ˆS, candidates {r1, · · · , rT }, number of
repetitions J, training ratio γtrain, trace gap ∆;
for j = 1 : J do

for t = 1 : T do

ˆS11, ˆS21, ˆS22 ← NodeSplitting( ˆS, n, γtrain);
A11, A21, A22 ← NodeSplitting(A, n, γtrain);
ˆZ 11 = A (A11, rt);
ˆZ 22 = ClusterTest(A21, ˆZ 11);
ˆX 22 = ˆZ 22( ˆZ 22T ˆZ 22)−1 ˆZ 22T
;
lrt,j = (cid:104) ˆS22, ˆX 22(cid:105);

end for
r∗
j = min{rt : lrt,j ≥ maxt lrt,j − ∆};
end for
ˆr = median{r∗
j }
Output: ˆr

Algorithm 3: NodeSplitting

Input: A, n, γtrain;
Randomly split [n] into Q1, Q2
of size nγtrain and n(1 − γtrain)
A11 ← AQ1,Q1, A21 ← AQ2,Q1,
A22 ← AQ2,Q2
Output: A11, A21, A22

Algorithm 4: ClusterTest
Input: A21 ∈ {0, 1}n×m,
ˆZ 11 ∈ {0, 1}m×k;
M ← A21 ˆZ 11( ˆZ 11T ˆZ 11)−1;
for i = 1 : n do

ˆZ 22(i, arg max M (i, :)) = 1

end for
Output: ˆZ 22

For each node in the testing set, using the estimated membership ˆZ 11, the corresponding row in M counts
the number of connections it has with nodes in the training set belonging to each cluster and normalizes
the counts by the cluster sizes. Finally, the estimated membership ˆZ 22 is determined by a majority vote.
For now we still assume B is weakly assortative, so majority vote is reasonable. As we later extend to more
general network structures in Section 4.2, we will also show how Algorithm 4 can be generalized.

Like other CV procedures, we note that MATR-CV requires specifying a training ratio γtrain and the
number of repetitions J. Choosing any γtrain = Θ(1) does not aﬀect our asymptotic results. Repetitions of
splits are used empirically to enhance stability; theoretically we show asymptotic consistency for any random
split. The general theoretical guarantee and the role of the trace gap ∆ are given in the next theorem.

Theorem 6. Given a candidate set of cluster numbers {r1, . . . , rT } containing the true number of cluster r,
let ˆX 22
be the normalized clustering matrix obtained from rt clusters, as described in MATR-CV. Assume the
rt
following is true:
(i) with probability at least 1 − δunder, maxrt<r(cid:104) ˆS22, ˆX 22
rt
(ii) with probability at least 1 − δover, maxr<rt≤rT (cid:104) ˆS22, ˆX 22
rt
(iii) for the true r, with probability at least 1 − δest, (cid:104) ˆS22, ˆX 22
(iv) there exists ∆ > 0 such that (cid:15)est + (cid:15)over ≤ ∆ < (cid:15)under − (cid:15)est.
Here (cid:15)under, (cid:15)est, (cid:15)over > 0. Then with probability at least 1 − δunder − δover − δest, MATR-CV will recover the
true r with trace gap ∆.

(cid:105) ≤ (cid:104) ˆS22, X 22
r (cid:105) ≥ (cid:104) ˆS22, X 22

0 (cid:105) + (cid:15)over;
0 (cid:105) − (cid:15)est;

(cid:105) ≤ (cid:104) ˆS22, X 22

0 (cid:105) − (cid:15)under;

The proof is deferred to the Appendix Section B.

Remark 7.

1. MATR-CV is also compatible with tuning multiple hyperparameters. For example, for
SDP-1, if the number of clusters is unknown, then for each ˆr, we can run MATR to ﬁnd the best λ for
the given ˆr, followed by running a second level MATR-CV to ﬁnd the best ˆr. As long as the conditions
in Theorems 1 and 6 are met, ˆr and the clustering matrix returned will be consistent.

2. As will be seen in the applications below, the derivations of (cid:15)under and (cid:15)over are general and only depend

8

on the properties of ˆS. On the other hand, (cid:15)est measures the estimation error associated with the
algorithm of interest and depends on its performance.

In what follows, we demonstrate MATR-CV can be applied to do model selection inherent to an SDP
method for SBM and more general model selection for MMSB. While we still assume an assortative structure
for the former model as required by the SDP method, the constraint is removed for MMSB. Furthermore, we
use these two models to illustrate how MATR-CV works both when (cid:15)est is zero (SBM) and nonzero (MMSB).

4.1 Model selection for SBM

We consider the SDP algorithm introduced in Peng & Wei (2007); Yan et al. (2017) as shown in SDP-2-λ for
community detection in SBM. Here X is a normalized clustering matrix, and in the case of exact recovery
trace(X) is equal to the number of clusters. In this way, r is implicitly chosen through λ, hence most of
the existing model selection methods with consistency guarantees do not apply directly. Yan et al. (2017)
proposed to recover the clustering and r simultaneously. However, λ still needs to be empirically selected
ﬁrst. We provide a systematic way to do this.

max
X

trace(AX) − λtrace(X)

s.t. X (cid:23) 0, X ≥ 0, X1 = 1

(SDP-2-λ)

We consider applying MATR-CV to an alternative form of SDP-2-λ as shown in SDP-2, where the cluster
number r(cid:48) appears explicitly in the constraint and is part of the input. SDP-2 returns an estimated normalized
clustering matrix, to which we apply spectral clustering to compute the cluster memberships. We name this
algorithm ASDP-2. In this case, we use A as ˆS, so P is the population similarity matrix.

max
X

trace(AX)

s.t. X (cid:23) 0, X ≥ 0, trace(X) = r(cid:48), X1 = 1

(SDP-2)

We have the following result ensuring MATR-CV returns a consistent cluster number.

Theorem 8. Suppose A is generated from a SBM model with r clusters and a weakly assortative B. We assume
r is ﬁxed, and πmin ≥ δ > 0 for some constant δ, and nρ/ log n → ∞. Given a candidate set of {r1, . . . , rT }
containing true cluster number r and rT = Θ(r), with high probability for n large, MATR-CV returns the
true number of clusters with ∆ = (1 + Bmax)

rmax log n + Bmaxrmax, where rmax := arg maxrt(cid:104)A, ˆXrt(cid:105).

√

√

Proof sketch. We provide a sketch of the proof here, the details can be found in the Appendix Section B.2.
We derive the three errors in Theorem 6. In this case, we show that w.h.p., (cid:15)under = Ω(npgapπ2
min/r2),
rT log n + Bmaxr, and MATR-CV achieves exact recovery when given the true r, that is,
(cid:15)over = (1 + Bmax)
(cid:15)est = 0. Since (cid:15)under (cid:29) (cid:15)over under the conditions of the theorem, by Theorem 6, taking ∆ = (cid:15)over MATR-CV
returns the correct r w.h.p. Furthermore, we can remove the dependence of ∆ on unknown r by noting that
rmax := arg maxrt(cid:104)A, ˆXrt(cid:105) ≥ r w.h.p., then it suﬃces to consider the candidate range {r1, . . . , rmax}. Thus
rT and r in ∆ can be replaced with rmax.

Remark 9.

1. Although we have assumed ﬁxed r, it is easy to see from the order of (cid:15)under and (cid:15)over
that the theorem holds for r5/n → 0, r4.5√
log n/(nρ) → 0 if we let πmin = Ω(1/r) for clarity. Many
other existing works on SBM model selection assume ﬁxed r. Lei et al. (2016) considered the regime
r = o(n1/6). Hu et al. (2017) allowed r to grow lineary up to a logarithmic factor, but at the cost of
making ρ ﬁxed.

2. Asymptotically, ∆ is equivalent to ∆SDP-2 :=

ﬁxed.

√

rmax log n. We will use ∆SDP-2 in practice when r is

4.2 Model selection for MMSB

In this section, we consider model selection for the MMSB model as introduced in Section 2.2 with a
soft membership matrix Θ, which is more general than the SBM model. As an example of estimation
algorithm, we consider the SPACL algorithm proposed by Mao et al. (2017), which gives consistent parameter
estimation when given the correct r. As mentioned in Section 2.2, a normalized clustering matrix in

9

this case is deﬁned analogously as X = Θ(ΘT Θ)−1ΘT for any Θ. X is still a projection matrix, and
X1n = Θ(ΘT Θ)−1ΘT 1n = Θ(ΘT Θ)−1ΘT Θ1r = 1n, since Θ1r = 1n. Following Mao et al.
(2017), we
consider a Bayesian setting for Θ: each row of Θ, Θi ∼ Dirichlet(α), α ∈ Rr
+. We assume r, α are all ﬁxed
constants. Note that the Bayesian setting here is only for convenience, and can be replaced with equivalent
assumptions bounding the eigenvalues of ΘT Θ. We also assume there is at least one pure node for each of
the r communities for consistent estimation at the correct r.

MATR-CV can be applied to the MMSB model with a few modiﬁcations. (i) Replace all ˆZ 11 by ˆΘ11, the
estimated soft memberships from the training graph. (ii) We take ˆS = A2 − diag(A2), S = P 2 − diag(P 2).
This allows us to remove the assortativity requirement on P and replace it with a full rank condition on B,
which is commonly assumed in the MMSB literature. The fact that P 2 is always positive semi-deﬁnite will
be used in the proof. The removal of diag(A2) and diag(P 2) leads to better concentration, since diag(A2) is
centered around a diﬀerent mean. (iii) We change Algorithm 4 to estimate ˆΘ22. Note that P 12 = Θ11B(Θ22)T ,
thus we can view the estimation of Θ22 as a regression problem with plug-in estimators of Θ11 and B. In
Algorithm 4, we use an estimate of the form ˆΘ22 = A21 ˆΘ11(( ˆΘ11)T ˆΘ11)−1 ˆB−1, where ˆB, ˆΘ11 are estimated
from A11.

We have the following consistency guarantee for ˆr returned by MATR-CV.

Theorem 10. Let A be generated from a MMSB model (see Section 2.2) satisfying λ∗(B) = Ω(ρ), where
λ∗(B) is the smallest singular value of B. We assume √
nρ/(log n)1+ξ → ∞ for some arbitrarily small ξ > 0.
Given a candidate set of {r1, . . . , rT } containing r and rT = Θ(1), with high probability for large n, MATR-CV
returns the true cluster number r if ∆ = O((nρ)3/2(log n)1.01).

√

Proof sketch. We ﬁrst show w.h.p., the underﬁtting and overﬁtting errors in Theorem 6 are (cid:15)under = Ω(n2ρ2),
(cid:15)over = O(nρ
log n). To obtain (cid:15)est, we show that given the true cluster number, the convergence rate of
the parameter estimates for the testing nodes obtained from the regression algorithm is the same as the
convergence rate for the training nodes. This leads to (cid:15)est = O((nρ)3/2(log n)1+ξ). For convenience we pick
ξ = 0.01. For details, see Section B.3 of the supplement.

Remark 11.

1. Compared with Fan et al. (2019) and Han et al. (2019), which consider the more general

degree-corrected MMSB model, our consistency result holds for ρ → 0 at a faster rate.

2. A practical note: due to the constant in the estimation error being tedious to determine, in this case we
only know the asymptotic order of the gap ∆. As has been observed in many other methods based on
asymptotic properties (e.g. Bickel & Sarkar (2016); Lei et al. (2016); Wang & Bickel (2017); Hu et al.
(2017)), performing an adjustment for ﬁnite samples often improves the empirical performance. In
practice we ﬁnd that if the constant factor in ∆ is too large, then we tend to underﬁt. To guard against
this, we note that at the correct r, the trace diﬀerence δr,r−1 := (cid:104) ˆS, ˆXr(cid:105) − (cid:104) ˆS, ˆXr−1(cid:105) should be much
larger than ∆. We start with ∆ = (nρ)3/2(log n)1.01 and ﬁnd ˆr by Algorithm 2; if δˆr,ˆr−1 is smaller
than ∆, we reduce ∆ by half and repeat the step of ﬁnding r∗
in Algorithm 2 until δˆr,ˆr−1 > ∆. This
j
adjustment is much more computationally eﬃcient than bootstrap corrections and works well empirically.

5 Numerical experiments

In this section, we present extensive numerical results on simulated and real data by applying MATR and
MATR-CV to diﬀerent settings considered in Sections 3 and 4.

5.1 MATR on SBM with known number of clusters

We apply MATR to tune λ in SDP-1 for known r. Since λ ∈ [0, 1] for SDP-1, we choose λ ∈ {0, · · · , 20}/20
in all the examples. For comparison we choose two existing data driven methods. The ﬁrst method (CL, Cai
et al. (2015)) sets λ as the mean connectivity density in a subgraph determined by nodes with “moderate”
degrees. The second is ECV (Li et al. (2016)) which uses CV with edge sampling to select the λ giving the
smallest loss on the test edges from a model estimated on training edges. We use a training ratio of 0.9 and
the L2 loss throughout.

10

Simulated data. Consider a strongly assortative SBM as required by SDP-1 for both equal sized and
unequal sized clusters. The details of the experimental setting can be found in the Appendix Section C.
Standard deviations are calculated based on random runs of the each parameter setting. We present NMI
comparisons for equal sized SBM (n = 400, r = 4) in Figure 2(A), and unequal sized SBM (two with 100
nodes, and two with 50) in Figure 2(B). In both, MATR outperforms others by a large margin as degree
grows.

(A)

(B)

(C)

(D)

Figure 2: Comparison of NMI for tuning λ for SDP-1 for equal (A) and unequal sized (B) SBMs. Comparison
of NMI for tuning bandwidth in spectral clustering for mixture models with (C) equal and (D) unequal
mixing coeﬃcients.

Real data. We also compare MATR with ECV and CL on three real datasets: the football dataset (Girvan
& Newman, 2002), the political books dataset and the political blogs dataset (Adamic & Glance, 2005). All
of them are binary networks with 115, 105 and 1490 nodes respectively. In the football dataset, the nodes
represent teams and an edge is drawn between two teams if any regular-season games are played between
them; there are 12 clusters where each cluster represents the conference among teams, and games are more
frequently between teams in the same conference. In the political blogs dataset, the nodes are weblogs
and edges are hyperlinks between the blogs; it has 2 clusters based on political inclination: "liberal" and
"conservative". In the political books dataset, the nodes represent books and edges indicate co-purchasing on
Amazon; the clusters represent 3 categories based on manual labeling of the content: "liberal", "neutral"
and "conservative". The clustering performance of each method is evaluated by NMI and shown in Table 1a.
MATR has performs the best out of the three methods on the football dataset, and is tied with ECV on the
political books dataset. MATR is not as good as CL on the poligical blogs dataset, but still outperforms
ECV.

Football
Political blogs
Political books

MATR ECV CL
0.924
0.258
0.549

0.895
0.142
0.549

0.883
0.423
0.525

Football
Polblogs
Polbooks

Truth MATR-CV ECV BH
10
12
8
2
4
3

12
6
6

10
1
2

(a) NMI with tuning λ on SBM

(b) Model selection with SBM

Table 1: Results obtained on real networks

5.2 MATR on mixture model with known number of clusters

We use MATR-CV to select the bandwidth parameter θ in spectral clustering applied to mixture data
when given the correct number of clusters.
In all the examples, our candidate set of θ is {tα/20} for
t = 1, · · · , 20 and α = maxi,j (cid:107)Yi − Yj(cid:107)2. We compare MATR with three other well-known heuristic methods.
The ﬁrst one was proposed by (Shi et al. , 2008) (DS), where, for each data point Yi, the 5% quantile of
{(cid:107)Yi − Yj(cid:107)2 , j = 1, ..., n} is denoted qi and then θ is set to be 95% quantile of {q1,...,qn}
. We also compare with

√

95% quantile of χ2
d

11

two other methods in Von Luxburg (2007): a method based on k-nearest neighbor (KNN) and a method
based on minimal spanned tree (MST). For KNN, θ is chosen in the order of the mean distance of a point
to its k-th nearest neighbor, where k ∼ log(n) + 1. For MST, θ is set as the length of the longest edge in a
minimal spanning tree of the fully connected graph on the data points.
Simulated data. We ﬁrst conduct experiments on simulated data generated from a 3-component Gaussian
mixture with d = 20. The means are multiplied by a separation constant which controls clustering diﬃculty
(larger, the better). Detailed descriptions of the parameter settings can be found in Section C of the Appendix.
n = 500 datapoints are generated for each mixture model and random runs are used to calculate standard
deviations for each parameter setting. In Figure 2 (A) and (B) we plot NMI on the Y axis against the
separation along the X axis for mixture models with equal and unequal mixing coeﬃcients respectively. For
all these settings, MATR performs as well or better than the best among DS, KNN and MST.
Real data. We also test MATR for tuning θ on a real dataset: Optical Recognition of Handwritten Digits
Data Set1. We use a copy of the test set provided by scikit-learn (Pedregosa et al. , 2011), which consists
of 1797 instances of 10 classes. We standardize the dataset before clustering. With 10 clusters, MATR,
DS, KNN and MST yield cluster results with NMI values 0.64, 0.45, 0.64 and 0.62 respectively. In other
words, MATR performs similarly to KNN but outperforms DS and MST. We also visualize and compare the
clustering results by diﬀerent methods in 2-D using tSNE (Maaten & Hinton, 2008), which can be found in
Section C of the Appendix.

5.3 Model selection with MATR-CV on SBM

√

We make comparisons among MATR-CV, Bethe-Hessian estimator (BH) (Le & Levina, 2015) and ECV (Li
et al. , 2016). For ECV and MATR-CV, we consider r ∈ {1, · · ·
Simulated data. We simulate networks from a 4-cluster strongly assortative SBM with equal and unequal
sized blocks (detailed in Section C of the Appendix). In Figure 3, we show NMI on Y axis vs. average degree
on Y axis. In Figure 3(a) and (b) we respectively consider equal sized (4 clusters of size 100) and unequal
sized networks (two with 120 nodes and two with 80 nodes). In all cases, MATR-CV has the highest NMI. A
table with median number of clusters selected by each method can be found in Section C of the Appendix.
Real data. The same set of methods are also compared on three real datasets: the football dataset, the
political blogs dataset and the political books dataset. The results are shown in Table 1b, where MATR-CV
ﬁnds the ground truth for the football dataset.

n}, where n is the number of nodes.

(a) NMI for equal sized case

(b) NMI for unequal sized case

Figure 3: Comparison of NMI with model selection for equal and unequal sized cases.

5.4 Model selection with MATR-CV on MMSB

We compare MATR-CV with Universal Singular Value Thresholding (USVT) (Chatterjee et al. , 2015), ECV
(Li et al. , 2016) and SIMPLE (Fan et al. , 2019) in terms of doing model selection with MMSB. For ECV
and MATR-CV, we consider the candidate set r ∈ {1, 2, · · · , (cid:98)ˆρn(cid:99)}, where ˆρ = (cid:80)
Simulated data. We ﬁrst apply all the methods to simulated data. We consider B = ρ × {(p − q)Ir + qEr}.
Following (Mao et al. , 2018), we sample Θi ∼ Dirichlet(α) and α = 1r/r. We generate networks with

i<j Aij/(cid:0)n

(cid:1).

2

1https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits

12

n = 2000 nodes with r = 4 and r = 8 respectively. We set p = 1, q = 0.1 when r = 4; p = 1, q = 0.01 when
r = 8 for a range of ρ.In Table 2a and 2b, we report the fractions of exactly recovering the true cluster
number r over 40 runs for each method across diﬀerent average degrees. We observe that in both r = 4 and
r = 8 cases, MATR-CV outperforms the other three methods with a large margin on sparse graphs. The
method SIMPLE consistently underﬁts in our sparsity regime, which is understandable, since their theoretical
guarantees hold for a dense degree regime.

ρ
0.01
MATR-CV 0.35
USVT
ECV

0
0

0.03
0.83
0
0

0.06
0.93
1
0

0.08
1
1
0.95

0.11
1
1
1

0.13
1
1
1

ρ
0.02
MATR-CV 0.10
USVT
ECV

0
0

0.05
0.43
0
0

0.09
0.95
0.58
0

0.12
0.93
1
0.93

0.16
0.95
1
1

0.21
1
1
1

(a) Exact recovery fractions for 4 clusters

(b) Exact recovery fractions for 8 clusters

Table 2: Results of MMSB on synthetic data

Real data. We also test MATR-CV with MMSB on a real network, the political books network, which
contains 3 clusters. Here ﬁtting a MMSB model is reasonable since each book can have mixed political
inclinations, e.g. a “conserved” book may be in fact mixed between “neutral” and “conservative”. With
MATR-CV, we found 3 clusters. With USVT, ECV and SIMPLE we found fewer than 3 clusters.

6 Discussion

Clustering data, both in i.i.d and network structured settings have received a lot of attention both from applied
and theoretical communities. However, methods for tuning hyperparameters involved in clustering problems
are mostly heuristic. In this paper, we present MATR, a provable MAx-TRace based hyperparameter tuning
framework for general clustering problems. We prove the eﬀectiveness of this framework for tuning SDP
relaxations for community detection under the block model and for learning the bandwidth parameter of the
gaussian kernel in spectral clustering over a mixture of subgaussians. Our framework can also be used to
do model selection using a cross validation based extension (MATR-CV) which can be used to consistently
estimate the number of clusters in blockmodels and mixed membership blockmodels. Using a variety of
simulation and real experiments we show the advantage of our method over other existing heuristics.

The framework presented in this paper is general and can be applied to doing model selection or tuning
for broader model classes like degree corrected blockmodels (Karrer & Newman, 2011), since there are many
exact recovery based algorithms for estimation in these settings (Chen et al. , 2018). We believe that our
framework can be extended to the broader class of degree corrected mixed membership blockmodels (Jin et al.
, 2017) which includes the topic model (Mao et al. , 2018). However, the derivation of the estimation error
(cid:15)est involves tedious derivations of parameter estimation error, which has not been done by existing works.
Furthermore, even though our work uses node sampling, we believe we can extend the MATR-CV framework
to get consistent model selection for other sampling procedures like edge sampling (Li et al. , 2016).

Appendix

This appendix contains detailed proofs of theoretical results in the main paper “A Uniﬁed Framework for
Tuning Hyperparameters in Clustering Problems”, additional theoretical results, and detailed description of the
experimental parameter settings. We present proofs for MATR and MATR-CV in Sections A and Sections B
respectively. Sections A.2 also contains additional theoretical results on the role of the hyperparameter in
merging clusters in SDP-1 and SDP-2 respectively. Finally, Section C contains detailed parameter settings
for the experimental results in the main paper.

13

A Additional theoretical results and proofs of results in Section 3

A.1 Proof of Theorem 1
Proof. If for tuning parameter λ, we have (cid:104) ˆS, ˆXλ(cid:105) ≥ (cid:104)S, X0(cid:105) − (cid:15), then

(cid:104)S, ˆXλ(cid:105) ≥ (cid:104)S, X0(cid:105) − |(cid:104) ˆS − S, ˆXλ(cid:105)| − (cid:15).

(5)

First we will prove that this immediately gives an upper bound on (cid:107) ˆXλ − X0(cid:107)F . We will remove the subscript
λ for ease of exposition. Denote ωk = (cid:104)X0, ˆXCk,Ck (cid:105), αij = (cid:104)Ei,j , ˆX(cid:105)
mk(1−ωk) , when ωk < 1 and 0 otherwise, and
oﬀ-diagonal set for kth cluster C c

k as {(i, j)|i ∈ Ck, j /∈ Ck}. Then we have
r0(cid:88)
r0(cid:88)

(cid:88)

akk(cid:104)ECk,Ck , ˆX(cid:105) +

aij(cid:104)Ei,j, ˆX(cid:105)

(cid:104)S, ˆX(cid:105) =

k=1

(i,j)∈Cc
k

mk(1 − ωk)

(cid:88)

aijαij

(6)

k=1

r0(cid:88)

k=1

r0(cid:88)

k=1

=

=

akkmkωk +

r0(cid:88)

k=1

mkωk(akk −

(cid:88)

(i,j)∈Cc
k
r0(cid:88)

aijαij) +

mk

(cid:88)

aijαij

(i,j)∈Cc
k

k=1

(i,j)∈Cc
k

k mkakk − |(cid:104)R, ˆX(cid:105)| − (cid:15), we have

Since (cid:104)S, X0(cid:105) = (cid:80)
(cid:88)

k mkakk, by (5), (cid:104)S, ˆX(cid:105) ≥ (cid:80)
(cid:88)

(cid:88)

mkωk(akk −

aijαij) +

mk

k

(i,j)∈Cc
k

k

(i,j)∈Cc
k

(cid:88)

aijαij ≥

mkakk − |(cid:104)R, ˆX(cid:105)| − (cid:15).

(cid:88)

k

Note that, since S is weakly assortative, akk −(cid:80)
Denote (cid:15)(cid:48) = |(cid:104)R, ˆX(cid:105)| + (cid:15), βk =

mk(akk−(cid:80)
Cc
k
k mk(akk−(cid:80)
Cc
k

(cid:80)

(i,j)∈Cc
k

αij aij )

αij aij ) ,

aijαij is always positive because (cid:80)

(i,j)∈Cc
k

αij ≤ 1.

mkωk(akk −

(cid:88)

aijαij) ≥

(cid:88)

k

βkωk ≥ 1 −

(cid:80)

(i,j)∈Cc
k
(cid:88)

k

(cid:88)

k

βk(1 − ωk) ≤

(cid:88)

k

mk(akk −

(cid:88)

αijaij) − (cid:15)(cid:48)

(i,j)∈Cc
k
(cid:15)(cid:48)

k mk(akk − (cid:80)
(cid:15)(cid:48)

αijaij)

Cc
k

(cid:80)

k mk(akk − (cid:80)

Cc
k

αijaij)

.

(1 − ωk) ≤

(cid:88)

k

(cid:88)

k

βk
βmin

(1 − ωk) ≤

βmin

where βmin = mink βk. Since trace( ˆX) = trace(X0),

(cid:15)(cid:48)
k mk(akk − (cid:80)

(cid:80)

Cc
k

,

αijaij)

(cid:13)
ˆX − X0
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

= trace(( ˆX − X0)T ( ˆX − X0))

= trace( ˆX + X0 − 2 ˆXX0)

= 2trace(X0) − 2

(cid:88)

(cid:104)X0, ˆXCk,Ck (cid:105)

= 2

(cid:88)

(1 − ωk) ≤

k

≤

nπmin mink(akk − maxCc

k

2(cid:15)(cid:48)

mink mk(akk − (cid:80)
Cc
k
2(cid:15)(cid:48)
τ

aij)

=

.

k

2(cid:15)(cid:48)

αijaij)

Now consider the λ∗ returned by MATR,

14

Then, following the above argument and from the condition from the theorem,

(cid:104) ˆS, ˆXλ∗ (cid:105) ≥ (cid:104) ˆS, ˆXλ(cid:105) ≥ (cid:104)S, X0(cid:105) − (cid:15).

(cid:107)Xλ∗ − X0(cid:107)2

F ≤

2(cid:15)(cid:48)

nπmin mink(akk − maxCc

k

aij)

≤

2
τ

((cid:15) + sup
X∈Xr0

|(cid:104)X, R(cid:105)|).

A.2 Range of λ for merging clusters in SDP-1
Proposition 12. Let ˜X be the optimal solution of SDP-1 for A ∼ SBM (B, Z0) with λ satisfying

max
k(cid:54)=(cid:96)

B∗

k,(cid:96) + Ω(

(cid:114) ρ log n
nπmin

) ≤ λ ≤ min

k

B∗

kk − max

k,(cid:96)=r−1,r

m(cid:96)
nk

(B(cid:96),(cid:96) − Br,r−1) + O(

(cid:115)

ρ log n
nπ2

max

),

then ˜X = X ∗ with probability at least 1 − 1
n
the last two clusters, B∗ is the corresponding (r − 1) × (r − 1) block probability matrix.

, where X ∗ is the unnormalized clustering matrix which merges

Remark: The proposition implies if the ﬁrst r − 2 clusters are more connected within each cluster than
the last two clusters and the connection between ﬁrst r − 2 clusters and last two clusters are weak, we can ﬁnd
a range for λ that leads to merging the last two clusters with high probability. The results can be generalized
to merging several clusters at one time. The result above highlights the importance of selecting λ as it aﬀects
the performance of SDP-1 signiﬁcantly.

Proof. We develop suﬃcient conditions with a contruction of the dual certiﬁcate which guarantees X ∗ to be
the optimal solution. The KKT conditions can be written as below:

First order stationary:

Primal feasibility:

Dual feasibility:

Complementary slackness

−A − Λ + λEn − diag(β) − Γ = 0

X (cid:23) 0, X ≥ 0, Xii = 1 ∀i = 1 · · · , n

Γ ≥ 0, Λ (cid:23) 0

(cid:104)Λ, X(cid:105) = 0, Γ ◦ X = 0.

Consider the following construction: denote Tk = Ck, nk = mk, for k < r − 1, Tr−1 = Cr−1

(cid:83) Cr, nr−1 =

mr−1 + mr.

XTk = Enk

XTkTl = 0, for k (cid:54)= l ≤ r − 1

ΛTk = −ATk + λEnk − λnkInk + diag(ATk 1nk )

ΛTkTl = −ATk,Tl +

1
nl

ATk,Tl Enl +

Enk ATkTl −

1
nk
ΓTk = 0

ΓTk,Tl = λEnk,nl −

ATk,Tl Enl −

1
nl
β = diag(−A − Λ + λEn − Γ)

Enk ATkTl +

1
nk

1
nlnk

1
nlnk

Enk ATk,Tl Enl

Enk ATk,Tl Enl

All the KKT conditions are satisﬁed by construction except for positive semideﬁniteness of Λ and

positiveness of Γ. Now, we show it one by one.

15

Positive Semideﬁniteness of Λ Since span(1Tk ) ⊂ ker(Λ), it suﬃces to show that for any u ∈

span(1Tk )⊥, uT Λu ≥ 0. Consider u = (cid:80)
(cid:88)
(cid:88)

uT Λu = −

uT
Tk

ATk uTk − λ

k uTk , where uTk := u ◦ 1Tk , then uTk ⊥ 1nk .

nkuT
Tk

uTk +

(cid:88)

uT
Tk

diag(ATk 1nk )uTk −

(cid:88)

k(cid:54)=l

uT
Tk

ATkTl uTl

k

k

= −uT (A − P )uT − uT P u − λ

k
nkuT
Tk

(cid:88)

k

= −uT (A − P )u − uT

Tk−1

PTk−1Tk−1uTk−1 − λ

uTk +

(cid:88)

uT
Tk

diag(ATk 1nk )uTk

(7)

k
nkuT
Tk

(cid:88)

k

uTk +

(cid:88)

uT
Tk

k

diag(ATk 1nk )uTk

For the ﬁrst term, we know

with high probability.

uT (A − P )u ≤ (cid:107)A − P (cid:107)2 (cid:107)u(cid:107)2

2 ≤ O(

√

nρ) (cid:107)u(cid:107)2
2

For the second term, and note that Tr−1 = Cr−1

(cid:83) Cr, and
(cid:20)Br−1,r−1Emr−1mr−1, Br−1,rEmr−1mr
Br,r−1Emrmr−1, Br,rEmrmr

(cid:21)

PTr−1Tr−1 =

Since uTr−1 ⊥ 1nr−1,

therefore

uT
Tr−1

(cid:20)Br−1,rEmr−1mr−1, Br−1,rEmr−1mr
Br,r−1Emrmr−1, Br,r−1Emrmr

(cid:21)

uTr−1 = 0,

uT
Tr−1

PTr−1Tr−1uTr−1 = uT

Tr−1

(cid:21)
(cid:20)(Br−1,r − Br−1,r−1)Emr−1mr−1 , 0
0, (Br−1,r − Br,r)Emrmr

uTr−1

(8)

≤ max{mr−1(Br−1,r−1 − Br−1,r), mr(Br,r − Br,r−1)} (cid:107)u(cid:107)2
2

Consider the last term (cid:80)

k uT
Tk

diag(ATk 1nk )uTk . Using Chernoﬀ, we know

||diag(ATk 1nk )||2 ≥ B∗

k,knk − (cid:112)6ρnk log nk

with high probability, where for k, l < r − 1,

B∗

kl = Bkl,

B∗

k,r−1 =

mr−1Bk,r−1 + mrBk,r
mr−1 + mr
r−1Br−1,r−1 + 2 ∗ mrmr−1Br−1,r + (m2
(mr−1 + mr)2

,

(m2

rBr,r)

.

B∗

r−1,r−1 =

Therefore, :

(cid:88)

−λ

nkuT
Tk

uTk +

(cid:88)

uT
Tk

k

k

diag(ATk 1nk )uTk ≥ min

k

(B∗

k,knk − Ω((cid:112)ρnk log n) − λnk) (cid:107)u(cid:107)2
2 .

So with equation 7, a suﬃcient condition for positive semideﬁniteness of Λ is

(B∗

√
k,knk − Ω((cid:112)ρnk log n) − λnk) ≥ O(

min
k

nρ) + max{mr−1(Br−1,r−1 − Br−1,r), mr(Br,r − Br,r−1)}

which implies,

λ ≤ min

k

B∗

kk − max

k

max{

mr−1
nk

(Br−1,r−1 − Br−1,r),

mr
nk

(Br,r − Br,r−1)} + O((cid:112)ρ log n/nπ2

max)

16

Positiveness of Γ For i ∈ Tk, j ∈ Tl, we have

Γi,j = λ −

(cid:80)

m∈Tl
nl

Ai,m

−

(cid:80)

m∈Tk
nk

Am,j

+

1
nknl

(cid:88)

Amo.

m∈Tk,o∈Tl

Therefore, block-wise mean of Γ will be

E[ΓTk,Tl ] = (λ − B∗

k,l)Enk,nl ,

and the variance for each entry belonging to cluster k and l will be in order of O(ρ/(nknl)).

Using Chernoﬀ bound, we have

p(|Γi,j − (λ − B∗

k,l)| > λ − B∗

k,l) ≤ 2 exp

(cid:20)

−

nknl
2ρ

(λ − B∗

k,l)2

(cid:21)
.

Therefore, as long as λ ≥ maxk(cid:54)=l B∗

k,l + Ω((cid:112)ρ log n/nπmin), we have
(cid:21)
(cid:20)

p(Γi,j < 0) ≤ 2 exp

−

nπmin log n
2

We then applying the union bound and conclude that ΓTkTl > 0 with a high probability when λ ≥
maxk(cid:54)=l B∗

k,l + Ω((cid:112)ρ log n/nπmin).

Proposition 13. As long as maxk(cid:54)=l Bk,l + Ω((cid:112)ρ log n/nπmin) ≤ λ ≤ mink Bkk + O((cid:112)ρ log n/nπ2
SDP-1 exactly recovers X0 with high probability.
Proof. We follow the same primal-dual construction as Proposition 12 without merging the last two clusters.
Consider the following construction: denote Tk = Ck, nk = mk, for k = 1, ..., r. We show the positive
semideﬁniteness and Positiveness of Λ and Γ respectively.

max),

Positve Semideﬁniteness of Λ Since span(1Tk ) ⊂ ker(Λ), it suﬃces to show that for any u ∈

span(1Tk )⊥, uT Λu ≥ 0. Consider u = (cid:80)

uT Λu = −

(cid:88)

k

uT
Tk

ATk uTk − λ

k

k uTk , where uTk := u ◦ 1Tk , and uTk ⊥ 1nk , we have
(cid:88)

(cid:88)

(cid:88)

nkuT
Tk

uTk +

uT
Tk

diag(ATk 1nk )uTk −

uT
Tk

ATkTl uTl

= −uT (A − P )uT − uT P u − λ

k
nkuT
Tk

(cid:88)

k

uTk +

(cid:88)

uT
Tk

k

k(cid:54)=l

diag(ATk 1nk )uTk

(9)

= −uT (A − P )u − λ

(cid:88)

k

nkuT
Tk

uTk +

(cid:88)

uT
Tk

diag(ATk 1nk )uTk

k

For the ﬁrst term, we know

uT (A − P )u ≤ (cid:107)A − P (cid:107)2 (cid:107)u(cid:107)2

2 ≤ O(

√

nρ) (cid:107)u(cid:107)2
2

with high probability, and using Chernoﬀ, we have

||diag(ATk 1nk )||2 ≥ Bk,knk − (cid:112)6ρnk log nk

with high probability. Therefore,

(cid:88)

−λ

nkuT
Tk

uTk +

(cid:88)

uT
Tk

k

k

diag(ATk 1nk )uTk ≥ min

k

(Bk,knk − Ω((cid:112)ρnk log n) − λnk) (cid:107)u(cid:107)2
2 ,

which implies a suﬃcient condition for positive semideﬁniteness of Λ is

λ ≤ min

k

Bkk + O((cid:112)ρ log n/nπ2

max),

and the lower bound can be obtained exactly the same way as Proposition 12. Using Chernoﬀ bound,
ΓTkTl > 0 with high probability as long as λ ≥ maxk(cid:54)=l Bk,l + Ω((cid:112)ρ log n/nπmin).

17

A.3 Proof of Corollary 2
Proof. This result comes directly from Theorem 1. We have S = ˜P , R = (A − P ) + (P − ˜P ). For λ0,

where rρ = o(τ ) since r

√

nρ = o(τ ), and for any ˆX ∈ Xr,

(cid:104) ˆXλ0 , A(cid:105) ≥ (cid:104)X0, ˜P (cid:105) − O(rρ) − (cid:15),

|(cid:104)A − ˜P , ˆX(cid:105)| ≤ ||A − P ||optrace( ˆX) + O(rρ) = OP (r

√

nρ).

The last inequality follows by Lei & Rinaldo (2015) and nρ ≥ c log n.

A.4 Proof of Corollary 4
Proof. First note that pgap as deﬁned in Eq 1 is δ2
two cluster centers. Using the argument in Mixon et al. (2017) and Theorem 1 we obtain:

sep/2, where δsep is the minimum Euclidean distance between

(cid:107) ˆXθ∗ − X0(cid:107)2

F ≤ 4

nπmin(cid:15) + supX∈Xr |(cid:104)X, ˆS − S(cid:105)|
nπminδ2

sep
max(α + min{r, d})

(cid:15) + rασ2

(i)
≤ C

δ2
sep

Step (i) is true with probability at least 1 − 2η, as long as n ≥ max{c1d, c2 log(2/η), log(c4/η)}, using the
argument from Theorem 2 in Mixon et al. (2017).

B Additional Theoretical Results and Proofs of Results in Section 4

B.1 Proof of Theorem 6
Proof. With probability greater than 1 − δest − δover − δunder, the following three inequalities hold.

For rT ≥ rt > r :

(cid:104) ˆS22, ˆX 22

r (cid:105) ≥ (cid:104) ˆS22, X 22

0 (cid:105) − (cid:15)est ≥ max

rT ≥rt>r

(cid:104) ˆS22, ˆX 22
rt

(cid:105) − (cid:15)est − (cid:15)over

≥ max

rT ≥rt>r

(cid:104) ˆS22, ˆX 22
rt

(cid:105) − ∆

For rt < r :

(cid:104) ˆS22, ˆX 22

r (cid:105) ≥ (cid:104) ˆS22, X 22

0 (cid:105) − (cid:15)est ≥ max
rt<r

(cid:104) ˆS22, ˆX 22
rt

(cid:105) − (cid:15)est + (cid:15)under

> max
rt<r

(cid:104) ˆS22, ˆX 22
rt

(cid:105) + ∆

Therefore, with probability at least 1 − δest − δover − δunder, (cid:104) ˆS22, ˆX 22

r (cid:105) ≥ maxt(cid:104) ˆS22, ˆX 22
rt

(cid:105) − ∆. Let

R = {rt : (cid:104) ˆS22, ˆX 22
rt

(cid:105) ≥ maxt(cid:104) ˆS22, ˆX 22
rt
Furthermore, with probabiltiy at least 1 − δest − δunder, for rt < r :

(cid:105) − ∆}. It follows then r ∈ R.

(cid:104) ˆS22, ˆX 22
rt

(cid:105) ≤ (cid:104) ˆS22, X 22

0 (cid:105) − (cid:15)under ≤ (cid:104) ˆS22, ˆX 22

r (cid:105) + (cid:15)est − (cid:15)under

Therefore, for any rt < r, rt /∈ R, and min{rt : rt ∈ R} = r.

< max

t

(cid:104) ˆS22, ˆX 22
rt

(cid:105) − ∆.

18

B.2 Proof of Theorem 8

We ﬁrst prove a concentration lemma that holds for any normalized clustering matrix X independent of A.

Lemma 14. Consider a an adjacency matrix A and its population version P . Let X be a normalized
clustering matrix independent of A. Then with probability at least 1 − O(n−1),

|(cid:104)A − P, X(cid:105)| ≤ (1 + Bmax)(cid:112)trace(X) log n

with Bmax = maxi,j Bij.

Proof. The result follows from Hoeﬀding’s inequality and the fact that X is a projection matrix.

By independence between A and X,



(cid:88)



P

i<j



(Aij − Pij)Xij > t

 ≤ exp(−

≤ exp(−

= exp(−

)

2t2

(1 + Bmax)2 (cid:80)

i<j X 2
ij

)

4t2
(1 + Bmax)2 (cid:107)X(cid:107)2
F
4t2
(1 + Bmax)2trace(X)

)

2 (1+Bmax)(cid:112)trace(X) log n, then by symmetry in A and X, P ((cid:104)A−P, X(cid:105) > (1+Bmax)(cid:112)trace(X) log n) =

Let t = 1
O(1/n). The other direction is the same.

In order to prove Theorem 8, we need to derive the three error bounds in Theorem 6 in this setting. For
notational convenience, we ﬁrst derive the bounds for A and a general normalized clustering matrix ˆX, with
the understanding that the same asymptotic bounds apply to estimates obtained from the training graph
provided the split is random and the number of training nodes is Θ(n).

Lemma 15. For a sequence of underﬁtting normalized clustering matrix { ˆXrt}rt<r, all independent of A,
provided nρ/

logn → ∞, we have

√

(cid:104)A, ˆXrt(cid:105) ≤ (cid:104)A, X0(cid:105) − ΩP (nρπ2

min/r2),

max
rt<r

for ﬁxed r and πmin.
Proof. Let { ˆCk} be the clusters associated with ˆX. Denote γk,i = | ˆCk ∩ Ci|, and ˆmk = | ˆCk| = (cid:80)
i γk,i,
rt = rank( ˆX) < r. First note that for each i ∈ [r], ∃k ∈ [rt], s.t. γk,i ≥ |Ci|/rt. Since r > rt, by the
Pigeonhole principle, we see that ∃i0, j0, k0, i0 (cid:54)= j0, such that,

γk0,i0 = | ˆCk0 ∩ Ci0| ≥ |Ci0|/rt ≥ πminn/rt

γk0,j0 = | ˆCk0 ∩ Cj0| ≥ |Cj0|/rt ≥ πminn/rt

(10)

For each k (cid:54)= k0,

(cid:80)

i,j Bi,jγk,iγk,j
ˆmk

≤

(cid:80)

i Bi,i

(cid:80)

j γk,iγk,j

ˆmk

(cid:88)

=

Bi,iγk,i.

i

19

For k = k0,

(cid:80)

i,j Bi,jγk,iγk,j
ˆmk

=

=

≤

=

(cid:88)

i
(cid:88)

i
(cid:88)

i

(cid:80)

i,j Bi,iγk,iγk,j
ˆmk

(cid:80)

i(cid:54)=j(Bi,j − Bi,i)γk,iγk,j
ˆmk

+

Bi,iγk,i +

(cid:80)

i(cid:54)=j(Bi,j − Bi,i)γk,iγk,j
ˆmk

Bi,iγk,i +

(2Bi0,j0 − Bi0,i0 − Bj0,j0 )γk,i0 γk,j0
ˆmk

Bi,iγk,i −

((Bi0,i0 − Bi0,j0) + (Bj0,j0 − Bi0,j0 )) γk,i0 γk,j0
ˆmk

(a)
≤

(cid:88)

Bi,iγk,i −

i
(cid:88)

i

≤

Bi,iγk,i −

2τ γk,i0 γk,j0
nπmin ˆmk

2τ πminn
r2
t ˆmk

,

where τ = nπminpgap, pgap := mini(Bi,i − maxj(cid:54)=i Bi,j). (a) is true by deﬁnition of τ and Eq (10).

Therefore, since ˆmk0 ≤ n,

(cid:104)P, ˆX(cid:105) =

(cid:80)

rt(cid:88)

k=1

i,j Bi,jγk,iγk,j
ˆmk

− O(ρrt) ≤

rt(cid:88)

r
(cid:88)

k=1

i=1

Bi,iγk,i − Ω(

τ πminn
r2
t ˆmk0

)

= (cid:104)P, X0(cid:105) − Ω

(cid:18) τ πmin
r2
t

(cid:19)

.

Next by Lemma 14, for each X with trace(X) ≤ r,

|(cid:104)A − P, X(cid:105)| ≤ (1 + Bmax)(cid:112)r log n

with probabiltiy at least 1 − O(1/n). By a union bound and using the same argument, w.h.p.

|(cid:104)A − P, ˆXrt(cid:105)| ≤ (1 + Bmax)(cid:112)r log n

max
rt<r

Eqs (12) and (13) imply w.h.p.

(cid:104)A, X0(cid:105) − max
rt<r
(cid:18) npgapπ2

min

(cid:104)A, ˆXrt(cid:105)
(cid:19)

=Ω

r2

− O((cid:112)r log n) = Ω

(cid:18) npgapπ2

min

(cid:19)

r2

(11)

(12)

(13)

(14)

using the condition in the Lemma.

Lemma 16. For a sequence of overﬁtting normalized clustering matrix { ˆXrt}r<rt≤rT
rT = Θ(r), we have w.h.p.

, all independent of A,

max
r<rt≤rT

(cid:104)A, ˆXrt(cid:105) ≤ (cid:104)A, X0(cid:105) + (1 + Bmax)(cid:112)rT log n + Bmaxr.

Proof. First note, for any ˆX, using weak assortativity on B,

(cid:104)P, ˆX(cid:105) ≤

≤

(cid:88)

i,j
(cid:88)

i

ˆXi,jBC(i),C(j)

BC(i),C(i)

(cid:88)

ˆXi,j

j

≤ (cid:104)P, X0(cid:105) + Bmaxr,

20

(15)

where C(i) denotes the cluster node i belongs to. By the same argument as in Eq (13), w.h.p.

max
r<rt≤rT

|(cid:104)A − P, ˆXrt(cid:105)| ≤ (1 + Bmax)(cid:112)rT log n

(16)

From the above

max
r<rt≤rT

(cid:104)A, ˆXrt(cid:105) ≤ (cid:104)A, X0(cid:105) + (1 + Bmax)(cid:112)rT log n + Bmaxr.

Lemma 17. With probabilitiy at least 1 − O(1/n), MATR-CV achieves exact recovery on the testing nodes
given the true cluster number r, i.e. ˆX 22
Proof. Denote m11
k , m22
graph respectively.

k as the number of nodes belonging to the cluster Ck in the training graph and testing

, provided nπminρ/ log n → ∞, γtrain = Θ(1).

r = X 22
0

First, with Theorem 2 in Yan et al. (2017) and Lemma 18, we know SDP-2 can achieve exact recovery on
traininng graph with high probability. Now, consider a node s in testing graph, and assume it belongs to

cluster Ck. The probability that it is assigned to cluster k is: P (

Using the Chernoﬀ bound, for some constant c,

A21
s,j

(cid:80)

j∈Ck
m11
k

≥ maxl(cid:54)=k

(cid:80)

A21
s,j

).

j∈Cl
m11
l

(cid:80)

P (

P (

j∈Ck
m11
k

(cid:80)

j∈Cl
m11
l

A21
s,j

A21
s,j

≥ Bk,k − c

(cid:113)

Bk,k log n/m11

k ) ≥ 1 − n−3;

(cid:113)

≤ Bl,k + c

Bl,k log n/m11

l ) ≥ 1 − n−3.

(17)

Since the graph split is random, for each k, with probability at least 1 − n−3, |m11
mk log n
for some constant c1. By a union bound, this holds for all k with probability at least 1 − rn−3. Then under
this event,

k − γtrainmk| ≤ c1

√

(cid:115)

Bl,k log n
m11
l

≤ c2

(cid:114) Bl,k log n
nπmin

for some c2 since nπmin/ log n → ∞. Since nπminρ/ log n → ∞, by Eq (17), with probabitliy at least
1 − O(rn−3),

(cid:113)

Bk,k − c

Bk,k log n/m11

k > max
l(cid:54)=k

Bl,k + c

Bl,k log n/m11
k ,

(cid:113)

and node s is assigned correctly to cluster k. Taking a union over all s in the training set, with probability
1 − O(rn−2), MATR-CV would give exact recovery for the testing graph given r.

Lemma 18. If mk ≥ πn, then m11
≤ δ, then maxk,l
maxk,l

k ≥ πnγtrain, and m22
≤ δ + o(1) with high probability.

mk
ml

m11
k
m11
l

k ≥ πn(1 − γtrain), with high probability.

If

Proof. The result follows from Skala (2013).

Proof for Theorem 8

Proof. First we note that by a similar argument as in Lemma 17, |m22
mk log n for all
k with probability at least 1 − rn−3. Then the size of the smallest cluster of the test graph A22 will be of the
same order as nπmin. Also A22 has size Θ(n). A22 is independent of any ˆX 22. Thus in Theorem 6, applying
Lemma 15 and Lemma 16 to A22 shows

k − (1 − γtrain)mk| ≤ c

√

(cid:15)under = Ω(nρπ2

min/r2),

(cid:15)over = (1 + Bmax)(cid:112)rT log n + Bmaxr,
and Lemma 17 shows (cid:15)est = 0, w.h.p. For ﬁxed r, πmin, we have (cid:15)under (cid:29) (cid:15)over. By Theorem 6, choosing
rT log n + Bmaxr leads to MATR-CV returning the correct r. We can further reﬁne
∆ = (1 + Bmax)
∆ by noting that rmax := arg maxrt(cid:104)A, ˆXrt(cid:105) ≥ r w.h.p., then it suﬃces to consider the candidate range
{r1, . . . , rmax}. The same arguments still hold for this range, thus rT and r in ∆ can be replaced with
rmax.

√

21

B.3 Proof of Theorem 10

In the following, we show theoretical guarantees of using MATR-CV to do model selection on MMSB with
the SPACL algorithm proposed by Mao et al. (2017). We assume A has self-loops for clarify of exposition.
Adding the diagonal terms introduces a term that is asymptotically negligible compared with other terms,
thus does not change our results.

First we have the following concentration lemma regarding the criterion (cid:104)A2 − diag(A2), X(cid:105) for a general

normalized clustering matrix X, which will be used to derive the three errors in Theorem 6.
Lemma 19. For any general normalized clustering matrix X satisfying X1n = 1n, and an adjacency matrix
A generated from its expectation matrix P independent of X, w.h.p.
(cid:104) ˆS − S, X(cid:105) = O(nρ(cid:112)log n),

(18)

where ˆS = A2 − diag(A2), S = P 2 − diag(P 2).

Proof.

(cid:104) ˆS − S, X(cid:105) =

(cid:88)

(AijAjk − PijPjk)Xik

j,i(cid:54)=k
(cid:88)

(AijAjk − E[AijAjk])Xik

=

(cid:88)

−

(Aij − E[Aij])Xij

(19)

i,j,k
(cid:124)

(cid:123)(cid:122)
Part (i)

i,j
(cid:124)

(cid:125)

(cid:123)(cid:122)
Part (ii)

(cid:125)

To bound Part (i), we will ﬁrst bound f (Aij, 1 ≤ i ≤ j ≤ n) = (cid:80)

i,j,k AijAjkXik/2. Let

fuv := f (Aij, 1 ≤ i ≤ j ≤ n, Auv = 0) = f (A) −

Auv

(cid:80)

k AvkXuk + Auv

2

(cid:80)

i AiuXiv

,

Clearly, 0 ≤ f − fuv ≤ 1 since X1n = 1n, and

(cid:88)

u<v

(f − fuv) ≤

≤

≤

(cid:88)

Auv

(cid:80)

k AvkXuk + Auv

(cid:80)

i AiuXiv

2

Auv

(cid:80)

k AvkXuk + Auv

(cid:80)

i AiuXiv

2

u<v
1
2

(cid:88)

u,v

1
2

(cid:88)

u,v,k

AuvAvkXuk + AiuAuvXiv
2

So f is (1/2, 0) self bounding. Hence,

≤ f /2

P (f − E[f ] ≥ t) ≤ exp

P (f − E[f ] ≤ −t) ≤ exp

(cid:18)

(cid:18) −t2

E[f ] + t
−t2
E[f ] + 2t/3

(cid:19)

(cid:19)

Note that E[f ] = Θ(n2ρ2), and so setting t = Θ(nρ

√

log n) we get:

P (f − E[f ] ≥ t) = O(1/n)

P (f − E[f ] ≤ −t) = O(1/n)

Using the same argument on Part (ii), shows that it is O(
Therefore using Eq (19), we see that:

√

nρ log n) w.h.p.

(cid:104) ˆS − S, X(cid:105) = O(nρ(cid:112)log n)

w.h.p.

22

The next two propositions are for general underﬁtting and overﬁtting normalized clustering matrices ˆX

independent of A, which will be used to derive (cid:15)under and (cid:15)over.
Proposition 20. For a sequence of underﬁtting normalized clustering matrix { ˆXrt}rt<r, all independent of
A, then with high probability, maxrt<r(cid:104)A2 − diag(A2), ˆXrt (cid:105) ≤ (cid:104)A2 − diag(A2), X0(cid:105) − Ω(n2ρ2).
Proof. Suppose P = ΘBΘT has eigenvalue decomposition QΛQT . Also consider the singular value decompo-
sition of ˆΘ = ˆU ˆD ˆV T , then ˆXrt = ˆΘ( ˆΘT ˆΘ)−1 ˆΘT = ˆU ˆU T . We have

(cid:104)P 2, X0 − ˆXrt(cid:105) = trace(P 2) − trace(P 2 ˆU ˆU T )

= trace(Λ2) − trace(Λ2QT ˆU ˆU T Q)
= trace(Λ2) − trace(Λ2 ˆQT ˆQ)
i,i = (λ∗(P ))2,
≥ min

Λ2

i

(20)

where ˆQ := ˆU T Q, λ∗(P ) is the smallest singular value of P . The last line follows from Von Neumann’s trace
≤ 1 and rank( ˆQT ˆQ) ≤ rt < r.
inequality and the fact that

(cid:13)
(cid:13)
ˆQT ˆQ
(cid:13)
(cid:13)
(cid:13)op
(cid:13)
Applying Lemma B.4 and Lemma 3.6 in Mao et al.

(2017) to (20), with probability at least 1 −

(cid:16)

r0 exp

−

n
36ν2(1+α0)2

(cid:17)

,

(cid:104)P 2, X0 − ˆXrt(cid:105) ≥

n2(λ∗(B))2
4ν2(1 + α0)2 ,

that is, (cid:104)P 2, X0 − ˆXrt(cid:105) = ΩP (n2ρ2).

Now

|(cid:104)diag(P 2), X0 − ˆXrt(cid:105)| ≤ 2 max

(P 2)iitrace(X0) = O(nρ2).

i

(21)

(22)

By Lemma 19, with a union bound since r is ﬁxed,

|(cid:104) ˆS − S, ˆXrt(cid:105)| = OP (nρ(cid:112)log n)

max
rt<r

We have the desired inequality.

Proposition 21. For a sequence of overﬁtting normalized clustering matrix { ˆXrt}r<rt≤rT
with high probability,

independent of A,

max
r<rt≤rT

(cid:104)A2 − diag(A2), ˆXrt(cid:105) ≤ (cid:104)A2 − diag(A2), X0(cid:105) + O(nρ(cid:112)log n)

.

Proof. First note by a similar argument as (20),

(cid:104)P 2, X0 − ˆXrt(cid:105) = trace(Λ2) − trace(Λ2 ˆQT ˆQ) ≥ 0.

Since ˆXrt and A are independent, by an argument similar to (22) and Lemma 19,

max
r<rt≤rT

(cid:104)A2 − diag(A2), ˆXrt(cid:105) ≤ (cid:104)A2 − diag(A2), X0(cid:105) + O(nρ(cid:112)log n)

(23)

(24)

w.h.p.

Next we derive the estimation error by considering the parameter estimates recovered by SPACL (Mao
et al. , 2017). For notational convenience, we ﬁrst derive some bounds for ˆΘ and ˆB estimated from the matrix
A, with the understanding that the same asymptotic bounds apply to estimates obtained from the training
graph provided the split is random and the number of training nodes is Θ(n). The next lemma states the
parameter estimation error from Mao et al. (2017).

23

Lemma 22. There exists a permutation matrix Π, such that, with probability larger than 1 − O(rn−2),

∆1 :=

(cid:13)
ˆΘ − ΘΠ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

= O

(cid:18) (log n)1+ξ
√

(cid:19)

,

∆2 :=

(cid:13)
ˆB − ΠT BΠ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

= O(

(log n)1+ξ).

ρ
(cid:114) ρ
n

(25)

(26)

Proof. These bounds follow directly from Corollary 3.7 of Mao et al. (2017), with α, r, λ∗(B/ρ) all being
constant.

In what follows, we omit the permutation matrix Π to simplify notation. If Π is not the identity matrix,
we can always redeﬁne Θ as ΘΠ, and B as ΠT BΠ. This would not aﬀect the results, since we want to prove
bounds on normalized clustering matrices where Π always cancels out, i.e., X = ΘΠ((ΘΠ)T ΘΠ)−1(ΘΠ)T =
Θ(ΘT Θ)−1ΘT .

We are interested in bounding the estimation error in ˆH = ˆB−1( ˆΘT ˆΘ)−1 ˆΘT . In order to build up the

bound, we will make repeated use of the following two facts.

Fact 23. For general matrices C, ˆC, D, ˆD,

(cid:13)
ˆC ˆD − CD
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

≤

(cid:13)
(cid:13)( ˆC − C)( ˆD − D)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)( ˆC − C)D
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)C( ˆD − D)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

Proof. The proof follows directly from expansion and the triangle inequality,

ˆC ˆD − CD = ( ˆC − C)( ˆD − D) + ( ˆC − C)D + C( ˆD − D).

Fact 24. For a general matrices C, ˆC, assume

ˆC −1 − C −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

Proof. First decompose

(cid:13)
(cid:13)

< 1, then

(cid:13)(C − ˆC)C −1(cid:13)
(cid:13)
(cid:13)F
(cid:13)(C − ˆC)C −1(cid:13)
(cid:13)C −1(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)F
(cid:13)
(cid:13)(C − ˆC)C −1
(cid:13)

1 −

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

≤

ˆC −1 − C −1 = ˆC −1CC −1 − C −1 = ( ˆC −1C − I)C −1 = ˆC −1(C − ˆC)C −1.

(27)

Taking Frobenius norms,

ˆC −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

Rearranging,

≤ (cid:13)

(cid:13)C −1(cid:13)

(cid:13)F +

ˆC −1(C − ˆC)C −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

≤ (cid:13)

(cid:13)C −1(cid:13)

(cid:13)F +

ˆC −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

(cid:13)
(cid:13)

(cid:13)(C − ˆC)C −1(cid:13)
(cid:13)
(cid:13)F

.

(cid:13)
ˆC −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

(cid:13)C −1(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)(C − ˆC)C −1
(cid:13)

.

(cid:13)
(cid:13)
(cid:13)F

1 −

Applying this to (27),

(cid:13)
ˆC −1 − C −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

(cid:13)
ˆC −1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)

(cid:13)(C − ˆC)C −1(cid:13)
(cid:13)
(cid:13)F

≤

(cid:13)
(cid:13)

(cid:13)(C − ˆC)C −1(cid:13)
(cid:13)C −1(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)F
(cid:13)
(cid:13)(C − ˆC)C −1
(cid:13)

1 −

(cid:13)
(cid:13)
(cid:13)F

.

Next we have a lemma bounding the error in estimating the quantity H = B−1(ΘT Θ)−1ΘT .

24

Lemma 25. Let H = B−1(ΘT Θ)−1ΘT , ˆH = ˆB−1( ˆΘT ˆΘ)−1 ˆΘT , then w.h.p.

(cid:13)
(cid:13)H − ˆH
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

= O

(cid:18) (log n)1+ξ
nρ3/2
(cid:18) 1
√

.

(cid:19)

(cid:19)

(28)

(cid:107)H(cid:107)F = O

nρ

Proof. We build up the estimator of H step by step by repeatedly using Facts 23 and 24. Denote F1 =
(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)

(cid:13)B−1(cid:13)

(cid:13)F , and F2 = (cid:13)

(cid:13)F , by Lemma 3.6 in Mao et al. (2017),
(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)

(cid:13)op = OP (1/n),

F1 ≤

r0

√

and

√

F2 ≤

(cid:13)B−1(cid:13)
r (cid:13)

(cid:13)op = O(1/ρ).

First, applying Fact 23,

(cid:13)
ˆΘT ˆΘ − ΘT Θ
(cid:13)
(cid:13)
(cid:13)
ˆΘT ˆΘ − ΘT Θ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)F

(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)
(cid:13)F

≤ ∆2

1 + 2 (cid:107)Θ(cid:107)F ∆1

≤ (∆2

= OP

1 + 2 (cid:107)Θ(cid:107)F ∆1)F1
(cid:18) (log n)2+2ξ
nρ

(cid:19)

+ OP

(29)

(30)

(cid:18) (log n)1+ξ
√

nρ

(cid:19)

= OP

(cid:18) (log n)1+ξ
√

nρ

(cid:19)

,

using Lemma 22 and eq (29). Thus for large n, (cid:13)
we have

(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)F

(cid:13)
ˆΘT ˆΘ − ΘT Θ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

< 1/2. Then using Fact 24,

(cid:13)
(cid:13)

(cid:13)( ˆΘT ˆΘ)−1 − (ΘT Θ)−1(cid:13)
(cid:13)
(cid:13)F

≤

≤

(cid:13)
(cid:13)

(cid:13)((ΘT Θ) − ( ˆΘT ˆΘ))(ΘT Θ)−1(cid:13)
(cid:13)
(cid:13)F

1 −

(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)((ΘT Θ) − ( ˆΘT ˆΘ))(ΘT Θ)−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)(ΘT Θ)−1(cid:13)
(cid:13)
2
(cid:13)((ΘT Θ) − ( ˆΘT ˆΘ))
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
F
(cid:13)
(cid:13)
(cid:13)((ΘT Θ) − ( ˆΘT ˆΘ))(ΘT Θ)−1
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)((ΘT Θ) − ( ˆΘT ˆΘ))
(cid:13)
(cid:13)
(cid:13)F

(cid:13)(ΘT Θ)−1(cid:13)
2
(cid:13)
F

1 −

(cid:13)
(cid:13)
(cid:13)F

≤ 2 (cid:13)

≤ (∆2

1 + 2 (cid:107)Θ(cid:107)F ∆1)F 2

1 = OP

(cid:18) (log n)1+ξ
n3/2ρ1/2

(cid:19)
.

(31)

Similarly using Lemma 22 and eq (30), by noting that

(cid:13)
(cid:13)B−1(cid:13)
(cid:13)F

(cid:13)
(cid:13)(B − ˆB)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

= ∆2F2 = O(

(log n)1+ξ
√
nρ

) < 1/2

for large n w.h.p.,

(cid:13)
ˆB−1 − B−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

(cid:13)
(cid:13)

(cid:13)(B − ˆB)B−1(cid:13)
(cid:13)B−1(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)F
(cid:13)
(cid:13)
(cid:13)(B − ˆB)B−1
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)(B − ˆB)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)B−1(cid:13)
2
(cid:13)
F

1 −

≤ 2 (cid:13)

(cid:18) (log n)1+ξ
n1/2ρ3/2

(cid:19)

(32)

≤ 2∆2F 2

2 = OP

25

using Fact 24.

Next applying Fact 23 to G := B−1(ΘT Θ)−1 and its estimate ˆG := ˆB−1( ˆΘT ˆΘ)−1,

(cid:13)
(cid:13)
(cid:13)G − ˆG
(cid:13)
(cid:13)
(cid:13)F
ˆB−1 − B−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

≤

≤2∆2F 2

2 F1 + (2∆2F 2

=OP

(cid:18) (log n)1+ξ
(nρ)3/2

(cid:19)

,

F1 + ((cid:13)

(cid:13)
(cid:13)

(cid:13)B−1(cid:13)
2 + F2)(∆2

(cid:13)B−1 − ˆB−1(cid:13)
(cid:13)
(cid:13)F +
(cid:13)F
1 + 2 (cid:107)Θ(cid:107)F ∆1)F 2

1

)

(cid:13)
(cid:13)

(cid:13)( ˆΘT ˆΘ)−1 − (ΘT Θ)−1(cid:13)
(cid:13)
(cid:13)F

using Eqs (29)-(32).

(cid:13)
(cid:13)H − ˆH
(cid:13)

Finally, since H = GΘT , and ˆH = ˆG ˆΘT ,
(cid:13)
(cid:13)
(cid:13)
(cid:13)G − ˆG
ˆG − G
(cid:13)
(cid:13)
(cid:13)
(cid:107)Θ(cid:107)F + ((cid:107)G(cid:107)F +
(cid:13)F
(cid:13)
√
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)G − ˆG
(cid:13)G − ˆG
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)F
(cid:18) (log n)1+ξ
nρ3/2

+ (F1F2 +
(cid:19)

= OP

(cid:13)
(cid:13)
(cid:13)F

≤

≤

n

,

(cid:13)
(cid:13)
(cid:13)F

)

(cid:13)
ˆΘ − Θ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

)∆1

and

(cid:107)G(cid:107)2

F = tr((ΘT Θ)−1(ΘT Θ)−1(BBT )−1)

≤ (cid:13)
(cid:13)(ΘT Θ)−1(cid:13)
2
op tr((BBT )−1)
(cid:13)
= OP (1/n2)F 2
2

by Eq (29), (cid:107)H(cid:107)F = OP ( 1√

nρ ) follows.

Lemma 26. Consider applying SPACL on the training graph A11 to obtain ( ˆΘ11)T and ˆB, and use regression
in MATR-CV to estimate membership matrix, i.e., ( ˆΘ22)T = ˆB−1(( ˆΘ11)T ˆΘ11)−1( ˆΘ11)T A12 := ˆHA12, then
ˆΘ22 − Θ22(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

(cid:16) (log n)1+ξ
√

w.h.p.

= O

(cid:17)

ρ

Proof. Since (Θ22)T = HΘ11B(Θ22)T , where H := B−1((Θ11)T Θ11)−1(Θ11)T ,

( ˆΘ22)T − (Θ22)T = ˆHA12 − HΘ11B(Θ22)T

= ˆHA12 − ˆHΘ11B(Θ22)T + ˆHΘ11B(Θ22)T − HΘ11B(Θ22)T
= ˆH(A12 − Θ11B(Θ22)T ) + ( ˆH − H)Θ11B(Θ22)T
= H(A12 − P 12)
(cid:123)(cid:122)
(cid:125)
Q1

+ ( ˆH − H)(A12 − P 12)
(cid:123)(cid:122)
(cid:125)
Q2

+ ( ˆH − H)P 12
(cid:123)(cid:122)
(cid:125)
Q3

(cid:124)

(cid:124)

(cid:124)

.

For Q1,

by Lemma 25. For Q2,

(cid:107)Q1(cid:107)F ≤ (cid:13)

(cid:13)A12 − P 12(cid:13)
√

= OP (

nρ)OP (

(cid:13)op (cid:107)H(cid:107)F
1
√
nρ

) = OP (1/

ρ)

√

(cid:107)Q2(cid:107)F ≤ (cid:13)

(cid:13)A12 − P 12(cid:13)
√

(cid:13)op

= OP (

nρ)OP

(cid:13)
(cid:13)
ˆH − H
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:19)
(cid:18) (log n)1+ξ
nρ3/2

= OP

(cid:18) (log n)1+ξ
√

nρ

(cid:19)

,

26

by Lemma 25. Finally for Q3,

(cid:107)Q3(cid:107)F ≤ (cid:13)

(cid:13)P 12(cid:13)
(cid:13)F

The above arguments lead to

= OP (nρ)OP

(cid:13)
ˆH − H
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F
(cid:18) (log n)1+ξ
nρ3/2

(cid:19)

= OP

(cid:18) (log n)1+ξ
√

ρ

(cid:19)

,

ˆΘ22 − Θ22(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

= OP

(cid:18) (log n)1+ξ
√

ρ

(cid:19)

.

Proposition 27. Given the correct number of clusters r, then with high probability, (cid:104)A22, ˆX 22
O((nρ)3/2(log n)1+ξ).
Proof. First note

r (cid:105) > (cid:104)A22, X 22

0 (cid:105)−

|(cid:104)(A22)2 − diag((A22)2), ˆX 22

r − X 22

0 (cid:105)| ≤ |(cid:104) ˆS22 − S22, ˆX 22
+ |(cid:104)S22, ˆX 22

r − X 22
0 (cid:105)|,

r − X 22

0 (cid:105)|

(33)

where ˆS22 = (A22)2 − diag((A22)2), S22 = (P 22)2 − diag((P 22)2).

Consider SVD of ˆΘ22 = ˆU ˆD ˆV T and Θ22 = U DV T , then ˆX 22

r = ˆU ˆU T , and X 22

0 = U U T . For any

orthogonal matrix O,
(cid:13)
ˆX 22
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)F

r − X 22
0

ˆU ˆU T − U U T (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)( ˆU − U O)( ˆU − U O)T (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
ˆU − U O
ˆU − U O
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Using the Theorem 2 in Yu et al. (2014), we know there exists O such that,

ˆU ˆU T − U O(U O)T (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)( ˆU − U O)(U O)T (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)F

(cid:13)
2
(cid:13)
(cid:13)

+ 2

+ 2

=

≤

≤

F

(cid:13)
ˆU − U O
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

≤

2

ˆΘ22 − Θ22(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
λr(Θ22)

,

where λr(Θ22) is the r-th largest singular value of Θ22. Using Lemma 3.6 in Mao et al.
λr0(Θ22) = Ω(

n). Now by Lemma 26 and Eq (34), w.h.p.

√

(cid:13)
ˆX 22
(cid:13)
(cid:13)
r0

− X 22
0

(cid:13)
(cid:13)
(cid:13)F

= O(

(log n)1+ξ
√
nρ

).

Now in Eq (33),

(34)

(2017), w.h.p,

|(cid:104)S22, ˆX 22

r − X 22

0 (cid:105)| ≤ (cid:13)

(cid:13)S22(cid:13)
(cid:13)F

(cid:13)
ˆX 22
(cid:13)
(cid:13)

r − X 22
0

(cid:13)
(cid:13)
(cid:13)F

= OP ((nρ)3/2(log n)1+ξ),

and

by Lemma 19.

|(cid:104) ˆS22 − S22, ˆX 22

r − X 22

0 (cid:105)| = OP (nρ(cid:112)logn)

Finally we prove Theorem 10.

Proof of Theorem 10. By Propositions 20, 21 and 27,

(cid:15)under = Ω(n2ρ2),

(cid:15)est = O((nρ)3/2(log n)1+ξ),
(cid:15)over = O(nρ(cid:112)log n).
Then the result follows by setting ∆ = O((nρ)3/2(log n)1+ξ).

27

C Detailed parameter settings in experiments and additional re-

sults

Motivating examples in Section 3 (Figure 1)

In Figure 1(a), we generate an adjacency matrix from a SBM model with four communities, each having 50
nodes, and

B =







0.8
0.6
0.4
0.4

0.6
0.8
0.4
0.4

0.4
0.4
0.8
0.6







0.4
0.4
0.6
0.8

.

The visualization of the underlying probability matrix is shown in Figure 4(a).

In Figure 1(b), we consider a four-component Gaussian mixture model, where the means µ1, . . . , µ4 are
generated from Gaussian distributions centered at (0, 0), (0, 0), (5, 5), (10, 10) with covariance 6I, so that the
ﬁrst two clusters are closer to each other than the rest. Then we generate 1000 data points centered at these
means with covariance 0.5I, each point assigned to one of the four clusters independently with probability
42 , 20
42 , 1
( 20
42 ). Finally, we introduce correlation between the two dimensions by multiplying each point by
(cid:20)2 1
(cid:21)
1 2

42 , 1
. A scatter plot example of the datapoints is shown in Figure 4(b).

(a) SBM

(b) Gaussian mixture

Figure 4: Datasets used for Figure 1.

Tuning with SDP-1 (Figure 2 (a)-(b))

Figure 2 (a): We consider graphs generated from a hierarchical SBM with equal sized clusters, where

B = ρ ×







0.8
0.6
0.3
0.3

0.6
0.8
0.3
0.3

0.3
0.3
0.8
0.6







0.3
0.3
0.6
0.8

.

Each cluster has 100 nodes and ρ ranges from 0.2 to 1.

Figure 2 (b): Next, we consider graphs generated from a SBM with the same B matrix, but with
unequal cluster sizes. Cluster 1 and 3 have 100 nodes each, while cluster 2 and 4 have 50 nodes each. ρ
ranges from 0.2 to 1.

Tuning with spectral clustering (Figure 2(c)-(d))

We generate the means µa, a ∈ [3] from d = 20 dimensional Gaussian distribution with covariance 0.01I. To
impose sparsity on each µa, we set all but the ﬁrst two dimensions to 0. To change the level of clustering

28

diﬃculty, we multiply µa with a separation constant c, and a larger c leads to larger separation and easier
clustering. We vary c from 0 to 200. We generate n = 500 samples from each mixture with a constant
covariance matrix (an identity matrix) using Eq 2. For Figure 2 (c), the probabilities of cluster assignment
are equal, while for Figure 2 (d), each point belongs to one of the three clusters with probability ( 20
22 ).
2D projections of the datapoints for the two settings are shown in Figure 5.

22 , 1

22 , 1

(a) Equal sized clusters

(b) Unequal sized clusters

Figure 5: 2D projections of the datapoints for Gaussian mixtures.

Additional ﬁgure for Section 5.2

(a) True clustering

(b) Clustering by MATR

(c) Clustering by DS

(d) Clustering by KNN

(e) Clustering by MST

Figure 6: Visualization of clustering results on handwritten digits dataset.

29

ρ
0.2
0.3
0.4
0.5
0.6

MATR-CV BH ECV
2
2
4
4
4

2
2
2
2
4

2
2
2
2
2

ρ
0.2
0.3
0.4
0.5
0.6

MATR-CV BH ECV
2
2
3
4
4

2
2
2
2
3

2
2
2
2
2

(a) Median number of clusters selected for equal size case

(b) Median number of clusters selected for unequal size case

Table 3: Comparison of model selection results along with ρ for all algorithms.

Tuning with SDP-2 (Figure 3)

Figure 3 (a): We ﬁrst consider graphs generated from a SBM with equal sized clusters, where

B = ρ ×







0.8
0.5
0.3
0.3

0.5
0.8
0.3
0.3

0.3
0.3
0.8
0.5







0.3
0.3
0.5
0.8

.

Each cluster has 100 nodes and 5 ρ’s are selected from 0.2 to 0.6 with even spacing.

Figure 3 (b): Here we consider graphs generated from an unequal-sized SBM , where the B matrix is

the same as above. The clusters have 120, 80, 120, 80 nodes respectively. The same ρ’s as above are used.

Table 3 (a,b): We show the median number of clusters selected by each method as ρ changes. The

ground truth is 4 clusters.

References

Abbe, Emmanuel, & Sandon, Colin. 2015. Recovering communities in the general stochastic block model

without knowing the parameters. Pages 676–684 of: Advances in NIPS.

Abbe, Emmanuel, Bandeira, Afonso S, & Hall, Georgina. 2015. Exact recovery in the stochastic block model.

IEEE Transactions on Information Theory, 62(1), 471–487.

Adamic, Lada A, & Glance, Natalie. 2005. The political blogosphere and the 2004 US election: divided they

blog. Pages 36–43 of: Proceedings of the 3rd international workshop on Link discovery. ACM.

Airoldi, Edoardo M., Blei, David M., Fienberg, Stephen E., & Xing, Eric P. 2008. Mixed Membership

Stochastic Blockmodels. J. Mach. Learn. Res., 9(June), 1981–2014.

Amini, Arash A, Levina, Elizaveta, et al. . 2018. On semideﬁnite relaxations for the block model. Ann.

Statist., 46(1), 149–179.

Bach, Francis R. 2008. Bolasso: model consistent lasso estimation through the bootstrap. Pages 33–40 of:

Proceedings of the 25th international conference on Machine learning. ACM.

Belkin, Mikhail, & Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality Reduction and Data

Representation. Neural Comput., 15(6), 1373–1396.

Bengio, Yoshua. 2000. Gradient-based optimization of hyperparameters. Neural computation, 12(8), 1889–

1900.

Bergstra, James, & Bengio, Yoshua. 2012. Random search for hyper-parameter optimization. Journal of

Machine Learning Research, 13(Feb), 281–305.

Bergstra, James S, Bardenet, Rémi, Bengio, Yoshua, & Kégl, Balázs. 2011. Algorithms for hyper-parameter

optimization. Pages 2546–2554 of: Advances in NIPS.

30

Bickel, Peter J, & Sarkar, Purnamrita. 2016. Hypothesis testing for automated community detection in

networks. JRSSb, 78(1), 253–273.

Birgé, Lucien, & Massart, Pascal. 2001. Gaussian model selection. Journal of the European Mathematical

Society, 3(3), 203–268.

Bozdogan, Hamparsum. 1987. Model selection and Akaike’s Information Criterion (AIC): The general theory

and its analytical extensions. Psychometrika, 52, 345–370.

Breiman, Leo, et al. . 1996. Heuristics of instability and stabilization in model selection. Ann. Statist., 24(6),

2350–2383.

Cai, T Tony, Li, Xiaodong, et al. . 2015. Robust and computationally feasible community detection in the

presence of arbitrary outlier nodes. Ann. Statist., 43(3), 1027–1059.

Chatterjee, Sourav, et al. . 2015. Matrix estimation by universal singular value thresholding. Ann. Statist.,

43(1), 177–214.

Chen, Kehui, & Lei, Jing. 2018. Network cross-validation for determining the number of communities in

network data. JASA, 113(521), 241–251.

Chen, Yudong, Li, Xiaodong, & Xu, Jiaming. 2018. Convexiﬁed modularity maximization for degree-corrected

stochastic block models. Ann. Statist., 46(4), 1573–1602.

Coifman, R. R., Shkolnisky, Y., Sigworth, F. J., & Singer, A. 2008. Graph Laplacian Tomography From

Unknown Random Projections. Trans. Img. Proc., 17(10), 1891–1899.

Drton, Mathias, & Plummer, Martyn. 2017. A Bayesian information criterion for singular models. JRSSb,

79(2), 323–380.

Fan, Jianqing, Fan, Yingying, Han, Xiao, & Lv, Jinchi. 2019. SIMPLE: Statistical Inference on Membership

Proﬁles in Large Networks. arXiv preprint arXiv:1910.01734.

Fang, Yixin, & Wang, Junhui. 2012. Selection of the number of clusters via the bootstrap method. Computa-

tional Statistics & Data Analysis, 56(3), 468–477.

Figueiredo, M. A. T., & Jain, A. K. 2002. Unsupervised learning of ﬁnite mixture models. IEEE Transactions

on PAMI, 24(3), 381–396.

Giné, Evarist, & Koltchinskii, Vladimir. 2006. Empirical graph Laplacian approximation of Laplace–Beltrami
operators: Large sample results. Lecture Notes–Monograph Series, vol. Number 51. Beachwood, Ohio, USA:
Institute of Mathematical Statistics. Pages 238–259.

Girvan, Michelle, & Newman, Mark EJ. 2002. Community structure in social and biological networks.

Proceedings of the national academy of sciences, 99(12), 7821–7826.

Guédon, Olivier, & Vershynin, Roman. 2016. Community detection in sparse networks via Grothendieck’s

inequality. Probability Theory and Related Fields, 165(3), 1025–1049.

Hajek, Bruce, Wu, Yihong, & Xu, Jiaming. 2016. Achieving exact cluster recovery threshold via semideﬁnite

programming. IEEE Transactions on Information Theory, 62(5), 2788–2797.

Han, Xiao, Yang, Qing, & Fan, Yingying. 2019. Universal Rank Inference via Residual Subsampling with

Application to Large Networks. arXiv preprint arXiv:1912.11583.

Hastie, Trevor, Tibshirani, Robert, Friedman, Jerome, & Franklin, James. 2005. The elements of statistical

learning: data mining, inference and prediction. The Mathematical Intelligencer, 27(2), 83–85.

Hein, Matthias. 2006. Uniform Convergence of Adaptive Graph-Based Regularization. Page 50–64 of:

Proceedings of COLT. COLT’06. Berlin, Heidelberg: Springer-Verlag.

31

Hein, Matthias, Audibert, Jean-Yves, & von Luxburg, Ulrike. 2005. From Graphs to Manifolds – Weak and

Strong Pointwise Consistency of Graph Laplacians. Pages 470–485 of: COLT.

Hu, Jianwei, Qin, Hong, Yan, Ting, Zhang, Jingfei, & Zhu, Ji. 2017. Using Maximum Entry-Wise Deviation

to Test the Goodness-of-Fit for Stochastic Block Models. arXiv preprint arXiv:1703.06558.

Jin, Jiashun, Ke, Zheng Tracy, & Luo, Shengming. 2017. Estimating network memberships by simplex vertex

hunting.

Karrer, Brian, & Newman, M. E. J. 2011. Stochastic blockmodels and community structure in networks.

Phys. Rev. E, 83(Jan), 016107.

Keribin, Christine. 2000. Consistent Estimate of the Order of Mixture Models. Sankhy=a, Series A, 62(01),

49–66.

Le, Can M, & Levina, Elizaveta. 2015. Estimating the number of communities in networks by spectral

methods. arXiv preprint arXiv:1507.00827.

Lei, Jing, & Rinaldo, Alessandro. 2015. Consistency of spectral clustering in stochastic block models. Ann.

Statist., 43(1), 215–237.

Lei, Jing, et al. . 2016. A goodness-of-ﬁt test for stochastic block models. Ann. Statist., 44(1), 401–424.

Leroux, Brian G. 1992. Consistent Estimation of a Mixing Distribution. Ann. Statist., 20(3), 1350–1360.

Li, Tianxi, Levina, Elizaveta, & Zhu, Ji. 2016. Network cross-validation by edge sampling. arXiv preprint

arXiv:1612.04717.

Li, Xiaodong, Chen, Yudong, & Xu, Jiaming. 2018. Convex relaxation methods for community detection.

arXiv preprint arXiv:1810.00315.

Lim, Chinghway, & Yu, Bin. 2016. Estimation stability with cross-validation (ESCV). Journal of Computa-

tional and Graphical Statistics, 25(2), 464–492.

Little, Anna, Maggioni, Mauro, & Murphy, James. 2017. Path-Based Spectral Clustering: Guarantees,

Robustness to Outliers, and Fast Algorithms. None, 12.

Löﬄer, Matthias, Zhang, Anderson Y., & Zhou, Harrison H. 2019. Optimality of Spectral Clustering for

Gaussian Mixture Model.

Maaten, Laurens van der, & Hinton, Geoﬀrey. 2008. Visualizing data using t-SNE. Journal of machine

learning research, 9(Nov), 2579–2605.

Maggioni, Mauro, & Murphy, James M. 2018. Learning by Unsupervised Nonlinear Diﬀusion. ArXiv,

abs/1810.06702.

Mao, Xueyu, Sarkar, Purnamrita, & Chakrabarti, Deepayan. 2017. Estimating Mixed Memberships with

Sharp Eigenvector Deviations. ArXiv, abs/1709.00407.

Mao, Xueyu, Sarkar, Purnamrita, & Chakrabarti, Deepayan. 2018. Overlapping clustering models, and one

(class) SVM to bind them all. Pages 2126–2136 of: Advances in Neurips.

Meila, Marina. 2018. How to tell when a clustering is (approximately) correct using convex relaxations. Pages

7407–7418 of: Advances in Neural Information Processing Systems.

Meinshausen, Nicolai, & Bühlmann, Peter. 2010. Stability selection. Journal of the Royal Statistical Society:

Series B (Statistical Methodology), 72(4), 417–473.

Mixon, Dustin G, Villar, Soledad, & Ward, Rachel. 2017. Clustering subgaussian mixtures by semideﬁnite

programming. Information and Inference: A Journal of the IMA, 6(4), 389–415.

32

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay,
E. 2011. Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research, 12, 2825–2830.

Peng, Jiming, & Wei, Yu. 2007. Approximating K-means-type Clustering via Semideﬁnite Programming.

SIAM J. on Optimization, 18(1), 186–205.

Perry, Amelia, & Wein, Alexander S. 2017. A semideﬁnite program for unbalanced multisection in the
stochastic block model. Pages 64–67 of: 2017 International Conference on Sampling Theory and Applications
(SampTA). IEEE.

Rousseeuw, Peter J. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis.

Journal of Computational and Applied Mathematics, 20, 53 – 65.

Schiebinger, Geoﬀrey, Wainwright, Martin J., & Yu, Bin. 2015. The geometry of kernelized spectral clustering.

Ann. Statist., 43(2), 819–846.

Shao, Jun. 1993. Linear model selection by cross-validation. Journal of the American statistical Association,

88(422), 486–494.

Shi, Tao, Belkin, Mikhail, & Yu, Bin. 2008. Data spectroscopy: Learning mixture models using eigenspaces
of convolution operators. Pages 936–943 of: Proceedings of the 25th international conference on Machine
learning. ACM.

Skala, Matthew. 2013. Hypergeometric tail inequalities: ending the insanity. arXiv preprint arXiv:1311.5939.

Snoek, Jasper, Larochelle, Hugo, & Adams, Ryan P. 2012. Practical bayesian optimization of machine learning

algorithms. Pages 2951–2959 of: Advances in neural information processing systems.

Srivastava, Prateek R., Sarkar, Purnamrita, & Hanasusanto, Grani A. 2019. A Robust Spectral Clustering

Algorithm for Sub-Gaussian Mixture Models with Outliers.

Stone, Mervyn. 1974. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal

Statistical Society: Series B (Methodological), 36(2), 111–133.

Tibshirani, Robert, Walther, Guenther, & Hastie, Trevor. 2001. Estimating the number of clusters in a data

set via the gap statistic. JRSSb, 63(2), 411–423.

Tibshirani, Robert, Wainwright, Martin, & Hastie, Trevor. 2015. Statistical learning with sparsity: the lasso

and generalizations. Chapman and Hall/CRC.

Von Luxburg, Ulrike. 2007. A tutorial on spectral clustering. Statistics and computing, 17(4), 395–416.

von Luxburg, Ulrike. 2007. A tutorial on spectral clustering. Statistics and Computing, 17(4), 395–416.

Von Luxburg, Ulrike, et al. . 2010. Clustering stability: an overview. Foundations and Trends® in Machine

Learning, 2(3), 235–274.

Wang, Junhui. 2010. Consistent selection of the number of clusters via crossvalidation. Biometrika, 97(4),

893–904.

Wang, Y. X. Rachel, & Bickel, Peter J. 2017. Likelihood-based model selection for stochastic block models.

Ann. Statist., 45(2), 500–528.

Wasserman, Larry. 2006. All of nonparametric statistics. Springer Science & Business Media.

Wasserman, Larry, & Roeder, Kathryn. 2009. High dimensional variable selection. Annals of statistics,

37(5A), 2178.

Yan, Bowei, & Sarkar, Purnamrita. 2019. Covariate Regularized Community Detection in Sparse Graphs.

JASA theory and methods.

33

Yan, Bowei, Sarkar, Purnamrita, & Cheng, Xiuyuan. 2017. Provable estimation of the number of blocks in

block models. arXiv preprint arXiv:1705.08580.

Yang, Yuhong, et al. . 2007. Consistency of cross validation for comparing regression procedures. Ann.

Statist., 35(6), 2450–2473.

Young, Stephen J, & Scheinerman, Edward R. 2007. Random dot product graph models for social networks.

Pages 138–149 of: International Workshop on Algorithms and Models for the Web-Graph. Springer.

Yu, Y., Wang, T., & Samworth, R. J. 2014. A useful variant of the Davis–Kahan theorem for statisticians.

Biometrika, 102(2), 315–323.

Zhang, Ping. 1993. Model selection via multifold cross validation. Ann. Statist., 299–313.

34

