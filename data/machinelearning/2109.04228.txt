Coordinate Descent Methods for DC Minimization

Ganzhao Yuan * 1

Abstract
Difference-of-Convex (DC) minimization, refer-
ring to the problem of minimizing the difference
of two convex functions, has been found rich ap-
plications in statistical learning and studied exten-
sively for decades. However, existing methods
are primarily based on multi-stage convex relax-
ation, only leading to weak optimality of critical
points. This paper proposes a coordinate descent
method for minimizing DC functions based on se-
quential nonconvex approximation. Our approach
iteratively solves a nonconvex one-dimensional
subproblem globally, and it is guaranteed to con-
verge to a coordinate-wise stationary point. We
prove that this new optimality condition is always
stronger than the critical point condition and the
directional point condition when the objective
function is weakly convex. For comparisons, we
also include a naive variant of coordinate descent
methods based on sequential convex approxima-
tion in our study. When the objective function
satisﬁes an additional regularity condition called
sharpness, coordinate descent methods with an
appropriate initialization converge linearly to the
optimal solution set. Also, for many applica-
tions of interest, we show that the nonconvex
one-dimensional subproblem can be computed
exactly and efﬁciently using a breakpoint search-
ing method. Finally, we have conducted extensive
experiments on several statistical learning tasks
to show the superiority of our approach.

2
2
0
2

b
e
F
8

]

C
O
.
h
t
a
m

[

2
v
8
2
2
4
0
.
9
0
1
2
:
v
i
X
r
a

1. Introduction

This paper mainly focuses on the following DC minimiza-
tion problem (‘(cid:44)’ means deﬁne):

¯x ∈ arg min
x∈Rn

F (x) (cid:44) f (x) + h(x) − g(x).

(1)

Throughout this paper, we make the following assumptions
on Problem (1). (i) f (·) is convex and continuously dif-

1Peng Cheng Laboratory, China. Correspondence to: Ganzhao

Yuan <yuangzh@pcl.ac.cn>.

ferentiable, and its gradient is coordinate-wise Lipschitz
continuous with constant ci ≥ 0 that (Nesterov, 2012; Beck
& Tetruashvili, 2013):

f (x + ηei) ≤ f (x) + (cid:104)∇f (x), ηei) +

ci
2

(cid:107)ηei(cid:107)2
2

(2)

∀x, η, i = 1, ..., n. Here c ∈ Rn, and ei ∈ Rn is an indica-
tor vector with one on the i-th entry and zero everywhere
else. (ii) h(·) is convex and coordinate-wise separable with
h(x) = (cid:80)n
i=1 hi(xi). Typical examples of h(x) include
the bound constrained function and the (cid:96)1 norm function.
(iii) g(·) is convex and its associated proximal operator:

min
η∈R

p(η) (cid:44) a
2

η2 + bη + hi(x + ηei) − g(x + ηei),

(3)

can be computed exactly and efﬁciently for given a ∈ R+,
b ∈ R and i ∈ {1, ..., n}. We remark that g(·) is nei-
ther necessarily smooth nor coordinate-wise separable,
and typical examples of g(x) are the (cid:96)p norm function
g(x) = (cid:107)Ax(cid:107)p with p = {1, 2, ∞}, the RELU function
g(x) = (cid:107) max(0, Ax)(cid:107)1, and the top-s norm function
g(x) = (cid:80)s
i=1 |x[i]|. Here A ∈ Rm×n is an arbitrary given
matrix and x[i] denotes the ith largest component of x in
magnitude. (iv) F (x) is bounded from below.

DC programming. DC Programming/minimization is an
extension of convex maximization over a convex set (Tao &
An, 1997). It is closely related to the concave-convex pro-
cedure and alternating minimization in the literature. The
class of DC functions is very broad, and it includes many
important classes of nonconvex functions, such as twice con-
tinuously differentiable function on compact convex set and
multivariate polynomial functions (Ahmadi & Hall, 2018).
DC programs have been mainly considered in global opti-
mization and some algorithms have been proposed to ﬁnd
global solutions to such problem (Horst & Thoai, 1999;
Horst & Tuy, 2013). Recent developments on DC program-
ming primarily focus on designing local solution methods
for some speciﬁc DC programming problems. For example,
proximal bundle DC methods (Joki et al., 2017), double
bundle DC methods (Joki et al., 2018), inertial proximal
methods (Maing´e & Moudaﬁ, 2008), and enhanced proxi-
mal methods (Lu & Zhou, 2019) have been proposed. DC
programming has been applied to solve a variety of statisti-
cal learning tasks, such as sparse PCA (Sriperumbudur et al.,
2007; Beck & Teboulle, 2021), variable selection (Gotoh

 
 
 
 
 
 
Coordinate Descent Methods for DC Minimization

et al., 2018; Gong et al., 2013), single source localization
(Beck & Hallak, 2020), and piecewise linear programming
(Beck & Hallak, 2020).

Coordinate descent methods. Coordinate descent is a
popular method for solving large-scale optimization prob-
lems. Advantages of this method are that compared with
the full gradient descent method, it enjoys faster conver-
gence (Tseng & Yun, 2009; Xu & Yin, 2013), avoids tricky
parameters tuning, and allows for easy parallelization (Liu
et al., 2015). It has been well studied for convex optimiza-
tion such as Lasso (Tseng & Yun, 2009), support vector
machines (Hsieh et al., 2008), nonnegative matrix factoriza-
tion (Hsieh & Dhillon, 2011), and the PageRank problem
(Nesterov, 2012). Its convergence and worst-case complex-
ity are well investigated for different coordinate selection
rules such as cyclic rule (Beck & Tetruashvili, 2013), greedy
rule (Hsieh & Dhillon, 2011), and random rule (Lu & Xiao,
2015; Richt´arik & Tak´avc, 2014). It has been extended
to solve many nonconvex problems such as penalized re-
gression (Breheny & Huang, 2011; Deng & Lan, 2020),
eigenvalue complementarity problem (Patrascu & Necoara,
2015), (cid:96)0 norm minimization (Beck & Eldar, 2013; Yuan
et al., 2020), resource allocation problem (Necoara, 2013),
leading eigenvector computation (Li et al., 2019), and sparse
phase retrieval (Shechtman et al., 2014).

Iterative majorization minimization. Iterative majoriza-
tion / upper-bound minimization is becoming a standard
principle in developing nonlinear optimization algorithms.
Many surrogate functions such as Lipschitz gradient surro-
gate, proximal gradient surrogate, DC programming surro-
gate, variational surrogate, saddle point surrogate, Jensen
surrogate, quadratic surrogate, cubic surrogate have been
considered, see (Mairal, 2013; Razaviyayn et al., 2013). Re-
cent efforts mainly focus on extending this principle to the
coordinate update, incremental update, and stochastic up-
date settings. However, all these methods are mainly based
on multiple-stage convex relaxation, while ours is based on
sequential nonconvex approximation. Thanks to the coor-
dinate update strategy, we can solve the one-dimensional
subproblem globally using a novel exhaustive breakingpoint
searching method even when g(·) is nonseparable and nons-
mooth.

Theory for Nonconvex Optimization.
Substantial
progress has been made recently on developing new the-
ory nonconvex algorithms. We pay speciﬁc attention to
two contrasting approaches. (i) Strong optimality. The ﬁrst
approach is to achieve stronger optimality guarantees for
nonconvex problems. For smooth optimization, canonical
gradient methods only converge to a ﬁrst-order stationary
point, recent works aim at ﬁnding a second-order stationary
point by using the negative curvature information of the
objective function or by adding randomness into its gradient

(Jin et al., 2017). For cardinality minimization, the work of
(Beck & Eldar, 2013; Yuan et al., 2020) introduces a new
optimality condition of (block) coordinate stationary point
which is stronger than that of the Lipschitz stationary point
(Yuan et al., 2017). (ii) Strong convergence. The second
approach is to provide convergence analysis for nonconvex
problems. The work of (Jin et al., 2017) establishes a sharp
global convergence rate for nonconvex matrix factorization
using a regularity condition which is more general and is
directly implied by standard smooth and strongly convex
condition. The work of (Attouch et al., 2010) establishes
the convergence rate for general nonsmooth problems by
imposing Kurdyka-Łojasiewicz inequality assumption of
the objective function. The work of (Davis et al., 2018;
Davis & Grimmer, 2019; Li et al., 2021) establish local
linear convergence rates for the subgradient method under
the assumption that the objective function is weakly convex
and satisﬁes a sharpness condition. Inspired by these works,
we prove that the proposed coordinate descent method has
strong optimality guarantees and convergence guarantees.

Contributions. The contributions of this paper are as fol-
lows: (i) We propose a new coordinate descent method for
minimizing DC functions based on sequential nonconvex ap-
proximation (See Section 4). (ii) We show that our method
converge to a coordinate-wise stationary point, and prove
that this optimality condition is always stronger than the
critical point condition and the directional point condition
when the objective function is weakly convex. When the
objective function satisﬁes an additional regularity condi-
tion called sharpness, we show that the coordinate descent
methods with an appropriate initialization converge linearly
to the optimal solution set (See Section 5). (iii) We show
that, for many applications of interest, the one-dimensional
subproblem can be computed exactly and efﬁciently using
a breakpoint searching method (See Section 6). (iv) We
have conducted extensive experiments on some statistical
learning tasks to show the superiority of our approach (See
Section 7). (v) We also provide several important discus-
sions and extensions of our proposed method (See Section
D in the Appendix).

Notations. Vectors are denoted by boldface lowercase
letters, and matrices by boldface uppercase letters. The
Euclidean inner product between x and y is denoted by
(cid:104)x, y(cid:105) or xT y. We denote (cid:107)x(cid:107) = (cid:107)x(cid:107)2 = (cid:112)(cid:104)x, x(cid:105). xi
denotes the i-th element of the vector x. E[·] represents
the expectation of a random variable. (cid:12) and ÷ denote
the element-wise multiplication and division between two
vectors, respectively. For any extended real-valued func-
tion h : Rn → (−∞, +∞], The set of all subgradients
of h at x is deﬁned as ∂h(x) = {g ∈ Rn : h(y) ≥
h(x) + (cid:104)g, y − x(cid:105)}, and the conjugate of h(x) is deﬁned
as h∗(x) (cid:44) maxy{(cid:104)y, x(cid:105) − h(y)}. diag(c) is a diago-
nal matrix with c as the main diagonal entries. We deﬁne

Coordinate Descent Methods for DC Minimization

c = (cid:80)

i cid2

(cid:107)d(cid:107)2
i . sign(·) is the signum function. I is the
identity matrix of suitable size. The directional derivative of
F (·) at a point x in its domain along a direction d is deﬁned
as: F (cid:48)(x; d) (cid:44) limt↓0

1
t (F (x + td) − F (x)).

2. Motivating Applications

1

2 (cid:107)σ(Gx) − y(cid:107)2

the problem of recovering a signal x by solving: ¯x =
2. When σ(z) = max(0, z),
arg minx∈Rn
this problem is connected with the one-hidden-layer ReLU
networks (Zhang et al., 2019); when σ(z) = |z|, this prob-
lem is related to the amplitude-base phase retrieval problem
(Cand`es et al., 2015). When y ≥ 0, we have the following
equivalent DC program:

A number of statistical learning models can be formulated
as Problem (1), which we present some instances below.

min
x∈Rn

1

2 (cid:107)σ(Gx)(cid:107)2

2 − (cid:104)1, σ(diag(y)G)x)(cid:105) + 1

2 (cid:107)y(cid:107)2
2.

(7)

• (cid:96)p Norm Generalized Eigenvalue Problem. Given arbi-
trary data matrices G ∈ Rm×n and Q ∈ Rn×n with Q (cid:31) 0,
it aims at solving the following problem:

¯v = arg max

v

(cid:107)Gv(cid:107)p, s.t. vT Qv = 1.

(4)

with p ≥ 1. When p = 2 and Q = I, Problem (4) is
the classical Principle Component Analysis (PCA) prob-
lem; when p = 1 and Q = I, Problem (4) reduces
to the (cid:96)1 PCA problem (Kim & Klabjan, 2019); when
p = 2, Problem (4) boils down to the generalized eigen-
value problem, and it has several equivalent formulations:
xT GT Gx
xT Qx = max{(cid:37) : (cid:37)Q − GT G (cid:23)
¯vT GT G¯v = maxx
0} = λmax(Q−1/2GT GQ−1/2), with λmax(X) being the
largest eigenvalue of X. Using the Lagrangian dual, we
have the following equivalent unconstrained problem:

¯x = arg min

x

α

2 xT Qx − (cid:107)Gx(cid:107)p,

for any given α > 0. The optimal solution to Problem (4)
can be computed as ¯v = ±¯x · (¯xT Q¯x)− 1
2 . Refer to Section
D.1 in the appendix for a detailed discussion.

• Approximate Sparse/Binary Optimization. Given a
channel matrix G ∈ Rm×n, a structured signal x is trans-
mitted through a communication channel, and received as
y = Gx + v, where v is the Gaussian noise. If x has
s-sparse or binary structure, one can recover x by solving
the following optimization problem (Gotoh et al., 2018; Jr.,
1972):

1

2 (cid:107)Gx − y(cid:107)2

minx
1
2 (cid:107)Gx − y(cid:107)2

2, s.t. (cid:107)x(cid:107)0 ≤ s,
2, s.t. x ∈ {−1 + 1}n.

or minx

Here, (cid:107)·(cid:107)0 is the number of non-zero components. Using the
equivalent variational reformulation of the (cid:96)0 (pseudo) norm:
(cid:107)x(cid:107)0 ≤ s ⇔ (cid:107)x(cid:107)1 = (cid:80)s
i=1 |x[i]| and the binary constraint:
{−1, +1}n ⇔ {x| − 1 ≤ x ≤ 1, (cid:107)x(cid:107)2
2 = n}, one can
solve the following approximate sparse/binary optimization
problem (Gotoh et al., 2018; Yuan & Ghanem, 2017):

minx

1

2 (cid:107)Gx − y(cid:107)2

min(cid:107)x(cid:107)∞≤1

2 + ρ((cid:107)x(cid:107)1 − (cid:80)s
√

1

2 (cid:107)Gx − y(cid:107)2

2 + ρ(

i=1 |x[i]|)
n − (cid:107)x(cid:107)).

(5)

(6)

3. Related Work

We now present some related DC minimization algorithms.

(i) Multi-Stage Convex Relaxation (MSCR)(Zhang, 2010;
Bi et al., 2014).
It solves Problem (1) by generating a
sequence {xt} as

xt+1 = arg min

x

f (x) + h(x) − (cid:104)x − xt, gt(cid:105)

(8)

where gt ∈ ∂g(xt). Note that Problem (8) is convex and
can be solved via standard proximal gradient method. The
computational cost of MSCR could be expensive for large-
scale problems, since it is K times that of solving Problem
(8) with K being the number of outer iterations.

(ii) Proximal DC algorithm (PDCA) (Gotoh et al., 2018).
To alleviate the computational issue of solving Problem (8),
PDCA exploits the structure of f (·) and solves Problem (1)
by generating a sequence {xt} as:

xt+1 = arg min

x

Q(x, xt) + h(x) − (cid:104)x − xt, gt(cid:105)

where Q(x, xt) (cid:44) f (xt)+(cid:104)∇f (xt), x−xt(cid:105)+ L
and L is the Lipschitz constant of ∇f (·).

2 (cid:107)x−xt(cid:107)2
2,

(iii) Toland’s duality method (Toland, 1979; Beck &
Teboulle, 2021). Assuming g(x) has the following structure
g(x) = ¯g(Ax) = maxy{(cid:104)Ax, y(cid:105)− ¯g∗(y)}. This approach
rewrites Problem (1) as the following equivalent problem
using the conjugate of g(x): minx miny f (x) + h(x) −
(cid:104)y, Ax(cid:105) + ¯g∗(y). Exchanging the order of minimization
yields the equivalent problem: miny minx f (x) + h(x) −
(cid:104)y, Ax(cid:105) + ¯g∗(y). The set of minimizers of the inner prob-
lem with respect to x is ∂h∗(AT y) + ∇f ∗(AT y), and the
minimal value is −f ∗(AT y)−h∗(AT y)+ ¯g∗(y). We have
the Toland-dual problem which is also a DC program:

¯g∗(y) − f ∗(AT y) − h∗(AT y)

(9)

min
y

This method is only applicable when the minimization prob-
lem with respect to x is simple so that it has an analytical
solution. Toland’s duality method could be useful if one of
the subproblems is easier to solve than the other.

• Generalized Linear Regression. Given a sensing matrix
G ∈ Rm×n and measurements y ∈ Rm, it deals with

(iv) Subgradient descent method (SubGrad). It uses the iter-
ation xt+1 = P(xt − ηtgt), where gt ∈ ∂F (xt), ηt is the

Coordinate Descent Methods for DC Minimization

step size, and P is the projection operation on some convex
set. This method has received much attention recently due to
its simplicity (Zhang et al., 2019; Davis et al., 2018; Davis
& Grimmer, 2019; Li et al., 2021).

4. Coordinate Descent Methods for DC

Minimization

This section presents a new Coordinate Descent (CD)
method for solving Problem (1), which is based on Sequen-
tial NonConvex Approximation (SNCA). For comparisons,
we also include a naive variant of CD methods based on Se-
quential Convex Approximation (SCA) in our study. These
two methods are denoted as CD-SNCA and CD-SCA, re-
spectively.

Coordinate descent is an iterative algorithm that succes-
sively minimizes the objective function along coordinate
directions. In the t-th iteration, we minimize F (·) with re-
spect to the it variable while keeping the remaining (n − 1)
variables {xt
j}j(cid:54)=it ﬁxed. This is equivalent to performing
the following one-dimensional search along the it-th coor-
dinate:

Algorithm 1 Coordinate Descent Methods for Minimiz-
ing DC functions using SNCA or SCA strategy.

Input: an initial feasible solution x0, θ > 0. Set t = 0.
while not converge do

(S1) Use some strategy to ﬁnd a coordinate it ∈
{1, ..., n} for the t-th iteration.
(S2) Solve the following nonconvex or convex subprob-
lem.
• Option I: Sequential NonConvex Approximation
(SNCA) strategy.

¯ηt = arg min

Sit(xt, η) + hit(xt + ηeit)

η
−g(xt + ηeit) + θ

2 (cid:107)(xt + ηeit) − xt(cid:107)2

2

(10)

• Option II: Sequential Convex Approximation (SCA)
strategy.

¯ηt = arg min

Sit(xt, η) + hit(xt + ηei)

η
+Git(xt, η) + θ

2 (cid:107)(xt + ηeit) − xt(cid:107)2

2

(11)

(S3) xt+1 = xt + ¯ηt · eit
(S4) Increment t by 1.

(⇔ xt+1

it = xt

it + ¯ηt)

¯ηt = arg min
η∈R

f (xt + ηeit) + h(xt + ηeit) − g(xt + ηeit).

end while

Then xt is updated via: xt+1 = xt + ¯ηteit. However, the
one-dimensional problem above could be still hard to solve
when f (·) and/or g(·) is complicated. One can consider
replacing f (·) and g(·) with their majorization function:

f (xt + ηeit) ≤ Sit(xt, η)

with Sit(xt, η) (cid:44) f (xt) + (cid:104)∇f (xt), ηeit(cid:105) + cit

2 η2,

−g(xt + ηeit) ≤ Git(xt, η)
with Git(xt, η) (cid:44) −g(xt) − (cid:104)∂g(xt), (xt + ηeit) − xt(cid:105).

(cid:73) Choosing the Majorization Function

1. Sequential NonConvex Approximation Strategy. If
we replace f (xt +ηeit) with its upper bound Sit(xt, η)
while keep the remaining two terms unchanged, we
have the resulting subproblem as in (10), which is a
nonconvex problem. It reduces to the proximal op-
erator computation as in (3) with a = cit + θ and
b = ∇itf (xt). Setting the gradient with respect to
η of the objective function in (10) to zero, we have
the following necessary but not sufﬁcient optimality
condition for (10):

0 ∈ [∇f (xt) + ∂h(xt+1) − ∂g(xt+1)]it + (cit + θ)¯ηt.

2. Sequential Convex Approximation Strategy. If we
replace f (xt + ηeit) and −g(xt + ηeit) with their re-
spective upper bounds Sit(xt, η) and Git(xt, η), while

keep the term h(xt + ηeit) unchanged, we have the
resulting subproblem as in (11), which is a convex prob-
lem. We have the following necessary and sufﬁcient
optimality condition for (11):

0 ∈ [∇f (xt) + ∂h(xt+1) − ∂g(xt)]it + (cit + θ)¯ηt.

(cid:73)Selecting the Coordinate to Update

(ii) Cyclic rule.

There are several fashions to decide which coordinate to
update in the literature (Tseng & Yun, 2009). (i) Random
rule. it is randomly selected from {1, ..., n} with equal
it takes all coordinates in
probability.
cyclic order 1 → 2 → ... → n → 1. It is similar to the
Gauss-Seidel iterative method for solving linear systems
of equations. (iii) Greedy rule. Assume that ∇f (x) is
Lipschitz continuous with constant L. The index it is chosen
j| where dt = arg mind h(xt + d) +
as it = arg maxj |dt
L
2 + (cid:104)∇f (xt) − ∂g(xt)), d(cid:105). Note that dt = 0 implies
2 (cid:107)d(cid:107)2
that xt is a critical point. It is also called Gauss-Southwell
rule.

We summarize CD-SNCA and CD-SCA in Algorithm 1.

Remarks. (i) We consider a proximal term for the sub-
problems in (10) and (11) with θ being the proximal point
parameter. This is to guarantee sufﬁcient descent condi-
tion and global convergence for Algorithm 1. As can be
seen in Theorem 5.9 and Theorem 5.10, the parameter θ is
critical for CD-SNCA. (ii) Problem (10) can be viewed as

Coordinate Descent Methods for DC Minimization

globally solving the following nonconvex problem which
has a bilinear structure: (¯ηt, y) = arg minη,y Sit(xt, η) +
θ
2 η2 + h(xt + ηeit) − (cid:104)y, xt + ηeit(cid:105) + g∗(y). (iii) While
we apply CD to the primal, one may apply to the dual
as in Problem (9). (iv) CD fails to converge for nonsep-
arable nonsmooth convex functions. We consider the fol-
lowing example: minx,y x2 + y2 + 2|x − y|. Let the ini-
tial point be (x, y) = (1, 1).
It gets stuck at the point
(x, y) = (1, 1), which is not the global minimum. (v) In
contract, CD always converges (to a critical point) for non-
separable nonsmooth concave functions. We consider the
following example: minx,y x2 + y2 − 2|x − y|. Let the
initial point be (x, y) = (1, 1). It terminates at the point
(x, y) = (−1, 1) or (x, y) = (1, −1), which are the global
minimum. (vi) We explain CD-SNCA is more accurate than
CD-SCA by using the following one-dimensional example:
minx(x − 1)2 − 4|x|. This problem contains three critical
points {−1, 0, 3}. While CD-SCA only ﬁnds one of the
critical points, CD-SNCA ﬁnds the global optimal solution
x = 3. This is achieved by using a cleaver breakpoint
searching algorithm (discussed later) to solve Problem (10).

5. Theoretical Analysis

In this section, we present some optimality analysis and
convergence analysis for Algorithm 1. Due to space limit,
all proofs are placed in Section A in the appendix.

We make the following additional assumptions.
Assumption 5.1. (weak convexity) ¨g(x) (cid:44) −g(x) is ρ-
weakly convex that (Davis et al., 2018; Li et al., 2021):

¨g(x) ≤ ¨g(y) + (cid:104)x − y, ∂¨g(x)(cid:105) +

ρ
2

(cid:107)x − y(cid:107)2

2, ∀x, y.

Assumption 5.2. (sharpness) F (x) is µ-sharp that (Davis
et al., 2018; Davis & Grimmer, 2019; Li et al., 2021):

µ(cid:107)x − ¯x(cid:107) ≤ F (x) − F (¯x), ∀x,

(12)

where ¯x denotes any global optimal solution of Problem (1).

Remarks. (i) Assumption 5.1 is equivalent to ¨g(x)+ ρ
is convex.
weakly convex.

2 (cid:107)x(cid:107)2
2
(ii) F (·) is ρ-weakly convex iff −g(x) is ρ-

We assume that the random-coordinate selection rule is used.
After t iterations, Algorithm 1 generates a random output xt,
which depends on the observed realization of the random
variable: ξt−1 (cid:44) {i0, i1, ..., it−1}. We let ¯x be any global
optimal solution of Problem (1) and deﬁne:

qt (cid:44) F (xt) − F (¯x), rt (cid:44) 1

2 (cid:107)xt − ¯x(cid:107)2

¯c, ¯c (cid:44) c + θ.

The quantities qt and rt which measure the distance between
xt and ¯x, will appear in our complexity results.

We now develop the following technical lemma that will be
used to analyze Algorithm 1 subsequently.

Lemma 5.3. For any x ∈ Rn and d ∈ Rn, we deﬁne
i=1 h(x + diei), g(cid:48)(x) (cid:44) (cid:80)n
h(cid:48)(x) (cid:44) (cid:80)n
i=1 g(x + diei),
and f (cid:48)(x) (cid:44) (cid:80)n
i=1 f (x + diei). We have:

(cid:80)n

f (cid:48)(x) ≤ f (x) + (cid:104)∇f (x), d(cid:105) + 1

2 = (cid:107)x + d(cid:107)2

i=1 (cid:107)x + diei(cid:107)2

2 + (n − 1)(cid:107)x(cid:107)2

2 (13)
h(cid:48)(x) = h(x + d) + (n − 1)h(x) (14)
c + (n − 1)f (x) (15)
−g(cid:48)(x) ≤ −g(x) − (cid:104)∂g(x), d(cid:105) − (n − 1)g(x) (16)
−g(cid:48)(x) ≤ −g(x + d) + ρ
2 − (n − 1)g(x) (17)

2 (cid:107)d(cid:107)2

2 (cid:107)d(cid:107)2

¯c = 1

Remarks. (i) Using (13) and (14), we have: Eit[(cid:107)xt+1 −
¯x, Eit[h(xt+1)] =
¯c + (1 − 1
¯x(cid:107)2
n (cid:107)xt+1 − ¯x(cid:107)2
n h(xt+1) + (1 − 1
1
n )h(xt). Lemma 5.3 reveals a relation
between the variable and its expectation for the consecutive
(ii) Note that (16) is only used for CD-SCA,
iterations.
while (17) is only used for CD-SNCA.

n )(cid:107)xt − ¯x(cid:107)2

5.1. Optimality Analysis

We now provide an optimality analysis of our method. Since
the coordinate-wise optimality condition is novel in this
paper, we clarify its relations with existing optimality con-
ditions formally. We use ˇx, `x, ¨x, and ¯x to denote a critical
point, a directional point, a coordinate-wise stationary point,
and an optimal point, respectively.

Deﬁnition 5.4. (Critical Point) A solution ˇx is called a
critical point if the following holds (Toland, 1979):

0 ∈ ∇f (ˇx) + ∂h(ˇx) − ∂g(ˇx)

Remarks. (i) The expression above is equivalent to (f (ˇx) +
∂h(ˇx)) ∩ ∂g(ˇx) (cid:54)= ∅. The sub-differential is always non-
empty on convex functions; that is why we assume that F (·)
can be repressed as the difference of two convex functions.
(ii) Existing methods such as MSCR, PDCA, and SubGrad
as shown in Section (3) are only guaranteed to ﬁnd critical
points of Problem (1).

Deﬁnition 5.5. (Directional Point) A solution `x is called a
directional point if the following holds (Pang et al., 2017):

F (cid:48)(`x; y − `x) ≥ 0, ∀y ∈ dom(F ) (cid:44) {x : |F (x)| < +∞}

Remarks. (i) When F (·) is continuously differentiable, the
optimality of critical points is equivalent to that of direc-
tional points. (ii) The work of (Pang et al., 2017) charac-
terizes different types of stationary points, and proposes
an enhanced DC algorithm that subsequently converges to
a directional point. However, they only consider the case
g(x) = maxi∈I gi(x) where each gi(x) is continuously
differentiable and convex and I is a ﬁnite index set.

Deﬁnition 5.6. (Coordinate-Wise Stationary Point) We let

Mi(x, η) (cid:44) ci+θ

2 η2 + ∇f (x)iη + h(x + ηei) − g(x + ηei)

Coordinate Descent Methods for DC Minimization

for a given constant θ ≥ 0. A solution ¨x is called a
coordinate-wise stationary point if the following holds:

Mi(¨x, 0) = min

η

Mi(¨x, η), ∀i = 1, ..., n.

Remarks. Coordinate-wise stationary point states that if
we minimize the majorization function Mi(x, η), we can
not improve the objective function value for Mi(x, η) for
all i.

The following lemma shows the property of any coordinate-
wise stationary point.
Lemma 5.7. For any coordinate-wise stationary point ¨x,
we have: F (¨x) ≤ F (¨x + d) + 1

(c+θ+ρ) for all d.

2 (cid:107)d(cid:107)2

Remarks. Recall that a solution x is said to be a local
minima if F (x) ≤ F (x + d) for some sufﬁciently small
constant δ that (cid:107)d(cid:107) ≤ δ. The coordinate-wise stationary
point ¨x can be viewed as an approximate local minima.
Note that neither the optimality condition of coordinate-
wise stationary point nor that of the local minima point is
stronger than the other.

The following theorem establishes the relations between the
different types of stationary points list above.

Theorem 5.8. (Optimality Hierarchy between the Opti-
mality Conditions). The following relations hold:

{¯x}

(a)
⊆ {¨x}

(b)
⊆ {`x}

(c)
⊆ {ˇx}

Remarks. (i) The coordinate-wise optimality condition is
always stronger than the critical point condition (Gotoh
et al., 2018; Zhang, 2010; Bi et al., 2014) and the directional
point condition (Pang et al., 2017) when the objective func-
tion is weakly convex. (ii) Our optimality analysis can be
also applied to the equivalent dual problem which is also a
DC program as in (9).

5.2. Convergence Analysis

We provide convergence analysis for CD-SNCA and CD-
SCA.

Theorem 5.9. (Global Convergence). We have the follow-
ing results:

(a) For CD-SNCA, it holds that:

F (xt+1) − F (xt) ≤ − θ

2 (cid:107)xt+1 − xt(cid:107)2

(18)

Algorithm 1 converges to a coordinate-wise stationary
point of Problem (1) with Eξt−1[mint
2] ≤
2n[F (x0)−F (¯x)]
θ(t+1)

i=0 (cid:107)xi+1 − xi(cid:107)2

.

(b) For CD-SCA, it holds that:

F (xt+1) − F (xt) ≤ − β

2 (cid:107)xt+1 − xt(cid:107)2

(19)

with β (cid:44) min(c) + 2θ. Algorithm 1 converges to a critical
point of Problem (1) with Eξt−1[mint
2] ≤
2n[F (x0)−F (¯x)]
β(t+1)

i=0 (cid:107)xi+1 − xi(cid:107)2

.

Remarks. While existing methods only ﬁnd critical points
or directional points of Problem (1), CD-SNCA is guaran-
teed to ﬁnd a coordinate-wise stationary point which has
stronger optimality guarantees (See Theorem 5.8).

We now establish linear convergence rate for Algorithm 1
when the objective function satisﬁes the weak convexity con-
dition and the sharpness condition. We obtain the following
two theorems.

Theorem 5.10. (Convergence Rate for CD-SNCA). Fix a
real χ ∈ (0, 1). We deﬁne

¯ρ = ρ

min(¯c) , γ (cid:44) 1 + ρ

θ , κ (cid:44)

1−χ

n(γ+ 1− ¯ρ

¯ρ χ)

Assume that xt is sufﬁciently close to the global optimal
max(¯c) ( µχ
solution such that rt ≤ 2
¯ρ )2 and the proximal pa-
rameter θ is sufﬁciently large such that ¯ρ < 1, we have
0 < κ < 1 and

Eξt−1[rt] ≤ (1 − κ)t · (r0 + γ

1− ¯ρ q0).

Theorem 5.11. (Convergence Rate for CD-SCA). Fix a
real χ ∈ (0, 1). We deﬁne:

¯ρ = ρ

min(¯c) , κ (cid:44) 1−χ
n(1+ χ

¯ρ ) ,

Assuming that xt is sufﬁciently close to the global optimal
max(¯c) ( χµ
solution such that rt ≤ 2
τ )2, we have 0 < κ < 1
and

Eit−1[rt] ≤ (1 − κ)t · (r0 + q0)

Remarks. When F (·) is ρ-weakly convex and µ-sharp,
CD-SNCA and CD-SCA with appropriate initializations
converge linearly to the optimal solution (which is also
a coordinate-wise stationary solution).

6. A Breakpoint Searching Method for
Proximal Operator Computation

This section presents a new breakpoint searching method
to solve Problem (3) exactly and efﬁciently for different
examples of h(·) and g(·).

This method ﬁrst identiﬁes all the possible critical points
/ breakpoints Θ for minη∈R p(η) as in Problem (3), and
then picks the solution that leads to the lowest value as the
optimal solution. We denote A ∈ Rm×n be an arbitrary
matrix, and deﬁne g = Aei ∈ Rm, d = Ax ∈ Rm.

Coordinate Descent Methods for DC Minimization

6.1. When g(y) = (cid:107)Ay(cid:107)1 and hi(·) (cid:44) 0

a

(cid:54)= 0.

2 η2 + bη − (cid:107)A(x + ηei)(cid:107)1. It
Consider the problem: minη
can be rewritten as: minη p(η) (cid:44) a
2 η2 + bη + (cid:107)gη + d(cid:107)1.
Setting the gradient of p(·) to zero yields: 0 = aη + b −
(cid:104)sign(ηg + d), g(cid:105) = aη + b − (cid:104)sign(η + d ÷ |g|), g(cid:105),
where we use: ∀ρ > 0, sign(x) = sign(ρx). We as-
sume gi
If this does not hold and there exists
gj = 0 for some j, then {gj, dj} can be removed since
it does not affect the minimizer of the problem. We deﬁne
} ∈ R2m×1, and assume
z (cid:44) {+ d1
, − dm
gm
g1
z has been sorted in ascending order. The domain p(η) can
be divided into 2m + 1 intervals: (−∞, z1), (z1, z2),..., and
(z2m, +∞). There are 2m + 1 breakpoints η ∈ R(2m+1)×1.
In each interval, the sign of (η + d ÷ |g|) can be deter-
mined. Thus, the i-th breakpoints for the i-th interval
can be computed as ηi = ((cid:104)sign(η + d ÷ |g|), g(cid:105) − b)/a.
Therefore, Problem (3) contains 2m + 1 breakpoints Θ =
{η1, η2, ..., η(2m+1)} for this example.

, ..., + dm
gm

, − d1
g1

6.2. When g(y) = (cid:107)Ay(cid:107)∞ and hi(·) (cid:44) 0

a

2 η2 + bη − (cid:107)A(x + ηei)(cid:107)∞.
Consider the problem: minη
a
2 η2 + bη + (cid:107)gη + d(cid:107)∞. It
It can be rewritten as: minη
is equivalent to minη p(η) (cid:44) a
2 η2 + bη + max2m
i=1(¯giη +
¯di) with ¯g = [g1, g2, ..., gm, −g1, −g2, ..., −gm] and ¯d =
[d1, d2, ..., dm, −d1, −d2, ..., −dm]. Setting the gradient
of p(·) to zero yields: aη+b+¯gi = 0 with i = 1, 2, ..., (2m).
We have η = (−b − ¯g)/a. Therefore, Problem (3) contains
2m breakpoints Θ = {η1, η2, ..., η2m} for this example.

6.3. When g(y) = (cid:80)s

i=1 |y[i]| and hi(y) (cid:44) |yi|

a

a

2 η2 + bη + |xi + η| −
Consider the problem: minη
(cid:80)s
i=1 |(x + ηei)[i]|. Since the variable η only affects
the value of xi, we consider two cases for xi + η.
(i)
xi + η belongs to the top-s subset. This problem reduces to
2 η2 + bη, which contains one unique breakpoint:
minη
{−b/a}.
(ii) xi + η does not belong to the top-s sub-
2 η2 + bt + |xi + η|,
set. This problem reduces to minη
which contains three breakpoints {−xi, (−1 − b)/a, (1 −
b)/a}. Therefore, Problem (3) contains 4 breakpoints
Θ = {−b/a, −xi, (−1 − b)/a, (1 − b)/a} for this ex-
ample.

a

When we have found the breakpoint set Θ for Problem (3),
we pick the solution that results in the lowest value as the
global optimal solution ¯η. In other words, we have:

¯η = arg minη p(η), s.t. η ∈ Θ.

The function hi(·) does not bring much difﬁculty for solv-
ing Problem (3). We let ¯p(η) (cid:44) a
2 η2 + bη − g(x + ηei).
(i) For the bound constrained function with hi(z) =
(cid:110) 0,
, we have: minη ¯p(η), s.t. lb ≤ xi +
∞,

lb ≤ z ≤ ub
else

η ≤ ub. One can consider the breakpoint set Θ (cid:44)
{lb, max(lb, min(ub, ς)), ub} with ς (cid:44) arg minη ¯p(η). (ii)
For the (cid:96)1 norm function with hi(z) = λ|z| for some
λ > 0, we have: minη ¯p(η) + λ|xi + η|. One can con-
sider the breakpoint set Θ (cid:44) {[arg minη ¯p(η) + λ(xi +
η)], [arg minη ¯p(η) − λ(xi + η)], −xi}.

7. Experiments

This section demonstrates the effectiveness and efﬁciency of
Algorithm 1 on two statistical learning tasks, namely the (cid:96)p
norm generalized eigenvalue problem and the approximate
sparse optimization problem. For more experiments, please
refer to Section C in the Appendix.

7.1. Experimental Settings

(ii) ‘e2006-m-n’: G = X.

We consider the following four types of data sets for the
sensing/channel matrix G ∈ Rm×n.
(i) ‘randn-m-n’:
G = randn(m, n).
(iii)
‘randn-m-n-C’: G = N (randn(m, n)). (iv) ‘e2006-m-n-
C’: G = N (X). Here, randn(m, n) is a function that re-
turns a standard Gaussian random matrix of size m × n.
X is generated by sampling from the original real-world
data set ‘e2006-tﬁdf’. N (G) is deﬁned as: [N (G)]I =
100 · GI , [N (G)] ¯I = G ¯I , where I is a random subset of
{1, ..., mn}, ¯I = {1, ..., mn} \ I, and |I| = 0.1 · mn. The
last two types of data sets are designed to verify the robust-
ness of the algorithms.

All methods are implemented in MATLAB on an Intel 2.6
GHz CPU with 32 GB RAM. Only our breakpoint searching
procedure is developed in C and wrapped into the MATLAB
code, since it requires elementwise loops that are less efﬁ-
cient in native MATLAB. We keep a record of the relative
changes of the objective by zt = [F (xt)−F (xt+1)]/F (xt),
and let all algorithms run up to T seconds and stop them at it-
eration t if mean([zt−min(t,υ)+1, zt−min(t,υ)+2, ..., zt]) ≤ (cid:15).
The default value (θ, (cid:15), υ, T ) = (10−6, 10−10, 500, 60) is
used. All methods are executed 10 times and the average
performance is reported. Some Matlab code can be found
in the supplemental material.

7.2. (cid:96)p Norm Generalized Eigenvalue Problem

α

We consider Problem (4) with p = 1 and Q = I. We
2 (cid:107)x(cid:107)2
2 − (cid:107)Gx(cid:107)1. It is
have the following problem: minx
2, h(x) (cid:44) 0,
consistent with Problem (1) with f (x) (cid:44) α
2 (cid:107)x(cid:107)2
and g(x) (cid:44) (cid:107)Gx(cid:107)1. The subgradient of g(x) at xt can be
computed as: gt (cid:44) GT sign(Gxt). ∇f (x) is L-Lipschitz
with L = 1 and coordinate-wise Lipschitz with c = 1. We
set α = 1.

We compare with the following methods. (i) Multi-Stage
Convex Relaxation (MSCR). It generates the new iter-

Coordinate Descent Methods for DC Minimization

ate using: xt+1 = arg minx f (x) − (cid:104)x − xt, gt(cid:105).
(ii)
Toland’s dual method (T-DUAL). It rewrite the problem as:
min−1≤y≤1 minx f (x) − (cid:104)Gx, y(cid:105). Setting the gradient of
x to zero, we have: αx − GT y = 0, leading to the follow-
ing dual problem: min−1≤y≤1 − 1
2α yT GGT y. Toland’s
dual method uses the iteration: yt+1 = sign(GGT yt), and
recovers the primal solution via x = 1
α GT y. Note that
the method in (Kim & Klabjan, 2019) is essentially the
Toland’s duality method and they consider a constrained
problem: min(cid:107)x(cid:107)=1 −(cid:107)Gx(cid:107)1.
(iii) Subgradient method
It generates the new iterate via: xt+1 =
(SubGrad).
xt − 0.1
· (∇f (xt) − gt). (iv) CD-SCA solves a convex
t
problem: ¯ηt = arg minη
it)η and
update xt via xt+1
it + ¯ηt. (v) CD-SNCA computes the
it = xt
nonconvex proximal operator of (cid:96)1 norm (see Section 6.1)
ci+θ
as: ¯ηt = arg minη
2 η2 + ∇itf (xt)η − (cid:107)G(x + ηei)(cid:107)1
and update xt via xt+1
it = xt

ci+θ
2 η2 + (∇itf (xt) − gt

it + ¯ηt.

As can be seen from Table 1, the proposed method CD-
SNCA consistently gives the best performance. Such results
are not surprising since CD-SNCA is guaranteed to ﬁnd
stronger stationary points than the other methods (while
CD-SNCA ﬁnds a coordinate-wise stationary point, all the
other methods only ﬁnd critical points).

7.3. Approximate Sparse Optimization

We consider solving Problem (5). To generate the orig-
inal signal ¨x of s-sparse structure, we randomly select
a support set S with |S| = 200 and set ¨x{1,...,n}\S =
0, ¨xS = randn(|S|, 1). The observation vector is generated
via y = A¨x + randn(m, 1) × 0.1 × (cid:107)A¨x(cid:107). This problem
is consistent with Problem (1) with f (x) (cid:44) 1
2 (cid:107)Gx − y(cid:107)2
2,
h(x) (cid:44) ρ(cid:107)x(cid:107)1, and g(x) (cid:44) ρ (cid:80)s
[i]|. ∇f (x) is L-
Lipschitz with L = (cid:107)G(cid:107)2
2 and coordinate-wise Lipschitz
with ci = (GT G)ii, ∀i. The subgradient of g(x) at xt can
be computed as: gt = ρ · arg maxy(cid:104)y, xt(cid:105), s.t.(cid:107)y(cid:107)∞ ≤
1, (cid:107)y(cid:107)1 ≤ k. We set ρ = 1.

i=1 |xt

1

L

2 (cid:107)Gx − y(cid:107)2

We compare with the following methods. (i) Multi-Stage
Convex Relaxation (MSCR). It generate a sequence {xt} as:
2 + ρ(cid:107)x(cid:107)1 − (cid:104)x − xt, gt(cid:105). (ii)
xt+1 = arg minx
Proximal DC algorithm (PDCA). It generates the new iterate
2 + (cid:104)x − xt, ∇f (x)(cid:105) +
2 (cid:107)x − xt(cid:107)2
using: xt+1 = arg minx
ρ(cid:107)x(cid:107)1 − (cid:104)x − xt, gt(cid:105). (iii) Subgradient method (SubGrad).
It uses the following iteration: xt+1 = xt − 0.1
· (∇f (x) +
t
ρsign(xt) − gt). (iv) CD-SCA solves a convex problem:
it + η| + [∇f (xt) −
¯ηt = arg minη 0.5(cit + θ)η2 + ρ|xt
gt]it · η and update xt via xt+1
it + ¯ηt. (v) CD-SNCA
it = xt
computes the nonconvex proximal operator of the top-s
ci+θ
2 η2+
norm function (see Section 6.3) as: ¯ηt = arg minη
it + η| − ρ (cid:80)s
i=1 |(xt + ηei)[i]| and update
∇itf (xt)η + ρ|xt
xt via xt+1
it = xt
it + ¯ηt.

MSCR
-1.329 ± 0.038
randn-256-1024
-1.132 ± 0.021
randn-256-2048
-5.751 ± 0.163
randn-1024-256
-9.364 ± 0.183
randn-2048-256
-28.031 ± 37.894
e2006-256-1024
-22.282 ± 24.007
e2006-256-2048
-43.516 ± 77.232
e2006-1024-256
-44.705 ± 47.806
e2006-2048-256
-1.332 ± 0.019
randn-256-1024-C
-1.161 ± 0.024
randn-256-2048-C
-5.650 ± 0.141
randn-1024-256-C
-9.236 ± 0.125
randn-2048-256-C
-4.841 ± 6.410
e2006-256-1024-C
-4.297 ± 2.825
e2006-256-2048-C
-6.469 ± 3.663
e2006-1024-256-C
e2006-2048-256-C -31.291 ± 60.597

PDCA
-1.329 ± 0.038
-1.132 ± 0.021
-5.751 ± 0.163
-9.364 ± 0.183
-28.031 ± 37.894
-22.282 ± 24.007
-43.516 ± 77.232
-44.705 ± 47.806
-1.332 ± 0.019
-1.161 ± 0.024
-5.650 ± 0.141
-9.236 ± 0.125
-4.841 ± 6.410
-4.297 ± 2.825
-6.469 ± 3.663
-31.291 ± 60.597

T-DUAL
-1.329 ± 0.038
-1.132 ± 0.021
-5.664 ± 0.173
-9.161 ± 0.101
-27.996 ± 37.912
-22.282 ± 24.007
-43.364 ± 77.265
-44.705 ± 47.806
-1.332 ± 0.019
-1.161 ± 0.024
-5.591 ± 0.145
-9.067 ± 0.137
-4.840 ± 6.410
-4.297 ± 2.823
-6.469 ± 3.663
-31.291 ± 60.597

CD-SCA
-1.426 ± 0.056
-1.192 ± 0.019
-5.755 ± 0.108
-9.405 ± 0.182
-27.880 ± 37.980
-22.113 ± 23.941
-43.283 ± 77.297
-44.633 ± 47.789
-1.417 ± 0.027
-1.212 ± 0.022
-5.716 ± 0.159
-9.243 ± 0.145
-4.837 ± 6.411
-4.259 ± 2.827
-6.470 ± 3.663
-31.284 ± 60.599

CD-SNCA
-1.447 ± 0.053
-1.202 ± 0.016
-5.817 ± 0.129
-9.408 ± 0.164
-28.167 ± 37.826
-22.448 ± 23.908
-44.269 ± 76.977
-45.176 ± 47.493
-1.444 ± 0.029
-1.219 ± 0.023
-5.808 ± 0.134
-9.377 ± 0.233
-5.027 ± 6.363
-4.394 ± 2.814
-6.881 ± 3.987
-32.026 ± 60.393

Table 1: Comparisons of objective values of all the meth-
ods for solving the (cid:96)1 norm PCA problem. The 1st, 2nd,
and 3rd best results are colored with red, green and blue,
respectively.

MSCR
0.090 ± 0.017
randn-256-1024
0.052 ± 0.009
randn-256-2048
1.887 ± 0.353
randn-1024-256
3.795 ± 0.518
randn-2048-256
0.217 ± 0.553
e2006-256-1024
0.050 ± 0.068
e2006-256-2048
3.078 ± 2.928
e2006-1024-256
1.799 ± 1.453
e2006-2048-256
randn-256-1024-C 0.086 ± 0.012
randn-256-2048-C 0.043 ± 0.006
randn-1024-256-C 1.997 ± 0.250
randn-2048-256-C 3.618 ± 0.681
e2006-256-1024-C 0.031 ± 0.031
e2006-256-2048-C 0.217 ± 0.575
e2006-1024-256-C 3.789 ± 4.206
e2006-2048-256-C 4.480 ± 6.916

PDCA
0.090 ± 0.016
0.052 ± 0.010
1.884 ± 0.352
3.794 ± 0.518
0.217 ± 0.553
0.050 ± 0.068
3.078 ± 2.928
1.799 ± 1.453
0.087 ± 0.012
0.044 ± 0.006
1.998 ± 0.250
3.617 ± 0.682
0.031 ± 0.031
0.217 ± 0.575
3.798 ± 4.213
4.482 ± 6.918

SubGrad
0.775 ± 0.040
1.485 ± 0.030
2.215 ± 0.379
4.127 ± 0.525
0.597 ± 0.391
0.837 ± 0.209
3.112 ± 2.844
1.918 ± 1.518
0.775 ± 0.038
1.472 ± 0.027
2.351 ± 0.297
3.965 ± 0.717
0.339 ± 0.073
0.596 ± 0.418
3.955 ± 4.363
4.710 ± 7.292

CD-SCA
0.092 ± 0.018
0.061 ± 0.012
1.881 ± 0.337
3.772 ± 0.522
0.218 ± 0.556
0.050 ± 0.068
3.097 ± 2.960
1.805 ± 1.456
0.083 ± 0.011
0.051 ± 0.009
1.979 ± 0.265
3.619 ± 0.679
0.030 ± 0.028
0.215 ± 0.568
3.851 ± 4.339
4.461 ± 6.844

CD-SNCA
0.034 ± 0.004
0.027 ± 0.002
1.681 ± 0.346
3.578 ± 0.484
0.087 ± 0.212
0.025 ± 0.032
2.697 ± 2.545
1.688 ± 1.398
0.033 ± 0.002
0.026 ± 0.001
1.781 ± 0.244
3.420 ± 0.673
0.015 ± 0.014
0.071 ± 0.176
3.398 ± 3.855
4.200 ± 6.608

Table 2: Comparisons of objective values of all the methods
for solving the approximate sparse optimization problem.
The 1st, 2nd, and 3rd best results are colored with red,
green and blue, respectively.

(a) randn-256-1024

(b) randn-256-2048

(c) randn-1024-256

(d) randn-2048-256

Figure 1: The convergence curve of the compared methods
for solving the (cid:96)p norm generalized eigenvalue problem on
different data sets.

As can be seen from Table 2, CD-SNCA consistently gives

the best performance.

10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)10-5100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10−210010−5100Time (seconds)Objective  MSCRPDCASubGradCD−SCACD−SNCA10−310−210−110−2100102Time (seconds)Objective  MSCRPDCASubGradCD−SCACD−SNCACoordinate Descent Methods for DC Minimization

7.4. Computational Efﬁciency

Figure 1 shows the convergence curve for solving the (cid:96)p
norm generalized eigenvalue problem. All methods take
about 30 seconds to converge. CD-SNCA generally takes a
little more time to converge than the other methods. How-
ever, we argue that the computational time is acceptable and
pays off as CD-SNCA generally achieves higher accuracy.

References

Ahmadi, A. A. and Hall, G. DC decomposition of noncon-
vex polynomials with algebraic techniques. Mathematical
Programming, 169(1):69–94, 2018.

Attouch, H., Bolte, J., Redont, P., and Soubeyran, A. Proxi-
mal alternating minimization and projection methods for
nonconvex problems: An approach based on the kurdyka-
lojasiewicz inequality. Mathematics of Operations Re-
search, 35(2):438–457, 2010.

Beck, A. and Eldar, Y. C. Sparsity constrained nonlinear op-
timization: Optimality conditions and algorithms. SIAM
Journal on Optimization, 23(3):1480–1509, 2013.

Beck, A. and Hallak, N. On the convergence to stationary
points of deterministic and randomized feasible descent
directions methods. SIAM Journal on Optimization, 30
(1):56–79, 2020.

Beck, A. and Teboulle, M. Dual randomized coordinate de-
scent method for solving a class of nonconvex problems.
SIAM Journal on Optimization, 31(3):1877–1896, 2021.

Beck, A. and Tetruashvili, L. On the convergence of block
coordinate descent type methods. SIAM journal on Opti-
mization, 23(4):2037–2060, 2013.

Bi, S., Liu, X., and Pan, S. Exact penalty decomposition
method for zero-norm minimization based on mpec for-
mulation. SIAM Journal on Scientiﬁc Computing, 36(4):
A1451–A1477, 2014.

Breheny, P. and Huang, J. Coordinate descent algorithms
for nonconvex penalized regression, with applications
to biological feature selection. The Annals of Applied
Statistics, 5(1):232, 2011.

Cand`es, E. J., Li, X., and Soltanolkotabi, M. Phase re-
trieval via wirtinger ﬂow: Theory and algorithms. IEEE
Transactions on Information Theory, 61(4):1985–2007,
2015.

Davis, D. and Grimmer, B. Proximally guided stochastic
subgradient method for nonsmooth, nonconvex problems.
SIAM Journal on Optimization, 29(3):1908–1930, 2019.

Davis, D., Drusvyatskiy, D., MacPhee, K. J., and Paquette,
C. Subgradient methods for sharp weakly convex func-
tions. Journal of Optimization Theory and Applications,
179(3):962–982, 2018.

Deng, Q. and Lan, C. Efﬁciency of coordinate descent
methods for structured nonconvex optimization. In Joint
European Conference on Machine Learning and Knowl-
edge Discovery in Databases, pp. 74–89. Springer, 2020.

Gong, P., Zhang, C., Lu, Z., Huang, J., and Ye, J. A gen-
eral iterative shrinkage and thresholding algorithm for
In In-
non-convex regularized optimization problems.
ternational Conference on Machine Learning (ICML),
volume 28, pp. 37–45, 2013.

Gotoh, J., Takeda, A., and Tono, K. Dc formulations and al-
gorithms for sparse optimization problems. Mathematical
Programming, 169(1):141–176, 2018.

Horst, R. and Thoai, N. V. Dc programming: overview.
Journal of Optimization Theory and Applications, 103(1):
1–43, 1999.

Horst, R. and Tuy, H. Global optimization: Deterministic
approaches. Springer Science & Business Media, 2013.

Hsieh, C.-J. and Dhillon, I. S. Fast coordinate descent meth-
ods with variable selection for non-negative matrix factor-
ization. In ACM International Conference on Knowledge
Discovery and Data Mining (SIGKDD), pp. 1064–1072,
2011.

Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., and
Sundararajan, S. A dual coordinate descent method for
large-scale linear svm. In International Conference on
Machine Learning (ICML), pp. 408–415, 2008.

Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan,
In In-
M. I. How to escape saddle points efﬁciently.
ternational Conference on Machine Learning (ICML),
volume 70, pp. 1724–1732, 2017.

Joki, K., Bagirov, A. M., Karmitsa, N., and M¨akel¨a, M. M. A
proximal bundle method for nonsmooth dc optimization
utilizing nonconvex cutting planes. Journal of Global
Optimization, 68(3):501–535, 2017.

Joki, K., Bagirov, A. M., Karmitsa, N., Makela, M. M.,
and Taheri, S. Double bundle method for ﬁnding clarke
stationary points in nonsmooth dc programming. SIAM
Journal on Optimization, 28(2):1892–1919, 2018.

Jr., G. D. F. Maximum-likelihood sequence estimation of
digital sequences in the presence of intersymbol interfer-
ence. IEEE Transactions on Information Theory, 18(3):
363–378, 1972.

Coordinate Descent Methods for DC Minimization

Kim, C. and Klabjan, D. A simple and fast algorithm for l1-
norm kernel pca. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 42(8):1842–1855, 2019.

Li, X., Chen, S., Deng, Z., Qu, Q., Zhu, Z., and Man-Cho So,
A. Weakly convex optimization over stiefel manifold us-
ing riemannian subgradient-type methods. SIAM Journal
on Optimization, 31(3):1605–1634, 2021.

Li, Y., Lu, J., and Wang, Z. Coordinatewise descent meth-
ods for leading eigenvalue problem. SIAM Journal on
Scientiﬁc Computing, 41(4):A2681–A2716, 2019.

Liu, J., Wright, S. J., R´e, C., Bittorf, V., and Sridhar, S.
An asynchronous parallel stochastic coordinate descent
algorithm. Journal of Machine Learning Research, 16
(285-322):1–5, 2015.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. Mathematical
Programming, 152(1-2):615–642, 2015.

Lu, Z. and Zhou, Z. Nonmonotone enhanced proximal
DC algorithms for a class of structured nonsmooth DC
programming. SIAM Journal on Optimization, 29(4):
2725–2752, 2019.

Maing´e, P.-E. and Moudaﬁ, A. Convergence of new inertial
proximal methods for dc programming. SIAM Journal on
Optimization, 19(1):397–413, 2008.

Mairal, J. Optimization with ﬁrst-order surrogate func-
tions. In International Conference on Machine Learning
(ICML), volume 28, pp. 783–791, 2013.

Necoara, I. Random coordinate descent algorithms for multi-
agent convex optimization over networks. IEEE Transac-
tions on Automatic Control, 58(8):2001–2012, 2013.

Richt´arik, P. and Tak´avc, M. Iteration complexity of random-
ized block-coordinate descent methods for minimizing
a composite function. Mathematical Programming, 144
(1-2):1–38, 2014.

Shechtman, Y., Beck, A., and Eldar, Y. C. Gespar: Efﬁcient
phase retrieval of sparse signals. IEEE Transactions on
Signal Processing, 62(4):928–938, 2014.

Sriperumbudur, B. K., Torres, D. A., and Lanckriet, G. R. G.
Sparse eigen methods by D.C. programming. In Interna-
tional Conference on Machine Learning (ICML), volume
227, pp. 831–838, 2007.

Tao, P. D. and An, L. T. H. Convex analysis approach to dc
programming: theory, algorithms and applications. Acta
mathematica vietnamica, 22(1):289–355, 1997.

Toland, J. F. A duality principle for non-convex optimisa-
tion and the calculus of variations. Archive for Rational
Mechanics and Analysis, 71(1):41–61, 1979.

Tseng, P. and Yun, S. A coordinate gradient descent method
for nonsmooth separable minimization. Mathematical
Programming, 117(1):387–423, 2009.

Xu, Y. and Yin, W. A block coordinate descent method for
regularized multiconvex optimization with applications
to nonnegative tensor factorization and completion. SIAM
Journal on Imaging Sciences, 6(3):1758–1789, 2013.

Yuan, G. and Ghanem, B. An exact penalty method for
In
binary optimization based on MPEC formulation.
AAAI Conference on Artiﬁcial Intelligence (AAAI), pp.
2867–2875, 2017.

Yuan, G., Shen, L., and Zheng, W.-S. A block decompo-
sition algorithm for sparse optimization. In ACM Inter-
national Conference on Knowledge Discovery and Data
Mining (SIGKDD), pp. 275–285, 2020.

Nesterov, Y. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Yuan, X., Li, P., and Zhang, T. Gradient hard thresholding
pursuit. Journal of Machine Learning Research, 18:166:1–
166:43, 2017.

Pang, J., Razaviyayn, M., and Alvarado, A. Computing
b-stationary points of nonsmooth DC programs. Mathe-
matics of Operations Research, 42(1):95–118, 2017.

Zhang, T. Analysis of multi-stage convex relaxation for
sparse regularization. The Journal of Machine Learning
Research, 11:1081–1107, 2010.

Patrascu, A. and Necoara, I. Efﬁcient random coordinate
descent algorithms for large-scale structured nonconvex
optimization. Journal of Global Optimization, 61(1):
19–46, 2015.

Zhang, X., Yu, Y., Wang, L., and Gu, Q. Learning one-
hidden-layer relu networks via gradient descent. In Inter-
national Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 1524–1534, 2019.

Razaviyayn, M., Hong, M., and Luo, Z. A uniﬁed conver-
gence analysis of block successive minimization methods
for nonsmooth optimization. SIAM Journal on Optimiza-
tion, 23(2):1126–1153, 2013.

Coordinate Descent Methods for DC Minimization

Appendix

The appendix is organized as follows.
Section A presents the mathematical proofs for the theoretical analysis.
Section B shows more examples of the breakpoint searching methods for proximal operator computation.
Section C demonstrates some more experiments.
Section D provides some discussions and extensions of our methods.

A. Mathematical Proofs

A.1. Proof for Lemma 5.3

Proof. (a) For any x ∈ Rn, we derive the following equalities:

1
n

n
(cid:88)

i=1

(cid:107)x + diei(cid:107)2

2 =

=

=

1
n

1
n
1
n

(cid:107)d(cid:107)2

2 +

2
n

(cid:104)x, d(cid:105) + (cid:107)x(cid:107)2
2

(cid:104)x, d(cid:105) +

2 + (cid:107)x(cid:107)2

2 −

2 +

(cid:107)d(cid:107)2

2
n
(cid:107)d + x(cid:107)2

2 + (1 −

(cid:107)x(cid:107)2

1
n
)(cid:107)x(cid:107)2
2

1
n

1
n

(cid:107)x(cid:107)2
2

(b) The proof for this equality is almost the same as Lemma 1 in (Lu & Xiao, 2015). For completeness, we include the proof
here. We have the following results:

1
n

n
(cid:88)

i=1

h(x + diei) =

=

=



hi(xi + di) +

(hi(xi + di)) +



hj(xj)



(cid:88)

j(cid:54)=i

1
n

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

hj(xj)

n
(cid:88)

i=1

n
(cid:88)

i=1

h(x + d) +

n − 1
n

h(x)

1
n

1
n

1
n

(c) We derive the following results:

1
n

1
n

(a)
≤

n
(cid:88)

f (x + diei)

i=1
(cid:32) n
(cid:88)

i=1

f (x) + (cid:104)∇f (x), diei(cid:105) +

(cid:33)

ci
2

(di)2

(b)
= f (x) +

1
n

[(cid:104)∇f (x), d(cid:105) +

1
2

(cid:107)d(cid:107)2
c]

where step (a) uses (2); step (b) uses (cid:80)n

= (1 −

1
n

)f (x) +

1
2
i=1(cid:104)∇f (xt), diei(cid:105) = (cid:104)∇f (xt), d(cid:105) and (cid:80)n

[f (x) + (cid:104)∇f (x), d(cid:105) +

1
n

(cid:107)d(cid:107)2
c]

i=1 cid2

i = (cid:107)d(cid:107)2
c.

(d) We derive the following inequalities:

−

1
n

n
(cid:88)

i=1

g(x + diei)

(a)
≤

1
n

n
(cid:88)

i=1

(−g(x) − (cid:104)∂g(x), diei(cid:105)) ,

(b)
= −g(x) −

1
n

(cid:104)∂g(x), d(cid:105)

(20)

where step (a) uses the fact g(x) is convex with −g(y) ≤ −g(x) − (cid:104)∂g(x), y − x(cid:105) for all x and y; step (b) uses the fact
that (cid:80)n

i=1(cid:104)y, diei(cid:105) = (cid:104)y, d(cid:105).

Coordinate Descent Methods for DC Minimization

(e) Using (20) and the fact that −g(x) is ρ-weakly convex with −g(x) ≤ −g(y) − (cid:104)x − y, ∂g(x)(cid:105) + ρ
y = x + d, we obtain:

2 (cid:107)x − y(cid:107)2

2 with

−

1
n

n
(cid:88)

i=1

g(x + diei) ≤ −g(x) +

(cid:16)

1
n

−g(x + d) + g(x) +

(cid:17)

(cid:107)d(cid:107)2
2

ρ
2

A.2. Proof for Lemma 5.7

By the optimality of ¨x, it follows that:

f (¨x) + h(¨x) − g(¨x) ≤ f (¨x) + (cid:104)diei, ∇f (¨x)(cid:105) +

ci + θ
2

d2

i + h(¨x + diei) − g(¨x + diei), ∀i = 1, ..., n.

Summing the inequality above over i = 1, ..., n, we have:

nh(¨x) − ng(¨x) ≤

(a)
≤

1
2

1
2

(cid:107)d(cid:107)2

(c+θ) + (cid:104)d, ∇f (¨x)(cid:105) +

n
(cid:88)

i=1

h(¨x + diei) −

n
(cid:88)

i=1

g(¨x + diei)

(cid:107)d(cid:107)2

(c+θ) + (cid:104)d, ∇f (¨x)(cid:105) + (n − 1)h(¨x) + h(¨x + d) − (n − 1)g(¨x) − g(¨x + d) +

ρ
2

(cid:107)d(cid:107)2
2

where step (a) uses (14) and (17) as shown in Lemma 5.3. Rearranging terms, we obtain the following inequalities:

0 ≤

(a)
≤

(b)
=

1
2
1
2
1
2

(cid:107)d(cid:107)2

(c+θ) + (cid:104)d, ∇f (¨x)(cid:105) − h(¨x) + h(¨x + d) + g(¨x) − g(¨x + d) +

ρ
2

(cid:107)d(cid:107)2
2

(cid:107)d(cid:107)2

(c+θ) − h(¨x) + h(¨x + d) + g(¨x) − g(¨x + d) +

ρ
2

(cid:107)d(cid:107)2

2 + f (¨x + d) − f (¨x)

(cid:107)d(cid:107)2

(c+θ+ρ) − F (¨x) + F (¨x + d)

where step (a) uses the convexity of f (·) that:

(cid:104)∇f (¨x), (¨x + d) − ¨x(cid:105) ≤ f (¨x + d) − f (¨x);

step (b) uses the deﬁnition of F (x) = f (x) + h(x) − g(x).

A.3. Proof for Theorem 5.8

Proof. (a) {Optimal Point ¯x} ⊆ {Coordinate-Wise Stationary Point ¨x}. By the optimality of ¯x, it follows that:

Since the gradient of f (·) is coordinate-wise Lipschitz continuous, we have:

f (¯x) + h(¯x) − g(¯x) ≤ f (x) + h(x) − g(x), ∀x

f (¯x + diei) ≤ f (¯x) + (cid:104)∇if (¯x), diei) +

ci
2

d2

i , ∀di

Adding these two inequalities together, we obtain:

f (¯x + diei) + h(¯x) − g(¯x) ≤ f (x) + h(x) − g(x) + (cid:104)∇if (¯x), diei) +

ci
2

d2

i , ∀x

Letting x = ¯x + diei and using the fact that θ ≥ 0, we have:

f (¯x + diei) + h(¯x) − g(¯x) ≤ f (¯x + diei) + h(¯x + diei) − g(¯x + diei) + (cid:104)∇if (¯x), diei) +

ci + θ
2

d2

i , ∀di

⇒ h(¯x) − g(¯x) ≤ h(¯x + diei) − g(¯x + diei) + (cid:104)∇if (¯x), diei) +

ci + θ
2

d2

i , ∀di

⇒ Mi(¯x, 0) = min

η

Mi(¯x, η), ∀i

Coordinate Descent Methods for DC Minimization

where the last step uses the fact that Mi(¯x, 0) = h(¯x) − g(¯x).

(b) {Coordinate-Wise Stationary Point ¨x} ⊆ {Directional Point `x}. Letting d = t(y − ¨x) the inequality in Lemma 5.7,
we have the following results:

lim
t↓0

F (¨x + t(y − ¨x)) − F (¨x)
t

≥ lim
t↓0

= lim
t↓0

−(cid:107)t(y − ¨x)(cid:107)2

(c+θ+ρ)

−t2 1

t
2 (cid:107)y − ¨x(cid:107)2
t

(c+θ+ρ)

= 0

Therefore, any coordinate-wise stationary point ¨x is also a directional point `x.

(c) {Directional Point `x} ⊆ {Critical Point ˇx}. Since −g(x) is weakly convex, we have:

−g(z) ≤ −g(x) − (cid:104)z − x, ∂g(z)(cid:105) +

ρ
2

(cid:107)x − z(cid:107)2

2, ∀z, x.

Since f (·) and h(·) are convex, we have:

f (z) + h(z) ≤ f (x) + h(x) − (cid:104)x − z, ∇f (z) + ∂h(z)(cid:105), ∀z, x.

Adding these two inequalities together, we ontain:

F (z) − F (x) ≤ (cid:104)z − x, ∂F (z)(cid:105) +

ρ
2

(cid:107)x − z(cid:107)2

2, ∀z, x.

We derive the following inequalities:

∀y ∈ dom(F ), 0 ≤

(a)
≤

=

(b)
=

lim
t↓0

lim
t↓0

lim
t↓0

lim
t↓0

F (`x + t(y − `x)) − F (`x)
t

(cid:104)`x + t(y − `x) − `x, ∂F (`x + t(y − `x))(cid:105) + ρ

2 (cid:107)`x + t(y − `x) − `x(cid:107)2

2

(cid:104)y − `x, ∂F (`x)(cid:105) +

t

ρ · t
2

(cid:107)y − `x(cid:107)2
2,

(cid:104)y − `x, ∂F (`x)(cid:105) + 0,

(21)

(22)

,

where step (a) uses (21) with z = `x + t(y − `x) and x = `x; step (b) uses `x + t(y − `x) = `x as t ↓ 0.

Noticing the inequality above holds for all y only when 0 ∈ ∂F (`x), we conclude that any directional point `x is also a
critical point ˇx.

A.4. Proof for Theorem 5.9

Proof. (a) We now prove the global convergence for CD-SNCA. Since ¯ηt is the global optimal solution to Problem (10), we
have:

f (xt) + (cid:104)¯ηteit, ∇f (xt)(cid:105) +

(¯ηt)2 + h(xt + ¯ηteit) − g(xt + ¯ηteit)

≤ f (xt) + (cid:104)ηeit, ∇f (xt)(cid:105) +

η2 + h(xt + ηeit ) − g(xt + ηeit), ∀η.

cit + θ
2
cit + θ
2

Letting η = 0 and using the fact that xt+1 = xt + ¯ηt · eit, we obtain:

f (xt) + (cid:104)xt+1 − xt, ∇f (xt)(cid:105) + 1

2 (cid:107)xt+1 − xt(cid:107)2

c+θ + h(xt+1) − g(xt+1) ≤ F (xt)

(23)

Coordinate Descent Methods for DC Minimization

We derive the following results:

F (xt+1) − F (xt)

(a)
≤ F (xt+1) − f (xt) − (cid:104)xt+1 − xt, ∇f (xt)(cid:105) − 1

(b)
= f (xt+1) − f (xt) − (cid:104)xt+1 − xt, ∇f (xt)(cid:105) − 1

2 (cid:107)xt+1 − xt(cid:107)2
2 (cid:107)xt+1 − xt(cid:107)2

c+θ,

c+θ − h(xt+1) + g(xt+1),

(c)
≤ −

θ
2

(cid:107)xt − xt+1(cid:107)2
2,

where step (a) uses (23); step (b) uses the deﬁnition F (xt+1) = f (xt+1) + h(xt+1) − g(xt+1); step (c) uses the fact that
∇f (·) is coordinate-wise Lipschitz.

Taking the expectation, we obtain a lower bound on the expected progress made by each iteration for CD-SNCA:

Eit[F (xt+1)] − F (xt) ≤ −Eit[ θ

2n (cid:107)xt+1 − xt(cid:107)2]

Summing up the inequality above over t = 0, 1, ..., T , we have:

E

ξ(T −1) [

θ
2

T
(cid:88)

t=0

(cid:107)xt+1 − xt(cid:107)2

2] ≤ nEξT [F (x0) − F (xT +1)] ≤ nE[F (x0) − F (¯x)]

Therefore, we conclude that: mint→∞ (cid:107)xt+1 − xt(cid:107)2

2 = 0.

Assume that CD-SNCA converges to a stationary point which is not the coordinate-wise stationary point. In expectation
there exists a coordinate it such that 0 (cid:54)= ¯ηt (cid:44) arg minη f (xt) + (cid:104)ηeit , ∇f (xt)(cid:105) + ci+θ
2 η2 + h(xt + ηeit) − g(xt + ηeit ).
However, according to the fact that xt+1 = xt and subproblem (10) in Algorithm 1, we have ¯ηt = 0 for all t. This
contradicts with the fact that xt+1 = xt as t → ∞. We conclude that 0 ∈ arg minη f (xt) + (cid:104)ηeit, ∇f (xt)(cid:105) + ci+θ
2 η2 +
h(xt + ηeit) − g(xt + ηeit) for all t, and xt converges to the coordinate-wise stationary point.

(b) We now prove the global convergence for CD-SCA. Since ∇f (·) is coordinate-wise Lipschitz continuous, we have:

f (xt+1) ≤ f (xt) + (cid:104)xt+1 − xt, ∇f (xt)(cid:105) +

1
2

(cid:107)xt+1 − xt(cid:107)2
c

Since h(·) is convex, we have:

Since g(·) is convex, we have:

h(xt+1) ≤ h(xt) − (cid:104)xt − xt+1, ∇h(xt+1)(cid:105)

−g(xt+1) ≤ −g(xt) − (cid:104)∂g(xt), xt+1 − xt(cid:105)

Adding these three inequalities together, we have:

F (xt+1) − F (xt)

≤ (cid:104)xt+1 − xt, ∇f (xt) + ∂h(xt+1) − ∂g(xt)(cid:105) + 1

2 (cid:107)xt+1 − xt(cid:107)2

c

(a)
= (cid:104)¯ηteit, ∇f (xt) + ∂h(xt+1) − ∂g(xt)(cid:105) +
ci
2

= ¯ηt(∇f (xt) + ∂h(xt+1) − ∂g(xt))it +

(cid:107)¯ηteit(cid:107)2
2

cit
2
(¯ηt)2

(b)
= −

c + 2θ
2

(¯ηt)2

(c)
≤ −

min(c) + 2θ
2

(cid:107)xt+1 − xt(cid:107)2

step (a) uses the fact that xt+1 − xt = ¯ηteit; step (b) uses the optimality condition for (11) that 0 ∈ [∇f (xt) + ∂h(xt+1) −
∂g(xt+1)]it + (cit + θ)¯ηt; step (c) uses (¯ηt)2 = (cid:107)xt+1 − xt(cid:107)2.

Coordinate Descent Methods for DC Minimization

Using similar strategies with CD-SNCA, we obtain: Eξt−1[mint
i=0 (cid:107)xi+1 − xi(cid:107)2
is only guaranteed to converge to a point ˇx that satisﬁes the following condition:

2] ≤ 2[F (x0)−F (¯x)]

β(t+1)

. We notice that CD-SCA

Therefore, CD-SCA converges to a critical point of Problem 1.

0 ∈ [∇f (ˇx) + ∂h(ˇx) − ∂g(ˇx)]i, ∀i.

A.5. Proof for Theorem 5.10

Proof. The optimality condition for the nonconvex subproblem as in (10) can be written as:

0 ∈ ∇itf (xt) + ¯cit ¯ηt + ∂ith(xt+1) − ∂itg(xt+1)

By (13), (14), (15), and (17) in Lemma 5.3, we have the following useful inequalities:

Eit[(cid:107)xt+1 − ¯x(cid:107)2

¯c =

Eit[h(xt+1)] =

1
n
1
n
1
n
−Eit[g(xt+1)] ≤ −

Eit[f (xt+1)] ≤

(cid:107)xt+1 − ¯x(cid:107)2

)(cid:107)xt − ¯x(cid:107)2
¯x

1
n
)h(xt)

¯c + (1 −
1
n

h(xt+1) + (1 −

f (xt) +

1
n
g(xt+1) + ρ

1
n

(cid:104)∇f (xt), xt+1 − xt(cid:105) + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

1
n

)f (xt)

2n (cid:107)xt+1 − xt(cid:107)2

2 − (1 −

1
n

)g(x2)

We derive the following inequalities:

Eit[

1
2

(cid:107)xt+1 − ¯x(cid:107)2

¯c] −

1
2

(cid:107)xt − ¯x(cid:107)2
¯c

(a)
= Eit[(cid:104)xt+1 − ¯x, ¯c (cid:12) (xt+1 − xt)(cid:105)] − Eit[

1
2

(cid:107)(xt+1 − xt)(cid:107)2
¯c]

(b)
= Eit[(cid:104)xt+1 − ¯x, −(∇itf (xt) + ∂ith(xt+1) − ∂itg(xt+1)) · eit(cid:105)] − Eit[

(c)
=

1
n

(cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1) − ∂g(xt+1)(cid:105) −

1
2n

(cid:107)xt+1 − xt(cid:107)2
¯c

1
2

(cid:107)(xt+1 − xt)(cid:107)2
¯c]

(24)

(25)

(26)

(27)

(28)

(29)

where step (a) uses uses the Pythagoras relation that: ∀x, y, z, 1
step (b) uses the optimality condition in (24); step (c) uses the fact that Eit[(cid:104)xiteit, y(cid:105)] = 1
n

2 (cid:107)y − z(cid:107)2

2 (cid:107)x − z(cid:107)2

2 − 1

2 = (cid:104)y − x, y − x(cid:105) − 1

2 (cid:107)x − y(cid:107)2
2;

(cid:80)n

j=1 xjyj = 1

n (cid:104)x, y(cid:105).

We now bound the term 1

n (cid:104)¯x − xt+1, −∂g(xt+1)(cid:105) for (29) by the following inequalities:

1
n

(cid:104)¯x − xt+1, −∂g(xt+1)(cid:105)

(a)
≤ −

(b)
≤ −

(c)
= −

(d)
≤ −

1
n
1
n
1
n

1
n

g(¯x) +

g(¯x) +

g(¯x) +

1
n
1
n
1
n

g(xt+1) +

g(xt+1) +

g(xt+1) +

(cid:107)¯x − xt+1(cid:107)2
2

ρ
2n
¯ρ
(cid:107)¯x − xt+1(cid:107)2
¯c
2n
(cid:18)
¯ρ
2

Eit[(cid:107)¯x − xt+1(cid:107)2

¯c] − (1 −

(cid:19)

)(cid:107)¯x − xt(cid:107)2
¯c

1
n

g(¯x) + Eit[g(xt+1)] +

ρ
2n

(cid:107)xt+1 − xt(cid:107)2

2 − (1 −

1
n

)g(xt) +

(cid:18)

¯ρ
2

Eit[(cid:107)¯x − xt+1(cid:107)2

¯c] − (1 −

)(cid:107)¯x − xt(cid:107)2
¯c

(cid:19)

(30)

1
n

where step (a) uses the weakly convexity of −g(·); step (b) uses the fact that ρ(cid:107)v(cid:107)2
step (c) uses (25); step (d) uses (28).

2 ≤ ρ ·

1

min(¯c) (cid:107)v(cid:107)2

¯c = ¯ρ(cid:107)v(cid:107)2

¯c for all v;

We now bound the term 1

n (cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1)(cid:105) for (29) by the following inequalities:

Coordinate Descent Methods for DC Minimization

1
n
1
n
1
n
1
n
1
n

=

(a)
≤

(b)
≤

(c)
≤

(cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1)(cid:105)

(cid:104)¯x − xt, ∇f (xt)(cid:105) +

1
n

(cid:104)¯x − xt+1, ∂h(xt+1)(cid:105) +

1
n

(cid:104)xt − xt+1, ∇f (xt)(cid:105)

[f (¯x) − f (xt) + h(¯x) − h(xt+1)] −

1
n

(cid:104)xt+1 − xt, ∇f (xt)(cid:105)

[f (¯x) + h(¯x) − h(xt+1)] − Eit[f (xt+1)] + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

1
n

)f (xt)

[f (¯x) + h(¯x)] − Eit[h(xt+1)] + (1 −

1
n

)h(xt) − Eit[f (xt+1)] + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

1
n

)f (xt) (31)

where step (a) uses the convexity of f (·) and h(·) that:

(cid:104)¯x − xt, ∇f (xt)(cid:105) ≤ f (¯x) − f (xt),
(cid:104)¯x − xt+1, ∂h(xt+1)(cid:105) ≤ h(¯x) − h(xt+1);

step (b) uses (27); step (c) uses (26).

Combining (29), (30), (31), and using the fact that F (x) = f (x) + h(x) − g(x), we obtain:

(cid:107)xt+1 − ¯x(cid:107)2
¯c]

Eit[

1 − ¯ρ
2
1 − ¯ρ(1 − 1
n )
2
1 − ¯ρ(1 − 1
n )
2

≤

(a)
≤

(cid:107)xt − ¯x(cid:107)2

¯c +

(cid:107)xt − ¯x(cid:107)2

¯c +

1
n
1
n

(F (¯x) − F (xt)) − Eit[F (xt+1)] + F (xt) +

(F (¯x) − F (xt)) − Eit[F (xt+1)] + F (xt) +

ρ
2n
ρ
θ

(cid:107)xt+1 − xt(cid:107)2
2

Eit[F (xt) − F (xt+1)]

(32)

where step (a) uses the sufﬁcient decrease condition that 1
can be rewritten as:

2 (cid:107)xt+1 − xt(cid:107)2

2 ≤ 1

θ (F (xt) − F (xt+1)). The inequality in (32)

(1 − ¯ρ)Eit[rt+1] + (1 +

ρ
θ

)Eit[qt+1] ≤ (1 − ¯ρ)rt + (1 +

ρ
θ

)qt −

qt
n

+

¯ρ
n

rt

We deﬁne

We have:

(cid:36) (cid:44) 1 − ¯ρ, γ (cid:44) (1 +

ρ
θ

)

(cid:36)Eit[rt+1] + γEit[qt+1] ≤ (cid:36)rt + γqt +

¯ρ
n

rt −

qt
n

We now discuss the case when rt ≤ ε with ε being sufﬁciently small such that ε ≤
following inequalities:

2

max(¯c) ( µχ

¯ρ )2. We ﬁrst obtain the

rt (cid:44) 1
2

(cid:107)xt − ¯x(cid:107)2
¯c

(a)
≤

max(¯c)
2

(cid:107)xt − ¯x(cid:107)2
2

(b)
≤

max(¯c)
2

· (

1
µ

(F (xt) − F (¯x)))2 (c)
=

max(¯c)
2

· (

qt
µ

)2,

(36)

where step (a) uses the fact that
the deﬁnition that qt = F (xt) − F (¯x).

1

max(¯c) (cid:107)x(cid:107)2

¯c ≤ (cid:107)x(cid:107)2

2 ≤ 1

min(¯c) (cid:107)x(cid:107)2

¯c, ∀x; step (b) uses the sharpness of F (·); step (c) uses

We deﬁne

A (cid:44) χ
¯ρ

, κ (cid:44)

1

n − ¯ρA
n
A(cid:36) + γ

(37)

(33)

(34)

(35)

and obtain:

Coordinate Descent Methods for DC Minimization

√

√

(a)
≤

rt

rt ·

rt =

√

2
(cid:112)max(¯c)

µχ
¯ρ

√

(b)
≤

rt

χ
¯ρ

qt = Aqt

where step (a) uses the condition that rt ≤ 2

max(¯c) ( µχ

¯ρ )2; step (b) uses (36).

Based on (35), we derive the following inequalities:

(cid:36)Eit[rt+1] + γEit [qt+1] ≤ ((cid:36)rt + γqt) +

(a)
≤ ((cid:36)rt + γqt) +

¯ρ
n
¯ρ
n

rt −

qt
n

Aqt −

qt
n

(b)
≤ ((cid:36)rt + γqt) − κ((cid:36)Aqt + γqt)
(c)
≤ ((cid:36)rt + γqt) − κ((cid:36)rt + γqt)
= (1 − κ)((cid:36)rt + γqt)

(38)

(39)

where step (a) uses rt ≤ Aqt as shown in (38); step (b) uses the deﬁnition of κ in (37); step (c) uses (38) again. Note that
the parameter κ in (37) can be simpliﬁed as:

κ =

1

n − χ
n
χ
¯ρ (cid:36) + γ

=

1 − χ
n(γ + 1− ¯ρ

¯ρ χ)

Solving the recursive formulation as in (52), we have:

Eit−1[(cid:36)rt + γqt] ≤ (1 − κ)t · ((cid:36)r0 + γq0).

Using the fact that qt ≥ 0, we obtain the following result:

Eit−1[rt] ≤ (1 − κ)t · (r0 + γ

(cid:36) q0),

A.6. Proof for Theorem 5.11

Proof. The optimality condition for the convex subproblem as in (11) can be written as:

0 ∈ ∇itf (xt) + ¯cit ¯ηt + ∂ith(xt+1) − ∂itg(xt).

By (14), (15), and (16) in Lemma 5.3, we have the following useful inequalities:

Eit[h(xt+1)] =

1
n
1
n
−Eit[g(xt+1)] ≤ −

Eit[f (xt+1)] ≤

h(xt+1) + (1 −

1
n

)h(xt)

f (xt) +

1
n

(cid:104)∇f (xt), xt+1 − xt(cid:105) + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

1
n

)f (xt)

1
n

(cid:104)∂g(xt), xt+1 − xt(cid:105) − g(xt)

We derive the following inequalities:

Eit[

1
2

(cid:107)xt+1 − ¯x(cid:107)2

¯c] −

1
2

(cid:107)xt − ¯x(cid:107)2
¯c

(a)
= Eit[(cid:104)xt+1 − ¯x, ¯c (cid:12) (xt+1 − xt)(cid:105)] − Eit[

1
2

(cid:107)(xt+1 − xt)(cid:107)2
¯c]

(b)
= Eit[(cid:104)xt+1 − ¯x, −(∇itf (xt) + ∂ith(xt+1) − ∂itg(xt)) · eit(cid:105)] − Eit[

(c)
=

1
n

(cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1) − ∂g(xt)(cid:105) −

1
2n

(cid:107)xt+1 − xt(cid:107)2
¯c

1
2

(cid:107)(xt+1 − xt)(cid:107)2
¯c]

(40)

(41)

(42)

(43)

(44)

Coordinate Descent Methods for DC Minimization

where step (a) uses uses the Pythagoras relation that: ∀x, y, z, 1
step (b) uses the optimality condition in (40); step (c) uses the fact that Eit[(cid:104)xiteit, y(cid:105)] = 1
n

2 (cid:107)y − z(cid:107)2

2 (cid:107)x − z(cid:107)2

2 − 1

2 = (cid:104)y − x, y − x(cid:105) − 1

2 (cid:107)x − y(cid:107)2
2;

(cid:80)n

j=1 xjyj = 1

n (cid:104)x, y(cid:105).

We now bound the term 1

n (cid:104)¯x − xt+1, −∂g(xt)(cid:105) for (44) by the following inequalities:

(cid:104)¯x − xt+1, −∂g(xt)(cid:105)

1
n
1
n

=

(cid:104)¯x − xt, −∂g(xt)(cid:105) +

(cid:104)xt − xt+1, −∂g(xt)(cid:105)

(a)
≤ −

(a)
≤ −

1
n
1
n

g(¯x) +

g(¯x) +

1
n
1
n

g(xt) +

g(xt) +

(cid:107)¯x − xt(cid:107)2

2 +

1
n

(cid:104)xt+1 − xt, ∂g(xt)

(cid:107)¯x − xt(cid:107)2

2 − g(xt) + Eit[g(xt+1)]

1
n
ρ
2n
ρ
2n

where step (a) uses the weakly convexity of −g(·); step (b) uses (43).

We now bound the term 1

n (cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1)(cid:105) for (44) by the following inequalities:

1
n
1
n
1
n
1
n
1
n

=

(a)
≤

(b)
≤

(c)
≤

(cid:104)¯x − xt+1, ∇f (xt) + ∂h(xt+1)(cid:105)

(cid:104)¯x − xt, ∇f (xt)(cid:105) +

1
n

(cid:104)¯x − xt+1, ∂h(xt+1)(cid:105) +

1
n

(cid:104)xt − xt+1, ∇f (xt)(cid:105)

[f (¯x) − f (xt) + h(¯x) − h(xt+1)] −

1
n

(cid:104)xt+1 − xt, ∇f (xt)(cid:105)

[f (¯x) + h(¯x) − h(xt+1)] − Eit[f (xt+1)] + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

1
n

)f (xt)

[f (¯x) + h(¯x)] − Eit[h(xt+1)] + (1 −

1
n

)h(xt) − Eit[f (xt+1)] + 1

2n (cid:107)xt+1 − xt(cid:107)2

c + (1 −

where step (a) uses the convexity of f (·) and h(·) that:

(cid:104)¯x − xt, ∇f (xt)(cid:105) ≤ f (¯x) − f (xt),
(cid:104)¯x − xt+1, ∂h(xt+1)(cid:105) ≤ h(¯x) − h(xt+1);

step (b) uses (42); step (c) uses (41).

Combining (44), (45), (46), and using the fact that F (x) = f (x) + h(x) − g(x), we obtain:

Eit[
ρ
2n
¯ρ
2n

≤

(a)
≤

(cid:107)xt+1 − ¯x(cid:107)2

1
2
(cid:107)xt − ¯x(cid:107)2

2 +

¯c] −
1
n
1
n

1
2

(cid:107)xt − ¯x(cid:107)2
¯c

F (¯x) − Eit[F (xt+1)] + (1 −

(cid:107)xt − ¯x(cid:107)2

¯c +

F (¯x) − Eit[F (xt+1)] + (1 −

1
n
1
n

)F (xt)

)F (xt)

(45)

1
n

)f (xt) (46)

(47)

(48)

where step (a) uses (cid:107)x(cid:107)2

2 ≤ 1

min(¯c) (cid:107)x(cid:107)2

¯c, ∀x. The inequality in (47) can be rewritten as:

Eit[rt+1] + E[qt+1] ≤ rt +

¯ρ
n

rt −

1
n

qt + qt

We now discuss the case when rt ≤ ε with ε being sufﬁciently small such that ε ≤
following inequality:

2

max(¯c) ( µχ

¯ρ )2. We ﬁrst obtain the

which is the same as (36).

rt ≤

max(¯c)
2

· (

qt
µ

)2,

(49)

We deﬁne

and obtain:

Coordinate Descent Methods for DC Minimization

A (cid:44) χ
¯ρ

, κ (cid:44)

1

n − ¯ρA
n
A + 1

√

√

(a)
≤

rt

rt ·

rt =

√

2
(cid:112)max(¯c)

µχ
¯ρ

√

(b)
≤

rt

χ
¯ρ

qt = Aqt

where step (a) uses the condition that rt ≤ 2

max(¯c) ( µχ

¯ρ )2; step (b) uses (49).

Based on (48), we derive the following inequalities:

Eit[rt+1] + Eit[qt+1] ≤ (rt + qt) +

(a)
≤ (rt + qt) +

¯ρ
n
¯ρ
n

rt −

qt
n

Aqt −

qt
n

(b)
≤ (rt + qt) − κ(Aqt + qt)
(c)
≤ (rt + qt) − κ(rt + qt)
= (1 − κ)(rt + qt)

(50)

(51)

(52)

where step (a) uses rt ≤ Aqt as shown in (51); step (b) uses the deﬁnition of κ in (50); step (c) uses (51) again. Note that
the parameter κ in (50) can be simpliﬁed as:

κ =

1

n − ¯ρA
n
A + 1

=

1 − χ
n(1 + χ
¯ρ )

Solving the recursive formulation as in (52), we have:

Eit−1 [rt + qt] ≤ (1 − κ)t · (r0 + q0).

Using the fact that qt ≥ 0, we obtain the following result:

Eit−1[rt] ≤ (1 − κ)t · (r0 + q0),

B. More Examples of the Breakpoint Searching Method for Proximal Operator Computation

B.1. When g(y) = (cid:107) max(0, Ay)(cid:107)1 and hi(·) (cid:44) 0

Consider the problem: minη
the following equivalent problem: minη
g(x) = (cid:107) max(0, Ax)(cid:107)1 can be transformed to the proximal operator of g(x) = (cid:107)Ax(cid:107)1 with suitable parameters.

2 (a + |a|), we have
2 (cid:107)A(x + ηei)(cid:107)1. Therefore, the proximal operator of

2 η2 + bη − (cid:107) max(0, A(x + ηei))(cid:107)1. Using the fact that max(0, a) = 1

2 (cid:104)1, Aei(cid:105)η − 1

2 η2 + bη − 1

a

a

B.2. When g(y) = (cid:107)Ay(cid:107)2 and hi(·) (cid:44) 0

a

2 η2 + bη − (cid:107)A(x + ηei)(cid:107)p. It can be rewritten as: minη p(η) (cid:44) a

2 η2 + bη + (cid:107)gη + d(cid:107)p.
Consider the problem: minη
(cid:104)g, sign(gη + d) (cid:12) |gη + d|p−1(cid:105). We only focus on
Setting the gradient of p(·) to zero yields: 0 = aη + b + (cid:107)gη − d(cid:107)1−p
p = 2. We obtain: −aη − b = (cid:104)g,gη+d(cid:105)
2(aη + b)2 = ((cid:104)g, gη + d(cid:105))2.
(cid:107)gη−d(cid:107) ⇔ (cid:107)gη − d(cid:107)(−aη − b) = (cid:104)g, gη + d(cid:105) ⇔ (cid:107)gη − d(cid:107)2
Solving this quartic equation we obtain all of its real roots {η1, η2, ..., ηc} with 1 ≤ c ≤ 4. Therefore, Problem (3) at most
contains 4 breakpoints Θ = {η1, η2, ..., ηc} for this example.

p

C. More Experiments

Coordinate Descent Methods for DC Minimization

In this section, we present the experiment results of the approximate binary optimization problem and the generalized linear
regression problem.

C.1. Approximate Binary Optimization

We consider Problem (6). We generate the observation vector via y = max(0, A¨x + randn(m, 1) × 0.1 × (cid:107)A¨x(cid:107)) with
¨x = randn(d, 1). This problem is consistent with f (x) (cid:44) 1
i hi(xi) with
hi(z) (cid:44) I[−1,1](z) where I[−1,1](z) denotes an indicator function on the box constraint (hi(z) = 0 if −1 ≤ z ≤ 1, +∞
otherwise). We notice that ∇f (·) is L-Lipschitz with constant L = (cid:107)G(cid:107)2
2 and coordinate-wise Lipschitz with constant
c = diag(GT G). The subgradient of g(x) at xt can be computed as: ∂g(xt) = − ρxt
(cid:44) gt. We set ρ = 5.
(cid:107)xt(cid:107)

2, g(x) (cid:44) −ρ(cid:107)x(cid:107), and h(x) = (cid:80)n

2 (cid:107)Gx − y(cid:107)2

1

L

2 (cid:107)Gx − y(cid:107)2

We compare with the following methods. (i) Multi-Stage Convex Relaxation (MSCR). It solves the following problem:
xt+1 = arg minx
2 − (cid:104)x − xt, gt(cid:105), s.t.(cid:107)x(cid:107)∞ ≤ 1. This is essentially equivalent to the alternating minimization
method in (Yuan & Ghanem, 2017). (ii) Proximal DC algorithm (PDCA). It considers the following iteration: xt+1 =
2 +(cid:104)x−xt, ∇f (xt)(cid:105)−(cid:104)x−xt, gt(cid:105). (iii) Subgradient method (SubGrad). It uses the following iteration:
2 (cid:107)x−xt(cid:107)2
arg minx
· (∇f (x) − gt)), where PΩ(x) (cid:44) max(−1, min(x, 1)) is the projection operation on the convex set
xt+1 = PΩ(xt − 0.1
t
Ω (cid:44) {x|(cid:107)x(cid:107)∞ ≤ 1}. (iv) CD-SCA solves a convex problem ¯ηt = arg minη 0.5(cit + θ)η2 + [∇f (xt) − gt]itη, s.t. − 1 ≤
it + η ≤ 1 and update xt via xt+1
xt
it + ¯ηt. (v) CD-SNCA computes the nonconvex proximal operator of (cid:96)2 norm
(see Section B.2) as ¯ηt = arg minη
it + η ≤ 1 and update xt via
xt+1
it = xt
As can be seen from Table 3, the proposed method CD-SNCA consistently gives the best performance. This is due to the fact
that CD-SNCA ﬁnds stronger stationary points than the other methods. Such results consolidate our previous conclusions.

it = xt
ci+θ
2 η2 + ∇itf (xt)η − ρ(cid:107)xt + ηei(cid:107), s.t. − 1 ≤ xt

it + ¯ηt.

MSCR
1.336 ± 0.108
randn-256-1024
1.359 ± 0.207
randn-256-2048
2.275 ± 0.096
randn-1024-256
3.569 ± 0.144
randn-2048-256
1.069 ± 0.313
e2006-256-1024
0.936 ± 0.265
e2006-256-2048
2.245 ± 0.534
e2006-1024-256
3.507 ± 0.529
e2006-2048-256
randn-256-1024-C 1.357 ± 0.130
randn-256-2048-C 1.260 ± 0.126
randn-1024-256-C 2.254 ± 0.097
randn-2048-256-C 3.531 ± 0.159
e2006-256-1024-C 1.281 ± 0.628
e2006-256-2048-C 1.254 ± 0.535
e2006-1024-256-C 2.308 ± 0.640
e2006-2048-256-C 3.429 ± 0.687

PDCA
1.336 ± 0.108
1.359 ± 0.207
2.275 ± 0.096
3.569 ± 0.144
1.069 ± 0.313
0.936 ± 0.265
2.245 ± 0.534
3.507 ± 0.529
1.357 ± 0.130
1.261 ± 0.126
2.254 ± 0.097
3.531 ± 0.159
1.323 ± 0.684
1.254 ± 0.535
2.308 ± 0.640
3.429 ± 0.687

SubGrad
1.280 ± 0.098
1.305 ± 0.199
2.268 ± 0.092
3.561 ± 0.143
0.605 ± 0.167
0.640 ± 0.187
1.670 ± 0.198
3.053 ± 0.250
1.302 ± 0.134
1.202 ± 0.122
2.226 ± 0.084
3.520 ± 0.150
0.473 ± 0.128
0.577 ± 0.144
1.570 ± 0.237
2.693 ± 0.335

CD-SCA
1.540 ± 0.236
1.503 ± 0.242
2.380 ± 0.180
3.614 ± 0.162
0.809 ± 0.222
0.798 ± 0.255
1.780 ± 0.238
3.307 ± 0.396
1.586 ± 0.192
1.444 ± 0.099
2.315 ± 0.154
3.544 ± 0.184
0.671 ± 0.257
0.717 ± 0.218
1.837 ± 0.322
2.790 ± 0.287

CD-SNCA
0.046 ± 0.010
0.021 ± 0.004
1.203 ± 0.043
2.492 ± 0.084
0.291 ± 0.025
0.263 ± 0.028
1.266 ± 0.057
2.532 ± 0.191
0.051 ± 0.012
0.019 ± 0.003
1.175 ± 0.045
2.445 ± 0.082
0.302 ± 0.043
0.287 ± 0.029
1.303 ± 0.060
2.431 ± 0.150

Table 3: Comparisons of objective values of all the methods for solving the approximate binary optimization problem. The
1st, 2nd, and 3rd best results are colored with red, green and blue, respectively.

MSCR
0.046 ± 0.019
randn-256-1024
0.023 ± 0.008
randn-256-2048
0.480 ± 0.063
randn-1024-256
1.335 ± 0.205
randn-2048-256
0.046 ± 0.093
e2006-256-1024
0.022 ± 0.009
e2006-256-2048
0.922 ± 0.754
e2006-1024-256
1.031 ± 0.835
e2006-2048-256
randn-256-1024-C 0.036 ± 0.012
randn-256-2048-C 0.019 ± 0.003
randn-1024-256-C 0.462 ± 0.089
randn-2048-256-C 1.155 ± 0.159
e2006-256-1024-C 0.023 ± 0.020
e2006-256-2048-C 0.034 ± 0.029
e2006-1024-256-C 1.772 ± 2.200
e2006-2048-256-C 1.474 ± 1.247

PDCA
0.046 ± 0.019
0.022 ± 0.007
0.473 ± 0.057
1.330 ± 0.205
0.047 ± 0.105
0.025 ± 0.012
0.925 ± 0.758
1.035 ± 0.838
0.036 ± 0.012
0.018 ± 0.003
0.465 ± 0.092
1.157 ± 0.165
0.025 ± 0.023
0.037 ± 0.034
1.788 ± 2.200
1.486 ± 1.249

SubGrad
0.077 ± 0.017
0.060 ± 0.006
0.771 ± 0.089
1.810 ± 0.262
0.050 ± 0.088
0.036 ± 0.040
0.941 ± 0.792
1.075 ± 0.867
0.069 ± 0.014
0.058 ± 0.004
0.768 ± 0.127
1.570 ± 0.238
0.032 ± 0.026
0.036 ± 0.066
1.797 ± 2.294
1.520 ± 1.278

CD-SCA
0.039 ± 0.018
0.021 ± 0.007
0.464 ± 0.059
1.329 ± 0.197
0.047 ± 0.100
0.029 ± 0.039
0.925 ± 0.757
1.024 ± 0.827
0.031 ± 0.012
0.016 ± 0.003
0.463 ± 0.088
1.161 ± 0.168
0.031 ± 0.038
0.034 ± 0.052
1.768 ± 2.195
1.446 ± 1.233

CD-SNCA
0.039 ± 0.019
0.021 ± 0.007
0.461 ± 0.060
1.325 ± 0.197
0.045 ± 0.097
0.020 ± 0.020
0.858 ± 0.717
1.010 ± 0.817
0.030 ± 0.010
0.016 ± 0.003
0.457 ± 0.092
1.147 ± 0.160
0.019 ± 0.018
0.025 ± 0.043
1.702 ± 2.162
1.431 ± 1.224

Table 4: Comparisons of objective values of all the methods for solving the generalized linear regression problem. The 1st,
2nd, and 3rd best results are colored with red, green and blue, respectively.

C.2. Generalized Linear Regression

We consider Problem (7). We have the following optimization problem: minx
2. We generate the
observation vector via y = max(0, A¨x + randn(m, 1) × 0.1 × (cid:107)A¨x(cid:107)) with ¨x = randn(d, 1). This problem is consistent
2 and g(x) (cid:44) (cid:107) max(0, Ax)(cid:107)1 with A = diag(y)G. We notice that ∇f (·)
with Problem (1) with f (x) (cid:44) 1

2 (cid:107) max(0, Gx) − y(cid:107)2

2 (cid:107) max(0, Gx)(cid:107)2

1

Coordinate Descent Methods for DC Minimization

(a) e2006-256-1024

(b) e2006-256-2048

(c) e2006-1024-256

(d) e2006-2048-256

(e) randn-256-1024-C

(f) randn-256-2048-C

(g) randn-1024-256-C

(h) randn-2048-256-C

(i) e2006-256-1024-C

(j) e2006-256-2048-C

(k) e2006-1024-256-C

(l) e2006-2048-256-C

Figure 2: The convergence curve of the compared methods for solving the (cid:96)p norm generalized eigenvalue problem on
different data sets.

is L-Lipschitz with L = (cid:107)G(cid:107)2
computed as: ∂g(xt) = AT max(0, Axt) (cid:44) gt.

2 and coordinate-wise Lipschitz with c = diag(GT G). The subgradient of g(x) at xt can be

We compare with the following methods. (i) Multi-Stage Convex Relaxation (MSCR). It solves the following problem:
xt+1 = arg minx f (x) − (cid:104)x − xt, gt(cid:105). (ii) Proximal DC algorithm (PDCA). It considers the following iteration: xt+1 =
arg minx(cid:104)x−xt, ∇f (x)(cid:105)+ L
2 −(cid:104)x−xt, gt(cid:105). (iii) Subgradient method (SubGrad). It uses the following iteration:
xt+1 = xt − 0.1
t ·(∇f (x)−gt). (iv) CD-SCA solves a convex problem ¯ηt = arg minη 0.5(cit +θ)η2 +[∇f (xt)−ρgt]it ·η
with and update xt via xt+1
it + ¯ηt. (v) CD-SNCA computes the nonconvex proximal operator (see Section B.1) as
¯ηt = arg minη

2 η2 + ∇itf (xt)η − (cid:107) max(0, A(xt + ηei))(cid:107)1 and update xt via xt+1
ci+θ
As can be seen from Table 4, the proposed method CD-SNCA consistently outperforms the other methods.

2 (cid:107)x−xt(cid:107)2

it = xt

it = xt

it + ¯ηt.

C.3. More Experiments on Computational Efﬁciency

Figure 2, Figure 3, Figure 4, and Figure 5 show the convergence curve of the compared methods for solving the (cid:96)p norm
generalized eigenvalue problem, the approximate sparse optimization problem, the approximate binary optimization problem,
and the generalized linear regression problem, respectively. We conclude that CD-SNCA at least achieves comparable
efﬁciency, if it is not faster than the compared methods. However, it generally achieves lower objective values than the other
methods.

D. Discussions and Extensions

This section presents some discussions and extensions of our method. First, we discuss the equivalent reformulations for
the (cid:96)p norm generalized eigenvalue problem (see Section D.1). Second, we extend our method to solve the classical PCA
problem, and provide a local analysis for it (see Section D.2). Third, we use several examples to explain the optimality

10-310-2Time (seconds)10-10100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-2Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)10-10100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)10-10100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10−310−210−110−5100Time (seconds)Objective  MSCRPDCASubGradCD−SCACD−SNCA10-2100Time (seconds)10-5100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-2Time (seconds)10-5100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)10-5100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)10-5100ObjectiveMSCRPDCASubGradCD-SCACD-SNCACoordinate Descent Methods for DC Minimization

(a) randn-256-1024

(b) randn-256-2048

(c) randn-1024-256

(d) randn-2048-256

(e) e2006-256-1024

(f) e2006-256-2048

(g) e2006-1024-256

(h) e2006-2048-256

(i) randn-256-1024-C

(j) randn-256-2048-C

(k) randn-1024-256-C

(l) randn-2048-256-C

(m) e2006-256-1024-C

(n) e2006-256-2048-C

(o) e2006-1024-256-C

(p) e2006-2048-256-C

Figure 3: The convergence curve of the compared methods for solving the approximate sparse optimization problem on
different data sets.

hierarchy between the optimality conditions (see Section D.3).

D.1. Equivalent Reformulations for the (cid:96)p Norm Generalized Eigenvalue Problem

First of all, using classical Lagigian dual theory, Problem (1) is equvilent to the following optimization problem.

min
x∈Rn
min
x∈Rn

f (x) + h(x), s.t. g(x) ≥ λ,

h(x) − g(x), s.t. f (x) ≤ δ,

for some suitable λ and δ. For some special problems where the parameters λ and δ that are not important, the two
constrained problems above can be solved by ﬁnding the solution to Problem (1).

10-2100Time (seconds)10-1100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)1.41.61.822.2ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)3.63.844.24.44.6ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)10-1100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)0.60.81ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100102Time (seconds)0.811.2ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)10-1100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)2.22.42.62.833.2ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)33.23.43.63.84ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100102Time (seconds)10-2100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100102Time (seconds)10-2100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100102Time (seconds)1.41.61.82ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100102Time (seconds)1212.51313.514ObjectiveMSCRPDCASubGradCD-SCACD-SNCACoordinate Descent Methods for DC Minimization

(a) randn-256-1024

(b) randn-256-2048

(c) randn-1024-256

(d) randn-2048-256

(e) e2006-256-1024

(f) e2006-256-2048

(g) e2006-1024-256

(h) e2006-2048-256

(i) randn-256-1024-C

(j) randn-256-2048-C

(k) randn-1024-256-C

(l) randn-2048-256-C

(m) e2006-256-1024-C

(n) e2006-256-2048-C

(o) e2006-1024-256-C

(p) e2006-2048-256-C

Figure 4: The convergence curve of the compared methods for solving the approximate binary optimization problem on
different data sets.

We pay special attention to the following problems with Q (cid:31) 0:

min
x

F1(x) (cid:44) α

2 xT Qx − (cid:107)Ax(cid:107)p
F2(x) (cid:44) −(cid:107)Ax(cid:107)p, s.t. xT Qx ≤ 1

F3(x) (cid:44) 1

2 xT Qx, s.t. (cid:107)Ax(cid:107)p ≥ 1

min
x
min
x

(53)

(54)

(55)

The following proposition establish the relations between Problem (53), Problem (54), and Problem (55).

Proposition D.1. We have the following results.
(a) If ¯x is an optimal solution to (53), then ±¯x(¯xT Q¯x)− 1

2 and ±¯x

(cid:107)A¯x(cid:107)p

are respectively optimal solutions to (54) and (55).

(b) If ¯y is an optimal solution to (54), then ±(cid:107)A¯y(cid:107)p·¯y

are respectively optimal solutions to (53) and (55).

(c) If ¯z is an optimal solution to (55), then ±¯z(cid:107)A¯z(cid:107)p

2 are respectively optimal solutions to (53) and (54).

α¯yT Q¯y and ±¯y
(cid:107)A¯y(cid:107)p
α¯zT Q¯z and ±¯z(¯zT Q¯z)− 1

10-310-210-1Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-210-1Time (seconds)5101520ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-210-1Time (seconds)100101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-210-1Time (seconds)100101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)100101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-310-210-1Time (seconds)101ObjectiveMSCRPDCASubGradCD-SCACD-SNCACoordinate Descent Methods for DC Minimization

(a) randn-256-1024

(b) randn-256-2048

(c) randn-1024-256

(d) randn-2048-256

(e) e2006-256-1024

(f) e2006-256-2048

(g) e2006-1024-256

(h) e2006-2048-256

(i) randn-256-1024-C

(j) randn-256-2048-C

(k) randn-1024-256-C

(l) randn-2048-256-C

(m) e2006-256-1024-C

(n) e2006-256-2048-C

(o) e2006-1024-256-C

(p) e2006-2048-256-C

Figure 5: The convergence curve of the compared methods for solving the generalized linear regression problem on different
data sets.

Proof. Using the Lagrangian dual, we introduce a multiplier θ1 > 0 for the constraint xT Qx ≤ 1 as in Problem (54) and a
multiplier θ2 > 0 for the constraint −(cid:107)Ax(cid:107)p ≤ −1 as in Problem (55), leading to the following optimization problems:

minx

θ1
2 xT Qx − (cid:107)Ax(cid:107)p

minx

1

2 xT Qx − θ2(cid:107)Ax(cid:107)p ⇔ minx

1
2θ2

xT Qx − (cid:107)Ax(cid:107)p

,

.

These two problems have the same form as Problem (53). It is not hard to notice that the gradient of F1(·) can be computed
as:

∇F1(x) = αQx − (cid:107)Ax(cid:107)1−p

p AT (sign(Ax) (cid:12) |Ax|p−1)

By the ﬁrst-order optimality condition, we have:

x =

1
α

Q−1 (cid:0)(cid:107)Ax(cid:107)1−p

p AT (sign(Ax) (cid:12) |Ax|p−1)(cid:1)

Therefore, the optimal solution for Problem (53), Problem (54), and Problem (55) only differ by a scale factor.

10-2100Time (seconds)0.30.40.50.60.7ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)0.30.40.50.60.70.8ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)1.522.5ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)44.555.5ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)22.12.22.32.4ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)0.30.40.50.60.70.8ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)0.30.40.50.60.7ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)11.21.41.61.82ObjectiveMSCRPDCASubGradCD-SCACD-SNCA10-2100Time (seconds)44.55ObjectiveMSCRPDCASubGradCD-SCACD-SNCACoordinate Descent Methods for DC Minimization

(a) Since ¯x is the optimal solution to (53), the optimal solution to Problem (54) and (55) can be respectively computed as
α2 ¯x and α3 ¯x with

α2 = arg min

α

α3 = arg min

α

F2(α¯x), s.t. (α¯x)T Q(α¯x) ≤ 1

F3(α¯x), s.t. (cid:107)α¯x(cid:107)p ≥ 1

After some preliminary calculations, we have: α2 = ±1/(cid:112)¯xT Q¯x and α3 = ±1/(cid:107)A¯x(cid:107)p.
(b) Since ¯y is an optimal solution to (54), the optimal solution to Problem (53) and Problem (55) can be respectively
computed as α1 ¯y and α3 ¯y with

α1 = arg min

α

α3 = arg min

α

F1(α¯y)

F3(α¯y), s.t. (cid:107)α¯y(cid:107)p ≥ 1

Therefore, α1 = ± (cid:107)A¯y(cid:107)p

α¯yT Q¯y and α3 = ±1/(cid:107)A¯y(cid:107)p.

(c) Since ¯z is an optimal solution to (55), the optimal solution to Problem (53) and Problem (54) can be respectively
computed as α1¯z and α2¯z with

α1 = arg min

α

F1(α¯z)

α2 = arg min

α

F2(α¯z), s.t. (α¯z)T Q(α¯z) ≤ 1

Therefore, α1 = ± (cid:107)A¯z(cid:107)p

α¯zT Q¯z and α2 = ±1/(cid:112)¯zT Q¯z.

D.2. A Local Analysis for the PCA Problem

This section provides a local analysis and a convergence rate analysis of Algorithm 1 when it is applied to solve the classical
PCA problem. We ﬁrst rewrite the classical PCA problem as an unconstraint smooth optimization problem and then prove
that it is smooth and strongly convex in the neighborhood of the global optimal solution. Finally, the local linear convergence
rate of the coordinate descent method can directly follow due to Theorem 1 in (Lu & Xiao, 2015).
Given a covariance matrix C ∈ Rn×n with C (cid:60) 0, PCA problem can be formulated as:

Using Proposition D.1, we have the following equivalent problem:

max
v

vT Cv, s.t. (cid:107)v(cid:107) = 1

min
x

F (x) =

√

α
2

(cid:107)x(cid:107)2

2 −

xT Cx.

(56)

for any given constant α > 0. In what follows, consider a positive semideﬁnite matrix C ∈ Rn×n (which is not necessarily
low-rank) with eigenvalue decomposition C = (cid:80)n
i = UT diag(λ)U. We assume that: λ1 ≥ λ2 ≥ ... ≥ λn ≥ 0.
Lemma D.2. The following result holds iff i = 1:

i=1 λiuiuT

O (cid:44) I − 1
λi

C + uiuT

i (cid:31) 0

(57)

Proof. When i = 1, it clearly holds that: λiI − C (cid:23) 0. Combining with the fact that λ1u1uT
λiuiuT

i (cid:31) 0 for i = 1.

1 (cid:31) 0, we have λiI − C +

We now prove that the inequality in (57) may fail to hold for i ≥ 2. We have the following results:

2 − uT

1 Cu1 + λi(uT

1 ui)2

1 Ou1 = λi(cid:107)u1(cid:107)2
uT
(a)
= λi − uT

1 Cu1 + 0
(c)
≤ 0

(b)
= λi − λ1 + 0

Coordinate Descent Methods for DC Minimization

step (a) uses the fact that (cid:107)ui(cid:107) = 1 for all i, and uT
λ1u1 ⇒ uT
positive deﬁnite.

i uj = 0 for all i (cid:54)= j; step (b) uses the fact that Cu1 =
1 Cu1 = λ1; step (c) uses the fact that λ1 ≥ λ2 ≥ ... ≥ λn ≥ 0. Therefore, the matrix O is not

We hereby ﬁnish the proof of this lemma.

Theorem D.3. We have the following results:

(a) The set of critical points of Problem (56) are {{0} ∪ {±

√
λk
α uk : k = 1, ..., n}}.

(b) Problem (56) has at most two local minima {±

√

λ1
α u1} which are the global optima with F (¯x) = − λ1
2α .

Proof. The ﬁrst-order and second-order gradient of F (x) can be computed as:

∇F (x) = αx − Cx√
√

xT Cx

∇2F (x) = α

xT Cx·I−C+α2xxT
xT Cx

√

(a) Setting ∇F (x) = 0 for (58), we have:

√

α

xT Cx · x = Cx

(58)

(59)

(60)

Therefore, we conclude that {
function is symmetric and the trivial solution {0}, we ﬁnish the proof of the ﬁrst part of this lemma.

√
λk
α uk, k = 1, ..., n} are feasible solutions to (60). Taking into account that the objective

(b) For any nontrivial (nonzero) critical point zi = ±

√
λi
α ui, we have the following results from (59):

=

∇2F (zi)
√
(cid:112)
(

α ·

√

λi

√

(

√

λiui)T C(
√
(cid:112)

λiui) · I − C + λiuiuT
i
λiui)T C(
λiui)
(cid:112)uT
i Cui · I − C + λiuiuT
i
√
i Cui

=

α ·

(a)
=

α ·

λi · (cid:112)uT
λi · I − C + λiuiuT
i
λi

= α · (I − 1
λi

C + uiuT
i )

where step (a) uses the fact that Cui = λiui ⇒ uT
only when i = 1. Therefore, the global optimal solution can be computed as ¯x = ±
Cu1 = λ1u1, we have the following results:

i Cui = λi. Using Lemma D.2, we conclude that ∇2F (zi) (cid:31) 0 holds
√
λ1
α u1. Finally, using the fact that

√

(cid:107)¯x(cid:107)2

2 −

¯xT C¯x
√

1 u1 −

1 u1 −

λ1
α
√
λ1
α

(cid:113)

(cid:113)

uT

1 Cu1

uT

1 λ1u1

F (¯x) =

=

=

α
2
α
2
α
2

= −

λ1
α2 uT
λ1
α2 uT
λ1
.
2α

To simplify our discussions, we only focus on α = 1 for Problem (56) in the sequel.

The following lemma is useful in our analysis.

Lemma D.4. Assume that 0 < ϕ < 1. We deﬁne

Coordinate Descent Methods for DC Minimization

K(ϕ) (cid:44) −1 + 3

(cid:114) 1

1 − ϕ

+ ¯τ + 3

(cid:114) 1

1 − ϕ

(cid:115)

− ¯τ , ¯τ =

1
(1 − ϕ)2 +

1
27(1 − ϕ)3

We have the following result:

0 ≤ ϑ ≤ K(ϕ) ⇒ 1 − ϑ − (1 + ϑ)3(1 − ϕ) ≥ 0.

Proof. We focus on the following equation:

Dividing both sides by 1 − ϕ, we have the following equivalent equation:

1 − ϑ − (1 + ϑ)3(1 − ϕ) = 0

)3 +

(1 + ϑ
(cid:124) (cid:123)(cid:122) (cid:125)
x

1
(1 − ϕ)
(cid:124) (cid:123)(cid:122) (cid:125)
p

) +

·(1 + ϑ
(cid:124) (cid:123)(cid:122) (cid:125)
x

= 0

−2
(1 − ϕ)
(cid:124) (cid:123)(cid:122) (cid:125)
q

(61)

(62)

Solving the depressed cubic equation x3 + px + q = 0 above using the well-known Cardano’s formula 1, we obtain the
unique root ¯x with

(cid:114)
¯x = 3

−

q
2

(cid:114)
+ ¯τ + 3

−

q
2

− ¯τ , ¯τ =

(cid:114)

q2
4

+

p3
27

Using the relations p = 1

1−ϕ and q = − 2

1−ϕ as in (62), we have:

(cid:115)
3

(− 2
1−ϕ )
2

−

¯x =

(cid:115)
3

(− 2
1−ϕ )
2

−

+ ¯τ +

− ¯τ , ¯τ =

(cid:115)

(− 2
1−ϕ )2
4

+

( 1
1−ϕ )3
27

Therefore, ¯ϑ = ¯x − 1 = K(ϕ) is the unique root for 1 − ϑ − (1 + ϑ)3(1 − ϕ) = 0. Hereby, we ﬁnish the proof of this
lemma.

Theorem D.5. We deﬁne δ (cid:44) 1 − λ2
λ1
sufﬁciently close to the global optimal solution ¯x such that (cid:107)x − ¯x(cid:107) ≤ (cid:36) with (cid:36) < ¯(cid:36) (cid:44) min{

−1 − 3√
λ1

(1 + 3√
λ1

, ξ (cid:44) λ1
6

)2 + 12
λ1

+

δ

√

(cid:16)

(cid:113)

(cid:17)

√

(a)

λ1 − (cid:36) ≤ (cid:107)x(cid:107) ≤

√

λ1 + (cid:36).

(b) λ1 − (cid:36)
(c) λ1u1uT

√

λ1 ≤

√

xT Cx ≤ λ1 + (cid:36)

√

λ1.

1 + ρI (cid:23) xxT (cid:23) λ1u1uT

(d) τ I (cid:23) ∇2F (x) (cid:23) σI with σ (cid:44) 1 − λ2
λ1

√

1 − ρI with ρ (cid:44) 3(cid:36)2 + 2(cid:36)
) − 3(cid:36)2
λ1

− (cid:36)(1 + 3√
λ1

λ1.
> 0 and τ (cid:44) 1 + λ2
1(
(λ1−(cid:36)

√

λ1+(cid:36))2
λ1)3 .

√

. Assume that 0 < δ < 1. When x is

λ1K( λ2
λ1

), ξ}, we have:

Proof. (a) We have the following results: (cid:107)x(cid:107) ≤ (cid:107)x − ¯x(cid:107) + (cid:107)¯x(cid:107) = (cid:107)x − ¯x(cid:107) +
(cid:107)x(cid:107) ≥ (cid:107)¯x(cid:107) − (cid:107)x − ¯x(cid:107) =

√

λ1 − (cid:36).

√

(b) We note that the matrix norm is deﬁned as: (cid:107)x(cid:107)C (cid:44)
notice that (cid:107)¯x(cid:107) =
(cid:107)x − ¯x(cid:107)C + λ1 ≤ (cid:36)

xT Cx. It satisﬁes the triangle inequality since C (cid:23) 0. We
λ1, (cid:107)¯x(cid:107)C = λ1. We deﬁne ∆ (cid:44) x − ¯x. We have the following results: (cid:107)x(cid:107)C ≤ (cid:107)x − ¯x(cid:107)C + (cid:107)¯x(cid:107)C =
√

λ1 + λ1. Moreover, we have: (cid:107)x(cid:107)C ≥ (cid:107)¯x(cid:107)C − (cid:107)x − ¯x(cid:107)C = λ1 − (cid:36)

λ1.

√

√

√

λ1 ≤ (cid:36) +

√

λ1. Moreover, we have:

1https://en.wikipedia.org/wiki/Cubic_equation

(c) We have the following inequalities:

Coordinate Descent Methods for DC Minimization

xxT = λ1u1uT
(cid:23) λ1u1uT
(cid:23) λ1u1uT
= λ1u1uT

1 + ∆xT + x∆T − ∆∆T
1 − (cid:107)∆(cid:107)(cid:107)x(cid:107) − (cid:107)x(cid:107)(cid:107)∆(cid:107) − (cid:107)∆(cid:107)(cid:107)∆(cid:107)
1 − (cid:36)(
1 − ρI

λ1 + (cid:36)) − (

(cid:112)

(cid:112)

λ1 + (cid:36))(cid:36) − (cid:36)2

Using similar strategies, we have: xxT (cid:22) (λ1u1uT
(d) First, we prove that ∇2F (x) (cid:23) σI. We deﬁne γ (cid:44) λ1 − (cid:36)
we invoke Lemma D.4 with ϑ (cid:44) (cid:36)√
and obtain:
λ1

and ϕ (cid:44) λ2
λ1

√

1 + (cid:107)∆(cid:107)(cid:107)x(cid:107) + (cid:107)x(cid:107)(cid:107)∆(cid:107) + (cid:107)∆(cid:107)(cid:107)∆(cid:107))I.

λ1 and η (cid:44) (λ1 + (cid:36)

λ1)3. Noticing (cid:36) <

√

√

λ1K( λ2
λ1

),

1 − ϑ ≥ (1 + ϑ)3(1 − λ2/λ1)

⇔

1 − ϑ
(1 − ϑ)(1 + ϑ)3 ≥

(1 + ϑ)3(1 − λ2/λ1)
(1 − ϑ)(1 + ϑ)3

λ2/λ1
1 − ϑ

1

(1 + ϑ)3 ≥ −
λ3
1
√
(λ1 + (cid:36)

λ1

+

√

⇔ −

1
1 − ϑ

+

⇔ −

⇔

λ1
λ1 − (cid:36)
λ1
γ

−

λ3
1
η

≥ −

λ2
γ

≥ −

λ2
λ1 − (cid:36)

√

λ1

λ1)3

(63)

δ <

We now prove that (cid:36) ≤ 1 − λ2
λ1
c2 + ( 6
λ1
(cid:16)

δ ⇒ c2 + 12
λ1

δ)2 + 12c
λ1
(cid:113)
c2 + 12
λ1

−c +

λ1
6

(cid:17)

δ

. We have the following inequalities: 1 < c ⇒ 12
λ1
(cid:113)
δ ⇒

δ)2 ⇒

δ < ( 6
λ1
(cid:16)
−c +

δ < c + 6
λ1

δ)2 + 12c
λ1
c2 + 12
λ1

(cid:113)

(cid:17)

δ

δ ⇒ c2 + 12
λ1

< 6
λ1

δ ⇒

δ < (c + 6
λ1
< δ. Applying this inequality with c (cid:44) 1 + 3√
λ1

c2 + 12
λ1

, we have:

δ >

λ1
6

(cid:32)

−1 −

3
√
λ1

(cid:115)

+

(1 +

3
√
λ1

)2 +

(cid:33)

12
λ1

δ

(a)
= ξ

(b)
> (cid:36)

step (a) uses the deﬁnition of ξ; step (b) uses (cid:36) < ¯(cid:36) (cid:44) min{
conclude that:

√

λ1K( λ2
λ1

), ξ}. Using the deﬁnition of δ (cid:44) 1 − λ2
λ1

, we

(cid:36) ≤ δ = 1 −

λ2
λ1

.

(64)

We naturally obtain the following inequalities:

Coordinate Descent Methods for DC Minimization

I −

√

C
xT Cx

+

CxxT C
√

xT Cx ·

xT Cx

CxxT C
η
C[λ1u1uT
1 − ρI]C
η

λ3

1u1uT
1
η

+

−

λ3

ρCC
η
1u1uT
1
η
i=2 λ2uiuT
i
γ
i=2 λ2uiuT
i
γ

C
γ
(cid:80)n

(cid:80)n

)I −

)I −

)I −

+

−

(cid:18) λ3

1u1uT
1
η

−

λ1u1uT
1
γ

(cid:19)

λ2
γ

· (cid:0)u1uT

1

(cid:1)

∇2F (x)

(a)
=

(b)
(cid:23)

(c)
(cid:23)

(d)
=

(e)
(cid:23)

(f )
(cid:23)

(g)
(cid:23)

(h)
=

(i)
=

+

+

+

C
γ
C
γ
C
γ
ρλ2
1
η
ρλ2
1
η
ρλ2
1
η
ρλ2
1
η

I −

I −

I −

(1 −

(1 −

(1 −

(1 −

(1 −

3((cid:36)2 + (cid:36)

√

λ1)λ2
1

) · I

(λ1 + (cid:36)λ1)3
λ1)

√

3((cid:36)2 + (cid:36)
λ1
√

3((cid:36)2 + (cid:36)
λ1

λ1)

) · I

) · I

(j)
(cid:23) (1 −

(k)
(cid:23) (1 −

= (1 −

−

λ2
γ

) · I

−

−

λ2
λ1 − (cid:36)λ1
λ2
λ1(1 − (cid:36))
λ2
λ1
λ2
λ1

− (cid:36) −

− (cid:36)(1 +

√

3
√
λ1

) −

3(cid:36)2
λ1

) · I (cid:44) σ · I

(65)

where step (a) uses (59) with α = 1; step (b) uses
1u1uT
step (d) uses the fact that CuuT
−λi ≥ −λ2, ∀i ≥ 2; step (g) uses the conclusion as in (63); step (h) uses I = (cid:80)n
ρ, η and γ; step (j) uses the fact that −

1 ; step (e) uses −CC (cid:23) −λ2

1 C = λ2

1

1I; step (f ) uses C = (cid:80)n

1 − 3((cid:36)2 + (cid:36)

λ1)I;
i and
i ; step (i) uses the deﬁnition of

i=1 λiuiuT

i=1 uiuT
; step (k) uses the result in (64) and the follow derivations:

xT Cx ≥ λ1 − ωλ1; step (c) uses xxT (cid:23) λ1u1uT

(λ1+(cid:36)λ1)3 ≥ − 1
λ3
1

√

(cid:36) ≤ 1 −

λ2
λ1
⇔ 0 ≤ (cid:36)(λ1 − λ2 − λ1(cid:36))

⇔

λ2
λ1(1 − (cid:36))

≤

λ2
λ1

+ (cid:36)

⇔ −

λ2
λ1(1 − (cid:36))

≥ −

λ2
λ1

− (cid:36)

Finally, solving the quadratic equation σ (cid:44) 1 − λ2
λ1
when (cid:36) < ξ, we have σ > 0.

− (cid:36)(1 + 3√
λ1

) − 3(cid:36)2
λ1

= 0 yields the positive root ξ. We conclude that

We now prove that ∇2F (x) (cid:52) τ I. We have the following results:

Coordinate Descent Methods for DC Minimization

CxxT C
√

xT Cx ·

xT Cx

∇2F (x)

(a)
=

(b)
(cid:22)

(c)
(cid:22)

I −

√

I + 0 +

+

C
xT Cx
λ2
1(cid:107)x(cid:107)2
2
(cid:107)x(cid:107)3
C
√
λ1 + (cid:36))2
√
λ1)3

λ2
1(
(λ1 − (cid:36)

· I

(1 +

)I (cid:44) τ I

where step (a) uses (59) with α = 1; step (b) uses −C (cid:22) 0 and CC (cid:22) λ2
(cid:107)x(cid:107)C ≤ λ1 + (cid:36)

λ1.

√

1I; step (c) uses (cid:107)x(cid:107) ≤

√

λ1 + (cid:36) and

We hereby ﬁnish the proof of this theorem.

Remarks. (i) The assumption 0 < δ (cid:44) 1 − λ2
λ1
down to a smooth and strongly convex optimization problem under some conditions.

< 1 is equivalent to λ1 > λ2 > 0, which is mild. (ii) Problem (56) boils

CD-SNCA with using θ = 0 reduces to a standard coodinate descent method applied to solve a strongly convex smooth
problem. Using Theorem 1 of (Lu & Xiao, 2015), one can prove the linear convergence rate for the coordinate descent
method.

Theorem D.6. (Convergence Rate of CD-SNCA for the PCA Problem). We assume that the random-coordinate selection
rule is used. Assume that (cid:107)xt − ¯x(cid:107) ≤ ¯(cid:36) that F (·) is σ-strongly convex and τ -smooth. Here the parameters ¯(cid:36), σ and τ are
2 and β (cid:44) 2σ
deﬁne in Theorem D.5. We deﬁne r2
t

(cid:107)x − ¯x(cid:107)2

(cid:44) (1+σ)τ

1+σ . We have:

2

Eξt−1 [r2

t ] ≤ (1 −

β
n

)t+1 (cid:0)r2

0 + F (x0) − F (¯x)(cid:1)

Remarks. Note that Theorem D.6 does not rely on the weak convexity condition or the sharpness condition of F (·).

D.3. Examples for Optimality Hierarchy between the Optimality Conditions

We show some examples to explain the optimality hierarchy between the optimality conditions. Since the condition of
directional point is difﬁcult to verify, we only focus on the condition of critical point and coordinate-wise stationary point in
the sequel.

• The First Running Example. We consider the following problem:

with using the following parameters:

min
x

1
2

xT Qx + (cid:104)x, p(cid:105) − (cid:107)Ax(cid:107)1





Q =

4
0
0 −1

0
0
2 −1
1





 , p =



1
1
1





 , A =



1 −1
1
0
1
3
2 −1
4



 .

First, using the Legendre-Fenchel transform, we can rewrite Problem (66) as the following optimization probelm:

min
x, y

1
2

xT Qx + (cid:104)x, p(cid:105) − (cid:104)Ax, y(cid:105), − 1 ≤ y ≤ 1.

Second, we have the following ﬁrst-order optimality condition for Problem (66):

(Qx + p − sign(Ax))J = 0, J (cid:44) {j | (Ax)j (cid:54)= 0}
−1 ≤ (Qx + p)I ≤ 1, I (cid:44) {i | (Ax)i = 0}

(66)

(67)

Coordinate Descent Methods for DC Minimization

y
[1; 1; 1]
[1; 1; [−1, 1]]
[1; 1; −1]
[1; [−1, 1]; 1]
[1; [−1, 1]; [−1, 1]]
[1; [−1, 1]; −1]
[1; −1; 1]
[1; −1; [−1, 1]]
[1; −1; −1]
[[−1, 1]; 1; 1]
[[−1, 1]; 1; [−1, 1]]
[[−1, 1]1; −1]
[[−1, 1]; [−1, 1]; 1]
[[−1, 1]; [−1, 1]; [−1, 1]]
[[−1, 1]; [−1, 1]; −1]
[[−1, 1]; −1; 1]
[[−1, 1]; −1; [−1, 1]]
[[−1, 1]; −1; −1]
[−1; 1; 1]
[−1; 1; [−1, 1]]
[−1; 1; −1]
[−1; [−1, 1]; 1]
[−1; [−1, 1]; [−1, 1]]
[−1; [−1, 1]; −1]
[−1; −1; 1]
[−1; −1; [−1, 1]]
[−1; −1; −1]

x
[1.75; 0; −1]
empty
[−0.25; −2; −1]
empty
empty
empty
[0.25; −2; −3]
[−0.3333; 0.2667; −0.1333]
[−1.75; −4; −3]
empty
empty
[0; −2; −2]
empty
[0; 0; 0]
[0; 0; 0]
empty
[0; 0; 0]
[0; 0; 0]
[1.25; 0; −3]
empty
[−0.75; −2; −3]
empty
[0; 0; 0]
[0; 0; 0]
[−0.25; −2; −5]
[0; 0; 0]
[−2.25; −4; −5]

Function Value Critical Point CWS Point

-6.625
empty
-8.125
empty
empty
empty
-4.1250
-1.9956
-16.1250
empty
empty
-6.0000
empty
0
0
empty
0
0
-7.6250
empty
-12.1250
empty
0
0
-6.6250
0
-18.625

Yes
No
No
No
No
No
No
No
No
No
No
No
No
Yes
Yes
No
Yes
Yes
Yes
No
No
No
Yes
Yes
No
Yes
Yes

No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
No
Yes

Table 5: Solutions satisfying optimality conditions for Problem (66).

Third, we notice the following relations between Ax and y:

(Ax)i > 0 ⇒ yi = 1
(Ax)i < 0 ⇒ yi = −1
(Ax)i = 0 ⇒ yi ∈ [−1, 1]

We enumerate all possible solutions for y (as shown in the ﬁrst column of Table 5), and then compute the solution satisfying
the ﬁrst-order optimality condition using (67). Table 5 shows the solutions satisfying optimality conditions for Problem (66).
The condition of the Coordinate-wise Stationary (CWS) point might be a much stronger condition than the condition of
critical point.

(λi, ui)
(0.5468, [−0.2934, 0.8139, 0.5015])
(7.8324, [0.1733, −0.4707, 0.8651])

x
±[−0.2169, 0.6019, 0.3709]
±[0.4850, −1.3172, 2.4212]
(33.6207, [−0.9402, −0.3407, 0.0030]) ±[−5.4514, −1.9755, 0.0172]

[0, 0, 0]

Function Value Critical Point CWS Point

-5.7418
-82.2404
-353.0178
0

Yes
Yes
Yes
Yes

No
No
Yes
No

Table 6: Solutions satisfying optimality conditions for Problem (68).

• The Second Running Example. We consider the following example:

with using the following parameter:

min
x

1
2

xT x − (cid:107)Ax(cid:107)2

A =







1
1 −1
2
0
2
1
3
0
2 −1
4







.

(68)

Coordinate Descent Methods for DC Minimization

Using the results of Theorem D.3, the basic stationary points are {0} ∪ {±
λiui}, where (λi, ui) are the eigenvalue
pairs of the matrix AT A. Table 6 shows the solutions satisfying optimality conditions for Problem (68). There exists two
coordinate-wise stationary points. Therefore, coordinate-wise-stationary might be a much stronger condition than criticality.

√

y
[1; 0; 0; 0]
[0; 1; 0; 0]
[0; 0; 1; 0]
[0; 0; 0; 1]
[−1; 0; 0; 0]
[0; −1; 0; 0]
[0; 0; −1; 0]
[0; 0; 0; −1]

x
[1; −1; 1]
[2; 0; 2]
[3; 1; 0]
[4; 2; −1]
[−1; 1; −1]
[−2; 0; −2]
[−3; −1; 0]
[−4; −2; 1]

Function Value Critical Point CWS Point

-2.5000
-4.0000
-9.0000
-10.5000
-2.5000
-4.0000
-9.0000
-10.5000

Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

No
No
No
Yes
No
No
No
Yes

Table 7: Solutions satisfying optimality conditions for Problem (69).

• The Third Running Example. We consider the following example:

min
x

1
2

xT x − (cid:107)Ax(cid:107)∞

(69)

with using the same value for A as in Problem (68). It is equivalent to the following problem:

min
x,y

1
2

xT x − (cid:104)Ax, y(cid:105), (cid:107)y(cid:107)1 ≤ 1.

We enumerate some possible solutions for y, and then compute the solution satisfying the ﬁrst-order optimality condition
via the optimality of x that: x = AT y. Table 7 shows the solutions satisfying optimality conditions for Problem (69). These
results consolidate our previous conclusions.

