0
2
0
2

b
e
F
1
1

]

G
L
.
s
c
[

1
v
9
7
6
4
0
.
2
0
0
2
:
v
i
X
r
a

IPBoost – Non-Convex Boosting via Integer Programming

Marc E. Pfetsch∗

Sebastian Pokutta†

February 13, 2020

Abstract

Recently non-convex optimization approaches for solving machine learning problems have
gained signiﬁcant attention. In this paper we explore non-convex boosting in classiﬁcation
by means of integer programming and demonstrate real-world practicability of the approach
while circumventing shortcomings of convex boosting approaches. We report results that are
comparable to or better than the current state-of-the-art.

1 Introduction

Boosting is an important (and by now standard) technique in classiﬁcation to combine several
‘low accuracy’ learners, so-called base learners, into a ‘high accuracy’ learner, a so-called boosted
learner. Pioneered by the AdaBoost approach of [19], in recent decades there has been extensive
work on boosting procedures and analyses of their limitations. In a nutshell, boosting procedures
are (typically) iterative schemes that roughly work as follows: for t = 1, . . . , T do the following:

1. Train a learner µt from a given class of base learners on the data distribution Dt.

2. Evaluate performance of µt by computing its loss.

3. Push weight of the data distribution Dt towards misclassiﬁed examples leading to Dt+1.

Finally, the learners are combined by some form of voting (e.g., soft or hard voting, averaging,
thresholding). A close inspection of most (but not all) boosting procedures reveals that they solve
an underlying convex optimization problem over a convex loss function by means of coordinate
gradient descent. Boosting schemes of this type are often referred to as convex potential boosters.
These procedures can achieve exceptional performance on many data sets if the data is correctly
labeled. However, it was shown in [27, 28] that any convex potential booster can be easily defeated
by a very small amount of label noise (this also cannot be easily ﬁxed by early termination). The
intuitive reason for this is that convex boosting procedures might progressively zoom in on the (small
percentage of) misclassiﬁed examples in an attempt to correctly label them, while simultaneously
moving distribution weight away from the correctly labeled examples. As a consequence, the
boosting procedure might fail and produce a boosted learner with arbitrary bad performance on
unseen data.

∗Department of Mathematics, TU Darmstadt, Germany, pfetsch@opt.tu-darmstadt.de
†Department of Mathematics, TU Berlin and Zuse Institute Berlin, Berlin, Germany; pokutta@zib.de

1

 
 
 
 
 
 
Let D = {(xi, yi) | i ∈ I} ⊆ Rd × {±1} be a set of training examples and for some logical
condition C, deﬁne I[C] = 1 if C is true and I[C] = −1 otherwise. Typically, the true loss function
of interest is of a form similar to

(cid:96)(D, θ) := ∑
i∈I

I[hθ(xi) (cid:54)= yi],

(1)

i.e., we want to minimize the number of misclassiﬁcations, where hθ is some learner parameterized
by θ; this function can be further modiﬁed to incorporate margin maximization as well as include a
measure of complexity of the boosted learner to help generalization etc. It is important to observe
that the loss in Equation (1) is non-convex and hard to minimize. Thus, traditionally this loss has been
replaced by various convex relaxations, which are at the core of most boosting procedures. In the
presence of mislabeled examples (or more generally label noise) minimizing these convex relaxations
might not be a good proxy for minimizing the true loss function arising from misclassiﬁcations.

Going beyond the issue of label noise, one might ask more broadly, why not directly minimizing
misclassiﬁcations (with possible regularizations) if one could? In the past, this has been out of the
question due to the high complexity of minimizing the non-convex loss function. In this paper, we
will demonstrate that this is feasible and practical with today’s integer programming techniques. We
propose to directly work with a loss function of the form as given in (1) (and variations) and solve
the non-convex combinatorial optimization problem with state-of-the-art integer programming (IP)
techniques including column generation. This approach generalizes previous linear programming
based approaches (and hence implicitly convex approaches) in, e.g., [14, 21, 22, 16], while solving
classiﬁcation problems with the true misclassiﬁcation loss. We acknowledge that (1) is theoretically
very hard (in fact NP-hard as shown, e.g., in [21]), however, we hasten to stress that in real-world
computations for speciﬁc instances the behavior is often much better than the theoretical asymptotic
complexity. In fact, most real-world instances are actually relatively “easy” and with the availability
of very strong integer programming solvers such as, e.g., the commercial solvers CPLEX, Gurobi,
and XPRESS and the academic solver SCIP, these problems can be often solved rather eﬀectively.
In fact, integer programming methods have seen a huge improvement in terms of computational
speed as reported in [31, 3]. The latter reports that integer programming solving performance
has seen a combined hardware and software speed-up of 80 billion from 1991 to 2015 (hardware:
570 000, software 1 400 000) using state-of-the-art hardware and solvers such as CPLEX (see [12]),
Gurobi (see [25]), XPRESS (see [36]), and SCIP (see [20]). With this, problems that traditionally
have been deemed unsolvable can be solved in reasonable short amounts of time making these
methods accessible, feasible, and practical in the context of machine learning allowing to solve a
(certain type of) non-convex optimization problems.

Contribution and Related Work

Our contribution can be summarized as follows:

IP-based boosting. We propose an integer programming based boosting procedure. The resulting
procedure utilizes column generation to solve the initial learning problem and is inherently robust
to labeling noise, since we solve the problem for the (true) non-convex loss function. In particular,
our procedure is robust to the instances from [27, 28] that defeat other convex potential boosters.
Linear Programming (LP) based boosting procedures have been already explored with LPBoost
[14], which also relies on column generation to price the learners. Subsequent work in [26] con-
sidered LP-based boosting for uneven datasets. We also perform column generation, however,
in an IP framework (see [15] for an introduction) rather than a purely LP-based approach, which
signiﬁcantly complicates things. In order to control complexity, overﬁtting, and generalization of

2

the model typically some sparsity is enforced. Previous approaches in the context of LP-based
boosting have promoted sparsity by means of cutting planes, see, e.g., [21, 22, 16]. Sparsiﬁcation can
be handled in our approach by solving a delayed integer program using additional cutting planes.
An interesting alternative use of boosting in the context of training average learners against
rare examples has been explored in [33]; here the ‘boosting’ of the data distribution is performed
while a more complex learner is trained. In [18] boosting in the context of linear regression has
been shown to reduce to a certain form of subgradient descent over an appropriate loss function.
For a general overview of boosting methods we refer the interested reader to [32]. Non-convex
approaches to machine learning problems gained recent attention and (mixed) integer programming,
in particular, has been used successfully to incorporate combinatorial structure in classiﬁcation, see,
e.g., [6, 10, 4, 5], as well as, [23, 3, 13, 24, 35]; note that [13] also uses a column generation approach.
Moreover, neural network veriﬁcation via integer programming has been treated in [34, 17]. See
also the references contained in all of these papers.

Computational results. We present computational results demonstrating that IP-based boosting
can avoid the bad examples of [27]: by far better solutions can be obtained via LP/IP-based boosting
for these instances. We also show that IP-based boosting can be competitive for real-world instances
from the LIBSVM data set. In fact, we obtain nearly optimal solutions in reasonable time for the
true non-convex cost function. Good solutions can be obtained if the process is stopped early. While
it cannot match the raw speed of convex boosters, the obtained results are (often) much better.
Moreover, the resulting solutions are often sparse.

2 IPBoost: Boosting via Integer Programming

We will now introduce the basic formulation of our boosting problem, which is an integer program-
ming formulation based on the standard LPBoost model from [14]. While we conﬁne the exposition
to the binary classiﬁcation case only, for the sake of clarity, we stress that our approach can be
extended to the multi-class case using standard methods. In subsequent sections, we will reﬁne the
model to include additional model parameters etc.

Let (x1, y1), . . . , (xN, yN) be the training set with points xi ∈ Rd and two-class labels yi ∈ {±1}.
Moreover, let Ω := {h1, . . . , hL : Rd → {±1}} be a class of base learners and let a margin ρ ≥ 0 be
given. Our basic boosting model is captured by the following integer programming problem:

min

N
∑
i=1
L
∑
j=1

zi

(2)

ηij λj + (1 + ρ)zi ≥ ρ ∀ i ∈ [N],

λj = 1, λ ≥ 0,

L
∑
j=1
z ∈ {0, 1}N,

where the error function η can take various forms depending on how we want to treat the output of
base learners. For learner hj and training example xi we consider the following choices:

(i) ±1 classiﬁcation from learners:

ηij := 2 I[hj(xi) = yi] − 1 = yi · hj(xi);

3

(ii) class probabilities of learners:
ηij := 2 P[hj(xi) = yi] − 1;

(iii) SAMME.R error function for learners:

ηij := 1

2 yi log

(cid:16) P[hj(xi)=1]
P[hj(xi)=−1]

(cid:17)

.

In the ﬁrst case we perform a hard minimization of the classiﬁcation error, in the second case we
perform a soft minimization of the classiﬁcation error, and in the last one we minimize the SAMME.R
error function as used in the (multi-class) AdaBoost variant in [37]. The SAMME.R error function
allows a very conﬁdent learner to overrule a larger number of less conﬁdent learners predicting the
opposite class.

The zi variable in the model above indicates whether example i ∈ [N] := {1, . . . N} satisﬁes the
classiﬁcation requirement: zi = 0 if example i is correctly labeled by the boosted learner ∑j hjλj with
margin at least ρ with respect to the utilized error function η; in an optimal solution, if a variable
if 1 this implies misclassiﬁcation, otherwise by minimizing you could have set it to zero. The λj
with j ∈ [L] form a distribution over the family of base learners. The only non-trivial family of
inequalities in (2) ranges over examples i ∈ [N] and enforces that the combined learner ∑j∈[L] hjλj
classiﬁes example i ∈ N correctly with margin at least ρ (we assume throughout that ρ ≤ 1) or
zi = 1, i.e., the example is disregarded and potentially misclassiﬁed. By minimizing ∑i∈N zi, the
program computes the best combination of base learners maximizing the number of examples that
are correctly classiﬁed with margin at least ρ. The margin parameter ρ helps generalization as it
prevents base learners to be used to explain low-margin noise.

Before we continue with the integer programming based boosting algorithm we would like to
remark the following about the solution structure of optimal solutions with respect to the chosen
margin:

Lemma 1 (Structure of high-margin solutions)
Let (λ, z) be an optimal solution to the integer program (2) for a given margin ρ using error function (i).
Further let I := {i ∈ [N] | zi = 0} and J := {j ∈ [L] | λj > 0}. If the optimal solution is non-trivial, i.e.,
I (cid:54)= ∅, then the following holds:
1. If ρ = 1, then there exists an optimal solution with margin 1 using only a single base learner hj for some

j ∈ J.

2. If there exists ¯ ∈ J with λ¯ > 1−ρ
3. If |J| < 2

2 , then h¯ by itself is already an optimal solution with margin 1.

1−ρ , then there exists ¯ ∈ J with h¯ by itself being already an optimal solution with margin 1. In

particular for ρ > 0, the statement is non-trivial.

Proof. For the ﬁrst case observe that

ηijλj ≥ 1,

∑
j∈J

holds for all i ∈ I. As ∑j∈J λj = 1 and λj > 0 for all j ∈ J, we have that ηij = 1 for all i ∈ I, j ∈ J.
Therefore the predictions of all learners hj with j ∈ J for examples i ∈ I are identical and we can
simply choose any such learner hj with j ∈ J arbitrarily and set λj = 1.

For the second case observe as before that ∑j∈J ηijλj ≥ ρ holds for all i ∈ I. We claim that ηi¯ = 1

for all i ∈ I. For contradiction suppose not, i.e., there exists ¯ı ∈ I with η¯ı¯ = −1. Then
(cid:18)

(cid:19)

∑
j∈J

η¯ıjλj <

(cid:124)

∑
j∈J\{¯}
(cid:123)(cid:122)
<1− 1−ρ
2

(cid:125)

λj

−

< ρ,

1 − ρ
2

4

using ηij ≤ 1, ∑j∈J λj = 1, and λj > 0 for all j ∈ J. This contradicts ∑j∈J ηijλj ≥ ρ and therefore
ηi¯ = 1 for all i ∈ I. Thus h¯ by itself is already an optimal solution satisfying even the (potentially)
higher margin of 1 ≥ ρ on examples i ∈ I.

Finally, for the last case observe that if |J| < 2

for all j ∈ J, it follows that there exists ¯ ∈ J with λ¯ > 1−ρ
contradiction. We can now apply the second case.

1−ρ , then together with ∑j∈J λj = 1, and λj > 0
2 . Otherwise ∑j∈J λj ≤ |J| 1−ρ
2 < 1; a
(cid:3)
Similar observations hold for error functions (ii) and (iii) with the obvious modiﬁcations to

include the actual value of ηij not just its sign.

Our proposed solution process consists of two parts. We ﬁrst solve the integer program in (2)
using column generation. Once this step is completed, the solution can be sparsiﬁed (if necessary)
by means of the model presented in Section 2.2, where we trade-oﬀ classiﬁcation performance with
model complexity.

2.1 Solution Process using Column Generation

The reader will have realized that (4) is not practical, since we typically have a very large if not
inﬁnite class of base learners Ω; for convenience we assume here that Ω is ﬁnite but potentially
very large. This has been already observed before and dealt with eﬀectively via column generation
in [14, 21, 22, 16]. We will follow a similar strategy here, however, we generate columns within a
branch-and-bound framework leading eﬀectively to a branch-and-bound-and-price algorithm that we
are using; this is signiﬁcantly more involved compared to column generation in linear programming.
We detail this approach in the following.

The goal of column generation is to provide an eﬃcient way to solve the linear programming (LP)
relaxation of (2), i.e., the zi variables are relaxed and allowed to assume fractional values. Moreover,
one uses a subset of the columns, i.e., base learners, L ⊆ [L]. This yields the so-call restricted master
(primal) problem

min

N
∑
i=1
∑
j∈L
∑
j∈L

zi

ηij λj + (1 + ρ)zi ≥ ρ ∀ i ∈ [N],

λj = 1, λ ≥ 0, z ∈ [0, 1]N.

Its restricted dual problem is

max ρ

ui

N
∑
i=1

wi + v −

N
∑
i=1
N
∑
i=1
(1 + ρ)wi − ui ≤ 1 ∀ i ∈ [N],
w ≥ 0, u ≥ 0, v free.

ηij wi + v ≤ 0 ∀ j ∈ L,

(3)

(4)

Consider a solution (w∗, v∗, u∗) ∈ RN × R × RN of (4). The so-called pricing problem is to decide
whether this solution is actually optimal or whether we can add further constraints, i.e., columns in
the primal problem. For this, we need to check whether (w∗, v∗, u∗) is feasible for the complete set
of constraints in (4). In the following, we will assume that the variables zi are always present in the

5

Algorithm 1 IPBoost
Input: Examples D = {(xi, yi) | i ∈ I} ⊆ Rd × {±1}, class of base learners Ω, margin ρ
Output: Boosted learner ∑j∈L∗ hjλ∗
1: T ← {([0, 1]N, ∅)}
2: U ← ∞, L∗ ← ∅
3: while T (cid:54)= ∅ do
4:

j with base learners hj and weights λ∗

j

// set of local bounds and learners for open subproblems

// Upper bound on optimal objective.

Choose and remove (B, L) from T .
repeat

Solve (3) using the local bounds on z in B with optimal dual solution (w∗, v∗, u∗).
Find learner hj ∈ Ω satisfying (5).

// Solve pricing problem.

until hj is not found
Let ( ˜λ, ˜z) be the ﬁnal solution of (3) with base learners ˜L = {j | ˜λj > 0}.
if ˜z ∈ ZN and ∑N

U ← ∑N

i=1 ˜zi < U then
i=1 ˜zi, L∗ ← ˜L, λ∗ ← ˜λ

else

Choose i ∈ [N] with ˜zi /∈ Z.
Set B0 ← B ∩ {zi ≤ 0}, B1 ← B ∩ {zi ≥ 1}.
Add (B0, ˜L), (B1, ˜L) to T .

// Update best solution.

// Create new branching nodes.

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

end if

16:
17: end while
18: Optionally sparsify ﬁnal solution L∗.

primal and therefore that the corresponding inequalities (1 + ρ)wi − v ≤ 1 are satisﬁed for each
i ∈ [N]. Thus, the main task of the pricing problem is to decide whether there exists j ∈ [L] \ L
such that

N
∑
i=1

ηij w∗

i + v∗ > 0.

(5)

If such an j exists, then it is added to L, i.e., to (3), and the process is iterated. Otherwise, both (3)
and (4) have been solved to optimality.

The pricing problem (5) can now be rephrased as follows: Does there exist a base learner hj ∈ Ω
such that (5) holds? For this, the w∗
i can be seen as weights over the points xi, i ∈ [N], and we have
to classify the points according to these weights. For most base learners, this task just corresponds
to an ordinary classiﬁcation or regression step, depending on the form chosen for ηij. Note, however,
that in practice (5) is not solved to optimality, but is rather solved heuristically. If we ﬁnd a base
learner hj that satisﬁes (5), we continue, otherwise we stop.

The process just described allows to solve the relaxation of (2). The optimal misclassiﬁcation
values are determined by a branch-and-price process that branches on the variables zi and solves
the intermediate LPs using column generation. Note that the zi are always present in the LP, which
means that no problem-speciﬁc branching rule is needed, see, e.g., [1] for a discussion. In total, this
yields Algorithm 1.

The output of the algorithm is a set of base learners L∗ and corresponding weights λ∗
j . A
classiﬁer can be obtained by voting, i.e., a given point x is classiﬁed by each of the base learners
resulting in ξ j, for which we again can use the three options (i)–(iii) above. We then take the
weighted combination and obtain the predicted label as sgn(∑j∈L∗ ξ jλ∗

j ).

In the implementation, we use the following important components. First, we use the framework
SCIP that automatically applies primal heuristics, see, e.g., [2] for an overview. These heuristics

6

usually take the current solution of the relaxation and try to build a feasible solution for (2). In the
current application, the most important heuristics are rounding heuristics, i.e., the zi variables are
rounded to 0 or 1, but large-scale neighborhood heuristics sometimes provide very good solutions
as well. Nevertheless, we disable diving heuristics, since these often needed a long time, but never
produced a feasible solution. In total, this often generates many feasible solutions along the way.
Another trick that we apply is the so-called stall limit. The solver automatically stops if the
best primal solution could not be (strictly) improved during the last K nodes processed in the
branch-and-bound tree (we use K = 5000).

Furthermore, preliminary experiments have shown that the intermediate linear programs that
have to be solved in each iteration become increasingly hard to solve by the simplex algorithm for a
large number of training points. We could apply bagging [9], but obtained good results with just
subsampling 30 000 points if their number N is larger than this threshold.

Furthermore, we perform the following post-processing. For the best solution that is available
at the end of the branch-and-bound algorithm, we ﬁx the integer variables to the values in this
solution. Then we maximize the margin over the learner variables that were used in the solution,
which is just a linear program. In most cases, the margin can be slightly improved in this way,
hoping to get improved generalization.

2.2 Sparsiﬁcation

One of the challenges in boosting is to balance model accuracy vs. model generalization, i.e., to
prevent overﬁtting. Apart from pure generalization considerations, a sparse model often lends itself
more easily to interpretation, which might be important in certain applications.

There are essentially two techniques that are commonly used in this context. The ﬁrst one is
early stopping, i.e., we only perform a ﬁxed number of boosting iterations, which would correspond
to only generating a ﬁxed number of columns. The second common approach is to regularize
the problem by adding a complexity term for the learners in the objective function, so that we
minimize ∑N
j=1 αjyj. Then we can pick αj as a function of the complexity of the learner
hj. For example, in [11] boosting across classes of more complex learners has been considered and
the αj are chosen to be proportional to the Rademacher complexity of the learners (many other
measures might be equally justiﬁed).

i=1 zi + ∑L

In our context, it seems natural to consider the following integer program for sparsiﬁcation:

min

N
∑
i=1

L
∑
j=1

zi +

L
∑
j=1

αjyj

ηij λj + (1 + ρ)zi ≥ ρ ∀ i ∈ [N],

λj = 1, 0 ≤ λj ≤ yj ∀ j ∈ [L],

L
∑
j=1
z ∈ {0, 1}N, y ∈ {0, 1}L,

with ηij as before. The structure of this sparsiﬁcation problem that involves additional binary
variables y cannot be easily represented within the column generation setup used to solve model (2),
because the upper bounds on λj implied by yj would need to represented in dual problem, giving
rise to exponentially many variables in the dual. In principle, one could handle a cardinality
constraint on the yj variables using a problem speciﬁc branching rule; this more involved algorithm

7

is however beyond the scope of this paper. In consequence, one can solve the sparsiﬁcation problem
separately for the columns that have been selected in phase 1 once this phase is completed. This is
similar to [21], but directly aims to solve the MIP rather than a relaxation. Moreover, one can apply
so-called IIS-cuts, following [30]. Using the Farkas lemma, the idea is to identify subsets I ⊆ [N]
such that the system

L
∑
j=1

ηij λj ≥ ρ, i ∈ I,

L
∑
j=1

λj = 1, λ ≥ 0,

is infeasible. In this case the cut

zi ≥ 1

∑
i∈I

is valid. Such sets I can be found by searching for vertices of the corresponding alternative polyhe-
dron. If this is done iteratively (see [30]), many such cuts can be found that help to strengthen the LP
relaxation. These cuts dominate the ones in [21], but one needs to solve an LP for each vertex/cut.

3 Computational Results

To evaluate the performance of IPBoost, we ran a wide range of tests on various classiﬁcation tasks.
Due to space limitations, we will only be able to report aggregate results here; additional more
extensive results can be found in the Supplementary Material B.

Computational Setup. All tests were run on a Linux cluster with Intel Xeon quad core CPUs with
3.50GHz, 10 MB cache, and 32 GB of main memory. All runs were performed with a single process
per node; we stress, in particular, that we run all tests as single thread / single core setup, i.e., each
test uses only a single node in single thread mode. We used a prerelease version of SCIP 7.0.0 with
SoPlex 5.0.0 as LP-solver; note that this combination is completely available in source code and free
for academic use. The main part of the code was implemented in C, calling the python framework
scikit-learn [29] at several places. We use the decision tree implementation of scikit-learn
with a maximal depth of 1, i.e., a decision stump, as base learners for all boosters. We benchmarked
IPBoost against our own implementation of LPBoost [14] as well as the AdaBoost implementation
in version 0.21.3 of scikit-learn using 100 iterations; note that we always report the number of
pairwise distinct base learners for AdaBoost. We performed 10 runs for each instance with varying
random seeds and we report average accuracy and standard deviations. Note that we use a time
limit of one hour for each run of IPBoost. The reported solution is the best solution available at that
time.

Results on Constructed Hard Instances. We start our discussion of computational results by
reporting on experiments with the hard instances of [27]. These examples are tailored to using the
±1 classiﬁcation from learners (option (i) in Section 2). Thus, we use this function for prediction
and voting for every algorithm. The performance of IPBoost, LPBoost and AdaBoost (using 100
iterations) is presented in Table 1. Here, N is the number of points and γ refers to the noise level.
Note that we randomly split oﬀ 20 % of the points for the test set.

On every instance class, IPBoost clearly outperforms LPBoost. AdaBoost performs much less well,
as expected; it also uses signiﬁcantly more base learners. Note, however, that the scikit-learn
implementation of AdaBoost produces much better results than the one in [27] (an accuracy of
about 53 % as opposed to 33 %). As noted above, the instances are constructed for a ±1 classiﬁcation

8

Table 1: Averages of the test accuracies for hard instances. The table shows the accuracies and standard
deviations as well as the number of learners L for three algorithms using ρ = 0.05 for 10 diﬀerent
seeds; best solutions are marked with *; using ±1 values for prediction and voting.

N

γ

2000
4000
8000
16000
32000
64000
2000
4000
8000
16000
32000
64000
2000
4000
8000
16000
32000
64000

0.1
0.1
0.1
0.1
0.1
0.1
0.075
0.075
0.075
0.075
0.075
0.075
0.05
0.05
0.05
0.05
0.05
0.05

IPBoost
score

L

69.05 ± 2.54 4.8
68.61 ± 1.50 4.6
67.26 ± 1.62 3.6
67.50 ± 1.48 3.3
67.36 ± 1.55 2.6
66.65 ± 1.04 2.5
71.30 ± 2.06 4.6
70.20 ± 1.69 4.1
68.41 ± 1.73 3.8
68.10 ± 2.18 2.9
68.06 ± 1.47 2.6
67.92 ± 1.05 2.4
72.20 ± 1.92 5.1
71.74 ± 1.59 4.9
70.09 ± 1.96 3.4
70.05 ± 1.57 3.3
69.25 ± 1.86 2.4
68.83 ± 1.44 2.3

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

LPBoost
score

66.22 ± 1.73
65.23 ± 1.95
64.58 ± 1.05
64.73 ± 0.80
65.18 ± 0.55
65.17 ± 0.35
66.55 ± 1.89
66.54 ± 1.58
65.38 ± 1.05
65.63 ± 0.81
66.17 ± 0.62
66.12 ± 0.33
67.05 ± 1.71
67.27 ± 1.69
66.19 ± 1.22
66.82 ± 0.81
67.30 ± 0.54
67.06 ± 0.37

averages:

18 69.03 ± 1.68

3.5

0

66.07 ± 1.06

L

2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0

2.0

AdaBoost
score

L

58.58 ±2.77 20.9
55.45 ±2.99 20.9
53.24 ±1.68 20.9
51.85 ±0.80 21.0
51.22 ±0.73 20.9
50.48 ±0.49 20.9
57.95 ±2.83 21.1
55.27 ±2.77 21.0
53.14 ±1.51 21.0
51.73 ±0.67 21.0
51.12 ±0.61 20.9
50.35 ±0.47 21.0
57.50 ±2.51 21.0
54.75 ±2.47 20.9
53.01 ±1.40 21.0
51.75 ±0.85 21.0
51.15 ±0.65 21.0
50.35 ±0.54 21.0

0

53.27 ±1.49 21.0

function. If we change to SAMME.R, AdaBoost performs much better: slightly worse that IPBoost,
but better than LPBoost.

LIBSVM Instances. We use classiﬁcation instances from the LIBSVM data sets available at https:
//www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. We selected the 40 smallest
instances. If available, we choose the scaled version over the unscaled version. Note that 25 instances
of those 40 instances come with a corresponding test set. Since the test sets for the instances a1a–
a9a looked suspicious (often more features and points than in the train set and sometimes only
features in one class), we decided to remove the test sets for these nine instances. This leaves 16
instances with test set. For the other 24, we randomly split oﬀ 20% of the points as a test set; we
provide statistics for the individual instances in Table 3 in the Supplementary Material B.

Results for LIBSVM. An important choice for the algorithm is how the error matrix η is set up,
i.e., which of the three options (i)–(iii) presented in Section 2 is used. In preliminary computations,
we compared all three possibilities. It turned out that the best option is to use the class probabilities
(ii) for η both for Model (2) and when using the base learners in a voting scheme, which we report
here.

Another crucial choice in our approach is the margin bound ρ. We ran our code with diﬀerent
values – the aggregated results are presented in Table 2; the detailed results are given in the
Supplementary Material. We report accuracies on the test set and train set, respectively. In each
case, we report the averages of the accuracies over 10 runs with a diﬀerent random seed and their
standard deviations. The accuracies of IPBoost are compared to LPBoost and AdaBoost. We also

9

Table 2: Aggregated results for LIBSVM: Average test/train accuracies and standard deviations
(STD) for three algorithms over 10 diﬀerent seeds, using class probabilities for prediction and voting;
we considered 40 instances as outlined in Section 3. Column “# best” represents the number of
instances on which the corresponding algorithm performed best (ties possible). Column “ER” gives
the error rate, i.e., 1/(1 − a) for the average accuracy a.

IPBoost

LPBoost

AdaBoost

type

ρ

# best

acc.

STD

ER

# best

acc.

STD

ER

# best

acc.

STD

test
test
test
test
test

0.1
0.075
0.05
0.025
0.01

train
0.1
train 0.075
0.05
train
train 0.025
0.01
train

24
25
26
25
26

25
24
26
29
31

80.70
80.77
80.78
80.63
80.59

83.52
83.94
84.34
84.97
85.69

4.08
3.89
4.07
3.94
3.91

2.51
2.38
2.43
2.44
2.48

5.18
5.20
5.20
5.16
5.15

6.07
6.23
6.38
6.65
6.99

7
6
7
7
7

1
1
1
1
3

80.16
80.21
80.31
80.21
79.80

82.29
82.52
82.90
83.39
84.20

3.93
3.89
3.73
3.91
3.67

2.61
2.54
2.40
2.43
2.26

5.04
5.05
5.08
5.05
4.95

5.65
5.72
5.85
6.02
6.33

9
9
8
8
7

15
15
14
10
6

79.79
79.79
79.79
79.79
79.79

84.36
84.36
84.36
84.36
84.36

3.82
3.82
3.82
3.82
3.82

1.99
1.99
1.99
1.99
1.99

ER

4.95
4.95
4.95
4.95
4.95

6.40
6.40
6.40
6.40
6.40

report the number L of learners in the detailed results. Note that the behavior of AdaBoost is
independent of ρ, i.e., the accuracies are the same over the diﬀerent values of ρ in Table 2.

The results show that IPBoost outperforms both LPBoost and AdaBoost. IPBoost clearly outper-
forms LPBoost, although there are instances where LPBoost generates slightly better results, both
for the train and the test accuracies. Interestingly, the accuracies of IPBoost (and LPBoost) increase
with respect to AdaBoost, when considering the test set instead of the training set: less overﬁtting
and better generalization. For the considered instances the best value for the margin ρ was 0.05 for
LPBoost and IPBoost; AdaBoost has no margin parameter.

Depending on the size of the instances, typical running times of IPBoost range from a few
seconds up to one hour. We provide details of the running times in Table 4 in the Supplementary
Material. The average run time of IPBoost for ρ = 0.05 is 1367.78 seconds, while LPBoost uses
164.35 seconds and AdaBoost 3.59 seconds. The main bottleneck arises from the solution of large
LP relaxations in each iteration of the algorithm. Note that we apply the simplex algorithm in
order to beneﬁt from hot start after changing the problem by adding columns or changing bounds.
Nevertheless, larger LPs turned out to be very hard to solve. One explanation for this is that the
matrix is very dense.

Feasible solutions of high quality are often found after a few seconds via primal heuristics. The
solution that is actually used for constructing the boosted learner is often found long before the
solution process ﬁnished, i.e., the algorithm continues to search for better solutions without further
progress. Note that in most cases, the algorithm is stopped, because no further improving solution
was found, i.e., the stall limit is applied (see Section 2.1). We have experimented with larger limits
(K > 5000), but the quality of the solutions only improved very slightly. This suggests that the
solutions we found are optimal or close to optimal for (2).

Also interesting is the number of base learners in the best solutions of IPBoost. The results
show that this is around 12 on average for ρ = 0.05; for ρ = 0.01 it is around 18. Thus, the optimal
solutions are inherently sparse and as such for these settings and instances the sparsiﬁcation
procedure described in Section 2.2 will likely not be successful. However, it seems likely that for
instance sets requiring diﬀerent margins the situation is diﬀerent.

10

Figure 1: Train vs. test performance for diﬀerent margins ρ on instance w1a. Each point represents
a solution encountered by IPBoost while solving the boosting problem. Grayscale values indicate
the number of base learners used in boosted learners; see the legend.

We have also experimented with diﬀerent ways to handle ρ. Following [14], one can set up a
model in which the margin ρ is a variable to be optimized in addition to the number of misclassiﬁca-
tions. In this model, it is crucial to ﬁnd the right balance between the diﬀerent parts of the objective.
For instance, on can run some algorithm (AdaBoost) to estimate the number of misclassiﬁcations
and then adjust the weight factor accordingly. In preliminary experiments, this option was inferior
to the approach described in this paper; we used an identical approach for LPBoost for consistency.

Generalization Performance. We found that the boosted learners computed via IPBoost general-
ize rather well. Figure 1 gives a representative example for generalization: here we plot the train
and test accuracy of the solutions encountered by IPBoost within a run, while solving the boosting
problem for various margins. We report more detailed results in Section B.1 in the Supplementary
Material.

We observe the following almost monotonous behavior: the smaller ρ, the more base learners are
used and the better the obtained training accuracy. This is of course expected, since smaller margins
allow more freedom to combine base learners. However, this behavior does not directly translate to
better testing accuracies, which indices overﬁtting. Note that IPBoost obtains better average test
accuracies than AdaBoost for every ρ, but this is not always the case for the train accuracies. This
again demonstrates the good generalization properties of IPBoost.

We would also like to point out that the results in Figure 1 and Section B.1 give an indication that
the often cited belief that “solving (close) to optimality reduces generalization” is not true in general.
In fact, minimizing the right loss function close to optimality can actually help generalization.

4 Concluding Remarks

In this paper, we have ﬁrst reproduced the observation that boosting based on column generation,
i.e., LP- and IP-boosting, avoids the bad performance on the well-known hard classes from the
literature. More importantly, we have shown that IP-boosting improves upon LP-boosting and
AdaBoost on the LIBSVM instances on which a consistent improvement even by a few percent
is not easy. The price to pay is that the running time with the current implementation is much
longer. Nevertheless, the results are promising, so it can make sense to tune the performance, e.g.,
by solving the intermediate LPs only approximately and deriving tailored heuristics that generate
very good primal solutions, see [8] and [7], respectively, for examples for column generation in
public transport optimization.

11

96.096.597.097.598.098.599.096.097.098.099.0trainaccuracy(%)testaccuracy(%)w1a–ρ=0.011520253096.096.597.097.598.098.599.096.097.098.099.0trainaccuracy(%)testaccuracy(%)w1a–ρ=0.051520253096.096.597.097.598.098.599.096.097.098.099.0trainaccuracy(%)testaccuracy(%)w1a–ρ=0.1101520Moreover, our method has a parameter that needs to be tuned, namely the margin bound ρ. It
shares this property with LP-boosting, where one either needs to set ρ or a corresponding objective
weight. AdaBoost, however, depends on the number of iterations which also has to be adjusted to
the instance set. We plan to investigate methods based on the approach in the current paper that
avoid the dependence on a parameter.

In conclusion, our approach is suited very well to an oﬄine setting in which training may take
time and where even a small improvement is beneﬁcial or when convex boosters behave very badly.
Moreover, it can serve as a tool to investigate the general performance of such methods.

References

[1] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:

Column generation for solving huge integer programs. Oper. Res., 46(3):316–329, 1998.

[2] T. Berthold. Heuristic algorithms in global MINLP solvers. PhD thesis, TU Berlin, 2014.

[3] D. Bertsimas and J. Dunn. Optimal classiﬁcation trees. Machine Learning, pages 1–44, 2017.

[4] D. Bertsimas and A. King. OR forum—an algorithmic approach to linear regression. Operations Research,

64(1):2–16, 2015.

[5] D. Bertsimas, A. King, R. Mazumder, et al. Best subset selection via a modern optimization lens. The

Annals of Statistics, 44(2):813–852, 2016.

[6] D. Bertsimas and R. Shioda. Classiﬁcation and regression via integer optimization. Operations Research,

55(2):252–271, 2007.

[7] R. Borndörfer, A. Löbel, M. Reuther, T. Schlechte, and S. Weider. Rapid branching. Public Transport,

5(1):3–23, 2013.

[8] R. Borndörfer, A. Löbel, and S. Weider. A bundle method for integrated multi-depot vehicle and duty
scheduling in public transit. In M. Hickman, P. Mirchandani, and S. Voß, editors, Computer-aided Systems
in Public Transport, volume 600, pages 3–24, 2008.

[9] L. Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996.

[10] A. Chang, D. Bertsimas, and C. Rudin. An integer optimization approach to associative classiﬁcation.

In Advances in Neural Information Processing Systems, pages 269–277, 2012.

[11] C. Cortes, M. Mohri, and U. Syed. Deep boosting. In 31st International Conference on Machine Learning,

ICML 2014. International Machine Learning Society (IMLS), 2014.

[12] CPLEX. IBM ILOG CPLEX Optimizer. https://www.ibm.com/analytics/cplex-optimizer,

2020.

[13] S. Dash, O. Günlük, and D. Wei. Boolean decision rules via column generation. In Advances in Neural

Information Processing Systems, pages 4655–4665, 2018.

[14] A. Demiriz, K. P. Bennett, and J. Shawe-Taylor. Linear programming boosting via column generation.

Machine Learning, 46(1-3):225–254, 2002.

[15] J. Desrosiers and M. E. Lübbecke. A primer in column generation. In Column generation, pages 1–32.

Springer, 2005.

[16] J. Eckstein and N. Goldberg. An improved branch-and-bound method for maximum monomial agree-

ment. INFORMS Journal on Computing, 24(2):328–341, 2012.

[17] M. Fischetti and J. Jo. Deep neural networks and mixed integer linear optimization. Constraints,

23(3):296–309, 2018.

[18] R. M. Freund, P. Grigas, and R. Mazumder. A new perspective on boosting in linear regression via

subgradient optimization and relatives. arXiv preprint arXiv:1505.04243, 2015.

12

[19] Y. Freund and R. E. Schapire. A desicion-theoretic generalization of on-line learning and an application
to boosting. In European Conference on Computational Learning Theory, pages 23–37. Springer, 1995.

[20] A. Gleixner, M. Bastubbe, L. Eiﬂer, T. Gally, G. Gamrath, R. L. Gottwald, G. Hendel, C. Hojny, T. Koch,
M. E. Lübbecke, S. J. Maher, M. Miltenberger, B. Müller, M. E. Pfetsch, C. Puchert, D. Rehfeldt, F. Schlösser,
C. Schubert, F. Serrano, Y. Shinano, J. M. Viernickel, M. Walter, F. Wegscheider, J. T. Witt, and J. Witzig.
The SCIP Optimization Suite 6.0. Technical report, Optimization Online, July 2018.

[21] N. Goldberg and J. Eckstein. Boosting classiﬁers with tightened l0-relaxation penalties. In Proceedings of

the 27th International Conference on Machine Learning (ICML-10), pages 383–390, 2010.

[22] N. Goldberg and J. Eckstein. Sparse weighted voting classiﬁer selection and its linear programming

relaxations. Information Processing Letters, 112(12):481–486, 2012.

[23] O. Günlük, J. Kalagnanam, M. Menickelly, and K. Scheinberg. Optimal generalized decision trees via

integer programming. arXiv preprint arXiv:1612.03225, 2016.

[24] O. Günlük, J. Kalagnanam, M. Menickelly, and K. Scheinberg. Optimal decision trees for categorical

data via integer programming. arXiv preprint arXiv:1612.03225, 2018.

[25] Gurobi. Gurobi Optimizer. http://www.gurobi.com, 2020.

[26] J. Leskovec and J. Shawe-Taylor. Linear programming boosting for uneven datasets. In Proceedings of the

Twentieth International Conference on Machine Learning (ICML-2003), pages 456–463, 2003.

[27] P. M. Long and R. A. Servedio. Random classiﬁcation noise defeats all convex potential boosters. In

Proceedings of the 25th international conference on Machine learning, pages 608–615. ACM, 2008.

[28] P. M. Long and R. A. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine

learning, 78(3):287–304, 2010.

[29] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research,
12(Oct):2825–2830, 2011.

[30] M. E. Pfetsch. Branch-and-cut for the maximum feasible subsystem problem. SIAM J. Optim., 19(1):21–38,

2008.

[31] P. Savick `y, J. Klaschka, and J. Antoch. Optimal classiﬁcation trees. In COMPSTAT, pages 427–432.

Springer, 2000.

[32] R. E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear estimation and

classiﬁcation, pages 149–171. Springer, 2003.

[33] S. Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and why. In Proceedings of the

32nd International Conference on Machine Learning, 2016.

[34] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed integer

programming. arXiv preprint arXiv:1711.07356, 2017.

[35] S. Verwer and Y. Zhang. Learning optimal classiﬁcation trees using a binary linear program formulation.

In 33rd AAAI Conference on Artiﬁcial Intelligence, 2019.

[36] XPRESS.

FICO Xpress Optimizer.

https://www.fico.com/en/products/

fico-xpress-optimization, 2020.

[37] J. Zhu, H. Zou, S. Rosset, and T. Hastie. Multi-class adaboost. Statistics and its Interface, 2(3):349–360,

2009.

13

Supplementary Material

A Detailed Computational Results

In the following tables, we report detailed computational results for our tests. We report problem
size statistics in Table 3 and running time statistics in Table 4.

For ρ = 0.1, 0.075, 0.05, 0.025, 0.01, we present train results in Tables 5, 7, 9, 13 and test results in

Tables 6, 8, 10, 14.

14

Table 3: Statistics on LIBSVM instances.

class 1

395
572
773
1188
1569
2692
3918
7841
307
239
19 845
22
23
300
3000
500
307
300
120
4853
225
27
55
1000
6157
194 198
97
517
2000
296
72
107
143
216
281
1479

d

–
–
–
–
–
–
–
–
–
–
8
–
7129
–
5000
–
–
–
–
22
–
7129
5
500
–
–
–
60
4
22
300
300
300
300
300
300

test set

N

class −1

class 1

–
–
–
–
–
–
–
–
–
–
271 617
–
38
–
1000
–
–
–
–
91 701
–
34
200
600
–
–
–
2175
4000
41
47 272
46 279
44 837
42 383
39 861
14 951

–
–
–
–
–
–
–
–
–
–
181 078
–
16
–
500
–
–
–
–
82 989
–
14
100
300
–
–
–
1044
2000
41
45 865
44 907
43 501
41 120
38 663
14 497

–
–
–
–
–
–
–
–
–
–
90 539
–
22
–
500
–
–
–
–
8712
–
20
100
300
–
–
–
1131
2000
0
1407
1372
1336
1263
1198
454

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w8a

d

119
119
122
122
122
122
122
123
14
10
8
2000
7129
24
5000
8
2
24
13
22
34
7129
5
500
68
3
60
60
4
22
300
300
300
300
300
300

train set

N

class −1

1605
2265
3185
4781
6414
11 220
16 100
32 561
690
683
59 535
62
44
1000
6000
768
862
1000
270
49 990
351
38
145
2000
11 055
245 057
208
1000
3089
1243
2477
3470
4912
7366
9888
49 749

1210
1693
2412
3593
4845
8528
12 182
24 720
383
444
39 690
40
21
700
3000
268
555
700
150
45 137
126
11
90
1000
4898
50 859
111
483
1089
947
2405
3363
4769
7150
9607
48 270

15

Table 4: Statistics for LIBSVM on average run times (in seconds) of diﬀerent algorithms with ρ = 0.05
for 10 diﬀerent seeds; “# optimal” gives the number of instances solved to optimality, “# time out”
the number of instances that ran into the time limit of 1 hour, and “best sol. time” the average time
at which the best solution was found.

IPBoost

# nodes

# optimal

# time out

best sol.
time

LPBoost
time

AdaBoost
time

name
name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

total
time

97.26
257.97
464.05
770.89
904.72
1712.09
2946.71
3251.73
3247.98
66.84
435.32
3603.83
0.09
0.25
78.61
3439.67
116.58
165.46
120.73
43.55
3604.29
67.56
0.20
48.69
2387.03
968.38
977.76
3576.38
61.81
128.63
1375.33
671.57
673.90
927.73
1665.23
2442.22
2583.16
3607.69
3607.88
3611.46

6669.9
8186.6
9096.6
9769.9
10 186.5
7763.4
8986.1
6370.4
4094.4
7400.4
7183.9
77.4
6.8
7.9
7215.8
3709.8
7313.7
9932.6
9495.2
10 170.0
1197.2
7375.7
5.8
9768.9
7619.7
6761.5
6726.0
2251.6
6531.4
5767.6
9187.1
10 522.1
8291.2
7860.0
8071.0
8113.1
8091.4
5791.5
4362.3
3204.5

0
0
0
0
0
0
0
0
0
1
0
0
10
10
0
0
0
0
0
0
0
0
10
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
1
3
8
7
0
1
10
0
0
0
8
0
0
0
0
10
0
0
0
5
0
0
8
0
0
0
0
0
0
0
3
2
10
10
10

26.08
128.55
230.18
381.76
465.58
736.32
1542.57
2217.36
1461.27
23.78
29.21
304.28
0.02
0.03
19.77
1991.59
47.29
102.16
62.95
21.58
708.19
24.33
0.02
24.51
1354.16
248.43
268.98
861.25
19.13
25.33
685.39
400.04
240.40
353.33
601.93
1256.18
1027.12
1881.83
2106.27
2022.04

0.15
0.26
0.37
0.73
1.11
2.76
5.40
9.33
17.96
0.07
0.06
3281.72
0.04
0.14
0.09
18.87
0.07
0.07
0.09
0.03
206.01
0.08
0.11
0.04
1.59
2.45
3.76
2967.50
0.07
0.32
0.97
0.38
3.28
3.53
3.80
3.97
4.53
6.17
7.79
18.14

averages

1367.78

6528.4

0.8

2.4

597.53

164.35

16

0.40
0.59
0.59
0.83
1.16
1.76
2.74
3.51
4.89
0.17
0.15
30.13
0.21
0.37
0.20
16.31
0.17
0.16
0.20
0.14
5.95
0.16
0.34
0.14
0.90
1.42
1.03
9.39
0.15
0.28
0.39
0.22
6.05
6.23
6.30
6.36
6.55
7.42
7.97
11.50

3.59

Table 5: Averages of the train accuracies and standard deviations for three algorithms with ρ = 0.1
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

79.23 ± 1.93
78.20 ± 1.82
77.42 ± 1.02
77.02 ± 1.03
77.75 ± 0.97
77.45 ± 0.79
77.11 ± 1.00
77.89 ± 2.84
77.34 ± 2.20
76.56 ± 10.27
95.89 ± 1.46
67.89 ± 3.82
99.00 ± 1.41
99.09 ± 1.59
71.65 ± 1.41
82.38 ± 4.23
69.20 ± 4.23
73.16 ± 1.93
71.66 ± 1.43
77.27 ± 3.83
90.31 ± 0.08
90.00 ± 2.89
98.42 ± 1.84
77.17 ± 1.76
55.80 ± 2.57
86.78 ± 9.01
79.39 ± 10.55
83.04 ± 4.50
79.34 ± 4.62
65.68 ± 7.26
92.61 ± 3.80
78.87 ± 1.46
97.76 ± 0.14
97.54 ± 0.14
97.56 ± 0.17
97.52 ± 0.12
97.56 ± 0.07
97.32 ± 0.13
97.38 ± 0.12
97.40 ± 0.12

*
*
*
*
*
*
*
*
*
*
*

*
*

*
*

*
*

*
*
*
*
*
*
*
*

averages

25 83.52 ± 2.51

L

4.8
4.1
4.2
4.0
4.3
3.9
3.7
2.9
3.0
4.4
6.2
1.3
7.6
6.4
4.3
4.8
4.2
3.1
4.6
4.3
3.5
6.4
3.4
5.4
6.0
5.2
2.5
2.4
5.1
3.7
5.5
4.8
6.6
6.1
6.1
6.0
6.3
5.9
5.9
5.8

4.7

LPBoost
score

77.41 ± 1.23
77.00 ± 1.79
76.41 ± 0.80
75.89 ± 0.82
76.39 ± 0.87
76.99 ± 1.02
76.37 ± 1.08
76.00 ± 0.32
76.09 ± 0.34
74.55 ± 9.16
95.32 ± 1.50
67.87 ± 3.80
95.20 ± 3.01
95.91 ± 2.79
70.66 ± 1.19
79.49 ± 6.40
69.28 ± 3.85
73.65 ± 1.79
70.66 ± 1.19
75.05 ± 3.57
* 90.45 ± 0.46
89.61 ± 2.89
92.37 ± 3.15
75.93 ± 2.61
55.08 ± 2.85
78.89 ± 10.42
76.63 ± 12.79
82.88 ± 4.69
80.36 ± 4.25
64.68 ± 7.70
92.28 ± 4.00
78.21 ± 1.32
97.47 ± 0.18
97.22 ± 0.16
97.31 ± 0.10
97.28 ± 0.08
97.29 ± 0.05
97.16 ± 0.11
97.18 ± 0.11
97.21 ± 0.10

1

82.29 ± 2.61

L

4.1
5.7
4.6
4.6
4.0
4.3
3.8
3.8
3.6
4.7
8.4
2.4
6.4
4.8
3.9
9.1
4.9
4.2
3.9
4.6
5.3
10.3
2.7
6.2
3.1
6.1
3.9
3.1
13.0
6.5
6.5
6.0
5.3
4.8
5.3
5.5
4.6
5.7
5.5
5.6

5.3

17

AdaBoost
score

L

*

*
*

77.31 ±1.00
9.3
76.42 ±0.94
9.6
77.00 ±1.10 10.9
76.84 ±1.07 11.0
77.19 ±0.99 11.0
77.30 ±1.03 11.0
77.11 ±1.09 11.0
76.56 ±0.61 11.0
76.58 ±0.65 10.9
76.03 ±8.71 24.1
95.27 ±1.65 20.2
68.00 ±4.21 55.3
*
* 100.00 ±0.00 58.7
* 100.00 ±0.00 56.7
71.34 ±1.62 19.1
82.23 ±3.26 95.1
73.54 ±3.60 46.2
76.41 ±1.40 38.0
71.34 ±1.62 19.1
77.13 ±5.64 20.1
90.34 ±0.06 71.6
97.62 ±2.07 49.1
*
* 100.00 ±0.00 36.9
84.83 ±1.81 37.5
*
67.29 ±1.79 97.2
*
77.60 ±6.31
9.7
71.67 ±8.51
8.0
83.33 ±4.64 50.4
99.46 ±0.82 64.6
66.89 ±6.23 21.0
92.92 ±3.71 62.9
81.36 ±2.38 70.7
97.31 ±0.15 14.9
97.13 ±0.10 15.5
97.25 ±0.12 16.1
97.21 ±0.10 16.1
97.28 ±0.10 16.1
97.11 ±0.13 16.1
97.16 ±0.11 16.1
97.19 ±0.11 16.5

*
*
*
*
*

15

84.36 ±1.99 31.4

Table 6: Averages of the test accuracies and standard deviations for three algorithms with ρ = 0.1
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

78.32 ± 3.25
77.88 ± 2.23
77.47 ± 1.39
76.44 ± 1.93
77.18 ± 1.51
77.77 ± 1.22
77.23 ± 1.26
77.96 ± 2.69
77.35 ± 2.29
75.00 ± 10.15
93.97 ± 1.89
67.82 ± 3.66
71.67 ± 13.15
98.95 ± 1.84
72.35 ± 3.64
81.40 ± 4.69
67.32 ± 6.22
72.33 ± 5.19
72.25 ± 3.63
73.70 ± 6.52
90.34 ± 0.22
84.71 ± 5.43
70.29 ± 7.26
54.35 ± 2.70
53.30 ± 4.44
87.08 ± 8.65
79.02 ± 10.87
83.06 ± 4.56
66.83 ± 8.54
65.14 ± 7.83
90.77 ± 4.88
79.76 ± 18.54
97.29 ± 0.17
97.38 ± 0.13
97.35 ± 0.11
97.37 ± 0.16
97.37 ± 0.11
97.38 ± 0.13
97.39 ± 0.12
97.26 ± 0.12

*
*
*
*
*

*
*
*
*

*
*

*
*

*
*

*

*
*
*
*
*
*
*
*

averages

24 80.70 ± 4.08

L

4.8
4.1
4.2
4.0
4.3
3.9
3.7
2.9
3.0
4.4
6.2
1.3
7.6
6.4
4.3
4.8
4.2
3.1
4.6
4.3
3.5
6.4
3.4
5.4
6.0
5.2
2.5
2.4
5.1
3.7
5.5
4.8
6.6
6.1
6.1
6.0
6.3
5.9
5.9
5.8

4.7

L

4.1
5.7
4.6
4.6
4.0
4.3
3.8
3.8
3.6
4.7
8.4
2.4
6.4
4.8
3.9
9.1
4.9
4.2
3.9
4.6
5.3
10.3
2.7
6.2
3.1
6.1
3.9
3.1
13.0
6.5
6.5
6.0
5.3
4.8
5.3
5.5
4.6
5.7
5.5
5.6

5.3

LPBoost
score

76.70 ± 2.94
76.51 ± 2.30
75.64 ± 1.62
75.23 ± 1.75
75.70 ± 1.85
77.58 ± 1.35
76.52 ± 1.10
75.97 ± 0.49
76.24 ± 0.58
72.32 ± 9.84
93.09 ± 2.53
67.78 ± 3.51
70.83 ± 9.82
95.53 ± 3.05
71.90 ± 3.26
79.05 ± 5.96
68.04 ± 4.97
72.44 ± 4.29
71.90 ± 3.26
71.30 ± 4.80
90.63 ± 0.89
85.57 ± 4.18
73.24 ± 8.48
55.95 ± 3.20
53.67 ± 4.22
79.35 ± 10.05
76.63 ± 12.76
82.91 ± 4.76
69.27 ± 9.14
64.67 ± 8.02
90.69 ± 4.78
86.10 ± 16.78
97.21 ± 0.12
97.21 ± 0.09
97.21 ± 0.10
97.18 ± 0.10
97.13 ± 0.08
97.23 ± 0.09
97.21 ± 0.10
97.13 ± 0.10

*
*

*

*
*

*

*

7

80.16 ± 3.93

18

AdaBoost
score

L

*
*

76.04 ± 3.30
9.3
75.67 ± 1.57
9.6
76.84 ± 1.03 10.9
76.18 ± 1.72 11.0
76.72 ± 1.77 11.0
77.84 ± 1.31 11.0
77.28 ± 1.39 11.0
76.46 ± 0.64 11.0
76.54 ± 0.82 10.9
72.61 ± 9.54 24.1
93.01 ± 2.48 20.2
67.94 ± 4.03 55.3
*
76.67 ±14.59 58.7
*
* 100.00 ± 0.00 56.7
71.80 ± 3.32 19.1
80.40 ± 3.62 95.1
66.80 ± 5.07 46.2
72.09 ± 4.02 38.0
71.80 ± 3.32 19.1
69.81 ± 5.09 20.1
90.30 ± 0.28 71.6
86.71 ± 4.86 49.1
80.88 ± 7.50 36.9
55.90 ± 3.02 37.5
52.12 ± 1.82 97.2
77.70 ± 6.29
9.7
71.27 ± 8.52
8.0
83.35 ± 4.71 50.4
63.90 ±11.14 64.6
65.19 ± 6.65 21.0
90.40 ± 5.01 62.9
63.90 ±23.36
70.7
97.11 ± 0.14 14.9
97.15 ± 0.11 15.5
97.18 ± 0.11 16.1
97.17 ± 0.12 16.1
97.14 ± 0.11 16.1
97.22 ± 0.11 16.1
97.21 ± 0.11 16.1
97.12 ± 0.10 16.5

*
*

*

*

9

79.79 ± 3.82 31.4

Table 7: Averages of the train accuracies and standard deviations for three algorithms with ρ = 0.075
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

79.35 ± 2.14
78.68 ± 1.60
78.05 ± 1.11
77.73 ± 0.95
78.18 ± 1.28
77.68 ± 0.99
77.45 ± 1.22
77.84 ± 2.45
76.87 ± 1.76
76.00 ± 9.33
96.36 ± 1.01
67.89 ± 3.82
98.80 ± 1.40
98.41 ± 1.87
71.84 ± 1.64
83.14 ± 3.31
69.98 ± 4.30
74.13 ± 1.81
71.73 ± 1.51
78.84 ± 3.54
90.42 ± 0.23
91.28 ± 2.72
98.68 ± 1.86
78.14 ± 2.03
56.13 ± 2.84
88.14 ± 7.75
83.89 ± 7.85
83.04 ± 4.54
80.30 ± 4.68
65.83 ± 7.11
92.73 ± 3.73
79.40 ± 1.59
97.88 ± 0.15
97.67 ± 0.21
97.65 ± 0.16
97.59 ± 0.10
97.61 ± 0.12
97.45 ± 0.15
97.43 ± 0.11
97.42 ± 0.12

*
*
*
*
*
*
*
*
*

*

*
*

*
*

*
*

*
*
*
*
*
*
*
*

averages

24 83.94 ± 2.38

L

5.5
5.4
6.0
6.0
5.3
4.6
5.4
3.6
3.0
5.6
7.5
1.5
6.3
5.9
5.1
5.5
5.7
4.0
5.5
6.3
4.4
7.1
3.4
6.7
9.9
6.0
3.5
2.0
6.1
4.8
6.3
6.4
7.9
8.5
7.9
8.4
7.6
7.9
7.2
7.2

5.8

LPBoost
score

77.37 ± 1.20
77.25 ± 1.88
76.43 ± 0.82
75.90 ± 0.82
76.40 ± 0.87
76.73 ± 0.95
76.37 ± 1.08
76.01 ± 0.32
76.09 ± 0.34
74.66 ± 9.10
95.36 ± 1.51
67.87 ± 3.80
94.40 ± 1.58
96.14 ± 2.84
70.67 ± 1.19
81.94 ± 4.56
69.67 ± 4.12
74.26 ± 1.43
70.67 ± 1.19
75.42 ± 3.80
* 90.45 ± 0.46
91.00 ± 2.98
92.37 ± 3.15
76.07 ± 2.43
55.47 ± 2.88
79.50 ± 10.21
76.54 ± 12.87
82.91 ± 4.73
83.11 ± 4.81
64.76 ± 7.64
92.34 ± 3.90
78.54 ± 1.24
97.46 ± 0.20
97.23 ± 0.18
97.31 ± 0.11
97.28 ± 0.09
97.30 ± 0.05
97.16 ± 0.11
97.19 ± 0.11
97.21 ± 0.10

1

82.52 ± 2.54

L

4.7
6.3
5.0
4.5
4.4
3.9
3.7
3.8
3.6
4.9
9.7
2.4
5.1
5.0
4.2
13.5
5.3
5.5
4.2
5.3
6.1
13.9
2.7
7.4
3.6
6.0
4.1
3.6
17.6
7.2
6.8
7.0
5.6
5.0
6.0
5.8
4.8
5.8
5.6
5.7

5.9

19

AdaBoost
score

L

77.31 ±1.00
9.3
76.42 ±0.94
9.6
77.00 ±1.10 10.9
76.84 ±1.07 11.0
77.19 ±0.99 11.0
77.30 ±1.03 11.0
77.11 ±1.09 11.0
76.56 ±0.61 11.0
76.58 ±0.65 10.9
76.03 ±8.71 24.1
95.27 ±1.65 20.2
68.00 ±4.21 55.3
100.00 ±0.00 58.7
100.00 ±0.00 56.7
71.34 ±1.62 19.1
82.23 ±3.26 95.1
73.54 ±3.60 46.2
76.41 ±1.40 38.0
71.34 ±1.62 19.1
77.13 ±5.64 20.1
90.34 ±0.06 71.6
97.62 ±2.07 49.1
100.00 ±0.00 36.9
84.83 ±1.81 37.5
67.29 ±1.79 97.2
77.60 ±6.31
9.7
71.67 ±8.51
8.0
83.33 ±4.64 50.4
99.46 ±0.82 64.6
66.89 ±6.23 21.0
92.92 ±3.71 62.9
81.36 ±2.38 70.7
97.31 ±0.15 14.9
97.13 ±0.10 15.5
97.25 ±0.12 16.1
97.21 ±0.10 16.1
97.28 ±0.10 16.1
97.11 ±0.13 16.1
97.16 ±0.11 16.1
97.19 ±0.11 16.5

*

*
*
*

*
*

*
*
*
*

*
*
*
*
*

15

84.36 ±1.99 31.4

Table 8: Averages of the test accuracies and standard deviations for three algorithms with ρ = 0.075
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

78.60 ± 3.49
78.28 ± 1.69
77.60 ± 1.19
76.90 ± 2.10
77.46 ± 1.71
78.08 ± 1.23
77.52 ± 1.33
77.84 ± 2.40
76.91 ± 2.09
72.46 ± 9.38
93.46 ± 2.18
67.82 ± 3.66
70.83 ± 9.82
98.16 ± 2.17
72.50 ± 3.44
82.38 ± 4.17
66.80 ± 5.09
72.50 ± 4.64
72.10 ± 3.21
71.48 ± 6.88
90.42 ± 0.34
86.14 ± 5.68
71.18 ± 7.57
54.90 ± 3.16
52.88 ± 4.27
88.17 ± 8.18
83.72 ± 8.15
83.03 ± 4.61
67.32 ± 9.97
65.03 ± 7.84
90.93 ± 4.72
78.29 ± 18.09
97.31 ± 0.22
97.45 ± 0.14
97.40 ± 0.14
97.46 ± 0.11
97.42 ± 0.14
97.47 ± 0.14
97.42 ± 0.08
97.27 ± 0.13

*
*
*
*
*
*
*
*
*

*

*
*

*
*

*
*

*

*
*
*
*
*
*
*
*

averages

25 80.77 ± 3.89

L

5.5
5.4
6.0
6.0
5.3
4.6
5.4
3.6
3.0
5.6
7.5
1.5
6.3
5.9
5.1
5.5
5.7
4.0
5.5
6.3
4.4
7.1
3.4
6.7
9.9
6.0
3.5
2.0
6.1
4.8
6.3
6.4
7.9
8.5
7.9
8.4
7.6
7.9
7.2
7.2

5.8

LPBoost
score

76.73 ± 2.94
76.62 ± 2.35
75.60 ± 1.60
75.23 ± 1.75
75.73 ± 1.87
77.42 ± 1.05
76.52 ± 1.10
75.97 ± 0.49
76.24 ± 0.58
72.54 ± 9.86
93.24 ± 2.13
67.77 ± 3.50
71.67 ± 10.54
95.79 ± 3.09
71.85 ± 3.22
81.28 ± 4.22
* 67.58 ± 4.02
* 73.08 ± 3.80
71.85 ± 3.22
71.11 ± 4.55
* 90.60 ± 0.90
86.14 ± 5.47
73.24 ± 8.48
55.85 ± 3.11
* 53.45 ± 4.24
79.76 ± 9.96
76.46 ± 12.91
82.94 ± 4.79
* 69.76 ± 8.70
64.69 ± 8.00
90.92 ± 4.48
* 83.17 ± 17.83
97.19 ± 0.11
97.21 ± 0.09
97.20 ± 0.11
97.20 ± 0.10
97.14 ± 0.08
97.23 ± 0.09
97.21 ± 0.11
97.13 ± 0.11

6

80.21 ± 3.89

L

4.7
6.3
5.0
4.5
4.4
3.9
3.7
3.8
3.6
4.9
9.7
2.4
5.1
5.0
4.2
13.5
5.3
5.5
4.2
5.3
6.1
13.9
2.7
7.4
3.6
6.0
4.1
3.6
17.6
7.2
6.8
7.0
5.6
5.0
6.0
5.8
4.8
5.8
5.6
5.7

5.9

20

AdaBoost
score

L

*

76.04 ± 3.30
9.3
75.67 ± 1.57
9.6
76.84 ± 1.03 10.9
76.18 ± 1.72 11.0
76.72 ± 1.77 11.0
77.84 ± 1.31 11.0
77.28 ± 1.39 11.0
76.46 ± 0.64 11.0
76.54 ± 0.82 10.9
72.61 ± 9.54 24.1
93.01 ± 2.48 20.2
67.94 ± 4.03 55.3
*
76.67 ±14.59
*
58.7
* 100.00 ± 0.00 56.7
71.80 ± 3.32 19.1
80.40 ± 3.62 95.1
66.80 ± 5.07 46.2
72.09 ± 4.02 38.0
71.80 ± 3.32 19.1
69.81 ± 5.09 20.1
90.30 ± 0.28 71.6
86.71 ± 4.86 49.1
80.88 ± 7.50 36.9
55.90 ± 3.02 37.5
52.12 ± 1.82 97.2
77.70 ± 6.29
9.7
71.27 ± 8.52
8.0
83.35 ± 4.71 50.4
63.90 ±11.14 64.6
65.19 ± 6.65 21.0
90.40 ± 5.01 62.9
63.90 ±23.36
70.7
97.11 ± 0.14 14.9
97.15 ± 0.11 15.5
97.18 ± 0.11 16.1
97.17 ± 0.12 16.1
97.14 ± 0.11 16.1
97.22 ± 0.11 16.1
97.21 ± 0.11 16.1
97.12 ± 0.10 16.5

*
*
*

*

*

9

79.79 ± 3.82 31.4

Table 9: Averages of the train accuracies and standard deviations for three algorithms with ρ = 0.05
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

* 79.97 ± 1.80
* 79.06 ± 1.73
* 78.67 ± 1.08
* 78.24 ± 1.27
* 78.46 ± 1.18
* 77.94 ± 0.84
* 77.69 ± 1.05
* 78.11 ± 2.44
76.31 ± 0.58
* 77.52 ± 9.86
* 96.49 ± 1.43
* 68.10 ± 4.49
98.20 ± 1.48
97.73 ± 1.86
* 71.90 ± 1.92
* 84.60 ± 2.93
71.07 ± 3.75
76.29 ± 1.56
* 72.01 ± 1.98
* 81.02 ± 2.90
* 90.47 ± 0.47
92.56 ± 2.59
98.68 ± 1.86
80.28 ± 2.16
56.45 ± 3.08
* 87.86 ± 7.83
* 80.86 ± 10.96
83.12 ± 4.60
83.53 ± 3.76
66.25 ± 6.92
92.90 ± 3.78
79.69 ± 1.79
* 98.07 ± 0.19
* 97.78 ± 0.22
* 97.75 ± 0.16
* 97.67 ± 0.13
* 97.68 ± 0.13
* 97.49 ± 0.16
* 97.48 ± 0.12
* 97.52 ± 0.11

averages

26 84.34 ± 2.43

L

7.0
6.2
7.9
7.8
7.8
6.7
7.1
4.5
2.9
7.3
9.6
1.8
5.5
5.6
6.5
7.0
7.4
8.6
6.5
7.3
5.8
9.2
3.5
9.1
31.4
8.6
3.5
2.5
12.3
5.0
8.6
8.6
10.8
10.7
11.0
10.9
11.1
10.2
10.1
9.8

8.1

LPBoost
score

77.41 ± 1.23
76.66 ± 1.01
76.37 ± 0.83
75.93 ± 0.82
76.41 ± 0.87
76.94 ± 0.96
76.37 ± 1.09
76.01 ± 0.32
76.09 ± 0.34
74.86 ± 8.97
95.63 ± 1.56
67.90 ± 3.91
94.20 ± 1.48
96.14 ± 2.84
70.67 ± 1.19
83.90 ± 3.73
69.93 ± 3.89
74.33 ± 1.58
70.68 ± 1.19
75.79 ± 4.08
* 90.47 ± 0.45
92.21 ± 3.06
92.37 ± 3.15
77.38 ± 2.00
56.69 ± 2.67
81.60 ± 8.18
76.96 ± 12.46
82.91 ± 4.73
88.08 ± 4.43
65.74 ± 6.97
92.53 ± 3.84
78.65 ± 1.35
97.46 ± 0.18
97.25 ± 0.17
97.33 ± 0.15
97.31 ± 0.10
97.32 ± 0.08
97.17 ± 0.11
97.19 ± 0.12
97.20 ± 0.10

1

82.90 ± 2.40

L

4.6
6.5
5.0
4.8
4.4
4.9
4.2
4.1
3.7
5.8
9.8
2.4
5.1
5.0
4.4
20.9
6.6
6.1
4.4
6.4
7.9
19.5
2.7
9.5
14.0
7.0
4.0
3.7
25.4
7.6
6.9
8.9
5.2
5.5
6.6
6.0
5.0
5.7
5.6
5.6

7.0

AdaBoost
score

L

77.31 ±1.00
9.3
76.42 ±0.94
9.6
77.00 ±1.10 10.9
76.84 ±1.07 11.0
77.19 ±0.99 11.0
77.30 ±1.03 11.0
77.11 ±1.09 11.0
76.56 ±0.61 11.0
76.58 ±0.65 10.9
76.03 ±8.71 24.1
95.27 ±1.65 20.2
68.00 ±4.21 55.3
100.00 ±0.00 58.7
100.00 ±0.00 56.7
71.34 ±1.62 19.1
82.23 ±3.26 95.1
73.54 ±3.60 46.2
76.41 ±1.40 38.0
71.34 ±1.62 19.1
77.13 ±5.64 20.1
90.34 ±0.06 71.6
97.62 ±2.07 49.1
100.00 ±0.00 36.9
84.83 ±1.81 37.5
67.28 ±1.79 97.2
77.60 ±6.31
9.7
71.67 ±8.51
8.0
83.33 ±4.64 50.4
99.46 ±0.82 64.6
66.89 ±6.23 21.0
92.92 ±3.71 62.9
81.36 ±2.38 70.7
97.31 ±0.15 14.9
97.13 ±0.10 15.5
97.25 ±0.12 16.1
97.21 ±0.10 16.1
97.28 ±0.10 16.1
97.11 ±0.13 16.1
97.16 ±0.11 16.1
97.19 ±0.11 16.5

*

*
*

*
*

*
*
*
*

*
*
*
*
*

14

84.36 ±1.99 31.4

21

Table 10: Averages of the test accuracies and standard deviations for three algorithms with ρ = 0.05
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

* 79.10 ± 3.44
* 78.08 ± 2.22
* 78.32 ± 1.42
* 77.55 ± 2.21
* 77.39 ± 1.25
* 78.48 ± 1.13
* 77.86 ± 1.20
* 77.98 ± 2.37
76.36 ± 0.67
* 74.35 ± 10.93
* 93.97 ± 2.19
* 68.08 ± 4.48
75.00 ± 13.03
97.37 ± 2.15
71.65 ± 3.67
* 83.69 ± 3.52
66.93 ± 5.60
72.38 ± 4.59
* 71.85 ± 3.57
* 73.89 ± 7.43
* 90.60 ± 0.92
85.43 ± 5.16
70.59 ± 7.59
54.25 ± 2.85
52.40 ± 4.42
* 87.91 ± 7.92
* 80.85 ± 10.96
83.15 ± 4.65
66.34 ± 8.03
65.13 ± 7.75
* 90.93 ± 4.81
73.66 ± 19.30
* 97.37 ± 0.26
* 97.49 ± 0.21
* 97.48 ± 0.15
* 97.47 ± 0.12
* 97.49 ± 0.15
* 97.53 ± 0.13
* 97.48 ± 0.10
* 97.37 ± 0.11

averages

26 80.78 ± 4.07

L

7.0
6.2
7.9
7.8
7.8
6.7
7.1
4.5
2.9
7.3
9.6
1.8
5.5
5.6
6.5
7.0
7.4
8.6
6.5
7.3
5.8
9.2
3.5
9.1
31.4
8.6
3.5
2.5
12.3
5.0
8.6
8.6
10.8
10.7
11.0
10.9
11.1
10.2
10.1
9.8

8.1

LPBoost
score

76.70 ± 2.99
75.83 ± 2.36
75.54 ± 1.46
75.26 ± 1.78
75.74 ± 1.88
77.54 ± 1.27
76.52 ± 1.10
75.97 ± 0.49
76.24 ± 0.58
72.75 ± 9.47
93.68 ± 1.90
67.82 ± 3.64
72.50 ± 11.82
95.79 ± 3.09
* 71.85 ± 3.22
83.14 ± 3.84
* 67.84 ± 3.47
* 73.43 ± 3.73
* 71.85 ± 3.22
71.11 ± 4.55
90.52 ± 0.93
85.86 ± 4.88
73.24 ± 8.48
55.70 ± 2.37
* 52.78 ± 4.16
81.77 ± 7.78
76.66 ± 12.71
82.94 ± 4.79
* 68.78 ± 7.07
65.01 ± 7.78
90.70 ± 4.66
* 83.90 ± 16.87
97.16 ± 0.13
97.20 ± 0.13
97.22 ± 0.11
97.21 ± 0.11
97.16 ± 0.10
97.23 ± 0.09
97.21 ± 0.11
97.12 ± 0.11

7

80.31 ± 3.73

L

4.6
6.5
5.0
4.8
4.4
4.9
4.2
4.1
3.7
5.8
9.8
2.4
5.1
5.0
4.4
20.9
6.6
6.1
4.4
6.4
7.9
19.5
2.7
9.5
14.0
7.0
4.0
3.7
25.4
7.6
6.9
8.9
5.2
5.5
6.6
6.0
5.0
5.7
5.6
5.6

7.0

22

AdaBoost
score

L

*

76.04 ± 3.30
9.3
75.67 ± 1.57
9.6
76.84 ± 1.03 10.9
76.18 ± 1.72 11.0
76.72 ± 1.77 11.0
77.84 ± 1.31 11.0
77.28 ± 1.39 11.0
76.46 ± 0.64 11.0
76.54 ± 0.82 10.9
72.61 ± 9.54 24.1
93.01 ± 2.48 20.2
67.94 ± 4.03 55.3
76.67 ±14.59
*
58.7
* 100.00 ± 0.00 56.7
71.80 ± 3.32 19.1
80.40 ± 3.62 95.1
66.80 ± 5.07 46.2
72.09 ± 4.02 38.0
71.80 ± 3.32 19.1
69.81 ± 5.09 20.1
90.30 ± 0.28 71.6
86.71 ± 4.86 49.1
80.88 ± 7.50 36.9
55.90 ± 3.02 37.5
52.12 ± 1.82 97.2
77.70 ± 6.29
9.7
71.27 ± 8.52
8.0
83.35 ± 4.71 50.4
63.90 ±11.14 64.6
65.19 ± 6.65 21.0
90.40 ± 5.01 62.9
63.90 ±23.36 70.7
97.11 ± 0.14 14.9
97.15 ± 0.11 15.5
97.18 ± 0.11 16.1
97.17 ± 0.12 16.1
97.14 ± 0.11 16.1
97.22 ± 0.11 16.1
97.21 ± 0.11 16.1
97.12 ± 0.10 16.5

*
*
*

*

*

8

79.79 ± 3.82 31.4

Table 11: Averages of the train accuracies and standard deviations for three algorithms with ρ = 0.025
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

* 80.80 ± 1.83
* 79.71 ± 1.46
* 78.77 ± 1.20
* 78.26 ± 1.42
* 78.99 ± 1.46
* 78.06 ± 1.17
* 77.56 ± 1.08
* 78.66 ± 3.08
* 77.11 ± 1.81
* 78.24 ± 9.62
* 96.82 ± 1.49
* 68.13 ± 4.59
97.60 ± 1.26
95.91 ± 2.58
* 72.25 ± 1.76
85.06 ± 3.48
72.49 ± 3.36
* 76.88 ± 1.26
* 72.29 ± 1.78
* 80.14 ± 4.41
* 90.56 ± 0.62
95.12 ± 2.41
98.68 ± 1.86
83.10 ± 2.28
57.26 ± 3.17
* 92.71 ± 5.86
* 80.02 ± 10.35
83.20 ± 4.69
91.02 ± 3.30
* 67.26 ± 6.44
* 93.11 ± 3.74
80.29 ± 1.69
* 98.24 ± 0.19
* 97.98 ± 0.21
* 97.94 ± 0.19
* 97.83 ± 0.14
* 97.79 ± 0.17
* 97.62 ± 0.11
* 97.60 ± 0.15
* 97.58 ± 0.11

averages

29 84.97 ± 2.44

L

10.8
10.7
11.9
11.4
11.6
8.6
8.6
5.9
5.2
9.0
11.9
2.2
5.4
5.1
8.2
13.2
12.1
11.1
8.4
9.3
11.9
21.9
3.3
14.4
9.4
12.3
5.8
3.8
42.1
7.7
12.0
12.7
20.4
16.7
17.7
17.5
17.5
16.9
14.8
14.1

11.8

LPBoost
score

77.38 ± 1.21
76.72 ± 1.02
76.43 ± 0.82
75.90 ± 0.82
76.41 ± 0.87
76.75 ± 0.96
76.37 ± 1.09
76.01 ± 0.32
76.09 ± 0.34
75.27 ± 8.74
95.89 ± 1.62
67.90 ± 3.90
94.20 ± 1.48
95.23 ± 2.92
70.91 ± 1.35
* 85.31 ± 4.74
71.01 ± 3.73
75.59 ± 1.47
70.91 ± 1.35
76.81 ± 4.89
90.53 ± 0.60
95.09 ± 2.39
92.37 ± 3.15
79.93 ± 1.94
59.02 ± 2.24
81.88 ± 9.32
77.03 ± 12.40
83.00 ± 4.84
92.93 ± 3.79
66.28 ± 6.61
92.71 ± 3.76
79.61 ± 1.60
97.44 ± 0.17
97.29 ± 0.21
97.31 ± 0.15
97.32 ± 0.10
97.33 ± 0.09
97.17 ± 0.11
97.20 ± 0.12
97.23 ± 0.10

1

83.39 ± 2.43

L

4.9
5.7
4.9
4.6
4.5
4.6
4.3
4.3
3.5
7.2
12.1
3.3
5.1
4.8
4.6
42.6
11.2
7.7
4.6
7.9
12.8
36.1
2.7
12.8
34.7
7.7
3.9
4.2
42.8
9.7
13.9
13.0
4.9
5.9
5.5
5.5
5.3
5.7
5.6
6.0

9.8

AdaBoost
score

L

77.31 ±1.00
9.3
76.42 ±0.94
9.6
77.00 ±1.10 10.9
76.84 ±1.07 11.0
77.19 ±0.99 11.0
77.30 ±1.03 11.0
77.11 ±1.09 11.0
76.56 ±0.61 11.0
76.58 ±0.65 10.9
76.03 ±8.71 24.1
95.27 ±1.65 20.2
68.00 ±4.21 55.3
100.00 ±0.00 58.7
100.00 ±0.00 56.7
71.34 ±1.62 19.1
82.23 ±3.26 95.1
73.54 ±3.60 46.2
76.41 ±1.40 38.0
71.34 ±1.62 19.1
77.13 ±5.64 20.1
90.34 ±0.06 71.6
97.62 ±2.07 49.1
100.00 ±0.00 36.9
84.83 ±1.81 37.5
67.28 ±1.79 97.2
77.60 ±6.31
9.7
71.67 ±8.51
8.0
83.33 ±4.64 50.4
99.46 ±0.82 64.6
66.89 ±6.23 21.0
92.92 ±3.71 62.9
81.36 ±2.38 70.7
97.31 ±0.15 14.9
97.13 ±0.10 15.5
97.25 ±0.12 16.1
97.21 ±0.10 16.1
97.28 ±0.10 16.1
97.11 ±0.13 16.1
97.16 ±0.11 16.1
97.19 ±0.11 16.5

*
*

*

*
*
*
*

*
*

*

10

84.36 ±1.99 31.4

23

Table 12: Averages of the test accuracies and standard deviations for three algorithms with ρ = 0.025
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

* 80.12 ± 2.80
* 78.98 ± 1.83
* 78.15 ± 1.64
* 76.98 ± 1.75
* 77.81 ± 1.72
* 78.40 ± 1.23
* 77.61 ± 1.08
* 78.67 ± 2.93
* 77.18 ± 1.97
* 74.28 ± 10.40
* 93.46 ± 1.85
* 68.11 ± 4.60
70.00 ± 9.78
95.79 ± 2.54
71.65 ± 3.07
84.29 ± 3.58
66.27 ± 4.58
71.74 ± 4.67
71.55 ± 3.13
70.56 ± 7.33
* 90.51 ± 0.79
85.57 ± 6.48
70.88 ± 8.02
54.85 ± 2.49
* 52.83 ± 3.83
* 92.83 ± 6.11
* 79.46 ± 10.76
83.23 ± 4.76
67.56 ± 7.97
65.26 ± 7.52
* 90.55 ± 4.78
70.00 ± 20.44
* 97.30 ± 0.20
* 97.47 ± 0.19
* 97.53 ± 0.16
* 97.58 ± 0.17
* 97.52 ± 0.15
* 97.64 ± 0.13
* 97.56 ± 0.11
* 97.41 ± 0.14

averages

25 80.63 ± 3.94

L

10.8
10.7
11.9
11.4
11.6
8.6
8.6
5.9
5.2
9.0
11.9
2.2
5.4
5.1
8.2
13.2
12.1
11.1
8.4
9.3
11.9
21.9
3.3
14.4
9.4
12.3
5.8
3.8
42.1
7.7
12.0
12.7
20.4
16.7
17.7
17.5
17.5
16.9
14.8
14.1

11.8

LPBoost
score

76.70 ± 2.99
75.96 ± 2.26
75.60 ± 1.60
75.23 ± 1.75
75.74 ± 1.88
77.44 ± 1.04
76.52 ± 1.10
75.97 ± 0.49
76.24 ± 0.58
72.68 ± 9.54
93.38 ± 1.70
67.81 ± 3.63
73.33 ± 10.97
94.74 ± 3.28
71.70 ± 3.21
* 84.35 ± 4.95
* 67.45 ± 3.56
* 72.44 ± 4.19
71.70 ± 3.21
* 71.30 ± 4.96
90.49 ± 0.81
85.43 ± 5.75
73.24 ± 8.48
55.80 ± 2.39
52.60 ± 4.13
81.94 ± 9.51
76.95 ± 12.43
83.03 ± 4.91
* 70.24 ± 11.02
* 65.38 ± 7.51
90.33 ± 4.62
* 79.02 ± 17.10
97.17 ± 0.12
97.27 ± 0.14
97.20 ± 0.12
97.20 ± 0.12
97.15 ± 0.11
97.23 ± 0.09
97.22 ± 0.12
97.15 ± 0.10

7

80.21 ± 3.91

L

4.9
5.7
4.9
4.6
4.5
4.6
4.3
4.3
3.5
7.2
12.1
3.3
5.1
4.8
4.6
42.6
11.2
7.7
4.6
7.9
12.8
36.1
2.7
12.8
34.7
7.7
3.9
4.2
42.8
9.7
13.9
13.0
4.9
5.9
5.5
5.5
5.3
5.7
5.6
6.0

9.8

24

AdaBoost
score

L

*

76.04 ± 3.30
9.3
75.67 ± 1.57
9.6
76.84 ± 1.03 10.9
76.18 ± 1.72 11.0
76.72 ± 1.77 11.0
77.84 ± 1.31 11.0
77.28 ± 1.39 11.0
76.46 ± 0.64 11.0
76.54 ± 0.82 10.9
72.61 ± 9.54 24.1
93.01 ± 2.48 20.2
67.94 ± 4.03 55.3
76.67 ±14.59
*
58.7
* 100.00 ± 0.00 56.7
71.80 ± 3.32 19.1
*
80.40 ± 3.62 95.1
66.80 ± 5.07 46.2
72.09 ± 4.02 38.0
71.80 ± 3.32 19.1
69.81 ± 5.09 20.1
90.30 ± 0.28 71.6
86.71 ± 4.86 49.1
80.88 ± 7.50 36.9
55.90 ± 3.02 37.5
52.12 ± 1.82 97.2
77.70 ± 6.29
9.7
71.27 ± 8.52
8.0
83.35 ± 4.71 50.4
63.90 ±11.14 64.6
65.19 ± 6.65 21.0
90.40 ± 5.01 62.9
63.90 ±23.36 70.7
97.11 ± 0.14 14.9
97.15 ± 0.11 15.5
97.18 ± 0.11 16.1
97.17 ± 0.12 16.1
97.14 ± 0.11 16.1
97.22 ± 0.11 16.1
97.21 ± 0.11 16.1
97.12 ± 0.10 16.5

*
*
*

*

8

79.79 ± 3.82 31.4

Table 13: Averages of the train accuracies and standard deviations for three algorithms with ρ = 0.01
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

* 79.97 ± 2.23
* 79.18 ± 1.86
* 78.63 ± 1.54
* 78.68 ± 1.55
* 79.05 ± 1.57
* 78.33 ± 1.31
* 78.30 ± 1.42
* 79.05 ± 2.84
* 77.07 ± 1.71
* 77.41 ± 9.06
* 96.93 ± 1.75
* 68.20 ± 4.80
97.60 ± 1.26
95.68 ± 2.26
* 72.32 ± 2.04
86.14 ± 4.28
* 74.18 ± 3.53
* 77.33 ± 1.39
* 72.27 ± 1.99
* 81.06 ± 8.01
* 90.59 ± 0.60
98.72 ± 1.75
98.42 ± 1.84
* 88.55 ± 1.27
60.06 ± 2.92
* 93.52 ± 4.28
* 81.05 ± 11.22
83.30 ± 4.67
99.82 ± 0.40
* 67.92 ± 6.69
* 93.35 ± 3.77
81.33 ± 1.90
* 98.49 ± 0.21
* 98.19 ± 0.25
* 98.17 ± 0.24
* 97.99 ± 0.18
* 97.94 ± 0.20
* 97.73 ± 0.16
* 97.62 ± 0.13
* 97.64 ± 0.10

averages

31 85.69 ± 2.48

L

10.3
11.8
13.9
13.7
14.7
11.5
12.5
8.2
5.9
11.9
15.0
3.4
5.9
5.0
9.6
33.7
19.1
13.8
9.3
12.5
28.9
44.9
3.3
23.5
21.7
14.6
7.5
4.2
52.4
7.1
27.7
22.4
33.8
32.3
35.3
30.6
25.4
25.7
20.2
21.7

18.1

LPBoost
score

77.41 ± 1.23
76.74 ± 0.92
76.39 ± 0.82
75.90 ± 0.82
76.31 ± 0.80
76.73 ± 0.96
76.37 ± 1.09
76.01 ± 0.32
76.09 ± 0.34
75.80 ± 8.55
96.00 ± 1.70
67.90 ± 3.90
94.00 ± 1.63
95.23 ± 2.92
71.01 ± 1.43
* 88.01 ± 3.94
72.50 ± 3.83
76.55 ± 1.38
71.01 ± 1.43
77.87 ± 6.09
90.54 ± 0.56
* 98.86 ± 1.65
92.37 ± 3.15
86.69 ± 1.72
64.33 ± 1.21
82.54 ± 7.77
77.00 ± 12.43
83.03 ± 4.88
* 99.88 ± 0.25
66.70 ± 6.37
93.10 ± 3.71
80.88 ± 1.67
97.47 ± 0.18
97.26 ± 0.21
97.30 ± 0.13
97.33 ± 0.15
97.34 ± 0.11
97.17 ± 0.11
97.21 ± 0.12
97.23 ± 0.10

L

4.5
5.4
4.5
4.6
4.4
4.4
4.3
4.2
3.5
8.6
12.8
3.3
5.0
4.8
5.7
106.5
18.8
11.2
5.7
12.6
33.2
47.0
2.7
26.4
85.7
8.1
3.9
4.2
54.2
11.6
29.0
34.1
5.1
5.4
5.2
5.4
5.4
5.8
5.8
6.2

AdaBoost
score

L

77.31 ±1.00
9.3
76.42 ±0.94
9.6
77.00 ±1.10 10.9
76.84 ±1.07 11.0
77.19 ±0.99 11.0
77.30 ±1.03 11.0
77.11 ±1.09 11.0
76.56 ±0.61 11.0
76.58 ±0.65 10.9
76.03 ±8.71 24.1
95.27 ±1.65 20.2
68.00 ±4.21 55.3
100.00 ±0.00 58.7
100.00 ±0.00 56.7
71.34 ±1.62 19.1
82.23 ±3.26 95.1
73.54 ±3.60 46.2
76.41 ±1.40 38.0
71.34 ±1.62 19.1
77.13 ±5.64 20.1
90.34 ±0.06 71.6
97.62 ±2.07 49.1
100.00 ±0.00 36.9
84.83 ±1.81 37.5
67.28 ±1.79 97.2
77.60 ±6.31
9.7
71.67 ±8.51
8.0
83.33 ±4.64 50.4
99.46 ±0.82 64.6
66.89 ±6.23 21.0
92.92 ±3.71 62.9
81.36 ±2.38 70.7
97.31 ±0.15 14.9
97.13 ±0.10 15.5
97.25 ±0.12 16.1
97.21 ±0.10 16.1
97.28 ±0.10 16.1
97.11 ±0.13 16.1
97.16 ±0.11 16.1
97.19 ±0.11 16.5

*
*

*

*

*

*

3

84.20 ± 2.26

15.5

6

84.36 ±1.99 31.4

25

Table 14: Averages of the test accuracies and standard deviations for three algorithms with ρ = 0.01
for 10 diﬀerent seeds on LIBSVM; best solutions are marked with *; using class probabilities for
prediction and voting.

name

a1a
a2a
a3a
a4a
a5a
a6a
a7a
a8a
a9a
australian_scale
breast-cancer_scale
cod-rna
colon-cancer
duke
german.numer
gisette_scale
diabetes_scale
fourclass_scale
german.numer_scale
heart_scale
ijcnn1
ionosphere_scale
leu
liver-disorders
madelon
mushrooms
phishing
skin_nonskin
sonar_scale
splice
svmguide1
svmguide3
w1a
w2a
w3a
w4a
w5a
w6a
w7a
w8a

IPBoost
score

78.47 ± 3.33
77.75 ± 1.97
78.02 ± 1.50
77.63 ± 2.54
77.96 ± 1.64
78.72 ± 1.32
78.40 ± 1.55
78.93 ± 2.79
77.06 ± 2.04
73.84 ± 9.59
93.38 ± 2.48
68.19 ± 4.87
73.33 ± 14.05
95.53 ± 2.17
71.90 ± 3.43
84.72 ± 4.34
65.49 ± 4.03
71.10 ± 4.44
72.00 ± 3.32
73.15 ± 5.03
90.46 ± 0.83
83.43 ± 5.05
70.59 ± 7.59
56.30 ± 3.17
53.18 ± 2.92
93.69 ± 4.27
80.56 ± 11.70
83.28 ± 4.74
65.85 ± 9.55
65.13 ± 7.19
90.20 ± 4.59
65.37 ± 16.97
97.09 ± 0.25
97.41 ± 0.17
97.54 ± 0.25
97.55 ± 0.17
97.55 ± 0.20
97.70 ± 0.17
97.59 ± 0.11
97.47 ± 0.11

*
*
*
*
*
*
*
*
*
*
*
*

*

*
*
*

*

*
*

*
*
*
*
*
*
*

averages

26 80.59 ± 3.91

L

10.3
11.8
13.9
13.7
14.7
11.5
12.5
8.2
5.9
11.9
15.0
3.4
5.9
5.0
9.6
33.7
19.1
13.8
9.3
12.5
28.9
44.9
3.3
23.5
21.7
14.6
7.5
4.2
52.4
7.1
27.7
22.4
33.8
32.3
35.3
30.6
25.4
25.7
20.2
21.7

18.1

LPBoost
score

76.73 ± 2.94
76.03 ± 2.36
75.59 ± 1.56
75.23 ± 1.75
75.59 ± 1.86
77.42 ± 1.04
76.52 ± 1.10
75.97 ± 0.49
76.24 ± 0.58
72.90 ± 9.29
93.16 ± 1.43
67.82 ± 3.63
73.33 ± 10.97
94.74 ± 3.28
71.85 ± 2.97
* 86.27 ± 3.89
65.49 ± 4.51
* 72.73 ± 3.72
71.85 ± 2.97
68.89 ± 5.00
90.44 ± 0.78
83.14 ± 6.05
73.24 ± 8.48
55.25 ± 2.62
* 53.42 ± 3.73
82.61 ± 7.74
76.88 ± 12.50
83.05 ± 4.94
* 66.83 ± 6.82
* 65.46 ± 7.41
89.98 ± 4.56
* 69.76 ± 14.82
* 97.14 ± 0.14
97.24 ± 0.10
97.19 ± 0.12
97.22 ± 0.12
97.16 ± 0.13
97.24 ± 0.10
97.23 ± 0.12
97.15 ± 0.10

L

4.5
5.4
4.5
4.6
4.4
4.4
4.3
4.2
3.5
8.6
12.8
3.3
5.0
4.8
5.7
106.5
18.8
11.2
5.7
12.6
33.2
47.0
2.7
26.4
85.7
8.1
3.9
4.2
54.2
11.6
29.0
34.1
5.1
5.4
5.2
5.4
5.4
5.8
5.8
6.2

AdaBoost
score

L

*

76.04 ± 3.30
9.3
75.67 ± 1.57
9.6
76.84 ± 1.03 10.9
76.18 ± 1.72 11.0
76.72 ± 1.77 11.0
77.84 ± 1.31 11.0
77.28 ± 1.39 11.0
76.46 ± 0.64 11.0
76.54 ± 0.82 10.9
72.61 ± 9.54 24.1
93.01 ± 2.48 20.2
67.94 ± 4.03 55.3
76.67 ±14.59
*
58.7
* 100.00 ± 0.00 56.7
71.80 ± 3.32 19.1
80.40 ± 3.62 95.1
66.80 ± 5.07 46.2
72.09 ± 4.02 38.0
71.80 ± 3.32 19.1
69.81 ± 5.09 20.1
90.30 ± 0.28 71.6
86.71 ± 4.86 49.1
80.88 ± 7.50 36.9
55.90 ± 3.02 37.5
52.12 ± 1.82 97.2
77.70 ± 6.29
9.7
71.27 ± 8.52
8.0
83.35 ± 4.71 50.4
63.90 ±11.14
64.6
65.19 ± 6.65 21.0
90.40 ± 5.01 62.9
63.90 ±23.36
70.7
97.11 ± 0.14 14.9
97.15 ± 0.11 15.5
97.18 ± 0.11 16.1
97.17 ± 0.12 16.1
97.14 ± 0.11 16.1
97.22 ± 0.11 16.1
97.21 ± 0.11 16.1
97.12 ± 0.10 16.5

*
*

*

*

7

79.80 ± 3.67

15.5

7

79.79 ± 3.82 31.4

26

B Additional Computational tests

B.1 Generalization of IPBoost: train vs. test error

Figure 2: Train vs. test performance for diﬀerent margins ρ on instance australian. Each point
represents a solution encountered by IPBoost while solving the boosting problem. Grayscale values
indicate the number of base learners used in boosted learners; see the legend.

Figure 3: Train vs. test performance for diﬀerent margins ρ on instance mushrooms. Each point
represents a solution encountered by IPBoost while solving the boosting problem. Grayscale values
indicate the number of base learners used in boosted learners; see the legend.

27

68.070.072.074.076.068.070.072.074.076.0trainaccuracy(%)testaccuracy(%)australian–ρ=0.01246868.070.072.074.076.068.070.072.074.076.0trainaccuracy(%)testaccuracy(%)australian–ρ=0.0545678968.070.072.074.076.068.070.072.074.076.0trainaccuracy(%)testaccuracy(%)australian–ρ=0.155.25.45.65.8680.082.084.086.088.080.082.084.086.088.0trainaccuracy(%)testaccuracy(%)mushrooms–ρ=0.01101214161880.082.084.086.088.080.082.084.086.088.0trainaccuracy(%)testaccuracy(%)mushrooms–ρ=0.05789101180.082.084.086.088.080.082.084.086.088.0trainaccuracy(%)testaccuracy(%)mushrooms–ρ=0.1345678