1
2
0
2

t
c
O
6

]
I

A
.
s
c
[

2
v
4
3
8
3
0
.
8
0
1
2
:
v
i
X
r
a

Bob and Alice Go to a Bar
Reasoning About Future With Probabilistic Programs

David Tolpin

Tomer Dobkin

Ben-Gurion University of the Negev, Israel

Abstract

It is well known that reinforcement learning can be cast as inference in an appropriate
probabilistic model. However, this commonly involves introducing a distribution
over agent trajectories with probabilities proportional to exponentiated rewards.
In this work, we formulate reinforcement learning as Bayesian inference without
resorting to rewards, and show that rewards are derived from agent’s preferences,
rather than the other way around. We argue that agent preferences should be
speciﬁed stochastically rather than deterministically. Reinforcement learning via
inference with stochastic preferences naturally describes agent behaviors, does not
require introducing rewards and exponential weighing of trajectories, and allows to
reason about agents using the solid foundation of Bayesian statistics. Stochastic
conditioning, a probabilistic programming paradigm for conditioning models on
distributions rather than values, is the formalism behind agents with probabilistic
preferences. We demonstrate realization of our approach on case studies using
both a two-agent coordinate game and a single agent acting in a noisy environment,
showing that despite superﬁcial diﬀerences, both cases can be modelled and reasoned
about based on the same principles.

1

Introduction

The ‘planning as inference’ paradigm [WGR+11, vdMPTW16] extends Bayesian
inference to future observations. The agent in the environment is modelled
as a Bayesian generative model, but the belief about the distribution of
agent’s actions is updated based on future goals rather than on past facts.
This allows to use common modelling and inference tools, notably probabilis-
tic programming, to represent computer agents and explore their behavior.
Representing agents as general programs provides ﬂexibility compared to
restricted approaches, such as Markov decision processes and their variants

1

 
 
 
 
 
 
2 The Fable: Bob and Alice Go to a Bar

2

and extensions, and allows to model a broad range of complex behaviors in
a uniﬁed and natural way.

Planning, or, more generally, reinforcement learning, as inference models
agent preferences through conditioning agents on preferred future behaviors.
Often, the conditioning is achieved through the Boltzmann distribution: the
probability of a realization of agent’s behavior is proportional to the exponent
of the agent’s expected reward. The motivation of using the Boltzmann
distribution is not clear though. A ‘rational’ agent should behave in a way
that maximizes the agent’s expected utility, shouldn’t it? One argument
is that the Boltzmann distribution models human errors and irrationality.
Sometimes, attempts are made to avoid using Boltzmann distribution by
explicitly conditioning the agent on future goals. However, such conditioning
can also lead to irrational behavior, as the famous Newcomb’s paradox [Noz69]
suggests.

2 The Fable: Bob and Alice Go to a Bar

Challenges of reinforcement learning as inference can be illustrated on the
following fable [SG14]:

Bob and Alice want to meet in a bar, but Bob left his phone
at home. There are two bars, which Bob and Alice visit with
diﬀerent frequencies. Which bar Bob and Alice should head if
they want to meet?

Many diﬀerent settings can be considered based on this story. Bob and
Alice may know each other’s preferences with respect to the bars and to the
meeting with the other person, be uncertain about the preferences, or hold
wrong beliefs about the preferences. Their preferences may be collaborative
(both want to meet) or adversarial (Bob wants to meet Alice, but Alice avoids
Bob). Bob may consider Alice’s deliberation about Bob’s behavior, and vice
versa, recursively. It turns out that all these scenarios can be represented by
a single model of interacting agents. However, doing this properly, from the
viewpoint of both speciﬁcation and inference, requires certain care.

3 Model Blueprint

Details of conditioning and inference set aside, the overall structure of the
model is more or less obvious. There are two thought and action models,

3 Model Blueprint

3

for each of the agents. The generative models have the same structure, but
apparently diﬀerent parameters (Model 1).

Model 1 Agent model: choosing an action based on the belief about prefer-
ences of the other agent
1: procedure Agent(θ, τ (cid:48))
2:

(cid:46) Draw other agent’s preferences
(cid:46) Draw the agent’s action
(cid:46) Draw the action’s success

3:

θ(cid:48) ∼ Dθ(cid:48)(τ (cid:48))
a ∼ Da(θ)
Is ∼ Ds(a, θ(cid:48))
4:
5: end procedure

The parameters reﬂect the agent’s preferences θ, as well as beliefs of the
agent about the other agent’s preferences τ (cid:48). The model ﬁrst draws the other
agent’s preferences θ(cid:48) and the agent’s action a. Then, the model draws the
action’s success from a distribution parameterized by a and θ(cid:48). The success
distribution Ds is intentionally kept vague here, and is the subject of the
rest of the article.

Interaction between Bob and Alice is simulated by running inference in
each of the models and then by performing an action following (determin-
istically or stochastically) from inference results. In an episodic game, the
model is conditioned on θ(cid:48) and Is for each agent, and the distributions of
agents’ actions are inferred. Then the move is simulated by drawing a sample
from each of the posterior action distributions (Algorithm 1).

Algorithm 1 Simulation of an episode
1: DAlice ← Infer(Agent(θAlice, τBob) | θ(cid:48) = θBob, Is)
2: DBob ← Infer(Agent(θBob, τAlice) | θ(cid:48) = θAlice, Is)
3: aAlice ∼ DAlice
4: aBob ∼ DBob

Since in a single episode there is no earlier evidence about the other

agent’s preferences, θ(cid:48) is taken to be known, and τ (cid:48) has no eﬀect.

Bob and Alice can even engage in a multiround game, in which each agent
updates his or her own beliefs about the other agent’s preferences based on
the observed actions. To update an agent’s belief about the other agent’s
preferences, the model is conditioned on an observed action a and success Is,
and θ(cid:48) is inferred, and is later used to choose an action in the next round.

A few ways to specify the agents and perform inference were proposed.
We believe that some of them are wrong, and others can be streamlined. In

4 Background

4

what follows, we propose a purely Bayesian generative approach to reasoning
about future in multi-agent environments.

4 Background

4.1 Planning as Inference

Planning, as a discipline of artiﬁcial intelligence, considers agents acting in
environments [PM17]. The agents have beliefs about their environments
and perform actions which bring them rewards (or regrets). AI planning
is concerned with algorithms that search for policies — mappings from
In planning-as-
agents’ beliefs about the environment to their actions.
inference approach [TS06, BA09], policy search is expressed as inference
in an appropriate probabilistic model. The prevailing approach to casting
planning as inference is inspired by the stochastic control theory [Kap07]
and is based on the use of Boltzmann distribution, which ascribes to a
policy the probability proportional to the exponent of the expected total
reward [WGR+11, vdMPTW16].

A probabilistic program reifying the planning-as-inference approach en-
codes instantiation of policy πθ(E), depending on latent parameters θ, in
environment E. In the course of execution, the program computes the re-
ward r ∼ πθ(E), stochastic in general. The posterior distribution of policy
parameters p(θ|E), conditioned on the environment, is deﬁned in terms of
their prior distribution p(θ) and of the expected reward:

p(θ|E) ∝ p(θ) exp (E [r ∼ πθ(E)])

(1)

Posterior inference on (1) gives a distribution of policy parameters, with
the mode of the distribution corresponding to the policy maximizing the
expected reward.

4.2 Stochastic Conditioning

Stochastic conditioning [TZRY21] extends deterministic conditioning p(x|y =
y0), i.e. conditioning on some random variable in our program y taking on a
particular value y0, to conditioning p(x|y ∼ D0) on y having the marginal
distribution D0. A probabilistic model with stochastic conditioning is a tuple
(p(x, y), D) where

• p(x, y) is the joint probability density of random variable x and obser-

vation y,

4 Background

5

• D is the distribution from which observation y is marginally sampled,

and it has a density q(y).

Unlike in the usual setting, the objective is to infer p(x|y ∼ D), the
distribution of x given distribution D, rather than an individual observation
y. To accomplish this objective, one must be able to compute p(x, y ∼ D), a
possibly unnormalized density on x and distribution D. As usual, p(x, y ∼ D)
is factored as p(x)p(y ∼ D|x) where p(y ∼ D|x) is the following unnormalized
conditional density:

p(y ∼ D|x) = exp

(cid:18)(cid:90)

Y

(log p(y|x)) q(y)dy

(2)

(cid:19)

An intuition behind the deﬁnition can be seen by rewriting (2) as a type

II geometric integral:

p(y ∼ D|x) =

(cid:89)
Y

p(y|x)q(y)dy.

Hence, (2) can be interpreted as the probability of observing all possible
draws of y from D, each occurring according to its probability q(y)dy.

By convention, in statistical notation y ∼ D is placed above a rule to
denote that distribution D is observed through y and is otherwise unknown
to the model, as in (3).

y ∼ D

x ∼ Prior

y|x ∼ Conditional(x)

(3)

4.3 Epistemic Reasoning (Theory of Mind)

Epistemic reasoning [SG14, KG15, HFWL17], also known as theory of mind,
is mutual reasoning of multiple agents about each others’ beliefs and inten-
tions. Epistemic reasoning comes up in the planning-as-inference paradigm
when each agent’s policy is mutually conditioned on other agents’ policies.
Probabilistic programming allows natural representation of epistemic rea-
soning: the program modelling an agent invokes inference on the programs
modelling the rest of the agents. However, this means that inference in multi-
agent settings requires nested conditioning [Rai18], which is computationally
challenging in general, although attempts are being made to ﬁnd eﬃcient
implementations of nested conditioning in certain settings [TZRY21]. Since
each agent’s model recursively refers to models of other agents, the recursion
can unroll indeﬁnitely. In a basic approach [SG14, ESSF17], the recursion
depth is bounded by a constant.

5 Common Mistakes

6

5 Common Mistakes

Statistical models are most suited for reasoning about the present. Reasoning
about the past or, in particular, the future is counterintuitive and hard to get
right (and should probably be avoided when possible [Tol99]). Multi-agent
planning as inference, exempliﬁed by the fable about Bob and Alice, presents
logical and statistical traps which are easy to fall into, often without even
noticing. One common mistake is conditioning on a future event as on a
present observation, which is related to the Newcomb’s paradox; however
there are also other mistakes. Incorrect treatments of the problem in the
literature, presented in the rest of this section, accentuate the need for
a probabilistically sound Bayesian approach to planning as inference, the
subject of this work.

5.1

Inconsistent Preferences

One seemingly natural way to account for agent’s preferences is to condition
the agent’s choices on the anticipated event [SG14, ESSF17]. Consider, for
simplicity, one particular variant of the fable, in which Bob and Alice both
generally choose the ﬁrst bar with probability 55% and the second bar with
probability 45% and want to meet [ESSF17, Chapter 6]. The agent model (of
Alice, just to be concrete) combines the prior on Alice’s behavior, which is the
Bernoulli distribution I1
Alice = 0.55), and the conditioning
on Alice meeting Bob. Alice does not know Bob’s location, but can reason
about the distribution of Bob’s choices Bernoulli(ˆp1
Bob coincides with
Bob’s prior p1
Bob = 0.55 in the simplest case, but can be also inﬂuenced
by Alice’s consideration of Bob’s reasoning about Alice, which is a case of
epistemic reasoning (Section 4.3).

Alice ∼ Bernoulli(p1

Bob). ˆp1

Model 2 Conditioning on the future as though it were the present.

I1
Alice ∼ Bernoulli(p1
if I1

Alice then
1 ∼ Bernoulli(ˆp1

Alice)

Bob)

else

0 ∼ Bernoulli(ˆp1

Bob)

end if

The posterior probability of Alice going to the ﬁrst bar according to
Bob > 0.5, that is, if Alice maintains that

0.55ˆp1

Bob

Model 2 is

0.55ˆp1

Bob+0.45(1−ˆp1

Bob) . If ˆp1

5 Common Mistakes

7

Bob prefers the ﬁrst bar, Alice will go to the ﬁrst bar more often if she wants
to meet Bob, with the probability in which it would meet Bob in the ﬁrst
bar if she would not adjust her behavior. It may seem (and it is indeed the
course of reasoning in [SG14] and [ESSF17]) that Alice makes a better choice
thinking about Bob, and the more she thinks about Bob (willing to meet
Alice and taking into consideration that Alice is willing to meet Bob, and so
on) the higher is the probability of Alice to choose the ﬁrst bar.

However, on a slightly more thorough consideration, Alice’s deliberation
is irrational. If Alice wants to meet Bob and knows that Bob chooses the
ﬁrst bar more often, no matter to which extent, she must always choose the
ﬁrst bar! Moreover, the model’s recommendation to choose the ﬁrst bar with
the probability that she meets Bob in the ﬁrst bar if she does not choose the
ﬁrst bar more often than usual, sounds at least surprising and suggests that
the model is wrong.

Indeed, the above model suﬀers from two problems. First, it conditions on
the future as though it were the present, that is as though Alice knew, in each
case, Bob’s choices. Second, the agents’ preferences with respect to the choice
of a bar and to meeting each other are expressed using diﬀerent languages.
Alice chooses the ﬁrst or the second bar with a non-trivial probability, but
wants to meet Bob non-probabilistically. Moreover, it is not immediately
obvious, whether and how a probability can be ascribed to a desire (rather
than a future event). Mixing two incompatible formulations causes confusion
and gives self-contradictory results.

5.2 Avoiding Nested Conditioning

[SvdMW18] approach a pursuit-evasion problem, in the form of a chaser
(sheriﬀ) and a runner (thief) moving through city streets, with tools of
probabilistic programming and planning as inference. A simpler but similar
setting can be expressed using Bob and Alice with adversarial preferences
with respect to meeting each other: Bob (the chaser) wants to meet Alice,
but Alice (the runner) wants to avoid Bob.

[SvdMW18] formulates the inference problem in terms of the Boltzmann
distribution of trajectories based on the travel time of the runner, as well as
the time the runner seen by the chaser, positive for the chaser (wants to see
the runner as long as possible) and negative for the runner (wants to avoid
being seen by the chaser as much as possible). However, looking for eﬃcient
inference, this work proposes to replace nested conditioning by conditioning
of each level of reasoning on a single sample from the previous level. This
is, again, conditioning on the future as though the future were known, and

6 Deterministic Preferences

8

leads to wrong results.

The problem can be hard to realize on the complicated setting explored
in the work, but becomes obvious on the example of Bob and Alice. Consider,
for simplicity, the setting in which Alice has equal preferences regarding the
bars and attributes reward 1 to avoiding Bob and 0 to meeting him. Bob
chooses the ﬁrst bar in 55% of cases; Alice employs a single level of epistemic
reasoning. It is easy to see that the optimal policy for Alice is to always go
to the second bar for the expected reward or 0.55. However, the mode of the
posterior inferred using the algorithm in [SvdMW18] is for Alice to choose
the second bar with probability of 0.55, for the expected reward of 1! This is
because Alice’s decision is (erroneously) conditioned on Bob’s anticipated
choice of a bar, and hence Alice pretends that she can always choose the
other bar (which she cannot).

6 Deterministic Preferences

The fable of Bob and Alice is underspeciﬁed: we know how strong Bob or
Alice prefer the ﬁrst bar over the second one, or vice versa, however we do
not know how strong Bob and Alice prefer to meet (or avoid) each other. We
need a consistent language for expressing both preferences. One common way
of expressing preferences is by ascribing rewards to action outcomes [ESSF17].
In this way, the agent’s preferences are speciﬁed using three quantities:

1. r1 — the reward for visiting the ﬁrst bar;

2. r2 — the reward for visiting the second bar;

3. rm — the reward for meeting the other person.

However, the preference of visiting the ﬁrst bar in 55% of cases (or any other
percentage diﬀerent from 100% and 0%) cannot be expressed using rewards
with a rational agent, that is with an agent maximizing his or her expected
utility. If Alice prefers the ﬁrst bar (r1
Alice) and is rational, she should
always go to the ﬁrst bar. If, in addition, Alice ascribes reward rm
Alice to
meeting Bob, and believes that Bob goes to the ﬁrst bar with probability ˆp1
Bob,
then she should go to the ﬁrst bar if r1
Bob)rm
Alice,
and to the second bar otherwise, ignoring ties. It is a well-known and easy
to prove fact that rational agents choose actions deterministically.

Alice+(1−ˆp1

Alice > r2

Alice > r2

Alice+ˆp1

Bobrm

However, stochastic behavior is not unusual, and we should be able to
model it. To reconcile rewards and stochastic action choice, the softmax
agent was proposed and is broadly used for modelling ‘approximately optimal

7 Probabilistic Preferences

9

agents’ [SG14]1. A softmax agent chooses actions stochastically, with prob-
abilities proportional to the exponentiated action utilities. If Bob or Alice
choose a bar without caring about meeting the other party, the action utility
is the reward for visiting a bar, so to model e.g. Alice choosing the ﬁrst
bar over the second one with probability p1
Alice, we should ascribe rewards
r1
Alice an r2
. Concretely, for
p1
Alice = 0.55 we obtain r1 ≈ r2 + 0.2; only the diﬀerence between rewards
matters in a softmax agent, rather than their absolute values.

Alice such that r1

Alice = r2

Alice + log

(cid:16) p1
1−p1

Alice

Alice

(cid:17)

If, however, Alice’s willingness to meet (or avoid) Bob is considered, the
utilities of Alice’s actions are updated based on Alice’s belief that Bob goes
to the ﬁrst bar:

u1
Alice = r1
Alice = r2
u2

Alice + ˆp1
Bobrm
Alice + (1 − ˆp1

Alice
Bob)rm

Alice

and Alice chooses the ﬁrst bar with probability

Pr(I1

Alice) =

Alice)

exp(u1
Alice) + exp(u2

Alice)

exp(u1

(4)

(5)

Alice’s behavior is stochastic, and depends on Alice’s belief ˆp1
similarly stochastic, behavior.

Bob about Bob’s,

7 Probabilistic Preferences

Section 6 shows how stochastic behavior can be modelled using a softmax
agent exhibiting an ‘approximately optimal’ behavior, that is, selecting ac-
tions with log-probabilities proportional to their expected utilities. However,
one may wonder what would be the optimal behavior if the behavior we
model with softmax is only ‘approximately optimal’ and hence suboptimal.
We believe that treating stochastic behavior as suboptimal because it does
not maximize the utility based on rewards we ascribe is contradictory. When
Alice chooses the ﬁrst bar 55% of time and the second bar 45% of time it
is not because she cannot choose better (that is, always the ﬁrst bar). It is
her choice to sometimes go for a drink and have a lot of people around and
a loud music, sometimes go for a drink and have fewer people around and
read a book without disturbing music in the background. Bob, in addition
to going to one of the two bars to enjoy some drinks and see Alice, most
probably eats breakfast every morning. Let us speculate that sometimes Bob

1 We delay discussion of the meaning of ‘approximate optimality’ and stochastic prefer-

ences till the next section.

7 Probabilistic Preferences

10

prefers scrambled eggs and sometimes porridge, randomly with probability
about 0.6 towards scrambled eggs. However, if Bob had to eat scrambled
eggs every morning and to give up on porridge entirely, because this is our
perception of the optimal behavior based on rewards we ourselves introduced,
rather arbitrarily, he would most probably be less happy about his diet.

Assuming that there is no shortage of either oat or eggs, and that Bob’s
income essentially frees him from ﬁnancial considerations in choosing one
meal over the other, we should perceive Bob’s behavior with respect to
either the breakfast menu or the choice of a bar as optimal, and suggest
means for specifying optimal stochastic behaviors and reasoning about them.
One option is to postulate that softmax action selection is indeed optimal.
However, this option involves an assumption that softmax is a law of nature,
and that there are latent rewards which we can only observe as choice
probabilities through the softmax distribution. We believe that a separate
theory of softmax-optimal stochastic behavior is unnecessary, and one can
reason about agent behavior, both in single-agent and multi-agent setting,
using Bayesian probabilities only. The rest of this section is an elaboration
of this attitude.

Let us return to the fable. According to the fable, the optimal probability
of Alice’s choice with respect to the bars is given (observed with high
conﬁdence or proclaimed by an oracle). To complete the formulation, it
remains to specify the optimal probability of Alice to meet Bob. The question
is: probability of what event reﬂects the preference? It turns out that we
need to specify the probability of Alice choosing to visit the bar where Bob is
heading, provided Alice knows Bob’s plans and would otherwise visit either
bar with equal probability. Indeed, this results in the following generative
model:

Model 3 Generative model with probabilistic preferences

Alice)

I1
Alice ∼ Bernoulli(p1
if I1
Alice then
I1
Bob ∼ Bernoulli(pm
else
I1
Bob ∼ Bernoulli(1 − pm
end if

Alice)

Alice)

The model, in a superﬁcially confusing way, generates Bob’s location I1
Bob
based on Alice’s desire to meet Bob. However, the confusion can be resolved
by the following interpretation: when Alice chooses to go to the ﬁrst bar, it
is because, in addition to the slight preference over the second bar (line 1),

7 Probabilistic Preferences

11

she believes that Bob will also go to the ﬁrst bar with probability pm
Alice. It
is Alice’s anticipations of Bob’s behavior that are generated by the model,
rather than events.

Since Model 3 generates anticipations, it should also be conditioned on
anticipations. Let us, at this point, accept as a fact that Alice concludes that
Bob goes to the ﬁrst bar with probability ˆp1
Bob. Then, the model must be
conditioned on I1
Bob). This involves
an application of stochastic conditioning (Section 4.2):

Bob being distributed as Bernoulli(ˆp1

Model 4 Stochastically conditioned Model 3

I1
Bob ∼ Bernoulli(ˆp1

Bob)

Alice)

I1
Alice ∼ Bernoulli(p1
if I1
Alice then
I1
Bob ∼ Bernoulli(pm
else
I1
Bob ∼ Bernoulli(1 − pm
end if

Alice)

Alice)

Alice] =

A connection between probabilistic preferences and deterministic prefer-
ences along with the Boltzmann distribution is revealed by writing down the
analytical form of the probability distribution speciﬁed by Model 4:
log Pr[I1
(cid:40)

if I1
Alice
otherwise
(6)
That is, according to Model 4, Alice’s actions are Boltzmann-distributed
with rewards

Alice)
Bob) log pm

log p1
log(1 − p1

Bob) log(1 − pm

Alice) + (1 − ˆp1

Bob log(1 − pm

Alice + (1 − ˆp1

Alice) + ˆp1

Bob log pm

Alice + ˆp1

Alice

Alice − r2
r1

Alice = log

rm
Alice = log

p1
Alice
1 − p1
pm
Alice
1 − pm

Alice

Alice

(7)

A direct corollary from (6) and (7) is that planning as inference with prob-
abilistic preferences can be encoded as a Bayesian generative model using
probabilities only, without resort to rewards. However, a more general
conjecture can be made:

Conjecture 1. Preferences are naturally probabilistic in general and should
be expressed as probabilities of choices under clairvoyance. ‘Deterministic’

8 Reinforcement Learning as Inference

12

speciﬁcation of preferences through rewards is an indirect way to encode
probabilistic preferences. Reward maximization algorithms for policy search
are in fact maximum a posteriori approximations of the distributions of
optimal behaviors, which are inherently stochastic.

Conjecture 1 may raise a question of the role of reward maximization in
cases where rewards are given, such as games or trade. However, rewards
in games or trade are themselves invented by humans because of diﬃculty
apprehending probabilistic preferences. Once the essence of probabilistic
preferences is understood, settings in which reward maximization is the
optimal strategy lose their importance as general models and become mere
edge cases.

8 Reinforcement Learning as Inference

Let us map certain well-known research problems of reinforcement learning to
Bayesian inference. Some of the problems are perceived as having paradoxical
properties, however their paradoxicality is resolved by reformulation in terms
of probabilities, without recourse to artiﬁcially introduced rewards and
utilities.

In the rest of the section, we provide an alternative, purely probabilistic,
deﬁnition of the model known as Markov decision process (including the
‘partially observable’ variant). Then, we formalize the tasks of planning and
apprenticeship learning. There is an ambiguity in reinforcement learning
literature with regard to notions of reinforcement learning, inverse reinforce-
ment learning, apprenticeship learning, and planning. In this work, we chose
to use the term planning to denote inferring the (optimal or rational) agent
behavior given the model, and apprenticeship learning to denote inferring
the model parameters given observed agent behavior. Note that the notion
of ’optimal’ behavior is diﬀerent here from the behavior considered optimal
in traditional reinforcement learning literature: rather than corresponding
to the mode of the posterior distribution of behaviors speciﬁed by the model,
it is the posterior distribution itself.

8.1 Markov Decision Process

Customarily, a Markov decision process (MDP) [Sze10] is deﬁned by tuple
(S, A, T, γ, D, R), where S is a set of states, A is a set of actions, T is a state
transition distribution, γ ∈ [0, 1) is a discount factor, and R is the reward
function. A policy π can be imposed on a Markov decision process, and

8 Reinforcement Learning as Inference

13

constitutes a mapping from states to actions. Realization of π produces
samples of agent behaviors, or trajectories, through the state-and-action space
of the MDP. A choice of policy depends on rewards R, an ‘optimal’ policy is
usually taken to maximize the expected discounted (by γ) reward. On the
other hand, the rewards may be unknown, and inferred from observations of
the agent behavior [NR00, AN04, HGEJ17].

The above deﬁnition of MDP provides a fertile ground for reinforcement
learning research, however it mixes the agent, the environment, the goal of
the agent in the environment, and the issues of practical computability, all in
a single deﬁnition. In particular, the space of states S and actions A describe
the agent (where it is and what it can do), the transition probabilities T
is a property of the environment, and the reward function R deﬁnes the
goal of the agent in the environment. The discount factor γ modiﬁes the
environment in such a way that the length of the episode trajectory is
geometrically distributed, through implicitly modifying S and T such that S
includes a terminal state and the transition matrix includes transition from
any state to the terminal state with probability 1−γ — and that ensures that
the policy value is bounded. The geometrical distribution is not the only and
not necessarily the most justiﬁed choice for trajectory length distribution.
The Poisson or the negative-binomial distribution may, for example, be used
instead when appropriate (for example, the negative-binomial distribution is
a natural model for an agent repeatedly performing an action, with a certain
number of failed attempts allowed). However, conventional MDP does not
provide for such ﬂexibility.

Bayesian modelling allows instead to deﬁne a Markov decision process such
that the agent, the environment, the agent preferences, and the computability
issues are explicit and clearly separated. Before we proceed to our deﬁnition,
let us draw connection between the fable of Bob and Alice and a single-agent
decision process. It is relatively easy to see how the case of two agents can
be extended to a greater number of agents. However, a two-agent setup can
as well be used naturally for modelling a single agent acting in a stochastic
environment — by representing the stochasticity of the environment as the
other, neutral (neither adversarial nor collaborative) agent, acting regardless
of the state of the ﬁrst agent. We will stick to this paradigm here and deﬁne a
2-agent Markov decision process, with a single-agent Markov decision process
as a special case of interaction of two agents in an environment with the
other agent being neutral.

Deﬁnition 1. A 2-agent Markov decision process (MDP) is a tuple
(S, A, A1, A2, ◦, t, m1, m2) where

8 Reinforcement Learning as Inference

14

• S is the set of states;

• A is the set of actions of the MDP;

• A1,A2 are the sets of actions of each agent;

• ◦ : A1 × A2 → A is the action composition operator;

• t : S × A → S is the transition function of the MDP;

• m1, m2 are the agent models.

An MDP invokes stochastic agents represented by their models m1 and m2
(Algorithm 2). Action a applied to the state is a composition a1 ◦a2 of actions

Algorithm 2 2-agent Markov decision process

1: s ← s0
2: while s (cid:54)= ⊥ do
3:

a1 ∼ m1(s)
a2 ∼ m2(s)
a ← a1 ◦ a2
s ← t(s, a)

6:
7: end while

4:

5:

(cid:46) m2(s, a1) in a sequential setting

a1 and a2 chosen by each agent. How exactly the composition is accomplished
is speciﬁc to a particular MDP instance. An MDP episode terminates when
the terminal state ⊥ is reached. Conditions for the ﬁniteness of the episode
trajectory in expectation are researched and known in the probabilistic
programming literature [vdMPYW18], where the program trace must be
ﬁnite in expectation for a probabilistic program to specify a distribution. Let
us deﬁne the notion of ‘episode trajectory’ though:

Deﬁnition 2. An episode trajectory of a given MDP is a tuple (s0, aaa) where
s0 is the initial state and aaa is the sequence of MDP actions taken until the
terminal state is reached.

An episode trajectory fully deﬁnes an episode — given the initial state
and the actions, the intermediate states are deterministic, according to
Algorithm 2.

While Algorithm 2 encodes actual unrolling of an MDP, an agent model
represents reasoning behind action selection by the agent. Both agent models
have the same structure, and deﬁne each agent’s preferences and beliefs
about the other agent.

8 Reinforcement Learning as Inference

Deﬁnition 3. A 2-agent MDP agent is a model



ab ∼ (cid:99)Db(s, aa)



aa ∼ ma(s) =












aa ∼ Da(s)
a ← aa ◦ ab
s(cid:48) ← t(s, a)
s(cid:48) ∼ Ds(s)
s(cid:48) (cid:54)= ⊥ ∧ ma(s(cid:48))












15

(8)

where

• a, b is either 1, 2 for m1 or 2, 1 for m2;

• s ∈ S is the current state;

• Da : S → Aa is the prior distribution of the agent’s actions in state s;

• (cid:99)Db : S × Aa ∪ {⊥} → Ab is the agent’s belief about the distribution of
the other agent’s actions in state s given that action aa is taken;

• Ds : S → S is the distribution of states to which the agent desires to

pass from state s.

The rule between the ﬁrst line and the rest of the model means that the
model is stochastically conditioned on the distribution of ab (see Section 4.2).
The last line of the model states that unless s(cid:48) is the terminal state, the
model is recursively conditioned in state s(cid:48).

Note that (cid:99)Db depends on both s and aa in Deﬁnition 3. This addresses
a sequential setting, in which agent b chooses an action after observing the
action of agent a. In a simultaneous setting, neither agent knows the other
agent’s choice, and the choice of an action depends on the current state only
(formally, ⊥ is passed as the second argument of (cid:99)Db).

In a partially observable MDP (POMDP), the states are not known to the
agents, but can be observed with uncertainty. From the Bayesian modelling
point of view though, this does not aﬀect the agent model; the only diﬀerence
between MDP and POMDP here is that in POMDP the state is a latent,
rather than an observable, variable. A proper name for POMDP from the
Bayesian perspective would be latent variable MDP.

8 Reinforcement Learning as Inference

16

8.2 Planning

Conventional planning consists in ﬁnding a policy that maximizes the ex-
pected total (discounted) reward. In the probabilistic formulation of Sec-
tion 8.1, a policy of an agent that maximizes the expected total reward
would correspond to marginal, with respect to the other agent, maximum
a posteriori (MMAP) trajectories. Obviously, MMAP trajectories are just
an approximation of the behavior speciﬁed by the agent, reasonably good if
the distribution is sharply peaked around the MMAP, but can be arbitrarily
bad in general, as the eggs or porridge for breakfast dilemma (Section 7)
demonstrates.

The rational behavior of the agent is speciﬁed by the agent model (9),
and is in general stochastic (a distribution of actions in each state). From
the Bayesian probabilistic standpoint, planning is just instantiation of the
distribution speciﬁed by the agent model. Common inference algorithms,
such as Markov chain Monte Carlo methods or variational inference, can be
used for representing or approximating the policy — the posterior distribution
of agent’s actions.

In the case of a neutral second agent, such that representing the environ-
ment noise, posterior inference is straightforward. However, if the agents are
either collaborative or adversarial, epistemic reasoning must be taken into
account, resulting in potentially unbounded nesting of inference.

8.3 Apprenticeship Learning

Apprenticeship learning [AN04] is concerned with a situation in which the
agent preferences are not given explicitly, however there are observed MDP
trajectories, from which the preferences can be inferred. Speciﬁcally, inverse
reinforcement learning [NR00] aims at learning the reward function of a
conventional MDP given observed trajectories. A famous paradox of inverse
reinforcement learning is that many reward functions may result in the
same optimal policy, hence the problem of learning the reward function is
unidentiﬁed per se.

Within the Bayesian probabilistic framework, apprenticeship learning
reduces to just swapping some latent and observed variables in the agent
model 9. Agent preferences are speciﬁed by Ds. Inference on essentially the
same model ma, with a slight modiﬁcation that Ds is a random variable
drawn from a prior, conditioned on MDP trajectories, reiﬁes apprenticeship
learning (Deﬁnition 4). Note that aa and ab are observed rather than drawn
here.

9 Numerical Experiments

Deﬁnition 4. A 2-agent MDP apprentice model is a model

Ds ∼ ma(s) =













Ds ∼ Hs
aa ∼ Da(s)
a ← aa ◦ ab
s(cid:48) ← t(s, a)
s(cid:48) ∼ Ds(s)
s(cid:48) (cid:54)= ⊥ ∧ ma(s(cid:48))













17

(9)

conditioned on an MDP trajectory, where Hs is a prior distribution of
distributions of agent preferences, and the rest is as in Deﬁnition 3.

9 Numerical Experiments

We illustrate reasoning about future with probabilistic preferences using
implementations of the fable of Bob and Alice and of the sailing prob-
lem [PG04, KS06, TS12].

9.1 The Fable of Bob and Alice

We implemented the model and inference in Gen [CTSLM19]. The model,
along with supporting data types and functions, is shown in Listing 1. Since
Gen does not have built-in support for stochastic conditioning, we emulate
stochastic conditioning using Bernoulli distribution (line 29). This is of
course speciﬁc to the simple case of Bob and Alice, and is not possible in
general.

The agent model, along with supporting data types and functions, is

shown in Listing 1.

Gen requires writing the inference code, shown in Listing 2. Conditioning
on :logp equal to true refers to emulation of stochastic conditioning in
the model.

The fable of Bob and Alice is simple enough to analyze using an analytical
model, without resorting to approximate inference. Listing 3 shows the agent
implemented analytically. The agent returns the exact rather than Monte-
Carlo approximated choice distribution.

In the experiments below, we show results obtained both by Monte-Carlo
inference on the Gen model and using the analytical model, both to demon-
strate that the Gen model, more complicated but generalizable to a wide
range of problems, returns essentially the same results as the analytical

9 Numerical Experiments

18

model and to help the reader discern easily between actual trends and ap-
proximation errors in inference outcomes. We consider several cases of Bob’s
and Alice’s preferences. We run Monte Carlo inference for 5000 iterations
with 10% burn-in. Each plot shows the posterior choice distributions of each
of the agents for a range of deliberation depths, with depth 0 corresponding
to the prior behavior, depth 1 to the posterior using the prior behavior of
the other agent, and so on. The source code of the studies is available at
https://bitbucket.org/dtolpin/playermodels/.

9.1.1 Bob and Alice Want to Meet

We ﬁrst explore rational behavior of Bob and Alice when they want to meet
each other. Figure 1 shows the case when Bob and Alice have the same
preferences with respect to the bar and to meeting the other person. Posterior
choice distributions of both agents are identical, and approach asymptotically
a limit probability of going to the ﬁrst bar, which is still smaller than 1, as
the depth goes to inﬁnity.

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 1: Bob and Alice like the same bar: p1

Alice = p1

Bob = 0.55, pm

Alice =

pm
Bob = 0.9

Figure 2 addresses contradictory preferences with respect to bars — Alice
strongly prefers the ﬁrst bar, while Bob slightly prefers the second bar. Since
they still want to meet, they both choose, when the deliberation depth is
suﬃcient, the ﬁrst bar more often than the second one.

9.1.2 Bob Chases Alice, Alice Avoids Bob

Another setting is an instance of pursuit-evasion game: Bob chases Alice,
but Alice avoids Bob. The choice distributions of both agents depend on

9 Numerical Experiments

19

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 2: Bob and Alice like diﬀerent bars: p1

Alice = 0.75, p1

Bob = 0.45, pm

Alice =

pm
Bob = 0.75

how strong their preferences to meet, or to avoid the meeting, are. Figure 3
shows inference results for the case where Bob’s and Alice’s feelings are
opposite but mild. As Bob and Alice deliberate deeper and deeper, their
choice distributions eventually converge to stable limits.

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 3: Bob chases Alice, mild feelings: p1

Alice = p1

Bob = 0.55, pm

Alice = 0.25,

pm
Bob = 0.75

However, if Bob’s and Alice’s feelings are opposite and strong (Figure 4),
the agents overthink and act impulsively — as the deliberation depth in-
creases, the choice distributions of both agents alternate between going to
the ﬁrst or to the second bar with increasing probability.

9 Numerical Experiments

20

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 4: Bob chases Alice, strong feelings: p1

Alice = p1

Bob = 0.55, pm

Alice = 0.1,

pm
Bob = 0.9

9.1.3 Bob and Alice Avoid Each Other

Further on, we explore the case when Bob and Alice avoid each other,
that is, both of them would rather go to the pub where they are unlikely
to meet the other person. When the preference towards avoiding is mild
(Figure 5), Bob and Alice, with suﬃcient deliberation depth, approach their
prior preferences, that is they both go to the ﬁrst bar with the probability
approaching p1
Bob. Indeed, rationally this the best they can do to
minimize their chances to meet each other without coordination, while still
respecting their preference of the ﬁrst bar.

Alice = p1

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 5: Bob and Alice avoid each other, mild feelings: p1

Alice = p1

Bob = 0.55,

Alice = pm
pm

Bob = 0.25

However, when Bob’s and Alice’s mutual despisal is too strong (Figure 6),

9 Numerical Experiments

21

mutual epistemic reasoning does a poor job — Bob and Alice switch their
increasingly strong posterior preferences between the ﬁrst and the second
bar with each level of deliberation depth, similarly to the case of strong
feelings in pursuit-evasion situation. One may argue that these outcomes
are algorithmically anomalous, or ‘irrational’; however, they may be also
interpreted as a demonstration that rational behavior is not unconditionally
stable, and cannot be guaranteed for arbitrary combinations of preferences.

(a) Monte Carlo approximation

(b) Analytical solution

Fig. 6: Bob and Alice avoid each other, strong feelings: p1

Alice = p1

Bob = 0.55,

Alice = pm
pm

Bob = 0.05

9.1.4 Alice Learns Bob’s Preferences

Finally, let us see how Alice can ﬁnd out whether Bob wants to meet her
by observing Bob’s behavior. We use the same model of Alice for inference,
but instead of conditioning the model on Bob’s preferences, we condition the
model on the history of Bob’s choices and infer Bob’s preference (log-odds)
of meeting Alice. We condition the model on three Bob’s choices in a row,
and compare Alice’s conclusions for two cases:

1. Bob chose the ﬁrst bar three times in a row;

2. Bob chose the second bar three times in a row.

To visualize the results, we draw 100 samples from the posterior distribution
of the probability with that Bob would choose to meet Alice everything
else being equal. Figure 7 shows a two-dimensional scatter plot where each
direction is the marginal distribution of log-odds of Bob willing to meet Alice.
Indeed, 96 out of 100 points are below the diagonal, meaning that Alice can
be very conﬁdent that Bob is willing to meet her if she observes Bob in the

9 Numerical Experiments

22

ﬁrst bar 3 times in a row. However, if Bob shows up three times in a row in
the second bar, Alice should conclude that Bob is avoiding her.

Fig. 7: Alice infers Bob preferences: three visits to the same bar in a row

provide strong evidence about Bob’s desire to meet Alice.

9.2 The Sailing Problem

The sailing problem (Figure 8) is a popular benchmark problem for search
and planning. A sailing boat must travel between the opposite corners A
and B of a square lake of a given size. At each step, the boat can head in
8 directions (legs) to adjacent squares (Figure 8a). The unit distance cost
of movement depends on the wind (Figure 8b), which can also blow in 8
directions. There are ﬁve relative boat and wind directions and associated
costs: into, up, cross, down, and away. The cost of sailing into the wind is
prohibitively high, upwind is the highest feasible, and away from the wind
is the lowest. The side of the boat oﬀ which the sail is hanging is called
the tack, either port or starboard. When the angle between the boat and
the wind changes sign, the sail must be tacked to the opposite tack, which
incurs an additional tacking delay cost. The objective is to ﬁnd a policy that
minimizes the expected travel cost. The wind is assumed to follow a random
walk, either staying the same or switching to an adjacent direction, with a
known probability.

For any given lake size, there is a non-parameteric stochastic policy that
tabulates the distribution of legs for each combination of location, tack,

9 Numerical Experiments

23

(a) lake

(b) points of sail

Fig. 8: The sailing problem

Tab. 1: Sailing problem parameters

cost

wind probability
into up cross down away delay same left right
0.3
1
∞ 4

0.3

0.4

3

4

2

and wind. However, such policy does not generalize well — if the lake area
increases, due to a particularly rainy year for example, the policy is not
applicable to the new parts of the lake. In this case study, we infer instead a
generalizable parametric policy balancing between hesitation in anticipation
for a better wind and rushing to the goal at any cost. The policy chooses
a leg with the log-probability equal to the euclidian distance between the
position after the log and the goal, multiplied by the policy parameter θ (the
leg directed into the wind is excluded from choices). The greater the θ, the
higher is the probability that a leg bringing the boat closer to the goal will
be chosen:

log Pr(leg) = θ · distance(next-location, goal ) − log Z

(10)

here, Z is the normalization constant ensuring that the probabilities of all
legs sum up to 1. It can be readily computed but is not required for inference.
We implemented the model and inference in Infergo [Tol19].

The model turns out to be similar in structure to that of Bob and Alice.

• The ﬁrst agent, Alice, is the boat. The agent chooses a path across the

lake.

9 Numerical Experiments

24

Fig. 9: The sailing problem: θ

• The second agent, Bob, is the wind. The agent chooses a random walk
of wind direction along the boat’s path. The wind is a neutral agent —
it does not try to either help or tamper with the boat.

• The boat’s path, stochastically conditioned on the wind, has the prob-
ability proportional to the product of exponentiated negated leg costs
(which are interpreted as negated log probabilities of the boat to choose
each leg regardless of the goal location).

Model (11) formalizes our setting:

wind -history ∼ RandomWalk

log θ ∼ Uniform(0, ∞)

boat-trajectory ∼ D(wind -history, θ)

Pr(boat-trajectory, θ) ∝ exp(−travel -cost(boat-trajectory, wind -history))

(11)
The model parameters (cost and wind change probabilities), same as in [KS06,
TS12], are shown in Table 1. A non-informative improper prior is imposed
on θ. We ﬁt the model using pseudo-marginal Metropolis-Hastings [AR09]
and used 10 000 samples to approximate the posterior. Figure 9 shows the
posterior distribution of the unit cost. Table 2 shows the expected travel
costs, with the expectations estimated both over the unit cost and the wind.
The inferred travel costs are compared to the travel costs of the ‘optimal’
policy and of the greedy policy, according to which the boat always heads in
the direction of the steepest decrease of the distance to the goal. One can
see that the inferred policy attains an expected travel cost lying between the
greedy policy and the ‘optimal’ policy, as one would anticipate.

2345670.00.51.01.5densitymode rangesize:  25size:  50size: 10010 Related Work

25

Tab. 2: The sailing problem: travel cost

25
Inferred 105
107
Greedy
Optimal
103

50
209
215
206

100
413
430
406

10 Related Work

Possibility and importance of casting reinforcement learning as probabilistic
inference are well understood in AI research [TS06, BA09]. [KZT11] explore
planning as inference in the case of multiple agents, exploring special cases
in which eﬃcient scalable inference is possible.

Although not strictly in the ﬁeld of reinforcement learning, [Kap07] lays
out stochastic control theory and shows the connection between Boltzmann
distribution and expected utility maximization. [WGR+11] demonstrates
usefulness and applicability of probabilistic programming for planning as in-
ference on deterministic planning domains. [vdMPTW16] apply probabilistic
programming to stochastic domains, using a custom policy search algorithm.
[SG14] explore multiagent settings using probabilistic programming, im-
plementing theory of mind through mutually recursive nested condition of
models. [SvdMW18] apply theory of mind using probabilistic programming
to an elaborated pursuit-evasion problem.

[TZRY21] introduces stochastic conditioning, a Bayesian inference for-
malism necessary to cast reinforcement learning as a purely probabilistic
Bayesian inference problem. In one of the case studies, [TZRY21] provide
an early indication that planning as inference can be implemented through
stochastic conditioning.

11 Discussion

We demonstrated that reinforcement learning, that is, agent’s reasoning
about preferred future behavior, can be formulated as Bayesian inference.
Probability distributions can be used to specify all aspects of uncertainty
or stochasticity, including agent preferences. Rewards can be interpreted
as log-odds of stochastic preferences and do not need to be explicitly intro-
duced. Planning algorithms maximizing the expected utility ﬁnd maximum
a posteriori characterization of the rational policy distribution.

11 Discussion

References

[AN04]

[AR09]

[BA09]

26

Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning
In Proceedings of the
via inverse reinforcement learning.
Twenty-First International Conference on Machine Learning,
ICML ’04, page 1, New York, NY, USA, 2004. Association
for Computing Machinery.

Christophe Andrieu and Gareth O. Roberts. The pseudo-
marginal approach for eﬃcient Monte Carlo computations.
The Annals of Statistics, 37(2):697–725, 2009.

Matthew Botvinick and Ja/mes An. Goal-directed decision
making in prefrontal cortex: a computational framework.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou,
editors, Advances in Neural Information Processing Systems,
volume 21. Curran Associates, Inc., 2009.

[CTSLM19] Marco F. Cusumano-Towner, Feras A. Saad, Alexander K.
Lew, and Vikash K. Mansinghka. Gen: A general-purpose
probabilistic programming system with programmable infer-
ence. In Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI
2019, page 221–236, New York, NY, USA, 2019. Association
for Computing Machinery.

[ESSF17]

[HFWL17]

Owain Evans, Andreas Stuhlm¨uller, John Salvatier, and
Daniel Filan. Modeling Agents with Probabilistic Programs.
http://agentmodels.org, 2017. Accessed: 2021-7-29.

Xiao Huang, Biqing Fang, Hai Wan, and Yongmei Liu. A
general multi-agent epistemic planner based on higher-order
belief change. In Proceedings of the Twenty-Sixth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-17, pages
1093–1101, 2017.

[HGEJ17]

Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and
Chrisina Jayne.
Imitation learning: A survey of learning
methods. ACM Comput. Surv., 50(2), April 2017.

[Kap07]

Hilbert J. Kappen. An introduction to stochastic control
theory, path integrals and reinforcement learning. In American

11 Discussion

27

[KG15]

[KS06]

[KZT11]

[Noz69]

[NR00]

[PG04]

[PM17]

[Rai18]

[SG14]

Institute of Physics Conference Series, volume 887, pages 149–
181, 2007.

Filippos Kominis and Hector Geﬀner. Beliefs in multiagent
planning: From one agent to many. ICAPS’15, page 147–155.
AAAI Press, 2015.

Levente Kocsis and Csaba Szepesv´ari. Bandit based Monte-
Carlo planning. In Proceedings of the European Conference
on Machine Learning, pages 282–293, 2006.

Akshat Kumar, Shlomo Zilberstein, and Marc Toussaint. Scal-
able multiagent planning using probabilistic inference. In Pro-
ceedings of the Twenty-Second International Joint Conference
on Artiﬁcial Intelligence - Volume Volume Three, IJCAI’11,
page 2140–2146. AAAI Press, 2011.

Robert Nozick. Newcomb’s Problem and Two Principles of
Choice, pages 114–146. Springer Netherlands, Dordrecht,
1969.

Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse
reinforcement learning. In Proceedings of the Seventeenth In-
ternational Conference on Machine Learning, ICML ’00, page
663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann
Publishers Inc.

Laurent P´eret and Fr´ed´erick Garcia. On-line search for solving
Markov decision processes via heuristic sampling. In Proceed-
ings of the 16th European Conference on Artiﬁcial Intelligence,
pages 530–534, 2004.

David L. Poole and Alan K. Mackworth. Artiﬁcial Intelligence:
Foundations of Computational Agents. Cambridge University
Press, USA, 2nd edition, 2017.

Tom Rainforth. Nesting probabilistic programs. In Proceedings
of the 34th Conference on Uncertainty in Artiﬁcial Intelligence,
pages 249–258, 2018.

A. Stuhlm¨uller and N.D. Goodman. Reasoning about reason-
ing by nested conditioning: Modeling theory of mind with
probabilistic programs. Cognitive Systems Research, 28:80–99,
2014. Special Issue on Mindreading.

11 Discussion

[SvdMW18]

[Sze10]

[Tol99]

[Tol19]

[TS06]

[TS12]

[TZRY21]

28

Iris Rubi Seaman, Jan-Willem van de Meent, and David
Wingate. Nested reasoning about autonomous agents using
probabilistic programs, 2018.

Csaba Szepesv´ari. Algorithms for reinforcement learning. Syn-
thesis Lectures on Artiﬁcial Intelligence and Machine Learning,
4(1):1–103, January 2010.

Eckhart Tolle. The power of now: a guide to spiritual enlight-
enment. Novato, California: New World Library, 1999.

David Tolpin. Deployable probabilistic programming.
In
Proceedings of the 2019 ACM SIGPLAN International Sym-
posium on New Ideas, New Paradigms, and Reﬂections on
Programming and Software, pages 1–16, 2019.

Marc Toussaint and Amos Storkey. Probabilistic inference
for solving discrete and continuous state markov decision
processes. In Proceedings of the 23rd International Conference
on Machine Learning, ICML’06, page 945–952, New York,
NY, USA, 2006. Association for Computing Machinery.

David Tolpin and Solomon Eyal Shimony. MCTS based on
simple regret. In Proceedings of The 26th AAAI Conference
on Artiﬁcial Intelligence, pages 570–576, 2012.

David Tolpin, Yuan Zhou, Tom Rainforth, and Hongseok
Yang. Probabilistic programs with stochastic conditioning.
In Marina Meila and Tong Zhang, editors, Proceedings of the
38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pages 10312–
10323. PMLR, 18–24 Jul 2021.

[vdMPTW16] Jan-Willem van de Meent, Brooks Paige, David Tolpin, and
Frank Wood. Black-box policy search with probabilistic pro-
grams. In Proceedings of the 19th International Conference on
Artiﬁcial Intelligence and Statistics, pages 1195–1204, 2016.

[vdMPYW18] Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and
Frank Wood. An introduction to probabilistic programming.
arXiv:1809.10756, 2018.

11 Discussion

[WGR+11]

29

David Wingate, Noah D. Goodman, Daniel M. Roy, Leslie P.
Kaelbling, and Joshua B. Tenenbaum. Bayesian policy search
with policy priors. In Proceedings of the 22nd International
Joint Conference on Artiﬁcial Intelligence, pages 1565–1570,
2011.

11 Discussion

30

Listing 1 Model of Bob or Alice in Gen

end

else

if I1

name::String
p1::Real
pm::Real

q * log(1 - a.pm) + (1 - q)*log(a.pm)

q * log(a.pm) + (1 - q)*log(1 - a.pm)

1 "Agent parameters"
2 struct Agent
3
4
5
6 end
7
8 "Computes conditional log-probability of agent’s choice"
9 function logp(a::Agent, q::Real, I1::Bool)
10
11
12
13
14
15 end
16
17 "Defines an agent reasoning about the other agent
18
19 @gen function model(me::Agent, buddy::Agent,
20
21
22
23
24
25
26
27
28
29
30 end

end
I1 = @trace(bernoulli(me.p1), :I1)
# Stochastic conditioning, emulated
@trace(bernoulli(exp(logp(me, q, I1))), :logp)

J1s = run_model(buddy, me, depth-1, niter)
q = sum(J1s)/length(J1s)

with a given recursion depth"

depth::Int, niter::Int)

if depth > 0

q = 0.5

else

11 Discussion

31

Listing 2 Inference on the model in Listing 1

depth::Int, niter=1000)

trace, _ = Gen.generate(model,

observations = Gen.choicemap()
observations[:logp] = true

1 @memoize function run_model(me::Agent, buddy::Agent,
2
3
4
5
6
7
8
9
10
11
12
13
14
15 end

trace, _ = Gen.mh(trace, select(:I1))
push!(J1s, get_choices(trace)[:I1])

J1s = []
for i = 1:niter

end
return J1s

(me, buddy, depth, niter),
observations)

Listing 3 Analytical model

p = me.p1
if depth > 0

1 "Defines an analytical model of the agent with choice
2 probabilities computed in closed form, rather than
3 through Monte Carlo inference."
4 @memoize function anamodel(me::Agent, buddy::Agent,
5
6
7
8
9
10
11
12
13
14 end

q = anamodel(buddy, me, depth-1)
u1 = log(p) + logp(me, q, true)
u2 = log(1-p) + logp(me, q, false)
p = exp(u1)/(exp(u1) + exp(u2))

depth=0)

end
p

