1
2
0
2

y
a
M
6

]

V
C
.
s
c
[

1
v
9
6
7
2
0
.
5
0
1
2
:
v
i
X
r
a

2021-04-28

Computer-Aided Design as Language

Yaroslav Ganin1, Sergey Bartunov1, Yujia Li1, Ethan Keller2 and Stefano Saliceti1
1DeepMind, 2Onshape

Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coﬀee
mugs to sports cars. These programs are complex and require years of training and experience to master.
A component of all CAD models particularly diﬃcult to make are the highly structured 2D sketches that
lie at the heart of every 3D construction. In this work, we propose a machine learning model capable
of automatically generating such sketches. Through this, we pave the way for developing intelligent
tools that would help engineers create better designs with less eﬀort. Our method is a combination of
a general-purpose language modeling technique alongside an oﬀ-the-shelf data serialization protocol.
We show that our approach has enough ﬂexibility to accommodate the complexity of the domain and
performs well for both unconditional synthesis and image-to-sketch translation.

Keywords: computer-aided design, generative models, transformers, structured objects

1. Introduction

coincident

mirror

Computer-Aided Design (CAD) is used in the pro-
duction of most everyday objects: from cars to
robots to stents to power plants. CAD has re-
placed pencil drawings with precise computer
sketches, enabling unparalleled precision, ﬂexi-
bility, and speed. Despite these improvements
the CAD engineer must still develop, relate and
annotate all the minutiae of their designs with
the same attention to detail as their drafting-table
forebears. The next step change in CAD produc-
tivity will come from the careful application of
machine learning to automate predictable design
tasks and free the engineer to focus on the big-
ger picture. The ﬂexibility and power of deep
learning is uniquely suited to the complexity of
design.

Sketches are at the heart of mechanical CAD.
They are the skeleton from which three dimen-
sional forms are made. A sketch consists of vari-
ous lines, arcs, splines and circles related by spe-
ciﬁc constraints such as tangency, perpendicu-
larity and symmetry. Figure 1 illustrates how
constraints relate diﬀerent geometric entities to
create well-deﬁned shapes. The dotted lines show
another equally valid solution to the heart if some
of the constraints were dropped. The geometric
entities all lie on a single sketch plane and to-
gether form enclosed regions used by subsequent

Corresponding author(s): ganin@google.com
© 2021 DeepMind. All rights reserved

orthogonal

tangent

Figure 1 | The anatomy of a CAD sketch.
Sketches are the main building block of every 3D
construction. A sketch consists of geometric enti-
ties (primitives like lines and arcs) and constraints
(e.g., tangent and mirror). The constraints are
meant to convey the design intent and deﬁne how
the shape should change under various transfor-
mations of the entities. The dotted sketch demon-
strates what happens if we drop some constraints
and modify the parameters of the primitives – the
design idea is lost.

 
 
 
 
 
 
Computer-Aided Design as Language

construction operations such as lofts and extru-
sions to generate complex 3D geometry.

Constraints make sketches much more compli-
cated than they initially appear. They express
relationships that can indirectly aﬀect every en-
tity in the sketch. For instance, in Figure 1, if the
point at which the two arcs meet is dragged up
while the bottom corner is kept stationary, the
heart will grow in size. Even though the transfor-
mation may seem simple, that growth is actually
the result of every constraint working in concert.
The constraints ensure the shape remains in the
state the designer conveyed despite every one of
the entities having changed size and location. Be-
cause of this complex interplay between entities,
it’s easy to accidentally specify a set of constraints
that is impossible for the constraint solver to sat-
isfy, resulting in an invalid sketch. For example,
two lines that satisfy both parallel and perpendic-
ular constraints are impossible to draw. In a com-
plex sketch, chains of constraint dependencies
can make it exceedingly diﬃcult for a designer to
determine which constraint to add without inval-
idating the sketch. Additionally, for a given set of
entities, there are many equally valid constraint
systems that yield similar sketch behavior. A high
quality sketch uses a set of constraints that pre-
serves design intent, meaning the semantics of
the sketch are preserved even as entity parame-
ters (such as dimensions) are altered (Barbero
et al., 2017; Hartman, 2005; Kyratzi and Azari-
adis, 2020; Wu, 2009). Simply put, no matter
how the entity dimensions are changed, the heart
in 1 will always be a heart. The challenge of cap-
turing design intent coupled with the inherent
complexity of choosing a consistent constraint
system make sketch generation an exceedingly
diﬃcult problem (Rynne and Gaughran, 2008).

The aforementioned complexities of sketch con-
struction are analogous to those of natural lan-
guage modeling. Selecting the next constraint
or entity in a sketch is like the generation of the
next word in a sentence. In both contexts, the
selection must function grammatically (form a
consistent constraint system in the case of the
sketch) and work towards some cohesive mean-
ing (preserve design intent). Luckily, there are
many tools that have proved highly successful in

generating natural language. The top performers
are unequivocally machine learning models that
train on vast amounts of real-world data (Brown
et al., 2020; Radford et al., 2019). The Trans-
former architecture (Vaswani et al., 2017) in par-
ticular demonstrates unparalleled ability to create
cohesive sentences, and is therefore a promising
choice for adapting to the task of sketch genera-
tion. This work is our take at this adaptation. We
make the following contributions:

• We devise a method for describing structured
objects using Protocol Buﬀers (Varda, 2008)
and demonstrate its ﬂexibility on the domain
of natural CAD sketches.

• We propose several techniques for capturing
distributions of objects represented as serial-
ized Protocol Buﬀers. Our approach draws
inspiration from recent advances in language
modeling while focusing on eliminating data
redundancy.

• We collect a dataset containing over 4.7M
of carefully preprocessed parametric CAD
sketches. We use this dataset to validate the
proposed generative models. To our knowl-
edge, the experiments presented in this work
signiﬁcantly surpass the scale of those re-
ported in the literature both in terms of the
amount of training data and the model ca-
pacity.

2. Related work

Constraints and design intent. Well-chosen
sketch constraints are essential to properly con-
vey design intent (Ault, 1999) and facilitate
its resilience to successive parameters modiﬁca-
tions which is often understood as a measure
of the quality of a design document (Company
et al., 2019). Among studies conducted on how
to predict constraints in a CAD sketch, Kyratzi
and Azariadis (2020) introduce an analysis of
intention-regularities and the relative set of meta-
constraints, thus prescribing a set of rules to prop-
erly design a constraint set. Veuskens et al. (2020)
present a framework to iteratively guide the user
in suggesting a set of missing constraints from a
sketch. None of the proposed approaches, derived
from the Human-Computer Interaction commu-

2

Computer-Aided Design as Language

Interpreter states

line.end.x

line.end.y

(0, 0.6, False)

Transformer (Vaswani et al., 2017) + Pointer Net (Vinyals et al., 2015)

. . .

. . .

line.start.y: . . .

line.end.x: 0.6

. . .

. . .

. . .

Tokens

tangent

Figure 2 | Interpreter-guided generation of a sketch. At each point in time, a Transformer (Vaswani
et al., 2017) outputs a raw value which is fed into an interpreter that decides which ﬁeld of a Protocol
Buﬀers message this value corresponds to. Once the ﬁeld is populated the interpreter communicates its
decision back to the Transformer and transitions to the next state. The system is capable of generating
sequences of arbitrary structured objects and therefore is suitable for synthesizing both sketch entities
and constraints in one go.

nity, leverages machine learning techniques to
infer those rule-sets from the data. Since explicit
programming of such rules can be very diﬃcult,
performance of rule-based systems is constrained
by the eﬀort put in curating the set of rules which,
we believe, is ultimately less eﬃcient than directly
capturing human practice by a high-capacity gen-
erative model.

Datasets and generative models for CAD. Un-
til recently there were very few parametric CAD
datasets large and varied enough to serve as train-
ing data for machine learning. This situation had
started to change with the release of the ABC
dataset (Koch et al., 2019).
It contains a col-
lection of 3D shapes from the Onshape public
repository (Onshape developers) and is intended
to be used in geometric deep learning research.
In order to enable the latter, the authors provide

ground-truth targets for a variety of tasks ranging
from estimation of diﬀerential surface properties
to feature detection. Unfortunately, the main fo-
cus of this work revolves around meshes and, as a
result, the parametric aspect of the data is largely
overlooked. This manifests itself in incomplete-
ness of the supplied symbolic representations of
the shapes. When it comes to the CAD sketches
speciﬁcally, little to no eﬀort was put to extract
and prepare them for machine learning applica-
tions.

Several works concurrent with ours aim to ad-
dress the limitations of (Koch et al., 2019) and
shift focus to CAD construction sequences, i.e., to
how of design rather than what. Seﬀ et al. (2020)
center their attention on contributing a more com-
plete and easy to use dataset of 2D sketches. As
we discuss in Section 5, the resulting data suf-
fers from a signiﬁcant amount of duplication and

3

Computer-Aided Design as Language

therefore should be used with caution. In addi-
tion to the dataset, the paper also proposes a base-
line generative model employing autoregressive
message passing networks (Gilmer et al., 2017).
This approach, however, can only handle simple
sketches and heavily relies on the external solver
to determine the ﬁnal placement of primitives.

Fusion 360 Gallery (Willis et al., 2020) attacks
CAD data from a diﬀerent angle. Here, the task is
to recover a sequence of extrusion operations that
gives rise to a particular target 3D shape. Despite
dealing with 3D, this setting is deliberately lim-
ited: sketches are assumed to be given and the
proposed model only decides on which sketch to
extrude and to what extent. While (Willis et al.,
2020) is a step towards full parametric CAD gen-
eration, it is unclear how well this approach will
scale to more real-world scenarios.

contrast to our approach, both of these methods
use highly domain-dependent architectures and,
therefore, it would be a non-trivial task to adapt
them for generation of complex sketch objects.
Similarly, (Li et al., 2020) use diﬀerentiable ren-
derers to allow gradient-based optimization of
visible entities constituting a vector image. How-
ever, this idea is not straightforwardly applicable
to sketch generation since the choice of what en-
tities to use in the sketch as well as handling of
constraints are discrete in their nature.

Egiazarian et al. (2020) address the task of
vectorizing raster technical drawings by means of
a custom, multi-stage computer vision pipeline.
As such, this system is not formulated as an end-
to-end trainable model and also does not concern
constraint inference which, as we argue, is an
important aspect of sketch modeling.

Image to CAD conversion. Beyond uncondi-
tional generation, the research community has a
long-standing interest in the problem of conver-
sion from a drawing (either computer-rendered
or hand-drawn) to a vectorized CAD descrip-
tion. This task can be seen as a special case
of image vectorization (Jimenez and Navalon,
1982) and has been extensively studied in the
past (Dori and Tombre, 1995; Dori and Wenyin,
1999; Nagasamy and Langrana, 1990; Vaxiviere
and Tombre, 1992). However, those systems
mostly relied on heuristic object recognition and
were not learning-driven. In this paper we present
an image conditional model as a solution to this
problem. In contrast to existing approaches we
make no assumptions on visual appearance, do
not program any object recognition heuristics,
and instead learn them from the training data.

Vector image generation and inference. Syn-
thesizing CAD sketches bears a lot of similarities
with predicting vector graphics. In this ﬁeld, sev-
eral recent works employ autoencoders to model
vector art. Carlier et al. (2020) present a hierar-
chical VAE (Kingma and Welling, 2013) capable
of producing novel SVG icons. Im2Vec (Reddy
et al., 2021) takes this idea even further and rely
on a diﬀerentiable renderer to remove the neces-
sity for having training data in vector format. In

Transformers for sequence modeling.
In re-
cent years, Transformers (Vaswani et al., 2017)
have become the dominating approach in many
sequence modeling applications. Works like
(Brown et al., 2020; Dhariwal et al., 2020;
Ramesh et al., 2021; Razavi et al., 2019) demon-
strate impressive and sometimes surprising ca-
pability of the architecture to exhibit intelligent
behaviour in tasks like image and audio synthe-
sis and generation of natural language. For that
reason, we employ Transfomers as a computa-
tional backbone for the system we present in this
manuscript.

Our method can be seen as generalization of
PolyGen (Nash et al., 2020), a Transformer-based
generative model for 3D meshes. Similarly to
(Nash et al., 2020), we use Pointer Networks
(Vinyals et al., 2015) to relate items in the syn-
thesized sequence. Unlike PolyGen, however, our
framework can handle non-homogeneous struc-
tures of arbitrary complexity. Moreover, we sim-
plify the architecture to use a single neural net-
work to generate the entire object of interest. All
these improvements make our approach a good ﬁt
for modeling CAD sketches and potentially other
components of CAD constructions.

4

Computer-Aided Design as Language

3. Data

3.1. Sketches

A CAD sketch is a 2D drawing that lives on some
3D plane and consists of a set of geometric primi-
tives such as points, line segments, arcs and cir-
cles called sketch entities (see Figure 3). Sketches
are considered a starting point for most CAD mod-
els: a 2D sketch can be “extruded” to create a 3D
shape; further sketches can be used to modify and
reﬁne existing 3D geometries by creating pockets,
ridges, holes or protrusions. The sketching oper-
ation in Onshape accounts for about 35% of the
features created every day (Chastell, 2015) mak-
ing it a potentially high impact target for machine
learning research.

Onshape is a parametric solid modeling soft-
ware. This means that designs are dimension-
driven, i.e., the geometry can be changed by mod-
ifying dimensions. Ideally, we would like these
changes to be predictable and to preserve the
integrity of the geometry, e.g., connected line
segments should remain connected, orthogonal
curves should remain orthogonal and so on. In
other words, sketch transformations should re-
spect the design intent. One way to achieve this
is by introducing a set of relationships between
sketch entities. For example, if we draw two cir-
cles that happen to be concentric we can state
this relation explicitly so that when one of the
circles is moved, the other moves with it, main-
taining the relation. We call such relations sketch
constraints. In this framework, in order to create
a simple rectangular shape we would start with
four line segments, constrain the end points to co-
incide forcing the shape to stay closed at all times
and, ﬁnally, make the opposite and the adjacent
segments parallel and orthogonal respectively.

A 2D sketch is thus deﬁned by two collections
of objects: entities and constraints. The parame-
ters of entities (e.g., coordinates of points, radii
of circles and so on) that we supply to the CAD
software do not need to be precise as they will
be used only as an initial guess by the underlying
constraint solver. The solver’s task is to adjust
the geometric conﬁguration so that all the con-
straints are satisﬁed.
It should be noted that,
in general, the solver considers entities and con-

Figure 3 | Examples from our sketch dataset.
Here, we only show entities. Although typical
sketch shapes are somewhat regular, the data
contains a signiﬁcant number of objects that may
seem invalid. The latter is usually due to the fact
that we are considering them outside the context
of the 3D construction they belong to.

straints holistically so the order in which they
appear in corresponding collections does not mat-
ter. As we model sketches as sequences, this does
mean that there are many diﬀerent orderings
(and also potentially diﬀerent parameter values
for the entities, as they do not need to be pre-
cise) that correspond to the same sketch. We will
expand more on this in Section 3.2.

3.2. Data layout

In order to store and process sketches, we employ
Protocol Buﬀers (PB) (Varda, 2008) rather than
the raw JSON format (Pezoa et al., 2016) pro-
vided by the Onshape API. The beneﬁt of using
this approach is twofold: the resulting data occu-
pies less space because unnecessary information
is removed, and the PB language makes it easy to
deﬁne precise speciﬁcations for complex objects
with varied structure.

Figure 4 shows how we represent the line entity
and the mirror constraint (see Appendix A for an
extensive list of supported objects). The line speci-
ﬁcation is straightforward: we ﬁrst need to decide
whether our entity should be treated as a con-
struction geometry (construction geometries will
not be displayed in the ﬁnal drawing) and then
provide pairs of coordinates for the beginning
and end of the segment. Constraints, such as the
mirror constraint, are generally deﬁned in a simi-
lar fashion to entities. The MirrorConstraint

5

Computer-Aided Design as Language

message LineEntity {

bool is_construction = 1;
message Vector {
double x = 1;
double y = 2;

}
Vector start = 2;
Vector end = 3;

}

message MirrorConstraint {

Pointer mirror = 1;
message Pair {

Pointer first = 1;
Pointer second = 2;

}
repeated Pair mirrored_pairs = 2;

}

Figure 4 | Examples of object speciﬁcations. We represent sketch entities and constraints using
Protocol Buﬀers. Protocol Buﬀers allow us to easily write speciﬁcations for structured objects of varying
complexity. For instance, a line entity is deﬁned by the coordinates of its end points and thus our
message has two corresponding ﬁelds each of which is a 2D vector. Constraints are speciﬁed in a
similar fashion except they additionally make use of the Pointer data type. Pointers refer to entities
that constraints are applied to.

is used to force an arbitrary number of pairs of
geometries (i.e., mirrored_pairs) to be sym-
metrical with respect to some axis (i.e., mirror).
One may notice that constraints make extensive
use of the Pointer data type. Pointer ﬁelds are
meant to refer to the entities present in the sketch.
In practice, we record all eligible pointees (i.e.,
entities and their parts) in a special table, and
a pointer is simply an index of an entry in that
table.

Once all the necessary object types are fully
speciﬁed we need to convert the data into a form
that can be processed by a machine learning
model. We choose to represent sketches as se-
quences of tokens. This allows us to pose sketch
generation as language modeling (LM) and take
advantage of the recent progress in this area
(Brown et al., 2020; Radford et al., 2019). To
achieve this, we ﬁrst pack collections of entities
and constraints into one Protocol Buﬀer message
in the sketch format deﬁned in Figure 5. We
assume some ordering of objects. For example,
we can adopt the ordering provided by the On-
shape API, which returns a list for the entities
followed by another list for the constraints. The
order within each list tends1 to reﬂect the order
in which the objects were created by the user.
Since constraints refer to entities it makes sense
to form the ﬁnal list of objects in such a way that
the beginning of the list is occupied by the enti-
ties and the remaining part is allocated for the

1Strictly speaking, it is not guaranteed

constraints. In our experiments (see Section 5),
we also explore an alternative ordering in which
we insert a constraint right after all the entities it
refers to.

There are a few ways to go about obtaining a se-
quence of tokens from a sketch message. Arguably
the most intuitive one is to format messages as
text. For a line entity connecting (0.0, 0.1) and
(−0.5, 0.2) this will result in something similar
to:

{

is_construction: true
start {

x: 0.0, y: 0.1

}
end {

x: -0.5, y: 0.2

}

}

The text format contains both the structure and
the content of the data. The advantage of using
this representation is that we can employ any
oﬀ-the-shelf approach for modeling textual data.
Unfortunately, this comes at a cost: the result-
ing sequences are prohibitively long even for the
modern LM techniques. Additionally, the model
would have to generate valid syntax, which would
take up some portion of the model’s capacity.

We can sidestep some of those limitations by
working with sequences of bytes forming the seri-
alized versions of sketch messages. Serialized PB
messages are much shorter since they do not con-

6

Computer-Aided Design as Language

message Sketch {

message Entity {
oneof kind {

LineEntity line = 1;
// And other entity types.

}

}
message Constraint {

oneof kind {

MirrorConstraint mirror = 1;
// And other constraint types.

}

}
message Object {
oneof kind {

Entity entity = 1;
Constraint constraint = 2;

}

}
repeated Object objects = 1;

}

Figure 5 | The speciﬁcation of a full sketch. A
sketch is deﬁned as a sequence of objects. Each
object can be either an entity or a constraint. Each
entity can be one of the supported types (e.g., a
LineEntity). Similarly, a constraint object is
resolved to a particular kind of constraint (e.g., a
MirrorConstraint).

tain any information about the object structure –
this burden is oﬄoaded on to an external parser
automatically generated from the data speciﬁca-
tion. The parser’s task is to interpret the incoming
stream of unstructured bytes and populate the
ﬁelds of PB messages. However, like the text for-
mat, not every sequence of bytes results in a valid
PB message.

Going one step further, we can utilize the struc-
ture of the sketch format, and build a custom de-
signed interpreter (rather than the generic parser
automatically generated from the PB deﬁnitions
used in the bytes format above), that takes as
input a sequence of tokens each representing a
valid choice at various decision steps in the sketch
creation process. We designed this interpreter in
such a way that all sequences of tokens in this
format lead to valid PB messages.

In this format we represent a message as a se-
quence of triplets (we call this the triplet format)

(𝑑𝑖, 𝑐𝑖, 𝑓𝑖) equipped with additional contextual in-
formation where 𝑖 is an index of the token. The
majority of tokens describe basic ﬁelds of the
sketch objects with each token representing ex-
actly one ﬁeld. The ﬁrst two positions in each
triplet are allocated for a discrete value and a con-
tinuous value respectively. Since each ﬁeld in a
message is either discrete or continuous only one
of two positions is active at a time (the other one
is set to a default zero value). Ignoring for now
the third position in the triplet we can write down
the sequence for the line entity above as follows
(highlighted are the active positions):

(1, 0.0), (0, 0.0), (0, 0.1), (0, −0.5), (0, 0.2) .

Note that we use the discrete component
of
token to set a boolean ﬁeld
line.is_construction .2

the ﬁrst

Besides basic ﬁelds, our sketch speciﬁcation
makes extensive use of the oneof construction.
In the PB language, oneof allows one to intro-
duce mutually exclusive ﬁelds. For example, in
Figure 5, we can see that a generic Object can
be either an entity or a constraint. To han-
dle this, we simply inject an additional token with
the discrete value set to the index of the active
ﬁeld.

Another feature of the PB language that is es-
sential for deﬁning sketches (and many other
structured objects) is repeated ﬁelds. A re-
peated ﬁeld is a list of an arbitrary number of
elements of the same type. We ﬁrst see this used
in the only top-level ﬁeld of the Sketch message:
a sketch is a list of Objects. In order to represent
such ﬁelds, we could just concatenate the tokens
of all the elements. The only problem we will
face is during generation when the model needs
to indicate that it has ﬁnished producing the list.
Speciﬁcally for this situation, we use the third
position of the token triplet, 𝑓𝑖. It is a boolean
ﬂag that signiﬁes the end of repetition. The “end”
token has the main portion set to (0, 0.0, True).
Note that since messages may have nested re-
peated ﬁelds the resulting sequence may contain
several tokens with 𝑓𝑖 = True.

2Here and further in the text, we employ the dotted path
notation to identify message ﬁelds occasionally dropping
preﬁxes where it leads to an unambiguous result.

7

Computer-Aided Design as Language

Triplet

Field

4. Model

1.

2.

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

(0, 0.0, False)
(0, 0.0, False)
(1, 0.0, False)
(0, 0.0, False)
(0, 0.1, False)
(0, −0.5, False)
(0, 0.2, False)
(0, 0.0, False)
(1, 0.0, False)
(0, 0.0, False)
(0, 0.0, False)
(0, 0.1, False)
(0, 0.0, True)

objects.kind
entity.kind
line.is_constr
line.start.x
line.start.y
line.end.x
line.end.y
objects.kind
entity.kind
point.is_const
point.x
point.y
objects.kind










L
i
n
e

P
o
i
n
t

Table 1 | A triplet representation of a simple
sketch. The sketch contains and a line and a
point. Within each triplet in the left column, the
active component (the value that is actually used)
is highlighted in bold. The right column shows
which ﬁeld of the object the triplet should be
associated with.

Given a sequence of such triplets, it is possible
to infer which exact ﬁeld each token corresponds
to. Indeed, the very ﬁrst token (𝑑1, 𝑐1, 𝑓1) is al-
ways associated with objects.kind since it is
the ﬁrst choice that needs to be made to create a
Sketch message (see Figure 5). The second ﬁeld
depends on the concrete value of 𝑑1. If 𝑑1 = 0 then
the ﬁrst object is an entity which means that
the second token corresponds to entity.kind .
The rest of the sequence is associated in a similar
fashion. Field identiﬁers along with their loca-
tions within an object form the context of the
tokens. We use this contextual information as an
additional input for our machine learning models
since it makes it easier to interpret the meaning
of the triplet values and to be aware of the overall
structure of the data.

Summarizing the above, a simple sketch with a
line entity and a point entity placed at one of its
ends will be described with the sequence shown
in Table 1.

Our main goal is to estimate the distribution 𝑝
data
of 2D sketches in a dataset D. As described in the
previous section, we are going to treat a sketch
as a sequence of tokens. In this work, we only
consider the byte and the triplet representations
due to the sequence length challenges associated
with the raw textual format.

In both cases, we decompose the joint distribu-
tion over the sequence of tokens t = (𝑡1, . . . , 𝑡𝑁)
as a product of conditional distributions:

𝑝(𝒕; 𝜃) =

𝑁
(cid:214)

𝑖=1

𝑝(𝑡𝑖 | 𝑡<𝑖 ; 𝜃) ,

(1)

where 𝑁 is the length of the sequence and 𝑡<𝑖 de-
notes all the tokens preceding 𝑡𝑖. Following the
standard approach for modeling sequential data
(Mikolov et al., 2010), we employ an autoregres-
sive neural network parameterized by 𝜃 to obtain
𝑝(𝑡𝑖
data estimation be-
comes maximization of the log-likelihood of D
under the model, i.e.,

| 𝑡<𝑖 ; 𝜃) ∀ 𝑖. The task of 𝑝

𝜃∗ = arg max

𝜃

∑︁

𝒕 ∈D

log 𝑝(𝒕; 𝜃) .

(2)

More concretely, we employ the Transformer
decoder architecture (Vaswani et al., 2017) that
accepts a vector representation of a token 𝒆𝑖−1 =
embed 𝑖 (𝑡𝑖−1) ∈ ℝ𝐷 (an embedding) and maps it
into another vector 𝒉𝑖 of the same dimensional-
ity. The latter is then decoded into parameters
of 𝑝(𝑡𝑖
| 𝑡<𝑖) by means of, for example, a linear
projection proj𝑖 (·).

4.1. Byte representation

When dealing with the bytes of a PB message,
each token is simply a discrete value in the range
{0, . . . , 255} ∪ {EOS}3 and therefore 𝑝(𝑡𝑖 | 𝑡<𝑖 ; 𝜃)
can be modeled as a categorical distribution sim-
ilar to how it’s done in typical LM approaches
(Bengio et al., 2003). In this setting, for each
time step 𝑖 of the sequence we have

embed 𝑖 (𝑡𝑖−1) = 𝑉 [𝑡𝑖−1] + 𝒆

pos
𝑖

,

(3)

3EOS denotes the end of the sequence.

8

Computer-Aided Design as Language

where [·] denotes the lookup operation and 𝒆
is a position embedding for position 𝑖. Both 𝑉 and
pos
are learned. Moreover, ∀𝑖 proj𝑖 ≡ proj :
𝒆
𝑖
ℝ𝐷 → ℝ257 and the output is treated as logits of
the distribution.

pos
𝑖

4.2. Triplet representation

In case of the triplet representation, we follow a
slightly more involved procedure. As outlined in
Section 3.2, tokens can be either discrete or con-
tinuous. Additionally, diﬀerent discrete tokens
may have diﬀerent ranges of values. For exam-
ple, there are only two possible values for the
object.kind token – either to an entity or a
constraint. On the other hand, the range of the
entity.kind token has cardinality of 4 since
we support 4 diﬀerent types of sketch entities.
This means that we can’t naively describe each
conditional in Equation 1 using the same tem-
plate distribution like we did for the bytes. We
circumvent this by introducing the notion of token
groups.

line.start.x ,

A token group G is a collection of related
token types that can be handled in a similar
fashion. Speciﬁcally, we use the same embedding
function and the same projection for each
𝑡 ∈ {𝑡 | type(𝑡) ∈ G}. For instance, we might
want to group all the tokens associated with
In the example from Table 1,
coordinates.
line.start.y ,
tokens
line.end.x , line.end.y , point.x and
point.y will all end up in the same G.4 When-
ever we need to compute the log-likelihood of
those tokens in the sequence we can employ the
same functional form of the output distribution,
e.g., a mixture of Gaussians (Graves, 2013;
Ha and Eck, 2017) with the same number of
components. In practice, however, we replace
continuous values with their 8-bit uniformly
quantized versions and model everything using
categorical distributions since we found it to
greatly improve the stability of training and the
overall performance. This is in line with the
observations made in (Nash et al., 2020; Oord
et al., 2016).

4For the full list of token groups used in the model see

Appendix B.

We embed 𝑡𝑖−1 that belongs to the group G as

(note the diﬀerence with Equation 3):

𝐾 [field(𝑡𝑖−1)] + 𝑉 G [𝑡𝑖−1] + 𝒆

obj
𝑛 + 𝒆rel
𝑚 ,

(4)

where field(𝑡) returns the categorical ﬁeld iden-
tiﬁer associated with 𝑡 and 𝐾 is a collection of
learnable embeddings for every possible ﬁeld type.
Unlike in Equation 3, instead of using global po-
pos
sition embedding 𝒆
𝑖 we describe the location
with the index 𝑛 of the current object as well as
the relative position 𝑚 of 𝑡𝑖−1 within an object.

We handle the “end” tokens (i.e., 𝑓𝑖 = True) sim-
ilarly to EOS in Section 4.1 – the output projection
produces an additional logit used to compute the
probability of ending the repetition. Since 𝑓𝑖 is
only expected to be True at particular points in
the sequence (i.e., right after tokens forming a
whole element of the list) we mask out the ex-
tra logit everywhere else. This ensures that the
“end” token can’t be predicted prematurely and
also eliminates its unnecessary contribution to
the optimized objective.

One signiﬁcant diﬀerence between the byte
setting and the triplet setting is how we process
pointer ﬁelds. In the former, pointers do not get
any special treatment and are generated just like
any other integer ﬁeld. We rely on the model’s
capability to make sense of the entity part index
and relate it to the corresponding locations in
the sequence via attention weights. This seems
to be a viable strategy since Transformers have
demonstrated an impressive referencing capacity
in recent works (Brown et al., 2020).

Since the triplet representation provides us
with direct access to the semantics of tokens it’s
possible to relate pointers to their pointees more
explicitly by using Pointer Networks (Vinyals
et al., 2015). The approach we are taking here
is similar to (Nash et al., 2020). In order to com-
pute 𝑝(𝑡𝑖
| 𝑡<𝑖 ; 𝜃), we ﬁrst project the output of
the Transformer 𝒉𝑖 into the ﬁnal pointer vector
𝒑𝑖 = 𝑊ptr 𝒉𝑖. The conditional is then obtained as
follows:

𝑇
𝑖 𝐻𝑖) , (5)
𝑝(𝑡𝑖 points to 𝑡𝑖𝑘 | 𝑡<𝑖 ; 𝜃) = softmax𝑘 ( 𝒑
𝐻𝑖 = [𝒉𝑖1, . . . , 𝒉𝑖𝑘, . . . , 𝒉𝑖𝑀] ,
(6)
where softmax𝑘 is the 𝑘-th element of the softmax
vector and 𝒓𝑖 = {𝑡𝑖1, . . . , 𝑡𝑖𝑀 } is the set of tokens

9

Computer-Aided Design as Language

that 𝑡𝑖 can point to. Naturally, 𝒓𝑖 only contains
tokens from 𝑡<𝑖. This is diﬀerent from (Nash et al.,
2020; Vinyals et al., 2015) where 𝒓𝑖 ≡ 𝒓 is external
to the predicted sequence and remains immutable
throughout the generation process.

The pointer mechanism takes into account that
there is no clear correspondence between the to-
kens that describe an entity and the entity’s parts
that pointers can refer to in the data. For example,
one may want to create a constraint that applies
to the midpoint of a line segment. Let’s imagine5
that the Onshape API exposed the midpoint as
an object part. Since the line is only deﬁned by
its endpoints (see Figure 5, lines 1 - 7) there is
no obvious candidate token that we could refer
to in the constraint deﬁnition. For that reason,
we introduce special referrable tokens decoupling
pointers from the object deﬁnition tokens. Re-
ferrables are injected after each entity and have
the same identiﬁer within each entity type. In or-
der to let the model distinguish between diﬀerent
entity parts, we add the index of the referrable to
the embedding of the token. Continuing with the
line segment example and assuming that the en-
tity has 4 parts, i.e., the line as a whole (part 1),
2 endpoints (parts 2 and 3), and the midpoint
(part 4), the token 𝑡𝑖 for the midpoint will have
the following embedding:

steps can no longer be processed in parallel dur-
ing training. Instead, we opt for a simpler solution
and employ the standard embedding scheme for
discrete tokens (Equation 4).

4.3. Sampling from the model

Sampling from the byte model is straightfor-
ward. The process is identical to any typical
Transformer-based LM. The triplet model, on the
other hand, requires slightly more bespoke han-
dling. Figure 2 illustrates the procedure. We
start by embedding and feeding a special BOS to-
ken into the Transformer. The Transformer then
outputs a collection of triplets, one for each pos-
sible token group. In order to determine which
concrete token needs to be emitted, we employ
an interpreter (a state machine) automatically
generated from the data speciﬁcation. Knowing
the current state allows us to choose the right
token group and associate the active component
of the triplet with a ﬁeld in the synthesized ob-
ject. Once the appropriate ﬁeld is populated the
interpreter transitions to the next state and pro-
duces an output token which is then fed back into
the model. The process stops when the state ma-
chine receives the “end” triplet for the outermost
repeated ﬁeld (i.e., object.kind ).

𝐾 [ line.ref ] + 𝑉ref [4] + 𝒆

pos
𝑖

,

(7)

4.4. Conditional generation

where 𝑉ref is a learnable vocabulary. As referrable
tokens do not need to be predicted the respective
terms are removed from Equation 1. For the same
reason, in Equation 6, 𝒉𝑖𝑘 corresponds to the time
step where 𝑡𝑖𝑘 is the output and not the input. This
allows us to avoid having unused Transformer
outputs.

Finally, we need to specify how we embed point-
ers as inputs to the Transformer network. Follow-
ing (Nash et al., 2020; Vinyals et al., 2015) we
could reuse 𝒉 𝑗 for tokens that point to 𝑡 𝑗. Unfor-
tunately, this creates output-to-input connections
which are extremely detrimental to the eﬃciency
of the Transformer architecture – diﬀerent time

5Hypothetically, since Onshape does not operate with
midpoints as object parts. A real example would involve
splines and their endpoints which are harder to use as an
illustration.

In addition to the model described above that
generates sketches from scratch, we explore a
modiﬁcation that allows generation of a sketch
based on its visual render or drawing. The condi-
tional model relies on the same transformer-based
architecture with an additional input sequence
𝐻img = [himg
] to which the transformer
can attend at every layer. We obtain 𝐻img by
embedding the conditioning image using visual
transformer (Dosovitskiy et al., 2020) which ex-
tracts patches of a certain size from the image
and processes them by a number of self-attention
layers.

, . . . , himg

1

𝑃

This scheme not only allowed us to conve-
niently contain the conditional model within the
same transformer framework, but also allowed
the generator to attend only to relevant parts of
the image when processing each of the objects.

10

Computer-Aided Design as Language

(a) Unconditional byte model.

(b) Unconditional triplet model.

(c) Conditional triplet model. Model predictions are rendered using a slightly thicker style.

Figure 6 | Random samples from various models. We use Nucleus Sampling with top-p = 0.9.

11

Computer-Aided Design as Language

This proved to be crucial for reconstructing ﬁner
details of the sketch and could not be achieved
with a more traditional single-vector representa-
tion of the image.

We train the conditional model in a manner

similar to (2):

𝜃∗ = arg max

𝜃

∑︁

𝑡 ∈D

log 𝑝(t|image(t); 𝜃),

(8)

where image(t) is a computer render for the to-
ken sequence t and 𝜃 now includes parameters
of the visual transformer too. Whenever a paral-
lel dataset is available consisting of sketches and
their drawings (e.g. human-drawn), the model
can be trained accordingly which we leave for
future work.

5. Experiments

We validate our proposed approaches on the data
that we obtained from the repository of docu-
ments publicly available on the Onshape platform
(Onshape developers). Following the standard
evaluation methodology for autoregressive gener-
ative models (Nash et al., 2020; Oord et al., 2016)
we use log-likelihood as our primary quantitative
metric. Additionally, we provide a variety of ran-
dom and selected model samples for qualitative
assessment.

5.1. Dataset

The data we rely on in the work originates from
the Onshape website. The website contains a
big collection of CAD models which can be used
freely for non-commercial purposes. Because of
this there have been several attempts to employ
Onshape as a source of research data (Koch et al.,
2019; Seﬀ et al., 2020). Unfortunately, in the con-
text of CAD sketches, all the available datasets
derived from the Onshape public repository seems
to have limitations. In particular, the Onshape
API makes it diﬃcult to obtain raw parameters
of sketch entities and constraints since they are
often deﬁned as symbolic expressions with vari-
ables. These variables may in turn correspond to
expressions that are speciﬁed elsewhere in the
document and so on. The simplest way to deal

Model

Sequence

Average bits per

object

sketch

Uniform unord. bytes

112.23 3683.56

unord. triplets

25.34

847.52

Byte

concatenated

4.394

133.281

interleaved

4.265

128.053

Triplet

concatenated

4.214

127.607

interleaved

4.092

122.818

Cond.

interleaved

2.620

79.979

Table 2 | Test likelihoods of various models.
The third column is computed as a mean num-
ber of bits per object in a sketch averaged across
test examples. The fourth column is similar except
we do not divide by the number of objects.

with this is to just ignore such sketches and only
consider instances relying on plain values. Prior
eﬀorts take this route and therefore discard a
signiﬁcant portion of the available data.

In order to collect a more complete dataset
of sketches, we performed a large-scale scrape
downloading around 6TB of public documents in
October, 2020. We only consider sketches con-
taining the most common entity and constraint
types (4 and 16 diﬀerent types respectively). Ad-
ditionally, we remove constraints that refer to
so-called “external” entities, i.e., elements of the
CAD model that exist outside the sketch plane.

We then preprocess the remaining data to re-
place symbolic expressions with plain values. To
that end, we ﬁrst employ the default conﬁguration
ﬁle to obtain the initial assignments of variables
and then scan through the document locating
further assignments and updating the variable
mapping. If an assignment contains a symbolic
expression we evaluate it using SymPy6 (Meurer
et al., 2017). Once the scan is ﬁnished we can
use plain values of the variables to resolve any
expressions in the sketch parameters.

CAD models in the public repository cover a

6This works most of the time but there are rare cases

when SymPy fails to parse Onshape’s syntax.

12

Computer-Aided Design as Language

Coincident

Diameter

Tangent

Horizontal

Midpoint

Coincident

Length

Horizontal

Parallel

Perpendicular

Vertical

Mirror

Diameter

Coincident

Diameter

Tangent

Equal

Vertical

Length

Parallel

Midpoint

Horizontal

Perpendicular

Distance

Mirror

Figure 7 | Entities and constraints sampled from the triplet model. The ﬁrst column of nodes
represents diﬀerent entities. The order of nodes (top to bottom) follows the generation order. The
second column represents diﬀerent constraints also ordered by their index in the sequence. Finally,
the third column is reserved for constraint types, from the most to the least frequent.

13

Computer-Aided Design as Language

Byte (1)

Byte (0.9)

Triplet (1)

Triplet (0.9)

Data

# objects

# entities

# constraints

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00

0

50

100

0

10

20

30

0

20

40

60

# regions

# arcs

# coincident

0.8

0.6

0.4

0.2

0.0

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.10

0.08

0.06

0.04

0.02

0.00

0.5

0.4

0.3

0.2

0.1

0.0

0

5

10

15

20

0

2

4

6

8

10

0

10

20

30

40

Figure 8 | Distribution of various sketch statistics for samples drawn from our unconditional models.
The top-p parameter for Nucleus Sampling is shown in parentheses.

wide range of objects in the real world, anything
from coﬀee mugs to sports cars. As a result,
sketches have signiﬁcant variability in size. To
make training of our models easier, we rescale ex-
amples in the dataset to lie in the [−1, 1] × [−1, 1]
bounding box.

Another topic that is somewhat overlooked in
(Koch et al., 2019; Seﬀ et al., 2020) is duplicate
data. As is the dataset ends up having many
sketches that are either single built-in primitives
or copied (sometimes with minor modiﬁcations)
from popular documents or tutorials. Not only
this restricts the diversity of samples generated by
machine learning models but also makes assess-
ment of such models diﬃcult – a random subset
chosen for testing is likely to signiﬁcantly overlap
with the training set.

We make a best-eﬀort attempt to address dupli-
cation by using the following ﬁltering procedure.
We ﬁrst remove simple axis-aligned rectangles
which constitute around ≈ 15% of the data. Next,
we divide the remaining examples into bins of
semantically equivalent sketches. Two sketches
are considered to be equivalent if they share the

same sequence of object types (e.g., a line followed
by a point followed by a coincidence constraint).
For more complex examples that have matching
sequences of types there is a good chance that
the examples themselves look very similar.

The resulting bins contain manageable num-
bers of sketches (up to several tens of thousands)
and therefore can undergo more computationally
expensive ﬁltering. Within each bin, we obtain
128 × 128 binary renders of each example and
apply hierarchical clustering7 with 0.1 threshold
and Jaccard distance as metric. The ﬁnal dataset
is formed by taking a single sketch per cluster
(unsurprisingly, sketches from the same cluster
end up looking almost identical).

For our experiments, we exclude the sketches
that have fewer than 4 or more than 100 enti-
ties. The dataset is split randomly into 3 parts:
4,671,037 examples for the training set and
50,000 sketches for each the validation and the
test set.

7We use cluster.hierarchy.fclusterdata from

SciPy.

14

Computer-Aided Design as Language

5.2. Training details

We train our models for 106 weight updates us-
ing batches with 128 lanes. Each lane can ﬁt
sequences up to 1024 tokens long in the triplet
setting and 1990 tokens long in the byte set-
ting. In order to improve occupancy and reduce
wasted computation we ﬁll up the lanes dynami-
cally packing as many examples as possible into
a lane before moving on to the next one. Each
batch is processed in parallel by 32 TPU cores.

We use the Adam optimizer with the learning
rate of 10−4 and clip the gradient norm to 1.0.
Additionally, we employ a dropout rate of 0.1 in
all the experiments.

5.3. Unconditional generation

In this series of experiments, the goal is to deter-
mine how well our models capture the distribu-
tion of sketches in the dataset. We use the same
network architecture for both the byte and the
triplet settings: embeddings and fully-connected
layers of size 384 and 24 Transformer blocks. For
the baseline, we employ a model that outputs
uniform distribution at every time step.

As we mentioned in Section 3.2, we have to
choose the ordering of objects in training se-
quences since both entities and constraints are
given to us as sets. We compare two regimes: in
the ﬁrst one (concatenated), constraint objects go
after the last entity while in the second (inter-
leaved), a constraint object is injected immedi-
ately after the entities it operates on.

Table 2 shows test log-likelihoods obtained by
diﬀerent models. Unsurprisingly, our proposed
methods (the last 4 rows) signiﬁcantly outper-
form the weak baselines. The diﬀerence between
the two uniform settings is due to the fact that the
byte description of a PB message is usually longer
than the triplet one: 239 vs. 456 tokens on aver-
age with the maximum length of 959 vs. 1987.
Additionally, the tokens in the triplet representa-
tion tend to have smaller cardinality (i.e., < 257).

These diﬀerences between representations may
partially explain why triplet models demonstrate
better performance on the hold-out test set. It’s
also worth emphasizing that the byte model does

not receive any explicit information about the
parsing state. This seem to make learning more
challenging and as a result compared to the triplet
model it takes roughly 3 times more network
updates to reach the highest data likelihood on
the validation.

We tried making byte sequences shorter by feed-
ing PB messages into a general purpose compres-
sion algorithm (Brotli (Alakuijala et al., 2018)
with the quality setting of 7). This allowed us
to reduce the maximum length to 930 tokens,
which is very similar to that of the triplet setting.
Unfortunately, this change renders learning im-
possible – a model of the same capacity as before
fails to improve beyond the initial reduction of the
training objective. We suspect that this happens
because the compression algorithm aggressively
mangles the contents of a PB message and thus
it is too diﬃcult for the model to make sense of
the data.

Another important factor aﬀecting the perfor-
mance of the models is the choice of the object
ordering. As it is evident from Table 2, the in-
terleaved ordering consistently leads to better re-
sults. One explanation for this is that at any point
in time, the model has more explicit information
about the relations between the sketch entities
produced so far. This early injection of the design
intent seems to make it easier to decide what to
generate next.

In addition to measuring likelihoods, we sam-
pled 10,000 sketches from the best performing
byte and triplet models and computed distribu-
tions of various high-level statistics (e.g., number
of objects, number of closed regions and num-
ber of coincident constraints). We repeated this
procedure both with and without using Nucleus
Sampling (Holtzman et al., 2019). Figure 8 (as
well as Figure 11 in the appendix) reveals that
both models follow the data distribution closely
when we use samples from the unmodiﬁed model
output. In this setting, however, a signiﬁcant frac-
tion of sketches is either malformed (e.g., the
generated PB message cannot be parsed) or un-
solvable: 36% for the byte model and 14% for the
triplet model. Nucleus Sampling with top-p = 0.9
skews the sample distribution and seems to have
a more pronounced negative eﬀect on the byte

15

Computer-Aided Design as Language

Figure 9 | Examples of solved sketches. Our
models predict both entities and constraints and
thus we can run samples through a dedicated
solver that adjusts the parameters of the entities
to respect the constraints. This process can ﬁx im-
perfections in the initial conﬁguration (rendered
in black).

model. The upside is that the resulting sketches
become “cleaner”: the percentage of invalid sam-
ples goes down to 25% and 6% for the byte and
the triplet settings respectively.

Figures 6a and 6b show renders of random
samples from several proposed models. Overall,
generated sketches look plausible and exhibit a
lot of desired properties: closedness of regions,
regularity, symmetry, a non-trivial amount of ﬁne
detail. We observe that the byte model tends to
produce slightly less complex samples with fewer
open arcs but this could be a side eﬀect of apply-
ing Nucleus Sampling with the chosen parame-
ter. We also note that the model does not always
synthesize sensible sketches – just like any other
typical autoregressive model trained with teacher
forcing it suﬀers from not being able to recover
from mistakes made early on in the sequence.
This can potentially be addressed by ﬁne-tuning
using, for example, reinforcement learning.

In order to make visualization of sketch con-
straints more manageable, we show several addi-
tional scaled up samples from the triplet model
in Figure 7.8 While not perfect, the inferred con-
straints are reasonable most of the time. The
model does a good job at connecting entities us-
ing Coincident constraints (perhaps unsurpris-
ingly since it’s the most common type in the data)

8Here, we do not distinguish between diﬀerent parts of
an entity and fold all of them into a single node in the ﬁrst
column.

but also successfully detects more complex rela-
tions spanning more than two primitives (e.g.,
Mirror). Having access to constraints gives us
opportunity to correct mistakes in entity predic-
tion by applying an external sketch solver (see
Figure 9). Although this aspect of sketching was
not the main focus of this work, we believe that
a tighter integration between the model and the
CAD software will lead to a signiﬁcant boost in
generation quality.

5.4. Conditional generation

As discussed in section 4.4, we also trained an
image-conditional model using the same regime
as for the main model. We used 16 and 22 lay-
ers for the visual and triplet transformers corre-
spondingly, both operating with 384-dimensional
embeddings. We employed 256 × 256 sized ren-
ders and 16 × 16 image patches resulting in total
𝑃 = 256 conditioning vectors himg

.

𝑝

As can be seen in table 2, the conditional model
(i.e., Cond.) achieves a signiﬁcantly better ﬁt than
the unconditional variant. One can notice, that
the model has still retained a non-trivial amount
of uncertainty arising from the fact there can
be many diﬀerent token sequences resulting in
the same image (e.g. due to diﬀerent permu-
tations), so the reverse transformation is non-
deterministic.

Image-conditional samples can be found on
Figure 6c. The model was able to nearly perfectly
reconstruct simpler sketches and mostly made
mistakes in the presence of a large number of
ﬁne details.

We were also interested if

the image-
conditional model has learned a meaningful algo-
rithm for CAD conversion that would generalize to
images that are implementable within our sketch
vocabulary but come from a diﬀerent distribution
of objects. Thus, we drew a smiley face in On-
shape and processed the image using our model.
We discovered that the model is able to very sensi-
bly convert that image, but unfortunately decides
to stop the generation process too early due to
the fact that the majority of training sequences
contained much smaller number of examples than
used in the smiley face.

16

Computer-Aided Design as Language

Coincident

Length

Equal

Midpoint

Horizontal

Distance

Vertical

Diameter

Mirror

Coincident

Horizontal

Perpendicular

Vertical

Parallel

Figure 10 | Entities and constraints inferred by the conditional model. Input bitmaps are shown
in the lower left corner. The bottom example demonstrates the model’s performance on an out-of-
distribution input. See Figure 7 for further explanation.

To overcome this arguably insigniﬁcant limi-
tation we enforced the interpreter to continue
generating tokens until the limit of 1024 tokens
is reached. We then selected the longest sub-
sequence from 64 sampled sequences that min-
imized the 𝐿2 diﬀerence between the condition-
ing image and rendered result (after Gaussian
smoothing with 𝜎 = 4), which can be seen as
a form of model-guided search procedure. The
resulting nearly perfect sketch can be seen on
Figure 10.

One can note than even though the model did
not place the arcs from which the eye-glasses

are built very precisely, it still supplemented
them with constraints that make sure their end-
points coincide. Besides this, mouth-forming lines
drawn as almost parallel in the original sketch
simply by the chance, were recognized as parallel
in the provided sample with the corresponding
constraints which is an example of good under-
standing of relationships between objects by the
model.

17

Computer-Aided Design as Language

6. Discussion

In this work, we have demonstrated how a com-
bination of a general-purpose language modeling
technique alongside an oﬀ-the-shelf data serial-
ization protocol can be used to eﬀectively solve
generation of complex structured objects. We
showcased the proposed system on the domain
of 2D CAD drawings and developed models that
can synthesize geometric primitives and relations
between them both unconditionally as well as
using a bitmap as a reference. These are only
initial proof-of-concept experiments and we are
hoping to see more applications taking advan-
tage of the ﬂexibility of the developed interface:
conditioning on various sketch properties, infer-
ring constraints given entities and automatically
completing drawings, to name a few.

Although we focused our attention on a particu-
lar dataset we argue that the approach described
in this paper is largely domain-agnostic. In or-
der to adapt the system to a new kind of data,
the algorithm designer only needs to provide an
appropriate Protocol Buﬀer speciﬁcation and if
the PB language is too restrictive one can always
replace it with a more powerful interpreter. As a
straightforward direction for future work, we can
consider extending the method to handle 3D. In
Onshape, 3D operations bear a lot of similarities
with sketch constraints – just like constraints, they
can be represented as nested messages contain-
ing references to the geometries existing in the
scene (e.g., an extrusion operation refers to a 2D
proﬁle to be extruded). Thus, most of the ideas
from the present work can be taken verbatim to
this new setting.

When it comes to model architecture, there
are a few ways in which we can potentially im-
prove it. For example, we can enhance the input
fed into the Transformer at every time step by
incorporating a visual representation of a par-
tially constructed sketch. There is evidence that
this may lead to better performance (Ganin et al.,
2018) as the model gains more direct access to the
holistic state of generation. Another improvement
we brieﬂy mentioned in Section 5.3 is tighter in-
teraction between the model and the CAD soft-
ware which makes ultimate use of the synthesized

data. For instance, currently, our training proce-
dure does not take into account how the sketch
solver would react to adding more entities and
constraints. The model is simply trying to imi-
tate valid examples from the dataset. As a result,
the model’s capacity is not always utilized eﬃ-
ciently: a lot of it may be allocated on capturing
the minutiae of sketch parameters with more im-
portant properties like solvability and stability
being overlooked. Therefore it makes sense to
introduce training objectives that are directly cor-
related with the desired behaviour of the system.
This brings us to the territory of reinforcement
learning with rich feedback from the software en-
vironment, a setting which is much closer to how
sketching is done by human designers.

We hope that this work will serve as a step-
ping stone for further advances in the ﬁeld of
automated CAD but also inspire new ideas and
approaches to generative modeling of arbitrary
structured data.

References

J. Alakuijala, A. Farruggia, P. Ferragina, E. Kli-
uchnikov, R. Obryk, Z. Szabadka, and L. Van-
devenne. Brotli: A general-purpose data com-
pressor. TOIS, 2018.

H. K. Ault. Using geometric constraints to capture

design intent. JGG, 1999.

B. R. Barbero, C. M. Pedrosa, and R. Z. Samperio.
Learning CAD at university through summaries
of the rules of design intent. ITDE, 2017.

Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
A neural probabilistic language model. JMLR,
2003.

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language mod-
arXiv preprint
els are few-shot learners.
arXiv:2005.14165, 2020.

A. Carlier, M. Danelljan, A. Alahi, and R. Timofte.
DeepSVG: A hierarchical generative network
for vector graphics animation. arXiv preprint
arXiv:2007.11301, 2020.

18

Computer-Aided Design as Language

Under the hood: Onshape
https://www.onshape.com/

P. Chastell.
sketches.
en/resource-center/articles/
under-the-hood-onshape-sketches,
2015. Accessed: 2021-03-10.

P. Company, F. Naya, M. Contero, and J. Camba.
On the role of geometric constraints to sup-
port design intent communication and model
reusability. Computer-Aided Design and Appli-
cations, 2019.

P. Dhariwal, H. Jun, C. Payne, J. W. Kim,
A. Radford, and I. Sutskever.
Jukebox: A
generative model for music. arXiv preprint
arXiv:2005.00341, 2020.

D. Dori and K. Tombre. From engineering draw-
ings to 3d CAD models: are we ready now?
Computer-Aided Design, 1995.

D. Dori and L. Wenyin. Automated cad conversion
with the machine drawing understanding sys-
tem: concepts, algorithms, and performance.
SMC, 1999.

A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-
senborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, et al. An
image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.

V. Egiazarian, O. Voynov, A. Artemov, D. Volkhon-
skiy, A. Saﬁn, M. Taktasheva, D. Zorin, and
E. Burnaev. Deep vectorization of technical
drawings. In ECCV, 2020.

Y. Ganin, T. Kulkarni, I. Babuschkin, S. A. Eslami,
and O. Vinyals. Synthesizing programs for im-
ages using reinforced adversarial learning. In
ICML, 2018.

N. W. Hartman. Deﬁning expertise in the use of
constraint-based cad tools by examining prac-
ticing professionals. EGDJ, 2005.

A. Holtzman, J. Buys, L. Du, M. Forbes, and
Y. Choi. The curious case of neural text de-
generation. arXiv preprint arXiv:1904.09751,
2019.

J. Jimenez and J. L. Navalon. Some experiments
in image vectorization. IBM Journal of research
and Development, 1982.

D. P. Kingma and M. Welling. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114,
2013.

S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Arte-
mov, E. Burnaev, M. Alexa, D. Zorin, and
D. Panozzo. ABC: A big CAD model dataset
for geometric deep learning. In CVPR, 2019.

S. Kyratzi and P. Azariadis. A constraint-based
framework to recognize design intent dur-
ing sketching in parametric environments.
Computer-Aided Design and Applications, 2020.

T.-M. Li, M. Lukáč, M. Gharbi, and J. Ragan-Kelley.
Diﬀerentiable vector graphics rasterization for
editing and learning. SIGGRAPH Asia, 2020.

A. Meurer, C. P. Smith, M. Paprocki, O. Čertík,
S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov,
J. K. Moore, S. Singh, et al. SymPy: symbolic
computing in Python. PeerJ Computer Science,
2017.

T. Mikolov, M. Karaﬁát, L. Burget, J. Černock`y,
and S. Khudanpur. Recurrent neural network
In INTERSPEECH,
based language model.
2010.

J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals,
and G. E. Dahl. Neural message passing for
quantum chemistry. In ICML, 2017.

V. Nagasamy and N. A. Langrana. Engineering
drawing processing and vectorization system.
CVGIP, 1990.

A. Graves.

Generating sequences with re-
arXiv preprint

current neural networks.
arXiv:1308.0850, 2013.

D. Ha and D. Eck.

tion of sketch drawings.
arXiv:1704.03477, 2017.

A neural representa-
arXiv preprint

C. Nash, Y. Ganin, S. A. Eslami, and P. Battaglia.
PolyGen: An autoregressive generative model
of 3D meshes. In ICML, 2020.

Onshape developers. Onshape website. https:
//www.onshape.com/. Accessed: 2021-03-
10.

19

Computer-Aided Design as Language

A. v. d. Oord, N. Kalchbrenner, O. Vinyals, L. Es-
peholt, A. Graves, and K. Kavukcuoglu. Con-
ditional image generation with PixelCNN de-
coders. arXiv preprint arXiv:1606.05328, 2016.

F. Pezoa, J. L. Reutter, F. Suarez, M. Ugarte, and
D. Vrgoč. Foundations of JSON schema.
In
WWW, 2016.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
and I. Sutskever. Language models are unsu-
pervised multitask learners. OpenAI blog, 2019.

A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss,
A. Radford, M. Chen, and I. Sutskever. Zero-
shot text-to-image generation. arXiv preprint
arXiv:2102.12092, 2021.

A. Razavi, A. v. d. Oord, and O. Vinyals. Generat-
ing diverse high-ﬁdelity images with VQ-VAE-2.
arXiv preprint arXiv:1906.00446, 2019.

P. Reddy, M. Gharbi, M. Lukac, and N. J. Mi-
Im2Vec: Synthesizing vector graphics
arXiv preprint

tra.
without vector supervision.
arXiv:2102.02798, 2021.

A. Rynne and W. Gaughran. Cognitive modeling
strategies for optimum design intent in para-
metric modeling. Computers in Education Jour-
nal, 2008.

A. Seﬀ, Y. Ovadia, W. Zhou, and R. P. Adams.
Sketchgraphs: A large-scale dataset for mod-
eling relational geometry in computer-aided
design. arXiv preprint arXiv:2007.08506, 2020.

K. Varda. Google protocol buﬀers: Google’s data
interchange format. Technical report, 2008.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-
sukhin. Attention is all you need. arXiv preprint
arXiv:1706.03762, 2017.

P. Vaxiviere and K. Tombre. Celesstin: CAD con-
version of mechanical drawings. Computer,
1992.

T. Veuskens, F. Heller, and R. Ramakers. CODA:
A design assistant to facilitate specifying con-
straints and parametric behavior in CAD mod-
els. 2020.

O. Vinyals, M. Fortunato, and N. Jaitly. Pointer

networks. In NeurIPS, 2015.

K. D. Willis, Y. Pu, J. Luo, H. Chu, T. Du, J. G.
Lambourne, A. Solar-Lezama, and W. Matusik.
Fusion 360 gallery: A dataset and environment
for programmatic CAD reconstruction. arXiv
preprint arXiv:2010.02392, 2020.

J.-C. Wu. A study of the learning models em-
ployed by industrial design students when
learning to use 3D computer-aided design
(CAD) software. International Journal of Arts
Education, 2009.

Acknowledgements

The authors would like to thank Charlie Nash and
Georg Ostrovski for helping with the manuscript
preparation as well as Igor Babuschkin, David
Choi, Nate Kushman, Andrew Kimpton, Jake
Rosenfeld, Lana Saksonov, John Rousseau, Greg
Guarriello, Andy Brock, Francesco Nori, Aäron
van den Oord and Oriol Vinyals for insightful
discussions and support.

Author Contributions

YG led the project, designed the data layout and
the triplet model, collected the dataset, imple-
mented and conducted the bulk of the experi-
ments, wrote the core of the manuscript. SB im-
plemented and evaluated the conditional model.
YL contributed the byte model. EK provided tech-
nical support. YG, SB, YL, EK and SS worked on
editing the paper.

20

Computer-Aided Design as Language

A. Object speciﬁcations

This section contains additional details on the ob-
ject speciﬁcations. As mentioned in Section 3.2,
we rely on the PB language to deﬁne the structure
for each object type that we would like to handle
with our model. Our framework supports all basic
constructions on the language including nested
messages, repeated ﬁelds and oneof clauses.
The latter is especially useful when a particular
object may appear in several mutually exclusive
conﬁgurations: for example, CircleArcEntity
represents both arcs and closed circles (for which
it does not make sense to specify end points).
Sometimes the conﬁguration is determined by a
group of preceding ﬁeld values. In that case, the
oneof branch is chosen by a special handler that
observes the ﬁelds populated so far. This handler
is passed as an option to the construction (see
DistanceConstraint deﬁnition below). Op-
tions provide a convenient way to extend the lan-
guage and make it better suited for a domain of
interest. For example, we use the at_least ﬁeld
option to indicate the minimum number of items
in a repeated ﬁeld.

Below we are listing all the supported object
speciﬁcations. Although most of them are direct
adaptations of the JSON structures returned by
the Onshape API we made an eﬀort to remove
redundant and irrelevant sections. We also at-
tempted to bring our constraint speciﬁcations
closer to how they behave in the software UI.
For instance, the coincident constraint can usu-
ally be applied to several entities at once. In the
API, however, this is translated into a collection of
pairwise constraints. In our dataset, we compress
this collection back into a single object.

We start with the sketch entities:

message Vector {
double x = 1;
double y = 2;

}

message PointEntity {

bool is_construction = 1;
Vector point = 2;

}

message LineEntity {

bool is_construction = 1;
Vector start = 2;
Vector end = 3;

}

message CircleArcEntity {

bool is_construction = 1;
Vector center = 2;
message CircleParams {
double radius = 1;

}
message ArcParams {
Vector start = 1;
Vector end = 2;
bool is_clockwise = 3;

}
oneof additional_params {

CircleParams circle_params = 3;
ArcParams arc_params = 4;

}

}

message InterpolatedSplineEntity {

bool is_construction = 1;
bool is_periodic = 2;
repeated Vector interp_points = 3

[(field_options).at_least = 2];

Vector start_derivative = 4;
Vector end_derivative = 5;
message TrimmedParams {
double start_phi = 1;
double end_phi = 2;

}
oneof additional_params {

Empty untrimmed_params = 6;
TrimmedParams trimmed_params = 7;

}

}

The constraints are speciﬁed are as follows:

message FixConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 1];

}

message CoincidentConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 2];

}

message ConcentricConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 2];

}

21

Computer-Aided Design as Language

message EqualConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 2];

}

message ParallelConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 2];

}

message TangentConstraint {

Pointer first = 1;
Pointer second = 2;

}

oneof additional_params {

option (oneof_options).handler =
"select_distance_params";

// The actual definition of the
// handler is shown further in
// the text.

Alignment alignment = 5;
HalfSpaceParams

half_space_params = 6;

}

}

message LengthConstraint {

Pointer entity = 1;
double length = 2;

message PerpendicularConstraint {

}

Pointer first = 1;
Pointer second = 2;

}

message DiameterConstraint {

Pointer entity = 1;
double length = 2;

message MirrorConstraint {

}

Pointer mirror = 1;
message Pair {

Pointer first = 1;
Pointer second = 2;

message RadiusConstraint {

Pointer entity = 1;
double length = 2;

}
repeated Pair mirrored_pairs = 2

}

[(field_options).at_least = 1];

message AngleConstraint {

}

message DistanceConstraint {

Pointer first = 1;
Pointer second = 2;
enum Direction {

HORIZONTAL = 0;
VERTICAL = 1;
MINIMUM = 2;

}
Direction direction = 3;
double length = 4;
enum Alignment {
ALIGNED = 0;
ANTI_ALIGNED = 1;

}
enum HalfSpace {

NOT_AVAILABLE = 0;
LEFT = 1;
RIGHT = 2;

}
message HalfSpaceParams {

HalfSpace half_space_first = 1;
HalfSpace half_space_second = 2;

}

Pointer first = 1;
Pointer second = 2;
double angle = 3;

}

message HorizontalConstraint {

repeated Pointer entities = 1

[(field_options).at_least = 1];

}

message VerticalConstraint {

repeated Pointer entities = 2

[(field_options).at_least = 1];

}

message MidpointConstraint {

Pointer midpoint = 1;
message Endpoints {

Pointer first = 1;
Pointer second = 2;

}
oneof additional_params {

Endpoints endpoints = 2;
Pointer entity = 3;

}

}

22

Discrete tokens

Computer-Aided Design as Language

Regular expression Range

objects\.kind

{0, 1}

.*\.entity.kind

{0, 1, 2, 3}

.*\.constraint.kind

{0, . . . , 15}

.*\.is_construction

.*\.is_clockwise

.*\.is_periodic

.*\.additional_params

{0, 1}

{0, 1}

{0, 1}

{0, 1}

.*\.(entity|entities|first|second|midpoint|mirror)

{0, . . . , 255}

.*\.direction

{0, 1, 2}

.*\.alignment

{0, 1}

.*\.half_space_(first|second)

{0, 1, 2}

Continuous tokens

.*\.(point|start|end|center|interpolation_points)\.(x|y)

[−1.0, 1.0]

.*\.(start|end)_derivative\.(x|y)

[−100.0, 100.0]

.*\.(start|end)_phi

[0.0, 3.0]

.*\.radius

.*\.length

[0.0, 1.0]
√

[0.0, 2

2]

Table 3 | Token groups used in our experiments. The left column contains python-style regular
expressions isolating speciﬁc subsets of tokens (i.e., dotted paths of the corresponding ﬁelds). The
right column shows the ranges of values for each group.

.*\.angle

[−10.0, 10.0]

The handler in DistanceConstraint is de-

ﬁned as a python function:
def select_distance_params(obj):

if obj.direction in {"HORIZONTAL",

"VERTICAL"}:

return 0

else:

return 1

B. Token groups

Table 3 details the speciﬁc grouping of tokens
used in our experiments. Groups are deﬁned by
regular expressions matching subsets of dotted

ﬁeld identiﬁers. For each group we provide the
range of possible values.

C. Additional statistics

We present the distributions of a variety of addi-
tional sketch statistics in Figure 11. These results
are discussed in Section 5.3.

23

Computer-Aided Design as Language

Byte (1)

Byte (0.9)

Triplet (1)

Triplet (0.9)

Data

# points

# lines

# circular

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.6
0.5
0.4

0.3

0.2

0.1

0.0

0

5

10

0

10

20

30

0

5

10

15

# splines

# mirror

# distance

1.0

0.8

0.6

0.4

0.2

0.0

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

0.5

1

1.5

2

0

0.5

1

1.5

2

0

5

10

# horizontal

# vertical

# parallel

0

2

4

# perpendicular

0.6
0.5
0.4

0.3
0.2
0.1
0.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0

2

4

# tangent

0.5

0.4

0.3

0.2

0.1

0.0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0

2

4

6

8

10

# midpoint

0

2

4

6

0

5

10

0

2

4

6

# length

# radius

# equal

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0.5

0.4

0.3

0.2

0.1

0.0

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

6

8

0

1

2

3

4

0

2

4

Figure 11 | Distribution of additional sketch statistics. See also Figure 8.

24

