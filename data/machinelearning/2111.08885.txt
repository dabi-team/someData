1
2
0
2

v
o
N
7
1

]
E
M

.
t
a
t
s
[

1
v
5
8
8
8
0
.
1
1
1
2
:
v
i
X
r
a

Jump Interval-Learning for Individualized
Decision Making

Hengrui Cai ∗ † 1, Chengchun Shi ∗ ‡ 2, Rui Song§1, and Wenbin Lu¶1

1Department of Statistics, North Carolina State University
2Department of Statistics, London School of Economics and Political Science

Abstract

An individualized decision rule (IDR) is a decision function that assigns each
individual a given treatment based on his/her observed characteristics. Most of the
existing works in the literature consider settings with binary or ﬁnitely many treatment
options. In this paper, we focus on the continuous treatment setting and propose a
jump interval-learning to develop an individualized interval-valued decision rule (I2DR)
that maximizes the expected outcome. Unlike IDRs that recommend a single treatment,
the proposed I2DR yields an interval of treatment options for each individual, making
it more ﬂexible to implement in practice. To derive an optimal I2DR, our jump interval-
learning method estimates the conditional mean of the outcome given the treatment
and the covariates via jump penalized regression, and derives the corresponding optimal
I2DR based on the estimated outcome regression function. The regressor is allowed
to be either linear for clear interpretation or deep neural network to model complex
treatment-covariates interactions. To implement jump interval-learning, we develop
a searching algorithm based on dynamic programming that eﬃciently computes the
outcome regression function. Statistical properties of the resulting I2DR are established
when the outcome regression function is either a piecewise or continuous function over
the treatment space. We further develop a procedure to infer the mean outcome under
the (estimated) optimal policy. Extensive simulations and a real data application to a
warfarin study are conducted to demonstrate the empirical validity of the proposed
I2DR.

∗Equal contribution.
†hcai5@ncsu.edu
‡C.Shi7@lse.ac.uk
§rsong@ncsu.edu
¶wlu4@ncsu.edu

1

 
 
 
 
 
 
1

Introduction

Individualized decision making is an increasingly attractive artiﬁcial intelligence paradigm

that proposes to assign each individual a given treatment based on their observed character-

istics. In particular, such a paradigm has been recently employed in precision medicine to

tailor individualized treatment decision rule. Among all individualized decision rules (IDR),

the one that maximizes the expected outcome is referred to as an optimal IDR. There is

a huge literature on learning the optimal decision rule. Some popular methods include

Q-learning (Watkins and Dayan, 1992; Chakraborty et al., 2010; Qian and Murphy, 2011;

Song et al., 2015), A-learning (Robins, 2004; Murphy, 2003; Shi et al., 2018), policy search

methods (Zhang et al., 2012, 2013; Wang et al., 2018; Nie et al., 2020), outcome weighted

learning (Zhao et al., 2012, 2015; Zhu et al., 2017; Meng et al., 2020), concordance-assisted

learning Fan et al. (2017); Liang et al. (2017), decision list-based methods (Zhang et al.,

2015, 2018), and direct learning (Qi et al., 2020). We note however, all these methods

consider settings where the number of available treatment options is ﬁnite.

In this paper, we consider individualized decision making in continuous treatment

settings. These studies occur in a number of real applications, including personalized dose

ﬁnding (Chen et al., 2016) and dynamic pricing (den Boer and Keskin, 2020). For instance,

in personalized dose ﬁnding, one wishes to derive a dose level or dose range for each patient.

Due to patients’ heterogeneity in response to doses, it is commonly assumed that there may

not exist a uniﬁed best dose for all patients. Thus, one major interest in precision medicine

is to develop an IDR that assigns each individual patient with a certain dose level or a

speciﬁed range of doses based on their individual personal information, to optimize their

health status. Similarly, in dynamic pricing, we aim to identify an IDR that assigns each

product an optimal price according to their characteristics to maximize the overall proﬁt.

In contrast to developing the optimal IDR under discrete treatment settings, individual-

ized decision making with a continuous treatment domain has been less studied. Among

those available, Rich et al. (2014) modeled the interactions between the dose level and

covariates to recommend personalized dosing strategies. Laber and Zhao (2015) developed

a tree-based method to derive the IDR by dividing patients into subgroups and assigning

2

each subgroup the same dose level. Chen et al. (2016) proposed an outcome weighted

learning method to directly search the optimal IDR among a restricted class of IDRs. Zhou

et al. (2018) proposed a dimension reduction framework to personalized dose ﬁnding that

eﬀectively reduces the dimensionality of baseline characteristics from high to a moderate

scale. Kallus and Zhou (2018) and Chernozhukov et al. (2019) evaluated and optimized

IDRs for continuous treatments by replacing the indicator function in the doubly-robust

approach with the kernel function, and by modeling the conditional mean outcome function

(i.e., the value) through a semi-parametric form, respectively. Zhu et al. (2020) focused on

the class of linear IDRs and proposed to compute an optimal linear IDR by maximizing a

kernel-based value estimate. Schulz and Moodie (2020) proposed a doubly robust estimation

method for personalized dose ﬁnding. The estimated optimal IDRs computed by these

methods typically recommend one single treatment level for each individual, making it hard

to implement in practice.

The focus of this paper is to develop an individualized interval-valued decision rule

(I2DR) that returns a range of treatment levels based on individuals’ baseline information.

Compared to the IDRs recommended by the existing works, the proposed I2DR gives

more options and is thus more ﬂexible to implement in practice. Take personalized dose

ﬁnding as an illustration. First, interval-valued dose levels may be applied to patients of

the same characteristics, when arbitrary dose within the given dose interval could achieve

the same eﬃcacy. Studies of the pharmacokinetics of vancomycin conducted by Rotschafer

et al. (1982) suggested that adults with normal renal function should receive an initial

dosage of 6.5 to 8 milligram of vancomycin per kilogram intravenously over 1 hour every

6 to 12 hours. In the review of warfarin dosing reported by Kuruvilla and Gurk-Turner

(2001), when the international normalized ratio (INR) approaches the target range or omit

dose, they suggested to give 1-2.5 milligram vitamin K1 if a patient has a risk factor for

bleeding, otherwise provide Vitamin K1 2-4 milligram orally. Second, a range of doses gives

instructions for designing the medicine speciﬁcation and helps to save cost on manufacturing

dosage. As such, an I2DR is preferred in these applications due to its necessity and ﬂexibility.

Our contributions are summarized as follows. Scientiﬁcally, individualized decision

making in a continuous treatment domain is a vital problem in many applications such as

3

precision medicine and dynamic pricing. To the best of our knowledge, this is the ﬁrst work

on developing individualized interval-valued decision rules. Our proposal thus ﬁlls a crucial

gap, extends the scope of existing methods that focus on recommending IDRs, and oﬀers a

useful tool for individualized decision making in a number of applications.

Methodologically, we propose a novel jump interval-learning (JIL) by integrating person-

alized decision making with multi-scale change point detection (see Niu et al., 2016, for a

selective overview). Our proposal makes useful contributions to the two aforementioned

areas simultaneously.

First, to implement personalized decision making, we propose a data-driven I2DR in

a continuous treatment domain. Our proposal is motivated by the empirical ﬁnding that

the expected outcome can be a piecewise function on the treatment domain in various

applications. Speciﬁcally, in dynamic pricing (Qiang and Bayati, 2016; den Boer and Keskin,

2020), the expected demand for a product has jump discontinuities as a function of the

charged price. This motivates us to impose a piecewise-function model for the expected

outcome. We then leverage ideas from the change point detection literature and propose

a jump-penalized regression to estimate the conditional mean of the expected outcome

as a function of the treatment level and the baseline characteristics (outcome regression

function). This partitions the entire treatment space into several subintervals. The proposed

I2DR is a set of decision rules that assign each subject to one of these subintervals. In

addition, we further develop a procedure to construct a conﬁdence interval (CI) for the

expected outcome under the proposed I2DR and the optimal IDR.

Second, we note that most works in the multi-scale change point detection literature

either focused on models without covariates, or required the underlying truth to be piecewise

constant (see e.g., Boysen et al., 2009; Frick et al., 2014; Fryzlewicz, 2014, and the references

therein). Our work goes beyond those cited above in that we consider a more complicated

(nonparametric) model with covariates, and allow the underlying outcome regression function

to be either a piecewise or continuous function over the treatment space. To approximate

the expected outcome as a function of baseline covariates, we propose a linear function

model and a deep neural networks model. We refer to the two procedures as L-JIL and

D-JIL, respectively. Here, the proposed L-JIL yields a set of linear decision rules that

4

is easy to interpret. See the real data analysis in Section 6 for details. On the contrary,

the proposed D-JIL employs deep learning (LeCun et al., 2015) to model the complicated

outcome-covariates relationships that often occur in high-dimensional settings. We remark

that both procedures are developed by imposing a piecewise-function model to approximate

the outcome-treatment relationship. Yet, they are valid when the expected outcome is a

continuous function of the treatment level as well.

Theoretically, we systematically study the statistical properties of the jump-penalized

estimators with linear regression or deep neural networks. Our theoretical approaches can

be applied to the analysis of general covariate-based change point models. The model

could be either parametric or nonparametric. Speciﬁcally, we establish the almost sure

convergence rates of our estimators. When the underlying outcome regression function is a

piecewise function of the treatment, we further derive the almost sure convergence rates

of the estimated change point locations, and show that with probability 1, the number of

change points can be correctly estimated with suﬃciently large sample size. These ﬁndings

are nontrivial extensions of classical results derived for models without covariates. For

instance, deriving the asymptotic behavior of change point estimators for these models

typically relies on the tail inequalities for the partial sum process (see e.g., Frick et al.,

2014). However, these technical tools are not directly applicable to our settings where deep

learning is adopted to model the outcome regression function. Moreover, we expect our

theories to also be of general interest to the line of work on developing theories for deep

learning methods (see e.g., Imaizumi and Fukumizu, 2019; Schmidt-Hieber et al., 2020;

Farrell et al., 2021).

The rest of this paper is organized as follows. In Section 2, we introduce the statistical

framework, deﬁne the notion of I2DR, and posit our working model assumptions. In Section

3, we propose the jump interval-learning method and discuss its detailed implementation.

Statistical properties of the proposed I2DR and the estimator for the mean outcome under

the proposed I2DR are presented in Section 4. We further develop a conﬁdence interval

for the expected outcome under the estimated I2DR. Simulation studies are conducted in

Section 5 to evaluate the ﬁnite sample performance of our proposed method. We apply our

method to a real dataset from a warfarin study in Section 6. In Section 7, we provide the

5

technical proof for one of our main theorem, followed by a concluding discussion in Section

8. The rest of proofs are provided in the supplementary article.

2 Statistical Framework

This section is organized as follows. We ﬁrst introduce the model setup in Section 2.1. The

deﬁnition of I2DR is formally presented in Section 2.2. In Section 2.3, we posit two working

models assumptions for the expected outcome as a function the treatment level. We aim to

develop a method that works under both working assumptions.

2.1 Model Setup

We begin with some notations. Let A denote the treatment level assigned to a randomly

selected individual in the population from a compact interval. Without loss of generality,
suppose A belongs to [0, 1]. Let X ∈ X be that individual’s baseline covariates where the
support X is a subset in Rp. We assume the covariance matrix of X is positive deﬁnite. Let
Y ∈ R denote that individual’s associated outcome, the larger the better by convention.

Let p(•|x) denote the probability density function of A given X = x. In addition, for any

a ∈ [0, 1], deﬁne the potential outcome Y ∗(a) as the outcome of that individual that would

have been observed if they were receiving treatment a. The observed data consists of the

covariate-treatment-outcome triplets {(Xi, Ai, Yi) : i = 1, . . . , n} where (Xi, Ai, Yi)’s are

i.i.d. copies of (X, A, Y ). Based on this data, we wish to learn an optimal decision rule to

possibly maximize the expected outcome of future subjects using their baseline information.

Formally speaking, an individualized decision rule (IDR) is a deterministic function d(·)
that maps the covariate space X to the treatment space [0, 1]. The optimal IDR is deﬁned

to maximize the expected outcome (value function) V (d) = E{Y ∗(d(X))} among all IDRs.

The following assumptions guarantee the optimal IDR is identiﬁable from the observed

data.

(A1.) Stable Unit Treatment Value Assumption (SUTVA): Y = Y ∗(A), almost surely,

(A2.) No unmeasured confounders: {Y ∗(a) : a ∈ [0, 1]} ⊥⊥ A | X,
(A3.) Positivity: there exists some constant c∗ > 0 such that p(a|x) ≥ c∗ for any x ∈ X and

6

a ∈ [0, 1].

SUTVA requires that the outcome of each individual depends on their own treatment only.

In other words, there is no interference eﬀect between individuals. Assumption (A2) requires

that the baseline covariates have included enough confounders given which the potential

outcomes and the received treatment are independent.

(A2) and (A3) automatically

hold in randomized studies. These three assumptions are commonly imposed in the

literature for estimating an optimal IDR (see e.g., Chen et al., 2016; Zhu et al., 2020;

Schulz and Moodie, 2020). Under (A1)-(A3), we have V (d) = E{Q(X, d(X))} where

Q(x, a) = E(Y |X = x, A = a) is the conditional mean of an individual’s outcome given

their received treatment and baseline covariates. We refer to this function as the outcome

regression function. As a result, the optimal IDR for an individual with covariates x is

given by arg maxa∈[0,1] Q(x, a). Let V opt denote the value function under the optimal IDR.
We have V opt = E{supa∈[0,1] Q(X, a)}.

2.2

I2DR

The focus of this paper is to develop an optimal individualized interval-based decision

rule (I2DR). As commented in the introduction, these decision rules are more ﬂexible to

implement in practice when compared to single-valued decision rules in personalized dose

ﬁnding and dynamic pricing.

We deﬁne an I2DR as a function d(·) that takes an individual’s covariates x as input

and outputs an interval I ⊆ [0, 1]. Given the recommended interval I, diﬀerent doctors /

agents might assign diﬀerent treatments to patients / products. The actual treatments that

subjects receive in the population will have a distribution function Π∗(·; x, I). Throughout

this paper, we assume Π∗(·; x, I) has a bounded density function π∗(·; x, I) for any x and I.
Apparently, we have (cid:82)
I π∗(a; x, I)da = 1, for any interval I and x ∈ X. When (A1)-(A3)

hold, the associated value function under an I2DR d(·) equals

(cid:18)(cid:90)

V π∗(d) = E

d(X)

Q(X, a)π∗(a; X, d(X))da

(cid:19)

.

Restricting d(·) to be a scalar-valued function, V π∗(d) is reduced to V (d).

7

Given the dataset, one may estimate V π∗(d) nonparametrically for any d(·) and directly

search the optimal I2DR based on the estimated value function. However, such a value
search method has the following two limitations. First, a nonparametric estimator of V π∗(d)

requires to specify the preference function π∗, which might be unknown to us. Second, even
though a nonparametric estimator of V π∗(d) can be derived, it remains unknown how to

eﬃciently compute the I2DR that maximizes the estimated value (see Section 8.2.2 for

details). To overcome these limitations, we propose a semiparametric model for the outcome

regression function and use a model-assisted approach to derive the optimal I2DR. We

formally introduce our method in Section 3.

2.3 Working Model Assumptions

In this section, we introduce two working models for the outcome regression function,

corresponding to a piecewise function and a continuous function of the treatment level.

Model I (Piecewise Functions). Suppose

Q(x, a) =

(cid:88)

I∈P0

qI,0(x)I(a ∈ I)

∀x ∈ X, a ∈ [0, 1],

(1)

for some partition P0 of [0, 1] and a collection of continuous functions (qI,0)I∈P0, where the

number of intervals in P0 is ﬁnite. Speciﬁcally, a partition P of [0, 1] is deﬁned as a collection

of mutually disjoint intervals {[τ0, τ1), [τ1, τ2), . . . , [τK−1, τK]} for some 0 = τ0 < τ1 < τ2 <

· · · < τK−1 < τK = 1 and some integer K ≥ 1. As commented in our introduction, we

expect the above model assumption holds in real-world examples such as dynamic pricing.

Model II (Continuous Functions). Suppose Q(x, a) is a continuous function of a

and x, for any x ∈ X and a ∈ [0, 1].

We aim to propose an optimal I2DR that optimizes the value function when either

Model I or Model II holds.

3 Methods

In this section, we ﬁrst present the proposed jump interval-learning and its motivation in

Section 3.1. We next introduce two concrete proposals, i.e., linear jump interval-learning

8

and deep jump interval-learning, to detail our methods in Section 3.2. We then present the

dynamic programming algorithm to implement jump interval-learning (see Algorithm ?? for

an overview) in Section 3.3. Finally, we provide more details on tuning parameter selection

in Section 3.4.

3.1 Jump Interval-learning

We use Model I to present the motivation of our jump interval-learning. In view of equation 1,

any treatment level within an interval I ∈ P0 will yield the same eﬃcacy to a given individual.

The optimal I2DR is then given by

dopt(x) = arg max

qI,0(x),

I∈P0

independent of the preference function π∗. To see this, notice that

(cid:32)(cid:90)

V π∗(dopt) = E

(cid:88)

qI,0(X)I(a ∈ I)π∗(a; X, dopt(X))da

(cid:33)

dopt(X)

I∈P0

qI,0(X)I(dopt(X) ∈ I)

(cid:90)

dopt(X)

= E

(cid:88)

I∈P0

π∗(a; X, dopt(X))da.

For any I2DR d(·), we have (cid:82)

dopt(X) π∗(a; X, dopt(X))da = (cid:82)

d(X) π∗(a; X, d(X))da = 1 by

deﬁnition. It follows that

V π∗(dopt) = E

≥ E

(cid:88)

I∈P0
(cid:90)

qI,0(X)I(dopt(X) ∈ I)

(cid:90)

d(X)

π∗(a; X, d(X))da

(cid:88)

qI,0(X)I(a ∈ I)π∗(a; X, d(X))da = V π∗(d),

d(X)

I∈P0

where the inequality is due to that Q(X, dopt(X)) = (cid:80)
(cid:80)

qI,0(X)I(dopt(X) ∈ I) ≥
qI,0(X)I(a ∈ I) = Q(X, a), almost surely for any a ∈ [0, 1]. Therefore, to derive

I∈P0

I∈P0

the optimal I2DR, it suﬃces to estimate qI,0(·). For notation simplicity, in the rest of this
paper, we denote V π∗(d) by V (d) for any decision rule d.

From now on, we focus on a subset of intervals in [0, 1]. By interval we always refer

to those of the form [a, b) for some 0 ≤ a < b < 1 or [a, 1] for some 0 ≤ a < 1. For

any partition P = {[0, τ1), [τ1, τ2), . . . , [τK−1, 1]}, we use J(P) to denote the set of change

9

point locations, i.e, {τ1, τ2, . . . , τK−1}, and |P| to denote the number of intervals in P. Our
proposed method yields a partition (cid:98)P and an I2DR (cid:98)d(·) such that (cid:98)d(x) ∈ (cid:98)P, ∀x ∈ X. The
number of intervals in (cid:98)P (denoted by | (cid:98)P|) involves a trade-oﬀ. If | (cid:98)P| is too large, then (cid:98)P

will contain many short intervals, making the resulting decision rule hard to implement in

practice. Yet, a smaller value of | (cid:98)P| might result in a smaller value function. Our proposed

method adaptively determines | (cid:98)P| based on jump-penalized regression.

We next detail our method. Jump interval-learning consists of the following two steps.

In the ﬁrst step, we estimate the outcome regression function using jump penalized least

squares regression. Then we derive the corresponding I2DR from the resulting estimator

(cid:98)qI(·). To begin with, we cut the entire treatment range into m initial intervals:

[0, 1/m), [1/m, 2/m), . . . , [(m − 1)/m, 1].

(2)

The integer m is allowed to diverge with the number of observations n. For instance, it can

be speciﬁed by the clinical physician such that the output dose interval for each individual

is at least of the length m−1. When no prior knowledge is available, we recommend to set

m to be proportional to n. It is worth mentioning that equation 2 is not the ﬁnal partition

that we recommend. Nor is it equal to P0 deﬁned in Model I. Given equation 2, we are

looking for a partition (cid:98)P such that each interval in (cid:98)P corresponds to a union of some of the

these m intervals. In other words, we will adaptively combine some of these intervals to

form (cid:98)P.

More speciﬁcally, let B(m) denote the set of partitions P that satisfy the following

requirement: the end-points of each interval I ∈ P lie on the grid {j/m : j = 0, 1, . . . , m}.
We associate to each partition P ∈ B(m) a collection of functions {q(·; θI)}I∈P ∈ (cid:81)
I∈P QI
for QI as some class of functions, where θI is the underlying parameter associated to interval

I. We propose to estimate (cid:98)P by solving

( (cid:98)P, {(cid:98)qI : I ∈ (cid:98)P}) =
(cid:16) 1
n

(cid:110) (cid:88)

n
(cid:88)

I∈P

i=1

arg min
P∈B(m)
{q(·;θI )∈QI :I∈P}

I(Ai ∈ I)(cid:8)Yi − q(Xi; θI)(cid:9)2 + λn|I|(cid:107)θI(cid:107)2

(cid:17)

+ γn|P|

(cid:111)
,

(3)

2

where λn and γn are some nonnegative regularization parameters speciﬁed in Section 3.4, and
(cid:107)θI(cid:107)2

2 denote the Euclidean norm of the model parameter θI. The purpose of introducing

10

the (cid:96)2-type penalty term λn|I|(cid:107)θI(cid:107)2

2 is to help to prevent overﬁtting in large p problems.
The purpose of introducing the (cid:96)0-type penalty term γn|P| is to control the total number of

jumps. When m = n, λn = 0, Ai = i/n, ∀1 ≤ i ≤ n, no baseline covariates are collected,

the above optimization corresponds to the jump-penalized least square estimator proposed

by Boysen et al. (2009). We refer to this step as jump interval-learning (JIL).

For a ﬁxed P, solving the optimization function in equation 3 yields its associated

outcome regression functions {(cid:98)qI}I∈P. This step involves parametric or nonparametric
regression and can be solved via existing statistical or machine learning approaches. We

provide two concrete study cases below, based on linear regression and deep learning. These

estimated outcome regression functions can be viewed as functions of P. As such, (cid:98)P is

adaptively determined by minimizing the penalized least square function in equation 3.

To maximize the expected outcome of interest, our proposed I2DR is then given by

(cid:98)d(x) = arg max

I∈ (cid:98)P

(cid:98)qI(x),

∀x ∈ X.

(4)

When the argmax in equation 4 is not unique, (cid:98)d(·) outputs the interval that contains the

smallest treatment.

We next evaluate the value function under the proposed I2DR V ( (cid:98)d) and V (dopt). For
each interval I in the estimated optimal partition (cid:98)P, we estimate the generalized propensity
score function e(I|x) ≡ Pr(A ∈ I|X = x). Let (cid:98)e(I|x) denote the resulting estimate.
Following the estimation strategy in Zhang et al. (2012), we propose the following value

estimator under equation 4,

(cid:98)V =

1
n

n
(cid:88)

i=1

(cid:34) I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:8)Yi − max
I∈ (cid:98)P

(cid:98)qI(Xi)(cid:9) + max

I∈ (cid:98)P

(cid:35)
(cid:98)qI(Xi)

.

(5)

Statistical properties of the estimates in equation 4 and equation 5 are studied in Section

4. Although we use the example of piecewise functions to motivate our procedure, the

proposed method allows the outcome regression function to be a continuous function of a

and x as well. See Section 4 for detail.

11

3.2 Linear- and Deep-JIL

In practice, we consider two concrete proposals to implementing jump interval-learning, by

considering a linear function class and a deep neural networks class for QI in equation 3.

3.2.1 Case 1: Linear-JIL

We use a linear regression model for QI. Speciﬁcally, we set q(x, θI) to ¯x(cid:62)θI for any
interval I and x ∈ X, where ¯x is a shorthand for the vector (1, x(cid:62))(cid:62). Adopting the linearity
assumption, we have (cid:98)qI(x) = ¯x(cid:62)(cid:98)θI for some (cid:98)θI. It follows from equation 4 that the proposed
I2DR corresponds to a linear decision rule, i.e., (cid:98)d(x) = arg maxI∈ (cid:98)P ¯x(cid:62)(cid:98)θI. As such, the
linearity assumption ensures our I2DR is interpretable to the domain experts.

We next discuss how to compute (cid:98)P and {(cid:98)θI : I ∈ (cid:98)P}. The objective function in

equation 3 is reduced to

( (cid:98)P, {(cid:98)θI : I ∈ (cid:98)P}) =
(cid:32)

(cid:40)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θI)2 + λn|I|(cid:107)θI(cid:107)2
2

(cid:33)

(6)

(cid:41)

+ γn|P|

,

(cid:88)

I∈P

=

arg min
(P∈B(m),{θI :I∈P})

where X i = (1, X (cid:62)
The ridge penalty λn|I|(cid:107)θI(cid:107)2
parameter (cid:98)θI is well deﬁned even when (cid:80)n
(cid:80)
i

i )(cid:62). We refer to this step as linear jump interval-learning (L-JIL).
2 in equation 6 guarantees that for any interval I ∈ (cid:98)P, the
I(Ai ∈ I) < p + 1 such that the matrix

is not invertible. It also prevents over-ﬁtting and yields more accurate

I(Ai ∈ I)X iX

(cid:62)
i

i=1

estimate in high-dimensional settings.

3.2.2 Case 2: Deep-JIL

We next consider using deep neural networks (DNNs) to approximate the outcome regression

function, so as to capture the complex dependence between the outcome and covariates.

Speciﬁcally, the network consists of p input units (coloured in blue in Figure 1), corresponding

to the covariates X. The hidden units (coloured in green) are grouped in a sequence of L

layers. Each unit in the hidden layer is determined as a nonlinear transformation of a linear

combination of the nodes from the previous layer. The total number of parameters in the

network is denoted by W . See Figure 1 for an illustration. The parameters in DNNs can be

12

Figure 1: Illustration of DNN with L = 2 and W = 25; here µ ∈ Rp is the input, the output is
given by A(3)σ(A(2)σ(A(1)µ + b(1)) + b(2)) + b(3) where A(l), b(l) denote the corresponding parameters
to produce the linear transformation for the (l − 1)th layer and that σ denotes the componentwise
ReLU function. In this example, W = (cid:80)3
j=1((cid:107)A(3)(cid:107)0 + (cid:107)b(3)(cid:107)0) = 25 where (cid:107) • (cid:107)0 denotes the
number of nonzero elements in the vector or matrix.

solved using a stochastic gradient descent algorithm. In our implementation, we apply the

Multi-layer Perceptron (MLP) regressor Pedregosa et al. (2011) for parameter estimation.

We refer to the resulting optimization as deep jump interval-learning (D-JIL).

Finally, we remark that alternative to our approach, one may directly apply DNN that

takes the covariate-treatment pair (X, A) as the input to learn the outcome regression

function. However, the resulting estimator for the outcome regression function is not

guaranteed to be a piecewise function of the treatment. As such, it cannot yield an I2DR.

3.3

Implementation

In this section, we present the computational details for jump interval-learning. We employ

the dynamic programming algorithm (see e.g., Friedrich et al., 2008) to ﬁnd the optimal

partition (cid:98)P that minimizes the objective function equation 3. Meanwhile, other algorithms

for multi-scale change point detection are equally applicable (see e.g., Scott and Knott,

1974; Harchaoui and Lévy-Leduc, 2010; Fryzlewicz, 2014). Speciﬁcally, we adopt the PELT

method proposed by Killick et al. (2012) that includes additional pruning steps within the

dynamic programming framework to achieve a linear computational cost. Given (cid:98)P, the
set of functions {(cid:98)qI : I ∈ (cid:98)P} can be computed via either linear regression or deep neural

13

network.

To detail our procedure, for any interval I ∈ [0, 1], we deﬁne the cost function

cost(I) = min

q(·;θI )∈QI

(cid:34)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(cid:8)Yi − q(Xi; θI)(cid:9)2 + λn||θI||2

2

(cid:35)

,

where QI is a class of linear functions or deep neural networks, corresponding to L-JIL and

D-JIL, respectively.

For any integer 1 ≤ r < m, denote by B(m, r) the set consisting of all possible

partitions Pr of [0, r/m) such that the end-points of each interval I ∈ Pr lie on the grid

{j/m : j = 0, 1, . . . , r}. Set B(m, m) = B(m), we deﬁne the Bellman function

B(r) = inf

Pr∈B(m,r)

(cid:32)

(cid:88)

I∈Pr

cost(I) + γn(|Pr| − 1)

.

(cid:33)

Let B(0) = −γn, the dynamic programming algorithm relies on the following recursion

formula,

B(r) = min
j∈Rr

{B(j) + γn + cost([j/m, r/m))} ,

∀r ≥ 1.

(7)

where Rr is the candidate change points list updated by

{j ∈ Rr−1 ∪ {r − 1} : B(j) + cost([j/m, (r − 1)/m)) ≤ B(r − 1)},

(8)

during each iteration with R0 = {0}. The constraint listed in equation 8 iteratively updates

the set of candidate change points and removes values that can never be the minima of the

objective function. It speeds up the computation, leading to a cost that is linear in the

number of observations (Killick et al., 2012).

We brieﬂy summarize our algorithm below. For a given integer r, we search the optimal

change point location j that minimizes the above Bellman function B(r) in equation 7. This

requires to apply the linear / MLP regression to learn (cid:98)q[j/m,r/m) and cost([j/m, r/m)) for
each j ∈ Rr. Let j∗ be the corresponding minimizer. We then deﬁne the change points list
τ (r) = {j∗, τ (j∗)}. This procedure is iterated to compute B(r) and τ (r) for r = 1, . . . , m.

The optimal partition (cid:98)P is determined by the values stored in τ (·). A pseudocode containing

more details is given in Algorithm 1.

14

Global: data {(Xi, Ai, Yi) : i = 1, . . . , n}; sample size n; covariates dimension p;

number of initial intervals m; penalty terms γn.

Local: integers l, r ∈ N; cost dictionary C; a vector of integers τ ∈ Nm;

Bellman function B ∈ Rm; a set of candidate point lists R.

Output: (cid:98)P and {(cid:98)qI : I ∈ (cid:98)P}.
I. Initialization.

1. Set B(0) ← −γn; (cid:98)P ← N ull; τ ← N ull; R(0) ← {0};
2. Deﬁne the cost function C(I):

(i). If C(I) ← N U LL:

(a). Apply Linear / MLP regression: (cid:98)qI(·) ← arg minq
(b). Calculate the cost: C(I) ← (cid:80)

I(Ai ∈ I)(cid:8)

i

(cid:98)qI(Xi) − Yi

(cid:80)
i

I(Ai ∈ I){Yi − q(Xi)}2;
(cid:9)2;

(ii). Return C(I).

II. Apply the PELT method. For r = 1, . . . , m:

1. B(r) = minj∈R(r){B(j) + C([j/m, r/m)) + γn};
2. j∗ ← arg minj∈R(r){B(j) + C([j/m, r/m)) + γn};
3. τ (r) ← {j∗, τ (j∗)};
4. R(r) ← {j ∈ R(r − 1) ∪ {r − 1} : B(j) + C([j/m, (r − 1)/m)) ≤ B(r − 1)};

III. Get Partitions. τ ∗ ← τ (m); r ← m; l ← τ ∗[r]; While r > 0:

1. Let I = [l/m, r/m) if r < m else I = [l/m, 1];
2. (cid:98)P ← (cid:98)P ∪ I;
3. (cid:98)qI(·) ← arg minq
4. r ← l; l ← τ ∗[r];
return (cid:98)P and {(cid:98)qI : I ∈ (cid:98)P}.

I(Ai ∈ I){Yi − q(Xi)}2;

(cid:80)

i

Algorithm 1: Jump interval-learning

3.4 Tuning Parameters

Our proposal requires to specify the tuning parameters m, λn and γn. We ﬁrst discuss the

choice of m. In practice, we recommend to set m = n/c with some constant c > 0 such that

m and n are of the same order. In our simulation studies, we tried several diﬀerent values

of c and found the resulting estimated I2DRs have approximately the same value function.

Thus, the proposed I2DR is not overly sensitive to the choice of this constant. Detailed

empirical results can be found in Section 5.3.

We next discuss the choices of λn and γn. Selection of these tuning parameters relies on

the concrete proposal to approximate the outcome regression function. We elaborate below.

15

3.4.1 Tuning in L-JIL

For L-JIL, we choose γn and λn simultaneously via cross-validation. The theoretical

requirements of γn and λn for L-JIL are imposed in the statement of Theorem 1. We further

develop an algorithm that substantially reduces the computation complexity resulting from

the use of cross-validation.

To be more speciﬁc, let Λn = {λ(1)

n , · · · , λ(H)

n } and Γn = {γ(1)

n , · · · , γ(J)

n } be the set of

candidate tuning parameters. For a given integer K0, we randomly split the data into
K0 equal sized subgroups. Let Gk denote indices of the subsamples in the kth subgroup,
for k = 1, · · · , K0. Let G−k denote the complement of Gk. For any λn ∈ Λn, γn ∈ Γn,

k ∈ {1, · · · , K0}, let ( (cid:98)Pλn,γn,k, {(cid:98)θI,γn,λn,k : I ∈ (cid:98)Pλn,γn,k}) denote the optimizer equation 6,
computed based on the data in G−k. We aim to choose γn and λn that minimizes

1
n

K0(cid:88)

(cid:88)

(cid:88)

k=1

i∈Gk

I∈ (cid:98)Pλn,γn,k

I(Ai ∈ I){Yi − X

(cid:62)
i (cid:98)θI,γn,λn,k}2.

(9)

To solve equation 9, we remark that there is no need to apply Algorithm 1 |Λn| × |Γn|

times to compute the minimizer of equation 6 over the set of candidate tuning parameters.

We develop an algorithm to facilitate the computation. The key observation is that, for any

interval I ⊆ [0, 1] and k ∈ {1, · · · , K0}, the set of estimators {(cid:98)θI,γn,λn,k : γn ∈ Γn, λn ∈ Λn}

can be obtained simultaneously over the set of candidate tuning parameters. This forms the

basis of our algorithm. More details are provided in Section A of the supplementary article.

3.4.2 Tuning in D-JIL

As for D-JIL, we ﬁnd that the MLP regressor is not overly sensitive to the choice of λn,

so we set λn = 0. The parameter γn is chosen based on cross-validation. The theoretical

requirement of γn for D-JIL is imposed in the statement of Theorem 2. To implement the

cross-validation, we randomly split the data into K0 equal sized subgroups, denoted by

{(Xi, Ai, Yi)}i∈G1, {(Xi, Ai, Yi)}i∈G2, · · · , {(Xi, Ai, Yi)}i∈GK0
, accordingly. For each γn and
k = 1, . . . , K0, we compute the estimators (cid:98)Pγn,k and (cid:98)qI,γn,k(·) based on the sub-dataset in

16

G−k. Then we choose γn that minimizes

1
n

K0(cid:88)

(cid:88)

(cid:88)

k=1

i∈Gk

I∈ (cid:98)Pγn,k

I(Ai ∈ I){Yi − (cid:98)qI,γn,k(Xi)}2.

We also remark that other tuning parameters, such as the learning rate, the numbers

of hidden nodes and hidden layers, are set to the default values of the MLP regressor

implementation (Pedregosa et al., 2011).

4 Theory

We establish the statistical properties of our proposed method in this section. As we

have commented, we consider both cases where the outcome regression function is either a

piecewise or continuous function of the treatment. We ﬁrst study the statistical properties

of L-JIL and D-JIL under Model I, respectively. We next outline a procedure to construct a

conﬁdence interval for the value under the proposed I2DR and prove its validity under these

two methods. Finally, we investigate the properties of our proposed method under Model

II. The theoretical results justify that our method will work when the outcome regression

function is either piecewise or continuous function.

4.1 Properties under Model I

4.1.1 Results for L-JIL

To establish the theoretical properties of the I2DR obtained by L-JIL, we ﬁrst assume
equation 1 holds with qI,0(x) = ¯x(cid:62)θI,0 for any x ∈ X and I ∈ P0. In other words, the

outcome regression function Q(x, a) is linear in x and piecewise constant in a. Without loss

of generality, assume θ0,I1 (cid:54)= θ0,I2 for any two adjacent intervals I1, I2 ∈ P0. This guarantees

that the representation in equation 1 is unique. We write an (cid:16) bn for two sequences
{an}, {bn} if there exists some universal constant c ≥ 1 such that c−1bn ≤ an ≤ cbn. Deﬁne
θ0(·) = (cid:80)
θI,0I(· ∈ I). Giving ( (cid:98)P, {(cid:98)θI : I ∈ (cid:98)P}), our estimator for the function θ0(·) is
deﬁned by

I∈P0

(cid:98)θ(·) =

(cid:88)

I∈ (cid:98)P

(cid:98)θII(· ∈ I).

17

(10)

This yields a piecewise constant approximation of θ0(·). We ﬁrst study the theoretical

properties of (cid:98)θ(·). Toward that end, we need to impose the following condition on the

probability tails of X and Y .

(A4) Suppose there exists some constant ω > 0 such that (cid:107)X (j)(cid:107)ψ2|A ≤ ω, for any j ∈
{1, . . . , p} almost surely and (cid:107)Y (cid:107)ψ2|A ≤ ω almost surely, where X (j) denotes the jth element
of X, and that for any random variable Z, (cid:107)Z(cid:107)ψ2|A denotes the conditional Orlicz norm

given the treatment A,

(cid:107)Z(cid:107)ψ2|A

∆= inf
C>0

(cid:20)

(cid:26)

E

exp

(cid:18) |Z|2
C 2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(cid:21)

A

≤ 2

.

We remark that Condition (A4) is automatically satisﬁed when the covariates and the

outcomes are bounded.

Theorem 1 Assume (A1)-(A4) hold and equation 1 holds with qI,0(x) = ¯x(cid:62)θI,0. Assume
A has a bounded probability density function on [0, 1]. Assume m (cid:16) n, λn = O(n−1 log n),

{γn}n∈N satisﬁes γn → 0 and γnn/ log n → ∞. Then, there exists some constant ¯c > 0 such

that the following events hold with probability at least 1 − O(n−2):

(i) | (cid:98)P| = |P0|.
(ii) maxτ ∈J(P0) minˆτ ∈J( (cid:98)P) |ˆτ − τ | ≤ ¯cn−1 log n.
(iii) (cid:82) 1

2da ≤ ¯cn−1 log n.

0 (cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

(cid:4)

In Theorem 1, results in (i) show the model selection consistency of our jump penalized

estimator. Results in (ii) imply that the estimated change point locations converge at a

rate of Op(n−1 log n). In (iii), we derive an upper error bound for the integrated (cid:96)2 loss of

(cid:98)θ(·). As discussed in the introduction, derivation of Theorem 1 is nontrivial. A number of

technical lemmas (see Lemma 1-4 in Section 7) are established to prove Theorem 1. These

results can be easily extended to study general covariate-based change point models.

We next establish the convergence rate of V opt − V ( (cid:98)d), where V opt = V (dopt). The
quantity V opt − V ( (cid:98)d) represents the diﬀerence between the optimal value and the value
under the proposed I2DR. The smaller the diﬀerence, the better the I2DR. Notice that

V opt ≥ V (d) for any I2DR d(·). It suﬃces to provide an upper bound for V opt − V ( (cid:98)d). We
impose the following condition.

18

(A5.) Assume for any I1, I2 ∈ P0, there exist some constants γ, δ0 > 0 such that

Pr(0 < |qI1,0(X) − qI2,0(X)| ≤ t) = O(tγ),

where the big-O term is uniform in 0 < t ≤ δ0.

Condition (A5) is commonly assumed in the literature to derive sharp convergence rate

for the value function under the estimated optimal IDR (Qian and Murphy, 2011; Luedtke

and Van Der Laan, 2016; Shi et al., 2020).

It is very similar to the margin condition

(Tsybakov, 2004; Audibert and Tsybakov, 2007) used in the classiﬁcation literature. This

condition is automatically satisﬁed with γ = 1 when qI,0(X) has a bounded probability

density function for any I ∈ P0.

Theorem 2 Assume the conditions in Theorem 1 are satisﬁed. Further assume (A5) holds.

Then, we have

V opt − V ( (cid:98)d) ≤ ¯c(n−1 log n)(1+γ)/2 + ¯cn−1 log n,

for some constant ¯c > 0, with probability at least 1 − O(n−2).

(11)

(cid:4)

When (A5) holds with γ = 1, Theorem 2 suggests that V ( (cid:98)d) converges to the optimal
value at a rate of Op(n−1) up to some logarithmic factor. Notice that the events deﬁned in
Theorem 1 and 2 occur with probability at least 1 − O(n−2). Since (cid:80)
n≥1 n−2 < +∞, an
application of Borel-Cantelli lemma implies that these events will occur for suﬃciently large

n almost surely.

4.1.2 Results for D-JIL

We study the theoretical properties of the proposed I2DR under Model I when D-JIL is

applied. Similar to the linear case, we assume qI1,0 (cid:54)= qI2,0 for any two adjacent intervals

I1, I2 ∈ P0. For any I, we set the regression class QI to a class of DNN with LI hidden

layers and WI many number of parameters.

To derive the theoretical properties of D-JIL, we assume the outcome regression function

is a smooth function of the baseline covariates (see assumption (A6) below). Meanwhile,

D-JIL is valid when Q(x, a) is a nonsmooth function of x as well (see e.g., Imaizumi and

19

Fukumizu, 2019). Speciﬁcally, deﬁne the class of β-smooth functions (also known as Hölder

smooth functions with exponent β) as

Φ(β, c) =




h :



sup
(cid:107)α(cid:107)1≤(cid:98)β(cid:99)

sup
x∈X

|Dαh(x)| ≤ c,

sup
(cid:107)α(cid:107)1=(cid:98)β(cid:99)

sup
x,y∈X
x(cid:54)=y

|Dαh(x) − Dαh(y)|
(cid:107)x − y(cid:107)β−(cid:98)β(cid:99)

2

≤ c






,

for some constant c > 0, where (cid:98)β(cid:99) denotes the largest integer that is smaller than β and

Dα denotes the diﬀerential operator Dα denote the diﬀerential operator:

Dαh(x) =

∂(cid:107)α(cid:107)1h(x)
1 · · · ∂xαd

∂xα1

d

.

We introduce the following conditions.

(A6.) Suppose Q(•, a) ∈ Φ(β, c), and p(a|•) ∈ Φ(β, c) for any a.

(A7.) Functions {(cid:98)qI}I∈ (cid:98)P are uniformly bounded.

Assumption (A7) ensures that the optimizer would not diverge in the (cid:96)∞ sense. Similar

assumptions are commonly imposed in the literature to derive the convergence rates of DNN

estimators (see e.g., Farrell et al., 2021). Combining (A7) with (A6) allows us to derive

the uniform rate of convergence for the class of DNN estimators {(cid:98)qI}I∈ (cid:98)P. The following
theorem summarizes the theoretical properties of the proposed method via deep neural

networks.

Theorem 3 Assume (A1)-(A3), (A6), (A7) and Model I hold. Assume X and Y are bounded

variables, and A has a bounded probability density function on [0, 1]. Assume m (cid:16) n, {γn}n∈N
satisﬁes γn → 0 and γn (cid:29) n−2β/(2β+p) log8 n. Then, there exist some constant ¯c > 0 and
DNN classes {QI : I} with LI (cid:16) log(n|I|) and WI (cid:16) (n|I|)p/(2β+p) log(n|I|) such that the

resulting D-JIL estimator computed by equation 3 satisﬁes

(i) | (cid:98)P| = |P0|;
(ii) maxτ ∈J(P0) minˆτ ∈J( (cid:98)P) |ˆτ − τ | ≤ ¯cn−2β/(2β+p) log8 n;
(iii) E|Q(X, A) − (cid:80)

I(A ∈ I)(cid:98)qI(X)|2da ≤ ¯cn−2β/(2β+p) log8 n,

I∈ (cid:98)P

with probability at least 1 − O(n−2).

(cid:4)

Theorem 3 establishes the properties of our method under settings where the Q(x, a)

is piecewise function in the treatment. Results in (i) imply that D-JIL correctly identiﬁes

20

the number of change points. Results in (ii) imply that any change point in P0 can be
consistently identiﬁed at a convergence rate of Op(n−2β/(2β+p)) up to some logarithmic
factors. Notice that we use the piecewise function (cid:80)
I(a ∈ I)(cid:98)qI(x) to approximate the
outcome regression function. In (iii), we show our estimator for function Q(X, A) converges

I∈ (cid:98)P

at a rate of Op(n−2β/(2β+p)) up to some logarithmic factors. The theoretical choices of LI

and WI in Theorem 3 are consistent with the literature of DNN estimators (Imaizumi and

Fukumizu, 2019; Farrell et al., 2021). These DNN architectures ensure the convergence rate

of our estimator for function Q(X, A), which achieves the minimax-optimal nonparametric

rate of convergence under (A6) (see e.g., Stone, 1982).

We next establish the convergence rate of V opt − V ( (cid:98)d) under Model I in the following

theorem.

Theorem 4 Assume the conditions in Theorem 3 are satisﬁed. Further assume (A5) holds.

Then, we have

V ( (cid:98)d) ≥ V opt − O(1)(n− 2β

2β+p log8 n + n− 2β(1+γ)

(2β+p)(2+γ) log

with probability at least 1 − O(n−2).

8+8γ
2+γ n),

(12)

(cid:4)

Theorem 4 suggests that V ( (cid:98)d) converges to the optimal value at a rate of Op{n− 2β(1+γ)
up to some logarithmic factors. This rate is slower than the rate (Op(n−1) up to some

(2β+p)(2+γ) }

logarithmic factor) we obtained in Theorem 2 where we posit a parametric (linear) model.

Suppose the condition 4β(1 + γ) > (2β + p)(2 + γ) holds, it follows that V ( (cid:98)d) = V opt +
op(n−1/2). This observation forms the basis of our inference procedure in Section 4.1.3.

Here, the extra margin parameter γ in our results is introduced by (A5) to bound the bias

due to the estimated decision rule (cid:98)d. If the margin parameter γ goes to inﬁnity, we only
require the smooth parameter β > p/2 to obtain V ( (cid:98)d) = V opt + op(n−1/2). This condition
(β > p/2) is commonly assumed in the literature on evaluating average treatment eﬀects

(see e.g., Chernozhukov et al., 2017; Farrell et al., 2021).

4.1.3 Evaluation of the Value Function

Suppose Model I holds. When L-JIL is used, it follows from Theorem 2 that V ( (cid:98)d) =
V opt + op(n−1/2). When D-JIL is used, if the smoothness parameter β (see (A6)) and the

21

margin parameter γ (see (A5)) satisfy 4β(1 + γ) > (2β + p)(2 + γ), it follows from Theorem

4 that V ( (cid:98)d) = V opt + op(n−1/2). In the following, we derive the asymptotic normality of
√
n{ (cid:98)V − V ( (cid:98)d)} is asymptotically

n( (cid:98)V − V opt). By Slutsky’s theorem, this implies that

√

normal as well.

(A8.) [E{(cid:98)e(I|X) − e(I|X)}2]1/2 = o(n−1/4) and that (cid:98)e(I; •) belongs to the class of VC-type
functions with VC-index upper bounded by O(n1/2) (see e.g. Chernozhukov et al., 2017, for

a detailed deﬁnition of the VC-type class), for any I ∈ I(m).

The ﬁrst part in Assumption (A8) requires the generalized propensity score function

to converge at certain rates. Similar assumptions are commonly imposed in the causal

inference literature to derive the asymptotic distribution of the estimated average treatment

eﬀect (see e.g., Chernozhukov et al., 2017). The second part of (A8) essentially controls

the model complexity of the estimator (cid:98)e. The more complicated (cid:98)e is, the larger the VC
index. Under (A6), we can show (A8) holds when DNN is used to model the generalized

propensity score.

Theorem 5 Assume (A8) holds and suppose functions {(cid:98)eI}I∈ (cid:98)P are uniformly bounded away
from zero. Further assume that for any I1, I2 ∈ P0 with I1 (cid:54)= I2, we have Pr(qI1,0(X) =

qI2,0(X)) = 0.

(i) Suppose conditions in Theorem 2 are satisﬁed. Then, under L-JIL, we have

√

n( (cid:98)V − V opt) d→ N (0, σ2

L),

for some σ2

L > 0.

(ii) Suppose conditions in Theorem 4 are satisﬁed with 4β(1 + γ) > (2β + p)(2 + γ).

Then, under D-JIL, we have

√

n( (cid:98)V − V opt) d→ N (0, σ2

D),

for some σ2

D > 0.

(cid:4)

We now introduce the estimator for the asymptotic variance σ2

D, and derive a
Wald-type 1 − α CI for V opt. Since V ( (cid:98)d) = V opt + op(n−1/2), the proposed CI also covers

L or σ2

22

V ( (cid:98)d) with probability tending to 1 − α. We estimate σ2

L or σ2

D by

(cid:98)σ2 =

1
n − 1

n
(cid:88)

i=1

(cid:34) I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:8)Yi − max
I∈ (cid:98)P

(cid:98)qI(Xi)(cid:9) + max

I∈ (cid:98)P

(cid:35)2

(cid:98)qI(Xi) − (cid:98)V

,

where {(cid:98)qI(·)} corresponds to the value estimations under L-JIL or D-JIL.

The corresponding 1 − α CI is given by (cid:98)V ± zα/2(cid:98)σ, where zα/2 denotes the upper α/2-th
quantile of a standard normal distribution. Similar to Theorem 5, we can show that (cid:98)σ is
consistent. This shows the validity of our inference procedure.

4.2 Properties under Model II

4.2.1 Properties of L-JIL under Varying Coeﬃcient Model

We ﬁrst consider the case when the outcome regression function can be represented by a

varying coeﬃcient model and investigate the theoretical properties of the proposed L-JIL.

Speciﬁcally, suppose the true outcome regression function takes the following form

Q(x, a) = ¯x(cid:62)θ0(a),

∀x ∈ X, a ∈ [0, 1],

(13)

where ¯x = (1, x(cid:62))(cid:62) and θ0(·) is some continuous (p + 1)-dimensional function. That is, we

assume the conditional mean of the outcome is a linear function of individuals’ covariates

for any treatment a ∈ [0, 1]. Yet, the model is ﬂexible in that θ0(·) is allowed to be either a

step function, or an arbitrary continuous function of a with certain smoothness constraints.

Models of this type belong to the class of varying coeﬃcient models popularly applied in

many scientiﬁc areas (see e.g., Fan and Zhang, 2008, for an overview).

Here, we consider the following class of Hölder continuous functions for θ0(·). Suppose

there exist some constants L > 0, 0 < α0 ≤ 1 such that θ0(·) satisﬁes

sup
a1,a2∈[0,1]

(cid:107)θ0(a1) − θ0(a2)(cid:107)2 ≤ L|a1 − a2|α0.

(14)

We ﬁrst sketch a few lines to see why our method works under equation 14. For a given

integer k > 0, we deﬁne θ∗

k(·) as

θ∗
k(a) =

k−1
(cid:88)

j=0

(cid:19)

(cid:18) j + 1/2
k + 1

θ0

I(j ≤ (k + 1)a < j + 1) + θ0

(cid:19)

(cid:18) k + 1/2
k + 1

I((k + 1)a ≥ k).

23

Apparently, θ∗

k(·) has at most k change points. In addition, with some calculations, we can
k(a) − θ0(a)(cid:107)2 ≤ 2−α0(k + 1)−α0L. Letting k → ∞, it is immediate to
show that supa∈[0,1] (cid:107)θ∗
see that θ0(·) can be uniformly approximately by a step function as the number of change

points increases.

In Theorems 1 and 2, we have shown the proposed I2DR is consistent under the

piecewise linear function assumption. Based on the above discussion, we expect that jump

interval-learning also works when the model equation 13 holds. We formally establish the

corresponding theoretical results in the following theorem.

Theorem 6 Assume (A1)-(A4) and equation 14 hold. Assume A has a bounded probability

on [0, 1]. Assume m (cid:16) n, λn = O(n−1 log n), γn satisﬁes γn → 0 and γn (cid:29) n−1 log n. Under

the model equation 13, there exists some constant ¯c > 0 such that the following holds with

probability at least 1 − O(n−2):

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da ≤ ¯cγ2α0/(1+2α0)
n

.

In addition, assume γn (cid:16) (n−1 log n)(1+2α0)/(1+4α0). Then there exists some constant ¯c∗ > 0
such that the following occurs with probability at least 1 − O(n−2) that

V opt − V ( (cid:98)d) ≤ ¯c∗(n−1 log n)α0/(1+4α0).

(15)

(cid:4)

It is worth mentioning that with proper choice of γn, the integrated (cid:96)2 loss of (cid:98)θ(·)
converges at a rate of Op(n−2α0/(1+2α0)) up to some logarithmic factor. The rate is slower

compared to the results in Theorem 1, since θ0(·) is only “approximately" piecewise constant.

When θ0(·) is Lipschitz continuous, it follows from equation 15 that the value under our
proposed I2DR will converge to the optimal value function at a rate of Op(n−1/5 log1/5 n).

4.2.2 Properties of D-JIL under the Continuous outcome regression function

We next consider the general case when the outcome regression function is speciﬁed by

model II and study the theoretical properties of the proposed D-JIL. The following theorem

proves the consistency of the proposed estimator.

24

Theorem 7 Suppose Q is a continuous function of a and x. Assume (A1)-(A3) and

(A6)-(A7) hold. Assume X and Y are bounded variables, and A has a bounded probability

density function on [0, 1]. Assume m (cid:16) n and {γn}n∈N satisﬁes γn → 0 and γn (cid:29)
n−2β/(2β+p) log8 n. Then, there exist some DNN classes {QI : I} with LI (cid:16) log(n|I|) and
WI (cid:16) (n|I|)p/(2β+p) log(n|I|) such that the resulting D-JIL estimator computed by equation 3

satisﬁes

(i) maxI∈ (cid:98)P supa∈I E|(cid:98)qI(X) − Q(X, a)|2 = op(1);
(ii) V opt − V ( (cid:98)d) = op(1).

(cid:4)

Theorem 7 establishes the properties of our method under settings where Q is continuous

in a. Results in (i) imply that (cid:98)qI(x) can be used to uniformly approximate Q(x, a) for any
a ∈ I. The consistency of the value in (ii) thus follows.

5 Simulations

5.1 Conﬁdence interval for the value

In this section, we focus on scenarios where the outcome regression function takes the

form of Model I and examine the coverage probability of the proposed CI in Section 4.1.3.

Simulated data are generated from the following model:

Y |X, A ∼ N (Q(X, A), 1), A|X ∼ Unif[0, 1] and X (1), X (2), . . . , X (p) iid∼ N (0, 1),

where Unif[a, b] denotes the uniform distribution on the interval [a, b]. We consider the

following two scenarios with diﬀerent choices of Q(X, A).

Scenario 1:

Q(x, a) =






a < 0.35,

1 + x(1),
x(1) − x(2), 0.35 ≤ a < 0.65,
1 − x(2),

a ≥ 0.65.

Under Scenario 1, the outcome regression function is piecewise constant as a function of a,

and is linear as a function of x. Here, we have J(P0) = {0.35, 0.65} and |P0| = 3. With

some calculations, one can show that the optimal value V opt equals 1.34.

25

Table 1: The estimated optimal value (cid:98)V with its standard error, the empirical coverage probability
of its associated conﬁdence interval, and the averaged number of estimated partitions computed by
the proposed L-JIL and D-JIL.

Scenario 1, p = 4

Scenario 2, p = 4

n = 200 n = 400 n = 800 n = 200 n = 400 n = 800

Method

L-JIL

D-JIL

Optimal value V opt
Estimated optimal value (cid:98)V
Mean of standard error (cid:98)σ
Coverage probabilities(%)
Number of partitions | (cid:98)P|
Estimated optimal value (cid:98)V
Mean of standard error (cid:98)σ
Coverage probabilities(%)
Number of partitions | (cid:98)P|

1.436
0.129
89.80
2.97

1.297
0.160
90.60
2.98

1.34

1.383
0.091
93.20
3.01

1.338
0.108
93.60
3.25

1.340
0.066
95.60
3.00

1.345
0.060
96.00
3.18

NA
NA
NA
NA

1.333
0.166
95.60
2.95

1.35

NA
NA
NA
NA

1.331
0.102
93.80
3.10

NA
NA
NA
NA

1.349
0.060
95.00
3.08

Scenario 2:

Q(x, a) =






1 + (x(1))3,
x(1) − log(1.5 + x(2)), 0.35 ≤ a < 0.65,
1 − sin(0.5πx(2)),

a ≥ 0.65.

a < 0.35,

Under Scenario 2, the outcome regression function is piecewise constant as a function of

a, but is nonlinear as a function of x. The change points are J(P0) = {0.35, 0.65} with

|P0| = 3. The optimal value equals 1.35, based on Monte Carlo approximations.

For each scenario, we set p = 4 and consider three diﬀerence choices of the sample size,

corresponding to n = 200, 400, 800. We apply the proposed D-JIL to both scenarios. L-JIL

is applied to Scenario 1 only, as it requires the outcome regression function to be linear in

the baseline covariates. The detailed implementation is discussed in Section 3.3. We set

m = n/5, λn = 0, γn = 4n−1 log(n), and construct the CI for V opt based on the procedure

described in Section 4.1.3. Reported in Table 1 are the estimated value function (cid:98)V with its
standard error (cid:98)σ, the empirical coverage probabilities of the proposed conﬁdence interval
for V opt, and the number of estimated partitions | (cid:98)P|, aggregated over 500 simulations.

Based on the results, it is clear that the estimated value function approaches the optimal

value as the sample size increases for both two methods. For instance, when n = 800, L-JIL

obtained an estimated value of 1.340 under Scenario 1 on average. D-JIL yields an average

value of 1.349 under Scenario 2. These values are very close to the truths 1.34 and 1.35,

26

respectively. The performance of our proposed L-JIL and D-JIL are comparable under

the Scenario 1. In addition, as the sample size increases, the coverage probability of the

Wald-type CI approaches to the nominal level. This veriﬁes our theoretical ﬁndings in

Theorem 5. Moreover, the averaged estimated number of partitions | (cid:98)P| is approximately 3

for all settings. This supports our theoretical ﬁndings in Theorems 1 and 3.

5.2 Value function under the proposed I2DR

In this section, we consider more general settings and compare the proposed procedure with

the existing state-of-the-art methods that outputs single-valued decision rule. Similar to

Section 5.1, we generate the data from the following model:

Y |X, A ∼ N (Q(X, A), 1), A|X ∼ Unif[0, 1] and X (1), X (2), . . . , X (p) iid∼ N (0, 1).

In addition to Scenarios 1 and 2, we consider several other choices of the outcome regression

function, allowing the working model assumption in Model I or Model II to be violated in

some scenarios.

Scenario 3:

Q(x, a) =





(cid:112)x(1)/2 + 0.5,
a < 0.25,
sin(2πx(2)),
0.25 ≤ a < 0.5,
0.5 − (x(1) + x(2) − 0.75)2, 0.5 ≤ a < 0.75,
0.5,

a ≥ 0.75.

The outcome regression function in Scenario 3 is piecewise function of a but nonlinear function

of x with complex treatment-covariates interactions. We have J(P0) = {0.25, 0.5, 0.75, 1}

with |P0| = 4, and the optimal value equals 0.76.

Scenario 4:

Q(x, a) = ¯x(cid:62){2|a − 0.5|θ∗},

where θ∗ = (1, 2, −2, 0(cid:62)

p−2)(cid:62). By setting θ0(a) = 2|a − 0.5|θ∗, it is immediate to see that
Q(x, a) = ¯x(cid:62)θ0(a) and satisﬁes the condition in equation 13. Note that θ0(·) here is a
continuous function. One can show that V opt = 1.28 under this scenario.

Scenario 5:

Q(x, a) = 8 + 4x(1) − 2x(2) − 2x(3) − 10(1 + 0.5x(1) + 0.5x(2) − 2a)2.

27

Table 2: The value function under the proposed I2DR and IDRs estimated based on outcome
weighted learning for Scenario 1-5.

n

p = 20

p = 20

50
0.783(0.016)
Scenario 1 L-JIL
0.914(0.012)
V = 1.34 D-JIL
L-O-L
0.558(0.004)
K-O-L 0.335(0.008)
0.741(0.021)
Scenario 2 L-JIL
0.900(0.012)
V = 1.35 D-JIL
0.450(0.009)
L-O-L
K-O-L 0.115(0.019)
0.227(0.020)
Scenario 3 L-JIL
0.453(0.019)
V = 0.76 D-JIL
0.002(0.010)
L-O-L
K-O-L -0.268(0.026)
Scenario 4 L-JIL
0.553(0.013)
V = 1.28 D-JIL
0.612(0.014)
0.525(0.016)
L-O-L
K-O-L 0.236(0.007)
5.82(0.05)
5.57(0.06)
5.92(0.07)
6.70(0.02)

Scenario 5 L-JIL
V = 8.00 D-JIL
L-O-L
K-O-L

p = 20

p = 20

p = 20

100
0.832(0.016)
0.967(0.008)
0.574(0.004)
0.415(0.006)
0.854(0.020)
0.978(0.008)
0.448(0.006)
0.213(0.010)
0.268(0.013)
0.469(0.009)
-0.009(0.008)
-0.233(0.015)
0.564(0.011)
0.651(0.008)
0.458(0.010)
0.260(0.004)
6.41(0.02)
5.79(0.03)
6.75(0.03)
7.05(0.02)

200
1.080(0.014)
1.050(0.005)
0.600(0.005)
0.441(0.006)
1.180(0.007)
1.074(0.004)
0.447(0.005)
0.229(0.007)
0.372(0.008)
0.511(0.005)
-0.060(0.006)
-0.260(0.009)
0.630(0.011)
0.684(0.004)
0.375(0.004)
0.252(0.003)
6.80(0.01)
5.97(0.02)
7.32(0.02)
7.38(0.01)

400
1.259(0.002)
1.071(0.005)
0.597(0.005)
0.457(0.005)
1.266(0.001)
1.102(0.003)
0.429(0.004)
0.241(0.004)
0.432(0.003)
0.526(0.004)
-0.090(0.005)
-0.251(0.006)
0.806(0.006)
0.653(0.006)
0.300(0.002)
0.244(0.001)
7.02(0.01)
6.10(0.01)
7.66(0.01)
7.58(0.01)

800
1.297(0.001)
1.138(0.001)
0.583(0.005)
0.489(0.004)
1.299(0.001)
1.141(0.001)
0.410(0.003)
0.276(0.002)
0.511(0.002)
0.545(0.002)
-0.107(0.004)
-0.233(0.003)
0.882(0.002)
0.801(0.001)
0.237(0.001)
0.246(0.001)
7.16(0.01)
6.26(0.01)
7.81(0.01)
7.56(0.01)

This scenario is considered in Chen et al. (2016). Note that the outcome regression function

is continuous in both the baseline covariate and the treatment. With some calculations, one

can show that V opt = 8.

We apply the proposed L-JIL and D-JIL to estimate the optimal I2DR for Scenario 1-5,

with p = 20 and n ∈ {50, 100, 200, 400, 800}. The tuning parameters in JILs are speciﬁed

according to Section 3.4. Here, we set m = n/c with c = 10 to save computational cost.

In Section 5.3, we report results with c ∈ {6, 8} and ﬁnd the values under the estimated

I2DRs are very similar to those with c = 10.

To evaluate the proposed I2DRs, we compare its value function V ( (cid:98)d) with the values

under estimated optimal IDRs obtained by linear outcome weighted learning (L-O-L) and

the nonlinear outcome weighted learning based on the Gaussian kernel function (K-O-L).

Both methods were proposed by Chen et al. (2016). To implement L-O-L and K-O-L, we ﬁx

the parameter φn = 0.1, and select other tuning parameters by ﬁve-fold cross-validation, as

28

in Chen et al. (2016). All the value functions are evaluated via Monte Carlo simulations. The

average value function as well as its standard deviation over 200 replicates are summarized

in Table 2.

It can be seen from Table 2 that both L-JIL and D-JIL are very eﬃcient when Model

I (Scenarios 1-3) holds, and perform reasonably well when Model II (Scenario 4 and 5)

holds or the sample size is small. For instance, the proposed L-JIL achieves a value of

1.297 in Scenario 1 and 1.299 in Scenario 2 , when n = 800. These value is very close to

the optimal values, given by 1.34 and 1.35. In Scenario 3, the proposed D-JIL performs

consistently better than L-JIL, due to the capacity of deep neural networks in approximating

complicated non-linear relationships. In addition, it can be seen that D-JIL achieves the

best performance with small sample size in most cases. Moreover, the value of the proposed

I2DR increases with the sample size in most cases. This supports our theoretical ﬁndings in

Section 4.

In comparison, the value function under the estimated IDR using L-O-L or K-O-L is no

more than half of the optimal value, for each setting in Scenario 1 to 3. In Scenario 4, both

L-JIL and D-JIL achieve much larger value functions than outcome weighted learning. In

Scenario 5, L-O-L and K-O-L have better performance, as the true optimal decision rule is

linear and the outcome regression function is very sensitive to the change of the treatment

level a (by noticing that the coeﬃcient of the quadratic term in Scenario 5 is 10). When

n = 800, the two competing methods achieve similar value functions.

5.3 Choice of m

Recall that we set n = m/c for some constant c > 0. In Sections 5.2, we report our simulation

results with c = 10. In this section, we set c ∈ {6, 8} and report the corresponding results

in Tables 3 and 4 for L-JIL and D-JIL, respectively. We also include results with c = 10

for completeness. It can be seen that the value functions are very similar across diﬀerent

choices of c.

29

Table 3: The value function of the proposed I2DR under L-JIL for Scenario 1-5 with diﬀerent
choice of m = n/c.

Scenario 1
V = 1.34
p = 20
Scenario 2
V = 1.35
p = 20
Scenario 3
V = 0.76
p = 20
Scenario 4
V = 1.28
p = 20
Scenario 5
V = 8.00
p = 20

n
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10

50
0.813(0.019)
0.836(0.022)
0.783(0.016)
0.804(0.025)
0.857(0.029)
0.741(0.021)
0.280(0.023)
0.229(0.019)
0.227(0.020)
0.565(0.015)
0.563(0.015)
0.553(0.013)
5.81(0.05)
5.82(0.05)
5.82(0.05)

100
0.858(0.017)
0.870(0.018)
0.832(0.016)
0.891(0.021)
0.935(0.021)
0.854(0.020)
0.310(0.014)
0.325(0.014)
0.268(0.013)
0.561(0.012)
0.564(0.012)
0.564(0.011)
6.38(0.02)
6.40(0.02)
6.41(0.02)

200
1.027(0.014)
1.024(0.014)
1.080(0.014)
1.132(0.008)
1.123(0.009)
1.180(0.007)
0.339(0.008)
0.326(0.008)
0.372(0.008)
0.639(0.011)
0.627(0.011)
0.630(0.011)
6.78(0.01)
6.78(0.01)
6.80(0.01)

400
1.249(0.003)
1.238(0.002)
1.259(0.002)
1.257(0.002)
1.241(0.002)
1.266(0.001)
0.422(0.003)
0.417(0.003)
0.432(0.003)
0.818(0.006)
0.810(0.006)
0.806(0.006)
6.99(0.01)
7.02(0.01)
7.02(0.01)

800
1.289(0.001)
1.295(0.001)
1.297(0.001)
1.290(0.001)
1.299(0.001)
1.299(0.001)
0.504(0.002)
0.512(0.002)
0.511(0.002)
0.884(0.002)
0.882(0.002)
0.882(0.002)
7.09(0.01)
7.12(0.01)
7.16(0.01)

Table 4: The value function of the proposed I2DR under D-JIL for Scenario 1-5 with diﬀerent
choice of m = n/c.

Scenario 1
V = 1.34
p = 20
Scenario 2
V = 1.35
p = 20
Scenario 3
V = 0.76
p = 20
Scenario 4
V = 1.28
p = 20
Scenario 5
V = 8.00
p = 20

n
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10
c = 6
c = 8
c = 10

50
0.941(0.012)
0.973(0.016)
0.914(0.012)
0.943(0.013)
1.002(0.015)
0.900(0.012)
0.475(0.018)
0.416(0.019)
0.453(0.019)
0.624(0.014)
0.622(0.014)
0.612(0.014)
5.49(0.06)
5.58(0.05)
5.57(0.06)

100
0.972(0.008)
0.990(0.008)
0.967(0.008)
0.980(0.008)
1.012(0.008)
0.978(0.008)
0.480(0.009)
0.497(0.009)
0.469(0.009)
0.655(0.008)
0.651(0.008)
0.651(0.008)
5.69(0.03)
5.77(0.03)
5.79(0.03)

200
1.028(0.004)
1.030(0.004)
1.050(0.005)
1.037(0.004)
1.039(0.004)
1.074(0.004)
0.481(0.006)
0.493(0.006)
0.511(0.005)
0.686(0.004)
0.684(0.004)
0.684(0.004)
5.82(0.02)
5.91(0.02)
5.97(0.02)

400
1.065(0.004)
1.053(0.005)
1.071(0.005)
1.087(0.003)
1.076(0.003)
1.102(0.003)
0.493(0.004)
0.506(0.003)
0.526(0.004)
0.687(0.005)
0.676(0.005)
0.653(0.006)
5.97(0.01)
6.04(0.01)
6.10(0.01)

800
1.127(0.001)
1.136(0.001)
1.138(0.001)
1.129(0.001)
1.137(0.001)
1.141(0.001)
0.521(0.002)
0.532(0.002)
0.545(0.002)
0.801(0.001)
0.801(0.001)
0.801(0.001)
6.12(0.01)
6.20(0.01)
6.26(0.01)

30

6 Real Data Analysis

In this section, we illustrate the empirical performance of our proposed method on a real

data from the International Warfarin Pharmacogenetics Consortium. Warfarin is commonly

used for preventing thrombosis and thromboembolism. High doses of Warfarin are more

beneﬁcial than its lower doses, but may lead to a high risk of bleeding as well. Proper

dosing of Warfarin is thus of signiﬁcant importance.

We use the dataset provided by the International Warfarin Pharmacogenetics Consortium

(2009) for analysis. We choose 6 baseline covariates, including age, height, weight, gender,

the VKORC1.AG genotype and VKORC1.AA genotype. This yields a total of 3848 with

complete records of baseline information. The outcome is deﬁned as the absolute distance

between the international normalized ratio (INR, a measurement of the time it takes for

the blood to clot) after the treatment and the ideal value 2.5, i.e, Y = −|INR − 2.5|. We

use the min-max normalization to convert the range of the dose level A into [0, 1].

To implement L-JIL and D-JIL, we set m = n/5, and select γn and λn via cross validation,

as in Section 5.2. To further evaluate the empirical performance of the proposed I2DRs, we

compare their values with the value under the IDR estimated by K-O-L. Speciﬁcally, we

randomly select 70% of the data to compute the proposed I2DR and the IDR obtained by

K-O-L, and evaluate their value functions using the remaining dataset. We then iterate this

procedure 50 times to calculate the average value function. For each iteration, the value

function is estimated based on the nonparametric estimator proposed by Zhu et al. (2020).
Speciﬁcally, let Gtest denote observations in the testing dataset. For the IDR (cid:101)d computed

by K-O-L, we consider the following nonparametric estimator for its value function,

(cid:101)V ( (cid:101)d) =

(cid:90)

(cid:80)

i∈Gtest

(cid:80)

x

i∈Gtest

YiK(h−1
K(h−1

x (x − Xi))K(h−1
x (x − Xi))K(h−1

a ( (cid:101)d(x) − Ai))
a ( (cid:101)d(x) − Ai))

(cid:26) (cid:88)

i∈Gtest

K(h−1

x (x − Xi))
|Gtest|hp
x

(cid:27)

dx,

where K(·) denotes the Gaussian kernel function, and hx and ha are some bandwidth

parameters. The tuning parameters hx and ha are chosen according to the numerical results

in Section 5 of Zhu et al. (2020).

To evaluate the value function under the proposed I2DR (cid:98)d(·), we set π∗ to be a uniform

31

Table 5: The regression coeﬃcients associated with three subintervals for the proposed I2DR
under L-JIL.

Intercept Age Weight Height Gender VKORC1.AG VKORC1.AA

(cid:98)θ1
(cid:98)θ2
(cid:98)θ3

-1.673
-1.741
-0.488

0.025
0.029
0.012

0.006
0.004
0.001

0.006
0.006
0.001

-0.158
-0.201
-0.033

-0.364
0.057
-0.002

-0.349
-0.051
-0.120

density function and compute (cid:101)V π∗( (cid:98)d), deﬁned as

(cid:90)

(cid:90)

x

(cid:98)d(x)

1
| (cid:98)d(x)|

(cid:80)
(cid:80)

i∈Gtest

i∈Gtest

YiK(h−1
K(h−1

x (x − Xi))K(h−1
x (x − Xi))K(h−1

a (d(x) − Ai))
a (d(x) − Ai))

(cid:26) (cid:88)

i∈Gtest

K(h−1

x (x − Xi))
|Gtest|hp
x

(cid:27)

dadx.

The integration is calculated via a midpoint rule with a uniform grid.

Over 50 iterations, the average value functions of our proposed I2DRs computed by L-JIL

and D-JIL are −0.332 and −0.331, larger than the value −0.344 of the IDR obtained by

K-O-L. In the following, we focus on one particular iteration in L-JIL and D-JIL, respectively,

to further interpret our results.

Finally, we show the results of one particular iteration under L-JIL whose value achieves

the largest as −0.307, to illustrate the interpretability of the resulting I2DR. Under this

iteration, L-JIL partitions [0, 1] into three subintervals: [0, 0.05), [0.05, 0.17) and [0.17, 1].

Let (cid:98)θ1, (cid:98)θ2 and (cid:98)θ3 denote the corresponding regression coeﬃcients associated with these three

subintervals. We report these estimators in Table 5. According to Table 5, the proposed

I2DR under L-JIL gives us a clear interpretation about the eﬀect of baseline information on

the dose assignment rule. For instance, patients whose genotype VKORC1 is AG or AA

are more likely to receive low doses of Warfarin to prevent bleeding; older patients with

larger weight shall be treated with higher dose levels. Future experiments are warranted to

conﬁrm these scientiﬁc ﬁndings. In addition, we also explore one particular iteration under

D-JIL whose value nearly achieves the largest as −0.315. Under this iteration, the I2DR

under D-JIL recommends most patients to receive dose level in [0, 0.05), [0.15, 0.20) and

[0.20, 1], respectively. This ﬁnding accords with the partition results under L-JIL.

32

7 Proof of Theorem 1

We provide the proof for Theorem 1 in this section. We present an outline of the proof ﬁrst.

Let δmin = minI∈P0 |I|/3 > 0. We divide the proof into four parts. In Part 1, we show that
the following event occurs with probability at least 1 − O(n−2),

max
τ ∈J(P0)

min
ˆτ ∈J( (cid:98)P)

|ˆτ − τ | < δmin.

By the deﬁnition of δmin, this implies that

Pr(| (cid:98)P| ≥ |P0|) ≥ 1 − O(n−2).

In Part 2, we show that

max
τ ∈J(P0)

min
ˆτ ∈J( (cid:98)P)

|ˆτ − τ | = O(n−1 log n),

(16)

(17)

(18)

with probability at least 1 − O(n−2). This proves (ii) in Theorem 1. In Part 3, we prove

Pr(| (cid:98)P| ≤ |P0|) ≥ 1 − O(n−2).

(19)

This together with equation 17 proves (i) in Theorem 1. In the last part, we show (iii) holds.

In the following, we ﬁrst introduce some notations and auxiliary lemmas. Then, we

present the proofs for Part 1, 2, 3 and 4.

Notations and technical lemmas: For any interval I ⊆ [0, 1], deﬁne

(cid:98)θI =

(cid:32)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)X iX

(cid:62)

i + λn|I|Ep+1

(cid:33)−1 (cid:32)

1
n

n
(cid:88)

i=1

(cid:33)

I(Ai ∈ I)X iYi

,

(cid:16)

θ0,I =

EI(A ∈ I)XX

(cid:62)(cid:17)−1

{EI(A ∈ I)XY },

where X = (1, X (cid:62))(cid:62). It is immediate to see that the deﬁnition of (cid:98)θI here is consistent with
the one deﬁned in equation 3 for any I ∈ (cid:98)P. In addition, under the model assumption in

equation 13, the deﬁnition of θ0,I here is consistent with the one deﬁned in step function
model θ0(a) = (cid:80)

θ0,II(a ∈ I) for any I ∈ P0.

I∈P0

Let I(m) denote the set of intervals

I(m) = {[i1/m, i2/m) : for some integers i1 and i2 that satisfy 0 ≤ i1 < i2 < m}

∪ {[i3/m, 1] : for some integers i3 that satisfy 0 ≤ i3 < m}.

33

Let {τ0,k}K−1

k=1 with 0 < τ0,1 < τ0,2 < · · · < τ0,K−1 < 1 be the locations of the true change

points of θ0(·). Set τ0,0 = 0, τ0,K = 1. We introducing the following lemmas.

Lemma 1 Assume conditions in Theorem 1 are satisﬁed. Then there exist some constants

¯c0 > 0, c0 ≥ 1 such that the following events occur with probability at least 1 − O(n−2): for
any interval I ∈ I(m) that satisﬁes |I| ≥ ¯c0n−1 log n, we have

1
n

1
n

i=1
n
(cid:88)

i=1
n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
n

1
n

(cid:107)(cid:98)θI − θ0,I(cid:107)2 ≤

√

log n
c0
(cid:112)|I|n

,

n
(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

c0

(cid:112)|I| log n
√
n

,

≤

I(Ai ∈ I){Yi − X

(cid:62)

θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

I(Ai ∈ I)[X

(cid:62)
i {θ0(Ai) − θ0,I}]2 ≥

(cid:107)θ0(a) − θ0,I(cid:107)2

2da −

i=1
n
(cid:88)

i=1

I(Ai ∈ I)(|Yi|2 + (cid:107)X i(cid:107)2

2) ≤ c0

(cid:90)

1
c0
(cid:32)(cid:112)|I| log n
√

I

n

(cid:33)

+ |I|

.

In addition, for any I ∈ [0, 1], we have

(cid:107)θ0,I(cid:107)2 ≤ c0.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c0

(cid:112)|I| log n
√
n

,

≤

c0

(cid:112)|I| log n
√
n

,

(20)

(21)

(22)

(23)

(24)

(25)

(cid:4)

Lemma 2 Assume conditions in Theorem 1 are satisﬁed. Then there exist some constants

¯c1 > 0, c1 ≥ 1 such that the following events occur with probability at least 1 − O(n−2): for
any interval I ∈ I(m) that satisﬁes (cid:82)
I (cid:107)θ0(a) − θ0,I(cid:107)2

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1
(cid:115)

2da ≥ ¯c1n−1 log n,
(cid:12)
(cid:12)
(cid:62)
(cid:12)
i {θ0(Ai) − θ0,I}
(cid:12)
(cid:12)

≤ c1

n

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n,

(26)

n
(cid:88)

I(Ai ∈ I)[X

(cid:62)
i {θ0(Ai) − θ0,I}]2

i=1
(cid:90)

n
c1

I

≥

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − c1

(cid:115)

(cid:90)

n

I

34

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n.

(27)

Lemma 3 Assume conditions in Theorem 1 are satisﬁed. Then for suﬃciently large

n and any interval I ⊆ [0, 1] of the form [i1, i2) or [i1, i2] with i2 = 1 that satisﬁes
(cid:82)
I (cid:107)θ0(a) − θ0,I(cid:107)2
2da = cn for some sequence {cn}n such that cn ≥ 0, ∀n and cn → 0 as
n → ∞, we have either τ0,k−1 ≤ i1 ≤ i2 ≤ τ0,k for some integer k such that 1 ≤ k ≤ K or

(cid:4)

τ0,k−2 ≤ i1 < τ0,k−1 < i2 ≤ τ0,k and min
j∈{1,2}

|ij − τ0,k−1| ≤ c2cn,

for some integer k such that 2 ≤ k ≤ K and some constant c2 > 0, or

τ0,k−3 ≤ i1 < τ0,k−2 < τ0,k−1 < i2 ≤ τ0,k and max
j∈{1,2}

|ij − τ0,k−3+j| ≤ ¯c2cn,

for some integer k such that 3 ≤ k ≤ K and some constant c2 > 0.

In addition, the following events occur with probability at least 1 − O(n−2): for any

interval I ∈ I(m) that satisﬁes (cid:82)

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da ≤ ¯c1n−1 log n, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ¯c2 log n,

for some constant ¯c2 > 0.

(28)

(cid:4)

Lemma 4 Under the conditions in Theorem 1, the following events occur with probability
at least 1 − O(n−2): there exists some constant ¯c3 > 0 such that minI∈ (cid:98)P |I| ≥ ¯c3γn.
Part 1: Assume |P0| > 1. Otherwise, equation 16 trivially hold. Consider the partition

(cid:4)

P = {[0, 1]} which consists of a single interval and a zero vector 0p+1. By deﬁnition, we

have

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P
n
(cid:88)

i=1

≤

i=1

(Yi − X

(cid:62)
i 0p+1)2 + nλn(cid:107)0p+1(cid:107)2

2 + nγn =

n
(cid:88)

i=1

Y 2
i + nγn.

In view of equation 24, we obtain with probability at least 1 − O(n−2),

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn(| (cid:98)P| − 1) ≤ c0n

(cid:18) √

log n
√
n

(cid:19)

+ 1

.

35

This implies that under the event deﬁned in equation 24, we have for suﬃciently large n,
(cid:18) √

(cid:19)

γn(| (cid:98)P| − 1) ≤ c0

log n
√
n

+ 1

,

and hence

for suﬃciently large n.

| (cid:98)P| ≤ 2c0γ−1
n ,

(29)

Under the event deﬁned in Lemma 4, we have minI∈ (cid:98)P |I| ≥ ¯c0n−1 log n for suﬃciently
large n, since γn (cid:29) n−1 log n. Thus, with probability at least 1 − O(n−2), the events deﬁned

in equation 20-equation 24 hold for any interval I ∈ (cid:98)P.

Notice that

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

(30)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nγn| (cid:98)P| ≥ nγn| (cid:98)P| +

(cid:88)

n
(cid:88)

i=1

I∈ (cid:98)P
(cid:124)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2

I(Ai ∈ I){X

(cid:62)
i ((cid:98)θI − θ0,I)}2

− 2

(cid:123)(cid:122)
η2

(cid:125)

(cid:124)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:88)

I∈ (cid:98)P

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X

(cid:123)(cid:122)
η3

(cid:123)(cid:122)
η1

(cid:62)
i ((cid:98)θI − θ0,I)

(cid:125)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

.

≥

+

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

(cid:88)

n
(cid:88)

i=1

I∈ (cid:98)P
(cid:124)

By equation 20 and equation 21, we obtain that

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

η3 ≤ 2

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X i

I∈ (cid:98)P

(cid:88)

i=1
0 log n ≤ 2c2

2c2

≤

0 log n| (cid:98)P|,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:107)(cid:98)θI − θ0,I(cid:107)2

(31)

I∈ (cid:98)P

with probability at least 1 − O(n−2). Since γn (cid:29) n−1 log n, η2 ≥ 0, for suﬃciently large n,
we have with probability at least 1 − O(n−2),

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P| ≥ η1.

(32)

36

Notice that

(cid:88)

n
(cid:88)

η1 =

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai) + X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}2

=

(cid:88)

I∈ (cid:98)P
(cid:124)

i=1

I∈ (cid:98)P
n
(cid:88)

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}2

+

i=1

(cid:123)(cid:122)
η4

(cid:125)

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}2

+ 2

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}{X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}.

Under the events deﬁned in equation 22 and equation 23, it follows that

η1 ≥ η4 + n

≥ η4 + n

1
c0

1
c0

(cid:90)

I
(cid:90)

I

(cid:88)

I∈ (cid:98)P
(cid:88)

I∈ (cid:98)P

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 2c0

(cid:88)

(cid:112)|I|n log n

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 2c0

I∈ (cid:98)P
(cid:113)

| (cid:98)P|n log n,

where the last inequality is due to Cauchy-Schwarz inequality. By equation 29 and the

condition that γn (cid:29) n−1 log n, we obtain

η1 ≥ η4 + n

1
c0

(cid:88)

I∈ (cid:98)P

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da + o(n),

(33)

with probability at least 1 − O(n−2). Notice that

η4 =

n
(cid:88)

i=1

{Yi − X

(cid:62)
i θ0(Ai)}2 =

(cid:88)

n
(cid:88)

I∈P0

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2.

Combining equation 32 with equation 33, we’ve shown that

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

≥

(cid:88)

n
(cid:88)

I∈P0

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + n

1
c0

(cid:88)

I∈ (cid:98)P

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da + o(n),

with probability at least 1−O(n−2). By equation 25 and the condition that λn = O(n−1 log n),

37

γn = o(1), this further implies

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P

(cid:88)

i=1
(cid:32) n

(cid:88)

≥

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + nλn|I|(cid:107)θ0,I(cid:107)2
2

(cid:33)

+ nγn|P0|

I∈P0
(cid:88)

+ n

I∈ (cid:98)P

i=1
(cid:90)
1
c0

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da + o(n).

(34)

that satisﬁes τ ∗

For any integer k such that 1 ≤ k ≤ K − 1, let τ ∗

0,k = i/m for some integer i and that |τ0,k − τ ∗

P ∗ the oracle partition formed by the change point locations {τ ∗

0,k be the change point location
0,k| < m−1. Denoted by
k=1 . Set τ ∗
0,0 = 0,
0,K = 1 and θ∗
τ ∗
0,k) = θ0,[τ0,k−1,τ0,k) for 1 ≤ k ≤ K − 1 and θ∗
0,K−1,1] = θ0,[τ0,K−1,1]. Let
[τ ∗
[τ ∗
0,k−1,τ ∗
0,K−1, 1] ∩ [τ0,K−1, 1]c.
0,k) ∩ [τ0,k−1, τ0,k)c for 1 ≤ k ≤ K − 1 and ∆K = [τ ∗
0,k−1, τ ∗
∆k = [τ ∗
The length of each interval ∆k is at most m−1. Since m (cid:16) n, we have m−1 (cid:28) ¯c0n−1 log n.

0,k}K−1

For any k and suﬃciently large n, we can ﬁnd an interval I ∈ I(m) with length between

¯c0n−1 log n and 2¯c0n−1 log n that covers ∆k. It follows that

(cid:40)

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P ∗|

(35)

(cid:40)

−

I∈P ∗

i=1

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + nλn|I|(cid:107)θ0,I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P0|

I∈P0

i=1

≤ nλn sup
I⊆[0,1]

(cid:107)θ0,I(cid:107)2

2 +

K
(cid:88)

n
(cid:88)

k=1

i=1

(cid:32)

(cid:33)

I(Ai ∈ ∆k)

Y 2
i + sup
I⊆[0,1]

(cid:107)θ0,I(cid:107)2

2(cid:107)X i(cid:107)2
2

≤ nλn sup
I⊆[0,1]

(cid:107)θ0,I(cid:107)2

2 + K

sup
I∈I(m)

1≤¯c−1

0 |I|n log−1 n≤2

(cid:32)

(cid:33)

I(Ai ∈ I)

Y 2
i + sup
I⊆[0,1]

(cid:107)θ0,I(cid:107)2

2(cid:107)X i(cid:107)2
2

.

n
(cid:88)

i=1

Since λn = O(n−1 log n), combining equation 35 together with equation 24 and equation 25,
we obtain with probability at least 1 − O(n−2),
(cid:32) n

(cid:41)

(cid:40)

(cid:33)

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:40)

I∈P ∗

(cid:88)

i=1
(cid:32) n

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + nλn|I|(cid:107)θ0,I(cid:107)2
2

−

+ nγn|P ∗|

(cid:33)

(cid:41)

+ nγn|P0|

i=1

I∈P0
0nλn + K(c2

≤ c2

0 + 1)c0(

√

2¯c0 + 2¯c0) log n = O(log n) = o(n).

(36)

38

By deﬁnition, we have

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P

(cid:88)

i=1
(cid:32) n

(cid:88)

I∈P ∗

i=1

≤

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

+ nγn|P ∗|.

In view of equation 34 and equation 36, we obtain that

(cid:90)

(cid:88)

I

I∈ (cid:98)P

(cid:107)θ0(a) − θ0,I(cid:107)2

2da = o(1),

(37)

with probability at least 1 − O(n−2). We now show equation 16 holds under the event

deﬁned in equation 37. Otherwise, there exists some τ0 ∈ J(P0) such that |ˆτ − τ0| ≥ δmin,

for all ˆτ ∈ J( (cid:98)P). Under the event deﬁned in equation 37, we obtain that

(cid:90) τ0+δmin

τ0−δmin

(cid:107)θ0(a) − θ0,[τ0−δmin,τ0+δmin)(cid:107)2

2da = o(1).

(38)

On the other hand, since θ0(a) is a constant function on [τ0 − δmin, τ0) or [τ0, τ0 + δmin), we

have

where

(cid:90) τ0+δmin

τ0−δmin

≥ min
θ∈Rp+1
δmin
2

≥

(cid:107)θ0(a) − θ0,[τ0−δmin,τ0+δmin)(cid:107)2

2da

(cid:0)δmin(cid:107)θ0,[τ0−δmin,τ0) − θ(cid:107)2

2 + δmin(cid:107)θ0,[τ0,τ0+δmin) − θ(cid:107)2
2

(cid:1)

(cid:107)θ0,[τ0−δmin,τ0) − θ0,[τ0,τ0+δmin)(cid:107)2

2 ≥

δminκ2
0
2

,

κ0 ≡

min
I1,I2∈P0
I1 and I2 are adjacent

(cid:107)θ0,I1 − θ0,I2(cid:107)2 > 0.

This apparently violates equation 38. equation 16 thus holds with probability at least

1 − O(n−2).

Part 2: By equation 30 and equation 31, we have with probability at least 1 − O(n−2) that

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P| ≥ η1 + nγn| (cid:98)P| − 2c2

0| (cid:98)P| log n.

39

Notice that

η1 = η4 + 2

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}{X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}

+

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}2.

Denoted by T(m) the set of intervals I ∈ I(m) with (cid:82)

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ ¯c1n−1 log n.

Under the events deﬁned in Lemma 2 and 3, we have

η1 ≥ η4 + 2

(cid:88)

n
(cid:88)

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}{X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}

I∈ (cid:98)P

(cid:88)

i=1

n
(cid:88)

I(Ai ∈ I){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ0,I}2 ≥ η4 − 2¯c2| (cid:98)P| log n

I∈ (cid:98)P,I∈T(m)

(cid:88)

I∈ (cid:98)P,I∈T(m)

i=1
(cid:32)

n
c1

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 3c1

n

(cid:115)

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n

.

(cid:33)

(cid:90)

I

+

+

To summarize, we’ve shown that with probability at least 1 − O(n−2),

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

(cid:88)

≥

I∈ (cid:98)P,I∈T(m)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

(cid:32)

n
c1

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 3c1

n

(cid:115)

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n

(39)

(cid:33)

+ η4 + nγn| (cid:98)P| − 2(c2

0 + ¯c2)| (cid:98)P| log n.

It follows from equation 35 and equation 36 that

(cid:107)θ0,I(cid:107)2

2 + nγn|P0|

η4 + nλn sup
I∈P0

(cid:40)

(cid:32) n

(cid:88)

(cid:88)

≥

I∈P ∗

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P ∗|

− c∗

0 log n,

for some constants c∗

i=1
0 > 0, with probability at least 1 − O(n−2). By equation 25 and the

condition that λn = O(n−1 log n), there exists some constant c∗

1 > c∗

0 such that

η4 + nγn|P0|
(cid:40)

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

≥

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P ∗|

− c∗

1 log n,

I∈P ∗

i=1

40

with probability at least 1 − O(n−2).

In view of equation 39, we’ve shown that with

probability at least 1 − O(n−2),

(cid:32) n

(cid:88)

(cid:88)

I∈ (cid:98)P

i=1

(cid:88)

I∈ (cid:98)P,I∈T(m)
(cid:32) n
(cid:40)

(cid:88)

(cid:88)

≥

+

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

(cid:32)

n
c1

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 3c1

n

(cid:115)

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P ∗|

(40)

(cid:33)

I∈P ∗
+ nγn| (cid:98)P| − (2c2

i=1

0 + 2¯c2)| (cid:98)P| log n − c∗

1 log n − nγn|P0|.

By deﬁnition,

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P

(cid:88)

i=1
(cid:32) n

(cid:88)

≤

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

+ nγn|P ∗|.

i=1
Thus, we have with probability at least 1 − O(n−2),

I∈P ∗

(cid:88)

I∈ (cid:98)P,I∈T(m)

(cid:32)

n
c1

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − 3c1

n

(cid:115)

(cid:107)θ0(a) − θ0,I(cid:107)2

2da log n

(cid:33)

(cid:90)

I

≤ (2c2

0 + 2¯c2)| (cid:98)P| log n + c∗

1 log n + nγn|P0| − nγn| (cid:98)P|,

and hence,

(cid:88)

(cid:32)(cid:115)(cid:90)

I

n
c1

(cid:107)θ0(a) − θ0,I(cid:107)2

2da −

(cid:33)2

n−1/2(cid:112)log n

3c1
2

(41)

I∈ (cid:98)P,I∈T(m)
≤ (2c2

0 + 2¯c2 + 9c1/4)| (cid:98)P| log n + c∗

1 log n + nγn|P0| − nγn| (cid:98)P|.

Under the event deﬁned in equation 17, we have either | (cid:98)P| ≥ 2|P0|, or |P0| ≤ | (cid:98)P| ≤ 2|P0|.

When | (cid:98)P| ≥ 2|P0|, it follows from the condition nγn (cid:29) log n that for suﬃciently large n,
γn/4 ≥ 2c2

0 + 2¯c2 + 9c1/4, n|P0|γn ≥ 2c∗

1 log n and hence

(2c2

0 + 2¯c2 + 9c1/4)| (cid:98)P| log n + c∗

1 log n + nγn|P0| − nγn| (cid:98)P|

≤ (2c2

0 + 2¯c2 + 9c1/4)| (cid:98)P| log n + c∗

1 log n − nγn| (cid:98)P|/4 − nγn|P0|/2

≤ (2c2

0 + 2¯c2 + 9c1/4)| (cid:98)P| log n − nγn| (cid:98)P|/4 ≤ 0,

41

When |P0| ≤ | (cid:98)P| ≤ 2|P0|, we have

(2c2

0 + 2¯c2 + 9c1/4)| (cid:98)P| log n + c∗

1 log n + nγn|P0| − nγn| (cid:98)P|

≤ 2(2c2

0 + 2¯c2 + 9c1/4)|P0| log n + c∗

1 log n.

In view of equation 41, we have with probability at least 1 − O(n−2),

(cid:88)

I∈ (cid:98)P,I∈T(m)

(cid:32)(cid:115)(cid:90)

I

n
c1

(cid:107)θ0(a) − θ0,I(cid:107)2

2da −

(cid:33)2

n−1/2(cid:112)log n

3c1
2

≤ c log n,

for some constant c > 0. Thus, with probability at least 1 − O(n−2), we have

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da = O(n−1 log n),

for any I ∈ (cid:98)P ∩ T(m). By the deﬁnition of T(m), we obtain that with probability at least
1 − O(n−2),

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da = O(n−1 log n), ∀I ∈ (cid:98)P.

(42)

Consider a given change point τ ∈ P0, there exists an interval I ∈ (cid:98)P of the form [i1, i2)

or [i1, i2] with i2 = 1 such that i1 ≤ τ < i2. Under the event deﬁned in equation 42, it follows
from Lemma 3 such that min(|i1 − τ |, |i2 − τ |) = O(n−1 log n). This proves equation 18.

Part 3: Using similar arguments in proving equation 28, we can show that the following

events occur with probability at least 1 − O(n−2): for any interval I ∈ (cid:98)P, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C log n,

for some constant C > 0.

By equation 42, using similar arguments in proving equation 39 and equation 40, we

can show the following event occurs with probability at least 1 − O(n−2),

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P
(cid:40)

(cid:88)

i=1
(cid:32) n

(cid:88)

I∈P ∗

i=1

≥

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

(cid:41)

+ nγn|P ∗|

+ nγn| (cid:98)P| − C| (cid:98)P| log n − C log n − nγn|P0|,

42

for some constant C > 0. By deﬁnition,

(cid:32) n

(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nλn|I|(cid:107)(cid:98)θI(cid:107)2
2

(cid:33)

+ nγn| (cid:98)P|

I∈ (cid:98)P

(cid:88)

i=1
(cid:32) n

(cid:88)

I∈P ∗

i=1

≤

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗

I)2 + nλn|I|(cid:107)θ∗

I(cid:107)2
2

(cid:33)

+ nγn|P ∗|.

Thus, we have with probability at least 1 − O(n−2),

nγn| (cid:98)P| − C| (cid:98)P| log n − C log n − nγn|P0| ≤ 0.

Since γn (cid:29) n−1 log n, the above event occurs only when | (cid:98)P| ≤ |P0|. To see this, notice that
if | (cid:98)P| > |P0|, we have

nγn − C log n −

C log n
| (cid:98)P|

− nγn

|P0|
| (cid:98)P|

≥ nγn − C log n −

C log n
|P0| + 1

− nγn

|P0|
|P0| + 1

=

nγn
|P0| + 1

− C log n −

C log n
|P0| + 1

(cid:29) 0,

since γn (cid:29) n−1 log n. This proves equation 19.

Part 4: In the ﬁrst three parts, we’ve shown that

| (cid:98)P| = |P0| and max
τ ∈J(P0)

min
ˆτ ∈J( (cid:98)P)

|ˆτ − τ | = O(n−1 log n),

(43)

with probability tending to 1. For suﬃciently large n, the event deﬁned in equation 43

implies that |I| ≥ ¯c0n−1 log n for any I ∈ (cid:98)P. Thus, it follows from Lemma 1 that the
following occurs with probability at least 1 − O(n−2): for any I ∈ (cid:98)P, we have

|I|(cid:107)(cid:98)θI − θ0,I(cid:107)2

2 ≤ c2

0n−1 log n.

(44)

Under the events deﬁned in equation 42, equation 43 and equation 44, we have

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da =

(cid:107)(cid:98)θI − θ0(a)(cid:107)2

2da =

(cid:107)(cid:98)θI − θ0,I + θ0,I − θ0(a)(cid:107)2

2da

(cid:90) 1

0
(cid:90)

(cid:88)

=

(cid:90)

(cid:88)

I

I∈ (cid:98)P
(cid:90)

(cid:88)

I

I∈ (cid:98)P
(cid:88)

(cid:90)

I

I∈ (cid:98)P

≤ 2

I∈ (cid:98)P

(cid:107)(cid:98)θI − θ0,I(cid:107)2

2da + 2

(cid:88)

I∈ (cid:98)P

I
(cid:90)

I

(cid:107)(cid:98)θI − θ0,I(cid:107)2

2da +

(cid:107)θ0,I − θ0(a)(cid:107)2

2da + 2

((cid:98)θ0,I − θ0,I)(cid:62){θ0,I − θ0(a)}da

(cid:107)θ0,I − θ0(a)(cid:107)2

2da = O(| (cid:98)P|n−1 log n) = O(|P0|n−1 log n),

(cid:90)

(cid:88)

I

I∈ (cid:98)P
(cid:88)

(cid:90)

I

I∈ (cid:98)P

where the ﬁrst inequality is due to Cauchy-Schwarz inequality. This proves (iii). The proof

is hence completed.

43

8 Discussions

8.1 Diverging Number of Change Points

Under Model I, we assume |P0| is ﬁxed to simplify the results in Theorems 1, 2, 3, and 4.

Our theoretical results can be generalized to the situation where |P0| diverges with n as

well. Take L-JIL as an example. Similar to Theorem 6, we can show that the (cid:96)2 integrated
loss satisﬁes (cid:82) 1
2da = Op(|P0|n−1 log n). Compared to the results in Theorem
1, the convergence rate here is slower by a factor |P0|. In addition, |P0| = o(n/ log n) is

0 (cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

required to guarantee the consistency of (cid:98)θ.

We next present more technical details. In the proof of Theorem 6 (see Section B.12 for

details), we consider a more general framework and establish the (cid:96)2 integrated loss of (cid:98)θ(·)
by assuming θ0 satisﬁes lim supk→∞ kα0AEk(θ0) < ∞ where

AEk(θ0) =

inf
P:|P|≤k+1

(θI )I∈P ∈(cid:81)

Rp+1

I∈P

(cid:40)

sup
a∈[0,1]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

θ0(a) −

(cid:13)
(cid:13)
θII(a ∈ I)
(cid:13)
(cid:13)
(cid:13)2

(cid:41)

.

(cid:88)

I∈P

By deﬁnition, AEk(θ0) describes how well θ0(·) can be approximated by a step function.

When θ0(·) is a step function with number of jumps equal to |P0|, we have AEk(θ0) = 0
for any k ≥ |P0|. As a result, θ0 satisﬁes the condition lim supk→∞ kα0AEk(θ0) < ∞ for any
α0 > 0. As a result, the assertion equation 148 in the proof of Theorem 6 also holds for
θ0(·) and we have with probability at least 1 − O(n−2) that

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da ≤ O(1)(|P0|−α0 + γn|P0|),

where O(1) denotes some positive constant. As |P0| → ∞ and α0 can be made arbitrarily

large, we have with probability at least 1 − O(n−2) that

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da ≤ O(1)(γn|P0|),

where O(1) denotes some positive constant.

In Theorem 6, we require γn (cid:29) n−1 log n. However, this condition can be relaxed to
γn ≥ M0n−1 log n for some suﬃciently large constant M0 > 0. Under the latter condition,

we have

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da = Op(|P0|n−1 log n).

44

This yields the convergence rate of the (cid:96)2 integrated loss of (cid:98)θ(·).

8.2 Potential alternative approaches

In this paper, we focus on modeling the outcome regression function to derive I2DR. Below,

we outline two other potential approaches and discuss their weaknesses.

8.2.1 A-learning Type Methods

Let’s assume qI(·) satisﬁes equation 1 and the partition P0 is known to us. In order to

eliminate the baseline function u0(·), we can apply Robinson’s transformation (see for

example, Robinson, 1988; Zhao et al., 2017; Chernozhukov et al., 2018, and the references

therein) and compute (cid:101)qI by minimizing

arg min
{qI ∈QI :I∈P0}

1
n

n
(cid:88)

i=1

[Yi − (cid:98)µ(Xi) −

(cid:88)

I∈P0

{I(Ai ∈ I) − (cid:98)e(I|Xi)}qI(X)]2,

where (cid:98)µ(x) correspond tos some nonparametric estimators for E(Y |X = x). Both (cid:98)µ and (cid:98)e can
be obtained by some generic machine learning methods with good prediction performance.

When P0 is unknown, one might consider estimating P0 and {qI : I ∈ P0} jointly by

1
n

(cid:34)

n
(cid:88)

i=1

Yi − (cid:98)µ(Xi) −

(cid:88)

I∈P

arg min
P∈B(m),
{qI ∈QI :I∈P}

{I(Ai ∈ I) − (cid:98)e(I|Xi)}qI(Xi)

+ γn|P|,

(cid:35)2

for some tuning parameter γn. However, diﬀerent from the objective function in equation 3,

for a given partition P, all the functions {qI : I ∈ P} need to be jointed estimated. As a

result, standard change point detection algorithms such as dynamic programming or binary

segmentation (Scott and Knott, 1974) cannot be applied. Exhaustive search among all

possible partitions is computationally infeasible. It remains unknown how to eﬃciently

solve the above optimization problem. We leave it for future research.

8.2.2 Policy Search

As commented in Section 2.2, to apply value search, we need to specify a preference function
π∗. To better illustrate the idea, let us suppose π∗(·; x, I) = p(a|x)/ (cid:82)
x(cid:48)∈I p(a|x(cid:48))dx(cid:48). That
is, the preference function is the same as the one we observe in our data. Then, for a

45

given I2DR d, we can consider the following inverse propensity score weighted estimator for
V π∗(d),

(cid:98)V π∗(d) =

1
n

n
(cid:88)

i=1

I(Ai ∈ d(Xi))
(cid:98)e(d(Xi)|Xi)

Yi.

For a given partition P, let DP denote the space of I2DRs that we consider. Then (cid:98)d can

be computed by maximizing

arg max
P∈B(m)

arg max
d∈DP

= arg max
P∈B(m)

arg max
d∈DP

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

Yi

I(Ai ∈ d(Xi))
(cid:98)e(d(Xi)|Xi)
(cid:88)

I(Ai ∈ I)
(cid:98)e(d(Xi)|Xi)

i=1

I∈P

YiI(d(Xi) = I).

Suppose we consider the class of linear decision rules, i.e.,

DP = {d : d(x) = arg max

I∈P

θ(cid:62)
I ¯x}.

It suﬃces to maximize

arg max
P∈B(m)
{qI ∈QI :I∈P}

1
n

n
(cid:88)

(cid:88)

i=1

I∈P

I(Ai ∈ I)
(cid:98)e(d(Xi)|Xi)

YiI{d(Xi) = arg max

I∈P

qI(Xi)}.

Similar to Section 8.2.1, for a given partition P, all the functions {qI : I ∈ P} need to

be jointed estimated. As a result, dynamic programming cannot be applied. It remains

unknown how to eﬃciently solve the above optimization problem. We leave it for future

research.

8.3 Other penalty functions in L-JIL

In L-JIL, We use a ridge penalty in equation 6 to prevent overﬁtting in large p problems.

When the true regression coeﬃcient θ0(·) is suﬃciently sparse, one can consider replacing

the ridge penalty with the LASSO Tibshirani (1996) to improve the estimation accuracy.

However, optimizing the resulting objective function requires to compute the LASSO

estimator m(m − 1)/2 times. This is far more computationally expensive than the proposed

method. It remains unknown whether the computation can be simpliﬁed. We leave it for

future research.

46

References

Adamczak, R. et al. (2008), ‘A tail inequality for suprema of unbounded empirical processes

with applications to markov chains’, Electronic Journal of Probability 13, 1000–1034.

Audibert, J.-Y. and Tsybakov, A. B. (2007), ‘Fast learning rates for plug-in classiﬁers’, Ann.

Statist. 35(2), 608–633.

Boysen, L., Kempe, A., Liebscher, V., Munk, A., Wittich, O. et al. (2009), ‘Consistencies

and rates of convergence of jump-penalized least squares estimators’, The Annals of

Statistics 37(1), 157–183.

Chakraborty, B., Murphy, S. and Strecher, V. (2010), ‘Inference for non-regular parameters

in optimal dynamic treatment regimes’, Stat. Methods Med. Res. 19(3), 317–343.

Chen, G., Zeng, D. and Kosorok, M. R. (2016), ‘Personalized dose ﬁnding using outcome

weighted learning’, Journal of the American Statistical Association 111(516), 1509–1521.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C. and Newey, W.

(2017), ‘Double/debiased/neyman machine learning of treatment eﬀects’, American

Economic Review 107(5), 261–65.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W. and

Robins, J. (2018), ‘Double/debiased machine learning for treatment and structural

parameters’, Econom. J. 21(1), C1–C68.

Chernozhukov, V., Chetverikov, D., Kato, K. et al. (2014), ‘Gaussian approximation of

suprema of empirical processes’, The Annals of Statistics 42(4), 1564–1597.

Chernozhukov, V., Demirer, M., Lewis, G. and Syrgkanis, V. (2019), ‘Semi-parametric eﬃ-

cient policy learning with continuous actions’, Advances in Neural Information Processing

Systems 32, 15065–15075.

Consortium, I. W. P. (2009), ‘Estimation of the warfarin dose with clinical and pharmaco-

genetic data’, New England Journal of Medicine 360(8), 753–764.

47

den Boer, A. V. and Keskin, N. B. (2020), ‘Discontinuous demand functions: estimation

and pricing’, Management Science .

Fan, C., Lu, W., Song, R. and Zhou, Y. (2017), ‘Concordance-assisted learning for estimating

optimal individualized treatment regimes’, Journal of the Royal Statistical Society. Series

B, Statistical methodology 79(5), 1565.

Fan, J. and Zhang, W. (2008), ‘Statistical methods with varying coeﬃcient models’, Stat.

Interface 1(1), 179–195.

Farrell, M. H., Liang, T. and Misra, S. (2021), ‘Deep neural networks for estimation and

inference’, Econometrica 89(1), 181–213.

Frick, K., Munk, A. and Sieling, H. (2014), ‘Multiscale change point inference’, J. R. Stat.

Soc. Ser. B. Stat. Methodol. 76(3), 495–580. With 32 discussions by 47 authors and a

rejoinder by the authors.

Friedrich, F., Kempe, A., Liebscher, V. and Winkler, G. (2008), ‘Complexity penalized

m-estimation: Fast computation’, Journal of Computational and Graphical Statistics

17(1), 201–224.

Fryzlewicz, P. (2014), ‘Wild binary segmentation for multiple change-point detection’, Ann.

Statist. 42(6), 2243–2281.

Harchaoui, Z. and Lévy-Leduc, C. (2010), ‘Multiple change-point estimation with a total

variation penalty’, J. Amer. Statist. Assoc. 105(492), 1480–1493.

Imaizumi, M. and Fukumizu, K. (2019), Deep neural networks learn non-smooth functions

eﬀectively, in ‘The 22nd International Conference on Artiﬁcial Intelligence and Statistics’,

PMLR, pp. 869–878.

Kallus, N. and Zhou, A. (2018), Policy evaluation and optimization with continuous

treatments, in ‘International Conference on Artiﬁcial Intelligence and Statistics’, PMLR,

pp. 1243–1251.

48

Killick, R., Fearnhead, P. and Eckley, I. A. (2012), ‘Optimal detection of changepoints with

a linear computational cost’, J. Amer. Statist. Assoc. 107(500), 1590–1598.

Kuruvilla, M. and Gurk-Turner, C. (2001), ‘A review of warfarin dosing and monitoring’,

Proceedings (Baylor University. Medical Center) 14(3), 305.

Laber, E. B. and Zhao, Y.-Q. (2015), ‘Tree-based methods for individualized treatment

regimes’, Biometrika 102(3), 501–514.

LeCun, Y., Bengio, Y. and Hinton, G. (2015), ‘Deep learning’, nature 521(7553), 436–444.

Liang, S., Lu, W., Song, R. and Wang, L. (2017), ‘Sparse concordance-assisted learning for

optimal treatment decision’, The Journal of Machine Learning Research 18(1), 7375–7400.

Luedtke, A. R. and Van Der Laan, M. J. (2016), ‘Statistical inference for the mean outcome

under a possibly non-unique optimal treatment strategy’, Annals of statistics 44(2), 713.

Massart, P. et al. (2000), ‘About the constants in talagrand’s concentration inequalities for

empirical processes’, Annals of Probability 28(2), 863–884.

Meng, H., Zhao, Y.-Q., Fu, H. and Qiao, X. (2020), ‘Near-optimal individualized treatment

recommendations’, Journal of Machine Learning Research 21(183), 1–28.

Murphy, S. A. (2003), ‘Optimal dynamic treatment regimes’, J. R. Stat. Soc. Ser. B Stat.

Methodol. 65(2), 331–366.

URL: https://doi.org/10.1111/1467-9868.00389

Nie, X., Brunskill, E. and Wager, S. (2020), ‘Learning when-to-treat policies’, Journal of

the American Statistical Association pp. 1–18.

Niu, Y. S., Hao, N. and Zhang, H. (2016), ‘Multiple change-point detection: A selective

overview’, Statistical Science pp. 611–623.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,

M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,

49

D., Brucher, M., Perrot, M. and Duchesnay, E. (2011), ‘Scikit-learn: Machine learning in

Python’, Journal of Machine Learning Research 12, 2825–2830.

Qi, Z., Liu, D., Fu, H. and Liu, Y. (2020), ‘Multi-armed angle-based direct learning for

estimating optimal individualized treatment rules with various outcomes’, Journal of the

American Statistical Association 115(530), 678–691.

Qian, M. and Murphy, S. A. (2011), ‘Performance guarantees for individualized treatment

rules’, Annals of statistics 39(2), 1180.

Qiang, S. and Bayati, M. (2016), ‘Dynamic pricing with demand covariates’, Available at

SSRN 2765257 .

Rich, B., Moodie, E. E. and Stephens, D. A. (2014), ‘Simulating sequential multiple

assignment randomized trials to generate optimal personalized warfarin dosing strategies’,

Clinical trials 11(4), 435–444.

Robins, J. M. (2004), Optimal structural nested models for optimal sequential decisions, in

‘Proceedings of the second seattle Symposium in Biostatistics’, Springer, pp. 189–326.

Robinson, P. M. (1988), ‘Root-N -consistent semiparametric regression’, Econometrica

56(4), 931–954.

Rotschafer, J., Crossley, K., Zaske, D., Mead, K., Sawchuk, R. and Solem, L. (1982), ‘Phar-

macokinetics of vancomycin: observations in 28 patients and dosage recommendations.’,

Antimicrobial Agents and Chemotherapy 22(3), 391–394.

Schmidt-Hieber, J. et al. (2020), ‘Nonparametric regression using deep neural networks with

relu activation function’, Annals of Statistics 48(4), 1875–1897.

Schulz, J. and Moodie, E. E. (2020), ‘Doubly robust estimation of optimal dosing strategies’,

Journal of the American Statistical Association pp. 1–13.

Scott, A. J. and Knott, M. (1974), ‘A cluster analysis method for grouping means in the

analysis of variance’, Biometrics pp. 507–512.

50

Shi, C., Fan, A., Song, R. and Lu, W. (2018), ‘High-dimensional a-learning for optimal

dynamic treatment regimes’, Annals of statistics 46(3), 925.

Shi, C., Lu, W. and Song, R. (2020), ‘Breaking the curse of nonregularity with subagging—

inference of the mean outcome under optimal treatment regimes’, Journal of Machine

Learning Research 21(176), 1–67.

Song, R., Wang, W., Zeng, D. and Kosorok, M. R. (2015), ‘Penalized q-learning for dynamic

treatment regimens’, Statistica Sinica 25(3), 901.

Stone, C. J. (1982), ‘Optimal global rates of convergence for nonparametric regression’,

Annals of Statistics pp. 1040–1053.

Tibshirani, R. (1996), ‘Regression shrinkage and selection via the lasso’, J. R. Stat. Soc.

Ser. B. Stat. Methodol. 58, 267–288.

Tsybakov, A. B. (2004), ‘Optimal aggregation of classiﬁers in statistical learning’, Ann.

Statist. 32(1), 135–166.

Van Der Vaart, A. W. and Wellner, J. A. (1996a), Weak convergence, in ‘Weak convergence

and empirical processes’, Springer, pp. 16–28.

van der Vaart, A. W. and Wellner, J. A. (1996b), Weak convergence and empirical processes,

Springer Series in Statistics, Springer-Verlag, New York. With applications to statistics.

Wang, L., Zhou, Y., Song, R. and Sherwood, B. (2018), ‘Quantile-optimal treatment regimes’,

Journal of the American Statistical Association 113(523), 1243–1254.

Watkins, C. and Dayan, P. (1992), ‘Q-learning’, Mach. Learn. 8, 279–292.

Zhang, B., Tsiatis, A. A., Laber, E. B. and Davidian, M. (2012), ‘A robust method for

estimating optimal treatment regimes’, Biometrics 68(4), 1010–1018.

Zhang, B., Tsiatis, A. A., Laber, E. B. and Davidian, M. (2013), ‘Robust estimation

of optimal dynamic treatment regimes for sequential treatment decisions’, Biometrika

100(3), 681–694.

51

Zhang, Y., Laber, E. B., Davidian, M. and Tsiatis, A. A. (2018), ‘Estimation of optimal

treatment regimes using lists’, J. Amer. Statist. Assoc. 113(524), 1541–1549.

Zhang, Y., Laber, E. B., Tsiatis, A. and Davidian, M. (2015), ‘Using decision lists to

construct interpretable and parsimonious treatment regimes’, Biometrics 71(4), 895–904.

Zhao, Q., Small, D. S. and Ertefaie, A. (2017), ‘Selective inference for eﬀect modiﬁcation

via the lasso’, arXiv preprint arXiv:1705.08020 .

Zhao, Y.-Q., Zeng, D., Laber, E. B. and Kosorok, M. R. (2015), ‘New statistical learning

methods for estimating optimal dynamic treatment regimes’, J. Amer. Statist. Assoc.

110(510), 583–598.

Zhao, Y., Zeng, D., Rush, A. J. and Kosorok, M. R. (2012), ‘Estimating individualized

treatment rules using outcome weighted learning’, J. Amer. Statist. Assoc. 107(499), 1106–

1118.

Zhou, W., Zhu, R. and Zeng, D. (2018), ‘A parsimonious personalized dose ﬁnding model

via dimension reduction’, arXiv preprint arXiv:1802.06156 .

Zhu, L., Lu, W., Kosorok, M. R. and Song, R. (2020), Kernel assisted learning for personalized

dose ﬁnding, in ‘Proceedings of the 26th ACM SIGKDD International Conference on

Knowledge Discovery & Data Mining’, pp. 56–65.

Zhu, R., Zhao, Y.-Q., Chen, G., Ma, S. and Zhao, H. (2017), ‘Greedy outcome weighted

tree learning of optimal personalized treatment rules’, Biometrics 73(2), 391–400.

52

This appendix is organized as follows. In Section A, we discuss more details on tuning

parameters in L-JIL. Technical proofs are given in Section B.

A More on Tuning in L-JIL

For L-JIL, we choose γn and λn simultaneously via cross-validation. As we will show below,

the use of cross-validation will not increase the computation complexity substantially in

L-JIL.

To elaborate, let us revisit the proposed jump interval-learning in Algorithm 1. The

most time consuming part lies in computing the ridge-type estimator

(cid:98)θI(λn) =





(cid:88)

i∈G−k

X iX

(cid:62)
i

I(Ai ∈ I) + nλn|I|Ep+1





(cid:88)

i∈G−k

X iYiI(Ai ∈ I)

 ,

(45)



−1 



where Ep+1 is the identity matrix with dimension p + 1, and the cost function

cost(I, λn) =

1
n

(cid:88)

i∈Gk

I(Ai ∈ I)

(cid:110)

Yi − X

(cid:111)2

(cid:62)
i (cid:98)θI(λn)

,

for any I ∈ {[l/m, r/m) : 1 ≤ l < r < m} ∪ {[l/m, 1] : 1 ≤ l < m}.

To compute {(cid:98)θI,γn,λn,k : γn ∈ Γn, λn ∈ Λn}, we need to calculate {(cid:98)θI(λn) : λn ∈ Λn} and
I(Ai ∈ I) as

{cost(I, λn) : λn ∈ Λn} for any I. We ﬁrst factorize the matrix (cid:80)

X iX

(cid:62)
i

i∈G−k

X iX

(cid:62)
i

I(Ai ∈ I) = U T U (cid:62),

(cid:88)

i∈G−k

according to the eigendecomposition, where U is some (p + 1) × (p + 1) orthogonal matrix
and T = diag(τ0, τ1, · · · , τp) is some diagonal matrix. Let φ = U (cid:62){(cid:80)
X iYiI(Ai ∈ I)}.
Then the set of estimators {(cid:98)θI(λn) : λ ∈ Λn} can be calculated by

i∈G−k

(cid:98)θI(λn) = U diag (cid:8)(τ0 + nλn|I|)−1, (τ1 + nλn|I|)−1, · · · , (τp + nλn|I|)−1(cid:9) φ,

simultaneously for all λn.

Compared to separately inverting the matrix (cid:80)

i∈G−k

X iX

(cid:62)
i

I(Ai ∈ I) + nλn|I|Ep+1 in

equation 45 for each λn to compute {(cid:98)θI(λn) : λn ∈ Λn}, the proposed method saves a lot of

53

time especially for large p. Similarly, based on the eigendecomposition, we have

ncost

(I, λn) =

(cid:88)

i∈G−k

Y 2
i

I(Ai ∈ I)

(46)

−2φ(cid:62)diag (cid:8)(τ0 + nλn|I|)−1, (τ1 + nλn|I|)−1, · · · , (τp + nλn|I|)−1(cid:9) φ
+φ(cid:62)diag (cid:8)τ0(τ0 + nλn|I|)−2, τ1(τ1 + nλn|I|)−2, · · · , τp(τp + nλn|I|)−2(cid:9) φ,

for all λn ∈ Λn. This facilitates the computation of {cost(I, λn) : λn ∈ Λn}.

After obtaining these cost functions, we can recursively compute the Bellman function

B(r, λn, γn) by

B(r, λn, γn) = min
j∈Rr

{B(j, λn, γn) + γn + cost([j/m, r/m), λn)} ,

for all r ≥ 1, λn ∈ Λn and γn ∈ Γn. Given the Bellman function, the set of estimators

{(cid:98)θI,γn,λn,k : γn ∈ Γn, λn ∈ Λn} thus can be computed eﬃciently.

B Technical proofs

In the proofs, we use c, C > 0 to denote some universal constants whose values are allowed
to change from place to place. For any vector φ ∈ Rq, we use φ(j) to denote the j-th element

of φ, for any j ∈ {1, . . . , q}. For any two positive sequences {an}, {bn}, an ∝ bn means that

an ≤ cbn for some universal constant c > 0

54

B.1 Proof of Lemma 1

Proof of equation 20: By deﬁnition, we have

i=1

(cid:32)

1
n




n
(cid:88)

(cid:107)(cid:98)θI − θ0,I(cid:107)2
(cid:13)
(cid:32)
(cid:13)
n
(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

n
(cid:88)

1
n

1
n



(cid:32)

i=1

i=1

≤

+

≤

+

I(Ai ∈ I)X iX

(cid:62)

i + λn|I|Ep+1

(cid:33)−1 (cid:32)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)X iYi − EI(A ∈ I)XY

I(Ai ∈ I)X iX

(cid:33)−1

(cid:62)

i + λn|I|Ep+1

(cid:16)

−

EI(A ∈ I)XX

(cid:62)(cid:17)−1






{EI(A ∈ I)XY }

I(Ai ∈ I)X iX

(cid:62)

i + λn|I|Ep+1

I(Ai ∈ I)X iYi − EI(A ∈ I)XY

(cid:33)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:125)

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

1
n

n
(cid:88)

i=1

(cid:123)(cid:122)
η1(I)

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:125)

1
n

n
(cid:88)

i=1

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

I(Ai ∈ I)X iX

(cid:62)

i + λn|I|Ep+1

(cid:33)−1

(cid:16)

−

EI(A ∈ I)XX

(cid:123)(cid:122)
η3(I)

(cid:62)(cid:17)−1

(cid:107)EI(A ∈ I)XY (cid:107)2
(cid:125)
(cid:123)(cid:122)
(cid:124)
η4(I)

.

(cid:123)(cid:122)
η2(I)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:125)

It follows from Cauchy-Schwarz inequality that

(cid:107)(cid:98)θI − θ0,I(cid:107)2

2 ≤ 2η2

1(I)η2

2(I) + 2η2

3(I)η2

4(I).

(47)

In the following, we provides upper bounds for

max
I∈I(m)
|I|≥¯c0n−1 log n

ηj(I),

for j = 1, 2, 3, 4, where the constant ¯c0 will be speciﬁed later. The uniform convergence

rates of (cid:107)(cid:98)θI − θ0,I(cid:107)2 can thus be derived.

Without loss of generality, assume the constant ω in Condition (A4) is greater than
or equal to log−1/2 2. Then, we have exp(1/ω2) ≤ exp(log 2) = 2 and hence (cid:107)1(cid:107)ψ2|A ≤ ω.
(cid:107)ψ2|A ≤ ω, almost surely. By the deﬁnition of the
Therefore, we have maxj∈{1,...,p+1} (cid:107)X

(j)

conditional Orlicz norm, this implies that

(cid:40)

E

1 +

+∞
(cid:88)

q=1

(j)

|2q
|X
ω2qq!

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

A

≤ 2,

∀j ∈ {1, . . . , p + 1},

55

almost surely, and hence

(j)

E(|X

|2q|A) ≤ q!ω2q,

∀j ∈ {1, . . . , p + 1}, q = 1, 2, . . .

(48)

By Cauchy-Schwarz inequality, we obtain that

E(|X

(j1)

X

(j2)

|q|A) ≤

(cid:113)

E(|X

(j1)

|2q|A)E(|X

(j2)

|2q|A) ≤ q!ω2q,

(49)

for any j1, j2 ∈ {1, . . . , p + 1} and any integer q ≥ 1, almost surely.

Since A has a bounded probability density function pA(·) in [0, 1], there exists some

constant C0 > 0 such that

sup
a∈[0,1]

pA(a) ≤ C0 and Pr(A ∈ I) ≤ C0|I|,

(50)

for any interval I ∈ [0, 1]. This together with equation 49 yields that for any integer q ≥ 1,

j1, j2 ∈ {1, . . . , p + 1} and any interval I ∈ [0, 1], we have

E|I(A ∈ I)X

(j1)

X

(j2)

|q = E{I(A ∈ I)E(|X

(j1)

X

(j2)

|q|A)} ≤ q!ω2qEI(A ∈ I) ≤ C0q!ω2q|I|.

It follows that

E|I(A ∈ I)X

(j1)

X

(j2)

− EI(A ∈ I)X

(j1)

(j2)

|q

X

≤ E|I(A1 ∈ I)X

(j1)
1 X
≤ 2q−1E|I(A1 ∈ I)X
(j1)

= 2qE|I(A ∈ I)X

X

(j2)
1 − I(A2 ∈ I)X
(j1)
1 X
(j2)

(j2)
1

|q ≤ C0q!(2ω2)q|I|,

(j1)
2 X

(j2)
2

|q

|q + 2q−1E|I(A2 ∈ I)X

(j1)
2 X

(j2)
2

|q

where the second inequality follows from Jensen’s inequality and the third inequality is due
to that |a + b|q ≤ 2q−1|a|q + 2q−1|b|q−1, for any a, b ∈ R and q ≥ 1.

By the Bernstein’s inequality (see Lemma 2.2.11, van der Vaart and Wellner, 1996b), we

obtain that

Pr

(cid:32)

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

≤ 2 exp

−

I(Ai ∈ I)X

(j1)
i X

(j2)
i − nEI(A ∈ I)X

(j1)

X

(j2)

≥ tω2(cid:112)|I|n log n

(cid:33)

(51)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t2 log n

16C0 + 4t(n|I|)−1/2

(cid:19)

,

√

log n

(cid:33)

(cid:18)

≤ 2 exp

−

1
2

ω4t2|I|n log n
8nC0ω4|I| + 2ω4t(cid:112)|I|n log n

56

for any t > 0, any integers j1, j2 ∈ {1, . . . , p + 1} and any interval I ∈ [0, 1]. Set t = 20
for any interval I with |I| ≥ C −1
0 n−1 log n, we have

√

C0,

t2 log n ≥ 4{16C0 + 4t(n|I|)−1/2(cid:112)log n}.

It follows from equation 51 that

Pr

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I)X

(j1)
i X

(j2)
i − nEI(A ∈ I)X

(j1)

X

(j2)

≥ 20ω2(cid:112)C0|I|n log n

(cid:33)

≤ 2n−4,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for any integers j1, j2 ∈ {1, . . . , p + 1} and any interval I that satisﬁes |I| ≥ C −1
0 n−1 log n.
Notice that the number of elements in I(m) is bounded by (m + 1)2. Since m (cid:16) n, it follows

from Bonferroni’s inequality that

Pr(A1) ≥ 1 − 2(m + 1)2(p + 1)2n−4 = 1 − O(n−2),

(52)

where the event A1 is deﬁned as

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I)X

(j1)
i X

(j2)
i − nEI(A ∈ I)X

(j1)

X

(j2)

≤ 20ω2(cid:112)C0|I|n log n

(cid:41)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|I|≥C−1
For any symmetric matrix A, we have (cid:107)A(cid:107)2 ≤ (cid:112)(cid:107)A(cid:107)∞(cid:107)A(cid:107)1 = (cid:107)A(cid:107)∞. Thus, under the
event deﬁned in A1, we have
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i − nEI(A ∈ I)XX

i − nEI(A ∈ I)XX

I(Ai ∈ I)X iX

I(Ai ∈ I)X iX

n
(cid:88)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:62)

(cid:62)

(cid:62)

i=1

i=1

(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
≤ 20ω2(p + 1)(cid:112)C0|I|n log n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:92)

j1,j2∈{1,...,p+1}
I∈I(m)
0 n−1 log n

for any I ∈ I(m) with |I| ≥ C −1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ I)X iX

0 n−1 log n. Since λn = O(n−1 log n), we obtain
(cid:13)
(cid:13)
+ nλnE|I|
(cid:13)
(cid:13)
(cid:13)2
≤ c(cid:112)|I|n log n,

i − nEI(A ∈ I)XX

i − nEI(A ∈ I)XX

(cid:62)

(cid:62)

(cid:62)

(cid:62)

I(Ai ∈ I)X iX

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ nλn|I| +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

for some constant c > 0. To summarize, under the event deﬁned in A1, we’ve shown that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ I)X iX

(cid:62)

i − nEI(A ∈ I)XX

(cid:62)

(cid:13)
(cid:13)
+ nλnE|I|
(cid:13)
(cid:13)
(cid:13)2

≤ c(cid:112)|I|n log n,

(53)

57

for any interval I ∈ I(m) with |I| ≥ C −1

0 n−1 log n.

Let Σ = EXX

. If Σ is singular, there exists some nonzero vector a ∈ Rp and some
b ∈ R such that a(cid:62)X = b, almost surely. As a result, the covariance matrix of X is

(cid:62)

degenerate. Thus, we’ve reached a contraction. Therefore, Σ is nonsingular. There exists

some constant ¯c∗ > 0 such that

λmin(Σ) ≥ ¯c∗.

(54)

By (A3), we have

Pr(A ∈ I|X) ≥ c∗|I|,

for any interval I ∈ [0, 1]. This together with equation 54 implies that

(cid:16)

λmin

EI(A ∈ I)XX

(cid:62)(cid:17)

= λmin

(cid:16)

EPr(A ∈ I|X)XX

(cid:62)(cid:17)

(55)

≥ c∗λmin(EXX

(cid:62)

)|I| ≥ c∗¯c∗|I|.

For any interval I with |I| ≥ 4c2(c∗¯c∗)−2n−1 log n, we have

c∗¯c∗|I| − c(cid:112)|I|n−1 log n ≥

c∗¯c∗|I|
2

.

In view of equation 53 and equation 55, we obtain that

λmin

(cid:32)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)X iX

(cid:33)

(cid:62)

i + λnE|I|

(cid:16)

≥ λmin

EI(A ∈ I)XX

(cid:62)(cid:17)

(56)

−

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ I)X iX

(cid:62)

i − nEI(A ∈ I)XX

(cid:62)

(cid:13)
(cid:13)
+ nλnE|I|
(cid:13)
(cid:13)
(cid:13)2

≥

c∗¯c∗|I|
2

.

Set ¯c0 = max(4c2(c∗¯c∗)−1, C −1

0 ), it is immediate to see that

max
I∈I(m)
|I|≥¯c0n−1 log n

η1(I) ≤

2
c∗¯c∗|I|

,

(57)

under the event deﬁned in A1.

58

For any I ∈ [0, 1], we have

(cid:33)−1

(cid:62)

i + λnE|I|

(cid:16)

−

EI(A ∈ I)XX

(cid:32)

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

≤

×

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

I(Ai ∈ I)X iX

I(Ai ∈ I)X iX

(cid:62)

i + λnE|I|

(cid:33)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

I(Ai ∈ I)X iX

(cid:62)

i − nEI(A ∈ I)XX

(cid:62)

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

EI(A ∈ I)XX

(cid:13)
(cid:13)
+ nλnE|I|
(cid:13)
(cid:13)
(cid:13)2

(cid:62)(cid:17)−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:62)(cid:17)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)2

This together with equation 53, equation 55 and equation 56 yields

max
I∈I(m)
|I|≥¯c0n−1 log n

η3(I) ≤

2c(cid:112)n−1 log n
∗|I|3/2

∗¯c2
c2

,

(58)

under the event deﬁned in A1.

Similar to equation 49, we can show that for any integer q ≥ 1 and j ∈ {1, . . . , p + 1},

(j)

E(|X

Y |q|A) ≤ q!ω2q,

(59)

almost surely. Speciﬁcally, set q = 1, we obtain E(|X

(j)

Y ||A) ≤ ω2. By equation 50, we

have that

(cid:107)EI(A ∈ I)XY (cid:107)2 ≤

(cid:32) p+1
(cid:88)

j=1

|EI(A ∈ I)X

(j)

Y |2

(cid:33)1/2

≤

(cid:32) p+1
(cid:88)

j=1

|E{I(A ∈ I)E(|X

(j)

Y ||A)}|2

(cid:33)1/2

≤

(cid:32) p+1
(cid:88)

j=1

(cid:33)1/2

|ω2E(A ∈ I)|2

≤ C0

(cid:112)p + 1ω2|I|.

for any I ∈ [0, 1]. This implies that

η4(I) ≤ C0

(cid:112)p + 1ω2|I|.

max
I∈I(m)
|I|≥¯c0n−1 log n

Moreover, in view of equation 51 and equation 52, we can similarly show that

Pr(A2) ≥ 1 − 2(m + 1)2(p + 1)n−4 = 1 − O(n−2),

(60)

(61)

59

where the event A2 is deﬁned as

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:92)

j∈{1,...,p+1}
I∈I(m)
|I|≥¯c0n−1 log n

I(Ai ∈ I)X

(j)

i Yi − nEI(A ∈ I)X

(j)

Y

≤ 20ω2(cid:112)C0|I|n log n

(cid:41)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Under the event deﬁned in A2, we have

η2(I) ≤ 20ω2(cid:112)(p + 1)C0|I|n−1 log n.

(62)

max
I∈I(m)
|I|≥¯c0n−1 log n

Combining equation 57 together with equation 58, equation 60, equation 62 yields

|I|(cid:107)(cid:98)θI − θ0,I(cid:107)2

2 = O

(cid:18) log n
n

(cid:19)

,

max
I∈I(m)
|I|≥¯c0n−1 log n

under the events deﬁned in A1 and A2. The proof is thus completed based on equation 52

and equation 61.

Proofs of equation 21, equation 22 and equation 25: We ﬁrst prove equation 25. By the

deﬁnition of θ0,I, we have

(cid:107)θ0,I(cid:107)2 ≤

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

EXX

(cid:62)I(A ∈ I)

(cid:17)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)EXY I(A ∈ I)(cid:13)

(cid:13)2 .

It follows from equation 50, equation 55 and equation 59 that

(cid:107)θ0,I(cid:107)2 ≤ (c∗¯c∗|I|)−1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

p+1
(cid:88)

(cid:104)
E

j=1

(cid:110)(cid:16)

E|X

(j)

(cid:17)

Y ||A

I(A ∈ I)

(cid:111)(cid:105)2

≤ (cid:112)p + 1(c∗¯c∗)−1C0ω2,

for any I ∈ [0, 1]. Assertion equation 25 thus follows.

Consider equation 21. Since p is ﬁxed, it suﬃces to show for any j ∈ {1, . . . , p + 1}, the

following event occurs with probability at least 1 − O(n−2):

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X

(j)
i

= O

(cid:32)(cid:112)|I| log n
√

n

(cid:33)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2

(63)

By equation 25, equation 63 can be proven in a similar manner as equation 52 and equation 61.

equation 22 can be similarly proven.

60

Proof of equation 23: Similar to equation 21, we can show that the following event occurs

with probability at least 1 − O(n−2): for any I ∈ I(m) such that |I| ≥ ¯c0n−1 log n,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i {θ0(Ai) − θ0,I}]2 − EI(A ∈ I)[X

{θ0(A) − θ0,I}]2

I(Ai ∈ I)[X

n
(cid:88)

(cid:32)(cid:112)|I| log n
√

= O

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

(cid:62)

(cid:62)

i=1

(cid:33)

.

Notice that

EI(A ∈ I)[X

(cid:62)

≥ c∗E

(cid:90)

I

[X

(cid:62)

≥ c∗λmin(Σ)

{θ0(A) − θ0,I}]2 = E
(cid:90)

(cid:90)

I

(cid:62)

[X

{θ0(a) − θ0,I}]2π(a|X)da

{θ0(a) − θ0,I}]2da = c∗
(cid:90)

{θ0(a) − θ0,I}(cid:62)Σ{θ0(a) − θ0,I}da
(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ c∗¯c∗

(cid:107)θ0(a) − θ0,I(cid:107)2

2da,

I

I

(64)

where the ﬁrst inequality is due to Condition (A3) and the last inequality is due to

equation 54.

It follows that

1
n

n
(cid:88)

i=1

I(Ai ∈ I)[X

(cid:62)
i {θ0(Ai) − θ0,I}]2 ≥ c∗¯c∗

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da − O

(cid:32)(cid:112)|I| log n
√

n

(cid:33)

,

for any I ∈ I(m) such that |I| ≥ ¯c0n−1 log n, with probability at least 1 − O(n−2). This

completes the proof.

Proof of equation 24: Similar to equation 21, we can show that the following event occurs

with probability at least 1 − O(n−2): for any I ∈ I(m) such that |I| ≥ ¯c0n−1 log n,
(cid:32)(cid:112)|I| log n
√

I(Ai ∈ I)(|Yi|2 + (cid:107)X i(cid:107)2

(cid:12)
(cid:12)
2) − EI(A ∈ I)(Y 2 + (cid:107)X(cid:107)2
(cid:12)
2)
(cid:12)
(cid:12)

n
(cid:88)

= O

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

i=1

(cid:33)

.(65)

By equation 50 and equation 48, we have

EI(A ∈ I)(cid:107)X(cid:107)2

2 ≤

p+1
(cid:88)

j=1

EI(A ∈ I)|X

(j)

|2 ≤ (p + 1)C0ω2|I|.

Similarly, we can show

and thus

EI(A ∈ I)Y 2 ≤ C0ω2|I|,

EI(A ∈ I)(Y 2 + (cid:107)X(cid:107)2

2) ≤ (p + 2)C0ω2|I|.

This together with equation 65 yields equation 24.

61

B.2 Proof of Lemma 2

We ﬁrst prove equation 26. By equation 25, we have supa∈[0,1] (cid:107)θ0(a)(cid:107)2 ≤ c0 and hence

sup
a∈[0,1]

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ 2c0.

Similar to equation 48, we can show that for any integer q ≥ 1,

E(|Y |2q|A) ≤ q!ω2q.

(66)

(67)

For any I ⊆ I(m) and integer q ≥ 2, it follows from equation 59, equation 66 and equation 67

that

≤

+

1
2

1
2

(cid:16)

E

(cid:62)

[Y X

{θ0(A) − θ0,I}]q|A

(cid:17)

2E(|Y |q(cid:107)X(cid:107)q

2|A)

(68)

(cid:32)

(cid:107)θ0(A) − θ0,I(cid:107)q

2E

|Y |2q +

(X

(j)

)2

A

≤

(cid:107)θ0(A) − θ0,I(cid:107)q

2q!ω2q

p+1
(cid:88)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p+1
(cid:88)

≤ (cid:107)θ0(A) − θ0,I(cid:107)q
(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

q(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

q!ω2q
2

(cid:107)θ0(A) − θ0,I(cid:107)q

2(p + 1)q−1

E(|X

(j)

|2q|A) ≤

{1 + (p + 1)q}(cid:107)θ0(A) − θ0,I(cid:107)q
2

≤ q!ω2q(p + 1)q(cid:107)θ0(A) − θ0,I(cid:107)q

j=1
2 ≤ q!ω2q(p + 1)q(2c0)q−2(cid:107)θ0(A) − θ0,I(cid:107)2
2.

Similarly, we can show

(cid:16)

E

(cid:62)

[{X

θ0(A)}X

(cid:62)

{θ0(A) − θ0,I}]q|A

(cid:17)

≤ q!ω2q(p + 1)q2q−2c2q−2

0

(cid:107)θ0(A) − θ0,I(cid:107)2
2.

This together with equation 68 yields that for any integer q ≥ 2, I ⊆ [0, 1], we have

(cid:16)

E

[{Y − X

(cid:62)

θ0(A)}X

(cid:62)

(cid:17)
{θ0(A) − θ0,I}]q|A

≤ q!cq(cid:107)θ0(A) − θ0,I(cid:107)2
2,

(69)

for some constant c > 0. Combining equation 50 together with equation 69, we obtain that

for any integer q ≥ 2, I ⊆ [0, 1],

E[I(A ∈ I){Y − X

(cid:62)

θ0(A)}X

(cid:62)

{θ0(A) − θ0,I}]q ≤ C0q!cq

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2pA(a)da

≤ C0q!cq

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da.

Applying the Bernstein’s inequality (using similar arguments in equation 51 and equation 52),

we can show that with probability at least 1 − O(n−2), we have for any interval I that

62

satisﬁes (cid:82)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ (C0)−1n−1 log n and I ∈ I(m),

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i {θ0(Ai) − θ0,I}
(cid:12)
(cid:12)

≤ O(1)(cid:112)n log n

(cid:107)θ0(a) − θ0,I(cid:107)2

2da

(cid:19)1/2

,

(cid:18)(cid:90)

I

where O(1) denotes some positive constant. This proves equation 26.

Similarly, we can show that with probability at least 1−O(n−2), there exists some constant

C > 0 such that for any interval I that satisﬁes (cid:82)

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ (C0)−1n−1 log n and

I ∈ I(m), we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I)[X

(cid:62)

i {θ0(Ai) − θ0,I}]2 − nEI(A ∈ I)[X

(cid:62)

{θ0(A) − θ0,I}]2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)1/2

,

≤ O(1)(cid:112)n log n

(cid:18)(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da

for some postive constant O(1). This together with equation 64 yields equation 27.

B.3 Proof of Lemma 3

Consider the following three categories of intervals.

Category 1: Suppose i1 and i2 satisfy τ0,k−1 ≤ i1 ≤ i2 ≤ τ0,k for some integer k such that 1 ≤
k ≤ K. Then apparently, we have θ0,I = θ0(a), ∀a ∈ I, and hence (cid:82)
The assertion (cid:82)

2da ≤ cn is thus automatically satisﬁed.

I (cid:107)θ0(a) − θ0,I(cid:107)2

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da = 0.

Category 2: Suppose there exists some integer k such that 2 ≤ k ≤ K and i1, i2 satisfy

τ0,k−2 ≤ i1 < τ0,k−1 < i2 ≤ τ0,k. Assume we have

min
j∈{1,2}

|ij − τ0,k−1| ≥

3
κ2
0

cn.

where

κ0 ≡

min
I1,I2∈P0
I1 and I2 are adjacent

(cid:107)θ0,I1 − θ0,I2(cid:107)2 > 0.

Since cn → 0, for suﬃciently large n, we have τ0,k > τ0,k−1 + 3κ−2

0 cn and τ0,k−2 + 3κ−2

0 cn <

τ0,k−1. Then, we have

(cid:90)

(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ min
θ∈Rp+1

(cid:90) τ0,k−1+3κ−2

0 cn

τ0,k−1−3κ−2

0 cn

(cid:107)θ − θ0(a)(cid:107)2

2da

I
6
κ−2
0

≥

cn min
θ∈Rp+1

(cid:0)(cid:107)θ − θ0,[τ0,k−2,τ0,k−1)(cid:107)2

2, (cid:107)θ − θ0,[τ0,k−1,τ0,k)(cid:107)2
2

(cid:1) ≥

6
κ−2
0

cn

κ−2
0
4

> cn.

63

This violates the assertion that (cid:82)

I (cid:107)θ0(a)−θ0,I(cid:107)2

2da ≤ cn. We’ve thus reached a contradiction.

As a result, we have

min
j∈{1,2}

|ij − τ0,k−1| ≤

3
κ2
0

cn.

Category 3: Suppose there exists some integer k such that 3 ≤ k ≤ K and i1, i2 satisfy

τ0,k−3 ≤ i1 < τ0,k−2 < τ0,k−1 < i2 ≤ τ0,k. Assume we have

Then for suﬃciently large n, we have

|i1 − τ0,k−2| ≥

3
κ2
0

cn.

(cid:90)

(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥ min
θ∈Rp+1

(cid:90) τ0,k−2+3κ−2

0 cn

τ0,k−2−3κ−2

0 cn

(cid:107)θ − θ0(a)(cid:107)2

2da

I
6
κ−2
0

≥

cn min
θ∈Rp+1

(cid:0)(cid:107)θ − θ0,[τ0,k−2,τ0,k−1)(cid:107)2

2, (cid:107)θ − θ0,[τ0,k−1,τ0,k)(cid:107)2
2

(cid:1) ≥

6
κ−2
0

cn

κ−2
0
4

> cn.

This violates the assertion that (cid:82)
As a result, we have |i1 − τ0,k−2| ≤ 3κ−2

I (cid:107)θ0(a)−θ0,I(cid:107)2

2da ≤ cn. We’ve thus reached a contradiction.
0 cn.

0 cn. Similarly, we can show |i2 − τ0,k−1| ≤ 3κ−2

Therefore, we obtain

max
j∈{1,2}

|ij − τ0,k−3+j| ≤

3
κ2
0

cn.

If I belongs to none of these categories, then there exists some integer k such that

2 ≤ k ≤ K and i1, i2 satisfy i1 ≤ τ0,k−2 and i2 ≥ τ0,k. Using similar arguments, we can show

that

(cid:90)

I

(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥

(cid:90) τ0,k

τ0,k−2

(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≥

κ2
0
4

min
I∈P0

|I|.

For suﬃciently large n, this violates the assertion that (cid:82)

2da ≤ cn. We’ve thus
reached a contradiction. Therefore, we shall have τ0,k−2 ≤ i1 < i2 ≤ τ0,k. This completes

I (cid:107)θ0(a) − θ0,I(cid:107)2

the ﬁrst part of the proof.

We now show equation 28. Take cn = ¯c1n−1 log n and consider any interval I ∈ I(m)

that satisﬁes (cid:82)

I (cid:107)θ0(a) − θ0,I(cid:107)2

2da ≤ ¯c1n−1 log n.

If I belongs to Category 1, then θ0(a) = θ0,I for any a ∈ I. As a result, we have

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I} = 0.

64

If I belongs to Category 2, then there exists some integer k such that 2 ≤ k ≤ K and

i1, i2 satisfy τ0,k−2 ≤ i1 < τ0,k−1 < i2 ≤ τ0,k. Thus, we have

n
(cid:88)

i=1
n
(cid:88)

i=1
(cid:124)

n
(cid:88)

i=1
(cid:124)

=

+

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

I(Ai ∈ [i1, τ0,k−1)){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

(cid:123)(cid:122)
ζ1

(cid:125)

I(Ai ∈ [τ0,k−1, i2)){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i {θ0(Ai) − θ0,I}

.

(cid:123)(cid:122)
ζ2

(cid:125)

Notice that we’ve shown

min
j∈{1,2}

|ij − τ0,k−1| ≤

3¯c1
κ2
0

n−1 log n.

Without loss of generality, suppose |i1 − τ0,k−1| ≤ 3¯c1κ−2

0 n−1 log n. Using similar arguments
in equation 35 and equation 36, we can show that ζ1 = O(log n), with probability at least

1 − O(n−2).

As for ζ2, consider intervals of the form [τ0,j, (m + 1)−1i) for j = 0, 1, . . . , K − 1,

i = 1, . . . , m + 1. Denoted by J(m) the set consisting of all such intervals. Similar to Lemma

1, we can show that the following event occurs with probability at least 1 − O(n−2):

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

= O((cid:112)n|I| log n),

(70)

for any I ∈ J(m) with |I| ≥ cn−1 log n for some constant c > 0. Suppose i2 − τ0,k−1 ≥
cn−1 log n. Under the event deﬁned in equation 70, it follows that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ [τ0,k−1, i2)){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

= O((cid:112)n|I| log n),

(71)

Since (cid:82)
I (cid:107)θ0(a) − θ0,I(cid:107)2
hence (i2 − τ0,k−1)(cid:107)θ0(a) − θ0,I(cid:107)2

2da ≤ ¯c1n−1 log n, we have (cid:82) i2

(cid:107)θ0(a) − θ0,I(cid:107)2
2da ≤ ¯c1n−1 log n, and
2 ≤ ¯c1n−1 log n, for any a ∈ [τ0,k−1, i2). This together with

τ0,k−1

65

equation 71 yields that

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i=1
(cid:13)
n
(cid:13)
(cid:88)
(cid:13)
(cid:13)
(cid:13)

i=1

≤

I(Ai ∈ [τ0,k−1, i2)){Yi − X

(cid:62)
i θ0(Ai)}X i{θ0(Ai) − θ0,I}

I(Ai ∈ [τ0,k−1, i2)){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:107)θ0(τ0,k−1) − θ0,I(cid:107)2 = O(log n),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

and hence ζ2 = O(log n). When i2 − τ0,k−1 ≤ cn−1 log n, using similar arguments in

equation 35 and equation 36, we can show that ζ2 = O(log n), with probability at least

1 − O(n−2). Thus, we’ve shown that with probability at least 1 − O(n−2), for any interval
I that belongs to the Category 2 with (cid:82)

2 ≤ ¯c1n−1 log n, we have

I (cid:107)θ0(a) − θ0,I(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i θ0(Ai)}X i{θ0(Ai) − θ0,I}
(cid:12)
(cid:12)

= O(log n).

Similarly, one can show that with probability at least 1 − O(n−2), for any interval I
I (cid:107)θ0(a) − θ0,I(cid:107)2

that belongs to the Category 3 with (cid:82)

2 ≤ ¯c1n−1 log n, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i θ0(Ai)}X i{θ0(Ai) − θ0,I}
(cid:12)
(cid:12)

= O(log n).

The proof is thus completed.

B.4 Proof of Lemma 4

Consider a given interval I ∈ (cid:98)P. Suppose |I| < ¯c3γn. The value of the constant ¯c3 will be
determined later. Then, for suﬃciently large n, we can ﬁnd some interval I (cid:48) ∈ I(m) ∩ (cid:98)P
that is adjacent to I. Thus, we have I ∪ I (cid:48) ∈ I(m), and hence

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 +

1
n

n
(cid:88)

i=1

I(Ai ∈ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 (72)

+ λn|I (cid:48)|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI∪I(cid:48))2 + λn|I ∪ I (cid:48)|(cid:107)(cid:98)θI∪I(cid:48)(cid:107)2

2 − γn.

Notice that the left-hand-side of the above expression is nonnegative. It follows that

γn ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI∪I(cid:48))2 + λn|I ∪ I (cid:48)|(cid:107)(cid:98)θI∪I(cid:48)(cid:107)2
2.

66

By deﬁnition, we have

(cid:98)θI∪I(cid:48) = arg min
θ∈Rp+1

(cid:32)

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i θ)2 + λn|I ∪ I (cid:48)|(cid:107)θ(cid:107)2
2

(cid:33)

.

(73)

Therefore, we obtain that

γn

≤

=

n
(cid:88)

i=1
n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i 0p+1)2

n

I(Ai ∈ I ∪ I (cid:48))Y 2
i
n

.

+ λn|I ∪ I (cid:48)|(cid:107)0p+1(cid:107)2
2

(74)

Suppose

|I ∪ I (cid:48)| ≤

γn
8c0

,

(75)

where the constant c0 is deﬁned in Lemma 1.

Since γn (cid:29) n−1 and m (cid:16) n, we can ﬁnd some interval I ∗ ∈ I(m) that covers I ∪ I (cid:48) and
satisﬁes (8c0)−1γn ≤ |I ∗| ≤ (4c0)−1γn. Under the event deﬁned in equation 24, it follows
from the condition γn (cid:29) n−1 log n that

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))Y 2

i ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗)Y 2

i ≤ c0

(cid:32)(cid:112)|(4c0)−1γn| log n
√

n

(cid:33)

+ (4c0)−1γn

≤ 2c0(4c0)−1γn =

γn
2

,

for suﬃciently large n. This apparently violates the results in equation 74. Thus, Assertion

equation 75 doesn’t hold. Therefore, we obtain that

|I ∪ I (cid:48)| ≥

γn
8c0

,

(76)

with probability at least 1 − O(n−2).

Suppose the constant ¯c3 satisﬁes ¯c3 ≤ (16c0)−1. Under the event deﬁned in equation 76,
we have |I (cid:48)| ≥ γn(16c0)−1. By equation 20, we have with probability at least 1 − O(n−2)

that (cid:107)(cid:98)θI(cid:48) − θ0,I(cid:48)(cid:107)2 ≤ c0
have with probability at least 1 − O(n−2) that

(cid:112)n−1 log n|I (cid:48)|−1/2 ≤ 4c3/2

0

(cid:112)n−1 log nγ−1

n (cid:28) 1. By equation 25, we

(cid:107)(cid:98)θI(cid:48)(cid:107)2 ≤ 2c0,

67

(77)

for suﬃciently large n.

In addition, it follows from equation 73 that

1
n

i=1
1
n

≤

n
(cid:88)

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI∪I(cid:48))2 + λn|I ∪ I (cid:48)|(cid:107)(cid:98)θI∪I(cid:48)(cid:107)2
2

n
(cid:88)

i=1

I(Ai ∈ I ∪ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I ∪ I (cid:48)|(cid:107)(cid:98)θI(cid:48)(cid:107)2
2.

By equation 72, this further implies that

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 ≤

1
n

n
(cid:88)

i=1

and hence

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 − γn,

γn ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I|(cid:107)(cid:98)θI(cid:48)(cid:107)2
2.

By equation 77 and the conditions that λn = O(n−1 log n), γn (cid:29) n−1 log n, we have for

suﬃciently large n,

γn
2

≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2.

It thus follows from Cauchy-Schwarz inequality and equation 77 that

γn
2

≤

2
n

n
(cid:88)

i=1

I(Ai ∈ I)(Y 2

i + (cid:107)X

(cid:62)
i (cid:107)2

2(cid:107)(cid:98)θI(cid:48)(cid:107)2

2) ≤

2(1 + 4c2
0)
n

n
(cid:88)

i=1

I(Ai ∈ I)(Y 2

i + (cid:107)X i(cid:107)2

2).

Using similar arguments in showing equation 76, we obtain that

|I| ≥

γn

32(1 + 4c2

0)c0

.

with probability at least 1−O(n−2). Set ¯c3 = 32−1(1+4c2
0 , this violates the assumption
that |I| < ¯c3γn. Thus, with probability at least 1 − O(n−2), we obtain that |I| ≥ ¯c3γn, for

0)−1c−1

any I ∈ (cid:98)P. The proof is hence completed.

B.5 Proof of Theorem 2

Let {(cid:98)τ1, (cid:98)τ2, . . . , (cid:98)τ
Theorem 1, we have (cid:98)K = K, and

(cid:98)K−1} be the set of change points in J( (cid:98)P). Under the events deﬁned in

max
k∈{1,...,K−1}

|(cid:98)τk − τ0,k| ≤ cn−1 log n,

(78)

68

for some constant c > 0. Set (cid:98)τ0 = 0 and (cid:98)τK = 1.

Under the event deﬁned in equation 78, we have for suﬃciently large n that

(cid:98)τk − (cid:98)τk−1 ≥ δmin,

∀k ∈ {1, . . . , K}.

(79)

Since π∗ satisﬁes supI⊆[0,1] supa∈I,x∈X |I|π∗(a; x, I) (cid:16) 1, there exists some constant ¯c4 > 0
such that π∗(a; x, (cid:98)d(x)) ≤ ¯c4| (cid:98)d(x)|−1 for all a and x. This together with equation 79 yields
that

π∗(a; x, (cid:98)d(x)) ≤ ¯c4δ−1
min,

∀a ∈ [0, 1], x ∈ X.

(80)

The rest of our proof is divided into three parts. In the ﬁrst part, we show that there

exists some constant C > 0 such that

(cid:107)(cid:98)θ[(cid:98)τk−1,(cid:98)τk) − (cid:98)θ[τ0,k−1,τ0,k)(cid:107)2 ≤

C log n
n

,

∀k ∈ {1, . . . , K},

(81)

with probability at least 1 − O(n−2). Using similar arguments in Lemma 1, we can show

that there exists some constant c3 > 0 such that the following events occur with probability

at least 1 − O(n−2):

(cid:107)(cid:98)θ[τ0,k−1,τ0,k) − θ0,[τ0,k−1,τ0,k)(cid:107)2 ≤

This together with equation 81 implies that

√

c3
√

log n
nδmin

,

∀k ∈ {1, . . . , K}.

√

2c3
√

log n

nδmin

(cid:107)(cid:98)θ[(cid:98)τk−1,(cid:98)τk) − θ0,[τ0,k−1,τ0,k)(cid:107)2 ≤

,

∀k ∈ {1, . . . , K},

(82)

for suﬃciently large n, with probability at least 1 − O(n−2).

In the second part, we deﬁne an integer-valued function (cid:98)K(x) as follows. We set (cid:98)K(x) = k
if (cid:98)d(x) = [(cid:98)τk−1, (cid:98)τk) for some integer k such that 1 ≤ k ≤ K − 1, and set (cid:98)K(x) = K if
(cid:98)d(x) = [(cid:98)τK−1, 1]. By the deﬁnition of (cid:98)θI and θ0,I, we have almost surely (cid:98)θ[(cid:98)τK−1,1) = (cid:98)θ[(cid:98)τK−1,1]
and θ0,[τ0,K−1,1) = θ0,[τ0,K−1,1]. It is immediate to see that

(cid:98)K(x) = sarg max

k∈{1,...,K}

¯x(cid:62)(cid:98)θ[(cid:98)τk−1,(cid:98)τk),

(83)

where sarg max denotes the smallest maximizer when the argmax is not unique. In Part 2,

we focus on proving

V π∗( (cid:98)d) ≥ E

(cid:16)

(cid:62)

X

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X))

(cid:17)

− O(1)n−1 log n,

(84)

69

with probability at least 1 − O(n−2), where O(1) denotes some positive constant.

In the last part, we provide an opper bound for

V opt − E

(cid:16)

(cid:62)

X

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X))

(cid:17)

.

This together with equation 84 yields the desired results.

Proof of Part 1: Let (cid:98)∆k = [(cid:98)τk−1, (cid:98)τk) ∪ [τ0,k−1, τ0,k)c + [(cid:98)τk−1, (cid:98)τk)c ∪ [τ0,k−1, τ0,k). With some
calculations, we can show that

where

ζ1(k) =

ζ2(k) =

ζ4(k) =

−

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:32)

1
n

(cid:32)

(cid:107)(cid:98)θ[(cid:98)τk−1,(cid:98)τk) − (cid:98)θ[τ0,k−1,τ0,k)(cid:107)2 ≤ ζ1(k)ζ2(k) + ζ3(k)ζ4(k),

1
n

n
(cid:88)

i=1

I(τ0,k−1 ≤ Ai < τ0,k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1)Ep+1

(cid:33)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

,

n
(cid:88)

i=1

I(Ai ∈ (cid:98)∆k)X iYi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

, ζ3(k) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

I(τ0,k−1 ≤ Ai < τ0,k)X iYi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

,

I(τ0,k−1 ≤ Ai < τ0,k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1)Ep+1

(cid:33)−1

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

I((cid:98)τk−1 ≤ Ai < (cid:98)τk)X iX

(cid:62)

i + λn((cid:98)τk − (cid:98)τk−1)Ep+1

(cid:33)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

Similar to equation 57, we can show with probability at least 1 − O(n−2) that

max
k∈{1,...,K}

ζ1(k) = O(1) and max

k∈{1,...,K}

ζ5(k) = O(1),

(85)

where

ζ5(k) =

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

I((cid:98)τk−1 ≤ Ai < (cid:98)τk)X iX

(cid:62)

i + λn((cid:98)τk − (cid:98)τk−1)Ep+1

(cid:33)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

Under the event deﬁned in equation 78, the Lebesgue measure of (cid:98)∆k is uniformly bounded by
2cn−1 log n, for any k ∈ {1, . . . , K}. Using similar arguments in equation 35 and equation 36,

we can show with probability at least 1 − O(n−2) that

max
k∈{1,...,K}

ζ2(k) = O(n−1 log n).

(86)

70

Similar to equation 60, we can show with probability at least 1 − O(n−2) that

max
k∈{1,...,K}

ζ3(k) = O(1).

(87)

Notice that ζ4(k) can be upper bounded by

ζ4(k) ≤ ζ1(k)ζ5(k)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

I(τ0,k−1 ≤ Ai < τ0,k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1)Ep+1

−

1
n

n
(cid:88)

i=1

I((cid:98)τk−1 ≤ Ai < (cid:98)τk)X iX

(cid:62)

i − λn((cid:98)τk − (cid:98)τk−1)Ep+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ ζ1(k)ζ5(k)

I(Ai ∈ (cid:98)∆k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1 − (cid:98)τk + (cid:98)τk−1)Ep+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

Under the condition λn = O(n−1 log n), using similar arguments in equation 35 and equa-
tion 36, we can show that with probability at least 1 − O(n−2), the absolute value of each

element in the matrix

1
n

n
(cid:88)

i=1

I(Ai ∈ (cid:98)∆k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1 − (cid:98)τk + (cid:98)τk−1)Ep+1

is upper bounded by O(n−1 log n), uniformly for any k ∈ {1, . . . , K}. It follows that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

I(Ai ∈ (cid:98)∆k)X iX

(cid:62)

i + λn(τ0,k − τ0,k−1 − (cid:98)τk + (cid:98)τk−1)Ep+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

= O(n−1 log n).

In view of equation 85, we obtain that

max
k∈{1,...,K}

ζ4(k) = O(n−1 log n),

(88)

with probability at least 1 − O(n−2). Combining equation 85-equation 88 yields equation 81.

Proof of Part 2: It follows from Condition (A4) and the deﬁnition of the conditional Orlicz

norm that

(cid:26)

E

exp

(cid:18) |X (j)|2
ω2

(cid:19)(cid:27)

(cid:20)

(cid:26)

= E

E

exp

(cid:18) |X (j)|2
ω2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)(cid:21)

A

≤ 2,

for any j ∈ {1, . . . , p}. Without loss of generality, suppose ω ≥ log−1/2 2. Then, we have

(cid:40)

E

exp

(cid:32)

|X

(j)

|2

(cid:33)(cid:41)

ω2

(cid:34)

(cid:40)

= E

E

exp

(cid:32)

|X

(j)

|2

ω2

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)(cid:35)

A

≤ 2,

71

for any j ∈ {1, . . . , p + 1}. As a result, it follows from Bonferroni’s inequality and Markov’s

inequality that

(cid:16)

Pr

(cid:107)X(cid:107)2 > ω(cid:112)2(p + 1) log n

p+1
(cid:88)

(cid:17)

≤

Pr(|X

(j)

| > ω(cid:112)2 log n)

(cid:40)

E

exp

≤

p+1
(cid:88)

j=1

(cid:32)

|X

(j)

|2

(cid:33)(cid:41)

ω2

/ exp

j=1
(cid:18) 2ω2 log n
ω2

(cid:19)

≤

2(p + 1)
n2

.

Thus, we obtain

where

Pr(A∗) ≥ 1 −

2(p + 1)
n2

,

(89)

A∗ = {(cid:107)X(cid:107)2 ≤ ω(cid:112)2(p + 1) log n}.

Consider the event

(cid:91)

A0 =

I1,I2∈P0

(cid:40)

0 <

(cid:12)
(cid:12)
(cid:12)X

(cid:62)

(cid:12)
(cid:12)
(θ0,I1 − θ0,I2)
(cid:12) ≤

4(cid:112)2(p + 1)c3ω log n
nδmin

√

(cid:41)

.

By Condition (A5) and Bonferroni’s inequality, we have

Pr(A0) ≤

(cid:88)

I1,I2∈P0
I1(cid:54)=I2

(cid:32)

Pr

0 <

(cid:62)

(cid:12)
(cid:12)
(cid:12)X

(θ0,I1 − θ0,I2)

(cid:12)
(cid:12)
(cid:12) ≤

4(cid:112)2(p + 1)c3ω log n
nδmin

√

(cid:33)

(90)

≤ K 2

(cid:32)

4(cid:112)2(p + 1)c3ω log n
nδmin

√

(cid:33)γ

.

By the deﬁnition of V π∗(·), we have

(cid:18)(cid:90)

V π∗( (cid:98)d) = E

(cid:62)

X

(cid:98)d(X)

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:19)

.

Notice that the expectation in the above expression is taken with respect to X. Deﬁne an
interval-valued function (cid:98)d0(x) = [τ0,(cid:98)K(x)−1, τ0,(cid:98)K(x)) and set (cid:98)∆(x) = (cid:98)d(x) ∩ { (cid:98)d0(x)}c. It follows
that

(cid:18)(cid:90)

V π∗( (cid:98)d) = E

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:19)

(cid:62)

X

(cid:98)d0(X)∩ (cid:98)d(X)

(cid:18)(cid:90)

(cid:62)

X

(cid:98)∆(X)

+ E

(cid:124)

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:19)

(cid:123)(cid:122)
χ1

(cid:125)

(cid:19)

+ χ1.

(cid:62)

X

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:18)(cid:90)

= E

(cid:98)d0(X)

72

Here, the second equality is due to that π∗(a; X, (cid:98)d(X)) = 0, for any a ∈ { (cid:98)d(X)}c. By
equation 25 and equation 80, we have

|χ1| ≤ c0¯c4δ−1

minE

(cid:18)(cid:90)

(cid:19)

(cid:107)X(cid:107)2da

(cid:98)∆(X)

= c0¯c4δ−1

minE(cid:107)X(cid:107)2λ( (cid:98)∆(X)),

where λ( (cid:98)∆(X)) denotes the Lebesgue measure of (cid:98)∆(X). Under the event deﬁned in
equation 78, we have λ( (cid:98)∆(X)) ≤ 2cn−1 log n, for any realization of X. It follows that

|χ1| ≤ 2cc0¯c4δ−1

min(n−1 log n)E(cid:107)X(cid:107)2.

By equation 48, we have

E(cid:107)X(cid:107)2

2 =

p+1
(cid:88)

j=1

E|X

(j)

|2 =

p+1
(cid:88)

j=1

(j)

E(E|X

|2|A) ≤ ω2(p + 1).

By Cauchy-Schwarz inequality, this further implies that

E(cid:107)X(cid:107)2 ≤

(cid:113)

E(cid:107)X(cid:107)2

2 ≤ ω(cid:112)p + 1.

This together with equation 91 yields

|χ1| ≤ 2cc0¯c4ω(cid:112)p + 1δ−1

minn−1 log n,

with probability at least 1 − O(n−2).

(91)

(92)

(93)

Notice that θ0(·) is a constant on (cid:98)d0(x) for any x. It follows that
(cid:19)

(cid:18)(cid:90)

(cid:17) (cid:90)

E

(cid:62)

X

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:16)

(cid:62)

= E

X

(cid:98)d0(X)

(cid:16)

(cid:62)

X

= E

θ0,[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))
(cid:17) (cid:90)

θ0,[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))

(cid:98)d0(X)

π∗(a; X, (cid:98)d(X))da

π∗(a; X, (cid:98)d(X))da

(cid:98)d0(X)∩ (cid:98)d(X)

(cid:16)

(cid:62)

X

θ0,[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))

(cid:17)

− χ2,

(cid:16)

(cid:62)

X

= E

θ0,[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))

(cid:17) (cid:90)

(cid:98)d(X)

π∗(a; X, (cid:98)d(X))da − χ2 = E

where

χ2 = E

(cid:16)

(cid:62)

X

θ0,[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))

(cid:17) (cid:90)

(cid:98)∆(X)

π∗(a; X, (cid:98)d(X))da.

Similar to equation 93, we can show that

|χ2| = O(n−1 log n),

73

with probability at least 1 − O(n−2). This together with equation 93 yields equation 84.

Proof of Part 3 : Similar to the deﬁnition of (cid:98)K, we deﬁne

K0(x) = sarg max
k∈{1,...,K}

¯x(cid:62)θ0,[τ0,k−1,τ0,k).

(94)

Let

(cid:40)

(cid:41)

K∗(x) =

k0 : k0 = arg max
k∈{1,...,K}

¯x(cid:62)θ0,[τ0,k−1,τ0,k)

,

denote the set that consists of all the maximizers. Apparently, K0(x) ∈ K∗(x), ∀x ∈ X.

We now claim that

(cid:98)K(X) ∈ K∗(X),

(95)

under the events deﬁned in Ac

0 ∩ A∗ and equation 82. Otherwise, suppose there exists some

k0 ∈ {1, . . . , K} such that

(cid:62)

X

(cid:98)θ[(cid:98)τk0−1,(cid:98)τk0 ) ≥ max
k(cid:54)=k0

(cid:62)

X

(cid:98)θ[(cid:98)τk−1,(cid:98)τk),

(cid:62)

X

max
k(cid:54)=k0

θ0,[τ0,k−1,τ0,k) > X

(cid:62)

θ0,[τ0,k0−1,τ0,k0 ).

Under Ac

0, it follows from equation 97 that

(cid:62)

X

max
k(cid:54)=k0

θ0,[τ0,k−1,τ0,k) > X

(cid:62)

θ0,[τ0,k0−1,τ0,k0 ) +

4(cid:112)2(p + 1)c3ω log n
nδmin

√

.

(96)

(97)

(98)

Under the events deﬁned in A∗ and equation 82, we have

max
k∈{1,...,K}

(cid:62)

|X

((cid:98)θ[(cid:98)τk−1,(cid:98)τk) − θ0,[τ0,k−1,τ0,k))| ≤ (cid:107)X(cid:107)2 max

k∈{1,...,K}

(cid:107)(cid:98)θ[(cid:98)τk−1,(cid:98)τk) − θ0,[τ0,k−1,τ0,k)(cid:107)2
2(cid:112)2(p + 1)c3ω log n
nδmin

√

≤

.

This together with equation 98 yields that

(cid:62)

X

max
k(cid:54)=k0

(cid:98)θ[(cid:98)τk−1,(cid:98)τk) > X

(cid:62)

(cid:98)θ[(cid:98)τk0−1,(cid:98)τk0 ).

In view of equation 96, we have reached an contradiction. Therefore, equation 95 holds

under the events deﬁned in Ac

0 ∩ A∗ and equation 82. When equation 95 holds, it follows

74

from the deﬁnition of K∗(·) that X

(cid:62)

θ0,[(cid:98)K(X)−1,(cid:98)K(X)) = X

(cid:62)

θ0,[K0(X)−1,K0(X)). Therefore, under

the event deﬁned in equation 82, we have

(cid:16)

(cid:62)

X

E

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X))

(cid:17)

(cid:16)

(cid:62)

X

= E

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X))

(cid:17)

(cid:16)

X

(cid:62)

+ E
(cid:124)

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X))

(cid:123)(cid:122)
χ3

(cid:17)

I(A0 ∪ A∗c)
(cid:125)

(cid:16)

(cid:62)

X

= E

I(Ac

0 ∩ A∗)
(cid:17)

(99)

θ0,[τ0,K0(X)−1,τ0,K0(X))

I(Ac

0 ∩ A∗)

+ χ3 = E

(cid:16)

(cid:62)

X

θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)

+ χ3 − E
(cid:124)

(cid:16)

(cid:62)

X

θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:123)(cid:122)
χ4

(cid:17)

I(A0 ∪ A∗c)
(cid:125)

.

Notice that

χ3 − χ4 = EX

(cid:62) (cid:16)

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)) − θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)

I(A0 ∪ A∗c).

Using similar arguments in showing equation 95, we can show that under the event deﬁned

in equation 82,

only when

(cid:62) (cid:16)

X

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)) − θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)

(cid:54)= 0,

0 <

(cid:62) (cid:16)

(cid:12)
(cid:12)
(cid:12)X

θ0,[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)) − θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤

4(cid:112)2(p + 1)c3ω log n
nδmin

√

.

Therefore, under the event deﬁned in equation 82, we have

|χ3 − χ4| ≤

4(cid:112)2(p + 1)c3ω log n
nδmin

√

Pr(A0 ∪ A∗c).

It follows from equation 89 and equation 90, we have
4(cid:112)2(p + 1)c3ω log n
nδmin

2(p + 1)
n2

|χ3 − χ4| ≤

+ K 2

√

(cid:40)

(cid:32)

4(cid:112)2(p + 1)c3ω log n
nδmin

√

(cid:33)γ(cid:41)

.

For suﬃciently large n, this together with equation 84 and equation 99 implies that we have

with probability at least 1 − O(n−2),

V π∗( (cid:98)d) ≥ E

(cid:16)

(cid:62)

X

θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)

− O(1)(n−1 log n + n−(1+γ)/2 log1+γ n),

for some positive constant O(1). The proof is hence completed by noting that

V opt = E

(cid:16)

(cid:62)

X

θ0,[τ0,K0(X)−1,τ0,K0(X))

(cid:17)

.

75

B.6 Proof of Theorem 3

We ﬁrst introduce some technical lemmas. We remark that the key ingredient of the proof

lies in Lemma 5, which establishes a uniform upper bound on the mean squared error of

(cid:98)qI. See Section B.7 for a detailed proof. The rest of the proof can be similarly proven
as Theorem 1. Speciﬁcally, we ﬁrst show the consistency of the estimated change point

locations. We then derive the rate of convergence of the estimated change point locations

and the estimated outcome regression function.

Lemma 5 Assume conditions in Theorem 3 are satisﬁed. Then there exists some constant
¯C > 0 such that the following holds with probability at least 1 − O(n−2): For any I ∈ I(m)

and |I| ≥ cγn,

E|qI,0(X) − (cid:98)qI(X)|2 ≤ ¯C(n|I|)−2β/(2β+p) log8 n,

where qI,0 = E(Y |A ∈ I, X) for any interval I.

(100)

(cid:4)

Lemma 6 Assume conditions in Theorem 3 are satisﬁed. Then there exists some constant
¯C > 0 such that the followings hold with probability at least 1 − O(n−2): For any I ∈ I(m)

and |I| ≥ cγn,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:88)

I∈ (cid:98)P

I(Ai ∈ I){Yi − qI,0(Xi)}{(cid:98)qI(Xi) − qI,0(Xi)}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ¯C(n|I|)p/(2β+p) log8 n,

for any I ∈ I(m) such that |I| ≥ cγn for any positive constant c > 0.

(cid:4)

Lemma 7 Under the conditions in Theorem 3, the following events occur with probability
at least 1 − O(n−2): there exists some constant C > 0 such that minI∈ (cid:98)P |I| ≥ Cγn.

(cid:4)

We next show the consistency of the estimated change-point locations. Using similar

arguments in proving equation 29, we can show that

| (cid:98)P| ≤ C0γ−1
n ,

(101)

for suﬃciently large n and some constant C0 > 0.

76

Notice that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

(cid:88)

n
(cid:88)

i=1

I∈ (cid:98)P
(cid:124)

I(Ai ∈ I){Yi − qI,0(Xi)}2

(cid:123)(cid:122)
η∗
1

(cid:125)

(cid:88)

n
(cid:88)

+

I∈ (cid:98)P

i=1

I(Ai ∈ I){(cid:98)qI(Xi) − qI,0(Xi)}2

I(Ai ∈ I){Yi − qI,0(Xi)}{(cid:98)qI(Xi) − qI,0(Xi)}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:88)

I∈ (cid:98)P

(cid:80)

The second line is non-negative. Under Lemmas 6 and 7, the third line is lower bounded by
I∈ (cid:98)P(n|I|)p/(2β+p) log8 n for some constant C1 > 0. By Hölder’s inequality, it can be
−C1
further lower bounded by −C1| (cid:98)P|2β/(2β+p)np/(2β+p) log8 n. By equation 101 and the given
condition on γn, the third line is o(n). It follows that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥ η∗

1 + o(n),

(102)

with probability at least 1 − O(n−2).

Similar to equation 22 and equation 23, we can show that the following events occur

with probability at least 1 − O(n−2),

n
(cid:88)

(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
n
(cid:12)
n−1/2(cid:113)

i=1

(cid:20)

≤ c0

I(Ai ∈ I){Yi − Q(Xi, Ai)}{Q(Xi, Ai) − qI,0(Xi)}

EI(A ∈ I){Q(X, A) − qI,0(X)}2 log n + n−1 log n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Q(Xi, Ai) − qI,0(Xi)}2 − EI(A ∈ I)|Q(X, A) − qI,0(X)|2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)
EI(A ∈ I){Q(X, A) − qI,0(X)}2 log n + n−1 log n

,

(cid:20)

≤ c0

n−1/2(cid:113)

for some constant c0 > 0 and any I. The two upper bounds are o(1). Similar to equation 33,

we can show that

η∗
1 =

n
(cid:88)

i=1

|Yi − Q(Xi, Ai)|2 + n

(cid:88)

I∈ (cid:98)P

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 + o(n),

77

with probability at least 1 − O(n−2). It follows from equation 102 that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

n
(cid:88)

i=1
(cid:124)

|Yi − Q(Xi, Ai)|2

(cid:123)(cid:122)
η∗
2

(cid:125)

+n

(cid:88)

I∈ (cid:98)P

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 + o(n),

(103)

with probability at least 1 − O(n−2).

Let us consider η∗

2. We observe that

η∗
2 =

(cid:88)

n
(cid:88)

I∈P0

i=1

I(Ai ∈ I)|Yi − qI,0(Xi)|2.

By the uniform approximation property of DNN, there exists some q∗

I ∈ QI such that

n
(cid:88)

i=1

|qI,0(Xi) − q∗

I(Xi)|2 ∝ n(n|I|)−2β/(2β+p).

See Part 1 of the proof of Lemma 5 for details. Similar to equation 22 and equation 23, we

can show that the following events occur with probability at least 1 − O(n−2),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − qI,0(Xi)}{qI,0(Xi) − q∗

I(Xi)}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c0

(cid:112)|I| log n
√
n

≤

(n|I|)−β/(2β+p),

for some constant c0 > 0 and any I ∈ P0. It follows that

η∗
2 −

(cid:88)

n
(cid:88)

I∈P0

i=1

−2

(cid:88)

I∈P0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I(Ai ∈ I)|Yi − q∗

I(Xi)|2 ≥ −

I(Ai ∈ I)|qI,0(Xi) − q∗

I(Xi)|2

I(Ai ∈ I){Yi − qI,0(Xi)}{qI,0(Xi) − q∗

I(Xi)}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ −¯cnp/(2β+p),

(cid:88)

n
(cid:88)

I∈P0

i=1

for some constant ¯c > 0. This together with equation 103 yields that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

(cid:88)

n
(cid:88)

I∈P0

i=1

I(Ai ∈ I)|Yi − q∗

I(Xi)|2

+n

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 + o(n) + O(np/(2β+p)),

(cid:88)

I∈ (cid:98)P

with probability at least 1 − O(n−2).

78

Next, using similar arguments in proving equation 37, we can show that there exist a

partition P ∗ ∈ B(m) and a set of functions {q∗∗

I : I ∈ P ∗} with |P ∗| = |P0| such that

(cid:88)

n
(cid:88)

I∈P0

i=1

I(Ai ∈ I)|Yi − q∗∗

I (Xi)|2 ≥

(cid:88)

n
(cid:88)

I∈P ∗

i=1

I(Ai ∈ I)|Yi − q∗∗

I (Xi)|2 + O(1).

It follows that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

(cid:88)

n
(cid:88)

I(Ai ∈ I)|Yi − q∗∗

I (Xi)|2

i=1
EI(A ∈ I)|Q(X, A) − qI,0(X)|2 + o(n) + O(np/(2β+p)),

I∈P ∗

+n

(cid:88)

I∈ (cid:98)P

with probability at least 1 − O(n−2). Since

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 + nγn| (cid:98)P|

(cid:88)

n
(cid:88)

≤

I∈P ∗

i=1

I(Ai ∈ I)|Yi − q∗∗

I (Xi)|2 + nγn|P0|,

and that γn → 0, we obtain that

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 = o(1).

(cid:88)

I∈ (cid:98)P

(104)

(105)

Under the condition that qI1,0 (cid:54)= qI2,0 for any adjacent I1, I2 ∈ P0, we have E|qI1,0(X) −
qI2,0(X)|2 > 0. Using similar arguments in the Part 1 of the proof of Theorem 1, we obtain
that maxτ ∈J(P0) minˆτ ∈J( (cid:98)P) |ˆτ − τ | ≤ δ for any constant δ > 0. This further implies that
| (cid:98)P| ≥ |P0|.

We next derive the rate of convergence of the estimated change point locations and

the estimated outcome regression function. Similar to equation 104, with a more reﬁned

analysis (see e.g., Step 2 of the proof of Theorem 1), we obtain that

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

(cid:88)

n
(cid:88)

I∈P ∗

i=1

I(Ai ∈ I)|Yi − q∗∗

I (Xi)|2

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 − C1| (cid:98)P|2β/(2β+p)np/(2β+p) log8 n + O(np/(2β+p)),

+n

(cid:88)

I∈ (cid:98)P

79

with probability at least 1 − O(n−2). This together with equation 105 yields that

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 ≤ C1| (cid:98)P|2β/(2β+p)np/(2β+p) log8 n

(cid:88)

n

I∈ (cid:98)P

+O(np/(2β+p)) + nγn(|P0| − | (cid:98)P|).

Under the given condition on γn, we obtain that | (cid:98)P| ≤ |P0|. Combining this together with

| (cid:98)P| ≥ |P0|, we obtain that | (cid:98)P| = |P0|. This proves the results in (i).

Consequently, we obtain that

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 = O(np/(2β+p) log8 n),

(cid:88)

n

I∈ (cid:98)P

As such, we have that

EI(A ∈ I)|Q(X, A) − qI,0(X)|2 = O(n−2β/(2β+p) log8 n),

(cid:88)

I∈ (cid:98)P

This together with Lemma 5 proves the result in (iii). Using similar arguments in Part 2 of

the proof of Theorem 1, we can show the result in (ii) holds. This completes the proof.

B.7 Proof of Lemma 5

By deﬁnition, (cid:98)qI is the minimizer of the least square loss, arg minq∈QI
q(Xi)|2. It follows that

(cid:80)n

i=1

I(Ai ∈ I)|Yi −

n
(cid:88)

i=1

I(Ai ∈ I)|Yi − (cid:98)qI(Xi)|2 ≥

n
(cid:88)

i=1

I(Ai ∈ I)|Yi − q(Xi)|2,

for all q ∈ QI. Let qI,0(x) = E(Y |A ∈ I, X = x), we have E[I(A ∈ I){Y − qI,0(X)}|X] = 0.

A simple calculation yields

n
(cid:88)

i=1

I(Ai ∈ I)|qI,0(Xi) − (cid:98)qI(Xi)|2 ≤

n
(cid:88)

i=1

I(Ai ∈ I)|qI,0(Xi) − q(Xi)|2

+2

n
(cid:88)

i=1

I(Ai ∈ I){Yi − qI,0(Xi)}{(cid:98)qI(Xi) − qI,0(Xi)},

for any q and I.

80

The ﬁrst term on the RHS measures the approximation bias of the DNN class. Since
E[I(A ∈ I){Y − qI,0(X)}|X] = 0, the second term corresponds to the stochastic error. The

rest of the proof is divided into three parts. In Part 1, we bound the approximation error.

In Part 2, we bound the stochastic error. Finally, we combine these two parts together to

derive the uniform convergence rate for (cid:98)qI.
Part 1. Under the given condition, we have Q(•, a) ∈ Φ(β, c), p(a|•) ∈ Φ(β, c) for some c > 0

and any a. We now argue that there exists some constant C > 0 such that qI ∈ Φ(β, C) for

any I. This can be proven based on the relation that

qI(x) =

(cid:82)

I Q(x, a)p(a|x)da
I p(a|x)da

(cid:82)

.

Speciﬁcally, we have that supx |qI(x)| ≤ supa,x |Q(x, a)| ≤ c. Suppose β ≤ 1. For
any x1, x2 ∈ X, consider the diﬀerence |qI(x1) − qI(x2)|. Under the assumption that

inf a,x p(a|x) ≥ c∗, it follows that

+

(cid:82)
I |Q(x2, a)||p(a|x1) − p(a|x2)|da
I p(a|x1)da

(cid:82)

(cid:82)

+

|qI(x1) − qI(x2)| ≤

(cid:82)
I |Q(x1, a) − Q(x2, a)|p(a|x1)da
(cid:82)
I p(a|x1)da
I |Q(x2, a)|p(a|x2)da (cid:82)
I |p(a|x1) − p(a|x2)|da
I p(a|x1)da (cid:82)
(cid:82)
I p(a|x2)da
c2
≤ c(cid:107)x1 − x2(cid:107)β−(cid:98)β(cid:99) + 2
c∗

(cid:107)x1 − x2(cid:107)β−(cid:98)β(cid:99).

Consequently, qI ∈ Φ(β, c + 2c2/c2

∗).

Suppose β > 1. Then both Q(•, a) and p(a|•) are (cid:98)β(cid:99)-diﬀerentiable. By changing the

order of integration and diﬀerentiation, we can show that qI(x) is (cid:98)β(cid:99)-diﬀerentiable as well.

As an illustration, when β < 2, we have (cid:98)β(cid:99) = 1. According to the chain rule, we have

(cid:82)

∂qI(x)
∂xj =

I{∂Q(x, a)/∂xj}p(a|x)da
(cid:82)
I p(a|x)da
(cid:82)

+
(cid:82)
I Q(a|x)p(a|x)da (cid:82)

I Q(a|x){∂p(a|x)/∂xj}da
I p(a|x)da
I{∂p(a|x)/∂xj}da

(cid:82)

−

.

{(cid:82)

I p(a|x)da}2

Moreover, using similar arguments in proving qI ∈ Φ(β, c + 2c2/c2

∗) when β < 1, we can
show that all the partial derivatives of qI(x) up to the (cid:98)β(cid:99)th order are uniformly bounded

for all I. In addition, all the (cid:98)β(cid:99)th order partial derivatives are Hölder continuous with

exponent β − (cid:98)β(cid:99). This implies that qI ∈ Φ(β, C) for some constant C > 0 and any I.

81

It is shown in Lemma 7 of Farrell et al. (2021) that for any (cid:15) > 0, there exists a DNN

architecture that approximates qI with the uniform approximation error upper bounded by
(cid:15), and satisﬁes WI ≤ ¯C(cid:15)−p/β(log (cid:15)−1 + 1) and LI ≤ ¯C(log (cid:15)−1 + 1) for some constant ¯C > 0.
These upper bounds will be used later in Part 2. The detailed value of (cid:15) will be speciﬁed

below. It follows that for any I, the bias term can be upper bounded by

n
(cid:88)

i=1

I(Ai ∈ I)|qI,0(Xi) − q(Xi)|2 ≤ (cid:15)2

n
(cid:88)

i=1

I(Ai ∈ I).

Using similar arguments in proving equation 23, we can show that uniformly for all I
such that |I| ≥ cγn, the RHS can be upper bounded by O(1)(nI + (cid:112)n|I| log n) with
probability tending to 1, where O(1) denotes some universally constant that is independent

of I. It follows that uniformly for all I such that |I| ≥ cγn, there exists a DNN with
WI ≤ ¯C(cid:15)−p/β(log (cid:15)−1 + 1) and LI ≤ ¯C(log (cid:15)−1 + 1) such that

n
(cid:88)

i=1

I(Ai ∈ I)|qI,0(Xi) − q(Xi)|2 ∝ (cid:15)2(n|I| + (cid:112)n|I| log n),

for any suﬃciently small (cid:15) > 0.

Set (cid:15) to (n|I|)−β/(2β+p), it follows that

n
(cid:88)

i=1

I(Ai ∈ I)|qI,0(Xi) − q(Xi)|2 ∝ (n|I|)−2β/(2β+p)(n|I| + (cid:112)n|I| log n).

(106)

WI and LI are upper bounded by ¯C(n|I|)p/(2β+p)(β log(n|I|)/(2β+p)+1) and ¯C(β log(n|I|)/(2β+
p) + 1), respectively. This completes the proof for Part 1.

Part 2. We will apply the empirical process theory (see e.g., Van Der Vaart and Wellner,

1996a) to bound the stochastic error. Let (cid:98)θI be the estimated parameter in (cid:98)qI. Deﬁne

σ2(I, θ) = EI(A ∈ I)|qI,0(X) − qI(X, θ)|2,

for any θ and I.

Consider two separate cases, corresponding to σ(I, (cid:98)θI) ≤ k0|I|1/2(n|I|)−β/(2β+p) log4 n
and σ(I, (cid:98)θI) > k0|I|1/2(n|I|)−β/(2β+p) log4 n, respectively, for some constant k0 > 0. The
detailed form of k0 will be speciﬁed later in Part 3. We focus our attentions on the latter

class of intervals. In Part 3, we will show that for those intervals, with proper choice of k0,

(cid:16)

(cid:17)
σ(I, (cid:98)θI) ≤ k0|I|1/2(n|I|)−β/(2β+p) log4 n

Pr

≥ 1 − O(n−4).

82

By Bonferroni’s inequality and the condition that m (cid:16) n, this implies that for any I, we

have

σ(I, (cid:98)θI) ≤ k0|I|1/2(n|I|)−β/(2β+p) log4 n,

(107)

with probability at least 1 − O(n−2).

For a given integer k ≥ k0. We consider the stochastic error,

I(Ai ∈ I){Yi − qI,0(Xi)}{(cid:98)q((cid:96))

I (Xi) − qI,0(Xi)},

(cid:88)

i∈Lc
(cid:96)

for any I such that

k|I|1/2(n|I|)−β/(2β+p) log4 n ≤ σ(I, (cid:98)θI)

≤ (k + 1)|I|1/2(n|I|)−β/(2β+p) log4 n.

(108)

The absolute value of the stochastic error can be upper bounded by

Z(I) ≡ sup
θ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈Lc
(cid:96)

I(Ai ∈ I){Yi − qI,0(Xi)}{qI,0(Xi, θ) − qI,0(Xi)}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where the supremum is taken over all θ such that equation 108 holds with (cid:98)θI replaced by θ.

For a given θ, the empirical sum has zero mean. Under the boundedness assumption

on Y , its standard deviation is upper bounded by O(1)k|I|1/2(n|I|)−β/(2β+p) log4 n for some
universal constant O(1). In addition, each quantity I(Ai ∈ I){Yi − qI,0(Xi)}{qI,0(Xi, θ) −

qI,0(Xi)} is upper bounded by some universal constant. This allows us to apply the tail

inequality developed by Massart et al. (2000) to bounded the empirical process. See also

Theorem 2 of Adamczak et al. (2008). Speciﬁcally, for all t > 0 and I, we obtain with

probability at least 1 − exp(t) that

Z(I) ≤ 2EZ(I) + ¯ck(cid:112)tn|I|(n|I|)−β/(2β+p) log4(n) + ¯ct,

(109)

for some constant ¯c > 0. By setting t = 4 log(nk), we have 1 − exp(t) = 1 − n−4k−4. Notice

that the number of intervals I is upper bounded by O(n2), under the condition that m

is proportional to n. By Bonferroni’s inequality, we obtain that equation 109 holds with

probability at least 1 − O(n2) for any I. Under the given condition on γn, for any interval

83

I such that |I| ≥ cγn, the last term on the right-hand-side of equation 109 is dominated by

the second term. It follows that the following occurs with probability 1 − O(k−2n−2),
Z(I) ≤ 2EZ(I) + 3¯ck(cid:112)n|I|(n|I|)−β/(2β+p)(cid:112)log(nk) log4 n,

(110)

for all I such that |I| ≥ cγn and that equation 108 holds.

We next provide an upper bound for EZ(I). Toward that end, we will apply the maximal

inequality developed in Corollary 5.1 of Chernozhukov et al. (2014). We ﬁrst observe that

the class of empirical sum indexed by θ belongs to the VC subgraph class with VC-index

upper bounded by O(WILI log(WI)). It follows that for any I such that |I| ≥ cγn and

equation 108 holds,

(cid:113)

EZ(I) ∝

k(n|I|)p/(2β+p)WILI log(WI) log(nk) log4 n + WILI log(WI) log(nk).

Based on the upper bounds on WI and LI developed in Part 1, the right-hand-side is upper

bounded by

√

O(1)

k(n|I|)p/(2β+p)(cid:112)log(nk) log11/2 n,

where O(1) denotes some universal constant.

This together with equation 109 and equation 110 yields that with probability at least
k(n|I|)p/(2β+p)(cid:112)log(nk) log11/2 n, for

1 − O(n−2k−2), the stochastic error is of the order

√

any I that satisﬁes |I| ≥ cγn and equation 108. This completes the proof for Part 2.

Part 3. Combining the results in Part 1 and Part 2, we obtain that for any I such that
|I| ≥ cγn, k|I|1/2(n|I|)−β/(2β+p) log4 n ≤ σ(I, (cid:98)θI) ≤ (k + 1)|I|1/2(n|I|)−β/(2β+p) log4 n,

I(Ai ∈ I)|qI,0(Xi) − (cid:98)q((cid:96))

I (Xi)|2 ≤ O(1)

√

k(n|I|)p/(2β+p)(cid:112)log(nk) log11/2 n,

(cid:88)

i∈Lc
(cid:96)

with probability at least 1 − O(n−2k−2). As for the left-hand-side, we notice that

I(Ai ∈ I)|qI,0(Xi) − (cid:98)q((cid:96))

I (Xi)|2

(cid:88)

i∈Lc
(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈Lc
(cid:96)

≥

|Lc

(cid:96)|σ2(I, (cid:98)θI) −

I(Ai ∈ I)|qI,0(Xi) − (cid:98)q((cid:96))

I (Xi)|2 − |Lc

(cid:96)|σ2(I, (cid:98)θI)

=

L−1(L − 1)k(n|I|)p/(2β+p) log8 n −

I(Ai ∈ I)|qI,0(Xi) − (cid:98)q((cid:96))

I (Xi)|2 − |Lc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:96)|σ2(I, (cid:98)θI)
(cid:12)
(cid:12)
(cid:12)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈Lc
(cid:96)

84

Using similar arguments in Part 2, we can show that the second term in the last line is of
k(n|I|)p/(2β+p)(cid:112)log(nk) log11/2 n), with probability at least 1 − O(n−2k−2),

the order O(

√

for any I such that |I| ≥ cγn and equation 108 holds. Since L ≥ 2, we obtain

k(n|I|)p/(2β+p) log8 n ≤ O(1)

√

k(n|I|)p/(2β+p)(cid:112)log(nk) log11/2 n.

For a suﬃciently large k0, the above equation will not hold for any k ≥ k0. As such, the

probability

(cid:16)

Pr

k|I|1/2(n|I|)−β/(2β+p) log4 n ≤ σ(I, (cid:98)θI) ≤ (k + 1)|I|1/2(n|I|)−β/(2β+p) log4 n

(cid:17)

≤ O(n−2k−2),

for any k ≥ k0. It follows from Bonferroni’s inequality that

(cid:16)

(cid:17)
σ(I, (cid:98)θI) ≥ k0|I|1/2(n|I|)−β/(2β+p) log4 n

Pr

≤ O

(cid:33)

n−2k−2

= O(k0n−2).

(cid:32) +∞
(cid:88)

k=k0

We thus obtain equation 107 holds with probability at least 1 − O(n−2). Under the

assumption that the density function b(a|x) is uniformly bounded away from zero, we obtain

σ2(I, (cid:98)θI) ≤ c|I|E|qI,0(X) − (cid:98)q((cid:96))

I (X)|2,

for some constant c > 0. This assertion thus follows.

B.8 Proof of Lemma 6

The assertion can be proven in a similar manner as Part 2 of the proof of Lemma 5. We

omit the details to save space.

B.9 Proof of Lemma 7

The assertion can be proven in a similar manner as Lemma 4. We omit the details to save

space.

B.10 Proof of Theorem 4

The proof of Theorem 4 is similar to that of Theorem 2. We provide the outline as below

and omit the duplicated arguments for brevity.

85

Under the events deﬁned in Theorem 3, we have (cid:98)K = K, and

max
k∈{1,...,K−1}

|(cid:98)τk − τ0,k| ≤ cn−2β/(2β+p) log8 n,

(111)

for some constant c > 0. By similar arguments in the proof of Theorem 2, there exists some
constant ¯C4 > 0 such that

π∗(a; x, (cid:98)d(x)) ≤ ¯C4δ−1
min,

∀a ∈ [0, 1], x ∈ X.

(112)

The rest of our proof is divided into two parts. In the ﬁrst part, we focus on proving

V π∗( (cid:98)d) ≥ E

(cid:16)

(cid:17)
q[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))(X)

− O(1)n−2β/(2β+p) log8 n,

(113)

with probability at least 1 − O(n−2), where O(1) denotes some positive constant.

In Part 2, we provide an upper bound for

V opt − E

(cid:16)

(cid:17)
q[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))(X)

.

This together with equation 113 yields the desired results.

Proof of Part 1: Recall the integer-valued function

(cid:98)K(x) = sarg max

k∈{1,...,K}

(cid:98)q[(cid:98)τk−1,(cid:98)τk)(x),

(114)

where sarg max denotes the smallest maximizer when the argmax is not unique. Similarly,
we have (cid:98)K(x) = k if (cid:98)d(x) = [(cid:98)τk−1, (cid:98)τk) for some integer k such that 1 ≤ k ≤ K − 1, and set
(cid:98)K(x) = K if (cid:98)d(x) = [(cid:98)τK−1, 1].

Let (cid:98)∆k = [(cid:98)τk−1, (cid:98)τk) ∪ [τ0,k−1, τ0,k)c + [(cid:98)τk−1, (cid:98)τk)c ∪ [τ0,k−1, τ0,k). Using similar arguments in

the proof of Theorem 2, we have

(cid:18)(cid:90)

V π∗( (cid:98)d) = E

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:19)

(cid:98)d0(X)∩ (cid:98)d(X)

(cid:18)(cid:90)

(cid:98)∆(X)

+ E

(cid:124)

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:123)(cid:122)
χ∗
1

(cid:19)

(cid:125)

(cid:18)(cid:90)

= E

(cid:98)d0(X)

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:19)

+ χ∗
1,

where (cid:98)d0(x) = [τ0,(cid:98)K(x)−1, τ0,(cid:98)K(x)) and (cid:98)∆(x) = (cid:98)d(x) ∩ { (cid:98)d0(x)}c.

86

By equation 112 and the assumption that Y is bounded, we have

|χ∗

1| ≤ c0

¯C4δ−1

minλ( (cid:98)∆(X)),

where λ( (cid:98)∆(X)) denotes the Lebesgue measure of (cid:98)∆(X). Under the event deﬁned in
equation 111, we have λ( (cid:98)∆(X)) ≤ 2cn−2β/(2β+p) log8 n, for any realization of X. It follows
that

|χ∗

1| ≤ ¯C0δ−1

minn−2β/(2β+p) log8 n,

(115)

for some constant ¯C0 with probability at least 1 − O(n−2).

Using similar arguments in the proof of Theorem 2, we have

(cid:18)(cid:90)

E

(cid:98)d0(X)

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:19)

= E

(cid:16)

(cid:17)
q[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x))(X)

− χ∗
2,

where

(cid:16)

χ∗

2 = E

q[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x)),0(X)

(cid:17) (cid:90)

(cid:98)∆(X)

π∗(a; X, (cid:98)d(X))da.

Similar to equation 115, we can show that

|χ∗

2| = O(n−2β/(2β+p) log8 n),

with probability at least 1 − O(n−2). This together with equation 115 yields equation 113.

Proof of Part 2: Let (cid:15)n = ¯C1(nδmin)−2β/{(2β+p)(2+γ)} log8/(2+γ) n for some constant ¯C1. Deﬁne
an event

A(cid:15) =

(cid:91)

k

(cid:8)|q[τ0,k−1,τ0,k)(X) − (cid:98)q[(cid:98)τk−1,(cid:98)τk)(X)| ≤ (cid:15)n

(cid:9) .

Based on Lemma 5, by Markov’s inequality, we can show that there exists some constant

¯c > 0 such that

Pr{|q[τ0,k−1,τ0,k),0(X) − (cid:98)q[(cid:98)τk−1,(cid:98)τk)(X)| > (cid:15)n}
¯C2(nδmin)−2β(1+γ)/{(2β+p)(2+γ)} log8(1+γ)/(2+γ) n, ∀k ∈ {1, . . . , K},

≤

(116)

with probability at least 1 − O(n−2) for some constant ¯C2. Thus, by Bonferroni’s inequality,
we have

Pr{Ac

(cid:15)} ≤ ¯C3(nδmin)−2β(1+γ)/{(2β+p)(2+γ)} log8(1+γ)/(2+γ) n

(117)

87

holds with probability at least 1 − O(n−2) for some constant ¯C3.

Consider the event

(cid:91)

A0 =

I1,I2∈P0

{0 < |qI1,0(X) − qI2,0(X)| ≤ 2(cid:15)n} .

By Condition (A5) and Bonferroni’s inequality, we have

Pr(A0) ≤

(cid:88)

I1,I2∈P0
I1(cid:54)=I2

Pr (0 < |qI1,0(X) − qI2,0(X)| ≤ 2(cid:15)n) ≤ K 2 (2(cid:15)n)γ .

(118)

Similar to the deﬁnition of (cid:98)K, we deﬁne

K0(x) = sarg max
k∈{1,...,K}

q[τ0,k−1,τ0,k),0(x).

(119)

Let

(cid:40)

(cid:41)

K∗(x) =

k0 : k0 = arg max
k∈{1,...,K}

q[τ0,k−1,τ0,k),0(x)

,

denote the set that consists of all the maximizers. Apparently, K0(x) ∈ K∗(x), ∀x ∈ X.

We now claim that

(cid:98)K(X) ∈ K∗(X),

(120)

under the events deﬁned in Ac

0 and A(cid:15). Otherwise, suppose there exists some k0 ∈ {1, . . . , K}

such that

(cid:98)q[(cid:98)τk0−1,(cid:98)τk0 )(X) ≥ max
max
k(cid:54)=k0

k(cid:54)=k0 (cid:98)q[(cid:98)τk−1,(cid:98)τk)(X),

q[τ0,k−1,τ0,k),0(X) > q[τ0,k0−1,τ0,k0 ),0(X).

Under Ac

0, it follows from equation 122 that

max
k(cid:54)=k0

q[τ0,k−1,τ0,k),0(X) > q[τ0,k0−1,τ0,k0 ),0 + 2(cid:15)n.

Under the event A(cid:15), we have

max
k∈{1,...,K}

|(cid:98)q[(cid:98)τk−1,(cid:98)τk)(X) − q[τ0,k−1,τ0,k),0(X)| ≤ (cid:15)n.

88

(121)

(122)

(123)

This together with equation 123 yields that

k(cid:54)=k0 (cid:98)q[(cid:98)τk−1,(cid:98)τk)(X) > (cid:98)q[(cid:98)τk0−1,(cid:98)τk0 )(X).
max

In view of equation 121, we have reached a contradiction. Therefore, equation 120 holds

under the events deﬁned in Ac

0 and A(cid:15).

By the deﬁnition of K∗(·) that q[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)),0(X) = q[τ0,K0(X)−1,τ0,K0(X)),0(X) when

equation 120 holds. Using the similar augments in equation 99, we have

(cid:16)

(cid:17)
q[τ0,(cid:98)K(x)−1,τ0,(cid:98)K(x)),0(X)

E

= E

(cid:16)

(cid:17)
q[τ0,K0(X)−1,τ0,K0(X)),0(X)

+ χ3 + χ4,

(124)

where

and

χ3 = E

(cid:16)

(cid:17)
q[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)),0(X) − q[τ0,K0(X)−1,τ0,K0(X)),0(X)

I(A0)I(A(cid:15)),

(cid:16)

χ4 = E

q[τ0,(cid:98)K(X)−1,τ0,(cid:98)K(X)),0(X) − q[τ0,K0(X)−1,τ0,K0(X)),0(X)

(cid:17)

I(Ac

(cid:15)),

Therefore, under the event A(cid:15), it follows from equation 118 that

|χ3| ≤ K 2 (2(cid:15)n)γ+1 .

(125)

Similarly, by Condition (A7) and the outcome is bounded, following Markov’s inequality,

we have

|χ4|

≤ ¯C3Pr{Ac
(cid:15)}

(126)

Based on equation 117 and (cid:15)n = ¯C1(nδmin)−2β/{(2β+p)(2+γ)} log8/(2+γ) n, for suﬃciently
large n, the above equation 126 and equation 125 together with equation 113 and equation 124

implies that we have with probability at least 1 − O(n−2),

V π∗( (cid:98)d) ≥ V opt − O(1)(n− 2β

2β+p log8 n + n− 2β(1+γ)

(2β+p)(2+γ) log

8+8γ
2+γ n),

for some positive constant O(1). The proof is hence completed.

89

B.11 Proof of Theorem 5

We focus on proving Theorem 5 (ii) when conditions in Theorem 4 are satisﬁed with

4β(1 + γ) > (2β + p)(2 + γ), where D-JIL is applied. Since the piecewise linear case requires

weaker conditions (when conditions in Theorem 2 are satisﬁed), one can similarly derive the

asymptotic normality of (cid:98)V under L-JIL.

We present an outline of the proof ﬁrst, which can be divided into two parts. Deﬁne
d0(x) = arg maxI∈P0 qI,0(x), ∀x ∈ X. Under the given conditions, the maximizers d0(Xi)’s
are almost surely unique. By the deﬁnition of (cid:98)K(·) in equation 114, we have

(cid:98)V =

1
n

n
(cid:88)

i=1

(cid:34) I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:8)Yi − (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:98)K(Xi))(Xi)(cid:9) + (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:35)
(cid:98)K(Xi))(Xi)

.

Given K0(·) deﬁned in equation 119, the above value estimator can be decomposed by

(cid:98)V = (cid:98)V1 +

n
(cid:88)

i=1

(cid:34)(cid:40)I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:41)

− 1

1
n
(cid:124)

(cid:8)q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi) − (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:98)K(Xi))(Xi)(cid:9)

(cid:123)(cid:122)
η7

(cid:35)

,

(cid:125)

where

(cid:98)V1 =

1
n

n
(cid:88)

i=1

(cid:34) I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:35)
(cid:8)Yi − q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)(cid:9) + q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)

.

In Part 1, we ﬁrst establish the following result that

This implies

η7 = op(n−1/2).

(cid:98)V = (cid:98)V1 + op(n−1/2).

(127)

(128)

In the second step, we further decompose (cid:98)V1 as
(cid:34)(cid:40)I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

n
(cid:88)

−

i=1

(cid:98)V1 = (cid:98)V2 +

I{Ai ∈ d0(Xi)}
e(d0(Xi)|Xi)
(cid:123)(cid:122)
η8

1
n
(cid:124)

(cid:41)

(cid:8)Yi − q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)(cid:9)

(cid:35)

,

(cid:125)

90

where

(cid:98)V2 =

1
n

n
(cid:88)

i=1

(cid:20)I{Ai ∈ d0(Xi)}
e(d0(Xi)|Xi)

We focus on proving

(cid:21)
(cid:8)Yi − q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)(cid:9) + q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)

.

This together with equation 128 leads to

η8 = op(n−1/2).

(cid:98)V = (cid:98)V2 + op(n−1/2).

(129)

(130)

Combing the results in the ﬁrst two steps, it follows from the deﬁnition of d0(·) that

(cid:98)V =

1
n

n
(cid:88)

(cid:88)

i=1

I∈P0

I(I = d0(Xi))

(cid:20)I{Ai ∈ d0(Xi)}
e(d0(Xi)|Xi)

(cid:21)
(cid:8)Yi − qI,0(Xi)(cid:9) + qI,0(Xi)

+ op(n−1/2),

almost surely. Notice that the ﬁrst term at RHS corresponds to a sum of i.i.d random

variables. Hence, based on Lindeberg-Feller central limit theorem, one can show the

asymptotic normality result of the value estimator under the proposed I2DR.

Proof of Part 1: We aim to show equation 127. Toward that end, we deﬁne

(cid:98)V3 =

1
n

n
(cid:88)

i=1

(cid:34) I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:35)
(cid:8)Yi − q[τ0,(cid:98)K(Xi)−1,τ0,(cid:98)K(Xi)),0(Xi)(cid:9) + q[τ0,(cid:98)K(Xi)−1,τ0,(cid:98)K(Xi)),0(Xi)

.

The diﬀerence |η7| can be upper bounded by | (cid:98)V1 − (cid:98)V3| + | (cid:98)V − (cid:98)V3|. Consider | (cid:98)V1 − (cid:98)V3| ﬁrst.

Under the given conditions, the term

− 1

is bounded, it suﬃces to show that

(cid:110) I{Ai∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

(cid:111)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi) − q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:98)K(Xi)),0(Xi)

(cid:12)
(cid:12) = op(n−1/2),
(cid:12)

where K0(·) and (cid:98)K(·) are deﬁned in equation 119 and equation 114, respectively. Under the
margin-type condition, the above expression can be proven using similar arguments in the

proof of Theorem 4. We omit the details to save space.

It remains to show | (cid:98)V − (cid:98)V3| = op(n1/2). Notice that | (cid:98)V − (cid:98)V3| can be further upper

bounded by

n
(cid:88)

(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
n
(cid:12)
(cid:40)I{Ai ∈ (cid:98)d(Xi)}
e( (cid:98)d(Xi)|Xi)

i=1

(cid:40)I{Ai ∈ (cid:98)d(Xi)}
e( (cid:98)d(Xi)|Xi)

(cid:41)

− 1

(cid:41)

−

I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:8)q[τ0,(cid:98)K(Xi)−1,τ0,(cid:98)K(Xi)),0(Xi) − (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:8)q[τ0,(cid:98)K(Xi)−1,τ0,(cid:98)K(Xi)),0(Xi) − (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:12)
(cid:12)
(cid:98)K(Xi))(Xi)(cid:9)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:98)K(Xi))(Xi)(cid:9)
(cid:12)
(cid:12)
(cid:12)

.

91

Consider the ﬁrst line. Notice that it can be represented by

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

(cid:26) I{Ai ∈ I}
e(I|Xi)

(cid:27)

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:8)qI,0(Xi) − (cid:98)qI(Xi)(cid:9)I(I = (cid:98)d(Xi))
(cid:12)
(cid:12)
(cid:12)

.

Since the number of intervals in (cid:98)P is ﬁnite with probability tending to 1 (see Results (i) in
Theorem 1), to show the above expression is op(n−1/2), it suﬃces to show

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

sup
I∈I(m)

(cid:26) I{Ai ∈ I}
e(I|Xi)

(cid:27)

− 1

(cid:12)
(cid:12)
(cid:8)qI,0(Xi) − (cid:98)qI(Xi)(cid:9)I(I = (cid:98)d(Xi))
(cid:12)
(cid:12)
(cid:12)

= op(n−1/2).

The key observation is that, by Corollary A.1 of Chernozhukov et al. (2014), the above

empirical sum forms a VC-type class. Using similar arguments in bounding the stochastic

error in Step 2 of the proof of Lemma 5, we can show the above assertion holds.

To bound the second line, notice that by Cauchy-Schwarz inequality, it is smaller than

or equal to the square root of

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n
(cid:124)

I{Ai ∈ (cid:98)d(Xi)}
e( (cid:98)d(Xi)|Xi)

−

(cid:123)(cid:122)
η(1)
7

I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

1
n
(cid:124)

n
(cid:88)

i=1

|q[τ0,(cid:98)K(Xi)−1,τ0,(cid:98)K(Xi)),0(Xi) − (cid:98)q[(cid:98)τ

(cid:98)K(Xi)−1,(cid:98)τ

(cid:98)K(Xi))(Xi)|2
(cid:125)

(cid:123)(cid:122)
η(2)
7

Using similar arguments in establishing the uniform convergence rate of (cid:98)qI, we can show
that η(2)
7 = op(n−c) for some c > 1/2. To prove the second line is op(n−1/2), it remains to
show η(1)
7 = Op(n−1/2 log n). Under the positivity assumption on e and (cid:98)e, it suﬃces to show

1
n

n
(cid:88)

i=1

|e( (cid:98)d(Xi)|Xi) − (cid:98)e( (cid:98)d(Xi)|Xi)|2 = Op(n1/2 log n).

(131)

The left-hand-side can be further upper bounded by

(cid:88)

≤

I∈ (cid:98)P

E|e(I|X) − (cid:98)e(I|X)|2 +

(cid:34)

1
n

n
(cid:88)

i=1

(cid:88)

I∈ (cid:98)P

1
n

(cid:88)

n
(cid:88)

I∈ (cid:98)P

i=1

|e(I|Xi) − (cid:98)e(I|Xi)|2

(cid:35)

|e(I|Xi) − (cid:98)e(I|Xi)|2 − E|e(I|X) − (cid:98)e(I|X)|2

.

The ﬁrst term on the second line is Op(n−1/2) under Condition (A8) and the fact that

| (cid:98)P| = O(1) with probability tending to 1. To prove equation 131, by the boundedness of

92

| (cid:98)P|, it suﬃces to show the supremum of the empirical process term

(cid:34)

1
n

n
(cid:88)

i=1

sup
I∈I(m)

|e(I|Xi) − (cid:98)e(I|Xi)|2 − E|e(I|X) − (cid:98)e(I|X)|2

= Op(n−1/2 log n).

(cid:35)

Under Condition (A8), this can be proven in a similar manner as Step 2 of the proof of

Lemma 5. We omit the details to save space.

Proof of Part 2: We next focus on proving equation 129. We notice that |η8| can be upper

bounded by
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

i=1
n
(cid:88)

i=1

(cid:34)(cid:40)I{Ai ∈ (cid:98)d(Xi)}
(cid:98)e( (cid:98)d(Xi)|Xi)
(cid:34)(cid:40)I{Ai ∈ d0(Xi)}
e(d0(Xi)|Xi)

−

−

I{Ai ∈ (cid:98)d(Xi)}
e( (cid:98)d(Xi)|Xi)
I{Ai ∈ (cid:98)d(Xi)}
e( (cid:98)d(Xi)|Xi)

(cid:41)

(cid:41)

(cid:35)(cid:12)
(cid:12)
(cid:8)Yi − q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)(cid:9)
(cid:12)
(cid:12)
(cid:12)
(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:8)Yi − q[τ0,K0(Xi)−1,τ0,K0(Xi)),0(Xi)(cid:9)

.

The ﬁrst line can be shown to be op(n−1/2) using similar arguments in the proof of Part 1.
The second line can be shown to be op(n1/2) by noting that the diﬀerence between d0 and (cid:98)d
is asymptotically negligible. This completes the proof.

B.12 Proof of Theorem 6

Before proving Theorem 6, it is worth mentioning that results in Lemma 1 and Lemma 4

do not rely on the assumption that θ0(·) is piecewise constant. These lemmas hold under

the conditions in Theorem 6 as well. The proof is divided into two parts. In the ﬁrst

part, we derive the convergence rate of the integrated (cid:96)2 loss for (cid:98)θ. Then, we establish the

convergence rate of the value under our I2DR.

Convergence rate of the integrated (cid:96)2 loss: We ﬁrst establish the upper error bound on the

integrated (cid:96)2 loss of (cid:98)θ(·). Here, we consider a more general framework. Speciﬁcally, deﬁne

AEk(θ0) =

inf
P:|P|≤k+1

(θI )I∈P ∈(cid:81)

I∈P

(cid:40)

sup
a∈[0,1]

Rp+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

θ0(a) −

(cid:13)
(cid:13)
θII(a ∈ I)
(cid:13)
(cid:13)
(cid:13)2

(cid:41)

.

(cid:88)

I∈P

It describes how well θ0(·) can be approximated by a step function with at most k change

points. Consider the following class of functions

(cid:26)

Bα0 =

θ0(·) : lim sup

k→∞

kα0AEk(θ0) < ∞

(cid:27)

,

93

for some α0 > 0. The parameter α0 characterizes the speed of approximation as the number

of change points increases. According to the discussion in Section 4.2.1, the class of Hölder
continuous functions in Model II belongs to Bα0. In the following, we show with probability
at least 1 − O(n−2) that (cid:82) 1

for any θ0(·) ∈ Bα0.

2da ≤ ¯cγ2α0/(1+2α0)

0 (cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

n

Since θ0(·) ∈ Bα0, for some sequence {kn}n that satisﬁes kn → ∞ as n → ∞, there exists

a piecewise constant function θ∗(·) such that

θ∗(a) =

(cid:88)

I∈P ∗

θ∗
I

I(a ∈ I),

∀a ∈ [0, 1],

for some partition P ∗ of [0, 1] with |P ∗| ≤ kn + 1 and some (θ∗

I)I∈P ∗ ∈ (cid:81)

I∈P ∗ Rp+1, and

sup
I∈P ∗

sup
a∈I

(cid:107)θ0(a) − θ∗

I(cid:107)2 ≤

c4
kα0
n

,

(132)

for some constant c4 > 0. Detailed choice of kn will be given later. Combining equation 132

together with equation 25, we obtain that

(cid:107)θ∗

I(cid:107)2 ≤ 2c0,

sup
I∈P ∗

(133)

for suﬃciently large n.

Let {τ ∗

k }|P ∗|−1

k=1 with 0 < τ ∗

1 < τ ∗

2 < · · · < τ ∗

points in J(P ∗). For 1 ≤ k ≤ |P ∗| − 1, deﬁne τ ∗∗
k

k ∈ {1/m, 2/m, . . . , 1}. Let k∗
τ ∗∗
τ ∗∗
k∗
n
partition P ∗∗ ∈ B(m) and the set of vectors (θ∗∗

< 1. Apparently, k∗

n ≤ kn. Set τ ∗

0 = τ ∗∗

n be the largest integer that satisﬁes k∗

such that 0 ≤ τ ∗∗

|P ∗|−1 < 1 be the locations of the change
k < 1/m and
n ≤ |P ∗| − 1 and
n+1 = τ ∗∗
0 = 0 and τ ∗
n+1 = 1. Deﬁne a new
k∗
k∗

k − τ ∗

I )I∈P ∗∗ as follows,

1 ), [τ ∗∗

1 , τ ∗∗

2 ), · · · , [τ ∗∗
k∗
n

, τ ∗∗
n+1]},
k∗

P ∗∗ = {[τ ∗∗

0 , τ ∗∗
k+1) = θ∗

[τ ∗

θ∗∗
[τ ∗∗

k ,τ ∗∗

k+1),

k ,τ ∗

∀k ∈ {0, 1, . . . , k∗

n − 1} and θ∗∗
[τ ∗∗
k∗
n

,1] = θ∗
[τ ∗
k∗
n

,τ ∗
k∗
n+1

) (or θ∗
[τ ∗
k∗
n

,1]).

Notice that it is possible that [τ ∗∗

k , τ ∗∗

k+1) = ∅ for some k < k∗
n.

Then, it follows from equation 133 that

Moreover, it follows from equation 25, equation 134 and the condition m (cid:16) n that

sup
I∈P ∗∗

(cid:107)θ∗∗

I (cid:107)2 ≤ 2c0,

(134)

(cid:90)

(cid:88)

I∈P ∗∗

I

(cid:107)θ0(a) − θ∗∗

I (cid:107)2

2da ≤

(cid:90)

(cid:88)

I
I∈P ∗
4k−2α0
n

≤ c2

(cid:107)θ0(a) − θ∗

I(cid:107)2

2da +

|P ∗∗|
m

sup
a∈[0,1],I∈P ∗∗

(cid:107)θ0(a) − θ∗∗

I (cid:107)2
2

+ 9c2

0(kn + 1)m−1 ≤ O(1)(k−2α0

n

+ n−1kn),

(135)

94

for suﬃciently large n, where O(1) denotes some positive constant.

Notice that

n
(cid:88)

(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 =

n
(cid:88)

(cid:88)

(cid:88)

i=1

I1∈ (cid:98)P

I2∈P ∗∗

I(Ai ∈ I1 ∩ I2)(Yi − X

(cid:62)
i (cid:98)θI1)2

I(Ai ∈ I1 ∩ I2)(Yi − X

(cid:62)
i θ∗∗

I2 + X

(cid:62)
i θ∗∗

I2 − X

(cid:62)
i (cid:98)θI1)2

I∈ (cid:98)P

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

i=1

n
(cid:88)

i=1

n
(cid:88)

=

=

i=1

I2∈P ∗∗

+ 2

n
(cid:88)

i=1
(cid:124)

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

(cid:88)

I(Ai ∈ I2)(Yi − X

(cid:62)
i θ∗∗

I2)2 +

n
(cid:88)

i=1
(cid:124)

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(Ai ∈ I1 ∩ I2)(X

(cid:62)
i θ∗∗

I2 − X

(cid:62)
i (cid:98)θI1)2

(cid:123)(cid:122)
χ5

(cid:125)

I(Ai ∈ I1 ∩ I2)(Yi − X

(cid:62)
i θ∗∗

I2)X

(cid:62)
i (θ∗∗

I2 − (cid:98)θI1)

.

(cid:123)(cid:122)
χ6

(cid:125)

By deﬁnition, we have

n
(cid:88)

(cid:88)

i=1

I∈ (cid:98)P

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + nγn| (cid:98)P| ≤

n
(cid:88)

(cid:88)

i=1

I∈P ∗∗

I(Ai ∈ I)(Yi − X

(cid:62)
i θ∗∗

I )2 + n(kn + 1)γn.

It follows that

χ5 + 2χ6 + nγn| (cid:98)P| ≤ n(kn + 1)γn.

(136)

We now give a lower bound for χ5. Similar to equation 55 and equation 56, we can show

that the following event occurs with probability at least 1 − O(n−2):

λmin

(cid:32) n

(cid:88)

i=1

I(Ai ∈ I)X iX

(cid:62)
i

(cid:33)

≥ c5n|I|,

(137)

for some constant c5 > 0, and any interval I ∈ I(m) that satisﬁes |I| ≥ ¯c0n−1 log n where

the constant ¯c0 is deﬁned in Lemma 1. Under the event deﬁned in equation 137, we obtain

that

χ5 ≥

(cid:88)

(cid:88)

n
(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

i=1

I(Ai ∈ I1 ∩ I2)I(|I1 ∩ I2| ≥ ¯c0n−1 log n)(X

(cid:62)
i θ∗∗

I2 − X

(cid:62)
i (cid:98)θI1)2

≥ c5n

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(|I1 ∩ I2| ≥ ¯c0n−1 log n)|I1 ∩ I2|(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2
2.

(138)

95

In addition, under the events deﬁned in equation 20 and Lemma 4, we have

sup
I∈ (cid:98)P

(cid:107)(cid:98)θI − θ0,I(cid:107)2 ≤ sup
I∈ (cid:98)P

√

c0
log n
(cid:112)|I|n

≤

√

c0
√

log n
¯c3nγn

= o(1),

since γn (cid:29) n−1 log n. In view of equation 25, we obtain that

for suﬃciently large n. This together with equation 134 yields that

(cid:107)(cid:98)θI(cid:107)2 ≤ 2c0,

sup
I∈ (cid:98)P

(139)

(cid:88)

(cid:88)

I(|I1 ∩ I2| ≤ ¯c0n−1 log n)|I1 ∩ I2|(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2

2

I1∈ (cid:98)P
≤ (4c2

I2∈P ∗∗
0¯c0n−1 log n)

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(|I1 ∩ I2| ≤ ¯c0n−1 log n),

with probability at least 1 − O(n−2). Recall that P ∗∗ has at most kn change points. The

number of nonempty intervals I1 ∩ I2 is at most kn + 1 + | (cid:98)P|. Thus, we obtain that

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(|I1 ∩ I2| ≤ ¯c0n−1 log n)|I1 ∩ I2|(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2

2 ≤ (kn + 1 + | (cid:98)P|)(4c2

0¯c0n−1 log n),

with probability at least 1 − O(n−2). This together with equation 138 yields that

χ5 ≥ c5n

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

|I1 ∩ I2|(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2

2 − c5(kn + 1 + | (cid:98)P|)(4c2

0¯c0 log n),

with probability at least 1 − O(n−2), or equivalently,

χ5 ≥ c5n

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da − c5(kn + 1 + | (cid:98)P|)(4c2

0¯c0 log n),

(140)

with probability at least 1 − O(n−2), where

θ∗∗(a) =

(cid:88)

I∈P ∗∗

θ∗∗
I

I(a ∈ I).

96

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I2∈P ∗∗

n
(cid:88)

(cid:88)

I1∈ (cid:98)P
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I2∈P ∗∗

(cid:88)

≤

I1∈ (cid:98)P

We now provide an upper bound for |χ6|. Notice that

χ6 =

=

+

n
(cid:88)

i=1

n
(cid:88)

i=1
(cid:124)

n
(cid:88)

i=1
(cid:124)

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(Ai ∈ I1 ∩ I2)(Yi − X

(cid:62)
i θ∗∗

I2)X

(cid:62)
i (θ∗∗

I2 − (cid:98)θI1)

(141)

I(Ai ∈ I1 ∩ I2){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i (θ∗∗

I2 − (cid:98)θI1)

(cid:123)(cid:122)
χ7

(cid:125)

I(Ai ∈ I1 ∩ I2){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ∗∗

I2}X

(cid:62)
i (θ∗∗

I2 − (cid:98)θI1)

.

(cid:123)(cid:122)
χ8

(cid:125)

It suﬃces to provide upper bounds for |χ7| and |χ8|.

Under the event deﬁned in equation 21, we obtain that

(cid:88)

(cid:88)

I(Ai ∈ I1 ∩ I2)I(|I1 ∩ I2| ≥ ¯c0n−1 log n){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i (θ∗∗

I(Ai ∈ I1 ∩ I2)I(|I1 ∩ I2| ≥ ¯c0n−1 log n){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2

i=1

(cid:88)

(cid:88)

≤

(cid:112)c0|I1 ∩ I2|n log n(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2 ≤

c5n
16

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da

I2∈P ∗∗

I1∈ (cid:98)P
4c0 log n
c5

+

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

|I1 ∩ I2| ≤

c5n
16

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da + 4c0c−1
5

log n,

where the third inequality is due to Cauchy-Schwarz inequality.

In addition, using similar arguments in equation 35 and equation 36, we have with

probability at least 1−O(n−2) that, for any interval I ∈ I(m) that satisﬁes |I| ≤ ¯c0n−1 log n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

I(Ai ∈ I){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ ¯c5 log n,

(142)

for some constant ¯c5 > 0. Since the number of nonempty intervals I1 ∩ I2 is at most

97

(cid:12)
(cid:12)
(cid:12)
I2 − (cid:98)θI1)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

kn + 1 + | (cid:98)P|, we obtain that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

I2∈P ∗∗

n
(cid:88)

(cid:88)

I1∈ (cid:98)P
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I2∈P ∗∗

i=1

(cid:88)

≤

I1∈ (cid:98)P

(cid:88)

(cid:88)

I(Ai ∈ I1 ∩ I2)I(|I1 ∩ I2| ≤ ¯c0n−1 log n){Yi − X

(cid:62)
i θ0(Ai)}X

(cid:62)
i (θ∗∗

I(Ai ∈ I1 ∩ I2)I(|I1 ∩ I2| ≤ ¯c0n−1 log n){Yi − X

(cid:62)
i θ0(Ai)}X i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:12)
I2 − (cid:98)θI1)
(cid:12)
(cid:12)
(cid:12)

(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2

≤ (kn + 1 + | (cid:98)P|)(¯c5 log n) sup
I1∈ (cid:98)P

sup
I2∈P ∗∗

(cid:107)θ∗∗

I2 − (cid:98)θI1(cid:107)2 ≤ 4c0(kn + 1 + | (cid:98)P|)(¯c5 log n),

with probability at least 1 − O(n−2). It follows that

|χ7| ≤

c5n
16

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da + 4c0c−1
5

log n + 4c0(kn + 1 + | (cid:98)P|)(¯c5 log n),(143)

with probability at least 1 − O(n−2).

As for |χ8|, it follows from Cauchy-Schwarz inequality that

|χ8| ≤

1
4

n
(cid:88)

(cid:88)

(cid:88)

i=1

I1∈ (cid:98)P

I2∈P ∗∗

I(Ai ∈ I1 ∩ I2)(X

(cid:62)
i θ∗∗

I2 − X

(cid:62)
i (cid:98)θI1)2

+

n
(cid:88)

i=1
(cid:124)

(cid:88)

(cid:88)

I1∈ (cid:98)P

I2∈P ∗∗

I(Ai ∈ I1 ∩ I2){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ∗∗

I2}2

=

χ5
4

+ χ9.

(144)

(cid:123)(cid:122)
χ9

(cid:125)

Notice that

χ9 =

n
(cid:88)

(cid:88)

i=1

I∈P ∗∗

I(Ai ∈ I){X

(cid:62)
i θ0(Ai) − X

(cid:62)
i θ∗∗

I }2

It follows from equation 48, equation 50, equation 135 and Cauchy-Schwarz inequality that

E(χ9) = n

(cid:88)

EI(A ∈ I){X

(cid:62)

θ0(A) − X

(cid:62)

I }2 ≤ n
θ∗∗

(cid:88)

E(cid:107)X(cid:107)2
2

I(A ∈ I)|θ0(A) − θ∗∗

I |2
2

I∈P ∗∗
(cid:88)

≤ n

E(E(cid:107)X(cid:107)2

2|A)I(A ∈ I)|θ0(A) − θ∗∗
I |2

I∈P ∗∗
2 ≤ ω2n

(cid:88)

EI(A ∈ I)(cid:107)θ0(A) − θ∗∗

I (cid:107)2
2

I∈P ∗∗

≤ C0ω2n

(cid:90)

(cid:88)

I∈P ∗∗

I

I∈P ∗∗

(cid:107)θ0(a) − θ∗∗

I (cid:107)2

2da ≤ O(1)(nκ−2α0

n

+ κn),

where O(1) denotes some positive constant. Using similar arguments in equation 68, we

98

have for any integer q ≥ 2 that

(cid:32)

E

(cid:88)

I∈P ∗∗

I(A ∈ I){X

(cid:62)

θ0(A) − X

(cid:62)

I }2
θ∗∗

(cid:33)q

≤ q!cq (cid:88)

≤

(cid:90)

I∈P ∗∗

I

(cid:88)

I∈P ∗∗

EI(A ∈ I){X

(cid:62)

θ0(A) − X

(cid:62)

I }2q
θ∗∗

(cid:107)θ0(a) − θ∗∗

I (cid:107)2

2da ≤ q!C q(nκ−2α0

n

+ κn),

for some constants c, C > 0. Using Bernstein’s inequality, we have for any t > 0 that

Pr(χ9 ≥ Eχ9 + t) ≤ exp

−

(cid:18)

1
2

t2

tC + 2C 2(nk−2α0

n

+ kn)

(cid:19)

.

We will require the sequence {kn}n to satisfy kn (cid:29) log n. Set t0 = 4C

(cid:113)

(nk−2α0
n

+ kn) log n,

we have

t2
0

t0C + 2C 2(nk−2α0

n

+ kn)

=

(cid:112)
8
√
2

nk−2α0
n
(cid:112)

log n +

+ kn log n
nk−2α0
n

+ kn

≥ 2 log n,

for suﬃciently large n. Therefore, we obtain with probability at least 1 − O(n−2) that

χ9 ≤ O(1)(nκ−2α0

n

+ κn) + 4C

(cid:113)

(nk−2α0
n

+ kn) log n = O(nk−2α0

n

+ kn).

This together with equation 141, equation 143 and equation 144 yields that

|χ6| ≤

c5n
16

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da + c6{(kn + | (cid:98)P|) log n + nk−2α0

n

} +

χ5
4

.

with probability at least 1 − O(n−2), for some constant c6 > 0 and suﬃciently large n. In
view of equation 136 and equation 140, we obtain with probability at least 1 − O(n−2) that

χ5 ≤ nγn(kn + 1 − (cid:98)P) + 2|χ6| and hence

3c5n
8

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da

≤

2c6{(kn + | (cid:98)P|) log n + nk−2α0

n

} + nγn(kn + 1 − (cid:98)P).

(145)

Suppose | (cid:98)P| ≥ 2kn + 1. Under the event deﬁned in equation 145, it follows from the

condition nγn (cid:29) log n that

nγn(kn + 1 − | (cid:98)P|) + 2c6(kn + | (cid:98)P|) log n ≤ 3c6| (cid:98)P| log n − 2−1nγn| (cid:98)P| ≤ 0,

99

for suﬃciently large n, and hence

3c5n
8

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da ≤ 2c6nk−2α0

n

.

(146)

Otherwise, suppose | (cid:98)P| ≤ 2kn. It follows from equation 145 that

3c5n
8

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da ≤ 6c6(kn log n + nk−2α0

n

) + nγnkn,

with probability at least 1 − O(n−2). This together with equation 146 yields that

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da ≤ 16c−1

5 c6n−1(kn log n + nk−2α0

n

) + 3γnkn,

(147)

with probability at least 1 − O(n−2).

By Cauchy-Schwarz inequality, we have

(cid:90) 1

0

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da =

(cid:107)(cid:98)θ(a) − θ∗∗(a) + θ∗∗(a) − θ0(a)(cid:107)2

2da

≤ 2

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ∗∗(a)(cid:107)2

2da + 2

(cid:90) 1

0

(cid:107)θ∗∗(a) − θ0(a)(cid:107)2

2da.

In view of equation 135 and equation 147, we obtain that

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da = O(n−1kn log n + k−2α0

n

+ γnkn) = O(k−2α0

n

+ γnkn), (148)

with probability at least 1 − O(n−2), where the last equality is due to the condition that
γn (cid:29) n−1 log n. Set kn = (cid:98)γ−(1+2α0)

(cid:99) (the largest integer that is smaller than γ−(1+2α0)

), we

n

n

obtain that

(cid:90) 1

0

(cid:107)(cid:98)θ(a) − θ0(a)(cid:107)2

2da = O(γ2α0/(1+2α0)
n

),

with probability at least 1 − O(n−2). The proof is hence completed.

Convergence rate of the value function: To derive the convergence rate of the value function

under the proposed I2DR, we introduce the following lemma.

Lemma 8 Assume conditions in Theorem 6 hold. Then for any interval I ∈ I(m) with

|I| ≥ ¯c0n−1 log n and any interval I (cid:48) ∈ (cid:98)P with I ⊆ I (cid:48), we have with probability at least
1 − O(n−2) that

(cid:107)θ0,I − θ0,I(cid:48)(cid:107)2 ≤ 3

c−1
5 γn|I|−1,

(cid:113)

100

where the constant c5 is deﬁned in equation 137.

Recall by the deﬁnition of the value function that

(cid:4)

V opt − V π∗( (cid:98)d) = E

(cid:32)

sup
a∈[0,1]

(cid:62)

X

θ0(a)

(cid:33)

(cid:18)(cid:90)

− E

(cid:62)

X

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:19)

.(149)

(cid:98)d(X)

We begin by providing an upper bound for

χ11 = E

(cid:32)

(cid:62)

X

sup
a∈[0,1]

(cid:33)

(cid:32)

θ0(a)

− E

sup
I∈ (cid:98)P

(cid:33)

.

(cid:62)

X

θ0,I

It follows from equation 48 and Cauchy-Schwarz inequality that

(cid:32)

χ11 = E

X

sup
a∈I

sup
I∈ (cid:98)P
(cid:107)θ0(a) − θ0,I(cid:107)2

(cid:33)

(cid:32)

(cid:62)

θ0(a)

− E

(cid:33)

(cid:62)

X

θ0,I

sup
I∈ (cid:98)P

(150)

p+1
(cid:88)

j=1

(j)

|X

|2 sup
I∈ (cid:98)P

sup
a∈I

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ (p + 1)1/2ω sup
I∈ (cid:98)P

sup
a∈I

(cid:107)θ0(a) − θ0,I(cid:107)2.

≤ E(cid:107)X(cid:107)2 sup
I∈ (cid:98)P

sup
a∈I

(cid:118)
(cid:117)
(cid:117)
(cid:116)E

≤

Consider a sequence {dn}n that satisﬁes dn ≥ 0, ∀n, dn → 0 as n → ∞ and dn (cid:29) n−1 log n.

By the deﬁnition of Hölder continuous functions, we have for any I with |I| ≤ dn that

sup
a1,a2∈I

(cid:107)θ0(a1) − θ0(a2)(cid:107)2 ≤ L sup
a1,a2∈I

|a1 − a2|α0 ≤ Ldα0
n .

It follows that

sup
a∈I

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ sup
a∈I
(cid:13)
(cid:13)
(cid:13){EXX
(cid:62)I(A ∈ I)}−1EXX

≤ sup
a∈I

(cid:13)
(cid:13)
(cid:13){EXX

≤ sup
a∈I

(cid:13)
(cid:13)
(cid:13)θ0(a) − {EXX

(cid:62)I(A ∈ I)}−1EXX

(cid:62)

θ0(A)I(A ∈ I)

(cid:62)I(A ∈ I)}−1EXX

(cid:62)I(A ∈ I){θ0(a) − θ0(A)}

(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)2

(151)

(cid:13)
(cid:62)I(A ∈ I)
(cid:13)
(cid:13)2

sup
a,a∗∈I

(cid:107)θ0(a) − θ0(a∗)(cid:107)2 ≤ Ldα0
n ,

for any I that satisﬁes |I| ≤ dn.

Consider an interval I ∈ I(m) that satisﬁes |I| > dn. For any a ∈ I, we can ﬁnd an
interval I (cid:48) ⊆ I with dn/2 ≤ |I (cid:48)| ≤ dn and I (cid:48) ∈ I(m) that covers a. Similar to equation 151,

we have

(cid:107)θ0,I(cid:48) − θ0(a)(cid:107)2 ≤ Ldα0
n .

(152)

101

Since dn (cid:29) n−1 log n, by Lemma 8, we have with probability at least 1 − O(n−2) that

(cid:107)θ0,I − θ0,I(cid:48)(cid:107)2 ≤ 3

2c−1

5 γnd−1
n .

(cid:113)

This together with equation 152 yields that

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ Ldα0

n + 3

sup
a∈I

(cid:113)

2c−1

5 γnd−1
n ,

for any I ∈ I(m) that satisﬁes |I| > dn. Combining this together with equation 151, we

obtain that

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ Ldα0

n + 3

sup
a∈I

(cid:113)

2c−1

5 γnd−1
n ,

for any I ∈ I(m), with probability at least 1 − O(n−2). Set dn (cid:16) γ(1+2α0)−1
probability at least 1 − O(n−2) that

n

, we have with

(cid:107)θ0(a) − θ0,I(cid:107)2 ≤ O(1)γα0/(1+2α0)

n

,

sup
a∈I

for any I ∈ I(m), where O(1) denotes some positive constant.

Therefore, we obtain with probability at least 1 − O(n−2) that

χ11 ≤ O(1)γα0/(1+2α0)

n

,

(153)

where O(1) denotes some positive constant. Similarly, we can show with probability at least

1 − O(n−2) that

χ12 = EX

(cid:62)

θ0, (cid:98)d(X) − E

(cid:18)(cid:90)

(cid:98)d(X)

(cid:62)

X

θ0(a)π∗(a; X, (cid:98)d(X))da

(cid:19)

≤ O(1)γα0/(1+2α0)
n

,

where O(1) denotes some positive constant. This together with equation 149 and equation 153

yields that,

V opt − V π∗( (cid:98)d) ≤ E

(cid:33)

(cid:32)

(cid:62)

X

θ0,I

sup
I∈ (cid:98)P

(cid:62)

− EX

θ0, (cid:98)d(X) + O(1)γα0/(1+2α0)

n

,

(154)

with probability at least 1 − O(n−2), where O(1) denotes some positive constant.

Using similar arguments in equation 150, we can show that

(cid:32)

E

sup
I∈ (cid:98)P

(cid:62)

X

θ0,I

(cid:33)

(cid:32)

− E

sup
I∈ (cid:98)P

(cid:33)

(cid:62)

X

(cid:98)θI

≤ (p + 1)1/2ω sup
I∈ (cid:98)P

(cid:107)θ0,I − (cid:98)θI(cid:107)2,

102

and

(cid:62)

EX

θ0, (cid:98)d(X) − EX

(cid:62)

(cid:98)d(X) ≤ (p + 1)1/2ω sup
(cid:98)θ
I∈ (cid:98)P

(cid:107)θ0,I − (cid:98)θI(cid:107)2.

Since supI ∈ X

(cid:62)

(cid:98)θI = X

(cid:62)

θ0, (cid:98)d(X), we have

V opt − V π∗( (cid:98)d) ≤ 2(p + 1)1/2ω sup
I∈ (cid:98)P

(cid:107)θ0,I − (cid:98)θI(cid:107)2 + O(1)γα0/(1+2α0)

n

,

(155)

under the event deﬁned in equation 154. It follows from Lemma 1 and 4 that

(cid:107)θ0,I − (cid:98)θI(cid:107)2 ≤

sup
I∈ (cid:98)P

√
√

log n
nγn

,

with probability at least 1 − O(n−2). This together with equation 155 yields that

V opt − V π∗( (cid:98)d) ≤ O(1)

(cid:18)

γα0/(1+2α0)
n

√
√

log n
nγn

(cid:19)

,

+

with probability at least 1 − O(n−2), where O(1) denotes some positive constant. Set γn (cid:16)
(n−1/2 log1/2 n)(2α0+1)/(4α0+1), we obtain that V opt − V π∗( (cid:98)d) = O(n−α0/(1+4α0) logα0/(1+4α0) n),
with probability at least 1 − O(n−2). The proof is hence completed.

B.13 Proof of Lemma 8

For a given interval I (cid:48) ∈ (cid:98)P, the set of intervals I considered in Lemma 8 can be classiﬁed
into the following three categories.

Category 1: I = I (cid:48). Then it is immediate to see that (cid:107)θ0,I − θ0,I(cid:48)(cid:107)2 = 0 and the assertion

automatically holds.

Category 2: There exists another interval I ∗ ∈ I(m) that satisﬁes I (cid:48) = I ∗ ∪ I. Notice that

the partition (cid:98)P ∗ = (cid:98)P ∪ {I ∗} ∪ I − {I (cid:48)} also belongs to B(m). By deﬁnition, we have

1
n

1
n

≥

n
(cid:88)

(cid:88)

I(Ai ∈ I0)(Yi − X

(cid:62)
i (cid:98)θI0)2 + λn|I0|(cid:107)(cid:98)θI0(cid:107)2

2 + γn| (cid:98)P ∗|

i=1

n
(cid:88)

I0∈ (cid:98)P ∗

(cid:88)

I(Ai ∈ I0)(Yi − X

(cid:62)
i (cid:98)θI0)2 + λn|I0|(cid:107)(cid:98)θI0(cid:107)2

2 + γn| (cid:98)P|,

i=1

I0∈ (cid:98)P

103

and hence
n
(cid:88)

1
n

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 +

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗)(Yi − X

(cid:62)
i (cid:98)θI∗)2 + λn|I ∗|(cid:107)(cid:98)θI∗(cid:107)2
2

≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I (cid:48))(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I (cid:48)|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 − γn.

It follows from the deﬁnition of (cid:98)θI∗ that

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗)(Yi − X

(cid:62)
i (cid:98)θI∗)2 + λn|I ∗|(cid:107)(cid:98)θI∗(cid:107)2

2 ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I ∗|(cid:107)(cid:98)θI(cid:48)(cid:107)2
2.

Therefore, we obtain

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

≥

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2
2

(156)

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 − γn.

Category 3: There exist two intervals I ∗, I ∗∗ ∈ I(m) that satisfy I (cid:48) = I ∗ ∪ I ∪ I ∗∗. Using

similar arguments in proving equation 156, we can show that

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 ≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 − 2γn.

Hence, regardless of whether I belongs to Category 2, or it belongs to Category 3, we have

1
n

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

≥

≥

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2
2

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 + λn|I|(cid:107)(cid:98)θI(cid:48)(cid:107)2

2 − 2γn

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 − 2γn.

(157)

Notice that |I (cid:48)| ≥ |I|. Under the event deﬁned in equation 20, we obtain that

(cid:107)(cid:98)θI(cid:48) − θ0,I(cid:48)(cid:107)2 ≤

√

c0
log n
(cid:112)|I|n

.

Similar to equation 31, we can show the following event occurs with probability at least

1 − O(n−2),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i ((cid:98)θI(cid:48) − θ0,I(cid:48))
(cid:12)
(cid:12)

≤ O(1)n−1 log n,

(158)

104

where O(1) denotes some positive constant. Similarly, using Cauchy-Schwarz inequality, we

can show with probability at least 1 − O(n−2) that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

1
4n

1
4n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

≤

≤

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i ((cid:98)θI(cid:48) − θ0,I(cid:48))
(cid:12)
(cid:12)

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2 +

1
n

n
(cid:88)

i=1

I(Ai ∈ I){X

(cid:62)
i ((cid:98)θI(cid:48) − θ0,I(cid:48))}2

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2 + O(1)n−1 log n,

where O(1) denotes some positive constant. This together with equation 158 yields

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

1
4n

n
(cid:88)

i=1
n
(cid:88)

i=1

≤

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X

(cid:12)
(cid:12)
(cid:62)
(cid:12)
i ((cid:98)θI(cid:48) − θ0,I(cid:48))
(cid:12)
(cid:12)

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2 + O(1)n−1 log n,

with probability at least 1 − O(n−2), where O(1) denote some positive constant. Using

similar arguments in proving equation 30, we can show the following event occurs with

probability at least 1 − O(n−2),

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI(cid:48))2 ≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I(cid:48))2

(159)

−

1
2n

n
(cid:88)

i=1

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2 − O(1)n−1 log n,

where O(1) denotes some positive constant.

In addition, it follows from the deﬁnition of (cid:98)θI that

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + λn|I|(cid:107)θ0,I(cid:107)2
2.

By equation 25 and the condition that λn = O(n−1 log n), we obtain that

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i (cid:98)θI)2 + λn|I|(cid:107)(cid:98)θI(cid:107)2

2 ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2 + O(1)n−1 log n,

105

where O(1) denotes some positive constant. This together with equation 157 and equation 159

yields

≥

n
(cid:88)

i=1
n
(cid:88)

i=1
1
2

−

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)2

(160)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I(cid:48))2 − 2nγn − O(1) log n

n
(cid:88)

i=1

I(Ai ∈ I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2,

with probability at least 1 − O(n−2), where O(1) denotes some positive constant.

Notice that

n
(cid:88)

i=1
n
(cid:88)

i=1

n
(cid:88)

i=1

=

+

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I(cid:48))2 =

n
(cid:88)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I + X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))2

I(Ai ∈ I)(Yi − X

i=1
n
(cid:88)

(cid:62)
i θ0,I)2 + 2

i=1
(cid:124)

I(Ai ∈ I)(X

(cid:62)
i θ0,I(cid:48) − X

(cid:62)
i θ0,I)2.

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)(X

(cid:62)
i θ0,I − X

(cid:62)
i θ0,I(cid:48))

(cid:123)(cid:122)
χ10

(cid:125)

Combining this with equation 160 yields that

n
(cid:88)

i=1

I(Ai ∈ I)(X

(cid:62)
i θ0,I(cid:48) − X

(cid:62)
i θ0,I)2 ≤ 4nγn + O(1) log n + 4|χ10|.

(161)

Under the event deﬁned in equation 137, we obtain that

n
(cid:88)

i=1

I(Ai ∈ I)(X

(cid:62)
i θ0,I(cid:48) − X

(cid:62)
i θ0,I)2 ≥ c5n|I|(cid:107)θ0,I(cid:48) − θ0,I(cid:107)2
2.

(162)

By the deﬁnition of θ0,I, we have EI(A ∈ I)(Y − X

(cid:62)

θ0,I)X = 0. Under the event deﬁned

in equation 21, it follows from Cauchy-Schwarz inequality that

|χ10| ≤

2
c5n|I|

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I(Ai ∈ I)(Yi − X

(cid:62)
i θ0,I)X i

i=1

≤

2c2

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
0 log n
c5

+

c5n
8

|I|(cid:107)θ0,I(cid:48) − θ0,I(cid:107)2
2

+

c5n
8

|I|(cid:107)θ0,I(cid:48) − θ0,I(cid:107)2
2.

106

This together with equation 161 and equation 162 yields that

|I|(cid:107)θ0,I − θ0,I(cid:48)(cid:107)2

2 ≤

8γn
c5

+ O(1)n−1 log n,

with probability at least 1 − O(n−2), where O(1) denotes some positive constant. Since

γn (cid:29) n−1 log n, for suﬃciently large n, we obtain with probability at least 1 − O(n−2) that

|I|(cid:107)θ0,I − θ0,I(cid:48)(cid:107)2

2 ≤ 9c−1

5 γn.

The proof is hence completed.

B.14 Proof of Theorem 7

The proof of Theorem 7 is divided into three parts. In Part 1, we prove that for any interval

I ∈ I(m) with |I| (cid:29) γn and any interval I (cid:48) ∈ (cid:98)P with I ⊆ I (cid:48), we have with probability
approaching 1 (w.p.a.1.) that

E|qI,0(X) − qI(cid:48),0(X)|2 ≤ ¯C|I|−1γn,

(163)

for some constant ¯C > 0.

In the second part, we show assertion (i) in Theorem 7 holds. Finally, we present the

proof for assertion (ii) in Theorem 7. It is worth mentioning that results in Lemma 5 and

Lemma 7 do not rely on the assumption that Q(·) is piecewise function. These lemmas hold

under the conditions in Theorem 7 as well.

Proof of Part 1: For a given interval I (cid:48) ∈ (cid:98)P, the set of intervals I considered in equation 163
can be classiﬁed into the following three categories.

Category 1: I = I (cid:48). It is immediate to see that qI,0 = qI(cid:48),0 and the assertion automatically

holds.

Category 2: There exists another interval I ∗ ∈ I(m) that satisﬁes I (cid:48) = I ∗ ∪ I. Notice that

the partition (cid:98)P ∗ = (cid:98)P ∪ {I ∗} ∪ I − {I (cid:48)} forms another partition. By deﬁnition, we have

1
n

n
(cid:88)

(cid:88)

i=1

I0∈ (cid:98)P ∗

I(Ai ∈ I0){Yi − (cid:98)qI0(Xi)}2 + γn| (cid:98)P ∗| ≥

1
n

n
(cid:88)

(cid:88)

i=1

I0∈ (cid:98)P

I(Ai ∈ I0){Yi − (cid:98)qI0(Xi)}2 + γn| (cid:98)P|,

107

and hence

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 +

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗){Yi − (cid:98)qI∗(Xi)}2

≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I (cid:48)){Yi − (cid:98)qI(cid:48)(Xi)}2 − γn.

It follows from the deﬁnition of (cid:98)qI∗ that

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗){Yi − (cid:98)qI∗(Xi)}2 ≤

1
n

n
(cid:88)

i=1

I(Ai ∈ I ∗){Yi − (cid:98)qI(cid:48)(Xi)}2.

Therefore, we obtain

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(cid:48)(Xi)}2 − γn.

(164)

Category 3: There exist two intervals I ∗, I ∗∗ ∈ I(m) that satisfy I (cid:48) = I ∗ ∪ I ∪ I ∗∗. Using

similar arguments in proving equation 164, we can show that

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(cid:48)(Xi)}2 − 2γn.

Hence, regardless of whether I belongs to Category 2, or it belongs to Category 3, we have

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 ≥

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(cid:48)(Xi)}2 − 2γn.

(165)

Notice that for any interval I0,

1
n

=

n
(cid:88)

I(Ai ∈ I0){Yi − (cid:98)qI0(Xi)}2 − E[I(A ∈ I0){Y − (cid:98)qI0(X)}2]

n
(cid:88)

i=1
1
n

i=1

I(Ai ∈ I0){(cid:98)qI0(Xi) − qI0,0(Xi)}{qI,0(Xi) − (cid:98)qI0,0(Xi)}2

+

1
n

n
(cid:88)

i=1

I(Ai ∈ I0){Yi − (cid:98)qI0(Xi)}2 − E[I(A ∈ I0){(cid:98)qI0(Xi) − (cid:98)qI0(X)}2].

Using similar arguments in bounding the stochastic error term in Part 2 of the proof of

Lemma 5, we can show w.p.a.1. that the RHS is of the order O(n−2β/(2β+p) log8 n), for any

108

I0 ∈ I(m). As such, we obtain w.p.a.1. that

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(Xi)}2 = E[I(A ∈ I){Y − (cid:98)qI(X)}2]

+O(1)|I|(n|I|)−2β/(2β+p) log8 n,

1
n

n
(cid:88)

i=1

I(Ai ∈ I){Yi − (cid:98)qI(cid:48)(Xi)}2 = E[I(A ∈ I){Y − (cid:98)qI(cid:48)(X)}2]

+O(1)|I|(n|I|)−2β/(2β+p) log8 n,

where O(1) denotes some universal positive constant. Combining these together with

equation 165 yields

E[I(A ∈ I){Y − (cid:98)qI(X)}2] ≥ E[I(A ∈ I){Y − (cid:98)qI(cid:48)(X)}2]
−2γn + O(1)|I|(n|I|)−2β/(2β+p) log8 n,

for any I and I (cid:48), w.p.a.1. Note that qI,0 satisﬁes E[I(A ∈ I){Y − qI,0(X)}|X] = 0. We

have

E[I(A ∈ I){qI,0(X) − (cid:98)qI(X)}2] ≥ E[I(A ∈ I){qI,0(X) − (cid:98)qI(cid:48)(X)}2]
−2γn + O(1)|I|(n|I|)−2β/(2β+p) log8 n.

Consider the ﬁrst term on the RHS. Note that

E[I(A ∈ I){qI,0(X) − (cid:98)qI(cid:48)(X)}2] = E[I(A ∈ I){qI,0(X) − qI(cid:48),0(X)}2]
+E[I(A ∈ I){(cid:98)qI(cid:48)(X) − qI(cid:48),0(X)}2]
−2E[I(A ∈ I){qI,0(X) − qI(cid:48),0(X)}{(cid:98)qI(cid:48)(X) − qI(cid:48),0(X)}].

By Cauchy-Schwarz inequality, the last term on the RHS can be lower bounded by

−

1
2

E[I(A ∈ I){qI,0(X) − qI(cid:48),0(X)}2] − 2E[I(A ∈ I){(cid:98)qI(cid:48)(X) − qI(cid:48),0(X)}2].

It follows that

E[I(A ∈ I){qI,0(X) − (cid:98)qI(cid:48)(X)}2] ≥

1
E[I(A ∈ I){qI,0(X) − qI(cid:48),0(X)}2]
2
−3E[I(A ∈ I){(cid:98)qI(cid:48)(X) − qI(cid:48),0(X)}2],

109

and hence

1
2

E[I(A ∈ I){qI,0(X) − qI(cid:48),0(X)}2] − 2γn + O(1)|I|(n|I|)−2β/(2β+p) log8 n
≤ E[I(A ∈ I){qI,0(X) − (cid:98)qI(X)}2] + 3E[I(A ∈ I){qI(cid:48),0(X) − (cid:98)qI(cid:48)(X)}2].

By Lemma 5, Lemma 7 and the positivity assumption, the RHS is upper bounded by

O(1)|I|(n|I|)−2β/(p+2β) log8 n for some universal positive constant O(1), w.p.a.1. We obtain

w.p.a.1. that

E[I(A ∈ I){qI(X) − qI(cid:48)(X)}2] = 4γn + O(1)|I|(n|I|)−2β/(2β+p) log8 n,

uniformly for any I and I (cid:48), or equivalently,

(cid:20)e(I|X)
|I|

E

{qI(X) − qI(cid:48)(X)}2

(cid:21)

=

4γn
|I|

+ O(1)(n|I|)−2β/(2β+p) log8 n.

By the positivity assumption, we have w.p.a.1. that

E[{qI(X) − qI(cid:48)(X)}2] = O(γn|I|−1) + O((n|I|)−2β/(2β+p) log8 n),

uniformly for any I and I (cid:48). The proof of equation 163 is hence completed by noting that

|γn| (cid:28) |I|.

Proof of Part 2: Consider a sequence {dn}n such that dn → 0 and dn (cid:29) γn. We aim to

show

inf
a∈I(cid:48)
I(cid:48)∈ (cid:98)P
By Lemma 5, it suﬃces to show

E[|Q(X, a) − (cid:98)qI(cid:48)(X)|2] = op(1).

E|Q(X, a) − qI(cid:48),0(X)|2 = o(1).

inf
a∈I(cid:48)
I(cid:48)∈ (cid:98)P

Suppose |I (cid:48)| ≥ dn. Then according to equation 163, we can ﬁnd some I such that

|I| (cid:29) γn and a ∈ I ⊆ I (cid:48),

E|qI,0(X) − qI(cid:48),0(X)|2 ≤ ¯C|I|−1γn.

Since |I| → 0 and a ∈ I, it follows from the uniform continuity of the outcome regression

function that |qI,0(X) − Q(X, a)| → 0. The assertion thus follows.

110

Next, suppose |I (cid:48)| < dn. Then |I (cid:48)| → 0 as well. It follows from the uniform continuity

of the outcome regression function that

inf
a∈I(cid:48)

|qI(cid:48),0(X) − Q(X, a)| → 0.

The assertion thus follows. This completes the proof for the result (i).

Proof of Part 3: This part follows the second part of the proof of Theorem 6. Recall by the

deﬁnition of the value function that

V opt − V π∗( (cid:98)d) = E

(cid:32)

sup
a∈[0,1]

(cid:33)

Q(X, a)

− E

(cid:18)(cid:90)

(cid:98)d(X)

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:19)

.(166)

Using similar arguments in equation 150, under the result (i), we can show that

(cid:32)

(cid:33)

(cid:32)

(cid:33)

χ13 = E

sup
a∈[0,1]

Q(X, a)

− E

sup
I∈ (cid:98)P

qI,0(X)

= op(1).

(167)

Similarly, we have

χ14 = EQ(X, (cid:98)d(X)) − E

(cid:18)(cid:90)

(cid:98)d(X)

Q(X, a)π∗(a; X, (cid:98)d(X))da

(cid:19)

= op(1).

This together with equation 166 and equation 167 yields that,

V opt − V π∗( (cid:98)d) ≤ E

(cid:32)

sup
I∈ (cid:98)P

(cid:33)

qI,0(X)

− EQ(X, (cid:98)d(X)) + op(1).

(168)

Using similar arguments in equation 150, we can obtain that

(cid:32)

E

sup
I∈ (cid:98)P

(cid:33)

(cid:32)

(cid:33)

qI,0(X)

− E

(cid:98)qI(X)

sup
I∈ (cid:98)P

≤ ¯C5 sup
I∈ (cid:98)P

E[|qI,0(X) − (cid:98)qI(X)|2],

for some constant ¯C5, and

EQ(X, (cid:98)d(X)) − E (cid:98)Q(X, (cid:98)d(X)) ≤ ¯C6 sup
I∈ (cid:98)P

E[|qI,0(X) − (cid:98)qI(X)|2],

for some constant ¯C6.

Since supI∈ (cid:98)P (cid:98)qI(X) = (cid:98)Q(X, (cid:98)d(X)), by Lemma 5 together with equation 168, we have

V opt − V π∗( (cid:98)d) = op(1).

(169)

The proof is hence completed.

111

