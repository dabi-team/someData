Stability Veriﬁcation of Neural Network Controllers using
Mixed-Integer Programming

Roland Schwan1,2, Colin N. Jones1, and Daniel Kuhn2

2
2
0
2

n
u
J

7
2

]

Y
S
.
s
s
e
e
[

1
v
4
7
3
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract— We propose a framework for the stability veri-
ﬁcation of Mixed-Integer Linear Programming (MILP) rep-
resentable control policies. This framework compares a ﬁxed
candidate policy, which admits an efﬁcient parameterization
and can be evaluated at a low computational cost, against a
ﬁxed baseline policy, which is known to be stable but expensive
to evaluate. We provide sufﬁcient conditions for the closed-
loop stability of the candidate policy in terms of the worst-
case approximation error with respect to the baseline policy,
and we show that these conditions can be checked by solving
a Mixed-Integer Quadratic Program (MIQP). Additionally, we
demonstrate that an outer approximation of the stability region
of the candidate policy can be computed by solving an MILP.
The proposed framework is sufﬁciently general to accommodate
a broad range of candidate policies including ReLU Neural
Networks (NNs), optimal solution maps of parametric quadratic
programs, and Model Predictive Control (MPC) policies. We
also present an open-source toolbox in Python based on the
proposed framework, which allows for the easy veriﬁcation of
custom NN architectures and MPC formulations. We showcase
the ﬂexibility and reliability of our framework in the context
of a DC-DC power convertor case study and investigate the
computational complexity.

I. INTRODUCTION

MPC has been extremely successful in control applications
for reﬁneries and chemical plants [1], building control [2],
the control of quadcopters [3], robotics [4], and power
electronics [5], [6]. The main advantages of MPC are its
versatility, stability, and ability to account for input and state
constraints. With the transition from the process industry
to robotics and power electronics, the sampling times have
decreased from hours to only a few milli- or even microsec-
onds. This is especially challenging if one wants to deploy
controllers on embedded systems with low computational
resources and limited memory.

Hence,

ideally, one would like to perform all heavy
computations ofﬂine and precompute the optimal control law
ψ(cid:63)(·) that maps any feasible state to an optimal control
input. A well-known technique to compute ψ(cid:63)(·) such an
optimal control law is explicit MPC [7]. For MPC controllers
with quadratic cost functions and linear dynamics, ψ(cid:63)(·)
is a piecewise afﬁne function deﬁned over a polyhedral

This work was supported by the Swiss National Science Foundation under

the NCCR Automation project, grant agreement 51NF40 180545.

We thank Silvia Mastellone for valuable feedback on an earlier version
of this paper, and Emilio Maddalena for providing the experimental setup
of the DC-DC power convertor.

1Roland Schwan and Colin N. Jones are with the Automatic Control Lab,

EPFL, Switzerland.

2Roland Schwan and Daniel Kuhn are with the Risk Analytics and

Optimization Chair, EPFL, Switzerland.

roland.schwan, colin.jones, daniel.kuhn

{

@epfl.ch

}

partition of the state space. Thus, in contrast to implicit MPC,
which computes the optimal control input online by solving
a different optimization problem for each state, explicit MPC
precomputes the optimal afﬁne control policy across prede-
ﬁned polytopic regions of the state space. Unfortunately, the
required number of polytopes explodes with the dimension
of the state and the number of constraints, which render
explicit MPC intractable for larger systems. Additionally, the
online search for the polytope containing the current state
may require excessive processing power or storage space.
Although these computational challenges can be mitigated by
using search trees [8] or hash tables [9], the required memory
may remain too large [10], especially for embedded systems.
Recent attempts to approximate explicit MPC directly either
suffer from a curse of dimensionality [11], or they rely on
expensive set projections [12], [13].

The limited scalability of explicit MPC and its variants has
promoted interest in general function approximators of MPC
policies, such as deep NNs [14]. Deep NNs are attractive
because they can exactly represent predictive controllers for
linear systems [15], while being relatively inexpensive during
online inference [16]. Unfortunately, one loses the stability
guarantees for the learned controllers. Statistical methods are
one way to verify the stability of the learned controllers
[17], with extensions to ﬁlter out severely suboptimal control
inputs with high probability [18].

However, NNs are not the only viable function approxima-
tors. In fact, every continuous nonlinear control law can be
represented as the minimizer mapping of a parametric convex
program [19]. Even the solutions of parametric linear pro-
grams (LPs) can represent any continuous piecewise afﬁne
function [20]. Hence, parametric optimization problems give
rise to implicit function approximators, and by leveraging
the implicit function theorem, one can directly optimize and
learn the underlying problem parameter settings. Approaches
for calibrating parametric quadratic programs (QPs), general
convex programs, and root ﬁnding problems are described in
[21], [22], and [23] respectively.

NN control policies have been successfully employed in
applications for controlling chemical plants [24], robotic
arms [25] or DC-DC power electronic converters [6] at a
signiﬁcant increase in computational speed. Thus, NN based
controllers can not only result in better control performance,
because of a tighter control loop, but also in drastic savings
of computational resources.

 
 
 
 
 
 
A. Related Works

The idea to approximate predictive controllers with NNs
enjoys growing popularity. Some of the earliest work dates
back to the 90s, when single hidden layer NN approximations
were used to learn a nonlinear MPC policy [26]. More
recently, plain vanilla NN architectures have been enhanced
with parametric QP implicit layers [27]. However, closed-
loop stability can also be guaranteed by projecting the output
of the NN into a safety set [13]. This projection can be
viewed as a “Safety Filter” [28]. However, a computationally
expensive optimization problem has to be solved online.

The satisfaction of the closed-loop state/input constraints
and stability requirements can also be veriﬁed by performing
a reachability analysis using an MILP representation of the
NN controller [29] or by over approximating the NN output
bounds via semideﬁnite programming formulations [30].

Other approaches show closed-loop stability by learning
and verifying Lyapunov functions [31], [32]. This leads to a
learner/veriﬁer pattern to ﬁnd a suitable Lyapunov function.
But this can be demanding, since at every step adversarial
points have to be found (veriﬁer), and a new Lyapunov
function candidate must then be found, without guarantees
of convergence.

Alternatively, one can guarantee stability by combining
the worst-case approximation error of the NN controller with
its Lipschitz constant, utilizing an MILP framework [33]. In
this paper, we propose direct sufﬁcient conditions for closed-
loop stability, which can be veriﬁed by solving an MIQP.
Compared to the work in [33] we show imperially that our
approach is less conservative and that solving an MIQP does
necessarily result in longer solve times.

B. Contributions

We introduce a general framework for the veriﬁcation
of NN controllers via mixed-integer programming (MIP)
together with an open source toolbox.1 Speciﬁcally, we
introduce the concept of MILP representable veriﬁcation
problems which allows us to model a variety of common
components like ReLU NNs, and parametric quadratic pro-
grams which include MPC policies. Given a baseline policy
(e.g., an MPC) and an approximate policy (e.g., a NN), we
propose two main approaches to verify closed-loop stability
of the approximate policy

• by computing the worst-case approximation error. Uti-
lizing robust MPC variants like Tube MPC [34], we
can guarantee closed-loop stability by bounding the
approximation error inside the disturbance set of the
robust MPC scheme.

• by providing sufﬁcient conditions on closed-loop sta-
bility, which can be directly formulated and checked
by solving an MIQP. We show that our conditions
are less conservative in practice, compared to previous
methods, i.e., are able to verify the closed-loop stability
of more approximate policies. Additionally, we show

1The toolbox can be accessed under the following link:

https://github/PREDICT-EPFL/evanqp

that an outer approximation of the stability region of
the approximate control scheme can be calculated by
solving MILPs.

We then exemplify the application of the proposed method
on a case-study of a DC-DC power convertor using different
NN architectures and MPC formulations. We compare our
methods against other approaches and show the superiority
of our formulation. Additionally, we provide numerical ex-
periments to show the performance of our approach.

The toolbox is written in Python and allows the automatic
transcription of the discussed veriﬁcation problems as MIPs.
It can directly import NN architectures from Pytorch [35],
and parametric QPs from CVXPY [36]. This not only
allows for a simpliﬁed problem formulation, but also a tight
integration with the existing machine learning ecosystem.

+ and Sn

NOTATION
We denote the set of real numbers by R, the set of n-
dimensional real-valued vectors by Rn and the set of n × m-
dimensional real-valued matrices by Rn×m. Furthermore, we
denote the subspace of symmetric matrices in Rn×n by Sn
and the cone of positive semi-deﬁnite and deﬁnite matrices
by Sn
++, respectively. We use In to denote the n-
dimensional identity matrix, 1n to denote the n-dimensional
column vector of ones, and diag(·) to represent the map-
ping that transforms a column vector to the corresponding
diagonal matrix. Given two sets A and B, we denote their
Minkowski sum as A ⊕ B := {a + b | a ∈ A, b ∈ B} and
their Pontryagin difference as A (cid:9) B := {a | a ⊕ B ⊆ A}.
The interior of a set S is denoted by int(S).

II. MILP REPRESENTABLE VERIFICATION PROBLEMS

Input Domain X

x0

x0

MPC

NN

ψ1(x0)

ψ2(x0)

Veriﬁcation Cost
f (ψ1(x0), ψ2(x0))

Fig. 1: Schematic of the veriﬁcation approach.

A veriﬁcation problem consists of a baseline control policy
ψ1(x) (e.g., an MPC policy) and an approximate policy
ψ2(x) (e.g., a NN) on a common input domain X. The
outputs of the policies then enter a veriﬁcation cost function
f (ψ1(x), ψ2(x)) that represents the objective of the veriﬁ-
cation problem. An overview of the proposed architecture is
shown in Figure 1.

Deﬁnition 1: A function ψ : X → U with domain X ⊆ Rn
and range U ⊆ Rm is MILP representable if there exists a

polyhedral set P ⊆ Rn × Rm × Rc × Rb such that (x, u) ∈
gr(ψ) if and only if there exists z ∈ Rc and β ∈ {0, 1}b
such that (x, u, z, β) ∈ P .

representable as ψ = ψ(cid:96) ◦ ψ(cid:96)−1 ◦ · · · ◦ ψ1, where ψi : Xi ⊆
Rni−1 → Ui ⊆ Rni corresponding to the i-th layer is deﬁned
through the componentwise maximum

A wide range of functions are MILP representable. In
fact, one can show that the MILP representable functions
are dense in the family of continuous functions on a compact
domain X with respect to the supremum norm. Examples of
MILP representable functions include the optimal solution
map of parametric QPs (e.g., MPC policies), ReLU NNs,
and piecewise afﬁne functions. Additionally, compositions of
MILP representable functions are also MILP representable.
Lemma 1: If two functions ψi : Xi → Ui, i = 1, 2 with
X2 ⊆ U1 are both MILP representable, then the composition
ψ1 ◦ ψ2 : X1 → U2 is MILP representable.

Proof: Let P1 ⊆ Rn × Rm × Rc1 × Rb1 and P2 ⊆
Rm ×Ro ×Rc2 ×Rb2 be the polyhedral sets corresponding to
the MILP representations of ψ1 and ψ2 respectively. Then,
(xi, ui) ∈ gr(ψi) if and only if there exist zi ∈ Rci and
βi ∈ {0, 1}bi such that (xi, ui, zi, βi) ∈ Pi for i = 1, 2.
To compute the composition of ψ1 and ψ2, we now set the
output u1 of ψ1 equal to the input x2 of ψ2. We can deﬁne
new variables z = (z1, z2, u1) and β = (β1, β2). Hence,
by construction, there exists a polyhedral set P ⊆ Rn ×
Ro × Rm+c1+c2 × Rb1+b2 such that (x1, u2) ∈ gr(ψ1 ◦ ψ2) if
and only if (x1, u2, z, β) ∈ P , showing that the composition
ψ1 ◦ ψ2 is MILP representable.

In order to formalize the veriﬁcation problem for compar-
ing the approximate control policy ψ2(·) against the baseline
policy ψ1(·), deﬁne X as the relevant input domain of the
control polices. For any property to be veriﬁed, we can pose
the veriﬁcation problem as the task of certifying the non-
negativity of a function f (·) over the input domain X:

0 ≤ min
x0∈X

f (ψ1(x0), ψ2(x0)).

(1)

Proposition 1: If X is a polyhedron and if ψ1, ψ2, and
f are MILP representable, then the veriﬁcation problem on
the right-hand side of (1) can be posed as the following
optimization problem, which is equivalent to an MILP:

min
τ,x0,u1,u2
s.t.

τ

x0 ∈ X,
(x0, u1) ∈ gr(ψ1)
(x0, u2) ∈ gr(ψ2)
(u1, u2, τ ) ∈ gr(f ).

(2)

Proof: The equivalence of the optimization problem in
(1) and (2) is immediate. In addition, one readily veriﬁes
that problem (2) is equivalent to an MILP because X is a
polyhedron and f , ψ1 and ψ2 are MILP representable.

In the next sections, we consider speciﬁc MILP repre-

sentable functions relevant for control.

A. ReLU Neural Networks

We now demonstrate that NNs with ReLU activation
functions are MILP representable. A fully-connected feed-
forward NN ψ : X ⊆ Rn → U ⊆ Rm is a function

ψi(zi−1) = ReLU(zi−1) = max(0, Wizi−1 + bi)

(3)

for some weight matrix Wi ∈ Rni×ni−1 and bias vector
bi ∈ Rni, i = 1, . . . , (cid:96), and where n0 = n is the input
dimension and n(cid:96) = m is the output dimension of the NN.
To prove that ψ is MILP representable, we use the piece-
wise linearity of the ReLU activation functions to represent
them via linear constraints and binary decision variables
indicating which piece is active [37], [38].

Lemma 2: If X is a compact polyhedron, then the ReLU
NN ψ : X → U is MILP representable, and there exist
constants mi, mi such that




gr(ψ) =

(x, u)

.

(4)




(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∀i = 1, . . . , n − 1,
∃βi ∈ {0, 1}ni,
∃zi ∈ Rni :
z0 = x,
z0 ∈ X,
zi ≥ 0
zi ≥ Wizi−1 + bi,
zi ≤ Wizi−1 + bi

− diag(mi)(1 − βi),

zi ≤ diag(mi)βi,
u = Wnzn−1 + bn




Proof: As X is bounded, there exist x, x ∈ Rn such that
X ⊆ [x, x]. Assume ﬁrst that ψ : X → U is a single ReLU
neuron deﬁned through u = max(0, x). Then the graph of ψ
can be represented via mixed-integer constraints as follows.

∃β ∈ {0, 1}n :

x ∈ X,
u ≥ 0,

u ≥ x,

gr(ψ) =






(x, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

u ≤ x − diag(x)(1 − β),

u ≤ diag(x)β






(5)

Here, the binary decision variable β indicates whether the
neuron is active (u = x) or inactive (u = 0). Extending
this reasoning to the full NN and given the element-wise
upper and lower bounds mi ≤ Wizi−1 + bi ≤ mi on the
input of the i-th activation layer, we can use Lemma 1 to
derive the graph representation (4), where the binary decision
variables βi correspond to layer i. The constants mi and
mi always exist because X is compact, ψi is continuous for
every i = 1, . . . , (cid:96), and continuous images of compact sets
are compact.

Remark 1: The bounds mi and mi can be computed using
interval arithmetic [39], zonotope propagation [40], or linear
programming [38]. State-of-the-art MILP solvers such as
Gurobi [41] are based on branch-and-cut methods. Hence,
tightening the bounds mi and mi can reduce computation

times in practice since branches can be pruned more efﬁ-
ciently. If mi and mi can not be calculated exante, e.g., if
X is unbounded, then mi and mi correspond to the usual
“big-M” constants and have to be determined endogenously.
Remark 2: Note that Lemma 2 extends to NNs with
general piecewise linear activation functions such as leaky
ReLUs. Note also that the convex hull of gr(ψ) is a strict
subset of the polyhedron obtained by relaxing βi ∈ [0, 1] in
(4). The approximation quality can be improved by adding
valid cuts [42].

B. Box Saturation

Control policies are often subject to physical constraints.
To enforce these constraints, one may project the output of
an approximate policy to the feasible set. For example, the
projection ψ onto the box [x, x] ⊆ Rm with x ≤ x can be
viewed as a vector of element-wise projections of the form

and sufﬁcient Karush-Kuhn-Tucker (KKT) conditions [43]

Primal feasibility
(cid:98)Az = Bx + b, F z ≤ Gx + g,

Stationarity
(cid:4)P z + (Qx + q) + AT µ + F T λ = 0,
Dual feasibility
(cid:98)λ ≥ 0,

Complementarity
(cid:4)λT (F z − Gx − g) = 0,

(9a)

(9b)

(9c)

(9d)

where µ ∈ Rneq and λ ∈ Rnineq are the Lagrange multipli-
ers associated with the equality and inequality constraints,
respectively. By introducing a binary decision variable βi
that evaluates to 0 if Fiz < gi and to 1 if λi > 0 for
each i = 1, . . . , nineq, we can linearize the complementarity
condition as

0 ≤ λi ≤ M βi,
0 ≤ gi − Fiz ≤ M (1 − βi)

(cid:27)

∀i = 1, . . . , nineq,

(10)

ψ(x)i =






xi if ui < xi,
ui if xi ≤ ui ≤ ui,
xi if ui > x.

where M is a suitable “big-M” constant [44]. Therefore, the
graph of ψ can be represented as

(6)

Lemma 3: The projection ψ onto the box [x, x] ⊆ Rm is

MILP representable.

Proof: The projection ψ deﬁned through (6) can be

rewritten in the following form using the ReLU function:

ψ(x) = x − ReLU(x − (ReLU(x − x) + x)).

(7)

gr(ψ) =

Hence, ψ can be recognized as a special instance of a ReLU
NN, which is MILP representable according to Lemma 2.

C. Parametric QPs

We now show that

the optimal solution mappings of
parametric QPs are MILP representable. Speciﬁcally, we
deﬁne ψ(x) as the unique minimizer of the parametric QP

zT P z + (Qx + q)T z

1
2

min
z
s.t. Az = Bx + b,
F z ≤ Gx + g,

(8)

with internal decision variable z ∈ Rnz and matrices P ∈
Snz
++, Q ∈ Rnz×n, q ∈ Rnz , A ∈ Rneq×nz , B ∈ Rneq×n,
b ∈ Rneq , F ∈ Rnineq×nz , G ∈ Rnineq×n, and g ∈ Rnineq.

Lemma 4: If the parametric QP (8) is feasible for every
x ∈ X, then its optimal solution mapping ψ(x) is MILP
representable.
Proof:

Since the feasible set of problem (8) is a
polyhedron, it admits a Slater point whenever it is non-empty.
Since the objective function of (8) is also strictly convex,
z(cid:63)(x) for x ∈ X is uniquely determined by the necessary






(x, z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∀i = 1, . . . , nineq
∃β ∈ {0, 1}nineq,
∃z ∈ Rnz ,
∃λ ∈ Rnineq,
∃µ ∈ Rneq :
x ∈ X,
0 = Az − Bx − b,
0 = P z + (Qx + q) + AT µ

+ F T λ,
0 ≤ λi ≤ M βi,
0 ≤ gi − (Gx)i − Fiz

≤ M (1 − βi)






.

(11)

Remark 3: Certain MILP solvers accept

logical con-
straints and automatically transform them into “big-M” con-
straints or special-ordered set constraints, with the advantage
that
the constraints are optimally chosen by the solver,
leading to faster convergence. Tight “big-M” bounds suitable
for numerical purposes can also be calculated by solving
auxiliary LPs. Details are relegated to Appendix I.

D. Piecewise Afﬁne Functions over Polyhedral Sets

Piecewise afﬁne functions deﬁned over polyhedral sets
lend themselves for modeling piecewise afﬁne hybrid dy-
namical systems [45]. A continuous function ψ : X → U
is called piecewise afﬁne if there exist polyhedra Xi =
{x ∈ X | Fix ≤ gi}, i ∈ I, with X = (cid:83)
Xi as well as
matrices Ai ∈ Rm×n and vectors ci ∈ Rn, i ∈ I, such that

i∈I

ψ(x) = Aix + ci ∀x ∈ Xi, ∀i ∈ I,

(12)

where I is a ﬁnite index.

Lemma 5: If X is a compact polyhedron, then the piece-
wise afﬁne function deﬁned in (12) is MILP representable.
Proof: By [45, Theorem 3.5] there exist binary vari-

ables βi ∈ {0, 1}, i ∈ I, such that

Fixi ≤ βigi ∀i ∈ I,
(cid:88)

(cid:88)

βi, x =

xi,

1 =

i∈I

i∈I

ψ(x) =

(cid:88)

i∈I

(Aixi + βici).

(13a)

(13b)

(13c)

if βi = 0,

Note ﬁrst that Xi ⊆ X inherits compactness from X. The
constraint (13a) implies that xi = 0 whenever βi = 0.
Indeed,
to fall within
the recession cone of the compact polyhedron Xi, which
coincides with the singleton {0}. If βi = 1, on the other
hand, then (13a) forces xi to fall within Xi. The graph of ψ,
thus, admits the following MILP representation:

then (13a) forces xi

gr(ψ) =






(x, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∃βi ∈ {0, 1} ∀i ∈ I,
∃xi ∈ Rn
∀i ∈ I :
x ∈ X,
Fixi ≤ βigi
(cid:88)

∀i ∈ I,

1 =

βj,

x =

u =

j∈I
(cid:88)

j∈I
(cid:88)

j∈I

xj,

(Ajxj + βjcj)






.

(14)

III. APPROXIMATION ERROR COMPUTATION

Consider now a veriﬁcation problem of the form (1) with
baseline policy ψ1(·) and approximate policy ψ2(·), and set
the error function f (·) to the inﬁnity norm (cid:107)·(cid:107)∞. The worst-
case error over a bounded polytopic input set X is given by

γ = max
x∈X

(cid:107)ψ1(x) − ψ2(x)(cid:107)∞.

(15)

The inﬁnity norm is a natural choice for measuring the
mismatch between ψ1 and ψ2, and it is MILP representable.
Lemma 6: The inﬁnity norm f (t) = (cid:107)t(cid:107)∞ is MILP

representable on any bounded polytopic set X ⊆ Rm.

Instead of the inﬁnity norm, one could also use the 1-norm

to quantify the worst-case error in (15).

Corollary 1: The 1-norm f (t) = (cid:107)t(cid:107)1 is MILP repre-

sentable on any bounded polytopic set X ⊆ Rm.

The proofs of Lemma 6 and Corollary 1 are quite standard

and can be found in Appendix II-A and II-B respectively.

Lemma 6 implies via Proposition 1 that if ψ1(x) and
ψ2(x) are MILP representable, then the worst-case approxi-
mation error (15) can be computed by solving an MILP.

Remark 4: By solving 2m MILPs it is also possible to
calculate a tight bounding box [γ, γ] ⊆ Rm that covers the
approximation error ψ1(x) − ψ2(x) for every x ∈ X. Indeed,

to compute the components of γ, we solve m MILPs of the
form

γ

i

= min
x,u1,u2

eT
i (u1 − u2)

s.t. x ∈ X,

(x, u1) ∈ gr(ψ1),
(x, u2) ∈ gr(ψ2),

(16)

where ei ∈ Rm is the i-th vector of the canonical basis.
To compute the components of γ, we simply convert the
minimization to a maximization in (16).

A. Veriﬁcation Against Robust MPC

We can use the worst-case approximation error (15) to
verify an approximate MPC scheme against a robustly
stable MPC policy. Let ψ1(x) be a control policy that
input disturbances in the set W =
is robust against
{w ∈ Rm | (cid:107)w(cid:107)∞ ≤ ˆγ}. An example for such a control
policy would be Tube MPC [34], which is also MILP
representable as we will see in Section V-B. Additionally,
let ψ2(x) be an approximation of ψ1(x). For example, ψ2(x)
can be obtained by sampling the baseline policy ψ1(x) and
learning the input/output map by training a NN on the
samples. Then, we can verify the closed-loop stability of
ψ2(x) as follows:

Theorem 1: The approximate control policy ψ2(x) is sta-
ble in closed-loop on a polyhedron X if (17) is satisﬁed. (17)
is a MILP if ψ1(x) and ψ2(x) are MILP representable.

0 ≤ min

τ,x,u1,u2

ˆγ − τ

s.t. x ∈ X,

(x, u1) ∈ gr(ψ1),
(x, u2) ∈ gr(ψ2),
(u1 − u2, τ ) ∈ gr((cid:107) · (cid:107)∞).

(17)

Proof: The worst case approximation error between
ψ1(x) and ψ2(x) over the domain X is given by γ as deﬁned
in (15). Hence, ψ2(x) applied to the real system deviates
maximally by γ with respect to the inﬁnity norm. Since the
original policy ψ1(x) is robust against input disturbances
of magnitude up to ˆγ in respect to the inﬁnity norm, the
inequality γ ≤ ˆγ is sufﬁcient for ψ2(x) to be stable. We
can verify the inequality γ ≤ ˆγ by solving (17) where, at
optionality, τ coincides with the worst case approximation
error γ. If ψ1(x) and ψ2(x) are MILP representable, then it
directly follows that (17) is also an MILP.

IV. STABILITY VERIFICATION FOR APPROXIMATE
LINEAR MPC

In Section III-A, we veriﬁed the stability of the approxi-
mate policy against a robustly stable MPC policy by checking
an inequality involving the worst-case approximation error.
In this section, we show that we can also directly verify a
Lyapunov decrease condition to show stability.

Assume we have a linear system

x+ = Ax + Bu,

(18)

where the state x ∈ Rn and the input u ∈ Rm are
constrained to lie in the polytopic sets X ⊆ Rn and U ⊆ Rm,
respectively, and we consider the following ﬁnite horizon
MPC scheme

J(x, u)

J (cid:63)(x) := min
x,u
s.t. ∀i = 0, . . . , N − 1,
xi+1 = Axi + Bui,
xi ∈ X, ui ∈ U,
xN ∈ XN ,
x0 = x,

(19a)

(19b)

(19c)

(19d)

(19e)

(19f)

with the cost function

J(x, u) = VN (xN ) +

N −1
(cid:88)

i=0

(cid:96)(xi, ui).

(20)

For ease of notation, we also henceforth use

F(x) =






(x, u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∀i = 0, . . . , N − 1,
xi ∈ X, ui ∈ U,
xN ∈ XN , x0 = x






(21)

as a shorthand for the feasible set of problem (19).

Assumption 1: There exists a linear control law v : Rn →
Rm such that the terminal set XN ⊆ X is a polytopic
invariant set for the system x+ = Ax + Bv(x) with state
and input constraints (19d) [46], and VN (x) is a Lyapunov
function for the same system that decreases by one stage
cost each time step, that is, VN (x+) − VN (x) ≤ (cid:96)(x, v(x))
for every x ∈ XN .
A standard approach to satisfy Assumption 1 is to choose
the classic stage cost

(cid:96)(xi, ui) = xT

i Qxi + uT

i Rui,

(22)

+ and R ∈ Sm

where Q ∈ Sn
++, and the solution of the
LQR problem as the Lyapunov function VN (x) and the LQR
controller for v(x). Then the control invariant set XN can
be calculated using the MPT toolbox [47], and the decrease
condition for VN (x) is fulﬁlled. This can be directly veriﬁed
by looking at the deﬁnition of the LQR problem, which
solves the following problem:

VN (x) = J (cid:63)

∞(x) := min
x,u

∞
(cid:88)

i=0

(cid:96)(xi, ui)

s.t. xi+1 = Axi + Bui,

x0 = x.

(23)

Clearly, if v(x) is an optimal solution for the ﬁrst input
u0 in (23), then VN (x+) = VN (Ax + Bv(x)) = VN (x) +
(cid:96)(x, v(x)). In addition, it is well known that J (cid:63) is a Lyapunov
function for the system x+ = Ax + Bv(x) [48].

Assume we approximate the MPC scheme (19). We then
get the approximate closed-loop system x+ = Ax+Bψ2(x),
where ψ2(x) is an MILP representable approximate MPC
controller (e.g., a NN).

A. Sufﬁcient Condition for Lyapunov Decrease

To verify stability, we will make use of the following
sufﬁcient condition for the approximate control law to be
stabilizing for a given initial state set. The condition is similar
as in [49], but has been extended to allow for the veriﬁcation
of asymptotic stability.

Lemma 7: Let Assumption 1 hold, and J (cid:63)(x) be the
optimal value function of (19). Also, let (cid:15) > 0 be a positive
constant. If ψ2(x) : Rn → Rm is a control law for the
closed-loop system deﬁned over the set X0,
then J (cid:63) is
also a Lyapunov function for x+ = Ax + Bψ2(x) on a
set Xψ2
0 ⊆ X0 if for all x0 ∈ X0, there exists a feasible
state/input sequence (˜x, ˜u) ∈ F(x0) such that ˜u0 = ψ2(x0)
and

J(˜x, ˜u) − J (cid:63)(x0) ≤ (cid:96)(˜x0, ˜u0) − (cid:15)(cid:107)˜x0(cid:107)2

2 ∀x0 ∈ Xψ2

0 , (24)

If

the

Proof:

with Xψ2

input/state

0 = {x0 ∈ X0 | ˜u0 = ψ2(x0) and ∃(˜x, ˜u) ∈ F(x0)}.
sequence
(ψ2(˜x0), ˜u1, . . . , ˜uN −1, ˜x0, . . . , ˜xN )
(19)
is
and satisﬁes (24) for all ˜x0 ∈ Xψ2
then the shifted
0 ,
sequence (˜u1, . . . , ˜uN −1, v(˜xN ), ˜x1, . . . , ˜xN , ˜xN +1) is also
feasible, where ˜xN +1 = A˜xN + Bv(˜xN ), and we used the
assumption that XN is invariant for the linear control law
v(·). Evaluating the cost function at this sequence gives

feasible for

J (cid:63) (˜x1) ≤ VN (˜xN +1) +

N
(cid:88)

(cid:96) (˜xi, ˜ui)

i=1
= VN (˜xN +1) − VN (˜xN ) + (cid:96) (˜xN , ˜uN )
N −1
(cid:88)

+ VN (˜xN ) +

(cid:96) (˜xi, ˜ui)

i=0
− (cid:96) (˜x0, ψ2 (˜x0))
≤ J (cid:63)(˜x0) − (cid:15)(cid:107)˜x0(cid:107)2
2

(25a)

(25b)

(25c)

(25d)

(25e)

for all ˜x0 ∈ Xψ2
0 . Equation (25b) is negative by the
assumption that VN decreases faster than the stage cost (cid:96)
in the set XN and (25c) is less than or equal to J (cid:63)(˜x0) +
(cid:96)(˜x0, ψ2(˜x0)) − (cid:15)(cid:107)˜x0(cid:107)2
2 because of (24). It follows that
J (cid:63)(˜x1) − J (cid:63)(˜x0) ≤ −(cid:15)(cid:107)˜x0(cid:107)2
and therefore
J (cid:63)(x) is a Lyapunov function for the approximate closed-
loop system x+ = Ax + Bψ2(x).

2 ∀˜x0 ∈ Xψ2

0

The conditions in Lemma 7 are not easy to verify, but can

be equivalently written as

∀x0 ∈ Xψ2
0
0 ≤ (cid:96)(˜x0, ˜u0) − (cid:15)˜xT

: ∃(˜x, ˜u) ∈ F(x0) : ˜u0 = ψ2(x0),

0 ˜x0 − J(˜x, ˜u) + J (cid:63)(x0)

(26)

which we can be formulated as a min-max problem

0 ≤ min
x0∈Xψ2
0

max
(˜x,˜u)∈F (x0)
˜u0=ψ2(x0)

(cid:96)(˜x0, ˜u0)−(cid:15)˜xT

0 ˜x0−J(˜x, ˜u)+J (cid:63)(x0).

By replacing the last term with

J (cid:63)(x0) =

min
(x,u)∈F (x0)

J(x, u)

(27)

(28)

and rearranging terms we get the problem

J(x, u)

0 ≤ min
x0∈Xψ2
0
(x,u)∈F (x0)
+ max

(˜x,˜u)∈F (x0)
˜u0=ψ2(x0)

(cid:96)(˜x0, ˜u0) − J(˜x, ˜u) − (cid:15)˜xT

0 ˜x0.

(29)

A standard approach in robust optimization would be to
dualize the max problem. Unfortunately, this leads to an
optimization problem with a bilinear term in the objective,
which can not be easily linearized. Empirically, we found
that the resulting optimization problem is almost impossible
to solve.

Instead, we write (29) it in terms of a bilevel optimization

problem

0 ˜x0

J(x, u) + (cid:96)(˜x0, ˜u0) − J(˜x, ˜u) − (cid:15)˜xT

ξ := min
x,u
s.t. x0 ∈ Xψ2
0 ,
(x, u) ∈ F(x0),
(˜x, ˜u) = arg min
˜x,˜u
s.t. (˜x, ˜u) ∈ F(x0),
˜u0 = ψ2(x0).

J(˜x, ˜u) + (cid:15)˜xT

0 ˜x0

(30a)

(30b)

(30c)

(30d)

We can see that the conditions in Lemma 7 are fulﬁlled
if and only if ξ is positive. Solving such bilevel problems
can be quite challenging. In Section IV-D we propose to
reformulate (30) as a mixed-integer program. Compared to
dualizing the min-max problem (29), we found that
the
mixed-integer reformulations described in Section IV-D tend
to be very solvable in practice. We believe that this is the case
because the root relaxation is bounded due to only having
bounded primal variables in the objective. Dualizing problem
(29) introduces dual variables in the objective, which are
generally unbounded.

Although J (cid:63)(x) is a Lyapunov function on the set Xψ2
0 , it
is not sufﬁcient to prove stability. The system must also be
invariant on Xψ2

0 . This leads us to the following result:

then there exists a set Ωψ2

Theorem 2: Let ψ2(0) = 0. If the solution of (30) is
ξ ≥ 0,
such that
applying the controller ψ2(x0) results in the closed-loop
system converging asymptotically to the origin for all x0 ∈
Ωψ2
0 .

0 ⊆ Xψ2

0 ⊆ Xψ2

Proof: By construction, (30) satisﬁes condition (24) if
and only if ξ ≥ 0. Hence, J (cid:63) is a Lyapunov function for
the system x+ = Ax + Bψ2(x) where x ∈ Xψ2
0 . We are
choosing Ωψ2
to be the set on which the closed-loop
is invariant. Thus, together with J (cid:63)(x) being a Lyapunov
function and Ωψ2
0 being invariant, it follows that the system
converges asymptotically to the origin for all x0 ∈ Ωψ2
0 . The
condition ψ2(0) = 0 ensures that the origin is actually an
equilibrium point, and that Ωψ2
0

is not empty.

0

0

B. Direct Veriﬁcation of Lyapunov Decrease

Instead of verifying the sufﬁcient condition (24), we can

also directly verify the Lyapunov decrease

J (cid:63)(x+) − J (cid:63)(x) ≤ −(cid:15)(cid:107)x(cid:107)2

2 ∀x ∈ X0,

(31)

with x+ = Ax + Bψ2(x). Hence, similar to the previous
section, we can formulate the condition (31) as a bilevel
optimization problem

J(x, u) − J(˜x, ˜u) − (cid:15)˜xT

0 ˜x0

ξ := min
x,u
s.t. x0 ∈ X0,

(x, u) ∈ F(x0),
(˜x, ˜u) ∈ ˜F(x0)

(32a)

(32b)

(32c)

(32d)

where we deﬁne the feasible set of the inner optimization
problem as

˜F(x) = arg min
˜x,˜u
s.t. (˜x, ˜u) ∈ F(Ax + Bψ2(x))

J(˜x, ˜u) + (cid:15)˜xT

0 ˜x0

(33)

|

Note that compared to problem (30), the feasible set of
(32) is actually an invariant set for the closed-loop system.
Thus, we get the following result:

= {x0 ∈ X0

Theorem 3: Let ˜Ωψ2
0

(x, u) ∈
F(x0), (˜x, ˜u) ∈ ˜F(x0)} be the feasible set of (32) for
varying x0. If the optimal value of (32) is ξ ≥ 0 and
ψ2(x0) ∈ U ∀x0 ∈ ˜Ωψ2
then applying the controller
0 ,
ψ2(x0) will result
in the closed-loop system converging
asymptotically to the origin and satisﬁes the state constraints
X for all x0 ∈ ˜Ωψ2
0 .

Proof: By construction, condition (31) is satisﬁed if and
only if ξ ≥ 0. Hence, it follows that J (cid:63)(x) is a Lyapunov
function for the system x+ = Ax + Bψ2(x) where x ∈ ˜Ωψ2
0 .
With the constraint ˜x0 = Ax0 + Bψ2(x0), we ensure that
˜Ωψ2
0 only contains x0 such that when ψ2(x0) gets applied
to the system, it is inside the state/input feasible set of the
MPC scheme (19). Since (19) satisﬁes the state constraints X,
together with the assumption that ψ2(x0) ∈ U ∀x0 ∈ ˜Ωψ2
0 ,
we get that ˜Ωψ2
is invariant for the closed-loop system and
0
˜Ω0 ⊆ X. Hence, the system converges asymptotically to the
origin and satisﬁes the state constraints in the set X for all
x0 ∈ ˜Ωψ2
0 .

Remark 5: Note that Theorem 3 does not require the MPC
scheme (19) to have a terminal set, nor to be known stable,
i.e., Assumption 1 does not have to hold. Hence, even MPC
schemes that are not known to be stable a priori can be
used, and asymptotic stability can still be veriﬁed because
we verify the Lyapunov decrease condition of the MPC cost
directly. Hence, if we chose ψ2(x) to be the same as the
control policy of the solution of (19), we can verify the
asymptotic stability of any MILP representable control law.
In fact, this is very similar to the approach taken in [50],
where the stability of MPC schemes is veriﬁed a posteriori
using an MILP.

C. Outer Approximation of Stable Region

In Theorem 3 we have seen that the approximate controller
ψ2(·) is stable if ξ ≥ 0 and satisﬁes state constraints on a
set ˜Ωψ2
0 . Unfortunately, calculating this exact set presents a
signiﬁcant challenge, but a convex polyhedral outer approx-
imation ¯Ωψ2
0

can be found. We can deﬁne the set ¯Ωψ2
0
¯Ωψ2
0 = {x ∈ Rn | Cstablex ≤ cstable} ,

(34)

as

where Cstable ∈ Rnstable×n and cstable ∈ Rnstable with nstable
being the number of hyperplanes in the polyhedron. Fixing
Cstable, we solve nstable bilevel optimizations problems

c(cid:63)
stablei = max
Cstableix0
x,u
s.t. x0 ∈ X,

(x, u) ∈ F(x0),
(˜x, ˜u) ∈ ˜F(x0),

(35a)

(35b)

(35c)

(35d)

for i = 1, . . . , nstable. The chosen Cstable dictates the shape of
the outer approximation of the polyhedron, and by solving
(35) we ﬁnd the smallest such polyhedron which encapsu-
lates the set ˜Ωψ2
0 . A simple choice for Cstable and cstable would
be a box, i.e.,

Cstable =

(cid:21)

(cid:20) In
−In

,

cstable = 12n.

(36)

D. MIP Formulation

Problems (30), (32) and (35) as stated in the previous
sections can not be directly solved. Fortunately, we can
reformulate them to ﬁt into the veriﬁcation framework intro-
duced at the beginning. We note that the inner optimization
problems (30d) and (32d) are QPs, and can be reformulated
as

˜z(x0, ψ2(x0)) = arg min

˜z

1
2
s.t. ˜A˜z + ˜B

˜zT ˜P ˜z + ˜qT ˜z
(cid:20) x0
ψ2(x0)

(cid:21)

= ˜b,

(37)

˜F ˜z ≤ ˜g,

with ˜z = (˜x, ˜u) and appropriate matrices ˜P , ˜q, ˜A, ˜B, ˜b,
˜F , and ˜g. Note that (37) is a positive deﬁnite parametric QP
with parameters x0 and ψ2(x0). Hence, using the formulation
introduced in Section II-C, we can rewrite (30) and (32) as

ξ := min
x,u,˜u,˜z
µLλL,βL

J(x, u) −

˜zT ˜P ˜z − ˜qT ˜z

1
2

s.t. x0 ∈ X0,

(x, u) ∈ F(x0),
(x0, ˜u) ∈ gr(ψ2),
(cid:20)x0
˜u

˜A˜z + ˜B

(cid:21)

= ˜b,

(38)

0 = ˜P ˜z + ˜q + ˜AT µL + ˜F T λL,
0 ≤ λL
i ≤ M βL
i ,
0 ≤ ˜gi − ˜F ˜z ≤ M (1 − βL
λL( ˜F ˜z − ˜g) = 0,

i ),

with µL and λL being the appropriate Lagrange multipliers
of the respective QP, and βL
i being the binary decision vari-
ables representing the active/inactive constraints. Assuming
ψ2(x0) is MILP representable (e.g., ReLU NN), problem
(38) becomes an indeﬁnite MIQP which can be solved with
commercial solvers like Gurobi [41]. The problem (35) to

ﬁnd the outer approximation of the stable region ¯Ωψ2
0
reformulated into something similar

is

c(cid:63)
stablei = max
x,u,˜u,˜z
µLλL,βL

Cstableix0

s.t. x0 ∈ X0,

(x, u) ∈ F(x0),
(x0, ˜u) ∈ gr(ψ2),
(cid:20)x0
˜u

˜A˜z + ˜B

(cid:21)

= ˜b,

(39)

0 = ˜P ˜z + ˜q + ˜AT µL + ˜F T λL,
0 ≤ λL
i ≤ M βL
i ,
0 ≤ ˜gi − ˜F ˜z ≤ M (1 − βL
λL( ˜F ˜z − ˜g) = 0,

i ),

but is easier to solve since it is an MILP and not an indeﬁnite
MIQP.

Remark 6: Note that, because the objective function is
indeﬁnite, problem (38) is considerably harder to solve than a
normal MILP or MIQP. Solvers only added support for such
problems in recent years (e.g., introduced in Gurobi 9.0), by
translating the indeﬁnite cost into bilinear constraints, which
are then solved using cutting planes and spatial branching.

V. CASE STUDY: DC-DC POWER CONVERTER

In this section, we consider the DC-DC power converter
from [6] as a case study. The original linear MPC controller
is approximated via a piecewise afﬁne NN. Evaluating the
NN is considerably cheaper, allowing the approximate con-
trol policy to be deployed on a low cost microcontroller
operating at 80MHz and with a sampling frequency of
10 kHz. Simulation results show the effectiveness of the
approximate control in some cases, however, the work gives
no guarantee of stability. Using the techniques introduced
above, we can show that the NN controller is indeed stable
for the closed-loop system, and give an outer approximation
of the stability region.

Additionally, we design a tube MPC robust

to input
disturbances. The robust controller is then approximated
using a NN. Using the formulation in Section III-A we
can guarantee the closed-loop system controlled by the NN
satisﬁes constraints and converges to the minimum robust
invariant set.

The model of the DC-DC converter is linearized and
discretized, giving us the following two-state x = (iL, vO)
(current and voltage), and one-input (duty cycle) linear
system

x+ = Ax + Bu =

(cid:20)0.971 −0.010
0.970
1.732

(cid:21)

x +

(cid:21)

(cid:20)0.149
0.181

u.

(40)

A more detailed derivation of the model dynamics along with
additional details on the physical system can be found in [6].

A. Nominal MPC

The problem of control design for tracking current and
voltage references in the DC-DC converter can be formulated

x

F x + f

y1

Hz + y1
0

≥
y2

arg min

||
s.t. z

2 + ε

2

z

||

||

||

Gy2 + g

y3

sat(y3)

u

Fig. 2: The piecewise afﬁne NN architecture.

as a linear MPC controller. We formulate the MPC controller
which regulates to the steady-state xeq = (cid:2)0.05
, and
ueq = 0.3379 as

5(cid:3)T

min
x,u

N −1
(cid:88)

(cid:16)

i=0

(cid:107)xi − xeq(cid:107)2

Q + (cid:107)ui − ueq(cid:107)2

R

(cid:17)

(41)

+ (cid:107)xN − xeq(cid:107)2
P

s.t. ∀i = 0, . . . , N − 1,
xi+1 = Axi + Bui,
(cid:21)
(cid:20)0
0

(cid:20)0.2
7

≤ xi ≤

(cid:21)

,

Fig. 3: Original, approximate/simpliﬁed control policy (top),
and approximation error to the original MPC control policy
with outer approximation of stability region (bottom).

is the case for this example. With this, we have successfully
veriﬁed the approximate control policy against the nominal
MPC. Additionally, we solve (35) to get the outer approx-
imation of the stability region ¯Ωψ2
0 , which can be seen in
Figure 3. Note that the region is smaller than the feasible
region of the MPC controller. This is expected, because due
to the approximation error, we can not expect to achieve the
same stability region.

0 ≤ ui ≤ 1

B. Robust Tube MPC

xN ∈ XN ,

x0 = x(0),

with horizon length N = 10, Q = diag(90, 1), R = 1, P
the solution of the associated discrete-time algebraic Riccati
equation, and XN the system’s maximal invariant set under
the LQR policy. The resulting control policy can be seen in
Figure 3.

We chose the same architecture for the approximate con-
troller as in [6], which can be seen in Figure 2, with the
appropriate dimensions of the layers fully determined by the
size of the optimization variable z ∈ Rnz , which was chosen
as nz = 3. Note that this is not a classic NN architecture,
since the second layer is the solution of a parametric QP.
The reader is referred to [6] for a more detailed description
and analysis of the architecture. This shows the ﬂexibility of
our toolbox, since we also allow more unconventional layer
structures.

For the training data, 5000 samples of the original con-
troller (41) uniformly distributed from the feasible region
are taken, and the approximate controller is then trained
using Adam [51] as the optimizer. To formulate the convex
optimization layer, cvxpylayers [22] was used. The trained
simpliﬁed controller and the absolute approximation error
can be seen in Figure 3.

Using the formulation in Section III we ﬁnd an absolute
worst-case approximation error of γ = 0.24 at iL = 0 and
vO = 6.79. Similarly, using both formulations in Sections
IV-A and IV-B, by solving an indeﬁnite MIQP respectively,
we can verify the stability of the closed-loop system, which

Starting from the MPC formulation in (41), we formulate
a robust tube MPC [34]. We assume that we have uncertain
dynamics

x+ = Ax + Bu + Bw,

(42)

with disturbance set W = {w ∈ R | −0.1 ≤ w ≤ 0.1}. The
objective is now to design a robust controller to withstand an
input disturbance with amplitude 0.1. The input disturbance
is typically associated to the distortion introduced by the
switching nature of the converter, but here we use it to verify
the stability of the approximate policy.

We extend the nominal MPC formulation (41) to the

following robust tube MPC formulation

min
z,v

N −1
(cid:88)

(cid:16)

i=0

(cid:107)zi − xeq(cid:107)2

Q + (cid:107)vi − ueq(cid:107)2

R

(cid:17)

+ (cid:107)zN − xeq(cid:107)2
P

s.t. ∀i = 0, . . . , N − 1,
zi+1 = Azi + Bvi,
zi ∈ X (cid:9) E,
zN ∈ XN ,

vi ∈ U (cid:9) KE
x(0) ∈ z0 ⊕ E,

(43)
where E is the minimum robust invariant set with respect to
a linear feedback gain K, and the control law is given by
ψ1(x) = K(x − z(cid:63)
0(x). Here K is the feedback
gain and P is the solution of the Riccati equation associated
with the discrete LQR problem. The policy is robustly stable
for input disturbances in W [34, Theorem 1]. The horizon
length was increased to N = 20 to have a bigger feasible

0 (x)) + v(cid:63)

0.00.10.2iL0.02.55.0vOOriginalController0.000.250.500.751.000.00.10.2iL0.02.55.0vOSimpliﬁedController0.000.250.500.751.000.00.10.2iL0.02.55.0vOApproximationError¯Ωπ00.00.10.2Fig. 4: Original, approximate/simpliﬁed robust control policy
(top), and approximation error to the original robust MPC
control policy (bottom).

region. Note that (43) is a pQP since the sets X(cid:9)E, U(cid:9)KE,
and E are polyhedra that can be precalculated.

We then approximate the tube MPC with a NN with 2
hidden layers, 50 neurons each, and a saturation layer at
the end to clip the input between -1 and 1. Similar as in
Section V-A, 5000 samples of the tube MPC uniformly in
the feasible region are taken and the NN is trained using
standard techniques. The resulting NN controller together
with the original tube MPC can be seen in Figure 4.

We apply the formulation from Section III to ﬁnd the
worst case approximation error, and get a value of γ =
0.073. Since we designed our controller to be robust for
input perturbations with a maximum magnitude of 0.1, we
have shown that the NN controller satisﬁes constraints and
converges to the minimum robust invariant set in the feasible
region of the tube MPC.

The set on which the closed-loop system is stable is
directly given by the feasible set of the tube MPC, compared
to the nominal MPC where we could only calculate an outer
approximation of the stability region. Additionally, to verify
stability, we only have to solve an MILP and not an indeﬁnite
MIQP, which might be considerably harder to solve. But
for larger problem sizes, the solve times are not necessarily
larger as we discuss in Section VI-B.

C. Experimental Validation

The approximate nominal controller in Section V-A has
been implemented on an inexpensive STM32L476 micro-
controller for a prototype of the described DC-DC power
converter. The controller is running at a frequency of 10 kHz,
with the execution time of the control law being between
22.0 µs and 27.5 µs. The closed-loop startup response can be
seen in Figure 5. We can see that the constraints for both the
voltage vO and the current iL are satisﬁed, and an excellent
transient with a settling time of 2.33 ms. For more elaborate
implementation details, the reader is referred to [6].

Fig. 5: Closed-loop startup response of the approximate
MPC.

VI. DISCUSSION

A. Comparison to Lipschitz Based Methods

We compare our method to the approach introduced in

[33]. The authors provide the following two main results.

Lemma 8 ([33, Lemma 3.2]): There exist ζc > 0 such
that, if γ < ζc, the LTI system in (18) with NN controller
ψ2(x) converges in ﬁnite time to a neighborhood of the
origin, for all x0 ∈ Ωc, with c := maxa {a ≥ 0 | Ωa ⊆ X}.

Theorem 4 ([33, Theorem 3.4]): Let Assumption 1 hold.
There exist ζc and ϑ such that,
if γ < ζc and
L∞ (ψ1(x) − ψ2(x), XN ) < ϑ, and b ≥ 0 can be chosen
so that Ωb ⊆ XN , then the LTI system in (18) with NN
controller ψ2(x) converges exponentially to the origin, for
all x0 ∈ Ωc, with c := maxa {a ≥ 0 | Ωa ⊆ X}.

Here Ωa denotes the a-sublevel set of J (cid:63)(x), and
Lα(F, S) is the local α-Lipschitz constant over some set
S ⊆ Rn for a given mapping F : Rn → Rm.

Lemma 8 states that the NN policy is input-to-state-stable
(ISS) if the worst case approximation error is smaller than
a constant ζc. This does not guarantee asymptotic stability
yet, but in Theorem 4 exponential convergence to the origin
is shown if the Lipschitz constant of the approximation error
is smaller than a constant ϑ.

We are not going in more detail on how to calculate
these constants. The reader is referred to [33] for more
details, but it turns out that these constants become small in
practice. Since the NN almost surely does not approximate
the baseline control policy arbitrary well in practice, the
Lipschitz constant of the approximation error can grow
signiﬁcantly, especially if the NN is over parameterized. To
demonstrate this, we consider a simple example of a double

0.00.10.2iL0.02.55.0vOOriginalRobustController0.000.250.500.751.000.00.10.2iL0.02.55.0vOSimpliﬁedRobustController0.000.250.500.751.000.00.10.2iL0.02.55.0vOApproximationError0.000.020.040.06051015Time[ms]012345vO[V]0.000.050.100.150.20iL[A]Fig. 6: Worst case approximation error γ.

Fig. 8: Stability certiﬁcate ξ for solving (32). The green area
shows successful veriﬁcation of asymptotic stability.

taken, and the approximate controller is then trained using
L-BFGS for different hidden layers and neurons per hidden
layer. The resulting worst case approximation error γ can
be seen in Figure 6. Note that for sufﬁciently large NN
architectures γ < ζ180, but γ > ζ1.1 for all architectures.
Hence, with the approach from [33] it is possible to show
ISS, but not exponential stability. In Figure 7 we also plot the
Lipschitz constant L∞ (ψ1(x) − ψ2(x), XN ) of the approx-
imation error. Note that it is generally orders of magnitude
larger than ϑ, making it impossible to verify asymptotic
stability even if the approximation error is small enough.

As a comparison, we run our direct veriﬁcation method
by solving (32). The results can be seen in Figure 8. Note
that we not only verify asymptotic stability, but did so even
for NN policies with a very small number of hidden layers
and neurons, where the method in [33] failed to show ISS.

Fig. 7: Lipschitz constant L∞ (ψ1(x) − ψ2(x), XN ) of the
approximation error.

integrator. We deﬁne the baseline MPC policy

B. Numerical Experiments

min
x,u

N −1
(cid:88)

(cid:16)

i=0

(cid:107)xi(cid:107)2

Q + (cid:107)ui(cid:107)2

R

(cid:17)

+ (cid:107)xN (cid:107)2
P

s.t. ∀i = 0, . . . , N − 1,

ui,

(44)

(cid:21)

xi+1 =
(cid:20)−10
−10
xN ∈ XN ,

(cid:21)
(cid:20)1 1
0 1

(cid:21)
(cid:20)0
1

xi +
(cid:21)
(cid:20)10
10
x0 = x(0),

≤ xi ≤

, −1 ≤ ui ≤ 1

with horizon length N = 10, Q = diag(1, 1), R = 0.1, P
the solution of the associated discrete-time algebraic Riccati
equation, and XN the system’s maximal invariant set under
the LQR policy.

We estimate the largest sublevel set Ωc ⊆ X with c = 180,
giving us ζ180 = 0.1673. This will only guarantee ISS. To
prove exponential stability, we estimate the largest sublevel
Ωb ⊆ XN with b = 1.1, giving us ζ1.1 = 0.0010 and ϑ =
0.0436.

For the training data, 1000 samples of the baseline con-
troller (44) uniformly distributed from the feasible region are

In this section, we show the numerical performance of our
approach. All problems were solved using Gurobi 9.5 [41]
using 32 Threads on a workstation with an AMD Ryzen
Threadripper 3990X 4.3 GHz CPU and 32 GB of RAM.

We approximate and verify against MPC problem (41),
and use a ReLU NN architecture as introduced in Section II-
A to approximate the MPC policy. For the training, we
collect 2000 samples from the MPC controller uniformly
distributed on the feasible region, and the NN is trained using
PyTorch [35] with ADAM [51] as the optimizer.

Here, we are not necessarily interested in the approxima-
tion accuracy of the NN, but more in the numerical perfor-
mance and scaling of the to-be-solved optimization problems.
Applying this method to a speciﬁc problem requires naturally
more tuning since the veriﬁcation will only succeed if the
NN approximates the MPC policy closely enough, but the
presented results will still give a quantitative insight in solve
times and what can be expected if applied to a more speciﬁc
problem.

In a ﬁrst experiment, we ﬁx the horizon of the MPC
controller to be N = 10, and solve problem (17) for
different NN architectures varying in the number of hidden

151015202530NeuronsperHiddenLayer4321HiddenLayers0.20.40.60.81.0151015202530NeuronsperHiddenLayer4321HiddenLayers0.00.51.01.52.02.5151015202530NeuronsperHiddenLayer4321HiddenLayers−400−300−200−1000Fig. 9: Solve times of (17) in seconds with MPC horizon
N = 10.

Fig. 11: Solve times of (17), (30), and (32) in seconds for 4
hidden layers and MPC horizon N = 10.

against each other. For this, we ﬁx the horizon to N =
10 and the NN to have four hidden layers. We solve for
the maximum absolute error, and verify the MPC approx-
imation using the sufﬁcient and direct methods introduced
in Section IV by solving problems (17), (30), and (32)
respectively. The resulting computation times can be seen
in Figure 11. We can see that for small problems, solving
for the maximum absolute error between the MPC and the
NN is faster than the veriﬁcation methods. But for bigger
problems the veriﬁcation problems are solved more quickly,
although solving an indeﬁnite MIQP is considered to be
more difﬁcult. Interestingly, the direct veriﬁcation method
is slower to solve than the sufﬁcient method. Potentially, the
cascading structure of the direct veriﬁcation problem makes
the problem harder to solve.

VII. CONCLUSIONS

We have introduced a ﬂexible framework that allows one
to formulate a variety of veriﬁcation problems as MILPs or
indeﬁnite MIQPs. In particular, we have shown that not only
NN with linear layers and ReLU activation functions are
representable in such a framework, but also more complex
layer structures such as parametric QPs and piecewise afﬁne
functions on polyhedral sets can be represented and com-
bined.

Taking these formulations, we then constructed an MILP
problem, which gives us the maximum approximation error
between a given MPC and its NN approximation. Together
with an MPC formulation robust
tube
MPC), we can show constraint satisfaction and stability in
the feasible region of the MPC controller. Alternatively, we
provided indeﬁnite MIQP formulations to directly verify
stability and calculate an outer approximation of the stability
region.

in the input (ex.

We compared our approach against a Lipschitz based
method and showed that our approach outperforms it, be-
ing able to verify asymptotic stability in cases where the
Lipschitz based method could only show ISS or could not
show stability at all.

Fig. 10: Solve times of (17) in seconds for 4 hidden layers
and MPC horizon lengths N = 5, 10, 15, 20.

layers and number of neurons per hidden layer. The resulting
solve times can be seen in Figure 9. We can note that the
computation time increases exponentially with the number
of hidden layers and neurons per hidden layer. This is as
expected, since each neuron adds a binary decision variable
to the MILP, increasing the solve time exponentially. On
the other hand, this also means that deeper architectures are
preferred since the representational power of the NN grows
faster with depth as compared to increasing the number of
neurons per hidden layer, resulting in architectures which can
be veriﬁed more quickly.

Next, we investigate the inﬂuence of the horizon length N
of the MPC problem. For this, we ﬁx the NN to have four
hidden layers. Increasing the horizon increases the number of
inequalities in the parametric QP. Hence, we expect the solve
times to increase since we add a binary decision variable
per inequality, which can be observed in Figure 10. The
computation time increases with the number of inequalities
and the number of neurons per hidden layer. Note that the
complexity does not depend on the state dimension of the
system, meaning that our method does not necessarily suffer
from the curse of dimensionality.

As a ﬁnal experiment, we compare the different methods

10255075100NeuronsperHiddenLayer654321HiddenLayers10010110210310420406080100NeuronsperHiddenLayer100101102103104Solvetime[s]nineq=23nineq=38nineq=53nineq=6820406080100NeuronsperHiddenLayer100101102103104Solvetime[s]errorsuﬃcientdirectREFERENCES

[1] S. Qin and T. A. Badgwell, “A survey of industrial model predictive
control technology,” Control Engineering Practice, vol. 11, no. 7, pp.
733–764, 2003.

[2] F. Oldewurtel, A. Parisio, C. N. Jones, D. Gyalistras, M. Gwerder,
V. Stauch, B. Lehmann, and M. Morari, “Use of model predictive
control and weather forecasts for energy efﬁcient building climate
control,” Energy and Buildings, vol. 45, pp. 15–27, 2012.

[3] M. W. M¨uller and R. D’Andrea, “A model predictive controller for
quadrocopter state interception,” in European Control Conference,
2013, pp. 1383–1389.

[4] M. Neunert, M. St¨auble, M. Giftthaler, C. D. Bellicoso, J. Carius,
C. Gehring, M. Hutter, and J. Buchli, “Whole-body nonlinear model
predictive control through contacts for quadrupeds,” IEEE Robotics
and Automation Letters, vol. 3, no. 3, pp. 1458–1465, 2018.

[5] P. Karamanakos, E. Liegmann, T. Geyer, and R. Kennel, “Model
predictive control of power electronic systems: Methods, results, and
challenges,” IEEE Open Journal of Industry Applications, vol. 1, pp.
95–114, 2020.

[6] E. T. Maddalena, M. W. F. Specq, V. L. Wisniewski, and C. N.
Jones, “Embedded pwm predictive control of dc-dc power converters
via piecewise-afﬁne neural networks,” IEEE Open Journal of
the
Industrial Electronics Society, vol. 2, pp. 199–206, 2021.

[7] A. Alessio and A. Bemporad, A Survey on Explicit Model Predictive

Control. Springer, 2009, pp. 345–369.

[8] C. N. Jones, P. Grieder, and S. V. Rakovi´c, “A logarithmic-
location problem for parametric linear

time solution to the point
programming,” Automatica, vol. 42, no. 12, pp. 2215–2218, 2006.
[9] F. Bayat, T. A. Johansen, and A. A. Jalali, “Using hash tables to
location problem:
manage the time-storage complexity in a point
Application to explicit model predictive control,” Automatica, vol. 47,
no. 3, pp. 571–577, 2011.

[10] M. Kvasnica and M. Fikar, “Clipping-based complexity reduction in
explicit MPC,” IEEE Transactions on Automatic Control, vol. 57,
no. 7, pp. 1878–1883, 2012.

[11] A. Domahidi, M. N. Zeilinger, M. Morari, and C. N. Jones, “Learning
a feasible and stabilizing explicit model predictive control law by
robust optimization,” in IEEE Conference on Decision and Control
and European Control Conference, 2011, pp. 513–519.

[12] S. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J. Pappas,
and M. Morari, “Approximating explicit model predictive control using
constrained neural networks,” in American Control Conference, 2018,
pp. 1520–1527.

[13] J. A. Paulson and A. Mesbah, “Approximate closed-loop robust model
predictive control with guaranteed stability and constraint satisfaction,”
IEEE Control Systems Letters, vol. 4, no. 3, pp. 719–724, 2020.
[14] D. Psaltis, A. Sideris, and A. Yamamura, “A multilayered neural
network controller,” IEEE Control Systems Magazine, vol. 8, no. 2,
pp. 17–21, 1988.

[15] B. Karg and S. Lucia, “Efﬁcient representation and approximation of
model predictive control laws via deep learning,” IEEE Transactions
on Cybernetics, vol. 50, no. 9, pp. 3866–3878, 2020.

[16] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward
networks are universal approximators,” Neural Networks, vol. 2,
no. 5, pp. 359–366, 1989.

[17] M. Hertneck, J. K¨ohler, S. Trimpe, and F. Allg¨ower, “Learning
an approximate model predictive controller with guarantees,” IEEE
Control Systems Letters, vol. 2, no. 3, pp. 543–548, 2018.

[18] X. Zhang, M. Bujarbaruah, and F. Borrelli, “Near-optimal rapid MPC
using neural networks: A primal-dual policy learning framework,”
IEEE Transactions on Control Systems Technology, vol. 29, no. 5,
pp. 2102–2114, 2021.

[19] M. Baes, M. Diehl, and I. Necoara, “Every continuous nonlinear
control system can be obtained by parametric convex programming,”
IEEE Transactions on Automatic Control, vol. 53, no. 8, pp. 1963–
1967, 2008.

[20] A. B. Hempel, P. J. Goulart, and J. Lygeros, “Every continuous
piecewise afﬁne function can be obtained by solving a parametric
linear program,” in European Control Conference, 2013, pp. 2657–
2662.

[21] B. Amos and J. Z. Kolter, “OptNet: Differentiable optimization as a
layer in neural networks,” in International Conference on Machine
Learning, 2017.

[22] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter,
“Differentiable convex optimization layers,” in Advances in Neural
Information Processing Systems, 2019.

[23] S. Bai, J. Z. Kolter, and V. Koltun, “Deep equilibrium models,” in

Advances in Neural Information Processing Systems, 2019.

[24] P. Kumar, J. B. Rawlings, and S. J. Wright, “Industrial, large-scale
model predictive control with structured neural networks,” Computers
& Chemical Engineering, vol. 150, p. 107291, 2021.

[25] J. Nubert, J. K¨ohler, V. Berenz, F. Allg¨ower, and S. Trimpe, “Safe and
fast tracking on a robot manipulator: Robust MPC and neural network
control,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.
3050–3057, 2020.

[26] T. Parisini and R. Zoppoli, “A receding-horizon regulator

for
nonlinear systems and a neural approximation,” Automatica, vol. 31,
no. 10, pp. 1443–1451, 1995.

[27] E. Maddalena, C. da S. Moraes, G. Waltrich, and C. N. Jones, “A
neural network architecture to learn explicit MPC controllers from
data,” IFAC-PapersOnLine, vol. 53, no. 2, pp. 11 362–11 367, 2020,
21th IFAC World Congress.

[28] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger,
“Learning-based model predictive control: Toward safe learning in
control,” Annual Review of Control, Robotics, and Autonomous
Systems, vol. 3, no. 1, pp. 269–296, 2020.

[29] B. Karg and S. Lucia, “Stability and feasibility of neural network-
based controllers via output range analysis,” in IEEE Conference on
Decision and Control, 2020, pp. 4947–4954.

[30] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints and
semideﬁnite programming,” IEEE Transactions on Automatic Control,
vol. 67, no. 1, pp. 1–15, 2022.

[31] H. Dai, B. Landry, M. Pavone, and R. Tedrake, “Counter-example
guided synthesis of neural network lyapunov functions for piecewise
linear systems,” in IEEE Conference on Decision and Control, 2020,
pp. 1274–1281.

[32] S. Chen, M. Fazlyab, M. Morari, G. J. Pappas, and V. M. Preciado,
“Learning lyapunov functions for hybrid systems,” in International
Conference on Hybrid Systems: Computation and Control, 2021, pp.
1–11.

[33] F. Fabiani and P. J. Goulart, “Reliably-stabilizing piecewise-afﬁne

neural network controllers,” 2021, arXiv:2111.07183.

[34] D. Mayne, M. Seron, and S. Rakovi´c, “Robust model predictive
control of constrained linear systems with bounded disturbances,”
Automatica, vol. 41, no. 2, pp. 219–224, 2005.

[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative
style, high-performance deep learning library,” in Advances in Neural
Information Processing Systems, 2019, pp. 8024–8035.

[36] S. Diamond and S. Boyd, “CVXPY: A Python-embedded modeling
language for convex optimization,” Journal of Machine Learning
Research, vol. 17, no. 83, pp. 1–5, 2016.

[37] R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K. Mudigonda,
“A uniﬁed view of piecewise linear neural network veriﬁcation,” in
Advances in Neural Information Processing Systems, 2018.

[38] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of
neural networks with mixed integer programming,” in International
Conference on Learning Representations, 2019.

[39] E. Wong and Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” in International
Conference on Machine Learning, 2018, pp. 5286–5295.

[40] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri,
and M. Vechev, “Ai2: Safety and robustness certiﬁcation of neural
networks with abstract interpretation,” in IEEE Symposium on Security
and Privacy, 2018, pp. 3–18.

[41] Gurobi Optimization, LLC, “Gurobi optimizer reference manual,”

2022.

[42] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P.
Vielma, “Strong mixed-integer programming formulations for trained
neural networks,” Mathematical Programming, vol. 183, no. 1, pp.
3–39, Sept. 2020.

[43] D. Bertsekas, Nonlinear Programming. Athena Scientiﬁc, 1999.
[44] A. Bemporad and M. Morari, “Control of systems integrating logic,
dynamics, and constraints,” Automatica, vol. 35, no. 3, pp. 407–427,
1999.

[45] T. Marcucci and R. Tedrake, “Mixed-integer formulations for optimal
control of piecewise-afﬁne systems,” in International Conference on
Hybrid Systems: Computation and Control, 2019, p. 230–239.
[46] F. Borrelli, A. Bemporad, and M. Morari, Predictive Control for Linear

and Hybrid Systems. Cambridge University Press, 2017.

[47] M. Herceg, M. Kvasnica, C. N. Jones, and M. Morari, “Multi-
parametric toolbox 3.0,” in European Control Conference, 2013, pp.
502–510.

[48] D. Mayne, J. Rawlings, C. Rao, and P. Scokaert, “Constrained model
predictive control: Stability and optimality,” Automatica, vol. 36,
no. 6, pp. 789–814, 2000.

[49] C. N. Jones and M. Morari, “Approximate explicit MPC using bilevel
optimization,” in European Control Conference, 2009, pp. 2396–2401.
[50] D. Simon and J. L¨ofberg, “Stability analysis of model predictive con-
trollers using mixed integer linear programming,” in IEEE Conference
on Decision and Control, 2016, pp. 7270–7275.

[51] D. P. Kingma and J. Ba, “Adam: A method for

stochastic
on Learning
optimization,”
Representations,
ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds.,
2015.

International Conference

3rd

in

APPENDIX I
BOUND IMPROVEMENTS FOR PARAMETRIC QP
FORMULATION

As we have seen in section II-C, the complementarity
constraints of the KKT conditions have been formulated as
“big-M” constraints (10). In practice, choosing M too large
can lead to numerical issues; hence, ideally, we can calculate
a ﬁnite bound beforehand [44]. Also, if the parametric QP
is not the last layer, then bounds of the output y may even
be required by the next layer.

Assuming we are given or have calculated upper and lower

bounds for the input x ≤ x ≤ x, we can calculate

M z

i = max
z,λ,µeq

gi − Fiz

s.t. x ≤ Dz ≤ x,

Az = b, F z ≤ g,
P z + q + AT µeq + F T λ = 0,
λ ≥ 0,

for i = 1, . . . , nineq and

M λ

i = max
z,λ,µeq

λi

s.t. x ≤ Dz ≤ x,

Az = b, F z ≤ g,
P z + q + AT µeq + F T λ = 0,
λ ≥ 0,

(45)

(46)

for i = 1, . . . , nineq. Then we can take

M = max(max

i

M z

i , max
i

M λ

i ).

(47)

In case the optimization problems are unbounded, one may
include the MILP constraints

we are solving 2nineq of them. In practice, these MILPs are
not solved to optimality but are interrupted after a predeﬁned
timeout, and the best LP-Relaxation bound is used.

APPENDIX II
PROOFS

A. Proof of Lemma 6

Proof: Since the polytopic set X is bounded, there exists
a bounded box [t, t] ⊇ X. For every i = 1, . . . , m introduce
a continuous decision variable zi ≥ 0 as well as a binary
decision variable βi ∈ {0, 1} subject to the constraints

ti ≤ zi ≤ ti + 2tβi,

−ti ≤ zi ≤ −ti − 2t(1 − βi).

(49a)

(49b)

If ti > 0, then (49b) implies that βi = 0, and (49a) implies
that zi = ti. If ti < 0, on the other hand, then (49a) implies
that βi = 1, and (49b) implies that zi = −ti. If ti = 0,
ﬁnally, then (49) implies that zi = 0 irrespective of βi. In
any case, we thus have zi = |ti|.

In order

to express

inﬁnity norm f (t) =
the
max(z1, . . . , zm) in terms of linear constraints, we introduce
the continuous decision variables τ and ˜zi for every i =
1, . . . , m, as well as the binary decision variables ˜βi ∈
{0, 1}, i = 1, . . . , m, subject to the constraints

1 =

τ =

m
(cid:88)

i=1
m
(cid:88)

i=1

˜βi,

˜zi,

τ ≥ zi
∀i = 1, . . . , m,
0 ≤ ˜zi ≤ max(|t|, |t|) ˜βi ∀i = 1, . . . , m,
∀i = 1, . . . , m.
˜zi ≤ zi

(50a)

(50b)

(50c)

(50d)

(50e)

0 ≤ λi ≤ M βi,
0 ≤ gi − Fiz ≤ M (1 − βi),

(48a)

(48b)

again into the formulations (45) and (46). Although these
MILPs run orders of magnitude faster than the actual ver-
iﬁcation problem, it may still take considerable time since

The forcing constraints (50d) ensure that ˜zi = 0 whenever
˜βi = 0, whereas (50a) ensures that ˜βi = 1 for exactly one i.
Hence, (50b) sets τ = ˜zi for the unique i with ˜βi = 1. By
(50e), we thus have τ = ˜zi ≤ zi ≤ max(z1, . . . , zm). The
constraint (50c), ﬁnally, ensure the converse inequality τ ≥
zi ≥ max(z1, . . . , zm). Hence, (cid:107)t(cid:107)∞ is MILP representable,

and its graph formulation can be expressed as

gr(f ) =






(t, τ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∀i = 1, . . . , m,
∃β, ˜β ∈ {0, 1}m,
∃z, ˜z ∈ Rm :
t ∈ X
ti ≤ zi ≤ ti + 2tβi,

−ti ≤ zi

≤ −ti − 2t(1 − βi),

1 =

τ =

m
(cid:88)

j=1
m
(cid:88)

j=1

˜βj,

˜zj,

0 ≤ ˜zi ≤ max(|t|, |t|) ˜βi,
˜zi ≤ zi ≤ τ






.

(51)

B. Proof of Corollary 1

Proof: The proof widely parallels that of Lemma 6.
However, there is no need to introduce decision variables ˜β
and ˜z because we can directly set

τ = (cid:107)t(cid:107)1 =

m
(cid:88)

i=1

zi.

(52)

