Binary Matrix Factorisation via Column Generation

R´eka ´A. Kov´acs,1 Oktay G ¨unl ¨uk, 2 Raphael A. Hauser 1
1 University of Oxford & The Alan Turing Institute
2 Cornell University
reka.kovacs@maths.ox.ac.uk, ong5@cornell.edu, hauser@maths.ox.ac.uk

1
2
0
2

g
u
A
3

]

C
O
.
h
t
a
m

[

3
v
7
5
4
4
0
.
1
1
0
2
:
v
i
X
r
a

Abstract

Identifying discrete patterns in binary data is an important
dimensionality reduction tool in machine learning and data
mining. In this paper, we consider the problem of low-rank
binary matrix factorisation (BMF) under Boolean arithmetic.
Due to the hardness of this problem, most previous attempts
rely on heuristic techniques. We formulate the problem as a
mixed integer linear program and use a large scale optimisation
technique of column generation to solve it without the need of
heuristic pattern mining. Our approach focuses on accuracy
and on the provision of optimality guarantees. Experimental
results on real world datasets demonstrate that our proposed
method is effective at producing highly accurate factorisations
and improves on the previously available best known results
for 15 out of 24 problem instances.

1

Introduction

Low-rank matrix approximation is an essential tool for dimen-
sionality reduction in machine learning. For a given n × m
data matrix X whose rows correspond to n observations or
items, columns to m features and a ﬁxed positive integer
k, computing an optimal rank-k approximation consists of
approximately factorising X into two matrices A, B of di-
mension n×k and k ×m respectively, so that the discrepancy
between X and its rank-k approximate A · B is minimum.
The rank-k matrix A·B describes X using only k derived fea-
tures: the rows of B specify how the original features relate
to the k derived features, while the rows of A provide weights
how each observation can be (approximately) expressed as a
linear combination of the k derived features.

Many practical datasets contain observations on categor-
ical features and while classical methods such as singular
value decomposition (SVD) (Golub and Van Loan 1989)
and non-negative matrix factorisation (NMF) (Lee and Se-
ung 1999) can be used to obtain low-rank approximates for
real valued datasets, for a binary input matrix X they can-
not guarantee factor matrices A, B and their product to be
binary. Binary matrix factorisation (BMF) is an approach
to compute low-rank matrix approximations of binary ma-
trices ensuring that the factor matrices are binary as well
(Miettinen 2012). More precisely, for a given binary ma-
trix X ∈ {0, 1}n×m and a ﬁxed positive integer k, the

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

rank-k BMF problem (k-BMF) asks to ﬁnd two matrices
A ∈ {0, 1}n×k and B ∈ {0, 1}k×m such that the product of
A and B is a binary matrix denoted by Z, and the distance
between X and Z is minimum in the squared Frobenius norm.
Many variants of k-BMF exist, depending on what arithmetic
is used when the product of matrices A and B is computed.
We focus on a variant where the Boolean arithmetic is used:

X = A ◦ B ⇐⇒ xij = (cid:87)k

(cid:96)=1 ai(cid:96) ∧ b(cid:96)j,

so that 1s and 0s are interpreted as True and False, addition
corresponds to logical disjunction (∨) and multiplication to
conjunction (∧). Apart from the arithmetic of the Boolean
semi-ring, other choices include standard arithmetic over the
integers or modulo 2 arithmetic over the binary ﬁeld. We
focus on the Boolean case, in which the property of Boolean
non-linearity, 1 + 1 = 1 holds because many natural processes
follow this rule. For instance, when diagnosing patients with
a certain condition, it is only the presence or absence of a
characteristic symptom which is important, and the frequency
of the symptom does not change the diagnosis. As an exam-
ple, consider the matrix (inspired by (Miettinen et al. 2008))

X =

(cid:105)

(cid:104) 1 1 0
1 1 1
0 1 1

where rows correspond to patients and columns to symptoms,
xij = 1 indicating patient i presents symptom j. Let

X = A ◦ B =

(cid:105)

(cid:104) 1 0
1 1
0 1

◦ [ 1 1 0
0 1 1 ]

denote the rank-2 BMF of X. Factor B reveals that 2 un-
derlying diseases cause the observed symptoms, α causing
symptoms 1 and 2, and β causing 2 and 3. Factor A reveals
that patient 1 has disease α, patient 3 has β and patient 2 has
both. In contrast, the best rank-2 real approximation

X ≈

(cid:104) 1.21 0.71
1.21 0.00
1.21 −0.71

(cid:105) (cid:2) 0.00 0.71 0.50
0.71 0.00 −0.71

(cid:3)

fails to reveal a clear interpretation, and the best rank-2 NMF
(cid:105)

X ≈

(cid:104) 1.36 0.09
1.05 1.02
0.13 1.34

[ 0.80 0.58 0.01
0.00 0.57 0.81 ]

of X suggests that symptom 2 presents with lower intensity
in both α and β, an erroneous conclusion (caused by patient
2) that could not have been learned from data X which is of
“on/off” type. BMF-derived features are particularly natural
to interpret in biclustering gene expression datasets (Zhang
et al. 2007), role based access control (Lu, Vaidya, and Atluri
2008, 2014) and market basket data clustering (Li 2005).

 
 
 
 
 
 
1.1 Complexity and Related Work

(cid:96)=1 a(cid:96)b(cid:62)

The Boolean rank of a binary matrix X ∈ {0, 1}n×m is
deﬁned to be the smallest integer r for which there ex-
ist matrices A ∈ {0, 1}n×r and B ∈ {0, 1}r×m such
that X = A ◦ B, where ◦ denotes Boolean matrix mul-
tiplication deﬁned as xij = (cid:87)r
(cid:96)=1 ai(cid:96) ∧ b(cid:96)j for all i ∈
{1, . . . , n} := [n], j ∈ [m] (Kim 1982). This is equivalent
to xij = min{1, (cid:80)r
(cid:96)=1 ai(cid:96)b(cid:96)j} using standard arithmetic.
Equivalently, the Boolean rank of X is the minimum value of
r for which it is possible to factor X into the Boolean com-
bination of r rank-1 binary matrices X = (cid:87)r
(cid:96) for
a(cid:96) ∈ {0, 1}n, b(cid:96) ∈ {0, 1}m. Interpreting X as the node-node
incidence matrix of a bipartite graph G with n vertices on the
left and m vertices on the right, the problem of computing
the Boolean rank of X is in one-to-one correspondence with
ﬁnding a minimum edge covering of G by complete bipar-
tite subgraphs (bicliques)(Monson, Pullman, and Rees 1995).
Since the biclique cover problem is NP-hard (Orlin 1977) and
hard to approximate (Simon 1990), computing the Boolean
rank is hard as well. Finding an optimal k-BMF of X has
a graphic interpretation of minimizing the number of errors
in an approximate covering of G by k bicliques. Even the
computation of 1-BMF is hard (Gillis and Vavasis 2018), and
can be stated in graphic form as ﬁnding a maximum weight
biclique of Kn,m with edge weights 1 for (i, j) : xij = 1
and −1 for (i, j) : xij = 0.

Many heuristic attempts have been made to approximately
compute BMFs by focusing on recursively partitioning the
given matrix X ∈ {0, 1}n×m and computing a 1-BMF at
each step. The ﬁrst such recursive method called Proximus
(Koyut¨urk, Grama, and Ramakrishnan 2002) is used to com-
pute BMF under standard arithmetic over the integers. For 1-
BMF Proximus uses an alternating iterative heuristic applied
to a random starting point which is based on the observation
that if a ∈ {0, 1}n is given, then a vector b ∈ {0, 1}m that
minimizes the distance between X and ab(cid:62) can be computed
in O(nm) time. Since the introduction of Proximus, much
research focused on computing efﬁcient and accurate 1-BMF.
(Shen, Ji, and Ye 2009) propose an integer program (IP) for
1-BMF and several linear programming (LP) relaxations of it,
one of which leads to a 2-approximation. (Shi, Wang, and Shi
2014) provide a rounding based 2-approximation for 1-BMF
by using an observation about the vertices of the polytope
corresponding to the LP relaxation of an integer program. In
(Beckerleg and Thompson 2020) a modiﬁcation of the Prox-
imus framework is explored using the approach of (Shen, Ji,
and Ye 2009) to compute 1-BMF at each step.

k-BMF under Boolean arithmetic is explicitly introduced
in (Miettinen et al. 2006, 2008), along with a heuristic algo-
rithm called ASSO. The core of ASSO is based on an associ-
ation rule-mining approach to create matrix B ∈ {0, 1}k×m
and greedily ﬁx A ∈ {0, 1}n×k with respect to B. The prob-
lem of ﬁnding an optimal A with respect to ﬁxed B is NP-
hard (Miettinen 2008) but can be solved in O(2kkmn) time
(Miettinen et al. 2008). The association rule-mining approach
of (Miettinen et al. 2008) is further improved in (Barahona
and Goncalves 2019) by a range of iterative heuristics em-
ploying this alternative ﬁxing idea and local search. Another

approach based on an alternating style heuristic is explored
in (Zhang et al. 2007) to solve a non-linear unconstrained
formulation of k-BMF with penalty terms in the objective for
non-binary entries. (Wan et al. 2020) proposes another itera-
tive heuristic which at every iteration permutes the rows and
columns of X to create a dense submatrix in the upper right
corner which is used as a rank-1 component in the k-BMF.
In (Lu, Vaidya, and Atluri 2008, 2014) an exponential size
IP for k-BMF is introduced, which uses an explicit enumera-
tion of all possible rows for factor matrix B and correspond-
ing indicator variables. To tackle the exponential explosion,
a heuristic row generation using association rule mining and
subset enumeration is developed, but no non-heuristic method
is considered. An exact linear IP for k-BMF with polynomi-
ally many variables and constraints is presented in (Kovacs,
Gunluk, and Hauser 2017). This model uses McCormick
envelopes (McCormick 1976) to linearise quadratic terms.

1.2 Our Contribution
In this paper, we present a novel IP formulation for k-BMF
that overcomes several limitations of earlier approaches. In
particular, our formulation does not suffer from permutation
symmetry, it does not rely on heuristic pattern mining, and
it has a stronger LP relaxation than that of (Kovacs, Gunluk,
and Hauser 2017). On the other hand, our new formulation
has an exponential number of variables which we tackle using
a column generation approach that effectively searches over
this exponential space without explicit enumeration, unlike
the complete enumeration used for the exponential size model
of (Lu, Vaidya, and Atluri 2008, 2014). Our proposed solution
method is able to prove optimality for smaller datasets, while
for larger datasets it provides solutions with better accuracy
than the state-of-the-art heuristic methods. In addition, due
to the entry-wise modelling of k-BMF in our approach, we
can handle matrices with missing entries and our solutions
can be used for binary matrix completion.

The rest of the paper is organised as follows. In Section 2
we brieﬂy discuss the model of (Kovacs, Gunluk, and Hauser
2017) and its limitations. In Section 3 we introduce our inte-
ger programming formulation for k-BMF, detail a theoretical
framework based on the large scale optimisation technique
of column generation for its solution and discuss heuristics
for the arising pricing subproblems. Finally, in Section 4 we
demonstrate the practical applicability of our approach on
several real world datasets.

2 Problem Formulation
Given a binary matrix X ∈ {0, 1}n×m, and a ﬁxed integer
k (cid:28) min(n, m) we wish to ﬁnd two binary matrices A ∈
{0, 1}n×k and B ∈ {0, 1}k×m to minimise (cid:107)X − A ◦ B(cid:107)2
F ,
where (cid:107) · (cid:107)F denotes the Frobenius norm and ◦ stands for
Boolean matrix multiplication. Since X and Z := A ◦ B
are binary matrices, the squared Frobenius and entry-wise (cid:96)1
norm coincide and we can expand the objective function

(cid:107)X − Z(cid:107)2

F =

(cid:88)

(1 − zij) +

(cid:88)

zij,

(1)

(i,j)∈E

(i,j)(cid:54)∈E

where E := {(i, j) : xij = 1} is the index set of the
positive entries of X. (Kovacs, Gunluk, and Hauser 2017)

formulate the problem as an exact integer linear program
by introducing variables yi(cid:96)j for the product of ai(cid:96) and b(cid:96)j
((i, (cid:96), j) ∈ F := [n] × [k] × [m]), and using McCormick en-
velopes (McCormick 1976) to avoid the appearance of a
quadratic constraint arising from the product. McCormick
envelopes represent the product of two binary variables a and
b by a new variable y and four linear inequalities given by
M C(a, b) = {y ∈ R : a + b − 1 ≤ y, y ≤ a, y ≤ b, 0 ≤ y}.
The model of (Kovacs, Gunluk, and Hauser 2017) reads as

(IPexact) ζIP = min
a,b,y,z

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

zij (2)

(i,j)(cid:54)∈E

s.t. yi(cid:96)j ≤ zij ≤

k
(cid:88)

(cid:96)=1

yi(cid:96)j,

(i, (cid:96), j) ∈ F, (3)

yi(cid:96)j ∈ M C(ai(cid:96), b(cid:96)j),
ai(cid:96), b(cid:96)j ∈ {0, 1}, zij ≤ 1,

(i, (cid:96), j) ∈ F, (4)
(i, (cid:96), j) ∈ F. (5)

The above model is exact in the sense that its optimal so-
lutions correspond to optimal k-BMFs of X. Most general
purpose IP solvers use an enumeration framework, which
relies on bounds from the LP relaxation of the IP and con-
sequently, it is easier to solve the IP when its LP bound is
tighter. For k = 1, we have yi1j = zij for all i, j and the
LP relaxation of the model is simply the LP relaxation of
the McCormick envelopes which has a rich and well-studied
polyhedral structure (Padberg 1989). However, for k > 1,
IPexact’s LP relaxation (LPexact) only provides a trivial bound.

(cid:1) solutions.

Lemma 1. For k > 1, LPexact has optimal objective value 0
which is attained by at least (cid:0)k

2
For the proof of Lemma 1, see Appendix 5.1. Furthermore,
for k > 1 the model is highly symmetric, since AP ◦ P −1B
is an equivalent solution for any permutation matrix P . These
properties of the model make it unlikely to be solved to opti-
mality in a reasonable amount of time for a large matrix X,
though the symmetries can be partially broken by incorporat-
ing constraints (cid:80)

i ai(cid:96)1 ≥ (cid:80)

Note that constraint (3) implies 1
(cid:96)=1 yi(cid:96)j ≤ zij ≤
k
(cid:96)=1 yi(cid:96)j as a lower and upper bound on each variable zij.

(cid:80)k
Hence, the objective function may be approximated by

i ai(cid:96)2 for all (cid:96)1 < (cid:96)2.
(cid:80)k

ζIP (ρ) =

(cid:88)

(i,j)∈E

(1 − zij) + ρ

(cid:88)

k
(cid:88)

(i,j)(cid:54)∈E

(cid:96)=1

yi(cid:96)j,

(6)

where ρ is a parameter of the formulation. By setting ρ = 1
k
we underestimate the original objective, while setting ρ = 1
we overestimate. Using (6) as the objective function reduces
the number of variables and constraints in the model. Vari-
ables zij need only be declared for (i, j) ∈ E, and constraint
(3) simpliﬁes to zij ≤ (cid:80)k

(cid:96)=1 yi(cid:96)j for (i, j) ∈ E.

3 A Formulation via Column Generation
The exact model presented in the previous section relies on
polynomially many constraints and variables and constitutes
the ﬁrst approach towards obtaining k-BMF with optimality
guarantees. However, such a compact IP formulation may

be weak in the sense that its LP relaxation is a very coarse
approximation to the convex hull of integer feasible points
and an IP formulation with exponentially many variables or
constraints can have the potential to provide a tighter relax-
ation(L¨ubbecke and Desrosiers 2005). Motivated by this fact,
we introduce a new formulation with an exponential number
of variables and detail a column generation framework for its
solution.

Consider enumerating all possible rank-1 binary matrices

of size n × m and let

R = {ab(cid:62) : a ∈ {0, 1}n, b ∈ {0, 1}m, a, b (cid:54)= 0}.
The size of R is |R| = (2n −1)(2m −1) as any pair of binary
vectors a, b (cid:54)= 0 leads to a unique rank-1 matrix Y = ab(cid:62)
with Yij = 1 for {(i, j) : ai = 1, bj = 1}. Deﬁne a binary
decision variable q(cid:96) to denote if the (cid:96)-th rank-1 binary matrix
in R is included in a rank-k factorisation of X (q(cid:96) = 1),
or not (q(cid:96) = 0). Let q ∈ {0, 1}|R| be a vector that has a
component q(cid:96) for each matrix in R. We form a {0, 1}-matrix
M of dimension nm × |R| whose rows correspond to entries
of an n × m matrix, columns to rank-1 binary matrices in
R and M(i,j),(cid:96) = 1 if the (i, j)-th entry of the (cid:96)-th rank-1
binary matrix in R is 1, M(i,j),(cid:96) = 0 otherwise. We split M
horizontally into two matrices M0 and M1, so that rows of
M corresponding to a positive entry of the given matrix X
are in M1 and the rest of rows of M in M0,

M =

(cid:21)

(cid:20)M0
M1

where

M0 ∈ {0, 1}(nm−|E|)×|R|,
M1 ∈ {0, 1}|E|×|R|.

(7)

The following Master Integer Program over an exponential
number of variables is an exact model for k-BMF,

(MIPexact) ζMIP = min 1(cid:62)ξ + 1(cid:62)π

s.t. M1q + ξ ≥ 1
M0q ≤ kπ
1(cid:62)q ≤ k
ξ ≥ 0, π ∈ {0, 1}nm−|E|,
q ∈ {0, 1}|R|.

(8)
(9)
(10)

(11)

(12)

(13)

Constraint (11) ensures that at most k rank-1 matrices are
active in a factorisation. Variables ξij correspond to positive
entries of X, and are forced by constraint (9) to take value
1 and increase the objective if the (i, j)-th positive entry of
X is not covered. Similarly, variables πij correspond to zero
entries of X and are forced to take value 1 by constraint (10)
if the (i, j)-th zero entry of X is erroneously covered in a
factorisation. One of the imminent advantages of MIPexact is
using indicator variables directly for rank-1 matrices instead
of the entries of factor matrices A, B, hence no permutation
symmetry arises. In addition, for all k not exceeding a certain
number that depends on X, the LP relaxation of MIPexact
(MLPexact) has strictly positive optimal objective value.
Lemma 2. Let i(X) be the isolation number of X. For all
k < i(X), we have 0 < ζMLP.

For the deﬁnition of isolation number and the proof of
Lemma 2 see Appendix 5.2. Similarly to the polynomial size
exact model IPexact in the previous section, we consider a

modiﬁcation of MIPexact with an objective that is analogous
to the one in Equation (6),

(MIP(ρ)) zMIP(ρ) = min 1(cid:62)ξ + ρ 1(cid:62)M0q
s.t. (9), (11) hold and

(14)

(15)

ξ ≥ 0, q ∈ {0, 1}|R|.

The objective of MIP(ρ) simply counts the number of posi-
tive entries of X that are not covered by any of the k rank-1
matrices chosen, plus the number of times zero valued en-
tries of X are erroneously covered weighted by parameter
ρ. Depending on the selection of parameter ρ, MIP(ρ) pro-
vides a lower or upper bound on MIPexact. We denote the LP
relaxation of MIP(ρ) by MLP(ρ).
Lemma 3. For ρ = 1
LP relaxations MLPexact and MLP( 1

k , the optimal objective values of the
k ) coincide.

For a short proof of Lemma 3 see Appendix 5.3. Com-
bining Lemmas 1, 2 and 3 we obtain the following relations
between formulations IPexact, MIPexact, MIP(ρ) and their LP
relaxations for k > 1,

k ) ≤ ζIP = ζMIP ≤ zMIP(1),

zMIP( 1
0 = ζLP ≤ zMLP( 1

k ) = ζMLP ≤ zMLP(1).

(16)

(17)

Let p be the dual variable vector associated to constraints (9)
and µ be the dual variable to constraint (11). Then the dual
of MLP(ρ) is given by

(MDP(ρ)) zMDP(ρ) = max 1(cid:62)p − kµ

s.t. M (cid:62)

1 p − µ1 ≤ ρ M (cid:62)
µ ≥ 0, p ∈ [0, 1]|E|.

0 1,

(18)

(19)

(20)

Due to the number of variables in the formulation, it is not
practical to solve MIP(ρ) or its LP relaxation MLP(ρ) explic-
itly. Column generation (CG) is a technique to solve large
LPs by iteratively generating only the variables which have
the potential to improve the objective function (Barnhart et al.
1998). The CG procedure is initialised by explicitly solv-
ing a Restricted Master LP which has a small subset of the
variables in MLP(ρ). The next step is to identify a missing
variable with a negative reduced cost to be added to this Re-
stricted MLP(ρ). To avoid explicitly considering all missing
variables, a pricing problem is formulated and solved. The
solution of the pricing problem either returns a variable with
negative reduced cost and the procedure is iterated; or proves
that no such variable exists and hence the solution of the
Restricted MLP(ρ) is optimal for the complete formulation
MLP(ρ).

We use CG to solve MLP(ρ) by considering a sequence
(t = 1, 2, ...) of Restricted MLP(ρ)’s with constraint matrix
M (t) being a subset of columns of M , where each column
y ∈ {0, 1}nm of M corresponds to a ﬂattened rank-1 binary
matrix ab(cid:62) according to Equation (7). The constraint matrix
of the ﬁrst Restricted MLP(ρ) may be left empty or can be
warm started by identifying a few rank-1 matrices in R, say
from a heuristic solution. Upon successful solution of the
t-th Restricted MLP(ρ), we obtain a vector of dual variables
[p∗, µ∗] ≥ 0 optimal for the t-th Restricted MLP(ρ). To

identify a missing column of M that has a negative reduced
cost, we solve the following pricing problem (PP):
(PP) ω(p∗) = max
a,b,y

p∗
ijyij − ρ

(cid:88)

(cid:88)

yij

(i,j)∈E

(i,j)(cid:54)∈E

s.t. yij ∈ M C(ai, bj), ai, bj ∈ {0, 1}, i ∈ [n], j ∈ [m].
The objective of PP depends on the current dual solution
[p∗, µ∗] and its optimal solution corresponds to a rank-1 bi-
nary matrix ab(cid:62) whose corresponding variable q(cid:96) in MLP(ρ)
has the smallest reduced cost. If ω(p∗) ≤ µ∗, then the
dual variables [p∗, µ∗] of the Restricted MLP(ρ) are feasi-
ble for MDP(ρ) and hence the current solution of the Re-
stricted MLP(ρ) is optimal for the full formulation MLP(ρ).
If ω(p∗) > µ∗, then the variable q(cid:96) associated with the rank-1
binary matrix ab(cid:62) is added to the Restricted MLP(ρ) and
the procedure is iterated. CG optimally terminates if at some
iteration we have ω(p∗) ≤ µ∗.

To apply the CG approach above to MLPexact only a small
modiﬁcation needs to be made. The Restricted MLPexact pro-
vides dual variables for constraints (10) which are used in
the objective of PP for coefﬁcients of yij (i, j) (cid:54)∈ E. Note
however, that CG cannot be used to solve a modiﬁcation of
MLPexact in which constraints (10) are replaced by exponen-
tially many constraints (M0)(cid:96) q(cid:96) ≤ π for (cid:96) ∈ [|R|] where
(M0)(cid:96) denotes the (cid:96)-th column of M0, see Appendix 5.4.

If the optimal solution of MLP(ρ) is integral, then it is also
optimal for MIP(ρ). However, if it is fractional, then it only
provides a lower bound on the optimal value of MIP(ρ). In
this case we obtain an integer feasible solution by simply
adding integrality constraints on the variables of the ﬁnal
Restricted MLP(ρ) and solving it as an integer program. If
ρ = 1, the optimal solution of this integer program is optimal
for MIP(1) if the objective of the Restricted MIP(1) and the
ceiling of the Restricted MLP(1) agree. To solve the MIP(ρ)
to optimality in all cases, one needs to embed CG into branch-
and-bound which we do not do. However, note that even if
the CG procedure is terminated prematurely, one can still
obtain a lower bound on MLP(ρ) and MIP(ρ) as follows. Let
the objective value of any of the Restricted MLP(ρ)’s be

0 q∗ = 1(cid:62)p∗ − kµ∗

zRMLP = 1(cid:62)ξ∗ + ρ1(cid:62)M ∗

(21)
where [ξ∗, q∗] is the solution of the Restricted MLP(ρ),
[p∗, µ∗] is the solution of the dual of the Restricted MLP(ρ)
and 1(cid:62)M ∗
0 is the objective coefﬁcient of columns in the Re-
stricted MLP(ρ). Assume that we solve PP to optimality and
we obtain a column y for which the reduced cost is negative,
ω(p∗) > µ∗. In this case, we can construct a feasible solution
to MDP(ρ) by setting p := p∗ and µ := ω(p∗) and get the
following bound on the optimal value zMLP(ρ) of MLP(ρ),
zMLP(ρ) ≥ 1(cid:62)p∗−k ω(p∗) = zRMLP−k(ω(p∗)−µ∗). (22)
If we do not have the optimal solution to PP but have an
upper bound ¯ω(p∗) on it, ω(p∗) can be replaced by ¯ω(p∗) in
equation (22) and the bound on MLP(ρ) still holds. Further-
more, this lower bound on MLP(ρ) provides a valid lower
bound on MIP(ρ). Consequently, our approach always pro-
duces a bound on the optimality gap of the ﬁnal solution
which heuristic methods cannot do. We have, however, no a
priory (theoretical) bound on this gap.

3.1 The Pricing Problem
The efﬁciency of the CG procedure described above greatly
depends on solving PP efﬁciently. In standard form PP can
be written as a bipartite binary quadratic program (BBQP)

(PP) ω(p∗) =

max
a∈{0,1}n,b∈{0,1}m

a(cid:62)Hb

(23)

for H an n × m matrix with hij = p∗
ij ∈ [0, 1] for (i, j) ∈ E
and hij = −ρ for (i, j) (cid:54)∈ E. BBQP is NP-hard in general
as it includes the maximum edge biclique problem (Peeters
2003), hence for large X it may take too long to solve PP
to optimality at each iteration. To speed up computations,
the IP formulation of PP may be improved by eliminating
redundant constraints. The McCormick envelopes set two
lower and two upper bounds on yij. Due to the objective
function it is possible to declare the lower (upper) bounds
yij for only (i, j) (cid:54)∈ E ((i, j) ∈ E) without changing the
optimum.

If a heuristic approach to PP provides a solution with
negative reduced cost, then it is valid to add this heuristic
solution as a column to the next Restricted MLP(ρ). Most
heuristic algorithms that are available for BBQP build on the
idea that the optimal a ∈ {0, 1}n with respect to a ﬁxed b ∈
{0, 1}m can be computed in O(nm) time and this procedure
can be iterated by alternatively ﬁxing a and b. (Karapetyan
and Punnen 2013) present several local search heuristics for
BBQP along with a simple greedy algorithm. Below we detail
this greedy algorithm and introduce some variants of it which
we use in the next section to provide a warm start to PP at
every iteration of the CG procedure.

The greedy algorithm of (Karapetyan and Punnen 2013)
aims to set entries of a and b to 1 which correspond to
rows and columns of H with the largest positive weights. In
the ﬁrst phase of the algorithm, the row indices i of H are
put in decreasing order according to their sum of positive
entries, so γ+
j=1 max(0, hij).
Then sequentially according to this ordering, ai is set to 1 if
(cid:80)m
(cid:96)=1 a(cid:96)h(cid:96)j +
hij) and 0 otherwise. In the second phase, bj is set to 1
if (a(cid:62)H)j > 0, 0 otherwise. The precise outline of the
algorithm is given in Appendix 5.5.

(cid:96)=1 a(cid:96)h(cid:96)j) < (cid:80)m

j=1 max(0, (cid:80)i−1

j=1 max(0, (cid:80)i−1

i+1 where γ+

i ≥ γ+

:= (cid:80)m

i

There are many variants of the greedy algorithm one can
explore. First, the solution greatly depends on the ordering of
i’s in the ﬁrst phase. If for some i1 (cid:54)= i2 we have γ+
= γ+
,
i1
i2
comparing the sum of negative entries of rows i1 and i2 can
put more “inﬂuential” rows of H ahead in the ordering. Let
us call this ordering the revised ordering and the one which
only compares the positive sums as the original ordering.
Another option is to use a completely random order of i’s or
to apply a small perturbation to sums γ+
to get a perturbed
i
version of the revised or original ordering. None of the above
ordering strategies clearly dominates the others in all cases
but they are fast to compute hence one can evaluate all ﬁve
ordering strategies (original, revised, original perturbed, re-
vised perturbed, random) and pick the best one. Second, the
algorithm as presented above ﬁrst ﬁxes a and then b. Chang-
ing the order of ﬁxing a and b can yield a different result
hence it is best to try for both H and H (cid:62). In general, it is

recommended to start the ﬁrst phase on the smaller dimen-
sion. Third, the solution from the greedy algorithm may be
improved by computing the optimal a with respect to ﬁxed
b. This idea then can be used to ﬁx a and b in an alternating
fashion and stop when no changes occur in either.

4 Experiments
The CG approach introduced in the previous section provides
a theoretical framework for computing k-BMF with optimal-
ity guarantees. In this section we present some experimental
results with CG to demonstrate the practical applicability of
our approach on eight real world categorical datasets that
were downloaded from online repositories (Dua and Graff
2017), (Krebs 2008). Table 1 shows a short summary of the
eight datasets used, details on converting categorical columns
into binary and missing value treatment can be found in Ap-
pendix 5.8. Table 1 also shows the value of the isolation
number i(X) for each dataset, which provides a lower bound
on the Boolean rank (Monson, Pullman, and Rees 1995).

zoo tumor hepat heart

lymp audio apb votes

101
n
17
m
i(X)
16
%1s 44.3

339
24
24
24.3

155
38
30

148
242
44
22
22
42
47.2 34.4 29.0

226 105
94 105
90 104

434
32
30
11.3 8.0 47.3

Table 1: Summary of binary real world datasets

Since the efﬁciency of CG greatly depends on the speed
of generating columns, let us illustrate the speed-up gained
by using heuristic pricing. At each iteration of CG, 30 vari-
ants of the greedy heuristic are computed to obtain an initial
feasible solution to PP. The 30 variants of the greedy algo-
rithm use the original and revised ordering, their transpose
and perturbed version and 22 random orderings. All greedy
solutions are improved by the alternating heuristic until no
further improvement is found. Under exact pricing, the best
heuristic solution is used as a warm start and PP is solved to
optimality at each iteration using (CPLEX Optimization). In
simple heuristic (heur) pricing, if the best heuristic solution
to PP has negative reduced cost, ωheur(p∗) > µ∗, then the
heuristic column is added to the next Restricted MLP(ρ). If at
some iteration, the best heuristic column does not have nega-
tive reduced cost, CPLEX is used to solve PP to optimality
for that iteration. The multiple heuristic (heur multi) pricing
is a slight modiﬁcation of the simple heuristic strategy, in
which at each iteration all columns with negative reduced
cost are added to the next Restricted MLP(ρ).

zoo

tumor

k MIP(1) KGH17 LVA08 MIP(1) KGH17 LVA08

2
5
10

0.0
0.0
3.0

0.0
59.2
95.8

100.0
100.0
100.0

0.9
9.3
28.4

40.8
98.0
100.0

*
*
*

Table 2: % optimality gap after 20 mins under objective (6)

k

2

5

10

k ) 206.5

MLP( 1
MLP(1) 272
MIP(1)
MLP( 1
k ) 42.8
MLP(1) 127
MIP(1)
MLP( 1
k ) 4.8
MLP(1) 38.8
MIP(1)

40

zoo

tumor hepat

heart

lymp

audio

apb

1178.9 978.7
1409.8 1384

1256.5 709
882.9 917.2
776
1499
1185 1188.8
776
1384(1382) 1185 1197(1184) 1499

272(271) 1411

127(125) 1029

463.9
1019.3 1041.1

333.1

1228

142.5
734.8
910

192.8
575.5
579

291.0 366.7
914.0
736
997(991)
736

102.3 165.1
653.2
419
737(732)
419

654.2
433.5
1159.3 683.0
1176

684(683) 2277(2274)

351.4
867.2
893

166.8
574.2
577(572) 1566(1549)

307.9
1409.5

votes

1953
2926
2926

715.5
2135.5

Table 3: Primal objective values of MLP(1), MLP( 1

k ), MIP(1) after 20 mins of CG

Figure 1 indicates the differences between pricing strate-
gies when solving MLP(1) via CG for k = 5, 10 on the zoo
dataset. The primal objective value of MLP(1) (decreasing
curve) and the value of the dual bound (increasing curve)
computed using the formula in equation (22) are plotted
against time. Sharp increases in the dual bound for heuristic
pricing strategies correspond to iterations in which CPLEX
was used to solve PP, as for the evaluation of the dual bound
on MLP(1) we need a strong upper bound on ω(p∗) which
heuristic solutions do not provide. While we observe a tail-
ing off effect (L¨ubbecke and Desrosiers 2005) on all three
curves, both heuristic pricing strategies provide a signiﬁcant
speed-up from exact pricing, with adding multiple columns
at each iteration being the fastest.

In Table 2 we present computational results comparing

Figure 1: Comparison of pricing strategies for solving
MLP(1) on the zoo dataset

best integer

the optimality gap (100 × best integer−best bound
) of MIP(1), the
compact formulation of (Kovacs, Gunluk, and Hauser 2017)
(KGH17) and the exponential formulation of (Lu, Vaidya,
and Atluri 2008) (LVA08) under objective (6) using a 20 mins
time budget. See Appendix 5.7 for the precise statement of
formulations KGH17 and LVA08 under objective (6). Read-
ing in the full exponential size model LVA08 using 16 GB
memory is not possible for datasets other than zoo. Table
2 shows that different formulations and algorithms to solve
them make a difference in practice: our novel exponential
formulation MIP(1) combined with an effective computa-
tional optimization approach (CG) produces solutions with
smaller optimality gap than the compact formulation as it
scales better and it has a stronger LP relaxation.

In order for CG to terminate with a certiﬁcate of optimality,
at least one pricing problem has to be solved to optimality.
Unfortunately for the larger datasets this cannot be achieved
in under 20 mins. Therefore, for datasets other than zoo, we
change the multiple heuristic pricing strategy as follows: We
impose an overall time limit of 20 mins on the CG process
and use the barrier method in CPLEX as the LP solver for
the Restricted MLP(ρ) at each iteration. In order to maximise
the diversity of columns added at each iteration, we choose at
most two columns with negative reduced cost that are closest
to being mutually orthogonal. If CPLEX has to be used to
improve the heuristic pricing solution, we do not solve PP
to optimality but abort CPLEX if a column with negative
reduced cost has been found. While these modiﬁcations result
in a speed-up, they reduce the chance of obtaining a strong
dual bound. In case a strong dual bound is desired, we may
continue applying CG iterations with exact pricing after the
20 mins of heuristic pricing have run their course.

In our next experiment, we explore the differences between
formulations MLP( 1
k ), MLP(1) and MIP(1). We warm start
CG by identifying a few heuristic columns using the code
of (Barahona and Goncalves 2019) and a new fast heuristic
(k-greedy) which sequentially computes k rank-1 binary ma-
trices via the greedy algorithm for BBQP starting with the
coefﬁcient matrix H = 2X − 1 and then setting entries of H
to zero that correspond to entries of X that are covered. For
the precise outline of k-greedy, see Appendix 5.6.

Table 3 shows the primal objective values of MLP(1) and
k ) with heuristic pricing using a time limit of 20 mins,

MLP( 1

CG
IPexact
ASSO++
k-greedy
pymf
ASSO
NMF
MEBF

CG
IPexact
ASSO++
k-greedy
pymf
ASSO
NMF
MEBF

CG
IPexact
ASSO++
k-greedy
pymf
ASSO
NMF
MEBF

zoo

271
271
276
325
276
367
291
348

125
133
133
233
142
354
163
173

40
41
55
184
96
354
153
122

tumor

hepat

1411
1408
1437
1422
1472
1465
1626
1487

1029
1055
1055
1055
1126
1092
1207
1245

579
583
583
675
703
587
826
990

1382
1391
1397
1483
1418
1724
1596
1599

1228
1228
1228
1306
1301
1724
1337
1439

910
902
910
1088
1186
1724
1337
1328

heart

1185
1187
1187
1204
1241
1251
1254
1289

736
738
738
748
835
887
995
929

419
419
419
565
581
694
995
777

lymp

audio

1184
1180
1202
1201
1228
1352
1366
1401

991
1029
1039
1063
1062
1352
1158
1245

730
805
812
819
987
1352
1143
1004

1499
1499
1503
1499
1510
1505
2253
1779

1176
1211
1211
1211
1245
1505
1565
1672

893
919
922
976
1106
1505
1407
1450

apb

776
776
776
776
794
778
809
812

683
690
694
690
730
719
762
730

572
590
591
611
602
661
689
662

votes

2926
2926
2926
2929
2975
2946
3069
3268

2272
2293
2302
2310
2517
2503
2526
2832

1527
1573
1573
1897
2389
2503
2481
2460

k=2

k=5

k=10

Table 4: Factorisation errors in (cid:107) · (cid:107)2

F for eight methods for k-BMF

and the objective value of MIP(1) solved on the columns gen-
erated by MLP(1). If the error measured in (cid:107) · (cid:107)2
F differs from
the objective of MIP(1), the former is shown in parenthesis.
It is interesting to observe that MLP(1) has a tendency to
produce near integral solutions and that the objective value of
MIP(1) often coincides with the error measured in (cid:107) · (cid:107)2
F . We
note that once a master LP formulation is used to generate
columns, any of the MIP models could be used to obtain
an integer feasible solution. In experiments, we found that
formulation MIP(ρ) is solved much faster than MIPexact and
that setting ρ to 1 or 0.95 provides the best integer solutions.
We compare the CG approach against the most widely used
k-BMF heuristics and the exact model IPexact.The heuristic al-
gorithms we evaluate include the ASSO algorithm (Miettinen
et al. 2006, 2008), the alternating iterative local search algo-
rithm (ASSO++) of (Barahona and Goncalves 2019) which
uses ASSO as a starting point, algorithm k-greedy detailed
in Appendix 5.6, the penalty objective formulation (pymf)
of (Zhang et al. 2007) via the implementation of (Schinnerl
2017) and the permutation-based heuristic (MEBF) (Wan
et al. 2020). We also evaluate IPexact with a time limit of 20
mins and provide the heuristic solutions of ASSO++ and k-
greedy as a warm start to it. In addition, we compute rank-k
NMF and binarise it with a threshold of 0.5. The exact de-
tails and parameters used in the computations can be found
in Appendix 5.10.1 Our CG approach (CG) results are ob-

1Appendix is available at arxiv.org/abs/2011.04457.

tained by generating columns for 20 mins using formulation
MLP(1) with a warm start of initial columns obtained from
ASSO++ and k-greedy, then solving MIP(ρ) for ρ set to 1
and 0.95 over the generated columns and picking the best.
Table 4 shows the factorisation error in (cid:107) · (cid:107)2
F after evaluating
the above described methods on all datasets for k = 2, 5, 10.
The best result for each instance is indicated in boldface. We
observe that CG provides the strictly smallest error for 15 out
of 24 cases.

5 Conclusion
In this paper, we studied the rank-k binary matrix factori-
sation problem under Boolean arithmetic. We introduced a
new integer programming formulation and detailed a method
using column generation for its solution. Our experiments
indicate that our method using 20 mins time budget is pro-
ducing more accurate solutions than most heuristics available
in the literature and is able to prove optimality for smaller
datasets. In certain critical applications such as medicine,
spending 20 minutes to obtain a higher accuracy factorisation
with a bound on the optimality gap can be easily justiﬁed. In
addition, solving BMF to near optimality via our proposed
method paves the way to more robustly benchmark heuristics
for k-BMF. Future directions that could be explored are re-
lated to designing more accurate heuristics and faster exact
algorithms for the pricing problem. In addition, a full branch-
and-price algorithm implementation would be beneﬁcial once
the pricing problems are solved more efﬁciently.

Acknowledgements
During the completion of this work R. ´A.K was supported
by The Alan Turing Institute and The Ofﬁce for National
Statistics.

References

Local
Barahona, F.;
Factor-
Binary Matrix
Search Algorithms
ization.
https://github.com/IBM/binary-matrix-
factorization/blob/master/code. Last accessed on 2020-03-30.

and Goncalves,
for

J. 2019.

Barnhart, C.; Johnson, E. L.; Nemhauser, G. L.; Savelsbergh,
M. W. P.; and Vance, P. H. 1998. Branch-and-Price: Column
Generation for Solving Huge Integer Programs. Operations
Research 46(3): 316–329.

Beckerleg, M.; and Thompson, A. 2020. A divide-and-
conquer algorithm for binary matrix completion. Linear
Algebra and its Applications 601: 113–133.

Cios, K. J.; and Kurgan, L. A. 2001. UCI Machine Learning
Repository: SPECT Heart Data. URL https://archive.ics.uci.
edu/ml/datasets/spect+heart. Last accessed on 2020-06-11.

CPLEX Optimization. 2018. Using the CPLEX Callable
Library, V.12.8. CPLEX Optimization, Inc., Incline Village,
NV.

Kovacs, R. A.; Gunluk, O.; and Hauser, R. A. 2017. Low-
Rank Boolean Matrix Approximation by Integer Program-
In NIPS 2017, Optimization for Machine Learn-
ming.
ing Workshop. https://opt-ml.org/papers/OPT2017 paper 34.
pdf.

Koyut¨urk, M.; Grama, A.; and Ramakrishnan, N. 2002. Al-
gebraic Techniques for Analysis of Large Discrete-Valued
Datasets. In Proceedings of the 6th European Conference on
Principles of Data Mining and Knowledge Discovery, PKDD
’02, 311–324. London, UK, UK: Springer-Verlag.

Krebs, V. 2008. Amazon Political Books. URL http://moreno.
ss.uci.edu/data.html#books. Last accessed on 2020-06-11.

Lee, D. D.; and Seung, H. S. 1999. Learning the parts of ob-
jects by non-negative matrix factorization. Nature 401(6755):
788–791.

Li, T. 2005. A General Model for Clustering Binary Data. In
Proceedings of the 11th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, KDD ’05.
New York, NY, USA: Association for Computing Machinery.

Lu, H.; Vaidya, J.; and Atluri, V. 2008. Optimal Boolean
Matrix Decomposition: Application to Role Engineering. In
Proceedings of the 2008 IEEE 24th International Conference
on Data Engineering, ICDE ’08, 297–306. Washington, DC,
USA: IEEE Computer Society.

Dua, D.; and Graff, C. 2017. UCI Machine Learning Repos-
itory. URL http://archive.ics.uci.edu/ml. Last accessed on
2020-06-11.

Lu, H.; Vaidya, J.; and Atluri, V. 2014. An optimization
framework for role mining. Journal of Computer Security
22(1): 1 – 31.

Forsyth, R. 1990. UCI Machine Learning Repository: Zoo
Data Set. URL http://archive.ics.uci.edu/ml/datasets/Zoo.
Last accessed on 2020-06-11.

Gillis, N.; and Vavasis, S. A. 2018. On the Complexity of
Robust PCA and l1-Norm Low-Rank Matrix Approximation.
Mathematics of Operations Research 43(4): 1072–1084.

Golub, G.; and Van Loan, C. 1989. Matrix Computations.
Baltimore: Johns Hopkins University Press, 2nd edition.

Gong, G. 1988. UCI Machine Learning Repository: Hep-
atitis Data Set. URL https://archive.ics.uci.edu/ml/datasets/
Hepatitis. Last accessed on 2020-06-11.

Karapetyan, D.; and Punnen, A. P. 2013. Heuristic algorithms
for the bipartite unconstrained 0-1 quadratic programming
problem. arXiv 1210.3684.

Kim, K. 1982. Boolean Matrix Theory and Applications.
Monographs and textbooks in pure and applied mathematics.
Dekker.

Kononenko, I.; and Cestnik, B. 1988a. UCI Mach. Learn.
Rep.: Primary Tumor Domain. URL https://archive.ics.uci.
edu/ml/datasets/Primary+Tumor. Last accessed on 2020-06-
11.

Kononenko, I.; and Cestnik, B. 1988b. UCI Machine Learn-
ing Repository: Lymphography Data Set. URL https://archive.
ics.uci.edu/ml/datasets/Lymphography. Last accessed on
2020-06-11.

L¨ubbecke, M. E.; and Desrosiers, J. 2005. Selected Topics in
Column Generation. Operations Research 53(6): 1007–1023.

McCormick, G. P. 1976. Computability of Global Solutions
to Factorable Nonconvex Programs: Part I – Convex Under-
estimating Problems. Math. Program. 10(1): 147–175.

Miettinen, P. 2008. On the Positive–Negative Partial Set
Cover Problem. Inf. Process. Lett. 108(4): 219–221.

Miettinen, P. 2012. Binary Matrix Factorisations Tutorial @
ECML PKDD 2012. URL https://people.mpi-inf.mpg.de/
∼pmiettin/bmf tutorial/tutorial.pdf. Last accessed on 2021-
03-11.

Miettinen, P.; Mielik¨ainen, T.; Gionis, A.; Das, G.; and Man-
nila, H. 2006. The Discrete Basis Problem. In Knowledge
Discovery in Databases: PKDD 2006, 335–346. Berlin, Hei-
delberg: Springer Berlin Heidelberg.

Miettinen, P.; Mielik¨ainen, T.; Gionis, A.; Das, G.; and Man-
nila, H. 2008. The Discrete Basis Problem. IEEE Trans. on
Knowl. and Data Eng. 20(10): 1348–1362.

Monson, S. D.; Pullman, N. J.; and Rees, R. 1995. A Sur-
vey of Clique and Biclique Coverings and Factorizations of
(0,1)–Matrices. Bulletin – Institute of Combinatorics and its
Applications 14: 17–86.

Orlin, J. 1977. Contentment in graph theory: Covering graphs
Indagationes Mathematicae (Proceedings)
with cliques.
80(5): 406 – 424.

Padberg, M. 1989. The Boolean Quadric Polytope: Some
Characteristics, Facets and Relatives. Math. Program. 45(1):
139–172.

Peeters, R. 2003. The maximum edge biclique problem is
NP-complete. Discrete Applied Mathematics 131(3): 651 –
654.

Quinlan, R. 1992. UCI Machine Learning Repository: Au-
diology (Standardized) Data Set. URL http://archive.ics.uci.
edu/ml/datasets/audiology+(standardized). Last accessed on
2020-06-11.

Schinnerl, C. 2017. PyMF - Python Matrix Factorization
Module. URL https://github.com/ChrisSchinnerl/pymf3. Last
accessed on 2021-03-11.

Schlimmer, J. 1987. UCI Machine Learning Repository: 1984
US Cong. Voting Records Database. URL https://archive.
ics.uci.edu/ml/datasets/Congressional+Voting+Records. Last
accessed on 2020-06-11.

Shen, B.-H.; Ji, S.; and Ye, J. 2009. Mining Discrete Patterns
via Binary Matrix Factorization. In Proceedings of the 15th
ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, KDD ’09, 757–766. New York, NY,
USA: ACM.

Shi, Z.; Wang, L.; and Shi, L. 2014. Approximation method
to rank-one binary matrix factorization. In 2014 IEEE Inter-
national Conference on Automation Science and Engineering
(CASE), 800–805.
Simon, H. U. 1990. On Approximate Solutions for Combi-
natorial Optimization Problems. SIAM J. Discrete Math. 3:
294–310.

Wan, C.; Chang, W.; Zhao, T.; Li, M.; Cao, S.; and Zhang, C.
2020. Fast and Efﬁcient Boolean Matrix Factorization by Ge-
ometric Segmentation. Proceedings of the AAAI Conference
on Artiﬁcial Intelligence 34(04): 6086–6093.
Zhang, Z.; Li, T.; Ding, C.; and Zhang, X. 2007. Binary
Matrix Factorization with Applications. In Proceedings of
the 2007 Seventh IEEE International Conference on Data
Mining, ICDM ’07, 391–400. USA: IEEE Computer Society.

Appendix

(cid:1) solutions.

(i,j)∈E(1 − zij) + (cid:80)

5.1 The strength of the LP relaxation of IPexact
Lemma 1. For k > 1, the LP relaxation of IPexact (LPexact)
has optimal objective value 0 which is attained by at least
(cid:0)k
2
Proof. Observe that the objective function of LPexact satisﬁes
0 ≤ (cid:80)
(i,j)(cid:54)∈E zij as constraints (3),
the McCormick envelopes and ai(cid:96), b(cid:96)j ∈ [0, 1] imply zij ∈
[0, 1]. Let us construct a feasible solution to LPexact which
attains this bound. Let ai(cid:96) = b(cid:96)j = 1
2 for all i ∈ [n], j ∈
[m], (cid:96) ∈ [k]. The McCormick envelopes then are equivalent
2 , 1
to M C( 1
2 , y ≤
2 , 0 ≤ y} = [0, 1
1
2 ] hence we may choose the value of yi(cid:96)j ∈
M C( 1
2 , 1
2 ) depending on the objective coefﬁcient of indices
(i, j). For (i, j) ∈ E the objective function is maximising zij
hence we set yi(cid:96)j = 1
(cid:96)=1 yi(cid:96)j on
zij becomes greater than equal to 1 and zij can take value 1.
For (i, j) (cid:54)∈ E the objective function is minimising zij hence
we set yi(cid:96)j = 0 so that the lower bounds yi(cid:96)j ≤ zij evaluate
to 0 and zij can take value 0. Therefore, the following setting
of the variables shows the lower bound of 0 on the objective
function is attained,

2 so that the upper bound (cid:80)k

2 ) = {y ∈ R : 1

2 − 1 ≤ y, y ≤ 1

2 + 1

ai(cid:96) =

yi(cid:96)j =

1
2
1
2

, i ∈ [n], (cid:96) ∈ [k];

b(cid:96)j =

1
2

, (cid:96) ∈ [k], j ∈ [m];

, (i, j) ∈ E, (cid:96) ∈ [k];

yi(cid:96)j = 0, (i, j) (cid:54)∈ E, (cid:96) ∈ [k];

zij = 1, (i, j) ∈ E;

zij = 0, (i, j) (cid:54)∈ E.

Furthermore, for all (i, j) ∈ E it is enough to set yi(cid:96)1j =
yi(cid:96)2j = 1
2 for only two indices (cid:96)1, (cid:96)2 ∈ [k] since this already
achieves the upper bound zij ≤ 1 = (cid:80)k
(cid:96)=1 yi(cid:96)j. Hence, there
is at least (cid:0)k
(cid:1) different solutions of LPexact with objective
value 0.

2

5.2 The strength of the LP relaxation of MIPexact
For our proof we will need a deﬁnition from the theory of
binary matrices.
Deﬁnition 2. (Monson, Pullman, and Rees 1995, Section
2.3) Let X be a binary matrix. A set S ⊆ E = {(i, j) :
xij = 1} is said to be an isolated set of ones if whenever
(i1, j1), (i2, j2) are two distinct members of S then
1. i1 (cid:54)= i2, j1 (cid:54)= j2 and
2. xi1,j2 xi2,j1 = 0.
The size of the largest cardinality isolated set of ones in X is
denoted by i(X) and is called the isolation number of X.

Observe that requirement (1.) implies that an isolated set of
ones can contain the index corresponding to at most one entry
in each column and row of X. Hence i(X) ≤ min(n, m).
Requirement (2.) implies that if (i1, j1), (i2, j2) are members
of an isolated set of ones then at least one of the entries
xi1,j2, xi2,j1 is zero, hence members of an isolated set of
ones cannot be contained in a common rank-1 submatrix of
X. Therefore if the largest cardinality isolated set of ones has
i(X) elements, to cover all 1s of X we need at least i(X)

many rank-1 binary matrices in a factorisation of X, so i(X)
provides a lower bound on the Boolean rank of X.
Lemma 2. Let X be a binary matrix with isolation num-
ber i(X). Then for rank-k binary matrix factorisation of X
with k < i(X), the LP-relaxation of MIPexact has non-zero
optimal objective value.

Proof. Let k be a ﬁxed positive integer and let X be a binary
matrix with isolation number i(X) > k. For a contradiction,
assume that MLPexact has objective value zero, ζMLP = 0.
(1.) Now, if ζMLP = 0 we must have M0q = 0 = π in
constraint (10) which implies that none of the rank-1 binary
matrices with q(cid:96) > 0 cover zero entries of X. In other words,
all the rank-1 binary matrices active are submatrices of X.
(2.) Now, if ζMLP = 0 we must also have ξ = 0 which
implies M1q ≥ 1 in constraint (9) for some q which
may be fractional but satisﬁes 1(cid:62)q ≤ k. Let S :=
{(i1, j1), . . . , (ir, jr), . . . , (i|S|, j|S|)} ⊆ E be an isolated
set of ones in X of cardinality i(X) = |S|. Since members
of S cannot be contained in a common rank-1 submatrix of X,
all columns M(cid:96) corresponding to rank-1 binary submatrices
of X for entries (ir, jr) ∈ S satisfy

M(ir,jr),(cid:96) = 1 ⇒ M(i,j),(cid:96) = 0 ∀(i, j) (cid:54)= (ir, jr), (i, j) ∈ S.

Therefore, we can partition the active rank-1 binary matrices
into i(X) + 1 groups Gr,

Gr = {(cid:96) : q(cid:96) > 0 and M(ir,jr),(cid:96) = 1}

(24)

for r = 1, . . . , i(X);

Gi(X)+1 = {(cid:96) : q(cid:96) > 0 and M(ir,jr),(cid:96) = 0 ∀(ir, jr) ∈ S}.
(25)

While Gi(X)+1 may be empty, Gr’s for r = 1, . . . , i(X) are
not empty because we know that for all (i, j) ∈ E we have
(cid:80)
(cid:96) M(i,j),(cid:96) q(cid:96) ≥ 1 and S ⊆ E. Hence for all (ir, jr) ∈ S
we have (cid:80)
q(cid:96) ≥ 1 which implies the contradiction
1(cid:62)q ≥ (cid:80)i(X)
q(cid:96) ≥ i(X) > k and therefore ζMLP >
r=1
0.

(cid:96)∈Gr
(cid:80)

(cid:96)∈Gr

Could we replace the condition k < i(X) in Lemma 2 by
a requirement that k has to be smaller than the Boolean rank
of X? The following example shows that we cannot.
Example 2. Let X = J4 − I4, where J4 is the 4 × 4 matrix
of all 1s and I4 is the 4 × 4 identity matrix. One can verify
that the Boolean rank of X is 4 and its isolation number is 3.




0
1


1
1

X =

1
0
1
1
For k = 3, the optimal objective value of MLPexact is 0 which
is attained by a fractional solution in which the following 6
rank-1 binary matrices are active.

1
1
1
0

1
1
0
1

(26)




q1 = 1
2
0
0
1
0
0
0
1
0


0
1


0
1






0
0
0
0


0
0


0
0

q2 = 1
2
1
1
0
0
0
0
1
1






0
0
0
0


0
0


0
0

q3 = 1
2
0 1
1
0 0
0
0 1
1
0 0
0






q4 = 1
2
0 0
0 0
0 0
0 0


0
1


1
0






0
1
1
0






q5 = 1
2
1
1
0
0

0 0
0 0
0 0
0 0


1
1


0
0


0
0


1
1

q6 = 1
2
0 0 0
0 0 0
1 0 0
1 0 0






5.3 The relation between the LP relaxations of
MIPexact and MIP( 1
k )

Lemma 3. For ρ = 1
LP relaxations MLPexact and MLP( 1

k , the optimal objective values of the
k ) coincide.

Proof. It is enough to observe that since MLPexact is a min-
imisation problem, π takes the minimal optimal value 1
k M0q
in MLPexact due to constraint (10) which equals the second
term in the objective (14) of MLP( 1

k ).

5.4 Column generation applied to the LP
relaxation of the strong formulation of
MIPexact

We obtain a modiﬁcation of MIPexact which we call the
“strong formulation” by replacing constraints (10) by expo-
nentially many constraints. The following is the LP relaxation
of the strong formulation of MIPexact,
(MLPexact strong)ζMLP = min 1(cid:62)ξ + 1(cid:62)π
s.t. M1q + ξ ≥ 1

(M0)(cid:96) q(cid:96) ≤ π, (cid:96) ∈ [(2n − 1)(2m − 1)]
1(cid:62)q ≤ k
ξ ≥ 0, π ∈ [0, 1]nm−|E|, q ∈ [0, 1]|R|.

Lemma 4. Applying the CG approach to MLPexact strong can-
not be used to generate sensible columns.

Proof. Let us try applying column generation to solve
MLPexact strong and add a column of all 1s as our ﬁrst col-
umn q1. Then at the 1st iteration, for q1 = 1 the objective
value of the Restricted MLP is ζ (1)
RMLP = 0 + (nm − |E|)
for solution vector [ξ(1), π(1), q(1)] = [0, 1, 1]. Adding
the same column of all 1s at the next iteration and set-
2 ], allows us to keep ξ(2) = 0 but set
ting [q1, q2] = [ 1
π(2) = 1
RMLP = 0 + 1
2 (nm − |E|). Therefore
continuing adding the same column of all 1s, after t itera-
tions we have ζ (t)
t (nm − |E|) for solution vec-
tor [ξ(t), π(t), q(t)] = [0, 1
t 1, 1
t 1]. Therefore for t → ∞
we have ζ (t)
RMLP → 0 and we have not generated any other
columns but the all 1s.

2 1 to get ζ (2)

RMLP = 0 + 1

2 , 1

5.5 The greedy algorithm for bipartite binary

quadratic optimisation

For a given n × m coefﬁcient matrix H, we aim to ﬁnd
a ∈ {0, 1}n and b ∈ {0, 1}m so that a(cid:62)Hb is maximised.
Let γ+
i be the sum of the positive entries of H for each row
i ∈ [n], γ+
j=1 max(0, hij). Reorder the rows of H
i
according to decreasing values of γ+
i . Algorithm 1 is the
greedy heuristic of (Karapetyan and Punnen 2013) which
provides the optimal solution to the bipartite binary quadratic

:= (cid:80)m

problem if min(n, m) ≤ 2 and has an approximation ratio of
1/(min(n, m) − 1) otherwise.

i ≥ γ+

i+1.

Algorithm 1: Greedy Algorithm
Phase I. Order i ∈ [n] so that γ+
Set a = 0n, s = 0m.
for i ∈ [n] do
f0 = (cid:80)m
f1 = (cid:80)m
if f0 < f1 then

j=1 max(0, sj)
j=1 max(0, sj + hij)

Set ai = 1, s = s + hi

end

end
Phase II. Set b = 0m.
for j ∈ [m] do

if (a(cid:62)H)j > 0 then
Set bj = 1

end

end

5.6 Rank-k greedy heuristic
For a given X ∈ {0, 1}n×m and k ∈ Z+, according
to Equation (1) we may write min (cid:107)X − Z(cid:107)F
2 as |E| −
max (cid:80)
i∈[n],j∈[m] hijzij were hij are entries of H := 2X −
1. We propose the heuristic in Algorithm 2 to compute k-
BMF by sequentially computing k rank-1 binary matrices
using the greedy algorithm of (Karapetyan and Punnen 2013)
given in Algorithm 1.

We remark that this rank-k greedy algorithm can be used
to obtain a heuristic solution to k-BMF under standard arith-
metic as well: simply modify the last line to

H[ab(cid:62) == 1] = −K

for a large enough positive number K (say K := (cid:80)
so that each entry of X is covered at most once.

ij xij)

Algorithm 2: Greedy algorithm for rank-k binary
matrix factorisation
Input: X ∈ {0, 1}n×m, k ∈ Z+.
Set H := 2X − 1.
for (cid:96) ∈ [k] do

a, b = Greedy(H) // Compute a rank-1

binary matrix via the greedy
algorithm

a, b = Alt(H, a, b) // Improve greedy

solution with the alternating
heuristic

A:,(cid:96) = a
B(cid:96),: = b(cid:62)
H[ab(cid:62) == 1] = 0 // Set entries of H

to zero that are covered

end
Output: A ∈ {0, 1}n×k, B ∈ {0, 1}k×m

5.7 Formulations evaluated in Table 2
The formulation KGH17 of (Kovacs, Gunluk, and Hauser
2017) with new objective function (6) for ρ = 1 that was
evaluated to get results in column KGH17 of Table 2 reads as

min
a,b,z

(cid:88)

(1 − zij) +

(cid:88)

k
(cid:88)

yi(cid:96)j

(i,j)∈E

(i,j)(cid:54)∈E

(cid:96)=1

s.t. zij ≤

k
(cid:88)

(cid:96)=1

yi(cid:96)j,

(i, j) ∈ E,

yi(cid:96)j ∈ M C(ai(cid:96), b(cid:96)j),
zij ∈ [0, 1],
ai(cid:96), b(cid:96)j ∈ {0, 1},

i ∈ [n], (cid:96) ∈ [k], j ∈ [m],
(i, j) ∈ E,
i ∈ [n], (cid:96) ∈ [k], j ∈ [m],

with M C(a, b) = {y ∈ R : a + b − 1 ≤ y, y ≤
a, y ≤ b, 0 ≤ y} denoting the McCormick envelopes as
deﬁned in Section 2. In the exponential formulation of (Lu,
Vaidya, and Atluri 2008) all possible non-zero binary row
vectors βt ∈ {0, 1}1×m (t ∈ [2m − 1]) for factor matrix
B ∈ {0, 1}k×m are explicitly enumerated and treated as
ﬁxed input parameters to the formulation. The formulation
LVA08 with objective function (6) for ρ = 1 that was evalu-
ated to get results in column LVA08 of Table 2 reads as

min
α,δ,z

(cid:88)

(i,j)∈E

(1 − zij) +

(cid:88)

2m−1
(cid:88)

(i,j)(cid:54)∈E

t=1

αit βtj

2m−1
(cid:88)

s.t. zij ≤

αit βtj,

(i, j) ∈ E,

t=1
αit ≤ δt,
2m−1
(cid:88)

δt ≤ k,

t=1

i ∈ [n], t ∈ [2m − 1],

zij ∈ [0, 1],
αit, δt ∈ {0, 1},

(i, j) ∈ E,
i ∈ [n], t ∈ [2m − 1].

5.8 Datasets
In general if a dataset has a categorical feature C with N
discrete options vj, (j ∈ [N ]), we convert feature C into
N binary features Bj (j ∈ N ) so that if the i-th sample
takes value vj for C that is (C)i = vj, then we have value
(Bj)i = 1 and (B(cid:96))i = 0 for all (cid:96) (cid:54)= j ∈ [N ]. This tech-
inque of binarisation of categorical columns has been applied
in (Kovacs, Gunluk, and Hauser 2017) and (Barahona and
Goncalves 2019). The following datasets were used:
• The Zoo dataset (zoo) (Forsyth 1990) describes 101 ani-
mals with 16 characteristic features. All but one feature is
binary. The categorical column which records the number
of legs an animal has, is converted into two new binary
columns indicating if the number of legs is less than or
equal or greater than four. The size of the resulting fully
binary dataset is 101 × 17.

• The Primary Tumor dataset (tumor) (Kononenko and Ces-
tnik 1988a) contains observations on 17 tumour features
detected in 339 patients. The features are represented by

13 binary variables and 4 categorical variables with dis-
crete options. The 4 categorical variables are converted
into 11 binary variables representing each discrete option.
Two missing values in the binary columns are set to value
0. The ﬁnal dimension of the dataset is 339 × 24.

• The Hepatitis dataset (hepat) (Gong 1988) consists of 155
samples of medical data of patients with hepatitis. The 19
features of the dataset can be used to predict whether a
patient with hepatitis will live or die. 6 of the 19 features
take numerical values and are converted into 12 binary
features corresponding to options: less than or equal to
the median value, and greater than the median value. The
column that stores the sex of patients is converted into two
binary columns corresponding to labels man and female.
The remaining 12 columns take values yes and no and
are converted into 24 binary columns. The raw dataset
contains 167 missing values, and according to the above
binarisation if a sample has missing entry for an original
feature it will have 0’s in both columns binarised from that
original feature. The ﬁnal binary dataset has dimension
155 × 38.

• The SPECT Heart dataset (heart) (Cios and Kurgan 2001)
describes cardiac Single Proton Emission Computed To-
mography images of 267 patients by 22 binary feature
patterns. 25 patients’ images contain none of the features
and are dropped from the dataset, hence the ﬁnal dimen-
sion of the dataset is 242 × 22.

• The Lymphography dataset (lymp) (Kononenko and Cest-
nik 1988b) contains data about lymphography examination
of 148 patients. 8 features take categorical values and are
expanded into 33 binary features representing each cate-
gorical value. One column is numerical and we convert
it into two binary columns corresponding to options: less
than or equal to median value, and larger than median
value. The ﬁnal binary dataset has dimension 148 × 44.
• The Audiology Standardized dataset (audio) (Quinlan
1992) contains clinical audiology records on 226 patients.
The 69 features include patient-reported symptoms, patient
history information, and the results of routine tests which
are needed for the evaluation and diagnosis of hearing dis-
orders. 9 features that are categorical valued are binarised
into 34 new binary variables indicating if a discrete option
is selected. The ﬁnal dimension of the dataset is 226 × 94.
• The Amazon Political Books dataset (books) (Krebs 2008)
contains binary data about 105 US politics books sold
by Amazon.com. Columns correspond to books and rows
represent frequent co-purchasing of books by the same
buyers. The dataset has dimension 105 × 105.

• The 1984 United States Congressional Voting Records
dataset (votes)(Schlimmer 1987) includes votes for each
of the U.S. House of Representatives Congressmen on the
16 key votes identiﬁed by the CQA. The 16 categorical
variables taking values of “voted for”, “voted against” or
“did not vote”, are converted into 32 binary variables. One
congressman did not vote for any of the bills and its corre-
sponding row of zero is dropped. The ﬁnal binary dataset
has dimension 434 × 32.

5.9 Data preprocessing
In practice, the input matrix X ∈ {0, 1}n×m may contain
zero rows or columns. Deleting a zero row (column) leads
to an equivalent problem whose solution A and B can eas-
ily be translated to a solution of the original dimension. In
addition, if a row (column) of X is repeated αi (βj) times,
it is sufﬁcient to keep only one copy of it, solve the reduced
problem and reinsert the relevant copies in the corresponding
place. To ensure that the objective function of the reduced
problem corresponds to the factorisation error of the original
problem, the variable corresponding to the representative row
(column) in the reduced problem is multiplied by αi (βj).

5.10 Comparison Methods
The following methods were evaluated for the comparison in
Table 4.
• The code for our methods can be found at https://github.

com/kovacsrekaagnes/rank k BMF.

• For the alternating iterative local search algorithm of (Bara-
hona and Goncalves 2019) (ASSO++) we obtained the
code from the author’s github page. The code implements
two variants of the algorithm and we report the smaller
error solution from two variants of it.

• The greedy algorithm (k-greedy) detailed in Appendix
5.6 is evaluated with nine different orderings and the best
result is chosen.

• For the method of (Zhang et al. 2007), we used an imple-
mentation in the github pymf package by Chris Schinnerl
and we ran it for 10000 iterations.

• We evaluated the heuristic method ASSO (Miettinen
et al. 2006) which depends on a parameter and we re-
port the best results across nine parameter settings (τ ∈
{0.1, 0.2, . . . , 0.9}). The code was obtained form the web-
page of the author.

• In addition, we computed rank-k non-negative matrix fac-
torisation (NMF) and binarise it by a threshold of 0.5:
after an NMF is obtained, values greater than 0.5 are set
to 1, otherwise to 0. For the computation of NMF we used
sklearn.decomposition module in Python.

• For the MEBF method (Wan et al. 2020) we used the code
from the author’s github page. The raw code downloaded
contained a bug and did not produce a solution for some
instances while for others it produced factorisations whose
error in (cid:107) · (cid:107)2
F increased with the factorisation rank k. We
ﬁxed the code and the results shown in 4 correspond to the
lowest error for each instance selected across 9 parameter
settings t ∈ {0.1, . . . , 0.9}.

