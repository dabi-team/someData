2
2
0
2

p
e
S
8

]

G
L
.
s
c
[

1
v
3
3
9
3
0
.
9
0
2
2
:
v
i
X
r
a

PREPRINT

Lars Mikelsons
Chair of Mechatronics
Augsburg University
lars.mikelsons@uni-a.de

Johannes Stoljar
Chair of Mechatronics
Augsburg University
johannes.stoljar@uni-a.de

Tobias Thummerer
Chair of Mechatronics
Augsburg University
tobias.thummerer@uni-a.de

NEURALFMU: PRESENTING A WORKFLOW FOR INTEGRATING
HYBRID NEURALODES INTO REAL WORLD APPLICATIONS

ABSTRACT
The term NeuralODE describes the structural combination of an ArtiÔ¨Åcal Neural Network (ANN)
and a numerical solver for Ordinary Differential Equations (ODEs), the former acts as the right-hand
side of the ODE to be solved. This concept was further extended by a black-box model in the form of
a Functional Mock-up Unit (FMU) to obtain a subclass of NeuralODEs, named NeuralFMUs. The
resulting structure features the advantages of Ô¨Årst-principle and data-driven modeling approaches in
one single simulation model: A higher prediction accuracy compared to conventional First Principle
Models (FPMs), while also a lower training effort compared to purely data-driven models. We present
an intuitive workÔ¨Çow to setup and use NeuralFMUs, enabling the encapsulation and reuse of existing
conventional models exported from common modeling tools. Moreover, we exemplify this concept
by deploying a NeuralFMU for a consumption simulation based on a Vehicle Longitudinal Dynamics
Model (VLDM), which is a typical use case in automotive industry. Related challenges that are often
neglected in scientiÔ¨Åc use cases, like real measurements (e.g. noise), an unknown system state or
high-frequent discontinuities, are handled in this contribution. For the aim to build a hybrid model
with a higher prediction quality than the original FPM, we brieÔ¨Çy highlight two open-source libraries:
FMI.jl for integrating FMUs into the Julia programming environment, as well as an extension to this
library called FMIFlux.jl, that allows for the integration of FMUs into a neural network topology to
Ô¨Ånally obtain a NeuralFMU.

P R E P RIN T

Hybrid modeling describes on the one hand the Ô¨Åeld of research in machine learning that focuses on the fusion of First
Principle Models (FPMs), often in the form of symbolic differential equations, and machine learning structures like
ArtiÔ¨Åcal Neural Networks (ANNs). On the other hand, in the Ô¨Åeld of Ordinary Differential Equations (ODEs), hybrid
models name the piece-wise concatenation of continuous models over time to obtain a discontinuous model, of which
the numerically simulated solutions may not be continuously differentiable over time. In this article, we present a
workÔ¨Çow concerning both interpretations of hybrid modeling by integrating custom, discontinuous simulation models
and ANNs into a discontinuous NeuralFMU. For illustration, we use the example of learning a friction model for an
industry typical automotive consumption simulation based on a Vehicle Longitudinal Dynamics Model (VLDM).

In the following, the term hybrid model is used to identify a model based on the combination of a FPM and Machine
Learning (ML), whereas the concatenation of multiple continuous systems is referred to as discontinuous model.

Keywords NeuralFMU; FMU; Functional Mock-up Unit; NeuralODE; hybrid model; FMI; vehicle longitudinal
dynamics model; PhysicsAI; scientiÔ¨Åc machine learning

Introduction

1

 
 
 
 
 
 
1.1 State-of-the-art: Hybrid modeling

PREPRINT

‚Ä¢ NeuralODE training tends to converge in local minima;

‚Ä¢ NeuralODEs training often takes a considerable amount of calculation time.

‚Ä¢ Real world models from common modeling tools are in general not available as symbolic ODEs;

As part of research applications, the structural integration of physical models inside ML-topologies like ANNs is
a topic growing attention. One approach for hybrid modeling is the integration of the FPM into the ML process
by evaluating the forward propagation of the physical model as part of the loss function during training, namely
Physics-informed Neural Networks (PINNs) [19]. In contrast, our method focuses on the structural integration of FPMs
into the ANN/ODE itself and not only in the cost function, allowing much more Ô¨Çexibility with respect to what can
be learned and inÔ¨Çuenced. However, it is also possible to build and train PINNs with the presented libraries. Another
approach uses Bayesian Neural Stochastic Differential Equations (BNSDEs), which applies model selection together
with Probably Approximately Correct (PAC) Bayesian bounds during the ANN training to improve hybrid model
accuracy on basis of noisy prior knowledge [9]. Besides, Deep Auto-Regressive Networks (DARNs) can also be used to
model physical systems. Similar to Recurrent Neural Networks (RNNs), the output of the last network inference is fed
back into the neural network itself as input [7]. Different from RNNs, this feedback is not modeled as neural network
state, but a simple feed-forward connection, and thus a DARN can be trained as conventional feed-forward network
with all related simpliÔ¨Åcations and beneÔ¨Åts. Finally, the combination of symbolic ODEs and object-orientated modeling
languages like Modia is a promising and emerging research Ô¨Åeld too, because of the beneÔ¨Åts of acausal modeling [3].
For a general overview on the growing Ô¨Åeld of hybrid modeling see e.g. [25] or [18].
With regards to learning system dynamics (the right-hand side of a differential equation), the structural integration of
algorithmic numerical ODE-solvers into ANNs leads to signiÔ¨Åcant improvements in performance, memory cost and
numerical precision in comparison to the use of residual neural networks [4], while offering a new range of possibilities,
e.g. Ô¨Åtting data observed at irregular time steps [11]. This integration of a numerical solver for ODEs into an ANN
is known as NeuralODE, which is further introduced in Section 2.2. Probably the most mentioned point of criticism
regarding NeuralODEs is the difÔ¨Åcult transfer to real world applications for the following reasons:

P R E P RIN T

In a preliminary publication [22], we started facing that issue and extended the concept of NeuralODEs by adding
FPMs in the form of Functional Mock-up Units (FMUs) into this topology (s. Section 2.2). The resulting subclass of
hybrid NeuralODEs, called NeuralFMUs, can be seen as the injection of system knowledge in the form of a FPM into
the ANN model, equivalently the right-hand side of the ODE. This not only enhances the prediction quality, but also
signiÔ¨Åcantly reduces the amount of necessary training data, because only missing physical effects need to be learned.
For example, the original FPMs were outperformed in terms of computational performance [23] and prediction accuracy
[22], with trained only on data gathered by a single, short part of simulation trajectory. Further, the integration of a
FPM can strongly enhance the extrapolation capabilities of the hybrid model compared to conventional pure data-driven
models. In this article, we want to follow up these publications [22, 23] by providing a workÔ¨Çow and results not only
for a synthetic example, but also a real world application in the form of an energy consumption simulation based on a
VLDM. The following challenges, which are common in industrial applications but are often neglected for scientiÔ¨Åc
experiments, are overcome as part of this publication: Discontinuity, closed-loop controllers inside of the system, an
unknown system state and typical measurement errors (e.g. noise).

Whereas the tendency to early converge to local minima and long training times can be tackled by different techniques
(s. Sections 2.6 and 2.7), the major technical challenge that hinders hybrid modeling in industrial applications remains:
FPMs are modeled and simulated inside closed tools. ML features, the foundation to allow for hybrid modeling, are
missing in such tools and a seamless interoperability to ML-frameworks is not given. For example, to train the data-
driven parts of hybrid models, determination of the loss function gradient through the ANNs and the FPMs themselves
is needed. This requires different high-performance sensitivity algorithms like Automatic Differentiation (AD). ML-
frameworks on the other hand, provide these abilities. To build high performing hybrid models, an interface between
these two application worlds is needed.

This article is further structured into three sections: A brief introduction to the used standards, methods, the correspond-
ing software libraries FMI.jl and FMIFlux.jl and the VLDM. This is followed by the presentation of results of the
example use case handling a NeuralFMU setup and training and Ô¨Ånally closed by a conclusion with future outlook.

1.2 Preliminary work: NeuralFMUs

2

2 Materials and Methods

PREPRINT

2.1 Functional Mock-up Interface (FMI)

In this section, a short overview over the used standards, software and methods is given. On foundation of these, a
workÔ¨Çow for the setup of NeuralFMUs in real applications is given and methods of initialization and training are
elaborated. Finally, the VLDM, the FPM for the considered example use case, is introduced.

The FMI standard1 allows for the distribution and exchange of models in a standardized format independent of the
modeling tool. The interface standard counts three version releases, the most popular version is 2.0 [14], the successor
version 3.0 [15] was released in May 2022. An exported model container that fulÔ¨Ålls the FMI requirements is called
FMU. FMUs can be used in other simulation environments or even as part of entire Co-Simulations or System
Structure and Parameterization (SSP) [16]. FMUs are subdivided into three major classes: Model Exchange (ME),
Co-Simulation (CS) and new in FMI 3.0 Scheduled Execution (SE). The possible use cases depend on the FMU-type
and the availability of standardized and optionally implemented FMI functions. Most relevant for the considered use
case are ME-FMUs, because this type allows for manipulation and extension of system dynamics before the numerical
integration.
To optimize simulation performance, fast physical effects like the change from stick- to slide-friction or the Ô¨Åring of an
electrical diode are often modeled in a discontinuous way. This means that the expressions of the right-hand side of
the ODE model may change depending on the current system state and time, whereas this transition is discrete. Inside
FMI, this means ME-FMUs may contain state- or time-dependent discontinuities, which are triggering so called events.
The actual event time point, the instant at which the equations and/or the state of the model is modiÔ¨Åed, is deÔ¨Åned by
a predeÔ¨Åned time point itself (time-events) or the zero-crossing of a scalar value (state-events), also called the event
indicator. For a detailed view on the event deÔ¨Ånition and handling, see [15]. Basically, any ME-FMU with continuous
state xc, discrete state xd and time- and/or state-events can be seen as a hybrid ODE, as in Figure 1. Continuous states
may change in time, while discrete states can only change their value at event instances.

P R E P RIN T

Figure 1: ExempliÔ¨Åed simulation of a ME-FMU with events (hybrid ODE). The single (piecewise) continuous system
state xc (blue) and state derivative Àôxc may depend on a discrete system state xd (red), which is unknown in general for
FMUs. Not handling events like e1 (black-dashed), leads to wrong system values (blue-, green-, red-dashed), because
the system state is not updated properly.

1https://fmi-standard.org/

3

ùë°ùë•ùëêùë•ùëë·à∂ùë•ùëêùëí12.2 NeuralODEs & NeuralFMUs

PREPRINT

Figure 2: Topology of a NeuralODE consisting of a feed-forward ANN and a numerical ODE-solver: The current
system state x is passed to the ANN, based on that the state derivative Àôx is calculated and integrated into the next
system state x(t + h) by the ODE-solver with time step size h.

We extend the concept of NeuralODEs by one or more FPMs in the form of FMUs to obtain a class of hybrid models,
named NeuralFMUs [22]. Using the example of a ME-NeuralFMU, the ME-FMU replaces the ANN of the NeuralODE,
because it calculates the system dynamics Àôx based on the current system state x. To optimize the system state, an
additional (state) ANN can be placed before, and to manipulate the system dynamics, an additional (derivative) ANN
can be placed after the FMU. This exempliÔ¨Åed structure of a NeuralFMU is given in Figure 3. However, the concept of

NeuralODEs are deÔ¨Åned by the structural combination of an ANN and a numerical ODE-solver, see Ô¨Ågure 2. As a
result, the ANN acts as the right-hand side of an ODE, whereas solving of this ODE is performed by a conventional
ODE-solver. If external requirements (tolerance or stiffness) change, the ODE-solver can be easily replaced by another
one. The scientiÔ¨Åc contribution at this point is not only the idea of this subdivision, but also, more importantly, a
concept to allow training this topology on a target solution for the ODE. This requires propagation of the parameter
sensitivities of the ANN through the ODE-solver [4]. DiffEqFlux.jl2 is already available as a ready-to-use library for
building and training NeuralODEs in the Julia programming language [17].

P R E P RIN T

NeuralFMUs itself is very generic and does not restrict the positions or number of FMUs inside the ANNs or limits
which signals are manipulated by the ANNs. Inference of a NeuralFMU can be achieved by evaluating each of the
considered blocks one after another. Whereas inference of the derivative ANN (between FMU and solver) is straight
forward, because the system dynamics are passed as input to the ANN, inference of the state ANN needs additional
attention. In case of an event inside the FMU, the FMU system state ÀÜx may be changed during event-handling. This
new state must be propagated backwards through the state ANN to calculate a new system state x for the ODE solver to
reinitialize the numerical integration at. Because ANNs are not invertible by default, the new state x must be determined
by solving an optimization problem. In case of a state event, the required accuracy for the optimization solution is high,
because solving for a state that is slightly before the event instance will trigger the event again. To prevent this, the
optimization objective can be deÔ¨Åned not only on hitting the FMU state ÀÜx, but also on the change of the sign of the
corresponding event indicator. This enhanced objective promotes Ô¨Ånding a state that lays after the event instance in
time. In case of time events, high accuracy for the optimization result is desirable but not required.

Figure 3: Topology of a ME-NeuralFMU (example) consisting of two feed-forward ANNs, a ME-FMU and a numerical
ODE solver: The current system state x is passed to a (state) ANN, based on that a manipulated system state ÀÜx is
calculated and passed to the ME-FMU. There, the system state derivative Àôx is computed and manipulated by another
(derivative) ANN into the changed system state derivative ÀÜÀôx, which is Ô¨Ånally integrated into the next system state
x(t + h) by the ODE-solver with time step size h. Also the FMU‚Äôs input u, output y or parameters (not shown) could
be connected to the ANNs.

2https://github.com/SciML/DiffEqFlux.jl

4

ANNùíô·à∂ùíôODE Solver‡∂±ùíô(ùë°+‚Ñé)ùíôùíñùë°ME-FMU‡∑ùùíôANNODE Solver‡∂±ùíô(ùë°+‚Ñé)‡∑°·à∂ùíô·à∂ùíôANNùíöPREPRINT

2.3 Software

2.3.2 FMI in Julia: FMI.jl

2.3.1 Julia Programming Language

For a more detailed view on the concept of NeuralFMUs and the technical training process, see [22]. In the following,
only ME-NeuralFMUs are considered and identiÔ¨Åed by the short term NeuralFMU.

Combining and training physical and data-driven models inside a single industry tool is currently not possible, therefore,
it is required to transfer FPMs to a more suitable environment. After the export from modeling environment and
import into the ML environment, the Ô¨Årst-principle is extended to a hybrid model. Finally after succeeded training, it
is necessary to re-import the hybrid model back into the original modeling environment, for further modeling or the
setup of larger system co-simulations. For the considered importing and exporting between environments, an industry
typical model exchange format is needed. Because the FMI is an open standard and widely used in industry as well as
in research applications, it is a suitable candidate for this aim. On the side of modeling environments, FMI is already
implemented in many common tools, but a software interface that integrates FMI into the ML environment is still
needed (s. Section 2.3.2).

In this section, it is shortly explained why the authors picked the Julia programming language (from here on simply
referred to as Julia) for the presented task as ML environment. Julia is a dynamic typing language developing since
2009 and Ô¨Årst published in 2012 [1], with the aim to provide fast numerical computations in a platform-independent,
high level programming language [2]. The language and interpreter was originally invented at the Massachusetts
Institute of Technology, but till today many other universities and research facilities have joined the development of
language expansions, which mirrors in many contributions from different countries and even in its own conference,
the JuliaCon3. Besides many great libraries in the Ô¨Åeld of scientiÔ¨Åc machine learning, there are multiple libraries for
AD like e.g. ForwardDiff.jl4[20] and Zygote.jl5[10]. Further, different modeling related libraries are available, like for
example Modia.jl6 [6], which allows for object-orientated white-box modeling of mechanical and electrical systems,
syntactically similar to Modelica¬Æ, in Julia.

P R E P RIN T

The Julia library FMI.jl provides high level commands to unzip, allocate, parameterize and simulate entire FMUs, as
well as plotting the solution and parsing model meta data from the model description. Because FMI has already three
released speciÔ¨Åcation versions and is under ongoing development, one major goal of FMI.jl is to provide the ability
to simulate different version FMUs with the same user front-end. To satisfy users who prefer close-to-speciÔ¨Åcation
programming, as well as users that are new to the topic and favor a smaller but more high level command set, we
provide high level Julia commands, but also the possibility to use the more low level commands speciÔ¨Åed in the FMI
standards [14, 15]. The library and its feature set are constantly growing.

The open-source library FMIFlux.jl extends FMI.jl and allows for the fusion of a FMU and an ANN. As in many other
machine learning frameworks, a deep ANN in Julia using Flux.jl7 is conÔ¨Ågured by chaining multiple neural layers
together. Probably the most intuitive way of integrating a FMU into this topology, is to simply handle the FMU as a
network layer. In general, FMIFlux.jl does not make restrictions to:

‚Ä¢ which FMU signals can be used as layer inputs and outputs. It is possible to use any variable that can be set via
fmi2SetReal or fmi2SetContinuousStates as input and any variable that can be obtained by fmi2GetReal or
fmi2GetDerivatives as output;

Dependent on the FMU type, ME, CS or SE, different setups for NeuralFMUs should be considered. In this article,
only ME-NeuralFMUs are highlighted.

3http://www.juliacon.org
4https://github.com/JuliaDiff/ForwardDiff.jl
5https://github.com/FluxML/Zygote.jl
6https://github.com/ModiaSim/Modia.jl
7https://fluxml.ai/Flux.jl/stable/

‚Ä¢ where to place FMUs inside the ANN topology, as long as all signals are traceable via AD (no signal cuts).

2.3.3 NeuralFMUs in Julia: FMIFlux.jl

5

2.4 WorkÔ¨Çow

PREPRINT

Figure 4: WorkÔ¨Çow of the presented hybrid modeling application using a modeling tool supporting FMI (export and
import) and FMI.jl together with FMIFlux.jl.

On the foundation of Julia, FMI, FMI.jl and FMIFlux.jl, we suggest the following workÔ¨Çow for designing custom
NeuralFMUs:

P R E P RIN T

In many ML applications, pre- and post-processing are not only instrumental, but necessary. For training conventional
ANNs, pre-processing of training data can often be performed for one single time, before batching and the actual
training. For NeuralFMUs, the FMUs may generate outputs within a range that is excessively saturated by the activation
functions inside the ANN. Further, the FMUs may expect inputs within a range not generatable by the ANN, because
of the limited output of the activation functions. Therefore, all signals must be processed at the interfaces between the
FMUs and ANNs. If no expert knowledge of the data range of the FMU inputs and outputs is available, a good starting
point can be to scale and shift data into a standard normal distribution. Because the FMU output and input may shrink
or grow during training because of new state exploration by the changed dynamics, scaling and shifting parameters
should be parts of the optimization parameters during training. See Section 3.2 for a visual example topology using
data pre- and post-processing around an ANN.

4. The FPM is extended to a hybrid model and trained on data, for example of a real system or a high-resolution

1. The FPM is designed by a domain expert inside of a familiar modeling tool supporting FMI (export and

7. The improved hybrid model FMU can further be used in different modeling and simulation tools (including

The presented development process of a NeuralODE/FMU in Figure 4 covers the following steps:

6. The hybrid model FMU is imported into the original modeling environment.

5. The trained hybrid model is exported as FMU using FMI.jl.

3. The FPM-FMU is imported into Julia using FMI.jl.

2. After modeling, the FPM is exported as FMU.

and high-Ô¨Ådelity simulation, using FMIFlux.jl.

the original tool the FPM was exported from).

2.5 Data pre- and post-processing

import).

6

Data2356Modeling-environmentFirst PrincipleModel1Hybrid Model4Modeling-environmentHybrid Model7FMUFMU2.6

Initialization (Pre-training)

PREPRINT

2.6.1 Neutral Initialization Pre-Training (NIPT)

For a better understanding, initialization strategies are not exempliÔ¨Åed at a NeuralFMU as in Figure 3, but a suitable
NeuralFMU topology for this use case, which includes only one FMU, one ANN (derivative) and the numerical solver.
The concepts can be easily modiÔ¨Åed to Ô¨Åt other topologies.

If the system state derivative is not known, cannot be measured and/or can hardly be approximated, NIPT of the ANNs
can deliver a good initialization for the ANN parameters for the later training. Similar to auto-encoder networks, the
aim is to train the ANN so, that the output equals the input for a set of training data, see Figure 5. Different from
auto-encoders, the hidden network layers don‚Äôt need to narrow in width. The training result is, that the solution of
the initialized NeuralFMU converges against the solution of the FMU itself - or if multiple FMUs are present - the
solution of the chained FMU-system without ANNs. If the FMU solution is already close to the target solution (the
term close strongly depends on the system constitution), this might be a suitable initialization method. Only for training
data aquisition, it is necessary to perform a single forward simulation. For the actual training, solving the ODE system
is not required.

Obtaining a trainable (solvable) NeuralFMU is not trivial. Using larger or complex FMUs together with randomly
initialized ANNs often leads to instable and/or stiff ODE-system. Whereas starting the training process with an
unnecessary stiff NeuralFMU (more stiff than the Ô¨Ånal solution) leads to long training times, an instable system might not
be trainable at all. Without further investigation, selecting random initialized ANNs as part of NeuralFMUs often lead to
hardly trainable systems in different use cases like e.g. a controlled EC-Motor Hardware in the Loop (HiL)-simulation,
a thermodynamic cabin simulation or in modeling the human cardiovascular system [23]. Therefore, we suggest
three different initialization strategies for ANNs inside of NeuralFMUs: Neutral Initialization Pre-Training (NIPT),
Collocation Pre-Training (CCPT) and the introduction of a FPM/ANN gates topology. A major advantage of all
initialization modes is, that sensitivities during the initialization process don‚Äôt need to be propagated through the ODE
solver (the actual ODE is not solved), therefore, the computational effort is much less, compared to the actual training
described in Section 2.7.

P R E P RIN T

This method needs knowledge of the entire system state trajectory Àúx(t) as well as the (at least approximated) state
derivative ÀúÀôx(t). In general, only a part of the system state and/or derivative of a real system can be measured. Different
methods allow for estimating the unknown states like e.g. Kalman-Ô¨Ålter [12]. To converge against the target solution,
CCPT needs high-quality data of the system state and derivative. Derivatives could be approximated by Ô¨Ånite differences
or Ô¨Ålters, see [21] for an overview. Note that approximating the derivatives may decrease the quality of the pre-training
process.

Figure 5: For NIPT, a single reference simulation with unchanged Àôx is performed to calculate the ANN output ÀÜÀôx(t)
for every system state x(t). Based on that, the actual training can be performed. As soon as the training goal Àôx ‚âà ÀÜÀôx
(red-dotted) is reached, the NeuralFMU dynamics equals the dynamics of the FMU itself, which results in the same
solution for both. In this case, the ANN behaves neutral.

Similar to the collocation training of NeuralODEs [21], collocation training can be performed for NeuralFMUs, too. The
CCPT is similar to NIPT, the major difference is the training goal, see Figure 6. Whereas NIPT focuses on propagating
the derivatives unchanged through the ANN, CCPT aims on hitting the derivatives of the ODE-solution, so that after
integration (solving), the target solution can be obtained.

2.6.2 Collocation Pre-Training (CCPT)

7

ùíôùë°ME-FMU·à∂ùíôANNODE Solver‡∂±ùíô(ùë°+‚Ñé)‡∑°·à∂ùíôùíñPREPRINT

2.6.3 FPM/ANN gates

The challenge of Ô¨Ånding a good initialization by a foregoing pre-training procedure can be bypassed by introducing a
slightly modiÔ¨Åed topology, that literally introduces a bypass around the ANN, see Figure 7.

CCPT is only usable, if the states of the training objective matching the state derivatives manipulated by the ANN.
Whereas this is often the case in academic examples, in real applications it is not, which is further exempliÔ¨Åed at the
VLDM in Section 3.2.
To summarize, NIPT doesn‚Äôt require the entire state, but converges only against the FMU solution. CCPT on the other
hand, converges against a given target solution, but requires a known target solution and derivative.

Figure 6: For CCPT, no reference simulation is performed. For a given state trajectory Àúx(t) (e.g. from data), every
state is propagated through the FMU and the ANN. As soon as the training goal ÀÜÀôx ‚âà ÀúÀôx (red-dotted) is reached, the
NeuralFMU dynamics equals the target dynamics ÀúÀôx (e.g. from data). As a result, the later NeuralFMU solution matches
the given state trajectory for a perfect known ÀúÀôx. The target dynamics ÀúÀôx may be estimated by deriving and Ô¨Åltering the
given system state Àúx(t) or might be known from measurements.

P R E P RIN T

On the one hand, for the case pAN N = 0 and pF M U = 1, the resulting simulation trajectory is just the simulation
trajectory of the original FMU, independent of the ANN parameters. In this way, the NeuralFMU can be initialized
without a special initialization routine, while also being capable of manipulating the system dynamics if the parameter
pAN N is changed to a non-zero value. On the other hand, for the case pAN N = 1 and pF M U = 0, only the ANN
affects the state dynamics and the original FMU dynamics are used only as input for the ANN. The parameters pAN N
and pF M U can be optimized along the other training parameters, or dependent on the use case, with a static or dynamic
decay/increase. As a Ô¨Ånal note, CCPT and the FPM/ANN gates topology doesn‚Äôt exclude each other and can be used
together on a NeuralFMU initialization.

Figure 7: ME-NeuralFMU with FPM/ANN gates: The ME-FMU receives the current time t and state x from the
numerical ODE solver together with an external input u and computes the corresponding state derivative Àôx. Two gates
pAN N and pF M U scale how much dynamic changes by the ANN and the FMU are introduced to the derivative vector
ÀÜÀôxG. Finally, the derivative vector ÀÜÀôxG is passed to the ODE solver with (adaptive) step size h and integrated to the next
system state x(t + h).

As in Figure 7, the system state derivative ÀÜÀôxG is deÔ¨Åned as follows, where ‚ó¶ stands for the Hadamard-product:

ÀÜÀôxG = pAN N ‚ó¶ ÀÜÀôx + pF M U ‚ó¶ Àôx

(1)

8

ùíñùë°ME-FMU·à∂ùíôANN‡∑°·à∂ùíô‡∑•ùíô‡∑©·à∂ùíôùíôME-FMUùíñùë°·à∂ùíôANNùíëùê¥ùëÅùëÅùíëùêπùëÄùëà+GatesODE Solver‡∂±ùíô(ùë°+‚Ñé)‡∑°·à∂ùíôùê∫‡∑°·à∂ùíô2.7 Batching & Training

PREPRINT

3 Results

Besides the ones mentioned, many techniques for NeuralODEs can be adapted to improve the training process in terms
of stability and convergence, like for example multiple shooting as in [24], because NeuralFMUs are a subclass of
NeuralODEs.

the used CADC (Road) is 1001.22 s
The training is not performed on the entire data trajectory at one time (e.g.
long). Instead the trajectory is batched. The major challenge at this point is that for a given batch element start time
t (cid:54)= t0, often only the continuous part of the model state vector xc(t) of the ME-FMU is known. In general, the
discrete part xd(t) is unknown. As a result, if not explicitly given, a suitable discrete state is determined during the
initialization procedure inside the FMU, but it is not guaranteed that this state matches the data and/or expectations.
This circumstance also applies to the determination of the initial value of the discrete states t0d, but measurements are
often started in a stationary state, where a good understanding of the correct discrete states is given, even if they are not
explicitly parts of the data measurements.
As a consequence, training cannot be initialized at an arbitrary element of the batch (time instant) because of the
unknown discrete states, which might be initialized unexpected if ignored. Estimating the discrete system state on basis
of data is not trivial and may need signiÔ¨Åcant expert knowledge about the model itself. So, a straight-forward strategy to
handle this is to simulate all batches in the correct order without resetting the FMU between batch elements. Although
this does not guarantee the correct discrete state when switching from one batch element to the next during training, the
discrete solution converges against the target together with the continuous solution.
Another option is to simulate the entire trajectory for one single time and making memory copies of the entire FMU
state using e.g. fmi2GetFMUstate (in FMI 2.0) at the very beginning of every batch element. This allows for random
batches, which might improve the training success and speed. Because the feature required to save and load the FMU
memory footprint is optional in FMI and thus often not implemented, this strategy is not further highlighted at this
point, but can be implemented in a straight-forward manner.

P R E P RIN T

In this section, the used FMU model is introduced: The model represents the longitudinal dynamics of an electric Smart
EQ fortwo and is an adaption from a model of the Technical University of Munich [5]. In automotive applications,
longitudinal dynamics models are often used to simulate energy consumption, thus only related effects are represented in
the model. The original model was created in MATLAB¬Æ/Simulink¬Æ, replicated analogously in the modeling language
Modelica¬Æ and exported as FMU. The following Figure 8 shows the topology of the simulation model. The full vehicle
model is modular and consists of six core components [8].

The considered method is validated at the following application: Based on the introduced VLDM, a hybrid model
is deployed leading to a signiÔ¨Åcant better consumption prediction than the original FPM. Even if the FPM already
considers multiple friction effects, it is assumed that the prediction error is the result of a wrongly parameterized or
an additional, non-modeled friction effect. The goal is to inject an additional vehicle acceleration to force the driver
controller to perform a higher engine torque, and thus to increase the vehicle energy consumption.

3.1 Example Model: Vehicle Longitudinal Dynamics Model (VLDM)

Figure 8: Topology of the VLDM (own representation adapted from [5]).

9

DriverVehicledynamicsControl unitDrivingcyclePower sourceDrive trainPREPRINT

In analogy, the six continuous state derivatives Àôxc are:

‚Ä¢ x1 the PI-controller state (integrated error) for the throttle pedal (Driver);
‚Ä¢ x2 the PI-controller state (integrated error) for the brake pedal (Driver);
‚Ä¢ x3 the integrated driving cycle speed, the cycle position (Driving cycle);
‚Ä¢ x4 the vehicle position (Vehicle dynamics);
‚Ä¢ x5 the vehicle velocity (Vehicle dynamics);
‚Ä¢ x6 the cumulative consumption (energy).

The top-layer components of the vehicle are the Driving cycle, the Driver, the Control unit, the Power source, the Drive
train and the Vehicle dynamics subsystem. The target vehicle speed, read from a driving cycle tabular in the Driving
cycle subsystem, is forwarded to the Driver. The Driver consists of two PI-controllers for the acceleration and brake
pedal, controlling the vehicle to follow the given driving cycle. The block Control unit calculates the desired torque
from the pedal position. The required power is provided by the Power source component, modeling the vehicle battery.
In the Drive train block, the sources of acceleration and braking torques are implemented. This component contains
the electric motor, the power electronics, the transmission and the tire model. In the component Vehicle dynamics, the
rolling-, air- and slope-resistance are implemented. The resulting force and torque determine the acceleration of the
vehicle, and after numerical integration the speed and position [5]. Finally, the vehicle speed is fed back into the Driver
subsystem and closes the controller loop. Together with the model itself, multiple measurements from an automotive
driving test rig with different driving cycles like the Common Artemis Driving Cycles (CADC), New European Driving
Cycle (NEDC) and Worldwide harmonized Light-duty vehicles Test Cycle (WLTC) where published8. We use the
driving cycles CADC (Road) and WLTC (class 2) for the presented experiment.
The simulation model counts six continuous states xc in total:

P R E P RIN T

‚Ä¢ Àôx1 the PI-controller error/deviation for the throttle pedal (Driver);
‚Ä¢ Àôx2 the PI-controller error/deviation for the brake pedal (Driver);
‚Ä¢ Àôx3 the driving cycle speed (Driving cycle);
‚Ä¢ Àôx4 the vehicle velocity (Vehicle dynamics);
‚Ä¢ Àôx5 the vehicle acceleration (Vehicle dynamics);
‚Ä¢ Àôx6 the current consumption (power).

‚Ä¢ The system is highly discontinuous, meaning it has a signiÔ¨Åcant amount of explicitly time-dependent events
(100 events per second). This further limits the maximum time step size for the numerical solver and so
worsens the simulation and training performance;

‚Ä¢ The simulation contains a closed-loop over multiple subsystems with two controllers running at 100 Hz (the

‚Ä¢ There is a hysteresis loop for the activation of the throttle and brake pedal, the PI-controller states are initialized

‚Ä¢ Characteristic maps (data models) for the electric power, inverted electric power and the electric power losses

‚Ä¢ Measurements of the real system are not equidistant in time (even if it was saved this way, which introduces a

‚Ä¢ Measurements are not exact and contain typical, sensor- and Ô¨Ålter-speciÔ¨Åc errors (like noise and oscillation);

Combining all these attributes results in a challenging FPM for the considered hybrid modeling use case.

‚Ä¢ Only a subpart of the system state vector is part of measurements, the remaining parts are estimated;

‚Ä¢ The system contains a large amount of state-dependent events, triggered by 22 event-indicators;

‚Ä¢ The system is highly non-linear, e.g. multiple quantities are saturated;

Note that this system features different challenging attributes, like:

at corresponding edges in the hysteresis loop;

source of the high-frequent time-events);

typical measurement error);

are used.

8https://github.com/TUMFTM/Component_Library_for_Full_Vehicle_Simulations

10

3.2 Topology

PREPRINT

Combining the orignal NeuralFMU topology (s. Figure 3) with pre- and post-processing (s. Section 2.5) and FPM/ANN
gates (s. Section 2.6.3) results in the topology as shown in Figure 9, which is used for training the hybrid model in the
considered use case. In general, understanding at least some aspects of the physical effect, which is to be modeled by the

Figure 9: Topology of the used ME-NeuralFMU: The ME-FMU receives the current time t and state x from the
numerical ODE solver and computes the corresponding state derivative Àôx. The state derivative is then trimmed to
a subset of derivatives ÀôxA. Before the ANN transforms ÀôxA, the signals are pre-processed to approximately Ô¨Åt a
standard normal distribution. After the ANN inference, the inverse transformation of the pre-process is applied in the
post-processed and ÀÜÀôxA is obtained. Two gates pAN N and pF M U scale how much ÀÜÀôxA and Àôx are introduced to the Ô¨Ånal
derivative vector ÀÜÀôx. Finally, the changed derivative vector ÀÜÀôx is passed to the ODE solver with (adaptive) step size h and
integrated to the next system state x(t + h). The used FMU has no continuous inputs u, the driving cycle is part of the
model and depends only on the time t.

P R E P RIN T

ANNs, is a great advantage. Basically, any value of the FMU that is accessible via the FMI can be used as input for the
ANNs: The system states and -derivatives, system inputs as well as any other system variable (or output) that depends
on the system state, input and/or time. This allows for a wide variety of NeuralFMU topologies. However, using all
available variables in the interface to the ANNs can result in sub-optimal training performance, because more variable
sensitivities need to be determined and signals without physical dependency could be misinterpreted as dependent. This
motivates the use of a clever, minimal subset of the available FMU variables. Often, state and state derivatives are
good choices for variables to feed into the ANN, because from a mathematical point of view, the system state holds all
relevant information in a minimal representation. Nevertheless, adding additional variables may be productive, even if
the encapsulated system information is redundant, if the correlations between these variables and the training objective
is easier to Ô¨Åt for an ANN compared to the correlation with system states and/or derivatives.
For the considered use case, a friction-effect is learned. Conventional mechanical friction models, like viscous damping
or slip-stick-friction, often depend on the physical body‚Äôs translational or rotational velocity. Therefore, especially the
vehicle speed should be considered. Further, the current vehicle acceleration and power is also given as inputs to the
ANN. The training objective is to match the cumulative consumption from training data by manipulating the vehicle
acceleration. Here, CCPT can‚Äôt be used, because the CCPT objective would be to Ô¨Åt the cumulative consumption
derivative, so the current consumption, but this value is not affected by the ANN.

‚Ä¢ ÀÜÀôxA = {0, 0, 0, 0, ÀÜÀôx5, 0} corresponds to the estimated vehicle acceleration by the ANN. This is the output of
the ANN (technically, it is the only output, because the other Ô¨Åve dynamics are assumed always zero and are
neglected);

‚Ä¢ ÀôxA = { Àôx4, Àôx5, Àôx6} corresponds to the vehicle speed, acceleration and power (current consumption). These are

‚Ä¢ pF M U = {1, 1, 1, 1, p2, 1}, only the inÔ¨Çuence of the vehicle acceleration from the FMU can be controlled via

‚Ä¢ pAN N = {0, 0, 0, 0, p1, 0}, only the inÔ¨Çuence of the vehicle acceleration from the ANN can be controlled via

Please note that if the considered effect could depend on the system states, states could also be passed as input to the
ANN. Because the hybrid model reuses the FPM and therefore the ANN only needs to approximate the unmodeled

To summarize, the following variables (compare to Figure 9) are used:

p2 (all other derivatives contribute by 100%).

p1 (this is the only ANN output);

the inputs for the ANN;

11

ME-FMUùíôùë°·à∂ùíôSeparation·à∂ùíôùê¥ANNPre-Processing‡∑°·à∂ùíôùê¥Post-ProcessingODE Solver‡∂±ùíô(ùë°+‚Ñé)‡∑°·à∂ùíôùíëùê¥ùëÅùëÅùíëùêπùëÄùëà+GatesPREPRINT

Type

Index

1
2
3
4
5

Activation

Parameters

3
32
1
1
1

3
3
32
1
1

0
32
1
0
0

3.3.1 Training

Inputs Outputs Bias

none
tanh
tanh
none
none

3.3 Consumption prediction

physical effect, a very light-weight net layout is sufÔ¨Åcient, as shown in Table 1. This results in a fast training because of
the small number of parameters.

Table 1: ANN layout and parameters of the used topology.

6
128
33
0
2
Sum: 169

Pre-process
Dense
Dense
Post-process
Gates

After training on a batch with a batch element length of 100 s, resulting in 11 batch elements, and 18 epochs on
the CADC (Road), the NeuralFMU is able to outperform the FPM on training data, see Figures 10, 11 and 12. The
following Ô¨Ågures show the predicted cumulative consumption over time of the original FPM, compared to the trained
NeuralFMU. Training is not converged at this point. The cost function is implemented as ordinary Mean Squared
Error (MSE) between data and predicted cumulative consumption. For parameter optimization Adam [13] together
with an exponential decay9 is used. The training is performed single-core on CPU10 and takes ‚âà 5 hours. During
interpretation of results, please note the small amount of data used for training: A single driving cycle measurement
trajectory (mean over two real experiments).

P R E P RIN T

Figure 10: Cumulative consumption prediction on the CADC (Road), that is part of training data. The NeuralFMU
(blue) lays almost on the training data mean (green) and inside the data uncertainty region (green, translucent). On the
other hand, the simulation results of the original FPM as FMU (orange) slowly drifts out the data uncertainty region,
resulting in a relative large error at the simulation stop time compared to the NeuralFMU.

9Initial step size: 1e ‚àí 3, Decay (new step size multiplier): 0.95 every step, Min. step size: 1e ‚àí 5.
10Intel¬Æ CoreTM i7-8565U on Windows 10 Enterprise 20H2

12

PREPRINT

Figure 11: Deviation on the last 10 % of the cumulative consumption prediction on the CADC (Road), that is part of
training data. The Ô¨Ånal consumption prediction accuracy of the NeuralFMU (blue) signiÔ¨Åcantly increases compared to
the original FMU (orange), lays inside the measurement uncertainty (green, translucent) and close to the data mean
(green). The original FPM prediction lays completely outside of the measurement uncertainty.

P R E P RIN T

Figure 12: Deviation (absolute error) of the consumption prediction between data and the NeuralFMU (blue) compared
to the original FPM as FMU (orange) on the CADC (Road), that was part of training data. After ‚âà 300 s, the
NeuralFMU solution lays inside the data uncertainty region (green, translucent) and outperforms the FPM in terms of
prediction accuracy. Further, the NeuralFMU error is at any time step signiÔ¨Åcantly smaller than the mean error of the
FMU (orange, dashed).

13

3.3.2 Testing (Validation)

PREPRINT

After training, the NeuralFMU is validated against unknown data by simulating another driving cycle, the WLTC (class
2), that is not known from training. Results and explanations can be seen in Figures 13, 14 and 15.

Figure 13: Comparison of the cumulative consumption prediction, using the WLTC (class 2), that is not part of the
NeuralFMU training data. As for training data, the NeuralFMU prediction (blue) is closer to the measurement data
mean (green), compared to the original FPM prediction (orange).

P R E P RIN T

Figure 14: Deviation on the last 10 % of the simulation trajectory of the NeuralFMU (blue) compared to the original
FPM as FMU (orange) and experimental data mean (green). The unknown WLTC (class 2) is used for testing. The
NeuralFMU prediction is much closer to the data mean than the original FPM and predicts a Ô¨Ånal value inside of the
measurement uncertainty.

14

PREPRINT

Figure 15: Absolute error of the consumption prediction of the NeuralFMU (blue) compared to the original FPM as
FMU (orange). The unknown WLTC (class 2) is used for testing. Even if the NeuralFMU solution (blue) does not
always lay inside the data uncertainty region (green), it leads to a much better prediction compared to the original FPM,
that in contrast almost completely misses the data uncertainty area (green, translucent).

P R E P RIN T

consumption on entire cycles, especially the Ô¨Ånal value of the solution is important and a key factor of model evaluation.
Please note, that this Ô¨Ånal error of the NeuralFMU is more than eight times smaller on training data and even 16
times smaller on validation data. Besides the Ô¨Ånal error, the NeuralFMU features a much lower MSE (factor ‚âà 63 on
training, factor ‚âà 16 on testing) and maximum error (factor ‚âà 7 on training, factor ‚âà 3 on testing), but the simulation
time increases about Ô¨Åve to six times. This is mainly because of the more expensive event-handling inside the hybrid
structure and further unused performance optimizations in the prototypical implementation. It can be seen, that the
number of events remains unchanged. The number of adaptive solver steps only slightly increases, which indicates that
the average system stiffness11 hardly changes. Finally, the training is not converged yet and further training epochs or
training on more data (e.g. multiple cycles) may reduce the remaining deviations.

For training as well as for testing data, the NeuralFMU solution leads to a smaller MSE, maximum error and Ô¨Ånal
error than the solution of the original FPM. For the training cycle the NeuralFMU solution proceeds inside of the
measurement uncertainty, which is a great success. Also for testing data, the NeuralFMU increases prediction accuracy
compared to the FPM, but leaves the data uncertainty region in some sections. A detailed overview of the training and
testing results can be seen in Table 2. Because longitudinal dynamics models are often used to predict the cumulative

Table 2: Training and testing results with solver Tsit5 after 18 training epochs. Errors are calculated against the data
mean of two chassis dynamometer runs.

11In this article, an ODE is considered stiff if the adaptive step size of the solver is controlled authoritative by the stability objective,

CADC
(Road)
CADC
(Road)
WLTC
(class 2)
WLTC
(class 2)

max. error
[W s]

Ô¨Ånal error
[W s]

sim. time
[s]

triggered
events

MSE
[W 2s2]

NeuralFMU

NeuralFMU

solver steps

-248286.71

-453614.80

249693.13

460185.79

-55610.81

15463.58

79822.16

63558.61

588.91e8

89.82e8

144590

144569

144519

144519

110301

110294

110247

110247

5.67e8

9.30e8

Model

Cycle

13.16

69.54

10.29

55.09

FMU

FMU

instead of the tolerance objective.

15

4 Conclusion

PREPRINT

Funding

The brieÔ¨Çy highlighted open source library FMI.jl allows the easy and seamless integration of FMUs into the Julia
programming language. FMUs can be loaded, parameterized, simulated and exported using the abilities of the FMI
standard. Optional functions like retrieving the partial derivatives or manipulating the FMU state are available if
supported by the FMU. The current library release version is compatible with FMI 2.0 (the common version at the time
of release) and initial support for FMI 3.0 is implemented. The library currently supports ME- as well as CS-FMUs,
running on Windows and Linux operating systems. Event-handling to simulate discontinuous ME-FMUs is supported.
The library extension FMIFlux.jl makes FMUs differentiable and opens the possibility to setup and train NeuralFMUs.
The framework supports proper event-handling during back-propagation whilst training of discontinuous NeuralFMUs.
Beside NeuralFMUs, FMIFlux.jl paves the way for other hybrid modeling techniques and new industrial use cases by
making FMUs differentiable in an AD-framework. The library repositories are constantly extended by new features and
maintained for the upcoming technology progress. Contributors are welcome.

We highlighted a workÔ¨Çow to allow for hybrid modeling with an industry typical FPM in form of a NeuralFMU.
Before training such models, a proper initialization is required. Because initialization of NeuralFMUs is not trivial, we
suggested three methods with different requirements: NIPT, CCPT and a topology using ANN/FMU gates, that makes
an initialization routine obsolete. The use of this topology was tested in practice using an industry typical FPM, the
VLDM. This model features multiple challenges like closed-loops and high-frequent discontinuities. The VLDM was
exported in a format that is common in industrial practice, the FMI. On the foundation of the exported FMU, a hybrid
model was built up and trained on real measurement data from a chassis dynamometer, including typical measurement
errors. The model was trained on only a single driving cycle measurement to show that the presented method is capable
of making good predictions on very little data. The trained hybrid model was able to make better predictions compared
to the FPM, on training as well as on testing data. To conclude, the presented workÔ¨Çow and software allows for the
re-use of already existing industrial models as cores of hybrid models in the form of NeuralFMUs. NeuralFMUs
allow for the data-driven modeling of physical effects, that are difÔ¨Åcult to model based on Ô¨Årst principle, by using the
presented methods and techniques.

P R E P RIN T

The used software FMI.jl and FMIFlux.jl is available open-source under https://github.com/ThummeTo/FMI.jl
and https://github.com/ThummeTo/FMIFlux.jl. The data used in the presented experiment (vehicle measure-
ments) as well as the original model are available for download under https://github.com/TUMFTM/Component_
Library_for_Full_Vehicle_Simulations. Further, a tutorial for reconstruction of the presented method focusing
on adapting custom use cases will be released soon after this article‚Äôs publication in the repository of FMIFlux.jl.

This research was partially funded by the ITEA3-Project UPSIM (Unleash Potentials in Simulation) N¬∞19006, see:
https://www.upsim-project.eu/ for more information.

The authors like to thank everyone that contributed to the library repositories, especially our students and student
assistants Josef Kircher, Jonas Wilfert and Adrian Brune.

BNSDE Bayesian Neural Stochastic Differential Equation

CADC Common Artemis Driving Cycles

ANN ArtiÔ¨Åcal Neural Network

AD Automatic Differentiation

Acknowledgments

Data availability

Abbreviations

CCPT Collocation Pre-Training

16

CS Co-Simulation

PREPRINT

ME Model Exchange

ML Machine Learning

SE Scheduled Execution

MSE Mean Squared Error

HiL Hardware in the Loop

FPM First Principle Model

FMU Functional Mock-up Unit

RNN Recurrent Neural Network

FMI Functional Mock-up Interface

ODE Ordinary Differential Equation

NEDC New European Driving Cycle

PAC Probably Approximately Correct

DARN Deep Auto-Regressive Network

NIPT Neutral Initialization Pre-Training

PINN Physics-informed Neural Network

P R E P RIN T

[5] Benedikt Danquah et al. ‚ÄúModular, Open Source Simulation Approach: Application to Design and Analyze
Electric Vehicles‚Äù. In: 2019 Fourteenth International Conference on Ecological Vehicles and Renewable Energies
(EVER). IEEE, 8.05.2019 - 10.05.2019, pp. 1‚Äì8. ISBN: 978-1-7281-3703-2. DOI: 10 . 1109 / EVER . 2019 .
8813568.

Jeff Bezanson et al. ‚ÄúJulia: A Fast Dynamic Language for Technical Computing‚Äù. In: CoRR abs/1209.5145
(2012). arXiv: 1209.5145. URL: http://arxiv.org/abs/1209.5145.
Jeff Bezanson et al. ‚ÄúJulia: A Fresh Approach to Numerical Computing‚Äù. In: CoRR abs/1411.1607 (2015). arXiv:
1411.1607. URL: http://arxiv.org/abs/1411.1607.

[8] Lino Guzzella and Antonio Sciarretta. Vehicle Propulsion Systems: Introduction to Modeling and Optimization.
3. 3rd ed. 2013. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. ISBN: 978-3-642-35913-2. URL: http:
//nbn-resolving.org/urn:nbn:de:bsz:31-epflicht-1554525.

[4] Tian Qi Chen et al. ‚ÄúNeural Ordinary Differential Equations‚Äù. In: CoRR abs/1806.07366 (2018). arXiv: 1806.

[7] Karol Gregor et al. ‚ÄúDeep AutoRegressive Networks‚Äù. In: (2013). DOI: 10.48550/ARXIV.1310.8499. URL:

[6] Hilding Elmqvist, Andrea Neumayr, and Martin Otter. ‚ÄúModia - Dynamic Modeling and Simulation with Julia‚Äù.

[3] Frederic Bruder and Lars Mikelsons. ‚ÄúModia and Julia for Grey Box Modeling‚Äù. In: Sept. 2021, pp. 87‚Äì95. DOI:

In: Juliacon 2018. 2018. URL: https://elib.dlr.de/124133/.

WLTC Worldwide harmonized Light-duty vehicles Test Cycle

07366. URL: http://arxiv.org/abs/1806.07366.

VLDM Vehicle Longitudinal Dynamics Model

SSP System Structure and Parameterization

https://arxiv.org/abs/1310.8499.

10.3384/ecp2118187.

References

[1]

[2]

17

[9] Manuel Haussmann et al. ‚ÄúLearning Partially Known Stochastic Dynamics with Empirical PAC Bayes‚Äù. In:

PREPRINT

(2021). arXiv: 2006.09914 [cs.LG].

1412.6980. URL: https://arxiv.org/abs/1412.6980.

arXiv: 1810.07951. URL: http://arxiv.org/abs/1810.07951.

abs/1902.02376 (2019). arXiv: 1902.02376. URL: http://arxiv.org/abs/1902.02376.

rep. Link√∂ping: Modelica Association, May 2022. URL: https://fmi-standard.org/docs/3.0/.

[17] Christopher Rackauckas et al. ‚ÄúDiffEqFlux.jl - A Julia Library for Neural Differential Equations‚Äù. In: CoRR

[10] Michael Innes. ‚ÄúDon‚Äôt Unroll Adjoint: Differentiating SSA-Form Programs‚Äù. In: CoRR abs/1810.07951 (2018).

[15] Modelica Association. Functional Mock-up Interface SpeciÔ¨Åcation. Document version: 3.0, 2022-05-10. Tech.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. 2014. DOI: 10.48550/ARXIV.

[18] Rahul Rai and Chandan K. Sahu. ‚ÄúDriven by Data or Derived Through Physics? A Review of Hybrid Physics
Guided Machine Learning Techniques With Cyber-Physical System (CPS) Focus‚Äù. In: IEEE Access 8 (2020),
pp. 71050‚Äì71073. DOI: 10.1109/ACCESS.2020.2987324.

[16] Modelica Association. System Structure and Parameterization. Document version: 1.0. Tech. rep. Link√∂ping:
Modelica Association, Mar. 2019. URL: https : / / ssp - standard . org / publications / SSP10 /
SystemStructureAndParameterization10.pdf.

[14] Modelica Association. Functional Mock-up Interface for Model Exchange and Co-Simulation. Document version:
2.0.2. Tech. rep. Link√∂ping: Modelica Association, Dec. 2020. URL: https://github.com/modelica/fmi-
standard/releases/download/v2.0.2/FMI-Specification-2.0.2.pdf.

[11] Mike Innes et al. ‚ÄúA Differentiable Programming System to Bridge Machine Learning and ScientiÔ¨Åc Computing‚Äù.
In: CoRR abs/1907.07587 (2019). arXiv: 1907.07587. URL: http://arxiv.org/abs/1907.07587.
[12] R. E. Kalman. ‚ÄúA New Approach to Linear Filtering and Prediction Problems‚Äù. In: Journal of Basic En-
gineering 82.1 (Mar. 1960), pp. 35‚Äì45. ISSN: 0021-9223. DOI: 10 . 1115 / 1 . 3662552. eprint: https : / /
asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.
pdf. URL: https://doi.org/10.1115/1.3662552.

P R E P RIN T

[19] M. Raissi, P. Perdikaris, and G.E. Karniadakis. ‚ÄúPhysics-informed neural networks: A deep learning framework
for solving forward and inverse problems involving nonlinear partial differential equations‚Äù. In: Journal of
Computational Physics 378 (2019), pp. 686‚Äì707. ISSN: 0021-9991. DOI: https://doi.org/10.1016/j.jcp.
2018.10.045. URL: https://www.sciencedirect.com/science/article/pii/S0021999118307125.
J. Revels, M. Lubin, and T. Papamarkou. ‚ÄúForward-Mode Automatic Differentiation in Julia‚Äù. In:
arXiv:1607.07892 [cs.MS] (2016). URL: https://arxiv.org/abs/1607.07892.

Series‚Äù. In: IEEE Control Systems Letters 6 (2022), pp. 1897‚Äì1902. DOI: 10.1109/LCSYS.2021.3135835.
Jared Willard et al. ‚ÄúIntegrating Physics-Based Modeling with Machine Learning: A Survey‚Äù. In: (2020). arXiv:
2003.04919 [physics.comp-ph].

[22] Tobias Thummerer, Lars Mikelsons, and Josef Kircher. ‚ÄúNeuralFMU: towards structural integration of FMUs
into neural networks‚Äù. In: Proceedings of 14th Modelica Conference 2021, Link√∂ping, Sweden, September 20-24,
2021. Ed. by Martin Sj√∂lund et al. 2021. ISBN: 978-91-7929-027-6. DOI: 10.3384/ecp21181297.

[21] Elisabeth Roesch, Chris Rackauckas, and Michael Stumpf. ‚ÄúCollocation based training of neural ordinary
differential equations‚Äù. In: Statistical Applications in Genetics and Molecular Biology 20 (July 2021). DOI:
10.1515/sagmb-2020-0025.

[23] Tobias Thummerer, Johannes Tintenherr, and Lars Mikelsons. ‚ÄúHybrid modeling of the human cardiovascular
system using NeuralFMUs‚Äù. In: Journal of Physics: Conference Series 2090.1 (2021), p. 012155. DOI: 10.1088/
1742-6596/2090/1/012155.

[24] Evren Mert Turan and Johannes J√§schke. ‚ÄúMultiple Shooting for Training Neural Differential Equations on Time

[25]

[20]

18

