9
1
0
2

y
a
M
4
2

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
9
5
3
1
1
.
5
0
9
1
:
v
i
X
r
a

Large-Scale Statistical Survey of
Magnetopause Reconnection

Computer Science
University of New Hampshire
Durham, New Hampshire, USA

Samantha Piatt

May 7, 2019

Abstract

The Magnetospheric Multiscale Mission (MMS) seeks to study the micro-
physics of reconnection, which occurs at the magnetopause boundary layer
between the magnetosphere of Earth and the interplanetary magnetic ﬁeld
originating from the sun. Identifying this region of space automatically will
allow for statistical analysis of reconnection events. The magnetopause
region is difﬁcult to identify automatically using simple models, and time
consuming for scientists to classify by hand. We introduced a hierarchi-
cal Bayesian mixture model with linear and auto regressive components
to identify the magnetopause. Using data from the MMS mission with the
programming languages R and Stan, we modeled and predicted possible
regions and evaluated our performance against a boosted regression tree
model. Our model selects twice as many magnetopause regions as the
comparison model, without signiﬁcant over selection, achieving a 31% true
positive rate and 93% true negative rate. Our method will allow scientists
to study the micro-physics of reconnection events in the magnetopause
using the large body of MMS data without manual classiﬁcation.

 
 
 
 
 
 
Figure 1: Magnetic ﬁelds.

Introduction

Magnetic reconnection is an important phenomenon in physics, which hap-
pens on a large and small scale. The Magnetospheric Multiscale (MMS)
project aims to study the micro-physics of magnetic reconnection. It con-
sists of four satellites orbiting the Earth in a tetrahedral formation, captur-
ing 3-dimensional data with multiple instruments to record magnetic ﬁeld,
ion, and electron data.

Because the micro physics of reconnection are hard to identify in low
resolution data, MP regions (which are easier to characterize), are selected
instead. During the selection process, a scientist-in-the-loop (SITL) veriﬁes
the selections made by on-board algorithms in low-resolution data, con-
ﬁrming or adjusting selected regions that they believe are of interest [1].
The selections are then transmitted to Earth in high-resolution, depending
on bandwidth available. Each selection also receives a priority code as
well as a comment to note what the SITL believed made this time region
signiﬁcant.

A set of guidelines make this process consistent and standard between
different scientists, but variations from subjectivity exist. Additionally, se-
lections are subject to bandwidth constraints, and therefore SITLs take this
into account when prioritizing which data are to be transmitted in high res-

Page 2

olution. The manpower and subjectivity inherent in selecting interesting
regions present a problem that can be solved with machine learning.

We generated a model that gives any scientist looking to study recon-
nection, or the magnetopause, a more automated and statistically mean-
ingful way of selecting these important regions of interest. Reconnection
events happen in either the dayside magnetopause (MP) or the night-side
magentotail – herein, we focus on the MP events only. Our model attempts
to identify the MP boundary layer, an arbitrary boundary between the Mag-
netosphere (Earth’s magnetic ﬁeld) and the magnetosheath (solar wind
facing side of the boundary layer). Each satellite captured data relevant
to the MP during phase 1 of their mission, March 2015 through February
2017.

Methods

Data for training and testing was sourced from only one of the four satel-
lites during a period of 2 months from January through February 2017.
Speciﬁcally, the data consists of readings from the Dual Ion Spectrome-
ter (DIS) and Dual Electron Spectrometer (DES) [3], as well as the Fluxgate
Magnetometer (FGM) [4, 6], which together record the magnetic ﬁeld and
ion density, temperature, and pressure. Data from the slow survey is in-
terpolated to that of the DES at 4.5 second intervals. This subset of data
contains 395,458 rows, with 246 excluded due to missing values.

Modeling in Stan and R

The raw data is parsed by removing empty elements, grouping by effective
orbit, and then split into test and training sets. Data was pre-processed in
R by segmenting different orbits by identifying gaps in timestamp values
greater than 100 seconds. 14 orbits were selected randomly, with 80% of
those orbits used for training data and 20% used for test data. This results
in 119,910 data points for training and 33,217 data points for testing, totaling
153,127 rows of data.

We use several values in the low-resolution data from the DIS and FGM
instruments in our model. The raw data is parsed by removing empty el-
ements and grouping by effective orbit. We generate the θCA and Ti as in

Page 3

equations 1 and 2, and then transform ni and Ti by taking their natural log
values.

1
3

Ti =

(cid:0)Ti,(cid:107) + 2Ti,⊥
θCA = tan−1 By/Bz

(cid:1)

(1)

(2)

Our model was written in Stan for both training and testing phases.
Stan is both a probabilistic programming language and a sampler that does
Bayesian inference. It uses variations of gradient descent for sampling and
optimization, and can run on command line, as well as through R or Python
[5]. For our model, 100 iterations were used for both training and testing,
on one core. Training and testing model code is included in the appendix,
Listings 1 and 2.

Mixture Model

The probability density distributions of the 395,458 values for each vari-
able in our data set are generally bi-modal. Therefore, we can characterize
the signals from either ﬁeld by attributing their inﬂuence as a mixture of
two normal distributions, representing the contributions from the magne-
tosheath (MSH) and magnetosphere (MSP) ﬁelds. When the satellite is in
the MSH, data is normally distributed around µM SH, with some standard
error σM SH. In the MSP it is the same, with µM SP and σM SP . This allows us
to, for any data point, establish the likelihood that the satellite is in either
the MSH or MSP, based solely on the probability that a data value comes
from either normal distribution.

This is extremely helpful in identifying the MP region, the boundary where
the inﬂuences of both ﬁelds blend. Data points with equal, but low likeli-
hood of being in either ﬁeld will have a high likelihood of belonging to the
MP region. We introduce a mixture model that takes the prior probability
density functions, or normal distributions, to estimate a mixing proportion
λ—the likelihood of being in either ﬁeld (Eq. 3).

Most of the variables we will use in our model can be estimated this
way. One variable, the Clock Angle (θCA, Figure 2d), cannot be estimated as
a mixture of two normal distributions based on its inherent qualities. Any
time the satellite is in the MSP, the θCA tends to remain close to 0 degrees.

Page 4

(a)

(b)

(c)

(d)

Figure 2: Density Distributions. Red lines represent distributions associ-
ated with the MSH region, where as Blue lines represent those associated
with the MSP.

Page 5

When the satellite is in the MSH region, θCA can rotate anywhere from π to
−π. This makes the θCA a mixture of normal and uniform distributions, as
in Eq. 4.

x ∼ λ N ormal(µM SH, σM SH) + (1 − λ) N ormal(µM SP , σM SP )
xθCA ∼ λ U nif orm(−π, π) + (1 − λ) N ormal(µM SP , σM SP )

(3)
(4)

In both Eq. 3 and Eq. 4, the variable being estimated by Stan is the λ
mixture ratio. When this ratio is close to 0, the likelihood that the satellite
is in the MSP is very small, but it is very likely to be in the MSH. The reverse
is true when the ratio is near 1, but more importantly, when the ratio is near
0.5 there is equal likelihood that the satellite is in either the MSH or MSP. It
can be concluded that there is a high likelihood of being in the MP boundary
layer when λ mixing ratios are close to 0.5.

Prior distribution µ and σ values, used in the mixutre model, were gener-
ated using the normalmixEM method of the mixtools 1.0.4 R library, with k
= 2 mixtures. This produced estimations using expectation maximization.
For Bt and θCA, the µ and σ values were adjusted manually for better pre-
dictive performance. The resulting values, represented by the red and blue
lines in Figure 2, were passed with the data to our model. All other α, σ, λ,
and β values in equations 3, 4, 5, and 6 are estimated internally by stan.

Auto Regression Analysis

Intuitively, points in this data set are not independent from one another.
As the satellite moves through space, its location is directly dependent
on where it was before. Each variable exhibits non-stationarity, where the
probability distribution changes based on which ﬁeld the satellite is in or
transitioning to at a given time. The data behaves as a Gaussian random
walk, with each time point normally distributed around that of the last with
a varying mean and standard deviation. This is veriﬁed by analyzing the
auto correlation of each variable at different lag points. Figure 3a illus-
trates the non-stationarity of the variables (Bt for example), and the scat-
ter plots show the linear relationship of each time point against that of the
next (Figure 3b), at a lag of one. Further, once the data is made stationary
by generating the difference at lag one, we can reanalyze the correlation

Page 6

(a)

(b)

(c)

(d)

Figure 3: Auto Regression Analysis. Plot A shows auto correlation values
of Bt at different lag times. C shows the same ACF plot, with Bt stationary.
Plots B and D show Bt and the resulting λBt mixing ratios plotted against
their lagged values, times t on the y axis and times t + 1 on the x axis.
Additional plots for other variables are included in the appendix.

Page 7

of each time point again (Figure 3c). Since there is no further relationship
(Figure 3d), we conclude that lag one is appropriate for this model [2].

λt ∼ N ormal(λt−1, σ)

(5)

Therefore, an auto-regressive component is added to our model to ac-
count for this aspect of the data, using Eq. 5. Each mixture ratio λ at any
time point is normally distributed around that of the last. Because the same
behavior is exhibited in the resulting mixing ratio as seen in Figure 3, we ap-
ply the auto regression to the mixing ratio λ. This has the effect of smooth-
ing out λ, reducing the noise generated by the latent variable of time.

Linear Regression

In order to retrieve a meaningful prediction, we regress on all resulting λ val-
ues. This produces a number we can use to generate selections. The SITL
provides a priority value, "Figure of Merit" (FOM), that can be used to train
and test against. During January-February 2017, the SITLS attributed FOM
values of 100 or more to MP regions. Eq. 6 models this priority variable us-
ing the λ mixing ratios for each variable. The resulting priority predictions
of 100 or more are classiﬁed as selections, to match what a SITL might
assign as originating from the MP boundary layer.

P riority ∼α + βF GMBt λF GMBt + βDISN λDISN +
βDIST λDIST + βθCA λθCA

(6)

Evaluation and Results

We evaluated our model by comparing its performance against a basic tree
boosting model. In addition to the true-positive rate, we evaluate with false-
positive, miss-classiﬁcation, and f-score. The true-positive rate reﬂects
each model’s ability to select points in the MP, as noted by the SITL, while
the true-negative rate reﬂects each model’s ability to ignore areas that the
SITL did not note as being in the MP. Miss-classiﬁcation shows how many
points mismatched overall, and f-score (or harmonic mean) gives a holis-
tic view of how balanced predictions are when compared to selections.

Page 8

Our Model
Boosting
Null Error

Miss-class. True-positive True-Negative F-Score
32.31%
18.14%
0.00%

93.02%
95.40%
100.00%

31.35%
14.10%
00.00%

13.16%
12.74%
10.02%

Table 1: Evaluation results. Plots for the two other obits present in test data
are included in the appendix.

Because SITL selections do not always completely encompass the MP re-
gion, and are somewhat subjective for each scientist, the SITL selections
are considered a baseline for evaluation. Therefore, each evaluation metric
on its own will not be a perfect indicator of the performance of models with
this data set and goal. The null error performance is provided as a baseline,
indicating what would happen if the dominant class were predicted 100%
of the time (in this case, no MP selections).

For predictions using Gradient Boosting Machines (gbm), data was clas-
siﬁed as selected or not for any FOM priority value of 100 or greater. Both
models used the same four variables as well as the same test and training
data set. The ’boosting’ model was run using R’s gbm library with a maxi-
mum tree depth of 4, a maximum trees of 1000, and a Bernoulli distribution
(giving binary 0/1 predictions using logistic regression).

Our model performs better or comparable on all four of the evaluation
metrics outlined. Of note is our model’s ability to predict two times as many
MP regions, as identiﬁed by the SITL. This is without additional over selec-
tion and a competitively low miss-classiﬁcation rate. We can also see, vi-
sually, that we are selecting regions that tend to align with shifts in values,
as visible in Figure 4.

Conclusions and Discussion

Our model performs exceptionally well based on evaluation metrics and
visual inspection of its resulting selections. Moreover, through this pro-
cess we have gathered meaningful information about the behavior of this
data over time, the distribution of data, as well as the contribution of each
feature variable we used to identify the MP boundary layer.

Page 9

Figure 4: Resulting mixture ratios and predicted priorities for a single orbit,
out of three test orbits.

Page 10

The resulting beta values from the linear regression component indi-
cate which mixing ratio contributes the most to accurate predictions. Be-
cause of the high evaluation scores of our model we can say that, of the
four variables used (Bt, ni, Ti, θCA), the Bt values tend to be the most in-
dicative of being in the MP region. Ti also contributes meaningfully to pre-
dictions.

As mentioned before, while MP selections from the SITL are not incor-
rect, they may not completely encompass the entire MP region because
of bandwidth constraints. Therefore, SITL selections tend to under select
the MP region, which makes it an imperfect gold standard for this partic-
ular goal. We are still working on developing a way to more appropriately
evaluate performance using SITL priority data.

In addition to developing a more robust evaluation metric, we would
like to increase our model’s performance by adding more variables, trying
different ways of combining λ mixing ratios, and ﬁne-tuning hyper param-
eters. There are several variables that we were not able to incorporate into
our model because of time and processing constraints. These variables
could increase our ability to accurately select more MP boundary layers.
Additionally, while linear regression allows us to see what mixture ratios
are more useful to predictions, this function might not be the best ﬁt with
our goals. Processing time also limited our ability to do cross validation
in order to tune model hyper parameters, such as selection threshold. Our
naïve approach of selecting a threshold of 100, following what the SITL
might give MP regions, works moderately well on most data but we could
perform better with tuning.

Our model selects twice as many MP regions than the comparative
model, allowing scientists to use our selections to study the MP boundary
layer and the reconnection events therein. Additionally, our model lays the
foundation for machine learning’s application to MMS data and has huge
potential to foster new discoveries to reconnection and the interaction be-
tween the solar wind and planetary magnetospheres.

Page 11

References

[1] S A Fuselier, W S Lewis, C Schiff, R Ergun, J L Burch, S M Petrinec, and
K J Trattner. Magnetospheric Multiscale Science Mission Proﬁle and
Operations. Space Science Reviews, 199(1):77–103, mar 2016.

[2] Rob J. Hyndman and George Athanasopoulos. Forecasting: principles

and practice. OTexts, 2018.

[3] C Pollock, T Moore, A Jacques, J Burch, U Gliese, Y Saito, T Omoto,
L Avanov, A Barrie, V Coffey, J Dorelli, D Gershman, B Giles, T Ros-
nack, C Salo, S Yokota, M Adrian, C Aoustin, C Auletti, S Aung, V Bi-
gio, N Cao, M Chandler, D Chornay, K Christian, G Clark, G Collinson,
T Corris, A De Los Santos, R Devlin, T Diaz, T Dickerson, C Dickson,
A Diekmann, F Diggs, C Duncan, A Figueroa-Vinas, C Firman, M Free-
man, N Galassi, K Garcia, G Goodhart, D Guererro, J Hageman, J Han-
ley, E Hemminger, M Holland, M Hutchins, T James, W Jones, S Kreisler,
J Kujawski, V Lavu, J Lobell, E LeCompte, A Lukemire, E MacDonald,
A Mariano, T Mukai, K Narayanan, Q Nguyan, M Onizuka, W Paterson,
S Persyn, B Piepgrass, F Cheney, A Rager, T Raghuram, A Ramil, L Re-
ichenthal, H Rodriguez, J Rouzaud, A Rucker, Y Saito, M Samara, J.-A.
Sauvaud, D Schuster, M Shappirio, K Shelton, D Sher, D Smith, K Smith,
S Smith, D Steinfeld, R Szymkiewicz, K Tanimoto, J Taylor, C Tucker,
K Tull, A Uhl, J Vloet, P Walpole, S Weidner, D White, G Winkert, P.-S.
Yeh, and M Zeuch. Fast Plasma Investigation for Magnetospheric Mul-
tiscale. Space Science Reviews, 199(1):331–406, mar 2016.

[4] C T Russell, B J Anderson, W Baumjohann, K R Bromund, D Dearborn,
D Fischer, G Le, H K Leinweber, D Leneman, W Magnes, J D Means, M B
Moldwin, R Nakamura, D Pierce, F Plaschke, K M Rowe, J A Slavin, R J
Strangeway, R Torbert, C Hagen, I Jernej, A Valavanoglou, and I Richter.
The Magnetospheric Multiscale Magnetometers. Space Science Re-
views, 199(1):189–256, mar 2016.

[5] Stan Development Team. Stan Modeling Language Users Guide, Ver-

sion 2.18.0, 2018.

[6] R B Torbert, H Vaith, M Granoff, M Widholm, J A Gaidos, B H Briggs, I G
Dors, M W Chutter, J Macri, M Argall, D Bodet, J Needell, M B Steller,

Page 12

W Baumjohann, R Nakamura, F Plaschke, H Ottacher, J Hasiba, K Hof-
mann, C A Kletzing, S R Bounds, R T Dvorsky, K Sigsbee, and V Kooi. The
Electron Drift Instrument for MMS. Space Science Reviews, 199(1):283–
305, mar 2016.

Page 13

Appendix

Figure 5: Auto Correlation Plots of raw variables and stationary variables.
Page 14

Figure 6: Lag one variable plots.

Page 15

Figure 7: Test data predictions from model.

Page 16

Figure 8: Test data predictions from model.

Page 17

Listing 1: Stan Training Model

1 data{
2

int numsteps;

// Mix Prior Vectors: mu[MSH,MSP], sigma[MSH,MSP], theta
vector[5] N_mix;
vector[5] T_mix;
vector[5] Bt_mix;

// mu[MSH], sigma[MSH], Min[MSP], max[MSP]
vector[4] Clock_mix;

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

22

23

24

25

26

27

28

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

55

vector[numsteps] By;
vector[numsteps] Bz;
vector[numsteps] Bt;
vector[numsteps] N;
vector[numsteps] T_perp;
vector[numsteps] T_para;
vector<lower=0, upper=255>[numsteps] Priority;

18
19 }
20 transformed data{
21

vector[numsteps] N_log;
vector[numsteps] T_log;
vector[numsteps] Clock_Angle;

for (i in 1:numsteps){
N_log[i] = log(N[i]);
T_log[i] = log((T_para[i] + 2 * T_perp[i]) / 3);
Clock_Angle[i] = atan2(By[i], Bz[i]);

}

29
30 }
31 parameters{
32

// Mixture Model
vector<lower=0, upper=1>[numsteps] Bt_Mixture;
vector<lower=0, upper=1>[numsteps] N_Mixture;
vector<lower=0, upper=1>[numsteps] T_Mixture;
vector<lower=0, upper=1>[numsteps] Clock_Mixture;
real<lower=0> Clock_sigma;

// Auto Regression
real<lower=0> Bt_mix_sigma;
real<lower=0> N_mix_sigma;
real<lower=0> T_mix_sigma;
real<lower=0> Clock_mix_sigma;

// Linear Regression
real mixture_alpha;
real<lower=0> mixture_sigma;
real Bt_beta;
real N_beta;
real T_beta;
real Clock_beta;

51
52 }
53 model{
54

// Mixture model
for (n in 1:numsteps){

Page 18

target += log_mix(Bt_Mixture[n],
normal_lpdf(Bt[n] | Bt_mix[1], Bt_mix[3]),
normal_lpdf(Bt[n] | Bt_mix[2], Bt_mix[4]));

target += log_mix(N_Mixture[n],
normal_lpdf(N_log[n] | N_mix[1], N_mix[3]),
normal_lpdf(N_log[n] | N_mix[2], N_mix[4]));

target += log_mix(T_Mixture[n],
normal_lpdf(T_log[n] | T_mix[1], T_mix[3]),
normal_lpdf(T_log[n] | T_mix[2], T_mix[4]));

target += log_mix(Clock_Mixture[n],
uniform_lpdf(Clock_Angle[n] | Clock_mix[3], Clock_mix[4]),
normal_lpdf(Clock_Angle[n] | Clock_mix[1], Clock_sigma));

}

// Auro-Regression
for (n in 2:numsteps){

Bt_Mixture[n] ~ normal(Bt_Mixture[n-1], Bt_mix_sigma);
N_Mixture[n] ~ normal(N_Mixture[n-1], N_mix_sigma);
T_Mixture[n] ~ normal(T_Mixture[n-1], T_mix_sigma);
Clock_Mixture[n] ~ normal(Clock_Mixture[n-1], Clock_mix_sigma);

}

// Linear Regression
Priority ~ normal(mixture_alpha + Bt_beta * Bt_Mixture +
N_beta * N_Mixture + T_beta * T_Mixture +
Clock_beta * Clock_Mixture, mixture_sigma);

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84
85 }

1 data{
2

int numsteps;

Listing 2: Stan Testing Model

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

// Mix Prior Vectors: mu[MSH,MSP], sigma[MSH,MSP], theta
vector[5] N_mix;
vector[5] T_mix;
vector[5] Bt_mix;

// mu[MSH], sigma[MSH], Min[MSP], max[MSP]
vector[4] Clock_mix;
real<lower=0> Clock_sigma;

// Vector values
vector[numsteps] By;
vector[numsteps] Bz;
vector[numsteps] Bt;
vector[numsteps] N;
vector[numsteps] T_perp;
vector[numsteps] T_para;

// Auto Regression
real<lower=0> Bt_mix_sigma;
real<lower=0> N_mix_sigma;

Page 19

real<lower=0> T_mix_sigma;
real<lower=0> Clock_mix_sigma;

// Linear Regression
real mixture_alpha;
real<lower=0> mixture_sigma;
real Bt_beta;
real N_beta;
real T_beta;
real Clock_beta;

33
34 }
35 transformed data{
36

vector[numsteps] N_log;
vector[numsteps] T_log;
vector[numsteps] Clock_Angle;

for (i in 1:numsteps){
N_log[i] = log(N[i]);
T_log[i] = log((T_para[i] + 2 * T_perp[i]) / 3);
Clock_Angle[i] = atan2(By[i], Bz[i]);

}

44
45 }
46 parameters{
47

// Mixture Model
vector<lower=0, upper=1>[numsteps] Bt_Mixture;
vector<lower=0, upper=1>[numsteps] N_Mixture;
vector<lower=0, upper=1>[numsteps] T_Mixture;
vector<lower=0, upper=1>[numsteps] Clock_Mixture;

51
52 }
53 model{
54

// Mixture model
for (n in 1:numsteps){

target += log_mix(Bt_Mixture[n],
normal_lpdf(Bt[n] | Bt_mix[1], Bt_mix[3]),
normal_lpdf(Bt[n] | Bt_mix[2], Bt_mix[4]));

target += log_mix(N_Mixture[n],
normal_lpdf(N_log[n] | N_mix[1], N_mix[3]),
normal_lpdf(N_log[n] | N_mix[2], N_mix[4]));

target += log_mix(T_Mixture[n],
normal_lpdf(T_log[n] | T_mix[1], T_mix[3]),
normal_lpdf(T_log[n] | T_mix[2], T_mix[4]));

target += log_mix(Clock_Mixture[n],
uniform_lpdf(Clock_Angle[n] | Clock_mix[3], Clock_mix[4]),
normal_lpdf(Clock_Angle[n] | Clock_mix[1], Clock_sigma));

}

// Auro-Regression
for (n in 2:numsteps){

Bt_Mixture[n] ~ normal(Bt_Mixture[n-1], Bt_mix_sigma);
N_Mixture[n] ~ normal(N_Mixture[n-1], N_mix_sigma);
T_Mixture[n] ~ normal(T_Mixture[n-1], T_mix_sigma);
Clock_Mixture[n] ~ normal(Clock_Mixture[n-1], Clock_mix_sigma);

24

25

26

27

28

29

30

31

32

37

38

39

40

41

42

43

48

49

50

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

}

79
80 }

Page 20

81 generated quantities {
82

vector[numsteps] Priority;

83

84

85

86

87

88

// Linear Regression
for (i in 1:numsteps){

Priority[i] = normal_rng(mixture_alpha + Bt_beta * Bt_Mixture[i] +
N_beta * N_Mixture[i] + T_beta * T_Mixture[i] +
Clock_beta * Clock_Mixture[i], mixture_sigma);

}

89
90 }

Additional R ﬁles used to process data and run the model can be found at:
https://github.com/srpiatt/MMS_Magnetopause_MixtureModel

Page 21

