Second-Order Sensitivity Analysis for Bilevel Optimization

Robert Dyro
Stanford University
rdyro@stanford.edu

Edward Schmerling
Stanford University
schmrlng@stanford.edu

Nikos Ar´echiga
Toyota Research Institute
nikos.arechiga@tri.global

Marco Pavone
Stanford University
pavone@stanford.edu

2
2
0
2

y
a
M
4

]

C
O
.
h
t
a
m

[

1
v
9
2
3
2
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

In this work we derive a second-order ap-
proach to bilevel optimization, a type of
mathematical programming in which the so-
lution to a parameterized optimization prob-
lem (the “lower” problem) is itself to be opti-
mized (in the “upper” problem) as a function
of the parameters. Many existing approaches
to bilevel optimization employ ﬁrst-order sen-
sitivity analysis, based on the implicit func-
tion theorem (IFT), for the lower problem
to derive a gradient of the lower problem so-
lution with respect to its parameters; this
IFT gradient is then used in a ﬁrst-order
optimization method for the upper problem.
This paper extends this sensitivity analysis
to provide second-order derivative informa-
tion of the lower problem (which we call the
IFT Hessian), enabling the usage of faster-
converging second-order optimization meth-
ods at the upper level. Our analysis shows
that (i) much of the computation already
used to produce the IFT gradient can be
reused for the IFT Hessian, (ii) errors bounds
derived for the IFT gradient readily apply to
the IFT Hessian, (iii) computing IFT Hes-
sians can signiﬁcantly reduce overall compu-
tation by extracting more information from
each lower level solve. We corroborate our
ﬁndings and demonstrate the broad range of
applications of our method by applying it to
problem instances of least squares hyperpa-
rameter auto-tuning, multi-class SVM auto-
tuning, and inverse optimal control.

1 INTRODUCTION

Optimization is the foundation of modern learning and
decision making systems, therefore a natural problem
of interest is how to improve, learn, or optimize the

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

Figure 1: The single (hyper-)parameter test loss land-
scape of a multi-class SVM on Fashion-MNIST. Eval-
uating a point on this curve takes ∼100 seconds. We
obtain a local quadratic approximation which leads to
a much faster (hyper-)parameter optimization.

optimization itself. Many practitioners of autonomous
driving, robotics, and machine learning employ opti-
mization on an everyday basis. Understanding how
best to adjust this tool to more accurately suit their
application needs is key to improving performance,
trust, and reliability of these systems.

A natural way to approach improving optimization
comes through formulating a bilevel program—users
solve an optimization problem constructed with given
data and parameters, and rely on a secondary metric
quantifying the quality of the optimization result (it
is of course optimal with respect to its own objective)
to inform system design. Examples include (i) tun-
ing regularization of a regression model to give good
results on the test set, while training it to optimal-
ity on the train set, (ii) designing an autonomous car
that drives in a human-like fashion, where it optimizes
a ﬁnite horizon trajectory planning problem at every
time step, (iii) setting parallel auction prices in such
a way that rational bidding (an optimization in itself)
leads to highest auction holder revenue. Any time op-
timization or decision making is applied, the question
of selecting the right (hyper-)parameters arises in or-
der to obtain, for example, (i) statistical models which
generalize better, (ii) autonomous agents that behave
more like an expert, (iii) auction systems that can-
not be exploited. This notion is formalized as a bilevel

 
 
 
 
 
 
Second-Order Sensitivity Analysis for Bilevel Optimization

program in which the optimization-to-be-improved rep-
resents the lower level:

minimize
p

subject to

fU (z(cid:63), p)

z(cid:63) = argmin

z∈Z

fL(z, p).

(1)

This bilevel program formulation is general and sub-
sumes the problems of test set model generaliza-
tion, Stackelberg competition (Von Stackelberg, 2010),
meta-learning (Finn et al., 2017) and few-shot learning
(Lee et al., 2019). The quality of the optimization re-
sult, z(cid:63), of the objective fL is quantiﬁed via the upper
objective, fU . In the example of a statistical model,
fL represents the loss on the train dataset and fU the
loss on the test dataset. Solving the bilevel program
requires selecting parameters p which produce such z(cid:63)
that together lead to the minimal upper level loss fU .1

This solution is often approximated by selecting pa-
rameters p by hand or via grid search. However,
these approaches suﬀer from (a) being limited to cases
where the dimension of p is low (usually below 4),
(b) requiring parallel computing resources to keep re-
evaluating z(cid:63)(p) and most critically, (c) the search is
often done manually and wastes the expert’s or prac-
titioner’s time.

A more principled way of solving Problem (1) is to
use derivative information and make use of general-
purpose solvers developed for optimization problems.
This, however, requires the derivative of z(cid:63) with re-
spect to p—quantifying how small changes in p aﬀect
the upper level objective fU not just directly, but also
by inﬂuencing z(cid:63). Although a closed-form expression
of z(cid:63) with respect to p rarely exists, because z(cid:63) is
the result of optimization, the dependence is implic-
itly deﬁned via the necessary conditions for optimality
of the lower optimization. For smooth-in-parameter
problems, the implicit function theorem (IFT) may be
employed to compute this derivative information rele-
vant to solving the upper problem.

The availability of gradient expressions in bilevel pro-
gramming obtained via sensitivity analysis enables
the use of existing powerful optimizer to tackle these
problems when they arise in real-world applications.
However, even though many optimization algorithms
use second-order information to converge fast in cases
where the forward function evaluation is the bottle-
neck, not much attention has been paid to extending
sensitivity analysis to second-order information. Do-
ing so would enable another class of faster optimization
algorithms to be applied to bilevel programming.

1When z(cid:63) itself has an interpretation as “parameters”,
e.g., in model learning, p may be referred to as “hyperpa-
rameters”; we will refer to p as parameters throughout the
remainder of this domain-agnostic work.

The theoretical application of sensitivity analysis to
bilevel programming relies on exact solutions to the
lower level problems, but in reality, the numerical lim-
itations rarely allow for that. Existing literature on
ﬁrst-order methods thus focuses on showing that the
error in the derivative can be bounded and goes to
zero as the approximation approaches the solution to
the lower level problem.

1.1 Contributions

In this work we extend the application of the implicit
function theorem, where gradients of inner optimiza-
tion result with respect to parameters are found as
in many existing works, (Gould et al., 2016; Agrawal
et al., 2019b; Barratt, 2018), and derive second-order
derivatives, i.e., the IFT Hessian. We leverage this re-
sult for three main contributions: (i) We show that
the computational complexity of obtaining second-
order derivatives is, in many cases, still dominated by
the same matrix inversion bottleneck required for the
IFT gradient and so our method can be implemented
equally eﬃciently.
(ii) We analyze our IFT Hessian
expression to derive computational complexity and er-
ror bound expressions. We derive a new form of the
regularized error bound under diagonal regularization
of the matrix inverse operation in the application of
IFT. (iii) We use our second-order derivative expres-
sion to apply second-order optimization methods to
two machine learning datasets and show that these
methods lead to faster bilevel optimization, requiring
fewer lower level problem evaluations.

We then further discuss the practical limitations and
advantages of second-order optimization for bilevel op-
timization.

We open-source our implementation in two popular
machine learning/scientiﬁc computing/automatic dif-
ferentiation frameworks, PyTorch2 and JAX3, in a
user-friendly format at https://github.com/Stanf
ordASL/sensitivity torch and https://github.c
om/StanfordASL/sensitivity jax.

1.2 Related Work

Practical Deployment Optimization improvement
or optimization tuning has a long history in prac-
tical applications. For systems for which gradient
derivation is non-trivial or more generally for systems
where local gradient information is not informative of
the global scope of the problem, gradient-free proxy
models may be employed as in Golovin et al. (2017).
Like hand-tuning or grid search, this approach con-

2pytorch.org
3github.com/google/jax

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

strains the number of parameters that can practically
be tuned to single or low double digits.

Formal Literature More formally the Bilevel Pro-
gramming Problem (BLPP) formulation has a long
history in literature (Bard, 1984; Von Stackelberg,
2010; Henrion and Surowiec, 2011; Liu et al., 2001;
Bard, 2013). Sinha et al. (2017) contains an extensive
review of approaches to solving BLPPs. Many of these
works focus on theoretical analysis/characterization of
BLPP approaches; the focus of this paper is more on
speciﬁc concrete applications.

interest

in
Applications-oriented Renewed
to
applications-oriented gradient-based solutions
BLPPs (Bengio, 2000),
led to several works es-
tablishing the techniques for obtaining lower level
solution gradients with respect to the parameters
(Gould et al., 2016) and doing so eﬃciently for convex
problems (Barratt, 2018; Agrawal et al., 2019b,a).
Several computationally optimized, program-form
speciﬁc approaches have been shown (Amos and
Kolter, 2017; Amos et al., 2018). Most recently
Lorraine et al. (2020); Blondel et al. (2021) apply
the techniques to large-scale programs. Most of these
applications-oriented works focus on deriving gradient
expressions—to be used with a gradient-only BLPP
optimizer. These works do not consider the loss
landscape or the local curvature of the BLPP;
in
contrast, in this work, we attempt to quantify that.

Machine Learning BLPPs also found applications
in meta-learning literature (Andrychowicz et al., 2016;
Finn et al., 2017; Harrison et al., 2018; Bertinetto
et al., 2018) with several works making explicit use
of the implicit function theorem (Lee et al., 2019; Ra-
jeswaran et al., 2019). While meta-learning literature
poses an important application for BLPP, so far little
attention has been given to improving the speciﬁcs of
the solution methods employed in this body of work.

Higher-Order Derivatives Works most closely re-
lated to ours, with a focus on ﬁnding higher deriva-
tive information and analyzing the curvature of the
BLPP are Wachsmuth (2014); Mehlitz and Zemkoho
(2021). These works do not demonstrate how to ef-
ﬁciently compute second order derivatives or demon-
strate their usage in Newton’s-Method-like optimiza-
tion, two key focuses of our work.

1.3 Notation

For an argument x ∈ Rd we denote the dimen-
sion of x by dim(x) = d. For a scalar function
f : Rdim(x) → R we denote its gradient and Hes-
sian as ∇xf (x) and ∇2
xf (x). For a vector function of

g : Rdim(x) → Rdim(g) we denote its Jacobian matrix as
Dxg(x) ∈ Rdim(g)×dim(x). Where a function takes two
arguments, we use the normal and mono-space font
respectively to denote whether the diﬀerentiation op-
erator is partial or total (i.e. does the derivative cap-
ture dependence between variables), e.g. for a function
g : Rdim(x) × Rdim(y) → Rdim(g), Dxg(x, y) denotes the
partial Jacobian of g w.r.t. x and Dxg(x) the total
Jacobian of g w.r.t. x accounting for possible depen-
In this work, we aim to describe
dence of x on y.
higher order derivatives of vector functions, which
leads us to deﬁne, for g : Rdim(x) × Rdim(y) → Rdim(g),
Dxyg ∈ Rdim(g) dim(x)×dim(y), represented as a two di-
mensional matrix s.t.




Dxyg(x, y) =




Dy

Dy

(cid:0)∇x (g(x, y)1) (cid:1)
...

(cid:0)g(x, y)dim(g)

(cid:0)∇x




(cid:1) (cid:1)

(2)

where g(x, y)i denotes the i-th scalar output of the
vector function g. We deﬁne the operator Hx ≡ Dxx
(and the total version Hx ≡ Dxx). We use ⊗ to denote
the Kronecker product.

2 PROBLEM STATEMENT

Recall the formulation of bilevel programming:

minimize
p

subject to

fU (z(cid:63), p)

z(cid:63) = argmin

z∈Z

fL(z, p).

(1)

The problem considered in this work is how to ob-
tain an explicit expression for the total second-order
derivative, the Hessian matrix, of fU with respect to
p, i.e., HpfU (z(cid:63), p). Access to this Hessian enables the
application of second-order optimization methods for
solving the upper problem, with the aim of reducing
the total number of lower problem optimizations (and
correspondingly, total overall computation).

3 METHOD

3.1 Preliminaries

Necessary Derivatives As overviewed in Sec-
tion 1.2, there are several approaches to solving the
bilevel problem (1). Here, we focus on derivation of
the ﬁrst and second derivatives of fU w.r.t. p: DpfU
and HpfU . Because z(cid:63) depends on p, the total deriva-
tives of fU can be written as

DpfU (z(cid:63), p) = DpfU + (cid:0)Dz(cid:63) fU
(cid:1)(cid:0)Dpz(cid:63)(cid:1)
HpfU (z(cid:63), p) = HpfU + (cid:0)Dpz(cid:63))T (cid:0)Hz(cid:63) fU
+ (cid:0)(cid:0)Dz(cid:63) fU

(cid:1) ⊗ I(cid:1)Hpz(cid:63)

(cid:1)(cid:0)Dpz(cid:63))

(3)

(4)

Second-Order Sensitivity Analysis for Bilevel Optimization

Importantly, Equations (3) and (4) depend on (i)
terms directly obtainable from the upper objective
function via analytical or automatic diﬀerentiation:
DpfU , HpfU , Dz(cid:63) fU , Hz(cid:63) fU and (ii) terms quanti-
fying the sensitivity of lower level optimization, i.e.,
the result z(cid:63) w.r.t. p: Dpz(cid:63), Hpz(cid:63).

The latter two terms are nominally obtainable through
automatic diﬀerentiation by unrolling the entire fL op-
timization process—in many cases this is computation-
ally undesirable or simply too memory intensive to be
feasible. An alternative way of obtaining optimization
sensitivity terms follows from the IFT.

Implicit Function The main insight which allows
diﬀerentiating z(cid:63) = argminz fL(z, p) is the implicit
condition imposed on z(cid:63) as a result of z(cid:63) being an op-
timal solution. An optimal solution must satisfy First
Order Optimality Conditions (FOOC)

Understanding the proof of the equation above is vital
to obtaining higher derivatives via the IFT, since under
suitable smoothness assumptions, Equation (6) can be
diﬀerentiated again and the resulting expression solved
for Hpz(cid:63).

We now present the rarely derived IFT Hessian, which
allows us to obtain the second derivative through an
optimization. This result follows from a repeated ap-
plication of the IFT to the same implicit function to
obtain higher derivatives of z(cid:63) w.r.t. p.

3.3 Second-Order IFT

Theorem 2 (Second-Order IFT). Let k : Rm × Rn →
Rm be a twice continuously diﬀerentiable multivariate
function of two variables z(cid:63) ∈ Rm and p ∈ Rn such
that k deﬁnes a ﬁxed point for x, i.e. k(z(cid:63), p) = 0 for
all values of p ∈ Rn. Then, the Hessian of (implicitly
deﬁned) z(cid:63) w.r.t. p is given by

k(z(cid:63), p) = 0

(5)

Hpz(cid:63) = −

(cid:20)
(cid:0)Dz(cid:63) k(cid:1)−1

⊗ I

(cid:21)(cid:20)

Hpk + (cid:0)Dpz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1)

which deﬁne an implicit equation for z(cid:63).4 For uncon-
strained optimization, the FOOC we consider are ex-
plicitly:

k(z(cid:63), p) = Dz(cid:63) fL(z(cid:63), p) = 0.

The following ﬁrst-order sensitivity analysis is stan-
dard in the literature; we reproduce it here to motivate
and enable our second-order analysis.

3.2

Implicit Function Theorem (IFT)

Theorem 1 (Implicit Function Theorem (IFT)). Let
k : Rm × Rn → Rm be a continuously diﬀerentiable
multivariate function of two variables z(cid:63) ∈ Rm and
p ∈ Rn such that k deﬁnes a ﬁxed point for z(cid:63), i.e.,
k(z(cid:63), p) = 0 for all values of p ∈ Rn. Then, the deriva-
tive of (implicitly deﬁned) z(cid:63) w.r.t. p is given by

Dpz(cid:63) = −(cid:0)Dz(cid:63) k(z(cid:63), p)(cid:1)−1

Dpk(z(cid:63), p).

Proof. Take k(z(cid:63), p) = 0 and apply the chain rule to
diﬀerentiate w.r.t. p, then, if Dz(cid:63) k(z(cid:63), p) is invertible

k(z(cid:63), p) = 0
Dpk(z(cid:63), p) + (cid:0)Dz(cid:63) k(z(cid:63), p)(cid:1)(cid:0)Dpz(cid:63)(cid:1) = 0

−(cid:0)Dz(cid:63) k(z(cid:63), p)(cid:1)−1(cid:0)Dpk(z(cid:63), p)(cid:1) = Dpz(cid:63).

(6)

4An implicit function is simply deﬁned as a function
k : Rm × Rn → Rm that equals 0 if its ﬁrst input x satisﬁes
the implicit deﬁnition: x ∈ {x | k(x, y) = 0, ∃y ∈ Rn}.
This is in contrast to an explicit function of the form f :
Rn → Rm: x ∈ {x | f (y) = x}.

+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(cid:1)
(cid:21)

+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1)

.

(7)

The proof of Theorem 2 is provided in the Appendix.

4 ANALYSIS

4.1 Computational Complexity

The expression for the Second-Order IFT features Kro-
necker product terms (denoted by ⊗) which serve to
broadcast over dimensions of either the embedding
dim(z) or the parameter dim(p). Eﬃciently broad-
casting over a particular dimension does not require
constructing full dense matrices.

The two broadcasting Kronecker product-based forms
that appear in Equation (4) are of the form A ⊗ I
and I ⊗ B which both correspond to a broadcasted
version of matrix multiplication. We refer the inter-
ested reader to the Appendix for the discussion of how
computation with these forms can be accomplished ef-
ﬁciently.

Since our analysis here focuses on sensitivity analysis
for optimization, we devote most of our attention to
analyzing the computational complexity of computing
the expression in Equation (4), which we recall here in
full

HpfU (z(cid:63), p) = HpfU + (cid:0)Dpz(cid:63))T (cid:0)Hz(cid:63) fU
+ (cid:0)(cid:0)Dz(cid:63) fU

(cid:1) ⊗ I(cid:1)Hpz(cid:63)

(cid:1)(cid:0)Dpz(cid:63))

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

where HpfU , Hz(cid:63) fU and Dz(cid:63) fU have explicit expres-
sions and the term Dpz(cid:63) is either already available
to us from ﬁrst-order analysis or can be computed
cheaply since the matrix Dz(cid:63) k had to be factorized
for ﬁrst-order analysis. Thus, the only term requiring
signiﬁcant computation, substituting Equation (7), is

(cid:0)(cid:0)Dz(cid:63) fU

(cid:1) ⊗ I(cid:1)

(cid:21)(cid:20)

(cid:20)
(cid:0)Dz(cid:63) k(cid:1)−1
(cid:123)(cid:122)
Equation (7)

⊗ I

(cid:124)

(cid:21)

. . .

=

(cid:125)

(cid:20) (cid:16)

Dz(cid:63) fU

(cid:0)Dz(cid:63) k(cid:1)−1(cid:17)

(cid:21)(cid:20)

(cid:21)

⊗ I

. . .

which is expected as the resulting matrix is in Rn×n.
The increase in computational complexity is thus mi-
nor. Here we use the computational complexity of a
matrix factorization operation to be O(cid:0)m3(cid:1).

We do not include the computational complexity of
obtaining the partial derivatives of fU and k, which
we denote as partial explicit derivatives, primarily be-
cause: they are heavily problem dependent; they van-
ish; can be precomputed; eﬃcient analytical expres-
sions exist; or they can be obtained by eﬃciently mak-
ing use of Jacobian-vector and vector-Jacobian prod-
ucts in an automatic diﬀerentiation engine.

Since the upper level objective is a scalar by deﬁnition,
the product

4.2 Error Analysis

Dz(cid:63) fU

(cid:0)Dz(cid:63) k(cid:1)−1

= vT ∈ R1×m

(7)

can be computed at the cost of a single matrix solve
using an already necessarily factorized matrix from
ﬁrst-order analysis, which can be done computationally
(cid:1)⊗I(cid:1)Hpz(cid:63)
cheaply. Thus, evaluating the term (cid:0)(cid:0)Dz(cid:63) fU
reduces to (a) caching a term from ﬁrst-order anal-
ysis and (b) broadcasted vector-matrix product (a
weighted summation of matrices). Alternatively, if an
automatic diﬀerentiation system is used to compute
the right bracket in Equation (7), then v can be used
as a sensitivity vector in vector-Jacobian or Jacobian-
vector products, signiﬁcantly reducing the number of
calls to the automatic diﬀerentiation (autodiﬀ) engine.

4.1.1 Big-O Notation

Computational Complexity
O(cid:0)m3 + mn(cid:1)

Operation
1st IFT
1st IFT w/ sens. O(cid:0)m3 + n(cid:1)
O(cid:0)m3 + m2n2(cid:1)
2nd IFT
2nd IFT w/ sens. O(cid:0)m3 + mn2(cid:1)

Table 1: Computational complexity of applying the
ﬁrst- and second-order implicit function theorem,
omitting computation of partial explicit derivatives.
In this table we deﬁne m = dim(z), n = dim(p). “w/
sens.” denotes that a sensitivity vector is available in
which case the matrix inverse can be computed for a
single left hand side as in Equation (7).

Following Section 4.1 we show computational com-
plexity of applying the ﬁrst- (1st) and second-order
(2nd) implicit function theorem in Table 1 and ob-
serve that the second-order expression with a sensitiv-
ity left-hand side—the term Dz(cid:63) fU in Equations (3)
(4)—has a computational complexity that diﬀers from
ﬁrst-order expression with a sensitivity left hand side
only by the additional quadratic term in n = dim(p),

Error analysis is vital for sensitivity analysis because
numerical limitations often result in lower level solu-
tions that are not quite optimal, but are instead at
a small distance from the optimum. Developing con-
ﬁdence in the sensitivity methods described here re-
quires the ability to bound the error caused by apply-
ing sensitivity analysis developed for optimal points to
suboptimal lower level solutions.

4.2.1 First-Order Error Bound

Assuming an inexact local solution, we state the fol-
lowing error bound on the Jacobian. Theorem 3 is
heavily inspired by Blondel et al. (2021), Theorem 1,
which is in turn inspired by Higham (2002), Theorem
7.2.

Theorem 3 (First-Order Error Bound). Given the re-
sult of IFT applied to an exact solution Dz(cid:63) p = g =
−A−1B, where A(z(cid:63), p) = Dz(cid:63) k(z(cid:63), p), B(z(cid:63), p) =
Dpk(z(cid:63), p) and an inexact solution (cid:101)A(z, p) = Dzk(z, p)
(where k(z, p) (cid:54)= 0) and (cid:101)B(z, p) = Dpk(z, p). Assume
(cid:12)
(cid:12)
||z − z(cid:63)|| ≤ δ,
≤ βδ,
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≥ α1||v||, ||Av|| ≥ α2||v||, then
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B

||B||F ≤ R,

(cid:12)
(cid:12)
(cid:12)op

≤ γδ,

(cid:12)
(cid:12)
(cid:12)F

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)J − J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

β
α1

δ +

γR
α1α2

δ

(8)

We point the reader to the Appendix for the proof.

4.2.2 Second-Order Error Bound

Assuming an inexact local solution, we state the fol-
lowing error bound on the Jacobian.

Bound).
(Second-Order
Theorem 4
Given the result of IFT applied to an exact so-
for z(cid:63) ∈ Rm, Hz(cid:63) p = H = −A−1B,
lution,
where A(z(cid:63), p) = Dz(cid:63) k(z(cid:63), p) ⊗ I, B(z(cid:63), p) =
Hpk + (cid:0)Dpz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1) + (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(cid:1) +

Error

Second-Order Sensitivity Analysis for Bilevel Optimization

(a) RR

(b) diag

(c) conv

Figure 2: Hyperparameter optimization of least-squares models with two gradient-only algorithms and one
gradient & Hessian—enabled by this work.

(a) RR

(b) diag

(c) conv

Figure 3: Hyperparameter optimization of least-squares models with two gradient-only algorithms and one
gradient & Hessian—enabled by this work.

≤ γδ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
||Dzpk(z, p) − Dz(cid:63)pk(z(cid:63), p)||F

(cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1) and an inexact solution
(cid:101)A(z, p) (cid:54)= 0 and (cid:101)B(z, p). Assume ||z − z(cid:63)|| ≤ δ,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
||Hpk(z, p) − Hpk(z(cid:63), p)||F ≤
(cid:12) (cid:101)A − A
(cid:12)
ζδ,
ηδ,
||Hzk(z, p) − Hz(cid:63) k(z(cid:63), p)||F ≤ νδ, ||Dzp − Dz(cid:63) p||F ≤
(cid:12)
(cid:12)
κJ δ deﬁned in Theorem 3,
(cid:12) ≥ α1||v||,
||Av|| ≥ α2||v||, then

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)H − H

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

ζ + 2ηκg + νκ2
g
α1

δ +

γRH
α1α2

δ.

(9)

(cid:12)
(cid:12)
(cid:12)

≤ γδ,

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)A − A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≥ α1||v|| and vT (cid:101)Av ≥ 0, so
(cid:12) (cid:101)Av
(cid:12)
(cid:12)
(cid:15))||v||, ||Av|| ≥ α2||v|| then

(cid:12)
(cid:12)
(cid:12)F

(cid:12)
(cid:12)
(cid:12)

≤ βδ, ||B||F ≤ R,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≥ (α1 +
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:98)Av

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)J − J
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

βδ
α1 + (cid:15)

+

R(cid:0)γδ + (cid:15)(cid:1)
(cid:0)α1 + (cid:15)(cid:1)α2

(10)

Selecting a post hoc arbitrary regularization allows to
tighten the bound. We refer the reader to the Ap-
pendix for the proof.

We point the reader to the Appendix for the proof.

5 EXPERIMENTS

4.2.3 Error Bound Optimization

5.1 Regression Model Auto-Tuning

Now, assuming an inexact solution we analyze the di-
agonal regularization of the inverse in the application
of IFT. We let (cid:98)A = (cid:101)A + (cid:15)I.
Theorem 5 (Regularized First-Order Error Bound).
Given the result of regularized IFT applied to an ex-
act solution Dz(cid:63) p = g = −A−1B, where A(z(cid:63), p) =
Dz(cid:63) k(z(cid:63), p), B(z(cid:63), p) = Dpk(z(cid:63), p) and an inexact so-
lution (cid:101)A(z, p) = Dzk(z, p) + (cid:15)I, (cid:101)B(z, p) = Dpk(z, p)
(cid:54)= 0) . Assume ||z − z(cid:63)|| ≤ δ,
(where k(z, p)

We compare the performance of three commonly used
nonlinear optimization algorithms on the problem of
linear model improvement via smooth hyperparameter
tuning. Using the above analysis we are able to apply
a second-order optimization method, which oﬀers to
dramatically reduce the number of lower function eval-
uations in BLPP and signiﬁcantly speed up optimiza-
tion problems where the lower level constraint eval-
uation dominates. To compare, we optimize the hy-

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

perparameters of 3 linear models on MNIST (LeCun,
1998) using three commonly used optimization algo-
rithms, two gradient-only: (i) Adam (Kingma and Ba,
2014), (ii) L-BFGS (Liu and Nocedal, 1989) and one
using second-order information (enabled by this work)
(iii) SQP (Nocedal and Wright, 2006). The three linear
models we choose all employ a least-squares lower level
loss where the target vector is the one-hot encoding of
the ten MNIST digits. We train on 1000 randomly
selected MNIST examples in the train set and evalu-
ate the upper level (test) loss on all examples in the
test set. The upper level loss is the cross-entropy clas-
i=1 ti,j log (cid:0)qi,j
(cid:1)
(cid:80)10
siﬁcation loss, fU (z(cid:63), p) = − (cid:80)Ntest
j
(cid:16)(cid:80)10
(cid:17)
where ti,j = δi,yj and qi,j = exT
j z(cid:63)
i=1 exT
.5
For all models z ∈ Rnd where n = 10 is the number of
MNIST classes and d is the number of features in the
data vector.

j z(cid:63)

i/

i

least-
Model 1 (RR) A single hyperparameter
squares model with Tikhonov
regularization
(Tikhonov, 1943), also known as ridge regression
(Gruber, 2017). The lower level loss takes the form
fL(z, p) = ||Xz − Y ||2
F + 10p||z||2
F. The features are
raw image pixels and a bias term.

Model 2 (diag) A least-squares model with
Tikhonov regularization where each weight is penal-
ized with a separate weight. The lower level loss takes
the form fL(z, p) = ||Xz − Y ||2
i . The
i
features are the same as in Model 1.

F +(cid:80)dim(z)

10pi z2

Model 3 (conv) A least-squares model where the
images are ﬁrst passed through a parametric 2D con-
volution ﬁlter with a 3 × 3 kernel, a stride of 2, a bias,
1 input channel and 2 output channels. The convolu-
tion output is passed through the tanh activation func-
tion, before adding bias and applying the Tikhonov
regularized least-squares model. The reduced images
have a dimension of 338. The convolution weight, bias
and the scalar Tikhonov regularization weight form the
vector p.

We show the optimization results in Figures 2 & 3.
The use of second-order derivative information signiﬁ-
cantly reduces the number of least-squares evaluations.

In RR our method, SQP, outperforms other optimizers
both in terms of test accuracy and test loss. In diag L-
BFGS goes unstable and the Adam optimizer tends to
outperform SQP in terms of the test loss, but it takes
a 100 evaluations and the SQP converges much quicker
to a high test accuracy. Finally, the L-BFGS also ex-
hibits poor performance on the conv model, SQP con-

5ti,j = δi,yj indicates that ti,j is equal to 1 if example j

is of the class i and 0 otherwise.

Figure 4: Optimization of multi-class SVM on
Fashion-MNIST. Newton’s method using second-order
derivatives derived in this paper leads to much faster
convergence.

verges to a lower classiﬁcation loss much faster. Adam
reaches a higher test accuracy, but also a higher clas-
siﬁcation loss.

5.2 Hyperparameter Optimization

We further verify the usefulness of computing second-
order derivatives in practical problem instances by
applying the Newton’s Method to tuning a multi-
class SVM model (Crammer and Singer, 2001) on the
Fashion-MNIST (Xiao et al., 2017) dataset for best
performance on the test set. We reformulate the con-
straints as log-barrier penalty6 to ensure smoothness,
with the reﬁnement value of α = 102. We solve the
resulting problem with the Mosek optimizer.7

We scale the images down to 14 × 14 to aid with the
memory requirements and ﬁt a multi-class SVM model
to 2,000 randomly selected samples in the train set.
We deﬁne upper level loss as the cross-entropy clas-
siﬁcation loss between the predictions and the labels.
We select 5 random seeds to verify algorithmic perfor-
mance.

We show a test accuracy plot in Figure 4. We note
that the optimization time, i.e., training the lower level
multi-class SVM, dominates both the gradient and
Hessian computation. The application of Newton’s
Method (enabled by second-order derivatives shown
in this work) signiﬁcantly reduces the number of nec-
essary lower level optimizations and reaches maximum
test accuracy much quicker in terms of wall-clock time.

6The log-barrier is deﬁned for a constraint x ≤ 0 as

− log (−αx) /α for a tunable reﬁnement constant α.

7www.mosek.com

Second-Order Sensitivity Analysis for Bilevel Optimization

5.3 Parameter Loss Landscape in Inverse

Optimal Control

We investigate the Inverse Optimal Control (IOC)
problem as a case study in loss landscape or curvature
analysis. IOC has a natural formulation as a BLPP.

IOC has a large body of literature of its own, e.g. Ke-
shavarz et al. (2011); Johnson et al. (2013); Terekhov
and Zatsiorsky (2011), but we focus here on some
simple examples that are illustrative to the general
BLPP curvature analysis. We show that the uncon-
strained Optimal Control Problem (OCP) problem we
formulate is globally convex in parameter, but that in
the presence of constraints in the OCP, the resulting
BLPP requires more care—linear inequalities, with-
out reformulation, violate our smoothness assumption
in Theorem 2.

Given known linear time-invariant discrete time dy-
namics x(i+1) = Ax(i) + Bu(i) and state and control
cost matrices Q, R, and control limits u(i) ∈ U = {u |
||u||∞ ≤ ulim}, we assume we observe some expert’s
trajectory Xe = [x(0), . . . , x(N )] and control history
Ue = [u(0), . . . , u(N )]. We seek to learn a reference
trajectory we assume the expert is tracking, i.e., the
expert behaves optimally under the cost J(X, U ):

Xe, Ue =

argmin
x(i+1)=Ax(i)+Bu(i),U ∈U

J(X, U, Xe, Ue)

(11)

where J(X, U, Xe, Ue) = (cid:80)
e,ref) + (u(i) − u(i)
x(i)
BLPP

e,ref)T R(u(i) − u(i)

i(x(i) − x(i)

e,ref)T Q(x(i) −
e,ref). This leads to a

(a) Unconstrained

(b) Control Constrained

Figure 5: Comparison of upper loss landscape in in-
verse optimal control without and with control con-
straints. Surface and the contour plots show the PCA
2D projections based on the optimization path.

violates the assumptions of Theorem 2, since FOOC
with linear constraints are not twice continuously dif-
ferentiable. In the presence of even the simple max-
imum control value constraints considered, the loss-
in-parameter (the upper loss) is highly non-convex.
We observe that any constraints can be smoothly ap-
proximated to any degree of precision using the log-
barrier function which is inﬁnitely diﬀerentiable. Fig-
ure 6 shows the 2D projected loss-in-parameter p land-
scape for a successive reﬁnement of the log-barrier con-
straints; for high values of reﬁnement α → ∞, the
loss landscape closely approximates the exactly con-
strained version, yet remains diﬀerentiable, so Theo-
rem 2 can be applied.

||Xe − X (cid:63)||2

2 + ||Ue − U (cid:63)||

min
Xref,Uref
s.t. X (cid:63), U (cid:63) =

argmin
x(i+1)=Ax(i)+Bu(i),
U ∈U

J(X, U, Xref, Uref).

(12)
In the typical notation of this paper, z(cid:63) = (cid:0)X (cid:63), U (cid:63)(cid:1),
p = (cid:0)Xref, Uref
(cid:1). The problem corresponds to discov-
ering the reference trajectory of an optimal agent, e.g.,
the centerline of a road using an observed trajectory
of an autonomous car.

We show the comparison between the upper loss land-
scapes for IOC with unconstrained/constrained con-
trols in Figure 5. We employ the Principle Compo-
nent Analysis (PCA) dimension reduction technique
on the optimization path to visualize a many dimen-
sional upper loss in 2 dimensions, inspired by Li et al.
(2017).

In the absence any constraints, the resulting problem
as stated is convex which follows from the application
of Equations (4) and (7).

Naive application of Equation (7) to a constrained IOC
problem yields a globally positive deﬁnite Hessian, but

(a) Log-barrier, α = 101

(b) Log-barrier, α = 102.5

(c) Log-barrier, α = 104

(d) Exactly constrained

Figure 6: Comparison of loss landscape for inverse
box-constrained-control MPC. The surface and con-
tour plots show the PCA 2D projections based on the
optimization path.

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

Code release The code for our experiments is con-
tained at https://github.com/StanfordASL/Secon
d-OrderSensitivityAnalysisForBilevelOptimiza
tion.

6 DISCUSSION

Limitations & Promises The method we propose
here oﬀers to make better use of every single lower
level problem evaluation, but comes with limitations.
Firstly, like any second-order optimization method, it
might not be best suited for highly non-convex land-
scapes, which can be common in BLPPs as Figure 5
shows. Secondly, in computational complexity analy-
sis we do not focus on obtaining partial explicit deriva-
tives necessary to compute the second-order sensitiv-
ity expression. We do so because these can be problem
speciﬁc and often be zero or have an analytic form, but
if they are obtained via automatic diﬀerentiation (the
method we employ), their computation time highly
depends on chosen software package.
In general, it
might turn out that computing second-order sensitiv-
ity information might take longer than several evalu-
ations of the lower level and the ﬁrst-order sensitivity
information evaluation—at which point gradient-only
optimization methods will likely function better than
our proposed approach. Nevertheless, our work ex-
pands the optimization toolbox where some examples
of BLPPs can be optimized much quicker, as Figure 4
shows.

7 CONCLUSIONS

In this work we derive the second-order derivatives of
the upper level objective in a general bilevel program
via sensitivity analysis using the implicit function the-
orem. Second-order information enables second-order
optimization to be applied to these problems, which
we argue can drastically reduce the number of lower
level function evaluations, and speed up optimization,
in cases where the lower level evaluation dominates.
We further show that the computational complexity
of our proposed approach is comparable to ﬁrst-order
IFT and we adapt error bound analysis for ﬁrst-order
IFT derivatives to our second-order IFT derivatives.

Future Work Future work includes quantifying how
well various approximations suggested for ﬁrst-order
IFT apply to second-order IFT and whether (and
when), for optimization purposes, some terms in our
second-order sensitivity expression can be omitted or
approximated to make their computation quicker. Fi-
nally, we are interested in further investigating loss
landscapes, and the associated diﬃculty of optimiza-
tion, for constrained bilevel problems.

Acknowledgments

Toyota Research Institute and the NASA University
Leadership Initiative (grant #80NSSC20M0163) pro-
vided funds to support this work; this article solely re-
ﬂects the opinions and conclusions of its authors and
not any Toyota or NASA entity.

References

Agrawal, A., Amos, B., Barratt, S., Boyd, S., Di-
amond, S., and Kolter, Z. (2019a). Diﬀeren-
tiable convex optimization layers. arXiv preprint
arXiv:1910.12430.

Agrawal, A., Barratt, S., Boyd, S., Busseti, E., and
Moursi, W. M. (2019b). Diﬀerentiating through a
cone program. arXiv preprint arXiv:1904.09043.

Ailon, N. and Chazelle, B. (2006). Approximate
nearest neighbors and the fast johnson-lindenstrauss
transform. In Proceedings of the thirty-eighth annual
ACM symposium on Theory of computing, pages
557–563.

Amos, B. and Kolter, J. Z. (2017). Optnet: Diﬀer-
entiable optimization as a layer in neural networks.
In International Conference on Machine Learning,
pages 136–145. PMLR.

Amos, B., Rodriguez, I. D. J., Sacks, J., Boots, B.,
and Kolter, J. Z. (2018). Diﬀerentiable mpc for
arXiv preprint
end-to-end planning and control.
arXiv:1810.13400.

Andrychowicz, M., Denil, M., Gomez, S., Hoﬀman,
M. W., Pfau, D., Schaul, T., Shillingford, B., and
De Freitas, N. (2016). Learning to learn by gradient
descent by gradient descent. In Advances in neural
information processing systems, pages 3981–3989.

Bard, J. F. (1984). Optimality conditions for the
bilevel programming problem. Naval research logis-
tics quarterly, 31(1):13–26.

Bard, J. F. (2013). Practical bilevel optimization: algo-
rithms and applications, volume 30. Springer Science
& Business Media.

Barratt, S. (2018). On the diﬀerentiability of the
solution to convex optimization problems. arXiv
preprint arXiv:1804.05098.

Barratt, S. T. and Boyd, S. P. (2021). Least squares
auto-tuning. Engineering Optimization, 53(5):789–
810.

Bengio, Y. (2000). Gradient-based optimization of
hyperparameters. Neural computation, 12(8):1889–
1900.

Bertinetto, L., Henriques, J. F., Torr, P. H., and
Meta-learning with dif-

Vedaldi, A.

(2018).

Second-Order Sensitivity Analysis for Bilevel Optimization

ferentiable closed-form solvers.
arXiv:1805.08136.

arXiv preprint

Bertrand, Q., Klopfenstein, Q., Blondel, M., Vaiter,
S., Gramfort, A., and Salmon, J. (2020). Implicit
diﬀerentiation of lasso-type models for hyperparam-
eter optimization.
In International Conference on
Machine Learning, pages 810–821. PMLR.

Blondel, M., Berthet, Q., Cuturi, M., Frostig, R.,
Hoyer, S., Llinares-L´opez, F., Pedregosa, F., and
Vert, J.-P. (2021). Eﬃcient and modular implicit
diﬀerentiation. arXiv preprint arXiv:2105.15183.

Crammer, K. and Singer, Y. (2001). On the algorith-
mic implementation of multiclass kernel-based vec-
tor machines. Journal of machine learning research,
2(Dec):265–292.

Diamond, S. and Boyd, S. (2016).

CVXPY: A
Python-embedded modeling language for convex op-
timization. Journal of Machine Learning Research,
17(83):1–5.

El Ghaoui, L. (2002). Inversion error, condition num-
ber, and approximate inverses of uncertain matrices.
Linear algebra and its applications, 343:171–193.

Finn, C., Abbeel, P., and Levine, S. (2017). Model-
agnostic meta-learning for fast adaptation of deep
networks. In International Conference on Machine
Learning, pages 1126–1135. PMLR.

Golovin, D., Solnik, B., Moitra, S., Kochanski, G.,
Karro, J., and Sculley, D. (2017). Google vizier: A
service for black-box optimization. In Proceedings of
the 23rd ACM SIGKDD international conference on
knowledge discovery and data mining, pages 1487–
1495.

Golub, G. H. and Van Loan, C. F. (1996). Matrix
computations. johns hopkins studies in the mathe-
matical sciences.

Gould, S., Fernando, B., Cherian, A., Anderson, P.,
Cruz, R. S., and Guo, E. (2016). On diﬀerentiating
parameterized argmin and argmax problems with
application to bi-level optimization. arXiv preprint
arXiv:1607.05447.

Gruber, M. H. (2017). Improving eﬃciency by shrink-
age: the James-Stein and ridge regression estima-
tors. Routledge.

Harrison, J., Sharma, A., and Pavone, M. (2018).
Meta-learning priors for eﬃcient online bayesian re-
gression. arXiv preprint arXiv:1807.08912.

Henrion, R. and Surowiec, T. (2011). On calmness
conditions in convex bilevel programming. Applica-
ble Analysis, 90(6):951–970.

Higham, N. J. (2002). Accuracy and stability of nu-

merical algorithms. SIAM.

Johnson, M., Aghasadeghi, N., and Bretl, T. (2013).
Inverse optimal control for deterministic continuous-
time nonlinear systems. In 52nd IEEE Conference
on Decision and Control, pages 2906–2913. IEEE.

Keshavarz, A., Wang, Y., and Boyd, S. (2011). Imput-
ing a convex objective function. In 2011 IEEE in-
ternational symposium on intelligent control, pages
613–619. IEEE.

Kingma, D. P. and Ba, J. (2014).

Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

LeCun, Y. (1998). The mnist database of handwritten

digits.

Lee, K., Maji, S., Ravichandran, A., and Soatto, S.
(2019). Meta-learning with diﬀerentiable convex op-
timization. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 10657–10665.

Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein,
T. (2017). Visualizing the loss landscape of neural
nets. arXiv preprint arXiv:1712.09913.

Liu, D. C. and Nocedal, J. (1989). On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1):503–528.

Liu, G., Han, J., and Zhang, J. (2001). Exact penalty
functions for convex bilevel programming problems.
Journal of Optimization Theory and Applications,
110(3):621–643.

Lorraine, J., Vicol, P., and Duvenaud, D. (2020). Opti-
mizing millions of hyperparameters by implicit dif-
ferentiation.
In International Conference on Arti-
ﬁcial Intelligence and Statistics, pages 1540–1552.
PMLR.

Magnus, J. R. and Neudecker, H. (2019). Matrix dif-
ferential calculus with applications in statistics and
econometrics. John Wiley & Sons.

Mehlitz, P. and Zemkoho, A. B. (2021). Suﬃcient op-
timality conditions in bilevel programming. Mathe-
matics of Operations Research.

Nocedal, J. and Wright, S. (2006). Numerical opti-

mization. Springer Science & Business Media.

Rajeswaran, A., Finn, C., Kakade, S., and Levine, S.

(2019). Meta-learning with implicit gradients.

Sinha, A., Malo, P., and Deb, K. (2017). A review on
bilevel optimization: from classical to evolutionary
approaches and applications. IEEE Transactions on
Evolutionary Computation, 22(2):276–295.

Terekhov, A. V. and Zatsiorsky, V. M. (2011). Analyt-
ical and numerical analysis of inverse optimization
problems: conditions of uniqueness and computa-
tional methods. Biological cybernetics, 104(1):75–
93.

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

Tikhonov, A. N. (1943). On the stability of inverse
problems. In Dokl. Akad. Nauk SSSR, volume 39,
pages 195–198.

Von Stackelberg, H. (2010). Market structure and equi-

librium. Springer Science & Business Media.

Wachsmuth, G. (2014). Diﬀerentiability of implicit
functions: Beyond the implicit function theorem.
Journal of Mathematical Analysis and Applications,
414(1):259–272.

Wiesemann, W., Tsoukalas, A., Kleniati, P.-M., and
Rustem, B. (2013). Pessimistic bilevel optimization.
SIAM Journal on Optimization, 23(1):353–380.

Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-
image dataset for benchmark-
arXiv preprint

mnist:
ing machine learning algorithms.
arXiv:1708.07747.

a novel

Yao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer,
K., and Mahoney, M. W. (2020). Adahessian: An
adaptive second order optimizer for machine learn-
ing. arXiv preprint arXiv:2006.00719.

Second-Order Sensitivity Analysis for Bilevel Optimization

Second-Order Sensitivity Analysis for Bilevel Optimization
Appendix

A PROOFS

Theorem 2 (Second-Order IFT). Let k : Rm × Rn → Rm be a twice continuously diﬀerentiable multivariate
function of two variables z(cid:63) ∈ Rm and p ∈ Rn such that k deﬁnes a ﬁxed point for x, i.e. k(z(cid:63), p) = 0 for all
values of p ∈ Rn. Then, the Hessian of (implicitly deﬁned) z(cid:63) w.r.t. p is given by

Hpz(cid:63) = −

(cid:20)
(cid:0)Dz(cid:63) k(cid:1)−1

⊗ I

(cid:21)(cid:20)

Hpk + (cid:0)Dpz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1)

+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(cid:1)
(cid:21)

+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1)

.

(7)

Proof. Diﬀerentiation of the implicit expression further requires establishing a convention for matrix expression
with vector inputs derivatives. We keep the standard convention of representing the partial Jacobian (with
operator D) and scalar function Hessian (with operator H). The missing representations include the Hessian
and the 2nd order mixed derivative. Let f : Rm × Rn (cid:55)→ Rp, then we represent Hessian and 2nd order mixed
derivatives as

Dabf (a, b) =

Haf (a, b) =

(cid:1)(cid:1)

(cid:0)f1

(cid:1)(cid:1)

(cid:0)fp






∈ Rpm×n

∈ Rpm×m.











Db

(cid:0)∇a
...
(cid:0)∇a
Db

Haf1
...
Hafp




We introduce the following derivatives rules

where the chain rule applies as

Da
Da

(cid:0)A(a)B(cid:1) = (cid:0)I ⊗ BT (cid:1)(cid:0)DaA(a)(cid:1)
(cid:0)BA(a)(cid:1) = (cid:0)B ⊗ I(cid:1)(cid:0)DaA(a)(cid:1)

Da

(cid:0)A(c(a)(cid:1) = (cid:0)DcA(c)(cid:1)(cid:0)Dac(a)(cid:1)

This allows to diﬀerentiate Dpk + (cid:0)Dz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1) = 0 giving

Hpk(z(cid:63), p)
+ (cid:0)Dpz(cid:63) k(z(cid:63), p)(cid:1)(cid:0)Dpz(cid:63)(cid:1)
+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(z(cid:63), p)(cid:1)
+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(z(cid:63), p)(cid:1)(cid:0)Dpz(cid:63)(cid:1)
+ (cid:0)(cid:0)Dz(cid:63) k(z(cid:63), p)(cid:1) ⊗ I(cid:1)(cid:0)Hpz(cid:63)(cid:1)
= 0

(13)

(14)

(15)

(16)

(17)

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

The implicit Hessian is given by solving Equation (17) for Hpz(cid:63).

Hpz(cid:63) = −

(cid:20)
(cid:0)Dz(cid:63) k(z(cid:63), p)(cid:1)−1

⊗ I

(cid:21)(cid:20)

Hpk(z(cid:63), p) + (cid:0)Dpz(cid:63) k(z(cid:63), p)(cid:1)(cid:0)Dpz(cid:63)(cid:1)

+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(z(cid:63), p)(cid:1)
+ (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(z(cid:63), p)(cid:1)(cid:0)Dpz(cid:63)(cid:1)

(cid:21)

(7)

exploiting the identity (cid:0)A ⊗ I(cid:1)−1

= A−1 ⊗ I.

Theorem 3 (First-Order Error Bound). Given the result of IFT applied to an exact solution Dz(cid:63) p = g =
−A−1B, where A(z(cid:63), p) = Dz(cid:63) k(z(cid:63), p), B(z(cid:63), p) = Dpk(z(cid:63), p) and an inexact solution (cid:101)A(z, p) = Dzk(z, p) (where
k(z, p) (cid:54)= 0) and (cid:101)B(z, p) = Dpk(z, p). Assume ||z − z(cid:63)|| ≤ δ,
≤ βδ, ||B||F ≤ R,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Av
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ α1||v||, ||Av|| ≥ α2||v||, then

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)F
(cid:12)

≤ γδ,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)J − J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

δ +

β
α1

γR
α1α2

δ

Proof.

−( (cid:101)J − J) = (cid:101)A−1 (cid:101)B − A−1B

= (cid:101)A−1 (cid:101)B − (cid:101)A−1B + (cid:101)A−1B − A−1B
= (cid:101)A−1( (cid:101)B − B) + ( (cid:101)A−1 − A−1)B

which allows to bound the implicit gradient error as

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)J − J
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12) (cid:101)A−1(cid:12)
(cid:12) (cid:101)A−1(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

β
α1

δ +

(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γR
α1α2

δ

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F
(cid:12)
(cid:12)
(cid:12)F

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12) (cid:101)A−1 − A−1(cid:12)
(cid:12) (cid:101)A−1(cid:12)

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A

(cid:12)
(cid:12)
(cid:12)op

||B||F
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)

(cid:12)A−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(18)

(19)

(20)

(21)

(cid:12)
(cid:12)op||B||F

2

(cid:1) = M −1

1 − M −2

exploiting the fact that for any invertible matrices M1, M2 (where (cid:101)A, A are invertible from
||Av|| ≥ α2||v||) (cid:0)M −1
Theorem 4 (Second-Order Error Bound). Given the result of IFT applied to an exact solution, for z(cid:63) ∈
Rm, Hz(cid:63) p = H = −A−1B, where A(z(cid:63), p) = Dz(cid:63) k(z(cid:63), p) ⊗ I, B(z(cid:63), p) = Hpk + (cid:0)Dpz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1) + (cid:0)I ⊗
(cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(cid:1) + (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Hz(cid:63) k(cid:1)(cid:0)Dpz(cid:63)(cid:1) and an inexact solution (cid:101)A(z, p), (cid:101)B(z, p) (where k(z, p) (cid:54)= 0).
Assume ||z − z(cid:63)|| ≤ δ,
≤ γδ, ||Hpk(z, p) − Hpk(z(cid:63), p)||F ≤ ζδ, ||Dzpk(z, p) − Dz(cid:63)pk(z(cid:63), p)||F ≤ ηδ,

(cid:0)M1 − M2

(cid:1)M −1
2 .

1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ α1||v||,

||Hzk(z, p) − Hz(cid:63) k(z(cid:63), p)||F ≤ νδ, ||Dzp − Dz(cid:63) p||F ≤ κJ δ deﬁned in Theorem 3, ||B||F ≤ RH ,
||Av|| ≥ α2||v||, then

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)H − H

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

ζ + 2ηκJ + νκ2
J
α1

δ +

γRH
α1α2

δ.

Proof.

−( (cid:101)H − H) =

(cid:16)
(cid:101)A−1 ⊗ I

(cid:17)

(cid:16)

(cid:16)

=

=

(cid:101)A−1 ⊗ I

(cid:101)A−1 ⊗ I

(cid:17)

(cid:17)

(cid:101)B − (cid:0)A−1 ⊗ I(cid:1) B
(cid:17)

(cid:16)

(cid:101)B −

(cid:101)A−1 ⊗ I

B +

(cid:16)

( (cid:101)B − B) +

(cid:16)

( (cid:101)A−1 − A−1) ⊗ I

B

(cid:17)

(cid:101)A−1 ⊗ I
(cid:17)

B − (cid:0)A−1 ⊗ I(cid:1) B

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ α1||v||,

(22)

(23)

(24)

(25)

(26)

Second-Order Sensitivity Analysis for Bilevel Optimization

which allows to bound the implicit Hessian error as

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)H − H

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

≤

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B

(cid:12)
(cid:12) (cid:101)A−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12) (cid:101)A−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
ζ + 2ηκJ + νκ2
J
α1

(cid:12)
(cid:12)

δ +

γRH
α1α2

δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F
(cid:12)
(cid:12)
(cid:12)F

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12) (cid:101)A−1 − A−1(cid:12)
(cid:12) (cid:101)A−1(cid:12)

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)A − A
(cid:12)

(cid:12)
(cid:12)
(cid:12)op

||B||

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)

(cid:12)A−1(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)op||B||F

exploiting the fact that for any invertible matrices M1, M2 (where (cid:101)A, A are invertible from
||Av|| ≥ α2||v||) (cid:0)M −1

(cid:0)M1 − M2

(cid:1) = M −1

1 − M −2

(cid:1)M −1
2 .

2

1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ α1||v||,

Theorem 5 (Regularized First-Order Error Bound). Given the result of regularized IFT applied to an exact
solution Dz(cid:63) p = g = −A−1B, where A(z(cid:63), p) = Dz(cid:63) k(z(cid:63), p), B(z(cid:63), p) = Dpk(z(cid:63), p) and an inexact solution
(cid:101)A(z, p) = Dzk(z, p) + (cid:15)I, (cid:101)B(z, p) = Dpk(z, p) (where k(z, p) (cid:54)= 0) . Assume ||z − z(cid:63)|| ≤ δ,
≤ γδ,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ (α1 + (cid:15))||v||, ||Av|| ≥ α2||v|| then

(cid:12)
(cid:12)
(cid:12) ≥ α1||v|| and vT (cid:101)Av ≥ 0, so

≤ βδ, ||B||F ≤ R,

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)Av
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)J − J
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

βδ
α1 + (cid:15)

+

R(cid:0)γδ + (cid:15)(cid:1)
(cid:0)α1 + (cid:15)(cid:1)α2

Proof. We observe that from singular value decomposition

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)Av

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≥ α1||v||, vT (cid:101)Av ≥ 0 =⇒
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)A + (cid:15)I

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)op

≥ α1 + (cid:15)

which gives

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:98)J − J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

≤

≤

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12) (cid:98)A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

β
α1 + (cid:15)

δ +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
(cid:12)
(cid:12)
(cid:12) (cid:101)B − B
R
(cid:0)α1 + (cid:15)(cid:1)α2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)op

||B||

(cid:12) (cid:98)A−1 − A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)
(cid:12) (cid:98)A−1(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:12)
(cid:12)
(cid:12)op

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)A − A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)A + (cid:15)I − A
(cid:12)
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:101)A − A

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(γδ + (cid:15))

(cid:12)A−1(cid:12)
(cid:12)
(cid:12)
(cid:12)op||B||
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)A−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)op||B||
(cid:12)
(cid:12)
(cid:12)op
(cid:12)
(cid:19)

+ ||(cid:15)I||op

(cid:12)
(cid:12)

(cid:12)A−1(cid:12)
(cid:12)
(cid:12)

which gives

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (cid:98)J − J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)F

≤

βδ
α1 + (cid:15)

+

R(cid:0)γδ + (cid:15)(cid:1)
(cid:0)α1 + (cid:15)(cid:1)α2

.

(27)

(28)

(29)

(cid:12)
(cid:12)op||B||

Robert Dyro, Edward Schmerling, Nikos Ar´echiga, Marco Pavone

B COMPUTATIONAL COMPLEXITY

Evaluating A ⊗ I Eﬃciently The product A ⊗ I features in the Hessian expression

Hpz(cid:63) =

(cid:104)(cid:0)Dz(cid:63) k(cid:1)−1

(cid:105)

⊗ I

[. . . ]

Given the expression M = (cid:0)A ⊗ I(cid:1)C, let A ∈ Rm×n, C ∈ R(np)×r, such that

C =






C1 ∈ Rp×r
...
Cn ∈ Rp×r






Let M = (cid:0)A ⊗ I(cid:1)C and deﬁne (cid:101)C ∈ Rn×p×r s.t. (cid:101)Ci∗∗ ∈ Rp×r ∀i ∈ [1..n], (cid:102)M ∈ Rm×p×r s.t. (cid:102)Mj∗∗ ∈ Rp×r ∀j ∈
[1..m]—a 3-d representation of C and M where the stacked matrices in C, M are concatenated along a third,
ﬁrst, dimension in (cid:101)C, (cid:102)M .

The operation can now be deﬁned formally in Einstein summation notation as

(cid:0)

(cid:102)M (cid:1)

ijk = (cid:0)A(cid:1)

il

(cid:0)

(cid:101)C(cid:1)

ljk

Intuitively, the operation M = (cid:0)A ⊗ I(cid:1) corresponds to a matrix multiplication performed for every vector in C
built from n elements, one from each Ci.

Evaluating I ⊗ B Eﬃciently The product I ⊗ B features in the Hessian expression

Hpz(cid:63) = [. . . ]

(cid:104)

· · · + (cid:0)I ⊗ (cid:0)Dpz(cid:63)(cid:1)T (cid:1)(cid:0)Dz(cid:63)pk(cid:1) + . . .

(cid:105)

Given the expression M = (cid:0)I ⊗ B(cid:1)C, let A ∈ Rm×n, C ∈ R(pn)×r, such that

C =






C1 ∈ Rn×r
...
Cp ∈ Rn×r






Let M = (cid:0)I ⊗ B(cid:1)C and deﬁne (cid:101)C ∈ Rp×n×r s.t. (cid:101)Ci∗∗ ∈ Rn×r ∀i ∈ [1..n], (cid:102)M ∈ Rp×m×r s.t. (cid:102)Mj∗∗ ∈ Rm×r ∀j ∈
[1..p]—a 3-d representation of C and M where the stacked matrices in C, M are concatenated along a third,
ﬁrst, dimension in (cid:101)C, (cid:102)M .

The operation can now be deﬁned formally in Einstein summation notation as

(cid:0)

(cid:102)M (cid:1)

ijk = (cid:0)A(cid:1)

jl

(cid:0)

(cid:101)C(cid:1)

ilk

Intuitively, the operation M = (cid:0)I ⊗ B(cid:1) corresponds to a matrix multiplication performed by broadcasting B and
performing matrix multiplication of B with every p element of C, so that BCi ∀i ∈ [1..p]—these products are
then stacked together to form M .

Second-Order Sensitivity Analysis for Bilevel Optimization

C ADDITIONAL INSIGHTS

Convexity-in-parameter Global convexity-in-parameter is obtained if the second-order order derivative of
the upper loss function w.r.t. to the parameter is globally positive semi-deﬁnite (PSD) while both the objective
functions are globally twice continuously diﬀerentiable. Recalling the full derivative expressions

HfU (z(cid:63), p) = HpfU + (cid:0)Dpz(cid:63))T (cid:0)Hz(cid:63) fU
+(cid:0)(cid:0)Dz(cid:63) fU

(cid:1)(cid:0)Dpz(cid:63))+
(cid:1) ⊗ I(cid:1)Hpz(cid:63)

we conclude that it is not possible to guarantee the global PSD property for this Hessian in general, because of
the reduction (cid:0)(cid:0)Dz(cid:63) fU
(cid:1) ⊗ I(cid:1)Hpz(cid:63) as outlined in Section B.8 However, in the special case when Hpz(cid:63) = 0 and
the problem is globally twice diﬀerentiable, global convexity is obtained, e.g., unconstrained quadratic programs
with linear parameterizations.

8A stack of Hessian matrices are reduced by a weighted summation with unknown sign weights.

