Meta Learning for Code Summarization

Moiz Rauf
Department of Computer Science
University of Stuttgart
moiz.rauf@iste.uni-stuttgart.de

Sebastian Padó
Institut für Maschinelle Sprachverarbeitung
University of Stuttgart
pado@ims.uni-stuttgart.de

Michael Pradel
Department of Computer Science
University of Stuttgart
michael@binaervarianz.de

2
2
0
2

n
a
J

0
2

]

G
L
.
s
c
[

1
v
0
1
3
8
0
.
1
0
2
2
:
v
i
X
r
a

Abstract

Source code summarization is the task of gen-
erating a high-level natural language descrip-
tion for a segment of programming language
code. Current neural models for the task dif-
fer in their architecture and the aspects of code
they consider.
In this paper, we show that
three state-of-the-art models for code summa-
rization work well on largely disjoint subsets
of a large code base. This complementar-
ity motivates model combination: We propose
three meta-models that select the best candi-
date summary for a given code segment. The
two neural models improve signiﬁcantly over
the performance of the best individual model,
obtaining an improvement of 2.1 BLEU points
on a dataset of code segments where at least
one of the individual models obtains a non-
zero BLEU.

1

Introduction

Source code summarization is the task of generat-
ing short natural language statements describing
a segment of code (Haiduc et al., 2010; Sridhara
et al., 2010). Such summaries serve an integral role
in software development by aiding code compre-
hension (Takang et al., 1996; Xia et al., 2018). The
recent availability of large code bases and advances
in machine learning have given this task signiﬁcant
attention at the interface between NLP and soft-
ware engineering. Most neural network-based ap-
proaches build on machine translation (MT) strate-
gies, framing code summarization as a text-to-text
generation task (Richardson et al., 2017).

A ﬁrst interesting parallel to MT research in NLP
is that code summarization models also differ sub-
stantially in their assumptions about the nature of
the task. Some adopt a sequence-to-sequence map-
ping approach (Iyer et al., 2016; Eberhart et al.,
2020), while others take into account code struc-
ture, e.g., abstract syntax trees (ASTs) (Hu et al.,
2018a; Wan et al., 2018; LeClair et al., 2019), or

infer latent structure with graph neural networks
(LeClair et al., 2020) or transformers (Ahmad et al.,
2020). Another active direction, again similar to
many NLP tasks, is the inclusion of contextual and
background information, through API calls (Hu
et al., 2018b), information from other methods or
projects (Haque et al., 2020; Bansal et al., 2021),
or exploiting the symmetry between code summa-
rization and generation (Wei et al., 2019).

In this paper, we follow up on the observation
by LeClair et al. (2019) that current models per-
form well for some examples. An analysis on
three state-of-the-art methods (NeuralCodeSum,
ast-attendgru, attendgru, cf. Sec. 2.1) on the Fun-
com dataset (Sec. 3) shows that the models are
indeed largely complementary (cf. Figure 1): Each
of the individual models creates the best summary
for a substantial number of code segments, with the
best model NeuralCodeSum, winning in about 6.4k
of 22k cases where any model predicts a summary
with non-zero BLEU. Table 1 illustrates this com-
plementarity on two short methods: even though
all models learn cues from code identiﬁers (here,
method and variable names), in most cases they are
only partially successful, and no single model is
always best.

Based on these observations, we propose to com-
bine the strengths of the individual code summa-
rization models with meta learning (Naik and Mam-
mone, 1992), training a new model that selects the
best summary, given the original code segment and
candidate summaries. We ﬁnd a statistically signif-
icant improvement over the best individual models.

2 Methods

Given a sequence T = (t1, ..., tl) of code tokens,
code summarization is the task to produce a se-
quence S = (w1, ..., wk) of words describing the
code. The predictions are evaluated against refer-
ence summaries, using BLEU score (Papineni et al.,
2002) as also customary in MT.

 
 
 
 
 
 
Code

Source

Summary

public BigInteger getHelpfulVotes(){

return helpfulVotes;

}

Reference
gets the value of the helpful votes property
NeuralCodeSum gets the value of the helpful votes property
attendgru
gets the value of the reason votes property
ast-attendgru
gets the value of the reason type property

Reference

determines whether to display the last button in the bottom pane

public void displayLastButton(boolean b) { NeuralCodeSum display the last button
displays the last button
bottomPane.lastButton.setVisible(b);
display the last button in the panel

attendgru
ast-attendgru

}

BLEU

1.00
0.59
0.54

0.17
0.00
0.46

Table 1: Summaries predicted by three state-of-the-art code summarization models and BLEU score compared to
a human-written reference.

2.2 Meta-Learning Model

Given the complementarity of these models (cf.
Figure 1), it would be very desirable to combine
their strengths. There are multiple ways to do so.
Straightforward combination of model output, as
usual in ensembling (Rokach, 2010), is difﬁcult
for highly structured output such as summaries.
LeClair et al. (2021) combine multiple source en-
coders with a joint decoder, which is effective but
requires disassembling models. In this paper, we
instead adopt a meta-learning approach (Naik and
Mammone, 1992) in which we learn a summary se-
lector. We formulate this task as multi-label binary
classiﬁcation tasks, where the meta-model predicts
the suitability of each candidate summary, given
the summary and the original code segment. We
propose three such classiﬁers.

2.2.1 Feature-Based Meta-Model
Our ﬁrst classiﬁer, metafeat, is a logistic regression
model (Cox, 1958) whose features are designed
to capture properties of code segments which may
determine the difﬁculty of generating code sum-
maries, building on ideas from performance predic-
tion (Papay et al., 2020) and conﬁdence estimation
for summarization (Louis and Nenkova, 2009). We
consider the following feature types:

Token and word frequencies Based on the fre-
quency of each code token and each word
across the codebase, we consider the harmonic
means freq(T ) and freq(S) of the code and
summary, respectively. The hypothesis is that
higher frequencies should make for simpler
summarization.

Figure 1: Complementarity of code summarization
models: # of FunCom methods for which each model
achieves highest BLEU score (, indicates draw).

2.1 Code Summarization Models

As sketched above, a number of code summariza-
tion models have been proposed in the literature.
We consider three models. All use an encoder-
decoder structure, and yield state-of-the-art results.

Text-based The attendgru model uses an LSTM
as encoder to summarize the token sequence
into a context vector (LeClair et al., 2019).
The decoder then uses this vector to generate
the summary.

Code structure-based The ast-attendgru model
is an extension of attendgru (LeClair et al.,
2019). In addition to the tokens, it also consid-
ers a ﬂattened abstract syntax tree (AST). It
encodes both inputs separately and feeds their
concatenation into a decoder.

Transformer-based. The

NeuralCodeSum
model (Ahmad et al., 2020) uses a trans-
former with relative positional encoding and
copy attention as encoder, and then predicts a
summary with a decoder.

Length We consider the number |T | of tokens in
a code segment and the number |S| of words
in a summary, with longer code segments and
summaries indicating higher complexity and
thus difﬁculty.

NeuralCodeSumast-attendgruattendgruNeuralCodeSum,ast-attendgruNeuralCodeSum,attendgruAst-attendgru,attendgruAll Models02000400060008000metaTRN, is based on a transformer.

2.2.3 Training and Querying Meta Models

As the goal of the meta-model is to maximize
the overall BLEU score of the predicted sum-
maries w.r.t. reference summaries, we train the
meta-models in a supervised manner based on la-
bels derived from BLEU scores. We label a sum-
mary as suitable if and only if it achieves the best
BLEU score among all available candidate sum-
maries. If multiple candidate summaries achieve
the same, non-zero BLEU score, then all these can-
didates are labeled as suitable. Let B be the set
of BLEU scores obtained by candidate summaries
S1, ..., Sj for a code sequence T , then the training
label for T, (S1, ..., Sj) is p1, ..., pj where

pi =

(cid:40)

1,
0,

if BLEU (Si, Sref ) = max (B)
otherwise

At inference time, we choose the candidate sum-
mary Si with the highest predicted probability pi.

3 Experimental Setup

Data. We use the FunCom dataset (LeClair and
McMillan, 2019).
It contains 2.1 million pairs
of Java code segments and summaries, with an
average of 51 tokens per segment and 15 words
per summary. We use the authors’ tokenization.
As shown in Table 2, we divide the dataset into
three partitions: for summary generation, for meta-
learning, and for testing. The test partition corre-
sponds to the one used in previous work (LeClair
et al., 2019, 2020; Haque et al., 2020; LeClair et al.,
2021), whereas the partition to train summariza-
tion models is smaller than in prior work, as we
keep some data for the meta-model. Because for a
substantial percentage of code segments, all sum-
marization models fail to produce a summary with
non-zero BLEU, we also consider a ﬁltered dataset
containing only segments where at least one sum-
marization model achieves BLEU > 0. The ﬁltered
dataset hence are the cases where the meta-model
has a chance to improve over the individual models.

Models and Evaluation. We ﬁrst train the three
code summarization models and then our meta-
models, as deﬁned in Section 2. We evaluate
the summaries by the standard choice of corpus-
level aggregated BLEU scores (Papineni et al.,
2002). We consider three scenarios, which dif-
fer on whether the meta-model is trained on the

Figure 2: Architecture of our neural meta-models.

Distinctiveness We measure how distinctive a can-
didate summary I is compared to all sum-
maries produced by the same model as the
Kullback–Leibler divergence Dis kl(Pi||P ),
where P is the unigram distribution of all sum-
maries, and Pi is the unigram distribution of
candidate summary i. We expect low distinc-
tiveness to lead to difﬁcult summarization.

2.2.2 Neural Meta-Models
As an alternative to specifying the relevant features
by hand, we deﬁne two neural meta-models that
select a summary based on self-learned distributed
features. More speciﬁcally, as shown in Figure 2,
we ﬁrst represent the input sequences (code tokens
and summaries) in terms of FastText token and
word embeddings, respectively, The choice of Fast-
Text is motivated by prior work showing that Fast-
Text outperforms other pre-trained token embed-
ding models at accurately representing identiﬁers
in source code (Wainakh et al., 2021). We pretrain
these embeddings on the training dataset used in
the evaluation (see below). After embedding, the
model consists of two encoders, one for the code
token sequence (generating a vector vT ) and one
for each summary (generating a vector vS).

The ﬁnal step is to concatenate, for each sum-
mary, vT and vS. The concatenation is passed
through two linear layers, and ﬁnally through a
sigmoid function so that each summary is associ-
ated with a probability. The two sequence encoders
and the linear layers are trained jointly.

Our two neural models differ only in the type
of sequence encoder they use. The ﬁrst model,
called metaLSTM, encodes sequences through a
bi-directional LSTM. The other model, called

Sequence Tof tokensPre-trained tokenembeddingSequenceencoderSequence S1 of wordsPre-trained wordembeddingSequenceencoderLinearProbability p1of suitability+Sequence Sj of wordsPre-trained wordembeddingSequenceencoderLinearProbability pjof suitability+...Summarizationmodel sum1Summarizationmodel sumjPartition

Split All

Filtered

Nenkova, 2009).

Summarization

Meta

Test

train
valid

train
valid

test

1.4 million NA
NA
60k

440k
70k

101k

101k
5.3k

22k

Table 2: Statistics of the experiment datasets

Model

Train/test of meta model

All/
all

All/
ﬁltered

Filtered/
ﬁltered

. attendgru
r
a
m
m
u
S

16.25
ast-attendgru
16.62
NeuralCodeSum 18.57

48.29
49.35
55.66

a metafeat

t
e

M

metaLSTM
metaTRN

17.93
52.47
18.94* 57.22*
19.18* 57.74*

48.29
49.35
55.66

55.06
57.08*
56.94*

Table 3: BLEU scores on test set for individual summa-
rization models and meta models. * indicates a statisti-
cally signiﬁcant improvement over N euralCodeSum
at α=0.05.

entire meta partition or only the ﬁltered portion,
and analogously whether it is evaluated on the full
or the ﬁltered portion of the test partition.

We make our code and data available. More
information, along with hyperparameters, can be
found at the repository 1.

If we evaluate the same models only on the ﬁl-
tered datasets – i.e., where the meta model has a
chance of improving over the individual models
(middle column) – we observe the same ranking
of the models, but the margin between the best in-
dividual summarization model (NeuralCodeSum,
55.7 BLEU) and the neural meta learning models
has increased: We obtain a BLEU of 57.2 for the
meta LSTM (+1.5 BLEU) and a BLEU of 57.7 for
the meta TRN (+2 BLEU); differences again are
signiﬁcant at α = 0.05. We take these numbers
as an indication that the neural meta-learning ap-
proach is generally successful for code segments
for which “sensible” summary candidates (with
BLEU > 0) have been produced by the individual
models.

Finally, the right-hand column assesses the con-
sequences of training the meta-models only on such
“sensible” summary candidates exist. Compared
to the middle column, the meta-model results are
slightly lower.In other words, the apparently unin-
formative summary candidates still contribute to
the success of the meta model. Taken together
with the observation that the results for the BiL-
STM changes much less (-0.1 BLEU) than for the
transformer (-0.8 BLEU), we propose the follow-
ing interpretation: Pairs of code segments and non-
sensical summaries may still help the neural model
in learning to encode typical code token and word
sequences, which is more important for the trans-
former, with its higher capacity, than for the BiL-
STM.

4 Results

5 Conclusions

Table 3 shows our main results. We ﬁrst consider
the setup with training and test on the full dataset.
Among the individual summarization models, the
transformer-based NeuralCodeSum model works
best, with a BLEU of 18.6. Both neural meta
models improve over the individual models; the
difference to NeuralCodeSum is statistically sig-
niﬁcant at α=0.05. The transformer-based meta
model achieves the best result at 19.2 BLEU (+0.6
BLEU). In contrast, the feature-based meta model
even underperforms the best individual code sum-
mary model. This highlights the difﬁculty of pre-
dicting the quality of summaries for code segments,
while the quality of summaries for natural language
texts has been predicted successfully (Louis and

1https://github.com/sola-st/MetaCodeSum

The present paper exploits the complementary
nature of different code summarization models
through a meta-learning approach. We ﬁnd that
neural models can predict the best summary from
a set of candidates created by three state-of-the-art
models, yielding an increase in BLEU of up to 2.1
points. We believe our results to be promising, and
future improvements of individual summarization
models will give our meta-models better predic-
tions to choose from. At the same time, our results
also highlight directions for future work, includ-
ing meta-model introspection (why does the trans-
former succeed where the manual features fail?)
and a re-evaluation of BLEU as summary evalua-
tion metric (Fabbri et al., 2021).

References

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. 2020. A transformer-
based approach for source code summarization. In
Proceedings of ACL.

Aakash Bansal, Sakib Haque, and Collin McMillan.
2021. Project-level encoding for neural source code
In Proceedings of
summarization of subroutines.
ICPC, pages 253–264. IEEE.

David R Cox. 1958. The regression analysis of binary
sequences. Journal of the Royal Statistical Society:
Series B (Methodological), 20(2):215–232.

Zachary Eberhart, Alexander LeClair, and Collin
McMillan. 2020. Automatically extracting subrou-
tine summary descriptions from unstructured com-
ments. In 2020 IEEE International Conference on
Software Analysis, Evolution, and Reengineering
(SANER).

Alexander R. Fabbri, Wojciech Kry´sci´nski, Bryan
McCann, Caiming Xiong, Richard Socher, and
Dragomir Radev. 2021. SummEval: Re-evaluating
Summarization Evaluation. Transactions of the As-
sociation for Computational Linguistics, 9:391–409.

Sonia Haiduc, Jairo Aponte, Laura Moreno, and An-
drian Marcus. 2010. On the use of automated text
summarization techniques for summarizing source
code. 17th Working Conference on Reverse Engi-
neering, pages 35–44.

Sakib Haque, Alexander LeClair, Lingfei Wu, and
Collin McMillan. 2020.
Improved automatic sum-
marization of subroutines via attention to ﬁle con-
text. In 2020 Mining Software Repositories (MSR).

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a.
Deep code comment generation. In Proceedings of
ICPC, page 200–210, New York, NY, USA.

Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi
Jin. 2018b. Summarizing source code with trans-
In Proceedings of IJCAI,
ferred api knowledge.
pages 2269–2275.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
ACL, pages 2073–2083, Berlin, Germany.

A. LeClair and C. McMillan. 2019. Recommendation
for datasets for source code summarization. In Pro-
ceedings of NAACL.

Alexander LeClair, Aakash Bansal, and Collin McMil-
lan. 2021. Ensemble models for neural source code
summarization of subroutines. In Proceedings of IC-
SME.

Alexander LeClair, Sakib Haque, Lingfei Wu, and
Collin McMillan. 2020. Improved code summariza-
tion via a graph neural network. In Proceedings of
ICPC.

Alexander LeClair, Siyuan Jiang, and Collin McMillan.
2019. A neural model for generating natural lan-
In Pro-
guage summaries of program subroutines.
ceedings of ICSE, page 795–806. IEEE Press.

Annie Louis and Ani Nenkova. 2009. Performance
conﬁdence estimation for automatic summarization.
In Proceedings of EACL, pages 541–548, Athens,
Greece.

D.K. Naik and R.J. Mammone. 1992. Meta-neural
networks that learn by learning. In Proceedings of
IJCNN, volume 1, pages 437–442 vol.1.

Sean Papay, Roman Klinger, and Sebastian Padó.
2020. Dissecting span identiﬁcation tasks with per-
In Proceedings of EMNLP,
formance prediction.
page 4881–4895, Online.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
In Proceedings of
uation of machine translation.
ACL, page 311–318, USA. Association for Compu-
tational Linguistics.

Kyle Richardson, Sina Zarrieß, and Jonas Kuhn. 2017.
The code2text challenge: Text generation in source
libraries. Proceedings of the 10th International Con-
ference on Natural Language Generation.

Lior Rokach. 2010. Ensemble-based classiﬁers. Artiﬁ-

cial Intelligence Review, 33(1):1–39.

Giriprasad Sridhara, Emily Hill, Divya Muppaneni,
Lori L. Pollock, and K. Vijay-Shanker. 2010. To-
wards automatically generating summary comments
for java methods. In Proceedings of ASE.

Armstrong Takang, Penny Grubb, and Robert Ma-
credie. 1996. The effects of comments and identi-
ﬁer names on program comprehensibility: An exper-
imental investigation. J. Prog. Lang., 4:143–167.

Yaza Wainakh, Moiz Rauf, and Michael Pradel. 2021.
Idbench: Evaluating semantic representations of
identiﬁer names in source code. In Proceedings of
ICSE, pages 562–573.

Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Im-
proving automatic source code summarization via
In Proceedings of
deep reinforcement learning.
ASE.

Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.
Code generation as a dual task of code summariza-
tion. In Proceeedings of NeurIPS.

Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing,
Ahmed E. Hassan, and Shanping Li. 2018. Mea-
suring program comprehension: A large-scale ﬁeld
IEEE Transactions on
study with professionals.
Software Engineering, 44(10):951–976.

