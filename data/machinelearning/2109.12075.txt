2
2
0
2

y
a
M
4
2

]
I

A
.
s
c
[

4
v
5
7
0
2
1
.
9
0
1
2
:
v
i
X
r
a

Towards A Measure of General Machine Intelligence

Gautham Venkatasubramanian*
Maya Labs
gautham@mayalabs.io

Sibesh Kar∗
Maya Labs
sibesh@mayalabs.io

Abhimanyu Singh
Maya Labs
abhimanyu@mayalabs.io

Shubham Mishra
Maya Labs
shubham@mayalabs.io

Dushyant Yadav
Maya Labs
dushyant@mayalabs.io

Shreyansh Chandak
Maya Labs
shreyansh@mayalabs.io

Abstract

To build general-purpose artiﬁcial intelligence systems that can deal with un-
known variables across unknown domains, we need benchmarks that measure how
well these systems perform on tasks they have never seen before. A prerequisite
for this is a measure of a task’s generalization difﬁculty, or how dissimilar it is
from the system’s prior knowledge and experience. If the skill of an intelligence
system in a particular domain is deﬁned as it’s ability to consistently generate a set
of instructions (or programs) to solve tasks in that domain, current benchmarks do
not quantitatively measure the efﬁciency of acquiring new skills, making it pos-
sible to brute-force skill acquisition by training with unlimited amounts of data
and compute power. With this in mind, we ﬁrst propose a common language of
instruction, a programming language that allows the expression of programs in the
form of directed acyclic graphs across a wide variety of real-world domains and
computing platforms. Using programs generated in this language, we demonstrate
a match-based method to both score performance and calculate the generalization
difﬁculty of any given set of tasks. We use these to deﬁne a numeric bench-
mark called the generalization index, or the g-index , to measure and compare
the skill-acquisition efﬁciency of any intelligence system on a set of real-world
tasks. Finally, we evaluate the suitability of some well-known models as general
intelligence systems by calculating their g-index scores.

*Contributions: Gautham V. and Sibesh K. led the research. Abhimanyu S. conducted the experiments.
Shubham M., Dushyant Y. and Shreyansh C. worked on program design and the training dataset. Thanks to
Nilay Savant for the DAG visualization diagrams.

1

 
 
 
 
 
 
Contents

1 Introduction

1.1 History of deﬁning intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Intelligence as benchmarks of skill . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2
1.3 Measuring general intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Setting up the evaluation

.

.

. . .

. . . . .

. . . . .

. . . . .

2.3.1
2.3.2 DAGs vs Abstract Syntax Trees

2.1 Components of the g-index
.
2.2 The Task Speciﬁcation .
.
.
2.3 The Skill Program .

. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Programs as directed acyclic graphs . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1 Divergence Metric for Flow-Based Programs . . . . . . . . . . . . . . . .
2.4.2 Computing ∆ for a Single Node .
. . . . . . . . . . . . . . . . . . .
2.4.3 Computing ∆ for the General Case
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . .
2.4.4
2.5 Measuring Generalization Difﬁculty . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 The scoring function .

Features of ∆ .

. . .
. .
. . .

.

.

.

.

.

3 The g-index benchmark

3.1 Deﬁning the formula .
.
3.2 Properties of the g-index .
.
3.3 Levels of generalization .

.

4 Experiments

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . .

. . . . .

5 Flatland - a toy environment for the g-index

6 Summary and Future Directions

A Sample Task Domains and Descriptions

B Node Library

C Calculating ∆ via subgraph isomorphism

1

Introduction

1.1 History of deﬁning intelligence

2
2
3
3

4
4
5
5
5
8
8
9
9
9
10
11

12
12
14
16

17

20

22

27

28

29

The concept of intelligence has been expressed in informal terms from time immemorial, but to date
there has been no consensus on a formal deﬁnition [18]. In psychology, deﬁnitions of intelligence
include “the faculty of adapting one’s self to circumstances” [3], and “the aggregate or global
capacity . .. to act purposefully, to think rationally, and to deal effectively with [the] environment”,
[1]. The deﬁnition of artiﬁcial intelligence observes similar variety, with common reliance on a
human reference; for eg. “machines capable of performing tasks that would require intelligence
if done by humans” [4, 16]. Developments in the ﬁeld of AI have led to further reﬁnements, with
terms such as skill-based, narrow, or weak AI versus general or strong AI [28, 13, 31].

The Turing test “imitation game” [2], one of the ﬁrst measures of artiﬁcial intelligence was qualita-
tive in nature. It required that an artiﬁcial intelligence convince a human judge that it was a human.
This created an inaccurate perception of an AI’s capabilities due to variance in the judge’s knowl-
edge [19]. The improvements to the Turing test (such as the Lovelace test [23] and its successor
[36]) maintain the requirement of a human or human-like judge, focusing on the judge’s available
resources and formal descriptions of what the judge can use for measuring the AI’s capabilities. Till
date, subjective human judgment plays a key role in evaluating any intelligence system.

2

1.2

Intelligence as benchmarks of skill

Recent breakthroughs in the capabilities of machine intelligence systems have relied upon the scal-
ing of computational methods such as support vector machines, random forests and neural networks.
While this was assisted by the increased availability of raw computing power, we note that framing
machine intelligence in terms of computation enabled quantitative methods for evaluation, as mea-
suring the performance of a system was simpliﬁed to computing a numerical benchmark score on
a publicly available dataset. The MNIST dataset [20] served as a benchmark for digit classiﬁcation
[24, 27, 25] , and is used today in introductory texts to showcase the power of deep neural networks.
Over the years, designing a good benchmark has developed into a specialized problem, involving
the collection of large diverse datasets spanning multiple years [52].

Figure 1: The state of the art in image classiﬁcation–in context of the Top-1 accuracy benchmark on
the ImageNet dataset–has improved almost every year since 2012. The presence of a consistent nu-
meric benchmark spurred the development of better systems for image classiﬁcation. Image source
: https://paperswithcode.com/sota/image-classiﬁcation-on-imagenet.

The presence of quantitative benchmarks was a boon for developing machine learning-based in-
telligence systems, as one could compare different methods and design targeted improvements to
build upon a consistent, computable, numerical score. The state-of-the-art in image classiﬁcation
improved every year since the ImageNet benchmark [30] became widely available, starting with
AlexNet in 2012 [35], to SENet [49] in 2017 (see Figure 1), with new techniques such as ResNet
skip-connections gaining prominence on the back of their ImageNet performance. Similar improve-
ments were also seen in the ﬁeld of language understanding, with the GLUE benchmark [47] and
the development of well-known transformer models like BERT [44] and GPT-2 [51].

1.3 Measuring general intelligence

Deep learning-based methods have achieved a high level of skill in specialized tasks, but it is still
hard to quantitatively measure the “generalization capability” of an intelligence system - or its “abil-
ity to handle situations (or tasks) that differ from previously encountered situations” [48]. This is due
to the difﬁculty of measuring the variables involved in current deﬁnitions of artiﬁcial intelligence.
Legg and Hutter [26] informally deﬁne intelligence as measuring an “agent’s ability to achieve goals
in a wide range of environments”. It further mentions the properties desirable in a measure of in-
telligence, such as a formal mathematical deﬁnition, applicability across different methods without
bias, and an informative, numerical score to enable comparison across agents. However, an agent’s
intelligence cannot be computed practically according to this deﬁnition as it relies on ﬁnding the
Kolmogorov complexity [29] of each environment .

Hern´andez-Orallo [40] bifurcates the measurement of intelligence systems into task-oriented and
ability-oriented evaluations. Both are important for evaluating a system, but the former is far more

3

common. Task-oriented evaluations include human judgments of AI performance, direct bench-
mark comparisons, and assessment of adversarial situations in games such as playing Chess or Go.
Ability-oriented evaluations take the form of psychometrics in the case of human intelligence, and
extend to artiﬁcial systems [34] when intelligence is viewed as a form of information processing
[17]. This allows the use of algorithmic information theory (AIT) [15] to perform ability-oriented
evaluations of artiﬁcial systems. Building on this, Chollet [48] provides an outline for the measure-
ment of intelligence via a framework that is easily mapped to current methods in machine learning.
The intelligence of a system here is deﬁned as “the measure of its skill-acquisition efﬁciency over a
scope of tasks, with respect to priors, experience, and generalization difﬁculty”, and involves testing
via a benchmark dataset called the Abstract Reasoning Corpus (ARC). However, this framework
does not offer a quantitative measure of generalization difﬁculty, and all evaluation is close-ended
and binary.

In this paper, we deﬁne the generalization index, abbreviated to g-index, a quantitative benchmark
to measure the intelligence of an artiﬁcial system as a computable, numerical value. The naming is
inspired from the g-factor, which is a measure of general ability in the ﬁeld of psychometrics [21].
It accounts for performance, generalization difﬁculty and sample efﬁciency across a wide range of
real-world tasks. Section 2 describes the experimental setup for the benchmark, and showcases
the components that enable calculating numerical values for evaluation. Section 3 formally deﬁnes
the parameters on which the g-index depends, constructs a mathematical formula, and shows how
the properties of the g-index follow the guidelines in current literature. Section 4 evaluates some
well-known transformer models as candidates for a general intelligence system by computing their
g-index scores, and provides a sample dataset of real-world tasks and their associated skill programs
that can be used with the g-index .1 Section 5 adapts the g-index for a toy environment with a simpler
program space. Finally, Section 6 notes possible directions for improvement in the current design
and of the g-index .

2 Setting up the evaluation

2.1 Components of the g-index

In this section, we describe the details of the components required to compute the g-index bench-
mark. We follow the terminology from Chollet [48], as it is easily mapped to supervised learning
and reinforcement learning. The description of the evaluation setup can be given as follows:

• A task T is speciﬁed to an intelligence system IS.
• The intelligence system generates a skill program P to solve the task. The intelligence
system has been trained on a training set (or a curriculum C) of tasks that may or may not
be related to T .

• A scoring function evaluates the responses of the skill program P against possible situa-

tions of the task, and provides a score along with some feedback if available.

• The intelligence system can be updated based on the evaluation and feedback of the scoring

function.

• The generalization difﬁculty GD(T, C) of a task T measures how different T is from the
curriculum C of the intelligence system. It can be used to weight the system’s performance
for varying degrees of unseen task speciﬁcations.

While previous deﬁnitions of general intelligence rely on quantities like Kolmogorov complexity
[29] which are difﬁcult to compute, the components in our setup together enable computing numer-
ical values that can be combined to measure the capabilities of an intelligence system. The skill
program P is expressed in a custom programming language that can be extended to construct new
programs without additional syntax complexity, which streamlines collecting and augmenting data
for the intelligence system. The scoring function does not require running the program to compute
the score, which means it can also be used as a loss function or reward function for training the
intelligence system. Finally, we construct an intuitive formulation of generalization difﬁculty based
on nearest neighbors that reuses the scoring function. These features are described in the subsections
that follow.

1https://github.com/mayahq/g-index-benchmark

4

2.2 The Task Speciﬁcation

Human beings follow a sequence of steps to perform a speciﬁed task. For an artiﬁcial intelligence
system, the equivalent sequence of steps is the program. Hence to perform a task, the system would
need to generate (or synthesize) programs, when provided a speciﬁcation via examples of inputs, in
natural language, images, audio, or video.

The ﬁeld of program synthesis deals with the automatic construction of programs that are consistent
with a given task speciﬁcation [5]. The common form of a task speciﬁcation is a set of input-output
examples [6, 11] from which the necessary program(s) can be synthesized. The application of deep
learning to program synthesis is called neural program synthesis . Many neural program synthesis
techniques have the task speciﬁed via a set of input-output examples [38] , but some also use natural
language text prompts [41], demonstration videos [46], or combinations of these as well[58, 68].

In our current setup, the tasks submitted to the intelligence system are speciﬁed in English without
listing any input-output examples. When scoring the generated program, an associated reference
program is provided.

2.3 The Skill Program

The choice of target programming language for synthesis varies across implementations. Yin and
Neubig [42] describe the generation of Python code snippets from a given description. Lin et al.
[41] use recurrent neural networks (RNNs) to produce shell scripts. Codex [63], which uses a large
language model similar to GPT-3 [53], generates entire functions in Python from documentation
strings, with similar capabilities being extended to other common programming languages. It is also
common to use a domain-speciﬁc language (DSL). DSLs for program synthesis may be designed
from scratch for a speciﬁc purpose [37], a restricted subset of a language [58, 43] or an extended
version of an existing language [66].

2.3.1 Programs as directed acyclic graphs

In our current setup, the intelligence system synthesizes programs that follow the ﬂow-based pro-
gramming (FBP) paradigm [7, 32]. Flow-based programs are networks of nodes, each encapsulating
a ”black box” process, transferring data across predeﬁned connections. Flow-based programs are
expressed as directed acyclic graphs (DAGs) consisting of nodes with various attributes, that are
designed to be reusable and wired together to perform any task. The exact syntax of ﬂow-based pro-
grams can vary, but can usually be converted into an order-independent array in Javascript Object
Notation (JSON [33, 39]), a data interchange format derived from Javascript. Node-RED [67] and
NoFloJS [61] are popular Javascript packages that enable writing ﬂow-based programs, and both
allow for the programs to be saved as JSON. There are several advantages to have the intelligence
system synthesizing programs in the ﬂow-based paradigm:

• Human-friendly and machine-friendly skill programs: Flow-based program JSONs can
be displayed in a visual programming interface where one can see all the nodes, their
connections, global program conﬁguration and possible errors. This makes it easy for
humans to construct, edit, and interpret ﬂow programs. The key-value syntax of JSON
imposes constraints that make it less complicated than full languages like Javascript and
Python, which helps when synthesizing the program DAG. Figure 2 provides an example
of a ﬂow program along with the JSON speciﬁcation of selected nodes.

• Extensible via custom nodes: Every node in a ﬂow-based program is assigned a special
type attribute, and all nodes with the same type contain the same attribute keys. Most
FBP implementations provide a default library of node types for constructing programs,
but we can also create and add new node types that encapsulate a custom functionality for
our own use.2 This allows for maximum extensibility and reusability within the same level
of expressivity: adding a new node type increases the number of possible skill programs
without changing the language syntax or bloating program size.

• Perform variety of tasks: Each node JSON description in the FBP is an abstraction linked
to a particular black box process. The functionality encapsulated in each node can be

2For examples of reusable nodes see the library in Appendix B.

5

Figure 2: Representing a ﬂow-based program as a DAG: The JSON descriptions of three nodes are
displayed - each node instance has unique id for reference and wires connecting it to other nodes.
For the maya-browser-query type node, the attributes include mergeOutputs and options. The
csv type node has a wire connecting it to the file node via its unique id.

implemented in any programming language across platforms. This means that ﬂow-based
program can be deployed on desktop computers, on servers in the cloud, and even on
embedded devices such as the Raspberry Pi and Arduino. This allows us to specify tasks
that may be performed across multiple devices, locally or over the internet. Figure 3 shows
four different tasks with their associated ﬂow-based skill programs.

• Constrained program design: The design of ﬂow-based programs can be restricted in

three ways:

1. The limited syntax of JSON and the DAG construction makes it difﬁcult to use ad-

vanced programming constructs like recursion and loops.

2. If necessary, the library of available nodes can be customized to prevent the intelli-

gence system from using speciﬁc nodes.

3. If certain node properties are confusing or risky (such as allowing arbitrary code exe-
cution), they can be restricted by designing new nodes that do not allow such modiﬁ-
cations.

Proper restrictions along these axes can limit program aliasing – the existence of multiple
valid programs that satisfy the task speciﬁcation – by providing one obvious direction to
solve a given task.

• Efﬁcient program generation - Program synthesis methods based on deep learning may
have limits on the size of synthesized programs. For instance, neural network architectures
like transformers have a ﬁxed upper bound on the number of tokens that can be generated,
so it is important to use the available token space efﬁciently. With ﬂow-based programs, we
can succinctly specify nodes that perform complex tasks due to the power of encapsulation.
This allows for a larger space from which the intelligence system can generate programs.

6

Figure 3: Program DAGs to : 1. Open browser tab, search for and play a video on Youtube. 2.
Search for a given query in a browser using Google Search, scrape the results and save them to a
ﬁle. 3. Show a dashboard for controlling an Industrial IOT setup and 4. Remotely control a drone
using MQTT messages

• Evaluating a generated program: The programs synthesized by the intelligence system
are evaluated by the scoring function. It is impractical to evaluate the outputs of the skill
program against all possible inputs, so approximate techniques are used, such as comparing
program structure or using special input cases. Programs written in Javascript or Python
may be difﬁcult to evaluate without testing due to program aliasing. Since program aliasing
can be minimized for ﬂow-based programs, they can be evaluated without having to run the
program.

• Language-agnostic skill programs: The nodes used in ﬂow-based programs encapsu-
late black-box processes, which means that the underlying implementations can be in any
programming language. Flow-based programs hence simply act as a coordination layer
between implementations of different pieces of logic. This means that models that learn
how to synthesize these DAGs only need to learn the abstract relationship between task
input and its solution program, leaving the low-level details free to be implemented in any
manner. Figure 4 shows ﬂow-based programs across different tasks.

7

Figure 4: Since ﬂow-based programs are simply the coordination logic to compose any set of pro-
cesses together, these process components can be built using any framework, language, application
or API, leading to a wide range of applications.

2.3.2 DAGs vs Abstract Syntax Trees

While programs in other programming languages can be analysed as graphs by examining their Ab-
stract Syntax Trees (AST) representations, the DAGs of ﬂow-based programs are easier to compare
with one another due to the following reasons:

• The order of nodes in the AST is dependent on the order of text in the program source.
For our program DAGs, the order of the program is speciﬁed implicitly in the program
source by the edges between nodes. DAG comparisons are less affected by the order of
information speciﬁed in the program source compared to AST comparisons.

• AST representations suffer from program aliasing–two programs that satisfy the same task
can have completely different ASTs. For our program DAGs, we can minimize program
aliasing by constraining the kinds of nodes that are allowed for use.

• For complex speciﬁcations, the program size (and therefore AST size) can grow arbitrarily
large if encapsulation is not used, which makes it harder to compare two given programs.
Our program DAGs are designed to beneﬁt from encapsulation: each node can encapsulate
a “black-box” process of arbitrary complexity, and the attributes of the node are used to
examine and control the behavior of the process.

• The AST is a low-level representation of the program: it is used to ensure that the program
source is syntactically valid, check for minor semantic errors (like dereferencing null point-
ers) and perform program optimizations. It is difﬁcult to reason about the behavior of the
program from looking at its AST representation. The DAG is a high-level representation of
the ﬂow-based program: we can infer the program’s general behavior from the DAG struc-
ture and the node types, and examine node attributes to understand or change the behavior
of any component.

2.4 The scoring function

After a skill program has been synthesized by the intelligence system, it is evaluated by a scoring
function. The program can be evaluated by its success on a special set of input-output pairs, or by
match-based metrics. For a given task, match-based metrics compute similarity by comparing the
structure a generated program with a known reference program. This reference could be provided
by a human, generated via a ﬁxed set of rules, augmented from existing data, or synthesized by
another intelligence system. The BLEU score [22] can be used to compare the text of the two given
programs, but it does not consider the structured syntax or the semantic features of the programs.
CodeBLEU [57] improves upon the BLEU score by comparing the abstract syntax trees (AST) and

8

the semantic dataﬂow of the programs. While match-based metrics do not need to run the generated
program for evaluation, they are affected by program aliasing. Recent synthesis methods [50, 56,
63] evaluate programs via functional correctness, wherein a generated program is considered correct
if it passes a set of unit tests.3 Functional correctness is useful because it is similar to how humans
evaluate programs written by each other, but it requires running the generated program to obtain a
score.

2.4.1 Divergence Metric for Flow-Based Programs

In Subsection 2.3 we noted that ﬂow-based programs can be constrained to minimize the program
aliasing, and the DAG representations are easier to compare than ASTs (Subsubsection 2.3.2).
Hence for our scoring function, we use a match-based divergence metric ∆ to compare the DAG
of the generated program with a known reference.

• Let P (cid:48) be a known reference skill program, and P (cid:48)(cid:48) be a skill program generated by the

intelligence system.

• Let G(cid:48)(V (cid:48), E(cid:48)) be the DAG denoting the P (cid:48). Here, V (cid:48) and E(cid:48) refer to the vertices and

edges of G(cid:48) respectively, and E(cid:48) ⊂ V (cid:48) × V (cid:48).

• Let G(cid:48)(cid:48)(V (cid:48)(cid:48), E(cid:48)(cid:48)) be the DAG denoting the program P (cid:48)(cid:48). Here, V (cid:48)(cid:48) and E(cid:48)(cid:48) refer to the

vertices and edges of G(cid:48)(cid:48) respectively, and E(cid:48)(cid:48) ⊂ V (cid:48)(cid:48) × V (cid:48)(cid:48).

Given two programs P (cid:48), P (cid:48)(cid:48), the divergence metric ∆ accepts their DAGs G(cid:48), G(cid:48)(cid:48) as input and is
constrained as follows:

=⇒ ∆(G(cid:48), G(cid:48)(cid:48)) ∈ [0, 1] ∀ DAGs G(cid:48), G(cid:48)(cid:48)

∆ is bounded
∆ is symmetric =⇒ ∆(G(cid:48), G(cid:48)(cid:48)) = ∆(G(cid:48)(cid:48), G(cid:48)) ∀ DAGs G(cid:48), G(cid:48)(cid:48)
∆(G(cid:48), G(cid:48)(cid:48)) = 0 =⇒ G(cid:48), G(cid:48)(cid:48) denote the exact same program P
∆(G(cid:48), G(cid:48)(cid:48)) = 1 =⇒ G(cid:48), G(cid:48)(cid:48) do not have anything in common

(1)

2.4.2 Computing ∆ for a Single Node

Consider the simple case for ∆, where the DAGs G(cid:48) and G(cid:48)(cid:48) both contain only one attributed node
If v(cid:48) is the only node in G(cid:48) and v(cid:48)(cid:48) the only node in G(cid:48)(cid:48), we can compare the
and zero edges.
attributes of v(cid:48) with those in v(cid:48)(cid:48) to obtain a node similarity value w.

• If both nodes have different types, w = 0
• If both nodes have the same type, they will have the same attribute keys. Node attribute
values follow a binary comparison. If p is number of attributes for a given node type, and
pmatch is the number of attribute values that are equal for both nodes, then

w(v(cid:48), v(cid:48)(cid:48)) =

pmatch
p

• If both nodes have the same type, and all node properties are equal, w = 1.

Thus when G(cid:48), G(cid:48)(cid:48) both contain only one node and zero edges, we deﬁne ∆ as

∆(G(cid:48), G(cid:48)(cid:48)) = 1 − (w(v(cid:48), v(cid:48)(cid:48)))2

(2)

We can see that the value of ∆ follows the constraints given in Equation 1 for the simple case.

2.4.3 Computing ∆ for the General Case

When computing the value of ∆(G(cid:48), G(cid:48)(cid:48)) between two arbitrary DAGs, we would need to consider
the common substructure between the DAGs in addition to the node-wise similarity values. The

3Unit tests include a set of known input-output pairs, but may also contain collections of inputs that together

test for the presence of certain properties such as types of failures in a given program.

9

value of ∆ should be low when the DAGs have highly similar structure, so we compute it by ﬁnding
the largest subgraph common to both DAGs. This is known as the maximum common edge sub-
graph problem (MCES) [14], an extension of the subgraph isomorphism problem [10]. Two graphs
G(cid:48)(V (cid:48), E(cid:48)) and G(cid:48)(cid:48)(V (cid:48)(cid:48), E(cid:48)(cid:48)) are isomorphic to each oher (G(cid:48) (cid:39) G(cid:48)(cid:48)) if there exists a bijection
φ : V (cid:48) → V (cid:48)(cid:48) that preserves the graph structure:

(v1, v2) ∈ E(cid:48) ⇐⇒ (φ(v1), φ(v2)) ∈ E(cid:48)(cid:48) ∀(v1, v2) ∈ E(cid:48)

(3)

We need to ﬁnd subgraphs G(cid:48)
opt. This requires that G(cid:48)
G(cid:48)(cid:48)

opt ⊆ G(cid:48) and G(cid:48)(cid:48)

opt ⊆ G(cid:48)(cid:48) that are isomorphic to each other, ie G(cid:48)

opt (cid:39)

opt and G(cid:48)(cid:48)

opt have the same number of vertices, ie

|V (cid:48)

opt| = |V (cid:48)(cid:48)

opt|

and a bijection φopt : V (cid:48)
also need to be the largest common subgraphs

opt → V (cid:48)(cid:48)

opt between the subgraphs that satisﬁes Equation 3. G(cid:48)

opt and G(cid:48)(cid:48)
opt

opt (cid:39) G(cid:48)(cid:48)
G(cid:48)
opt
(cid:64) G(cid:48)
opt2 (cid:39) G(cid:48)(cid:48)
opt2 such that
opt ⊂ G(cid:48)(cid:48)
opt2,G(cid:48)(cid:48)
opt ⊂ G(cid:48)
G(cid:48)

opt2

(4)

opt, G(cid:48)(cid:48)

We obtain G(cid:48)
opt, and the bijective mapping φopt by constructing an association graph G
between the nodes of G(cid:48) and G(cid:48)(cid:48), and ﬁnding a maximum clique in G [9, 12]. Appendix C explains
the process of obtaining φopt in detail, including the rules for construction of the association graph
and ensuring the properties of ∆ from Equation 1 are satisﬁed. Figure 5 provides a visual overview
of the process. Once we obtain φopt, we calculate the value of the metric ∆ is using each node in
opt, and its image via φopt in V (cid:48)(cid:48)
V (cid:48)
1, v(cid:48)

opt = {v(cid:48)

opt. If V (cid:48)

2, ...v(cid:48)

N }, then

∆(G(cid:48), G(cid:48)(cid:48)) = 1 −

((cid:80)N

i=1 w(v(cid:48)

i, φopt(v(cid:48)

i))2

|V (cid:48)||V (cid:48)(cid:48)|

(5)

2.4.4 Features of ∆

While ∆ has an exponential complexity due to the use of subgraph isomorphism, in practice, the
computation of ∆ is sped up as Equation 11 reduces the number of vertices in the association graph
G, and Equation 12 tends to produces sparser graphs. As ∆ does not require the execution of the
generated programs, it can be used in the training loop of supervised or reinforcement learning
methods.
With the match-based DAG divergence metric ∆(G(cid:48), G(cid:48)(cid:48)), we now have a scoring function to com-
pare a generated ﬂow-based program P (cid:48)(cid:48) with a reference program P (cid:48). Flow-based programs can
be expressed in JSON, so a JSON parser can be used to ensure valid syntax. The DAGs constructed
from the valid JSON can then be input to ∆ to compare the semantic dataﬂow of the programs. Since
∆ enables the comparison of any two ﬂow-based programs, it can be extended to measure the dis-
similarity of a program from a given set of programs, and more generally measure the dissimilarity
between two sets of programs, by computing all necessary pairwise comparisons.

The values of ∆ are based on the structural differences between the DAGs and consider degrees of
errors in the generated program:

• Syntax Errors: The generated program P (cid:48)(cid:48) has incorrect JSON syntax, resulting in a re-

duced or incomplete DAG after parsing.

• Function Errors: Some nodes are incorrectly speciﬁed or missing from the generated
DAG. If a node in the generated DAG has only one differing attribute compared to the
reference, it is reﬂected in when computing the node similarity w.

• Dataﬂow Errors: The generated program has the same nodes as the reference, but has

different edges, denoting different semantics.

10

Figure 5: Computing ∆ for the general case: For two DAGs with an arbitrary number of nodes
and edges, we compute the value of ∆ by ﬁnding the largest common subgraph isomorphism φ :
P → Q via the association graph G. In the above ﬁgure, a node (pi, qj) in G refers to a possible
association φ(pi) = qj. An edge between two nodes in G means that the respective associations
satisfy Equation 3. The largest clique in G (with colored nodes and darkened edges) signiﬁes the
maximum association between P and Q i.e. the largest common subgraph. The process is explained
in detail in Appendix C.

2.5 Measuring Generalization Difﬁculty

Humans have the capability to generalize, or they are able to use past learning experience to navigate
new situations in the present. A machine intelligence would need to possess similar capabilities to
deal with new or unseen task speciﬁcations. In machine learning, generalization deals with a model’s
performance on previously unseen data samples that are similar to the distribution of the training set
of the model. A model is said to have overﬁt the training data if predicts well on the training data, but
fails to predict on new unseen samples. In deep learning, it is common to use some regularization
techniques to avoid overﬁtting.

In the context of the g-index and our experimental setup, we focus on the generalization difﬁculty
of a speciﬁed task. For a given task T (cid:48), Chollet [48] informally deﬁnes generalization difﬁculty as
“a measure of how much the shortest training-time solution program needs to be edited in order
to become an appropriate evaluation-time solution program”, and relies on Relative Algorithmic
Complexity to compute the edit difference between the programs. We rephrase this statement for
our experiment setup as follows: Suppose an intelligence system IS, trained on a curriculum of
opt is an appropriate evaluation-time program that solves the task T (cid:48),
tasks C, is given a task T (cid:48). If P (cid:48)
how much does P (cid:48)

opt differ from the training-time solution programs PT for tasks T ∈ C?

Since the skill programs generated by the intelligence system in our experimental setup are ﬂow-
based programs, they can be expressed as DAGs. Therefore, if P (cid:48)
opt is an optimal ﬂow-based pro-
gram that solves the task T (cid:48), we can use the ∆ metric deﬁned in Subsection 2.4 and quantify the
difference between P (cid:48)
opt and programs generated for tasks in the curriculum C by a nearest neighbor
search. Thus, we can deﬁne the domain distance Ω of a single task T (cid:48) for an intelligence system
trained on a curriculum of tasks C as :

11

∆(GP (cid:48)

opt

, GPT )

Ω(T (cid:48), C) = min
T ∈C
where GP (cid:48)
and GP (cid:48)

T

opt

denotes the DAG of the optimal solution P (cid:48)

opt of T (cid:48)
denotes the DAG for a program PT that solves T ∈ C

(6)

Note that Ω is bounded between [0, 1]. If Ω(T (cid:48), C) = 0 it means that the task T (cid:48) can be found in the
training set C, and hence no generalization is required.

Scores of the divergence metric ∆ between skill programs across seven different
Figure 6:
task domains: cron-schedule, form-options, google-search, googlesearch-to-csv,
telegram-2-reply, twitter, and youtube-play. Skill programs within the same domain
have scores closer to 0, and those from different domains have scores closer to 1. A description
of task domains is given in Appendix A.

If C is too large, the value Ω(T (cid:48), C) can be approximated by clustering the elements of C into task
domains using the divergence metric ∆: two tasks within the same domain Ci are “closer” to each
other ( ¯∆ (cid:46) 0.15) than tasks in different domains. Figure 6 shows an example of task domains and
their related divergence scores.
The deﬁnition of Ω(T (cid:48), C) in Equation 6 enables the computation of generalization difﬁculty within
of the training-set/test-set paradigm that is commonly used for training machine learning models.
By computing Ω, a model’s performance on an unseen dataset can be weighted in context of the
generalization difﬁculty of the tasks in the set.

3 The g-index benchmark

3.1 Deﬁning the formula

We now deﬁne the formula for computing the g-index based on the experimental setup deﬁned in
Section 2. First, we describe the necessary variables:

• An intelligence system IS generates a ﬂow-based skill program P (cid:48) when input the spec-
iﬁcation for a given task T (cid:48). This formulation is method-agnostic, and allows the g-
index benchmark to apply not only for deep-learning based systems of today, but also
be extended to any new techniques in the future, by plugging in the variables measured
below.

12

• The intelligence system is trained on a curriculum C, consisting of tasks T and their
associated reference skill programs PT . We expect that the ideal intelligence system would
also be the most sample-efﬁcient in terms of training – it would learn to solve a large variety
of tasks from just a single demonstration. Hence the value of g-index should decrease as
the number of training samples increases.

• A curriculum domain Ci, refers to a subset of C where all tasks T ∈ Ci belong to the same
task domain. We expect that an ideal intelligence system would be able to generalize from
the least number of tasks provided per domain. So, the value of the g-index for the system
should decrease as more tasks are provided for a given domain. We deﬁne WCi ∈ [0, 1],
the weight of considering curriculum tasks from Ci as:

WCi =

1
1 + log2(|Ci|)

• The priors ρ of the system refer to knowledge built into the system before any training.
Examples of priors include previously learnt weights, neural network architectures, data
pre-processing techniques, program optimizations etc. We expect that the ideal intelligence
system would be able to generalize from having the least amount of built-in priors. Hence
the value of the g-index should decrease as more priors are encoded into the system. For
our experiments, we consider the value of ρ as a ﬁxed constant, but we expect this to change
as more complex systems are evaluated.

• When training the intelligence system IS on a curriculum C, it is important to mea-
sure the experience of the system with C. The units of measure for compute power are
FLOPS(Floating Point Operations Per Second) or MIPS(Million Instructions Per Second),
which are reﬂective of the rate at which the system is exposed to the data. We expect
that the ideal intelligence system would be one that exhibits high performance after being
trained for the least amount of time, with the least amount of compute power. So, the value
of the g-index should decrease as the intelligence system is trained for longer and on larger
amounts of compute power. We deﬁne the experience E of the system for a given curricu-
lum C in terms of the compute power used for training IS on C (measured in teraFLOPS)
multiplied by the amount of time IS was trained on C (measured in seconds).

E(C) = log2(compute power used on C · time trained on C)
• A scoring function evaluates the skill program P (cid:48) for the task T (cid:48) to measure the perfor-
mance of the intelligence system IS . We expect that an ideal intelligence system would
produce a perfect skill program P (cid:48) for T (cid:48). So, the value g-index for the system should in-
crease as its performance on the scoring function improves. We use the divergence metric
∆ from Subsection 2.4 and compute the difference of the generated skill program P (cid:48) with
opt to calculate the performance θ of IS on the task T (cid:48) as:
a known reference program P (cid:48)

θ(IS, T (cid:48)) = 1 − ∆(P (cid:48), P (cid:48)
• When testing the capabilities of IS after training on a curriculum C, we wish to see how
IS performs on tasks of varying dissimilarity from C. We expect that the ideal intelligence
system would be able to generalize to tasks that are highly dissimilar from those on which
it was trained. Hence the value of the g-index should increase non-linearly if the system
performs well on tasks of increasing generalization difﬁculty. We deﬁne the generaliza-
tion difﬁculty GD of a task T (cid:48) for a system trained on a curriculum C using Ω deﬁned in
Equation 6 and the exponential function exp:

opt)

GD(T (cid:48), C) = exp(10 · Ω(T (cid:48), C))

(7)

1, T (cid:48)
We use a set of tasks {T (cid:48)
lum C. The task contribution T C of a single task T (cid:48)
generalization difﬁculty GD, the priors ρ, and the experience E:

j, ...} to evaluate an intelligence system IS trained on a curricu-
j to the g-index using the performance θ, the

2, ... T (cid:48)

T C(IS, T (cid:48)

j) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)exp(12 ∗ θ(IS, T (cid:48)

j)) ·

(cid:34)

(cid:88)

Ci⊂C

13

WCi ·

(cid:19)(cid:35)

(cid:18) GD(T (cid:48)

j, Ci)
ρ + E(Ci)

(8)

The constants and component functions used to calculate each variable’s impact are determined by
trends seen during experimentation. Thus we obtain the formula for the g-index by averaging over
the evaluations for the set of tasks {T (cid:48)

j}:

g-index (IS, {T (cid:48)

j}) =

=

1
|{T (cid:48)
j}|

1
|{T (cid:48)
j}|

·

·

(cid:88)

T (cid:48)
j

(cid:88)

T (cid:48)
j

3.2 Properties of the g-index

T C(IS, T (cid:48)
j)

(cid:118)
(cid:117)
(cid:117)
(cid:116)exp(12 ∗ θ(IS, T (cid:48)

j)) ·

(cid:34)

(cid:88)

Ci⊂C

WCi ·

(cid:19)(cid:35)

(cid:18) GD(T (cid:48)

j, Ci)
ρ + E(Ci)

(9)

If the skill of an intelligence system in a particular domain is deﬁned as it’s ability to consistently
generate a set of instructions (or programs) to solve tasks in that domain, the ideal benchmark should
aim to measure the efﬁciency of acquiring new skills. It should penalise the amount of data and
compute power required to train the system, and should reward the performance of the system on
tasks of increasing generalization difﬁculty. If an intelligence system trained using the least amount
of training data and compute power obtains the highest performance on tasks of high generalization
difﬁculty, it should have the highest score on this benchmark. Keeping these constraints in mind,
we observe the responsiveness of the g-index benchmark to variations in the number of training
samples (Figure 7), compute power (Figure 8), performance and generalization difﬁculty (Figure 9)
by running simulations across these variables.

Figure 7: Responsiveness of the g-index to an increase in the number of training samples (the size
of curriculum C). We assume the priors and experience of the system to be constant across all
the lines. Each line assumes the performance of the intelligence system IS to be ﬁxed at a certain
value (shown in the legend) and computes the g-index for the training samples split evenly across all
task domains. The lightly-shaded region around each line denote the variance of the g-index with
respect to the possible uneven distributions across the task domains – uneven distributions may have
noticeable effect on the g-index value, depending on how difﬁcult the tasks were to generalize. We
notice that the g-index steadily decreases as the number of training samples increase. We expect that
the ideal intelligence system would be closer to the top left of the plot to achieve a high g-index .

14

Figure 8: Responsiveness of the g-index to an increase in available computing power (used in
computing E). We assume the priors of the system to be ﬁxed across all the lines. Each line
assumes the performance of the intelligence system IS to be ﬁxed at a certain value (shown in the
legend) and computes the g-index for a given amount compute power, with training samples split
evenly across all task domains. The lightly-shaded region around each line denotes the variance
of the g-index with respect to possible uneven distributions across the task domains – compared to
Figure 7 uneven distributions do not have as high an effect on the g-index as the compute power
increases. We expect that the ideal intelligence system would be closer to the top left of the plot to
achieve a high g-index .

Figure 9: Responsiveness of the g-index to an increase in performance of the system θ. We assume
the priors and experience of the system to ﬁxed across all the points. The color of the points indicates
the domain distance of tasks that the system was tested on – darker points indicate the tasks were of
low domain distance (the system did not display much generalization power), lighter points indicate
the tasks were highly dissimilar from the curriculum C of the system. We note that while the g-
index value increases as performance increases, generalization difﬁculty also plays a huge role,
meaning systems that perform well on tasks with high generalization difﬁculty will have a high g-
index . We expect that the ideal intelligence system would be closer to the top right of the plot to
achieve a high g-index .

15

We see that the value of the g-index decreases with an increase in the number of training samples
from Figure 7, decreases with an increase in compute usage from Figure 8, and increases with an in-
crease performance and generalization difﬁculty from Figure 9, which makes it useful for measuring
skill-acquisition efﬁciency.

3.3 Levels of generalization

From our deﬁnition of the g-index , we reason that general intelligence is not a binary property
that a system either possesses or lacks, but is better described as a continuous spectrum . Where an
intelligence system lies on this spectrum depends on the following factors of the evaluation setup:

• The diversity of the evaluation scope of tasks T (cid:48) - whether or not they lie within the

similar domains where all tasks have low ∆ divergence score relative to each other.

• The generalization difﬁculty of tasks T (cid:48) with respect to curriculum C, or how different
the tasks in the test scope are from the curriculum it has seen. We use the distance score Ω
to refer to this.

• The efﬁciency with which a system converts its priors, experience with curriculum C to a

high performance on the task scope T (cid:48)

We categorise intelligence systems into four levels of generalization power based on the proper-
ties considered above. Each is harder to achieve than the previous one, and the formulation of the
g-index makes it difﬁcult to brute-force a higher score by utilising unlimited amounts of priors,
data and compute. The aim of these levels is to aid subjective discussions about the strengths and
weaknesses of different approaches to build intelligence systems. We note that these levels merely
represent approximate demarcations of generalization difﬁculty; as the measurement of general in-
telligence systems becomes more reﬁned, these demarcations may change, or a new categorisation
may be formulated.

Level 0 - No generalization. L0 broadly describes a system which encounters zero uncertainty in the
tasks on which it is evaluated. This is because all edge cases are handled internally via hard-coded
rules and the system does not act upon any learned heuristic. For example, a sorting algorithm that
outputs a sorted array every time in a rule-based manner, or a chess playing algorithm that iterates
through all possible moves to win a game of chess, can be said to not display any generalization.

Level 1 - Generalization to known variables in known domains. An L1 general intelligence
is able to generalize across predictable amounts of uncertainty within a set of related tasks in the
same domain. Consider a set of task speciﬁcations of the form “Buy X stock every Y hours”.
The variables X and Y here are ‘known variables’. In the program DAG P (cid:48)
opt for any of these
tasks, the variable X maps to the tickername attribute of the submit-order node type and the
variable Y maps to the frequency attribute of the schedule-trigger node type, both of which
are wired together (see Appendix B). The degree of uncertainty is only in the attributes of the nodes
submit-order and schedule-trigger, hence the generalization difﬁculty is low ( ¯Ω (cid:46) 0.15).
If an intelligence system trained only on program DAG samples with different combinations of X
and Y is able to learn to generate the correct program DAG for unknown combinations of X and Y ,
then it can be said to have reached L1 generalization. Current deep learning-based approaches are
successful at attaining this level.

Level 2 - Generalization to unknown variables within known domains. An L2 intelligence sys-
tem is able to generalize to unknown amounts of uncertainty within similar task domains. For exam-
ple, when a system which has been shown two different DAG programs for the task speciﬁcations
“Search for query on Google”, and another to “Save a list of items to ﬁle” is able to successfully
generate a program DAG for the new task speciﬁcation T (cid:48) : “Search for a query on Google and
save results to ﬁle”. This task has mid-range generalization difﬁculty ( 0.4 (cid:46) ¯Ω (cid:46) 0.7) because
the system has to learn how to combine two different program DAGs it has seen before. Unlike
L1, the uncertainty here is not just with the attributes of nodes, but also with the extra nodes and
wires needed to solve the task. This sort of composite synthesis within known domains can be said
an outcome of L2 generalization. While some deep learning systems occasionally demonstrate this
when exposed to large amounts of data and compute power, we expect that sample-efﬁcient methods
of reaching L2 will require new approaches.

16

Level 3 - Generalization to unknown variables across unknown domains. An L3 intelligence
system is able to adapt to arbitrary amounts of uncertainty in the tasks on which it is evaluated. This
is the most challenging level because it requires the system to perform well on tasks with high gen-
eralization difﬁculty ( ¯Ω (cid:38) 0.85), i.e. the program DAG required to solve a task is highly dissimilar
to any task it has seen before. For example, if an intelligence system that is shown only web nav-
igation tasks of the form “Summarize page from Wikipedia.com”, is asked to “Learn how to drive
a Toyota on a given city street”, it has to ﬁnd novel ways to convert its experience in one domain
into mastery in another unknown domain. It could do this is by using its web navigation skills to
watch an online city-driving video tutorial, create a web-based simulation sandbox of the street with
virtual car controls, program new nodes to interface with the controls on a real car, and then generate
a program DAG to drive a car down a street. Current learning methods are insufﬁciently equipped
to create or scale up to such an L3 intelligence system. We expect new methods will need to emerge
which incorporate high sample-efﬁciency, embodied agency, long-term memory and elements of
self-improvement.

4 Experiments

In this section, we compute the values of the g-index and its components for some well-known large
language models. We use a small dataset of text prompts and their associated ﬂow-based programs
to ﬁnetune transformer-based models before measuring their g-index scores. We construct a
small dataset of real-world tasks from 16 task domains to train the models. The task domains are
described in Appendix A. A sample of the dataset is available at https://github.com/mayahq/g-index-
benchmark. We ﬁnetune four transformer models: GPT2-345M, GPT2-774M, and GPT2-1.5B
from [51], and GPT-Neo-2.7B from [62, 55]. We use the HuggingFace implementations [59] of the
transformer models in the experiments.

With the current set of experiments, we aim to measure skill-acquisition efﬁciency via the
g-index with tasks of low generalization difﬁculty. The average domain distance Ω between the
training set and test sets across all experiments is 0.09. The training samples range from 640
to 10240 across all the experiments.
In every experiment, the training samples were distributed
equally across all 16 task domains. After training, the models are tested with 5 unseen samples per
task to obtain their average performance θ. In every experiment, the number of training epochs was
held constant at 30. When synthesizing the programs, we hold the temperature of the models at
a constant 0.7, and allow only one attempt at synthesis. We expect more attempts for a given task
speciﬁcation will yield better performance [63]. We examine the following relationships:

1. average performance θ vs program size (Figure 10): The programs generated for each
task are of different sizes. The size of the program (number of characters in the program
text) affects how easily it can be generated. For instance, the number of tokens transformer
models can generate is bounded by their context window. We expect model performance
to falter as the size of the program to generate increases.

2. skill levels vs program size (Figure 11): The skill of an intelligence system is its ability
to consistently generate correct programs to solve tasks in a given domain. In addition to
performance, a measure of the system’s skill in a particular domain helps contextualize its
potential for real-world use. When choosing an intelligence system to deploy in real-world
tasks, we would prefer a system that generates correct programs more often.

3. average performance θ vs number of training samples (Figure 12): The g-index penal-
izes increments in training samples, but rewards improvements in performance. We expect
that the ideal system with a high g-index would occur at an optimal tradeoff between these
two quantities.

4. average performance θ vs compute used (Figure 13): The g-index penalizes high com-
pute usage, but rewards improvements in performance. Compute is measured in terms of
available compute power and training time, so systems that use multiple processors in par-
allel are penalized accurately. We expect the ideal system with a high g-index would occur
at an optimal tradeoff between compute usage and performance.

17

Figure 10: Average performance θ of each model as program size increases. The point color refers
to a particular model (speciﬁed in the legend). Note that the decrease in θ is not uniform for all
the models, suggesting that raw program length may not directly affect performance. However,
the transformer models we use have a limited context window for generation, hence they falter in
performance when the program size crosses a particular limit.

Figure 11: Skill level of each model as generated program size increases. The point color refers
to a particular model (speciﬁed in the legend). The skill of the system is its ability to generate the
correct program for a given task. For real-world use, we expect the systems to be able to generate
the correct program in one attempt. However, we see that the skill levels of these models do not
show that they can be used in real-world cases reliably just yet. The effect of program size is also
more pronounced: beyond a certain limit, none of the models were able to produce a fully correct
program for the speciﬁed task in one attempt. If the best out of multiple attempts is considered, we
expect skill level to increase.

18

Figure 12: Average performance θ as the number of training samples increases. The bubble color
refers to a particular model (speciﬁed in the legend). The bubble size is a relative measure of
the g-index score. The larger models require more samples to achieve better performance, but
the increased performance does not offset the training samples penalty enough, and so their g-
index scores are lower.

Figure 13: Average performance θ vs compute used. Compute used is measured as total peta ﬂoating
point operations (total petaFLOPS used × total training time in seconds). The bubble color refers
to a particular model (speciﬁed in the legend), and the bubble size is a relative measure of the g-
index score. The larger models take use far more compute, but the improvement in performance is
marginal. This results in their g-index scores being affected by to the compute penalty. Hence the
models with the best performance may not have the best g-index score.

19

Model Name

# Training Samples Compute Used4

θ

1.

GPT2-345M

2. GPT Neo-2.7B

3.

4.

5.

GPT2-1.5B

GPT2-1.5B

GPT2-774M

6. GPT Neo-2.7B

7.

8.

GPT2-345M

GPT2-774M

2560

2560

5120
10240+

2560
1280−
1280−

5120

127.530

8969.100

5927.400
11563.320+

1516.640

5063.380
74.750−

2941.941

0.697

0.682
0.708+

0.683

0.620

0.582
0.547−

0.585

g-index
7902.972+

6421.049

6390.314

6006.261

4872.334

4476.680

4399.190
4070.117−

Table 1: The best 8 performing models sorted according to their g-index scores. Values with + indi-
cate the maximum of the column, and values with − indicate the minimum. We note that the system
with the best θ is not the one with the highest g-index , and the system with the worst θ score does
not have the lowest g-index . The differences between the top three rows indicate the tradeoffs in-
volved: while row 3 has better performance with less compute, the increased performance does not
offset the penalty of double the number of training samples.

Different intelligence systems may obtain the best measurement at any individual component of
the g-index . However, the best-performing system may not be the most resource-efﬁcient, and
vice-versa. The ideal system would be one that has the right combination of priors, experience,
sample-efﬁciency, and maximal performance. Table 1 shows the top 8 models according to their
g-index values, along with the models’ best component scores.

5 Flatland - a toy environment for the g-index

While the evaluation setup in Subsection 2.1 maps well to real-world tasks, the development of
new learning methods will require toy environments with simpler program spaces and diverse task
speciﬁcations. Since the g-index rewards sample-efﬁciency and performance on tasks of high gen-
eralization difﬁculty, new methods that score a high g-index within these environments will help
surface insights for real-world tasks. Recent efforts in program synthesis use visual examples as
training input to the system, such as images [45, 54] or video [46, 68]. To examine the g-index in
such cases, we construct a toy environment called flatland, where intelligence systems must infer
the correct program to draw shapes in an image.

Figure 14: An example task in flatland with associated program and JSON representation. The
JSON representation is similar to the ﬂow-based programs for real-world tasks shown in Figure 2.

4Compute used is measured as total peta ﬂoating point operations (total petaFLOPS used × total training

time in seconds)

20

The evaluation setup for flatland is similar to Section 2 and is described below5 :

• The task T (cid:48) speciﬁed to the intelligence system IS is a 128x128 binary image. Each image

contains shapes drawn using the commands loop, move, and turn as primitives.

• The program spec P (cid:48) generated by IS can be expressed in a ﬂow-based syntax, LISP
expressions, or as JSON similar to programs for real-world tasks. Figure 14 shows an
example. We use the turtle graphics library in Python [64] (similar to LOGO [8]) to
draw the images from the DAG program speciﬁcation. The DAG program is converted into
a JSON list of primitives for comparison.

• The curriculum C for IS consists of images T and the corresponding DAG programs P
that generate them. flatland has in-built data augmentation to create programs with slight
variations from an existing program.

• The scoring function evaluates the output of IS to the ideal program P (cid:48) with a match-
based metric ∆ that calculates the largest common subsets between the two lists. The
largest common subset is computed via an association graph, similar to Subsection 2.4.
• The generalization difﬁculty GD(T (cid:48), C) of a task T (cid:48) with the ideal program P (cid:48) is computed
by ﬁnding the nearest neighbor of P (cid:48) in C, similar to Subsection 2.5. Figure 15 shows
some test samples along with their domain distance Ω from a curriculum C of just lines
and circles.

Figure 15: Samples from the training and test sets for flatland. The training set consists of various
images containing one to ﬁve circles or lines. Above each test sample we display its domain distance
Ω from the training set.

With flatland, we wish to examine intelligence systems by their g-index values in a controlled
environment, so that we can test which learning methods generalize well when some constraints are
relaxed. We also hope to improve the g-index deﬁnition by varying the evaluation setup itself. In
the flatland environment, every aspect of program synthesis can be controlled and tested:

• The flatland environment currently provides programs that produce images, but this can
be changed to accompany the image with text, or even use video. The program can be
expressed in a ﬂow-based syntax, or with LISP expressions, or also as JSON. This can help
test synthesis methods that depend on different modalities.

• The DAG programs in flatland currently consist of three primitives loop, move, and
turn. By varying the base set of primitives, we can attempt to ﬁnd an ideal combination
with which a system can learn to express the widest range of programs.

5Code available at https://github.com/mayahq/ﬂatland

21

• The primitives in flatland can be combined to draw shapes, which can further be com-
bined to produce more complex drawings. Thus new programs can be created via reuse
and composition of existing programs, while keeping the program text minimal. This hi-
erarchical composability could help create more ﬂexible learners which build concepts on
top of existing concepts to synthesize programs.

• flatland has in-built data augmentation to create programs with slight variations from
an existing program, allowing the generation of arbitrarily large datasets on demand. As
flatland programs are structurally similar to those used for real-world tasks (see Fig-
ure 14 and Figure 2), this could help assess the format, quantity, and diversity of training
data that will be needed if a system has to adapt to tasks in the real world.

6 Summary and Future Directions

In this paper, we describe an experiment framework to obtain a quantitative measurement of skill-
acquisition efﬁciency of machine intelligence systems. We model the intelligence system to accept a
wide range of real-world tasks speciﬁed in natural language and synthesize programs representable
as directed acyclic graphs in a ﬂow-based programming syntax. We deﬁne a match-based metric to
compare the DAG structures of two given programs to score the performance of the system, and use
this metric to measure the generalization difﬁculty of any task provided to the system. We formulate
the g-index benchmark and show that its changes with respect to dataset size, available compute,
and performance align with intuitive expectations of an intelligence system with high generalization
power. We then measure and compare the g-index scores of some ﬁne-tuned transformer models
and estimate their suitability for general-purpose intelligence systems. Finally, we provide a toy
environment with a relaxed set of constraints to examine potential designs and improvements to the
g-index .

While the g-index benchmark shifts the evaluation of intelligence systems into a quantitative context
akin to skill-based evaluations, it is not yet a complete measure of general machine intelligence.
However, we believe that future evaluations of intelligence systems will require a similar framework,
one that reﬂects the potential real-world use of such systems. Over the course of our experiments,
we have found some possible directions for improving the g-index measurement. We describe these
possibilities and their effects below.

The evaluation framework can be improved to represent more real-world use cases for a machine
intelligence system. The task speciﬁcation can be expanded to include different natural languages,
audio, video, and input-output examples. The task may even be speciﬁed in parts across an interac-
tion between a human and the system [60]. Flow-based programs face limitations with maintaining
state, so there is potential for improving the language of synthesized programs to account for multi-
stage tasks. It is also possible to create more nodes with new functionality, enabling the construction
of larger, diverse datasets of task speciﬁcations and their associated programs. With larger datasets,
better data augmentation techniques can be built to generate reference programs to evaluate tasks. As
more nodes are designed and ﬂow-based programs grow larger, the subgraph isomorphism compu-
tation with ∆ may slow down, and the chance of program aliasing issues may also increase. In such
cases, we may also need to use functional correctness like [63] to score the programs synthesized by
the system.

With a given a set of tasks, the components of the g-index such as compute and domain distance can
be reﬁned by testing with a wider variety of intelligence systems, to ensure that the g-index value
accurately represents their capabilities. Additionally, since the intelligence systems in our frame-
work are evaluated on real-world tasks, human feedback can be incorporated when attempting to
understand or improve the calculation of these components.

After calculating the scores for a wide range of systems, the g-index formula would need to be
updated with additional speciﬁcations. If a system’s g-index score is unnaturally high, perhaps the
system actually exhibits high skill-acquisition efﬁciency, or the formula contains some poorly spec-
iﬁed variables which unfairly portray the system’s ability. For example, we consider the weightage
for the priors encoded in the system (ρ in Equation 8) to be a constant neglible value for the current
set of experiments that use transformer models ﬁne-tuned from pre-trained weights. When compar-
ing the g-index of a ﬁne-tuned model to a model trained from scratch, the priors/compute tradeoff
would be different, but it is not clear how such differences can be measured. Going further, it is

22

difﬁcult to quantify the beneﬁt of priors like hyperparameters, data preprocessing, hardcoded rules
and initial primitives in a manner that translates fairly across different kinds of intelligence systems.
Perhaps the weight of some priors Pk can be calculated by comparing the system’s performance
with and without the prior:

ρ(Pk) ∝

g-index (IS with prior Pk, T (cid:48))
g-index (IS without prior Pk, T (cid:48))

(10)

The arrangement of Equation 10 along with the levels of generalization discusssed in Subsection 3.3
leads to an interesting question. When comparing machine intelligence with human intelligence,
given that human beings have had thousands of years to build priors, should human priors have
a weight of ρhuman → ∞? Is it necessary to compare the built-in priors of humans with that of
machines?

The overarching aim of this work is not only to propose a method-agnostic way to compare different
techniques quantitatively, but also to spark a conversation on the relative merits of different ap-
proaches that could help reach higher levels of general machine intelligence. While we don’t expect
our g-index deﬁnition to be complete, it anchors a previously abstract concept in a mathematical
formulation made up of quantities that can be measured during experiments. The ﬂow-based pro-
gramming language we propose can act as a common language to express and compare programs
of real-world utility across a wide variety of domains. The g-index explicitly rewards resource-
efﬁciency, which we hope incentivises sustainable ways of achieving generalization which do not
rely on unlimited amounts of compute and data. A good outcome of this would be the patronage and
competitive development of new methods and technologies to reach higher g-index levels, similar
in spirit to the Hutter compression challenge [65]. Ultimately, our belief is that intelligent machines
will only be able to contribute to real technological progress when they can learn how do more from
less, not the other way around.

23

References

[1] David Wechsler. “The measurement of adult intelligence”. In: (1944).
[2] Alan M Turing and J Haugeland. Computing machinery and intelligence. MIT Press Cam-

bridge, MA, 1950.

[3] Alfred Binet and Theophile Simon. “The development of intelligence in children.” In: (1961).
[4] M Ross Quillian and Marvin Minsky. Semantic information processing. 1968.
[5] RJ Waldinger and RCT PROW LEE. “A step toward automatic program writing”. In: Proc.

[6]
[7]

Int. Joint Conf. on Artiﬁcial Intelligence (Washington DC). 1969, pp. 241–252.
Saul Amarel. “Representations and modeling in problems of program formation”. In: (1970).
J Paul Morrison. “Data responsive modular, interleaved task programming system”. In: IBM
Technical Disclosure Bulletin 13.8 (1971), pp. 2425–2426.

[8] Wallace Feurzeig and George Lukas. “LOGO—A programming language for teaching math-

ematics”. In: Educational Technology 12.3 (1972), pp. 39–46.

[9] Harry G Barrow and BURSTALL RM. “Subgraph isomorphism, matching relational struc-

[10]

[11]

tures and maximal cliques.” In: (1976).
Julian R Ullmann. “An algorithm for subgraph isomorphism”. In: Journal of the ACM
(JACM) 23.1 (1976), pp. 31–42.
Phillip D Summers. “A methodology for LISP program construction from examples”. In:
Journal of the ACM (JACM) 24.1 (1977), pp. 161–175.

[12] Dexter Kozen. “A clique problem equivalent to graph isomorphism”. In: ACM SIGACT News

[13]

[14]

10.2 (1978), pp. 50–52.
John R Searle. “Minds, brains, and programs”. In: Behavioral and brain sciences 3.3 (1980),
pp. 417–424.
Shahid H. Bokhari. “On the mapping problem”. In: IEEE Trans. Computers 30.3 (1981),
pp. 207–214.

[15] Gregory J Chaitin. “G¨odel’s theorem and information”. In: International Journal of Theoret-

ical Physics 21.12 (1982), pp. 941–954.

[16] Marvin Minsky. “Semantic information processing”. In: (1982).
[17] B Chandrasekaran. “What kind of information processing is intelligence?” In: The foundation

of artiﬁcial intelligence—a sourcebook. 1990, pp. 14–46.

[18] Linda S. Gottfredson. “Mainstream science on intelligence: An editorial with 52 signatories,
history, and bibliography”. In: Intelligence 24.1 (1997). Special Issue Intelligence and So-
cial Policy, pp. 13–23. ISSN: 0160-2896. DOI: https : / / doi . org / 10 . 1016 / S0160 -
2896(97)90011-8. URL: https://www.sciencedirect.com/science/article/pii/
S0160289697900118.

[19] Daniel Clement Dennett. Brainchildren: Essays on designing minds. MIT Press, 1998.
[20] Yann LeCun. “The MNIST database of handwritten digits”. In: http://yann.

lecun.

com/exdb/mnist/ (1998).

[21] Arthur R Jensen. “The g factor: The science of mental ability”. In: Psicothema 11.2 (1999),

pp. 445–446.

[22] Kishore Papineni et al. “Bleu: a method for automatic evaluation of machine translation”. In:
Proceedings of the 40th annual meeting of the Association for Computational Linguistics.
2002, pp. 311–318.
Selmer Bringsjord, Paul Bello, and David Ferrucci. “Creativity, the Turing test, and the (bet-
ter) Lovelace test”. In: The Turing Test. Springer, 2003, pp. 215–239.

[23]

[24] Luiz S Oliveira and Robert Sabourin. “Support vector machines for handwritten numerical
string recognition”. In: Ninth International Workshop on Frontiers in Handwriting Recogni-
tion. IEEE. 2004, pp. 39–44.

[25] Daniel Keysers. “Comparison and combination of state-of-the-art techniques for handwritten
character recognition: topping the mnist benchmark”. In: arXiv preprint arXiv:0710.2231
(2007).
Shane Legg and Marcus Hutter. “Universal intelligence: A deﬁnition of machine intelli-
gence”. In: Minds and machines 17.4 (2007), pp. 391–444.

[26]

24

[27] VN Manjunath Aradhya, G Hemantha Kumar, and S Noushath. “Unconstrained Handwritten
Digit Recognition: Experimentation on MNIST Database”. In: Advances In Pattern Recog-
nition. World Scientiﬁc, 2007, pp. 140–143.

[28] Cassio Pennachin and Ben Goertzel. “Contemporary Approaches to Artiﬁcial General In-
telligence”. In: Artiﬁcial General Intelligence. Ed. by Ben Goertzel and Cassio Pennachin.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, pp. 1–30. ISBN: 978-3-540-68677-4.
DOI: 10.1007/978- 3- 540- 68677- 4_1. URL: https://doi.org/10.1007/978- 3-
540-68677-4_1.

[29] Ming Li, Paul Vit´anyi, et al. An introduction to Kolmogorov complexity and its applications.

[30]

[31]

[32]

Vol. 3. Springer, 2008.
Jia Deng et al. “Imagenet: A large-scale hierarchical image database”. In: 2009 IEEE confer-
ence on computer vision and pattern recognition. Ieee. 2009, pp. 248–255.
J. Searle. “Chinese room argument”. In: Scholarpedia 4.8 (2009). revision #66188, p. 3100.
DOI: 10.4249/scholarpedia.3100.
J Paul Morrison. Flow-Based Programming: A new approach to application development.
CreateSpace, 2010.

[33] Douglas Crockford. “Json”. In: ECMA International (2012).
[34] David L Dowe and Jos´e Hern´andez-Orallo. IQ tests are not for machines, yet. 2012.
[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “Imagenet classiﬁcation with deep
convolutional neural networks”. In: Advances in neural information processing systems 25
(2012), pp. 1097–1105.

[36] Mark O Riedl. “The Lovelace 2.0 test of artiﬁcial creativity and intelligence”. In: arXiv

preprint arXiv:1410.6142 (2014).

[37] Mohammad Raza, Sumit Gulwani, and Natasa Milic-Frayling. “Compositional program syn-
thesis from natural language and examples”. In: Twenty-Fourth International Joint Confer-
ence on Artiﬁcial Intelligence. 2015.

[38] Emilio Parisotto et al. Neuro-Symbolic Program Synthesis. 2016. arXiv: 1611 . 01855

[39]

[40]

[cs.AI].
Felipe Pezoa et al. “Foundations of JSON schema”. In: Proceedings of the 25th Interna-
tional Conference on World Wide Web. International World Wide Web Conferences Steering
Committee. 2016, pp. 263–273.
Jos´e Hern´andez-Orallo. “Evaluation in artiﬁcial intelligence: from task-oriented to ability-
oriented measurement”. In: Artiﬁcial Intelligence Review 48.3 (Oct. 2017), pp. 397–447.
ISSN: 1573-7462. DOI: 10 . 1007 / s10462 - 016 - 9505 - 7. URL: https : / / doi . org /
10.1007/s10462-016-9505-7.

[41] Xi Victoria Lin et al. “Program synthesis from natural language using recurrent neural net-
works”. In: University of Washington Department of Computer Science and Engineering,
Seattle, WA, USA, Tech. Rep. UW-CSE-17-03-01 (2017).
Pengcheng Yin and Graham Neubig. A Syntactic Neural Model for General-Purpose Code
Generation. 2017. arXiv: 1704.01696 [cs.CL].

[42]

[43] Rudy Bunel et al. “Leveraging Grammar and Reinforcement Learning for Neural Program
Synthesis”. In: CoRR abs/1805.04276 (2018). arXiv: 1805.04276. URL: http://arxiv.
org/abs/1805.04276.
Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers for language under-
standing”. In: arXiv preprint arXiv:1810.04805 (2018).

[44]

[45] Kevin Ellis et al. Learning to Infer Graphics Programs from Hand-Drawn Images. 2018.

[46]

arXiv: 1707.09627 [cs.AI].
Shao-Hua Sun et al. “Neural program synthesis from diverse demonstration videos”. In: In-
ternational Conference on Machine Learning. PMLR. 2018, pp. 4790–4799.

[47] Alex Wang et al. “GLUE: A multi-task benchmark and analysis platform for natural language

understanding”. In: arXiv preprint arXiv:1804.07461 (2018).
Franc¸ois Chollet. “On the Measure of Intelligence”. In: CoRR abs/1911.01547 (2019). arXiv:
1911.01547. URL: http://arxiv.org/abs/1911.01547.
Jie Hu et al. Squeeze-and-Excitation Networks. 2019. arXiv: 1709.01507 [cs.CV].

[48]

[49]

25

[50]

Sumith Kulal et al. “Spoc: Search-based pseudocode to code”.
arXiv:1906.04908 (2019).

In: arXiv preprint

[51] Alec Radford et al. “Language models are unsupervised multitask learners”. In: OpenAI blog

1.8 (2019), p. 9.

[52] Lucas Beyer et al. “Are we done with imagenet?” In: arXiv preprint arXiv:2006.07159

(2020).

[53] Tom B Brown et al. “Language models are few-shot

learners”. In: arXiv preprint

arXiv:2005.14165 (2020).

[54] Kevin Ellis et al. “Dreamcoder: Growing generalizable, interpretable knowledge with wake-

sleep bayesian program learning”. In: arXiv preprint arXiv:2006.08381 (2020).

[55] Leo Gao et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. In:

arXiv preprint arXiv:2101.00027 (2020).

[56] Marie-Anne Lachaux et al. “Unsupervised translation of programming languages”. In: arXiv

[57]

preprint arXiv:2006.03511 (2020).
Shuo Ren et al. “Codebleu: a method for automatic evaluation of code synthesis”. In: arXiv
preprint arXiv:2009.10297 (2020).

[58] Kensen Shi, David Bieber, and Rishabh Singh. TF-Coder: Program Synthesis for Tensor

Manipulations. 2020. arXiv: 2003.09040 [cs.PL].

[59] Thomas Wolf et al. “Transformers: State-of-the-Art Natural Language Processing”. In: Pro-
ceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
ing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020,
pp. 38–45. URL: https://www.aclweb.org/anthology/2020.emnlp-demos.6.
Jacob Austin et al. Program Synthesis with Large Language Models. 2021. arXiv: 2108 .
07732 [cs.PL].

[60]

[61] Henri Bergius. NoFloJS: Flow-based programming for Javascript. Version 1.3.0. Aug. 14,

[62]

2021. URL: https://noflojs.org/.
Sid Black et al. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-
Tensorﬂow. Version 1.0. 2021. URL: http://github.com/eleutherai/gpt-neo.
[63] Mark Chen et al. “Evaluating Large Language Models Trained on Code”. In: (2021). arXiv:

[64]

2107.03374 [cs.LG].
Python Software Foundation. turtle graphics in Python 3.7.12. Version 3.7.12. Sept. 6, 2021.
URL: https://docs.python.org/3.7/library/turtle.html.

[65] Marcus Hutter. Hutter Prize: Human Knowledge Compression Contest. Sept. 22, 2021. URL:

http://prize.hutter1.net/.

[66] Vadim Liventsev, Aki H¨arm¨a, and Milan Petkovic. “BF++: a language for general-purpose
neural program synthesis”. In: CoRR abs/2101.09571 (2021). arXiv: 2101 . 09571. URL:
https://arxiv.org/abs/2101.09571.

[67] OpenJS Foundation. Node-RED: Low-code programming for event-driven applications. Ver-

sion 2.0.3. July 26, 2021. URL: https://nodered.org/about/.

[68] Tianmin Shu et al. “AGENT: A Benchmark for Core Psychological Reasoning”. In: arXiv

preprint arXiv:2102.12321 (2021).

26

A Sample Task Domains and Descriptions

Domain Name

Example Text Prompt

Nodes

Program
size(chars)

Best Avg Per-
formance

Skill
Level **

template-slider

Two sliders with values ranging from 0 to 20 changing
in steps of 2

template-form

Create a form with ﬁelds for entering Name, Bench-
mark Score and Date of Submission

cron-reminder

cron-schedule

facebook

gmail-send

Set a reminder for ’Send Daily Digest’ every ﬁrst day
of the week, Tuesday through Saturday, only in Jan-
uary

Repeat At 48, 9, 14, 7, 6, 56, 46, 39, 15, 3, 37, and 30
minutes past the hour, between 05:00 AM and 08:59
PM, on day 1,4,8 and 9 of the month, only on Tues-
day, every 3 months, November through December

On Facebook, when user says ’Hello’, reply with ’Hi
there! How can I help you?’, and when user says
’Bye’, reply with ’Goodbye!’

Send email with body ’Upcoming Meeting’ and
subject
to email al-
pha@beta.com

’Discuss paper appendices’

6

5

3

3

8

1493

1.00

1473

1.00

935

1.00

1.00

1.00

1.00

896

1.00

1.00

2253

1.00

1.00

12

4708

0.63

0.00

google-search

Search Google for ’How to make tables on LaTeX’
and scrape results

googlesearch-to-
csv

Search Google for ”Papers on measuring general in-
telligence”, scrape results and put into singularity.csv

http

Create a HTTP POST endpoint called /agents

telegram-2-reply

Reply ’Yes, detective?’ to ’Sonny!’, and ’Of course,
Dave’, to ’Open the pod bay doors’ on Telegram

9

12

4

8

2963

0.60

3894

0.59

857

2148

telegram-3-reply

On Telegram, when user says ’Are friends electric?’,
reply with ’No, only sheep’, when user says ’How
deep is your love?’ reply with ’6.5 meters’, and when
user says ’Is that all there is?’, reply with ’Yes’

10

2910

twitter

url-skill

Obtain tweets about the topic #Alignment

Create a skill called ’Open LessWrong’ which opens
url https://www.lesswrong.com/

youtube-pause

Pause Youtube video

youtube-play

Find and play Vivaldi Four Seasons on Youtube

youtube-resume

Resume Youtube Video

4

4

3

9

3

892

1067

919

3050

916

1.0

1.0

1.0

1.0

1.0

1.0

0.89

1.0

0.0

0.0

1.0

1.0

1.0

1.0

1.0

1.0

0.0

1.0

Table 2: Task domains consist of DAGs with mean Ω < 0.1, i.e. with different types of known variables

B Node Library

Categories

Description

Node Types

common utility

Custom triggers, catch bugs, add comments

inject, debug, complete, catch,
status, link in, link out, comment

functional

network

sequence

parser

storage

dashboard

Change, switch, ﬁlter or delay the passed mes-
sage object or add custom logic to manipulate
it

function, switch, change, range,
template, delay, trigger, exec,
filter

Different kinds of network interfaces to send
and receive data

mqtt in, mqtt out, http in, http
response, http request, websocket
in, websocket out, tcp in, tcp out,
tcp request, udp in, udp out

Manipulate sequences and arrays in pre-
dictable ways

split, join, sort, batch

Parse data from different ﬁles into ﬁxed for-
mats for easy processing

csv, html, json, xml, yaml

Read and write to ﬁles

file, file in, watch

Make dynamic dashboards with forms and
charts by linking UI elements to other pieces
of logic

browser-automation

Interact with the browser to navigate the web
and scrape websites

button, dropdown, switch, slider,
numeric, text input, date picker,
colour picker, form, text, gauge,
chart, audio out, notification

open, click, type, press, execute
function, find tab, scrape, query,
bookmark

spotify-automation

Integrate with spotify and control music
played on any device

play, search, control playback, get
playback state, control playlist

gdrive-automation

Search through, read, export and append to
ﬁles in on your google drive

search gdrive, gsheet append,
gdrive-export-file

scheduling

Schedule triggers to run events at any ﬁxed in-
terval

schedule-trigger

zoom-automation

Create, view and attend zoom meetings

system utilities

Interact with various system level utilities on
the desktop

create-meeting, list-meetings,
list-meetings-registrants

clipboard-add, clipboard-get,
open-target, file-search,
desktop-notify

stock-automation

Buy, sell and view orders on the stock market
via third party API

submit-order, get-order, get-bars,
get-account

home-automation

Control lights and switches remotely

light-control, switch-control

Table 3: Each node is a reusable, encapsulated ”black box” function that can be wired to other nodes to form a program DAG and
automate any task.

C Calculating ∆ via subgraph isomorphism

When computing the value of ∆ between two arbitrary DAGs, we use the similarity function w to
compare individual nodes, and account for structural similarity by computing the largest subgraph
common to both DAGs. This is known as the maximum common edge subgraph problem, [14],
an extension of the subgraph isomorphism problem [10]. In this section, we explain in detail the
calculation of ∆ outlined in Subsubsection 2.4.3.

Two graphs G(cid:48)(V (cid:48), E(cid:48)) and G(cid:48)(cid:48)(V (cid:48)(cid:48), E(cid:48)(cid:48)) are isomorphic to each oher (G(cid:48) (cid:39) G(cid:48)(cid:48)) if there exists a
bijection φ : V (cid:48) → V (cid:48)(cid:48) that preserves the graph structure:

(v1, v2) ∈ E(cid:48) ⇐⇒ (φ(v1), φ(v2)) ∈ E(cid:48)(cid:48) ∀(v1, v2) ∈ E(cid:48)

(3)

opt (cid:39) G(cid:48)(cid:48)

opt ⊆ G(cid:48) and G(cid:48)(cid:48)

opt ⊆ G(cid:48)(cid:48) that are isomorphic to each other
We need to ﬁnd the largest subgraphs G(cid:48)
(G(cid:48)
opt), and the isomorphism φopt between the nodes of the two subgraphs. We obtain
these by constructing the association graph G and ﬁnding a maximum clique in G. The association
graph G(V, E) for given graphs G(cid:48), G(cid:48)(cid:48) contains the vertices V ⊂ V (cid:48) × V (cid:48)(cid:48) × (0, 1]: a vertex
(v(cid:48), v(cid:48)(cid:48), w) ∈ V associates a node v(cid:48) ∈ V (cid:48) to a node v(cid:48)(cid:48) ∈ V (cid:48)(cid:48) with their node similarity w(v(cid:48), v(cid:48)(cid:48)).
Only nodes of the same type are considered for association:

(v(cid:48)

i, v(cid:48)(cid:48)

i , wi) ∈ V ⇐⇒ wi = w(v(cid:48)

i, v(cid:48)(cid:48)

i ) > 0

(11)

An edge in G connects a vertex (v(cid:48)
1, v(cid:48)(cid:48)
structure-preserving condition is satisﬁed (note the resemblance to Equation 3):

1 , w1) to another vertex (v(cid:48)

2, v(cid:48)(cid:48)

2 , w2) provided the below

((v(cid:48)

1, v(cid:48)(cid:48)

1 , w1), (v(cid:48)

2, v(cid:48)(cid:48)

2 , w2)) ∈ E ⇐⇒ ((v(cid:48)

1, v(cid:48)

2) ∈ E(cid:48) ⇐⇒ (v(cid:48)(cid:48)

1 , v(cid:48)(cid:48)

2 ) ∈ E(cid:48)(cid:48))

(12)

It can be shown that ﬁnding a maximum clique6in the association graph G is equivalent to ﬁnding
the largest common subgraph between G(cid:48) and G(cid:48)(cid:48) [9, 12]. Once a maximum clique in G has been
opt. Since nodes in the DAGs G(cid:48)
obtained, we can construct the common subgraphs G(cid:48)
and G(cid:48)(cid:48) cannot have self-loops, Equation 12 ensures that any clique in the association graph E will
always provide a one-to-one mapping between the corresponding vertices of G(cid:48) and G(cid:48)(cid:48). If

opt and G(cid:48)(cid:48)

Copt = {(v(cid:48)

1, v(cid:48)(cid:48)

1 , w1), (v(cid:48)

2, v(cid:48)(cid:48)

2 , w2)...(v(cid:48)

N , v(cid:48)(cid:48)

N , wN )}

is the maximum clique in the association graph G, then

G(cid:48)

opt = G(cid:48)(V (cid:48)

opt, the subgraph of G(cid:48) induced from V (cid:48)

opt = {v(cid:48)

1, v(cid:48)

2...v(cid:48)

N } and

G(cid:48)(cid:48)

opt = G(cid:48)(cid:48)(V (cid:48)(cid:48)

opt), the subgraph of G(cid:48)(cid:48) induced from V (cid:48)(cid:48)

opt = {v(cid:48)(cid:48)

1 , v(cid:48)(cid:48)

2 ...v(cid:48)(cid:48)

N }

are the required largest common subgraphs. The maximum subgraph isomorphism φopt is the map-
ping expressed via the pairs (v(cid:48)

i ) in the elements of of the clique Copt.

i, v(cid:48)(cid:48)

φopt : V (cid:48)

opt → V (cid:48)(cid:48)

opt =⇒ φopt(v(cid:48)

i) = v(cid:48)(cid:48)

i , where (v(cid:48)

i, v(cid:48)(cid:48)

i , w(v(cid:48)

i, v(cid:48)(cid:48)

i )) ∈ C ∀ v(cid:48)

i ∈ V (cid:48)
opt

Thus with Copt, G(cid:48)
vided in Equation 5:

opt, G(cid:48)(cid:48)

opt, and φopt we can compute the value of the metric ∆(G(cid:48), G(cid:48)(cid:48)) as pro-

6Note that the node similarities wi are used to ﬁlter out elements from V and when computing the maximum

clique (a node-weight maximum clique is computed).

∆(G(cid:48), G(cid:48)(cid:48)) = 1 −

∆(G(cid:48), G(cid:48)(cid:48)) = 1 −

= 1 −

((cid:80)N

i=1 w(v(cid:48)

i, φopt(v(cid:48)

i))2

|V (cid:48)||V (cid:48)(cid:48)|
i )2
i, v(cid:48)(cid:48)

((cid:80)N

i=1 w(v(cid:48)
|V (cid:48)||V (cid:48)(cid:48)|
((cid:80)N
i=1 w2
i
|V (cid:48)||V (cid:48)(cid:48)|

, (v(cid:48)

i, v(cid:48)(cid:48)

i , w(v(cid:48)

i, v(cid:48)(cid:48)

i ) = wi) ∈ C

(5)

(13)

We note that ∆ is 1 when either of the DAGs are empty, and reduces to the node similarity function
w in Equation 2 when G(cid:48), G(cid:48)(cid:48) both have one vertex and zero edges. ∆ also satisﬁes the constraints
in Equation 1.

• ∆ is bounded:

wi ∈ [0, 1] ∀ wi

n
(cid:88)

i=1
n
(cid:88)

=⇒

wi = |V (cid:48)

opt| = |V (cid:48)(cid:48)

opt| ⇐⇒ wi = 1 ∀ wi

wi ≤ |V (cid:48)

opt| = |V (cid:48)(cid:48)

opt|

i=1
|V (cid:48)| = |V (cid:48)

opt| = |V (cid:48)(cid:48)

opt| = |V (cid:48)(cid:48)| ⇐⇒ G(cid:48) (cid:39) G(cid:48)(cid:48)

(14)

=⇒ |V (cid:48)(cid:48)

opt| ≤ |V (cid:48)|, |V (cid:48)(cid:48)
((cid:80)n
i=1 wi)2
|V (cid:48)||V (cid:48)(cid:48)|

opt| ≤ |V (cid:48)(cid:48)|
opt||V (cid:48)(cid:48)
|V (cid:48)
opt|
|V (cid:48)||V (cid:48)(cid:48)|
=⇒ ∆(G(cid:48), G(cid:48)(cid:48)) ∈ [0, 1] ∀ G(cid:48), G(cid:48)(cid:48)

=⇒ 0 ≤

≤

≤ 1

• ∆ is symmetric because the node similarities wi are symmetric, and ﬁnding the largest
opt, we can
opt, and use its inverse φ−1 for the symmetric

common subgraph between two graphs is also symmetric: since G(cid:48)
construct a bijection φ between G(cid:48)
case.

opt and G(cid:48)(cid:48)

opt (cid:39) G(cid:48)(cid:48)

• If ∆(G(cid:48), G(cid:48)(cid:48)) = 1,

∆(G(cid:48), G(cid:48)(cid:48)) = 1 =⇒

((cid:80)n
i=1 wi)2
|V (cid:48)||V (cid:48)(cid:48)|

= 0

opt| = |G(cid:48)(cid:48)

opt| = |Copt| = 0

=⇒ w(v(cid:48), v(cid:48)(cid:48)) = 0 ∀v(cid:48) ∈ V (cid:48), ∀v(cid:48)(cid:48) ∈ V (cid:48)
=⇒ |G(cid:48)
=⇒ no clique C could be found in the association graph G(V, E)
=⇒ |G| = 0, G contains no vertices
=⇒ G(cid:48), G(cid:48)(cid:48) do not have any nodes in common

(15)

• If ∆(G(cid:48), G(cid:48)(cid:48)) = 0,

∆(G(cid:48), G(cid:48)(cid:48)) = 0 =⇒ (

n
(cid:88)

wi)2 = |V (cid:48)||V (cid:48)(cid:48)|

n
(cid:88)

but (

wi) ≤ |V (cid:48)

opt| ≤ |V (cid:48)| =⇒ (

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

but (

wi) ≤ |V (cid:48)(cid:48)

opt| ≤ |V (cid:48)(cid:48)| =⇒ (

i=1
n
(cid:88)

wi) = |V (cid:48)

opt| = |V (cid:48)|

wi) = |V (cid:48)(cid:48)

opt| = |V (cid:48)(cid:48)|

(16)

i, v(cid:48)(cid:48)

i=1
=⇒ wi = 1 ∀(v(cid:48)
=⇒ |V (cid:48)| = |V (cid:48)
=⇒ G(cid:48) = G(cid:48)
=⇒ the two DAGs denote the same program

i , wi) ∈ Copt
opt| = |Copt| = |V (cid:48)(cid:48)
opt = G(cid:48)(cid:48)
opt (cid:39) G(cid:48)(cid:48)

opt| = |V (cid:48)(cid:48)|

