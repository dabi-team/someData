1
2
0
2

b
e
F
1
1

]

G
L
.
s
c
[

1
v
8
2
2
6
0
.
2
0
1
2
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings 1–24

Learning Gaussian-Bernoulli RBMs using Diﬀerence of
Convex Functions Optimization

Vidyadhar Upadhya
Electrical Engineering Dept, Indian Institute of Science, Bangalore

vidyadhar@ee.iisc.ernet.in

P. S. Sastry
Electrical Engineering. Dept, Indian Institute of Science, Bangalore

sastry@ee.iisc.ernet.in

Abstract
The Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM) is a useful generative
model that captures meaningful features from the given n-dimensional continuous data.
The diﬃculties associated with learning GB-RBM are reported extensively in earlier studies.
They indicate that the training of the GB-RBM using the current standard algorithms,
namely, contrastive divergence (CD) and persistent contrastive divergence (PCD), needs
a carefully chosen small learning rate to avoid divergence which, in turn, results in slow
learning.
In this work, we alleviate such diﬃculties by showing that the negative log-
likelihood for a GB-RBM can be expressed as a diﬀerence of convex functions if we keep
the variance of the conditional distribution of visible units (given hidden unit states) and the
biases of the visible units, constant. Using this, we propose a stochastic diﬀerence of convex
functions (DC) programming (S-DCP) algorithm for learning the GB-RBM. We present
extensive empirical studies on several benchmark datasets to validate the performance of
this S-DCP algorithm. It is seen that S-DCP is better than the CD and PCD algorithms
in terms of speed of learning and the quality of the generative model learnt.
Keywords: RBM, Gaussian-Bernoulli RBM, Contrastive Divergence, Diﬀerence of Convex
Programming

1. Introduction

Probabilistic generative models are good for extracting meaningful representations from the
data by learning high-dimensional multi-modal distributions in an unsupervised manner.
The restricted Boltzmann machine (RBM) is one such generative model which was originally
proposed for modeling distributions of binary data variables (called visible units) with the
help of binary latent variables (called hidden units) (Smolensky, 1987; Freund and Haussler,
1992; Hinton, 2002). The RBM is found to be eﬀective for unsupervised representation
learning in several applications and has been used as a building block for models such
as deep belief networks, deep Boltzmann machines (Hinton et al., 2006; Salakhutdinov
and Hinton, 2009; Goh et al., 2014; Wang et al., 2019; Chen et al., 2018b) and also in
combination with the Convolutional Neural Networks (Zhang et al., 2018). It can also be
used as a discriminative model (Hinton et al., 2006; Zhang et al., 2019).

The RBM (like the Boltzmann machine) uses a suitable energy function to represent
the probability distribution as a Gibbs distribution. The distinguishing feature of an RBM
(compared to the Boltzmann machine) is that the hidden units are conditionally indepen-
dent, conditioned on all the visible units and vice versa. These conditional distributions are

© V. Upadhya & P.S. Sastry.

 
 
 
 
 
 
Upadhya Sastry

enough to deﬁne an energy function and hence, the joint distribution. The normalizing con-
stant for the distribution, known as the partition function, is computationally intractable.
However, one can eﬃciently sample from the distribution through Markov Chain Mote Carlo
(MCMC) methods.

In the original RBM, both visible and hidden units are assumed to be binary. Hence, the
model is also called Bernoulli-Bernoulli RBM (BB-RBM). It is to be noted that the visible
units, being observable, constitute the data variables. To model continuous valued data,
the binary visible units must be replaced with continuous ones though the hidden units can
still remain binary. One possibility is to model the distribution of visible units conditioned
on the hidden units as a Gaussian. The resulting model is called Gaussian-Bernoulli RBM
(GB-RBM) (Welling et al., 2005; Hinton and Salakhutdinov, 2006). In this work, we address
the important problem of ﬁnding eﬃcient algorithms for learning GB-RBMs.

The usual approach to learning an RBM is by minimizing the KL divergence between
the data and the model distributions which is equivalent to ﬁnding the maximum likelihood
estimate of the parameters of the model. The maximum likelihood estimates have no
closed form solution and the parameters are learnt using iterative algorithms to minimize
the negative log likelihood. The log-likelihood gradient requires calculating an expectation
w.r.t. the model distribution which is computationally expensive due to the presence of
the intractable partition function. Therefore, this expectation is estimated using samples
obtained from the model distribution using MCMC methods. This strategy is followed in the
popular Contrastive Divergence (CD) algorithm and its variant, the Persistent Contrastive
Divergence (PCD); the only algorithms currently used for learning GB-RBMs. However, the
estimated gradient may have a large variance due to limitations in the MCMC procedure
and can cause these stochastic gradient descent based algorithms to diverge sometimes (Cho
et al., 2011). The issue of divergence is more prominent in the case of GB-RBM compared
to BB-RBM (Wang et al., 2014).

It was shown in Lee et al. (2007) that learning GB-RBM with a sparse penalty provides
meaningful representations for natural images. Although it was empirically observed that
simple mixture models can outperform the GB-RBM models in terms of the likelihood esti-
mation (Theis et al., 2011), with the re-parameterization of energy function and improved
learning algorithm in Cho et al. (2011) the GB-RBM model was found to extract more
meaningful representations. However, learning GB-RBM is a challenging task and diﬃcul-
ties associated with learning are reported extensively in the literature (Krizhevsky, 2009;
Theis et al., 2011; Cho et al., 2011; Wang et al., 2014; Melchior et al., 2017). For instance,
the analysis in Wang et al. (2014) showed that the failures in learning GB-RBMs are due
to the ineﬃciency of training algorithms rather than any inherent limitations of the model
itself. Several training recipes which make use of some knowledge of the data distribution
were also suggested. They also showed that the features learnt on benchmark datasets
are comparable to those learnt using sparse coding and Independent Component Analysis
(ICA). This claim is theoretically justiﬁed in Karakida et al. (2016) where they showed
that the GB-RBM, learnt using an exact maximum likelihood or an approximate maximum
likelihood with CD estimates, extracts independent components at one of their stable ﬁxed
points. In addition, the diﬃculties to capture the geometric structure of the data manifold
by GB-RBMs was overcome using approaches where the local manifold structure of the

2

Learning GB-RBM with DC

data is captured through graph regularization (GraphRBM) (Chen et al., 2018a) and using
the guidance from the available label information (pcGRBM) (Chu et al., 2019).

Currently, the CD and PCD algorithms are the standard algorithms for learning GB-
RBM models. They require a carefully chosen learning rate (which is usually small) so as
to avoid divergence of the log-likelihood. The small value of the learning rate results in slow
convergence. In addition, one also needs careful parameter initialization and restriction of
the gradient during the update, for avoiding the divergence of the log-likelihood. Thus,
there is a need for an eﬃcient algorithm for learning GB-RBMs.

In this work, we show that the log-likelihood function of a GB-RBM can be expressed as
a diﬀerence of two convex functions under the assumption that the conditional distribution
of visible units is Gaussian with unit variance. There are eﬃcient algorithms for optimizing
diﬀerence of convex functions and this strategy has been used earlier for learning BB-RBMs
(Carlson et al., 2015; Nitanda and Suzuki, 2017; Upadhya and Sastry, 2017). We adopt
the stochastic diﬀerence of convex functions programming (S-DCP) (Upadhya and Sastry,
2017) algorithm to learn GB-RBMs. Through extensive empirical studies using benchmark
datasets, we show that the S-DCP algorithm is more eﬃcient compared to the existing CD
and PCD algorithms for learning GB-RBMs. The main contributions of this work are as
follows.

We show that if the conditional distribution of the visible units is Gaussian with ﬁxed
variance, then the log likelihood function for GB-RBMs can be expressed as a diﬀerence
of two convex functions. We then show how we can employ a two loop stochastic gradient
descent algorithm (called S-DCP) to learn the GB-RBM and demonstrate through empirical
studies that this algorithm is more eﬃcient than CD and PCD. Though we ﬁx the variance
of the conditional distribution of the visible units in our algorithm, we still eﬃciently learn
good representations. Further, we modify the S-DCP algorithm to accommodate variance
update and show that the resulting method is also more eﬃcient than CD and PCD. We
show that the proposed algorithm is better in terms of the speed of learning, achieved
log-likelihood, ability to extract meaningful features and to generate high quality samples.
The rest of the paper is organized as follows. In section 2, we brieﬂy describe the RBM
model and the maximum likelihood approach to learn a GB-RBM. We show that the GB-
RBM log-likelihood is a diﬀerence of convex functions and describe the DC programming
approach in section 3. In section 4, we describe the experimental setup and present the
results. We conclude the paper in section 5.

2. Background

The RBM is a two layer recurrent neural network, in which m visible stochastic units
(denoted as v) in one layer are connected to n hidden stochastic units (denoted as h) in the
other (Rumelhart and McClelland, 1987; Freund and Haussler, 1992; Hinton, 2002). There
are no connections among the units within each layer and the connections between the two
layers are undirected. The weight of the connection between the ith visible unit and the
jth hidden unit is denoted as wij. The bias for the ith visible unit and the jth hidden unit
are denoted as bi and cj, respectively. Let θ =
denote the
parameters of the model. RBM represents probability distribution over the states of its

Rn×m, b

Rm, c

Rn

w

∈

∈

∈

}

{

3

Upadhya Sastry

units as,

θ) = e−E(v,h;θ)/Z(θ),
p(v, h
|

(1)

where Z(θ) is the normalizing constant, called the partition function and E(v, h; θ) is the
energy function. The bipartite connectivity structure in RBM implies conditional indepen-
dence of visible units conditioned on all hidden units and vice-versa, i.e.,

p(v

h, θ) =
|

m
(cid:89)

i=1

p(vi

h, θ),
|

p(h

v, θ) =
|

n
(cid:89)

i=1

v, θ).

p(hi

|

2.1. Bernoulli-Bernoulli Restricted Boltzmann Machines

The RBM was originally proposed for binary visible and hidden units (v
h

n). In the BB-RBM the conditional distributions are prescribed as,
}

∈ {

0, 1

0, 1
}

∈ {

m and

p(vi = 1
|

h, θ) = sigmoid

bi +

p(hj = 1
|

v, θ) = sigmoid

cj +

(cid:32)





wijhj



(cid:33)

wijvi

n
(cid:88)

j=1

m
(cid:88)

i=1

where, sigmoid(x) = 1/(1 + e−x) is the logistic sigmoid function. Using these, we obtain
the following energy function.

E(v, h; θ) =

(cid:88)

−

i,j

wijvi hj

m
(cid:88)

i=1

−

bi vi

−

n
(cid:88)

j=1

cj hj.

(2)

The data distribution represented by an RBM is that of the visible units which is obtained
m can
by marginalizing from eq.
be approximated by a BB-RBM with (m visible units and) suﬃcient number of hidden
units (Mont´ufar and Rauh, 2017; Gu et al., 2019).

(1). One can show that any distribution over

0, 1
}
{

2.2. Gaussian-Bernoulli Restricted Boltzmann Machines

The BB-RBM can represent a data distribution only if the data variables are binary. The
natural extension of the BB-RBM is to choose continuous visible units and binary hidden
n. In this case, the conditional distribution of the units
units, i.e., v
}
∈ R
are prescribed as,

m and h

∈ {

0, 1



h, θ) =

p(vi

|

vi

|

N

bi +

(cid:32)



wij hj, σ2
i



(cid:88)

j

p(hj = 1
|

v, θ) = sigmoid

cj +

(cid:33)

(cid:88)

wij

i

vi
σ2
i

(3)

µ, σ2) denotes Gaussian probability density function with mean µ and variance
where,
σ2 Cho et al. (2011). This RBM model is called the Gaussian-Bernoulli RBM (GB-RBM).

(.
|

N

4

Learning GB-RBM with DC

The choice of the following energy function results in the desired conditional distributions

as in eq. (3) ,

E(v, h

θ) =
|

(cid:88)

(vi

bi)2

(cid:88)

−
2σ2
i

−

wijvi hj
σ2
i

−

(cid:88)

cj hj

i,j
is the variance associated with the ith Gaussian visible unit (Cho et al., 2011).
i, we obtain (from eq. (1)) the partition function

j

i

where, σ2
i
Using this energy function with σ2
Z(θ) as,

i = 1,

∀

Z(θ) =

(cid:90)

(cid:88)

Rm

h

(vi−bi)2
2

− (cid:80)
e
i

+(cid:80)
i,j

wij vi hj +(cid:80)
j

cj hj

dv.

2.3. Maximum Likelihood Learning

One can learn the GB-RBM parameters to represent the data distribution, given some
training data, by maximizing the log-likelihood. For simplicity of notation, let us consider
a single training sample (v). Then log-likelihood is given by,

(θ

L

v) = log p(v
|

θ) = log
|

(cid:88)

h

p(v, h

θ)
|

(cid:44) (g(θ, v)

f (θ))

−

where,

g(θ, v) = log

(cid:88)

h

e−E(v,h:θ)

f (θ) = log Z(θ) = log

(cid:90)

(cid:88)

Rm

h

e−E(v,h;θ)dv.

The required RBM parameters can be found as

θ∗ = arg max

θ L

(θ

|

v) = arg max

θ

(g(θ, v)

f (θ)).

−

Since there is no closed form solution, iterative gradient ascent procedure is used as,

It is easily shown that the gradient of g and f are given by,

θt+1 = θt + η [

∇

θ g(θ, v)

θ f (θ)]θ=θt .

− ∇

∇

θ g(θ, v) =
θ f (θ) =

∇

Ep(h|v;θ) [
Ep(v,h;θ) [

−

−

∇

∇

θ E(v, h; θ)]
θ E(v, h; θ)] .

(4)

(5)

(6)

Here, Eq denotes the expectation w.r.t. the distribution q. Since

have,

∂E(v, h; θ)
∂wij

=

vihj, we

−

∂g(θ, v)
∂wij

=

(cid:88)

h

vi hj p(h
|

v, θ)
v, θ) = vi p(hj = 1
|

(cid:32)

= vi σ

cj +

(cid:33)

wijvi

.

m
(cid:88)

i=1

5

Upadhya Sastry

Similar to this case, the partial derivatives of g(θ, v) w.r.t. bi and cj yield expressions that
are analytically easy to evaluate.

In contrast, it is computationally very expensive to calculate

tation in eq. (6). For example,

θ f (θ) due to the expec-

∇

∂f (θ)
∂wij

=

=

(cid:90)

Rm

(cid:90)

Rm

(cid:88)

h

vi hj p(v, h

θ) dv
|

vi p(hj = 1
|

v, θ) p(v

θ) dv
|

θ f .

∇

which is computationally hard. Hence, MCMC sampling methods are used to estimate the
expectation to obtain

Due to the bipartite connectivity structure of the model, MCMC sampling can be ef-
ﬁciently implemented through block Gibbs sampler. This is followed in the contrastive
divergence (CD) algorithm (Hinton, 2002) which is currently the most popular algorithm
for learning GB-RBMs. In the CD(k) algorithm, the Markov chain is run for k block Gibbs
sampling iterations and the kth state is used for a single-sample approximation of the expec-
tation. The training samples are used to initialize the Markov chain for every mini-batch.
In a variant of this algorithm, called persistent contrastive divergence (PCD) (Tieleman,
2008), the Markov chain is initialized with the kth state of the previous iteration’s Markov
chain.

Currently, CD and PCD are the standard algorithms used to train GB-RBMs. However,
these algorithms are sensitive to hyperparameters and require a carefully chosen learning
rate. Generally, the learning rate needs to be very small to avoid divergence and hence,
these algorithms learn very slowly. This issue is more prominent in GB-RBMs for reasons
described in Hinton (2010); Cho et al. (2011). As explained earlier, our objective is to train
the GB-RBMs eﬃciently using algorithms that optimize the diﬀerence of convex functions.

3. Diﬀerence of Convex Functions Formulation

In this section, we show that the f and g functions given by eq. (5) are convex w.r.t. w and
c under the assumption of ﬁxed variance, σ2
i . This implies that the log-likelihood function
(considered as function of w, c), given by eq. (4), is a diﬀerence of two convex functions.
This property can be exploited to design eﬃcient optimization algorithms through the
Diﬀerence of Convex (DC) programming approach. Here we choose σ2
i, for the sake
∀
of convenience. It is easy to see that the result holds for any ﬁxed value of σ2
i .

i = 1,

3.1. Convexity of g(θ, v)

The function g(θ, v) is given by,

g(θ, v) = log

(cid:88)

e

h

− (cid:80)
i

(vi−bi)2
2

+(cid:80)
j

cj hj +(cid:80)
i,j

wij vi hj

.

(7)

6

Learning GB-RBM with DC

If the m
then we can express eq. (7) as a function of w and c as,

n matrix, w, is stacked to form a vector of dimension mn, denoted as vec(w),

×

g(θ, v) = log

2n
(cid:88)

k=1

βk eaT

k vec(w) eh(k)T

c

(vi−bi)2
2

i

where, βk = e− (cid:80)
, ak = vec(v(h(k ))T ) and h(k), k = 1, . . . , 2n are all the n-bit
binary vectors or, equivalently, all possible values of the binary hidden vector h (Note that
v(h(k))T is a matrix of dimension m
n and vec(v(h(k ))T ) is a vectorized version of it).
The following Lemma now proves that g is convex w.r.t. w and c.

×

Lemma 1 A function of the following form is convex:

lse a(u) = log

N
(cid:88)

k =1

βk e aT
k u

where, a1, . . . , aN are some ﬁxed vectors in Rd and βk
lse a maps Rd to R).

0,

≥

k. (Note that the function

∀

The proof of the above Lemma is given in appendix B.

Hence we can conclude that g is convex w.r.t. w, c. It is to be noted that the function

g is not convex w.r.t. b.

3.2. Convexity of f (θ)

As shown in appendix A, we can write f (θ) as,

f (θ) = Kf + log

(cid:80)
j

e

(cid:88)

h

cj hj +(cid:80)
i

(cid:32)

(cid:80)
j

(cid:80)
j

(cid:33)


wij hj



2

+bi



wij hj

.

This can also be expressed as,

f (θ) = Kf + log

2n
(cid:88)

k=1

βkes(k)

(8)

(cid:80)

where, s(k)= 1
2
ponent of the binary vector hk. It is to be noted that βk
denoted as wi∗. Then,

j bi and βk=e

i,j wij hk

j wij hk

j )2+ (cid:80)

i((cid:80)

(cid:80)

j cj hk

j with hk
0,

j being the jth com-
k. Let the ith row of w be
∀

≥

(cid:16)(cid:88)
j

(cid:17)2

wij hk
j

= (cid:0)wT

i∗ hK(cid:1)2

= wT

i∗ hK(hK)T wi∗.

Using the above and by treating vec(w) as an mn-dimensional vector where rows of w are
stacked one below the other (as done earlier), the eq. (8) can be rewritten in the following
form,

f (θ) = Kf + log

(cid:88)

ecT h(k)

e

1
2 vec(w)T Ak vec(w)+aT

k vec(w)

(9)

k

7

Upadhya Sastry

Here, Ak is an mn
mn block diagonal matrix in which each block is a rank one matrix
of the form hk(hk)T and ak = vec(b(hk )T ) is a vector of dimension mn. The form of Ak
ensures that it is a positive semi-deﬁnite matrix. The following Lemma now proves f is
convex w.r.t. w, c.

×

Lemma 2 A function of the form

lseq(u) = log

(cid:88)

βie

1
2 uT Aiu+aT

i u

i

is convex if the matrix Ai is positive semi-deﬁnite and βi

0,

i.

∀

≥

The proof of the above Lemma is given in appendix C.1

Having proved that both f and g are convex w.r.t. w and c, we can now conclude, using
eq. (4), that the log-likelihood function of the GB-RBM is a diﬀerence of convex functions
w.r.t. weights, w, and bias of hidden units, c.

3.3. Diﬀerence of Convex Functions Optimization

The diﬀerence of convex programming (DCP) (Yuille et al., 2002; An and Tao, 2005) is a
method used for solving optimization problems of the form,

θ∗ = arg min

θ

F (θ) = arg min

θ

(f (θ)

−

g(θ)) ,

where, both f and g are convex functions of θ. In the RBM setting, F corresponds to the
negative log-likelihood and the functions f and g correspond to those deﬁned in eq. (5).
While both f and g are convex and smooth, F is non-convex.

The DCP is an iterative procedure deﬁned by,

θ(t+1) = arg min

θ

(cid:16)

f (θ)

θT

∇

−

g(θ(t))

(cid:17)

.

(10)

Note that the optimization problem on the RHS of eq. (10) is convex and hence, the DCP
consists of solving a convex optimization problem in each iteration. It can be shown, using
convexity of f and g, that the iterates deﬁned by eq. (10) constitute a descent method for
minimizing F (Yuille et al., 2002; An and Tao, 2005). One can also show that the iterates
would constitute a descent on F as long as a descent on the objective function in (the RHS
of) eq. (10) is ensured at each iteration even if this optimization problem is not solved
exactly. Such an approach, called the stochastic-DC programming (S-DCP) algorithm, was
proposed earlier for learning BB-RBMs (Upadhya and Sastry, 2017).

In the S-DCP algorithm, the convex optimization problem described in eq.(10) is solved
(approximately) using a ﬁxed d number of iterations of stochastic gradient descent (w.r.t.
f is estimated using samples obtained though
θ) on f (θ)
MCMC, as in the case of CD (the estimate is denoted as ˆf (cid:48)(θ, v)). The S-DCP algorithm
is described in detail via Algorithm 1.

g(θ(t), v). For this, the

θT

∇

∇

−

1. We call the function in Lemma 2 as a log-sum-exponential-quadratic function and hence, the notation
lseq. While the convexity of lse functions given by (12) in appendix B is known, we do not know of any
discussion or reported proofs of convexity of the lseq functions considered here.

8

Learning GB-RBM with DC

Algorithm 1 S-DCP update for a single training sample v (Upadhya and Sastry, 2017).
f is denoted as ˆf (cid:48)(θ, v).
The estimate of the
, η, d, K(cid:48)
Input: v, θ(t) =
}
Initialize ˜θ(0) = θ(t), ˜v(0) = v
for l = 0 to d

∇
w(t), c(t)
{

1 do

−
for k = 0 to K(cid:48)
sample h(k)
i ∼
sample ˜v(k+1)
j

end for
˜θ(l+1) = ˜θ(l)
˜v(0) = ˜v(K(cid:48))

η

−

1 do

−
p(hi

˜v(k), ˜θ(l)),

i
∀
h(k), ˜θ(l)),

|
p(vj

∼
|
(cid:104) ˆf (cid:48)(˜θ(l), ˜v(K(cid:48)))

j
∀
(cid:105)
g(θ(t), v)

− ∇

end for
Output: θ(t+1) = ˜θ(d)

If the computational cost of one Gibbs transition is T and that of evaluating gradient of
g (or f ) is L, the computational cost of the CD-K algorithm for a mini-batch of size NB is
NB(KT +2L) whereas the S-DCP algorithm has cost d NB(K(cid:48)T +L)+NBL when K(cid:48) MCMC
steps and d inner loop iterations are chosen (Upadhya and Sastry, 2017). It may be noted
that, by choosing the S-DCP hyperparameters d and K(cid:48) such that KT = dK(cid:48)T + (d
1)L,
the amount of computations required is approximately same as that for CD algorithm
with K steps (Upadhya and Sastry, 2017). If we assume that (d
1)L is small, then the
computational cost of both CD and S-DCP algorithms can be made approximately equal
by choosing dK(cid:48) = K. However, S-DCP updates the parameter (θ) d times per mini-
batch whereas CD updates the parameter only once per mini-batch. Therefore, S-DCP
computational cost is slightly more compared to CD (refer table 2 for the comparison of
the exact computational time for diﬀerent algorithms).

−

−

In sections 3.1 and 3.2 we showed that the functions f and g are convex only with
respect to the parameters w and c. Hence, the S-DCP cannot be directly used for learning
the biases of the visible units, b. We could update b outside the S-DCP loop using the
same update as in CD. Instead, we ﬁx this parameter to be the mean of the data directly
without training, as suggested in Melchior et al. (2017). Our experimental results show that
even with a ﬁxed b, we could still learn good models.

∀

i = 1,

With regard to the variance parameter, the choice of σ2

i, indicates that we learn
a Gaussian with unit variance as the conditional distribution of visible units. Therefore,
we suitably normalize the input data to ensure unit variance and compare the performance
of the S-DCP algorithm with that of the CD and PCD algorithms. We also explore the
performance of these algorithms when the variance parameter not ﬁxed, but is learnt by the
model. In this case, only w and c are updated in the inner loop of the S-DCP algorithm and
the variance parameter is updated only once per mini-batch outside the S-DCP loop. We
use the re-parameterization of the variance parameter as in Cho et al. (2011) to constrain
them to stay positive during the update. A detailed description of this variant of the S-DCP
algorithm is given in Algorithm 2.

9

Upadhya Sastry

Algorithm 2 S-DCP update for a single training sample v when learning variance

Input: v, θ(t) =
, σ(t), η, d, K(cid:48)
}
Initialize ˜θ(0) = θ(t), ˜v(0) = v
for l = 0 to d

w(t), c(t)
{

1 do

−
for k = 0 to K(cid:48)
sample h(k)
i ∼
sample ˜v(k+1)
j

end for
˜θ(l+1) = ˜θ(l)
˜v(0) = ˜v(K(cid:48))

η

−

1 do

−
p(hi

˜v(k), ˜θ(l)),

i
∀
h(k), ˜θ(l)),

|
p(vj

∼
|
(cid:104) ˆf (cid:48)(˜θ(l), ˜v(K(cid:48)))

j
∀
(cid:105)
g(θ(t), v)

− ∇

end for
Update σ using re-parameterization as in Cho et al. (2011).
Output: θ(t+1) = ˜θ(d), σ(t+1)

4. Experiments and Discussion

4.1. Datasets

In order to analyse the performance of the diﬀerent algorithms, we consider four benchmark
grey scale image datasets namely Natural images (van Hateren J H and van der Schaaf A,
1998), Olivetti faces (Samaria and Harter, 1994), MNIST (LeCun et al., 2012) and Fashion-
MNIST (FMNIST)(Xiao et al., 2017). The details of each of these datasets, the data
dimension (m), number of training samples (Ntr) and number of test samples (Nte) along
with the other hyperparameters of algorithms (to be explained in Section 4.2) are provided
in Table 1.

Table 1: The Details of the Datasets and the Hyperparameters

Dataset
Nat. Images
MNIST
FMNIST

m Ntr Nte
196 40K 30K 196 200 100 12 3
784 60K 10K 200 300 200 24 4
784 60K 10K 200 300 200 24 4
500 3000 10 12 3

n NE NB K d K(cid:48)
4
6
6
4

Olivetti Faces 4096 400

−

Similar to Wang et al. (2014) the Natural images dataset used is a subset of the Van
Hateren’s Natural image database (van Hateren J H and van der Schaaf A, 1998). These
images are whitened using Zero-phase Component Analysis (ZCA). As mentioned in section
2, the features learnt using GB-RBM on this dataset are comparable to those learnt using
sparse coding and Independent Component Analysis (ICA) (Wang et al., 2014).

In addition to the Natural image dataset, we also consider the Olivetti faces dataset
which was used to study deep Boltzmann machines (Cho et al., 2013). There are ten
diﬀerent images for each of the 40 subjects in this dataset. They are whitened using ZCA.
All the images are used to train the GB-RBM model primarily because the number of data
samples are less in this dataset. Training using all the images also alleviates the sensitivity

10

Learning GB-RBM with DC

issues of the RBM (RBMs are sensitive to translation in images) which could be caused by
the mismatch in the alignment of training and testing images.

The MNIST dataset contains handwritten digits, whereas the FMNIST contains images

of fashion products. All these images are normalized to have mean 0 and variance 1.

4.2. Experimental setup

For each dataset, the hyperparameters such as the number of hidden nodes (n), mini-batch
size (NB) etc are chosen to be the same across all the algorithms, as provided in Table 1.
The learning rate (η) is chosen separately for each algorithm as the maximum possible value
(such that, increasing it further will cause divergence of the log-likelihood). The number of
hidden nodes (denoted by n) in the RBM models for diﬀerent datasets is shown in Table 1.
To study the speed of learning, we train the GB-RBM model for a ﬁxed number of epochs
(denoted as NE in Table 1). In each epoch, the mini-batch learning procedure is employed
and the training data is shuﬄed after every epoch. In order to get an unbiased comparison,
we did not use momentum or weight decay in any of the algorithms.

We evaluate the algorithms using the performance measures obtained from multiple
trials, where each trial ﬁxes the initial conﬁguration of the weights and biases as follows. The
a, a]
weights are initialized to samples drawn from a uniform distribution with support [
where, a = 6/√m + n. The bias of the hidden units is initialized to cj =
+
log τj, as proposed in Wang et al. (2014) where, τj is ﬁxed to 0.01. The bias of the visible
units (b) is not learnt (as explained in the section 3.3) but ﬁxed to the mean of the training
samples.

(cid:107)b−W∗j (cid:107)2−(cid:107)b(cid:107)2
2

−

−

We compare the performance of S-DCP with CD and PCD keeping the computational
complexity of S-DCP roughly the same as that of CD/PCD by choosing K, d and K(cid:48) such
that K = dK(cid:48) (Upadhya and Sastry, 2017). We have experimentally observed that a large
K generally results in a sensible generative model, also supported by the ﬁndings from
Wang et al. (2014); Carlson et al. (2015). Therefore, we choose K = 24 in CD and PCD
(with d = 4, K(cid:48) = 6 for S-DCP) for learning MNIST and FMNIST datasets and we choose
K = 12 in CD and PCD (with d = 3, K(cid:48) = 4 for S-DCP) for the other two datasets, as
shown in Table 1.

The algorithms are implemented using Python and CUDAMat (a CUDA-based matrix
class for Python bindings) (Mnih, 2009) on a system with Intel processor i7
7700 (4 CPU
cores and processor base frequency 3.60 GHz), NVIDIA Titan X Pascal GPU and a 16 GB
RAM conﬁguration.

−

4.3. Evaluation Criterion

We compare the performance of diﬀerent algorithms based on the following criteria: speed
of learning, quality and generalization capacity of the learnt model, quality of the learnt
ﬁlters and generative ability of the learnt model.

We use the achieved log-likelihood on the train and the test samples to evaluate the
quality of learnt models (Hinton, 2010). While the achieved average train log-likelihood
provides a reasonable measure to compare the quality of the learnt model, its evolution
(over epochs) indicates the speed of learning. The average train log-likelihood (denoted as

11

Upadhya Sastry

ATLL) is evaluated as,

AT LL =

1
N

N
(cid:88)

i=1

log p(v(i)

θ)

train|

(11)

where, v(i)
train denotes the ith training sample. The average test log-likelihood (denoted as
ATeLL), on the other hand, provides a good indication of the generalization capacity of
the model. Similar to ATLL, we evaluate the average test log-likelihood by using the test
samples rather than the training samples. The computation of ATLL and ATeLL require
the estimate of the partition function. It is estimated using annealed importance sampling
(Neal, 2001) with 100 particles and 10000 intermediate distributions according to a linear
temperature scale between 0 and 1. In the following section, we show the evolution (over
epochs) of mean and maximum (over all trials) of the average train and test log-likelihood,
for all the algorithms considered, for both the cases of ﬁxed variance and learnt variance.

The learning procedure is eﬀective when the weights learnt by the model are able to
capture the important/crucial features of the dataset. Therefore, we interpret the model
weights as ﬁlters and examine the ﬁlters learnt. The evaluation in terms of the generative
ability of the learnt models is carried out by observing the samples that these models
generate. For this, we randomly initialize the states of the visible units (in the learnt
model) and run the alternating Gibbs Sampler for 200 steps and plot the state of the visible
units.

4.4. Performance Comparison

4.4.1. Speed of Learning

As mentioned in section 4.2 all the three algorithms have comparable computational load
(per mini-batch) if we select K = d K(cid:48). However, the computations performed by the
diﬀerent algorithms are not identical (S-DCP updates the parameter d times per mini-
batch whereas CD updates the parameter only once per mini-batch). Therefore, we present
the actual computational time of diﬀerent algorithms. Table 2 provides the mean and the
standard deviation (σt) of the utilized time (in seconds), computed over 20 trials, for a ﬁxed
100 epochs of learning by each of the algorithms for the diﬀerent datasets considered. As
seen from the table, the computation time per epoch is roughly the same for all algorithms.

Table 2: The system time statistics (The mean and standard deviation of the system time

in seconds) for diﬀerent datasets

Algorithm Natural Images Olivetti Faces MNIST/FMNIST
Mean
7.96
8.03
9.19

σt Mean
77.94
0.03
79.05
0.03
88.64
0.02

Mean
40.42
41.74
43.87

CD
PCD
S-DCP

σt
0.19
0.15
0.11

σt
0.29
0.26
0.23

To compare the speed of learning as well as the quality of the learnt models, we plot
the progress of the normalized mean average train log-likelihood over epochs for diﬀerent

12

Learning GB-RBM with DC

Table 3: The number of epochs and system time statistics (in seconds) for achieving the
reference normalized mean average log-likelihood by diﬀerent algorithms for each
datasets.

Dataset

Nat. Images
MNIST
FMNIST
Olivetti Faces

Ref. LL
(%)
95
95
95
80

CD

PCD

S-DCP

Time Epoch
Time Epoch
Epoch
125
125
60
52 .17
50 .52
100
160 126 .48
247 192 .51
183 142 .63
79
94 .86
120
1750 140 .52
2400 191 .04

Time
29 .39
88 .64
70 .02
1500 137 .85

algorithms in Fig. 1. The log-likelihood is normalized using min-max normalization to
restrict the range between 0 and 1, and to easily compare the learning speeds. For each
dataset we take the maximum and minimum log-likelihoods among all algorithms to do
this normalization. From Fig. 1, we observe that the log-likelihood achieved at the end of
training is diﬀerent for each algorithm. The average train log-likelihood achieved by the
S-DCP algorithm is superior to those achieved by the CD and PCD algorithms across all
datasets.

In order to better illustrate the eﬃciency of the S-DCP algorithm, we choose a reference
log-likelihood (denoted as Ref. LL), as shown in Fig. 1. For the Natural image, MNIST and
FMNIST datasets the Ref. LL is chosen as 95% (i.e., 95% of the highest ATLL achieved).
For the Olivetti faces dataset we choose the Ref. LL as the maximum log-likelihood achieved
by the CD algorithm (since CD attains the least log-likelihood among all the algorithms)
which is approximately 80% of the maximum. Table 3 provides the Ref. LL along with
the number of epochs and the computational time required to achieve this for the diﬀerent
algorithms.

Both, Table 3 and Fig. 1, demonstrate that the S-DCP algorithm is much faster com-
pared to other algorithms both in terms of the number of epochs and the actual computa-
tional time. For instance, the speed of learning on the Natural image dataset, indicated in
the Fig. 1 by the epoch at which the model achieves 95% of the highest mean ATLL, shows
that CD and PCD algorithms require around 60 more epochs of training compared to the
S-DCP algorithm which translates to approximately twice the required computation time.
On MNIST and FMNIST datasets, S-DCP is approximately twice as fast as CD and 1.4
times as fast as the PCD. On Olivetti faces dataset, the CD algorithm achieves only 80%
of the ﬁnal log-likelihood achieved by the S-DCP algorithm and S-DCP achieves this value
approximately 900 and 250 epochs earlier than CD and PCD, respectively. In terms of the
computational time (to learn the Olivetti faces dataset) S-DCP takes about 25% less time
compared to CD though the times taken by PCD and S-DCP are about the same.

Thus, we ﬁnd that S-DCP has a higher speed of learning compared to the other al-
gorithms. We also observed a similar behaviour when the normalized maximum ATLL is
considered instead of normalized mean ATLL.

4.4.2. Quality and Generalization capacity

As mentioned earlier, the quality of the trained model could be captured by the average train
log-likelihood (ATLL) and the generalization capacity could be inferred from the average

13

Upadhya Sastry

Figure 1: The normalized mean average train log-likelihood achieved with diﬀerent algo-
rithms on all the datasets with ﬁxed variance. For S-DCP, Algorithm 1 is used.

test log-likelihood (ATeLL). We now show these results in both the cases of ﬁxed variance
and learnt variance.
Fixed Variance: The evolution of the train and the test log-likelihoods shown in Fig.
2, for each dataset, demonstrates that the S-DCP algorithm performs consistently better
compared to the other two algorithms across all the datasets.
Learning Variance: The evolution of the train and the test log-likelihoods, over epochs,
when the variance parameter is also learnt using the Algorithm 2, is shown in Fig. 3 for
diﬀerent datasets. We observe that in this case also S-DCP outperforms both the CD and
PCD algorithms.

The higher train and test log-likelihood (across datasets) achieved by the S-DCP com-
pared to those achieved using CD and PCD indicates the better quality and generalization
capacity of the models learnt using the S-DCP algorithm.

In Fig. 4, we plot the typical evolution of the variance as learning progresses for Natural
images and MNIST dataset using CD and S-DCP algorithm (We have observed that PCD
learns similar models as that of CD in terms of learnt variance and hence to avoid clutter
PCD plots are omitted). We selected two visible units, and plot the mean (over 20 trials)
of the variances of these two units as learning evolves. We also show the variance over 20
trials of this evolution as a shaded region around the plot of the mean. These two units
seem to be typical of a large number of visible units. All units start with unit variance and
most units either settle to a variance close to 1 or much lower.

From the Fig. 4, we observe that both CD and S-DCP converge to approximately similar
variance values though S-DCP converges faster. We have observed similar behaviour with
the other two datasets.

14

4060801001201401601802000.750.800.850.900.951.00S-DCPPCDCDNaturalImagesS-DCPPCDCD751001251501752002252502753000.900.920.940.960.981.00S-DCPPCDCDMNIST501001502002503000.8250.8500.8750.9000.9250.9500.9751.000S-DCPPCDCDFMNIST1000125015001750200022502500275030000.20.30.40.50.60.70.80.91.0S-DCPPCDCDOlivettiFacesEpochsNormalizedMeanAvg.TrainLLLearning GB-RBM with DC

Figure 2: The performance of diﬀerent algorithms on each dataset with ﬁxed variance. For
S-DCP, Algorithm 1 is used.
(Note that, unlike in Fig. 1, here we show the
evolution of log likelihood starting from the ﬁrst epoch; consequently here the
range of y-axis is more and hence the scale on y-axis is compressed).

Figure 3: The performance of diﬀerent algorithms on each dataset while also learning vari-
ance. For S-DCP, Algorithm 2 is used. It may be noted that the average train
log-likelihood for the Olivetti faces dataset evolves to reach positive value since
the learnt variance is small and contributes high negative value to the log partition
function.

15

−278−277−276−275−274−273−272−271−270MeanAvg.TrainLLMax.AvgTrainLL255075100125150175200Epochs−277.5−277.0−276.5−276.0−275.5−275.0MeanAvg.TestLL255075100125150175200EpochsMax.Avg.TestLLS-DCPPCDCD−940−930−920−910−900−890MeanAvg.TrainLLMax.Avg.TrainLL50100150200250300Epochs−940−930−920−910−900−890−880MeanAvg.TestLL50100150200250300EpochsMax.Avg.TestLLS-DCPPCDCD50100150200250300−880−870−860−850−840−830MeanAvgTrainLL50100150200250300Max.AvgTrainLL50100150200250300Epochs−880−870−860−850−840−830MeanAvgTestLL50100150200250300EpochsMax.AvgTestLLS-DCPPCDCD050010001500200025003000Epochs−3960−3950−3940−3930−3920−3910−3900−3890MeanAvg.TrainLL050010001500200025003000EpochsMax.AvgTrainLLS-DCPPCDCD−278−276−274−272−270−268MeanAvg.TrainLLMax.Avg.TrainLL255075100125150175200Epochs−278−277−276−275−274−273−272MeanAvg.TestLL255075100125150175200EpochsMax.Avg.TestLLS-DCPPCDCD−1050−1000−950−900−850−800−750−700MeanAvg.TrainLLMax.Avg.TrainLL050100150200250300Epochs−1050−1000−950−900−850−800−750MeanAvg.TestLL050100150200250300EpochsMax.Avg.TestLLS-DCPPCDCD−950−900−850−800−750MeanAvgTrainLLMax.AvgTrainLL50100150200250300Epochs−950−900−850−800−750MeanAvgTestLL50100150200250300EpochsMax.AvgTestLLS-DCPPCDCD050010001500200025003000Epochs−4000−3000−2000−10000MeanAvg.TrainLL050010001500200025003000EpochsMax.Avg.TrainLLS-DCPPCDCDUpadhya Sastry

Figure 4: The evolution of the variance of two speciﬁc visible units as learning progresses by
CD and S-DCP algorithms on Natural images and MNIST dataset. For S-DCP,
Algorithm 2 is used.

4.4.3. Quality of the learnt filters

We next look at the ﬁlters learnt by diﬀerent algorithms. In the following discussion, we
present the results only for the ﬁxed variance case. The same conclusions hold true even in
the case of learning the variance.

A few sample weights from the model learnt (at the end of epochs at which the Ref. LL
is achieved) are plotted as ﬁlters in Fig. 5. To do so, the set of weights connected to each
hidden unit (of dimension m) is reshaped to the original image size and plotted as grey
scale images.

As observed from the Fig. 4.4.3 and 4.4.3, the weights are able to capture the relevant
information from the training data. Speciﬁcally, we observe that the ﬁlters capture the edge
related information in the Natural Images dataset (4.4.3) and extract the facial features from
the Olivetti faces dataset (4.4.3). Thus, we ﬁnd that S-DCP learns ﬁlters comparable to
those learnt by CD and PCD; but it learns them faster.

Models tend to learn point ﬁlters in the initial stages of training and slowly converge to
edge-like ﬁlters when mean zero and unit variance normalization is used instead of ZCA. This
phenomenon was observed in Krizhevsky (2009) on natural images. The same phenomenon
is observed here in Fig. 4.4.3 and Fig. 4.4.3, for the MNIST and FMNIST datasets. From
the ﬁgures we ﬁnd that most of the ﬁlters have only been able to capture the point-like
information from the data. However, we observe that S-DCP results in more ﬁlters that
capture edge-like information compared to the CD and PCD algorithms. This indicates that
the transition from point ﬁlters to edge ﬁlters happens earlier with the S-DCP algorithm.

4.4.4. Generative ability

The samples from the models learnt (at the end of epochs at which the Ref. LL is achieved)
using diﬀerent algorithms are given in Fig. 6. From the ﬁgure we observe that the model
learnt using S-DCP generates samples that are sharper and less noisy compared to those
generated by the models learnt using the CD and PCD algorithms.

16

050100150200Epochs0.40.50.60.70.80.91.01.1Varianceσ2180σ2120CDS-DCP050100150200250300Epochs0.40.50.60.70.80.91.01.1Varianceσ280σ2120CDS-DCPLearning GB-RBM with DC

Figure 5: A few sample weights (obtained at the end of epochs at which the Ref. LL is
achieved) or ﬁlters learnt using diﬀerent algorithms on each dataset with ﬁxed
variance (For S-DCP, Algorithm 1 is used) are plotted as grey-scale images.

17

S-DCPCDPCDS-DCPCDPCDS-DCPCDPCDS-DCPCDPCDUpadhya Sastry

Figure 6: The samples from the models learnt (at the end of epochs at which the Ref. LL
is achieved) using diﬀerent algorithms on each dataset with ﬁxed variance. For
S-DCP, Algorithm 1 is used.

18

S-DCPCDPCDS-DCPCDPCDS-DCPCDPCDLearning GB-RBM with DC

5. Conclusions

While GB-RBMs are good models for unsupervised representation learning, training them
successfully is a challenging task. The current standard algorithms namely, CD and PCD are
very sensitive to the hyperparameters and hence, are not very eﬃcient to train GB-RBMs.
In this work, we showed that the negative log-likelihood of a GB-RBM is a diﬀerence of
convex functions with respect to the weights, wij, and the biases of hidden units, cj (under
the assumption that the variances of visible units are ﬁxed) and this is utilized to employ
the diﬀerence of convex functions programming approach to propose an eﬃcient algorithm,
called S-DCP. We also proposed a variant of S-DCP where the learning of this variance is
also facilitated. Through extensive empirical results on multiple datasets, we showed that
S-DCP results in a considerable improvement in the speed of learning and the quality of the
ﬁnal generative models learnt compared to those of CD and PCD. The results presented in
this study support the claim that the S-DCP algorithm is currently a very eﬃcient method
to learn GB-RBMs. In the S-DCP algorithm, the noisy gradient estimated through MCMC
sampling is used for optimizing a convex function and this may be the reason why the
optimization method is more eﬀective compared to CD and PCD. Further work is needed
in terms of theoretical analysis of such stochastic gradient descent on convex functions to
better understand the optimization dynamics of S-DCP and to possibly further improve
it. Another direction in which the work presented here can be extended is in terms of
exploring the utility of second order methods based on estimated Hessian in the inner loop
of the S-DCP which solves a convex optimization problem.

Appendix A. The function f

Consider the partition function,

Z(θ) =

(cid:90)

(cid:88)

v

h

e− (cid:80)

i

(vi−bi)2
2

+(cid:80)

i,j wij vi hj +(cid:80)

j cj hj dv

(cid:88)

(cid:80)
e

=

j cj hj +(cid:80)

i((cid:80)

j wij hj)

(cid:18) (cid:80)

j wij hj
2

(cid:19)

+bi

×

h
(cid:90)

v

e− (cid:80)

i

(vi−((cid:80)

j wij hj +bi))2

2

dv

j cj hj +(cid:80)

i((cid:80)

j wij hj)

(cid:18) (cid:80)

j wij hj
2

+bi

(cid:19)

.

= (2π)

(cid:80)

(cid:88)

m
2

e

h

Therefore, f (θ) = log Z(θ) is given by,

f (θ) = Kf + log

(cid:80)
j

e

(cid:88)

h

(cid:32)

cj hj +(cid:80)
i

(cid:80)
j

wij hj

(cid:80)
j

(cid:33)


wij hj



2

+bi



where, Kf = m

2 log(2π).

19

Upadhya Sastry

Appendix B. Convexity of log-sum-exponential function

We ﬁrst prove that lse function (Rd

→

R) given by,

lse(u) = log

βi eui = log βT eu

(cid:88)

i

(12)

is convex w.r.t. u. Here, ui denotes ith element of the vector u and eu denotes element-wise
exponential of u. The gradient and Hessian of lse are given as,

lse(u) =

∇

2lse(u) =

∇

eu
β
(cid:12)
βT eu
βT eudiag(β

(cid:12)

=

eu)

diag(β

(cid:12)
βT eu

eu)
(β
−
(βT eu)2
(cid:18) β
eu
(cid:12)
βT eu

−

eu)(β

eu)T

(cid:12)

(cid:12)

(cid:19)T

(cid:19) (cid:18) β

eu
(cid:12)
βT eu

where,
(cid:12)
can be written as,

denotes element-wise multiplication. Let us denote, a = β

eu. Now, the Hessian

(cid:12)

2lse(u) =

∇

diag(a)

1T a −

aaT
(1T a)2

where, 1 is a vector of all 1’s. Now we have,

2lse(u)x = xT

xT

∇

(cid:18) 1T a diag(a)
(1T a)2

−

aaT

(cid:19)

x

(cid:80) ak

=

(cid:80) akx2

((cid:80) akxk)2

k −
((cid:80) ak)2

0.

≥

Hence, the lse function given in eq. (12) is convex. Now, consider the function of the form
(given in Lemma 1),

lse a(u) = log

N
(cid:88)

i=1

βi e aT
i u

(13)

d matrix whose rows are

where, a1, . . . , aN are some ﬁxed vectors in Rd. Let A be an N
ai, then lse a(s) = lse(As). Now, for any two vectors s1, s2

×

Rd we have,

lse a(αs1 + (1

−

∈
α)s2 ) = lse(A(αs1 + (1
α lse(As1 ) + (1
≤
= α lse a(s1 ) + (1

α)s2 ))
α) lse(As2 )
α) lse a(s2 ).

−

−

−

Hence, the lse a function is convex which proves Lemma 1.

Appendix C. Convexity of log-sum-exponential-quadratic function

Consider the log-sum-exponential-quadratic function,

lseq(u) = log

βie

1
2 uT Aiu+aT

i u.

(cid:88)

i

20

Learning GB-RBM with DC

The gradient of the above function is,

lseq(u) =

∇

βie

1
2 uT Aiu+aT

i u(Aiu + ai)

(cid:80)
i

1
2 uT Aiu+aT

i u

(cid:80)
i

βie

.

Let γ(i) = βie

1
2 uT Aiu+aT

i u and ¯γ(i) = γ(i)

(cid:80)

i(cid:48) γ(i(cid:48)) . The Hessian of lse q (u) is given as,

2lseq(u) =

(cid:88)

¯γ(i)Ai +

∇

¯γ(i)(Aiu + ai)(Aiu + ai)T

(cid:88)

i

i

(cid:88)

−

i

¯γ(i)(Aiu + ai)

(cid:88)

i

¯γ(i)(Aiu + ai)T .

Now, consider xT

2lseq(u)x,

∇

xT

∇

2lseq(u)x =

(cid:88)

i

¯γ(i)xT Aix +

(cid:88)

i

¯γ(i)α2

i −

(cid:32)

(cid:88)

i

(cid:33)2

¯γ(i)αi

where, αi = xT (Aiu + ai). By noting that (cid:80)
i

¯γ(i)α2

i −

((cid:80)
i

¯γ(i)αi)2

≥

0, we can write

xT

∇

2lseq(u)x

(cid:88)

≥

i

¯γ(i)xT Aix.

If Ai,
quadratic functions of the considered form are convex.

i is positive semi-deﬁnite then xT
∀

2lseq(u)x

∇

≥

0. Therefore, the log-sum-exponential-

Acknowledgment

The authors thank the NVIDIA Corporation for the donation of the Titan X Pascal GPU
used in this research.

References

Le Thi Hoai An and Pham Dinh Tao. The DC (diﬀerence of convex functions) programming
and DCA revisited with DC models of real world nonconvex optimization problems.
Annals of Operations Research, 133(1):23–46, 2005. ISSN 1572-9338. .

David Carlson, Volkan Cevher, and Lawrence Carin. Stochastic spectral descent for re-
stricted Boltzmann machines. In Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics, pages 111–119, 2015.

D. Chen, J. Lv, and Z. Yi. Graph regularized restricted Boltzmann machine. IEEE Trans-
actions on Neural Networks and Learning Systems, 29(6):2651–2659, June 2018a. ISSN
2162-2388. .

21

Upadhya Sastry

X. Chen, J. Weng, W. Lu, J. Xu, and J. Weng. Deep manifold learning combined with con-
volutional neural networks for action recognition. IEEE Transactions on Neural Networks
and Learning Systems, 29(9):3938–3952, 2018b.

K. H. Cho, T. Raiko, and A. Ilin. Gaussian-Bernoulli deep Boltzmann machine. In The
2013 International Joint Conference on Neural Networks (IJCNN), pages 1–7, Aug 2013.
.

KyungHyun Cho, Alexander Ilin, and Tapani Raiko.

Improved Learning of Gaussian-
Bernoulli Restricted Boltzmann Machines. In Artiﬁcial Neural Networks and Machine
Learning – ICANN 2011, pages 10–17. Springer Berlin Heidelberg, 2011. ISBN 978-3-
642-21735-7.

J. Chu, H. Wang, H. Meng, P. Jin, and T. Li. Restricted Boltzmann machines with Gaussian
visible units guided by pairwise constraints. IEEE Transactions on Cybernetics, 49(12):
4321–4334, Dec 2019.

Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors
using two layer networks. In Advances in Neural Information Processing Systems 4, pages
912–919. Morgan-Kaufmann, 1992.

H. Goh, N. Thome, M. Cord, and J. Lim. Learning deep hierarchical visual feature coding.
IEEE Transactions on Neural Networks and Learning Systems, 25(12):2212–2225, 2014.

L. Gu, J. Huang, and L. Yang. On the representational power of restricted Boltzmann
machines for symmetric functions and boolean functions. IEEE Transactions on Neural
Networks and Learning Systems, 30(5):1335–1347, May 2019. ISSN 2162-2388.

G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural

networks. Science, 313(5786):504–507, 2006. ISSN 0036-8075. .

Geoﬀrey Hinton. A practical guide to training restricted Boltzmann machines. Momentum,

9(1):926, 2010.

Geoﬀrey E Hinton. Training products of experts by minimizing contrastive divergence.

Neural computation, 14(8):1771–1800, 2002.

Geoﬀrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep

belief nets. Neural Computation, 18:1527–1554, 2006.

Ryo Karakida, Masato Okada, and Shun ichi Amari. Dynamical analysis of contrastive
divergence learning: Restricted Boltzmann machines with Gaussian visible units. Neural
Networks, 79:78 – 87, 2016. ISSN 0893-6080. .

Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis,

Computer Science Department, University of Toronto, 2009.

Yann A. LeCun, L´eon Bottou, Genevieve B. Orr, and Klaus-Robert M¨uller. Eﬃcient Back-

Prop. Neural Networks: Tricks of the Trade: Second Edition, pages 9–48, 2012.

22

Learning GB-RBM with DC

Honglak Lee, Chaitanya Ekanadham, and Andrew Y. Ng. Sparse deep belief net model for
visual area V2. In Proceedings of the 20th International Conference on Neural Information
Processing Systems, pages 873–880, Red Hook, NY, USA, 2007. ISBN 9781605603520.

Jan Melchior, Wang Nan, and Wiskott Laurenz. Gaussian-binary restricted Boltzmann
machines for modeling natural image statistics. PLOS ONE, 12(2):1–24, Feb 2017. .

Volodymyr Mnih. Cudamat: a CUDA-based matrix class for Python. 2009.

Guido Mont´ufar and Johannes Rauh. Hierarchical models as marginals of hierarchical
models. International Journal of Approximate Reasoning, 88:531–546, 2017. ISSN 0888-
613X. .

Radford M Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139,

2001.

Atsushi Nitanda and Taiji Suzuki. Stochastic Diﬀerence of Convex Algorithm and its Ap-
plication to Training Deep Boltzmann Machines. In Proceedings of the 20th International
Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine
Learning Research, pages 470–478, 20–22 Apr 2017.

D. E. Rumelhart and J. L. McClelland.

Information processing in dynamical systems:
Foundations of harmony theory. Parallel Distributed Processing: Explorations in the
Microstructure of Cognition: Foundations, pages 194–281, 1987.

Ruslan Salakhutdinov and Geoﬀrey E Hinton. Deep Boltzmann machines. In AISTATS,

volume 1, page 3, 2009.

F. S. Samaria and A. C. Harter. Parameterisation of a stochastic model for human face iden-
tiﬁcation. In Proceedings of 1994 IEEE Workshop on Applications of Computer Vision,
pages 138–142, Dec 1994. .

P. Smolensky. Information processing in dynamical systems: Foundations of harmony the-
ory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing:
Explorations in the Microstructure of Cognition: Foundations, pages 194–281, 1987.

Lucas Theis, Sebastian Gerwinn, Fabian Sinz, and Matthias Bethge. In all likelihood, deep
belief is not enough. Journal of Machine Learning Research, 12(94):3071–3096, 2011.

Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the like-
lihood gradient. In Proceedings of the 25th International Conference on Machine Learn-
ing, pages 1064–1071. Association for Computing Machinery, 2008. ISBN 9781605582054.
.

Vidyadhar Upadhya and P. S. Sastry. Learning RBM with a DC programming approach.
In Proceedings of the Ninth Asian Conference on Machine Learning, volume 77 of Pro-
ceedings of Machine Learning Research, pages 498–513. PMLR, 15–17 Nov 2017.

van Hateren J H and van der Schaaf A. Independent component ﬁlters of natural images
compared with simple cells in primary visual cortex. Proceedings of the Royal Society B:
Biological Sciences, 265(1394):359–366, Mar 1998. ISSN 0962-8452 1471-2954.

23

Upadhya Sastry

G. Wang, J. Qiao, J. Bi, Q. Jia, and M. Zhou. An adaptive deep belief network with sparse
restricted Boltzmann machines. IEEE Transactions on Neural Networks and Learning
Systems, pages 1–12, 2019.

Nan Wang, Jan Melchior, and Laurenz Wiskott. Gaussian-binary restricted boltzmann

machines on modeling natural image statistics. CoRR, abs/1401.5900, 2014.

Max Welling, Michal Rosen-zvi, and Geoﬀrey E Hinton. Exponential Family Harmoni-
ums with an Application to Information Retrieval. In Advances in Neural Information
Processing Systems 17, pages 1481–1488. MIT Press, 2005.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for

benchmarking machine learning algorithms, 2017.

Alan L Yuille, Anand Rangarajan, and AL Yuille. The concave-convex procedure (CCCP).

Advances in neural information processing systems, 2:1033–1040, 2002.

Ji Zhang, Hongjun Wang, Jielei Chu, Shudong Huang, Tianrui Li, and Qigang Zhao. Im-
proved Gaussian-Bernoulli restricted Boltzmann machine for learning discriminative rep-
resentations. Knowledge-Based Systems, 185:104911, 2019. ISSN 0950-7051.

Jian Zhang, Shifei Ding, and Nan Zhang. An overview on probability undirected graphs
and their applications in image processing. Neurocomputing, 321:156 – 168, 2018. ISSN
0925-2312.

24

