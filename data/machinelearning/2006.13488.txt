0
2
0
2

n
u
J

4
2

]

G
L
.
s
c
[

1
v
8
8
4
3
1
.
6
0
0
2
:
v
i
X
r
a

1

Distributionally-Robust Machine Learning
Using Locally Differentially-Private Data

Farhad Farokhi, Senior Member, IEEE

Abstract—We consider machine learning, particularly regression, using locally-differentially private datasets. The Wasserstein
distance is used to deﬁne an ambiguity set centered at the empirical distribution of the dataset corrupted by local differential privacy
noise. The ambiguity set is shown to contain the probability distribution of unperturbed, clean data. The radius of the ambiguity set is a
function of the privacy budget, spread of the data, and the size of the problem. Hence, machine learning with locally-differentially
private datasets can be rewritten as a distributionally-robust optimization. For general distributions, the distributionally-robust
optimization problem can relaxed as a regularized machine learning problem with the Lipschitz constant of the machine learning model
as a regularizer. For linear and logistic regression, this regularizer is the dual norm of the model parameters. For Gaussian data, the
distributionally-robust optimization problem can be solved exactly to ﬁnd an optimal regularizer. This approach results in an entirely
new regularizer for training linear regression models. Training with this novel regularizer can be posed as a semi-deﬁnite program.
Finally, the performance of the proposed distributionally-robust machine learning training is demonstrated on practical datasets.

Index Terms—Local differential privacy, Machine learning, Distributionally-robust optimization, Regularization, Wasserstein distance.
(cid:70)

1 INTRODUCTION

A DVANCES in artiﬁcial intelligence, particularly machine

learning, have opened new possibilities for data an-
alytic to address important societal challenges. However,
these achievements can be stiﬂed by privacy concerns. Local
differential privacy, a variant of the popular differential
privacy framework [1], [2], has been touted as an approach
for providing privacy guarantees in the presence of an
untrusted aggregator or data analysts [3]–[5]. This is be-
cause, with local differential privacy, the data can be freely
aggregated and shared. Even, commercial entities, such as
Microsoft and Apple, have started using local differen-
tial privacy to deploy privacy-preserving data aggregation
mechanisms [6]–[8].

The additive noise in local differential privacy can de-
grade the performance of machine learning models trained
on perturbed privacy-preserving datasets. Several studies
have looked into providing bounds for the performance
degradation caused by local differential privacy noise as a
function of the privacy budget and the dataset size [5], [9]–
[12]. These studies however do not use recent advances in
distributionally-robust optimization and machine learning
(see, e.g., [13]–[15]) to compute robust machine learning
models with out-of-sample performance guarantees in the
presence of local differential privacy noise. They are more
concerned by ﬁnding the effect of privacy-preserving noise
on established machine learning algorithms.

Distributionally-robust optimization considers uncertain
stochastic programs [16] with the ambiguity set for the
distribution modeled by discrete distributions [17], moment
constraints [18], Kullback-Leibler divergence [19], and the
Wasserstein distance [13]. Distributionally-robust optimiza-

tion has shown signiﬁcant promises in adversarial machine
learning [20], [21] or machine learning with outlier data [22].
However, so far, this approach has not been used for
training robust machine learning models based on datasets
perturbed using local differential privacy.

In this paper, we use the Wasserstein distance to deﬁne
an ambiguity set centered at the empirical distribution of
the training dataset that is corrupted with local differential
privacy noise. This ambiguity set is shown to contain the
probability distribution of unperturbed data. The radius of
the ambiguity set is a function of the privacy budget, spread
of the data, and the size of the problem (i.e., number of
inputs and outputs of the machine learning model). Armed
with this description of the ambiguity set, we can cast
the problem of learning with locally-differentially private
data as a distributionally-robust optimization problem. We
show that, for general distributions, an upper bound for
the worst-case expected loss in the distributionally-robust
optimization problem is the empirical sampled-averaged
loss plus the Lipschitz-constant of the loss function. Using
this, we can relax the distributionally-robust optimization
problem as a regularized machine learning problem with
the Lipschitz constant as a regularizer. For linear and lo-
gistic regression models, this regularizer is equal to the
dual norm of the model parameters. For Gaussian data, the
distributionally-robust optimization problem can be solved
exactly to ﬁnd an optimal regularizer for the problem. This
approach results in an entirely new regularizer for linear
regression. We ﬁnally demonstrate the performance of the
proposed distributionally-robust optimization problems on
practical datasets.

•

F. Farokhi is with the Department of Electrical and Electronic Engineering
at the University of Melbourne, Australia.
E-mail: farhad.farokhi@unimelb.edu.au

Manuscript received June 25, 2020.

2 OVERVIEW OF THE WASSERSTEIN DISTANCE

In this section, a brief overview of the Wasserstein distance
is provided. The set of probability distributions Q over

 
 
 
 
 
 
the set Ξ ⊆ Rm such that EQ{(cid:107)ξ(cid:107)} < ∞ is denoted by
M(Ξ). For all P1, P2 ∈ M(Ξ), the Wasserstein distance
W1 : M(Ξ) × M(Ξ) → R≥0 := {x ∈ R|x ≥ 0} is

Wq(P1, P2) := inf
Π

(cid:40) (cid:20)(cid:90)

Ξ2

(cid:107)ξ1 − ξ2(cid:107)qΠ(dξ1, dξ2)

(cid:21)1/q

:

Π is a joint disribution on ξ1 and ξ2

with marginals P1 and P2, respectively

(cid:41)
.

The Wasserstein distance is symmetric, i.e., Wq(P1, P2) =
Wq(P2, P1). The Wasserstein distance satisﬁes the triangle
i.e., Wq(P1, P3) ≤ Wq(P1, P2) +
inequality [23, p. 170],
Wq(P2, P3). Also, the Wasserstein distance is convex [24,
Lemma 2.1], i.e., Wq(αP1 + (1 − α)P2, Q) ≤ αWq(P1, Q) +
(1 − α)Wq(P2, Q) for all α ∈ [0, 1]. For q = 1, the duality
theorem of Kantorovich and Rubinstein [25] implies that
(cid:41)
,

f (ξ)P1(dξ) −

f (ξ)P2(dξ)

(cid:40)(cid:90)

(cid:90)

W1(P1, P2) := sup
f ∈L

Ξ

Ξ

where L denotes the set of all Lipschitz functions with
Lipschitz constant upper bounded by one, i.e., all functions
f such that |f (ξ1) − f (ξ2)| ≤ (cid:107)ξ1 − ξ2(cid:107) for all ξ1, ξ2 ∈ Ξ.

3 DISTRIBUTIONALLY-ROBUST MACHINE LEARN-
ING WITH PRIVATE DATA
3.1 Expected Risk Minimization

We consider supervised learning based on a training dataset
i=1, where xi ∈ X ⊆ Rpx is the input or feature
{(xi, yi)}n
vector (e.g., pixels of an image or features extracted from
it) and yi ∈ Y ⊆ Rpy is the output or label (e.g., image
content). The training dataset is composed of independently
and identically distributed (i.i.d.) samples from probability
distribution P.

Training machine learning models refers to extracting a
model M : Rpx × Rpθ → Rpy (sometimes referred to as
hypothesis) to describe the relationship between inputs and
outputs distributed according to P. This can be done by
solving the stochastic program

J ∗ := min
θ∈Θ

EP{(cid:96)(M(x; θ), y)},

(1)

where θ is the machine learning model parameter, Θ ⊆ Rpθ
is the set of feasible parameters, and (cid:96) : Rpy × Rpy → R
is the loss function. An example of a loss function is
(cid:96)(M(x; θ), y) = (M(x; θ) − y)(cid:62)(M(x; θ) − y). The existence
of a minimizer in (1), and the subsequent approximations in
this paper, is guaranteed if the loss function is continuous
and the feasible set is compact. This problem is sometimes
referred to as expected risk minimization.

In the absence of the knowledge of the distribution P,
the training dataset {(xi, yi)}n
i=1, i.e., samples from this
distributions, can be used to solve the sample-averaged
approximation problem

ˆJ := min
θ∈Θ

1
n

n
(cid:88)

i=1

(cid:96)(M(xi; θ), yi).

(2)

The approximation in (2) is often the starting point of
machine learning. This is because (2) can be a good proxy

2

for (1), when n is large enough, in the sense of probably
approximately correct (PAC) learnability [26]. We make the
following standing assumption regarding the distribution of
the training data.
Assumption 1. EP{exp((cid:107)ξ(cid:107)a)} < ∞ for some a > 1.

This assumption implies that P is a light-tailed distri-
bution. All probability distributions with compact support
set are light-tailed; however, unbounded noises, such as
Gaussian or Laplace, are also light-tailed. This is often an
implicit assumption in the machine learning literature as,
for heavy-tailed distributions, the sample average of the
loss in (2) may not even generally converge to the expected
loss in (1) and hence the PAC learnability might not even
hold [27], [28].

3.2 Local Differential Privacy

Due to privacy concerns, the training dataset is sometimes
replaced with a noisy dataset {(˜xi, ˜yi)}n

i=1 in which

˜xi :=xi + wi,
˜yi :=yi,

(3a)

(3b)

where (wi)n
i=1 are identically and independently distributed
(i.i.d.) according to the probability distribution W. Local dif-
ferential privacy is a useful and versatile notion of privacy.

Deﬁnition 1 (Local Differential Privacy). The reporting
mechanism with additive noise in (3) is ((cid:15), δ)-locally
differentially private if, for all (x, y), (x(cid:48), y(cid:48)) ∈ X × Y
and any Lebesgue-measurable set A ⊆ X × Y,

P{(˜xi, ˜yi) ∈A|xi = x, yi = y}

≤ exp((cid:15))P{(˜xi, ˜yi) ∈ A|xi = x(cid:48), yi = y(cid:48)}.

Assumption 2. X ⊆ [x, x]px .

The box constraint nature of Assumption 2 is not strictly-
speaking necessary; however, it simpliﬁes the closed-form
expression of the results. The results can be readily extended
to any compact sets by using diameter of the sets. It is
widely known that we can ensure local differential privacy
with Laplace and Gaussian additive noises. This is explored
in the next theorem.

Theorem 1. The following statements hold:

1)

2)

For (cid:15) > 0, the mechanism in (3) is (cid:15)-locally differ-
entially private if wi is a vector of zero-mean i.i.d.
Laplace noise with scale ∆/(cid:15), where ∆ := (x−x)px;
For (cid:15), δ > 0, the mechanism in (3) is ((cid:15), δ)-locally
differentially private if wi is a vector of zero-mean
i.i.d. Gaussian noise with standard deviation σ :=
(cid:112)2 log(1.25/δ)∆/(cid:15).

Proof: The proof for the Laplace mechanism follows
from [2, Theorem 3.6] while noting that the (cid:96)1-sensitivity
of the query (which is equal to the identity function for
local differential privacy) is given by ∆. The proof for the
Gaussian noise follows from [2, Theorem A.1]. Note that the
the (cid:96)1-sensitivity is an upper bound for the (cid:96)2-sensitivity.

(5)

continuous in (x, y) for a ﬁxed θ ∈ Θ. Then,

3.3 Distributionally-Robust Machine Learning

The privacy-preserving records in the training dataset
(˜xi, ˜yi)n
i=1 are independently and identically distributed
(i.i.d.) according to D, which can be characterized by the
convolution of P and W. We can deﬁne the empirical proba-
bility distribution

(cid:98)Dn :=

1
n

n
(cid:88)

i=1

d(˜xi,˜yi),

where dξ is the Dirac distribution function. Following the
deﬁnition of the Dirac distribution function, we have

1
n

n
(cid:88)

i=1

(cid:96)(M(xi; θ), yi) = E(cid:98)Dn {(cid:96)(M(x; θ), y)},

(4)

and, as a result, we can rewrite (2) as

ˆJ := min
θ∈Θ

E(cid:98)Dn {(cid:96)(M(x; θ), y)}.

We can show that the empirical probability distribution (cid:98)Dn
is in a vicinity of the original probability distribution P with
a high probability.
Theorem 2. Assume that W is the distribution in Theorem 1.

There exist constants c1, c2 > 0 such that
(cid:27)

√

(cid:26)

Dn

W1((cid:98)Dn, P) ≤ ζ(γ) +

2p∆
(cid:15)

≥ 1 − γ,

for the Laplace mechanism and

(cid:40)

Dn

W1((cid:98)Dn, P) ≤ ζ(γ) +

(cid:112)2 log(1.25/δ)p∆
(cid:15)

(cid:41)

≥ 1 − γ,

for the Gaussian mechanism, where
(cid:18) log(c1/γ)
c2n
(cid:18) log(c1/γ)
c2n

(cid:19)1/ max{p,2}





ζ(γ) :=

(cid:19)1/a

,

, n ≥

n <

log(c1/γ)
c2
log(c1/γ)
c2

,

,

for all n ≥ 1, p = px + py (cid:54)= 2, and γ > 0.

Proof: Note that, since P is light-tailed, D is also light-
tailed if we use the privacy-preserving noises in Theorem 1.
Following [13], we know that Dn{W1((cid:98)Dn, D) ≤ ζ(γ)} ≤
1−γ. Using [29, Lemma 8.6], we get W1(D, P) ≤ W1(P, P)+
W1(d0, W) = W1(d0, W) ≤ EW{(cid:107)w(cid:107)} ≤ (cid:112)EW{(cid:107)w(cid:107)2},
where the last inequality follows from the Jensen’s in-
equality [30, p. 27]. Furthermore, for the Laplace noise, we
get EW{(cid:107)w(cid:107)2} = trace(EW{ww(cid:62)}) = 2p∆2/(cid:15)2, and, as
a result, W1(D, P) ≤
2p∆/(cid:15). Therefore, W1((cid:98)Dn, P) ≤
2p∆/(cid:15) if W1((cid:98)Dn, D) ≤ ζ(γ), which implies that
ζ(γ) +
Dn{W1((cid:98)Dn, P) ≤ ζ(γ) +
2pd/(cid:15)} ≥ 1 − γ. The proof
for the Gaussian noise is the same with the exception that
EW{(cid:107)w(cid:107)2} = trace(EW{ww(cid:62)}) = 2p log(1.25/δ)∆2/(cid:15)2.

√

√

√

Hence, if we select ρ large enough, the original distribu-
tion P would belong to the ambiguity set {G : W1(G, (cid:98)Dn) ≤
ρ}. This observation motivates training the machine learn-
ing model by solving the distributionally-robust optimiza-
tion problem in

ˆJn := min
θ∈Θ

sup
G:W1(G,(cid:98)Dn)≤ρ

EG{(cid:96)(M(x; θ), y)},

(6)

3

for some constant ρ > 0. The correct value of ρ is discussed
in the next theorem.
Theorem 3. Assume that W is the distribution in Theorem 1.
2p∆/(cid:15) for the Laplace mechanism or
If ρ = ζ(β) +
if ρ = ζ(β) + (cid:112)2p log(1.25/δ)∆/(cid:15) for the Gaussian
mechanism, then

√

Dn{J ∗ ≤ EP{(cid:96)(M(x; ˆθn), y)} ≤ ˆJn} ≥ 1 − β,

(7)

where β ∈ (0, 1) is a signiﬁcance parameter and the
trained model parameter ˆθn ∈ Θ is the minimizer of (6).

The optimization problem in (6) involves taking a supre-
mum over the probability density function. This is an
inﬁnite-dimensional optimization problem and is hence
computationally difﬁcult to solve. Therefore, we relax this
problem in the remainder of this section.
Proposition 1. Assume that (cid:96)(M(x; θ), y) is L(θ)-Lipschitz

sup
G:W1(G,(cid:98)Dn)≤ρ

EG{(cid:96)(M(x; θ), y)}

≤ E(cid:98)Dn{(cid:96)(M(x; θ), y)} + L(θ)ρ.

Proof: The proof follows from the duality theorem of

Kantorovich and Rubinstein [25].

Now, we can deﬁne the regularized sample-averaged

optimization problem in

˜Jn := min
θ∈Θ

(cid:104)

(cid:105)
E(cid:98)Dn{(cid:96)(M(x; θ), y)} + ρL(θ)

.

(8)

We can still prove a performance guarantee for the optimizer
of (8). This is done in the next theorem.
Theorem 4. Assume that W is the distribution in Theorem 1.
If ρ = ζ(β) +
2p∆/(cid:15) for the Laplace mechanism or
if ρ = ζ(β) + (cid:112)2p log(1.25/δ)∆/(cid:15) for the Gaussian
mechanism, then

√

Dn{J ∗ ≤ EP{(cid:96)(M(x; ˜θn), y)} ≤ ˜Jn} ≥ 1 − β,
(9)
where the trained model parameter ˜θn ∈ Θ is the
minimizer of (8).

Proof: The proof is similar to [13, Theorem 3.4]
with an extra step with the aid of Proposition 1. By
selecting ρ as in the statement of the theorem, P be-
longs to a ball around (cid:98)Dn with radius ρ with proba-
bility greater than or equal to 1 − β according to The-
orem 2. Therefore, with probability of at least 1 − β,
EG{(cid:96)(M(x; ˜θn), y)}.
EP{(cid:96)(M(x; ˜θn), y)} ≤ supG:W1(G,(cid:98)Dn)≤ρ
EG{(cid:96)(M(x; ˆθn), y)} ≤
Proposition 1 states supG:W1(G,(cid:98)Dn)≤ρ
E(cid:98)Dn {(cid:96)(M(x; ˜θ), y)} + ρL(˜θ) = ˜Jn. Therefore, with probabil-
ity of at least 1 − β, EP{(cid:96)(M(x; ˜θn), y)} ≤ ˜Jn.

Theorem 4 shows that by regularizing the sample-
averaged cost function, we can train a machine learning
model that performs better in the presence of locally dif-
ferentially private noises. This is an interesting observation
demonstrating the value of regularization in privacy ma-
chine learning with private data.
for large n,
Remark 1 (Large Datasets).
ζ(n) ≈ 0. Therefore,
following Theorem 4, ρ =
O(px(px + py)1/2/(cid:15)) for the Laplace mechanism and
ρ = O(px(px + py)1/2 log(1/δ)1/2/(cid:15)) for the Gaussian
mechanism. This implies that the regularization weight

In the limit

ρ should increase when reducing the privacy budget (cid:15).
Also, higher-dimensional problems, i.e., when px or py
is larger, requires larger regularization weights ρ.

Remark 2 (Linear and Logistic Regression). Without loss of
generality, consider py = 1. If py > 1, each output can be
treated independently. In this case, M(x; θ) = θ(cid:62)[x(cid:62) 1](cid:62)
and (cid:96)(M(x; θ), y) = (M(x; θ) − y)2/2. We also assume
that (x, y) belong to compact set X × Y ⊆ Rpx × R.
Following [31], we know that L(θ) = (X + 1 +
Y )(cid:107)θ(cid:107)2
∗, where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107), and
X = maxx∈X (cid:107)x(cid:107) and Y = maxy∈Y |y|. An alterna-
tive to the quadratic loss function is (cid:96)(M(x; θ), y) =
|M(x; θ) − y|. Again, following [31], L(θ) = (cid:107)θ(cid:107)∗. For
the logistic regression, M(x; θ) = 1/(1 + exp(−[x(cid:62) 1]θ))
and (cid:96)(M(x; θ), y) = −y log(M(x; θ)) − (1 − y) log(1 −
M(x; θ)). Following [31], L(θ) = (Y + X + 2)(cid:107)θ(cid:107)∗.

4 LINEAR REGRESSION WITH GAUSSIAN DATA
In this section, we consider the speciﬁc case that (xi, yi)n
i=1
is Gaussian distributed with mean µ and covariance Σ.
Furthermore, we assume that M(x; θ) = Ax + B with A, B
modeling the machine learning model parameters (instead
of θ) and (cid:96)(M(x; A, B), y) = (cid:107)M(x; A, B) − y(cid:107)2. Therefore,
by using the Gaussian mechanism in Theorem 1, (˜xi, ˜yi)n
i=1
is also Gaussian distributed. Note that Assumption 2 no
longer holds in this section (as the Gaussian process behind
the data has an inﬁnite support). However, with high prob-
ability, the data belongs to a bounded set. Therefore, we can
adopt local randomized differential privacy instead of local
differential privacy [32]–[34].

In the Gaussian linear regression case described above,
we can redeﬁne deﬁne the empirical probability distribution
(cid:98)Dn to be Gaussian with mean (cid:98)µn and covariance (cid:98)Σn, where

(cid:98)µn :=

(cid:98)Σn :=

n
(cid:88)

1
n

i=1
1
n − 1

(cid:21)

(cid:20)˜xi
˜yi

,

n
(cid:88)

i=1

(cid:21)

(cid:18)(cid:20)˜xi
˜yi

− (cid:98)µn

(cid:21)

(cid:19) (cid:18)(cid:20)˜xi
˜yi

(cid:19)(cid:62)

.

− (cid:98)µn

Theorem 5. Assume that W is the Gaussian distribution in
Theorem 1. With high probability, for any ε > 0, there
exists nε ≥ 0 such that if n ≥ nε,

W2((cid:98)Dn, P) ≤ ε +

(cid:112)2 log(1.25/δ)p∆
(cid:15)

.

Proof: The proof is similar to Theorem 2 with the
exception of using the fact that W2((cid:98)Dn, D) → 0 with
probability one as n → ∞ [35, Theorem 2.1].

In this section, we consider the big data regime (n (cid:29) 1)
so, without loss of generality, ε (cid:117) 0 and µ (cid:117)
(cid:98)µn. Therefore,
by using Theorem 5, the original distribution P would be-
long to the ambiguity set {G : G is Gaussian ∧ EG{(x, y)} =
(cid:98)µn ∧ W2(G, (cid:98)Dn) ≤ ρ} if we select ρ = (cid:112)2 log(1.25/δ)p∆/(cid:15).
This observation motivates training the machine learning
model by solving the distributionally-robust optimization
problem in

ˆJn := min
A,B

sup
G : G is Gaussian
EG{(x, y)} = (cid:98)µn
W2(G, (cid:98)Dn) ≤ ρ

EG{(cid:96)(M(x; A, B), y)},

(10)

for constant ρ = (cid:112)2 log(1.25/δ)p∆/(cid:15). Following [36, Propo-
sition 7], if (cid:98)Dn and G are both Gaussian with same mean
µ (cid:117)

(cid:98)µn, we get

4

W2((cid:98)Dn, G)2 =trace(ΣG + (cid:98)Σn − 2( (cid:98)Σ1/2

n ΣG (cid:98)Σ1/2

n )1/2),

(11)

where ΣG denotes the covariance of G. The expected risk is
given by

EG{(cid:96)(M(x; A, B), y)}

= trace(EG{(Ax + B − y)(Ax + B − y)(cid:62)})

(cid:40)

(cid:32)

EG

(cid:2)A −I(cid:3)

= trace

(cid:21)(cid:62)

(cid:20)x
y

(cid:21) (cid:20)x
y

(cid:2)A −I(cid:3)(cid:62) + BB(cid:62)

+ (cid:2)A −I(cid:3)

(cid:21)

(cid:20)x
y

B(cid:62) + B

(cid:21)(cid:62)

(cid:20)x
y

(cid:2)A −I(cid:3)(cid:62)

(cid:41)(cid:33)

= trace

(cid:16) (cid:2)A −I(cid:3) (ΣG + (cid:98)µn (cid:98)µ(cid:62)
+ (cid:2)A −I(cid:3)

(cid:98)µnB(cid:62) + B (cid:98)µ(cid:62)

n

n ) (cid:2)A −I(cid:3)(cid:62) + BB(cid:62)
(cid:2)A −I(cid:3)(cid:62) (cid:17)

.

Using [14, Proposition 2.8], it can be deduced that

EG{(cid:96)(M(x; A, B), y)}

sup
G : G is Gaussian
EG{(x, y)} = (cid:98)µn
W2(G, (cid:98)Dn) ≤ ρ

=trace

(cid:16) (cid:2)A −I(cid:3)

(cid:2)A −I(cid:3)(cid:62)

n

(cid:98)µn (cid:98)µ(cid:62)
+ BB(cid:62) + (cid:2)A −I(cid:3)
(cid:2)A −I(cid:3)(cid:62) (cid:17)
+ B (cid:98)µ(cid:62)

n

(cid:98)µnB(cid:62)
+ f (A),

where

(cid:34)

f (A) := inf
ξ

ξ(ρ2 − trace( (cid:98)Σn))

(cid:32)(cid:32)

+ ξ2trace

ξI −

(cid:21)

(cid:20)A(cid:62)
−I

(cid:2)A −I(cid:3)

(cid:33)−1

(cid:33)(cid:35)
,

(cid:98)Σn

s.t. ξI (cid:31)

(cid:21)

(cid:20)A(cid:62)
−I

(cid:2)A −I(cid:3) .

Further,

trace

(cid:16) (cid:2)A −I(cid:3)
+ (cid:2)A −I(cid:3)

(cid:98)µn (cid:98)µ(cid:62)

n

(cid:2)A −I(cid:3)(cid:62) + BB(cid:62)

(cid:98)µnB(cid:62) + B (cid:98)µ(cid:62)

n

(cid:2)A −I(cid:3)(cid:62) (cid:17)

= E(cid:98)Dn {(cid:96)(M(x; A, B), y)} − (cid:2)A −I(cid:3)

(cid:98)Σn

(cid:2)A −I(cid:3)(cid:62) .

Therefore, the optimization problem in (10) can be rewritten
as

ˆJn := min
A,B

E(cid:98)Dn {(cid:96)(M(x; A, B), y)} + λ(A),

(12)

where λ(A) := f (A)−(cid:2)A −I(cid:3)
is the optimal
regularization for linear regression with locally-differential
private data with Gaussian data. This regularization is com-
pletely novel in the context of linear regression. In what
follows, we provide a more tractable formulation for (12).

(cid:2)A −I(cid:3)(cid:62)

(cid:98)Σn

Theorem 6. The solution to (12) is given by
ξ(ρ2 − trace( (cid:98)Σn)) + trace(Z)
(cid:16) (cid:2)A −I(cid:3)

min
A,B,ξ,Z

+ trace

n

(cid:98)µn (cid:98)µ(cid:62)
(cid:98)µnB(cid:62) + B (cid:98)µ(cid:62)

(cid:2)A −I(cid:3)(cid:62) + BB(cid:62)
(cid:2)A −I(cid:3)(cid:62) (cid:17)

n

,

+ (cid:2)A −I(cid:3)

s.t.







Z
(cid:98)Σ1/2
n ξ
0

(cid:98)Σ1/2
n ξ
ξI
(cid:2)A −I(cid:3)



(cid:21)





0
(cid:20)A(cid:62)
−I
I

(13a)

(cid:23) 0.

(13b)

Proof: Let Z (cid:23) 0 be such that

(cid:32)

ξ2 (cid:98)Σ1/2
n

ξI −

(cid:21)

(cid:20)A(cid:62)
−I

(cid:2)A −I(cid:3)

(cid:33)−1

(cid:98)Σ1/2

n (cid:22) Z.

Using the Schur complement [37], we can transform this
inequality into



Z

(cid:98)Σ1/2
n ξ

ξI −

(cid:98)Σ1/2
n ξ
(cid:21)

(cid:20)A(cid:62)
−I

(cid:2)A −I(cid:3)


 (cid:23) 0.

Again, using the Schur complement, this matrix inequality
can be transformed into
(cid:34)





(cid:35)

Z
(cid:98)Σ1/2
n ξ

(cid:98)Σ1/2
n ξ
ξI

0
A(cid:62)
−I

−



(cid:2)0 A −I(cid:3)





Z

(cid:98)Σ1/2
n ξ


0

(cid:98)Σ1/2
n ξ
ξI
(cid:2)A −I(cid:3)



(cid:21)





0
(cid:20)A(cid:62)
−I
I

(cid:23) 0.

(14)

Finally, using the Schur complement, the constraint in com-
puting f (A) can rewritten as



ξI

(cid:2)A −I(cid:3)

(cid:21)

 (cid:23) 0.

(cid:20)A(cid:62)
−I
I

(15)

Note that (15) is a subset of (14) and thus need not be added
to the constraints.

5 EXPERIMENTAL RESULT
Here, we demonstrate the performance of distributionally-
robust machine learning on two practical datasets.

5.1 Loan Dataset

roughly 887,000
The dataset contains information of
loans [38]. The inputs contain loan information, e.g., total
loan size, and borrower information, e.g., age. The outputs
are the interest rates of loans. Categorical features, e.g.,
state of residence and loan grade, are encoded with integer
numbers. Unique identiﬁers, e.g., identity, and irrelevant
attributes, e.g., URLs, are removed. We scale all input at-
tributes and outputs to be between zero and one to meet
Assumption 2. We consider linear regression framework in
Remark 2 and Section 4. We use the Gaussian mechanism in
Theorem 1 to generate locally-differentially private datasets

5

(12)
(8) with ρ = 10−2
(2)

}
)
y
,
)
B

,

A

;

x
(
M

(
(cid:96)
{
P
E

(cid:15)
Fig. 1. Performance of the linear regression for the loan dataset trained
on the locally-differential private dataset tested on the original probability
distribution.

(12)
(8) with ρ = 10−2
(2)

}
)
y
,
)
B

,

A

;

x
(
M

(
(cid:96)
{
P
E

(cid:15)
Fig. 2. Performance of the linear regression for the Adult dataset trained
on the locally-differential private dataset tested on the original probability
distribution.

with δ = 10−2 and (cid:15) ∈ [100, 102]/px. We use 50 entries of the
dataset for training and the remaining entries for evaluation.
Note that we are using such a low number of data entries as
we are using linear regression. Figure 1 illustrates the perfor-
mance of the linear regression for the loan dataset trained on
the locally-differential private dataset tested on the original
probability distribution. Regularization clearly improves the
out-of-sample performance of the model. Furthermore, the
optimal regularization in Section 4 can be better than the
generic regularization based on the Lipschitz constant of the
loss function in (8).

5.2 Adult Dataset

This dataset contains nearly 49,000 records from the 1994
Census database [39]. The records contain features, e.g.,
age and education. The output is binary number indicating
whether an individuals earns more than $50,000. Similarly,
we scale all input attributes and outputs to be between
zero and one in line with Assumption 2. We consider the
linear regression framework in Remark 2 and Section 4.
We use the Gaussian mechanism in Theorem 1 to generate
locally-differentially private datasets with δ = 10−2 and

10010110210-310-210-11001011020.150.1550.160.1650.170.1750.180.185(cid:15) ∈ [100, 102]/px. We use 50 entries of the dataset for
training and the remaining entries for evaluation. Again,
we are using such a low number of data entries as we are
using linear regression. Figure 2 illustrates the performance
of the linear regression for the adult dataset trained on
the locally-differential private dataset tested on the original
probability distribution. Regularization clearly improves the
performance of the model.

6 CONCLUSIONS
We considered machine learning, particularly regression, us-
ing locally-differentially private datasets is considered. We
posed machine learning with locally-differentially private
datasets as a distributionally-robust optimization with an
ambiguity set parameterized by the Wasserstein distance.
For general distributions, the distributionally-robust opti-
mization problem was relaxed as a regularized machine
learning problem with the Lipschitz constant of the machine
learning model as a regularizer. For Gaussian data, the
distributionally-robust optimization problem was solved
exactly to ﬁnd an optimal regularizer.

REFERENCES

[1] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating
noise to sensitivity in private data analysis,” in Theory of Cryptog-
raphy (S. Halevi and T. Rabin, eds.), (Berlin, Heidelberg), pp. 265–
284, Springer Berlin Heidelberg, 2006.

[2] C. Dwork and A. Roth, “The algorithmic foundations of differ-
ential privacy,” Foundations and Trends® in Theoretical Computer
Science, vol. 9, no. 3–4, pp. 211–407, 2014.

[3] P. Kairouz, S. Oh, and P. Viswanath, “Extremal mechanisms for
local differential privacy,” The Journal of Machine Learning Research,
vol. 17, no. 1, pp. 492–542, 2016.

[4] R. Dewri, “Local differential perturbations: Location privacy un-
der approximate knowledge attackers,” IEEE Transactions on Mo-
bile Computing, vol. 12, no. 12, pp. 2360–2372, 2013.
J. C. Duchi, M. I. Jordan, and M. J. Wainwright, “Local privacy and
statistical minimax rates,” in 2013 IEEE 54th Annual Symposium on
Foundations of Computer Science, pp. 429–438, 2013.

[5]

[7]

[6] X. Ren, C.-M. Yu, W. Yu, S. Yang, X. Yang, J. A. McCann, and S. Y.
Philip, “LoPub: High-dimensional crowdsourced data publication
with local differential privacy,” IEEE Transactions on Information
Forensics and Security, vol. 13, no. 9, pp. 2151–2166, 2018.
´U. Erlingsson, V. Pihur, and A. Korolova, “RAPPOR: Randomized
aggregatable privacy-preserving ordinal response,” in Proceedings
of the 2014 ACM SIGSAC conference on computer and communications
security, pp. 1054–1067, 2014.
J. Tang, A. Korolova, X. Bai, X. Wang, and X. Wang, “Privacy loss in
Apple’s implementation of differential privacy on MacOS 10.12,”
arXiv preprint arXiv:1709.02753, 2017.

[8]

[9] A. Smith, A. Thakurta, and J. Upadhyay, “Is interaction necessary
for distributed private learning?,” in 2017 IEEE Symposium on
Security and Privacy (SP), pp. 58–77, IEEE, 2017.

[10] D. Wang, M. Gaboardi, and J. Xu, “Empirical risk minimization in
non-interactive local differential privacy revisited,” in Advances in
Neural Information Processing Systems, pp. 965–974, 2018.

[11] K. Zheng, W. Mou, and L. Wang, “Collect at once, use effectively:
making non-interactive locally private learning possible,” in Pro-
ceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 4130–4139, 2017.

[12] D. Wang, A. Smith, and J. Xu, “Noninteractive locally private
learning of linear models via polynomial approximations,” in
Algorithmic Learning Theory, pp. 898–903, 2019.

[13] P. M. Esfahani and D. Kuhn, “Data-driven distributionally robust
optimization using the wasserstein metric: Performance guar-
antees and tractable reformulations,” Mathematical Programming,
vol. 171, no. 1-2, pp. 115–166, 2018.

[14] V. A. Nguyen, D. Kuhn, and P. M. Esfahani, “Distributionally
robust inverse covariance estimation: The wasserstein shrinkage
estimator,” arXiv preprint arXiv:1805.07194, 2018.

6

[15] D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shaﬁeezadeh-
Abadeh, “Wasserstein distributionally robust optimization: The-
ory and applications in machine learning,” in Operations Research
& Management Science in the Age of Analytics, pp. 130–166, IN-
FORMS, 2019.

[16] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski, Robust optimization,

vol. 28. Princeton University Press, 2009.

[17] K. Postek, D. den Hertog, and B. Melenberg, “Computationally
tractable counterparts of distributionally robust constraints on risk
measures,” SIAM Review, vol. 58, no. 4, pp. 603–650, 2016.

[18] E. Delage and Y. Ye, “Distributionally robust optimization under
moment uncertainty with application to data-driven problems,”
Operations research, vol. 58, no. 3, pp. 595–612, 2010.

[19] Z. Hu and L. J. Hong, “Kullback-leibler divergence constrained
distributionally robust optimization,” Available at Optimization On-
line, 2013.

[20] A. Sinha, H. Namkoong, and J. Duchi, “Certiﬁable distributional
robustness with principled adversarial training,” in Proceedings of
the Machine Learning and Computer Security Workshop (co-located with
Conference on Neural Information Processing Systems 2017), vol. 2,
2017.

[21] F. Farokhi, “Regularization helps with mitigating poisoning at-
tacks: Distributionally-robust machine learning using the wasser-
stein distance,” arXiv preprint arXiv:2001.10655, 2020.

[22] R. Chen and I. C. Paschalidis, “A distributionally robust optimiza-
tion approach for outlier detection,” in 2018 IEEE Conference on
Decision and Control (CDC), pp. 352–357, IEEE, 2018.

[23] A. Pr ¨ugel-Bennett, The Probability Companion for Engineering and

Computer Science. Cambridge University Press, 2020.

[24] G. C. Pﬂug and A. Pichler, Multistage Stochastic Optimization.
Springer Series in Operations Research and Financial Engineering,
Springer International Publishing, 2014.

[25] L. V. Kantorovich and G. Rubinshtein, “On a space of totally

additive functions,” Vestn. Lening. Univ, vol. 13, pp. 52–59, 1958.

[26] M. Anthony and P. L. Bartlett, Neural Network Learning: Theoretical

Foundations. Cambridge University Press, 2009.

[27] C. Brownlees, E. Joly, G. Lugosi, et al., “Empirical risk minimiza-
tion for heavy-tailed losses,” The Annals of Statistics, vol. 43, no. 6,
pp. 2507–2536, 2015.

[28] O. Catoni, “Challenging the empirical mean and empirical vari-
ance: A deviation study,” in Annales de l’Institut Henri Poincar´e,
Probabilit´es et Statistiques, vol. 48, pp. 1148–1185, 2012.

[29] P. J. Bickel, D. A. Freedman, et al., “Some asymptotic theory for
the bootstrap,” The annals of statistics, vol. 9, no. 6, pp. 1196–1217,
1981.

[30] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley,

2012.

[31] F. Farokhi, “Regularization helps with mitigating poisoning at-
tacks: Distributionally-robust machine learning using the wasser-
stein distance,” 2020.
arXiv:2001.10655, https://arxiv.org/abs/
2001.10655.

[32] A. Machanavajjhala, D. Kifer, J. Abowd, J. Gehrke, and L. Vilhuber,
“Privacy: Theory meets practice on the map,” in Proceedings of
the 2008 IEEE 24th International Conference on Data Engineering,
pp. 277–286, 2008.

[33] B. I. Rubinstein and F. Ald`a, “Pain-free random differential privacy
with sensitivity sampling,” in International Conference on Machine
Learning, pp. 2950–2959, 2017.

[34] R. Hall, A. Rinaldo, and L. Wasserman, “Random differential
privacy,” Journal of Privacy and Conﬁdentiality, vol. 4, no. 2, pp. 43–
59, 2012.

[35] T. Rippl, A. Munk, and A. Sturm, “Limit laws of the empirical
wasserstein distance: Gaussian distributions,” Journal of Multivari-
ate Analysis, vol. 151, pp. 90–109, 2016.

[36] C. R. Givens and R. M. Shortt, “A class of wasserstein metrics
for probability distributions.,” The Michigan Mathematical Journal,
vol. 31, no. 2, pp. 231–240, 1984.

[37] F. Zhang, The Schur complement and its applications, vol. 4. Springer

Science & Business Media, 2006.

[38] W. Kan, “Lending club loan data: Analyze lending club’s is-
sued loans,” 2016. Kaggle, https://www.kaggle.com/wendykan/
lending-club-loan-data.

[39] D. Dheeru and E. Karra Taniskidou, “UCI machine learning repos-
itory,” 2017. University of California, Irvine, http://archive.ics.uci.
edu/ml.

