0
2
0
2

v
o
N
7
1

]
L
P
.
s
c
[

1
v
1
8
8
8
0
.
1
1
0
2
:
v
i
X
r
a

Learning functional programs
with function invention and
reuse

Andrei Diaconu

University of Oxford

Honour School of Computer Science

Trinity 2020

 
 
 
 
 
 
Abstract

Inductive programming (IP) is a ﬁeld whose main goal is synthe-
sising programs that respect a set of examples, given some form of

background knowledge. This paper is concerned with a subﬁeld

of IP, inductive functional programming (IFP). We explore the

idea of generating modular functional programs, and how those
allow for function reuse, with the aim to reduce the size of the

programs. We introduce two algorithms that attempt to solve

the problem and explore type based pruning techniques in the

context of modular programs. By experimenting with the imple-
mentation of one of those algorithms, we show reuse is important

(if not crucial) for a variety of problems and distinguished two

broad classes of programs that will generally beneﬁt from func-

tion reuse.

Contents

1 Introduction

1.1
Inductive programming . . . . . . . . . . . . . . . . . . . . . .
1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Structure of the report . . . . . . . . . . . . . . . . . . . . . .

5

5
6

7

8

2 Background and related work

10
2.1 Background on IP . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2.1 Metagol

. . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2.2 Magic Haskeller . . . . . . . . . . . . . . . . . . . . . . 13
2.2.3 λ2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Invention and reuse . . . . . . . . . . . . . . . . . . . . . . . . 13

2.3

3 Problem description

15

3.1 Abstract description of the problem . . . . . . . . . . . . . . . 15
Invention and reuse . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2

4 Algorithms for the program synthesis problem

21

4.1 Preliminaries

. . . . . . . . . . . . . . . . . . . . . . . . . . . 22

4.1.1 Target language and the type system . . . . . . . . . . 22
4.1.2 Combinatorial search . . . . . . . . . . . . . . . . . . . 23

4.1.3 Relations concerning the program space

. . . . . . . . 25

4.1.4 Partitioning the templates . . . . . . . . . . . . . . . . 27

4.2 The algorithms

. . . . . . . . . . . . . . . . . . . . . . . . . . 27

3

4.2.1 Only linear function templates algorithm . . . . . . . . 28

4.2.2 Consequences of adding branching templates . . . . . . 33

4.2.3 An algorithm for branching templates . . . . . . . . . . 35

5 Experiments and results

37
5.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

5.1.1

5.1.2

addN . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

ﬁlterUpNum . . . . . . . . . . . . . . . . . . . . . . . . 38

addRevFilter

. . . . . . . . . . . . . . . . . . . . . . . 39
5.1.3
5.1.4 maze . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5.1.5

droplasts . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5.2 On function reuse and function templates . . . . . . . . . . . . 41

6 Conclusions

45
6.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

6.2 Reﬂections, limitations and future work . . . . . . . . . . . . . 46

A Implementation details

47

A.1 Language implementation . . . . . . . . . . . . . . . . . . . . 47
A.2 Algorithms implementation . . . . . . . . . . . . . . . . . . . 48

Bibliography

49

4

Chapter 1

Introduction

1.1

Inductive programming

Inductive programming (IP) [8] - also known as program synthesis or example

based learning - is a ﬁeld that lies at the intersection of several computer sci-
ence topics (machine learning, artiﬁcial intelligence, algorithm design) and is

a form of automatic programming. IP, as opposed to deductive programming

[15] (another automatic programming approach, where one starts with a full

speciﬁcation of the target program) tackles the problem starting with an in-
complete speciﬁcation and tries to generalize that into a program. Usually,

that incomplete speciﬁcation is represented by examples, so we can infor-

mally deﬁne inductive programming to be the process of creating programs

from examples using a limited amount of background information - we shall
call this process the program synthesis problem [20]. We give an example of

what an IP system might produce, given a task:

Example 1.1

Input: The deﬁnitions of map and increment and the examples f ([1, 2, 3]) =

[2, 3, 4] and f ([5, 6]) = [6, 7]).

Output: The deﬁnition f = map increment.

One of the key challenges of IP (and what makes it attractive) is the need

to learn from small numbers of training examples, which mostly rules out

statistical machine learning approaches, such as SVMS and neural networks.

5

Figure 1.1: Flash ﬁll in action

This can clearly create problems:

if the examples are not representative

enough, we might not get the program we expect.

As noted in the survey by Gulwani et al [8], one of the main areas of

research in IP has been end-user programming. More often than not, an ap-

plication will be used by a non-programmer, and hence that user will proba-

bly not be able to write scripts that make interacting with that application
easier. IP tries to oﬀer a solution to that problem: the user could supply a

(small) amount of information, such as a list of examples that describe the

task, and an IP system could generate a small script that automates the task.

Perhaps one of the most noteworthy applications of this idea is in the MS
Excel plug-in Flash Fill [7]. Its task is to induce a program that generalizes

some spreadsheet related operation, while only being given a few examples -

usage of Flash Fill can be seen in ﬁgure 1.1.

1.2 Motivation

Two main areas of research in IP are inductive functional programming (IFP,

which we will focus on in this paper) and inductive logic programming (ILP).

The idea of function invention in the IFP context is not new, and indeed some
systems use it, such as IGOR 2 and λ2. In informal terms, function invention
mimics the way humans write programs: instead of writing a long one-line

program, we break the bigger program into auxiliary functions that can be

6

used to build a modular (equivalent) program.

In this context, we have asked the question of whether another program

writing technique could be useful for inductive programming: reusing the

functions that we have already invented. By reuse we mean that once a
function has been invented, it can then be used in the deﬁnition of another

function. While some ILP systems have explored the idea of reusing func-

tions (such as Metagol and Hexmil [2] and to a lesser extent DILP [5] and

ILASP [14]), function reuse and its beneﬁts (if any) have not really been
explored in the IFP context, as noted by Cropper [1]. When investigating

the existing systems with invention capabilities, we have observed that the

way the invention process is conducted makes reuse practically impossible.

Moreover, even though predicate invention and reuse have been claimed as
useful (at least in the ILP context [18]), to our knowledge there has been no

work that empirically demonstrates that it is, nor any work discussing when

it may be useful. To address those limitations, in this work we are interesting

in the following research questions:

Q1 Can function reuse improve learning performance (ﬁnd programs faster)?

Q2 What impact does modularity have on pruning techniques, especially

type based ones?

Q3 What impact does the grammar of the synthesized programs have on

function reuse?

Q4 What classes of problems beneﬁt from it; that is, can we describe the

kinds of programs where function reuse is useful?

1.3 Contributions

In this paper, we make the following contributions:

• We provide a formal framework to describe IFP approaches that solve
the program synthesis problem by creating modular programs and that

can exploit function reuse.

7

• Given this formal framework, we create two algorithms that solve the
synthesis problem. One of them uses type based pruning to speed up

the searching process, but uses a restrictive grammar; we have proven

that for general grammars, this algorithm (which uses a “natural” type
inference based pruning approach) loses completeness (which in partic-

ular greatly hinders function reuse). The second algorithm does not use

type based pruning and works with general grammars, but we propose

a way in which this might be achieved.

• Our experimental work has provided positive results, which shed light
on the usefulness of reuse in the IFP context; for example, we have

shown that reuse can decrease the size of the synthesized programs
and hence reduce the overall computation time (in some cases dra-

matically). Through experimentation, we have also distinguished two

classes of problems for which reuse is important: AI planning problems

and problems concerned with nested data structures (we have focused
on lists).

1.4 Structure of the report

The rest of the paper is structured as follows:

• chapter 2: Presents background on inductive programming, function

invention and reuse, and a variety of other systems.

• chapter 3: Presents a formal framework for describing the program

synthesis problem and formalizes function reuse.

• chapter 4: Presents two algorithms that attempt to solve the program
synthesis problem, in light of the description presented in chapter 3.

• chapter 5: Explores the role of function reuse through experimenta-
tion and contains a variety of experiments that validate our hypothesis;

we also explore the various use cases of function reuse.

8

• chapter 6: Presents the conclusions, limitations, possible extensions

of the project.

9

Chapter 2

Background and related work

In the previous chapter, we have informally introduced the concept of induc-

tive programming (IP), presented its relevance and showcased our ideas. In

this chapter, we ﬁrst provide the reader with more background on IP (areas
of research, approaches) and then switch to literature review, showing diﬀer-

ent IP systems and their relevance to ours. We ﬁnish the chapter by talking

about the idea of function invention and reuse.

2.1 Background on IP

IP has been around for almost half a century, with a lot of systems trying

to tackle the problem of ﬁnding programs from examples.

It is a subject

that is placed at the crossroad between cognitive sciences, artiﬁcial intelli-
gence, algorithm design and software development [13]. An interesting fact

to note is that IP is a machine learning problem (learning from data) and

moreover, in recent years it has gained attention because of the inherent

transparency of its approach to learning, as opposed to the black box nature
of statistical/neuronal approaches, as noted by Schmid [19].

IP has two main research areas, as noted by Gulwani et al. [8]:

• Inductive functional programming (IFP): IFP focuses on the synthesis
of functional programs, typically used to create programs that manip-

ulate data structures.

10

• Inductive logical programming (ILP): ILP started as research on in-
duction in a logical context [8], generally used for learning AI tasks.

It’s aim is to construct a hypothesis (logic programs) h which explain

examples E in terms of some background knowledge B [17].

As highlighted in the review by Kitzelmann [13], there have been two

main approaches to inductive programming (for both IFP and ILP):

• analytical approach: Its aim is to exploit features in the input-output
examples; the ﬁrst systematic attempt was done by Summers’ THESIS

[23] system in 1977. He observed that using a few basic primitives and
a ﬁxed program grammar, a restricted class of recursive LISP programs

that satisfy a set of input-output examples can be induced. Because of

the inherent restrictiveness of the primitives, the analytical approach

saw little innovation in the following decade, but systems like IGOR1,
IGOR2 [11] have built on Summers’ work. The analytical approach

is also found in ILP, a well known example being Muggleton’s Progol

[16].

• generate-and-test approach (GAT): In GAT, examples are not
used to actually construct the programs, but rather to test streams of

possible programs, selected on some criteria from the program space.

Compared to the analytical approach, GAT tends to be the more ex-
pressive approach, at the cost of higher computational time. Indeed,

the ADATE system, a GAT system that uses genetic programming

techniques to create programs, is one of the most powerful IP system

with regards to expressivity [13]. Another well known GAT system is
Katayama’s Magic Haskeller [12], which uses type directed search and

higher-order functions as background knowledge. Usually, to compen-

sate for the fact that the program space is very big, most GAT systems

will include some sort of pruning that discards undesirable programs.

11

2.2 Related work

We now present three systems that helped us develop our ideas and contrast

them with our work.

2.2.1 Metagol

Metagol [2] is an ILP system that induces Prolog programs. It uses an idea
called MIL, or meta-interpretative learning, to learn logic programs from

examples. It uses three forms of background information:

• compiled background knowledge (CBK): those are small, ﬁrst order
Prolog programs that are deductively proven by the Prolog interpreter.

• interpreted background knowledge (IBK): this is represented by higher-
order formulas that are proven with the aid of a meta-interpreter (since
Prolog does not allow clauses with higher-order predicates as variables);

for example, we could describe map/3 using the following two clauses:

map([], [], F ) : − and map([A|As], [B|Bs], F ) : −F (A, B), map(As, Bs, F ).

• metarules: those are rules that enforce the form (grammar) of the in-
duced program’s clauses; an example would be P (a, b) : −Q(a, c), R(c, b),

where upper case letters are existentially quantiﬁed variables (they will

be replaced with CBK or IBK).

The way the hypothesis search works is as follows: try to prove the re-

quired atom using CBK; if that fails, fetch a metarule, and try to ﬁll in the

existentially quantiﬁed variables; continue until a valid hypothesis (one that
satisﬁes the examples) is found. Something to note here is that Metagol gen-

erates new examples:

if we select the map metarule, based on the existing

examples we can infer a set of derived examples that the functional argument

of map must satisfy. This technique is used to prune incorrect programs from
an early stage. All this process is wrapped in a depth-bounded search, so as

to ensure the shortest program is found.

Our paper has started as an experiment to see whether ideas from Metagol

could be transferred to a functional setting; hence, in the next chapters we use

12

similar terminology, especially around metarules and background knowledge.

We will also use depth-bounded search in our algorithm, for similar reasons

to Metagol.

2.2.2 Magic Haskeller

Katayama’s Magic Haskeller [12] is a GAT approach that uses type pruning

and exhaustive search over the program space. Katayama argues that type

pruning makes the search space manageable. One of the main innovation

of the system was the usage of higher-order functions, which speeds up the
searching process and helps simplify the output programs (which are chains

of function applications). Our system diﬀers in the fact that our programs

are modular, which allow for function reuse. One of Magic Haskeller ’s limi-

tations is the inability to provide user supplied background knowledge. The
implementation of our algorithms enable a user to experiment with the back-

ground functions in a programmatic manner, and we also make it fairly easy

to change the grammar of the programs.

2.2.3 λ2

λ2 [6] is an IFP system which combines GAT and analytical methods: the
search is similar to Magic Haskeller, in the way that it uses higher order

functions and explores the program space using type pruning, but diﬀers in

the fact that programs have a nested form (think of where clauses in Haskell)
and uses an example propagation pruning method, similar to Metagol. How-

ever, such an approach does not allow function reuse, since an inner function

can’t use an “ancestor” function in its deﬁnition (possible inﬁnite loop). Our

paper tries to address this, exploring the possibility of creating non-nested
programs and hence allowing function reuse.

2.3

Invention and reuse

Generally, most IP approaches tend to disregard the extra knowledge found
during the synthesis process as another form of background knowledge. In

13

fact, systems like λ2 and Magic Haskeller make this impossible because of
how the search is conducted. Some systems, like Igor 2 do have a limited

form of it, but it is very restrictive and does not allow function reuse in a

general sense. This usually stems from what grammars the induced programs
use. One of our main interests has been the usefulness of function reuse by

allowing a modular (through function invention) way of generating programs

(that is, we create “standalone” functions that can then be pieced together

like a puzzle). For example, consider the drop lasts problem: given a non-
empty list of lists, remove the last element of the outer list as well as the last

elements of all the inner ones. Example 2.1 shows a possible program that

was synthesized using only invention. However, if function reuse is enabled,

example 2.2 shows how we can synthesize a simpler program, which we would
expect to reduce the searching time.

Example 2.1 (droplasts - only invention)
t a r g e t = f 1 . f 2
f 1 = f 2 . f 4
f 2 = reverse . f 3
f 3 = map reverse
f 4 = t a i l . f 5
f 5 = map t a i l

Example 2.2 (droplasts - invention + reuse)

Note how f 1 is reused to create a shorter program.

t a r g e t = f 1 . f 2
f 2 = map f 1
f 1 = reverse . f 3
f 3 = t a i l . reverse

An interesting questions when considering function reuse is what kind of

programs beneﬁt from it, which we explore in chapter 5, but we will now

move to formalizing the program synthesis problem.

14

Chapter 3

Problem description

Before presenting algorithms that solve the synthesis problem, we need to

formalize it. We will assume, for the rest of the chapter, that all deﬁnitions

will be relative to a target language L, whose syntax will be speciﬁed in the
next chapter.

3.1 Abstract description of the problem

A program synthesis algorithm’s aim is to induce programs with respect to
some sort of user provided speciﬁcation. The synthesis process will create

programs which we call induced programs, that are composed of a set of

functions which we call induced functions. For each induced program we will

distinguish a function called the target function, which is to be applied to
the examples to check whether a candidate program is a solution. Intuitively,

the output shall be an induced program whose target function satisﬁes the

provided speciﬁcation.

The provided speciﬁcation in this paper shall be divided in two parts:

background knowledge and input-output examples.

Deﬁnition 3.1 (Background knowledge (BK)). We deﬁne background knowl-

edge to be the information used during the synthesis process. The BK com-
pletely determines the possible forms an induced program can have. There

are three types of BK that we consider:

15

• Background functions: represents the set of functions provided via an
external source. We require those functions to be total so as to not

introduce non-termination in an induced program. We use the notation
BKF to refer to this kind of knowledge.

• Invented functions: represents the set of functions that are invented
during the synthesis process; this set grows dynamically during the

synthesis process (with each new invented function). We use the nota-
tion BKI to refer to this kind of knowledge.

• Function templates: a set of lambda calculus-style contexts that de-
scribe the possible forms of the induced functions. We use the notation
BKT to refer to it.

Let us unpack this deﬁnition. We have referred to both BKI and BKF to
be sets of functions: more precisely, they are sets containing pairs of the form
(fname, fbody): fname represents the identiﬁer (function name) of function f ,
whereas fbody corresponds to the body of its deﬁnition. When we write f , we
refer to the pair. Example 3.1 shows an example of two functions that might
be part of BKF .

Example 3.1 (Background functions)
r e v xs = i f xs == [ ]

then [ ]
e l s e r e v ( t a i l xs ) ++ [ head xs ]

addOne x = x + 1

Function templates represent the blueprints that we use when deﬁning
invented functions. They are contexts in the meta-theory of lambda calcu-

lus sense, that represent the application of a higher-order combinator, where

the “holes” are place-holders for the required number of functional inputs

for such a combinator. Those place-holders signify that there is missing in-
formation in the body of the induced function that needs to be ﬁlled with

either background functions or synthesized functions. We have chosen those

to specify the grammar of the invented functions because higher-order func-

tions let us combine the background and invented functions in complex ways,

16

and provide great ﬂexibility. We note the similarity of our function templates

to metarules [3] and sketches [21], which serve similar purposes in the respec-

tive systems where they are used. Example 3.2 shows the form of a few such

templates. For convenience, we number the “holes”, e.g.
starting from 1.

i , with indices

Example 3.2 (Function templates)

Conditional templates: if 1 then 2 else 3 .
Identity: 1 .

Higher-order templates: 1 . 2 , map 1 , ﬁlter 1 .

We say an induced function is complete if its body has no holes and all

functions mentioned in it have a deﬁnition, or incomplete otherwise. Simi-

larly, we say an induced program is complete if it is composed of complete
functions. We give a short example to see how templates and functions in-

teract with each other, which will provide some intuition for the algorithmic

approach to the inductive process presented in the next chapter.

Example 3.3 (Derivation)

Suppose we wish to ﬁnd the complete function F = map reverse. The

following process involving the BK will take place: we invent a new function

F , and assign it the map template to obtain the deﬁnition F = map 1 ; we
then ﬁll the remaining hole using reverse.

The second part of the speciﬁcation is represented by examples.

Deﬁnition 3.2 (Examples). Examples are user provided input-output pairs

that describe the aim of the induced program. We shall consider two types

of examples:

• Positive examples: those specify what an induced program should pro-

duce;

• Negative examples: those specify what an induced program should not

produce;

17

We use the relation in (cid:55)→+ out to refer to positive examples, and the
relation in (cid:55)→− out to refer to negative ones. While the positive examples
have a clear role in the synthesis process, the negative ones serve a slightly

diﬀerent one: they try to remove ambiguity, which is highlighted in example
3.4. Something to note is that both the positive and the negative examples

need to have compatible types, meaning that if a type checker is used, all

the inputs would share the same type, and so should the outputs.

Example 3.4 (Examples)
Given the positive examples [3, 2, 1] (cid:55)→+ [1, 2, 3] and [5, 4] (cid:55)→+ [4, 5], and the
negative examples [1, 3, 2] (cid:55)→− [2, 3, 1] and [5, 4] (cid:55)→− [5, 4] then the program
we want to induce is likely to be a list sorting algorithm. Note that if we only
look at the positive examples, another possible induced program is reverse,
but the negative example [1, 3, 2] (cid:55)→− [2, 3, 1] removes this possibility.

Deﬁnition 3.3 (Satisﬁability). We say an complete induced program P
whose target function is f satisﬁes the relations (cid:55)→+ and (cid:55)→− if:

• ∀(in, out) ∈(cid:55)→+ .f (in) = out

• ∀(in, out) ∈(cid:55)→− .f (in) (cid:54)= out

Deﬁnition 3.4 (Program space). Assume we are given background knowl-
edge BK and let Tcheck be a type checking algorithm for L. We deﬁne the
program space PBK to be composed of programs written in L such that:

1. the bodies of the induced functions are either function templates (which

still have holes in them) or function applications (for completed func-

tions).

2. the inputs for the higher-order functions (of the templates) are either

functions from BKI or BKF .

3. they are typeable w.r.t. Tcheck.

4. they contain no cyclical deﬁnitions (guard against non-termination).

18

Note how the PBK contains induced programs whose functions could still
have unﬁlled holes. We now describe the solution space, which contains the

programs we consider solutions.

Deﬁnition 3.5 (Solution space). Given BK, (cid:55)→+ and (cid:55)→−, we deﬁne the
solution space SBK,→+,→− ⊂ PBK to be composed of complete programs
whose target functions satisfy both →+ and →−.

We now formulate the program synthesis problem.

Deﬁnition 3.6 (Program Synthesis problem). Given:

• a set of positive input/output examples,

• a set of negative input/output examples,

• background knowledge BK

the Program Synthesis problem is to ﬁnd a solution S ∈ SBK,(cid:55)→+,(cid:55)→− that has
the minimum number of functions (textually optimal solution).

3.2

Invention and reuse

For this section, suppose A is an algorithm that solves the Program Synthesis

problem. First we formalize the concepts of invention and reuse, which we
mentioned in chapters 1 and 2.

Deﬁnition 3.7 (Invention). We say that A can invent functions if at any
point during the synthesis process it is able to ﬁll a hole with a fresh function

name (i.e. does not appear in any of the previous deﬁnitions).

Deﬁnition 3.8 (Reuse). We say that A can reuse functions if at any point
during the synthesis process it is able to ﬁll a hole with a function name that

has been invented at some point during the synthesis process (be it deﬁned

or yet undeﬁned).

19

As we can see, the two deﬁnitions are intertwined: we can not have reuse

without invention. The motivation for inventing functions is that this creates

modular programs, which naturally support function reuse. As we shall see

in the next chapter, one of the main consequences with modularity is its
eﬀect on type based pruning techniques.

When function reuse is used, certain problems will beneﬁt from this (such

as droplasts from chapter 2): we could ﬁnd solutions closer to the root, which

can have noticeable eﬀects on the computation time. However, enabling func-
tion reuse means that the BK increases with each newly invented function,

and hence the branching factor of the search tree increases dynamically: in

the end, function reuse can be seen as a trade-oﬀ between the depth and the

branching factor of the search tree; this will beneﬁt some sorts of problems,
but for others it will just increase the computation time. The concerns we

talked in this paragraph are related to the research questions posed in section

1.2, which we will address in the next two chapters.

20

Chapter 4

Algorithms for the program
synthesis problem

As previously presented, we want to create an algorithm that is able to create

modular programs and hence able to reuse already induced functions. Two

of the main aims of such algorithms should be soundness and completeness,
which we deﬁne next (assume BK, (cid:55)→− and (cid:55)→+ are given).

Deﬁnition 4.1 (Completeness). We say an algorithm that solves the pro-
gram synthesis problem is complete if it is able to synthesize any program in
SBK,(cid:55)→+,(cid:55)→−.

Deﬁnition 4.2 (Soundness). We say an algorithm that solves the program
synthesis problem is sound if the complete programs it synthesizes have their
target function satisfy (cid:55)→+ and (cid:55)→+.

Motivated by Q2 from section 1.2, we are interested in another property

of such algorithms, namely type pruning: we wish to discard undesirable

programs based on their types (e.g. when they become untypable). As we

shall see, this third property will lead to the creation of two algorithms, but
next we present some preliminaries required for understanding them.

21

4.1 Preliminaries

4.1.1 Target language and the type system

We have chosen the target language to be a λ-like language that supports

contexts, since we don’t want to introduce too much added complexity, while
still having enough expressivity. Its syntax can be seen in ﬁgure 4.1. For sim-

plicity, when we provide code snippets in this language we will adopt a slightly

simpler (but equivalent) notation: for example, val f = map ( Variable g ) will

be written as f = map g. The language supports both recursive and non-
It also has a number
recursive deﬁnitions for the background knowledge.

primitives such as +, ==, nil or (:) (the last two let us work with lists).

To support type based pruning, our language will be fully typed, the

typing rules being shown in ﬁgure 4.2 (for brevity we have omitted the typing
rules for primitives, apart for nil and (:)). We deﬁne some standard type

theoretic terms we will use later:

• typing environment: Usually denoted using Γ, it represents a map that
associates a type (quantiﬁed or not) to a name or a type variable.

• substitution: This represent a way to replace existing type variables
in an unquantiﬁed type with other types; those can also be applied to

typing environments (i.e. mapping the application of the substitution

over all the types in the environment).

• type inference: The process of inferring the type of an expression, given

some typing environment.

• free variable: The free variables of a type or environment are those
typing variables which are not bound; we use the notation ftv (...) to

denote the set of free variables.

• generalizing: We can generalize over a type by closing over the free

variables of that type.

22

(cid:104)decl (cid:105) ::= ‘val’ (cid:104)ident(cid:105) ‘=’ (cid:104)expr (cid:105) – non recursive deﬁnition

|
|
|

‘rec’ (cid:104)ident(cid:105) ‘=’ (cid:104)expr (cid:105) – recursive deﬁnition
‘Pex’ (cid:104)expr (cid:105) ⇒ (cid:104)expr (cid:105) – a way to specify positive examples
‘Nex’ (cid:104)expr (cid:105) ⇒ (cid:104)expr (cid:105)– a way to specify negative examples

(cid:104)expr (cid:105) ::= ‘Num’ n
‘Char’ c
‘True’
‘False’
‘Variable’ (cid:104)ident(cid:105)
‘Lambda’ [(cid:104)ident(cid:105)] (cid:104)expr (cid:105)
(cid:104)expr (cid:105) (cid:104)expr (cid:105)
‘If’ (cid:104)expr (cid:105) ‘then’ (cid:104)expr (cid:105) ‘else’ (cid:104)expr (cid:105)’
i – we need to represent holes in the syntax

|
|
|
|
|
|
|
|

Figure 4.1: BNF Syntax

• instantiating: We can instantiate a quantiﬁed type by replacing all
the bound variables in it with fresh type variables (and hence make it
unquantiﬁed).

• uniﬁcation: Given two types, uniﬁcation is the process that yields a
substitution that when applied to the types makes them equal; we will

use the function unify to denote this process (which can fail; when
we write unify(τ1, τ2) in a condition, we implicitly mean ”if they can
unify”).

4.1.2 Combinatorial search

Most systems that have a generate-and-test or hybrid approach use some

form of combinatorial search as a means to ﬁnd new programs. Metagol uses
iterative deepening depth ﬁrst search (IDDFS), MagicHaskeller uses BFS

etc. It is natural then that we shall also use a form of combinatorial search,

more speciﬁcally we use IDDFS. We have chosen this approach because we

want to synthesize programs that gradually increase in size, so as to ensure

23

n ∈ Z
Γ (cid:96) N um n : Integer

(TNum)

c ∈ {(cid:48)a(cid:48), ...(cid:48)z(cid:48),(cid:48) A(cid:48)...(cid:48)Z (cid:48),(cid:48) 0(cid:48), ...,(cid:48) 9(cid:48)}
Γ (cid:96) Char c : Character

(TChar)

-
Γ (cid:96) T rue : Boolean

(TTrue)

-
Γ (cid:96) F alse : Boolean

(TFalse)

x : τ ∈ Γ
Γ (cid:96)Variable x : τ

(TVar)

Γ (cid:96) e0 : τ → τ (cid:48)

Γ (cid:96) e1 : τ

Γ (cid:96) e0 e1 : τ (cid:48)

(TApp)

Γ (cid:96) e : Boolean

Γ (cid:96) e(cid:48) : τ

Γ (cid:96) If e then e(cid:48) else e(cid:48)(cid:48) : τ

Γ (cid:96) e(cid:48)(cid:48) : τ (TIf)

-
Γ (cid:96) nil : ∀α . [α]

(TNil)

Γ (cid:96) e1 : τ

Γ (cid:96) e2 : [τ ]

Γ (cid:96) (e1 : e2) : [τ ]

(TCons)

Γ, x : τ (cid:96) e : τ (cid:48)
Γ (cid:96) λx . e : τ → τ (cid:48)

(TAbs)

Γ (cid:96) e : τ

¯α /∈ f tv(Γ)

Γ (cid:96) e : ∀¯α . τ

(TGen)

Γ (cid:96) e : τ

α is an instantiation of τ (TInst)

Γ (cid:96) e : α

α (cid:54)∈ f tv(Γ)

Γ (cid:96) i

: ∀α . α

(THole)

Figure 4.2: Typing rules for expr

24

that the induced program will be the shortest one in terms of the number of

synthesized functions.

4.1.3 Relations concerning the program space

We ﬁrst formalize what was meant by “cyclical deﬁnitions” in the deﬁnition
of the program space.

name appears in fbody .

Deﬁnition 4.3 (Name usage). We say that a function f directly uses another
function f (cid:48) if f (cid:48)
We say that a function f uses another function f (cid:48) if f directly uses f (cid:48) or f
uses g and g directly uses f (cid:48) (transitive closure of directly uses).
We say that a program is acyclic if no function in P uses itself, or cyclic

otherwise.

Intuitively, if a complete program is cyclic, then it might not terminate

(although this is not always the case, e.g. programs that include co-recursive

functions), so we want to avoid such programs to not introduce non termi-
nation.

Deﬁnition 4.4 (Specialisation step). Given a function f , a typing environ-
ment Γ, the specialisation step is a relation on Expr ×(Expr , TypingEnvironment)

indexed by a typing environment deﬁned by the following rule:

• if

1. a hole i appears in fbody;

2. g ∈ BKI ∪ BKF and g does not use f OR g is a fresh function we
invent and add to BKI, whose type in Γ is a fresh type variable
(use an existing function or invent);

3. ρ = unify(τ, Γ(gname)), where τ is the type inferred for i

in the

type derivation tree for fbody;

then we write fbody
the substitution ρ to Γ).

Γ−→ (fbody[ gname / i ], ρ Γ) (note, ρ Γ means we apply

25

We brieﬂy give some intuition on why the environment is updated. What

we do in step 3 is we try to mimic the TApp typing rule. Since the type of the

hole we are to ﬁll represents a “minimum requirement” for what types can ﬁll

it (see the THole rule), it suﬃces to make sure the type of the ﬁller function is
uniﬁable with the type of the hole. Now, because of our top-down approach

to typing, we keep the types of the invented functions unquantiﬁed, since the

types of those functions are intertwined. Hence, changes we make in one can

lead to changes in other functions’ types, so we need to apply the substitution
to the whole environment (note that both the background functions and the

higher-order combinators are quantiﬁed, so this won’t aﬀect them).

Based on this relation, we give an ordering on pairs of induced programs

and their typing environment (we will call those pairs program states).

Deﬁnition 4.5 (Ordering). We say that a program state (P, Γ) is more
concrete than another program state (P (cid:48), Γ(cid:48)) if either

• (specialize) names(P ) = names(P (cid:48)) and there exist exactly two func-

tions f ∈ P and f (cid:48) ∈ P (cid:48) such that

1. fname = f (cid:48)
Γ−→ (f (cid:48)

2. fbody

name
body, Γ(cid:48))

• or (deﬁne) P (cid:48) = P ∪ {fname = T }, with

1. f used by another function but not yet deﬁned in P

2. T ∈ BKT

3. ρ = unify(τ, Γ(fname)) (where τ is the type that can be inferred

for the template T ) and Γ(cid:48) = ρ Γ.

We write this as (P, Γ) (cid:52) (P (cid:48), Γ(cid:48)).

Here, step 3 in the deﬁne rule makes sure that the type of the template

“agrees” with all the constraints collected from other previous uses of the to
be deﬁned function (and again we need to update the whole environment).

Let (cid:52)∗ be the reﬂexive, transitive closure of (cid:52), deﬁned by:

26

• P = P (cid:48) ⇒ P (cid:52)∗ P (cid:48)

• ∃P (cid:48)(cid:48).P (cid:52)∗ P ” ∧ P ” (cid:52) P (cid:48) ⇒ P (cid:52)∗ P (cid:48).

4.1.4 Partitioning the templates

Before continuing with the algorithm, we will divide the templates in two

categories, based on their type signatures.

Deﬁnition 4.6 (Linear function templates). We deﬁne a linear function

template to be a template where the types of the functional inputs of the

associated higher-order function (the function that is applied in the template)

share no type variables.

Example 4.1 (Linear function templates)

The map template is a linear function template. The type of the associated

combinator is ∀ab.(a → b) → [a] → [b], so the only input (the function we
are mapping) has type a → b, hence it is trivially a linear function template.

Deﬁnition 4.7 (Branching function templates). We deﬁne a branching func-
tion template to be a template where the types of the functional inputs of

the associated higher-order function share type variables.

Example 4.2 (Branching function templates)
The composition template is a branching function template.

Its type is

∀abc.(b → c) → (a → b) → a → c, so we have two functional inputs to

the template, whose types share a type variable, namely b.

4.2 The algorithms

We will now present two algorithms that solve the program synthesis problem

and focus on synthesizing modular programs that allow function reuse to be

employed.

The reason we show two such algorithms is because we wish to explore

Q2 from section 1.2 here:

27

• we will show that when only linear templates are considered, there
exists a sound and complete algorithm that is capable of eﬀective type

pruning.

• we show how the addition of branching function templates breaks the
“naive” algorithm’s completeness, which highlights that “natural” type

pruning techniques do not transpose well when general background

knowledge is used (and hence it is not a trivial task).

4.2.1 Only linear function templates algorithm

We begin with the algorithm that uses only linear function templates. We

assume that the BK for the remainder of the subsection will only contain
linear templates. Also assume we are given the relations (cid:55)→+ and (cid:55)→−.

The core parts of the algorithm are 3 procedures that work together:

programSearch, expand and check. programSearch does the actual search

and represents the entry point for the algorithm; expand and check are both

used in programSearch to generate new programs and to check whether a
certain program is a solution, respectively.

We now describe each one in more detail:

• programSearch: this procedure does a depth bounded search; it takes
3 inputs: a procedure that expands nodes in the search tree (ex-

pand ), a function that checks whether the current node is a solution

(check ) and an initial program state. The initial state is deﬁned to be
(Pempty, Γempty), where Pempty is the empty set and Γempty is the typing
environment that contains the (quantiﬁed ) types of the higher-order

functions used in templates and the background functions. The proce-

dure uses an auxiliary function, boundedSearch, which actually does the
search by expanding and checking nodes in a similar fashion to DFS,

but the search is conducted to a certain depth (cut-oﬀ point), which

is gradually increased. Pseudocode for this procedure can be seen in

Pseudocode 1.

28

• expand : given a program state (P, Γ), this procedure implements one
use of the (cid:52) relation to create a stream of more concrete programs.
Note that because of the typing conditions in the rules of (cid:52) it is here
that we are pruning untypable programs. To make this procedure de-
terministic, we must specify how the two rules from (cid:52) are used to
if P contains a function that has at least a hole,
create the stream:

pick that hole and ﬁll it using the specialize rule. If no hole remains,

use the deﬁne rule to deﬁne one of the previously invented but un-
deﬁned functions (the target function is the ﬁrst invented function).

The reason behind this strategy is that it makes sure we are ﬁlling

the holes as soon as possible, and hence detect untypable programs

early. Of course, when applying the specialize rule, we will try to ﬁll
the hole in all the possible ways (by either using previously invented

or background functions OR by using a freshly invented function), to

ensure determinism; similarly, when the deﬁne rule is used, we will try

to assign all the possible templates to the function we want to deﬁne.

• check : checks whether a given node (a program state) is a solution.
First, we make sure that the type of the candidate program’s target is

compatible (uniﬁable) with the type of the examples. If it is, we then
check whether the program satisﬁes the examples using an interpreter

for the language (using a program environment that contains the deﬁ-

nitions of the background functions and the higher-order functions, to

which we add the invented functions); pseudocode for this can be found
in Pseudocode 2.

We shall call this algorithm Alinear. We will prove Alinear is sound and
complete. One of the main diﬃculties is to show that our approach to typ-

ing works correctly even thought we go about it in a top-down manner (as

opposed to the usual bottom-up one): there are functions whose types we do

not fully know yet; this means that we work with partial information a lot of
the time, and by acquiring more and more constraints on the types we reach

the ﬁnal ones. In contrast, a normal type inference algorithm knows the full

types of a program’s function.

29

Pseudocode 1 progSearch

procedure progSearch(expand, check, init)

Output: An induced program

for depth = 1 to ∞ do

result ← boundedSrc(expand, check, init, depth)
if result (cid:54)= f ailure then

return result

end if

end for
end procedure

Pseudocode 2 check

procedure check(progEnv, exam, progState)

Output: True if the program satisﬁes the examples, false otherwise

target ← getT argetF unction(progState)

(cid:46) the fn. that must satisfy

exam

if not compatible(exam.type, target.type) then

return f alse

end if
for def ∈ progState.complete do

addDef (progEnv, def )

end for
for pos ∈ exam.positive do

(cid:46) add complete defn. to env

if eval(progEnv, (apply(target.body, pos.in))) (cid:54)= pos.out then

return f alse

end if

end for
for neg ∈ exam.negative do

if eval(progEnv, (apply(target.body, neg.in))) = neg.out then

return f alse

end if

end for
return true
end procedure

30

Lemma 4.1. Let P(cid:52) = { P |(Pempty, Γempty) (cid:52)∗ (P, Γ) ∧ Γ is a consistent
typing environment for P }. Then we have P(cid:52) = PBK (where BK only
contains linear templates).

Proof. We do so by double inclusion.

⊆ : We need to show that if P ∈ P(cid:52) then P ∈ PBK. This follows from

the rules of (cid:52), since the programs in P(cid:52):

– only use functions from BKI and BKF to ﬁll holes, and templates

from BKT when assigning templates;

– are not cyclical (see the second condition in the specialization step

relation);

– they are always well typed:

this can be proven using a short

inductive argument. The empty state is well typed because it

contains the empty program. Now, suppose we have reached a

program state (P, Γ), where P is typable wrt. Γ and we have
(P, Γ) (cid:52) (P (cid:48), Γ(cid:48)). If this resulted via the deﬁne rule, P (cid:48) must be
typable wrt. Γ(cid:48) by how Γ(cid:48) was deﬁned: the to be deﬁned function
takes all the constraints that were created from using it in other

deﬁnitions, and makes sure that the template we apply is com-
patible with them. Now, if the specialize rule was used, we only

replace a hole when the function we ﬁll it with has a uniﬁable type

with the type that can be inferred for the hole (we essentially use
the TApp rule). So P (cid:48) must be typable.

The highlighted points correspond to the conditions a program must
satisfy to be in PBK.

⊇ : we must prove that if P ∈ PBK then P ∈ P(cid:52). Since P ∈ PBK, we
know it is typable wrt. a typing environment Γ. Now, observe that
if we pick a function name that occurs in P , and replace that with

a hole, the resulting program must be typable (by the THole rule)

wrt. a typing environment (1). Also, when the body of a function

is a template which has no ﬁlled holes, we can delete that deﬁnition

31

and still have a typable program (the function’s name will still be in

the typing environment, but its type becomes more general because we

removed a constraint) (2). Those two actions can be seen as the reverse

of the deﬁne and specialize rules. Now, consider the following process:
pick a complete function; replace each function name with a hole, until

we are left with a template that contains only holes; then delete that

function; repeat the process again.

It is clear that this process will

eventually lead to the empty program, since at each step we either
delete a function, or get closer to deleting a function. Using (1) and

(2), we know that all the intermediary programs will be typable, wrt.

some typing environments. Furthermore, each of those programs will
be acyclical and will only contain information from BKI and BKF .
This means that our process creates a (cid:52)-path in reverse, say (P, Γ) (cid:60)
(P1, Γ1) (cid:60) · · · (cid:60) (PN , ΓN ) (cid:60) (Pempty, Γempty), for some states (Pi, Γi),
since what we have essentially done is apply the two rules of (cid:52) in
reverse. We do need to make the following note though: this reverse
constructions works when we consider only linear templates, because

those guarantee that no undesirable side eﬀects occur when a hole is
ﬁlled, so we can construct a (cid:52)-path starting at either end. Hence, we
have that P ∈ P(cid:52).

From the above cases, we have the conclusion.

Theorem 4.1. Alinear is sound and complete when considering only linear
templates.

Proof. By lemma 4.1 and since expand implements the (cid:52) relation (and hence
synthesizes all programs in P(cid:52)), we have that expand produces all programs
in PBK (note that it does not matter that expand uses the rules in a speciﬁc
order, since the proof of lemma 4.1 did not consider a speciﬁc application or-
der). Since SBK,(cid:55)→+,(cid:55)→− ⊂ PBK and check is used on all programs synthesized
by expand, we are certain that we will be able to synthesize all functions in
SBK,(cid:55)→+,(cid:55)→− (1). Furthermore, since a program is the output of the algorithm

32

only if it satisﬁes the examples (enforced by check ), this means that the pro-

grams we synthesize are indeed solutions (2). From (1) and (2) we have that

the algorithm is sound and complete.

Hence, From theorem 4.1 we have that Alinear solves the program synthe-

sis problem.

Motivated by question Q2 from section 1.2, in this subsection we have
focused on creating an algorithm that employs eﬀective and early type based

pruning. We have seen that if linear templates are used, the pruning tech-

nique is a “natural” extension of what a normal type inference system would

do. While only using linear templates might seem restrictive, we note that
templates such as map, ﬁlter, fold (with the type of the base element a base

type, so that we don’t have shared type variables) are all linear templates,

and they can express a wide range of problems.

We make one last observation here. One can notice that expand does
not care about the types of the examples being compatible with the type of

the incomplete programs’ target type (this is done in check ). To increase

the amount of pruning, we can make expand discard those programs whose

target type does not agree with the type of the examples. We observe that
this does not break the completeness of the algorithm, since the programs

we wish to synthesize are in the solution space, and hence must have their

target’s type compatible with the type of the examples.

4.2.2 Consequences of adding branching templates

We now investigate what eﬀect branching templates have on Alinear. We ﬁrst
prove the next important result.

Theorem 4.2. Alinear is no longer complete when its input contains branch-
ing function templates.

Proof. We will show that Alinear is unable to synthesize the following prob-
lem: given a list of lists, reverse all inner lists as well as the outer list. Suppose

that the available function templates are map, composition and identity and

that the only background function is reverse. Given those, we should be able

33

to infer the following program:

target = gen1 . gen2

gen2 = map gen1

gen1 = reverse

Since we have ﬁxed the order in which we deﬁne functions and ﬁll the holes

(see the description of expand ), the following derivation sequence will happen

on the path to ﬁnding the solution.

1. target = 1 . 2 (we have no holes to ﬁll, deﬁne the target function)

2-3. target = gen1 . gen2 (before inventing any other function ﬁll the 2

existing holes)

4. target = gen1 . gen2, gen2 = map 3 (all holes ﬁlled, deﬁne gen2)

5. target = gen1 . gen2, gen2 = map gen1 (before inventing any other

function ﬁll the existing hole)

...

After step 1, because (.) is applied to the two holes, we infer that the types

of 1 and 2 must be of the form b → c and a → b, respectively. In steps 2-3,
since we invent the functions gen1 and gen2, their types will be b → c and

a → b (since they don’t have a deﬁnition, and are also not used elsewhere,

they just take the types of the holes they ﬁlled). In step 4, the type of gen2

will change: we must then unify gen2 ’s type with that of the map template
([d] → [e]). After this uniﬁcation takes place, gen2 ’s type will be [d] → [e],

the type that can be inferred for 3 is d → e, but gen1’s type will also change

(since it shares a type variable with gen2’s type) to [e] → c. Now, in step 5,

we need to unify gen1 ’s type with the type of 3 , and we will get [e] → e for
gen1. Now, since we know that the type of reverse is of the form [f ] → [f ],

it is clear that the deﬁnition gen1 = reverse is impossible, since gen1 ’s type

is incompatible with reverse’s type. We conclude the program we considered

can not be synthesized and hence the algorithm is no longer complete.

34

The problem here is caused by combining general polymorphism with

branching templates. The fact that we are trying to type programs progres-

sively, in top-down manner, creates the possibility of constraining types too

early. An example can be seen in the previous proof: gen1 and gen2 ini-
tially shared a type variable, because we eagerly adjusted their types to ﬁt

with the type of the composition template; this, in turn, lead to the type of

gen1 being ultimately incompatible with the type of reverse. Normal type

inference can deal with this problem because it fully knows the types of all
functions (whereas we are progressively approaching those ﬁnal types).

Our attempt to solving this involved transforming the branching tem-

plates into linear templates and generating uniﬁcation constraints on rele-

vant type variables to correctly deal with polymorphism. For example, take
the composition template, whose “branching” type is (b → c) → (a → b) →
(a → c); the new “linear” type would be (b1 → c) → (a → b0) → (a → c),
with the constraint that b0 and b1 are uniﬁable. The idea here was that this
gives us some “wiggle room” when deciding the types (and hence potentially
ﬁxes the problem introduced by polymorphism). However, we were unable

to use this approach to develop an algorithm we were certain was sound and

complete. After this attempt, we managed to ﬁnd two papers that talk about

typing a lambda like language with ﬁrst class contexts, by Hashimoto and
Ohori [10] and by Hashimoto [9]. The former paper provides a theoretical

basis by creating a typed calculus for contexts, while the latter develops an

ML-like programming language that supports contexts and furthermore pro-

vides a sound and complete inference system for it. This suggests that there
might be a way to have meaningful type pruning when branching templates

are allowed, but we will reserve exploring this avenue for future work.

4.2.3 An algorithm for branching templates

We now propose an algorithm that is complete and sound when considering
branching templates, which we will call Abranching. Motivated by the obser-
vations shown in the last subsection, we will completely disregard early type
pruning for the purposes of this algorithm. Abranching is similar in structure

35

to Alinear, but it defers type checking until after we synthesize a complete
program (no early type pruning):

• progSearch will remain the same.

• expand will follow the same ﬁlling and deﬁning strategy as before and
make sure the synthesized programs are acyclical, but will now com-
pletely disregard anything type related.

• check must now have an additional step, checking whether the program
is indeed typable (using a “normal” inference algorithm based on the
typing rules of the language) before checking for example satisﬁability.

It is easy to see that this algorithm is sound (because of the extra check

in check ) and complete (expand produces every possible program that is not
cyclical, which is clearly a superset of the solution space), and hence solves

the program synthesis problem. For the experiments involving reuse in the

next chapter we shall use an implementation of this algorithm, because, as we

will see in the next chapter, branching templates are important for eﬀective
function reuse.

36

Chapter 5

Experiments and results

In this chapter, through experimentation, we will attempt to answer ques-

tions Q1, Q3 and Q4 from section 1.2, which we reiterate:

Q1 Can function reuse improve learning performance (ﬁnd programs faster)?

Q3 What impact does the grammar of the synthesized programs have on

function reuse?

Q4 What classes of problems beneﬁt from it?

The implementation we shall use was written in Haskell and closely follows
Abranching, whose outline can be found in Appendix A. We focus on this
algorithm’s implementation because, as we will see in section 5.2, using only
linear templates makes it almost impossible to reuse functions.

5.1 Experiments

We begin by showcasing the experiments we conducted in order to answer
Q1. For simplicity (and since they have enough expressive power for our

purpose), we will use three templates: map, ﬁlter and composition. The

results of the experiments are summarised in tables 5.1 (output programs)

and 5.2 (learning times); the times shown are of the form [mean over 5
repetitions] ± standard error.

37

5.1.1

addN

We begin with a simple yet interesting example.

Problem: Given a number, add N to it.

Method: We will be considering this problem for N ∈ {4, 5, . . . , 10}.

The only background function we use is add1. Example wise, we will
use 2 positive examples of the form x →+ x+N and 2 negative examples
of the form x →− x + M , with M (cid:54)= N .

Results: Figure 5.1 plots the mean of the learning times for diﬀerent
values of N (5 repetitions). As we can see, function reuse is vital

here: by creating a function that adds two, we can reuse it to create a

function that adds 4, and so on; this means that there is a logarithmic

improvement in program size from the no reuse variant, as can be seen
in table 5.1 (which shows the solution for add8 ), which in turn leads to

an increase in performance, as can be seen in table 5.2. Something to

note is that for N = 16, if reuse is used, the solution is found in under

a second, whereas if reuse is not used no solution is found even after
10 minutes. The result here suggest that the answer to question Q1 is

yes.

5.1.2 ﬁlterUpNum

Problem: Given a list of characters, remove all upper case and numeric

elements from it.

Method: We use the following background functions: isUpper, isAl-

pha, isNum, not and will use 2 positive and 2 negative examples.

Results: As can be seen in the table 5.1, this is a problem where
only function invention suﬃces, and reuse shows no improvement wrt.

program size. However, this is a good example that shows that, for pro-

grams which have a reasonably small number of functions, reuse does

38

Reuse
No reuse

)
c
e
s
(

e
m

i
t

i

g
n
n
r
a
e
l

40

30

20

10

0

4

5

6

7
N

8

9

10

Figure 5.1: Learning times for addN

not introduce too much computational overhead:
in our case it dou-
bles the execution time, but this is not noticeable since both execution

times are under half a second.

5.1.3

addRevFilter

Problem: Given a list of integers, add 4 to all elements, ﬁlter out the
resulting even elements and reverse the new list.

Method: The background functions used are: add1, add2, isOdd, re-

verse; we use 2 positive examples and 2 negative examples.

Results: Again, in this case function reuse does not lead to a shorter

solution. However, in this case there is a noticeable increase in the

execution time when reuse is used, from around 2 seconds to around
9 seconds. Another interesting observation here is that, while both

programs in table 5.1 are the smallest such programs (given our BK),

the one that employs reuse is actually less eﬃcient than the one that

only uses invention: one maps add2 twice over the list, whereas the
other one creates a function that adds 4 and then maps that over the list

39

once. The result here, together with the result in ﬁlterUpNum suggest

the following about Q1: while function reuse could be very helpful in

some situations, sometimes it will not help in ﬁnding a shorter solution;

furthermore, while in some cases the computational overhead is not too
noticeable, in others the overhead will be quite sizeable.

5.1.4 maze

Problem: Given a maze that can have blocked cells, a robot must ﬁnd

its way from the start coordinate to the end coordinate.

Method: The background functions used represent the basic move-

ments of the robot: mRight, mLeft, mDown, mUp (if the robot tries

to move out of the maze, ignore that move). The mazes we will con-
sider will be 4x4, 6x6 and 8x8; the start will always be cell (0, 0) and

the goals (3, 3), (5, 5) and (7, 7), respectively. We will use one pos-

itive example and no negative examples (no need for them in such a

problem).

Results: Reuse has a dramatic eﬀect on the learning times, as can be

seen in table 5.2 (for the 4x4 problem). Interesting here are the 6x6 and
8x8 variants, since when enabling reuse we managed to ﬁnd solutions

for both in under 10 seconds, but when reuse was not employed, the

system was not able to produce results even after 10 minutes. The

result here enforces our previous assertion about question Q1: when
reuse is applicable, it can make a big diﬀerence.

5.1.5 droplasts

Problem: Given a list of lists, remove the last element of the outer

list as well as the last elements of the inner lists.

Method: The background functions we use are reverse, tail, addOne,

addTwo, isOdd, id (the latter 4 functions are noise, to put stress on

the system).

40

Results: From the formulation, we can get a sense that tail combined

with reverse will represent the building block for the solution, since

intuitively this operation would need to be performed on both the outer

list as well as on the inner lists. Indeed, the solution that uses function
reuse is both shorter and it is found much faster than the no reuse

variant. As we can see, reuse has had a major impact here, drastically

reducing the computation time (as can be seen in table 5.2).

Curious about how the system (using reuse) would behave when vary-

ing the number of background functions, we have conducted a few more

experiments to test this. To make it challenging, we have only retained
reverse and tail, and all the functions we added were the identity func-

tions (with diﬀerent names), so even if type based pruning would be

used, it would not really make a diﬀerence. The results can be seen

in ﬁgure 5.2 (we plotted the means of 3 executions ± the standard
error). The results here enforce our previous assertion about question

Q1, solidifying our belief that reuse is indeed useful, while also show-

ing that the system behaves respectably when increasing the number

of background functions.

5.2 On function reuse and function templates

After attempting to answer Q1 in the previous subsections, we now con-

sider Q3 and Q4. As previously discussed, function reuse does not come

without a cost: in some cases it negatively aﬀects the execution time. Fur-

thermore, it is clear that not all programs take advantage of function reuse,
so an answer to question Q4 is quite important.

We have been able to distinguish two classes of problems that beneﬁt from

function reuse: problems that involve repetitive tasks (especially planning in

the AI context) and problems that involve operations on nested structures.
For the former class, we can give the maze problem as an example. In that

case, function reuse lead to the creation of a function that was equivalent

to moveRight then moveUp, which helped reach shorter solutions because

41

)
c
e
s
(

e
m

i
t

i

g
n
n
r
a
e
l

25

20

15

10

5

0

2

4

6
# of BK functions used

8

10

Figure 5.2: Learning times for droplasts (with reuse)

the robot used this combination of moves frequently. For the latter class,

droplasts is a perfect illustration. The solution acts on both the inner and

outer lists, which is a good indication that the operation might be repetitive

and hence beneﬁt from the reuse of functions. We note that both classes of
programs we presented contain quite a lot of programs (lots of AI planning

tasks can be encoded in our language and tasks that act on nested structures

are common), which is a good indication that reuse is applicable and can

make a diﬀerence in practical applications.

Another interesting point (raised by Q3) is how the presence of vari-

ous function templates (which induce the grammar of our programs) aﬀects

function reuse, and whether the partition we have presented in the previous

chapter plays a part in this. If we think about the graph the uses relation
induces on the invented functions (call it a functional dependency graph, or

FDG), the programs our algorithms synthesize have acyclic FDGs, because

we never introduce cyclical deﬁnitions (see deﬁnition 4.2). Now, the fact

that the types of the holes do not share type variables for linear templates
suggests that in practice the majority of those templates are actually likely

to have one single hole.

If this is the case, this means they create linear

42

Problem

Reuse + Invention

add8

g3 = add1.add1
gen2 = g3.g3
target = g2.g2

ﬁlterUpNum

addRevFilter

maze(4x4)

g3 = ﬁlter isAlpha
g4 = not.isUpper
g2 = ﬁlter g4
target = g2.g3
g5 = g4.reverse
g4 = map add2
g3 = g4.g5
g2 = ﬁlter isOdd
target = g2.g3

g3 = mRight.mUp
g2 = g3.g3
target = g2.g3

dropLasts

g4 = reverse.tail
g3 = g4.reverse
g2 = map g3
target = g2.g3

Only invention
g3 = add1.add1
g5 = add1.add1
g7 = add1.add1
g6 = add1.add1
g4 = g6.g7
g2 = g4.g5
target = g2.g3

same as R + I

g5 = add2.add2
g4 = map g5
g3 = g4.reverse
g2 = ﬁlter isOdd
target = g2.g3
g3 = mRight.mUp
g5 = mRight.mRight
g4 = mUp.mUp
g2 = g4.g5
target = g2.g3
g6 = reverse.tail
g3 = g6.reverse
g5 = reverse.tail
g4 = g5 reverse
g2 = map g4
target = g2.g3

Table 5.1: Programs output for the experimental problems

FDGs, which make function reuse impossible (otherwise, the FGD would

be cyclic). Indeed, we can make the following remark: to enhance function

reuse, branching templates should always be used. In particular, composition
and other similar branching templates that encapsulate the idea of chaining

computations are very eﬀective: they create branches in the FDG, and a

function invented on such a branch could be reused on another one.

43

Problem
add8

Reuse + Invention
13.34 ms ± 0.57
ﬁlterUpNum 338.29 ms ± 11.16
addRevFilter
maze(4x4)
dropLasts

9.11 sec ± 0.06
67.50 ms ± 4.85
1.84 sec ± 0.01

Only invention
1.18 sec ± 0.01
153.90 ms ± 4.57
1.97 sec ± 0.01
5.37 sec ± 0.05
252.24 sec ± 7.16

Table 5.2: Learning times for the experimental problems

44

Chapter 6

Conclusions

6.1 Summary

This project was motivated by the fact that, to the best of our knowledge,

invention and in particular function reuse has not been properly researched
in the context of inductive functional programming.

In chapter 3, we formalized the program synthesis problem, which pro-

vided us with the language necessary to create algorithms. Chapter 4 repre-

sents an important part of the project, since that is where we have presented
two approaches to solving the program synthesis problem, namely Alinear
(which works with a speciﬁc type of background knowledge) and Abranching
(which works on general background knowledge). An interesting result we
found was that the form of type pruning Alinear uses, which relies on a normal
type inference process (extended in a natural way to work with contexts),

breaks its completeness if general background knowledge is used: hence, we

observed that type pruning is not a trivial task when synthesizing modular
programs. In chapter 5 we have relied on the implementations of Abranching
to show a variety of situations where function reuse is important: examples

such as droplasts, add8 and maze showed how crucial it can be. We have also

distinguished two broad classes of programs that will generally beneﬁt from

function reuse and discussed the impact the used background knowledge has
on reuse.

Overall, we have seen that there is value in exploring the ideas of modular

45

programs and function reuse, and believe that this project can serve as the

base for future work in this direction.

6.2 Reﬂections, limitations and future work

The project is limited in a few ways and can be further enhanced. One
major limitation is the lack of any form of pruning for Abranching (the algo-
rithm that works with linear templates beneﬁts from type based pruning).

As we have stated in chapter 4, a possible way to overcome the problems of
typing with contexts could be solved by attempting to create a type system

and type inference system similar to the ones described in [10] and [9]. Fur-

thermore, a possible extension of the project could examine the beneﬁts of
pruning programs through example propagation, in a similar way to how λ2
does it [6]. An interesting point to explore is whether branching templates

would hinder this approach to pruning in any way (more speciﬁcally, whether

templates such as composition would prevent any such pruning to be done

before the program is complete). Another avenue to explore would be to see
whether there are other major classes of programs that beneﬁt from function

reuse, speciﬁcally problems related to AI and game playing.

46

Appendix A

Implementation details

We brieﬂy give some implementation details for the algorithm Abranching. The
implementation can be found at https://github.com/reusefunctional/reusefunctional,

which also contains details on how to run the system.

A.1 Language implementation

The language that we have presented in chapter 4 is very similar to the

language Fun, presented in the Oxford PoPL course [22]. Hence, we have
used the parser and lexer, together with parts of the interpreter for our

implementation, but those have been extended in multiple ways. We have

added types to the language and added a type inference system (which can

be found in the ﬁles Types.hs and Infer.hs). The inference system follows
classical algorithms, and a similar implementation by Stephen Diehl can be

found in [4], which we have used as a guide. To support the synthesis process

inside the language, we have added three constructs to the language presented

in chapter 4.

Listing A.1 shows the test ﬁle used for the add problem mentioned in

chapter 5, which highlights most features of the language. Note that the ﬁrst

three functions represent the implementation of the higher order functions

used in the templates (this is part of our idea that templates should be easy
to modify) and the fourth is a background function (hence the BK preﬁx).

Listing A.1: add8 test ﬁle

47

v a l comp ( f , g ) = lambda ( x )

f ( g ( x ) )

; ;

r e c map( f ) = lambda ( xs )

( i f xs = n i l
then n i l
e l s e f ( head ( xs ) ) :map( f ) ( t a i l ( xs ) ) )

; ;

r e c

f i l t e r ( p ) = lambda ( xs )
( i f xs = n i l
then n i l
e l s e

( p ( head ( xs ) ) )

i f
then head ( xs ) : f i l t e r ( p ) ( t a i l ( xs ) )
e l s e f i l t e r ( p ) ( t a i l ( xs ) ) )

; ;

v a l BK addOne( x ) = x + 1 ; ;

NEx ( 1 ) => 2 ; ;
NEx ( 3 ) => 5 ; ;
PEx ( 1 ) => 9 ; ;
PEx ( 7 ) => 15 ; ;
S y n t h e s i z e ( Int ) => Int ; ;

A.2 Algorithms implementation

The implementation of the searching algorithm closely follows the algorithm

described in chapter 4 and can be found in the ﬁle Search.hs. The uses rela-

tion and the cycle check is done by creating a dependency graph and checking
for cycles when adding edges during the creation of declarations (DepGraph).

The check function has been implemented with the help of an interpreter for

our language (Interpreter.hs). To have the induced functions in the right

order when deﬁning them for the purpose of checking the examples, we use
the topological ordering of the dependency graph’s nodes (which denote the

functions) to make sure a function can only be deﬁned once all the functions

that appear in its body are also deﬁned.

48

References

[1] Andrew Cropper. Eﬃciently learning eﬃcient programs. PhD thesis,

Imperial College London, UK, 2017.

[2] Andrew Cropper, Rolf Morel, and Stephen H. Muggleton. Learning

higher-order logic programs. CoRR, abs/1907.10953, 2019.

[3] Andrew Cropper and Sophie Tourret. Logical reduction of metarules.

CoRR, abs/1907.10952, 2019.

[4] Stephen

Diehl.

Hindley-Milner

Inference.

http://dev.stephendiehl.com/fun/006 hindley milner.html.

[5] Richard Evans and Edward Grefenstette. Learning explanatory rules

from noisy data. CoRR, abs/1711.04574, 2017.

[6] John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data

structure transformations from input-output examples. In David Grove
and Steve Blackburn, editors, Proceedings of the 36th ACM SIGPLAN

Conference on Programming Language Design and Implementation,

Portland, OR, USA, June 15-17, 2015, pages 229–239. ACM, 2015.

[7] Sumit Gulwani, William R. Harris, and Rishabh Singh. Spreadsheet

data manipulation using examples. Commun. ACM, 55(8):97–105, Au-

gust 2012.

[8] Sumit Gulwani,

Jos´e Hern´andez-Orallo, Emanuel Kitzelmann,

Stephen H. Muggleton, Ute Schmid, and Benjamin G. Zorn.

Induc-

tive programming meets the real world. Commun. ACM, 58(11):90–99,

2015.

49

[9] Masatomo Hashimoto. First-class contexts in ML. Int. J. Found. Com-

put. Sci., 11(1):65–87, 2000.

[10] Masatomo Hashimoto and Atsushi Ohori. A typed context calculus.

Theor. Comput. Sci., 266(1-2):249–272, 2001.

[11] Martin Hofmann, Emanuel Kitzelmann, and Ute Schmid. A unifying

framework for analysis and evaluation of inductive programming sys-

tems. In Proceedings of the 2nd Conference on Artiﬁciel General Intel-

ligence (2009), pages 74–79. Atlantis Press, 2009/06.

[12] Susumu Katayama. Systematic search for lambda expressions. In Marko

C. J. D. van Eekelen, editor, Revised Selected Papers from the Sixth
Symposium on Trends in Functional Programming, TFP 2005, Tallinn,

Estonia, 23-24 September 2005, volume 6 of Trends in Functional Pro-

gramming, pages 111–126. Intellect, 2005.

[13] Emanuel Kitzelmann.

Inductive programming: A survey of program

synthesis techniques.

In Ute Schmid, Emanuel Kitzelmann, and Ri-

nus Plasmeijer, editors, Approaches and Applications of Inductive Pro-
gramming, Third International Workshop, AAIP 2009, Edinburgh, UK,

September 4, 2009. Revised Papers, volume 5812 of Lecture Notes in

Computer Science, pages 50–73. Springer, 2009.

[14] Mark Law, Alessandra Russo, and Krysia Broda. The ILASP system

for inductive learning of answer set programs. CoRR, abs/2005.00904,

2020.

[15] Zohar Manna and Richard J. Waldinger. A deductive approach to pro-

gram synthesis. ACM Trans. Program. Lang. Syst., 2(1):90–121, 1980.

[16] Stephen Muggleton.

Inverse entailment and progol. New Generation

Comput., 13(3&4):245–286, 1995.

[17] Stephen Muggleton. Inductive logic programming: Issues, results and

the challenge of learning language in logic. Artif. Intell., 114(1-2):283–

296, 1999.

50

[18] Stephen Muggleton, Luc De Raedt, David Poole, Ivan Bratko, Peter A.

Flach, Katsumi Inoue, and Ashwin Srinivasan. ILP turns 20 - biography

and future challenges. Mach. Learn., 86(1):3–23, 2012.

[19] Ute Schmid.

Inductive programming as approach to comprehensible

machine learning. In Christoph Beierle, Gabriele Kern-Isberner, Marco

Ragni, Frieder Stolzenburg, and Matthias Thimm, editors, Proceedings

of the 7th Workshop on Dynamics of Knowledge and Belief (DKB-2018)
and the 6th Workshop KI & Kognition (KIK-2018) co-located with 41st

German Conference on Artiﬁcial Intelligence (KI 2018), Berlin, Ger-

many, September 25, 2018, volume 2194 of CEUR Workshop Proceed-

ings, pages 4–12. CEUR-WS.org, 2018.

[20] Ehud Y. Shapiro. Algorithmic Program DeBugging. MIT Press, Cam-

bridge, MA, USA, 1983.

[21] Armando Solar-Lezama. The sketching approach to program synthe-

sis. In Zhenjiang Hu, editor, Programming Languages and Systems, 7th

Asian Symposium, APLAS 2009, Seoul, Korea, December 14-16, 2009.
Proceedings, volume 5904 of Lecture Notes in Computer Science, pages

4–13. Springer, 2009.

[22] Mike Spivey and Sam Statton.

POPL course Oxford, 2020.

https://www.cs.ox.ac.uk/teaching/materials19-20/principles/.

[23] Phillip D. Summers. A methodology for LISP program construction

from examples. J. ACM, 24(1):161–175, 1977.

51

