A Fast Spectral Algorithm for Mean Estimation with
Sub-Gaussian Rates

Zhixian Lei∗

Kyle Luh†

Prayaag Venkat∗

Fred Zhang‡

Abstract
We study the algorithmic problem of estimating the mean of a heavy-tailed random vector in Rd
,
given n i.i.d. samples. The goal is to design an eﬃcient estimator that attains the optimal sub-gaussian
error bound, only assuming that the random vector has bounded mean and covariance. Polynomial-time
solutions to this problem are known but have high runtime due to their use of semi-deﬁnite programming
(SDP). Moreover, conceptually, it remains open whether convex relaxation is truly necessary for this
problem.

In this work, we show that it is possible to go beyond SDP and achieve better computational eﬃciency.
In particular, we provide a spectral algorithm that achieves the optimal statistical performance and runs
O (cid:0)n3.5 + n2d(cid:1) by Cherapanamjeri et al.
in time
e
(COLT ’19). Our algorithm is spectral in that it only requires (approximate) eigenvector computations,
which can be implemented very eﬃciently by, for example, power iteration or the Lanczos method.

O (cid:0)n2d(cid:1), improving upon the previous fastest runtime
e

At the core of our algorithm is a novel connection between the furthest hyperplane problem intro-
duced by Karnin et al. (COLT ’12) and a structural lemma on heavy-tailed distributions by Lugosi and
Mendelson (Ann. Stat. ’19). This allows us to iteratively reduce the estimation error at a geometric
rate using only the information derived from the top singular vector of the data matrix, leading to a
signiﬁcantly faster running time.

0
2
0
2

b
e
F
7
1

]
T
S
.
h
t
a
m

[

2
v
8
6
4
4
0
.
8
0
9
1
:
v
i
X
r
a

∗Harvard John A. Paulson School of Engineering and Applied Sciences

(SEAS), Harvard University.

Email:

{zhixianlei,pvenkat}@g.harvard.edu.

†Center of Mathematical Sciences and Applications, Harvard University. Email: kyleluh@gmail.com.
‡Department of Electrical Engineering and Computer Sciences, UC Berkeley. Email: z0@berkeley.edu. Part of the work

was done at Harvard University.

1

 
 
 
 
 
 
1 Introduction

Estimating the mean of a multivariate distribution from samples is among the most fundamental statisti-
cal problems. Surprisingly, it was only recently that a line of works in the statistics literature culminated
in an estimator achieving the optimal statistical error under minimal assumptions (Lugosi and Mendelson
(2019b)). However, from an algorithmic point of view, computation of this estimator appears to be in-
tractable. On the other hand, fast estimators, such as the empirical average, tend to achieve sub-optimal
statistical performance. The following question remains open:

Can we provide simple, fast algorithm that computes a statistically optimal mean estimator in high
dimensions, under minimal assumptions?

In this paper, we make progress towards this goal, under the classic setting where only ﬁnite mean and
covariance are assumed. Formally, our problem is deﬁned as follows. Given n i.i.d. copies X1, . . . , Xn of a
µ)T , compute an
random vector X
(0, 1],
estimate

µ(X1, . . . , Xn) of the mean µ. Our goal is to show that for any failure probability δ

Rd with bounded mean µ = E X and covariance Σ = E(X

µ)(X

µ =

−

−

∈

∈

b

b

Pr (

µ

k

−

µ

k

> rδ)

δ,

≤

µ eﬃciently. The naïve estimator is
for as small a radius rδ as possible. Moreover, we would like to compute
n
simply the empirical mean µ = 1
i=1 Xi. It is well known that among all estimators, the empirical mean
n
minimizes mean squared error. However, if we instead use the size of the deviations to quantify the quality of
b
the estimator, the empirical mean is only optimal for sub-gaussian random variables (Catoni (2012)). When
X
δ,

(µ, Σ) we have with probability at least 1

P

b

∼ N

−
Tr(Σ)
n

µ

k

−

µ

k ≤ r

Σ

2

k

k

log(1/δ)
n

+

r

(1.1)

An estimator that achieves above is said to have sub-gaussian performance or sub-gaussian rate.

In practical settings, assuming that the samples obey a Gaussian distribution may be unrealistic.
In
an eﬀort to design robust estimators, it is natural to study the mean estimation problem under very weak
assumptions on the data. A recent line of works (Catoni (2012); Minsker (2015); Devroye et al. (2016);
Joly et al. (2017); Lugosi and Mendelson (2019a,b)) study the mean estimation problem when the samples
obey a heavy-tailed distribution.

For heavy-tailed distributions the performance of the empirical mean is abysmal. If we only assume that
X has ﬁnite mean µ and covariance Σ, then by Chebyshev’s inequality, the empirical mean only achieves
Tr(Σ)/δn, which is worse than the sub-gaussian rate in two ways. First, its dependence on
error of order
δ is exponentially worse. Second, the Tr(Σ) term, which may grow with the dimension d, is multiplied the
1
p
dimension-independent term

1/δn, whereas in the Gaussian case, the two are separate.

p

Median-of-means paradigm Surprisingly, recent work has shown that it is possible to improve on the
performance of the empirical mean using the median-of-means approach. For d = 1, the following con-
struction, originally due to Nemirovsky and Yudin (1983); Jerrum et al. (1986); Alon et al. (1999), achieves
sub-gaussian performance:

(i) First, bucket the data into k =

disjoint groups and compute their means Z1, Z2,

, Zk.

· · ·

(ii) Then, output the median

⌈
µ of Z1, Z2,

10 log(1/δ)
⌉
, Zk.

· · ·

A long line of work has followed this paradigm and generalized it to higher dimensions (Catoni (2012);
Devroye et al. (2016); Joly et al. (2017); Lugosi and Mendelson (2019a,b)). The key challenge is to correctly
deﬁne a notion of median for a collection of points in Rd. Minsker (2015) considered
µGM deﬁned to be the
geometric median of the bucket means Z1, . . . , Zk. For some constant cGM , with probability at least 1
δ,
it satisﬁes

−

b

b

(1.2)

Tr Σ

·

log(1/δ)
n

.

µGM

k

µ

−

k ≤

cGM

r

b

2

This achieves the correct dependence on δ, but the dimension dependent and independent terms are still
µLM which
not separated. Following this work, Lugosi and Mendelson (2019a) described another estimator
ﬁnally achieved the optimal sub-gaussian radius. The idea behind their construction is to consider all 1-
dimensional projections of the bucket means and try to ﬁnd an estimate that is close to the median of the
means of all projections. Formally, the estimator is given by

b

µLM = arg min

x∈Rd

max
u∈Sd−1

median

Zi, u

{h

k
i=1 − h
i}

x, u

.

(1.3)

(cid:12)
(cid:12)
(cid:12)

i
(cid:12)
(cid:12)
(cid:12)

Clearly, searching over all directions in Sd−1 requires exponential time. The key question, therefore, is
whether one can achieve both computational and statistical eﬃciency simutaneously.

b

Computational considerations A priori, it is unclear that the Lugosi-Mendelson estimator can be com-
puted in polynomial time as a direct approach involves solving an intractable optimization problem. More-
over, the Lugosi-Mendelson analysis seems to suggest that estimation in the heavy-tailed model is concep-
tually harder than under (adversarial) corruptions. In the latter, each sample can be classiﬁed as either
an inlier or an outlier. In the heavy-tailed setting, Lugosi-Mendelson shows that there is a majority of the
bucket means that cluster around the true mean along any projection. However, a given sample may be
an inlier by being close to the mean when projected onto one direction, but an outlier when projected onto
another. In other words, the set of inliers may change from one direction to another.

Surprisingly, a recent line of works have established the polynomial-time computability of Lugosi-Mendelson

b

µLM as the solution of a low-degree polynomial optimization prob-
estimator. Hopkins (2018) formulates
lem and showed that using the Sum-of-Squares SDP hierarchy to relax this problem yields a sub-gaussian
estimator. While the run-time of this algorithm is polynomial, it involves solving a large SDP. Soon after,
Cherapanamjeri et al. (2019a) provided an iterative method in which each iteration involves solving a smaller,
1. Even more recently, a concurrent and independent
explicit SDP, leading to a run-time of
work by Lecué and Depersin (2019) gave an estimator with sub-gaussian performance that can be computed
(cid:1)
O(n2d). The construction is inspired by a near-linear time algorithm for robust mean estimation
in time
under adversarial corruptions due to Cheng et al. (2019). The algorithm requires solving (covering) SDPs.
We note, however, that a common technique in these algorithms is SDP, which tends to be impractical for
large sample sizes and in high dimensions. In contrast, our algorithm only requires approximate eigenvector
computations. For a problem as fundamental as mean estimation, it is desirable to obtain simple and ideally
practical solutions. A key conceptual message of our work is that SDP is indeed unnecessary and can be
replaced by simple spectral techniques.

n3.5 + n2d

O

e

e

(cid:0)

Our result
In this work, we demonstrate for the ﬁrst time that mean estimation with sub-gaussian rates
can be achieved eﬃciently without SDP. The runtime of the algorithm matches the independent work of
Lecué and Depersin (2019).
In addition, our algorithm enjoys robustness against (additive) corruptions,
where the number of adversarial points is a small fraction of k.

It is known that there exists an information-theoretic requirement for achieving such rates—that is,
2−O(n) (Devroye et al. (2016)). Under this assumption, we give an eﬃcient spectral algorithm.

≥

δ

Ae−n for a constant A and k =
Theorem 1.1. Let δ
are i.i.d. samples from a distribution over Rd with mean µ and covariance Σ and
with
at least 1

k/200, there is an eﬃcient algorithm that outputs an estimate
δ,

3600 log(1/δ)
⌉

≥

µ

∈

⌈

. Given n points

, where

G
G ∪ B
a set of arbitrary points
Rd such that with probability

B

|B| ≤
−

µ

k

−

µ

k ≤

C

Tr(Σ)
n

+

Σ

k

k

log(1/δ)
n

,

b

!

r

 r

for a constant C. Furthermore, the algorithm runs in time O

b

nd + k2d polylog(k, d)

.

1Throughout we use

e
O(·) to hide polylogarithmic factors (in n, d and log(1/δ)).

(cid:0)

(cid:1)

3

The algorithm is iterative. Each iteration only requires an (approximate) eigenvector computation, which
can be implemented in nearly linear time by power iteration or the Lanczos algorithm. We believe that our
algorithm can be fairly practical.

Other related works Recently, Prasad et al. (2019) established a formal connection between the Huber
contamination model and the heavy-tailed model we study in this paper. They leverage this connection to
O(nd2)-time mean estimation algorithm of Diakonikolas et al. (2019) to design estimators
use an existing
for the heavy-tailed model. Under moment assumptions, their estimator achieves performance better than
geometric median (1.2), yet worse than sub-gaussian.

e

In addition, algorithmic robust statistics has gained much attention in the theoretical computer science
community in recent years. A large body of works have studied the mean estimation problem with adversari-
ally corrupted samples, with the focus on providing eﬃcient algorithms (Diakonikolas et al. (2019); Lai et al.
(2016); Cheng et al. (2019); Dong et al. (2019)). For a more complete survey, see Diakonikolas and Kane
(2019)

Going beyond mean estimation, there has been a recent spate of works on other statistical problems

under heavy-tailed distributions. We refer the readers to Lugosi and Mendelson (2019c) for a survey.

Technical overview Our main algorithm builds upon the iterative approach of Cherapanamjeri et al.
(2019a). For simplicity, assume there is no adversarial point. At a high level, for each iteration t, the
algorithm will maintain a current guess xt of the true mean. To update, Cherapanamjeri et al. study
µLM (1.3) with x = xt. They showed that under Lugosi-Mendelson structral
the inner maximization of
condition, the problem is essentially equivalent of following program, which we call

(xt, Z):

M

b

subject to

max θ
Zi
bi
h
k

xt, u

−

i ≥

biθ for i = 1, . . . , k

0.95k

bi

≥

i=1
X
b

k, u

0, 1

}

∈

Sd−1.

∈ {

It can be shown that an optimal solution u
µ
and θ approximates
constant γ, to geometrically decrease the distance of xt to µ.

. Hence, one can perform the update xt+1 ←

xt

−

∈

k

k

Sd−1 will align with the unit vector in the direction of µ

xt,
xt + γθu, for some appropriate

−

In this work, we start by drawing a connection between the above program and the furthest hyperplane
problem (FHP) of Karnin et al. (2012). This allows us to avoid the SDP approach in Cherapanamjeri et al.
(2019a). The problem can be formulated as the following:

max θ

subject to

| h
u

Zi

∈

xt, u

−
Sd−1.

i | ≥

θ for i = 1, . . . , k

(FHP)

(1.4)

In the original formulation due to Karnin et al., the goal is to ﬁnd a maximum margin linear classiﬁer for a
(xt, Z) satisﬁes at
collection of points, where the margin is two-sided. Notice that any feasible solution to
least 0, 95k constraints of FHP as well. For an arbitrary dataset, the two-sided margin requirement indeed
provides a relaxation. One technical observation of this work is that it is not a signiﬁcant one, for the
random data we care about—if a major fraction of the constraint (1.4) are satisﬁed, then most constraints
of

M
Unfortunately, the algorithm of Karnin et al. cannot directly apply, as it only works under a strong
promise that there exists a feasible solution that satisﬁes all of the constraints (1.4). In our setting, there
may not be such a feasible solution; we can only guarantee that there exists a unit vector (namely, the one
in the direction of µ

xt) that satisﬁes most of constraints with large margin.

(xt, Z) are satisﬁed as well.

M

−

4

Our main contribution is to provide an algorithm that works even under this weak promise. We now
brieﬂy review the algorithm of Karnin et al., show why it fails for our purpose, and explain how we address
the issues that arise. Suppose that there exists a unit vector u∗ and θ∗ which are feasible for the FHP
problem. Then, averaging the constraints tells us that

1
k

k

i=1
X

Zi, u∗
h

2
i

≥

θ∗2.

Hence, if we deﬁne u to be the top right singular vector of the matrix A whose rows are Zi, then

Au

2
2 =

k

k

k

h

i=1
X

Zi, u

2
i

≥

k

i=1
X

2

Zi, u∗
h

i

≥

kθ∗2,

2 may
so u satisﬁes the constraints in (FHP) on average. However, the distribution of the quantities
i
be extremely skewed, so that u only satisﬁes a few of the constraints with large margin. If this happens,
however, we can downweight those constraints which are satisﬁed by u with large slack to encourage it
to satisfy more constraints. This reweighting procedure is repeated several times, and at the end we use a
simple rounding scheme to yield a single output vector with the desired properties from all the repetitions. In
particular, this weighting scheme is essentially the same as the classic multiplicative weights update (MWU)
method (Arora et al. (2012)) for regret minimization, as we show in Appendix H.

Zi, u
h

≥

2
i

Zi, u∗

k
i=1 h

If we are only guaranteed that u∗ satisﬁes most, but not all, of the constraints, then the inequality
kθ∗2 may no longer hold when the points Zi get re-weighted and the algorithm of Karnin
et al. cannot be guaranteed to converge. To illustrate this point, consider the following extreme case. Suppose
P
that after the ﬁrst iteration, the algorithm ﬁnds the vector u∗ as the top right singular vector of A. In the re-
θ∗2 may be down-weighted signiﬁcantly, whereas
weighting procedure, the constraints i for which
the remaining constraints may be unaﬀected. This may result in most of the weight being concentrated on
θ∗2. In the second iteration, we have no guarantee of the behavior of the
the constraints i where
top singular vector of the re-weighted matrix because all the weight is concentrated on a small set consisting
of these “bad” constraints.

Zi, u∗
h

Zi, u∗
h

2
i

≪

≥

i

2

To address this scenario, our key technical idea is to project the weights onto the set of smooth distributions
after each update. Informally, the notion of smooth distribution enforces that no point can take too much
probability mass—say, more than 4/k. This prevents the weights from ever being concentrated on too
k
kθ∗2 still holds approximately. Moreover,
small a subset and allows us to guarantee that
i=1 h
the appropriate notion of projection here is that of a Bregman projection. Leveraging our earlier MWU
interpretation of the algorithm (Appendix H), we apply a classic regret bound for MWU under Bregman
projection (Arora et al. (2012)), and this yields the same guarantee of the original algorithm. Finally, we
remark that the projection can be computed quickly. Combining all these ideas together, we manage to
bypass the barrier of having bad points, under the much weaker assumption on u∗.

Zi, u∗

P

≥

i

2

Organization The remainder of this article is organized as follows. In Section 2, we set up the notations
and specify assumptions on the data. In Section 3, we explain the high level approach based on an iterative
descent procedure from Cherapanamjeri et al. (2019a). The procedure requires us to approximately maximize
a (non-convex) objective, and we discuss its properties in Section 4. Section 5 contains the main technical
innovations of this work, where we design and analyze a faster algorithm for the aforementioned optimization
problem.

2 Preliminaries and Assumptions

In the following, we use rδ =
rate and k =
of adversarial points, with

3200 log(8/δ)
⌉
B

⌈

Tr(Σ)/n +
k
. The input data
p
{

log(1/δ)/n to denote the optimal, sub-gaussian error
, a set
G
k/200. Our algorithm preprocesses the data Xi into the bucket means

, a set of i.i.d. points, and

n
i=1 consist of

Σ
k
Xi

B

}

p
| ≤

|

5

, Z2k

Z1, Z2,
if Bj contains an adversarial Xi
bucket means is at most k/200.

Rd.2 Let

· · ·

∈

B

∈

j be the set of Xi in bucket j. We say that a bucket mean Zj is contaminated
B and uncontaminated otherwise. Note that the number of contaminated

Our argument is built on the Lugosi-Mendelson condition.

It states that under any one-dimensional
projection, most of the (uncontaminated) bucket means are close to the true mean, by an additive factor of
O(rδ). Throughout, we pessimistically assume all contaminated bucket means do not satisfy this property
(under any projection) and condition on the following event.

Assumption 2.1 (Lugosi-Mendelson condition). Under the setting above, for all unit v, we have

i − h
Lemma 2.1 (Lugosi and Mendelson (2019b)). Assumption 2.1 holds with probability at least 1

}| ≤

i ≥

|{

i :

v, Zi
h

v, µ

600rδ

0.05k.

δ/8.

−

3 Descent Procedure

At a high level, our algorithm builds upon the iterative descent paradigm of Cherapanamjeri et al. (2019a).
It maintains a sequence of estimates and updates via distance and gradient estimate.

Deﬁnition 3.1 (distance estimate). We say that dt is a distance estimate (with respect to xt) if

(i) when

(ii) when

µ

µ

k

k

−

−

xt

xt

14000rδ, we have dt

28000rδ; and

≤

k ≤

> 14000rδ, we have

k

1
21 k

µ

−

xt

dt

µ

2

k

−

xt

k

≤

k ≤

Deﬁnition 3.2 (gradient estimate). We say that gt is a gradient estimate (with respect to xt) if

whenever

µ

k

−

xt

k

> 14000rδ.

gt,

(cid:28)

µ
µ

k

−
−

xt
xt

≥

k (cid:29)

1
200

1. Input: Buckets means Z1, . . . , Zk

step size η.

∈

Rd, initial estimate x0, iteration count Tdes, and

(3.1)

(3.2)

2. For t = 1, . . . , Tdes:

(a) Compute dt = DistEst(Z ′, xt).
(b) Compute gt = GradEst(Z ′, xt).
(c) Update xt+1 = xt + ηdtgt.

3. Output: xt∗, where t∗ = arg mint dt.

Algorithm 3.1: Main algorithm—Descent

k

Suppose we initialize the estimate with coordinate-wise median-of-means which achieves an error rate
Σ
kd/n (Lemma B.2). The following lemma states that if DistEst and GradEst provide distance
k
and gradient estimate, then the algorithm Descent succeeds in logarithmic iterations. The lemma has
p
essentially appeared in Cherapanamjeri et al. (2019a), albeit with a general initialization and a diﬀerent set
of constants. We give a proof in Appendix C for completeness.

2We assume δ is such that k ≤ n/2; as we mentioned in the introduction, this is information-theoretically necessary, up to

a constant (Devroye et al. (2016)).

6

Lemma 3.1 (convergence rate; see Cherapanamjeri et al. (2019a)). Assume that for all t
Tdes, dt is a
distance estimate and gt is a gradient estimate (with respect to xt). Suppose
x0k ≤
.
O
Then the output of Algorithm 3.1 Descent instantiated with Tdes = Θ (log d) and η = 1/8000 satisﬁes
(cid:17)
xt∗

k
(cid:16)p

O (rδ).

kd/n

Σ

≤

−

µ

µ

k

k

k

−

k ≤

4 Inner Maximization and its Two-Sided Relaxation

Cherapanamjeri et al. (2019a) obtains gradient and distance estimates by solving the inner maximization
problem of the Lugosi-Mendelson estimator, denoted by

(x, Z):

M

subject to

max θ
bi
h
k

Zi

−

x, w

i ≥

biθ for i = 1, . . . , k

0.95k

bi

≥

i=1
X
b

k, w

0, 1

}

∈

Sd−1.

∈ {

(x, Z)
We also denote its feasibility version for a ﬁxed θ by
dictates that 0.95 fraction of the data must lie on one side of the hyperplane w with a margin θ. As
discussed in the introduction, we relax it by allowing a two-sided margin:

(θ, x, Z). Note that the constraint of

M

M

M2(x, Z).

subject to

max θ
bi
| h
k

Zi

x, w

−

i | ≥

biθ for i = 1, . . . , k

0.95k

bi

≥

i=1
X
b

k, w

0, 1

}

∈

SD−1.

∈ {

One technical observation here is that under the Lugosi-Mendelson condition, this relaxation is insigniﬁcant.
Indeed, approximately solving the problem suﬃces for gradient and distance estimates.

Lemma 4.1. Let θ∗ be the optimal value of
Zi, we have
or

w, Zi
w is a gradient estimate.

i | ≥

| h

−

x

(x, Z) and w be a unit vector such that for at least k/8 of the
θ, where θ = 0.1θ∗. We have that (i) θ is a distance estimate and (ii) either w

M

−
We give a proof in Appendix D. The intuition here is simple. If

rδ, then the Lugosi-Mendelson
condition ensures at most 0.05k points are far from x by O(rδ) (under any projection), so θ = O(rδ). On
rδ, along the gradient direction, a majority of data lie only on one side of the
the other hand, if
hyperplane, the side that contains the true mean, so the two-sided constraint does not make a diﬀerence.

k ≫

k ≪

−

−

µ

µ

x

x

k

k

5 Approximating the Inner Maximization

We now give an algorithm that eﬃciently computes a approximate solution to the relaxation of the inner
maximization. This will provide gradient and distance estimates for each iteration of the main Descent
algorithm (Algorithm 3.1).

x

The run-time of the algorithm is proportional 1/θ2. For technical reasons, we need to ensure that
1 for all i. However, naïvely scaling all the data would decrease θ, thereby blowing up the
x with large norm

Zi
k
running time. Hence, as a preprocessing step, we prune out a small fraction of points Zi
before scaling.

k ≤

−

−

7

5.1 Pruning and scaling

The preprocessing step (Algorithm 5.1) will be executed only once in the algorithm. After the pruning step
and an appropriate scaling, we may assume the following structures on the data.

Assumption 5.1. Given a current estimate x, the pruned dataset Z
x
where B = maxi
. We assume (i)
k
a unit vector w such that for at least 0.8k points

x),
0.9k; and (iii) there exists θ = Ω(1/√d) and
θ.

×d of size k′, let Z ′

B (Zi

i = 1

1; (ii) k′
Z ′
i, w

i −

Z ′

Z ′

−

∈

k

k

ik ≤
| h

≥
i | ≥

Rk′

We analyze the subroutine and prove the following lemma in Appendix E.

Lemma 5.1. With probability at least 1
O

kd/n

Ω(rδ).

and

Σ

µ

x

k

−

k ≥

k

k
(cid:16)p

(cid:17)

δ/8, Assumption 5.1 holds for any x such that

x

k

−

µ

k ≤

−

1. Input: Dataset Z1, Z2,

· · ·
2. Compute the distances di =

, Zk

Rd, initial estimate x0

∈

Zi

k

.

x0k

−

3. Sort the points by di in decreasing order.

4. Remove the top 1/10 fraction of them. Let Z1,

, Zk′ be the remaining data.

· · ·

5. Output: Z1,

, Zk′

· · ·

Algorithm 5.1: Prune

In the remainder of the section, given a current estimate x, we work with the pruned and scaled data,

centered at x, which we call Z ′

Rk′

×d.

∈

We will aim at proving the following lemma, under Assumption 5.1.

Lemma 5.2 (key lemma). Assume Assumption 5.1. Let δ
there exists w∗
i | ≥
ApproxBregman which, with probability at least 1
fraction of the points Z ′

Sd−1 which satisﬁes

i, it holds that

i, w∗

Z ′

Z ′

| h

∈

i, w
i | ≥
O

k2d

−
0.1θ∗.
.

| h

Further, ApproxBregman runs in time

∈
θ∗ for 0.8k points in
δ/4Tdes, outputs w

(0, 1) and Tdes = Θ(log d). Suppose that
Z ′
. Then there is an algorithm
i}
Sd−1 such that for at least 0.45

{
∈

5.2 Approximation via Bregman Projection

(cid:0)

e

(cid:1)

i has margin

M2. Suppose (by binary search) that we know
In this section, we give the main algorithm for approximating
the optimal margin θ in Lemma 5.2. The goal is to ﬁnd a unit vector w such that a constant fraction of
Z ′
θ. The intuition is that we can start by computing the top singular vector of
Z ′. Then the margin would be large on average: certain points may overly satisfy the margin demand while
other may under-satisfy it. Hence, we would downweight those data poitns that achieve large margin and
compute the top singular vector of the weighted matrix again.

i | ≥

i, w

Z ′

| h

However, it may stop making progress if it puts too much weight on the points that do not satisfy
the margin bound. In this section, we show how to prevent this scenario from occurring. The key idea is
that at every iteration, we “smooth” the weight vector τt so that we can guarantee progress is being made.
We will formulate our algorithm in the well-studied regret-minimization framework and appeal to existing
machinery (Arora et al. (2012)) to derive the desired approximation guarantees.

First, we deﬁne what type of distribution we would like τt to be.

8

Deﬁnition 5.1 (Smooth distributions). The set of smooth distributions on [k′] is deﬁned to be

where ∆k′ is the set of probability distributions on [k′],

K

=

p

(cid:26)

∈

∆k′ : p(i)

4
k′ for every i

∈

≤

[k′]

,

(cid:27)

∆k′ =

p : [k′]

→




[0, 1] :

p(i) = 1

Xi∈[k′]

.




In the course of the algorithm, after updating τt as in the previous section, it may no longer be smooth.
Hence, we will replace it by the closest smooth weight vector (under KL divergence). The following fact
conﬁrms that ﬁnding this closest smooth weight vector can be done quickly.





Fact 5.3 (Barak et al. (2009)). For any p

∈

∆k with support size at least k′/2, computing

ΠK(p) = arg min

q∈K

KL(p

q)

||

) denotes the Kullback-Leibler divergence.

can be done in ˜O(k′) time, where KL(

·||·

Remark 5.1. In our algorithm, we will only compute Bregman projections of distributions of support
size at least k′/2. This is because neither our reweighting procedure nor the actual projection algorithm
of Barak et al. (2009) sets any coordinates to 0 and the initial weight is uniform.

1. Input: Buckets means Z ′

Rk′

∈

2. Initialize weights: τ1 = 1

k′ (1, . . . , 1)

3. For t = 1, . . . , T , repeat:

Rk′

.

∈

×d, margin θ, iteration count T

N

∈

(a) Let At be the k′

d matrix whose ith row is

τt(i)(Z ′

i) and wt be its approximate

top right singular vector .

×

Z ′

(b) Set σt(i) =

i, wt
i |
Atwt
Otherwise, do not change the weights.

2
2 ≥

| h
If

k

k

.

(c) Reweight:

θ2
10 , then τt+1(i) = τt(i)

p

(d) Normalize: Let Z =
(e) Compute the Bregman projection: τt+1 ←
Z ′,

Round

P

wj

T
t=1, θ

4. Output: w

←

{

}

(cid:0)
i∈[k′] τt+1(i) and redeﬁne τt+1 ←

(cid:1)

1
Z τt+1.

ΠK(τt+1).

(or report Fail if Round fails).

σt(i)2/2

for i

1

−

[k′].

∈

Algorithm 5.2: Approximate inner maximization via Bregman projection—ApproxBregman

(cid:0)

(cid:1)

Since Algorithm 5.2 is the MWU method with Bregman projections onto the set

following regret guarantee.3
Theorem 5.4 (Theorem 2.4 of Arora et al. (2012)). Suppose that for σ2
t

[T ]. Then after T iterations of Algorithm 5.2, for any p

t (i)
, it holds that:

, we will apply the

K

[0, 1] for all i

∈

[k′] and

∈

∈

∈ K

τt, σ2
t

3
2

≤

T

p, σ2
t

+ 2KL(p

τ1).

||

T

t=1
X

(cid:10)

3To be more precise, the iterations t in which kAtwtk2

θ2
10 , the algorithm does not update the weights, which has no eﬀect on the other iterations.

(cid:11)

t=1
X
(cid:10)
θ2
10 behave according to the MWU method. Whenever kAtwtk2

(cid:11)

2 ≥

2 <

9

Finally, we comment that we cannot naïvely apply the power method for the singular vector computation.
The power method has failure probability of 1/10, whereas our algorithm should fail with probability at most
δ = O(exp(
k)) that is exponentially low. However, we note that the algorithm computes the top singular
vectors of a sequence of matrices A1, A2, . . . , AT . Observe that as long as T = Ω(log(1/δ)) = Ω(k), with
δ/8, the power method will succeed for 0.9T of the matrices. We will show that this
probability at least 1
many successes suﬃce to guarantee correctness of our algorithm.

−

−

We ﬁrst prove the following lemma, a requirement for the rounding algorithm to succeed.

Lemma 5.5 (regret analysis). After T = O
but a 1/4 fraction of i

[k′]:

max

log k′
θ2

(cid:16)

(cid:16)

, log(Tdes/δ)

iterations of Algorithm 5.2, for all

(cid:17)(cid:17)

∈

i, wt

Z ′
h

2

i

≥

100 log k′.

T

t=1
X
θ

Proof. Let S =
existence is guaranteed in the hypothesis of Lemma 5.2. By assumption, we have that
simply calculate each of the terms in Theorem 5.4.

be the set of constraints satisﬁed by the unit vector w∗ whose
0.8k′. We

i, w∗

[k′] :

i | ≥

| ≥

Z ′

| h

∈

S

{

}

i

|

First, let

=

t

{

∈

I

[T ] : wt is a 1/2-approximate top singular vector of At

. Then we have for any t

}

:

∈ I

k′

τt, σ2
t

=

τt(i)

′

i=1
X
k
1
2

i=1
X

i∈S
X

1
2

1
2

(cid:10)

(cid:11)

≥

≥

≥

≥

i∈S
X
1
5

1
2 ·

θ2 =

Summing this inequality over t

i, wt

Z ′
h

2
i

(by deﬁnition)

τt(i)

τt(i)

h

h

Z ′

i, w∗

Z ′

i, w∗

2
i

2
i

(because wt is an approximate top eigenvector)

τt(i)θ2

(by deﬁnition of S)

θ2
10

∈

(because

S

|

| ≥

0.8k′ and τt

).

∈ K

[T ], we have that

T

τt, σ2
t

t=1
X

(cid:10)

τt, σ2
t

t∈I
X

(cid:10)

(cid:11)

≥

(cid:11)

θ2.

|I|
10

≥

By Chernoﬀ-Hoeﬀding bound combined with the guarantee of power iteration (Fact B.3), as long as T =
δ
5 T iterations, it will be the case that wt
Ω(log(Tdes/δ)), then with probability at least 1
8Tdes
is an approximate top singular vector. In other words,

, for at least 4
4
5 T , so that we have:

−

|I| ≥

Next, note that if we choose p = ei, then

T

p, σ2
t

=

τt, σ2
t

2T
25

θ2.

≥

T

t=1
X

(cid:10)

(cid:11)

T

i, wt

Z ′
h

i

2 .

t=1
X

(cid:10)

t=1
X

(cid:11)

Because τ1 is uniform, the relative entropy term in Theorem 5.4 is at most log k′. Let’s pretend for a moment
that ei
(it is not). Then after plugging in the above calculations to Theorem 5.4 and rearranging, we

∈ K

10

have that for every i

[k′]

∈

i, wt

Z ′
h

2

i

≥

2T
25

θ2

−

2 log k′

≥

100 log k′,

T

t=1
X

5
10

log k′
θ2

by setting T
ﬁx the invalid assumption that ei
p′

such that

≥

∈ K

∈ K

. This gives the bound claimed in the statement of the lemma, but it remains to
[k′], another distribution

. To do so, we will construct, for most i

∈

T

T

ei, σ2
t

≥

p′, σ2
t

.

t=1
X
100 log k′ gives the desired lower bound, for most i. Write α =

t=1
X

(cid:10)

(cid:10)

(cid:11)

(cid:11)

Combining this with

T
t=1

p′, σ2
t

T
t=1 σ2

t , and without loss of generality assume that

P

(cid:10)

≥

(cid:11)

P

α2 ≥
For i = 1, . . . , 4k′/5, take p′ to be uniform on those j
i). By construction, we have that
set of size at least k′/5.

α, ei
h

α1 ≥

α, p′

i ≥ h

. . .

≥

αk′ .

[k′] such that αi

∈

≥
. Finally, observe that p′
i

αj (there are at least k′/5 such
because p′ is uniform on a

∈ K

Observe that the ApproxBregman produces a sequence of vectors by the end. Karnin et al. (2012)
provides a rounding algorithm that combines them into one with the desired margin bound. We describe
the algorithm and prove the following lemma in Appendix F.

Lemma 5.6. The algorithm Round (Algorithm F.1) outputs w that satisﬁes
the points, with probability at least 1

δ/4Tdes.

−

Z ′

i, w

| h

i | ≥

0.1θ for 0.45k of

Finally, we are now ready to prove the key lemma using ApproxBregman.

≤

k. Hence, Lemma 5.5 implies that the iteration count is

Proof of Lemma 5.2. The correctness follows from Lemma 5.6. We focus on run-time. By Assumption 5.1,
we have that 1/θ2 = O(d). By projecting onto the subspace spanned by the bucket means, we can assume
O(k′). The runtime of each iteration is bounded
d
by the cost of computing an approximate top singular vector of a k′ by d matrix via the power method,
O(k′d) by Fact B.3. Finally, each repetition of the rounding algorithm Round takes time
O(k′d),
which is
and the number of trials is at most O(log(1/δ′)) by deﬁnition. Thus, the runtime of the rounding algorithm
is

O(k2d) .
e

e

e

e

5.3 Putting it Together

2k
Our main algorithm begins with the initial guess as the coordinate-wise median-of-means of
i=k+1.
Then it proceeds via the Descent procedure, where the gradient and distance estimates are given by
ApproxBregman. To ensure independence, we only use the
k+1
i=1 for the descent part. We provide the
full description in Appendix A.

Zi

Zi

}

{

}

{

We now give a proof sketch our main theorem. The formal proof is found in Appendix G.

Proof sketch of Theorem 1.1. Our argument is conditioned on (i) that the Lugosi-Mendelson condition holds,
/n, and (iii) that the Prune procedure succeeds.
(ii) that the initial guess x0 satisﬁes an error bound
kd
k
Each fails with probability at most δ/8.
The guarantee of ApproxBregman, along with Lemma 4.1, implies that GradEst and DistEst suc-
ceed with probability at least 1
δ/4Tdes each iteration. Taking union bound over all above events, the
failure probability of the ﬁnal algorithm is at most δ. Applying the guarantee of the Descent procedure
and error bound of the initial guess ﬁnishes the proof.

p

Σ

−

k

11

6 Conclusion and Discussion

In this paper, we provided a faster algorithm for estimating the mean of a heavy-tailed random vector that
achieves subgaussian performance. Unlike previous algorithms, our faster running time is achieved by the use
of a simple spectral method that iteratively updates the current estimate of the mean until it is suﬃciently
close to the true mean.

Our work suggests two natural directions for future research. First, is it possible to achieve subgaussian
performance for heavy-tailed covariance estimation in polynomial time? Currently, the best polynomial-
time covariance estimators do not achieve the optimal statistical rate (see Lugosi and Mendelson (2019c);
Cherapanamjeri et al. (2019b)), while a natural generalization of the (computationally intractable) Lugosi-
Mendelson estimator is known to achieve subgaussian performance. One approach would be to build on
our framework; the key technical challenge is to design an eﬃcient subroutine for producing bi-criteria
approximate solutions to the natural generalization of the inner maximization problem to the covariance
setting.

Another direction is to achieve a truly linear-time algorithm for the mean estimation problem. Our
O(k) iterations; is it possible to reduce

iterative procedure for solving the inner maximization problem take
this to a constant?

e

Acknowledgements

The authors would like to thank Boaz Barak and Jelani Nelson for helpful conversations. In particular, we
would like to thank Boaz for directing us to the paper Barak et al. (2009).

Zhixian Lei and Prayaag Venkat have been supported by NSF awards CCF 1565264 and CNS 1618026,
and the Simons Foundation. Kyle Luh has been partially supported by NSF postdoctoral fellowship DMS-
1702533. Prayaag Venkat has also been supported by an NSF Graduate Fellowship under grant DGE1745303.

References

Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments.
J. Comput. System Sci., 58(1, part 2):137–147, 1999. ISSN 0022-0000. doi: 10.1006/jcss.1997.1545. 2

Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm
and applications. Theory of Computing, 8(1):121–164, 2012. doi: 10.4086/toc.2012.v008a006. 5, 8, 9, 22

Boaz Barak, Moritz Hardt, and Satyen Kale. The uniform hardcore lemma via approximate Bregman
In ACM-SIAM Symposium on Discrete Algorithms (SODA ’09), pages 1193–1200, 2009.

projections.
URL http://dl.acm.org/citation.cfm?id=1496770.1496899. 9, 12

Avrim Blum, John Hopcroft, and Ravindran Kannan. Foundations of Data Science. Cambridge University

Press, 2019. 14

Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. Ann. Inst. Henri

Poincaré Probab. Stat., 48(4):1148–1185, 2012. ISSN 0246-0203. doi: 10.1214/11-AIHP454. 2

Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estimation in nearly-linear
In ACM-SIAM Symposium on Discrete Algorithms (SODA ’19), pages 2755–2771, 2019. doi:

time.
10.1137/1.9781611975482.171. 3, 4

Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L Bartlett. Fast mean estimation with
In Conference on Learning Theory (COLT ’19), pages 786–806, 2019a. URL

sub-gaussian rates.
https://arxiv.org/abs/1902.01998. 3, 4, 5, 6, 7, 15, 16

12

Yeshwanth Cherapanamjeri, Samuel B. Hopkins, Tarun Kathuria, Prasad Raghavendra, and Nilesh Tripura-
neni. Algorithms for heavy-tailed statistics: Regression, covariance estimation, and beyond. arXiv preprint
arXiv:1912.11071, 2019b. 12

Luc Devroye, Matthieu Lerasle, Gabor Lugosi, and Roberto I Oliveira. Sub-gaussian mean estimators.

Annals of Statistics, 44(6):2695–2725, 2016. doi: 10.1214/16-AOS1440. 2, 3, 6

Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic high-dimensional robust statistics.

arXiv preprint arXiv:1911.05911, 2019. URL https://arxiv.org/abs/1911.05911. 4

Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust
estimators in high-dimensions without the computational intractability. SIAM Journal on Computing, 48
(2):742–864, 2019. doi: 10.1137/17M1126680. 4

Yihe Dong, Samuel B Hopkins, and Jerry Li.
estimation and improved outlier detection.
https://arxiv.org/abs/1906.11366. 4

Quantum entropy scoring for fast robust mean
URL
arXiv preprint arXiv:1906.11366,

2019.

Samuel B Hopkins. Sub-gaussian mean estimation in polynomial time. arXiv preprint arXiv:1809.07425,

2018. URL https://arxiv.org/abs/1809.07425. 3

Mark R. Jerrum, Leslie G. Valiant, and Vijay V. Vazirani. Random generation of combinatorial structures
from a uniform distribution. Theoret. Comput. Sci., 43(2-3):169–188, 1986. ISSN 0304-3975. doi: 10.
1016/0304-3975(86)90174-X. 2

Emilien Joly, Gábor Lugosi, and Roberto Imbuzeiro Oliveira. On the estimation of the mean of a random

vector. Electronic Journal of Statistics, 11(1):440–451, 2017. doi: 10.1214/17-EJS1228. 2

Zohar Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz, and Omri Weinstein. Unsupervised SVMs: On
the complexity of the furthest hyperplane problem. In Conference on Learning Theory (COLT ’12), pages
1–17, 2012. URL http://proceedings.mlr.press/v23/karnin12.html. 4, 11, 19, 20, 21, 23

Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Symposium
on Foundations of Computer Science (FOCS ’16), pages 665–674, 2016. doi: 10.1109/FOCS.2016.76. 4

Guillaume Lecué and Jules Depersin. Robust subgaussian estimation of a mean vector in nearly linear time.

arXiv preprint arXiv:1906.03058, 2019. URL https://arxiv.org/abs/1906.03058. 3

Gábor Lugosi and Shahar Mendelson. Near-optimal mean estimators with respect to general norms. Proba-

bility Theory and Related Fields, 2019a. ISSN 1432-2064. doi: 10.1007/s00440-019-00906-4. 2, 3

Gábor Lugosi and Shahar Mendelson. Sub-Gaussian estimators of the mean of a random vector. Annals of

Statistics, 47(2):783–794, 2019b. ISSN 0090-5364. doi: 10.1214/17-AOS1639. 2, 6

Gábor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A
survey. Foundations of Computational Mathematics, Aug 2019c. doi: 10.1007/s10208-019-09427-x. 4, 12

Stanislav Minsker. Geometric median and robust estimation in banach spaces. Bernoulli, 21(4):2308–2335,

2015. doi: 10.3150/14-BEJ645. 2

A. S. Nemirovsky and D. B. and Yudin. Problem complexity and method eﬃciency in optimization. A Wiley-
Interscience Publication. John Wiley & Sons, Inc., New York, 1983. ISBN 0-471-10345-4. Translated from
the Russian and with a preface by E. R. Dawson, Wiley-Interscience Series in Discrete Mathematics. 2

Masashi Okamoto. Some inequalities relating to the partial sum of binomial probabilities. Annals of the

institute of Statistical Mathematics, 10(1):29–35, 1959. 15

Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A uniﬁed approach to robust mean

estimation. arXiv preprint arXiv:1907.00927, 2019. URL https://arxiv.org/abs/1907.00927. 4

13

A Main algorithm description

1. Input: Dataset Z ′ and current estimate xt

2. Z ′

i ←

(Z ′

i −

xt) /B by scaling each point by B = maxi

Z ′

i −

k

xt

.

k

3. θ

the largest margin θ such that ApproxBregman(Z ′, θ, T ) does not Fail, where

←

T = O(log k/θ2).

4. Output:

d = B

10 θ.

b

Algorithm A.1: Distance estimation—DistEst

1. Input: Dataset Z ′ and current estimate xt

2. Z ′

i ←

(Z ′

i −

xt) /B by scaling each point by B = maxi

Z ′

i −

k

xt

.

k

3. θ

the largest margin θ such that ApproxBregman(Z ′, θ, T ) does not Fail, where

←
T = O

log k/θ2

.

ApproxBregman (Z ′, θ, T )

(cid:0)

(cid:1)

4.

g

5. If
b

←
g, Z ′
h

ii ≥

0.1θ for at least 0.5k of the Z ′

i, output

g; otherwise, output

b

Algorithm A.2: Gradient estimation—GradEst

b

g.

−

b

B Technical facts

We formally state the statistical guarantee of empirical average and coordinate-wise median-of-means. The
former is an application of the Chebyshev’s inequality. The latter is folklore but can follow easily from the
Lugosi-Mendelson condition by considering the projections onto standard basis vectors.

Lemma B.1 (empirical mean). Let δ
with mean µ and covariance Σ, let µ = 1
n

∈

(0, 1). Given n i.i.d. copies X1, . . . , Xn of a random vector X

n
i=1 Xi. Then with probability at least 1

δ,

−

Rd

∈

k ≤ r
Lemma B.2 (coordinate-wise median-of-means). Assume the Lugosi-Mendelson condition (Assumption 2.1).
µ be their coordinate-
Let
wise median-of-means. Then with probability at least 1

k
i=1 be the bucket means from n points (with at most k/200 contaminated) and

δ/8,

Zi

−

{

}

k

P
µ

µ

Tr(Σ)
δn

.

−

µ

k

−

µ

k ≤

600√drδ .

Σ

d
k

k

log(1/δ)
n

.

r

b

Our algorithm requires computing an approximation of the top (right) singular vector of a matrix A

Rm×n. The classic power method is eﬃcient for this task.

b

∈

2
Fact B.3 (power iteration; see Theorem 3.1 of Blum et al. (2019)). Let λ(A) = maxx∈Sn−1
2. With
probability at least 9/10, the power method (with random initialization) outputs a unit vector w such that
Aw

in O(log n) iterations. Moreover, each iteration can be performed in O(mn) time.

Ax

k

k

2
2 ≥

k

λ(A)
2

k

14

1. Input: Dataset X1, X2,

, Xn

· · ·
2. Let k = 3600 log(1/δ). Divide the data into 2k groups.

∈

Rd

3. Compute the bucket mean of each group: Z1, Z2,

, Z2k

Rd.

∈

· · ·

4. Compute the coordinate-wise median-of-means of the second half of bucket means:

x0 ←

MedianOfMeans(
{

Zk+1,

, Z2k

).

}

· · ·

5. Prune the ﬁrst half of bucket means, where Z is the data matrix of

Zi

{

}

k
i=17:

Z ′

←

Prune(Z, x0).

6. Tdes

←

Θ(log d), η

1/8000

←

7. Run the main descent procedure:

GradEst as above.

8. Output:

µ

b

Algorithm A.3: Final algorithm

Descent(Z ′, x0, Tdes, η), using DistEst and

←

µ

b

The following is a standard bound on binomial tail.

Lemma B.4 (Okamoto (1959)). Let H(n, p) be a binomial random variable. Then

Pr (H(n, p)

2np)

exp (

−

≤

≥

np/3) .

C Omitted proofs from Section 3

Proof of Lemma 3.1. First, suppose that in some iteration t it holds that

1
21 k

µ

−

xt∗

µ

k

−

xt∗

k ≤

dt∗

dt

2

µ

xt

28000rδ,

≤

k ≤
588000rδ. Second, suppose that in all iterations t it holds that

k ≤

−

≤

k

so that we may conclude

µ

k

−

xt

k

> 14000rδ. Then by the update rule with η = 1/8000,

µ

k

−

xt

k ≤

14000rδ. Then

xt+1 −

k

µ

k

2 =

k

≤ k

≤ k

xt

xt

xt

−

−

−

=

1
(cid:18)

−

µ

µ

k

k

µ

2

−
dt

xt
h

2 + 2ηdt
1
800000
1
1680000k

−

2

k

−
179
336000000

xt

2

µ

k

−

k

(cid:19)

µ, gt

+ η2d2
t k

i
xt

µ

−

k

xt

µ

−

k

+

k
2 +

2

k

gt
1
16000000k
1
16000000k

2

2

xt

µ

k

−

xt

µ

−

k

Hence, the error bound drops at a geometric rate. The conclusion follows since
O(√drδ).

µ

x0k ≤

−

k

O(

Σ

kd
k

k

/n)

≤

p

D Omitted proof from Section 4

First recall that Cherapanamjeri et al. (2019a) showed that the optimal solution to
property that θ is a valid distance estimate (Deﬁnition 3.1) and w a gradient estimate (Deﬁnition 3.2).

(xt, Z) satisﬁes the

M

15

Lemma D.1 (Lemma 1 of Cherapanamjeri et al. (2019a)). For all t = 1, 2,
xt
value of

(xt, Z). Then

600rδ, so dt is a distance estimate with respect to xt.

· · ·

dt

, T , let dt = θ∗ be the optimal

µ

M

|

− k

−

k| ≤

Lemma D.2 (Lemma 2 of Cherapanamjeri et al. (2019a)). For all t = 1, 2,
optimal solution of

· · ·
(xt, Z). Then gt is a distance estimate with respect to xt.

M

, T , let (θ∗, b∗, w∗) be the

We now start by proving a generic claim that any reasonably good bicriteria approximation of

suﬃces to provide gradient and distance estimates.

(xt, Z)

M

Deﬁnition D.1 (bicriteria solution). Let θ∗ be the optimal value of
(x, Z) if
x, w
(α, β)-bicriteria solution to

αk and bi

(x, Z). We say that (θ, b, w) is a
biθ for all i, where θ = βθ∗.

M

i bi

≥

Zi
h

−

M
i ≥

Lemma D.3 (distance estimate). Let (θ, b, w) be a (1/10, 1/20)-bicriteria solution to
dt = θ is a distance estimate with respect to xt.

P

M

(xt, Z). Then

Proof of Lemma D.3. By Lemma D.1, the optimal value θ∗ lies in the range

Moreover, since θ∗/20

θ

≤

≤

[

µ

xt

k

−

k −
θ∗, we have that

600rδ,

µ

k

−

xt

k

+ 600rδ] .

µ

k

−
20

x

k

30rδ

θ

≤

≤

−

x

µ

k

−
20

k

+ 30rδ.

(D.1)

When

µ

k

−

x

k ≥

14000rδ, we get from the inequality (D.1) that

µ

k

−
21

x

k

θ

≤

≤

µ

k

−
19

x

k

.

When

µ

k

−

x

k ≤

14000rδ, θ

≤

730rδ < 28000rδ, again by (D.1).

•

•

Lemma D.4 (gradient estimate). Let (θ, b, w) be a (1/10, 1/20)-bicriteria solution to
gt = w is a gradient estimate with respect to xt.

M

(xt, Z). Then

Proof of Lemma D.4. Let g∗ = (µ
µ
k
On the one hand, by Lemma D.1, we have

xt)/

−

xt

k

−

be the true gradient. We need to show that

g∗, gt
h

i ≥

1/20.

dt = θ

1
20

≥

u

(
k

−

xt

k −

600rδ).

(D.2)

On the other hand, for at least k/10 points, we have
Zi
h
i ≤
it follows that

xt, gt
dt and for at least 0.95k points, we have
600rδ by Assumption 2.1. Hence, there must be a point Zj that satisﬁes both inequalities, so

µ, gt

i ≥

Zi

−

−

h

dt

Zj

xt, gt

=

Zj

≤ h
Using (D.2) and (D.3) and rearranging,

−

h

i

µ, gt

+

µ
h

i

−

−

xt, gt

i ≤

600rδ +

µ

k

−

xt

g∗, gt

k h

.

i

(D.3)

g∗, gt
h

i ≥

1
20 −

630rδ
xt
µ

−

k

k

1
200

,

≥

where we use

µ

k

−

xt

k ≥

14000rδ.

Now we show that the optimal solution to the two-sided relaxation give distance and gradient estimate.

Lemma D.5. Let (θ′, b′, w′) be an optimal solution of

M2(x, Z). We have that

16

(i) the value θ′ lies in [

µ

k

−

x

k −

600rδ,

µ

k

−

x

k

+ 600rδ]; and

(ii) one of the following two statements must hold, if

µ

k

there is a set

there is a set

C

C

•

•

of at least 0.9k points such that

of at least 0.9k points such that

x

−
Zi
h
Zi
h

14000rδ:

k ≥

x, w′
x,

i ≥
w′

−

i ≥

−

−

θ′ for all i

θ′ for all i

; or

∈ C

.

∈ C

(x, Z). To prove (i), ﬁrst recall that Lemma D.1
Proof of Lemma D.5. Let θ be the optimal value of
M
600rδ, as θ′
µ
θ. For the upper
states that θ
bound, assume for the sake of a contradiction that θ′ >
+ 600rδ. Then one side of the hyperplane
deﬁned by w′ must contain at least 19/40 fraction of points, so let’s suppose without loss of generality that

600rδ. Therefore, we get that θ′
µ

≥ k
x
k

≥ k

k −

k −

−

≥

−

−

µ

x

x

k

−
for at least 19k/40 Zi’s. Also, note that

Zi
h

x, w′

θ′ >

µ

k

−

x

k

i ≥

+ 600rδ

h
Combining (D.4) and (D.5), it follows that for at least 19k/40 Zi’s we have

i ≤ k

−

−

−

−

k

i

i

h

x, w′

=

Zi

µ, w′

+

x, w′

µ

x

+

Zi
h

µ
h

Zi

−

µ, w′

.

i

On the other hand, consider projections of all bucket means Zi onto w′. Assumption 2.1 implies that

Zi
h

−

µ, w′

i

> 600rδ.

(D.4)

(D.5)

(D.6)

This means that at most k/20 points satisfy
x, w′

To prove (ii), let S+ =

Zi
h
14000rδ, S+ and S− are disjoint. Now let

−
θ′
}

i :

−

{

µ

k

−

x

k ≥

w′, Zi

i :

|{

h

w′, µ

600rδ)

0.05k.

}| ≤

i − h
Zi
h
i ≥

i ≥
µ, w′
i ≥
and S− =

600rδ, contradicting (D.6).
θ′
x,

w′

i :

Zi
h

−

{

−

i ≥

}

. Notice that since

B =

{
By Assumption 2.1,

w′, Zi

i :

| h

µ

−

i | ≤

600rδ

=

i :
{

}

w′, Zi

| h

x

−

i − h

w′, µ

x

−

i | ≤

600rδ

.

}

B

|

19k/20. Consider the two cases.

| ≥
0, observe that B must intersect S+ but not S−. This implies that

x

−

i ≥
9k/10, since

w′, µ
If
h
S+

|
If

| ≥
w′, µ
h

•

•

S+

|

+

|

|

S−

|

= 19k/20 and they are disjoint.

x

i

−

< 0, by the same argument, we have

S−

|

| ≥

9k/10.

S−

|

| ≤

k/20, so

Next, we show that approximating

M2 in a bicriteria manner achieves a similar guarantee.

Lemma D.6. Let θ∗ be the optimal value of
of the Zi, we have
µ

w′, Zi

14000rδ.

i | ≥

| h

−

x

x

k

−

k ≥

(x, Z) and w′ be a unit vector such that for at least k/8
θ′, where θ′ = 0.1θ∗. One of the following two statements must hold if

M

there is a set

C

•

of at least 0.95k points such that

there is a set

of at least 0.95k points such that

x, w′

θ′

i ≥
w′

−
θ′

x,

600rδ for all i

;

∈ C

600rδ for all i

.

•

C

−
be the set of “good” points with respect to
Proof of Lemma D.6. Let
{
direction w′. By Assumption 2.1,
, which we assume
19k/20. Further, let S =
has size at least k/8. Thus, by pigeonhole principle, there must be a point, say Zj , that is in both sets.
There are two cases.

| h
|C| ≥

w′, Zi

w′, Zi

i | ≤

i | ≥

∈ C

i ≥

{| h

i :

θ′

−

=

−

−

µ

x

C

}

}

−

Zi
h
Zi
h
−
600rδ

17

•

0. Since j
Suppose
−
1340rδ. On the other hand, since j

w′, µ
h

i ≥

x

S and θ∗

13400rδ by Lemma D.5, we have

≥

w′, Zi

| h

x

−

i | ≥

w′, Zj

|h
w′, Zj
Hence, we observe that
h
by an additive factor of 600rδ.

−
x

−

θ′

≥

i ≥

|h

w′, Zj

x

w′, µ

x

−

i − h
1340rδ. By deﬁnition of

−

i| ≤

600rδ.

(D.7)

, all its points cluster around Zj

C

,

∈
∈ C
µ

i|

=

Suppose

w′, µ
h

−

x

i ≤

•

0. We get the second case in the claim by the same argument.

Finally, we are ready to prove Lemma 4.1.

Proof of Lemma 4.1. Let’s ﬁrst check the distance estimate (Deﬁnition 3.1) guarantee.

If

µ

k

−

x

k ≥

•

14000rδ, we have

θ′

≥

1
10 k

µ

x

−

k −

60rδ

≥

2
35 k

µ

x

,

k

−

since θ′ = 0.1θ∗ and θ∗

µ

x

−

k −

≥ k

600rδ. The upper bound of (3.1) obviously holds.

If

µ

k

−

x

k ≤

•

14000rδ, we have θ′

≤

1460rδ by Lemma D.5.

For gradient estimate, we appeal to Lemma D.6 and get that if
or (θ′, b′,
M
Thus, we can apply Lemma D.4, and this completes the proof.

w′) is a (19/20, 1/20)-bicriteria approximation of

−

x

µ

k
(x, Z), where b′ is the indicator vector of

14000rδ, then either (θ′, b′, w′)
.

k ≥

−

C

E Omitted proof from subsection 5.1

We remark that under Lugosi-Mendelson condition, the assumption
/n can be easily
achieved by initializing x0 to be the coordinate-wise median-of-means (Lemma B.2) (with a failure proba-
bility at most δ/8).

kd
k

x0k

p

−

.

µ

k

k

Σ

β. Given the bucket means
/n, and suppose
Rk×d such that at most k/200 points are contaminated, the algorithm Prune removes k/10 of the

Lemma E.1 (pruning). Let β = 600
Z
points and guarantees that with probability at least 1

δ/8, among the remaining data,

x0k ≤

kd
k

p

−

µ

∈

k

k

Σ

−
µ

max

i k

Zi

−

O(β).

k ≤

Further, Prune(Z, x0) can be implemented in

O(kd) time.

Proof of Lemma E.1. For correctness, consider

µ

, and by triangle inequality,

Zi
e
k
x0k ≤ k

−
Zi

−

k
µ

k ≤ k

Zi

+

x0k

µ

k

−

.

x0k

−

Zi

x0k − k

µ

k

−
β by our assumption,

−

Since

µ

k

−

x0k ≤

Zi

good =

Let
S
at least 1
uncontaminated Zi is an average of
of empirical mean, we obtain that for each uncontaminated i, with probability at least 1

x0k
−
. It suﬃces to show that with probability
bad are removed. We ﬁrst lower bound the number of good points. Each
i.i.d. random vectors. Applying Lemma B.1 on estimation error

}
−
{
δ/8 all the points in

x0k −
bad =

k
and

≤ k
i :

k ≤ k

(E.1)

+ β.

n/k

k ≤

k ≥

20β

Zi

Zi

Zi

Zi

i :

−

−

−

−

µ

µ

µ

S

S

β

β

k

}

{

k

⌋

⌊

1/1000,

−

Zi

k

−

µ

k ≤

1000

·

Tr(Σ)k/n

β.

≤

p

18

Therefore, each uncontaminated Zi is in
of uncontaminated points not in
and each uncontaminated point is independent, by a binomial tail bound (Lemma B.4)

1/1000. Let H be the number
good and p = 1/1000. Since there are at most k/200 contaminated points

good with probability at least 1

−

S

S

Pr (H

2p

·

≤

(199/200)k)

1
1

≥
≥
= 1

−
−

−

exp (
exp (

δ/8,

−
−

(199/200)k/3)

p
log (8/δ))

·

where we used k =
(uncontaminated) points. We condition on this event for the rest of the proof.

. Hence, with probability at least 1

3600 log(8/δ)
⌉

δ/8,

−

S

⌈

good contains at least (399/400)k

Now observe that

Zi

k

x0k

−

<

k

Zj

by (E.1). Suppose for a contradiction that j
k/10 of the Z ′
This contradicts condition (E.2).

i’s. By pigeonhole principle, this implies dj

−

x0k
∈ S

for each j

bad and i

∈ S

good

∈ S

(E.2)

bad is not removed by line 4. Then it means that dj

di for
≤
(399/400)k.

di for some i

good, since

∈ S

Sgood

|

| ≥

≤

Computing the distances takes O(kd) time and sorting takes O(k log k) time. Thus, the algorithm Prune

runs in time O(kd + k log k) and succeeds with probability at least 1

δ/8.

−

Pruning allows us to bound the norms of the points Zi

xt for each iteration t.

−

Corollary E.2 (scaling and margin). Suppose

k

µ

the pruned dataset of size k′
9k/10 such that
scaling factor B, θ > 0 and unit vector w such that for at least 4k/5 points in

k ≤

−

≥

k

k

(cid:17)
kd/n

k
(cid:16)p

(cid:17)

,

S

x

O

−
Zi

k ≤
µ

k
(cid:16)p
O

k
Σ

Σ

kd/n

and

µ

k

−

x

k ≥

Ω (rδ). Let

be

S

for each i

. There exists a

∈ S

Further, we have that 1/θ2 = O(d).

1
B (Zi

−

x), w

θ.

≥

(cid:12)
(cid:10)
(cid:12)

(cid:11)(cid:12)
(cid:12)

Proof of Corollary E.2. Let B = maxi∈S k
Zi

Zi

x

Zi

x

k

−

. Then B is bounded by

k

−

k ≤ k

+

µ

k

µ

k

−

x

−

k ≤

O

kd/n

.

(E.3)

By Lemma D.5, there exists a unit vector w such that for at least 0.8k points in
θ′ = Ω(rδ). Hence, we get that

,

S

Zi
h

−

x, w

i ≥

θ′ and

Σ

k

k
(cid:16)p

(cid:17)

θ = Ω

rδ
B

(cid:16)

(cid:17)

= Ω

k

  p

/n +
Σ

k ·

Tr Σ/n

kd/n
p

!

= Ω

1/√d

.

(cid:16)

(cid:17)

Σ

k

k

k
p

Proof of Lemma 5.1. The lemma follows directly from Lemma E.1 and Corollary E.2.

F Omitted proofs from subsection 5.2

Karnin et al. (2012) provides a rounding scheme that combines the sequence of vectors produced by Ap-
proxBregman into one vector that satisﬁes the desired margin bound. The original routine succeeds with
constant probability. We simply perform independent trials to boost the rate. We now analyze the algorithm.
We cite the following lemma for the guarantee of the rounding algorithm (Algorithm F.1).

19

1. Input: Buckets means Z ′, unit vectors w1, . . . , wT
2. Round to a single vector: w = w′

kw′k , where w′ =

∈

Rd, margin θ,

T
t=1 gtwt and gt

(0, 1), for

∼ N

t = 1, . . . , T .

3. Repeat until

Z ′

i, w

P
10 θ for at least 0.6k′ of Z ′
i:

1

i | ≥
(0, 1), for t = 1, . . . , T .

| h
(a) Sample gt
, where w′ =
(b) Recompute w = w′/
(c) Report Fail if more than Ω (log (Tdes/δ)) trials have been performed.

T
t=1 gtwt.

∼ N

w′

k

k

P

4. Output: w

Algorithm F.1: Rounding algorithm—Round

Lemma F.1 (Lemma 6 of Karnin et al. (2012)). Suppose that for at least 3

4 fraction of i

T

t=1
X

i, wt

Z ′
h

2

i

≥

log k′.

[k′], it holds that

∈

(F.1)

Let w1, . . . , wT be the unit vectors satisfying the above condition. Then with constant probability, the vector
w in each repetition of the step 3 of the Round algorithm (Algorithm F.1) satisﬁes
θ/10 for at
least a 0.45 fraction of i

Zi, w

[k′].

i | ≥

| h

Now we prove the guarantee of Round.

∈

Proof of Lemma 5.6. By Lemma F.1, it suﬃces to prove inequality (F.1) holds for at least a 3/4 fraction
of the points. By the regret analysis (Lemma 5.5), the vectors w1, . . . , wT produced during the iterations
of Algorithm 5.2 satisfy the hypothesis of Lemma F.1. Hence, the guarantee of Lemma F.1 holds with
constant probability. Moreover, we can test that this guarantee holds in time O(T k′d). To boost the success
δ′ (with δ′ = δ/4Tdes), Round algorithm performs O(log(1/δ′)) independent trials. Hence,
probability to 1
it reports Fail with probably at most δ′. Otherwise, by its deﬁnition, the output w satisﬁes desired bound

−

Z ′

i, w

| h

0.1θ for 0.45k of the points.

i | ≥

G Full proof of main theorem

Proof of Theorem 1.1. Our argument assumes the following global events.

(i) The Lugosi-Mendelson condition (Assumption 2.1) holds.

(ii) The initial estimate x0 satisﬁes

µ

k

−

(iii) The pruning step succeeds:

x0k ≤
O

600

kd/n.

Σ

k

k
p
Σ
k

kd/n

Z ′

i −

µ

k

k ≤

k
We consider our main algorithm (Algorithm A.3) and ﬁrst prove the correctness of DistEst and GradEst.
Let Z ′ be deﬁned as in line 2 of DistEst and GradEst. Lemma D.5 states that there exists a margin θ∗
14000rδ, we have that for at least 0.8k points Z ′
xt
x
µ
in [
i
−
k
θ∗ for some unit vector w∗, since the data are scaled by B. Furthermore, when the
it holds B
pruning step succeeds, Assumption 2.1 holds. This allows us to apply the key lemma (Lemma 5.2).

600rδ,
i, w∗
Z ′

+ 600rδ]. When

k
i | ≥

k −
· | h

(cid:16)p

k ≥

−

−

µ

µ

x

(cid:17)

k

k

(i) For GradEst, we use binary search in line 3 to ﬁnd θ = θ∗/B. By the guarantee of Lemma 5.2,
Bθ
10 for at least k/8 of the Zi.

θ
xt
10 for at least k/8 of the Zi. It follows that
| h
Thus, Lemma 4.1 implies that the output gt is a gradient estimate.

ii | ≥

w, Z ′

w, Zi

i | ≥

| h

−

20

(ii) By the same argument, we apply Lemma 4.1 and conclude that

dt of DistEst is a distance estimate.

Finally, we apply Lemma 3.1 for the guarantee of Descent.

b

Next we bound several failure probabilities of the algorithm. The ﬁrst three correspond to the global

conditions.

•

•

•

•

By Lemma 2.1, the Lugosi-Mendelson condition Assumption 2.1 fails with probability at most δ/8.

By Lemma B.2, the coordinate-wise median-of-means error bound fails with probability at most δ/8

By Lemma 5.1, the guarantee of our pruning and scaling procedure (Assumption 5.1) fails with prob-
ability at most δ/8.

Conditioned on above, the ApproxBregman satisﬁes the guarantee of the key lemma (Lemma 5.2).
The failure probability is at most δ/4Tdes each iteration. We take union bound over all these iterations.

Overall, the failure probability of the entire algorithm (Algorithm A.3) is bounded by δ via union bound.

The runtime follows from Lemma 5.2 which claims each iteration takes time

O(k2d) and the fact that

Tdes =

O(1).

e

H Interpretation of FHP algorithm Karnin et al. (2012) as regret minimization

e

Here, we review the bicriteria approximation algorithm of Karnin et al. Karnin et al. (2012) and show how
it can be interpreted in the multiplicative weights update (MWU) framework for regret minimization. Given
Z1, . . . , Zk

1, we study the following furthest hyperplane problem:

Rd such that

Zi

∈

k

k ≤

Find w

Sd−1

∈
Zi, w

subject to

r for i = 1, . . . , k,

i | ≥
where we are promised that there does indeed exist a feasible solution w∗. Since this problem is (provably)
hard (even to approximate) we will settle for bicriteria approximate solutions. By this, we simply mean
that we require the algorithm to output some w such that
[k]. For our
applications, the particular constants will not matter much, as long as they are actually constants.

r
10 for most of the i

Zi, w

i | ≥

| h

| h

∈

See Algorithm H.1 for a formal description. First we give some intuition and then we sketch the important

steps in the analysis.

H.1 Intuition

Because we are promised that w∗ exists, averaging the constraints yields:

1
k

k

h

i=1
X

Zi, w∗

2
i

≥

r2.

Note that if we deﬁne A1 as in Algorithm H.1, then the deﬁnition of singular vector tells us that:

max
w∈Sd−1 k

A1w

k

2 =

1
k

k

i=1
X

Zi, w
h

2
i

≥

1
k

k

i=1
X

Zi, w∗
h

2
i

≥

r2.

Thus, w1, the top singular vector as deﬁned in Algorithm H.1, satisﬁes the constraints on average. It could
r2 for all i
r2 but
= 4. To ﬁx this issue, we would simply down-
be the case that
weight Z4 in the next iteration, so that w2 aligns more with Zi for i
= 4. We repeat this several times, with
each wt improving upon wt−1.

Z4, w1i

Zi, w

2
i

≪

≫

h

h

2

At the end, the algorithm produces a collection of vectors w1, . . . , wT which each satisfy a certain
property. While it seems natural to just output wT as the ﬁnal answer, it turns out that this will not work.
Instead, we need to apply a randomized rounding procedure to extract a single vector w from w1, . . . , wT .

21

6
6
1. Input: Z1, . . . , Zk

∈

Rd and iteration count T

N.

∈

2. Initialize weights: τ1 = 1

k (1, . . . , 1)

Rk.

∈

3. For t = 1, . . . , T , repeat:

(a) Let At be the k

d matrix whose ith row is

τt(i)Zi and wt be the top right

unit singular vector of At.

×

Zi, wt

(b) Set σt(i) =
| h
(c) Reweight: τt+1(i) = τt(i)η−σ2
η. In MWU language, σ2

i |

.

(d) Normalize: Let Z =

p

t (i) for i

[k] for an appropriately chosen constant

∈
t is the loss vector at time t.
i∈[k] τt+1(i) and redeﬁne τt+1 ←

1
Z τt+1.

4. Output: w1, . . . , wT

P
Sd−1.

∈

Algorithm H.1: Iterative MWU procedure

H.2 Analysis
Lemma H.1. When Algorithm H.1 terminates after T = O( log k

r2 ) iterations, for every i

[k] it holds that:

∈

2

Zi, wt
h

i

≥

log k
log η

.

T

t=1
X

Proof. Algorithm H.1 is simply the MWU algorithm with the experts corresponding to the k constraints
and the loss of expert i at time t being σ2
t (i). Using the regret guarantee from Theorem 2.1 in Arora et al.
(2012) with respect to the ﬁxed expert ei and step size η:

Note that

and

T

τt, σ2
t

t=1
X

(cid:10)

−

(cid:11)

T

T

(1 + η)

ei, σ2
t

t=1
X

(cid:10)

T

log k
η

.

≤

(cid:11)

ei, σ2
t

=

t=1
X

(cid:10)

t=1
X

(cid:11)

Zi, wt
h

2
i

(H.1)

T

T

k

τt, σ2
t

=

τt(i)σ2

t (i)

t=1
X

(cid:10)

(cid:11)

t=1
X
T

i=1
X
k

τt(i)

τt(i)

h

h

Zi, wt

2
i

Zi, w∗

2
i

=

≥

t=1
X
T

i=1
X
k

t=1
X
T

i=1
X
k

(by deﬁnition of the algorithm)

(since w∗ is the top eigenvector)

τt(i)r2

≥

t=1
X
= T r2.

i=1
X

Substituting into and simplifying the regret formula and taking η = 1/3 gives the claim.

22

Given the previous lemma, we can just apply the rounding algorithm as a black-box to the output of

Algorithm H.1.

Lemma H.2 (Karnin et al. (2012)). Let α
(0, 1) and w1, . . . , wT be unit vectors satisfying the conclusion
of the previous lemma. Then with probability at least 1/147, the output w of the Rounding Algorithm F.1
satisﬁes

αr for at least a 1

3α fraction of i

[k].

Zi, w

∈

| h

i | ≥

−

∈

23

