1
2
0
2

l
u
J

0
3

]

G
L
.
s
c
[

1
v
1
6
0
0
1
.
8
0
1
2
:
v
i
X
r
a

Manuscript (2021)

2021.07.21

An Extensible and Modular Design and Implementation of
Monte Carlo Tree Search for the JVM

Larkin Liu
University of Toronto

Jun Tao Luo
Carnegie Mellon University

larkin.liu@mail.utoronto.ca

jtluo@andrew.cmu.edu

Abstract

Flexible implementations of Monte Carlo Tree Search (MCTS), combined with domain
speciﬁc knowledge and hybridization with other search algorithms, can be very powerful
for ﬁnding the solutions to problems in complex planning. We introduce mctreesearch4j, a
MCTS implementation written as a standard JVM library following key design principles of
object oriented programming. We deﬁne key class abstractions allowing the MCTS library
to ﬂexibly adapt to any well deﬁned Markov Decision Process or turn-based adversarial
game. Furthermore, our library is designed to be modular and extensible, utilizing class
inheritance and generic typing to standardize custom algorithm deﬁnitions. We demon-
strate that the design of the MCTS implementation provides ease of adaptation for unique
heuristics and customization across varying Markov Decision Process (MDP) domains. In
addition, the implementation is reasonably performant and accurate for standard MDP’s.
Furthermore, the nuances of diﬀerent types of MCTS algorithms are discussed.

Keywords: Monte Carlo Tree Search, Software Design, Markov Decision Process

1. Introduction

Sampling based strategies for the optimal planning of Markov Decision Processes (MDP)
and adversarial games have been of great importance to the improvement to discovery of
approximate solutions for complex high dimensionality domains where a full solution us-
ing complete methods are often infeasible. Algorithms such as Monte Carlo Tree Search
(MCTS) (Kocsis and Szepesv´ari, 2006) have displayed a promising ability to generate com-
petitive solutions for domains exhibiting high dimensionality search spaces.

In recent memory, MCTS used in combination with other game solving strategies have
shown to be the world’s state-of-the-art for high branching factor games such as Go (Silver
et al., 2016). Other algorithms also frequently apply MCTS to solve for games with high
branching factors such as Kriegspiel (Ciancarini and Favini, 2010) and Othello (Norvig,
1992). Our objective in this software is to create a extensible implementation of MCTS
which is adaptable into multiple domains, applicable to either single player of multi player
games.

©2021 Liu & Luo.

 
 
 
 
 
 
Liu & Luo 2021

1.1 Discrete Markov Decision Process

The Markov Decision Process (MDP), described in (Puterman, 1994), is a well-developed
framework for planning under uncertainty. The application of MDP’s can be found in
various optimization strategies to prescribe an optimal set of actions given a stochastic
environment where perfect observations are available. An M DP (cid:104)S, A, P, R(cid:105) designates a
set of states S, where the agent traverses from St to St+1, for a horizon of T in T distinct
time increments t (starting from t = 1). A is the action space, which is the set of actions
the agent can take at any t and may or may not be a function of t itself. P is the probability
space that outlines the transition probability space for state-to-state transitions for any
action taken at time t, P : {St × A × St+1} → {R ∈ R} subsequently returning a reward
value, which belongs to R.

In a fundamental MDP, the state at any time St is visible to the agent, and a corre-
sponding reward is associated with each state. Therefore, if the state is observable, the
value of each observable state is the reward associated with the state, and the availability
of actions leading to rewards in the future. Evidently, they can form a recursive pattern to
determine the value of the state, which can be solved via dynamic programming. Suppose
we use Q(St, a) to denote the immediate value when accounting for the immediate reward
as well as the transitioned next state St+1. Where the Value of any state is is denoted by
V (s).

Q(St, a) = R(St, a) + γ

(cid:88)

St+1∈S

P (St+1|St, a)V (St+1)

V (S) = argmax

Q(S, a)

a∈A

(1)

(2)

Traditionally the exact solution to Eq. (2) can be found using dynamic programming
algorithms such as Value Iteration or Policy Iteration (Bellman, 1957). Nevertheless, if
the state space or branching factor is so large that traditional solutions such as dynamic
programming cannot produce a solution within reasonable time or resource constraints,
approximate methods via MCTS is a viable alternative for producing approximate optimal
policies. The optimal policy denoted π(S, a), is therefore the action, a that maximizes Eq.
(2) at any particular state.

1.2 Monte Carlo Tree Search Algorithm

Monte Carlo Tree Search (MCTS) serves as an approach to solve the optimal policy for
any well-deﬁned MDP that does not always involve an exhaustive search through the entire
search space. It has the potential to provide feasible solutions to high dimension and high
branching MDP’s, where dynamic programming techniques are not feasible provided con-
straints on computing resources. (Chang et al., 2010) demonstrates the speciﬁc use of the
bandit algorithm, especially its ability to converge to the optimal policy of an MDP. Much
research, such as (Coquelin and Munos, 2007) and (Kocsis and Szepesv´ari, 2006), demon-
strates the eﬀective usage of MCTS to obtain π(S, a). The primary advantage of MCTS
over dynamic programming solutions of the MDP is that, when implemented correctly, the
number of iterations through the state space is drastically reduced. Nevertheless, the use

2

Monte Carlo Tree Search for the JVM

of DP or MCTS is not mutually exclusive, as the combination of both algorithms can yield
speciﬁc advantages and/or disadvantages (Feldman and Domshlak, 2014). The key notion
of MCTS is the ability to guide the MC search down the tree eﬃciently, which borrows
itself from the bandit strategy UCB1 (Auer et al., 2002).

U CT = E(S(cid:48)) + C

(cid:114)

2 ln n
n(cid:48)

(3)

MCTS primarily makes use of a deterministic selection of actions and resulting out-
comes to estimate the reward function of the system. MCTS is a tree search adaptation
of the UCB1 Multi-Armed Bandit Strategy (Auer et al., 2002). When performing one tree
traversal in MCTS, a series of actions is randomly played. This tree search is not entirely
random as it is guided by the UCB1 illustrated in Eq. (3). The MCTS algorithm is dis-
tinctly divided into 4-phases, Selection, Expansion, Simulation, and Backpropagation, which
are clearly illustrated in Fig. 1. In Selection, a policy deterministically selects which action
to play, based on previously expanded states. This selection is typically guided by the UCT
measure, from Eq. (3). In the Expansion phase, states that are unexplored, represented
by a leaf node, are added to the search tree one at time. Subsequently, in the Simulation
phase, a simulation is stochastically played out. Finally Backpropagation propagates the
ﬁnal reward of either a terminal state, or a node at an arbitrary depth limit, back to the
root node. This 4-phase process is repeated until a maximum number of iterations or a
convergence criteria is established.

Figure 1: Outline of MCTS - from (Browne et al., 2012).

2. Software Framework

mctreesearch4j is designed follow three key design principles. Firstly, we design for adapt-
ability. Adaptability is deﬁned as the ability for MDP domain to be easily integrated into
the mctreesearch4j framework with ease via class abstractions. Our implementation seeks
to simplify the adoption of MCTS solutions for a variety of domains. Secondly, we design a
software that is fully compatible with the Java Virtual Machine (JVM), for the reasons to
be discussed in Section 2.1. And lastly, we design to achieve a high degree of extensibility

3

Liu & Luo 2021

within the framework. Extensibility is the deﬁned as the ability for key mechanisms to be
reused, redeﬁned, and enhanced, without sacriﬁcing interoperability.

2.1 JVM Library

Open source implementations of MCTS exist, but have not gained widespread adoption.
(Cowling et al., 2012) presents an implementation in Python, where multiple game engines
can be plugged into the MCTS solver.
(Lucas, 2018) presents a simple implementation
written in Java, written with some categorization of the key MCTS mechanisms outlined
in Fig. 1. However, both implementations do not provide easy access to heuristics, nor do
they implement extensible and modular state action space abstractions. Chatzilygeroudis
(2016) presents a low-level implementation of MCTS in C++. Yet because of its low-level
implementation, it is not easy to interface a custom MDP deﬁnitions with the MCTS solver,
hindering its adaptability across domains. Some MCTS implementations are available in
Python, a widely adopted scripting language popular in Machine Learning, but suﬀer per-
formance issues from utilizing an interpreter, causing computing overhead at runtime. Some
workarounds do exist, such as dispatching to more performant libraries written in compiled
code, but this is additional complexity we desire to avoid. Therefore, compiled languages
such as Java or C are preferred for MCTS implementation primarily because of its more
eﬃcient resource consumption at runtime compared to native Python. Thus in the current
landscape, there is a lack of open source MCTS implementations that provides the full list
of features oﬀered by mctreesearch4j outlined in Section 2.

As of today, JVM language are widely adopted among many open source frameworks∗.
JVM languages are used in a variety of workloads including web, desktop and Android
applications etc. The long history of the Java language also contributes to the JVM exten-
sive knowledge base, language support and backwards compatibility. Furthermore, JVM is
known for its wide adoption particularly in the creation of mobile games, many of which
require online and/or oﬄine planning solvable via MCTS. Therefore, we chose to develop
an MCTS Framework for the JVM ecosystem.

The Kotlin programming language was chosen for the development of mctreesearch4j.
Kotlin features a more modern syntax that is succinct and expressive, containing powerful
primitives, while simultaneously supporting a wide range of programming paradigms. This
language is also popular among many software developers, particularly for the Android
mobile operating system, a platform where mctreesearch4j may ﬁnd suitable adoption. The
full compatibility of Kotlin with other JVM languages also allows the adoption by other
languages within the JVM ecosystem. In addition Kotlin, along with many JVM languages,
has strong support for type generics enabling us to build and reuse functions independent of
types. It also oﬀers better development experience with compile time type checks without
explicit casts.

2.2 MDP Domain Abstraction

Attempts to rigorously deﬁne MDP interfaces to plug into custom MDP solvers have been
attempted in the past. For example, RDDL (Sanner, 2010) provides an interface for a formal

∗https://www.tiobe.com/tiobe-index/

4

Monte Carlo Tree Search for the JVM

Listing 1 Deﬁnition of the MDP abstract class.
abstract class MDP<StateType, ActionType> {

abstract fun transition(StateType, ActionType) : StateType
abstract fun reward(StateType, ActionType?, StateType) : Double
abstract fun initialState() : StateType
abstract fun isTerminal(StateType) : Boolean
abstract fun actions(StateType) : Collection<ActionType>

}

1

2

3

4

5

6

7

MDP deﬁnition, but it is rather complex for straightforward software implementations, and
also adds additional overhead by introducing a new programming language speciﬁcally for
the deﬁnition of MDP’s. We simplify the formal MDP deﬁnition by abstracting the MDP
into its base characteristics outlined in Listing 1.

The main abstraction that is used to deﬁne an MDP problem is the abstract class
deﬁned in Listing 1, using generic type variables. Each of the methods correspond to
speciﬁc behaviour of a discrete MDP. In mctreesearch4j we use generic types StateType
and ActionType to represent the MDP states and actions respectively. This abstract class
has ﬁve members that must be implemented. These abstract class methods deﬁne the
functionality of an MDP. The MDP abstraction will be used by core MCTS solvers to
compute the optimal policy. The MDP interface can be written in any JVM language, we
use Kotlin and Scala for this paper, with the Scala implementation from (Liu, 2021).

mctreesearch4j is designed to easily adapt to a variety of MDP deﬁnitions without com-
plex mathematical constructs. The computational behaviour of mctreesearch4j is deﬁned
separately by user code outside of the core library, for example via a game controller, see
Fig. 2. This allows programmers to deﬁne complex ﬂexible systems, integrating seamlessly
into turn-based game applications, including single and multi agent turn-based games such
as 2048 and Reversi (Othello), respectively. In the user code layer of the framework, the
programmer can deﬁne custom solvers, where unique heuristics can be implemented to add
domain speciﬁc knowledge to improve the performance of the base MCTS solver. Thus the
adaptability of the solver to a wide variety of MDP domains is one key design principle of
mctreesearch4j, see Section 3 for an overview of adapted domains.

mctreesearch4j is able to use the MDP deﬁnitions to execute a tree search over the
state action space. The MDP deﬁnitions used to interface with mctreesearch4j should opti-
mize its save and load functionalities, as it can impact MCTS search performance if poorly
constructed. Nevertheless, deﬁning the MDP space programmatically gives the strong ad-
vantage of having the ability to deﬁne complex MDP’s without the need for rigorous math-
ematical deﬁnitions. In our implementation, we aim to deﬁne a clear common declarative
interface to provide abstractions for the solver and the MDP representation. The MDP
abstraction ensures that the user deﬁnes the MDP scenario consistently such that it can be
queried by the default solver provided in the library.

5

Liu & Luo 2021

Figure 2: Inheritance and class structure of mctreesearch4j.

2.3 MCTS Solver Design

mctreesearch4j also implements the core Monte Carlo Tree Search algorithm in an extensible
paradigm, maximizing the ﬂexibility of the implementation by following Object Oriented
Programming (OOP) principles. This is useful because it is possible to run MCTS with
customized key mechanisms, and it is highly eﬃcient if the programmer is able to reuse,

6

Monte Carlo Tree Search for the JVM

Listing 2 Key Mechanisms of an MDP Solver
abstract class Solver<ActionType, NodeType>(

protected val verbose: Boolean,
protected val C: Double // Exploration Constant
) where NodeType : Node<ActionType, NodeType> {

protected abstract var root: NodeType
abstract fun select(node: NodeType) : NodeType
abstract fun expand(node: NodeType) : NodeType
abstract fun simulate(node: NodeType) : Double
abstract fun backpropagate(node: NodeType, reward: Double)
open fun runTreeSearch(iterations: Int) {/* Predefined method */ }

1

2

3

4

5

6

7

8

9

10

11

}

and/or redeﬁne complete or partial segments of the key mechanisms. In Listing 2 we deﬁne
such key mechanisms, all of which are inherited, and can be redeﬁned or enhanced. This is
important both for the eﬃciency of development, as well as the interoperability of the key
MCTS mechanisms.

Listing 2 deﬁnes four abstract methods which are used to implement MCTS as illus-
trated in Fig. 1. The 4 phases, Selection, Expansion, Simulation, and Backpropagation
deﬁned in lines 6 to 9 serve as the key mechanisms to run mctreesearch4j and the class
method fun runTreeSearch() is a predeﬁned method from the base class Solver() re-
sponsible for executing the key MCTS solver mechanisms deﬁned in Lines 6 to 9. These
open functions, which can be overridden in subclasses, provide the ﬂexibility to deﬁne the
implementation of the solver that is appropriate for the MDP domain. Furthermore, we
introduce the NodeType which is an abstract representation of the search space.
It de-
notes how the state action space is represented in stages of the MCTS search, and must be
explicitly deﬁned by a subclass.

mctreesearch4j provides a default implementation known as class GenericSolver, and
an alternate class StatefulSolver. The abstract class Solver serves as the base class
for both versions, and deﬁnes a set of functionalities that all solver implementations must
provide as well as a set of extensible utilities. Similar to the MDP abstraction, the solver
uses a set of type parameters to provide strongly typed methods that unify and simplify the
MCTS implementation. The programmer is free to select the data type or data structure
that best deﬁnes how states and actions are represented in their MDP domain. Thus we
can infer that, Generic and Stateful solvers have diﬀerent representations of the NodeType.
The diﬀerentiation lies in their respective memory utilization of abstract node types to
track states during MCTS iterations. The default class GenericSolver provides a leaner
implementation, where actions are tracked and no explicit states are stored permanently.
The states tracked with class GenericSolver are dynamic and the MDP transitions must
be rerun when traversing the search tree during selection in order to infer the state. The
class StatefulSolver keeps an explicit record of the visited states, and additional infor-
mation, such as terminality and availability of downstream actions. The extra overhead of
storing the state explicitly in the MCTS node, allows the algorithm to optimize its search

7

Liu & Luo 2021

Listing 3 Generic Solver Selection.
fun select(node: ActionNode): ActionNode {

if (node.getChildren().isEmpty()) {return node}
var currentNode = node
simulateActions(node)
while (true) {

if (mdp.isTerminal(currentNode.state)) {return currentNode}
val currentChildren = currentNode.getChildren()
if (currentNode.isNotFullyExplored()) {return currentNode}
currentNode = currentChildren.maxByOrNull{a -> calculateUCT(a)}
simulateActions(currentNode)

}

}

1

2

3

4

5

6

7

8

9

10

11

12

using information from previously visited states. This is particularly useful for deterministic
games, where a re-computation of the MDP transition is not necessary to determine the
state of the agent after a particular taking a speciﬁc action. This diﬀerentiation results in
diﬀerent implementations of the Selection step, while maintaining identical implementations
of Expansion, Simulate and Backpropagation. Both implementation iterates the algorithm
in the most common pattern of Selection, Expansion, Simulation, Backpropagation until a
certain number of iterations.

2.3.1 Generic Solver Design

The Generic solver provides an MCTS solution where the states are not explicitly stored in
the search tree, but instead inferred from the actions taken by the agent. Generic solvers
are more memory eﬃcient, and are preferred for scenarios of high state State-Action-State
(SAS) branching factor. We deﬁne SAS branching factor as the average number of possible
states that the agent can arrive at, given any action from any state. For stochastic domains,
any action can lead to a multitude of states, so storing states explicitly in the search tree
can be infeasible. In this case the Generic solver is preferred as it obviates the need to track
states explicitly. A downside of the Generic solver is that additional computation is required
to infer the state of the agent during the Selection phase of MCTS, as accomplished via
the method on Line 4 of Listing 3. The Generic solver can also be extended to Partially
Observable MDP’s (POMDP’s), where we abstract away the state space altogether, and
solve over the estimation of state space as inferred by the action reward history, this is also
known as the Belief MDP approach. In the Belief MDP, a POMDP is simpliﬁed into an
MDP, by ignoring the potential discrepancy between the observation and true state, treating
the current observation as the true state. This can create a tractable MDP solution at the
cost of strict accuracy, and is in principle, fully programmable into mctreesearch4j via the
Generic solver.

The most signiﬁcant diﬀerence is the trade-oﬀ between memory and runtime. The
Generic solver does not track the possible states for each node of the search tree and
therefore reduces the memory consumption. The key beneﬁt is the ability to track the state

8

Monte Carlo Tree Search for the JVM

Listing 4 Stateful Selection.
fun select(node: StateNode): StateNode {

if (mdp.isTerminal(node.state)) {return node}
if (node.validActions.size > node.exploredActions.size) {return node}
var bestAction = exploredActions.getActionUCT(node, node.getChildren())
val newState = mdp.transition(node.state, bestAction)
val actionState = node.getChildren(bestAction)

.firstOrNull { s -> s.state == newState }
?: return createNode(node, bestAction, newState)

return select(actionState)

1

2

3

4

5

6

7

8

9

10

}

space with actions alone. A disadvantage, is that the agent must replay the game engine to
approximate the state at any time, which is where a computational bottleneck can arise.

2.3.2 Stateful Solver Design

We provide an alternate solver design targeted for MDP domains with low SAS branching
factors. In StatefulSolver the nodes of the solver explicitly track the state, the metadata
contained within the state. An NodeType is used for Stateful search trees where the node
explicitly contains both the state of the agent, and the inducing action that led to said
state. The Stateful solver is computationally eﬃcient when the SAS branching factor is
feasible, as it is able to store and look up the current state of the agent and continue MCTS
iterations from the recorded state. Stateful solvers are generally preferred for low state
space MDP’s, where the SAS branching factor, deﬁned earlier in Section 2.3.1, is relatively
manageable, such as in games where low SAS branching factors are exhibited. However,
this optimization will incur additional memory usage.

Listing 4 illustrates the Selection and Expansion algorithm for Stateful MCTS. The
Stateful solver keeps track of all possible states at each node illustrated in Lines 2 to 10. In
the Selection phase, the Stateful solver iterates until a leaf node is reached.

2.3.3 Expansion, Simulation and Backpropagation

The Expansion, Simulation, and Backpropagation steps for Generic and Stateful solvers
remain virtually identical, notwithstanding speciﬁcation of a diﬀerent NodeType for each
solver, also resulting minor nuances in instantiating said nodes in the Expansion phase. The
base NodeType, which is used by both the Generic and Stateful Solvers, store key metadata
such as the inducing action, expected reward, and number of visits. The fun expand()
method randomly selects an unexplored node, and adds it to the search tree. This is done
using the fun createNode() method, which diﬀers between Generic and Stateful solvers.
In the Generic solver, no actual MDP state is required to create a new node of type
ActionNode. The current state of the agent is computed based on the sequence of actions
taken utilizing the speciﬁed mdp object, deﬁned in Listing 1. The Stateful solver, on the
other hand, explicitly requires the tracking of states when creating a StateNode via the
fun createNode() method. The fun simulate() method runs a purely randomized ex-

9

Liu & Luo 2021

ploration of the MDP state-action space from a speciﬁc state, until a terminal state or
the simulation depth is reached. The fun backpropagate() method is used to update the
reward values for the node where the simulation was run as well as all of its ancestors.

2.3.4 Extensibility and Heuristics

Though the default MCTS implementation described in Section 1.2 works well in many
scenarios, there are situations where knowledge about speciﬁc problem domains can be
applied to improve the MCTS performance. Improvements to MCTS, such as heuristics
driven simulation, exploit domain knowledge to improve solver performance performance.
We demonstrate that a Reversi AI that uses heuristics derived from (Guenther and Klinik,
2004) is able to outperform the basic MCTS implementation. These heuristics are pro-
grammed via extensibility points in the mctreesearch4j solver implementation, where the
key mechanisms can be altered or augmented. Our heuristic, illustrated in Fig. 3, in-
troduces the heuristicWeight array, a 2D array storing domain speciﬁc ratings of every
position on a Reversi board representing the desirability of that position on the board.
The negative numbers represent a likely loss and positive numbers representing a likely
win, again as represented in Fig. 3. This value is taken into consideration when traversing
down the simulation tree. The heuristicWeight array adjusts the propensity to explore
any position for both agents based on the heurisitc’s belief about the desirability of the
position. The eﬀectiveness of this heuristic is later presented in Section 3.3. To alter the
MCTS simulation phase we override the fun simulate() method and create a new deﬁni-
tion for it. The application of this heuristicWeight only requires minor alterations to the
fun simulate() method, as illustrated from Lines 5 until 15 in Listing 6.

Figure 3: Reversi heuristicWeight lookup table.

The default solver behaviour continues running the MCTS iteration until the iteration
count is reached. This may not be the best criteria and convergence metrics can be used to
better determine when to stop the tree construction. For example, if no signiﬁcant improve-
ments in the best action’s score is made, the algorithm can be terminated early. Additional
functionality can also be added to the solver in the tree construction function. For example,

10

Monte Carlo Tree Search for the JVM

Listing 5 Expansion, Simulation and Backpropagation pseudo-code.
fun expand(node: StateNode): StateNode {

if (node.isTerminal) {return node}
val unexploredActions = node.getUnexploredActions()
val actionTaken = unexploredActions.random()
return createNode(node, actionTaken)

}

fun simulate(node: NodeType): Double {

if (mdp.isTerminal(node.state)) {

return mdp.reward(node.parent?.state, node.inducingAction, node.state)

}
var depth = 0
var currentState = node.state
var discount = rewardDiscountFactor
while(true) {

val validActions = mdp.actions(currentState)
val randomAction = validActions.random()
val newState = mdp.transition(currentState, randomAction)
if (mdp.isTerminal(newState)) {

return mdp.reward(currentState, randomAction, newState) * discount

}
currentState = newState
depth++
discount *= rewardDiscountFactor
if (depth > simulationDepthLimit) {

return mdp.reward(currentState, randomAction, newState) * discount

}

}

}

fun backpropagate(node: NodeType, reward: Double) {

var currentNode = node
var currentReward = reward
while (true) {

currentStateNode.maxReward = max(currentReward, currentNode.maxReward)
currentStateNode.reward += currentReward
currentStateNode.n++
currentStateNode = currentStateNode.parent ?: break
currentReward *= rewardDiscountFactor

}

}

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

11

Liu & Luo 2021

Listing 6 Heuristic implementation in Reversi.
override fun simulate(node: NodeType): Double {

/*... Original Simulation code ...*/
while(true) {

val validActions = mdp.actions(currentState)
var bestActionScore = Int.MIN_VALUE // Begin heuristic

var bestActions = mutableListOf<Point>()
for (action in validActions) {

val score = heuristicWeight[action.x][action.y]
if (score > bestActionScore) {
bestActionScore = score
bestActions = mutableListOf(action)

}
if (score == bestActionScore) {bestActions.add(action)}

}

val randomAction = bestActions.random() // End heuristic
val newState = mdp.transition(currentState, randomAction)
/*... Original Simulation code ...*/

}

}

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

if additional metrics need to be measured after each MCTS iteration, mctreesearch4j can
be overridden to add this functionality. For example, the exploration term, C, from Eq. 3
can be exported via a class extension that tracks the exploration term.

3. Evaluation & Function

We evaluate our MCTS implementation on its ability to converge to an accurate solution.
In our experiments, the MDP has one or many optimal solutions, and the solver should
accurately converge to these solutions. We also measure the performance of the implemen-
tation by the average time to extract one optimal action taken from any state, we denote
this simply as runtime per decision or ∆d. ∆d is an estimate of the time to make a decision
averaged over 100 trials. This can vary from computer to computer, but serves as a rough
estimate. We demonstrate that mctreesearch4j is adaptable across multiple domains, in
addition to the fact that ∆d can be kept at manageable levels for operation on a common
laptop computer. The exact hardware speciﬁcations are outlined in Appendix A.1,

For adversarial search domains featuring two agents, the terminal state of victory or
loss results in a score of +1 or -1 respectively, from the perspective of the agent when
competing against the opponent. Later in Section 3.3 we demonstrate a adversarial setup
where 2 MCTS solvers with novel characteristics, play against each other.

We examine the performance of the MCTS solver across ﬁve distinct MDP domains.
These domains are GridWorld, a probabilistic single agent MDP illustrated in Fig. 4, the
game of 2048 analyzed in Kohler et al. (2019), Push Your Luck (PYL), a single agent MDP

12

Monte Carlo Tree Search for the JVM

deﬁned in Geißer and Speck (2018), the game of Connect 4, a deterministic multi-agent
game, and the game of Reversi, a deterministic adversarial multi-agent game.

3.1 Solver Runtime per Domain

Domain

No. Agents Stochastic Avg. Br. Fac. Avg. Depth Solver ∆d

GridWorld
PYL
2048
Connect4
Reversi

1
1
1
2
2

yes
yes
yes
no
no

4
2
4
4
10

5
∞
940
36
58

Gen.
Gen.
Gen.
SF
SF

0.1
0.3
0.5
0.06
0.12

Table 1: MDP domain characteristics, where ∆d is in seconds.

Furthermore, we select either Generic or Stateful solvers based on the characteristics of
the MDP domain, with the considerations outlined in Sections 2.3.2 and 2.3.1. The max
tree depth of the MCTS was set to 1000, this number typically far exceeds the average
depth before reaching a terminal state for most MDP domains, with the exception of 2048.
The number of iterations set to 500 arbitrarily. The MDP domains, where mctreesearch4j
was applied, contain one or two agents, and state action transitions can be stochastic or
deterministic. We also note the average branching factor of the domain (Avg. Br. Fac.), and
average depth (Avg. Depth)† of the search tree until a terminal state is reached. For some
games, such as GridWorld, average depth is heavily determined by the state conﬁguration,
we refer to the conﬁguration set out in Fig 4. We record the average runtime to compute
the immediate next optimal action from an arbitrary state in the state space of the MDP
domain in milliseconds‡.

3.2 Solver Convergence

To demonstrate the accuracy of mctreesearch4j, we examine the performance of the solver
with respect to convergence of rewards, convergence of exploration terms, and visiting of
the optimal state subspace more than non-optimal state subspace with increasing iterations.
Variables such as the exploration constant C, as denoted in Eq.
(3), aﬀect the overall
exploration of mctreesearch4j. If C is too high, there can be over-exploration causing the
UCT function to swing between optimal actions and non-optimal action selection. Over-
exploration aﬀects the smoothness of the exploration term convergence (James et al., 2017).
If C is too low, then it is not able to suﬃciently explore all states to ﬁnd an optimal action.
We provide a simple example of convergence evaluation in the domain of Gridworld.
The grid displayed in Fig. 4, represents the state space, and each cell demarcates the state
which corresponds to a reward value (unﬁlled cell indicates a reward of 0). In our example
any non-zero state represents a terminal state. The agent represented by ♦, has 4 possible
actions from its starting point. In the speciﬁc conﬁguration shown by Fig. 4, the MCTS

†https://en.wikipedia.org/wiki/Game_complexity illustrates Avg. Br. Fac. and Avg. Depth.
‡The exact speciﬁcation of the computer used to run the simulation is contained in Appendix A.1.

13

Liu & Luo 2021

solver should ultimately select the actions ← or ↓, which are optimal, and mctreesearch4j
should accurately converge to the appropriate values.

-1

+5

♦

-1

Figure 4: Illustration of the Gridworld MDP domain. An agent, is represented by ♦, lies in
a domain with terminal state reward +5, and terminal reward negative state -1.

Figure 5: Convergence of exploration terms

Our MCTS solver§, with no additional heuristics, is capable of converging to the correct
solution space. Fig. 5 displays a convergence of the exploration term C(cid:112)2 log n/n(cid:48), which
decreases as n → ∞, and n(cid:48) ≤ n. Provided this, the correct optimal policy will yield smaller
exploration terms for optimal versus non-optimal solutions because the number of visits to
the non-optimal nodes, n(cid:48), will be generally smaller than the optimal nodes.

§The ﬁgures 5 to 7 represent solutions of the Stateful solver, nevertheless convergence plots using Generic

solver yields similar behaviour.

14

Monte Carlo Tree Search for the JVM

Figure 6: Convergence of rewards

Naturally, the reward value associated with each optimal state induced by the optimal
policy action at t + 1 should yield higher backpropagated rewards. In Fig. 6 we see that
the two optimal actions produce higher reward values than non-optimal solutions.

Figure 7: Convergence of visits

When the MCTS solver is accurately selecting the optimal solutions, it will continue to
cause the agent to explore in the optimal subset of the state space, and reduce its exploration
in the non-optimal subset of the state space as evidenced in Fig. 7. The cumulative number
of visits corresponding to the optimal policy is proportionally increasing with respect to the
number of MCTS iterations. Whereas for non-optimal solutions, the cumulative visits are
signiﬁcantly less because the solver will avoid visiting the non-optimal state subspace.

15

Liu & Luo 2021

3.3 Adversarial Game Benchmarking

We demonstrate the possibility of heuristic adaptation to improve mctreesearch4j in an
adversarial game. For this, we create a two-player game in the deterministic multiplayer
domain of Reversi, where our AI will play as adversaries against each other. The Reversi
heuristic is implemented to enable the search algorithm to compute more realistic estimates
of reward, using speciﬁc domain knowledge (Guenther and Klinik, 2004). The intended
eﬀect of this heuristic is to enable a more eﬃcient search of the state action space, as
presented earlier in Section 2.3.4

The heuristic augmented mctreesearch4j is played against the original mctreesearch4j to
prove that the adversarial MDP solver is capable of improving via domain knowledge. Table
2 demonstrates that when mctreesearch4j is augmented with heuristics it defeats the original
mctreesearch4j solver up to 75% of the time, under ceteris paribus conditions. Nevertheless
this win rate metric depends on several factors. Firstly, we see that the heuristic solver is
only marginally better than the original solver when we limit the maximum simulation depth
(Max. Sim. Depth), and the number of simulation iterations (No. Iter). The limitation
on these two hyper-parameters limit the MCTS solver’s ability to explore the entire state
action space to determine accurate rewards, thereby also limiting the eﬀectiveness of the
heuristic. The degree of contrast between mctreesearch4j with heuristic and without is
greatly enhanced when the solver is provided enough simulation iterations and simulation
depth to fully take advantage of the additional domain information.

No. Iter Max. Sim. Depth No. Wins No. Loss No. Ties Win Rate

50
500
50
500

40
40
1000
1000

98
121
111
151

87
75
75
46

15
4
14
3

0.49
0.605
0.555
0.755

Table 2: Reversi results comparing MCTS solver with heuristics and without.

Via this experiment, we justify that our MCTS solver is capable of being modiﬁed to
adapt new heuristics which improve the outcomes of the MDP policy for an adversarial
game. The heuristic presented here actively biases the random simulation of MCTS to
favour actions which induce better outcomes, based on domain knowledge. We also see
that as we adjust hyper-parameters of the model, such as simulation depth and number of
iterations, the heuristic augmented solver can become more eﬀective.

4. Conclusion & Future Work

Due to the extensibility and modularity of mctreesearch4j, many improvements and experi-
ments could be performed by extending and modifying the capabilities of the base software.
mctreesearch4j is designed to enable researchers to experiment with various MCTS strate-
gies while standardizing the functionality of the MCTS solver and ensuring reliability, where
the experiments are reproducible and the solver is compatible with common JVM runtimes
¶.

¶To be speciﬁc mctreesearch4j is fully compatible with Java Runtime Environment (JRE) 8 and 11.

16

Monte Carlo Tree Search for the JVM

4.1 Future Work

We encourage the eﬀort to enhance mctreesearch4j further. For example, while the default
implementations create a new node and adds it to the node chosen by the Selection phase,
it is possible to rely on domain knowledge to avoid superﬂuous game states. For example, in
Gridworld, where each unique position is functionally the same regardless of how it arrived
to that position, there is no need to add a newly created node to the search tree. Instead, a
cache of nodes representing known positions can be used to dramatically reduce the MCTS
search space, yielding more accurate results. Furthermore, creating an mapping for more
expressive and rigorous MDP deﬁning meta-languages such as RDDL (Sanner, 2010) to bind
to mctreesearch4j, could be an important feature in the near future. This enables complex
and rigorous deﬁnitions of MDP’s to be benchmarked using mctreesearch4j, opening many
new opportunities for research.

Also, as described in Baier and Winands (2013), some games can be solved more eﬃ-
ciently if Minimax algorithms can be applied in the Selection phase. Instead of the default
solver which uses UCT to select leaf nodes, a Minimax algorithm can be used instead to
detect shallow wins and losses. This has been shown to work well for adversarial games to
avoid traps set by the opposite player. For adversarial games, another common strategy
is to improve simulation accuracy by incorporating a Minimax algorithm for the Simula-
tion phase. This ensures the rewards computed are more realistic than two players who
play randomly. This can signiﬁcantly improve the MCTS performance. In addition, the
backpropagation step can be augmented with Minimax concepts to mark known losing po-
sitions. This allows subsequent Selection phase to ignore parts of the search tree and speed
up convergence (Baier and Winands, 2013). Finally, instead of running simulated games to
compute a reward, it is possible to rely on an external source to predict the reward value
of a given state. An example of this is the state-of-the-art Go AI, AlphaGo Silver et al.
(2016), which uses Deep Learning trained on expert positions and via self-play to compute
the expected reward for a given game position. Combining Deep Learning techniques with
mctreesearch4j to provide external intelligence is an open avenue of research.

The phase order of MCTS deﬁned in the base class Solver() can also be modiﬁed to
explore diﬀerent avenues. Although the most common MCTS phase order of Selection →
Expansion → Simulation → Backpropagation works well in most scenarios, the MCTS
phase order can be altered to Selection → Simulation → Expansion → Backpropagation as
described in Winands et al. (2010). The advantages and/or disadvantages of altering the
MCTS phase order is an under explored topic.

4.2 Summary

In closing, mctreesearch4j presents a framework which enables programmers to adapt an
MCTS solver to a variety of MDP domains. This is important because software application
was a main focus of mctreesearch4j. Furthermore, mctreesearch4j is fully compatible with
JVM, and this design decision was made due to the excellent support of class structure
and generic variable typing in Kotlin, and other JVM languages, as well as support for
mobile applications. Yet most importantly, mctreesearch4j is modular and extensible, the
key mechanism of MCTS are broken down, and the programmer is able inherit class charac-

17

Liu & Luo 2021

teristics, redeﬁne and/or re-implement certain sections of the algorithm while maintaining
a high degree of MCTS standardization.

4.3 Software Availability

• https://mvnrepository.com/artifact/ca.aqtech/mctreesearch4j contains the com-
piled core library for mctreesearch4j available via the Maven Central Repository for
the JVM ecosystem.

• http://mctreesearch4j.aqtech.ca/ contains the source code for implementation
of mctreesearch4j core library, as well as programs for various domains and game
controllers.

Acknowledgements

We would like to acknowledge the kind support of our reviewers, and Maven Central Repos-
itory for repository hosting of our code base.

Appendix A.

A.1 Simulation Hardware Speciﬁcations

All of the benchmarks run in Section 3 were performed on an Apple Macbook Pro, containing
a 2.6GHz dual-core Intel Core i5 processor with 3MB shared L3 cache with 8GB of 1600MHz
DDR3L onboard memory. We expect that a computer with similar hardware capabilities
will yield similar results.

References

Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Mach. Learn., 47(2-3):235–256, May 2002. ISSN 0885-6125. doi: 10.
1023/A:1013689704352. URL https://doi.org/10.1023/A:1013689704352.

Hendrik Baier and Mark H. M. Winands. Monte-carlo tree search and minimax hybrids. In
2013 IEEE Conference on Computational Inteligence in Games (CIG), pages 1–8, 2013.
doi: 10.1109/CIG.2013.6633630.

Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics,

6(5):679–684, 1957. URL http://www.jstor.org/stable/24900506.

Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter I. Cowling,
Stephen Tavener, Diego Perez, Spyridon Samothrakis, Simon Colton, and et al. A survey
of monte carlo tree search methods. IEEE TRANSACTIONS ON COMPUTATIONAL
INTELLIGENCE AND AI, 2012.

Hyeong Soo Chang, Jiaqiao Hu, Michael C. Fu, and Steven I. Marcus. Adaptive ad-
versarial multi-armed bandit approach to two-person zero-sum markov games.
IEEE
Trans. Autom. Control., 55(2):463–468, 2010. doi: 10.1109/TAC.2009.2036333. URL
https://doi.org/10.1109/TAC.2009.2036333.

18

Monte Carlo Tree Search for the JVM

Konstantinos Chatzilygeroudis. Monte carlo tree search - c++. https://github.com/

resibots/mcts, 2016.

Paolo Ciancarini and Gian Piero Favini. Monte carlo tree search in kriegspiel. Arti-
ﬁcial Intelligence, 174(11):670–684, 2010.
ISSN 0004-3702. doi: https://doi.org/10.
1016/j.artint.2010.04.017. URL https://www.sciencedirect.com/science/article/
pii/S0004370210000536.

Pierre-Arnaud Coquelin and R´emi Munos. Bandit algorithms for tree search. CoRR,

abs/cs/0703062, 2007. URL http://arxiv.org/abs/cs/0703062.

Peter Cowling, Edward Powley, and Daniel Whitehouse. Information set monte carlo tree
search. IEEE Transactions on Computational Intelligence and Ai in Games, 4:120–143,
06 2012. doi: 10.1109/TCIAIG.2012.2200894.

Zohar Feldman and Carmel Domshlak. Monte-carlo tree search: To MC or to dp? In Torsten
Schaub, Gerhard Friedrich, and Barry O’Sullivan, editors, ECAI 2014 - 21st European
Conference on Artiﬁcial Intelligence, 18-22 August 2014, Prague, Czech Republic - Includ-
ing Prestigious Applications of Intelligent Systems (PAIS 2014), volume 263 of Frontiers
in Artiﬁcial Intelligence and Applications, pages 321–326. IOS Press, 2014. doi: 10.3233/
978-1-61499-419-0-321. URL https://doi.org/10.3233/978-1-61499-419-0-321.

Florian Geißer and David Speck. Prost-dd-utilizing symbolic classical planning in thts.

2018.

Mathias Guenther and Jobas Klinik.

O-thell-us – an
https://courses.cs.washington.edu/courses/cse573/04au/Project/

A new experience:

ai project.
mini1/O-Thell-Us/Othellus.pdf, 2004.

Steven James, George Konidaris, and Benjamin Rosman. An analysis of monte carlo
tree search, 2017. URL https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/
14886.

Levente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In Johannes
F¨urnkranz, Tobias Scheﬀer, and Myra Spiliopoulou, editors, Machine Learning: ECML
2006, pages 282–293, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-
540-46056-5.

Iris Kohler, Theresa Migler, and Foaad Khosmood. Composition of basic heuristics for
the game 2048. In Proceedings of the 14th International Conference on the Foundations
of Digital Games, FDG ’19, New York, NY, USA, 2019. Association for Computing
Machinery. ISBN 9781450372176. doi: 10.1145/3337722.3341838. URL https://doi.
org/10.1145/3337722.3341838.

Larkin Liu.

Connect 4 implementation in scala.

https://github.com/larkz/

connect4-scala, commit = 4549c00398e7987814d83a2bd0760bbaedeb879b, 2021.

Simon Lucas. Montecarlots - java. https://github.com/DrumerJoe21/MonteCarloTS,

2018.

19

Liu & Luo 2021

Peter Norvig. Paradigms of Artiﬁcial Intelligence Programming: Case Studies in Common
Lisp. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 1992.
ISBN 1558601910.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. John Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779.

Scott Sanner. Relational dynamic inﬂuence diagram language (rddl): Language description,

2010.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-
tot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,
Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis
Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature,
529(7587):484–489, January 2016. doi: 10.1038/nature16961.

M. H. M. Winands, Y. Bjornsson, and J. Saito. Monte carlo tree search in lines of action.
IEEE Transactions on Computational Intelligence and AI in Games, 2(4):239–250, 2010.
doi: 10.1109/TCIAIG.2010.2061050.

20

