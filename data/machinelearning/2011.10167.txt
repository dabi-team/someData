0
2
0
2

v
o
N
0
2

]

C
O
.
h
t
a
m

[

1
v
7
6
1
0
1
.
1
1
0
2
:
v
i
X
r
a

Proceedings of Machine Learning Research vol xxx:1–19, 2021

When to stop value iteration:
stability and near-optimality versus computation

Mathieu Granzotto†
†Université de Lorraine, CNRS, CRAN, F-54000 Nancy, France.

MATHIEU.GRANZOTTO@UNIV-LORRAINE.FR

Romain Postoyan†

ROMAIN.POSTOYAN@UNIV-LORRAINE.FR

Dragan Neši´c
DNESIC@UNIMELB.EDU.AU
Electrical and Electronic Engineering Department, University of Melbourne, Parkville, VIC 3010, Australia.

Lucian Bu¸soniu
Department of Automation, Technical University of Cluj-Napoca, Memorandumului 28, 400114, Romania.

LUCIAN.BUSONIU@AUT.UTCLUJ.RO

Jamal Daafouz†

JAMAL.DAAFOUZ@UNIV-LORRAINE.FR

Abstract
Value iteration (VI) is a ubiquitous algorithm for optimal control, planning, and reinforcement
learning schemes. Under the right assumptions, VI is a vital tool to generate inputs with desirable
properties for the controlled system, like optimality and Lyapunov stability. As VI usually requires
an inﬁnite number of iterations to solve general nonlinear optimal control problems, a key ques-
tion is when to terminate the algorithm to produce a “good” solution, with a measurable impact
on optimality and stability guarantees. By carefully analysing VI under general stabilizability and
detectability properties, we provide explicit and novel relationships of the stopping criterion’s im-
pact on near-optimality, stability and performance, thus allowing to tune these desirable properties
against the induced computational cost. The considered class of stopping criteria encompasses
those encountered in the control, dynamic programming and reinforcement learning literature and
it allows considering new ones, which may be useful to further reduce the computational cost while
endowing and satisfying stability and near-optimality properties. We therefore lay a foundation to
endow machine learning schemes based on VI with stability and performance guarantees, while
reducing computational complexity.

1. Introduction

Value iteration (VI) is an established method for optimal control, which plays a key role in reinforce-
ment learning (Sutton and Barto, 2017; Lewis and Vrabie, 2009; Bu¸soniu et al., 2018; Pang et al.,
2019). This algorithm consists in iteratively constructing approximations of the optimal value func-
tion, based on which near-optimal control inputs are derived for a given dynamical nonlinear sys-
tems and a given stage cost. The convergence of said approximations to the optimal value function
is established in, e.g., (Bertsekas, 2012, 2017) under mild conditions. To beneﬁt from this conver-
gence property, VI often needs to be iterated inﬁnitely many times. However, in practice, we cannot
do so and must stop iterating the algorithm before to manage the computational burden, which may
be critical in online applications. Heuristics are often used in the literature to stop iterating by com-
paring the mismatch between the value functions obtained at the current step and at the previous
one, see, e.g., (Bertsekas, 2012; Sutton and Barto, 2017; Pang et al., 2019; Kiumarsi et al., 2017;
Liu et al., 2015). An important question is then how far the obtained approximate value function
is to the optimal one. To the best of our knowledge, this is only analysed in general when the cost

© 2021 M. Granzotto†, R. Postoyan†, D. Neši´c, L. Bu¸soniu & J. Daafouz†.

 
 
 
 
 
 
WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

is discounted and the stage cost takes values in a bounded set (Bertsekas, 2012). An alternative
consists in asking for a sufﬁciently large number of iterations, as the near-optimality gap vanishes
as the number of iterations increases, e.g. (Bertsekas, 2012; Heydari, 2018, 2014, 2016; Liu et al.,
2015; Granzotto et al., 2020a), but the issue is then the computational cost. Indeed, any estimate of
the number of iterations is in general subject to conservatism, and, as a result, we may iterate many
more times than what is truly required to ensure “good’ ’ near-optimality properties. There is there-
fore a need for stopping criteria for VI whose impact on near-optimality is analytically established,
and which are not too computationally demanding.

Our main goal is to use VI to simultaneously ensure near-optimal control and stability properties
for physical systems. Stability is critical in many applications, as: (i) it provides analytical guar-
antees on the behavior of the controlled system solutions as time evolves; (ii) endows robustness
properties and is thus associated to safety considerations, see, e.g., (Berkenkamp et al., 2017). We
therefore consider systems and costs where general stability properties are bestowed by VI based
schemes, which follows from assumed general stabilizability and detectability properties of the plant
model and the stage cost as in (Grimm et al., 2005; Postoyan et al., 2017; Granzotto et al., 2020a).

In this context, we consider state-dependent stopping criteria for VI and we analyse their im-
pact on the near-optimality and stability properties of the obtained policies for general deterministic
nonlinear plant models and stage costs, where no discount factor is employed. Instead of relying
on a uniform contraction property as in, e.g., (Bertsekas, 2012; Liu et al., 2015), our analysis is
centered on and exploits Lyapunov stability properties. Our work covers the state-independent stop-
ping criteria considered in the control, dynamical programming and reinforcement learning litera-
ture (Sutton and Barto, 2017; Lewis and Vrabie, 2009; Bu¸soniu et al., 2018), but provides analytical
guarantees for undiscounted stage costs taking values in unbounded sets. By carefully analysing the
stopping criterion’s impact on near-optimality, stability and closed-loop cost guarantees, we provide
means to tune these properties against the induced computational cost, thus clarifying the tradeoff
between “good enough” convergence of VI and “good properties” of generated inputs. Considering
that VI is, via Q-learning, the basis of many state-of-the-art reinforcement learning methods, we be-
lieve the results of this paper contribute to the (near)-optimality analysis for reinforcement learning,
as we lay a foundation to endow such schemes with stability and performance guarantees, while
reducing computational complexity.

The paper and its contributions are organized as follows. In Section 2, we formally state the
problem and the main assumptions. We introduce the design of stopping criteria for VI in Section
3, and show that the VI stopping criterion is indeed veriﬁed with a ﬁnite number of iterations. Our
main results are found in Section 4. There, we provide near-optimal guarantees, i.e. a bound on
the mismatch between the approximated value function and the true optimal value function. The
bound can be easily and directly tuned by the designed stopping criterion. Additionally, stability
and performance guarantees of the closed-loop system with inputs generated by VI are provided,
given that the stopping criterion is appropriately chosen. In Section 5, we provide an example to
illustrate our results. Concluding remarks are drawn in Section 6. The proofs are provided in this
technical report in Section 7.
Prior literature. The classical stopping criterion is analysed in (Bertsekas, 2012), albeit restricted
to when the cost is discounted and the stage cost takes values in a bounded set. Concerning stability,
works like (Granzotto et al., 2020a; Heydari, 2017; Wei et al., 2015) provide conditions to ensure
that the feedback law obtained ensures a stability property for a dynamical system. In particular,
it is required in (Granzotto et al., 2020a) that the number of iteration d be sufﬁciently large, and

2

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

lower bounds on d are provided, but these are subject to some conservatism. As explained above, by
adapting the number of iterations with data available during computations, the algorithm avoids the
conservatism often incurred by ofﬂine estimations for stability and near-optimality guarantees. This
is indeed the case in an example (see Section 5), where we observe 91% fewer iterations for compa-
rable guarantees. Similar ideas related to the stopping criterion were exploited in (Granzotto et al.,
2020b), for a different purpose, namely for the redesign of optimistic planning (Hren and Munos,
2008) to address the near-optimal control of switched systems. We are also aware of work of
(Pavlov et al., 2019), which adapts the stopping criterion with stability considerations for interior
point solvers for reduced computational complexity for nonlinear model predictive control applica-
tions.
Notation. Let R := (−∞, ∞), R≥0 := [0, ∞), Z≥0 := {0, 1, 2, . . .} and Z>0 := {1, 2, . . .}. We
use (x, y) to denote [x⊤, y⊤]⊤, where (x, y) ∈ Rn × Rm and n, m ∈ Z>0. A function χ : R≥0 →
R≥0 is of class K if it is continuous, zero at zero and strictly increasing, and it is of class K∞ if it
is of class K and unbounded. A continuous function β : R≥0 × R≥0 → R≥0 is of class KL when
β(·, t) is of class K for any t ≥ 0 and β(s, ·) is decreasing to 0 for any s ≥ 0. The notation I stands
for the identity map from R≥0 to R≥0. For any sequence u = [u0, u1, . . . ] of length d ∈ Z≥0 ∪{∞}
where ui ∈ Rm, i ∈ {0, . . . , d}, and any k ∈ {0, . . . , d}, we use u|k to denote the ﬁrst k elements
of u, i.e. u|k = [u0, . . . , uk−1] and u|0 = ∅ by convention. Let g : R≥0 → R≥0, we use g(k) for
the composition of function g with itself k times, where k ∈ Z≥0, and g(0) = I.

2. Problem Statement

Consider the system

x+ = f (x, u),
(1)
with state x ∈ Rn, control input u ∈ U (x) where U (x) ⊆ Rm is the set of admissible inputs, and
f : W → Rn where W := {(x, u) : x ∈ Rn, u ∈ U (x)}. We use φ(k, x, u|k) to denote the solution
to system (1) at time k ∈ Z≥0 with initial condition x and inputs sequence u|k = [u0, u1, . . . , uk−1],
with the convention φ(0, x, u|0) = x.

We consider the inﬁnite-horizon cost

∞

J∞(x, u) :=

ℓ(φ(k, x, u|k), uk),

(2)

k=0
X

where x ∈ Rn is the initial state, u is an inﬁnite sequence of admissible inputs, ℓ : W → R≥0
is the stage cost. Finding an inﬁnite sequence of inputs which minimizes (2) given x ∈ Rn is
very difﬁcult in general. Therefore, we instead generate sequences of admissible inputs that nearly
minimize (2), in a sense made precise below, while ensuring the stability of the closed-loop system.
For this purpose, we consider VI, see e.g. (Bertsekas, 2012). VI is an iterative procedure based on
Bellman equation, which we brieﬂy recall next. Assuming the optimal value function, denoted V∞,
exists for any x ∈ Rn, the Bellman equation is

V∞(x) = min
u∈U (x)

ℓ(x, u) + V∞(f (x, u))

.

(cid:26)

(cid:27)

(3)

If we could solve (3) and ﬁnd V∞, it would then be easy to derive an optimal policy, by computing
the arg min corresponding to the right hand-side of (3). However, it is in general very difﬁcult to

3

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

solve (3). VI provides an iterative procedure based on (3) instead, which allows obtaining value
functions (and associated control inputs), which converge to V∞. Hence, given an initial cost func-
tion V−1 : Rn → R≥0, VI generates a sequence of value functions Vd, d ∈ Z≥0, for any x ∈ Rn, by
iterating

(cid:26)
For any d ∈ Z≥0, the associated input, also called policy, is deﬁned as, for any x ∈ Rn,

(cid:27)

Vd(x) := min
u∈U (x)

ℓ(x, u) + Vd−1(f (x, u))

.

u∗
d(x) ∈ arg min

ℓ(x, u) + Vd−1(f (x, u))

,

u∈U (x) (cid:26)

(cid:27)

(4)

(5)

which may be set-valued. The convergence of Vd, d ∈ Z≥0, to V∞ in (3) is ensured under mild
conditions in (Bertsekas, 2017). In the sequel we make assumptions that ensure that the arg min in
(5) exists for each x ∈ Rn.

In practice, we often stop iterating VI when a stopping criterion is veriﬁed, such as, for instance,

when for any x ∈ Rn,

Vd(x) − Vd−1(x) ≤ ε,

(6)

where ε ∈ R>0, see, e.g.,
(Bertsekas, 2012; Sutton and Barto, 2017; Pang et al., 2019;
Kiumarsi et al., 2017). However, this stopping criterion leaves much to be desired in control
applications, for the following reasons: (i) it is not yet established how ε impacts the stability
properties of the closed-loop system; (ii) tools to bound the mismatch between Vd and V∞ for this
stopping criterion often requires a discount factor in cost function (2), which impacts stability,
as shown in (Postoyan et al., 2017, 2019); (iii) when Vd is radially unbounded, i.e. Vd(x) → ∞
when |x| → ∞, this stopping criterion is in general impossible to verify for all x ∈ Rn. When the
system is linear and the cost quadratic, as in (Arnold and Laub, 1984; Anderson and Moore, 2007;
Jiang and Jiang, 2012; Bian and Jiang, 2016), the convergence to the optimal cost function is shown
to be quadratic and often the stopping criterion is instead of the form Vd(x) − Vd−1(x) ≤ |ε||x|2.
However, the link between the value of ε and resulting near-optimality and stability guarantees is
not established, and in practice it is implicitly assumed that parameter ε is small enough.
We consider VI terminated by a general stopping criterion. That is, for any x ∈ Rn,

Vd(x) − Vd−1(x) ≤ cstop(ε, x),

(7)

where cstop(ε, x) ≥ 0 is a stopping function, which we design and which may depend on state vector
x and a vector of tuneable parameters ε ∈ Rnε with nε ∈ Z>0. The design of cstop is explained in
Section 3. In that way, we cover the above examples as particular cases, namely cstop(x, ε) = |ε| and
cstop(ε, x) = |ε||x|2 and allow considering more general ones, e.g. cstop(ε, x) = max{|ε1|, |ε2||x|2}
where (ε1, ε2) := ε ∈ R2 or cstop(ε, x) = x⊤S(ε)x for some positive deﬁnite matrix S(ε) with
ε ∈ Rnε and nε ∈ Z>0. The main novelty of this work is the provided explicit link between
cstop(ε, x), near-optimality and stability guarantees. As a result, we can tune ε for the desired near-
optimality and stability properties, and the algorithm stops when the cost (hence, the generated
inputs) are such that these properties are veriﬁed.

The analysis relies on the next assumption1 like in e.g., (Grimm et al., 2005; Postoyan et al., 2017;

Granzotto et al., 2020a).

1. The assumption is stated globally, for any x ∈ Rn and u ∈ U(x). We leave for future work the case where the

assumption holds on compact sets.

4

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

Standing Assumption 1 (SA1) There exist αV , αW ∈ K∞ and continuous function σ : Rn →
R≥0 such that the following conditions hold.

(i) For any x ∈ Rn, there exists an inﬁnite sequence of admissible inputs u∗
optimal input sequence, which minimizes (2), i.e. V∞(x) = J∞(x, u∗∗∗
αV (σ(x)).

∞(x), called
∞(x)), and V∞(x) ≤

(ii) For any (x, u) ∈ W, αW (σ(x)) ≤ ℓ(x, u).

(cid:3)

Function σ : Rn → R≥0 in SA1 is a “measuring” function that we use to deﬁne stability, which
depends on the problem. For instance, by deﬁning σ = | · |, σ = | · |2 or σ : x 7→ x⊤Qx with
Q = Q⊤ > 0, one would be studying the stability of the origin, and by taking σ = | · |A, one would
study stability of non-empty compact set A ⊂ Rn. General conditions to ensure the ﬁrst part of item
(i), i.e. the fact that V∞(x) is ﬁnite for any x ∈ Rn and the existence of optimal inputs, can be found
in (Keerthi and Gilbert, 1985). The second part of item (i) is related to the stabilizability of system
(1) with respect to stage cost ℓ in relation to σ. Indeed, it is shown in (Grimm et al., 2005, Lemma
1) that, for instance, when the stage cost ℓ(x, u) is uniformly globally exponentially controllable to
zero with respect to σ for system (1), see (Grimm et al., 2005, Deﬁnition 2), then item (i) of SA1 is
satisﬁed. We do not need to know V∞ to guarantee the last inequality in item (i) of SA1. Indeed,
it sufﬁces to ﬁnd, for any x ∈ Rn, a sequence of inputs u(x), such the associated inﬁnite-horizon
costs veriﬁes J(x, u(x)) ≤ αV (σ(x)) for some αV ∈ K∞. Then, since V∞ is the optimal value
function, for any x ∈ Rn, V∞(x) ≤ J(x, u(x)) ≤ αV (σ(x)). On the other hand, item (ii) of SA1
is a detectability property of the stage cost ℓ with respect to σ, as when ℓ(x, u) is small, so is σ(x).

We are ready to explain how to design the stopping criterion in (7).

3. Stopping criterion design

3.1. Key observation
We start with the known observation (Granzotto, 2019; Bertsekas, 2005) that, given2 V−1 = 0, at
each iteration d ∈ Z≥0, VI generates the optimal value function for the ﬁnite-horizon cost

d

Jd(x, ud) :=

ℓ(φ(k, x, ud|k), uk),

(8)

k=0
X

where ud = [u0, u1, ..., ud] are admissible inputs. We assume below that the minimum of (8) exists
with relation to ud for any x ∈ Rn and d ∈ Z≥0.

Standing Assumption 2 (SA2) For every d ∈ Z≥0, x ∈ Rn, there exists u∗∗∗

d(x) such that

Vd(x) = Jd(x, u∗∗∗

d) = minud

Jd(x, ud).

(9)

(cid:3)

2. The case where V−1 6= 0 will be investigated in further work.

5

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

SA2 is for instance veriﬁed when f and ℓ are continuous and U (x) = U is a compact set. More
general conditions to verify SA2 can be found in e.g. (Keerthi and Gilbert, 1985). For the sake of
convenience, we employ the following notation for the technical aspects of this paper. For any k ∈
{0, 1, . . . , d} and x ∈ Rn, we denote ℓ∗
d(x)|k)
is the solution to system (1) with optimal inputs for cost Vd(x), so that

d(x)|k), uk), where φ(k, x, u∗∗∗

d(k, x) := ℓ(φ(k, x, u∗∗∗

Vd(x) =

ℓ∗
d(k, x).

d

k=0
X

The next property plays a key role in the forthcoming analysis.

Proposition 1 For any x ∈ Rn and d ∈ Z≥0, ℓ∗

d(d, x) ≤ Vd(x) − Vd−1(x).

(10)

(cid:3)

d(x)|d)), namely σ(φ(d, x, u∗∗∗

When the stopping criterion (7) is veriﬁed, i.e. Vd(x) − Vd−1(x) ≤ cstop(ε, x), then ℓ∗

d(d, x) ≤
cstop(ε, x) in view of Proposition 1. Therefore, cstop(ε, x) is an upper-bound on the value of stage
cost ℓ∗
d(d, x). By item (ii) of SA1, this implies that we also have an upper-bound for d-horizon
state measure σ(φ(d, x, u∗∗∗
W (cstop(ε, x)), which can be
made as small as desired by reducing cstop(ε, x), which, again, we design. We exploit this property
to analyse the near-optimality and the stability of the closed-loop system. Having said that, the
challenges are: (i) to show that condition (7) is indeed veriﬁed for any x ∈ Rn and some d ∈ Z≥0;
(ii) to select cstop to ensure stability properties when closing the loop of system (1) with inputs
(5); (iii) to study the impact of cstop on the performance, that is, the cost along solutions, of the
closed-loop system.

d(x)|d)) ≤ α−1

3.2. Satisfaction of the stopping criterion

We make the next assumption without loss of generality as we are free to design cstop.

Assumption 1 One of the next properties is veriﬁed.

(i) For any ε ∈ Rnε, there is ǫ > 0 such that, for any x ∈ Rn, cstop(ε, x) ≥ ǫ.

(ii) There exist L, ¯aV , aW > 0, such that SA1 holds with αV (s) ≤ ¯aV s, αW (s) ≤ ¯aW s and
αW (s) ≥ aW s for any s ∈ [0, L]. Furthermore, for any ε ∈ Rnε, there is ǫ > 0 such that for
(cid:3)
any x ∈ Rn, cstop(ε, x) ≥ ǫσ(x).

Item (i) of Assumption 1 can be ensured by taking cstop(ε, x) = |ε| + ˜cstop(x, ε) with ˜cstop(x, ε) ≥
0 for any x ∈ Rn, ε ∈ Rnε, which covers (6), to give an example. Item (ii) of Assumption 1 means
that the functions αV , αW , αW in SA1 can be upper-bounded, respectively lower-bounded, by linear
functions on the interval [0, L]. These conditions allow to select cstop such that cstop(ε, x) → 0
when σ(x) → 0 with x ∈ Rn, contrary to item (i) of Assumption 1, that is, cstop may vanish on
set {x : σ(x) = 0}. This is important to provide stronger stability and performance properties
for systems whose inputs are given by our VI scheme as shown in Section 4. Under item (ii) of
Assumption 1, we can design cstop as, e.g., cstop(ε, x) = |ε|σ(x), cstop(ε, x) = min{|ε1|, |ε2||x|2}
where (ε1, ε2) =: ε ∈ R2 or cstop(ε, x) = x⊤S(ε)x for some positive deﬁnite matrix S(ε) as
mentioned before.

The next theorem ensures the existence of d ∈ Z≥0 such that, for any x ∈ Rn, (7) holds based on

Assumption 1.

6

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

Theorem 2 Suppose Assumption 1 holds. Then, for any ∆ > 0 there exists d ∈ Z≥0 such that, for
any x ∈ {z ∈ Rn : σ(z) ≤ ∆}, (7) holds. Moreover, when item (ii) of Assumption 1 holds with
(cid:3)
L = ∞, there exists d ∈ Z≥0 such that, for any x ∈ Rn, (7) is satisﬁed.

Theorem 2 guarantees the stopping condition in (7) is always satisﬁed by iterating the VI algo-
rithm sufﬁciently many times, and that the required number of iterations is uniform over sets of
initial conditions of the form {x : σ(x) ≤ ∆} for given ∆ > 0 in general, unless item (ii) of
Assumption 1 holds with L = ∞, in which case there exists a common, global, d for any x ∈ Rn.
Note that, while the proof of Theorem 2 provides a conservative estimate of d such that (7) is ver-
iﬁed,
this horizon estimate is not utilized in the stopping criterion, which in turn implies that VI
stops with smaller horizon, in general, as illustrated in Section 5.

In the following, we denote the cost calculated at iteration d as Vε(x) := Vd(x), like in
(Granzotto et al., 2020b), to emphasize that the cost returned is parameterized by ε via cstop(ε, ·),
and denote by u∗∗∗

ε(x) an associated optimal sequence of inputs, i.e.

Vε(x) = Jd(x, u∗∗∗

ε(x)).

(11)

We are ready to state the main results.

4. Main results

In this section, we analyze the near-optimality properties of VI with the stopping criterion in (7).
We then provide conditions under which system (1), whose inputs are generated by applying the
state-feedback u∗∗∗
ε(x) in receding-horizon fashion, exhibits stability properties. Afterwards, the cost
function (8) along the solutions of the induced closed-loop system are analysed, which we refer to
by performance or running-cost (Grüne and Rantzer, 2008).

4.1. Relationship between Vε and V∞

A key question is how far is Vε from V∞ when we stop VI using (7). Since ℓ(x, u) is not constrained
to take values in a given compact set, and we do not consider discounted costs, the tools found in the
dynamic programming literature to analyze this relationship are no longer applicable, see (Bertsekas,
2012). We overcome this issue by exploiting SA1, and adapting the results of (Granzotto et al.,
2020a) with the stopping criterion and Proposition 1 in the next theorem.

Theorem 3 Suppose Assumption 1 holds. For any ε ∈ Rnε, ∆ > 0 and x ∈ {z ∈ Rn, σ(z) ≤ ∆},

Vε(x) ≤ V∞(x) ≤ Vε(x) + vε(x),

(12)

where vε(x) := αV ◦ α−1
tion holds with L = ∞, we accept ∆ = ∞ and vε(x) ≤ ¯aV

W (cstop(ε, x)) with αV , αW from SA1. Moreover, when item (ii) of Assump-
(cid:3)

aW cstop(ε, x).

The lower-bound in (12) trivially holds from the optimality of Vε(x) = Vd(x) for some d < ∞,
and the fact that ℓ(x, u) ≥ 0 for any x ∈ Rn and u ∈ U (x). The upper-bound, on the other hand,
implies that the inﬁnite-horizon cost is at most vε(x) away from the ﬁnite-horizon Vε(x). The error
term vε(x) is small when cstop(ε, x) is small as αV ◦ α−1
W a
priori, and we are free to design cstop as wanted, we can therefore directly make Vε(x) as close as
desired to V∞(x) by adjusting cstop; the price to pay will be more computations. Moreover, when
item (ii) of Assumption holds with L = ∞, inequality (12) is veriﬁed for every x ∈ Rn.

W ∈ K∞. Given that we know αV , α−1

7

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

4.2. Stability

We now consider the scenario where system (1) is controlled in a receding-horizon fashion by inputs
that calculate cost (11). That is, at each time instant k ∈ Z≥0, the ﬁrst element of optimal sequence
u∗∗∗

ε(xk), calculated by VI, is then applied to system (1). This leads to the closed-loop system

x+ ∈ f (x, U ∗

ε (x)) =: F ∗

ε (x),

(13)

ε (x)) is the set {f (x, u) : u ∈ U ∗

where f (x, U ∗
u0 : ∃u1, . . . , ud ∈
U (x) such that Vε(x) = Jd(x, [u0, . . . , ud])
is the set of the ﬁrst input of d-horizon optimal input
sequences at x, with d as deﬁned in (7). We denote by φ(k, x) a solution to (13) at time k ∈ Z≥0
with initial condition x ∈ Rn, with some abuse of notation.

ε (x)} and U ∗

ε (x) :=

(cid:8)

(cid:9)

We assume next that cstop can be made as small as desirable by taking |ε| sufﬁciently small. As

we are free to design cstop as wanted, this is without loss of generality.
Assumption 2 There exists θ : R≥0 × R≥0 → R≥0, with θ(·, s) ∈ K and θ(s, ·) non-decreasing
(cid:3)
for any s > 0, such that cstop(ε, x) ≤ θ(|ε|, σ(x)) for any x ∈ Rn and ε ∈ Rnε.

Example of functions cstop which satisfy Assumption 2 are cstop(ε, x) = |ε|σ(x), cstop(ε, x) =

max{|ε1|α(σ(x)), |ε2|} for ε = (ε1, ε2) ∈ R2, α ∈ K and x ∈ Rn to give a few.

The next theorem provides stability guarantees for system (13).

Theorem 4 Consider system (13) and suppose cstop veriﬁes Assumptions 1 and 2. There exists
β ∈ KL such that, for any δ, ∆ > 0, there exists ε∗ > 0 such that for any x ∈ {z ∈ Rn :
σ(z) ≤ ∆} and ε ∈ Rnε with |ε| < ε∗, any solution φ(·, x) to system (13) satisﬁes, for all k ∈ Z≥0,
(cid:3)
σ(φ(k, x)) ≤ max{β(σ(x), k), δ}.

Theorem 4 provides a uniform semiglobal practical stability property for the set {z : σ(z) = 0}.
This implies that solutions to (13), with initial state x such that σ(x) ≤ ∆, where ∆ is any given
(arbitrarily large) strictly positive constant, will converge to the set {z : σ(z) ≤ δ}, where δ is
any given (arbitrarily small) strictly positive constant, by taking ε∗ sufﬁciently close to 0, thereby
making cstop sufﬁciently small. An explicit formula for ε∗ is given in the proof of Theorem 4, which
is nevertheless subject to some conservatism. The result should rather be appreciated qualitatively,
in the sense that Theorem 4 holds for small enough ε∗.

Under stronger assumptions, global exponential stability is ensured as shown in the next corollary.

Corollary 5 Suppose item (ii) of Assumption 1 holds and that cstop(ε, x) ≤ |ε|σ(x) for any x ∈ Rn
and ε ∈ Rnε. Let ε∗ > 0 be such that ε∗ < a2
¯aV . Then, for any x ∈ Rn and ε ∈ Rnε such that
|ε| ≤ ε∗, any solution φ(·, x) to system (13) satisﬁes σ(φ(k, x)) ≤ ¯aV
σ(x) for
aW
(cid:3)
all k ∈ Z≥0.

a2
W −|ε|aV
¯aV aW

1 −

W

k

(cid:16)

(cid:17)

Corollary 5 ensures a uniform global exponential stability property of set {x : σ(x) = 0} for
and take values in

a2
W −|ε|¯aV
system (13). Indeed, in Corollary 5, the decay rate is given by 1 −
¯aV aW
a2
(0, 1) as |ε| ≤ ε∗ < a2
W −|ε|
→ 0 as k → ∞.
¯aV aW
Furthermore, the estimated decay rate can be tuned via ε from 1 to 1 − aW
¯aV as |ε| decreases to zero.
We can therefore make the decay smaller by adjusting cstop, as in Theorem 3. Hence, by tuning ε,
we can tune how fast the closed-loop converges to the attractor {x : σ(x) = 0}, and the price to pay
is more computations in general.

¯aV as required by Corollary 5, hence

1 −

(cid:17)

(cid:16)

W

k

8

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

4.3. Policy performance guarantees

In Section 4.1, we have provided relationships between the ﬁnite-horizon cost Vε and the inﬁnite-
horizon cost V∞. This is an important feature of VI, but this does not directly provide us with
information on the actual value of the cost function (2) along solutions to (13). Therefore, we
analyse the running cost (Grüne and Rantzer, 2008) deﬁned as

V run
ε (x) :=

∞

(

k=0
X

ℓU ∗ε (φ(k,x))(φ(k, x)) : φ(·, x) is a solution to (13)

,
)

(14)

where ℓU ∗ε (φ(k,x))(φ(k, x)) is the actual stage cost incurred at time step k. It has to be noted that
ε (x) is a set, since solutions of (13) are not necessarily unique. Each element V run
V run
ε (x)
ε
corresponds then to the cost of a solution of (13). Clearly, V run
(x) is not necessarily bounded, as
the stage costs may not decrease to 0 in view of Theorem 4. Indeed, only practical convergence is
ensured in Theorem 4 in general. On the other hand, when the set {x ∈ Rn : σ(x) = 0} is globally
exponentially stable as in Corollary 5, the elements of V run
ε (x) in (14) are bounded and satisfy the
next property.

(x) ∈ V run

ε

Theorem 6 Consider system (13) and suppose the conditions of Corollary 5 hold. For any ε such
that |ε| < ε∗, x ∈ Rn, and V run

(x), it follows that,

(x) ∈ V run

ε

ε

with wε :=

¯a3
V
aW

|ε|
a2
W − ¯aV |ε|

V∞(x) ≤ V run

ε

(x) ≤ V∞(x) + wεσ(x),

, where the constants come from Corollary 5.

(15)

(cid:3)

ε

ε

The inequality V∞(x) ≤ V run

(x) ≤ V∞(x)+wεσ(x) provides a relationship between the running cost V run

(x) of Theorem 6 directly follows from the optimality of V∞. The
inequality V run
(x) and
the inﬁnite-horizon cost at state x, V∞(x). The inequality V run
(x) ≤ V∞(x) + wεσ(x) conﬁrms the
intuition coming from Theorem 3 that a smaller stopping criterion leads to tighter near-optimality
(x) → V∞(x) for any x ∈ Rn, provided that
guarantees. That is, when |ε| → 0, wε → 0 and V run
ε
Corollary 5 holds. In contrast with Theorem 3, stability of system (13) is essential in Theorem 6.
in the expression of wε shows that the running cost is large when |ε| is
Indeed, the term
close to a2

¯aV , hence, when stability is not guaranteed, the running cost might be unbounded.

1
a2
W −¯aV |ε|

W

ε

ε

5. Illustrative Example

We consider the discrete cubic integrator, also seen in (Grimm et al., 2005; Granzotto et al., 2020a),
which is given by

x+
1 = x1 + u
2 = x2 + u3,
x+
where (x1, x2) := x ∈ R2 and u ∈ R. Let σ(x) = |x1|3 + |x2| and consider cost (8) with
ℓ(x, u) = |x1|3 + |x2| + |u|3 for any (x, u) ∈ R2 × R. It is shown in (Granzotto et al., 2020a) that
SA1 holds with αV = 14I and αW := I.

(16)

Because it is notoriously difﬁcult to exactly compute Vd(x) and associated sequence of optimal
inputs for every x ∈ R2, we use an approximate scheme. In particular, we rely on a simple ﬁnite

9

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

cstop(ε, x) :

|ε|
|ε|σ(x)
|ε| min{σ(x), 1}

10
d = 6
d = 0
d = 6

0.75
d = 7
d = 1
d = 7

0.1
d = 8
d = 3
d = 8

ε
0.075
d = 8
d = 4
d = 8

0.05
d = 8
d = 5
d = 8

0.025
d = 8
d = 6
d = 8

0.005
d = 9
d = 7
d = 9

Table 1: Required iterations to fulﬁll each stopping criteria for N = 3402 points equally distributed
in {z ∈ Rn : σ(z) ≤ 2000}.

difference approximation, with N = 3402 points equally distributed in [−10, 10] × [−103, 103] for
the state space or, equivalently, {x ∈ Rn : σ(x) ≤ 2000}, and 909 equally distributed quantized
inputs in [−20, 20] centered at 0. We consider three types of stopping criteria for which ε is a scalar.
For each stopping criterion, we discuss the type of guaranteed stability and we provide in Table 1 the
corresponding horizon for different values of ε, which is related to the computation cost. Then, for
each horizon, we give in Table 2 estimates of the running cost for initial condition x = (10, −103),
by computing the sum in (14) up to k = 40 instead of k = ∞, as well as the value of σ(φ(40, x))
to evaluate the convergence accuracy of the corresponding policy.

W

We ﬁrst take the uniform stopping criterion uniform stopping criterion as in (6),

like in,
e.g.,(Bertsekas, 2012; Sutton and Barto, 2017; Pang et al., 2019; Kiumarsi et al., 2017; Liu et al.,
2015), i.e. cstop(ε, x) := |ε|, with different values of ε. In this case, we have no global exponential
stability or performance guarantees like in Corollary 5 and Theorem 6 a priori. Only near-optimal
guarantees as in Theorem 3 and semiglobal practical stability as in Theorem 4 hold. For instance,
by taking ε = 0.01, Theorem 3 holds with vε(x) = 14 · 0.01 = 0.14 for any x ∈ Rn.

¯aV = 1

We also consider the following relative stopping criterion, for any x ∈ Rn and ε ∈ R,
cstop(ε, x) := |ε|σ(x). The exponential stability of Corollary 5 holds for any ε ∈ R such that
|ε| < a2
14 in this case. Moreover, we have near-optimality and performance properties as in
Theorems 3 and 6, which were not available for the previous stopping criterion cstop(ε, x) = |ε|.
Moreover, for ε = 0.01 < 1
14 , Theorem 3 holds with vε(x) = 14 · 0.01 · σ(x) = 0.14σ(x)
for any x ∈ Rn, which is small when σ(x) is small, and vice versa. Compared to the previous
stopping criterion, which leads to constant guaranteed near-optimality bound, here we have better
guarantees when σ(x) is small (and worse ones when σ(x) is large). We observe less computations
for better a priori near-optimality properties for states near the attractor, i.e. when σ(x) < 1, when
compared to the previous stopping criterion. We ﬁnally consider the mixed stopping criterion
cstop(ε, x)
:= |ε| min{σ(x), 1}, which provides better near-optimality guarantees than both
considered stopping criteria. We see from Table 1, and Table 2, that by increasing iterations, we
usually obtain smaller and thus better running costs as well as tighter convergence properties.

Compared to previous work (Granzotto et al., 2020a), where stability properties to (approxi-
Indeed, in view of
mate) value iteration are given, we require a smaller number of iterations.
(Granzotto et al., 2020a, Corollary 2), d ≥ ¯d =
= 71. Of course, this analysis is
conservative and a different derivation of αV might provide different bounds on ¯d. Here, as the al-
gorithm is free to choose the required number of iterations via the stopping criterion, we signiﬁcantly
reduce its conservatism. This induces smaller computational complexity, as, e.g. for ε = 0.01 < 1
14 ,
exponential stability is ensured with the stoping criterion veriﬁed at d = 6, that is, 8.5% of iterations
required by the lower bound ¯d = 71 of (Granzotto et al., 2020a).

0−ln 142
ln 13−ln 14

k

j

10

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

V run

d (x)
σ(φ(40, x))

0
77313
1982

1
45497
1138

3
19931
2.56

4
19965
2.84

d
5
19802
1.71

6
20090
2.25

7
20359
1.84

8
20261
1.62

9
20261
1.62

Table 2: Estimation of the running cost V run

d

(x) for x = (10, −103) and the value of σ(φ(40, x)).

6. Concluding remarks

Future work includes relaxing the initial condition for VI and the main assumptions. Another di-
rection is extending the work towards stochastic problems and online algorithms, towards the ﬁnal
goal of stability-based computational-performance tradeoffs in reinforcement learning.

7. Proofs

7.1. Proof of Proposition 1
Let x ∈ Rn and d ∈ Z>0. Since Vd−1(x) is the optimal value function associated to the (d − 1)-

horizon cost (9), it follows Vd−1(x) ≤

d(k, x), and by deﬁnition of ℓ∗
ℓ∗

d, Vd(x) =

ℓ∗
d(k, x).

d

k=0
P

d−1

k=0
P

Hence ℓ∗

d(d, x) =

ℓ∗
d(k, x) −

d

k=0
P

d−1

k=0
P

ℓ∗
d(k, x) ≤ Vd(x) − Vd−1(x), which gives the desired result.

7.2. Proof of Theorem 2
We ﬁrst consider the case when item (i) of Assumption 1 holds. Let ∆ > 0 and x ∈ Rn such that
σ(x) ≤ ∆, ε ∈ Rnε and ǫ > 0 as in item (i) of Assumption 1. In view of (Granzotto et al., 2020a,
Theorem 3) with γ = 1, it follows that

Vd(x) ≤ V∞(x) ≤ Vd(x) + αV ◦ α−1

Y ◦

I − αY ◦ α−1
Y

d

◦ αY (σ(x)),

(17)

for all d ∈ Z>0 and x ∈ Rn, where αY := αV and αY , αY := αW . Hence −Vd−1(x) − αV ◦ α−1
I − αY ◦ α−1
Y

◦ αY (σ(x)) ≤ −V∞(x) and since Vd(x) ≤ V∞(x), we derive that

d−1

Y ◦

(cid:1)

(cid:0)

(cid:0)

Vd(x) − Vd−1(x) − αV ◦ α−1

Y ◦

(cid:1)

I − αY ◦ α−1
Y

d−1

◦ αY (σ(x)) ≤ V∞(x) − V∞(x) = 0,

(18)

thus

(cid:0)

Vd(x) − Vd−1(x) ≤ αV ◦ α−1

Y ◦

(cid:1)
I − αY ◦ α−1
Y

d−1

◦ αY (σ(x)).

(19)

When σ(x) = 0, Vd(x) − Vd−1(x) ≤ αV (0) = 0. Hence, Vd(x) − Vd−1(x) ≤ cstop(ε, x) for any
(cid:0)
d ∈ Z>0 since cstop(ε, x) ≥ 0. When σ(x) > 0, αV ◦ α−1
◦ αY (σ(x)) is
upper-bounded by αV ◦ α−1
◦ αY (σ(∆)) as σ(x) ≤ ∆ since the considered
function is non-decreasing. The term αV ◦ α−1
◦ αY (σ(∆)) can be made
Y ◦
(cid:1)
arbitrarily close to 0 by increasing d, according to (Granzotto et al., 2020a, Lemma 3) as γ = 1 and
σ(x) ≤ ∆. In particular, we take d∗ sufﬁciently large such that αV ◦ α−1
◦
αY (σ(∆)) ≤ ǫ where ǫ comes from item (i) of Assumption 1 and is such that ǫ ≤ cstop(ε, x). Hence

(cid:1)
I − αY ◦ α−1
Y

I − αY ◦ α−1
Y

I − αY ◦ α−1
Y

I − αY ◦ α−1
Y

Y ◦

Y ◦

Y ◦

d∗−1

d−1

d−1

d−1

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

11

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

1 − aW
¯aV
¯d−1
(cid:16)

d−1

Vd(x) − Vd−1(x) ≤ αV ◦ α−1
d∗ depends on ∆ a priori, but not on x. We have proved the desired result.

I − αY ◦ α−1
Y

Y ◦

◦ αY (σ(x)) ≤ cstop(ε, x) for any d ≥ d∗ and

(cid:0)

(cid:1)
Consider now the case when item (ii) of Assumption 1 holds with L < ∞. Let ∆ > 0 and
x ∈ Rn such that σ(x) ≤ ∆, ε ∈ Rnε and ǫ > 0 as in item (ii) of Assumption 1 and is such
that ǫσ(x) ≤ cstop(ε, x). It follows from the inequalities of item (ii) of Assumption 1 that there
I − αY ◦ α−1
exists some δ > 0 such that αV ◦ α−1
Y
s ∈ [0, δ], see (Granzotto et al., 2020a, proof of Corollary 1, equation (47)). Note that 1 − aW

s for any
¯aV ∈
can be made as small as desired. Therefore, there exists ¯d such that

◦ αY (s) ≤ ¯a2

1 − aW
¯aV

(0, 1), hence

Y ◦

V
aW

d−1

(cid:17)

(cid:16)

(cid:1)

(cid:0)

d

d

d

d

(cid:0)

(cid:17)

V
aW

Y ◦

1 − aW
¯aV
(cid:16)

≤ ǫ where ǫ. Thus, when σ(x) ∈ [0, δ] and in view of (19), it follows that
◦ αY (σ(x)) ≤ ¯a2

¯a2
V
aW
(cid:17)
Vd(x) − Vd−1(x) ≤ αV ◦ α−1
σ(x) ≤ ǫσ(x) ≤
cstop(ε, x) for any d ≥ ¯d. When σ(x) ∈ [δ, ∆], we have cstop(ε, x) ≥ ǫδ > 0, and we recover
(cid:1)
d∗ such that Vd(x) − Vd−1(x) ≤ cstop(ε, x) for σ(x) ∈ [δ, ∆] by applying the steps made above,
for when item (i) of Assumption 1 holds. Therefore, it follows that, for any d ≥ min{ ¯d, d∗},
Vd(x) − Vd−1(x) ≤ cstop(ε, x) holds for any σ(x) ∈ [0, ∆]. We have proved the desired result.

I − αY ◦ α−1
Y

1 − aW
¯aV

Suppose now that item (ii) of Assumption 1 holds with L = ∞. Let x ∈ Rn, ε ∈ Rnε and ǫ > 0
as in item (ii) of Assumption 1. It follows then that αV ◦ α−1
◦ αY (σ(x)) ≤
¯a2
σ(x) for all x ∈ Rn. Hence, by invoking the same arguments as above when item
V
aW
(ii) of Assumption 1 holds, there exists ¯d > 0 such that Vd(x)− Vd−1(x) ≤ cstop(ε, x) for any d ≥ ¯d.
The proof is complete.

1 − aW
¯aV
(cid:16)

I − αY ◦ α−1
Y

Y ◦

(cid:16)

(cid:17)

(cid:17)

(cid:1)

(cid:0)

d

d

0, u∗

d] := u∗∗∗

1, . . . , u∗

7.3. Proof of Theorem 3
Let x ∈ Rn such that σ(x) ≤ ∆ and ε ∈ Rnε, d ∈ Z≥0 as in (7), which exists since Theorem 2 holds.
Hence, optimal sequence [u∗
ε(x) and cost Vε(x) deﬁned in (11) are well-deﬁned.
Since Vε is a ﬁnite-horizon optimal cost, Vε(x) ≤ V∞(x). On the other hand, consider the inﬁnite-
∞(φ(d, x, u∗∗∗
ε(x)|d))], where u∗∗∗
1, . . . u∗
0, u∗
horizon sequence u = [u∗
ε (x)|d)) exists
It follows from the optimality of V∞(x) that V∞(x) ≤ J∞(x, u),
in view of item (i) of SA1.
and from the deﬁnition of u that J∞(x, u) = Vε(x) + V∞(φ(d, x, u∗∗∗
ε (x)|d)), which is ﬁnite. By
invoking item (i) of SA1, we derive V∞(x) ≤ Vε(x) + αV (σ(φ(d, x, u∗∗∗
ε (x)|d))). In view of Lemma
1 and item (ii) of SA1, σ(φ(d, x, u∗∗∗
W (cstop(ε, x)), thus V∞(x) ≤ Vε(x) + αV ◦
α−1

W (cstop(ε, x)) and the proof is complete.

ε (x)|d)) ≤ α−1

∞(φ(d, x, u∗∗∗

d−1, u∗∗∗

7.4. Proof of Theorem 4

First, we prove the following result, which provides Lyapunov properties for system (13) that we
use to derive the main stability result afterwards.

Theorem 7 Let Y := V∞, the following holds.

(i) For any x ∈ Rn,

where αY := αW , αY := αV , with αW , αV from SA1.

αY (σ(x)) ≤ Y(x) ≤ αY (σ(x)),

(20)

12

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

(ii) For any x ∈ Rn, ε ∈ Rnε, v ∈ F ∗

ε (x),

Y(v) − Y(x) ≤ −αY (σ(x)) + αV ◦ α−1

W (cstop(ε, x))

where αY = αW , with αW and αV from SA1, and cstop comes from (7).

(21)

(cid:3)

0, u∗

Proof. Let ε ∈ Rnε, x ∈ Rn and v ∈ F ∗
0) and u∗
exists [u∗
system (1) and cost (8) with horizon d, hence Vε(x) = Jd(x, u∗
1 and item (ii) of SA1, σ(φ(d, x, u∗∗∗
W (cstop(ε, x)).

ε(x) such that v = f (x, u∗

ε (x), which is well-deﬁned in view of Theorem 2. There
ε(x) is an optimal input sequence for
ε(x)). Moreover, in view of Lemma

ε (x)|d)) ≤ α−1

1, . . . , u∗

d] = u∗

From items (i) and (ii) of SA1, we have Y(x) = V∞(x) ≤ αV (σ(x)) =: αY (σ(x)). On the other
d(0, x). This implies that αW (σ(x)) ≤

hand, we have from item (ii) of SA1 that αW (σ(x)) ≤ ℓ∗
Vε(x) ≤ V∞(x) = Y(x). Hence item (i) of Theorem 7 holds with αY = αW .

1, u∗

0, . . . , u∗

2, . . . , u∗

Consider the sequence ˆu := [u∗
ε(x)|d = [u∗

ε (x)|d)),
d−1] and φ denotes the solution of system (1). The sequence ˆu consists of
0, followed by an optimal input sequence of inﬁnite length
ε(x)|d), which exists according to item (i) of SA1. Sequence ¯u minimizes
ε(x)|d), ¯u) by virtue of item (i) of SA1. From the deﬁnition of cost Jd in (8) and

d−1, ¯u] where ¯u := u∗

u∗
the ﬁrst d elements of u∗
at state φ(d, x, u∗∗∗
J∞(φ(d, x, u∗∗∗
V∞(v) in view of item (i) of SA1,

∞(φ(d, x, u∗

ε(x) after u∗

V∞(v) ≤ J∞(v, ˆu)

= Jd−1(v, ˆu|d−1)

+ J∞(φ(d − 1, v, ˆu|d−1), ¯u).

(22)

From Bellman optimality principle, we have Vε(x) = Vd(x) = ℓ∗
Jd−1(v, ˆu|d−1), hence

d(0, x) + Vd−1(v) = ℓ∗

d(0, x) +

Jd−1(v, ˆu|d−1) = Vε(x) − ℓ∗

d(0, x).

Moreover, by item (i) of SA1,

J∞(φ(d − 1, v, ˆu|d−1), ¯u)

≤ αV (σ(φ(d − 1, v, ˆu|d−1))).

Consequently, in view of (22), (23) and (24),

V∞(v) ≤ Vε(x) − ℓ∗

d(0, x)

+ αV (σ(φ(d − 1, v, ˆu|d−1))).

(23)

(24)

(25)

Since φ(d − 1, v, ˆu|d−1) = φ(d, x, u∗∗∗
follows

ε(x)|d) and σ(φ(d, x, u∗∗∗

ε(x)|d)) ≤ α−1

W (cstop(ε, x)) holds, it

V∞(v) ≤ Vε(x) − ℓ∗

d(0, x) + αV ◦ α−1

W (cstop(ε, x)).

By Theorem 3, Vε(x) ≤ V∞(x), thus

V∞(v) ≤ V∞(x) − ℓ∗

d(0, x) + αV ◦ α−1

W (cstop(ε, x)).

(26)

(27)

By invoking item (ii) of SA1, we derive V∞(v) ≤ V∞(x) − αW (σ(x)) + αV ◦ α−1
since Y = V∞, the proof is completed with αY := αW .

W (cstop(ε, x)), and
(cid:4)

13

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

Item (i) states that Y is positive deﬁnite and radially unbounded with respect to the set {x :
σ(x) = 0}. Item (ii) of Theorem 7 shows that Y strictly decreases along the solutions to (13) up to
a perturbative term αV ◦ α−1
W (cstop(ε, x)), which can be made as small as desired by selecting |ε|
close to 0 as αV ◦ α−1
W (θ(|ε|, σ(x))), per Assumption 2.

W (cstop(ε, x)) ≤ αV ◦ α−1

We are ready to prove Theorem 4. The proof heavily borrows from (Granzotto et al., 2019, The-
orem2) and (Granzotto, 2019, Theorem 3.3), we nevertheless include it in this technical report for
self-completeness. Let ∆, δ > 0. We select ε∗ > 0 such that

θ(ε∗, α−1
Y (

∆)) < αW ◦ α−1
V (

1
2

αY (

δ)),

(28)

I −
(cid:16)

where

αY := αW ◦ α−1
Y ,

∆ := αY (∆),

e
δ :=

−1

eαY
2

e

e

◦ αY (δ) and θ comes from Assumption

e

(cid:17)

(cid:17)

eαY
2

−1
I −
e
αY ∈ K∞, hence I −
(cid:16)

is indeed of class K∞ as we assume without loss of generality that3
2. Note that
e
I −
αY +
Inequality (30) can always
be veriﬁed by taking ε∗ sufﬁciently small since θ(·, α−1
V ( 1
δ)) >
Y (
2
0. It follows from θ(·, s) ∈ K for any s > 0 and θ(s, ·) is non-decreasing for any s ≥ 0, that
e
e
θ(|ε|, α−1
∆] and |ε| < ε∗. Furthermore, from Assumption 2
∆)) for any s ∈ [0,
e
and item (i) of Theorem 7, we derive cstop(ε, x) ≤ θ(|ε|, α−1
Y (Y(x))). Thus, in view of (28),
e

eαY
2 ∈ K∞ and so is its inverse.

∆)) ∈ K, and αW ◦ α−1

Y (s)) ≤ θ(ε∗, α−1
Y (

αY (

e

e

e

cstop(ε, x) ≤ αW ◦ α−1
V

(cid:18)

αY (

δ)

(cid:19)

(29)

1
2

for any x such that Y(x) ≤
α−1
V ( 1
2

αY (s)) for any s ∈ [

δ, ∞). Hence, for any x ∈ Rn such that Y(x) ∈ [

∆. On the other hand, we have αW ◦ α−1
V ( 1
2
∆] and |ε| < ε∗,
δ,
e

αY (

δ)) ≤ αW ◦

e

e

e

e

e

e
αV ◦ α−1

W (cstop(ε, x)) ≤

δ)

αY (
2

≤

αY (Y(x))
2

.

e

e

(30)

Let x ∈ Rn with σ(x) ≤ ∆ and v ∈ F ∗

ε (x). In view of (29) and items (i) and (ii) of Theorem 7,

e

e

e

Y(v) − Y(x) ≤ −

αY (Y(x)) + αV ◦ α−1

W (cstop(ε, x)).

(31)

Since σ(x) ≤ ∆, Y(x) ≤ αY (σ(x)) ≤ αY (∆) =
αW ◦ α−1
without loss of generality, and in view of (31),

δ)) holds for Y(x) ≤

V ( 1
2

αY (

e

∆. Consider Y(x) ∈ [0,

e
∆, it holds here. Furthermore, since I −

δ). Since cstop(ε, x) ≤
αY ∈ K∞ holds

e

e

e

e
Y(v) ≤ Y(x) −

αY (Y(x)) + αV (θ(ε∗, σ(x)))

e

≤ (I −

αY ) (
e

δ) +

1
2

αY (

δ).

Given the deﬁnition of

δ,

e

e

e

e

e

3. See (Granzotto, 2019) for details.

Y(v) ≤

I −

(cid:18)

αY
2
e

δ) = αY (δ).
(

(cid:19)

e

(32)

(33)

14

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

When Y(x) ≥

δ, we derive from (30) that −

αY (Y(x)) + αV (cstop(ε, x)) ≤ − 1
2

αY (Y(x)). Thus,

from (31),

e

Y(v) − Y(x) ≤ − 1
2

e

αY (Y(x)).

e

In view of (33) and (34), it follows for any k ∈ Z≥0 that
e
(I − 1
αY )(Y(x)), αY (δ)
2

Y(φ(k + 1, x)) ≤ max

,

(34)

(35)

where φ(k, x) is a solution starting at x for system (13). Furthermore, when Y(x) ≤ αY (δ), Y(v) ≤
δ), we
αY (δ) follows. Indeed, if Y(x) ∈ [
deduce Y(v) ≤ αY (δ) from (33). Hence the set {z ∈ Rn : Y(z) ≤ αY (δ)} is forward invariant for
system (13). By iterating (35), we obtain

∆], Y(v) ≤ Y(x) ≤ αY (δ) from (34), and if Y(x) ∈ [0,

δ,

(cid:9)

(cid:8)

e

e

e

e

Y(φ(k, x)) ≤ max

β(Y(x), k), αY (δ)

,

(36)

(k)

I − 1
2

β(s, k) =
where
for any s ≥ 0, since4
αY (σ(x)) ≤ Y(x) ≤ αY (σ(x)), we deduce

αY
I − 1
2
(cid:1)
e
(cid:0)

αY

e

(cid:0)

(s) < s for s > 0 and
e

n

o

I − 1
2

αY

(s) for any s ≥ 0, with

e

β ∈ KL as limk→∞

(cid:1)
σ(φ(k, x)) ≤ max

e

(cid:0)
e
β(αY (σ(x)), k)

(cid:1)
, δ

α−1
Y

(k)

I − 1
2

(s) = 0
αY
(0) = 0. Finally, invoking
e

(cid:0)

(cid:1)

.

(37)

Thus Theorem 4 holds with β(s, k) = α−1
Y

(cid:16)
n
β(αY (s), k)

e

o
for any s ≥ 0 and k ∈ Z≥0.

(cid:17)

(cid:16)

(cid:17)

e
7.5. Proof of Corollary 5
We follow the steps made in (Granzotto, 2019, Corollary 3.2). Let x ∈ Rn. We select ε∗ < a2
aV as
in Corollary 5 and let ε ∈ Rnε such that |ε| ≤ ε∗ and v ∈ F ∗
ε (x). In particular, from item (i) of
Theorem 7, αW (σ(x) ≤ Y(x) ≤ αV (σ(x)), and since item (ii) of Assumption 1 holds with L = ∞
and aW s ≤ αW (s), αV (s) ≤ aV s with aW , aV > 0 for any s > 0, we obtain

W

Similarly, in view of item (ii) of Theorem 7, Y(v) − Y(x) ≤ −αW (σ(x)) + αV ◦ α−1
and since cstop(ε, x) ≤ |ε|σ(x) holds, we derive

W (cstop(ε, x)),

aW σ(x) ≤ Y (x) ≤ ¯aV σ(x).

(38)

Note that for |ε| ≤ ε∗,

a2
W −|ε|¯aV
aW

(cid:16)

(cid:17)

Y(v) − Y(x) ≤ −

a2
W − |ε|¯aV
aW

(cid:18)

σ(x).

(cid:19)

> 0. Hence, in view of (39) and (38),

Y(v) − Y(x) ≤ −

a2
W − |ε|¯aV
¯aV aW (cid:19)

Y(x)

(cid:18)
holds for any σ(x) ≥ 0. Let x ∈ Rn and denote φ(k, x) be a corresponding solution to (13)
at time k ∈ Z≥0, it holds that Y(φ(k, x)) ≤

Y(x). In view of (38), it follows

1 −

k

a2
W −|ε|¯aV
¯aV aW

from Y(φ(k, x)) ≤

1 −

σ(φ(k, x)) ≤ ¯aV

(cid:16)
aW σ(x)

k

a2
W −|ε|¯aV
¯aV aW
a2
(cid:17)
W −|ε|¯aV
¯aV aW

1 −

4. See (Granzotto, 2019, Lemma B.1)

(cid:16)

Y(x) that aW σ(φ(k, x)) ≤

(cid:16)

(cid:17)

1 −

a2
W −|ε|¯aV
¯aV aW

(cid:16)
and the proof is concluded.

k

(cid:17)

15

¯aV σ(x) hence

k

(cid:17)

(39)

(40)

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

7.6. Proof of Theorem 6
Let x ∈ Rn, ε ∈ Rnε such that |ε| < ε∗ where ε∗ is selected as in Corollary 5, φ(k + 1, x) ∈
ε (φ(k, x)) for any k ∈ Z≥0 where φ is a solution to (13) initialized at x. Let ur
F ∗
ε (φ(k, x))
such that φ(k + 1, x) = f (φ(k, x), ur
k), and note, since inputs from (13) are the ﬁrst input of
u∗∗∗
k) =
ℓ(φ(k, x), u∗∗∗

ε applied in a receding horizon fashion, ur

ε(φ(k, x))|0 and therefore ℓ(φ(k, x), ur

d(0, φ(k, x)) by deﬁnition of (10). Consider then

ε(φ(k, x))|0) = ℓ∗

k = u∗∗∗

k ∈ U ∗

∞

V run
ε

(x) =

ℓ∗
d(0, φ(k, x)),

Xk=0

(41)

Note that indeed V run

ε

(x) ∈ V run

ε (x). It follows from the proof of Theorem 7, in particular (27), that

V∞(φ(k + 1, x)) ≤ V∞(φ(k, x)) − ℓ∗

d(0, φ(k, x)) + αV ◦ α−1

W (cstop(ε, φ(k, x))),

from which we deduce, for any N ≥ 0,

N

Xk=0

ℓ∗
d(0, φ(k, x))

≤ V∞(φ(0, x)) − V∞(φ(1, x)) + αV ◦ α−1

W (cstop(ε, φ(0, x)))

+ V∞(φ(1, x)) − V∞(φ(2, x)) + αV ◦ α−1
+ . . .
+ V∞(φ(N, x)) − V∞(φ(N + 1, x))
+ αV ◦ α−1

W (cstop(ε, φ(N, x)))

W (cstop(ε, φ(1, x)))

≤ V∞(φ(0, x)) +

Hence, when N → ∞,

αV ◦ α−1

W (cstop(ε, φ(k, x))),

N

Xk=0

V run
ε

(x) ≤ Vε(φ(0, x)) +

αV ◦ α−1

W (cstop(ε, φ(k, x))).

∞

(42)

(43)

(44)

aW σ(x)

Xk=0
∞
k=0 αV ◦ α−1
W (cstop(ε, φ(k, x))), which is possi-
All that remains is to compute a bound on
k
a2
W −|ε|¯aV
ble by recalling that σ(φ(k, x)) ≤ ¯aV
holds from Corollary 5 and αV ◦
¯aV aW
W (cstop(ε, φ(k, x))) ≤ ¯aV
α−1
aW |ε|σ(φ(k, x)) as the conditions of Corollary 5 are assumed to hold.
(cid:17)
W (cstop(ε, φ(k, x))) ≤ |ε| ¯a2
∞
k=0 αV ◦ α−1
V
Speciﬁcally,
a2
W
a2
W −|ε|¯aV
∞
k=0
¯aV aW

(x) follows
vides (15) as
from the optimality of V∞(x). Since (44) holds for an arbitrary solution of (13), φ(k + 1, x) =
f (φ(k, x), ur

P
k) for any k ∈ Z≥0, the resulting bound holds for any V run

(cid:17)
(cid:16)
. The lower bound V∞(x) ≤ V run
ε

= ¯aV aW
a2
W −¯aV |ε|

a2
W −|ε|¯aV
¯aV aW

1 −
P
(cid:16)

(x) ∈ V run

, which pro-

ε (x).

∞
k=0

σ(x)

1 −

1 −

P

P

(cid:16)

(cid:17)

k

k

ε

References

B. D. O. Anderson and J. B. Moore. Optimal control: linear quadratic methods. Courier Corpora-

tion, 2007.

16

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

W. F. Arnold and A. J. Laub. Generalized eigenproblem algorithms and software for algebraic

riccati equations. Proceedings of the IEEE, 72(12):1746–1754, 1984.

F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learn-
In Advances in Neural Information Processing Systems, pages

ing with stability guarantees.
908–918, 2017.

D. P. Bertsekas. Dynamic programming and suboptimal control: A survey from ADP to MPC.

European Journal of Control, 11(4-5):310–334, 2005.

D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, Nashua,

USA, 4th edition, 2012.

D. P. Bertsekas. Value and policy iterations in optimal control and adaptive dynamic programming.
IEEE Transactions on Neural Networks and Learning Systems, 28(3):500–509, 2017. doi: 10.1
109/TNNLS.2015.2503980.

T. Bian and Z.-P. Jiang. Value iteration and adaptive dynamic programming for data-driven adaptive
optimal control design. Automatica, 71:348 – 360, 2016. ISSN 0005-1098. doi: https://doi.org/
10.1016/j.automatica.2016.05.003.

L. Bu¸soniu, T. de Bruin, D. Toli´c, J. Kober, and I. Palunko. Reinforcement learning for control:
Performance, stability, and deep approximators. Annual Reviews in Control, 46:8 – 28, 2018.
ISSN 1367-5788. doi: https://doi.org/10.1016/j.arcontrol.2018.09.005.

M. Granzotto. Near-optimal control of discrete-time nonlinear systems with stability guarantees.

PhD thesis, Université de Lorraine, 2019. URL http://www.theses.fr/2019LORR0301.

M. Granzotto, R. Postoyan, L. Bu¸soniu, D. Neši´c, and J. Daafouz. Optimistic planning
for the near-optimal control of nonlinear switched discrete-time systems with stability guar-
In IEEE Conference on Decision and Control, Nice, France, 2019.
antees.
URL
https://arxiv.org/pdf/1908.01404.pdf.

M. Granzotto, R. Postoyan, L. Bu¸soniu, D. Neši´c, and J. Daafouz. Finite-horizon discounted optimal
control: stability and performance. IEEE Transactions on Automatic Control, 2020a. doi: 10.1
109/TAC2020.2985904.

M. Granzotto, R. Postoyan, L. Bu¸soniu, D. Neši´c, and J. Daafouz. Stable near-optimal control of
nonlinear switched discrete-time systems: a planning-based approach. In Submitted to journal
publication, 2020b.

G. Grimm, M. J. Messina, S. E. Tuna, and A. R. Teel. Model predictive control: for want of a
local control Lyapunov function, all is not lost. IEEE Transactions on Automatic Control, 50(5):
546–558, 2005. ISSN 0018-9286. doi: 10.1109/TAC.2005.847055.

L. Grüne and A. Rantzer. On the inﬁnite horizon performance of receding horizon controllers. IEEE
Transactions on Automatic Control, 53(9):2100–2111, 2008. ISSN 0018-9286. doi: 10.1109/TA
C.2008.927799.

17

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

A. Heydari. Revisiting approximate dynamic programming and its convergence. IEEE Transactions

on Cybernetics, 44(12):2733–2743, 2014. doi: 10.1109/TCYB.2014.2314612.

A. Heydari. Analysis of stabilizing value iteration for adaptive optimal control. In 2016 American

Control Conference (ACC), pages 5746–5751, 2016. doi: 10.1109/ACC.2016.7526570.

A. Heydari. Stability analysis of optimal adaptive control under value iteration using a stabilizing
initial policy. IEEE Transactions on Neural Networks and Learning Systems, 29(9):4522–4527,
2017.

A. Heydari. Stability analysis of optimal adaptive control using value iteration with approximation
errors. IEEE Transactions on Automatic Control, 2018. ISSN 0018-9286. doi: 10.1109/TAC.20
18.2790260.

J.-F. Hren and R. Munos. Optimistic planning of deterministic systems. In European Workshop on

Reinforcement Learning, pages 151–164, Villeneuve d’Ascq, France, 2008.

Y. Jiang and Z.-P. Jiang. Computational adaptive optimal control for continuous-time linear systems
with completely unknown dynamics. Automatica, 48(10):2699 – 2704, 2012. ISSN 0005-1098.
doi: https://doi.org/10.1016/j.automatica.2012.06.096.

S. Keerthi and E. Gilbert. An existence theorem for discrete-time inﬁnite-horizon optimal control
ISSN 0018-9286.

IEEE Transactions on Automatic Control, 30(9):907–909, 1985.

problems.
doi: 10.1109/TAC.1985.1104084.

B. Kiumarsi, F. L. Lewis, and Z.-P. Jiang. H∞ control of linear discrete-time systems: Off-policy

reinforcement learning. Automatica, 78:144–152, 2017.

F. L. Lewis and D. Vrabie. Reinforcement learning and adaptive dynamic programming for feedback
control. IEEE Circuits and Systems Magazine, 9(3):32–50, 2009. doi: 10.1109/MCAS.2009.93
3854.

D. Liu, H. Li, and D. Wang. Error bounds of adaptive dynamic programming algorithms for solving
undiscounted optimal control problems. IEEE Transactions on Neural Networks and Learning
Systems, 26(6):1323–1334, 2015. doi: 10.1109/TNNLS.2015.2402203.

B. Pang, T. Bian, and Z.-P. Jiang. Adaptive dynamic programming for ﬁnite-horizon optimal control
of linear time-varying discrete-time systems. Control Theory and Technology, 17(1):73–84, 2019.

A. Pavlov, I. Shames, and C. Manzie. Early termination of NMPC interior point solvers: Relating
the duality gap to stability. In 2019 18th European Control Conference (ECC), pages 805–810,
2019. doi: 10.23919/ECC.2019.8795629.

R. Postoyan, L. Bu¸soniu, D. Neši´c, and J. Daafouz. Stability analysis of discrete-time inﬁnite-
horizon optimal control with discounted cost. IEEE Transactions on Automatic Control, 62(6):
2736–2749, 2017. ISSN 0018-9286. doi: 10.1109/TAC.2016.2616644.

R. Postoyan, M. Granzotto, L. Bu¸soniu, B. Scherrer, D. Neši´c, and J. Daafouz. Stability guarantees
for nonlinear discrete-time systems controlled by approximate value iteration. In IEEE Confer-
ence on Decision and Control, Nice, France, 2019.

18

WHEN TO STOP VALUE ITERATION:STABILITY AND NEAR-OPTIMALITY VERSUS COMPUTATION

R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,

USA, 2nd edition, 2017.

Q. Wei, D. Liu, and H. Lin. Value iteration adaptive dynamic programming for optimal control of

discrete-time nonlinear systems. IEEE Transactions on Cybernetics, 46(3):840–853, 2015.

19

