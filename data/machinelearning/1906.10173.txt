The Finite-Horizon Two-Armed Bandit Problem

with Binary Responses

A Multidisciplinary Survey of the History, State of the Art, and Myths

9
1
0
2

n
u
J

0
2

]

C
O
.
h
t
a
m

[

1
v
3
7
1
0
1
.
6
0
9
1
:
v
i
X
r
a

Peter Jacko
Department of Management Science
Lancaster University, UK

June 18, 2019

Abstract

In this paper we consider the two-armed bandit problem, which often naturally appears
per se or as a subproblem in some multi-armed generalizations, and serves as a starting
point for introducing additional problem features. The consideration of binary responses
is motivated by its widespread applicability and by being one of the most studied settings.
We focus on the undiscounted ﬁnite-horizon objective, which is the most relevant in many
applications. We make an attempt to unify the terminology as this is different across disci-
plines that have considered this problem, and present a uniﬁed model cast in the Markov
decision process framework, with subject responses modelled using the Bernoulli distribu-
tion, and the corresponding Beta distribution for Bayesian updating. We give an extensive
account of the history and state of the art of approaches from several disciplines, including
design of experiments, Bayesian decision theory, naive designs, reinforcement learning,
biostatistics, and combination designs. We evaluate these designs, together with a few
newly proposed, accurately computationally (using a newly written package in Julia pro-
gramming language by the author) in order to compare their performance. We show that
conclusions are different for moderate horizons (typical in practice) than for small horizons
(typical in academic literature reporting computational results). We further list and clarify
a number of myths about this problem, e.g., we show that, computationally, much larger
problems can be designed to Bayes-optimality than what is commonly believed.

Keywords: Multi-armed bandit problem; Design of sequential experiments; Bayesian de-
cision theory; Dynamic programming; Index rules; Response-adaptive randomization;

1

 
 
 
 
 
 
1 Introduction

Statistical testing based on randomized equal allocation is a widespread state-of-the-art ap-
proach in the design of experiments for around 100 years, known today as the randomized
controlled trial in biostatistics, the between-group design in social sciences, and the A/B testing
in digital marketing. Already Thompson (1933), a biostatistician from Yale University, pro-

posed a data-driven approach which would in expectation lead to a higher reward from an

experiment, using the following words:

“...there can be no objection to the use of data, however meagre, as a guide to ac-

tion required before more can be collected ... Indeed, the fact that such objection can

never be eliminated entirely—no matter how great the number of observations—

suggested the possible value of seeking other modes of operation than that of tak-

ing a large number of observations before analysis or any attempt to direct our

course...”

Robbins (1952), a prominent mathematician and statistician, emphasized that this problem is

of a much wider importance:

“In fact, the problem represents in a simpliﬁed way the general question of how we

learn—or should learn—from past experience.”

A formulation of the problem using the Bayesian decision-theoretic framework allows for

Bayes-optimality. Practical application of this Bayesian approach has however been long hin-

dered by its computational complexity, since the optimal solution is known in analytical form

only for inﬁnite horizon (Kelly, 1981). A variety of practical approximations and heuristics

have been developed and studied across several disciplines in order to overcome this issue,

but their analysis failed to give exact results or bounds sufﬁciently close to Bayes-optimality

for ﬁnite horizon problems, which are the problems most relevant to many situations in prac-

tice.

1.1 Paper Structure and Contributions

In this paper we thus focus on the ﬁnite-horizon setting. We also restrict the discussion to

two arms, which often naturally appears per se or as a subproblem in some multi-armed gen-

eralizations (e.g. if new arms appear over time), and serves as a starting point for introduc-

ing additional problem features. The consideration of binary responses is motivated by its

widespread applicability and by being one of the most studied settings.

Our main objective is to give an account of modelling and solution approaches arising in

different disciplines, in a uniﬁed framework and using a uniﬁed terminology. The problem

description, origins and terminology is given in Section 2. Our uniﬁed model is described in

Section 3, cast in the Markov decision process framework, with subject responses modelled us-

ing the Bernoulli distribution, and the corresponding Beta distribution for Bayesian updating.

Different problem settings, assumptions and objectives are summarized in Section 4. Section 5

2

gives an account of the history and state of the art of approaches from several disciplines. In

Section 6 we evaluate these designs, together with a few newly proposed, accurately com-

putationally (using a newly written package in Julia programming language by the author)

in order to compare their performance, showing that conclusions are different for moderate

horizons (typical in practice) than for small horizons (typical in academic literature report-

ing computational results). We further list and clarify a number of myths about this problem

in Section 7, e.g., we show that, computationally, much larger problems can be designed to

Bayes-optimality than what is commonly believed. Section 8 concludes.

2 Problem

We consider the problem with two arms (or, interventions), called C (mnemonically for “con-
trol” or “comparator” or standard of “care”) and D (for “discovery” or “development”).1 T
subjects become available one by one, and each subject must be allocated to exactly one of the
arms. Upon allocation of a subject to arm C (D), subject’s response is observed, which is binary
(success/failure), where the success probability is θC (θD) and the failure probability is 1 − θC
(1 − θD). The primary objective is to ﬁnd a design, i.e. a strategy composed of randomized ac-
tions of allocating the subjects to arms, which, in expectation, achieves the highest number of
observed successes from the T subjects, assuming that the success probabilities are unknown.
A formal model is given in Section 3.

2.1 Problem Origins

The ﬁrst statement of the two-armed bandit problem is in Thompson (1933), extended in

Thompson (1935) to multiple arms, in a Bayesian setting. Apparently unaware of Thomp-

son’s works, Robbins (1952) formulated the two-armed bandit problem in a frequentist set-

ting. Neither Thompson (1933, 1935) nor Robbins (1952) used the terms “arm” or “bandit”.
The term two-armed bandit problem ﬁrst appeared in Bradt et al. (1956), referring to the setting
with binary responses in which one knows the set {θC, θD}, but does not know which arm is
which. Bradt et al. (1956) also proposed a generalization of that problem, in which {θC, θD} is
unknown, which is the two-armed bandit problem as known today and as considered in this
paper. Bellman (1956) referred to the latter problem as the two-machine problem.

P. Whittle stated on several occasions that researchers were aware of this type of problem
since the 1940s and considered it an important but very hard open problem.2 That could be
attributed to the absence of a suitable mathematical framework and theory, as the progress

on the problem occurred slowly alongside the emergence and the development of areas such

1In the existing literature, it is common to denote the two arms as 1, 2 (but we prefer to keep these names for

actions deﬁned below) or A, B (but we prefer to keep A for the action process and B for the Beta function).

2“...it was formulated during the war, and efforts to solve it so sapped the energies and minds of Allied analysts
that the suggestion was made that the problem be dropped over Germany, as the ultimate instrument of intellectual
sabotage” Whittle (1979); “...propounded during the Second World War, and soon recognized as so difﬁcult that it
quickly became a classic, and a by-word for intransigence.” Whittle (1989); “...had resisted analysis, however, to
the point of being regarded by some as intrinsically insoluble.” Whittle (2002).

3

as sequential analysis, Bayesian statistics, decision theory, dynamic programming, stochastic

processes, and concentration inequalities.

Indeed, early papers describing theoretical solutions on the bandit problem were often

among the pioneers in these areas, introducing novel terminology and notation, not all of

which has been adopted more generally, and might thus be hard to read for today’s researchers.

Drawing on the early research in 1950s and 1960s, three dominant “schools” have emerged:

• the Berry’s school: starting with Berry (1972), and rewritten and further developed in

Berry and Fristedt (1985), focussing predominantly on the ﬁnite-horizon setting;

• the Gittins’ school: starting with Gittins and Jones (1974), and rewritten and further de-
veloped in Gittins (1979, 1989); Gittins et al. (2011), focussing predominantly on the dis-

counted inﬁnite-horizon setting;

• the Robbins’ school: starting with Lai and Robbins (1985), and rewritten and further devel-
oped in Agrawal (1995); Katehakis and Robbins (1995); Burnetas and Katehakis (1996),

focussing predominantly on the time-average inﬁnite-horizon setting.

The author’s suggestion is that this pioneering literature should be on the must-read list of

researchers on bandit problems, regardless of their discipline.

While the Robbins’ school makes complete learning (i.e. identiﬁcation of the better arm in

inﬁnite time almost surely) lexicographically more important than the way of attaining it, both

the Berry’s and Gittins’ schools replace the lexicographic ordering by resolving the trade-off
between complete learning and earning (of rewards), with the relative weights implicitly given
by the horizon and the discount factor, respectively. In Section 5 we describe these “schools”

and their relationships in more detail.

Of course, there are several other fascinating variants of the bandit problem, with differ-

ent objective (e.g., risk-averse, adversarial, ﬁnal-period-only, etc.), different control (e.g., ran-

domized, multi-mode, multi-resource, duelling, multi-player, etc.), and/or different dynamics

(e.g., non-binary responses, delayed responses, partial observability, arriving arms, covariates,

correlation, restlessness, non-stationarity, non-Markovian, etc.); all these are unfortunately be-

yond the scope of this paper.

2.2 Applications

The problem has been formulated, addressed or applied in a number of disciplines, each devel-

oping its own terminology, see Table 1. In this paper we use the terminology which we believe

is a reasonable compromise and should not cause confusion for researches and practitioners

from all the disciplines. The author wishes to encourage researchers from all the disciplines

to follow this terminology as closely as possible to facilitate for researchers and practitioners

from other disciplines to learn about their work.

According to Scott (2010): “Multi-armed bandits have an important role to play in mod-
ern production systems that emphasize continuous improvement, where products remain in a

4

perpetual state of feature testing even after they have been launched.” The most commonly

listed applications, often requiring to adapt the generic multi-armed bandit problem to speciﬁc

features, are as follows:

• Digital marketing: In the digital world it is relatively easy to introduce and quick to get
feedback on new document variants, and so bandit problems have been proposed for

social media advertising, personalized websites and user interfaces, email campaigns,

inﬂuence maximization, etc; see, e.g., Liberali et al. (2017). Bandit problems can also

be used to address the problem of dynamic pricing with demand uncertainty, which

requires to solve a trade-off of learning (of the demand curve) and earning (the highest

revenue); for a survey, see, e.g., den Boer (2015).

• Clinical trials: Thompson (1933) pointed out that his bandit problem “...would be im-
portant in cases where either the rate of accumulation of data is slow or the individuals

treated are valuable, or both.” Gluss (1962) further explained the motivation primar-
ily by rare diseases. Following the focus on rare and/or life-threatening diseases, a few
novel bandit-based designs have been developed and proposed recently, and are being

implemented in a growing number of trials, mainly in several types of cancer, where

patients are stratiﬁed into smaller groups using genetic biomarkers. Discussions about

the advantages and disadvantages of bandit-based designs are ongoing, e.g., Berry and

Esserman (2016) argue that, in certain clinical trials, data-driven approaches make great

sense ethically, statistically, economically, scientiﬁcally, and logistically. For a survey on

real adaptive trials, see e.g., Bothwell et al. (2018).

• Search: Bandit designs have been proposed for recommender systems in which new items
and users appear frequently in order to assure sufﬁcient exploration, see, e.g., Aggarwal

(2016). Although digital search is typically considered by recommender systems, many

non-digital search applications exist, e.g. search for natural resources, search and rescue,

surveillance and monitoring. Related to this category are also the so-called best-arm

identiﬁcation problem and problems appearing in ranking and selection.

3 Model

In this section we formulate a general two-armed problem with binary responses as a Markov

decision process, which provides sufﬁcient generality to accommodate all the solution ap-

proaches discussed in Section 5.

Interventions. We consider arms (or, interventions) labelled by k ∈ K := {C, D}. A subject
must be allocated to exactly one intervention, and such allocation yields a binary response from
that intervention: 0 (failure) or 1 (success). The response set is denoted by O := {0, 1}. Subject
responses are uncertain, i.e., modelled as Bernoulli-distributed with parameter 0 ≤ θk ≤ 1,

5

Anecdotic

strategy

choice

pull

arms

Operations & Management
Reinforcement learning
Biometrics & Biostatistics
Ranking & selection
Economics
Computing & Telecom.
Marketing
Transportation

policy
algorithm
design
policy
strategy
scheduler
policy
driver

allocation
decision
randomization
spread over
choice
allocation
allocation
selection

resource
time step
patient
measurement
resource
server
impression
vehicle

projects
actions
treatments
alternatives
experiments
jobs
advertisements
roads

This paper

design

randomized action

subject

interventions/arms

Table 1: An illustration of typical terminology for bandit problems across disciplines

the success probability, independent across arms. The responses are immediate, meaning that the
response of an allocated subject is observed before the next decision needs to be done.

Timing. Subjects arrive (i.e., are recruited) sequentially (i.e., one by one) at random moments
in continuous time. Since we do not discount the future, we can without loss of generality
focus only on the moments of subjects’ arrivals, which we call discrete time epochs and see as
regularly spaced. That is, equivalently, we can consider that subjects arrive at time epochs
t ∈ T := {0, 1, 2, . . . , T − 1}, where T ≤ +∞ is the number of subjects in the trial, i.e., the trial
size, or the time horizon. To clarify, the (t + 1)-st subject arrives at time epoch t. Note that t = T
is the time epoch denoting the end of the trial, when the response of the last subject is observed

and no subject arrives.

States. At any moment in continuous time, the physical state is represented by the numbers
of observed successes and failures on each arm, the number of allocated subjects without an

observed response on each arm, the number of arrived subjects without being allocated, and
the number of remaining subjects to arrive. This is a vector with 8 elements summing up to T at
any moment (all are non-negative integers). At time epochs, this can be simpliﬁed without loss
of generality to a vector with 5 elements, with the numbers of observed successes and failures
on arm C denoted by sC and fC, respectively, the numbers of observed successes and failures
on arm D denoted by sD and fD, respectively, and the number of remaining subjects to be
allocated (exactly one of which has arrived), n. Since at time epochs sC + fC + sD + fD + n = T ,
it is sufﬁcient to keep track of any four of these ﬁve numbers, leading to a state as vector
with 4 elements, which we choose to be x := (sC, fC, sD, fD). Note that at time epoch t,
sC + fC + sD + fD = t.

In addition to the physical state, there is an information state, which at any moment in con-
tinuous time captures all the information that could possibly affect the decisions. This may

include real-world evidence and/or modelling assumptions. The real world evidence may

be available before the start and/or it can arrive anytime during the trial. The modelling as-

sumptions typically refer to the parameters of the prior distributions (built on historical data or

6

expert opinions) for the success probability of each arm (whose weight may change over time,

and can be either informative or non-informative), but may also include other parameters such

as the probability of dropouts, the probability of errors in recording the observations and/or

the subject allocations, the probability of mistakes in the statistical analysis and/or in the ad-

ministration process, the timing of planned interim analyses, the probability and/or timing of

unplanned stopping of the trial due to safety concerns, the estimate of the size of the subject
population after the end of the trial, etc. For full generality, we consider the information state i
(potentially dependent on the current physical state x and/or otherwise changing during the
trial), and thus the state is (x, i).

Actions. At every time epoch t ∈ T the design must prescribe how the arrived subject should
be randomized (i.e., randomly allocated) to interventions. While there are only two possible
allocations, in every state we consider a possibly inﬁnite action set A(x,i) of randomized actions
a identiﬁed by probabilities (pa
D), meaning that the subject is allocated to intervention C
(B) with probability pa
D = 1)}. Since
from the theory of Markov decision processes it follows that an action which is a randomized

D). Formally, A(x,i) ⊆ {a : pa

D ≥ 0, pa

C ≥ 0, pa

C + pa

C (pa

C, pa

C, p1

D)) and action 2 (identiﬁed by (p2

combination of other two actions is optimal only if all three are optimal, it is sufﬁcient to
consider only an action set of two pure randomized actions, which we call action 1 (identiﬁed
C, p2
by (p1
D)). For convenience in situations when both
actions are optimal, we also consider an equally-weighted mixed randomized action, which we call
C)/2, (p1
action 3 (identiﬁed by (p3
D)/2)), which is a combination of the
two pure randomized actions with equal weights. Formally, A(x,i) = {1, 2, 3}, and without
loss of generality we assume p1
C. In some approaches discussed in Section 5, there is no
choice of actions, meaning that the cardinality of A(x,i) is one, which can be obtained by setting
1 ≡ 2 ≡ 3, effectively reducing the Markov decision process to a Markov reward process.

D) := ((p1

C ≥ p2

D + p2

C + p2

C, p3

In some approaches, the action set depends on the observations only via their sum t =
sC + fC + sD + fD, thus can be written as A(t). Finally, the simplest case is the one in which
the action set is constant, which we write as A.

Transition Probabilities. Denote by qk,(x,i),o the probability of observing response o ∈ O for
the current subject if it is allocated to arm k ∈ K in state (x, i). We assume that (cid:80)
o∈O qk,(x,i),o =
1 for all k, but this can be relaxed in some models, e.g. if allowing for dropouts (i.e., missing
responses).

If the information state i does not change during the trial, then the transition probabilities

of moving from state (x, i) to state (x(cid:48), i) under action a are

ha
(x,i),(x(cid:48),i) =






pa
CqC,(x,i),1
pa
CqC,(x,i),0
pa
DqD,(x,i),1
pa
DqD,(x,i),0

if x(cid:48) = x + e1
if x(cid:48) = x + e2
if x(cid:48) = x + e3
if x(cid:48) = x + e4

7

where ej is the standard basis vector. If the information state changes during the trial, then
these transition probabilities need to be amended to reﬂect its dynamics.

Expected One-Period Rewards. The expected one-period reward ra
(x,i) for all states (x, i)
and all actions a needs to be deﬁned. If the information state i does not change during the
trial, then it is as follows: ra
(x,i) = 0 for all states such that sC + fC + sD + fD ≤ T − 1 and
ra
(x,i) = sC + sD for all states such that sC + fC + sD + fD = T (i.e., the reward is the number
of observed successes in all states in which the trial can eventually end).

The above deﬁnition of the reward is novel. The conventional one is to set the reward to
(x,i) =
(x,i) = 0 at time epoch t = T .

the expected value of observing one success in a given state under a given action, i.e., ra
pa
CqC,(x,i),1 + pa
In Appendix B we prove the following theorem.

DqD,(x,i),1 at all time epochs t = 0, 1, . . . , T − 1 and ra

Theorem 1. The two reward deﬁnitions give the same expected total reward for any ﬁxed design.

State and Action Processes. The evolution of a Markov decision process is captured by the
state process, which in full generality is 2-dimensional in order to keep the physical and in-
formation states separately, (X(·), I(·)), and the action process which depends on the state
process, but can be brieﬂy written as A(·), where A(X(t),I(t)) ∈ A(X(t),I(t)).

4 Assumptions, Settings and Objectives

4.1 Usage

There are three principal types of usage of the model described above.

Evaluation by Simulation. Computer simulation is now a commonly used evaluation tool
as it is relatively straightforward and the accuracy vs runtime trade-off can be addressed by
adjusting the number of simulation runs. But we believe that it has the law-of-the-hammer syn-
drome of all simple universal tools: “if the only tool you have is a hammer, to treat everything

as if it were a nail.”

Evaluation by Backward Recursion.

In this paper, we give evidence that it is possible and

preferable to use backward recursion instead of simulation for evaluation. This yields a per-

fectly accurate evaluation (subject to computational accuracy of the chosen numerical type).

We discuss its runtime in Section 7.

Optimization. When the action set is not singular in all states, there is room for choosing
one of the actions for every state according to an objective of maximizing some function. This

does not necessarily need to be done by backward recursion; we describe several approaches

in Section 5. In this case we assume that the success probabilities are unknown as otherwise it

is trivial to optimize.

8

4.2 Knowledge Assumptions

While the success probabilities are assumed unknown for optimization, they may not neces-
sarily be so for evaluation. There are two principal ways of specifying the probabilities qk,(x,i),o
of observing response o, which depend on the knowledge assumption about the success prob-
abilities.

Known Success Probabilities. Evaluation of all the approaches described in Section 5 can be
done by assuming that success probabilities θk are known, and part of I(t) for all t. In that case
the transition probabilities are independent of the physical state, so we can write qk,i,o. If the
information state is i = (θC, θD) during the whole trial, then

qk,i,o =




θk

if o = 1



1 − θk

if o = 0

Unknown Success Probabilities. Approaches that allow for optimization assume that the
success probabilities are unknown (otherwise the decision with the objective of maximising

the expected number of successes is trivial), and so require estimates of these, which can be

obtained using Bayesian updating. Following the existing literature, we use the Bayesian Beta-
Bernoulli model for each arm k ∈ K, in which θk is assumed to be a random variable drawn
from Beta distribution dependent on the state. At the initial time epoch t = 0, i.e., in physical
state (0, 0, 0, 0), each arm k is given a prior Beta distribution with parameters (cid:101)sk(0), (cid:101)fk(0). These
parameters can be interpreted as the numbers of pseudo-observations of successes and failures
before the start of the trial. They are thus part of the information state I(t) for all t and do not
change over time. At every time epoch t, in physical state x = (sC, fC, sD, fD), each arm k has
the posterior distribution given, because of conjugacy, by the Beta distribution with parameters
(cid:101)sk, (cid:101)fk, brieﬂy Beta((cid:101)sk, (cid:101)fk), where (cid:101)sk = (cid:101)sk(0) + sk and (cid:101)fk = (cid:101)fk(0) + fk.

If the information state is i = ((cid:101)sC(0), (cid:101)fC(0), (cid:101)sD(0), (cid:101)fD(0)) during the whole trial, then

qk,(x,i),o =






(cid:101)sk
(cid:101)sk+ (cid:101)fk
(cid:101)fk
(cid:101)sk+ (cid:101)fk

if o = 1

if o = 0

The conventional assumption is to take the uniform distribution as a prior distribution
on each arm k, i.e., ((cid:101)sk(0), (cid:101)fk(0)) = (1, 1); for a discussion on choosing a different prior Beta
distribution, see Appendix A.

4.3 Performance Measures and Objectives

In this paper we focus on the number of successes as the principal performance measure (a.k.a.
operating characteristic in clinical trials literature). But instead of the number of successes, we
report two equivalent measures: the proportion of successes and the regret number of successes, as
these provide complementary interpretation and insights. Yet another equivalent measure (not

9

reported in this paper) is the fraction of subjects allocated to the better arm. Many other additive
measures, e.g. monetary cost typical in health economics and health technology assessment,

can be deﬁned analogously, but are not discussed in this paper.

A particular design π prescribes the action process A(·). Let Π be the set of designs that are

non-anticipating3 and satisfy the above constraints on A(·).

Let us denote by Eπ

t [·] the expectation under design π ∈ Π conditioned on information

available at time epoch t ∈ T . The mean number of successes is

Nπ

t (x, i) := Eπ
t

(cid:34) T

(cid:88)

u=t

A(X(u),I(u))
r
(X(u),I(u))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(X(t), I(t)) = (x, i)

.

(cid:35)

and the mean proportion of successes is

Pπ

t (x, i) :=

1
T − t

Nπ

t (x, i) .

(1)

(2)

These two are measures of subject beneﬁt, while the former is on the absolute scale, the latter
yields the average per-subject probability of observed success, i.e., the subject beneﬁt on the

percentage scale. We further deﬁne the mean regret number of successes,

Rπ

t (x, i) := E [ max{θC, θD}| I(t) = i] − Nπ

t (x, i) ,

(3)

which is a measure of subject loss. Note that all the three measures depend on parameter T ,
although we have suppressed the explicit notation.

The objective is to ﬁnd an optimal design π∗ that maximises the mean number of successes

as evaluated at time epoch t = 0 when there are no observations (x = 0), i.e.,

π∗ := arg max

π∈Π

Nπ

0 (0, i)

(4)

or, equivalently, maximizes the mean proportion of successes, or, equivalently, minimizes the

mean regret number of successes.

Following the two knowledge assumptions above, we have two approaches to perfor-

mance evaluation.

Known Success Probabilities. When the success probabilities are assumed to be known, we
call the above measures the frequentist number of successes, the frequentist proportion of successes,
and the frequentist regret number of successes, respectively. Due to symmetry, we can assume that
θC ≤ θD without loss of generality, and thus

Rπ

t (x, i) = θD − Nπ

t (x, i) .

(5)

3A non-anticipating design is a design which cannot see into the future; i.e., an action prescribed by the design

at a given time epoch does not require the knowledge of states which have not been observed yet.

10

Unknown Success Probabilities. When the success probabilities are assumed to be unknown,
we evaluate performance in terms of the quantities known in the literature as the Bayes return,
Bayes worth, or Bayes risk. In particular, we call the above measures the Bayes number of suc-
cesses, the Bayes proportion of successes, and the Bayes regret number of successes, respectively.

For a problem with uniform distribution as a prior distribution on each arm, as considered

in this paper, E [ max{θC, θD}| I(t) = i] = 2/3 (Berry, 1978), thus

Rπ

t (x, i) =

2
3

− Nπ

t (x, i) ,

(6)

Note that our deﬁnition is different from the so-called Bayesian regret (cf. Lattimore and

Szepesv´ari, 2019, Section 34.6), which is the average of the frequentist regret with respect to

the prior distribution, i.e. it is Bayesian only in the initial time epoch.

5 Approaches

In this section we describe a number of common approaches to the problem described in the

previous section. We start with Equal Randomized Allocation, which was originally developed

for static setting, in which all the subjects arrive at the same time. All the remaining approaches

are suitable only in dynamic setting, i.e., are data-driven.

5.1 Design of Experiments

Statistical testing based on equal (i.e., 1:1) randomized allocation (a.k.a. random assignment) is a
widespread state-of-the-art approach in the design of experiments today, known as randomized
controlled trial in medicine, between-group design in social sciences, and A/B testing in digital
marketing. I theory, it allows the greatest reliability and validity of statistical estimate of the

intervention effect (i.e., the difference between the two success probabilities) under an initial

equipoise assumption. Originally developed, advocated and popularized by the founders of

statistics such as C. S. Peirce in the ﬁelds of psychology and education in the late 19th century,

and J. Neyman and R. A. Fisher in agriculture and other ﬁelds in the early 20th century. In the

middle of the 20th century, A. B. Hill popularized the method in the ﬁeld of medicine and it is

now the preferred approach by regulatory agencies for assessing efﬁcacy when deciding about

marketing authorisation of new medicinal interventions in many countries. The approach can

be evaluated using routine (frequentist) statistical methods.

In this approach the transition probabilities are frequentist, and

• the information state i is ignored;

• the action set is constant, |A| = 1 and pa

C = pa

D = 1/2;

If the success probabilities are known, then its performance can be evaluated directly: the

(cid:112)(θC + θD)(2 − θC − θD)/4T ;

proportion of successes with mean (θC+θD)/2 and standard deviation

11

the regret number of successes with mean T max{(θC − θD)/2, (θD − θC)/2} and standard de-
viation

(cid:112)T (θC + θD)(2 − θC − θD)/4.

The approach is believed to be well understood. However, there are several potential con-

cerns, e.g., (i) it was developed under the assumption of an inﬁnite population of subjects to

which the best arm will be applied, but it is not clear how valid it is when the population is

ﬁnite or when new, potentially better arms become available in future; (ii) it was developed un-

der the assumption of parallel allocation of subjects, but in dynamic (i.e. sequential) setting, in

which subjects arrive one by one, there is a risk of introducing accrual and/or allocation bias in

the estimation of the success probabilities if the person (or machine) delivering an intervention

is aware of the characteristics and/or responses of the previous subjects; (iii) it was developed

under the assumption of the simple random sampling method, which is not true if the subjects

need to provide a consent (which is a legal requirement, for instance, in clinical trials); (iv) it

was developed under the assumption of the use of random numbers, which not true for com-

puter generated randomization (pseudo-random numbers) provided by error-prone software

and procedures of external companies.

5.2 Bayesian Decision Theory

Bayesian Decision Theory emerged together with game theory and mathematical program-

ming in the middle of the 20th century, building on models and techniques from Bayesian

statistics, applied mathematics, and economics. A number of prominent researchers con-

tributed to the development of the ﬁeld, including A. Wald, J. Wolfowitz, L. J. Savage, K. J.

Arrow, D. Blackwell, H. Raiffa, R. Schlaifer, R. Bellman, etc.

The two-armed problem with binary responses was ﬁrst formulated in this framework in
Bellman (1956), with a known success probability of one arm, over an inﬁnite horizon T = +∞,
and with geometric discounting of future rewards. He proposed to employ backward recur-

sion based on the Bellman equation from dynamic programming he had developed. Gluss

(1962) extended the model to two unknown arms using the same approach and discussed the

theoretical memory requirement relevant for online optimization assuming a ﬁnite-horizon

truncation, after which the better arm is used forever. Steck (1964) programmed the backward

recursion on one of the scientiﬁc computers of those times (Univac 1105) and listed the optimal
allocations with the truncation at T = 25 (and the discount factor 0.95).

For ﬁnding the best action for each state via this approach, the transition probabilities are

Bayesian, and

• the conventional assumption is to take as information state i (that does not change over

time) the prior distribution on each arm k;

• the action set is constant, |A| = 3;

Since Theorem 1 is true for any design, it is also true for the optimal design, which is de-

rived using the Bellman equation, which is an optimization equivalent of the Poisson equation.

12

Figure 1: An illustration of computational complexity of online calculation of the deterministic
Bayesian decision-theoretic design over a range of trial sizes.

Theorem 2. The two reward deﬁnitions give the same optimal design and to the same optimal expected
total reward.

The conventional assumption is that the two pure actions are deterministic, i.e., p1
C = p1

D =
D = 0, which we will refer to as the deterministic Bayesian decision-theoretic design,

C = p2

1, p2

despite the fact that randomization is allowed when both actions are optimal.

A more general setting, in which the pure actions are allowed to be randomized instead of

deterministic, was proposed in Cheng and Berry (2007) and further developed in Williamson

et al. (2017). In fact, such a setting provides a continuum of designs, ranging from the determin-

istic Bayesian decision-theoretic design to the equal randomized allocation design, recovered
by setting p1

C = p2

C = p1

D = 1/2.

D = p2

5.2.1 Optimal — Dynamic Programming

The Bayesian decision-theoretic model can be solved by (stochastic) dynamic programming
(DP), which comprises of a calculation starting by enumerating all the possible states in the
ﬁnal time period, continuing backwards in time while employing the Bellman equation in ev-

ery state. Unfortunately, complete structure of the optimal design is unknown, thus numerical

computation is the only way of obtaining it.

Figure 1 illustrates the computational complexity of online calculation (i.e., outputting the
optimal action of the initial state) of this design, using a state-of-the-art package BinaryBandit
in Julia programming language. In the number of elements (i.e., without multiplying by the

memory required for each element), the memory requirement presented in Figure 1(right) is
2 times larger than that of Gluss (1962) because of his elimination of symmetric states, which
can be done if the prior Beta distributions are the same for the two arms.

The computational complexity of ofﬂine calculation (i.e., outputting the optimal actions of
all possible states) of this design is very similar to online calculation in terms of the runtime,
but radically different in terms of memory requirement. Using the BinaryBandit Julia package,
a computer with 32GB RAM is able to solve the problem of up to trial size T = 1, 440, keeping
all the optimal actions in RAM during the calculations. For practical purposes, however, it

may be possible to store parts of the solution on hard disk and thus relieve RAM memory to

13

Trial size120240360480600720840960108012001320144015601680180019202040216022802400252026402760288030003120324033603480360037203840396040804200432044400.01 sec0.1 sec1 sec10 sec1 min10 min1 hr10 hr3 daysRuntime (log scale)Trial size120240360480600720840960108012001320144015601680180019202040216022802400252026402760288030003120324033603480360037203840396040804200432044401 MB10 MB100 MB1 GB10 GB32 GB100 GBMemory requirement (log scale)Software

RAM

T = 60 T = 120 T = 180 T = 240 T = 300 T max

Julia 0.6.2 & ad hoc
Julia 1.0.1 & ad hoc
R & ad hoc
Julia 1.0.1 & BB

2sec
12 GB
1sec
12 GB
1sec
16 GB
31 GB 0.0036sec

22sec
17sec
12sec
0.046sec

R & ad hoc
Julia 1.0.1 & BB

5 GB
1sec
31 GB 0.0040sec

6sec
0.056sec

108sec
82sec
59sec
0.23sec

26sec
0.27sec

331sec
262sec
191sec
0.73sec

84sec
0.91sec

789sec
643sec
N/A
1.6sec

209sec
2.8sec

420
420
240
1440

420
4440

Table 2: A comparison of runtime of the Bayesian decision-theoretic design for T = 60 : 60 :
300 and the largest horizon T max (as a multiple of 60) which does not give an out-of-memory
error using code implementations by author’s colleagues and students. The top four are for
ofﬂine calculation, the bottom two are for online calculation. BB refers to the use of the Bina-
ryBandit package.

allow calculation for larger trial sizes.

A number of author’s colleagues and PhD students who had programmed this design for

their needshave kindly provided runtimes of their code implementations, which are presented

in Table 2 for comparison, indicating that the BinaryBandit package is two orders of magnitude

faster than ad hoc codes and is able to solve a few times larger problems.

5.2.2 Asymptotically Optimal — Gittins and Whittle Index Rules

The structure of the optimal Bayesian decision-theoretic designs with deterministic pure ac-
tions is however known when considering the rewards over an inﬁnite horizon T → +∞ and
discounted with a geometrically-distributed discount factor 0 < γ < 1. Gittins and Jones
(1974) discovered that optimal allocations can be characterized by an index rule, which allows
(Gittins) index values to be calculated for every arm separately, and which at every time epoch

allocates a subject to the arm with the highest Gittins index value (breaking the ties arbitrarily).

See, e.g., Gittins (1979, 1989); Gittins et al. (2011) for general theory on the Gittins index.

Although theoretically appealing and useful in many other problems, in the setting of the

Bayesian decision-theoretic design it does not provide an ultimate solution, because the states
are time-dependent and the horizon T is ﬁnite. Still, the Gittins index rule can be used as an
approximation to the optimal design, being asymptotically optimal as T → +∞. It decreases
the computational complexity of the ofﬂine calculation notably because of a decreased size of

the problem (one arm, i.e. two dimensions), but is still computed by dynamic programming,

which is computationally demanding. In its calculation, however, it is necessary either to use
a discount factor and/or a horizon truncation, or to compute it for every horizon T considered
by adding the remaining number of subject allocations as a third dimension. In the former

case, the resulting index rule is only an approximation and tends to focus on learning less than

the optimal design, see e.g. Villar et al. (2015). In the latter case, Ni ˜no-Mora (2011, Section

6.2) provides a comparison of an approximate, so-called calibration, algorithm (using a grid

of values to desired accuracy) and an exact algorithm for the calculation of the Whittle index

14

values, showing that already T ≤ 100 requires several GB of RAM, which suggests that the
calibration algorithm with a grid of not more than 4 signiﬁcant digits is the only practical
method for larger horizons.

Besides the requirement of inﬁnite horizon, the theory of the Gittins index only applies

when the pure actions are deterministic. When these are randomized, the problem becomes
so-called restless, meaning that more than one arm can change its state in every period. Also,
even when the pure actions are deterministic, but the horizon is ﬁnite, the problem can be seen

as restless, by adding the remaining number of subject allocations to the state of each arm.

Whittle (1988) proposed to solve the restless problem also by an index rule, acknowledging

that such a rule would not necessarily be optimal, but conjecturing that it would admit a form

of asymptotic optimality as both the number of arms and the number of allocated arms in

each period grow to inﬁnity at a ﬁxed proportion, which was eventually proved in Weber and

Weiss (1990) under some technical assumptions. Whittle deﬁned an index, which reduces to
the Gittins index in the non-restless setting, which became known as the Whittle index. The
above discussion suggests that the Whittle index rule is conceptually more appropriate and

more accurate than the Gittins index rule, and this was conﬁrmed for the Bayesian decision-

theoretic design numerically, see, e.g., Villar et al. (2015); Villar (2018). The computational

complexity of the approximate Whittle index values calculated by the calibration method for
every horizon T considered (using a grid of values to desired accuracy) is similar to that of the
approximate Gittins index values using the same method.

Note that both the Gittins and Whittle index rule require to keep 3 elements in the one-
arm state, so the reduction from the optimal two-armed problem is only by one dimension.

Moreover, in ofﬂine calculation, the index values that need to be stored are non-integer, thus
require 64 bits per one-arm state, while the ofﬂine calculation of the solution to the two-armed
problem stores directly optimal actions, which require 2 bits per two-arm state. Thus, the index
rules (calculated to a few signiﬁcant digits) are typically preferable to dynamic programming
in calculation for horizons around T ≥ 1000 and only if planned to be used for evaluation by
simulation or for implementation in practice. However, Kaufmann (2018) reports that she was
able to compute the Gittins index values only up to T = 1000.

Of course, advantages of index rules become important in problems with more than two
arms, in which dynamic programming suffers from the curse of dimensionality. A discussion of
such problems is however beyond the scope of this paper.

5.2.3 Approximately Optimal — Approximate Dynamic Programming

Several general approaches have been proposed to deal with the curse of dimensionality of

stochastic optimization problems, which are collectively known as the approximate dynamic

programming. There is a number of approximation techniques, but broadly focus on prob-

lem size reduction (e.g., the state space is approximated by a grid for which optimal actions

are computed, and interpolated on non-included states) and/or on simpliﬁcation of function

computations (e.g. the value function is approximated by looking at decisions only a few pe-

15

riods ahead). See, e.g., Powell and Ryzhov (2018, Section 6.6), Ahuja and Birge (2019).

We will describe one approach, which leads to a Bayesian design known as the knowledge
gradient (BKG). The fundamental idea to reduce the amount of information and computation
required for a decision in a given state at a given time epoch is to assume that this decision

is the last one, and in the next period we will identify the better arm which will whence be

allocated to all the remaining subjects. Frazier et al. (2008, Section 7.2) showed that it is opti-

mal for a search variant (i.e., maximizing the ﬁnal-period expected reward) of the two-armed

bandit problem with continuous responses. A speciﬁc variant of this approach for our setting

was presented in Powell and Ryzhov (2012, Section 4.7.1) and further studied and improved

in Edwards et al. (2017).

Let us denote by µk((cid:101)sk(t), (cid:101)fk(t)), or brieﬂy (cid:101)µk, the belief, i.e., the mean of the posterior Beta
distribution of arm k calculated using posterior observations (i.e., both the observations and
prior pseudo-observations), i.e.,

(cid:101)µk :=

(cid:101)sk(t)
(cid:101)sk(t) + (cid:101)fk(t)

.

(7)

We will further denote the belief conditional on an additional (not necessarily binary) obser-
vation δ, respectively, by

(cid:101)µ+δ

k

:=

(cid:101)sk(t) + δ
(cid:101)sk(t) + (cid:101)fk(t) + 1

.

(8)

Under independent prior Beta distributions, the allocation at epoch t by this design is to

the arm k with currently the largest value of the following score


(cid:101)µk + (T − t − 1)(cid:101)µ(cid:96)

(cid:101)µk + (T − t − 1) (cid:2)(1 − (cid:101)µk)(cid:101)µ(cid:96) + (cid:101)µk (cid:101)µ+1

(cid:101)µk + (T − t − 1)(cid:101)µ+(cid:101)µk

k

k

(cid:3)

k

if (cid:101)µ(cid:96) ≥ (cid:101)µ+1
if (cid:101)µ+1
if (cid:101)µ+0

k ≥ (cid:101)µ(cid:96) ≥ (cid:101)µ+0
k ≥ (cid:101)µ(cid:96)

k

(9)

where (cid:96) refers to the other arm, and ties are broken randomly. Although this score can be
interpreted as capturing the allocation priority similarly to the Gittins and Whittle indices, it

depends on the state of the other arm, thus, strictly speaking, it is not an index.

5.3 Na¨ıve Designs

While in the previous subsection we described designs which are horizon-dependent and

forward-looking in the way of making allocation decisions, we now turn our attention to

horizon-independent designs and present a number of different na¨ıve approaches in this sub-

section. Such designs are computationally simple and easy to interpret, hence sometimes more
appealing for practical use. It turns out that such deterministic designs are either related to my-
opia (T = 1) or to utopia (T = +∞).

We will say that a design leads to complete learning, if in the problem considered over an

16

inﬁnite horizon (i.e., T = +∞) it identiﬁes the better arm almost surely. It is easy to see that to
achieve complete learning it is necessary to allocate each arm inﬁnitely often.

5.3.1 Myopic Index Rules

Bradt et al. (1956) considered the setting in which one knows the two-point set {θC, θD}, but
does not know which arm is which, and proved that if θC +θD = 1, then it is optimal, under any
prior distributions, to deterministically allocate every subject to the arm with currently largest
Bayesian expected one-period reward. The also proved the same in the case θC + θD = 1 when
the set {θC, θD} is unknown. For the case θC + θD (cid:54)= 1 with known set {θC, θD} they proved
that such a design leads to complete learning and conjectured that it is optimal in terms of the

Bayesian expected number of successes. They also discussed that such a design is not optimal

if both of these assumptions are dropped.

Note that under independent Beta prior distributions, the allocation at epoch t by this de-
sign is to the arm k with currently the largest mean calculated using posterior observations
(i.e., both the observations and prior pseudo-observations), i.e.,

(cid:101)sk(t)
(cid:101)sk(t) + (cid:101)fk(t)

(10)

where ties are broken randomly. We call this the Bayesian myopic (BM) design, because it takes
an action which is best for the next subject only and ignores the future.

Feldman (1962) also considered the setting with known two-point set {θC, θD} and further
generalized the optimality of the Bayesian myopic design. Berry (1972, Section 8) realized that
the setting with known two-point set {θC, θD} (i.e., with strong between-arm dependence) is
equivalent to the two-armed problem with independent arms with two-point prior distribu-

tions. Kelley (1974) further generalized the setting with dependent arms and identiﬁed condi-

tions for optimality of the Bayesian myopic design.

We also consider the frequentist myopic (FM) design, which allocates each arm once in the
ﬁrst two periods, and then deterministically allocates every subject to the arm with currently

the largest mean calculated using the observations only (breaking the ties randomly), i.e.,

sk(t)
sk(t) + fk(t)

(11)

Such a design was mentioned in Bather (1981, Eq. (1.1)) and called “play the favourite”. 4 Note
that BM and FM are equivalent if and only if (cid:101)sC(0) = (cid:101)fC(0) = (cid:101)sD(0) = (cid:101)fD(0) = 0.

Berry (1978, Section 3) established that, in the setting with known two-point set {θC, θD},

4In the machine learning literature since the 2000s, this design is also known as “follow the leader” (as a simpler
variant of “follow the perturbed leader”). The term has however been used also for other designs: Sobel and Weiss
(1972) are the ﬁrst to use the term “follow the leader”, but they mean a variant of “stay-with-a-winner & switch-
on-a-loser”, with a difference of randomizing (rather than switching) when a failure is observed and the number of
failures on both arms is equal. Many papers studying continuous bandit problems use the term “follow the leader”
for (Gittins) index rule.

17

in which the BM design is optimal, at any epoch t at which the posterior probability that arm C
is better than arm D is 1/2, it is optimal to allocate to the arm with currently the greatest differ-
ence between the number of successes and the number of failures. Several tie-breaking rules

can be used but they only affect the performance marginally, so these are not discussed here
in detail. We refer to this as the Bayesian greatest difference ﬁrst (BGDF) design. The Bayesian
performance of this design was illustrated numerically to be near optimal (Berry, 1978) and to
outperform the BM design (Villar, 2018). The mentioned condition applies when the one-arm
prior distributions satisfy (cid:101)sC(0) = (cid:101)fC(0) = (cid:101)sD(0) = (cid:101)fD(0), as in this paper, and note that in
that case the frequentist analogue, FGDF, yields an equivalent design.

5.3.2 Utopic Index Rules

In the frequentist setting, Robbins (1952) introduced the deterministic “stay-with-a-winner &

switch-on-a-loser” design, in which the ﬁrst allocation is made randomly, and then the same

arm is allocated whenever the observed response is a success, while the other arm is allocated

whenever the observed response is a failure. He realized that it leads to complete learning, but

fails to achieve the maximum expected proportion of successes, because it allocates the worse

arm at a regular rate.

Interestingly, many of the deterministic designs actually have the “stay-with-a-winner”
property. It is easy to see that the BM, FM, BGDF and FGDF all have it. In the Bayesian setting,
Berry (1972, Theorem 6.2) proved the “stay-with-a-winner” property for the deterministic DP
design. Formally, the “stay-with-a-winner” property in this approach is: if arm k is optimal
in period t ≤ T − 2, the subject is allocated to it and its response is a success, then arm k is
uniquely optimal in period t + 1. Contrary to the above, the “switch-on-a-loser” property is
not, in general, satisﬁed by these designs. When one arm looks signiﬁcantly better than the

other one, the designs would not switch after observing a failure on the former arm. See, e.g.,

Berry and Fristedt (1985, p. 79).

Berry (1972, Conjecture A, p. 892) conjectured that for a large number of remaining alloca-
tions (i.e. at epochs t (cid:28) T ), the only criterion for Bayesian optimality in the DP design is the
difference between the posterior number of failures on the two arms. (He did not specify what

to do if there is the same number of failures on the two arms.)

Kelly (1981) studied the Gittins index rule in the inﬁnite-horizon setting (in which it is

optimal) as the discount factor approaches one (i.e. the undiscounted setting) and established,

under a technical condition on the prior distributions, that the Gittins index rule reduces to the
following rule which we call the frequenist least failures ﬁrst (FLFF) design: at every time epoch,
allocate the subject to the arm with least observed number of failures, breaking the ties in favor

of any arm with greatest observed number of successes (breaking the double ties arbitrarily).

Although the similarity to Berry’s conjecture is remarkable, Kelly (1981, Remark 4.15)

stated that there seems to be no immediate link between these two statements, and that Berry’s

conjecture may require a technical condition on the prior distributions. Kelly (1981, Remark
4.7) further elucidated that FLFF is a slight variation of the “stay-with-a-winner & switch-on-

18

a-loser” design in Robbins (1952). Indeed, it is easy to see that FLFF has the “stay-with-a-
winner” property. After observing a failure, it switches if the other arm has lower number of

failures, but it stays or switches accordingly to the greater number of successes if the number

of failures on both arms is the same.

Analogously, we call the Bayesian least failures ﬁrst (BLFF) design the one based on posterior
numbers of successes and failures rather than on the observed ones. If the prior distributions
on the two arms are the same, then this design is equivalent to the FLFF design, but if they are
not, then the arm with lower number of posterior failures will be allocated without switching

(at least) until the number of failures on the two arms becomes the same.

We would like to highlight that the three radically different “schools” of the bandit prob-
lem, namely Robbins’, Berry’s and Gittins’, all led to the FLFF design with only slight varia-
tions. The “stay-with-a-winner & switch-on-a-loser” design may be useful in some practical

situations because it only depends on the last observation, not on the numbers of successes

and failures over the whole history.

5.4 Reinforcement Learning — UCB Index Rules

Lai and Robbins (1985) laid out the theory of asymptotically optimal allocation and were the
ﬁrst to actually use the term “upper conﬁdence bound” (UCB). They wrote it in quotes, as the
quantity it refers to depends on time period t, and is thus not the conventional upper bound
of conﬁdence intervals, but can be interpreted as the upper conﬁdence bound with signiﬁ-
cance level 1/t. Lai (1987, Eq. (2.6)) introduced a UCB index rule using the Kullback-Leiber
divergence and Agrawal (1995, Example 5.7) developed UCB index values which are inﬂations

of the observed mean proportion of successes. The theory was further extended in Burnetas

and Katehakis (1996) to multivariate and non-parametric distributions. The ﬁrst simple UCB

index rules with ﬁnite-time theoretical guaranties were developed in Auer et al. (2002). See,

e.g., Bubeck and Cesa-Bianchi (2012); Kaufmann and Garivier (2017); Lattimore and Szepesv´ari

(2019) for accounts of subsequent developments.

The use of the observed mean proportion of successes in deﬁning an index rule is attractive

mainly because it is the maximum likelihood estimator of the success probability in the static

setting of the design of experiments.

Index rules that use time-dependent inﬂations of the

observed mean proportion of successes were proposed and investigated even before Lai and

Robbins (1985), e.g., Bather (1980, 1981); Abdel Hamid (1981) in frequentist setting and Gittins

and Jones (1974); Glazebrook (1980); Gittins and Wang (1992) in Bayesian setting.

Following Bubeck and Cesa-Bianchi (2012, Section 2), we consider the popular αUCB de-
sign which allocates each arm once in the ﬁrst two periods, and then deterministically allocates

every subject to the arm with currently the largest index (breaking ties randomly) of the form

sk(t)
sk(t) + fk(t)

+

(cid:115)

α · ln(t + 1)
sk(t) + fk(t)

(12)

where α > 0. The original design introduced in Auer et al. (2002) used α = 2. Theoretical

19

upper bounds currently exist for α > 1, but researchers have noticed empirically that lower
values of α typically lead to better performance and some used α = 1, see, e.g., Cserna et al.
(2017). In our numerical experiments (not reported here) we found that approximately the best
performance is achieved with α = 0.18.

Many other types of UCB designs have been developed, see e.g., Kaufmann (2018, Figure

2) for comparison of some of them. Besides designs based on idea of UCB, there is a num-

ber of popular designs with randomized actions, e.g., epsilon-greedy, Boltzmann exploration,

Thompson sampling, etc.

5.5 Biostatistics

Blinding of patients and personnel to the allocated treatment is an important desideratum in

many types of clinical trials to mitigate a variety of biases such as performance bias, detection

bias and attrition bias (Higgins and Green, 2011, Section 8.4). If allocation is deterministic, the

patients and personnel can, if they know the state of the trial, identify the allocated treatment

with certainty or high probability. Thus, some amount of randomness in the allocation deci-

sion is desirable. Moreover, the theory of design of experiments explains that randomization

is important for mitigation of the selection bias, for ensuring similarity in the treatment groups

and for providing a basis for inference, which are essential for making valid conclusions at the

end of a trial (Rosenberger et al., 2019). Thus, in clinical trials theory and practice, a lot of at-

tention is paid to designs with randomized actions (Rosenberger and Lachin, 2015), although it

should also be noted that not all types of clinical trials require or allow to implement blinding
and randomization. There are three major approaches to adaptive randomization: (i) Bayesian
Response-Adaptive Randomization, starting with Thompson (1933), generalized in Thall and Wa-
then (2007) (see also Berry et al. (2011, p. 156)), and recently studied also in the reinforcement
learning literature (see, e.g., Agrawal and Goyal (2012)); (ii) Frequentist P´olya Urn Randomiza-
tion, which is a randomized version of the “stay-with-a-winner & switch-on-a-loser” design,
starting with Wei and Durham (1978); and (iii) Randomized Index Rules such as those proposed
by Bather (1980, 1981); Abdel Hamid (1981) in frequentist setting and by Glazebrook (1980) in

Bayesian setting.

Note that the term “bandit” usually does not appear in the relevant biostatistics literature.

5.6 Combination Designs

Many researchers have suggested to use one design for the ﬁrst N subjects and use another
one for the remaining (T − N ) subjects. This idea appeared implicitly in early papers on
sequential design of experiments, considering the 1:1 design initially (on a sample of size
N ), and then sticking to the arm with higher mean (concluded at a given signiﬁcance level)
forever, since that approach assumes T = +∞. Cheng et al. (2003) found that when T is ﬁnite,
for this combination design it is optimal for N to be of the order of
T , and, in the particular
case of prior Beta distribution with parameters (1, 1) on each arm, the optimal N →
2T
asymptotically as T → +∞ (and is slightly lower for ﬁnite horizons).

√

√

20

Hoel et al. (1972) considered a setting in which T is not ﬁxed in advance, but the allocation
is stopped if a predeﬁned difference in the number of successes is reached. They proposed to
start with the arm alternating design (i.e., a deterministic version of 1:1), and then to switch to
the “stay-with-a-winner & switch-on-a-loser” design if the estimate of max{θC, θD} is greater
than 0.6 and continue with arm alternating otherwise. This paper is just one example from the
work in the area of ranking and selection.

Zelen (1969, Section 4) proposed to initially use the “stay-with-a-winner & switch-on-a-

loser” design, and then to stick to the arm with higher mean. He found that this is better
than the classic combination if θC + θD ≥ 1 and the value of N ≈ T /3 yields a nearly-optimal
number of successes.

Kelly (1981, Remark 4.10) elucidated that the optimal Bayesian decision-theoretical design
in the discounted setting can be regarded as having three stages: ﬁrst, BLFF, which he refers
to as “information gathering”, then it moves further and further away from that design, and

ﬁnally it always allocates to the same arm. Villar (2018, Section 3.4) provided some illustrative

numerical evaluation of these stages for the Gittins and Whittle index rules.

Motivated by the above and by the discussion in subsubsection 5.3.2, in this paper we con-
sider three combination designs in which the designs used are BLFF in the ﬁrst stage, followed
by BM, αUCB and BMSF, respectively, and one combination rule in which the designs used
are 1:1 in the ﬁrst stage, followed by BMSF:

• BLFF+BM with N =

√

4T
√

• BLFF+αUCB with N =

4T

• BLFF+BMSF with N =
√

• 1:1+BMSF with N =

√

9T

2T

The lengths of the ﬁrst stage using BLFF have been obtained heuristically based only on a
small numerical study and so they are not necessarily overall optimal, and surely not optimal

for each particular scenario. To the best of our knowledge, these three designs have not been
studied previously and their theoretical analysis is an open problem. Note that using BMSF
in the second stage represents the situation in which the arm with higher mean (most suc-
cesses is approximately equivalent to highest mean since BLFF keeps the number of failures
balanced up to a difference of 1) after the ﬁrst stage is allocated to all the subjects in the second
stage since only the allocated arm can collect additional successes. This is inspired by but not
fully equivalent to the design studied by Zelen (1969, Section 4), because BLFF is not fully
equivalent to “stay-with-a-winner & switch-on-a-loser”.

Using BM in the second stage is similar to BMSF, but allows for additional learning during
the second stage: the mean of the arm with higher mean at the end of the ﬁrst stage may

eventually decrease below the mean of the other arm, at which moment the allocation switches

to that arm (there may be more than one such “correction”). The combination design that uses
αUCB in the second stage is included because it has interesting performance, to be discussed
in Section 6.

21

Design 1:1+BMSF is inspired by Cheng et al. (2003), although using BMSF in the second
stage is not fully equivalent to sticking to the arm with higher mean at the end of the ﬁrst stage,
because 1:1 may lead to unbalanced allocation due to sampling variability.

6 Designs Performance

For some of the response-adaptive designs, theoretical values of or bounds on their perfor-
mance exist. However, these are either asymptotic as T → +∞, or up to an additive and/or
multiplicative constants, which are often large or unknown. For small (T ≈ 1 : 102) and mod-
erate (T ≈ 102 : 104) horizons, computational evaluation is the most appropriate to illustrate
designs performance.

In this section we report computational experiments in which every design is evaluated by

backward recursion, i.e. at full computational accuracy (of Float64, which is of the order of
10log(T )−16). For fairness, we only include deterministic designs and 1:1 as a benchmark (i.e.
we exclude the biostatistics ones).

To the best of our knowledge, such accurate evaluation has only been reported for small

horizons in the existing literature. For moderate horizons, which are often the most relevant

in practice, for instance in clinical trials, designs are usually evaluated by simulation, which is

remarkably less accurate.

We present the performance in terms of both the proportion of successes and (equivalently)

the regret number of successes, as these provide complementary insights. See Appendix C for
small horizons, in which the performance of some of the designs is fundamentally different.

6.1 Bayesian Performance — Moderate Trial Sizes

Figure 2 illustrates the Bayesian performance of the Bayesian decision-theoretic design for
T = 120 : 120 : 4440. Although it is an interesting way of summarizing the performance,
it has three major drawbacks: (i) it is a simple average over all the possible pairs of parameters
(θC, θD), while in practice one would be interested in a particular subset of the whole parame-
ter space, possibly weighted in a particular way; (ii) it presents the Bayesian value in which the

observations happen according to the belief at every time epoch rather than being ﬁxed over

the whole horizon, which blurs its interpretation because the beliefs are biased and which is

less desirable since in practice one would be interested in the value under the true (unknown)

success probabilities; (iii) it depends on the prior distributions, so a choice needs to be made

or it needs to be computed for a number of different options.

Nevertheless, Figure 2 is instrumental in giving an idea about the order of magnitude of

the objective, and in particular it is interesting to observe that the regret number of successes is
increasing and concave, taking value of around 5 for the trial size of 3000 and, by extrapolation,
it is likely to be below one per mille of the trial sizes beyond 104.

22

Figure 2: An illustration of two (equivalent) Bayesian performance measures evaluated for the
deterministic Bayesian decision-theoretic design over a range of moderate trial sizes.

6.2 Frequentist Performance — Moderate Trial Sizes

In this subsection we evaluate and compare the frequentist performance of the above designs
for for T = 120 : 60 : 1200 ∪ 1200 : 300 : 2400. We do so in four scenarios: (θC, θD) ∈
{(0.1, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 0.9)}. These scenarios are a natural choice, and have been
previously considered, e.g., (0.3, 0.5) in Lai (1987, Table 3), Brezzi and Lai (2002, Table 2), Villar
et al. (2015, Table 5) and Villar (2018, Table 6), (0.7, 0.9) in Lai (1987, Table 3), and all of them
in Hoel et al. (1972). Although conclusions from a number scenarios do not guarantee their

validity in other scenarios, we have found these four scenarios to be illustrative enough to

provide negative conclusions for many designs.

We report both the proportion of successes, in Figure 3, and the regret number of successes,

in Figure 4. Besides the mean, for each measure we report also the standard deviation, which

can be considered as a secondary criterion providing additional insights into the performance

of the designs.

1:1 is the worst design in terms of the mean and often the best in terms of the SD but,
surprisingly, it is not always the best, meaning that in some scenarios there are designs that

are better under both measures (all of these are combination designs, to be discussed below).
BLFF notably improves the mean, increasingly so in the scenarios with higher success proba-
bilities, while only marginally deteriorates the SD. They both over-explore.

The curves of BMSF, FM, BM, BGDF and BKG look approximately linear in both the
mean and SD of the regret number of successes (approximately constant in both the mean
and SD of the proportion of successes). The performance of BGDF and BKG is quite bad
in general and heavily scenario dependent. BGDF’s mean deteriorates with more extreme
success probabilities, but its SD improves with lower success probabilities. BKG’s mean and
SD both improve with lower success probabilities, remarkably so in scenario (0.1, 0.3), in which
the mean regret is the best of all designs and seems to be constant, while all the other designs’

mean regret is increasing.

There are three designs that perform particularly badly in all four scenarios: BMSF, FM
and BM are dominated by almost all the other designs. High SD indicates that these three de-
signs are extremely under-exploring: too aggressively sticking to one of the arms, and relatively

23

Trial size120240360480600720840960108012001320144015601680180019202040216022802400252026402760288030003120324033603480360037203840396040804200432044400.6500.6520.6540.6560.6580.6600.6620.6640.6662/3Bayesian expected proportion of successesTrial size12024036048060072084096010801200132014401560168018001920204021602280240025202640276028803000312032403360348036003720384039604080420043204440123456Bayesian regret number of successesoften choosing the worse one. Among these three, BMSF is worse than the other two in both
the mean and SD in all four scenarios. The performance order of FM and BM depends on
scenario; BM’s mean remarkably improves with lower success probabilities, while FM’s mean
improves with more extreme success probabilities. For these three designs it also holds that

the better the mean, the better the SD, so in every scenario the order is identical under both

measures.

The curves for the mean of all the remaining designs (DP, αUCB, and the combination
designs) look approximately concave, leading to signiﬁcant improvement in the mean. Quan-
titatively, all three versions of αUCB are almost identical across the four scenarios, indicating
that its performance depends on the difference θC − θD rather than on their respective values.
Interestingly, the mean regret number of successes of 2UCB is more than 50% higher than that
of 1UCB, which is in turn around 3 times higher than that of 0.18UCB, which is in turn still
distinctively higher (between 20% − 100%, depending on scenario) than that of DP which is
the best performing design except when dominated by BKG in scenario (0.1, 0.3). The mean
regret of DP in scenario (0.7, 0.9) seems to be constant, while all the other designs’ mean regret
is increasing. The SD of 2UCB and 1UCB is practically undistinguishable, while the SD of
0.18UCB is notably higher in the scenarios with higher success probabilities, with the SD of
DP being in between.

The combination designs bring some surprising results. First, BLFF+2UCB is practically
identical to 2UCB and BLFF+1UCB to 1UCB in all the scenarios and trial sizes, indicating
that αUCB initially behaves essentially as BLFF. That is however not true for BLFF+0.18UCB,
which is similar but not identical to 0.18UCB: in terms of the mean being better in scenarios
with higher success probabilities while being worse in those with lower success probabilities.
The SD of BLFF+0.18UCB is always lower than that of 0.18UCB, and in particular it is the
lowest of all designs except when dominated by 1:1 and BLFF in scenarios with low success
probabilities.

Somewhat surprisingly, BLFF+BM performs quite well, roughly similarly to BLFF+0.18UCB

in terms of the mean, although its SD is sometimes higher. BLFF+BMSF performs worse than
BLFF+BM but still signiﬁcantly better than 1UCB in terms of the mean, but its SD is notably
higher. 1:1+BMSF is in most instances signiﬁcantly worse than BLFF+BMSF both in terms
of the mean and SD.

We summarize the above discussion as follows:

• DP may be the preferred design in terms of the mean in scenarios with high and moder-

ate success probabilities;

• BKG may be the preferred design in terms of the mean in scenarios with low success

probabilities;

• BLFF+0.18UCB may be the overall preferred design in terms of both the mean (higher

weight) and the SD (lower weight);

24

• BLFF may be the overall preferred design in terms of both the mean (lower weight) and

the SD (higher weight);

• 1:1 may be the preferred design in terms of the SD in scenarios with low and moderate

success probabilities;

• BLFF+0.18UCB may be the preferred design in terms of the SD in scenarios with high

success probabilities;

7 Myths

The hardness of the problem and the diversity of approaches mean that it is difﬁcult for a single

person to have complete information and full understanding of the many details. This paper

is, to the best of our knowledge, ﬁrst attempt to survey key ideas from multiple disciplines.

Author’s engagement with researchers and practitioners from a number of ﬁelds has given

him an impression of common beliefs that may not be fully correct, listed next.

7.1 Myth #1: The Bayesian Decision-Theoretic Design is Intractable

This myth seems to be widespread across disciplines, e.g., “The curse of dimensionality ren-

ders exact approaches impractical.” (Ahuja and Birge, 2019); “Even in relatively benign setups

the computation of the Bayesian optimal policy appears hopelessly intractable.” (Lattimore

and Szepesv´ari, 2019, Section 35.1); “Trials of a size up to 3500 patients would be feasible with

todays number #1 supercomputer (with 1.3 PB of RAM).” (Williamson et al., 2017).

Table 3 illustrates the evolution of what was reported as computationally tractable in the

literature. Indeed, besides the two results in the 1960s that pre-date the era of personal com-
puters, the existing literature gives an impression that solving the problem beyond T = 100 is
impractical or unfeasible. A closer look however reveals that no improvement seem to have

been achieved since Berry (1978) which appeared more than 40 years ago, despite the theoret-

ical progress in computer science and technological progress in personal computers. In this

paper we use a state-of-the-art package in Julia programming language written by the author,
and show that on a standard laptop or desktop computer, the DP design can be computed
for ofﬂine use or evaluated in online fashion in a few minutes (T ≈ 1, 000), in a few hours
(T ≈ 2, 000), or in a few days (T ≈ 4, 000). 32GB of RAM allows storing (e.g. for ofﬂine use) of
the whole design up to trial size around 1440; when its storing is not needed (e.g. for Bayesian
evaluation or for calculation of the initial action) it allows up to trial size around 4440. 5

5The ﬁrst version of the package is planned to be released to public in mid-2019. The package will be described
in a separate paper, whose abstract has been accepted by the editors of the special issue on Bayesian Statistics of
the Journal of Statistical Software, and will be submitted by its deadline 30 June 2019.

25

Figure 3: An illustration of performance (mean on the left, standard deviation on the right) in
terms of the expected proportion of successes evaluated for deterministic designs over a range
of moderate trial sizes, for (θC, θD) = (0.7, 0.9) in the ﬁrst row, (0.5, 0.7) in the second row,
(0.3, 0.5) in the third row, (0.1, 0.3) in the fourth row.

26

Trial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.800.820.840.860.880.90Mean proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.12SD of proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.600.620.640.660.680.70Mean proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.12SD of proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.400.420.440.460.480.50Mean proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.12SD of proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.200.220.240.260.280.30Mean proportion of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.12SD of proportion of successesFigure 4: An illustration of performance (mean on the left, standard deviation on the right)
in terms of the regret number of successes evaluated for deterministic designs over a range
of moderate trial sizes, for (θC, θD) = (0.7, 0.9) in the ﬁrst row, (0.5, 0.7) in the second row,
(0.3, 0.5) in the third row, (0.1, 0.3) in the fourth row.

27

Trial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550Mean regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550SD of regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550Mean regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550SD of regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550Mean regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550SD of regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550Mean regret number of successesTrial size0300600900120015001800210024001:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns05101520253035404550SD of regret number of successesPublication

T

T max

SW, HW, RAM

Steck (1964)
Yakowitz (1969)
Berry (1978)
Ginebra and Clayton (1999)
Hardwick et al. (2006)
Ahuja and Birge (2016)
Williamson et al. (2017)
Villar (2018)
Kaufmann (2018)
This paper

N/A N/A, UNIVAC 1105, 54 kB
N/A Fortran, N/A, N/A

25
5
100 N/A Basic (?), Atari (?), N/A
150
100
96
100
100 N/A Matlab, PC, N/A
70
N/A N/A, N/A, N/A
4440
4440

N/A, N/A, N/A
N/A, N/A, N/A
N/A, Mac 4GB
R, PC, 16GB

Julia 1.0.1 & BB, PC, 32GB

180
200
240
215

Table 3: Horizons T with reported results and T max reported as the largest computationally
tractable on a standard computer by backward recursion in the literature on two-armed prob-
lem with binary responses.

7.2 Myth #2: The Bayesian Decision-Theoretic Design is Optimal

The DP is Bayes-optimal, but that does not necessarily mean that it is optimal in the non-
Bayesian objective, i.e. that it achieves the largest possible mean number of successes. Figure 4

and Figure 6 clearly show that other designs perform better in some scenarios.

7.3 Myth #3: The Gittins Index Rule Leads to Incomplete Learning

According to Brezzi and Lai (2000): “...we give in Section 3 a simple proof of the incomplete-

ness of optimal learning from endogenous data in the discounted multi-armed bandit prob-

lem... This generalizes Rothschild (1974)’s result for Bernoulli two-armed bandits, and also the

result of Banks and Sundaram (1992) who show that there is positive probability of incomplete

learning in multi-armed bandits with general distributions of rewards if the priors have ﬁnite

support.” This is however true only in the discounted setting; Kelly (1981) proved that the

structure of the Gittins index rule in the undiscounted setting (see subsubsection 5.3.2) leads

to complete learning.

7.4 Myth #4: The Gittins Index is Computationally Simpler than Dynamic Pro-

gramming for the Two-Armed Problem

As we explain in subsubsection 5.2.2, there is a trade-off between accuracy and computational

complexity of the Gittins index. For computing the Gittins index, all the algorithms use a

truncation of the horizon, and due to numerical instability most of the numerical algorithms

use a discount factor strictly lower than one although the appropriate one would be equal to

one. So, there are three levels of approximation that the Gittins index requires, and such an

approximate Gittins index rule may in the end not even perform better than the Myopic index

rule (see e.g. Ahuja and Birge (2016)).

28

7.5 Myth #5: The UCB Index Rule is Near-Optimal for the Finite-Horizon Problem

Section 6 illustrates that the 2UCB and 1UCB designs are signiﬁcantly suboptimal, yielding
ﬁve to tenfold mean regret number of successes than the optimal design. Tuning the coefﬁ-

cient even beyond the intervals required by theoretical analysis can signiﬁcantly improve the
performance, e.g. 0.18UCB yields around twofold mean regret number of successes than the
optimal design, which is still too large to be considered near-optimal.

7.6 Myth #6: The Frequentist Mean Regret Number of Successes is Increasing

Lai and Robbins (1985) presented a lower bound, valid under certain technical assumptions,

indicating that the frequentist mean regret number of successes increases proportionally to
log T as T → ∞. Our computational results indicate that this is not necessarily true for all
designs over ﬁnite horizons. For instance, the mean regret of DP in scenario θC = 0.7, θD = 0.9
(see Table 5) at T = 240 is lower than that at T = 300, and it is non-increasing over T = 660 :
60 : 1200.

8 Conclusion

The aim of this paper to provide a uniﬁed account of approaches from various disciplines

to the two-armed bandit problem. We have proposed problem terminology that we believe

should not create conﬂicts with other existing terminology in most of the disciplines, to facil-

itate mutual learning. We have proposed to use backward recursion instead of simulation for

more accurate evaluation. We have created a computational comparison of designs from dif-

ferent disciplines in a standardized set of scenarios. The computational results have showed
that some of the simple ones (e.g. BLFF+BM and BLFF+BMSF) perform surprisingly well in
our scenarios and outperform may of the more sophisticated and more studied ones. We have
also showed that DP is tractable for much larger horizons than it is commonly believed. This
suggests that there is a case for these to be used among benchmark designs when developing

new designs.

We have given an account of approaches to the problem with the objective of maximizing

the mean number of successes, which is linear across arms and over time. Most of the above

designs, especially those horizon-ignorant, crucially depend on that property, and it is not clear

how they could be modiﬁed if the objective changed to another one. The only exception is the
DP and its variants, which are quite ﬂexible to accommodate other ﬁnite-horizon objectives.

A signiﬁcant area of research left out of this paper deals with practicalities of implemen-

tation of the designs, especially in the context of clinical trials, in which (i) the objective is

different, because it focusses much more on estimation of the success probabilities, and (ii)

there are additional constraints, e.g. requirement of a certain degree of randomization. A fair

comparison of such designs would actually require a series of comparisons ﬁxing the random-

ization degree and including only those designs that satisfy it. Such work is extensive and is

29

left for a separate paper.

References

Abdel Hamid, A. R. (1981). Randomized Sequential Decision Rules. PhD thesis, D. Phil. Thesis,

University of Sussex.

Aggarwal, C. C. (2016). Recommender Systems. Springer.

Agrawal, R. (1995). Sample mean based index policies with O(log n) regret for the multi-armed

bandit problem. Advances in Applied Probability, 27(4):1054–1078.

Agrawal, S. and Goyal, N. (2012). Analysis of Thompson sampling for the multi-armed bandit

problem. In Conference on Learning Theory, pages 39–1.

Ahuja, V. and Birge, J. R. (2016). Response-adaptive designs for clinical trials: Simultaneous

learning from multiple patients. European Journal of Operational Research, 248:619–633.

Ahuja, V. and Birge, J. R. (2019). An approximation approach for response adaptive clinical

trial design. SMU Cox School of Business Research Paper No. 18-26.

Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit

problem. Machine Learning, 47(2-3):235–256.

Banks, J. S. and Sundaram, R. K. (1992). Denumerable-armed bandits. Econometrica: Journal of

the Econometric Society, pages 1071–1096.

Bather, J. (1980). Randomised allocation of treatments in sequential trials. Advances in Applied

Probability, 12(1):174–182.

Bather, J. A. (1981). Randomized allocation of treatments in sequential experiments. Journal of

the Royal Statistical Society: Series B (Methodological), 43(3):265–283.

Bellman, R. (1956). A problem in the sequential design of experiments. Sankhy¯a: The Indian

Journal of Statistics, 16(3/4):221–229.

Berry, D. A. (1972). A Bernoulli two-armed bandit. Annals of Mathematical Statistics, 43(3):871–

897.

Berry, D. A. (1978). Modiﬁed two-armed bandit strategies for certain clinical trials. Journal of

the American Statistical Association, 73:339–345.

Berry, D. A. and Esserman, L. (2016). Adaptive randomization of neratinib in early breast
cancer: Drs. Berry and Esserman reply. New England Journal of Medicine, 375(16):1592–1593.

Berry, D. A. and Fristedt, B. (1985). Bandit Problems: Sequential Allocation of Experiments.

Springer Netherlands.

30

Berry, S. M., Carlin, B. P., Lee, J. J., and Muller, P. (2011). Bayesian Adaptive Methods for Clinical

Trials. CRC press.

Bothwell, L. E., Avorn, J., Khan, N. F., and Kesselheim, A. S. (2018). Adaptive design clinical

trials: a review of the literature and ClinicalTrials.gov. BMJ open, 8(2):e018320.

Bradt, R. N., Johnson, S. M., and Karlin, S. (1956). On sequential designs for maximizing the

sum of n observations. Annals of Mathematical Statistics, 27(4):1060–1074.

Brezzi, M. and Lai, T. L. (2000). Incomplete learning from endogenous data in dynamic alloca-

tion. Econometrica, 68(6):1511–1516.

Brezzi, M. and Lai, T. L. (2002). Optimal learning and experimentation in bandit problems.

Journal of Economic Dynamics and Control, 27(1):87–108.

Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-

armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning, 5(1):1–122.

Burnetas, A. N. and Katehakis, M. N. (1996). Optimal adaptive policies for sequential alloca-

tion problems. Advances in Applied Mathematics, 17:122–142.

Cheng, Y. and Berry, D. A. (2007). Optimal adaptive randomized designs for clinical trials.

Biometrika, 94(3):673–689.

Cheng, Y., Su, F., and Berry, D. A. (2003). Choosing sample size for a clinical trial using decision

analysis. Biometrika, 90(4):923–936.

Cserna, B., Petrik, M., Russel, R. H., and Ruml, W. (2017). Value directed exploration in multi-
armed bandits with structured priors. In Proceedings of the 33rd Conference on Uncertainty in
Artiﬁcial Intelligence.

den Boer, A. V. (2015). Dynamic pricing and learning: Historical origins, current research, and

new directions. Surveys in Operations Research and Management Science, 20(1):1–18.

Edwards, J., Fearnhead, P., and Glazebrook, K. (2017). On the identiﬁcation and mitigation
of weaknesses in the knowledge gradient policy for multi-armed bandits. Probability in the
Engineering and Informational Sciences, 31(2):239–263.

Feldman, D. (1962). Contributions to the ”two-armed bandit” problem. The Annals of Mathe-

matical Statistics, 33(3):847–856.

Frazier, P. I., Powell, W. B., and Dayanik, S. (2008). A knowledge-gradient policy for sequential

information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439.

Ginebra, J. and Clayton, M. K. (1999). Small-sample performance of Bernoulli two-armed

bandit Bayesian strategies. Journal of Statistical Planning and Inference, 79(1):107–122.

31

Gittins, J. and Wang, Y.-G. (1992). The learning component of dynamic allocation indices. The

Annals of Statistics, 20(3):1625–1636.

Gittins, J. C. (1979). Bandit processes and dynamic allocation indices.

Journal of the Royal

Statistical Society, Series B, 41(2):148–177.

Gittins, J. C. (1989). Multi-Armed Bandit Allocation Indices. J. Wiley & Sons, New York.

Gittins, J. C., Glazebrook, K., and Weber, R. (2011). Multi-Armed Bandit Allocation Indices. Wiley-

Blackwell.

Gittins, J. C. and Jones, D. M. (1974). A dynamic allocation index for the sequential design of
experiments. In Gani, J., editor, Progress in Statistics, pages 241–266. North-Holland, Amster-
dam.

Glazebrook, K. D. (1980). On randomized dynamic allocation indices for the sequential design
of experiments. Journal of the Royal Statistical Society. Series B (Methodological), pages 342–346.

Gluss, B. (1962). A note on a computational approximation to the two-machine problem. In-

formation and Control, 5(3):268–275.

Hardwick, J., Oehmke, R., and Stout, Q. F. (2006). New adaptive designs for delayed response

models. Journal of Statistical Planning and Inference, 136:1940–1955.

Higgins,

J. P. T. and Green, S. (2011).

Cochrane Handbook for Systematic Reviews of

Interventions.
www.handbook.cochrane.org.

The Cochrane Collaboration, version 5.1.0 edition.

Available from

Hoel, D. G., Sobel, M., and Weiss, G. H. (1972). A two-stage procedure for choosing the better

of two binomial populations. Biometrika, 59(2):317–322.

Katehakis, M. N. and Robbins, H. (1995). Sequential choice from several populations. Proceed-

ings of the National Academy of Sciences of the United States of America, 92(19):8584.

Kaufmann, E. (2018). On Bayesian index policies for sequential resource allocation. The Annals

of Statistics, 46(2):842–865.

Kaufmann, E. and Garivier, A. (2017). Learning the distribution with largest mean: Two bandit

frameworks. ESAIM: Proceedings and Surveys, 60:114–131.

Kelley, T. A. (1974). A note on the Bernoulli two-armed bandit problem. The Annals of Statistics,

2(5):1056–1062.

Kelly, F. (1981). Multi-armed bandits with discount factor near one: the Bernoulli case. Annals

of Statistics, 9(5):987–1001.

Lai, T. L. (1987). Adaptive treatment allocation and the multi-armed bandit problem. The

Annals of Statistics, 15(3):1091–1114.

32

Lai, T. L. and Robbins, H. (1985). Asymptotically efﬁcient adaptive allocation rules. Advances

in Applied Mathematics, 6(1):4–22.

Lattimore, T. and Szepesv´ari, C. (2019). Bandit Algorithms. Cambridge University Press.

Liberali, G. B., Hauser, J. R., and Urban, G. L. (2017). Morphing theory and applications. In

Handbook of Marketing Decision Models, pages 531–562. Springer.

Makowski, A. M. and Shwartz, A. (2002). The Poisson equation for countable Markov chains:
Probabilistic methods and interpretations. In Handbook of Markov Decision Processes, pages
269–303. Springer.

Ni ˜no-Mora, J. (2011). Computing a classic index for ﬁnite-horizon bandits. INFORMS Journal

on Computing, 23(2):254–267.

Powell, W. B. and Ryzhov, I. O. (2012). Optimal Learning, volume 841. John Wiley & Sons.

Powell, W. B. and Ryzhov, I. O. (2018). Optimal Learning. John Wiley & Sons, 2nd (draft) edition.

Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the Amer-

ican Mathematical Society, 55:527–535.

Rosenberger, W. F. and Lachin, J. M. (2015). Randomization in Clinical Trials: Theory and Practice.

John Wiley & Sons.

Rosenberger, W. F., Uschner, D., and Wang, Y. (2019). Randomization: The forgotten compo-

nent of the randomized clinical trial. Statistics in Medicine, 38(1):1–12.

Rothschild, M. (1974). A two-armed bandit theory of market pricing. Journal of Economic Theory,

9(2):185–202.

Scott, S. L. (2010). A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models

in Business and Industry, 26(6):639–658.

Sobel, M. and Weiss, G. H. (1972). Recent results on using the play the winner sampling rule
with binomial selection problems. In Proceedings of the Sixth Berkeley Symposium on Mathe-
matical Statistics and Probability, Volume 1: Theory of Statistics. The Regents of the University
of California.

Steck, R. (1964). A dynamic programming strategy for the two machine problem. Mathematics

of Computation, 18(86):285–291.

Thall, P. F. and Wathen, J. K. (2007). Practical Bayesian adaptive randomisation in clinical trials.

European Journal of Cancer, 43(5):859–866.

Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in

view of the evidence of two samples. Biometrika, 25(3/4):285–294.

33

Thompson, W. R. (1935). On the theory of apportionment. American Journal of Mathematics,

57(2):450–456.

Villar, S. S. (2018). Bandit strategies evaluated in the context of clinical trials in rare life-
threatening diseases. Probability in the Engineering and Informational Sciences, 32:229–245.

Villar, S. S., Bowden, J., and Wason, J. (2015). Multi-armed bandit models for the optimal

design of clinical trials: Beneﬁts and challenges. Statistical Science, 30(2):199–215.

Weber, R. and Weiss, G. (1990). On an index policy for restless bandits.

Journal of Applied

Probability, 27(3):637–648.

Wei, L. J. and Durham, S. (1978). The randomized play-the-winner rule in medical trials. Journal

of the American Statistical Association, 73(364):840–843.

Whittle, P. (1979). Discussion on: ”Bandit processes and dynamic allocation indices”. Journal

of the Royal Statistical Society, Series B, 41(2):165–165.

Whittle, P. (1988). Restless bandits: Activity allocation in a changing world. A Celebration of

Applied Probability, J. Gani (Ed.), Journal of Applied Probability, 25A:287–298.

Whittle, P. (1989). Foreword, in Multi-armed Bandit Allocation Indices, page ix. J. Wiley & Sons.

Whittle, P. (2002). Applied probability in Great Britain. Operations Research, 50(1):227–239.

Williamson, S. F., Jacko, P., Villar, S. S., and Jaki, T. (2017). A Bayesian adaptive design for

clinical trials in rare diseases. Computational Statistics and Data Analysis, 113C:136–153.

Yakowitz, S. J. (1969). Mathematics of Adaptive Control Processes. New York, NY: North-Holland.

Zelen, M. (1969). Play the winner rule and the controlled clinical trial. Journal of the American

Statistical Association, 64(325):131–146.

A Choosing the Prior Beta Distribution Parameters

While in the paper we have used the conventional Bayes parameters

= (1, 1) for
the prior Beta distribution of each arm, the trials in practice are often designed based on pilot

(cid:16)

(cid:17)
(cid:101)s(0), (cid:101)f (0)

trials, expert opinion, or real-world evidence, which can be summarized by the mean and the

variance of the estimate of the success probability.

For Beta distribution with parameters (cid:101)s > 0, (cid:101)f > 0, the mean and the variance are

µ((cid:101)s, (cid:101)f ) = (cid:101)s
,
(cid:101)s + (cid:101)f

σ2((cid:101)s, (cid:101)f ) =

µ((cid:101)s, (cid:101)f )(1 − µ((cid:101)s, (cid:101)f ))
(cid:101)s + (cid:101)f + 1

.

(13)

34

From these expressions we can obtain that for given mean µ and variance σ2, the parameters
of the Beta distribution are

(cid:101)s(µ, σ2) = µ

(cid:20) µ(1 − µ)
σ2

(cid:21)

− 1

,

(cid:101)f (µ, σ2) = (1 − µ)

(cid:20) µ(1 − µ)
σ2

(cid:21)

− 1

.

(14)

If parameters are (cid:101)s = 0, (cid:101)f > 0, the distribution is degenerate and the mean and the variance

are

µ((cid:101)s, (cid:101)f ) = 0,

σ2((cid:101)s, (cid:101)f ) = 0.

(15)

These are the only Beta distributions with mean equal to 0. Such mean thus requires variance
equal to 0, and parameter (cid:101)s = 0 while (cid:101)f cannot be uniquely determined.

If parameters are (cid:101)s > 0, (cid:101)f = 0, the distribution is degenerate and the mean and the variance

are

µ((cid:101)s, (cid:101)f ) = 1,

σ2((cid:101)s, (cid:101)f ) = 0.

(16)

These are the only Beta distributions with mean equal to 1. Such mean thus requires variance
equal to 0, and parameter (cid:101)f = 0 while (cid:101)s cannot be uniquely determined.

If parameters are (cid:101)s = 0, (cid:101)f = 0, the distribution is a symmetric two-point Bernoulli distribu-

tion with support {0, 1}, and the mean and the variance are

µ((cid:101)s, (cid:101)f ) =

1
2

,

σ2((cid:101)s, (cid:101)f ) =

1
4

.

(17)

This is the only Beta distribution with the largest possible variance, 1/4. Such variance thus
requires mean equal to 1/2, and parameters are (cid:101)f = 0, (cid:101)s = 0.

Table 4 illustrates values of the parameters of the Beta distribution that correspond to par-
ticular pairs of mean and variance. Three well-known priors are highlighted in bold: the Bayes
= (1/2, 1/2) with variance
prior

= (1, 1) with variance 1/12, the Jeffreys prior

(cid:16)

(cid:16)

(cid:17)
(cid:101)s, (cid:101)f

(cid:17)
(cid:101)s, (cid:101)f

= (0, 0) with variance 1/4. All three are considered non-
1/8, and the Haldane prior
informative, although it may be argued that they provide different amount of information, e.g.

(cid:16)

(cid:17)
(cid:101)s, (cid:101)f

the Bayes prior provides information that it is possible to observe each response, while the

Haldane prior includes the setting that only one of the responses is possible.

B Equivalence of Reward Deﬁnitions

Throughout this section, we assume that information state i is constant and drop the explicit
dependence on it. A particular design prescribes ax ∈ A := {1, 2, 3} for every state x =
(sC, fC, sD, fD). We ﬁrst consider that the design is ﬁxed.

Using the classic reward deﬁnition, the Poisson equation (cf. Makowski and Shwartz, 2002)

35

0
5
0

.

5
4
0

.

0
4
0

.

5
3
0

.

0
3
.
0

5
2
.
0

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

)
0
0
7
1

.

,

.

0
0
7
1
(

)
0
5
6
1

.

,

.

0
5
6
1
(

)
0
0
6
1

.

,

.

0
0
6
1
(

)
0
5
5
1

.

,

.

0
5
5
1
(

)
0
0
5
1

.

,

.

0
0
5
1
(

)
0
5
4
1

.

,

.

0
5
4
1
(

)
0
0
4
1

.

,

.

0
0
4
1
(

)
0
5
3
1

.

,

.

0
5
3
1
(

)
0
0
3
1

.

,

.

0
0
3
1
(

)
0
5
2
1

.

,

.

0
5
2
1
(

)
0
0
2
1

.

,

.

0
0
2
1
(

)
0
5
1
1

.

,

.

0
5
1
1
(

)
0
0
1
1

.

,

.

0
0
1
1
(

)
0
5
0
1

.

,

.

0
5
0
1
(

)
0
0
0
1

.

,

.

0
0
0
1
(

)
0
5
9

.

,

0
5
9
(

.

)
0
0
9

.

,

0
0
9
(

.

)
0
5
8

.

,

0
5
8
(

.

)
0
0
8

.

,

0
0
8
(

.

)
0
5
7

.

,

0
5
7
(

.

)
0
0
7

.

,

0
0
7
(

.

)
0
5
6

.

,

0
5
6
(

.

)
0
0
6

.

,

0
0
6
(

.

)
0
5
5

.

,

0
5
5
(

.

)
0
0
5

.

,

0
0
5
(

.

)
0
5
4

.

,

0
5
4
(

.

)
0
0
4

.

,

0
0
4
(

.

)
0
5
3

.

,

0
5
3
(

.

)
0
0
3

.

,

0
0
3
(

.

)
0
5
2

.

,

0
5
2
(

.

)
0
0
2

.

,

0
0
2
(

.

)
0
5
1

.

,

0
5
1
(

.

)
0
0
1

.

,

0
0

.

1
(

)
0
5
0

.

,

0
5

.

0
(

)
0
0
0

.

,

0
0

.

0
(

)
1
5
8
1

.

,

4
1

.

5
1
(

)
6
9
7
1

.

,

0
7

.

4
1
(

)
2
4
7
1

.

,

5
2

.

4
1
(

)
7
8
6
1

.

,

1
8

.

3
1
(

)
3
3
6
1

.

,

6
3

.

3
1
(

)
9
7
5
1

.

,

2
9

.

2
1
(

)
4
2
5
1

.

,

7
4

.

2
1
(

)
0
7
4
1

.

,

2
0

.

2
1
(

)
5
1

.

4
1

,

.

8
5
1
1
(

)
1
6

.

3
1

,

.

3
1
1
1
(

)
6
0

.

3
1

,

.

9
6
0
1
(

)
2
5

.

2
1

,

.

4
2
0
1
(

)
7
9
1
1

.

,

0
8

.

9
(

)
3
4
1
1

.

,

5
3

.

9
(

)
8
8
0
1

.

,

1
9

.

8
(

)
4
3

.

0
1

,

6
4
8
(

.

)
0
8
9

.

,

1
0

.

8
(

)
5
2

.

9

,

7
5
7
(

.

)
1
7

.

8

,

2
1
7
(

.

)
6
1

.

8

,

8
6
6
(

.

)
2
6

.

7

,

3
2
6
(

.

)
7
0

.

7

,

9
7
5
(

.

)
3
5

.

6

,

4
3
5
(

.

)
8
9

.

5

,

0
9
4
(

.

)
4
4

.

5

,

5
4
4
(

.

)
0
9

.

4

,

0
0
4
(

.

)
5
3

.

4

,

6
5
3
(

.

)
1
8

.

3

,

1
1
3
(

.

)
6
2

.

3

,

7
6
2
(

.

)
2
7

.

2

,

2
2
2
(

.

)
7
1
2

.

,

8
7

.

1
(

)
3
6
1

.

,

3
3

.

1
(

)
8
0
1

.

,

9
8

.

0
(

)
4
5
0

.

,

4
4

.

0
(

A
/
N

)
6
5

.

9
1

,

.

4
0
3
1
(

)
8
9

.

8
1

,

.

6
6
2
1
(

)
1
4

.

8
1

,

.

7
2
2
1
(

)
3
8

.

7
1

,

.

9
8
1
1
(

)
6
2

.

7
1

,

.

0
5
1
1
(

)
8
6

.

6
1

,

.

2
1
1
1
(

)
0
1

.

6
1

,

.

4
7
0
1
(

)
3
5

.

5
1

,

.

5
3
0
1
(

)
5
9

.

4
1

,

7
9
9
(

.

)
8
3

.

4
1

,

8
5
9
(

.

)
0
8

.

3
1

,

0
2
9
(

.

)
2
2

.

3
1

,

2
8
8
(

.

)
5
6

.

2
1

,

3
4
8
(

.

)
7
0

.

2
1

,

5
0
8
(

.

)
0
5

.

1
1

,

6
6
7
(

.

)
2
9
0
1

.

,

8
2

.

7
(

)
4
3
0
1

.

,

0
9

.

6
(

)
7
7
9

.

,

1
5

.

6
(

)
9
1
9

.

,

3
1

.

6
(

)
2
6
8

.

,

4
7

.

5
(

)
4
0
8

.

,

6
3

.

5
(

)
6
4
7

.

,

8
9

.

4
(

)
9
8
6

.

,

9
5

.

4
(

)
1
3
6

.

,

1
2

.

4
(

)
4
7
5

.

,

2
8

.

3
(

)
6
1
5

.

,

4
4

.

3
(

)
8
5
4

.

,

6
0

.

3
(

)
1
0
4

.

,

7
6

.

2
(

)
3
4
3

.

,

9
2

.

2
(

)
6
8
2

.

,

0
9

.

1
(

)
8
2
2

.

,

2
5

.

1
(

)
0
7
1

.

,

4
1

.

1
(

)
3
1
1

.

,

5
7

.

0
(

)
5
5
0

.

,

7
3

.

0
(

A
/
N

)
5
0
0
2

.

,

0
8

.

0
1
(

)
6
4
9
1

.

,

8
4

.

0
1
(

)
7
8
8
1

.

,

6
1

.

0
1
(

)
8
2

.

8
1

,

4
8
9
(

.

)
9
6

.

7
1

,

2
5
9
(

.

)
0
1

.

7
1

,

0
2
9
(

.

)
0
5

.

6
1

,

9
8
8
(

.

)
1
9

.

5
1

,

7
5
8
(

.

)
2
3

.

5
1

,

5
2
8
(

.

)
3
7

.

4
1

,

3
9
7
(

.

)
4
1

.

4
1

,

1
6
7
(

.

)
5
5

.

3
1

,

9
2
7
(

.

)
5
9

.

2
1

,

8
9
6
(

.

)
6
3

.

2
1

,

6
6
6
(

.

)
7
7

.

1
1

,

4
3
6
(

.

)
8
1
1
1

.

,

2
0

.

6
(

)
9
5
0
1

.

,

0
7

.

5
(

)
0
0

.

0
1

,

8
3
5
(

.

)
1
4
9

.

,

6
0

.

5
(

)
1
8
8

.

,

5
7

.

4
(

)
2
2
8

.

,

3
4

.

4
(

)
3
6
7

.

,

1
1

.

4
(

)
4
0
7

.

,

9
7

.

3
(

)
5
4
6

.

,

7
4

.

3
(

)
6
8
5

.

,

5
1

.

3
(

)
6
2
5

.

,

3
8

.

2
(

)
7
6
4

.

,

2
5

.

2
(

)
8
0
4

.

,

0
2

.

2
(

)
9
4
3

.

,

8
8

.

1
(

)
0
9
2

.

,

6
5

.

1
(

)
1
3

.

2

,

4
2
1
(

.

)
2
7

.

1

,

2
9
0
(

.

)
2
1

.

1

,

1
6
0
(

.

)
3
5

.

0

,

9
2
0
(

.

A
/
N

)
8
8
.
9
1

,
2
5
.
8
(

)
9
2
.
9
1

,
7
2
.
8
(

)
0
7
.
8
1

,
2
0
.
8
(

)
2
1
.
8
1

,
6
7
.
7
(

)
3
5
.
7
1

,
1
5
.
7
(

)
4
9
.
6
1

,
6
2
.
7
(

)
5
3
.
6
1

,
1
0
.
7
(

)
6
7
.
5
1

,
6
7
.
6
(

)
8
1
.
5
1

,
0
5
.
6
(

)
9
5
.
4
1

,
5
2
.
6
(

)
0
0
.
4
1

,
0
0
.
6
(

)
1
4
.
3
1

,
5
7
.
5
(

)
2
8
.
2
1

,
0
5
.
5
(

)
4
2
.
2
1

,
4
2
.
5
(

)
5
6
.
1
1

,
9
9
.
4
(

)
6
0
.
1
1

,
4
7
.
4
(

)
7
4
.
0
1

,
9
4
.
4
(

)
8
8
.
9

,
4
2
.
4
(

)
0
3
.
9

,
8
9
.
3
(

)
1
7
.
8

,
3
7
.
3
(

)
2
1
.
8

,
8
4
.
3
(

)
3
5
.
7

,
3
2
.
3
(

)
4
9
.
6

,
8
9
.
2
(

)
6
3
.
6

,
2
7
.
2
(

)
7
7
.
5

,
7
4
.
2
(

)
8
1
.
5

,
2
2
.
2
(

)
9
5
.
4

,
7
9
.
1
(

)
0
0
.
4

,
2
7
.
1
(

)
2
4
.
3

,
6
4
.
1
(

)
3
8
.
2

,
1
2
.
1
(

)
4
2
.
2

,
6
9
.
0
(

)
5
6
.
1

,
1
7
.
0
(

)
6
0
.
1

,
6
4
.
0
(

)
8
4
.
0

,
0
2
.
0
(

A
/
N

)
4
9
.
8
1

,
1
3
.
6
(

)
8
3
.
8
1

,
2
1
.
6
(

)
1
8
.
7
1

,
4
9
.
5
(

)
5
2
.
7
1

,
5
7
.
5
(

)
9
6
.
6
1

,
6
5
.
5
(

)
2
1
.
6
1

,
8
3
.
5
(

)
6
5
.
5
1

,
9
1
.
5
(

)
0
0
.
5
1

,
0
0
.
5
(

)
4
4
.
4
1

,
1
8
.
4
(

)
8
8
.
3
1

,
2
6
.
4
(

)
1
3
.
3
1

,
4
4
.
4
(

)
5
7
.
2
1

,
5
2
.
4
(

)
9
1
.
2
1

,
6
0
.
4
(

)
2
6
.
1
1

,
8
8
.
3
(

)
6
0
.
1
1

,
9
6
.
3
(

)
0
5
.
0
1

,
0
5
.
3
(

)
4
9
.
9

,
1
3
.
3
(

)
8
3
.
9

,
2
1
.
3
(

)
1
8
.
8

,
4
9
.
2
(

)
5
2
.
8

,
5
7
.
2
(

)
9
6
.
7

,
6
5
.
2
(

)
2
1
.
7

,
8
3
.
2
(

)
6
5
.
6

,
9
1
.
2
(

)
0
0
.
6

,
0
0
.
2
(

)
4
4
.
5

,
1
8
.
1
(

)
8
8
.
4

,
2
6
.
1
(

)
1
3
.
4

,
4
4
.
1
(

)
5
7
.
3

,
5
2
.
1
(

)
9
1
.
3

,
6
0
.
1
(

)
2
6
.
2

,
8
8
.
0
(

)
6
0
.
2

,
9
6
.
0
(

)
0
5
.
1

,
0
5
.
0
(

)
4
9
.
0

,
1
3
.
0
(

)
8
3
.
0

,
2
1
.
0
(

A
/
N

)
2
1
.
7
1

,
8
2
.
4
(

)
1
6
.
6
1

,
5
1
.
4
(

)
0
1
.
6
1

,
2
0
.
4
(

)
8
5
.
5
1

,
0
9
.
3
(

)
7
0
.
5
1

,
7
7
.
3
(

)
6
5
.
4
1

,
4
6
.
3
(

)
5
0
.
4
1

,
1
5
.
3
(

)
4
5
.
3
1

,
8
3
.
3
(

)
2
0
.
3
1

,
6
2
.
3
(

)
1
5
.
2
1

,
3
1
.
3
(

)
0
0
.
2
1

,
0
0
.
3
(

)
9
4
.
1
1

,
7
8
.
2
(

)
8
9
.
0
1

,
4
7
.
2
(

)
6
4
.
0
1

,
2
6
.
2
(

)
5
9
.
9

,
9
4
.
2
(

)
4
4
.
9

,
6
3
.
2
(

)
3
9
.
8

,
3
2
.
2
(

)
2
4
.
8

,
0
1
.
2
(

)
0
9
.
7

,
8
9
.
1
(

)
9
3
.
7

,
5
8
.
1
(

)
8
8
.
6

,
2
7
.
1
(

)
7
3
.
6

,
9
5
.
1
(

)
6
8
.
5

,
6
4
.
1
(

)
4
3
.
5

,
4
3
.
1
(

)
3
8
.
4

,
1
2
.
1
(

)
2
3
.
4

,
8
0
.
1
(

)
1
8
.
3

,
5
9
.
0
(

)
0
3
.
3

,
2
8
.
0
(

)
8
7
.
2

,
0
7
.
0
(

)
7
2
.
2

,
7
5
.
0
(

)
6
7
.
1

,
4
4
.
0
(

)
5
2
.
1

,
1
3
.
0
(

)
4
7
.
0

,
8
1
.
0
(

)
2
2
.
0

,
6
0
.
0
(

A
/
N

)
2
3
.
4
1

,
3
5
.
2
(

)
9
8
.
3
1

,
5
4
.
2
(

)
6
4
.
3
1

,
7
3
.
2
(

)
2
0
.
3
1

,
0
3
.
2
(

)
9
5
.
2
1

,
2
2
.
2
(

)
6
1
.
2
1

,
4
1
.
2
(

)
2
7
.
1
1

,
7
0
.
2
(

)
9
2
.
1
1

,
9
9
.
1
(

)
5
8
.
0
1

,
2
9
.
1
(

)
2
4
.
0
1

,
4
8
.
1
(

)
9
9
.
9

,
6
7
.
1
(

)
5
5
.
9

,
9
6
.
1
(

)
2
1
.
9

,
1
6
.
1
(

)
9
6
.
8

,
3
5
.
1
(

)
5
2
.
8

,
6
4
.
1
(

)
2
8
.
7

,
8
3
.
1
(

)
9
3
.
7

,
0
3
.
1
(

)
5
9
.
6

,
3
2
.
1
(

)
2
5
.
6

,
5
1
.
1
(

)
9
0
.
6

,
7
0
.
1
(

)
5
6
.
5

,
0
0
.
1
(

)
2
2
.
5

,
2
9
.
0
(

)
9
7
.
4

,
4
8
.
0
(

)
5
3
.
4

,
7
7
.
0
(

)
2
9
.
3

,
9
6
.
0
(

)
8
4
.
3

,
1
6
.
0
(

)
5
0
.
3

,
4
5
.
0
(

)
2
6
.
2

,
6
4
.
0
(

)
8
1
.
2

,
9
3
.
0
(

)
5
7
.
1

,
1
3
.
0
(

)
2
3
.
1

,
3
2
.
0
(

)
8
8
.
0

,
6
1
.
0
(

)
5
4
.
0

,
8
0
.
0
(

)
2
0
.
0

,
0
0
.
0
(

A
/
N

)
4
4
.
0
1

,
6
1
.
1
(

)
2
1
.
0
1

,
2
1
.
1
(

)
9
7
.
9

,
9
0
.
1
(

)
7
4
.
9

,
5
0
.
1
(

)
4
1
.
9

,
2
0
.
1
(

)
2
8
.
8

,
8
9
.
0
(

)
0
5
.
8

,
4
9
.
0
(

)
7
1
.
8

,
1
9
.
0
(

)
5
8
.
7

,
7
8
.
0
(

)
2
5
.
7

,
4
8
.
0
(

)
0
2
.
7

,
0
8
.
0
(

)
8
8
.
6

,
6
7
.
0
(

)
5
5
.
6

,
3
7
.
0
(

)
3
2
.
6

,
9
6
.
0
(

)
0
9
.
5

,
6
6
.
0
(

)
8
5
.
5

,
2
6
.
0
(

)
6
2
.
5

,
8
5
.
0
(

)
3
9
.
4

,
5
5
.
0
(

)
1
6
.
4

,
1
5
.
0
(

)
8
2
.
4

,
8
4
.
0
(

)
6
9
.
3

,
4
4
.
0
(

)
4
6
.
3

,
0
4
.
0
(

)
1
3
.
3

,
7
3
.
0
(

)
9
9
.
2

,
3
3
.
0
(

)
6
6
.
2

,
0
3
.
0
(

)
4
3
.
2

,
6
2
.
0
(

)
2
0
.
2

,
2
2
.
0
(

)
9
6
.
1

,
9
1
.
0
(

)
7
3
.
1

,
5
1
.
0
(

)
4
0
.
1

,
2
1
.
0
(

)
2
7
.
0

,
8
0
.
0
(

)
0
4
.
0

,
4
0
.
0
(

)
7
0
.
0

,
1
0
.
0
(

A
/
N

A
/
N

)
7
3
.
5
,
8
2
.
0
(

)
9
1
.
5
,
7
2
.
0
(

)
1
0
.
5
,
6
2
.
0
(

)
3
8
.
4
,
5
2
.
0
(

)
5
6
.
4
,
4
2
.
0
(

)
6
4
.
4
,
4
2
.
0
(

)
8
2
.
4
,
3
2
.
0
(

)
0
1
.
4
,
2
2
.
0
(

)
2
9
.
3
,
1
2
.
0
(

)
4
7
.
3
,
0
2
.
0
(

)
6
5
.
3
,
9
1
.
0
(

)
8
3
.
3
,
8
1
.
0
(

)
0
2
.
3
,
7
1
.
0
(

)
2
0
.
3
,
6
1
.
0
(

)
4
8
.
2
,
5
1
.
0
(

)
6
6
.
2
,
4
1
.
0
(

)
8
4
.
2
,
3
1
.
0
(

)
0
3
.
2
,
2
1
.
0
(

)
2
1
.
2
,
1
1
.
0
(

)
4
9
.
1
,
0
1
.
0
(

)
6
7
.
1
,
9
0
.
0
(

)
8
5
.
1
,
8
0
.
0
(

)
0
4
.
1
,
7
0
.
0
(

)
2
2
.
1
,
6
0
.
0
(

)
4
0
.
1
,
5
0
.
0
(

)
5
8
.
0
,
4
0
.
0
(

)
7
6
.
0
,
4
0
.
0
(

)
9
4
.
0
,
3
0
.
0
(

)
1
3
.
0
,
2
0
.
0
(

)
3
1
.
0
,
1
0
.
0
(

A
/
N

A
/
N

A
/
N

A
/
N

A
/
N

0
4
1
/
1

6
3
1
/
1

2
3
1
/
1

8
2
1
/
1

4
2
1
/
1

0
2
1
/
1

6
1
1
/
1

2
1
1
/
1

8
0
1
/
1

4
0
1
/
1

0
0
1
/
1

6
9
/
1

2
9
/
1

8
8
/
1

4
8
/
1

0
8
/
1

6
7
/
1

2
7
/
1

8
6
/
1

4
6
/
1

0
6
/
1

6
5
/
1

2
5
/
1

8
4
/
1

4
4
/
1

0
4
/
1

6
3
/
1

2
3
/
1

8
2
/
1

4
2
/
1

0
2
/
1

6
1
/
1

2
1
/
1

8
/
1

4
/
1

36

)
s
n
m
u
l
o
c
(

n
a
e
m

f
o

s
e
u
l
a
v

d
e
t
a
c
i
d
n
i

e
h
t

e
v
e
i
h
c
a

t
a
h
t

n
o
i
t
u
b
i
r
t
s
i
d

a
t
e
B

e
h
t

f
o

,
s
e
c
a
l
p

l
a
m
i
c
e
d

o
w

t

o
t

d
e
d
n
u
o
r

e
h
t

s
e
v
i
g

s
r
e
t
e
m
a
r
a
p
e
h
t

g
n
i
p
p
a
w
s

t
a
h
t

n
o
i
t
u
b
i
r
t
s
i
d
a
t
e
B
e
h
t

f
o

y
t
r
e
p
o
r
p
e
h
t

e
s
u
n
a
c

e
n
o

,
5
.
0

n
a
h
t

r
e
t
a
e
r
g

n
a
e
m

r
o
F

,
)
(cid:101)f
(cid:101)s
(

,

s
r
e
t
e
m
a
r
a
P

:

4

e
l
b
a
T

.
)
s
w
o
r
(

e
c
n
a
i
r
a
v
d
n
a

.

n
a
e
m
y
r
a
t
n
e
m
e
l
p
m
o
c

is: for all states satisfying sC + fC + sD + fD = T ,

FT (sC, fC, sD, fD) = 0

(18)

and for all t ∈ T and for all states satisfying sC + fC + sD + fD = t,

Ft(sC, fC, sD, fD) = p

a(sC ,fC ,sD ,fD )
C

+p

a(sC ,fC ,sD ,fD )
D

(cid:2)qC,(sC ,fC ,sD,fD),1 (1 + Ft+1(sC + 1, fC, sD, fD))
+qC,(sC ,fC ,sD,fD),0 (Ft+1(sC, fC + 1, sD, fD))(cid:3)
(cid:2)qD,(sC ,fC ,sD,fD),1 (1 + Ft+1(sC, fC, sD + 1, fD))
+qD,(sC ,fC ,sD,fD),0 (Ft+1(sC, fC, sD, fD + 1))(cid:3)

(19)

Using the new reward deﬁnition, the Poisson equation is: for all states satisfying sC + fC +

sD + fD = T ,

GT (sC, fC, sD, fD) = sC + sD

(20)

and for all t ∈ T and for all states satisfying sC + fC + sD + fD = t,

Gt(sC, fC, sD, fD) = p

a(sC ,fC ,sD ,fD )
C

+p

a(sC ,fC ,sD ,fD )
D

(cid:2)qC,(sC ,fC ,sD,fD),1 (Gt+1(sC + 1, fC, sD, fD))
+qC,(sC ,fC ,sD,fD),0 (Gt+1(sC, fC + 1, sD, fD))(cid:3)
(cid:2)qD,(sC ,fC ,sD,fD),1 (Gt+1(sC, fC, sD + 1, fD))
+qD,(sC ,fC ,sD,fD),0 (Gt+1(sC, fC, sD, fD + 1))(cid:3)

(21)

Lemma 1. The recursion (20)–(21) evaluates the design equivalently to the recursion (18)–(19), and in
particular, G0(0, 0, 0, 0) = F0(0, 0, 0, 0).

Proof. We will prove a more general result that for all t ∈ T ∪ {T } and for all states satisfying
sC + fC + sD + fD = t,

Gt(sC, fC, sD, fD) = Ft(sC, fC, sD, fD) + sC + sD.

From (18) and (20) it is clear that and for all states satisfying sC + fC + sD + fD = T ,

GT (sC, fC, sD, fD) = FT (sC, fC, sD, fD) + sC + sD.

It remains to prove that for all t ∈ T and for all states satisfying sC + fC + sD + fD = t,

Gt(sC, fC, sD, fD) = Ft(sC, fC, sD, fD) + sC + sD

Using induction assumption for t + 1

Gt+1(sC, fC, sD, fD) = Ft+1(sC, fC, sD, fD) + sC + sD

(22)

(23)

(24)

(25)

37

and plugging it into (21), we have

Gt(sC, fC, sD, fD) = p

a(sC ,fC ,sD ,fD )
C

+p

a(sC ,fC ,sD ,fD )
D

(cid:2)qC,(sC ,fC ,sD,fD),1 (Ft+1(sC + 1, fC, sD, fD) + sC + sD + 1)
+qC,(sC ,fC ,sD,fD),0 (Ft+1(sC, fC + 1, sD, fD) + sC + sD)(cid:3)
(cid:2)qD,(sC ,fC ,sD,fD),1 (Ft+1(sC, fC, sD + 1, fD) + sC + sD + 1)
+qD,(sC ,fC ,sD,fD),0 (Ft+1(sC, fC, sD, fD + 1) + sC + sD)(cid:3)

which after repetitively using (19) yields

Gt(sC, fC, sD, fD) = Ft(sC, fC, sD, fD) + p

a(sC ,fC ,sD ,fD )
C

+p

a(sC ,fC ,sD ,fD )
D

(cid:2)qC,(sC ,fC ,sD,fD),1 (sC + sD)
+qC,(sC ,fC ,sD,fD),0 (sC + sD)(cid:3)
(cid:2)qD,(sC ,fC ,sD,fD),1 (sC + sD)
+qD,(sC ,fC ,sD,fD),0 (sC + sD)(cid:3)

which gives Gt(sC, fC, sD, fD) = Ft(sC, fC, sD, fD) + sC + sD because (cid:80)
and (cid:80)

k = 1 for all a.

k pa

o qk,(x,i),o = 1 for all k

Note that these two theorems do not hold in the model with (non-uniform) geometric dis-

counting of future rewards, because there the order of observations matters.

C Designs Performance — Small Trial Sizes

Figure 5 and Figure 6 present the performance results for small trial sizes, analogously to

Figure 3 and Figure 4, respectively.

D Designs Performance — Tables

In Table 5, Table 6, Table 7, Table 8 we present values of the mean regret number of successes
(rounded to 3 signiﬁcant digits) for trial sizes 60 : 60 : 1200, which is a subset of those presented
in the ﬁgures in Section 6 and Appendix C. In bold font are all the values which are equal or
better than that of DP for each trial size. We further highlight with dark grey background (light
grey background) the values that are lower than or equal to 10% (greater than 10% and lower
than or equal to 20%) of the difference between DP and 2UCB.

38

Figure 5: An illustration of performance (mean on the left, standard deviation on the right) in
terms of the expected proportion of successes evaluated for deterministic designs over a range
of small trial sizes, for (θC, θD) = (0.7, 0.9) in the ﬁrst row, (0.5, 0.7) in the second row, (0.3, 0.5)
in the third row, (0.1, 0.3) in the fourth row.

39

Trial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.800.820.840.860.880.90Mean proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.120.140.160.180.200.220.24SD of proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.600.620.640.660.680.70Mean proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.120.140.160.180.200.220.24SD of proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.400.420.440.460.480.50Mean proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.120.140.160.180.200.220.24SD of proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.200.220.240.260.280.30Mean proportion of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns0.000.020.040.060.080.100.120.140.160.180.200.220.24SD of proportion of successesFigure 6: An illustration of performance (mean on the left, standard deviation on the right) in
terms of the proportion of successes evaluated for deterministic designs over a range of small
trial sizes, for (θC, θD) = (0.7, 0.9) in the ﬁrst row, (0.5, 0.7) in the second row, (0.3, 0.5) in the
third row, (0.1, 0.3) in the fourth row.

40

Trial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910Mean regret number of successesTrial size01:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910SD of regret number of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910Mean regret number of successesTrial size01:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910SD of regret number of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910Mean regret number of successesTrial size01:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910SD of regret number of successesTrial size03060901201501802102401:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910Mean regret number of successesTrial size01:1DPBMFM2UCB1UCB0.18UCBBKGBLFFBMSFBGDFBLFF+BMBLFF+2UCBBLFF+1UCBBLFF+0.18UCBBLFF+BMSF1:1+BMSFDesigns012345678910SD of regret number of successes0
0
2
1

0
4
1
1

0
8
0
1

0
2
0
1

0
6
9

0
0
9

0
4
8

0
8
7

0
2
7

0
6
6

0
0
6

0
4
5

0
8
4

0
2
4

0
6
3

0
0
3

0
4
2

0
8
1

0
2
1

0
6

e
z
i
s

l
a
i
r
T
\
n
g
i
s
e
D

0
2
1

4
5
2

.

.

3
4
7

.

6
6
3

.

5
8
2

.

1
8
1

4
0
6

.

.

9
7
3

.

9
9
5

.

3
5
9

.

1
3
8

7
3

.

.

5
8
2

.

1
8
1

7
2
5

.

6
9
5

.

.

0
1
1

4
1
1

5
5
2

.

.

6
0
7

.

8
4
3

.

8
7
2

.

7
7
1

8
9
5

.

.

1
6
3

.

9
6
5

.

5
0
9

.

0
9
7

3
6
3

.

.

8
7
2

.

7
7
1

9
1
5

.

8
8
5

.

.

5
0
1

8
0
1

5
5
2

.

.

9
6
6

.

0
3
3

.

0
7
2

.

3
7
1

2
9
5

.

.

2
4
3

.

9
3
5

.

8
5
8

.

8
4
7

7
5
3

.

.

0
7
2

.

3
7
1

1
5

.

3
8
5

.

.

7
0
1

2
0
1

7
5
2

.

.

2
3
6

.

2
1
3

.

3
6
2

.

9
6
1

5
8
5

.

.

4
2
3

.

9
0
5

.

0
1
8

.

7
0
7

1
5
3

.

.

3
6
2

.

9
6
1

1
0
5

.

7
5

.

.

2
0
1

.

0
6
9

8
5
2

.

.

5
9
5

.

4
9
2

.

5
5
2

.

5
6
1

9
7
5

.

.

5
0
3

.

9
7
4

.

2
6
7

.

5
6
6

6
4
3

.

.

5
5
2

.

5
6
1

2
9
4

.

5
6
5

.

.

3
0
1

.

0
0
9

6
2

.

.

8
5
5

.

6
7
2

.

7
4
2

.

0
6
1

2
7
5

.

.

7
8
2

.

9
4
4

.

5
1
7

.

4
2
6

2
4
3

.

.

7
4
2

.

0
6
1

4
8
4

.

5
5
5

.

.

4
0
1

.

0
4
8

1
6
2

.

.

1
2
5

.

9
5
2

.

8
3
2

.

6
5
1

4
6
5

.

.

8
6
2

.

9
1
4

.

7
6
6

.

2
8
5

5
3
3

.

.

8
3
2

.

6
5
1

2
7
4

.

6
4
5

.

.

4
0
1

.

0
8
7

2
6
2

.

.

4
8
4

.

1
4
2

.

9
2
2

.

1
5
1

6
5
5

.

.

0
5
2

.

9
8
3

.

9
1
6

.

1
4
5

3
3

.

.

9
2
2

.

1
5
1

2
6
4

.

3
5

.

3
7
9

.

.

0
2
7

3
6
2

.

.

6
4
4

.

3
2
2

.

9
1
2

.

5
4
1

6
4
5

.

.

1
3
2

.

9
5
3

.

2
7
5

.

9
9
4

5
2
3

.

.

9
1
2

.

5
4
1

1
5
4

.

5
2
5

.

6
6
9

.

0
.
6
6

4
6
.
2

9
.
0
4

5
.
0
2

9
.
0
2

0
.
4
1

5
3
.
5

3
.
1
2

9
.
2
3

4
.
2
5

8
.
5
4

2
.
3

9
.
0
2

0
.
4
1

9
3
.
4

8
0
.
5

8
5
.
9

0
.
0
6

3
6
.
2

2
.
7
3

7
.
8
1

8
.
9
1

4
.
3
1

1
2
.
5

4
.
9
1

9
.
9
2

7
.
7
4

6
.
1
4

6
1
.
3

8
.
9
1

4
.
3
1

5
2
.
4

5
9
.
4

1
4
.
9

0
.
4
5

2
6
.
2

5
.
3
3

9
.
6
1

6
.
8
1

7
.
2
1

5
0
.
5

6
.
7
1

9
.
6
2

9
.
2
4

5
.
7
3

1
1
.
3

6
.
8
1

7
.
2
1

2
1
.
4

2
8
.
4

6
1
.
9

0
.
8
4

1
6
.
2

8
.
9
2

2
.
5
1

4
.
7
1

0
.
2
1

6
8
.
4

7
.
5
1

9
.
3
2

1
.
8
3

3
.
3
3

6
0
.
3

4
.
7
1

0
.
2
1

6
9
.
3

7
6
.
4

2
8
.
8

0
.
2
4

8
5
.
2

1
.
6
2

4
.
3
1

0
.
6
1

2
.
1
1

5
6
.
4

9
.
3
1

9
.
0
2

4
.
3
3

2
.
9
2

2
0
.
3

0
.
6
1

2
.
1
1

8
.
3

9
4
.
4

9
3
.
8

0
.
6
3

7
5
.
2

4
.
2
2

6
.
1
1

6
.
4
1

3
.
0
1

3
4
.
4

0
.
2
1

9
.
7
1

6
.
8
2

0
.
5
2

6
9
.
2

6
.
4
1

3
.
0
1

2
6
.
3

7
3
.
4

3
8
.
7

0
.
0
3

7
5
.
2

7
.
8
1

8
7
.
9

0
.
3
1

3
3
.
9

7
1
.
4

1
.
0
1

9
.
4
1

8
.
3
2

9
.
0
2

8
8
.
2

0
.
3
1

3
3
.
9

3
4
.
3

6
0
.
4

3
1
.
7

0
.
4
2

8
5
.
2

0
.
5
1

9
9
.
7

2
.
1
1

2
.
8

4
8
.
3

5
2
.
8

9
.
1
1

1
.
9
1

7
.
6
1

8
.
2

2
.
1
1

2
.
8

9
1
.
3

2
8
.
3

4
2
.
6

0
.
8
1

4
5
.
2

2
.
1
1

8
1
.
6

5
1
.
9

9
.
6

9
2
.
3

6
3
.
6

8
8
.
8

3
.
4
1

6
.
2
1

5
6
.
2

5
1
.
9

9
.
6

5
8
.
2

2
4
.
3

9
5
.
5

0
.
2
1

6
3
.
2

3
5
.
7

6
3
.
4

1
8
.
6

1
3
.
5

6
7
.
2

5
4
.
4

8
8
.
5

5
5
.
9

4
.
8

9
3
.
2

1
8
.
6

1
3
.
5

5
4
.
2

8
8
.
2

5
1
.
4

0
.
6

5
8
.
1

2
8
.
3

5
.
2

4
9
.
3

6
2
.
3

5
1
.
2

8
4
.
2

9
.
2

8
7
.
4

5
2
.
4

2
8
.
1

4
9
.
3

6
2
.
3

9
8
.
1

6
9
.
1

1
8
.
2

B
C
U
8
1
.
0

B
C
U
2

B
C
U
1

G
K
B

F
F
L
B

F
S
M
B

F
D
G
B

1
:
1

P
D

M
B

M
F

B
C
U
2
+
F
F
L
B

B
C
U
1
+
F
F
L
B

M
B
+
F
F
L
B

B
C
U
8
1
.
0
+
F
F
L
B

F
S
M
B
+
F
F
L
B

F
S
M
B
+
1
:
1

.

9

.

0
d
n
a

7
0

.

s
e
i
t
i
l
i
b
a
b
o
r
p
s
s
e
c
c
u
s

r
e
d
n
u
s
e
s
s
e
c
c
u
s

f
o
r
e
b
m
u
n
t
e
r
g
e
r
n
a
e
m
e
h
T

:
5

e
l
b
a
T

41

0
0
2
1

0
4
1
1

0
8
0
1

0
2
0
1

0
6
9

0
0
9

0
4
8

0
8
7

0
2
7

0
6
6

0
0
6

0
4
5

0
8
4

0
2
4

0
6
3

0
0
3

0
4
2

0
8
1

0
2
1

0
2
1

5
6
4

.

.

4
4
5

.

9
1
6

.

6
8
2

.

1
8
1

5
4
7

.

.

1
4
2

.

9
9
8

.

8
1
9

.

4
5
1

7
7
6

.

.

6
8
2

.

1
8
1

4
6
6

.

.

5
1
1

.

2
1
2

4
1
1

2
6
4

.

.

8
1
5

.

8
8
5

.

9
7
2

.

7
7
1

7
3
7

.

.

0
3
2

.

4
5
8

.

2
7
8

.

0
5
1

7
6

.

.

9
7
2

.

7
7
1

2
5
6

.

.

4
1
1

.

2
0
2

8
0
1

8
5
4

.

.

1
9
4

.

8
5
5

.

1
7
2

.

4
7
1

3
7

.

.

0
2
2

.

9
0
8

.

6
2
8

.

5
4
1

3
6
6

.

.

1
7
2

.

4
7
1

1
4
6

.

.

3
1
1

.

1
0
2

2
0
1

5
5
4

.

.

5
6
4

.

7
2
5

.

4
6
2

.

0
7
1

1
2
7

.

.

9
0
2

.

4
6
7

.

0
8
7

.

1
4
1

6
5
6

.

.

4
6
2

.

0
7
1

9
2
6

.

.

0
1
1

.

1
9
1

.

0
6
9

1
5
4

.

.

8
3
4

.

7
9
4

.

6
5
2

.

5
6
1

3
1
7

.

.

8
9
1

.

9
1
7

.

4
3
7

.

7
3
1

8
4
6

.

.

6
5
2

.

6
6
1

7
1
6

.

.

0
1
1

.

9
8
1

.

0
0
9

8
4
4

.

.

2
1
4

.

6
6
4

.

8
4
2

.

1
6
1

4
0
7

.

.

7
8
1

.

4
7
6

.

9
8
6

.

2
3
1

9
3
6

.

.

8
4
2

.

1
6
1

9
0
6

.

.

7
0
1

.

7
8
1

.

0
4
8

3
4
4

.

.

5
8
3

.

6
3
4

.

9
3
2

.

6
5
1

3
9
6

.

.

6
7
1

.

9
2
6

.

3
4
6

.

7
2
1

2
3
6

.

.

9
3
2

.

6
5
1

3
9
5

.

.

6
0
1

.

3
8
1

.

0
8
7

9
3
4

.

.

8
5
3

.

5
0
4

.

0
3
2

.

2
5
1

2
8
6

.

.

5
6
1

.

4
8
5

.

7
9
5

.

2
2
1

3
2
6

.

.

0
3
2

.

2
5
1

8
5

.

.

2
0
1

.

1
7
1

.

0
2
7

5
3
4

.

.

2
3
3

.

5
7
3

.

0
2
2

.

6
4
1

9
6
6

.

.

4
5
1

.

9
3
5

.

1
5
5

.

7
1
1

3
1
6

.

.

0
2
2

.

6
4
1

8
6
5

.

.

1
0
1

.

7
6
1

0
.
6
6

3
.
4

5
.
0
3

4
.
4
3

0
.
1
2

1
.
4
1

4
5
.
6

3
.
4
1

4
.
9
4

5
.
0
5

2
.
1
1

2
0
.
6

0
.
1
2

1
.
4
1

4
5
.
5

9
6
.
9

1
.
6
1

0
.
0
6

5
2
.
4

8
.
7
2

4
.
1
3

9
.
9
1

5
.
3
1

6
3
.
6

2
.
3
1

9
.
4
4

9
.
5
4

6
.
0
1

3
9
.
5

9
.
9
1

5
.
3
1

8
3
.
5

4
.
9

5
.
5
1

0
.
4
5

9
1
.
4

1
.
5
2

3
.
8
2

8
.
8
1

8
.
2
1

5
1
.
6

1
.
2
1

4
.
0
4

3
.
1
4

1
.
0
1

8
7
.
5

8
.
8
1

8
.
2
1

4
2
.
5

7
0
.
9

7
.
4
1

0
.
8
4

2
1
.
4

5
.
2
2

3
.
5
2

5
.
7
1

1
.
2
1

9
.
5

0
.
1
1

9
.
5
3

7
.
6
3

3
4
.
9

5
6
.
5

5
.
7
1

1
.
2
1

6
0
.
5

9
6
.
8

9
.
3
1

0
.
2
4

5
0
.
4

8
.
9
1

2
.
2
2

2
.
6
1

3
.
1
1

4
6
.
5

3
8
.
9

4
.
1
3

1
.
2
3

7
7
.
8

8
4
.
5

2
.
6
1

3
.
1
1

7
8
.
4

5
2
.
8

9
.
2
1

0
.
6
3

6
9
.
3

1
.
7
1

1
.
9
1

7
.
4
1

5
.
0
1

6
3
.
5

8
6
.
8

9
.
6
2

6
.
7
2

6
0
.
8

7
2
.
5

7
.
4
1

5
.
0
1

7
6
.
4

6
8
.
7

7
.
1
1

0
.
0
3

6
8
.
3

4
.
4
1

0
.
6
1

1
.
3
1

8
4
.
9

5
0
.
5

2
5
.
7

4
.
2
2

0
.
3
2

9
2
.
7

0
.
5

1
.
3
1

8
4
.
9

5
4
.
4

7
1
.
7

4
.
0
1

0
.
4
2

7
.
3

6
.
1
1

0
.
3
1

3
.
1
1

6
3
.
8

7
.
4

3
3
.
6

9
.
7
1

4
.
8
1

4
4
.
6

8
6
.
4

3
.
1
1

6
3
.
8

7
1
.
4

3
5
.
6

7
8
.
8

0
.
8
1

7
4
.
3

8
8
.
8

6
8
.
9

2
2
.
9

5
0
.
7

7
2
.
4

1
1
.
5

4
.
3
1

8
.
3
1

7
4
.
5

2
2
.
4

2
2
.
9

5
0
.
7

3
8
.
3

3
6
.
5

3
5
.
7

0
.
2
1

8
0
.
3

1
.
6

4
7
.
6

4
8
.
6

5
4
.
5

7
6
.
3

3
8
.
3

1
9
.
8

1
2
.
9

2
3
.
4

6
5
.
3

4
8
.
6

5
4
.
5

3
3
.
3

9
4
.
4

2
4
.
5

0
6

0
.
6

3
.
2

7
2
.
3

7
5
.
3

6
9
.
3

6
3
.
3

6
.
2

1
4
.
2

2
4
.
4

2
6
.
4

3
8
.
2

7
4
.
2

6
9
.
3

6
3
.
3

4
4
.
2

4
8
.
2

3
3
.
3

e
z
i
s

l
a
i
r
T
\
n
g
i
s
e
D

B
C
U
8
1
.
0

B
C
U
2

B
C
U
1

G
K
B

F
F
L
B

F
S
M
B

F
D
G
B

1
:
1

P
D

M
B

M
F

B
C
U
2
+
F
F
L
B

B
C
U
1
+
F
F
L
B

M
B
+
F
F
L
B

B
C
U
8
1
.
0
+
F
F
L
B

F
S
M
B
+
F
F
L
B

F
S
M
B
+
1
:
1

.

7

.

0
d
n
a

5
0

.

s
e
i
t
i
l
i
b
a
b
o
r
p
s
s
e
c
c
u
s

r
e
d
n
u
s
e
s
s
e
c
c
u
s

f
o
r
e
b
m
u
n
t
e
r
g
e
r
n
a
e
m
e
h
T

:
6

e
l
b
a
T

42

0
0
2
1

0
4
1
1

0
8
0
1

0
2
0
1

0
6
9

0
0
9

0
4
8

0
8
7

0
2
7

0
6
6

0
0
6

0
4
5

0
8
4

0
2
4

0
6
3

0
0
3

0
4
2

0
8
1

0
2
1

0
6

e
z
i
s

l
a
i
r
T
\
n
g
i
s
e
D

0
2
1

2
9
4

.

.

2
6
3

.

3
1
7

.

7
8
2

.

2
8
1

5
8
5

.

.

1
1
1

.

9
9
9

.

1
3
8

.

4
3
1

6
2
7

.

.

7
8
2

.

2
8
1

3
0
7

.

.

3
2
1

.

2
1
2

4
1
1

8
8
4

.

.

4
4
3

.

8
7
6

.

0
8
2

.

9
7
1

8
5

.

.

7
0
1

.

9
4
9

.

0
9
7

.

1
3
1

6
1
7

.

.

0
8
2

.

9
7
1

9
6

.

.

1
2
1

.

3
0
2

8
0
1

4
8
4

.

.

7
2
3

.

2
4
6

.

3
7
2

.

5
7
1

5
7
5

.

.

3
0
1

.

9
9
8

.

8
4
7

.

7
2
1

6
0
7

.

.

3
7
2

.

5
7
1

7
7
6

.

.

1
2
1

.

2
0
2

2
0
1

9
7
4

.

.

0
1
3

.

7
0
6

.

5
6
2

.

1
7
1

7
5

.

9
8
9

.

.

9
4
8

.

7
0
7

.

4
2
1

6
9
6

.

.

5
6
2

.

1
7
1

3
6
6

.

.

8
1
1

.

2
9
1

.

0
6
9

4
7
4

.

.

3
9
2

.

2
7
5

.

7
5
2

.

7
6
1

4
6
5

.

8
4
9

.

.

9
9
7

.

5
6
6

.

0
2
1

6
8
6

.

.

7
5
2

.

7
6
1

5
6

.

.

7
1
1

.

9
8
1

.

0
0
9

9
6
4

.

.

5
7
2

.

6
3
5

.

9
4
2

.

2
6
1

8
5
5

.

6
0
9

.

.

9
4
7

.

4
2
6

.

6
1
1

7
7
6

.

.

9
4
2

.

2
6
1

2
4
6

.

.

4
1
1

.

7
8
1

.

0
4
8

3
6
4

.

.

8
5
2

.

1
0
5

.

0
4
2

.

8
5
1

1
5
5

.

4
6
8

.

.

9
9
6

.

2
8
5

.

2
1
1

5
6
6

.

.

0
4
2

.

8
5
1

3
2
6

.

.

2
1
1

.

3
8
1

.

0
8
7

8
5
4

.

.

1
4
2

.

5
6
4

.

1
3
2

.

3
5
1

3
4
5

.

2
2
8

.

.

9
4
6

.

1
4
5

.

8
0
1

4
5
6

.

.

1
3
2

.

3
5
1

1
6

.

.

9
0
1

.

2
7
1

.

0
2
7

2
5
4

.

.

3
2
2

.

0
3
4

.

1
2
2

.

8
4
1

5
3
5

.

8
7

.

.

9
9
5

.

9
9
4

.

3
0
1

2
4
6

.

.

1
2
2

.

8
4
1

6
9
5

.

.

7
0
1

.

7
6
1

0
.
6
6

5
4
.
4

6
.
0
2

4
.
9
3

1
.
1
2

2
.
4
1

6
2
.
5

7
3
.
7

9
.
4
5

8
.
5
4

6
8
.
9

9
2
.
6

1
.
1
2

2
.
4
1

2
8
.
5

3
.
0
1

2
.
6
1

0
.
0
6

8
3
.
4

9
.
8
1

9
.
5
3

0
.
0
2

6
.
3
1

5
1
.
5

5
9
.
6

9
.
9
4

6
.
1
4

8
3
.
9

5
1
.
6

0
.
0
2

6
.
3
1

3
6
.
5

5
9
.
9

5
.
5
1

0
.
4
5

3
.
4

1
.
7
1

3
.
2
3

9
.
8
1

9
.
2
1

5
0
.
5

1
5
.
6

9
.
4
4

4
.
7
3

8
8
.
8

9
9
.
5

9
.
8
1

9
.
2
1

9
4
.
5

9
5
.
9

7
.
4
1

0
.
8
4

2
2
.
4

4
.
5
1

8
.
8
2

6
.
7
1

2
.
2
1

3
9
.
4

8
0
.
6

9
.
9
3

3
.
3
3

5
3
.
8

2
8
.
5

6
.
7
1

2
.
2
1

9
2
.
5

7
1
.
9

9
.
3
1

0
.
2
4

3
1
.
4

6
.
3
1

2
.
5
2

3
.
6
1

5
.
1
1

8
.
4

3
6
.
5

9
.
4
3

1
.
9
2

8
7
.
7

2
6
.
5

3
.
6
1

5
.
1
1

9
0
.
5

7
.
8

9
.
2
1

0
.
6
3

2
0
.
4

9
.
1
1

7
.
1
2

8
.
4
1

6
.
0
1

5
6
.
4

8
1
.
5

9
.
9
2

0
.
5
2

7
1
.
7

9
3
.
5

8
.
4
1

6
.
0
1

8
8
.
4

7
2
.
8

7
.
1
1

0
.
0
3

9
8
.
3

1
.
0
1

1
.
8
1

2
.
3
1

1
6
.
9

6
4
.
4

2
7
.
4

9
.
4
2

8
.
0
2

1
5
.
6

1
.
5

2
.
3
1

1
6
.
9

5
6
.
4

4
5
.
7

4
.
0
1

0
.
4
2

2
7
.
3

5
3
.
8

6
.
4
1

4
.
1
1

9
4
.
8

1
2
.
4

3
2
.
4

9
.
9
1

7
.
6
1

8
7
.
5

6
7
.
4

4
.
1
1

9
4
.
8

5
3
.
4

6
8
.
6

6
8
.
8

0
.
8
1

8
4
.
3

5
5
.
6

0
.
1
1

1
3
.
9

8
1
.
7

8
8
.
3

2
7
.
3

9
.
4
1

5
.
2
1

5
9
.
4

9
2
.
4

1
3
.
9

8
1
.
7

0
.
4

2
9
.
5

1
5
.
7

0
.
2
1

1
.
3

7
.
4

5
4
.
7

2
9
.
6

6
5
.
5

9
3
.
3

3
1
.
3

3
9
.
9

7
3
.
8

6
9
.
3

2
6
.
3

2
9
.
6

6
5
.
5

8
4
.
3

2
7
.
4

2
4
.
5

0
.
6

3
3
.
2

4
7
.
2

6
8
.
3

1
0
.
4

4
4
.
3

8
4
.
2

1
3
.
2

4
9
.
4

2
2
.
4

6
6
.
2

5
5
.
2

1
0
.
4

4
4
.
3

6
5
.
2

1
0
.
3

2
3
.
3

B
C
U
8
1
.
0

B
C
U
2

B
C
U
1

G
K
B

F
F
L
B

F
S
M
B

F
D
G
B

1
:
1

P
D

M
B

M
F

B
C
U
2
+
F
F
L
B

B
C
U
1
+
F
F
L
B

M
B
+
F
F
L
B

B
C
U
8
1
.
0
+
F
F
L
B

F
S
M
B
+
F
F
L
B

F
S
M
B
+
1
:
1

.

5

.

0
d
n
a

3
0

.

s
e
i
t
i
l
i
b
a
b
o
r
p
s
s
e
c
c
u
s

r
e
d
n
u
s
e
s
s
e
c
c
u
s

f
o
r
e
b
m
u
n
t
e
r
g
e
r
n
a
e
m
e
h
T

:
7

e
l
b
a
T

43

0
0
2
1

0
4
1
1

0
8
0
1

0
2
0
1

0
6
9

0
0
9

0
4
8

0
8
7

0
2
7

0
6
6

0
0
6

0
4
5

0
8
4

0
2
4

0
6
3

0
0
3

0
4
2

0
8
1

0
2
1

0
6

e
z
i
s

l
a
i
r
T
\
n
g
i
s
e
D

0
2
1

6
1
4

.

8
9
6

.

.

0
5
5

.

8
8
2

.

4
8
1

5
9
4

.

9
8
2

.

5
0
1

.

4
5
5

.

2
0
8

9
2
6

.

.

8
8
2

.

4
8
1

5
6
6

.

7
7
9

.

.

6
1
1

4
1
1

1
1
4

.

6
7
6

.

.

3
2
5

.

1
8
2

.

0
8
1

9
4

.

8
8
2

.

.

7
9
9

.

6
2
5

.

2
6
7

4
1
6

.

.

1
8
2

.

0
8
1

5
6

.

1
6
9

.

.

2
1
1

8
0
1

7
0
4

.

3
5
6

.

.

6
9
4

.

4
7
2

.

6
7
1

4
8
4

.

7
8
2

.

.

4
4
9

.

9
9
4

.

2
2
7

9
9
5

.

.

4
7
2

.

6
7
1

5
3
6

.

3
4
9

.

.

2
1
1

2
0
1

2
0
4

.

3
6

.

.

8
6
4

.

6
6
2

.

2
7
1

8
7
4

.

6
8
2

.

.

2
9
8

.

1
7
4

.

2
8
6

4
8
5

.

.

6
6
2

.

2
7
1

1
2
6

.

8
1
9

.

.

7
0
1

.

0
6
9

7
9
3

.

7
0
6

.

.

1
4
4

.

8
5
2

.

8
6
1

2
7
4

.

5
8
2

.

.

9
3
8

.

3
4
4

.

2
4
6

9
6
5

.

.

8
5
2

.

8
6
1

6
0
6

.

1
0
9

.

.

7
0
1

.

0
0
9

1
9
3

.

4
8
5

.

.

4
1
4

.

0
5
2

.

4
6
1

5
6
4

.

4
8
2

.

.

7
8
7

.

6
1
4

.

2
0
6

1
6
5

.

.

0
5
2

.

4
6
1

7
9
5

.

4
8
8

.

.

7
0
1

.

0
4
8

6
8
3

.

1
6
5

.

.

6
8
3

.

1
4
2

.

9
5
1

8
5
4

.

3
8
2

.

.

4
3
7

.

8
8
3

.

2
6
5

4
5

.

.

1
4
2

.

9
5
1

7
7
5

.

7
5
8

.

.

7
0
1

.

0
8
7

8
3

.

8
3
5

.

.

9
5
3

.

2
3
2

.

4
5
1

1
5
4

.

2
8
2

.

.

2
8
6

.

1
6
3

.

2
2
5

6
2
5

.

.

2
3
2

.

4
5
1

2
6
5

.

1
3
8

.

.

0
0
1

.

0
2
7

3
7
3

.

4
1
5

.

.

2
3
3

.

2
2
2

.

9
4
1

3
4
4

.

1
8
2

.

.

9
2
6

.

3
3
3

.

2
8
4

1
1
5

.

.

2
2
2

.

9
4
1

7
4
5

.

2
1
8

.

9
8
9

.

0
.
6
6

6
6
.
3

9
.
4

4
.
0
3

2
.
1
2

3
.
4
1

4
3
.
4

8
.
2

7
.
7
5

6
.
0
3

2
.
4
4

7
9
.
4

2
.
1
2

3
.
4
1

1
3
.
5

4
8
.
7

6
7
.
9

0
.
0
6

9
5
.
3

6
6
.
4

7
.
7
2

1
.
0
2

7
.
3
1

4
2
.
4

8
7
.
2

4
.
2
5

8
.
7
2

2
.
0
4

7
7
.
4

1
.
0
2

7
.
3
1

1
1
.
5

6
5
.
7

5
.
9

0
.
4
5

1
5
.
3

1
4
.
4

0
.
5
2

9
.
8
1

0
.
3
1

4
1
.
4

7
7
.
2

2
.
7
4

0
.
5
2

2
.
6
3

3
6
.
4

9
.
8
1

0
.
3
1

5
9
.
4

6
2
.
7

8
1
.
9

0
.
8
4

2
4
.
3

6
1
.
4

2
.
2
2

7
.
7
1

3
.
2
1

3
0
.
4

6
7
.
2

9
.
1
4

3
.
2
2

2
.
2
3

2
4
.
4

7
.
7
1

3
.
2
1

3
7
.
4

5
9
.
6

8
7
.
8

0
.
2
4

3
3
.
3

9
.
3

5
.
9
1

3
.
6
1

5
.
1
1

9
.
3

4
7
.
2

7
.
6
3

5
.
9
1

2
.
8
2

3
2
.
4

3
.
6
1

5
.
1
1

1
5
.
4

2
6
.
6

3
.
8

0
.
6
3

2
2
.
3

4
6
.
3

7
.
6
1

9
.
4
1

7
.
0
1

5
7
.
3

2
7
.
2

4
.
1
3

8
.
6
1

2
.
4
2

2
0
.
4

9
.
4
1

7
.
0
1

8
2
.
4

8
2
.
6

2
7
.
7

0
.
0
3

1
.
3

6
3
.
3

0
.
4
1

2
.
3
1

8
6
.
9

8
5
.
3

7
.
2

2
.
6
2

0
.
4
1

2
.
0
2

8
.
3

2
.
3
1

8
6
.
9

4
0
.
4

9
7
.
5

2
0
.
7

0
.
4
2

5
9
.
2

7
0
.
3

3
.
1
1

4
.
1
1

5
5
.
8

8
3
.
3

7
6
.
2

9
.
0
2

3
.
1
1

2
.
6
1

2
5
.
3

4
.
1
1

5
5
.
8

4
7
.
3

4
3
.
5

2
1
.
6

0
.
8
1

7
7
.
2

5
7
.
2

4
5
.
8

6
3
.
9

3
2
.
7

2
1
.
3

2
6
.
2

7
.
5
1

1
5
.
8

2
.
2
1

2
2
.
3

6
3
.
9

3
2
.
7

4
.
3

2
7
.
4

8
3
.
5

0
.
2
1

1
5
.
2

7
3
.
2

8
.
5

6
9
.
6

6
.
5

7
7
.
2

2
5
.
2

4
.
0
1

5
7
.
5

6
1
.
8

8
.
2

6
9
.
6

6
.
5

5
9
.
2

8
8
.
3

3
0
.
4

0
.
6

2
0
.
2

4
8
.
1

5
0
.
3

4
0
.
4

6
4
.
3

4
1
.
2

5
1
.
2

2
.
5

9
9
.
2

6
1
.
4

3
1
.
2

4
0
.
4

6
4
.
3

3
2
.
2

6
6
.
2

1
6
.
2

B
C
U
8
1
.
0

B
C
U
2

B
C
U
1

G
K
B

F
F
L
B

F
S
M
B

F
D
G
B

1
:
1

P
D

M
B

M
F

B
C
U
2
+
F
F
L
B

B
C
U
1
+
F
F
L
B

M
B
+
F
F
L
B

B
C
U
8
1
.
0
+
F
F
L
B

F
S
M
B
+
F
F
L
B

F
S
M
B
+
1
:
1

.

3

.

0
d
n
a

1
0

.

s
e
i
t
i
l
i
b
a
b
o
r
p
s
s
e
c
c
u
s

r
e
d
n
u
s
e
s
s
e
c
c
u
s

f
o
r
e
b
m
u
n
t
e
r
g
e
r
n
a
e
m
e
h
T

:
8

e
l
b
a
T

44

