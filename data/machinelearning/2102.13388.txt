1
2
0
2

b
e
F
6
2

]
L
M

.
t
a
t
s
[

1
v
8
8
3
3
1
.
2
0
1
2
:
v
i
X
r
a

Zoetrope Genetic Programming for Regression

Aur´elie Boisbunon∗, Carlo Fanara∗, Ingrid Grenet∗,
Jonathan Daeden∗, Alexis Vighi∗, Marc Schoenauer†

∗MyDataModels, Sophia Antipolis, France
†INRIA Saclay, LRI, Orsay, France

Abstract

e Zoetrope Genetic Programming (ZGP) algorithm is
based on an original representation for mathematical
expressions, targeting evolutionary symbolic regression.
e zoetropic representation uses repeated fusion opera-
tions between partial expressions, starting from the ter-
minal set. Repeated fusions within an individual grad-
ually generate more complex expressions, ending up in
what can be viewed as new features. ese features are
then linearly combined to best t the training data. ZGP
individuals then undergo specic crossover and mutation
operators, and selection takes place between parents and
ospring. ZGP is validated using a large number of pub-
lic domain regression datasets, and compared to other
symbolic regression algorithms, as well as to traditional
machine learning algorithms. ZGP reaches state-of-the-
art performance with respect to both types of algorithms,
and demonstrates a low computational time compared to
other symbolic regression approaches.

1 Introduction

Symbolic Regression (SR) is a supervised learning ap-
proach that consists in searching through a vast space
of predictive models. is space encompasses the rigid
linear and polynomial models by enabling other transfor-
mations such as trigonometric and logarithmic, as well as
the generalized additive models by allowing (linear and)
nonlinear combinations of the transformed variables (see
[37] and references therein). SR models are oen repre-
sented via expression trees, and may include decision tree
models if we consider equality and inequality operators
instead of functions (see e.g. the Boolean multiplexer in
[16]). Models can also be unravelled through mathemati-
cal formulae, which make them much more interpretable
than other tree or network-based machine learning al-
gorithms such as random forests [6] or neural networks
where, with few recent exceptions [14], the relationships
among the variables remain hidden. SR thus oers a good
tradeo between exibility and interpretability. More-

over, it does not need large numbers of observations as
in deep neural networks, and can be applied to smaller
datasets (typically from several dozens to tens of thou-
sands).

e history of SR is closely related to that of Genetic
Programming (GP), starting with the early works of Koza
[16, 20]. Indeed, most of the approaches for SR are GP
algorithms (oen denoted GPSR), as these oer a nice
framework with expression trees representing the poten-
tial models. GPSR algorithms start with a pool of ini-
tial models, which are then iteratively and randomly per-
turbed to create new ones, until the one that ts the data
best is nally selected. Variants to this scheme are dis-
cussed in [34], whereas newer approaches are enlisted in
[35]. On the contrary, ”standard” machine learning al-
gorithms are not well suited for the complex task of op-
timizing both parameters and model shape at the same
time [28, 36]. Several exceptions are worth noting, com-
bining the idea of expression trees with classical ML [22]
or with neural networks [14].

While SR is very present in the eld of Evolutionary
Algorithms (EA) [22], it is almost completely absent in
Machine Learning (ML) reference books [12, 5] and tool-
boxes [27]. Possible reasons could be the following: rst,
there are many SR algorithms in the literature, each of-
fering various advantages [36], and it might be dicult to
know which one to use and how to tune their oen large
number of parameters; second, these algorithms are of-
ten slower than most ML algorithms, with performance
that did not match those of e.g. random forests up un-
til recently; nally, earlier works mostly tested symbolic
regression on synthetic data with a known equation in-
volving few input variables, with the aim of recovering
exactly this equation [15], and not oen on real datasets
with more than 5 variables. For a critical view on SR
benchmarks we refer the reader to [23, 25] and references
therein.

ese limitations have been overcome in the last
decade, at least partially. e issue of computational time
has been treated by Geometric Semantic Genetic Pro-
gramming (GSGP) 2.0 [7] with the proposition of a very
ecient algorithm. However, this eciency comes at the

1

 
 
 
 
 
 
cost of interpretability, as the use of geometric seman-
tic variation operators results in exponentially growing
trees. e performance of GPSR has been increased for
instance by the combination of GP with more standard
ML approaches [2]. Finally, novel benchmarks were es-
tablished lately that also compare SR and classical ML
algorithms on real datasets [26, 38, 1]. ese bench-
marks show that the performance of random forests can
be matched by increasing the number of individuals and
generations, considerably slowing down the computa-
tions. So the issue remains for GPSR to get good per-
formance in a reasonable time without losing its charac-
teristic interpretability. A recent and promising work has
been proposed in that sense [18], as well as our own work
that we present here.

In this article we present a new GP algorithm called
Zoetrope Genetic Programming (ZGP), which brings the
following main contributions: (1) a new and unseen rep-
resentation of models, allowing fast computation and fea-
ture engineering, while keeping the interpretability ad-
vantage of most SR methods through the explicit model
(2) novel mutation and crossover processes,
formula;
leading to improvement of models over the generations;
(3) performance that is comparable to the best ML (Gra-
dient Boosting) and SR algorithms. While ZGP can han-
dle the three main supervised learning tasks, namely re-
gression, binary classication and multiclass classica-
tion, we focus here only on the regression one.

e paper is organized as follows. Section 2 briey
presents the general context and introduces GPSR state-
of-the-art frameworks which are related to our work.
Section 3 describes the entire ZGP algorithm, with its un-
common representation of individuals and variation op-
erators, as well as the choices for tness and cost. Sec-
tion 4 presents and discusses the results of our bench-
mark against state-of-the-art SR frameworks and ML al-
gorithms on 98 regression datasets. Finally, we conclude
on our main contributions in Section 5, and suggest po-
tential future work.

2 Background

2.1 Context

Given a dataset D made of i.i.d. observations (𝑋𝑖, 𝑦𝑖 ) ∈
R𝑑 × R with 𝑋𝑖 = (𝑋𝑖1, . . . , 𝑋𝑖𝑑 ), the goal of regression
algorithms is to nd a function M : R𝑑 ↦→ R modelling
the link between 𝑦 and 𝑋 , such that it generalizes well on
unseen data from the same distribution.

As common in regression problems, as performance
measure for M on dataset D we use the Mean Squared

2

Error (MSE) dened by:

𝑀𝑆𝐸 (M, D) =

1
#D

∑︁

(𝑋𝑖,𝑦𝑖 ) ∈D

(𝑦𝑖 − M (𝑋𝑖 ))2 .

(1)

Any dataset D used in this work will be divided into
training (D𝑇 ), validation (D𝑉 ) and test, or holdout (D𝐻 )
sets. e holdout set D𝐻 is never to be seen during the
learning procedure, and is only used to assess the nal
performance of the model. e uses of the training and
validation sets is detailed in Section 3.2.

2.2 Related work

As mentioned above, several SR frameworks have been
recently proposed and already compared to classical ML
algorithms. First, some SR techniques are not based on
evolutionary process. For example, Fast Function Extrac-
tion (FFX) [22] only generates a large set of linear and
non linear features and then ts a linear model on the fea-
tures using elastic net [39]. While the deterministic part
can be aractive to avoid geing dierent models from
one run to another, it turns out that FFX oen results in
much larger models than conventional GP. Evolutionary
Feature Synthesis (EFS) [3] uses a similar idea, but avoids
building the basis entirely by randomly generating them.
It is however not a GP algorithm. e idea of linearly
combining branches of a tree is also very present in GP,
as it allows the construction of new features. Multiple
Regression Genetic Programming (MRGP) [2] combines
all the possible subtrees of a tree through LASSO [33],
thereby decoupling the linear regression from the con-
struction of a tree. More recently, La Cava et al. devel-
oped Feature Engineering Automation Tool (FEAT) [18],
which trades conciseness for accuracy. It is a stochastic
optimization providing a succinct syntactic representa-
tion with variable dependencies explicitly shown (in con-
trast to the semantic approach [29]). Another related re-
cent work is the Interaction-Transformation Evolution-
ary Algorithm (ITEA) [9], which builds generalized addi-
tive models including interactions between variables.

e eciency of GP has been another direction
of study. Geometric Semantic Genetic Programming
(GSGP) [24] is a technique combining trees to get new
individuals and adds semantic methods for crossover and
mutation in order to introduce a degree of ’awareness’.
However, in GSGP the generated individuals are larger
than their parents, resulting in large bloat, and longer
computing times. is is addressed by using a practi-
cal development environment, GSGP-C++ [7] with oper-
ators in native C++. Finally, other frameworks propose
ecient selection techniques. Age-Fitness Pareto Opti-
mization (AFP) [31] is meant to prevent premature con-
vergence in evolutionary algorithms by including age as
an optimization criterion using a Pareto front between

age and tness. is allows younger individuals to com-
pete with older and er ones. Also, 𝜖-lexicase selection
(EPLEX) [19] performs parent selection according to their
tness on few random training examples, dropping all the
population individuals with error higher than the best er-
ror. is selection technique is used in FEAT.

With respect to the above works, ZGP proposes two
novelties. First, ZGP uses a parametric representation
for its models. Second, within its complex genotype-to-
phenotype mapping, ZGP borrows to Geometric Seman-
tic Crossover [24], and thus compensates the could-be
limitations of a xed representation by creating a richer
set of smoother trajectories in the space of all possible
analytical expressions / programs. Furthermore, this pro-
cess sets a strict bound on the complexity of the resulting
expressions, and thus limits the bloat.

3 e ZGP algorithm

e Zoetrope Genetic Programming1 (ZGP) algorithm is
based on the original Zoetropic representation for pro-
grams, together with the corresponding variation op-
erators (crossover and mutation). However, the ”natu-
ral selection” components of all evolutionary algorithms
are here directly incorporated into the variation opera-
tors (i.e., selection takes place between the parents and
their ospring only). Furthermore, ZGP uses evolution-
ary components to build possible branches of a regression
tree, and standard ML techniques to optimize the combi-
nation of those branches.

3.1 e Zoetropic Representation

is section describes both the genotype and the
genotype-to-phenotype mapping of ZGP individuals. As
in standard tree-based GP [16, 4], ZGP individuals are
built from a set of unary or binary operators O and a set
of terminals T , variables of the problem and ephemeral
constants. e genotype of a ZGP individual is built us-
ing elements (partial expressions built on T and O) and
fusion operations (see below). Two parameters control
the size of the genotype as well as the derivation of the
corresponding phenotype (the nal expression used to
evaluate the tness of the individual): the number of ini-
tial elements 𝑛𝑒 and the number of maturation stages 𝑛𝑚.
An individual is built as follows:

Overview and notations e elements used during the
process can be seen as organized in 𝑛𝑚 levels, one per
maturation step. e 𝑛𝑒 elements of level 𝑘, denoted
(𝐸𝑘
𝑛𝑒 ), are constructed by maturation step 𝑘 from
1
the elements of level 𝑘 − 1. However, due to boundary

, . . . , 𝐸𝑘

1ZGP is a proprietary algorithm with patent pending. An open

source version is currently under development.

𝑖 , 𝐸 (cid:48)(cid:48)

conditions depending on the parity of 𝑛𝑒 , it is more con-
venient to visualize all the elements in a circle (reminding
the original zoetrope mechanism2). Figure 1 illustrates
the creation process, but for the sake of simplicity, the el-
ements of levels 0, 1, 2, 3 are denoted 𝐸𝑖 , 𝐸 (cid:48)
𝑖 and 𝑍𝑖
respectively (explanations below).
Initialization e 𝑛𝑒 elements (𝐸0
𝑛𝑒 ) are ran-
1
domly drawn in T , being a uniformly chosen variable
with 90% probability, or an ephemeral constant with
10% probability. e laer are uniformly drawn in
[𝐶𝑚𝑖𝑛, 𝐶𝑚𝑎𝑥 ], for some user-dened parameters 𝐶𝑚𝑖𝑛 and
𝐶𝑚𝑎𝑥 .
Fusion e fusion operation F transforms a pair (𝐸𝑖, 𝐸 𝑗 )
of elements into a new pair (𝐸 (cid:48)
𝑗 ) = F (𝐸𝑖, 𝐸 𝑗 ). It starts
by computing

, . . . , 𝐸0

𝑖 , 𝐸 (cid:48)

𝑓 (𝐸𝑖, 𝐸 𝑗 ) = 𝑟 · op1(𝐸𝑖, 𝐸 𝑗 ) + (1 − 𝑟 ) · op2(𝐸𝑖, 𝐸 𝑗 ),

(2)

where op𝑖 , 𝑖 = 1, 2 are operators uniformly chosen in O,
and 𝑟 = 𝑈 [0, 1] (in case op1 or op2 is unary, only 𝐸𝑖 is
taken into account). Elements 𝐸 (cid:48)
𝑗 are then dened
by

𝑖 and 𝐸 (cid:48)

𝐸 (cid:48)
𝑖 = 𝑏 · 𝐸𝑖 + (1 − 𝑏) · 𝑓 (𝐸𝑖, 𝐸 𝑗 )
𝐸 (cid:48)
𝑗 = (1 − 𝑏) · 𝐸 𝑗 + 𝑏 · 𝑓 (𝐸𝑖, 𝐸 𝑗 ),

, op2

where 𝑏 = 𝑈 {0, 1}, i.e., one new element is equal to one
randomly chosen original element, while the other is de-
, 𝑟, 𝑏).
ned by Eq. (2). e fusion F is dened by (op1
Note that these fusion operations, and in particular
Equation (2), are somehow similar to the Geometric Se-
mantic Crossover [24]. But the linear combination with
random weight is done here at the level of simple opera-
tors, not subtree, and during the genotype-to-phenotype
mapping, not during crossover. In both situation, this re-
sults in a smoother landscape than only allowing blunt
choices between one or the other argument, oering
more transitional states to the evolutionary process.
Maturation e 𝑘𝑡ℎ maturation step, or stage, consists
of the sequence of (cid:98)𝑛𝑒 /2(cid:99) fusions dening elements 𝐸𝑘
𝑖
from pairs of elements 𝐸𝑘−1
.
e Zoetrope model Aer 𝑛𝑚 maturation steps, the 𝑛𝑒
elements of level 𝑛𝑚, called ”Zoetropes”, are linearly com-
bined to obtain the nal model (see Section 3.2.1).
Complexity analysis ere are 𝑛𝑓 = 𝑛𝑚 · (cid:98)𝑛𝑒 /2(cid:99) + 𝑛𝑒 %2
fusions in total. At each fusion, the size of the elements
increases by the application of Eq. (2). In terms of stan-
dard GP indicators (though we never express the ZGP
models as trees), the depth of F (𝐸𝑖, 𝐸 𝑗 ) is three more
than the maximum depth of 𝐸𝑖 and 𝐸 𝑗 . Hence the depth

𝑖

2e term zoetrope historically denes one of the rst animation
devices before the camera, consisting of a cylinder with images inside,
that seem to be moving as the cylinder is turned.

3

applied is also specic. is section will hence describe
the operators as well as the choice of the individuals they
are applied to.

3.3.1 e Crossover Operator

e crossover process of ZGP uses two parents, but works
one-way: it only propagates components from the est
parent to the other one, somehow similarly to the In-
verOver operator for permutations [32]. It starts by se-
lecting 𝑛𝑡 individuals uniformly from the population (as
in standard tournament selection). en, it randomly re-
places some of the ’genes’ of the weakest parent by the
corresponding genes of the est parent. e genes to re-
place are randomly chosen from the initial elements (ter-
minals in T ) and the fusions, each fusion being consid-
ered as a single gene here.

3.3.2 Applying the Crossover

Our GP strategy for applying the crossover amounts to re-
peat 𝜌𝑋 ·𝑃 times the above procedure (tournament of size
𝑛𝑡 , one-way gi of genes from best to worst), for some
hyperparameter 𝜌𝑋 ∈ [0, 1].
Its actual implementation
runs some tournaments in parallel (i.e., without replace-
ment between the tournaments), in order to decrease the
overall computational time.

3.3.3 Point Mutation

e point mutation operator considers one parent, and
works as expected: it replaces some ’genes’ of the par-
ents by random values. However, the fusions are here
considered made of four ’genes’ here, the four compo-
, 𝑟, 𝑏) (Section 3.1), that can be modied
nents (op1
independently. For each point mutation, either one ele-
ment or one fusion is randomly chosen from the ”genes”
and mutated.

, op2

When an element is to be mutated, it is replaced by
a constant with probability 𝜌𝑐𝑠𝑡 , or with a variable uni-
formly chosen (and dierent from the current one if the
element is a variable). When replacing a variable with
a constant, this constant is simply chosen uniformly in
[𝐶𝑚𝑖𝑛, 𝐶𝑚𝑎𝑥 ]. When mutating a constant 𝐶 to a new con-
stant, an auxiliary constant ˆ𝐶 is uniformly drawn also
in [𝐶𝑚𝑖𝑛, 𝐶𝑚𝑎𝑥 ], an operator 𝑜 is uniformly drawn in
{×, /, +, 𝑝𝑜𝑤𝑒𝑟, 𝑛𝑖𝑙 }, and 𝐶 is replaced by 𝐶 𝑜 ˆ𝐶 (where
𝐶𝑛𝑖𝑙 ˆ𝐶 = ˆ𝐶).

, op2

When a fusion is to be mutated, only one (uniformly
, 𝑏, 𝑟 is modied. In
drawn) of its four components op1
case of an operator, a new operator is chosen uniformly
in O. 𝑟 is modied by ipping one bit of its binary rep-
resentation, and 𝑏 is simply ipped. Note that in ZGP,
each individual designated for mutation is actually mu-
tated twice. A rst point mutation is applied to a compo-

Figure 1: Illustration of Zoetropic representation build-
ing: for 𝑛𝑒 = 𝑛𝑚 = 3, there are 𝑛𝑓 = 4 fusions in total,
and for the sake of readability, the third one, generating
(𝐸”2, 𝐸”3) from (𝐸 (cid:48)
3), taking place between center and
2
right gures, is not represented. Note that 𝑍3 = 𝐸”3 as no
element is le for a fusion.

, 𝐸 (cid:48)

of the zoetropes is at most 3 ∗ 𝑛𝑚 + 1. e linear com-
bination applied to the zoetropes using the 𝑛𝑒 -ary addi-
tion operator can be viewed as adding two more levels of
depth. In particular, because all created individuals use
the same template, the complexity of any ZGP model re-
mains bounded. erefore, ZGP individuals are not sub-
ject to uncontrolled bloat.

3.2 Fitness and Cost Functions

3.2.1 Combination of zoetropes

As said in previous Section, at the end of all fusions, the
zoetropes are combined to obtain the full model as:

M𝜶 (𝑋 ) =

𝑛𝑒∑︁

𝑗=1

𝛼 𝑗𝑍 𝑗 (𝑋 ),

(3)

for some weights 𝜶 = (𝛼1, . . . , 𝛼𝑛𝑒 ) ∈ R𝑛𝑒 .

3.2.2 e Fitness Function

e tness function, used in Darwinian selection, is the
MSE of the best linear combination of the zoetropes on
the training set, 𝑀𝑆𝐸 (M𝜶 ∗, D𝑇 ).
In ZGP, this tness
function is applied within the variation operators, be-
tween parents and ospring. Furthermore, the best in-
dividual in the population w.r.t. the MSE on the valida-
tion set D𝑉 is stored at every generation, and aer the
algorithm has stopped, the overall best of these best-per-
generation is returned (still according to the MSE on the
validation set).

3.3 e variation operators

is section introduces the representation-specic varia-
tion operators, i.e., crossover and mutation (the initializa-
tion has been described in Section 3.1). As said, in ZGP,
the selection is made within these variation operators,
between parents and their ospring, using the MSE for
comparisons. Furthermore, the way these operators are

4

nent (element or fusion) randomly chosen from the ”ef-
fective components”, i.e., the components which are actu-
ally used by the model, thus ensuring that the mutation
has an impact on the model. A second point mutation
is applied to a component randomly chosen from all the
components (eective or not), allowing components free
from tness pressure to dri and preserve diversity once
they become eective [17].

4 Experimental Validation

is section describes and analyzes the performance of
ZGP on regression tasks with tabular data, and compares
them with those of state-of-the-art symbolic regression
and classical machine learning algorithms.

R2-score (computed with scikit-learn), and the computa-
tional time for each algorithm and each run. Note that
in [26], only 10 independent runs were run for each al-
gorithm, with random train-test splits. However, given
the variability of symbolic regression approaches, we be-
lieve it is more robust to increase the number of runs, and
fairer to compare them on exactly the same data.

All experiments were performed on a HP Z8 server
with 40 cores4. All runs end when the maximum number
of generation 𝐺 is reached, or when the standard devia-
tion of the best tness over a window of size 𝐿 reaches
some user-dened threshold 𝜏𝜎 , whichever comes rst.
e code to replicate the comparison experiments will be
provided together with the nal paper. A le containing
the results for all the algorithms, runs and datasets is pro-
vided in CSV format in the supplementary material.

4.1 Experimental Setting

4.2 Hyperparameters

e experiment closely follows the benchmark in [26],
where the algorithms were run on the Penn Machine
Learning Benchmarks (PMLB) database [25], a collection
of real-world, synthetic and toy datasets, with a restric-
tion to datasets with less than 3000 observations (small
data regime). We compare ZGP with the same SR al-
gorithms as in [26], namely MRGP [2], GSGP [24, 7],
EPLEX [19], AFP [31], with the best parameters their au-
thors found by 5-fold cross-validation. To this list, we
also added the more recent FEAT [18] and the determin-
istic FFX [22]. We also chose those algorithms because
of the availability of a Python interface (provided by the
benchmark’s authors in the case of GSGP and MRGP). In-
deed, while many state-of-the-art SR algorithms are open
source, their source code comes in dierent languages
(C++, Java, Matlab), hence quite some work is needed to
re-implement and run those under the same conditions.
As for classical ML approaches, we chose the following
algorithms from scikit-learn [27]: gradient boosting, ran-
dom forests (RF), decision trees, elastic net, kernel ridge
and linear SVR, the laer three being optimized by 5-fold
cross validation. Finally, we added a multi-layer percep-
tron with keras [8] as in our experience, the one from
scikit-learn does not perform well in general. e param-
eters for each algorithm are provided in supplementary
material.

e experiment consisted in 20 runs of each algorithm,
based on the same splits of training and test sets (70-30%)
for all algorithms3. All datasets were standardized with
scikit-learn’ StandardScaler. We computed the Normal-
ized Root Mean Squared Error (NRMSE), i.e., the square-
root of the MSE divided by the range of target values, the

3Note that some of the algorithms, including ZGP, further split the
training set into training and validation, the rate of which was let to
each algorithm’s default parameters.

ZGP has quite a large number of hyperparameters. On
the one hand, this allows a great exibility when tuning
the algorithm. But on the other hand, it makes its use
time-consuming. Hence some default values have been
xed by intensive trial-and-error experiments, which are
reported in Table 1. e optimization of these hyper-
paramters by some automatic Hyper Parameter Opti-
mization (HPO) procedure, like SMAC [13], AutoSkLearn
[10] or HyperBand [21] will be the subject of further
work.

4.3 Results and Discussion

As in [26], we report the median values for R2 and
NRMSE over the 20 runs, for each algorithm and each
dataset.

Figure 2 compares the distribution of these median
R2 scores (2a) and NRMSE (2b) over all datasets, while
the red dots show the average of the median R2/NRMSE
scores over all datasets (”average R2/NRMSE” in the se-
quel) . e algorithms are ordered by decreasing average
R2 and increasing average NRMSE, the best one being on
the le. Table 2 gives the average rank for each algorithm,
based on the median R2 scores (middle column) and me-
dian NRMSE (right column) for each dataset. Standard
deviations of the ranks are given in parenthesis.

Figure 2 and Table 2 show that gradient boosting at-
tains the best performance, closely followed by random
forests, FEAT and ZGP, and less closely by MRGP. Note
that ZGP has lower average R2 than FEAT and beer av-
erage rank in R2 and NRMSE. is fact comes from a few
worse estimations for ZGP on some datasets (lower out-
liers in R2, Figure 2a), and beer ones for other datasets

4CPU Intel(R) Xeon(R) Silver 4114, 2.20GHz, 64 GigaBytes of RAM.

5

Table 1: Hyperparameters and default values used in ZGP

(a) R2

Hyperparameter
name

Symbol

Value

Operator set

O

con-

tournament

for

# elements
# maturation stages
Interval
stants
Proba. of constants
Xover
size
Xover param.
Mutation param.
reshold
regime
Population size
Max.
tions
Stopping criterion∗

# of genera-

mut.

𝑛𝑒
𝑛𝑚
[𝐶𝑚𝑖𝑛, 𝐶𝑚𝑎𝑥 ]

𝜌𝑐𝑠𝑡
𝑛𝑡

𝜌𝑋
𝑚𝑚𝑢𝑡
𝑟𝑙𝑖𝑚

𝑃
𝐺

𝐿, 𝜏𝜎

{+,-,*,/,abs,
sqrt,
sin, cos, (cid:98)(cid:99), (cid:100)(cid:101), int,
mod}
7
3
[-3, 3]

0.1
12

0.1
4
0.1

500
100

30, 1𝑒−3

(b) NRMSE

∗e algorithm is stopped if either one of the two following criterion is
reached: 𝑔 = 𝐺 (the number of generations reaches the maximum) or
the standard deviation of the best tness over 𝐿 generations goes below
a threshold 𝜏𝜎 (inspired by [30]).

(higher rst and third quartiles in R2). e remaining al-
gorithms display lower performance with a much higher
variance, especially on the R2 scores.

To further the comparison for symbolic regression, Fig-
ure 3 displays the performance in R2 against the com-
putational time for all SR algorithms (top), and for the
top three SR algorithms (boom), namely ZGP, FEAT and
MRGP. A ’good’ algorithm should be in the upper le
corner of these graphs (high performance and low com-
putational time). Note that this comparison of runtime
is somewhat qualitative because the algorithms rely on
dierent programming languages (ZGP and FEAT are in
C++ with a Python interface, while MRGP is in Java). In
order to make the comparison as fair as possible, we pro-
vided FEAT and MRGP with the maximum execution time
as run by ZGP, because both FEAT and MRGP require an
upper limit in their input time parameter.

ese gures show that ZGP and FEAT share the best
performance. Moreover, while all SR algorithms have
scaered computational time, GSGP is the fastest SR algo-
rithm, complying with its claim. However, it has a large
variance in performance, as does FFX. Among the best
three, ZGP thus shows the highest performance and the
shortest computational time. Also, we note that the al-
gorithms with the lowest performance, GSGP, AFP and

Figure 2: Distribution of the median performance (top:
R2, boom: NRMSE) on test set for each dataset. Red
points show the average of median R2 over all datasets,
and the algorithms are ordered by this measure (best is
le, with value closest to 1).

EPLEX, are those relying on the smallest operator set
O = {+, −, ∗, /}. On the contrary, the best performance
is obtained by algorithms with a wider operator set, in-
cluding trigonometric functions and square roots among
others (the full list of operators for each algorithm is pro-
vided in supplementary material). e choice of the op-
erator set appears to be an important one: we investi-
gated it further by expanding it for EPLEX and AFP to
{+, −, ∗, /, sin, cos, sqrt}, and their performance was in-
deed greatly improved, but still far from those of ZGP and
FEAT; we therefore decided to keep the default set in the
results presented here to be consistent with the bench-
mark in [26], and to report the corresponding metrics in
supplementary material. Note also that EPLEX and AFP
are selection methods, and not full GPSR algorithm, and
that EPLEX is used as a selection mechanism for FEAT.
As for the deterministic FFX, it turns out that both per-
formance and computational time are widely scaered,

6

Table 2: Average (and standard deviation) of the ranks in
median R2-scores and NRMSE on test set.

(a) All SR algorithms

Algorithm

GradBoost
ZGP
RF
FEAT
KernelRidge
MRGP
FFX
MLP
AFP
EnetCV
LinearSVR
Tree
EPLEX
GSGP

R2
avg
rank (std)

NRMSE
avg rank
(std)

3.7 (2.9)
4.9 (3.0)
5.0 (2.7)
5.6 (2.7)
6.0 (3.5)
7.2 (3.4)
7.5 (5.1)
7.9 (4.3)
8.4 (2.0)
8.4 (4.0)
9.2 (3.9)
9.6 (2.9)
10.3 (3.4)
11.5 (2.8)

3.7 (2.9)
5.0 (3.0)
5.1 (2.7)
5.4 (2.8)
6.1 (3.5)
7.2 (3.3)
7.5 (5.2)
7.9 (4.2)
8.4 (1.9)
8.4 (4.0)
9.1 (4.1)
9.6 (3.0)
10.2 (3.4)
11.5 (2.8)

ranging from very good R2 and low computational time
for some datasets, to the worst R2 or computational time
on others. It is also worth noting that FFX is the only al-
gorithm that cannot be parametrized directly to run on
one thread only and it actually spans all 40 cores of our
server, while all the other algorithms were limited to one
core per run.

We are aware of the limitations of the datasets utilized.
Despite their number, as mentioned in this section and
in the introduction, a good portion (62 of them) consists
of simulated data from the Friedman collection of arti-
cial datasets, that follow a known nonlinear function with
only 3 relevant variables, as described in [11]. Hence, the
chosen database may be a favorable seing for symbolic
regression approaches that tend to select few variables in
the nal model (among which ZGP).

5 Conclusion and future work

ZGP, a novel GPSR algorithm, is presented and its inner
working explained in detail. e algorithm has been val-
idated on regression tasks, in comparison with several-
state-of-the art algorithms, both from classic ML tools
and from existing GP-based SR frameworks.

e performance of ZGP is comparable or beer than
the state-of-the-art SR algorithms, in terms of accuracy
of the resulting model (measured both by the RMSE or
the R2), and of computational time. It is also compara-
ble to state-of-the-art classic ML algorithms, but performs
somewhat worse than the most advanced ML algorithms
like gradient boosting. However, the comparison of the

(b) Top 3 SR algorithms

Figure 3: Performance (R2 on test) vs CPU time for SR al-
gorithms (top:all, boom: best ones, ZGP, FEAT, MRGP).
Transparent markers are the medians for each dataset,
large plain markers are the medians over all datasets,
and rectangle transparent patches are the 25%-75% per-
centiles. e closest from the upper le corner (high R2,
low CPU time) the beer.

computational time is semi-quantitative as it is dicult
to guarantee conditions that are fully equivalent for all.
Similarly to the majority of the SR algorithms, ZGP’s
interpretability is aained through a tight selection of
variables, and the output of an analytical formula, link-
ing the selected variables to the target. However, and
dierent from most other SR algorithms, ZGP is ”bloat-
adverse by design”: the zoetropic representation, and the
genotype-to-phenotype mapping give an upper bound
for the complexity of all ZGP models. Last but not least,
ZGP performs feature construction and selection:
the
”zoetropes”, the elements obtained on the last layer of
the development process, are simply combined by linear
regression. erefore, they do represent useful features,
being a by-product of the algorithm rather than a sepa-
rate pre-processing step. ese features oer yet another
insight on the interpretation of the model.

7

101102Time (s)0.00.20.40.60.81.0Test R2R2=0.82R2=0.82R2=0.74R2=0.66R2=0.32R2=0.56R2=0.34ZGPFEATMRGPFFXEPLEXAFPGSGP101Time (s)0.00.20.40.60.81.0Test R2R2=0.82R2=0.82R2=0.74ZGPFEATMRGPOne of the specics of ZGP is the use of the fusion op-
eration during the genotype-to-phenotype mapping (see
Section 3.1 and in particular Equation 2). No operator is
applied alone, and smooth transitions from one operator
to the other are possible through modications by mu-
tation of the random weight 𝑟 , in a way similar to that
of Geometric Semantic Crossover [24]. However, the lin-
ear combination is limited here to simple operators, and
is only performed 𝑛𝑚 times, thus does not result in un-
controlled bloat: instead of increasing the search space
by augmenting the complexity of the trees, as in tradi-
tional GP, the search space is extended in ZGP by replac-
ing the discrete set of operators by the continuous family
obtained by their linear combinations. On-going ablation
studies are investigating this hypothesis.

In contrast to algorithms designed for big data, ZGP,
like all GP-based SR algorithms, can aain its results
by handling datasets with less than a few thousands of
observations (less than 3000 in the present experiments),
a context oen loosely referred to today as ”small data”.
Whereas emphasis has been put on Big Data in the recent
years due to impressive results in image recognition and
Natural Language Processing, to name a few, for many
more companies out there the available data does not
qualify as ”Big”.

As mentioned in the introduction, ZGP can also be ap-
plied to classication and benchmarking in both binary,
and multi-class tasks is the subject of on-going work. Fur-
thermore, an extension of the benchmark to a database
including more real-world datasets for all three tasks will
provide a fuller assessment for those algorithms. Even
though the inner mechanism of the ZGP algorithm does
limit the bloat, a major eort for future work is the quan-
titative assessment of the model complexity. Several mea-
sures may capture the complexity of symbolic regression
models, and we plan to assess it as an additional level for
model selection. Finally, hyperparameter tuning is oen
performed ad-hoc, whereas a systematic treatment may
help, in particular to select the number of elements and
stages, which can be constraining at present.

References

[1] M. Aenzeller, B. Burlacu, V. Dorfer, S. Dorl, G. Halmer-
bauer, T. K¨onigswieser, M. Kommenda, J. Veer, and
S. Winkler. White box vs. black box modeling: On the per-
formance of deep learning, random forests, and symbolic
regression in solving regression problems. In International
Conference on Computer Aided Systems eory, pages
288–295. Springer, 2019.

[3] I. Arnaldo, U.-M. O’Reilly, and K. Veeramachaneni.
Building predictive models via feature synthesis.
In
Proceedings of the 2015 Annual Conference on Genetic
and Evolutionary Computation, pages 983–990, 2015.

[4] W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone.

Genetic programming. Springer, 1998.

[5] C. M. Bishop. Paern recognition and machine learning.
Information science and statistics. Springer, New York, NY,
2006. Socover published in 2016.

[6] L. Breiman. Random forests. Machine learning, 45(1):5–32,

2001.

[7] M. Castelli and L. Manzoni. Gsgp-c++ 2.0: A geometric
semantic genetic programming framework. SowareX,
10:100313, 2019.

[8] F. Chollet et al. Keras. https://keras.io, 2015.

[9] F. O. de Franc¸a and G. S.

Interaction-
transformation evolutionary algorithm for symbolic re-
gression. Evolutionary Computation, pages 1–25, 2020.

I. Aldeia.

[10] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg,
M. Blum, and F. Huer. Ecient and robust automated
machine learning.
In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garne, editors, Advances in Neural
Information Processing Systems, volume 28, pages 2962–
2970. Curran Associates, Inc., 2015.

[11] J. H. Friedman. Greedy function approximation: a gradient
boosting machine. Annals of statistics, pages 1189–1232,
2001.

[12] T. Hastie, R. Tibshirani, and J. Friedman. e elements of
statistical learning: data mining, inference, and prediction.
Springer Science & Business Media, 2009.

[13] F. Huer, H. H. Hoos, and K. Leyton-Brown.

Se-
quential Model-based Optimization for General Algo-
rithm Conguration.
the 5th
International Conference on Learning and Intelligent
Optimization, LION’05, pages 507–523, Berlin, Heidelberg,
2011. Springer-Verlag. event-place: Rome, Italy.

In Proceedings of

[14] S. Kim, P. Y. Lu, S. Mukherjee, M. Gilbert, L. Jing,
V. ˇCeperi´c, and M. Soljaˇci´c. Integration of neural network-
based symbolic regression in deep learning for scientic
IEEE Transactions on Neural Networks and
discovery.
Learning Systems, 2020.

[15] M. F. Korns. Accuracy in symbolic regression. In Genetic
Programming eory and Practice IX, pages 129–151.
Springer, 2011.

[16] J. R. Koza. Genetic programming: on the programming of
computers by means of natural selection, volume 1. MIT
press, 1992.

[17] W. La Cava, T. Helmuth, L. Spector, and K. Danai. Ge-
netic programming with epigenetic local search.
In
Proceedings of the 2015 Annual Conference on Genetic
and Evolutionary Computation, pages 1055–1062, 2015.

[2] I. Arnaldo, K. Krawiec, and U.-M. O’Reilly. Multiple re-
In Proceedings of the
gression genetic programming.
2014 Annual Conference on Genetic and Evolutionary
Computation, pages 879–886, 2014.

[18] W. La Cava, T. R. Singh, J. Taggart, S. Suri, and J. H. Moore.
Learning concise representations for regression by evolv-
In International Conference on
ing networks of trees.
Learning Representations, ICLR, 2019.

8

[19] W. La Cava, L. Spector, and K. Danai. Epsilon-lexicase se-
lection for regression. In Proceedings of the Genetic and
Evolutionary Computation Conference 2016, pages 741–
748, 2016.

[33] R. Tibshirani. Regression shrinkage and selection via the
lasso: a retrospective.
Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 73(3):273–282,
2011.

[34] L. Vanneschi, M. Castelli, and S. Silva. A survey of
semantic methods in genetic programming.
Genetic
Programming and Evolvable Machines, 15(2):195–214,
2014.

[35] M. Virgolin, T. Alderliesten, C. Wieveen, and P. A. N.
Improving model-based genetic programming
Bosman.
for symbolic regression of small expressions. Evolutionary
Computation, page 1–27, Jun 2020.

[36] S. d. Vries. Sensitivity analysis based feature-guided evo-
lution for symbolic regression. Master’s thesis, 2018.

[37] J. ˇZegklitz and P. Poˇs´ık.

rithms with built-in linear regression.
arXiv:1701.03641, 2017.

Symbolic regression algo-
arXiv preprint

[38] J. ˇZegklitz and P. Poˇs´ık. Benchmarking state-of-the-art
symbolic regression algorithms. Genetic Programming
and Evolvable Machines, pages 1–29, 2020.

[39] H. Zou and T. Hastie. Regularization and variable selection
via the elastic net. Journal of the royal statistical society:
series B (statistical methodology), 67(2):301–320, 2005.

[20] W. B. Langdon and R. Poli.

Foundations of genetic
programming. Springer Science & Business Media, 2013.

[21] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and
A. Talwalkar. Hyperband: A Novel Bandit-Based Ap-
proach to Hyperparameter Optimization.
Journal of
Machine Learning Research, 18(185):1–52, 2018.

[22] T. McConaghy. FFX: Fast, scalable, deterministic symbolic
In Genetic Programming eory

regression technology.
and Practice IX, pages 235–260. Springer, 2011.

[23] J. McDermo, D. R. White, S. Luke, L. Manzoni, M. Castelli,
L. Vanneschi, W. Jaskowski, K. Krawiec, R. Harper,
K. De Jong, et al. Genetic programming needs beer
benchmarks. In Proceedings of the 14th annual conference
on Genetic and evolutionary computation, pages 791–798,
2012.

[24] A. Moraglio, K. Krawiec, and C. G. Johnson. Geometric se-
mantic genetic programming. In International Conference
on Parallel Problem Solving from Nature, pages 21–31.
Springer, 2012.

[25] R. S. Olson, W. La Cava, P. Orzechowski, R. J. Urbanow-
icz, and J. H. Moore. PMLB: a large benchmark suite for
machine learning evaluation and comparison. BioData
Mining, 10(1):36, Dec 2017.

[26] P. Orzechowski, W. La Cava, and J. H. Moore. Where
are we now? a large benchmark study of recent symbolic
In Proceedings of the Genetic and
regression methods.
Evolutionary Computation Conference, pages 1183–1190,
2018.

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. irion, O. Grisel, M. Blondel, P. Preenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning
Research, 12:2825–2830, 2011.

[28] R. Poli, W. B. Langdon, N. F. McPhee, and J. R. Koza. A
eld guide to genetic programming. Lulu. com, 2008.

[29] M. T. Ribeiro, S. Singh, and C. Guestrin.

” why should
i trust you?” explaining the predictions of any classier.
In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining,
pages 1135–1144, 2016.

[30] O. Rudenko and M. Schoenauer. A steady performance
stopping criterion for pareto-based evolutionary algo-
rithms. In 6th International Multi-Objective Programming
and Goal Programming Conference, 2004.

[31] M. Schmidt and H. Lipson. Age-tness Pareto optimiza-
In Genetic programming theory and practice VIII,

tion.
pages 129–146. Springer, 2011.

[32] G. Tao and Z. Michalewicz. Inver-over operator for the tsp.
In International Conference on Parallel Problem Solving
from Nature, pages 803–812. Springer, 1998.

9

