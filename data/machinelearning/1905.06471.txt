Synthesis of Provably Correct Autonomy Protocols
for Shared Control

Murat Cubuktepe, Nils Jansen, Mohammed Alsiekh, Ufuk Topcu

9
1
0
2

y
a
M
5
1

]

O
R
.
s
c
[

1
v
1
7
4
6
0
.
5
0
9
1
:
v
i
X
r
a

Abstract—We synthesize shared control protocols subject to
probabilistic temporal logic speciﬁcations. More speciﬁcally, we
develop a framework in which a human and an autonomy
protocol can issue commands to carry out a certain task.
We blend these commands into a joint input to a robot. We
model the interaction between the human and the robot as
a Markov decision process (MDP) that represents the shared
control scenario. Using inverse reinforcement learning, we obtain
an abstraction of the human’s behavior and decisions. We use
randomized strategies to account for randomness in human’s
decisions, caused by factors such as complexity of the task
speciﬁcations or imperfect interfaces. We design the autonomy
protocol to ensure that the resulting robot behavior satisﬁes given
safety and performance speciﬁcations in probabilistic temporal
logic. Additionally, the resulting strategies generate behavior
as similar to the behavior induced by the human’s commands
as possible. We solve the underlying problem efﬁciently using
quasiconvex programming. Case studies involving autonomous
wheelchair navigation and unmanned aerial vehicle mission
planning showcase the applicability of our approach.

I. INTRODUCTION

In shared control, a robot executes a task to accomplish the
goals of a human operator while adhering to additional safety
and performance requirements. Applications of such human-
robot interaction include remotely operated semi-autonomous
wheelchairs [15], robotic teleoperation [19], and human-in-
the-loop unmanned aerial vehicle mission planning [11]. A
human operator issues a command through an input interface,
which maps the command directly to an action for the robot.
The problem is that a sequence of such actions may fail
to accomplish the task at hand, due to limitations of the
interface or failure of the human operator in comprehending the
complexity of the problem. Therefore, a so-called autonomy
protocol provides assistance for the human in order to complete
the task according to the given requirements.

At the heart of the shared control problem is the design of
an autonomy protocol. In the literature, there are two main
directions, based on either switching the control authority
between human and autonomy protocol [28], or on blending
their commands towards joined inputs for the robot [9], [18].
One approach to switching the authority ﬁrst determines
the desired goal of the human operator with high conﬁdence,
and then assists towards exactly this goal [10], [21]. In [14],

M. Cubuktepe and U. Topcu are with the Department of Aerospace Engi-
neering and Engineering Mechanics, University of Texas at Austin, 201 E 24th
St, Austin, TX 78712, USA. Nils Jansen is with the Department of Software
Science, Radboud University Nijmegen, Comeniuslaan 4, 6525 HP Nijmegen,
the Netherlands. Mohammed Alsiekh was with the Institute for Computational
Engineering and Sciences, University of Texas at Austin, 201 E 24th St,
Austin, TX 78712, USA. email:({mcubuktepe,malsiekh,utopcu}@utexas.edu,
n.jansen@science.ru.nl).

switching the control authority between the human and au-
tonomy protocol ensures satisfaction of speciﬁcations that are
formally expressed in temporal logic. In general, switching of
authority may cause a decrease in human’s satisfaction, who
usually prefers to retain as much control as possible [20].

Blending incorporates providing an alternative command
in addition to the one of the human operator. To introduce a
more ﬂexible trade-off between the human’s control authority
and the level of autonomous assistance, both commands are
then blended to form a joined input for the robot. A blending
function determines the emphasis that is put on the autonomy
protocol in the blending, that is, regulating the amount of
assistance provided to the human. Switching of authority
can be seen as a special case of blending, as the blending
function may assign full control to the autonomy protocol
or to the human. In general, putting more emphasis on the
autonomy protocol in blending may lead to greater accuracy
in accomplishing the task [8], [9], [23]. However, as humans
prefer to retain control of the robot and may not approve if a
robot issues a set of commands that is signiﬁcantly different
to the human’s command [19], [20]. None of the existing
blending approaches provide formal correctness guarantees
that go beyond statistical conﬁdence bounds. Correctness here
refers to ensuring safety and optimizing performance according
to the given requirements. Our goal is to design an autonomy
protocol that admits formal correctness while rendering the
robot behavior as close to the human’s commands as possible,
which is shown to enhance the human experience.

A human may be uncertain about which command to issue
in order to accomplish a task. Moreover, a typical interface
used to parse human’s commands, such as a brain-computer
interface, is inherently imperfect. To capture such uncertainties
and imperfections in the human’s decisions, we introduce
randomness to the commands issued by humans. It may not
be possible to blend two different deterministic commands. If
the human’s command is “up” and the autonomy protocol’s
command is “right”, we cannot blend these two commands
to obtain another deterministic command. By introducing
randomness to the commands of the human and the autonomy
protocol, we ensure that the blending is always well-deﬁned.
Take as an example a scenario involving a semi-autonomous
wheelchair [15] whose navigation has to account for a randomly
moving autonomous vacuum cleaner, see Fig. 1. The wheelchair
needs to navigate to the exit of a room, and the vacuum
cleaner moves in the room according to a probabilistic transition
function. The task of the wheelchair is to reach the exit gate
while not crashing into the vacuum cleaner. The human may not
fully perceive the motion of the vacuum cleaner. Note that the

 
 
 
 
 
 
0.2

0.2

0.4

0.2

(a) Autonomy perspective

(b) Human perspective

Fig. 1. A wheelchair in a shared control setting.

human’s commands, depicted with the solid red line in Fig 1(a),
may cause the wheelchair to crash into the vacuum cleaner.
The autonomy protocol provides another set of commands,
which is indicated by the solid red line in Fig 1(b), to carry
out the task safely without crashing. However, the autonomy
protocol’s commands deviate highly from the commands of
the human. The two sets of commands are then blended into
a new set of commands, depicted using the dashed red line
in Fig 1(b). The blended commands perform the task safely
while generating behavior as similar to the behavior induced
by the human’s commands as possible.

We model the behavior of the robot as a Markov decision
process (MDP) [27], which captures the robot’s actions inside a
potentially stochastic environment. Problem formulations with
MDPs typically focus on maximizing an expected reward (or,
minimizing the expected cost). However, such formulations may
not be sufﬁcient to ensure safety or performance guarantees in
a task that includes a human operator. Recently, it was shown
that a reward structure is not sufﬁcient to capture temporal logic
constraints in general [17]. We design the autonomy protocol
such that the resulting robot behavior satisﬁes probabilistic
temporal logic speciﬁcations. Such veriﬁcation problems have
been extensively studied for MDPs [2] and mature tools exist
for efﬁcient veriﬁcation [22], [7].

In what follows, we call a formal

interpretation of a
sequence of the human’s commands the human strategy, and
the sequence of commands issued by the autonomy protocol
the autonomy strategy. In [18], we formulated the problem of
designing the autonomy protocol as a nonlinear programming
problem. However, solving nonlinear programs is generally
intractable [3]. Therefore, we proposed a greedy algorithm
iteratively repairs the human strategy such that
that
the
speciﬁcations are satisﬁed without guaranteeing optimality,
based on [26]. Here, we propose an alternative approach for
the blending of the two strategies. We follow the approach of
repairing the strategy of the human to compute an autonomy
protocol. We ensure that the resulting robot behavior induced
by the repaired strategy deviates minimally from the human
strategy, and satisﬁes safety and performance properties given
in temporal logic speciﬁcations. We formally deﬁne the problem
as a quasiconvex optimization problem, which can be solved
efﬁciently by checking feasibility of a number of convex

optimization problems [4].

The question remains how to obtain the human strategy in
the ﬁrst place. It may be unrealistic that a human can provide
the strategy for an MDP that models a realistic scenario. To this
end, we create a virtual simulation environment that captures
the behavior of the MDP. We ask humans to participate in two
case studies to collect data about typical human behavior. We
use inverse reinforcement learning to get a formal interpretation
as a strategy based on human’s inputs [1], [30]. We model
a typical shared control scenario based on an autonomous
wheelchair navigation [15] in our ﬁrst case study. In our second
case study, we consider an unmanned aerial vehicle mission
planning scenario, where the human operator is to patrol certain
regions while keeping away from enemy aerial vehicles.

In summary, the main contribution this paper is to efﬁciently
synthesize an autonomy protocol such that the resulting blended
or repaired strategy meets all given speciﬁcations while only
minimally deviating from the human strategy. We present a
new technique based on quasiconvex programming, which can
be solved efﬁciently using convex optimization [4].

Organization. We introduce all formal foundations that we
need in Section II. We provide an overview of the general
shared control concept in Section III. We present the shared
control synthesis problem and provide a solution based on
convex optimization in Section IV. We indicate the applicability
and scalability of our approach on experiments in Section V and
draw a conclusion and critique of our approach in Section VI.

II. PRELIMINARIES

In this section, we introduce the required formal models and
speciﬁcations that we use to synthesize the autonomy protocol,
and we give a short example illustrating the main concepts.

A. Markov Decision Processes

A probability distribution over a ﬁnite set X is a function
x∈X µ(x) = µ(X) = 1. The set

µ : X → [0, 1] ⊆ R with (cid:80)
X of all distributions is Distr (X).

Deﬁnition 1 (MDP). A Markov decision process (MDP) M =
(S, sI , Act, P, AP, L) has a ﬁnite set S of states, an initial
state sI ∈ S, a ﬁnite set Act of actions, a transition probability
function P : S × Act → Distr (S), a ﬁnite set AP of atomic
propositions, and a labeling function L : S → 2AP that labels
each state s ∈ S with a subset of atomic propositions L(s) ⊆
AP. We extend L to a sequence of states by L(s0s1 . . .) =
L(s0)L(s1) . . . for s0, s1 ∈ S.

MDPs have nondeterministic choices of actions at
the
states; the successors are determined probabilistically via the
associated probability distribution. We assume that the MDP
contains no deadlock states, that is, at every state at least
one action is available. A cost function C : S × Act → R≥0
associates a cost to state-action pairs. If there is only a single
action available at each state, the MDP reduces to a discrete-
time Markov chain (MC). We use strategies to resolve the
choices of actions in order to deﬁne a probability and expected
cost measure for MDPs.

Deﬁnition 2 (Strategy). A memoryless and randomized strategy
for an MDP M is a function σ : S → Distr (Act). The set of
all strategies over M is Str M.

A special case are deterministic strategies which are functions
of the form σ : S → Act with σ(s) ∈ Act(s). Resolving all
the nondeterminism for an MDP M with a strategy σ ∈ Str M
yields an induced Markov chain Mσ.

b

a

s0

1

s3

1

s4

1

s3

1

s4

0.6

0.4

0.6

0.4

0.5

0.5

0.4

d

c

0.6

s1
(a) MDP M

1

0.4

0.6

s2

1

0.5

s2
s1
s0
(b) Induced MC Mσunif

0.5

Deﬁnition 3
(S, sI , Act, P, AP, L) and strategy σ ∈ Str M,
induced by M and σ is Mσ = (S, sI , Act, P σ), where

(Induced MC). For an MDP M =
the MC

P σ(s, s(cid:48)) =

(cid:88)

σ(s, α) · P(s, α, s(cid:48)) for all s, s(cid:48) ∈ S.

α∈Act(s)

In following, we assume that for a given MDP M and
for any state s ∈ S, there exists a strategy σ that induces a
MC Mσ that ensures state s is reachable under that strategy.
Note that this is a standard assumption for MDPs, and we can
remove the unreachable states by doing a graph search over
the MDP M as a preprocessing step [2].

A ﬁnite or inﬁnite sequence (cid:37)σ = s0s1s2 . . . of states
generated in M under a strategy σ ∈ Str M is called a path.
Given an induced MC Mσ, starting from the initial state s0,
the state visited at step t is given by a random variable Xt. The
probability of reaching state s(cid:48) from state s in one step, denoted
P(Xt+1 = s(cid:48)|Xt = s) is equal to P σ(s, s(cid:48)). We can extend one-
step reachability over a set of paths (cid:37)σ, i.e., P(s0s1s2 . . . sn) =
P(Xn = sn|Xn−1 = sn−1) · P(s0s1s2 . . . sn−1). We denote
the set of all paths in M under the strategy σ by P athσ(M).

Deﬁnition 4 (Occupancy Measure). The occupancy measure
xσ of a strategy σ for an MDP M is deﬁned as

xσ(s, α) = E

(cid:34) ∞
(cid:88)

t=0

(cid:35)

P (αt = α|st = s)

,

(1)

where st and αt denote the state and action in M at time step
t. The occupancy measure xσ(s, α) is the expected number of
times to take action α at state s under the strategy σ.

In our solution approach, we use the occupancy measure of

a strategy to compute an autonomy protocol.

B. Speciﬁcations

We use linear temporal logic (LTL) to specify a set of
tasks [2]. A speciﬁcation in LTL is built from a set AP
of atomic propositions, true, false and the Boolean and
temporal connectives ∧, ∨, ¬, ⇒, ⇔, and (cid:3) (always), U (until),
♦ (eventually), and (cid:13) (next). An inﬁnite sequence of subsets of
AP deﬁnes an inﬁnite word, and an LTL speciﬁcation is inter-
preted over inﬁnite words on 2AP . If a word w = w0w1w2 . . .
satisﬁes an LTL speciﬁcation ϕ, we denote it by w |= ϕ.

Deﬁnition 5. (DRA) A deterministic Rabin automaton (DRA)
is a tuple A = (Q, qI , Σ, µ, Acc, L), with a ﬁnite set Q of
states, an initial state qI ∈ Q, the alphabet Σ, the transition
relation µ : Q × Σ → Q between states of a DRA, and the set
of accepting state pairs Acc ⊆ 2Q × 2Q.

Fig. 2. MDP M with target state s2 and induced MC for strategy σunif

A run of a DRA A, denoted by γ = q0q1q2 . . ., is an inﬁnite
sequence of states. For each i ≥ 0, qi+1 = µ(si, vi) for some
vi ∈ Σ. A run γ is accepting if there exists a pair (A, B) ∈ Acc
and n ≥ 0 such that, for all m ≥ n, we have qm /∈ A and
there exists inﬁnitely many k that satisﬁes qk ∈ B. Given an
LTL speciﬁcation ϕ with atomic propositions AP, a DRA Aϕ
can be constructed with alphabet 2AP that accepts all words
that satisfy the LTL speciﬁcation ϕ [2].

For an induced DTMC Mσ of an MDP and a strategy σ, a
path (cid:37)σ = s0s1 . . . generates a word w = w0w1 . . . where wk
= L(sk) for all k ≥ 0. We denote the word that is generated by
(cid:37)σ as L((cid:37)σ). For an LTL speciﬁcation ϕ, the set of words that
is accepted by the DRA and satisﬁes the LTL speciﬁcation ϕ is
given by {(cid:37)σ ∈ P athσ(M) : L((cid:37)σ) |= ϕ} , and is measurable
[2]. We deﬁne

PMσ (sI |= ϕ) = PMσ {(cid:37)σ ∈ P athσ(M) : L((cid:37)σ) |= ϕ}

as the probability of satisfying the LTL speciﬁcation ϕ for an
MDP M under the strategy σ ∈ Str M.

The synthesis problem is to ﬁnd one particular strategy σ
for an MDP M such that given an LTL speciﬁcation ϕ and a
threshold β ∈ [0, 1], the induced DTMC Mσ satisﬁes

PMσ (sI |= ϕ) ≥ β,

(2)

which implies that the strategy σ satisﬁes the speciﬁcation ϕ
with at least a probability of β.

We also consider expected cost properties ϕc = E≤κ(♦G),
that restricts the expected cost to reach the set G ⊆ S of goal
states by an upper bound κ ∈ Q.

Example 1. Fig. 2(a) depicts an MDP M with initial state
s0. In state s0, the available actions are a and b. Similarly
for state s1, the two available actions are c and d. If action a
is selected in state s0, the agent transitions to s1 and s3 with
probabilities 0.6 and 0.4. For states s2, s3 and s4 we omit
actions, because of the self loops.

For a safety speciﬁcation φ = P≥0.21(♦s2), the deterministic
strategy σ1 ∈ Str M with σ1(s0, a) = 1 and σ1(s1, c) = 1
induces a probability of 0.36 to reach s2. Therefore, the speciﬁ-
cation is not satisﬁed, see the induced MC in Fig. 2(b). Likewise,
the randomized strategy σunif ∈ Str M with σunif(s0, a) =
σunif(s0, b) = 0.5 and σunif(s1, c) = σunif(s1, d) = 0.5
violates the speciﬁcation, as the probability of reaching s2
is 0.25. However, the deterministic strategy σsafe ∈ Str M with

σsafe(s0, b) = 1 and σsafe(s1, d) = 1 induces a probability of
0.16, thus σsafe |= φ.

Human
strategy

Speciﬁcations
ϕ1, ϕ2, . . . , ϕn

Formal
model M

Blending
function b

C. Strategy synthesis in an MDP

Given an MDP, and an LTL speciﬁcation ϕ, we aim to
synthesize a strategy that satisﬁes ϕ, or equivalently, a strategy
that satisﬁes the condition in (2).

Deﬁnition 6. (Product MDP) Let M = (S, sI , Act, P, AP, L)
be an MDP and A = (Q, qI , Σ, µ, Acc, L) be a DRA. The
product MDP is a tuple Mp = (Sp, sIp , Act, Pp, Accp)
with a ﬁnite set Sp = S × Q of states, an initial state
sIp = (sI , q) ∈ Sp that satisﬁes q = δ(qI , L(sI )), a
ﬁnite set Act of actions, a transition probability function
Pp((s, q), a, (s(cid:48), q(cid:48))) = P(s, a, s(cid:48)) if
q(cid:48) = δ(q, L(s(cid:48))), and
(cid:54)= δ(q, L(s(cid:48))), a labeling
Pp((s, q), a, (s(cid:48), q(cid:48))) = 0 if
function Lp((s, q)) = {q}, and the acceptance condition
k, Bp
Accp = {(Ap
i = S × Ai and
Bp
i = S × Bi for all (Ai, Bi) ∈ Acc and i = 1, . . . , k.

k)} where Ap

1 ), . . . , (Ap

1, Bp

q(cid:48)

Deﬁnition 7. (AEC) The end component for the product MDP
Mp is given by a pair (C, D), where a non-empty set C ⊆ Sp
of states and a function D : C → Act is deﬁned such that for
any s ∈ C we have

Pp(s, D(s), s(cid:48)) = 1,

(cid:88)

s(cid:48)∈C

i = ∅ and C ∩ Bp

Given a product MDP Mp, we modify it to M(cid:48)

and the induced directed graph is strongly connected. An
accepting end component (AEC) is an end component that
satisﬁes C ∩ Ap
i (cid:54)= ∅ for some i ∈ 1, . . . , k.
p by making
all states in the end components absorbing, i.e., for all states
s ∈ C, Pp(s, α, s) = 1 for all α ∈ Act in the modiﬁed MDP
M(cid:48)
p. Making all end components absorbing is commonly used
in tools for model checking of LTL speciﬁcations in MDPs [2],
[22], [7]. We further assume that all states in the end component
are absorbing. The modiﬁcation does not change the probability
of satisfying an LTL speciﬁcation as stated below.

Lemma 1. (From [6]) In each end component of an MDP,
there exists a strategy in each state s ∈ C that reaches any
other state s(cid:48) ∈ C with a probability of 1.

A memoryless and randomized strategy for a product MDP
Mp is a function σp : Sp → Distr (Act). A memoryless
strategy σp is a ﬁnite-memory strategy σ(cid:48) in the underlying
MDP M. Given a state (s, q) ∈ Sp, we consider q to be
a memory state and deﬁne σ(cid:48)(γ) = σp(s, q), where the run
γ = q0q1q2 . . . qn satisﬁes qn = q and Σ(qI , L(ρ)) = s. For
the MDPs given in Deﬁnition 1 and LTL speciﬁcations, memo-
ryless strategy in the product MDP Mp are sufﬁcient to achieve
the maximal probability of satisfying the speciﬁcation [2].

Some states in the product MDP Mp may be unreachable
from the initial state sIp . These states do not affect the strategy
synthesis in Mp, and can be removed from Mp. We assume
that there is no unreachable states in the product MDP Mp.
Let σp ∈ Str Mp be a strategy for Mp and let σ(cid:48) ∈ σM be
the strategy on M constructed from σp through the procedure

Human
Strategy

command

Blended
Strategy

blended command

Robot
execution

Autonomy
Strategy

command

Fig. 3. Shared control architecture.

explained above. The paths of the MDP M under the strategy
σ(cid:48) satisfy the LTL speciﬁcation ϕ with a probability of at
least β, i.e., PMσ (sI |= ϕ) ≥ β, if and only if the paths of
the induced DTMC Mσ
p from the product MDP Mp under
the strategy σp reach and stay in some AECs in Mp with a
probability of at least β [2].

III. CONCEPTUAL DESCRIPTION OF SHARED CONTROL

We now detail the general shared control concept adopted in
this paper and state the formal problem. Consider the setting
in Fig. 3. As inputs, we have a set of task speciﬁcations, a
model M for the robot behavior, and a blending function b.
The given robot task is described by certain performance and
safety speciﬁcations ϕ = ϕ1 ∧ ϕ2 . . . ∧ ϕn. For example, it may
not be safe to take the shortest route because there may be too
many obstacles in that route. In order to satisfy performance
considerations, the robot should prefer to take the shortest
route possible while not violating the safety speciﬁcations. We
model the behavior of the robot inside a stochastic environment
as an MDP M.

In our setting, a human issues a set of commands for the
robot to execute. It may be unrealistic that a human can
grasp an MDP that models a realistic shared control scenario.
Indeed, a human will likely have difﬁculties interpreting a
large number of possibilities and the associated probability
of paths and payoffs [13], and it may be impractical for
the human to provide the human strategy to the autonomy
protocol, due to the possibly large state space of the MDP.
Therefore, we compute a human strategy σh as an abstraction
of a sequence of human’s commands, which we obtain using
inverse reinforcement learning [1], [30].

We design an autonomy protocol that provides another
strategy σa, which we call the autonomy strategy. Then, we
blend the two strategies according to the blending function b
into the blended strategy σha. The blending function reﬂects
preference over the human strategy or the autonomy strategy.
We ensure that the blended strategy deviates minimally from
the human strategy.

At runtime, we can then blend commands of the human
with commands of the autonomy strategy. The resulting
“blended” commands will induce the same behavior as the

blended strategy σha, and the speciﬁcations are satisﬁed. Note
that blending commands at runtime according to predeﬁned
blending function and autonomy protocol simply requires a
linear combination of real values and is thus very efﬁcient.

The shared control synthesis problem is then the synthesis of
the repaired strategy σha such that it holds that σha |= ϕ while
deviating minimally from σh. The deviation between the human
strategy σh and the repaired strategy σha is measured by the
maximal difference between the two strategies in each state of
the MDP. We state the problem that we study as follows.

Problem 1. Let M be an MDP, ϕ be an LTL speciﬁcation, σh
be a human strategy, and β be a constant. Synthesize a repaired
strategy σha ∈ Str M that solves the following problem.

minimize
σha∈StrM
subject to

max
s∈S,α∈Act

|σh(s, α) − σha(s, α)|

(3)

PMσha (sI |= ϕ) ≥ β.

(4)

For convenience, we will use the original MDP M instead
of the product MDP Mp in what follows as all concepts are
directly transferrable.

IV. SYNTHESIS OF THE AUTONOMY PROTOCOL

In this section, we describe our approach to synthesize an
autonomy protocol for the shared control synthesis problem.
We start by formalizing the concepts of strategy blending
and strategy repair. We then show how we can synthesize
a repaired strategy that deviates minimally from the human
strategy based on quasiconvex programming. We discuss how
we can include additional speciﬁcations to the problem and
discuss other measures for the human and the repaired strategy
that induce a similar behavior.

A. Strategy blending
Given the human strategy σh ∈ Str M and the autonomy
strategy σa ∈ Str M, a blending function computes a weighted
composition of the two strategies by favoring one or the other
strategy in each state of the MDP [19], [8], [9].

Reference [9] argues that the weight of blending shows the
conﬁdence in how well the autonomy protocol can assist to
perform the human’s task. Put differently, the blending function
should assign a low conﬁdence to the actions of the human
if they may lead to a violation of the speciﬁcations. Recall
Fig. 1 and the example in the introduction. In the cells of the
gridworld where some actions may result in a collusion with
the vacuum cleaner with a high probability, it makes sense to
assign a higher weight to the autonomy strategy.

We pick the blending function as a state-dependent function
that weighs the conﬁdence in both the human strategy and the
autonomy strategy at each state of the MDP M [19], [8], [9].

Deﬁnition 8 (Linear blending). Given the MDP M =
(S, sI , Act, P, AP, L), two memoryless strategies σh, σa ∈
Str M, and a blending function b : S → [0, 1], the blended

strategy σha ∈ Str M for all states s ∈ S, and actions α ∈ Act
is

σha(s, α) = b(s) · σh(s, α) + (1 − b(s)) · σa(s, α).

(5)

For each s ∈ S, the value of b(s) represents the “weight” of
σh at s, meaning how much emphasis the blending function
puts on the human strategy at state s. For example, referring
back to Fig. 1, the critical cells of the gridworld correspond to
certain states of the MDP M. At these states, we may assign
a very low conﬁdence in the human strategy. For instance at
such a state s ∈ S, we might have b(s) = 0.1, meaning the
blended strategy in state s puts more emphasis on the autonomy
strategy σa.

B. Solution to the shared control synthesis problem

In this section, we propose an algorithm for solving the shared
control synthesis problem. Our solution is based on quasiconvex
programming which can be solved by checking feasibility of
a number of convex optimization problems. We show that the
result of the quasiconvex program is the repaired strategy as in
Problem 1. The strategy satisﬁes the task speciﬁcations while
deviating minimally from the human strategy. We use that
result to compute the autonomy strategy σa that may then be
subject to blending.

1) Perturbation of strategies: As mentioned in the introduc-
tion, the blended strategy should deviate minimally from the
human strategy. To measure the quantity of such a deviation,
we introduce the concept of perturbation, which was used
in [5]. To modify a (randomized) strategy, we employ additive
perturbation by increasing or decreasing the probabilities of
action choices in each state. We also ensure that for each state,
the resulting strategy is a well-deﬁned distribution over the
actions.

Deﬁnition 9 (Strategy perturbation). Given the MDP M and a
strategy σ ∈ Str M, a perturbation δ is a function δ : S×Act →
[−1, 1] with

(cid:88)

α∈Act

δ(s, α) = 0 ∀s ∈ S.

The perturbation value at state s for action α is δ(s, α).
Overloading the notation, the perturbed strategy δ(σ) is

δ(σ)(s, α) = σ(s, α) + δ(s, α) ∀s ∈ S, α ∈ Act.

(6)

2) Dual linear programming formulation for MDPs: In this
section, we recall the LP formulation to compute a strategy
that maximizes the probability of satisfying a speciﬁcation ϕ
in an MDP [27], [12]. Let B the set of states in accepting
end components in M (or in fact within in the product MDP
Mp) and let Sr be the set of all states that are not in B and
have nonzero probability of reaching a state s ∈ B. These
sets can be computed in time polynomial in the size of M by
doing a graph search over the MDP M [2]. In this section,
we assume that there exists a strategy σ ∈ Str M that satisﬁes
an LTL formula with a probability of at least β, which can be
veriﬁed in time polynomial by solving a linear programming
problem [2].

The variables of the dual LP formulation are following:
• xσha(s, α) ∈ [0, ∞) for each state s ∈ Sr and action
α ∈ Act deﬁnes the occupancy measure of a state-action
pair for the strategy σha, i.e., the expected number of
times of taking action α in state s.

• xσha(s) ∈ [0, 1] for each state s ∈ B deﬁnes the
probability of reaching a state s ∈ B in an accepting
end component.

maximize

(cid:88)

s∈B

xσha (s)

(7)

subject to
∀s ∈ Sr.
(cid:88)

xσha(s, α) =

(cid:88)

(cid:88)

P(s(cid:48), α, s)xσha(s(cid:48), α) + αs

α∈Act

s(cid:48)∈Sr

α∈Act

∀s ∈ B.

xσha(s) =

(cid:88)

(cid:88)

P(s(cid:48), α, s)xσha(s(cid:48), α) + αs

α∈Act

s(cid:48)∈Sr
xσha(s) ≥ β

(cid:88)

s∈B

(8)

(9)

(10)

where αs = 1 if s = sI and αs = 0 if s (cid:54)= sI . The
constraints (8) and (9) ensure that the expected number of
times transitioning to a state s ∈ S is equal to the expected
number of times to take action α that transitions to a different
state s(cid:48) ∈ S. The constraint (10) ensures that the speciﬁcation
ϕ is satisﬁed with a probability of at least β. We refer the
reader to [27], [12] for details about the constraints in the LP.

For any optimal solution x to the LP in (7)–(10),

• xσha(s, α) ∈ [0, ∞) for each state s ∈ Sr and action
α ∈ Act and xσha(s) ∈ [0, 1] for each state s ∈ B as
deﬁned for the optimization problem in (7)–(10).

• ˆδ ∈ [0, 1] gives the maximal deviation between the human

strategy σh and the repaired strategy σha.

minimize

ˆδ

subject to
∀s ∈ Sr.
(cid:88)

xσha (s, α) =

(12)

(cid:88)

(cid:88)

P(s(cid:48), α, s)xσha (s(cid:48), α) + αs

α∈Act

s(cid:48)∈Sr

α∈Act

∀s ∈ B.

xσha(s) =

(cid:88)

(cid:88)

P(s(cid:48), α, s)xσha(s(cid:48), α) + αs

(cid:88)

α∈Act

s(cid:48)∈Sr
xσha (s) ≥ β

(13)

(14)

(15)

s∈B
∀s ∈ Sr.

|xha(s, α) −

(cid:88)

α∈Act

xha(s, α)σh(s, α)| ≤ ˆδ

(cid:88)

α∈Act

xha(s, α).

(16)
Proof. For any solution to the optimization problem above,
the constraints in (13)–(15) ensure that the strategy computed
by (11) satisﬁes the speciﬁcation. We now show that by
minimizing ˆδ, we minimize the maximal deviation between
the human strategy and the repaired strategy.

As in Deﬁnition 9, we perturb the human strategy σh to the

repaired strategy σha by

∀s ∈ Sr.α ∈ Act. σha(s, α) = σh(s, α) + δ(s, α).

(17)

σha(s, α) =

xσha(s, α)

xσha(s, α)

(cid:88)

α∈Act

(11)

Note that this constraint is not a function of the occupancy
measure of σha. By using the deﬁnition of occupancy measure
in (11), we reformulate the constraint in (17) into the constraint

is an optimal strategy, and xσha is the occupancy measure of
σha, see [27] and [12] for details. After ﬁnding an optimal
solution to the LP in (7)–(10), we can compute the probability
of satisfying a speciﬁcation by

∀s ∈ Sr.α ∈ Act.

xha(s, α)

(cid:88)

xha(s, α)

= σh(s, α) + δ(s, α)

α∈Act

(18)

(cid:88)

s∈B

xσha (s).

3) Strategy repair using quasiconvex programming: Given
the human strategy, σh ∈ Str M, the aim of the autonomy
protocol is to compute the blended strategy, or the repaired
strategy σha that induces a similar behavior to the human
strategy while satisfying the speciﬁcations. We compute the
repaired strategy by perturbing the human strategy, which is
introduced in Deﬁnition 9. We show our formulation to compute
the repaired strategy in the following Lemma.

Lemma 2. The shared control synthesis problem can be
formulated as the following nonlinear programming program
with following variables:

or equivalently to the constraint

∀s ∈ Sr.α ∈ Act.

xha(s, α) =

(cid:88)

α∈Act

xha(s, α) (σh(s, α) + δ(s, α)) .

(19)

Since we are interested in minimizing the maximal deviation,
we assign a common variable ˆδ ∈ [0, 1] for all state-action
pairs in the MDP M to put an upper bound on the deviation
by

∀s ∈ Sr.α ∈ Act.
(cid:88)

|xha(s, α) −

α∈Act

xha(s, α)σh(s, α)| ≤ ˆδ

(cid:88)

α∈Act

xha(s, α).

(20)

Therefore, by minimizing ˆδ subject to the constraints in (13)–
(16) we ensure that the repaired strategy σha deviates minimally
from the human strategy σh.

The constraint in (20) is a nonlinear constraint. In fact,
it is a quadratic constraint due to multiplication of ˆδ and
xha. However, we show that the problem in (12)–(16) is
a quasiconvex programming problem, which can be solved
efﬁciently using bisection over ˆδ [4].

Lemma 3. The constraint in (20) is quasiconvex, therefore the
nonlinear programming problem in (12)–(16) is a quasiconvex
programming problem.
Proof. For a ﬁxed ˆδ, the set described by the inequality in (20)
is convex, that is, the sublevel sets of the function are convex [4,
Section 3.4]. Therefore, the constraint in (20) is quasiconvex
and the nonlinear programming problem in (12)–(16) is a
quasiconvex programming problem (QCP).

We solve the QCP in (12)–(16) by employing bisection over
the variable ˆδ. We initialize a lower and upper bound of the
maximal deviation between the human strategy and the repaired
strategy to 0 and 1 respectively. Then, we iteratively reﬁne the
bounds by solving a number of convex feasibility problems. A
method to solve quasiconvex optimization problems is given
in [4, Algorithm 4.1]. Our approach is given in Algorithm 1
based on the Algorithm 4.1 in [4]. We now state the main
result of the paper.

Algorithm 1: Bisection method to synthesize an optimal
repaired strategy σha for the shared control synthesis
problem.
given M = (S, sI , Act, P, AP, L), σh, l = 0, u = 1,
tolerance (cid:15) > 0.
repeat

1. Set ˆδ = (l + u)/2.
2. Solve the convex feasibility problem in (13)–(16).
3. if the problem in (13)–(16) is feasible, then

u := ˆδ, σha(s, α) =

xσha(s, α)

(cid:88)

xσha(s, α)

α∈Act

else l := ˆδ.
until u − l ≤ (cid:15).

Theorem 1. The repaired strategy σha obtained from Algo-
rithm 1 satisﬁes the task speciﬁcations and it deviates minimally
from the human strategy σh, and is an optimal solution to the
shared control synthesis problem.

Proof. From a satisfying assignment to the constraints in (12)–
(16), we compute a strategy that satisﬁes the speciﬁcation
using (11). Using Algorithm 1, we can compute the repaired
strategy σha that deviates minimally from the human strategy
(cid:24)
log2(

σh up to (cid:15) accuracy in

iterations. Therefore,

(cid:25)
)

1
(cid:15)

Algorithm 1 computes an optimal strategy for the shared control
synthesis problem.

The strategy given by Algorithm 1 computes the minimally
deviating repaired strategy σha that satisﬁes the LTL speciﬁca-
tion. In [18], we considered computing an autonomy protocol
with a greedy approach. That approach requires solving possibly
an unbounded number of LPs to compute a feasible strategy that
is not necessarily optimal. On the other hand, using Algorithm 1,
we only need to check feasibility of a number of LPs that
can be determined to compute an optimal strategy. Note that
we do not compute the autonomy strategy σa with the QCP
in (12)–(16) directly. After computing the repaired strategy
σha, we compute the autonomy strategy σa according to the
Deﬁnition 8.

Computationally, the most expensive step of the Algorithm 1
is checking the feasibility of the optimization problem in (13)–
(16). The number of variables and constraints in the optimiza-
tion problem are linear in the number of states and actions in
M, therefore, checking feasibility of the optimization problem
can be done in time polynomial in the size of M with interior
(cid:25)
)

point methods [24]. Algorithm 1 terminates after

(cid:24)
log2(

1
(cid:15)

iterations, therefore we can compute an optimal strategy up to
(cid:15) accuracy in time polynomial in the size of M.

4) Additional speciﬁcations: The QCP in (12)–(16) com-
putes an optimal strategy for a single LTL speciﬁcation
ϕ. Suppose that we are given a reachability speciﬁcation
ϕr = P≥λ(♦T ) with T ∈ S in addition to the LTL speciﬁcation
ϕ. We can handle this speciﬁcation by appending the constraint

(cid:88)

s∈B

xσha(s) ≥ λ

(21)

to the QCP in (12)–(16). The constraint in (21) ensures that
the probability of reaching T is greater than λ.

We handle an expected cost speciﬁcation E≤κ(♦G) for G ⊆

S, by adding the constraint

(cid:88)

(cid:88)

s∈Sr\G

α∈Act

C(s, α)xσha(s, α) ≤ κ

(22)

to the QCP in (12)–(16). The constraint in (21) ensures that
the expected cost of reaching G is less than κ.

5) Additional measures: We discuss additional measures
that can be used to render the behavior between the human
and the autonomy protocol similar based on the occupancy
measure of a strategy. Instead of minimizing the maximal
deviation between the human strategy and the repaired strategy,
we can also minimize the maximal difference of occupancy
measures of the strategies. In this case, the difference between
the human strategy and the repaired strategy will be smaller in
states where the expected number of being in a state is higher,
and will be higher if the state is not visited frequently. We can
minimize the maximal difference of occupancy measures by
adding the following objective to the constraints in (13)–(15):

minimize

max
s∈S,α∈Act

|xσha(s, α) − xσh (s, α)|

humans to send commands to one or multiple vehicles at run
time. It includes three main programs: a simulator, a data
playback tool, and a scenario setup tool.

We use the model checker PRISM [22] to verify if the
computed strategies satisfy the speciﬁcation. We use the LP
solver Gurobi [16] to check the feasibility of the LP problems
that is given in Section IV. We also implemented the greedy
approach for strategy repair in [18]. In this section, we refer
to the procedure given by Algorithm 1 as QCP method, and
the procedure from [18] as greedy method.

B. Data collection

We asked ﬁve participants to accomplish tasks in the
wheelchair scenario. The goal is moving the wheelchair to a
target cell in the gridworld while never occupying the same cell
as the moving obstacle. Similarly, three participants performed
the surveillance task in the AMASE environment.

From the data obtained from each participant, we compute
an individual randomized human strategy σh via MEIRL.
Reference [19] uses inverse reinforcement learning to reason
about the human’s commands in a shared control scenario from
human’s demonstrations. However, they lack formal guarantees
on the robot’s execution.

In our setting, we denote each sample as one particular
command of the participant, and we assume that the participant
issues the command to satisfy the speciﬁcation. Under this
assumption, we can bound the probability of a possible
deviation from the actual intent with respect to the number of
samples using Hoeffding’s inequality for the resulting strategy,
see [29] for details. Using these bounds, we can determine
the required number of commands to get an approximation
of a typical human behavior. The probability of a possible
deviation from the human behavior is given by O(exp(−nγ2)),
where n is the number of commands from the human and γ
is the upper bound on the deviation between the probability
of satisfying the speciﬁcation with the true human strategy
and the probability obtained by the strategy that is computed
by inverse reinforcement learning. For example, to ensure an
upper bound γ = 0.05 on the deviation of the probability
of satisfying the speciﬁcation with a probability of 0.99, we
require 1060 demonstrations from the human.

We design the blending function by assigning a low weight
to the human strategy at states where it yields a low probability
of reaching the target set. Using this function, we create the
autonomy strategy σa and pass it (together with the blending
function) back to the environment. Note that the repaired
strategy σha satisﬁes the speciﬁcation, by Theorem 1.

C. Gridworld

The size of the gridworld in Fig. 1 is variable, and we
generate a number of randomly moving (e.g., the vacuum
cleaner) and stationary obstacles. An agent (e.g., the wheelchair)
moves in the gridworld according to the commands from a
human. For the gridworld scenario, we construct an MDP
where the states represent the positions of the agent and the
obstacles and the actions induce changes in the agent position.

Fig. 4. The setting of the case study for the shared control simulation. We
collect sample data from a simulation environment, and compute the human
strategy using maximum-entropy inverse reinforcement learning (MEIRL).
From the human strategy, we compute an autonomous strategy based on our
approach to the shared control synthesis problem.

or, equivalently,

minimize

||xσha − xσh ||∞.

The occupancy measure of the human strategy can be computed
by ﬁnding a feasible solution to the constraints in (13)–(14)
for the induced DTMC Mσha. We can also minimize further
convex norms of the human strategy and the repaired strategy,
such as 1-norm or 2-norm.

V. CASE STUDY AND EXPERIMENTS

We present two numerical examples that illustrate the efﬁcacy
of the proposed approach. In the ﬁrst example, we consider a
wheelchair scenario from Fig. 1. The goal in this scenario is to
reach the target state while not crashing with the obstacle. In
the second example, we consider an unmanned aerial vehicle
(UAV) mission, where the objective is to survey certain regions
while avoiding enemy agents.

We require an abstract representation of the human’s com-
mands as a strategy to use our synthesis approach in a shared
control scenario. We ﬁrst discuss how such strategies may be
obtained using inverse reinforcement learning and report on
case study results.

A. Experimental setting

We give an overview of the workﬂow of the experiments in
Fig. 4. In an simulation environment, we collect sample data
from the human’s commands. Based on these commands, we
compute a human strategy σh using maximum-entropy inverse
reinforcement learning (MEIRL) [30]. After computing the
human strategy, we synthesize the repaired strategy σha using
the procedure in Algorithm 1. After synthesizing the repaired
strategy, we compute the autonomous strategy using (5). We
can further reﬁne our representation of the human strategy
by collecting more sample data from the human’s commands
before blending with the autonomous strategy.

We model the wheelchair scenario inside an interactive
Python environment. In the second scenario, we use the
UAV simulation environment AMASE1, developed at Air Force
Research Laboratory. AMASE can be used to simulate multi-
UAV missions. The graphical user interfaces of AMASE allow

1https://github.com/afrl-rq/OpenAMASE

(a) Strategy σh

(b) Strategy σah

(c) Strategy σa

Fig. 5. Graphical representation of the obtained human, blended, and autonomy
strategy in the grid.

The safety speciﬁcation states that the agent has to reach a
target cell while not crashing into an obstacle with a certain
probability β ∈ [0, 1], formally P≥β(¬crash U target).
First, we report results for one particular participant in a
gridworld scenario with a 8 × 8 grid and one moving obstacle.
The states of the MDP are generated by the Cartesian product
of the states of the agent and the obstacle. The agent and the
obstacle have four actions in all states, namely left, right, up
and down. At each state, a transition to the chosen direction
occurs with a probability of 0.7, and the agent transitions to
each adjacent state in the chosen direction with a probability
0.15. If a transition to the wall occurs, the agent remains in
the same state. We ﬁx a particular strategy for the obstacle,
and determine the transition probabilities between states as a
product of transitioning to the next states for the agent and
the obstacle. The resulting MDP has 2304 states and 36864
transitions. We compute the human strategy using MEIRL
where the features are the components of the cost function of
the human, for instance the distance to the obstacle and the
goal state.

We instantiate the safety speciﬁcation with β = 0.7, which
means the target should be reached with at least a probability
of 0.7. The human strategy σh induces a probability of 0.546
to satisfy the speciﬁcation. That is, it does not satisfy the
speciﬁcation.

We compute the repaired strategy σha using the greedy and
the QCP approach, and both strategies satisfy the speciﬁcation
with a probability larger than β. On the one hand, the maximum
deviation between σh and σha is 0.15 with the greedy approach,
which implies that the strategy of the human and the autonomy
protocol deviates at most 15% for all states and actions. On
the other hand, the maximum deviation between σh and σha
is 0.03 with the QCP approach. The results show that the QCP
approach computes a repaired strategy that induces a more
similar strategy to the human strategy compared to the LP
approach.

We give a graphical representation of the human strategy
σh, repaired strategy σha, and the autonomy strategy σa in
Fig. 5. For each strategy, we indicate the average probability
of safely reaching the target with the QCP approach. Note that
the probability of reaching the target depends on the current
position of the obstacle. Therefore, the probability for satisfying
a speciﬁcation could be higher or lower than shown in Fig. 5.
In Fig. 5, the probability of reaching the target increases with

(a) Snapshot of a simulation using the
AMASE simulator. The objective of
the agent is to keep surveilling the
green regions while avoiding enemy
agents and restricted operating zones.

(b) The graphical user interface of
the AMASE simulator for a UAV
mission. The user interface contains
various information about
the ve-
hicles such as the speed and the
heading.

Fig. 6. An example of UAV mission that is simulated on AMASE.

a darker color, and black indicates a probability of 1 to reach
the target. We observe that the human strategy induces a lower
probability of reaching the target in most of the states, while
for the repaired strategy, the probability of reaching target is
higher in all cells. Note that the autonomy strategy induces a
very high probability of reaching the target in each cell, but
the autonomy strategy may be too safe and may not be similar
to the human strategy.

To ﬁnally assess the scalability of our approach, consider
Table I. We generated MDPs for different gridworlds with a
different number of states and number of obstacles. We list the
number of states in the MDP and the number of transitions.
We report on the time that the synthesis process took with the
greedy approach and QCP approach, which includes the time
of solving the LPs in the greedy method or QCPs measured
in seconds. It also includes the model checking times using
PRISM for the greedy approach. To represent the optimality
of the synthesis, we list the maximal deviation between the
repaired strategy and the human strategy for the greedy and
QCP approach (labeled as ”δG” and ”δQCP”). In all of the
examples, we observe that the strategies obtained by the QCP
approach yield autonomy strategies with less deviation to the
human strategy while having similar computation time with
the greedy approach.

D. UAV mission planning

Similar to the gridworld scenario, we generate an MDP where
states denote the position of the agent and the enemy agents in
an AMASE scenario. Consider an example scenario in Fig. 6:
The speciﬁcation (or the mission) of the agent (blue UAV) is
to keep surveilling the green regions (labeled as w1, w2, w3)
while avoiding restricted operating zones (labeled as ”ROZ1,
ROZ2”) and enemy agents (purple and green UAVs). We asked
the participants to visit the regions in a sequence, i.e., visiting
the ﬁrst region, then second, and then the third region. After
visiting the third region, the task is to visit the ﬁrst region
again to perform the surveillance.

TABLE I
SCALABILITY RESULTS FOR THE GRIDWORLD EXAMPLE. WE LIST THE SYNTHESIS TIME OF THE BOTH APPROACHES IN SECONDS. ’δG’ AND δQCP’ REFER
TO THE MAXIMAL DEVIATION OF THE GREEDY AND QCP APPROACH.

Gridworld size

Number of states

Number of transitions

Synthesis time with the
greedy approach (sec)

δG

Synthesis time with the
QCP approach (sec)

8 × 8
10 × 10
12 × 12
20 × 20

2, 304
3, 600
14, 400
40, 000

36, 864
57, 600
230, 400
640, 000

14.12
23.80
250.78
913.23

0.145
0.231
0.339
0.373

31.49
44.61
452.27
1682.05

δQCP

0.031
0.042
0.050
0.048

TABLE II
RESULTS FOR DIFFERENT SPECIFICATION THRESHOLDS FOR THE
PROBABILITY AND EXPECTED TIME IN THE AMASE EXAMPLE. ’β’ AND ’κ’
REFER TO THE THRESHOLD FOR THE PROBABILITY AND THE EXPECTED
TIME OF THE SPECIFICATION.

TABLE III
RESULTS FOR DIFFERENT PERTURBATIONS OF THE HUMAN STRATEGY IN
THE AMASE EXAMPLE. ’δmax’ REFERS TO THE MAXIMAL PERTURBATION
INTRODUCED TO THE HUMAN STRATEGY. δap REFERS TO THE MAXIMAL
DEVIATION BETWEEN THE REPAIRED STRATEGY AND THE HUMAN
STRATEGY.

β

κ

Synthesis time with the
QCP approach (sec)

0.7
0.7
0.7
0.9
0.9
0.9

20
40
80
20
40
80

827.37
749.14
722.81
888.29
795.98
732.41

δQCP

0.380
0.126
0.054
0.598
0.163
0.100

β

δmax

δap

Synthesis time with the
QCP approach (sec)

0.7
0.7
0.7
0.9
0.9
0.9

0.1
0.2
0.5
0.1
0.2
0.5

0.170
0.274
0.506
0.270
0.345
0.534

725.93
718.14
696.60
732.54
745.05
798.01

δQCP

0.032
0.036
0.037
0.091
0.092
0.101

For example, if the last visited region is w3, then the
safety speciﬁcation in this scenario is P≥β((¬crash ∧
¬ROZ) U target), where ROZ is to visit the ROZ areas
and target is visiting w1.

We synthesize the autonomy protocol on the AMASE
scenario with two enemy agents. The underlying MDP has
15625 states. We use the same blending function and same
threshold β = 0.7 as in the gridworld example. The features
to compute the human strategy with MEIRL are given by the
distance to the closest ROZ, enemy agents, and the target
region.

The human strategy σh violates the speciﬁcation with a
probability of 0.496. Again, we compute the repaired strategy
σha with the greedy and the QCP approach. Both strategies
satisfy the speciﬁcation. On the one hand, the maximum
deviation between σh and σha is 0.418 with the greedy
approach, which means the strategies of the human and the
autonomy protocol are signiﬁcantly different in some states
of the MDP. On the other hand, the QCP approach yields
a repaired strategy σha that is more similar to the human
strategy σh with a maximum deviation of 0.038. The time of
the synthesis procedure with the LP approach is 481.31 seconds
and the computation time with the QCP approach is 749.18
seconds, showing the trade-offs between the greedy approach
and the QCP approach. We see that the greedy approach can
compute a feasible solution slightly faster, however the resulting
blended strategy may be less similar to the human strategy
compared to the QCP approach.

To assess the effect of changing the threshold of satisfying the
speciﬁcation, we use a different threshold β = 0.9. The greedy
approach did not terminate within one hour, and could not
ﬁnd a repaired strategy that satisﬁes the speciﬁcation after 45

iterations. We compute a repaired strategy σha using the QCP
approach with a maximum deviation of 0.093. The computation
time with the QCP approach is 779.81 seconds, showing that
the QCP approach does not take signiﬁcantly more time to
compute a repaired strategy even with a higher threshold. We
conclude that the greedy approach may not be able to ﬁnd a
feasible strategy efﬁciently if most of the strategies in an MDP
do not satisfy the speciﬁcation.

We also assess the effect of adding additional constraints
to the task, i.e., surveilling the next green region within a
certain time step. We synthesize different policies for different
expected times until the UAV reaches the next region. We
summarize the results in Table II. For each different probability
thresholds (labeled as ”β”) and expected times to complete the
mission (labeled as ”κ”), we report the synthesis time and the
maximal deviation. The results in Table II illustrate that the
maximal deviation δQCP increases with increasing threshold
and decreasing expected time to complete the mission. For
example, with the threshold β = 0.9 and expected time κ = 20,
the maximal deviation between the human and the repaired
strategy is 0.598, which shows that the strategies of the human
and the autonomy protocol can be signiﬁcantly different in
some states. On the other hand, with the threshold β = 0.7
and expected time κ = 80, the maximal deviation between
the human strategy and the repaired strategy is 0.054, which
is signiﬁcantly smaller than the previous examples. We also
note that there is no signiﬁcant difference in synthesis time
for different thresholds and expected times.

E. Effect of changing the human strategy

In this section, we investigate how changing the human
strategy changes the strategy of the autonomy protocol. We
perturb the human strategy from the previous example using (6)
with different perturbation functions δ. We use three different
values for maximal perturbation for every state and action
between the human strategy and the repaired strategy and two
different thresholds to satisfy the speciﬁcation with β = 0.7
and β = 0.9.

We summarize our results in Table III. We generated
three different perturbed human strategies with perturbation
functions that have a different maximal perturbation (labeled as
”δmax”). We report the maximal deviation between the autonomy
protocol that is synthesized using the original human strategy
and the perturbed human strategy (labeled as δap), the time
that the synthesis process took with the QCP approach (labeled
as ”QCP synth.”), and the maximal deviation between the
perturbed human strategies and the repaired strategies (labeled
as δQCP).

The results in Table III show that the maximal deviation
between the repaired strategy and the human strategy does not
depend on the perturbation, and it depends on the threshold of
satisfying the speciﬁcation. The maximal deviation between
the repaired strategies increases with larger perturbations
introduced to the human strategy and with a larger threshold β.
The values for δQCP show that the maximal deviation between
the human strategy and the repaired strategy does not depend
heavily on a speciﬁc human strategy, and it mostly depends on
the threshold. We also note that the synthesis time is similar
for all cases.

VI. CONCLUSION AND CRITIQUE

We introduced a formal approach to synthesize an autonomy
protocol in a shared control setting subject to probabilistic
temporal logic speciﬁcations. The proposed approach utilizes
inverse reinforcement learning to compute an abstraction of
a human’s behavior as a randomized strategy in a Markov
decision process. We designed an autonomy protocol such that
the resulting robot strategy satisﬁes safety and performance
speciﬁcations. We also ensured that the resulting robot be-
havior is as similar to the behavior induced by the human’s
commands as possible. We synthesized the robot behavior using
quasiconvex programming. We showed the practical usability
of our approach through case studies involving autonomous
wheelchair navigation and unmanned aerial vehicle planning.
There is a number of limitations and also possible extensions
of the proposed approach. First of all, we computed a globally
optimal strategy by bisection, which requires checking feasi-
bility of a number of linear programming problems. A direct
convex formulation of the shared control synthesis problem
would make computing the globally optimal strategy more
efﬁcient.

We assumed that the human’s commands are consistent
through the whole execution, i. e., the human issues each
command to satisfy the speciﬁcation. Also, this assumption
implies the human does not consider assistance from the robot

while providing commands - and in particular, the human does
not adapt the strategy to the assistance. It may be possible
to extend the approach to handle non-consistent commands
by utilizing additional side information, such as the task
speciﬁcations.

Finally, in order to generalize the proposed approach to
other task domains, it is worth to explore transfer learning [25]
techniques. Such techniques will allow us to handle different
scenarios without requiring to relearn the human strategy from
the human’s commands.

REFERENCES

[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse

reinforcement learning. In ICML, page 1. ACM, 2004.

[2] Christel Baier and Joost-Pieter Katoen. Principles of Model Checking.

The MIT Press, 2008.

[3] Mihir Bellare and Phillip Rogaway. The complexity of approximating
a nonlinear program. In Complexity in numerical optimization, pages
16–32. World Scientiﬁc, 1993.

[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cam-

bridge University Press, New York, NY, USA, 2004.

[5] Taolue Chen, Yuan Feng, David S. Rosenblum, and Guoxin Su.
Perturbation analysis in veriﬁcation of discrete-time Markov chains.
In CONCUR, volume 8704 of LNCS, pages 218–233. Springer, 2014.

[6] Luca De Alfaro. Formal veriﬁcation of probabilistic systems. Number

1601. Citeseer, 1997.

[7] Christian Dehnert, Sebastian Junges, Joost-Pieter Katoen, and Matthias
Volk. A storm is coming: A modern probabilistic model checker. In
International Conference on Computer Aided Veriﬁcation, pages 592–600.
Springer, 2017.

[8] Anca D. Dragan and Siddhartha S. Srinivasa. Formalizing assistive

teleoperation. In Robotics: Science and Systems, 2012.

[9] Anca D. Dragan and Siddhartha S. Srinivasa. A policy-blending
formalism for shared control. I. J. Robotic Res., 32(7):790–805, 2013.
[10] Andrew Fagg, Michael Rosenstein, Robert Platt, and Roderic Grupen.
In

in mixed initiative teleoperator control.

Extracting user intent
Intelligent Systems Technical Conference, page 6309, 2004.

[11] Lu Feng, Clemens Wiltsche, Laura Humphrey, and Ufuk Topcu. Synthesis
of human-in-the-loop control protocols for autonomous systems. IEEE
Transactions on Automation Science and Engineering, 13(2):450–462,
2016.

[12] Vojtˇech Forejt, Marta Kwiatkowska, Gethin Norman, David Parker, and
Hongyang Qu. Quantitative multi-objective veriﬁcation for probabilistic
systems. In TACAS, pages 112–127. Springer, 2011.

[13] Roland Fryer and Matthew O Jackson. A categorical model of cognition
and biased decision making. The BE Journal of Theoretical Economics,
8(1).

[14] Jie Fu and Ufuk Topcu. Synthesis of shared autonomy policies with
temporal logic speciﬁcations. IEEE Transactions on Automation Science
and Engineering, 13(1):7–17, 2016.

[15] F. Gal´an, M. Nuttin, E. Lew, P. W. Ferrez, G. Vanacker, J. Philips,
and J. del R. Mill´an. A brain-actuated wheelchair: Asynchronous and
non-invasive brain-computer interfaces for continuous control of robots.
Clinical Neurophysiology, 119(9):2159–2169, 2016/05/28.

[16] Gurobi Optimization,

Inc.

Gurobi optimizer

reference manual.

url=http://www.gurobi.com, 2013.

[17] Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh
Trivedi, and Dominik Wojtczak. Omega-regular objectives in model-
free reinforcement learning. In International Conference on Tools and
Algorithms for the Construction and Analysis of Systems, pages 395–412.
Springer, 2019.

[18] Nils Jansen, Murat Cubuktepe, and Ufuk Topcu. Synthesis of shared
control protocols with provable safety and performance guarantees. In
ACC, pages 1866–1873. IEEE, 2017.

[19] Shervin Javdani, J Andrew Bagnell, and Siddhartha Srinivasa. Shared
autonomy via hindsight optimization. In Robotics: Science and Systems,
2015.

Mohammed Alshiekh was a research assistant in
the Department of Aerospace Engineering at the
University of Texas at Austin from 2016 to 2018. He
received his BEng degree in Electrical and Electron-
ics Engineering from the University of Birmingham
in 2008 and his M.S degree in Systems Engineering
from the University of Pennsylvania in 2016.

Ufuk Topcu joined the Department of Aerospace
Engineering at the University of Texas at Austin as
an assistant professor in Fall 2015. He received his
Ph.D. degree from the University of California at
Berkeley in 2008. He held research positions at the
University of Pennsylvania and California Institute of
Technology. His research focuses on the theoretical,
algorithmic and computational aspects of design and
veriﬁcation of autonomous systems through novel
connections between formal methods, learning theory
and controls.

[20] Dae-Jin Kim, Rebekah Hazlett-Knudsen, Heather Culver-Godfrey, Greta
Rucks, Tara Cunningham, David Portee, John Bricout, Zhao Wang, and
Aman Behal. How autonomy impacts performance and satisfaction:
Results from a study with spinal cord injured subjects using an assistive
robot. IEEE Transactions on Systems, Man, and Cybernetics-Part A:
Systems and Humans, 42(1):2–14, 2012.

[21] Jonathan Kofman, Xianghai Wu, Timothy J Luu, and Siddharth Verma.
Teleoperation of a robot manipulator using a vision-based human-robot
interface. IEEE transactions on industrial electronics, 52(5):1206–1219,
2005.

[22] Marta Kwiatkowska, Gethin Norman, and David Parker. PRISM 4.0:
Veriﬁcation of probabilistic real-time systems. In CAV, volume 6806 of
LNCS, pages 585–591. Springer, 2011.

[23] Adam Leeper, Kaijen Hsiao, Matei Ciocarlie, Leila Takayama, and David
Gossow. Strategies for human-in-the-loop robotic grasping. In HRI, pages
1–8. IEEE, 2012.

[24] Yurii Nesterov and Arkadii Nemirovskii.

Interior-point polynomial

algorithms in convex programming, volume 13. Siam, 1994.

[25] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering, 22(10):1345–1359,
2010.

[26] Shashank Pathak, Erika ´Abrah´am, Nils Jansen, Armando Tacchella,
and Joost-Pieter Katoen. A greedy approach for the efﬁcient repair of
In NFM, volume 9058 of LNCS, pages 295–309.
stochastic models.
Springer, 2015.

[27] Martin L Puterman. Markov decision processes: discrete stochastic

dynamic programming. John Wiley & Sons, 2014.

[28] Jian Shen, Javier Ibanez-Guzman, Teck Chew Ng, and Boon Seng
Chew. A collaborative-shared control system with safe obstacle avoidance
capability. In Robotics, Automation and Mechatronics, volume 1, pages
119–123. IEEE, 2004.

[29] Brian D Ziebart. Modeling purposeful adaptive behavior with the principle

of maximum causal entropy. 2010.

[30] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey.

Maximum entropy inverse reinforcement learning. 2008.

Murat Cubuktepe
joined the Department of
Aerospace Engineering at the University of Texas at
Austin as a Ph.D. student in Fall 2015. He received
his B.S degree in Mechanical Engineering from
Bogazici University in 2015 and his M.S degree in
Aerospace Engineering and Engineering Mechanics
from the University of Texas at Austin in 2017.
His current research interests are veriﬁcation and
synthesis of parametric and partially observable prob-
abilistic systems. He also focuses on applications of
convex optimization in formal methods and controls.

Nils Jansen is an assistant professor with the Institute
for Computing and Information Science (iCIS) at
the Radboud University, Nijmegen, The Netherlands.
He received his Ph.D. in computer science with
distinction from RWTH Aachen University, Germany,
in 2015. Prior to Radboud University, he was a
postdoctoral researcher and research associate with
the Institute for Computational Engineering and
Sciences at the University of Texas at Austin. His
current research focuses on formal reasoning about
safety aspects in machine learning and robotics. At
the heart is the development of concepts inspired from formal methods to
reason about uncertainty and partial observability.

