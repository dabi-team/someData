Linear Transformers Are Secretly Fast Weight Programmers

Imanol Schlag∗ 1 Kazuki Irie∗ 1 J ¨urgen Schmidhuber 1

1
2
0
2

n
u
J

9

]

G
L
.
s
c
[

3
v
4
7
1
1
1
.
2
0
1
2
:
v
i
X
r
a

Abstract

We show the formal equivalence of linearised self-
attention mechanisms and fast weight controllers
from the early ’90s, where a “slow” neural net
learns by gradient descent to program the “fast
weights” of another net through sequences of ele-
mentary programming instructions which are ad-
ditive outer products of self-invented activation
patterns (today called keys and values). Such Fast
Weight Programmers (FWPs) learn to manipulate
the contents of a ﬁnite memory and dynamically
interact with it. We infer a memory capacity limi-
tation of recent linearised softmax attention vari-
ants, and replace the purely additive outer prod-
ucts by a delta rule-like programming instruction,
such that the FWP can more easily learn to cor-
rect the current mapping from keys to values. The
FWP also learns to compute dynamically chang-
ing learning rates. We also propose a new kernel
function to linearise attention which balances sim-
plicity and effectiveness. We conduct experiments
on synthetic retrieval problems as well as standard
machine translation and language modelling tasks
which demonstrate the beneﬁts of our methods.

1. Introduction

Transformers (Vaswani et al., 2017) have achieved impres-
sive results in a myriad of sequence processing tasks, in-
cluding machine translation, language modelling (Al-Rfou
et al., 2019; Dai et al., 2019; Baevski & Auli, 2019; Radford
et al., 2019), and question answering (Devlin et al., 2019),
domains previously dominated by recurrent neural networks
(Graves, 2013; Bahdanau et al., 2015).

The core component of a Transformer is the self-attention
mechanism (Cheng et al., 2016; Parikh et al., 2016; Lin et al.,
2017) which was recently connected to the modern Hop-

*Equal contribution

1The Swiss AI Lab IDSIA, USI &
SUPSI. Correspondence to: Imanol Schlag <imanol@idsia.ch>,
Kazuki Irie <kazuki@idsia.ch>, J¨urgen Schmidhuber <juer-
gen@idsia.ch>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

ﬁeld network (Ramsauer et al., 2021; Krotov & Hopﬁeld,
2016; Demircigil et al., 2017). It extends a form of attention
(Bahdanau et al., 2015) originally introduced to complement
recurrent neural networks, e.g., (Hochreiter & Schmidhuber,
1997). While relinquishing the recurrence property, all com-
putations across the time axis can be parallelised. However,
this comes with drawbacks: self-attention computations
scale quadratically with sequence length while the mem-
ory of the model grows linearly. Therefore, practitioners
are forced to limit the context window to a reasonable size,
which in turn makes it impossible to capture longer-term
dependencies.

Recent work proposed “linear Transformers” with constant
size memory and time complexity linear in sequence length
(Katharopoulos et al., 2020; Choromanski et al., 2021; Peng
et al., 2021; Shen et al., 2018). This complexity reduction
is mainly due to a linearisation of the softmax (reviewed in
Sec. 3.2).

Here we emphasize the formal equivalence of this family of
linear Transformers and the Fast Weight Controllers or Fast
Weight Programmers (FWPs) from the ’90s (Schmidhuber,
1991; 1992; 1993; AI Blog, 2021) (apart from normalisa-
tion). The memories of such FWPs contain key-value asso-
ciations, and an FWP can learn to reprogram them through
sequences of differentiable elementary instructions (also
called update rules), which are additive outer products be-
tween keys and values invented by the FWP.

This view allows us to derive a limitation of the memory
capacity of linear Transformers and similar models. When
the sequence length exceeds storage capacity, the model
may end up in an overcapacity regime (discussed in depth
in Sec. 4.1). To properly operate under such a regime, the
model should learn to dynamically interact with the memory
contents and selectively decide which key-value associations
to keep and which ones to delete. The purely additive in-
struction may be inappropriate for this purpose. Therefore,
inspired by recent work on FWPs (Schlag et al., 2021), we
introduce an improved programming instruction akin to the
famous error-correcting delta-rule (Widrow & Hoff, 1960).

Furthermore, softmax linearisation techniques for Trans-
The existing tech-
formers are still underexplored.
niques are either very simplistic (Katharopoulos et al.,
2020) or mathematically well explained but complex

 
 
 
 
 
 
Linear Transformers Are Secretly Fast Weight Programmers

(Choromanski et al., 2021; Peng et al., 2021). We provide
a comprehensive comparison and propose a new method
which is both simple and effective.

We demonstrate the beneﬁts of the proposed methods on
our own synthetic retrieval dataset (Sec. 6.1), the stan-
dard WMT14 English to German machine translation task
(Sec. 6.2), and the Wikitext-103 (Merity et al., 2017) lan-
guage modelling task (Sec. 6.3)2.

2. Background on Fast Weight Programmers

Here we review the concepts of Fast Weight Programmers
(FWPs) before relating them to linear Transformer variants
in Sec. 3.

In standard neural networks, the weights remain ﬁxed after
training, unlike the activations, which change depending on
the inputs at test time. The general idea of fast weights is to
make the weights also variable and input-dependent. This
concept was called synaptic modulation (von der Malsburg,
1981), a method for variable binding in neural networks (see
e.g. the recent survey by Greff et al. (2020)), or dynamic
connections (Feldman, 1982). Von der Malsburg deﬁnes
the effective weights as a (multiplicative) superposition of
conventional, context-independent slow weights, and fast
changing, context-dependent fast weights. Hinton & Plaut
(1987) studied a net with (additive) superposition of two sets
of weights with two different learning rates in a scenario
of model retraining. Before 1991, however, no network
learned by gradient descent to quickly compute the changes
of the fast weight storage of another network or of itself.

Context-dependent FWPs were introduced in two-network
systems of the early ’90s (Schmidhuber, 1991; 1992; 1993;
AI Blog, 2021). A traditional slow net with slow weights
continually changes or reprograms the fast weights of a fast
net, making the fast weights effectively dependent on the
spatio-temporal context of a given input stream. Simply
put, the slow net learns to program its fast net. Among
the proposed elementary differentiable instructions that the
slow net can use to program the fast weights, a particularly
attractive one makes use of outer products (Schmidhuber,
i=1, x(i) ∈ Rdin,
1991; 1992): for a sequential input {x(i)}L
i=1, y(i) ∈ Rdout as
the model outputs the sequence {y(i)}L

a(i), b(i) = Wax(i), Wbx(i)

W (i) = σ(cid:0)W (i−1) + a(i) ⊗ b(i)(cid:1)
y(i) = W (i)x(i)

(1)

(2)

(3)

where ⊗ denotes the outer product, σ is an activation func-
tion, Wa and Wb are trainable slow weights, while the fast
weights W (i) are generated at each time step i and serve

2Source

code used in this paper
github.com/ischlag/fast-weight-transformers.

is

available

at

as a short-term memory. This is a key-value associative
memory model in which the write operation is based on
a summation (Eq. 2) and the retrieval is a matrix-vector
multiplication (Eq. 3). Schmidhuber (1993) describes a
recurrent version and discusses “internal spotlights of atten-
tion” (such attention terminology is now widely used in the
context of transformers). The use of outer products results
in a model of associations similar to tensor product pre-
sentations (Smolensky, 1990). In fact, outer-product based
associative memory can be found in numerous works since
Hebb’s informal rule (Hebb, 1949) and its more concrete
formal variants (Steinbuch, 1961; Steinbuch & Piske, 1963;
Kohonen, 1972; Palm, 1980) including Hopﬁeld networks
(Hopﬁeld, 1982; Little, 1974) and bi-directional associa-
tive nets (Kosko, 1988). However, these authors described
pre-wired rules to associate given patterns with each other.
Their systems did not learn to use such rules for associating
self-invented patterns like the FWPs since 1991.

The concept of FWPs has been revisited recently (Ba et al.,
2016; Schlag & Schmidhuber, 2017), also under different
names, e.g., hypernetworks (Ha et al., 2017; Perez et al.,
2018; Galanti & Wolf, 2020), dynamic plasticity (Miconi
et al., 2018; 2019), dynamic convolution (Klein et al., 2015;
Noh et al., 2016; Jia et al., 2016), or lambda networks
(Bello, 2021) used for applications including meta-learning
(Munkhdalai & Yu, 2017; Munkhdalai & Trischler, 2018;
Munkhdalai et al., 2019; Kirsch & Schmidhuber, 2020).
FWPs recently also improved memory models through ex-
plicit mechanisms for facilitating the replacement of dep-
recated information and updating associations (Schlag &
Schmidhuber, 2018; Schlag et al., 2021).

3. Relation to Transformers

Ba et al. (2016) have already pointed out a relation between
a variant of outer product-based FWPs (Schmidhuber, 1993)
and attention (Bahdanau et al., 2015). Katharopoulos et al.
(2020) have analysed linearised transformers. We review
these derivations, emphasising the relation between Trans-
formers and the FWPs of the previous section.

3.1. Self-Attention Without Softmax Is a Fast Weight

Programmer

∈

A self-attention layer
in auto-regressive Transform-
ers (Vaswani et al., 2017) maps an input sequence
{x(i)}L
sequence
{y(i)}L

to an output

Rd×1
i=1, x(i)
i=1, y(i) ∈ Rdvalue×1 as
k(i), v(i), q(i) = Wkx(i), Wvx(i), Wqx(i)
K(i) = (cid:2)K(i−1), k(i)] ∈ Rdkey×i
V (i) = (cid:2)V (i−1), v(i)] ∈ Rdvalue×i
y(i) = V (i)softmax((K(i))(cid:62)q(i))

(4)

(5)

(6)

(7)

Linear Transformers Are Secretly Fast Weight Programmers

where [A, a] denotes the concatenation of vector a to matrix
A along the time dimension, softmax is applied along the
time dimension, and Wk, Wv, Wq are trainable weight ma-
trices. We omit the scaling by 1/(cid:112)dkey inside the softmax
without loss of generality.

Using the outer-product notation, the numerator is analo-
gous to the case without softmax (Sec. 3.1):

i
(cid:88)

j=1

(cid:0)v(j)φ(k(j))(cid:62)(cid:1)φ(q(i)) = (cid:0)

i
(cid:88)

j=1

v(j) ⊗ φ(k(j))(cid:1)φ(q(i))

Now if we remove the softmax in Eq. 7 we obtain:

y(i) = V (i)(cid:0)(K(i))(cid:62)q(i)(cid:1) = (cid:0)V (i)(K(i))(cid:62)(cid:1)q(i)

= (cid:0)

i
(cid:88)

j=1

v(j) ⊗ k(j)(cid:1)q(i)

(8)

Denoting by W (i) the corresponding weight matrix gener-
ated from key and value vectors:

W (i) = (cid:0)

i
(cid:88)

j=1

v(j) ⊗ k(j))

(9)

we can rewrite Eqs. 4-7 such that they directly relate to
Eqs. 1-3 where the activation function σ is the identity func-
tion and without query projection Wq:

k(i), v(i), q(i) = Wkx(i), Wvx(i), Wqx(i)

W (i) = W (i−1) + v(i) ⊗ k(i)

y(i) = W (i)q(i)

(4)

(10)

(11)

3.2. Linearising Self-Attention

Instead of removing the softmax as in Sec. 3.1, prior works
have introduced techniques for linearising the softmax (Tsai
et al., 2019), which has been shown to improve com-
putational efﬁciency of self-attention for long sequences
(Katharopoulos et al., 2020; Choromanski et al., 2021; Peng
et al., 2021).

By writing the softmax explicitly, Eq. 7 can be written as:

y(i) =

i
(cid:88)

j=1

v(j)κ(k(j), q(i))
j(cid:48)=1 κ(k(j(cid:48)), q(i))

(cid:80)i

(12)

where κ(k, q) = exp(k · q) ∈ R>0 is the softmax kernel
and k · q = k(cid:62)q is the vector dot product.

The general idea is to replace the softmax kernel κ by an-
other kernel: κ(cid:48)(k, q) = φ(k)(cid:62)φ(q) where φ is a function
Rdkey → Rddot. We discuss the necessary properties of φ in
Sec. 5.1. By replacing κ in Eq. 12 by κ(cid:48), we obtain

i
(cid:88)

y(i) =

(cid:80)i

v(j)φ(k(j))(cid:62)φ(q(i))
j(cid:48)=1 φ(k(j(cid:48))) · φ(q(i))
(cid:0)v(j)φ(k(j))(cid:62)(cid:1)φ(q(i))
j(cid:48)=1 φ(k(j(cid:48)))(cid:1) · φ(q(i))

j=1
(cid:80)i

=

j=1
(cid:0) (cid:80)i

(13)

(14)

By introducing the fast weight matrix W (i) and an addi-
tional vector z(i) for the denominator,

W (i) =

z(i) =

i
(cid:88)

j=1

i
(cid:88)

j=1

v(j) ⊗ φ(k(j))

φ(k(j))

(15)

(16)

forward computations of linear Transformers can be written
as (Katharopoulos et al., 2020):

k(i), v(i), q(i) = Wkx(i), Wvx(i), Wqx(i)
W (i) = W (i−1) + v(i) ⊗ φ(k(i))

z(i) = z(i−1) + φ(k(i))

y(i) =

1
z(i) · φ(q(i))

W (i)φ(q(i))

(4)

(17)

(18)

(19)

which is a Fast Weight Programmer (Sec. 2) with normali-
sation. Hence, the core of linear Transformer variants are
outer product-based Fast Weight Programmers.

4. Analysing and Improving Linear

Transformers as Fast Weight Programmers

Viewing linear Transformer variants as Fast Weight Pro-
grammers provides us with two insights which we investi-
gate in this work: their capacity limits as associative memo-
ries (Sec. 4.1), and their ineptness to edit previously stored
associations (Sec. 4.2).

4.1. Capacity Limitation

Intuition. Endlessly adding new associations to a memory
of ﬁnite size, as in Eq. 17, inevitably will reach a limit. In
linear attention, information is stored in a matrix and is
retrieved using matrix multiplication (see Eq. 19). As a
consequence, to prevent associations from interfering with
each other upon retrieval, the respective keys need to be
orthogonal. Otherwise, the dot product will attend to more
than one key and return a linear combination of values.
With keys embedded in a ddot space, there cannot be more
than ddot orthogonal vectors. That is, storing more than
ddot associations will result in a retrieval error. In linear
Transformers, when the length of the sequence is longer than
ddot, the model might be in such an overcapacity regime.
While we experimentally demonstrate this effect on toy tasks
(Sec. 6.1), prior work on tensor product representations
allows for a more formal discussion.

Linear Transformers Are Secretly Fast Weight Programmers

Tensor Product Representation Theory. Early work in
connectionist research investigated the usage of distributed
representations as a means for storing symbolic structures.
One highly-inﬂuential work is the tensor-product-based vari-
able binding mechanism (Smolensky, 1990). A tensor prod-
uct representation (TPR) of a structured symbolic system
consisting of a set of variables and values constructed from
outer products of the so called role and ﬁller vectors. These
terms directly translate into keys and values in our context.
The fast weight memories of Eq. 17 are the most basic form
of such representations (second order tensors). Therefore,
many results discussed in Smolensky’s work transfer to our
model. In particular, Theorem 3.3 and 3.1 of Smolensky
(1990) discuss more formally the crosstalk and retrieval
error intuitively described in the previous paragraph.

However, we also note an important difference: the classic
TPRs of Smolensky (1990) are constructed with a priori
knowledge of the symbolic structure. In contrast, our FWPs
since 1991, including recent FWPs (Schlag & Schmidhuber,
2018), learn all the vectors involved in constructing such a
representation.

4.2. Improving the FWP’s Programming Instruction

Sec. 4.1 argues that the linear Transformers can end up in
an overcapacity regime, if the sequence length L exceeds
the dimension ddot of the keys. Once in overcapacity, an
ideal memory model should dynamically interact with the
memory contents and selectively determine which associa-
tions to remember or to forget. This is in stark contrast to
the standard Transformer which stores immutable pairs of
key and value vectors by concatenation, thus increasing the
storage size. While such models work well in practice, we
consider a model’s capability to update previously acquired
knowledge to be critical for many problems. Hence, from
the perspective of dynamic interaction with the memory, the
purely additive update rule of Eqs. 17 may be sub-optimal.
This motivates us to improve the elementary differentiable
programming instruction (i.e. the update rule) of FWPs.

Inspired by the recent work by Schlag et al. (2021), we
propose a basic instruction that essentially implements the
famous error-correcting delta rule (Widrow & Hoff, 1960)
in an end-to-end differentiable way, such that the FWP
can learn to use it wisely, through self-invented, dynami-
cally changing learning rates. Given a new input key-value
pair (k(i), v(i)), the FWP ﬁrst accesses the current state of
the memory W (i−1) and retrieves the value ¯v(i) currently
paired with the key k(i). Then the model stores a convex
combination v(i)
new of the retrieved value ¯v(i) and the input
v(i) using an interpolation weight 0 ≤ β(i) ≤ 1 also gener-
ated by the model. The model thus sequentially transforms
i=1, x(i) ∈ Rd×1 into an output
an input sequence {x(i)}L

sequence {y(i)}L

i=1, y(i) ∈ Rdvalue×1 as:

k(i), v(i), q(i) = Wkx(i), Wvx(i), Wqx(i)
¯v(i) = W (i−1)φ(k(i))
β(i) = σ(Wβx(i))
new = β(i)v(i) + (1 − β(i))¯v(i)
v(i)

(4)

(20)

(21)

(22)

where Wβ ∈ R1×d, and σ is the sigmoid function. The
interpolation weight β(i) is the “write-strength” as it deﬁnes
to which extent the new value will replace the previous value.
We note that while β(i) only depends on x(i), in a multi-
layer model, x(i) has the full context information except in
the ﬁrst layer. We set W (0) = 0 and z(0) = 0. Then the
fast weight update rule and the ﬁnal output y(i) are deﬁned
as follows (see Appendix A.1 for detailed derivations):

W (i) = W (i−1) +v(i)

new ⊗ φ(k(i))
(cid:123)(cid:122)
(cid:125)
write
= W (i−1) + β(i)(v(i) − ¯v(i)) ⊗ φ(k(i))

−¯v(i) ⊗ φ(k(i))
(cid:123)(cid:122)
(cid:125)
(cid:124)
remove

(cid:124)

y(i) = W (i)φ(q(i))

(23)

(24)

(25)

As shown in Eq. 24, our programming instruction or up-
date rule is effectively a delta rule with a dynamic learning
rate β(i). The model thus learns to correct the current key
to value association. In Appendix B, we formally show
the advantage of this approach over the gated update rule
concurrently proposed by Peng et al. (2021).

Normalisation.
In the equations above, no normalisation
is applied to the value we retrieve. A straightforward nor-
malisation can be obtained by following the derivation in
Sec. 3.2, i.e. by introducing an accumulator:

z(i) = z(i−1) + φ(k(i))

and replacing Eqs. 20 and 25 respectively by:

¯v(i) =

y(i) =

W (i−1)φ(k(i))
z(i−1) · φ(k(i))
W (i)φ(q(i))
z(i) · φ(q(i))

(26)

(27)

(28)

where we deﬁne ¯v(1) = 0. In this approach, the output y(i)
is a weighted average of β(j)(v(j) − ¯v(j)) for 1 ≤ j ≤ i.
We refer to this approach as attention normalisation.

This approach, however, has drawbacks. First, the accu-
mulation of positive values in Eq. 26 always grows with
the number of steps, and may result in instability. Second,
speciﬁcally for our update rule, this normalisation is not
sufﬁcient to balance the weights between write and remove
operations in Eq. 23 (see derivations in Appendix A.2). Here

Linear Transformers Are Secretly Fast Weight Programmers

we propose a better approach based on simple normalisation.
We divide the effective key and query vectors φ(k(i)) and
φ(q(i)) by the sum of its components, e.g., for the query:

φ(cid:48)(q(i)) =

φ(q(i))

ddot(cid:88)

j=1

φ(q(i))j

(29)

before applying Eqs. 20-25. A general consequence of this
normalisation is intuitively understood by noticing that the
output of any matrix-vector operations (like Eq. 25) is a
weighted sum of columns of the matrix where weights are
the components of the vector; thus, if the vector components
sum up to one, the operation can be viewed as an attention
over the columns of the matrix. We provide further expla-
nations and precise implications for our FWP in Appendix
A.2. We refer to this approach as sum normalisation.

Since this is a simple substitution of φ(k(i)) and φ(q(i)) in
Eqs. 20-25, one might still ask whether additional attention
normalisation is needed. In language modelling experiments
(Sec. 6.3), we show that this is not the case.

5. Linear Attention Functions

The central component of softmax linearisation (Sec. 3.2)
is the φ function which maps key and query vectors to the
space where the dot product is executed: Rdkey → Rddot. We
ﬁrst list desirable properties of such a function, and review
the existing φ functions from the perspective of fast weight
memories. Finally, we also propose our own φ function.

5.1. Properties

For Eq. 13 to deﬁne proper attention weights between 0 and
1, the codomain of φ should be positive. Another property
of φ derives from the discussion of memory capacity in
Sec. 4.1. The dimensionality of its codomain ddot deﬁnes
the model’s capacity. Therefore, by including a transfor-
mation which projects the input dimension dkey to a larger
dimension ddot, the φ function can potentially increase the
upper bound of the capacity.

5.2. Katharopoulos’ Linear Attention

Katharopoulos et al. (2020) propose to use the simple
element-wise ELU + 1 function (Clevert et al., 2016):

φ(x) = ELU(x) + 1 =

(cid:40)

x + 1,
exp(x),

if x > 0
if x ≤ 0

(30)

The choice of ELU over ReLU is motivated by non-zero
gradients on the negative part. Importantly, as a simple
element-wise function, this φ function preserves the dimen-
sion of the input key vector (dkey = ddot), without modifying
the memory capacity as discussed in Sec. 4.1.

5.3. FAVOR+

In contrast to Katharopoulos et al. (2020)’s φ function which
merely satisﬁes positivity (and a good gradient) property,
Choromanski et al. (2021) propose a mathematically rigor-
ous method to approximate the softmax with random fea-
tures. They propose the following φ function:

h(x) =

φ(x) =

1
√
2
h(x)
√
m

exp(−

||x||2
2

)

(cid:20) exp(Rx)
exp(−Rx)

(cid:21)

(31)

(32)

where the concatenation

(cid:20)a
(cid:21)
b

of two vectors a and b is along

the feature dimension, and R ∈ Rm×dkey is a matrix with
m random features where each row vector r ∈ R1×dkey is
drawn from N (0, Idkey ). A similar approach is also proposed
by Peng et al. (2021).

With FAVOR+, the dimension of the codomain ddot is 2m
which increases the theoretical capacity of the memory if
2m > dkey. At the same time, the model’s capacity is still
limited, and equals the inﬁnite capacity of the softmax mem-
ory only when m goes to inﬁnity, which is never achieved
in practice. During training, we redraw these m random
vectors for each mini-batch. During evaluation, we draw
a set of m random vectors once, and keep them ﬁxed. m
is the only hyperparameter of FAVOR+ and inﬂuences the
quality of the softmax approximation. Choromanski et al.
(2021) suggest to choose m in the order of dkey log(dkey).
This sampling process is the main drawback of FAVOR+ as
it introduces variance into the model’s output.

5.4. Deterministic Parameter-Free Projection (DPFP)

The two previous sub-sections highlight the sub-optimality
of the existing φ functions. Sampling introduces extra com-
plexity to FAVOR+ (Sec. 5.3), while the Linear Transformer
(Sec. 5.2) lacks the ability to project up the dot product
dimension. Here we propose an alternative approach called
deterministic parameter-free projection (DPFP). It is de-
terministic and easy to compute like Linear Transformers
while increasing the dot product dimension without requir-
ing FAVOR+’s random features.

We begin with a low-dimensional example to foster an in-
tuitive understanding before moving on to the general for-
mulation. Consider 4 keys k(i), i ∈ {1, 2, 3, 4} in R2 and
φ : R2 → R4
≥0 where the l-th element of φ(x) is gener-
ated by the partial function φl : R2 → R≥0. We design φ
such that it facilitates orthogonality in the projected space,
i.e. φ(k(i)) · φ(k(j)) = 0 for i (cid:54)= j. Towards this end,
we construct φ such that if φl(x) > 0 then φn(x) = 0 for
all n (cid:54)= l. Such a constraint can be enforced by limiting
the domains of the partial functions to be non-overlapping.

Linear Transformers Are Secretly Fast Weight Programmers

With the element-wise rectiﬁer function r(a) = max(0, a)
the partial functions are deﬁned as:

φ1(k) = r(k1)r(k2)
φ2(k) = r(−k1)r(k2)
φ3(k) = r(k1)r(−k2)
φ4(k) = r(−k1)r(−k2)

(33)

(34)

(35)

(36)

Figure 1 illustrates this function. The elements of the 4-
dimensional space are displayed as the z component of the
four coloured surfaces. The ﬁgure shows how each vector
in the 2d plane will have a single non-zero component in the
4d space and equally splits the input space into four areas
which will be orthogonal in the projected space.

6.1. Synthetic Settings

We illustrate the capacity issue (Sec. 4.1) of linear attention
and the effectiveness of our new update rule (Sec. 4.2) on
two synthetic problems.

In both settings, our toy problem consists of retrieving the
correct value from a sequence of randomly sampled key-
value associations when queried with one of the used keys.
Crucially, the query is given at the end of the sequence, such
that the model is not aware of it while processing the inputs.
To succeed, the model has to learn to store the observed
associations in its memory without interference.

Let K and V be the ﬁnite and ﬁxed sets of keys and values
and S = |K| = |V|. Then, the input to the model is the
sequence [(k, v)1, ..., (k, v)L] followed by q where every
pair (k, v) ∈ K × V is sampled randomly, and q is randomly
chosen to be one of the L keys.

Each value v(i), i ∈ [1, .., S] is assigned a ﬁxed one-hot
vector v(i) ∈ RS. Hence, the set of value vectors is an
orthonormal basis. In contrast, the vector embedding of
the key symbols is the learned function e : K → Rdemb and
k = WK[e(k); v] where WK ∈ Rdkey×(demb+S).

Figure 1. A visualisation of a DPFP from a 2d space (the xy-plane)
to a 4d space (the four colored surfaces). Each surface is a partial
function which represents one element of the 4d vector.

We generalise this method to higher dimensional inputs by
constructing additional two-factor features. Given an input
vector k ∈ Rdkey and i ∈ [1, 2dkey], the partial function

φiν(k) = r(

(cid:20) k
−k

(cid:21)
)ir(

(cid:20) k
−k

(cid:21)
)i+ν

(37)

where ν ∈ {1, 2, .., dkey2 − 1} is a capacity controlling
hyperparameter. The codomain dimensionality of φ(k) is
thus ddot = 2dkeyν. Eq. 37 is highly parallelisable because
each partial function can be computed independently. This
can be implemented in few lines of code as we show in
Appendix C.

Finally we note that Choromanski et al. (2021) empirically
show that replacing exp in Eq. 32 by ReLU typically im-
proves model performance. While this result has not been
theoretically justiﬁed, it supports the design of our DPFP
which aims for sparsity and orthogonality.

6. Experimental Results

Now we present our experimental results on synthetic re-
trieval problems (Sec. 6.1.1 and 6.1.2), machine translation
(Sec. 6.2), and language modelling (Sec. 6.3).

1

2 (v∗

Following the L write operations, the read function and
the query vector q = WQe(q), WQ ∈ Rdkey×demb are used
to retrieve ˆv ∈ RS from memory. Finally, the loss is de-
ﬁned as l(ˆv, v∗) = (cid:80)S
j − ˆvj)2 where v∗ is the value
j
vector assigned to q in the input sequence. Each model
is trained in mini-batches using this loss and Adam with
default hyperparameters unless stated otherwise. For evalu-
ation, we sample 20 sequences and test all possible queries,
e.g., with S = 100 unique keys, the evaluation batch is of
size 100 ∗ 20 = 2000.

6.1.1. SETTING 1: TESTING CAPACITY

In this setting, we experimentally demonstrate the capacity
limit of linear attention (Sec. 4.1). We conduct experiments
for the various φ functions described in Sec. 5. We ﬁx dkey
to be 64, while different φ functions produce different ddot.
We set the sequence length to be equal to the number of
unique keys (L = S), and sample the keys and values with-
out replacement to generate the sequences. By varying the
sequence length S, our goal is to show that all linear atten-
tion models (using the simple sum update rule of Sec. 3.2)
fail at retrieving when S exceeds ddot.

All models are trained with a mini-batch size of 32 until the
evaluation loss falls below 0.001 or until lack of progress for
1000 steps. In Figure 2, the best validation set performance
for each model and each S is displayed (for the learning
curves see Appendix D.1). The number of unique keys
is initially S = 20 and is incremented by 20 until S =
600. The following models are compared: Softmax, Linear-

Linear Transformers Are Secretly Fast Weight Programmers

Attention, FAVOR+ with 64, 128, and 512 random features,
DPFP-ν with ν ∈ {1, 2, 3}.

Schlag (2021) is simply the model from Schlag et al. (2021)
ported to this setting (i.e. without the LSTM layer). Schlag
(2021) has neither a φ function, nor the sum normalisation
term of Sec. 4.2. Instead it uses a tanh nonlinearity for its
key representations. As an ablation we replace it with our
DPFP-1 but we don’t use the normalisation term of Sec. 4.2,
which we refer to as Schlag (2021) with DPFP.

Figure 3 presents the learning curves. They demonstrate
that our new update rule outperforms all other variants. As
expected, the baseline sum update rule fails.

Figure 2. Final evaluation loss of the softmax memory and various
linear attention mechanisms on associative retrieval problems with
the total number of unique associations ranging from 20 to 600.
Each individual symbol is a model trained until convergence.

The results support our theoretical analysis.
Linear-
Attention has a capacity of 64 due to the choice of dkey =
ddot = 64. Experimentally, Linear-Attention begins to ac-
cumulate errors with 60 or more associations. Similarly,
DPFP projections 1, 2 and 3 start to accumulate errors as
they approach their respective limits at 128, 256, and 384.
FAVOR+, on the other hand, fails to achieve a loss of 0 in
any experiment. Finally, as expected, softmax attention is
outperforming all φ functions, although it struggles to fully
converge with more than 500 keys.

6.1.2. SETTING 2: COMPARING UPDATE RULES

In the second setting, we compare variations of the update
rule. Unlike in setting 1, keys and values will be sampled
with replacement and sequence length L = 2S. As a result,
in the same sequence, multiple keys can be re-assigned to a
new value more than once. The expected value to retrieve
is the most recent one associated with the query. With
every new key, the previous value associated with this key
deprecates and the model is required to update its ﬁnite size
memory. The ability to update values associated with keys
is essential to bind context-speciﬁc values to a key.

We use DPFP-1 as the φ function. The sequence length
is ﬁxed at 40 with 20 unique keys and values. While this
setting does not exceed the capacity of DPFP-1, our result is
independent of the capacity regime (see results for different
S and φ in Appendix D.2).

We compare the proposed fast weight memory programming
instruction with normalisation of Sec. 4.2 (denoted here by
ours) to three baselines: the sum update rule of Sec. 3 (sum
rule), and two variants of previous update rules (Schlag
et al., 2021): Schlag (2021) and Schlag (2021) with DPFP.

Figure 3. Learning curves for different update rules. Sequence
length of 40 and 20 unique keys/values sampled with replacement.

6.2. Machine Translation Experiments

Here we compare φ functions on the standard machine trans-
lation task. We compare Linear Transformer (Katharopou-
los et al., 2020), Performer (Choromanski et al., 2021) and
our φ function DPFP (Sec. 5.4) to the regular Transformer,
complementing prior comparisons, e.g., Tay et al. (2021).

We use the standard WMT14 English to German Translation
dataset and standard data setups (Ott et al., 2018; Vaswani
et al., 2017). We adapt the recipe of Ott et al. (2019) (see
Appendix E) and train Vaswani et al. (2017)’s “big” models
for about 4 days on three V100 GPUs. We use the exact
same training conﬁgurations for all models without model-
speciﬁc hyper-parameter tuning. We only vary the model
hyper-parameters m in Performers and ν in DPFP models.

Table 1 shows the BLEU score (Papineni et al., 2002; Post,
2018) results. The Performer is as good as the basic Trans-
former when the number of samples m is large enough (for
ddot = 512, we have m = 256). In fact, with dkey = 64,
the recommended value for m is ddot log(ddot) = 266. Our
DPFP model outperforms the Linear Transformer as well
as the Performer when ddot is relatively small; providing a
good trade-off between simplicity and performance.

Linear Transformers Are Secretly Fast Weight Programmers

Table 1. WMT14 En-De Translation BLEU scores for various
Transformer models. Neither model averaging, nor model spe-
ciﬁc tuning is done. Standard denotes the basic Transformer.

ddot

Standard
Linear
Performer
DPFP (ours)

64

26.6
25.5
24.2
-

Valid

256

-
-
24.9
26.2

512

-
-
26.7
26.2

64

27.7
26.8
24.4
-

Test

256

-
-
25.3
26.9

512

-
-
27.7
27.1

Table 2. WikiText-103 language model perplexity results showing
effects of our update rule. The number of trainable parameters
are almost the same for all models, up to the small difference
introduced by gating in our update rule (16 K and 33 K parameters
respectively for the small and medium conﬁgurations). We have
D = 128, L = 256 (40 M parameters) in the small, and D = 256,
L = 384 (90 M parameters) in the medium conﬁguration. For
Performers, m is 8 and 16, respectively.

6.3. Language Modelling Experiments

Toy experimental Setting 2 (Sec. 6.1.2) illustrated the effect
of our update rule. Now our goal is to conﬁrm its effective-
ness on a large-vocabulary word-level language modelling
task, and investigate its further potential.

Transformer

Linear Transformer
Delta Network

Performer

Update

small

medium

Rule

Valid

Test Valid

-

sum
delta

sum
delta

33.0

37.1
34.1

39.0
36.1

34.1

38.3
35.5

39.6
37.2

27.9

31.1
29.7

32.2
30.0

Test

29.6

33.0
31.5

33.8
31.8

Experimental setups. Our update rule should be evalu-
ated on a dataset with sufﬁciently long contextual depen-
dencies. We use the standard WikiText-103 (Merity et al.,
2017) dataset. WikiText-103 consists of long articles from
Wikipedia; the training set contains about 28 K articles with
a total of 103 M running words. This results in contextual
text blocks of about 3600 words. The validation and test
sets also contain similarly long dependencies, respectively
with 218 K and 246 K running words for 60 articles each.
The vocabulary size is about 268 K words.

We split the training data into L-word long segments (which
is the backpropagation span). Unless stated otherwise, we
treat these segments independently during training. For eval-
uation, we use a batch size of one, and go through the text
with a sliding window of size L, taking into account only
the last position for computing perplexity (except in the ﬁrst
segment where all positions are evaluated). This is usually
done for Transformers with a limited context (Al-Rfou et al.,
2019). Appendix F provides further experimental details.

Effectiveness of our new update rule. We ﬁrst evaluate
our update rule in two conﬁgurations. In the small conﬁg-
uration, we set the model dimension (same for key, value,
and query) D to 128, and the training and evaluation context
length L to 256. We note that D = H ∗ ddot where H is
the number of heads. H is set to 8. The feed-forward layer
dimension is 2048. The number of layers is 16 in all conﬁg-
urations. In the medium conﬁguration, we set D = 256 and
L = 384. Both conﬁgurations represent an overcapacity
regime. We evaluate both Linear Transformers (Katharopou-
los et al., 2020) and Performers (Choromanski et al., 2021).
However, to keep the comparison simple, we set the ca-
pacity of Performers (Sec. 5.3) equal to the one of linear
Transformers, by the right choice of projection dimension
(m = 8 and m = 16, respectively, in small and medium

conﬁgurations), even though this limits performance. We
do not include DPFP here, since in both conﬁgurations even
the smallest value for ν provides enough capacity. Here we
investigate the effect of the update rule in an overcapacity
scenario (see Appendix D.3 for experimental results in a
non-overcapacity regime including DPFP). All models can
be trained using two V100 GPUs in less than four days. We
refer to the Linear Transformer with our delta update rule
as a Delta Network. Table 2 shows the perplexity results.
In both conﬁgurations, our update rule provides convincing
improvements over the models with the sum update rule.

We also conduct an ablation study to test the effect of the
absolute positional encoding and an extra attention normal-
isation (Sec. 4.2). Table 3 shows the results. The sum
normalisation (Sec. 4.2) is used in all cases: the models
diverged otherwise. In contrast, better perplexities are ob-
tained when no additional attention normalisation is applied.
We also observe that the absolute positional encoding is not
needed, conﬁrming results of prior work (Irie et al., 2019a).

Table 3. WikiText-103 language model perplexities for Linear
Transformers (medium conﬁguration) with our update rule.

Position Encoding Attn. Normalisation Valid

Yes
No
Yes
No

Yes
Yes
No
No

30.4
29.2
29.7
28.1

Test

32.1
31.2
31.5
31.1

Complexity, wall clock time, memory. All methods we
propose are within the framework of “linear Transform-
ers”. Thus, there is no change to be discussed in terms of
complexity which is constant in space and linear in time
w.r.t. sequence length. However, our modiﬁed update rule in-

Linear Transformers Are Secretly Fast Weight Programmers

Table 4. WikiText-103 language model perplexities when the
model is trained and evaluated without truncating context, as
opposed to Table 2 where the context window is limited. The
medium conﬁg is used. Neither positional encoding nor attention
normalisation is used for the Delta Net. The numbers of trainable
parameters (Prms.) are given in millions. We compare with the
Transformer-XL at different memory segment lengths. This re-
sults in different state sizes which are proportional to the memory
requirements during evaluation, and highlights the memory efﬁ-
ciency of the Delta Network. The state sizes are given in millions.

Model

Prms. State size
in M.

in M.

Perplexity

Valid

Test

Linear Transformer
Delta Network

Transformer-XL

89.8
89.9

90.9

0.13
0.13

0.13
1.05
2.10
6.29

>260 >260
29.4

27.8

65.7
29.3
26.4
24.6

65.5
30.1
27.4
25.5

troduces a few extra computations. The wall clock time and
memory requirement (for the small LM setting) for the Lin-
ear Transformer with and without our delta update rule are:
63 K and 66 K words/sec, and 14 and 13 GB respectively in
our implementation. The extra resource requirement is thus
marginal. As we use custom CUDA kernels for these linear
Transformers, they are faster than the regular Transformers
implemented in PyTorch which process 33K words/sec and
require 17 GB memory. The speed of the DPFP and Per-
former models (for Table 5 in Appendix with a larger ddot)
are 63 K and 57 K words/sec. Performers are slower because
of the sampling logic, which also motivates our DPFP.

Without truncating context. Given the constant space
requirements, we can feed inputs to linear Transformers
for an arbitrary number of steps. To properly assess the
model’s ability to process arbitrary long sequences, it is
crucial to make the training consistent with the evaluation
mode (Irie et al., 2019b). During training, we carry over
the fast weight memory from one training segment to the
following one, while still limiting the backpropagation span
to be within the segment. We train a Delta Net, using neither
positional encoding nor attention normalisation (the best
setting from Table 3). It was crucial to remove the attention
normalisation for the Delta Net since the accumulator blows
up as indicated in Sec. 4.2, while for the Linear Transformer,
removing it resulted in an even worse perplexity of over
1600. Table 4 shows the corresponding results. The Delta
Net yields a slight improvement over the best model with a
limited context window (Table 3), unlike the baseline Linear
Transformer model with the naive sum update rule which
breaks. We also train a Transformer-XL in our medium
conﬁguration as a baseline model speciﬁcally designed for

this use case (Dai et al., 2019; Rae et al., 2020). We evaluate
it using different state sizes by changing the Transformer
XL’s memory and target segment lengths (see Appendix F
for further details). Performance of the Delta Net does not
yet match the performance of the Transformer XL when
the latter is evaluated with a large state size (large attention
window). However, when we take the state size into account
(Table 4), we observe that the Delta Net performs very
well with a small state size, which is a crucial property in
some practical applications (Irie et al., 2020). These results
are promising for future work on alternative Transformer
models which can run for an unlimited number of steps.

7. Conclusion

We emphasise the connection between linearised self-
attention and Fast Weight Programmers (FWPs, 1991) that
program their fast weight memories through sequences of
outer products between self-invented key and value pat-
terns. The FWP perspective allows for discussing associa-
tive memory capacity limitations of linear attention, and
for introducing an alternative differentiable elementary pro-
gramming instruction that the FWP can use to dynamically
edit the memory, akin to the famous delta rule, but such that
the FWP can learn to use the rule wisely through gradient
descent. We also propose and discuss a new method for
linearising attention. Experiments on synthetic and real lan-
guage tasks demonstrate the effectiveness of our proposals.
The FWP perspective opens up new avenues for investigat-
ing even better programming instructions and designs for
Transformers with ﬁnite memory.

Acknowledgements

We thank Sjoerd van Steenkiste, Hubert Ramsauer and Sepp
Hochreiter for valuable comments and suggestions on the
ﬁrst version of the manuscript. This research was partially
funded by ERC Advanced grant no: 742870, project Algo-
RNN, and by Swiss National Science Foundation grant no:
200021 192356, project NEUSYM. We thank NVIDIA Cor-
poration for donating several DGX machines, and IBM for
donating a Minsky machine. We also thank Katharopoulos
et al. (2020) for releasing their CUDA implementation of
Linear Transformers, which was helpful to implement our
models.

References

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,
L. Character-level language modeling with deeper self-
attention. In Proc. Conference on Artiﬁcial Intelligence
(AAAI), pp. 3159–3166, Honolulu, HI, USA, January
2019.

Linear Transformers Are Secretly Fast Weight Programmers

Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu, C.
Using fast weights to attend to the recent past. In Proc. Ad-
vances in Neural Information Processing Systems (NIPS),
pp. 4331–4339, Barcelona, Spain, December 2016.

Baevski, A. and Auli, M. Adaptive input representations
for neural language modeling. In Int. Conf. on Learning
Representations (ICLR), New Orleans, LA, USA, May
2019.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate. In Int.
Conf. on Learning Representations (ICLR), San Diego,
CA, USA, May 2015.

Bello, I. Lambdanetworks: Modeling long-range interac-
tions without attention. In Int. Conf. on Learning Repre-
sentations (ICLR), Virtual only, May 2021.

Cheng, J., Dong, L., and Lapata, M. Long short-term
memory-networks for machine reading. In Proc. Conf.
on Empirical Methods in Natural Language Processing
(EMNLP), pp. 551–561, Austin, TX, USA, November
2016.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,
A., Kaiser, L., et al. Rethinking attention with performers.
In Int. Conf. on Learning Representations (ICLR), Virtual
only, 2021.

Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast
and accurate deep network learning by exponential linear
units (ELUs). In Int. Conf. on Learning Representations
(ICLR), San Juan, Puerto Rico, May 2016.

Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J.,
Le, Q. V., and Salakhutdinov, R. Transformer-XL: Atten-
tive language models beyond a ﬁxed-length context. In
Proc. Association for Computational Linguistics (ACL),
pp. 2978–2988, Florence, Italy, July 2019.

Demircigil, M., Heusel, J., L¨owe, M., Upgang, S., and
Vermet, F. On a model of associative memory with huge
storage capacity. Journal of Statistical Physics, 168(2):
288–299, 2017.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. In Proc. North American Chapter
of the Association for Computational Linguistics on Hu-
man Language Technologies (NAACL-HLT), pp. 4171–
4186, Minneapolis, MN, USA, June 2019.

Feldman, J. A. Dynamic connections in neural networks.

Biological cybernetics, 46(1):27–39, 1982.

Galanti, T. and Wolf, L. On the modularity of hypernet-
works. In Proc. Advances in Neural Information Process-
ing Systems (NeurIPS), Virtual only, 2020.

Graves, A. Generating sequences with recurrent neural

networks. Preprint arXiv:1308.0850, 2013.

Greff, K., van Steenkiste, S., and Schmidhuber, J. On the
binding problem in artiﬁcial neural networks. Preprint
arXiv:2012.05208, 2020.

Ha, D., Dai, A., and Le, Q. V. Hypernetworks. In Int. Conf.
on Learning Representations (ICLR), Toulon, France,
April 2017.

Hanson, S. J. A stochastic version of the delta rule. Physica

D: Nonlinear Phenomena, 42(1-3):265–272, 1990.

Hebb, D. O. The organization of behavior: a neuropsy-
cholocigal theory. A Wiley Book in Clinical Psychology,
62:78, 1949.

Hinton, G. E. and Plaut, D. C. Using fast weights to de-
blur old memories. In Proc. Conf. of Cognitive Science
Society, pp. 177–186, Seatle, WA, USA, July 1987.

Hochreiter, S. and Schmidhuber, J. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

Hopﬁeld, J. J. Neural networks and physical systems with
emergent collective computational abilities. Proc. of the
national academy of sciences, 79(8):2554–2558, 1982.

Irie, K., Zeyer, A., Schl¨uter, R., and Ney, H. Language
modeling with deep Transformers. In Proc. Interspeech,
pp. 3905–3909, Graz, Austria, September 2019a.

Irie, K., Zeyer, A., Schl¨uter, R., and Ney, H. Training lan-
guage models for long-span cross-sentence evaluation. In
Proc. IEEE Automatic Speech Recog. and Understand-
ing Workshop (ASRU), pp. 419–426, Sentosa, Singapore,
December 2019b.

Irie, K., Gerstenberger, A., Schl¨uter, R., and Ney, H. How
much self-attention do we need? Trading attention for
feed-forward layers. In Proc. IEEE Int. Conf. on Acous-
tics, Speech and Signal Processing (ICASSP), pp. 6154–
6158, Virtual only, May 2020.

Jia, X., De Brabandere, B., Tuytelaars, T., and Gool, L. V.
Dynamic ﬁlter networks. In Proc. Advances in Neural
Information Processing Systems (NIPS), pp. 667–675,
Barcelona, Spain, 2016.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
In Proc. Int. Conf. on Machine
with linear attention.
Learning (ICML), Virtual only, July 2020.

Linear Transformers Are Secretly Fast Weight Programmers

Kingma, D. P. and Ba, J. Adam: A method for stochastic

optimization. Preprint arXiv:1412.6980, 2014.

Kirsch, L. and Schmidhuber, J. Meta learning backprop-
agation and improving it. NeurIPS Workshop on Meta-
Learning, 2020.

Klein, B., Wolf, L., and Afek, Y. A dynamic convolu-
tional layer for short rangeweather prediction. In Proc.
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), pp. 4840–4848, Boston, MA, USA, June 2015.

Kohonen, T. Correlation matrix memories. IEEE Transac-

tions on Computers, 21(4):353–359, 1972.

Kosko, B. Bidirectional associative memories. IEEE Trans-
actions on Systems, Man, and Cybernetics, 18(1):49–60,
1988.

Krotov, D. and Hopﬁeld, J. J. Dense associative memory
In Proc. Advances in Neural
for pattern recognition.
Information Processing Systems (NIPS), pp. 1172–1180,
Barcelona, Spain, December 2016.

Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou,
B., and Bengio, Y. A structured self-attentive sentence
embedding. In Int. Conf. on Learning Representations
(ICLR), Toulon, France, April 2017.

Little, W. A. The existence of persistent states in the brain.

Mathematical biosciences, 19(1-2):101–120, 1974.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
sentinel mixture models. In Int. Conf. on Learning Rep-
resentations (ICLR), Toulon, France, April 2017.

Miconi, T., Stanley, K., and Clune, J. Differentiable plastic-
ity: training plastic neural networks with backpropaga-
tion. In Proc. Int. Conf. on Machine Learning (ICML),
pp. 3559–3568, Stockholm, Sweden, July 2018.

Miconi, T., Rawal, A., Clune, J., and Stanley, K. O. Back-
propamine: training self-modifying neural networks with
In Int. Conf.
differentiable neuromodulated plasticity.
on Learning Representations (ICLR), New Orleans, LA,
USA, May 2019.

Munkhdalai, T. and Trischler, A. Metalearning with hebbian

fast weights. Preprint arXiv:1807.05076, 2018.

Noh, H., Seo, P. H., and Han, B. Image question answering
using convolutional neural network with dynamic param-
eter prediction. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), pp. 30–38, Las Vegas,
NV, USA, 2016.

Ott, M., Edunov, S., Grangier, D., and Auli, M. Scaling
neural machine translation. In Proc. Conf. on Machine
Translation: Research Papers, pp. 1–9, Brussels, Bel-
gium, October 2018.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M. fairseq: A fast, extensible
toolkit for sequence modeling. In Proc. North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technologies (NAACL-HLT),
Demonstrations, pp. 48–53, Minneapolis, MN, USA,
June 2019.

Palm, G. On associative memory. Biological cybernetics,

36(1):19–31, 1980.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:
a method for automatic evaluation of machine transla-
tion. In Proc. Association for Computational Linguistics
(ACL), pp. 311–318, Philadelphia, PA, USA, July 2002.

Parikh, A. P., T¨ackstr¨om, O., Das, D., and Uszkoreit, J. A
decomposable attention model for natural language infer-
ence. In Proc. Conf. on Empirical Methods in Natural
Language Processing (EMNLP), pp. 2249–2255, Austin,
TX, USA, November 2016.

Paszke, A. et al. Pytorch: An imperative style, high-
performance deep learning library. In Proc. Advances in
Neural Information Processing Systems (NeurIPS), pp.
8026–8037, Vancouver, Canada, December 2019.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,
N. A., and Kong, L. Random feature attention. In Int.
Conf. on Learning Representations (ICLR), Virtual only,
2021.

Perez, E., Strub, F., De Vries, H., Dumoulin, V., and
Courville, A. FiLM: Visual reasoning with a general
In Proc. AAAI Conf. on Artiﬁcial
conditioning layer.
Intelligence, pp. 3942–3951, New Orleans, LA, USA,
February 2018.

Munkhdalai, T. and Yu, H. Meta networks. In Proc. Int.
Conf. on Machine Learning (ICML), pp. 2554–2563, Syd-
ney, Australia, August 2017.

Post, M. A call for clarity in reporting BLEU scores. In Proc.
Conf. on Machine Translation, pp. 186–191, Brussels,
Belgium, October 2018.

Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A.
Metalearned neural memory. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 13310–
13321, Vancouver, Canada, December 2019.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multi-
task learners. [Online]. : https://blog.openai.com/better-
language-models/, 2019.

Linear Transformers Are Secretly Fast Weight Programmers

Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
and Lillicrap, T. P. Compressive transformers for long-
In Int. Conf. on Learning
range sequence modelling.
Representations (ICLR), Virtual only, April 2020.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overﬁtting. The Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

Ramsauer, H., Sch¨aﬂ, B., Lehner, J., Seidl, P., Widrich, M.,
Gruber, L., Holzleitner, M., Adler, T., Kreil, D., Kopp,
M. K., Klambauer, G., Brandstetter, J., and Hochreiter, S.
Hopﬁeld networks is all you need. In Int. Conf. on Learn-
ing Representations (ICLR), Virtual only, May 2021.

Schlag, I. and Schmidhuber, J. Gated fast weights for on-
the-ﬂy neural program generation. In NIPS Metalearning
Workshop, Long Beach, CA, USA, December 2017.

Schlag, I. and Schmidhuber, J. Learning to reason with
third order tensor products. In Proc. Advances in Neural
Information Processing Systems (NIPS), pp. 9981–9993,
Montr´eal, Canada, December 2018.

Schlag, I., Munkhdalai, T., and Schmidhuber, J. Learning
associative inference using fast weight memory. In Int.
Conf. on Learning Representations (ICLR), Virtual only,
May 2021.

Schmidhuber, J. Learning to control fast-weight memories:
An alternative to recurrent nets. Technical Report FKI-
147-91, Institut f¨ur Informatik, Technische Universit¨at
M¨unchen, March 1991.

Schmidhuber, J. Learning to control fast-weight memories:
An alternative to dynamic recurrent networks. Neural
Computation, 4(1):131–139, 1992.

Schmidhuber, J. Reducing the ratio between learning com-
plexity and number of time varying variables in fully
recurrent nets. In International Conference on Artiﬁcial
Neural Networks (ICANN), pp. 460–463, Amsterdam,
Netherlands, September 1993.

Steinbuch, K. Die lernmatrix. Kybernetik, 1(1):36–45, 1961.

Steinbuch, K. and Piske, U. A. W. Learning matrices and
their applications. IEEE Transactions on Electronic Com-
puters, 12(6):846–862, 1963.

Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
range arena: A benchmark for efﬁcient transformers. In
Int. Conf. on Learning Representations (ICLR), Virtual
only, May 2021.

Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and
Salakhutdinov, R. Transformer dissection: An uniﬁed
understanding for transformer’s attention via the lens of
kernel. In Proc. Conf. on Empirical Methods in Natural
Language Processing (EMNLP), pp. 4344–4353, Hong
Kong, China, November 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In Proc. Advances in Neural Information
Processing Systems (NIPS), pp. 5998–6008, Long Beach,
CA, USA, December 2017.

von der Malsburg, C. The correlation theory of brain func-
tion. Internal Report 81-2, G¨ottingen: Department of Neu-
robiology, Max Planck Intitute for Biophysical Chemistry,
1981.

Widrow, B. and Hoff, M. E. Adaptive switching circuits.
In Proc. IRE WESCON Convention Record, pp. 96–104,
Los Angeles, CA, USA, August 1960.

Schmidhuber, J. 26 March 1991: Neural nets learn to
program neural nets with fast weights—like today’s
Transformer variants. 2021: New stuff!, AI Blog, 2021.
URL https://people.idsia.ch/˜juergen/
fast-weight-programmer-1991-transformer.
html.

Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. In Proc. As-
sociation for Computational Linguistics (ACL), pp. 1715–
1725, Berlin, Germany, August 2016.

Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efﬁcient
attention: Attention with linear complexities. Preprint
arXiv:1812.01243, 2018.

Smolensky, P. Tensor product variable binding and the
representation of symbolic structures in connectionist
systems. Artiﬁcial intelligence, 46(1-2):159–216, 1990.

Linear Transformers Are Secretly Fast Weight Programmers

A. Update Rule Derivation

A.1. The Update Rule

basis k =

dkey
(cid:88)

i=1

kie(i) with ki ∈ R, we obtain:

Here we provide the intermediate steps from Eq. 23 to
Eq. 24.

W (i) = W (i−1) +v(i)

new ⊗ φ(k(i))
(cid:123)(cid:122)
(cid:125)
write
= W (i−1) + β(i)(v(i) − ¯v(i)) ⊗ φ(k(i))

−¯v(i) ⊗ φ(k(i))
(cid:124)
(cid:123)(cid:122)
(cid:125)
remove

(cid:124)

(23)

(24)

By grouping the last two terms, Eq. 23 becomes:

W (i) = W (i−1) + (v(i)

new − ¯v(i)) ⊗ φ(k(i))

(38)

By using the deﬁnition of v(i)

new from Eq. 22:

new = β(i)v(i) + (1 − β(i))¯v(i)
v(i)

(22)

we obtain:

v(i)
new − ¯v(i) = β(i)v(i) + (1 − β(i))¯v(i) − ¯v(i)

= β(i)(v(i) − ¯v(i))

(39)

(40)

By substituting this expression to Eq. 38, we obtain Eq. 24

.

A.2. Key Sum Normalisation

By considering one-hot vectors {e(1), ..., e(i), ..., e(dkey)}
which form the Cartesian basis of Rdkey , any matrix W ∈
Rdvalue×dkey can be written as

W =

dkey
(cid:88)

i=1

w(i) ⊗ e(i)

(41)

where {w(1), ..., w(i), ..., w(dkey)} are the column vectors of
W . In the context of associative memory, we can interpret
this expression as a set of associations with ﬁxed keys e(i)
and the associated values w(i).

In this view, any update of W can be written as updates
of each w(i). This perspective allows us to derive the sum
normalisation of Sec. 4.2. For that, we start by deriving the
update of w(i).

Given an arbitrary weight W , we consider updating it to
W (cid:48) by adding a new association (k, v) using our update
rule of Sec. 4.2 (where we omit β):

¯v = W k

W (cid:48) = W + (v − ¯v) ⊗ k

(42)

(43)

By substituting k in Eq. 43 by its expression in the Cartesian

W (cid:48) = W + (v − ¯v) ⊗

dkey
(cid:88)

i=1

kie(i)

= W +

dkey
(cid:88)

i=1

ki(v − ¯v) ⊗ e(i)

(44)

(45)

Now by substituting W by its expression of Eq. 41:

W (cid:48) =

=

dkey
(cid:88)

i=1
dkey
(cid:88)

i=1

w(i) ⊗ e(i) +

dkey
(cid:88)

i=1

ki(v − ¯v) ⊗ e(i)

(46)

(cid:0)w(i) + ki(v − ¯v)(cid:1) ⊗ e(i)

The column-wise update is thus:

w(cid:48)(i) = w(i) + ki(v − ¯v)

We can explicitly write down ¯v as:

(47)

(48)

¯v = W k = W

dkey
(cid:88)

j=1

kje(j) =

dkey
(cid:88)

j=1

kjw(j)

(49)

which we can substitute in Eq. 48 to obtain:

w(cid:48)(i) = w(i) + ki(v −

dkey
(cid:88)

j=1

kjw(j))

(50)

= w(i) + kiv −

dkey
(cid:88)

j=1

kikjw(j)

(51)

In Eq. 51, the weight ki on the positive term v is in gen-
eral not equal to the total weights on the negative terms
(cid:80)dkey
j=1 kikj. We can force these weights to be balanced by

introducing the normalisation:

If ki is non zero, we obtain:

dkey
(cid:88)

j=1

kikj = ki.

dkey
(cid:88)

j=1

kj = 1

This corresponds to the sum normalisation we introduced
in Sec. 4.2

.

B. Formal comparison to Peng et al. (2021)

Concurrently to our work, Peng et al. (2021) proposed the
following gated update rule:

W (i) = (1 − β(i))W (i−1) + β(i)v(i) ⊗ φ(k(i))

(52)

Linear Transformers Are Secretly Fast Weight Programmers

which is motivated by the gating mechanism in recurrent
neural networks (Hochreiter & Schmidhuber, 1997). In
contrast, our update rule of Eq. 24

W (i) = W (i−1) + β(i)(v(i) − ¯v(i)) ⊗ φ(k(i))

(24)

is driven by an associative memory perspective, relates to
the famous error-correcting delta rule, and offers a crucial
property.

To illustrate a similarity and a crucial difference between the
two update rules, we consider a fast weight matrix W which
is constructed by two associations (k1, v1) and (k2, v2), i.e.

W = v1 ⊗ k1 + v2 ⊗ k2

(53)

where we assume k1 and k2 to be orthonormal, and we
omit φ. Now we consider updating W to W (cid:48) by adding a
new association (k3, v3) where k3 = k2. Using Peng et al.
(2021)’s update rule, we have:

W (cid:48) = (1 − β)W + βv3 ⊗ k3

This rule thus updates the value associated with the key
k2 = k3 to be a convex combination of the old and the new
values (1 − β)v2 + βv3:

W (cid:48)k3 = (1 − β)W k3 + βv3

= (1 − β)v2 + βv3

However, it also modiﬁes or in the worst case erases the
value associated with the key k1:

W (cid:48)k1 = (1 − β)W k1 = (1 − β)v1

In contrast, using our update rule, we have:

W (cid:48) = W + β(v3 − v2) ⊗ k3

since ¯v = W k3 = W k2 = v2.
Our rule thus also updates the value associated with the key
k2 = k3 to be a convex combination of the old and the new
values (1 − β)v2 + βv3:

W (cid:48)k3 = W k3 + β(v3 − v2)

= v2 + β(v3 − v2)
= (1 − β)v2 + βv3

7

8

9

10

C. DPFP-ν Implementation

Listing 1 is a simple PyTorch implementation of DPFP-ν
(Eq. 37) which consist of two concatenations followed by
one element-wise multiplication.

1 import torch
2 from torch import cat
3 from torch.nn.functional import relu as r
4
5 def dpfp(x, nu=1):
6

x = cat([r(x), r(-x)], dim=-1)
x_rolled = cat([x.roll(shifts=j, dims=-1)
for j in range(1,nu+1)], dim=-1)

x_repeat = cat([x] * nu, dim=-1)
return x_repeat * x_rolled

Listing 1. Simple PyTorch implementation of DPFP-ν (Eq. 37).

D. Additional Experimental Results

In this section, we provide additional experimental results
which we could not include in the main paper because of
space limitations.

D.1. Synthetic Task Setting 1

Figure 4 shows learning curves for the synthetic setting 1
(without replacement) with 600 unique keys and values. The
scripts used to generate such ﬁgures can be found in our
GitHub repository.

Figure 4. Training curves for setting 1 with 600 unique keys/values
(sampled without replacement) as described in Sec. 6.1.1.

while crucially, it keeps the value associated with k1 un-
modiﬁed:

W (cid:48)k1 = W k1 = v1

Our update rule thus differs from Peng et al. (2021)’s one on
this property of updating associations while keeping other
“unrelated” ones intact in an associative memory.

D.2. Synthetic Task Setting 2

Figure 5 is a capacity plot for setting 2 with an increasing
number of unique keys and queries (analogous to Figure 2
of setting 1 apart from the log-scale of the y-axis). We did
not include FAVOR+ in this plot, because its combination
with our update rule resulted in not-a-number in this setting.

Linear Transformers Are Secretly Fast Weight Programmers

Table 5. WikiText-103 language model perplexity results showing
effects of our update rule in non-overcapacity regime. The number
of trainable parameters are almost the same for all models, up to
the small difference introduced by gating in our update rule (16 K
parameters). The small conﬁg is used, i.e. D = 128, L = 256
(40 M parameters). We set m = 16 for the Performers and ν = 1
for the DPFP models, which result in ddot = 256 for both cases.
The model is thus not necessary in an overcapacity regime.

Update
Rule

Transformer

-

Performer

DPFP

sum
delta

sum
delta

small

Valid

33.0

38.0
36.0

37.7
33.9

Test

34.1

38.8
37.0

38.8
35.0

former paper (Vaswani et al., 2017): the model has 6 layers
each in the encoder and the decoder, with a hidden layer size
of 1024 with 16 attention heads, 4096-dimensional feed-
forward layers, using 32 K byte-pair encoding sub-word
units (Sennrich et al., 2016). FAIRSEQ provides a training
conﬁguration for the corresponding model (Ott et al., 2018),
which we adapted for our infrastructure. We trained our
models on three GPUs using a batch size of up to 3584
tokens per GPU and accumulating gradients over 16 batches
for 45 epochs, and selected the best model based on the
validation BLEU score. In Table 1, we directly report BLEU
for different values of ddot; Table 6 provides the conversion
from hyper-parameters m of Performers or ν in the DPFP
to ddot.

Table 6. Relation between dot product space dimension and the
hyper-parameters in the Performer and our DPFP models. dkey =
64 in all our translation models.

ddot

256

Performer m 128
DPFP ν
2

384

192
3

512

256
4

F. Details on Language Modelling

Experiments

Implementation notes. All our implementations are
based on PyTorch (Paszke et al., 2019). Our base language
modelling code has been developed by using the public code
by Dai et al. (2019) for Transformer-XL as a starting point.
For φ functions, we ported the same implementation we
used for our translation experiments. For the implementa-
tion of our update rule, we modiﬁed the CUDA kernel for the
Linear Transformer made publicly available by Katharopou-
los et al. (2020). We note that a custom implementation of

Figure 5. Final evaluation loss on synthetic setting 2 (with replace-
ment) problems with the total number of unique associations rang-
ing from 20 to 200. Each individual symbol is a model trained
until convergence as described in Sec. 6.1.2. In all problems, with
different sequence lengths and a different number of unique keys,
our update rule outperforms all other approaches.

D.3. Language Modelling

In Sec. 6.3, we evaluated our update rule when the model
is under overcapacity regime. Here we present an extra
language modelling experiment which evaluate the bene-
ﬁts of our update rule in non-overcapacity scenarios. This
also allows us to include DPFP in the evaluation. We train
both, Performer and DPFP, in the small setting (D = 128,
L = 256) with m = 16 and ν = 1, resulting in ddot = 256
for both cases. Table 5 shows the perplexity results. First
we observe that the Performer and DPFP baseline mod-
els with the sum update rule do not outperform the Linear
Transformer baseline from Table 2. In fact, language mod-
elling might be less affected by the capacity issue than the
synthetic retrieval task, as it might not require the exact
retrieval. Second we observe that our update rule improves
both variants of linear attention over the sum update-rule
baselines even in this condition. This indicates the general
beneﬁts of our update rule in Fast Weight Programmers. We
note that the improvement is larger for the DPFP model
than for the Performer. This is similar to Table 2 where our
update rule improves the deterministic Linear Transformers
more than the Performers. Finally, we note that we also
tried the DPFP and Performer models with an increased
ddot by setting ν = 2 and m = 32 respectively. While this
increases ddot by a factor of two, it was not beneﬁcial for
this language modelling setting.

E. Details on Machine Translation

Experiments

We implemented different φ functions in the FAIRSEQ tookit
(Ott et al., 2019). The Transformer architecture used in the
experiment is the one referred to as big in the original Trans-

Linear Transformers Are Secretly Fast Weight Programmers

the backward pass for fast weights is crucial for language
modelling. A naive backward computation generated by au-
tomatic differentiation would store the fast weights for each
time step, which can quickly hit the GPU memory limit.
The custom implementation ensures that we need to store
only one set of weights by recomputing the fast weights
needed for computing the gradients for each time step in
the backward pass (which still remains time-efﬁcient as the
operations involved in the computation of our fast weights
are rather inexpensive).

Experimental details. Here we provide extra experimen-
tal details to complement the descriptions of Sec. 6.3. For
the small and medium conﬁgurations, we use batch sizes
of 96 and 56 sequences, respectively, and train for about
120 and 70 epochs. In both settings, we apply 10% dropout
(Hanson, 1990; Srivastava et al., 2014), and train using the
Adam optimiser (Kingma & Ba, 2014) with an initial learn-
ing rate of 0.00025 and 2000 learning rate warm-up steps.
For further details, we refer the readers to our code. For
experiments with Transformer-XL (Table 4), we train it
with the same backpropagation span as our models (i.e. 384
words in the medium conﬁguration). The model is trained
with memory and target segment lengths of 384. The mod-
els with different state sizes in Table 4 are obtained by using
different Transformer-XL memory segment lengths at eval-
uation time. The models with state sizes of 1.05 M, 2.10 M,
and 6.29 M are obtained by using memory and target lengths
of 64, 128, and 384, respectively. The model with a state
size of 0.13 M uses a memory length of 15 and a target
length of 1. Like for other models, a batch size of 1 is used
for evaluating the Transformer XL. The state sizes in Table
4 are computed as follows. The per-layer state size of the
Linear Transformer and the Delta Net are: number of heads
(here 8) × fast weight matrix size which is per-head key
dimension (here 32) × per-head value dimension (here 32).
This yields a total size of 8,192. The per-layer state size of
the Transformer XL is: memory segment length × target
segment length × (total key dimension, here 256 + total
value dimension, here 256). We obtain the total state size
we report in Table 4 by multiplying the per-layer state size
by the number of layers which is 16 for all our models.

