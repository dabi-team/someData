â€œProgrammingâ€ Quantum Hardware via 
Levenberg Marquardt Machine Learning 

James E Steck  
Dept. of Aerospace Engineering 
Wichita State University 
Wichita, KS USA 
ORCID 0000-0002-6195-9122 

Nathan L. Thompson 
Dept. of Mathematics, Statistics and Physics 
Wichita State University 
Wichita, KS USA 

Elizabeth C Behrman 
Dept. of Mathematics, Statistics and Physics 
Wichita State University 
Wichita, KS USA  

Abstractâ€” Significant challenges remain with the development 
of macroscopic quantum computing: hardware problems of noise, 
decoherence, and scaling; software problems of error correction; 
and,  most  important,  algorithm  construction.  Finding  truly 
quantum  algorithms  is  quite  difficult,  and  many  quantum 
algorithms,  like  Shorâ€™s  prime  factoring  or  phase  estimation, 
require extremely long circuit depth for any practical application, 
necessitating error correction. Machine learning can be used as a 
systematic  method  to  non-algorithmically  â€œprogramâ€  quantum 
computers.  Quantum  machine  learning  enables  us  to  perform 
computations  without  breaking  down  an  algorithm  into  its  gate 
â€œbuilding  blocksâ€,  eliminating  that  difficult  step  and  potentially 
reducing unnecessary complexity. In addition, we have shown that 
our  machine  learning  approach  is  robust  to  both  noise  and  to 
decoherence, which is ideal for running on inherently noisy NISQ 
devices  which  are  limited  in  the  number  of  qubits  available  for 
error  correction.  We  demonstrated  this  using  a  fundamentally 
non-classical 
the 
entanglement  of  an  unknown  quantum  state.  Results  from  this 
have been successfully ported to the IBM hardware and trained 
using a powerful hybrid reinforcement learning technique which 
is  a  modified  Levenberg-Marquardt  (LM)  method.  The  LM 
method is ideally suited to quantum machine learning as it only 
requires knowledge of the final measured output of the quantum 
computation, not intermediate quantum states which are generally 
learning  data 
not  accessible.  Since 
simultaneously,  it  also  requires  significantly  fewer  hits  on  the 
quantum  hardware.  Machine  learning  is  demonstrated  with 
results  from  simulations  and  runs  on  the  IBM  Qiskit  hardware 
interface.  

it  processes  all 

experimentally 

calculation: 

estimating 

the 

Keywordsâ€”quantum machine learning, Levenberg-Marquardt, 

entanglement  

I.  INTRODUCTION  

For  over  thirty  years  now  the  scientific  community  has 
known that quantum computing has the potential to take a giant 
leap  forward  in  computing  capabilities,  solving  problems 
difficult or even impossible to solve  classically. Recently the 
broader  communityâ€™s  interest  has  also  been  engaged,  and 
startups  and  investment  are  growing.  But  there  remain  major 
obstacles  to  the  implementation  of  macroscopic  quantum 
computing:  hardware  problems  of  noise,  decoherence,  and 

transform  methods, 

scaling;  and  software  problems  of  error  correction  and,  most 
important,  algorithm  construction.  Finding  truly  quantum 
algorithms turns out to be quite difficult. There are still only a 
very  few.  Most  are  based  on  one  of  three  methods:  quantum 
Fourier 
like  Shorâ€™s  [1];  amplitude 
amplification, like Groverâ€™s [2]; and quantum walks [3]. Shorâ€™s, 
and  some  quantum  walk  algorithms,  provide  an  exponential 
advantage over the best-known classical algorithm, and this is 
usually  what  people  are  thinking  of  for  the  really  hard 
problems. But ampl1itude amplification only gives a quadratic 
speedup.  It  is  not  known  whether  a  quantum  advantage  even 
exists  for  broad  classes  of  problems  [4]  [5],  or,  if  it  does, 
whether it will be exponential or somewhat weaker.  Even when 
the  existence  of  an  advantage  can  be  proved,  explicit 
construction of the algorithm is still not easy let alone obvious. 

Now, the usual algorithmic approach is a kind of â€œbuilding 
blockâ€  strategy,  in  which  the  procedure  is  formulated  as  a 
sequence of steps (quantum gates) from a universal set, e.g., a 
sequence of CNOT, Hadamard, and phase shift gates. This is of 
course  exactly  analogous  to  the  way  in  which  we  usually  do 
classical  computing,  as  a  series  of  logical  gates  operating  on 
bits. But there is another computing paradigm we could follow: 
distributed  computing,  the  approach  of  biological  and  of 
artificial neural networks. Since the 1990s our research group 
has been investigating this different approach, a combining of 
quantum  computing  and  artificial  neural  networks,  as  an 
alternative to the building block paradigm. With this approach, 
the  quantum  systems  itself  learns  how  to  solve  the  problem, 
designing  its  own  algorithm  in  a  sense.  Moreover,  we  have 
shown [6] [7] that not only does this give us obviation of the 
program  design  obstacle,  but  also  gives  us  near-automatic 
scaling  [8],  robustness  to  noise  and  to  decoherence  [9],  and 
speedup over classical learning [10].  

The basic idea is that a quantum system can itself act as a 
neural network: The state of the system at the initial time is the 
â€œinputâ€; a measurement (observable) on the system at the final 
time  is  the  â€œoutputâ€;  the  states  of  the  system  at  intermediate 
times are the hidden layers of the network. If we know enough 

1 This work used the Extreme Science and 
Engineering Discovery Environment (XSEDE), 

which is supported by National Science 
Foundation grant number ACI-1548562. 

XXX-X-XXXX-XXXX-X/XX/$XX.00 Â©20XX IEEE 

 
 
 
 
 
 
 
 
about  the  computation  desired  to  be  able  to  construct  a 
comprehensive set of input-output pairs from which the net can 
generalize, then, we can use techniques of machine learning to 
bypass the algorithm-construction problem.  

is  an 

Entanglement 

intrinsically  quantum  mechanical 
property,  essential  for  most  quantum  speedups.  It  can  be 
calculated,  given  the  state  of  the  system  (wavefunction  or 
density matrix) for a two-qubit system, but if the state of the 
system is unknown, determination of the entanglement requires 
quantum tomography. And for larger systems than two qubits, 
determination of the entanglement is an NP-hard problem [11] 
Thus,  entanglement  estimation  is  a  good  example  of  a 
nontrivial,  intrinsically  quantum  mechanical,  calculation  for 
which we have no general algorithm [12].  

trained.  We 

In previous work we succeeded in finding a time dependent 
Hamiltonian  for  a  multiqubit  system  such  that  a  chosen 
measurement at the final time  is a witness of the entanglement 
of the initial state of the quantum system [7] [8]. The â€œoutputâ€ 
(result of the measurement of the witness at the final time) will 
change depending on the time evolution of the system, which is 
of  course  controlled  by  the  Hamiltonian:  by  the  tunneling 
amplitudes,  the  qubit  biases,  and  the  qubit-qubit  coupling. 
Thus, we can consider these parameters to be the â€œweightsâ€ to 
be 
then  use  a  quantum  version  [7]  of 
backpropagation  [13]  to  find  optimal  functions  such  that  the 
desired  mapping  is  achieved.  (It  should  be  noted  that  the 
method  of  quantum  backprop  has  recently  [14]  [15]  been 
rediscovered by several groups.) Full details are provided in [7]. 
From a training set of only four pure states, our quantum neural 
network successfully generalized the witness to large classes of 
states,  mixed  as  well  as  pure  [8].  Qualitatively,  what  we  are 
doing  is  using  machine  learning  techniques  to  find  a  â€œbestâ€ 
hyperplane  to divide separable states from entangled ones,  in 
the Hilbert space.  

Of course, this method is necessarily â€œofflineâ€ training, since 
it is not possible to do backpropagation without knowing the 
state  of  the  the  system  at  intermediate  times  (in  the  hidden 
layers);  quantum  mechanically,  this  is  impossible  without 
collapsing  the  wavefunction  and  thereby  destroying  the 
superposition,  which  rather  obviates  the  whole  purpose  of 
doing quantum computation. That is, quantum backpropagation 
can  only  be  done  on  an  (auxiliary)  classical  computer, 
simulating  the  quantum  computer,  and  this  simulation  will 
necessarily  contain  uncertainties  and  errors  in  modeling  the 
behavior  of  the  actual  quantum  computer.  The  results  from 
offline quantum backpropagation, can, of course, be used as a 
good  starting  point  for  true  online  quantum  learning,  where 
reinforcement learning is used to correct for uncertainty, noise, 
and  decoherence  in  the  actual  hardware  of  the  quantum 
computer. Here, we present such a method, port it to the IBM 
Qiskit system [16], and demonstrate its effectiveness. The next 
section introduces our machine learning method for deep time 
quantum  networks,  including  our  original  quantum  backprop 
compatible 
equivalent 
method 

hardware 

and 

our 

implementation  of  the  results.  In  Section  III  we  describe  a 
learning model implementable on hardware, a finite difference 
method, and show results on both Matlab simulation and Qiskit. 
In  Section  IV  we  develop  our  new  model  using  Levenberg 
Marquardt,  and  again  show  and  compare  results  using  both 
Matlab  and  Qiskit.  We  conclude  in  Section  V.  This  is  an 
extension of work previously presented here [17]. 

II.  MACHINE LEARNING FOR DEEP TIME QUANTUM NETWORKS 

A.  Machine Learning in Simulation 
A general quantum state is mathematically represented by its 
density matrix, Ï, whose time evolution obeys the SchrÃ¶dinger 
equation 

ğ‘‘ğœŒ

ğ‘‘ğ‘¡

=

1

ğ‘–â„

[ğ», ğœŒ]  

(1) 

where H is the Hamiltonian and â„ is Planckâ€™s constant divided 
by 2Ï€. The formal solution of the equation is 

ğœŒ(ğ‘¡)   = ğ‘’ğ‘–â„’ğ‘¡ğœŒ(ğ‘¡0)   

(2) 

1

â„

where â„’is  the  Liouville  operator,  defined  as  the  commutator 
[ğ»,   ].  We can think 
with the Hamiltonian in units of â„,  â„’ =  
of  (1) as analogous to the equation for information propagation 
in a neural network, Output = FW*Input, where FW represents 
the action of the network as it acts on the input vector Input. 
The time evolution of the quantum system, given in (2), maps 
the  initial  state  (input)  to  the final  state  (output)  in  much  the 
same way. The mapping is accomplished by the exponential of 
the Liouville operator, ğ‘’ğ‘–â„’ğ‘¡ . The parameters playing the role of 
the adjustable weights in the neural network are the parameters 
in  the  Hamiltonian  that  control  the  time  evolution  of  the 
system:  the  physical  interactions  and  fields  in  the  quantum 
hardware, which can be specified as functions of time. Because 
we want to be able physically to implement our method, we 
use not the final state of the system, ğœŒ(ğ‘¡), as our output, but 
instead a measure, M, applied to the quantum system at that 
final 
the  output  O(tf)  =  M(Ï(tf)). 
â€œProgrammingâ€ this quantum computer, the act of finding the 
â€œprogram  steps  or  algorithmâ€  involves  finding  the  parameter 
functions that yield the desired computation. We use machine 
learning to find the needed quantum algorithm. This means we 
learn the system parameters inside H to evolve in time initial 
(input) to target final (output) states; yielding a quantum system 
that accurately approximates a chosen function, such as logic 
gates,  benchmark  classification  problems,  or,  since  the  time 
evolution  is  quantum  mechanical,  a  quantum  function  like 
entanglement.  If  we  think  of  the  time  evolution  operator  in 
terms of the Feynman path integral picture [18], the system can 
be  seen  as  analogous  to  a  deep  neural  network,  yet  quantum 
mechanical. That is, instantaneous values taken by the quantum 
system at intermediate times, which are  integrated over, play 
the role of â€œvirtual neuronsâ€ [7]. In fact, this system is a deep 
learning system, as the time dimension controls the propagation 

time,  producing 

 
 
 
 
  
 
 
 
 
of  information  from  the  input  to  the  output  of  the  quantum 
system,  and  the  depth  is  controlled  by  how  finely  the 
parameters  are  allowed  to  vary  with  time.  We  use  the  term 
â€œdynamic  learningâ€  to  describe  the  process  of  adjusting  the 
parameters in this differential equation describing the quantum 
dynamics  of  the  quantum  computer  hardware.  The  real  time 
evolution  of  a  multi-qubit  system  can  be  treated  as  a  neural 
network,  because  its  evolution  is  a  nonlinear  function  of  the 
various adjustable parameters (weights) of the Hamiltonian.  

We define a cost function, the Lagrangian L, to be minimized, 
as 
ğ¿ =

+ ğ‘–â„[ğ», ğœŒ]) ğ›¾(ğ‘¡)ğ‘‘ğ‘¡

+ âˆ« ğœ†â€ (ğ‘¡) (

[ğ‘‘ âˆ’ ğ‘‚(ğ‘¡ğ‘“)]

 (3) 

ğœ•ğœŒ

2

1

ğ‘¡ğ‘“
ğ‘¡0

ğœ•ğ‘¡

2

where  the  Lagrange  multiplier  vectors  are ğœ†â€  and ğ›¾(row  and 
column, respectively), where d is the desired value, and where 
ğ‘‚(ğ‘¡ğ‘“)  is  the  output  at  the  final  time.    Note  that  this  will 
constrain the density matrix to satisfy the Schrodinger equation 
during the time interval. In the example application presented 
later in this paper, we define the output for a quantum system 
to be trained as a pairwise  entanglement witness for qubits  Î± 
and Î² [7] as 

âŒ©ğ‘‚(ğ‘¡ğ‘“)âŒª = ğ‘¡ğ‘Ÿ[ğœŒ(ğ‘¡ğ‘“)ğœğ‘§ğ›¼ğœğ‘§ğ›½] 

(4) 

where tr stands for the trace of the matrix, the pointed brackets 
indicate the expectation value,  and ğœğ‘§ is the usual Pauli matrix. 
That is, the output is the qubit-qubit correlation function at the 
final time. This measure is chosen for this calculation because 
entanglement  is  a  kind  of  quantum  correlation,  so  it  makes 
sense to choose to map the entanglement of the initial state of 
the  system  to  an  experimental  measure  of  correlation.  To 
implement quantum backprop we  take the first variation of L 
with respect to Ï, set it equal to zero, then integrate by parts to 
give the following equation which can be used to calculate the 
vector elements of the Lagrange multipliers (â€œerror deltasâ€ in 
neural network terminology) that will be used in the learning 
rule: 

ğœ†ğ‘–

ğœ•ğ›¾ğ‘—
ğœ•ğ‘¡

+

ğœ•ğœ†ğ‘—
ğœ•ğ‘¡

ğ›¾ğ‘— âˆ’

ğ‘–

â„

âˆ‘ ğœ†ğ‘˜ğ»ğ‘˜ğ‘–ğ›¾ğ‘— +

ğ‘˜

ğ‘–

â„

âˆ‘ ğœ†ğ‘–ğ»ğ‘—ğ‘˜

ğ‘˜

ğ›¾ğ‘˜ = 0  

(5) 

which  is  solved  backward  in  time  under  the  boundary 
conditions  at  final  time  tf  given  by  âˆ’[ğ‘‘ âˆ’ âŒ©ğ‘‚(ğ‘¡ğ‘“)âŒª]ğ‘‚ğ‘—ğ‘– +
ğœ†ğ‘–(ğ‘¡ğ‘“)ğ›¾ğ‘—(ğ‘¡ğ‘“) = 0.  In  optimization  and  optimal  control  this  is 
called  the  co-state  equation.  The  gradient  descent  rule  to 
minimize L with respect to each network weight w is ğ‘¤ğ‘›ğ‘’ğ‘¤ =

ğ‘¤ğ‘œğ‘™ğ‘‘ âˆ’ ğœ‚

ğœ•ğ¿

ğœ•ğ‘¤

 . Because this technique uses the density matrix it 

is applicable to any state of the quantum system, pure or mixed.  
While  this  method  works  extremely  well  to  train  quantum 
systems  in  simulation,  it  requires  knowledge  of  the  quantum 
states,  the density matrix Ï, in (3) at each time t from t0 to tf.  
This  makes  this  method  not  amenable  to  real  time  quantum 

hardware  training,  since  measuring  the  quantum  state  at 
intermediate times collapses the quantum state and destroys the 
quantum  mechanics  computation  [19].    In  other  words, 
quantum backprop can be run in simulation and the resulting 
approximate parameters executed on quantum hardware as we 
have done in [20], but training on the hardware itself cannot be 
accomplished.  Also, because the H used in the above â€œoff lineâ€ 
machine learning does not exactly match the quantum hardware 
due  to  unknown  dynamics  and  uncertainties  in  the  physical 
system,  the  resulting  calculations  on  the  hardware  will  have 
some  error.    The  â€œoff-lineâ€  parameters  can,  however,  be  a 
starting  point  for  further  machine  learning  refinement  on  the 
hardware using the techniques described in the next sections. 

B.  A Hardware Compatible Model for IBM Qiskit 

Implementing the pairwise entanglement witness in a hardware 
compatible  model,  such  as  IBMâ€™s  Qiskit  [16],  requires  some 
modifications.  The  Qiskit  library  utilizes  a  quantum  gate 
model,  so  we  must  use  a  gate  representation  of  the  operator. 
The witness is constructed by first approximating the values of 
the  tunneling,  bias,  and  coupling  parameters  as  piecewise 
constant,  where  the  total  evolution  time  is  divided  into  4 
segments.  These  piecewise  constant  parameters  are  used  to 
form the Hamiltonian for the time evolution operator, which is 
converted into a sequence of gates,  a quantum circuit for the 
IBM Qiskit implementation. This circuit representation results 
in 20 independent weights wj for the entanglement witness. One 
of the time segments of the circuit is pictured in Fig. 1, where 
Ry and Rz are the rotations around the y and z axes, respectively,  
and  the  other  circuit  symbol  represents  the  CNOT  gate.  The 
structure from Fig. 1 is repeated for each time segment, with 
weights on each segment determined by the piecewise constant 
approximations  to  the  continuous  parameter  functions.  Full 
details  of  the  gate  representation  and  a  comparison  with  the 
behavior of the continuous parameters are presented in [10]. 

III.  FINITE DIFFERENCE GRADIENT LEARNING ON QUANTUM 
HARDWARE 

A.  Fourier Quantum Parameters for Simulations 

As  described  above,  each  of  the  quantum  system  parameters 
that  serve  as  the  quantum  computer  algorithm  can  vary  with 
time. For learning in the hardware environment, each quantum 
parameter P(t) is represented as a Fourier series expansion in 
time: 

where  tf  is  the  final  time  when  the  quantum  system  output  
measures are taken. This gives a limited population of Fourier 
coefficients for which to calculate the gradients needed for the 

 
 
 
 
 
 
 
 
in 

results 

shown 

LM learning algorithm described below.  This is motivated by 
the 
[21]  where  offline  quantum 
backpropagation is used to train general time varying quantum 
parameters. The function P(t) may be any continuous function 
of time, but it was discovered that the resulting parameters have 
obvious simple frequency content. In that paper we showed that 
fitting  the  parameters  with  Fourier  series  for  sine  and  cosine 
gave equivalent computing results. 

Learning  of  each  of  the  parameters  is  done  via  a  hybrid 
method which uses small variations of the Fourier coefficients 
to calculate the gradient of the output error which is then used 
in a straightforward gradient descent learning rule. For a given 
training pair in the training set, the quantum system is presented 
with  the  input,  the  system  runs  (with  the  current  parameters 
calculated from the current Fourier coefficients) until the final 
time tf where the output is calculated via the output measures 
on the  final state. The output is compared to the  target value 
and an output error is calculated  Eold. In  the backpropagation 
method, this output error is then backpropagated via quantum 
backprop to calculate gradients at each time step. In the hybrid 
reinforcement learning method, the following happens. 

Choosing a single parameter and Fourier coefficient in  (6), 
this coefficient is varied by a small amount. For example, the 
new value parameter P0 would be given by 

P0,new = P0 + âˆ†P0. 

(7) 

The  quantum  system  is  again  presented  with  the  input;  the 
system  then  runs  with  the  parameters  calculated  using  the 
modified  Fourier  coefficients;  the  output  is  calculated;  an 
output error Eold is calculated and compared to the error Eold; a 
gradient is calculated 

âˆ‡ğ¸ =

ğ¸ğ‘›ğ‘’ğ‘¤âˆ’ğ¸ğ‘œğ‘™ğ‘‘
âˆ†ğ‘ƒ0

(8) 

and, finally, this gradient is used to update the parameter using 
a specified learning rate Î·P0 via 

P0 = P0 + Î·P0âˆ‡E. 

(9) 

This  is  repeated  for  each  Fourier  coefficient  and  each 
quantum parameter, using the same input and target output. Each 
successive training pair is then processed in the same way until 
the  entire  list  of  training  pairs  is  exhausted,  constituting  one 
epoch of training. 

B.  Finite Difference Gradients Learning Results 

MATLABÂ® code implements the learning algorithm above 
and  calls  a  SimulinkÂ®  simulation  of  the  quantum  system. 
Compared  to  the  quantum  backprop  method,  finite  difference 
gradients, in simulation, takes about 25 times more computation 
time. The tunneling frequency is initialized to 2.5Ã—10âˆ’3 GHz, is 
changed by 0.02% to calculate the gradient and a learning rate 

of 2 Ã— 10âˆ’8 is used. The bias is initialized to 1.0 Ã— 10âˆ’4 GHz, is 
changed by 0.02% to calculate the gradient and a learning rate 
of  zero  (not  trained)  is  used.  The  qubit  coupling  matrix  off-
diagonal  elements  representing  qubit-to-qubit  coupling  are 
initialized to 1.0 Ã— 10âˆ’4 GHz, is changed by 0.02% to calculate 
the  gradient  and  a  learning  rate  of  4  Ã—  10âˆ’7 is  used.  The  on-
diagonal  coupling  of  a  qubit  to  itself  is,  of  course,  zero.  The 
entanglement  witness  calculation  described  above  is  the 
quantum  â€œprogramâ€  to  be  learned.  Three  pairs  Fourier 
parameters in (6) are used, that is n = 3. Systems with 2, 3, 4 and 
5 qubits are trained. A plot of the root-mean-square (RMS) error 
vs  training  epochs  as  well  as  plots  of  how  each  quantum 
parameter varies with time after training is complete have been 
presented  previously  in  [17].    In  this  previous  paper,  the 
quantum backprop method is compared to the finite difference 
gradient methods.   This  current paper  is  focused on using  the 
Levenberg  Marquardt method  to  learn  the  same  entanglement 
witness algorithm. 

C.  Finite Difference Learning on IBM Qiskit 
For finite difference gradients for Qiskit, the training process is 
very similar to the MATLABÂ® implementation, with necessary 
changes for the Qiskit system. First, one of the training states is 
evolved on the quantum system, then the measure chosen for 
the entanglement witness is applied to it giving an expectation 
value  for  the  witness.  Using  the  current  weights,  expectation 
values  for  each  state  in  the  training  set  are  computed  and 
subtracted from the target values to generate an RMS difference 
output  error  Eold.  A  single  weight  wj  is  adjusted  by  a  small 
amount as in (7), and the output error is then computed with the 
modified wj,new, yielding Enew. Equations (8) and (4) are used to 
update wj according to the specified learning rate  Î·wj, and the 
process is repeated for each of the 20 weights and all 4 training 
pairs,  constituting  one  epoch  of  training.  Qiskit  system 
initialization  and  training  parameters  are  given  in  Table  1. 
Experimentation revealed that the system was most sensitive to 
changes in the tunneling, which is why it has a higher learning 
rate. Training was successful, but improvement stopped after 
approximately  2500  epochs  where  the  RMS  error  oscillated 
near 0.02  Again, full results for finite difference training on 
IBM Qiskit have been presented in our previous paper [17]. 

TABLE 1. Qiskit Reinforcement Learning Initial Values 
Quantum Parameter 

Initial Value 

Perturbation  Learning 

Tunneling K 
Bias  
Coupling Î¶ 

2.0Ã—10âˆ’3 GHz 
1.0Ã—10âˆ’4 GHz 
1.0Ã—10âˆ’4 GHz 

0.02% 
0.02% 
0.02% 

Rate 
1Ã—10âˆ’2 
1Ã—10âˆ’3 
1Ã—10âˆ’3 

IV.  LEVENBERG MARQUARDT LEARNING FOR QUANTUM 
HARDWARE 

A.  Levenberg Marquardt Algorithm applied to quantum 

computing 

 
 
 
 
 
 
 
 
 
 
 
 
Straightforward finite difference gradients learning works and 
training converges, but the requirement of 2500 training epochs 
is untenable on near-term hardware. As such, we seek a more 
efficient  learning  scheme.  A  candidate  is  the  Levenberg 
Marquardt  (L.M.)  algorithm  [22]  [23]  for  solving  non-linear 
least-square  problems.  Strictly 
the 
entanglement  witness  is  not  a  least-square  problem  (our 
training set has only 4 elements in the 2-qubit case), but we will 
show  that  a  L.M.-inspired  weight  update  rule  is  nonetheless 
effective. 

speaking, 

training 

The L.M. algorithm uses the learning rule 

(12) 

to update each weight vector w, where Î» is the damping factor, 
DTD  is  the  scaling  matrix,  and  âˆ‡E  is  the  error  gradient.  (The 
specifics of selecting the damping factor and scaling matrix are 
presented later in this section.) The Jacobian matrix J is given 
by 

 . 

(13) 

where O(xi,w) is the network output function evaluated at the ith 
input vector xi using the weights w with N and W being the total 
number of training inputs and weights, respectively. 

For small Î», the update rule is similar to the Gauss-Newton 
algorithm,  allowing  larger  steps  when  the  error  is  decreasing 
rapidly.  For  larger  Î»,  the  algorithm  pivots  to  be  closer  to 
gradient descent and makes smaller updates to the weights. This 
flexibility is the key to L.M.â€™s efficacy, changing Î» to adapt the 
step  size  to  respond  to  the  needs  of  convergence:  moving 
quickly through the parameter space where the error function is 
steep and slowly when near an error plateau and thereby finding 
small improvements. Our implementation is a modified L.M. 
algorithm following several suggestions in [24]. One epoch of 
training consists of the following: 

1)  Compute the Jacobian (13) with current weights w 
2)  Update the scaling matrix DTD and damping parameter Î» 
3)  Calculate a potential update Î´w using (12), setting wnew = 

w + Î´w 

4)  Find if RMS error has decreased with new weights, or if 

an acceptable uphill step is found 

while somewhat lost in the parameter space. Following [24], we 
choose  DTD to  be  a  diagonal matrix  with  entries  equal  to  the 
largest diagonal entries of JTJ yet encountered in the algorithm, 
with a minimum value of 10âˆ’6. Updates to the damping factor 
may be done directly or indirectly; our results here use a direct 
method. Analyzing the eigenvalues of the approximate Hessian 
JTJ, we note that there is a cluster on the order of 10âˆ’4 and the 
rest nearly vanish. Testing showed that direct adjustments to the 
damping factor within a couple of orders of magnitude of these 
values resulted in more consistent training. For the minimum 
and maximum Hessian eigenvalues lmin and lmax in this cluster, 
we  establish  a  logarithmic  scale  that  ranges  [lmin/10,1/lmax] 
with  100  elements.  Following  the  principle  of  â€œdelayed 
gratificationâ€ [26], we move 10 steps down the scale when an 
update is accepted and move 1 step up after rejecting an update. 
The log scale is desirable, because it allows the damping factor 
to change more slowly when close to the top end of the range. 
Classically,  Î» is  modified  by a  multiplicative  factor  [23],  but 
this  causes  the  damping  factor  to  change  too  rapidly  for  our 
problem once it becomes large. 

Occasionally, the algorithm will fail to find a suitable update 
prior to reaching the top of the damping factor range. This could 
be due to a plateau [27] in the cost (RMS error) function or due 
to noise in the measurements causing the algorithm to miss a 
step it could have taken at a particular Î». When this occurs, we 
recompute the range for Î» using the current Hessian and set the 
damping factor to be equal to the minimum value. Doing this 
provides  the  L.M.-algorithm  the  opportunity  to  randomly 
search the parameter space due to the stochastic nature of the 
quantum  hardware  measurements  used 
the 
Jacobian. To allow further exploration of the parameter space, 
we allow for uphill steps following the criterion suggested in 
[24]. This approach will accept an uphill step of the form 

to  compute 

(1 âˆ’ Î²)Ei+1 â‰¤ min(E1,E2,...,Ei) 

(14) 

where Ei is the error in the ith iteration and 

Î² = cos(Î´wnew,Î´wold) 

(15) 

is  the  cosine  of  the  angle  between  the  vectors  formed  by  the 
proposed  and  last  accepted  weight  updates,  respectively. 
Criterion (14) checks if the angle between those update vectors 
is acute and will accept an uphill step within a tolerance. The 
longer training goes, the smaller an uphill step must be  to be 
accepted. This feature allows the training to more easily move 
out of shallow local minima in the cost function. 

5)  If neither condition in step 3 is satisfied, reject the update, 

B.  Levenberg Marquardt training: MATLAB simulation 

increase Î», and return to step 2 

results 

6)  For an accepted downhill or uphill step, set w = wnew and 

decrease Î», ending the epoch 

Partial  derivatives  in  the  Jacobian  are  computed  using  the 
parameter-shift  rule  [25].  The  scaling  matrix  DTD  serves  the 
primary  purpose  of  combating  parameter  evaporation  [26], 
which is the tendency of the algorithm to push values to infinity 

The  LM  algorithm  was  added  as  a  training  option  in  the 
backprop  and  finite  difference  MATLAB  code.    For  any 
method, training in simulation for more than 5 qubits resulted 
in code runtimes longer than several days on a Windows PC.  
With  the  efficiency  of  the  LM  algorithm,  6  qubits  could  be 
accomplished on a PC.  Beyond 6 qubits, the  code execution 

 
 
 
 
 
 
 
was moved to a XSEDE cluster computer. [28]   The training 
pair loop was distributed to a pool of 36 cores running on a node 
with a GPU via a MATLAB â€œparforâ€ statement replacing the 
for loop. After the Jacobian was completed for all training pairs, 
the LM matrix operations were done entirely on the GPU via 
GPU array functions and then gathered back from the GPU for 
the LM parameter (weight) updates.  Training for the 2 qubit 
case  was  completed  first.    The  RMS-vs-epoch,  LM  Lambda 
Parameter are shown in Figs. 1-6.  Transfer learning was again 
used to apply the 2 qubit parameters to 3 qubits, followed by 4, 
5, 6, 7, 8, and 9 qubits.  Table I contains information for each 
qubit  case.  As  the  number  of  qubits  increases,  the  transfer 
learning is more effective, and a significantly fewer number of 
epochs  are  needed  as 
the 
entanglement witness converge to constant values as shown in 
Figs. 7-8. 

the  parameters  needed  for 

Table I Transfer learning for increasing numbers of qubits 

#qubits  #Training 

2 
3 
4 
5 
6 
7 
8 
9 

Pairs 
4 
12 
24 
40 
60 
84 
112 
144 

#epochs  Start 
RMS 
.5439 
.3108 
.0569 
.0263 
.0204 
.0166 
.0160 

20 
20 
20 
10 
10 
10 
10 

Finish 
RMS 
.0024 
.0066 
.0045 
.0058 
.0028 
.0087 
.0110 

converged in approximately 30 epochs, nearly 100 times faster 
than the previous method shown in [17]. In addition, the RMS 
error  reduced  to  a  lower  value,  on  the  order  of  10âˆ’4,  in  the 
Qiskit simulator; that is, the training with L.M. is both faster 
and better. This improvement is likely linked to the way L.M. 
deals with the ubiquitous problem [27] of barren plateaus and 
makes the L.M. method a major upgrade for online training on 
quantum hardware. 

Fig. 2. LM Lambda vs epoch for 2-qubit entanglement witness training using 
the Levenberg-Marquardt method in a MATLAB Simulation. 

Fig. 1. RMS error vs epoch for 2-qubit entanglement witness training using the 
Levenberg-Marquardt method in a MATLAB Simulation. 

C.  Levenberg Marquardt Qiskit training results 

Training  with  L.M.  in  Qiskit  is  markedly  faster  than 
straightforward reinforcement  learning.  Fig.  9  shows  training 

Fig. 3. RMS error vs epoch for3-qubit entanglement witness training using the 
Levenberg-Marquardt method in a MATLAB Simulation. 

V.  DISCUSSIONS AND CONCLUSIONS 

The major contribution of this paper is the demonstration of the 
feasibility of true online training of a quantum system to do a 
quantum  calculation.  It  is  a  well-known  theorem  that  a  very 
small set of gates (e.g., the set {H, T, S, CNOT}) is universal. 
This  means  that  any  N-qubit  unitary  operation  can  be 
approximated to an arbitrary precision by a sequence of gates 
from that set. But there are many calculations we might like to 
do, for which we do not know an optimal sequence to use, or 
even,  perhaps,  any  sequence  to  use.  And  there  are  many 
questions we might want to answer for which we do not even 

 
 
 
 
 
 
 
 
 
 
have  a  unitary, 
is,  an  algorithm.  Calculation  of 
entanglement of an N-qubit system is a good example of such 
a question: we do not have a general closed form solution, much  

that 

Fig. 6. LM Lambda vs epoch for 4-qubit entanglement witness training using 
the Levenberg-Marquardt method in a MATLAB Simulation. 

Fig. 4. LM Lambda vs epoch for 3-qubit entanglement witness training using 
the Levenberg-Marquardt method in a MATLAB Simulation. 

Fig. 7. Convergence of Tunneling Parameter vs time as the number of qubits 
increases for the entanglement witness LM training in MATLAB Simulation. 

Fig. 5. RMS error vs epoch for 4-qubit entanglement witness training using the 
Levenberg-Marquardt method in a MATLAB Simulation. 

Fig. 8. Convergence of Coupling Parameter vs time as the number of qubits 
increases for the entanglement witness LM training in MATLAB Simulation 

less know an optimal set of measurements to make on a system 
whose  density  matrix 
its 
entanglement. 

to  determine 

is  unknown, 

Quantum machine learning methods like the ones used here 
are systematic methods for dealing with these problems. Here 
we show that they are in fact directly implementable on existing 
hardware.  Our  iterative  staging  technique  makes  scale-up 
relatively easy, as most of the training for a system of N qubits 
has already been accomplished in the system for (N-1) qubits. 
Additionally, this does not bias the results, as this entanglement  

 
 
 
 
[5]   T. F. Ronnow, Z. Wang, J. Job, S. Boixo, S. V. Isakov, 
D. Wecker, J. M. Martinis, D. A. Lidar and M. Troyer, 
"Defining and detecting quantum speedup," Science, 
vol. 345, pp. 420-424, 2014.  
E. C. Behrman, L. R. Nash, J. E. Steck, V. G. 
Chandrashekar and S. R. Skinner, "Simulations of 
quantum neural networks," Information Sciences, vol. 
128, pp. 257-269, 2000.  

[6]  

[8]  

[7]   E. C. Behrman, J. E. Steck, P. Kumar and K. A. Walsh, 
"Quantum algorithm design using dynamic learning," 
Quantum Information & Computation, vol. 8, pp. 12-29, 
2008.  
E. C. Behrman and J. E. Steck, "Multiqubit 
entanglement of a general input state," Quantum 
Information & Computation, vol. 13, pp. 36-53, 2013.  
N. H. Nguyen, E. C. Behrman and J. E. Steck, 
"Quantum learning with noise and decoherence: a robust 
quantum neural network," Quantum Machine 
Intelligence, vol. 2, pp. 1-15, 2020.  

[9]  

Fig. 9. RMS error vs. training epoch for the Levenberg-Marquardt method in 
Qiskit. 

witness performs very well when tested on larger systems, even 
in  the  face  of  noise  [21]  [9].  And  while  training  on  actual 
quantum  hardware  does  prove  somewhat  more  challenging, 
that is all the more reason for a machine learning approach. Any 
physical  implementation  features  sources  of  error  that  in 
general  are  unknown  (interactions,  flaws,  incomplete  and 
damaged  data).  With  machine  learning  we  can  deal  with  all 
these problems automatically. 

ACKNOWLEDGMENT 

We all thank the entire research group for helpful discussions: 
Nam  Nguyen,  Saideep  Nannapaneni,  William  Ingle,  Henry 
Elliott, Ricardo Rodriguez, and Sima Borujeni. 

VI.  REFERENCES 

[1]   P. Shor, "Algorithms for quantum computation: discrete 
logarithms and factoring," in Proceedings of the 35th 
Annual Symposium on Foundations of Computer 
Science, 1994.  

[2]   L. Grover, "A fast quantum mechanical algorithm for 
database search," in Proceedings of the 28th Annual 
ACM Symposium on Theory of Computing, 1996.  

[3]   A. M. Childs, R. Cleve, E. Deotto, E. Farhi, S. Gutman 
and D. A. Spielman, "Exponential algorithmic speedup 
by a quantum walk," in Proceedings of the 35th Annual 
ACM Symposium on Theory of Computing, 2003.  
S. Bravyi, D. Gosset and R. Konig, "Quantum 
advantage with shallow circuits," Science, vol. 362, pp. 
308-311, 2018.  

[4]  

[10]  N. H. Nguyen, E. C. Behrman, M. A. Moustafa and J. E. 

Steck, "Benchmarking neural networks for quantum 
computations," IEEE Transactions of Neural Networks 
and Learning Systems, vol. 31, pp. 2522-2531, 2020.  

[11]   L. Gurvitz, "Classical deterministic complexity of 
Edmonds problem and quantum entanglement," in 
Proceedings of the 35th Annual ACM Symposium on 
Theory of Computing, 2003.  
J. Preskill, "Quantum entanglement and quasntum 
computing," in Proceedings of the 25th Solvey 
Conference on Physics, 2013.  

[12]  

[13]   P. J. Werbos, "Neurocontrol and supervised learning: 

An overview and evaluation," in Handbook of 
Intelligent Control, 1992.  

[14]   C. Goncalves, "Quantum neural machine learning: 
Backpropagation and dynamics," NeuroQuantology, 
vol. 15, pp. 22-41, 2017.  

[15]   G. Verdun, J. Pye and M. Broughton, "A universal 
training algorithm for quantum deep learning," 2018. 
[Online]. Available: arXiv:1806.09729. 
H. Abraham and et al., "Qiskit: An open source 
framework for quantum computing," 2019.  

[16]  

[17]   N. L. Thompson, J. E. Steck and E. C. Behrman, "A 
non-algorithmic approach to "programming" quantum 
computers via machine learning," in IEEE International 
Conference on Quantum Computing and Engineering, 
2020.  
R. P. Feynman, "An operator calculus having 
applications in quantum electrodynamics," Physical 
Review, vol. 84, pp. 108-128, 1951.  
J. J. Sakurai, Modern Quantum Mechanics, 2017.  

[19]  
[20]   N. L. Thompson, N. H. Nguyen, E. C. Behrman and J. 

[18]  

E. Steck, "Experimental pairwise entanglement 
estimation for an N-qubit system: A machine learning 

 
 
[21]  

approach for programming quantum hardware," 
Quantum Information Processing, vol. 19, pp. 1-18, 
2020.  
E. C. Behrman, N. H. Nguyen, J. E. Steck and M. 
McCann, "Quantum neural computation of 
entanglement is robust to noise and decoherence," in 
Quantum Inspired Computational Intelligence, Boston, 
Morgan-Kauffmann, 2017, pp. 3-32. 

[22]   K. Levenberg, "A method for the solution of certain 
non-linear problems in least squares," Quarterly of 
Applied Mathematics , vol. 2, pp. 164-168, 1944.  
[23]   D. W. Marquardt, "An algorithm for least-squares 
estimation of nonlinear parameters," Journal of the 
Society for Industrial and Applied Mathematics, vol. 11, 
pp. 431-441, 1963.  

[24]  M. K. Transtrum and J. P. Sethna, "Improvements to the 

Levenberg Marquardt algorithm for nonlinear least-
squares minimization," 2012. [Online]. Available: 
arXiv:1201.5885. 

[25]  G. E. Crooks, "Gradients of parametrized quantum gates 
using the parameter shift rule and gate decomposition," 
2019. [Online]. Available: arXiv:1905.1311. 

[26]   M. K. Transtrum, B. B. Machta and J. P. Sethna, 

[27]  

[28]  

"Geometry of nonlinear least squares with applications 
to sloppy models and optimization," Physical Review E, 
vol. 83, p. 036701, 2011.  
J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. 
Babbush and H. Neven, "Barren plateaus in quantum 
neural network training landscapes," Nature 
Communications, vol. 9, p. 4812, 2018.  
John Towns, Timothy Cockerill, Maytal Dahan, Ian 
Foster, Kelly Gaither, Andrew Grimshaw, Victor 
Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. 
Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-
Diehr, ""XSEDE: Accelerating Scientific Discovery",," 
Computing in Science & Engineering, vol. 16, no. 
doi:10.1109/MCSE.2014.80, pp. 62-74, 2014.  

 
 
 
