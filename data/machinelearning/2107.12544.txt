1
2
0
2

l
u
J

7
2

]
I

A
.
s
c
[

1
v
4
4
5
2
1
.
7
0
1
2
:
v
i
X
r
a

Human-Level Reinforcement Learning through
Theory-Based Modeling, Exploration, and Planning

Pedro A. Tsividis,1,3∗ Jo˜ao Loula,1 Jake Burga,1
Nathan Foss1, Andres Campero1, Thomas Pouncy,2
Samuel J. Gershman,2,3(cid:5) Joshua B. Tenenbaum1,3(cid:5)

1Massachusetts Institute of Technology
Cambridge, MA 02139, USA
2Harvard University, Cambridge, MA 02138 USA
3Center for Brains, Minds, and Machines
Cambridge, MA 02139, USA
(cid:5)Equal contribution
∗To whom correspondence should be addressed; E-mail: tsividis@mit.edu.

Reinforcement learning (RL) studies how an agent comes to achieve reward in an envi-

ronment through interactions over time. Recent advances in machine RL have surpassed

human expertise at the world’s oldest board games and many classic video games, but they

require vast quantities of experience to learn successfully — none of today’s algorithms

account for the human ability to learn so many different tasks, so quickly. Here we pro-

pose a new approach to this challenge based on a particularly strong form of model-based

RL which we call Theory-Based Reinforcement Learning, because it uses human-like in-

tuitive theories — rich, abstract, causal models of physical objects, intentional agents, and

their interactions — to explore and model an environment, and plan effectively to achieve

task goals. We instantiate the approach in a video game playing agent called EMPA (the

Exploring, Modeling, and Planning Agent), which performs Bayesian inference to learn

1

 
 
 
 
 
 
probabilistic generative models expressed as programs for a game-engine simulator, and

runs internal simulations over these models to support efﬁcient object-based, relational

exploration and heuristic planning. EMPA closely matches human learning efﬁciency on

a suite of 90 challenging Atari-style video games, learning new games in just minutes of

game play and generalizing robustly to new game situations and new levels. The model

also captures ﬁne-grained structure in people’s exploration trajectories and learning dy-

namics. Its design and behavior suggest a way forward for building more general human-

like AI systems.

Games have long served as readily available microcosms through which to compare the

ﬂexibility and speed of human and machine learners. Atari-style video games are particularly

revealing: While model-free RL systems inspired by basic animal learning processes (1–4)

can learn to play many classic Atari games with a relatively simple neural policy network (5–

12), humans learn much more quickly and generalize far more broadly (13). Human players

can reach a competent level of play — scoring much better than a random policy and making

signiﬁcant progress that generalizes to many game board variations — in just a few minutes or

a handful of episodes of play (13). Model-free RL systems require tens or even hundreds of

hours of play to reach the same level (7, 12). Even after reaching apparent mastery of a game,

RL systems fail to generalize to even small variations of the game board or its dynamics (14).

Our goal here is to develop a computational account of how humans learn so efﬁciently

and generalize so robustly, in terms that can also guide the development of more human-like

learning in machines. In contrast to most work in RL and game AI (15–20), our intention is not

to explain how a class of intelligent behaviors could evolve from scratch in a system that starts

with zero knowledge. Rather we seek to capture learning as we see it in humans, modeling

as closely as possible the trajectories people show from the beginning to the end of learning a

complex task such as a new video game. To do this, we start from the knowledge that humans

2

bring to these tasks: ﬂexible but powerful inductive biases that are the product of evolution,

development and culture, and that can be deployed to learn quickly even in environments very

different from their prior experience at the level of pixels or raw observations.

Our approach can be seen as a particularly strong form of model-based RL (21), which has a

long history in both human and machine learning (14,22–45), but as of yet no proposals for how

to capture human-level learning in complex tasks. In cognitive neuroscience, model-based RL

has primarily been studied in simple laboratory tasks consisting of a small number of sequential

decisions (46–48), where the algorithms and representations that are most successful do not

directly transfer to the much more complex tasks posed by video games or the real world. In

AI, model-based RL approaches are an active area of current neural network research and have

the potential for greater sample-efﬁciency relative to model-free systems (42–45). But these

approaches do not attempt to capture human inductive biases or learn in human-like ways, and

they are still far from achieving human-level learning trajectories in most games.

We refer to our approach as Theory-Based Reinforcement Learning, because it explicitly

attempts to incorporate the ways in which humans, from early childhood, are deeply guided by

intuitive theories in how they learn and act — how they explore the world, model its causal struc-

ture, and use these models to plan actions that achieve their goals (27–29, 49, 50). Exploration

stems from theory-based curiosity, an intrinsic motivation to know the causal relations at work

in the world and to take actions directed to reveal those causes (26,51–54). Causal model learn-

ing can be formalized as Bayesian inference of probabilistic generative models (55), guided by

priors based on intuitive theories — especially core knowledge representations dating to infancy

that carve the world into a natural ontology of objects, physics, agents, and goals (22–25). Plan-

ning exploits intuitive theories to constrain the search over an otherwise intractably large space

of possible action sequences; knowing which future states are worth thinking further about helps

even young children guide their search for effective actions and explanations (56, 57).

3

We embody this general theory-based RL approach in a speciﬁc architecture that learns to

solve new game tasks, which we call EMPA (the Exploring, Modeling, Planning Agent). We

directly compare EMPA and alternative models on a new human-learning benchmark dataset (to

be made available for download along with example videos of human and EMPA gameplay at

https://github.com/tsividis/Theory-based-RL upon publication). The data

consist of video game play from 300 human participants distributed across a set of 90 challeng-

ing games inspired by (and some directly drawn from) the General Video-Game AI (GVGAI)

competition (58). These games are simpler than classic Atari games but present fundamentally

similar challenges. Some require fast reactive game play, while many require problem solving

with nontrivial exploration and long-range planning. Like many real-world tasks and in contrast

to Atari, a number of our games feature very sparse rewards, which present an additional chal-

lenge for artiﬁcial agents. As in Atari, people typically learn to play these games and generalize

across levels in just a few minutes, or several hundred agent actions (Figure 2). By comparison,

as we show below, conventional deep RL algorithms typically take many thousands of steps to

reach comparable performance on these games and often fail to solve even a single game level

after hundreds of thousands of steps. EMPA aims to close this gap.

Learning, exploration and planning

EMPA interacts with the environment in a continuous cycle of exploring, modeling, planning,

and acting (Figure 1). The Modeling component learns symbolic program-like descriptions

of game dynamics, including types of entities, causal laws governing the interaction between

entities, and win and loss conditions, as well as other sources of positive or negative rewards.

Learned models support simulation of future states conditioned on a current (or imagined) game

state and the agent’s actions. The Planning component searches efﬁciently for plans that achieve

goals, guided by theory-based heuristics that decompose win and loss conditions into intrinsi-

4

Figure 1: Schematic representation of the EMPA architecture. EMPA takes as input a sym-
bolic description of its environment, speciﬁed in terms of objects and their locations as well
as interactions that occur between objects. Bayesian inference scores candidate models (θ) on
their ability to explain observed state sequences. Theory-based curiosity generates exploratory
goals. The planner decomposes exploratory and win-related goals into subgoals and goal gradi-
ents, and uses this hierarchical decomposition to effectively ﬁnd high-value actions for EMPA
to take in the game environment.

5

Figure 2: Humans (green) learn new games in a matter of minutes (corresponding to a few
hundred steps). EMPA (blue) closely matches human learning curves in these and many other
games, while DDQN (grey) generally makes little or no progress on the same time scale. Each
dot shows the cumulative number of levels won at each point in time by an individual instance of
each agent type (individual human or run of model); lines show smoothed scores from individual
runs. Learning efﬁciency, κ = levels completed
steps to completion, compactly summarizes each agent’s
performance over the indicated range of experience (10,000 game steps for “Avoidgeorge”,
1,000 for the remaining games).

levels in game × levels completed

6

cally rewarding reachable subgoals, and goal gradients that intrinsically reward steps toward

those sub-goals. The Exploring component generates theory-guided exploratory goals for the

planner, so that the agent most efﬁciently generates the data needed to learn a game’s causal in-

teractions and win or loss conditions. Extended Data Figure 1 illustrates how these components

allow the agent to learn dynamics models rapidly, even in games that pose severe exploration

challenges, and to immediately generalize plans to win new game levels, even with substantially

different object layouts than those encountered during initial learning.

Implementing EMPA requires many detailed technical choices (see Methods and Supple-

mentary Information), but here we focus on the key ideas behind each of the Exploration, Mod-

eling and Planning components. The models learned by EMPA are object-oriented, relational,

and compositional, and can be thought of as a projection of the core intuitive theories that al-

low infants to learn so rapidly about the real world, onto the virtual world of video games.

Speciﬁcally, we represent models using a subset of the Video Game Description Language

(VGDL) (59), a lightweight language for Atari-like games in which all the GVGAI games are

written. (See examples in Extended Data Figures 2 and 3.) A VGDL description of a given

game speciﬁes the appearance and basic properties of all objects, which manifest as causal con-

straints on their dynamic properties: whether they move by default and in what directions, how

quickly they move, what their goals are, and so on. Interaction rules specify the outcomes of

contact events between pairs of objects as a function of their classes; these rules encompass

natural concepts such as pushing, destroying, picking up, and so forth. Finally, termination

conditions specify when a game is won or lost. These transition dynamics, together with a

description of the game’s initial state, completely specify a given game world.

EMPA scores models according to a Bayesian criterion, with a hypothesis space correspond-

ing to a restricted but still vast space of possible VGDL descriptions: for example, if we restrict

to games with only 10 unique classes of entities, there are over 3.6 × 1037 possible VGDL

7

descriptions or games that could be learned. The posterior probability of a speciﬁc game de-

scription θ conditioned on observed game states s and actions a is

p(θ | s0:T , a0:T −1) ∝ p(s0:T | θ, a0:T −1) p(θ).

(1)

The likelihood function, p(s0:T | θ, a0:T −1), can be decomposed as

p(s0:T | θ, a0:T −1) = p(s0)

T
(cid:89)

t=1

p(st

G | θG, st

S, st

I)p(st

I | θI, st−1, at−1)p(st

S | θS, st−1, at−1) (2)

which is a factorization of the state into goals, interaction events, and objects, respectively.

EMPA’s exploration component implements a theory-based notion of curiosity that inte-

grates its learning and planning functions, as the agent seeks to observe events that are most

informative about unknown aspects of its hypothesis space. These exploratory goals can be

formalized as a proxy reward function maximizing expected information gain (60, 61), but be-

cause of the nature of the models, the most informative events are always agent-object and

object-object interactions for classes of objects that have not previously been observed in con-

tact (see examples in Extended Data Figure 1). The agent thus explicitly makes plans whose

goals are to produce these events. Crucially, this sense of curiosity is not myopic; EMPA often

generates long-ranging plans whose sole purpose is to generate informative interactions. This

enables EMPA to solve games that are highly challenging for traditional stochastic exploration

methods.

EMPA’s planner uses the maximum probability model as a simulator to imagine future states

conditioned on candidate actions, searching a tree of future states to ﬁnd high-value action se-

quences. Sparse external rewards are a fundamental challenge for RL agents; EMPA addresses

these by assigning several kinds of intrinsic rewards to imagined states. Treating exploration

as a ﬁrst-class goal, alongside winning, is one such intrinsic reward. EMPA also uses its do-

main theory to automatically decompose goals into easier-to-achieve subgoals, as well as goal

8

gradients that help the planner more quickly ﬁnd goals and subgoals by exploiting the spatial

structure of the environment. A single intrinsic reward function summing these three quantities,

V (s, θ) = RG(s, θ) + RSG(s, θ) + RGG(s, θ) (for goals, subgoals, and goal gradients, respec-

tively) is applied to every environment, in all states s the agent encounters or imagines, for

any model θ it currently considers most likely. Generating intrinsic rewards in terms of these

abstract goal-based concepts, which are in turn deﬁned using the same description language

that underlie EMPA’s model-learning and exploration (see Methods), leads to a planner that

effectively understands the agent’s environments in a deep and generalizable way, and that is

sufﬁciently powerful to ﬁnd plans for most environments and tasks by simple best-ﬁrst search

through the space of imagined trajectories. The planner is also aided by Iterative Width (IW)

pruning of insufﬁciently novel states from the search tree (62, 63), and several different search

modes specialized for different time-scales of planning. A meta-controller manages decisions

about whether to continue to execute previously-conceived plans or search for new ones, as

well as which search mode to use, as a function of the highest probability model, the current

game state, and the predictions of the previously-conceived plan. The meta-controller calls for

long-term planning with deeper search trees when the environment is stable and predictable,

short-term planning with shallower trees when the environment is rapidly changing or less pre-

dictable, and “stall” planning when other modes have failed to return satisfactory plans.

Evaluating EMPA and alternatives versus human learners

We compared people, EMPA, and two leading single-agent deep-reinforcement learning agents,

Double DQN (DDQN) (7) and Rainbow (12), on a suite of 90 games (see Methods for hu-

man experimental procedures and a summary of the games; see Supplementary Information for

detailed descriptions of all games). We also tested a number of ablated versions of EMPA,

designed to highlight the relative contributions of its modules. Deep RL methods provide

9

a valuable comparison point for our work as they represent the class of most successful and

actively-developed RL agents on Atari-style tasks. We focus on DDQN in particular because it

is a simple and widely-known variant of DQN, the original deep RL system for playing Atari-

like video games; we also evaluate Rainbow, a recently developed method designed to combine

the best features of DDQN and other DQN variants, and which achieves greater learning efﬁ-

ciency on many tasks. However, we hasten to add that we do not intend either of these deep RL

methods to be seen as a baseline model for EMPA, or as a serious alternative account for how

humans learn to play new games. Deep RL methods do not attempt to capture the background

knowledge humans bring to a new task, nor how people explore and plan so efﬁciently using

that knowledge, and so they necessarily will perform differently on our tasks. We explore the

performance gap between humans and these methods only to quantify how much value there

could be in building more human-like forms of RL — our goal in developing EMPA and more

generally the goal of the theory-based RL approach.

Our primary criterion for comparing humans and models is learning efﬁciency, which re-

ﬂects both depth and speed of learning: how many levels of a game does an agent win, and

how quickly does it progress through these levels? Learning curves show intuitively that EMPA

matches human learning efﬁciency across most games (Figure 2; all 90 games shown in Ex-

tended Data Figures 5-7). To more quantitatively compare human and model performance, we

deﬁne a learning efﬁciency metric, κ = levels completed

levels in game × levels completed

steps to completion, a product of two terms

measuring the degree to which a task is completed and the speed with which it is accomplished.

On balance, EMPA matches human learning efﬁciency according to this metric: It is sometimes

better and sometimes worse, but almost always (on 79 out of 90 games) within an order of

magnitude (0.1x to 10x) of human performance (Figure 3). By contrast, DDQN almost always

progresses much more slowly than humans: It is more than 100x less efﬁcient on 67 out of 90

games, more than 1,000x worse on 45 out of 90 games, and over 10,000x worse on 22 out of 90

10

games. Rainbow, despite being signiﬁcantly more sample-efﬁcient than DDQN on Atari (12),

performs no better on average here (Figure 4). This is likely in part due to the fact that humans

generally complete our tasks within 1,000 actions, which is well within the margin during which

both DDQN and Rainbow (and indeed all deep RL algorithms) are still exploring randomly. The

fact that DDQN and Rainbow learn orders of magnitude more slowly than humans should thus

not be surprising: They are not designed with the objective of learning as quickly as possible

and do not attempt to embody the model-building or planning capacities that allow humans to

learn without an extended period of stochastic exploration. But this comparison does illustrate

starkly the value of building in more human-like cognitive capacities for RL — potentially a

thousand-fold gain in learning efﬁciency, which for this limited class of video game tasks, the

EMPA model largely achieves.

Although our game tasks are still relatively simple, their goals and dynamics vary consider-

ably in ways that reﬂect the complexity and variability of tasks humans solve in the real world.

For instance, in Bait (Figure 2, top left), the player must push boxes into holes in the ground in

order to clear a path to a key that unlocks the exit. EMPA learns as fast as the median human,

solving all ﬁve levels in fewer than 1000 steps; DDQN wins the ﬁrst level as fast as humans

do, but takes 250,000 steps to solve the ﬁrst two levels and fails to solve the rest within the

allotted 1 million steps. In Zelda (top middle), the player must also reach an exit that only

opens once the player has obtained a key, but here the player has to complete this task while

avoiding rapidly-moving dangerous creatures. Humans and EMPA can solve this game within

500 steps, whereas DDQN takes 100,000 steps to win four levels and fails to reach the ﬁfth in

1 million steps. A variant of the game (Zelda 1) that requires the player to pick up three keys

before reaching the door is roughly equally challenging for humans and EMPA, who can win

all levels within 2,000 steps, but is far more challenging for DDQN — it takes 800,000 steps

to reach the third level and never progresses beyond it, despite the similarity of the variant to

11

Figure 3: Across 90 games, EMPA achieves comparable learning efﬁciency to humans, out-
performing them on roughly two-thirds of games and underperforming them on roughly a third,
but is almost always within an order of magnitude (0.1x-10x) of human performance. Shown in
blue are mean scores and bootstrapped 95% conﬁdence intervals for 10 runs of EMPA on each
game (adjusted in two games for sampling edge effects and excluding one game (light blue) on
which no run of EMPA progressed beyond the ﬁrst level; see Methods for details). By contrast,
DDQN (grey vertical lines, with dark grey indicating the highest-performing run on each game)
usually learns several orders of magnitude less efﬁciently than humans or EMPA do, and fails
to win even a single level on multiple games.

12

the original. In Butterﬂies (top right), the player must catch randomly-moving butterﬂies before

they touch all the cocoons on the screen. EMPA performs as well as the best humans, winning

all ﬁve levels within 1000 steps. DDQN solves the ﬁrst four levels in as few as 20,000 steps,

which is relatively fast compared to its performance on other games. However, it fails to solve

the last level within 1 million steps, due to a subtle variation in the obstacle geometry which is

fatal for the network’s learned policy but poses no problem to EMPA’s or humans’ model-based

planning.

The modular nature of EMPA allows us to ablate different components and thereby gain in-

sight into their relative contributions to the full system’s success across our suite of games (see

Methods for details). Many games pose signiﬁcant exploration challenges, requiring an agent

to traverse a complex sequence of intermediate states (e.g., pick up a key to open a door, cross

a road ﬁlled with dangerous moving cars, or push several boxes into holes) in order to reach a

state where the game’s win conditions can be discovered (Extended Data Figure 1). Such traver-

sals are highly unlikely to occur by chance, and thus EMPA ablations that replace theory-based

exploration strategies with (cid:15)-greedy exploration — essentially reducing exploration to stochas-

tic exhaustive search — fail dramatically in these games (Figure 4). Likewise, ablations to any

of EMPA’s theory-based heuristics for planning (subgoal and goal gradient intrinsic rewards, or

IW pruning) primarily lead to failures to discover solutions to even a single level for a subset

of games (Figure 4); ablating multiple planner components leads to higher failure rates and

lower mean and median efﬁciency across games. Many games challenge the ablated planners

for the same underlying reason that they challenge exploration: When multiple subgoals must

be reached in a speciﬁc sequence before any reward is received, successful plans are hard to

discover by a simple model-predictive search. Only EMPA’s full complement of theory-guided

planning heuristics and exploration mechanisms let it achieve human-level learning efﬁciency

across the complete set of game environments tested.

13

Figure 4: Ablations to EMPA’s exploration (purple) or planning modules (yellow, turquoise,
and green) cripple the agent’s ability to win many games, producing comparable failure rates to
DDQN, and suggesting that nontrivial exploration and planning abilities are a signiﬁcant com-
ponent of humans’ ability to quickly perform well across many tasks. The vertical dotted line
shows average human performance across all games. Horizontal line segments show 25th-75th
percentiles (with medians marked by a central hash); dashed lines show 10th-90th percentiles;
diamonds show geometric means.

14

Fine-grained analysis of learning and exploring behavior

EMPA matches humans not only in its learning efﬁciency but also in the ﬁne-grained structure

of its behavior as it explores and solves game levels (Figure 5A) . While the speciﬁcs of effective

agent trajectories vary greatly across games as a function of their object dynamics and layouts,

humans and EMPA share consistent similarities: They generate short, direct paths to speciﬁc

objects, in both “explore” and “exploit” phases of learning a new task, reﬂecting efﬁcient plans

to reach objects with potential (epistemic) or known (instrumental) value. By contrast, DDQN’s

stochastic exploration and slow learning generate behavior that is much more diffuse and often

restricted to a small subset of possible game board locations for long intervals (Figure 5A).

Humans and EMPA also sometimes exhibit diffuse behavior early on in games, but then become

more efﬁcient on later levels (Figure 5B) as they move from exploring the properties of new

objects to exploiting their learned models in order to win the game.

There are also signiﬁcant differences between EMPA and humans, which in some cases

lead to differences in behavior. EMPA is able to move faster than most humans do, and can thus

ﬁnd solutions that require fast coordination which humans often do not attempt. In addition,

EMPA is always aware of all of its action choices, whereas humans sometimes neglect useful

actions that would lead them to more efﬁcient strategies. On the other hand, EMPA has weaker

priors on higher-level aspects of games, leading it to sometimes be more exploratory than hu-

mans tend to be: for instance, EMPA doesn’t know that games almost always have exactly one

win condition, so after ﬁnding one way to win a game it may continue to explore new ways

of winning, whereas humans tend to win more efﬁciently by exploiting a single already-known

win condition. Empirically, these differences tend to balance out and produce similar qualita-

tive and quantitative behavior, particularly when comparing EMPA and human performance to

conventional deep RL agents.

In order to quantify the ways in which EMPA explores, learns, and plans in human-like

15

Figure 5:
(A) Representative behavior traces of individual human and EMPA players show
qualitative similarities across a wide range of games. Heat maps show the time spent by each
agent in each location of the game board for a given level, across all episodes, normalized by that
agent’s total experience on that level. Yellow indicates the most-visited locations; dark purple
locations are not visited. (B) Humans and EMPA sometimes produce more diffuse trajectories
during initial levels of games, as they explore the results of interactions with most objects. An
agent only needs to make contact with a few objects in order to win, resulting in more targeted
behavior in subsequent levels, once dynamics are known.

16

ways, we classiﬁed objects across 78 games as “positive”, “instrumental”, “neutral”, or “nega-

tive” (see Methods) and coded how much time humans and model agents spent interacting with

each class of objects. Across games, EMPA’s proﬁle of object interactions closely tracks the

human proﬁle (Figure 6A), matching the distribution of interactions better than DDQN does,

particularly in slow-paced games in which only the agent moves (see Extended Data Figure 8).

DDQN spends the greatest proportion of its time interacting with neutral objects (e.g., walls),

as these are most numerous across all games and therefore will be collided with frequently as

a result of DDQN’s random-exploration policy. By contrast, humans and EMPA display much

more focused exploration that targets contact with object instances of unknown type; once these

interactions are learned, further contact occurs only if an object is positive or instrumental (im-

plicated in win-related goals, as in a key needed to open a door), or if contact is unavoidable

given the constraints of the game state.

In the real world, humans (and many other animals) learn quickly when a class of objects

is dangerous, and avoid them whenever possible. In our games, too, people typically interact

with objects that result in immediate loss only a handful of times across all episodes (Figure

6B). EMPA behaves similarly, because of its rapid model-building and planning capacities: as

soon as an object type is discovered to be dangerous, the planner avoids that object if at all

possible. In contrast, DDQN typically interacts hundreds or thousands of times with deadly

objects before it learns to avoid them (Figure 6B).

Towards more human-like learning in AI

A longstanding goal of both cognitive science and artiﬁcial intelligence has been to build mod-

els that capture not only what humans learn, but how they learn — that can learn as efﬁciently

and generalize as ﬂexibly as people do. Our work represents a ﬁrst step towards this goal, by

achieving human-level learning efﬁciency in a large set of simple but challenging and widely

17

Figure 6: (A) EMPA and humans interact with objects of different types in similar proportions;
shown here are distributions of agent interactions with objects classiﬁed as “positive”, “instru-
mental”, “neutral”, and “negative”, for 18 representative games (See Methods). (B) Humans
and EMPA learn quickly from interactions with items that result in immediate loss, and interact
with such objects very few times, in absolute terms.

18

varying video-game tasks. Yet much remains open for future research. The assumptions of

causal determinism and uniformity that let humans and EMPA learn so quickly — assuming

that interactions always produce the same effects, and objects of a given class have the same

causal powers — are powerful and pervasive. Even young children have these biases (64, 65).

But while these principles often hold in the real world, humans can still learn when they are vi-

olated. Hierarchical Bayesian cognitive models capture this aspect of human ﬂexibility (55,66),

and EMPA’s Bayesian model learning could likewise be extended hierarchically to infer prob-

abilistic interactions and object sub-types based on non-deterministic data. Humans also learn

at least as much from others’ experiences as from their own: In the domain of video games,

watching an experienced player or reading instructions supports even more efﬁcient learning

about game dynamics or strategy than playing on one’s own (13, 67). EMPA could be extended

to capture at least basic forms of these social and linguistic learning abilities, by exploiting the

interpretable structure of the learned model’s dynamics and win-loss conditions. Most intrigu-

ingly, humans can use their learned models to achieve completely novel goals outside of those

incentivized during training — an important aspect of children’s play more generally that shows

up speciﬁcally in how children approach video games (49). For instance, a child might adopt

the goal of losing (rather than winning) as quickly as possible, or of spending as much time as

possible on a certain object without dying, or of teaching somebody else how the game works

by touching each type of object exactly once. EMPA could be extended to play games in these

ways as well, as its architecture makes it easy to specify arbitrary intrinsic goals for the planner.

In attempting to shed light on the inductive biases that make human learning so powerful, we

were inspired by earlier model-based RL approaches based on Object-Oriented MDPs (68), Re-

lational MDPs (30), and probabilistic relational rules (31, 32). Our work more generally shares

threads with recent methods that learn dynamics models giving objects, parts, and physics ﬁrst-

class status (14, 33–35), and to a lesser degree, with methods that attempt more implicit un-

19

structured model learning, including Bayesian RL (36–41, 69). We go beyond such approaches

by using explicit representations of physics, agents, events, and goals more directly inspired

by human psychology (22–26, 51–54, 56, 57, 70), and by exploiting these inductive biases in

each of the three main components of our agent — exploration, modeling, and planning. But

the inductive biases in EMPA represent only a step towards the full richness of human intu-

itive theories about physical objects, intentional agents, and how they can interact in the real

world; it remains an open question whether a theory-based approach would work as well in en-

vironments that do not so closely match the agent’s hypothesis space of potential world models.

Our hope is that by building an even richer compositional language for theories and combining

this with more general-purpose program learning techniques (71), this approach can scale to

increasingly varied and complex environments — not just simple two-dimensional video games

but more realistic three-dimensional environments, and eventually, the real world.

Despite our demonstration of the value of building in strong inductive biases, we do not

mean to suggest that AI approaches with less built-in structure could not be developed to achieve

similar performance. On the contrary, we hope that our work will inspire other AI researchers to

set this degree of rapid learning and generalization as their target, and to explore how to incor-

porate — whether through deep model-based learning (42–45), meta-learning (19,72–74), sim-

ulated evolution (75), or hybrid neuro-symbolic architectures (76–80) — inductive biases like

those we have built into our model. We suspect that any system that eventually matches human-

level learning in games or any space of complex novel tasks will exhibit, or at least greatly

beneﬁt from, a decomposition of the problem into learning and planning, and from inductive bi-

ases, theory-based exploration, planning mechanisms like those of EMPA and humans. Where

this prior knowledge ultimately comes from in human beings, and to what extent it can be ac-

quired by machines learning purely from experience or is better built in by reverse-engineering

what we learn from studying humans, remain outstanding questions for future work.

20

Acknowledgments

The authors would like to acknowledge Aritro Biswas, Jacqueline Xu, Kevin Wu, Eric Wu,

Zhenglong Zhou, Michael Janner, Kris Brewer, Hector Geffner, Marty Tenenbaum, and Alex

Kell. The work was supported by the Center for Brains, Minds, and Machines (NSF STC award

CCF-1231216), the Ofﬁce of Naval Research (grant #N00014-17-1-2984), the Honda Research

Institute (Curious-Minded Machines), and gifts from Google and Microsoft.

21

Methods

Informed consent

Experiments were approved by the MIT Committee on the Use of Humans as Experimental

Subjects and the Harvard Committee on the Use of Human Subjects. Informed consent was

obtained from respondents using text approved by the committees.

Games and human behavioral procedures

Games were generated from a core set of 27 games, 17 drawn from the General Video-Game AI

(GVGAI) competition (58) and 10 in a similar style that we designed with particular learning

challenges in mind. Each game is comprised of 4-6 levels, where different levels of a game use

the same object types and dynamics but require different solutions due to their different layouts

(board sizes and object conﬁgurations). For each of these 27 base games, we generated 1-4

variant games by altering different game features, such as the kinds of objects present, object

dynamics, and the game’s win or loss conditions. Humans found these variants as interesting

and challenging to play as they did the core set of games (Extended Data Figure 4). All 90

games are described in the Supplementary Information.

300 human participants were recruited through Amazon Mechanical Turk and were paid

$3.50 plus a bonus of up to $1.00 depending on their cumulative performance across all the

games they played. Prior to playing the games, they were told only that they could use the

arrow keys and the space bar, and that they should try to ﬁgure out how each game worked in

order to play well. They were also told that each game had several levels, and that they needed

to complete a level in order to go to the next level, but that they could have the option to restart

a given level if they were stuck, or to forfeit the rest of the game (and any potential earned

bonus) if they failed to make progress after several minutes. Each participant played 6 games in

random order. Twenty participants played each game, and no participant played more than one

22

variant of a game. At the end of each game, participants were asked if they had ever played that

game before; games for which participants indicated “yes” were excluded from analyses.

Humans interacted with all games via the arrow keys and the spacebar, and the games

evolved through discrete time steps regardless of whether the human pressed a key or not. By

contrast, all models interacted with an environment that evolved only after receiving a keypress,

which for models additionally included the option of a “no-op” action. To make for a fair com-

parison between humans and models we use agent steps, rather than environment steps, as our

metric when reporting learning curves and learning efﬁciency.

In order to test learning as it occurs independently of semantic content conveyed by object

appearance, we displayed all games in “color-only” mode, where each object is depicted as a

uniformly-sized block of a single color, and objects differ in appearance only in their color.

Additionally, color assignments were randomized across participants to ensure that no consis-

tent color-related semantic associations were accidentally conveyed to people. See Extended

Data Figures 2-3 for screen-shots of representative game levels in “normal” and “color-only”

mode. We did not ask participants to indicate whether they were color-blind, but no participants

reported having trouble distinguishing colors in our games.

All models also played games in “color-only” mode. Results reported for EMPA were

averaged over ten runs with different random seeds; EMPA ablations were averaged over ﬁve

different runs, and results for DDQN and Rainbow were each averaged over three runs with

different random seeds.

All games can be played online in “color-only” mode, using a Chrome browser, at

http://pedrotsividis.com/vgdl-games/.

23

Model learning: representation

EMPA builds models of each game environment using the Video Game Description Language,

or VGDL (59). VGDL speciﬁes an environment in terms of three components corresponding to

core aspects of human intuitive theories (49, 81):

Objects: A description of the appearance and basic properties of the different object classes

in the game. These properties all manifest as causal constraints on the dynamic properties of

the objects: whether they move by default, how and how often they move, whether they have

simple goals (chasing or avoiding other objects), and if so, what those goals are. One object

class is always the “avatar”, representing and controlled by the player. We call the set of these

properties the dynamic type of each object. At every time-step, VGDL uses the dynamic type

to calculate proposed positions for each object. If the proposed positions are empty, the objects

move into those positions. If they do not, VGDL handles the resulting collisions by looking at

the rules speciﬁed in the Interactions.

Interactions: A list of rules that specify how pairs of objects interact to produce events.

These events correspond to intuitively natural action concepts, such as pushing, destroying,

or picking up. All state changes beyond the movement patterns speciﬁed in the object-class

descriptions are caused by these events. In turn, events are caused only by contact (collision)

between objects.

For the sake of simplicity, EMPA only considers the following interaction types: stepBack,

bounceForward, reverseDirection, turnAround, wrapAround, pullWithIt,

undoAll, collectResource, changeResource, changeScore, teleport, clone,

destroy, and transform, but could easily be extended to learn additional interaction types

and thus model a far greater space of games.

Termination conditions: A speciﬁcation of the win and loss conditions for a game. We

restrict our scope to games that are won or lost when counts of particular objects on the screen

24

reach zero (e.g., WIN IF count(BLUE)==0).

A VGDL description can also be thought of as procedurally specifying a Markov Decision

Process consisting of the initial state of the game board, a transition function that speciﬁes how

the state evolves between successive time-steps (conditioned on the player’s actions), and a

reward function. The state at any time can be described by the object instances, their classes

and locations; the avatar’s internal state (the agentState), which tracks a set of resources

the player has access to (e.g., health levels, or items carried); and any events occurring between

pairs of objects that are participating in collisions at that time-step. It is useful to decompose

this state s into sI, which refers to the pairs of objects that are participating in events as well

as the nature of those events; sS, which refers to the remaining objects; and sG, which refers to

the Win/Loss/Continue status of the environment. For the purposes of learning, we assume

that the state-space is fully observed.

Our concrete task is to learn a distribution over the set of possible VGDL models, Θ, that

best explain an observed sequence of frames of game-play. For clarity, we decompose a par-

ticular model θ into three components corresponding to the objects (sometimes referred to as

sprites) θS, interactions, θI, and termination conditions (or the agent’s ultimate goals), θG.

The SpriteSet, θS consists of dynamic type deﬁnitions for each unique object class in the

game. For a given class c, its deﬁnition is a vector θSc assigning values to each of the parameters

needed to fully describe a VGDL sprite (see (59) for these complete descriptions). One of

these parameters (vgdlType) always encodes the abstract type of the object, which places

high-level constraints on its behavior: whether it can move on its own or not, and if it can,

whether it is an agent with chasing or avoidance goals, a random agent, or a projectile. The

remaining parameters specify details of the object’s behavior, such as its speed and orientation.

The positions of sprites are updated at each step using simple programs that describe their

behavior. For example:

25

• nextPos(Missile, speed=2, orientation=Right, pos=(x,y)) =

(x+2, y)

• nextPos(Random, speed=1, pos=(x,y)) =

random.choice([(x,y), (x+1,y), (x-1,y), (x,y+1), (x,y-1)]

These programs imply a distribution over next positions for any given parameterization; we

use this distribution to calculate per-object likelihoods in Equation 6.

The InteractionSet, θI, consists of rules, one or more for each pair of object classes, speci-

fying the effects of those classes’ interactions. A rule ri is a tuple (cj, ck, ξ, π) that prescribes

a corresponding event, e = event(ri) = (cj, ck, ξ), which is the application of predicate ξ to

an object of class cj whenever that object collides with an object of class ck, and preconditions

π have been met. When an event e occurs in the current state, we say that it is True.

In

accord with the intuitive concept of “no action at a distance”, events are triggered only by con-

tact between objects. For convenience, we denote by pair(·) the ordered pair of object classes

implicated in a rule or event; a rule and its corresponding event always share the same class

pair, pair(event(ri)) = pair(ri). The predicates ξ ∈ Ξ are observable state transformations on

objects, such as push, destroy, pickUp, and so on. Preconditions in VGDL are restricted

to statements of the form count(agentState, item)< N or count(agentState,

item)>= N , and are meant to capture common contingencies such as, “The Avatar cannot go

through a door unless it holds at least one key” (or, “at least three keys”, or “has been touched

by enemies no more than ﬁve times”). We use πri to refer to the precondition of rule ri, and

write πri = True when the precondition is met.

If an event e occurs between a given class pair (cj, ck), all the other rules in the model that

correspond to that class pair are expected to occur, provided their preconditions πr are met.

We call these expected events implications. Formally, the implications υ(e, θ) of an event are:

26

υ(e, θ) = {event(ri) | (cid:0)pair(ri) = pair(e)(cid:1) ∧ (cid:0)πri = True(cid:1)}. Abusing notation, we will

write that υ(e, θ) = True ⇐⇒ υj = True, ∀υj ∈ υ(e, θ).

Finally, we use η(e, θ) = True to denote that an event e, which is currently occurring, is

explained by a model, θ. An event is explained in a model if there is a rule in the model that can
account for the event. Formally, η(e, θ) = True ⇐⇒ ∃ri ∈ θI | (cid:0)event(ri) = e(cid:1) ∧ (cid:0)πri =
True(cid:1).

The TerminationSet, θG, consists of termination rules, G, each of which causes the episode

to end (transition to a Win or Loss state) if some condition is met; if no termination condi-

tions are met, the episode continues. For simplicity, EMPA only attempts to learn termination

conditions that can be expressed in terms of the count of objects in a given class in some class

equaling zero. This assumption can capture the win and loss conditions of almost all games (for

instance, the agent loses when the number of avatars goes to zero, or wins when the number

of goal ﬂags goes to zero), and could easily be generalized if necessary. We use γ(sI, sS) to

refer to the termination state sg prescribed by rule g, given the remaining aspects of the state,

(sI, sS).

Model learning: Bayesian inference

The posterior probability of a model after a sequence of time steps, 1 : T , is p(θ | s0:T , a0:T −1) ∝

p(s0:T | θ, a0:T −1) p(θ). For simplicity, we use a uniform prior over theories.

When an object is not involved in a collision, its movement patterns result only from its

type and previous state. By contrast, when an object is involved in a collision, its subsequent

state is a function only of the interaction rules that govern the classes of the objects involved

in the collision. This enables us to decompose the likelihood function into two components

corresponding to sI (the colliding objects and the events occurring between them) and sS (the

freely moving objects). Once the state of all objects for a time-step has been resolved, the

27

termination conditions can be evaluated, resulting in whether the the environment is in Win,

Loss, or Continue status. This enables us to additionally factorize out the sG component of

the state when we write the likelihood function, below.

The likelihood function can be decomposed as

p(s0:T | θ, a0:T −1) = p(s0)

T
(cid:89)

t=1

where

p(st

G | θG, st

S, st

I)p(st

I | θI, st−1, at−1)p(st

S | θS, st−1, at−1) (3)

p(st

G | st

S, st

I, θG) =

L
(cid:89)

l=0

1[st

G = γl(st

S, st

I)]

(4)

checks that the model’s ensemble of termination rules correctly predict the termination state at

time t,

p(st

I | θI, st−1, at−1) =

(cid:40)

1 − (cid:15)
(cid:15)

(cid:0)η(ei, θ) = True(cid:1) ∧ (cid:0)υ(ei, θ) = True(cid:1), ∀ei ∈ st
otherwise

I

(5)

checks that all events in st are explained by the rules and that all events expected to occur by

the rules did, in fact, occur, and

p(st

S | θS, st−1, at−1) =

K
(cid:89)

k=0

p(ot

k | θS, ot−1

k

, at−1)

(6)

encodes predictions for the objects o1:K not involved in collisions at time t. These per-object

likelihoods are a function of the particular object parameterizations (dynamic types) speciﬁed

by the model.

The above allows us to maintain a factorized posterior that corresponds to the three model

components we aim to learn: the dynamic type for each class, the interaction rules that re-

solve collisions, and the termination rules that explain the win/loss criteria for a game. For

the dynamic types we maintain an independent posterior over each parameterization, which we

enumerate and update. For the interaction rules, we represent only the maximum a posteriori

hypothesis, which is easy to update because the interactions are deterministic. When we see

28

a violation of a learned rule, we assume the violation is explained by some conditional rule

instead, so we propose the minimal conditional rule that would explain it. The conditional rules

we propose are limited to existential and universal quantiﬁers over aspects of the agent’s state.

For the termination rules we maintain a superset of possible explanations, which the planner

tries to simultaneously satisfy.

To enable more efﬁcient large-scale evaluation, we make several simpliﬁcations to EMPA’s

learing module that could easily be relaxed if desired. We assume the agent has knowledge of

which object corresponds to the avatar, its type (e.g., Shooter, MovingAvatar), as well as

the projectile the avatar emits, if any. All of these model components could be learned by the

inferential mechanisms above, at the cost of expanding the hypothesis space of dynamic types.

Additionally, most games include a class of objects that function as “walls”: they do not move,

and they are more numerous than any other object class. We assume the agent knows the identity

and dynamic type of walls, although their interaction rules (e.g., that they block the motion of

other objects) must still be discovered. Given how numerous these objects are, and that none

of them move, Bayesian evidence for their identity and dynamic type accumulates very rapidly

across the ﬁrst few game trials; hence EMPA’s learning module could easily identify them, at

the cost of more computation early on in learning a new game.

Exploration

EMPA’s exploration module works by setting epistemic goals for the planner, to observe the

data most needed to resolve the learner’s model uncertainty. Because the agent’s knowledge is

encoded in a posterior distribution over a hypothesis space of simulatable models, an optimal

exploration strategy could search over agent trajectories and select ones that, over some time

horizon, maximize expected information gain (EIG) (60, 61) with respect to the agent’s model

posterior. However, the dynamic types of objects, θS, can be quickly learned by observation and

29

do not need to be explicit targets of exploration; only the interaction rules θI and termination

conditions θG depend on the agent’s actions.

Because of the object-oriented, relational nature of the model space, the uniform prior

over interactions, the assumption that objects’ interactions are determined completely by their

classes, and the form of the likelihood function in Equation 5, seeing the events that occur after

just one collision between objects of a given pair of classes cj, ck is highly informative about

that pair’s interaction rules, for all models θ in the hypothesis space. The module therefore sets

exploratory goals of generating interactions between every pair of classes whose interactions

have not yet been observed. In addition, because interactions between the avatar and other ob-

jects can vary as a function of the agentState, exploratory goals between the avatar and

other objects are reset when the agentState changes (See Extended Data Figure 1 for an

example).

Because of the likelihood function for termination conditions in Equation 4, the only states

that are informative about termination rules for any hypothesis are ones where count(cj)==0

for some class cj. When the model learns that the count of some class can be reduced, it sets

reducing its count to zero as an exploratory goal that allows it to learn about θG.

The initially large number of exploratory goals decreases as the model learns more about the

game. At all points in the game, the planner mediates between exploratory goals and win-related

goals as explained below.

Planning: overview

The goal of the EMPA planner is to return high-value action sequences, together with the states

predicted to obtain after each action. The EMPA planner takes as input a model, θ and state,

s, and searches action sequences using the procedure detailed in Algorithm 1. Once a plan is

found, it is executed until completion, or until the world state diverges sufﬁciently from the

30

predicted state, in which case EMPA re-plans. We explain prediction-error monitoring and

re-planning in a subsection below.

Algorithm 1 Pseudocode for the EMPA planner. The planner returns a high-value action se-
quence, as well as the states that are predicted to result from each action.

numNodes, frontier, nodes = 0, [s0],[s0]
while numNodes < maxNodes do

s = argmax([v(s) for s in frontier])
frontier.remove(s)
for a ∈ actions do

1: function PLAN(s0, maxNodes, plannerMode)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

s(cid:48) ∼ p(s(cid:48) | s, a, θ)
numNodes + = 1
if novel(s(cid:48), IW) then
frontier.add(s(cid:48))
nodes.add(s(cid:48))

end for
s(cid:48) = argmax([v(s) for s in nodes])

end while
return actions(s(cid:48).sequence), s(cid:48).sequence

end if

end if
if winCriterion(s(cid:48), plannerMode) then

return actions(s(cid:48).sequence), s(cid:48).sequence

19: end function

In order to plan efﬁciently in games with sparse rewards, EMPA evaluates plans in a hierar-

chical manner, with three distinct levels of representation: goals, subgoals, and goal gradients.

Goals correspond to the fulﬁllment of known and hypothesized termination conditions, as

well as to the fulﬁllment of contact goals speciﬁed by the exploration module. For exam-

ple, if the agent’s highest-probability theory posits that the game is won by eliminating all

objects of class cj (e.g., the goal is to pick up all the diamonds), the planner will set as a

goal count(cj) = 0. Alternatively, before the agent has experienced any interactions with

objects in cj, the exploration module would set contact with any object of class cj as a goal:

contact(avatar, cj).

31

Subgoals represent partial progress towards termination goals. Since all termination goals

specify conditions on the size of a set of objects in a certain class (of the form, |cj| = N ),

subgoals are deﬁned to be any change in the number of instances of a relevant class that moves

in the desired direction relative to the current game state. For example, if the agent currently

has a goal to pick up all diamonds, then any state which decreases the number of diamonds on

the game board counts as fulﬁlling a subgoal.

Goal gradients represent preferences for states that are spatially closer to achieving a sub-

goal, computed based on the distances between pairs of objects in classes that are relevant to

the agent’s current goals. In our running example, a goal gradient expresses a preference for

any action that moves the agent closer to the nearest diamond. But a goal gradient could also

be deﬁned based on bringing an object of one class closer to the nearest object of another class,

if the agent has an exploratory goal to observe an interaction between those two classes.

Goals, subgoals and goal gradients are used both to deﬁne intrinsic rewards, supplementing

the very sparse environmental reward structure, and to deﬁne the planner’s completion criteria.

Planning: modes and metacontroller

The planner operates in three modes, “long-term”, “short-term”, and “stall”, with a metacon-

troller that determines which mode to plan in at a given moment of game play (see Extended

Data Figure 9). Long-term mode returns the ﬁrst plan that leads to a known or hypothesized

win state, as well as any plan that satisﬁes the interaction goals of the exploration module. For a

given game state, long-term mode searches up to a limit of 1,000 imagined states in order to ﬁnd

such a plan, and this limit progressively doubles over repeated failures to ﬁnd successful plans.

Short-term mode returns any plan that would have been returned by the long-term mode, as well

as any plan that fulﬁlls any subgoal. That is, if the number of objects in a class moves closer

to the count speciﬁed by any termination condition, a subgoal has been reached and the plan-

32

ner returns that plan. In addition to terminating under different conditions, this mode searches

shorter plans, initializing the search budget each time the short-term planner is called at one of

three randomly chosen limits, {200, 500, 1,000}. Stall mode simply returns short plans that do

not result in Loss states, and searches only up to 50 nodes to ﬁnd such plans; this mode is only

run when the other two modes have failed to return a satisfactory plan.

Because we did not attempt any performance engineering on the Python VGDL game en-

gine, and this engine can slow down considerably when there are many objects or many interac-

tion rules in play, we allowed the planner for EMPA (and all ablations described below) to run

for as long as needed in real time to ﬁnd a solution, subject to the node search budget for each

move described above and a total time budget of 24 hours for playing each game.

Planning: intrinsic rewards

The agent’s intrinsic reward function is a sum of the goal reward, subgoal reward and goal

gradient reward: V (s, θ) = RG(s, θ) + RSG(s, θ) + RGG(s, θ).

Goal Reward: All planner modes return a plan if a goal state is reached, and they stop

searching states that stem from loss states, so Rg(s, θ) is effectively ∞ for Win states, −∞ for

Loss states, and 0 otherwise.

Subgoal Reward: By contrast, subgoal rewards only cause the short-term planner to return

a plan, but they drive all planners’ intrinsic rewards. Taking advantage of the fact that all

termination goals specify conditions of the form |cj| = N , states are penalized proportional to

their distance from object-count goals. Speciﬁcally, the subgoal reward for a state s is

RSG(s, θ) = ρ

(cid:88)

g∈θG

Ngc − |c(g)|
|c(g)|2

(−1)1[g=W in]

(7)

where Ngc is the class count speciﬁed by g and |c(g)| is the actual class count in the state, of

the class speciﬁed in g. ρ calibrates the relative value of subgoal reward to goal gradient reward

(explained below); we use ρ = 100.

33

Goal gradient reward: The numeric subgoal reward operates at the abstract level of object

counts. Changes in this quantity are too sparse to guide search on their own, so the planner uses

“goal gradients” to score states at a lower level of abstraction by maximizing

RGG(s, θ) =

(cid:88)

dmin

g∈θG

(cid:0)c(cid:48)(g), c(g)(cid:1)
|c(g)|2

(−1)1[g=W in]

(8)

where c(cid:48)(g) refers to the class (if one exists) that can destroy items of class c(g), and dmin(cj, ck)

refers to the distance between the most proximal instances of classes cj and ck. This biases the

agent to seek proximity between objects that need to be destroyed and the objects that can

destroy those objects, and vice-versa for objects that need to be preserved.

The speciﬁcs of the goal gradients (that is, which particular objects to approach, avoid,

ignore, and so on) are generated by the planner within each game by analyzing EMPA’s highest

probability model. For example, if the highest probability model has “The agent wins if there

are 0 diamonds on the screen” as a (paraphrased) termination condition, the planner ﬁnds all

classes that, according to the model, can remove diamonds from the screen, and gives high value

to states in which such objects are near diamonds. More generally, if the highest probability

model has |cj| = 0 as a Win condition for some class, cj, the planner ﬁnds all classes ck that

the model believes can destroy cj and generates goal gradients as speciﬁed above. This simply

involves ﬁnding all the ck in rules (cj, ck, ξ) in which the affected class is cj and the predicate,

ξ, is one of {destroy, pickUp, transformTo}.

In addition to these goal-gradient heuristics, the planner adopts several special ways of

treating resources and projectiles that are not strictly necessary, but are helpful given the use of

a restricted planning budget (see Supplementary Information).

Planning: state pruning

The planner ameliorates the exponential explosion of the state space by using Iterative Width

(IW) (62, 63), which prunes states that are not sufﬁciently different from states previously en-

34

countered in the course of search. This is achieved by deﬁning atoms, logical propositions that

can evaluate to True or False. We indicate the presence of each object in the state with the

atom, (object, True), and we use (object, False) for a known object that is no longer in

the game state. We encode the location of each object with the atom, (object, posx, posy).

For the agent avatar, whose transition probabilities are additionally affected by its orientation,

we use a (object, posx, posy, orientation) tuple. The algorithm maintains a table of

all atoms that have been made true at some point in the search originating from a particular

actual game state. During search, IW prunes any state that fails to make some atom true for the

ﬁrst time in the search.

In games with many moving objects, IW can experience sufﬁcient novelty over a great

many states without the agent needing to make a move; this leads to very inefﬁcient search.

To incentivize the agent to move, we additionally use a penalty on repeated agent positions,

α·count(agent, posx, posy, orientation)2, with α = −10 when there are moving

objects other than the avatar in the game; otherwise α = −1. To further reduce IW’s sensitivity

to moving objects, we do not generate atoms for the locations of objects hypothesized to be

random or whose presence in the game is predicted to last fewer than 5 seconds, and do not

track the location of projectiles produced by the agent.

Planning: prediction-error monitoring and re-planning

In all planning modes, the planner takes as input an (st, θ) pair and returns a list of high-value

actions, at:N , together with their corresponding predicted states from the simulator, ˆst+1:N +1.

When the agent takes an action at, the true environment returns st+1 ∼ pat(st+1, st). After

each action, the agent compares the true st+1 to the predicted ˆst+1. If the agent’s own position

is not as predicted, or if the agent is too close to a dangerous object whose position was not

as predicted, the planner is run again; otherwise, the agent continues execution of the action

35

sequence. We deﬁned “close” as within 3 game squares, but any distance greater than 3 would

work as well, at the cost of more frequent re-planning.

Because planning and acting without a conﬁdent dynamics model will almost always lead to

immediate prediction errors and re-planning, the agent waits for a small number of time-steps

before attempting to move in a new game, and at the beginning of a new level, while it observes

the motions of any dynamic objects in that level. We set these waiting thresholds at 15 and 5

steps, respectively, but any similarly low values would work as well.

DDQN Implementation

We ran DDQN (based on the public repository https://github.com/dxyang/DQN_

pytorch) with parameter settings α = 0.00025, γ = 0.999, τ = 100, experience-replay max

= 50, 000, batch size = 32, and image input recrop size = 64 × 64 × 3. For exploration, (cid:15) was

annealed linearly from 1 to .1 over either 1, 000, 10, 000, or 100, 000 steps. Each model was

run for a maximum of 1 million agent steps. Annealing over 100, 000 steps performed best, and

these results (averaged across three independent runs, each with a different random seed) are

reported in the main text.

Rainbow Implementation

We ran Rainbow (based on the repository https://github.com/google/dopamine

(82)) with parameter settings optimized for sample-efﬁcient performance taken from (42): num-

atoms = 51, vmax = 10, γ = 0.99, update-horizon = 3, min-replay-history = 20, 000, target-

update-period = 50, update-period = 1, epsilon-train = 0.01, replay-scheme = “prioritized”,

optimizer-learning-rate = 0.0000625, optimizer-epsilon = 0.00015. We found that an anneal-

ing period for (cid:15) of 150, 000 steps produced greater sample efﬁciency, and report these results

(averaged across three independent runs, each with a different random seed) in the main text.

36

Each model was run for a maximum of 1 million agent steps.

EMPA ablations: exploration

We considered two (cid:15)-greedy variants to EMPA’s exploration. In both variants, the planner ig-

nores exploration goals and only pursues win-related goals. The ﬁrst variant executes a random

action with probability (cid:15), and executes the action suggested by the planner with probability 1−(cid:15).

The second (cid:15)-greedy DF variant executes the same random-exploration policy, but re-sets the

long-term planning node budget to its initial value after any level forfeit. We annealed (cid:15) linearly

from 1 to 0.1 for both variants, and ran each variant with both a 1000 and 2000 step annealing

schedule.

EMPA ablations: planning

We evaluated ﬁve versions of EMPA with ablations to the planner. Three of these ablations

removed representations of subgoals, goal gradients, or both subgoals and goal gradients, which

affect both the intrinsic reward function as well as the planner’s completion criteria. We also

considered an ablation without IW state pruning, and ﬁnally an ablation that lacks all three of

these features.

Learning efﬁciency

Conﬁdence intervals shown in Figure 3 were generated by calculating the efﬁciency ratio of

the mean EMPA run to the mean human participant over 10,000 bootstrapped samples for each

game, with the exception of two games (Boulderdash 2 and Sokoban) on which EMPA obtained

an efﬁciency score greater than 0 fewer than three times; in those cases only human participants

were resampled.

37

Behavioral analyses

Fine-grained behavioral analyses in Figure 6 were generated by classifying objects from 78

games into the following categories: Positive: if instances of this class participate in a win

condition and the avatar can destroy these objects, or if touching instances of this class grants

points. Instrumental: if touching an instance of this class is necessary for a solution or leads to

a signiﬁcantly shorter solution than is possible without touching the item. Negative: if touch-

ing an instance of this class results in a loss, loss of points, or signiﬁcantly delays a solution.

Neutral: if none of the above obtain.

38

Supplementary Information

Game Descriptions

Antagonist The player has to eat all the hot dogs before the antagonists eat the burger. The

player can win by playing keepaway with the burger while trying to reach the hot dogs, but in

the last level there are too many hot dogs to eat and only one burger — the only way to win is

to push the burger onto a piece of land that the antagonists cannot get to.

Antagonist 1 The player can no longer play keepaway with the burger, but can push boxes

into the antagonists’ path to block them.

Antagonist 2 The hot dogs are harder to get to. To make things a bit easier, the player can

put frosting on the burgers, which makes the antagonists not recognize their food. There’s a

smarter antagonist that isn’t hungry but takes the frosting off the burgers, making them recog-

nizable again. The player has to balance frosting burgers and eating hotdogs in order to win.

Aliens (GVGAI) This is the VGDL version of the classic “Space Invaders”. The player

moves a spaceship left and right and can shoot surface-to-air missiles. Alien bombers move

side-to-side and drop bombs at the agent, who can hide under a base. The base slowly gets

destroyed, so the player must often dodge bombs while shooting all the aliens.

Aliens — Variant 1 The bombs shot by the aliens move randomly.

Aliens — Variant 2 The way to win now is to destroy the protective base.

Aliens — Variant 3 The player’s surface-to-air missiles move three times as slowly, making

aiming more challenging.

Aliens — Variant 4 Aliens move three times as fast, shoot bombs six times as often, and

the player’s base no longer destroys incoming bombs. To compensate, the player can now shoot

surface-to-air missiles continuously.

Avoidgeorge (GVGAI) Evil George chases citizens. If he touches them, they become an-

39

noyed, and if there are no calm citizens in the game, the player loses. To avoid this, the player

can feed the annoyed citizens candy, which makes them calm down. If George touches the

player, the player dies. Keeping citizens calm for 500 game steps results in a win.

Avoidgeorge — Variant 1 The player can now use the candy to make annoyed citizens

disappear, and if all annoyed citizens disappear, the player wins.

Avoidgeorge — Variant 2 The player can still make annoyed citizens disappear, and if all

annoyed citizens disappear, the player wins. George moves faster, but the player can now throw

the candy, making it easier to reach the citizens.

Avoidgeorge — Variant 3 Same as Variant 2, but the player begins the game stuck in a

room and needs to tunnel out of it before doing anything else.

Avoidgeorge — Variant 4 Same as Variant 3, but George moves even faster, and the candy

no longer makes citizens disappear.

Bait (GVGAI) A puzzle game. The player has to pick up a key to go through a door to win.

Falling into holes in the ground results in death, but pushing a box into a hole covers it, destroys

the box, and clears the path.

Bait — Variant 1 Same as the original, but there are dirt cannons lying around that ﬁll any

holes in the dirt’s path. Filling the wrong holes can make it impossible to remove certain boxes

from the game board.

Bait — Variant 2 In some levels, the player has to make the key by pushing metal into a

mold. In other levels, the normal key exists, but is harder to reach than are the metal and the

mold.

Bees and Birds The player has to get the goal while avoiding swarming bees. In level 2, the

goal is surrounded by an electric fence, but the player can release a bear that can eat through the

fence. In level 3, the player can either go through a dangerous swarm of bees or take a longer

path that involves releasing a bear that eats the fence. In level 4, the player can get to the goal

40

simply by going around the fence. Releasing the bear will also cause the bees to be released,

making winning more challenging.

Bees and Birds — Variant 1 The player has to get the goal while avoiding swarming bees.

In all subsequent levels, there are lots of swarming bees, so the best stategy is to release a

mockingbird that will eat the bees, clearing the path to the goal.

Boulderdash (GVGAI) This is the VGDL version of the classic “Dig Dug”. The player has

to pick up 9 diamonds and then exit the level. Boulders in the game are supported by dirt, and if

a boulder falls on the player, the player dies. The player has to avoid two types of subterranean

enemies to avoid dying.

Boulderdash — Variant 1 Same as the original, except the player only has to pick up three

diamonds to win.

Boulderdash — Variant 2 Same as the original, except that one of the subterranean ene-

mies converts dirt to diamonds instead of being trapped by it.

Butterﬂies The player has to catch all the butterﬂies before the butterﬂies activate all the

cocoons. The butterﬂies move randomly, and if they touch a cocoon, the cocoon turns into a

butterﬂy.

Butterﬂies — Variant 1 Same as the original, but butterﬂies are faster.

Butterﬂies — Variant 2 There are more cocoons, meaning there will soon be many more

butterﬂies. The player can destroy cocoons, but still dies if all cocoons are cleared.

Chase (GVGAI) The player has to chase all the birds, which ﬂee the player. Touching a

bird turns it into a carcass. If a scared bird touches a carcass, the carcass turns into a predator

bird which chases and can kill the player.

Chase — Variant 1 Same as the original, except that there are a few gates that release

predator birds sporadically.

Chase — Variant 2 Touching a bird kills it, rather than turning it into a carcass. The game

41

now contains a gate, which if the player shoots, releases a wolf that quickly chases and kills the

player.

Chase — Variant 3 In addition to the birds, there are sheep that rapidly pace in a predeter-

mined path. If these touch a carcass, they turn into zombies that chase the player.

Closing Gates The player has to get to the exit before large sliding gates close.

Closing Gates — Variant 1 The gates close more quickly. The only way for the player to

escape is to shoot a bullet onto the gates’ path; this blocks the gates and lets the player squeeze

through.

Corridor The player has to get to the exit at the end of a long corridor while avoiding

ﬁreballs of different speeds that ﬂy toward the player.

Corridor — Variant 1 The player can now shoot bullets at the ﬁreballs. Depending on the

speed of a ﬁreball, the bullet will either slow it down or speed it up (in fact, the speed of any

ﬁreball can be toggled in a cycle by hitting it with more bullets).

Explore/Exploit The board is full of colorful gems, and the player wins by picking up all

gems of any given color. Different board conﬁgurations incentivize exploring new ways of

winning versus exploiting known ways.

Explore/Exploit — Variant 1 Same as the original game, except evil gnomes chase the

player as the player collects the gems.

Explore/Exploit — Variant 2 There are no evil gnomes, but now the gems are being carried

by gnomes that ﬂee the player.

Explore/Exploit — Variant 3 Same as the original, except one of the gems the player

encounters early on is poisonous.

Helper The player has to help minions get to their food. Sometimes the food is blocked by

a boiling pot of water that only the player can destroy, and sometimes the minions are boxed in

by a fence that the player can push food through.

42

Helper — Variant 1 The player wins by eating all the food, but can only eat it after taking

it to the minions for processing.

Helper — Variant 2 The player’s goal is once again to feed minions. In this variant the

player can shoot a path through red fences to free minions. In the last level, the player has

manually clear one path, then shoot through some fences, and then push food through a third

fence in order to feed the minion.

Frogs (GVGAI) This is the classic “Frogger”. The player has to cross a road while avoiding

dangerous cars and then step carefully on moving logs to cross a river, to get to an exit. The

level layouts of this game are slightly modiﬁed from the original GVGAI layouts, in order to

make the game playable in the original version of VGDL this project was built on.

Frogs — Variant 1 The cars move differently now; they move at different speeds, and when

they hit the edge of the screen they rapidly turn around and drive in the other direction.

Frogs — Variant 2 The cars and logs move faster than the original version, and fewer logs

ﬂoat down the river, making it more difﬁcult to reach the goal by ordinary means. But there is

a device that, if used, teleports the player nearer to the goal.

Frogs — Variant 3 Now there aren’t any logs to help the player cross the water, but the

player can throw mud at the water to build bridges.

Jaws (GVGAI) The player dies if touched by a chasing shark. Cannons on the side of

the screen shoot cannonballs at the player. The player can shoot bullets at the cannonballs, to

convert them into a new kind of metal. Picking this metal up gives points. The player wins by

surviving for 500 game steps.

Jaws — Variant 1 Now there is a fence that prevents the player from getting too close to

the cannons.

Jaws — Variant 2 Each piece of metal grants 5 health points; having any health points

protects the player from the shark; the shark takes away one health point each time there’s

43

contact. The shark can be destroyed if the player has 15 health points. The player wins after

destroying the shark or surviving for 500 game steps.

Lemmings (GVGAI) In this classic game, the player has to help a group of lemmings get to

their own exit. To do so, the player must shovel a tunnel through dirt, and incur a loss of points

with every shovel action.

Lemmings — Variant 1 Now the player gets points for shoveling, rather than losing points.

Lemmings — Variant 2 The player can release a mole, which loves to shovel and will clear

all the dirt. But if the mole happens to touch the player, the player dies.

Lemmings — Variant 3 The mole is still there and will shovel dirt when released. But if

the mole touches a lemming, the mole turns into a snake that chases and kills the player.

Missile Command (GVGAI) Spaceships want to destroy the player’s bases. The player

defends the bases by shooting short-range lasers, and wins upon destroying all the spaceships,

or loses after all the bases are destroyed.

Missile Command — Variant 1 The bases don’t stay in place; they ﬂoat around randomly.

Missile Command — Variant 2 The player can now shoot long-range lasers. However,

these lasers bounce off the edges of the game and kill the player upon contact.

Missile Command — Variant 3 A newer, faster spaceship tries to destroy the player’s base,

and the player is only equipped with a short-range laser.

Missile Command — Variant 4 There are more fast spaceships, making it very difﬁcult to

destroy all of them before they reach the bases. However, the player can shoot the short-range

laser at the ozone layer, which transforms into a shield that the enemies cannot pass through.

MyAliens (GVGAI) The player can only move sideways. Fast-moving bombs drop from

the top. One type of bomb kills the player; the other gives points. Surviving the onslaught of

bombs for 500 game steps results in a win.

MyAliens — Variant 1 The player can now collect the safe bombs, and can go to an exit

44

once ﬁve of those have been collected.

MyAliens — Variant 1 The player can now move in all directions, but bombs come from

all directions, too. The player wins by either picking up ﬁve safe bombs and getting to the exit,

or by surviving for 500 game steps.

Plaqueattack (GVGAI) 40 Burgers and hotdogs emerge from their bases and attack cavi-

ties; if they reach all the cavities the player dies. The player wins by destroying all the hotdogs

and burgers with a laser.

Plaqueattack — Variant 1 The burgers and hotdogs move faster, but there are only 24 of

them. The player has to destroy them while avoiding a roving drill.

Plaqueattack — Variant 2 There is no drill, but now there are bouncing projectiles that kill

the player on contact.

Plaqueattack — Variant 3 Now the player has to face 40 burgers and hotdogs again, as

well as the bouncing projectiles, but can win by destroying all the gold ﬁllings, instead.

Portals (GVGAI) The player wins by reaching an exit. Certain rooms, including the room

with the exit, are accessible only by going through portals that teleport the player to their portal-

exits. Most portals have multiple portal-exits, and the player’s ﬁnal location is randomly chosen

from those. While completing this task, the player has to avoid bouncing missiles, roving space-

monsters, and inert traps.

Portals — Variant 1 Same dynamics as above, with a new portal type and more complicated

level layouts, but with fewer missiles and space-monsters.

Portals — Variant 2 Uses the complex layouts as in Variant 1. The player now has to

contend with roving monsters that can pass through walls.

Preconditions The player wins by picking up a diamond. Getting the differently-colored

fake diamond does nothing. Usually the diamond is surrounded by poison, which the player can

pass through only after drinking an antidote. In the last level the player has to drink an antidote

45

to pass through the poison that blocks two antidotes, in order to pass through the double-poison

in order to get to the triple antidote, in order to get through the triple poison that guards the

diamond.

Preconditions — Variant 1 The player can also pick up a more powerful antidote that

allows passage through two poisons. Different level layouts encourage using either the single

or double antidote.

Preconditions — Variant 2 Same rules as the original, except only one antidote can be

ingested at a time.

Push Boulders The goal is to get to the exit. Touching silver or green ore results in a loss.

Boulders can be pushed by the player; these destroy the ore. Limestone can be destroyed by

the player but otherwise has no effect on the game. The game often involves solving maze-like

challenges and necessitates using boulders to clear dangerous obstacles.

Push Boulders — Variant 1 Boulders destroy green (but they don’t destroy or push silver).

The mazes are more difﬁcult and involve multiple uses of boulders to clear ore.

Push Boulders — Variant 2 Now the boulders are too heavy to move, but one-time-use-

only cannons shoot cannon balls that destroy any boulders or ore in their path. The player has

to ﬁre up the right cannons to clear paths. Dust clouds do nothing.

Relational The goal is to make all the blue potions disappear, which happens when they

are pushed into ﬁre. In the second level, there is no ﬁre, but touching the red potion converts

it to ﬁre. In the third level there is no red potion or ﬁre, but pushing a box into a purple potion

converts the two into ﬁre. In the fourth level, a green potion can be converted into a box that

can be pushed into a purple potion to make the ﬁre.

Relational — Variant 1 Same relational rules as above, but the blue potion is now carried

by a gnome that chases the player. While nothing happens if the gnome touches the player, the

player has to move around in such a way as to get the gnome to run into ﬁre.

46

Relational — Variant 2 Now the gnome is ﬂeeing the player. While nothing happens if the

player touches the gnome, the player has to move around in such a way as to get the gnome to

run into ﬁre.

Sokoban (GVGAI) In this classic puzzle game, the player wins by pushing boxes into holes.

Sokoban — Variant 1 The player can make use of portals, which teleport either the player

or boxes to a particular portal-exit location on the game screen.

Sokoban — Variant 2 This variant does not have portals. Instead, there is dirt lying around

that can ﬁll holes. Filling certain holes makes the levels unwinnable.

Surprise The player has to pick up all of the red apples. Touching a cage releases a

randomly-moving Tasmanian devil. If the Tasmanian devil touches a green gem, the Tasmanian

devil gets cloned and the gem disappears. The player can pass through gems and Tasmanian

devils

Surprise — Variant 1 The player can no longer pass through the Tasmanian devils; releas-

ing the devils makes winning much more challenging.

Surprise — Variant 2 The goal is to destroy all the gems, which can only be done by

releasing the Tasmanian devil.

Survive Zombies (GVGAI) The player has to survive for 500 game steps while avoiding

zombies that emerge from ﬁery gates. The ﬁery gates are dangerous, too. Fortunately, bees

come out of ﬂowers, and when bees touch zombies, the two collide and turn into honey. Eating

honey gives the player temporary immunity from zombies.

Survive Zombies — Variant 1 The way to win is to collect all the honey.

Survive Zombies — Variant 2 The way to win is now to kill all the zombies, by ﬁrst eating

some honey for immunity.

Watergame (GVGAI) A puzzle game. The player has to reach an exit, which is usually

surrounded by water that drowns the player. Pushing dirt onto water clears the water.

47

Watergame — Variant 1 Red boulders can be pushed onto the dirt, thereby destroying it.

Destroying some dirt is necessary in order to clear space, but destroying the wrong pieces of

dirt makes levels unwinnable.

Watergame — Variant 2 More dirt can be made by mixing light-green and yellow potions.

Zelda (GVGAI) The player has to pick up a key and get to a door, while using a sword to

destroy randomly-moving dangerous creatures.

Zelda — Variant 1 The player has to pick up three keys before getting to the door.

Zelda — Variant 2 The player only needs one key, now, but both the key and the door move

randomly throughout the level.

Zelda — Variant 3 Now the key and door are carried by elves that ﬂee the player.

48

Resource and Projectile Heuristics

• Picking up resources is a subgoal: Conditional interactions in our version of VGDL

occur as a function of either having 1 instance of a resource or having as many as the

agent can carry. Both of these are treated as planner subgoals.

• Transfer goal gradients from “Flicker” objects emitted by the agent to the agent

itself: Such objects (e.g., swords) appear directly in front of the avatar and disappear

shortly after use. Since the avatar must be near an object in order to make contact between

the Flicker and that object, this heuristic incentivizes the avatar to approach objects it

wants to contact with the Flicker.

• Use rollouts when the agent ﬁres a projectile: The intrinsic reward for each action

is ordinarily calculated as a function of the immediately-resulting state. When the agent

shoots what it believes to be a projectile, it instead simulates random actions forward until

that projectile is no longer on the screen or until N steps have passed (where N is equiv-

alent to the block length of the longest axis on the game screen). Any subgoals reached

during the rollout that involve the projectile are considered to be subgoals achieved by

the original action of shooting.

• Do not explore the consequences of agent motion when exploring the consequences

of ﬁring projectiles: In the normal course of search, when the agent explores the part

of the search tree in which it has shot a projectile that is still on the game board, it only

takes the no-op action as long as it is at least 3 block lengths away from the nearest object

thought to be dangerous.

49

Extended Data Figure 1: EMPA learning to play two games (Zelda and Frogs), showing
human-like exploration and planning behaviors. Solving these (and many other) tasks requires
ﬁrst learning about objects that are difﬁcult to reach using simpler stochastic exploration policies
(6, 83, 84): In order to achieve the ﬁrst win in Zelda, the agent has to learn to pick up a key
and then use it to open a door; in order to achieve the ﬁrst win in Frogs, the agent has to
learn to cross dangerous water by stepping on moving logs. EMPA, like humans, learns about
these goal-relevant objects rapidly because it explicitly speciﬁes reaching unknown objects as
exploratory goals for the planner, rather than waiting to encounter them by chance as in (cid:15)-greedy
exploration (6) or surprise-based curiosity (83, 84). (A) In Zelda, EMPA begins by observing
that spiders move but has not yet learned how they move, so it predicts random motions (pink
arrows). (B) After several time-steps, EMPA learns to predict their motions and now begins to
plan. It does not know what occurs upon contact with any of the objects, so it sets as exploration
goals (white squares) to reach an object of each type. The planner ﬁnds efﬁcient paths (yellow)
to those objects and chooses whichever is found ﬁrst. (C) After several dozen time-steps, the
agent has learned that walls block motion and that spiders are dangerous (red x-shapes) but is
still uncertain about the key and the door, so it plans paths to these exploration goals while
avoiding the spiders. (D) After a few hundred time-steps, the agent has learned that it can pick
up a key, which as a resource changes the state of the agent. This resets all exploration goals:
maybe a key allows the agent to pass through walls, or makes it immune to spiders? (E) After
another hundred steps, the agent has interacted with all objects both before and after touching
the key, and has learned that the only effect of touching the key is that now touching the door
wins the level. The agent’s knowledge generalizes to any board layout that contains the same
object types. Here it immediately plans a winning path (orange) to pick up the key and reach the
door, though it may also generate paths to exploration goals (yellow) to new object types that
appear, such as the bat and the scorpion. (F-H) EMPA begins learning in Frogs just as in Zelda:
it initially predicts random motion of moving objects until it receives enough evidence to predict
the regular motion of the cars and logs. It then learns that cars and water are dangerous but that
standing on a log gives it the ability to safely cross the water. (I) Equipped with this knowledge,
EMPA can generate a path that allows it to fulﬁll its ﬁnal exploration goal of reaching the ﬂag.
(J) Having learned to win the game, the agent now generates winning paths (orange) on entirely
new game levels, although it may also generate paths to exploration goals (yellow) if new object
types appear.

50

Extended Data Figure 2: VGDL descriptions of “Frogs” and “Plaqueattack”, together with
screenshots of representative levels in “color-only” and “normal” mode.

51

Extended Data Figure 3: VGDL descriptions of “Avoidgeorge”, “Bait”, “Butterﬂies”, and
“Zelda”, together with screenshots of representative levels in “color-only” and “normal” mode.

52

Extended Data Figure 4: Mean and 95% conﬁdence intervals of each game’s subjective
difﬁculty (panel A) and “interestingness” (panel B), grouped by source game. Our variant
games are comparable to their source games on both metrics.

53

Extended Data Figure 5: Humans (green), EMPA (blue), DDQN (grey), and Rainbow (red)
learning curves over the initial 1,000 steps of play.

54

Extended Data Figure 6: Humans (green), EMPA (blue), DDQN (grey), and Rainbow (red)
learning curves over the initial 10,000 steps of play.

55

Extended Data Figure 7: Humans (green), EMPA (blue), DDQN (grey), and Rainbow (red)
learning curves over the initial 1 million steps of play.

56

Extended Data Figure 8: EMPA’s distributions of object interactions match human distribu-
tions better than those of DDQN, particularly in slow-paced games in which the agent is the
only moving item. Diamonds show mean distances; lines show 95% conﬁdence intervals.

57

Slow-paced gamesFast-paced games0.00.10.20.30.4DDQNEMPADDQNEMPAL1 distance between model and human interaction distributionsModel typeExtended Data Figure 9: The metacontroller determines which of EMPA’s three planning
modes are engaged at a given moment of gameplay. The planner always begins a new game
in short-term mode, and switches to other modes when it fails to ﬁnd a plan within the current
search budget. Which mode it switches to depends on whether it considers itself “stuck”. The
agent considers itself stuck in short-term mode when it has either lost twice in the same way, or
the game contains no moving objects other than the agent itself (and hence no new possibilities
to act constructively will arise merely by waiting); in each of these cases the planner switches
to long-term mode. If the planner fails but the agent is not stuck, it switches to stall mode for
one planning cycle. The agent may also consider itself stuck in long-term mode, if it has failed
to ﬁnd a plan from a given state more than once, in which case it restarts the level and doubles
its planning budget.

58

References

1. Sutton, R. S. & Barto, A. G. Toward a modern theory of adaptive networks: expectation

and prediction. Psychological review 88, 135 (1981).

2. Schultz, W., Dayan, P. & Montague, P. R. A neural substrate of prediction and reward.

Science 275, 1593–1599 (1997).

3. Watkins, C. J. & Dayan, P. Q-learning. Machine learning 8, 279–292 (1992).

4. Daw, N. D., O’doherty, J. P., Dayan, P., Seymour, B. & Dolan, R. J. Cortical substrates for

exploratory decisions in humans. Nature 441, 876 (2006).

5. Guo, X., Singh, S., Lee, H., Lewis, R. L. & Wang, X. Deep learning for real-time atari game

play using ofﬂine monte-carlo tree search planning.

In Advances in neural information

processing systems, 3338–3346 (2014).

6. Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518,

529–533 (2015).

7. Van Hasselt, H., Guez, A. & Silver, D. Deep reinforcement learning with double q-learning.

In AAAI, vol. 16, 2094–2100 (2016).

8. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. arXiv

preprint arXiv:1511.05952 (2015).

9. Stadie, B. C., Levine, S. & Abbeel, P. Incentivizing exploration in reinforcement learning

with deep predictive models. arXiv preprint arXiv:1507.00814 (2015).

10. Mnih, V. et al. Asynchronous methods for deep reinforcement learning. In International

Conference on Machine Learning, 1928–1937 (2016).

59

11. He, F. S., Liu, Y., Schwing, A. G. & Peng, J. Learning to play in a day: Faster deep

reinforcement learning by optimality tightening. arXiv preprint arXiv:1611.01606 (2016).

12. Hessel, M. et al. Rainbow: Combining improvements in deep reinforcement learning.

arXiv preprint arXiv:1710.02298 (2017).

13. Tsividis, P., Pouncy, T., Xu, J., Tenenbaum, J. B. & Gershman, S. J. Human learning in

Atari. In AAAI Spring Symposium Series (2017).

14. Kansky, K. et al. Schema networks: Zero-shot transfer with a generative causal model of

intuitive physics. arXiv preprint arXiv:1706.04317 (2017).

15. Silver, D. et al. A general reinforcement learning algorithm that masters chess, shogi, and

go through self-play. Science 362, 1140–1144 (2018).

16. Vinyals, O. et al. Alphastar: Mastering the real-time strategy game starcraft ii (2019).

17. OpenAI. OpenAI Dota 2 1v1 bot. https://openai.com/the-international/

(2017).

18. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience

replay in distributed reinforcement learning. In International Conference on Learning Rep-

resentations (2019). URL https://openreview.net/forum?id=r1lyTjAqYX.

19. Jaderberg, M. et al. Human-level performance in ﬁrst-person multiplayer games with

population-based deep reinforcement learning. arXiv preprint arXiv:1807.01281 (2018).

20. Badia, A. P. et al. Agent57: Outperforming the Atari human benchmark. In III, H. D. &

Singh, A. (eds.) Proceedings of the 37th International Conference on Machine Learning,

vol. 119 of Proceedings of Machine Learning Research, 507–517 (PMLR, 2020). URL

http://proceedings.mlr.press/v119/badia20a.html.

60

21. Sutton, R. S. & Barto, A. G. Reinforcement learning: An introduction (MIT press, 2018).

22. Spelke, E. S. Principles of object perception. Cognitive science 14, 29–56 (1990).

23. Baillargeon, R. Infants’ physical world. Current directions in psychological science 13,

89–94 (2004).

24. Spelke, E. S. & Kinzler, K. D. Core knowledge. Developmental science 10, 89–96 (2007).

25. Csibra, G. Goal attribution to inanimate agents by 6.5-month-old infants. Cognition 107,

705–717 (2008).

26. Gopnik, A. et al. A theory of causal learning in children: causal maps and bayes nets.

Psychological review 111, 3 (2004).

27. Murphy, G. L. & Medin, D. L. The role of theories in conceptual coherence. Psychological

Review 92, 289–316 (1985).

28. Carey, S. Conceptual Change in Childhood (MIT press, 1985).

29. Gopnik, A., Meltzoff, A. N. & Bryant, P. Words, thoughts, and theories, vol. 1 (Mit Press

Cambridge, MA, 1997).

30. Guestrin, C., Koller, D., Gearhart, C. & Kanodia, N. Generalizing plans to new environ-

ments in relational mdps.

In Proceedings of the 18th international joint conference on

Artiﬁcial intelligence, 1003–1010 (Morgan Kaufmann Publishers Inc., 2003).

31. Pasula, H., Zettlemoyer, L. S. & Kaelbling, L. P. Learning probabilistic relational planning

rules. In ICAPS, 73–82 (2004).

32. Pasula, H. M., Zettlemoyer, L. S. & Kaelbling, L. P. Learning symbolic models of stochastic

domains. Journal of Artiﬁcial Intelligence Research 29, 309–352 (2007).

61

33. Xia, V., Wang, Z. & Kaelbling, L. P. Learning sparse relational transition models. arXiv

preprint arXiv:1810.11177 (2018).

34. Scholz, J., Levihn, M., Isbell, C. & Wingate, D. A physics-based model prior for object-

oriented mdps. In International Conference on Machine Learning, 1089–1097 (2014).

35. Keramati, R., Whang, J., Cho, P. & Brunskill, E. Strategic object oriented reinforcement

learning. arXiv preprint arXiv:1806.00175 (2018).

36. Oh, J., Guo, X., Lee, H., Lewis, R. L. & Singh, S. Action-conditional video prediction

using deep networks in atari games. In Advances in Neural Information Processing Systems,

2863–2871 (2015).

37. Fragkiadaki, K., Agrawal, P., Levine, S. & Malik, J. Learning visual predictive models of

physics for playing billiards. arXiv preprint arXiv:1511.07404 (2015).

38. Chiappa, S., Racaniere, S., Wierstra, D. & Mohamed, S. Recurrent environment simulators.

arXiv preprint arXiv:1704.02254 (2017).

39. Leibfried, F., Kushman, N. & Hofmann, K. A deep learning approach for joint video frame

and reward prediction in atari games. arXiv preprint arXiv:1611.07078 (2016).

40. Ha, D. & Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122 (2018).

41. Racani`ere, S. et al.

Imagination-augmented agents for deep reinforcement learning.

In

Advances in neural information processing systems, 5690–5701 (2017).

42. Kaiser, L. et al. Model-based reinforcement

learning for atari.

arXiv preprint

arXiv:1903.00374 (2019).

62

43. Watters, N., Matthey, L., Bosnjak, M., Burgess, C. P. & Lerchner, A. Cobra: Data-efﬁcient

model-based rl through unsupervised object discovery and curiosity-driven exploration.

arXiv preprint arXiv:1905.09275 (2019).

44. Schrittwieser, J. et al. Mastering atari, go, chess and shogi by planning with a learned

model. Nature 588, 604–609 (2020).

45. Hafner, D., Lillicrap, T. P., Norouzi, M. & Ba, J. Mastering atari with discrete world

models. In International Conference on Learning Representations (2021). URL https:

//openreview.net/forum?id=0oabwyZbOu.

46. Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. & Dolan, R. J. Model-based inﬂuences

on humans’ choices and striatal prediction errors. Neuron 69, 1204–1215 (2011).

47. Otto, A. R., Gershman, S. J., Markman, A. B. & Daw, N. D. The curse of planning: dissect-

ing multiple reinforcement-learning systems by taxing the central executive. Psychological

science 24, 751–761 (2013).

48. Kool, W., Gershman, S. J. & Cushman, F. A. Cost-beneﬁt arbitration between multiple

reinforcement-learning systems. Psychological science 28, 1321–1333 (2017).

49. Lake, B. M., Ullman, T. D., Tenenbaum, J. B. & Gershman, S. J. Building machines that

learn and think like people. Behavioral and Brain Sciences 40 (2017).

50. Allen, K., Smith, K. & Tenenbaum, J. B. Rapid trial-and-error learning with simulation

supports ﬂexible tool use and physical reasoning. Proceedings of the National Academy of

Sciences 117, 29302–29310 (2020).

51. Xu, F. & Garcia, V. Intuitive statistics by 8-month-old infants. Proceedings of the National

Academy of Sciences 105, 5012–5015 (2008).

63

52. Schulz, L., Goodman, N. D., Tenenbaum, J. B. & Jenkins, A. C. Going beyond the ev-

idence: Abstract laws and preschoolers’ responses to anomalous data. Cognition 109,

211–223 (2008).

53. Cook, C., Goodman, N. D. & Schulz, L. Where science starts: Spontaneous experiments

in preschoolers’ exploratory play. Cognition 120, 341–349 (2011).

54. Tsividis, P., Gershman, S., Tenenbaum, J. & Schulz, L.

Information selection in noisy

environments with large action spaces. In Proceedings of the 36th Annual Conference of

the Cognitive Science Society, 1622–1627 (Quebec City, Canada, 2014).

55. Grifﬁths, T. L. & Tenenbaum, J. B. Theory-based causal induction. Psychological review

116, 661 (2009).

56. Schulz, L. Finding new facts; thinking new thoughts. In Xu, F., Kushnir, T. & Benson, J.

(eds.) Advances in Child Development and Behavior (Academic Press, Oxford, 2012).

57. Tsividis, P., Tenenbaum, J. B. & Schulz, L. Hypothesis-space constraints in causal learning.

In CogSci (2015).

58. Perez-Liebana, D. et al. The 2014 general video game playing competition. IEEE Trans-

actions on Computational Intelligence and AI in Games 8, 229–243 (2016).

59. Schaul, T. A video game description language for model-based or interactive learning. In

Computational Intelligence in Games (CIG), 2013 IEEE Conference on Games, 1–8 (IEEE,

2013).

60. Lindley, D. V. et al. On a measure of the information provided by an experiment. The

Annals of Mathematical Statistics 27, 986–1005 (1956).

64

61. Bernardo, J. M. Expected information as expected utility. the Annals of Statistics 686–690

(1979).

62. Geffner, H. & Lipovetzky, N. Width and serialization of classical planning problems

(2012).

63. Lipovetzky, N. & Geffner, H. Best-ﬁrst width search: Exploration and exploitation in

classical planning. In AAAI, 3590–3596 (2017).

64. Schulz, L. E. & Sommerville, J. God does not play dice: Causal determinism and

preschoolers’ causal inferences. Child development 77, 427–442 (2006).

65. Gelman, S. A. The essential child: Origins of essentialism in everyday thought (Oxford

Series in Cognitive Dev, 2003).

66. Kemp, C., Goodman, N. D. & Tenenbaum, J. B. Learning to learn causal models. Cognitive

Science 34, 1185–1243 (2010).

67. Tessler, M. H., Goodman, N. D. & Frank, M. C. Avoiding frostbite: It helps to learn from

others. Behavioral and Brain Sciences 40 (2017).

68. Diuk, C., Cohen, A. & Littman, M. L. An object-oriented representation for efﬁcient

reinforcement learning. In Proceedings of the 25th international conference on Machine

learning, 240–247 (ACM, 2008).

69. Ghavamzadeh, M., Mannor, S., Pineau, J. & Tamar, A. Bayesian reinforcement learning:

A survey. arXiv preprint arXiv:1609.04436 (2016).

70. Pouncy, T., Tsividis, P. & Gershman, S. J. What is the model in model-based planning?

Cognitive Science 45, e12928 (2021).

65

71. Ellis, K., Morales, L., Meyer, M. S., Solar-Lezama, A. & Tenenbaum, J. B. Dreamcoder:

Bootstrapping domain-speciﬁc languages for neurally-guided bayesian program learning.

In Neural Abstract Machines and Program Induction Workshop at NIPS (2018).

72. Wang, J. X. et al. Prefrontal cortex as a meta-reinforcement learning system. Nature

neuroscience 21, 860–868 (2018).

73. Espeholt, L. et al. Impala: Scalable distributed deep-rl with importance weighted actor-

learner architectures. arXiv preprint arXiv:1802.01561 (2018).

74. Dasgupta, I. et al. Causal reasoning from meta-reinforcement learning. arXiv preprint

arXiv:1901.08162 (2019).

75. Salimans, T., Ho, J., Chen, X., Sidor, S. & Sutskever, I. Evolution strategies as a scalable

alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017).

76. Battaglia, P. W. et al. Relational inductive biases, deep learning, and graph networks. arXiv

preprint arXiv:1806.01261 (2018).

77. Chang, M. B., Ullman, T., Torralba, A. & Tenenbaum, J. B. A compositional object-based

approach to learning physical dynamics. arXiv preprint arXiv:1612.00341 (2016).

78. Watters, N. et al. Visual interaction networks: Learning a physics simulator from video. In

Advances in neural information processing systems, 4539–4547 (2017).

79. Zambaldi, V. et al.

Relational deep reinforcement

learning.

arXiv preprint

arXiv:1806.01830 (2018).

80. Garnelo, M., Arulkumaran, K. & Shanahan, M. Towards deep symbolic reinforcement

learning. arXiv preprint arXiv:1609.05518 (2016).

66

81. Carey, S. The origin of concepts (Oxford University Press, 2009).

82. Castro, P. S., Moitra, S., Gelada, C., Kumar, S. & Bellemare, M. G. Dopamine: A Research

Framework for Deep Reinforcement Learning. arXiv preprint arXiv:1812.06110 (2018).

83. Pathak, D., Agrawal, P., Efros, A. A. & Darrell, T. Curiosity-driven exploration by self-

supervised prediction. In International Conference on Machine Learning (ICML), vol. 2017

(2017).

84. Haber, N., Mrowca, D., Wang, S., Fei-Fei, L. F. & Yamins, D. L. Learning to play with

intrinsically-motivated, self-aware agents. In Advances in Neural Information Processing

Systems, 8388–8399 (2018).

67

