1
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

5
v
6
1
9
9
0
.
5
0
9
1
:
v
i
X
r
a

Generative Grading: Near Human-level Accuracy for
Automated Feedback on Richly Structured Problems

Ali Malik1*, Mike Wu1*,
Vrinda Vasavada1, Jinpeng Song1, Madison Coots1,
John Mitchell1, Noah Goodman1,2, Chris Piech1

1Department of Computer Science, Stanford University
2Department of Psychology, Stanford University
{malikali, wumike, vrindav, jsong5, mcoots, jcm, ngoodman, piech}@cs.stanford.edu

ABSTRACT
Access to high-quality education at scale is limited by the
diﬃculty of providing student feedback on open-ended as-
signments in structured domains like computer program-
ming, graphics, and short response questions. This prob-
lem has proven to be exceptionally diﬃcult: for humans, it
requires large amounts of manual work, and for computers,
until recently, achieving anything near human-level accuracy
has been unattainable. In this paper, we present generative
grading: a novel computational approach for providing feed-
back at scale that is capable of accurately grading student
work and providing nuanced, interpretable feedback. Our
approach uses generative descriptions of student cognition,
written as probabilistic programs, to synthesise millions of
labelled example solutions to a problem; we then learn to
infer feedback for real student solutions based on this cog-
nitive model.

We apply our methods to three settings.
In block-based
coding, we achieve a 50% improvement upon the previous
best results for feedback, achieving super-human accuracy.
In two other widely diﬀerent domains—graphical tasks and
short text answers—we achieve major improvement over the
previous state of the art by about 4x and 1.5x respectively,
approaching human accuracy. In a real classroom, we ran
an experiment where we used our system to augment hu-
man graders, yielding doubled grading accuracy while halv-
ing grading time.

INTRODUCTION

1.
Enabling global access to high-quality education is a long-
standing challenge. The combined eﬀect of increasing costs
per student [3] and rising demand for higher education makes
this issue particularly pressing. A major barrier to provid-
ing quality education has been the ability to automatically
provide meaningful and accurate feedback on student work,

Figure 1: Students solve problems by making decisions that re-
sult in their ﬁnal solution (generative model). Providing feedback
requires the reverse task of seeing a solution and inferring the stu-
dent decisions that lead to this solution (inference model).

particularly for open-ended tasks like programming or sim-
ple natural language response.

Learning to provide feedback on richly structured problems
beyond simple multiple-choice has proven to be a hard ma-
chine learning problem. Five issues have emerged, many of
which are typical of human-centred AI problems: (1) stu-
dent work is extremely diverse, exhibiting a heavy tailed
distribution where most solutions are rare, (2) student work
is diﬃcult and expensive to label with ﬁne-grained feedback,
(3) we want to provide feedback (without historical data) for
even the very ﬁrst student, (4) grading is a precision-critical
domain with a high cost to misgrading, and (5) predictions
must be explainable and justiﬁable to instructors and stu-
dents. Despite extensive research that combines massive
education data with cutting-edge deep learning [23, 1, 33,
28, 18, 14], these issues make traditional supervised learning
inadequate for automatic feedback at scale.

Human instructors are experts at providing student feed-
back. When grading assignments, they have an understand-
ing of the decisions and missteps students might make when
solving a problem, and what corresponding solutions these
choices would result in. For example, an instructor un-
derstands that a student wanting to repeat something in
a programming assignment might use a for loop or manu-
ally write repeated statements. And given that the student

Generative modelInference modelDecision processOutput solution 
 
 
 
 
 
uses a loop, their loop could be correct or oﬀ-by-one.

In essence, instructors mentally possess generative models
of student decision-making and how these decisions mani-
fest in a ﬁnal solution (Fig. 1, forwards). When providing
feedback, the instructor does inference: given a student so-
lution, they use their mental model to try and determine
the underlying student decisions that could have resulted in
this solution (Fig. 1, backwards).

In this paper, we propose an automated feedback system
that mimics the instructor process as closely as possible.
Firstly, the system elicits a literal generative model from
an instructor in the form of a concrete student simulator.
Secondly, it uses deep neural networks and a novel inference
method with this simulator to learn how to do inference on
student solutions, without using any labelled data. Finally,
the inference model is used to provide automated feedback
to real student solutions. We call this end-to-end approach
generative grading.

When used across a spectrum of public education data sets,
our automated feedback system is able to grade student work
with close to expert human-level ﬁdelity. In block-based cod-
ing, we attain super-human accuracy, achieving a 50% im-
provement over the previous best results for feedback. In two
other widely diﬀerent domains—graphical tasks and short
text answers—we achieve major improvement over the pre-
vious state of the art by about 4x and 1.5x respectively,
approaching human accuracy. We used our system in a real
classroom to augment human graders in a CS1 class, yielding
doubled grading accuracy while halving grading time.

1.1 Main contributions
In Sec. 4, we present an easy-to-use and highly expressive
class of generative models called Idea2Text simulators that
allow an instructor to encode their mental models of stu-
dent decision-making. These simulators can succinctly ex-
press student decisions and how these decisions manifest in a
ﬁnal solution for a broad set of problem domains like graph-
ics programming, short-answer questions, and introductory
programming in Java. We provide a Python implementation
that allows any instructor to easily write these simulators.

In Sec. 5, we show how to use Idea2Text simulators with
deep neural networks to infer students’ decision processes
from their solutions. This extracted decision process is a
general representation of a student’s solution and can be
used for several downstream tasks such as providing auto-
mated feedback, assisting human grading, auditing and in-
terpreting the model decisions, and improving the quality of
the simulator itself.

In order to do inference successfully on our expressive class
of simulators, we must overcome several interesting technical
challenges (Sec. 5). Learning to map solutions to sequences
of decisions speciﬁed by the simulator is a nonstandard ma-
chine learning task, with non-ﬁxed labels, varied sequence
lengths, and unexpected trajectories. Moreover, generating
simulated training data from the simulators requires an in-
telligent sampling method to work eﬀectively.

In Sec. 6 we show the eﬃcacy of our approach in practice on

a diverse set of richly structured problems. We attain close
to human level accuracy on providing feedback and surpass
many previous state of the art results. We also discuss sev-
eral interesting extensions in Sec. 7 that use our system to
go beyond just providing automated feedback.

The generative grading system is powerful because it ad-
dresses many of the issues of traditional supervised learning
mentioned above. We ﬁnd that the cost of writing simula-
tors for a new assignment is orders of magnitude cheaper
for instructors than manually annotating individual student
work. The simulators allow us to sample inﬁnite data, and
our adaptive sampling strategy lets us explore diverse stu-
dent solutions in our training data. It is “zero-shot”, requir-
ing no historical data nor annotation, and thus works for
the very ﬁrst student. Moreover, our novel inference system
allows for interpretable and explainable decisions.

2. RELATED WORK
“Rubric sampling” [32] ﬁrst introduced the concept of en-
coding expert priors in grammars of student decisions, and
was the inspiration for our work. The authors design Prob-
abilistic Context Free Grammars (PCFGs) to curate syn-
thetically labelled datasets to train supervised classiﬁers for
feedback. Our approach builds on this, but presents a more
expressive family of generative models of student decision-
making that are context sensitive and comes with new in-
novations that enable eﬀective inference. From our results
on Code.org, we see that this expressivity is responsible for
pushing our model past human level performance. Further-
more, this prior work only used to PCFGs to create simu-
lated datasets of feedback labels for supervised learning. In
contrast, we learn to infer the entire decision trajectory of a
student solution, allowing us to do things like dense feedback
and human-in-the-loop grading.

We draw theoretical inspiration for our generative grading
system from Brown’s “Repair Theory” which argues that the
best way to help students is to understand the generative
origins of their mistakes [4]. Building systems of student
cognition has been used in K-12 arithmetic problems [16]
and subtraction mistakes [8].

Automated feedback for open-ended richly structured prob-
lems has been studied through a few lenses. In many ap-
proaches, traditional supervised learning is employed to map
solutions to feedback [21, 30, 1, 33]. These methods require
large hand-labelled datasets of diverse student solutions,
which is diﬃcult due to heavy-tailed distributions. Feed-
back speciﬁc to computer programming problems has been
explored based on executing student solutions and compar-
ing to a reference solution [13, 19]. An interesting parallel
to our work is found in [13], where the instructor is asked
to specify the kinds of mistakes students can make. These
approaches are limited to code and don’t provide feedback
on the problem-solving process of a student.

Extracting expert-written generative models for inference
has seen enormous use in ﬁelds where domain expertise is
critical. Some key example include medical diagnosis, engi-
neering, ecology, and ﬁnance, where a generative model like
a Bayesian network is elicited from experts. [20, 22]. In ed-
ucation, instructors have domain expertise about students,

and Idea2Text serves as an easy-to-use generative model for
instructors to encode this expertise.

Inference over decision trajectories of Idea2Text simulators
is similar to “compiled inference” for execution traces in
probabilistic programs. As such, our inference engine shares
similarities to this literature [25, 17]. With Idea2Text simu-
lators, we get a nice interpretation of compiled inference as
a parsing algorithm.

3. BACKGROUND
In this section, we introduce the feedback challenge and
what makes it a diﬃcult machine learning problem.

3.1 Feedback as a Prediction Problem
The feedback prediction task is to automatically provide feed-
back to a given student solution. While this is easy to do for
simple multiple-choice problems, we focus on the challeng-
ing task of providing feedback on richly-structured problems
like computer programming or short-answer responses.

Both the type of student solutions and the type of feedback
required for the task can take many forms. A student solu-
tion can be a piece of text, which could represent problems
It could
like an essay, a maths proof, or a code snippet.
also be graphical output in the form of an image. Similarly,
feedback can take the form of something simple like classify-
ing solutions to a ﬁxed set of misconceptions, or something
complex such as highlighting and annotating speciﬁc parts
of a student solution

3.2 Difﬁculty of Automated Feedback
Feedback prediction on richly structured problems has been
an extremely diﬃcult challenge in education research. Even
limited to simple problems in computer science like beginner
block-based programming, automated solutions to providing
feedback have been restricted by scarce data and lack of
robustness. We discuss a few of the properties of student
work that make predicting feedback such a diﬃcult challenge
in education.

(1) Heavy-tailed Distributions: Student work in the form
of natural language, mathematical symbols, or code follow
heavy-tailed Zipf distributions. This means that a few so-
lutions are extremely common whereas almost all other ex-
amples are unique and show up rarely. Fig. 2 plots the
log-frequency of unique examples against the log of the rank
across four datasets of student work in block-based program-
ming code, Java code, and free response. For all datasets,
we observe a linear relationship in log-log space, which is a
characteristic property of Zipf distributions.

These heavy-tailed Zipf distributions pose a hard generali-
sation problem for traditional supervised machine learning:
a handful of similar examples appear very frequently in the
training data whereas almost all other examples are unique.
This means at test time, examples are likely to introduce un-
seen tokens, new misconceptions, and novel student strate-
gies.
In a Zipf distribution, even if we observe a million
student solutions, there is roughly a 15% chance that the
next student generates a unique solution.

(a) Code.org

(b) Liftoﬀ

(c) Pyramid

(d) Power

Figure 2: Student solutions (across many domains) exhibit
heavy-tailed Zipf distributions, meaning a few solutions are ex-
tremely common but all other solutions are highly varied and
show up rarely. This suggests that the probability of a student
submission not being present in a dataset is high, making super-
vised learning on a small data set diﬃcult.

(2) Diﬃculty of Annotation: Annotating student work with
feedback requires an instructor level expertise of the domain.
Providing ﬁne-grained feedback also takes eﬀort as the an-
notator must read and understand student solutions before
inferring possible misconceptions. In [32], the authors found
that 800 student block-based programmings solutions took
26 hours to label.

This diﬃculty, combined with Zipf properties, makes su-
pervised learning intractable. Even in the extreme cases,
annotation does not scratch the surface of the Zipf distri-
bution. As an example, in 2014, Code.org, a widely used
resource for beginners in computer science, ran an initia-
tive to crowdsource thousands of instructors to label 55,000
student solutions in a block-based programming language.
Yet, despite having access to an unprecedented amount of
labelled data, traditional supervised methods failed to per-
form well on even these “simple” questions.

(3) Limitations on Data Size: Even if the Code.org approach
succeeded, most classrooms do not share the same scale as
Code.org. A method that relies heavily on historical data
is not widely applicable in the average classroom setting.
In our experiments, our data sets contain less than a few
hundred examples, again barring application of supervised
algorithms. The ideal feedback model will be zero-shot so
that it works even for the very ﬁrst student.

4. MODELLING STUDENT COGNITION
Having presented the feedback challenge and motivated why
supervised learning cannot solve this problem alone, we dis-
cuss the idea of modelling the cognitive process of student
decision-making when producing a solution.

When an instructor provides feedback on student work, they
often have a latent mental model of student decision making;
this captures the kinds of steps and mistakes they think
students will make and what solutions are indicative of those
steps. The instructor (or TAs) then grade solutions, one at
a time, by essentially inferring the steps in the decision-
making model that lead to the produced solution.

As a concrete example from introductory programming, sup-
pose a student is trying to print a countdown of numbers
from 10 to 1. An instructor understands that as a ﬁrst step,
a student might use a for loop or manually write ten print
statements. Given that the student uses a loop, they could
increment up or down. And given that they increment down,

0510log Rank0246810log Count0246log Rank0123log Count0.02.55.07.5log Rank0.02.55.07.5log Count0246log Rank024log Counttheir loop comparison could be correct or oﬀ-by-one. At each
of these decision points, the instructor can conceive how a
speciﬁc choice would manifest in the solution.

We want to allow instructors to capture their mental model
of student decision-making for a given problem and distil
it into a concrete executable simulator that generates solu-
tions. Such a generative model could be used to simulate
unlimited student data, including their decision process and
resulting solutions. This simulated dataset can then be used
for learning how to provide feedback.

In this section, we formalise the idea of a student’s decision
process for generating the solution to a problem and discuss
how we can represent the instructor’s latent model of this
generative process as a concrete simulator.

4.1 Student Decision Process (SDP)
A student’s decision process (SDP) can be seen as the se-
quence of choices the student makes while solving a prob-
lem, and how those choices correspond to diﬀerent outputs.
The sequence is intended to reﬂect all of the critical deci-
sions made by the student, and in particular those a teacher
would like to recover from the solution. Importantly not all
students will encounter the same sequence of decisions since
an early decision choice may determine which decisions are
faced later in problem solving.

We can formally think of a decision point as a categorical
random variable, X, and the speciﬁc choices that can be
made as the diﬀerent values, x ∈ V al(X), that the random
variable can take. The decision process of a student can be
seen as a sequence trajectory (Xt, xt)T
t=1, of decisions en-
countered and made by the student in solving the problem.

Under this interpretation, specifying a model for an SDP
amounts to deﬁning the space of possible decision trajecto-
ries and a probability distribution over this space. By the
chain rule for probabilities, we can decompose this over-
all distribution as a sequence of conditional probabilities
P(Xt = xt | X<t = x<t). Here xt is the choice made at step
t from domain Xt and X<t is shorthand for the sequence of
decisions before time t.

We want to make this general formulation more tractable
to allow us to specify useful SDPs as generative models we
can sample from. Prior work [32] has attempted to express
SDPs by restricting the class of generative models to prob-
abilistic context free grammars (PCFGs). They found that
instructor-written PCFGs could often be used to emulate
student problem-solving and generate student solutions for
small problems. In this setting, the non-terminal nodes of
the PCFG represent decisions to be made (e.g. syntax con-
fusion) and the production rules represent how decisions are
made and manifested into output (e.g. the code is missing
a semicolon). Instructors create student simulators by spec-
ifying decision points, rules, and probabilities for each rule
(e.g. missing a semicolon is much more likely than missing
a function statement).

τ ← [ ]
y ← Generate(S, τ )
return (τ, y)

Algorithm 1 Idea2Text Simulation
Input: Idea2Text simulator (D, Σ, S)
Output: Tuple (τ, y) of decision trajectory and output solution.
1: procedure Simulate(D, Σ, S)
2:
3:
4:
5: procedure Generate(N , τ )
6:
7:
8:
9:
10:
11:

a, Xa, Πa ← N
xa, y ← Πa(Xa, τ )
τ.append((a, xa))
for decision node N (cid:48) in y do
y(cid:48) ← Generate(N (cid:48), τ )
y ← Replace(y, N (cid:48), y(cid:48))

(cid:46) Unpack current decision node
(cid:46) Get decision choice and output

(cid:46) Replace N (cid:48) with y(cid:48)

(cid:46) Begin from start node

(cid:46) In order left to right

return y

i.e. P(Xt = xt | X<t = x<t) = P(Xt = xt). This context-
independence is a strong restriction that severely limits what
instructors can express about the student decision-making
process and fails to faithfully model student reasoning. As
can be seen in even the simple countdown example above,
the oﬀ-by-one error would manifest diﬀerently in student
output depending on whether the student chose to increment
up or down. Thus context dependence of decision making is
an important property to model.

Idea2Text

4.2
In this section, we deﬁne a broader class of generative mod-
els that is powerful enough to capture more complexities of
expert models of student cognition. Similar to PCFGs we
structure our models around a set of non-terminal symbols
that correspond to student decisions and contribute to the
ﬁnal output. However, drawing from work on probabilistic
programs [10, 11], we allow these choices to be made depend-
ing on previous choices. While dependence on context leads
to extremely expressive models, we will show that requiring
some text to be generated at each step is enough for infer-
ence to remain tractable. We call this class of generative
models Idea2Text.1

Concretely, an Idea2Text simulator consists of a tuple of
(D, Σ, S) denoting a set of nonterminal decision nodes, a set
of terminal nodes, and a starting root node, respectively.
Intuitively, decision nodes correspond to decisions a student
might make and the terminal nodes correspond to literal text
tokens in the ﬁnal output. Each run of the simulator also
keeps a global state τ which stores the history of all decisions
made during the execution (often called an execution trace
for probabilistic programs [2, 29]).

Each decision node in D is a tuple (a, Xa, Πa) consisting of
a unique name, a random variable representing the decision
choices, and a production program, which (1) speciﬁes how
this decision should be made based on the decisions made so
far, and (2) produces an output solution for a given decision
choice.

More concretely, the production program is a probabilistic
function that takes the current decision history τ and does
the following:

A PCFG is compact and useful, but makes the indepen-
dence assumption that that the choice made at time t is
independent of past choices made while solving the problem

1A very similar class of models, used in the very diﬀer-
ent domain of customer service, was independently named
Idea2Text by scientists at Gamalon, Inc.

Figure 3: An example decision expansion step in an Idea2Text
simulation. The “loop style” decision node chooses a type of for
loop (e.g. increment vs decrement) and outputs a string contain-
ing the header of the for loop (terminals) plus another decision
node.

(1) Samples the random variable xa ∼ Xa, from a dis-
tribution, P(Xa|τ ), that can depend on the decision
history.

(2) Based on the sampled choice, xa, produces an output
string, y, consisting of literal text (terminal nodes) and
incomplete segments (decision nodes). These incom-
plete segments correspond to future decisions that will
be expanded later (see Fig. 3).

(3) Returns the sampled choice and output string: (xa, y).

An output from Idea2Text is a generation from the root node
to a sequence of literal text by recursively expanding the de-
cision nodes, as shown in Algorithm 1. Each output is asso-
ciated with the ﬁnal decision trajectory, τ = [(at, xat )]T
t=1,
of random variables encountered during generation. Here, at
denotes the unique name for the random variable encoun-
tered at timestep t, and xat is the sampled value for that
variable.

We point out some important properties of Idea2Text simu-
lators. First, each decision node’s choice can depend on the
decision history τ , allowing past decisions to inﬂuence cur-
rent behaviour. This is strictly more expressive than PCFGs
[32] and allows instructors to write highly contextual models
of student problem-solving. Second, production programs
have the full power of programming languages at their dis-
posal and can use arbitrarily complicated transformations to
produce their output sequence. As an example, a production
program can transform terminal nodes into images or use a
machine-learning conjugator to conjugate it’s produced text
into a proper sentence.

INFERENCE

5.
In this section we describe how we can use an instructor-
written Idea2Text simulator to learn how to infer the deci-
sions underlying a student solution.

At a high-level, the Idea2Text simulator contains the in-
structor’s mental model for the sequence of decisions stu-
dents make that result in diﬀerent solutions. For inference
we want to do the reverse: given a student solution, we want
to ﬁnd a trajectory of decisions in the simulator that would
produce that solution.

A model that could successfully do this inference could be
used to map real student solutions to decision steps in the
simulator. These extracted decisions are a rich and general
representation of a student’s solution and can be used for
downstream tasks such as automated feedback, assisting hu-
man grading, auditing and interpreting the model decisions,
and improving the quality of the simulator.

More formally, let G be a given Idea2Text simulator. Each
execution of G produces a decision trajectory τ and corre-
sponding production y. Since the execution is probabilistic,
the simulator induces a probability distribution pG(τ, y) over
trajectories and productions.

Given a student solution, y, we are interested in the task of
parsing: this is the task of mapping y to the most likely tra-
jectory in the Idea2Text simulator, arg maxτ pG(τ |y), that
could have produced y. This is a diﬃcult search problem:
the number of trajectories grows exponentially even for sim-
ple grammars, and common methods for parsing by dynamic
programming (Viterbi, CYK) are not applicable in the pres-
ence of context-sensitivity and functional transformations.
What’s more, in order to transfer robustly to real student
solutions, we would like to be able to approximately parse
solutions which it is not possible to generate from the sim-
ulator but are suﬃciently “nearby”.

At a high level, our approach is to construct a large data
set from the simulator and then learn an inverse “inference”
neural net that can reconstruct the decision trajectory from
the solution.

5.1 Adaptive Grammar Sampling
To train our models, we generate a large dataset of N trajec-
tories and their associated productions, D = {(τ (m), y(m))}N
by repeatedly executing G.

m=1,

However, due to the Zipf-like nature of student work (see
Sec. 3.2), standard i.i.d. sampling from the simulator will
tend to over-represent the most probable productions. For
our models, the more diverse student cognition we can sim-
ulate in the training data, the more we expect to generalise
to the long tail of real students. Thus, we need sampling
strategies that prioritise diversity.

A simple but ﬂawed idea for generating diverse solutions
would be to make choices at decision nodes uniformly ran-
domly instead of using the expert-written distributions. This
approach will generate more unique productions, but disre-
garding the expert-written distributions will result in un-
likely and less realistic productions.

Ideally, we want to sample in a manner that covers all the
most likely productions ﬁrst, and then smoothly transition
into sampling increasingly unlikely productions. This would
generate unique productions eﬃciently while also retaining
the expert-written distributions speciﬁed in the production
programs. With these desiderata in mind, we propose a
method called Adaptive Grammar Sampling. For each deci-
sion node in the simulator, we down-weight the probability
of sampling each choice proportional to how many times it
has been sampled in the past. To avoid overly punishing
decision nodes early in the execution trace, we discount this
down-weighting by a decay factor d that depends on the
depth of the decision in the trajectory.2 This method is in-
spired by Monte-Carlo Tree Search [5] and shares similarities
with Wang-Landau sampling from statistical physics [27].

Fig. 4 shows a comparison of the eﬀectiveness of adaptive

2The details can be found in the code.

loop styleforloop bodycomparefor (int i = 10; loop body; i--):encode the solution and the history of choices into consistent
vectors.

Firstly, to encode the solution y, we use standard machinery
(e.g. CNNs for images, RNNs for text) with a ﬁxed output
dimension. To represent the nonterminal choices with dif-
ferent support, we deﬁne three layers for each random vari-
able xat : (1) a one-hot embedding layer that uses the unique
name at to lexically identify the random variable, (2) a value
embedding layer that maps the value of xat to a ﬁxed dimen-
sion vector and (3) a value decoding layer that transforms
the hidden output state of M into parameters of the pos-
terior for the next nonterminal xat+1. Thus, the input to
the M is a ﬁxed size, being the concatenation of the value
embedding, name embedding, and production encoding.3

To train the GG-NAP, we optimise the objective,

L(θ) = EpG (τ,y)[log pθ(τ |y)] ≈

1
M

N
(cid:88)

m=1

log pθ(τ (m)|y(m))

(2)

where θ are all trainable parameters and pθ(τ |y) represents
the posterior distribution deﬁned by the inference engine. At
test time, given only a production y, GG-NAP recursively
samples xat ∼ pθ(xat |y, x<at ) for t = 1, . . . , T and uses each
sample as the input to the next step in M, as usual for
sequence generation models [12].

5.3 kNN Baseline
As a strong baseline for the parsing task, we consider a near-
est neighbour classiﬁer. We store our large dataset of sam-
ples D = {(τ (m), y(m))}N
m=1. At test time, given an input
solution to parse, we can ﬁnd its nearest neighbour in the
samples with a linear search of D, and return its associated
trajectory. Depending on the problem, the solutions y will
be in a diﬀerent output space (image, text) and thus the dis-
tance metric used for the nearest-neighbour search will be
domain dependent. We refer to this baseline as GG-kNN.
Note that GG-kNN is quite costly in memory and runtime
as it needs to store and iterate through all samples in the
dataset.

6. EXPERIMENTS
We test generative grading on a suite of education data
sets focusing on introductory courses from online platforms
and large universities. For each dataset, we compare our
approach to supervised learning, PCFGs, k-nearest neigh-
bours, and human performance. In Sec. 6.1, we introduce
the data sets, then present results in Sec. 6.3.

6.1 Datasets
We consider four educational contexts. Fig. 5 shows example
student solutions for each problem.

Block-based Programming Code.org released a data set of
student responses to eight Blocky exercises from one of their
curriculums online, which focuses on drawing shapes with
nested loops. We take the last problem in the curriculum
(the most diﬃcult one): drawing polygons with an increas-
ing number of sides—which has 302 human graded responses

3Speciﬁc details can be found in the code.

(a) Uniqueness

(b) Good-Turing

(c) Likelihood of samples over time

Figure 4: Eﬃciency of sampling strategies for the Liftoﬀ simu-
lator. (a) Number of unique samples vs total samples so far. (b)
Probability of sampling a unique next program given samples so
far. (c) Likelihood of generated samples over time for diﬀerent
sampling strategies.

sampling to uniform and i.i.d. sampling. Adaptive sampling
interpolates nicely between sampling likely examples early
on, as i.i.d. sampling does, to sampling unlikely examples
later, as uniform-choice sampling does. Note that adaptive
sampling is customisable: as shown in 4c, the algorithm has
parameters (r and d) that can be adjusted to control how
fast we explore increasingly unlikely productions.

5.2 Neural Approximate Parsing
With good diverse samples available, we now aim to learn
an approximation to the posterior pG(τ |y). We will do so
by training a deep neural network to reconstruct the trajec-
tory step-by-step. We call this approach neural approximate
parsing with generative grading, or GG-NAP.

The challenge of inference over trajectories is a diﬃcult one.
Trajectories can vary in length and contain decision nodes
with diﬀerent support. To approach this, we decompose
the inference task into a set of easier sub-tasks, similar to
[25, 17]. The posterior distribution over a trajectory τ =
(at, xat )T
t=1 given a production y can be written as the prod-
uct of individual posteriors over each decision node xat using
the chain rule:

pG (xa1 , . . . xaT |y) =

T
(cid:89)

t=1

pG (xat |y, x<at )

(1)

where x<at denotes previous (possibly non-contiguous) non-
terminals (xa1 , . . . , xat−1 ). Eqn. 1 shows that we can learn
each posterior p(xat |x<at , y) separately. With an autore-
gressive model M, we can eﬃciently represent the inﬂuence
of previous nonterminals x<at using a shared hidden repre-
sentation over T timesteps. Since most standard choices for
M (e.g. an RNN) require ﬁxed-dimension inputs, we need to

0500010000# of samples0200040006000# of unique programs0250050007500100000.20.40.60.81.0StandardAdaptiveUniform# of SamplesPr. ofUnseen Program0200040006000800010000# of samples201510log prob. of samplestandarduniformadaptive (r=0.01, d=0.3)adaptive (r=0.01, d=0.6)adaptive (r=0.01, d=0.9)adaptive (r=0.1, d=0.3)adaptive (r=0.1, d=0.6)adaptive (r=0.1, d=0.9)adaptive (r=1.0, d=0.3)adaptive (r=1.0, d=0.6)adaptive (r=1.0, d=0.9)student thinking. Procedurally, the ﬁrst random variable is
choosing whether the production will be correct or incor-
rect. It then chooses a subject, verb, and noun dependent
on the correctness. Correct answers lead to topics like re-
ligion, politics, and economics while incorrect answers are
about taxation, exploration, or physical goods. Finally, we
add a random variable to decide a writing style to craft a
sentence. To capture variations in tense, we use a conjuga-
tor [7] for the ﬁnal production. This simulator contains 53
decision nodes.

Graphics Programming The primary decision in this simula-
tor decides between 13 “strategies” (e.g. making a parallel-
ogram, right triangle, a brick wall, etc.) that the instructor
believed students would use. Each of the 13 options leads to
its own set of nodes that are responsible for deciding shape,
location, and colour. The production uses Java to render an
image output. This simulator contains 121 decision nodes,
and required looking at 200 unlabelled student solutions in
its design.

University Programming Assignment To model student think-
ing on Liftoﬀ, this simulator ﬁrst determines whether to use
a loop, and, if so, chooses between “for” and “while” loop
structures. It then formulates the loop syntax, choosing a
condition statement and whether to count up or count down.
Finally, it chooses the syntax of the print statements. No-
tably, each choice is dependent on previous ones. For exam-
ple, choosing an end value in a for loop is sensibly condi-
tioned on a chosen start value. This simulator contains 26
decision nodes.

6.3 Results for Feedback Prediction
We show the results of generative grading for each of the
datasets above.

For each dataset, we have access to a set of real student so-
lutions and corresponding human-provided feedback labels,
which we use for evaluation.

We ask instructors to create an Idea2Text simulator for each
dataset, and train the deep inference network GG-NAP us-
ing simulated student solutions. At test time, we pass a real
student solution into the inference model, and get back a
trajectory of the simulator. This trajectory contains deci-
sion node choices that correspond to the human-provided
feedback labels, and we use these as the model’s feedback
prediction.

Our performance metric for evaluating the model’s predicted
feedback labels is accuracy or F1 score, depending on the
convention of prior work. Computing an average of the
metric across the evaluation dataset would over-prioritise
examples that appear frequently; this is particularly impor-
tant to avoid for the Zipf distributed solutions. Since we
care about providing feedback to struggling students in the
tail of the distribution, we separately calculate performance
for diﬀerent “regions” of the Zipf. Speciﬁcally, we deﬁne the
head as the k most popular solutions, the tail as solutions
that appear only once or twice, and the body as the rest. As
solutions in the head can be trivially memorised, we focus
on performance on the body and tail.

Figure 5: We show the prompt and example solutions for our
four datasets.

with 26 misconceptions regarding looping and geometry (e.g.
“missing for loop” or “incorrect angle”) from [32].

Free Response Language Powergrading [1] contains 700 re-
sponses to a United States citizenship exam, each graded for
correctness by 3 humans. Responses are in natural language,
but are typically short (average of 4.2 words). We focus on
the most diﬃcult question, as measured by [24]: “name one
reason the original colonists came to America”. Correct re-
sponses span economic, political, and religious reasons.

Graphics Programming PyramidSnapshot is a university CS1
course assignment intended to be a student’s ﬁrst exposure
to variables, objects, and loops. The task is to build a pyra-
mid using Java’s ACM graphics library by placing individ-
ual blocks. The dataset is composed of images of rendered
pyramids from intermediary “snapshots” of student work.
[33] annotated 12k unique snapshots with 5 categories rep-
resenting “knowledge stages” of understanding.

University Programming Assignment Liftoﬀ is a second as-
signment from a university CS1 course that tests looping.
Students are tasked to write a program that prints a count-
down from 10 to 1 followed by the phrase “Liftoﬀ”. In Sec. 7,
we will use Liftoﬀ for a human-in-the-loop study where ex-
perts generatively grade 176 solutions from a semester of
students and measure accuracy and grading time.

6.2 Simulator Descriptions
We provide a brief overview of the Idea2Text simulators con-
structed for each domain.

Block-based Programming The primary innovation is to use
the ﬁrst decision node random variable to represents student
ability. This ability variable will aﬀect the distributions for
random variables later in the trajectory such as deciding the
loop structure and body. The intuition this captures is that
high ability students make very few to no mistakes whereas
low ability students tend to make many correlated misun-
derstandings. This simulator contains 52 decision nodes.

Free Response Language Idea2Text simulators over natural
language need to explain variance in both semantic mean-
ing and prose. We inspected the ﬁrst 100 responses to gauge

forfromdotoby3iTurnRightdoMoveforwardi10fori10xi360/fromto0j2forfromdotoby3iTurnRightdoMoveforwardi9Repeat fori10xi360/2TurnLeftMovebackward30Movebackward30Movebackward30TurnLeftTurnLeftDraw me!Code.org Problem 8Write a Java Program to print the numbers 10 down to 1 and then write liftoff. You must use a loop.		public	void	run()	{				for(int	i	=	START;	i>0;	i--)					{						println(i);						pause(1000);				}				println("Liftoff!");		}		public	void	run()	{				for	(int	i=START;	i>0;	i	-=1)					{						println(i);				}				println("Liftoff");		}		public	void	run()	{				int	x	=	START;				int	y	=	1;				int	z	=	9;				while	(x>=1)	{						println(x);						x=z;						z=x-y;				}				println("Liftoff");		}CS1: LiftoffWhat is one reason the original colonists came to America? • Religuous freedom• For religious freedom• Freedom•declared our independence from england•religeous freedom•as a criminal punishment•to create a new colony•to find better economic prospects•to break away from the church in great britainPowergrading P13Use the graphics library to construct a symmetric and centered pyramid with a base width of 14 bricks.PyramidSnapshotFigure 6: Summary of results for providing feedback to student work in three educational contexts: block-based programming, graphics
programming, and free response language. Generative grading shows strong performance in all three settings, closely approximating
human-level performance in two data sets, and surpassing human-level performance in the other.

Training Details We report averages over three runs; error
bars are shown in Fig. 6. We use a batch size of 64, train for
20 epochs on 100k unique samples adaptively sampled from
the simulator. We optimise using Adam [15] with a learning
rate of 5e-4 and weight decay of 1e-7. For PyramidSnap-
shot, we use VGG-11 [26] with Xavier initialisation [9] as
the encoder network. For other data sets, we use a Recur-
rent Neural Network (RNN) with 4 layers, a hidden size of
256. The deep inference network itself is an unrolled RNN:
we use a gated recurrent unit with a hidden dimension of
256 and no dropout. The value and index embedding layers
output a vector of dimension 32. These hyperparameters
were chosen using grid search.

Code.org As feedback for Code.org exercises has been stud-
ied in prior work [32], we compare generative grading to
a suite of baselines including supervised models trained to
classify misconceptions from the hand-labelled dataset (Out-
put CNN [28] + Program RNN [32]), unsupervised models
that learn a latent vector representation of student work
(MVAE), to the k-nearest neighbours baseline GG-kNN from
Sec. 5. Most relevant to our approach is the “rubric sam-
pling” [32] comparison, which uses a PCFG to simulate stu-
dents and generate a supervised data set to train a RNN
classiﬁer. Human accuracy is measured by comparing the
feedback provided by multiple annotators to the mode.

As shown in Fig. 6, generative grading is able to provide
accurate feedback (historically measured as F1) beyond the
level of individual human annotators, setting the new state-
of-the-art. We observe a large improvement over prior work,
which perform signiﬁcantly worse than human graders. Com-
pared to rubric sampling, we ﬁnd a 18% (absolute) improve-
ment in the body and a 30% (absolute) improvement in the
tail. This clearly demonstrates the practical importance of
being context-sensitive. The global state of Idea2Text simu-
lators allow us to easily write richer generative models that
are capable of better simulating real students. The poten-
tial impact of a human-level autonomous grader is large:
Code.org is used by 610 million students, and our approach

could save thousands of human hours for teachers by pro-
viding the same quality of feedback at scale.

Powergrading We ﬁnd similarly strong performance on the
Powergrading corpus of short answer responses to a citizen-
ship question. Fig. 6 shows that generative grading reaches a
F1 score of 0.93, an increase of 0.35 points above prior work
that used hand-crafted features to predict correctness [6],
and 0.38 points above supervised neural networks [24]. We
were unable to compare to rubric sampling [31] as it was too
diﬃcult to write a faithful PCFG to describe free response
language. Generative grading takes a large step towards
closing the gap to human performance, measured to be F1 =
0.97 (within 0.04). We are especially optimistic about these
results as Powergrading responses contain natural language,
this is promising signal that ideas from generative grading
could generalise beyond computer science education.

PyramidSnapshot Investigating a third modality of image
output from a graphics assignment, we ﬁnd similar results
comparing generative grading to the k-nearest neighbour
baseline and a VGG image classiﬁer presented in [33], out-
performing the latter by nearly 50% absolute.

Unlike other datasets, the PyramidSnapshot dataset includes
student’s intermediary work, showing stages of progression
through multiple attempts at solving the problem. With our
near-human level performance, instructors could use GG-
NAP to measure student cognitive understanding over time
as students work. This builds in a real-time feedback loop
between the student and teacher that enables a quick and
accurate way of assessing teaching quality and characteris-
ing both individual and classroom learning progress. From a
technical perspective, since PyramidSnapshot only includes
rendered images (and not student code), generative grad-
ing was responsible for parsing student solutions from just
images alone, a feat not possible without the ﬂexibility of
probabilistic programs used in Idea2Text. For this reason,
we could not apply rubric sampling in this context either.

0.00.20.40.60.80.00.20.40.60.80.00.20.40.60.8HumanGG-NAPOldSOTA0.680.690.690.780.510.480.810.800.790.670.200.210.58ModelBody F1Tail F1Output CNN [26]0.100.10Program RNN [23]0.270.22MVAE [24]0.380.26Rubric Sampling [26]0.510.48GG-kNN0.310.33GG-NAP0.690.78Human0.680.69ModelBody AccTail AcckNN[28]0.200.12NeuralNet[28]0.200.21GG-kNNtimeouttimeoutGG-NAP0.790.67Human0.810.80ModelAvgF1Tail F1Handcrafted [6]0.58-T&N Best [17]0.55-GG-kNN0.780.63GG-NAP0.930.76Human0.970.90(a) Code: Code.orgP8(c) Sentences:PowergradingP13(b) Graphical output: PyramidSnapshotF1F1AccuracyHuman levelHuman (tail)Human(body)Human (avg)GG-NAP (tail)GG-NAP (body)GG-NAP (avg)Old SOTA (tail)OldSOTA (body)OldSOTA(avg)0.970.93Figure 7: CDF of Levenshtein edit distance between student
programs and nearest-neighbours using various algorithms.

Figure 8: Human-in-the-loop Generative Grading UI

7. EXTENSIONS
Our results show that generative grading is a powerful tool
for the feedback prediction task. However, our system is
much more general than this and has many interesting ex-
tensions that we discuss in this section.

7.1 Nearest In-Simulator Neighbour
As described in Section 5, our inference model learns to map
a given solution, y, to a decision trajectory in the simula-
tor. So far, we have used this only to provide feedback with
a ﬁxed set of labels. However, the decision trajectory has
a much more powerful interpretation; it represents the se-
quence of decisions in the simulator that the inference model
thinks will produce y. Since we have the simulator in hand,
we can actually execute it with these predicted sequence of
decisions and inspect the simulated output, (cid:98)y.

If the simulated output is exactly equal to the original input
solution, i.e. if y = (cid:98)y, then we can make a strong claim: the
predicted trajectory from the inference model was provably
correct and the corresponding labels can be assigned with
100% conﬁdence. This is a claim that is seldom possible
with traditional supervised learning methods and advances
eﬀorts towards creating explainable AI.

What about when y (cid:54)= (cid:98)y? In this case, the simulated output
is not an exact match to the student solution, but we can still
treat it as a “nearest in-simulator neighbour ” to y. Fig. 7
shows the quality of these nearest neighbours to the student
solutions using a distance metric like edit distance.

As we show below, these nearest neighbours can be used for
powerful forms of feedback mechanisms.

7.2 Human-in-the-loop Grading
In a real-world setting, predicting feedback labels could be
unreliable due to the high risk of giving students incorrect
feedback. Beyond automated feedback, we explore how gen-
erative grading can be used to make human graders more
eﬀective using a human-in-the-loop approach.

To do this, we created a human-in-the-loop grading system
using GG-NAP. For each student solution, we use the in-
ference model to ﬁnd the nearest in-simulator neighbour
(Sec. 7.1); this nearest neighbour already has associated la-
bels that are correct for the nearest neighbour. A human
grader is presented with the original student solution, as

well as a diﬀ to the nearest neighbour; the grader then ad-
justs the labels of the nearest neighbour based on the diﬀ to
determine grades for the real solution. We show an image
of the user-interface of this system in Fig. 8.

We investigated the impact of this human-in-the-loop sys-
tem on grading accuracy and speed in a real classroom set-
ting. We hired a cohort of expert graders (teaching assis-
tants with similar experience from a large private university)
who graded 30 real student solutions to Liftoﬀ. For control,
half the graders proceeded traditionally, assigning a set of
feedback labels by just inspecting the student solutions. The
other half of graders additionally had access to (1) the feed-
back assigned to the nearest neighbour by GG-NAP and
(2) a code diﬀerential between the student program and the
nearest neighbour. Some example feedback labels included
“oﬀ by one increment”, “uses while loop”, or “confused >
with <”. All grading was done on a web application that
kept track of the time taken to grade a problem.

We found that the average time for graders using our sys-
tem was 507 seconds while the average time using tradi-
tional grading was 1130 seconds, a more than double in-
crease. Moreover, with our system, only 3 grading errors
(out of 30) were made with respect to gold-standard feed-
back given by the course Professor, compared to the 8 errors
made with traditional grading. Fig. 9a shows these results
for each of the 30 solutions.

The improved performance stems from the semantically mean-
ingful nearest neighbours provided by GG-NAP. Having ac-
cess to graded nearest neighbours helps increase grader ef-
ﬁciency and reliability by allowing them to focus on only
“grading the diﬀ” between the real solution and the near-
est neighbour. By halving both the number of errors and
the amount of time, GG-NAP can have a large impact in
classrooms today, saving instructors and teaching assistants
unnecessary hours and worry over grading assignments.

7.3 Highlighting feedback in student solutions.
The inferred decision trajectory for a student solution can
also be used to provide “dense” feedback that highlights the
section of the code or text responsible for each misunder-
standing. This would be much more eﬀective for student
learning than vague error messages currently found on most
online education platforms.

To achieve this, we leverage the fact that each decision node

05101520253035400.00.20.40.60.81.0Token edit distanceFrequencyRandomGG-kNNGG-NAP (adapt)GG-NAP (std)15%55%(a) Classroom Experiment Results

(b) Automated Dense Feedback

(c) Auto-improving Simulators

Figure 9: (a) Plot of average time taken to grade 30 student solutions to Liftoﬀ. Generative grading reduces grading time for 26 out of
30 solutions. The amount of time saved correlates with the token edit distance (yellow) to the nearest neighbour in the simulator. (b)
Our approach allows for automatically highlighting which part of the student solution is responsible for a predicted misconception. (c)
Given a Liftoﬀ simulator that is missing a key “decrement loop” decision, we can automatically ﬁnd decision nodes where inference often
fails on real student solutions. The highest scoring decision nodes are all correctly related looping.

in the simulator gets recursively expanded to produce the
ﬁnal solution. This means it is easy to track the portions of
the output that each decision node is responsible for. For
decision nodes related to student confusions, we can high-
light the portion of the output in the student solution which
corresponds to this confusion. Fig. 9b shows a random pro-
gram with automated, segment-speciﬁc feedback given by
GG-NAP. This level of explainability is sorely needed in both
education and AI.

7.4 Automatically Improving Simulators
Building Idea2Text simulators is an iterative process; a user
wishing to improve their simulator would want a sense of
where it is lacking. Fortunately, given a set of diﬃcult exam-
ples where GG-NAP does poorly, we can deduce the decision
nodes in the simulator that consistently lead to mistakes and
use these to suggest components to improve.

To do this, for each nearest neighbour to a student solu-
tion we can ﬁnd decision nodes that cause substring mis-
matches in the student solution, using regular expressions.
This is possible because each decision node is responsible for
a scoped substring in the nearest neighbour output solution
(Sec. 7.3). By ﬁnding the decision nodes where the sub-
string often diﬀers between the neighbour and the solution,
we can identify decisions that often causes mismatches.

To illustrate this, we took the Liftoﬀ simulator, which con-
tains a crucial decision node that decides between increment-
ing up or down in a “for” loop, and removed the option of
incrementing down. We trained GG-NAP on this smaller
simulator, and used a scoring mechanism to identify rele-
vant decision nodes responsible for failing to parse student
solutions that “increment down”. Fig. 9c shows the distribu-
tion over which nodes GG-NAP believes to be responsible
for the failed parses. The top 6 decisions that GG-NAP
picked out all rightfully relate to looping and increments.

8. LIMITATIONS AND FUTURE WORK
Cost of writing good simulators. One of the most critical
steps in our approach is the ability to write good Idea2Text
simulators. Writing a good simulator does not require spe-
cial expertise and can be undertaken by a novice in a short
time. For instance, the PyramidSnapshot simulator that
sets the new state of the art was written by a ﬁrst-year un-
dergraduate within a day. Furthermore, many aspects of

simulators are re-usable: similar problems will share non-
terminals and some invariances (e.g. the nonterminals that
capture diﬀerent ways of writing for loops are the same ev-
erywhere). This means every additional grammar is easier
to write since it likely shares a lot in structure with exist-
ing grammars. Moreover, compared to weeks spent hand-
labelling data, the cost of writing a grammar is orders of
magnitude cheaper and leads to much better performance.

That being said, we believe there is room for interesting
future work that explores how to make grammars easy to
write and improve, with the extension in Sec. 7.4 already
make some headway in this direction. There is also room
for better formalising which types of problem domains can
be faithfully modelled with Idea2Text simulators, and which
domains are infeasible, like general essay writing. Lastly,
more sophisticated inference approaches could be explored
for handling semantic invariances in student output such as
code reordering or variable renaming.

Connections to IRT. We ﬁnd an interesting parallel of our
work to Item Response Theory (IRT). IRT is essentially an
extremely simple generative model that relates a student
parameter θ to the probability of getting a question correct
or incorrect. Some of our Idea2Text simulators also incor-
porate a student ability parameter θ to dictate likelihoods
of making mistakes at diﬀerent decisions, and can thus be
seen as a more expressive and nuanced extension of the IRT
generative model. Exploring this further is an interesting
direction of research.

Generating questions with Idea2Text. We use Idea2Text sim-
ulators to model student decision-making and corresponding
example solutions. This could be used to automatically gen-
erate example solutions with known issues to show students
for pedagogical purposes. The Idea2Text library can also
been used to generating questions corresponding to confu-
sions instead of solutions corresponding to confusions.

9. CONCLUSION
We proposed a method for providing automated student
feedback that showed promising results across multiple modal-
ities and domains. Our proposed feedback system is capable
of predicting student decisions corresponding to a given so-
lution, allowing us to do nuanced forms of automated feed-
back. With it, “generative grading” can be used to automate

051015202530Student (sorted by grading time)020406080Grading Time (sec.)Traditional GradingGG­NAP020406080NN Token Edit DistanceToken Edit DistanceprivatestaticfinalintSTART = 10;publicvoidrun() {inti= START;while(i>= 0) {println(i);i= i-1;}}Off-by-one loopUses >= operatorCorrect variable typeUses constantwhile loopsolutionloop counting down05101520Non­Terminal Index0.00.1p(improve)Loop Non­Terminalotherfeedback, visualise student approaches for instructors, and
make grading easier, faster, and more consistent. Although
more work needs to be done on making powerful grammars
easier to write, we believe this is an exciting direction for the
future of education and a step towards combining machine
learning and human-centred artiﬁcial intelligence.

10. REFERENCES
[1] S. Basu, C. Jacobs, and L. Vanderwende.

Powergrading: a clustering approach to amplify
human eﬀort for short answer grading. Transactions of
the Association for Computational Linguistics,
1:391–402, 2013.

[2] E. Bingham, J. P. Chen, M. Jankowiak,

F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh,
P. Szerlip, P. Horsfall, and N. D. Goodman. Pyro:
Deep universal probabilistic programming. Journal of
Machine Learning Research, 20(28):1–6, 2019.

[3] W. G. Bowen. The ‘cost disease’in higher education: is
technology the answer? The Tanner Lectures Stanford
University, 2012.

[4] J. S. Brown and K. VanLehn. Repair theory: A
generative theory of bugs in procedural skills.
Cognitive science, 4(4):379–426, 1980.

[5] H. S. Chang, M. C. Fu, J. Hu, and S. I. Marcus. An
adaptive sampling algorithm for solving markov
decision processes. Operations Research,
53(1):126–139, 2005.

[6] J. Daxenberger, O. Ferschke, I. Gurevych, and

T. Zesch. Dkpro tc: A java-based framework for
supervised learning experiments on textual data. In
Proceedings of 52nd Annual Meeting of the
Association for Computational Linguistics: System
Demonstrations, pages 61–66, 2014.

[7] S. Diao. mlconjug: A python library to conjugate
verbs using machine learning techniques. GitHub:
https://github.com/SekouD/mlconjug, 2018.
[8] M. Q. Feldman, J. Y. Cho, M. Ong, S. Gulwani,

Z. Popovi´c, and E. Andersen. Automatic diagnosis of
students’ misconceptions in k-8 mathematics. In
Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems, pages 1–12, 2018.
[9] X. Glorot and Y. Bengio. Understanding the diﬃculty

of training deep feedforward neural networks. In
Proceedings of the thirteenth international conference
on artiﬁcial intelligence and statistics, pages 249–256,
2010.

[10] N. Goodman, V. Mansinghka, D. M. Roy,

K. Bonawitz, and J. B. Tenenbaum. Church: a
language for generative models. arXiv preprint
arXiv:1206.3255, 2012.

[11] N. D. Goodman and A. Stuhlm¨uller. The design and

implementation of probabilistic programming
languages, 2014.

[12] A. Graves. Generating sequences with recurrent neural

networks. CoRR, abs/1308.0850, 2013.

[13] S. Gulwani and R. Singh. Automated feedback

generation for introductory programming assignments.
In ACM SIGPLAN Conference on Programming
Language Design and Implementation (PLDI 2013),
pages 15–26, July 2013.

[14] Q. Hu and H. Rangwala. Reliable deep grade

prediction with uncertainty estimation. arXiv preprint
arXiv:1902.10213, 2019.

[15] D. P. Kingma and J. Ba. Adam: A method for

stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

[16] K. R. Koedinger, N. Matsuda, C. J. MacLellan, and
E. A. McLaughlin. Methods for evaluating simulated
learners: Examples from simstudent. In AIED
Workshops, 2015.

[17] T. A. Le, A. G. Baydin, and F. Wood. Inference

compilation and universal probabilistic programming.
arXiv preprint arXiv:1610.09900, 2016.

[18] J. Liu, Y. Xu, and L. Zhao. Automated essay scoring

based on two-stage learning. arXiv preprint
arXiv:1901.07744, 2019.

[19] X. Liu, S. Wang, P. Wang, and D. Wu. Automatic
grading of programming assignments: An approach
based on formal semantics. In Proceedings of the 41st
International Conference on Software Engineering:
Software Engineering Education and Training,
ICSE-SEET ’19, page 126–137. IEEE Press, 2019.
[20] T. G. MARTIN, M. A. BURGMAN, F. FIDLER,

P. M. KUHNERT, S. LOW-CHOY, M. MCBRIDE,
and K. MENGERSEN. Eliciting expert knowledge in
conservation science. Conservation Biology,
26(1):29–38, 2012.

[21] H. Nilforoshan and E. Wu. Leveraging quality

prediction models for automatic writing feedback. In
Proceedings of the International AAAI Conference on
Web and Social Media, volume 12, 2018.

[22] A. O’Hagan. Eliciting expert beliefs in substantial

practical applications. Journal of the Royal Statistical
Society: Series D (The Statistician), 47(1):21–35,
1998.

[23] C. Piech, J. Bassen, J. Huang, S. Ganguli, M. Sahami,

L. J. Guibas, and J. Sohl-Dickstein. Deep knowledge
tracing. In Advances in neural information processing
systems, pages 505–513, 2015.

[24] B. Riordan, A. Horbach, A. Cahill, T. Zesch, and

C. M. Lee. Investigating neural architectures for short
answer scoring. In Proceedings of the 12th Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 159–168, 2017.

[25] D. Ritchie, P. Horsfall, and N. D. Goodman. Deep

Amortized Inference for Probabilistic Programs.
Technical report, 2016.

[26] K. Simonyan and A. Zisserman. Very deep

convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
[27] F. Wang and D. Landau. Eﬃcient, multiple-range
random walk algorithm to calculate the density of
states. Physical review letters, 86:2050–3, 04 2001.
[28] L. Wang, A. Sy, L. Liu, and C. Piech. Learning to
represent student knowledge on programming
exercises using deep learning. In EDM, 2017.
[29] D. Wingate, A. Stuhlmueller, and N. Goodman.
Lightweight implementations of probabilistic
programming languages via transformational
compilation. In G. Gordon, D. Dunson, and M. Dud´ık,
editors, Proceedings of the Fourteenth International
Conference on Artiﬁcial Intelligence and Statistics,
volume 15 of Proceedings of Machine Learning

Research, pages 770–778, Fort Lauderdale, FL, USA,
11–13 Apr 2011. JMLR Workshop and Conference
Proceedings.

[30] B. Woods, D. Adamson, S. Miel, and E. Mayﬁeld.
Formative essay feedback using predictive scoring
models. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, KDD ’17, page 2071–2080, New York,
NY, USA, 2017. Association for Computing
Machinery.

[31] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth,

and F. Doshi-Velez. Beyond sparsity: Tree
regularization of deep models for interpretability. In
Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018.

[32] M. Wu, M. Mosse, N. Goodman, and C. Piech. Zero

shot learning for code education: Rubric sampling
with deep learning inference. arXiv preprint
arXiv:1809.01357, 2018.

[33] L. Yan, N. McKeown, and C. Piech. The

pyramidsnapshot challenge: Understanding student
process from visual output of programs. In Proceedings
of the 50th ACM Technical Symposium on Computer
Science Education, SIGCSE ’19, pages 119–125, New
York, NY, USA, 2019. ACM.

