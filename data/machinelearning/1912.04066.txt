Feasibility-Guided Learning for Robust Control in Constrained Optimal
Control Problems

Wei Xiao, Calin A. Belta and Christos G. Cassandras

9
1
0
2

c
e
D
6

]

Y
S
.
s
s
e
e
[

1
v
6
6
0
4
0
.
2
1
9
1
:
v
i
X
r
a

Abstract— Optimal control problems with constraints ensur-
ing safety and convergence to desired states can be mapped
onto a sequence of real time optimization problems through
the use of Control Barrier Functions (CBFs) and Control
Lyapunov Functions (CLFs). One of the main challenges in
these approaches is ensuring the feasibility of the resulting
quadratic programs (QPs) if the system is afﬁne in controls.
The recently proposed penalty method has the potential to
improve the existence of feasible solutions to such problems.
In this paper, we further improve the feasibility robustness
(i.e., feasibility maintenance in the presence of time-varying
and unknown unsafe sets) through the deﬁnition of a High
Order CBF (HOCBF) that works for arbitrary relative degree
constraints; this is achieved by a proposed feasibility-guided
learning approach. Speciﬁcally, we apply machine learning
techniques to classify the parameter space of a HOCBF into
feasible and infeasible sets, and get a differentiable classiﬁer
that is then added to the learning process. The proposed
feasibility-guided learning approach is compared with the
gradient-descent method on a robot control problem. The
simulation results show an improved ability of the feasibility-
guided learning approach over the gradient-decent method to
determine the optimal parameters in the deﬁnition of a HOCBF
for the feasibility robustness, as well as show the potential
of the CBF method for robot safe navigation in an unknown
environment.

I. INTRODUCTION

Stabilizing a dynamical system while optimizing a cost
function and satisfying safety constraints is a fundamental
and challenging problem in control. Typically, such problems
include autonomous driving in road trafﬁc and robot safe
exploration in unknown environments. When safety becomes
critical, it is desired to prioritize the strict satisfaction of
constraints instead of optimality. The barrier function method
[2], [12], [9], [21] has been proposed as an approach to this
problem.

Barrier functions (BFs) are Lyapunov-like functions [19],
whose use can be traced back to optimization problems
[4]. More recently, they have been employed in veriﬁcation
and control, e.g.,
invariance [3], [16], [20]
and for multi-objective control [14]. Control BFs (CBFs)
are extensions of BFs for control systems. Recently,
it
has been shown that CBFs can be combined with control
Lyapunov functions (CLFs) [17], [6], [1] as constraints to
form quadratic programs (QPs) [7] for nonlinear control

to prove set

This work was supported in part by the NSF under grants IIS-1723995,
CPS-1446151, ECCS-1931600, DMS-1664644, and CNS-1645681, by
ARPA-Es NEXTCAR program under grant de-ar0000796, by AFOSR
under grant FA9550-19-1-0158, and by the MathWorks. The authors are
with the Division of Systems Engineering and Center for Information
and Systems Engineering, Boston University, Brookline, MA, 02446, USA
{xiaowei, cbelta, cgc}@bu.edu

systems that are afﬁne in controls, and these QPs can be
solved in real time. While computationally efﬁcient, the CBF
and CLF-based QPs can easily become infeasible in the
presence of both stringent safety constraints and tight control
limitations, especially for high relative degree systems in a
highly dynamical environment.

The CLF constraints are usually relaxed [2] such that they
do not conﬂict with the CBF constraints in the QPs. Recent
work showed that rich speciﬁcations given in signal temporal
logic [9] and linear temporal logic [13], [18] can be translated
to constraints and implemented by the CBF method with
good solution feasibility if the constraints are with relative
degree one. Several approaches to improve feasibility for the
CBF and CLF-based QPs on speciﬁc applications have been
proposed. For the adaptive cruise control (ACC) problem
(the system is with relative degree 2) deﬁned in [2], the
infeasibility issue is addressed by including the minimum
braking distance in the safety constraint. An approximation
of the braking distance was used in [22] for a cooperative
optimization control problem with non-linear dynamics. In
both cases, an additional complex safety constraint needs
to be added. Further, this approach does not scale well for
high-dimensional systems. We recently developed the penalty
method [21], which can improve the feasibility of the QPs
by penalizing the class K functions in the deﬁnition of a
High Order CBF (HOCBF) for an arbitrary relative degree
constraint.

The use of machine learning techniques to improve fea-
sibility was recently proposed for legged robots. Feasibility
constraints for probabilistic models are learned in [5] based
on simpliﬁed models. Since the learned constraints are com-
plex, they are simpliﬁed by expectation-maximization (EM).
Robot footstep limits are modeled as hyper-planes based on
success and failure datasets in [15]. Reinforcement learning
(RL) [11] [10] has the potential to address the infeasibility
to
issue for optimal control problems, but
quantify feasibility as a reward and the optimized parameters
may also go to a local infeasible region where a feasible
solution could never be found.
In this paper, we adopt

the CBF method to improve
the feasibility and feasibility robustness of optimal control
problems with stringent safety constraints (usually with high
relative degree) and tight control limitations in an unknown
environment. The feasibility robustness is deﬁned by the QP
feasibility maintenance in the presence of a number of time-
varying and unknown unsafe sets. Based on our proposed
penalty method from [21], we parameterize a HOCBF, and
use the parameters to improve the feasibility of the CBF

is difﬁcult

it

 
 
 
 
 
 
and CLF-based QPs. Since trajectories of a system may be
required to avoid a number of unsafe sets at the same time,
we propose the idea of minimizing the value of a HOCBF
(usually the distance to an unsafe set) when the correspond-
ing HOCBF constraint ﬁrst becomes active. In other words,
we want the HOCBF constraint to become active as late as
possible in the QPs. In this way, the feasibility robustness
of the controller with respect to unknown unsafe sets is
maximized. The main beneﬁts of maximizing the robustness
lie in the fact
the QP feasiblity can be maintained
when the unsafe sets are unknown and with detection noise,
as will be shown later. Another contribution of this paper
is to put forward a feasibility-guided method to learn the
optimal parameters in a HOCBF corresponding to a speciﬁc
type of unsafe set such that the robustness is maximized.
We compare the proposed feasibility-guided method with
the gradient-descent method with results showing improved
controller robustness in a robot control problem.

that

This paper is structured as follows. In Sec. II, we give
preliminaries on HOCBFs and CLFs. In Sec. III, we formu-
late an optimal control problem with safety constraints and
control limitations. The framework of learning the optimal
penalties and powers for a speciﬁc type of unsafe set is given
in Sec. IV. We provide simulations and comparisons in Sec.
V, and conclude with ﬁnal remarks and directions for future
work in Sec. VI.

II. PRELIMINARIES
Deﬁnition 1: (Class K function [8]) A continuous func-
tion α : [0, a) → [0, ∞), a > 0 is said to belong to class K
if it is strictly increasing and α(0) = 0.

Consider an afﬁne control system of the form

˙x = f (x) + g(x)u

(1)

where x ∈ Rn, f : Rn → Rn and g : Rn → Rn×q are
globally Lipschitz, and u ∈ U ⊂ Rq (U denotes the control
constraint set). Solutions x(t) of (1), starting at x(0) (we set
the initial time to 0 without loss of generality), t ≥ 0, are
forward complete.

Deﬁnition 2: A set C ⊂ Rn is forward invariant for
system (1) if its solutions starting at any x(0) ∈ C satisfy
x(t) ∈ C for ∀t ≥ 0.

Deﬁnition 3: (Relative degree) The relative degree of a
continuously differentiable function b : Rn → R with respect
to system (1) is the number of times we need to differentiate
it along its dynamics until the control u explicitly shows in
the corresponding derivative.

In this paper, since function b is used to deﬁne a constraint
b(x) ≥ 0, we will also refer to the relative degree of b as
the relative degree of the constraint.

For a constraint b(x) ≥ 0 with relative degree m, b :
Rn → R, and ψ0(x) := b(x), we deﬁne a sequence of
functions ψ1 : Rn → R, ψ2 : Rn → R, . . . , ψm : Rn → R:
ψi(x) := ˙ψi−1(x) + αi(ψi−1(x)), i ∈ {1, 2, . . . , m},

(2)

where αi(·), i ∈ {1, 2, . . . , m} denotes a differentiable class
K function.

We further deﬁne a sequence of sets C1, C2, . . . , Cm

associated with (2) in the form:

Ci := {x ∈ Rn : ψi−1(x) ≥ 0}, i ∈ {1, 2, . . . , m}.

(3)

Deﬁnition 4: (High Order Control Barrier Function
(HOCBF) [21]) Let C1, C2, . . . , Cm be deﬁned by (3) and
ψ1(x), ψ2(x), . . . , ψm(x) be deﬁned by (2). A function b :
Rn → R is a high order control barrier function (HOCBF) of
relative degree m for system (1) if there exist differentiable
class K functions α1, α2, . . . , αm such that

f

f b(x) + LgLm−1
Lm

b(x)u + O(b(x)) + αm(ψm−1(x)) ≥ 0,
(4)
for all x ∈ C1 ∩ C2∩, . . . , ∩Cm. In (4), Lf , Lg denote Lie
derivatives along f and g, respectively, O(·) denotes the
remaining Lie derivatives along f with degree less than or
equal to m − 1 (omitted for simplicity, see [21]).

Given a HOCBF b, we deﬁne the set of all control values

that satisfy (4) as:

Kcbf = {u ∈ U : Lm

f b(x) + LgLm−1
+O(b(x)) + αm(ψm−1(x)) ≥ 0}

b(x)u

f

(5)

Theorem 1: ([21]) Given a HOCBF b(x) from Def. 4
with the associated sets C1, C2, . . . , Cm deﬁned by (3), if
x(0) ∈ C1 ∩ C2∩, . . . , ∩Cm, then any Lipschitz continuous
controller u(t) ∈ Kcbf , ∀t ≥ 0 renders C1 ∩C2∩, . . . , ∩Cm
forward invariant for system (1).

Deﬁnition 5: (Control Lyapunov function (CLF) [1]) A
continuously differentiable function V : Rn → R is a glob-
ally and exponentially stabilizing control Lyapunov function
(CLF) for system (1) if there exist constants c1 > 0, c2 >
0, c3 > 0 such that

c1||x||2 ≤ V (x) ≤ c2||x||2

[Lf V (x) + LgV (x)u + c3V (x)] ≤ 0.

inf
u∈U
for ∀x ∈ Rn.

(6)

(7)

Theorem 2: ([1]) Given an exponentially stabilizing CLF
V as in Def. 5, any Lipschitz continuous controller u ∈
Kclf (x), with

Kclf (x) := {u ∈ U : Lf V (x) + LgV (x)u + c3V (x) ≤ 0},

exponentially stabilizes system (1) to its zero dynamics
(deﬁned by the dynamics of the internal part if we transform
the system to standard form and set the output to zero [8]).
Note that (7) can be relaxed by adding a relaxation at its
right-hand side [1].

Recent works [2], [9], [12] combine CBFs and CLFs
with quadratic costs to form optimization problems. Time
is discretized and an optimization problem with constraints
given by CBFs and CLFs is solved at each time step. Note
that these constraints are linear in control since the state
is ﬁxed at the value at the beginning of the interval, and
therefore the optimization problem is a quadratic program

(QP). The optimal control obtained by solving the QP is
applied at the current time step and held constant for the
whole interval. The dynamics (1) are updated, and the
procedure is repeated. This method works conditioned on the
fact that the QP is always feasible. We will show how we
can further improve the QP feasibility by maximizing the
feasibility robustness (will be formally deﬁned in the next
section) of the controller with respect to unknown unsafe
sets in this paper.

III. PROBLEM FORMULATION

Consider an optimal control problem for system (1) with

the cost deﬁned as:

(cid:90) tf

J(u(t)) =

C(||u(t)||)dt,

(8)

min
u(t)

0
where || · || denotes the 2-norm of a vector; tf denotes the
ﬁnal time; and C(·) is a strictly increasing function of its
argument.

State convergence: We want the state of system (1) to

converge to a point K ∈ Rn, i.e.,

||x(t) − K|| ≤ ξ, ∀t ∈ [t(cid:48), tf ],

(9)

where ξ > 0 is arbitrarily small and t(cid:48) ∈ [0, tf ].

Constraint 1 (Unsafe Sets): Let So denote a set of unsafe
sets. System (1) should always avoid all unsafe regions
(obstacles) j ∈ So, i.e.,

hj(x(t)) ≥ 0, ∀t ∈ [0, tf ].
(10)
where hj : Rn → R, ∀j ∈ So is continuously differentiable.
A HOCBF constraint for (10) becomes active when a
control u makes inquality (4) become an equality for b = hj.
Feasibility robustness: The feasibility robustness of a
controller with respect to a constraint (10) can be quantiﬁed
by the value of hj(x(ta)) (the value of hj(·) usually denotes
the distance to the unsafe set j ∈ So) when the HOCBF
constraint (4) for (10) ﬁrst becomes active (and active after-
wards) at ta ∈ [0, tf ]. In order to maximize the feasibility
robustness, we need to minimize

min
ta

hj(x(ta)), j ∈ So.

(11)

As an example, consider the adaptive cruise control prob-
lem [21]. The distance z(t) between the controlled vehicle
and the vehicle in front (both vehicles have double integrator
dynamics and control constraints) should be greater than a
constant δ > 0, i.e., z(t) ≥ δ, ∀t ≥ 0. Then we can deﬁne
a HOCBF h(x) := z(t) − δ (m = 2 in Def. 4 since the
relative degree of h(·) is 2 for double integrator dynamics)
for this safety constraint, and any control input should satisfy
the HOCBF constraint (4). If the HOCBF constraint is ﬁrst
active at ta and the value of h(x(ta)) is very small (while
the control constraints should be always satisﬁed), i.e., the
distance between these two vehicles is small, then the con-
troller (from the QPs) is less constrained (before the HOCBF
constraint becomes active) and robust to perturbations (such
as noise). Thus, the feasibility robustness of the controller is
improved and we wish to solve minta h(x(ta)).

Remark 1: There are three main advantages of maximiz-
ing the feasibility robustness of the controller. (i) The QPs
are more likely to become feasible since fewer (HOCBF)
constraints will become active when a system gets close to a
number of unsafe sets; (ii) In an unknown environment, the
controller obtained through the QPs is more robust to the
change of environment and the detection of unknown unsafe
sets since the corresponding HOCBF constraints only work
(become active) when a system gets close to these unsafe
sets. If the corresponding HOCBF constraints become active
before the unsafe sets are detected, the system will fail to
avoid these unsafe sets (i.e., QPs become infeasible). (iii)
There is higher probability to ﬁnd a better solution (e.g.,
energy optimal) if the feasibility robustness is maximized
since the QP solutions are less constrained.

Constraint 2 (State Limitations): Assume we have a set

of constraints on the state of system (1) in the form:

xmin ≤ x(t) ≤ xmax, ∀t ∈ [0, tf ],
(12)
where xmax := (xmax,1, xmax,2, . . . , xmax,n) ∈ Rn and
xmin := (xmin,1, xmin,2, . . . , xmin,n) ∈ Rn denote the
maximum and minimum state vectors, respectively, and the
inequality is interpreted componentwise.

Constraint 3 (Control limitations): Assume we have a set

of constraints on control inputs of system (1) in the form:

umin ≤ u(t) ≤ umax, ∀t ∈ [0, tf ].
(13)
where umin ∈ Rq and umax ∈ Rq denote the minimum
and maximum control input vectors, respectively (i.e., the
constraint set U in (1) is rectangular).

A control policy for system (1) is f easible if constraints
(10), (12) and (13) are satisﬁed. In this paper, we consider
the following problem:

Problem: Find a feasible control policy for system (1)
such that cost (8) is minimized, robustness is maximized (i.e.,
(11) is minimized), constraints 1, 2, 3 ((10), (12) and (13))
are strictly satisﬁed, and state convergence (9) is satisﬁed
with the smallest possible ξ and t(cid:48).

Approach: The robustness objective (11) depends on the
time ta, while ta is determined once a HOCBF in the above
problem is deﬁned. Therefore, we need to consider objective
(11) in the deﬁnition of a HOCBF. We break the above
problem into two sub-problems: (i) objective (8) subject to
(10), (12), (13) and (9) that is solved with the QP-based
method introduced at the end of Sec. II; (ii) objective (11)
after solving sub-problem (i) ∀t ∈ [0, tf ].
IV. LEARNING TO INCREASE FEASIBILITY ROBUSTNESS
In this section, we introduce how to learn the optimal
the
parameters in the deﬁnition of a HOCBF such that
feasibility robustness of the controller with respect to un-
known unsafe sets is maximized, i.e., how to reformulate
sub-problem (i), (ii) introduced at the end of the last section.
We deﬁne unsafe sets as being of the same “type” if they
have the same geometry. For example, circular unsafe sets
are the same type if they have the same radius but different
locations. Let St ⊆ So denote the index set of all the unsafe
set types in So.

A. HOCBF and CLF-based QP (sub-problem (i))

C. Feasibility-Guided Optimization (sub-problem (ii))

The approach to sub-problem (i) is based on partitioning
the time interval [0, tf ] into a set of equal time intervals
{[0, ∆t), [∆t, 2∆t), . . . }, where ∆t > 0. In each interval
[ω∆t, (ω +1)∆t) (ω = 0, 1, 2, . . . ), we assume the control is
constant (i.e., the overall control will be piece-wise constant).
Then at t = ω∆t, we solve

min
u(t),δ(t)

C(||u(t)||) + pδ2(t)

(14)

subject to (13), the CLF constraint (7) for (9) (by deﬁning
a CLF for (9) such that a CLF constraint similar to (7 is
satisiﬁed) and the HOCBF constraints (4) corresponding to
(10) and (12), where p > 0 is a penalty on the relaxation
δ(t) (δ(t) is a relaxation variable that replaces 0 on the
right-hand side of (7)). Since the state is kept constant at its
value at the beginning of the interval, the above optimization
problem is a QP, which can easily become infeasible. In
the rest of the paper, we show how we can use machine
learning techniques in ﬁnding the optimal parameters in the
deﬁnition of a HOCBF such that the feasibility robustness is
maximized.

B. The Penalty Method

To improve the feasibility [21] of the problem (14), we add
penalties on the class K functions α1(·), α2(·), . . . , αm(·) in
(2) in the deﬁnition of a HOCBF b(x). In the set of class
K functions that consist of power functions, we explictly
rewrite (2) as

ψ0(x) :=b(x)
ψ1(x) := ˙ψ0(x) + p1ψq1

0 (x),

...

ψm(x) := ˙ψm−1(x) + pmψqm

m−1(x),

(15)

where p1 > 0, p2 > 0, . . . , pm > 0 and q1 > 0, q2 >
0, . . . , qm > 0.

For each type of unsafe set j ∈ St, we consider an
arbitrary location for it and get an unsafe set constraint
hj(x(t)) ≥ 0 (similar to (10)). Let p := (p1, p2, . . . , pm),
q := (q1, q2, . . . , qm). We know from [21] that the values
of q1, q2, . . . , qm affect
the feasibility region of (14), as
well as what time the HOCBF constraint (4) will be ac-
tive, i.e., we can rewrite hj(x(ta)) as hj(x(ta), p, q). Let
Dj(p, q) := hj(x(ta), p, q) (since hj(x(ta), p, q) is ﬁxed
once p, q are given, hj(·) does not actually depend on x(ta)).
We reformulate (11) as

min
p,q

Dj(p, q), j ∈ St.

(16)

We can view the minimization of Dj(p, q) as the maximiza-
tion of the feasibility robustness that depends on p, q.

Then, we need to ﬁnd the optimal p and q that minimize
(16) for each unsafe set type j ∈ St. However, this optimiza-
tion problem is hard to solve. We will introduce an approach
using machine learning techniques in the following section.

The optimization problem (16) is a typical problem that
can be solved with reinforcement learning approaches. How-
ever, most of the p, q values result in infeasible solutions of
problem (14), which makes (16) difﬁcult to solve. Therefore,
we need to ﬁrst solve the infeasiblity problem of sub-problem
(i). We randomly sample p, q values over their domain, and
for each set of p, q values, we solve problem (14) until the
state convergence (9) is achieved. If problem (14) (the QPs)
is feasible at all times, then we label this set of p, q values (as
a whole) as +1, otherwise, we label it as −1. Eventually, we
get sets of feasible and infeasible p, q points. Then we can
apply a machine learning technique (such as support vector
machine, deep neural network etc.) to classify these two sets
and get a continuously differentiable hypersurface:

where

Hj : R2m → R,

Hj(p, q) ≥ 0

(17)

(18)

denotes the set of p, q values (as a whole) which leads to the
feasible solution of QPs (14), i.e., the feasibility constraint
for the set of p, q values associated with the QPs (14). We
can use a HOCBF to enforce (18) if p, q are state variables
of a system, which motivates us to deﬁne dynamics for p, q,
as shown later.

Based on the feasiblity classiﬁcation hypersurface (17), we
look further to optimize (16), i.e., we consider (16) subject
to (18). However, the learned hypersurface (17) is generally
complex, and thus makes this optimization problem very hard
to solve. We use the following approach to simplify this
optimization problem.

We start at some feasible p0 ∈ Rm, q0 ∈ Rm to
search for the optimal p, q values. Since the determination
of the optimal p, q is a dynamic process, we deﬁne the
gradient (dynamics) for p, q (as the variations of p, q that
are controlled), i.e., we have

( ˙p(t), ˙q(t)) = ν(t), ˙p(t0) = p0, ˙q(t0) = q0,

(19)

where ν ∈ R2m denotes a controllable input vector in
the dynamic process constructed in order to determine the
optimal p, q. t denotes the dynamic process time for the
optimization of (16), which is different and independent from
t in the system (1) and problem (14). t0 ∈ R denotes the
initial time.

Considering feasibility of the problem (14), the dynamic
process (determined by ν) should be subjected to (19) and
(18). Since we want to ﬁnd the control ν such that the
resulting p, q (determined by ν) always lead to the feasible
solution of QPs (14) with the CBF method, i.e., we need to
take the derivative of (17), we minimize the derivative of (16)
(the fastest decreasing direction of the value of Dj(p, q) in
(16)) in the dynamic process to make ν also show up in the
cost function. As long as the derivative of (16) is negative,
we make sure that (16) is decreasing in each time step (by
discretizing t similar to sub-problem (i)).

By taking the derivative of (16) with respect to t, we have

dDj(p(t), q(t))
dt

=

=

dDj(p(t), q(t))
d(p(t), q(t))
dDj(p(t), q(t))
d(p(t), q(t))

d(p(t), q(t))
dt

ν.

(20)

The relative degree of the feasibility constraint (18) with
respect to (19) is 1. We then use a HOCBF with m = 1
(as in Def. 4) to enforce (18) and ﬁnd a control ν that can
satisfy (18) in the dynamic process:

dHj(p, q)
d(p, q)

ν + α1(Hj(p, q)) ≥ 0,

(21)

where α1(·) is a class K function as in Def. 4 (the deﬁnition
of ψ1(·) in (2)). Any control input ν that satisﬁes (21) implies
that the resulting p, q (determined by ν) lead to a feasible
solution of QPs (14) in the dynamic process.

Then, we reformulate sub-problem (ii) by the dynamic
process (feasibility-guided optimization (FGO)). We use
the approach introduced as in Sec. IV-A to solve the dynamic
process, i.e., we discretize t, at each t = ω∆t, ω ∈ {0, 1, . . . }
(∆t > 0 denotes the discretization constant), and we solve

min
ν(t)

dDj(p(t), q(t))
d(p(t), q(t))

ν(t)

(22)

subject to (21), (19). Then update (19) for t ∈ (ω∆t, (ω +
1)∆t) with ν∗(t). Note that in the last equation, dDj (p(t),q(t))
d(p(t),q(t))
is a vector of dimension 1 × 2m, while ν is a vector of
dimension 2m × 1. Therefore, the cost function in the last
equation is a scalar function of ν.

The optimization problem (22) is a linear program (LP)
(to determine ν) at each time step for each initial p, q (we
need to reset t for each set of initial p, q values). Without
any constraint on ν, the LP (22) is ill-posed because it leads
to unbounded solutions. In fact, the value of ν determines
the search step length of the dynamic process, and we want
to limit this step length. Otherwise, the solution of the LP
at each step is inﬁnity (i.e., the dynamic process search
step length is inﬁnity, and fails to work). Therefore, we add
limitations to ν for the LP (22):

νmin ≤ ν ≤ νmax.

(23)

where νmin < 0, νmax > 0 (interpreted componentwise),
0 ∈ R2m.

After adding (23) to (22), the dynamic process search
step length will become bounded. Although there are control
limitations on ν, the resulting LP from the optimization
(22) is always feasible since the relative degree of (18) with
respect to (19) is 1.

Note that in (22), we have

dDj
d(p, q)

= (

∂Dj
∂p1

, . . . ,

∂Dj
∂pm

,

∂Dj
∂q1

, . . . ,

∂Dj
∂qm

)

and we also need to evaluate ∂Dj
, . . . , ∂Dj
∂qm
∂p1
at each time step (i.e., evaluate the coefﬁcients of the cost
function (22)).

, . . . , ∂Dj
∂pm

, ∂Dj
∂q1

, ∂Dj
∂q1

We present

, . . . , ∂Dj
∂qm

, . . . , ∂Dj
∂pm

the FGO algorithm from Alg. 1,

the FGO algorithm in Alg. 1. For each
step of
the follow-
ing four conditions may terminate the algorithm: (i) the
problem (14) becomes infeasible (since the hypersurface
(17) from the machine learning techniques cannot ensure
100% classiﬁcation accuracy), (ii) the evaluated values of
∂Dj
are all 0, (iii) the objective
∂p1
function value of (16) is greater than the current known
minimum value. (iv) the iteration time exceeds some N ∈ N.
If we consider Alg. 1 without the constraint (21), then we
have the commonly used gradient descent (GD) algorithm.
The FGO algorithm is more conservative compared with GD
since the solution searching path is guided by the feasibility
of (14). We can apply GD one step forward whenever the
FGO algorithm terminates to alleviate this limitation, which
is shown in the last part of Alg. 1.

Algorithm 1: FGO algorithm
Input: Constraints (10) (9), system (1) with (13), N
Output: p∗, q∗, Dmin
Sample p, q in the deﬁnition of the HOCBF;
Discard samples that do not meet the initial conditions
of HOCBF constraint (4);
Solve (14) for each sample for t ∈ [0, tf ] and label all
samples;
Use machine learning to ﬁnd classiﬁer (17);
Pick a feasible p0, q0, Dmin := Dj(p0, q0), iter. = 1;
while iter.++ ≤ N do
Evaluate ∂Dj
∂p1
if ∂Dj
∂pk
Jump to the very begining of the loop;

, ∀k ∈ {1, 2, . . . , m} is infeasible then

, . . . , ∂Dj
∂pm

, . . . , ∂Dj
∂qm

at p0, q0;

, ∂Dj
∂qk

, ∂Dj
∂q1

else

∂Dj
∂pk
that ∂Dj
∂pk

= 0, ∂Dj
∂qk
, ∂Dj
∂qk

= 0 if ∃k ∈ {1, 2, . . . , m} such

is infeasible to evaluate;

end
Solve optimization (22) and get new p, q;
Solve problem (14) with p, q;
if (14) is feasible then

if Dmin ≥ Dj(p, q) then

Dmin = Dj(p, q), p0 = p, q0 = q;

else

break;

end

else

Solve optimization (22) without (21) and get
new p, q;
Solve problem (14) with p, q;
if Dmin ≥ Dj(p, q) then

Dmin = Dj(p, q), p0 = p, q0 = q;

else

break;

end

end

end
p∗ = p0, q∗ = q0;

D. Feasibility Generalization

The feasibility and feasibility robustness of the controller
for problem (14) is sensitive to the “shape” of the unsafe
sets, but not to the “location”. For example, the location of a
circular obstacle does not affect the feasibility and feasibility
robustness of the controller for a robot, but the geometry of
this circular obstacle does. In this case, we do not need to
know the exact location of the obstacle. If we have learned
feasibility for a speciﬁc location obstacle and get optimal
p∗, q∗ with the FGO algorithm, then the optimal p∗, q∗ apply
to other located obstacles of the same geometry.

In the case that we know the type of unsafe sets but
not the locations, we can learn feasibility and robustness
for each type of unsafe set given an arbitrary location with
the FGO algorithm. Since the initial system condition may
also affect the feasibility of problem (14), we may learn
the optimal p∗, q∗ under the worst initial conditions (e.g.,
with maximum obstacle-approaching speed for a robot), and
then these optimal p∗, q∗ may also apply to other initial
conditions. For example, we may set the initial heading angle
(as well as the target heading angle) of a robot so as to
initially pass through the center of the circle obstacle and
set the speed to its maximum speed. Once the optimal p∗, q∗
are found under this conditon, they may also be applied to
other conditions.

In an unknown environment, system (1) may even not
know the type of the unsafe sets, i.e., the formulation of
(10). We can learn feasibility and robustness for some type-
known unsafe sets with the FGO algorithm, and then use
these unsafe sets to approximate any other types of unsafe
sets.

V. IMPLEMENTATION AND CASE STUDIES

We implemented the FGO algorithm in MATLAB and
performed simulations for a robot control problem. Suppose
all the obstacles are of the same type but the obstacle number
and their locations are unknown to the robot, and the robot
is equipped with a sensor ( 2
3 π ﬁeld of view (FOV) and 7m
sensing distance with 1m sensing uncertainty) to detect the
obstacles.

The robot dynamics are deﬁned in the form:













˙x
˙y
˙θ
˙v
(cid:124) (cid:123)(cid:122) (cid:125)
˙x

=







(cid:124)

v cos(θ)
v sin(θ)
0
0
(cid:123)(cid:122)
f (x)







(cid:125)

+







(cid:124)







(cid:125)

0 0
0 0
1 0
0 1
(cid:123)(cid:122)
g(x)

(cid:21)

(cid:20) u1
u2
(cid:124) (cid:123)(cid:122) (cid:125)
u

(24)

where x, y denote the location along x, y axis, respectively,
θ denotes the heading angle of the robot, v denotes the linear
speed, and u1, u2 denote the two control inputs for turning
and acceleration, respectively.

We consider cost (8) as the energy consumption in the

form:

J(u(t)) =

(cid:90) tf

η

0

max{u2
max{u2

2,min, u2
1,min, u2

2,max}
1,max}

1(t)+(1−η)u2
u2

2(t)dt (25)

TABLE I
SIMULATION PARAMETERS

Name
p
(cid:15)
vmin
u1,min
u2,min

value
1
10
0
-0.2
-0.5

unit
unitless
unitless
m/s
rad/s
m/s2

Name
r
∆t
vmax
u1,max
u2,max

value
7
0.1
2
0.2
0.5

unit
m
s
m/s
rad/s
m/s2

where u1,min < 0, u1,max > 0, u2,min < 0, u2,max > 0
denote the minimum and maximum turning control and
acceleration control, respectively. η ∈ [0, 1] denotes a weight
factor which captures the tradeoff between the two compo-
nents.

We also want the robot to arrive at a destination (xd, yd) ∈
R2,
i.e., drive (x(t), y(t)) to (xd, yd), ∀t ∈ [t(cid:48), tf ], t(cid:48) ∈
[0, tf ], as deﬁned in (9). The dynamics (24) are not full
state linearizable [8] and the relative degree of the position
(output) is 2. Therefore, we cannot directly apply a CLF.
However, the robot can arrive at the destination if its heading
angle θ stabilizes to the desired direction and its speed v
stabilizes to a desired speed v0 > 0, i.e.,

θ(t) → arctan(

yd − y(t)
xd − x(t)

), v(t) → v0, ∀t ∈ [0, tf ].

(26)

Now, we can apply the CLF method to (26) (as introduced
in Def. 5) since the relative degrees of the heading angle and
speed are 1.

The unsafe sets (10) are deﬁned as circular obstacles:
(cid:112)(x(t) − xi)2 + (y(t) − yi)2 ≥ r, ∀t ∈ [0, tf ], ∀i ∈ S,

(27)

where (xi, yi) denotes the location of the obstacle i, and
r > 0 denotes the safe distance to the obstacle.

The speed and control constraints (13) are deﬁned as:

vmin ≤ v(t) ≤ vmax, ∀t ∈ [0, tf ],
u1,min ≤ u1(t) ≤ u1,max, ∀t ∈ [0, tf ],
u2,min ≤ u2(t) ≤ u2,max, ∀t ∈ [0, tf ].

(28)

vmin ≥ 0, vmax > 0 denote the minimum and maximum
speed, respectively. The simulation parameters are listed in
Table I.

We set up the FGO algorithm training environment with
the initial position of the robot, the location of the ob-
stacle and the destination as (5m, 25m), (32m, 25m) and
(45m, (25 + ε)m) where ε ∈ R, respectively. The initial
heading angle and speed of the robot are 0 deg and vmax, re-
spectively, ∆t = 0.1, νmax = −νmin = (0.1, 0.1, 0.1, 0.1).
The map for FGO training is shown in Fig. 1.

Note that the value of ε in this example will affect the
trajectory of the robot since we have a circular obstacle. If
ε = 0, the robot will eventually stop at the equilibrium point
shown in Fig. 1 since the desired heading angle (26) in the
CLF exactly passes through the origin of the obstacle. If
ε > 0, the robot goes left around the obstacle as shown in
Fig. 1. Otherwise, the robot turns right and then goes to the
destination.

We choose a very small ε (cid:54)= 0 in our FGO algorithm.
Since the obstacle constraint (27) is with relative degree 2

We also compared the CBF-based robot exploration frame-
work with the RRT and A* algorithms by picking one frame
from the last simulation and ﬁxing the location of all the ob-
stacles, as shown in Fig. 3. Both the RRT and A* algorithms
have global environment information such that they tend to
choose shorter-length trajectories compared with the CBF
method. But this advantage may disappear if the environment
is changing fast, in which case the CBF method tends to
be more robust and computationally efﬁcient. Comparisons
based on four different criteria are shown in Table III. The
computation time for the CBF method is only shown for
the solution of one-step QPs in Table III since it does not
need a receding horizon, while the computation time for the
RRT and A* algorithms are for the path planning time. In a
dynamic environment, the RRT and A* algorithms need to
re-plan their path at each time step, which may take less time
than the ones shown in Table III but is more demanding for
these two algorithms. Therefore, we can see that the CBF-
based framework is more adaptive in robot safe exploration.

VI. CONCLUSIONS

We improved the constrained optimal control problem
feasibility by maximizing the feasibility robustness through
the learning of optimal parameters in the deﬁnition of a high
order control barrier function that works for arbitrary relative
degree constraints. This is achieved by a feasibility-guided
learning approach. The proposed feasibility-guided learn-
ing approach has shown an improved ability to determine
the optimal parameters compared with the gradient-descent
method. The implementation on a robot safe exploration
problem has shown good potential and adaptivity of the
proposed framework for planning with safety guarantees
compared with other path planning algorithms. Future work
will focus on how to deal with traps formed by obstacles,
including environment and system noise.

REFERENCES

[1] A. D. Ames, K. Galloway, and J. W. Grizzle. Control lyapunov
functions and hybrid zero dynamics. In Proc. of 51rd IEEE Conference
on Decision and Control, pages 6837–6842, 2012.

[2] A. D. Ames, J. W. Grizzle, and P. Tabuada. Control barrier function
based quadratic programs with application to adaptive cruise control.
In Proc. of 53rd IEEE Conference on Decision and Control, pages
6271–6278, 2014.

[3] J. P. Aubin. Viability theory. Springer, 2009.
[4] S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge

university press, New York, 2004.

[5] J. Carpentier, R. Budhiraja, and N. Mansard. Learning feasibility
constraints for multi-contact locomotion of legged robots. In Robotics:
Science and Systems, Cambridge, MA, 2017.

[6] R. A. Freeman and P. V. Kokotovic. Robust Nonlinear Control Design.

Birkhauser, 1996.

[7] K. Galloway, K. Sreenath, A. D. Ames, and J.W. Grizzle. Torque sat-
uration in bipedal robotic walking through control lyapunov function
based quadratic programs. preprint arXiv:1302.7314, 2013.

[8] H. K. Khalil. Nonlinear Systems. Prentice Hall, third edition, 2002.
[9] L. Lindemann and D. V. Dimarogonas. Control barrier functions for
signal temporal logic tasks. IEEE Control Systems Letters, 3(1):96–
101, 2019.

[10] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. preprint arXiv:1602.01783, 2016.

[11] V. Mnih, K. Kavukcuoglu, D. Silver, and A. A. Rusu et.al. Human-
level control through deep reinforcement learning. Nature, 518, 2015.

FGO algorithm pre-training map with some feasible example

Fig. 1.
trajectories.
with respect to system (24), we have p = (p1, p2), q =
(q1, q2). We randomly sample M (M is a positive integer)
training and 1000 testing samples (around half of them are
feasible for this robot path planning problem) for p and q
over interval (0, 3] and (0, 2], respectively.

The classiﬁcation model is the support vector machine
(SVM) with polynomial kernel of degree 7, i.e., the kernel
function k(y, z) is deﬁned as

k(y, z) = (c1 + c2yT z)7.

(29)

where y, z denote input vectors of SVM (i.e., y := (p, q),
as well as for z). We set c1 = 0.8, c2 = 0.5, and the
comparisons between FGO and GD are shown in Table II.
The FGO has better performance compared with GD in
ﬁnding Dmin when the number of training samples M for
the hypersurface (21) is large enough, as shown in Table II.
But this advantage decreases when the classiﬁcation accuracy
of the hyper surface (21) further increases, which may be due
to over-ﬁtting. One comparison example between FGO and
GD search paths is shown in Fig. 2(a), 2(b). We can combine
the FGO and GD algorithms, i.e., we choose the best result
from the FGO and GD algorithms by implementing both
of them. If we continue to apply the FGO algorithm to the
good results from GD, the further improvement percentage
is around 5% among all the testing samples.

2, q∗

1, p∗

We have also implemented the learned optimal penal-
1, q∗
ties and powers ((p∗
2) = (0.7426, 1.9745, 1.9148,
0.7024), not unique) in the deﬁnition of all the HOCBFs for
all obstacles in a robot exploration problem in an unknown
environment. All the circular obstacles (with different size
to test the robustness of the penalty method with the learned
optimal penalties and powers) move randomly. It is assumed
that any obstacles that are detected by the robot will stop
moving until the robot moves away. This is to ensure that the
robot will not collide with obstacles passively (i.e., collisions
happen due to the movement of obstacles). The robot can
safely avoid all the obstacles and arrive at its destination if
the obstacles do not form traps such that the robot has no
way to escape.

TABLE II
COMPARISONS BETWEEN THE GD AND FGO ALGORITHMS

items
Training sample number M
Classiﬁcation accuracy
Better than GD percentage
Worse than GD percentage
Dmin/m (samples min.: 5.0)

GD

4.6

FGO

500
0.879
0.210
0.270
4.6

1000
0.927
0.248
0.190
4.6

1500
0.939
0.254
0.232
4.6

2000
0.953
0.252
0.204
4.8

2500
0.960
0.244
0.218
4.6

3000
0.963
0.282
0.218
4.6

3500
0.966
0.288
0.240
4.6

4000
0.970
0.266
0.240
4.6

(a) FGO and GD algorithm search paths in 2D.

(b) FGO and GD algorithm search paths in 3D.

Fig. 2. FGO and GD algorithm search implementation. The red circles denote infeasible points and the green circles denote feasible points for p, q in
the training samples.

[12] Q. Nguyen and K. Sreenath. Exponential control barrier functions for
enforcing high relative-degree safety-critical constraints. In Proc. of
the American Control Conference, pages 322–328, 2016.

[13] P. Nillson and A. D. Ames. Barrier functions: Bridging the gap
between planning from speciﬁcations and safety-critical control.
In
Proc. of 57th IEEE Conference on Decision and Control, pages 765–
772, Miami, 2018.

[14] D. Panagou, D. M. Stipanovic, and P. G. Voulgaris. Multi-objective
control for multi-agent systems using lyapunov-like barrier functions.
In Proc. of 52nd IEEE Conference on Decision and Control, pages
1478–1483, Florence, Italy, 2013.

[15] N. Perrin, O. Stasse, L. Baudouin, F. Lamiraux, and E. Yoshida. Fast
humanoid robot collision-free footstep planning using swept volume
approximations. IEEE Transactions on Robotics, 28(2):427–439, 2012.
[16] S. Prajna, A. Jadbabaie, and G. J. Pappas. A framework for worst-
case and stochastic safety veriﬁcation using barrier certiﬁcates. IEEE
Transactions on Automatic Control, 52(8):1415–1428, 2007.

[17] E. Sontag. A lyapunov-like stabilization of asymptotic controllability.
SIAM Journal of Control and Optimization, 21(3):462–471, 1983.
[18] M. Srinivasan, S. Coogan, and M. Egerstedt. Feasibility envelopes for
metric temporal logic speciﬁcations. In Proc. of 57th IEEE Conference
on Decision and Control, pages 1991–1996, Miami Beach, FL, 2018.
[19] P. Wieland and F. Allgower. Constructive safety using control barrier
In Proc. of 7th IFAC Symposium on Nonlinear Control

functions.
System, 2007.

[20] R. Wisniewski and C. Sloth. Converse barrier certiﬁcate theorem.
In Proc. of 52nd IEEE Conference on Decision and Control, pages
4713–4718, Florence, Italy, 2013.

[21] W. Xiao and C. Belta. Control barrier functions for systems with high
relative degree. In Proc. of 58th IEEE Conference on Decision and
Control, 2019. available in arXiv:1903.04706.

[22] W. Xiao, C. Belta, and C. G. Cassandras. Decentralized merging
control in trafﬁc networks: A control barrier function approach.
In
Proc. ACM/IEEE International Conference on Cyber-Physical Sys-
tems, pages 270–279, Montreal, Canada, 2019.

Fig. 3.

Comparison of robot paths between CBF, A* and RRT.

TABLE III
PERFORMANCE COMPARISON BETWEEN CBF, A* AND RRT IN HIGHLY

DYNAMIC UNKNOWN ENVIRONMENT

item

CBF
A*
RRT

R.T.
compute
time
< 0.01s
1.3s
0.3s

safety
guarantee

Environment
knowledge

pre-training

Yes
No
No

not required
required
required

required
not required
not required

