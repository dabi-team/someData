2
2
0
2

n
u
J

9
1

]

C
O
.
h
t
a
m

[

2
v
7
6
4
1
0
.
2
1
9
1
:
v
i
X
r
a

Linear Convergence of Frank-Wolfe for Rank-One
Matrix Recovery Without Strong Convexity

Dan Garber
Technion - Israel Institute of Technology

dangar@technion.ac.il

Abstract

We consider convex optimization problems which are widely used as con-
vex relaxations for low-rank matrix recovery problems. In particular, in sev-
eral important problems, such as phase retrieval and robust PCA, the under-
lying assumption in many cases is that the optimal solution is rank-one. In
this paper we consider a simple and natural suﬃcient condition on the ob-
jective so that the optimal solution to these relaxations is indeed unique and
rank-one. Mainly, we show that under this condition, the standard Frank-
Wolfe method with line-search (i.e., without any tuning of parameters what-
soever), which only requires a single rank-one SVD computation per iteration,
ﬁnds an ǫ-approximated solution in only O(log 1/ǫ) iterations (as opposed to
the previous best known bound of O(1/ǫ)), despite the fact that the objective
is not strongly convex. We consider several variants of the basic method with
improved complexities, as well as an extension motivated by robust PCA,
and ﬁnally, an extension to nonsmooth problems.

1

Introduction

Optimization problems in which the goal is to recover a low-rank matrix given
certain data/measurements are ubiquitous in machine learning, statistics and re-
lated ﬁelds. These include for instance the well known matrix completion problem
[8, 33, 24, 21], the robust PCA problem [7, 36, 32, 37, 28], matrix formulations of
phase retrieval problems [6, 35, 38], and more. While the natural low-rank for-
mulations of these problems are NP-Hard, due to the non-convexity of the rank
constraint/objective, all of these problems admit well known and highly popular
convex relaxations in which the low-rank constraint is relaxed to a trace-norm con-
straint which is convex. These convex relaxations are well motivated both em-
pirically and from statistical theory point of view (see above references). On the
downside, the scalability of these convex relaxations to high-dimensional instances
is questionable, since, despite the implicit assumption that an optimal solution of
low-rank should exist, due to the relaxed trace-norm constraint, standard convex
optimization methods, such as projected gradient methods [30, 3] and even condi-
tional gradient-based methods (aka Frank-Wolfe), which are often the “weapon of

1

 
 
 
 
 
 
choice” for such problems [24, 27, 14, 1, 38, 13, 20, 19], may require in worst-case
to compute singular value decompositions (SVD) of high-rank matrices, and/or to
store in memory high-rank matrices, which greatly limits their applicability. Also,
since the objective in our case is not strongly convex, exiting analyses of conditional
gradient-based methods only give a slow O(1/ǫ) convergence rate [14, 1].

In this paper, we focus on low-rank matrix optimization problems in which the
goal is to recover a rank-one matrix. These include for instance important phase-
retrieval problems and several applications of robust PCA, just to name a few. We
begin by considering a simple and natural condition that certiﬁes that the convex
relaxation indeed admits a unique and rank-one optimal solution. This condition
simply requires that at an optimal point, the (minus) gradient matrix admits a
non-zero spectral gap between the ﬁrst and second leading components. Mainly, we
show that under this condition, the standard Frank-Wolfe method with line-search
converges to an ǫ-approximated solution with number of iterations that scales only
with log 1/ǫ, as opposed to 1/ǫ in standard Frank-Wolfe analyzes. In particular, we
obtain this exponential improvement without requiring the objective to be strongly
convex as required in several recent works (e.g., [14, 1, 20]). Moreover, our use of
the Frank-Wolfe method with line-search does not require any tuning of parameters
whatsoever.

Concretely, we consider the following canonical optimization problem:

f (X),

min
X
∈S

n

(1)

|

{

∈

×

X

X

Sn

Sn =

0, Tr(X) = 1
n real symmetric matrices and we use the standard notation X

is the spectrahedron in the space Sn of
where
n
0 to indicate
that X is a positive semideﬁnite matrix. The function f : Sn
R is assumed to be
convex, and unless stated otherwise it is also assumed to be β-smooth. We let f ∗
denote the optimal value of Problem (1).

→

(cid:23)

(cid:23)

}

We refer to Problem (1) as canonical, since it is well known that the highly

popular low-rank matrix convex relaxations:

min
k

X

∗
k

g(Y)

and

Y

τ

≤

∈

Y

Rm×n:

∈

min
(cid:23)

Sn: Y

0, Tr(Y)

≤

g(Y),

τ

(2)

could be directly formulated in the form of Problem (1) (in the above we let
denote the trace-norm, i.e., sum of singular values), see for instance [24] 1.

k·k∗

We now describe a simple suﬃcient condition so that the canonical problem
(1) indeed admits a unique optimal solution which is also a rank-one matrix. This
condition was already suggested in our recent work [16], however there it was con-
sidered for the purpose of controlling the rank of SVD computations required by
projected gradient methods to solve problems closely related to (1), and not for the
purpose of obtaining fast convergence rates for globally-convergent methods, which
is our main concern in this work.

1Here we note that while some problems, such as phase retrieval, are usually formulated as
optimization over matrices with complex entries, our results are applicable in a straightforward
X
.
manner to optimization over the corresponding spectrahedron
However, for simplicity of presentation we focus on matrices with real entries.

0, Tr(X) = 1

Cn×n

X

(cid:23)

∈

}

{

|

2

Assumption 1. There exists an optimal solution X∗ to Problem (1) such that
δ := λn

f (X∗)) > 0.

f (X∗))

λn(

1(

−

∇

−

∇

Lemma 1. [Lemma 7 in [16]] Under Assumption 1, Problem (1) admits a unique
optimal solution X∗ which is also a rank-one matrix, i.e., X∗ = x∗x∗⊤, where x∗ is
the eigenvector corresponding to the eigenvalue λn(

f (X∗)).

∇

While Assumption 1 is a suﬃcient condition for the the existence of a unique
and rank-one optimal solution, it is not a necessary condition. However, the fol-
lowing lemma suggests that this condition is necessary (and also suﬃcient) for the
robustness of the rank of optimal points to arbitrarily-small perturbations. In par-
n[X∗
ticular recall that by the ﬁrst-order optimality condition it holds that Π
−
S
β−
Sn. The
lemma is a simple adaptation of Lemma 3 in [16]. A proof is given in the appendix
for completeness.

] denotes the Euclidean projection onto

f (X∗)] = X∗, where Π
S

n[
·

∇

1

Lemma 2. Let f : Sn
solution of rank-one to the optimization problem minX
note the eigenvalues of
scalar. It holds that

∈ Sn be an optimal
n f (X). Let λ1, . . . , λn de-
∈S
f (X∗) in non-increasing order. Let ζ be a non-negative

R be β-smooth and convex. Let X∗

→

∇

] denotes the Euclidean

n[
·

rank(Π(1+ζ)
S

n[X∗

1

β−

f (X∗)]) > 1

ζ > β(λn

λn),

1 −

−

∇
−
(1 + ζ)X

Sn =

where (1 + ζ)
projection onto the convex set (1 + ζ)

∈ Sn}
Sn.
Lemma 2 shows that an eigen-gap in

{

|

X

⇐⇒
, and Π(1+ζ)
S

f (X∗) implies certain rank-robustness of
∇
Problem (1) to small perturbations in the trace bound. In particular, in case the
f (X∗) is zero, we see that an arbitrarily-small perturbation to the trace
gap in
bound will map an original optimal solution to a higher-rank matrix, which suggests
that in such a case, the convex relaxation is ill-posed for the purpose of rank-one
matrix recovery.

∇

The following lemma suggests that Assumption 1 is also robust to certain pertur-
), which can occur due to noise in the underlying

bations in the objective function f (
data/misspeciﬁcation. The proof is given in the appendix.

·

Lemma 3. Let f : Sn
holds w.r.t. f (
·
convex, and suppose that supX
2(1 + 2β
for ν < 1
˜δ = δ
2ν(1 + 2β

δ )−
δ ) > 0.

→

) with some parameter δ > 0. Let ˜f : Sn
kF ≤

R be β-smooth and convex. Suppose that Assumption 1
R be diﬀerentiable and
→
ν, for some ν > 0. Then,
) with parameter

1δ, Assumption 1 holds w.r.t. the function ˜f (

˜f (X)

f (X)

n k∇

− ∇

∈S

·

−

In Section 5 we bring empirical motivation for Assumption 1.

In this paper we leverage Assumption 1 to derive improved complexities for
the Frank-Wolfe method, and certain variants of, all demonstrating linear rate of
convergence for Problem (1) (at least under smoothness of f (
)). In fact, as we shall
show in the sequel (see Lemma 4 in Section 2), Assumption 1 in particular implies
that Problem (1) satisﬁes the quadratic growth property, which is well known to be

·

3

useful for proving linear convergence rates for ﬁrst-order methods (see for instance
[12, 29]). Nevertheless, even with such a property, achieving faster rates for Frank-
Wolfe-type methods is non-trivial since the standard O(1/ǫ) rate of the method is
not improvable in general, even under strong convexity (see for instance [14]). Here
we should also mention that, while Assumption 1 implies that the gradient vector
of f (
) is non-zero over the feasible set, a property which is known to result in a
linear convergence rate for the Frank-Wolfe method whenever the feasible set is
strongly convex (see for instance [10, 17]), in the case of Problem (1) (and also for
the related problems appearing in (2)), the feasible set is not strongly convex (or
curved), and thus such arguments do not apply in our case.

·

We focus on the Frank-Wolfe method since i) aside from achieving faster con-
vergence rates, we are also interested in methods that are computationally eﬃcient,
and in particular avoid high-rank singular value decompositions (SVD), and ii) the
Frank-Wolfe method allows to easily incorporate line-search, which avoids the need
to tune parameters, and in particular avoids the need to estimate the eigen-gap in
Assumption 1.

Concretely, our main algorithmic result in this paper is the proof of the following

theorem, which we currently present only informally.

Theorem 1. [informal] Under Assumption 1, the Frank-Wolfe method with line-
search (Algorithm 1), ﬁnds an ǫ-approximate solution (in function value) to Problem
(1), after O(log 1/ǫ) iterations (treating all other quantities, except for the dimen-
sion n, as constants). Moreover, it also ﬁnds in O(log 1/ǫ) iterations a unit vector
v such that

vv⊤

X∗

ǫ.

k

−

2
F ≤

k

A formal and complete description of this result is given in Theorem 2 in Section
In that section we also present two variants of the Frank-Wolfe method for
2.
In Section 3 we present an extension
Problem (1) with improved complexities.
of Assumption 1 and Theorem 1 to a class of problems that is motivated by the
robust PCA problem and takes the form of minimizing a function of the sum of
two blocks of variables, one corresponding to a rank-one matrix, and the other lies
in some convex and compact set (see Assumption 3 and Theorem 6). In Section 4
we consider Problem (1) in case the objective function is nonsmooth. Finally, in
Section 5 we present numerical simulations in the support of Assumption 1 and also
a preliminary comparison of the diﬀerent Frank-Wolfe variants considered in this
work.

Table 1 gives a quick summary of our results concerning Problem (1).
After the ﬁrst version of this work appeared online (see [15]), a followup work
[11] managed to extend the main result of this paper beyond the rank-one case
by providing a Frank-Wolfe-type method with a linear convergence rate under the
assumption that there exists a unique optimal solution X∗ with rank(X∗) = r∗
1
and under a natural modiﬁcation of Assumption 1, i.e., that X∗ satisﬁes an eigen-
f (X∗)) > 0 (note that as opposed
gap condition of the form: λn
to the case r∗ = 1 considered here, in case r∗ > 1, this eigen-gap assumption
In [11]
is not suﬃcient to imply that there exists a unique optimal solution).
it was also established that such an eigen-gap assumption in the gradient at the
optimal solution is equivalent to a strict complementarity condition for Problem

f (X∗))

λn(

r∗(

∇

∇

≥

−

−

4

(1). Additionally, while the linear convergence results in this paper only require
convexity and smoothness of f (
) and Assumption 1, the result in [11] requires the
C, X
objective function to be of the form f (X) = g(
) is smooth
i
h
Sn. Finally, and similarly to
and strongly convex,
[1], the algorithm in [11] requires on each iteration a SVD computation of rank
k

A
is a linear map, and C

r∗ = rank(X∗).

, where g(

X) +

A

∈

·

·

≥

1.1 Additional related work

A

In [39] the authors have considered an optimization problem closely related to (1),
which takes the form of unconstrained minimization of a smooth convex function
plus a nuclear norm regularizer. They showed that under the assumption that
X) where g is smooth and strongly convex and
the objective is of the form g(
is a linear map, and assuming there exists an optimal solution which satisﬁes
A
a condition somewhat similar to our Assumption 1, a proximal gradient method
converges linearly for the problem. While their result allows to consider optimal
solutions with arbitrary rank (not only one, as in our case), this current work has
three main advantages:
i) we do not require that the objective takes the form of
strongly convex and smooth function applied to linear map which, while capturing
several important applications, is also quite restrictive. Our result only requires the
objective to be smooth (and we also obtain a result for nonsmooth problems). ii) [39]
only establishes a linear convergence rate but does not detail how it depends on the
natural parameters of the problem (such as the condition they require on the optimal
solution). We on the other-hand, give fully-detailed convergence results with explicit
dependency on all relevant parameters.
iii) While the linear convergence rate in
[39] is relevant to proximal gradient methods, these are often not considered the
methods of choice for such problems because of the high complexity of computing
the proximal step which can require high-rank SVD computations 2. On the other
hand, here we establish linear convergence rates for the Frank-Wolfe method and
simple variants of, which require only rank-one SVD computation on each iteration,
and hence are often more suitable for such problems. Moreover, the Frank-Wolfe
method can be used with line-search which does not require any parameter tuning.
Finally, it is important to emphasize that there is a very active and recent
research eﬀort to analyze nonconvex optimization algorithms for low-rank matrix
optimization problems, such as the ones mentioned above, with global convergence
guarantees and often with linear convergence rates. However, these results are
usually obtained in a statistical setting, in which the data is assumed to follow a
very speciﬁc and potentially unrealistic statistical model, see for instance [25, 31,
26, 9, 5, 21] and references therein. On the contrary, in this work, we are free from
any statistical assumption/model.

2in the close proximity of an optimal solution it is quite plausible that only low-rank SVD
computations will be needed to compute the proximal step, see for instance our recent work [16]

5

Algorithm

FW (Alg
Thm 2)

FW (Alg
Thm 3)

assumption on
objective f (
·

)

requires
gap
(δ)?

1,

smooth

1,

X)+

C, X
g(
,
i
h
g smooth & s.c.

A

FWPG (Alg 2,
Thm 4)

FWPG (Alg 2,
Thm 4)

RegFW (Alg 3,
Thm 5)

RegFW (Alg 3,
Thm 7)

smooth

X)+

C, X
g(
,
i
h
g smooth & s.c.

A

smooth

nonsmooth

x

x

x

x

X

X

burn-
in
phase
3
β
δ3

3
β
δ2

3
β
δ3

3
β
δ2

x

x

SVD
rank

conv.
rate

max iterate rank

1

1

2

2

1

1

}

}

e−δt/β min

e−δt/β min

e−δt/β min

e−δt/β min

{

{

{

{

3

β

ǫ , β

δ3 + β log 1/ǫ

δ

3

β

ǫ , β

δ2 + β log 1/ǫ

δ

β

3
ǫ , β
δ3

β

3
ǫ , β
δ3

}

}

e−δt/β

β
δ log 1/ǫ

e−δǫt

1
δǫ log 1/ǫ

·

In all cases f (

Table 1: Summary of main results.
) is assumed convex. Burn-
in phase is number of iterations in which the method converges with standard
rate β/t, before shifting to the fast rate, SVD rank is the rank of SVD used on
each iteration, conv.
rate is the fast convergence rate after the initial burn-in
phase, and max iterate rank gives an upper bound on the number of rank-one
components in the representation of the iterate throughout the run, until reaching
an ǫ-approximate solution. The result for nonsmooth f (last line), applies to a
smooth ǫ-approximation of f , see details in Section 4. All results are given in
simpliﬁed form, omitting all constants except for ǫ, δ, β, and focusing on the most
interesting cases. s.c. stands for strongly convex.

6

1.2 Additional notation

k·k

denote the spectral norm (i.e., largest singular value),
For real matrices we let
k·kF denote the Frobenius (Euclidean) norm. For vectors in Rn we let
and we let
In any Euclidean space (e.g., Rn, Sn), we let
k·k2 denote the Euclidean norm.
Sn,
,
h·
n
i=1 λiuiu⊤i , we adopt the standard
when writing its eigen-decomposition A =
λn, and that the eigenvectors u1, . . . , un form an
convention that λ1 ≥
orthonormal basis for Rn (i.e., they have unit norm and are mutually orthogonal).

denote the standard inner product. For a symmetric real matrix A

λ2 ≥

P

...

≥

∈

·i

1.3 View as a non-linear extension of the leading eigenvec-

tor problem

Our main result (Theorem 1) could be seen as a faster reduction from nonlinear
optimization problems for which the optimal solution is just a leading eigenvector
of a certain matrix, to the standard leading eigenvector problem.

∈

Sn, i.e., f is a simple linear function.

Consider optimization problem (1) in the special case in which f (X) =

X, A
,
i
where A
It is well known that in this
case, Problem (1) becomes a tight semideﬁnite relaxation to computing the leading
1(A) is
eigenvector of the matrix
suﬃcient and necessary for this problem to admit a unique optimal solution which
A in this case is unique), i.e., the
is also rank-one (since the leading eigenvector of
unique optimal solution is X∗ = x∗x∗⊤, where x∗ is the eigenvector corresponding
to λn(A).

In particular, the condition λn(A) < λn

A.

−

−

h

−

·

For such f (

) it clearly holds that

f (X∗) = A. Thus, X∗ in particular corre-
sponds to the eigenvector of the smallest eigenvalue of the gradient vector at the
f (X∗)). More-
optimal solution (or equivalently to the leading eigenvector of
over, it is well known that standard iterative methods for leading eigenvector com-
putation, such as the well-known power iterations method (see for instance [22]),
converge with linear rate when such an eigen-gap exists.

−∇

∇

Indeed, Lemma 1 shows that for smooth and convex f , the condition λn(
1(

f (X∗)) is a suﬃcient condition so that X∗ is a unique optimal solution
λn
and also rank-one. In particular, it also corresponds to the eigenvector associated
f (X∗)) (or equivalently, the largest eigenvalue of
with the smallest eigenvalue λn(

∇

∇

−

f (X∗)) <

∇

−∇

f (X∗)). We thus refer to such problems as nonlinear eigenvector problems.
Thus, given the arsenal of eﬃcient methods for leading eigenvector computa-
tions, it is quite natural to ask if such nonlinear eigenvector problems could be
reduced to solving only a short sequence of the standard leading eigenvector prob-
lem. The standard Frank-Wolfe analysis (e.g., [24]) provides such a reduction, but
requires O(1/ǫ) leading eigenvector computations to ﬁnd an ǫ-approximated solu-
tion (treating all quantities except than 1/ǫ as constants, for simplicity). To the
best of our knowledge, Theorem 1 gives the ﬁrst reduction which requires only
O(log 1/ǫ) eigenvector computations without requiring the objective function to be
strongly convex.

7

compute an (approximated) leading eigenvector of

Sn

arbitrary point in

Algorithm 1 Frank-Wolfe with line-search for Problem (1)
1: X1 ←
2: for t = 1 . . . do
vt ←
3:
−∇
}
choose step size ηt ∈
Option 1:

[0, 1] using one of the two options:

η)Xt + ηvtv⊤t )

f (Xt))

f (Xt)

EV(

f ((1

−∇

4:

{

ηt ←

arg min
[0,1]
η
∈

−

Option 2:

ηt ←

arg min
[0,1]
η
∈

f (Xt) + η

vtv⊤t −

h

Xt,

∇

f (Xt)

+

i

5: Xt+1 ←
6: end for

(1

−

ηt)Xt + ηtvtv⊤t

η2β
2 k

Xt −

vtv⊤t k

2
F

2 Frank-Wolfe-Type Algorithms for Problem (1)

2.1 Proof of Theorem 1

We begin with a lemma that will be key to deriving novel bounds on the convergence
of Algorithm 1 under Assumption 1. This lemma also establishes that Assumption
1 implies that Problem (1) satisﬁes a quadratic growth property.

Lemma 4. Let X
some δX > 0. Let un be an eigenvector of
λn(

∈ Sn and suppose that λn
∇

f (X)). Then,

∇

1(

f (X))

δX for
∇
−
f (X) associated with the eigenvalue

f (X))

λn(

∇

−

≥

Y

∀

∈ Sn :

Y

h

−

unu⊤n ,

∇

f (X)

i ≥

δX(1

−

u⊤n Yun)

δX
2 k

Y

≥

unu⊤n k

2
F .

−

(3)

In particular, this implies that if X∗ is an optimal solution for Problem (1) which
satisﬁes Assumption 1 with parameter δ, then Problem (1) has the quadratic growth
property, that is

Y

Y

∈ Sn :

2
F ≤
∀
f (X) as
∈ Sn and let us write the eigen-decomposition of
n
i=1 λiuiu⊤i , where the eigenvalues are ordered in non-increasing order.

(f (Y)

f ∗).

X∗

(4)

∇

−

−

k

k

2
δ

Proof. Fix some Y

f (X) =
∇
It holds that
P

Y

h

−

unu⊤n ,

f (X)
i

∇

=

Y

h

−

unu⊤n ,

λiuiu⊤i i

λiu⊤i Yui −

λn

n

n

=

n

1

−

i=1
X

i=1
X
u⊤i Yui + λnu⊤n Yun −

λn

(λn + δX)

≥

=δX

n

1

−

i=1
X

i=1
X
u⊤i Yui=δX

8

u⊤n Yun

,

1

−

(cid:0)

(cid:1)

where the last two equalities follow since

Now, since since

Y

k

kF ≤

1, we can write
P

n
i=1 u⊤i Yui = 1.

Y

h

−

unu⊤n ,

∇

f (X)

i ≥

δX(1

−

u⊤n Yun)

δX
2

≥

unu⊤n k

k

2 +

Y

2
F −

k

k

2u⊤n Yun

=

δX
2 k

Y

unu⊤n k

2
F .

−

(cid:0)

(cid:1)

(5)

To prove the quadratic growth consequence under Assumption 1 (Eq. (4)), we
recall that it follows from Lemma 1 that X∗ is a rank-one matrix which corresponds
f (X∗) associated with the lowest eigenvalue. Thus, by
to the eigenvector of
(5) with X = unu⊤n = X∗ and δX = δ (where δ is as deﬁned in
invoking Eq.
Assumption 1), we indeed have that

∇

δ
2 k

Y

X∗

2
F ≤ h

k

Y

−

−

X∗,

∇

f (X∗)

i ≤

f (Y)

−

f (X∗),

where the last inequality follows from convexity.

Theorem 2 (formal version of Theorem 1). Let
Algorithm 1 and denote for all t

{
1: ht := f (Xt)

≥

f ∗. Then,

1 be a sequence produced by

Xt}t
−
ht = O (β/t) .

≥
1 :

t
∀

≥

Moreover, if Assumption 1 holds then there exists T0 = O ((β/δ)3) such that

t
∀

≥

T0 :

ht+1 ≤

ht

min

1

−

(cid:18)

n

δ
12β

,

1
2

.

o(cid:19)

Finally, if Assumption 1 holds then it also holds that

t
∀

≥

1 :

vtv⊤t −

k

X∗

k

2
F = O

β2
δ3 ht

,

(cid:19)

(cid:18)

where vt is the eigenvector computed in line 3 of the algorithm.

(6)

(7)

(8)

Proof. The ﬁrst part of the theorem (Eq. (6)) follows from standard results on the
convergence of Frank-Wolfe with line-search, see for instance [23].

To prove the second part (Eq. (7)), we note that using the quadratic growth

property (Eq. (4)), we have that for all t

1

≥

f (Xt)

f (X∗)

kF ≤

β

Xt −

k

X∗

kF ≤(a)

β

− ∇

k∇

2δ−

1ht =
(b)

p

O

β

δ−

1β/t

,

(cid:16)

p

(cid:17)

where (a) follows from Eq. (4) and (b) follows from the ﬁrst part of the theorem
(Eq. (6)). Thus, for some T0 = O((β/δ)3) we have that

t
∀

≥

T0 :

f (Xt)

k∇

− ∇

f (X∗)

kF ≤

δ
3

.

(9)

9

Let us denote the eigen-decomposition of

∇
where the eigenvalues are ordered in non-increasing order.
Weyl’s inequality for the eigenvalues we have that

∇

f (Xt) as

f (Xt) =

n
i=1 λiuiu⊤i ,
In particular, using

P

λn

1 −

−

λn = λn
−
+ (λn
λn

≥

δ

≥

−

1(

∇
1 −
−
1(
∇
2δ
3

f (X∗))
λn
1(
∇
−
f (X∗))
δ
3

=

.

−

∇

λn(

f (X∗))
−
f (X∗))) + (λn(
f (X∗))

λn(

−

∇

λn)

f (X∗))
2

−
f (Xt)

k∇

∇
−

f (X∗)

kF

− ∇

(10)

Let us now recall the Frank-Wolfe update on iteration t of the algorithm: Xt+1 ←
Xt+ηt(vtv⊤t −
1, we have that the FW linear subproblem admits
a unique optimal solution (the eigenvector un), and we can substitute vt with un,
i.e., set vt =

Xt). Since λn < λn

un.

−

Note that both line-search options in the algorithm imply that

±

η
∀

∈

[0, 1] : f (Xt+1)

f (Xt) + η

≤

= f (Xt) + η

vtv⊤t −
h
unu⊤n −

h

Xt),

f (Xt)

+

i

∇

Xt,

f (Xt)

+

i

∇

η2β
2 k

η2β
2 k

2
F

vtv⊤t −
unu⊤n −

Xtk
Xtk

2
F ,
(11)

·

).

·

where the ﬁrst inequality is due to the smoothness of f (

).

Now, subtracting f (X∗) from both sides and using Eq. (3) from Lemma 4 with

X = Y = Xt and gap δX = δ/3, we have that for all 0

η

≤

≤

δ
3β :

η

ht +

ht+1 ≤

1
(cid:18)
(cid:19)
where the last inequality follows from the convexity of f (

unu⊤n −

f (Xt)

Xt,

i ≤

∇

−

(cid:18)

h

3η2β
δ

−

η
(cid:18)

−

3η2β
δ

ht,

(cid:19)(cid:19)

We now consider two cases.

δ
12β

ht+1 ≤
1
have that ht+1 ≤
(cid:16)
(cid:17)

−

If

δ
6β ≤

6β we have that
ht. Otherwise, setting η = 1 and using the fact that δ > 6β we
1
2ht. Overall, we have that for all t

1, then setting η = δ

T0,

≥

(cid:19)
which proves the second part of the theorem (Eq. (7)).

ht+1 ≤

ht

1
(cid:18)

−

min

δ
12β

,

1
2 }

{

,

Finally, we turn to prove the third part of the theorem (Eq. (8)). Recall that
under Assumption 1, Lemma 1 implies that X∗ is a rank-one matrix corresponding
f (X∗) associated with the smallest eigenvalue. By applying
to the eigenvector of

∇

10

Eq. (3) from Lemma 4 with X = X∗, Y = vtv⊤t we have that

vtv⊤t −

k

X∗

2
F ≤

k

≤(a)

≤(b)

≤(c)

vtv⊤t −
vtv⊤t −

2
δ h
2
δ h
2β
δ k
2√2β
δ3/2 k

X∗,

∇

f (X∗)

i

X∗,

∇

f (X∗)

f (Xt)

i

− ∇

vtv⊤t −

X∗

kF k

X∗

XtkF

−

vtv⊤t −

X∗

kF

ht,

(12)

p
where (a) follows since by deﬁnition of vt we have that
(b) follows from the Cauchy-Schwarz inequality and the smoothness of f (
follows from the quadratic growth property (Eq. (4)).

vtv⊤t −

X∗,

∇

h

·

f (Xt)

0,
i ≤
), and (c)

Rearranging, we indeed get

vtv⊤t −

k

X∗

2
F ≤

k

8β2ht
δ3

.

(13)

Remark: Eﬃcient methods for leading eigenvector computations, as required by
Algorithm 1 and all other algorithms we consider in this work, do not produce an
accurate solution, but only an approximated leading eigenvector. However, since
accounting for these possible approximation errors in the convergence analysis is
straight-forward (see for instance [24, 23, 14]), for ease of presentation we assume
all such computations are accurate.

2.2 Some improvements to Theorem 2 under additional

structure of objective

We note that the dependence on δ in terms of number of iterations until entering
the regime of linear convergence (Eq. (7)) and the distance to the optimal rank-one
3). We now
solution (Eq. (8)) in Theorem 2, could be quite high (scales with δ−
show that for an important family of structured objective functions, namely those
captured by the following Assumption 2, this dependence can be improved without
changing Algorithm 1 and with only minor changes to the proof of Theorem 2.

: Sn

A
C

→
Sn.

Assumption 2. The function f (

·
Rp is a linear map, g : Rp

) is of the form f (X) = g(

, where
A
R is βg-smooth and αg-strongly convex, and

C, X
i

X) +

h

→

∈
We note that Assumption 2 is also an underlying assumption in [39] which, as
discussed in the related work section, studied linear convergence rates for proximal
gradient methods for the highly related problem of smooth convex minimization
with nuclear norm regularization.

In the following we let
kF .
2=1 kA
k

⊤x

Rp,

∈

x

k

maxx

denote the operator norm of the map

, i.e.,

A

kAk

=

kAk

11

Xt}t
{
1: ht := f (Xt)
−
3β3
g

Theorem 3. Let
t
exists T0 = O

kAk

≥

αg δ2

such that

1 be a sequence produced by Algorithm 1 and denote for all
f ∗. If both Assumption 1 and Assumption 2 hold, then there

≥

(cid:16)

(cid:17)

t
∀

≥

T0 :

ht+1 ≤

ht

min

1

−

(cid:18)

n

δ
12β

,

1
2

.

o(cid:19)

Moreover,

vtv⊤t −
where vt is the eigenvector computed in line 3 of the algorithm.

t
∀

X∗

1 :

≥

(cid:18)

(cid:19)

k

k

2
F = O

,

2

β2
g kAk
αgδ2 ht

(14)

(15)

In a nutshell, Theorem 3 replaces a factor of δ−

in the
constant T0 and the RHS of guarantee (8) in Theorem 2. In order to demonstrate
the possible improvement, consider the highly popular case in which the objective
b
function f (
is
k
a linear map. In this case we have g(z) := 1
2
2 and so αg = 1, while the
1
1 in favor of α−
eigen-gap δ can be arbitrarily small. Thus, replacing a factor of δ−
g
in the bounds, can be quite signiﬁcant.

) is a least-squares objective, i.e., f (X) = 1

1 with a factor of α−
g

2
2, where

b
k

2kA

2k

X

A

−

−

z

·

1

Proof. Under the additional structural assumption on f (
any X, Y

Sn,

), it clearly holds that for

·

∈
f (X)

k∇

f (Y)

kF =

⊤(

kA

− ∇

Using the strong convexity of g(

g(

X)

g(

Y))

A

− ∇

∇
), we have that for all X

A

kF ≤ kAk
∈ Sn,

·

βgkA

X

Y

k2.

− A

X

kA

− A

X∗

k2 ≤ s

=

s

2
αg

2
αg

X) +

(g(

A

h

C, X

i −

X∗)

g(

A

C, X∗i

)

− h

(f (X)

−

f (X∗)).

Thus, for any iteration t of Algorithm 1, it holds that

f (Xt)

k∇

− ∇

f (X∗)

kF ≤ s

2β2
g

2

kAk
αg

ht.

We can now plug-in Eq. (6) and further obtain that

f (Xt)

f (X∗)

kF = O

− ∇

k∇

kAk

3β3
g
αgt !

,

 s

(16)

(17)

where we have used the fact that the smoothness parameter of f is at most βgkAk
.
Now, we can see that in-order to obtain the bound (9) in the proof of Theorem
3β3
g
, which proves the ﬁrst part of the

kAk

2, it indeed suﬃces to take T0 = O
theorem.

αgδ2

(cid:16)

(cid:17)

12

To prove the second part, we observe that using (16), Eq. (12) in the proof of

Theorem 2 could now be replaced with:

vtv⊤t −

k

X∗

2
F ≤

k

≤

2
δ h
2
δ k

vtv⊤t −
vtv⊤t −

2
δ k

≤

vtv⊤t −

X∗

Rearranging, we get

X∗,

∇

f (X∗)

f (Xt)
i

− ∇

f (X∗)

f (Xt)

kF

− ∇

X∗

kF k∇
2

kF

s

kAk
αg

2β2
g

ht.

vtv⊤t −

k

X∗

2
F ≤

k

8β2

2

g kAk
αgδ2 ht.

2.2.1 Verifying Assumption 1 under Assumption 2

We now describe how under Assumption 2 one can obtain a practically veriﬁable
lower-bound for the parameter δ in Assumption 1 (provided that it is indeed greater
than zero).

Suppose Assumption 2 holds and let X∗ denote an optimal solution to Problem
(1). Combining Weyl’s inequality for the eigenvalues and Eq. (16) from the proof
1, the matrix Xt from Algorithm 1, satisﬁes
of Theorem 3, we have that for any t

≥

[n] :

i
∀

∈

λi(

|

∇

f (Xt))

λi(

∇

−

f (X∗))

2β2
g

2

kAk
αg

| ≤ s

(f (Xt)

−

f (X∗)).

This implies that

(λn

|

−

1(

f (X∗))

λn(

∇

−

f (X∗)))

(λn

1(

−

∇

−

f (Xt))

λn(

∇

−

f (Xt)))

| ≤

2β2
g

(f (Xt)

−

f (X∗)).

(18)

∇
2

2

s

≤

kAk
αg

Suppose now that Assumption 1 indeed holds with parameter δ > 0. Using Eq.
(18), once we arrive at an iteration t for which it holds that the RHS of (18) is smaller
f (Xt))3,
for instance than δ/3, by computing the eigenvalues λn
we can verify that Assumption 1 holds with parameter at least δ/3, and in particular
that there exists a unique optimal solution which is also rank-one.

f (Xt)), λn(

∇

∇

1(

−

Note that in order to verify that the RHS of (18) is indeed smaller than δ/3, it
f (X∗) with the simple upper-
suﬃces to replace the approximation error f (Xt)
, where vt is the eigenvector computed on iteration t
f (Xt)
bound
of Algorithm 14. Note that for Algorithm 1, it is known that the non-negative
f (Xt)
also converges to zero as a function of t, with rate
quantity
at least O(1/t) (without requiring Assumption 1) [23].

Xt −
Xt −

vtv⊤t ,

vtv⊤t ,

∇

∇

−

h

h

i

i

3extending this discussion to the case in which these eigenvalues are only approximated up to

suﬃcient precision is straightforward

4this quantity is known as the duality gap and it is indeed an upper-bound on the approximation

error since f (
·

) is convex, see [23]

13

2.3 Bounded-rank algorithm

Despite the linear convergence result for the Frank-Wolfe method detailed in The-
orem 2, still a certain disadvantage is that the rank of the iterates (or number
of rank-one components that needs to be stored in memory to maintain a factor-
ization of the current iterate Xt) grows linearly with the iteration counter t. We
now suggest a simple modiﬁcation, that actually combines the Frank-Wolfe method
and the projected gradient method, and guarantees that the number of rank-one
components is always bounded and is independent of 1/ǫ, where ǫ is the target
accuracy.

The main idea is to use the recent results in [16] which show that under Assump-
tion 1, in a ball of radius Θ(δ/β) around X∗, the projected gradient method, when
applied to Problem (1), will always produce iterates that are rank-one. Moreover,
whether the projection is indeed rank-one or not could be veriﬁed by examining the
ﬁrst and second leading eigenvalues of the corresponding matrix. This leads to an
algorithm that applies either conditional gradient steps or projected gradient steps
(when the projection is rank-one), until entering the above mentioned ball around
X∗. Once the iterates are inside the ball, it is guaranteed that only projected gra-
dient steps which result in a rank-one matrix will be used, and thus from this point
on, only a single rank-one matrix needs to be stored in memory.

This modiﬁcation comes with the price that now each iteration of the algorithm
(see Algorithm 2 below) requires, in worst case, a rank-two SVD computation of a
n

n matrix, and an additional one leading eigenvector computation.

×

Algorithm 2 Frank-Wolfe meets Projected Gradient for Problem (1)
1: input: smoothness parameter β
2: let X1 be an arbitrary point in
3: for t = 1 . . . do
Xt −
4: Yt+1 ←

f (Xt)

1
β ∇

Sn

let λ1u1u⊤1 + λ2u2u⊤2 be the rank-two truncated eigen-decomposition of Yt+1
(i.e., taking the two leading components with largest eigenvalues)
if λ1 ≥
else

1 + λ2 then
u1u⊤1

EV(

Xt+1 ←
vt ←
f (Xt))
−∇
choose step size ηt ∈
Option 1:

[0, 1] using one of the two options:

ηt ←

arg min
[0,1]
η
∈

f ((1

−

η)Xt + ηvtv⊤t )

5:

6:
7:
8:
9:
10:

Option 2:

ηt ←

arg min
[0,1]
η
∈

f (Xt) + η

vtv⊤t −

h

Xt,

∇

f (Xt)

+

i

Xt+1 ←

11:
end if
12:
13: end for

(1

−

ηt)Xt + ηtvtv⊤t

14

η2β
2 k

Xt −

vtv⊤t k

2
F

Theorem 4. The sequence
1 produced by Algorithm 2 has all the guarantees
stated in Theorem 2 (or Theorem 3 if Assumption 2 also holds). Moreover, there
exists T1 = O ((β/δ)3), such that for all t

T1 it holds that rank(Xt) = 1.

Xt}t

{

≥

≥

Proof. Note that according to the structure of the Euclidean projection over
Sn
(see for instance Lemma 6 in [16]), when the condition in the if statement (line 6 of
the algorithm) holds on some iteration t, then indeed the projection of Yt+1 onto
Sn is given by the rank-one matrix u1u⊤1 , and thus in this case Xt+1 is equivalent
f (Xt)].
to the standard projected gradient update step: Xt+1 ←
Thus, Algorithm 2 either applies a standard projected gradient update (when the
projection is rank-one), or otherwise a Frank-Wolfe update with line-search.
β−

f (Xt)] then, as it is well known, we have

n[Xt −

Π
S

β−

∇

1

1

In particular, if Xt+1 ←

Π
S

n[Xt −

∇

that for any Y

∈ Sn,

Xt,

f (Xt)

f (Xt+1)

≤

f (Xt) +

= f (Xt) +

f (Xt) +

≤

Xt+1 −
h
β
2 k
β
2 k

Xt+1 −
Y

−

∇
(Xt −
β−

(Xt −

1

f (Xt))
k

∇

β−

f (Xt))

1

∇

k

2
F

Xtk
1
2β k∇

2
F −
1
2β k∇

β
2 k

+

i

Xt+1 −

f (Xt)

2
F

k

f (Xt)

2
F

k

= f (Xt) +

Y

h

−

Xt,

∇

f (Xt)

+

i

β
2 k

Y

2
F −
Xtk

−

2
F .

In particular, for any ηt ∈
−∇

the leading eigenvector of

[0, 1], setting Y = (1
ft(Xt), we obtain

ηt)Xt + ηtvtv⊤t , with vt being

−

f (Xt+1)

f (Xt) + ηth

vtv⊤t −

≤

Xt,

∇

f (Xt)

+

i

η2
t β
2 k

Xt −

vtv⊤t k

2
F

(which is the same as Eq. (11) in the proof of Theorem 2).

Thus, a projected gradient update enjoys a per-iteration worst-case error reduc-
tion that is no worse than that of a Frank-Wolfe step with line-search (option 2 in
Algorithm 1). This implies that both the O(1/t) convergence rate and the linear
convergence rates for the sequence
1 guaranteed in Theorems 2 and 3, also
hold for Algorithm 2.

Xt}t

{

≥

In particular, using the quadratic growth property (Eq. (4)) together with the
O(β/t) convergence rate (Eq. (6)), we have that there exists T1 = O((β/δ)3) such
that for all t

T1,

≥

Xt −

k

X∗

k

2
F = O

β
δT1 (cid:19)

(cid:18)

δ
4β

2

.

(cid:19)

≤

(cid:18)

(19)

Thus, starting from iteration T1 and onwards, all iterates of the algorithm lie
inside the Euclidean ball of radius δ/(4β) around X∗. Invoking Theorem 7 in [16],
it is guaranteed that once the iterates are inside this ball, the projection of Yt+1
Sn is indeed rank-one, or equivalently, the condition on the eigenvalues in line
onto
6 of the algorithm, always holds.

15

2.4 No burn-in phase when gap is known

1(

f (X∗))

Another disadvantage of Theorems 2, 3 is that the linear convergence applies only
after a certain “burn-in” phase. Here we show that if an estimate for the eigen-gap
f (X∗)) is available, then it is possible to modify the
δ = λn
Frank-Wolfe method, without essentially changing the complexity of each iteration,
so that it enjoys a global linear convergence rate. This modiﬁcation and convergence
analysis follows in an almost straight-forward manner from the work [1], when
combined with the quadratic growth property (Lemma 4).

λn(

∇

∇

−

−

Algorithm 3 Regularized Frank-Wolfe for Problem (1)
smoothness parameter β, gap estimate ˆδ

(0, λn

1(

−

∇

∈

f (X∗))

−

1: input:
λn(

∇

f (X∗))]

Sn

1, ˆδ

2: let X1 be an arbitrary point in
3: η
min
2β }
←
{
4: for t = 1 . . . do
vt ←
f (Xt)
arg min
5:
=1h
∇
k
k
EV (
f (Xt) + ηβXt)
to vt ←
−∇
η)Xt + ηvtv⊤t
6: Xt+1 ←
(1
7: end for

vv⊤,

−

v

i
}

+ ηβ

2 k

vv⊤

Xtk

2
F {

−

note this is equivalent

Theorem 5. Under Assumption 1, the iterates of Algorithm 3 satisfy

t
∀

≥

1 :

f (Xt+1)

f ∗

−

min

1

−

≤  

1
2

,

{

ˆδ
4β }!

(f (Xt)

f ∗) .

−

As discussed, the proof is a simple application of the arguments used in [1] and

Lemma 4, however since it is very short, we include it here for completeness.

Proof. On any iteration t it holds that

f (Xt+1)

f ∗

−

≤(a)

f (Xt)

f ∗ + η

vtv⊤t −

h

Xt,

∇

f (Xt)

+

i

η2β
2 k

vtv⊤t −

Xtk

2
F

−

−

f (Xt)

f ∗ + η

X∗

h

−

Xt,

f (Xt)

η2β
2 k

+

i

X∗

2
F

Xtk

−

≤(b)

≤(c)

≤(d)

(f (Xt)

(f (Xt)

−

−

f ∗)

1
(cid:18)

−

η +

f ∗)

1

(cid:18)

η +

−

∇
η2β
δ
η2β
ˆδ (cid:19)

(cid:19)

,

(20)

where (a) follows from smoothness of f , (b) follows from the optimal choice of vt
and since, under Assumption 1, X∗ is rank-one, (c) follows from convexity of f (
)
and Eq. (4) in Lemma 4, and (d) follows since ˆδ

δ.

·

1, then plugging-in η = ˆδ

2β into the RHS of (20), we have that

≤

Now, if ˆδ

2β ≤

f (Xt+1)

f ∗

−

≤

(f (Xt)

−

f ∗)

1

ˆδ
4β !

.

−

16

 
Otherwise, we have that η = 1 and β
ˆδ
RHS of (20), we have that

< 1

2 . In this case, plugging-in η = 1 into the

f (Xt+1)

f ∗

−

≤

1
2

(f (Xt)

f ∗) .

−

Combining these two cases yields the theorem.

Remark:
It is possible to combine the use of the projected gradient method,
as applied in Algorithm 2, and the regularized Frank-Wolfe update, as applied
in Algorithm 3, to obtain an algorithm that has both bounded rank and global
linear convergence rate. This derivation is quite straightforward given these two
ingredients and we omit it.

3 Extension Motivated by Robust-PCA

We now consider the following extension of Problem (1).

min
n,y
∈S

X

∈K{

f (X, y) := g(

X + y) +

C, X
i

h

+

h

c, y

,

i}

A

where g : Rp
→
is a linear map,
Throughout this section we use D

K ⊂

R is assumed αg-strongly convex and βg-smooth,
Rp is assumed convex and compact, and C

to denote the Euclidean diameter of

A
∈

: Sn
Sn, c
.

For instance, the Robust-PCA problem [7, 28, 20]:

K

K

(21)

Rp
Rp.

→
∈

min
τ, Y
≤

Rm×n:

∈

Y

k

k

1
k

≤

1
2 k

X + Y

2
F ,

M
k

−

X

∗
k

k

X

Rm×n:

∈

Rm

n is some input matrix, and

where M
k·k1 is the standard entry-wise ℓ1 norm,
could be formulated as Problem (21) via standard transformations (see for instance
[24]).

∈

×

Another relevant example is that of phase retrieval with corrupted measure-
can be taken

ments, in which case the vector y accounts for the corruptions, and
to be some norm-induced ball (e.g., ℓ1 ball in case of sparse corruptions).

K

In the sequel, we let

fyf (X, Y) the derivative w.r.t. y. Also, as before, we denote

Xf (X, Y) denote the derivative of f w.r.t. the block
=

∇

kAk

X and
maxx

∇
Rp,
x
k

∈

2=1 kA
k

⊤x

kF .

Towards extending our results for Problem (1) to Problem (21), we consider
a standard ﬁrst-order method which combines the use of Frank-Wolfe with line-
search in order to update the matrix variable X (as done for Problem (1)) with the
standard projected gradient method for updating the variable y 5. See Algorithm
4.

5Here we make an implicit assumption that it is computationally eﬃcient to compute Euclidean

projections onto the set

.

K

17

arbitrary point in

Algorithm 4 Projected Gradient combined with Frank-Wolfe for Problem (21)
1: input: smoothness parameter βg
2: (X1, y1)
←
3: for t = 1 . . . do
yt+1 ←
4:
vt ←
ηt ←
7: Xt+1 ←
8: end for

1
[yt −
Π
2βg ∇
K
EV(
Xf (Xt, yt))
−∇
[0,1] f ((1
arg minη
∈
ηt)Xt + ηtvtv⊤t
(1

η)Xt + ηvtv⊤t , yt+1)

yf (Xt, yt)]

Sn × K

5:
6:

−

−

Working towards proving an analogue of Theorem 2 for Problem (21), we begin

by extending our underlying gap assumption to the new setting.

Lemma 5. Let
Then

∗
f (X, y) is constant over

⊂ Sn × K

W

∇

∗.

W

denote the set of optimal solutions to Problem (21).

Proof. Since g is strongly convex it follows that
that for any X, y,
Hence, it follows that indeed

A
A
f is constant over

Xf (X, y) =

g(

∇

∇

X+y)+C,

A

⊤

X + y is constant over
yf (X, y) =

g(

∗. Note
W
X+y)+c.

∇

A

∇

∇
∗.

W

Assumption 3. The gradient vector at every optimal solution (X∗, y∗) satisﬁes:
λn

Xf (X∗, y∗)) = δ > 0.6

Xf (X∗, y∗))

λn(

1(

−

∇

−

∇

Lemma 6. Under Assumption 3 there exists a unique optimal solution (X∗, y∗) to
Problem (21). Moreover, X∗ is rank-one, that is X∗ = x∗x∗⊤ for some unit vector
x∗

Rn.

∈

1(

∇

∇

q(X) =

∈
q(X∗))

arg minX
λn(

Xf (X, y∗) and X∗

Proof. Fix some optimal solution (X∗, y∗) and consider the function q(X) = f (X, y∗).
n q(X). Thus, according to
Clearly
∈S
q(X∗)) = δ > 0. Thus, by
Assumption 3 it follows that λn
∇
−
Lemma 1 it follows that X∗ is the unique minimizer of q(X) over
Sn, and more-
over, X∗ = x∗x∗⊤ is rank-one, where x∗ is the eigenvector which corresponds to
Xf (X∗)). However, by Lemma 5, the gradient
the eigenvalue λn(
vector of f (
) is constant over the optimal set, and hence, if there exists another
optimal solution (X∗2, y∗2) to Problem (21), by the above reasoning it must hold that
X∗2 = X∗ = x∗x∗⊤.
Now, since g(

) is strongly convex it follows that the vector

q(X∗)) = λn(

∇

∇

∇

−

·

·

,

X + y is con-
. Thus, for any two optimal solutions

A

·

stant over the optimal set
∗
W
(X∗, y∗1), (X∗, y∗2) we have that

⊆ Sn × K

y∗1 −

y∗2 = (

A

X∗ + y∗1)

(

A

−

X∗ + y∗2) = 0.

Hence, the lemma follows.

The proof of the following lemma follows essentially from the same arguments

used to derive Eq. (16) and thus we omit it.

6Recall that according to the previous lemma the gradient vector is constant over the set of
optimal solutions and thus, this is equivalent to assuming the eigen-gap holds for some optimal
solution.

18

Lemma 7. For any (X, y)
holds that

∈ Sn × K

and optimal solution (X∗, y∗)

∈ Sn × K

it

Xf (X, y)

k∇

− ∇

Xf (X∗, y∗)

kF ≤

√2βgkAk
√αg

p

f (X, y)

f ∗.

−

(22)

We can now ﬁnally present and prove our main result for Problem (21).

Theorem 6. Let
for all t

(Xt, yt)

}t
{
≥
1: ht := f (Xt, yt)
−

≥

1 be a sequence produced by Algorithm 4 and denote
f ∗. Then,

t
∀

≥

1 :

ht = O

(cid:18)

βg(

2 + D2
kAk
K
t

)

.

(23)

(cid:19)
2β3
g (
kAk
αg δ2

2+D2

K)

Moreover, under Assumption 3, there exists T0 = O

kAk

such that

t
∀

≥

T0:

(cid:16)

(cid:17)

ht+1 ≤

ht

min

1

−

1
6

,

{

αgδ2
2 + 4δ2 + 64

4βg

10αgδ

kAk

Finally, under Assumption 3, it also holds that

(cid:0)

4β2
g

kAk

(cid:1)

,

δ
72βgkAk

2 }!

.

(24)

t
∀

≥

1 :

vtv⊤t −

k

X∗

2
F = O

k

2

β2
g kAk
αgδ2 ht

(cid:19)
where vt is the eigenvector computed in line 5 of the algorithm.

(cid:18)

,

(25)

Proof. Fix some iteration t. By the optimal choice of ηt, we have that for any
ηX

[0, 1] it holds that

∈

f (Xt+1, yt+1)

f ((1

≤

We introduce the notation zt =
[0, 1] that

for any ηX

∈

A

ηX)Xt + ηXvtv⊤t , yt+1).

−
Xt + yt. Using the smoothness of g(

), it holds

·

f (Xt+1, yt+1)

≤
+

≤
+

h

Xt +

(ηX(vtv⊤t −
X)
(ηX(vtv⊤t −

g(
A
A
C, Xt + ηX(vtv⊤t −
h
g(zt) +
A
βg
(ηX(vtv⊤t −
2 kA
C, Xt + ηX(vtv⊤t −
h
f (Xt, yt) + ηX

Xt)) + yt + (yt+1 −
c, yt+1i
+
i
Xt)) + yt+1 −
ytk
c, yt+1i
fX(Xt, yt)
∇

Xt)) + yt+1 −
+
i
Xt,

yt

2
2

(cid:0)

(cid:1)

h

⊤

yt))

g(zt)

∇

+

X)
vtv⊤t −
h
yf (Xt, yt)
∇
2
Xtk
vtv⊤t −
F +
where in the last inequality we have used the triangle inequality for the Euclidean
norm and (a + b)2

≤
+ (yt+1 −
+ βg
kAk

yt+1 −

yt)⊤
2η2

2a2 + 2b2.

ytk

Xk

k

2
2

(cid:1)

(cid:0)

i

,

≤

19

 
From the choice of yt+1, it follows that for all ηy

[0, 1],

∈

yt)⊤
(yt+1 −
((yt + ηy(y∗
yt)⊤
ηy(y∗

−

∇

yt+1 −

yf (Xt, yt) + βgk
yt))
yt)⊤
−
yf (Xt, yt) + βgη2
yk

2
2 ≤
yf (Xt, yt) + βgk
2
y∗
2.
−

ytk

ytk

∇

∇

−

(yt + ηy(y∗

yt))

2
2 =

ytk

−

−

Combining both inequalities we have that for any (ηX, ηy)

[0, 1]

[0, 1],

×

∈
Xf (Xt, yt)

i

∇

Xt,

yt)⊤
2

f (Xt+1, yt+1)

f (Xt, yt) + ηX

≤
+ ηy(y∗
−
η2
+ βg
XkAk

vtv⊤t −
h
yf (Xt, yt)
∇
F + η2
2
Xtk
vtv⊤t −
yk
Now, part one of the Theorem (Eq. (23)) follows from setting the standard ob-
, and from here the O(1/t)
) and the fact

servation that
rate follows from standard arguments involving the convexity of f (
that

Sn,
We now continue to prove the second part of the theorem (Eq. (24)). Note
that from Lemma 7 and the ﬁrst part of the theorem, it follows that there exists
T0 = O

are bounded.

Xf (Xt, yt)

Xf (Xt, yt)

such that

vtv⊤t ,

ytk

i ≤ h

(26)

X∗,

2+D2

y∗

kAk

∇

∇

−

K

K)

k

2
2

(cid:1)

(cid:0)

h

i

·

.

2β3
g (
kAk
αgδ2

(cid:16)

(cid:17)

t
∀

≥

T0 :

Xf (Xt, yt)

k∇

− ∇

Xf (X∗, y∗)

δ
3

.

kF ≤

Throughout the rest of the proof we focus on some iteration t

(27)

T0. Denote

≥

z∗ =

X∗ + y∗.
Observe that

A

y∗

k

ytk2 =

−

(z∗

k

− A

X∗)

−

≤ s

2
αg

p

f (Xt, yt)

Xt)

(zt − A
f ∗ +

−

k2 ≤ k
X∗

kAkk

z∗

−

ztk2 +

−
XtkF ,

X∗

Xtk2

− A

kA

(28)

where the last inequality follows from the strong convexity of g(

Let us write the eigen-decomposition of

Xf (Xt, yt) as

).
Xf (Xt, yt) =

·

∇

where the eigenvalues are ordered in non-increasing order. We now observe that
Xf (Xt, yt), and since by Lemma 6, X∗ is
since un is the leading eigenvector of
Xf (X∗, y∗),
a rank-one matrix which corresponds to the leading eigenvector of
then under the gap assumption (Assumption 3), and using the Davis-Kahan sin θ
theorem (see for instance Theorem 4 in [18]), we have that

−∇

−∇

P

∇

n
i=1 λiuiu⊤i ,

k

−

X∗

XtkF ≤ k

unu⊤n −
unu⊤n −
Using Lemma 7 we have

≤ k

unu⊤n −
XtkF +
k
XtkF + 2√2 k∇

X∗

kF
Xf (Xt, yt)

Xf (X∗, y∗)

kF

.

(29)

− ∇
δ

X∗

k

XtkF ≤ k

unu⊤n −

XtkF + C0

−

f (Xt, yt)

f ∗.

−

p

20

for C0 = 4

βg
√αg δ .

kAk

Plugging into (28) and using (a + b)2

2a2 + 2b2, we have that

k

−

y∗

2
2 ≤

ytk
2 and C2 = 4
αg

C1k

+ 4

unu⊤n −
2C 2

kAk

≤
Xtk
0 = 4
αg

+ 64

4β2
g

.

kAk
αgδ2

2
F + C2 (f (Xt, yt)

f ∗) ,

−

(30)

for C1 = 4

kAk
Note that since t

≥

T0, using (27), similarly to (10), it follows that λn

δ
3 .
Hence, the FW linear subproblem admits a unique optimal solution, and we can
substitute vt with un — the leading eigenvector of
f (Xt, yt). Recall also that
−∇
2
u⊤n Xtun). Thus, plugging-back into (26), we have that for
Xtk
unu⊤n −
F ≤
k
any (ηX, ηy)
∈

−
[0, 1],

λn ≥

[0, 1]

1−

2(1

×

−

f (Xt+1, yt+1)

f (Xt, yt) + ηX

≤
+ 2βg(1
+ ηy(y∗

−

−

h

Xt,
unu⊤n −
∇
X + η2
2η2
kAk
yf (Xt, yt) + βgη2

Xf (Xt, yt)
yC1)
yC2 (f (Xt, yt)

i

u⊤n Xtun)(
yt)⊤

∇

f ∗) .

−

(31)

We now consider two cases. If (1

f ∗), for some
C3 > 0 to be determined later on, then, letting ηX = ηy = η and using the convexity
of f (

), we have that for any η

[0, 1] it holds that

C3(f (Xt, yt)

u⊤n Xtun)

≤

−

−

,

·

·

f (Xt+1, yt+1)

f ∗

−

∈
(f (Xt, yt)
≤
= (f (Xt, yt)

f ∗)

1

f ∗)

1
(cid:0)

−

−

−

−

η + βgη2(2
η + η2βgC4

kAk
,

2C3 + 2C1C3 + C2)

(cid:1)

.

where we deﬁne C4 = 2

If 2βgC4 > 1, then taking η = 1

(cid:0)

kAk

2C3 + 2C1C3 + C2 = 10
2βg C4 , we get

(cid:1)
2C3 + 4
αg

kAk

4β2
g

+ 64

kAk
αg δ2

ht+1 ≤

ht

1
(cid:18)

−

1
4βgC4 (cid:19)

.

Else, taking η = 1 (and recalling that 1/2

βgC4) we obtain

≥
ht
2

.

ht+1 ≤

In the second case ((1

−

u⊤n Xtun) > C3ht), setting ηy = 0 in (31) we have

f (Xt+1, yt+1)

f ∗

−

≤
+ 2η2

f (Xt, yt)
−
2(1
XβgkAk

f ∗ + ηX

unu⊤n −

Xt,

∇

Xf (Xt, yt)

i

h
u⊤n Xtun).

−
Using Eq. (3) from Lemma 4 w.r.t. the function w(X) := f (X, yt) and with Y =
δ
3

X = Xt, and recalling that according to Eq. (27), λn
(see similar calculation in (10)), we have that

w(Xt))

w(Xt))

λn(

∇

∇

1(

−

≥

−

f (Xt+1, yt+1)

f ∗

−

≤

f (Xt, yt)

f ∗

−

−

ηX(1

−

u⊤n Xtun)

δ
3 −

(cid:18)

2ηXβgkAk

2

.

(cid:19)

21

Thus, for any ηX

6βg

≤

δ

2 (recalling (1

kAk

−

u⊤n Xtun) > C3ht) we have that

f (Xt+1, yt+1)

f ∗

−

≤

f (Xt, yt)

f ∗

−

−

In particular, if

δ
12βg

2
kAk

≤

1, setting ηX =

ηX

(cid:18)
δ
12βg

kAk

δ
3 −

2ηXβgkAk

2

C3ht.

(cid:19)

2 we obtain

f (Xt+1, yt+1)

f ∗

−

≤

f (Xt, yt)

f ∗

−

Else, setting ηX = 1 (and recalling δ/6

2βgkAk

≥

2 .

−

δ2C3ht
72βgkAk
2) we have

f (Xt+1, yt+1)

f ∗

−

≤

f (Xt, yt)

f ∗

−

−

C3δht
6

.

Thus, considering all four cases, we have that

ht+1 ≤

ht

(cid:18)

min

1

−

1
2

,

{

1
4βgC4

,

min

−

1
2

,

{

4βg

10

= ht 
1


2 ,

C3δ2
72βgkAk
1

C3δ

6 }

(cid:19)

2C3 + 4δ2+64
αgδ2

kAk

4β2
g

kAk

Choosing for instance C3 = 1/δ we get

,

C3δ2
72βgkAk

2 ,

C3δ
6 }

.



(cid:17)

(cid:16)

(cid:0)

ht+1 ≤

ht

1

min
{

−

1
6

,

4βg

10αgδ

kAk

αgδ2
2 + 4δ2 + 64

,

δ
72βgkAk

.

2 }!

4β2
g

kAk

Finally, we turn to prove the third part of the theorem (Eq. (25)). Applying the
Davis-Kahan sin θ theorem in the same way as in the derivation of Eq. (29) above,
and recalling that w.l.o.g. vt = un, we have that

(cid:1)

vtv⊤t −

k

X∗

2
F ≤

k

8 k∇

Xf (Xt, yt)

Xf (X∗, y∗)
k

2
F

− ∇
δ2

16β2

g kAk
αgδ2

2

ht,

≤

were the last inequality follows from plugging-in the bound in Lemma 7.

3.0.1 Verifying Assumption 3

Similarly to Section 2.2.1, for Problem (21) we can also suggest a simple procedure
for practical veriﬁcation of a lower-bound for the parameter δ in Assumption 3
(provided it is greater than zero).

If we let (X∗, y∗) denote an optimal solution to Problem (21), then combining
1, the

Weyl’s inequality for the eigenvalues and Lemma 7, we have that for any t
pair (Xt, yt) from Algorithm 4, satisﬁes for all i

[n]:

≥

∈

λi(

|

∇

Xf (Xt, yt))

λi(

∇

−

Xf (X∗, y∗))

| ≤ s

2β2
g

2

kAk
αg

(f (Xt, yt)

−

f (X∗, y∗)).

22

 
Using the short notation gap(X, y) = λn
above implies that

1(

−

∇

Xf (X, y))

λn(

∇

−

Xf (X, y)), the

gap(X∗, y∗)

|

−

gap(Xt, yt)

2

s

| ≤

2β2
g

2

kAk
αg

(f (Xt, yt)

−

f (X∗, y∗)).

(32)

Thus, as discussed in Section 2.2.1, if Assumption 3 indeed holds with parameter
δ > 0, then using Eq. (32), once we arrive at an iteration t for which it holds that
the RHS of (32) is smaller for instance than δ/3, by computing the eigenvalues
Xf (Xt, yt)), we can verify that Assumption 3 holds with
λn
parameter at least δ/3, and in particular that there exists a unique optimal solution
and that its low-rank matrix component is indeed rank-one.

Xf (Xt, yt)), λn(

∇

∇

1(

−

Finally, note that since f is convex, we have that

f (Xt, yt)

−

f (X∗, y∗)

≤ h

Xt −
Xt −
≤ h
yt,
+
∇
h

X∗,
∇
vtv⊤t ,
yf (Xt, yt)

∇

Xf (Xt, yt)

+
i
Xf (Xt, yt)
i
u,
min
∈Kh
u

i −

h

y∗,

yt −

∇

yf (Xt, yt)

i

yf (Xt, yt)

,

i

∇

(33)

where vt is the eigenvector computed on iteration t of Algorithm 4.

Thus, when linear minimization over

is eﬃcient, we can upper-bound the
approximation error in the RHS of (32) with the RHS of (33) which is eﬃcient to
compute.

K

4 Extension to Nonsmooth Functions

We now consider an extension of our results to the case in which f (
) is convex
over Sn but not smooth. For instance, as an example, two applications of interest
in the context of rank-one matrix recovery are f (X) :=
k1, which is also
k
a popular formulation of the Robust-PCA problem (here M is the observed data),
and f (X) := 1
k1, which is useful when attempting to recover
a matrix X that is both low-rank and sparse from the noisy observation M (e.g.,
[34, 19]).

2
F + λ

M
k

M

2k

X

X

X

−

−

k

·

Towards this end, we recall the following suﬃcient and necessary optimality

condition for constrained nonsmooth convex optimization.

Lemma 8 (Corollary 3.68 in [2]). X∗
when f is nonsmooth) if and only if there exists G∗

∈ Sn is an optimal solution of (1) (even
∂f (X∗) such that

∈

h
The following assumption extends Assumption 1 to nonsmooth functions.

i ≥

−

∀

X

X∗, G∗

0.

(34)

X

∈ Sn :

Assumption 4. There exists an optimal solution X∗ to Problem (1) such that
) at X∗ for which Eq.
1(G∗)
λn
−
(34) holds.

λn(G∗) = δ > 0, where G∗ is a subgradient of f (

−

·

23

u∗nu∗⊤n −

h

i

Lemma 9. Suppose Assumption 4 holds for some optimal solution X∗
∈ Sn. Then,
X∗ is both the unique optimal solution to Problem (1), and rank-one. Moreover,
Problem (1) has the quadratic growth property

X

∀

∈ Sn :
) is nonsmooth.

even though f (

·

X

k

−

X∗

2
F ≤

k

2
δ

(f (X)

f ∗)) ,

−

Proof. From Lemma 8 if follows that under Assumption 4, X∗ must be the unique
rank-one matrix corresponding to the eigenvector of G∗ with smallest eigenvalue
(where G∗ is the subgradeint deﬁned in Lemma 8), since otherwise, letting u∗n
denote the eigenvector of G∗ corresponding to the smallest eigenvalue, we will have
that

< 0, which contradicts the optimality of X∗.

X∗, G∗

Using the above, the quadratic growth property follows from repeating the steps

of the proof of Eq. (4) in Lemma 4, replacing

f (X∗) with G∗.

∇

Towards applying Frank-Wolfe-type methods to Problem (1) with nonsmooth
) with a

f , we will consider a standard approach of replacing the nonsmooth f (
smooth approximation.

·

Deﬁnition 1. We say a convex function f(α,β) : Sn
imation of a convex function f : Sn
α, and ii) f(α,β) is β-smooth.

→

R, if i) for all X

→

R is an (α, β)-smooth approx-

Sn:

∈

f (X)

|

−

f(α,β)(X)

| ≤

We refer the interested reader to [4] for an in-depth treatment of the subject of

constructing smooth approximations with many important examples.

We note that typically β scales with 1/α. In particular, usually α is chosen so
that α = O(ǫ), where ǫ is the target approximation-error desired, which causes β to
be of the order β = O(1/ǫ). Note however that since, as discussed, the smoothness
parameter will typically scale with 1/ǫ, the results in Theorems 2 and 3, when
applied to the smooth approximation f(α,β), give fast convergence rates only after
roughly O(β3) = O(1/ǫ3) initial iterations. Since applying the standard Frank-
) will already result in a O(1/ǫ2) rate, these
Wolfe convergence result to f(α,β)(
fast rate results become meaningless. We thus consider only adapting the result of
Theorem 5, which does not have a “burn-in” phase, but does require an estimate
of the gap δ.

·

Theorem 7. Under Assumption 4, the iterates of Algorithm 3, when applied to an
(α, β)-smooth approximation f(α,β) of f , and with gap estimate ˆδ such that 0 < ˆδ
δ
(where δ is as deﬁned in Assumption 4), satisfy

≤

t
∀

≥

0 :

f (Xt+1)

f ∗

−

≤

(f (X1)

−

f ∗) exp

min
{

−

1
2

,

ˆδ
4β }

(cid:16)

t

+ O(α).

(cid:17)

Indeed, we see that in the typical case ˆδ

1
2, and when α = O(ǫ), β = O(1/ǫ),
the number of iterations to reach O(ǫ) approximation error is of the order O
,
which up to a log 1/ǫ factor, is what we expect when optimizing a nonsmooth ˆδ-
(cid:17)
strongly convex function.

log 1/ǫ
ˆδǫ

4β ≤

(cid:16)

24

Proof. The proof follows from simple modiﬁcations of the proof of Theorem 5, as
n f(α,β)(X). On any iteration t it holds
we now detail. Let us denote f ∗(α,β) = minX
that

∈S

f(α,β)(Xt+1)

f ∗(α,β) ≤(a)

−

f(α,β)(Xt)

f ∗(α,β) + η

vtv⊤t −

h

Xt,

f(α,β)(Xt)
i

∇

−

+

≤(b)

+

≤(c)

+

η2β
2 k
f(α,β)(Xt)

vtv⊤t −
−

2
F

Xtk
f ∗(α,β) + η

X∗

h

−

Xt,

∇

f(α,β)(Xt)

i

X∗

η2β
2 k
−
f(α,β)(Xt)

2
F

Xtk
f ∗(α,β) −
−

η

f(α,β)(Xt)

f(α,β)(X∗)

−

η2β
ˆδ

(f (Xt)

−

f (X∗)) ,

(cid:0)

(cid:1)

where (a) follows from the β-smoothness of f(α,β), (b) follows from the optimal
choice of vt and since, under Assumption 4, X∗ is rank-one, and (c) follows from
the convexity of f(α,β), Lemma 9, and since ˆδ
Let Z∗ be the minimizer of f(α,β) over
2α and f (X∗)

Sn. Since f ∗(α,β) = f(α,β)(Z∗)
≥
f ∗(α,β) −
−
≥

f (Z∗)
≥
α, the above

f(α,β)(X∗)

f(α,β)(X∗)

≥

−

≥

≤

−

δ.

α

α

α

f (X∗)
−
leads to

f(α,β)(Xt+1)

f ∗(α,β) ≤

−

f(α,β)(Xt)

f ∗(α,β)

−

+ α(2η + 2η2β/ˆδ).

(cid:0)

η +

1

−

(cid:1) (cid:16)

η2β
ˆδ

(cid:17)

Plugging-in the value of η and considering the two possible cases as in the proof of
Theorem 5, we have

f(α,β)(Xt+1)

f ∗(α,β) ≤

−

f(α,β)(Xt)

f ∗(α,β)

(cid:0)

+ 3α min
{

1,

−
ˆδ
2β }

(cid:1) (cid:16)

.

min

1

−

1
2

,

{

ˆδ
4β }

(cid:17)

x
Unrolling the recursion, using 1
converging geometric series, we get

−

≤

e−

x, and the formula for the sum of an inﬁnite

f(α,β)(Xt+1)

f ∗(α,β) ≤

−

Finally, replacing f(α,β)(

f(α,β)(X1)

f ∗(α,β)

exp

−

(cid:0)
) with f (

·

·

(cid:16)
), we have that

(cid:1)

min

1
2

,

ˆδ
4β }

{

t

+ O(α).

(cid:17)

−

f (Xt+1)

f ∗

−

≤

(f (X1)

−

f ∗) exp

min

1
2

,

ˆδ
4β }

{

−

(cid:16)

t

+ O(α).

(cid:17)

25

5 Numerical Experiments

In this section we bring empirical evidence in support of our main assumption, As-
sumption 1 (and the closely-related Assumption 3), and some empirical comparison
between the various methods considered in this work.

5.1 Empirical evidence for gap assumption

We consider two tasks, one of recovering a rank-one matrix from quadratic mea-
surements, a problem closely related to phase-retrieval (for which the underlying
assumption is Assumption 1), and rank-one robust PCA (for which the underly-
ing assumption is Assumption 3).
In both cases we construct synthetic random
instances of the problems and demonstrate that i) the proposed models indeed re-
cover the ground-truth signal with low error, and ii) the data indeed satisfy the gap
assumption.

(ai, bi)

Rank-one recovery from quadratic measurements: We let x0 = √nv0,
Rn is a random unit vector, and we draw m pairs of random unit
where v0 ∈
Rn. The vector of quadratic measurements of x0 is
vectors
×
}
Rm, and the observed noisy vector is given by
given by y0(i) = a⊤i x0x⊤0 bi, y0 ∈
Rm is a vector with standard Gaussian entries. The goal
y = y0 + √cn, where n
is to recover the rank-one matrix x0x⊤0 from the noisy measurements vector y, and
towards this we consider the problem

m
i=1 ⊂

Rn

∈

{

min
0 Tr(X)=τ{

X

(cid:23)

f (X) :=

1
2

m

i=1
X

(cid:0)

a⊤i Xbi −

y(i)

2

.

}

(cid:1)

(35)

−∇

We solve Problem (35) to high accuracy (approximation error w.r.t.

function
value less than 1e-12 in our MATLAB implementation7) using the standard Frank-
Wolfe method (Algorithm 1), and we denote the found solution by X∗. We produce
our estimate for the ground-truth vector x0 by computing the leading eigenvector
f (X∗), which we denote by v∗, and scaling it to have the same
of the matrix
norm as x0, i.e., we take the vector √nv∗. Note that estimation based on the
eigenvector v∗ is motivated by Eq.
(8) (in particular, since X∗ is only a high-
accuracy approximated solution, it may not be rank-one). We measure the relative
x0x⊤0 k
recovery error by
In our experiments we set m = 20n, τ = 0.5n, and the noise parameter c is set
to either 0.5 or 1.5. We note that we choose the trace bound τ strictly smaller than
Tr(x0x⊤0 ) = n, since otherwise the optimal solution will naturally also ﬁt some of
the noise and will result in a higher-rank matrix. All results are averaged over 20
i.i.d runs. The results are presented in Table 2.

x0x⊤0 k

x0x⊤0 k

F = 1
2
n2

nv∗v∗⊤

nv∗v∗⊤

2
F /

2
F .

−

−

k

k

k

As it can be seen in Table 2, all random instances indeed satisfy Assumption 1
with substantial eigen-gap. Moreover, the gap does not vary much with the dimen-
sion. We note that even though X∗ is only a high-accuracy approximated solution to

7the bound on the approximation error is veriﬁed by computing the duality gap, which is an

upper-bound on the approximation error w.r.t. the function value (see for instance [23])

26

noise level (c)

dimension (n)

avg. recovery error min/avg. gap in

f (X∗)

∇

0.5

0.5

0.5

0.5

1.5

1.5

1.5

1.5

100

200

400

600

100

200

400

600

0.0638

0.0621

0.0625

0.0623

0.1146

0.1129

0.1142

0.1143

2.9730 / 4.5488

3.7889 / 4.3656

3.8897 / 4.3656

3.9671 / 4.3927

1.1551 / 2.3836

1.5993 / 1.9936

1.4172 / 1.9756

1.1452 / 1.9320

avg. SNR

1.9931

1.9935

2.0053

2.0141

0.6736

0.6735

0.6547

0.6582

for Problem (35).
Results
Table 2:
2
x0x⊤0 k
nv∗v∗⊤
x0x⊤0 k
F /
k
k
f (X∗) is given by λn
1(
∇
∇
−
2/
y0k
√cn
(SNR) is given by
k

f (X∗))
2.

−

−

k

k

λn(

∇

2
F (v∗ is leading eigenvector of

The recovery error

is given by
f (X∗)), the gap in
f (X∗)), and the signal-to-noise ratio

−∇

(35) (approximation error < 1e-12), since Problem (35) satisﬁes Assumption 2 (i.e.,
X) with g(x) = 1
2
it can be written as f (X) = g(
2, and so αg = βg = 1), a
simple calculation using Eq. (18), and recalling that ai, bi, i = 1, . . . , m are all unit
vectors, implies that the eigen-gap estimates in Table 2 represent, up to negligible
error, the eigen-gaps in the gradient vector at the exact optimal solution. Using
Lemma 1, this in turn veriﬁes that for all random instances it holds that there is a
unique optimal solution and that it is indeed rank-one.

2k

A

−

x

y

k

Rank-one Robust PCA: We consider the task of extracting a rank-one matrix
from its sparsely-corrupted observation. We let M = x0x⊤0 + 1
2 (Y0 + Y⊤0 ), where
Rn is a random unit vector (x0x⊤0 is the rank-one matrix to recover), and Y0 is
x0 ∈
1 with probability p and zero otherwise
sparse, with each entry being either 1 or
(p << 1).Towards recovering X0 = x0x⊤0 , we consider the optimization problem

−

min

0 Tr(X)=τ, Y:

X

(cid:23)

Y

k

1
k

≤

s{

f (X, Y) :=

1
2 k

X + Y

M
k

2
F }

.

−

(36)

Similarly to the previous example, we solve Problem (36) to high accuracy (ap-
proximation error w.r.t.
function value less than 1e-12) using our Algorithm 4,
and we denote by (X∗, Y∗) the obtained solution. As before, since X∗ may not be
rank-one, we produce our estimate for the ground-truth matrix X0 by taking the
Xf (X∗, y∗) (note this is
matrix v∗v∗⊤, where v∗ is the leading eigenvector of
2
motivated by Eq. (25)), and we measure the recovery error by
F . In
k
1
2(Y0 + Y⊤0 )
all experiments we set s = 0.97
k1, τ = 0.7, and the noise sampling
probability p is either 1/√25n or 1/√n. All results are averaged over 20 i.i.d runs.
The results are presented in Table 3.

X0k

v∗v∗⊤

−∇

· k

−

As in the previous example, Table 2 shows that all random instances indeed
satisfy Assumption 3 with substantial gap, and the gap does not change drastically
with the dimension. Here also we note that even though (X∗, y∗) is only a high-
accuracy approximated solution (approximation error < 1e-12), a simple calculation

27

dimension (n)

avg. recovery error min/avg. gap in

Xf (X∗)

∇

noise prob (p)
1/√25n
1/√25n
1/√25n
1/√25n
1/√25n

1/√n

1/√n

1/√n

1/√n

1/√n

100

200

400

600

1000

100

200

400

600

1000

0.0026

0.0028

0.0040

0.0046

0.0058

0.0153

0.0178

0.0216

0.0260

0.0323

0.2017 / 0.2179

0.2091 / 0.2169

0.1995 / 0.2056

0.1966 / 0.2010

0.1850 / 0.1888

0.0996 / 0.1177

0.0956 / 0.1080

0.0793 / 0.0953

0.0724 / 0.0792

0.0484 / 0.0599

avg. SNR

0.0098

0.0035

0.0012

6.7945e-04

3.1631e-04

0.0020

7.0348e-04

2.4988e-04

1.3607e-04

6.3072e-05

Table 3: Results for Problem (36). The recovery error is given by
(v∗ is leading eigenvector of
λn
1(
∇
x0x⊤0 k

2
x0x⊤0 k
F
−
Xf (X∗) is given by
Xf (X∗)), and the signal-to-noise ratio (SNR) is given by
2
F .

Xf (X∗))
λn(
∇
−
1
2
2(Y0 + Y⊤0 )
F /
k

Xf (X∗, y∗)), the gap in

v∗v∗⊤

−∇

∇

k

k

k

−

using Eq. (32) implies that the eigen-gap estimates in Table 2 represent, up to
negligible error, the eigen-gaps in the gradient vector at the exact optimal solution.
Using Lemma 6, this veriﬁes that for all random instances it holds that there is
a unique optimal solution pair, and that the low-rank matrix component of it is
indeed rank-one.

5.2 Comparison of Frank-Wolfe variants

We turn to present preliminary empirical comparison between four Frank-Wolfe
variants presented, on the rank-one recovery from quadratic measurements task —
Problem (35), ﬁxing the dimension to n = 200 and setting the noise parameter c to
either 0.5 or 1.5. The tested algorithms are detailed in Table 4.

algorithm

FW-ls(opt1)

FW-ls(opt2)

FWPG

RegFW-ls(opt1)

description

Frank-Wolfe with exact line search (Algorithm 1 with option 1)

Frank-Wolfe with line-search over quadratic upper-bound (Algorithm 1 with op-
tion 2)

Frank-Wolfe + projected gradient steps (Algorithm 2 with option 1)
Regularized Frank-Wolfe (Algorithm 3). After computing the eigenvector vt on
each iteration t, the step-size is set via exact line-search (similarly to option 1 in
Algorithm 1). This does not change the theoretical convergence guarantees but
signiﬁcantly improves the convergence in practice. The gap estimate ˆδ which the
algorithm requires is taken from Table 2: we set ˆδ = 3 when c = 0.5 and ˆδ = 1
when c = 1.5

Table 4: Description of Frank-Wolfe variants used in the numerical comparison.

28

Since all variants except for FW-ls(opt1) rely on the smoothness parameter β,
we try several values. We begin with β = √n = √200 and observe that this
choice seems quite conservative, and thus we also try β = 1 and β = 0.1. All
algorithms are initialized with the same matrix which is generated as follows: we
Rn to be a random unit-norm vector. We then set the initialization to
pick x
8. Note that X1 simply corresponds to
X1 ←
·
xx⊤) and returning the corresponding
computing the leading eigenvector of
rank-one matrix scaled by τ . The results are the average of 20 i.i.d runs.

∈
arg minY

0,Tr(Y)=τ h

xx⊤)

i
f (τ

−∇

f (τ

Y,

∇

(cid:23)

·

The results are given in Figure 1. We see that the variants RegFW-ls(opt1)
and FWPG can indeed be faster than standard Frank-Wolfe with line-search (FW-
ls(opt1) and FW-ls(opt2)) when tuned properly. In particular, for β = 0.1 which
gives the best results for all variants, we see that FWPG has the fastest convergence
(with either c = 0.5 or c = 1.5).
Importantly, when examining the rank of the
iterates of FWPG, we observe that in all cases except for (c = 1.5, β = √200) and
(c = 1.5, β = 1), our initialization already starts FWPG in the regime in which
only projected gradient steps are used which means that FWPG only maintains a
rank-one matrix throughout the run, as opposed to all other variants.

A Proof of Lemma 2

The lemma is an adaptation of Lemma 3 in [16] (which considers optimization
over trace-norm balls). We restate and prove a slightly more general version of the
lemma.

Lemma 10. Let f : Sn
→
solution of rank r to the optimization problem minX
the eigenvalues of
It holds that

∈ Sn be an optimal
n f (X). Let λ1, . . . , λn denote
∈S
f (X∗) in non-increasing order. Let ζ be a non-negative scalar.

R be β-smooth and convex. Let X∗

∇

rank(Π(1+ζ)
S

n[X∗

1

β−

f (X∗)]) > r

ζ > rβ(λn

λn),

r −

−

−

∇
(1 + ζ)X

where (1 + ζ)
projection onto the convex set (1 + ζ)

Sn =

{

|

X

∈ Sn}
Sn.

⇐⇒
, and Π(1+ζ)
S

] denotes the Euclidean

n[
·

r
i=1 λ∗i viv⊤i . It follows
Proof. Let us write the eigen-decomposition of X∗ as X∗ =
from the optimality of X∗ that for all i
f (X∗)
∈
which corresponds to the smallest eigenvalue λn (see Lemma 7 in [16]). Thus, if
we let ρ1, . . . , ρn denote the eigenvalues (in non-increasing order) of Y := X∗
β−

[r], vi is also an eigenvector of

f (X∗), it holds that

P

∇

−

1

∇

[r] :
∈
i > r :

i
∀
∀

ρi = λ∗i −
ρi = λ∗i −

β−
β−

1λn;
1λn

−

i+1.

Recall that

r
i=1 λ∗i = 1 and λ∗r+1 = 0.

8we note this is a common initialization for Frank-Wolfe, and actually is equivalent to ini-
xx⊤, and running for one iteration with the classical step-size rule

P

tializing Frank-Wolfe with τ
ηt = 2
t+1

·

29

1490

1480

e
u
a
v

l

.
c
n
u

f

1470

1460

1450

1440

1430

0

e
u
a
v

l

.
c
n
u
f

e
u
a
v

l

.
c
n
u
f

1500

1490

1480

1470

1460

1450

1440

1430

0

1510

1500

1490

1480

1470

1460

1450

1440

0

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

20

40

60

80

100

120

140

160

180

200

iteration

e
u
a
v

l

.
c
n
u

f

3440

3430

3420

3410

3400

3390

3380

3370

0

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

20

40

60

80

100

120

140

160

180

200

iteration

(a) c = 0.5, β = √200

(b) c = 1.5, β = √200

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

5

10

15

20

25

30

iteration

(c) c = 0.5, β = 1

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

5

10

15

20

25

30

iteration

e
u
a
v

l

.
c
n
u
f

e
u
a
v

l

.
c
n
u
f

3470

3460

3450

3440

3430

3420

3410

3400

3390

0

3470

3460

3450

3440

3430

3420

3410

0

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

5

10

15

20

25

30

iteration

(d) c = 1.5, β = 1

FW-ls(opt1)
FW-ls(opt2)
RegFW-ls(opt1)
FWPG

5

10

15

20

25

30

iteration

(e) c = 0.5, β = 0.1

(f) c = 1.5, β = 0.1

Figure 1: Comparison of Frank-Wolfe variants for rank-one matrix recovery from
quadratic measurements (Problem (35)).

30

 
 
 
 
 
 
P

where σ

It is well known that for any matrix M
n
i=1 σiuiu⊤i , the projection of M onto the set (1 + ζ)

∈

Sn with eigen-decomposition M =
0 is given by

Π(1+ζ)
S

n[M] =

max

0, σi −

{

σ

}

n

i=1
X

Sn, for any ζ
uiu⊤i ,

≥

R is the unique scalar such that
n[Y])

Now, we can see that rank(Π(1+ζ)
S

∈

Thus, if rank(Π(1+ζ)
S
that

n[Y])

≤

n
= 1 + ζ.
0, σi −
i=1 max
}
{
ρr+1 =
r if and only if σ
≥
1λn
β−

1λn

β−
r.
r which implies

−

σ

−

≤
P

≥ −

−

r then it must hold that σ

1 + ζ =

=

n

i=1
X
r

i=1
X

max
{

0, ρi −

σ

}

=

r

max

i=1
X

r

0, ρi −

{

σ

} ≤

r

i=1
X

max
{

0, ρi −

(

β−

−

1λn

r)

−

}

(ρi −

(

β−

−

1λn

−

r)) =

i=1
X

(λi + β(λn

r −

−

λn)) = 1 + βr(λn

λn).

r −

−

(37)

βr(λn

−

≤

λn). Thus, we have rank(Π(1+ζ)
S

r−

n[Y])

≤

However, (37) can hold only if ζ
r =

λn).

ζ

≤

βr(λn
⇒
On the other-hand,
1λn

r −

−

β−

−

r which, using the same arguments as above, implies that
n

r

−

if rank(Π(1+ζ)
S

n[Y]) > r then it must hold that σ <

1 + ζ =

i=1
X

max

0, ρi −

{

σ

}

>

i=1
X

max

0, ρi −

{

(

β−

−

1λn

r)

−

}

= 1 + βr(λn

r −

−

λn).

(38)

We see that (38) can hold only if ζ > βr(λn
r =

λn), and the lemma follows.

ζ > βr(λn

r−

−

⇒

r −

−

λn). Thus, we also have rank(Π(1+ζ)
S

n[Y]) >

B Proof of Lemma 3

→

) with some parameter δ > 0. Let ˜f : Sn
kF ≤

R be β-smooth and convex. Suppose that Assumption 1
R be diﬀerentiable and
→
ν, for some ν > 0. Then,
) with parameter

1δ, Assumption 1 holds w.r.t. the function ˜f (

˜f (X)

f (X)

n k∇

− ∇

·

We ﬁrst restate the lemma and then prove it.
Lemma 11. Let f : Sn
holds w.r.t. f (
·
convex, and suppose that supX
2(1 + 2β
for ν < 1
˜δ = δ
2ν(1 + 2β
Proof. Let X∗ and ˜X∗ denote minimizers of f (
Since Assumption 1 holds w.r.t. f (
4 we have that

δ )−
δ ) > 0.

−

∈S

·

Sn, respectively.
·
), using the quadratic growth result of Lemma

) over

·

) and ˜f (

f ( ˜X∗)

f (X∗)

−

2
δ h

˜X∗

−

X∗,

∇

f ( ˜X∗)

i

˜X∗

k

X∗

2
F ≤

k

−

=

≤(b)

(cid:16)

2
δ
2
δ
h
(cid:16)
2
˜X∗
δ h

≤(a)

(cid:17)
˜f ( ˜X∗)

∇
f ( ˜X∗)

˜X∗

X∗,

−

X∗,

−

∇

+

i

˜X∗

−
h
˜f ( ˜X∗)

− ∇

i ≤(c)

∇
2ν
δ k

X∗,

f ( ˜X∗)

˜f ( ˜X∗)
i

− ∇

˜X∗

X∗

kF ,

−

(cid:17)

31

·

where (a) follows from convexity of f (
˜f (
lemma that supX

), (b) follows from optimality of ˜X∗ w.r.t.
), and (c) follows from the Cauchy-Schwarz inequality and the assumption of the
˜f (X)
kF ≤
2ν
δ .

Thus, we get that
Using Weyl’s inequality for the eigenvalues we have that

f (X)
˜X∗

n k∇
k

− ∇
X∗

kF ≤

ν.

−

∈S

·

˜f ( ˜X∗))

λn(

∇

λn(

≤
= λn

−
λn

−

≤

∇
1(

1(

∇

∇

f (X∗)) +

f (X∗))
˜f ( ˜X∗))

˜f ( ˜X∗)
δ +

k∇

δ + 2

k∇

−

−

k∇

− ∇
˜f ( ˜X∗)
˜f ( ˜X∗)

f (X∗)

kF
f (X∗)

− ∇

− ∇

kF
f (X∗)
kF .
˜f (X)

− ∇

f (X)

(39)

ν,

kF ≤

Using the smoothness of f (
we have that

·

) and the assumption supX

∈S

n k∇

˜f ( ˜X∗)

k∇

− ∇

f (X∗)

kF ≤ k∇

˜f ( ˜X∗)

− ∇

f ( ˜X∗)

ν + β

˜X∗

k

−

X∗

≤

kF ≤

kF +
ν

k∇
1 +

(cid:18)

f ( ˜X∗)
2β
δ

.

(cid:19)

Plugging-in (40) into (39) and rearranging we obtain

f (X∗)

kF

− ∇

(40)

.

λn

1(

−

∇

˜f ( ˜X∗))

λn(

∇

−

˜f ( ˜X∗))

δ

−

≥

2β
δ

2ν

1 +

(cid:18)

) whenever ν < 1

(cid:19)
2(1 + 2β

δ )−

1δ.

Thus, Assumption 1 indeed holds w.r.t. ˜f (

·

References

[1] Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, and Yuanzhi Li. Linear convergence
of a frank-wolfe type algorithm over trace-norm balls. In Advances in Neural
Information Processing Systems, pages 6192–6201, 2017.

[2] Amir Beck. First-order methods in optimization, volume 25. SIAM, 2017.

[3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algo-
rithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183–
202, 2009.

[4] Amir Beck and Marc Teboulle. Smoothing and ﬁrst order methods: A uniﬁed

framework. SIAM Journal on Optimization, 22(2):557–580, 2012.

[5] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality
of local search for low rank matrix recovery. In Advances in Neural Information
Processing Systems, pages 3873–3881, 2016.

[6] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav
Voroninski. Phase retrieval via matrix completion. SIAM review, 57(2):225–
251, 2015.

32

[7] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal

component analysis? Journal of the ACM (JACM), 58(3):11, 2011.

[8] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex
optimization. Foundations of Computational mathematics, 9(6):717–772, 2009.

[9] Yudong Chen and Martin J Wainwright. Fast low-rank estimation by pro-
jected gradient descent: General statistical and algorithmic guarantees. arXiv
preprint arXiv:1509.03025, 2015.

[10] Vladimir F. Demyanov and Aleksandr M. Rubinov. Approximate methods in

optimization problems. Elsevier Publishing Company, 1970.

[11] Lijun Ding, Yingjie Fei, Qiantong Xu, and Chengrun Yang. Spectral frank-
wolfe algorithm: Strict complementarity and linear convergence. In Interna-
tional Conference on Machine Learning, pages 2535–2544. PMLR, 2020.

[12] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth,
and linear convergence of proximal methods. Mathematics of Operations Re-
search, 43(3):919–948, 2018.

[13] Robert M Freund, Paul Grigas, and Rahul Mazumder. An extended frank–
wolfe method with in-face directions, and its application to low-rank matrix
completion. SIAM Journal on Optimization, 27(1):319–346, 2017.

[14] Dan Garber. Faster projection-free convex optimization over the spectrahe-
dron. In Advances in Neural Information Processing Systems, pages 874–882,
2016.

[15] Dan Garber. Linear convergence of frank-wolfe for rank-one matrix recovery

without strong convexity, 2019.

[16] Dan Garber. On the convergence of projected-gradient methods with low-rank
projections for smooth convex minimization over trace-norm balls and related
problems. CoRR, abs/1902.01644, 2019.

[17] Dan Garber and Elad Hazan. Faster rates for the frank-wolfe method over
strongly-convex sets. In 32nd International Conference on Machine Learning,
ICML 2015, 2015.

[18] Dan Garber, Elad Hazan, and Tengyu Ma. Online learning of eigenvectors. In
Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, pages 560–568, 2015.

[19] Dan Garber and Atara Kaplan. Fast stochastic algorithms for low-rank and
nonsmooth matrix problems.
In The 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha,
Okinawa, Japan, pages 286–294, 2019.

33

[20] Dan Garber, Shoham Sabach, and Atara Kaplan. Fast generalized conditional
gradient method with applications to matrix recovery problems. arXiv preprint
arXiv:1802.05581, 2018.

[21] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious
local minimum. In Advances in Neural Information Processing Systems, pages
2973–2981, 2016.

[22] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU

Press, 2012.

[23] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimiza-
tion. In Proceedings of the 30th International Conference on Machine Learning,
ICML, 2013.

[24] Martin Jaggi and Marek Sulovsk´y. A simple algorithm for nuclear norm reg-
In Proceedings of the 27th International Conference on

ularized problems.
Machine Learning, ICML, 2010.

[25] Prateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank mini-
mization via singular value projection. In J. D. Laﬀerty, C. K. I. Williams,
J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural
Information Processing Systems 23, pages 937–945. 2010.

[26] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresh-
In Advances in Neural

olding methods for high-dimensional m-estimation.
Information Processing Systems, pages 685–693, 2014.

[27] S¨oren Laue. A hybrid algorithm for convex semideﬁnite optimization.

In
Proceedings of the 29th International Conference on International Conference
on Machine Learning, pages 1083–1090. Omnipress, 2012.

[28] Cun Mu, Yuqian Zhang, John Wright, and Donald Goldfarb. Scalable robust
matrix recovery: Frank–wolfe meets proximal methods. SIAM Journal on
Scientiﬁc Computing, 38(5):A3291–A3317, 2016.

[29] Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of ﬁrst
order methods for non-strongly convex optimization. Mathematical Program-
ming, 175(1-2):69–107, 2019.

[30] Yurii Nesterov. Introductory lectures on convex optimization: A basic course,

volume 87. Springer Science & Business Media, 2013.

[31] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval us-
ing alternating minimization. In Advances in Neural Information Processing
Systems, pages 2796–2804, 2013.

[32] Praneeth Netrapalli, UN Niranjan, Sujay Sanghavi, Animashree Anandkumar,
and Prateek Jain. Non-convex robust pca. In Advances in Neural Information
Processing Systems, pages 1107–1115, 2014.

34

[33] Benjamin Recht. A simpler approach to matrix completion. The Journal of

Machine Learning Research, 12:3413–3430, 2011.

[34] Emile Richard, Pierre-Andr‘e Savalle and Nicolas Vayatis. Estimation of simul-
taneously sparse and low rank matrices. Proceedings of the 29th International
Conference on Machine Learning, 2012.

[35] Joel A Tropp. Convex recovery of a structured signal from independent ran-
dom linear measurements. In Sampling Theory, a Renaissance, pages 67–101.
Springer, 2015.

[36] John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust
principal component analysis: Exact recovery of corrupted low-rank matrices
via convex optimization. In Advances in neural information processing systems,
pages 2080–2088, 2009.

[37] Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis. Fast
algorithms for robust pca via gradient descent. In Advances in neural infor-
mation processing systems, pages 4152–4160, 2016.

[38] Alp Yurtsever, Madeleine Udell, Joel A. Tropp, and Volkan Cevher. Sketchy
decisions: Convex low-rank matrix optimization with optimal storage. In Pro-
ceedings of the 20th International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages
1188–1196, 2017.

[39] Zirui Zhou and Anthony Man-Cho So. A uniﬁed approach to error bounds
for structured convex optimization problems. Mathematical Programming,
165(2):689–728, Oct 2017.

35

