1
2
0
2

t
c
O
5
2

]

C
O
.
h
t
a
m

[

1
v
0
3
6
2
1
.
0
1
1
2
:
v
i
X
r
a

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Approach for Best
Hyperparameter Selection Problem Using a
Bilevel Optimization Strategy

Chieu Thanh Nguyen1, Jan Harold Alcantara2, Takayuki
Okuno3, Akiko Takeda4 and Jein-Shan Chen5*

1Department of Mathematics, National Cheng Kung University,
Tainan, 701, Taiwan.
2Institute of Statistical Science, Academia Sinica, Taipei, 11677,
Taiwan.
3Center for Advanced Intelligence Project, RIKEN, Tokyo,
103-0027, Japan.
4Department of Creative Informatics, Graduate School of
Information Science and Technology, The University of Tokyo,
Tokyo, 113-8656, Japan; Center for Advanced Intelligence
Project, RIKEN, Tokyo, 103-0027, Japan.
5*Department of Mathematics, National Taiwan Normal
University, Taipei, 11677, Taiwan.

*Corresponding author(s). E-mail(s): jschen@math.ntnu.edu.tw;
Contributing authors: thanhchieu90@gmail.com;
janharold@stat.sinica.edu.tw; takayuki.okuno.ks@riken.jp;
takeda@mist.i.u-tokyo.ac.jp;

Abstract

Strongly motivated from use in various ﬁelds including machine learning,
the methodology of sparse optimization has been developed intensively
so far. Especially, the recent advance of algorithms for solving problems
with nonsmooth regularizers is remarkable. However, those algorithms
suppose that weight parameters of regularizers, called hyperparameters
hereafter, are pre-ﬁxed, and it is a crucial matter how the best hyperpa-
rameter should be selected. In this paper, we focus on the hyperparameter
selection of regularizers related to the (cid:96)p function with 0 < p ≤ 1 and

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

consider to apply the bilevel programming strategy, wherein we need
to solve a bilevel problem, whose lower-level problem is a nonsmooth,
possibly nonconvex and non-Lipschitz. Recently, for solving a bilevel
problem for hyperparameter selection of the pure (cid:96)p (0 < p ≤ 1)
regularizer Okuno et al has discovered the new necessary optimality con-
ditions, called SB(scaled bilevel)-KKT conditions, and further proposed
a smoothing-type algorithm using a certain speciﬁc smoothing function.
Our
to propose a uniﬁed smoothing approach
using smoothing functions that belong to the Chen-Mangasarian
class, and then prove that generated iteration points accumulate
at SB-KKT points under milder assumptions than Okuno et al.
Another contribution is that our approach and analysis are appli-
cable to a wider class of regularizers than Okuno et al. Numer-
ical comparisons demonstrate which smoothing functions work well
for hyperparameter optimization via bilevel optimization approach.

contribution is

Keywords: hyperparameter learning, smoothing functions, bilevel
optimization

1 Introduction

A learning algorithm in machine learning usually involves solving the uncon-
strained optimization problem

min
ω∈(cid:60)n

g(ω) +

r
(cid:88)

i=1

λiRi(ω),

(1)

where λ = (λ1, . . . , λr) is called a hyperparameter, whose value is decided prior
to implementation of the learning algorithm. Here, Ri, g : (cid:60)n → (cid:60), i = 2, . . . , r
are twice continuously diﬀerentiable functions, and R1(ω) := (cid:80)n
i=1 ψ(|ωi|p)
(0 < p ≤ 1) with ψ satisfying the following assumption:

(A) ψ : [0, ∞) → (cid:60) is twice continuously diﬀerentiable on [0, ∞) and there exist

two positive constants α, β such that ∀t ∈ [0, ∞)

0 < ψ(cid:48)(t) ≤ α and − β ≤ ψ(cid:48)(cid:48)(t) ≤ 0.

In this manuscript, we make (A) our blanket assumption on ψ. It is well-known
that the function R1(ω) is nonsmooth, nonconvex, even non-Lipschitz when
p ∈ (0, 1). There are many penalty functions often used in statistics and signal
reconstruction satisfying Assumption (A) (see Appendix A).

For notation purposes, we denote

G(ω, ¯λ) := g(ω) +

r
(cid:88)

i=2

λiRi(ω),

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

3

with ¯λ := (λ2, . . . , λr)T ∈ (cid:60)r−1. Then problem (1) can be rewritten as

min
ω∈(cid:60)n

G(ω, ¯λ) + λ1R1(ω).

(2)

The problem of ﬁnding optimal values of the hyperparameters can be accom-
plished using grid search and Bayesian optimization [1, 30]. This paper, on
the other hand, is devoted to a bilevel optimization strategy to ﬁnd the best
hyperparameter.

In particular, we focus on the bilevel nonsmooth programming problem

min
ω∗
λ,λ
s.t ω∗

λ ∈ argmin
ω∈(cid:60)n

f (ω∗
λ)
G(ω, ¯λ) + λ1R1(ω)

λ ≥ 0,

(3)

where f : (cid:60)n → (cid:60) is continuously diﬀerentiable. Problem (2) which appears in
the constraint set of (3) is called the lower-level problem, and the minimization
of f is called the upper-level problem.

Bilevel optimization problems were introduced by Bracken and McGill [5].
The reader is referred to [13, 14, 26] for a survey of methods for solving the
bilevel optimization problem and their applications. Eﬀorts have been put forth
by many researchers in the past few decades to use bilevel optimization strat-
egy to the problem of ﬁnding the best hyperparameter values. In particular,
[2, 3] focused on a bilevel support-vector regression (SVR) problem where the
lower-level optimization problem is cast as a convex quadratic program. [18, 19]
proposed a bilevel cross-validation program for support-vector machine (SVM),
where the upper-level problem is convex and nonsmooth, while the lower-level
problem is diﬀerentiable. [22] used gradient-based methods for the bilevel opti-
mization problem with nonsmooth convex lower-level problem (for example,
sparse models based on the (cid:96)1-norm). However, [2, 18, 22] only proposed the
algorithms to solve the bilevel optimization without providing any theoretical
analysis. [28] constructed the hyperparameter optimization problem through
K-fold cross-validation as a bilevel optimization problem with LASSO regres-
sion and an (cid:96)1-norm support-vector machine (SVM) in the lower-level problem.
They used parametric programming theory to reformulate the bilevel optimiza-
tion problem as a single level problem which is called the bilevel and parametric
optimization approach to hyperparameter optimization (HY-POP). Similar to
that in [28], the authors only provided the numerical experiments to show
the eﬃciency of HY-POP without any theoretical analysis. [16] considered
bilevel optimization problems for variational image denoising models, where
the upper-level problem is smooth while the lower-level problem is the (cid:96)p
regularizer with p = 1
2 , 1, 2. They proposed semismooth Newton method for
solving the bilevel optimization problem including the (cid:96)2-norm and the (cid:96)1-
-norm
norm. Especially, they only provided numerical experiments for the (cid:96) 1

2

Springer Nature 2021 LATEX template

4

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

and leave the theoretical analysis for nonconvex (cid:96) 1
-norm to future work. Nev-
2
ertheless, they showed that the (cid:96) 1
-norm has better denoising performance than
2
the (cid:96)1-norm. Recently, [23] considered the bilevel program (3) with the func-
tion R1(ω) := (cid:80)n
i=1|ωi|p (0 < p ≤ 1) (i.e. the (cid:96)p-regularizer) by employing
smoothing method via the twice continuously diﬀerentiable function

ϕµ(ω) =

n
(cid:88)

i=1

(ω2

i + µ2)

p
2

(4)

as a smooth approximation of R1. Using such a smoothing function, problem
(3) can be approximated by a smooth bilevel program, which then allows for
use of several optimization techniques which normally require diﬀerentiability.
Thus, they established the convergence analysis for nonsmooth, nonconvex
(cid:96)p-norm with (0 < p ≤ 1).

In this paper, we consider a general framework for constructing smoothing
functions for R1, where the associated ψ is any function that satisﬁes Assump-
tion (A). On the one hand, this work is an important extension of [23] where
the speciﬁc smoothing function used for R1 is given by (4) and the function
ψ considered is ψ(t) = t. However, while some results in [23] can possibly be
extended to the general case, i.e. when other smoothing functions ϕµ for R1
are used, several arguments and fundamental results in [23] are speciﬁc to the
chosen approximation (4). For instance, [23, Lemma 7] provides an inequal-
ity which describes the relationship between the rates of decay to zero of the
smoothing parameter µ and the sequence of iterates {ωk} produced by the
smoothing algorithm. However, this can only be veriﬁed for the smoothing
function (4), and it is worthwhile to note that this proposition is a key result
to establish the convergence results. When a general smoothing function is
used for R1, the analysis is more diﬃcult and requires some subtle arguments.
Therefore, important contributions of this paper involve the considerably wider
class of regularizers for the hyperparameter optimization problem, as well as
the uniﬁed convergence analysis of the smoothing algorithm for the said model
under less restrictive constraint qualiﬁcations.

The smoothing framework we consider is based on [7], where a density
function is used to generate a smooth approximation of the plus function
x+ := max{0, x}, and in turn a smoothing function for R1 (see Section 2.3).
Indeed, the theoretical novelty of our work lies on our strategies to establish
the convergence results using only our knowledge of the density function used
to obtain the smooth approximation of R1. That is, we do not rely on the
speciﬁc formula of the smoothing function as in [23], thus making our analysis
generalized. Despite the diﬃculty in accomplishing this task, we successfully
identiﬁed important properties of density functions that will suﬃce to prove
our main results, particularly the convergence of the smoothing algorithm
to a candidate solution of (3). Of course, the main results of the proposed
smoothing framework in this present work subsume those presented in [23].

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

5

However, we emphasize that some results in [23] cannot be simply extended to
the general case since in the said work, the formula of the speciﬁc smoothing
function chosen was fully exploited.

From a practical point of view, the choice of smoothing functions is critical
in achieving successful simulations with fast convergence rates. We compare
the numerical performance of six smoothing functions generated via Chen and
Mangasarian’s method [7] to determine which function is more suitable for the
smoothing approach.

This paper is organized as follows: In Section 2, we review some fundamen-
tal concepts in analysis and a brief introduction about ﬁrst-order necessary
optimality condition for lower-level problem. We also review the method pro-
posed in [7] to construct smoothing functions of the plus function by means of
density functions. This will serve as our basis to construct smoothing functions
for R1(ω), and our theoretical analysis will all be dependent on the density
function. In Section 3, the approximation of lower-level problem and the con-
vergence analysis are proposed for the bilevel nonsmooth optimization problem
(3). In Section 4, we list six smoothing functions of R1(ω) and compare the
applicability and eﬃciency of these smoothing functions in solving (3).

Throughout this paper, we denote the vector ω ∈ (cid:60)n by ω = (ω1, . . . , ωn)T ,
and we let |ω| := (|ω1|, . . . , |ωn|)T , and |ω|p := (|ω1|p, . . . , |ωp|p)T . We deﬁne
the sgn function as sgn(t) = 1 (t > 0), sgn(t) = 0 (t = 0), and sgn(t) = −1
(t < 0). For a diﬀerentiable function f : (cid:60)n → (cid:60), we denote the gradi-
ent function of f by ∇f with ∇f (ω) := ( ∂f (ω)
)T ∈ (cid:60)n and if f
∂ω1
is twice diﬀerentiable, we denote the Hessian of f by ∇2f with ∇2f (ω) :=
(cid:16) ∂2f (ω)
∂ωi∂ωj

, . . . , ∂f (ω)
∂ωn

∈ (cid:60)n×n.

(cid:17)

1≤i,j≤n

2 Preliminaries

We review some important concepts in nonsmooth analysis. We also present
the corresponding scaled bilevel Karush-Kuhn-Tucker conditions of our main
problem (3). Finally, we review the method of Chen and Mangasarian to con-
struct smoothing functions for the plus function and discuss how to use this
to obtain a smoothing function for the absolute value function.

2.1 Some concepts in analysis

The following facts can be found in the books of Rockafellar and Wet [24].

Deﬁnition 2.1 [24, Deﬁnition 8.3] Let f : (cid:60)n → (cid:60) ∪ {∞} be a proper function. For
vectors v ∈ (cid:60)n and ¯x ∈ (cid:60)n, one say that
1. v is a regular subgradient of f at ¯x, written v ∈ ˆ∂f (¯x), if

f (x) ≥ f (¯x) + vT (x − ¯x) + o((cid:107)x − ¯x(cid:107)).

Springer Nature 2021 LATEX template

6

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

2. v is a general subgradient of f at ¯x, written v ∈ ∂f (¯x), if there are sequences

{xν} ⊆ (cid:60)n and {vν} ⊆ (cid:60)n such that

lim
ν→∞

xν = ¯x and vν ∈ ˆ∂f (xν) with lim
ν→∞

vν = v.

Note that a regular subgradient of f at ¯x is also called a Fr´echet subgradient
of f at ¯x (see in [15]). Moreover, if f is a proper and convex function, the
regular subgradient of f coincides the subgradient of f in the sense of convex
analysis (see in [24, Proposition 8.12]).

Proposition 2.1 [24, Theorem 8.6] For a function f : (cid:60)n → (cid:60) ∪ {∞} and a point ¯x
where f is ﬁnite, the subgradient sets ˆ∂f (¯x) and ∂f (¯x) are closed, with ˆ∂f (¯x) convex
and ˆ∂f (¯x) ⊂ ∂f (¯x).

Proposition 2.2 [24, Theorem 10.1] If a proper function f : (cid:60)n → (cid:60) ∪ {∞} has a
local minimum at ¯x, then 0 ∈ ˆ∂f (¯x) ⊂ ∂f (¯x).

2.2 First-order optimality condition for the lower-level

problem

Using Proposition 2.2, the ﬁrst-order optimality condition for the lower-level
problem (2) is given by

0 ∈ ∂ω(G(ω∗, ¯λ) + λ1R1(ω∗)),

where ∂ω(G(ω∗, ¯λ) + λ1R1(ω∗)) is the general subgradient with respect to ω
of G(ω, ¯λ) + λ1R1(ω) at ω∗. Then problem (3) can be transformed into the
following one-level problem

min
ω,λ
s.t 0 ∈ ∂ω(G(ω, ¯λ) + λ1R1(ω))

f (ω)

λ ≥ 0.

(5)

To obtain candidate solutions of problem (5), we employ the scaled ﬁrst-order
necessary condition for a non-Lipschitz continuous function which was pro-
posed by Chen, Xu and Ye [9]. Since the function G(ω, ¯λ)+λ1R1(ω) (0 < p < 1)
is non-Lipschitz continuous function, we deﬁne the scaled ﬁrst-order necessary
condition for the lower-level problem (2) as follows.

Deﬁnition 2.2 We say that ω∗ satisﬁes the scaled ﬁrst-order necessary condition
of (2) if

W∗∇ωG(ω∗, ¯λ) + pλ1|W∗|pψ(cid:48)(|ω∗|p) = 0,

where W∗ := diag(ω∗), |W∗|p := diag(|ω∗|p), and

ψ(cid:48)(|ω∗|p) := (ψ(cid:48)(|ω∗

1 |p), ψ(cid:48)(|ω∗

2 |p), . . . , ψ(cid:48)(|ω∗

n|p))T .

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

7

Using this scaled ﬁrst-order necessary condition, we obtain the following

one-level problem.

min
ω,λ
s.t W ∇ωG(ω, ¯λ) + pλ1|W |pψ(cid:48)(|ω|p) = 0

f (ω)

λ ≥ 0,

(6)

where W := diag(ω), |W |p := diag(|ω|p), and

ψ(cid:48)(|ω|p) := (ψ(cid:48)(|ω1|p), ψ(cid:48)(|ω2|p), . . . , ψ(cid:48)(|ωn|p))T .

Notice that this problem is diﬀerent from (5). The following result shows the
relationship between the feasible regions of problems (5) and (6) which can
proved in the same manner as [23, Lemma 3].

+, if 0 ∈ ∂ω(G(ω, ¯λ) + λ1R1(ω)), then
Proposition 2.3 For ω ∈ (cid:60)n and λ ∈ (cid:60)r
W ∇ωG(ω, ¯λ) + pλ1|W |pψ(cid:48)(|ω|p) = 0. In particular, when p < 1, the converse is also
true.

From the above proposition, we observe that the feasible region of (6) is
larger than the feasible region of (5) when p = 1. Moreover, the feasible regions
of problems (5) and (6) are identical when p < 1.

Now, we state a new optimality condition for problem (3) based on the
scaled ﬁrst-order necessary condition, namely scaled bilevel KKT (SB-KKT)
conditions as the following deﬁnition.

Deﬁnition 2.3 We say that (ω∗, λ∗) ∈ (cid:60)n × (cid:60)r is a scaled bilevel Karush-Kuhn-
Tucker (SB-KKT) point for problem (3) if there exists a pair of vectors (ζ∗, η∗) ∈
(cid:60)n × (cid:60)r such that

W 2

∗ ∇f (ω∗) + H(ω∗, λ∗)ζ∗ = 0,

W∗∇ωG(ω∗, ¯λ∗) + pλ∗
p (cid:80)
j )|ω∗
j /∈I(ω∗) sgn(ω∗

1|W∗|pψ(cid:48)(|ω∗|p) = 0,
j = η∗
j |p−1ψ(cid:48)(|ω∗
1 ,
j = 0 (j ∈ I(ω∗)),
ζ∗

j |p)ζ∗

∇Rj (ω∗)T ζ − η∗

j = 0 (j = 2, 3, . . . , r),

0 ≤ λ∗, 0 ≤ η∗, (λ∗)T η∗ = 0,

(7)

(8)

(9)

(10)

(11)

(12)

where W∗ := diag(ω∗), I(ω) := {j ∈ {1, 2, . . . , n} | ωj = 0}. Here, we write
ωωG(ω, ¯λ)+λ1p(p−1)diag(|W |pψ(cid:48)(|ω|p))+λ1p2diag(|W |2pψ(cid:48)(cid:48)(|ω|p))
H(ω, λ) = W 2∇2
with W := diag(ω), |W |p := diag(|ω|p), and |W |2p := diag(|ω|2p) for ω ∈ (cid:60)n and
λ ∈ (cid:60)r.

Springer Nature 2021 LATEX template

8

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

The SB-KKT conditions are necessary optimality conditions for problem
(6) as asserted by the following results. The proof of this proposition is essen-
tially similar to [23, Theorems 2,4]. Hence, we only state the result and omit
the detailed proof.

Proposition 2.4 Let (ω∗, λ∗) ∈ (cid:60)n × (cid:60)r be a local optimum of (6). Then, (ω∗, λ∗)
together with some pair of vectors (ζ∗, η∗) ∈ (cid:60)n ×(cid:60)r satisﬁes the SB-KKT conditions
(7)-(12) under an appropriate constraint qualiﬁcation concerning the constraints
∂G(ω,¯λ)
+ psgn(ωj )λ1|ωj |p−1ψ(cid:48)(|ωj |p) = 0 (j /∈ I(ω∗)), ωj = 0 (j ∈ I(ω∗)), and
∂ωj
λ ≥ 0.

Proposition 2.5 Let p < 1 and (ω∗, λ∗) ∈ (cid:60)n × (cid:60)r be a local optimum of (5).
Then, (ω∗, λ∗) together with some pair of vectors (ζ∗, η∗) ∈ (cid:60)n × (cid:60)r satisﬁes the SB-
KKT conditions (7)-(12) under an appropriate constraint qualiﬁcation concerning
the constraints ∂G(ω,¯λ)
+ psgn(ωj )λ1|ωj |p−1ψ(cid:48)(|ωj |p) = 0 (j /∈ I(ω∗)), ωj = 0 (j ∈
I(ω∗)), and λ ≥ 0.

∂ωj

2.3 Smoothing functions of |x| via density functions

We next recall the general deﬁnition of a smoothing function.

Deﬁnition 2.4 [6, Deﬁnition 1] Let h : (cid:60)n → (cid:60) be a continuous function. We say
that φ : (cid:60)++ × (cid:60)n → (cid:60) is a smoothing function of h if it satisﬁes the following:

(i) φ(µ, ·) is continuously diﬀerentiable for any µ > 0;

(ii)

lim
w→z,µ↓0

φ(µ, w) = h(z) for any z ∈ (cid:60)n.

We now review the construction way of smoothing functions of the plus
function (x)+ = max{x, 0} for x ∈ (cid:60) which was studied by Chen and Man-
gasarian [7]. It is well-known that (x)+ = (cid:82) x
−∞ σ(t)dt, where σ is the step
function given by

σ(x) =

(cid:26) 1 if x > 0,
0 if x ≤ 0.

On the other hand, the step function σ can be described as σ(x) = (cid:82) x
where δ is the Dirac delta function which satisﬁes the following properties

−∞ δ(t)dt,

δ(x) =

(cid:26) 0 if x (cid:54)= 0
∞ if x = 0

and

(cid:90) ∞

−∞

δ(x)dx = 1.

Clearly, the plus function can be obtained by integrating twice the Dirac delta
function. To obtain a smoothing function of the plus function, we start by
considering a probability density function, which will serve as a smoothing
function of the Dirac delta function. More precisely, let ρ : (cid:60) → (cid:60)+ be a

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

9

piecewise continuous function, called a density or kernel function, with a ﬁnite
number of pieces satisfying the following properties

ρ(x) ≥ 0 and

(cid:90) +∞

−∞

ρ(x)dx = 1.

Deﬁne

1
µ
where µ is a positive parameter. When µ goes to 0, the limit of ˆt(µ, x) is the
Dirac delta function δ(x). A smooth approximation of σ can then be formed as

ˆt(µ, x) :=

ρ

,

(cid:19)

(cid:18) x
µ

ˆs(µ, x) :=

(cid:90) x

−∞

ˆt(µ, t)dt ≈ σ(x),

and ﬁnally, this yields a smoothing function ˆφ for the plus function given by

ˆφ(µ, x) :=

(cid:90) x

−∞

ˆs(µ, t)dt ≈ (x)+.

The above-mentioned relationship among the functions is summarized as

follows:

δ(x)

≈

(cid:82) x
−∞ δ(t)dt
−−−−−−−→ σ(x)

(cid:82) x
−∞ σ(t)dt
−−−−−−−→ (x)+

≈

≈

ρ(x)

1

µ ρ( x
µ )
−−−−→ ˆt(µ, x)

ˆt(µ,t)dt

(cid:82) x
−−−−−−−−→ ˆs(µ, x)

−∞

(cid:82) x
−∞ ˆs(µ,t)dt
−−−−−−−−→ ˆφ(µ, x)

In addition, assume that the function ρ : (cid:60) → (cid:60)+ satisﬁes

ρ(x) = ρ(−x) and κ :=

(cid:90) +∞

−∞

|x|ρ(x)dx < +∞,

(13)

implying that, for any µ > 0,

ˆt(µ, x) = ˆt(µ, −x) and

(cid:90) +∞

−∞

|x|ˆt(µ, x)dx < +∞.

Therefore, the smooth approximation ˆφ of (x)+ can be described by

convolution [6, 7], i.e.,

ˆφ(µ, x) =

(cid:90) +∞

−∞

(x − t)+ˆt(µ, t)dt =

(cid:90) x

−∞

(x − t)ˆt(µ, t)dt.

(14)

Springer Nature 2021 LATEX template

10

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

The following proposition shows the properties of ˆφ(µ, x), whose proof can

be found in [7, Proposition 2.2].

Proposition 2.6 [7, Proposition 2.2] Suppose that ˆφ(µ, x) is deﬁned by (14). Then,
for a ﬁxed µ > 0, ˆφ(µ, x) possesses the following properties.
(a) ˆφ(µ, x) is continuously diﬀerentiable.
(b) −D2µ ≤ ˆφ(µ, x) − (x)+ ≤ D1µ, where
(cid:90) 0

(cid:26)(cid:90) +∞

(cid:27)

D1 :=

|x|ρ(x)dx, and D2 := max

xρ(x)dx, 0

.

−∞

−∞

(c) ˆφ(cid:48)(µ, x) is bounded satisfying 0 ≤ ˆφ(cid:48)(µ, x) ≤ 1, where ˆφ(cid:48)(µ, x) is the ﬁrst
derivative of ˆφ(µ, x) with respect to variable x.

By Deﬁnition 2.4 and Proposition 2.6, we see that ˆφ(µ, x) is a smoothing
function of the plus function (x)+. In fact, from (13) and Proposition 2.6(b),
we have that

which implies that for any x ∈ (cid:60),

0 ≤ ˆφ(µ, x) − (x)+ ≤

1
2

κµ

lim
xk→x,µk↓0

ˆφ(µk, xk) = (x)+.

Meanwhile, since ˆφ(cid:48)(µ, x) = (cid:82) x/µ

−∞ ρ(t)dt, then

lim
xk→x,µk↓0

ˆφ(cid:48)(µk, xk) =

(cid:40)

1 if x > 0
if x < 0.
0

(15)

(16)

Although the above limit may not exist when x = 0, we have from Proposition
2.6(c) that subsequential limits exist and belong to the interval [0, 1].

(cid:90) +∞

To obtain a smoothing function for the absolute value function, observe
that |x| = (x)+ + (−x)+. Then we achieve a smoothing function of |x| as
follows:

φ(µ, x) := ˆφ(µ, x) + ˆφ(µ, −x) =

|x − t|ˆt(µ, t)dt.

(17)

In a manner similar to Proposition 2.6, we also have the following properties

−∞

for φ(µ, x).

Proposition 2.7 Suppose that φ(µ, x) is deﬁned as in (17). Then, for a ﬁxed µ > 0,
we have

(a) φ(µ, x) is continuously diﬀerentiable.

(b) 0 ≤ φ(µ, x) − |x| ≤ κµ with the constant κ > 0 deﬁned in (13).
(c) φ(cid:48)(µ, x) is bounded satisfying −1 ≤ φ(cid:48)(µ, x) ≤ 1.

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

11

From Proposition 2.7, we obtain results analogous to (15) and (16). That

is,

and

lim
xk→x,µk↓0

φ(µk, xk) = |x|,

∀x ∈ (cid:60)

lim
xk→x,µk↓0

φ(cid:48)(µk, xk) = sgn(x) ∀x (cid:54)= 0.

(18)

Likewise, we have from Proposition 2.7(c) that subsequential limits of the
sequence on the left-hand side of (18) exist and belong to [−1, 1].

There are various choices of the density function ρ and corresponding
smoothing function φ. Many instances will be presented in the section of
numerical experiments.

3 Proposed algorithm and convergence results

In this section, we describe our smoothing algorithm for (3) and present our
convergence results.

3.1 Smoothing approach and the algorithm

One main source of diﬃculty in solving the bilevel program (3) is the
nonsmooth, nonconvex and possibly non-Lipschitz component R1(ω) =
(cid:80)n
i=1 ψ(|ωi|p), where p ∈ (0, 1]. To overcome this diﬃculty, we apply the
smoothing technique to R1 with the smoothing function φ deﬁned in the
previous section, yielding the following smoothed function:

ϕµ(ω) :=

n
(cid:88)

j=1

ψ ([φ(µ, ωj)]p) .

(19)

Then, as in [23], we consider problem (3) with ϕµ in place of R1, and fur-
ther replace the obtained smoothed lower-level problem with its ﬁrst-order
condition. Hence, the following problem is obtained:

min
ω,λ
s.t ∇ωG(ω, ¯λ) + λ1∇ϕµ(ω) = 0

f (ω)

λ ≥ 0.

(20)

Next, let us suppose that ϕµ is twice continuously diﬀerentiable from this
moment and recall Assumption (A) together with our diﬀerentiability assump-
tions on Ri (i = 2, . . . , r) and g. These properties enable us to consider the
KKT conditions.1 By virtue of this fact, we can ﬁnd candidate solutions to
(20) by looking at its KKT points.

In fact, it is suﬃcient to obtain approximate KKT points as introduced
in [23]: Given a parameter ˆε > 0, we deﬁne an ˆε-approximate KKT point for

1Without the C2 property of ϕµ, the KKT conditions cannot be taken because the constraint

function ∇ωG(ω, ¯λ) + λ1∇ϕµ(ω) is not necessarily smooth.

Springer Nature 2021 LATEX template

12

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

problem (20) as follows: We say that {(ω, λ, ζ, η)} ⊆ (cid:60)n × (cid:60)r × (cid:60)n × (cid:60)r is an
ˆε-approximate KKT point for (20) if there exists a vector (ε1, ε2, ε3, ε4, ε5) ∈
(cid:60)n × (cid:60) × (cid:60)r−1 × (cid:60)n × (cid:60) such that

∇f (ω) + (∇2

ωωG(ω, ¯λ) + λ1∇2ϕµ(ω))ζ = ε1,
∇ϕµ(ω)T ζ − η1 = ε2,
∇Rj(ω)T ζ − ηj = (ε3)j (j = 2, 3, . . . , r),
∇ωG(ω, ¯λ) + λ1∇ϕµ(ω) = ε4,
λT η = ε5,
0 ≤ λ,

0 ≤ η,

(21)

(22)

(23)

(24)

(25)

and

where ∇2
an ˆε-approximate KKT point is identical to a KKT point.

ωωG(ω, ¯λ) is the Hessian of G with respect to ω. Note that when ˆε = 0,

(cid:107)(ε1, ε2, ε3, ε4, ε5)(cid:107) ≤ ˆε,

Now, by iteratively computing an ˆε-approximate KKT point while decreas-
ing ˆε and the smoothing parameter µ, we obtain the following smoothing
algorithm:

Algorithm 1 (A Smoothing Method for Nonsmooth Bilevel Optimization)
Step 0 Choose µ0 (cid:54)= 0, β1, β2 ∈ (0, 1) and ˆε0 ≥ 0. Set k := 0.
Step 1 Find an ˆεk-approximate KKT point {(ωk+1, λk+1, ζ k+1, ηk+1)} for
problem (20) with µ = µk.
Step 2 Set µk+1 = β1µk, ˆεk+1 = β2 ˆεk and k := k + 1.

Algorithm 1 is quite similar to the one proposed by Okuno et al [23]. How-
p
2 as the

ever, whereas Okuno et al supposed to employ only (cid:80)n
smoothing function ϕµ, it enjoys much more freedom in choices.

i + µ2)

i=1(ω2

In order to establish the global convergence of Algorithm 1, we will require
some more properties of the density function ρ used for constructing the
smoothing function φ, which also accomplish the C2-property of ϕµ supposed
above.

3.2 Assumptions for convergence analysis

3.2.1 Assumptions on iterates

We assume that at every iteration, an ˆεk-approximate KKT point can always
be computed henceforth. We also make the following technical assumptions
about the sequence generated by Algorithm 1:

Assumption B. Let {(ωk, λk, ζ k, ηk)} ⊆ (cid:60)n × (cid:60)r × (cid:60)n × (cid:60)r be the sequence
produced by Algorithm 1. Then, the following properties hold, where (ω∗, λ∗)
denotes an arbitrary accumulation point of the sequence {(ωk, λk)}:

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

13

(B1) lim inf k→∞ λk
1 > 0.
(B2) The sequence {(ωk, λk)} is bounded.
(B3) Let p = 1 and λ∗ = (λ∗

1, ¯λ∗). Assume that λ∗

1ψ(cid:48)(0) (cid:54)= | ∂G(ω∗,¯λ∗)

∂ωj

| for any

j ∈ I(ω∗), where I(ω∗) is described in Deﬁnition 2.3.

(B4) Denote

and

I(λ∗) := {i ∈ {1, 2, . . . , r} | λ∗

i = 0}

Φj(ω, λ) :=

∂G(ω, ¯λ)
∂ωj

+ psgn(ωj)λ1|ωj|p−1ψ(cid:48)(|ωj|p) (j /∈ I(ω∗)).

Then, the Mangasarian-Fromovitz constraint qualiﬁcation (MFCQ) holds at
(ω, λ) = (ω∗, λ∗) for the constraints Ψj(ω, λ) = 0 for all j = 1, . . . , n and
λ ≥ 0, where

Ψj(ω, λ) :=

(cid:26) Φj(ω, λ) if j /∈ I(ω∗),
if j ∈ I(ω∗).

ωj

That is, {∇(ω,λ)Ψj(ω∗, λ∗)}n
(cid:60)n+r such that

j=1 is linearly independent and there exists ¯d ∈

∇(ω,λ)Ψj(ω∗, λ∗)T ¯d = 0 ∀j = 1, . . . , n,
∀i ∈ I(λ∗).

(∇(ω,λ)λi|(ω,λ)=(ω∗,λ∗))T ¯d > 0

(26)

(27)

Note that Assumption (B4) is weaker than the linearly independent con-
straint qualiﬁcation (LICQ) which was used in [23]. The following lemma is
needed for subsequent analysis.

Lemma
3.1 Suppose
the
of
sequence
{∇(¯ω,λ)Φj (ω∗, λ∗)}j /∈I(ω∗)
d ∈ (cid:60)n−|I(ω∗)|+r such that

that
{(ωk, λk)}

is

(ω∗, λ∗)
such

an arbitrary accumulation point
holds. Then,
(B4)
is linearly independent and there exists a vector

that Assumption

∇(¯ω,λ)Φj (ω∗, λ∗)T d = 0 ∀j /∈ I(ω∗),
(∇(¯ω,λ)λi|(ω,λ)=(ω∗,λ∗))T d > 0 ∀i ∈ I(λ∗),

where ¯ω := (ωj )j∈I(ω∗).

Proof See Appendix B.

(28)

(29)

(cid:3)

3.2.2 Assumptions on density function

We have mentioned in the Introduction that the setting of all our analysis is
based on density functions. That is, we wish to prove all our convergence results
by solely looking at density functions used to induce the smoothing functions.
To this end, we must be able to identify necessary properties of a given density

Springer Nature 2021 LATEX template

14

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

function so that Algorithm 1 converges to a candidate solution of the main
problem (3). Indeed, the novelty of our work is precisely the identiﬁcation of
these required properties and its application to the convergence analysis.

We summarize necessary assumptions on the density function ρ that we

will use in the next subsection.

Assumption C. Let ρ : (cid:60) → (cid:60)+ be a density function. Then, the following
properties hold:

(C1) ρ is symmetric, i.e. ρ(x) = ρ(−x) for all x ∈ (cid:60).
(C2) ρ is continuous and nonincreasing on [0, ∞).
(C3) There exist positive constants c, r > 0 such that

S
(cid:90)

2

0

ρ(x) dx ≥ 1 −

c
Sr + c

for all S ≥ 0.

(C4) If p = 1, we have ρ(x) > 0 for all x ∈ (cid:60).

Some remarks are in order: First, although Assumption (C1) was already
supposed in Section 2.3, we have restated it for later use. Under this assump-
tion, note that the smoothing function φ(µ, x) is strictly positive for all µ > 0
and x ∈ (cid:60). Indeed, we already have from Proposition 2.7(b) that φ(µ, x) > 0
for x (cid:54)= 0. On the other hand, since ρ is a symmetric density function by
Assumption (C1), then ρ is not identical to the zero function on the inter-
val [0, ∞). Consequently, (cid:82) +∞
tρ(t)dt > 0, which together with (14) and (17)
yields φ(µ, 0) > 0. Moreover, we can easily calculate the ﬁrst and second
derivatives of the induced φ(µ, x) as

0

φ(cid:48)(µ, x) = 2sgn(x)

(cid:90) |x|

0

(cid:19)

1
µ

ρ

(cid:18) t
µ

dt = 2sgn(x)

(cid:90) |x|

µ

0

ρ(t) dt,

(30)

and

φ(cid:48)(cid:48)(µ, x) =

2
µ

ρ

(cid:18) x
µ

(cid:19)

,

(31)

respectively. From equation (31) and strict positivity of φ(µ, x), we see
that φ(µ, ·) is twice continuously diﬀerentiable approximation of |x| by the
continuity assumption in (C2).

Assumptions (C1) and (C2) will also have other important roles in the
proofs of our main result. On the other hand, the other two technical
assumptions on ρ, namely (C3) and (C4), are important in our subsequent
analysis. Without knowing deﬁnitively the formula for φ(µ, x), the analysis
becomes extremely diﬃcult. In particular, it is challenging to understand the

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

15

convergence behavior of the sequence (cid:8)Sk

j

(cid:9), where

Sk

j :=

|ωk
j |
µk−1

,

(32)

1 , . . . , ωk

n) and µk−1 are generated from Algorithm 1 when ωk

ωk = (ωk
j → 0
as k → ∞ for some j ∈ {1, . . . , n}. Nonetheless, this problem can be allevi-
ated thanks to the simple Assumption (C3). On the other hand, Assumption
(C4) will later be important in proving the unboundedness of the sequence
(cid:8)|∇2ϕµk−1(ωk))jj|(cid:9) when p = 1 and ωk
j → 0 as k → ∞. Interestingly, we shall
see shortly that (C4) is not needed for the case p ∈ (0, 1).

In the next subsection, we will see in great detail how these assumptions on
ρ will play a central role in establishing the main convergence result. In Section
4.1, we will provide some speciﬁc examples of density functions satisfying
Assumption (C).

3.3 Convergence analysis

We now prove our main result that accumulation points of the sequence gen-
erated by Algorithm 1 are in fact SB-KKT points (see Deﬁnition 2.3), which
in turn are candidate solutions for (6).

Theorem 3.1 Any accumulation point of the sequence {(ωk, λk, ζk, ηk)} satisﬁes the
SB-KKT conditions (7)-(12) for the original problem (3) provided that Assumptions
(A), (B) and (C) hold.

An accumulation point of {(ωk, λk, ζ k, ηk)} will be denoted by
(ω∗, λ∗, ζ ∗, η∗). To prove Theorem 3.1, we need to show that (ω∗, λ∗, ζ ∗, η∗)
satisﬁes equations (7)-(12). We show this by proving a series of lemmas. In
particular, we do the following:

j }k∈K is bounded, where Sk

(i) Prove that {Sk
j is given by (32), j ∈ I(ω∗),
K ⊂ {1, 2, . . . , } such that (ωk, λk) → (ω∗, λ∗) as k ∈ K → ∞ and {(ωk, λk)}
is generated by Algorithm 1. Such an index set K exists by Assumption (B2);
(ii) Compute the limit of the sequence (cid:8)(∇2ϕµk−1(ωk))jj
k∈K, where j ∈
I(ω∗) and the index set K is as described in (i);
(iii) Compute
limit
(cid:8)(∇2ϕµk−1(ωk))jj
(iv) Prove that the sequence {(ζ k, ηk)} generated by Algorithm 1 is bounded,
and thus, accumulation points of the sequence of iterates exist;
(v) Prove that any accumulation point (ω∗, λ∗, ζ ∗, η∗) of the sequence
{(ωk, λk, ζ k, ηk)} generated by Algorithm 1 satisﬁes equations (9) and (10);
and ﬁnally,

the
the
(cid:9), where j /∈ I(ω∗);

(cid:8)(∇ϕµk−1 (ωk))j

sequences

and

of

(cid:9)

(cid:9)

Springer Nature 2021 LATEX template

16

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

the
k ∇2ϕµk−1(ωk)(cid:9)

(vi) Compute
(cid:8)W 2
(ω∗, 0), where Wk := diag(ωk).

limits of

k∈K and
k∈K for an arbitrary sequence {(ωk, µk−1)} converging to

sequences (cid:8)Wk∇ϕµk−1 (ωk)(cid:9)

the

in
The above objectives are formally stated and proved, respectively,
Lemma 3.2 to Lemma 3.7. We will prove these results without knowledge of
the exact formula for the smoothing function φ(µ, x) used to construct ϕµ
given by (19), that is, only using Assumption (C) on the density function.

In [23], the authors proved that when ψ(t) = t and φ(µ, x) = (cid:112)x2 + µ2

and under Assumptions (B1)-(B3), there exists some γ > 0 such that

k−1 ≥ γ|ωk
µ2
j |

2
2−p

(j ∈ I(ω∗))

(33)

for all suﬃciently large k ∈ K, where K is the subsequence described in the
above (i). This result is especially important in proving results analogous to
(ii)-(iv) above. However, in order to derive inequality (33), [23] takes advantage
of the speciﬁc function φ(µ, x) chosen, which is not the case in the present
manuscript. Nevertheless, we have found out that such a strong result is not
necessarily required to prove (ii)-(iv). In particular, it suﬃces to establish (i),
which is indeed a weaker property. To this end, Assumption (C3) will play a
very signiﬁcant role without which the analysis becomes extremely diﬃcult.

First of all, it can be easily calculated that the components of ∇ϕµ(ω) ∈ (cid:60)n

are given by

(∇ϕµ(ω))j = pψ(cid:48) ([φ(µ, ωj)]p) φ(cid:48)(µ, ωj)[φ(µ, ωj)]p−1,

(34)

while ∇2ϕµ(ω) ∈ (cid:60)n×n is a diagonal matrix whose diagonal entries are given
by

(∇2ϕµ(ω))jj = p2ψ(cid:48)(cid:48) ([φ(µ, ωj)]p) [φ(cid:48)(µ, ωj)[φ(µ, ωj)]p−1]2

+ p(p − 1)ψ(cid:48) ([φ(µ, ωj)]p) [φ(cid:48)(µ, ωj)]2[φ(µ, ωj)]p−2
+ pψ(cid:48) ([φ(µ, ωj)]p) [φ(µ, ωj)]p−1φ(cid:48)(cid:48)(µ, ωj),

(35)

for j = 1, . . . , n. We note that as discussed in Section 3.2.2, the smoothing
function φ(µ, x) is strictly positive under Assumption (C1) and thus, the factor
[φ(µ, ωj)]p−1 that appears in (34) and (35) is real-valued. In addition, it is
clear from (35) that the components of the Hessian of ϕµ are continuous by
Assumption (A), equation (31), and Assumptions (C1)-(C2).

We now prove our ﬁrst lemma which establishes property (i).

Lemma 3.2 Suppose that Assumptions (B1)-(B3) and (C1)-(C3) hold. Let (ω∗, λ∗)
be an arbitrary accumulation point of the sequence {(ωk, λk)} generated by Algo-
rithm 1, and let {(ωk, λk)}k∈K be an arbitrary subsequence converging to (ω∗, λ∗). If
j ∈ I(ω∗), then {Sk
j → 0 as k ∈ K → ∞ if p ∈ (0, 1).

j }k∈K is bounded. Moreover, Sk

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

17

Proof Denote

Fj (ωk, ¯λk) :=

∂G(ωk, ¯λk)
∂ωj

.

From equation (24) and (34), we have
j )]p(cid:17)
1 ψ(cid:48) (cid:16)
Fj (ωk, ¯λk)+pλk
Case 1. Suppose that p = 1. Then

[φ(µk−1, ωk

φ(cid:48)(µk−1, ωk

j )[φ(µk−1, ωk

j )]p−1 = (εk−1

4

(36)

)j . (37)

1 ψ(cid:48)(φ(µk−1, ωk

Fj (ωk, ¯λk) + λk

(38)
Rearranging the terms and using Assumptions (A) and (C3), there are constants
c, r > 0 such that for suﬃciently large k,
)j − Fj (ωk, ¯λk)|

(cid:90) Sk
j

)j .

j ))φ(cid:48)(µk−1, ωk

j ) = (εk−1

4

= |φ(cid:48)(µk−1, ωk

j )| = 2

ρ(s) ds ≥ 1 −

|(εk−1
4
1 ψ(cid:48)(φ(µk−1, ωk
λk

j ))

c
j )r + c

,

(Sk

0
where the second equality holds by (30), and λk
Assumption (B1). Consequently, we get

1 > 0 for suﬃciently large k by

c
j )r + c

(Sk

≥ 1 −

)j − Fj (ωk, ¯λk)|

|(εk−1
4
1 ψ(cid:48)(φ(µk−1, ωk
λk

1 ψ(cid:48)(φ(µk−1, ωk
λk

j ))
)j − Fj (ωk, ¯λk)|
j )) − |(εk−1
λk
1 ψ(cid:48)(φ(µk−1, ωk
j ))
)j − Fj (ωk, ¯λk)| > 0 for all
j )) − |(εk−1

4

=

large k by

1 ψ(cid:48)(φ(µk−1, ωk

Note that λk
Assumption (B3), Proposition 2.7(c) and (38). Then
j )r + c
c

(Sk
j )r
c

(Sk

0 ≤

≤

≤

4

1 ψ(cid:48)(φ(µk−1, ωk
λk
j )) − |(εk−1

4

j ))
)j − Fj (ωk, ¯λk)|

as k ∈ K → ∞

1 ψ(cid:48)(φ(µk−1, ωk
λk
1ψ(cid:48)(0)
λ∗
1ψ(cid:48)(0) − |Fj (ω∗, ¯λ∗)|
λ∗

→

where the ﬁniteness of the limit is guaranteed by Assumption (B3). Hence, it easily
follows that {Sk

j }k∈K is bounded.

Case 2. Now, suppose p ∈ (0, 1). From equation (37), we have

|(εk−1
4
pλk

)j − Fj (ωk, ¯λk)|
j ))

1 ψ(cid:48)(φ(µk−1, ωk

= |φ(cid:48)(µk−1, ωk

j )| · [φ(µk−1, ωk

j )]p−1.

Using Assumptions (C1) and (C3) and by (30), we get

[φ(µk−1, ωk

|(εk−1
4
pλk
Meanwhile, note that since 1 − p > 0 and ωk

)j − Fj (ωk, ¯λk)|
j ))

1 ψ(cid:48)(φ(µk−1, ωk

j )]1−p ·

= |φ(cid:48)(µk−1, ωk

j )| ≥ 1 −

c
j )r + c

(Sk

≥ 0.

j → 0 as k ∈ K → ∞, we have

Since λ∗

lim
k∈K→∞

[φ(µk−1, ωk
1 > 0 by Assumption (B1) and ψ(cid:48)(0) > 0 by Assumption (A), then
)j − Fj (ωk, ¯λk)|
j ))

|Fj (ω∗, ¯λ∗)|
pλ∗
1ψ(cid:48)(0)

1 ψ(cid:48)(φ(µk−1, ωk

|(εk−1
4
pλk

j )]1−p = 0.

lim
k∈K→∞

=

.

Thus,

lim
k∈K→∞

c
j )r + c

(Sk

= 1.

It follows that limk∈K→∞ Sk

j = 0, as desired. This completes the proof.

(cid:3)

Springer Nature 2021 LATEX template

18

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

We now focus on the sequence (cid:8)∇2ϕµk−1 (ωk)(cid:9)

k∈K. Using Lemma 3.2, we
prove the following important result, which will later be used to establish the
boundedness of the sequence {ζ k, ηk} generated by Algorithm 1.

Lemma 3.3 Suppose that Assumptions (B1)-(B3) and (C1)-(C4) hold. Let (ω∗, λ∗)
be an arbitrary accumulation point of the sequence {(ωk, λk)} generated by Algo-
rithm 1 and let {(ωk, λk)}k∈K be an arbitrary subsequence converging to (ω∗, λ∗).
Then,

lim
k∈K→∞

|(∇2ϕµk−1 (ωk))jj | = ∞ for j ∈ I(ω∗).

Proof We ﬁrst consider the case when p = 1. In this instance, we have from equation
(35) and (31) that

(∇2ϕµk−1 (ωk))jj = φ(cid:48)(cid:48)(µk−1, ωk

j ) =

2
µk−1

ρ

(cid:32) ωk
j
µk−1

(cid:33)

.

By Lemma 3.2, there exists M > 0 such that
nonincreasing on [0, ∞) by Assumption (C2), we have
(cid:33)

|ωk
j |
µk−1

lim
k∈K→∞

(∇2ϕµk−1 (ωk))jj = lim

k∈K→∞

2
µk−1

ρ

(cid:32) ωk
j
µk−1

≥ lim

k∈K→∞

2
µk−1

ρ(M ) = ∞,

≤ M for all k ∈ K. Since ρ is

where the rightmost equality holds since ρ(t) > 0 on (cid:60) by Assumption (C4). This
proves the claim for p = 1.

We now consider the case when 0 < p < 1. We consider two disjoint subsets of K
and U j
U j
1 := {k ∈ K | ωk
and the corresponding subsequences. For k ∈ U j
0. From (35), we have

2 := {k ∈ K | ωk
1 , we get from (30) that φ(cid:48)(µk−1, 0) =

j (cid:54)= 0},

j = 0},

(∇2ϕµk−1 (ωk))jj = pψ(cid:48) (cid:16)

[φ(µk−1, ωk

j )]p(cid:17)

[φ(µk−1, ωk

j )]p−1φ(cid:48)(cid:48)(µk−1, ωk

j ).

(39)

Meanwhile,

lim
1 →∞

k∈U j

φ(cid:48)(cid:48)(µk−1, ωk

j ) = lim

k∈U j

1 →∞

2
µk−1

ρ

(cid:33)

(cid:32) ωk
j
µk−1

= lim
k∈U j

1 →∞

2
µk−1

ρ(0) = ∞,

(40)
where we note that ρ(0) > 0 by Assumption (C1) and the deﬁnition of density
function. Moreover, it is clear that

lim
1 →∞

k∈U j

[φ(µk−1, ωk

j )]p−1 = ∞.

(41)

It follows from (39), (40), (41) and Assumption (A) that

lim
1 →∞

k∈U j

|(∇2ϕµk−1 (ωk))jj | = ∞.

For k ∈ U j

2 , we obtain

lim
2 →∞

k∈U j

φ(cid:48)(cid:48)(µk−1, ωk

j ) = lim
k→∞

2
µk−1

ρ

(cid:33)

(cid:32) ωk
j
µk−1

= ∞.

(42)

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

19

by using equation (31) and the facts that limk∈U j
and ρ is continuous by Assumption (C2).

2 →∞

|ωk
j |
µk−1

= 0 from Lemma 3.2

Meanwhile, φ(cid:48)(cid:48)(µk−1, ωk

j ) (cid:54)= 0 for suﬃciently large k using Lemma 3.2, equation
(31) and Assumption (C1). Thus, invoking Assumption (A), we have for large k ∈ U j
2
that

ψ(cid:48)(cid:48) (cid:16)

[φ(µk−1, ωk

j )]p(cid:17) [φ(cid:48)(µk−1, ωk

j )]2[φ(µk−1, ωk

j )]p−1

= ψ(cid:48)(cid:48) (cid:16)

[φ(µk−1, ωk

j )]p(cid:17) [φ(cid:48)(µk−1, ωk
φ(cid:48)(cid:48)(µk−1, ωk

φ(cid:48)(cid:48)(µk−1, ωk
j )
j )]p
j )]2[φ(µk−1, ωk
j )φ(µk−1, ωk
j )

≥ −β

[φ(cid:48)(µk−1, ωk

φ(cid:48)(cid:48)(µk−1, ωk

j )]2[φ(µk−1, ωk
j )]p
j )φ(µk−1, ωk
j )

.

(43)

Using (31) and Proposition 2.7(b), we have

φ(cid:48)(cid:48)(µk−1, ωk

j )φ(µk−1, ωk

j ) ≥

2
µk−1

ρ

(cid:33)

(cid:32) ωk
j
µk−1

|ωk

j | = 2Sk

j ρ(Sk

j ).

Using this fact together with (43) and equation (30), we obtain
j )]2[φ(µk−1, ωk

j )]p−1

ψ(cid:48)(cid:48) (cid:16)

[φ(µk−1, ωk

j )]p(cid:17) [φ(cid:48)(µk−1, ωk

φ(cid:48)(cid:48)(µk−1, ωk
j )

(cid:20)

2

(cid:82) Sk
0 ρ(t)dt

j

(cid:21)2

[φ(µk−1, ωk

j )]p

≥ −β

≥ −β

2Sk

Sk
j ρ(Sk
j )
j (ρ(0))2[φ(µk−1, ωk
ρ(Sk
j )
(cid:20)

j )]p

(cid:82) Sk
0 ρ(t)dt

j

→ 0 as k ∈ U j

2 → ∞,

(44)

(cid:21)2

≤ (Sk

j )2(ρ(0))2. Similarly, we also

where the last inequality holds since

have

(p − 1)

[φ(cid:48)(µk−1, ωk

j )]2
j )φ(µk−1, ωk
j )

φ(cid:48)(cid:48)(µk−1, ωk

≥ (p − 1)

≥ (p − 1)

(cid:20)

(cid:21)2

j

2

(cid:82) Sk
0 ρ(t)dt
j ρ(Sk
Sk
j )
j (ρ(0))2
ρ(Sk
j )
k := φ(cid:48)(µk−1, ωk

2Sk

→ 0 as k ∈ U j

2 → ∞. (45)

j ), φ(cid:48)(cid:48)

k := φ(cid:48)(cid:48)(µk−1, ωk

j ).

For brevity, denote φk := φ(µk−1, ωk
Then (35) can be written as

j ), φ(cid:48)

(∇2ϕµk−1 (ωk))jj = pφ(cid:48)(cid:48)

kφp−1
k

(cid:34)
pψ(cid:48)(cid:48) (cid:0)φp
k

(cid:1) [φ(cid:48)

k]2φp−1
φ(cid:48)(cid:48)
k

k

Since 0 < ψ(cid:48)(t) ≤ α by Assumption (A), we have

+ (p − 1)ψ(cid:48) (cid:0)φp
k

k]2
(cid:1) [φ(cid:48)
φ(cid:48)(cid:48)
kφk

+ ψ(cid:48) (cid:0)φp
k

(cid:35)
(cid:1)

.

(46)

(cid:34)

pψ(cid:48)(cid:48) (cid:0)φp
k

lim
2 →∞

k∈U j

(cid:1) [φ(cid:48)

k]2φp−1
φ(cid:48)(cid:48)
k

k

+ (p − 1)ψ(cid:48) (cid:0)φp
k

k]2
(cid:1) [φ(cid:48)
φ(cid:48)(cid:48)
kφk

+ ψ(cid:48) (cid:0)φp
k

(cid:35)
(cid:1)

> 0

Springer Nature 2021 LATEX template

20

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

by using the obtained limits (44) and (45). On the other hand, it is clear from (42)
that φ(cid:48)(cid:48)

k → ∞ as k ∈ U j

kφp−1

2 → ∞. Hence, taking the limit in (46),
(∇2ϕµk−1 (ωk))jj = ∞.

lim
2 →∞

k∈U j

This completes the proof.

(cid:3)

We next prove the following lemma which will be used to prove the bound-
edness of the iterates {(ζ k, ηk)}. In this result, we do not actually need
{ωk, µk−1} to be generated from the algorithm.

Lemma 3.4 Suppose that Assumption (C1)-(C2) hold. Let {(ωk, µk−1)} ⊆ (cid:60)n ×
(cid:60)++ be an arbitrary sequence converging to (ω∗, 0). Then for j /∈ I(ω∗),
j )|ω∗

j |p−1ψ(cid:48) (cid:0)|ω∗

j |p(cid:1) ,

(47)

lim
k→∞

(∇ϕµk−1 (ωk))j = psgn(ω∗
(∇2ϕµk−1 (ωk))jj = p2ψ(cid:48)(cid:48) (cid:0)|ω∗

lim
k→∞

j |p(cid:1) |ω∗

j |2p−2 + p(p − 1)ψ(cid:48) (cid:0)|ω∗

j |p(cid:1) |ω∗

j |p−2. (48)

Proof Let j /∈ I(ω∗). We have from (18) that
φ(cid:48)(µk−1, ωk

lim
k→∞

j ) = sgn(ω∗

j ).

Since φ(µ, x) is a smoothing function of |x| that is strictly positive by Assumption
(C1), we have [φ(µk−1, ωk
j |p−1 as k → ∞. Moreover, since ψ is C2 by
Assumption (A), then we easily obtain (47) by letting k → ∞ in equation (34).

j )]p−1 → |ω∗

For equation (48), we ﬁrst show that limk→∞ φ(cid:48)(cid:48)(µk−1, ωk

j ) = 0, which by (31)

is equivalent to showing that

1
µk−1

ρ

(cid:33)

(cid:32) ωk
j
µk−1

lim
k→∞

= lim
k→∞

1
µk−1

ρ

(cid:33)

(cid:32) |ωk
j |
µk−1

= 0.

(49)

Since j /∈ I(ω∗), then |ω∗
(cid:32) ωk
j
µk−1

1
µk−1

0 ≤

ρ

j |/2 < |ωk
(cid:33)

j | for all k suﬃciently large. Thus,

1
µk−1

(cid:33)

(cid:32) ω∗
j
2µk−1

ρ

→ 2δ(ω∗

j ) = 0

as k → ∞,

≤

by invoking Assumptions (C1)-(C2) and the deﬁnition of the Dirac delta function.
This proves (49). Finally, taking the limit in (35) when k → ∞, we obtain

lim
k→∞
= p2ψ(cid:48)(cid:48) (cid:0)|ω∗
= p2ψ(cid:48)(cid:48) (cid:0)|ω∗

(∇2ϕµk−1 (ωk))jj
j |p(cid:1) [sgn(ω∗
j |p(cid:1) |ω∗
This completes the proof of the lemma.

j )|ω∗

j |p−1]2 + p(p − 1)ψ(cid:48) (cid:0)|ω∗

j |p(cid:1) [sgn(ω∗

j )]2|ω∗

j |p−2

j |2p−2 + p(p − 1)ψ(cid:48) (cid:0)|ω∗

j |p(cid:1) |ω∗

j |p−2.

(cid:3)

We will now show the boundedness of the sequence of Lagrange multiplier
vectors {(ζ k, ηk)} in the following lemma. The proof follows the same ideas as
those in [23, Proposition 9], but is nonetheless presented to show speciﬁcally
why the result holds when a general smoothing function is used.

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

21

Lemma 3.5 Suppose that Assumptions (B1)-(B4) and (C1)-(C4) hold. Let
{(ζk, ηk)} ⊆ (cid:60)n ×(cid:60)r be a sequence of the accompanying Lagrange multiplier vectors
generated by Algorithm 1. Then {(ζk, ηk)} is bounded.

Proof For convenience, denote

ξk := ((ζk)T , (ηk)T )T ,

ˆζk :=

ζk
(cid:107)ξk(cid:107)

,

ˆηk :=

ηk
(cid:107)ξk(cid:107)

for each k. We prove by contradiction that the sequence {(ξk, ηk)} is bounded.
Without loss of generality, we may assume that
ξk
(cid:107)ξk(cid:107)

(cid:107)ξk(cid:107) → ∞,

lim
k→∞

= ˆξ∗,

where ˆξ∗ := ((ζ∗)T , (η∗)T )T with ζ∗ and η∗ are accumulation points of {ˆζk} and
{ˆηk}, respectively. By Assumption (B2), we may suppose without loss of generality
that limk→∞(ωk, λk) = (ω∗, λ∗). Dividing by (cid:107)ξk(cid:107) both sides of (21),(22),(23) and
(25) evaluated at (ω, λ, ζ, η) = (ωk, λk, ζk, ηk), we have for each k the following
equations:
(∇f (ωk))j
(cid:107)ξk(cid:107)

ωωG(ωk, ¯λk)ˆζk(cid:17)
(cid:16)
∇2

1 (∇2ϕµk−1 (ωk))jj

(εk−1
1
(cid:107)ξk(cid:107)

ˆζk
j =

+λk

)j

+

j

(j = 1, 2, . . . , n),

∇ϕµk−1 (ωk)T ˆζk − ˆηk

1 =

εk−1
2
(cid:107)ξk(cid:107)

,

∇Rj (ωk)T ˆζk − ˆηk

j =

)j

(εk−1
3
(cid:107)ξk(cid:107)

(j = 2, 3, . . . , r),

(50)

(51)

j ˆηk
λk

j ≤

r
(cid:88)

i=1

j ˆηk
λk

j =

εk−1
5
(cid:107)ξk(cid:107)

,

λk
j ≥ 0,

ˆηk
j ≥ 0

(j = 1, 2, 3, . . . , r),

(52)

where
(εk−1
, εk−1
2
1
limk→∞ λk

(ω, λ, ζ, η)
, εk−1
3
1 = λ∗

, εk−1
4

=
, εk−1
5

(ωk, λk, ζk, ηk)

and

(ε1, ε2, ε3, ε4, ε5)

=

). Since limk→∞

εk−1
(cid:107)ξk(cid:107) = 0,
l

l = 1, 2, 3, 5, and

1 > 0 by Assumption (B1), letting k → ∞ in inequality (52) gives

ˆη∗
1 = 0,

and similarly,

j = 0 (j /∈ I(λ∗)) and ˆη∗
ˆη∗

j ≥ 0 (j ∈ I(λ∗)).

Since (cid:107) ˆξ∗(cid:107) = 1, we get from (54) that

1 = (cid:107)ˆζ∗(cid:107)2 + (cid:107)ˆη∗(cid:107)2 = (cid:107)ˆζ∗(cid:107)2 +

(cid:88)

|ˆη∗

j |2.

j∈I(λ∗)

(53)

(54)

(55)

Meanwhile, since G is twice continuously diﬀerentiable and f is continuously diﬀer-
entiable, then both {∇2
boundedness of {λk
ˆζk
j = 0 for j ∈ I(ω∗) by using Assumption (B1) and Lemma 3.3. That is,
limk→∞

ˆζk
j } for each j by (50). Consequently, we obtain

are bounded. This implies the

ωωG(ωk, ¯λk)ˆζk} and

1 (∇2ϕµk−1 (ωk))jj

(cid:110) ∇f (ωk)
(cid:107)ξk(cid:107)

(cid:111)

j = 0 (j ∈ I(ω∗)).
ˆζ∗

(56)

Springer Nature 2021 LATEX template

22

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

By invoking Assumption (B1) and letting k → ∞ in (24), it is clear that

|∇(ϕµk−1 (ωk))j | =

lim
k→∞

|Fj (ω∗, ¯λ∗)|
λ∗
1

,

where Fj (ω∗, ¯λ∗) is given by (36). This together with (56) gives us
(cid:88)

(∇ϕµk−1 (ωk))j

ˆζk
j = 0.

lim
k→∞

j∈I(ω∗)

(57)

(58)

On the other hand, we get

∇ϕµk−1 (ωk)T ˆζk = 0

lim
k→∞

by letting k → ∞ in (51) and by using (53). Together with (58), it yields


0 = lim
k→∞

∇ϕµk−1 (ωk)T ˆζk = lim
k→∞

= lim
k→∞

(cid:88)

=

(cid:88)



(∇ϕµk−1 (ωk))j

ˆζk
j +

(cid:88)

(∇ϕµk−1 (ωk))j

ˆζk
j

j∈I(ω∗)
(cid:88)

(∇ϕµk−1 (ωk))j

ˆζk
j

j /∈I(ω∗)

j /∈I(ω∗)

psgn(ω∗

j )|ω∗

j |p−1ψ(cid:48)(|ω∗

j |p)ˆζ∗
j ,





j /∈I(ω∗)

where the last equality holds by (47). With this equation, we may employ similar
techniques as in the proof of [23, Proposition 9] to derive the equation

(cid:88)

j /∈I(ω∗)

j ∇(¯ω,λ)Φj (ω∗, λ∗) −
ˆζ∗

(cid:88)

j∈I(λ∗)

j ∇(¯ω,λ)λj (ω∗, λ∗) = 0,
ˆη∗

(59)

where ¯ω := (ωj )j /∈I(ω∗) and Φj (ω, λ) (j /∈ I(ω∗)) are as deﬁned in Assumption (B4).
On the other hand, by Lemma 3.1, we can ﬁnd a vector d ∈ (cid:60)n−|I(ω∗)|+r such that
(28) and (29) hold. From (59), we have
j ∇(¯ω,λ)Φj (ω∗, λ∗)T d −
ˆζ∗

j ∇(¯ω,λ)λj (ω∗, λ∗)T d = 0.
ˆη∗

(cid:88)

(cid:88)

j /∈I(ω∗)

j∈I(λ∗)

Together with equation (28), we obtain

(cid:88)

j∈I(λ∗)

j ∇(¯ω,λ)λj (ω∗, λ∗)T d = 0.
ˆη∗

Consequently, we have from (54) and (29) that ˆη∗
implies that

j = 0 for all j ∈ I(λ∗). In turn, (59)

(cid:88)

j ∇(¯ω,λ)Φj (ω∗, λ∗) = 0.
ˆζ∗

j /∈I(ω∗)

Since {∇(¯ω,λ)Φj (ω∗, λ∗)}j /∈I(ω∗) is linearly independent by Lemma 3.1, then ˆζ∗
j = 0
for all j /∈ I(ω∗). Together with (56), we have ˆζ∗ = 0 which in turn implies that
(cid:107)ˆζ∗(cid:107)2 + (cid:80)
j | = 0. This contradicts (55). Therefore, the sequence {(ζk, ηk)}
(cid:3)
is bounded.

j∈I(λ∗)|ˆη∗

By

the

above

of
{(ωk, λk, ζ k, ηk)} exist. We now show that such point satisﬁes condition
(9)-(10).

accumulation points

lemma, we

know that

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

23

Lemma 3.6 Suppose that Assumptions (B1)-(B4) and (C1)-(C4) hold. Then, an
accumulation point of the sequence {(ωk, λk, ζk, ηk)} generated by Algorithm 1
satisﬁes

(i) ζ ∗
(ii) p (cid:80)

j = 0 for all j ∈ I(ω∗) satisﬁes condition (10).

j /∈I(ω∗) sgn(ω∗

j )|ω∗

j |p−1ψ(cid:48)(|ω∗

j |p)ζ ∗

j = η∗

1 satisﬁes condition (9).

Proof The proof essentially follows ideas similar to Lemma 3.5. Indeed, from Lemma
3.5 and Assumption (B2), the sequence {(ωk, λk, ζk, ηk)} is bounded. Hence, without
loss of generality, we assume that

(ωk, λk, ζk, ηk) = (ω∗, λ∗, ζ∗, η∗).

lim
k→∞

(60)

1 (∇2ϕµk−1 (ωk))jj ζk

Since G is twice continuously diﬀerentiable and f is continuously diﬀerentiable, then
{λk
j } is bounded for each j by using (60) and equation (21). From
Lemma 3.3 and Assumption (B1), we obtain that limk→∞ ζk
j = 0 for j ∈ I(ω∗), that
is, ζ∗

j = 0 for j ∈ I(ω∗). This proves (i).
We now prove (ii). From (57), we know that {(∇ϕµk−1 (ωk))j } is bounded.
j = 0, (j ∈ I(ω∗)) and

Hence, we obtain from item (i) that limk→∞(∇ϕµk−1 (ωk))j ζk
therefore,

lim
k→∞

(cid:88)

j∈I(ω∗)

(∇ϕµk−1 (ωk))j ζk

j = 0.

Together with (47), it follows that

lim
k→∞

∇ϕµk−1 (ωk)T ζk = lim
k→∞

(cid:88)

(∇ϕµk−1 (ωk))j ζk
j

j /∈I(ω∗)

(cid:88)

=

j /∈I(ω∗)

psgn(ω∗

j )|ω∗

j |p−1ψ(cid:48)(|ω∗

j |p)ζ∗
j .

Using this and taking the limit in (22), we obtain item (ii). The proof is complete.

(cid:3)

We now establish our last lemma which will later be used to prove that
accumulation points of the sequence generated by the algorithm satisfy con-
ditions (7)-(8). The following result, similar to Lemma 3.4,
is in fact a
general formula. That is, the sequence {(ωk, µk−1)} need not be generated by
Algorithm 1.

Lemma 3.7 Suppose that Assumption (C1)-(C2) hold. Let {(ωk, µk)} ⊆ (cid:60)n × (cid:60)++
be an arbitrary sequence converging to (ω∗, 0). Then, we have

(i) limk→∞ Wk∇ϕµk−1(ωk) = p|W ∗|pψ(cid:48)(|ω∗|p).
(ii) limk→∞ W 2

k ∇2ϕµk−1 (ωk)
p2diag(|W ∗|2pψ(cid:48)(cid:48)(|ω∗|p))

=

p(p − 1)diag(|W ∗|pψ(cid:48)(|ω∗|p)) +

where Wk := diag(ωk) for each k, |W ∗|p := diag(|ω∗|p), and |W ∗|2p := diag(|ω∗|2p).

Springer Nature 2021 LATEX template

24

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

Proof We show ﬁrst that (i) holds. To this end, we need to show that for each
j ∈ {1, . . . , n}, we have

j ψ(cid:48) (cid:16)
pωk
j |pψ(cid:48) (cid:0)|ω∗

lim
k→∞

j (∇ϕµk−1 (ωk))j = lim
ωk
k→∞
= p|ω∗

[φ(µk−1, ωk
j |p(cid:1) ,
(61)
where the ﬁrst equality holds because of (34). In the case that j /∈ I(ω∗), the above
can be easily veriﬁed by using equation (47). Now, suppose that j ∈ I(ω∗), i.e.
ω∗

j = 0. Let K = {1, 2, . . . } and as in Lemma 3.3, deﬁne

j )[φ(µk−1, ωk

φ(cid:48)(µk−1, ωk

j )]p(cid:17)

j )]p−1

U j
1 := {k ∈ K | ωk

j = 0},

and U j

2 := {k ∈ K | ωk

j (cid:54)= 0}.

It is clear that

lim
1 →∞

k∈U j

j (∇ϕµk−1 (ωk))j = 0.
ωk

(62)

For k ∈ U j

2 , we have from Proposition 2.7(b) that φ(µk−1, ωk
[φ(µk−1, ωk

j )]p−1 ≤ |ωk

j |p−1

j ) ≥ |ωk

j |, which gives us

for any p ∈ (0, 1]. Together with Proposition 2.7(c), we have
|ωk
Since 0 < ψ(cid:48)(t) ≤ α by Assumption (A), we get

j )[φ(µk−1, ωk

j )]p−1| ≤ |ωk

j φ(cid:48)(µk−1, ωk

j |p−1 = |ωk

j |.|ωk

j |p → |ω∗

j |p = 0 as k ∈ U j

2 → ∞.

lim
2 →∞

k∈U j

j (∇ϕµk−1 (ωk))j = 0.
ωk

(63)

Equations (62) and (63) conﬁrm the desired equation (61).

Next, we show that (ii) holds. It is enough to show that for each j ∈ {1, . . . , n},

we have

(ωk

j )2(∇2ϕµk−1 (ωk))jj = p2ψ(cid:48)(cid:48) (cid:0)|ω∗

j |p(cid:1) |ω∗

j |2p + p(p − 1)ψ(cid:48) (cid:0)|ω∗

j |p(cid:1) |ω∗

j |p. (64)

lim
k→∞

The case when j /∈ I(ω∗) easily follows from (48). Now, suppose that j ∈ I(ω∗). Since
φ is twice continuously diﬀerentiable, we may consider the Maclaurin expansion of φ:

φ(µk−1, ωk

j ) = φ(µk−1, |ωk

j |) = φ(µk−1, 0) + φ(cid:48)(µk−1, 0)|ωk

j | +

φ(cid:48)(cid:48)(µk−1, γk
j )
2

|ωk

j |2

= φ(µk−1, 0) +

φ(cid:48)(cid:48)(µk−1, γk
j )
2

|ωk

j |2

where γk
last equality follows from (30). Now, since φ(µk−1, 0) > 0,

j ∈ [0, |ωk

j |), the ﬁrst equality holds by Assumption (C1) and (17), and the

φ(cid:48)(cid:48)(µk−1, γk

j )|ωk

j |2 = 2(φ(µk−1, ωk

j ) − φ(µk−1, 0)) ≤ 2φ(µk−1, ωk

j ).

Then

[φ(µk−1, ωk

j )]p−1φ(cid:48)(cid:48)(µk−1, γk

j )(ωk

j )2 ≤ 2[φ(µk−1, ωk

j )]p → 2|ω∗

j |p = 0 as k → ∞.

On the other hand, since ρ is nonincreasing on [0, ∞),
(cid:32) |ωk
j |
µk−1

(cid:32) γk
j
µk−1

φ(cid:48)(cid:48)(µk−1, γk

2
µk−1

2
µk−1

j ) =

(cid:33)

≥

ρ

ρ

(65)

(cid:33)

= φ(cid:48)(cid:48)(µk−1, ωk

j ).

(66)

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

25

It follows from (65) and (66) that

lim
k→∞

Next, we will show that

[φ(µk−1, ωk

j )]p−1φ(cid:48)(cid:48)(µk−1, ωk

j )(ωk

j )2 = 0.

(ωk

j )2[φ(cid:48)(µk−1, ωk

j )]2[φ(µk−1, ωk

j )]p−2 = 0.

lim
k→∞

(67)

(68)

Using again Proposition 2.7(b), we get [φ(µk−1, ωk
j )]2[φ(µk−1, ωk
Letting k → ∞, we obtain the desired limit (68). As a consequence of (68), we obtain

j )]p−2 ≤ |ωk
j )]p−2 ≤ |ωk

j |p−2. Then
j |p.

j )2[φ(cid:48)(µk−1, ωk

(ωk

(ωk

j )2[φ(cid:48)(µk−1, ωk

j )[φ(µk−1, ωk

j )]p−1]2 = 0.

(69)

lim
k→∞

Equation (64) now follows by invoking Assumption (A), using the formula (35) and
(cid:3)
the derived limits (67), (68) and (69). This completes the proof of (ii).

Having derived all the necessary lemmas, we can now prove our main result.

Proof of Theorem 3.1 Let (ω∗, λ∗, ζ∗, η∗) be an arbitrary accumulation point of the
sequence (ωk, λk, ζk, ηk). By taking a subsequence, if necessary, we may assume
without loss of generality that

(ωk, λk, ζk, ηk) = (ω∗, λ∗, ζ∗, η∗).

lim
k→∞

We need to show that (ω∗, λ∗, ζ∗, η∗) satisﬁes SB-KKT conditions (7)-(12). Indeed,
multiplying by W 2
k and Wk both sides of (21) and (24) evaluated at (ω, λ, ζ, η) =
(ωk, λk, ζk, ηk), respectively, we have
k ∇f (ωk) + W 2

1 ∇2ϕµk−1 (ωk))ζk = W 2

k εk−1
1

W 2

,

k (∇2
Wk∇ωk G(ωk, ¯λk) + λk
where Wk = diag(ωk), (ω, λ, ζ, η) = (ωk, λk, ζk, ηk) and

ωωG(ωk, ¯λk) + λk

1 Wk∇ϕµk−1 (ωk) = Wkεk−1

4

(ε1, ε2, ε3, ε4, ε5) = (εk−1

1

, εk−1
2

, εk−1
3

, εk−1
4

, εk−1
5

).

Recall that G is twice continuously diﬀerentiable and f is continuously diﬀerentiable.
Thus, letting k → ∞ in the above two equations and using Lemma 3.7, we obtain
that

W 2

∗ ∇f (ω∗) + H(ω∗, λ∗)ζ∗ = 0,

W∗∇ω∗ G(ω∗, ¯λ∗) + λ∗

1W∗∇ϕµk−1 (ω∗) = 0,

where
H(ω, λ) = W 2∇2

ωG(ω, ¯λ)+λ1p(p−1)diag(|W |pψ(cid:48)(|ω|p))+λ1p2diag(|W |2pψ(cid:48)(cid:48)(|ω|p)).
The conditions (9) and (10) can be derived by Lemma 3.6. Let k → ∞ in (23) and
(25) with (ω, λ, ζ, η) = (ωk, λk, ζk, ηk), we obtain the conditions (11) and (12). This
(cid:3)
completes the proof of Theorem 3.1.

Springer Nature 2021 LATEX template

26

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

4 Numerical results

4.1 Examples of Smoothing Functions

A key aspect in successful numerical implementations of a smoothing algorithm
is the choice of the approximating functions. Here, we enumerate six smoothing
functions that we will use in our numerical simulations.

There are many density functions commonly used and called kernel func-
tions in statistics (see also [7]). Some density functions satisfying (13) are given
as follows.

ρ1(x) :=

ρ2(x) :=

ρ3(x) :=

ρ4(x) :=

ρ5(x) :=

ρ6(x) :=

(cid:26) 35

(cid:26) 15

0

32 (1 − x2)3 if |x| ≤ 1,
otherwise.
16 (1 − x2)2 if |x| ≤ 1,
otherwise.

0

(cid:26) 3

4 (1 − x2) if |x| ≤ 1,
otherwise,

∀x ∈ (cid:60).

0
e− x2

2

1
√
2π

e−x
(1 + e−x)2 .
1
(x2 + 4) 3

.

2

Following the discussion in Section 2.3, the corresponding smoothing

functions of |x| are given as follows:

(cid:40)

φ1(µ, x) :=

− 5x8

128µ7 + 7x6

32µ5 − 35x4

64µ3 + 35x2
|x|

32µ + 35µ

128 if |x| ≤ µ,
if |x| > µ.

φ2(µ, x) :=

(cid:40)

φ3(µ, x) :=

(cid:40) x6

16µ5 − 5x4

16µ3 + 15x2
|x|

16µ + 5µ

16 if |x| ≤ µ,
if |x| > µ.

− x4

8µ3 + 3x2
4µ + 3µ
|x|

8 if |x| ≤ µ,
if |x| > µ.

φ4(µ, x) := xerf

(cid:18) x
√

2µ

(cid:19)

+

(cid:114) 2
π

µe− x2
2µ2 ,

φ5(µ, x) := µ (cid:2)log (cid:0)1 + e− x
(cid:112)
φ6(µ, x) :=

4µ2 + x2.

µ (cid:1) + log (cid:0)1 + e

x

µ (cid:1)(cid:3) .

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

27

Here, the error function is deﬁned by

erf(x) =

2
√
π

(cid:90) x

0

e−u2

du ∀x ∈ (cid:60).

The graphs of |x| and φi(µ, x), i = 1, 2, . . . , 6 with µ = 0.25 are illustrated
in Figure 1. From the graphs, we infer the following inequality relating the
smoothing functions:

|x| ≤ φ1(µ, x) ≤ φ2(µ, x) ≤ φ3(µ, x) ≤ φ4(µ, x) ≤ φ5(µ, x) ≤ φ6(µ, x).

It is not diﬃcult to show that the relation |x| ≤ φ1(µ, x) ≤ φ2(µ, x) ≤ φ3(µ, x),
while the proof of the relation φ3(µ, x) ≤ φ4(µ, x) ≤ φ5(µ, x) ≤ φ6(µ, x) can be
found in [25]. On the other hand, the graphs of the corresponding smoothing
functions for |x|p where p ∈ (0, 1] is shown in Figure 2.

Fig. 1 Graph of |x| and φi(µ, x), i = 1, 2, . . . , 6 with µ = 0.25.

The smooth approximation φ(µ, x) := (cid:112)µ2 + x2 is the function used in [23]
i=1|ωi|p (0 < p ≤ 1).
2 , x) and can in fact be obtained by choosing the

for their smoothing algorithm for (3) with R1(ω) := (cid:80)n
This function is simply φ6( µ
density function ρ(t) = 1

2 (x2 + 1)−3/2.

-1-0.8-0.6-0.4-0.200.20.40.60.81x00.20.40.60.811.2i(,x)|x|1(,x)2(,x)3(,x)4(,x)5(,x)6(,x)Springer Nature 2021 LATEX template

28

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

Fig. 2 Graph of |x|p and (φ(µ, x))p, i = 1, 2, . . . , 6 with µ = 0.25 and p = 0.5.

In this paper, we consider the six functions above and determine which
approximation is the best suitable in solving (3) with R1(ω) satisfying Assump-
tion (A). It is easy to check that the six density functions as above satisfy
Assumptions (C1)-(C2). Condition (C3), on the other hand, holds by choosing
c = 4, and r = 2. Indeed,

1 −

4
4 + S2 ≤

(cid:114)

1 −

4

4 + S2 = 2

S
(cid:90)

0

ρ3(s) ds ≤ 2

S
(cid:90)

0

ρi(s) ds ∀i = 1, . . . , 6.

According to Assumption (C4), only the functions ρ4, ρ5 and ρ6 can be used
(theoretically) for the case p = 1.

4.2 Simulations

We compare eﬃciency of the smoothing functions φi (i = 1, 2, . . . , 6) by means
of numerical simulation. The program is coded in MATLAB R2019a and run
on a machine with Intel(R) Core(TM) i7-8565U CPU@1.80GHz and 16.0 GB
RAM.

-1-0.8-0.6-0.4-0.200.20.40.60.81x00.20.40.60.811.2ip(,x)|x|p1p(,x)2p(,x)3p(,x)4p(,x)5p(,x)6p(,x)Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

29

4.2.1 Problem with an Elastic-Net-type regularizer

We solve the following bilevel problem arising from squared linear regression
using an Elastic-Net-type regularizer:

min
ω,λ

s.t.

1
2

(cid:107)A1ω − b1(cid:107)2
2
(cid:26) 1
2

ω ∈ argmin

ˆω

(cid:107)A2 ˆω − b2(cid:107)2

2 + λ1(cid:107)ˆω(cid:107)p

p + λ2(cid:107)ˆω(cid:107)2
2

(cid:27)

(70)

(λ1, λ2) ≥ 0,

where Ai ∈ (cid:60)mi×n and bi ∈ (cid:60)mi for i ∈ {1, 2}. We produce 20 synthetic
problems for each case of (n, m1, m2) = (500, 300, 300), (500, 1000, 1000) by

Ai := rand(mi, n),

(cid:21)

(cid:20)b1
b2

:=

(cid:20)

A1 ∗ θ
A2 ∗ θ + 0.01 ∗ (2 ∗ rand (m2, 1) − ones (m2, 1))

(cid:21)

,

θ := 10 ∗ (cid:2)zeros((cid:98) n

2 (cid:99), 1); rand(n − (cid:98) n

2 (cid:99), 1)(cid:3)

with rand, ones, and zeros being MATLAB commands, and apply Algo-
rithms 1 with the smoothing functions φi (i = 1, 2, . . . , 6) to these problems.
The random number generator is initialized by rng(0.111111). Moreover, the
parameter p in the (cid:96)p-regularizer is set to p = 1, 0.5 for each problem.

In order to compute a KKT point of the smoothed subproblem for (70)
in Step 1 of Algorithm 1, we utilize the MATLAB solver fmincon with
“MaxIterations= 107” and opt for the interior-point method as an algo-
rithm that runs within fmincon.2 As a starting point of fmincon, we choose
ω = 100 ∗ ones(n, 1) at the ﬁrst iteration k = 0 and then use the previous
iteration point (ωk−1, λk−1) after the second iteration, i.e., for k ≥ 1. We set
(µ0, β1) = (1, 0.8) in Step 0 for Algorithm 1. To compute the value of the
function erf in φ4, we utilize the MATLAB function erf.

In light of the SB-KKT conditions (7)-(12) and the value of the smoothing
parameter µ, we terminate the algorithm when either one of the following
criteria is met:

1. The norms of the residuals of the equations in (7)-(12) are smaller than
10−2. As for condition (12), we observe only the value of (λ∗)(cid:62)η∗ because
the nonnegativity of (λ∗, η∗) is assured in the interior-point method. To
estimate the index set I(ω∗) in conditions (9) and (10), we regard ωk
i as
zero if |ωk
2. µk+1 ≤ 10−8.

i | ≤ 10−5.

2One may think of using the SQP method in place of the interior-point method. In the prelim-
inary numerical experiments, fmincon using the interior point method (fminconIPM) performed
better than the one with the SQP method (fminconSQP). Indeed, we observed that fminconIPM
solved each smoothed subproblem for (70) more stably than fminconSQP. We also observed that
fminconIPM was more likely to ﬁnd a positive solution λ1, leading to a sparse solution. This result
may be peculiar to an interior point method that reaches a solution from the interior of a feasible
region.

Springer Nature 2021 LATEX template

30

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

The obtained results are summarized in Tables 1-4, in which each column
is described as follows. Here, the averages are taken over the set of problems
that are counted in success(%).

i:
obj:
sbkkt:
µend:
sparsity(%):

ave.time(s):
ite:
success(%):

the smoothing function φi
averaged objective value at the resulting solution
averaged residual of the SB-KKT conditions
averaged value of µk+1 on termination
averaged ratio of zero elements of the resulting solution ω,
in which each element wi is counted as zero if |ωi| ≤ 10−5
averaged spent time in seconds
averaged number of iterations
ratio of problems for which SB-KKT points are computed successfully
in the sense that the ﬁrst termination condition in the above is satisﬁed

The best values for obj, sbkkt, sparsity(%), and ave.time(s) are displayed
in bold in the tables. In addition, the smallest ones for µend and ite are also in
bold. The obtained results with φ5 were excluded from the tables because the
overﬂow often occurred when computing its gradient as µ gets smaller and it
did not work well. Now, the following insights are obtained from the numerical
results:

Comparison of results with p = 0.5, 1:
From sparsity(%), we see that (cid:96)0.5 tends to attain sparser solutions than (cid:96)1.
For example, by Tables 3 and 4, sparsity(%) for p = 1 takes values less than
10%, whereas they are more than 60% for p = 0.5. This is by virtue of the
nonconvexity of (cid:96)p with p < 1. Moreover, (cid:96)0.5 tends to attain solutions with
better objective values than (cid:96)1. Indeed, values of obj in Table 3 with p = 1 are
around 3.5 · 10−3, while those in Table 4 with p = 0.5 are around 7.2 · 10−4.
Similar observations can be gathered from Tables 1 and 2.

Comparison of results with m2 = 300, 1000:
The problems with m2 < n can be related to the problem of ﬁnding a sparse
solution of underdetermined linear system. Such kind of problems are often
regarded more intractable than those with m2 ≥ n, as illustrated by the
obtained numerical results: Any ave.time(s) are more than 450 seconds in
Tables 1 and 2 with m2 = 300 < n = 500, whereas they are less than 400
seconds in Tables 3 and 4 with m2 = 1000 > n.

Comparison of results of the four smoothing functions:

In view of the objective values and residual of the SB-KKT conditions, the
qualities of the resulting solutions are comparable. Meanwhile, substantial
diﬀerences are observed in tendency of the smoothing parameter µ on termina-
tion, i.e., µend. The function φ6 takes the smallest µend throughout the tables,
while φ1 the largest. A possible reason for this situation is that as illustrated
by Figures 1 and 2, φ6(x) deviates from |x| most, while φ1(x) ﬁts to it the best.

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

31

On the other hand, from ave.time(s), we observe that φ4 attains the small-
est cpu time. In particular, cpu-time per a single iteration for φ4 also tends
to be the smallest. Indeed, for example, from Table 4, ave.time(s)
is around 7.5
seconds for φ4 while more than 8 seconds for the others. This fact may suggest
that smoothed subproblems organized with φ4 are easier for fmincon to solve
than those with the other smoothing functions.

ite

Table 1 Averaged results for (n, m2, m1, p) = (500, 300, 300, 1)

i
1
2
3
4
6

obj
3.77e-03
3.75e-03
3.74e-03
3.76e-03
3.77e-03

sbkkt
8.06e-03
7.75e-03
7.19e-03
8.16e-03
8.10e-03

µend
4.07e-04
3.65e-04
2.97e-04
1.44e-04
5.55e-05

sparsity(%)
5.17
5.17
5.08
5.31
7.96

ave.time(s)
509.95
514.40
505.11
418.04
473.83

ite
35.2
35.7
36.6
39.8
44.1

success(%)
100
95
95
95
100

Table 2 Averaged results for (n, m2, m1, p) = (500, 300, 300, 0.5) (Note: Although obj with
i = 3 is remarkably higher than the others in the table, this is caused by one outlier instance.
After precluding that outlier, we conﬁrmed that the retaken obj was comparative to the others.)

i
1
2
3
4
6

obj
7.88e-04
8.32e-04
6.64e+02
8.34e-04
8.38e-04

sbkkt
5.56e-03
6.16e-03
5.32e-03
6.21e-03
5.94e-03

µend
5.78e-04
5.17e-04
3.44e-04
2.06e-04
1.31e-04

sparsity(%)
54.29
50.68
56.60
52.61
52.07

ave.time(s)
473.49
450.14
524.97
389.90
479.38

ite
34.6
35.3
37.8
39.3
41.7

success(%)
90
95
95
100
90

Table 3 Averaged results for (n, m2, m1, p) = (500, 1000, 1000, 1)

i
1
2
3
4
6

obj
2.76e-03
2.77e-03
2.79e-03
2.78e-03
2.81e-03

sbkkt
8.17e-03
8.33e-03
8.73e-03
8.30e-03
8.50e-03

µend
4.31e-04
3.83e-04
3.27e-04
1.52e-04
7.27e-05

sparsity(%)
5.00
4.83
4.71
5.06
6.49

ave.time(s)
158.12
157.58
170.79
137.85
154.41

ite
34.8
35.3
36
39.5
42.8

success(%)
100
100
100
100
100

4.2.2 Problems with other regularizers

In this section, we solve problem (70) with the regularizers ψ2((cid:107)ω(cid:107)p
p) and
ψ3((cid:107)ω(cid:107)p
p, where ψ2 and ψ3 are deﬁned in Appendix A. We
set a = 1 in ψ2 and ψ3, and also set β1 = 0.5 together with (n, mtr, mval) =
(500, 1000, 1000). The experimental settings are the same as in the previous

p) in place of (cid:107)ω(cid:107)p

Springer Nature 2021 LATEX template

32

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

Table 4 Averaged results for (n, m2, m1, p) = (500, 1000, 1000, 0.5)

i
1
2
3
4
6

obj
7.28e-04
7.28e-04
7.32e-04
7.28e-04
7.27e-04

sbkkt
6.00e-03
6.70e-03
6.46e-03
6.70e-03
6.43e-03

µend
4.51e-04
3.92e-04
3.22e-04
1.62e-04
8.53e-05

sparsity(%)
60.97
61.09
60.75
60.92
61.95

ave.time(s)
338.62
325.36
329.30
298.07
343.90

ite
35.1
35.7
36.5
39.6
42.5

success(%)
100
100
95
100
100

section, and in particular the 20 synthetic problem-data of Ai, bi (i = 1, 2) are
also identical to the ones used there. The obtained results are summarized in
Tables 5 and 6. In both the cases, more than 95% instances are successfully
solved.

In both Tables 5 and 6, we observe that µend for φ6 are the smallest among
the ﬁve smoothing functions, respectively, as well as the results in Table 4
with the (cid:96)0.5 regularizer. Moreover, those of φ1 are almost the largest although
the exactly largest one in Table 5 is attained by φ2. However, unlike the (cid:96)0.5
regularizer, superiority of φ4 in ave.time(s) is not observed. In comparison of
Tables 4, 5, and 6, the (cid:96)0.5 regularizer tends to gain sparser solutions according
to sparsity(%). On the other hand, from ave-time(s), we see that the spent
time for ψ3 tends to be the smaller than those for the (cid:96)0.5-regularizer and ψ2.

Table 5 Averaged results for (n, mtr, mval, p) = (500, 1000, 1000, 0.5) using ψ2

i
1
2
3
4
6

obj
5.53e-03
5.57e-03
5.57e-03
5.53e-03
5.57e-03

sbkkt
5.42e-03
4.37e-03
4.80e-03
4.55e-03
5.40e-03

µend
4.49e-04
5.41e-04
4.36e-04
1.85e-04
9.13e-05

sparsity(%)
26.95
27.40
25.48
28.40
27.10

ave.time(s)
308.42
298.43
290.19
304.39
322.90

ite
36.7
37.0
37.8
41.4
44

success(%)
95
100
100
95
100

Table 6 Averaged results for (n, mtr, mval, p) = (500, 1000, 1000, 0.5) using ψ3

i
1
2
3
4
6

obj
5.58e-03
5.55e-03
5.99e-03
5.56e-03
5.59e-03

sbkkt
4.38e-03
4.93e-03
5.23e-03
5.12e-03
4.41e-03

µend
5.98e-04
3.60e-04
3.90e-04
1.66e-04
1.21e-04

sparsity(%)
26.15
28.14
25.88
27.64
27.12

ave.time(s)
285.68
295.70
304.30
321.49
313.94

ite
36.5
37.9
38.5
41.5
43.8

success(%)
100
100
100
100
100

5 Conclusion

This paper considers a class of nonsmooth, possibly nonconvex and non-
Lipschitz regularizers for the best hyperparameter selection problem using a
bilevel programming strategy. The class of regularizers we consider subsumes

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

33

the traditional (cid:96)p regularizer, which is the focus of the earlier work [23]. More-
over, the convergence analysis of the smoothing algorithm presented in this
paper is uniﬁed, in the sense that it is not limited to the chosen smoothing
function, unlike the previous work [23] where the analysis is centered on the
selected smoothing function. Finally, we proved our main convergence result
under a milder constraint qualiﬁcation. More precisely, we only assumed the
Mangasarian-Fromovitz constraint qualiﬁcation (MFCQ) for our convergence
analysis, which is weaker than the linearly independent constraint qualiﬁcation
(LICQ) used in [23]. For our numerical simulations, we compared the perfor-
mance of six smoothing functions in solving the bilevel programming problem
using diﬀerent regularizers. Theoretically, we can use these smoothing func-
tions for all the regularizers considered as their corresponding density functions
satisfy Assumption (C). On the other hand, our practical experience revealed
that the choice of smoothing function is dependent as well on the chosen reg-
ularizer. When using the traditional (cid:96)p (p = 1, 0.5) regularizer, the function
φ4 has the best performance in terms of CPU time spent. For the regularizers
ψ2 and ψ3, the smoothing functions φ3 and φ1, respectively, have the most
competitive performance in terms of CPU time. However, we see that the (cid:96)p
regularizer tends to obtain sparser solutions than the other two.

Acknowledgement

This work was conducted while C. T. Nguyen was a research assistant at
National Taiwan Normal University and J. H. Alcantara was a postdoctoral
fellow at National Taiwan Normal University. The work of J.-S. Chen was
supported by the Ministry of Science and Technology, Taiwan.

Appendix A Penalty functions that satisfy
Assumption (A)

We consider four penalty functions as follows:

ψ1(t) = t, ψ2(t) = log(1 + at), ψ3(t) =

at
1 + at

, ψ4(t) =

−1
1 + at

,

where a is positive number. In particular,

(1) ψ1 is a soft-thresholding penalty function [17, 27]. We have ψ(cid:48)
ψ(cid:48)(cid:48)
1 (t) = 0. Hence, it satisﬁes Assumption (A).
(2) ψ2 is a logistic penalty function [20]. We have

1(t) = 1 and

ψ(cid:48)

2(t) =

a
1 + at

, ψ(cid:48)(cid:48)

2 (t) = −

a2
(1 + at)2 ,

which implies that 0 < limt→0 ψ(cid:48)
Assumption (A).

2(t) = a and |ψ(cid:48)(cid:48)

2 (t)| ≤ a2. Hence, it satisﬁes

Springer Nature 2021 LATEX template

34

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

(3) ψ3 is fraction penalty function [10, 20]. We have

ψ(cid:48)

3(t) =

a

(1 + at)2 , ψ(cid:48)(cid:48)

3 (t) = −

2a2
(1 + at)3 ,

which implies that 0 < limt→0 ψ(cid:48)
Assumption (A).
(4) For function ψ4, we have

3(t) = a and |ψ(cid:48)(cid:48)

3 (t)| ≤ 2a2. Hence, it satisﬁes

ψ(cid:48)

4(t) =

a

(1 + at)2 , ψ(cid:48)(cid:48)

4 (t) = −

2a2
(1 + at)3 ,

which implies that 0 < limt→0 ψ(cid:48)
Assumption (A).

4(t) = a and |ψ(cid:48)(cid:48)

4 (t)| ≤ 2a2. Hence, it satisﬁes

Appendix B Proof of Lemma 3.1

In this appendix, we give a proof of Lemma 3.1.

Proof By Assumption (B4), we know that there exists ¯d ∈ (cid:60)n+r such that (26) and
(27) hold. Meanwhile, we have from the formula of Ψj that

∇(ω,λ)Ψj (ω, λ) =






∇(ω,λ)Φj (ω, λ)
(cid:34)

(cid:35)

ej
0r

if j /∈ I(ω∗)

if j ∈ I(ω∗),

(B1)

where ej is the jth standard unit vector in (cid:60)n and 0r denotes the zero vector in
(cid:60)r. It is then clear from (B1) and (26) that ¯dj = 0 for all j ∈ I(ω∗). Consequently,
letting d ∈ (cid:60)n−|I(ω∗)|+r be the vector d := ( ¯d)j /∈I(ω∗), it follows from (26) and (27)
that equations (28) and (29) hold.

It remains to show that {∇(¯ω,λ)Φj (ω∗, λ∗)}j /∈I(ω∗) is linearly independent. To
this end, note ﬁrst that we have from Assumption (B4) the linear independence of
{∇(ω,λ)Ψj (ω∗, λ∗)}n
(cid:20)(cid:16)

j=1, that is, the matrix

(cid:16)

(cid:17)

(cid:17)

(cid:21)

M :=

∇(ω,λ)Ψj (ω∗, λ∗)

,

∇(ω,λ)Ψj (ω∗, λ∗)

∈ (cid:60)(n+r)×n

(j /∈I(ω∗))

(j∈I(ω∗))

has full column rank. Using (B1) and switching the rows of M so that the ﬁrst |I(ω∗)|
rows correspond to the index set I(ω∗), we have that the matrix

(cid:20) (∇ˆωΦj (ω∗, λ∗))j∈I(ω∗)
(∇(¯ω,λ)Φj (ω∗, λ∗))j /∈I(ω∗) O(n−|I(ω∗)|+r)×|I(ω∗)|
has full column rank, where ˆω := (ωj )j∈I(ω∗), Es denotes the identity matrix of
order s, and Os×t is the zero matrix of size s × t. Since the upper and lower right
blocks of the above matrix are the identity matrix and zero matrix, respectively, a
series of elementary column operations leads us to conclude that

E|I(ω∗)|

(cid:21)

(cid:20) O|I(ω∗)|×(n−|I(ω∗)|)
(∇(¯ω,λ)Φj (ω∗, λ∗))j /∈I(ω∗) O(n−|I(ω∗)|+r)×|I(ω∗)|
also has full column rank. As a consequence, {∇(¯ω,λ)Φj (ω∗, λ∗)}j /∈I(ω∗) is linearly
(cid:3)
independent, as desired.

E|I(ω∗)|

(cid:21)

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

35

References

[1] Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization.

J. Mach. Learn. Res. 13(10), 281–305 (2012)

[2] Bennett, K. P., Hu, J., Ji, X., Kunapuli, G., Pang, J-S.: Model selection
via bilevel optimization. In The 2006 IEEE International Joint Conference
on Neural Network Proceedings. 1922–1929 (2006)

[3] Bennett, K. P., Hu, J., Ji, X., Kunapuli, G., Pang, J-S.: Bilevel opti-
mization and machine learning. Computational Intelligence: Research
Frontiers. WCCI 2008. Lecture Notes in Computer Science, vol. 5050,
Berlin, Heidelberg, 2008. Springer.

[4] Bian, W., Chen, X.: Worst-case complexity of smoothing quadratic regular-
ization methods for non-Lipschitzian optimization. SIAM J. Optim. 23(3),
1718–1741 (2013)

[5] Bracken, J., McGill, J.: Mathematical programs with optimization prob-

lems in the constraints. Oper. Res. 21(1), 37–44 (1973)

[6] Chen, X.: Smoothing methods for nonsmooth, nonconvex minimization.

Math. Program. 134(1), 71–99 (2012)

[7] Chen, C., Mangasarian, O. L.: A class of smoothing functions for nonlinear
and mixed complementarity problems. Comput. Optim. Appl. 5(2), 97–138
(1996)

[8] Chen, X., Niu, L., Yuan, Y.: Optimality conditions and a smoothing trust
region Newton method for nonLipschitz optimization. SIAM J. Optim.
23(3), 1528–1552 (2013)

[9] Chen, X., Xu, F., Ye, Y.: Lower bound theory of nonzero entries in solutions
of (cid:96)2-(cid:96)p minimization. SIAM J. Sci. Comput. 32(5), 2832–2852 (2010)

[10] Chen, X., Zhou, W.: Smoothing nonlinear conjugate gradient method
for image restoration using nonsmooth nonconvex minimization. SIAM J.
Imaging Sciences. 3(4), 765–790 (2010)

[11] Clarke, F. H.: Generalized gradients and applications. Trans. Amer. Math.

Soc. 205, 247–262 (1975)

[12] Clarke, F. H.: Optimization and nonsmooth analysis. Classics Appl. Math.

5, SIAM, Philadelphia (1990)

[13] Colson, B., Marcotte, P., Savard, G.: An overview of bilevel optimization.

Ann. Oper. Res. 153, 234–256 (2007)

Springer Nature 2021 LATEX template

36

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

[14] Dempe, S.: Annotated bibliography on bilevel programming and math-
ematical programs with equilibrium constraints. Optimization. 52(3),
333–359 (2003)

[15] Kruger, A. Y.: On Fr´echet subdiﬀerentials. J. Math. Sci. 116(3), 3325–

3358 (2003)

[16] Kunisch, K., Pock, T.: A bilevel optimization approach for parameter
learning in variational models. SIAM J. Imaging Sciences. 6(2), 938–983
(2013)

[17] Huang, J., Horowitz, J. L., Ma, S.: Asymptotic properties of bridge esti-
mators in sparse high-dimensional regression models. Ann. Statist. 36(2)
587–613 (2008)

[18] Moore, G. M., Bergeron, C., Bennett, K. P.: Nonsmooth bilevel program-
ming for hyperparameter selection. In IEEE International Conference on
Data Mining Workshops. pp. 374–381 (2009)

[19] Moore, G. M., Bergeron, C., Bennett, K. P.: Model selection for primal

SVM. 85, 175–208 (2011)

[20] Nikolova, M., Ng, M. K., Zhang, S., Ching, W.-K.: Eﬃcient reconstruction
of piecewise constant images using nonsmooth nonconvex minimization.
SIAM J. Imaging Sciences. 1(1), 2–25 (2008)

[21] Ochs, P., Ranftl, R., Brox, T., Pock, T.: Bilevel optimization with non-
smooth lower level problems. In International Conference on Scale Space
and Variational Methods in Computer Vision (SSVM) (2015)

[22] Ochs, P., Ranftl, R., Brox, T., Pock, T.: Techniques for gradient-based
bilevel optimization with non-smooth lower level problems. J. Math.
Imaging Vis. 56(2), 175–194 (2016)

[23] Okuno, T., Takeda, A., Kawana, A., Watanabe, M.: On (cid:96)p-
hyperparameter
learning via bilevel nonsmooth optimization. arXiv
preprint arXiv:1806.01520v3 (2021), to appear in Journal of Machine
Learning Research

[24] Rockafellar, R. T., Wets, R. J.-B.: Variational analysis. vol. 317. Springer

Science & Business Media (2009)

[25] Saheya, B., Nguyen, C. T., Chen, J.-S.: Neural network based on sys-
tematically generated smoothing functions for absolute value equation. J.
Appl. Math. Comput. 61, 533-558 (2019)

Springer Nature 2021 LATEX template

Uniﬁed Smoothing Bilevel Approach for Hyperparameter Selection

37

[26] Sinha, A., Malo, P., Deb, K.: A review on bilevel optimization: from
classical to evolutionary approaches and applications. IEEE Trans. Evol.
Comput. 22(2), 276–295 (2018)

[27] Tibshirani, R.: Regression shrinkage and selection via the Lasso. J. R.

Statist. Soc. Ser. B. 58(1), 267–288 (1996)

[28] Tso, W. W., Burnak, B., Pistikopoulos, E. N.: HY-POP: Hyperparameter
optimization of machine learning models through parametric programming.
Comput. Chem. Eng. 139, 106902 (2020)

[29] Voronin, S., Ozkaya, G., Yoshida, D.: Convolution based smooth approx-
imations to the absolute value function with application to non-smooth
regularization. arXiv preprint arXiv:1408.6795v2 (2015)

[30] Wang, Z., Hutter, F., Zoghi, M., Matheson, D., de Freitas, N.: Bayesian
optimization in a billion dimensions via random embeddings. J. Artif.
Intell. Res. 55, 361–387 (2016)

