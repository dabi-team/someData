2
2
0
2

y
a
M
1
3

]

C
O
.
h
t
a
m

[

1
v
7
5
4
0
0
.
6
0
2
2
:
v
i
X
r
a

Automatic diﬀerentiation of
nonsmooth iterative algorithms

Jérôme Bolte∗

Edouard Pauwels†

Samuel Vaiter‡

June 2, 2022

Abstract

Diﬀerentiation along algorithms, i.e., piggyback propagation of derivatives, is now
routinely used to diﬀerentiate iterative solvers in diﬀerentiable programming. Asymp-
totics is well understood for many smooth problems but the nondiﬀerentiable case is
hardly considered. Is there a limiting object for nonsmooth piggyback automatic diﬀer-
entiation (AD)? Does it have any variational meaning and can it be used eﬀectively in
machine learning? Is there a connection with classical derivative? All these questions are
addressed under appropriate nonexpansivity conditions in the framework of conservative
derivatives which has proved useful in understanding nonsmooth AD. For nonsmooth
piggyback iterations, we characterize the attractor set of nonsmooth piggyback itera-
tions as a set-valued ﬁxed point which remains in the conservative framework. This
has various consequences and in particular almost everywhere convergence of classical
derivatives. Our results are illustrated on parametric convex optimization problems with
forward-backward, Douglas-Rachford and Alternating Direction of Multiplier algorithms
as well as the Heavy-Ball method.

Dedicated to the memory of Andreas Griewank – a pioneer in automatic diﬀeren-
tiation and optimization – who passed away on September 2021.

1

Introduction

Diﬀerentiable programming. We consider a Lipschitz
Rp, representing an iterative algo-
function F : Rp
(cid:55)→
×
rithm, parameterized by θ
Rm, with Lipschitz initialization
x0(θ) and
x0 : θ

Rm

∈

(cid:55)→

xk+1(θ) = F (xk(θ), θ) = Fθ(xk(θ)),

(1)

xk(θ)

nonsmooth

autodiﬀ

Jxk (θ)

∞
+
→
k

¯x(θ)

?
t
i

m

i
l

J pb
¯x (θ)

derivative?

, θ), under the assumption that xk(θ) converges
where Fθ := F (
·
to the unique ﬁxed point of Fθ: ¯x(θ) = ﬁx(Fθ). Such recursion
represent for instance algorithms to solve an optimization
problem minx h(x) (e.g. empirical risk minimization), such as
h(x). But (1) could also be
gradient descent: F (x, θ) = x
θ
−
a ﬁxed-point equation such as a deep equilibrium network [5].
In the last years, a paradigm shift occurred: such algorithms
are now implemented in algorithmic diﬀerentiation (AD)-friendly frameworks such as Ten-
sorﬂow [1], PyTorch [39] or JAX [13] to name a few. Assuming that F is diﬀerentiable, it is

Figure 1: We study exis-
tence and meaning of J pb
¯x
as a derivative of ¯x, com-
patible with automatic dif-
ferentiation of the iterates
(xk(θ))k∈N.

∇

∗Toulouse School of Economics, University of Toulouse Capitole.
†IRIT, CNRS, Université de Toulouse, ANITI, Toulouse, France.
‡CNRS & Laboratoire J. A. Dieudonné, Université Côte d’Azur, Nice, France.

1

 
 
 
 
 
 
possible to compute iteratively the derivatives of xk+1 with respect to θ using the diﬀerential
calculus chain rule resulting in so called “piggyback” recursion:

·

∂
∂θ

xk(θ) + ∂2F (xk(θ), θ),

xk+1(θ) = ∂1F (xk(θ), θ)

∂
∂θ
where ∂
∂θ xk is the Jacobian of xk with respect to θ. In practice, automatic diﬀerentiation
frameworks do not compute the full Jacobian, but compute either vector-Jacobian products
in reverse-mode (or backpropagation) [45] or Jacobian-vector products in forward mode [50].
We rather consider the full Jacobian, and therefore, our ﬁndings apply to both modes. We
focus on two issues arising with nonsmooth recursions, illustrated in Figure 1. (i) what can
be said about the chain rule (2) and its asymptotics when the function F is not smooth
(for example a projected gradient step)? (ii) how to interpret its asymptotics as a notion
of derivative for ¯x, the ﬁxed point of Fθ? We propose a joint answer to both questions,
providing a solid theoretical ground to the idea of algorithmic diﬀerentiation of numerical
solvers involving nonsmooth components in a diﬀerentiable programming context.

(2)

Related works. Algorithmic use of the chain rule (2) to diﬀerentiate programs takes its
root in [50], where forward diﬀerentiation was ﬁrst proposed, and later in reverse mode [33].
Along with the practical development of automatic diﬀerentiation, the question on how
to prove the convergence of the iterative sequence (2) was investigated, notably in the
optimization community as reviewed in [27]. This is an important paper containing several
ideas in diﬀerentiable programming rediscovered/reused later: implicit diﬀerentiation [40, 42]
and its stability [8], adjoint ﬁxed point iteration [5] that is a key aspect of the deep equilibrium
network and linear convergence of (2) as discussed below. Notably, the linear convergence of
the Jacobians was investigated in [24, 26] for the forward mode and in [15, Theorem 2.3] for
the reverse mode. This was more recently investigated – for C 2 functions – in the imaging
community for primal-dual algorithms [14, 9] and in the machine learning community for
gradient descent [37, 34] and the Heavy-ball [37] method. Note that in the speciﬁc context
where F solves a min-min problem, the authors in [2] proved the linear convergence of the
Jacobians. The use of automatic diﬀerentiaton for nonsmooth functions was justiﬁed by the
development of the notion of conservative Jacobians [12, 11] with a nonsmooth version of the
chain rule for compositional models. The correctness of automatic diﬀerentiation was also
investigated in [32] for a large class of functions that are piecewise analytic, and also in [31]
where a qualiﬁcation condition is used to compute a Clarke Jacobian. Along with automatic
diﬀerentiation, a natural way to diﬀerentiate a model such as (1) is by implicit diﬀerentiation,
recently applied in several works [5, 3, 20]. To study these models with nonsmooth functions,
an implicit function theorem [10] was proved for path-diﬀerentiable functions.

Contributions: Under suitable nonexpansivity assumptions, our contributions are as
follows.
• We address both questions illustrated in Figure 1 for nonsmooth recursions: set-valued
nonsmooth extensions of the piggyback recursion (2) have a well deﬁned limit, described as
the ﬁxed point of subset map (Theorem 1), it is conservative for the ﬁxed point map ¯x. This
is a nonsmooth “inﬁnite” chain rule for AD (Theorem 2).
• For almost all θ, despite nonsmoothness, recursion (2) is well deﬁned using the classical
Jacobian and converges to the classical Jacobian of the ﬁxed point ¯x (Corollary 2). This has
implications for both forward and reverse modes of AD.
• For a large class of functions (Lipschitz-gradient selection), it is possible to give a quantita-
tive rate estimate (Corollary 3), namely to prove linear convergence of the derivatives.
• We show that these results can be applied to proximal splitting algorithms in nonsmooth con-
vex optimization. We include forward–backward (Proposition 2), as well Douglas–Rachford
(Proposition 3) and ADMM, a numerical illustration of the convergence of derivatives is
given in Figure 2.
• We also illustrate that, contrarily to the smooth case, nonsmooth piggy back derivatives

2

Figure 2: Illustration of the linear convergence of proximal splitting methods. (First line)
Distance of the iterates to the ﬁxed point. (Second line) Distance of the piggyback Jacobians
to the Jacobian of the ﬁxed point. The acronyms are FB for Forward-Backward, DR for
Douglas-Rachford and ADMM for Alternating Direction Method of Multipliers.
In all
cases, despite nonsmoothness, piggyback Jacobians converge, illustrating Corollary 2. Blue
lines represent the median of 100 repetitions with random data, and the blue shaded area
represents the ﬁrst and last deciles.

of momentum methods such as the Heavy-ball algorithm, may diverge even if the iterates
converge linearly (Proposition 4).

Notations. A function f : Rp
neighborhood of x on which f is Lipschitz. Denoting by R
f is diﬀerentiable, the Clarke Jacobian [16] at x

→

Rm is locally Lipschtiz if, for each x

⊆
Rp is deﬁned as

Rn, there exists a
Rp, the full measure set where

∈

∈

Jac cf (x) = conv

(cid:26)

M

Rm×p,

(yk)k≥0 s.t.
∃

∈

lim
k→∞

yk = x, yk

R, lim
k→∞

∈

(cid:27)

(yk) = M

.

∂f
∂y

(3)
The Clarke subdiﬀerential, ∂cf is deﬁned similarly. Given two matrices A, B with compatible
dimension, [A, B] is their concatenation. For a set
is its convex hull (the smallest
convex set containing
). The symbol B denotes a unit ball, the corresponding norm should
be clear from the context.

, conv

X

X

X

2 Nonsmooth piggyback diﬀerentiation

We ﬁrst show how the use of the notion of conservative Jacobians allow us to justify rigorously
the existence of a nonsmooth equivalent of piggyback iterations in (2) that is compatible
with AD.

Conservative Jacobians. Conservative Jacobians were introduced in [12] as a generaliza-
tion of derivatives to study automatic diﬀerentiation of nonsmooth functions. Given a locally
Lipschitz continuous function f : Rp
Rm, we say that the set-valued J : Rp ⇒ Rp×m is a
conservative Jacobian for the path diﬀerentiable f if J has a closed graph, is locally bounded
and nowhere empty with

→

d
dt

f (γ(t)) = J(γ(t)) ˙γ(t) a.e.

(4)

→

for any γ : [0, 1]
Rp absolutely continuous with respect to the Lebesgue measure. Conser-
vative gradients are deﬁned similarly. We refer to [12] for extensive examples and properties
of this class of function. Let us mention that the classes convex functions, deﬁnable functions,
or semialgebraic functions are contained in the set of path diﬀerentiable functions. Given
Df : Rp ⇒ Rp, a conservative gradient for f : Rp

R, we have:

→

3

0500100010−410−1kxk−¯xkRidge(FB)0500100010−410−1Lasso(FB)0500100010−310−1SparseInv.Covar.(DR)0500100010−310−1TrendFiltering(ADMM)05001000iterationk10−310−1kJxk−Jpb¯xk05001000iterationk10−310−105001000iterationk10−110105001000iterationk10−210−1100Rp, ∂cf (x)

• (Clarke subgradient), for all x
• (Gradient almost everywhere) Df (x) =
• (Calculus) diﬀerential calculus rules preserve conservativity, e.g. sum and compositions
of conservative Jacobians are conservative Jacobians.
An important point is that Df can be used as a ﬁrst order optimization oracle for methods
of gradient type, while preserving usual convergence guaranties [11].

for almost all x

conv(Df (x)).

⊂
f (x)
}

Rp.

{∇

∈

∈

Piggyback diﬀerentiation of recursive algorithms. The following is standing through-
out the text

Assumption 1 (The conservative Jacobian of the iteration mapping is a contraction)
F is locally Lipschitz, path diﬀerentiable, jointly in (x, θ), and JF is a conservative Jacobian
Rm and any pair
ρ < 1, such that for any (x, θ)
for F . There exists 0
Rp×p and B
JF (x, θ), with A
[A, B]

∈
Rp×m, the operator norm of A is at most ρ.

Rp

×

≤
∈

∈

∈

Under Assumption 1, Fθ is a strict contraction so that (xk(θ))k∈N converges linearly to
¯x(θ) = ﬁx(Fθ) the unique ﬁxed point of the iteration mapping Fθ. More precisely, for all
k

N, we have

∈

xk
(cid:107)

−

¯x(θ)

(cid:107) ≤

x0

ρk (cid:107)

−
1

Fθ(x0)
ρ

(cid:107)

.

−

Furthermore, for every k

N, let us deﬁne the following set-valued piggyback recursion:

∈
Jxk+1(θ) =

∈
We will show in Section 3 that (PB) plays the same role as (2) in the nonsmooth setting.
Note that one can recursively evaluates a sequence Jk

N as follows

, k

∈

Jxk

AJ + B, [A, B]
{

JF (xk(θ), θ), J

Jxk (θ)
}

.

(PB)

∈

∈

Jk+1 = AkJk + Bk where

[Ak, Bk]

JF (xk(θ), θ),

∈

(5)

which corresponds to the operations actually implemented in nonsmooth AD frameworks.

¯x(θ) as k

Remark 1 (Local contractions) Assumption 1 may be relaxed locally as follows:
for
all θ, the ﬁxed point set ﬁx(Fθ) of the iteration mapping Fθ is a singleton ¯xθ such that
, and the operator norm condition on JF in Assumption 1 holds at
xk(θ)
the point (¯x(θ), θ). By graph closedness of JF , in a neighborhood of (¯x(θ), θ), Fθ is a strict
contraction and the operator norm condition on JF holds, possibly with a larger contraction
factor ρ. After ﬁnitely many steps, the iterates (xk)k∈N remain in this neighborhood and all
our convergence results hold, due to their asymptotic nature.

→ ∞

→

3 Asymptotics of nonsmooth piggyback diﬀerentiation

3.1 Fixed point of aﬃne iterations

Gap and Haussdorf distance. Being given two nonempty compact subsets
set

,

X

Y

of Rp,

X
and deﬁne the Hausdorﬀ distance between

Y

Y

gap(

,

) = max
x∈X

d(x,

) where d(x,

) = min

y∈Y (cid:107)

x

y

,
(cid:107)

−

Y

and

by

Y

X

dist(

,

) = max(gap(

,

Note that gap(

,
. Moreover,

X

=

Y
X
Y
X ⊆ Y
“measures” the default of inclusion of

X

Y

Y

X
) = 0 if, and only if,
+ gap(

,

in

X

)).

), gap(

,

Y

Y
, whereas dist(

X

X ⊆ Y

)B where B is the unit ball. It means that gap(

,

X

Y

) = 0 if, and only if,
)

,

, see [43, Chapter 4] for more details.

X

Y

X

Y

4

k+1 =

k).

(
X

J

X

Aﬃne iterations by packets of matrices. Let
matrices such that any matrix of the form [A, B]
operator norm at most ρ < 1. We let
follows
J
each X
∈
through, for any

Rp×(p+m) be a compact subset of
Rp×p is such that A has
with A
m as
: Rp×m ⇒ Rp×m the function from Rp×m to subsets of Rp×m which is deﬁned for
. This deﬁnes a set-valued map
Rp×n as follows:

act naturally on the matrices of size p

AX + B, [A, B]
{

J
Rp×m,

(X) =

∈ J }

J ⊂

∈ J

×

J

∈

X ⊂

{
On the model of recursions of the form (PB), we consider sequences (
Rp×m satisfying the recursion

∈ X }

J

∈

X

) =

AX + B, [A, B]

J, X

.

(
X

(6)

k)k∈N of subsets of

(7)

We have the following instance of the Banach–Picard theorem (proved in Appendix A).

Theorem 1 (Set-valued aﬃne contractions) Let
matrices as above with ρ < 1. Then there is a unique nonempty compact set ﬁx(
satisfying ﬁx(
)), where the action of
Let (
recursion (7). We have for all k

k)k∈N be a sequence of compact subsets of Rp×m, such that
N

J ⊂
is given in (6).
0

(ﬁx(

) =

=

X

X

J

J

J

J

Rp×(p+m) be a compact subset of
Rp×m

, and satisfying the
∅

)

J

⊂

∈

where dist is the Hausdorﬀ distance related to the Euclidean norm on p

dist(

k, ﬁx(

X

ρk dist(

))

≤

0,
X
1

(
J
X
ρ

0))

,

J

−

m matrices.

×

3.2 An inﬁnite chain rule and its consequences
Deﬁne the following set-valued map based on the ﬁx operator from Theorem 1,

J pb
¯x : θ ⇒ ﬁx [JF (¯x(θ), θ)] .
where ¯x(θ) is the unique ﬁxed point of the algorithmic recursion. Note that since ¯x(θ) =
ﬁx(Fθ), we also have equivalently that J pb
is the ﬁxed-point of the Jacobian of the ﬁxed-point
¯x
of Fθ:

We have the following (proved in Appendix B).

J pb
¯x : θ ⇒ ﬁx [JF (ﬁx(Fθ), θ)] .

Assump-
is well-deﬁned, and is a conservative Jacobian for the ﬁxed point map

Theorem 2 (A conservative mapping for the ﬁxed point map) Under
tion 1, J pb
¯x
¯x.
Combining with Theorem 1 ensures the convergence of the set-valued piggyback iterations
(PB).

Corollary 1 (Convergence of the piggyback derivatives) Under Assumption 1, for
all θ, the recursion (PB) satisﬁes

Unrolling the expression of Jxk

gap(Jxk (θ), J pb

lim
k→∞
, we can rewrite (8) as a set-valued product such that

¯x (θ)) = 0.

(8)

lim
K→+∞

gap

(cid:32) K
(cid:89)

k=0

JF (xk(θ), θ), J pb

¯x (θ)

= 0.

(cid:33)

In plain words, this a limit-derivative exchange result: Asymptotically, the gap between
the automatic diﬀerentiation of xk and the derivative of the limit is zero. This implies in
particular that the recursion (5) produces bounded sequences and all its accumulation points
are in J pb
. Using the fact that conservative Jacobians equal classical Jacobians almost
¯x
everywhere [12], this implies convergence of derivatives in a classical sense.

5

(cid:54)
Corollary 2 (Convergence of the classical piggyback derivatives) Under Assump-
tion 1, for almost all θ, the classical Jacobian ∂
∂θ xk(θ), is well deﬁned for all k and converges
towards the classical Jacobian of ¯x:

lim
k→∞

∂
∂θ

xk(θ) =

∂
∂θ

¯x(θ).

∈

Remark 2 (Connection to implicit diﬀerentiation) The authors in [10] proved a
qualiﬁcation-free version of the implicit function theorem. Assuming that for every
[A, B]

A is invertible, we have that

J(¯x(θ), θ), the matrix I

J imp
¯x

JF (¯x(θ), θ)(cid:9)
is a conservative Jacobian for ¯x. Under Assumption 1, one has J imp
J pb
¯x (θ) for all θ.
Unfortunately, as soon as F is not diﬀerentiable, the inclusion may be strict, see details in
Appendix C.

A)−1B, [A, B]

(θ)

(9)

⊂

−

∈

¯x

−
: θ ⇒ (cid:8)(I

N,

3.3 Consequence for algorithmic diﬀerentiation
Rp, the following algorithms allow us to compute ˙xk = Jk ˙θ
Given k
using the forward mode of automatic diﬀerentation (Jacobian Vector Products, JVP), and
¯θT
k Jk using the backward mode of automatic diﬀerentiation (Vector Jacobian Products,
k = ¯wT
VJP).

Rm, ¯wk

∈

∈

∈

˙θ

Algorithm 1: Algorithmic diﬀerentiation of recursion (1), forward and reverse
modes

N, θ

Input: k
∈
function F (x, θ), conservative Jacobians JF (x, θ) and Jx0(θ). Initialize:
x0 = x0(θ)

Rp, initialization function x0(θ), recursion

∈
Rp.

Rm, ¯wk

Rm, ˙θ

∈

∈

∈

∈

Jx0 (θ).

Forward mode (JVP):
˙x0 = J ˙θ, J
for i = 1, . . . , k do
xi = F (xi−1, θ)
˙xi = Ai−1 ˙xi−1 + Bi−1 ˙θ
[Ai−1, Bi−1]

Reverse mode (VJP): ¯θk = 0.
for i = 1, . . . , k do
xi = F (xi−1, θ)
for i = k, . . . , 1 do
¯θk = ¯θk + BT
i−1 ¯wi
[Ai−1, Bi−1]
¯θk = ¯θk + J T ¯w0, J
Return: ¯θk
The following result is a consequence of Corollary 2 combined with algorithmic diﬀerentiation
arguments, its proof is given in Appendix C.

JF (xi−1, θ)
Jx0(θ)

Return: ˙xk

¯wi−1 = AT

JF (xi−1, θ)

i−1 ¯wi

∈

∈

∈

→

∈
∂ ¯x
∂θ

k ∈

Rp, ¯θT
˙θ.

Rm be as in Agorithm 1 under Assumption 1. Then for almost all θ

Proposition 1 (Convergence of VJP and JVP) Let k
Rp, ˙xk
˙xk
Assume furthermore that, as k
Rm, ¯θT
then for almost all θ
Remark 3 In addition to Proposition 1, in both cases, for all θ, all accumulation points of
both ˙xk and ¯θT
k

are elements of J pb
¯x

(cid:96)(xk) for a C 1 loss (cid:96)),

¯w (for example, ¯wk =

˙θ and ¯wT J pb
¯x

, ¯wk
¯wT ∂ ¯x
∂θ .

respectively.

∈
Rm,

k →

→ ∞

→

∇

∈

∈

∈

∈

∈

Rm, ¯wk

Rp, xk

N, ˙θ

3.4 Linear convergence rate for semialgebraic piecewise smooth se-

lection function

Semialgebraic functions are ubiquitous in machine learning (piecewise polynomials, (cid:96)1, (cid:96)2
norms, determinant matrix rank . . . ). We refer the reader to [11] for a thorough discussion
of their extensions, and use in machine learning. For more technical details, see [17, 18] for
introductory material on semialgebraic and o-minimal geometry.

6

Lipschitz gradient selection functions. Let F : Rp
ous. We say that F has a Lipschitz gradient selection (s, F1, . . . , Fm) if s : Rp
semialgebraic and there exists L
with L-Lipschitz Jacobian, and for all x
Rp, set I(x) =
For any x
Rp×q given by

0 such that for i = 1 . . . , m, Fi : Rp
Rp, F (x) = Fs(x)(x).
, F (x) = Fi(x)

(cid:55)→
. The set-valued map J s
}

Rq be semialgebraic and continu-
(1, . . . , m) is
Rp is semialgebraic

∈
1, . . . , m
}

F : Rp ⇒

∈ {

(cid:55)→

(cid:55)→

≥

∈

{

i

F : x ⇒ conv
J s

(cid:18)(cid:26) ∂Fi
∂x

(x), i

∈

(cid:27)(cid:19)

I(x)

,

is a conservative Jacobian for F as shown in [11]. Here ∂Fi
∂x
Fi. Let us stress that such a structure is ubiquitous in applications [11, 32].

denotes the classical Jacobian of

Rate of convergence. We may now strengthen Corollary 1 by proving the linear conver-
gence of piggyback derivatives towards the ﬁxed point. The following is a consequence of
the fact that the proposed selection conservative Jacobians of Lipschitz gradient selection
functions are Lipschitz-like (Lemma 3 in Appendix D.1). Note that semialgebraicity is only
used as a suﬃcient condition to ensure conservativity of the selection Jacobian together
with this Lipschitz like property. It could be relaxed if it can be guaranteed by other means,
in particular one could consider the broader class of deﬁnable functions in order to handle
log-likelihood data ﬁtting terms.

Corollary 3 (Linear convergence of piggyback derivatives) In addition to Assump-
tion 1, assume that F has a Lipschitz gradient selection structure as above. Then, for any θ
and (cid:15) > 0, there exists C > 0 such that the recursion (PB) with JF = J s

F satisﬁes

≤
Moreover, classical Jacobians in Corollary 2 converge at a linear rate for almost all θ.

∈

∀

gap(Jxk (θ), J pb

¯x (θ))

C(√ρ + (cid:15))k,

k

N.

4 Application to proximal splitting methods in convex

optimization

Consider the composite parametric convex optimization problem, where θ
parameters and x

Rp is the decision variable

∈

Rm represents

∈

¯x(θ) = arg minxf (x, θ) + g(x, θ).

The purpose of this section is to construct examples of function F used in recursion (1) based
on known algorithms. The following assumption will be standing throughout the section.

xf is path-diﬀerentiable jointly in (x, θ), we denote by J 2
f

xf ,
Assumption 2 f is semialgebraic, convex, its gradient with respecto to x for ﬁxed θ,
is locally Lipschitz jointly in (x, θ) and L-Lipschitz in x for ﬁxed θ. Semialgebraicity implies
that
∇
g is semialgebraic, convex in x for ﬁxed θ, and lower semicontinuous. For all α > 0, we assume
proxαg(·,θ)(x) is locally Lipschitz jointly in (x, θ). semialgebraicity implies
that Gα(x, θ)
that it is also path diﬀerentiable jointly in (x, θ), we denote by JGα

its Clarke Jacobian.

its Clarke Jacobian.

(cid:55)→

∇

This assumption covers a very large diversity of problems in convex optimization as most
gradient and prox operations used in practice are semialgebraic. Under Assumption 2,
we will provide suﬃcient conditions on f and g for Assumption 1 to hold for diﬀerent
algorithmic recursions. These are therefore suﬃcient for the validity of the convergence
results in Corollary 1 and Corollary 2, Proposition 1, as well Corollary 3 in the piecewise
selection case. The proofs are postponed to Appendix E.

7

4.1 Splitting algorithms

In this section we provide suﬃcient condition for Assumption 1 to hold. The underlying
conservative Jacobian is obtained by combining Clarke Jacobians of elementary algorithmic
operations (gradient, proximal operator in Assumption 2), using the compositional rules
of diﬀerential calculus [11] and implicit diﬀerentiation [10]. Using [12], such Jacobians are
conservative by semialgebraicity and their combination provide conservative Jacobians for
the corresponding algorithmic recursion F . These objects are explicitly constructed in
Appendix E.

Forward–backward algorithm. The forward–backward iterations are given for α > 0
by

xk+1 = proxαg(·,θ) (xk

α

∇

−

xf (xk, θ)) .

(10)

Rp the
Proposition 2 Under Assumption 2 with 0 < α < 2
forward-backward recursion in (10). For µ > 0, if either f or g is µ-strongly convex in x for
all θ, then Fα is a strict contraction and Assumption 1 holds.

L , denote by Fα : Rp×m

→

Douglas–Rachford. Given α > 0, the algorithm goes as follows

yk+1 =

1
2

(I + Rαf (·,θ)Rαg(·,θ))yk,

(11)

where Rαf (·,θ) = 2proxαf (·,θ) −
I is the reﬂected proximal operator, which is 1-Lipschitz (and
similarly for g). Following [6, Theorem 26.11], if the problem has a minimizer, then (yk)k∈N
converges to a ﬁxed point of (11), ¯y such that ¯x = proxαg(¯y) is a solution to the optimization
problem. Following [25, Theorem 1], if f is strongly convex, then Rαf (·,θ) is ρ-Lipschitz
for some ρ < 1 and our diﬀerentiation result applies to Douglas-Rachford splitting in this
setting.

Proposition 3 Under Assumption 2 with α > 0, denote by Fα : Rp×m
Rp the Douglas-
Rachford recursion in (11). If f is µ-strongly convex in x for all θ, then Fα is a strict
contraction and Assumption 1 holds.

→

Alternating Direction Method of Multipliers algorithms. Consider the separable
convex problem

φθ(u) + ψθ(v)

subject to Aθu + Bθv = cθ.

min
u,v

(12)

The alternating direction method of multipliers (ADMM) algorithm combines two partial
minimization of an augmented Lagrangian, and a dual update:

uk+1 = arg min

u

vk+1 = arg min

v

(cid:110)

φθ(u) + x(cid:62)Aθu +

(cid:110)

ψθ(v) + x(cid:62)Bθv +

xk+1 = xk + α(Aθuk+1 + Bθvk+1

α
2 (cid:107)
α
2 (cid:107)
cθ).

−

Aθu + Bθvk

Aθuk+1 + Bθvk

(cid:111)

cθ

2
2

(cid:107)

−

cθ

2
2

(cid:107)

−

(cid:111)

(13)

As observed in [22], the ADMM algorithm can be seen as the Douglas-Rachford splitting
method applied to the Fenchel dual of problem (12) (see Appendix E.3 for more details).
More precisely, ADMM updates are equivalent to Douglas-Rachford iterations applied to the
following problem

min
x

θ x + φ∗
c(cid:62)
θ(
−
(cid:123)(cid:122)
(cid:124)
f (x,θ)

A(cid:62)

θ x)
(cid:125)

+ ψ∗
θ (
(cid:124)

B(cid:62)
−
(cid:123)(cid:122)
g(x,θ)

θ x)
(cid:125)

.

(14)

Therefore, if φθ is strongly convex with Lipschitz gradient and Aθ is injective, then ADMM
converges linearly and one is able to combine derivatives of proximal operators to diﬀerentiate
ADMM.

8

4.2 Numerical illustration.

We now detail how Figure 2 discussed in the introduction is obtained, and how it illustrates
our theoretical results. We consider four scenarios (Ridge, Lasso, Sparse inverse covariance
selection and trend ﬁltering) corresponding to the four columns. For each of them, the ﬁrst
line shows the empirical linear rate of the iterates xk and the second line shows the empirical
linear rate of the derivative ∂
∂θ xk. All experiments are repeated 100 times and we report the
median along with ﬁrst and last deciles.

Forward–Backward for the Ridge. The Ridge estimator is deﬁned for θ > 0 as ¯x(θ) =
Among several possibilities to solve it, one can use the
2
arg minx∈Rp
2 + θ
Forward–Backward algorithm applied to f : (x, θ)
. Since g is
b
strongly convex, the operator Fα is strongly convex, and thus Proposition 2 may be applied.

and g : θ

1
2 (cid:107)

1
2 (cid:107)

x
(cid:107)

2
2
(cid:107)

2
2
(cid:107)

b
(cid:107)

Ax

Ax

(cid:55)→

−

−

x

2
2

(cid:107)

(cid:107)

(cid:107)

(cid:107)

x

∈

−

(cid:55)→

Ax

Ax

1
2 (cid:107)

1.
(cid:107)

x
L (cid:107)

2 + θ
2

1
2L (cid:107)

θ
x
L (cid:107)

arg minx∈Rp

1 = arg minx
(cid:107)

Forward–Backward algorithm for the Lasso. Consider the Forward–Backward algo-
rithm applied to the Lasso problem [46], with parameter θ > 0, ¯x(θ)
−
1, where L is any upper bound on the operator
2
b
2 + θ
b
(cid:107)
(cid:107)
norm of AT A. The gradient of the quadratic part is 1 Lipschitz so we may consider the
forward backward algorithm (10), with unit step size with f : (x, θ)
and
g : (x, θ)
A well known qualiﬁcation condition involving a generalized support at optimality ensures
uniqueness of the Lasso solution [19, 35]. This conditions holds for generic problem data [47].
Following [10, Proposition 5], under this qualiﬁcation condition, the implicit conservative
Jacobian JF is such that, at the solution x∗, JF (x∗) has an operator norm of at most 1,
and the matrix set I
JF only contains invertible matrices. This means that there exists
−
JF (x∗) has operator norm at most ρ. Following Remark 1, all our
ρ < 1, such that any M
convergence results apply qualitatively. Note that we recover the results of [7, Proposition 2]
for the Lasso.

1
2L (cid:107)

2
b
2
(cid:107)

Ax

(cid:55)→

−

∈

Douglas–Rachford for the Sparse Inverse Covariance Selection. The Sparse In-
verse Covariance Selection [49, 21] reads ¯x(θ)
,
xi,j
∈
|
where C is a symmetric positive matrix and θ > 0. It is possible to apply Douglas–Rachford
to f : (x, θ)
1,1. It is known that f is locally
(cid:107)
−
log det x is a standard self-concordant barrier in semideﬁnite
strongly convex, indeed x
programming [38]. Following Remark 1, all our convergence results apply qualitatively.

log det x and g : (x, θ)

arg minx∈Rn×n tr(Cx)

log det x+θ (cid:80)

tr(Cx)

(cid:55)→ −

i,j |

(cid:55)→

(cid:55)→

−

x

(cid:107)

θ

ADMM for Trend Filtering.
the Total Variation, the trend ﬁltering estimator with observation θ
D(k)x
arg minx∈Rp
of a diﬀerential operator of order k (here k = 2). Using ψθ : u
φθ : v
v
(cid:55)→ (cid:107)
solve trend ﬁltering.

Introduced in [48] in statistics as a generalization of
Rp reads ¯x(θ) =
∈
1, where D(k) is a forward ﬁnite–diﬀerence approximation
2
2 + λ
(cid:107)
(cid:107)
1 (strongly convex),
I (injective), Bθ = D(k), and cθ = 0, we can apply the ADMM to

−
(cid:107)
, Aθ =

1
x
2 (cid:107)

u
(cid:107)
(cid:107)

2
2
(cid:107)

(cid:55)→

−

−

λ

θ

θ

5 Failure of automatic diﬀerentiation for inertial meth-

ods

In this section we consider the Heavy-Ball method for strongly convex objectives, in its global
linear convergence regime. When applied to a C 2 objective, accumulation of derivatives
converges to the derivative of the solution map [27, 37, 34]. However, we provide a C 1,1
strongly convex parametric objective with path diﬀerentiable derivative, such that forward
propagation of derivatives along the Heavy Ball algorithm contains diverging vectors for a
given parameter value. In this example, one may obtain a conservative Jacobian by other

9

means, such as implicit diﬀerentiation or algorithmic diﬀerentiation of the gradient descent
algorithm, both avoiding this divergent behaviors.

5.1 Heavy-ball algorithm and global convergence

(cid:18)

(cid:55)→

Rm

R, and β > 0, for simplicity, when the second argument
y), x),

Consider a function f : Rp
×
→
is ﬁxed we write fθ : x
f (x, θ). Set for all x, y, θ, F (x, y, θ) = (x
consider the Heavy-Ball algorithm (xk+1, yk+1) = F (xk, yk, θ) for k
If fθ is µ-strongly convex with L-Lipschitz gradient, then, choosing α = 1/L and β <
, the algorithm will converge globally at a linear rate to the unique
1
2
solution, ¯x(θ) [23, Theorem 4], local convergence is due to Polyak [41]. Furthermore, if in
addition f is C 2 forward propagation of derivatives converge to the derivative of the solution
[27, 28, 37].

fθ(x) + β(x
N.

− ∇
∈

4L2 + 2

µ
2L +

(cid:113) µ2

−

(cid:19)

5.2 A diverging Jacobian accumulation

Details and proof of the following result are given in Section F.

Proposition 4 (Piggyback diﬀerentiation fails for the Heavy Ball method)
Consider f : R2
0 and f (x, θ) = x2/8 if
x < 0. Assume that α = 1 and β = 3/4. Then the heavy ball algorithm converges globally to
0 and
f is path diﬀerentiable. The Clarke Jacobian of F with respect to (x, y) at (0, 0, 0)
is JF (0, 0, 0) = conv

R, f (x, θ) = x2/2 if x

R, such that for all θ

, where the product M1M1M2M2 has eigenvalue
}

M1, M2
{

9/8.

→

∇

−

≥

∈

The presence of an eigenvalue with modulus greater than 1 may produce divergence in (PB).
Set

f1 : (x, θ)

(cid:40)

x2/2
x2/8

(cid:55)→

if x
0
≥
if x < 0.

f2 : (x, θ)

(cid:40)

x2/2
x2/8

(cid:55)→

if x > 0
if x
0.

≤

Note that f1 and f2 are both equivalent to f as they implement the same function. With
initializations x(θ) = y(θ) = θ, we run a few iterations of the Heavy Ball algorithm for
θ = 0, and implement (PB) alternating between two steps on f1 and two steps on f2 and
diﬀerentiate the resulting sequence (xk)k∈N with respect to θ using algorithmic diﬀerentiation.
The divergence phenomenon predicted by Proposition 4 is illustrated in Figure 3, while the
true derivative is 0 (the sequence is constant).

6 Conclusion

We have developed a ﬂexible theoretical framework to describe convergence of piggyback
diﬀerentiation applied to nonsmooth recursions – providing, in particular, a rigorous meaning

Figure 3: Behaviour of automatic diﬀerentiation for ﬁrst-order methods on a quadratic
function. (Left) Stability of the propagation of derivatives for the ﬁxed step-size gradient
descent. (Right) Instability of the propagation of Heavy-Ball initialized. Both methods are
initialized at optimum.

10

050100150200iterationk0.00.5JGDk(0)050100150200iterationk−2000200JHBk(0)to the diﬀerentiation of nonsmooth solvers. The relevance of our approach is illustrated
on some major composite convex optimization problems through widely used methods as
forward-backward, Douglas-Rachford or ADMM algorithms. Our framework allows however
to consider many other abstract algorithmic recursions and provides thus theoretical ground
for more general problems such as variational inequalities or saddle point problems as in
[14, 9]. As a matter for future work, we shall consider relaxing Assumption 1 to study a
wider class of methods, e.g., when F is not a strict contraction.

Acknowledgments

J. B. and E. P. acknowledge the ﬁnancial support of the AI Interdisciplinary Institute
ANITI funding under the grant agreement ANR-19-PI3A-0004, Air Force Oﬃce of Scientiﬁc
Research, Air Force Material Command, USAF, under grant numbers FA9550-19-1-7026, and
ANR MaSDOL 19-CE23-0017-01. J. B. also acknowledges the support of ANR Chess, grant
ANR-17-EURE-0010, TSE-P and the Centre Lagrange. S. V. acknowledges the support
ANR GraVa, grant ANR-18-CE40-0005.

References

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig
Citro, Greg S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat,
Ian Goodfellow, Andrew Harp, Geoﬀrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat
Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. Software available from tensorﬂow.org.

[2] Pierre Ablin, Gabriel Peyré, and Thomas Moreau. Super-eﬃciency of automatic dif-
ferentiation for functions deﬁned as a minimum. In Hal Daumé III and Aarti Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pages 32–41. PMLR, 13–18 Jul 2020.

[3] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and
J. Zico Kolter. Diﬀerentiable convex optimization layers. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

[4] Charalambos D Aliprantis and Kim C Border.

Inﬁnite Dimensional Analysis: A

Hitchhiker’s Guide. Springer Science & Business Media, 2006.

[5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in

Neural Information Processing Systems, 32, 2019.

[6] Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator

theory in Hilbert spaces, volume 408. Springer, 2011.

[7] Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre
Gramfort, and Joseph Salmon. Implicit diﬀerentiation of lasso-type models for hyperpa-
rameter optimization. In International Conference on Machine Learning, pages 810–821.
PMLR, 2020.

[8] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe
Llinares-López, Fabian Pedregosa, and Jean-Philippe Vert. Eﬃcient and modular
implicit diﬀerentiation, 2021.

11

[9] Lea Bogensperger, Antonin Chambolle, and Thomas Pock. Convergence of a Piggyback-
style method for the diﬀerentiation of solutions of standard saddle-point problems.
Technical report, 2022.

[10] Jérôme Bolte, Tam Le, Edouard Pauwels, and Antonio Silveti-Falls. Nonsmooth
Implicit Diﬀerentiation for Machine Learning and Optimization. In Advances in Neural
Information Processing Systems, Online, 2021.

[11] Jerome Bolte and Edouard Pauwels. A mathematical model for automatic diﬀerenti-
ation in machine learning. In Conference on Neural Information Processing Systems,
Vancouver, Canada, 2020.

[12] Jérôme Bolte and Edouard Pauwels. Conservative set valued ﬁelds, automatic diﬀer-
entiation, stochastic gradient method and deep learning. Mathematical Programming,
2020.

[13] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,
Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-
Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs,
2018.

[14] Antonin Chambolle and Thomas Pock. Learning consistent discretizations of the total

variation. SIAM Journal on Imaging Sciences, 14(2):778–813, 2021.

[15] Bruce Christianson. Reverse accumulation and attractive ﬁxed points. Optimization

Methods and Software, 3(4):311–326, 1994.

[16] Frank H. Clarke. Optimization and nonsmooth analysis. Canadian Mathematical Society
Series of Monographs and Advanced Texts. John Wiley & Sons, Inc., New York, 1983.
A Wiley-Interscience Publication.

[17] Michel Coste. An introduction to o-minimal geometry. Istituti editoriali e poligraﬁci

internazionali Pisa, 2000.

[18] Michel Coste. An introduction to semialgebraic geometry, 2000.

[19] Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle

regression. Annals of statistics, 32(2):407–499, 2004.

[20] Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai.
Implicit deep learning. SIAM Journal on Mathematics of Data Science, 3(3):930–958,
2021.

[21] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance

estimation with the graphical lasso. Biostatistics, 9(3):432–441, 12 2007.

[22] Daniel Gabay. Applications of the method of multipliers to variational inequalities. In
Studies in mathematics and its applications, volume 15, pages 299–331. Elsevier, 1983.

[23] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global con-
vergence of the heavy-ball method for convex optimization. In 2015 European control
conference (ECC), pages 310–315. IEEE, 2015.

[24] Jean Charles Gilbert. Automatic diﬀerentiation and iterative processes. Optimization

Methods and Software, 1(1):13–21, 1992.

[25] Pontus Giselsson and Stephen Boyd. Linear convergence and metric selection for douglas-
rachford splitting and admm. IEEE Transactions on Automatic Control, 62(2):532–544,
2016.

12

[26] Andreas Griewank, Christian Bischof, George Corliss, Alan Carle, and Karen Williamson.
Derivative convergence for iterative equation solvers. Optimization Methods and Software,
2(3-4):321–355, 1993.

[27] Andreas Griewank and Christèle Faure. Piggyback diﬀerentiation and optimization. In

Large-scale PDE-constrained optimization, pages 148–164. Springer, 2003.

[28] Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and tech-

niques of algorithmic diﬀerentiation. SIAM, 2008.

[29] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli
Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J.
Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew
Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre
Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi,
Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature,
585(7825):357–362, September 2020.

[30] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science &

Engineering, 9(3):90–95, 2007.

[31] Sham M Kakade and Jason D Lee. Provably correct automatic sub-diﬀerentiation for
qualiﬁed programs. In Advances in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018.

[32] Wonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. On correctness of
automatic diﬀerentiation for non-diﬀerentiable functions. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing
Systems, volume 33, pages 6719–6730. Curran Associates, Inc., 2020.

[33] Seppo Linnainmaa. The representation of the cumulative rounding error of an algorithm
as a Taylor expansion of the local rounding errors. Master’s thesis, Univ. Helsinki, 1970.

[34] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperpa-
rameters by implicit diﬀerentiation. In Silvia Chiappa and Roberto Calandra, editors,
Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and
Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1540–1552.
PMLR, 26–28 Aug 2020.

[35] Julien Mairal and Bin Yu. Complexity analysis of the lasso regularization path. In
Proceedings of the 29th International Coference on International Conference on Machine
Learning, pages 1835–1842, 2012.

[36] Swann Marx and Edouard Pauwels. Path diﬀerentiability of ode ﬂows. arXiv preprint

arXiv:2201.03819, 2022.

[37] Sheheryar Mehmood and Peter Ochs. Automatic diﬀerentiation of some ﬁrst-order meth-
ods in parametric optimization. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 1584–1594. PMLR, 2020.

[38] Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex

programming. SIAM, 1994.

[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

13

[40] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Inter-

national conference on machine learning, pages 737–746. PMLR, 2016.

[41] Boris Polyak. Introduction to optimization. In Optimization Software, Publications

Division. Citeseer, 1987.

[42] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning
with implicit gradients. Advances in neural information processing systems, 32, 2019.

[43] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer

Science & Business Media, 2009.

[44] Halsey Lawrence Royden and Patrick Fitzpatrick. Real analysis, volume 32. Macmillan

New York, 1988.

[45] David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representa-

tions by back-propagating errors. Nature, 323(6088):533–536, October 1986.

[46] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

[47] Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of statistics,

7:1456–1490, 2013.

[48] Ryan J. Tibshirani. Adaptive piecewise polynomial estimation via trend ﬁltering. The

Annals of Statistics, 42(1):285 – 323, 2014.

[49] Martin J Wainwright, John Laﬀerty, and Pradeep Ravikumar. High-dimensional graphi-
cal model selection using \ell_1-regularized logistic regression. In B. Schölkopf, J. Platt,
and T. Hoﬀman, editors, Advances in Neural Information Processing Systems, volume 19.
MIT Press, 2006.

[50] R. E. Wengert. A simple automatic derivative evaluation program. Communications of

the ACM, 7(8):463–464, August 1964.

14

This is the appendix for “Convergence of piggyback diﬀerentiation of nonsmooth iterative
solvers”.

Appendices

A Properties of aﬃne iterations on compact subsets

B Existence of a conservative Jacobian for autodiﬀ

C Connection with implicit diﬀerentiation

D Semialgebraic Lipschitz gradient selection functions

E Proximal splitting algortihms in convex optimization

F Inertial methods

G Experiments details

H Assets used

15

19

22

22

23

26

27

28

A Properties of aﬃne iterations on compact subsets

A.1 Banach–Picard theorem: Proof of Theorem 1

For a compact set,

we denote by

Z

(cid:107)Z(cid:107)

sup the maximal norm of elements in

:

Z

sup = sup

z
z∈Z (cid:107)

(cid:107)

.

(cid:107)Z(cid:107)

In order to prove our ﬁxed point result, we need ﬁrst the following lemma.

Lemma 1 (Bounding Hausdorﬀ distances) Let
sets, such that

then

and

+

+

X ⊂ Y

Z

Y ⊂ X

,

,

Y

X

Z ⊂

Rp be nonempty compact

Z
,

X

dist(

)

Y

sup

≤ (cid:107)Z(cid:107)

Proof : The ﬁrst inclusion says that for any x
x = y(x) + z(x). We deduce that for any x

, there is y(x)

, z(x)

∈ Y

∈ Z

such that

∈ X

min
y∈Y (cid:107)

x

y

(cid:107)

= min

y∈Y (cid:107)

−

y(x)

−

y

−

(cid:107) ≤ (cid:107)

z(x)

z
max
z∈Z (cid:107)

(cid:107)

(cid:107) ≤

∈ X
z(x)

. Symmetrically, maxy∈Y minx∈X
(cid:107)

y

x
(cid:107)

−

(cid:107) ≤
(cid:3)

and the result follows.

Therefore, maxx∈X miny∈Y
maxz∈Z
We now prove Theorem 1.
Proof of Theorem 1: Recall that the action of
by

maxz∈Z

x
(cid:107)

z
(cid:107)

(cid:107) ≤

−

(cid:107)

(cid:107)

y

z

the projections of

and
A
A,
=
A
∃
{
matrices in
compact subsets is a strict contraction in Hausdorﬀ metric. Indeed, for any
subsets of Rp×m, we have by using Lemma 1 and noting that

on matrices is deﬁned in (6) and
on the ﬁrst p and last l columns respectively, that is
is a compact set and that all
to
J
compact
X
preserves the inclusion,

have an operator norm of at most ρ. We claim that the restriction of

and similarly for B. Note that

B
B, [A, B]

∈ J }

A

A

J

J

Y

,

J

)

(
X
)

J
(
Y

J

(
Y

⊂ J
(
X

⊂ J

+ dist(

,

+ dist(

)B)
Y
)B)

X
,

X

Y

(
Y

⊂ J
(
X

⊂ J

) + dist(

) + dist(

,

,

X

X

)
A

B

B

A

Y
)

Y

(

Y

(
X

⊂ J

⊂ J

) + ρdist(

) + ρdist(

)B
)B

,

,

Y

Y

X

X

15

where the last inclusion follows because
norm) of p
at most ρ. We deduce that dist(
action of
J
Let us show that (

×
on subsets of p

B
A
m matrices, since by assumption all square matrices in
),

ρB, where B is the unit ball (for the Euclidean
have operator norm
) using Lemma 1, that is the

,
Y
m matrices is ρ Lipschitz with respect to Hausdorﬀ metric.

Y
k)k∈N remains in a bounded set, we have for all k

ρdist(

A

))

⊂

≤

×

X

X

J

J

(

(

X

k+1

(cid:107)X
which entails

sup
(cid:107)

k +

≤ (cid:107)AX

sup

B(cid:107)

k
≤ (cid:107)AX

sup +
(cid:107)

sup

(cid:107)B(cid:107)

ρ

(cid:107)X

k

sup +
(cid:107)

≤

(cid:107)B(cid:107)

sup,

k+1

(cid:107)X

sup

(cid:107)

−

(cid:18)

ρ

sup
ρ ≤

(cid:107)B(cid:107)
1
−

(cid:107)X

k

(cid:107)

sup

−

(cid:19)

.

sup
ρ

(cid:107)B(cid:107)
1
−

We distinguish two cases

• if

sup > (cid:107)B(cid:107)sup
1−ρ

k
(cid:107)
(cid:107)X
it decreases.

, then

k+1

(cid:107)X

(cid:107)

sup gets either closer to (cid:107)B(cid:107)sup
1−ρ

or below it, in particular

• if

k
(cid:107)X
all k.

sup
(cid:107)

≤

(cid:107)B(cid:107)sup
1−ρ

then

(cid:107)X

k+1

sup
(cid:107)

≤

(cid:107)B(cid:107)sup
1−ρ

and we remain below this threshold for

All in all, for all k

N,

∈

(cid:107)X

k+1

sup
(cid:107)

≤

(cid:26)

max

k

(cid:107)X

sup, (cid:107)B(cid:107)
(cid:107)
−

1

sup
ρ

(cid:27)

(cid:26)

. . .

≤

≤

max

(cid:107)X

0

(cid:107)

(cid:27)

,

sup, (cid:107)B(cid:107)
−

1

sup
ρ

.

k

(cid:107)

≤

sup

(cid:107)B(cid:107)sup
1−ρ

C ⊂

and lim supk (cid:107)X
We have shown that the sequence remains in a bounded set so that the recursion actually
takes place in a compact set
Rp×m which contains all the iterates in its interior, we
consider the restriction of the topology to this subset. By [4, Theorem 3.85], the closed
subsets form a complete metric space. The result is an application of Banach-Picard theorem
is the unique ﬁxed
(for example [44, Section 10.3]). In particular (see [4, Theorem 3.88]),
point of
and it is closed and bounded, hence compact. Note that we can consider larger
compact sets to take into account larger initializations, the ﬁxed point remains the same.
Indeed for a larger compact ˜
and is still a ﬁxed point
C
(cid:3)
of

,
C
when the topology is restricted to ˜
and this ﬁxed point must be unique.
C

is in the interior of

containing

L

L

J

J

C

A.2 Properties of the ﬁxed-set mapping

We now equip the set of matrices Rp×(p×m) with the norm
where A
the corresponding Hausdorﬀ distance.

Rp×p and B

p,m = max
[A, B]
(cid:107)}
(cid:107)
(cid:107)
Rp×m. The set of compact subsets of Rp×(p+m) is endowed with

op,
(cid:107)

{(cid:107)

B

A

∈

∈

(cid:107)

Deﬁnition 1 (Aﬃne contraction sets) For ρ
∈
pact subsets of matrices in Rp×(p+m) such that for all
we have

ρ where A

op

[0, 1), we denote by
Rp×(p+m),

ρ, the set of com-
C
,
ρ and all M
S ⊂ C

∈ S

S ⊂

Rp×p is the matrix made of the ﬁrst p columns of M .

A
(cid:107)

(cid:107)

≤

∈

Given
mapping as deﬁned in Theorem 1. We have the following

ρ, we denote by ﬁx(

J ∈ C

J

) the unique ﬁxed point of the corresponding iteration

Proposition 5 (Monotonicity of the ﬁxed set) Given
nition 1), such that

, we have

˜
J

J ⊂

ρ and ˜

J ∈ C

J ∈ C

ρ (as in Deﬁ-

ﬁx(

J

)

ﬁx( ˜
J

).

⊂

16

Proof : Setting

0 = ﬁx(

J

X

), we have

0 =

X

0)

(
X

J

⊂

(

˜
J

0),

X

and the result follows by the same argument as in the last paragraph of the proof of Theorem
(cid:3)
1.

Proposition 6 (The ﬁxed-set mapping is locally Lipschitz continuous) The func-
tion ﬁx is locally Lipschitz continuous on
ρ (as in Deﬁnition 1). More precisely, for
any

C

ρ and

ρ,

0
J

∈ C

J ∈ C

dist (ﬁx(

J

0), ﬁx(

))

J

≤

(cid:18) 1
1

−

+

ρ

B0

sup[A0,B0]∈J0 (cid:107)
ρ)2
(1
−

(cid:19)

(cid:107)

dist(

0,

J

J

)

Proof : Given
ρ and
and Bpm are considered with respect to the norm

ρ, we remark that

J ∈ C

∈ C

J

0

0 + dist(
J ⊂ J
J
pm. This means

(cid:107) · (cid:107)

)Bpm, where dist

0,

J

J ⊂ {

[A0, B0] + [C, D], [A0, B0]

0,

∈ J

[C, D]
p,m
(cid:107)

(cid:107)

≤

dist(

J

0,

)
}

J

We have

J

(ﬁx(

0)) =

J

{

, X

AX + B, [A, B]
∈ J
A0X + B0, [A0, B0]
⊂ {
+
[C, D]
(cid:107)
{
0(ﬁx(

CX + D,
0)) +

∈ J
mp
(cid:107)
CX + D,
{
CX + D,
{

=

J

ﬁx(

∈
0, X

∈
dist(

≤
[C, D]

0)
J
}
ﬁx(
0,

J
mp
(cid:107)

mp

≤

(cid:107)
[C, D]
(cid:107)
(cid:107)
This sets one inclusion. Similarly, we have

J
= ﬁx(

J
0) +

0)
}
), X

J

J

∈
dist(
0,

J

J

J

0)
J
}
), X

ﬁx(
0,
J
), X

∈

0)
}

∈
ﬁx(

ﬁx(
0)

J
.

J

}

≤
dist(

ﬁx(

J

0) =

J

0(ﬁx(
(ﬁx(

0))
J
0)) +

⊂ J

J

CX + D,

{

[C, D]
(cid:107)

mp
(cid:107)

≤

dist(

0,

J

J

), X

∈

ﬁx(

0)
}

J

.

Recall that
dist(

0,

J

J

[C, D]

(cid:107)
) and X

mp = max
(cid:107)
0),
ﬁx(
∈

J

C

op,
(cid:107)

(cid:107)

D

{(cid:107)

, we have for any [C, D] with

(cid:107)}

[C, D]
(cid:107)

mp
(cid:107)

≤

CX + D
(cid:107)

C

op

ﬁx(

sup +

0)
(cid:107)

D
(cid:107)

(cid:107) ≤

dist(

0,

J

J

)(1 +

ﬁx(
(cid:107)

J

0)
(cid:107)

sup).

(cid:107) ≤ (cid:107)
We deduce using Lemma 1 that dist(ﬁx(
Setting

0),
J
0), invoking Theorem 1 with

0 = ﬁx(

J

J

(cid:107)

(cid:107)

X

J

dist(ﬁx(

0), ﬁx(

J

J

))

≤

≤

dist(

0,

J

J

dist(

J

0,

J

J
)(1 +
1
(1

−
−

)

(ﬁx(

0)))

0,
≤
and k = 0, we have

dist(

J

J

)(1 +

ﬁx(

0)

J

(cid:107)

(cid:107)

J

sup).

J

0)

sup)
(cid:107)

ﬁx(
(cid:107)
ρ
ρ + sup[A0,B0]∈J0 (cid:107)
−

ρ)2

(1

B0

)
(cid:107)

.

(cid:3)

A.3 Perturbed iterations

The following proposition shows that the linear convergence property is actually stable to
perturbations. It will be useful to show that all potential limits of unrolling algorithmic
diﬀerentiation recursions are contained in the corresponding ﬁxed point set.

17

Proposition 7 (Perturbed set sequences) Let ρ < 1 and (cid:15) > 0 such that ρ + (cid:15) < 1. Let
N
(

ρ (as in Deﬁnition 1). Assume that for all k

k)k∈N be a sequence in

ρ+(cid:15) and ¯
C

J ∈ C

J

∈

¯
or in other words
J
recursion on compact sets

k
J

⊂

gappm(

k, ¯
J
+ (cid:15)Bpm where Bpm is the unit ball of the norm

≤

J

(cid:15)

)

pm. Then the

(cid:107) · (cid:107)

k+1 =

X

k(

k)

J

X

satisﬁes for all k

N

∈
gap(
k, ﬁx(
(ρ + (cid:15))k (1 + ρ + (cid:15))

))

X

J

≤

In other words,

k
X
(cid:15) :=

ﬁx( ¯
⊂
J
J + [C, D], J
{

k+1 =

Proof : Set
J
satisfying the recursion, ˜
X

and by recursion

k
X

⊂

(cid:107)X

0

B
sup + sup[A,B]∈ ¯J (cid:107)
(cid:107)
(cid:15)
1
−

−

ρ

+ (cid:15)

(cid:107)

(1

−

+ (cid:15)

B
ρ + sup[A,B]∈ ¯J (cid:107)
ρ)2
−

(1

)
(cid:107)

.

) + C(ρ, (cid:15), k)B where C(ρ, (cid:15), k) is the constant above.

. Denote by ( ˜
X
}

k)k∈N the sequence

(cid:15)
0. We have

¯
[C, D]
,
mp
≤
(cid:107)
(cid:107)
J
∈
0 = ˜
(cid:15)( ˜
k) with
X
J
X
X
0) = ˜
1 = ¯
1
J
X
X
N. By Theorem 1, we have

(
X

⊂ J

0)

(cid:15)(

(cid:15)))

≤

J

(ρ + (cid:15))k dist(
X
1
−

0,
J
ρ
−

(cid:15)(
X
(cid:15)

0))

.

X
˜
k for all k
X
dist( ˜
X

k, ﬁx(

∈

We deduce from Proposition 6 that for all k

N,

∈

))

(cid:15))) + dist(ﬁx(

(cid:15)), ﬁx( ¯
J
J
(1

))

k, ﬁx( ¯
J

dist( ˜
X
dist( ˜
k, ﬁx(
X
J
(ρ + (cid:15))k dist(
X
1
−

0,
J
ρ
−
(ρ + (cid:15))k (1 + ρ + (cid:15))

≤

≤

≤

And the result follows because

0))

(cid:15)(
X
(cid:15)

0

(cid:107)X

+

−

(1

B
ρ + sup[A,B]∈ ¯J (cid:107)
ρ)2
−
B
sup + sup[A,B]∈ ¯J (cid:107)
(cid:107)
(cid:15)
1
−

+ (cid:15)

−

ρ

(cid:107)

+

)
(cid:107)

dist(

(1

−

(cid:15), ¯
J

)

J
B
ρ + sup[A,B]∈ ¯J (cid:107)
ρ)2
−

(1

max
X∈Xk

min
L∈ﬁx( ¯J ) (cid:107)

X

−

L

(cid:107) ≤

max
X∈ ˜Xk

min
L∈ﬁx( ¯J ) (cid:107)

X

−

L

(cid:107) ≤

dist( ˜
X

k, ﬁx( ¯
J

)).

This allows to obtain explicit convergence results as follows

)
(cid:107)

(cid:15).

(cid:3)

Corollary 4 (Limit of iterations with vanishing perturbations) Let ρ < 1 and ¯
ρ (as in Deﬁnition 1). Let (

k)k∈N be a sequence of matrices such that for all k

N

J ∈

C

J

∈

gappm(

J

k, ¯
J

)

≤

(cid:15)k

where ((cid:15)k)k∈N is a positive sequence such that there exists a constant a > 0 such that (cid:15)k
for all k

N. Then for the recursion on compact sets of p

m matrices

aρk

≤

∈

×

J
There are constants C, c > 0 such that for all k

X

X
N

k+1 =

k(

k)

Furthermore, one can take c = log

k, ﬁx(

gap(
X
(cid:16) 1√

(cid:17)

ρ+(cid:15)

∈
))

J

≤

Ce−ck.

for arbitrary (cid:15) > 0.

18

N where (cid:15) + ρ < 1. Without
Proof : We consider K
≤
∈
loss of generality, we may assume that K = 0. Using the same notations as in the proof
˜
of Proposition 7, we have
N. Furthermore, it follows from the same
k for all k
X
arguments as in the proof of Theorem 1 that

N such that (cid:15)k

(cid:15) for all k

⊂

X

∈

∈

k

(cid:107)X
for a constant M > 0. Now choose k
to k, we have for all m

N

∈

k

sup
(cid:107)

˜
k
X

≤ (cid:107)

sup
(cid:107)

≤

M,

(15)

N, applying Proposition 7 shifting the initialization 0

∈

L
(cid:107)

max
X∈Xk+m

min
L∈ﬁx(J ) (cid:107)

X

−
(ρ + (cid:15)k)m (1 + ρ + (cid:15)k)

B
sup + sup[A,B]∈ ¯J (cid:107)
(cid:107)
(cid:15)k
1
−
(ρ + (cid:15))m (1 + ρ + (cid:15))M + sup[A,B]∈ ¯J (cid:107)

k
(cid:107)X

+ (cid:15)

−

B

ρ

(cid:107)

+ (cid:15)k

(cid:107)

+ (cid:15)k

+ aρk (1

−

1

ρ

(cid:15)

≤

≤

−
where we have used the bound (15) and the fact that (cid:15)k
(1−ρ+sup[A,B]∈ ¯J (cid:107)B(cid:107))
u =
(1−ρ)2

(1+ρ+(cid:15))M +sup[A,B]∈ ¯J (cid:107)B(cid:107)+(cid:15)
1−ρ−(cid:15)

and v = a

−

)
(cid:107)

−

(1

(1

B
ρ + sup[A,B]∈ ¯J (cid:107)
ρ)2
−
B
ρ + sup[A,B]∈ ¯J (cid:107)
ρ)2
−
(cid:15) and (cid:15)k

)
(cid:107)

(1

,

≤
we have

≤

aρk. Setting

max
X∈X2k

min
L∈ﬁx(J ) (cid:107)

X

−

L

(cid:107) ≤

u(ρ + (cid:15))k + vρk

≤

(u + v)(ρ + (cid:15))2k/2

u + v
(ρ + (cid:15))1/2 (ρ + (cid:15))2k/2,

≤

max
X∈X2k+1

min
L∈ﬁx(J ) (cid:107)

X

−

L

(cid:107) ≤

u(ρ + (cid:15))k+1 + vρk

u + v
(ρ + (cid:15))1/2 (ρ + (cid:15))(2k+1)/2.

≤

Since k was arbitrary, this proves the desired result.

(cid:3)

B Existence of a conservative Jacobian for autodiﬀ

B.1 Regularity of J pb
¯x
We recall the main notations and elements of Assumption 1. We assume that F is locally
Lipschitz, path diﬀerentiable, and denote by JF : Rp+m ⇒ Rp×(p+m) a conservative Jacobian
JF (x, θ) is such that the operator norm of A is at
for F . Now assume that any pair [A, B]
most ρ < 1, that is for all x and θ, JF (x, θ)
ρ (as in Deﬁnition 1). Deﬁne the following
set-valued map

∈ C

∈

J pb
¯x : θ ⇒ ﬁx [JF (¯x(θ), θ)] .

Here, ¯x(θ) = ﬁx(Fθ) is the unique ﬁxed point of the algorithmic recursion so that we actually
have

We have the following

J pb
¯x : θ ⇒ ﬁx [JF (ﬁx(Fθ), θ)] .

Lemma 2 (Regularity of J pb
has a closed graph.

¯x ) The mapping J pb

¯x is nonempty valued, locally bounded and

Proof : The fact that J pb
is locally bounded and non empty valued comes from the fact
¯x
that JF is locally bounded with nonempty values and ¯x is locally Lipschitz combined with
Theorem 1.
By local Lipschitz continuity of ¯x and the fact that JF has a closed graph, the set-valued
) with respect to the
map θ ⇒ JF (¯x(θ), θ) also has a closed graph. By continuity of ﬁx(
Hausdorﬀ distance, see Proposition 6, J pb
(cid:3)
¯x

has a closed graph.

J

19

B.2 Proof of Theorem 2

Proof : Following Remark 2, we set

J imp
¯x

: θ ⇒ (cid:8)(I

−

A)−1B, [A, B]

JF (¯x(θ), θ)(cid:9) ,

∈

a conservative Jacobian for ¯x and L0 = J imp

¯x

. Now set by recursion for all k

N

∈

Lk+1 : θ ⇒ JF (¯x(θ), θ)(Lk(θ)).

Recall that this means for all θ

Rm and k

∈

∈
AL + B, [A, B]
{

N

∈

JF (¯x(θ), θ), L

Lk(θ)

.
}

∈

Lk+1(θ) =

Since F (¯x(θ), θ) = ¯x(θ) for all θ, JF is conservative for F and L0 is conservative for ¯x, we
have by induction that for all k
Fix l : Rm
→
all θ
Lemma 2. Set for all k

Rm. Such a selection exist by [4, Theorem 18.20] because J pb
¯x
N a measurable selection

Rm an arbitrary Borel measurable selection in J pb
¯x

J pb
¯x (θ) for
∈
has a closed graph by

N, Lk is conservative for ¯x.

, that is l(θ)

∈

∈

∈

lk : θ

arg min

z
z∈Lk(θ) (cid:107)

.
l(θ)
(cid:107)

−

→

(cid:107)

−

l(θ)

z
→ (cid:107)

is Caratheodory (continuous in z, measurable in θ), so such a
The function (z, θ)
selection exists (Aliprantis Theorem 18.19). By Theorem 1, we have that dist(Lk(θ), J pb
¯x (θ))
tends to 0 as k grows, for all θ
Rm, where the convergence is in Hausdorﬀ distance.
Actually since all set-valued objects are locally bounded, the convergence occurs uniformly
on every compact. This implies in particular that lk converges pointwise to l.
Fix an absolutely continuous path γ : [0, 1]

Rm. We have for all k

N, by conservativity,

∈

→

∈

¯x(γ(1))

−

¯x(γ(0)) =

lk(γ(t)) ˙γ(t)dt.

0

(cid:90) 1

γ is measurable, converges pointwise to l

Furthermore, lk
◦
bounded, let K be such a bound. The integrable function g : t
integrand and lk
γ
theorem (see [44, Section 4.4] ), we have

˙γ converges pointwise to l

×

×

γ

◦

◦

◦

γ and lk
K

γ can be uniformly
◦
dominates the
˙γ(t)
(cid:107)
˙γ. By the dominated convergence

(cid:55)→

(cid:107)

¯x(γ(1))

−

¯x(γ(0)) =

l(γ(t)) ˙γ(t)dt.

0

(cid:90) 1

L has a Castaing representation with a dense sequence of measurable selection [4, Theorem
18.14]. Since l was an arbitrary measurable selection in L, conservativity of L follows by [36,
(cid:3)
Lemma 8].

B.3 Proof of Corollary 1
Proof : Fix θ. We have xk(θ)
that JF (xk(θ), θ)
Proposition 7, letting (cid:15)
a singleton almost everywhere, equal to the classical Jacobian.

→
JF (¯x(θ), θ) + (cid:15)B for all k

N such
K. The result is then a consequence of
which must be
(cid:3)

0. The last part is due to the conservativity of J pb
¯x

¯x(θ), so that for any (cid:15) > 0, there exists K

→

≥

⊂

∈

B.4 Proof of Corollary 3
Proof : Deﬁne (Lk)k∈N, a sequence of conservative Jacobians for ¯x as in the begining of the
proof of Theorem 2 in Appendix B.2. By [12, Theorem 1], for each k
N, there is a full
Sk. Similarly, there exists a full
measure set Sk

Rm such that Lk(θ) = (cid:8) ∂ ¯x

∂θ (θ)(cid:9) for all θ

∈

⊂

∈

20

measure set S−1
has full measure and for all θ

Rm such that J pb

⊂

¯x (θ) = (cid:8) ∂ ¯x
S and for all k
∈
(cid:26) ∂ ¯x
(cid:27)
∂θ

(θ)

∈

J pb
¯x (θ) =

Lk(θ) =

(cid:26) ∂ ¯x
∂θ

(cid:27)

(θ)

.

∂θ (θ)(cid:9) for all θ
N,

S−1. Setting S =

+∞

i=−1Si, S
∩

∈

Following the proof of Theorem 2 in Appendix B.2, Lk converges to J pb
in Hausdorﬀ distance,
¯x
which means that convergence occurs in the classical sense since all sets in the sequence are
(cid:3)
singletons.

B.5 Proof of Proposition 1
Proof : Under the setting of Corollary 2, for almost all θ
to the following, and all k

N

∈

Rm, recursion (PB) or (5) reduce

∈

Jk+1 = AkJk + Bk

(16)

, Ak = ∂F

∂x (xk, θ) and Bk = ∂F

where Jk = ∂xk
∂θ (xk, θ) are classical Jacobians and Jk converges
∂θ
1. With the notation
to the classical Jacobian of ∂ ¯x
of Algorithm 1, for the forward mode, multiplying (16) on the right by ˙θ, we have for all
i

∂θ (θ). Fix such a θ

Rm and k

1, . . . k

N, k

≥

∈

∈

∈

Ji ˙θ = Ai−1Ji−1 ˙θ + Bi−1 ˙θ.

Setting ˙xi = Ji ˙θ, this is exactly the recursion implemented by Algorithm 1 in forward mode.
Corollary 2 and the result follows from convergence of Jk.
As for the backward mode a simple recursion shows that

Jk = Ak−1Ak−2 . . . A0J0
+ Ak−1Ak−2 . . . A1B0
+ . . .
+ Ak−1Ak−2 . . . AiBi−1
+ . . .
+ Ak−1Bk−2
+ Bk−1.

Setting B−1 = J0, we may rewrite equivalently,


Jk = Bk−1 +

k−1
(cid:88)



i
(cid:89)



Aj

 Bi−1.

Transposing and multiplying on the right by ¯wk, we have

i=0

j=k−1

k ¯wk = BT
J T

k−1 ¯wk +

k−1
(cid:88)

i=0

BT

i−1

We set for all i = 0, . . . , k

1,

−





k−1
(cid:89)



AT
j

 ¯wk.

j=i

¯wi =

k−1
(cid:89)

j=i

AT

j ¯wk.

We have the backward recursion relation, for i = k, . . . , 1

¯wi−1 = AT

i−1 ¯wi,

21

(17)

(18)

(19)

(20)

which is the recursion implemented by Algorithm 1 in reverse mode. Combining (19) and
(20), we obtain

k ¯wk = BT
J T

k−1 ¯wk +

k−1
(cid:88)

Bi−1 ¯wT =

k
(cid:88)

BT

i−1 ¯wi + J T

0 ¯w0,

i=0
which is the quantity accumulated in ¯θk in Algorithm 1. This proves that ¯θT
k
the backward mode is indeed equal to ¯wT
of both ¯wk and Jk as k

returned by
k Jk and the convergence follows from convergence
(cid:3)

i=1

.
→ ∞

C Connection with implicit diﬀerentiation

Recall that for all θ

J imp
¯x

(θ) = (cid:8)(I
M,
=
{

−

∃

A)−1B, [A, B]
[A, B]

JF (¯x(θ), θ)(cid:9)
JF (¯x(θ), θ) M = AM + B

∈

∈

.

}

J
N, J imp
¯x
∈
) = J pb

= JF (¯x(θ), θ), we have therefore that J imp

Setting
all k
ﬁx(
Jacobians converges towards a classical implicit derivative.
However, the inclusion J imp
(θ)

k(J imp
¯x

⊂ J

(θ)

(θ)

J

¯x

⊂
¯x (θ). In particular, if F is continuously diﬀerentiable, then (PB) with classical

(θ)) and passing to the limit using Theorem 1, J imp

(θ)

¯x

J pb
¯x (θ) may be strict as the following example shows.

(J imp
¯x

(θ)). By recursion, for

⊂ J

Example 1 Set

=

J

=

A

We set

¯x

⊂
[A, B], A
{
(cid:26)(cid:18) λ+1

, B
(cid:19)

∈ A
0
2−λ
4

∈ B}

, where

(cid:27)

, λ

∈

[0, 1]

(cid:26)(cid:18)1
1

(cid:19)(cid:27)

.

=

B

4
0

= (I

)−1

=

B

− A

T

(cid:19)

(cid:26)(cid:18) 4
3−λ
4
2+λ

, λ

∈

(cid:27)

[0, 1]

.

As already observed, we have
ﬁxed point of the aﬃne iteration and it is only contained in it.
Indeed, we have

T ⊂ AT

+

B

, but the inclusion is strict. Therefore

is not a

T

(cid:18) 1+1
4
0

(cid:19)

0
2−1
4

(cid:19) (cid:18) 4
3−0
4
2+0

(cid:19)

(cid:18)1
1

+

=

(cid:19)

(cid:18) 5
3
3
2

∈ AT

+

.

B

However solving for λ

(cid:19)

(cid:18) 5
3
3
2

=

(cid:19)

,

(cid:18) 4
3−λ
4
2+λ

the ﬁrst equation requires λ = 3
vector does not belong to

.

T

5 while the second requires λ = 2

3 which shows that the given

D Semialgebraic Lipschitz gradient selection functions

D.1 Lipschitz property of conservative Jacobians of selections

Lemma 3 (Conservative Jacobians of selections are Lipschitz-like) Let F be con-
Rp, there exists
tinuous, semialgebraic with Lipschitz gradient selection. Then for each x0
R > 0 such that

∈

gap(J s

F (x), J s

F (x0))

x
L
(cid:107)

x0

,
(cid:107)

−

≤

x,

∀

x
(cid:107)

−

x0

(cid:107) ≤

R,

where L is the Lipschitz constant given by the selection structure of F .

22

Proof : Fix x0
1, . . . , m
{

}

∈
deﬁned as

Rp and consider the function g which associates to r > 0 a subset of

g(r) =

∪(cid:107)x−x0(cid:107)≤r I(x).

0. The function g is
The function g is semialgebraic and therefore it admits a limit as r
actually piecewise constant so that the limit is reached for some R > 0 by semialgebraicity.
This means that there is R > 0 and an index set I
I for all x
⊂
such that
R, there exists x such
R. Furthermore, for each i
x0
(cid:107) ≤
r and Fi(x) = F (x). By continuity of each component Fi, we have for each
that
x
(cid:107)
I, Fi(x0) = F (x0), that is I
i
We deduce that for each x such that

1, . . . , m
⊂ {
I and all 0 < r
∈

such that I(x)

I(x), we have

I(x0).
x

R and i

−
(cid:107) ≤

x
(cid:107)
x0

→

≤

−

⊂

∈

}

x0

min

V ∈J s

F (x0)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

V

∂Fi
∂x

−

(x)

(cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13) ≤

−
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107) ≤

∈

∂Fi
∂x

(x0)

∂Fi
∂x

−

(x)

(cid:13)
(cid:13)
(cid:13)
(cid:13) ≤

L

x
(cid:107)

x0

.
(cid:107)

−

Fix any Z
distance, we have

∈

F (x), it is a convex combination of ∂Fi
J s

∂x (x) for i

I(x) so by convexity of the

∈

min

V ∈J s

F (x0) (cid:107)

V

Z

−

(cid:107) ≤

x
L
(cid:107)

x0

,
(cid:107)

−

which proves the result since this allows to bound the supremum over Z
desired quantity.

F (x) by the
J s
(cid:3)

∈

D.2 Proof of Corollary 3
Proof : This is a consequence of linear convergence of the recursion xk+1 = F (xk, θ)
(cid:3)
combined with Lemma 3 and Corollary 4.

E Proximal splitting algortihms in convex optimization

E.1 Proof of Proposition 2
Proof : We consider the gradient step operation Hα : (x, θ)
all (x, θ),

x

α

∇

−

(cid:55)→

xf (x, θ). We have for

Fα(x, θ) = Gα(Hα(x, θ), θ).

[I

αA,

By Assumption 2, both Gα and Hα are 1-Lipschitz in x for ﬁxed θ and we are going
to show that if either f or g satisfy the strong convexity condition, the corresponding
map is a strict contraction in x for ﬁxed θ. Furthermore, the mapping Jac c
Hα : (x, θ) ⇒
(cid:110)
is the Clarke Jacobian of Hα. By Assumption 2, all the
functions are path-diﬀerentiable [12] and one may obtain a conservative jacobian for F by
applying diﬀerential calculus rules [12]. We set for all (x, θ) a conservative Jacobian for Fα,
xf (x, θ), θ)(cid:9)
JFα (x, θ) = (cid:8)[C(I
(21)

J 2
f (x, θ), [C, D]

αB], [A, B]

αCB + D],

J 2
f (x, θ)

JGα (x

[A, B]

αA),

∇

−

−

−

−

−

(cid:111)

∈

∈

∈

α

−

∇

αA) in (21) is symmetirc with eignevalues in [

Whenever
xf is diﬀerentiable at (x, θ), the ﬁrst p columns of its Jacobian form a symmetric
positive deﬁnite square matrix with eigenvalues at most L. This implies that the matrix
1.
(I
−
Similarly, whenever Gα is diﬀerentiable, since it is 1-Lipschitz in x for ﬁxed θ and the
gradient of a C 1 function, the ﬁrst p columns of its Jacobian form a symmetric positive
deﬁnite square matrix with eigenvalues at most 1. This implies that the matrix C in (21) is
symmetric with eignevalues in [0, 1]. In addition, we have the following;

1, 1] and strictly greater than

−

23

• Assume that for all θ, f is µ-strongly convex. In this case, similarly as above the

matrix (I

αA) in (21) has eigenvalue in (

1, 1) for all (x, θ).

−

−

• Assume that for all θ, g is µ-strongly convex. In this case, similarly as above the matrix

C in (21) has eigenvalue in [0, 1/(1 + αµ)] for all (x, θ) [6, Proposition 23.13].

In both cases, the product C(I
Assumption 1 holds.

αA) in (21) has operator norm strictly smaller than 1 and
(cid:3)

−

E.2 Proof of Proposition 3
Proof : From [6, Proposition 23.11], both Rαf and Rαg are 1-Lipschitz. We are going to
show that Rαf is a strict contraction and the result will follow. Since f is C 1,1 in x, we have
for all θ

Rm,

∈

Set Hα(z, x, θ) = z + α

xf (z, θ)

∇

−

z = proxαf (·,θ)(x)

z + α

⇔
∇
x, we have that

xf (z, θ)

x = 0

−

Jac c

Hα (z, x, θ) ⇒

[I + αA,

I, αB]
}

{
is the Clarke Jacobian of Hα. Similarly as in Appendix E.1, by strong convexity of f , the
matrix I + αA in (22) is symmetric with eigenvalues strictly greater than 0 and smaller than
1. By implicit diﬀerential calculus rule in [10, Theorem 2], the mapping

−

(22)

Jproxαf (·,θ) (x, θ) ⇒

(cid:110)

[(I + αA)−1,

α(I + αA)−1B], [A, B]

−

∈

J 2
f (proxαf (·,θ), θ)

(cid:111)

(23)

is conservative for (x, θ)
symmetric eigenvalues in (0, 1). This entails that the mapping

proxαf (·,θ)

(cid:55)→

. Furthermore, the matrix (I + αA)−1 in (23) is

JRαf (·,θ) (x, θ) ⇒

(cid:110)

[2(I + αA)−1

I,

−

−

2α(I + αA)−1B

I], [A, B]

−

∈

J 2
f (proxαf (·,θ), θ)

(cid:111)

(24)

is conservative for Rαf (·,θ) and the matrix 2(I + αA)−1
(
−
Similarly, the mapping

1, 1).

−

I is symmetric with eigenvalues in

JRαg(·,θ) (x, θ) ⇒

(cid:110)

[2C

I, 2D

I], [C, D]

Jproxαg(x,θ)

−

1, 1]. One may combine JRαf (·,θ)

−
I in (25) is symmetric with eigenvalues
is the Clarke Jacobian of Rαg(·,θ) and the matrix 2C
, using diﬀerential calculus rule to obtain a
in [
JFα (x, θ), the square
conservative Jacobian JFα
for Fα, such that for all (x, θ) and [E, F ]
matrix E is of the form I
I) where A is from (24) and C is from
I)(2C
(25). Such a matrix E has operator norm strictly smaller than 1 which is Assumption 1. (cid:3)

2 + ((I + αA)−1

and JRαg·,θ)

−

−

−

−

∈

∈

(25)

(cid:111)

E.3 Equivalence between ADMM and dual Douglas–Rachford

We need the following lemma.

Lemma 4 Let F, G two convex, lower semicontinuous and closed functions and h deﬁned by

Then, h is convex, lower semicontinuous, closed, and

h(x) = F ∗(

A(cid:62)x) + G∗(x).

−

proxαh(x) = x + α(Aˆu

ˆv)

−

(26)

where

(ˆu, ˆv)

arg min
u,v

∈

(cid:110)

F (u) + G(v) + x(cid:62)(Au

v) +

−

α
2 (cid:107)

Au

v

2
2

(cid:107)

−

(cid:111)

.

24

The material contained in this section is already known in the litterature accross several
papers and lecture notes, but for the sake of completeness, we include a full derivation of the
equivalence.
In this appendix, we drop the dependency to the variable θ since we are only concerned
on the behaviour with respect to x. We recall that the iteration of Douglas–Rachford are
deﬁned by an initialization y0 and the recursion

xk+1 = proxf (yk)
yk+1 = yk + proxg(2xk+1

yk)

−

−

xk+1.

(27)

By denoting ˜xk = xk+1 and ˜yk = yk, we can rewrite the updates of Douglas–Rachford (given
˜x0 and ˜y0) as

˜yk+1 = ˜yk + proxg(2˜xk
˜xk+1 = proxf (˜yk+1)

˜yk)

˜xk.

−

−

Introducing the variable ˆr = proxg(2ˆx

ˆy), this is also equivalent to

−
ˆrk+1 = proxg(2ˆxk
ˆxk+1 = proxf (ˆyk + ˆrk+1
ˆyk+1 = ˆyk + ˆrk+1

ˆyk)

ˆxk

−

−

ˆxk)

−

Using the change of variable ˆwk = ˆxk

ˆyk, we have

−

ˆrk+1 = proxg(ˆxk + ˆwk)
ˆxk+1 = proxf (ˆrk+1
ˆwk+1 = ˆwk + ˆxk+1

−

ˆwk)

ˆrk+1.

−

(28)

(29)

(30)

This formulation will be convenient to show how to retrieve the equations of ADMM (13).
The dual problem of (12) is given by (14)

where f (x) = φ(cid:63)(
We consider the update rules given by (30), i.e.,

Ax) + c(cid:62)x and g(x) = ψ(

−

−

Bx)

max

x −

f (x)

−

g(x).

ˆr = proxαg(x + w)
ˆx = proxαf (ˆr
w)
ˆw = w + ˆx

−
ˆr.

−

Applying Lemma 4 to F = φ and G = ιc, we rewrite (32) by

ˆr = x + w + α(Aˆu

c)

−

where

ˆu = arg min

u

(cid:110)

φ(x) + x(cid:62)(Au

v) +

−

α
2 (cid:107)

Au

−

c + w/α

(cid:111)

.

2
2
(cid:107)

Using the same lemma to F = ψ and G = 0, we rewrite (33) by

ˆx = x + α(Aˆu + Bˆv

c)

−

(31)

(32)

(33)

(34)

where

α
2 (cid:107)
Finaly, combining the expression of ˆr and ˆx, we obtain

ψ(v) + x(cid:62)Bv +

ˆv = arg min

v

(cid:110)

Aˆu + Bv

(cid:111)
c

.

−

ˆw = αBˆv.

25

F Inertial methods

Let us ﬁrst recall notations from Section 5. Consider a function f : Rp
×
β > 0, for simplicity, when the second argument is ﬁxed we write fθ : x
for all x, y, θ, F (x, y, θ) = (x
− ∇
(xk+1, yk+1) = F (xk, yk, θ) for k
∈
then, choosing α = 1/L and β < 1
2
at a linear rate to the unique solution,

R, and
f (x, θ). Set
y), x), consider the Heavy-Ball algorithm
fθ(x) + β(x
N. If fθ is µ-strongly convex with L-Lipschitz gradient,
(cid:18)
, the algorithm will converge globally

−
(cid:113) µ2

4L2 + 2

µ
2L +

Rm

(cid:55)→

→

(cid:19)

F.1 Failure of Forward diﬀerentiation for C 1,1 objectives
The Jacobian of F for the Heavy-Ball agorithm (in x, y) is of the form

JacF (x, y, θ) =

(cid:18)(I

α

∇

−

2fθ(x)) + βI

I

(cid:19)

,

βI
−
0

(35)

when f is C 2. If f is C 1,1, then the Hessian can be replaced by a set-valued conservative
Jacobian of the gradient: J∇fθ
Proof of Proposition 4:
Recall that the function f : R2

R is given by

.

→

f : (x, θ)

(cid:40) x2
2
x2
8

(cid:55)→

if x
0
≥
if x < 0.

0 and f (cid:48)(x) = x
4
for t < 0,

We have f (cid:48)(x) = x for t
subdiﬀerential of f (cid:48) is
1
{
f is µ = 1
4
the Heavy-Ball algorithm applied to f (

for t < 0, therefore, f (cid:48) is 1-Lipschitz. The Clarke
4 , 1(cid:3) at t = 0. Finally,
for t > 0 and the segment (cid:2) 1
}
strongly convex and has L = 1 Lipschitz gradient and the unique ﬁxed point of
, θ) is x = y = θ. Choosing α = 1, β = 0.75, we have
·
(cid:33)

≥
1
4 }

(cid:32)

(cid:33)

(cid:32)

{

(cid:114)

(cid:114)

β <

1
2

µ
2L

+

µ2
4L2 + 2

=

1
2

1
8

+

1
64

+ 2

0.77.

(cid:39)

Therefore, the heavy ball algorithm with this choice of parameter converges linearly to the
unique solution which is 0, a ﬁxed point of the iteration mapping.
Set

F (x, y, θ) = (x

xf (x, θ) + β(x

y), x).

−

− ∇

At (0, 0, 0), the last column of the Jacobian of F is (0, 0) and the ﬁrst two columns are given
by

where

J = conv

M1, M2

,

}

{

M1 =

(cid:19)

(cid:18) 3

3
2 −
4
0
1

M2 =

(cid:18) 3

3
4 −
4
0
1

(cid:19)

.

Therefore, the Clarke Jacobian of F (with respect to x, y) at (0, 0, 0) is given by
(cid:19)

(cid:18) 3

(cid:18) 3

(cid:19)

3
2 −
4
0
1

,

M2 =

3
4 −
4
0
1

.

JF (0, 0, 0) = conv

M1, M2

,

}

{

M1 =

We have

1
M1M1M2M2 = −
32

(cid:18)36
27

(cid:19)

,

0
9

26

which has two eigenvalues −9
have for all k
Heavy-Ball algorithm.

∈

R x0(θ) = θ, y0(θ) = θ, we
. Setting for any θ
N xk(θ) = yk(θ) = θ, in other words, this is the unique ﬁxed point of the

1 and −9
32

8 <

−

∈

(cid:3)
N, the forward propagation recursion in (PB) presented in Figure 3 satisﬁes for

Given l
k = 8l

∈

(M1M1M2M2)2l

(cid:19)

(cid:18)1
1

∈

N, Jxk

This products will diverge diverge due to the eigenvalue of (M1M1M2M2)2 strictly above 1.
given by (PB) contains elements which magnitude diverges at
In other words, for all k, Jx8k
a geometric rate. We conclude that, for all k
contains elements which magnitude
diverge at a geometric rate.
This illustrates the failure of forward derivative propagation on f (
, θ): the Heavy Ball
·
algorithm is stable and globally linearly convergent, its ﬁxed point is diﬀerentiable (it is
actually constant in θ), yet there is a parametric initialization x(θ), y(θ) such that forward
propagation of derivatives produces diverging elements for θ = 0. Note that implicit
diﬀerentiation provides the correct derivative, which is 0, since x(θ) = 0 is the unique ﬁxed
point of the gradient iterations. Forward derivative propagation on the gradient descent
algorithms also results in the limit in 0 derivative since it only contains element which
converge to 0 at a geometric rate.
Le us emphasize again that such pathology would not happen if f was C 2. Indeed, in this
case, J 2
would be single valued and the divergence phenomenon would not appear. This
f
illustrate a fundamental diﬀerence between C 1,1 and C 2 objectives in terms of forward
derivative propagation for second order inertial methods.

G Experiments details

All the experiments where run on a MacBook M1 Pro (arm64), on Python 3.9 and numpy
1.21 for a compute time inferior to one hour. They are repeated 100 times, and we report the
median as a blue line and the ﬁrst and last deciles as a blue shaded area. The solutions are
computed with 2000 iterations, and the curves are reported for the 1000 ﬁrst iterations. The
diﬀerentiation of all methods is performed in forward-mode with jacfwd of the module jax.

Forward–Backward for the Ridge. The dimensions of the problem are n = 500, p = 300.
i.i.d
The design matrix is Gaussian, i.e., Xi,j
(0, 1).
∼ N
The regularization parameter is set to θ = 0.05.

(0, 1) and the observations yi

i.i.d
∼ N

Forward–Backward algorithm for the Lasso. The dimensions of the problem are
(0, 1) and the observations
n = 50, p = 500. The design matrix is Gaussian, i.e., Xi,j
∞.
yi
(cid:107)

(0, 1). The regularization parameter is set to θ = 0.2

θmax where θmax =

i.i.d
∼ N

i.i.d
∼ N

X (cid:62)y

×

(cid:107)

Douglas–Rachford for the Sparse Inverse Covariance Selection. We consider co-
variance matrices of size n
n where n = 50 and θ = 0.1. The matrix C is generated as
C = V (cid:62)V where Vi,j

×
(0, 1).

i.i.d
∼ N

ADMM for Trend Filtering. We consider the cyclic 1D Total Variation n = p = 75
and λ = 3.0. Here θ

(0, 1).

i.i.d
∼ N

27

H Assets used

Our numerical experiments rely on:

• numpy [29], released under BSD-3 license.

• matplotlib [30], released under PSF license.

• jax [13], released under Apache-2.0 license.

28

