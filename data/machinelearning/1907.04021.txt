SVGD: A VIRTUAL GRADIENTS DESCENT METHOD FOR
STOCHASTIC OPTIMIZATION

A PREPRINT

Zheng Lia and Shi Shu∗

,a,b

aSchool of Mathematics and Computational Science, Xiangtan University, Xiangtan, Hunan, 411105, China
bHunan Key Laboratory for Computation and Simulation in Science and Engineering, Xiangtan University, Xiangtan
411105, China

August 1, 2019

ABSTRACT

Inspired by dynamic programming, we propose Stochastic Virtual Gradient Descent (SVGD) algo-
rithm where the Virtual Gradient is deﬁned by computational graph and automatic differentiation.
The method is computationally efﬁcient and has little memory requirements. We also analyze the
theoretical convergence properties and implementation of the algorithm. Experimental results on mul-
tiple datasets and network models show that SVGD has advantages over other stochastic optimization
methods.

Keywords computational graph

automatic differentiation

stochastic optimization

machine learning

·

·

·

1

Introduction

Stochastic gradient-based optimization is most widely used in many ﬁelds of science and engineering. In recent years,
many scholars have compared SGD[1] with some adaptive learning rate optimization methods[2, 3]. [4] shows that
adaptive methods often display faster initial progress on the training set, but their performance quickly plateaus on the
development/test set. Therefore, many excellent models [5, 6, 7] still use SGD for training. However, SGD is greedy
for the objective function with many multi-scale local convex regions (cf. Figure 1 of [8] or Fig. 1, left) because the
negative of the gradient may not point to the minimum point on coarse-scale. Thus, the learning rate of SGD is difﬁcult
to set and signiﬁcantly affects model performance[9].

Unlike greedy methods, dynamic programming (DP) [10] converges faster by solving simple sub-problems that
decomposed from the original problem. Inspired by this, we propose the virtual gradient to construct a stochastic
optimization method that combines the advantages of SGD and adaptive learning rate methods.

Consider a general objective function with the following composite form:

9
1
0
2

l
u
J

1
3

]

G
L
.
s
c
[

2
v
1
2
0
4
0
.
7
0
9
1
:
v
i
X
r
a

where θ

Ωθ = Rn, Ωσ = f (Ωθ)

∈

We note that:

J = F (σ), σ = f (θ)

(1)
Rm, functions F and each component function of f is ﬁrst-order differentiable.

Ωσ,

∈

⊆

F (σ∗) = F (f (θ∗)), σ∗ = arg min

F (σ),

F (f (θ)).

(2)

Ωσ
In addition, when we minimize F (σ) and F (f (θ)) with the same iterative method, the former should converge faster
that
because the structure of F is simpler than F

f . Based on these facts, we construct sequences

θ(t)

and

Ωθ

σ

∈

∈

θ∗ = arg min
θ

σ(t)
{

}

{

}

◦

∗Corresponding author

This work is supported by the National Natural Science Foundation of China (Grant No. 11571293) and Key Research and
Development Program of Hunan Province, P. R. China (Grant No. 2017SK2014 ).
Email addresses: lizheng.math.ai@gmail.com (Zheng Li), shushi@xtu.edu.cn (Shi Shu)

 
 
 
 
 
 
SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

converge to σ∗ and θ∗, respectively, with equations:

Fig. 1 (right) shows the relationship between
ﬁrst-order iterative methods (see Sec.5 for details):

σ(t)
{

}

and

θ(t)

{

}

. The sequence

σ(t)
{

}

can be obtained by using

σ(t) = f (θ(t)), t = 0, 1,

.

· · ·

(3)

σ(t+1) = σ(t)

αT ∗

−

∇σJ

σ=σ(t),
Rm.
(cid:12)
(cid:12)

where α is the learning rate, T ∗ is an operator of mappping Rm

→

Figure 1

The difﬁculty in constructing operator T ∗ is how to make the condition (3) holds true. Let M =
T is an operator of mapping Rm

Rm, we give the following iterations:

→

σ(t+1) = σ(t)

−
θ(t+1) = θ(t)

αMM T T

αM T T

−

∇σJ
∇σJ

σ=σ(t) ,

(cid:12)
(cid:12)
θ=θ(t) .

(4)

∂f i(θ)
∂θj

(cid:16)

(cid:12)
(cid:12)
(cid:12)

,

θ=θ(t)

m

n

×

(cid:17)

(5)

(6)

Since M T T
as the virtual gradient of function J for variable θ.

∇σJ in Eqn.(6) is equivalent to the position of

∇θJ in gradient descent method, we deﬁne M T T

(cid:12)
(cid:12)

∇σJ

For Eqn.(6), it is easy to prove that the condition (3) holds when f is a linear mapping. If f is a nonlinear mapping, let
the second-derivatives of f be bounded and α = o(1), σ(t) = f (θ(t)), owing to (5) and (6) and Taylor formula, the
following holds true:

σ(t+1)
||

f (θ(t+1))
||

−

=

(σ(t)
||
= O(

αMM T T
∇σJ

−
αM T T
||

∇σJ
σ=σ(t))
2) = O(α2),
2
(cid:12)
θ=θ(t) ||
(cid:12)

−

f (θ(t)

αM T T

−

∇σJ

θ=θ(t) )
||

(cid:12)
(cid:12)

(7)

In this case, the condition (3) holds, approximately.

F (σ(t))
According to the analysis above, the sequence
{
Eqn.(5), but faster than minimizing the function F (f (θ)) with the same ﬁrst-order method, directly.

yields similar convergence as

}

in Eqn.(6) and

}

Note that the iterative method (6) is derived based on the composite form (1) and this form is generally not unique, it is
inconvenient for our algorithm design. We begin by introducing the computational graph. It is a directed graph, where
each node indicates a variable that may be a scalar, vector, matrix, tensor, or even a variable of another type, and each
edge unique corresponds to an operation which maps a node to another. We sometimes annotate the output node with
the name of the operation applied. In particular, the computational graph corresponding to the objective function is a
DAG(directed acyclic graphs) [11]. For example, the computational graph of the objective function J shown in Fig. 2
(a), the corresponding composite form (1) is:

J = F (σ), σ =

˜f 1 : Rnθ

1

Rnθ

2

×






˜σ1
˜σ2
(cid:20)
→

(cid:21)
Rnσ
1 ,

= f (θ) =

˜f 1(˜θ1, ˜θ2)
˜f 2(˜θ2, ˜θ3; ˜σ1)
(cid:20)
(cid:21)
Rnσ
Rnθ
˜f 2 : Rnθ

3

2

1

×

×

,

θ = [˜θ

T

1 , ˜θ

T

2 , ˜θ

T
3 ]T

.

(8)

Rnσ

2

→

2

(cid:12)
(cid:12)
F (f (θ(t)))
{

−1.5−1−0.500.511.5−0.500.511.5F(σ)=(σ+1)σ(σ−1)2F(f(θ))=F(θ+0.05sin(50θ))···σ(t)σ(t+1)···σ∗···θ(t)θ(t+1)···θ∗SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

Figure 2:
edges associated with operations ˜f 1, ˜f 2, F .

,

,

are nodes associated with leaf values, hidden values and output value, and

,

,

are

θ =

For a given general objective function, let G correspond to a computational graph that maps the set of leaf values
V G
to the output value J, where the set of hidden values is V G
. Let
}
{
V G
. In this paper, the objective function J in Eqn.(1)
θ
→
will be expressed as the following composite form:

˜θj|
{
σ :=
{

j = 1,
˜σ(cid:48)i|

dist( ˜σi, V G

θ ) = 1
}

˜σi ∈
{

· · ·
i = 1,

V G
σ |

}
, M (cid:48)

i = 1,

˜σi|

σ =

, M

, N

· · ·

· · ·

=

}

J = F (σ), σ = 

˜σ(cid:48)1
...
˜σ(cid:48)M (cid:48)







, ˜θi,Ni; ˜σ(cid:48)i,1,

˜θi,1,
{
˜σ(cid:48)i,1,
{

· · ·
· · ·

(cid:26)

˜f 1(˜θ1,1,

= ˜f (θ) = 

(˜θM (cid:48),1,

· · ·

˜f M (cid:48)




· · ·

, ˜θ1,N1; ˜σ(cid:48)1,1,
...
, ˜θM (cid:48),NM (cid:48)

; ˜σ(cid:48)M (cid:48),1,

, ˜σ(cid:48)1,N (cid:48)1

)

· · ·

, ˜σ(cid:48)M (cid:48),N (cid:48)M (cid:48)

· · ·


)




)

, ˜σ(cid:48)i,N (cid:48)i

· · ·
∈
, ˜θi,Ni }
˜θk ∈
=
{
˜σk ∈
, ˜σ(cid:48)i,N (cid:48)i }
=
{

V G
θ
→
V G
θ |
V G
σ |

σ, i = 1,

, M (cid:48), and

· · ·
dist(˜θk, ˜σ(cid:48)i) = 1
dist( ˜σk, ˜σ(cid:48)i) = 1, ˜σk (cid:22)

}

.

˜σ(cid:48)i}

where ˜σ(cid:48)i = ˜f i(˜θi,1,

· · ·

(9)

For example, Eqn.(8) can be expressed as:

where

J = F (σ), σ =

= ˜f (θ) =

˜σ(cid:48)1
˜σ(cid:48)2(cid:21)
(cid:20)

˜f 1(˜θ1,1, ˜θ1,2)
˜f 2(˜θ2,1, ˜θ2,2; ˜σ2,1)
(cid:21)
(cid:20)

,

˜σ(cid:48)1 = ˜σ1, ˜θ1,1 = ˜θ1, ˜θ1,2 = ˜θ2
˜σ(cid:48)2 = ˜σ2, ˜σ2,1 = ˜σ1, ˜θ2,1 = ˜θ2, ˜θ2,2 = ˜θ3

.

(cid:26)

In deeping learning, the gradient of the objective function is usually calculated by the Automatic Differentiation (AD)
technique[12, 9]. Our following example introduces how to calculate the gradient of J in Eqn.(8) using AD technique.

1. Find the Operation F associated with output value J and its input node

calculate the following gradients:

˜σ(cid:48)1, ˜σ(cid:48)2}
{

, cf. Fig. 2 (b). Then,

2. Perform the following steps by the partial order
(a) Find Operation ˜f 2 and it’s input nodes

2 (c). Let:

gJ
w := gw

→

J ( ˜σ(cid:48)1, ˜σ(cid:48)2) =

∇wF, w = ˜σ(cid:48)1, ˜σ(cid:48)2.
of
˜σ(cid:48)1, ˜σ(cid:48)2}
{
˜θ2,1, ˜θ2,2, ˜σ(cid:48)2,1}

(cid:23)

:

{
F2(˜θ2,1, ˜θ2,2; ˜σ(cid:48)2,1) := F (
·

, ˜f 2(˜θ2,1, ˜θ2,2; ˜σ(cid:48)2,1)),

which associated with hidden value ˜σ(cid:48)2, cf. Fig.

where ’
later. Calculate the following gradients:

’ denotes that it is treated as a constant during the calculation of gradients and will not be declared
·

g ˜σ(cid:48)2
w := gw

˜σ(cid:48)2

→

(
∇ ˜σ(cid:48)2

J; ˜θ2,1, ˜θ2,2; ˜σ(cid:48)2,1) =

∇wF2 =

T

∂ ˜σ(cid:48)2
∂w

(cid:18)

(cid:19)

∇ ˜σ(cid:48)2

J, w = ˜θ2,1, ˜θ2,2, ˜σ(cid:48)2,1,

where

∇ ˜σ(cid:48)2

J =

F .

∇ ˜σ(cid:48)2

3

˜θ1˜θ2˜σ1˜θ3˜σ2J(a)˜σ01˜σ02J(b)˜θ2,1˜σ2,1˜θ2,2˜σ02(c)˜θ1,1˜θ1,2˜σ01(d)SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

(b) Find Operation ˜f 1 and it’s input nodes

(d). Let:

˜θ1,1, ˜θ1,2}
{
F1(˜θ1,1, ˜θ1,2) := F (˜f 1(˜θ1,1, ˜θ1,2), ˜f 2(

Calculate following gradients:

, ˜f 1(˜θ1,1, ˜θ1,2))).
·

,
·

which associated with hidden value ˜σ(cid:48)1, cf. Fig. 2

g ˜σ(cid:48)1
w := gw

(

˜σ(cid:48)1

∇ ˜σ(cid:48)1

→

J; ˜θ1,1, ˜θ1,2) =

∇wF1 =

T

∂ ˜σ(cid:48)1
∂w

(cid:18)

(cid:19)

∇ ˜σ(cid:48)1

J, w = ˜θ1,1, ˜θ1,2,

where

J =

∇ ˜σ(cid:48)1
3. Calculate the gradients of J:

∇ ˜σ(cid:48)1

F +

F2.

∇ ˜σ(cid:48)1

J = g ˜σ(cid:48)1
˜θ1,1

,

˜θ1

∇

˜θ2

∇

J = g ˜σ(cid:48)1
˜θ1,2

+ g ˜σ(cid:48)2
˜θ2,1

,

˜θ3

∇

J = g ˜σ(cid:48)2
˜θ2,2

.

can be shown as Fig. 3 (a). If
According to the analysis above, the computational graph of
T is a broadcast-like operator, the computational graph of vitrual gradients can be shown as Fig. 3 (b), where
is deﬁned by the
z1 = g ˜θ2→
Eqn.(10).

{∇
∇ ˜σ2J; ˜θ2, ˜θ3; ˜σ1) and

∇ ˜σ1 J; ˜θ1, ˜θ2), z2 = g ˜θ2→

k = 1, 2, 3
}

k = 1, 2, 3
}

(G,T )
˜θk

{∇

(T

(T

˜θk

˜σ1

˜σ2

J

J

|

|

Figure 3:

are edges associated with operation + and x

y denotes T x

y.

According to the deﬁnition of virtual gradient, for any ˜θk ∈

V G
θ :

(G,T )
˜θk

J =

∇

=

(cid:88)˜σ(cid:48)i∈
V θ

→

σ (cid:18)

(cid:88)˜σ(cid:48)i∈
V θ

→

j
σ (cid:88)

T

J

Ti∇ ˜σ(cid:48)i

∂ ˜σ(cid:48)i
∂ ˜θk (cid:19)
δ(˜θi,j, ˜θk)g ˜θi,j→

(Ti,j∇ ˜σ(cid:48)i

J; ˜θi,1,

· · ·

, ˜θi,N1; ˜σ(cid:48)i,1,

, ˜σ(cid:48)i,N (cid:48)i

).

· · ·

(10)

˜σ(cid:48)i

(G,I)
˜θk

˜θk

∇

∇

J =

J where I is an identity operator. The bprop operation gw

Obviously,
˜f i.
Then, the Eqn.(6) can be written as the following virtual gradient descent iteration:
˜θk ∈
∀

(G,T )
˜θk
We prove that the SVGD (Alg. 1) has advantages over SGD, RMSProp and Adam in training speed and test accuracy
by experiments on multiple network models and datasets.

is uniquely determined by

˜σ(cid:48)i

→

(t)
k −

θ=θ(t),

= ˜θ

(t+1)
k

V G
θ

(11)

∇

˜θ

α

J

(cid:12)
(cid:12)

In Sec.2 we describe the operator T and the SVGD algorithm of stochastic optimization. Sec.3 introduce two methods
to encapsulate SVGD, and Sec.4 provides a theoretical analysis of convergence. Sec.6 compares our method with other
methods by experiments.

4

˜σ1˜σ2gJ˜σ1∇˜σ2J˜θ2˜θ3g˜σ2˜σ1g˜σ2˜θ2∇˜θ3J∇˜σ1J˜θ1g˜σ1˜θ2∇˜θ1J∇˜θ2J(a)˜σ1˜σ2gJ˜σ1∇˜σ2J˜θ2˜θ3g˜σ2˜σ1z2∇(G,T)˜θ3J∇˜σ1J˜θ1z1∇(G,T)˜θ1J∇(G,T)˜θ2J(b)SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

2 Stochastic Virtual Gradients Descent Method

In this section, we will use the accumulate squared gradient in the RMSProp to construct the operator T . According
to Eqn.(7), Eqn.(3) holds when the mapping f is linear. Based on this fact, we designed the following SVGD algorithm.
The functions and variables in the algorithm are given by Eqn.(9) and Eqn.(10).

J)2 indicates the elementwise square
6 and ρ = 0.9. All operations

∇

Algorithm 1: SVGD, our proposed algorithm for stochastic optimization. (

J

(cid:12) ∇

J. Good default settings for the tested machine learning problems are (cid:15) = 10−

∇
are element-wise.
Require: G: computational graph associated with function J (τ ) = L(ˆy(x(τ ), θ), y(τ ))
Require: α: Learning rate
Require: m: Minibatch size
Require: s
Require: θ: Initial parameter
/* define operator ∇(G,T ) before training */
for ˜σ(cid:48)i ∈
ri = 0 // Initialize gradient accumulation variable
for j

): Scaling coefﬁcient

σ do

[0, +

do

∞

∈

;

;

· · ·

· · ·

) = g ˜θ(cid:48)

˜σ(cid:48)i

i,j→

) = g ˜θ(cid:48)

˜σ(cid:48)i

i,j→

(s ∇ ˜σ(cid:48)i

J (τ )
√ri+(cid:15) ;

;

· · ·

· · ·

) // define Ti,j

J (τ );

(
∇ ˜σ(cid:48)i

;

· · ·

· · ·

) // define Ti,j

V G
θ
→
1,
∈ {
if ˜f i about ˜θ(cid:48)
g ˜θ(cid:48)

· · ·

˜σ(cid:48)i

i,j→

else

, Ni}
i,j is linear then
(Ti,j∇ ˜σ(cid:48)i

J (τ );

· · ·

g ˜θ(cid:48)

˜σ(cid:48)i

i,j→

end

end

(Ti,j∇ ˜σ(cid:48)i

J (τ );

· · ·

end
/* update θ */
while V G

θ(t) not converged do

Sample a minibatch of m examples from the training set
for ˜σ(cid:48)i ∈
ri ←

V G
θ
→
ρri + (1

σ parallel do
m

ρ) 1
m

∇ ˜σ(cid:48)i

J (τ )

−

2

i=τ
(cid:80)

(cid:0)

(cid:1)

// Accumulate squared gradient

x(1),
{

· · ·

, x(m)

}

with corresponding targets y(i).

end
for ˜θj ∈
˜θj ←

V G
θ parallel do
˜θj −
α 1
m

m

i=τ ∇
(cid:80)

end

end

(G,T )
˜θj

J (τ ) // apply update

SVGD works well in neural network training tasks (Fig.9, 11, 12), it has a relatively faster convergence rate and better
test accuracy than SGD, RMSProp, and Adam.

For the linear operation Conv2D [13] and matrix multiplication MatMul as follows:

Conv2D : (RN
MatMul : (RN

×
×

(cid:26)

RWin

RHin
×
RCin , RCin

×

RCin , RHk
RN

×
RCout)

RWk
×
RCout

×
×

→

RCin

RCout)

RN

×

→

×

RHout

×

RWout

×

RCout

,

there are Dim(rConv2D) = HoutWoutCout < HkWkCinCout and Dim(rMatMul) = Cout < CinCout. Thus, SVGD
also has less memory requirements than RMSProp and Adam for deep neural networks.

For the same stochastic objective function, the learning rate at timestep t in SVGD has the following relationship with
the stepsize in the SGD and RMSProp:

αSV GD(t)

αSGD(t),

≈

s

∗

5

αSV GD(t)

≈

αRM SP rop(t).

SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

3 Encapsulation

In this section, we introduce two methods to generate the computational graph of virtual gradient. We begin by
assumming that the objective function is J (cf. Fig. 4 (b)), the set V G
(cf. Fig. 4 (a)), and the function
θ
→
used to construct the computational graph of gradients is "gradients", cf. Fig. 4 (c).

˜σ(cid:48)1, ˜σ(cid:48)2}
{

σ =

Figure 4

We hope to generate the computational graph of virtual gradients by using the function "gradients", Fig. 4 (c).

3.1 Extend the API libraries

As shown in Fig. 4, We begin by replacing ˜f i with ˆf i, where ˆf i is a copy of ˜f i but corresponds to a new bprob
operation. Then, call the function "gradients" to generate the computational graph of virtual gradients.

In order to achieve the idea above, in programming, we need to extending core libraries to customize new operations of
ˆf i and its bprop operation. Fig. 5 shows that we need to extend 3 libraries in the layered architecture of TensorFlow
[14].

Figure 5: Layered architecture of Tensorﬂow

3.2 Modify the topology of the calculation graph

According to Eqn.(10) and Fig. 3, the computational graph of the virtual gradients can be obtained by adding new nodes
on the computational graph of the gradients and reroute the inputs and outputs of new nodes. cf. Fig. 6.

6

˜fi∼gw→˜σ0i(∇˜σ0iJ;···;···)||⇓ˆfi∼gw→˜σ0i(T∇˜σ0iJ;···;···)(a)J=F(˜f2◦˜f1,˜f1)⇓ˆJ=F(ˆf2◦ˆf1,ˆf1)(b)∇wJ=gradients(J,w)⇓∇(G,T)wJ=gradients(ˆJ,w)(c)MathematicalMappingConv2DMatMulfgx→f(x,θ)gθ→f(x,θ)fgx→f(x,θ)gθ→f(x,θ)···KernelFunction(CUDAAPI)Conv2DMatMulfgx→f(x,θ)gθ→f(x,θ)fgx→f(x,θ)gθ→f(x,θ)···RegisterOperation(C++API)functionautomaticdiﬀerentiationConv2DMatMul···Conv2DMatMul···FrontEnd(PythonAPI)conv2dmatmul···TTfunctionautomaticdiﬀerentiationConv2DMatMul···Conv2DMatMul···conv2dmatmul···OriginlibsExtensionlibs···SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

Figure 6: Subgraph views of gradients and virtual gradients. Left: the part of the computational graph of the gradients.
Right: the part of the computational graph of the virtual gradients.

4 Convergence Analysis

In this section, we will analyze the theoretical convergence of Eqn.(6) under some assumptions.
Lemma 4.1. Let M be a random (m
n)-matrix, M 11,
Then

, M ij,

· · ·

, M mn be an i.i.d. variable from U (

fλ(v, u) := E

×
vT M T M u

· · ·
||2 = λ

M

||

(cid:104)

(cid:12)
(cid:12)

vT u,

λ > 0,

∀

v, u

∀

∈

Rn.

∝

(cid:105)

Proof. Let ei be the unit vector whose i-th component is 1, fλ is bilinear, Then

, +

−∞

).

∞

(12)

n

n

fλ(v, u) =

viujfλ(ei, ej) = vT Cu, Cij := E

i M T M ej
eT

j=1
(cid:88)

i=1
(cid:88)
, M mn be an i.i.d. variable from U (

E[eT
E[eT

· · ·
1 M T M e1| ||
i M T M ej| ||

M
M

||2 = λ] =
· · ·
||2 = λ] = 0,

∞

, +
−∞
n M T M en| ||
, n
1,
}
· · ·
∈ {

= E[eT
i, j

Since M 11,

· · ·

, M ij,

(cid:26)

Thus:

M

||2 = λ

||

(13)

.

(cid:105)

(cid:104)
), the following holds true:

(cid:12)
(cid:12)

M

||2 = λ] := c0 > 0

.

fλ(v, u) = c0vT u.

(14)

Fig. 7 proof our lemma.

Figure 7: The relationship between vT u and the estimate of fλ(v, u). Each point corresponds to a pair of random
vector (v, u) and a random matrix set
, 10000

t = 1,

M t|
{

· · ·

.
}

7

˜θ1˜σ01∇˜σ02J˜σ03˜θ2g˜σ02˜σ01∇˜θ2Jg(˜σ02)˜σ01g(˜σ02)˜θ2˜θ1˜σ01∇˜σ02J˜σ03˜θ2T∇˜σ02Jg˜σ02˜σ01∇(G,T)˜θ2Jg(˜σ02)˜σ01g(˜σ02)˜θ2T−1−0.500.51−101vTu˜fλ(v,u)˜fλ(v,u)=110000P10000t=1uTMTtMtv,||Mt||2=λn=2;λ=1n=5;λ=2n=10;λ=3n=25;λ=4SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

Corollary 4.1.1. For M deﬁned in Lemma 4.1, if vT u > 0, then:

E

vT M T M u
(cid:104)

(cid:105)

> 0.

(15)

Theorem 4.2. Let F and f be second-order differentiable functions with random variables in their expression, we set:

vT T v > 0,

1,

i

∈

· · ·

, m,

v

∀

∈

Rm

0
.
}

\{

θ(t)

If each component of Jacobian matrix M =
|θ=θ(t) and
E

∇f (θ)F (f (θ))

M T

−

α

∗

∂f i(θ)
∂θj

m
n
×
|θ=θ(t)

(cid:17)
(cid:16)
∇θF (f (θ)
F (f (θ(t+1)))

−

Proof. Without loss of generality, we can assume θ
F (f (θ))) around the point θ(t) is:

∈

(cid:104)

is an i.i.d. variable from U (

∞
= 0 there exists a α > 0 such that

−∞

, +

), then, for θ(t+1) =

F (f (θ(t)))

< 0,

Rn, σ = f (θ)

(cid:105)

∈

Rm, m > n. Then, the Maclaurin series for

Let v =

∇σJ

F (f (θ(t+1)))

−

F (f (θ(t))) =

−
|θ=θ(t). According to corollary 4.1.1:
F (f (θ(t+1)))
(cid:104)

−

E

F (f (θ(t)))

−

α(

α(

∇θJ
∇σJ

|θ=θ(t))T M T
∇σJ
|θ=θ(t))T M T M T

|θ=θ(t) + o(α)
∇σJ

|θ=θ(t) + o(α).

=

=

αE

−

vT M T M T v
(cid:104)

(cid:105)

(cid:105)

+ o(α) < 0.

Although our convergence analysis in Thm.4.2 only applies to the assumption of uniform distribution, we empirically
found that SVGD often outperforms other methods in general cases.

5 Related Work

First-order methods. For general ﬁrst-order methods, The moving direction p(t) of the variables can be regarded as
the function of the stochastic gradient g(t):

•

•

•

•

SGD: p(t) = T g(t) :=
Momentum:[15] Let m(0) = 0, m(t+1) = c m(t) + g(t). Then:

g(t).

−

RMSProp: Let r(0) = 0, r(t+1) = ρ r(t) + (1

ρ) g(t)

−

(cid:12)

g(t). Then:

p(t) = T g(t) :=

m(t+1).

−

p(t) = T g(t) :=

g(t)
√δ + r(t+1)

.

−

Adam: Let s(0) = r(0) = 0, s(t+1) = ρ1 s(t) + (1
Then:

ρ1) g(t), r(t+1) = ρ2 r(t) + (1

ρ2) g(t)

g(t).

(cid:12)

−

p(t) = T g(t) :=

−

−
s(t+1)/(1
−
δ + r(t+1)/(1

ρt
1)

−

.

ρt
2)

However, in SVGD method, p(t) =
a ﬁrst-order method.

−∇

(G,T )
θ

J cannot be written as a function of g(t). Thus, SVGD is not essentially

(cid:112)

Global minimum. A central challenge of non-convex optimization is avoiding sub-optimal local minima. Although
it has been shown that the variable can sometimes converges to a neighborhood of the global minimum by adding
noise[16, 17, 18, 19, 20], the convergence rate is still a problem. Note that the DP method has some probability to
escape “appropriately shallow” local minima because the moving direction of the variable is generated by solving
several sub-problems instead of the original problem. We use computational graph and automatic differentiation to
generate the sub-problems in DP, such as what we did in the SVGD method.

8

(cid:54)
SVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

6 Experiments

In this section, we evaluated our method on two benchmark datasets using several different neural network architectures.
We train the neural networks using RMSProp, Adam, SGD, and SVGD to minimize the cross-entropy objective function
with L1 weight decay on the parameters to prevent over-ﬁtting. To be fair, for different methods, a given objective
function will be minimized with different learning rates. All extension libs, algorithm, and experimental logs in this
paper can be found at the URL: https://github.com/LizhengMathAi/svgd.

The following experiments show that SVGD has a relatively faster convergence rate and better test accuracy than SGD,
RMSProp, and Adam.

6.1 Multi-layer neural network

In our ﬁrst set of experiments, we train a 5-layer neural network (Fig. 8) on the MNIST [21] handwritten digit
classiﬁcation task.

Figure 8: MLP architecture for MNIST with 5 parameter layers (245482 params).

4. In Table 1, we decay α at 1.6k
The model is trained with a mini-batch size of 32 and weight decay of 1.0
and 3.6k iterations and summarize the optimal learning rates for RMSProp, Adam, SGD, and SVGD by hundreds of
experiments.

10−

×

α

1599]

iter: [0
∼
iter: [1600
iter: [1600
test top-1 error

∼
∼

3599]
5999]

RMSProp Adam
0.001
0.001
0.00005
0.0005
0.00005
0.00005
1.94%
1.80%

SGD
0.1
0.05
0.01
1.76%

SVGD(s=0.1)
0.01
0.005
0.001
1.60%

Table 1: The test error and learning rates in MLP experiments.

In Table 1 and Fig. 9 we compare the error rates and their descent process process on the CIFAR-10 test set, respectively.

Figure 9: Comparison of ﬁrst-order methods on MNIST digit classiﬁcation for 3.75 epochs.

6.2 Convolutional neural network

We train a VGG model (Fig. 10) on the CIFAR-10 [22] classiﬁcation task and follow the simple data augmentation in
[23, 24] for training and evaluate the original image for testing.

9

image784×256MatMulﬂat256×128MatMulbias/BNReLU128×64MatMulbias/BNReLU64×32MatMulbias/BNReLU32×10MatMulbias/BNReLUlogitsbiasSoftmax0.511.522.533.541.522.533.5epochtesttop-1error(%)RMSPropAdamSGDSVGD12346·10−28·10−20.10.120.14epochtestlossRMSPropAdamSGDSVGDSVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

Figure 10: VGG model architecture for CIFAR-10 with 6 parameter layers (46126 params).

5. In Table 2, we decay α at 12k
The model is trained with a mini-batch size of 128 and weight decay of 1.0
and 24k iterations and summarize the optimal learning rates for RMSProp, Adam, SGD, and SVGD by hundreds of
experiments.

10−

×

α

∼

11999]

iter: [0
iter: [12000
iter: [24000
test top-1 error

∼
∼

23999]
34999]

RMSProp Adam
0.02
0.01
0.002
17.78%

0.02
0.01
0.005
18.02%

SGD
2.0
0.5
0.005
17.32%

SVGD(s=0.001)
2.0
0.5
0.005
17.07%

Table 2: The test error and learning rates in VGG experiment.

In Table 2 and Fig. 11 we compare the error rates and their descent process on the CIFAR-10 test set, respectively.

Figure 11: Comparison of ﬁrst-order methods on CIFAR-10 dataset for 90 epochs.

6.3 Deep neural network

We use the same hyperparameters with [24] to train ResNet-20 model(0.27M params) on the CIFAR-10 classiﬁcation
task. In Table 3, we decay α at 12k and 24k iterations and summarize the optimal learning rates for RMSProp, Adam,
SGD, and SVGD by hundreds of experiments.

α

∼

31999]

iter: [0
iter: [32000
iter: [42000
test top-1 error

∼
∼

41999]
49999]

RMSProp Adam
0.001
0.001
0.0001
0.0001
0.00005
0.0001
11.12%
11.18%

SGD
0.1
0.01
0.001
10.69%

SVGD(s=0.01)
0.5
0.02
0.01
8.62%

Table 3: The test error and learning rates in ResNet experiments.

In Table 3 and Fig. 12 we compare the error rates and their descent process on the CIFAR-10 test set, respectively. The
top-1 error ﬂuctuations in experiments do not exceed 1%. See [25] for more information on the CIFAR-10 experimental
record.

10

image3×3,3conv3×3,16convbias/ReLU3×3,16convbias/ReLUBN/MaxPool3×3,32convbias/ReLU3×3,128convbias/ReLUBN/MaxPool128×10MatMulbias/ReLUBN/AvgPoollogitsbiasSoftmax02040608015202530epochtesttop-1error(%)RMSPropAdamSGDSVGD0204060800.50.60.70.80.91epochtestlossRMSPropAdamSGDSVGDSVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

Figure 12: Comparison of ﬁrst-order methods on CIFAR-10 dataset for 125 epochs.

References

[1] David Saad. Online algorithms and stochastic approximations. Online Learning, 5, 1998.

[2] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview

of mini-batch gradient descent. Cited on, 14, 2012.

[3] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

[4] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of
adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages
4148–4158, 2017.

[5] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object
detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 580–587, 2014.

[6] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with

region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.

[7] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for
deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
1492–1500, 2017.

[8] Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin dynamics.

arXiv preprint arXiv:1702.05575, 2017.

[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.

deeplearningbook.org.

[10] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms. MIT

press, 2009.

[11] Krishnaiyan Thulasiraman and Madisetti NS Swamy. Graphs: theory and algorithms. Wiley Online Library,

1992.

[12] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic

differentiation in machine learning: a survey. Journal of Marchine Learning Research, 18:1–43, 2018.

[13] Yann LeCun et al. Generalization and network design strategies. In Connectionism in perspective, volume 19.

Citeseer, 1989.

[14] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale machine learning. In 12th
16), pages 265–283, 2016.
USENIX
}
{

Symposium on Operating Systems Design and Implementation (
{

[15] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational

OSDI
}

Mathematics and Mathematical Physics, 4(5):1–17, 1964.

11

02040608010012051015202530epochtesttop-1error(%)RMSPropAdamSGDSVGD0204060801001200.30.40.50.60.7epochtestlossRMSPropAdamSGDSVGDSVGD: A Virtual Gradients Descent Method for Stochastic Optimization

A PREPRINT

[16] Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient

descent. arXiv preprint arXiv:1511.04834, 2015.

[17] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens.
Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.
[18] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. arXiv preprint

arXiv:1511.06392, 2015.

[19] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.
[20] Albert Zeyer, Patrick Doetsch, Paul Voigtlaender, Ralf Schlüter, and Hermann Ney. A comprehensive study of
deep bidirectional lstm rnns for acoustic modeling in speech recognition. In 2017 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 2462–2466. IEEE, 2017.

[21] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE

Signal Processing Magazine, 29(6):141–142, 2012.

[22] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,

Citeseer, 2009.

[23] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised nets. In

Artiﬁcial intelligence and statistics, pages 562–570, 2015.

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[25] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very deep networks. In Advances in neural

information processing systems, pages 2377–2385, 2015.

12

