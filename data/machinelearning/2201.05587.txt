Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor
Program Code Generation

Perry Gibson, José Cano
School of Computing Science, University of Glasgow, Scotland, UK

2
2
0
2

p
e
S
7

]

G
L
.
s
c
[

2
v
7
8
5
5
0
.
1
0
2
2
:
v
i
X
r
a

ABSTRACT
Auto-scheduling for tensor programs is a process where a search al-
gorithm automatically explores candidate schedules (program trans-
formations) for a given program on a target hardware platform to
improve its performance. However this can be a very time consum-
ing process depending on the complexity of the tensor program and
the capacity of the target device, with often many thousands of pro-
gram variants being explored. To address this, in this paper we in-
troduce the idea of transfer-tuning, a novel approach to identify and
reuse auto-schedules between tensor programs. We demonstrate
this concept using Deep Neural Networks (DNNs), taking sets of
auto-schedules from pre-tuned DNNs and using them to reduce the
inference time of a new DNN. We compare transfer-tuning against
the state-of-the-art Ansor auto-scheduler, defining the maximum
possible speedup for a given DNN model as what Ansor achieves
using its recommended full tuning time. On a server-class CPU
and across 11 widely used DNN models, we observe that transfer-
tuning achieves up to 88.41% (49.13% on average) of this maxi-
mum speedup, while Ansor requires 6.5× more search time on av-
erage to match it. We also evaluate transfer-tuning on a constrained
edge CPU and observe that the differences in search time are exacer-
bated, with Ansor requiring 10.8× more time on average to match
transfer-tuning’s speedup, which further demonstrates its value.
Our code is available at https://github.com/gicLAB/transfer-tuning.

CCS CONCEPTS
• Software and its engineering → Compilers; • Computing
methodologies → Artificial intelligence; Machine learning.

KEYWORDS
compute schedules, auto-tuning, DNNs, TVM, auto-scheduling,
tensor programs, tensor compilers

ACM Reference Format:
Perry Gibson, José Cano. 2022. Transfer-Tuning: Reusing Auto-
Schedules for Efficient Tensor Program Code Generation. In PACT
’22: International Conference on Parallel Architectures and Com-
pilation Techniques (PACT), October 10–12, 2022, Chicago, IL. ACM,
New York, NY, USA, 12 pages. https://doi.org/XX.XXXX/XXXXXXX.
XXXXXXX

Figure 1:
Inference time speedup and auto-scheduling
search time when running Ansor on an Intel Xeon E5-2620.

1 INTRODUCTION
Computationally expensive tensor programs such as Deep Neural
Networks (DNNs) have broad applications across fields such as
computer vision [14, 15, 21, 27], natural language processing [11,
16, 17], scientific computing [19, 41], and many more. To achieve
high performance on these DNN models, a range of efficient hand-
tuned kernel libraries such as OpenBLAS [40] and oneDNN [25]
for CPUs and cuDNN [9] for NVIDIA GPUs have been developed
to accelerate the performance of common operations. However,
these libraries require a great engineering effort to be optimized,
do not necessarily see performance portability to new hardware
architectures, and optimize for common cases often leaving novel
operations with poor performance, such as capsule networks [29].
Reliance on hand-tuned kernel libraries can reduce the speed of
adoption of new solutions [4], since communities in both academia
and industry must wait for optimized operations to be developed
by kernel library contributors. Schedule based approaches such as
Apache TVM [7] and Halide [26] can reduce some of these barriers
by decoupling the high level description of the target algorithm
from its platform specific optimizations. This can make it easier
to port algorithms to new systems, although still requires domain
expertise to write optimized schedules.

Ansor [43] is an auto-scheduling system which extends TVM
by automatically generating schedules for a given tensor program
and target hardware device, compared to the hand-tuned schedules
and kernels used by TVM, cuDNN, etc. The approach can produce
state-of-the-art inference time performance on a range of platforms
and programs, and in particular can show improvement compared
to existing approaches on novel operations such as capsule 2D
convolution [29]. Ansor splits the computation graph into a set of
kernels, where kernels are loop nests of easily fused operations
such as convolutional layers and their complementary activation
function, which are tuned individually for a given hardware device
to produce their corresponding auto-schedules.

However, this tuning process can be very time consuming and
specific to a given kernel of a given size. For example, if we tune
a convolutional layer of some size and we are presented with a

 
 
 
 
 
 
new convolutional layer of a different size, we must tune the new
layer from scratch. Figure 1 shows the tuning time of the Ansor
auto-scheduler for a number of widely used DNN models on a
common server-class CPU, an Intel Xeon E5-2620. We observe that
the maximum speedup varies between models and the search time
can take several hours, which is a large upfront cost. If a range of
applications are to be deployed on a given platform, this upfront cost
may be further exacerbated and make the potential performance
improvements of auto-scheduling impractical to achieve.

To reduce tuning time we could sacrifice potential performance
improvements by stopping early or only tuning a subset of the
kernels. However Ansor gets its improvements by evaluating a vast
array of possible schedules, and both early stopping and tuning
a subset of kernels may miss performant schedules. Our main ob-
servation is that many applications, such as DNNs, feature high
similarity between kernels in terms of the types of operations they
compute and the sizes of their tensors. For example, most DNNs
contain a limited set of operations such as convolutional, dense,
and pooling layers. The DNN models in Figure 1 contain 22 unique
kernel types (which we call kernel classes), with every model having
at least 1 kernel class in common with every other model, and often
many more. Thus, if we have already found performant schedules
for some tensor program, perhaps we could reuse this information
on other tensor programs which contain similar kernels.

In this paper we introduce transfer-tuning, a novel approach
which can improve execution performance for a given tensor pro-
gram with reduced tuning time. Transfer-tuning exploits the simi-
larity between kernels containing the same operations with vary-
ing data sizes, such that we can reuse schedules from other tensor
programs. Therefore, we can achieve performance improvements
while reducing the costs associated with auto-scheduling. Transfer-
tuning’s main value comes in use-cases where tensor program
deployment requires performance efficiency but has reduced re-
sources to perform costly auto-scheduling.

The contributions of this paper include the following:

• We introduce transfer-tuning, a new approach to reduce the
costs of auto-scheduling based on reusing auto-schedules
between tensor programs.

• We discuss in detail the key components of transfer-tuning
such as kernel classes and a model selection heuristic, and
how it is enabled by the features of the compute schedule
programming paradigm. We also demonstrate the principles
with an example using auto-schedules of two GEMM kernels
of different sizes with each other, and tune the ResNet18
model using schedules from ResNet50.

• We evaluate transfer-tuning for 11 representative DNN mod-
els obtaining a maximum speedup over untuned models of
between 1.13× and 59.4×, and compare it against Ansor
which requires over 6.5× as much search time to match our
performance on average. We also evaluate transfer-tuning
on a Raspberry Pi 4, a common constrained edge device, and
observe that the differences in search time are exacerbated,
with Ansor requiring over 10.8× as much time to match our
speedup on average.

• We present an alternative view on transfer-tuning, where
the DNN model is the same but the input data size changes.

Gibson and Cano

Figure 2: Basic overview of the compute schedule paradigm
for a tensor program. Non-shaded boxes are optional, but
can improve performance significantly.

2 BACKGROUND
Compute schedules is a programming paradigm which decouples
the high-level description of an algorithm from the description of
how it should be optimized for a given hardware platform, as seen
in systems such as Halide [26], TVM [7], and RISE/ELEVATE [13].
The schedule is expressed in a domain-specific language, where op-
timization choices such as decisions about the intermediate storage
and the order of computation are defined by the programmer as
transformations to code describing the algorithm. The separation
between high-level algorithm and platform specific transformations
can allow more clear reasoning about the performance impact of
optimizations when compared to defining the algorithm and its
optimizations in the same code, as seen in system programming
languages such as C, C++, and Rust. Additionally, a single algorithm
can have multiple schedules for different cases, such as hardware
architectures or kernel properties, as seen in TVM which defines
different schedules for the same algorithm for CPUs, GPUs, and
specialized hardware accelerators [24, 33].

Figure 2 shows a tensor program being decomposed into kernels.
For example a kernel could be a layer of a neural network. Each
kernel corresponds to an algorithm (a high level description of the
desired computation), and can have a schedule (the transformations
to be applied to the algorithm) applied to it at compile time to
produce efficient code on a target hardware platform. These opti-
mized kernels are composed together into a full program, where
optimizations can include tiling, loop reordering, and where to
apply vectorization. The purpose of splitting a tensor program into
kernels is so that they can be independently optimized. This is
valuable, since it makes the optimization problem more tractable,
and kernels are assumed to be independent of each other.

TVM applies the compute schedule paradigm to the domain
of DNN inference, exploiting the fact that the shapes of arrays
are known beforehand, and thus integrating this knowledge when
compiling models ahead-of-time. TVM then applies pre-defined
high-level optimizations to the whole computation graph, such
as operator fusion (e.g., batch normalization layers can be com-
pletely removed from the graph for inference by combining their
parameters with prior layers), and leverages its library of algo-
rithms and schedules to compile for backends including LLVM [20],
OpenCL [34], CUDA [22], and more.

However, writing optimized schedules by hand requires domain
expertise, including understanding of the algorithm’s design [12]
and the behavior of the target hardware [42]. In addition, an efficient
schedule may not be efficient for all variants of the target algorithm,
for example if the schedule was optimized assuming that the size
of the inner loop would always be small. In this case, optimizations

Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation

which were used to exploit this assumed kernel characteristic might
not help, and may even hinder performance.

Ansor [43] expands on the schedule model by introducing auto-
schedules, where efficient schedules for given kernels are generated
automatically for a given hardware platform. This reduces the need
for specialized workload and platform expertise to produce effi-
cient code, especially when new operations are introduced. In this
case, only the high-level algorithm needs to be written, and an
efficient schedule for the target platform can be found via evolu-
tionary search. For this search, a space of potential schedules is
explored (without needing to be defined by the designer, such as
in AutoTVM [8]) applying rules and random perturbations to find
an efficient schedule for a particular instantiation of a given op-
eration. Auto-schedules in Ansor are tuned for individual kernels
of a tensor program, with kernels being given a unique workload
ID based on the hash of its key parameters (e.g., operation type,
input data sizes). If another tensor program contains an identical
kernel (such a convolutional layer featuring the input and output
dimensions), then the schedule can be reused, since the IDs of the
kernels will match even if their weight data differs. However, if
these parameters change then a new tuning must be performed,
since the kernel defines a different computation, thus having a
different ID and optimal schedule.

3 MOTIVATION
Auto-schedules, such as those provided by Ansor [43], can yield
state-of-the-art performance on a number of tensor programs (such
as DNNs) and hardware devices by automatically generating opti-
mized schedules. Figure 3a shows an example of a simple computa-
tion Directed Acyclic Graph (DAG) for a tensor program, such as a
DNN, with default untuned kernels. Each node of the graph repre-
sents a unit of computation, also known as a kernel. In the figure
colors represent the kernel classes, meaning the types of operations
contained within (e.g., DNN layer types such as convolutional lay-
ers), with there being three classes of kernels in this example and
four kernels in total. The shapes of the nodes represent the size
of the data computed in the kernel, which in this example varies
between kernels. In this illustrative example if we auto-schedule
this model, as shown in Figure 3b, we see that we can speed up the
inference time by 2× with a tuning time of 36, where all values are
for illustrative purposes.

However, it should be noted that auto-scheduling can be a very
time consuming process, as shown in Figure 1, where the tuning
time can be on the order of several hours for a whole DNN model
depending on the complexity of the tensor program, the number of
schedule variants chosen to evaluate, and the resources available
of the target hardware device. This high cost can be a bottleneck to
deployment, since if we want to get the best inference time for a
given DNN model we must spend a long period of time exploring
schedule space. Approaches to reduce search time include: reducing
the time we allow the tuner as shown in Figure 3c; tuning a subset
of the model’s kernels as shown in Figure 3d; or some combina-
tion of the two. These trade-offs allow users to sacrifice potential
improvements in performance for reduced tuning time.

Our main observation in Figure 3 is that for kernels of the same
class (where class is represented as color), since they define the

(a) Kernels of a tensor program, 4 kernels of 3 classes.

(b) Inference time and tuning costs when kernels are fully tuned.

(c) Inference time and tuning costs when tuning with reduced time.

(d) Inference time and tuning costs when tuning fewer kernels.

(e) Inference time and tuning costs when using transfer-tuning to
reuse the auto-schedule of Kernel 1 with Kernel 2.

Figure 3: Illustrative example of the costs and benefits of dif-
ferent approaches of auto-scheduling for a tensor program.

same high level algorithm over varying data sizes, they may produce
auto-schedules with similar properties. A question posed by our
observation is “Could an auto-schedule from a given kernel be reused
on a different kernel of the same class?” If so, a further question is
“How different will the optimizations found via auto-scheduling be
between two kernels of the same class but varying data sizes?” The
answer to the second question will vary depending on the structure
of the computations defining the class, with factors such as access
patterns and costs of the loop body playing important roles as well
as the architecture of the target platform that the auto-schedule ex-
ploits, since the organization of the memory hierarchy and features
such as vector-instruction size may make some optimizations more
or less relevant. Perhaps having some dimensions being similar
(such as the extent of the innermost loop) could be more important
than others (such as the extent of the outermost loop).

Therefore, answering the first question, we define the process of
reusing an auto-schedule for a given kernel on a different kernel
as transfer-tuning. In Figure 3e we show an example of transfer-
tuning, where we reduce tuning time by reusing auto-schedules.
We reuse the auto-schedule for Kernel 1 with Kernel 2 to reduce the
inference time without requiring any additional tuning. We also use
the auto-schedule of Kernel 1 with itself, which we refer to as the
“native schedule”. Note that we should expect some penalty when
running Schedule 1 with Kernel 2 compared to running a natively
auto-schedule for Kernel 2, since a native schedule will exploit the
specific data sizes of the computation to find optimizations for the
target hardware and data size specific optimizations. This kernel

specific information would not be exploited by using Schedule 1
for Kernel 2, as the schedule is tuned for Kernel 1. The target of
transfer-tuning is to improve the inference time of the overall tensor
program, while being cheaper than running an auto-scheduler. The
trade-off between search time and performance improvement is
interesting to explore and exploit, as long search times may not
always be acceptable. For example, a developer of AI applications for
smartphones may not have the resources to provide auto-scheduling
for their DNN model for the wide range of heterogeneous devices
their app will be deployed on. Nor will it be likely that smartphone
users be willing to wait several hours for the DNN model to auto-
schedule on their device. However, in this case transfer-tuning
could provide some performance speedups in a shorter period of
time. This reduced search time may also translate to reduced energy
usage, as auto-scheduling is an energy intensive process that can
saturate all CPU cores. However in this work we focus purely on
the inference time performance improvements of transfer-tuning.

4 TRANSFER-TUNING
First, in Section 4.1 we discuss some of the types of optimizations
used by tensor program auto-schedulers, how transfer-tuning is
possible, and how it can be beneficial. Then in Section 4.2, we
further discuss the idea of kernel classes introduced in Section 3
and how they are a key part of transfer-tuning. In Section 4.3 we
explore some of the behaviors of transfer-tuning on a full DNN
model (ResNet18), and in Section 4.4 we discuss some other practical
considerations for transfer-tuning.

4.1 Principles of Transfer-Tuning
To understand how transfer-tuning works, we must first briefly
explain the relevant concepts of schedules and auto-schedules. Let
us consider an operation such as a matrix-multiply, which has a
fixed loop structure but may have varying data sizes, as shown in
lines 1 − 5 of Algorithm 1.

There are a variety of code transformations which can be applied
to this operation, some of which are applicable to all instantiations
of the operation and others which are specific to a particular input
matrix size. For example, some transformations are valid regardless
of the data sizes involved, such as if we instruct a given loop to be
unrolled to its maximum depth. No matter how many iterations
are in a loop, it can be applied as long as we know the number
of iterations ahead of time, thus it is valid regardless of if there
are 3 iterations or 300, 000. However, the performance benefit of
the transformation will be different depending on the number of
iterations, with relevant factors including the architecture of the
underlying hardware (e.g., the features of its cache) and properties
of the computation such as the cost of the loop body.

In contrast, some transformations may only be valid for a given
data shape. For example, if we have a loop over the range (0, 𝑁 )
where 𝑁 = 32, we could apply a loop splitting optimization defined
as Split(𝑁 , 4, 8) that breaks the loop into two loops in the ranges
(0, 4) and (0, 8), which would allow us to traverse the full 32 ele-
ments. If we try to apply this optimization to a similar loop where
𝑁 = 128, then splitting it into the two prior ranges would produce
invalid code, since we will not be able to cover the full space of the
loop. However, if we reformulate our transformation such that we

Gibson and Cano

apply it as Split(𝑁 , (𝑁 /8), 8) our transformation becomes valid
for all programs where {𝑁 ∈ N : 8 | 𝑁 }.

Therefore some transformations can be applied to a kernel re-
gardless of the data-shape, others can be reformulated to be valid
for more than one data-shape, and some may not be valid for any
data-shape other than the one they were originally defined for. The
performance benefits of these transformations may be data-shape
dependent, for example a loop unrolling that brings benefit for a
small loop range could bring a penalty for a larger loop range. How-
ever, we claim that even with large data-shape differences some
of these transformations can potentially improve performance, as
compared to a generic schedule.

As discussed in Section 2, the process of auto-scheduling takes a
set of kernels representing operations in a tensor program (such
as a DNN) and iteratively explores the space of transformations
that we can apply to each of them. In this paper, when we apply
the schedule produced for a given kernel via auto-scheduling and
apply it to a kernel other than the one the schedule was tuned for,
and we call this technique transfer-tuning. Transfer-tuning exploits
the fact that many schedule transformations can be formulated to
be data-shape agnostic, meaning that we can adapt schedules for
kernels that they were not tuned for.

Let us take a look at a simple example of an auto-scheduled ker-
nel, a row-major square matrix-multiply as defined in Algorithm 1.
For more complex kernels (such as kernels containing convolu-
tional layers) auto-schedules can be verbose, difficult to interpret,
and intuitions as to why they provide good performance may be
unclear, since they are automatically generated to exploit hardware
performance dynamics that may not be evident, such as cache be-
havior. We look at two data sizes for the operation, 𝐶1 = 𝐴1𝐵2
which multiplies two (512 × 512) matrices, and 𝐶2 = 𝐴2𝐵2 which
multiplies two 1024×1024 matrices. We use Ansor to produce auto-
schedules for the two kernels, observing an improvement of 246×
and 308× for 𝐶1 = 𝐴1𝐵2 and 𝐶2 = 𝐴2𝐵2 respectively compared
to using an unoptimized schedule. The auto-scheduling of the two
kernels produces different schedules since they have different sizes.
Additionally, auto-scheduling in Ansor is non-deterministic due to
the use of genetic algorithms to mutate schedules, and a stochastic
learned cost model to reduce evaluation costs; thus differences in
the auto-schedule may emerge even when re-running Ansor for the
same kernel. Lines 6-17 and 18-35 of Algorithm 1 show a simplified
representation of auto-schedules generated for 𝐶1 = 𝐴1𝐵1 and
𝐶2 = 𝐴2𝐵2 respectively on an Intel Xeon E5-2620 CPU. Next, we
briefly explain the schedule primitives used in this example, which
is a subset of all the primitives available to Ansor and by extension,
transfer-tuning.

• Split([range], [factor]): split a loop range into inner and

outer ranges.

• Reorder([set of ranges]): specify a reordering of a set of

nested loops.

• Fuse([range], [range]): fuse two consecutive loop ranges

into a single range.

• Parallel([range]): mark an axis to be used for multi-threa-

ded computation.

• Unroll([range], [max unroll factor]): unroll a loop range up

to a maximum depth.

Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation

Algorithm 1 Auto-schedules for a GEMM operation

𝑨 : input matrix of size 𝑁 × 𝐾
𝑩 : input matrix of size 𝐾 × 𝑀
𝑪 : output matrix of size 𝑁 × 𝑀

Unmodified row-major matrix-multiply computation

1: for 𝑛 ← 0 to 𝑁 do
2:
3:
4:
5:

for 𝑚 ← 0 to 𝑀 do
𝐶 [𝑛] [𝑚] ← 0
for 𝑘 ← 0 to 𝐾 do

⊲ Initialize output value to zero

𝑪 [𝑛] [𝑚] += 𝑨[𝑛] [𝑘 ] × 𝑩 [𝑘 ] [𝑚]
Simplified auto-schedule where 𝑁 = 𝑃 = 𝐾 = 512

⊲ note 𝑁𝑜 redefined

6: 𝑁𝑜, 𝑁𝑖 ← Split(𝑁 , 8)
7: 𝑁𝑜𝑜, 𝑁𝑜 ← Split(𝑁𝑜, 1)
8: 𝑁𝑜𝑜𝑜, 𝑁𝑜𝑜 ← Split(𝑁𝑜𝑜, 16)
9: 𝑀𝑜, 𝑀𝑖 ← Split(𝑀, 8)
10: 𝑀𝑜𝑜, 𝑀𝑜 ← Split(𝑀𝑜, 1)
11: 𝑀𝑜𝑜𝑜, 𝑀𝑜𝑜 ← Split(𝑀𝑜𝑜, 16)
12: 𝐾𝑜, 𝐾𝑖 ← Split(𝐾, 1)
13: Reorder(𝑁𝑜𝑜𝑜, 𝑀𝑜𝑜𝑜, 𝑁𝑜𝑜, 𝑀𝑜𝑜, 𝐾𝑜, 𝑁𝑜, 𝑀𝑜, 𝐾𝑖, 𝑁𝑖, 𝑀𝑖 )
14: 𝐹𝑁 𝑀 ← Fuse(𝑁𝑜𝑜𝑜, 𝑀𝑜𝑜𝑜 )
15: Parallel(𝐹𝑁 𝑀 )
16: Unroll(𝐹𝑁 𝑀 , 512)
17: Vectorize(𝑀𝑖 )

Simplified auto-schedule where 𝑁 = 𝑃 = 𝐾 = 1024

18: 𝑁𝑜, 𝑁𝑖 ← Split(𝑁 , 32)
19: 𝑀𝑜, 𝑀𝑖 ← Split(𝑀, 256)
20: Reorder(𝑁𝑜, 𝑀𝑜, 𝑁𝑖, 𝑀𝑖 )
21: ^𝑁 ← 𝑁𝑖, ^𝑀 ← 𝑀𝑖
22: Create Local Cache Buffer 𝑫 of size ^𝑁 × ^𝑀
23: ^𝑁𝑜, ^𝑁𝑖 ← Split( ^𝑁 , 1)
24: ^𝑁𝑜𝑜, ^𝑁𝑜 ← Split( ^𝑁𝑜, 16)
25: ^𝑁𝑜𝑜𝑜, ^𝑁𝑜𝑜 ← Split( ^𝑁𝑜𝑜, 2)
26: ^𝑀𝑜, ^𝑀𝑖 ← Split( ^𝑀, 8)
27: ^𝑀𝑜𝑜, ^𝑀𝑜 ← Split( ^𝑀𝑜, 4)
28: ^𝑀𝑜𝑜𝑜, ^𝑀𝑜𝑜 ← Split( ^𝑀𝑜𝑜, 8)
29: 𝐾𝑜, 𝐾𝑖 = Split(𝐾, 4)
30: Reorder( ^𝑁𝑜𝑜𝑜, ^𝑀𝑜𝑜𝑜, ^𝑁𝑜𝑜, ^𝑀𝑜𝑜, 𝐾𝑜, ^𝑁𝑜, ^𝑀𝑜, 𝐾𝑖, ^𝑁𝑖, ^𝑀𝑖 )
31: ComputeAt(𝑫, 𝑀𝑜 )
32: 𝐹𝑁 𝑀 ← Fuse(𝑁𝑜, 𝑀𝑜 )
33: Parallel(𝐹𝑁 𝑀 )
34: Unroll(𝐹𝑁 𝑀 , 64)
35: Vectorize( ^𝑀𝑖 )

Algorithm 2 High level definition of a kernel class with a convo-
lutional layer, bias addition, and ReLU activation.

1: Define placeholders for inputs 𝑋 , weights 𝑊 , and bias 𝐵
2: Pad the input 𝑋 ′ ← Pad(𝑋 )
3: 𝑌 ← Conv2d(𝑋 ′)
4: 𝑌 ← 𝑌 + 𝐵
5: 𝑌 ← ReLU(𝑌 )
6: Return 𝑌

• Vectorize([range]): apply SIMD vectorization to a loop

range.

• ComputeAt([output tensor], [axis]): move a loop body com-

putation such that it is computed at a given axis.

Applying transfer-tuning to these GEMM computations, i.e. us-
ing the schedule generated for 𝐶1 = 𝐴1𝐵1 with 𝐶2 = 𝐴2𝐵2 and
vice-versa, we observe that we still produce valid code, obtain per-
formance within 5% of the native tuning for both kernels, and a
speedup of nearly 270× when compared to the unmodified compu-
tation without a schedule. The core difference in the auto-schedules
produced for 𝐶1 = 𝐴1𝐵1 and 𝐶2 = 𝐴2𝐵2 is that the latter uses
a temporary cache buffer to store intermediate results, as seen
on Line 22. Other differences are the unroll factors chosen by the
auto-scheduler, 512 for 𝐴1𝐵1 as seen in Line 16 and 64 for 𝐴2𝐵2

in Line 34. Note that in this case, when applying transfer-tuning
all of the transformations being applied are still valid, since both
computations are defined with the same initial loop structure and
no transformation is strongly dependent on a given data size.

4.2 Kernel Classes
We briefly introduced the idea of kernel classes in Figure 3, where
we can reuse auto-schedules between kernels if they contain the
same operations. We now discuss the concept in more detail. Kernels
are the units of computation which we pass to the auto-scheduler,
for example in DNNs a kernel may be a layer, or a set of layers
that can be composed together. Often kernels can contain several
operations, especially when they can be fully fused to encompass
the same loop structure (such as in the case of many activation
functions like ReLU). In this paper, we implement transfer-tuning
using TVM, and defer to the kernel partitioning generated by TVM
for a given DNN model, since the choices it makes are reasonable
(such as combining activation functions and bias additions with
larger layers such convolutional layers) and leads to state-of-the-art
performance in many benchmarks [7].

For example, a convolutional layer followed by a ReLU activa-
tion function can be treated as a single kernel, since we can fuse
the operations such that the ReLU function is applied within the
loop nest of the convolutional layer as soon as all the partial sums
for an output have been computed. This operation fusion saves a
full traversal of the output data and can thus reduce cache misses
significantly. The purpose of having distinct kernels, rather than
treating the whole program as a single function to be optimized,
is that it allows the kernels to be optimized independently and in
parallel. We define a kernel class to be a set of kernels that share
the same sequence of operations, regardless of their data sizes. For
example, one kernel class could be characterized by containing only
convolutional layers, another by containing a composition of dense
and ReLU layers, etc.

In Table 1 we observe the characteristics of the kernels and their
classes in the ResNet18 [14] model, a Convolutional Neural Network
(CNN) defined on the ImageNet dataset [28]. Most kernels include
a 2D convolutional layer, some of which include an activation
function, or a bias or a skip-connection addition. However we also
observe some pooling layers and a fully-connected layer. Some
kernels are repeated more than once in the model1, as represented
by the “Use Count” column. However, for the purposes of auto-
scheduling repeated kernels are only tuned once, although may be
given a higher proportion of the search time. Overall in ResNet18
we identify 6 kernel classes, labeled A-F: with classes A, E, and F
representing kernels featuring convolutional layers; B and C being
max-pooling and average-pooling layer kernels; and D being the
final fully-connected layer. We highlight that there are a variety of
kernel classes featuring convolutional layers (class labels A , E, and
F), with kernels of class E also including a bias addition, followed by
a ReLU activation, an overview of which we describe in Algorithm 2.
The operations of class E can be further decomposed into lower
level loop structures such as those describing the Conv2d algorithm,
however we do not include these details for brevity.

1Note that in ResNet18 the 18 refers to the total number of convolutional and fully
connected layers.

Table 1: Features of kernels in ResNet18, where class is a la-
bel for the operations in the kernel (seen in TVM Ops).

ID

Class

input_shape

kernel_shape

TVM Ops

A

A

A

E

E

F

E

E

F

E

E

F

E

E

F

[1, 256, 14, 14]
[1, 128, 28, 28]
[1, 64, 56, 56]
[1, 3, 224, 224]
[1, 64, 56, 56]
[1, 64, 56, 56]
[1, 64, 56, 56]
[1, 128, 28, 28]
[1, 128, 28, 28]
[1, 128, 28, 28]
[1, 256, 14, 14]
[1, 256, 14, 14]
[1, 256, 14, 14]
[1, 512, 7, 7]
[1, 512, 7, 7]

conv2d_add

conv2d_add

conv2d_add

conv2d_bias_relu

conv2d_bias_relu

conv2d_bias_relu

conv2d_bias_add_relu

[512, 256, 7, 7]
[256, 128, 14, 14]
[128, 64, 28, 28]
[64, 3, 112, 112]
[64, 64, 56, 56]
[64, 64, 56, 56]
[128, 64, 28, 28]
[128, 128, 28, 28]
[128, 128, 28, 28] conv2d_bias_add_relu
[256, 128, 14, 14]
[256, 256, 14, 14]
[256, 256, 14, 14] conv2d_bias_add_relu
[512, 256, 7, 7]
[512, 512, 7, 7]
[512, 512, 7, 7]

conv2d_bias_add_relu

conv2d_bias_relu

conv2d_bias_relu

conv2d_bias_relu

conv2d_bias_relu

conv2d_bias_relu

Class

input_shape

pool_size

TVM Ops

Use
Count
1

1

1

1

2

2

1

1

2

1

1

2

1

1

2

Use

Count
1

B

C

[1, 64, 112, 112]
[1, 512, 7, 7]

[2, 2]
[7, 7]

max_pool2d

global_avg_pool2d

1

Class

input_shape

weights_shape

TVM Ops

D

[1, 512]

[1, 1000]

dense_add

Use
Count
1

1

2

3

4

6

7

8

9

10

11

12

13

14

15

16

ID

5

17

ID

18

Along with characterizing operations of its class, kernels are also
defined by the data size of their inputs and weights. A schedule for a
kernel would apply transformations to the code in a manner similar
to the one seen in Algorithm 1, albeit with the transformations
being applied to a more complex initial loop structure. Much like
the GEMM example in Section 4.1, we observe that schedules can
be reused between kernels of the same class in ResNet18, even if
they are defined with different sizes. Thus, we could run transfer-
tuning using a schedule of class E on another kernel of class E.
In some cases the generated code may be invalid, for example if
the schedule defines a loop splitting factor which is larger than
the loop itself. Attempting to apply a schedule from class E to
another class, such as one defined by a fully-connected (dense)
layer of class D, would always be invalid as the schedule would try
to apply transformations to computations and loops not present in
the computation. In principle, for kernel classes which share some
of the operations (e.g., classes E and F), their schedules could be
adapted to allow a form of across-class transfer-tuning. Exploration
of this idea, as well as its impact on performance are outside the
scope of this paper.

4.3 Applying transfer-tuning
Now that we have discussed the principles of transfer-tuning, in-
cluding how it works and why kernel classes are relevant, next
we look at how it performs using a real DNN model. We take the
ImageNet definition of ResNet18 and use the auto-schedules of
ResNet50 generated by Ansor, a model chosen because it is likely to
have good potential for transfer-tuning due to its similar structure.
First, we evaluate each of the 18 kernels of ResNet18 with all
compatible schedules of ResNet50. Figure 4 shows the inference

Gibson and Cano

time of all of these kernel/schedule pairs, running as distinct pro-
grams. The purpose of this evaluation is to give us insights into
which schedules provide good performance improvements for each
kernel. We also compare against the performance of the kernel
when it uses the default schedule provided by TVM, which we refer
to as “untuned”.

We observe that there are six kernel classes in ResNet18, with
no schedules for classes F found in ResNet50. For class F we use
the default schedule provided by TVM, represented as a black bar.
For kernels of class A we have 4 compatible schedules to try from
ResNet50, kernels of class E can be compiled with 16 possible sched-
ules, and for kernels of classes B, C, and D note that we only have
one compatible schedule each. For class E we observe that some
schedules on some kernels produce invalid code, which we rep-
resent with a value of −1. There are 16 schedules of class E from
which 7 always produce invalid code for the kernels of ResNet18,
hence we do not represent them in the graph. We also observe sig-
nificant differences in inference time between schedules for some
kernels, for example for kernel 2 schedule A3 has over double the
inference time of A4.

Taking the best schedule found via transfer-tuning for each ker-
nel of ResNet18 and using them when compiling the full model
we can observe a speedup of 1.2×, as shown in the leftmost bar
of Figure 5a. The bar next to it shows that given the same search
time Ansor can only achieve a speedup of 1.01×. Search time for
transfer-tuning means the time for testing each kernel of the target
model with each valid schedule of the model chosen for transfer-
tuning, and choosing the best. This search time (around 1.2 minutes
for ResNet18) is shown in Figure 5b. In addition, we compare how
long Ansor requires to achieve our speedup, which in the case of
ResNet18 is 4.8× longer. This validates that transfer-tuning can
work in the context of a full DNN model. However, we chose the
model to tune with (ResNet50) arbitrarily. Thus in Section 4.4 we
give an overview of how we might select a model in a more sys-
tematic manner.

4.4 Model selection
In Section 4.3 we demonstrated the core concepts of transfer-tuning
using the ResNet18 model tuned from ResNet50. This was a reason-
able choice, since the model architectures are similar (they belong
to the same family of models). However, we need a more robust
approach to select the model we will use for transfer-tuning, which
we explore using 10 other models.

Selection heuristic. Table 2 shows a set of DNN models, their
4.4.1
kernel classes, the frequency of kernels of each class, and the pro-
portion of the untuned inference time that kernels of a given class
represent. For example, ResNet50 has 6 kernel classes representing
27 unique kernels (some kernels are repeated in the model), with
kernels of class E representing the majority of the untuned infer-
ence time (67%), and kernels of classes B, C, and D representing a
negligible proportion of the inference time. For brevity we do not
include details of each class, however expensive kernel classes tend
to include convolutional layers or fully connected layers, while
cheaper classes tend to contain operations such as pooling layers.
When choosing a model we want to maximize the likelihood that
it will provide a good tuning for a target model. We hypothesized in

Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation

Figure 4:
schedules which produced invalid code.

Inference time of ResNet18 kernels using ResNet50 schedules, with lower being better. Negative values denote

Section 3 that perhaps similarities between the kernels (e.g., having
the same convolutional kernel size, or similar memory footprint)
could be used to predict how successful a given transfer-tuning for
a kernel using a given schedule would be. However, in our initial
study we did not find any feature which had strong predictive power.
Thus, in this paper we adopt a more coarse-grained approach which
chooses a model to tune from based on the number of available
schedules of a given class, and the proportional cost of that kernel
class in untuned inference in the target model.

We define a selection heuristic which for a target model chooses
a model to tune with that maximizes the number of available tuned
schedules, giving preference for kernel classes that represent a
higher proportion of the untuned inference time. To avoid models
with very high numbers of schedules dominating the heuristic, we
increase the influence of the untuned costs by squaring it and reduce
the influence of the number of schedules in the tuning model by
taking the square root. Thus, we formulate our heuristic for a given
target model 𝑀, which has a set of kernel classes 𝐶, as choosing a
tuning model 𝑇 which maximizes the following:

𝑃 2
𝑐

√︁|𝑊𝑇 𝑐 |,

∑︁

𝑐 ∈𝐶

(1)

where 𝑃𝑐 is the proportional cost of kernel class 𝑐 in 𝑀, and 𝑊𝑇 𝑐
is the set of kernels of class 𝑐 in the candidate model 𝑇 . Looking at
Table 2, we can see for ResNet50 that the model which maximizes
Equation 1 is GoogLeNet, and the two versions of EfficientNet
maximize each other. For BERT and MobileBERT it is clear why
they are chosen for each other, as both contain kernels of class Q
(containing only a “dense” operation) representing 98% and 97%
of the inference time respectively.

However this heuristic is not guaranteed to make optimal deci-
sions. For example, the heuristic chose GoogLeNet for ResNet50 in
part because it had a high number of schedules for class E, which
represents 67% of ResNet50’s untuned inference time. This means
that for each of the 16 kernels of class E in ResNet50 there are
49 schedules that may reduce the inference time. However, the
9 schedules of class E in VGG-16 may be better at reducing the
overall inference time in ResNet50, even though there are fewer
of them. Note that the heuristic could be improved if we had a
better predictive model of which schedules may perform well for
transfer-tuning, however in this work we observe that this basic
heuristic gets reasonable results.

Table 3 shows transfer-tuning’s maximum speedup by applying
the top 3 models suggested by the heuristic. As we can see, the

trend is that the best speedup is achieved by Choice 1, and the
maximum speedup decreases with subsequent options. Note that
for BERT and MobileBERT every other model ties for second and
third place and gives no speedup, hence we leave these entries
blank (represented with “-”). This is because the only kernel class
in BERT and MobileBERT shared by other models is class D, which
represents less than 0.1% of their inference time.

4.4.2 Alternative heuristics. In Figure 5 we provide an evaluation
of the choices made by the heuristic described in Equation 1, demon-
strating that it can outperform tuning the DNN models from scratch
with Ansor. However, we could explore extensions to this heuristic
which may allow greater exploitation of transfer-tuning, improv-
ing the speedup and/or reducing the search time. For example, the
heuristic chooses a single model to transfer-tune from, however in
principle we could use all of the tuned schedules we have available
in Table 2. We evaluate the impact of using all the available tuned
schedules in Section 5.5. The caveat to consider is that this could
translate into a very high number of schedules to evaluate, which
would increase search time significantly. Thus, a more intelligent
heuristic might discard schedules that are less likely to improve
performance, and prioritize kernel classes by the potential improve-
ment they could get, since we observe that the average speedups
achievable by different kernel classes vary. We will explore this,
and other potential extensions to transfer-tuning in future work.

5 EVALUATION
In this section we evaluate the performance of 11 common DNN
models, ResNet18 and 10 more shown in Table 2, applying transfer-
tuning using auto-schedules from the model selected using the
heuristic described in Section 4.4. In Sections 5.3, 5.4, and 5.5 we
explore transfer-tuning on an edge platform, varying the sequence
length, and the impact of using a larger pool of schedules.

5.1 Experimental Setup
In addition to ResNet18 discussed in Section 4 we evaluate 10
more DNN models. The first 8 are CNNs defined on the ImageNet
dataset [28] and the final 2 being Transformer-based [39] models
for natural language sequence classification. The machine used to
evaluate the models includes an 8 core Intel Xeon E5-2620 CPU.
Auto-scheduling, compilation, and inference are executed using the
CPU, using 1 thread per CPU core. For our baselines, we take the
median inference time for each model over 10 runs, compiled using
TVM’s standard untuned schedules and the -o3 flag.

Table 2: Kernel classes of DNN models, with the number of kernels of each class, and the proportion of the untuned inference
time these kernels represent. Also shown is the model chosen for transfer-tuning.

Gibson and Cano

Model
ID
ResNet50
M1
AlexNet
M2
VGG-16
M3
MobileNetV2
M4
EfficientNetB0
M5
EfficientNetB4
M6
GoogLeNet
M7
MnasNet1.0
M8
BERT
M9
M10 MobileBERT

Kernel classes (number of kernels, percentage of inference time)
A (4, 17%); B (1, 0%); C (1, 0%); D (1, 6%); E (16, 67%); G (4, 10%)
B (3, 0%); D (1, 6%); E (5, 14%); H (2, 80%); I (1, 0%)
B (5, 0%); D (1, 1%); E (9, 59%); H (2, 40%); I (1, 0%)
A (7, 15%); C (1, 0%); D (1, 24%); J (8, 32%); K (5, 15%); L (10, 14%)
A (14, 9%); C (11, 4%); D (1, 12%); K (5, 9%); M (8, 39%); N (12, 27%); O (7, 0%)
A (16, 11%); C (13, 3%); D (1, 10%); K (7, 14%); M (9, 39%); N (14, 23%); O (9, 0%)
B (10, 1%); C (1, 0%); D (1, 4%); E (49, 95%)
A (7, 17%); D (1, 25%); E (9, 31%); K (5, 15%); P (12, 13%)
D (1, 0%); Q (3, 98%); R (2, 2%); S (1, 0%); T (1, 0%); U (1, 0%); V (1, 0%)
D (1, 0%); Q (4, 97%); R (2, 3%); S (1, 0%)

Tuning Model
GoogLeNet
VGG-16
GoogLeNet
EfficientNetB4
EfficientNetB4
EfficientNetB0
ResNet50
GoogLeNet
MobileBERT
BERT

(a) Speedup for transfer-tuning and Ansor given the same search time.

(b) Search time for transfer-tuning, and Ansor to match its speedup.

Figure 5: Transfer-tuning results for several models on a server-class CPU (Intel Xeon E5-2620).

Table 3: Transfer-tuning performance in terms of speedup
using the top 3 choices from the heuristic.

Model
ResNet50
AlexNet
VGG-16
MobileNetV2
EfficientNetB0
EfficientNetB4
GoogLeNet
MnasNet1.0
BERT
MobileBERT
† Note that models M1-M8 tie for Choices 2 and 3 giving no speedup (i.e., 1.0×),

Choice 1
M7 (1.16×)
M3 (4.6×)
M7 (1.19×)
M6 (1.20×)
M6 (1.23×)
M5 (1.13×)
M1 (1.15×)
M7 (1.26×)
M10 (59×)
M9 (13×)

Choice 2
M8 (1.0×)
M7 (1.05×)
M1 (1.0×)
M5 (1.21×)
M4 (1.08×)
M4 (1.04×)
M3 (1.04×)
M1 (1.25×)
-†
-†

Choice 3
M3 (1.09×)
M1 (1.03×)
M8 (1.0×)
M8 (1.19×)
M8 (1.09×)
M8 (1.03×)
M8 (1.0×)
M3 (1.18×)
-†
-†

hence we leave these entries blank (represented with “-”).

ResNet18 and ResNet50 [14] have 18 and 50 layers respec-
tively and consist of residual blocks. Each block contains two con-
volutional layers (that include between 64 and 2048 filters of size
3 × 3 and 1 × 1) and blocks are connected in a feed-forward manner.
AlexNet [18] is a canonical CNN model and consists of 5 convo-
lutional, 3 max-pooling, and 3 fully connected layers. Newer DNNs
use fewer fully connected layers to increase efficiency.

VGG-16 [31] is a CNN with 13 convolutional layers and 3 fully
connected layers. Some versions include batch normalization layers,
but in TVM these are removed/fused for inference.

MobileNetV2 [30] is a lighter weight model with 53 layers,
many of which feature depthwise convolutions which reduce the
number of parameters and operations required. This makes it ideal
for constrained edge devices.

EfficientNet [38] is a family of models with a focus on scala-
bility. The architecture of the smallest model (EfficentNetB0) was

found using neural architecture search (NAS) [45], and the accu-
racy of the model is improved by applying a novel scaling method
which changes the architecture to efficiently increase the number
of parameters and operations. There are sizes ranging from B0-B7,
and in this paper we evaluate EfficentNetB0 and EfficentNetB4.

GoogLeNet [36] (or InceptionV1) is a 22 layer model containing
9 so-called “inception” modules. This technique allows a deeper
model to be trained more efficiently.

MnasNet [37] is an model architecture designed for edge de-
vices such as mobile phones with an architecture generated using
NAS. We evaluate the model using a depth multiplier of 1.0, which
contains 52 convolutional layers and a dense layer.

BERT [11] is a Transformer-based [39] model which excels in
several natural language processing (NLP) tasks. It contains 12
layers, where a layer is a so called “transformer block”. We take a
definition of BERT for sequence classification tasks.

MobileBERT [35] is a compressed model inspired by the BERT
architecture. It has 24 layers and around 4.4× fewer parameters
than BERT. For both BERT and MobileBERT which can take variable
length input, we fix the sequence length at 256, with a discussion
of the impact of varying the sequence length in Section 5.4.

5.2 Results
Figure 5 shows the results of running transfer-tuning across the 11
models, with each model being tuned using the model suggested
by our heuristic described in Section 4.4. The only exception to this
is ResNet18, which was used as in illustrative example in Section 4.
Figure 5a shows the speedup achieved by transfer-tuning and An-
sor given the same search time. Figure 5b shows the search time
required by transfer-tuning and how much time Ansor requires to
match transfer-tuning’s speedup.

Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation

For ResNet50 we observe a speedup of 1.16×, requiring 7.2 min-
utes to achieve. Given the same search time, Ansor gets a speedup
of 1.03× and requires 1.8× as much search time to reach the same
speedup. For AlexNet we observe a speedup of 4.6× which takes
1.7 minutes to achieve. The maximum search time for AlexNet is
lower than ResNet50, as it has a smaller number of kernels. Ansor
given the same search time gets a speedup of 1.39× and requires
3.1× more search time to reach the same speedup. For VGG-16, we
observe a speedup of 1.19× which takes 5.2 minutes to achieve. To
achieve the same speedup Ansor requires 2.6× as much time.

MobileNetV2 (tuned using schedules from EfficientNetB4) ob-
tains a maximum speedup of 1.2×, which takes 1.8 minutes. Ansor
given the same time gets a speedup of 1.12× and requires 4.2×
more search time to reach the same speedup. We also observe
that over half of its kernels (those of classes J and L), representing
around 46% of the untuned inference time, are not transfer-tuned
by EfficientNetB4 since it does not contain them. This suggests that
there is further scope for improvement, for example tuning using
schedules from a model which included those kernel classes could
increase the maximum speedup obtained.

EfficientNetB0 (also tuned with EfficientNetB4) gets a maximum
speedup of 1.23×, which takes 12 minutes. The search time is
much higher than most other models, since there are 58 kernels to
evaluate with 764 unique kernel/schedule pairs. Ansor given the
same time gets a speedup of 1.03× and requires 2.15× as much
search time to reach the same speedup.

For EfficientNetB4 (tuned with EfficientNetB0) the situation is
similar with a high search time of 13 minutes due to having 69
kernels, or 775 kernel/schedule pairs to evaluate. The speedup is
1.13× which is lower than EfficientNetB0, with Ansor requiring
2.23× more time to reach the same speedup. We observe that given
the same time as transfer-tuning Ansor sees a slowdown by 0.96×
compared to the baseline. This is not unexpected, as due to their
stochasticity, auto-schedulers can sometimes hurt performance
initially even if they eventually converge on an improved schedule.
For GoogLeNet, we observe a speedup of 1.15× which takes
8.8 minutes to achieve. Like the two EfficientNet models, a higher
number of kernels (61) make the search time higher than other
models. Given the same time, Ansor achieves a speedup of 1.08×
and requires 5.3× more time to reach the same speedup.

As the final ImageNet model, MnasNet1.0 tuned with GoogLeNet
achieves a maximum speedup of 1.26×, taking 4.8 minutes. Given
the same time, Ansor takes 1.08× and requires 2.5× as much time
to achieve the same speedup.

Finally, BERT and MobileBERT see the most dramatic perfor-
mance improvements of 59× and 13× respectively. In addition, they
see the largest relative difference in search time required compared
to Ansor, 33× and 10× respectively.

Overall, these results show that transfer-tuning can outperform
Ansor when given a limited amount of search time. Figure 1 shows
that each model varies in the potential maximum speedup it can
achieve, where we take the maximum speedup to be achieved us-
ing Ansor’s recommended 20, 000 schedule variants. Therefore, to
compare the performance of our DNN models fairly we show the
proportion of this maximum speedup transfer-tuning achieves in
Table 4. On average, transfer-tuning achieves 49.12% of Ansor’s
maximum speedup, with VGG-16 being the lowest with 17.69% and

Table 4: Transfer-tuning versus 20, 000 Ansor iterations.

Model
ResNet18
ResNet50
AlexNet
VGG-16
MobileNetV2
EfficentNetB0
EfficentNetB4
MnasNet1.0
GoogLeNet
BERT
MobileBERT

Speedup (%)
49.20
40.65
71.54
17.69
29.16
80.14
52.35
44.18
48.00
88.41
18.96

Search time (%)
0.48
2.91
0.64
1.41
1.10
5.34
6.41
2.40
2.00
0.08
0.10

Mean

49.12

2.08

BERT being the highest with 88.41% Compared to the search time
required by Ansor to explore 20, 000 schedule variants, transfer-
tuning requires only 2.08% of this time on average. However, the
values in Figure 5b give a more informative comparison showing
that to achieve the same speedup as transfer-tuning, Ansor requires
over 6.5× more time than transfer-tuning on average, with the low-
est relative difference being for ResNet50 (1.8×), and the highest
being for BERT (33×).

5.3 Exploring a constrained edge platform
To further validate transfer-tuning we evaluate our models on a
Raspberry Pi 4 B, a common low-power edge device with an Arm
Cortex-A72 CPU. Such devices represent another potential applica-
tion of transfer-tuning, as they may not have the resources to under-
take auto-scheduling themselves. Ansor allows edge devices to be
connected to a more powerful server which runs auto-scheduling
over RPC. However, this process can still be slow, may not always
be available, and is not scalable to deployment across large hetero-
geneous fleets of devices.

Figure 6a shows the speedups achieved on the Raspberry Pi 4,
and Figure 6b shows the search time required. We observe that
the relative differences between transfer-tuning and Ansor become
exacerbated in terms of tuning time, with Ansor requiring over
10.8× as much time to reach the same speedup on average, which
is significantly higher than the 6.5× difference observed on the x86
platform. In future work we will explore if transfer-tuning is viable
between hardware platforms.

5.4 Varying sequence length
Unlike the ImageNet models, which takes input data of fixed sizes
(224 × 224), sequence models such as BERT and MobileBERT can
take variable input sizes, e.g. a longer or shorter sentence. However,
from the perspective of Ansor varying the input size means the
whole model is different, since every single kernel has different
data sizes to process. In principle, auto-scheduling could occur
with a given dimension being specified as being dynamic. However,
to date TVM has poor support for this and no support for this
when tuning 2. In addition, this could potentially lose out on some
optimizations by keeping the input size fixed.

2See https://discuss.tvm.apache.org/t/does-tvm-support-dynamic-input-shape/11069

Gibson and Cano

(a) Speedup for transfer-tuning and Ansor given the same search time.

(b) Search time for transfer-tuning, and Ansor to match its speedup.

Figure 6: Transfer-Tuning results for several models on an edge CPU (Arm Cortex-A72).

Figure 7: Transfer-tuning varying the sequence length of
BERT models (Intel Xeon E5-2620).

Therefore, as an alternative view on transfer-tuning, we evalu-
ate models of the same architecture but with different input sizes,
namely BERT and MobileBERT for sequence lengths of 128 and
256. In Section 5.2 we evaluated these models for a sequence length
of 256, therefore we must also tune versions of these models with
sequence length 128. Figure 7 shows the results, with for example
“BERT-128” representing the BERT model for sequence length 128
being tuned using schedules from “BERT-256”.

We observe that the improvement is greater applying tuning
from a larger sequence length to a smaller sequence length, 3.3×
as much improvement on average. We also note that compared
to the results of Figure 5 (which shows BERT and MobileBERT
with sequence length 256 being tuned with each other), BERT in
Figure 7 gets less of a speedup (3.87× less) and MobileBERT gets
approximately the same speedup (around 13×).

Varying input data sizes are common in sequence models such
as BERT and MobileBERT. However, in CNNs for computer vision
often publicly available models trained on a common dataset (e.g.,
ImageNet) are fine-tuned on a new dataset which may have a dif-
ferent input data size. Thus, this could represent another use-case
for transfer-tuning, which we leave for future work.

5.5 Alternative heuristics
As discussed in Section 4.4.2, there is more than one way for transfer-
tuning to select schedules, and this can be an implementation detail
to suit the needs of a given use-case. Throughout the paper we have
demonstrated the core concepts and functionality of transfer-tuning
by implementing “one-to-one” model transfer-tuning, described
by our heuristic in Section 4.4.1. This heuristic was devised from
analytical observations about the features of models and their kernel
classes, and has demonstrated speedups successfully. However, as
discussed in Section 4.4.2, if we have tuned schedules available for
a set of DNN models, as an alternative approach we could explore
exploiting all of these schedules regardless of model.

Thus in this section we provide a brief evaluation of how transfer-
tuning can be implemented to deal with this possibility. For each
of our models described Section 5.1, we take the pool of schedules
from Table 2 and make all of them available to the target model.
Note that the concept of “models” is irrelevant to the pool, and
for every kernel in the target model transfer-tuning picks the best
schedule according to the standalone performance of its kernel.

We show the results of this evaluation in Figure 8. Our first obser-
vation in Figure 8b is that the search time increases by around 2×
on average, with the highest being ResNet18 with a 5.34× increase.
An increase in search time is expected, since we increased the num-
ber of kernel/schedule pairs we evaluate. However, because each
model contains varying kernel classes, this increase in pairs varies
between models. For instance, BERT and MobileBERT see a negligi-
ble increase in search time, as only kernels of class D are given new
schedules to explore. In situations with many kernel/schedule pairs,
we could reduce the search time by sampling a subset of schedules,
either randomly or using some other selection heuristic.

For speedup, we can make several observations from Figure 8a.
First we see that the maximum speedups achieved for AlexNet,
VGG-16, and MnasNet1.0 increase compared to “one-to-one”. For
MnasNet1.0 the improvement is modest (1.27× speedup compared
to 1.26×), however for AlexNet and VGG-16 it is more significant:
from 4.6× to 5.2× and from 1.19× to 2.0× respectively. This makes
intuitive sense: either we use the best schedules found in the one-
to-one approach, or we have new schedules in the pool that allow
us to improve further. We observe that MobileNetV2 chooses the
same schedules as before and provides the same speedup.

However, contrary to this intuition, we observe that 7 models
see a reduced speedup. Despite the fact we are selecting the ker-
nel/schedule pairs with the lowest standalone inference time, our
overall speedup when running the full model is lower than the
initial one-to-one approach, even though the kernel/schedules used
in the one-to-one case gave higher standalone inference times.

Our conclusion is that although the performance of kernels run-
ning in a standalone manner are a proxy to their performance in the
context of a full tensor program, they do not capture all potentially
relevant interactions. Our implementation of transfer-tuning as-
sumes that the fastest kernel running as a standalone program will
also be the fastest when running in the context of the full tensor
program. This assumption of the independence of kernels is also
used by Ansor, which tunes all kernels as standalone programs, and
combines the best schedules together.

Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation

(a) Speedup for transfer-tuning using schedules from a single model,
and a mixed pool of models.

(b) Search time for transfer-tuning using schedules from a single
model, and a mixed pool of models.

Figure 8: Transfer-tuning using a schedule pool of several models on a server-class CPU (Intel Xeon E5-2620).

It is clear that this assumption is sufficient for transfer-tuning to
provide improvements over Ansor, however the results in Figure 8
demonstrate that there may be performance considerations of “inter-
kernel” relationships that are not captured by standalone kernel
evaluation. For example, the output data of one kernel may be used
as the input data for a subsequent kernel. The data access patterns
of the first kernel will dictate the cache placement of the output
data, which will impact the read times of the data when it is used
in the second kernel. We can imagine an extreme case where the
average reuse distance of the output/input data between kernels
is at its maximum. This could significantly increase the inference
time of the second kernel.

Therefore, awareness and exploitation of this dynamic may en-
able further optimizations for transfer-tuning and related methods.
We leave a thorough exploration of inter-kernel relationships for
future work, however approaches could include per-kernel profiling
when running the full program, and evaluating kernels pairwise.

6 RELATED WORK
Schedule based computation was popularized in Halide [26], how-
ever it does not provide all of the graph level optimizations avail-
able in TVM [7]. TVM builds on the ideas of Halide to bring a us-
able schedule compiler for machine learning. Other works include
RISE/ELEVATE [13] which are well defined functional languages
for compute declaration and scheduling respectively, however are
not currently production ready.

Auto-tuning frameworks, especially for compute-schedule based
systems like TVM, are a popular area of research. AutoTVM [8]
takes hand-engineered schedules for operations and explores pa-
rameter tunings across a space defined by the schedule author, such
as tiling sizes, unrolling factors, and others. However, it should be
noted that AutoTVM compared to Ansor cannot reach the same
same maximum speedup, as AutoTVM constrains the search space.
Choices to efficiently explore AutoTVM’s search space vary, includ-
ing approaches such as gradient boosting [6] and genetic algorithms.
Chameleon [1] improves the search strategies of AutoTVM by lever-
aging reinforcement learning. In future work transfer-tuning could
be extended to vary parameters from schedules transfer-tuned from
another model to further increase performance.

Regarding auto-scheduling, FlexTensor [44] is an auto-scheduling
system similar to Ansor, although it relies on more hand written
templates, thus seeing worse performance on some benchmarks.

LIFT [32] explores the use of rewrite rules on high level representa-
tions of programs to generate OpenCL code, although it does not
explore its large search space as efficiently as Ansor. The Tiramisu
deep learning compiler [3] recently added support for an auto-
scheduling system [2]. Overall, none of these approaches exploit
the notion of transfer-tuning to reduce search time.

The reuse of bundles of optimizations has been explored in
other works beyond tensor programs. For example, Martins et
al. [23] looks at similarities between C functions to cluster them
into groups and applies compiler passes based on group member-
ship. In transfer-tuning we have more domain specific knowledge
we can leverage, since we are in the space of tensor programs with
well-defined operations. CompilerGym [10] exposes LLVM com-
piler optimizations to reinforcement learning agents via the OpenAI
Gym [5]. Like Martins et al. [23], its focus is on more general pur-
pose program optimization and does not exploit domain specific
knowledge of tensor programs.

Overall, transfer-tuning leverages the ideas of schedule based
programming paradigms such as Halide and TVM, as well as auto-
scheduling introduced by Ansor. Other works have exploited simi-
larity between programs to make compilation optimization more
efficient. However, transfer-tuning’s novelty comes from leveraging
this workload similarity in the domain of schedule based tensor
compilers to reduce search costs.

7 CONCLUSION
In this paper we proposed transfer-tuning as a new approach to
exploit similarities in tensor programs to reuse efficient schedules
found via auto-scheduling. We have discussed how transfer-tuning
is feasible in a compute/schedule programming paradigm, and ex-
plained the key components necessary to accelerate tuning of a
full model. We defined an implementation of transfer-tuning and
evaluated the performance on 11 models on a server-class x86 CPU,
achieving 49.12% of the maximum speedup achieved by the Ansor
auto-scheduler on average, and with Ansor requiring over 6.5× as
much time to match our performance. We also evaluated transfer-
tuning on a constrained edge device, the Raspberry Pi 4, showing
that the gap between Ansor and transfer-tuning is exacerbated,
with Ansor requiring over 10.8× as much time to match our perfor-
mance. For future work, we will explore the impact of across-kernel
interactions, the viability of applying transfer-tuning across similar
kernel classes, and transfer-tuning across hardware devices.

REFERENCES
[1] Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh, and Hadi Es-
maeilzadeh. 2020. Chameleon: Adaptive Code Optimization for Expedited Deep
Neural Network Compilation. In 8th International Conference on Learning Repre-
sentations, ICLR 2020.

[2] Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel
Abdous, Taha Arbaoui, Karima Benatchba, and Saman Amarasinghe. 2021. A
Deep Learning Based Cost Model for Automatic Code Optimization. Proceedings
of Machine Learning and Systems (MLSys), 181–193.

[3] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Ab-
durrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, and Saman
Amarasinghe. 2019. Tiramisu: A Polyhedral Compiler for Expressing Fast and
Portable Code. In Proceedings of the 2019 IEEE/ACM International Symposium on
Code Generation and Optimization (CGO). 193–205.

[4] Paul Barham and Michael Isard. 2019. Machine Learning Systems Are Stuck in a
Rut. In Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS
’19). 177–183.

[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-
(June 2016).

man, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym.
arXiv:1606.01540

[6] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD ’16). 785–794.

[7] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan
Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin,
and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning. In Proceedings of the 13th USENIX Conference on
Operating Systems Design and Implementation (OSDI’18). 579–594.

[8] T. Chen, L. Zheng, E. Yan, Z. Jiang, T. Moreau, L. Ceze, C. Guestrin, and A.
Krishnamurthy. 2018. Learning to Optimize Tensor Programs. In Advances in
Neural Information Processing Systems 31 (NeurIPS). 3393–3404.

[9] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient Primitives
for Deep Learning. (Oct. 2014). arXiv:1410.0759

[10] Chris Cummins, Bram Wasti, Jiadong Guo, Brandon Cui, Jason Ansel, Sahir
Gomez, Somya Jain, Jia Liu, Olivier Teytaud, Benoit Steiner, Yuandong Tian, and
Hugh Leather. 2022. CompilerGym: Robust, Performant Compiler Optimization
Environments for AI Research. In CGO 2022.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Vol. 1. 4171–4186.
[12] Perry Gibson, José Cano, Jack Turner, Elliot J. Crowley, Michael O’Boyle, and
Amos Storkey. 2020. Optimizing Grouped Convolutions on Edge Devices. In 2020
IEEE 31st International Conference on Application-Specific Systems, Architectures
and Processors (ASAP). 189–196.

[13] Bastian Hagedorn, Johannes Lenfers, Thomas Kœhler, Xueying Qin, Sergei Gor-
latch, and Michel Steuwer. 2020. Achieving High-Performance the Functional
Way: A Functional Pearl on Expressing High-Performance Optimizations as
Rewrite Strategies. Proceedings of the ACM on Programming Languages 4, ICFP
(Aug. 2020), 92:1–92:29.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. 2017. Densely
Connected Convolutional Networks. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). 2261–2269.

[16] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A Convolutional
Neural Network for Modeling Sentences. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguistics.

[17] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In

The Conference on Empirical Methods in Natural Language Processing.

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classifi-
cation with Deep Convolutional Neural Networks. Communications of the ACM
60, 6 (2012), 84–90.

[19] J. Nathan Kutz. 2017. Deep Learning in Fluid Dynamics. Journal of Fluid Mechanics

814 (March 2017), 1–4.

[20] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In Proceedings of the International
Symposium on Code Generation and Optimization (CGO).

[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional
Networks for Semantic Segmentation. In 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). 3431–3440.

[22] D. Luebke. 2008. CUDA: Scalable parallel programming for high-performance
scientific computing. In 2008 5th IEEE International Symposium on Biomedical
Imaging: From Nano to Macro. 836–838.

[23] Luiz G. A. Martins, Ricardo Nobre, João M. P. Cardoso, Alexandre C. B. Delbem,
and Eduardo Marques. 2016. Clustering-Based Selection for the Exploration of

Gibson and Cano

Compiler Optimization Sequences. ACM Transactions on Architecture and Code
Optimization 13, 1 (March 2016), 8:1–8:28.

[24] Thierry Moreau, Tianqi Chen, Luis Vega, Jared Roesch, Eddie Yan, Lianmin Zheng,
Josh Fromm, Ziheng Jiang, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy.
2019. A Hardware-Software Blueprint for Flexible Deep Learning Specialization.
(April 2019). arXiv:1807.04188

[25] oneDNN. 2020. oneDNN. oneAPI. https://github.com/oneapi-src/oneDNN
[26] Jonathan Ragan-Kelley, Andrew Adams, Dillon Sharlet, Connelly Barnes, Syl-
vain Paris, Marc Levoy, Saman Amarasinghe, and Frédo Durand. 2017. Halide:
Decoupling Algorithms from Schedules for High-Performance Image Processing.
Commun. ACM 61, 1 (Dec. 2017), 106–115.

[27] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You
Only Look Once: Unified, Real-Time Object Detection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV) (2015).

[29] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic Routing
In Advances in Neural Information Processing Systems 30

Between Capsules.
(NeurIPS). 3856–3866.

[30] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In
Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[31] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In International Conference on Learning
Representations (ICLR).

[32] Michel Steuwer, Christian Fensch, Sam Lindley, and Christophe Dubach. 2015.
Generating Performance Portable Code Using Rewrite Rules: From High-Level
Functional Expressions to High-Performance OpenCL Code. In Proceedings of the
20th ACM SIGPLAN International Conference on Functional Programming (ICFP
2015). 205–217.

[33] Axel Stjerngren, Perry Gibson, and José Cano. 2022. Bifrost: End-to-End Evalua-
tion and Optimization of Reconfigurable DNN Accelerators. In IEEE International
Symposium on Performance Analysis of Systems and Software (ISPASS). 288–299.
[34] J. E. Stone, D. Gohara, and G. Shi. 2010. OpenCL: A Parallel Programming

Standard for Heterogeneous Computing Systems. IEEE CiSE (2010).

[35] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny
Zhou. 2020. MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited
Devices. In Proceedings of the 58th Annual Meeting of the Association for Compu-
tational Linguistics. 2158–2170.

[36] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-
novich. 2015. Going Deeper with Convolutions. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR. 1–9.

[37] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
Howard, and Quoc V. Le. 2019. MnasNet: Platform-Aware Neural Architecture
Search for Mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR). 2820–2828.

[38] Mingxing Tan and Quoc Le. 2019. EfficientNet: Rethinking Model Scaling for
Convolutional Neural Networks. In International Conference on Machine Learning.
PMLR, 6105–6114.

[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. In Advances in Neural Information Processing Systems 30 (NeurIPS).
5998–6008.

[40] Zhang Xianyi, Wang Qian, and Zhang Yunquan. 2012. Model-Driven Level 3
BLAS Performance Optimization on Loongson 3A Processor. In 2012 IEEE 18th
International Conference on Parallel and Distributed Systems (ICPADS). 684–691.
[41] Xinyue Zhang, Yanfang Wang, Wei Zhang, Yueqiu Sun, Siyu He, Gabriella Con-
tardo, Francisco Villaescusa-Navarro, and Shirley Ho. 2019. From Dark Matter to
Galaxies with Convolutional Networks. (March 2019). arXiv:1902.05965
[42] Lanmin Zheng and Tianqi Chen. 2018. Optimizing Deep Learning Workloads on
ARM GPU with TVM. In Proceedings of the 1st on Reproducible Quality-Efficient
Systems Tournament on Co-Designing Pareto-Efficient Deep Learning.

[43] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer
Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez,
and Ion Stoica. 2020. Ansor: Generating High-Performance Tensor Programs
for Deep Learning. In 14th USENIX Symposium on Operating Systems Design and
Implementation, OSDI 2020. 863–879.

[44] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. 2020. Flex-
Tensor: An Automatic Schedule Exploration and Optimization Framework for
Tensor Computation on Heterogeneous System. In Proceedings of the Twenty-Fifth
International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS ’20). 859–873.

[45] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement

Learning. (Feb. 2017). arXiv:1611.01578

