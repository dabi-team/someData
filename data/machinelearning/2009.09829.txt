Generalized Leverage Score Sampling for Neural Networks∗

Jason D. Lee†

Ruoqi Shen‡

Zhao Song§

Mengdi Wang¶

Zheng Yuk

Abstract

Leverage score sampling is a powerful technique that originates from theoretical computer
linear
science, which can be used to speed up a large number of fundamental questions, e.g.
regression, linear programming, semi-deﬁnite programming, cutting plane method, graph spar-
siﬁcation, maximum matching and max-ﬂow. Recently, it has been shown that leverage score
sampling helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker and
Zandieh 17].

In this work, we generalize the results in [Avron, Kapralov, Musco, Musco, Velingker and
Zandieh 17] to a broader class of kernels. We further bring the leverage score sampling into the
ﬁeld of deep learning theory.

•

•

We show the connection between the initialization for neural network training and approx-
imating the neural tangent kernel with random features.

We prove the equivalence between regularized neural network and neural tangent kernel
ridge regression under the initialization of both classical random Gaussian and leverage
score sampling.

0
2
0
2

p
e
S
1
2

]

G
L
.
s
c
[

1
v
9
2
8
9
0
.
9
0
0
2
:
v
i
X
r
a

∗The authors would like to thank Michael Kapralov for suggestion of this topic.
†jasonlee@princeton.edu Princeton University.
‡shenr3@cs.washington.edu University of Washington. Work done while visiting Institute for Advanced Study.
§magic.linuxkde@gmail.com Columbia University, Princeton University and Institute for Advanced Study.
¶mengdiw@princeton.edu Princeton University.
kzhengy@princeton.edu Princeton University.

 
 
 
 
 
 
Contents

1 Introduction

2 Related work

3 Main results

3.1 Kernel approximation with leverage score sampling . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Application in training regularized neural network . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Equivalence I, training with random Gaussian initialization . . . . . . . . . . . . . . .
3.2.2 Equivalence II, training with leverage scores . . . . . . . . . . . . . . . . . . . . . . . .

4 Overview of techniques

5 Conclusion

A Preliminaries

A.1 Probability tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Neural tangent kernel and its properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B Equivalence between suﬃciently wide neural net and kernel ridge regression

B.1 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Gradient, gradient ﬂow, and linear convergence . . . . . . . . . . . . . . . . . . . . . . . . . .
B.4 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5 Equivalence between training net with regularization and kernel ridge regression for test data
prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
u∗
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5.1 Upper bounding
test
|
by bounding initialization and kernel per-
B.5.2 Upper bounding
untk,test(T )
|
turbation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5.3 Upper bounding initialization perturbation . . . . . . . . . . . . . . . . . . . . . . . .
B.5.4 Upper bounding kernel perturbation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.5.5 Final result for upper bounding
. . . . . . . . . . . . . . . .
B.5.6 Main result for test data prediction equivalence . . . . . . . . . . . . . . . . . . . . . .
B.6 Equivalence between training net with regularization and kernel ridge regression for training
data prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

untk,test(T )
unn,test(T )

untk,test(T )
|

unn,test(T )

−
−

|
|

−

|

C Generalization result of leverage score sampling for approximating kernels

C.1 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Equivalence between training neural network with regularization and kernel ridge re-

gression under leverage score sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.1 Preliminaries
D.2 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.3 Leverage score sampling, gradient ﬂow, and linear convergence
. . . . . . . . . . . . . . . . .
D.4 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.5 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.5.1 Upper bounding
u∗
D.5.2 Upper bounding
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.5.3 Main result for equivalence with leverage score sampling initialization . . . . . . . . .

u∗
u∗
−
unn(T )

k
−

k
k

k

2

E Extension to other neural network models

References

i

1

2

3
3
5
5
6

7

10

11
11
12

13
13
14
16
20

21
21

22
26
27
34
35

35

36
36
38

41
41
42
44
50
51
51
52
59

59

61

1 Introduction

Kernel method is one of the most common techniques in various machine learning problems. One
classical application is the kernel ridge regression (KRR). Given training data X = [x1,
, xn]⊤ ∈
Rn and regularization parameter λ > 0, the output
Rn
estimate of KRR for any given input z can be written as:

d, corresponding labels Y = [y1,

, yn]

· · ·

· · ·

∈

×

f (z) = K(z, X)⊤(K + λIn)−

1Y,

(1)

×

∈

where K(
,
·

n denotes the kernel matrix.

) denotes the kernel function and K
·

Rn
Despite being powerful and well-understood, the kernel ridge regression suﬀers from the costly
computation when dealing with large datasets, since generally implementation of Eq. (1) requires
O(n3) running time. Therefore, intensive research have been dedicated to the scalable methods
for KRR [Bac13, AM15, ZDW15, ACW17, MM17, ZNV+20]. One of the most popular approach
is the random Fourier features sampling originally proposed by [RR08] for shift-invariant kernels.
Cs through sampling that
They construct a ﬁnite dimensional random feature vector φ : Rd
→
approximates the kernel function K(x, z)
Rd. The random feature helps
approximately solves KRR in O(ns2 + n2) running time, which improves the computational cost if
n. The work [AKM+17] advanced this result by introducing the leverage score sampling to
s
take the regularization term into consideration.

φ(x)∗φ(z) for data x, z

≪

≈

∈

In this work, we follow the the approach in [AKM+17] and naturally generalize the result to a

broader class of kernels, which is of the form

K(x, z) = E
w
∼

[φ(x, w)⊤φ(z, w)],
p

Rd1

where φ : Rd
Rd2 is a ﬁnite dimensional vector and p : Rd1
0 is a probability
distribution. We apply the leverage score sampling technique in this generalized case to obtain a
tighter upper-bound on the dimension of random features.

→

→

×

R

≥

Further, We discuss the application of our theory in neural network training. Over the last two
years, there is a long line of over-parametrization theory works on the convergence results of deep neu-
ral network [LL18, DZPS19, AZLS19b, AZLS19a, DLL+19, ADH+19b, ADH+19a, SY19, BPSW20],
all of which either explicitly or implicitly use the property of neural tangent kernel [JGH18]. How-
ever, most of those results focus on neural network training without regularization, while in practice
regularization (which is originated from classical machine learning) has been widely used in training
deep neural network. Therefore, in this work we rigorously build the equivalence between training
a ReLU deep neural network with ℓ2 regularization and neural tangent kernel ridge regression. We
observe that the initialization of training neural network corresponds to approximating the neural
tangent kernel with random features, whose dimension is proportional to the width of the net-
work. Thus, it motivates us to bring the leverage score sampling theory into the neural network
training. We present a new equivalence between neural net and kernel ridge regression under the
initialization using leverage score sampling, which potentially improves previous equivalence upon
the upper-bound of network width needed.

We summarize our main results and contribution as following:

Generalize the leverage score sampling theory for kernel ridge regression to a broader class of
kernels.

Connect the leverage score sampling theory with neural network training.

•

•

1

•

Theoretically prove the equivalence between training regularized neural network and kernel
ridge regression under both random Gaussian initialization and leverage score sampling ini-
tialization.

2 Related work

×

Leverage scores Given a m
n matrix A. Let a⊤i be the i-th rows of A and the leverage score
of the i-th row of A is σi(A) = a⊤i (A⊤A)†ai. A row’s leverage score measures how important it
is in composing the row space of A. If a row has a component orthogonal to all other rows, its
leverage score is 1. Removing it would decrease the rank of A, completely changing its row space.
The coherence of A is
. If A has low coherence, no particular row is especially important.
If A has high coherence, it contains at least one row whose removal would signiﬁcantly aﬀect the
composition of A’s row space.

σ(A)
k

k∞

Leverage score is a fundamental concept in graph problems and numerical linear algebra. There
are many works on how to approximate leverage scores [SS11, DMIMW12, CW13, NN13] or more
general version of leverages, e.g. Lewis weights [Lew78, BLM89, CP15]. From graph perspective,
it has been applied to maximum matching [BLN+20, LSZ20], max-ﬂow [DS08, Mad13, Mad16,
LS20b, LS20a], generate random spanning trees [Sch18], and sparsify graphs [SS11]. From matrix
perspective, it has been used to give matrix CUR decomposition [BW14, SWZ17, SWZ19] and tensor
CURT decomposition [SWZ19]. From optimization perspective, it has been used to approximate the
John Ellipsoid [CCLY19], linear programming [LS14, BLSS20, JSWZ20], semi-deﬁnite programming
[JKL+20], and cutting plane methods [Vai89, LSW15, JLSW20].

Kernel methods Kernel methods can be thought of as instance-based learners: rather than
learning some ﬁxed set of parameters corresponding to the features of their inputs, they instead
“remember” the i-th training example (xi, yi) and learn for it a corresponding weight wi. Prediction
for unlabeled inputs, i.e., those not in the training set, is treated by the application of similarity
function K, called a kernel, between the unlabeled input x′ and each of the training inputs xi.

There are three lines of works that are closely related to our work. First, our work is highly re-
lated to the recent discoveries of the connection between deep learning and kernels [DFS16, Dan17,
JGH18, CB18]. Second, our work is closely related to development of connection between lever-
age score and kernels [RR08, CW17, CMM17, MW17b, MW17a, LTOS18, AKM+17, AKM+19,
ACSS20]. Third, our work is related to kernel ridge regression [Bac13, AM15, ZDW15, ACW17,
MM17, ZNV+20].

Convergence of neural network There is a long line of work studying the convergence of neu-
ral network with random input assumptions [BG17, Tia17, ZSJ+17, Sol17, LY17, ZSD17, DLT+18,
GLM18, BJW19]. For a quite while, it is not known to remove the randomness assumption from the
input data points. Recently, there is a large number of work studying the convergence of neural net-
work in the over-parametrization regime [LL18, DZPS19, AZLS19b, AZLS19a, DLL+19, ADH+19b,
ADH+19a, SY19, BPSW20]. These results don’t need to assume that input data points are random,
and only require some much weaker assumption which is called “data-separable”. Mathematically,
it says for any two input data points xi and xj, we have
δ. Suﬃciently wide neural
xjk2 ≥
network requires the width m to be at least poly(n, d, L, 1/δ), where n is the number of input data
points, d is the dimension of input data point, L is the number of layers.

xi −
k

2

Continuous Fourier transform The continuous Fourier transform is deﬁned as a problem
xj ,t
[JLS20] where you take samples f (t1),
i,
h
· · ·
and try to reconstruct function f : Rd
Rd. The data separation
C or even recover
connects to the sparse Fourier transform in the continuous domain. We can view the n input data
points [LL18, AZLS19b, AZLS19a] as n frequencies in the Fourier transform [Moi15, PS15]. The
separation of the data set is equivalent to the gap of the frequency set (mini
δ).
In the continuous Fourier transform, there are two families of algorithms: one requires to know
the frequency gap [Moi15, PS15, CM20, JLS20] and the other doesn’t [CKPS16]. However, in the
over-parameterized neural network training, all the existing work requires a gap for the data points.

, f (tm) from the time domain f (t) :=
(vj, xj)
{

j=1 vje2πi

xjk2 ≥

xi −

=j k

} ∈

P

→

×

C

n

Notations We use i to denote √
k2 to denote the ℓ2 norm of x. For
x
1. For vector x, we use
k
kF to denote the Frobenius norm
A
to denote the spectral norm of A and
A
matrix A, we use
k
k
k
A is positive semi-deﬁnite. For a
of A. For matrix A and B, we use A
B to denote that B
−
1 to denote the true inverse of an
square matrix, we use tr[A] to denote the trace of A. We use A−
invertible matrix. We use A† to denote the pseudo-inverse of matrix A. We use A⊤ to denote the
transpose of matrix A.

(cid:22)

−

3 Main results

In this section, we state our results. In Section 3.1, we consider the large-scale kernel ridge regression
(KRR) problem. We generalize the Fourier transform result [AKM+17] of accelerating the running
time of solving KRR using the tool of leverage score sampling to a broader class of kernels.
In
Section 3.2, we discuss the interesting application of leverage score sampling for training deep
learning models due to the connection between regularized neural nets and kernel ridge regression.

3.1 Kernel approximation with leverage score sampling

In this section, we generalize the leverage score theory in [AKM+17], which analyzes the number of
random features needed to approximate kernel matrix under leverage score sampling regime for the
kernel ridge regression task. In the next a few paragraphs, we brieﬂy review the settings of classical
kernel ridge regression.

Given training data given training data matrix X = [x1,

Y = [y1,
· · ·
can be written as1

, yn]⊤ ∈

Rn and feature map φ : Rd

→ F

d, corresponding labels
, a classical kernel ridge regression problem

, xn]⊤ ∈

· · ·

×

Rn

min
β

1
2 k

Y

−

φ(X)⊤β

2
2 +
k

1
2

λ

β
k

2
2
k

where λ > 0 is the regularization parameter. By introducing the corresponding kernel function
K(x, z) =
Rd, the output estimate of the kernel ridge regression for
φ(x), φ(z)
i
h
any data x

∈
Rd can be denoted as f ∗(x) = K(x, X)⊤α, where α

Rn is the solution to

for any data x, z

∈

∈

(K + λIn)α = Y.

Here K

∈

Rn

n is the kernel matrix with Ki,j = K(xi, xj),

×

i, j

∀

∈

[n]

×

[n].

1Strictly speaking, the optimization problem should be considered in a hypothesis space deﬁned by the reproducing
kernel Hilbert space associated with the feature/kernel. Here, we use the notation in ﬁnite dimensional space for
simplicity.

3

6
Note a direct computation involves (K +λIn)−

1, whose O(n3) running time can be fairly large in
tasks like neural network due to the large number of training data. Therefore, we hope to construct
feature map φ : Rd
Rs, such that the new feature approximates the kernel matrix well in the
sense of

→

(1

ǫ)

(K + λIn)

−
(0, 1) is small and Φ = [φ(x1),

·

(cid:22)

ΦΦ⊤ + λIn (cid:22)
Rn
, φ(xn)]⊤ ∈

where ǫ
we can approximate the solution by u∗(z) = φ(z)⊤(Φ⊤Φ + λIs)−
O(ns2 + n2) time. In the case s = o(n), computational cost can be saved.

· · ·

∈

×

s. Then by Woodbury matrix equality,
1Φ⊤Y , which can be computed in

In this work, we consider a generalized setting of [AKM+17] as a kernel ridge regression problem

(1 + ǫ)

(K + λIn),

·

(2)

with positive deﬁnite kernel matrix K : Rd

×

Rd

R of the form

→
[φ(x, w)⊤φ(z, w)],
p

K(x, z) = E
w
∼

(3)

R

≥

→

0 denotes a

where φ : Rd
probability density function.

Rd1

→

×

Rd2 denotes a ﬁnite dimensional vector and p : Rd1

Due to the regularization λ > 0 in this setting, instead of constructing the feature map directly

from the distribution q, we consider the following ridge leveraged distribution:

Deﬁnition 3.1 (Ridge leverage function). Given data x1,
deﬁne the ridge leverage function as

, xn ∈

· · ·

Rd and parameter λ > 0, we

qλ(w) = p(w)

·

tr[Φ(w)⊤(K + λIn)−

1Φ(w)],

), φ are deﬁned in Eq. (3), and Φ(w) = [φ(x1, w)⊤,
where p(
·
deﬁne statistical dimension sλ(K) as

, φ(xn, w)⊤]⊤ ∈

· · ·

Rn

×

d2. Further, we

sλ(K) =

qλ(w)dw = tr[(K + λIn)−

1K].

(4)

The leverage score sampling distribution qλ/sλ(K) takes the regularization term into consider-

Z

ation and achieves Eq. (2) using the following modiﬁed random features vector:

Deﬁnition 3.2 (Modiﬁed random features). Given any probability density function q(
) whose
·
Rd1, we deﬁne modiﬁed random
support includes that of p(
). Given m vectors w1,
·
Rn
md2 as Ψ := [ϕ(x1),
features Ψ

· · ·
, ϕ(xn)]⊤, where

, wm ∈

×

∈

· · ·

ϕ(x) =

1
√m " p
p

p(w1)
q(w1)

φ(x, w1)⊤,

,

· · ·

p(wm)
q(wm)

p

φ(x, wm)⊤

⊤

.

#

Now we are ready to present our result.

p

(0,

K
k

qλ : Rd1
Rd1. Assume seqλ =

→
Rd1
(0, 1/2) and failure probability δ

Theorem 3.3 (Kernel approximation with leverage score sampling, generalization of Lemma 8 in
). Let qλ : Rd1
[AKM+17]). Given parameter λ
0 be the leverage score deﬁned in
k
Deﬁnition 3.1. Let
qλ(w) holds for
≥
qλ(w)/seqλ . Given any accuracy
all w
∈
Rd denote m samples
parameter ǫ
, wm ∈
e
draw independently from the distribution associated with the density qλ(
), and construct the modiﬁed
e
·
md2 as in Deﬁnition 3.2 with q = qλ. Let sλ(K) be the statistical dimension
random features Ψ
∈
2seqλ ln(16seqλ ·
deﬁned in (4). If m
(K + λIn)

∈
R be any measurable function such that
qλ(w)dw is ﬁnite. Let qλ(w) =
(0, 1). Let w1,

sλ(K)/δ), then we have

(K + λIn)

×
3ǫ−

qλ(w)

(1 + ǫ)

· · ·
e

ΨΨ

Rn

(5)

→

≥

(1

R

∈

∈

ǫ)

e

R

≥

⊤

(cid:22)

+ λIn (cid:22)

·

·
holds with probability at least 1

−

δ.

−

4

Remark 3.4. Above results can be generalized to the complex domain C. Note for the random
C and
Fourier feature case discussed in [AKM+17], we have d1 = d, d2 = 1, φ(x, w) = e−
) denotes the Fourier transform density distribution, which is a special case in our setting.
p(
·

2πiw⊤x

∈

3.2 Application in training regularized neural network

In this section, we consider the application of leverage score sampling in training ℓ2 regularized
neural networks.

Past literature such as [DZPS19],[ADH+19a] have already witnessed the equivalence between
training a neural network and solving a kernel regression problem in a broad class of network models.
In this work, we ﬁrst generalize this result to the regularization case, where we connect regularized
neural network with kernel ridge regression. Then we apply the above discussed the leverage score
sampling theory for KRR to the task of training neural nets.

3.2.1 Equivalence I, training with random Gaussian initialization

To illustrate the idea, we consider a simple model two layer neural network with ReLU activation
function as in [DZPS19, SY19]2.

fnn(W, a, x) =

1
√m

m

r=1
X

arσ(w⊤r x)

R,

∈

where x
[w1,
×
ReLU activation function: σ(z) = max

Rd is the input, wr ∈
R, r
m, ar ∈
∈
∈

∈
, wm]

· · ·

Rd

Rd, r
[m] is the output weight, a = [a1,

[m] is the weight vector of the ﬁrst layer, W =
) is the
, am]⊤ and σ(
·

· · ·

∈

Here we consider only training the ﬁrst layer W with ﬁxed a, so we also write fnn(W, x) =
d and labels Y =
Rn. We formally

fnn(W, a, x). Again, given training data matrix X = [x1,
[y1,
deﬁne training neural network with ℓ2 regularization as follows:

Rn, we denote fnn(W, X) = [fnn(W, x1),

Rn
, xn]⊤ ∈
· · ·
, fnn(W, xn)]⊤ ∈

, yn]⊤ ∈

· · ·

· · ·

×

0, z
{

.
}

Deﬁnition 3.5 (Training neural network with regularization). Let κ
Let λ
∈
wr(0) i.i.d.
descent:

(0, 1] be a small multiplier3.
] and
1, 1
}
(0, Id). Then we consider solving the following optimization problem using gradient

(0, 1) be the regularization parameter. We initialize the network as ar

i.i.d.
∼

∼ N

unif[

{−

∈

min
W

1
2 k

Y

−

κfnn(W, X)

k2 +

1
2

λ

W
k

2
F .
k

(6)

Let wr(t), r
iteration t as unn(t) = κfnn(W (t), X)
unn,test(t) = κfnn(W (t), xtest)

[m] be the network weight at iteration t. We denote the training data predictor at
Rd, we denote

Rn. Further, given any test data xtest ∈

R as the test data predictor at iteration t.

∈

∈

∈

On the other hand, we consider the following neural tangent kernel ridge regression problem:

min
β

1
2 k

Y

2
2 +
κfntk(β, X)
k

−

1
2

λ

β
k

2
2,
k

(7)

2Our results directly extends to multi-layer deep neural networks with all layers trained together
3To establish the training equivalence result, we assign κ = 1 back to the normal case. For the training equivalence
result, we pick κ > 0 to be a small multiplier only to shrink the initial output of the neural network. The is the same
as what is used in [AKM+17].

5

where κ, λ are the same parameters as in Eq. (6), fntk(β, x) = Φ(x)⊤β
[fntk(β, x1),
, fntk(β, xn)]⊤ ∈
sponding to the neural tangent kernel (NTK):

R and fntk(β, X) =
Rn are the test data predictors. Here, Φ is the feature map corre-

· · ·

∈

Kntk(x, z) = E

∂fnn(W, x)
∂W

,

∂fnn(W, z)
∂W

(8)

(cid:20)(cid:28)
(cid:29)(cid:21)
Rd are any input data, and the expectation is taken over wr

∈

where x, z

, m.
Under the standard assumption Kntk being positive deﬁnite, the problem Eq. (7) is a strongly
1Y for training
1Y for the test data

convex optimization problem with the optimal predictor u∗ = κ2H cts(κ2H cts + λI)−
data, and the corresponding predictor u∗test = κ2Kntk(xtest, X)⊤(κ2H cts + λI)−
xtest, where H cts

n is the kernel matrix with [H cts]i,j = Kntk(xi, xj).

(0, I), r = 1,

Rn

· · ·

×

i.i.d.
∼ N

We connect the problem Eq. (6) and Eq. (7) by building the following equivalence between their

∈

training and test predictors with polynomial widths:

Theorem 3.6 (Equivalence between training neural net with regularization and kernel ridge re-
gression for training data prediction). Given any accuracy ǫ
(0, 1/10) and failure probability
O( n4d
(0, 1/10). Let multiplier κ = 1, number of iterations T =
δ
0ǫ )
Λ4
δ over the Gaussian

), network width m

√m ). Then with probability at least 1

∈
O( 1
Λ0

O( 1

≥

∈

e

−

e

and the regularization parameter λ
random initialization, we have

≤

e

unn(T )
k

−

u∗

k2 ≤

ǫ.

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

We can further show the equivalence between the test data predictors with the help of the

e
multiplier κ.

Theorem 3.7 (Equivalence between training neural net with regularization and kernel ridge regres-
sion for test data prediction). Given any accuracy ǫ
(0, 1/10).
Let multiplier κ =
) and

(0, 1/10) and failure probability δ
O(

∈
n ), number of iterations T =

), network width m

O( ǫΛ0

1
κ2Λ0

∈
O( n10d
ǫ6Λ10
0

regularization parameter λ
e
initialization, we have

≤

O( 1

√m ). Then with probability at least 1

e

−

≥
δ over the Gaussian random

e

e

unn,test(T )
k

−

u∗testk2 ≤

ǫ.

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

3.2.2 Equivalence II, training with leverage scores

e

To apply the leverage score theory discussed in Section 3.1, Note the deﬁnition of the neural tangent
kernel is exactly of the form:

Kntk(x, z) = E

∂fnn(W, x)
∂W

,

∂fnn(W, z)
∂W

(cid:20)(cid:28)

= E
w
∼

(cid:29)(cid:21)

[φ(x, w)⊤φ(z, w)]
p

Rd and p(
) denotes the probability density function of standard
where φ(x, w) = xσ′(w⊤x)
·
Gaussian distribution
(0, Id). Therefore, we try to connect the theory of training regularized
neural network with leverage score sampling. Note the width of the network corresponds to the
size of the feature vector in approximating the kernel. Thus, the smaller feature size given by the

N

∈

6

leverage score sampling theory helps us build a smaller upper-bound on the width of the neural
nets.

Speciﬁcally, given regularization parameter λ > 0, we can deﬁne the ridge leverage function with

respect to neural tangent kernel H cts deﬁned in Deﬁnition 3.1 as

qλ(w) = p(w) tr[Φ(w)⊤(H cts + λIn)−

1Φ(w)]

and corresponding probability density function

q(w) =

qλ(w)
sλ(H cts)

(9)

where Φ(w) = [φ(x1, w)⊤,

d2.
We consider training the following reweighed neural network using leverage score initialization:

, φ(xn, w)⊤]⊤ ∈

· · ·

×

Rn

Deﬁnition 3.8 (Training reweighed neural network with regularization). Let κ
) : Rd
(0, 1) be the regularization parameter. Let q(
multiplier. Let λ
·
denotes the probability density function of Gaussian distribution
] and wr(0) i.i.d.
1, 1
as ar
∼
}
problem using gradient descent:

(0, 1] be a small
R>0 deﬁned in (9). Let p(
)
·
(0, Id). We initialize the network
q. Then we consider solving the following optimization

i.i.d.
∼

unif[

{−

→

N

∈

∈

min
W

1
2 k

Y

−

κf nn(W, X)

k2 +

1
2

λ

W
k

2
F .
k

(10)

where

f nn(W, x) =

1
√m

m

r=1
X

arσ(w⊤r X)

s

p(wr(0))
q(wr(0))

and f nn(W, X) = [f nn(W, x1),

, f nn(W, xn)]⊤.

· · ·

We denote wr(t), r

as the training data predictor at iteration t. Given any test data xtest ∈
κf nn(W (t), xtest) as the test data predictor at iteration t.

∈

[m] as the estimate weight at iteration t. We denote unn(t) = κf nn(W (t), X)
Rd, we denote unn,test(t) =

We show that training this reweighed neural net with leverage score initialization is still equiv-

alence to the neural tangent kernel ridge regression problem (7) as in following theorem:

Theorem 3.9 (Equivalence between training reweighed neural net with regularization and kernel
ridge regression for training data prediction). Given any accuracy ǫ
(0, 1) and failure probability
∈
Λ0 log( 1
(0, 1/10). Let multiplier κ = 1, number of iterations T = O( 1
ǫ )), network width m =
δ
∈
poly( 1
√m ). Then with probability at least
Λ0
1

δ )) and regularization parameter λ =
δ over the random leverage score initialization, we have

ǫ , log( 1

, n, d, 1

O( 1

−

e

unn(T )
k

−

u∗

k2 ≤

ǫ.

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

e

4 Overview of techniques

Generalization of leverage score theory To prove Theorem 3.3, we follow the similar proof
framework as Lemma 8 in [AKM+17].

7

Let K + λIn = V ⊤Σ2V be an eigenvalue decomposition of K + λIn. Then conclusion (5) is

equivalent to

1V ΨΨ⊤V ⊤Σ−

1

Σ−
k

−

Σ−

1V KV ⊤Σ−

1

ǫ

k ≤

(11)

Let random matrix Yr ∈

Rn

×

n deﬁned as

Yr :=

p(wr)
qλ(wr)

Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1.

where Φ(w) = [φ(x1, w),

· · ·

Rn

×

d2. Then we have

, φ(xn, w)]⊤ ∈
p(wr)
qλ(wr)

Σ−

[Yl] = E

qλ (cid:20)

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1

= Σ−

1V KV ⊤Σ−

1,

(cid:21)

p(wr)
qλ(wr)

Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1 = Σ−

1V ΨΨ⊤V ⊤Σ−

1.

E
qλ

m

and

1
m

Yr =

m

1
m

r=1
X
Thus, it suﬃces to show that

r=1
X

1
m

m

r=1
X

Yr −

E
qλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

[Yl]

ǫ

≤

(12)

holds with probability at least 1
Note

−

δ, which can be shown by applying matrix concentration results.

Ylk ≤
k

sqλ and E
qλ

[Y 2
r ]

seqλ ·

(cid:22)

diag

λ1/(λ1 + λ),
{

· · ·

.
, λn/(λn + λ)
}

Applying matrix concentration Lemma 7 in [AKM+17], we complete the proof.

Equivalence between regularized neural network and kernel ridge regression To estab-
lish the equivalence between training neural network with regularization and neural tangent kernel
ridge regression, the key observation is that the dynamic kernel during the training is always close
to the neural tangent kernel.

Speciﬁcally, given training data x1,
n along training process as
×

Rn

, xn ∈

· · ·

Rd, we deﬁne the dynamic kernel matrix H(t)

∈

[H(t)]i,j =

dfnn(W (t), xi)
dW (t)

,

dfnn(W (t), xj)
dW (t)

(cid:28)
Then we can show the gradient ﬂow of training regularized neural net satisﬁes

(cid:29)

d

2
unn(t)
u∗ −
2
k
k
dt

=

2(u∗

−

−
+ 2(unn(t)

unn(t))⊤(H(t) + λI)(u∗

unn(t))

−

(13)

u∗)⊤(H(t)

H cts)(Y

u∗)

(14)

−
where term (13) is the primary term characterizing the linear convergence of unn(t) to t∗, and
term (14) is the additive term that can be well controlled if H(t) is suﬃciently close to H cts. We
argue the closeness of H(t)

H cts as the consequence of the following two observations:

−

−

≈

8

•

•

Initialization phase: At the beginning of the training, H(0) can be viewed as approximating
the neural tangent kernel H cts using ﬁnite dimensional random features. Note the size of these
random features corresponds to the width of the neural network (scale by the data dimension
d). Therefore, when the neural network is suﬃciently wide, it is equivalent to approximate the
neural tangent kernel using suﬃcient high dimensional feature vectors, which ensures H(0) is
suﬃciently close to H cts.

In the case of leverage score initialization, we further take the regularization into consideration.
We use the tool of leverage score to modify the initialization distribution and corresponding
network parameter, to give a smaller upper-bound of the width of the nets needed.

Training phase: If the net is suﬃciently wide, we can observe the over-parametrization phe-
nomenon such that the weight estimate W (t) at time t will be suﬃciently close to its initial-
ization W (0), which implies the dynamic kernel H(t) being suﬃciently close to H(0). Due
H cts throughout the
to the fact H(0)
algorithm.

H cts argued in initialization phase, we have H(t)

≈

≈

Combining both observations, we are able to iteratively show the (nearly) linear convergence

property of training the regularized neural net as in following lemma:

Lemma 4.1 (Bounding kernel perturbation, informal). For any accuracy ∆
work width m = poly(1/∆, 1/T, 1/ǫtrain, n, d, 1/κ, 1/Λ0, log(1/δ)) and λ = O( 1
1
0

(0, 1/10). If the net-
∈
√m ), with probability
(0, ∆) that are independent of t, such that the following hold for all

δ, there exist ǫW , ǫ′H, ǫ′K ∈
t

T :

−
≤
1.

2.

≤
wr(0)
k
H(0)
k
unn(t)
k

wr(t)

k2 ≤

ǫW ,

r

∀

∈

[m]

−

H(t)

−

ǫ′H

k2 ≤
2
2 ≤

(κ2Λ0+λ)t/2

ǫ2
train, e−
{

−

3.

max

u∗k
1
Given arbitrary accuracy ǫ
) and m suﬃciently
κ2Λ0
ǫ, indicating the equivalence between training
u∗k2 ≤
large in Lemma D.14, then we have
neural network with regularization and neural tangent kernel ridge regression for the training data
predictions.

(0, 1), if we choose ǫtrain = ǫ, T =

unn(0)
k

unn(t)
k

u∗k

O(

−

−

2
2}

∈

e

To further argue the equivalence for any given test data xtest, we observe the similarity be-
tween the gradient ﬂows of neural tangent kernel ridge regression untk,test(t) and regularized neural
networks unn,test(t) as following:

duntk,test(t)
dt
dunn,test(t)
dt

= κ2Kntk(xtest, X)⊤(Y

untk(t))

λ

·

−

−

untk,test(t).

= κ2Kt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

unn,test(t).

(15)

(16)

By choosing the multiplier κ > 0 small enough, we can bound the initial diﬀerence between these
two predictors. Combining with above similarity between gradient ﬂows, we are able to show
ǫ/2 for appropriate T > 0. Finally, note the linear convergence property
unn,test(T )
|
of the gradient of the kernel ridge regression, we can prove

untk,test(T )

| ≥

−

ǫ.

Using the similar idea, we can also show the equivalence for test data predictors and the case
of leverage score initialization. We refer to the Appendix for a detailed proof sketch and rigorous
proof.

unn,test(T )
|

−

u∗ntk,test| ≥

9

Remark 4.2. Our results can be naturally extended to multi-layer ReLU deep neural networks with
all parameters training together. Note the core of the connection between regularized NNs and KRR
is to show the similarity between their gradient ﬂows, as in Eq. (15), (16). The gradient ﬂows consist
of two terms: the ﬁrst term is from normal NN training without regularizer, whose similarity has been
shown in broader settings, e.g. [DZPS19, SY19, ADH+19a, AZLS19b, AZLS19a]; the second term
is from the ℓ2 regularizer, whose similarity is true for multi-layer ReLU DNNs if the regularization
parameter is divided by the number of layers of parameters trained, due to the piecewise linearity of
the output with respect to the training parameters.

5 Conclusion

In this paper, we generalize the leverage score sampling theory for kernel approximation. We
discuss the interesting application of connecting leverage score sampling and training regularized
neural networks. We present two theoretical results: 1) the equivalence between the regularized
neural nets and kernel ridge regression problems under the classical random Gaussian initialization
for both training and test predictors; 2) the new equivalence under the leverage score initialization.
We believe this work can be the starting point of future study on the use of leverage score sampling
in neural network training.

Roadmap In the appendix, we present our complete results and rigorous proofs. Section A
presents some well-known mathematically results that will be used in our proof. Section B discusses
our ﬁrst equivalence result between training regularized neural network and kernel ridge regression.
Section C discusses our generalization result of the leverage score sampling theory. Section D
discusses our second equivalence result under leverage score initialization and potential beneﬁts
compared to the Gaussian initialization. Section E discusses how to extend our results to a broader
class of neural network models.

10

Appendix

A Preliminaries

A.1 Probability tools

In this section we introduce the probability tools we use in the proof.

We state Chernoﬀ, Hoeﬀding and Bernstein inequalities.

Lemma A.1 (Chernoﬀ bound [Che52]). Let X =
Xi = 0 with probability 1
1. Pr[X
2. Pr[X

pi, and all Xi are independent. Let µ = E[X] =
P
δ2µ/3),
δ2µ/2),

δ > 0 ;
0 < δ < 1.

(1 + δ)µ]
δ)µ]
(1

−
exp(
exp(

≥
≤

−

≤
≤

−
−

∀
∀

P

n
i=1 Xi, where Xi = 1 with probability pi and

n
i=1 pi. Then

Lemma A.2 (Hoeﬀding bound [Hoe63]). Let X1,
in [ai, bi]. Let X =

n
i=1 Xi, then we have

· · ·

, Xn denote n independent bounded variables

P

X
Pr[
|

−

E[X]

t]

2 exp

| ≥

≤

−

(cid:18)
Lemma A.3 (Bernstein inequality [Ber24]). Let X1,
Xi| ≤
variables. Suppose that
|

P
· · ·
M almost surely, for all i. Then, for all positive t,

(cid:19)

n

2t2
i=1(bi −
, Xn be independent zero-mean random

ai)2

.

Pr

n

"
Xi=1

Xi > t

exp

# ≤

 −

n
j=1

t2/2
E[X 2

j ] + M t/3 !

.

We state three inequalities for Gaussian random variables.

P

Lemma A.4 (Anti-concentration of Gaussian distribution). Let X
bility density function of X is given by φ(x) = 1

x2
2σ2 . Then

√2πσ2 e−

N (0, σ2), that is, the proba-

∼

X
Pr[
|

| ≤

t]

∈

Lemma A.5 (Gaussian tail bounds). Let X
µ and variance σ2. Then for all t

0, we have

∼ N

≥

2
3

t
σ

,

4
5

t
σ

.

(cid:19)

(cid:18)
(µ, σ2) be a Gaussian random variable with mean

X
Pr[
|

−

µ

| ≥

t]

≤

−t2
2σ2 .

2e−

2
Lemma A.6 (Lemma 1 on page 1325 of Laurent and Massart [LM00]). Let X
k be a chi-squared
distributed random variable with k degrees of freedom. Each one has zero mean and σ2 variance.
Then

∼ X

Pr[X

kσ2
−
≥
Pr[kσ2

(2√kt + 2t)σ2]
2√ktσ2]

X

−

≥

exp(

−

exp(

−

t),

t).

≤

≤

We state two inequalities for random matrices.

11

Lemma A.7 (Matrix Bernstein, Theorem 6.1.1 in [Tro15]). Consider a ﬁnite sequence
Rn1

n2 of independent, random matrices with common dimension n1 ×
[m].
i

E[Xi] = 0,

and

[m]

M,

n2. Assume that

×

X1,
{

i

∈
∈
m
i=1 Xi. Let Var[Z] be the matrix variance statistic of sum:

∀

∀

Xik ≤
k

, Xm} ⊂

· · ·

Let Z =

P

Then

Furthermore, for all t

E[

Z

k
k
0,

≥

Var[Z] = max

m

,

E[XiX ⊤i ]
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Xi=1

(

(cid:13)
(cid:13)
(cid:13)

m

Xi=1

.

)

E[X ⊤i Xi]
(cid:13)
(cid:13)
(cid:13)

(2Var[Z]

]

≤

·

log(n1 + n2))

1/2

+ M

·

log(n1 + n2)/3.

Z
Pr[
k

k ≥

t]

≤

(n1 + n2)

exp

·

−

(cid:18)

t2/2
Var[Z] + M t/3

.

(cid:19)
Rd1

d2 be a ﬁxed matrix.

×

Lemma A.8 (Matrix Bernstein, Lemma 7 in [AKM+17]). Let B
Rd1
Construct a random matrix R

d2 satisﬁes

×

∈

∈

Let M1 and M2 be semideﬁnite upper bounds for the expected squares:

E[R] = B and

R
k

k ≤

L.

E[RR⊤]

(cid:22)

M1 and E[R⊤R]

M2.

(cid:22)

Deﬁne the quantities

Form the matrix sampling estimator

m = max

,
M1k

{k

M2k}
k

and d = (tr[M1] + tr[M2])/m.

Rn =

1
n

n

Xk=1

Rk

where each Rk is an independent copy of R. Then, for all t

m/n + 2L/3n,

≥

(cid:16)
A.2 Neural tangent kernel and its properties

Pr[
Rn −
k

B

k2 ≥

t]

≤

4d exp

nt2
p
m + 2Lt/3

−

(cid:17)

Lemma A.9 (Lemma 4.1 in [SY19]). We deﬁne H cts, H dis

Rn

×

n as follows

∈

H cts

i,j =

H dis

i,j =

E

w

∼N
m
1
m

r=1
X

[x⊤i xj1

w⊤xi

0,w⊤xj

≥

0],

≥

(0,I)

[x⊤i xj1

w⊤

r xi

0,w⊤

r xj

≥

0].

≥

Let λ = λmin(H cts). If m = Ω(λ−

2n2 log(n/δ)), we have

H dis
k
hold with probability at least 1

−
δ.

−

H cts

kF ≤

λ
4

, and λmin(H dis)

3
4

λ

≥

12

Lemma A.10 (Lemma 4.2 in [SY19]). Let R

(0, I). For any set of weight vectors w1,

N
then the H : Rm
×

d

Rn

n deﬁned

×

→

∈
, wm ∈

· · ·

w1,
(0, 1). If
Rd that satisfy for any r

wm are i.i.d. generated from
R,

[m],

· · ·

,

∈

wr −
k

wrk2 ≤

e

H(W ) =

1
m

x⊤i xj

m

r=1
X

1

w⊤

r xi

e

e

0,w⊤

r xj

≥

0.

≥

Then we have

holds with probability at least 1

n2

−

·

H(w)
k
exp(

H(

w)

−
mR/10).

−

e

kF < 2nR

B Equivalence between suﬃciently wide neural net and kernel ridge

regression

In this section, we extend the equivalence result in [JGH18, ADH+19a] to the case with regular-
ization term, where they showed the equivalence between a fully-trained inﬁnitely wide/suﬃciently
wide neural net and the kernel regression solution using the neural tangent kernel (NTK). Speciﬁ-
cally, we prove Theorem 3.6 and Theorem 3.7 in this section.

Section B.1 introduces key notations and standard data assumptions. Section B.2 restates and
supplements the deﬁnitions introduced in the paper. Section B.3 presents several key lemmas about
the gradient ﬂow and linear convergence of neural network and kernel ridge regression predictors,
which are crucial to the ﬁnal proof. Section B.4 provides a brief proof sketch. Section B.5 restates the
main equivalence Theorem 3.7 and provides a complete proof following the proof sketch. Section B.6
restates and proves Theorem 3.6 by showing it as a by-product of previous proof.

B.1 Preliminaries

Let’s deﬁne the following notations:

X

Rn

×

d be the training data

Rd be the test data

∈
xtest ∈
untk(t) = κfntk(β(t), X) = κΦ(X)β(t)
for the training data at time t. (See Deﬁnition B.5)

∈

Rn be the prediction of the kernel ridge regression

u∗ = limt

→∞

untk(t) (See Eq. (24))

untk,test(t) = κfntk(β(t), xtest) = κΦ(xtest)β(t)
regression for the test data at time t. (See Deﬁnition B.5)

∈

R be the prediction of the kernel ridge

u∗test = limt

untk,test(t) (See Eq. (25))

→∞
Kntk(x, y) = E[
D
Rn be the induced kernel between the training data and test data at time t,

∂W , ∂fnn(W,y)

] (See Deﬁnition B.5)

∂fnn(W,x)

∂W

E

Kt(xtest, X)
where

∈

•

•

•

•

•

•

•

•

[Kt(xtest, X)]i = Kt(xtest, xi) =

(see Deﬁnition B.6)

(cid:28)

13

∂f (W (t), xtest)
∂W (t)

,

∂f (W (t), xi)
∂W (t)

(cid:29)

unn(t) = κfnn(W (t), X)
time t. (See Deﬁnition B.3)

∈

Rn be the prediction of the neural network for the training data at

unn,test(t) = κfnn(W (t), xtest)
time t (See Deﬁnition B.3)

∈

R be the prediction of the neural network for the test data at

•

•

Assumption B.1 (data assumption). We made the following assumptions:
1. For each i
= O(1).
yi|
|
2. H cts is positive deﬁnite, i.e., Λ0 := λmin(H cts) > 0.
3. All the training data and test data have Euclidean norm equal to 1.

[n], we assume

∈

B.2 Deﬁnitions

To establish the equivalence between neural network and kernel ridge regression, we prove the
similarity of their gradient ﬂow and initial predictor. Note kernel ridge regression starts at 0 as
initialization, so we hope the initialization of neural network also close to zero. Therefore, using the
same technique in [ADH+19a], we apply a small multiplier κ > 0 to both predictors to bound the
diﬀerent of initialization.

Deﬁnition B.2 (Neural network function). We deﬁne a two layer neural networks with rectiﬁed
linear unit (ReLU) activation as the following form

fnn(W, a, x) =

1
√m

m

r=1
X

arσ(w⊤r x)

R,

∈

Rd

· · ·

∈
, wm]

Rd is the input, wr ∈
[m] is the weight vector of the ﬁrst layer, W =
where x
R, r
m, ar ∈
) is
, am]⊤ and σ(
[w1,
·
∈
In this paper, we consider only training the
.
the ReLU activation function: σ(z) = max
}
ﬁrst layer W while ﬁx a. So we also write fnn(W, x) = fnn(W, a, x). We denote fnn(W, X) =
[fnn(W, x1),

[m] is the output weight, a = [a1,

0, z
{

Rd, r

Rn.

· · ·

∈

∈

×

, fnn(W, xn)]⊤ ∈

· · ·

Deﬁnition B.3 (Training neural network with regularization, restatement of Deﬁnition 3.5). Given
Rn. Let fnn be deﬁned as in
d and corresponding label vector Y
training data matrix X
Deﬁnition B.2. Let κ
(0, 1) be the regularization parameter.
(0, Id). Then we consider solving
We initialize the network as ar
the following optimization problem using gradient descent:

∈
(0, 1) be a small multiplier. Let λ

] and wr(0) i.i.d.
1, 1
}

i.i.d.
∼

∼ N

unif[

{−

Rn

∈

∈

∈

×

min
W

1
2 k

Y

−

κfnn(W, X)

k2 +

1
2

λ

W
k

2
F .
k

We denote wr(t), r

∈

[m] as the variable at iteration t. We denote

unn(t) = κfnn(W (t), X) =

κ
√m

m

r=1
X

arσ(wr(t)⊤X)

Rn

∈

as the training data predictor at iteration t. Given any test data xtest ∈

Rd, we denote

unn,test(t) = κfnn(W (t), xtest) =

κ
√m

m

Xr=1

arσ(wr(t)⊤xtest)

R

∈

as the test data predictor at iteration t.

(17)

(18)

(19)

14

Deﬁnition B.4 (Neural tangent kernel and feature function). We deﬁne the neural tangent ker-
nel(NTK) and the feature function corresponding to the neural networks fnn deﬁned in Deﬁnition B.2
as following

Kntk(x, z) = E

∂fnn(W, x)
∂W

,

∂fnn(W, z)
∂W

(cid:29)(cid:21)
(cid:20)(cid:28)
Rd are any input data, and the expectation is taking over wr

, xn]⊤ ∈

· · ·

Rn

×

d, we deﬁne H cts

∈

where x, z
Given training data matrix X = [x1,
between training data as

∈

i.i.d.
∼ N
Rn
×

(0, I), r = 1,

, m.
n as the kernel matrix

· · ·

[H cts]i,j = Kntk(xi, xj)

R.

∈

We denote the smallest eigenvalue of H cts as Λ0 > 0, where we assume H cts is positive deﬁnite.
Rn
Further, given any data z
as

Rd, we write the kernel between test and training data Kntk(z, X)

∈

∈

Kntk(z, X) = [Kntk(z, x1),

, Kntk(z, xn)]⊤

Rn.

∈

· · ·

We denote the feature function corresponding to the kernel Kntk as we deﬁned above as Φ : Rd
which satisﬁes

,

→ F

for any data x, z

∈

Φ(x), Φ(z)
h
Rd. And we write Φ(X) = [Φ(x1),

iF

= Kntk(x, z),

, Φ(xn)]⊤.

· · ·

×

Rn

Deﬁnition B.5 (Neural tangent kernel ridge regression). Given training data matrix X = [x1,

d and corresponding label vector Y

∈
kernel and corresponding feature functions deﬁned as in Deﬁnition B.4. Let κ
multiplier. Let λ
∈
tangent kernel ridge regression problem:

n and Φ be the neural tangent
(0, 1) be a small
(0, 1) be the regularization parameter. Then we consider the following neural

Rn. Let Kntk, H cts

Rn

· · ·

∈

∈

∈

×

, xn]⊤

where fntk(β, x) = Φ(x)⊤β
[fntk(β, x1),
ization β(0) = 0. We denote β(t) as the variable at iteration t. We denote

, fntk(β, xn)]⊤ ∈

· · ·

∈

Y

−

min
β

2
2 +
κfntk(β, X)
k

2
2.
k
R denotes the prediction function is corresponding RKHS and fntk(β, X) =
Rn. Consider the gradient ﬂow of solving problem (20) with initial-

β
k

(20)

λ

1
2 k

1
2

untk(t) = κΦ(X)β(t)

Rn

∈

as the training data predictor at iteration t. Given any test data xtest ∈

untk,test(t) = κΦ(xtest)⊤β(t)

∈

R

Rd, we denote

(21)

(22)

as the test data predictor at iteration t. Note the gradient ﬂow converge the to optimal solution of
problem (20) due to the strongly convexity of the problem. We denote

β∗ = lim
t
→∞

β(t) = κ(κ2Φ(X)⊤Φ(X) + λI)−

1Φ(X)⊤Y

and the optimal training data predictor

u∗ = lim
t
→∞

untk(t) = κΦ(X)β∗ = κ2H cts(κ2H cts + λI)−

1Y

Rn

∈

and the optimal test data predictor

u∗test = lim
t
→∞

untk,test(t) = κΦ(xtest)⊤β∗ = κ2Kntk(xtest, X)⊤(κ2H cts + λI)−

1Y

R.

∈

(23)

(24)

(25)

15

Deﬁnition B.6 (Dynamic kernel). Given W (t)
training time t as deﬁned in Deﬁnition B.3. For any data x, z

m as the parameters of the neural network at
Rd, we deﬁne Kt(x, z)

R as

∈

×

Rd

∈

Given training data matrix X = [x1,

Rn

×

d, we deﬁne H (t)

Kt(x, z) =

dfnn(W (t), x)
dW (t)

,

∈
dfnn(W (t), z)
dW (t)

(cid:28)
, xn]⊤ ∈

· · ·
[H(t)]i,j = Kt(xi, xj)

∈
Rd, we deﬁne Kt(xtest, X)

R.

Rn as

∈
, Kt(xtest, xn)]⊤

Kt(xtest, X) = [Kt(xtest, x1),

· · ·

(cid:29)

∈

Rn

n as

×

Rn.

∈

Further, given a test data xtest ∈

B.3 Gradient, gradient ﬂow, and linear convergence

Lemma B.7 (Gradient ﬂow of kernel ridge regression). Given training data matrix X
corresponding label vector Y
and untk(t)
Deﬁnition B.4. Then for any data z

Rn
Rn. Let fntk be deﬁned as in Deﬁnition B.5. Let β(t), κ

Rn be deﬁned as in Deﬁnition B.5. Let Kntk : Rd

d and
∈
(0, 1)
Rn be deﬁned as in

Rd, we have

Rn

→

×

∈

∈

∈

×

×

d

∈

dfntk(β(t), z)
dt

= κ

·

Kntk(z, X)⊤(Y

untk(t))

λ

·

−

−

fntk(β(t), z).

Proof. Denote L(t) = 1

2
2. By the rule of gradient descent, we have
β(t)
k
k

Y
2 k

2 + 1
2
2 λ
untk(t)
k
dL
dβ

−
dβ(t)
dt

=

−

= κΦ(X)⊤(Y

untk(t))

λβ(t),

−

−

where Φ is deﬁned in Deﬁnition B.4. Thus we have

dfntk(β(t), z)
dt

=

dfntk(β(t), z)
dβ(t)

dβ(t)
dt

= Φ(z)⊤(κΦ(X)⊤(Y
= κKntk(z, X)⊤(Y
= κKntk(z, X)⊤(Y

−

−

untk(t))

−
untk(t))

untk(t))

−

−

λβ(t))

−
λΦ(z)⊤β(t)

λfntk(β(t), z),

where the ﬁrst step is due to chain rule, the second step follows from the fact dfntk(β, z)/dβ = Φ(z),
the third step is due to the deﬁnition of the kernel Kntk(z, X) = Φ(X)Φ(z)
Rn, and the last step
is due to the deﬁnition of fntk(β(t), z)

R.

∈

∈

Corollary B.8 (Gradient of prediction of kernel ridge regression). Given training data matrix
Rd.
Rn. Given a test data xtest ∈
X = [x1,
Rn be deﬁned as in
Let fntk be deﬁned as in Deﬁnition B.5. Let β(t), κ
∈
∈
Deﬁnition B.5. Let Kntk : Rd
Rn
n be deﬁned as in Deﬁnition B.4. Then we
×
have

d and corresponding label vector Y

(0, 1) and untk(t)

, xn]⊤ ∈

Rn, H cts

Rn

Rn

· · ·

→

×

∈

∈

×

×

d

duntk(t)
dt

duntk,test(t)
dt

= κ2H cts(Y

untk(t))

λ

·

−

untk(t)

−

= κ2Kntk(xtest, X)⊤(Y

untk(t))

λ

·

−

−

untk,test(t).

16

Proof. Plugging in z = xi ∈

Rd in Lemma B.7, we have

dfntk(β(t), xi)
dt

= κKntk(xi, X)⊤(Y

untk(t))

λ

·

−

−

fntk(β(t), xi).

Note [untk(t)]i = κfntk(β(t), xi) and [H cts]:,i = Kntk(xi, X), so writing all the data in a compact
form, we have

duntk(t)
dt

= κ2H cts(Y

untk(t))

λ

·

−

untk(t).

−

Plugging in data z = xtest ∈

Rd in Lemma B.7, we have

dfntk(β(t), xtest)
dt

= κKntk(xtest, X)⊤(Y

Note by deﬁnition, untk,test(t) = κfntk(β(t), xtest)

untk(t))

−

−
R, so we have

∈

fntk(β(t), xtest).

λ

·

duntk,test(t)
dt

= κ2Kntk(xtest, X)⊤(Y

untk(t))

λ

·

−

−

untk,test(t).

Lemma B.9 (Linear convergence of kernel ridge regression). Given training data matrix X =
Rn be
Rn
[x1,
Rn be deﬁned in Deﬁnition B.5. Let Λ0 > 0 be deﬁned as in
deﬁned as in Deﬁnition B.5. Let u∗ ∈
Deﬁnition B.4. Let λ > 0 be the regularization parameter. Then we have

d and corresponding label vector Y

(0, 1) and untk(t)

, xn]⊤ ∈

Rn. Let κ

· · ·

∈

∈

∈

×

d

untk(t)
k
dt

2
2

u∗k

−

2(κ2Λ0 + λ)
untk(t)
k

u∗

2
2.
k

−

≤ −

Further, we have

Proof. Let H cts

Rn

∈

×
κ2H cts(Y

−

untk(t)
k

k2 ≤
n be deﬁned as in Deﬁnition B.4. Then

−

untk(0)
k

e−

u∗

(κ2Λ0+λ)t

u∗

k2.

−

u∗) = κ2H cts(Y

κ2H cts(κ2H cts + λIn)−
κ2H cts(κ2H cts + λI)−

1Y )
1)Y
κ2H cts)(κ2H cts + λIn)−
1Y

−
= κ2H cts(In −
= κ2H cts(κ2H cts + λIn −
= κ2λH cts(κ2H cts + λIn)−
= λu∗,

1Y

(26)
Rn, the second to fourth step simplify the formula,

Rn again. So we have

where the ﬁrst step follows the deﬁnition of u∗ ∈
and the last step use the deﬁnition of u∗ ∈

d

untk(t)
k
dt

2
2

u∗k

−

= 2(untk(t)

u∗)⊤

−

duntk(t)
dt

=

=

=

=

−

−

−

−

−

−

≤ −

2κ2(untk(t)
2κ2(untk(t)
2λ(untk(t)
−
2κ2(untk(t)
2λ(untk(t)

−

−

−

−

u∗)⊤H cts(untk(t)
u∗)⊤H cts(untk(t)
u∗)⊤untk(t)
u∗)⊤H cts(untk(t)
u∗)⊤untk(t)
−
u∗)⊤(κ2H cts + λI)(untk(t)
u∗

−

−

2
2,
k

2(untk(t)
−
2(κ2Λ0 + λ)
untk(t)
k

−

Y )
2λ(untk(t)
−
u∗) + 2κ2(untk(t)

−

u∗)⊤untk(t)

u∗)⊤H cts(Y

u∗)

−

−

u∗) + 2λ(untk(t)

u∗)⊤u∗

−

u∗)

−

(27)

17

where the ﬁrst step follows the chain rule, the second step follows Corollary B.8, the third step uses
basic linear algebra, the fourth step follows Eq. (26), the ﬁfth step simpliﬁes the expression, and
the last step follows the deﬁnition of Λ0. Further, since

d(e2(κ2Λ0+λ)t

untk(t)
k
dt

2
2)
u∗k

−

= 2(κ2Λ0 + λ)e2(κ2Λ0+λ)t

untk(t)
k

−

u∗

2 + e2(κ2Λ0+λ)t
2
k

·

d

untk(t)
k
dt

2
2

u∗k

−

0,

≤

where the ﬁrst step calculates the gradient, and the second step follows from Eq. (27). Thus,
e2(κ2Λ0+λ)t

2
2 is non-increasing, which implies

untk(t)
k

−

u∗k

untk(t)
k

−

u∗

k2 ≤

e−

(κ2Λ0+λ)t

untk(0)
k

u∗

k2.

−

Lemma B.10 (Gradient ﬂow of neural network training). Given training data matrix X
and corresponding label vector Y
W (t)
be deﬁned as in Deﬁnition B.6. Then for any data z

→
×
Rn be deﬁned as in Deﬁnition B.3. Let Kt : Rd
Rd, we have

d
×
R be deﬁned as in Deﬁnition B.2. Let
Rn

Rn. Let fnn : Rd
×

(0, 1) and unn(t)

m, κ

Rn

Rd

Rd

→

×

∈

∈

∈

∈

∈

m

×

×

d

Rn

∈

dfnn(W (t), z)
dt

= κKt(z, X)⊤(Y

unn(t))

λ

·

−

−

fnn(W (t), z).

Proof. Denote L(t) = 1

Y
2 k

2 + 1
2
2 λ
unn(t)
k

−
dwr
dt

=

∂L
∂wr

−

2
F . By the rule of gradient descent, we have
W (t)
k
k
∂unn
∂wr

λwr.

)⊤(Y

unn)

−

−

= (

Also note for ReLU activation σ, we have

dfnn(W (t), z)
dW (t)

D

m

, λW (t)

=

1
√m

m

arzσ′(wr(t)⊤z)

⊤(λwr(t))

(cid:17)

arwr(t)⊤zσ′(wr(t)⊤z)

E

=

=

Xr=1 (cid:16)
λ
√m

λ
√m

r=1
X
m

arσ(wt(t)⊤z)

r=1
X
= λfnn(W (t), z),

(28)

(29)

where the ﬁrst step calculates the derivatives, the second step follows basic linear algebra, the third
step follows the property of ReLU activation: σ(l) = lσ′(l), and the last step follows from the

18

deﬁnition of fnn. Thus, we have

dfnn(W (t), z)
dt

dfnn(W (t), z)
dW (t)

,

dW (t)
dt

E
κfnn(W (t), xj ))

(yj −

=

=

D

n

Xj=1
n

= κ

dfnn(W (t), z)
dW (t)

,

dκfnn(W (t), xj)
dW (t)

dfnn(W (t), z)
dW (t)

, λW (t)

E

−

D

E

D

κfnn(W (t), xj))Kt(z, xj)

(yj −

λ

·

−

fnn(W (t), z)

Xj=1

= κKt(z, X)⊤(Y

unn(t))

λ

·

−

−

fnn(W (t), z),

where the ﬁrst step follows from chain rule, the second step follows from Eq. (28), the third step
follows from the deﬁnition of Kt and Eq. (29), and the last step rewrites the formula in a compact
form.

· · ·
m

Corollary B.11 (Gradient of prediction of neural network). Given training data matrix X =
Rd. Let fnn :
[x1,
Rn be
Rd
m, κ
(0, 1) and unn(t)
×
deﬁned as in Deﬁnition B.3. Let Kt : Rd
n be deﬁned as in Deﬁnition B.6.
Then we have

Rn
d and corresponding label vector Y
∈
R be deﬁned as in Deﬁnition B.2. Let W (t)
Rn, H(t)

Rn. Given a test data xtest ∈
∈

, xn]⊤ ∈
Rd
→
×

Rd
×
Rn

∈
∈

Rn

→

×

∈

×

×

×

d

dunn(t)
dt

dunn,test(t)
dt

= κ2H(t)(Y

unn(t))

λ

·

−

unn(t)

−

= κ2Kt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

unn,test(t).

Proof. Plugging in z = xi ∈

Rd in Lemma B.10, we have

dfnn(W (t), xi)
dt

= κKt(xi, X)⊤(Y

unn(t))

λ

·

−

−

fnn(W (t), xi).

Note [unn(t)]i = κfnn(W (t), xi) and [H(t))]:,i = Kt(xi, X), so writing all the data in a compact form,
we have

dunn(t)
dt

= κ2H(t)(Y

unn(t))

λ

·

−

unn(t).

−

Plugging in data z = xtest ∈

Rd in Lemma B.10, we have

dfnn(W (t), xtest)
dt

= κKt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

fnn(W (t), xtest).

Note by deﬁnition, unn,test(t) = κfnn(W (t), xtest), so we have

dunn,test(t)
dt

= κ2Kt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

unn,test(t).

19

×

· · ·

Lemma B.12 (Linear convergence of neural network training). Given training data matrix X =
Rn. Fix the total number of iterations
[x1,
, xn]⊤ ∈
Rn be deﬁned in
T > 0. Let , κ
∈
Rn
n be deﬁned
Eq. (24). Let H cts
Λ0/2
as in Deﬁnition B.6. Let λ > 0 be the regularization parameter. Assume
holds for all t

n be deﬁned as in Deﬁnition B.3. Let u∗ ∈
∈
H(t)
k

d and corresponding label vector Y
Rn

n and Λ0 > 0 be deﬁned as in Deﬁnition B.4. Let H(t)

Rn
(0, 1) and unn(t)
×

[0, T ]. Then we have

×
H cts

k ≤

Rn

−

∈

∈

∈

×

∈
u∗k

2
2

d

unn(t)
k
dt

−

(κ2Λ0 + λ)
unn(t)
k

−

u∗

2
2 + 2κ2
k

H(t)
k

−

≤ −

H cts

unn(t)

k · k

u∗

Y
k2 · k

−

u∗

k2.

−

Proof. Note same as in Lemma B.9, we have

κ2H cts(Y

−

u∗) = κ2H cts(Y

κ2H cts(κ2H cts + λIn)−
κ2H cts(κ2H cts + λI)−

1Y )
1)Y
κ2H cts)(κ2H cts + λIn)−
1Y

−
= κ2H cts(In −
= κ2H cts(κ2H cts + λIn −
= κ2λH cts(κ2H cts + λIn)−
= λu∗,

1Y

(30)

where the ﬁrst step follows the deﬁnition of u∗ ∈
and the last step use the deﬁnition of u∗ ∈
u∗k

−

d

2
2

unn(t)
k
dt

Rn, the second to fourth step simplify the formula,

Rn again. Thus, we have

dunn(t)
dt

= 2(unn(t)

u∗)⊤

=

=

=

=

−

−

−

−

−

u∗)⊤(H(t)

u∗)⊤H(t)(unn(t)

u∗)⊤H(t)(unn(t)

−
2κ2(unn(t)
2κ2(unn(t)
−
+ 2κ2(unn(t)
2κ2(unn(t)
−
+ 2κ2(unn(t)
−
−
−
u∗)⊤(κ2H(t) + λI)(unn(t)
2(unn(t)
−
2 + 2κ2
2
(κ2Λ0 + λ)
H(t)
unn(t)
k
k
k

u∗)⊤H(t)(unn(t)

−
H cts)(Y

−
H cts)(Y

u∗)⊤(H(t)

u∗

−

−

−

−

−

−

≤ −

Y )
2λ(unn(t)
−
u∗) + 2κ2(unn(t)

u∗)⊤unn(t)

u∗)

2λ(unn(t)

−

u∗) + 2λ(unn(t)

−
u∗)⊤u∗

u∗)⊤H cts(Y

u∗)

−
u∗)⊤unn(t)

−

−

u∗)⊤unn(t)

2λ(unn(t)
−
−
u∗) + 2κ2(unn(t)
H cts

unn(t)

−
u∗

u∗)

−

−

kk

u∗)⊤(H(t)

Y
k2k

u∗

−

−

−
k2

H cts)(Y

u∗)

−

where the ﬁrst step follows the chain rule, the second step follows Corollary B.11, the third step
uses basic linear algebra, the fourth step follows Eq. (30), the ﬁfth step simpliﬁes the expression,
and the last step follows the assumption

H cts

Λ0/2.

H(t)
k

−

k ≤

B.4 Proof sketch

Our goal is to show with appropriate width of the neural network and appropriate training iterations,
the neural network predictor will be suﬃciently close to the neural tangent kernel ridge regression
predictor for any test data. We follow similar proof framework of Theorem 3.2 in [ADH+19a]. Given
any accuracy ǫ

(0, 1), we divide this proof into following steps:

∈

1. Firstly, according to the linear convergence property of kernel ridge regression shown in
u∗test −
|

Lemma B.9, we can choose suﬃciently large training iterations T > 0, so that
untk,test(T )

ǫ/2, as shown in Lemma B.13.

| ≤

20

2. Once ﬁx training iteration T as in step 1, we bound

the following:

unn,test(T )
|

−

untk,test(T )

| ≤

ǫ/2 by showing

(a) Due to the similarity of the the gradient ﬂow of neural network training and neural
tangent kernel ridge regression, we can reduce the task of bounding the prediction per-
, back to bounding
untk,test(T )
turbation at time T , i.e.,
|

unn,test(T )
|

−

i. the initialization perturbation
ii. kernel perturbation

H(t)
k

−

unn,test(0)
|
H cts
,
k

untk,test(0)
|
−

Kntk(xtest, X)
k

−

and
Kt(xtest, X)

k2, as shown in

Lemma B.14.

untk,test(0)
|

small enough by choosing suﬃciently small κ

(b) According to concentration results, we can bound the initialization perturbation

unn,test(0)
|
(0, 1), as shown in Lemma B.20.
(c) We characterize the over-parametrization property of the neural network by inductively
show that we can bound kernel perturbation
H(t)
k2
k
small enough by choosing network width m > 0 large enough, as shown in Lemma B.21.

Kntk(xtest, X)
k

Kt(xtest, X)

H cts

,
k

−

−

∈

−

3. Lastly, we combine the results of step 1 and 2 using triangle inequality, to show the equivalence
between training neural network with regularization and neural tangent kernel ridge regression,
i.e.,

ǫ, as shown in Theorem B.28.

unn,test(T )
|

−

u∗test| ≤

B.5 Equivalence between training net with regularization and kernel ridge re-

gression for test data prediction

In this section, we prove Theorem 3.7 following the proof sketch in Section B.4.

B.5.1 Upper bounding

−
In this section, we give an upper bound for

untk,test(T )
|

u∗test|
untk,test(T )
|

u∗test|
.

−

Lemma B.13. Let untk,test(T )
ǫ > 0, if κ

(0, 1), then by picking T =

R and u∗test ∈
O(

∈

R be deﬁned as Deﬁnition B.5. Given any accuracy
), we have

1
κ2Λ0

∈

untk,test(T )
e
|

u∗test| ≤

−

ǫ/2.

where

) here hides poly log(n/(ǫΛ0)).
O(
·

Proof. Due to the linear convergence of kernel ridge regression, i.e.,

e

d

β(t)
k

−
dt

2
2

β∗k

2(κ2Λ0 + λ)
β(t)
k

β∗

2
2
k

−

≤ −

Thus,

untk,test(T )
|

−

u∗test|

=

≤

≤

≤

κΦ(xtest)⊤β(T )
|
Φ(xtest)
κ
k
κe−

k2k
(κ2Λ0+λ)T

−
β(T )

κΦ(xtest)β∗

|

β∗

−

k2
k2

β∗

β(0)
k
poly(κ, n, 1/Λ0)

−

(κ2Λ0+λ)T

e−

·
β∗k2 = poly(κ, n, 1/Λ0).
k
1
κ2Λ0

), we have

where the last step follows from β(0) = 0 and
O(

(0, 1). Thus, by picking T =

Note κ

∈

untk,test(T )
k

e

u∗testk2 ≤

−

ǫ/2,

where

) here hides poly log(n/(ǫΛ0)).
O(
·

e

21

B.5.2 Upper bounding

perturbation

unn,test(T )
|

−

untk,test(T )
|

by bounding initialization and kernel

The goal of this section is to prove Lemma B.14, which reduces the problem of bounding prediction
perturbation to the problem of bounding initialization perturbation and kernel perturbation.

×

∈

Rn

d and corresponding label vector Y
∈
Rd. Let unn,test(t)
∈
Rn, H cts

Lemma B.14 (Prediction perturbation implies kernel perturbation). Given training data matrix
Rn. Fix the total number of iterations T > 0. Given
X
Rn be the test data predictors
Rn and untk,test(t)
arbitrary test data xtest ∈
∈
(0, 1) be the corresponding multi-
deﬁned in Deﬁnition B.3 and Deﬁnition B.5 respectively. Let κ
plier. Let Kntk(xtest, X)
Rn
n, Λ0 > 0 be deﬁned
∈
∈
Rn be deﬁned as in Eq. (24). Let λ > 0 be the
in Deﬁnition B.4 and Deﬁnition B.6. Let u∗ ∈
(0, 1) denote parameters that are
(0, 1) and ǫH ∈
(0, 1), ǫinit ∈
regularization parameter. Let ǫK ∈
independent of t, and the following conditions hold for all t

Rn, Kt(xtest, X)

∈
n, H(t)

[0, T ],

Rn

∈

∈

×

×

∈

unn(0)

k2 ≤

• k

√nǫinit and

unn,test(0)
|

ǫinit

| ≤
ǫK

Kntk(xtest, X)

• k

Kt(xtest, X)

k2 ≤

−

H(t)

• k
then we have

−

H cts

ǫH

k ≤

unn,test(T )
|

−

untk,test(T )

| ≤

(1 + κ2nT )ǫinit + κ2ǫK ·
+ √nT 2κ4ǫH(
k2 +
u∗
k

(cid:16)
u∗
k

u∗k2
k
κ2Λ0 + λ
k2)
Y

−

+

u∗
k

Y

k2T

−

(cid:17)

Proof. Combining results from Lemma B.15, Claim B.16. B.17, B.18, we complete the proof. We
have

unn,test(T )
|

−

untk,test(T )

| ≤ |

unn,test(0)
T

untk,test(0)
−
|
(Kntk(xtest, X)

+ κ2

0

Z

T

(cid:12)
(cid:12)
(cid:12)

+ κ2

0

Kt(xtest, X))⊤(untk(t)

−

Y )dt

−

(cid:12)
(cid:12)
(cid:12)

Kt(xtest, X)⊤(untk(t)

unn(t))dt

−

≤

Z
(cid:12)
u∗k
(cid:12)
ǫinit + κ2ǫK ·
k
(cid:12)
κ2Λ0 + λ
+ κ2nǫinitT + √nT 2
·
(1 + κ2nT )ǫinit + κ2ǫK ·
+ √nT 2κ4ǫH (
k2 +
u∗
k
where the ﬁrst step follows from Lemma B.15, the second step follows from Claim B.16, B.17 and
B.18, and the last step simpliﬁes the expression.

+
u∗
−
k
κ4ǫH ·
(
u∗
k
u∗k2
k
κ2Λ0 + λ
k2)
Y

k2T
k2 +
+

(cid:17)
u∗
k
u∗
k

k2)
k2T

(cid:16)
u∗
k

−

−

−

≤

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

Y

Y

Y

To prove Lemma B.14, we ﬁrst bound

unn,test(T )
|
then we bound each term individually in Claim B.16, Claim B.17, and Claim B.18.

untk,test(T )
|

−

by three terms in Lemma B.15,

Lemma B.15. Follow the same notation as Lemma B.14, we have

unn,test(T )
|

−

untk,test(T )

| ≤

A + B + C,

22

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

where

Proof.

A =

unn,test(0)
|
T

untk,test(0)
|

−

(Kntk(xtest, X)

Kt(xtest, X))⊤(untk(t)

−

Y )dt

−

B = κ2

C = κ2

0

Z

T

0

Z

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Kt(xtest, X)⊤(untk(t)

unn(t))dt

−

(cid:12)
(cid:12)
(cid:12)

unn,test(T )
|

untk,test(T )
|

−

=

unn,test(0)
(cid:12)
(cid:12)
(cid:12)
unn,test(0)
≤ |

untk,test(0) +

−

0
Z

untk,test(0)
|

−

+

T

(

dunn,test(t)
dt

−
dunn,test(t)
dt

T

(

duntk,test(t)
dt

)dt

duntk,test(t)
dt

−

(cid:12)
(cid:12)
(cid:12)
)dt

,

(31)

0

Z

(cid:12)
(cid:12)
(cid:12)

where the ﬁrst step follows from the deﬁnition of integral, the second step follows from the triangle
inequality. Note by Corollary B.8, B.11, their gradient ﬂow are given by

duntk,test(t)
dt
dunn,test(t)
dt
Rn and unn(t)

=

=

κ2Kntk(xtest, X)⊤(untk(t)

−

Y )

−

−

λuntk,test(t)

(32)

κ2Kt(xtest, X)⊤(unn(t)

Y )

λunn,test(t)

−
Rn are the predictors for training data deﬁned in Deﬁnition B.5

−

−

(33)

where untk(t)
and Deﬁnition B.3. Thus, we have

∈

∈

dunn,test(t)
dt

duntk,test(t)
dt

−
κ2Kt(xtest, X)⊤(unn(t)

=
= κ2(Kntk(xtest, X)

−

−

λ(unn,test(t)

−

−

Y ) + κ2Kntk(xtest, X)⊤(untk(t)

Y )

λ(unn,test(t)

untk,test(t))

Kt(xtest, X))⊤(untk(t)

Y )

−

−

−
untk,test(t)),

−
κ2Kt(xtest, X)⊤(untk(t)

−

−
unn(t))

−

(34)

where the ﬁrst step follows from Eq. (32) and Eq. (33), the second step rewrites the formula. Note
the term

untk,test(t)) will only make

λ(unn,test(t)

−

−

smaller, so we have

0

Z

(cid:12)
(cid:12)
(cid:12)

T

(

dunn,test(t)
dt

duntk,test(t)
dt

−

)dt

(cid:12)
(cid:12)
(cid:12)

(

dunn,test(t)
dt

−

duntk,test(t)
dt

)dt

κ2((Kntk(xtest, X)

−

(cid:12)
(cid:12)
Kt(xtest, X))⊤(untk(t)
(cid:12)

T

0

Z

T

0

Z

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤

Y )

−

−

κ2Kt(xtest, X)⊤(untk(t)

−

unn(t)))dt

(cid:12)
(35)
(cid:12)
(cid:12)

23

Thus,

unn,test(T )
|

untk,test(T )
|

−

unn,test(0)

≤ |

untk,test(0)
|

−

+

(

dunn,test(t)
dt

−

duntk,test(t)
dt

)dt

T

0

Z

T

(cid:12)
(cid:12)
(cid:12)

0

Z
unn(t)))dt

(cid:12)
(cid:12)
(cid:12)
−

unn,test(0)

≤ |

untk,test(0)
|

−

+

κ2((Kntk(xtest, X)

−

κ2Kt(xtest, X)⊤(untk(t)

−

unn,test(0)

≤ |

T

+

T

+

untk,test(0)
|

−

Z
κ2Kt(xtest, X)⊤(untk(t)

(cid:12)
(cid:12)
(cid:12)

0

0

Z
= A + B + C,

(cid:12)
(cid:12)
(cid:12)

κ2(Kntk(xtest, X)

(cid:12)
(cid:12)
(cid:12)

unn(t))dt

−

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
Kt(xtest, X))⊤(untk(t)
(cid:12)

Y )

−

Kt(xtest, X))⊤(untk(t)

−

Y )dt

−

(cid:12)
(cid:12)
(cid:12)

where the ﬁrst step follows from Eq. (31), the second step follows from Eq. (35), the third step
follows from triangle inequality, and the last step follows from the deﬁnition of A, B, C.

Now let us bound these three terms A, B and C one by one. We claim

Claim B.16 (Bounding the term A). We have

Proof. Note untk,test(0) = 0, so by assumption we have

A

≤

ǫinit.

A =

unn,test(0)
|

| ≤

ǫinit.

Claim B.17 (Bounding the term B). We have

B

κ2ǫK ·

≤

u∗k
k
κ2Λ0 + λ

(cid:16)

+

u∗
k

Y

k2T

−

Proof. Note

B = κ2

T

(Kntk(xtest, X)

.

(cid:17)

−

∈
k2.

0

Z
(cid:12)
(cid:12)
κ2 max
(cid:12)
t
∈

[0,T ] k

≤

Kntk(xtest, X)

Kt(xtest, X))⊤(untk(t)

−

Y )dt

Kt(xtest, X)

k2

−

T

untk(t)

0 k
Z

(cid:12)
(cid:12)
(cid:12)
−

Y

k2dt,

where the ﬁrst step follows from deﬁnition of B, and the second step follows from the Cauchy-
Rn con-
Schwartz inequality. Note by Lemma B.9, the kernel ridge regression predictor untk(t)
verges linearly to the optimal predictor u∗ = κ2H cts(κ2H cts + λI)−

Rn, i.e.,

1Y

∈

(36)

untk(t)
k

−

u∗

k2 ≤

e−

(κ2Λ0+λ)t

untk(0)
k

−

u∗

24

Thus, we have

T

T

untk(t)

0 k

Y

k2dt

−

≤

Z

≤

≤

untk(t)

0 k
Z
T

(κ2Λ0+λ)

e−

0
Z
untk(0)
k

−

κ2Λ0 + λ
u∗k
κ2Λ0 + λ

+

= k

u∗

k2dt +

−

T

u∗

0 k
Z

Y

k2dt

−

u∗

k2dt +

−

T

u∗

0 k
Z

Y

k2dt

−

untk(0)
k
u∗k2

+

u∗
k

Y

k2T

−

u∗
k

Y

k2T,

−

(37)

where the ﬁrst step follows from the triangle inequality, the second step follows from Eq. (36), the
third step calculates the integration, and the last step follows from the fact untk(0) = 0. Thus, we
have

B

≤

κ2 max
t
∈

[0,T ] k

Kntk(xtest, X)

Kt(xtest, X)

k2 ·

−

T

untk(t)

0 k
Z

Y

k2dt

−

κ2ǫK ·

k2T
where the ﬁrst step follows from Eq. (36), the second step follows from Eq. (37) and deﬁnition of
ǫK.

u∗
k

−

+

≤

(cid:17)

(cid:16)

Y

.

u∗k
k
κ2Λ0 + λ

Claim B.18 (Bounding the term C). We have

C

≤

nǫinitT + √nT 2

κ2ǫH ·

·

u∗
(
k

k2 +

u∗
k

Y

k2)

−

Proof. Note

T

C = κ2

Kt(xtest, X)⊤(untk(t)

unn(t))dt

0
Z
(cid:12)
κ2 max
(cid:12)
(cid:12)
t
∈

[0,T ] k

≤

Kt(xtest, X)

−
k2 max

[0,T ] k

t
∈

untk(t)

(cid:12)
unn(t)
(cid:12)
(cid:12)

k2 ·

−

T

(38)

where the ﬁrst step follows from the deﬁnition of C, and the second step follows the Cauchy-Schwartz
inequality.

To bound term maxt
[0,T ] k
∈

untk(t)

−

unn(t)

untk(t)
k

−

unn(t)

k2 ≤ k

untk(0)

unn(0)

−

k2, notice that for any t
d(untk(τ )

t

∈

[0, T ], we have

0

k2 +
Z
(cid:13)
d(untk(τ )
(cid:13)
(cid:13)
−
dτ

−
dτ

unn(τ ))

dτ

,

2
(cid:13)
(cid:13)
(cid:13)

(39)

unn(τ ))

dτ

where the ﬁrst step follows the triangle inequality, and the second step follows the assumption.
Further,

d(untk(τ )

−
dτ

unn(τ ))

=

=

−

−

κ2H cts(untk(τ )
−
(κ2H(τ ) + λI)(untk(τ )

Y )

−

−

λuntk(τ ) + κ2H(τ )(unn(τ )
unn(τ )) + κ2(H(τ )

Y ) + λunn(τ )

−

H cts)(untk(τ )

Y ),

−

−

where the ﬁrst step follows the Corollary B.8, B.11, the second step rewrites the formula. Since the
term

(κ2H(τ ) + λI)(untk(τ )

unn(τ )) makes

unn(τ ))

dτ

t
0

d(untk(τ )
−
dτ

−

−

k2 smaller.

k

R
25

= √nǫinit +

t

0
Z

(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)

Taking the integral and apply the ℓ2 norm, we have

d(untk(τ )

−
dτ

unn(τ ))

dτ

t

0

Z

(cid:13)
(cid:13)
(cid:13)

2 ≤

(cid:13)
(cid:13)
(cid:13)

0

Z

(cid:13)
(cid:13)
(cid:13)

Thus,

t

κ2(H(τ )

H cts)(untk(τ )

Y )dτ

−

−

(40)

.

2
(cid:13)
(cid:13)
(cid:13)

untk(t)

max
t
∈

[0,T ] k

unn(t)

k2 ≤

−

√nǫinit + max
[0,T ]

t
∈

≤

≤

≤

≤

√nǫinit + max
[0,T ]

t
∈

√nǫinit + max
[0,T ]

t
∈

√nǫinit + max
[0,T ]

t
∈

√nǫinit + max
[0,T ]
√nǫinit + max
[0,T ]

t
∈

t

t

t

0
Z

(cid:13)
(cid:13)
(cid:13)

0
Z
t

(cid:13)
(cid:13)
(cid:13)
Z
κ2ǫH

0

d(untk(τ )

unn(τ ))

−
dτ

dτ

2
(cid:13)
(cid:13)
H cts)(untk(τ )
(cid:13)

κ2(H(τ )

−

κ2

H(τ )
k

−

H cts

untk(τ )

k · k

(cid:16) Z

κ2ǫH

t

untk(τ )

0 k
t

u∗

k2dτ +

−

untk(0)

u∗

−

k2dτ +
k2)
Y

u∗
k

(cid:16) Z
κ2ǫH ·
u∗
(
k

0 k
u∗
(
k
k2 +

k2 +
u∗
k

Y )dτ

−

2
(cid:13)
(cid:13)
(cid:13)
k2dτ

−

Y

t

u∗

0 k
Z
t

Y

k2dτ

−

u∗

0 k
Z

Y

k2dτ

−

(cid:17)

(cid:17)

≤

≤

t
∈
√nǫinit + T

·
κ2ǫH ·
where the ﬁrst step follows from Eq. (39), the second step follows from Eq. (40), the third step follows
H cts
T
from triangle inequality, the fourth step follows from the condition
≤
and the triangle inequality, the ﬁfth step follows from the linear convergence of
u∗k2 as in
Lemma B.9, the sixth step follows the fact untk(0) = 0, and the last step calculates the maximum.
Therefore,

k ≤
untk(τ )
k

ǫH for all τ

H(τ )
k

−
k2)

(41)

−

−

−

Y

·

C

≤

≤

[0,T ] k

κ2 max
t
∈
κ2 max
t
∈

[0,T ] k

Kt(xtest, X)

Kt(xtest, X)

untk(t)

k2 max

[0,T ] k
−
t
∈
(√nǫinitT + T 2
k2 ·
κ4ǫH ·

u∗
(
k

T

unn(t)
k2 ·
κ2ǫH ·
u∗
(
k
k2)
Y

·

κ2nǫinitT + √nT 2

k2 +
where the ﬁrst step follows from Eq. (38), and the second step follows from Eq. (41), and the last
step follows from the fact that Kt(x, z)
1 holds for any

u∗
k

−

≤

·

Remark B.19. Given ﬁnal accuracy ǫ, to ensure
ǫ, we need to choose
κ > 0 small enough to make ǫinit = O(ǫ) and choose width m > 0 large enough to make ǫH and ǫtest
both O(ǫ). And we discuss these two tasks one by one in the following sections.

| ≤

−

≤

x
k

k2,
unn,test(T )
|

1.

z
k2 ≤
k
untk,test(T )

k2 +

u∗
k

Y

k2))

−

B.5.3 Upper bounding initialization perturbation

In this section, we bound ǫinit to our wanted accuracy ǫ by picking κ large enough. We prove
Lemma B.20.

Lemma B.20 (Bounding initialization perturbation). Let fnn be as deﬁned in Deﬁnition B.2. As-
, m as deﬁned in Deﬁnition B.3
sume the initial weight of the network work wr(0)
are drawn independently from standard Gaussian distribution
, m
N
· · ·
(0, 1), unn(t)
as deﬁned in Deﬁnition B.2 are drawn independently from unif[

Rd, r = 1,

R, r = 1,

· · ·
(0, Id). And ar ∈
]. Let κ
1, +1
}
{−

∈

∈

26

and unn,test(t)
have with probability 1

∈

R be deﬁned as in Deﬁnition B.3. Then for any data x

Rd with

∈

x
k

k2 ≤

1, we

δ,

−

fnn(W (0), x)
|

| ≤

2 log(2m/δ).

Further, given any accuracy ǫ

(0, 1), if κ =

O(ǫ(Λ0 + λ)/n), let ǫinit = ǫ(Λ0 + λ)/n, we have

∈
unn(0)
|

√nǫinit and
e

unn,test(0)
|

| ≤

ǫinit

| ≤

hold with probability 1

δ, where

) hides the poly log(n/(ǫδΛ0)).
O(
·

−

Proof. Note by deﬁnition,

e

Since wr(0)
bounds Lemma A.5, we have with probability 1

(0, Id), so wr(0)⊤xtest ∼

∼ N

N (0,

xtestk2 ≤
k

1, by Gaussian tail

fnn(W (0), x) =

m

arσ(wr(0)⊤x).

1
√m

r=1
X
xtestk2). Note
k
δ/(2m):
−

Condition on Eq. (42) holds for all r
and

∈

2 log(2m/δ). By Lemma A.2, with probability 1

Zr| ≤
|

δ/2:

−

wr(0)⊤x
|

| ≤

2 log(2m/δ).

(42)

[m], denote Zr = arσ(wr(0)⊤x), then we have E[Zr] = 0

p

p

Since unn,test(0) = 1
√n
δ:
with probability 1

−

P

Zr

m

r=1
X

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2√m log (2m/δ).

≤

Zr, by combining Eq. (42), (43) and union bound over all r

(43)

[m], we have

∈

Further, note [unn(0)]i = κfnn(W (0), xi) and unn,test(0) = κfnn(W (0), xtest). Thus, by choosing
κ =

O(ǫ(Λ0 + λ)/n), taking the union bound over all training and test data, we have

unn,test(0)
|

| ≤

2 log(2m/δ).

e

unn(0)
|

| ≤

√nǫinit and

unn,test(0)
|

| ≤

ǫinit

hold with probability 1

δ, where

) hides the poly log(n/(ǫδΛ0)).
O(
·

−

B.5.4 Upper bounding kernel perturbation

e

In this section, we try to bound kernel perturbation by induction. We want to prove Lemma B.21,
which also helps to show the equivalence for training data prediction as shown in Section B.6.

Rn and
Lemma B.21 (Bounding kernel perturbation). Given training data X
Rd. Let T > 0 denotes the total number of iterations, m > 0 denotes the
a test data xtest ∈
width of the network, ǫtrain denotes a ﬁxed training error threshold, δ > 0 denotes the failure
Rn be the training data predictors deﬁned in Deﬁ-
probability. Let unn(t)
∈
nition B.3 and Deﬁnition B.5 respectively. Let κ
(0, 1) be the corresponding multiplier. Let
∈
Kntk(xtest, X)
Rn, Kt(xtest, X)
Rn
n, Λ0 > 0 be the kernel related quantities de-
×
Rn be deﬁned as in Eq. (24). Let λ > 0 be the
ﬁned in Deﬁnition B.4 and Deﬁnition B.6. Let u∗ ∈

∈
Rn, H(t)

Rn and untk(t)

d, Y

Rn

∈

∈

∈

∈

∈

×

27

· · ·
O( ǫΛ0

n ), T =

regularization parameter. Let W (t) = [w1(t),
network deﬁned in Deﬁnition B.3.

For any accuracy ǫ

(0, 1/10). If κ =
O( 1

∈
) and λ =

O( n10d
ǫ6Λ10
0

≥

e
1.

√m ), with probability 1
m
independent of t, such that the following hold for all 0
e
k2 ≤
k2 ≤
2
2 ≤

(κ2Λ0 + λ)t/2)

wr(t)

H(t)

ǫW ,

[m]

ǫ′H

2.

3.

∈

∀

e

•

•

•

r

−

−

wr(0)
k
H(0)
k
u∗k
unn(t)
k
−
K0(xtest, X)
k
O( ǫλ2

4.

•

Further, ǫW ≤

max

exp(
{
Kt(xtest, X)
O( ǫλ2

ǫ′K

−
k2 ≤
n ) and ǫ′K ≤

0

−
n2 ), ǫ′H ≤

0

, wm(t)]

Rd

×

m be the parameters of the neural

∈

1

O(

κ2(Λ0+λ) ), ǫtrain =

u∗k2),
δ, there exist ǫW , ǫ′H, ǫ′K > 0 that are
t

unn(0)
O(
k

T :

−

e

−
≤

e
≤

unn(0)

· k

u∗k

−

2, ǫ2
2

train}

O( ǫλ2

0

n1.5 ). Here

) hides the poly log(n/(ǫδΛ0)).
O(
·

We ﬁrst state some concentration results for the random initialization that can help us prove

e

e

e

e

the lemma.

Lemma B.22 (Random initialization result). Assume initial value wr(0)
drawn independently from standard Gaussian distribution
have

N

· · ·
∈
(0, Id), then with probability 1

Rd, r = 1,

, m are
3δ we

−

k2 ≤

wr(0)
k
H cts
H(0)
k
Kntk(xtest, X)

−

log (m/δ) for all r

2√d + 2
4n(log(n/δ)/m)1/2
p
(2n log (2n/δ)/m)1/2

[m]

∈

(44)

(45)

(46)

k ≤
k2 ≤
Proof. By lemma A.6, with probability at least 1

K0(xtest, X)
k

−

wr(0)
k

k2 ≤

holds for all r

[m].

∈

Using Lemma A.9 in [SY19], we have

δ,

−
√d +

log(m/δ)

p

H(0)
k

holds with probability at least 1
Note by deﬁnition,

−

ǫ′′H = 4n(log (n/δ)/m)1/2

H cts

k ≤

−
δ.

E[K0(xtest, xi)] = Kntk(xtest, xi)

holds for any training data xi. By Hoeﬀding inequality, we have for any t > 0,
mt2/2).

Kntk(xtest, xi)

2 exp (

t]

K0(xtest, xi)
Pr[
|

| ≥
m log (2n/δ))1/2, we can apply union bound on all training data xi to get with proba-

−

≤

−

Setting t = ( 2
bility at least 1

−

δ, for all i

[n],

∈
K0(xtest, xi)
|

Kntk(xtest, xi)

| ≤

−

(2 log(2n/δ)/m)1/2.

Thus, we have

K0(xtest, X)
k

−
δ.

holds with probability at least 1
Using union bound over above three events, we ﬁnish the proof.

−

Kntk(xtest, X)

k2 ≤

(2n log(2n/δ)/m)1/2

(47)

28

Now conditioning on Eq. (44), (45), (46) holds, We show all the four conclusions in Lemma B.21

holds using induction.

We deﬁne the following quantity:

ǫW :=

√n
√m

+

(cid:16)
ǫ′H := 2nǫW
ǫK := 2√nǫW

√n
√m k

Y

max

unn(0)
4
k
{

−

u∗

k2/(κ2Λ0 + λ), ǫtrain ·

T

u∗

k2 + λ(2√d + 2

−

p

log(m/δ))

(cid:17)

·

}

T

(48)

which are independent of t.

Note the base case when t = 0 trivially holds. Now assuming Lemma B.21 holds before time
[0, T ], we argue that it also holds at time t. To do so, Lemmas B.23, B.24, B.25 argue these

t
conclusions one by one.

∈

Lemma B.23 (Conclusion 1). If for any τ < t, we have

unn(τ )
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)τ /2)

unn(0)

· k

u∗

2, ǫ2
2
k

train}

−

and

and

hold, then

wr(0)
k

−

wr(τ )

k2 ≤

ǫW ≤

1

wr(0)
k

k2 ≤

√d +

log(m/δ) for all r

[m]

∈

p

wr(0)
k

−

wr(t)

k2 ≤

ǫW

Proof. Recall the gradient ﬂow as Eq. (28)

dwr(τ )
dτ

=

1
√m

ar(yi −

n

Xi=1

unn(τ )i)xiσ′(wr(τ )⊤xi)

λwr(τ )

−

(49)

29

So we have

dwr(τ )
dτ

(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)

=

≤

≤

≤

≤

≤

≤

=

ar(yi −

unn(τ )i)xiσ′(wr(τ )⊤xi)

−

1
√m

n

yi −
|

unn(τ )i|

+ λ

wr(τ )
k

k2

λwr(τ )

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

unn(τ )

k2 + λ

wr(τ )
k

k2

−

unn(τ )

unn(τ )

unn(τ )

k2 + λ(
wr(0)
k
k2 + λ(√d +
p
k2 + λ(2√d + 2

−

−

−

k2 +

wr(τ )
k

−

wr(0)

k2)

log(m/δ) + 1)

log(m/δ))

u∗

k2 +

unn(τ )
k

−

−

u∗

p
k2) + λ(2√d + 2

log(m/δ))

unn(τ )

u∗

−

k2
k2 + λ(2√d + 2

Y

u∗

−

(κ2Λ0+λ)τ /4

p

p

log(m/δ))

n

(cid:13)
Xi=1
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
√m

Xi=1
Y

Y

Y

√n
√m k
√n
√m k
√n
√m k
√n
√m k
√n
√m
√n
√m k
√n
√m k

Y
(
k

+

Y

√n
√m

+

≤

−

u∗

max

e−
{

√n
√m k

k2, ǫtrain}

unn(0)
k
k2 + λ(2√d + 2
where the ﬁrst step follows from Eq. (49), the second step follows from triangle inequality, the third
step follows from Cauchy-schwarz inequality, the forth step follows from triangle inequality, the ﬁfth
log(m/δ), the seventh step
wr(0)
1,
wr(0)
step follows from condition
k
k
(κ2Λ0 +
follows from triangle inequality, the last step follows from
u∗k
p
−
λ)τ /2)

√d +
k2 ≤
unn(τ )
k

log(m/δ)),

exp(
{

2
2 ≤

k2 ≤

wr(τ )

max

(50)

p

u∗

−

−

−

Y

unn(0)

· k

−
Thus, for any t

.
train}

2, ǫ2
2
u∗k
T ,
≤

wr(0)
k

−

wr(t)

k2 ≤

≤

dτ

t

dwr(τ )
dτ

0
Z
(cid:13)
√n
(cid:13)
max
(cid:13)
√m

2
(cid:13)
(cid:13)
unn(0)
4
(cid:13)
k
{

u∗

k2/(κ2Λ0 + λ), ǫtrain ·

T

−

√n
√m k

Y

u∗

k2 + λ(2√d + 2

−

p

log(m/δ))

(cid:17)

·

+

(cid:16)

= ǫW

}

T

where the ﬁrst step follows triangle inequality, the second step follows Eq. (50), and the last step
follows the deﬁnition of ǫW as Eq. (48).

Lemma B.24 (Conclusion 2). If

[m],

r

∀

∈
wr(0)
k

wr(t)

k2 ≤

−

ǫW < 1,

30

then

holds with probability 1

n2

exp (

·
Proof. Directly applying Lemma A.10, we ﬁnish the proof.

−

−

H(t)

kF ≤

2nǫW

H(0)
k
−
mǫW /10).

Lemma B.25 (Conclusion 3). Fix ǫ′H > 0 independent of t. If for all τ < t

H(0)
k

−

H cts

k ≤

4n(log (n/δ)/m)1/2

Λ0/4

≤

and

and

and

then we have

H(0)
k

−

H(τ )

k ≤

ǫ′H ≤

Λ0/4

4n(log(n/δ)/m)1/2

≤

8κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

ǫ′H ≤

8κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

unn(t)
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)t/2)

unn(0)

· k

u∗

2, ǫ2
2
k

.
train}

−

Proof. By triangle inequality we have

H(τ )
k

−

H cts

+

−

H(0)
H(0)
H(τ )
k
k
ǫ′H + 4n(log (n/δ)/m)1/2
Λ0/2

k ≤ k

≤

≤

H cts

k

−

(51)

(52)

(53)

holds for all τ < t. Denote ǫH = ǫ′H + 4n(log (n/δ)/m)1/2, we have
H(τ )
k
which satisﬁes the condition of Lemma B.12. Thus, for any τ < t, we have

−

H cts

ǫH ≤

k ≤

Λ0/2,

d

unn(τ )
k
dτ

2
2

u∗k

−

(κ2Λ0 + λ)
(κ2Λ0 + λ)

unn(τ )

· k

unn(τ )

· k

u∗

u∗

−

−

≤ −

≤ −

2 + 2κ2
2
H(τ )
k
k
2 + 2κ2ǫH · k
2
k

H cts

−
unn(τ )

k · k
u∗

−

unn(τ )

Y
k2 · k

u∗

u∗

Y
k2 · k
k2

−

−

−

u∗

k2
(54)

where the ﬁrst step follows from Lemma B.12, the second step follows from Eq. (53). Now let us
discuss two cases:

Case 1. If for all τ < t,

unn(τ )
k
unn(t)
k

u∗

−

2
2 ≤
k

−

u∗k2 ≥
exp(

−

ǫtrain always holds, we want to argue that

(κ2Λ0 + λ)t/2)

unn(0)

· k

u∗

k2.

−

Note by assumption (51) and (52), we have

ǫH ≤

4κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

31

implies

k2 ≤
holds for any τ < t. Thus, plugging into (54),

−

2κ2ǫH · k
Y

u∗

(κ2Λ0 + λ)/2

unn(τ )

· k

u∗

k2

−

d

unn(τ )
k
dτ

2
2

u∗k

−

(κ2Λ0 + λ)/2

unn(τ )

· k

u∗

2
2,
k

−

≤ −

holds for all τ < t, which implies

unn(t)
k
Case 2. If there exist τ < t, such that

u∗

−

2
2 ≤
k

exp (

(κ2Λ0 + λ)t/2)

unn(0)

−
unn(τ )
k

−

· k

−
u∗k2 < ǫtrain, we want to argue that

u∗

2
2.
k

u∗k2 < ǫtrain. Note by assumption (51) and (52), we have

unn(t)
k

−

implies

ǫH ≤

4κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

2κ2ǫH · k

unn(τ )

u∗

Y
k2 · k

−

−

u∗

k2 ≤

(κ2Λ0 + λ)

ǫ2
train.

·

Thus, plugging into (54),

unn(τ )
d(
k

−

u∗k
dτ

2
2 −

ǫ2
train)

(κ2Λ0 + λ)

unn(τ )
(
k

·

−

u∗

2
2 −
k

ǫ2
train)

≤ −

holds for τ = τ , which implies e(κ2Λ0+λ)τ (
unn(τ )
k
unn(τ )
k
and

ǫ2
2
train) is non-increasing at τ = τ . Since
u∗k
2 −
train < 0, by induction, e(κ2Λ0+λ)τ (
2
ǫ2
ǫ2
u∗k
train) being non-increasing
u∗k
unn(τ )
2 −
k
−
2
t, which implies
u∗k
unn(τ )
2 −
≤
−
k
k2 < ǫtrain.

ǫ2
train < 0 holds for all τ

unn(t)
k

2
2 −

u∗

−

≤

−

−

τ

Combine above two cases, we conclude

unn(t)
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)t/2)

unn(0)

· k

u∗

2, ǫ2
2
k

.
train}

−

Lemma B.26 (Conclusion 4). Fix ǫW ∈

(0, 1) independent of t. If

r

∀

∈

[m], we have

wr(t)
k

−

wr(0)

k2 ≤

ǫW

then

Kt(xtest, X)
k

−

K0(xtest, X)

k2 ≤

ǫ′K = 2√nǫW

holds with probability at least 1

n

·

−

exp (

−

mǫW /10).

32

Proof. Recall the deﬁnition of K0 and Kt

K0(xtest, xi) =

Kt(xtest, xi) =

1
m

1
m

m

r=1
X
m

Xr=1

x⊤testxiσ′(x⊤testwr(0))σ′(x⊤i wr(0))

x⊤testxiσ′(x⊤testwr(t))σ′(x⊤i wr(t))

By direct calculation we have

K0(xtest, X)
k

Kt(xtest, X)
2
2 ≤
k

−

n

Xi=1 (cid:16)

1
m

m

2

,

sr,i

r=1
X

(cid:17)

where

sr,i = 1[wr(0)⊤xtest ≥

0, wr(0)⊤xi ≥

0]

−

1[wr(t)⊤xtest ≥

0, wr(t)⊤xi ≥

0],

r

∀

∈

[m], i

[n].

∈

Fix i

∈

[n], by Bernstein inequality (Lemma A.3), we have for any t > 0,

Pr

1
m

h

m

r=1
X

sr,i ≥

2ǫW

≤

i

exp(

−

mǫW /10).

Thus, applying union bound over all training data xi, i

K0(xtest, X)
Pr[
k

−

Kt(xtest, X)

k2 ≤

∈
2√nǫW ]

[n], we conclude

1

n

·

−

≤

exp (

−

mǫW /10).

Note by deﬁnition ǫ′K = 2√nǫW , so we ﬁnish the proof.

Now we summarize all the conditions need to be satisﬁed so that the induction works as in

Table 1.

Table 1: Summary of conditions for induction

No. Condition
1
ǫW ≤
1
Λ0/4
ǫ′H ≤
2
4n( log (n/δ)
m )1/2
3
4n( log (n/δ)
m )1/2
4
ǫtrain
ǫ′H ≤
5
8κ2
u∗
Y

k

−

Λ0/4

≤
ǫtrain
8κ2
u∗
Y
≤
2
k
k
(κ2Λ0 + λ)

−

2
k

Place
Lem. B.23
Lem. B.25
Lem. B.25
(κ2Λ0 + λ) Lem. B.25
Lem. B.25

Note by choosing If κ =

O( ǫΛ0

n ), T =

O(

1

κ2(Λ0+λ) ), ǫtrain =

unn(0)
O(
k

u∗k2), m

−

≥

O( n10d
ǫ6Λ10
0

)

and λ =

O( 1

√m ), we have

e

e

Y
k

−

u∗

k2 =
≤

≤

≤

e

e

e
(κ2λ(H cts + λIn)−
1Y
k
(κ2H cts + λIn)−
1
λ
k
λ√n
κ2Λ0
1
n2.5 )

e
O(

O(

kk

)

k2
Y
k2

(55)

e

33

where the ﬁrst step follows from the deﬁnition of u∗, the second step follows from Cauchy-Schwartz
inequality, the third step follows from Y = O(√n), and the last step follows from the choice of the
parameters.

Further, with probability 1

−
unn(0)
k

u∗

−

δ, we have

k2 ≤ k
O(

unn(0)
ǫΛ0
√n
O(√n)
e

≤

≤

u∗

Y
k2 +
k
−
1
n2.5 ) +
O(
) +

Y
k

k2 +
O(√n)

k2

e

e

where the ﬁrst step follows from triangle inequality, the second step follows from Lemma B.20,
Eq. (55) and Y = O(√n), and the last step follows ǫ, Λ0 < 1. With same reason,

e

unn(0)
k

−

u∗

k2 ≥ − k

u∗

unn(0)
ǫΛ0
√n

Y
k2 − k
−
1
n2.5 ) +
O(
)

−

O(

Y
k

k2 +
O(√n)

k2

≥ −

Thus, we have

unn(0)
k

u∗k2 =

−

≥
O(√n).

e

O(√n).
e

e

e

Now, by direct calculation, we have all the induction conditions satisﬁed with high probability.
Note the failure probability only comes from Lemma B.20, B.22, B.24, B.26, which only depend
on the initialization. By union bound over these failure events, we have all four conclusions in
Lemma B.21 holds with high probability, which completes the proof.

e

B.5.5 Final result for upper bounding

unn,test(T )
|

untk,test(T )
|

−

In this section, we prove Lemma B.27.

∈

Rd. Let unn,test(t)

Lemma B.27 (Upper bounding test error). Given training data matrix X
ing label vector Y
xtest ∈
∈
nition B.3 and Deﬁnition B.5 respectively. Let κ
accuracy ǫ > 0, if κ =
xtest ∈

d and correspond-
∈
Rn. Fix the total number of iterations T > 0. Given arbitrary test data
Rn be the test data predictors deﬁned in Deﬁ-
(0, 1) be the corresponding multiplier. Given
√m ). Then for any

O( n10d
ǫ6Λ10
0
δ over the random initialization, we have
e

Rd, with probability at least 1

Rn and untk,test(t)

n ), T =

∈
), m

) and λ

O( ǫΛ0

1
κ2Λ0

O( 1

O(

≥

≤

∈

e

e

×

Rn

−
unn,test(T )
k

e
untk,test(T )
k2 ≤

−

ǫ/2,

where

) hides poly log(n/(ǫδΛ0)).
O(
·

Proof. By Lemma B.14, we have

e

unn,test(T )
|

−

untk,test(T )

| ≤

(1 + κ2nT )ǫinit + κ2ǫK ·
+ √nT 2κ4ǫH(
k2 +
u∗
k

(cid:16)
u∗
k

u∗k2
k
κ2Λ0 + λ
k2)
Y

−

+

u∗
k

Y

k2T

−

(cid:17)

(56)

By Lemma B.20, we can choose ǫinit = ǫ(Λ0)/n.

Further, note

Kntk(xtest, X)
k

−

Kt(xtest, X)

k2 ≤ k

Kntk(xtest, X)
(2n log (2n/δ)/m)1/2 +

−

K0(xtest, X)

K0(xtest, X)
k2 +
k
K0(xtest, X)
k

−
Kt(xtest, X)

−

k2

Kt(xtest, X)

k2

≤

≤

O(

ǫΛ2
0
n1.5 )

e

34

where the ﬁrst step follows from triangle inequality, the second step follows from Lemma B.22, and
the last step follows from Lemma B.21. Thus, we can choose ǫK = ǫΛ2
n1.5 .

0

Also,

H cts
k

−

H(t)

k ≤ k

H cts
H(0)
k
4n(log(n/δ)/m)1/2 +

H(0)
k

−

+

H(t)

−
H(0)
k

−

k2
H(t)

k2

≤

≤

O(

ǫΛ2
0
n

)

where the ﬁrst step follows from triangle inequality, the second step follows from Lemma B.22, and
the last step follows from Lemma B.21. Thus, we can choose ǫH = ǫΛ2
n .

e

0

√n, plugging the value of ǫinit, ǫK, ǫH into Eq. (56), we have

Note

u∗k2 ≤
k

√n and

u∗ −
k

Y

k ≤
unn,test(T )
|

untk,test(T )

| ≤

−

ǫ/2.

B.5.6 Main result for test data prediction equivalence

In this section, we restate and prove Theorem 3.7.

Theorem B.28 (Equivalence between training net with regularization and kernel ridge regression
d and
for test data prediction, restatement of Theorem 3.7). Given training data matrix X
Rn. Let T > 0 be the total number of iterations. Given arbitrary
corresponding label vector Y
Rn be the test data predictors deﬁned in
test data xtest ∈
Deﬁnition B.3 and Deﬁnition B.5 respectively.

Rn

∈

∈

×

Rd. Let unn,test(t)

Rn and u∗test ∈
(0, 1/10) and failure probability δ
O( 1
m
√m ). Then for any xtest ∈
random initialization, we have

For any accuracy ǫ

O( n10d
ǫ6Λ10
0

) and λ

≥

≤

∈

∈

∈

(0, 1/10), if κ =

O( ǫΛ0

n ), T =

O(

1
κ2Λ0

),

Rd, with probability at least 1

e

δ over the

e

−

e

e

unn,test(T )
k

−

u∗testk2 ≤

ǫ.

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

Proof. It follows from combining results of bounding
Lemma B.27 and

e

untk,test(T )
k

−

u∗testk2 ≤

unn,test(T )
k

−

untk,test(T )

ǫ/2 as shown in

k2 ≤

ǫ/2 as shown in Lemma B.13 using triangle inequality.

B.6 Equivalence between training net with regularization and kernel ridge re-

gression for training data prediction

In this section, we restate and proof Theorem 3.6.

Note the proof of equivalence results for the test data in previous sections automatically gives
us an equivalence results of the prediction for training data. Speciﬁcally, the third conclusion in
Lemma B.21 characterizes the training prediction unn(t) throughout the training process. Thus, we
have the following theorem characterize the equivalence between training net with regularization
and kernel ridge regression for the training data.

Theorem B.29 (Equivalence between training net with regularization and kernel ridge regression
d
for training data prediction, restatement of Theorem 3.6). Given training data matrix X
×
Rn
and corresponding label vector Y

∈
Rn. Let T > 0 be the total number of iterations. Let unn(t)

Rn

∈

∈

35

Table 2: Summary of parameters of main results in Section B

Statement
Theorem B.28
Theorem B.29

T

κ
m
ǫΛ0/n 1/(κ2Λ0) Λ−
ǫ−
0
4
0 ǫ−
Λ−
1/Λ0
1

10

6n10d
1n4d

Comment

λ
1/√m test
1/√m train

Rn be the training data predictors deﬁned in Deﬁnition B.3 and Deﬁnition B.5 respectively.

and u∗ ∈
Let κ = 1 be the corresponding multiplier.

Given any accuracy ǫ
O( n4d
0ǫ ) and regularization parameter λ
λ4
δ over the random initialization, we have

network width m
1

(0, 1/10) and failure probability δ

≤

≥

∈

(0, 1/10), if κ = 1, T =
Λ0 ),
∈
O( 1
√m ), then with probability at least

O( 1

e

−

e

unn(T )
k

−

u∗

k2 ≤

ǫ.

e

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

e

O( n4d
0ǫ ) and ǫtrain = ǫ in Lemma B.21. We can
Proof. Let κ = 1, T =
λ4
≥
see all the conditions in Table 1 hold. Thus, the third conclusion in Lemma B.21 holds. So with
probability 1

e
δ, we have

√m ), m

Λ0 ), λ

O( 1

O( 1

≤

e

e

−

unn(T )
k

−

u∗

2
2 ≤
k
≤

≤

max

max
ǫ2

exp(
{
exp(
{

−

−

(κ2Λ0 + λ)t/2)
(Λ0 + λ)T /2)

unn(0)

· k
unn(0)

−
u∗

2, ǫ2
2
u∗
k
2, ǫ2
2
k

train}
}

−

· k

where the ﬁrst step follows from Lemma B.21, the second step follows from κ = 1 and ǫtrain = ǫ,
the last step follows from T =

n with high probability.

O( 1

Λ0 ) and

unn(0)
k

−

u∗k

2
2 ≤

C Generalization result of leverage score sampling for approximat-

e

ing kernels

In this section, we generalize the result of Lemma 8 in [AKM+17] for a more broad class of kernels
and feature vectors. Speciﬁcally, we prove Theorem 3.3.

Section C.1 introduces the related kernel and random features, we also restate Deﬁnition 3.1
and 3.2 for leverage score sampling and random features in this section. Section C.2 restates and
proves our main result Theorem 3.3.

C.1 Preliminaries

Deﬁnition C.1 (Kernel). Consider kernel function k : Rd

Rd

×

→

R which can be written as

[φ(x, w)⊤φ(z, w)],
p

K(x, z) = E
w
∼
Rd2 denotes a ﬁnite dimensional vector and p : Rd1

Rd1

Rd, where φ : Rd

for any data x, z
R
kernel matrix K

≥

∈

∈

Rn

n as

×

0 denotes probability density function. Given data x1,

×

→

→
Rd, we deﬁne the corresponding

, xn ∈

· · ·

Ki,j = K(xi, xj) = E
w
∼

[φ(xi, w)⊤φ(xj, w)]
p

36

Deﬁnition C.2 (Random features). Given m weight vectors w1,
be deﬁne as

, wm ∈

· · ·

Rd1. Let ϕ : Rd

Rmd2

→

ϕ(x) =

φ(x, w1)⊤,

,

1
√m

· · ·

1
√m

h

φ(x, wm)⊤

⊤

i

If w1,

), then
, wm are drawn according to p(
·

· · ·

K(x, z) = E
p

[ϕ(x)⊤ϕ(z)]

Given data matrix X = [x1,

· · ·

Rn

×

, xn]⊤ ∈
Φ(w) = [φ(x1, w)⊤,

d, deﬁne Φ : Rd1

Rn

d2 as

×

→

, φ(xn, w)⊤]⊤

· · ·

), then
If w are drawn according to p(
·

K = E
p

[Φ(w)Φ(w)⊤]

(57)

Further, deﬁne Ψ

Rn

md2 as

×

∈

Then we have

Ψ = [ϕ(x1),

, ϕ(xn)]⊤.

· · ·

ΨΨ⊤ =

1
m

m

r=1
X

Φ(wr)Φ(wr)⊤

If w1,

), then
, wm are drawn according to p(
·

· · ·

K = E
p

[ΨΨ⊤]

Deﬁnition C.3 (Modiﬁed random features, restatement of Deﬁnition 3.2). Given any probability
Rd1.
density function q(
). Given m weight vectors w1,
) whose support includes that of p(
·
·
Let ϕ : Rd

Rmd2 be deﬁned as

, wm ∈

· · ·

→

ϕ(x) =

1
√m

p(w1)
q(w1)

φ(x, w1)⊤,

h p
p

If w1,

· · ·

), then
, wm are drawn according to q(
·

,

· · ·

p(wm)
q(wm)

p

p

φ(x, wm)⊤

⊤

i

K(x, z) = E
q

[ϕ(x)⊤ϕ(z)]

Given data matrix X = [x1,

Φ(w) =

· · ·

, xn]⊤ ∈
p(w)

Rn

×

d, deﬁne Φ : Rd1

Rn

d2 as

×

→

[φ(x1, w)⊤,

q(w)

p

, φ(xn, w)⊤]⊤ =

· · ·

Φ(w)

p(w)

q(w)

p

p

p
), then
If w are drawn according to q(
·

K = E
q

[Φ(w)Φ(w)⊤]

37

Further, deﬁne Ψ

Rn

md2 as

×

∈

then

Ψ = [ϕ(x1),

, ϕ(xn)]⊤.

· · ·

ΨΨ⊤ =

1
m

m

Xr=1

Φ(wr)Φ(wr)⊤ =

1
m

m

Xr=1

p(wr)
q(wr)

Φ(wr)Φ(wr)⊤

(58)

If w1,

), then
, wm are drawn according to q(
·

· · ·

K = E
q

[ΨΨ⊤]

Deﬁnition C.4 (Leverage score, restatement of Deﬁnition 3.1). Let p : Rd1
ability density function deﬁned in Deﬁnition C.1. Let Φ : Rd1
For parameter λ > 0, we deﬁne the ridge leverage score as

Rn

→

×

R

0 denote the prob-
d2 be deﬁned as Deﬁnition C.2.

→

≥

qλ(w) = p(w) tr[Φ(w)⊤(K + λIn)−

1Φ(w)].

Deﬁnition C.5 (Statistical dimension). Given kernel matrix K
deﬁne statistical dimension sλ(K) as:

∈

Rn

×

n and parameter λ > 0, we

sλ(K) = tr[(K + λIn)−

1K].

Note we have

Rd2 qλ(w)dw = sλ(K). Thus we can deﬁne the leverage score sampling distribu-

tion as

R

Deﬁnition C.6 (Leverage score sampling distribution). Let qλ(w) denote the leverage score deﬁned
in Deﬁnition C.4. Let sλ(K) denote the statistical dimension deﬁned in Deﬁnition C.5. We deﬁne
the leverage score sampling distribution as

q(w) =

qλ(w)
sλ(K)

.

C.2 Main result

×

≥

∈

Rd

(0,

· · ·

Rn

R, K

, xn ∈

Rd. Let k : Rd

→
Rd1. Assume seqλ =

→
Rd1
→
), let qλ : Rd1
k
R be any measurable function such that
qλ(w)dw is ﬁnite. Denote qλ(w) =

Theorem C.7 (Restatement of Theorem 3.3, generalization of Lemma 8 in [AKM+17]). Given
n be the kernel deﬁned in
n data points x1, x2,
×
Rd2 and probability density function
Deﬁnition C.1, with corresponding vector φ : Rd
R
p : Rd1
0. Given parameter λ
0 be the leverage score deﬁned
qλ : Rd1
in Deﬁnition C.4. Let
qλ(w) holds for
all w
qλ(w)/seqλ . For any accuracy
∈
Rd denote m samples
, wm ∈
parameter ǫ
e
), and construct the matrix
draw independently from the distribution associated with the density qλ(
e
·
md2 according to Deﬁnition C.3 with q = qλ. Let sλ(K) be deﬁned as Deﬁnition C.5. If
Ψ
2seqλ ln(16seqλ ·
m
(1

→
Rd1
(0, 1/2) and failure probability δ

sλ(K)/δ), then we have

(0, 1). Let w1,

Rn
×
3ǫ−

(1 + ǫ)

qλ(w)

K
k

∈
≥

(59)

· · ·

(K + λIn)

(K + λIn)

→

ǫ)

≥

×

R

∈

∈

∈

e

e

R

≥

ΨΨ⊤ + λIn (cid:22)

(cid:22)

·

−

·

holds with probability at least 1

δ.

−

38

To prove the theorem, we follow the same proof framework as Lemma 8 in [AKM+17].

Proof. Let K + λIn = V ⊤Σ2V be an eigenvalue decomposition of K + λIn. Note that Eq. (59) is
equivalent to

K

−

ǫ(K + λIn)

ΨΨ⊤

(cid:22)

(cid:22)

K + ǫ(K + λIn).

(60)

1V on the left and V ⊤Σ−

1 on the left for both sides of Eq. (60), it suﬃces to show

Multiplying Σ−
that

1V ΨΨ⊤V ⊤Σ−

1

Σ−
k

−

Σ−

1V KV ⊤Σ−

1

ǫ

k ≤

(61)

holds with probability at least 1

δ. Let

−

We have

Yr =

p(wr)
qλ(wr)

Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1.

E
qλ

[Yl] = E
qλ

[

p(wr)
qλ(wr)

Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1]

= Σ−

= Σ−

Φ(wr)Φ(wr)⊤]V ⊤Σ−

[

p(wr)
qλ(wr)
[Φ(wr)Φ(wr)⊤]V ⊤Σ−

1

1V E
qλ
1V E
p

1

= Σ−

1V KV ⊤Σ−

1

where the ﬁrst step follows from the deﬁnition of Yr, the second step follows the linearity of expec-
tation, the third step calculations the expectation, and the last step follows Eq. (57).

Also we have

1
m

m

r=1
X

Yr =

1
m

= Σ−

m

r=1
X
1V

p(wr)
qλ(wr)
m

1
m

(cid:16)

Xr=1

= Σ−

1V ΨΨ⊤V ⊤Σ−

1

Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1

p(wr)
qλ(wr)

Φ(wr)Φ(wr)⊤

V ⊤Σ−

1

(cid:17)

where the ﬁrst step follows from the deﬁnition of Yr, the second step follows from basic linear algebra,
and the last step follows from Eq. (58).

Thus, it suﬃces to show that

1
m

k

m

r=1
X

Yr −

E
qλ

[Yl]

k ≤

ǫ

(62)

holds with probability at least 1

δ.

−

39

We can apply matrix concentration Lemma A.8 to prove Eq. (62), which requires us to bound

Yrk
k

and E[Y 2

l ]. Note

tr[Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1]

tr[Φ(wr)⊤V ⊤Σ−

1Σ−

1V Φ(wr)]

tr[Φ(wr)⊤(K + λIn)−

1Φ(wr)]

Ylk ≤
k

=

=

=

≤

p(wr)
qλ(wr)
p(wr)
qλ(wr)
p(wr)
qλ(wr)
qλ(wr)sqλ
qλ(wr)

sqλ.
e

where the ﬁrst step follows from
tr[A] for any positive semideﬁnite matrix, the second step
follows tr[AB] = tr[BA], the third step follows from the deﬁnition of V, Σ, the fourth step follows
) as deﬁned in Deﬁnition C.4, and the last step follows from
from the deﬁnition of leverage score qλ(
·
the condition

A
k

k ≤

qλ(w).

qλ(w)
Further, we have

≥

e
Y 2
r =

=

(cid:22)

=

=

=

(cid:22)

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1

1V Φ(wr)Φ(wr)⊤(K + λIn)−

1Φ(wr)Φ(wr)⊤V ⊤Σ−

1

p(wr)2
qλ(wr)2 Σ−
p(wr)2
qλ(wr)2 Σ−
p(wr)2
qλ(wr)2 tr[Φ(wr)⊤(K + λIn)−
p(wr)qλ(wr)

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1

1Φ(wr)]Σ−

1V Φ(wr)Φ(wr)⊤V ⊤Σ−

1

qλ(wr)2 Σ−
Yr

qλ(wr)
qλ(wr)
qλ(wr)seqλ
qλ(wr)

Yr

seqλYr.
e

where the ﬁrst step follows from the deﬁnition of Yr, the second step follows from the deﬁnition of
tr[A] for any positive semideﬁnite matrix, the fourth step
V, Σ, the third step follows from
) as deﬁned in Deﬁnition C.4, the ﬁfth step follows
follows from the deﬁnition of leverage score qλ(
·
from the deﬁnition of Yr, the sixth step follows from the deﬁnition of qλ(
), and the last step follows
·
from the condition
Thus, let λ1 ≥

λn be the eigenvalues of K, we have

qλ(w).

A
k

k ≤

qλ(w)
≥
λ2 ≥ · · · ≥
e
E
[Y 2
r ]
qλ

[seqλYr]

E
(cid:22)
qλ
= seqλΣ−
= seqλ(In −
diag
= seqλ ·

1

1V KV Σ−
2)
λΣ−
λ1/(λ1 + λ),
{

40

, λn/(λn + λ)
}

· · ·

:= D.

So by applying Lemma A.8, we have

1
m

m

r=1
X

Yr −

Pr

(cid:13)
(cid:13)
(cid:13)

E[Yr]
(cid:13)
(cid:13)
(cid:13)

ǫ]

≥

≤

≤

exp

8 tr[D]
D
k
k
(cid:16)
sλ(K)
8seqλ ·
λ1/(λ1 + λ)

−
k

D
k

exp

(cid:16)
sλ(K) exp

mǫ2/2
+ 2seqλǫ/3

(cid:17)
mǫ2
2seqλ(1 + 2ǫ/3)

−

mǫ2
2seqλ(1 + 2ǫ/3)

−

(cid:17)

(cid:17)

sλ(K) exp

(cid:16)

(cid:16)

−

3mǫ2
8seqλ

(cid:17)

16seqλ ·

≤

16seqλ ·
δ

≤

≤

where the ﬁrst step follows from Lemma A.8, the second step follows from the deﬁnition of D
n
1K] =
i λi/(λi + λ) = tr[D], the third step follows the condition
and sλ(K) = tr[(K + λIn)−
(0, 1/2), and the last step follows from the
), the fourth step follows the condition ǫ
(0,
λ
k
bound on m.

K
k

P

∈

∈

Remark C.8. Above results can be generalized to C. Note in the random Fourier feature case,
C and p(
we have d1 = d, d2 = 1, φ(x, w) = e−
) denotes the Fourier transform density
·
distribution. In the Neural Tangent Kernel case, we have d1 = d2 = d, φ(x, w) = xσ′(w⊤x) and
p(
(0, Id). So they are
) denotes the probability density function of standard Gaussian distribution
·
both special cases in our framework.

2πiw⊤x

N

∈

D Equivalence between training neural network with regularization
and kernel ridge regression under leverage score sampling

In this section, we connected the neural network theory with the leverage score sampling theory
by showing a new equivalence result between training reweighed neural network with regularization
under leverage score initialization and corresponding neural tangent kernel ridge regression. Specif-
ically, we prove Theorem D.21. Due to the similarity of the results to Section B, we present this
section in the same framework.

Section D.1 introduces new notations and states the standard data assumptions again. Sec-
tion D.2 restates and supplements the deﬁnitions in the paper. Section D.3 presents the key lemmas
about the leverage score initialization and related properties, which are crucial to the proof. Sec-
tion D.4 provides a brief proof sketch. Section D.5 restates and proves the main result Theorem D.21
following the proof sketch.

Here, we list the locations where deﬁnitions and theorems in the paper are restated. Deﬁni-

tion 3.8 is restated in Deﬁnition D.2. Theorem 3.9 is restated in Theorem D.21.

D.1 Preliminaries

Let’s deﬁne the following notations:

•

•

untk(t) = κf ntk(β(t), X) = κΦ(X)β(t)
for the training data with respect to H(0) at time t. (See Deﬁnition D.4)

∈

Rn be the prediction of the kernel ridge regression

u∗ = limt

→∞

untk(t) (See Eq. (70))

41

untk,test(t) = κf ntk(β(t), xtest) = κΦ(xtest)β(t)
regression for the test data with respect to H(0) at time t. (See Deﬁnition D.4)

∈

R be the prediction of the kernel ridge

untk,test(t) (See Eq. (71))

→∞

u∗test = limt
Kt(xtest, X)
where

Rn be the induced kernel between the training data and test data at time t,

∈

[Kt(xtest, X)]i = Kt(xtest, xi) =

∂f (W (t), xtest)
∂W (t)

,

∂f (W (t), xi)
∂W (t)

(cid:28)

(cid:29)

(see Deﬁnition D.3)

unn(t) = κf nn(W (t), X)
score initialization for the training data at time t. (See Deﬁnition D.2)

Rn be the prediction of the reweighed neural network with leverage

∈

unn,test(t) = κf nn(W (t), xtest)
leverage score initialization for the test data at time t (See Deﬁnition D.2)

R be the prediction of the reweighed neural network with

∈

•

•

•

•

•

Assumption D.1 (data assumption). We made the following assumptions:
yi|
1. For each i
= O(1).
|
2. H cts is positive deﬁnite, i.e., Λ0 := λmin(H cts) > 0.
3. All the training data and test data have Euclidean norm equal to 1.

[n], we assume

∈

D.2 Deﬁnitions

(0, 1) be a small multiplier. Let λ

Deﬁnition D.2 (Training reweighed neural network with regularization, restatement of Deﬁni-
Rn. Let
tion 3.8). Given training data matrix X
∈
(0, 1) be the regularization parameter. Given any prob-
κ
∈
R>0. Let p(
) : Rd
ability density distribution q(
(0, Id).
) denotes the Gaussian distribution
·
·
i.i.d.
We initialize the network as ar
q. Then we consider solving the
] and wr(0)
1, 1
}
∼
following optimization problem using gradient descent:

d and corresponding label vector Y

→
unif[

i.i.d.
∼

{−

Rn

N

∈

∈

×

min
W

1
2 k

Y

−

κf nn(W, X)

k2 +

1
2

λ

W
k

2
F .
k

where f nn(W, x) = 1
√m
Rn. We denote wr(t), r

m
r=1 arσ(w⊤r X)
[m] as the variable at iteration t. We denote

p(wr(0))
q(wr(0)) and f nn(W, X) = [f nn(W, x1),

q

P
∈

(63)

, f nn(W, xn)]⊤ ∈

· · ·

unn(t) = κf nn(W (t), X) =

κ
√m

m

r=1
X

arσ(wr(t)⊤X)

s

p(wr(0))
q(wr(0))

as the training data predictor at iteration t. Given any test data xtest ∈

Rd, we denote

unn,test(t) = κf nn(W (t), xtest) =

κ
√m

m

r=1
X

arσ(wr(t)⊤xtest)

s

p(wr(0))
q(wr(0))

as the test data predictor at iteration t.

(64)

(65)

42

Deﬁnition D.3 (Reweighed dynamic kernel). Given W (t)
network at training time t as deﬁned in Deﬁnition D.2. For any data x, z
R as

∈

×

∈

Rd

m as the parameters of the neural

Rd, we deﬁne Kt(x, z)

∈

Kt(x, z) =

(cid:28)

df nn(W (t), x)
dW (t)

,

df nn(W (t), z)
dW (t)

Given training data matrix X = [x1,

d, we deﬁne H

(t)

Rn

, xn]⊤ ∈

· · ·
[H(t)]i,j = Kt(xi, xj)

×

R.

∈

(cid:29)

∈

Rn

n as

×

p(w1(0))
We denote Φ(x) = [x⊤σ′(w1(0)⊤x)
q(w1(0)) ,
vector corresponding to H(0), which satisﬁes

q

· · ·

, x⊤σ′(wm(0)⊤x)

q

p(wr(0))

q(wr(0)) ]⊤ ∈

Rmd as the feature

[H(0)]i,j = Φ(xi)⊤Φ(xj)

for all i, j
xtest ∈

∈

[n]. We denote Φ(X) = [Φ(x1),

Rd, we deﬁne Kt(xtest, X)

Rn as

∈

, Φ(xn)]⊤ ∈

· · ·

Rn

×

md. Further, given a test data

Kt(xtest, X) = [Kt(xtest, x1),

, Kt(xtest, xn)]⊤

Rn.

∈

· · ·

×

d and corresponding label vector Y

Deﬁnition D.4 (Kernel ridge regression with H(0)). Given training data matrix X = [x1,
Rn
kernel and corresponding feature functions deﬁned as in Deﬁnition D.3. Let κ
multiplier. Let λ
∈
tangent kernel ridge regression problem:

, xn]⊤ ∈
n and Φ be the neural tangent
(0, 1) be a small
(0, 1) be the regularization parameter. Then we consider the following neural

Rn. Let K0, H(0)

Rn

· · ·

∈

∈

∈

×

min
β

1
2 k

Y

2
2 +
κf ntk(β, X)
k

−

1
2

λ

β
k

2
2.
k

(66)

where f ntk(β, x) = Φ(x)⊤β denotes the prediction function is corresponding RKHS and f ntk(β, X) =
Rn. Consider the gradient ﬂow of solving problem (66) with initial-
[f ntk(β, x1),
, f ntk(β, xn)]⊤ ∈
ization β(0) = 0. We denote β(t)

Rmd as the variable at iteration t. We denote

· · ·

∈

untk(t) = κΦ(X)β(t)

as the training data predictor at iteration t. Given any test data xtest ∈

Rd, we denote

untk,test(t) = κΦ(xtest)⊤β(t)

(67)

(68)

as the test data predictor at iteration t. Note the gradient ﬂow converge the to optimal solution of
problem (66) due to the strongly convexity of the problem. We denote

β∗ = lim
t
→∞

β(t) = κ(κ2Φ(X)⊤Φ(X) + λI)−

1Φ(X)⊤Y

and the optimal training data predictor

u∗ = lim
t
→∞

untk(t) = κΦ(X)β∗ = κ2H(0)(κ2H(0) + λI)−

1Y

and the optimal test data predictor

(69)

(70)

u∗test = lim
t
→∞

untk,test(t) = κΦ(xtest)⊤β∗ = κ2K0(xtest, X)⊤(κ2H(0) + λI)−

1Y.

(71)

43

D.3 Leverage score sampling, gradient ﬂow, and linear convergence

Recall in the main body we connect the leverage score sampling theory and convergence theory of
the neural network training by observing

Kntk(x, z) = E

∂fnn(W, x)
∂W

,

∂fnn(W, z)
∂W

(cid:20)(cid:28)

(cid:29)(cid:21)

[φ(x, w)⊤φ(z, w)]
p

= E
w
∼
Rd and p(
) denotes the probability density function of standard
·
(0, Id). Thus, given regularization parameter λ > 0, we can deﬁne the ridge

∈

where φ(x, w) = xσ′(w⊤x)
Gaussian distribution
leverage function with respect to H cts deﬁned in Deﬁnition B.4 as

N

qλ(w) = p(w) tr[Φ(w)⊤(H cts + λIn)−

1Φ(w)]

and corresponding probability density function

q(w) =

qλ(w)
sλ(H cts)

(72)

where Φ(w) = [φ(x1, w)⊤,

, φ(xn, w)⊤]⊤ ∈
Lemma D.5 (property of leverage score sampling distribution). Let p(
) denotes the standard
·
) be deﬁned as in (72). Assume tr[Φ(w)Φ(w)⊤] = O(n) and
Gaussian distribution
(0, Id). Let q(
·
Rd we have
λ

Λ0/2. Then for all w

· · ·

N

×

Rn

d2.

≤

∈

where c1 = O( 1

n ) and c2 = O( 1

Λ0 ).

c1p(w)

q(w)

≤

≤

c2p(w)

Proof. Note by assumption sλ(H cts) = tr[(H cts +λIn)−

1H cts] = O(n). Further, note for any w

Rd,

∈

qλ(w) = p(w) tr[Φ(w)⊤(H cts + λIn)−

1Φ(w)]

p(w) tr[Φ(w)⊤Φ(w)]

≤

1
Λ0 + λ

·

n

= p(w) tr[

φ(xi, w)φ(xi, w)⊤]

n

Xi=1
tr[φ(xi, w)φ(xi, w)⊤]

= p(w)

Xi=1
n

= p(w)

tr[φ(xi, w)⊤φ(xi, w)]

1
Λ0 + λ

1
Λ0 + λ

1
Λ0 + λ

·

·

·

Xi=1
n

= p(w)

p(w)

≤

2σ′(w⊤xi)2
2

xik
k
Xi=1
n
Λ0 + λ

1
Λ0 + λ

·

where the ﬁrst step follows from H cts
Λ0In, the second step follows from the deﬁnition of Φ, the
third step follows from the linearity of trace operator, the fourth step follows from tr(AB) = tr(BA),
1.
the ﬁfth step follows from the deﬁnition of φ, and the last step follows from

(cid:23)

)
xik2 = 1 and σ′(
·
k

≤

44

Thus, combining above facts, we have

q(w) =

qλ(w)
sλ(H cts) ≤

p(w)

n
(Λ0 + λ)sλ(H cts)

= c2p(w)

hold for all w

Similarly, note H cts

∈

Rd, where c2 = O( 1

Λ0 ).
nIn, we have

(cid:22)

qλ(w)

≥

p(w) tr[Φ(w)⊤Φ(w)]

1
n + λ

·

which implies

q(w) =

qλ(w)
sλ(H cts) ≥

p(w)

tr[Φ(w)⊤Φ(w)]
(n + λ)sλ(H cts)

= c1p(w)

hold for all w

∈

Rd, where c1 = O( 1

n ). Combining above results, we complete the proof.

Lemma D.6 (leverage score sampling). Let H(t), H cts be the kernel deﬁned as in Deﬁnition D.3
and Deﬁnition B.4. Let Λ0 > 0 be deﬁned as in Deﬁnition B.4. Let p(
) denotes the probability
·
) denotes the leverage sampling distribution with
(0, Id). Let q(
density function for Gaussian
·
(0, 1/4). Then we have
), H(0) and λ deﬁned in Deﬁnition C.6. Let ∆
respect to p(
·

N

∈

[H(0)] = H cts.

E
q

By choosing m

≥

O(∆−

2sλ(H cts), with probability at least 1

δ,

−

e

(1

−

∆)(H cts + λI)

H(0) + λI

(cid:22)

(cid:22)

(1 + ∆)(H cts + λI)

Further, if λ

≤

Λ0, we have with probability at least 1

δ,

−

H(0)

Λ0
2

In.

(cid:23)

Here

) hides poly log(sλ(H(0))/δ).
O(
·

Proof. Note for any i, j

e

[n],

[H(0)]]i,j = E
q

[Φ(xi)⊤Φ(xj)]

∈
[E
q

df nn(W (0), xi)
dW
dfnn(W (0), xi)
dW

,

,

df nn(W (0), xj )
dW
dfnn(W (0), xj )
dW

]
i

]
i

= E
[
h
q

= E
[
h
p
= [H cts]i,j

where the ﬁrst step follows from the deﬁnition of H(0), the second step follows the deﬁnition of Φ,
the third step calculates the expectation, and the last step follows the deﬁnition of H cts.

Also, by applying Theorem C.7 directly, we have with probability at least 1

δ,

−

∆)(H cts + λI)

(1

−

H(0) + λI

(cid:22)

(cid:22)

(1 + ∆)(H cts + λI)

45

if m

O(∆−

≥
Since
e

we have

2sλ(H(0)).

H(0) + λI

(1

−

(cid:23)

∆)(H cts + λI)

(1

−

(cid:23)

∆)(Λ0 + λ)In,

H(0)

[(1

−

(cid:23)

∆)(Λ0 + λ)

λ]In (cid:23)

−

Λ0
2

In,

which completes the proof.

Lemma D.7 (Gradient ﬂow of kernel ridge regression, parallel to Lemma B.7). Given training data
(0, 1) and
matrix X
∈
∈
Rn be deﬁned as in Deﬁnition D.4. Let Φ, Kt be deﬁned as in Deﬁnition D.3. Then for
untk(t)
any data z

d and corresponding label vector Y

Rn. Let f ntk, β(t)

Rd, we have

Rmd, κ

Rn

∈

∈

∈

×

∈

df ntk(β(t), z)
dt

= κ

K0(z, X)⊤(Y

untk(t))

λ

·

−

−

f ntk(β(t), z).

Proof. Denote L(t) = 1

2
2. By the rule of gradient descent, we have
β(t)
k
k

Y
2 k

·
2 + 1
2
untk(t)
2 λ
k
dL
dβ

=

−

−
dβ(t)
dt

= κΦ(X)⊤(Y

untk(t))

λβ(t).

−

−

Thus we have

df ntk(β(t), z)
dt

=

df ntk(β(t), z)
dβ(t)

dβ(t)
dt

= Φ(z)⊤(κΦ(X)⊤(Y
= κK0(z, X)⊤(Y
= κK0(z, X)⊤(Y

−

−

untk(t))

λβ(t))

−

−

untk(t))

λΦ(z)⊤β(t)

untk(t))

λf ntk(β(t), z),

−

−

where the ﬁrst step is due to chain rule, the second step follows from the fact df ntk(β, z)/dβ = Φ(z),
the third step is due to the deﬁnition of the kernel K0(z, X) = Φ(X)Φ(z)
Rn, and the last step is
due to the deﬁnition of f ntk(β(t), z)

R.

∈

∈

Corollary D.8 (Gradient of prediction of kernel ridge regression, parallel to Corollary B.8). Given
Rn. Given a
training data matrix X = [x1,
∈
R be deﬁned as
test data xtest ∈
in Deﬁnition D.4. Let Kt, H(0)

d and corresponding label vector Y
×
Rn and untk,test(t)
(0, 1), untk(t)

n be deﬁned as in Deﬁnition D.3. Then we have

, xn]⊤ ∈
Rmd, κ
∈
Rn
×

· · ·
Rd. Let f ntk, β(t)

Rn

∈

∈

∈

∈
= κ2H(0)(Y

duntk(t)
dt

untk(t))

λ

·

−

untk(t)

−

duntk,test(t)
dt
Proof. Plugging in z = xi ∈

= κ2K0(xtest, X)⊤(Y

−
Rd in Lemma D.7, we have

untk(t))

λ

·

−

untk,test(t).

df ntk(β(t), xi)
dt

= κK0(xi, X)⊤(Y

untk(t))

λ

·

−

−

f ntk(β(t), xi).

Note [untk(t)]i = κf ntk(β(t), xi) and [H(0)]:,i = K0(xi, X), so writing all the data in a compact form,
we have

duntk(t)
dt

= κ2H(0)(Y

untk(t))

λ

·

−

untk(t).

−

46

Plugging in data z = xtest ∈

Rd in Lemma D.7, we have

df ntk(β(t), xtest)
dt

= κK0(xtest, X)⊤(Y

untk(t))

λ

·

−

−

f ntk(β(t), xtest).

Note by deﬁnition, untk,test(t) = κf ntk(β(t), xtest)

R, so we have

∈

duntk,test(t)
dt

= κ2K0(xtest, X)⊤(Y

untk(t))

λ

·

−

−

untk,test(t).

Lemma D.9 (Linear convergence of kernel ridge regression, parallel to Lemma B.9). Given training
data matrix X = [x1,
(0, 1),
· · ·
Rn be deﬁned as in Deﬁnition D.4. Let Λ0 > 0 be deﬁned as in Deﬁnition B.4.
Rn and u∗ ∈
untk(t)
Let λ > 0 be the regularization parameter. Then we have

d and corresponding label vector Y

, xn]⊤ ∈

Rn. Let κ

Rn

∈

∈

∈

×

Further, we have

Proof. Let H(0)

Rn

×

∈
κ2H(0)(Y

d

untk(t)
k
dt

2
2

u∗k

−

(κ2Λ0 + λ)
untk(t)
k

−

u∗

2
2.
k

≤ −

untk(t)
k

−

u∗

k2 ≤

e−

(κ2Λ0+λ)t/2

untk(0)
k

u∗

k2.

−

n be deﬁned as in Deﬁnition D.3. Then

u∗) = κ2H(0)(Y

−

κ2H(0)(κ2H(0) + λIn)−
κ2H(0))(κ2H(0) + λI)−

1Y )
1)Y

−
= κ2H(0))(In −
= κ2H(0)(κ2H(0) + λIn −
= κ2λH(0)(κ2H(0) + λIn)−
= λu∗,

κ2H(0))(κ2H(0) + λIn)−
1Y

1Y

(73)

where the ﬁrst step follows the deﬁnition of u∗ ∈
and the last step use the deﬁnition of u∗ ∈
d

= 2(untk(t)

u∗)⊤

−

duntk(t)
dt

untk(t)
k
dt

2
2

u∗k

−

Rn, the second to fourth step simplify the formula,

Rn again. So we have

−

−

2κ2(untk(t)
2κ2(untk(t)
2λ(untk(t)
−
2κ2(untk(t)
2λ(untk(t)

−

=

=

=

=

−

−

−

−

−

−

≤ −

u∗)⊤H(0)(untk(t)

u∗)⊤H(0)(untk(t)

Y )
2λ(untk(t)
−
u∗) + 2κ2(untk(t)

−

u∗)⊤untk(t)

u∗)⊤H(0))(Y

u∗)

−

−

u∗)⊤untk(t)

u∗)⊤H(0)(untk(t)

u∗) + 2λ(untk(t)

u∗)⊤u∗

−

−

−

−

u∗)⊤untk(t)
−
u∗)⊤(κ2H(0) + λI)(untk(t)
2(untk(t)
−
(κ2Λ0 + λ)
2
2,
untk(t)
k
k

u∗

−

u∗)

−

(74)

where the ﬁrst step follows the chain rule, the second step follows Corollary D.8, the third step uses
basic linear algebra, the fourth step follows Eq. (73), the ﬁfth step simpliﬁes the expression, and

47

the last step follows from Lemma D.6. Further, since

d(e(κ2Λ0+λ)t

untk(t)
k
dt

2
2)
u∗k

−

= (κ2Λ0 + λ)e(κ2Λ0+λ)t

untk(t)
k

−

u∗

2 + e(κ2Λ0+λ)t
2
k

·

d

untk(t)
k
dt

2
2

u∗k

−

0,

≤

where the ﬁrst step calculates the gradient, and the second step follows from Eq. (74). Thus,
e(κ2Λ0+λ)t

2
2 is non-increasing, which implies

untk(t)
k

u∗k

−

untk(t)
k

−

u∗

k2 ≤

e−

(κ2Λ0+λ)t/2

untk(0)
k

u∗

k2.

−

Lemma D.10 (Gradient ﬂow of neural network training, Parallel to Lemma B.10). Given training
data matrix X
Rd
m, κ
deﬁned as in Deﬁnition D.3. Then for any data z

×
∈
Rn be deﬁned as in Deﬁnition D.2. Let Kt : Rd
Rd, we have

d and corresponding label vector Y

×
(0, 1) and unn(t)

Rn. Let f nn : Rd
×

R, W (t)
d

∈
Rn be

→
Rn
×

Rn

Rd

→

×

∈

∈

∈

m

×

∈

df nn(W (t), z)
dt

= κKt(z, X)⊤(Y

unn(t))

λ

·

−

−

f nn(W (t), z).

Proof. Denote L(t) = 1

Y
2 k

2 + 1
2
unn(t)
2 λ
k

−
dwr
dt

=

∂L
∂wr

−

2
F . By the rule of gradient descent, we have
W (t)
k
k
∂unn
∂wr

λwr.

)⊤(Y

unn)

−

−

= (

Also note for ReLU activation σ, we have

df nn(W (t), z)
dW (t)

h

, λW (t)
i

=

=

=

m

Xr=1 (cid:16)
λ
√m

λ
√m

Xr=1
m

arzσ′(wr(t)⊤z)

s

1
√m

m

p(wr(0))
q(wr(0))

arwr(t)⊤zσ′(wr(t)⊤z)

s

(cid:17)
p(wr(0))
q(wr(0))

⊤(λwr(t))

arσ(wt(t)⊤z)

p(wr(0))
q(wr(0))

s

Xr=1
= λf nn(W (t), z),

where the ﬁrst step calculates the derivatives, the second step follows basic linear algebra, the third
step follows the property of ReLU activation: σ(l) = lσ′(l), and the last step follows from the

48

(75)

(76)

deﬁnition of f nn. Thus, we have

df nn(W (t), z)
dt
df nn(W (t), z)
dW (t)

h

,

dW (t)
dt

i

n

(yj −

Xj=1
n

κf nn(W (t), xj))
h

df nn(W (t), z)
dW (t)

,

dκf nn(W (t), xj)
dW (t)

df nn(W (t), z)
dW (t)

, λW (t)
i

i − h

=

=

= κ

(yj −

κf nn(W (t), xj ))Kt(z, xj )

λ

·

−

f nn(W (t), z)

Xj=1

= κKt(z, X)⊤(Y

unn(t))

λ

·

−

−

f nn(W (t), z),

where the ﬁrst step follows from chain rule, the second step follows from Eq. (75), the third step
follows from the deﬁnition of Kt and Eq. (76), and the last step rewrites the formula in a compact
form.

Corollary D.11 (Gradient of prediction of neural network, Parallel to Lemma B.11). Given training
Rn. Given a test data
data matrix X = [x1,
· · ·
Rn be deﬁned as in
Rd. Let f nn : Rd
m
xtest ∈
×
∈
Deﬁnition D.2. Let Kt : Rd
n be deﬁned as in Deﬁnition D.3. Then we
have

d and corresponding label vector Y
m, κ
×
Rn

, xn]⊤ ∈
Rd
×
Rn
×

∈
(0, 1) and unn(t)

×
R, W (t)

∈
Rn, H(t)

→
d
×

Rn

Rd

→

∈

∈

×

dunn(t)
dt

dunn,test(t)
dt

= κ2H(t)(Y

unn(t))

λ

·

−

unn(t)

−

= κ2Kt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

unn,test(t).

Proof. Plugging in z = xi ∈

Rd in Lemma D.10, we have

df nn(W (t), xi)
dt

= κKt(xi, X)⊤(Y

unn(t))

λ

·

−

−

f nn(W (t), xi).

Note [unn(t)]i = κf nn(W (t), xi) and [H(t))]:,i = Kt(xi, X), so writing all the data in a compact
form, we have

dunn(t)
dt

= κ2H(t)(Y

unn(t))

λ

·

−

unn(t).

−

Plugging in data z = xtest ∈

Rd in Lemma D.10, we have

df nn(W (t), xtest)
dt

= κKt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

f nn(W (t), xtest).

Note by deﬁnition, unn,test(t) = κf nn(W (t), xtest), so we have

dunn,test(t)
dt

= κ2Kt(xtest, X)⊤(Y

unn(t))

λ

·

−

−

unn,test(t).

49

×

· · ·

Rn

, xn]⊤ ∈

Lemma D.12 (Linear convergence of neural network training, Parallel to Lemma B.12). Given
training data matrix X = [x1,
(0, 1)
Rn be deﬁned in Eq. (70). Let
and unn(t)
∈
Rn
n be deﬁned as in Deﬁnition D.3. Let λ
H(t)
(0, Λ0) be the regularization parameter.
×
∈
H(t)
Assume
k
unn(t)
k
dt

Rn
n be deﬁned as in Deﬁnition D.2. Let u∗ ∈
H(0)

d and corresponding label vector Y

(κ2Λ0 + λ)
unn(t)
k

Λ0/4 holds for all t

[0, T ]. Then we have

2 + 2κ2
2
k

k ≤
1
2

−
2
u∗k
2

Rn. Let κ

Y
k2 · k

H(t)
k

unn(t)

H(0)

k2.

≤ −

k · k

u∗

u∗

u∗

−

−

−

−

−

∈

∈

∈

∈

d

×

Proof. Note same as in Lemma D.9, we have

κ2H(0)(Y

−

u∗) = κ2H(0)(Y

κ2H(0)(κ2H(0) + λIn)−
κ2H(0))(κ2H(0) + λI)−

1Y )
1)Y

−
= κ2H(0))(In −
= κ2H(0)(κ2H(0) + λIn −
= κ2λH(0)(κ2H(0) + λIn)−
= λu∗,

κ2H(0))(κ2H(0) + λIn)−
1Y

1Y

(77)

where the ﬁrst step follows the deﬁnition of u∗ ∈
and the last step use the deﬁnition of u∗ ∈

Rn again. Thus, we have

Rn, the second to fourth step simplify the formula,

d

unn(t)
k
dt

−

2
2

u∗k

= 2(unn(t)

u∗)⊤

=

=

=

=

−

−
2κ2(unn(t)
2κ2(unn(t)
−
+ 2κ2(unn(t)
2κ2(unn(t)
−
+ 2κ2(unn(t)
2(unn(t)
1
2

−

−

dunn(t)
dt

u∗)⊤H(t)(unn(t)

u∗)⊤H(t)(unn(t)

−

u∗)⊤(H(t)

−

−
H(0))(Y

−

−

−

u∗)⊤unn(t)

−

Y )
2λ(unn(t)
−
u∗) + 2κ2(unn(t)
u∗)

−

−

u∗) + 2λ(unn(t)

−

2λ(unn(t)

−
u∗)⊤u∗

u∗)⊤H(0)(Y

u∗)

−
u∗)⊤unn(t)

u∗)⊤H(t)(unn(t)

−

u∗)⊤(H(t)

−
H(0)))(Y
−
−
u∗)⊤(κ2H(t) + λI)(unn(t)

−

−
2λ(unn(t)

u∗)
−
u∗) + 2κ2(unn(t)

−

u∗)⊤unn(t)

u∗)⊤(H(t)

−

≤ −

(κ2Λ0 + λ)
unn(t)
k

u∗

2 + 2κ2
2
k

−

H(0))

unn(t)

kk

u∗

Y
k2k

−

−

−

−
H(t)
k

−
u∗

H(0))(Y

u∗)

−

k2

where the ﬁrst step follows the chain rule, the second step follows Corollary D.11, the third step
uses basic linear algebra, the fourth step follows Eq. (77), the ﬁfth step simpliﬁes the expression,
and the last step follows the assumption

Λ0/4 and the fact

Λ0/2.

H(0))

H(t)
k

−

k ≤

H(0))
k

k ≤

D.4 Proof sketch

We introduce a new kernel ridge regression problem with respect to H(0) to decouple the prediction
perturbation resulted from initialization phase and training phase. Speciﬁcally, given any accuracy
ǫ

(0, 1), we divide this proof into following steps:

∈

1. Firstly, we bound the prediction perturbation resulted from initialization phase
ǫ/2 by applying the leverage score sampling theory, as shown in Lemma D.13.

u∗ −
k

u∗k2 ≤

2. Then we use the similar idea as section B to bound the prediction perturbation resulted from
ǫ/2 by showing the over-parametrization and convergence

training phase
property of neural network inductively, as shown in Lemma D.14 and Corollary D.20.

unn(T )
k

u∗k2 ≤

−

50

3. Lastly, we combine the results of step 1 and 2 using triangle inequality to show

ǫ, as shown in Theorem D.21.

u∗k2 ≤
D.5 Main result

In this section, we prove Theorem D.21 following the above proof sketch.

unn(T )
k

−

D.5.1 Upper bounding

u∗ −
u∗k2
k
Rn and u∗ ∈
Lemma D.13. Let u∗ ∈
tion B.5 and Deﬁnition D.4. Let H(0)
∈
ability density function for Gaussian
N
), H(0) and λ deﬁned in Deﬁnition C.6. Let ∆
with respect to p(
·
then we have

Rn be the optimal training data predictors deﬁned in Deﬁni-
Rn
n be deﬁned in Deﬁnition D.3. Let p(
) denotes the prob-
·
) denotes the leverage sampling distribution
(0, Id). Let q(
·
2sλ(H cts)),

(0, 1/2). If m

O(∆−

≥

∈

×

u∗
k

−

u∗

k2 ≤

λ∆√n
Λ0 + λ

e

with probability at least 1
we have

−

δ. Particularly, given arbitrary ǫ

(0, 1), if m

∈

O( n

ǫΛ0 ) and λ

O( 1

√m ),

≤

≥

e

e

u∗
k

−

u∗

k2 ≤

ǫ/2.

Here

) hides poly log(sλ(H cts)/δ).
O(
·

Proof. Note
e

u∗ = λ(H cts + λIn)−

1Y

u∗ = λ(H(0) + λIn)−

1Y

Y

−

Y

−

u∗

−

u∗ = λ[(H cts + λIn)−

1

(H(0) + λIn)−

1]Y

−

O(∆−

2sλ(H cts), we have

∆)(H cts + λI)

e
−

H(0) + λI

(cid:22)

(cid:22)

(1 + ∆)(H cts + λI)

and

So

By Lemma D.6, if m

which implies

≥
(1

1
1 + ∆

(H cts + λI)−

1

(cid:22)

(H(0) + λI)−

1

1

−

∆

(cid:22)

1

(H cts + λI)−

1

i.e.,

∆
1 + ∆

−

(H cts + λI)−

1

Assume ∆

∈

(0, 1/2), we have

∆(H cts + λI)−

1

−

(cid:22)

(cid:22)

(H(0) + λI)−

1

(H cts + λI)−

1

−

∆

−

∆

(cid:22)

1

(H cts + λI)−

1

(H(0) + λI)−

1

(H cts + λI)−

1

−

(cid:22)

2∆(H cts + λI)−

1

(78)

51

Thus,

u∗
k

−

u∗

k2 ≤
≤

1

λ

(H cts + λIn)−
k
2λ∆

−
1
(H cts + λI)−
k

Y

kk

k2

(H(0) + λIn)−

1

Y

kk

k2

O(

≤

λ∆√n
Λ0 + λ

)

where the ﬁrst step follows from Cauchy-Schwartz inequality, the second step follows from Eq. (78),
and the last step follows from the deﬁnition of Λ0 and

Y
k

k2 = O(√n).

D.5.2 Upper bounding

unn(T )
k

u∗k2

−

×

∈

∈

d, Y

Rn and a test data xtest ∈

Lemma D.14 (Bounding kernel perturbation, Parallel to Lemma B.21). Given training data X
∈
Rn
Rd. Let T > 0 denotes the total number of iterations, m > 0
denotes the width of the network, ǫtrain denotes a ﬁxed training error threshold, δ > 0 denotes the
Rn be the training data predictors deﬁned in Deﬁnition D.2. Let
failure probability. Let unn(t)
∈
(0, 1) be the corresponding multiplier. Let Kt(xtest, X)
Rn
n, Λ0 > 0 be the
κ
×
∈
Rn be deﬁned as in Eq. (70). Let λ > 0
kernel related quantities deﬁned in Deﬁnition D.3. Let u∗ ∈
Rd
m be the parameters of the
be the regularization parameter. Let W (t) = [w1(t),
, wm(t)]
(0, 1/10) and failure probability
neural network deﬁned in Deﬁnition D.2. Given any accuracy ǫ
O( n4d
O( 1
0ǫ ) and regularization
δ
λ4
√m ), then there exist ǫW , ǫ′H, ǫ′K > 0 that are independent of t, such that the

(0, 1/10). If κ = 1, T =
O( 1

Λ0 ), ǫtrain = ǫ/2, network width m

Rn, H(t)

∈
∈

· · ·

≥

∈

∈

×

e

parameter λ
following hold for all 0

≤

e
T :

t

[m]

r

1.

2.

ǫW ,

e
−

wr(t)

≤
≤
wr(0)
k2 ≤
k
H(0)
k2 ≤
k
2
unn(t)
2 ≤
k
) hides the poly log(n/(ǫδΛ0)).
O(
·

exp(
{

u∗k

H(t)

max

ǫ′H

3.

−

−

−

∈

∀

•

•

•
Here

(κ2Λ0 + λ)t/4)

unn(0)

· k

−

u∗k

2, ǫ2
2

train}

We ﬁrst state the following concentration result for the random initialization that can help us

e

prove the lemma.

Lemma D.15 (Random initialization result). Assume initial value wr(0)
, m are
drawn independently according to leverage score sampling distribution q(
) deﬁned in (72), then with
·
probability 1

δ we have

· · ·

∈

Rd, r = 1,

−

wr(0)
k

k2 ≤

2√d + 2

log (mc2/δ) := αw,0

(79)

hold for all r

[m], where c2 = O(n).

∈
Proof. By lemma A.6, if wr(0)

p

(0, In), then with probability at least 1

δ,

−

2√d + 2

log(m/δ)

∼ N

wr(0)
k

k2 ≤

holds for all r
≤
c2 = O(1/Λ0) and p(
) is the probability density function of
·
with probability at least 1

p
[m]. By Lemma D.5, we have q(w)

δ,

∈

c2p(w) holds for all w
(0, Id). Thus, if wr(0)

N

Rd, where
q, we have

∈
∼

−

holds for all r

[m].

∈

wr(0)
k

k2 ≤

2√d + 2

log(mc2/δ)

p

52

Now conditioning on Eq. (44), (45), (46) holds, We show all the four conclusions in Lemma B.21

holds using induction.

We deﬁne the following quantity:

max

unn(0)
4
k
{

−

u∗

k2/(κ2Λ0 + λ), ǫtrain ·

T

}

u∗

k2 + 2λαw,0

−

T

·

(cid:17)

(80)

ǫW :=

√n
√m

+

(cid:16)
ǫ′H := 2nǫW
ǫK := 2√nǫW

√n
√m k

Y

which are independent of t. Here αw,0 are deﬁned in Eq. (79).

Note the base case when t = 0 trivially holds. Now assuming Lemma D.14 holds before time
[0, T ], we argue that it also holds at time t. To do so, Lemmas D.16, D.17, D.19 argue these

t
conclusions one by one.

∈

Lemma D.16 (Conclusion 1). If for any τ < t, we have

unn(τ )
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)τ /4)

unn(0)

· k

u∗

2, ǫ2
2
k

train}

−

and

and

hold, then

wr(0)
k

−

wr(τ )

k2 ≤

ǫW ≤

1

wr(0)
k

k2 ≤

αw,0 for all r

[m]

∈

wr(0)
k

−

wr(t)

k2 ≤

ǫW

Proof. Recall the gradient ﬂow as Eq. (75)

dwr(τ )
dτ

=

1
√m

ar(yi −

n

Xi=1

unn(τ )i)xiσ′(wr(τ )⊤xi)

λwr(τ )

−

(81)

53

So we have

dwr(τ )
dτ

(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)

=

≤

≤

≤

≤

≤

≤

=

n

(cid:13)
Xi=1
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
√m

Xi=1
Y

Y

Y

√n
√m k
√n
√m k
√n
√m k
√n
√m k
√n
√m
√n
√m k
√n
√m k

Y
(
k

+

Y

ar(yi −

unn(τ )i)xiσ′(wr(τ )⊤xi)

−

1
√m

n

yi −
|

unn(τ )i|

+ λ

wr(τ )
k

k2

λwr(τ )

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

unn(τ )

k2 + λ

wr(τ )
k

k2

unn(τ )

wr(0)
k2 + λ(
k

k2 +

wr(τ )
k

−

wr(0)

k2)

unn(τ )

k2 + λ(αW,0 + 1)

unn(τ )

k2 + 2λαW,0

−

−

−

−

u∗

k2 +

unn(τ )
k

u∗

k2) + 2λαW,0

−

−

unn(τ )

u∗

k2

−

Y

u∗

k2 + 2λαW,0

−

√n
√m

≤

max

e−
{

(κ2Λ0+λ)τ /8

unn(0)
k

u∗

k2, ǫtrain}

−

+

√n
√m k

Y

u∗

k2 + 2λαW,0,

−

(82)

where the ﬁrst step follows from Eq. (81), the second step follows from triangle inequality, the third
step follows from Cauchy-Schwartz inequality, the forth step follows from triangle inequality, the
αW,0, the seventh step follows
ﬁfth step follows from condition
wr(0)
1,
k2 ≤
k
(κ2Λ0 + λ)τ /4)
from triangle inequality, the last step follows from
unn(τ )
k
unn(0)
k

k2 ≤
2
u∗k
2 ≤

wr(0)
k

exp(
{

wr(τ )

max

−

−

−

−

·

2, ǫ2
2
u∗k
Thus, for any t

.
train}
T ,
≤

wr(0)
k

−

wr(t)

k2 ≤

≤

dτ

t

dwr(τ )
dτ

0
Z
(cid:13)
√n
(cid:13)
max
(cid:13)
√m

2
(cid:13)
(cid:13)
4
unn(0)
(cid:13)
k
{

u∗

k2/(κ2Λ0 + λ), ǫtrain ·

T

}

−

√n
√m k

Y

u∗

k2 + 2λαW,0

−

T

·

(cid:17)

+

(cid:16)

= ǫW

where the ﬁrst step follows triangle inequality, the second step follows Eq. (82), and the last step
follows the deﬁnition of ǫW as Eq. (80).

Lemma D.17 (Conclusion 2). If

[m],

r

∀

∈
wr(0)
k

wr(t)

k2 ≤

−

ǫW < 1,

54

then

holds with probability 1

n2

exp (

·
Proof. Directly applying Lemma D.18, we ﬁnish the proof.

−

−

H(t)

H(0)
kF ≤
k
mǫW c1/10), where c1 = O(1/n).

2nǫW

−

Lemma D.18 (perturbed w). Let R
w1,
wm are i.i.d. generated from the leverage
· · ·
) as in (72). Let p(
score sampling distribution q(
) denotes the standard Gaussian distribution
·
·
Rd that satisfy for any r
R,
, wm ∈
e

(0, Id). For any set of weight vectors w1,

wrk2 ≤

wr−
k

(0, 1). If

[m],

· · ·

∈

∈

e

,

N
then the H : Rm
×

d

Rn

n deﬁned

×

→

H(w)i,j =

1
m

x⊤i xj

m

Xr=1

1

w⊤

r xi

e

0,w⊤

r xj

0

≥

≥

.

p(
q(

wr)
wr)
e
e

H(

w)

kF < 2nR,

−
mRc1/10), where c1 = O(1/n).

e

Then we have

H(w)
k

holds with probability at least 1

n2

·

−

exp(

−

Proof. The random variable we care is

n

n

H(
|

w)i,j −

2

H(w)i,j|

Xj=1
n

n

e

m

Xi=1
n

Xj=1  

n

r=1
X
m

Xi=1
1
m2

1
m2

≤

=

1

w⊤

r xi

1 ew⊤

r xi

0, ew⊤

r xj

≥

0 −

≥

sr,i,j

2

,

0,w⊤

r xj

0

≥

≥

2

p(
q(

wr)
wr) !
e
e

Xj=1 (cid:16)
where the last step follows from for each r, i, j, we deﬁne

Xi=1

r=1
X

(cid:17)

p(
q(

.

wr)
wr)
e
e
m
r=1 are independent,

wr}
{

sr,i,j := (1 ew⊤

r xi

0, ew⊤

r xj

≥

0 −

≥

1

w⊤

r xi

0,w⊤

r xj

≥

0)

≥

We consider i, j are ﬁxed. We simplify sr,i,j to sr.
Then sr is a random variable that only depends on

wr. Since

are also mutually independent.
Now we deﬁne the event

e

e

Then we have

Ai,r =

∃

n

u :

u
k

wrk2 ≤

−

R, 1

x⊤
i

ewr

0 6

≥

= 1

x⊤
i u

.

0

≥

o

e

ewr

Pr
∼N

(0,I)

[Ai,r] = Pr

z

∼N

(0,1)

< R]

z
[
|

|

≤

2R
√2π

.

m
r=1

sr}
{

(83)

where the last step follows from the anti-concentration inequality of Gaussian (Lemma A.4).

55

Ai,r and

If

¬

¬

Aj,r happen, then

If Ai,r or Aj,r happen, then

So we have

1 ew⊤

r xi

0, ew⊤

r xj

≥

0 −

≥

1 ew⊤

r xi

0, ew⊤

r xj

≥

0 −

≥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1

w⊤

r xi

1

w⊤

r xi

0,w⊤

r xj

0

≥

≥

0,w⊤

r xj

0

≥

≥

= 0.

1.

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

[sr]

E
ewr

∼

q

E
≤
ewr
∼
= E
ewr
∼

q

p

1Ai,r

(cid:20)
1Ai,r

Aj,r

∨

Aj,r

∨

p(
q(

wr)
wr)
e
e

(cid:21)

(cid:2)
Pr
(0,In)

[Ai,r] +

(cid:3)

Pr

[Aj,r]

ewr

∼N

(0,In)

∼N

ewr
4R
√2π
2R,

≤

≤

≤

and

E
ewr

∼

q "(cid:18)

sr −

E
ewr

∼

q

[sr]

(cid:19)

2

#

= E
ewr
∼
E
ewr

≤

∼

E
ewr

∼

≤

= E
ewr
∼
2R
c1

≤

[s2
r]

[s2
r]

q

q

q "(cid:18)

[sr]2

E
ewr

∼

q

−

1Ai,r

Aj,r

∨

1Ai,r

Aj,r

∨

p

(cid:20)

p(
q(

2

#

(cid:19)

p(
q(

wr)
wr)
e
wr)
e
wr)
e
e

(cid:21)

where the last step follows from Lemma D.5 and c1 = O(n). We also have
apply Bernstein inequality (Lemma A.3) to get for all t > 0,

sr| ≤
|

1/c1. So we can

Pr

m

"

r=1
X

sr ≥

2mR + mt

Pr

# ≤

m

"

r=1
X

exp

≤

−

(cid:18)

E[sr])

(sr −

mt

#

≥

m2t2/2
2mR/c1 + mt/3c1 (cid:19)

.

Choosing t = R, we get

Pr

m

"

r=1
X

sr ≥

3mR

# ≤

≤

exp

(cid:18)
exp (

−

m2R2/2
2mR/c1 + mR/3c1 (cid:19)
mRc1/10) .

−

Plugging back, we complete the proof.

56

Lemma D.19 (Conclusion 3). Fix ǫ′H > 0 independent of t. If for all τ < t

and

then we have

H(0)
k

−

H(τ )

k ≤

ǫ′H ≤

Λ0/4

ǫ′H ≤

8κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

(84)

unn(t)
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)t/2)

unn(0)

· k

u∗

2
2, ǫ2
k

.
train}

−

Proof. Note

d

unn(τ )
k
dτ

−

H(τ )
k
u∗k

2
2

H(0)

k ≤

ǫH ≤

−

Λ0/4. By Lemma D.12, for any τ < t, we have

1
2
1
2

(κ2Λ0 + λ)

(κ2Λ0 + λ)

unn(τ )

· k

unn(τ )

· k

u∗

u∗

−

−

2 + 2κ2
2
H(τ )
k
k
2 + 2κ2ǫ′H · k
2
k

≤ −

≤ −

H(0)

unn(τ )

k · k

u∗

Y
k2 · k

−

u∗

k2

−

−

unn(τ )

u∗

Y
k2 · k

−

u∗

k2

−

(85)

where the ﬁrst step follows from Lemma D.12, the second step follows from deﬁnition of ǫ′H.

Now let us discuss two cases:
Case 1. If for all τ < t,

unn(τ )
k

−

unn(t)
k

−

u∗

2
2 ≤
k

Note by assumption (84), we have

u∗k2 ≥
exp(

−

ǫtrain always holds, we want to argue that

(κ2Λ0 + λ)t/4)

unn(0)

· k

u∗

k2.

−

implies

ǫ′H ≤

8κ2

ǫtrain
Y
k

−

u∗k2

(κ2Λ0 + λ)

k2 ≤
holds for any τ < t. Thus, plugging into (85),

−

2κ2ǫH · k
Y

u∗

(κ2Λ0 + λ)/4

unn(τ )

· k

u∗

k2

−

d

unn(τ )
k
dτ

2
2

u∗k

−

(κ2Λ0 + λ)/4

unn(τ )

· k

u∗

2
2,
k

−

≤ −

holds for all τ < t, which implies

2
2 ≤
k

−

u∗

exp (

unn(t)
k
Case 2. If there exist τ < t, such that
u∗k2 < ǫtrain. Note by assumption (84), we have
ǫtrain
Y
k

−
unn(τ )
k

ǫ′H ≤

8κ2

−

(κ2Λ0 + λ)

u∗k2

(κ2Λ0 + λ)t/4)

unn(0)

u∗

2
2.
k

· k

−
u∗k2 < ǫtrain, we want to argue that

−

unn(t)
k

−

implies

4κ2ǫ′H · k

unn(τ )

u∗

Y
k2 · k

−

−

u∗

k2 ≤

(κ2Λ0 + λ)

ǫ2
train.

·

57

Thus, plugging into (85),

unn(τ )
d(
k

−

u∗k
dτ

2
2 −

ǫ2
train)

(κ2Λ0 + λ)/2

(
unn(τ )
k

·

−

u∗

2
2 −
k

ǫ2
train)

≤ −

holds for τ = τ , which implies e(κ2Λ0+λ)τ /2(
unn(τ )
k
unn(τ )
k
and

ǫ2
2
train) is non-increasing at τ = τ . Since
u∗k
2 −
−
train < 0, by induction, e(κ2Λ0+λ)τ /2(
ǫ2
ǫ2
2
u∗k
unn(τ )
u∗k
train) being non-increasing
2 −
−
k
2
t, which implies
u∗k
unn(τ )
2 −
k
≤
−
k2 < ǫtrain.

ǫ2
train < 0 holds for all τ

unn(t)
k

2
2 −

u∗

−

≤

−

τ

Combine above two cases, we conclude

unn(t)
k

−

u∗

2
2 ≤
k

max

exp(
{

−

(κ2Λ0 + λ)t/2)

unn(0)

· k

u∗

2, ǫ2
2
k

.
train}

−

Now we summarize all the conditions need to be satisﬁed so that the induction works as in

Table 3.

Table 3: Summary of conditions for induction

No. Condition
1
ǫW ≤
ǫ′H ≤
2
ǫ′H ≤
3

1
Λ0/4

8κ2

ǫtrain
Y

u∗

k

−

Place
Lem. D.16
Lem. D.19
(κ2Λ0 + λ) Lem. D.19

2
k

Compare Table 1 and Table 3, we can see by picking the same value for the parameters as in

Theorem B.29, we have the induction holds, which completes the proof.

As a direct corollary, we have

Corollary D.20. Given any accuracy ǫ
(0, 1/10) and failure probability δ
∈
O( n4d
O( 1
), network width m
T =
0ǫ ) and regularization parameter λ
λ4
Λ0
probability at least 1

δ,

≥

e

−

e

unn(T )
k

−

u∗

k2 ≤

ǫ/2.

(0, 1/10). If κ = 1,
O( 1
√m ), then with

∈
≤

e

Here

) hides the poly log(n/(ǫδΛ0)).
O(
·

Proof. By choosing ǫtrain = ǫ/2 in Lemma D.14, the induction shows

e

unn(t)
k

−

u∗

2
2 ≤
k

max

(κ2Λ0 + λ)t/4)

unn(0)

· k

u∗

2, ǫ2/4
2
}
k

−

−

exp(
{
O( 1
Λ0

holds for all t

≤

T . By picking T =

), we have

exp(

−

(κ2Λ0 + λ)T /4)

e

which implies

unn(T )
k

−

u∗k

2
2 ≤

max

ǫ2/4, ǫ2/4
}
{

u∗

2
2 ≤
k

−

ǫ2/4

unn(0)

· k
= ǫ2/4.

58

D.5.3 Main result for equivalence with leverage score sampling initialization

Theorem D.21 (Equivalence between training reweighed neural net with regularization under
leverage score initialization and kernel ridge regression for training data prediction, restatement of
Rn. Let
Rn
Theorem 3.9). Given training data matrix X
Rn be the training data predictors
T > 0 be the total number of iterations. Let unn(t)
deﬁned in Deﬁnition D.2 and Deﬁnition B.5 respectively. Let κ = 1 be the corresponding multiplier.
O( n4d
Given any accuracy ǫ
0ǫ ) and regularization
λ4
O( 1
δ over the random initialization, we have

d and corresponding label vector Y
Rn and u∗ ∈
), network width m

(0, 1), if κ = 1, T =

√m ), then with probability at least 1

parameter λ

O( 1
Λ0

≥

∈

∈

∈

∈

×

e

−

e

≤

e

unn(T )
k

−

u∗

k2 ≤

ǫ.

Here

) hides poly log(n/(ǫδΛ0)).
O(
·

Proof. Combining results of Lemma D.13 and Corollary D.20 using triangle inequality, we ﬁnish
the proof.

e

Remark D.22. Despite our given upper-bound of network width under leverage score sampling
is asymptotically the same as the Gaussian initialization, we point out the potential beneﬁts of
introducing leverage score sampling to training regularized neural networks.

Note the bound for the width consists of two parts: 1) initialization and 2) training. Part 1,
requires the width to be large enough, so that the initialized dynamic kernels H(0) and H(0) are
close enough to NTK by concentration, see Lem B.20 and D.13. Part 2, requires the width to be
large enough, so that the dynamic kernels H(t) and H(t) are close enough to the NTK during the
training by the over-parameterization property, see Lem B.21 and D.14. Leverage score sampling
optimizes the bound for part 1 while keeping the bound for part 2 the same. The current state-of-art
analysis gives a tighter bound in part 2, so the ﬁnal bound for width is the same for both cases. If
analysis for part 2 can be improved and part 1 dominates, then initializing using leverage score will
be beneﬁcial in terms of the width needed.

E Extension to other neural network models

In previous sections, we discuss a simple neural network model: 2-layer ReLu neural network with
ﬁrst layer trained. We remark that our results can be naturally extended to multi-layer ReLU deep
neural networks with all parameters training together.

Note the core of the connection between regularized NNs and KRR is to show the similarity
between their gradient ﬂows, as shown in Corollary B.8 and Corollary B.11: their gradient ﬂow are
given by

duntk,test(t)
dt
dunn,test(t)
dt

=

=

−

−

κ2Kntk(xtest, X)⊤(untk(t)

Y )

−

−

λuntk,test(t)

κ2Kt(xtest, X)⊤(unn(t)

−

Y )

λunn,test(t)

−
κ2K(xtest, X)⊤(u(t)

Y ) comes from
Note these gradient ﬂows consist of two terms: the ﬁrst term
the normal neural network training without ℓ2regularization, the second term
λutest(t) comes from
the regularizer and can be directly derived using the piece-wise linearity property of the 2-layer ReLu
NN (in this case, with respect to the parameters in the ﬁrst layer).

−

−

−

Now consider the case of training multi-layer ReLu neural network with regularization. We
claim above similarity between the gradient ﬂows of NN and KRR still holds as long as we scale

59

κ2K(xtest, X)⊤(u(t)

up the network width by the number of layers trained: as 1) the similarity of the ﬁrst term
Y ) has already been shown in previous literature [ADH+19a, AZLS19a],
−
and 2) the similarity of the second term
λutest(t) comes from the piece-wise linearity property
of deep ReLu neural network with respect to all training parameters. In the common case where
we train all the parameters together, the equivalence still holds as long as we scale up the network
width by the number of layers, as shown in the following theorem:

−

−

Theorem E.1. Consider training a L-layer ReLU neural network with ℓ2 regularization. Let
unn,test(t) denote the neural network predictor at time t, and u∗test denote the kernel ridge regres-
sion predictor. Then Given any accuracy ǫ
(0, 1/10). Let
multiplier κ = poly(ǫ, Λ0, 1/n, 1/L), number of iterations T = poly(1/ǫ, 1/Λ0, n, L), network width
poly(1/n, 1/d, ǫ, Λ0, 1/L). Then with
m
probability at least 1

poly(n, d, 1/ǫ, 1/Λ0, L) and regularization parameter λ
δ over random initialization, we have

(0, 1/10) and failure probability δ

≤

≥

∈

∈

−

Here we omit poly log(n/(ǫδΛ0)) factors.

unn,test(T )
k

−

u∗testk2 ≤

ǫ.

The results under leverage score sampling can be argued in the same way.
We also remark that it is possible to extend our results further to the model of convolutional
neural network (CNN) by making use the convolutional neural tangent kernel (CNTK) discussed
in [ADH+19a], and to the case using stochastic gradient descent in training rather than gradient
descent. However, these discussion require more detailed proof and is out of the scope of this work.

60

References

[ACSS20]

[ACW17]

Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness
for linear algebra on geometric graphs. In FOCS, 2020.

Haim Avron, Kenneth L Clarkson, and David P Woodruﬀ. Faster kernel ridge re-
gression using sketching and preconditioning. SIAM Journal on Matrix Analysis and
Applications, 38(4):1116–1138, 2017.

[ADH+19a] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong
Wang. On exact computation with an inﬁnitely wide neural net. In NeurIPS. arXiv
preprint arXiv:1904.11955, 2019.

[ADH+19b] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained
analysis of optimization and generalization for overparameterized two-layer neural
networks. In ICML. arXiv preprint arXiv:1901.08584, 2019.

[AKM+17] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Vel-
ingker, and Amir Zandieh. Random fourier features for kernel ridge regression: Ap-
proximation bounds and statistical guarantees. In ICML, 2017.

[AKM+19] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Vel-
ingker, and Amir Zandieh. A universal sampling method for reconstructing signals
with simple fourier transforms. In STOC. arXiv preprint arXiv:1812.08723, 2019.

[AM15]

Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression
with statistical guarantees.
In Advances in Neural Information Processing Systems
(NeurIPS), pages 775–783, 2015.

[AZLS19a]

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning
via over-parameterization. In ICML, 2019.

[AZLS19b]

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training
recurrent neural networks. In NeurIPS, 2019.

[Bac13]

[Ber24]

[BG17]

[BJW19]

[BLM89]

[BLN+20]

Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference
on Learning Theory (COLT), pages 185–209, 2013.

Sergei Bernstein. On a modiﬁcation of chebyshev’s inequality and of the error formula
of laplace. Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924.

Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet
with gaussian inputs. In ICML, 2017.

Ainesh Bakshi, Rajesh Jayaram, and David P Woodruﬀ. Learning two layer rectiﬁed
neural networks in polynomial time. In COLT. http://arxiv.org/pdf/:1811.01885,
2019.

Jean Bourgain, Joram Lindenstrauss, and V Milman. Approximation of zonoids by
zonotopes. Acta mathematica, 162(1):73–141, 1989.

Jan van den Brand, Yin-Tat Lee, Danupon Nanongkai, Richard Peng, Thatchaphol
Saranurak, Aaron Sidford, Zhao Song, and Di Wang. Bipartite matching in nearly-
linear time on moderately dense graphs. In FOCS, 2020.

61

[BLSS20]

Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense
linear programs in nearly linear time. In STOC, 2020.

[BPSW20]

Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-
parametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
2020.

[BW14]

Christos Boutsidis and David P Woodruﬀ. Optimal cur matrix decompositions. In
Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),
pages 353–362. ACM, https://arxiv.org/pdf/1405.7910, 2014.

[CB18]

Lenaic Chizat and Francis Bach. A note on lazy training in supervised diﬀerentiable
programming. arXiv preprint arXiv:1812.07956, 8, 2018.

[CCLY19] Michael B Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm
for approximating the john ellipsoid. In COLT. arXiv preprint arXiv:1905.11580, 2019.

[Che52]

[CKPS16]

[CM20]

[CMM17]

[CP15]

[CW13]

[CW17]

[Dan17]

[DFS16]

Herman Chernoﬀ. A measure of asymptotic eﬃciency for tests of a hypothesis based
on the sum of observations. The Annals of Mathematical Statistics, pages 493–507,
1952.

Xue Chen, Daniel M Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation
without a frequency gap. In 57th Annual Symposium on Foundations of Computer
Science (FOCS), pages 741–750. IEEE, 2016.

Sitan Chen and Ankur Moitra. Algorithmic foundations for the diﬀraction limit. arXiv
preprint arXiv:2004.07659, 2020.

Michael B Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-
rank approximation via ridge leverage score sampling. In Proceedings of the Twenty-
Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1758–
1777. https://arxiv.org/pdf/1511.07263.pdf, 2017.

In
Michael B. Cohen and Richard Peng.
Proceedings of
the Forty-Seventh Annual ACM on Symposium on Theory of
Computing (STOC), STOC ’15, pages 183–192, New York, NY, USA, 2015.
https://arxiv.org/pdf/1412.0588.

ℓp row sampling by lewis weights.

Low rank approximation and
Kenneth L. Clarkson and David P. Woodruﬀ.
regression in input sparsity time.
In Symposium on Theory of Computing
Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013, pages 81–90.
https://arxiv.org/pdf/1207.6365, 2013.

Kenneth L Clarkson and David P Woodruﬀ. Low-rank psd approximation in input-
sparsity time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 2061–2072. SIAM, 2017.

Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in
Neural Information Processing Systems (NeurIPS), pages 2422–2430, 2017.

Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural
networks: The power of initialization and a dual view on expressivity. In Advances In
Neural Information Processing Systems (NeurIPS), pages 2253–2261, 2016.

62

[DLL+19]

[DLT+18]

Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang,
Gradient descent ﬁnds global minima of deep neural networks.
https://arxiv.org/pdf/1811.03804, 2019.

and Xiyu Zhai.
In ICML.

Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabás Póczos, and Aarti Singh. Gra-
dient descent learns one-hidden-layer CNN: don’t be afraid of spurious local minima.
In ICML. http://arxiv.org/pdf/1712.00779, 2018.

[DMIMW12] Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruﬀ.
Fast approximation of matrix coherence and statistical leverage. Journal of Machine
Learning Research, 13(Dec):3475–3506, 2012.

[DS08]

[DZPS19]

Samuel I Daitch and Daniel A Spielman. Faster approximate lossy generalized ﬂow
via interior point algorithms. In Proceedings of the fortieth annual ACM symposium
on Theory of computing (STOC), pages 451–460, 2008.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent
provably optimizes over-parameterized neural networks.
In ICLR. arXiv preprint
arXiv:1810.02054, 2019.

[GLM18]

Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks
with landscape design. In ICLR, 2018.

[Hoe63]

[JGH18]

Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables.
Journal of the American Statistical Association, 58(301):13–30, 1963.

Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Con-
vergence and generalization in neural networks. In Advances in neural information
processing systems (NeurIPS), pages 8571–8580, 2018.

[JKL+20]

Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song.
A faster interior point method for semideﬁnite programming. In FOCS, 2020.

[JLS20]

Yaonan Jin, Daogao Liu, and Zhao Song. A robust multi-dimensional sparse fourier
transform in the continuous setting. arXiv preprint arXiv:2005.06156, 2020.

[JLSW20]

Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cut-
ting plane method for convex optimization, convex-concave games and its applications.
In STOC, 2020.

[JSWZ20]

Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic
matrix inverse for faster lps. arXiv preprint arXiv:2004.07470, 2020.

[Lew78]

[LL18]

[LM00]

D. Lewis. Finite dimensional subspaces of Lp. Studia Mathematica, 63(2):207–212,
1978.

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via
stochastic gradient descent on structured data. In NeurIPS, 2018.

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional
by model selection. Annals of Statistics, pages 1302–1338, 2000.

63

[LS14]

[LS20a]

[LS20b]

[LSW15]

Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving
linear programs in O(√rank) iterations and faster algorithms for maximum ﬂow. In
2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS),
pages 424–433. IEEE, 2014.

Yang P Liu and Aaron Sidford. Faster divergence maximization for faster maximum
ﬂow. In FOCS, 2020.

Yang P Liu and Aaron Sidford. Faster energy maximization for faster maximum ﬂow.
In STOC, 2020.

Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method
In Foundations of
and its implications for combinatorial and convex optimization.
Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1049–1065.
IEEE, 2015.

[LSZ20]

S. Cliﬀ Liu, Zhao Song, and Hengjie Zhang. Breaking the n-pass barrier: A streaming
algorithm for maximum weight bipartite matching. arXiv preprint arXiv:2009.06106,
2020.

[LTOS18]

Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a uniﬁed anal-
ysis of random fourier features. arXiv preprint arXiv:1806.09178, 2018.

[LY17]

[Mad13]

[Mad16]

[MM17]

[Moi15]

[MW17a]

[MW17b]

[NN13]

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with
ReLU activation. In NeurIPS. http://arxiv.org/pdf/1705.09886, 2017.

Aleksander Madry. Navigating central path with electrical ﬂows: From ﬂows to match-
ings, and back. In 2013 IEEE 54th Annual Symposium on Foundations of Computer
Science (FOCS), pages 253–262. IEEE, 2013.

Aleksander Madry. Computing maximum ﬂow with augmenting electrical ﬂows. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),
pages 593–602. IEEE, 2016.

Cameron Musco and Christopher Musco. Recursive sampling for the nystrom method.
In Advances in Neural Information Processing Systems (NeurIPS), pages 3833–3845,
2017.

Ankur Moitra. The threshold for super-resolution via extremal functions. In STOC,
2015.

Cameron Musco and David Woodruﬀ.
low-rank approximation?
(NeurIPS), pages 4435–4445, 2017.

Is input sparsity time possible for kernel
In Advances in Neural Information Processing Systems

Cameron Musco and David P Woodruﬀ. Sublinear time low-rank approximation of
positive semideﬁnite matrices. In 2017 IEEE 58th Annual Symposium on Foundations
of Computer Science (FOCS), pages 672–683. IEEE, 2017.

Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical
gorithms via sparser subspace embeddings.
posium on Foundations of Computer Science (FOCS), pages 117–126.
https://arxiv.org/pdf/1211.1002, 2013.

linear algebra al-
In 2013 IEEE 54th Annual Sym-
IEEE,

64

[PS15]

[RR08]

[Sch18]

[Sol17]

[SS11]

Eric Price and Zhao Song. A robust sparse Fourier transform in the continuous
setting.
In 56th Annual Symposium on Foundations of Computer Science (FOCS),
pages 583–600, 2015.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.
In Advances in neural information processing systems (NeurIPS), pages 1177–1184,
2008.

Aaron Schild. An almost-linear time algorithm for uniform random spanning tree
generation. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
of Computing (STOC), pages 214–227, 2018.

Mahdi Soltanolkotabi. Learning ReLUs via gradient descent.
http://arxiv.org/pdf/1705.04591, 2017.

In arXiv preprint.

Daniel A Spielman and Nikhil Srivastava. Graph sparsiﬁcation by eﬀective resistances.
SIAM Journal on Computing, 40(6):1913–1926, 2011.

[SWZ17]

Zhao Song, David P Woodruﬀ, and Peilin Zhong. Low rank approximation with
entrywise ℓ1-norm error. In Proceedings of the 49th Annual Symposium on the Theory
of Computing (STOC), 2017.

[SWZ19]

Zhao Song, David P Woodruﬀ, and Peilin Zhong. Relative error tensor low rank
approximation. In SODA. arXiv preprint arXiv:1704.08246, 2019.

[SY19]

[Tia17]

[Tro15]

[Vai89]

[ZDW15]

[ZNV+20]

[ZSD17]

[ZSJ+17]

Zhao Song and Xin Yang. Quadratic suﬃces for over-parametrization via matrix
chernoﬀ bound. arXiv preprint arXiv:1906.03593, 2019.

Yuandong Tian. An analytical formula of population gradient for two-layered ReLU
In ICML.
network and its applications in convergence and critical point analysis.
http://arxiv.org/pdf/1703.00560, 2017.

Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and
Trends R
(cid:13)

in Machine Learning, 8(1-2):1–230, 2015.

Pravin M Vaidya. A new algorithm for minimizing convex functions over convex
sets. In Foundations of Computer Science, 1989., 30th Annual Symposium on, pages
338–343. IEEE, 1989.

Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge
regression: A distributed algorithm with minimax optimal rates. The Journal of
Machine Learning Research, 16(1):3299–3340, 2015.

Amir Zandieh, Navid Nouri, Ameya Velingker, Michael Kapralov, and Ilya Razen-
shteyn. Scaling up kernel ridge regression via locality sensitive hashing. In AISTATS,
2020.

Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional
neural networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017.

Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Re-
covery guarantees for one-hidden-layer neural networks. In ICML, 2017.

65

