0
2
0
2

g
u
A
7
2

]

C
O
.
h
t
a
m

[

1
v
0
7
1
2
1
.
8
0
0
2
:
v
i
X
r
a

Complexity Aspects of Fundamental

Questions in Polynomial Optimization

Jeffrey Zhang

A Dissertation

Presented to the Faculty

of Princeton University

in Candidacy for the Degree

of Doctor of Philosophy

Recommended for Acceptance

by the Department of

Operations Research and Financial Engineering

Adviser: Amir Ali Ahmadi

September 2020

 
 
 
 
 
 
c(cid:13) Copyright by Jeﬀrey Zhang, 2020.

All rights reserved.

Abstract

In this thesis, we settle the computational complexity of some fundamental questions in polyno-

mial optimization. These include the questions of (i) ﬁnding a local minimum, (ii) testing local

minimality of a candidate point, and (iii) deciding attainment of the optimal value. Our results

characterize the complexity of these three questions for all degrees of the deﬁning polynomials left

open by prior literature.

Regarding questions (i) and (ii), we show that unless P=NP, there cannot be a polynomial-

time algorithm that ﬁnds a point within Euclidean distance cn (for any constant c) of a local

minimum of an n-variate quadratic function over a polytope. This result answers a question of

Pardalos and Vavasis that appeared in 1992 on a list of seven open problems in complexity theory

for numerical optimization. By contrast, through leveraging techniques from algebraic geometry,

we show that a local minimum of a cubic polynomial can be found eﬃciently by semideﬁnite

programming.

Interestingly, we prove that second-order points of cubic polynomials admit an

eﬃcient semideﬁnite representation, even though their critical points are NP-hard to ﬁnd. We also

give an eﬃciently-checkable necessary and suﬃcient condition for local minimality of a point for a

cubic polynomial.

Regarding question (iii), we prove that testing whether a quadratically constrained quadratic

program with a ﬁnite optimal value has an optimal solution is NP-hard. We also show that

testing coercivity of the objective function, compactness of the feasible set, and the Archimedean

property associated with the description of the feasible set are all NP-hard. The latter property

is the assumption on which convergence of the Lasserre hierarchy relies. We also give a new

characterization of coercive polynomials that lends itself to a hierarchy of semideﬁnite programs.

In our ﬁnal chapter, we present a semideﬁnite programming relaxation for the problem of ﬁnding

approximate Nash equilibria in bimatrix games. We show that for a symmetric game, a 1/3-Nash

equilibrium can be eﬃciently recovered from any rank-2 solution to this relaxation. We also propose

semideﬁnite programming relaxations for NP-hard problems related to Nash equilibria, such as that

of ﬁnding the highest achievable welfare under any Nash equilibrium.

iii

Acknowledgements

I would like to ﬁrst and foremost thank my adviser, Amir Ali Ahmadi, for making all of this

possible. Thank you for being an amazing adviser, and for teaching me everything you have.

Thanks to you, I feel ready to be a professor and a researcher. Thank you for all the long

hours, for not letting me take the easy road, and for all the feedback on every little thing.

Thank you for giving me every opportunity to grow as a student, as a future teacher, and as

a person. Thank you for broadening my horizons, both academically and literally. I know

that so much of where I am now is due to you, and I know that all the lessons will stick with

me for a long future to come.

I also want to thank the other professors who helped me along my way; Bob Vanderbei,

for being there for me from the very beginning in so many ways, and I hope my life can be

as interesting as yours one day. Nicolas Boumal and Anirudha Majumdar for also serving

on my thesis committee. Jianqing Fan, for introducing me to ORFE which has become a

home to me. Matt Weinberg, for a most interesting course and new research ideas. I would

also like to thank all the members of ORFE who have made my experience what it was.

Thank you Kim, Tara, Michael, Tiﬀany, and Melissa for putting up with me and answering

questions when I had them.

I would also like to thank all the friends I made along the way; Bachir and Cemil, who

made working with Amirali all the more interesting, Galen and Dan, for all the hours spent

in the STWG, Thomas, Elahe, Yiqiao, Sinem, Yair, Kaizheng, Suqi, for making the time

spent more interesting. A special thanks to Georgina, who was a guiding light in the early

years when I needed one. Thank you also to David, Chris, Alan, Dan, Andre, Josh, Connor,

Demi, and Jenny for keeping me grounded and always diﬀerent perspectives on life.

Last but not least I want to thank my family. My brother Leon for all the motivation,

ideas, and discussion, and being with me my whole life. My father Heping, for setting me in

the path to being a professor, keeping me motivated, and all the advice over the years. And

ﬁnally my mother Julan, without whose love and passion I may not be here today.

iv

To my mother, Julan, for all her loving care.

v

Contents

Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

iv

x

List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xii

1 Introduction

1.1 Preliminaries

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Outline of this thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Related Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 On Local Minima and Related Notions in Unconstrained Polynomial Op-

timization

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 Organization and Main Contributions of the Chapter . . . . . . . . .

2.1.2 Preliminaries and Notation . . . . . . . . . . . . . . . . . . . . . . . .

2.2 NP-hardness Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Checking Local Minimality of a Point for a Cubic Polynomial

. . . . . . . .

2.3.1 Preliminaries on Cubic Polynomials . . . . . . . . . . . . . . . . . . .

2.3.2 Local Minimality of a Point for a Cubic Polynomial

. . . . . . . . . .

2.3.3 Examples

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 On the Geometry of Local Minima of Cubic Polynomials . . . . . . . . . . .

1

7

10

11

12

12

16

17

18

21

21

22

29

31

vi

2.4.1 Convexity of the Set of Local Minima . . . . . . . . . . . . . . . . . .

2.4.2 Local Minima and Solutions to a “Convex” Problem . . . . . . . . .

2.4.3 Distinction Between Local Minima and Second-Order Points . . . . .

2.4.4

Spectrahedra and Convexity Regions of Cubic Polynomials . . . . . .

2.5 Complexity Justiﬁcations for an Exact SDP Oracle . . . . . . . . . . . . . .

2.6 Finding Local Minima of Cubic Polynomials . . . . . . . . . . . . . . . . . .

2.6.1 Preliminaries from Semideﬁnite and Sum of Squares Optimization . .

2.6.2 A Sum of Squares Approach for Finding Second-Order Points

. . . .

2.6.3 A Simpliﬁed Semideﬁnite Representation of Second-Order Points and

an Algorithm for Finding Local Minima . . . . . . . . . . . . . . . .

2.7 Conclusions and Future Directions

. . . . . . . . . . . . . . . . . . . . . . .

2.7.1 Approximate Local Minima . . . . . . . . . . . . . . . . . . . . . . .

2.7.2 Unregularized Third-Order Newton Methods . . . . . . . . . . . . . .

3 On the Complexity of Finding a Local Minimizer of a Quadratic Function

over a Polytope

3.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.1 Notation and Basic Deﬁnitions

. . . . . . . . . . . . . . . . . . . . .

3.2 The Main Result

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1 Complexity of Deciding Existence of Local minima . . . . . . . . . .

3.2.2 Complexity of Finding a Local minimum of a Quadratic Function Over

32

37

40

44

46

50

50

53

57

70

70

72

77

77

80

81

81

a Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

87

4 On Attainment of the Optimal Value in Polynomial Optimization

4.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.1 Organization and Contributions of the Chapter

. . . . . . . . . . . .

4.2 NP-hardness of Testing Attainment of the Optimal Value . . . . . . . . . . .

4.3 NP-hardness of Testing Suﬃcient Conditions for Attainment . . . . . . . . .

91

91

93

94

98

vii

4.3.1 Coercivity of the Objective Function . . . . . . . . . . . . . . . . . .

98

4.3.2 Closedness and Boundedness of the Feasible Set . . . . . . . . . . . . 100

4.4 Algorithms for Testing Attainment of the Optimal Value . . . . . . . . . . . 108

4.4.1 Compactness of the Feasible Set . . . . . . . . . . . . . . . . . . . . . 108

4.4.2 Coercivity of the Objective Function . . . . . . . . . . . . . . . . . . 110

4.5 Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

5 Semideﬁnite Programming Relaxations for Nash Equilibria in Bimatrix

Games

118

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

5.1.1 Organization and Contributions of the chapter . . . . . . . . . . . . . 121

5.1.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

5.2 The Formulation of our SDP Relaxation . . . . . . . . . . . . . . . . . . . . 122

5.2.1 Nash Equilibria as Solutions to Quadratic Programs . . . . . . . . . . 123

5.2.2

SDP Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

5.2.3 Valid Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127

5.2.4

Simplifying our SDP . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

5.3 Exactness for Strictly Competitive Games

. . . . . . . . . . . . . . . . . . . 133

5.4 Algorithms for Lowering Rank . . . . . . . . . . . . . . . . . . . . . . . . . . 136

5.4.1 Linearization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 137

5.4.2 Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 142

5.5 Bounds on (cid:15) for General Games . . . . . . . . . . . . . . . . . . . . . . . . . 144

5.5.1 Lemmas Towards Bounds on (cid:15) . . . . . . . . . . . . . . . . . . . . . . 144

5.5.2 Bounds on (cid:15) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

5.5.3 Bounds on (cid:15) in the Rank-2 Case . . . . . . . . . . . . . . . . . . . . . 152

5.6 Bounding Payoﬀs and Strategy Exclusion in Symmetric Games . . . . . . . . 156

5.6.1 Bounding Payoﬀs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156

5.6.2

Strategy Exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

viii

5.7 Connection to the Sum of Squares/Lasserre Hierarchy . . . . . . . . . . . . . 160

5.7.1

Sum of Squares/Lasserre Hierarchy . . . . . . . . . . . . . . . . . . . 161

5.7.2 The Lasserre Hierarchy and SDP1 . . . . . . . . . . . . . . . . . . . . 161

5.8 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

A Appendices for Nash Equilibria

168

A.1 Statistics on (cid:15) from Algorithms in Section 5.4 . . . . . . . . . . . . . . . . . 168

A.2 Comparison with an SDP Approach from [60]

. . . . . . . . . . . . . . . . . 170

A.3 Lemmas for Extreme Nash Equilibria . . . . . . . . . . . . . . . . . . . . . . 171

A.4 A Note on Reductions from General Games to Symmetric Games

. . . . . . 173

A.4.1 The Reduction of [45] . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

A.4.2 The Reduction of [54] . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

Bibliography

176

ix

List of Tables

1.1 Complexity of deciding whether a point is a local minimum of a POP, based

on the degree of the objective p and the maximum degree of any constraint

function qi. Entries without a reference are either classical or implied by a

stronger hardness result in the table.1 . . . . . . . . . . . . . . . . . . . . . .

3

1.2 Complexity of deciding whether a POP has a local minimum, based on the

degree of the objective p and the maximum degree of any constraint function

qi. Entries without a reference are either classical or implied by a stronger

hardness result in the table.

. . . . . . . . . . . . . . . . . . . . . . . . . . .

4

1.3 Complexity of deciding whether a POP with a ﬁnite optimal value has an

optimal solution, based on the degree of the objective p and the maximum

degree of any constraint function qi. Entries without a reference are either

classical or implied by a stronger hardness result in the table. Note that

whenever the degree of the objective is at most three, and the feasible set is

a polyhedron, there is no algorithm required; the answer is simply ‘yes’. . . .

6

2.1 Complexity of deciding whether a given point is of a certain type, based on

the degree of the polynomial. Entries without a reference are classical.

. . .

15

2.2 Complexity of deciding whether a polynomial has a point of a certain type,

based on the degree of the polynomial. Entries without a reference are classical. 15

2.3

Iterations of the third-order Newton method (left) and the classical Newton

method (right) on the function f in (2.34) starting at x0 = 1.5. . . . . . . . .

74

x

5.1 Statistics on (cid:15) for 20 × 20 games after 20 iterations.

. . . . . . . . . . . . . . 143

5.2 Performance of LP2 and SDP4 on 5 × 5 games . . . . . . . . . . . . . . . . . 160

5.3 Performance of LP2 and SDP4 on 10 × 10 games

. . . . . . . . . . . . . . . 160

A.1 Statistics on (cid:15) for 5 × 5 games after 20 iterations.

. . . . . . . . . . . . . . . 168

A.2 Statistics on (cid:15) for 10 × 5 games after 20 iterations. . . . . . . . . . . . . . . . 168

A.3 Statistics on (cid:15) for 10 × 10 games after 20 iterations.

. . . . . . . . . . . . . . 169

A.4 Statistics on (cid:15) for 15 × 10 games after 20 iterations.

. . . . . . . . . . . . . . 169

A.5 Statistics on (cid:15) for 15 × 15 games after 20 iterations.

. . . . . . . . . . . . . . 169

A.6 Statistics on (cid:15) for 20 × 15 games after 20 iterations.

. . . . . . . . . . . . . . 169

A.7 Statistics on (cid:15) for 20 × 20 games after 20 iterations.

. . . . . . . . . . . . . . 169

A.8 Statistics on (cid:15) for ﬁrst level of the hierarchy in [60].

. . . . . . . . . . . . . . 170

A.9 Statistics on (cid:15) for SDP2 with Tr(M ) as the objective function.

. . . . . . . . 170

xi

List of Figures

2.1 Contour plots of x2

1x2 (left) and x2

2 − x2

1x2 (right) from Examples 2.3.1 and

2.3.2. The polynomials are zero on the black lines, positive on the gray regions,

and negative on the white regions. The dashed line in the right-side ﬁgure

denotes a descent parabola at the origin.

. . . . . . . . . . . . . . . . . . . .

29

2.2 The critical points of the polynomial (2.9). One can verify that the set of

critical points is {(x1, x2) | (x1 + x2)2 = 1}, and that the set of local minima

is {(x1, x2) | x1 + x2 = 1}. The points on the dashed line are local maxima. .

34

2.3 The projection of the set of local minima of the polynomial in (2.10) onto the

x1 and x2 variables. This example shows that LMp is not always a polyhedral

set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

2.4 The set of local minima (left) and second-order points (right) of the cubic

polynomial p(x1, x2) = x2

1x2. Note that SOp is the closure of LMp (Theorem

2.4.7) and LMp is the relative interior of SOp (Theorem 2.4.10).

. . . . . . .

43

2.5 The plots of the function f in (2.34) and its second and third-order Taylor

expansions around x0 = 1.5. One can see that one iteration of the third-order

Newton method in (2.33) brings us closer the global minimum of f compared

to one iteration of the Newton method in (2.32). . . . . . . . . . . . . . . . .

75

2.6 Sensitivity of the limits of the iterates (2.32), (2.33), and (2.35) respectively

to initial conditions. Regions with the same color denote initial conditions

which converge to the same critical point.

. . . . . . . . . . . . . . . . . . .

76

xii

5.1 Distribution of (cid:15) over numbers of iterations for the square root algorithm (left)

and the diagonal gap algorithm (right). . . . . . . . . . . . . . . . . . . . . . 143

5.2 The quality of the upper bound on the maximum welfare obtained by LP1

and SDP3 on 100 5 × 5 games (left) and 100 10 × 10 games (right).

. . . . . 158

xiii

Chapter 1

Introduction

In this thesis, we concern ourselves with polynomial optimization problems (POPs), i.e.,

problems of the type

inf
x∈Rn

p(x)

subject to qi(x) ≥ 0, ∀i ∈ {1, . . . , m},

(1.1)

where p, q1, . . . qm are polynomial functions. In Chapters 2-4, we address the complexity of

the following questions for a problem in the form of (1.1):

Q1: Is a given a point x a local minimum of (1.1)?

Q2: Does (1.1) have a local minimum (and if so, can one be found eﬃciently)?

Q3: If (1.1) has a ﬁnite optimal value, does it have an optimal solution?

Precise deﬁnitions and in-depth study of related problems can be found in their respective

chapters.

POPs have wide modeling capabilities and arise ubiquitously in applications, either as

exact models of objective functions or as approximations thereof. Perhaps the most well-

known special case of POPs is linear programming, but POPs have much richer expressive

power. For example, in full generality any decision problem in NP, a class of yes-no decision

problems with the property that any yes answer can be certiﬁed eﬃciently, can be posed as

1

a POP.1 An example of a POP we will see in this thesis is the search for Nash equilibria

in bimatrix games, but other examples arise from optimal power ﬂow [51], the quadratic

assignment problem [65], robotics and control [6], and statistics and machine learning [106,

104]. Even when the goal is not to minimize a polynomial function, optimization algorithms

that involve minimizing Taylor expansions of functions solve POPs as a subroutine.

With such modeling power comes the price of computational intractability, and unfor-

tunately POPs become intractable to solve even when the degrees of the deﬁning polyno-

mials are low. The study of local minima in Q1 and Q2 is in large part motivated by this

intractability of ﬁnding global minima in polynomial optimization. It is common for opti-

mization algorithms to instead search for local minima, with the hope that local minima are

easier to ﬁnd. This notion is not new; for example [72] provides an explicit example of a

class of POP where global minima are hard to ﬁnd but local minima are not. There has

also been renewed interest in ﬁnding local minima due to the growth of machine learning

applications, where local minima of highly nonconvex functions are sought for in practice

with simple ﬁrst-order methods like gradient descent. Our goal in Chapters 2 and 3 of this

thesis is to more formally understand the complexity of ﬁnding local minima (as well as some

related questions).

We point out that a priori there are no complexity implications between questions Q1

and Q2 stated above. For example, there is no reason to expect that an eﬃcient algorithm

for verifying that a given point is a local minimum would provide any guidance on how

one decide if a problem has a local minimum (this is in essence the dilemma of the famous

question “P = NP?”). Conversely, even if local minimality of a given point cannot always

be eﬃciently certiﬁed, that does not rule out the existence of algorithms that can eﬃciently

ﬁnd particular local minima that are easy to certify; see e.g. Question 3 of [87]. Thus the

complexities of these two questions need to be studied separately.

1This follows straightforwardly from the fact that NP-complete problems can be formulated as POPs; see

for example Section 2.2, Section 3.2, or Section 4.2.

2

One of the ﬁrst hardness results on local minima in the literature is due to Murty and

Kabadi [75], who show that the problems of deciding whether a given point is a local mini-

mum of a quadratic program or a local minimum of a quartic polynomial are NP-hard (see

Table 1.1). These were two of the ﬁrst results indicating that local minima are not neces-

sarily “easier” than global minima. In regards to Q1, along with prior classical results, they

only leave open the complexity of deciding whether a given point is a local minimum of a

cubic polynomial. We show that this last case is polynomial-time solvable in Section 2.3 of

this thesis.

max deg(qi)

deg(p)

1

2

3

∅

P

P

P

1

P

≥2

NP-hard

NP-hard NP-hard

[75],[85]

NP-hard NP-hard

(Theorem 2.3.3)

≥4

NP-hard

NP-hard NP-hard

[75]

Table 1.1: Complexity of deciding whether a point is a local minimum of a POP, based on

the degree of the objective p and the maximum degree of any constraint function qi. Entries

without a reference are either classical or implied by a stronger hardness result in the table.3

We next comment on Q2, the question of deciding whether a POP has a local minimum.

The complexity of Q2 based on the degrees of the deﬁning polynomials is presented in

Table 1.2. This problem has not been as extensively studied in prior literature, though it

3The NP-hardness of the case of linear objective and quadratic constraints is implied by the NP-hardness
of the quadratic programming case. Indeed, minimizing a quadratic function p(x) over a polyhedron Ax = b
can be reduced to minimizing a variable γ over the set {(x, γ) | Ax = b, , p(x) = γ}.

3

is more closely related to the problem of searching for a local minimum as compared to

Q1. In fact, for the cases labeled “P” in Table 1.2, some natural algorithms that ﬁnd local

minima implicitly check that they exist; see Section 2.1 for details in the unconstrained case.

In Chapter 2 of this thesis, we show that this is also the case for cubic polynomials.

In

particular, the problem of deciding if a cubic polynomial has a local minimum (and then

ﬁnding one) can be done by solving a polynomial number of polynomially-sized semideﬁnite

programs (SDPs), hence the label in Table 1.2. By contrast, we show that Q2 is intractable

in the same cases that Q1 is.

max deg(qi)

deg(p)

1

2

3

∅

P

P

1

P

≥2

NP-hard

NP-hard

NP-hard

(Theorem 3.2.5)

SDP

NP-hard

NP-hard

(Algorithm 2)

≥4

NP-hard

NP-hard

NP-hard

(Theorem 3.2.1)

Table 1.2: Complexity of deciding whether a POP has a local minimum, based on the degree

of the objective p and the maximum degree of any constraint function qi. Entries without a

reference are either classical or implied by a stronger hardness result in the table.

In many settings the existence of a local minimum in a POP is guaranteed; the focus is

then on ﬁnding a local minimum without needing to consider whether one exists. One way

this can be the case is when the feasible set is bounded, as is commonly the case in both

applications and POPs encoding classical combinatorial problems. For the speciﬁc case of

4

quadratic programs with bounded feasible sets, the question of the complexity of ﬁnding a

local minimum has appeared explicitly in the literature in [87]:

“What is the complexity of ﬁnding even a local minimizer for nonconvex quadratic

programming, assuming the feasible set is compact? Murty and Kabadi (1987)

and Pardalos and Schnitger (1988) have shown that it is NP-hard to test whether

a given point for such a problem is a local minimizer, but that does not rule out

the possibility that another point can be found that is easily veriﬁed as a local

minimzer.”

We settle this question in Chapter 3, where we show that unless P=NP, no polynomial-time

algorithm can even ﬁnd a point within a Euclidean distance of cn (for any constant c ≥ 0)

of a local minimum.

The ﬁnal complexity question we study for POPs is testing the existence of an optimal

solution when the optimal value of the problem is ﬁnite. This problem is in part motivated

by a question of Nie, Dummel, and Sturmfels [84], who provide an algorithm for solving an

unconstrained POP under the assumption that the optimal value is attained. The authors

remark

“This assumption is non-trivial, and we do not address (the important and diﬃ-

cult) question of how to verify that a given polynomial f (x) has this property.”

Most prior work on this question has focused on identifying cases where the existence of opti-

mal solutions is guaranteed. Perhaps the most classical example is the Bolzano-Weirstrauss

theorem for continuous functions over compact sets. Such theorems in the setting of POPs

are commonly referred to as Frank-Wolfe type theorems, due to the eponymous authors’

result that quadratic programs attain their optimal value when that value is ﬁnite [38]. For

POPs, this result was extended to cubic programs in [10], where it is shown that ﬁnite op-

timal values are always attained if the objective is at most cubic, and the constraints are

aﬃne. However, we will show that these are the only cases where this is true, and that

5

deciding whether a POP attains its optimal value is NP-hard in the remaining cases (see

Table 1.3).

deg(p)

max deg(qi)

∅

1

≥2

1

2

3

YES

YES

NP-hard

(Theorem 4.2.2)

NP-hard

NP-hard

YES

YES

YES

[38]

YES

[10]

≥4

NP-hard

NP-hard

NP-hard

(Theorem 4.2.1)

Table 1.3: Complexity of deciding whether a POP with a ﬁnite optimal value has an optimal

solution, based on the degree of the objective p and the maximum degree of any constraint

function qi. Entries without a reference are either classical or implied by a stronger hardness

result in the table. Note that whenever the degree of the objective is at most three, and the

feasible set is a polyhedron, there is no algorithm required; the answer is simply ‘yes’.

In this thesis, we also study an application of POPs to the problem of ﬁnding Nash equi-

libria in bimatrix games. Nash equilibria are a fundamental concept in economics, but they

also arise frequently in disciplines such as biology and ﬁnance. Finding a Nash equilibrium

however, is computationally intractable [31]. In the ﬁnal chapter of this thesis, we formulate

the problem of ﬁnding Nash equilibria in bimatrix games as a POP, and explore semideﬁ-

nite programming relaxations for ﬁnding approximate Nash equilibria. We also apply these

techinques to certain decision problems related to Nash equilibria.

6

1.1 Preliminaries

In this thesis, we will study the complexity of Q1-Q3 in the Turing model of computation.

Since polynomial functions of a given degree are ﬁnitely parameterized, they allow for a

convenient study of complexity questions in this setting. The size of a given instance is

determined by the number of bits required to write down the coeﬃcients of the polynomial

(and, in the case of Q1, the entries of the point x), which are all taken to be rational numbers.

For the purposes of analyzing the complexity of these three questions for POPs, we consider

the relevant setting in applications where the degrees of any polynomials are ﬁxed and the

number of variables in the POP increases. We are interested in the existence or non-existence

of eﬃcient algorithms for solving Q1-Q3 in this setting, as established theory (e.g. quantiﬁer

elimination theory [105, 100]) already yields exponential-time algorithms for them. We also

point out that our intractability results are in the strong sense, meaning that the problems

remain NP-hard even if the bitsize of all numerical data is O(log(n)), where n is the number

of variables in the problem. Unless P = NP, not even a pseudo-polynomial time algorithm

(an algorithm whose running time is polynomial in the magnitude of the numerical data, but

not their bitsize) can exist that solves a strongly NP-hard problem on all instances. This is in

contrast to problems such as knapsack [39], which can be solved tractably, e.g. by dynamic

programming, when the size of the numerical data is “small”. See [39] or [5, Section 2] for

more details on the distinction between weakly and strongly NP-hard problems.

A prevalent tool in this thesis will be sum of squares programming. More details will

be provided in each chapter as they are used, but we provide an introduction here. We say

that a polynomial p is a sum of squares (sos) if there exist polynomials q1, . . . , qr such that
p = (cid:80)r

i . This is an algebraic suﬃcient condition for global nonnegativity of a polynomial

i=1 q2

which is in general not necessary [49]. While deciding nonnegativity of a polynomial is in

general NP-hard (e.g., as a consequence of [75]), deciding whether a polynomial is sos can

be done via semideﬁnite programming. This is because a polynomial p of degree 2d in n
(cid:1) positive semideﬁnite
variables is a sum of squares if and only if there exists an (cid:0)n+d

(cid:1) × (cid:0)n+d

d

d

7

matrix Q satisfying the identity

p(x) = z(x)T Qz(x),

(1.2)

where z(x) denotes the vector of all monomials in x of degree less than or equal to d. Note

that because of this equivalence, one can also impose the constraint that a polynomial p with

unknown coeﬃcients is sos by semideﬁnite programming (see, e.g., [88]). Given a rank-r psd
matrix Q that satisﬁes (1.2), one can write Q as (cid:80)r
and obtain an sos decomposition of p as p = (cid:80)r

i (e.g. via a Cholesky factorization),

i=1 vivT

i=1(vT

i z(x))2.

Sum of squares polynomials have gained interest in the ﬁeld of polynomial optimization

because the problem of ﬁnding the inﬁmum of some polynomial p can be straightforwardly

reformulated into the problem

sup
γ∈R

γ

subject to p(x) − γ is a nonnegative polynomial.

(1.3)

This formulation has the interpretation of ﬁnding the largest lower bound on a polynomial.

Unfortunately, this problem cannot be eﬃciently solved, since testing whether a polynomial

of degree at least 4 is nonnegative is NP-hard and thus the constraint “p(x) − γ is a non-

negative polynomial” cannot be imposed in a tractable fashion. Therefore, to obtain what

is known as a “sum of squares relaxation”, this constraint is replaced by a sum of squares

constraint, which can be imposed tractably:

sup
γ∈R

γ

subject to p(x) − γ is sos.

(1.4)

Since the constraint “p(x) is sos” is a semideﬁnite constraint, (1.4) is an SDP. As any sos

polynomial is nonnegative, this gives a lower bound on the inﬁmum of p(x).

8

There are many extensions for constrained problems, with one of the more well-known

being based on Putinar’s Positivstellensatz [92]. Putinar’s Positivstellensatz states that if

the so-called Archimedean property is satisﬁed, then if a polynomial p is positive for all x in

the set {x ∈ Rn | qi(x) ≥ 0, ∀i = 1, . . . , m}, there exist sos polynomials σ0, . . . , σm such that

p(x) = σ0(x) +

m
(cid:88)

i=1

σi(x)qi(x).

(1.5)

The Archimedean property requires existence of a scalar R such that the polynomial
R − (cid:80)n

i belongs to the quadratic module of q1, . . . , qm, i.e., the set of polynomials that

i=1 x2

can be written as

τ0(x) +

m
(cid:88)

i=1

τi(x)qi(x),

where τ0, . . . , τm are sum of squares polynomials. This is an algebraic notion of the com-

pactness of the set {x ∈ Rn | qi(x) ≥ 0, ∀i = 1, . . . , m}, which is stronger than the geometric

notion. Similar to the construction of (1.4), for any positive integer d the following problem

gives a lower bound on the optimal value of (1.1):

γd := sup
γ∈R,σi

γ

subject to p(x) − γ = σ0(x) +

n
(cid:88)

i=1

σi(x)qi(x),

(1.6)

σi is an sos polynomial of degree at most 2d, ∀i = 0, . . . , m.

Taking d = 1, 2, . . . deﬁnes a sequence of problems referred to as the Lasserre hierarchy [61].

There are two primary properties of the Lasserre hierarchy which are of interest. The ﬁrst

is that for any ﬁxed d, the d-th problem in this sequence is an SDP of size polynomial in n.

The second is that under the Archimedean property,

γd = p∗, where p∗ is the optimal

lim
d→∞

value of the POP in (1.1). While this is a powerful property of the hierarchy, and in practice

the hierarchy is often exact at low levels, in general the level of the hierarchy needed can

be arbitrarily high, and the semideﬁnite programs involved become expensive very quickly.

9

Additionally, the Archimedean property is NP-hard to check, as we show in Section 4.3 of

this thesis.

1.2 Outline of this thesis

Complexity Results in Polynomial Optimization Chapters 2-4 this thesis focus on fun-

damental problems in polynomial optimization from an algorithmic perspective. Chapter 2

concerns itself with the complexity of local minima and related notions in unconstrained poly-

nomial optimization. In particular, it establishes that local minima and second-order points

of cubic polynomials can be found by solving polynomially many semideﬁnite programs of

polynomial size. In the negative direction, it establishes that the problems of deciding if

quartic polynomials have second-order points and whether cubic polynomials have critical

points are NP-hard. Notably, our approach for ﬁnding local minima of cubic polynomials

relies on circumventing the search for critical points. Chapter 3 establishes our intractability

results related to local minima in quadratic programming. In particular, we show that unless

P=NP, no polynomial-time algorithm can ﬁnd points within distance cn, for any constant

c ≥ 0, of a local minimum in an n-variate quadratic program with a bounded feasible set.

Chapter 4 focuses on the problem of testing attainment of optimal values, and settles the

complexity of deciding whether a POP with a ﬁnite optimal value has an optimal solution.

Semideﬁnite Relaxations for Bimatrix Games In the second part of this thesis, we

explore an application of semideﬁnite programming to game theory, in particular semideﬁnite

programming relaxations for ﬁnding Nash equilibria in bimatrix games. We show that for

a symmetric game, a symmetric 1/3-Nash equilibrium can be eﬃciently recovered from any

rank-2 solution to this relaxation. We also present semideﬁnite programming relaxations for

NP-hard decision problems related to Nash equilibria, such as that of ﬁnding the highest

achievable welfare under any Nash equilibrium.

10

1.3 Related Publications

The material in this thesis is based on the following work:

Chapter 2: Ahmadi, A. A., and Zhang, J. Complexity Aspects of Local Minima and

Related Notions. Available at arXiv:2008.06148.

Chapter 3: Ahmadi, A. A., and Zhang, J. On the Complexity of Finding a Local

Minimizer of a Quadratic Function over a Polytope. Available at arXiv:2008.05558.

Chapter 4: Ahmadi, A. A., and Zhang, J. On the Complexity of Testing Attainment of

the Optimal Value in Nonlinear Optimization. Mathematical Programming, 2019. Available

at arXiv:1803.07683.

Chapter 5: Ahmadi, A. A., and Zhang, J. Semideﬁnite Programming and Nash Equi-

libria in Bimatrix Games. To appear in INFORMS Journal on Computing. Available at

arXiv:1706.08550.

11

Chapter 2

On Local Minima and Related

Notions in Unconstrained Polynomial

Optimization

2.1

Introduction

In this chapter of the thesis, we address the complexity of questions Q1 and Q2 from Chap-

ter 1, but more generally for the following four types of points for a given polynomial

p : Rn → R:

(i) a critical point, i.e., a point x where the gradient ∇p(x) is zero,

(ii) a second-order point, i.e., a point x where ∇p(x) = 0 and the Hessian ∇2p(x) is positive

semideﬁnite (psd), i.e. has nonnegative eigenvalues,

(iii) a local minimum,

i.e., a point x for which there exists a scalar (cid:15) > 0 such that

p(x) ≤ p(y) for all y with (cid:107)y − x(cid:107) ≤ (cid:15),

(iv) a strict local minimum, i.e., a point x for which there exists a scalar (cid:15) > 0 such that

p(x) < p(y) for all y (cid:54)= x with (cid:107)y − x(cid:107) ≤ (cid:15).

12

We note the following straightforward implications between (i)-(iv):

strict local minimum ⇒ local minimum ⇒ second-order point ⇒ critical point.

Notions (i)-(iv) appear ubiquitously in nonconvex continuous optimization as surrogates

for global minima, since it is well understood that ﬁnding a global minimum of f is in general

an intractable problem. With regard to each of these four notions, we restate Q1 and Q2:

Q1*: Given a polynomial p : Rn → R and a point x ∈ Rn, is x of a given type (i)-(iv)?

Q2*: Given a polynomial p : Rn → R, does p have a point of a given type (i)-(iv) (and if

so, can one be found eﬃciently)?

As discussed in Chapter 1, a priori there are no complexity implications between these

two questions.

Let us ﬁrst comment on the complexity of Q1* and Q2* for some simple and classical

cases. For Q1*, checking whether a given point is a critical point of a polynomial function

(of any degree) can trivially be done in polynomial time simply by evaluating the gradient at

that point. To check that a given point is a second-order point, one can additionally compute

the Hessian matrix at that point and check that it is positive semideﬁnite. This can be done

in polynomial time, e.g., by performing Gaussian pivot steps along the main diagonal of the

matrix [76, Section 1.3.1] or by computing its characteristic polynomial and checking that

the signs of its coeﬃcients alternate [50, p. 403]. Since for aﬃne or quadratic polynomials,

any second-order point is a local minimum, the only remaining case of Q1* for them is that of

strict local minima. Aﬃne polynomials never have strict local minima, making the question

uninteresting. A point is a strict local minimum of a quadratic polynomial if and only if

it is a critical point and the associated Hessian matrix is positive deﬁnite (pd), i.e., has

positive eigenvalues. The latter property can be checked in polynomial time, for example

by computing the leading principal minors of the Hessian and checking that they are all

positive. As for Q2*, the aﬃne case is again uninteresting since there is a critical point

13

(which will also be a second-order point and a local minimum) if and only if the coeﬃcients

of all degree-one monomials are zero. For quadratic polynomials, since the entries of the

gradient are aﬃne, searching for critical points can be done in polynomial time by solving a

linear system. A candidate critcal point will be a second-order point (and a local minimum)

if and only if the Hessian is psd, and a strict local minimum if and only if the Hessian is pd.

Other than the aforementioned cases, the only prior result in the literature that we are

aware of is due to Murty and Kabadi [75], which settles the complexity of Q1* for degree-4

polynomials. Our contribution in this chapter is to settle the complexity of the remaining

cases for both Q1* and Q2*. A summary of the results is presented in Tables 2.1 and 2.2.

Entries denoted by “P” indicate that the problem can be solved in polynomial time. The

notation “SDP” indicates that the problem of interest can be reduced to solving either one

or polynomially-many semideﬁnite programs (SDP) whose sizes are polynomial in the size of

the input. (In fact, the reduction also goes in the other direction for second-order points and

local minima; see Theorems 2.5.3 and 2.5.4.) Finally, recall that a strong NP-hardness result

implies that the problem of interest remains NP-hard even if the size (i.e. bit length) of the

coeﬃcients of the polynomial is O(log(n)), where n is the number of variables. Therefore,

unless P=NP, even a pseudo-polynomial time algorithm (i.e., an algorithm whose running

time is polynomial in the magnitude of the coeﬃcients, but not necessarily their bit length)

cannot exist for the indicated problems in these tables.

14

Q1*: property vs. degree

1

2

Critical point

Second-order point

Local minimum

P P

P P

P P

3

P

P

P

≥ 4

P

P

strongly NP-hard [75]1

Strict local minimum

P P

P

strongly NP-hard [75]1

(Theorem 2.3.3)

(Corollary 2.3.5)

Table 2.1: Complexity of deciding whether a given point is of a certain type, based on the

degree of the polynomial. Entries without a reference are classical.

Q2*: property vs. degree

1

2

3

≥ 4

Critical point

P P

strongly NP-hard

strongly NP-hard

(Theorem 2.2.1)

(Theorem 2.2.1)

Second-order point

P P

SDP

strongly NP-hard

(Corollary 2.6.5)

(Theorem 2.2.2)

Local minimum

P P

SDP

strongly NP-hard

(Algorithm 2)

(Theorem 3.2.1)

Strict local minimum

P P

SDP

strongly NP-hard

(Algorithm 2, Remark 2.6.1)

(Corollary 3.2.4)

Table 2.2: Complexity of deciding whether a polynomial has a point of a certain type, based

on the degree of the polynomial. Entries without a reference are classical.

1The proof in [75] is based on a reduction from the “matrix copositivity” problem. However, [75] only
shows that this problem (and thus deciding if a quartic polynomial has a local minimum) is weakly NP-
hard, since the reduction to matrix copositivity there is from the weakly NP-hard problem of Subset Sum.
Nonetheless, their result can be strengthened by observing that testing matrix copositivity is in fact strongly
NP-hard. This claim is implicit, e.g., in [35, Corollary 2.4]. The NP-hardness of testing whether a point is
a strict local minimum of a quartic polynomial is not explicitly stated in [75], though it follows in the weak
sense from the weak NP-hardness of Problem 8 of [75]. Again, with some work, this can be strengthened to
a strong NP-hardness result.

15

The majority of the technical work in this chapter is spent on the case of cubic polyno-

mials. It is somewhat surprising that many of the problems of interest to us are tractable

for cubics, especially the search for local minima. This is in contrast to the intractability of

other interesting problems related to cubic polynomials, for example, minimizing them over

the unit sphere [79], or checking their convexity over a box [5]. It is also interesting to note

that second-order points of cubic polynomials are easier to ﬁnd than their critical points,

despite being a more restrictive type of point. This shows that the right approach to ﬁnding

second-order points involves bypassing the search for critical points as an initial step.

2.1.1 Organization and Main Contributions of the Chapter

Section 2.2 covers the NP-hardness results from Table 2.2. The remainder of the chapter is

devoted to our results on cubic polynomials, which ﬁlls in the remaining entries of Tables 2.1

and 2.2.

In Section 2.3, we give a characterization of local minima of cubic polynomials

(Theorem 2.3.1) and show that it can be checked in polynomial time (Theorem 2.3.3). In

Section 2.4, we give some geometric facts about local minima of cubic polynomials. For

example, we show that the set of local minima of a cubic polynomial p is convex (Theo-

rem 2.4.3), and we relate this set to the second-order points of p and to the set of minima

of p over points where ∇2p is positive semideﬁnite (Theorem 2.4.7 and Theorem 2.4.10). In

Section 2.4.4, we show that the interior of any spectrahedron is the projection of the local

minima of some cubic polynomial (Theorem 2.4.12). In Section 2.5, we use this result to

show that deciding if a cubic polynomial has a local minimum or a second-order point is at

least as hard as some semideﬁnite feasibility problems.

In Section 2.6, we start from a “sum of squares” approach to ﬁnding second-order points

of a cubic polynomial (Theorem 2.6.2 and Theorem 2.6.3), and build upon it (Section 2.6.3)

to arrive at an eﬃcient semideﬁnite representation of these points (Corollary 2.6.5). This also

leads to an algorithm for ﬁnding local minima of cubic polynomials by solving polynomially-

many SDPs of polynomial size (Algorithm 2).

In Section 2.7, we take preliminary steps

16

towards some interesting future research directions, such as the design of an unregularized

third-order Newton method that would use as a subroutine our algorithm for ﬁnding local

minima of cubic polynomials (Section 2.7.2).

2.1.2 Preliminaries and Notation

We review some standard facts about local minina; more preliminaries speciﬁc to cubic poly-

nomials appear in Section 2.3.1. Three well-known optimality conditions in unconstrained

optimization are the ﬁrst-order necessary condition (FONC), the second-order necessary

condition (SONC), and the second-order suﬃcient condition (SOSC). Respectively, they are

that the gradient at any local minimum is zero, the Hessian at any local minimum is psd,

and that any critical point at which the Hessian is positive deﬁnite is a strict local minimum.

A vector d ∈ Rn is said to be a descent direction for a function p : Rn → R at a point ¯x ∈ Rn

if there exists a scalar (cid:15) > 0 such that p(¯x + αd) < p(¯x) for all α ∈ (0, (cid:15)). Existence of a

descent direction at a point clearly implies that the point is not a local minimum. However,

in general, the lack of a descent direction at a point does not imply that the point is a local

minimum (see, e.g., Example 2.3.2).

Next, we establish some basic notation which will be used throughout the chapter. We

denote the set of n × n real symmetric matrices by Sn×n. For a matrix M ∈ Sn×n, the

notation M (cid:23) 0 denotes that M is positive semideﬁnite, M (cid:31) 0 denotes that it is positive

deﬁnite, and Tr(M ) denotes its trace, i.e. the sum of its diagonal entries. For a matrix M ,

the notation N (M ) denotes its null space, and C(M ) denotes its column space. All vectors

are taken to be column vectors. For two vectors x and y, the notation (x, y) denotes the





vector




x

y


. The notation 0n denotes the vector of length n containing only zeros. The

notation ei denotes the i-th coordinate vector, i.e., the vector with a one in its i-th entry

and zeros everywhere else.

17

2.2 NP-hardness Results

In this section, we present reductions that show our NP-hardness results from Tables 2.1 and

2.2. For concreteness, we construct these reductions from the (simple) MAXCUT problem,

though our proof can work with any NP-hard problem that can be encoded by quadratic

equations with “small enough” coeﬃcients. Recall that in the (simple) MAXCUT problem,

we are given as input an undirected and unweighted graph G on n vertices and an integer

k ≤ n. We are then asked whether there is a cut in G of size k, i.e. a partition of the

vertices into two sets S1 and S2 such that the number of edges with one endpoint in S1 and

one endpoint in S2 is equal to k. It is well known that the (simple) MAXCUT problem is

strongly NP-hard [39].

If we denote the adjacency matrix of G by E ∈ Sn×n, it is straightforward to see that G

has a cut of size k if and only if the following system of quadratic equations is feasible:

q0(x) :=

1
4

n
(cid:88)

n
(cid:88)

i=1

j=1

Eij(1 − xixj) − k = 0,

qi(x) := x2

i − 1 = 0, i = 1, . . . , n.

(2.1)

Indeed, the second set of constraints enforces each variable xi to be −1 or 1, and any

x ∈ {−1, 1}n encodes a cut in G by assigning vertices with xi = 1 to one side of the

partition, and those with xi = −1 to the other. Observe that with this encoding, xixj equals

1 whenever the two vertices i and j are on the same side and −1 otherwise. The size of the

cut is therefore given by 1
4

(cid:80)n

i=1

(cid:80)n

j=1 Eij(1 − xixj), noting that every edge is counted twice.

Theorem 2.2.1. It is strongly NP-hard to decide whether a polynomial p : Rn → R of degree

greater than or equal to three has a critical point.

Proof. Let d ≥ 3 be ﬁxed. Given an instance of the (simple) MAXCUT problem with a

graph on n vertices, let the quadratic polynomials q0, . . . , qn be as in (2.1), and consider the

18

following degree-d polynomial in 2n + 2 variables (x1, . . . , xn, y0, y1, . . . , yn, z):

p(x, y, z) =

n
(cid:88)

i=0

yiqi(x) + zd.

Note that all coeﬃcients of this polynomial take O(log(n)) bits to write down. We show that

p(x, y, z) has a critical point if and only if the quadratic system q0(x) = 0, . . . , qn(x) = 0 is

feasible. Observe that the gradient of p is given by






















=











































∂p
∂x

∂p
∂y

∂p
∂z

∂qi
∂x1

(x)

(cid:80)n

i=0 yi
...

(cid:80)n

i=0 yi

∂qi
∂xn

(x)

q0(x)
...

qn(x)

dzd−1






















.

If ¯x ∈ Rn is a solution to (2.1), then the point (¯x, 0n+1, 0) is a critical point of p. Con-

versely, if (¯x, ¯y, ¯z) is a critical point of p, then, since ∂p

∂y (¯x, ¯y, ¯z) = 0, ¯x must be a solution to

(2.1).

Theorem 2.2.2. It is strongly NP-hard to decide whether a polynomial p : Rn → R of degree

greater than or equal to four has a second-order point.

Proof. Let d ≥ 4 be ﬁxed. Given an instance of the (simple) MAXCUT problem with a

graph on n vertices, let the quadratic polynomials q0, . . . , qn be as in (2.1), and consider the

following degree-d polynomial in 3n + 3 variables (x1, . . . , xn, y0, y1, . . . , yn, z0, z1, . . . , zn, w):

p(x, y, z, w) =

n
(cid:88)

i=0

(cid:0)y2

i qi(x) − z2

i qi(x)(cid:1) + wd.

19

Note that all coeﬃcients of this polynomial take O(log(n)) bits to write down. We

show that p(x, y, z, w) has a second-order point if and only if the quadratic system

q0(x) = 0, . . . , qn(x) = 0 is feasible.

Observe that ∂2p

∂y2 is an (n + 1) × (n + 1) diagonal matrix with 2q0(x), . . . , 2qn(x) on its

diagonal. Similarly, ∂2p

∂z2 is an (n + 1) × (n + 1) diagonal matrix with −2q0(x), . . . , −2qn(x)

on its diagonal.

Suppose ﬁrst that (¯x, ¯y, ¯z, ¯w) is a second-order point of p.

Since

∇2p(¯x, ¯y, ¯z, ¯w) (cid:23) 0, and since ∂2p

∂y2 and ∂2p

∂z2 are both principal submatrices of ∇2p,

it

must be that q0(¯x) = 0, . . . , qn(¯x) = 0.

Now suppose that ¯x ∈ Rn is a solution to (2.1). We show that (¯x, 0n+1, 0n+1, 0), is a

second-order point of p. Note that ∂p

∂x is quadratic in y and z, ∂p
∂z is lin-
∂w = dwd−1. Thus (¯x, 0n+1, 0n+1, 0) is a critical point of p. Now observe

∂y is linear in y, ∂p

ear in z, and ∂p

that the entries of ∂2p

∂x2 are quadratic in y and z or are zero, the entries of

∂2p
∂x∂y are lin-
∂w2 = d(d − 1)wd−2,
ear in y or are zero, the entries of
∂y2 (¯x, 0n+1, 0n+1, 0) and ∂2p
∂z2 (¯x, 0n+1, 0n+1, 0) are both zero, and all other entries of ∇2p are
zero. Thus ∇2p(¯x, 0n+1, 0n+1, 0) = 0, and we conclude that (¯x, 0n+1, 0n+1, 0) is a second-order

∂x∂z are linear in z or are zero, ∂2p

∂2p

∂2p

point of p.

The remaining two NP-hardness results from Table 2.2 are stated next, but proven in

Chapter 3, since a corollary of them is the main result of that chapter.

Theorem 2.2.3. It is strongly NP-hard to decide whether a polynomial p : Rn → R of degree

greater than or equal to four has a local minimum. The same statement holds for testing

existence of a strict local minimum.

20

2.3 Checking Local Minimality of a Point for a Cubic

Polynomial

As the reader can observe from Tables 2.1 and 2.2 from Section 2.1, the remaining entries

all have to do with the case of cubic polynomials. To answer these questions about cubics,

we start in this section by showing that the problem of deciding if a given point is a local

minimum (or a strict local minimum) of a cubic polynomial is polynomial-time solvable.

This answers the remaining cases in Table 2.1. We ﬁrst make certain observations about

cubic polynomials that will be used throughout the chapter.

2.3.1 Preliminaries on Cubic Polynomials

It is easy to observe that a univariate cubic polynomial has either no local minima, exactly

one local minimum (which is strict), or inﬁnitely many non-strict local minima (in the case

that the polynomial is constant). Further observe that if a point ¯x ∈ Rn is a (strict) local

minimum of a function p : Rn → R, then for any ﬁxed point ¯y ∈ Rn, the restriction of p to

the line going through ¯x and ¯y —i.e. the univariate function q(α) := p(¯x + α(¯y − ¯x))—has a

(strict) local minimum at α = 0. Since the restriction of a multivariate cubic polynomial to

any line is a univariate polynomial of degree at most three, the previous two facts imply that

(i) if a cubic polynomial has a strict local minimum, then it must be the only local minimum

(strict or non-strict), and that (ii) if a cubic polynomial has multiple local minima, then the

polynomial must be constant on the line connecting any two of these (necessarily non-strict)

local minima.

Observe that for any cubic polynomial p, the error term of the second-order Taylor

expansion is given by the cubic homogeneous component of p. More formally, for any point

¯x ∈ Rn and direction v ∈ Rn,

p(¯x + λv) = p3(v)λ3 +

1
2

vT ∇2p(¯x)vλ2 + ∇p(¯x)T vλ + p(¯x),

(2.2)

21

where p3 is the collection of terms of p of degree exactly 3.

Note that the Hessian of any cubic n-variate polynomial is an aﬃne matrix of the form

(cid:80)n

i=1 xiHi + Q, where Hi and Q are all n × n symmetric matrices and the Hi satisfy

(Hi)jk = (Hj)ik = (Hk)ij

(2.3)

for any i, j, k ∈ {1, . . . , n}. This is because an n × n symmetric matrix A(x) := A(x1, . . . , xn)

is a valid Hessian matrix if and only if
{1, . . . , n}. If (cid:80)n

∂
∂xi

Ajk(x) = ∂
∂xj

Aik(x) = ∂
∂xk

Aij(x) for all i, j, k ∈

i=1 xiHi + Q is a valid Hessian matrix, then the cubic polynomial which

gives rise to it is of the form

1
6

n
(cid:88)

i=1

xT xiHix +

1
2

xT Qx + bT x + c.

(2.4)

In this chapter, it is sometimes convenient for us to parametrize a cubic polynomial in the

above form. As the scalar term in (2.4) is irrelevant for deciding local minimality or ﬁnding

local minima, in the remainder of this chapter, we take c = 0 without loss of generality.

Observe that the gradient of the polynomial in (2.4) is 1
2

(cid:80)n

i=1 xiHix+Qx+b, or equivalently

a vector whose i-th entry is 1

2xT Hix + eT

i Qx + bi.

2.3.2 Local Minimality of a Point for a Cubic Polynomial

In this section, we give a characterization of local minima of cubic polynomials and show that

this characterization can be checked in polynomial time. Recall that we use the notation

p3 to denote the cubic homogeneous component of a cubic polynomial p, and N (M ) (resp.

C(M )) to denote the null space (resp. column space) of a matrix M .

Theorem 2.3.1. A point ¯x ∈ Rn is a local minimum of a cubic polynomial p : Rn → R if

and only if the following three conditions hold:

• ∇p(¯x) = 0,

22

• ∇2p(¯x) (cid:23) 0,

• ∇p3(d) = 0, ∀d ∈ N (∇2p(¯x)).

Note that the ﬁrst two conditions are the well-known FONC and SONC. Throughout the

chapter, we refer to the third condition as the third-order condition (TOC) for optimality.

This condition is requiring the gradient of the cubic homogeneous component of p to vanish

on the null space of the Hessian of p at ¯x. We remark that the FONC, SONC, and TOC

together are in general neither suﬃcient nor necessary for a point to be a local minimum of

a polynomial of degree higher than three. The ﬁrst claim is trivial (consider, e.g., p(x) = x5

at x = 0); for the second claim see Example 2.3.3.

Remark 2.3.1. It is straightforward to see that any local minimum ¯x of a cubic polynomial

p satisﬁes a condition similar to the TOC, that p3(d) = 0, ∀d ∈ N (∇2p(¯x)). Indeed, if ¯x is a

second-order point and d ∈ N (∇2p(¯x)), then Equation (2.2) gives p(¯x+λd) = p3(d)λ3 +p(¯x).

Hence, if p3(d) is nonzero, then either d or −d is a descent direction for p at ¯x, and so ¯x

cannot be a local minimum. This observation was made in [9] for three-times diﬀerentiable

functions, and is referred to as the “third-order necessary condition” (TONC) for optimality.

Note that because p3 is homogeneous of degree three, from Euler’s theorem for homogeneous

functions we have 3p3(x) = xT ∇p3(x). We can then see that ∇p3(d) = 0 ⇒ p3(d) = 0, and

therefore the TOC is a stronger condition than the TONC. Indeed, the FONC, SONC, and

TONC together are not suﬃcient for local optimality of a point for a cubic polynomial; see

Example 2.3.2. Intuitively, this is because the FONC, SONC, and TONC together avoid

existence of a descent direction for cubic polynomials, but as the proof of Theorem 2.3.1 will

show, existence of a “descent parabola” must also be avoided.

We will need the following fact from linear algebra for the proof of Theorem 2.3.1.

Lemma 2.3.2. Let M ∈ Sn×n be a symmetric positive semideﬁnite matrix and denote its

smallest positive eigenvalue by λ+. Then if z ∈ C(M ) and (cid:107)z(cid:107) = 1, zT M z ≥ λ+.

23

Proof. Suppose M has eigenvalues λ1 ≥ λ2 ≥ · · · ≥ λk > λk+1 = · · · = λn = 0 (so λ+ = λk).

Let v1, . . . , vn be a set of corresponding mutually orthogonal unit-norm eigenvectors of M .
Observe that any z ∈ C(M ) can be written as z = (cid:80)n

i=1 αivi, for some scalars αi with αi = 0

for i = k + 1, . . . , n. This is because the column space is orthogonal to the null space, and

the eigenvectors corresponding to zero eigenvalues span the null space.

Since v1, . . . , vk are mutually orthogonal unit vectors, we have

zT M z =

(cid:32) k

(cid:88)

i=1

(cid:33)T (cid:32) k

(cid:88)

(cid:33) (cid:32) k

(cid:88)

(cid:33)

αivi

=

λivivT
i

αivi

i=1

i=1

k
(cid:88)

i=1

i λivT
α2

i vivT

i vi =

k
(cid:88)

i=1

α2

i λi,

and

1 = (cid:107)z(cid:107)2 =

(cid:32) k

(cid:88)

i=1

(cid:33)T (cid:32) k

(cid:88)

(cid:33)

αivi

=

αivi

i=1

k
(cid:88)

i=1

i vT
α2

i vi =

k
(cid:88)

i=1

α2
i .

These two equations combined imply that zT M z ≥ λk = λ+.

Proof (of Theorem 2.3.1). As any local minimum must satisfy the FONC and SONC, it

suﬃces to show that a second-order point is a local minimum for a cubic polynomial if and

only if it also satisﬁes the TOC.

We ﬁrst observe that for any second-order point ¯x, scalars α and β, and vectors d ∈

N (∇2p(¯x)) and z ∈ Rn, the following identity holds:

p(¯x + αd + βz) = p3(αd + βz) +

1
2

(αd + βz)T ∇2p(¯x)(αd + βz) + p(¯x)

= β3p3(z) +

= β3p3(z) +

β2
2
αβ2
2

zT ∇2p3(αd)z + β∇p3(αd)T z + p3(αd) +

β2
2

zT ∇2p(¯x)z + p(¯x)

zT ∇2p3(d)z + α2β∇p3(d)T z + α3p3(d) +

β2
2

zT ∇2p(¯x)z + p(¯x).

(2.5)

The ﬁrst equality follows from (2.2) and the FONC. The second equality follows from the

Taylor expansion of p3(αd + βz) around αd and using the fact that d ∈ N (∇2p(¯x)). The

last equality follows from homogeneity of p3.

(second-order point) + TOC ⇒ local minimum:

24

Let ¯x be any second-order point at which the TOC holds. Note that any vector v ∈ Rn can

be written as αd + βz for some (unique) scalars α and β, and unit vectors d ∈ N (∇2p(¯x))

and z ∈ C(∇2p(¯x)). Since from the TOC we have ∇p3(d) = 0 (which also implies that

p3(d) = 0, as seen e.g. by Euler’s theorem for homogeneous functions mentioned above), the

identity in (2.5) reduces to

p(¯x + v) − p(¯x) = β2

(cid:18)

βp3(z) +

α
2

zT ∇2p3(d)z +

zT ∇2p(¯x)z

(cid:19)

.

1
2

(2.6)

Let λ > 0 be the smallest nonzero eigenvalue of ∇2p(¯x). From Lemma 2.3.2 we have that

zT ∇2p(¯x)z ≥ λ. Thus, if α and β satisfy

(cid:18)

|α| + |β| ≤ λ ·

max
(cid:107)z(cid:107)=1,(cid:107)d(cid:107)=1

max{zT ∇2p3(d)z, 2p3(z)}

(cid:19)−1

,

(2.7)

the expression on the right-hand side of (2.6) is nonnegative. As the set {(cid:107)z(cid:107) = 1} ∩ {(cid:107)d(cid:107) = 1}

is compact and p3 is continuous and odd, the quantity

γ := max

(cid:107)z(cid:107)=1,(cid:107)d(cid:107)=1

max{zT ∇2p3(d)z, 2p3(z)}

is ﬁnite and nonnegative, and thus λ/γ is positive (or potentially +∞). Finally, note that

for any v ∈ Rn such that (cid:107)v(cid:107) ≤ λ/γ, the corresponding α and β satisfy (2.7), and thus

p(¯x + v) − p(¯x) ≥ 0 as desired.

Local minimum ⇒ TOC:

Note that if ¯x is a local minimum, then we must have p3(d) = 0 whenever d ∈ N (∇2p(¯x))

(see Remark 2.3.1). We also assume that p3 is not the zero polynomial, as then the TOC

would be automatically satisﬁed.

25

Now suppose for the sake of contradiction that there exists a vector ˆd ∈ N (∇2p(¯x)) such

that ∇p3( ˆd) (cid:54)= 0. Consider the sequence of points given by

ˆxi := ¯x + αi

ˆd + βiz,

(2.8)

where

z = −

∇p3( ˆd)
(cid:107)∇p3( ˆd)(cid:107)

, αi =

(cid:115)

1
i

zT ∇2p(¯x)z
|∇p3( ˆd)T z|

, βi =

1
i2 .

Observe that ˆxi → ¯x as i → ∞. From (2.5), we have

p(¯x + αi

ˆd + βiz) − p(¯x) = p3(z)β3

i +

1
2

zT ∇2p3( ˆd)zαiβ2

i + ∇p3( ˆd)T zα2

i βi +

1
2

zT ∇2p(¯x)zβ2
i .

Note that because αi ∝

√

βi, the third and fourth terms of the right-hand side of the above

expression will be the dominant terms as i → ∞. For our choices of αi and βi, the sum of

these two dominant terms simpliﬁes to − 1

2i4 zT ∇2p(¯x)z. Observe that for any w ∈ N (∇2p(¯x))
and any α ∈ R, p3( ˆd + αw) = 0. Since the gradient of p3 is orthogonal to its level sets, we
must then have ∇p3( ˆd)T w = 0 for any w ∈ N (∇2p(¯x)). Thus, ∇p3( ˆd) is in the orthogonal

complement of N (∇2p(¯x)), i.e.

in C(∇2p(¯x)), and hence zT ∇2p(¯x)z > 0. Thus, for any

suﬃciently large i, p(ˆxi) < p(¯x), and so ¯x is not a local minimum.

Remark 2.3.2. Note that the points ˆxi constructed in (2.8) trace a parabola as i ranges from

−∞ to +∞. Thus as a corollary of the proof of Theorem 2.3.1, we see that if a point ¯x ∈ Rn

is not a local minimum of a cubic polynomial p : Rn → R, then there must exist a “descent

parabola” that certiﬁes that; i.e. a parabola q(t) : R → Rn and a scalar ¯α satisfying q(0) = ¯x

and p(q(α)) < p(¯x) for all α ∈ (0, ¯α).

Theorem 2.3.1 gives rise to the following algorithmic result.

Theorem 2.3.3. Local minimality of a point ¯x ∈ Rn for a cubic polynomial p : Rn → R can

be checked in polynomial time.

26

Proof. In view of Theorem 2.3.1, we show that the FONC, SONC, and TOC can be checked

in polynomial time (in the Turing model of computation). Checking that the gradient of p

vanishes at ¯x and that the Hessian at ¯x is positive semideﬁnite can be done in polynomial time

as explained in Section 2.1. We give the following polynomial-time algorithm for checking

the TOC:

Algorithm 1 Algorithm for checking the TOC.
1: Input: Coeﬃcients of a cubic polynomial p : Rn → R, a point ¯x ∈ Rn

2: Compute ∇2p(¯x)

3: Compute a rational basis {v1, . . . , vk} for the null space of ∇2p(¯x)
4: Check if coeﬃcients of g(λ) := ∇p3((cid:80)k

i=1 λivi) are all zero

5:

6:

7:

8:

if YES

¯x is a local minimum of p

if NO

¯x is a not local minimum of p

Note that the entries of the function g : Rk → Rn that appears in this algorithm are homo-

geneous quadratic polynomials in λ := (λ1, . . . , λk), where k is the dimension of N (∇2p(¯x)).

For the TOC to hold, g must be zero for all λ ∈ Rk, which happens if and only if all

coeﬃcients of every entry of g are zero.

A rational basis for the null space of a symmetric matrix can be computed in polynomial

time, for example through the Bareiss algorithm [15]. For completeness, we give a less

eﬃcient but also polynomial-time algorithm which solves a series of linear systems. The ﬁrst

linear system ﬁnds a nonzero vector v1 ∈ Rn such that ∇2p(¯x)T v1 = 0. The successive linear

systems solve for nonzero vectors vi ∈ Rn such that ∇2p(¯x)T vi = 0, vT

j vi = 0, ∀j = 1, . . . , i−1.

To ensure nonzero solutions, some entry of the vector is ﬁxed to 1, and if the system is

infeasible, the next entry is ﬁxed to 1 and the system is re-solved. Once the only feasible

vector is the zero vector, the basis is complete.

27

The next step is to compute the coeﬃcients of g. To do this, one can ﬁrst compute the
(cid:1) coeﬃcients to compute, and each is a coeﬃcient of

coeﬃcients of ∇p3. There are n × (cid:0)n+1
p3, multiplied by 1, 2, or 3. If the m-th entry of ∇p3 is given by (cid:80)n
the m-th entry of g is equal to gm(λ) = (cid:80)n

(cid:80)n

j≥i cijxixj, then

j≥i cij(va)i(vb)j)λaλb, where the

b=1((cid:80)n

(cid:80)n

(cid:80)n

a=1

i=1

i=1

2

vectors {vi} are our rational basis for N (∇2p(¯x)). Observe that gm is a polynomial in λ whose

coeﬃcients can be computed with a polynomial number of additions and multiplications over

polynomially-sized scalars, and thus checking if all these coeﬃcients are zero for every m can

be done in polynomial time.

Let us end this subsection by also giving an eﬃcient characterization of strict local minima

of cubic polynomials.

Corollary 2.3.4. A point ¯x ∈ Rn is a strict local minimum of a cubic polynomial p : Rn → R

if and only if

• ∇p(¯x) = 0,

• ∇2p(¯x) (cid:31) 0.

Proof. The fact that these two conditions are suﬃcient for local minimality is immediate

from the SOSC. To show the converse, in view of the FONC, we only need to show that

positive deﬁniteness of the Hessian is necessary. Suppose for the sake of contradiction that

for some nonzero vector d ∈ Rn, we have dT ∇2p(¯x)d = 0 (note that in view of the SONC,

we cannot have dT ∇2p(¯x)d < 0). From (2.2), we have p(¯x + αd) = p(¯x) + p3(d)α3. Hence,

α = 0 is not a strict local minimum of the univariate polynomial p(¯x + αd), and so ¯x is not

a strict local minimum of p.

Corollary 2.3.5. Strict local optimality of a point ¯x ∈ Rn for a cubic polynomial p : Rn → R

can be checked in polynomial time.

28

Proof. This follows from the characterization in Corollary 2.3.4. Checking the FONC is

straightforward as before. As explained in Section 2.1, to check that ∇2p(¯x) is positive

deﬁnite, one can equivalently check that all n leading principal minors of ∇2p(¯x) are positive.

This procedure takes polynomial time since determinants can be computed in polynomial

time.

2.3.3 Examples

We give a few illustrative examples regarding the application and context of Theorem 2.3.1.

Figure 2.1: Contour plots of x2
1x2 (right) from Examples 2.3.1 and 2.3.2.
The polynomials are zero on the black lines, positive on the gray regions, and negative on
the white regions. The dashed line in the right-side ﬁgure denotes a descent parabola at the
origin.

1x2 (left) and x2

2 − x2

Example 2.3.1. A cubic polynomial with local minima

Consider the polynomial p(x1, x2) = x2

1x2. By inspection (see Figure 2.1), one can see

that points of the type {(x1, x2) | x1 = 0, x2 > 0} are local minima of p, as p is nonnegative

when x2 > 0, zero whenever x1 = 0, and positive whenever x2 > 0 and x1 (cid:54)= 0. As a sanity

check, we use Theorem 2.3.1 to verify that the point (0, 1) is a local minimum of p (the same

reasoning applies to all other local minima).

Through straightforward computation, we ﬁnd

29

∇p(x) =






2x1x2

x2
1






 , ∇p3(x) =




2x1x2

x2
1


 , ∇2p(x) =







2x2 2x1

2x1

0




 .

We can see that the FONC and SONC are satisﬁed at (0, 1). The null space of ∇2p(0, 1) is

spanned by (0, 1). We have





∇p3


α









 =





0


1

2(0)(α)

(0)2




 = 0,

which shows that the TOC is satisﬁed, verifying that (0, 1) is a local minimum of p.

One can also verify that {(x1, x2) | x1 = 0, x2 > 0} are the only local minima. Indeed, the

critical points of p are those where x1 = 0, and the second-order points are those where x1 = 0

and x2 ≥ 0. To see that (0, 0) is not a local minimum, observe that (1, 1) ∈ N (∇2p(0, 0)),

but ∇p3(1, 1) = (2, 1) (cid:54)= 0, and thus the TOC is violated.

Example 2.3.2. A cubic polynomial with no local minima

We use Theorem 2.3.1 to show that the polynomial p(x1, x2) = x2

2 − x2

1x2 has no local

minima. We have

∇p(x) =










−2x1x2

2x2 − x2
1


 , ∇p3(x) =





 , ∇2p(x) =










 .

−2x2 −2x1

−2x1

2

−2x1x2

−x2
1

Observe that (0, 0) is the only second-order point of p. The null space of ∇2p(0, 0) is spanned

by (1, 0). We have





∇p3


α





1


0







−2(α)(0)


 =





 =

−(α)2






0

−α2




 (cid:54)= 0,

which shows that the TOC is violated, and hence (0, 0) is not a local minimum. Note that

the TONC is in fact satisﬁed at (0, 0), since p3(α, 0) = 0 for any scalar α.

30

It is also interesting to observe that there are no descent directions for p at (0, 0) (this is

implied, e.g., by satisfaction of the TONC, along with the FONC and SONC). However, we

can use the proof of Theorem 2.3.1 to compute a descent parabola, thereby more explicitly

demonstrating that (0, 0) is not a local minimum. The column space of ∇2p(0, 0) is spanned

by (0, 1). Then, following the proof of Theorem 2.3.1 with z = (0, 1) and ˆd = (1, 0),

we have zT ∇2p(0, 0)z = 2 and |∇p3( ˆd)T z| = 1. The parabola prescribed is then the set

{(x1, x2) | x2 = 1

2x2

1}. Indeed, one can now verify that except at (0, 0), p is negative on the

entire parabola; see the dashed line in Figure 2.1.

Example 2.3.3. A quartic polynomial with a local minimum that does not satisfy

the TOC

We show in this example that for polynomials of degree higher than three, the TOC

is not a necessary condition for local minimality. Consider the quartic polynomial given

by p(x1, x2) = 2x4

1 + 2x2

1x2 + x2

2. The point (0, 0) is a local minimum, as p(0, 0) = 0 and

p(x1, x2) = x4

1 + (x2

1 + x2)2 is nonnegative. However, the Hessian of p at (0, 0) is

∇2p(0, 0) =






0 0

0 2




 ,





which has a null space spanned by (1, 0). We observe that ∇p3(x1, x2) =

vanish on this null space, as it evaluates, for example, to (0, 2) at (1, 0).




4x1x2

2x2
1


 does not

2.4 On the Geometry of Local Minima of Cubic Poly-

nomials

We have shown that deciding local minimality of a given point for a cubic polynomial is a

polynomial-time solvable problem. We now turn our attention to the remaining unresolved

31

entries in Table 2.2 from Section 2.1, which are on the problems of deciding whether a

cubic polynomial has a second-order point, a local minimum, or a strict local minimum. In

Sections 2.5 and 2.6, we will show that these problem can all be reduced to semideﬁnite

programs of tractable size. In the current section, we present a number of geometric results

about local minima and second-order points of cubic polynomials which are used in those

sections, but are possibly of independent interest. For the remainder of this chapter, we use

the notation SOp to denote the set of second-order points of a polynomial p, LMp to denote

the set of its local minima, and ¯S to denote the closure of a set S.

2.4.1 Convexity of the Set of Local Minima

We begin by showing that for any cubic polynomial p, the set LMp is convex. We go

through two lemmas; the ﬁrst is a simple algebraic observation, and the second contains

information about some critical points. Recall that the Hessian of a cubic polynomial p
written in the form of (2.4) is given by (cid:80)n

i=1 xiHi + Q. Furthermore, its gradient is given by

(cid:80)n

i=1 xiHix + Qx + b, or equivalently a vector whose i-th entry is xT Hix + eT

i Qx + bi.

1
2

Lemma 2.4.1. Let H1, . . . , Hn ⊆ Sn×n satisfy (2.3). Then for any two vectors y, z ∈ Rn,

(cid:33)

yiHi

z =

(cid:32) n

(cid:88)

i=1

(cid:33)

ziHi

y.

(cid:32) n

(cid:88)

i=1

Proof. Observe that for any index k ∈ {1, . . . , n}, we have

(cid:32)(cid:32) n

(cid:88)

(cid:33)

(cid:33)

yiHi

z

=

n
(cid:88)

n
(cid:88)

(Hi)kjyizj

i=1

k

i=1
n
(cid:88)

j=1
n
(cid:88)

(Hj)kiyizj

j=1
i=1
(cid:32)(cid:32) n

(cid:88)

j=1

(cid:33)

(cid:33)

zjHj

y

,

k

=

=

where the second equality follows from (2.3).

32

Lemma 2.4.2. Let ¯x ∈ Rn be a local minimum of a cubic polynomial p : Rn → R, and let

d ∈ N (∇2p(¯x)). Then for any scalar α, ¯x + αd is a critical point of p.

Proof. Let p be given in our canonical form as 1
6

(cid:80)n

i=1 xT xiHix + 1

2xT Qx + bT x. We have

∇p(¯x + αd) =

=

+

+

(cid:32)

(cid:32)

1
2

1
2

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:33)

¯xiHi + αdiHi

(¯x + αd) + Q(¯x + αd) + b

(cid:33)

¯xiHi

¯x + Q¯x + b

n
(cid:88)

αdiHi ¯x +

1
2

n
(cid:88)

i=1

α¯xiHid + αQd

1
2

α2
2

i=1
n
(cid:88)

i=1

diHid

= ∇p(¯x) + α∇2p(¯x)d + α2∇p3(d)

= 0 + 0 + 0 = 0,

where the third equality follows form Lemma 2.4.1, and the last follows from the FONC and

TOC.

Theorem 2.4.3. The set of local minima of any cubic polynomial is convex.

Proof. If for some cubic polynomial p, the set LMp of its local minima is empty or a singleton,

the claim is trivially established. Otherwise, let ¯x, ¯y ∈ LMp with ¯x (cid:54)= ¯y. Consider any convex

combination z := ¯x + α(¯y − ¯x), where α ∈ (0, 1). We show that z satisﬁes the FONC, SONC,

and TOC, and therefore by Theorem 2.3.1, z ∈ LMp.

Note from (2.2) that the restriction of p to the line passing through ¯x and ¯y is

p(¯x + α(¯y − ¯x)) = p3(¯y − ¯x)α3 +

1
2

(¯y − ¯x)T ∇2p(¯x)(¯y − ¯x)α2 + ∇p(¯x)T (¯y − ¯x)α + p(¯x).

Since this univariate cubic polynomial has two local minima at α = 0 and α = 1, it must

be constant. In particular, the coeﬃcient of α2 must be zero, and because ∇2p(¯x) is psd,

that implies ¯y − ¯x ∈ N (∇2p(¯x)). Hence, by Lemma 2.4.2, the FONC holds at z. To

33

show the SONC and TOC at z, note that because ∇2p(x) is aﬃne in x, ∇2p(z) can be

written as a convex combination of ∇2p(¯x) and ∇2p(¯y), both of which are psd. The SONC

is then immediate. To see why the TOC holds, recall that the null space of the sum of

two psd matrices is the intersection of the null spaces of the summand matrices. Thus

N (∇2p(z)) ⊆ N (∇2p(¯x)), and the TOC is satisﬁed.

As a demonstration of Theorem 2.4.3, Figure 2.2 shows the critical points and the local

minima of the cubic polynomial

1 + 3x2
x3

1x2 + 3x1x2

2 + x3

2 − 3x1 − 3x2.

(2.9)

Note that the critical points form a nonconvex set, while the local minima constitute a convex

subset of the critical points.

Figure 2.2: The critical points of the polynomial (2.9). One can verify that the set of critical
points is {(x1, x2) | (x1+x2)2 = 1}, and that the set of local minima is {(x1, x2) | x1+x2 = 1}.
The points on the dashed line are local maxima.

34

Unlike the above example, LMp (or even LMp as LMp is in general not closed) may not

be a polyhedral2 set for cubic polynomials. For instance, the polynomial

p(x1, x2, x3, x4) = −x1x2

3 + x1x2

4 + 2x2x3x4 + x2

3 + x2
4,

(2.10)

has LMp = {x ∈ R4 | x2

1 + x2

2 < 1, x3 = x4 = 0} (see Figure 2.3). This is in contrast

to quadratic polynomials, whose local minima always form a polyhedral set. We show in

Theorem 2.4.5, however, that LMp is always a spectrahedron3. We ﬁrst need the following

lemma.

Figure 2.3: The projection of the set of local minima of the polynomial in (2.10) onto the

x1 and x2 variables. This example shows that LMp is not always a polyhedral set.

Lemma 2.4.4. For any cubic polynomial p : Rn → R, suppose ¯x ∈ Rn and ¯y ∈ Rn satisfy

• ¯x ∈ SOp,

• ∇2p(¯y) (cid:23) 0,

2Recall that a polyhedron is a set deﬁned by ﬁnitely many aﬃne inequalities.
3Recall that a spectrahedron is a set of the type S = {x ∈ Rn|A0 + (cid:80)n

i=1 xiAi (cid:23) 0}, where A0, . . . An are

symmetric matrices of some size m × m [110].

35

• p(¯x) = p(¯y).

Then p(¯x + α(¯y − ¯x)) = p(¯x) for any scalar α, and ¯y − ¯x ∈ N (∇2p(¯x)).

Note in particular that this lemma applies if ¯y is simply a second-order point, since p

must take the same value at any two second-order points. This is because any non-constant

univariate cubic polynomial can have at most one second-order point.

Proof. Consider the Taylor expansion of p around ¯x in the direction ¯y − ¯x (see (2.2)):

q(α) := p(¯x + α(¯y − ¯x)) = p3(¯y − ¯x)α3 +

1
2

(¯y − ¯x)T ∇2p(¯x)(¯y − ¯x)α2 + ∇p(¯x)T (¯y − ¯x)α + p(¯x).

Note that q is a univariate cubic polynomial which has a second-order point at α = 0. It is

straightforward to see that if a univariate cubic polynomial is not constant and has a second-

order point, then any other point which takes the same function value as the second-order

point must have a negative second derivative. As this is not the case for q (in view of α = 0

and α = 1), q must be constant, i.e., p(¯x + α(¯y − ¯x)) = p(¯x) for any α. Now observe that for

p(¯x + α(¯y − ¯x)) to be constant, we must have (¯y − ¯x)T ∇2p(¯x)(¯y − ¯x) = 0. As ∇2p(¯x) (cid:23) 0,

we have ¯y − ¯x ∈ N (∇2p(¯x)).

Theorem 2.4.5. For a cubic polynomial p : Rn → R, LMp is a spectrahedron.

Proof. If LMp is empty, the claim is trivial. Otherwise, let ¯x ∈ LMp. We show that LMp is

given by the spectrahedron

M := {x ∈ Rn | ∇2p(x) (cid:23) 0, ∇2p(¯x)(x − ¯x) = 0}.

(2.11)

First consider any ¯y ∈ LMp. From the SONC we know that ∇2p(¯y) (cid:23) 0 and from Lemma

2.4.4, we know that ¯y − ¯x ∈ N (∇2p(¯x)). Thus ¯y ∈ M . Since M is closed, we get that

LMp ⊆ M .

Now consider any ¯y ∈ M . By the deﬁnition of M , ¯y satisﬁes the SONC, and by Lemma

2.4.2, it also satisﬁes the FONC. Since for any scalar α ∈ (0, 1), ∇2p(¯x+α(¯y − ¯x)) is a convex
36

combination of the two psd matrices ∇2p(¯x) and ∇2p(¯y), N (∇2p(¯x + α(¯y − ¯x))) ⊆ ∇2p(¯x)

and thus ¯x + α(¯y − ¯x) satisﬁes the TOC (since ¯x does). Thus ¯y can be written as the limit

of local minima of p (e.g. {¯x + α(¯y − ¯x)} as α → 1).

Remark 2.4.1. We will soon show that for a cubic polynomial p, if LMp is nonempty, then

LMp = SOp (see Theorem 2.4.7). In Section 2.6, we will give other representations of SOp,

which in contrast to the representation in (2.11), do not rely on access to or even existence

of a local minimum.

2.4.2 Local Minima and Solutions to a “Convex” Problem

In Section 2.6, we present an SDP-based approach for ﬁnding local minima of cubic polyno-

mials. (We note again that the SDP representation in (2.11) is useless for this purpose as it

already assumes access to a local minimum.) Many common approaches for computing local

minima of twice-diﬀerentiable functions involve ﬁrst ﬁnding critical points of the function,

and then checking whether they satisfy second-order conditions. However, as discussed in the

introduction and in Section 2.2, such approaches are unlikely to be eﬀective for cubic poly-

nomials as critical points of these functions are in fact NP-hard to ﬁnd (see Theorem 2.2.1).

Interestingly, however, we show in Section 2.6 that by bypassing the search for critical points,

one can directly ﬁnd second-order points and local minima of cubic polynomials by solving

semideﬁnite programs of tractable size. The key to our approach is to relate the problem of

ﬁnding a local minimum of a cubic polynomial p to the following optimization problem:

inf
x∈Rn

p(x)

subject to ∇2p(x) (cid:23) 0.

(2.12)

The connection between solutions of (2.12) and local minima of p is established by Theorem

2.4.7 below. The feasible set of (2.12) has interesting geometric properties (see, e.g., Corollary

2.4.12) and will be referred to with the following terminology in the remainder of the chapter.

37

Deﬁnition 2.4.6. The convexity region of a polynomial p : Rn → R is the set

CRp := {x ∈ Rn | ∇2p(x) (cid:23) 0}.

Observe that for any cubic polynomial, its convexity region is a spectrahedron, and thus

a convex set. As p is a convex function when restricted to its convexity region, one can

consider (2.12) to be a convex problem in spirit.

Theorem 2.4.7. Let p be a cubic polynomial with a second-order point. Then the following

sets are equivalent:

(i) SOp

(ii) Minima of (2.12).

Furthermore, if p has a local minimum, then these two sets are equivalent to:

(iii) LMp.

Proof. (i) ⊆ (ii).

Let ¯y ∈ SOp and ¯x be any feasible point to (2.12).

If we consider the univariate cubic

polynomial q(α) := p(¯x + α(¯y − ¯x)), i.e., the restriction of p to the line passing through ¯x

and ¯y, we can see that α = 1 is a second-order point of q. Note that if any univariate cubic

polynomial has a second-order point, then that second-order point is a minimum of it over

its convexity region. In particular, because ¯x is feasible to (2.12) and thus α = 0 is in the

convexity region of q, we have p(¯y) = q(1) ≤ q(0) = p(¯x). As ¯y is feasible to (2.12) and has

objective value no higher than any other feasible point, it must be optimal to (2.12).

(ii) ⊆ (i)

Let ¯y be a minimum of (2.12) (we know that such a point exists because we have shown SOp

is a subset of the minima of (2.12), and SOp is nonempty by assumption). Let ¯x ∈ SOp and

d := ¯y − ¯x. Observe that p(¯y) = p(¯x), and so by Lemma 2.4.4, we must have d ∈ N (∇2p(¯x)).

38

It follows that ∇p(¯y) = ∇p3(d) (cf. the proof of Lemma 2.4.2). Now suppose for the sake of

contradiction that ¯y is not a second-order point. Since ¯y is feasible to (2.12), we must have

∇p(¯y) = ∇p3(d) (cid:54)= 0. As p(¯x) = p(¯x + αd) for any scalar α due to Lemma 2.4.4, we must

have p3(d) = 1

6dT ∇2p3(d)d = 0 (see (2.2)). Thus we can write

(cid:0)d − α∇p3(d)(cid:1)T ∇2p(¯y)(cid:0)d − α∇p3(d)(cid:1) =(cid:0)d − α∇p3(d)(cid:1)T (cid:0)∇2p(¯x) + ∇2p3(d)(cid:1)(cid:0)d − α∇p3(d)(cid:1)

=α2∇p3(d)T ∇2p(¯x)∇p3(d) − 2α∇p3(d)T ∇2p3(d)T d

+ α2∇p3(d)T ∇2p3(d)∇p3(d)

=α2 (cid:0)∇p3(d)T ∇2p(¯x)∇p3(d) + ∇p3(d)T ∇2p3(d)T ∇p3(d)(cid:1)

− 4α∇p3(d)T ∇p3(d),

where the last equality follows from that ∇p3(d) = 1

2∇2p3(d)T d due to Euler’s theorem for

homogeneous functions. Note that the right-hand side of the above expression is negative

for suﬃciently small α > 0, and so ∇2p(¯y) is not psd, which contradicts feasibility of ¯y to

(2.12).

For the second claim of the theorem, suppose that p has a local minimum. The following

arguments will show (i) = (ii) = (iii).

(iii) ⊆ (i)

Clearly any local minimum of p is a second-order point. Since the gradient and the Hessian

of p are continuous in x and as the cone of psd matrices is closed, the limit of any convergent

sequence of second-order points is a second-order point.

(ii) ⊆ (iii).

Let ¯y be any minimum of (2.12). Consider any local minimum ¯x of p and let zα := ¯x+α(¯y−¯x).

As both ∇2p(¯y) and ∇2p(¯x) are psd, any point zα with α ∈ [0, 1) satisﬁes the SONC and

TOC, by the same arguments as in the proof of Theorem 2.4.3.

39

Now note that since ¯x is a second-order point, it is also a minimum of (2.12) (as (i) ⊆ (ii))

and thus p(¯y) = p(¯x). From Lemma 2.4.4, we then have ¯y − ¯x ∈ N (∇2p(¯x)), and so from

Lemma 2.4.2, zα satisﬁes the FONC for any α. Thus, in view of Theorem 2.3.1, for any

α ∈ [0, 1), zα is a local minimum of p. Therefore ¯y can be written as the limit of a sequence

of local minima (i.e., {zα} as α → 1), and hence ¯y ∈ LMp.

Remark 2.4.2. Note that as a consequence of Theorems 2.4.5 and 2.4.7, if a cubic polynomial

p has a local minimum, then SOp is a spectrahedron. In fact, SOp is a spectrahedron for

any cubic polynomial p; see Theorem 2.6.3. In that theorem, we will give a more useful

spectrahedral representation of SOp which does not rely on knowledge of a local minimum.

Corollary 2.4.8. Let p be a cubic polynomial with a second-order point. Then the optimal

value of (2.12) is the value that p takes at any of its second-order points (and in particular,

at any of its local minima if they exist).

Proof. This is immediate from the equivalence of (i) and (ii) in Theorem 2.4.7.

2.4.3 Distinction Between Local Minima and Second-Order Points

We have shown that the optimization problem in (2.12) gives an approach for ﬁnding second-

order points of a cubic polynomial p without computing its critical points. However, not all

second-order points are local minima, and so in this subsection, we characterize the diﬀerence

between the two notions more precisely. We ﬁrst recall the concept of the relative interior

of a (convex) set (see, e.g., [97, Chap. 6]).

Deﬁnition 2.4.9. The relative interior of a nonempty convex set S ⊆ Rn is the set

ri(S) := {x ∈ S | ∀y ∈ S, ∃λ > 1 s.t. λx + (1 − λ)y ∈ S}.

This deﬁnition generalizes the notion of interior to sets which do not have full dimension.

One can show that for a convex set S, ri(S) is convex, ri( ¯S) = ri(S), and ri(S) = ¯S [97].
40

In general, for a nonempty convex set S, we have ri( ¯S) = ri(S) ⊆ S, but we may not have

ri( ¯S) = S. (For example, let S be a line segment with one of its endpoints removed.) It

turns out, however, that for a cubic polynomial p with a local minimum, ri(LMp) = LMp.

Theorem 2.4.10. Let p : Rn → R be a cubic polynomial with a local minimum. Then the

following three sets are equivalent:

(i) LMp

(ii) ri(SOp)

(iii) Intersection of critical points of p with ri(CRp).

Proof. (ii) ⊆ (i)

Recall from Theorem 2.4.3 that LMp is convex, and from Theorem 2.4.7 that SOp = LMp.

Then we have ri(SOp) = ri(LMp) = ri(LMp) ⊆ LMp.

(i) ⊆ (ii)

We prove the contrapositive. Let ¯x be a point which is not in ri(SOp). If ¯x is not a second-

order point, then it clearly cannot be a local minimum. Suppose now that ¯x ∈ SOp\ri(SOp).

Then there is another second-order point ¯y such that ¯y + λ(¯x − ¯y) is not a second-order point

for any λ > 1. Note from Lemma 2.4.4 and the statement after it that p(¯y + λ(¯x − ¯y)) is a

constant univariate function of λ. Now for any (cid:15) > 0, deﬁne the point ¯z(cid:15) := ¯x + (cid:15)

2(cid:107)¯x−¯y(cid:107)(¯x − ¯y).

Since ¯z(cid:15) is not a second-order point and thus not a local minimum, there is a point z(cid:15) satisfying

(cid:107)¯z(cid:15) − z(cid:15)(cid:107) < (cid:15)

2 and

p(z(cid:15)) < p(¯z(cid:15)) = p(

(cid:15)
2(cid:107)¯x − ¯y(cid:107)

(¯x − ¯y)) = p(¯x).

Furthermore, by the triangle inequality, z(cid:15) also satisﬁes (cid:107)z(cid:15) − ¯x(cid:107) < (cid:15). Thus, by considering

{z(cid:15)} as (cid:15) → 0, we can conclude that ¯x is not a local minimum.

(i) ⊆ (iii)

Consider any local minimum ¯x of p, which clearly must also be a critical point of p, and a

member of CRp. Suppose for the sake of contradiction that ¯x (cid:54)∈ ri(CRp). Then there exists

41

y ∈ CRp such that for any scalar α > 0, ∇2p(¯x + α(¯x − y)) is not psd. In particular, for any

α > 0 there exists a unit vector zα ∈ Rn such that zT

α ∇2p(¯x + α(¯x − y))zα < 0.

We now show that for any α, zα can be taken to be in C(∇2p(¯x)). This is because, as we

will show, if zα = d + v, where d ∈ N (∇2p(¯x)) and v ∈ C(∇2p(¯x)),

(d + v)T ∇2p(¯x + α(¯x − y))(d + v) = vT ∇2p(¯x + α(¯x − y))v.

(2.13)

Observe that if p is written in the form (2.4), for any d ∈ N (∇2p(¯x)), we have

dT ∇2p(¯x + α(¯x − y))d = dT

(¯xi + α(¯xi − yi))Hi + Q

d

(cid:32) n

(cid:88)

(cid:33)

= dT

i=1
(cid:32) n

(cid:88)

i=1

(cid:33)

¯xiHi + Q

d + α

n
(cid:88)

(dT Hid)(¯xi − yi) = 0,

i=1

where the last equality follows from that d ∈ N (∇2p(¯x)), and the TOC, recalling that the

i-th entry of ∇p3(d) is 1

2dT Hid. Note in particular that the expression above also holds for

α = −1, and so d ∈ N (∇2p(y)). Now observe that because we can write

∇2p(¯x + α(¯x − y)) = (1 + α)∇2p(¯x) − α∇2p(y),

we have ∇2p(¯x + α(¯x − y))d = 0. Thus, we have shown (2.13), and we can take

zα ∈ C(∇2p(¯x)).

Note that if zα ∈ C(∇2p(¯x)), then by Lemma 2.3.2 we have zT

α ∇2p(¯x)zα ≥ λ, where

λ is the smallest nonzero eigenvalue of ∇2p(¯x). Thus, for small enough α, the quantity

zT
α ∇2p(¯x + α(¯x − y))zα is positive and so we arrive at a contradiction.

(iii) ⊆ (i)

Let ¯x be a critical point which is in ri(CRp). Clearly ¯x ∈ SOp. Consider any local minimum

42

¯y of p, and observe that for any α (cid:54)= 0, we can write

¯x =

1
α

(α¯x + (1 − α)¯y) +

α − 1
α

¯y.

(2.14)

As ¯x ∈ ri(CRp) and ¯y ∈ CRp, α¯x + (1 − α)¯y ∈ CRp for some α > 1. In particular, for

that α, ∇2p(α¯x + (1 − α)¯y) (cid:23) 0 and thus in view of (2.14), we can see that N (∇2p(¯x)) ⊆

N (∇2p(¯y)). Hence, because the TOC holds at ¯y, it must also hold at ¯x. Thus ¯x is a local

minimum.

Figure 2.4 demonstrates the relation between LMp and SOp for the polynomial

p(x1, x2) = x2

1x2. For this example, SOp = {(x1, x2) | x1 = 0, x2 ≥ 0}, and LMp =

{(x1, x2) | x1 = 0, x2 > 0} (see Example 2.3.1).

Figure 2.4: The set of local minima (left) and second-order points (right) of the cubic

polynomial p(x1, x2) = x2

1x2. Note that SOp is the closure of LMp (Theorem 2.4.7) and LMp

is the relative interior of SOp (Theorem 2.4.10).

Theorem 2.4.10 gives rise to the following interesting geometric fact about local minima

of cubic polynomials.

Corollary 2.4.11. Let ¯x and ¯y be two local minima of a cubic polynomial. Then

N (∇2p(¯x)) = N (∇2p(¯y)).
43

Proof. It is known ([93, Corollary 1]) that for a spectrahedron {x ∈ Rn | A0 + (cid:80)n
and any two points x and y in its relative interior, N (A0 + (cid:80)n

i=1 xiAi) = N (A0 + (cid:80)n

i=1 xiAi (cid:23) 0}

i=1 yiAi).

In view of the facts that for any cubic polynomial p, CRp is a spectrahedron and

LMp ⊆ ri(CRp) (from Theorem 2.4.10), the result is immediate.

2.4.4 Spectrahedra and Convexity Regions of Cubic Polynomials

We end this section with a result relating general spectrahedra and convexity regions of cubic
polynomials. Recall from the end of Section 2.3.1 that if S := {x ∈ Rn | A0 +(cid:80)n

i=1 xiAi (cid:23) 0}

is a special spectrahedron, where A0, . . . , An are n × n symmetric matrices satisfying

(Ai)jk = (Aj)ik = (Ak)ij

for any i, j, k ∈ {1, . . . , n}, then S is the convexity region of the cubic polynomial

p(x) =

1
6

n
(cid:88)

i=1

xT xiAix +

1
2

xT A0x.

The following theorem shows that if the number of variables is allowed to increase, then

any spectrahedron can be represented by the convexity region of a cubic polynomial.

Theorem 2.4.12. Let a spectrahedron S ⊆ Rn be given by S := {x ∈ Rn | A0 + (cid:80)n

i=1 xiAi (cid:23) 0},

where A0, . . . , An ∈ Sm×m. There exists a cubic polynomial p in at most m + n variables

such that S is a projection of its convexity region; i.e.,

S = {x ∈ Rn | ∃y ∈ Rm such that (x, y) ∈ CRp}.

Furthermore, the interior of S is a projection of the set of local minima of p.

Proof. Let A(x) := A0 + (cid:80)n

i=1 xiAi. We ﬁrst present a characterization of the interior of S

following the developments in Section 2.4 of [93]. Let NA := N (A0) ∩ . . . ∩ N (An), and V

44

be a full-rank matrix whose columns span the orthogonal complement of NA. Suppose that

NA is (m − k)-dimensional. Then there exist matrices B0, . . . , Bn ∈ Sk×k with N (B0) ∩ . . . ∩

N (Bn) = {0k} such that

B(x) := B0 +

n
(cid:88)

i=1

xiBi = V T A(x)V.

In [93, Corollary 5], it is shown that B(x)

{x ∈ Rn | A(x) (cid:23) 0} = {x ∈ Rn | B(x) (cid:23) 0}

(2.15)

and that the set {x ∈ Rn | B(x) (cid:31) 0} gives the interior of S. Now consider the following

cubic polynomial in n + k variables:

p(x, y) := yT B(x)y.

(2.16)

Observe that the partial derivative of p with respect to y is 2B(x)y, the partial derivative of

p with respect to xi is yT Biy, and the Hessian of p is

∇2p(x, y) = 2






0

C(y)T

C(y) B(x)




 ,

where C(y) is an k × n matrix whose i-th column equals Biy. One can then immediately

see that if (¯x, ¯y) ∈ CRp, then we must have B(¯x) (cid:23) 0. Conversely, if B(¯x) (cid:23) 0, then

(¯x, 0k) ∈ CRp. Hence, in view of (2.15), we have shown that the spectrahedron S is the

projection of CRp onto the x variables.

We now show that LMp = {x ∈ Rn | B(x) (cid:31) 0} × {0k}. This would prove the second

claim of the theorem. First let ¯x be such that B(¯x) (cid:31) 0. Note that p(¯x, 0k) = 0 and that for

45

any two vectors χ ∈ Rn and ψ ∈ Rk,

p(¯x + χ, ψ) = ψT

B(¯x) +

(cid:32)

(cid:33)

Biχi

ψ.

n
(cid:88)

i=1

Since B(¯x) (cid:31) 0, then for any χ of suﬃciently small norm, B(¯x) + (cid:80)n

i=1 Biχi is still positive

deﬁnite, and hence for any ψ, p(¯x + χ, ψ) ≥ 0 = p(¯x, 0k). Thus (¯x, 0k) is a local minimum of

p.

Now let (¯x, ¯y) be a local minimum of p. From the SONC, we must have B(¯x) (cid:23) 0 and

C(¯y) = 0, which implies that Bi ¯y = 0k, ∀i ∈ {1, . . . , n}. Since

(¯x, ¯y) = 2B(¯x)¯y = 2

B0 +

(cid:32)

∂p
∂y

(cid:33)

¯xiBi

¯y = 2B0 ¯y + 2

n
(cid:88)

i=1

n
(cid:88)

i=1

¯xi(Bi ¯y),

it further follows from the FONC that B0 ¯y = 0. As N (B0)∩. . .∩N (Bn) = {0k} by construc-

tion, it follows that we must have ¯y = 0k. Next, observe that N (∇2p(¯x, 0k)) = Rn×N (B(¯x)).

Let d ∈ N (B(¯x)), and note that for any i ∈ {1, . . . , n}, (ei, d) ∈ N (∇2p(¯x, 0k)) and

∂p3
∂y (ei, d) = Bid. Then from the TOC, we must have Bid = 0k, ∀i ∈ {1, . . . , n}. Furthermore,

since d ∈ N (B(¯x)), it follows that B0d = 0k as well. Again, as N (B0) ∩ . . . ∩ N (Bn) = {0k}

by construction, it follows that we must have d = 0k and thus B(¯x) (cid:31) 0.

2.5 Complexity Justiﬁcations for an Exact SDP Oracle

In the next section, we show that second-order points and local minima of cubic polynomials

can be found by solving polynomially-many semideﬁnite programs with a polynomial number

of variables and constraints. One caveat however is that the inputs and outputs of these

semideﬁnite programs can sometimes be algebraic but not necessarily rational numbers. As

a result, we cannot claim that second-order points and local minima of cubic polynomials

can be found in polynomial time in the Turing model of computation. In this subsection, we

46

give evidence as to why establishing the complexity of these problems in the Turing model

is at the moment likely out of reach.

Deﬁnition 2.5.1. The SDP Feasibility Problem (SDPF) is the following decision question:

Given m×m symmetric matrices A0, . . . , An with rational entries, decide whether there exists
a vector x ∈ Rn such that A0 + (cid:80)n

i=1 xiAi (cid:23) 0.

Deﬁnition 2.5.2. The SDP Strict Feasibility Problem (SDPSF) is the following decision

question: Given m × m symmetric matrices A0, . . . , An with rational entries, decide whether
there exists a vector x ∈ Rn such that A0 + (cid:80)n

i=1 xiAi (cid:31) 0.

Even though semideﬁnite programs can be solved to arbitrary accuracy in polynomial

time [108], the complexities of the decision problems above remain as two of the outstand-

ing open problems in semideﬁnite programming. At the moment, it is not known if these

two decision problems even belong to the class NP [95, 90, 34]. We show next that the

complexities of these problems are a lower bound on the complexities of testing existence of

second-order points and local minima of cubic polynomials. (In Section 2.6, we accomplish

the more involved task of giving the reduction in the opposite direction.)

Theorem 2.5.3. If the problem of deciding whether a cubic polynomial has any second-order

points is in P (resp. NP), then SDPF is in P (resp. NP).

Proof. Given matrices A0, . . . , An ∈ Sm×m, let A(x) := A0 + (cid:80)n

i=1 xiAi. By noting that the

cubic polynomial p(x, y) = yT A(x)y has as its Hessian

∇2p(x, y) = 2






0

B(y)T

B(y) A(x)




 ,

where B(y) is an m × n matrix whose i-th column equals Aiy, we can see that if A(¯x) (cid:23) 0 for

some ¯x ∈ Rn, then ∇2p(¯x, 0k) (cid:23) 0. Since p is quadratic in the variables y, ∇p(¯x, 0k) = 0m+n,

and hence (¯x, 0k) is a second-order point of p. Conversely, if A(x) (cid:54)(cid:23) 0 for any x ∈ Rn, then

47

clearly ∇2p(x, y) (cid:54)(cid:23) 0 for any x ∈ Rn and y ∈ Rm, and thus p cannot have any second-order

points.

The above reduction shows that any polynomial-time algorithm (or polynomial-time

veriﬁable certiﬁcate) for existence of second-order points of cubic polynomials translates

into one for SDPF.

Theorem 2.5.4. If the problem of deciding whether a cubic polynomial has any local minima

is in P (resp. NP), then SDPSF is in P (resp. NP).

Proof. Given matrices A0, . . . , An ∈ Sm×m, let A(x) := A0 + (cid:80)n

i=1 xiAi and consider the
It is not diﬃcult to see that there exists ¯x ∈ Rn such

set S := {x ∈ Rn | A(x) (cid:23) 0}.

that A(¯x) (cid:31) 0 if and only if S has a nonempty interior and NA := N (A0) ∩ N (A1) ∩

N (A2) . . . ∩ N (An) = {0m}.4 The latter condition can be checked in polynomial time by

solving linear systems. The former can be reduced—due to the second claim of Theorem

2.4.12—to deciding if the cubic polynomial constructed in (2.16) has a local minimum. Note

that the polynomial in (2.16) has coeﬃcients polynomially sized in the entries of the matrices

Ai, since the matrix V in the proof of Theorem 2.4.12 can be taken to be the identity matrix

when NA = {0m}.

In addition to the diﬃculties alluded to in the above two theorems, the following three

examples point to concrete representation issues that one encounters in the Turing model

when dealing with local minima of cubic polynomials. The same complications are known

to arise for SDP feasibility problems [34].

Example 2.5.1. A cubic polynomial with only irrational local minima. Consider the

univariate cubic polynomial p(x) = x3 − 6x. One can easily verify that its unique local

minimum is at x =

√

2, which is irrational even though the coeﬃcients of p are rational.

4The “only if” direction is straightforward and the “if” direction follows from [93, Corollary 5].

48

Example 2.5.2. A cubic polynomial with an irrational convexity region. Consider

the quintary cubic polynomial p(x, y) = yT A(x)y, where

A(x) =












0

0












.

2 x 0

x 1

0

0 0 2x 2

0 0

2 x

One can easily verify that x =

√

2 is the only scalar satisfying A(x) (cid:23) 0. Since the matrix

2A(x) is a principal submatrix of ∇2p(x, y), any point in the convexity region of p must

√

satisfy x =

2 (even though the coeﬃcients of p are rational).

Example 2.5.3. A family of cubic polynomials whose local minima have exponential

bitsize. Consider the family of cubic polynomials pn(x, y) = yT An(x)y in 3n variables, where

An(x) =






















x1

2

0

0

2

1

0

0

0

0

0

0

· · ·

· · ·

x2 x1

· · ·

x1

1

· · ·

· · ·

· · ·

· · ·

0

0

0

0

0

0

0

0

· · ·
. . .

· · ·

0

0

0

0

0

0

0

0

· · ·

xn

· · ·

xn−1






















.

· · · xn−1

1

We show that even though these polynomials have some rational local minima, it takes

exponential time to write them down. From the proof of Theorem 2.4.12, one can infer that

the set of local minima of pn is the set {x ∈ Rn | An(x) (cid:31) 0} × {02n}. However, observe that

to have An(x) (cid:31) 0 (or even An(x) (cid:23) 0), we must have

x1 ≥ 4, x2 ≥ 16, . . . , xn ≥ 22n.

49

Hence, any local minimum of pn has bit length at least O(2n) even though the bit length of

the coeﬃcients of pn is O(n).

2.6 Finding Local Minima of Cubic Polynomials

In this section, we derive an SDP-based approach for ﬁnding second-order points and local

minima of cubic polynomials. This, along with the results established in Section 2.2, will

complete the entries of Table 2.2 from Section 2.1. We begin with some preliminaries that

are needed to present the theorems of this section.

2.6.1 Preliminaries from Semideﬁnite and Sum of Squares Opti-

mization

The Oracle E-SDP

Recall that a spectrahedron is a set of the type

(cid:40)

x ∈ Rn | A0 +

(cid:41)

xiAi (cid:23) 0

,

n
(cid:88)

i=1

where A0, . . . , An are symmetric matrices of some size m × m. A semideﬁnite representable

set (also known as a spectrahedral shadow ) is a set of the type

(cid:40)

x ∈ Rn | ∃y ∈ Rk such that A0 +

n
(cid:88)

i=1

xiAi +

k
(cid:88)

i=1

(cid:41)

yiBi (cid:23) 0

,

(2.17)

for some integer k ≥ 0 and symmetric m × m matrices A0, A1, . . . , An, B1, . . . , Bk. These are

exactly sets which semideﬁnite programming can optimize over.

We show in Theorem 2.6.3 and Corollary 2.6.5 that the set of second-order points of any

cubic polynomial is a spectrahedron and describe how a description of this spectrahedron

50

can be obtained from the coeﬃcients of p only.5 Since relative interiors of semideﬁnite rep-

resentable sets (and in particular spectrahedra) are semideﬁnite representable [81, Theorem

3.8], it follows from our Theorem 2.4.10 that the set of local minima of any cubic polynomial

is semideﬁnite representable.

Due to the complexity results and representation issues presented in Section 2.5, we

assume in this section that we can do arithmetic over real numbers and have access to an

oracle which solves SDPs exactly. This oracle—which we call E-SDP —takes as input an

SDP with real data and outputs the optimal value as a real number if it is ﬁnite, or reports

that the SDP is infeasible, or that it is unbounded.6 The following lemma shows that E-

SDP can ﬁnd a point in the relative interior of a semideﬁnite representable set. This will be

relevant for us later in this section when we search for local minima of cubic polynomials.

Lemma 2.6.1. Let S be a nonempty semideﬁnite representable set in Rn. Then a point in

ri(S) can be recovered in 2n calls to E-SDP.

Proof. Consider the following procedure. Let S1 = S, and for i ∈ {1, . . . , n} let

Si+1 = Si ∩ {x ∈ Rn | xi = x∗

i },

where the scalar x∗

i is chosen to be any “intermediate” value of xi on Si. More precisely, let

¯xi (resp. xi) be the supremum (resp. inﬁmum) of xi over Si (these two values may or may

not be ﬁnite). If ¯xi = xi, then set x∗

i = ¯xi. Otherwise, set x∗

i to be any scalar satisfying

xi < x∗

i < ¯xi. Note that for each i, x∗

i can be computed using 2 calls to E-SDP. Hence, after

2n calls to E-SDP, we arrive at a set Sn+1 which is a singleton by construction.

We next show, by induction, that the point in Sn+1 belongs to ri(S). First note that as

S is nonempty, ri(S) is nonempty [97, Theorem 6.2], which implies that S1 ∩ ri(S) = ri(S)

5Recall that the results of Section 2.4 by contrast established spectrahedrality of the set of second-
order points under the assumption of existence of a local minimum (see Remark 2.4.2). Furthermore, the
spectrahedral representation that we gave there (see Theorem 2.4.5) required knowledge of a local minimum.
6Though this will not be needed for our purposes, it is straightforward to show that for an SDP with n
scalar variables, the oracle E-SDP can be called twice to test attainment of the optimal value, and a total
of n + 1 times to recover an optimal solution.

51

is nonempty. Now suppose that Si ∩ ri(S) is nonempty for i ∈ {1, . . . , k}. We show that

Sk+1 ∩ ri(S) is nonempty.

First suppose that k is such that ¯xk = x∗

k = xk. In this case, because ∀x ∈ Sk, xk = x∗
k,

Sk+1 ∩ ri(S) = Sk ∩ {x ∈ Rn | xk = x∗

k} ∩ ri(S) = Sk ∩ ri(S) (cid:54)= ∅.

Now suppose that xk < x∗
k < ¯xk. By the deﬁnition of ¯xk, there exists a sequence of points
{yj} ⊆ Sk such that (yj)k → ¯xk. We recall that for any z ∈ ri(S), y ∈ ¯S, and λ ∈ (0, 1],

λz + (1 − λ)y ∈ ri(S) [97, Theorem 6.1]. Now let z ∈ Sk ∩ ri(S). Since Sk is convex, for any
y ∈ Sk ∩ ¯S and λ ∈ (0, 1], λz + (1 − λ)y ∈ Sk ∩ ri(S). In particular, since Sk ∩ ¯S = Sk, the

sequence {zj} := { 1

j z + j−1

j yj} satisﬁes {zj} ⊆ Sk ∩ ri(S) and (zj)k → ¯xk. Similarly, there

exists a sequence of points {wj} ⊆ Sk ∩ ri(S) such that (wj)k → xk. As Sk ∩ ri(S) is convex,

there must then be a point x ∈ Sk ∩ ri(S) satisfying xk = x∗

k, and so

Sk+1 ∩ ri(S) = Sk ∩ {x ∈ Rn | xk = x∗

k} ∩ ri(S)

is not empty.

Overview of Sum of Squares Polynomials

In order to describe our SDP-based approach for ﬁnding local minima of cubic polynomi-

als, we also need to brieﬂy review the connection between sum of squares polynomials and

matrices to semideﬁnite programming. Approaches to ﬁnding local minima based on sum

of squares have been studied before, such as in [83]. The SDP approach in this chapter,

however, is based partially on ﬁnding critical points and does not formally study the case of

cubic polynomials.

Recall that a (multivariate) polynomial p : Rn → R is nonnegative if p(x) ≥ 0, ∀x ∈ Rn,

and that a polynomial p is said to be a sum of squares (sos) if p = (cid:80)r

i=1 q2
i

for some

polynomials q1, . . . , qr. The notion of sum of squares also extends to polynomial matrices (i.e.,

52

matrices whose entries are multivariate polynomials). We say that symmetric polynomial

matrix M (x) : Rn → Rm×Rm is an sos-matrix if it has a factorization as M (x) = R(x)T R(x)

for some r × m polynomial matrix R(x) [47]. Observe that if M is an sos-matrix, then

M (x) (cid:23) 0 for any x ∈ Rn. One can check that M (x) is an sos-matrix if and only if the scalar-

valued polynomial yT M (x)y in variables (x1, . . . , xn, y1, . . . , ym) is sos. Indeed, the “only if”
direction is clear, the “if” direction is because when yT M (x)y = (cid:80)r
polynomials q1, . . . , qr, each qi must be linear in y and thus writable as qi(x) = (cid:80)m

i (x, y) for some

i=1 q2

j=1 yjqij(x)

for some polynomials qij. Then if R(x) is the r × m matrix where Rij(x) = qij(x), we will

have M (x) = RT (x)R(x).

2.6.2 A Sum of Squares Approach for Finding Second-Order

Points

We have shown in Theorem 2.4.7 that if a cubic polynomial p has a second-order point, the

solutions of the optimization problem in (2.12) exactly form the set SOp of its second-order

points. The same theorem further showed that if p has a local minimum, then the solutions

of (2.12) also coincide with LMp, i.e. the closure of the set of its local minima. Our goal in

this section is to develop a semideﬁnite representation of SOp which can be obtained directly

from the coeﬃcients of p (Corollary 2.6.5). To arrive to this representation, we ﬁrst present

an sos relaxation of problem (2.12), which we prove to be tight when SOp is nonempty

(Theorem 2.6.2). We then provide a more eﬃcient representation of the SDP underlying

this sos relaxation in Section 2.6.3. This will lead to an algorithm (Algorithm 2) for ﬁnding

local minima of cubic polynomials which is presented in Section 2.6.3.

53

Theorem 2.6.2. If a cubic polynomial p : Rn → R has a second-order point, the optimal

value of the following semideﬁnite program7 is attained and is equal to the value of p at all

second-order points:

sup
γ∈R,σ(x),S(x)

γ

subject to

p(x) − γ = σ(x) + Tr(S(x)∇2p(x)),

σ(x) is a degree-2 sos polynomial,

(2.18)

S(x) is an n × n sos-matrix with degree-2 entries.

Proof. Let ¯x be a second-order point of p and γ∗ be the optimal value of (2.18). Consider

any feasible solution (γ, σ, S) to (2.18) (nonemptiness of the feasible set is established in the

next paragraph). Since ∇2p(¯x) (cid:23) 0 and S(¯x) (cid:23) 0, we have Tr(∇2p(¯x)S(¯x)) ≥ 0. Since

σ(¯x) ≥ 0 as well, it follows that p(¯x) ≥ γ. Hence, p(¯x) ≥ γ∗.

To show that p(¯x) ≤ γ∗ and that the value γ∗ = p(¯x) is attained, we establish that

(cid:18)

(γ, σ, S) =

p(¯x),

1
3

(x − ¯x)T ∇2p(¯x)(x − ¯x),

(x − ¯x)(x − ¯x)T

(cid:19)

1
6

is feasible to (2.18). Note that 1

be factored into V T V ), and that 1

3(x − ¯x)T ∇2p(¯x)(x − ¯x) is an sos polynomial (as ∇2p(¯x) can
6(x − ¯x)(x − ¯x)T is an sos-matrix by construction. To show

that the ﬁrst constraint in (2.18) is satisﬁed, consider the Taylor expansion of p around ¯x in

the direction x − ¯x (see (2.2), noting that ∇p(¯x) = 0):

p(¯x + (x − ¯x)) = p(¯x) +

1
2

(x − ¯x)T ∇2p(¯x)(x − ¯x) + p3(x − ¯x).

(2.19)

7To clarify, x is not a decision variable in this problem. The decision variables are γ, the coeﬃcients of
σ, and the coeﬃcients of the entries of S. The identity in the ﬁrst constraint must hold for all x, and this
can be enforced by matching the coeﬃcient of each monomial on the left with the corresponding coeﬃcient
on the right.

54

Observe that if p is written in the form (2.4), then we have

p3(x − ¯x) =

=

=

=

1
6

1
6

1
6

1
6

(x − ¯x)T

(x − ¯x)T

(x − ¯x)T

(cid:32) n

(cid:88)

i=1
(cid:32) n

(cid:88)

i=1
(cid:32) n

(cid:88)

i=1

(cid:33)

(xi − ¯xi)Hi

(x − ¯x)

(xi − ¯xi)Hi + Q − Q

(x − ¯x)

(cid:33)

xiHi + Q −

n
(cid:88)

(cid:33)

¯xiHi − Q

(x − ¯x)

(x − ¯x)T ∇2p(x)(x − ¯x) −

i=1
1
6

(x − ¯x)T ∇2p(¯x)(x − ¯x).

Note further that due to the cyclic property of the trace, we have

1
6

(x − ¯x)T ∇2p(x)(x − ¯x) = Tr

(x − ¯x)(x − ¯x)T )∇2p(x)

(cid:19)

.

(cid:18)

(

1
6

Hence, (2.19) reduces to the following identity

p(x) − p(¯x) =

1
3

(x − ¯x)T ∇2p(¯x)(x − ¯x) + Tr

(x − ¯x)(x − ¯x)T )∇2p(x)

(cid:19)

,

(2.20)

(cid:18)

(

1
6

and thus the claim is established.

Since (2.18) is a tight sos relaxation of (2.12) when SOp is nonempty, it is interesting to

see how an optimal solution to (2.12) can be recovered from an optimal solution to (2.18).

This is shown in the next theorem, keeping in mind that optimal solutions to (2.12) are

second-order points of p (see Theorem 2.4.7).

Theorem 2.6.3. Let p : Rn → R be a cubic polynomial with a second-order point, and let

(γ∗, σ∗, S∗) be an optimal solution of (2.18) applied to p. Then, the set

Γ := {x ∈ Rn | ∇2p(x) (cid:23) 0, σ∗(x) = 0, Tr(S∗(x)∇2p(x)) = 0}

(2.21)

is a spectrahedron, and Γ = SOp.

55

Proof. We ﬁrst show that Γ = SOp. Let ¯x be a second-order point of p. From Theorem 2.6.2

and the ﬁrst constraint of (2.18) we have

0 = p(¯x) − p(¯x) = p(¯x) − γ∗ = σ∗(¯x) + Tr(S∗(¯x)∇2p(¯x)).

As σ∗(¯x) and Tr(S∗(¯x)∇2p(¯x)) are both nonnegative, the above equation implies they must

both be zero, and hence SOp ⊆ Γ. To see why Γ ⊆ SOp, let ¯y be a point in Γ and ˆx be

an arbitrary second-order point (which by the assumption of the theorem exists). Observe

from Theorem 2.6.2 and the ﬁrst constraint of (2.18) that

p(¯y) − p(ˆx) = p(¯y) − γ∗ = σ∗(¯y) + Tr(S∗(¯y)∇2p(¯y)) = 0.

Additionally, because ∇2p(¯y) (cid:23) 0, it follows from Corollary 2.4.8 that ¯y is optimal to (2.12),

and thus is a second-order point by Theorem 2.4.7.

Now we show that Γ is a spectrahedron by “linearizing” the quadratic and cubic equations

that appear in (2.21). Since σ∗ is a quadratic sos polynomial, it can be written equivalently
as σ∗(x) = (cid:80)m

i (x) for some aﬃne polynomials q1, . . . , qm. Similarly, since S∗ is an sos-

i=1 q2

matrix with quadratic entries, it can be written as S∗(x) = R(x)T R(x) for some k × n matrix

R with aﬃne entries. First note that as ∇2p(x) is aﬃne in x and σ∗ is a sum of squares of

aﬃne polynomials, the set

{x ∈ Rn | ∇2p(x) (cid:23) 0, σ∗(x) = 0} = {x ∈ Rn | ∇2p(x) (cid:23) 0, q1(x) = 0, . . . , qm(x) = 0}

is clearly a spectrahedron.

Now let y be any point in ri(CRp). Such a point exists because CRp is nonempty by

assumption, and relative interiors of nonempty convex sets are nonempty [97, Theorem 6.2].

Now let ri be the i-th column of the matrix RT . We claim that Γ is equivalent to the

56

following set:

(cid:8)x ∈ Rn | ∇2p(x) (cid:23) 0, q1(x) = 0, . . . , qm(x) = 0, ∇2p(y)r1(x) = 0, . . . , ∇2p(y)rk(x) = 0(cid:9).

(2.22)

Note that this set is a spectrahedron, and that the ﬁnal k equality constraints are enforcing

that each column of RT be in the null space of ∇2p(y).

To prove the claim, ﬁrst let x be in (2.22). Note that N (∇2p(y)) ⊆ N (∇2p(x)), as

y ∈ ri(CRp) and so ∇2p(y) = λ∇2p(x) + (1 − λ)∇2p(z) for some z ∈ CRp and λ ∈ (0, 1).

Then,

Hence (2.22) ⊆ (2.21).

Tr(S∗(x)∇2p(x)) =

k
(cid:88)

i=1

i (x)∇2p(x)ri(x) = 0.
rT

To show the reverse inclusion, let x be a point in (2.21). It is easy to check that Tr(AB) =

0 for two psd matrices A = C T C and B if and only if the columns of C T belong to the null

space of B. Hence, we must have ri(x) ∈ N (∇2p(x)). Assume ﬁrst that x ∈ ri(CRp). Then

we must have ri(x) ∈ N (∇2p(x)) = N (∇2p(y)) as CRp is a spectrahedron and any two

matrices in the relative interior of a spectrahedron have the same null space [93, Corollary 1].

To see why we must also have ri(x) ∈ N (∇2p(y)) for any x ∈ CRp\ri(CRp), observe that

N (∇2p(y)) is closed, the vector-valued functions ri are continuous in x, and the preimage of

a closed set under a continuous function is closed.

2.6.3 A Simpliﬁed Semideﬁnite Representation of Second-Order

Points and an Algorithm for Finding Local Minima

In this subsection, we derive a semideﬁnite representation of the set SOp, which will be given

in (2.31). In contrast to the semideﬁnite representation in (2.22), which requires ﬁrst solving

(2.18) and then performing some matrix factorizations, the representation in (2.31) can be

immediately obtained from the coeﬃcients of p. To ﬁnd a second-order point of an n-variate

57

cubic polynomial via the representation in (2.31), one needs to solve an SDP with (n+2)(n+1)

2

scalar variables and two semideﬁnite constraints of size (n + 1) × (n + 1). This is in contrast

to ﬁnding a second-order point via the representation in (2.22), which requires solving two

SDPs: (2.18) which has

(cid:16) n(n+1)

2 + 1

(cid:17) (cid:16) (n+2)(n+1)

(cid:17)

2

+ 1 scalar variables and two semideﬁnite

constraints of sizes (n + 1) × (n + 1) and n(n + 1) × n(n + 1) (coming from the two sos

constraints), and then the SDP associated with (2.22), which has n scalar variables and a

semideﬁnite constraint of size n × n. Another purpose of this subsection is to present our

ﬁnal result, which is an algorithm for testing for existence of a local minimum (Algorithm 2

in Section 2.6.3).

A Simpliﬁed Sos Relaxation

Recall from the proof of Theorem 2.6.2 that if p has a second-order point ¯x, then there is an

optimal solution to (2.18) of the form

(cid:18)

(γ, σ, S) =

p(¯x),

1
3

(x − ¯x)T ∇2p(¯x)(x − ¯x),

(x − ¯x)(x − ¯x)T

(cid:19)

.

1
6

(2.23)

In particular, for this solution, the coeﬃcients of σ and S can both be written entirely in

terms of the entries of ¯x and the coeﬃcients of p. In what follows, we attempt to optimize

over solutions to (2.18) which are of the form in (2.23). However, imposing this particu-

lar structure on the solution requires nonlinear equality constraints (in fact, it turns out

quadratic constraints suﬃce). Instead, we will impose an SDP relaxation of these nonlinear

constraints and show that the relaxation is exact. We follow a standard technique in deriving

SDP relaxations for quadratic programs, where the outer product xxT of some variable x is

replaced by a new matrix variable X satisfying X − xxT (cid:23) 0. The latter matrix inequality

that can be imposed as a semideﬁnite constraint via the Schur complement [22]. The variable

¯x will be represented by a variable y ∈ Rn, and the symmetric matrix variable Y ∈ Sn×n will

represent yyT . In addition, we will need another scalar variable z.

58

Assume p is given in the form (2.4), and let us expand σ in (2.23) (disregarding the factor

1
3) as follows:

(x − ¯x)T ∇2p(¯x)(x − ¯x) = xT

= xT

(cid:32) n
(cid:88)

i=1
(cid:32) n
(cid:88)

i=1

(cid:33)

¯xiHi + Q

x − 2¯xT

(cid:33)

¯xiHi + Q

x − 2

(cid:33)

¯xiHi + Q

x + ¯xT

(cid:32) n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

Tr(Hi ¯x¯xT )xi − 2¯xT Qx + ¯xT

(cid:33)

¯xiHi + Q

¯x

(cid:33)

¯xiHi + Q

¯x,

(cid:32) n
(cid:88)

i=1

where in the last equality we used Lemma 2.4.1. If we replace any occurrence of ¯x with y,
any occurrence of ¯x¯xT with Y and any occurrence of ¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x with z, we can

rewrite the above expression as

σY,y,z(x) :=

n
(cid:88)

n
(cid:88)

(cid:32) n

(cid:88)

j=1

k=1

i=1

(cid:33)

(Hi)jkyi + Qjk

xjxk − 2

n
(cid:88)

i=1

(Tr(HiY ) + eT

i Qy)xi + z.

(2.24)

Similarly, the matrix S in (2.23) can be written as xxT − xyT − yxT + Y (disregarding the

factor 1

6). Note that if Y − yyT (cid:23) 0, then the matrix xxT − xyT − yxT + Y is an sos-matrix

(as a polynomial matrix in x). By making these replacements, we arrive at an SDP which

attempts to look for a solution to the sos program in (2.18) which is of the structure in

(2.23). This is the following SDP8:

sup
γ∈R,Y ∈Sn×n,y∈Rn,z∈R

γ

subject to

p(x) − γ =

1
3

σY,y,z(x) +

1
6

Tr (cid:0)∇2p(x)(xxT − xyT − yxT + Y )(cid:1) ,

σY,y,z is sos,




(2.25)




Y

yT

y

1


 (cid:23) 0.

8Note that x is not a decision variable in this SDP as the ﬁrst constraint needs to hold for all x.

59

Through straightforward algebra and matching coeﬃcients, the ﬁrst constraint (keeping

in mind that p is as in (2.4)) can be more explicitly written as:

bi = −eT

i Qy −

1
2

Tr(HiY ), i = 1, . . . , n,

−γ =

1
6

Tr(QY ) +

z
3

.

These constraints reﬂect that the coeﬃcients of the linear terms and the scalar coeﬃcient

match on both sides; the cubic and quadratic coeﬃcients are automatically the same. We

can rewrite (2.24) as





T

σY,y,z(x) =




x

1




T (Y, y, z)








x

1


 ,

where



(cid:80)n

i=1 yiHi + Q

(cid:80)n

i=1 Tr(HiY )ei + Qy

T (Y, y, z) :=



((cid:80)n

i=1 Tr(HiY )ei + Qy)T

z




 .

The constraint in (2.25) that σ be sos is the same as the matrix T being psd. Putting

everything together, the problem in (2.25) can be rewritten as the following SDP:9

inf
Y ∈Sn×n,y∈Rn,z∈R

subject to

1
6
1
2

Tr(QY ) +

z
3

Tr(HiY ) + eT

i Qy + bi = 0, ∀i = 1, . . . , n,

T (Y, y, z) (cid:23) 0,




(2.26)




Y

yT

y

1


 (cid:23) 0.

It is interesting to observe that the ﬁrst constraint is a relaxation of the quadratic con-

straint which would impose ∇p(y) = 0, and that the constraint T (Y, y, z) (cid:23) 0 in particular

9Recall that the data to this SDP is obtained from the representation of p in the form of (2.4).

60

implies ∇2p(y) (cid:23) 0. One can think of (2.26) as another SDP relaxation of (2.12) which is

tight when p has a second-order point.

Combining the SDP in (2.26) with its Dual

In this subsection, we write down an SDP (given in (2.28)) whose optimal value can be

related to the existence of second-order points of a cubic polynomial. To arrive at this SDP,

we ﬁrst take the dual of (2.26). It will turn out that the constraints in the dual follow a very

similar structure to those in the primal, and that any feasible solution of the primal yields

a feasible solution of the dual. We then combine the primal-dual pair of SDPs to arrive at

a single SDP, which is the one in (2.28). To this end, let us write down the dual of (2.26):

sup
R,S,r,s,λ,σ,ρ,γ

subject to

γ

1
6

Tr(QY ) +

z
3

− γ =

n
(cid:88)

i=1

λi

(cid:18) 1
2


Tr(HiY ) + eT

i Qy + bi

(cid:19)






















R r

rT

ρ



 + Tr



T (Y, y, z)






 , ∀(Y, y, z)


S

s

sT σ

+ Tr











Y

yT

y

1




R r

rT

ρ


 (cid:23)0,





S

s

sT σ


 (cid:23)0,







where R, S ∈ Sn×n, r, s, λ ∈ Rn, and σ, ρ, γ ∈ R. The right-hand side of the ﬁrst constraint

simpliﬁes to

bT λ+ρ+Tr(QS)+Tr

(cid:32)(cid:32) n

(cid:88)
(

i=1

1
2

(cid:33)

(cid:33)

(cid:32)

λi + 2si)Hi + R

Y

+

Q(λ + 2s) +

(cid:33)T

Tr(HiS)ei + 2r

y+σz.

n
(cid:88)

i=1

61

After matching coeﬃcients, the dual problem can be rewritten as

sup
R,S,r,s,λ,ρ

subject to

− bT λ − ρ − Tr(QS)

n
(cid:88)

i=1

(

1
2

λi + 2si)Hi + R =

1
6

Q,

n
(cid:88)

i=1

Q(λ + 2s) +





Tr(HiS)ei + 2r = 0,

R r




rT


 (cid:23) 0,
ρ








S

sT


 (cid:23) 0,

s

1
3

Substituting R and r using the ﬁrst two constraints into the ﬁrst psd constraint and then

multiplying by 6, we arrive at the problem

sup
S,s,λ,ρ

subject to

− bT λ − ρ − Tr(QS)



(cid:80)n

i=1(−3λi − 12si)Hi + Q



(Q(−3λ − 6s) − 3 (cid:80)n

i=1 Tr(HiS)ei)T

Q(−3λ − 6s) − 3 (cid:80)n

i=1 Tr(HiS)ei

6ρ




 (cid:23) 0,





S s




sT

1
3


 (cid:23) 0.

62

Replacing S with 1

3S, s with − 1

3s, and ρ with 1

6ρ, we can reparameterize this problem

and arrive at our ﬁnal form for the dual of (2.26):

sup
S,s,λ,ρ

− bT λ −

1
6

ρ −

1
3

Tr(QS)

subject to



(cid:80)n

i=1(4si − 3λi)Hi + Q



(Q(2s − 3λ) − (cid:80)n

i=1 Tr(HiS)ei)T

Q(2s − 3λ) − (cid:80)n

i=1 Tr(HiS)ei

ρ




 (cid:23) 0,








S s

sT 1


 (cid:23) 0.

One can easily verify that if (Y, y, z) is feasible to (2.26), then (Y, y, y, z) is feasible to

(2.27). Replacing (S, s, λ, γ) with (Y, y, y, z) in (2.27) gives an SDP whose constraints are

(2.27)

the two psd constraints in (2.26) and whose objective function is −bT y − 1

6z − 1

3Tr(QY ).

We now create a new SDP, which has the same decision variables and constraints as (2.26),

but whose objective function is the diﬀerence between the objective function of (2.26) and

−bT y − 1

6z − 1

3Tr(QY ). The optimal value of this new SDP is an upper bound on the duality

gap of the primal-dual SDP pair (2.26) and (2.27). If our cubic polynomial p is written in

the form (2.4) and



(cid:80)n

i=1 yiHi + Q

(cid:80)n

i=1 Tr(HiY )ei + Qy

T (Y, y, z) =



((cid:80)n

i=1 Tr(HiY )ei + Qy)T

z






as before, the new SDP we just described can be written as

63

inf
Y ∈Sn×n,y∈Rn,z∈R

subject to

1
2
1
2

Tr(QY ) + bT y +

z
2

Tr(HiY ) + eT

i Qy + bi = 0, ∀i = 1, . . . , n,

T (Y, y, z) (cid:23) 0,




(2.28)




Y

yT

y

1


 (cid:23) 0.

The following theorem relates the optimal value of this SDP to the existence of second-order

points of p.

Theorem 2.6.4. For a cubic polynomial p given in the form (2.4), consider the SDP in

(2.28). For any feasible solution (Y, y, z) to (2.28), the objective value of (2.28) is nonnega-

tive. Furthermore, the optimal value of (2.28) is zero and is attained if and only if p has a

second-order point.

Proof. Suppose (Y, y, z) is a feasible solution to (2.28). Note that (Y, y, z) is feasible to (2.26)

and (Y, y, y, z) is feasible to (2.27), and so

1
2

Tr(QY ) + bT y +

z
2

=

1
6

Tr(QY ) +

(cid:18)

z
3

−

−bT y −

1
6

z −

1
3

(cid:19)

Tr(QY )

≥ 0

by weak duality applied to (2.26) and (2.27). Hence, the objective of (28) is nonnegative at

any feasible solution.

Now suppose that p has a second-order point ¯x. We claim that the triplet

(cid:32)

¯x¯xT , ¯x, ¯xT

(cid:32) n

(cid:88)

i=1

(cid:33)

(cid:33)

¯xiHi + Q

¯x

is feasible to (2.28) and achieves an objective value of zero.

Indeed, the ﬁrst constraint

of (2.28) is satisﬁed because its left-hand side reduces to ∇p(¯x), which is zero. The third

constraint is satisﬁed since the matrix (¯x, 1)(¯x, 1)T is clearly psd. The second constraint is

64

satisﬁed since T (¯x¯xT , ¯x, ¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x) can be written as



(cid:80)n



¯xT ((cid:80)n

((cid:80)n
i=1 ¯xiHi + Q
i=1 ¯xiHi + Q) ¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x

i=1 ¯xiHi + Q)

1
2

((cid:80)n

i=1 ¯xiHi + Q)

1
2






 =

((cid:80)n



¯xT ((cid:80)n








1
2



¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x

i=1 ¯xiHi + Q)

i=1 ¯xiHi + Q)


T

.




1
2

The objective value at (¯x¯xT , ¯x, ¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x) is

1
2

1
2

=

Tr(Q¯x¯xT ) + bT ¯x +

1
2

¯xT

(cid:32) n

(cid:88)

(cid:33)

¯xiHi + Q

¯x

¯xT Q¯x −

(cid:32)

1
2

n
(cid:88)

i=1

i=1

(cid:33)T

¯xiHi ¯x + Q¯x

¯x +

1
2

¯xT

(cid:32) n

(cid:88)

i=1

(cid:33)

¯xiHi + Q

¯x

=0.

Since we have already shown that the objective function of (2.28) is nonnegative over its

feasible set, it follows that when p has a second-order point, the optimal value of (2.28) is

zero and is attained.

To prove the converse, suppose the optimal value of (2.28) is zero and is attained. Let

(Y ∗, y∗, z∗) be an optimal solution to (2.28). We will show that y∗ is a second-order point

for p. Clearly ∇2p(y∗) is psd, since T (Y ∗, y∗, z∗) (cid:23) 0. To show that ∇p(y∗) = 0, let us start
by letting D := Y ∗ − y∗y∗T , and d := (cid:80)n

i=1 Tr(HiD)ei. Note that

1
2

Tr(Hiy∗y∗T ) +

1
2

Tr(HiD) + eT

i Qy∗ + bi = 0

(2.4)
⇒ −2(∇p(y∗))i = Tr(HiD),

or equivalently d = −2∇p(y∗). In the remainder of the proof, we show that d = 0.

Since (cid:80)n

i=1 y∗

i Hiy∗ is the vector whose i-th entry is y∗T Hiy∗, we have that

n
(cid:88)

i=1

Tr(HiY ∗)ei + Qy∗ =

(cid:33)

y∗
i Hi + Q

y∗ + d.

(cid:32) n

(cid:88)

i=1

(2.29)

65

Then from the generalized10 Schur complement condition applied to T (Y ∗, y∗, z∗), we have

(cid:32)(cid:32) n

(cid:88)

z∗ ≥

(cid:33)

(cid:33)T (cid:32) n

(cid:88)

y∗
i Hi + Q

y∗ + d

y∗
i Hi + Q

(cid:33)+ (cid:32)(cid:32) n

(cid:33)

(cid:33)

y∗
i Hi + Q

y∗ + d

(cid:88)

i=1

= y∗T

i=1
(cid:32) n

(cid:88)

i=1

(cid:33)

y∗
i Hi + Q

y∗ + 2dT

i=1
(cid:32) n

(cid:88)

i=1

y∗
i Hi + Q

(cid:33)+ (cid:32) n

(cid:33)

y∗
i Hi + Q

y∗ + dT

(cid:33)+

y∗
i Hi + Q

d.

(cid:32) n

(cid:88)

i=1

(cid:88)

i=1

It is not diﬃcult to verify that since T (Y ∗, y∗, z∗) (cid:23) 0, we have

n
(cid:88)

i=1

Tr(HiY ∗)ei + Qy∗ ∈ C(

n
(cid:88)

y∗
i Hi + Q),

i=1

and thus (2.29) implies d ∈ C((cid:80)n
that d = ((cid:80)n

i=1 y∗

i Hi + Q)v. We then have

i=1 y∗

i Hi + Q). Therefore, there exists a vector v ∈ Rn such

(cid:33)

y∗
i Hi + Q

y∗

y∗
i Hi + Q

(cid:33)+ (cid:32) n

(cid:88)

i=1

(cid:33)

y∗
i Hi + Q

y∗

y∗
i Hi + Q

y∗
i Hi + Q

(cid:33)+ (cid:32) n

(cid:88)

i=1

(cid:33) (cid:32) n

(cid:88)

i=1

(cid:33)

y∗
i Hi + Q

y∗

dT

=vT

=vT

(cid:32) n

(cid:88)

i=1
(cid:32) n

(cid:88)

i=1
(cid:32) n

(cid:88)

i=1

=dT y∗.

Now let

δ := z∗ − y∗T

(cid:33)

y∗
i Hi + Q

y∗ − 2dT y∗ − dT

(cid:32) n

(cid:88)

i=1

(cid:33)+

y∗
i Hi + Q

d

(cid:32) n

(cid:88)

i=1

and observe that δ ≥ 0. We can then write the objective value of (2.28) at (Y ∗, y∗, z∗) in

terms of D, d, and δ:

10Here, A+ refers to any pseudo-inverse of A, i.e. a matrix satisfying AA+A = A.

66

Tr(QY ∗) + bT y∗ +

1
2

z∗

(cid:0)y∗T Qy∗ + Tr(QD)(cid:1) +

1
2
1
2

n
(cid:88)

i=1
(cid:33)

(cid:32)

y∗T

(cid:32) n

(cid:88)

i=1
(cid:19)
1
2

y∗T Qy∗ +

− 1 +

(cid:18)

Tr(QD) +

−

=

=

+

1
2
(cid:18) 1
2

+

1
2

=

1
2

Tr

(cid:32)(cid:32) n

(cid:88)

i=1

≥ 0,

(cid:18)

−eT

i Qy∗ −

1
2

Tr(Hiy∗y∗T ) −

Tr(HiD)

(cid:19)

y∗
i

1
2
(cid:33)+

y∗
i Hi + Q

y∗ + 2dT y∗ + dT

y∗
i Hi + Q

d + δ

(cid:32) n

(cid:88)

i=1

(cid:18)

−

1
2

+

1
2

(cid:19) n

(cid:88)

i=1

y∗T y∗

i Hiy∗

(cid:33)

(2.30)

1
2

(cid:19) n

(cid:88)

+ 1

i=1
(cid:33)

(cid:33)

Tr(HiD)y∗

i +

1
2

dT

(cid:32) n

(cid:88)

i=1

y∗
i Hi + Q

D

+

y∗
i Hi + Q

d +

δ
2

(cid:33)+

y∗
i Hi + Q

d +

δ
2

1
2

dT

(cid:32) n

(cid:88)

i=1
(cid:33)+

where in the last inequality we used the facts that D (cid:23) 0 and that the pseudo-inverse of a

psd matrix is psd.

Since the left-hand side of the above equation is zero by assumption, and since all three

terms on the right-hand side are nonnegative, it follows that ((cid:80)n
null space of ((cid:80)n
((cid:80)n

i=1 y∗
i Hi + Q)+ is the same as the null space of ((cid:80)n

i Hi + Q)d = 0. However, because d ∈ C((cid:80)n

i=1 y∗

i=1 y∗

i=1 y∗

i Hi + Q)+d = 0. As the

i=1 y∗

i Hi + Q), we have

i Hi + Q), it must be that d = 0.

An Algorithm for Finding Local Minima

Theorem 2.6.4 leads to the following characterization of second-order points of a cubic poly-

nomial.

67

Corollary 2.6.5. Let p : Rn → R be a cubic polynomial written in the form (2.4). Then the

set of its second-order points is equal to

{y ∈ Rn | ∃Y ∈ Sn×n, z ∈ R such that
1
2

Tr(QY ) + bT y +

z
2

1
2

= 0,


y

1


 (cid:23) 0}.






Y

yT

T (Y, y, z) (cid:23) 0,

Tr(HiY ) + eT

i Qy + bi = 0, ∀i = 1, . . . , n,

(2.31)

Proof. Recall from the proof of Theorem 2.6.4 that if ¯x is a second-order point of p, then
the triplet (¯x¯xT , ¯x, ¯xT ((cid:80)n

i=1 ¯xiHi + Q)¯x) is feasible solution to (2.28) with objective value

zero. Hence any second-order point belongs to (2.31). Conversely, recall that if (Y, y, z) is

a feasible solution to (2.28) with objective value zero, then y is a second-order point of p.

Therefore any point in (2.31) is a second-order point of p.

In view of Theorem 2.4.7, we observe that if p has a local minimum, the set in (2.31) is

a semideﬁnite representation of LMp. This observation gives rise to the following algorithm

which tests if a cubic polynomial has a local minimum.

68

Algorithm 2 Algorithm for ﬁnding a local minimum of a cubic polynomial using a polyno-

mial number of calls to E-SDP.
1: Input: A cubic polynomial p : Rn → R in the form (2.4)

2: TEST1 test using E-SDP if (2.31) is empty

3:

4:

5:

6:

if YES

return NO LOCAL MINIMUM

if NO

Find (via Lemma 2.6.1) a point x∗ in the relative interior of (2.31)

7: TEST2 test (via Theorem 2.3.3) if x∗ is a local minimum

8:

9:

10:

11:

if YES

return x∗

if NO

return NO LOCAL MINIMUM

Complexity and correctness of Algorithm 2. By design, if p has no local minimum,

Algorithm 2 will return NO LOCAL MINIMUM since TEST2 answers NO for every point. If p has

a local minimum, then SOp is nonempty. Since SOp is given by (2.31) due to Corollary 2.6.5,

TEST1 answers YES. Then, by Theorem 2.4.10, any point in the relative interior of (2.31) is

a local minimum. Hence x∗ will pass TEST2. Note that this algorithm makes 2n + 1 calls to

E-SDP, and then runs Algorithm 1.11

Remark 2.6.1. Finding strict local minima. If we are speciﬁcally interested in searching

for a strict local minimum of a cubic polynomial, we can simply check if the point x∗ returned

by Algorithm 2 satisﬁes ∇2p(x∗) (cid:31) 0. If the answer is yes, we return x∗; if the answer is

no, we declare that p has no strict local minimum. Clearly, if a local minimum x∗ satisﬁes

∇2p(x∗) (cid:31) 0, it must be a strict local minimum due to the SOSC. Furthermore, recall from

11In fact, the number of calls to E-SDP can be reduced to 2n if the very ﬁrst call to E-SDP uses x1 as the

objective function.

69

Section 2.3.1 that if p has a strict local minimum, then it has a unique local minimum, and

thus that must be the output of Algorithm 2.

2.7 Conclusions and Future Directions

In this chapter, we considered the notions of (i) critical points, (ii) second-order points,

(iii) local minima, and (iv) strict local minima for multivariate polynomials. For each type

of point, and as a function of the degree of the polynomial, we studied the complexity of

deciding (1) if a given point is of that type, and (2) if a polynomial has a point of that

type. See Tables 2.1 and 2.2 in Section 2.1 for a summary of how our results complement

prior literature. The majority of our work was dedicated to the case of cubic polynomials,

where some new tractable cases were revealed based in part on connections with semideﬁnite

programming. In this ﬁnal section, we outline two future research directions which also have

to do with cubic polynomials.

2.7.1 Approximate Local Minima

In Sections 2.5 and 2.6, we established polynomial-time equivalence of ﬁnding local min-

ima and second-order points of cubic polynomials and some SDP feasibility problems (see

Corollary 2.6.5, Algorithm 2, Theorem 2.5.3, Theorem 2.5.4). Unless some well-known open

problems around the complexity of SDP feasibility are resolved (see Section 2.5), one cannot

expect to make claims about ﬁnding local minima of cubic polynomials in polynomial time

in the Turing model of computation. Nonetheless, it is known that under some assump-

tions, one can solve semideﬁnite programs to arbitrary accuracy in polynomial time (see,

e.g.

[94, 8, 108, 90, 80, 46]). It is therefore reasonable to ask if one can ﬁnd local minima

of cubic polynomials to arbitrary accuracy in polynomial time. This is a question we would

like to study more rigorously in future work. We present a partial result in this direction in

Theorem 2.7.1 below.

70

Recall from Section 2.6.2 that our ability to ﬁnd local minima of a cubic polynomial p

depended on our ability to minimize p over its convexity region CRp. We show next that we

can ﬁnd an (cid:15)-minimizer of p over CRp by approximately solving a semideifnite program.

Theorem 2.7.1. For a cubic polynomial p given in the form (2.4), consider the SDP in

(2.28). If the objective value at a feasible point (Y, y, z) is (cid:15) ≥ 0, then p(y) ≤ p(x) + 2

3(cid:15),

∀x ∈ CRp.

Proof. Consider a feasible solution (Y, y, z) to (2.28). Let γ∗ be the inﬁmum of p over CRp.

Observe that

−

1
6

Tr(QY ) −

z
3

≤ γ∗.

This is because the SDPs in (2.28) and (2.26) have the same constraints, and the opti-

mal value of (2.26) is the negative of the optimal value of (2.25), which by construction

is a lower bound on γ∗. Similarly as in the proof of Theorem 2.6.4, let D := Y − yyT ,
d := (cid:80)n

i=1 Tr(HiD)ei, and

δ := z − yT

(cid:32) n

(cid:88)

i=1

(cid:33)

yiHi + Q

y − 2dT y − dT

(cid:33)+

yiHi + Q

d.

(cid:32) n

(cid:88)

i=1

71

We can then write:

1
6

Tr(QY ) +

z
3

=

=

+

−

1
6

1
6

1
3

1
2

Tr(QY ) +

z
3

−

n
(cid:88)

i=1

(cid:18) 1
2

Tr(HiY ) + eT

i Qy + bi

(cid:19)

yi

(cid:0)Tr(QyyT ) + Tr(QD)(cid:1)
(cid:33)
(cid:32)

(cid:32) n

(cid:88)

yT

yiHi + Q

y + 2dT y + dT

i=1
(cid:32) n

(cid:88)

(cid:32)

Tr

i=1

(cid:33)

yiHiyyT

+ Tr

(cid:32) n

(cid:88)

i=1

n
(cid:88)

yT yiHiy −

1
2

yT Qy − bT y

(cid:33)+

(cid:33)

yiHi + Q

d + δ

(cid:32) n

(cid:88)

i=1
(cid:33)(cid:33)

yiHiD

− yT Qy − bT y

= −

1
6

+

1
6

Tr

i=1
(cid:32)(cid:32) n

(cid:88)

i=1

(cid:32)(cid:32) n

(cid:88)

i=1

= −p(y) +

≤ −p(y) +

1
6

2
3

Tr

(cid:15),

(cid:33)

(cid:33)

yiHi + Q

D

+

(cid:33)+

(cid:33)

yiHi + Q

d

+

(cid:32) n

(cid:88)

i=1

(cid:32)

1
3
(cid:33)

dT

(cid:33)

δ
3
(cid:33)+

(cid:33)

yiHi + Q

D

+

yiHi + Q

d

+

1
3

(cid:32)

dT

(cid:32) n

(cid:88)

i=1

δ
3

where the ﬁrst equality is due to the ﬁrst constraint in (2.28), and the last inequality follows

from the last equation of (2.30) with (Y ∗, y∗, z∗) replaced by (Y, y, z) and the fact that

(cid:80)n

i=1 yiHi + Q and D are both psd matrices. We therefore conclude that

p(y) −

2
3

(cid:15) ≤ −

1
6

Tr(QY ) −

z
3

≤ γ∗.

We then have that p(y) ≤ p(x) + 2

3(cid:15), ∀x ∈ CRp as desired.

2.7.2 Unregularized Third-Order Newton Methods

We end our chapter with an interesting application of the problem of ﬁnding a local minimum

of a cubic polynomial. Recall that Newton’s method for minimizing a twice-diﬀerentiable

72

function proceeds by approximating the function with its second-order Taylor expansion at

the current iterate, and then moving to a critical point12 of this quadratic approximation. It is

natural to ask whether one can lower the iteration complexity of Newton’s method for three-

times-diﬀerentiable functions by using third-order information. An immediate diﬃculty,

however, is that the third-order Taylor expansion of a function around any point will not

be bounded below (unless the coeﬃcients of all its cubic terms are zero). In previous work

(see, e.g. [78]), authors have gotten around this issue by adding a regularization term to the

third-order Taylor expansion. In future work, we aim to study an unregularized third-order

Newton method which in each iteration moves to a local minimum of the third-order Taylor

approximation by applying Algorithm 2. We would like to explore the convergence properties

of this algorithm and conditions under which the algorithm is well deﬁned at every iteration.

As a ﬁrst step, let us consider the univariate case. For a function f : R → R, the

iterations of (classical) Newton’s method read

xk+1 = xk −

f (cid:48)(xk)
f (cid:48)(cid:48)(xk)

.

(2.32)

The update rule of a third-order Newton method, which in each iteration moves to the local

minimum of the third-order Taylor approximation, is given by

xk+1 = xk −

f (cid:48)(cid:48)(xk) − (cid:112)f (cid:48)(cid:48)(xk)2 − 2f (cid:48)(xk)f (cid:48)(cid:48)(cid:48)(xk)
f (cid:48)(cid:48)(cid:48)(xk)

.

(2.33)

We have already observed that in some settings, these iterations can outperform the classical

Newton iterations. For example, consider the univariate function

f (x) = 20x arctan(x) − 10 log(1 + x2) + x2,

(2.34)

12If the function to be minimized is convex, this critical point will be a global minimum of the quadratic

approximation.

73

which is strongly convex and has a (unique) global minimum at x = 0, where f (x) = 0; see

Figure 2.5. The ﬁrst three derivatives of this function are

f (cid:48)(x) = 20 arctan(x) + 2x,

f (cid:48)(cid:48)(x) = 2 +

f (cid:48)(cid:48)(cid:48)(x) =

20
1 + x2 ,
−40x
(1 + x2)2 .

One can show that the basin of attraction of the global minimum of f under the classical New-

ton iterations in (2.32) is approximately [−1.7121, 1.7121]. Starting Newton’s method with

|x0| ≥ 1.7122 results in the iterates eventually oscillating between ±13.4942. In contrast,

the iterates of our proposed third-order Newton method in (2.33) are globally convergent

to the global minimum of f . The iterations of both methods starting at x0 = 1.5 are com-

pared in Table 2.3 and Figure 2.5, showing faster convergence to the global minimum for the

third-order approach.

k

0

1

2

3

4

xk

1.5

f (xk)

19.9473

-.2327

.5910

-.0030

1.0014e-4

-8.3227e-9

1.4546e-15

2.3490e-9

1.1587e-16

k

0

1

2

3

4

xk

1.5

f (xk)

19.9473

-1.2786

15.1411

.8795

7.7329

-.3396

1.2477

.0230

.0058

Table 2.3:

Iterations of the third-order Newton method (left) and the classical Newton

method (right) on the function f in (2.34) starting at x0 = 1.5.

74

Figure 2.5: The plots of the function f in (2.34) and its second and third-order Taylor

expansions around x0 = 1.5. One can see that one iteration of the third-order Newton

method in (2.33) brings us closer the global minimum of f compared to one iteration of the

Newton method in (2.32).

In addition to potential beneﬁts regarding convergence, we have also observed that the

behavior of the algorithm can be less sensitive to the initial condition when compared to

Newton’s method. As an example, we used Newton’s method to ﬁnd the critical points

{1, −1, i, −i} of f (x) = x5 − 5x on the complex plane, using the iterates (2.32), (2.33), and

iterates given by

xk+1 = xk −

f (cid:48)(cid:48)(xk) + (cid:112)f (cid:48)(cid:48)(xk)2 − 2f (cid:48)(xk)f (cid:48)(cid:48)(cid:48)(xk)
f (cid:48)(cid:48)(cid:48)(xk)

,

(2.35)

which can be interpreted as the iterates for moving to the local maximum of a third-order

approximation of f . For each of the three iterations, the plots below demonstrate which

initial conditions converge to the same critical point. As can be seen, sensitivity of Newton’s

75

method to the initial condition demonstrates fractal behavior, while the third-order iterates

do not.

Figure 2.6: Sensitivity of the limits of the iterates (2.32), (2.33), and (2.35) respectively to

initial conditions. Regions with the same color denote initial conditions which converge to

the same critical point.

76

Chapter 3

On the Complexity of Finding a Local

Minimizer of a Quadratic Function

over a Polytope

3.1

Introduction

In this chapter of the thesis, we consider quadratic programs, which are polynomial optimiza-

tion problems of the form (1.1) where the objective function p is quadratic and all constraint

functions qi are aﬃne. Recall that a local minimum of a function f : Rn → R over a set

Ω ⊆ Rn is a point ¯x ∈ Ω for which there exists a scalar (cid:15) > 0 such that p(¯x) ≤ p(x) for

all x ∈ Ω with (cid:107)x − ¯x(cid:107) ≤ (cid:15). In the case where p and all the constraint functions qi are

aﬃne (i.e., linear programming), it is well known that a local minimum (which also has to

be a global minimum) can be found in polynomial time in the Turing model of computa-

tion [56, 57]. Perhaps the next simplest constrained optimization problems to consider are

77

quadratic programs, which can be written as

min
x∈Rn

xT Qx + cT x

subject to aT

i x ≤ bi, ∀i ∈ {1, . . . , m},

(3.1)

where Q ∈ Rn×n, c, a1, . . . , am ∈ Rn, and b1, . . . , bm ∈ R. The matrix Q is taken without

loss of generality to be symmetric. When complexity questions about quadratic programs

are studied in the Turing model of computation, all these data are rational and the input

size is the total number of bits required to write them down. It is well known that ﬁnding

a global minimum of a quadratic program is NP-hard, even when the matrix Q has a single

negative eigenvalue [86]. It is therefore natural to ask whether one can instead ﬁnd a local

minimum of a quadratic program eﬃciently. In fact, this precise question appeared in 1992

on a list of seven open problems in complexity theory for numerical optimization [87]:

“What is the complexity of ﬁnding even a local minimizer for nonconvex quadratic

programming, assuming the feasible set is compact? Murty and Kabadi (1987,

[75]) and Pardalos and Schnitger (1988, [85]) have shown that it is NP-hard to

test whether a given point for such a problem is a local minimizer, but that does

not rule out the possibility that another point can be found that is easily veriﬁed

as a local minimizer.”

A few remarks on the phrasing of this problem are in order. First, note that in this ques-

tion, the feasible set of the quadratic program is assumed to be compact (i.e., a polytope).

Therefore, there is no need to focus on the related and often prerequisite problem of deciding

the existence of a local minimum (since any global minimum e.g. is a local minimum). The

latter question makes sense in the case where the feasible set of the quadratic program is

unbounded; the complexity of this question is also addressed in this chapter (Theorem 3.2.5).

Second, as the quote points out, the question of ﬁnding a local minimum is also separate from

a complexity viewpoint from that of testing if a given point is a local minimum. This related

78

question has been studied more extensively and its complexity has already been settled for

optimization problems whose objective and constraints are given by polynomial functions of

any degree; see [75, 85, 7].

To point out some of the subtle diﬀerences between these variations of the problem more

speciﬁcally, we brieﬂy review the reduction of Murty and Kabadi [75], which shows the NP-

hardness of deciding if a given point is a local minimum of a quadratic program. In [75],

the authors show that the problem of deciding if a symmetric matrix Q is copositive—i.e.

whether xT Qx ≥ 0 for all vectors x in the nonnegative orthant—is NP-hard. From this, it

is straightforward to observe that the problem of testing whether a given point is a local

minimum of a quadratic function over a polyhedron is NP-hard: Indeed, the origin is a local

minimum of xT Qx over the nonnegative orthant if and only if the matrix Q is copositive.

However, it is not true that xT Qx has a local minimum over the nonnegative orthant if and

only if Q is copositive. Although the “if” direction holds, the “only if” direction does not.

For example, consider the matrix

Q =


0



1




 ,

1 −2

which is clearly not copositive, even though the point (1, 0)T is a local minimum of xT Qx

over the nonnegative orthant.

Our main results in this chapter are as follows. We show that unless P=NP, no

polynomial-time algorithm can ﬁnd a point within Euclidean distance cn (for any constant

c ≥ 0) of a local minimum of an n-variate quadratic program with a bounded feasible

set (Theorem 3.2.6). See also Corollaries 3.2.7 and 3.2.8. To prove this, we show as an

intermediate step that deciding whether a quartic polynomial or a quadratic program has

a local minimum is strongly NP-hard1 (Theorems 3.2.1 and 3.2.5). Finally, we show that

1This implies that these problems remain NP-hard even if the bitsize of all numerical data are O(log n),
where n is the number of variables. For a strongly NP-hard problem, even a pseudo-polynomial time
algorithm—i.e., an algorithm whose running time is polynomial in the magnitude of the numerical data of
the problem but not necessarily in their bitsize—cannot exist unless P=NP. See [39] or [5, Section 2] for
more details.

79

unless P=NP, there cannot be a polynomial-time algorithm that decides if a quadratic

program with a bounded feasible set has a unique local minimum and if so returns this

minimum (Theorem 3.2.9).

Overall, our results suggest that without additional problem structure, questions related

to ﬁnding local minima of quadratic programs are not easier (at least from a complexity

viewpoint) than those related to ﬁnding global minima. It also suggests that any eﬃcient

heuristic that aims to ﬁnd a local minimum of a quadratic program must necessarily fail on a

“signiﬁcant portion” of instances; see e.g. Corollary 2.2 of [48] for a more formal complexity

theoretic statement.

3.1.1 Notation and Basic Deﬁnitions

For a vector x ∈ Rn, the notation x2 denotes the vector in Rn whose i-th entry is x2

i , and

diag(x) denotes the diagonal n × n matrix whose i-th diagonal entry is xi. The notation

x ≥ 0 denotes that the vector x belongs to the nonnegative orthant, and for such a vector,
√

√

x denotes the vector in Rn whose i-th entry is

xi. For two matrices X, Y ∈ Rm×n, we

denote by X · Y the matrix in Rm×n whose (i, j)-th entry is XijYij. For vectors x, y ∈ Rn,

the notation yx (sometimes (y)x if there is room for confusion with other indices) denotes

the vector containing the entries of y where xi is nonzero in the same order (the length of yx

is hence equal to the number of nonzero entries in x). Similarly, for a vector x ∈ Rn and a

matrix Y ∈ Rn×n, the notation Yx (sometimes (Y )x if there is room for confusion with other

indices) denotes the principal submatrix of Y consisting of rows and columns of Y whose

indices correspond to indices of nonzero entries of x. The notation I (resp. J) refers to

the identity matrix (resp. the matrix of all ones); the dimension will be clear from context.

For a symmetric matrix M ∈ Rn×n, the notation M (cid:23) 0 (resp. M (cid:31) 0) denotes that M is

positive semideﬁnite (resp. positive deﬁnite), i.e. that it has nonnegative (resp. positive)

eigenvalues. As mentioned already, we say that M is copositve if xT M x ≥ 0, ∀x ≥ 0. The

80

simplex in Rn is denoted by ∆n := {x ∈ Rn | x ≥ 0, (cid:80)n

i=1 xi = 1}. Finally, for a scalar c,

the notation (cid:100)c(cid:101) denotes the ceiling of c, i.e. the smallest integer greater than or equal to c.

We recall that a form is a homogeneous polynomial; i.e. a polynomial whose monomials

all have the same degree. A form p : Rn → R is said to be nonnegative if p(x) ≥ 0, ∀x ∈ Rn,

and positive deﬁnite if p(x) > 0, ∀x (cid:54)= 0. A critical point of a diﬀerentiable function f :

Rn → R is a point x ∈ Rn at which the gradient ∇f (x) is zero. A second-order point of a

twice-diﬀerentiable function f : Rn → R is a critical point x at which the Hessian matrix

∇2f (x) is positive semideﬁnite.

All graphs in this chapter are undirected, unweighted, and have no self-loops. The

adjacency matrix of a graph G on n vertices is the n × n symmetric matrix whose (i, j)-th

entry equals one if vertices i and j share an edge in G and zero otherwise. The complement

of a graph G, denoted by ¯G, is the graph with the same vertex set as G and such that two

distinct vertices are adjacent if and only if they are not adjacent in G. An induced subgraph

of G is a graph containing a subset of the vertices of G and all edges connecting pairs of

vertices in that subset.

3.2 The Main Result

3.2.1 Complexity of Deciding Existence of Local minima

To show that a polynomial-time algorithm for ﬁnding a local minimum of a quadratic function

over a polytope (i.e., a bounded polyhedron) implies P = NP, we show as an intermediate

step that it is NP-hard to decide whether a quadratic program with an unbounded feasible

set has a local minimum (Theorem 3.2.5). To achieve this intermediate step, we ﬁrst establish

the following hardness result.

Theorem 3.2.1. It is strongly NP-hard to decide if a degree-4 polynomial has a local mini-

mum.

81

We will prove this theorem by presenting a polynomial-time reduction from the STABLE-

SET problem, which is known to be (strongly) NP-hard [39]. Recall that in the STABLESET

problem, we are given as input a graph G on n vertices and a positive integer r ≤ n. We are

then asked to decide whether G has a stable set of size r, i.e. a set of r pairwise non-adjacent

vertices. We denote the size of the largest stable set in a graph G by the standard notation

α(G). We also recall that a clique in a graph G is a set of pairwise adjacent vertices. The

size of the largest clique in G is denoted by ω(G). The following theorem of Motzkin and

Straus [74] relates ω(G) to the optimal value of a quadratic program.

Theorem 3.2.2 ([74]). Let G be a graph on n vertices with adjacency matrix A and clique

number ω. The optimal value of the quadratic program

max
x∈Rn

xT Ax

subject to x ≥ 0,

n
(cid:88)

i=1

xi = 1

(3.2)

is 1 − 1
ω .

For a scalar k and a symmetric matrix A (which will always be an adjacency matrix),

the following notation will be used repeatedly in our proofs:

and

MA,k := kA + kI − J,

qA,k(x) := xT MA,kx,

pA,k(x) := (x2)T MA,kx2.

(3.3)

(3.4)

(3.5)

Note that nonnegativity of the quadratic form qA,k over the nonnegative orthant is equivalent

to (global) nonnegativity of the quartic form pA,k and to copositivity of the matrix MA,k.

82

The following corollary of Theorem 3.2.2 will be of more direct relevance to our proofs.

The ﬁrst statement in the corollary has been observed e.g. by de Klerk and Pashechnik [35],

but its proof is included here for completeness. The second statement, which will also be

needed in the proof of Theorem 3.2.1, follows straightforwardly.

Corollary 3.2.3. For a scalar k > 0 and a graph G with adjacency matrix A, the matrix

MA,k in (3.3) is copositive if and only if α(G) ≤ k. Furthermore, if α(G) < k, the quartic

form pA,k in (3.5) is positive deﬁnite.2

Proof. First observe that α(G) = ω( ¯G), and that the adjacency matrix of ¯G is J − A − I.

Thus from Theorem 3.2.2, the maximum value of xT (J − A − I)x over ∆n is 1 − 1

α(G) , and

hence the minimum value of xT (A + I)x over ∆n is

1
α(G) . Therefore, for any k > 0, α(G) ≤ k
k for all x ∈ ∆n, which holds if and only if xT (k(A + I) − J)x ≥ 0

if and only if xT (A + I)x ≥ 1

for all x ∈ ∆n. The ﬁrst statement of the corollary then follows from the homogeneity of

xT (k(A + I) − J)x.

To show that pA,k is positive deﬁnite when α(G) < k, observe that

kA + kI − J = (α(G)(A + I) − J) + (k − α(G))(A + I).

Considering the two terms on the right separately, we observe that (x2)T (α(G)(A + I) − J)x2

(i.e., pA,α(G)) is nonnegative since MA,α(G) is copositive, and that (x2)T (k − α(G))(A + I)x2

is positive deﬁnite. Therefore, their sum (x2)T (kA + kI − J)x2 is positive deﬁnite.

We now present the proof of Theorem 3.2.1. While the statement of the theorem is

given for degree-4 polynomials, it is straightforward to extend the result to higher-degree

polynomials. We note that degree four is the smallest degree for which deciding existence of

local minima is intractable. For degree-3 polynomials, it turns out that this question can be

answered by solving semideﬁnite programs of polynomial size [7].

2The converse of this statement also holds, but we do not need it for the proof of Theorem 3.2.1.

83

Proof (of Theorem 3.2.1). We present a polynomial-time reduction from the STABLESET

problem. Let a graph G on n vertices with adjacency matrix A and a positive integer r ≤ n

be given. We show that G has a stable set of size r if and only if the quartic form pA,r−0.5

deﬁned in (3.5) has no local minimum. This is a consequence of the following more general

fact that we prove below: For a noninteger scalar k, the quartic form pA,k has no local

minimum if and only if α(G) ≥ k.

We ﬁrst observe that if α(G) < k, then pA,k has a local minimum. Indeed, recall from the

second claim of Corollary 3.2.3 that under this assumption, pA,k is positive deﬁnite. Since

pA,k vanishes at the origin, it follows that the origin is a local minimum. Suppose now that

α(G) ≥ k. Since k is noninteger, this implies that α(G) > k. We show that in this case, pA,k

has no local minimum by showing that the origin must be the only second-order point of

pA,k. Since any local minimum of a polynomial is a second-order point, only the origin can

be a candidate local minimum for pA,k. However, by the ﬁrst claim of Corollary 3.2.3, the

matrix MA,k is not copositive and hence pA,k is not nonnegative. As pA,k is homoegenous,

this implies that pA,k takes negative values arbitrarily close to the origin, ruling out the

possibility of the origin being a local minimum.

To show that when α(G) > k, the origin is the only second-order point of pA,k, we

compute the gradient and Hessian of pA,k. We have

∇pA,k(x) = 4x · MA,kx2,

and

∇2pA,k(x) = 8MA,k · xxT + 4diag(MA,kx2).

Suppose for the sake of contradiction that pA,k has a nonzero second-order point ¯x. Since ¯x

is a critical point, ∇pA,k(¯x) = 0 and thus (MA,k ¯x2)¯x = 0. It then follows that

(∇2pA,k(¯x))¯x = 8(MA,k)¯x · ¯x¯x ¯xT
¯x .

84

Because ∇2pA,k(¯x) (cid:23) 0 and thus all its principal submatrices are positive semideﬁnite, we

have 8(MA,k)¯x · ¯x¯x ¯xT

¯x (cid:23) 0. Since

(MA,k)¯x · ¯x¯x ¯xT

¯x = diag(¯x¯x)(MA,k)¯xdiag(¯x¯x),

and since diag(¯x¯x) is an invertible matrix, it follows that (MA,k)¯x (cid:23) 0.

We now consider the induced subgraph G¯x of G with vertices corresponding to the indices

of the nonzero entries of ¯x. Note that the adjacency matrix of G¯x is A¯x. Furthermore, observe

that MA¯x,k = (MA,k)¯x, and therefore MA¯x,k is positive semideﬁnite and thus copositive. We

conclude from the ﬁrst claim of Corollary 3.2.3 that α(G¯x) ≤ k. We now claim that

MA¯x,k ¯x2

¯x = (MA,k)¯x ¯x2

¯x = (MA,k ¯x2)¯x = 0.

The ﬁrst equality follows from that MA¯x,k = (MA,k)¯x, the second from that the indices of

the nonzero entries of ¯x are the same as those of ¯x2, and the third from that ∇pA,k(¯x) = 0.

Hence, pA¯x,k(¯x¯x) = 0. Since ¯x¯x is nonzero, pA¯x,k is not positive deﬁnite. By the second claim

of Corollary 3.2.3, we must have α(G¯x) ≥ k. Therefore, α(G¯x) = k. However, because k was

assumed to be noninteger, we have a contradiction.

It turns out that the proof of Theorem 3.2.1 also shows that it is NP-hard to decide if

a quartic polynomial has a strict local minimum. Recall that a strict local minimzer of a

function f : Rn → R over a set Ω ⊆ Rn is a point ¯x ∈ Ω for which there exists a scalar (cid:15) > 0

such that p(¯x) < p(x) for all x ∈ Ω\¯x with (cid:107)x − ¯x(cid:107) ≤ (cid:15).

Corollary 3.2.4. It is strongly NP-hard to decide if a degree-4 polynomial has a strict local

minimum.

Proof. Observe from the proof of Theorem 3.2.1 that for a graph G and a noninteger scalar

k, the quartic form pA,k has a local minimum if and only if α(G) < k. In the case where

85

pA,k does have a local minimum, we showed that pA,k is positive deﬁnite, and thus the local

minimum (the origin) must be a strict local minimum.

We now turn our attention to local minima of quadratic programs.

Theorem 3.2.5. It is strongly NP-hard to decide if a quadratic function has a local minimum

over a polyhedron. The same is true for deciding if a quadratic function has a strict local

minimum over a polyhedron.

Proof. We present a polynomial-time reduction from the STABLESET problem to the prob-

lem of deciding if a quadratic function has a local minimum over a polyhedron. The reader

can check that same reduction is valid for the case of strict local minima.

Let a graph G on n vertices with adjacency matrix A and a positive integer r ≤ n

be given. Let k = r − 0.5, qA,k be the quadratic form deﬁned in (3.4), and consider the

optimization problem

min
x∈Rn

qA,k(x)

subject to x ≥ 0.

(3.6)

We show that a point x ∈ Rn is a local minimum of (3.6) if and only if

√

x is a local minimum

of the quartic form pA,k deﬁned in (3.5). By the arguments in the proof of Theorem 3.2.1,

we would have that (3.6) has no local minimum if and only if G has a stable set of size r.

Indeed, if x is not a local minimum of (3.6), there exists a sequence {yj} ⊆ Rn with

yj → x and such that for all j, yj ≥ 0 and qA,k(yj) < qA,k(x). The sequence {
√

√

√

√

√

then satisfy pA,k(

yj) < pA,k(

x) and

yj →

x, proving that

x is not a local minimum

√

yj} would

of pA,k. Similarly, if x is not a local minimum of pA,k, there exists a sequence {zj} ⊆ Rn such

that zj → x and pA,k(zj) < pA,k(x) for all j. The sequence {z2

j } would then prove that x2 is

not a local minimum of (3.6).

86

3.2.2 Complexity of Finding a Local minimum of a Quadratic

Function Over a Polytope

We now address the original question posed by Pardalos and Vavasis concerning the com-

plexity of ﬁnding a local minimum of a quadratic program with a compact feasible set. Note

again that if the feasible set is compact, the existence of a local minimum is guaranteed.

In fact, there will always be a local minimum that has rational entries with polynomial

bitsize [109].

Theorem 3.2.6. If there is a polynomial-time algorithm that ﬁnds a point within Euclidean

distance cn (for any constant c ≥ 0) of a local minimum of an n-variate quadratic function

over a polytope, then P = N P .

Proof. Fix any constant c ≥ 0. We show that if an algorithm could take as input a quadratic

program with a bounded feasible set and in polynomial time return a point within distance

cn of any local minimum, then this algorithm would solve the the STABLESET problem in

polynomial time.

Let a graph G on n vertices with adjacency matrix A and a positive integer r ≤ n be

given. Let k = r −0.5, qA,k be the quadratic form deﬁned in (3.4), and consider the quadratic

program3

min
x∈Rn

qA,k(x)

subject to x ≥ 0,

xi ≤ 3cn√

n.

n
(cid:88)

i=1

(3.7)

Note that the feasible set of this problem is bounded. Moreover, the number of bits required
to write down this quadratic program is polynomial in n. This is because the scalar 3cn√
n

takes 2+n(cid:100)log2(c+1)(cid:101)+ 1

2(cid:100)log2(n+1)(cid:101) bits to write down, and the remaining O(n2) numbers

3Without loss of generality, we suppose that c is rational. If c is irrational, one can e.g. replace it in (3.7)

with (cid:100)c(cid:101).

87

in the problem data are bounded in magnitude by n, so they each take O(log2(n)) bits to

write down.

We will show that if α(G) < k, the origin is the unique local minimum of (3.7), and that if
i=1 ¯xi = 3cn√
i=1 xi = 3cn√
n}

α(G) ≥ k (equivalently α(G) > k), any local minimum ¯x of (3.7) satisﬁes (cid:80)n
Since the (Euclidean) distance from the origin to the hyperplane {x ∈ Rn| (cid:80)n

n.

is 3cn, there is no point that is within distance cn of both the origin and this hyperplane.

Thus, the graph G has no stable set of size r (or equivalently α(G) < k) if and only if the

Euclidean norm of all points within distance cn of any local minimum of (3.7) is less than or

equal to cn.

To see why α(G) < k implies that the origin is the unique local minimum of (3.7), recall

from the second claim of Corollary 3.2.3 that the quartic form pA,k deﬁned in (3.5) must be

positive deﬁnite. Thus, for any nonzero vector x ≥ 0, we have qA,k(x) > 0. This implies that

the origin is a local minimum of (3.7). Moreover, since qA,k is homogeneous, we have that

no other feasible point can be a local minimum. Indeed, for any nonzero vector x ≥ 0 and

any nonnegative scalar (cid:15) < 1, qA,k((cid:15)x) < qA,k(x).

To see why when α(G) > k, the last constraint of (3.7) must be tight at all local minima,

recall from the proof of Theorem 3.2.5 that when α(G) > k, the optimization problem in

(3.6) has no local minimum. Therefore, for any vector x that is feasible to (3.7) and satisﬁes

(cid:80)n

i=1 xi < 3cn√

n, there exists a sequence {yi} ⊆ Rn with yi → x, and satisfying

yi ≥ 0,

n
(cid:88)

i=1

yi < 3cn√

n, qA,k(yi) < qA,k(x), ∀i.

As the points yi are feasible to (3.7), any vector x satisfying (cid:80)n

i=1 xi < 3cn√

n cannot

be a local minimum of (3.7). Thus, if α(G) > k, any local minimum ¯x of (3.7) satisﬁes

(cid:80)n

i=1 ¯xi = 3cn√
n.
By replacing the quantity 3cn√

n in the proof of Theorem 3.2.6 with 3nc+0.5 and 2n

respectively, we get the following two corollaries.

88

Corollary 3.2.7. If there is a pseudo-polynomial time algorithm that ﬁnds a point within

Euclidean distance nc (for any constant c ≥ 0) of a local minimum of an n-variate quadratic

function over a polytope, then P = N P .

Corollary 3.2.8. If there is a polynomial-time algorithm that ﬁnds a point within Euclidean

√

distance (cid:15)

n (for any constant (cid:15) ∈ [0, 1)) of a local minimum of a restricted set of quadratic

programs over n variables whose numerical data are integers bounded in magnitude by 2n,

then P = N P .

In [87], Pardalos and Vavasis also propose two follow-up questions about quadratic pro-

grams with compact feasible sets. The ﬁrst is about the complexity of ﬁnding a “KKT

point”. As is, our proof does not have any implications for this question since the origin is

always a KKT point of the quadratic programs that arise from our reductions. The second

question asks whether ﬁnding a local minimum is easier in the special case where the prob-

lem only has one local minimum (which is thus also the global minimum). Related to this

question, we can prove the following claim.

Theorem 3.2.9. If there is a polynomial-time algorithm which decides whether a quadratic

program with a bounded feasible set has a unique local minimum, and if so returns this

minimum4, then P=NP.

Proof. Suppose there was such an algorithm (call it Algorithm U). We show that Algo-

rithm U would solve the STABLESET problem in polynomial time. Let a graph G on n

vertices with adjacency matrix A and a positive integer r ≤ n be given, and input the

quadratic program (3.7), with k = r − 0.5, into Algorithm U. Observe from the proof of

Theorem 3.2.6 that there are three possibilities for this quadratic program: (i) the origin is

the unique local minimum, (ii) there is a unique local minimum and it is on the hyperplane
{x ∈ Rn| (cid:80)n
the hyperplane {x ∈ Rn| (cid:80)n

n}, and (iii) there are multiple local minima and they are all on

n}. Case (i) indicates that α(G) < k, and the

i=1 xi = 3cn√

i=1 xi = 3cn√

4This unique local (and therefore global) minimum is guaranteed to have rational entries with polynomial

bitsize; see [109].

89

output of Algorithm U in this case would be the origin. Cases (ii) and (iii) both indicate

that α(G) > k. The output of Algorithm U is a point away from the origin in case (ii), and

the declaration that the local minimum is not unique in case (iii). Thus Algorithm U would

reveal which case we are in, and that would allow us to decide if G has a stable set of size r

in polynomial time.

To conclude, we have established intractability of several problems related to local minima

of quadratic programs. We hope our results motivate more research on identifying classes of

quadratic programs where local minima can be found more eﬃciently than global minima.

One interesting example is the case of the concave knapsack problem, where Mor´e and

Vavasis [72] show that a local minimum can be found in polynomial time even though,

unless P=NP, a global minimum cannot.

90

Chapter 4

On Attainment of the Optimal Value

in Polynomial Optimization

4.1

Introduction

In this chapter, we again consider problems of the form

inf
x

p(x)

subject to qi(x) ≥ 0, ∀i ∈ {1, . . . , m},

(4.1)

where p, qi are polynomial functions, and address the problem of testing whether the optimal

value is attained, provided that the optimal value p∗ is ﬁnite. More formally, does there exist

a feasible point x∗ such that p(x∗) = p∗? Such a point x∗ will be termed an optimal solution.

For this chapter, we will refer to sets of the type {x ∈ Rn | qi(x) ≥ 0, ∀i ∈ {1, . . . , m}} as

closed basic semialgebraic sets.

Existence of optimal solutions is a fundamental question in optimization and its study

has a long history, dating back to the nineteenth century with the extreme value theorem of

91

Bolzano and Weierstrass.1 The question of testing attainment of the optimal value for POPs

has appeared in the literature explicitly. For example, Nie, Demmel, and Sturmfels describe

an algorithm for globally solving an unconstrained POP which requires as an assumption

that the optimal value be attained [84]. This leads them to make the following remark in

their conclusion section:

“This assumption is non-trivial, and we do not address the (important and diﬃ-

cult) question of how to verify that a given polynomial f (x) has this property.”

Prior literature on existence of optimal solutions to POPs has focused on identifying

cases where existence is always guaranteed. The best-known result here is the case of linear

programming (i.e., when the degrees of p and qi are one). In this case, the optimal value of

the problem is always attained. This result was extended by Frank and Wolfe to the case

where p is quadratic and the polynomials qi are linear [38]. Consequently, results concerning

attainment of the optimal value are sometimes referred to as “Frank-Wolfe type” theorems

in the literature [17, 68]. Andronov et al. showed that the same statement holds again when

p is cubic (and the polynomials qi are linear) [10].

Our results in this chapter show that in all other cases, it is strongly NP-hard to de-

termine whether a polynomial optimization problem attains its optimal value. This implies

that unless P=NP, there is no polynomial-time (or even pseudo-polynomial time) algorithm

for checking this property. Nevertheless, it follows from the Tarski-Seidenberg quantiﬁer

elimination theory [100, 105] that this problem is decidable, i.e., can be solved in ﬁnite time.

There are also probabilistic algorithms that test for attainment of the optimal value of a

POP [43, 44], but their complexities are exponential in the number of variables.

In this chapter, we also study the complexity of testing several well-known suﬃcient con-

ditions for attainment of the optimal value (see Section 4.1.1 below). One suﬃcient condition

that we do not consider but that is worth noting is for the polynomials p, −q1, . . . , −qm to

1To remove possible confusion, we emphasize that our focus in this chapter is not on the complexity of
testing feasibility or unboundedness of problem (4.1), which have already been studied extensively. On the
contrary, all optimization problems that we consider are by construction feasible and bounded below.

92

all be convex (see [17] for a proof, [68] for the special case where p and qi are quadratics, and

[20] for other extensions). The reason we exclude this suﬃcient condition from our study is

that the complexity of checking convexity of polynomials has already been analyzed in [3].

4.1.1 Organization and Contributions of the Chapter

As mentioned before, this chapter concerns itself with the complexity of testing attainment

of the optimal value of a polynomial optimization problem. More speciﬁcally, we show in

Section 4.2 that it is strongly NP-hard to test attainment when the objective function has

degree 4, even in absence of any constraints (Theorem 4.2.1), and when the constraints are

of degree 2, even when the objective is linear (Theorem 4.2.2).

In Section 4.3, we show that several well-known suﬃcient conditions for attainment of

the optimal value in a POP are also strongly NP-hard to test. These include coercivity of

the objective function (Theorem 4.3.1), closedness of a bounded feasible set (Theorem 4.3.2

and Remark 4.3.1), boundedness of a closed feasible set (Corollary 4.3.3), a robust analogue

of compactness known as stable compactness (Corollary 4.3.5), and an algebraic certiﬁcate

of compactness known as the Archimedean property (Theorem 4.3.8). The latter property is

of independent interest to the convergence of the Lasserre hierarchy, as discussed in Section

4.3.2.

In Section 4.4, we give semideﬁnite programming (SDP) based hierarchies for testing

compactness of the feasible set and coercivity of the objective function of a POP (Proposi-

tions 4.4.1 and 4.4.2). The hierarchy for compactness comes from a straightforward appli-

cation of Stengle’s Positivstellensatz (cf. Theorem 4.3.7), but the one for coercivity requires

us to develop a new characterization of coercive polynomials (Theorem 4.4.3). We end the

chapter in Section 4.5 with a summary and some brief concluding remarks.

93

4.2 NP-hardness of Testing Attainment of the Optimal

Value

In this section, we show that testing attainment of the optimal value of a polynomial opti-

mization problem is NP-hard. Throughout this chapter, when we study complexity questions

around problem (4.1), we ﬁx the degrees of all polynomials involved and think of the number

of variables and the coeﬃcients of these polynomials as input. Since we are working in the

Turing model of computation, all the coeﬃcients are rational numbers and the input size can

be taken to be the total number of bits needed to represent the numerators and denominators

of these coeﬃcients.

Our proofs of hardness are based on reductions from ONE-IN-THREE 3SAT which is

known to be NP-hard [99]. Recall that in ONE-IN-THREE 3SAT, we are given a 3SAT

instance (i.e., a collection of clauses, where each clause consists of exactly three literals, and

each literal is either a variable or its negation) and we are asked to decide whether there

exists a {0, 1} assignment to the variables that makes the expression true with the additional

property that each clause has exactly one true literal.

Theorem 4.2.1. Testing whether a degree-4 polynomial attains its unconstrained inﬁmum

is strongly NP-hard.

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables x1, . . . , xn, and k

clauses. Let sφ(x) : Rn → R be deﬁned as

sφ(x) =

k
(cid:88)

(φi1 + φi2 + φi3 + 1)2 +

i=1

n
(cid:88)

i=1

(1 − x2

i )2,

(4.2)

where φit = xj if the t-th literal in the i-th clause is xj, and φit = −xj if it is ¬xj (i.e., the

negation of xj). Now, let

pφ(x, y, z, λ) := λ2sφ(x) + (1 − λ)2(y2 + (yz − 1)2),

(4.3)

94

where y, z, λ ∈ R. We show that pφ achieves its inﬁmum if and only if φ is satisﬁable. Note

that the reduction is polynomial in length and the coeﬃcients of pφ are at most a constant

multiple of n + k in absolute value.

If φ has a satisfying assignment, then for any y and z, letting λ = 1, xi = 1 if the variable

is true in that assignment and xi = −1 if it is false, results in a zero of pφ. As pφ is a sum

of squares and hence nonnegative, we have shown that it achieves its inﬁmum.

Now suppose that φ is not satisﬁable. We will show that pφ is positive everywhere but

gets arbitrarily close to zero. To see the latter claim, simply set λ = 0, z = 1

y , and let y → 0.

To see the former claim, suppose for the sake of contradiction that pφ has a zero. Since

y2 + (yz − 1)2 is always positive, we must have λ = 1 in order for the second term to be zero.

Then, in order for the whole expression to be zero, we must also have that sφ(x) must vanish

at some x. But any zero of sφ must have each x ∈ {−1, 1}n, due to the second term of sφ.

However, because the instance φ is not satisﬁable, for any such x, there exists i ∈ {1, . . . , k}

such that φi1 + φi2 + φi3 + 1 (cid:54)= 0, as there must be a clause where not exactly one literal is

set to one. This means that sφ is positive everywhere, which is a contradiction.

We have thus shown that testing attainment of the optimal value is NP-hard for uncon-

strained POPs where the objective is a polynomial of degree 6. In the interest of minimality,

we now extend the proof to apply to an objective function of degree 4. To do this, we ﬁrst

introduce n + 1 new variables χ1, . . . , χn and w. We replace every occurrence of the product

λxi in λ2sφ with the variable χi. For example, the term λ2x1x2 would become χ1χ2. Let

ˆsφ(x, χ, λ) denote this transformation on λ2sφ(x). Note that ˆsφ(x, χ, λ) is now a quartic

polynomial. Now consider the quartic polynomial (whose coeﬃcients are again at most a

constant multiple of n + k in absolute value)

ˆpφ(x, y, z, λ, χ, w) = ˆsφ(x, χ, λ) + (1 − λ)2(y2 + (w − 1)2) + (w − yz)2 +

n
(cid:88)

(χi − λixi)2. (4.4)

i=1

95

Observe that ˆpφ is a sum of squares as ˆsφ can be veriﬁed to be a sum of squares by bringing

λ inside every squared term of sφ. Hence, ˆpφ is nonnegative. Furthermore, its inﬁmum is

still zero, as the choice of variables λ = 0, w = 1, χ = 0, x arbitrary, z = 1

y , and letting y → 0

will result in arbitrarily small values of ˆpφ. Now it remains to show that this polynomial

will have a zero if and only if pφ in (4.3) has a zero. Observe that if (x, y, z, λ) is a zero of

pφ, then (x, y, z, λ, λx, yz) is a zero of ˆpφ. Conversely, if (x, y, z, λ, χ, w) is a zero of ˆpφ, then

(x, y, z, λ) is a zero of pφ.

Remark 4.2.1. Because we use the ideas behind this reduction repeatedly in the remainder

of this chapter, we refer to the quartic polynomial deﬁned in (4.2) as sφ throughout. The

same convention for φit relating the literals of φ to the variables x will be assumed as well.

We next show that testing attainment of the optimal value of a POP is NP-hard when the

objective function is linear and the constraints are quadratic. Together with the previously-

known Frank-Wolfe type theorems which we reviewed in the introduction, Theorems 4.2.1

and 4.2.2 characterize the complexity of testing attainment of the optimal value in polynomial

optimization problems of any given degree. Indeed, our reductions can trivially be extended

to the case where the constraints or the objective have higher degrees. For example to

increase the degree of the constraints to some positive integer d, one can introduce a new

variable γ along with the trivial constraint γd = 0. To increase the degree of the objective

from four to a higher degree 2d, one can again introduce a new variable γ and add the term

γ2d to the objective function.

Theorem 4.2.2. Testing whether a degree-1 polynomial attains its inﬁmum on a feasible set

deﬁned by degree-2 inequalities is strongly NP-hard.

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses. Deﬁne

the following POP, with x, χ ∈ Rn and λ, y, z, w, γ, ζ, ψ ∈ R:

96

min
x,χ,λ,y,z,w,γ,ζ,ψ

subject to

γ

γ ≥ λ

n
(cid:88)

i=1

χi + (1 − λ)(ψ + ζ)

1 − x2

i = 0, ∀i ∈ {1, . . . , n},

χi = (φi1 + φi2 + φi3 + 1)2, ∀i ∈ {1, ..., k},

ψ = y2,

yz = w,

ζ = (w − 1)2,

λ(1 − λ) = 0.

(4.5)

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

(4.11)

(4.12)

We show that the inﬁmum of this POP is attained if and only if φ is satisﬁable. Note

ﬁrst that the objective value is always nonnegative because of (4.6) and in view of (4.8),

(4.9), (4.11), and (4.12). Observe that if φ has a satisfying assignment, then letting xi = 1

if the variable is true in that assignment and xi = −1 if it is false, along with λ = 1, y and z

arbitrary, ψ = y2, w = yz, and ζ = (w − 1)2, results in a feasible solution with an objective

value of 0.

If φ is not satisﬁable, the objective value can be made arbitrarily close to zero by taking

an arbitrary x ∈ {−1, 1}n, χi accordingly to satisfy (4.8), λ = 0, ψ = y2, z = 1

y , w = 1, ζ = 0,

and letting y → 0. Suppose for the sake of contradiction that there exists a feasible solution

to the POP with γ = 0. As argued before, because of the constraints (4.8), (4.9), (4.11), and
(4.12), λ (cid:80)n

i=1 χi + (1 − λ)(ψ + ζ) is always nonnegative, and so for γ to be exactly zero, we

need to have

λ

n
(cid:88)

i=1

χi + (1 − λ)(ψ + ζ) = 0.

97

From (4.12), either λ = 0 or λ = 1. If λ = 1, then we must have χi = 0, ∀i = 1, . . . , n, which

is not possible as φ is not satisﬁable. If λ = 0, then we must have ψ + ζ = y2 + (yz − 1)2 = 0,

which cannot happen as this would require y = 0 and yz = 1 concurrently.

4.3 NP-hardness of Testing Suﬃcient Conditions for

Attainment

Arguably, the two best-known suﬃcient conditions under which problem (4.1) attains its

optimal value are compactness of the feasible set and coercivity of the objective function.

In this section, we show that both of these properties are NP-hard to test for POPs of low

degree. We also prove that certain stronger conditions, namely the Archimedean property of

the quadratic module associated with the constraints and stable compactness of the feasible

set, are NP-hard to test.

4.3.1 Coercivity of the Objective Function

A function p : Rn → R is coercive if for every sequence {xk} such that (cid:107)xk(cid:107) → ∞, we have

p(xk) → ∞. It is well known that a continuous coercive function achieves its inﬁmum on a

closed set (see, e.g., Appendix A.2 of [19]). This is because all sublevel sets of continuous

coercive functions are compact.

Theorem 4.3.1. Testing whether a degree-4 polynomial is coercive is strongly NP-hard.

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses, and the

associated quartic polynomial sφ(x) as in (4.2). Let sφh : Rn+1 → R be the homogenization

of this polynomial:

sφh(x0, x) := x4

0sφ

(cid:19)

(cid:18) x
x0

=

k
(cid:88)

i=1

0(φi1 + φi2 + φi3 + x0)2 +
x2

98

n
(cid:88)

(x2

0 − x2

i )2.

i=1

(4.13)

By construction, sφh is a homogeneous polynomial of degree 4. We show that sφh is coercive

if and only if φ is not satisﬁable.

Suppose ﬁrst that the instance φ has a satisfying assignment ˆx ∈ {−1, 1}n. Then it is

easy to see that sφh(1, ˆx) = 0. As sφh is homogeneous, sφh(α, αˆx) = 0 for all α, showing that

sφh is not coercive.

Now suppose that φ is not satisﬁable. We show that sφh is positive deﬁnite (i.e.,

sφh(x0, x) > 0 for all (x0, x) (cid:54)= (0, 0)). This would then imply that sφh is coercive as

sφh(x0, x) = ||(x0, x)T ||4 · sφh

(cid:18) (x0, x)

(cid:19)

||(x0, x)T ||

≥ µ||(x0, x)T ||4,

where µ > 0 is deﬁned as the minimum of sφh on the unit sphere:

µ = min

(x0,x)∈Sn

sφh(x0, x).

Suppose that sφh was not positive deﬁnite. Then there exists a point (ˆx0, ˆx) (cid:54)= (0, 0) such

that sφh(ˆx0, ˆx) = 0. First observe ˆx0 cannot be zero due to the (x0 − xi)2 terms in (4.13).

As ˆx0 (cid:54)= 0, then, by homogeneity, the point (1, ˆx
ˆx0

) is a zero of sφh as well. This however

implies that sφ(ˆx) = 0, which we have previously argued (cf. the proof of Theorem 4.2.1) is

equivalent to satisﬁability of φ, hence a contradiction.

We remark that the above hardness result is minimal in the degree as odd-degree poly-

nomials are never coercive and a quadratic polynomial xT Qx + bT x + c is coercive if and only

if the matrix Q is positive deﬁnite, a property that can be checked in polynomial time (e.g.,

by checking positivity of the leading principal minors of Q).

99

4.3.2 Closedness and Boundedness of the Feasible Set

The well-known Bolzano-Weierstrass extreme value theorem states that the inﬁmum of a

continuous function on a compact (i.e., closed and bounded) set is attained. In this section,

we show that testing closedness or boundedness of a basic semialgebraic set deﬁned by

degree-2 inequalities is NP-hard. Once again, these hardness results are minimal in degree

since these properties can be tested in polynomial time for sets deﬁned by aﬃne inequalities,

as we describe next.

To check boundedness of a set P := {x ∈ Rn | aT

i x ≥ bi, i = 1, . . . , m} deﬁned by aﬃne

inequalities, one can ﬁrst check that P is nonempty, and if it is, for each i minimize and

maximize xi over P . Note that P is unbounded if and only if at least one of these 2n

linear programs is unbounded, which can be certiﬁed e.g. by detecting infeasibility of the

corresponding dual problem. Thus, boundedness of P can be tested by solving 2n + 1 linear

programming feasibility problems, which can be done in polynomial time.

To check closedness of a set P := {x ∈ Rn | aT

i x ≥ bi, i = 1, . . . , m, cT

j x > dj, j = 1, . . . , r},

one can for each j minimize cT

j x over {x ∈ Rn | aT

i x ≥ bi, i = 1, . . . , m} and declare that P is

closed if and only if all of the respective optimal values are greater than dj. Thus, closedness

of P can be tested by solving r linear programs, which can be done in polynomial time.

Theorem 4.3.2. Given a set of quadratic polynomials qi, i = 1, . . . , m, tj, j = 1, . . . , r, it is

strongly NP-hard to test whether the basic semialgebraic set

{x ∈ Rn| qi(x) ≥ 0, i = 1, . . . , m, tj(x) > 0, j = 1, . . . , r}

is closed2.

2Note that m is not ﬁxed in this statement or in Corollaries 4.3.3 and 4.3.5 below.

100

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses. Let φij

be as in the proof of Theorem 4.2.1 and consider the set

Sφ = (cid:8)(x, y) ∈ Rn+1| (φi1 + φi2 + φi3 + 1)y = 0, i = 1, . . . , k, 1 − x2

j = 0, j = 1, . . . , n, y < 1(cid:9).

(4.14)

We show that Sφ is closed if and only if the instance φ is not satisﬁable. To see this, ﬁrst

note that we can rewrite Sφ as

(cid:110)

Sφ =

{−1, 1}n × {0}

(cid:111)

∪

(cid:110)

{x ∈ Rn| sφ(x) = 0} × {y ∈ R| y < 1}

(cid:111)
,

where sφ is as in the proof of Theorem 4.2.1. If φ is not satisﬁable, then Sφ = {−1, 1}n × {0},

which is closed. If φ is satisﬁable, then {x ∈ Rn| sφ(x) = 0} is nonempty and

{x ∈ Rn| sφ(x) = 0} × {y ∈ R| y < 1}

is not closed and not a subset of {−1, 1}n × {0}. This implies that Sφ is not closed.

Remark 4.3.1. We note that the problem of testing closedness of a basic semialgebraic set

remains NP-hard even if one has a promise that the set is bounded. Indeed, one can add

the constraint y ≥ −1 to the set Sφ in (4.14) to make it bounded and this does not change

the previous proof.

Corollary 4.3.3. Given a set of quadratic polynomials qi, i = 1, . . . , m, it is strongly NP-

hard to test whether the set

{x ∈ Rn| qi(x) = 0, i = 1, . . . , m}

is bounded.

101

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses. Let φij

be as in the proof of Theorem 4.2.1 and consider the set

S = (cid:8)(x, y) ∈ Rn+1| (φi1 + φi2 + φi3 + 1)y = 0, i = 1, . . . , k, 1 − x2

j = 0, j = 1, . . . , n(cid:9).

This set is bounded if and only if φ is not satisﬁable. One can see this by following the

proof of Theorem 4.3.2 and observing that y will be unbounded in the satisﬁable case, and

only 0 otherwise.

Note that it follows immediately from either of the results above that testing compactness

of a basic semialgebraic set is NP-hard. We end this subsection by establishing the same

hardness result for a suﬃcient condition for compactness that has featured in the literature

on polynomial optimization (see, e.g., [69], [84, Section 7]).

Deﬁnition 4.3.4. A closed basic semialgebraic set S = {x ∈ Rn| qi(x) ≥ 0, i = 1, . . . , m} is

stably compact if there exists (cid:15) > 0 such that the set {x ∈ Rn| δi(x)+qi(x) ≥ 0, i = 1, . . . , m}

is compact for any set of polynomials δi having degree at most that of qi and coeﬃcients at

most (cid:15) in absolute value.

Intuitively, a closed basic semialgebraic set is stably compact if it remains compact under

small perturbations of the coeﬃcients of its deﬁning polynomials. A stably compact set is

clearly compact, though the converse is not true as shown by the set

S = (cid:8)(x1, x2) ∈ R2| (x1 − x2)4 + (x1 + x2)2 ≤ 1(cid:9).

Indeed, this set is contained inside the unit disk, but for (cid:15) > 0, the set

S(cid:15) = (cid:8)(x1, x2) ∈ R2| (x1 − x2)4 − (cid:15)x4

1 + (x1 + x2)2 ≤ 1(cid:9)

is unbounded as its deﬁning polynomial tends to −∞ along the line x1 = x2.

102

Section 5 of [69] shows that the set S in Deﬁnition 4.3.4 is stably compact if and only if

the function

q(x) = max

i,j

{−qij(x)}

is positive on the unit sphere. Here, qij(x) is a homogenenous polynomial that contains all

terms of degree j in qi(x). Perhaps because of this characterization, the same section in [69]

remarks that “stable compactness is easier to check than compactness”, though as far as

polynomial-time checkability is concerned, we show that the situation is no better.

Corollary 4.3.5. Given a set of quadratic polynomials qi, i = 1, . . . , m, it is strongly NP-

hard to test whether the set

{x ∈ Rn| qi(x) = 0, i = 1, . . . , m}

is stably compact.

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses. Let φij

be as in the proof of Theorem 4.2.1 and consider the set

Tφ = (cid:8)(x0, x) ∈ Rn+1| (φi1 + φi2 + φi3 + x0)2 = 0, i = 1, . . . , k, x2

0 − x2

j = 0, j = 1, . . . , n(cid:9).

We show that the function

qφ(x0, x) =

max
i=1,...,k,j=1,...,n

{−(φi1 + φi2 + φi3 + x0)2, (φi1 + φi2 + φi3 + x0)2, x2

0 − x2

j , x2

j − x2
0}

is positive on the unit sphere if and only if the instance φ is not satisﬁable. Suppose ﬁrst

that φ is not satisﬁable and assume for the sake of contradiction that there is a point (x0, x)

on the sphere such that qφ(x0, x) = 0. This implies that φi1 + φi2 + φi3 + x0 = 0, ∀i = 1, . . . , k

and x2

0 = x2

j , ∀j = 1, . . . , n. Hence, x0 (cid:54)= 0 and x
x0

is a satisfying assignment to φ, which is

a contradiction. Suppose now that φ has a satisfying assignment ˆx ∈ {−1, 1}n. Then it is

103

easy to check that

(cid:18) (1, ˆx)

(cid:19)

||(1, ˆx)T ||

qφ

= 0.

The Archimedean Property

An algebraic notion closely related to compactness is the so-called Archimedean property.

This notion has frequently appeared in recent literature at the interface of algebraic geometry

and polynomial optimization. The Archimedean property is the assumption needed for

the statement of Putinar’s Positivstellensatz [92] and convergence of the Lasserre hierarchy

[61]. In this subsection, we recall the deﬁnition of the Archimedean property and study the

complexity of checking it. To our knowledge, the only previous result in this direction is

that testing the Archimedean property is decidable [111, Section 3.3].

Recall that the quadratic module associated with a set of polynomials q1, . . . , qm is the

set of polynomials that can be written as

σ0(x) +

m
(cid:88)

i=1

σi(x)qi(x),

where σ0, . . . , σm are sum of squares polynomials.

Deﬁnition 4.3.6. A quadratic module Q is Archimedean if there exists a scalar R > 0 such
that R − (cid:80)n

i=1 x2

i ∈ Q.

Several equivalent characterizations of this property can be found in [63, Theorem 3.17].

Note that a set {x ∈ Rn| qi(x) ≥ 0} for which the quadratic module associated with the

polynomials {qi} is Archimedean is compact. However, the converse is not true. For example,

for n > 1, the sets

(cid:40)

x ∈ Rn| x1 −

1
2

≥ 0, . . . , xn −

1
2

≥ 0, 1 −

(cid:41)

xi ≥ 0

n
(cid:89)

i=1

104

are compact but not Archimedean; see [63], [91] for a proof of the latter claim. Hence,

hardness of testing the Archimedean property does not follow from hardness of testing com-

pactness.

As mentioned previously, the Archimedean property has received recent attention in the

optimization community due to its connection to the Lasserre hierarchy.

Indeed, under

the assumption that the quadratic module associated with the deﬁning polynomials of the

feasible set of (4.1) is Archimedean, the Lasserre hierarchy [61] produces a sequence of

SDP-based lower bounds that converge to the optimal value of the POP. Moreover, Nie has

shown [82] that under the Archimedean assumption, convergence happens in a ﬁnite number

of rounds generically. One way to ensure the Archimedean property—assuming that we know

that our feasible set is contained in a ball of radius R— is to add the redundant constraint
R2 ≥ (cid:80)n

i to the constraints of (4.1). This approach however increases the size of the

i=1 x2

SDP that needs to be solved at each level of the hierarchy. Moreover, such a scalar R may

not be readily available for some applications.

Our proof of NP-hardness of testing the Archimedean property will be based on showing

that the speciﬁc sets that arise from the proof of Corollary 4.3.3 are compact if and only if

their corresponding quadratic modules are Archimedean. Our proof technique will use the

Stengle’s Positivstellensatz, which we recall next.

Theorem 4.3.7 (Stengle’s Positivstellensatz [103]). A basic semialgebraic set

S := {x ∈ Rn| qi(x) ≥ 0, i = 1, . . . , m, rj(x) = 0, j = 1, . . . , k}

is empty if and only if there exist sos polynomials σc1,...,cm and polynomials ti such that

−1 =

k
(cid:88)

j=1

tjrj +

(cid:88)

σc1,...,cm(x)Πm

i=1qi(x)ci.

c1,...,cm∈{0,1}m

105

Remark 4.3.2. Note that if only equality constraints are considered, the second term on the

right hand side is a single sos polynomial σ0,...,0. In the next theorem, we only need this

special case, which is also known as the Real Nullstellensatz [59].

Theorem 4.3.8. Given a set of quadratic polynomials q1, . . . , qm, it is strongly NP-hard to

test whether their quadratic module has the Archimedean property.

Proof. Consider a ONE-IN-THREE 3SAT instance φ with n variables and k clauses. Let φij

be as in the proof of Theorem 4.2.1 and consider the set of quadratic polynomials

(cid:8)(φi1 + φi2 + φi3)y, −(φi1 + φi2 + φi3)y, i = 1, . . . , k; 1 − x2

j , x2

j − 1, j = 1, . . . , n(cid:9).

We show that the quadratic module associated with these polynomials is Archimedean

if and only if φ is not satisﬁable. First observe that if φ is satisﬁable, then the quadratic

module cannot be Archimedean as the set

S = (cid:8)(x, y) ∈ Rn+1| (φi1 + φi2 + φi3 + 1)y = 0, i = 1, . . . , k, 1 − x2

j = 0, j = 1, . . . , n(cid:9)

is not compact (see the proof of Corollary 4.3.3).

Now suppose that the instance φ is not satisﬁable. We need to show that for some scalar

R > 0 and some sos polynomials σ0, σ1, . . . , σk, ˆσ1, . . . , ˆσk,τ1, . . . , τn,ˆτ1, . . . , ˆτn, we have

R −

n
(cid:88)

i=1

i − y2 = σ0(x, y) +
x2

k
(cid:88)

i=1

σi(x, y)(φi1 + φi2 + φi3 + 1)y

+

k
(cid:88)

i=1

ˆσi(x, y)(−φi1 − φi2 − φi3 − 1)y +

n
(cid:88)

j=1

τj(x, y)(1 − x2

j ) +

n
(cid:88)

j=1

ˆτj(x, y)(x2

j − 1).

Since any polynomial can be written as the diﬀerence of two sos polynomials (see, e.g., [4,

Lemma 1]), this is equivalent to existence of a scalar R > 0, an sos polynomial σ0, and some

106

polynomials v1, . . . , vk, t1, . . . , tn such that

R −

n
(cid:88)

i=1

i − y2 = σ0(x, y) +
x2

k
(cid:88)

i=1

vi(x, y)(φi1 + φi2 + φi3 + 1)y +

n
(cid:88)

j=1

tj(x, y)(1 − x2

j ). (4.15)

First, note that

n −

n
(cid:88)

i=1

x2
i =

n
(cid:88)

(1 − x2

i ).

i=1

(4.16)

Secondly, as φ is not satisﬁable, we know that the set

{x ∈ Rn| 1 − x2

j = 0, j = 1, . . . , n, φi1 + φi2 + φi3 + 1 = 0, i = 1, . . . , k}

is empty. From Stengle’s Positivstellensatz, it follows that there exist an sos polynomial ˜σ0

and some polynomials ˜v1, . . . , ˜vk, ˜t1, . . . , ˜tn such that

−1 = ˜σ0(x) +

k
(cid:88)

i=1

˜vi(x)(φi1 + φi2 + φi3 + 1) +

n
(cid:88)

j=1

˜tj(x)(1 − x2

j ).

Multiplying this identity on either side by y2, we obtain:

−y2 = y2˜σ0(x) +

k
(cid:88)

i=1

˜vi(x)y · (φi1 + φi2 + φi3 + 1)y +

n
(cid:88)

j=1

˜tj(x)y2(1 − x2

j ).

(4.17)

Note that if we sum (4.16) and (4.17) and take R = n, σ0(x, y) = y2˜σ0(x), vi(x, y) = y˜vi(x)

for all i = 1, . . . , k, and tj(x, y) = y2 · ˜tj(x) + 1 for all j = 1, . . . , n, we recover (4.15).

107

4.4 Algorithms for Testing Attainment of the Optimal

Value

In this section, we give a hierarchy of suﬃcient conditions for compactness of a closed basic

semialgebraic set, and a hierarchy of suﬃcient conditions for coercivity of a polynomial.

These hierarchies are amenable to semideﬁnite programming (SDP) as they all involve, in

one way or another, a search over the set of sum of squares polynomials. The connection

between SDP and sos polynomials is well known: recall that a polynomial σ of degree

2d is sos if and only if there exists a symmetric positive semideﬁnite matrix Q such that

σ(x) = z(x)T Qz(x) for all x, where z(x) here is the standard vector of monomials of degree

up to d in the variables x (see, e.g. [88]).

The hierarchies that we present are such that if the property in question (i.e., compactness

or coercivity) is satisﬁed on an input instance, then some level of the SDP hierarchy will be

feasible and provide a certiﬁcate that the property is satisﬁed. The test for compactness is a

straightforward application of Stengle’s Positivstellensatz, but the test for coercivity requires

a new characterization of this property, which we give in Theorem 4.4.3.

4.4.1 Compactness of the Feasible Set

Consider a closed basic semialgebraic set

S := {x ∈ Rn| qi(x) ≥ 0, i = 1, . . . , m},

where the polynomials qi have integer coeﬃcients3 and are of degree at most d. A result of

Basu and Roy [16, Theorem 3] implies that if S is bounded, then it must be contained in a

3If some of the coeﬃcients of the polynomials qi are rational (but not integer), we can make them integers

by clearing denominators without changing the set S.

108

ball of radius

√

R∗ :=

n (cid:0)(2d + 1)(2d)n−1 + 1(cid:1) 2(2d+1)(2d)n−1(2nd+2)(2τ +bit((2d+1)(2d)n−1)+(n+1)bit(d+1)+bit(m)),

(4.18)

where τ is the largest bitsize of any coeﬃcient of any qi, and bit(η) denotes the bitsize of η.

With this result in mind, the following proposition is an immediate consequence of Sten-

gle’s Positivstellensatz (c.f. Theorem 4.3.7) after noting that the set S is bounded if and

only if the set

is empty.

{x ∈ Rn| qi(x) ≥ 0, i = 1, . . . , m,

n
(cid:88)

i=1

i ≥ R∗ + 1}
x2

Proposition 4.4.1. Consider a closed basic semialgebraic set S := {x ∈ Rn| qi(x) ≥ 0,

i = 1, . . . , m}, where the polynomials qi have integer coeﬃcients and are of degree at most d.
Let R∗ be as in (4.18) and let q0(x) = (cid:80)n

i − R∗ − 1. Then the set S is compact if and

i=1 x2

only if there exist sos polynomials σh0,...,hm such that

−1 =

(cid:88)

σh0,...,hm(x)Πm

i=0qi(x)hi.

h0,...,hm∈{0,1}m+1

This proposition naturally yields the following semideﬁnite programming-based hierarchy

indexed by a nonnegative integer r:

min
σh0,...,hm

0

subject to − 1 =

(cid:88)

σh0,...,hm(x)Πm

i=0qi(x)hi,

(4.19)

h0,...,hm∈{0,1}m+1

σh0,...,hm is sos and has degree ≤ 2r.

Note that for a ﬁxed level r, one is solving a semideﬁnite program whose size is polynomial

in the description of S. If for some r the SDP is feasible, then we have an algebraic certiﬁcate

of compactness of the set S. Conversely, as Proposition 4.4.1 implies, if S is compact, then

109

the above SDP will be feasible for some level r∗. One can upper bound r∗ by a function

of n, m, and d only using the main theorem of [66]. This bound is however very large and

mainly of theoretical interest.

4.4.2 Coercivity of the Objective Function

It is well known that the inﬁmum of a continuous coercive function over a closed set is

attained. This property has been widely studied, even in the case of polynomial functions;

see e.g.

[14, 13, 53]. A simple suﬃcient condition for coercivity of a polynomial p is for

its terms of highest order to form a positive deﬁnite (homogeneous) polynomial; see, e.g.,

[53, Lemma 4.1]. One can give a hierarchy of SDPs to check for this condition as is done

in [53, Section 4.2]. However, this condition is suﬃcient but not necessary for coercivity.

For example, the polynomial x4

1 + x2

2 is coercive, but its top homogeneous component is

not positive deﬁnite. Theorem 4.4.3 below gives a necessary and suﬃcient condition for a

polynomial to be coercive which lends itself again to an SDP hierarchy. To start, we need

the following proposition, whose proof is straightforward and thus omitted.

Proposition 4.4.2. A function f : Rn → R is coercive if and only if the sets

Sγ := {x ∈ Rn| f (x) ≤ γ}

are bounded for all γ ∈ R.

A polynomial p is said to be s-coercive if p(x)/(cid:107)x(cid:107)s is coercive. The order of coercivity

of p is the supremum over s ≥ 0 for which p is s-coercive. It is known that the order of

coercivity of a coercive polynomial is always positive [13, 42].

Theorem 4.4.3. A polynomial p is coercive if and only if there exist an even integer c > 0

and a scalar k ≥ 0 such that for all γ ∈ R, the γ-sublevel set of p is contained within a ball

of radius γc + k.

110

Proof. The “if” direction follows immediately from the fact that each γ-sublevel set is

bounded. For the converse, suppose that p is coercive and denote its order of coercivity

by q > 0. Then, from Observation 2 of [13], we get that there exists a scalar M ≥ 0 such

that

(cid:107)x(cid:107) ≥ M ⇒ p(x) ≥ (cid:107)x(cid:107)q,

(4.20)

or equivalently (cid:107)x(cid:107) ≤ p(x)

1

q . Now consider the function Rp : R → R which is deﬁned as

Rp(γ) := max
p(x)≤γ

(cid:107)x(cid:107),

i.e. the radius of the γ-sublevel set of p. We note two relevant properties of this function.

• The function Rp(γ) is nondecreasing. This is because the γ-sublevel set of p is a subset

of the (γ + (cid:15))-sublevel set of p for any (cid:15) > 0.

• Let m = inf{γ| Rp(γ) ≥ M }. We claim that

m ≥ M q.

(4.21)

Suppose for the sake of contradiction that we had m < M q. By the deﬁnition of m,

there exists ¯γ ∈ (m, M q) such that Rp(¯γ) ≥ M . This means that there exists ¯x ∈ Rn

such that p(¯x) ≤ ¯γ < M q and (cid:107)¯x(cid:107) ≥ M . From (4.20) we then have p(¯x) ≥ (cid:107)¯x(cid:107)q ≥ M q,

which is a contradiction.

We now claim that Rp(γ) ≤ γ

1
q for all γ > m. Suppose for the sake of contradiction that

there exists γ0 > m such that Rp(γ0) > γ

1
q

0 . This means that there exists x0 ∈ Rn such that

(cid:107)x0(cid:107) > γ

0 but p(x0) ≤ γ0.

1
q

Consider ﬁrst the case where p(x0) ≥ m. Since γ0 > m, we have

(cid:107)x0(cid:107) > γ1/q

0 > m1/q ≥ M,

111

where the last inequality follows from (4.21). It follows from (4.20) that p(x0) ≥ (cid:107)x0(cid:107)q > γ0

which is a contradiction.

Now consider the case where p(x0) < m. By deﬁnition of m, we have Rp(p(x0)) < M ,

and so (cid:107)x0(cid:107) < M . Furthermore, since

γ0 > m

(4.21)
≥ M q,

, which gives (cid:107)x0(cid:107) < M < γ1/q

0

. This contradicts our previous assumption

we have M < γ1/q

0
that (cid:107)x0(cid:107) > γ1/q

.

0

If we let c be the smallest even integer greater than 1/q, we have shown that

Rp(γ) ≤ γ1/q ≤ γc

on the set γ > m. Finally, if we let k = Rp(m), by monotonicity of Rp, we get that

Rp(γ) ≤ γc + k, for all γ.

Remark 4.4.1. One can easily show now that for any coercive polynomial p, there exist an

integer c(cid:48) > 0 and a scalar k(cid:48) ≥ 0 (possibly diﬀering from the scalars c and k given in the

proof of Theorem 4.4.3) such that

R2

p(γ) < γ2c(cid:48) + k(cid:48).

For the following hierarchy it will be easier to work with this form.

In view of the above remark, observe that coercivity of a polynomial p is equivalent to

existence of an integer c(cid:48) > 0 and a scalar k(cid:48) ≥ 0 such that the set

(cid:40)

(γ, x) ∈ Rn+1| p(x) ≤ γ,

(cid:41)

i ≥ γ2c(cid:48) + k(cid:48)
x2

n
(cid:88)

i=1

(4.22)

112

is empty. This formulation naturally leads to the following SDP hierarchy indexed by a

positive integer r.

Proposition 4.4.4. A polynomial p of degree d is coercive if and only if for some integer

r ≥ 1, the following SDP is feasible:

min
σ0,...,σ3

0

subject to −1 =σ0(x, γ) + σ1(x, γ)(γ − p(x)) + σ2(x, γ)

(cid:33)

i − γ2r − 2r
x2

(cid:32) n

(cid:88)

i=1

(cid:33)

+ σ3(x, γ)(γ − p(x))

(cid:32) n

(cid:88)

i=1

x2
i − γ2r − 2r

,

(4.23)

σ0 is sos and of degree ≤ 4r,

σ1 is sos and of degree ≤ max{4r − d, 0},

σ2 is sos and of degree ≤ 2r,

σ3 is sos and of degree ≤ max{2r − d, 0}.

Proof. If the SDP in (4.23) is feasible for some r, then the set

(cid:40)

(γ, x) ∈ Rn+1| p(x) ≤ γ,

(cid:41)

i ≥ γ2r + 2r
x2

n
(cid:88)

i=1

(4.24)

must be empty. Indeed, if this was not the case, a feasible (γ, x) pair would make the right

hand side of the equality constraint of (4.23) nonnegative, while the left hand side is negative.

As the set in (4.24) is empty, then for all γ, the γ-sublevel set of p is contained within a ball
of radius (cid:112)γ2r + 2r and thus p is coercive.

To show the converse, suppose that p is coercive. Then we know from Theorem 4.4.3

and Remark 4.4.1 that there exist an integer c(cid:48) > 0 and a scalar k(cid:48) ≥ 0 such that the set in

(4.22) is empty. From Stengle’s Positivstellensatz (c.f. Theorem 4.3.7), there exist an even

113

nonnegative integer ˆr and sos polynomials σ(cid:48)

0, . . . , σ(cid:48)

3 of degree at most ˆr such that

−1 =σ(cid:48)

0(x, γ) + σ(cid:48)

1(x, γ)(γ − p(x)) + σ(cid:48)

2(x, γ)

+ σ(cid:48)

3(x, γ)(γ − p(x))

(cid:32) n

(cid:88)

i=1

i − γ2c(cid:48) − k(cid:48)
x2

.

(cid:33)

i − γ2c(cid:48) − k(cid:48)
x2

(cid:32) n

(cid:88)

i=1

(cid:33)

(4.25)

Let r∗ = (cid:100)max{c(cid:48), log2(k(cid:48) + 1), ˆr+d

2 }(cid:101). We show that the SDP in (4.23) is feasible for r = r∗

by showing that the polynomials

σ0(x, γ) = σ(cid:48)

0(x, γ) + σ(cid:48)

2(x, y)(γ2r∗ − γ2c(cid:48) + 2r∗ − k(cid:48)),

σ1(x, γ) = σ(cid:48)

1(x, γ) + σ(cid:48)

3(x, γ)(γ2r∗ − γ2c(cid:48) + 2r∗ − k(cid:48)),

σ2(x, γ) = σ(cid:48)

2(x, γ),

σ3(x, γ) = σ(cid:48)

3(x, γ)

are a feasible solution to the problem. First, note that

4r∗ − d ≥ 2r∗ − d ≥ ˆr ≥ 0,

and so σ0 is of degree at most ˆr+2r∗ ≤ 4r∗, σ1 is of degree at most ˆr + 2r∗ ≤ max{4r∗ − d, 0},

σ2 is of degree at most ˆr ≤ 2r∗, and σ3 is of degree at most ˆr ≤ max{2r∗−d, 0}. Furthermore,

these polynomials are sums of squares. To see this, note that γ2r∗−γ2c(cid:48)+2r∗−k(cid:48) is nonnegative

as r∗ ≥ c(cid:48) and 2r∗ ≥ k(cid:48) + 1. As any nonnegative univariate polynomial is a sum of squares

(see, e.g., [21]), it follows that γ2r∗ − γ2c(cid:48) + 2r∗ − k(cid:48) is a sum of squares. Combining this with

the facts that σ(cid:48)

0, . . . , σ(cid:48)

3 are sums of squares, and products and sums of sos polynomials are

114

sos again, we get that σ0, . . . , σ3 are sos. Finally, the identity

(cid:16)

−1 =

0(x, γ) + σ(cid:48)
σ(cid:48)
(cid:16)

+

1(x, γ) + σ(cid:48)
σ(cid:48)

2(x, y)(γ2r∗ − γ2c(cid:48) + 2r∗ − k(cid:48))

(cid:17)

(cid:17)
3(x, γ)(γ2r∗ − γ2c(cid:48) + 2r∗ − k(cid:48)))(γ − p(x)

n
(cid:88)

+ σ(cid:48)

2(x, γ)(

i − γ2r∗ − 2r∗) + σ(cid:48)
x2

3(x, γ)(γ − p(x))(

n
(cid:88)

i − γ2r∗ − 2r∗)
x2

i=1

i=1

holds by a simple rewriting of (4.25).

As an illustration, we revisit the simple example p(x) = x4

1 + x2

2, whose top homogeneous

component is not positive deﬁnite. The hierarchy in Proposition 4.4.4 with r = 1 gives an

automated algebraic proof of coercivity of p in terms of the following identity:

− 1 =

(cid:18) 2
3

(x2

1 −

1
2

)2 +

2
3

(γ −

(cid:19)

1
2

)2

+

2
3

(γ − x4

1 − x2

2) +

2
3

(x2

1 + x2

2 − γ2 − 2).

(4.26)

Note that this is a certiﬁcate that the γ-sublevel set of p is contained in a ball of radius

(cid:112)γ2 + 2.

Remark 4.4.2. From a theoretical perspective, our developments so far show that coercivity

of multivariate polynomials is a decidable property as it can be checked by solving a ﬁnite

number of SDP feasibility problems (each of which can be done in ﬁnite time [90]). Indeed,

given a polynomial p, one can think of running two programs in parallel. The ﬁrst one solves

the SDPs in Proposition 4.4.4 for increasing values of r. The second uses Proposition 4.4.1

and its degree bound to test whether the β-sublevel set p is compact, starting from β = 1, and

doubling β in each iteration. On every input polynomial p whose coercivity is in question,

either the ﬁrst program halts with a yes answer or the second program halts with a no answer.

We stress that this remark is of theoretical interest only, as the value of our contribution

is really in providing proofs of coercivity, not proofs of non-coercivity. Moreover, coercivity

115

can alternatively be decided in ﬁnite time by applying the quantiﬁer elimination theory of

Tarski and Seidenberg [105, 100] to the characterization in Proposition 4.4.2.

4.5 Summary and Conclusions

We studied the complexity of checking existence of optimal solutions in mathematical pro-

grams (given as minimization problems) that are feasible and lower bounded. We showed

that unless P=NP, this decision problem does not have a polynomial time (or even pseudo-

polynomial time) algorithm when the constraints and the objective function are deﬁned by

polynomials of low degree. More precisely, this claim holds if the constraints are deﬁned by

quadratic polynomials (and the objective has degree as low as one) or if the objective function

is a quartic polynomial (even in absence of any constraints). For polynomial optimization

problems with linear constraints and objective function of degrees 1,2, or 3, previous re-

sults imply that feasibility and lower boundedness always guarantee existence of an optimal

solution.

We also showed, again for low-degree polynomial optimization problems, that several

well-known suﬃcient conditions for existence of optimal solutions are NP-hard to check.

These were coercivity of the objective function, closedness of the feasible set (even when

bounded), boundedness of the feasible set (even when closed), an algebraic certiﬁcate of

compactness known as the Archimedean property, and a robust analogue of compactness

known as stable compactness.

Our negative results should by no means deter researchers from studying algorithms that

can eﬃciently check existence of optimal solutions—or, for that matter, any of the other

properties mentioned above such as compactness and coercivity—on special instances. On

the contrary, our results shed light on the intricacies that can arise when studying these

properties and calibrate the expectations of an algorithm designer. Hopefully, they will

even motivate further research in identifying problem structures (e.g., based on the Newton

116

polytope of the objective and/or constraints) for which checking these properties becomes

more tractable, or eﬃcient algorithms that can test useful suﬃcient conditions that imply

these properties.

In the latter direction, we argued that sum of squares techniques could be a natural tool

for certifying compactness of basic semialgebraic sets via semideﬁnite programming. By

deriving a new characterization of coercive polynomials, we showed that the same statement

also applies to the task of certifying coercivity. This ﬁnal contribution motivates a problem

that we leave for our future research. While coercivity (i.e., boundedness of all sublevel sets)

of a polynomial objective function guarantees existence of optimal solutions to a feasible

POP, the same guarantee can be made from the weaker requirement that some sublevel

set of the objective be bounded and have a non-empty intersection with the feasible set. It

is not diﬃcult to show that this property is also NP-hard to check. However, it would be

useful to derive a hierarchy of suﬃcient conditions for it, where each level can be eﬃciently

tested (perhaps again via SDP), and such that if the property was satisﬁed, then a level of

the hierarchy would hold.

117

Chapter 5

Semideﬁnite Programming

Relaxations for Nash Equilibria in

Bimatrix Games

5.1

Introduction

A bimatrix game is a game between two players (referred to in this chapter as players A

and B) deﬁned by a pair of m × n payoﬀ matrices A and B. Let (cid:52)m and (cid:52)n denote the

m-dimensional and n-dimensional simplices

(cid:52)m = {x ∈ Rm| xi ≥ 0, ∀i,

m
(cid:88)

i=1

xi = 1}, (cid:52)n = {y ∈ Rn| yi ≥ 0, ∀i,

n
(cid:88)

i=1

yi = 1}.

These form the strategy spaces of player A and player B respectively. For a strategy pair

(x, y) ∈ (cid:52)m × (cid:52)n, the payoﬀ received by player A (resp. player B) is xT Ay (resp. xT By).

In particular, if the players pick vertices i and j of their respective simplices (also called

pure strategies), their payoﬀs will be Ai,j and Bi,j. One of the prevailing solution concepts

for bimatrix games is the notion of Nash equilibrium. At such an equilibrium, the players

are playing mutual best responses, i.e., a payoﬀ maximizing strategy against the opposing

118

player’s strategy.

In our notation, a Nash equilibrium for the game (A, B) is a pair of

strategies (x∗, y∗) ∈ (cid:52)m × (cid:52)n such that

and

x∗T Ay∗ ≥ xT Ay∗, ∀x ∈ (cid:52)m,

x∗T By∗ ≥ x∗T By, ∀y ∈ (cid:52)n.1

Nash [77] proved that for any bimatrix game, such pairs of strategies exist (in fact his

result more generally applies to games with a ﬁnite number of players and a ﬁnite number of

pure strategies). While existence of these equilibria is guaranteed, ﬁnding them is believed

to be a computationally intractable problem. More precisely, a result of

[31] implies that

computing Nash equilibria is PPAD-complete (see [31] for a deﬁnition) even when the number

of players is 3. This result was later improved by [24] who showed the same hardness result

for bimatrix games.

These results motivate the notion of an approximate Nash equilibrium, a solution concept

in which players receive payoﬀs “close” to their best response payoﬀs. More precisely, a pair

of strategies (x∗, y∗) ∈ (cid:52)m × (cid:52)n is an (additive) (cid:15)-Nash equilibrium for the game (A, B) if

and

x∗T Ay∗ ≥ xT Ay∗ − (cid:15), ∀x ∈ (cid:52)m,

x∗T By∗ ≥ x∗T By − (cid:15), ∀y ∈ (cid:52)n.2

Note that when (cid:15) = 0, (x∗, y∗) form an exact Nash equilibrium, and hence it is of interest to

ﬁnd (cid:15)-Nash equilibria with (cid:15) small. Unfortunately, approximation of Nash equilibria has also

proved to be computationally diﬃcult. [25] have shown that, unless PPAD ⊆ P, there cannot

1In this chapter we assume that all entries of A and B are between 0 and 1, and argue at the beginning

of Section 5.2 why this is without loss of generality for the purpose of computing Nash equilibria.

2There are also other important notions of approximate Nash equilibria, such as (cid:15)-well-supported Nash

equilibria [37] and relative approximate Nash equilibria [30] which are not considered in this chapter.

119

be a fully polynomial-time approximation scheme for computing Nash equilibria in bimatrix

games. There have, however, been a series of constant factor approximation algorithms for

this problem ([33, 32, 58, 107]), with the current best producing a .3393 approximation via

an algorithm by [107].

We remark that there are exponential-time algorithms for computing Nash equilibria,

such as the Lemke-Howson algorithm ([64, 98]). There are also certain subclasses of the

problem which can be solved in polynomial time, the most notable example being the case

of zero-sum games (i.e. when B = −A). This problem was shown to be solvable via linear

programming by [29], and later shown to be polynomially equivalent to linear programming

by [1]. Aside from computation of Nash equilibria, there are a number of related decision

questions which are of economic interest but unfortunately NP-hard. Examples include

deciding whether a player’s payoﬀ exceeds a certain threshold in some Nash equilibrium,

deciding whether a game has a unique Nash equilibrium, or testing whether there exists a

Nash equilibrium where a particular set of strategies is not played ([40, 27]).

Our focus in this chapter is on understanding the power of semideﬁnite programming3

(SDP) for ﬁnding approximate Nash equilibria in bimatrix games or providing certiﬁcates

for related decision questions. The goal is not to develop a competitive solver, but rather to

analyze the algorithmic power of SDP when applied to basic problems around computation

of Nash equilibria. Semideﬁnite programming relaxations have been analyzed in depth in

areas such as combinatorial optimization ([41], [67]) and systems theory ([23]), but not to

such an extent in game theory. To our knowledge, the appearance of SDP in the game

theory literature includes the work of [102] for exchangeable equilibria in symmetric games,

of [89] on zero-sum polynomial games, of [101] for zero-sum stochastic games, and of [60] for

semialgebraic min-max problems in static and dynamic games.

3The unfamiliar reader is referred to [108] for the theory of SDPs and a description of polynomial-time

algorithms for them based on interior point methods.

120

5.1.1 Organization and Contributions of the chapter

In Section 5.2, we formulate the problem of ﬁnding a Nash equilibrium in a bimatrix game as

a nonconvex quadratically constrained quadratic program and pose a natural SDP relaxation

for it. In Section 5.3, we show that our SDP is exact when the game is strictly competitive

(see Deﬁnition 5.3.3).

In Section 5.4, we design two continuous but nonconvex objective

functions for our SDP whose global minima coincide with rank-1 solutions. We provide a

heuristic based on iterative linearization for minimizing both objective functions. We show

empirically that these approaches produce (cid:15) very close to zero (on average in the order of

10−3). In Section 5.5, we establish a number of bounds on the quality of the approximate

Nash equilibria that can be read oﬀ of feasible solutions to our SDP. In Theorems 5.5.5,

5.5.6, and 5.5.8, we show that when the SDP returns solutions which are “close” to rank-1,

the resulting strategies have have small (cid:15). We then present an improved analysis in the

rank-2 case which shows how one can recover a 5

11-Nash equilibrium from the SDP solution

(Theorem 5.5.10). We further prove that for symmetric games (i.e., when B = AT ), a 1

3-Nash

equilibrium can be recovered in the rank-2 case (Theorem 5.5.17). We do not currently know

of a polynomial-time algorithm for ﬁnding rank-2 solutions to our SDP. If such an algorithm

were found, it would, together with our analysis, improve the best known approximation

bound for symmetric games. In Section 5.6, we show how our SDP formulation can be used

to provide certiﬁcates for certain (NP-hard) questions of economic interest about Nash equi-

libria in symmetric games. These are the problems of testing whether the maximum welfare

achievable under any symmetric Nash equilibrium exceeds some threshold, and whether a

set of strategies is played in every symmetric Nash equilibrium. In Section 5.7, we show that

the SDP analyzed in this chapter dominates the ﬁrst level of the Lasserre hierarchy (Propo-

sition 5.7.1). Some directions for future research are discussed in Section 5.8. The four

appendices of the chapter add some numerical and technical details.

121

5.1.2 Notation

We establish some notation that will be used throughout the chapter. The symbol (cid:52)k

denotes the k-dimensional simplex. For a matrix A, the notation Ai, refers to its i-th

row, and A,j refers to its j-th column. The notation ei refers to the elementary vector

(0, . . . , 0, 1, 0, . . . , 0)T with the 1 being in position i, 0m refers to the m-dimensional vector of

zero’s, 1m refers to the m-dimensional vector of one’s, and Jm×n refers to the m × n matrix

of one’s. The notation A (cid:23) 0 (resp A ≥ 0) denotes that a matrix A is positive semideﬁnite

(resp. elementwise nonnegative), Sk×k denotes the set of symmetric k × k matrices, and

Tr(A) denotes the trace of a matrix A, i.e., the sum of its diagonal elements. For two

matrices A and B, A (cid:23) B denotes that A − B is positive semideﬁnite and A ⊗ B denotes

their Kronecker product. Finally, for a vector v, diag(v) denotes the diagonal matrix with v

on its diagonal. For a square matrix M , diag(M ) denotes the vector containing its diagonal

entries.

5.2 The Formulation of our SDP Relaxation

In this section we present an SDP relaxation for the problem of ﬁnding Nash equilibria in

bimatrix games. This is done after a straightforward reformulation of the problem as a

nonconvex quadratically constrained quadratic program. We also assume that all entries of

the payoﬀ matrices A and B are between 0 and 1. This can be done without loss of generality

because Nash equilibria are invariant under certain aﬃne transformations in the payoﬀs. In

particular, the games (A, B) and (cA + dJm×n, eB + f Jm×n) have the same Nash equilibria

122

for any scalars c, d, e, and f , with c and e positive. This is because

x∗T Ay ≥ xT Ay

⇔ c(x∗T Ay∗) + d ≥ c(xT Ay∗) + d

⇔ c(x∗T Ay∗) + d(x∗T Jm×ny∗) ≥ c(xT Ay∗) + d(xT Jm×ny∗)

⇔ x∗T (cA + dJm×n)y∗ ≥ xT (cA + dJm×n)y

Identical reasoning applies for player B.

5.2.1 Nash Equilibria as Solutions to Quadratic Programs

Recall the deﬁnition of a Nash equilibrium from Section 5.1. An equivalent characterization

is that a strategy pair (x∗, y∗) ∈ (cid:52)m × (cid:52)n is a Nash equilibrium for the game (A, B) if and

only if

x∗T Ay∗ ≥ eT

i Ay∗, ∀i ∈ {1, . . . , m},

x∗T By∗ ≥ x∗T Bei, ∀i ∈ {1, . . . , n}.

(5.1)

The equivalence can be seen by noting that because the payoﬀ from playing any mixed

strategy is a convex combination of payoﬀs from playing pure strategies, there is always a

pure strategy best response to the other player’s strategy.

We now treat the Nash problem as the following quadratic programming (QP) feasibility

problem:

123

min
x∈Rm,y∈Rn

0

subject to xT Ay ≥ eT

i Ay, ∀i ∈ {1, . . . , m},

xT By ≥ xT Bej, ∀j ∈ {1, . . . , n},

xi ≥ 0, ∀i ∈ {1, . . . , m},

yi ≥ 0, ∀j ∈ {1, . . . , n},

m
(cid:88)

i=1
n
(cid:88)

i=1

xi = 1,

yi = 1.

(5.2)

Similarly, a pair of strategies x∗ ∈ (cid:52)m and y∗ ∈ (cid:52)n form an (cid:15)-Nash equilibrium for the

game (A, B) if and only if

x∗T Ay∗ ≥ eT

i Ay∗ − (cid:15), ∀i ∈ {1, . . . , m},

x∗T By∗ ≥ x∗T Bei − (cid:15), ∀i ∈ {1, . . . , n}.

Observe that any pair of simplex vectors (x, y) is an (cid:15)-Nash equilibrium for the game (A, B)

for any (cid:15) that satisﬁes

(cid:15) ≥ max{max

i

i Ay − xT Ay, max
eT

i

xT Bei − xT By}.

We use the following notation throughout the chapter:

· (cid:15)A(x, y) := max

i

eT
i Ay − xT Ay,

· (cid:15)B(x, y) := max

i

xT Bei − xT By,

· (cid:15)(x, y) := max{(cid:15)A(x, y), (cid:15)B(x, y)},

and the function parameters are later omitted if they are clear from the context.

124

5.2.2 SDP Relaxation

The QP formulation in (5.2) lends itself to a natural SDP relaxation. We deﬁne a matrix

M :=






X P

Z Y




 ,

and an augmented matrix

M(cid:48) :=









X P x

Z Y y

x

y

1









,

with X ∈ Sm×m, Z ∈ Rn×m, Y ∈ Sn×n, x ∈ Rm, y ∈ Rn and P = Z T .

The SDP relaxation can then be expressed as

min
M(cid:48)∈Sm+n+1,m+n+1

subject to

0

(SDP1)

(5.3)

(5.4)

(5.5)

(5.6)

(5.7)

(5.8)

(5.9)

Tr(AZ) ≥ eT

i Ay, ∀i ∈ {1, . . . , m},

Tr(BZ) ≥ xT Bej, ∀j ∈ {1, . . . , n},
m
(cid:88)

xi = 1,

i=1
n
(cid:88)

i=1

yi = 1,

M(cid:48) ≥ 0,

M(cid:48)

m+n+1,m+n+1 = 1,

M(cid:48) (cid:23) 0.

125

We refer to the constraints (5.3) and (5.4) as the relaxed Nash constraints and the

constraints (5.5) and (5.6) as the unity constraints. This SDP is motivated by the following

observation.

Proposition 5.2.1. Let M(cid:48) be any rank-1 feasible solution to SDP1. Then the vectors x

and y from its last column constitute a Nash equilibrium for the game (A, B).

Proof. We know that x and y are in the simplex from the constraints (5.5), (5.6), and (5.7).

If the matrix M(cid:48) is rank-1, then it takes the form









xxT xyT x

yxT

yyT

xT

yT

y

1









=









x

y

1


T







.

(5.10)

















x

y

1

Then, from the relaxed Nash constraints we have that

i Ay ≤ Tr(AZ) = Tr(AyxT ) = Tr(xT Ay) = xT Ay,
eT

xT Aei ≤ Tr(BZ) = Tr(ByxT ) = Tr(xT By) = xT By.

The claim now follows from the characterization given in (5.1).

Remark 5.2.1. Because a Nash equilibrium always exists, there will always be a matrix of

the form (5.10) which is feasible to SDP1. Thus we can disregard any concerns about SDP1

being feasible, even when we add valid inequalities to it in Section 5.2.3.

Remark 5.2.2. It is intuitive to note that the submatrix P = Z T of the matrix M(cid:48) corresponds

to a probability distribution over the strategies, and that seeking a rank-1 solution to our

SDP can be interpreted as making P a product distribution.

The following theorem shows that SDP1 is a weak relaxation and stresses the necessity

of additional valid constraints.

126

Theorem 5.2.2. Consider a bimatrix game with payoﬀ matrices bounded in [0, 1]. Then for










x

y

1







any two vectors x ∈ (cid:52)m and y ∈ (cid:52)n, there exists a feasible solution M(cid:48) to SDP1 with

as its last column.

Proof. Consider any x, y, γ > 0, and the matrix

















x

y

1









x

y

1


T







+






γJm+n,m+n 0m+n

0T
m+n

0




 .

This matrix is the sum of two nonnegative psd matrices and is hence nonnegative and psd. By

assumption x and y are in the simplex, and so constraints (5.5) − (5.9) of SDP1 are satisﬁed.

To check that constraints (5.3) and (5.4) hold, note that since A and B are nonnegative, as

long as the matrices A and B are not the zero matrices, the quantities Tr(AZ) and Tr(BZ)

will become arbitrarily large as γ increases. Since eT

i Ay and xT Bei are bounded by 1 by

assumption, we will have that constraints (5.3) and (5.4) hold for γ large enough. In the

case where A or B is the zero matrix, the Nash constraints are trivially satisﬁed for the

respective player.

5.2.3 Valid Inequalities

In this subsection, we introduce a number of valid inequalities to improve upon the SDP

relaxation in SDP1. These inequalities are justiﬁed by being valid if the matrix returned

by the SDP is rank-1. The terminology we introduce here to refer to these constraints is

used throughout the chapter. Constraints (5.11) and (5.12) will be referred to as the row

inequalities, and (5.13) and (5.14) will be referred to as the correlated equilibrium inequalities.

Proposition 5.2.3. Any rank-1 solution M(cid:48) to SDP1 must satisfy the following:

127

m
(cid:88)

j=1

Xi,j =

n
(cid:88)

j=1

Yi,j =

n
(cid:88)

j=1

m
(cid:88)

j=1

Pi,j = xi, ∀i ∈ {1, . . . , m},

Zi,j = yi, ∀i ∈ {1, . . . , n}.

n
(cid:88)

j=1

Ai,jPi,j ≥

m
(cid:88)

j=1

Bj,iPj,i ≥

n
(cid:88)

j=1

m
(cid:88)

j=1

Ak,jPi,j, ∀i, k ∈ {1, . . . , m},

Bj,kPj,i, ∀i, k ∈ {1, . . . , n}.

(5.11)

(5.12)

(5.13)

(5.14)

Proof. Recall from (5.10) that if M(cid:48) is rank-1, it is of the form









xxT xyT x

yxT

yyT

xT

yT

y

1









=









x

y

1










T







.









x

y

1

To show (5.11), observe that

m
(cid:88)

j=1

Xi,j =

m
(cid:88)

j=1

xixj = xi, ∀i ∈ {1, . . . , m}.

An identical argument works for the remaining matrices P, Z, and Y . To show (5.13) and

(5.14), observe that a pair (x, y) is a Nash equilibrium if and only if

∀i, xi > 0 ⇒ eT

i Ay = xT Ay = max

i

eT
i Ay,

∀i, yi > 0 ⇒ xT Bei = xT By = max

i

xT Bei.

This is because the Nash conditions require that xT Ay, a convex combination of eT

i Ay, be

at least eT

i Ay for all i. Indeed, if xi > 0 but eT

i Ay < xT Ay, the convex combination must be

less than max

i

xT Ay.

128

For each i such that xi = 0 or yi = 0, inequalities (5.13) and (5.14) reduce to 0 ≥ 0, so

we only need to consider strategies played with positive probability. Observe that if M(cid:48) is

rank-1, then

n
(cid:88)

j=1

Ai,jPi,j = xi

n
(cid:88)

j=1

Ai,jyj = xieT

i Ay ≥ xieT

k Ay =

m
(cid:88)

j=1

Bj,iPj,i = yi

m
(cid:88)

j=1

Bj,ixj = yixT Bei ≥ yixT Bek =

n
(cid:88)

j=1

Ak,jPi,j, ∀i, k

m
(cid:88)

j=1

Bj,iPj,k, ∀i, k.

Remark 5.2.3. There are two ways to interpret the inequalities in (5.13) and (5.14): the

ﬁrst is as a relaxation of the constraint xi(eT

i Ay − eT

j Ay) ≥ 0, ∀i, j, which must hold since

any strategy played with positive probability must give the best response payoﬀ. The other

interpretation is to have the distribution over outcomes deﬁned by P be a correlated equilib-

rium [11]. This can be imposed by a set of linear constraints on the entries of P as explained

next.

Suppose the players have access to a public randomization device which prescribes a

pure strategy to each of them (unknown to the other player). The distribution over the

assignments can be given by a matrix P , where Pi,j is the probability that strategy i is

assigned to player A and strategy j is assigned to player B. This distribution is a correlated

equilibrium if both players have no incentive to deviate from the strategy prescribed, that

is, if the prescribed pure strategies a and b satisfy

n
(cid:88)

j=1

m
(cid:88)

i=1

Ai,jP rob(b = j|a = i) ≥

Bi,jP rob(a = i|b = j) ≥

n
(cid:88)

j=1

m
(cid:88)

i=1

Ak,jP rob(b = j|a = i),

Bi,kP rob(a = i|b = j).

If we interpret the P submatrix in our SDP as the distribution over the assignments by

the public device, then because of our row constraints, P rob(b = j|a = i) = Pi,j
xi

whenever

129

xi (cid:54)= 0 (otherwise the above inequalities are trivial). Similarly, P (a = i|b = j) = Pi,j
yj

for

nonzero yj. Observe now that the above two inequalities imply (5.13) and (5.14). Finally,

note that every Nash equilibrium generates a correlated equilibrium, since if P is a product

distribution given by xyT , then P rob(b = j|a = i) = yj and P (a = i|b = j) = xi.

Implied Inequalities

In addition to those explicitly mentioned in the previous section, there are other natural

valid inequalities which are omitted because they are implied by the ones we have already

proposed. We give two examples of such inequalities in the next proposition. We refer to

the constraints in (5.15) below as the distribution constraints. The constraints in (5.16) are

the familiar McCormick inequalities [70] for box-constrained quadratic programming.

Proposition 5.2.4. Let z :=

following:






x

y




. Any rank-1 solution M(cid:48) to SDP1 must satisfy the

m
(cid:88)

m
(cid:88)

n
(cid:88)

m
(cid:88)

Zi,j =

n
(cid:88)

n
(cid:88)

Xi,j =

Yi,j = 1.

i=1

j=1

i=1

j=1

i=1

j=1

Mi,j ≤ zi, ∀i, j ∈ {1, . . . , m + n},

Mi,j + 1 ≥ zi + zj, ∀i, j ∈ {1, . . . , m + n}.

(5.15)

(5.16)

Proof. The distribution constraints follow immediately from the row constraints (5.11) and

(5.12), along with the unity constraints (5.5) and (5.6).

The ﬁrst McCormick inequality is immediate as a consequence of (5.11) and (5.12), as all

entries of M are nonnegative. To see why the second inequality holds, consider whichever

submatrix X, Y, P , or Z that contains Mi,j. Suppose that this submatrix is, e.g., P . Then,

since P is nonnegative,

m
(cid:88)

n
(cid:88)

Pk,l

(5.11)
=

m
(cid:88)

0 ≤

k=1,k(cid:54)=i

l=1,l(cid:54)=j

k=1,k(cid:54)=i

(xk − Pk,j)

(5.12)
= (1 − xi) − (yj − Pi,j) = Pi,j + 1 − xi − yj.

130

The same argument holds for the other submatrices, and this concludes the proof.

5.2.4 Simplifying our SDP

We observe that the row constraints (5.11) and (5.12) along with the correlated equilibrium

constraints (5.13) and (5.14) imply the relaxed Nash constraints (5.3) and (5.4). Indeed, if

we ﬁx an index k ∈ {1, . . . , m}, then

Tr(AZ) =

m
(cid:88)

n
(cid:88)

i=1

j=1

Ai,jPi,j

(5.13)
≥

m
(cid:88)

n
(cid:88)

i=1

j=1

Ak,jPi,j ≥

n
(cid:88)

j=1

m
(cid:88)

Ak,j(

(5.12),P =ZT
≥

Pi,j)

i=1

n
(cid:88)

j=1

Ak,jyj = eT

k Ay.

The proof for player B proceeds identically. Then, after collecting the valid inequalities and

removing the relaxed Nash constraints, we arrive at an SDP given by

min
M(cid:48)∈S(m+n+1)×(m+n+1)

0

(SDP1’)

subject to

(5.5) − (5.9), (5.11) − (5.14).

We make the observation that the last row and column of M(cid:48) can be removed from this

SDP, that is, there is a one-to-one correspondence between solutions to SDP1’ and those to

the following SDP (where M :=








X P

Z Y


 , with P = Z T ):

131

min
M∈S(m+n)×(m+n)

0

subject to

M (cid:23) 0,

M ≥ 0,

n
(cid:88)

n
(cid:88)

Pi,j = 1,

i=1
m
(cid:88)

j=1
n
(cid:88)

j=1
n
(cid:88)

j=1
m
(cid:88)

j=1

j=1

n
(cid:88)

Xi,j =

Pi,j, ∀i ∈ {1, . . . , m},

j=1
m
(cid:88)

j=1

Yi,j =

Zi,j, ∀i ∈ {1, . . . , n},

Ai,jPi,j ≥

Bj,iPj,i ≥

n
(cid:88)

j=1
m
(cid:88)

j=1

Ak,jPi,j, ∀i, k ∈ {1, . . . , m},

Bj,kPj,i, ∀i, k ∈ {1, . . . , n}.

(SDP2)

(5.17)

(5.18)

(5.19)

(5.20)

(5.21)

(5.22)

(5.23)

Indeed, it is readily veriﬁed that the submatrix M from any feasible solution M(cid:48) to

SDP1’ is feasible to SDP2. Conversely, let M be any feasible matrix to SDP2. Consider an

eigendecomposition M = (cid:80)k

i=1 λivivT

i and let

. Then the matrix


 := M 1m+n


2






x

y

M(cid:48) :=










(cid:20)

M

(cid:21)

xT

yT










x

y

1











=

k
(cid:88)

i=1

λi












T

vi

vi







1T
m+nvi/2

1T
m+nvi/2




(5.24)

is easily seen to be feasible to SDP1’.

Given any feasible solution M to SDP2, observe that the submatrix P is a correlated

equilibrium. We take our candidate approximate Nash equilibrium to be the pair x = P 1n

132

and y = P T 1m. If the correlated equilibrium P is rank-1, then the pair (x, y) so deﬁned

constitutes an exact Nash equilibrium. In Section 5.4, we will add certain objective functions

to SDP2 with the interpretation of searching for low-rank correlated equilibria.

5.3 Exactness for Strictly Competitive Games

In this section, we show that SDP1 recovers a Nash equilibrium for any zero-sum game, and

that SDP2 recovers a Nash equilibrium for any strictly competitive game (see Deﬁnition 5.3.3

below). Both these notions represent games where the two players are in direct competition,

but strictly competitive games are more general, and for example, allow both players to

have nonnegative payoﬀ matrices. These classes of games are solvable in polynomial time

via linear programming. Nonetheless, it is reassuring to know that our SDPs recover these

important special cases.

Deﬁnition 5.3.1. A zero-sum game is a game in which the payoﬀ matrices satisfy A = −B.

Theorem 5.3.2. For a zero-sum game, the vectors x and y from the last column of any

feasible solution M(cid:48) to SDP1 constitute a Nash equilibrium.

Proof. Recall that the relaxed Nash constraints (5.3) and (5.4) read

Tr(AZ) ≥ eT

i Ay, ∀i ∈ {1, . . . , m},

Tr(BZ) ≥ xT Bej, ∀j ∈ {1, . . . , n}.

Since B = −A, the latter statement is equivalent to

Tr(AZ) ≤ xT Aej, ∀j ∈ {1, . . . , n}.

133

In conjunction these imply

i Ay ≤ Tr(AZ) ≤ xT Aej, ∀i ∈ {1, . . . , m}, j ∈ {1, . . . , n}.
eT

(5.25)

We claim that any pair x ∈ (cid:52)m and y ∈ (cid:52)n which satisﬁes the above condition is a Nash

equilibrium. To see that xT Ay ≥ eT

i Ay, ∀i ∈ {1, . . . , m}, observe that xT Ay is a convex

combination of xT Aej, which are at least eT

i Ay by (5.25). To see that xT By ≥ xT Bej ⇔

xT Ay ≤ xT Aej, ∀j ∈ {1, . . . , n}, observe that xT Ay is a convex combination of eT

i Ay, which

are at most xT Aej by (5.25).

Deﬁnition 5.3.3. A game (A, B) is strictly competitive if for all x, x(cid:48) ∈ (cid:52)m, y, y(cid:48) ∈ (cid:52)n,

xT Ay − x(cid:48)T Ay(cid:48) and x(cid:48)T By(cid:48) − xT By have the same sign.

The interpretation of this deﬁnition is that if one player beneﬁts from changing from

one outcome to another, the other player must suﬀer. Adler, Daskalakis, and Papadimitriou

show in [2] that the following much simpler characterization is equivalent.

Theorem 5.3.4 (Theorem 1 of [2]). A game is strictly competitive if and only if there exist

scalars c, d, e, and f, with c > 0, e > 0, such that cA + dJm×n = −eB + f Jm×n.

One can easily show that there exist strictly competitive games for which not all feasible

solutions to SDP1 have Nash equilibria as their last columns (see Theorem 5.2.2). However,

we show that this is the case for SDP2.

Theorem 5.3.5. For a strictly competitive game, the vectors x := P 1n and y := P T 1m from

any feasible solution M to SDP2 constitute a Nash equilibrium.

To prove Theorem 5.3.5 we need the following lemma, which shows that feasibility of a

matrix M in SDP2 is invariant under certain transformations of A and B.

134

Lemma 5.3.6. Let c, d, e, and f be any set of scalars with c > 0 and e > 0. If a matrix M

is feasible to SDP2 with input payoﬀ matrices A and B, then it is also feasible to SDP2 with

input matrices cA + dJm×n and eB + f Jm×n.

Proof. It suﬃces to check that constraints (5.22) and (5.23) of SDP2 still hold, as only the

correlated equilibrium constraints use the matrices A and B. We only show that constraint

(5.22) still holds because the argument for constraint (5.23) is identical.
Note from the deﬁnition of x that for each i ∈ {1, . . . , m}, xi = (cid:80)n

j=1(Jm×n)i,jPi,j. To

check that the correlated equilibrium constraints hold, observe that for scalars c > 0, d, and

for all i, k ∈ {1, . . . , m},

Ai,jPi,j ≥

n
(cid:88)

Ak,jPi,j

n
(cid:88)

j=1

j=1
n
(cid:88)

Pi,j ≥ c

Ak,jPi,j + d

⇔ c

n
(cid:88)

j=1

Ai,jPi,j + d

n
(cid:88)

j=1

Ai,jPi,j + d

n
(cid:88)

(Jm×n)i,jPi,j ≥ c

j=1
n
(cid:88)

Ak,jPi,j + d

j=1

(cAi,j + dJm×n)k,jPi,j ≥

j=1
n
(cid:88)

⇔

n
(cid:88)

j=1

(cAi,j + dJm×n)k,jPi,j.

j=1

n
(cid:88)

j=1
n
(cid:88)

j=1

Pi,j

(Jm×n)k,jPi,j

⇔ c

n
(cid:88)

j=1

Proof. Proof (of Theorem 5.3.5). Let A and B be the payoﬀ matrices of the given

strictly competitive game and let M be a feasible solution to SDP2. Since the game is

strictly competitive, we know from Theorem 5.3.4 that cA + dJm×n = −eB + f Jm×n for
some scalars c > 0, e > 0, d, f . Consider a new game with input matrices ˜A = cA + dJm×n
and ˜B = eB − f Jm×n. By Lemma 5.3.6, M is still feasible to SDP2 with input matrices ˜A







and ˜B. By the arguments in Section 5.2.4, the matrix M(cid:48) :=








(cid:20)

M

(cid:21)

xT

yT

to SDP1’, and hence also to SDP1. Now notice that since ˜A = − ˜B, Theorem 5.3.2 implies

135






x

y

1











is feasible

that the vectors x and y in the last column form a Nash equilibrium to the game ( ˜A, ˜B).

Finally recall from the arguments at the beginning of Section 5.2 that Nash equilibria are

invariant to scaling and shifting of the payoﬀ matrices, and hence (x, y) is a Nash equilibrium

to the game (A, B).

5.4 Algorithms for Lowering Rank

In this section, we present heuristics which aim to ﬁnd low-rank solutions to SDP2 and

present some empirical results. Recall that our SDP2 in Section 5.2.4 did not have an

objective function. Hence, we can encourage low-rank solutions by choosing certain objective

functions, in particular the trace of the matrix M, which is a general heuristic for minimizing

the rank of symmetric matrices [96, 36]. This simple objective function is already guaranteed

to produce a rank-1 solution in the case of strictly competitive games (see Proposition 5.4.1

below). For general games, however, one can design better objective functions in an iterative

fashion (see Section 5.4.1).

Notational Remark: For the remainder of this section, we will use the shorthand x := P 1n

and y := P T 1m, where P is the upper right submatrix of a feasible solution M to SDP2.

Proposition 5.4.1. For a strictly competitive game, any optimal solution to SDP2 with

Tr(M) as the objective function must be rank-1.

Proof. Let



M :=




X P

P T Y






be a feasible solution to SDP2.

In the case of strictly competitive games, from Theo-

rem 5.3.5 we know that that (x, y) is a Nash equilibrium. Then because the matrix M

is psd, from (5.24) and an application of the Schur complement (see, e.g. [22, Sect. A.5.5])

136










(cid:20)

to

M

(cid:21)

xT

yT










x

y

1











, we have that M (cid:23)








T




x

y







x

y




. Hence, M =






xxT xyT

yxT

yyT




 + P for

some psd matrix P and the Nash equilibrium (x, y). Given this expression, the objective

function Tr(M) is then xT x + yT y + Tr(P). As (x, y) is a Nash equilibrium, the choice of

P = 0 results in a feasible solution. Since the zero matrix has the minimum possible trace

among all psd matrices, the solution will be the rank-1 matrix











T

x

y







x

y




.

Remark 5.4.1. If the row constraints and the nonnegativity constraints on X and Y are

removed from SDP2, then this SDP with Tr(M) as the objective function can be interpreted

as searching for a minimum-rank correlated equilibrium P via the nuclear norm relaxation;

see [96, Section 2].

5.4.1 Linearization Algorithms

The algorithms we present in this section for minimzing the rank of the matrix M in SDP2

are based on iterative linearization of certain nonconvex objective functions. Motivated

by the next proposition, we design two continuous (nonconvex) objective functions that, if

minimized exactly, would guarantee rank-1 solutions. We will then linearize these functions

iteratively.

Proposition 5.4.2. Let the matrices X and Y and vectors x := P 1n and y := P T 1m be

taken from a feasible solution to SDP2. Then the matrix M is rank-1 if and only if Xi,i = x2
i

and Yi,i = y2

i for all i.

Proof. Note that if M is rank-1, then it can be written as zzT for some z ∈ Rm+n. The i-th

diagonal entry in the X submatrix will then be equal to

(5.15)
=

z2
i

1
4

i (1T
z2

m+nzzT 1m+n) = (

1
2

Mi,1m+n)2 (5.11)

= (Pi,1n)2 = x2
i ,

137

where the second equality holds because Mi,—the i-th row of M—is zizT . An analogous

statement holds for the diagonal entries of Y , and hence the condition is necessary.

To show suﬃciency, let z :=


. Since M is psd, we have that Mi,j ≤ (cid:112)Mi,iMj,j,







x

y

which implies Mi,j ≤ zizj by the assumption of the proposition. Recall from the distribution
constraint (5.15) that (cid:80)m+n
deﬁnitions of x and y imply that (cid:80)m+n

j=1 Mi,j = 4. Further, the same constraint along with the

i=1 zi = 2, which means that (cid:80)m+n

j=1 zizj = 4.

(cid:80)m+n

(cid:80)m+n

i=1

i=1

Hence in order to have the equality

m+n
(cid:88)

m+n
(cid:88)

4 =

Mi,j ≤

m+n
(cid:88)

m+n
(cid:88)

zizj = 4,

i=1

j=1

i=1

j=1

we must have Mi,j = zizj for each i and j. Consequently M is rank-1.

We focus now on two nonconvex objectives that as a consequence of the above proposition

would return rank-1 solutions:

Proposition 5.4.3. All optimal solutions to SDP2 with the objective function (cid:80)m+n

i=1

(cid:112)Mi,i

or Tr(M) − xT x − yT y are rank-1.

Proof. We show that each of these objectives has a speciﬁc lower bound which is achieved if

and only if the matrix is rank-1.






Observe that since M (cid:23)




x

y







x

y


T




, we have (cid:112)Xi,i ≥ xi and (cid:112)Yi,i ≥ yi, and hence

m+n
(cid:88)

i=1

(cid:112)Mi,i ≥

m
(cid:88)

i=1

xi +

n
(cid:88)

i=1

yi = 2.

Further note that






Tr(M) −



T 







T 







T 






x

y


 −




x

y







x

y


 = 0.

x

y







x

y


 ≥




x

y




138

We can see that the lower bounds are achieved if and only if Xi,i = x2

i and Yi,i = y2

i for

all i, which by Proposition 5.4.2 happens if and only if M is rank-1.

We refer to our two objective functions in Proposition 5.4.3 as the “square root objec-

tive” and the “diagonal gap objective” respectively. While these are both nonconvex, we

will attempt to iteratively minimize them by linearizing them through a ﬁrst order Taylor

expansion. For example, at iteration k of the algorithm,

(cid:113)

M(k)

i,i (cid:39)

m+n
(cid:88)

i=1

(cid:113)

M(k−1)

i,i +

m+n
(cid:88)

i=1

1

(cid:113)
2

M(k−1)
i,i

(M(k)

i,i − M(k−1)

i,i

).

Note that for the purposes of minimization, this reduces to minimizing (cid:80)m+n

i=1

(cid:113)

1
M(k−1)
i,i

M(k)
i,i .

In similar fashion, for the second objective function, at iteration k we can make the

approximation





(k)T 



(k)





(k−1)T 



(k−1)T

Tr(M) −




x

y







x

y




(cid:39) Tr(M) −




x

y







x

y









− 2



(k−1)T





(k)





(k−1)

x

y





(


x

y




−




x

y




).

Once again,
(k−1)T 




for the purposes of minimization this reduces to minimizing Tr(M) −


(k)

2




x

y







x

y




. This approach then leads to the following two algorithms.4

4An algorithm similar to Algorithm 4 is used in [52].

139

Algorithm 3 Square Root Minimization Algorithm
1: Let x(0) = 1m, y(0) = 1n, k = 1.

2: while !convergence do

Solve SDP2 with (cid:80)m

i=1

(cid:113)

1
x(k−1)
i

Xi,i + (cid:80)n

i=1

(cid:113)

1
y(k−1)
i

Yi,i as the objective, and let M∗ be

an optimal solution.

Let x(k) = diag(X ∗), y(k) = diag(Y ∗).

Let k = k + 1.

3:

4:

5:

6: end while

Algorithm 4 Diagonal Gap Minimization Algorithm
1: Let x(0) = 0m, y(0) = 0n, k = 1.

2: while !convergence do





(k−1)T 



(k)




x

y







x

y




as the objective, and let M∗

3:

Solve SDP2 with Tr(X) + Tr(Y ) − 2

be an optimal solution.

4:

5:

Let x(k) = P ∗1n, y(k) = P ∗T 1m.

Let k = k + 1.

6: end while

Remark 5.4.2. Note that the ﬁrst iteration of both algorithms uses the nuclear norm (i.e.

trace) of M as the objective.

The square root algorithm has the following property.

Theorem 5.4.4. Let M(1), M(2), . . . be the sequence of optimal matrices obtained from the

square root algorithm. Then the sequence

(cid:113)

M(k)
i,i }

m+n
(cid:88)

{

i=1

(5.26)

is nonincreasing and is lower bounded by two. If it reaches two at some iteration t, then the

matrix M(t) is rank-1.

140

Proof. Observe that for any k > 1,

(cid:113)

M(k)

i,i ≤

m+n
(cid:88)

i=1

1
2

m+n
(cid:88)
(

i=1

M(k)
i,i
M(k−1)
i,i

(cid:113)

(cid:113)

+

M(k−1)
i,i

) ≤

1
2

m+n
(cid:88)
(

i=1

M(k−1)
i,i
(cid:113)
M(k−1)
i,i

(cid:113)

+

M(k−1)
i,i

) =

m+n
(cid:88)

i=1

(cid:113)

M(k−1)
i,i

,

where the ﬁrst inequality follows from the arithmetic-mean-geometric-mean inequality, and

the second follows from that M(k)
i,i

is chosen to minimize (cid:80)m+n

i=1

M(k)
i,i
M(k−1)
i,i

(cid:113)

and hence achieves

a no larger value than the feasible solution M(k−1). This shows that the sequence is nonin-

creasing.

The proof of Proposition 5.4.3 already shows that the sequence is lower bounded by

two, and Proposition 5.4.3 itself shows that reaching two is suﬃcient to have the matrix be

rank-1.

The diagonal gap algorithm has the following property.

Theorem 5.4.5. Let M(1), M(2), . . . be the sequence of optimal matrices obtained from the

diagonal gap algorithm. Then the sequence

{Tr(M(k)) −








(k)T 



(k)

x

y







x

y




}

(5.27)

is nonincreasing and is lower bounded by zero. If it reaches zero at some iteration t, then

the matrix M(t) is rank-1.

Proof. Observe that





(k)T 



(k)

Tr(M(k)) −




x

y







x

y








(k)T 



(k)







(k)



(k−1)



T 





(k)

≤Tr(M(k)) −




x

y







x

y




+








x

y















x

y




−




x

y




−

141






(k−1)



x

y












(k)T 



(k−1)





(k−1)T 

= Tr(M(k)) − 2




x

y







x

y




+




x

y







x

y

(k−1)










(k−1)T 



(k−1)





(k−1)T 



(k−1)

≤ Tr(M(k−1)) − 2




x

y







x

y




+




x

y







x

y




= Tr(M(k−1)) −








(k−1)T 



(k−1)

x

y







x

y




,

where the second inequality follows from that M(k) is chosen to minimize





(k−1)T 



(k−1)

Tr(M(k−1)) − 2




x

y







x

y




and hence achieves a no larger value than the feasible solution M(k−1). This shows that the

sequence is nonincreasing.

The proof of Proposition 5.4.3 already shows that the sequence is lower bounded by

zero, and Proposition 5.4.3 itself shows that reaching zero is suﬃcient to have the matrix be

rank-1.

We also invite the reader to also see Theorem 5.5.6 in the next section which relates the

objective value of the diagonal gap minimization algorithm and the quality of approximate

Nash equilibria that the algorithm produces.

5.4.2 Numerical Experiments

We tested Algorithms 3 and 4 on games coming from 100 randomly generated payoﬀ matrices

with entries bounded in [0, 1] of varying sizes. Below is a table of statistics for 20 × 20

142

matrices; the data for the rest of the sizes can be found in Appendix A.1.5 We can see that

our algorithms return approximate Nash equilibria with fairly low (cid:15) (recall the deﬁnition

from Section 5.2.1). We ran 20 iterations of each algorithm on each game. Using the SDP

solver of [73], each iteration takes on average under 4 seconds to solve on a standard personal

machine with a 3.4 GHz processor and 16 GB of memory.

Table 5.1: Statistics on (cid:15) for 20 × 20 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0198

0.0046

0.0039

0.0034

Diagonal Gap 0.0159

0.0032

0.0024

0.0032

The histograms below show the eﬀect of increasing the number of iterations on lowering (cid:15)

on 20 × 20 games. For both algorithms, there was a clear improvement of the (cid:15) by increasing

the number of iterations.

Figure 5.1: Distribution of (cid:15) over numbers of iterations for the square root algorithm (left)

and the diagonal gap algorithm (right).

5The

code

and

instance

data

that

https://github.com/jeffreyzhang92/SDP Nash.
Nash equilibrium using one of our two algorithms as speciﬁed by the user.

produced

these

at
The function nash.m computes an approximate

available

publicly

results

is

143

5.5 Bounds on (cid:15) for General Games

Since the problem of computing a Nash equilibrium to an arbitrary bimatrix game is PPAD-

complete, it is unlikely that one can ﬁnd rank-1 solutions to this SDP in polynomial time.

In Section 5.4, we designed objective functions (such as variations of the nuclear norm) that

empirically do very well in ﬁnding low-rank solutions to SDP2. Nevertheless, it is of interest

to know if the solution returned by SDP2 is not rank-1, whether one can recover an (cid:15)-Nash

equilibrium from it and have a guarantee on (cid:15). Our goal in this section is to study this

question.

Notational Remark: Recall our notation for the matrix

M :=






X P

Z Y




 .

Throughout this section, any matrices X, Z, P = Z T and Y are assumed to be taken from a

feasible solution to SDP2. Furthermore, x and y will be P 1n and P T 1m respectively.

The ultimate results of this section are the theorems in Sections 5.5.2 and 5.5.3. To work

towards them, we need a number of preliminary lemmas which we present in Section 5.5.1.

5.5.1 Lemmas Towards Bounds on (cid:15)

We ﬁrst observe the following connection between the approximate payoﬀs Tr(AZ) and

Tr(BZ), and (cid:15)(x, y), as deﬁned in Section 5.2.1.

Lemma 5.5.1. Consider any feasible solution to SDP2. Then

(cid:15)(x, y) ≤ max{Tr(AZ) − xT Ay, Tr(BZ) − xT By}.

Proof. Recall from the argument at the beginning of Section 5.2.4 that constraints (5.13)

and (5.14) imply Tr(AZ) ≥ eT

i Ay and Tr(BZ) ≥ xT Bei for all i. Hence, we have

(cid:15)A ≤ Tr(AZ) − xT Ay and (cid:15)B ≤ Tr(BZ) − xT By.

144

We thus are interested in the diﬀerence of the two matrices P = Z T and xyT . These

two matrices can be interpreted as two diﬀerent probability distributions over the strategy

outcomes. The matrix P is the probability distribution from the SDP which generates

the approximate payoﬀs Tr(AZ) and Tr(BZ), while xyT is the product distribution that

would have resulted if the matrix had been rank-1. We will see that the diﬀerence of these

distributions is key in studying the (cid:15) which results from SDP2. Hence, we ﬁrst take steps to

represent this diﬀerence.

Lemma 5.5.2. Consider any feasible matrix M to SDP2 with an eigendecomposition

M =

k
(cid:88)

i=1

λivivT

i =:

k
(cid:88)

i=1

λi








T




ai

bi







ai

bi




,

(5.28)

so that the eigenvectors vi ∈ Rm+n are partitioned into vectors ai ∈ Rm and bi ∈ Rn. Then
for all i, (cid:80)m

j=1(ai)j = (cid:80)n

j=1(bi)j.

Proof. We know from (5.19), (5.20), and (5.21) that

k
(cid:88)

i=1

λi1T

maiaT

i 1m

(5.19),(5.20)
=

1,

k
(cid:88)

i=1
k
(cid:88)

i=1

λi1T

maibT

i 1n

λi1T

n biaT

i 1m

(5.19)

= 1,

(5.19)

= 1,

k
(cid:88)

i=1

λi1T

n bibT

i 1n

(5.19),(5.21)
=

1.

Then by subtracting terms we have

(5.29) − (5.30) =

k
(cid:88)

λi1T

mai(aT

i 1m − bT

i 1n) = 0,

(5.31) − (5.32) =

i=1
k
(cid:88)

i=1

λi1T

n bi(aT

i 1m − bT

i 1n) = 0.

145

(5.29)

(5.30)

(5.31)

(5.32)

(5.33)

(5.34)

By subtracting again these imply

(5.33) − (5.34) =

k
(cid:88)

i=1

λi(1T

mai − 1T

n bi)2 = 0.

(5.35)

As all λi are nonnegative due to positive semideﬁniteness of M, the only way for this equality

to hold is to have 1T

mai = 1T

n bi, ∀i. This is equivalent to the statement of the claim.

From Lemma 5.5.2, we can let si := (cid:80)m

j=1(ai)j = (cid:80)n

j=1(bi)j, and furthermore we assume

without loss of generality that each si is nonnegative. Note that from the deﬁnition of x we

xi =

m
(cid:88)

j=1

Pij =

k
(cid:88)

m
(cid:88)

l=1

j=1

λl(al)i(bl)j =

k
(cid:88)

j=1

λjsj(al)i.

(5.36)

have

Hence,

Similarly,

x =

y =

k
(cid:88)

i=1

k
(cid:88)

i=1

λisiai.

λisibi.

Finally note from the distribution constraint (5.15) that this implies

k
(cid:88)

i=1

λis2

i = 1.

Lemma 5.5.3. Let

M =

k
(cid:88)

i=1

λi








T




ai

bi







ai

bi




,

be a feasible solution to SDP2, such that the eigenvectors of M are partitioned into ai and
bi with (cid:80)m

j=1(bi)j = si, ∀i. Then

j=1(ai)j = (cid:80)n

P − xyT =

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλj(sjai − siaj)(sjbi − sibj)T .

146

(5.37)

(5.38)

(5.39)

Proof. Using equations (5.37) and (5.38) we can write

P − xyT =

=

k
(cid:88)

i=1

k
(cid:88)

i=1

λiaibT

i − (

k
(cid:88)

i=1

k
(cid:88)

λisiai)(

λjsjbj)T

j=1

λiai(bi − si

k
(cid:88)

j=1

λjsjbj)T

(5.39)
=

k
(cid:88)

i=1

k
(cid:88)

λiai(

j=1

λjs2

j bi − si

k
(cid:88)

j=1

λjsjbj)T

=

=

k
(cid:88)

k
(cid:88)

i=1

j=1

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλjaisj(sjbi − sibj)T

λiλj(sjai − siaj)(sjbi − sibj)T ,

where the last line follows from observing that terms where i and j are switched can be

combined.

We can relate (cid:15) and P − xyT with the following lemma.

Lemma 5.5.4. Let the matrix P and the vectors x := P 1n and y := P T 1m come from any

feasible solution to SDP 2. Then

(cid:15) ≤

(cid:107)P − xyT (cid:107)1
2

,

where (cid:107) · (cid:107)1 here denotes the entrywise L-1 norm, i.e., the sum of the absolute values of the

entries of the matrix.

Proof. Let D := P − xyT . From Lemma 5.5.1,

(cid:15)A ≤ Tr(AZ) − xT Ay = Tr(A(Z − yxT )).

147

If we then hold D ﬁxed and restrict that A has entries bounded in [0,1], the quantity Tr(ADT )

is maximized when

Ai,j =






1 Di,j ≥ 0

.

0 Di,j < 0

The resulting quantity Tr(ADT ) will then be the sum of all nonnegative elements of D. Since

the sum of all elements in D is zero, this quantity will be equal to 1

2(cid:107)D(cid:107)1.

The proof for (cid:15)B is identical, and the result follows from that (cid:15) is the maximum of (cid:15)A and

(cid:15)B.

5.5.2 Bounds on (cid:15)

We provide a number of bounds on (cid:15)(x, y)for x := P 1n and y := P T 1m coming from any

feasible solution to SDP2. Our ﬁrst two theorems roughly state that solutions which are

“close” to rank-1 provide small (cid:15).

Theorem 5.5.5. Consider any feasible solution M to SDP2. Suppose M is rank-k and its

eigenvalues are λ1 ≥ λ2 ≥ ... ≥ λk > 0. Then x and y constitute an (cid:15)-NE to the game (A, B)

with (cid:15) ≤ m+n

2

(cid:80)k

i=2 λi.

Proof. By the Perron Frobenius theorem (see e.g. [71, Chapter 8.3]), the eigenvector corre-

sponding to λ1 can be assumed to be nonnegative, and hence

s1 = (cid:107)a1(cid:107)1 = (cid:107)b1(cid:107)1.

(5.40)





We further note that for all i, since

1, we must have




ai

bi


 is a vector of length m + n with 2-norm equal to






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ai

bi

√

≤

m + n.

(5.41)






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

148

Since si is the sum of the elements of ai and bi, we know that

This then gives us

si ≤ min{(cid:107)ai(cid:107)1, (cid:107)bi(cid:107)1} ≤

√

m + n
2

.

s2
i ≤ (cid:107)ai(cid:107)1(cid:107)bi(cid:107)1 ≤

m + n
4

,

(5.42)

(5.43)

with the ﬁrst inequality following from (5.42) and the second from (5.41). Finally note that

a consequence of the nonnegativity of (cid:107) · (cid:107)1 and (5.41) is that for all i, j,

(cid:107)ai(cid:107)1(cid:107)bj(cid:107)1 + (cid:107)bi(cid:107)1(cid:107)aj(cid:107)1 ≤ ((cid:107)ai(cid:107)1 + (cid:107)bi(cid:107)1)((cid:107)aj(cid:107)1 + (cid:107)bj(cid:107)1) =






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ai

bi






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

aj

bj






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

(5.41)

≤ m + n.

(5.44)

Now we let D := P − xyT and upper bound 1

2(cid:107)D(cid:107)1 using Lemma 5.5.3.

1
2

(cid:107)D(cid:107)1 =

≤

≤

≤

1
2

1
2

1
2

1
2

k
(cid:88)

k
(cid:88)

(cid:107)

i=1

j>i

λiλj(sjai − siaj)(sjbi − sibj)T (cid:107)1

k
(cid:88)

k
(cid:88)

i=1

j>i

k
(cid:88)

k
(cid:88)

i=1

j>i

k
(cid:88)

k
(cid:88)

i=1

j>i

(cid:107)λiλj(sjai − siaj)(sjbi − sibj)T (cid:107)1

λiλj(cid:107)sjai − siaj(cid:107)1(cid:107)sjbi − sibj(cid:107)1

λiλj(sj(cid:107)ai(cid:107)1 + si(cid:107)aj(cid:107)1)(sj(cid:107)bi(cid:107)1 + si(cid:107)bj(cid:107)1)

(5.45)

(5.40),(5.43)
≤

1
2

k
(cid:88)

j=2

λ1s2

1λj(sj + (cid:107)aj(cid:107)1)(sj + (cid:107)bj(cid:107)1)

+

1
2

k
(cid:88)

k
(cid:88)

i=2

j>i

λiλj(s2
j

m + n
4

+ s2
i

m + n
4

(5.41),(5.44),(5.42)
≤

m + n
2

λ1s2
1

k
(cid:88)

i=2

λi

+ sisj(cid:107)ai(cid:107)1(cid:107)bj(cid:107)1 + sisj(cid:107)aj(cid:107)1(cid:107)bi(cid:107)1)

+

1
2

k
(cid:88)

k
(cid:88)

i=2

j>i

λiλj

m + n
4

(s2

i + s2

j ) + λiλjsisj(m + n)

149

AMGM6
≤

m + n
2

λ1s2
1

k
(cid:88)

i=2

λi +

m + n
2

k
(cid:88)

k
(cid:88)

i=2

j>i

λiλj(

i + s2
s2
j
4

+

i + s2
s2
j
2

)

=

=

=

m + n
2

λ1s2
1

m + n
2

λ1s2
1

m + n
2

λ1s2
1

≤

m + n
2

λ1s2
1

k
(cid:88)

i=2

k
(cid:88)

i=2

k
(cid:88)

i=2

k
(cid:88)

i=2

λi +

3(m + n)
8

k
(cid:88)

k
(cid:88)

i=2

j>i

λiλj(s2

i + s2
j )

λi +

3(m + n)
8

(

λi +

3(m + n)
8

(

k
(cid:88)

i=2

k
(cid:88)

λis2
i

k
(cid:88)

j>i

λj +

k
(cid:88)

λi

k
(cid:88)

i=2

j>i

λjs2
j )

k
(cid:88)

λj

λis2

i +

k
(cid:88)

λi

k
(cid:88)

i=2

j>i

λjs2
j )

j=2

2≤i<j

λi +

3(m + n)
8

k
(cid:88)
(

j=2

λjs2
j )

k
(cid:88)

i=2

λi

(5.39)
=

m + n
2

λ1s2
1

k
(cid:88)

i=2

λi +

3(m + n)
8

(1 − λ1s2
1)

k
(cid:88)

i=2

λi

=

m + n
8

(3 + λ1s2
1)

k
(cid:88)

i=2

λi

(5.39)
≤

m + n
2

k
(cid:88)

i=2

λi.

The following theorem quantiﬁes how making the objective of the diagonal gap algorithm

from Section 5.4 small makes (cid:15) small. The proof is similar to the proof of Theorem 5.5.5.

Theorem 5.5.6. Let M be a feasible solution to SDP2. Then, x and y constitute an (cid:15)-NE

to the game (A, B) with (cid:15) ≤ 3(m+n)

8

(Tr(M) − xT x − yT y).

Proof. Let M be rank-k with eigenvalues λ1 ≥ λ2 ≥ . . . ≥ λk > 0 and eigenvectors v1, . . . , vk

 with (cid:80)m


partitioned as in Lemma 5.5.2 so that vi =

j=1(bi)j for i = 1, . . . , k.

j=1(ai)j = (cid:80)n




ai



bi

Let si := (cid:80)m

j=1(ai)j. Then we have Tr(M) = (cid:80)k

i=1 λi, and

xT x + yT y

(5.37),(5.38)
=

k
(cid:88)
(

i=1

k
(cid:88)

λisivi)T (

λisivi) =

i=1

k
(cid:88)

i=1

i s2
λ2
i .

(5.46)

6AMGM is used to denote the arithmetic-mean-geometric-mean inequality.

150

We now get the following chain of inequalities (the ﬁrst one follows from Lemma 5.5.4 and

inequality (5.45)):

(cid:15) ≤

1
2

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλj(sj(cid:107)ai(cid:107)1 + si(cid:107)aj(cid:107)1)(sj(cid:107)bi(cid:107)1 + si(cid:107)bj(cid:107)1)

(5.40),(5.43)
≤

1
2

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλj(s2
j

m + n
4

+ s2
i

m + n
4

+ sisj(cid:107)ai(cid:107)1(cid:107)bj(cid:107)1 + sisj(cid:107)aj(cid:107)1(cid:107)bi(cid:107)1)

(5.44)
≤

1
2

AM GM
≤

k
(cid:88)

k
(cid:88)

i=1

j>i

m + n
2

λiλj

m + n
4

(s2

i + s2

j ) + λiλjsisj(m + n)

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλj(

i + s2
s2
j
4

+

i + s2
s2
j
2

)

=

=

=

3(m + n)
8

k
(cid:88)

k
(cid:88)

i=1

j>i

λiλj(s2

i + s2
j )

3(m + n)
8

3(m + n)
8

k
(cid:88)
(

i=1

k
(cid:88)
(

λis2
i

k
(cid:88)

j>i

λj +

k
(cid:88)

λi

k
(cid:88)

i=1

j>i

λjs2
j )

k
(cid:88)

λj

λis2

i +

k
(cid:88)

λi

k
(cid:88)

i=1

j>i

λjs2
j )

j=1

1≤i<j

=

3(m + n)
8

k
(cid:88)
(

λi

(cid:88)

i=1

j(cid:54)=i

λjs2
j )

(5.39)
=

3(m + n)
8

k
(cid:88)
(

i=1

λi(1 − λis2

i ))

=

3(m + n)
8

k
(cid:88)
(

i=1

λi −

k
(cid:88)

i=1

i s2
λ2
i )

(5.46)
=

3(m + n)
8

(Tr(M) − xT x − yT y).

We now give a bound on (cid:15) which is dependent on the nonnegative rank of the matrix

returned by SDP2. Our analysis will also be useful for the next subsection. To begin, we

ﬁrst recall the deﬁnition of the nonnegative rank.

Deﬁnition 5.5.7. The nonnegative rank of a (nonnegative) m × n matrix M is the smallest

k for which there exist a nonnegative m × k matrix U and a nonnegative n × k matrix V

151

such that M = U V T . Such a decomposition is called a nonnegative matrix factorization of

M .

Theorem 5.5.8. Consider the matrix P from any feasible solution to SDP2. Suppose its

nonnegative rank is k. Then x := P 1n and y := P T 1m constitute an (cid:15)-NE to the game (A, B)

with (cid:15) ≤ 1 − 1
k .

Proof. Since P has nonnegative rank k and its entries sum up to 1, we can write
P = (cid:80)k

i , where ai ∈ (cid:52)m, bi ∈ (cid:52)n, and (cid:80)k

i=1 σi = 1. From Lemma 5.5.4 and

i=1 σiaibT

inequality (5.45) (keeping in mind that si = 1, ∀ i) we have

(cid:15) ≤

1
2

≤ 2

k
(cid:88)

k
(cid:88)

σiσj((cid:107)ai(cid:107)1 + (cid:107)aj(cid:107)1)((cid:107)bi(cid:107)1 + (cid:107)bj(cid:107)1)

i=1

j>i

k
(cid:88)

k
(cid:88)

i=1

j>i

σiσj

= 2(

1
2

(

k
(cid:88)

σi

k
(cid:88)

i=1

j=1

σj −

k
(cid:88)

i=1

σ2
i ))

= 1 −

k
(cid:88)

σ2
i

≤ 1 −

i=1
1
k

,

where the last line follows from the fact that (cid:107)v(cid:107)2

2 ≥ 1

k for any vector v ∈ (cid:52)k.

5.5.3 Bounds on (cid:15) in the Rank-2 Case

We now provide a number of bounds on (cid:15)(x, y) with x := P 1n and y := P T 1m which hold

for rank-2 feasible solutions M to SDP2 (note that P will have rank at most 2 in this case).

This is motivated by our ability to show stronger (constant) bounds in this case, and the

fact that we often recover rank-2 (or rank-1) solutions with our algorithms in Section 5.4.

Furthermore, our analysis will use the special property that a rank-2 nonnegative matrix

will have nonnegative rank also equal to two, and that a nonnegative factorization of it can

152

be computed in polynomial time (see, e.g., Section 4 of [26]). We begin with the following

observation, which follows from Theorem 5.5.8 when k = 2.

Corollary 5.5.9. If the matrix P from a feasible solution to SDP2 is rank-2, then x and y

constitute a 1

2−NE.

We now show how this pair of strategies can be reﬁned.

Theorem 5.5.10. If the matrix P from a feasible solution to SDP2 is rank-2, then either x

and y constitute a 5

11-NE, or a 5

11-NE can be recovered from P in polynomial time.

Proof. We consider 3 cases, depending on whether (cid:15)A(x, y) and (cid:15)B(x, y) are greater than or

less than .4. If (cid:15)A ≤ .4, (cid:15)B ≤ .4, then (x, y) is already a .4-Nash equilibrium. Now consider

the case when (cid:15)A ≥ .4, (cid:15)B ≥ .4. Since (cid:15)A ≤ Tr(A(P − xyT )T ) and (cid:15)B ≤ Tr(B(P − xyT )T )

as seen in the proof of Lemma 5.5.1, we have, reusing the notation in the proof of Theorem

5.5.8,

σ1σ2(a1 − a2)T A(b1 − b2) ≥ .4, σ1σ2(a1 − a2)T B(b1 − b2) ≥ .4.

Since A, a1, a2, b1, and b2 are all nonnegative and σ1σ2 ≤ 1
4,

aT
1 Ab1 + aT

2 Ab2 ≥ (a1 − a2)T A(b1 − b2) ≥ 1.6,

and the same inequalities hold for for player B. In particular, since A and B have

entries bounded in [0,1] and a1, a2, b1, and b2 are simplex vectors, all the quantities

aT
1 Ab1, aT

2 Ab2, aT

1 Bb1, and aT

2 Bb2 are at most 1, and consequently at least .6. Hence (a1, a2)

and (a2, b2) are both .4-Nash equilibria.

Now suppose that (x, y) is a .4-NE for one player (without loss of generality player A)

but not for the other (without loss of generality player B). Then (cid:15)A ≤ .4, and (cid:15)B ≥ .4. Let

y∗ be a best response for player B to x, and let p =

1
1+(cid:15)B−(cid:15)A

. Consider the strategy proﬁle

(˜x, ˜y) := (x, py + (1 − p)y∗). This can be interpreted as the outcome (x, y) occurring with

probability p, and the outcome (x, y∗) happening with probability 1 − p. In the ﬁrst case,
153

1+(cid:15)B−(cid:15)A
2) and

player A will have (cid:15)A(x, y) = (cid:15)A and player B will have (cid:15)B(x, y) = (cid:15)B. In the second outcome,

player A will have (cid:15)A(x, y∗) at most 1, while player B will have (cid:15)B(x, y∗) = 0. Then under this

strategy proﬁle, both players have the same upper bound for (cid:15), which equals (cid:15)Bp = (cid:15)B

.

To ﬁnd the worst case for this value, let (cid:15)B = .5 (note from Theorem 5.5.9 that (cid:15)B ≤ 1

(cid:15)A = .4, and this will return (cid:15) = 5
11.

We now show a stronger result in the case of symmetric games.

Deﬁnition 5.5.11. A symmetric game is a game in which the payoﬀ matrices A and B

satisfy B = AT .

Deﬁnition 5.5.12. A Nash equilibrium strategy (x, y) is said to be symmetric if x = y.

Theorem 5.5.13 (see Theorem 2 in [77]). Every symmetric bimatrix game has a symmetric

Nash equilibrium.

For the proof of Theorem 5.5.17 below we modify SDP2 so that we are seeking a symmetric

solution. We also need a more specialized notion of the nonnegative rank.

Deﬁnition 5.5.14. A matrix M is completely positive (CP) if it admits a decomposition

M = U U T for some nonnegative matrix U .

Deﬁnition 5.5.15. The CP-rank of an n × n CP matrix M is the smallest k for which there

exists a nonnegative n × k matrix U such that M = U U T .

Theorem 5.5.16 (see e.g. Theorem 2.1 in [18]). A rank-2, nonnegative, and positive semidef-

inite matrix is CP and has CP-rank 2.

It is also known (see e.g., Section 4 in [55]) that the CP factorization of a rank-2 CP

matrix can be found to arbitrary accuracy in polynomial time.

Theorem 5.5.17. Suppose the constraint P (cid:23) 0 is added to SDP2. Then if in a feasible

solution to this new SDP the matrix P is rank-2, either x and y constitute a symmetric

1

3-NE, or a symmetric 1

3-NE can be recovered from P in polynomial time.

154

Proof. If (x, y) is already a symmetric 1

3-NE, then the claim is established. Now suppose

that (x, y) does not constitute a 1
5.5.8, we can decompose P into (cid:80)2

3-Nash equilibrium. Similarly as in the proof of Theorem

i=1 σiaiaT

i , where (cid:80)2

i=1 σi = 1 and each ai is a vector on

the unit simplex. Then we have

σ1σ2(a1 − a2)T A(a1 − a2) ≥

1
3

.

Since A, a1, and a2 are all nonnegative, and σ1σ2 ≤ 1

4, we get

1 Aa1 + aT
aT

2 Aa2 ≥ (a1 − a2)T A(a1 − a2) ≥

4
3

.

In particular, at least one of aT

1 Aa1 and aT

2 Aa2 is at least 2

3. Since the maximum possible

payoﬀ is 1, at least one of (a1, a1) and (a2, a2) is a (symmetric) 1

3-Nash equilibrium.

Remark 5.5.1. For symmetric games, instead of the construction stated in Theorem 5.5.17,

one can simply optimize over a smaller m × m matrix (note m = n). This is the relaxed

version of exchangeable equilibria [102], with the completely positive constraint relaxed to a

psd constraint.

Remark 5.5.2. The statements of Corollary 5.5.9, and Theorem 5.5.10, and Theorem

5.5.17 hold for any rank-2 correlated equilibrium. Indeed, given any rank-2 (equivalently,

nonnegative-rank-2) correlated equilibrium P , one can complete it to a (rank-2) feasible
solution to SDP2 as follows. Let P = (cid:80)2

i , where ai ∈ (cid:52)m, bi ∈ (cid:52)n, and σ1 + σ2 = 1.

i=1 σiaibT

It is easy to check that

is feasible to SDP2.

M :=

2
(cid:88)

i=1

σi








T




ai

bi







ai

bi




155

5.6 Bounding Payoﬀs and Strategy Exclusion in Sym-

metric Games

In addition to ﬁnding (cid:15)-additive Nash equilibria, our SDP approach can be used to answer

certain questions of economic interest about Nash equilibria without actually computing

them. For instance, economists often would like to know the maximum welfare (sum of

the two players’ payoﬀs) achievable under any Nash equilibrium, or whether there exists a

Nash equilibrium in which a given subset of strategies (corresponding, e.g., to undesirable

behavior) is not played. Both these questions are NP-hard for bimatrix games [40], even when

the game is symmetric and only symmetric equilibria are considered [28]. In this section,

we consider these two problems in the symmetric setting and compare the performance of

our SDP approach to an LP approach which searches over symmetric correlated equilibria.

For general equilibria, it turns out that for these two speciﬁc questions, our SDP approach

is equivalent to an LP that searches over correlated equilibria.

5.6.1 Bounding Payoﬀs

When designing policies that are subject to game theoretic behavior by agents, economists

would often like to ﬁnd one with a good socially optimal outcome, which usually corresponds

to an equilibrium giving the maximum welfare. Hence, given a game, it is of interest to

know the highest achievable welfare under any Nash equilibrium. For symmetric games,

symmetric equilibria are of particular interest as they reﬂect the notion that identical agents

should behave similarly given identical options.

Note that the maximum welfare of a symmetric game under any symmetric Nash equi-

librium is equal to the optimal value of the following quadratic program:

max
x∈(cid:52)m

2xT Ax

subject to xT Ax ≥ eT

i Ax, ∀i ∈ {1, . . . , m}.

156

(5.47)

One can ﬁnd an upper bound on this number by solving an LP which searches over symmetric

correlated equilibria:

max
P ∈Sm,m

subject to

Tr(AP T )

m
(cid:88)

m
(cid:88)

Pi,j = 1

j=1

Ai,jPi,j ≥

i=1
m
(cid:88)

j=1

m
(cid:88)

j=1

P ≥ 0.

Ak,jPi,j, ∀i, k ∈ {1, . . . , m},

(LP1)

(5.48)

(5.49)

(5.50)

A potentially better upper bound on the maximum welfare can be obtained from a version

of SDP2 adapted to this speciﬁc problem:

max
P ∈Sm,m

subject to

Tr(AP T )

(SDP3)

(5.48), (5.49), (5.50)

P (cid:23) 0.

To test the quality of these upper bounds, we tested this LP and SDP on a random sample

of one hundred 5×5 and 10×10 games7. The resulting upper bounds are in Figure 5.2, which

shows that the bound returned by SDP3 was exact in a large number of the experiments.8

7The matrix A in each game was randomly generated with diagonal entries uniform and independent in

[0,.5] and oﬀ-diagonal entries uniform and independent in [0,1].

8The computation of the exact maximum payoﬀs was done with the lrsnash software [12], which computes
all extreme Nash equilibria. For a deﬁnition of extreme Nash equilibria and for understanding why it is
suﬃcient for us to compare against extreme Nash equilibria (both in Section 5.6.1 and in Section 5.6.2), see
Appendix A.3. The computation of the SDP upper bound has been implemented in the ﬁle nashbound.m,
which is publicly available at https://github.com/jeffreyzhang92/SDP Nash along with the instance data.
This ﬁle more generally computes an SDP-based lower bound on the minimum of an input quadratic function
over the set of Nash equilibria of a bimatrix game. The ﬁle also takes as an argument whether one wishes
to only consider symmetric equilibria when the game is symmetric.

157

Figure 5.2: The quality of the upper bound on the maximum welfare obtained by LP1

and SDP3 on 100 5 × 5 games (left) and 100 10 × 10 games (right).

5.6.2 Strategy Exclusion

The strategy exclusion problem asks, given a subset of strategies S = (Sx, Sy), with

Sx ⊆ {1, . . . , m} and Sy ⊆ {1, . . . , n}, is there a Nash equilibrium in which no strategy in S

is played with positive probability. We will call a set S “persistent” if the answer to this

question is negative, i.e. at least one strategy in S is played with positive probability in

every Nash equilibrium. One application of the strategy exclusion problem is to understand

whether certain strategies can be discouraged in the design of a game, such as reckless

behavior in a game of chicken or defecting in a game of prisoner’s dilemma.

In these

particular examples these strategy sets are persistent and cannot be discouraged.

158

As in the previous subsection, we consider the strategy exclusion problem for symmetric

strategies in symmetric games (such as the aforementioned games of chicken and prisoner’s

dilemma). A quadratic program which addresses this problem is as follows:

min
x∈(cid:52)m

(cid:88)

i∈Sx

xi

subject to xT Ax ≥ eT

i Ax, ∀i ∈ {1, . . . , m}.

(5.51)

Observe that by design, S is persistent if and only if this quadratic program has a positive

optimal value. As in the previous subsection, an LP relaxation of this problem which searches

over symmetric correlated equilibria is given by

min
P ∈Sm,m

(cid:88)

m
(cid:88)

i∈Sx

j=1

Pij

subject to

(5.48), (5.49), (5.50).

(LP2)

The SDP relaxation that we propose for the strategy exclusion problem is the following:

min
P ∈Sm,m

(cid:88)

m
(cid:88)

i∈Sx

j=1

Pij

subject to

(5.48), (5.49), (5.50)

P (cid:23) 0.

(SDP4)

Our approach would be to declare that the strategy set Sx is persistent if and only if SDP4

has a positive optimal value.

Note that since the optimal value of SDP4 is a lower bound for that of (5.51), SDP4

carries over the property that if a set S is not persistent, then the SDP for sure returns

zero. Thus, when using SDP4 on a set which is not persistent, our algorithm will always

be correct. However, this is not necessarily the case for a persistent set. While we can be

certain that a set is persistent if SDP4 returns a positive optimal value (again, because the

159

optimal value of SDP4 is a lower bound for that of (5.51)), there is still the possibility that

for a persistent set SDP4 will have optimal value zero. The same arguments hold for the

optimal value of LP2.

To test the performance of LP2 and SDP4, we generated 100 random games of size

5 × 5 and 10 × 10 and computed all their symmetric extreme Nash equilibria9. We then,

for every strategy set S of cardinality one and two, checked whether that set of strategies

was persistent, ﬁrst by checking among the extreme Nash equilibria, then through LP2 and

SDP4. The results are presented in Tables 5.2 and 5.3. As can be seen, SDP4 was quite

eﬀective for the strategy exclusion problem.

Table 5.2: Performance of LP2 and SDP4 on 5 × 5 games

|S|
Number of total sets
Number of persistent sets
Persistent sets certiﬁed (LP2)
Persistent sets certiﬁed (SDP4)

1
500
245
177 (72.2%)
245 (100%)

2
1000
748
661 (88.7%)
748 (100%)

Table 5.3: Performance of LP2 and SDP4 on 10 × 10 games

|S|
Number of total sets
Number of persistent sets
Persistent sets certiﬁed (LP2)
Persistent sets certiﬁed (SDP4)

1
1000
326
39 (12.0%)
318 (97.5%)

2
4500
2383
630 (26.4%)
2368 (99.4%)

5.7 Connection to the Sum of Squares/Lasserre Hier-

archy

In this section, we clarify the connection of the SDPs we have proposed in this chapter to

those arising in the sum of squares/Lasserre hierarchy. We start by brieﬂy reviewing this

hierarchy.

9The exact computation of the exact Nash equilibria was done again with the lrsnash software [12], which

computes extreme Nash equilibria. To understand why this suﬃces for our purposes see Appendix A.3.

160

5.7.1 Sum of Squares/Lasserre Hierarchy

The sum of squares/Lasserre hierarchy10 gives a recipe for constructing a sequence of SDPs

whose optimal values converge to the optimal value of a given polynomial optimization

problem. Recall that for a POP of the form

min
x∈Rn

p(x)

subject to qi(x) ≥ 0, ∀i ∈ {1, . . . , m},

where p, qi are polynomial functions, k-th level of the Lasserre hierarchy is given by

γk
sos :=max
γ,σi

γ

subject to p(x) − γ = σ0(x) +

m
(cid:88)

i=1

σi(x)qi(x),

σi is sos, ∀i ∈ {0, . . . , m},

(5.52)

(5.53)

σ0, giσi have degree at most 2k, ∀i ∈ {1, . . . , m}.

Recall that any ﬁxed level of this hierarchy gives an SDP of size polynomial in n and

that, if the quadratic module generated by {x ∈ Rn|gi(x) ≥ 0} is Archimedean (see, e.g. [63]

for deﬁnition), then lim
k→∞

sos = p∗, where p∗ is the optimal value of the pop in (5.52). The
γk

latter statement is a consequence of Putinar’s positivstellensatz (see, e.g. [92], [61]).

5.7.2 The Lasserre Hierarchy and SDP1

One can show, e.g. via the arguments in [62], that the feasible sets of the SDPs dual to

the SDPs underlying the hierarchy we summarized above produce an arbitrarily tight outer

approximation to the convex hull of the set of Nash equilibria of any game. The downside

of this approach, however, is that the higher levels of the hierarchy can get expensive very

10The unfamiliar reader is referred to [61, 88, 63] for an introduction to this hierarchy and the related

theory of moment relaxations.

161

quickly. This is why the approach we took in this chapter was instead to improve the ﬁrst

level of the hierarchy. The next proposition formalizes this connection.

Proposition 5.7.1. Consider the problem of minimizing any quadratic objective function

over the set of Nash equilibria of a bimatrix game. Then, SDP1 (and hence SDP2) gives a

lower bound on this problem which is no worse than that produced by the ﬁrst level of the

Lasserre hierarchy.

Proof. To prove this proposition we show that the ﬁrst level of the Lasserre hierarchy is dual

to a weakened version of SDP1.

Explicit parametrization of ﬁrst level of the Lasserre hierarchy. Consider the

formulation of the Lasserre hierarchy in (5.53) with k = 1. Suppose we are minimizing a

quadratic function

f (x, y) =


T







C









x

y

1

















x

y

1

over the set of Nash equilibria as described by the linear and quadratic constraints in (5.2).

If we apply the ﬁrst level of the Lasserre hierarchy to this particular pop, we get

162

max
Q,α,χ,β,ψ,η

subject to

γ

− γ =










T







C









x

y

1









x

y

1









x

y

1


T







Q









x

y

1









+

m
(cid:88)

i=1

αi(xT Ay − eT

i Ay)

(5.54)

+

+

n
(cid:88)

i=1
m
(cid:88)

i=1

βi(xT By − xT Bei)

χixi +

n
(cid:88)

i=1

ψiyi

xi − 1) + η2(

n
(cid:88)

yi − 1),

i=1

+ η1(

m
(cid:88)

i=1

Q (cid:23) 0,

α, χ, β, ψ ≥ 0,

where Q ∈ Sm+n+1×m+n+1, α, χ ∈ Rm, β, ψ ∈ Rn, η ∈ R2.

By matching coeﬃcients of the two quadratic functions on the left and right hand sides

of (5.54), this SDP can be written as

max
γ,α,β,χ,ψ,η

γ

subject to H (cid:23) 0,

α, β, χ, ψ ≥ 0,

(5.55)

where



H :=

1
2



(− (cid:80)m




(cid:80)n

0
i=1 αi)A + (− (cid:80)n
i=1 βiBT

,i − χT − η11T
m

i=1 βi)B

(− (cid:80)m

i=1 αi)A + (− (cid:80)m

i=1 βi)B (cid:80)n
(cid:80)m

i=1 βiB,i − χ − η11m
i=1 αiAT

i, − ψ − η21n

0

(cid:80)m

i=1 αiAi, − ψT − η21T
n

2η1 + 2η2 − 2γ

163



+C.







(5.56)

Dual of a weakened version of SDP1. With this formulation in mind, let us consider

a weakened version of SDP1 with only the relaxed Nash constraints, unity constraints, and

nonnegativity constraints on x and y in the last column (i.e., the nonegativity constraint is

not applied to the entire matrix). Let the objective be Tr(CM(cid:48)). To write this new SDP in

standard form, let



Ai :=

1
2

0

AT







A

0

0

−AT
i,









, Bi :=

0 −Ai,

0

S1 :=

1
2









0

0

0 1m

0

0

1T
m 0 −2









, S2 :=









0

BT

−BT
,i


0


0




0

0

1
2

1
2









,

B −B,i

0

0

0

1n

0

0

.









0 1T

n −2

Let Ni be the matrix with all zeros except a 1

2 at entry (i, m + n + 1) and (m + n + 1, i) (or

a 1 if i = m + n + 1).

Then this SDP can be written as

min
M(cid:48)

subject to

Tr(CM(cid:48))

M(cid:48) (cid:23) 0,

Tr(NiM(cid:48)) ≥ 0, ∀i ∈ {1, . . . , m + n},

Tr(AiM(cid:48)) ≥ 0, ∀i ∈ {1, . . . , m},

Tr(BiM(cid:48)) ≥ 0, ∀i ∈ {1, . . . , n},

Tr(S1M(cid:48)) = 0,

Tr(S2M(cid:48)) = 0,

Tr(Nm+n+1) = 1.

164

(SDP0)

(5.57)

(5.58)

(5.59)

(5.60)

(5.61)

(5.62)

(5.63)

We now create dual variables for each constraint; we choose αi and βi for the relaxed

Nash constraints (5.59) and (5.60), η1 and η2 for the unity constraints (5.61) and (5.62), χ

for the nonnegativity of x (5.58), ψ for the nonnegativity of y (5.58), and γ for the ﬁnal

constraint on the corner (5.63). These variables are chosen to coincide with those used in the

parametrization of the ﬁrst level of the Lasserre hierarchy, as can be seen more clearly below.

We then write the dual of the above SDP as

max
α,β,λ,γ

subject to

γ

m
(cid:88)

i=1

αiAi +

n
(cid:88)

i=1

βiBi +

2
(cid:88)

i=1

ηiSi +

m
(cid:88)

i=1

Ni+nχi +

n
(cid:88)

i=1

Niψi + γNm+n+1 (cid:22) C,

α, β, χ, ψ ≥ 0.

which can be rewritten as

max
α,β,χ,ψ,γ

γ

subject to G (cid:23) 0,

α, β, χ, ψ ≥ 0,

(5.64)

where



G :=

1
2



(− (cid:80)m




(cid:80)n

0
i=1 αi)A + (− (cid:80)n
i=1 βiBT

,i − χT − η11T
m

i=1 βi)B

(− (cid:80)m

i=1 αi)A + (− (cid:80)m

i=1 βi)B (cid:80)n
(cid:80)m

i=1 βiB,i − χ − η11m
i=1 αiAT

i, − ψ − η21n

0

(cid:80)m

i=1 αiAi, − ψT − η21T
n

2η1 + 2η2 − 2γ



+C.







We can now see that the matrix G coincides with the matrix H in the SDP (5.55). Then

we have

(5.54)opt = (5.55)opt = (5.64)opt ≤ SDP 0opt ≤ SDP 1opt,

where the ﬁrst inequality follows from weak duality, and the second follows from that the

constraints of SDP0 are a subset of the constraints of SDP1.

165

Remark 5.7.1. The Lasserre hierarchy can be viewed in each step as a pair of primal-dual

SDPs: the sum of squares formulation which we have just presented, and a moment formu-

lation which is dual to the sos formulation [61]. All our SDPs in this chapter can be viewed

more directly as an improvement upon the moment formulation.

Remark 5.7.2. One can see, either by inspection or as an implication of the proof of Theo-

rem 5.2.2, that in the case where the objective function corresponds to maximizing player

A’s and/or B’s payoﬀs11, SDPs (5.55) and (5.64) are infeasible. This means that for such

problems the ﬁrst level of the Lasserre hierarchy gives an upper bound of +∞ on the max-

imum payoﬀ. On the other hand, the additional valid inequalities in SDP2 guarantee that

the resulting bound is always ﬁnite.

5.8 Future Work

Our work leaves many avenues of further research. Are there other interesting subclasses of

games (besides strictly competitive games) for which our SDP is guaranteed to recover an

exact Nash equilibrium? Can the guarantees on (cid:15) in Section 5.5 be improved in the rank-2

case (or the general case) by improving our analysis? Is there a polynomial time algorithm

that is guaranteed to ﬁnd a rank-2 solution to SDP2? Such an algorithm, together with

our analysis, would improve the best known approximation bound for symmetric games (see

Theorem 5.5.17). Can this bound be extended to general games? We show in Appendix A.4

that some natural approaches based on symmetrization of games do not immediately lead to

a positive answer to this question. Can SDPs in a higher level of the Lasserre hierarchy be

used to achieve better (cid:15) guarantees? What are systematic ways of adding valid inequalities

11This would be the case, for example, in the maximum social welfare problem of Section 5.6.1, where the

matrix of the quadratic form in the objective function is given by

C =





0
−A − B
0

−A − B 0
0
0

0
0



 .

166

to these higher-order SDPs by exploiting the structure of the Nash equilibrium problem?

For example, since any strategy played with positive probability must give the same payoﬀ,

one can add a relaxed version of the cubic constraints

xixj(eT

i Ay − eT

j Ay) = 0, ∀i, j ∈ {1, . . . , m}

to the SDP underlying the second level of the Lasserre hierarchy. What are other valid

inequalities for the second level? Finally, our algorithms were speciﬁcally designed for two-

player one-shot games. This leaves open the design and analysis of semideﬁnite relaxations

for repeated games or games with more than two players.

167

Appendix A

Appendices for Nash Equilibria

A.1 Statistics on (cid:15) from Algorithms in Section 5.4

Below are statistics for the (cid:15) recovered in 100 random games of varying sizes using the

algorithms of Section 5.4.

Table A.1: Statistics on (cid:15) for 5 × 5 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0702

0.0040

0.0004

0.0099

Diagonal Gap 0.0448

0.0027

0

0.0061

Table A.2: Statistics on (cid:15) for 10 × 5 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0327

0.0044

0.0021

0.0064

Diagonal Gap 0.0267

0.0033

0.0006

0.0053

168

Table A.3: Statistics on (cid:15) for 10 × 10 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0373

0.0058

0.0039

0.0065

Diagonal Gap 0.0266

0.0043

0.0026

0.0051

Table A.4: Statistics on (cid:15) for 15 × 10 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0206

0.0050

0.0034

0.0045

Diagonal Gap 0.0212

0.0038

0.0025

0.0039

Table A.5: Statistics on (cid:15) for 15 × 15 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0169

0.0051

0.0042

0.0039

Diagonal Gap 0.0159

0.0038

0.0029

0.0034

Table A.6: Statistics on (cid:15) for 20 × 15 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0152

0.0046

0.0035

0.0036

Diagonal Gap 0.0119

0.0032

0.0022

0.0027

Table A.7: Statistics on (cid:15) for 20 × 20 games after 20 iterations.

Algorithm

Max Mean Median StDev

Square Root

0.0198

0.0046

0.0039

0.0034

Diagonal Gap 0.0159

0.0032

0.0024

0.0032

169

A.2 Comparison with an SDP Approach from [60]

In this section, at the request of a referee, we compare the ﬁrst level of the SDP hierarchy

given in [60, Section 4] to SDP2 using Tr(M ) as the objective function on 100 randomly

generated games for each size given in the tables below. The ﬁrst level of the hierarchy in

[60] optimizes over a matrix which is slightly bigger than the one in SDP2, though it has a

number of constraints linear in the size of the game considered, as opposed to the quadratic

number in SDP2. We remark that the approach in [60] is applicable more generally to many

other problems, including several in game theory.

The scalar (cid:15) reported in Table A.8 is computed using the strategies (x, y) extracted from

the ﬁrst row of the optimal matrix M1 as described in Section 4.1 of [60]. The scalar (cid:15)

reported in Table A.9 is computed using x = P 1n and y = P T 1m from the optimal solution

to SDP2 with Tr(M) as the objective function.

Table A.8: Statistics on (cid:15) for ﬁrst level of the hierarchy in [60].

5 × 5

10 × 5

10 × 10

15 × 10

15 × 15

20 × 15

20 × 20

Max

0.3357

0.3304

0.2557

0.2189

0.1987

0.1837

0.1828

Mean

0.1883

0.1889

0.1513

0.1446

0.1262

0.1217

0.1087

Median 0.1803

0.1865

0.1452

0.1418

0.1271

0.1208

0.1070

Table A.9: Statistics on (cid:15) for SDP2 with Tr(M ) as the objective function.

5 × 5

10 × 5

10 × 10

15 × 10

15 × 15

20 × 15

20 × 20

Max

0.1581

0.1589

0.115

0.1335

0.0878

0.082

0.0619

Mean

0.0219

0.0332

0.0405

0.04

0.0366

0.0356

0.0298

Median 0.0046

0.0233

0.036

0.0346

0.0345

0.0325

0.0293

We also ran the second level of the hierarchy in [60] on the same 100 5 × 5 games. The

maximum (cid:15) observed was .3362, while the mean was .1880 and the median was .1800. The

size of the variable matrix that needs to be positive semideﬁnite for this level is 78 × 78.

170

A.3 Lemmas for Extreme Nash Equilibria

The results reported in Section 5.6 were found using the lrsnash [12] software which com-

putes extreme Nash equilibria (see deﬁnition below). In particular the true maximum welfare

and the persistent strategy sets were found in relation to extreme symmetric Nash equilibria

only. We show in this appendix why this is suﬃcient for the claims we made about all sym-

metric Nash equilibria. We prove a more general statement below about general games and

general Nash equilibria since this could be of potential independent interest. The proof for

symmetric games is identical once the strategies considered are restricted to be symmetric.

Deﬁnition A.3.1. An extreme Nash equilibrium is a Nash equilibrium which cannot be

expressed as a convex combination of other Nash equilibria.

Lemma A.3.2. All Nash equilibria are convex combinations of extreme Nash equilibria.

Proof. It suﬃces to show that any extreme point of the convex hull of the set of Nash

equilibria must be an extreme Nash equilibrium, as any point in a compact convex set can

be written as a convex combination of its extreme points. Note that this convex hull contains

three types of points: extreme Nash equilibria, Nash equilibria which are not extreme, and

convex combinations of Nash equilibria which are not Nash equilibria. The claim then follows

because any extreme point of the convex hull cannot be of the second or third type, as they

can be written as convex combinations of other points in the hull.

The next lemma shows that checking extreme Nash equilibria are suﬃcient for the max-

imum welfare problem.

Lemma A.3.3. For any bimatrix game, there exists an extreme Nash equilibrium giving the

maximum welfare among all Nash equilibria.

171

Proof. Consider any Nash equilibrium (˜x, ˜y), and let it be written as






˜x


 = (cid:80)r

˜y

i=1 λi











xi

yi

for some set of extreme Nash equilibria

i, j,






x1

y1




 , . . . ,






xr

yr




 and λ ∈ (cid:52)r. Observe that for any

xiT Ayj ≤ xjT Ayj, xiT Byj ≤ xiT Byi,

(A.1)

from the deﬁnition of a Nash equilibrium. Now note that

˜xT (A + B)˜y = (

r
(cid:88)

λixi)T (A + B)(

r
(cid:88)

λiyi)

i=1
r
(cid:88)

r
(cid:88)

i=1
r
(cid:88)

j=1
r
(cid:88)

=

=

i=1

λiλjxiT (A + B)yj

λiλjxiT Ayj +

r
(cid:88)

r
(cid:88)

λiλjxiT Byj

i=1

j=1
r
(cid:88)

r
(cid:88)

(A.1)
≤

i=1

j=1
r
(cid:88)

r
(cid:88)

λiλjxjT Ayj +

λiλjxiT Byi

i=1

j=1

i=1

j=1

λixiT Ayi +

r
(cid:88)

i=1

λixiT Byi

λixiT (A + B)yi.

=

=

r
(cid:88)

i=1
r
(cid:88)

i=1

In particular, since each (xi, yi) is an extreme Nash equilibrium, this tells us for any Nash

equilibrium (˜x, ˜y) there must be an extreme Nash equilibrium which has at least as much

welfare.

Similarly for the results for persistent sets in Section 5.6.2, there is no loss in restricting

attention to extreme Nash equilibria.

Lemma A.3.4. For a given strategy set S, if every extreme Nash equilibrium plays at least

one strategy in S with positive probability, then every Nash equilibrium plays at least one

strategy in S with positive probability.

172

Proof. Let S be a persistent set of strategies. Since all Nash equilibria are composed of

nonnegative entries, and every extreme Nash equilibrium has positive probability on some

entry in S, any convex combination of extreme Nash equilibria must have positive probability

on some entry in S.

A.4 A Note on Reductions from General Games to

Symmetric Games

An anonymous referee asked us if our guarantees for symmetric games transfer over to general

games by symmetrization. Indeed, there are reductions in the literature that take a general

game, construct a symmetric game from it, and relate the Nash equilibria of the original

game to symmetric Nash equilibria of its symmetrized version. In this Appendix, we review

two well-known reductions of this type, which are shown in [45] and [54], and show that the

quality of approximate Nash equilibria can diﬀer greatly between the two games. We hope

that our examples can be of independent interest.

A.4.1 The Reduction of [45]

Consider a game (A, B) with A, B > 0 and a Nash equilibrium (x∗, y∗) of it with payoﬀs

pA := x∗T Ay∗ and pB := x∗T By∗. Then the symmetric game (SAB, ST

AB) with

SAB :=






0 A

BT

0






173

admits a symmetric Nash equilibrium in which both players play


















pA
pA+pB

x∗

pB
pA+pB

y∗




. In the reverse

direction, any symmetric equilibrium







x

y


 ,




x

y



 of (SAB, ST


AB) yields a Nash equilibrium

( x
1T

n y ) to the original game (A, B).
1T

mx , y
To demonstrate that high-quality approximate Nash equilibria in the symmetrized game

can map to low-quality approximate Nash equilibria in the original game, consider the game

given by (A, B) =









 for some (cid:15) > 0. The symmetric strategy















 ,




(cid:15) 0

1 1

(cid:15)2 0

0

1


































1
1+(cid:15)

0

(cid:15)
1+(cid:15)

0

,


































1
1+(cid:15)

0

(cid:15)
1+(cid:15)

0

is an (cid:15) 1−(cid:15)

1+(cid:15)-NE for (SAB, ST

AB), but the strategy pair








1


 ,

0





1



 is a (1 − (cid:15))-NE for (A, B).


0

A.4.2 The Reduction of [54]

Consider a game (A, B) with A > 0, B < 0 and a Nash equilibrium (x∗, y∗) of it with payoﬀs

pA := x∗T Ay∗ and pB := x∗T By∗. Then the symmetric game (SAB, ST

AB) with

















SAB :=

0m×m

A −1m

BT

0n×n

1T
m −1T
n

1n

0

174

admits a symmetric Nash equilibrium in which both players play









x∗
2−pB
y∗
2+pA

1 − 1

2−pB

− 1

2+pA









.

In the reverse direction, any symmetric equilibrium

of (SAB, ST

AB) yields a

Nash equilibrium ( x
1T

mx, y
over the previous one (see [54, Section 1]).

z
n y ) to the original game (A, B). This reduction has some advantages
1T

z

















x

y









x

y

























,

To demonstrate that high-quality approximate Nash equilibria in the new symmetrized

game can again map to low-quality approximate Nash equilibria in the original game, consider

the game given by (A, B) =







0 0

0 1










 ,




−1 −1

0

0








. Let (cid:15) ∈ (0, 1

2). The symmetric strategy





























(cid:15)

0

0

0

























































(cid:15)

0

0

0

1 − (cid:15)

,

1 − (cid:15)

is an (cid:15)

2(1 − (cid:15))-NE1 for (SAB, ST

AB), but the strategy pair








1


 ,

0





1



 is a 1-NE for (A, B).


0

1Note that approximation factor is halved since the range of the entries of the payoﬀ matrix in the

symmetrized game is [−1, 1].

175

Bibliography

[1] Ilan Adler. The equivalence of linear programs and zero-sum games. International

Journal of Game Theory, 42(1):165–177, 2013.

[2] Ilan Adler, Constantinos Daskalakis, and Christos H Papadimitriou. A note on strictly
competitive games. In International Workshop on Internet and Network Economics,
pages 471–474. Springer, 2009.

[3] A. A. Ahmadi, A. Olshevsky, P. A. Parrilo, and J. N. Tsitsiklis. NP-hardness of decid-
ing convexity of quartic polynomials and related problems. Mathematical Programming,
137(1-2):453–476, 2013.

[4] Amir Ali Ahmadi and Georgina Hall. DC decomposition of nonconvex polynomials

with algebraic techniques. Mathematical Programming, pages 1–26, 2015.

[5] Amir Ali Ahmadi and Georgina Hall. On the complexity of detecting convexity over

a box. Mathematical Programming, pages 1–15, 2019.

[6] Amir Ali Ahmadi and Anirudha Majumdar. Some applications of polynomial opti-
mization in operations research and real-time decision making. Optimization Letters,
10(4):709–729, 2016.

[7] Amir Ali Ahmadi and Jeﬀrey Zhang. Complexity aspects of local minima and related

notions. In Preparation.

[8] Farid Alizadeh. Interior point methods in semideﬁnite programming with applications
to combinatorial optimization. SIAM Journal on Optimization, 5(1):13–51, 1995.

[9] Animashree Anandkumar and Rong Ge. Eﬃcient approaches for escaping higher order
In Conference on learning theory, pages

saddle points in non-convex optimization.
81–102, 2016.

[10] VG Andronov, EG Belousov, and VM Shironin. On solvability of the problem of
polynomial programming. Izvestija Akadem. Nauk SSSR, Tekhnicheskaja Kibernetika,
4:194–197, 1982.

[11] Robert J Aumann. Subjectivity and correlation in randomized strategies. Journal of

Mathematical Economics, 1(1):67–96, 1974.

176

[12] David Avis, Gabriel D Rosenberg, Rahul Savani, and Bernhard Von Stengel. Enumer-
ation of Nash equilibria for two-player games. Economic Theory, 42(1):9–37, 2010.

[13] Tom´aˇs Bajbar and S¨onke Behrends. How fast do coercive polynomials grow? Tech-
nical report, Instituts f¨ur Numerische und Angewandte Mathematik, Georg-August-
Universit¨at G¨ottingen, 2017.

[14] Tomas Bajbar and Oliver Stein. Coercive polynomials and their Newton polytopes.

SIAM Journal on Optimization, 25(3):1542–1570, 2015.

[15] Erwin H Bareiss. Sylvesters identity and multistep integer-preserving Gaussian elimi-

nation. Mathematics of computation, 22(103):565–578, 1968.

[16] Saugata Basu and Marie-Fran¸coise Roy. Bounding the radii of balls meeting ev-
ery connected component of semi-algebraic sets. Journal of Symbolic Computation,
45(12):1270–1279, 2010.

[17] Evgeny G Belousov and Diethard Klatte. A Frank–Wolfe type theorem for convex poly-

nomial programs. Computational Optimization and Applications, 22(1):37–48, 2002.

[18] Abraham Berman and Naomi Shaked-Monderer. Completely positive matrices. World

Scientiﬁc, 2003.

[19] Dimitri P Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.

[20] Dimitri P Bertsekas and Paul Tseng. Set intersection theorems and existence of optimal

solutions. Mathematical Programming, 110(2):287–314, 2007.

[21] Grigoriy Blekherman, Pablo A Parrilo, and Rekha R Thomas. Semideﬁnite Optimiza-

tion and Convex Algebraic Geometry. SIAM, 2012.

[22] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university

press, 2004.

[23] Stephen P Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan.
Linear matrix inequalities in system and control theory, volume 15. SIAM, 1994.

[24] Xi Chen and Xiaotie Deng. Settling the complexity of two-player Nash equilibrium.

In FOCS, volume 6, page 47th, 2006.

[25] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Computing Nash equilibria: Approxima-
tion and smoothed complexity. In 2006 47th Annual IEEE Symposium on Foundations
of Computer Science (FOCS’06), pages 603–612. IEEE, 2006.

[26] Joel E Cohen and Uriel G Rothblum. Nonnegative ranks, decompositions, and fac-
torizations of nonnegative matrices. Linear Algebra and its Applications, 190:149–168,
1993.

[27] Vincent Conitzer and Tuomas Sandholm. Complexity results about nash equilibria.

arXiv preprint cs/0205074, 2002.

177

[28] Vincent Conitzer and Tuomas Sandholm. New complexity results about Nash equilib-

ria. Games and Economic Behavior, 63(2):621–641, 2008.

[29] George B Dantzig. A proof of the equivalence of the programming problem and the
game problem. Activity analysis of production and allocation, 13:330–338, 1951.

[30] Constantinos Daskalakis. On the complexity of approximating a Nash equilibrium.

ACM Transactions on Algorithms (TALG), 9(3):23, 2013.

[31] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The com-
plexity of computing a Nash equilibrium. SIAM Journal on Computing, 39(1):195–259,
2009.

[32] Constantinos Daskalakis, Aranyak Mehta, and Christos Papadimitriou. A note on
In International Workshop on Internet and Network

approximate Nash equilibria.
Economics, pages 297–306. Springer, 2006.

[33] Constantinos Daskalakis, Aranyak Mehta, and Christos Papadimitriou. Progress in
approximate Nash equilibria. In Proceedings of the 8th ACM conference on Electronic
commerce, pages 355–358. ACM, 2007.

[34] Etienne De Klerk. Aspects of semideﬁnite programming: interior point algorithms and

selected applications, volume 65. Springer Science & Business Media, 2006.

[35] Etienne De Klerk and Dmitrii V Pasechnik. Approximation of the stability number of
a graph via copositive programming. SIAM Journal on Optimization, 12(4):875–892,
2002.

[36] Maryam Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis,

Stanford University, 2002.

[37] John Fearnley, Paul W Goldberg, Rahul Savani, and Troels Bjerre Sørensen. Approx-
imate well-supported Nash equilibria below two-thirds. Algorithmica, 76(2):297–319,
2016.

[38] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval

Research Logistics (NRL), 3(1-2):95–110, 1956.

[39] Michael R Garey and David S Johnson. Computers and Intractability, volume 29. WH

Freeman New York, 2002.

[40] Itzhak Gilboa and Eitan Zemel. Nash and correlated equilibria: Some complexity

considerations. Games and Economic Behavior, 1(1):80–93, 1989.

[41] Michel X Goemans and David P Williamson. Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming. Journal of
the ACM (JACM), 42(6):1115–1145, 1995.

[42] Evgenii Alekseevich Gorin. Asymptotic properties of polynomials and algebraic func-

tions of several variables. Russian mathematical surveys, 16(1):93–119, 1961.

178

[43] Aur´elien Greuet and Mohab Safey El Din. Deciding reachability of the inﬁmum of
In Proceedings of the 36th International Symposium on

a multivariate polynomial.
Symbolic and Algebraic Computation, pages 131–138. ACM, 2011.

[44] Aur´elien Greuet and Mohab Safey El Din. Probabilistic algorithm for polynomial
optimization over a real algebraic set. SIAM Journal on Optimization, 24(3):1313–
1343, 2014.

[45] JH Griesmer, AJ Hoﬀman, and A Robinson. On symmetric bimatrix games. IBM
Research Paper RC-959. IBM Corp, Thomas J Watson Research Center, Yorktown
Heights, New York, 1963.

[46] Martin Gr¨otschel, L´aszl´o Lov´asz, and Alexander Schrijver. Geometric Algorithms and
Combinatorial Optimization, volume 2. Springer Science & Business Media, 2012.

[47] J William Helton and Jiawang Nie. Semideﬁnite representation of convex sets. Math-

ematical Programming, 122(1):21–64, 2010.

[48] Lane A Hemaspaandra and Ryan Williams. SIGACT News Complexity Theory Col-
umn 76: An atypical survey of typical-case heuristic algorithms. ACM SIGACT News,
43(4):70–89, 2012.

[49] David Hilbert. ¨Uber die darstellung deﬁniter formen als summe von formenquadraten.

Mathematische Annalen, 32(3):342–350, 1888.

[50] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press,

2012.

[51] M Huneault and FD Galiana. A survey of the optimal power ﬂow literature. IEEE

Transactions on Power Systems, 6(2):762–770, 1991.

[52] Soichi Ibaraki and Masayoshi Tomizuka. Rank minimization approach for solving BMI
problems with random search. In American Control Conference, 2001. Proceedings of
the 2001, volume 3, pages 1870–1875. IEEE, 2001.

[53] Vaithilingam Jeyakumar, Jean B Lasserre, and Guoyin Li. On polynomial optimization
over non-compact semi-algebraic sets. Journal of Optimization Theory and Applica-
tions, 163(3):707–718, 2014.

[54] AP Jurg, MJM Jansen, Jos AM Potters, and SH Tijs. A symmetrization for ﬁnite

two-person games. Zeitschrift f¨ur Operations Research, 36(2):111–123, 1992.

[55] Vassilis Kalofolias and Efstratios Gallopoulos. Computing symmetric nonnegative rank

factorizations. Linear Algebra and its Applications, 436(2):421–435, 2012.

[56] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In
Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing, pages
302–311, 1984.

179

[57] Leonid Genrikhovich Khachiyan. A polynomial algorithm in linear programming. In
Doklady Akademii Nauk, volume 244, pages 1093–1096. Russian Academy of Sciences,
1979.

[58] Spyros C Kontogiannis, Panagiota N Panagopoulou, and Paul G Spirakis. Polynomial
In International

algorithms for approximating Nash equilibria of bimatrix games.
Workshop on Internet and Network Economics, pages 286–296. Springer, 2006.

[59] Jean-Louis Krivine.
12(1):307–326, 1964.

Anneaux pr´eordonn´es.

Journal d’analyse math´ematique,

[60] Rida Laraki and Jean B Lasserre. Semideﬁnite programming for min–max problems

and games. Mathematical Programming, 131(1-2):305–332, 2012.

[61] Jean B Lasserre. Global optimization with polynomials and the problem of moments.

SIAM Journal on Optimization, 11(3):796–817, 2001.

[62] Jean B Lasserre. Convex sets with semideﬁnite representation. Mathematical program-

ming, 120(2):457–477, 2009.

[63] Monique Laurent. Sums of squares, moment matrices and optimization over poly-
nomials. In Emerging Applications of Algebraic Geometry, pages 157–270. Springer,
2009.

[64] Carlton E Lemke and Joseph T Howson, Jr. Equilibrium points of bimatrix games.

Journal of the Society for Industrial and Applied Mathematics, 12(2):413–423, 1964.

[65] Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Oswaldo Boaventura-Netto,
Peter Hahn, and Tania Querido. A survey for the quadratic assignment problem.
European journal of operational research, 176(2):657–690, 2007.

[66] Henri Lombardi, Daniel Perrucci, and Marie-Fran¸coise Roy. An elementary recur-
sive bound for eﬀective Positivstellensatz and Hilbert’s 17th problem. Available at
arXiv:1404.2338, 2014.

[67] L´aszl´o Lov´asz. On the shannon capacity of a graph. IEEE Transactions on Information

theory, 25(1):1–7, 1979.

[68] Z.-Q. Luo and S. Zhang. On extensions of the Frank-Wolfe theorems. Computational

Optimization and Applications, 13(1-3):87–110, 1999.

[69] Murray Marshall. Optimization of polynomial functions. Canadian Mathematical

Bulletin, 46(4):575–587, 2003.

[70] Garth P McCormick. Computability of global solutions to factorable nonconvex
programs: Part I : Convex underestimating problems. Mathematical Programming,
10(1):147–175, 1976.

[71] Carl D Meyer. Matrix analysis and applied linear algebra, volume 2. SIAM, 2000.

180

[72] Jorge J Mor´e and Stephen A Vavasis. On the solution of concave knapsack problems.

Mathematical programming, 49(1-3):397–411, 1990.

[73] MOSEK. MOSEK reference manual, 2013. Version 7. Latest version available at

http://www.mosek.com/.

[74] Theodore S Motzkin and Ernst G Straus. Maxima for graphs and a new proof of a

theorem of tur´an. Canadian Journal of Mathematics, 17:533–540, 1965.

[75] Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and

nonlinear programming. Mathematical Programming, 39(2):117–129, 1987.

[76] Katta G Murty and Feng-Tien Yu. Linear Complementarity, Linear and Nonlinear

Programming, volume 3. Berlin: Heldermann, 1988.

[77] John Nash. Non-cooperative games. Annals of mathematics, pages 286–295, 1951.

[78] Yurii Nesterov. Implementable tensor methods in unconstrained convex optimization.

Mathematical Programming, pages 1–27, 2019.

[79] Yurii Nesterov et al. Random walk in a simplex and quadratic optimization over convex

polytopes. Technical report, CORE, 2003.

[80] Yurii Nesterov and Arkadii Nemirovskii. Interior-Point Polynomial Algorithms in Con-

vex Programming, volume 13. SIAM, 1994.

[81] Tim Netzer. On semideﬁnite representations of non-closed sets. Linear algebra and its

applications, 432(12):3072–3078, 2010.

[82] Jiawang Nie. Optimality conditions and ﬁnite convergence of the Lasserre hierarchy.

Mathematical programming, 146(1-2):97–121, 2014.

[83] Jiawang Nie. The hierarchy of local minimums in polynomial optimization. Mathe-

matical Programming, 151(2):555–583, 2015.

[84] Jiawang Nie, James Demmel, and Bernd Sturmfels. Minimizing polynomials via sum
of squares over the gradient ideal. Mathematical Programming, 106(3):587–606, 2006.

[85] Panos M Pardalos and Georg Schnitger. Checking local optimality in constrained
quadratic programming is np-hard. Operations Research Letters, 7(1):33–35, 1988.

[86] Panos M Pardalos and Stephen A Vavasis. Quadratic programming with one negative

eigenvalue is np-hard. Journal of Global optimization, 1(1):15–22, 1991.

[87] Panos M Pardalos and Stephen A Vavasis. Open questions in complexity theory for

numerical optimization. Mathematical Programming, 57(1-3):337–339, 1992.

[88] Pablo A Parrilo. Semideﬁnite programming relaxations for semialgebraic problems.

Mathematical Programming, 96(2):293–320, 2003.

181

[89] Pablo A Parrilo. Polynomial games and sum of squares optimization. In Proceedings
of the 45th IEEE Conference on Decision and Control, pages 2855–2860. IEEE, 2006.

[90] L. Porkolab and L. Khachiyan. On the complexity of semideﬁnite programs. Journal

of Global Optimization, 10(4):351–365, 1997.

[91] Alexander Prestel and Charles N Delzell. Positive Polynomials: from Hilbert’s 17th
Problem to Real Algebra. Springer Monographs in Mathematics. Springer, Berlin,
Germany, 2001.

[92] Mihai Putinar. Positive polynomials on compact semi-algebraic sets. Indiana Univer-

sity Mathematics Journal, 42(3):969–984, 1993.

[93] Motakuri Ramana and AJ Goldman. Some geometric results in semideﬁnite program-

ming. Journal of Global Optimization, 7(1):33–50, 1995.

[94] Motakuri V Ramana. An exact duality theory for semideﬁnite programming and its

complexity implications. Mathematical Programming, 77(1):129–162, 1997.

[95] Motakuri Venkata Ramana. An algorithmic analysis of multiquadratic and semideﬁnite

programming problems. PhD thesis, Citeseer, 1993.

[96] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization. SIAM Review,
52(3):471–501, 2010.

[97] R Tyrrell Rockafellar. Convex Analysis, volume 28. Princeton University Press, 1970.

[98] Rahul Savani and Bernhard Stengel. Hard-to-solve bimatrix games. Econometrica,

74(2):397–429, 2006.

[99] Thomas J Schaefer. The complexity of satisﬁability problems. In Proceedings of the

tenth annual ACM symposium on Theory of computing, pages 216–226. ACM, 1978.

[100] A. Seidenberg. A new decision method for elementary algebra. Annals of Mathematics,

pages 365–374, 1954.

[101] Parikshit Shah and Pablo A Parrilo. Polynomial stochastic games via sum of squares
optimization. In Decision and Control, 2007 46th IEEE Conference on, pages 745–750.
IEEE, 2007.

[102] Noah D Stein. Exchangeable equilibria. PhD thesis, Massachusetts Institute of Tech-

nology, 2011.

[103] Gilbert Stengle. A Nullstellensatz and a Positivstellensatz in semialgebraic geometry.

Mathematische Annalen, 207(2):87–97, 1974.

[104] Johan AK Suykens and Joos Vandewalle. Least squares support vector machine clas-

siﬁers. Neural processing letters, 9(3):293–300, 1999.

182

[105] Alfred Tarski. A decision method for elementary algebra and geometry. In Quantiﬁer

Elimination and Cylindrical Algebraic Decomposition, pages 24–84. Springer, 1998.

[106] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996.

[107] Haralampos Tsaknakis and Paul G Spirakis. An optimization approach for approx-
imate Nash equilibria. In International Workshop on Web and Internet Economics,
pages 42–56. Springer, 2007.

[108] Lieven Vandenberghe and Stephen Boyd. Semideﬁnite programming. SIAM Review,

38(1):49–95, 1996.

[109] Stephen A Vavasis. Quadratic programming is in np. Information Processing Letters,

36(2):73–77, 1990.

[110] Cynthia Vinzant. What is... a spectrahedron? Notices Amer. Math. Soc, 61(5):492–

494, 2014.

[111] Sven Wagner. Archimedean quadratic modules: a decision problem for real multivariate

polynomials. PhD thesis, University of Konstanz, 2009.

183

