Vulnerability Detection with Fine-Grained Interpretations

Yi Li
New Jersey Inst. of Technology
New Jersey, USA
yl622@njit.edu

Shaohua Wangâˆ—
New Jersey Inst. of Technology
New Jersey, USA
davidsw@njit.edu

Tien N. Nguyen
University of Texas at Dallas
Texas, USA
tien.n.nguyen@utdallas.edu

ABSTRACT

1 INTRODUCTION

1
2
0
2

n
u
J

9
1

]

R
C
.
s
c
[

1
v
8
7
4
0
1
.
6
0
1
2
:
v
i
X
r
a

Despite the successes of machine learning (ML) and deep learning
(DL) based vulnerability detectors (VD), they are limited to provid-
ing only the decision on whether a given code is vulnerable or not,
without details on what part of the code is relevant to the detected
vulnerability. We present IVDetect, an interpretable vulnerability
detector with the philosophy of using Artificial Intelligence (AI) to
detect vulnerabilities, while using Intelligence Assistant (IA) via
providing VD interpretations in terms of vulnerable statements.

For vulnerability detection, we separately consider the vulnera-
ble statements and their surrounding contexts via data and control
dependencies. This allows our model better discriminate vulnerable
statements than using the mixture of vulnerable code and con-
textual code as in existing approaches. In addition to the coarse-
grained vulnerability detection result, we leverage interpretable AI
to provide users with fine-grained interpretations that include the
sub-graph in the Program Dependency Graph (PDG) with the cru-
cial statements that are relevant to the detected vulnerability. Our
empirical evaluation on vulnerability databases shows that IVDe-
tect outperforms the existing DL-based approaches by 43%â€“84%
and 105%â€“255% in top-10 nDCG and MAP ranking scores. IVDe-
tect correctly points out the vulnerable statements relevant to
the vulnerability via its interpretation in 67% of the cases with a
top-5 ranked list. It improves over baseline interpretation models
by 12.3%â€“400% and 9%â€“400% in accuracy.

CCS CONCEPTS
â€¢ Security and privacy â†’ Software security engineering.

KEYWORDS

Vulnerability Detection; Deep Learning; Explainable AI; Interpretable
AI

ACM Reference Format:
Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Vulnerability Detection
with Fine-Grained Interpretations. In Proceedings of the 29th ACM Joint Eu-
ropean Software Engineering Conference and Symposium on the Foundations
of Software Engineering (ESEC/FSE â€™21), August 23â€“28, 2021, Athens, Greece.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3468264.3468597

âˆ—Corresponding Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08. . . $15.00
https://doi.org/10.1145/3468264.3468597

Software vulnerabilities have caused substantial damage to soci-
etyâ€™s software infrastructures. Automated vulnerability detection
(VD) approaches can be broadly classified into two categories: pro-
gram analysis (PA)-based [1, 2, 7â€“9, 33] and machine learning (ML)-
based [23, 28, 30]. The PA-based VD techniques have often focused
on solving the specific types of vulnerabilities such as BufferOver-
flow [3], SQL Injection [6], Cross-site Scripting [5], Authentication
Bypass [4], etc. In addition to those types, the more general software
vulnerabilities, e.g., in API usages of libraries/frameworks, have
manifested in various forms. To detect them, machine learning (ML)
and deep learning (DL) have been leveraged to implicitly learn the
patterns of vulnerabilities from prior vulnerable code [15, 20, 37].
Despite several advantages, the ML/DL-based VD approaches are
still limited to providing only coarse-grained detection results on
whether an entire given method is vulnerable or not. In comparison
with the PA-based approaches, they fall short in the ability to elab-
orate on the fine-grained details of the lines of code with specific
statements that might be involved in the detected vulnerability.
One could use fault localization (FL) techniques [16] to locate the
vulnerable statements, however they require large, effective test
suites. Due to such feedback at the coarse granularity from the
existing ML/DL-based VD tools, developers would not know where
and what to look for and to fix the vulnerability in their code. This
hinders them in investigating the potential vulnerabilities.

To raise the level of ML/DL-based VD, we present IVDetect, an
interpretable VD with the philosophy of using Artificial Intelligence
to detect coarse-grained vulnerability, while leveraging Intelligence
Assistant via interpretable ML to provide fine-grained interpreta-
tions in term of vulnerable statements relevant to the vulnerability.
For coarse-grained vulnerability detection, our novelty is the
context-aware representation learning of the vulnerable code. During
training, the existing ML/DL-based VD approaches [20, 37] take the
entire vulnerable code in a method as the input without distinguish-
ing the vulnerable statements from the surrounding contextual
code. Such distinction from vulnerable code and the contexts dur-
ing training enable IVDetect to better learn to discriminate the
vulnerable code and benign ones. We represent source code via
program dependence graph (PDG) and we treat the vulnerability
detection problem as graph-based classification via Graph Con-
volution Network (GCN) [17] with feature-attention (FA), namely
FA-GCN. The vulnerable statements, along with surrounding code,
are encoded during the code representation learning.

For fine-grained interpretation, as the given method is deemed
as vulnerable by IVDetect, our novelty is to leverage interpretable
ML [36] to provide the interpretation in term of the vulnerable state-
ments as part of the PDG that are involved to the detected vulnerability.
The rationale for choosing PDG sub-graph as an interpretation is

 
 
 
 
 
 
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

that a vulnerability often involves the data and control dependen-
cies among the statements [26].

To derive the vulnerable statements as the interpretation, we
leverage the interpretable ML model, GNNExplainer [36], that â€œex-
plainsâ€ on why a model has arrived at its decision. Specifically, after
vulnerability detection, to produce interpretation, IVDetect takes
as input the FA-GCN model along with its decision (vulnerable or
not), and the input PDG ğºğ‘€ of the given method ğ‘€. The goal is
to find the interpretation subgraph, which is defined as a minimal
sub-graph G in the PDG of ğ‘€ that minimizes the prediction scores
between using the entire ğºğ‘€ and using G. To that end, we leverage
GNNExplainer [36] in which the searching for G is formulated as
the learning of the edge-mask set ğ¸ğ‘€. The idea is that if an edge
belongs ğ¸ğ‘€, i.e., if it is removed from ğºğ‘€ , and the decision of the
model is affected, then the edge is crucial and must be included in the
interpretation for the detection result. Thus, the minimal sub-graph G
in PDG contains the nodes and edges, i.e., the crucial statements and
program dependencies, that are most decisive/relevant to the detected
vulnerability when the decision is vulnerable.

Using our results, a practitioner would 1) examine the ranked
list of potentially vulnerable methods, and 2) use the interpretation
to further investigate what statements in the code that caused the
model to predict that vulnerability.

We conducted several experiments to evaluate IVDetect in both
vulnerability detection at the method level and interpretation in
term of vulnerable statements. We use 3 large C/C++ vulnerability
datasets: Fan [13], Reveal [11] and FFMPeg+Qemu [37]. For the
method-level VD, our results show that IVDetect outperforms the
existing ML/DL-based approaches [11, 19, 20, 27, 37] by 43%â€“84%
and 105%â€“255% at the top 10 list for two ranking scores nDCG and
MAP, respectively. For the statement-level interpretation, IVDe-
tect correctly points out the vulnerable statements relevant to
the vulnerability in 67% of the cases with a top-5 ranked list. It
improves over the baseline ATT [36] and GRAD [36] interpretation
models by 12.3%â€“400% and 9%â€“400% in accuracy, respectively.

The contributions of this paper include:

A. Interpretable VD with Fine-grained Interpretations

a. Vulnerability Detection with Fine-grained Interpreta-
tions: IVDetect is the first approach to leverage interpretable
ML to enhance VD with fine-grained details on PDG sub-graphs,
statements, and dependencies relevant to the detected vulnerability.
b. Context-aware Representation Learning of vulnerable
code: The novelty of our representation learning of vulnerable code
is our consideration of the contextual code surrounding the vulnerable
statements and fixes to better train the VD model.
B. Empirical Evaluation. Our results show IVDetectâ€™s high ac-
curacy in both detection and interpretation (See data/results at [10]).

2 MOTIVATION
2.1 Motivating Example

Figure 1 shows the method ec_device_ioctl_xcmd in Linux 4.6, which
constructs the I/O control command for the CromeOS devices. This
is listed as a vulnerable code within Common Vulnerabilities and
Exposures (CVE-2016-6156) in the National Vulnerability Database.

The commit log of the corresponding fix stated that

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33

static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)
{

long ret;
struct cros_ec_command u_cmd;
struct cros_ec_command *s_cmd;
if (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))

return -EFAULT;

if ((u_cmd.outsize > EC_MAX_MSG_BYTES) || (u_cmd.insize > EC_MAX_MSG_BYTES))

return -EINVAL;

s_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize), GFP_KERNEL);
if (!s_cmd)

return -ENOMEM;

if (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {

+
+
+
+
+

ret = -EFAULT;
goto exit;

}
if (u_cmd.outsize != s_cmd->outsize ||
u_cmd.insize != s_cmd->insize) {
ret = -EINVAL;
goto exit;

}
s_cmd->command += ec->cmd_offset;
ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
/* Only copy data to userland if data was received. */
if (ret < 0)

goto exit;

-
+

if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))

ret = -EFAULT;

exit:

kfree(s_cmd);
return ret;

}

Figure 1: CVE-2016-6156 Vulnerability in Linux 4.6

â€œAt line 6 and line 13, the driver fetches user space data by pointer
arg via copy_from_user(). The first fetched value (stored in u_cmd) (line
6) is used to get the in_size and out_size elements and allocation a
buffer (s_cmd) at line 10 so as to copy the whole message to driver
later at line 13, which means the copy size of the whole message
(s_cmd) is based on the old value (u_cmd.outsize) from the first fetch.
Besides, the whole message copied at the second fetch also contains
the elements of in_size and out_size, which are the new values. The
new values from the second fetch might be changed by another user
thread under race condition, which will result in a double-fetch bug
when the inconsistent values are used.â€

Thus, to fix this bug, a developer added the code at lines 17â€“21
to make sure that u_cmd.outsize and u_cmd.insize have not changed
due to race condition between the two fetching calls. Moreover,
memory access might be also beyond the array boundary, causing
a buffer overflow within the method call cros_ec_cmd_xfer(...), when
the command is transferred to the ChromeOS device at line 23.

Another issue is at line 27 with copy_to_user. The method call
cros_ec_cmd _xfer(...) can set s_cmd->insize to a lower value. Thus, the
new smaller value must be used to avoid copying too much data to
the user: u_cmd.insize at line 27 is changed into s_cmd->insize.

This vulnerable code could potentially cause the damages such as
denial of service, buffer overflow, program crash, etc. Deep learning
(DL) advances enable several approaches [20, 37] to implicitly learn
from the history the patterns of vulnerable code, and to detect more
general vulnerabilities. However, they are still limited in comparison
with program analysis-based approaches in the ability to provide
any detail on the fine-grained level of the vulnerable statements, and
on why the model has decided on the vulnerability. For example,
the PA-based approaches, e.g., a race detection technique could
potentially detect the involvement of the two fetching statements
at line 6 and line 13. The method in Figure 1 might be deemed
as vulnerable by a DL-based model. But without any fine-grained
details, a developer would not know where and what to investigate

Vulnerability Detection with Fine-Grained Interpretations

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

2.2 Key Ideas and Architecture Overview

IVDetect has two main modules (Figure 3): graph-based vulnera-
bility detection model, and graph-based interpretation model. The
input is the source code of all methods in a project. The output is
the ranked list of methods with the detection result/score and the
interpretation (PDG sub-graph). Let us explain our key ideas.

2.2.1 Graph-based Vulnerability Detection Model (Section 3).
As seen in Section 2.1, a vulnerability is usually exhibited as multiple
statements are exploited, thus, it is natural to capture the vulnerable
code as a sub-graph in the PDG with the data and control flows. To
do so, we model the vulnerability detection via the Graph Convo-
lutional Network (GCN) [17] as follows. The PDG of a method ğ‘€
is represented as a graph ğºğ‘ = (ğ‘‰ , ğ¸) in which ğ‘‰ is a set of nodes
representing the statements, and ğ¸ is a set of edges representing
the data/control dependencies. A feature description ğ‘¥ğ‘‰ is for every
node ğ‘£, which represents a property of a node, e.g., variable name,
etc. Features are summarized in a ğ‘ Ã— ğ· feature matrix ğ‘‹ğ‘€ (ğ‘ :
number of nodes and ğ· is the number of input features). Let ğ‘“ be
a label function on the statements and methods ğ‘“ : ğ‘‰ â†’ {1, ..., ğ¶}
that maps a node in ğ‘‰ and an entire method to one of the ğ¶ classes.
In IVDetect, ğ¶=2 for vulnerable (V) and non-vulnerable (N V).
For training on (non-)vulnerable code in the training set, GCN
performs similar operations as CNN where it learns the features
with a small filter/window sliding over PDG sub-structure. Differing
from image data with CNN, the neighbors of a node in GCN are un-
ordered and variable in size. To predict if a method ğ‘€ is vulnerable,
its PDG ğºğ‘€ with the associated feature set ğ‘‹ğ‘€ = {ğ‘¥ ğ‘— |ğ‘£ ğ‘— âˆˆ ğºğ‘€ } are
built. GCN learns a conditional distribution ğ‘ƒ (ğ‘Œ |ğºğ‘€, ğ‘‹ğ‘€ ), where
ğ‘Œ is a random variable representing the labels {1, ..., ğ¶}. That distri-
bution indicates the probability of the graph ğºğ‘€ belonging to each
of the classes {1, ..., ğ¶}, i.e., ğ‘€ is vulnerable or not (Section 3).

2.2.2 Distinction between Vulnerable Statements and Sur-
rounding Contexts. During training, for each vulnerable state-
ment ğ‘  in a method in the training dataset, we distinguish ğ‘  and the
surrounding contextual statements for ğ‘ . A context consists of the
statements with data and/or control dependencies with ğ‘ . This is
expected to help our model recognize better the vulnerable code
appearing in specific surrounding contexts, and have better discrim-
inating the vulnerable code from the benign one. For example, the
existing approaches feed the entire PDG of the method in Figure 2
into a model. IVDetect distinguishes and learns the vector repre-
sentation for the vulnerable statement at line 27 while considering
as contexts the statements with data/control dependencies with
line 27: the data-dependency context (lines 31, 22, 13, 10, and 6),
and the control-dependency context (lines 29, 25, 23, and 13).
2.2.3 Graph-based Interpretation Model for Vulnerability
Detection (Section 4). After prediction, IVDetect performs fine-
grained interpretation. It uses both the PDG ğºğ‘€ of the method
ğ‘€ and the GCN model as the input to obtain the interpretation.
To that end, we leverage the interpretable ML technique GNNEx-
plainer [36]. Its goal is to take the GCN and a specific input graph
ğºğ‘€ , and produce the crucial sub-graph structures and features in ğºğ‘€
that affect the decision of the model. GNNExplainerâ€™s idea is that if
removing or altering a node/feature does affect the prediction outcome,
the node/feature is considered as essential and thus must be included

Figure 2: Interpretation Sub-Graph for Figure 1

Figure 3: Overview of IVDetect

next. This would make the output of a DL model less constructive
in VD. Moreover, a fault localization technique [16], which locates
buggy statements, would need a large, effective test suite.

Regarding detection, the existing DL-based approaches [20, 37]
do not fully exploit all the available information on the vulnera-
ble code during training. For example, during training, we know
that lines 23 and 27 are vulnerable/buggy, and other relevant state-
ments via data/control dependencies provide contextual information
for the vulnerable ones. However, the existing approaches [20, 37]
do not consider the vulnerable statements and do not use the con-
textual code to help a model discriminate the vulnerable and non-
vulnerable ones. The entire method would be fed to a DL model.
IVDetect Approach. We introduce IVDetect, an DL-based, in-
terpretable vulnerability detection approach that goes beyond the
decision of vulnerability by providing the fine-grained interpreta-
tion in term of the vulnerable statements. Specifically, as the method
is deemed as vulnerable by IVDetect, it will provide a list of impor-
tant statements as part of the program dependence graph (PDG) that
are relevant to the detected vulnerability. For example, it provides
the partial sub-graph of the PDG including the statements at the
lines 13â€“15, 22â€“23, and 25â€“27 in Figure 2 for the vulnerable code at
line 23 and line 27. We use the PDG sub-graph including important
statements for fine-grained VD since they will give a developer the
hints on the program dependencies relevant to the vulnerability
for further investigation. Moreover, if our model determines the
code as non-vulnerable, it can also produce the key sub-graph of
the PDG with key statements that are deemed to be safe.

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

in the crucial set (let us call it the interpretation set). GNNExplainer
searches for a sub-graph Gğ‘€ in ğºğ‘€ that minimizes the difference in
the prediction scores between using the whole graph ğºğ‘€ and using
the minimal graph Gğ‘€ (Section 4). Because without that subgraph
Gğ‘€ in the input PDG ğºğ‘€ , GCN model would not decide ğºğ‘€ as
vulnerable, Gğ‘€ is considered as crucial PDG sub-graph consisting
of crucial statements and data/control dependencies relevant to
the detected vulnerability (if the outcome is V). If the outcome is
non-vulnerability, Gğ‘€ can be considered as the safe statements in
PDG for the model to decide the input method ğ‘€ as benign code.

3 GRAPH-BASED VULNERABILITY

DETECTION MODEL
3.1 Representation Learning

Let us present how we build the vector representations for code
features. For a statement, we extract the following types of features:
1. Sequence of Sub-tokens of a Statement. At the lexical level,
we capture the content of a statement in term of the sequence
of sub-tokens. We choose the sub-token granularity because the
sub-tokens are more likely to be repeated than the entire lexical
tokens in source code [31]. We tokenize each statement and keep
only the variables, method and class names. The names are broken
into sub-tokens using CamelCase or Hungarian convention. We
remove the sub-tokens with one character to avoid the influence
of noises. For example, in Figure 4, the tokens of ğ‘†27 are collected
and broken down into the sequence: copy, to, user, arg, etc. Then,
we use GloVe [25], to build the vectors for tokens, together with
Gate Recurrent Unit (GRU) [12] to build the feature vector for the
sequence of sub-tokens for ğ‘†27. GloVe is known to capture well
semantic similarity among tokens. GRU is chosen to summarize
the sequence of vectors into one feature vector for the next step.
2. Code Structure of a Statement. We capture code structure via
the AST sub-tree. In Figure 4, the AST sub-tree for ğ‘†27 is extracted
and fed to Tree-LSTM [32] to capture the structure into a vector ğ¹2.
3. Variables and Types. For each node (i.e., a statement), we col-
lect the names of the variables and their static types at their lo-
cations, break them into the sub-tokens. For example, we collect
the variable s_cmd and its static type cross_ec_command. We use the
same vector building techniques as for the sub-token sequences as
in feature 1, including GloVe and GRU, to apply on the sequences of
sub-tokens built from the variablesâ€™ names (e.g., s_cmd) and those
from the variablesâ€™ types (e.g, cross_ec_command).
4. Surrounding Contexts. During training, for a statement ğ‘ , we
also encode the statements surrounding ğ‘ , which we refer to as
context. Data- and Control-dependency contexts contain the state-
ments having such dependencies with the current statement. For
example, the data-dependency context for ğ‘†27 includes the state-
ments at the lines 31, 22, 13, 10, and 6. If the control dependencies
are considered, the statements with control dependencies with ğ‘†27
at the lines 29, 25, 23, and 13 are included. The vectors for the state-
ments in the context are calculated via GloVe and GRU as described
earlier. The number of dependencies could be different, then the
lengths of the GRU model inputs could be different. Therefore, we
apply zero padding with a masking layer, which allows the model

to skip the zeros at the end of the sequence of sub-tokens. Those
zeros will not be included in the training.
5. Attention-based Bidirectional GRU. After having all vectors
for the features ğ¹1, ğ¹2, ..., we use a bi-directional GRU and an atten-
tion layer to learn the weight vector ğ‘Šğ‘– for each feature ğ¹ğ‘– , based on
the hidden states from that model. Then, we compute the weighted
vector for each feature by multiplying the original vector for the
feature by the weight: ğ¹ â€²

ğ‘– = ğ‘Šğ‘– .ğ¹ğ‘– .

Finally, we need to consider the impacts from the dependent state-
ments to the current statement in the PDG. The rationale is that those
neighboring statements in the PDG must have the influence on the
current statement if one of them is vulnerable. For example, the
neighboring statements for ğ‘†27 in the PDG include the statements
at lines 6, 22, 25, and 29. Thus, we combine and summarize them
into the final feature vector ğ¹ğ‘†27 for the statement ğ‘†27 as follows:

ğ¹ğ‘†27 =

âˆ‘ï¸

ğ‘–

ğ‘Šğ‘–ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â„(ğ¹ â€²

ğ‘– , ğ‘—))

(1)

ğ‘Šğ‘– is the trainable weight for combination; ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ is the concate-
nate layer to link all values into one vector; â„ is the hidden layer to
summarize vector into a value; ğ‘– = S6, S22, S25, S27, S29; ğ‘— is feature
index. ğ¹27 is used in the next step with GCN model for detection.

3.2 Vulnerability Detection with FA-GCN

Figure 5 presents how we use Feature-Attention GCN model (FA-
GCN) [29] for detection. The rationale is that FA-GCN can deal well
with the graphs with sparse features (not all the statements share
the same properties), and potentially noisy features in a PDG. First,
we parse the method ğ‘€ into PDG. Similar to CNN using the filter
on an image, FA-GCN performs sliding a small window along all
the nodes (statements) of the PDG. For example, in Figure 5, the
window marked with A for the node ğ‘†27 consists of itself and
the neighboring statements/nodes ğ‘†6, ğ‘†22, ğ‘†25, and ğ‘†29. Another
window (marked with B ) is for the node ğ‘†23, including itself and
the neighboring nodes: ğ‘†22 and ğ‘†25. For each window, FA-GCN
generates the feature representation matrix for the statement at the
center. For example, for the window centered at ğ‘†27, it generates the
feature vector ğ¹ğ‘†27 for ğ‘†27, using the process explained in Figure 4.
From the representation vectors for all statements, FA-GCN uses a
join layer to link all these vectors into the Feature Matrix Fğ‘š for
method ğ‘€. A row in Fğ‘š corresponds to a window in PDG.

Next, FA-GCN performs the convolution operation by first calcu-
lating the symmetric normalized Laplacian matrix Ëœğ´ [17], and then
calculating the convolution to generate the representation matrix
ğ‘€ğ‘š for the method ğ‘š. After that, we use the traditional steps as in
a CNN model: using a spatial pyramid pooling layer (to normalize
the method representation matrix into a uniform size, and reduce
its total size), and connecting its output to a fully connected layer
to transform the matrix into a vector ğ‘‰ğ‘š to represent ğ‘š. With ğ‘‰ğ‘š,
we perform classification by using two hidden layers (controlling
the length of vectors and output) and a softmax function to produce
a prediction score for ğ‘š. We use those scores as vulnerability scores
to rank the methods in a project. The decision for ğ‘š as V or N V is
done via a trainable threshold on the prediction score [18, 20].

Vulnerability Detection with Fine-Grained Interpretations

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Figure 4: Code Representation Learning for Statement S27 in Graph-based Vulnerable Code Detection

Figure 5: Vulnerability Detection with FA-GCN

4 GRAPH-BASED INTERPRETATION MODEL

Let us explain how we use GNNExplainer [36] to build our graph-
based interpretation. The input includes the trained FA-GCN model,
the PDG (ğºğ‘€ ) of the method ğ‘€, and the detection result V or N V,
and prediction score. Figure 6 illustrates our process for the case of
V (Vulnerable) (the case of N V is done similarly).

To derive the interpretations, the key goal is to find a sub-graph
Gğ‘€ in the PDG ğºğ‘€ of the method ğ‘€ that minimizes the difference
in the prediction scores between using the entire graph ğºğ‘€ and
using the minimal graph Gğ‘€ . To do so, we use GNNExplainer
with the masking technique [36], which treats the searching for the
minimal graph Gğ‘€ as a learning problem of the edge-mask set ğ¸ğ‘€
of the edges. The idea is that learning ğ¸ğ‘€ helps IVDetect derive
the interpretation sub-graph Gğ‘€ by masking-out the edges in ğ¸ğ‘€
from ğºğ‘€ (â€œmasked-outâ€ is denoted by (cid:199)):

Gğ‘€ = ğºğ‘€

(cid:200) ğ¸ğ‘€

(2)

Figure 6 illustrates GNNExplainerâ€™s principle. As an edge-mask set
is applied, GNNEXplainer checks if the FA-GCN model produces
the same result (in this case the result is V). If yes, the edge in the
edge-mask is not important and is not included in Gğ‘€ . Otherwise,
the edge is important and included in Gğ‘€ . Because the numbers

Figure 6: Masking to Derive Interpretation Sub-Graphs

of possible sub-graphs and the edge-mask sets are untractable,
GNNExplainer uses a learning approach for the edge-mask ğ¸ğ‘€.

Let us formally explain how GNNEXplainer [36] works. It for-
mulates the problem by maximizing the mutual information (MI)
between the minimal graph Gğ‘€ and the input ğºğ‘€ :

ğ‘€ğ¼ (ğ‘Œ, Gğ‘€ ) = ğ» (ğ‘Œ ) âˆ’ ğ» (ğ‘Œ |ğº = Gğ‘€ )

(3)

max
Gğ‘€

ğ‘Œ is the outcome decision by the FA-GCN model. Thus, the entropy
term ğ» (ğ‘Œ ) is constant for the trained FA-GCN model. Maximizing
the ğ‘€ğ¼ value for all Gğ‘€ is equivalent to minimizing conditional
entropy ğ» (ğ‘Œ |ğº = Gğ‘€ ), which by definition of conditional entropy
can be expressed as

âˆ’ Eğ‘Œ | Gğ‘€ [ğ‘™ğ‘œğ‘”ğ‘ƒğ¹ğ´âˆ’ğºğ¶ğ‘ (ğ‘Œ |ğº = Gğ‘€ ]

(4)

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

Table 1: Three Datasets

Dataset
Vulnerabilities
Non-vulnerabilities
Ratio (Vul:Non-vul)

Fan
10,547
168752
1:16

Reveal Devign
10,067
12,294
1:1.2

1,664
16505
1:9.9

The meaning of this conditional entropy formula is a measure of
how much uncertainty remains about the outcome ğ‘Œ when we
know ğº = Gğ‘€ . GNNEXplainer also limits the size of Gğ‘€ by ğ¾ğ‘€ ,
i.e., taking ğ¾ğ‘€ edges that give the highest mutual information with
the prediction outcome ğ‘Œ . Direct optimization of the formula 4 is
not tractable, thus, GNNExplainer treats Gğ‘€ as a random graph
variable G. The objective in Equation 4 becomes:

EGğ‘€ âˆ¼G ğ» (ğ‘Œ |ğº = Gğ‘€ )

min
G

ğ» (ğ‘Œ |ğº = EG [Gğ‘€ ])

min
G

(5)

(6)

From Equation 5, we obtain Equation 6 with Jensenâ€™s inequality. The
conditional entropy in Equation 6 can be optimized by replacing
EG [Gğ‘€ ] to be optimized by masking with ğ¸ğ‘€ on the input graph
ğºğ‘€ . Now, we can reduce the problem to learning the mask ğ¸ğ‘€.
Details on training can be found in [36]. The resulting sub-graph
Gğ‘€ is directly used as an interpretation. We can similarly produce
the interpretations for the cases of non-vulnerability result.

5 EMPIRICAL EVALUATION
5.1 Research Questions

To evaluate IVDetect, we seek to answer the following questions:
RQ1. Comparison on the Method-Level Vulnerability Detec-
tion (VD). How well does IVDetect perform in comparison with
the state-of-the-art method-level Deep Learning VD approaches?
RQ2. Comparison with other Interpretation Models for Fine-
grained VD Interpretation. How well does IVDetect perform
in comparison with the state-of-the-art interpretation models for
fine-grained VD interpretation to point out vulnerable statements?
RQ3. Vulnerable Code Patterns and Fixing Patterns. Is IVDe-
tect useful in detecting vulnerable code patterns and fixes?
RQ4. Sensitivity Analysis for Internal Features. How do inter-
nal features affect the overall performance of IVDetect?
RQ5. Sensitivity Analysis on Training Data. How do different
data splitting schemes affect IVDetectâ€™s performance?
RQ6. Time Complexity. What is time complexity of IVDetect?

5.2 Datasets

We have conducted our study on three vulnerability datasets in-
cluding Fan et al.â€™s [13], Reveal [11] and FFMPeg+Qemu [37] (Ta-
ble 1). Fan et al. [13] dataset covers the CWEs from 2002 to 2019
with 21 features for each vulnerability. At the method level, the
dataset contains +10K vulnerable methods and fixed code. The
Reveal dataset [11] contains +18K methods with 9.16% of the vul-
nerable ones. The FFMPeg+Qemu dataset has been used in Devign
study [37] with +22K data, and 45.0% of the entries are vulnerable.

5.3 Experimental Methodology
RQ1. Comparison on Method-Level DL-based VD Approaches.

Baselines. We compare IVDetect with the state-of-the-art DL-
based vulnerability detection approaches: 1) VulDeePecker [20]: a
DL-based approach using Bidirectional LSTM on the statements and
their data/control dependencies. 2) Devign [37]: an DL-based ap-
proach that uses GGCN model with Gated Graph Recurrent Layers
on the AST, CFG, DFG, and code sequences for graph classification.
3) SySeVR [19]: in addition to statements and program dependen-
cies, this approach also uses program slicing and leverages several
DL models (LR, MLP, DBN, CNN, LSTM, etc.). 4) Russell et al. [27]:
This DL approach encodes source code as matrices of code tokens
and leverages convolution model with random forest (RF) via en-
semble classifier. 5) Reveal [11]: This approach uses GGNN, MLP,
and with Triplet Loss on graph representations of source code.

Procedure. A dataset contains a number of vulnerable and non-
vulnerable methods. We first randomly split all of its vulnerable
methods into 80%, 10%, and 10% to be used for training, tuning,
and testing, respectively. For training, we add to that 80% part the
same number of non-vulnerable methods as the vulnerable ones to
obtain the balanced training data. For tuning and testing, we also
add the non-vulnerable methods but we use the real ratio between
vulnerable and non-vulnerable methods in the original dataset to
build tuning/testing data. We use AutoML [21] on all models to
automatically tune hyper-parameters on the tuning dataset.

We also performed the evaluation across the datasets. We first
trained our model on the combination of two datasets Reveal and
FFMPeg+Qemu, which has a balanced number of vulnerable methods
and non-vulnerable ones. We then tested the model on Fan dataset,
which has a more realistic ratio of vulnerable and non-vulnerable
methods. To ensure the model suitable for cross-data evaluation,
we also used 20% of Fan dataset for tuning the parameters and
performed prediction on the remaining 80%.

Evaluation Metrics: We use the following evaluation metrics.

(cid:205)ğ‘„

ğ‘=1

ğ‘˜=1

Mean Average Precision ğ‘€ğ´ğ‘ƒ =

ğ´ğ‘£ğ‘”ğ‘ƒ (ğ‘)
, with Average
ğ‘„
Precision ğ´ğ‘£ğ‘”ğ‘ƒ = (cid:205)ğ‘›
ğ‘ƒ (ğ‘˜)ğ‘Ÿğ‘’ğ‘™ (ğ‘˜), where ğ‘› is the total number
of results ğ‘˜ is the current rank in the list, ğ‘Ÿğ‘’ğ‘™ (ğ‘˜) is an indicator
function equaling to 1 if the item at rank ğ‘˜ is actually vulnerable,
and to zero otherwise. ğ‘„ is the total number of classification types.
It is 1 because we only have two types including vulnerable and
non-vulnerable classes, however, we rank all the methods based on
their scores (1 indicates vulnerable, and 0 otherwise).

ğ·ğ¶ğºğ‘˜
ğ¼ ğ·ğ¶ğºğ‘˜ , with Discounted Cu-
Normalized DCG at ğ‘˜: ğ‘›ğ·ğ¶ğºğ‘˜ =
mulative Gain at rank ğ‘˜, ğ·ğ¶ğºğ‘˜ = (cid:205)ğ‘˜
ğ‘™ğ‘œğ‘”2 (ğ‘–+1) ; and Ideal DCG
ğ‘–=1
at ğ‘˜ ğ¼ğ·ğ¶ğºğ‘˜ = (cid:205)|ğ‘…ğ‘˜ |
2ğ‘Ÿğ‘– âˆ’1
ğ‘™ğ‘œğ‘”2 (ğ‘–+1) ; where ğ‘Ÿğ‘– is the score of the result
ğ‘–=1
at position ğ‘–, and ğ‘…ğ‘˜ the rank of the actual vulnerable methods
(ordered by their scores) in the resulting list up to the position ğ‘˜.
First Ranking (ğ¹ğ‘…) is the rank of the first correctly predicted
vulnerable method. Average ranking (ğ´ğ‘…) is the average rank of
the correctly predicted vulnerable methods in the top-ranked list.
Accuracy under curve (AUC) is defined as ğ´ğ‘ˆ ğ¶ = ğ‘ƒ (ğ‘‘ (ğ‘š1) >
ğ‘‘ (ğ‘š2)) in which ğ‘ƒ is the probability, ğ‘‘ is the detection model (can
be regarded as a binary classifier), ğ‘š1 is a randomly chosen positive
instance, and ğ‘š2 is a randomly chosen negative instance.

Precision (P) is the fraction of relevant instances among the
ğ‘‡ ğ‘ƒ +ğ¹ ğ‘ƒ while ğ‘‡ ğ‘ƒ is

retrieved ones. It is calculated as ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ ğ‘ƒ

ğ‘Ÿğ‘–

Vulnerability Detection with Fine-Grained Interpretations

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

the number of true positives and the ğ¹ ğ‘ƒ is the number of false
positives.

Recall (R) is the fraction of relevant instances that were retrieved.
ğ‘‡ ğ‘ƒ +ğ¹ ğ‘ while ğ‘‡ ğ‘ƒ is the number of true

It is calculated as ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = ğ‘‡ ğ‘ƒ
positives and the ğ¹ ğ‘ is the number of false negatives.

F score (F) is the harmonic mean of precision and recall. It is

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
calculated as ğ¹ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ = 2
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ .
RQ2. Comparison with other Interpretation Models for Fine-
grained Interpretation.

Baselines. We compare IVDetect with the following interpre-
tation models. 1) ATT [36]: This approach is a graph attention
network that uses the attention mechanism to evaluate the weights
(importance levels) of the edges in the input graph. 2) GRAD [36]:
This approach is a gradient-based method that computes the gradi-
ent of the GNNâ€™s loss function w.r.t. the adjacency matrix.

Procedure. Our goal here is to evaluate how well IVDetect
produces the fine-grained interpretations pointing to vulnerable
statements. Thus, to train/test the interpretation model, we need to
use the Fan dataset because it contains the vulnerable statements
and respective fixes. The other two datasets contain only the vul-
nerabilities at the method level and no fixes. Therefore, in this RQ2,
for the vulnerability prediction part, we used the GCN-FA model
that was trained on Reveal and FFMPeg+Qemu and predicted on the
Fan dataset. For the methods that are vulnerable, but predicted as
non-vulnerable, we considered those cases as incorrect because the
resulting interpretations do not make sense for incorrect detection.
For the methods that are actually non-vulnerable (regardless of the
predictions), we could not use them because the non-vulnerable
methods do not have the fixed statements as the ground truth for
interpretations. Thus, we use the set of methods that are vulnera-
ble and correctly detected as vulnerable for the evaluation of the
interpretation model. Let us use ğ· to denote this set.

For the interpretation, we randomly split ğ· into 80%, 10%, and
10% for training, tuning, and testing. For training, we used the fixed
statements as the labels for interpretation because those fixed ones
were the vulnerable ones. For testing, we compared the relevant
statements from the interpretation model against the actual fixed
statements. Each method in the testing set and the trained GCN-FA
model are the input of the interpretation model in this RQ2.

Evaluation Metrics. Given an interpretation sub-graph Gğ‘€ gen-
erated from the graph-based interpretation model, we evaluate the
accuracy of the interpretation for a model as follows. For a method,
if Gğ‘€ has an overlap with any statement in the code changes that fix
the vulnerability, Gğ‘€ is considered as a correct interpretation, i.e.,
relevant to the VD. We then calculate Accuracy as the ratio between
the number of correct interpretations over the total number of inter-
pretations. Because code changes could include addition, deletion,
and modification, we further define such overlap as follows.

If one of the statements ğ‘† in the vulnerable version was deleted
or modified for fixing, and if Gğ‘€ âˆ‹ ğ‘†, then we consider the inter-
pretation sub-graph Gğ‘€ is correct, otherwise incorrect. If one of
the statements ğ‘† â€² was added to the vulnerable version for fixing,
we check on the fixed version whether Gğ‘€ contains any statement
with data or control dependencies with ğ‘† â€², we consider it as cor-
rect, otherwise, incorrect. For example, in Fig. 2, Gğ‘€ contains the
statement S23 with data and control dependencies with one of the

Table 2: RQ1. Top-10 Vulnerability Detection Ranked Re-
sults on FFMPeg+Qemu Dataset. 0: incorrect, 1: correct

Top-10 result
VulDeePecker
SySeVR
Russell et al.
Devign
Reveal
IVDetect

1
0
0
0
0
0
1

2
0
0
0
0
0
0

3
0
0
0
0
0
1

4
0
0
0
0
1
1

5
0
0
1
1
0
1

6
0
1
0
0
1
0

7
1
1
1
1
0
1

8
0
1
0
1
1
1

9
1
0
1
1
1
0

10
1
1
1
0
1
0

Total
3
4
4
4
5
6

added lines from 17â€“21. Thus, Gğ‘€ is correct. The rationale is that if
the interpretation sub-graph Gğ‘€ contains some statement relevant
to the added statement to fix the vulnerability, that interpretation
is useful in pointing out the code relevant to the vulnerability.

We also use Mean First Ranking (MFR), i.e., the mean of the
rankings for the first statement that needs to be fixed in the inter-
pretation statements, and Mean Average Ranking (MAR), i.e., the
mean of the rankings for all statements to be fixed in the interpreta-
tion statements. If a statement to be fixed has not been selected as
interpretation, we do not consider it when calculating MFR/MAR.
RQ3. Vulnerable Code Patterns and Fixing Patterns.

Procedure. We use a mining algorithm on the set of interpretation
sub-graphs to mine patterns of vulnerable code. We also mine fixing
patterns for those vulnerabilities. See details in Section 6.3.
Evaluation Metrics. We counted the identified patterns.

RQ4. Sensitivity Analysis for Features.

Procedure. We first built a base model with only the feature
that represents the code as the sequence of tokens. We then built
other variants of our model by gradually adding one more feature
in Section 3.1 to the base model including the sequence of sub-
tokens, AST subtree, variable names, data dependencies, and control
dependencies. We measured accuracy for each variant. We used the
Fan dataset and the same experiment setting as in RQ1.

Evaluation Metrics. We use the same metrics as in RQ1.

RQ5. Sensitivity Analysis for Training Data. We used different
ratios in data splitting for training, tuning, and testing: (80%, 10%,
10%), (70%, 15%, 15%), (60%, 20%, 20%), and (50%, 25%, 25%). We used
the same Fan dataset and setting as in RQ1.

Evaluation Metrics. We use the same metrics as in RQ1.

RQ6. Time Complexity Analysis. We measure the actual train-
ing and predicting time.

6 EXPERIMENTAL RESULTS
6.1 RQ1. Comparison on Method-Level VD

In Table 2, among the top 10 prediction results, IVDetect has the
most correct predictions (6 vulnerable methods). The vulnerable
methods correctly detected by IVDetect are also pushed higher
in the top-10 ranked list with 4 correct results out of 5 top results.
All other baselines have only 0â€“1 correct detection in the top-5 list.
Importantly, the first rank for IVDetect (i.e., the rank of the first
correctly detected vulnerable methods) is 1st, while those of the
baselines are 4ğ‘¡â„, 5ğ‘¡â„, 5ğ‘¡â„, 6ğ‘¡â„, and 7ğ‘¡â„ (the bold values in Table 2).
Moreover, IVDetect can detect 14, 35, and 64 vulnerabilities among
top-20, top-50, and top-100 prediction results.

Tables 3, 4, and 5 show the comparison among the approaches
on three datasets. IVDetect consistently performs better in all
the metrics (Table 3). For nDCG@{1,3}, all the baselines get zeros

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

Table 3: RQ1. Method-Level VD on FFMPeg+Qemu Dataset

Table 4: RQ1. Method-Level VD on Fan Dataset

VulDee-
-Pecker
0
0
0
0.37
0.45
0.48
0
0
0
0.22
0.29
0.32
n/a
n/a
7
7
7
7
n/a
n/a
n/a
8.7
11.2
13.3
0.68

SySeVR

0
0
0
0.44
0.48
0.51
0
0
0
0.31
0.33
0.35
n/a
n/a
6
6
6
6
n/a
n/a
n/a
7.8
10
12.1
0.72

Russell
et al.
0
0
0.43
0.45
0.49
0.54
0
0
0.20
0.30
0.34
0.37
n/a
n/a
5
5
5
5
n/a
n/a
5
7.8
9.5
12.6
0.79

nDCG@1
nDCG@3
nDCG@5
nDCG@10
nDCG@15
nDCG@20
MAP@1
MAP@3
MAP@5
MAP@10
MAP@15
MAP@20
FR@1
FR@3
FR@5
FR@10
FR@15
FR@20
AR@1
AR@3
AR@5
AR@10
AR@15
AR@20
AUC

Devign

Reveal

IVDetect

0
0
0.45
0.46
0.52
0.56
0
0
0.20
0.32
0.37
0.42
n/a
n/a
5
5
5
5
n/a
n/a
5
7.4
10
12.1
0.77

0
0
0.5
0.5
0.55
0.6
0
0
0.25
0.38
0.41
0.45
n/a
n/a
4
4
4
4
n/a
n/a
4
7.4
9.1
12.4
0.79

1
0.63
0.65
0.68
0.75
0.82
1
0.83
0.80
0.78
0.72
0.69
1
1
1
1
1
1
1
2
3.3
4.7
7.6
10.3
0.84

because they did not have correct detections in top-3 results. IVDe-
tect can improve nDCG@10 from 43%â€“84% and nDCG@20 from
37%â€“71% as compared to the baselines. Higher nDCG indicates that
IVDetect achieves the ranking closer to the perfect ranking and
the correct vulnerable methods appear higher in the top list.

For MAP scores, IVDetect relatively improves over the baselines
from 105%â€“255% for top-10 and from 53%â€“116% for top 20. With
higher MAP, IVDetect has higher precision on average for all the
top-ranked positions in the top list. That is, the top-ranked result
is highly precise in detecting the vulnerable methods.

IVDetect also achieves better first ranking (FR) and average
ranking (AR). While its best FR is 1 and that of next best performer
is 4. For AR@10, a correct vulnerable method is on average ranked
by IVDetect 2.7â€“4.0 positions higher in the ranked list than by the
baselines. Our tool also has relatively higher AUC from 6%â€“24%.

The comparative results on Fan and Reveal datasets are similar
(Tables 4 and 5). In Fan dataset, IVDetect can improve the nDCG
and MAP scores over the baselines by 26%â€“43%, 50%â€“170% for top-
10, and 21%â€“475%, 40%â€“250% for top-20. IVDetectâ€™s FRs and ARs
are better from 2â€“6 positions and 0.7â€“2.7 positions for top 10, and
2â€“13 positions and 1.6â€“9.1 positions for top 20. In Reveal dataset,
the improvements in nDCG, MAP, FR, and AR are 33%â€“73%, 42%â€“
209%, 2â€“7 positions, and 0â€“4 positions for top 10, and 19%â€“111%,
28%â€“236%, 2â€“12 positions, and 1.2â€“6.2 positions for top 20.

The results on three datasets are different due to the ratio be-
tween the vulnerable and non-vulnerable methods. That ratio is
1:16 and 1:9.9 in Fan and Reveal datasets. That number is 1:1.2 in
FFMPeg+Qemu dataset, thus, there are more vulnerable methods,
and the results are consistently higher across all the models.

Table 6 shows the results of Precision and Recall of our IVDe-
tect and the baselines. Specifically, IVDetect has higher precision
than all the baselines on three datasets. IVDetect can improve
the Precision by 2.6%-105%. For the Recall, IVDetect is marginally
lower than Reveal on Fan and FFMPeg+Qemu datasets (i.e., 1.4%

VulDee-
-Pecker
0
0
0
0
0.08
0
0
0
0
0.08
n/a
n/a
n/a
n/a
19
n/a
n/a
n/a
n/a
19.5
0.72

SySeVR

0
0
0
0
0.23
0
0
0
0
0.24
n/a
n/a
n/a
n/a
16
n/a
n/a
n/a
n/a
18
0.81

Russell
et al.
0
0
0.30
0.28
0.31
0
0
0.1
0.12
0.14
n/a
n/a
10
10
10
n/a
n/a
10
12
13.3
0.82

nDCG@1
nDCG@5
nDCG@10
nDCG@15
nDCG@20
MAP@1
MAP@5
MAP@10
MAP@15
MAP@20
FR@1
FR@5
FR@10
FR@15
FR@20
AR@1
AR@5
AR@10
AR@15
AR@20
AUC

Devign

Reveal

IVDetect

0
0
0.33
0.30
0.32
0
0
0.13
0.14
0.15
n/a
n/a
8
8
8
n/a
n/a
8
10.5
13.3
0.75

0
0
0.34
0.37
0.38
0
0
0.18
0.21
0.20
n/a
n/a
6
6
6
n/a
n/a
8
9.3
12
0.82

0
0.5
0.43
0.45
0.46
0
0.25
0.27
0.28
0.28
n/a
4
4
4
4
n/a
4
7.3
8.5
10.4
0.9

Table 5: RQ1. Method-Level VD on Reveal Dataset

VulDee-
-Pecker
0
0
0
0
0.26
0.27
0
0
0
0
0.07
0.11
n/a
n/a
n/a
n/a
15
15
n/a
n/a
n/a
n/a
15
18
0.65

SySeVR

0
0
0
0.30
0.28
0.33
0
0
0
0.11
0.12
0.15
n/a
n/a
n/a
10
10
10
n/a
n/a
n/a
10
12.5
15.5
0.76

Russell
et al.
0
0
0
0.32
0.32
0.35
0
0
0
0.11
0.16
0.18
n/a
n/a
n/a
9
9
9
n/a
n/a
n/a
9
12
13.3
0.75

nDCG@1
nDCG@3
nDCG@5
nDCG@10
nDCG@15
nDCG@20
MAP@1
MAP@3
MAP@5
MAP@10
MAP@15
MAP@20
FR@1
FR@3
FR@5
FR@10
FR@15
FR@20
AR@1
AR@3
AR@5
AR@10
AR@15
AR@20
AUC

Devign

Reveal

IVDetect

0
0
0
0.34
0.39
0.43
0
0
0
0.18
0.23
0.36
n/a
n/a
n/a
7
7
7
n/a
n/a
n/a
8
10.5
12.7
0.72

0
0
0.43
0.39
0.42
0.48
0
0
0.2
0.24
0.25
0.29
n/a
n/a
5
5
5
5
n/a
n/a
5
6
9.8
13
0.74

0
0.63
0.53
0.52
0.55
0.57
0
0.33
0.37
0.34
0.36
0.37
n/a
3
3
3
3
3
n/a
3
4
6
9.5
11.8
0.81

Table 6: RQ1. Precision and Recall Results of Method-Level
VD on Three Datasets (P: Precision; R: Recall; F: F score)

FFMPeg+Qemu
F
R
P
0.35
0.27
0.49
0.56
0.66
0.50
0.45
0.41
0.55
0.57
0.63
0.52
0.73
0.62
0.55
0.65
0.60
0.72

Fan
R
0.49
0.74
0.48
0.52
0.74
0.72

P
0.12
0.15
0.16
0.18
0.19
0.23

Reveal
R
0.14
0.42
0.12
0.32
0.58
0.52

P
0.19
0.24
0.26
0.33
0.31
0.39

F
0.17
0.31
0.16
0.32
0.40
0.45

F
0.19
0.27
0.24
0.26
0.30
0.35

VulDeePecker
SySeVR
Russell et al.
Devign
Reveal
IVDetect

and 2.7%) and SySeVR on Fan dataset (i.e., 2.7%). On the Reveal
Dataset, IVDetect can improve Reveal by 25.8% in terms of Preci-
sion, but decrease Recall by 10.3%. However, in terms of F1 score,
IVDetect can improve the best performed baseline Reveal by 4.8%

Vulnerability Detection with Fine-Grained Interpretations

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Table 7: RQ2. Fine-grained VD Interpretation Comparison

Accuracy

Interp.
Model N1 N2 N3 N4 N5 N6 N7 N8 N9 N10
6.3
0.01 0.16 0.41 0.54 0.59 0.60 0.62 0.63 0.64 0.65 4.8
ATT
5.6
GRAD 0.01 0.19 0.43 0.54 0.59 0.62 0.63 0.65 0.66 0.67 4.2
GE
5.0
0.05 0.30 0.54 0.63 0.67 0.68 0.70 0.72 0.72 0.73 3.5
GE: GNNExplainer; Nx: x is the number of nodes in the interpretation

MFR MAR

(a) MAP Scores

(b) nDCG Scores

Figure 7: Scores from Top 1 to Top 100 on Fan Dataset

Table 8: RQ3. Numbers of Vulnerable Code Patterns

size=2
size=3
size=4
size=5
Total

thres=2
47
25
23
22
117

thres=3
36
27
22
21
102

thres=4
22
19
16
11
68

thres=5
7
6
5
2
21

statement level. That is, in more cases, if IVDetect detects correctly
vulnerable methods, it can point out more precisely the vulnerable
statements relevant to the vulnerabilities. For ranking vulnerable
statements, using GNNExplainer improves MFR by 0.7 and 1.3 ranks,
and improves MAR by 0.6 and 1.3 ranks over ATT and GRAD.

ATT uses the edge attention in the Graph Attention Network to
assign the weights for the edges, while GNNExplainer directly gives
a score for the subgraph after masking. Thus, for the case in which
there are more than one path from a node to another, the weight
for an edge is the average weight of the weights through multiple
paths, i.e., ATT might be less precise than GNNExplainer. GRAD
computes the gradient of the loss function with respect to the input
for computing the weight of an edge. However, such gradient-based
approach may not perform well with respect to the discrete inputs
(an input graph is represented as an adjacency matrix).

As the number of nodes in Gğ‘€ increases, the number of state-
ments covered also increases, accuracy is higher. However, the com-
putation time is higher and developers need to investigate more
statements. As seen, when the number of statements is higher than
5, accuracy increases more slowly. Thus, we chose 5 as a default.

6.3 RQ3. Vulnerable Code Pattern Analysis

This section describes another experiment that we exploit IVDe-
tectâ€™s capability of providing interpretation sub-graphs to mine the
patterns of vulnerable code. A vulnerable code pattern is a fragment
of vulnerable code that repeats frequently, i.e., more than a certain
threshold. The detected vulnerability patterns and corresponding
fixes can be the good sources for developers to learn about the
vulnerable code that others have frequently made, and learn to fix
vulnerable code in the same patterns.

From the results in RQ2, we first collected into a set G the inter-
pretation sub-graphs Gğ‘€ s with the correctly detected statements
as relevant to the vulnerability in the methods. In total, we obtain
+700 Gğ‘€ s. Note that Gğ‘€ is a sub-graph of PDG. For each Gğ‘€ , we
abstract out the variablesâ€™ names with a keyword VAR, and the
literals with their data types. We then ran the sub-graph pattern
mining algorithm [24] on G with different thresholds of frequen-
cies and collected different sizes of the sub-graph patterns. The
outputs are the frequent isomorphic sub-graphs within Gğ‘€ s, which

(a) nDCG & MAP Scores

(b) FR and AR

Figure 8: RQ1. Cross-Dataset Validation: Training on Reveal
and FFMPeg+Qemu Datasets, testing on Fan Dataset.

Figure 9: Overlapping Analysis

on FFMPeg+Qemu dataset, 16.7% on Fan dataset, and 12.5% on the
Reveal Dataset.

Figure 7 shows that IVDetect consistently has better MAP and

nDCG scores when considering top-1 to top-100 ranked lists.

For cross-dataset validation, as seen in Figure 8, the results for
MAP and nDCG in within-dataset setting are better than those in
cross-dataset setting. This is expected because the model might see
similar vulnerable code before in the same projects in the same
dataset. The FR and AR values for cross-dataset setting are one rank
higher than those of within-dataset setting.

Figure 9 shows our analysis on the overlapping results between
IVDetect and the baselines on Fan dataset for top-100. As seen,
IVDetect can detect 17, 13, 13, 11, and 10 vulnerable methods
that VulDeePecker, SySeVR, Russell, Devign, and Reveal missed,
respectively, while they can detect only 2,3, 4, 5, and 5 vulnerable
methods that IVDetect missed. In summary, IVDetect can detect
15, 10, 9, 6, and 5 more vulnerable methods than the baselines.

6.2 RQ2. Comparison with other Interpretation
Models for Fine-grained VD Interpretation

Table 7 shows the accuracy of different interpretation models. As
seen, using GNNExplainer improves over ATT and GRAD from
12.3%â€“400% and 9.0%â€“400% in accuracy, respectively, as we vary
the size of interpretation sub-graphs (i.e., the number of statements)
from 1â€“10. Higher accuracy indicates that IVDetect can provide
better fine-grained vulnerability detection interpretation at the

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

Table 9: RQ4. Evaluation for the Impact of Internal Features.

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9
10
11
12
13

// ========================PATTERN 1 =======================================

if (is_link(STRINGLITERAL)) {

fprintf(stderr, "Error: invalid /etc/skel/.zshrc file\n"); // not in pattern
exit(INTLITERAL);

}
if (copy_file(STRINGLITERAL, VAR) == INTLITERAL) { ...

// ========================PATTERN 2 =======================================

VAR = udf_get_filename(VAR, VAR, VAR, VAR);

if (VAR && ...) goto LABEL;

Figure 10: Vulnerable Code Patterns

// ===================== FIXING PATTERN 1 =========================
- VAR = fl6_update_dst(VAR, VAR, VAR);
+ rcu_read_lock();
+ final_p = fl6_update_dst(VAR, rcu_dereference(VAR), VAR);
+ rcu_read_unlock();
// ===================== FIXING PATTERN 2 =========================
- char VAR = malloc (VAR);
+ char VAR;
+ if (VAR < 0 || VAR > LITCONST) {
+
+
+ }
+ VAR = malloc (VAR);

error_line (STRINGLITERAL, VAR);
return LITCONST;

Figure 11: Fixing Patterns (-: removal, +: addition)

are considered as vulnerable code patterns because we chose Gğ‘€
that contains correct interpretation statements relevant to the cor-
rectly detected vulnerabilities. After manual verification, we obtain
a number of correct patterns (Table 8). As seen, as the frequency
threshold or the size of pattern is larger, the number of patterns
decreases as expected. When they are both larger than 5, we found
no pattern. Let us explain a few examples.

Figure 10 shows two examples of vulnerable code patterns. The
first pattern (lines 2,4, and 6) shows an API misuse in the project
firejail involving is_link(...), exit, and copy_file(...). The usage is to check
the validity of a link, and if yes to copy the file, or otherwise to
stop the execution. This pattern appeared three times with different
string literals and was fixed by developers to replace the statements.
An interesting observation is that IVDetect is able to eliminate
the fprintf statement at line 2 from the interpretation sub-graph,
thus, eliminating it from the pattern, even though the fprintf state-
ment appears with the other statements three times in the project.
This shows a benefit of IVDetect because if a tool does not have
statement-level VD interpretation and it mines pattern from the
entire methods, it will incorrectly include fprintf in the pattern. The
second pattern (lines 8â€“9) shows a pattern involving a vulnerable
method call udf_get_filename, and the checking on its return value.
The method later was fixed to add the 5ğ‘¡â„ parameter.

Another interesting finding is that IVDetect enables the discov-
ery of not only vulnerable code patterns but also the fixing patterns
for them. Figure 11 shows two fixing patterns for vulnerable code.
The first vulnerability (from Linux kernel), lines 2â€“5, is about the
method f16_update_dst(...). According to the commit log, to avoid an-
other thread changing a data record concurrently, developers need
to provide mutual exclusion access and deferencing. This fixing
pattern was repeated 3 times in the methods dccp_v6_send_response,
inet6_csk_route_req, and net6_csk_route_socket. This fixing pattern
would be useful for a developer to learn the fix from one method
and apply to the other two methods. The second pattern (lines 7â€“13)
shows a fixing pattern to a vulnerability on buffer overflow with
the malloc call in ParseDsdiffHeaderConfig method of WavPack 5.0.
According to CVE-2018-7253, this problem â€œallows a remote attacker

(B)+AST
(C)
0.29
0.29
0.12
0.13
11
11
11
13.5
0.77
ST: sequence of tokens; SST: sequence of sub-tokens; AST: sub-AST; Var: variables;
CD: control dependencies; DD: data dependencies; F = IVDetect

nDCG@15
nDCG@20
MAP@15
MAP@20
FR@15
FR@20
AR@15
AR@20
AUC

(A)+SST
(B)
0.27
0.27
0.11
0.11
12
12
13.5
15
0.76

(C)+Var
(D)
0.35
0.37
0.19
0.19
7
7
10.3
12.5
0.83

(D)+CD
(E)
0.42
0.44
0.26
0.26
5
5
9
11.2
0.85

(E)+DD
(F)
0.45
0.46
0.28
0.28
4
4
8.5
10.4
0.9

ST
(A)
0.25
0.26
0.07
0.09
14
14
14
19.5
0.75

to cause a denial-of-service (heap-based buffer over-read) or possibly
overwrite the heap via a maliciously crafted DSDIFF fileâ€. This fixing
pattern occurred three times in the same project.

6.4 RQ4. Sensitivity Analysis for Features

Table 9 shows the changes to the metrics as we incrementally added
each internal feature into our model in Figure 4. Generally, each
internal feature contributes positively to the better performance of
IVDetect, as both the score metrics (nDCG, MAP, and AUC) and
the ranking metrics (FR and AR) are improved.

When IVDetect considers only the sequence of tokens (ST) in
the code, the first correct detection (FR) is at the position 14, thus,
nDCG@{1,5,10}=0 and MAP@{1,5,10}=0 (not shown). When consid-
ering the code as the sequence of sub-tokens (SST), IVDetect deals
with the unique tokens better because the sub-tokens appear more
frequently than the tokens [31]. At top-20, FR improves 2 positions,
AR improves 4.5 positions, and nDCG and MAP relatively improve
3.8% and 22.2%. When AST is additionally considered, the model can
distinguish vulnerable code structures and statements. At top-20,
FR and AR improve 1 and 1.5 positions, and nDCG and MAP im-
prove 7.4% and 18.1%. However, FR is still 11 and nDCG@{1,5,10}=0
and MAP@{1,5,10}=0 (not shown), because tokens and AST do not
help much discriminate the vulnerable statements.

The feature on variables also helps improve FR and AR from 11 to
7 and 13.5 to 12.5, and nDCG and MAP relatively improve 27.6% and
46.2% at top 20. nDCG@10 and MAP@10 improve from 0 to 0.33 and
to 0.18, respectively (not shown). This feature allows the model to
detect similar incorrect variable usages. By additionally integrating
control dependencies (CD), FR and AR improve from 7 down to 5
and 12.5 down to 11.2, and nDCG and MAP relatively improve 18.9%
and 36.8%. By adding data dependencies (DD), FR and AR improve
from 5 to 4 and 11.2 to 10.4. nDCG and MAP improve 4.5% and 7.7%
for top 20. This result confirms that vulnerable code often involves
the statements with control and/or data dependencies [11, 37].

Figure 12 shows a detected vulnerable method: validate_event(...)
was vulnerable and replaced with a new version with an additional
parameter. We used the models (A)â€“(F) for detection, and observed
that the rank for validate_event(...) in the candidate list improves
from 140 (A), to 121 (B), 99 (C), 71 (D), 48 (E), and 19 (F). While the
features on tokens, sub-tokens, and AST are contributing, they do
not help much because the model did not see them in vulnerable
methods before. However, the variable/method names, especially
control/data dependencies between the surrounding statements and
validate_event(...) help discriminate this vulnerability, and push it to

Vulnerability Detection with Fine-Grained Interpretations

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

static int validate_group(struct perf_event *event)
{
-
+

if (!validate_event(&fake_pmu, leader))
if (!validate_event(event->pmu, &fake_pmu, leader))

...

return -EINVAL;

list_for_each_entry(sibling, &leader->sibling_list, group_entry) {

if (!validate_event(&fake_pmu, sibling))
if (!validate_event(event->pmu, &fake_pmu, sibling))

return -EINVAL;

}

if (!validate_event(&fake_pmu, event))
if (!validate_event(event->pmu, &fake_pmu, event))

return -EINVAL; ...

-
+

-
+

}

Figure 12: A Detected Vulnerable Method in Android kernel

Table 10: RQ5. Sensitivity Analysis on Training Data

Train/Tune/Test
40%/30%/30%
50%/25%/25%
60%/20%/20%
70%/15%/15%
80%/10%/10%

nDCG@20 MAP@20

0.26
0.33
0.43
0.44
0.46

0.09
0.16
0.25
0.26
0.28

FR@20
12
8
5
5
4

AR@20
15.5
12.3
11.6
11.2
10.4

AUC
0.69
0.74
0.85
0.87
0.9

the top-20 list. Control dependencies (e.g., between validate_event(...)
and return -EINVAL) help improve 29 ranks. Generally, the improve-
ment in ranking shows the positive contributions of all the features.
This example also shows a fixing pattern appearing three times

with different variables leader, sibling, and event.

6.5 RQ5. Sensitivity Analysis on Training Data

As seen in Table 10, with more training data, the performance is
better as expected. Even with 60%/20%/20%, IVDetect still achieves
nCDG of 0.43 and MAP of 0.25, which are still higher than those
of the other baselines for top 20 (highest nDCG and MAP of the
baselines are 0.38 and 0.20). With 20% less training data (60% vs
80%), IVDetect only drops AUC by 5.5%.
Time Complexity. To generate the interpretation sub-graphs for
all methods, it takes about 9 days, 2 days, and 3 days to finish on
Fan, Reveal, and FFMPeg+Qemu datasets, respectively. It took 23,
7, 10 hours to train IVDetect on Fan, Reveal, and FFMPeg+Qemu
datasets. For VD prediction, it takes only 1-2s per method.
Threats to Validity. We only tested on the vulnerabilities in C and
C++ code. In principle, IVDetect can apply to other programming
languages. We tried our best to tune the baselines on same dataset
for fair comparisons. We focus only on DL-based VD models.
7 RELATED WORK

Various techniques have been developed to detect vulnerabilities.
The rule-based approaches were developed to leverage known vul-
nerability patterns to discover possible vulnerable code, such as
FlawFinder [7], RATS [9], ITS4 [33], Checkmarx [1], Fortify [8]
and Coverity [2]. Typically, the patterns are manually defined by
human experts. The state-of-the-art vulnerability detection tools
using static analysis provide the rules for each vulnerability type.
Another type is machine learning (ML)-based or metrics-based.
Typically, these approaches require the human-crafted or summa-
rized metrics as features to characterize vulnerabilities and train ma-
chine learning models on the defined features to predict whether a
given code is vulnerable or not. Various ML-based approaches have

been built on top of distinct metrics, such as terms and their occur-
rence frequencies [28], imports and function calls [23], complexity,
code churn, and developer activity [30], dependency relation [22],
API symbols and subtrees [34, 35].

Recently, deep learning (DL) has been applied to detect vulnera-
bilities. For example, some approaches train a DL model on different
code representations to detect vulnerabilities, such as the lexical
representations of functions in a synthetic codebase [14], code snip-
pets related to API calls to detect two types of vulnerabilities [20],
syntax-based, semantics-based, and vector representations [19],
graph-based representations [37]. None of them is designed to pro-
vide interpretations for a model in term of vulnerable statements.

8 CONCLUSION

We present IVDetect, a novel DL-based approach to provide sub-
graphs in PDG, that explains the prediction results of graph-based
vulnerability detection. Our empirical evaluation on vulnerability
databases shows that IVDetect outperforms the existing DL-based
approaches by 64%â€“122% and 105%â€“255% in top-10 nDCG and MAP
ranking scores.

Our key limitations include 1) un-seen vulnerabilities, 2) the
vulnerable statements incorrectly identified due to data/control
dependencies with vulnerable ones, 3) missed vulnerable statements
due to multiple edges of data/control dependencies.

With IVDetect being a ML/DL-based vulnerability detection
model, we aim to raise the level of ML/DL-based approaches, which
are not able to point out the statements that caused the model to
predict the vulnerability. Thus, we compared IVDetect with the
detection approaches of the same category, rather than with static-
analysis tools. In the future, we plan to compare IVDetect with
static analysis tools.

ACKNOWLEDGMENTS

This work was supported in part by the US National Science Foun-
dation (NSF) grants CCF-1723215, CCF-1723432, TWC-1723198,
CCF-1518897, and CNS-1513263.

REFERENCES
[1] [n.d.]. Checkmarx. https://www.checkmarx.com/
[2] [n.d.]. Coverity. https://scan.coverity.com/
[3] [n.d.]. CWE-120: Buffer Overflow. https://cwe.mitre.org/data/definitions/120.html
[4] [n.d.]. CWE-290: Authentication Bypass by Spoofing. https://cwe.mitre.org/data/

definitions/290.html

[5] [n.d.]. CWE-79: Cross-site Scripting. http://cwe.mitre.org/data/definitions/79.html
[6] [n.d.]. CWE-89: SQL Injection. https://cwe.mitre.org/data/definitions/89.html
[7] [n.d.]. FlawFinder. http://www.dwheeler.com/FlawFinder
[8] [n.d.]. HP Fortify. https://www.hpfod.com/
[9] [n.d.]. RATS: Rough Audit Tool for Security. https://code.google.com/archive/p/

rough-auditing-tool-for-security/

[10] 2021.

The GitHub Repository for This Study.

https://github.com/

vulnerabilitydetection/VulnerabilityDetectionResearch

[11] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2020.
Deep Learning based Vulnerability Detection: Are We There Yet? arXiv preprint
arXiv:2009.07235 (2020).

[12] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).

[13] Jiahao Fan, Yi Li, Shaohua Wang, and Tien Nguyen. 2020. A C/C++ Code Vulner-
ability Dataset with Code Changes and CVE Summaries. In The 2020 International
Conference on Mining Software Repositories (MSR). IEEE.

[14] Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher Reale, Rebecca Russell,
Louis Kim, et al. 2018. Learning to repair software vulnerabilities with generative

ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece

Li, Wang, and Nguyen

adversarial networks. In Advances in Neural Information Processing Systems. 7933â€“
7943.

[15] Jacob A Harer, Louis Y Kim, Rebecca L Russell, Onur Ozdemir, Leonard R Kosta,
Akshay Rangamani, Lei H Hamilton, Gabriel I Centeno, Jonathan R Key, Paul M
Ellingwood, et al. 2018. Automated software vulnerability detection with machine
learning. arXiv preprint arXiv:1803.04497 (2018).

[16] Fabian Keller, Lars Grunske, Simon Heiden, Antonio Filieri, Andre van Hoorn,
and David Lo. 2017. A critical evaluation of spectrum-based fault localization
techniques on a large-scale software system. In 2017 IEEE International Conference
on Software Quality, Reliability and Security (QRS). IEEE, 114â€“125.

[17] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. CoRR abs/1609.02907 (2016). arXiv:1609.02907
http://arxiv.org/abs/1609.02907

[18] Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019. Improving bug
detection via context-based code representation learning and attention-based
neural networks. Proceedings of the ACM on Programming Languages 3, OOPSLA
(2019), 1â€“30.

[19] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2018.
Sysevr: A framework for using deep learning to detect software vulnerabilities.
arXiv preprint arXiv:1807.06756 (2018).

[20] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for
vulnerability detection. arXiv preprint arXiv:1801.01681 (2018).

[21] Microsoft. [n.d.]. Neural Network Intelligence. https://github.com/microsoft/nni.

Last Accessed August 28th, 2020.

[22] Stephan Neuhaus and Thomas Zimmermann. 2009. The Beauty and the Beast:
Vulnerabilities in Red Hatâ€™s Packages.. In USENIX Annual Technical Conference.
[23] Stephan Neuhaus, Thomas Zimmermann, Christian Holler, and Andreas Zeller.
2007. Predicting vulnerable software components. In Proceedings of the 14th ACM
conference on Computer and communications security. 529â€“540.

[24] Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H. Pham, Jafar M. Al-Kofahi, and
Tien N. Nguyen. 2009. Graph-Based Mining of Multiple Object Usage Patterns. In
Proceedings of the 7th Joint Meeting of the European Software Engineering Confer-
ence and the ACM SIGSOFT Symposium on The Foundations of Software Engineering
(Amsterdam, The Netherlands) (ESEC/FSE â€™09). Association for Computing Ma-
chinery, New York, NY, USA, 383â€“392. https://doi.org/10.1145/1595696.1595767
[25] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe:
Global Vectors for Word Representation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP). 1532â€“1543. http://www.aclweb.org/anthology/D14-
1162

[26] Nam H Pham, Tung Thanh Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2010.
Detection of recurring software vulnerabilities. In Proceedings of the IEEE/ACM

international conference on Automated software engineering. 447â€“456.

[27] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur
Ozdemir, Paul Ellingwood, and Marc McConley. 2018. Automated vulnerability
detection in source code using deep representation learning. In 2018 17th IEEE
International Conference on Machine Learning and Applications (ICMLA). IEEE,
757â€“762.

[28] Riccardo Scandariato, James Walden, Aram Hovsepyan, and Wouter Joosen. 2014.
Predicting vulnerable software components via text mining. IEEE Transactions
on Software Engineering 40, 10 (2014), 993â€“1006.

[29] Min Shi, Yufei Tang, Xingquan Zhu, and Jianxun Liu. 2019. Feature-attention
arXiv preprint

graph convolutional networks for noise resilient learning.
arXiv:1912.11755 (2019).

[30] Yonghee Shin, Andrew Meneely, Laurie Williams, and Jason A Osborne. 2010.
Evaluating complexity, code churn, and developer activity metrics as indicators
of software vulnerabilities. IEEE transactions on software engineering 37, 6 (2010),
772â€“787.

[31] Trinh Le Son Nguyen, Hung Dang Phan and Tien N. Nguyen. 2020. Suggesting
Natural Method Names to Check Name Consistencies. In Proceedings of the 42nd
International Conference on Software Engineering (ICSE â€™20). ACM Press, 12 pages.
[32] Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved
semantic representations from tree-structured long short-term memory networks.
arXiv preprint arXiv:1503.00075 (2015).

[33] John Viega, Jon-Thomas Bloch, Yoshi Kohno, and Gary McGraw. 2000. ITS4:
A static vulnerability scanner for C and C++ code. In Proceedings 16th Annual
Computer Security Applications Conference (ACSACâ€™00). IEEE, 257â€“267.

[34] Fabian Yamaguchi, Felix Lindner, and Konrad Rieck. 2011. Vulnerability extrapola-
tion: Assisted discovery of vulnerabilities using machine learning. In Proceedings
of the 5th USENIX conference on Offensive technologies. 13â€“13.

[35] Fabian Yamaguchi, Markus Lottmann, and Konrad Rieck. 2012. Generalized
vulnerability extrapolation using abstract syntax trees. In Proceedings of the 28th
Annual Computer Security Applications Conference. 359â€“368.

[36] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. GNNExplainer: Generating Explanations for Graph Neural Networks. In
Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates,
Inc., 9244â€“9255.

[37] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019.
Devign: Effective vulnerability identification by learning comprehensive program
semantics via graph neural networks. In Advances in Neural Information Processing
Systems. 10197â€“10207.

