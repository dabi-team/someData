A Uniﬁed View of SDP-based Neural Network Veriﬁcation through
Completely Positive Programming

2
2
0
2

r
a

M
6

]

C
O
.
h
t
a
m

[

1
v
4
3
0
3
0
.
3
0
2
2
:
v
i
X
r
a

Robin Brown
Stanford University
rabrown1@stanford.edu

Edward Schmerling
Stanford University
schmrlng@stanford.edu

Navid Azizan
MIT
azizan@mit.edu

Marco Pavone
Stanford University
pavone@stanford.edu

Abstract

Verifying that input-output relationships of
a neural network conform to prescribed op-
erational speciﬁcations is a key enabler to-
wards deploying these networks in safety-
critical applications. Semideﬁnite program-
ming (SDP)-based approaches to Rectiﬁed
Linear Unit (ReLU) network veriﬁcation
transcribe this problem into an optimization
problem, where the accuracy of any such
formulation reﬂects the level of ﬁdelity in
how the neural network computation is rep-
resented, as well as the relaxations of in-
tractable constraints. While the literature
contains much progress on improving the
tightness of SDP formulations while main-
taining tractability, comparatively little work
has been devoted to the other extreme, i.e.,
how to most accurately capture the original
veriﬁcation problem before SDP relaxation.
In this work, we develop an exact, convex for-
mulation of veriﬁcation as a completely pos-
itive program (CPP), and provide analysis
showing that our formulation is minimal—
the removal of any constraint fundamentally
misrepresents the neural network computa-
tion. We leverage our formulation to provide
a unifying view of existing approaches, and
give insight into the source of large relaxation
gaps observed in some cases.

1 INTRODUCTION

While neural networks today empower many consumer
products in image and natural language understand-
ing, they have been shown to fail in surprising and
unexpected ways, potentially deterring broad deploy-

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

ment in safety critical settings. One avenue for in-
spiring conﬁdence in neural networks is through the
formal veriﬁcation of safety rules speciﬁed as input-
output relationships describing limits on the expected
In this paper, we consider
behavior of a network.
algorithms that pose veriﬁcation as an optimization
problem, where the objective encodes a metric of sat-
isfaction of the safety rule, the constraints represent
the neural network computation, and the optimal ob-
jective value directly corresponds to conﬁrmation or
denial of the safety rule.

Sound and complete, or exact, veriﬁers must always re-
turn the correct decision, which inherently requires ex-
act representation of the neural network computations.
Due to the NP-completeness of veriﬁcation Katz et al.
(2017), exact veriﬁers face a complexity barrier pro-
hibiting faster than exponential run-time in the worst
case. This suggests that faithfully representing the for-
ward pass of a neural network is at odds with tractable
optimization formulations.

Sound but incomplete, or relaxed, veriﬁers must never
return a false assertion of safety, but may conserva-
tively suggest a network is unsafe when it is truly safe.
The conservatism is a result of approximating the neu-
ral network computation, and is the trade-oﬀ for im-
In the context of optimization-
proved tractability.
based veriﬁcation, this is achieved by loosening ex-
act constraints into approximate constraints; the mis-
match between corresponding optimal objective values
is termed the relaxation gap. In this work we aim to
develop a deeper understanding of how to tune the bal-
ance between tightness and eﬃciency, motivated by
the central challenge of devising a systematic family
of relaxations with exact representation of neural net-
work computations as a limiting case.

Contributions. Our contributions are twofold:

1. We develop an exact, convex formulation of the
veriﬁcation problem as a completely positive pro-
gram (CPP); these are linear optimization prob-
lems over the cone of completely positive ma-
trices. While the complexity of veriﬁcation is
not resolved by the proposed formulation, it is

 
 
 
 
 
 
A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

packaged entirely in the complete positivity con-
straint, with the neural network computation be-
ing exactly represented by tractable linear con-
straints. This gives a clean separation between
the two competing desiderata of accuracy and
tractability in relaxed veriﬁcation, opening the
door for new classes of veriﬁers that predictably
trade-oﬀ tightness and eﬃciency.

2. We also provide analysis explaining how proper-
ties of the CPP formulation evolve when the com-
plete positivity constraint is relaxed. We ﬁnd that
many of the favorable properties of the CPP for-
mulation are retained in SDP relaxations, show-
ing that it is a convenient starting point for con-
structing tight SDP-based veriﬁers. Finally, we
contextualize existing work in this shared frame-
work, clearly laying out their similarities and dif-
ferences with the proposed framework.

1.1 Related Work

Sound and Complete Veriﬁers. A number of ex-
act veriﬁers rely on reformulating veriﬁcation as a
mixed integer linear program (MILP) Cheng et al.
(2017); Lomuscio and Maganti (2017); Fischetti and
Jo (2017); Tjeng et al. (2019) or Satiﬁability Mod-
ulo Theories (SMT) problem Scheibler et al. (2015)
and calling an oﬀ-the-shelf solver. Others have de-
veloped custom solvers speciﬁcally meant to leverage
the structure of the veriﬁcation problem Katz et al.
(2017); Ehlers (2017); De Palma et al. (2021); Jaeckle
et al. (2021). Bunel et al. (2018) provides a uniﬁed per-
spective of exact veriﬁers through the lens of branch
and bound. The primary diﬀerences across the various
methods stem from (1) the way bounds are computed,
(2) the type of branching that is considered, and (3)
strategies to guide branching. Whether deferring to
the decisions of an oﬀ-the-shelf solver, or employing
veriﬁcation-speciﬁc decisions, all exact veriﬁers ulti-
mately rely on exhaustive search.

Sound but Incomplete Veriﬁers. Sound but in-
complete veriﬁers bypass exhaustive search by solving
convex relaxations of the veriﬁcation problem, where
the neuron values either appear as or can be derived
from variables in the optimization problem. In our dis-
cussion, we distinguish between ﬁrst-order and second-
order relaxations. Broadly speaking, we categorize a
relaxation as ﬁrst order if the optimization variables
represent degree one monomials of the neuron values—
in this case, the neuron values can be directly read oﬀ
the from the optimal solution. We consider a relax-
ation to be second order if the optimization variables
represent degree two monomials of the neuron values—
in this case, recovering neuron values typically requires
factoring the optimal solution. Because relaxed veri-
ﬁers rely on approximating the neural network compu-
tations, the optimal neuron values may not necessarily
correspond with a valid forward pass of the network.

Under the category of ﬁrst-order relaxations are meth-
ods that are based on linear outer bounds of activa-
tions functions Ehlers (2017); Singh et al. (2018), in-
terval bound propagation Gowal et al. (2018); Wang
et al. (2018a,b), and the dual of the relaxed or orig-
inal non-convex problem Wang et al. (2018b); Wong
et al. (2018); Dvijotham et al. (2018b,a). Salman et al.
(2019) gives a unifying perspective of ﬁrst-order relax-
ations based on a layer-wise convex relaxation frame-
work, and shows that the optimal layer-wise convex re-
laxation exhibits a non-trivial gap. This gap highlights
the inability of ﬁrst-order relaxations to capture the
ReLU activation, the complementarity of which is nat-
urally represented with second-order (i.e., quadratic)
constraints.

An approach for addressing these limitations is to lift
to a second-order space where the quadratic ReLU
constraints can be linearized via the reformulation lin-
earization technique (RLT) Sherali and Adams (1998).
Raghunathan et al. (2018) ﬁrst proposed represent-
ing ReLUs with a quadratic constraint and lifting the
problem to a SDP. Work in this area is primarily based
on SDPs both from the primal perspective Raghu-
nathan et al. (2018); Dathathri et al. (2020); Ander-
son et al. (2021); Ma and Sojoudi (2020), and from the
dual perspective Fazlyab et al. (2022); Newton and Pa-
pachristodoulou (2021). One exception is Dvijotham
et al. (2020), which uses diagonally dominant matrices
to relax the SDP further and represent it as a linear
program (LP).

One challenge speciﬁc to second-order relaxations is
that the quantities of interest, i.e., neuron values, are
obfuscated in the relaxation formulation. This makes
it diﬃcult to tell how the constraints interact, and to
what extent they accurately capture the neural net-
work computation. The analysis provided in this work
aims also to clarify these points.

2 PRELIMINARIES

2.1 Notation and Terminology

For a matrix M , we use Mi,j to denote the entry in
the ith row and jth column, Mi,∗ denotes the entire
ith row, and M∗,j denotes the entire jth column. For
sets of matrices, SM +SN denotes the Minkowski sum:
SM +SN := {M +N | M ∈ SM , N ∈ SN }. The matrix
inner product (cid:104)A, B(cid:105) is deﬁned as (cid:104)A, B(cid:105) = Tr(A(cid:62)B).
S+ is the set of positive semideﬁnite (PSD) matrices;
N is the set of entrywise non-negative square matrices
(we use S+
n and Nn to specify the dimension if it is un-
clear from context). The cone of doubly non-negative
matrices is deﬁned as S+ ∩ N , where ∩ denotes the set
intersection.

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

2.2 Problem Setting

Feedforward ReLU Networks. We consider an n-
layer feedforward ReLU network representing a func-
tion f with input z0,∗ ∈ Rh0 and output f (z0,∗) =
zn,∗ ∈ Rhn , with f being the composition of layer-wise
functions, i.e., f = fn ◦ fn−1 ◦ · · · ◦ f1. The ith layer
of f corresponds to a function fi : Rhi−1 → Rhi (hi is
the dimension of the hidden variable zi,∗) of the form

zi,∗ = fi(zi−1,∗) = σ(W [i − 1]zi−1,∗ + b[i − 1]).

(1)

where W [i − 1] ∈ Rhi×hi−1
is the weight matrix,
b[i − 1] ∈ Rhi is the bias. The vector of all neurons in
the i-th layer is denoted by zi,∗, while zi,j refers specif-
ically to the j-th neuron in the i-th layer. The func-
tion σ(ˆzi,j) = max(0, ˆzi,j) is the Rectiﬁed Linear Unit
(ReLU) function; on vectors, σ(·) operates element-
wise. We assume that this activation is not applied in
the last layer fn. We also model the preactivation val-
ues ˆzi,∗ = W [i−1]zi−1,∗ +b[i−1], so that zi,∗ = σ(ˆzi,∗)
for i ≥ 1, and make the identiﬁcation ˆz0,∗ = z0,∗.

In this paper, we use a positive/negative splitting for
each neuron, denoted by λ+ ≥ 0, λ− ≥ 0, rather than
the typical pre/post-activation splitting. The posi-
tive/negative and pre/post-activation splittings are re-
lated by the following equations:

λ+
i,j = zi,j
λ−
i,j = zi,j − ˆzi,j

(2)

(3)

We will often need to make a number of routine con-
versions, chieﬂy between matrices that act on post-
activation neurons in single layer, zi,∗, to those that act
on the collection of all positive/negative splittings of
variables. To emphasize that these conversions main-
tain equivalence, we refer to these converted matrices
using the same letter and use an overline (e.g., M ) to
denote matrices that act on single layers, and an un-
marked matrix (e.g., M ) to denote matrices that act
on the collection of all variables. For inequalities, this
conversion also includes the addition of a slack variable
to convert to an equality constraint. We use an under-
line (e.g., M ) to denote that a slack variable has not
been included and the inequality should be treated as
such. Concrete examples illustrating these conversions
are included in the Appendix.

Safety Rules. We consider safety rules speciﬁed as
input-output relationships on f . Speciﬁcally, for all
inputs from the set X ⊆ Rh0 , we aim to ensure that
the output belongs to the set Y ⊆ Rhn . We consider
bounded, polytopic input sets

X = {x ∈ Rh0 | Ax ≤ a},

(4)

This is not restrictive, as this construction can be ex-
tended to polytopic output sets as well; because poly-
topes are the intersection of half-spaces, each inequal-
ity deﬁning the polytope can be veriﬁed independently.
In this paper, we consider the formulation of ﬁnding
output violations given input constraints. However,
the methods developed in this paper can be readily
adapted to ﬁnding minimal adversarial disturbances
by imposing constraints on the output and optimizing
over the input.

Optimization Formulation. Veriﬁcation can be
posed as a non-convex optimization problem:

OPT = min

λ+, λ−
s.t.

c(cid:62)(λ+

n,∗ − λ−

n,∗)

0,∗) ≤ a,
i+1,∗ = W [i]λ+

A(λ+
0,∗ − λ−
i+1,∗ − λ−
λ+
λ+
i,jλ−
λ+ ≥ 0, λ− ≥ 0

i,j = 0 ∀i, j,

i,∗ + b[i],

where the network is safe if and only if OPT ≥ d.

(6)

2.3 Completely Positive Programming

Completely positive programs (CPP) are linear opti-
mization problems over matrix variables Du¨r (2010):

minimize
X
subject to

(cid:104)M0, X(cid:105)

(cid:104)Mi, X(cid:105) = mi,
X ∈ C ∗

i ∈ {1, . . . , L},

(7)

where C ∗ ⊂ S+ ∩ N is the cone of completely positive
(CP) matrices, that is, matrices that have a factoriza-
tion with entrywise non-negative entries:

n := {X ∈ Rn×n | X =
C∗

(cid:88)

k

x(k)(x(k))(cid:62), x(k) ∈ Rn

≥0}

(8)
Based on the Sum of Squares (SOS) hierarchy, Par-
rilo (2000) constructed a hierarchy of cones {(Kr)∗}
approximating the completely positive cone from the
exterior. Meaning, C∗ = (cid:84)

r≥0(Kr)∗, and

S+ ∩ N = (K0)∗ ⊃ (K1)∗ ⊃ . . .

(9)

Optimizing over each cone, (Kr)∗, can be posed as
an SDP. In addition, the hierarchy gives a formulaic
methodology for constructing relaxations of CPPs that
trade oﬀ accuracy and tractability.

3 GEOMETRY OF COMPLETE

POSITIVITY

and output sets speciﬁed by a half-space constraint,

Y = {y ∈ Rhn | c(cid:62)y ≥ d}.

(5)

Before presenting the full CPP formulation, this sec-
tion will provide an introductory exploration of the

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

interaction between complete positivity and the veriﬁ-
cation problem. We focus on a single neuron and pro-
vide intuition for the geometry of CP matrices when
representing a ReLU activation.1 A single neuron is
represented by two variables, λ+ ≥ 0 and λ− ≥ 0, de-
noting the positive and negative split of the neuron,
respectively. The ReLU function is then deﬁned by
the quadratic constraint λ+λ− = 0. The completely
positive formulation entails lifting to a matrix,

(cid:18)Λ[λ+, λ+] Λ[λ+, λ−]
Λ[λ+, λ−] Λ[λ−, λ−]

λ+

λ−

(cid:19)

λ+
λ−
1

∈ C ∗

(10)

where the elements of Λ ∈ R2×2 correspond to cross
terms of λ+ and λ−, and are denoted with symbolic
indexing. In the lifted formulation, the ReLU is fur-
ther deﬁned by the linear constraint Λ[λ+, λ−] = 0.
Complete positivity means there exists a factorization

(cid:18)Λ[λ+, λ+] Λ[λ+, λ−]
Λ[λ+, λ−] Λ[λ−, λ−]

λ+

λ−

(cid:19)

λ+
λ−
1

=

K
(cid:88)

k=1

(cid:32)λ+,(k)
λ−,(k)
ξ(k)

(cid:33) (cid:32)λ+,(k)
λ−,(k)
ξ(k)

(cid:33)(cid:62)

(11)
where λ+,(k), λ−,(k), ξ(k) ≥ 0, and K is the rank of
the factorization. The variables in CPPs and SDPs
are weighted (convex in the case of CPPs) combina-
tions of the factors, e.g., neurons λ± = (cid:80)
k ξ(k)λ±,(k).
As we will further explore in Section 4, it is impor-
tant that each factor satisﬁes the constraints, how-
ever we are only able to impose them on their aggre-
gate. A relaxation gap arises when constraints that
hold for the sum of factors do not hold for individ-
ual factors, e.g., Λ[λ+, λ−] = (cid:80)
k λ+,(k)λ−,(k) = 0, but
λ+,(k)λ−,(k) (cid:54)= 0.

of
of ReLUs,
In the
context
λ+,(k), λ−,(k), ξ(k)
that
critical
is
each factor corresponds to a valid ReLU, even if
K (cid:54)= 1. In terms of the pre/post-activation splitting,

non-negativity
ensuring

for

z(k) ≥ 0,

z(k) ≥ ˆz(k),

K
(cid:88)

k=1

z(k)(z(k) − ˆz) = 0. (12)

Figure 1 depicts three diﬀerent cases when K = 2.
In the diagram, each factor satisﬁes the ReLU con-
straints, i.e., z(k) = σ(ˆz(k)). Intuitively, the exactness
of the CPP formulation follows from analogous argu-
ments that each term in the factorization of an optimal
solution is feasible for (6); this notion is formalized in
the proof of Theorem 4.1.

4 RESULTS

In this section, we ﬁrst state our main result, the ex-
act reformulation of (6) as a CPP, in Theorem 4.1.
In order to understand SDP relaxations of this refor-
mulation, the remainder of this section is dedicated to

1Although, for dimensions n ≤ 4, C ∗

n = S+

n ∩ Nn.

Figure 1: This ﬁgure depicts the geometry of CP ma-
trices and ReLU constraints. (cid:80)
k z(k)(ˆz(k) − z(k)) = 0
constrains z to the dotted red circle, z(k) ≥ ˆz(k) is
depicted by the shaded red region, and z(k) ≥ 0 is de-
picted by the shaded green region. z (indicated by the
red cross) is fully determined by these constraints.

breaking down the constraints in the CPP and illus-
trating how the guarantees change when the CP con-
straint is relaxed. The proofs of all results stated in
this section are provided in the Supplemental Material.

(cid:17)

λ
1

(cid:16) Λ
λ(cid:62)

In this section, the objects of interest will be matrices
of the form
, where Λ is a square matrix rep-
resenting cross terms of elements in λ, a vector. In the
proposed formulation, λ(cid:62) = (cid:0)(λ+)(cid:62) (λ−)(cid:62) s(cid:62)(cid:1) is
the concatenation of λ+, λ−, the positive and negative
splitting for each neuron, and s, slack variables used
to represent inequality constraints. To emphasize the
relationship with λ, we use symbolic indexing to refer
to speciﬁc terms in Λ; for example, Λ[λ+
i,j, sk] refers to
the element in Λ corresponding to the dot product of
factors corresponding to λ+
Theorem 4.1. Problem (6) is equivalent to the fol-
lowing convex optimization problem:

i,j and sk.

j,∗Aj,∗, Λ(cid:11) = a2
j ,

c(cid:62)λ

OPTCPP = min
λ, Λ
s.t. Aj,∗λ = aj,
(cid:10)A(cid:62)
W [i]j,∗λ = b[i]j,
(cid:10)W [i](cid:62)
Λ[λ+
(cid:16) Λ
λ(cid:62)

i,j, λ−
i,j] = 0,
(cid:17)
λ
∈ C∗
1

j,∗W [i]j,∗, Λ(cid:11) = b[i]2
j ,

(13a)

(13b)

(13c)

(13d)

(13e)

(13f)

(13g)

Exactness is deﬁned by the following conditions:

1. OPT = OPTCPP: Problem (6) and Problem (13) have

the same objective value.

2. If (λ∗, Λ∗) = arg min (13) then λ∗ is in the convex

hull of optimal solutions for Problem (6).

The salient features of (13) are the pairs of linear and
“self-quadratic” constraints and the CP constraint.

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

To motivate the following discussion, we introduce the
zeroth order sum of squares (0-SOS) relaxation as a
natural SDP relaxation for Problem (13):

minimize
λ, Λ
subject to

c(cid:62)λ

(13b) − (13f),
(cid:16) Λ
λ(cid:62)

λ
1

(cid:17)

∈ S+ ∩ N

(14)

tractable CP constraint,

This relaxation (derived from (9)) replaces the in-
∈ C ∗ , with a
(cid:16) Λ
λ(cid:62)

tractable doubly non-negative constraint,
S+ ∩ N .

(cid:16) Λ
λ(cid:62)

(cid:17)
λ
1

(cid:17)
λ
1

∈

Next, we explore the cooperative role that the linear
and self-quadratic constraints play in enforcing linear
constraints on individual factors. In Theorem 4.2, we
highlight the identical role they play in Problems (13)
and (14) when representing equality constraints. They
do, however, result in diﬀerent guarantees when rep-
resenting inequality constraints. These diﬀerences are
explored in Corollary 4.2.1. We present results in a
generic form, because none of these results rely on the
speciﬁc form of the veriﬁcation problem, and it allows
us to focus on the core assumptions needed to derive
each result. To tie things back to (13) and (14), we in-
tersperse these results with discussion of implications
for veriﬁcation.

The following theorem shows that pairs of linear and
self-quadratic constraints play an identical role in both
the CPP and SDP-based formulations.
Theorem 4.2. Suppose there is a factorization

framework, we convert inequality constraints to equal-
ity constraints through the introduction of slack vari-
ables. For a CP matrix, each factor’s slack variables
are guaranteed to be non-negative, whereas for dou-
bly non-negative matrices, only their weighted sum is
guaranteed to be non-negative. That is, individual
slack variables may be negative, and thus individual
factors are not guaranteed to satisfy the inequalities.
The following corollary highlights these diﬀerences.
Corollary 4.2.1. Suppose there is a factorization,

(cid:18)Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)]
Λ[s, s(cid:62)]
s(cid:62)

Λ[s, λ(cid:62)]
λ(cid:62)

(cid:19)

λ
s
1

=

K
(cid:88)

k=1

(cid:33)(cid:62)

(cid:32)λ(k)
s(k)
ξ(k)

(cid:33) (cid:32)λ(k)
s(k)
ξ(k)

and for all i = {1, . . . L}, (λ, Λ) satisfy

Vi,∗λ + si = vi,
(cid:10)V (cid:62)

i,∗Vi,∗, Λ[λ, λ(cid:62)](cid:11) + 2Vi,∗Λ[λ, si] + Λ[si, si] = v2
i .
(20)

(18)

(19)

Then we must have:

1. For

(cid:18)Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)]
Λ[s, s(cid:62)]
s(cid:62)
dividually satisﬁes the inequalities:

Λ[s, λ(cid:62)]
λ(cid:62)

(cid:19)
λ
s
1

∈ C ∗, each factor in-

Vi,∗λ(k) ≤ ξ(k)vi.

(21)

2. For

(cid:18)Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)]
Λ[s, s(cid:62)]
s(cid:62)

(cid:19)
λ
s
1
weighted sum of factors satisﬁes the inequalities:

∈ S+, and s ≥ 0 the

Λ[s, λ(cid:62)]
λ(cid:62)

Vi,∗λ =

(cid:88)

k

ξ(k)Vi,∗λ(k) ≤ vi.

(22)

(cid:16) Λ
λ(cid:62)

(cid:17)
λ
1

=

K
(cid:88)

k=1

(cid:16)λ(k)
ξ(k)

(cid:17) (cid:16)λ(k)
ξ(k)

(cid:17)(cid:62)

.

Then, Vi,∗λ(k) = ξ(k)vi for all k if and only if

Vi,∗λ = vi
i,∗Vi,∗, Λ(cid:11) = v2
i .

(cid:10)V (cid:62)

Input and Non-negativity Constraints. This
corollary shows that constraints (13b) and (13c) are
suﬃcient for ensuring that each factor satisﬁes the in-
put constraints in (13). However, we cannot derive the
same result for (14) because membership in the cone
S+ ∩ N does not necessarily imply the existence of a
non-negative factorization. An analogous conclusion
applies to the non-negativity of λ+ and λ−.

(15)

(16)

(17)

This theorem underscores the importance of pairing
linear and self-quadratic constraints to enforce linear
constraints on the factors, λk and ξk. Notice that this
result does not rely on complete positivity, and holds
even with a weaker positive semideﬁnite constraint.

Network-Deﬁning Constraints. Theorem 4.2
characterizes how constraints (13d) and (13e) act in
Problems (13) and (14). They collectively ensure that
the neurons in consecutive layers are related through
the weights and biases of the networks.

Because the equality constraints act the same way
in (13) and (14), the only potential source of a re-
laxation gap is due to inequality constraints. In our

ReLU Constraints.
i,j ≥ 0. Thus the constraint (cid:80)
λ±,(k)
can only hold if λ+,(k)
factor satisﬁes the ReLU constraints.
only guaranteed that (cid:80)
conclusion cannot be drawn.

In (13), each factor satisﬁes
k λ+,(k)
i,j = 0
i,j = 0 for all k, i.e., each
In (14), it is
i,j ≥ 0, so the same

k ξ(k)λ±,(k)

i,j λ−,(k)

i,j λ−,(k)

Veriﬁcation-Deﬁning Constraints.
In the pro-
posed CPP formulation, constraints (13b)-(13f) are
critical for encoding the veriﬁcation problem. By the
results presented in this section, the removal of any
of these constraints would fundamentally misrepre-
sent the problem. For this reason, we call them the
veriﬁcation-deﬁning constraints.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

In the (0-SOS) re-
Strengthening Constraints.
laxation (14), however, the veriﬁcation-deﬁning con-
straints are not suﬃcient to ensure exactness. One
common strategy is to impose additional “strengthen-
ing constraints.” These are constraints encoding a pri-
ori knowledge of the optimal solution. They will have
no eﬀect on (13), but can potentially reduce the feasi-
ble domain in relaxations, thereby generating tighter
relaxations.
In the context of veriﬁcation, these are
typically derived from bound constraints:
ˆui,j − (λ+
i,j − λ−
(λ+

i,j − λ−
i,j) ≥ 0 (23)
i,j) − ˆli,j ≥ 0 (25)

ui,j − λ+
λ+
i,j − li,j ≥ 0

i,j ≥ 0

(26)

(24)

These are lower and upper bounds for each neuron’s
pre/post-activation values (denoted by ˆli,j, ˆui,j, and
li,j, ui,j respectively)—they can be found eﬃciently by
forward propagating bounds on the input set. These
bounds can be further reﬁned by leveraging the follow-
ing characterization of the convex hull of the graph of
ReLUs:

λ+
i,j ≤

ui,j − li,j
ˆui,j − ˆli,j

((λ+

i,j − λ−

i,j) − ˆli,j) + li,j

(27)

This inequality is commonly referred to as the triangle
relaxation due to the geometry of the convex hull (Liu
et al., 2021, Fig. 6.4).

In our framework, each of these inequality constraints
would be imposed by introducing a slack variable
and including a pair of linear and self-quadratic con-
straints. In principle, (23)-(27) could be further com-
bined with additional valid inequalities to provide even
stronger relaxations. However, the following Corol-
lary shows that simply combining existing linear in-
equalities, e.g., by multiplication, will not actually
strengthen the relaxation.

Corollary 4.2.2. If

Λ[s, λ(cid:62)]
λ(cid:62)
and (18), (19), (20) hold, then the following holds:

(cid:18)Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)]
Λ[s, s(cid:62)]
s(cid:62)

(cid:19)
λ
s
1

∈ S+ ∩N ,

vivj − viVj,∗λ − vjVi,∗λ + (cid:10)V (cid:62)

i,∗Vj,∗, Λ[λ, λ(cid:62)](cid:11) ≥ 0

(28)

Inequality (28) is the linearized inequality representing
(vi − Vi,∗λ)(vj − Vj,∗λ) ≥ 0. In contrast to the self-
quadratic constraints, this is a product of two diﬀerent
linear constraints. For this reason, we call constraints
of this form “cross-quadratic” constraints. Corollary
4.2.2 states that if we enforce constraints of the form
Vi,∗λ ≤ vi through the introduction of a slack variable
and paired linear/self-quadratic constraints, then the
derived cross-quadratic constraint will also hold. This
is signiﬁcant because in a number of existing works,
cross-quadratic constraints derived from (23)-(26) are
introduced, however the linear and self-quadratic con-
straints are excluded.

Figure 2: This table summarizes the constraints in-
cluded in existing work. The † indicates partial en-
coding of constraints/categorizations that require ad-
ditional qualiﬁcation. Cross-quadratic constraints are
marked with a × in the self-quadratic row.

5 UNIFYING SDP-BASED

VERIFICATION

Now that we have
established the notions of
veriﬁcation-deﬁning and strengthening constraints, we
are in a position to show how existing work ﬁts into our
framework. We discuss two seemingly disparate frame-
works that have served as the basis for subsequent
work, Raghunathan et al. (2018) and Fazlyab et al.
(2022). In this section we show how these frameworks,
as well as subsequent works, are related by couch-
ing their approaches in the language of our proposed
veriﬁcation-deﬁning and strengthening constraints. A
number of subsequent works focus on improving com-
putational aspects of existing relaxations (e.g., time to
solution, memory usage) without aiming to improve
the relaxation gap. These works are discussed in the
Appendix rather than the main body of this paper be-
cause they largely have the same constraint structure
at work.

A number of works in this section construct strength-
ening cuts derived from pre/post-activation bounds on
neurons. Without loss of generality, we will assume
that ˆli,j < 0 < ˆui,j (otherwise the ReLU could be
treated as the identity, if ˆli,j ≥ 0, or zero, if ˆui,j ≤ 0).
The post-activation bounds are then li,j = 0 and
ui,j = ˆui,j.

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

5.1 Direct SDP

To the best of our knowledge, Raghunathan et al.
(2018) was the ﬁrst to propose an SDP relaxation of
the veriﬁcation problem:

min
z, Z

s.t.

c(cid:62)zn,∗

zi,∗ ≥ 0, zi,∗ ≥ W [i − 1]zi−1,∗,
Z[zi,j, zi,j] = W [i − 1]j,∗Z[zi−1,∗, zi,j],
− Z[zi,j, zi,j] + (ui,j + li,j)zi,j − li,jui,j ≥ 0, ∀i, j,
(cid:16) Z

(cid:17)

z
z(cid:62) 1

∈ S +

(29)
Although their approach does not include biases, it
is straightforward to include them. Problem (29) is
equivalent to the following optimization problem:

i,∗Ui,∗, Λ(cid:11) − uili ≥ 0,

i,j] ≥ 0, i ≥ 1,

c(cid:62)λ

min
λ, Λ

s.t.

i,j, λ+

(l0,i + u0,i)Ui,∗λ − (cid:10)U (cid:62)
ˆui,jλ+
i,j − Λ[λ+
W [i]j,∗λ = b[i]j,
i,j, λ−
Λ[λ+
i,j] = 0,
(cid:17)
(cid:16) Λ
∈ S+,
λ(cid:62)
λ ≥ 0

λ
1

0,i − λ−

(30)
where U is deﬁned as Ui,∗λ = λ+
0,i. In the con-
text of (14), we see that (30) does not account for
non-negativity of Λ, i.e., Λ ∈ N , and the network
(30) also includes
self-quadratic constraints (13e).
strengthening constraints derived from λ+
i,j ≥ 0 and
(ˆui,j − λ+

i,j) ≥ 0.

Tightening Extensions. Based on the observation
that (30) sometimes produces bounds looser than the
triangle LP relaxation , Batten et al. (2021) proposes
including linear cuts derived from (27). In the follow-
ing, we provide intuition for the gap that the triangle
inequality closes.

The PSD constraint ensures that each neuron has a
factorization in the form of (11) where λ+,(k), λ−,(k)
are not necessarily non-negative (without loss of gen-
erality ξ(k) can be normalized to be non-negative). In
contrast to the CP case, the constraint λ+ ≥ 0 only
guarantees that (cid:80)K
i=1 ξ(k)λ+,(k) ≥ 0, and similarly for
other inequalities. In terms of the pre/post-activation

Figure 3: This ﬁgure depicts the geometry of the tri-
angle cut. (cid:80)
k z(k)(ˆz(k) − z(k)) = 0 constrains z to
the dotted red circle. z ≥ ˆz and z ≥ 0 constrain z to
the right of the dotted green and purple lines, respec-
tively. The triangle cut (34) constrains z to the left of
the solid blue line. The lower and upper bounds (ˆl ≤ ˆz
and ˆz ≤ ˆu) used to construct the triangle cut are de-
marcated by the dotted black lines. The red cross
marks the factor-wise ReLU (as in Figure 1), and the
shaded orange region indicates the region (in terms of
ˆz) for which the factor-wise ReLU is feasible.

splittings, it is guaranteed that:

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1

ξ(k)z(k) ≥ 0,

ξ(k)z(k) ≥

K
(cid:88)

i=1

ξ(k) ˆz(k),

z(k)(ˆz(k) − z(k)) = 0.

(31)

(32)

(33)

These are the analogs of (12). The triangle cut pro-
vides the additional guarantee that

K
(cid:88)

i=1

ξ(k)z(k) ≤

ˆui,j
ˆui,j − ˆli,j

(cid:32) K
(cid:88)

i=1

(cid:33)

ξ(k) ˆz(k) − ˆli,j

.

(34)

Figure (27) illustrates the geometry of these con-
straints when K = 2. The plots, from left to right
and top to bottom, correspond to the cases where (34)
is (i) non-redundant and includes σ(ˆz), (ii) redundant,
(iii) infeasible, and (iv) non-redundant but eliminates
σ(ˆz) given (31) - (33). The diagrams in the top row
illustrate the eﬀect of the triangle cut in restricting
the feasible region of z given ˆz. In the ﬁrst case, the
triangle cut strictly reduces the feasible region, result-
ing in a strengthened relaxation. In the second case,
the region deﬁned by the intersection of (31) - (33)
lies entirely within the region deﬁned by (34), i.e., the
triangle cut does not add any additional information.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

The diagrams in the bottom row illustrate the tri-
angle cut’s role in enforcing “acceptable” values of
ˆz. The third (infeasible) case occurs if and only if
the pre-activation bounds are actually not satisﬁed
(cid:80)K
i=1 ξ(k) ˆz(k) (cid:54)∈ [ˆl, ˆu], i.e., the triangle cut implicitly
constrains ˆz ∈ [ˆl, ˆu].
In the fourth case, the trian-
gle cut is feasible but counter-intuitively renders the
factor-wise ReLU, z(k) = σ(ˆz(k)), infeasible. This is
because the triangle cut is only valid for ˆz that are
convex combinations of

with parameters, Γi,j ≥ 0, Γi,i = 0. The equivalent
atomic constraints are

(Ai,∗λ − ai)(Aj,∗λ − aj) ≥ 0,

∀i (cid:54)= j.

(37)

ReLU Constraints. The ReLU constraints are de-
ﬁned through a quadratic constraint

0 ≥

(cid:88)

(i,j)

+

(cid:2)ρ(i,j)λ+

i,jλ−

i,j − νi,jλ+

i,j − ηi,jλ−

i,j

(cid:3)

(cid:88)

γ(i,j),(k,l)(λ+

i,j − λ+

k,l)(λ−

i,j − λ−

k,l)

(38)

ˆz(k) ∈

(cid:34) ˆl
ξ(k) ,

ˆu
ξ(k)

(cid:35)

,

(35)

(i,j)(cid:54)=(k,l)

even if (cid:80)K
i=1 ξ(k) ˆz(k) ∈ [ˆl, ˆu]. Condition (35) is sat-
isﬁed by any z(k) corresponding to a forward pass of
the network, but cannot be explicitly enforced in an
SDP. The behavior of the triangle cut illustrates the
contrast between the guarantees of the CPP formu-
lation, which can be leveraged to (implicitly) impose
constraints on individual factors, and SDP relaxations,
which can only enforce constraints on the aggregate of
factors.

5.2 Quadratic Constraints and the

S-procedure

Fazlyab et al. (2022) proposes a novel framework based
on the notion of quadratic constraints.2 In contrast
with existing work, the proposed framework deals with
classes of constraints obtained by taking inﬁnite com-
binations of “atomic” constraints. For example, the
atomic constraints x ≥ 0, y ≥ 0 are subsumed by the
inﬁnite class of constraints {αx+βy ≥ 0} parametrized
by α ≥ 0, β ≥ 0. Critically, αx + βy ≥ 0 for all
α ≥ 0, β ≥ 0 if and only if x ≥ 0 and y ≥ 0, so these
constructions are equivalent. Similarly, for each of the
inﬁnite classes of quadratic constraints proposed, we
will identify the equivalent set of atomic constraints,
and rewrite the formulation of Fazlyab et al. (2022) in
a form that parallels the (0-SOS) relaxation (14).

Polytopic Input Sets. Fazlyab et al. (2022) pro-
poses encoding polytopic input sets, X = {x ∈ Rk0 |
Ax ≤ a}, via a quadratic constraint

(cid:88)

i,j

Γi,j(Ai,∗λ − ai)(Aj,∗λ − aj) ≥ 0

(36)

2The techniques of Fazlyab et al. (2022) can be applied
to activations other that ReLUs by abstracting non-ReLU
activations with linear bounds characterizing various prop-
erties of activation functions (e.g., monotonicity, bounded
slope, bounded values). In this section, we will focus on
results pertaining to ReLU networks with polytopic input
constraints, and halfspace output constraints, in order to
draw a direct comparison to the other methods discussed
in this paper. However, the analysis of linear and self-
quadratic constraints applies to SDP encodings of the prop-
erties arising from general activation functions, even in the
absence of exactness.

for parameters ρ(i,j) ∈ R, γ(i,j),(k,l), νi,j, ηi,j ≥ 0. The
equivalent atomic constraints are:

λ+
i,j ≥ 0,
i,jλ−
λ+

k,l ≥ 0,

λ−
i,j ≥ 0,
i,jλ+
λ−

i,jλ−
λ+
k,l ≥ 0 ∀(i, j) (cid:54)= (k, l)

i,j = 0 ∀(i, j) (39)
(40)

Notice that (39) is equivalent to λ ≥ 0 and (13f),
and (40) is equivalent to the non-negativity constraint
Λ[λ+, λ−] ≥ 0. In summary, the formulation proposed
by Fazlyab et al. (2022) can be shown to be equivalent
to

min
λ, Λ

s.t.

c(cid:62)λ

(cid:68)

A(cid:62)

i,∗Aj,∗, Λ

(cid:69)

− aiAj,∗λ − ajAi,∗λ ≥ aiaj, i (cid:54)= j,

W [i]j,∗λ = b[i]j,
i,j, λ−
Λ[λ+
i,j] = 0,
(cid:17)
(cid:16) Λ
λ
∈ S+,
λ(cid:62)
1

λ ≥ 0, Λ[λ+, (λ−)(cid:62)] ≥ 0

(41)

In the context of (14), we see that (41) does not ac-
count for (13b), (13c), (13e), and only partially ac-
counts for Λ ∈ N . We also observe that (41) has
included cross-quadratic input constraints that are re-
dundant in (14).

Using the 0-SOS relaxation as a shared framework, we
can thus unify many of the existing works on SDP-
based veriﬁcation. Our analysis clearly lays out the
constraints that are included in or excluded from ex-
isting work, with Figure 2 summarizing these points.

6 EXPERIMENTS

In Sections 4 and 5, we have claimed that the
“veriﬁcation-deﬁning” constraints are critical for accu-
rately representing the neural network computation,
and shown that a number of existing SDP-based re-
laxations omit various such constraints.
In this sec-
tion, we empirically evaluate some of the questions we
have left unanswered. Chieﬂy, to what degree do the
veriﬁcation-deﬁning constraints matter, i.e., is it pos-
sible to get a reasonable veriﬁcation gap even without

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

all of the veriﬁcation-deﬁning constraints? And, do
proposed strengthening constraints suﬃciently make
up for the missing veriﬁcation-deﬁning constraints?

In this section, we consider a ReLU network with two
inputs, one output, and one hidden layer with 10 neu-
rons. We generated 100 network instances by selecting
the weights and biases uniformly between -1.0 and 1.0.
We consider a safety rule where the input set is deﬁned
as X := {z0,∗ | z0,∗ ∈ [−1.0, 0.1]2} and the output set
is deﬁned as Y := {zn,∗ | zn,∗ ≥ 0}.

We computed the ground-truth minimum by formu-
lating the veriﬁcation problem as a MILP Tjeng et al.
(2019), and solving to global optimality using Gurobi
Optimization, LLC (2020), with a tolerance of 1 ×
10−5. We solve all SDPs using Splitting Conic Solver
(SCS) O’Donoghue et al. (2016), also with a tolerance
of 1 × 10−5.

Ablating Veriﬁcation-Deﬁning Constraints.
In
Section 4, we have shown that removal of any
of the veriﬁcation-deﬁning constraints fundamentally
changes the underlying problem being relaxed, how-
ever we have left unanswered the degree to which re-
moving individual constraints increases the relaxation
gap. We evaluate this empirically by considering vari-
ants of (14) where constraints are removed individu-
ally from the problem (termed the ablated SDPs). We
focus on ablating (13b)-(13e), and Λ ≥ 0, because con-
straints (13f) and λ ≥ 0, are not typically omitted in
the literature. The relative errors of each of the ab-
lated SDPs are plotted in Figure 4a.

We ﬁnd that removing the input linear constraints,
(13b), has the smallest eﬀect on the relaxation gap,
even sometimes resulting in exact relaxations. On the
other hand, removing the non-negativity constraint,
Λ ≥ 0, typically leads to bounds that are entirely un-
informative, with relative errors ranging from 243 to
5.1 × 104. We also ﬁnd that without the network self-
quadratic constraints, (13e), the relative errors range
from 63 to 4 × 104. The results of this ablation study
are signiﬁcant, because in the context of Figure (2), we
see that there are no existing approaches that entirely
encode (13b), (13e), and Λ ≥ 0.

Comparison to the Literature. While we have
shown that removing the veriﬁcation-deﬁning con-
straints typically introduces a signiﬁcant relaxation
gap, all existing work surveyed above also includes
various strengthening constraints that imbue addi-
tional knowledge of the problem structure.
In this
experiment, we evaluate whether the strengthening
constraints are suﬃcient to make up for the missing
veriﬁcation-deﬁning constraints. Figure 4b plots the
relative errors of the 0-SOS relaxation (14), and that
of several relaxations proposed in the literature.

We ﬁnd that (14) typically results in the tightest relax-
ations, and is often exact. Batten et al. (2021) and Fa-

(a) This ﬁgure plots the rel-
ative error when constraints
in (14) are ablated. While
removing (13b) typically re-
sults in reasonable relaxation
gaps,
the removal of any
other constraint renders the
relaxation entirely uninfor-
mative with gaps typically
on the order of 102.

(b) This ﬁgure shows the rel-
ative relaxation gap of the 0-
SOS relaxation, (14), as well
as those of several existing
relaxations. The 0-SOS re-
laxation is often exact, with
a median error that is orders
of magnitude smaller than
the other relaxations.

zlyab et al. (2022) are sometimes exact, while Raghu-
nathan et al. (2018) never is. All exhibit median relax-
ation gaps that are orders of magnitudes larger than
that of (14). Notably, all exhibit considerably smaller
relaxation gaps than the ablated SDPs. This indicates
that the gaps observed in the baselines are largely de-
pendent on the strengthening constraints.

7 CONCLUSION

In this paper, we have shown that veriﬁcation of ReLU
networks can be formulated exactly as a complete pos-
itive program, and provided analysis showing that the
formulation is minimal. Leveraging the similarity be-
tween the form of the CPP formulation and SDP-based
relaxations, we use our formulation as the basis for a
unifying perspective on existing approaches. We also
provide empirical evaluation demonstrating (1) that
the veriﬁcation deﬁning constraints are indispensable
for accurately representing the veriﬁcation problem,
and (2) the 0-SOS relaxation of the proposed formula-
tion exhibits relaxation gaps that are orders of magni-
tude smaller than existing methods, and is often exact.

While we have focused on the 0-SOS relaxation as a
comparison to existing approaches, it is just one in
a hierarchy of relaxations now at our disposal. Be-
cause higher order order SOS relaxations scale poorly,
however, their application to solving veriﬁcation re-
quires further investigation. Fortunately, there is an
abundance of literature devoted to systematically im-
proving the tractability of SOS-based methods.
In
particular, we highlight the r-DSOS and r-SDSOS hi-
erarchies proposed by Ahmadi and Majumdar (2019)
as promising algorithmic paradigms for addressing the
CPP formulation we have proposed. We believe the
most valuable contribution of this work is the poten-
tial for a new class of veriﬁcation methods that can
systematically trade-oﬀ tightness and eﬃciency, with
exactness as the limiting case.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

Acknowledgements

This work was
supported by NSF CCF (grant
#1918549) and the NASA University Leadership Ini-
tiative (grant #80NSSC20M0163); this article solely
reﬂects the opinions and conclusions of its authors and
not any NSF or NASA entity.

References

Ahmadi, A. and Majumdar, A. (2019). DSOS and
SDSOS optimization: More tractable alternatives to
sum of squares and semideﬁnite optimization. SIAM
Journal on Optimization, 3(2):193–230.

Anderson, B., Ma, Z., Li, S., and Sojoudi, S. (2021).
Partition-based convex relaxations for certifying the
robustness of relu neural networks. arXiv preprint
arXiv:2004.00570.

Batten, B., Kouvaros, P., Lomuscio, A., and Zheng,
Y. (2021). Eﬃcient neural network veriﬁcation via
layer-based semideﬁnite relaxations and linear cuts.
In Int. Joint Conf. on Artiﬁcial Intelligence.

Bunel, R., Turkaslan, I., Torr, P., Kohli, P., and Ku-
mar, M. (2018). A uniﬁed view of piecewise linear
neural network veriﬁcation. In Conf. on Neural In-
formation Processing Systems.

Burer, S. (2009). On the copositive representation
of binary and continuous nonconvex quadratic pro-
grams. Mathematical Programming, 120(2):479–495.
Cheng, C., N¨uhrenberg, G., and Ruess, H. (2017).
Maximum resilience of artiﬁcial neural networks. In
International Symposium on Automated Technology
for Veriﬁcation and Analysis.

Dathathri, S., Dvijotham, K., Kurakin, A., Raghu-
nathan, A., Uesato, J., Bunel, R., Shankar, S.,
Steinhardt, J., Goodfellow, I., Liang, P., and Kohli,
P. (2020). Enabling certiﬁcation of veriﬁcation-
agnostic networks via memory-eﬃcient semideﬁnite
programming. In Conf. on Neural Information Pro-
cessing Systems.

De Palma, A., Bunel, R., Desmaison, A., Dvijotham,
K., Kohli, P. Torr, P., and P., K. (2021).
Im-
proved branch and bound for neural network veriﬁ-
cation via lagrangian decomposition. arXiv preprint
arXiv:2002.10410.

Du¨r, M. (2010). Copositive programming–a survey. In
Diehl, M., Glineur, F., Jarlebring, E., and Michiels,
W., editors, Recent Advances in Optimization and
its Applications in Engineering. Springer.

Dvijotham, K., Gowal, S., Stanforth, R., Arandjelovic,
R., O’Donoghue, B., Uesato, J., and Kohli, P.
(2018a). Training veriﬁed learners with learned ver-
iﬁers. arXiv preprint arXiv:1805.10265.

Dvijotham, K., Stanforth, R., Gowal, S., Mann, T.,
and Kohli, P. (2018b). A dual approach to scalable
veriﬁcation of deep networks. In Proc. Conf. on Un-
certainty in Artiﬁcial Intelligence.

Dvijotham, K., Stanforth, R., Gowal, S., Qin, C.,
De, S., and Kohli, P. (2020). Eﬃcient neural net-
work veriﬁcation with exactness characterization. In
Adams, R. P. and Gogate, V., editors, Proc. Conf.
on Uncertainty in Artiﬁcial Intelligence, volume 115
of Proceedings of Machine Learning Research, pages
497–507. PMLR.

Ehlers, R. (2017). Formal veriﬁcation of piece-wise lin-
ear feed-forward neural networks. In International
Symposium on Automated Technology for Veriﬁca-
tion and Analysis.

Fazlyab, M., Morari, M., and Pappas, G. (2022).
Safety veriﬁcation and robustness analysis of neu-
ral networks via quadratic constraints and semidef-
inite programming.
IEEE Transactions on Auto-
matic Control, 67(1):1–15.

Fischetti, M. and Jo, J. (2017). Deep neural networks
as 0-1 mixed integer linear programs: A feasibility
study. arXiv preprint arXiv:1712.06174.

Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R.,
Qin, C., Uesato, J., Arandjelovic, R., Mann, T.,
and Kohli, P. (2018). On the eﬀectiveness of inter-
val bound propagation for training veriﬁably robust
models. arXiv preprint arXiv:1810.12715.

Gurobi Optimization, LLC (2020). Gurobi Opti-
mizer Reference Manual. Available at http://www.
gurobi.com.

Jaeckle, F., Lu, J., and Kumar, M. (2021). Neural
network branch-and-bound for neural network veri-
ﬁcation. arXiv preprint arXiv:2107.12855.

Katz, G., Barrett, C., Dill, D., Julian, K., and Kochen-
derfer, M. (2017). Reluplex: An eﬃcient SMT solver
for verifying deep neural networks.
In Proc. Int.
Conf. Computer Aided Veriﬁcation.

Lin, C.-J. (2007). Projected gradient methods for non-
negative matrix factorization. Neural Computation,
19(10):2756–2779.

Liu, C., Arnon, T., Lazarus, C., Strong, C., Barrett,
C., and Kochenderfer, M. (2021). Algorithms for
verifying deep neural networks. Foundations and
Trends in Optimization, 4(3-4):244–404.

Lomuscio, A. and Maganti, L. (2017). An approach
to reachability analysis for feed-forward relu neural
networks. arXiv preprint arXiv:1706.07351.

Ma, Z. and Sojoudi, S. (2020). Strengthened SDP veri-
ﬁcation of neural network robustness via non-convex
cuts. arXiv preprint arXiv:2010.08603.

Newton, M. and Papachristodoulou, A. (2021). Ex-
ploiting sparsity for neural network veriﬁcation.
In Jadbabaie, A., Lygeros, J., Pappas, G. J.,
A.&nbsp;Parrilo, P., Recht, B., Tomlin, C. J., and
Zeilinger, M. N., editors, Learning for Dynamics
& Control, volume 144 of Proceedings of Machine
Learning Research, pages 715–727. PMLR.

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

O’Donoghue, B., Chu, E., Parikh, N., and Boyd, S.
(2016). Conic optimization via operator splitting
and homogeneous self-dual embedding. Journal of
Optimization Theory & Applications, 169(3):1042–
1068.

Parrilo, P. A. (2000). Structured Semideﬁnite Pro-
grams and Semialgebraic Geometry Methods in Ro-
bustness and Optimization.
PhD thesis, Mas-
sachusetts Inst. of Technology.

Raghunathan, A., Steinhardt, J., and Liang, P. (2018).
Semideﬁnite relaxations for certifying robustness to
adversarial examples. In Conf. on Neural Informa-
tion Processing Systems, pages 10900–10910.

Salman, H., Yang, G., Zhang, H.and Hsieh, C., and
Zhang, P. (2019). A convex relaxation barrier to
tight robustness veriﬁcation of neural networks. In
Conf. on Neural Information Processing Systems.

Scheibler, K., Winterer, L., Wimmer, R., and Becker,
B. (2015). Towards veriﬁcation of artiﬁcial neural
networks. In MBMV.

Sherali, H. and Adams, W. P. (1998). Reformulation-
linearization techniques for discrete optimization
problems. In Handbook of Combinatorial Optimiza-
tion. Springer.

Singh, G., Gehr, T., Mirman, M., P¨uschel, M., and
Vechev, M. (2018). Fast and eﬀective robustness
certiﬁcation. In Conf. on Neural Information Pro-
cessing Systems.

Tjeng, V., Xiao, K., and Tedrake, R. (2019). Evaluat-
ing robustness of neural networks with mixed integer
programming. In Int. Conf. on Learning Represen-
tations.

Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana,
S. (2018a). Eﬃcient formal safety analysis of neural
networks. In Conf. on Neural Information Process-
ing Systems.

Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana,
S. (2018b). Formal security analysis of neural net-
works using symbolic intervals.
In Proc. USENIX
Conf. on Security Symposium.

Wong, E., Schmidt, F., Metzen, J., and Kolter, Z.
In
(2018). Scaling provable adversarial defenses.
Conf. on Neural Information Processing Systems.

Zhang, R. (2020). On the tightness of semideﬁnite re-
laxations for certifying robustness to adversarial ex-
amples. In Conf. on Neural Information Processing
Systems.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

A APPENDIX

A.1

Illustration of expanded out matrices

In this paper, we have made a number of routine conversions between matrices that act on pre/post-activation
variables corresponding to a single layer, to those that act on the collection of all positive/negative splittings of
variables from all layers. In this section, we provide a concrete example of how these conversions are carried out.
Speciﬁcally, we illustrate the relationship between the matrices that act on single layers, M , those that act on
all variables, including slack variables M , and those that act on all variables except slack variables M .
Consider an input set deﬁned as X = {x ∈ Rh0 | Ax ≤ a}. The matrix A ∈ Rm×h0 only acts on the input vector,
z0,∗. In this paper, we use the positive/negative splitting, so the input constraint is given by

In addition, we concatenate all of the neurons and slack variables in a single vector

A(λ+

0,∗ − λ−

0,∗) ≤ a.

λ =





































λ+
0,∗
λ+
1,∗
...
λ+
n,∗
λ−
0,∗
λ−
1,∗
...
λ−
n,∗
s

(42)

(43)

where s ∈ Rm is a vector of slack variables corresponding to the input inequality constraints. Let N := (cid:80)n
i=0 hi
be the number of neurons, λ ∈ R2N +m (the factor of two is because there are two variables for every neuron).
Then, the constraint A(λ+

0,∗) ≤ a may be written as

0,∗ − λ−

(cid:0)A 0

. . .

0 −A 0

. . .

0

0(cid:1)



















≤ a.

(44)



















λ+
0,∗
λ+
1,∗
...
λ+
n,∗
λ−
0,∗
λ−
1,∗
...
λ−
n,∗
s

We deﬁne A ∈ Rm×(2N +m), which acts on all variables but does not incorporate the eﬀect of slack variables, as

A := (cid:0)A 0

. . . 0 −A 0

. . . 0

0(cid:1) .

(45)

Similarly, for appropriate s ≥ 0, then A(λ+

0,∗ − λ−

0,∗) ≤ a may be written as

(cid:0)A 0

. . .

0 −A 0

. . .

0

I(cid:1)



















= a.

(46)



















λ+
0,∗
λ+
1,∗
...
λ+
n,∗
λ−
0,∗
λ−
1,∗
...
λ−
n,∗
s

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

We deﬁne A ∈ Rm×(2N +m), which acts on all variables including slacks, as
I(cid:1) .

A := (cid:0)A 0

. . . 0 −A 0

. . . 0

A.2 Proofs of Section 4

Theorem (4.1). Problem (6) is equivalent to the following convex optimization problem:

OPTCPP= minimize

λ, Λ
subject to

c(cid:62)λ

(cid:10)W [i](cid:62)

,

,

,

,

j

(cid:10)A(cid:62)

Aj,∗λ = aj
j,∗Aj,∗, Λ(cid:11) = a2
W [i]j,∗λ = b[i]j
j,∗W [i]j,∗, Λ(cid:11) = b[i]2
j
Λ[λ+
i,j, λ−
i,j] = 0
,
(cid:19)
(cid:18) Λ λ
λ(cid:62) 1

∈ C∗

(47)

(13b)

(13c)

(13d)

(13e)

(13f)

(13g)

Exactness is deﬁned by the following conditions:

1. OPT = OPTCPP: Problem (6) and Problem (13) have the same objective value.
2. If (λ∗, Λ∗) = arg min (13) then λ∗ is in the convex hull of optimal solutions for Problem (6).

Proof. This follows as a direct application of Theorem 3.2 from Burer (2009). The two assumptions required
are:

• The binary variables satisfy the key assumption, chieﬂy, boundedness. This assumption is automatically

satisﬁed as formulation (6) does not have any binary variables.

• Each variable in a complementarity constraint is bounded. This follows from our assumption of a bounded

input set. Explicit bounds on each neuron can be derived by forward propagating the input bounds.

Theorem (4.2). Suppose there is a factorization

(cid:19)

(cid:18) Λ λ
λ(cid:62) 1

=

K
(cid:88)

k=1

(cid:18)λ(k)
ξ(k)

(cid:19) (cid:18)λ(k)
ξ(k)

(cid:19)(cid:62)

.

Then, Vi,∗λ(k) = ξ(k)vi for all k if and only if

Vi,∗λ = vi
i,∗Vi,∗, Λ(cid:11) = v2
i .

(cid:10)V (cid:62)

Proof. (⇐) We will ﬁrst show that if (λ, Λ) satisfy (16) and (17), then Vi,∗λ(k) = ξ(k)vi.
Expanding out the outer product,

(cid:19)

(cid:18) Λ λ
λ(cid:62) 1

=

K
(cid:88)

k=1

(cid:18)λ(k)
ξ(k)

(cid:19) (cid:18)λ(k)
ξ(k)

(cid:19)(cid:62)

=

K
(cid:88)

k=1

(cid:18)λ(k)(λ(k))(cid:62) ξ(k)λ(k)
(ξ(k))2

ξ(k)(λ(k))(cid:62)

(cid:19)

.

Matching up terms, {λ(k), ξ(k)} must satisfy

(cid:88)

(ξ(k))2 = 1

k
(cid:88)

ξ(k)λ(k) = λ

k
λ(k)(λ(k))(cid:62) = Λ

(cid:88)

k

(48)

(16)

(17)

(49)

(50)

(51)

(52)

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

Substituting these summations into the constraints, we get that

Vi,∗λ =

(cid:88)

k

ξ(k)Vi,∗λ(k) = vi

(cid:10)V (cid:62)

i,∗Vi,∗, Λ(cid:11) =

(cid:88)

k

(Vi,∗λ(k))2 = v2
i

By squaring (53) and using the fact that (cid:80)

k(ξ(k))2 = 1, we get

i = (Vi,∗λ)2
v2
(cid:88)

= (

ξ(k)Vi,∗λ(k))2

k
(cid:88)

≤ (

(ξ(k))2)

(cid:88)

(Vi,∗λ(k))2

k
(cid:88)

k
(Vi,∗λ(k))2

=

k
= v2
i

(53)

(54)

(55)

(56)

(57)

(58)

(59)

where (57) follows from Cauchy-Schwarz. More speciﬁcally, this shows that in (57) Cauchy-Schwarz holds with
equality, which means that [ξ(k)]k and [Vi,∗λ(k)]k are collinear. This means that there is a scalar α ∈ R such that
αξ(k) = Vi,∗λ(k) for all k. Substituting this into (53), we get

vi =

(cid:88)

k

ξ(k)Vi,∗λ(k) =

(cid:88)

(ξ(k))2α = α.

k

(60)

This shows Vi,∗λ(k) = ξ(k)vi, thus concluding the proof of the reverse implication.
(⇒) We now just have to show that if Vi,∗λ(k) = ξ(k)vi for all k then Vi,∗λ = vi and (cid:10)V (cid:62)
former follows by substituting in the relation (cid:80)

k ξ(k)λ(k) = λ

i,∗Vi,∗, Λ(cid:11) = v2

i . The

Vi,∗λ =

(cid:88)

ξ(k)Vi,∗λ(k)

k
(cid:88)

(ξ(k))2vi

=

k
= vi

(61)

(62)

(63)

where the last line follows because (cid:80)
(cid:10)V (cid:62)

i,∗Vi,∗, Λ(cid:11) = (cid:80)

k(Vi,∗λ(k))2:

k(ξ(k))2 = 1. The latter follows from substituting in the relationship

(cid:10)V (cid:62)

i,∗Vi,∗, Λ(cid:11) =

(cid:88)

(cid:10)V (cid:62)

i,∗Vi,∗, λk(λk)(cid:62)(cid:11)

k
(cid:88)

(Vi,∗λk)2

k
(cid:88)

(ξk)2v2
i

=

=

k
= v2
i .

Corollary (4.2.1). Suppose there is a factorization,





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)



 =

K
(cid:88)

k=1





λ(k)
s(k)
ξ(k)









(cid:62)





λ(k)
s(k)
ξ(k)

(64)

(65)

(66)

(67)

(18)

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

and for all i = {1, . . . L}, (λ, Λ) satisfy

Vi,∗λ + si = vi,
(cid:10)V (cid:62)

i,∗Vi,∗, Λ[λ, λ(cid:62)](cid:11) + 2Vi,∗Λ[λ, si] + Λ[si, si] = v2
i .

Then we must have:

1. For

2. For





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)


 ∈ C ∗, each factor individually satisﬁes the inequalities:

Vi,∗λ(k) ≤ ξ(k)vi.


 ∈ S+, and s ≥ 0 the weighted sum of factors satisﬁes the inequalities:

Proof. Leveraging Theorem 4.2, we know that if

Vi,∗λ =

(cid:88)

k

ξ(k)Vi,∗λ(k) ≤ vi.





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)



 =

K
(cid:88)

k=1





λ(k)
s(k)
ξ(k)









λ(k)
s(k)
ξ(k)

(cid:62)





(19)

(20)

(68)

(69)

(18)

and (λ, Λ) satisfy (19) and (20), then Vi,∗λ(k) + s(k)

 ∈ C ∗ then in particular s(k) ≥ 0 so Vi,∗λ(k) ≤ ξ(k)vi.

i = ξ(k)vi for all k.





If

Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)




In the case where

λ(cid:62)
Vi,∗λ + si = vi and si ≥ 0.

Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

s(cid:62)


 ∈ S+, and s ≥ 0, the result follows directly from the constraints

Corollary (4.2.2). If





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)


 ∈ S+ ∩ N , and the following hold:

1. There is a factorization





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)



 =

K
(cid:88)

k=1









λ(k)
s(k)
ξ(k)

(cid:62)









λ(k)
s(k)
ξ(k)

2.

3.

Vi,∗λ + si = vi

(cid:10)V (cid:62)

i,∗Vi,∗, Λ[λ, λ(cid:62)](cid:11) + 2Vi,∗Λ[λ, si] + Λ[si, si] = v2

i

then the following inequality also holds:

vivj − viVj,∗λ − vjVi,∗λ + (cid:10)V (cid:62)

i,∗Vj,∗, Λ[λ, λ(cid:62)](cid:11) ≥ 0

(18)

(19)

(20)

(28)

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

Proof. From Theorem 4.2, Vi,∗λ(k) + s(k)

i = ξ(k)vi holds. Substituting we have

vivj − viVj,∗λ − vjVi,∗λ + (cid:10)V (cid:62)
(cid:16)

(cid:88)

i,∗Vj,∗, Λ[λ, λ(cid:62)](cid:11)

= vivj +

−viVj,∗(ξ(k)λ(k)) − vjVi,∗(ξ(k)λ(k)) +

(cid:68)

i,∗Vj,∗, λ(k)(λ(k))(cid:62)(cid:69)(cid:17)
V (cid:62)

k

(ξ(k))2vivj − ξ(k)viVj,∗λ(k) − ξ(k)vjVi,∗λ(k) + Vi,∗λ(k)Vj,∗λ(k)

(ξ(k)vi − Vi,∗λ(k))(ξ(k)vj − Vj,∗λ(k))

i s(k)
s(k)

j

(cid:88)

k
(cid:88)

k
(cid:88)

=

=

=

k
≥ 0.

The last line follows from the equality

Λ[si, sj] =

i s(k)
s(k)

j

(cid:88)

k

and the entrywise non-negativity of





Λ[λ, λ(cid:62)] Λ[λ, s(cid:62)] λ
Λ[s, λ(cid:62)] Λ[s, s(cid:62)]
s
1

λ(cid:62)

s(cid:62)



 ∈ N .

A.3 Reformulation of Raghunathan et al. (2018)

Raghunathan et al. (2018) proposed the following SDP relaxation:

min
z, Z

s.t.

c(cid:62)zn,∗

zi,∗ ≥ 0, zi,∗ ≥ W [i − 1]zi−1,∗,
Z[zi,j, zi,j] = W [i − 1]j,∗Z[zi−1,∗, zi,j],
− Z[zi,j, zi,j] + (ui,j + li,j)zi,j − li,jui,j ≥ 0, ∀i, j,
(cid:18) Z z
z(cid:62) 1

∈ S +

(cid:19)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(29)

The proposed SDP relaxation is based on directly transcribing the constraints in the following quadratically
constrained quadratic program:

min
z
s.t.

c(cid:62)zn,∗

zi,∗ ≥ 0, zi,∗ ≥ W [i − 1]zi−1,∗,
z2
i,j = zi,jW [i − 1]j,∗zi−1,∗,
(zi,j − li,j)(ui,j − zi,j) ≥ 0, ∀i, j

(77)

To clarify the relationship with our proposed framework, we include the biases, introduce an additional equality
constraint ˆzi+1,∗ = W [i]zi,∗ + b[i] and the change the inequality from zi+1,∗ ≥ W [i]zi,∗ + b[i] to zi,∗ ≥ ˆzi,∗.
Without loss of generality, we will assume that ˆli,j < 0 < ˆui,j for all i ≥ 1, otherwise the neuron (i, j) could
be treated as the identity, if ˆli,j ≥ 0, or zero, if ˆui,j ≤ 0. The post-activation bounds are then li,j = 0 and
ui,j = ˆui,j, and the constraint (zi,j − li,j)(ui,j − zi,j) ≥ 0 becomes zi,j(ˆui,j − zi,j) ≥ 0. With a change of variables
to the positive/negative splitting, (77) is equivalent to:

min
λ
s.t.

c(cid:62)(λ+

n,∗ − λ−

n,∗)

i,∗ ≥ 0, λ−
λ+
λ+
i,∗λ−
i,∗ = 0,
λ+
i,j(ˆui,j − λ+
0,∗ − λ−
((λ+

i,∗ ≥ 0, λ+

i+1,∗ − λ−

i+1,∗ = W [i]λ+

i,∗ + b[i],

i,j) ≥ 0, i ≥ 1,
0,∗) − l0,∗)(u0,∗ − (λ+

0,∗ − λ−

0,∗)) ≥ 0

(78)

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

Directly transcribing the constraints of (78) results in the following SDP:

min
λ, Λ

s.t.

c(cid:62)λ

(30)

i,∗Ui,∗, Λ(cid:11) − uili ≥ 0,

i,j] ≥ 0, i ≥ 1,

i,j, λ+

(l0,i + u0,i)Ui,∗λ − (cid:10)U (cid:62)
ˆui,jλ+
i,j − Λ[λ+
W [i]j,∗λ = b[i]j,
i,j, λ−
Λ[λ+
i,j] = 0,
(cid:19)
(cid:18) Λ λ
λ(cid:62) 1

∈ S+,

where U is deﬁned as Ui,∗λ = λ+

0,i − λ−
0,i.

λ ≥ 0

A.3.1 Extensions to Raghunathan et al. (2018)

Direct Extensions. The following works have focused on improving some of the computational aspects of
Raghunathan et al. (2018) (e.g., time to solution, memory usage) without aiming to improve the relaxation gap
of Raghunathan et al. (2018).

The construction in Dvijotham et al. (2020) is based on the diagonally dominant SOS hierarchy (0-DSOS)
relaxation. It approximates S+ with the cone of diagonally dominant matrices, ultimately resulting in an LP.
This is a further relaxation of (30), and reduces computation time at the expense of a larger relaxation gap.

Based on the fact that memory usage, rather than compute, is the main barrier for scaling of SDPs, Dathathri
et al. (2020) proposes solving (30) using ﬁrst order methods. This comes at the expense of increased time to
solution.

SDP-based Branching. Previously, Zhang (2020) deﬁned an SDP relaxation to be tight if it has a unique
rank-one solution. Based on this, Anderson et al. (2021) proposed a metric measuring how far the solution is
from being rank one and shows that a uniform partitioning of the input (before neuron bound propagation) leads
to the greatest reduction of an upper bound of this metric. This approach can be thought of as branching on
the input.

Critically, Theorem 4.1 states that the CPP (13) is always exact, even if the optimal solution is not rank one. If
an optimizer of the (0-SOS) relaxation (14) happens to be completely positive, it is exact even if it is not rank
one. This means that the rank-one condition is a suﬃcient but not necessary condition for exactness. For this
reason, we propose a less-restrictive deﬁnition of tightness, allowing exact but non-rank-one optimizers; these
points are further clariﬁed in Section A.5.

Ma and Sojoudi (2020) work directly with (30), and propose a method equivalent to spatial branch and bound.
In particular, they choose a basis {φi} for RN , and partition the search space along the axes aligned with {φi}.
To illustrate how their approach works, consider {φi} given by the standard basis vectors, and a partitioning of
λ+
i,j. Their approach generates M sub-problems that are determined by points li,j = γ0 < γ1 < . . . < γM = ui,j,
where the mth sub-problem is deﬁned by the following additional constraints:

λ+
i,j ≤ γm
λ+
i,j ≥ γm−1
γmλ+

i,j − γmγm−1 − Λ[λ+

i,j, λ+

i,j] + γmλ+

i,j ≥ 0

(79)

(80)

(81)

Notice that equation (81) is the cross-quadratic constraint derived from λ+

i,j ≤ γm and λ+

i,j ≥ γm−1.

A.4 Reformulation of Fazlyab et al. (2022)

Fazlyab et al. (2022) develop a method where they abstract the deﬁnition of particular constraint sets as quadratic
constraints. In contrast to the quadratic constraints presented thus far in this paper, the quadratic constraints
in Fazlyab et al. (2022) are deﬁned by (potentially inﬁnite) matrix cones.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

A.4.1 Background on quadratic constraints

For example, if X is the input set, then the matrix cone describing X is deﬁned as

(cid:40)

PX :=

P |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:41)

≥ 0, ∀λ ∈ X

In the converse, PX can be used to over-approximate X via an inﬁnite set of constraints:

X ⊆

(cid:40)

(cid:92)

P ∈PX

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:41)

≥ 0

(82)

(83)

The quadratic constraints are related to the SDP/CPP formulations of veriﬁcation through the following equality:
(cid:42)

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

=

P,

(cid:19)(cid:62)(cid:43)

(cid:18)λ
1

(cid:19) (cid:18)λ
1

.

(84)

The premise of the SDP/CPP formulations is relaxing the rank-one outer product

(cid:18)λ
1

(cid:19)(cid:62)

(cid:19) (cid:18)λ
1

(cid:19)

(cid:18) Λ λ
λ(cid:62) 1

to

∈ S +.

A.4.2 Equivalence between atomic and inﬁnite quadratic constraints

Our analysis is based on identifying an equivalent, ﬁnite set of quadratic constraints (“atomic constraints”) for
each inﬁnite set of quadratic constraints proposed in Fazlyab et al. (2022)—these correspond with the extreme
rays of P. In other words, for each quadratic constraint set P with inﬁnite cardinality, we will ﬁnd a ﬁnite set
ˆP such that

In particular, if

then

(cid:40)

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:92)

P ∈P

(cid:41)

(cid:40)

≥ 0

=

(cid:92)

P ∈ ˆP

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

P =




Nineq
(cid:88)



i=1

αiP (i)

ineq +

Neq
(cid:88)

i=1

βiP (i)

eq | αi ∈ R≥0, βi ∈ R

(cid:41)

≥ 0

.






(cid:40)

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:41)

(cid:40)

≥ 0

=

λ |

(cid:92)

P ∈P

(cid:18)λ
1

(cid:18)λ
1

(cid:19)(cid:62)

P (i)
ineq

(cid:19)

(cid:18)λ
1

≥ 0, i = 1, . . . , Nineq,

(cid:19)(cid:62)

P (j)
eq

(cid:19)

(cid:18)λ
1

= 0, j = 1, . . . , Neq

.

(cid:41)

The equivalence can be shown as follows:

• If λ satisﬁes

(cid:19)(cid:62)

(cid:19)(cid:62)

(cid:18)λ
1
(cid:18)λ
1

P (i)
ineq

P (j)
eq

(cid:19)

(cid:18)λ
1
(cid:19)
(cid:18)λ
1

≥ 0, i = 1, . . . , Nineq,

= 0, j = 1, . . . , Neq,

then for any P ∈ P using the decomposition P = (cid:80)Nineq

i=1 αiP (i)

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

=

(cid:18)λ
1

(cid:19)(cid:62) 


Nineq
(cid:88)

Neq
(cid:88)

αiP (i)

ineq +

βiP (i)
eq



(cid:19)

(cid:18)λ
1

i=1

i=1

ineq + (cid:80)Neq
i=1 βiP (i)


eq with αi ≥ 0,

(cid:19)(cid:62)

(cid:18)λ
1

αi

P (i)
ineq

(cid:19)

(cid:18)λ
1

+

Neq
(cid:88)

i=1

(cid:19)(cid:62)

(cid:18)λ
1

βi

P (i)
eq

(cid:19)

(cid:18)λ
1

=

Nineq
(cid:88)

i=1

≥ 0.

(85)

(86)

(87)

(88)

(89)

(90)

(91)

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

So

(cid:40)

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:41)

(cid:40)

≥ 0

⊇

λ |

(cid:92)

P ∈P

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)(cid:62)

(cid:18)λ
1

P (i)
ineq

(cid:19)

(cid:18)λ
1

≥ 0, i = 1, . . . , Nineq,

P (j)
eq

(cid:19)

(cid:18)λ
1

= 0, j = 1, . . . , Neq

(cid:41)
.

(92)

(cid:19)

(cid:18)λ
1
i=1 βiP (i)

ineq

(cid:19)(cid:62)

(cid:18)λ
P (i∗)
1
ineq + (cid:80)Neq
i=1 αiP (i)
(cid:18)λ
(cid:19)(cid:62)
(cid:18)λ
1
1

P

(cid:19)

• Given λ, suppose there exists i∗ such that

< 0. Then, letting αi∗ = 1, αi = 0 for i (cid:54)= 0 and

βj = 0, gives an example of P = (cid:80)Nineq

eq ∈ P with

< 0

(93)

An analogous argument holds if there exists i∗

such that

−sign

(cid:19)(cid:62)

(cid:32)(cid:18)λ
1

P (i∗)

eq

(cid:19)(cid:33)

.

(cid:18)λ
1

This shows that

(cid:19)(cid:62)

(cid:18)λ
1

P (i∗)

eq

(cid:19)

(cid:18)λ
1

(cid:54)= 0 with βi∗ =

(cid:40)

λ |

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)

(cid:18)λ
1

P

(cid:41)

(cid:40)

≥ 0

⊆

λ |

(cid:92)

P ∈P

(cid:19)(cid:62)

(cid:18)λ
1

(cid:19)(cid:62)

(cid:18)λ
1

P (i)
ineq

(cid:19)

(cid:18)λ
1

≥ 0, i = 1, . . . , Nineq,

P (j)
eq

(cid:19)

(cid:18)λ
1

= 0, j = 1, . . . , Neq

(cid:41)
.

(94)

Combining the inclusions in both directions shows that the two constraints are equal.

A.4.3 Polytopic input constraints

To encode polytopic input sets, X = {x ∈ Rk0 | Ax ≤ a}, Fazlyab et al. (2022) use the following quadratic
constraint set:

(cid:26)

PX =

P | P =

(cid:20) A(cid:62)ΓA −A(cid:62)Γa
−a(cid:62)ΓA a(cid:62)Γa

(cid:21)(cid:27)

where Γ ≥ 0, Γi,i = 0 and Γ = Γ(cid:62). Each P ∈ PX can be decomposed as

P =

(cid:88)

i(cid:54)=j

Γi,j

(cid:34)

A(cid:62)

i,∗Aj,∗
−aj Ai,∗−aiAj,∗
2

−aj A(cid:62)

i,∗−aiA(cid:62)

j,∗

(cid:35)

2
aiaj

As a consequence, PX deﬁnes constraints of the form:

(cid:88)

i(cid:54)=j

Γi,j(Ai,∗λ − ai)(Aj,∗λ − aj) ≥ 0

with parameters Γi,j ≥ 0, Γi,i = 0. The equivalent atomic constraints are

(Ai,∗λ − ai)(Aj,∗λ − aj) ≥ 0,

∀i (cid:54)= j.

A.4.4 ReLU constraints

Fazlyab et al. (2022) deﬁned a global quadratic constraint for ReLUs, acting on

of the form

P =





P1,1 P1,2 P1,3
P2,1 P2,2 P2,3
P3,1 P3,2 P3,3





(95)

(96)

(97)

(98)




z
ˆz
, through a constraint set

1

(99)

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

where

P1,1 = 0 ∈ RN ×N
P1,2 = diag(ρ) + T ∈ RN ×N
P1,3 = −ν ∈ RN
P2,2 = −2(diag(ρ) + T ) ∈ RN ×N
P2,3 = ν + η ∈ RN
P3,3 = 0 ∈ R

where η, ν ≥ 0, ρ is unconstrained, and the matrix T is deﬁned as

T :=

(cid:88)

1≤i<j≤N

γi,j(ei − ej)(ei − ej)(cid:62)

where ei is the ith basis vector, and γi,j ≥ 0. Each quadratic constraint, P , acting on

an equivalent quadratic constraint acting on





 by pre- and post-multiplying by







λ+
λ−
1







z
ˆz
 can be converted to
1
0
I
I −I
0
0


0
0

1

We have taken the liberty to substitute in α = 0, β = 1 in zi,j = max(αˆzi,j, β ˆzi,j). We have also renamed
constraints to prevent notational clash with our notation.

The quadratic constraint set, P, thus enforces quadratic constraints of the form (Fazlyab et al., 2022, Lemma 3
and Appendix D):

(cid:88)

0 ≥

(cid:2)ρ(i,j)λ+

i,jλ−

i,j − νi,jλ+

i,j − ηi,jλ−

i,j

(cid:3)

(i,j)

+

(cid:88)

(i,j)(cid:54)=(k,l)

γ(i,j),(k,l)(λ+

i,j − λ+

k,l)(λ−

i,j − λ−

k,l)

Each term in the sum decomposes into the atomic constraints (together, equivalent to the full sum)

λ+
i,j ≥ 0
λ−
i,j ≥ 0
i,jλ−
λ+
i,j = 0
i,j − λ−
k,l) ≤ 0

(λ+

i,j − λ+

k,l)(λ−

Expanding out (111),

i,j − λ+
i,jλ−

0 ≥ (λ+
= λ+
= −λ+

k,l)(λ−
i,jλ−
k,l − λ+
i,j = 0, the constraint λ+

i,j − λ−
k,l)
k,l − λ+
k,lλ−
k,lλ−
i,jλ−

i,j − λ+

k,l + λ+

i,jλ−

i,j

k,lλ−

i,j + λ+

k,lλ−

k,l

Since either λ+
constraints λ+

i,j = 0 or λ−
i,jλ−

k,l ≥ 0 and λ+

k,lλ−

i,j ≥ 0.

i,j ≥ 0 can be decomposed into two separate

In summary, the equivalent atomic constraints are:

λ+
i,j ≥ 0,
i,jλ−
λ+

k,l ≥ 0,

A.4.5 Extensions to Fazlyab et al. (2022)

λ−
i,j ≥ 0,
i,jλ+
λ−

i,jλ−
λ+

i,j = 0 ∀(i, j)

k,l ≥ 0 ∀(i, j) (cid:54)= (k, l).

(115)

(116)

Direct Extensions. Newton and Papachristodoulou (2021) showed that the formulation proposed in Fazlyab
et al. (2022) exhibits chordal sparsity. They exploit this insight by using an oﬀ-the-shelf solver, SparseCoLo,
capable of taking advantage of chordal sparsity. This approach oﬀers an advantage for deep networks. Because
the formulation is the same as Fazlyab et al. (2022), it exhibits the same relaxation gap.

(100)

(101)

(102)

(103)

(104)
(105)

(106)

(107)

(108)

(109)

(110)

(111)

(112)

(113)

(114)

Robin Brown, Edward Schmerling, Navid Azizan, Marco Pavone

A.5 Rank and Exactness

Zhang (2020) deﬁned an SDP relaxation to be tight if it has a unique rank-one solution. The rationale for such
a deﬁnition is because if an interior-point method converges to a maximum rank solution, the optimal solution
will not be rank-one unless it is unique.

Based on Theorem 4.1, however, we propose a less restrictive deﬁnition of tightness for SDP-based relaxations.
Speciﬁcally, we say that an SDP-relaxation is tight if the optimal value is equal to that of (6). This deﬁnition is
motivated by the fact that Theorem 4.1 does not preclude high rank solutions. As a consequence, if the optimal
solution for Problem (14)(0-SOS) is feasible for Problem (13) (CPP), then it is exact, regardless of its rank.

In this section, we show that the revised deﬁnition is not vacuous, and there are in fact veriﬁcation instances
where the optimal solution of the SDP-relaxation is exact despite not being rank-one, and show how the optimal
solutions of (6) can be recovered from the SDP solution.

Consider the ReLU network given by the following weights and biases,






1
1
1 −1
−1
1
−1 −1




 ,






0
1
0
0
−1 1
0
0
1
0 −1
0
(cid:18) 1

1
0
−1 −1 1

W [0] =

W [1] =

W [2] =


0
0

 ,
0
1
(cid:19)
0
1

,

b[0] =

b[1] =

b[2] =


0
0


0
0





(cid:18)0
0











2.1
0
2.1
0
(cid:19)

W [3] =

(cid:19)

(cid:18)−1
−1

,

b[3] = (cid:0)2.1(cid:1)

and the safety rule to be veriﬁed deﬁned by:

X := {z0,∗ | −1 ≤ z0,∗ ≤ 1} ⊂ R2
Y := {z3,∗ | z3,∗ ≥ −2.1} ⊂ R.

(117)

(118)

(119)

(120)

(121)
(122)

We form the 0-SOS relaxation, (14), and solve the SDP using SCS; this had an optimal value of −2.0002017.

We then factored the optimal solution using the using the Alternate Least Square Using Projected Gradient
Descent algorithm—Lin (2007) (implemented in the NMF.jl package). We found a rank-four factorization with a
maximum entry-wise error of 0.0001158, and objective value of −2.0002022. The corresponding values of λ±,(k)
and ξ(k) are as follows:

0,∗

k = 1

k = 2

k = 3

k = 4

λ+,(1)
0,∗ =

λ+,(2)
0,∗ =

λ+,(3)
0,∗ =

(cid:18) 0.491844

3.32366 × 10−5
(cid:18)4.76418 × 10−5
0.504452
(cid:19)

(cid:18)0.0
0.0

,

(cid:19)

(cid:19)

,

,

λ−,(1)
0,∗ =

λ−,(2)
0,∗ =

λ−,(3)
0,∗ =

λ+,(4)
0,∗ =

(cid:19)

(cid:18)0.411613
0.411621

,

(cid:18) 0.0

(cid:19)

0.491843
(cid:18)0.504447
0.0
(cid:18)0.522856
0.522838
(cid:18)0.0
0.0

(cid:19)

(cid:19)

(cid:19)

λ−,(3)
0,∗ =

ξ(1) = 0.491849

(123)

ξ(2) = 0.504478

(124)

ξ(3) = 0.522831

(125)

ξ(4) = 0.411569

(126)

It can be seen that each solution input corresponds to a corner of X . Each of these solutions are plotted
alongside the level sets of the network in Figure 5. In this example, the optimal SDP-relaxation had a non-
negative factorization, but this is not generally guaranteed. This does suggest, however, that one should attempt
to ﬁnd a non-negative factorization of the 0-SOS relaxation as a certiﬁcate of optimality.

A Uniﬁed View of SDP-based Neural Network Veriﬁcation through Completely Positive Programming

Figure 5: This ﬁgure plots the level set of the network deﬁned by the weights and biases in Equations (117)-(120),
as well as the inputs, (123)-(126), corresponding to individual factors in the SDP solution.

