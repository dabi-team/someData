1
2
0
2

c
e
D
1
1

]

G
L
.
s
c
[

2
v
4
2
8
5
1
.
0
1
1
2
:
v
i
X
r
a

Tractability from overparametrization:
The example of the negative perceptron

Andrea Montanari∗, Yiqiao Zhong∗, Kangjie Zhou†

December 14, 2021

Abstract

In the negative perceptron problem we are given n data points (xi, yi), where xi is a d-dimensional
vector and yi ∈ {+1, −1} is a binary label. The data are not linearly separable and hence we content
ourselves to ﬁnd a linear classiﬁer with the largest possible negative margin. In other words, we want to
ﬁnd a unit norm vector θ that maximizes mini≤n yi(cid:104)θ, xi(cid:105). This is a non-convex optimization problem
(it is equivalent to ﬁnding a maximum norm vector in a polytope), and we study its typical properties
under two random models for the data.

We consider the proportional asymptotics in which n, d → ∞ with n/d → δ, and prove upper and
lower bounds on the maximum margin κs(δ) or —equivalently— on its inverse function δs(κ). In other
words, δs(κ) is the overparametrization threshold: for n/d ≤ δs(κ) − ε a classiﬁer achieving vanishing
training error exists with high probability, while for n/d ≥ δs(κ) + ε it does not. Our bounds on
δs(κ) match to the leading order as κ → −∞. We then analyze a linear programming algorithm to
ﬁnd a solution, and characterize the corresponding threshold δlin(κ). We observe a gap between the
interpolation threshold δs(κ) and the linear programming threshold δlin(κ), raising the question of the
behavior of other algorithms.

Contents

1 Introduction and main results

1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Summary of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Connection with random polytope geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Related work

2.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 The case of random labels

3.1 Existence of κ-margin classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 A linear programming algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Labels correlated with a linear signal

4.1 Existence of κ-margin classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Linear programming algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Asymptotic estimation error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Polytope geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Gradient descent

5.1 A diﬀerentiable loss function without tuning parameters . . . . . . . . . . . . . . . . . . . . .
5.2 Numerical experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

∗Department of Statistics and Department of Electrical Engineering, Stanford University
†Department of Statistics, Stanford University

2
2
4
6

7
8

8
8
9

10
11
12
14
15

16
16
17

1

 
 
 
 
 
 
6 Proof ideas

7 Conclusion

A κ-margin classiﬁers in the pure noise model:

Proofs of Theorems 3.1 and 3.2
A.1 Phase transition lower bound: Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . .
A.2 Phase transition upper bound: Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . .
A.3 Additional proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B Linear programming algorithm in the pure noise model:

Proof of Theorem 3.3

C κ-margin classiﬁers in the linear signal model:
Proofs of Theorems 4.1, 4.2, 4.4, 4.5 and 4.6
C.1 Phase transition lower bound: Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . .
C.2 Phase transition upper bound: Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . .
C.3 Solution space geometry: Proofs of Theorems 4.4, 4.5 and 4.6 . . . . . . . . . . . . . . . . . .
C.4 Additional proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Linear programming algorithm in the linear signal model:

Proofs of Theorems 4.3 and 4.7
D.1 Reduction via Gordon’s comparison theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Convergence to the asymptotic limit: Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . .
D.3 Analysis of maximization of M (ρ, r): Proof of Theorem 4.3 continued . . . . . . . . . . . . .
D.4 Estimation error: Proof of Theorem 4.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.5 Additional proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Gradient descent: Proof of Theorem 5.1

1 Introduction and main results

1.1 Background

19

21

26
26
30
34

38

42
43
45
47
52

68
69
70
76
78
83

87

Empirical risk minimization (ERM) is the dominant paradigm in statistical learning. Given n data points
(xi, yi) ∈ Rd × {+1, −1} (with xi a feature vector and yi a label), ERM proposes to ﬁnd a model f ( · ; θ) :
Rd → R by minimizing the following empirical risk :

(cid:98)Rn(θ) =

1
n

n
(cid:88)

i=1

(cid:96)(cid:0)yi; f (xi; θ)(cid:1) , θ ∈ Θ .

(1)

Here (cid:96) : {+1, −1} × R → R is a loss function1 such that minx∈R (cid:96)(+1; x) = minx∈R (cid:96)(−1; x) = 0.

By construction, the empirical risk is always non-negative, and hence so is the training error: minθ∈Θ (cid:98)Rn(θ) ≥

0. However, state-of-the-art models in machine learning are often overparametrized, in the sense that they
can achieve vanishing, or nearly vanishing training error, even with noisy labels: minθ∈Θ (cid:98)Rn(θ) = 0 [NTS15].
The emphasis on noise is crucial here: classically one would expect vanishing empirical risk only in the ab-
sence of noise, i.e., if we had E{(cid:96)(y; f∗(x))} = 0 for some function f∗ that happens to be in the model class
{f ( ·; θ)}θ∈Θ. In contrast, modern neural networks often achieve vanishing training error even if the true
labels are replaced by purely random ones [ZBH+21, BMR21]. Of course, if labels are noisy (and excluding
degenerate choices of the loss (cid:96)) E{(cid:96)(y; f∗(x))} > 0 holds strictly for any ﬁxed f∗.

Overparametrization is believed to play a crucial role from the optimization viewpoint.

In general,
optimizing the empirical risk is a highly non-convex problem, but it has been informally conjectured that
highly overparametrized models are easy to optimize despite non-convexity.

1If this matters to the reader, they can assume that Θ ⊆ Rp is a closed set and (cid:96)(±1; · ) are lower semicontinuous fuctions,

although we will soon restrict ourselves to a very speciﬁc example.

2

In view of these ﬁndings, it is natural to study the geometrical and computational properties of the set

of interpolators:

ERM0 := (cid:8)θ ∈ Θ : (cid:96)(cid:0)yi; f (xi; θ)(cid:1) = 0 ∀i ≤ n(cid:9) .

(2)

This deﬁnition can be of course generalized by considering the set ERMε of vector parameters θ such that
(cid:98)Rn(θ) ≤ ε. In this paper we will focus on ERM0.

Two important questions are:

Q1. Existence: Is the set ERM0 non-empty?

Q2. Tractability: Can we ﬁnd θ ∈ ERM0 using eﬃcient algorithms?

Of course, answering these questions is only a ﬁrst step towards other questions that are important from
a statistical viewpoint: When do gradient-based algorithms succeed in ﬁnding θ ∈ ERM0? What are
the special properties of the points θ ∈ ERM0 selected by such algorithms? What are the generalization
properties of the resulting model f ( · ; θ)?

In the above formulation, learning is equivalent to solving a constraint satisfaction problem (CSP) with
continuous variables, as deﬁned in Eq. (2): We want to assign values to the variables θ = (θ1, . . . , θd)
If data {(yi, xi)}i≤n are assumed to be i.i.d.
as to satisfy the n constraints (cid:96)(yi; f (xi; θ)) = 0, i ≤ n.
from a common distribution P (as we will do below), this is a random CSP. Questions Q1 and Q2 have
been extensively studied for some canonical random CSPs over small alphabets (i.e., with decision variables
θ ∈ {1, . . . , q}d for small constant q). Our analysis draws inspiration form this literature and we will provide
pointers in Section 2.

In contrast, much less is known rigorously for models with continuous variables, and for models with
relevance in statistical learning. A notable exception is of course provided by convex models, namely models
in which each constraint set Ci := (cid:8)θ ∈ Θ :
(cid:96)(cid:0)yi; f (xi; θ)(cid:1) = 0} is convex. However, in this case the
tractability question is typically trivial and also the existence question can be addressed using tools that are
not available more generally.

The simplest example of a convex constraint satisfaction problem is overparametrized linear regression:
(cid:96)(cid:0)yi; f (xi; θ)(cid:1) = (yi − (cid:104)θ, xi(cid:105))2. In this case Ci is an aﬃne space of codimension 1. Both existence (Q1) and
tractability (Q2) are elementary. For Q1, a solution exists (for any set of labels yi), as soon as rank(X) = n
(here X is the matrix with rows x(cid:62)
i ). In particular, if the vectors xi are in generic positions, this happens
if and only if n ≤ d. For Q2, ﬁnding a solution is equivalent to solving a linear system of equations, and can
be done, e.g., minimizing the square loss via gradient descent. Despite the simplicity of overparametrized
linear regression, a number of insights have been gained from the study of interpolation in high-dimensional
linear regression, see e.g. [LR20, HMRT21, BLLT20, BMR21].

In binary linear classiﬁcation we have

ERM0(κ) := (cid:8)θ ∈ Sd−1 : yi(cid:104)xi, θ(cid:105) ≥ κ ∀i ≤ n(cid:9) .

(3)

where Sd−1 := {θ ∈ Rd : (cid:107)θ(cid:107)2 = 1} is the unit sphere in d dimensions, and κ is a margin parameter.
Therefore, each constraint set Ci is the intersection of a half-space with the unit sphere. For κ ≥ 0, the
problem of ﬁnding θ ∈ ERM0(κ) is equivalent to the one in which the constraint (cid:107)θ(cid:107)2 = 1 is replaced by
(cid:107)θ(cid:107)2 ≤ 1. Indeed, if a positive margin solution is found with (cid:107)θ(cid:107)2 < 1, θ can be rescaled to yield an even
better margin. The set {θ : (cid:107)θ(cid:107)2 ≤ 1, yi(cid:104)xi, θ(cid:105) ≥ κ ∀i ≤ n} is convex. A solution can be found by standard
convex optimization algorithms, and hence question Q2 is easy to address.

For binary classiﬁcation with positive margin, the question Q1 of existence of solutions has also been
studied in detail. For several distributions over the data {(yi, xi)}, it is known that the problem undergoes
a sharp phase transition when n/d crosses a threshold δs(κ), which is precisely characterized. Solutions exist
with high probability for n/d ≤ δs(κ) − ε, and they do not exist for n/d ≥ δs(κ) + ε; see Section 2.

In contrast, for κ < 0, the constraint θ ∈ Sd−1 cannot be modiﬁed, and ERM0(κ) is the random set
obtained by carving n spherical caps out of the sphere. This is arguably the simplest non-convex example
of the problem introduced above and was studied in the statistical physics literature under the name of
negative perceptron [FP16]. While physicists derived a detailed picture of certain geometric features of this
model, their analysis use sophisticated non-rigorous techniques and does not formally address the tractability
question.

3

1.2 Summary of results

In this paper we consider the negative perceptron problem under two simple data distributions.

Random labels. We assume data to be isotropic Gaussian, and labels to be pure noise.

xi ∼ N(0, Id) ⊥ yi ∼ Unif({+1, −1}) .

(4)

Labels correlated with a linear signal. We assume the same simple model for the feature vectors, but

now the labels depend on an one-dimensional projection of the feature vectors:

P(yi = +1(cid:12)
Here, without loss of generality (cid:107)θ∗(cid:107)2 = 1. Further, we will assume ϕ : R → [0, 1] to be monotone
increasing, with ϕ(t) approaching 0 or 1 exponentially fast as t → ±∞.

(cid:12)xi) = ϕ(cid:0)(cid:104)xi, θ∗(cid:105)(cid:1) .

xi ∼ N(0, Id)

Under each of these data distributions, we study the existence and tractability questions.

In order
to discuss our result it is useful to deﬁne an interpolation threshold δs(κ) (the subscript ‘s’ stands for
satisﬁability) and an algorithmic threshold δalg(κ) as follows. The satisﬁability threshold is the largest value
of n/d such that ERM0 is non-empty with probability bounded away from zero. In formulas, denote by
Pn,d( · ) the probability distributions over instances of negative perceptron, we let:
(cid:110)

(cid:111)

(cid:16)

(cid:17)

δs(κ) := sup

δ :

lim inf
n→∞

Pn,n/δ

ERM0 (cid:54)= ∅

> 0

.

(5)

The algorithmic threshold is the largest value of n/d such that there exists a polynomial-time algorithm

alg

that takes as input the data y, X and outputs a global empirical risk minimizer (i.e., an element of

(cid:98)θ
ERM0), with probability bounded away from zero. Namely

δalg(κ) := sup

(cid:110)

δ : ∃ (cid:98)θ

alg

∈ Poly s.t.

Pn,n/δ

alg

(cid:16)

(cid:98)θ

lim inf
n→∞

(y, X) ∈ ERM0

(cid:17)

(cid:111)

,

> 0

(6)

(Here we denote by Poly the class of estimators that can be implemented by polynomial-time algorithms2.)
Note that we expect δs(κ), δalg(κ) to be sharp thresholds in the sense that for δ < δs(κ) (or δ < δalg(κ)), the
above probabilities are not only bounded away from zero but actually converge to one. However we will not
attempt to prove the sharp threshold property here.

Our main results are upper and lower bounds on the threshold δs(κ) within both of the pure noise model
and the linear signal model. These bounds are within a factor 1 + oκ(1) of each other for large negative
κ. We further obtain lower bounds on δalg(κ) by constructing simple linear programming surrogates of the
original non-convex problem.

More in detail, denote by Φ(x) := (cid:82) x

√

−∞ φ(t) dt the Gaussian distribution function and by φ(x) :=
2π the Gaussian density. For random labels we prove that, as κ becomes large a negative

exp(−x2/2)/
(of course after n, d → ∞)

δs(κ) =

log |κ|
Φ(κ)

(cid:0)1 + oκ(1)(cid:1) ,

δalg(κ) ≥

1
Φ(κ)

(cid:0)1 + oκ(1)(cid:1) .

(7)

(Recall that for large κ < 0, 1/Φ(κ) = (1 + oκ(1))|κ|/φ(κ).)

For linear signals, let α > 0 be such that ϕ(x) = 1 − e−αx + o(e−αx) as x → ∞, ϕ(x) = eαx + o(eαx) as

x → −∞. Then we prove that

δs(κ) =

eα|κ| log |κ|
4Φ(κ)

(cid:0)1 + oκ(1)(cid:1) ,

δalg(κ) ≥

eα|κ|
2Φ(κ)

(cid:0)1 + oκ(1)(cid:1) .

(8)

In the linear signal case, we also prove bounds on the values of correlation (cid:104)θ, θ∗(cid:105) that are achieved by
, θ∗(cid:105). We refer to
empirical risk minimizers θ ∈ ERM0, and on the ones generated by our algorithm (cid:104)(cid:98)θ
Section 4 for precise statements.

alg

2This requires a slight modiﬁcation of the problem in which a ﬁnite-precision version of the covariates xi is accepted as

input.

4

Figure 1: Phase diagram for the negative perceptron. The ‘replica symmetric’ prediction δRS(κ) coincides
with the satisﬁability threshold δs(κ) for κ ≥ 0 but is only an upper bound for κ < 0. Our improved δub(κ)
is strictly better than δRS(κ) for all negative values of κ. The lower bound δlb(κ) is inferior to the linear
programming threshold δlin(κ) (which is a lower bound on δalg(κ); see its precise deﬁnition in Eq. (20)) when
|κ| is small. As κ decreases, δlb(κ) surpasses δlin(κ). Finally, δlb(κ), δs(κ) and δub(κ) become asymptotically
equivalent as κ → −∞. The phase transition for the existence of κ-margin solution occurs in the region
delimited by max{δlb(κ), δlin(κ)} and δub(κ) (gray area).

While we summarized the behavior of our bounds for large negative κ, we obtain explicit (although
sometimes cumbersome) expressions for all κ < 0. These bounds are plotted in Figure 1 for the pure noise
case, and compared with the only previously known upper bound, which is given by the so called ‘replica
symmetric’ formula. The latter gives the correct phase transition for κ > 0 and is an upper bound for all κ
[Sto13a].

It is perhaps useful to pause for a few comments on the above results:

Volume heuristics. Φ(κ) is the asymptotic normalized volume of the spherical cap {θ ∈ Sd−1 : yi(cid:104)θ, xi(cid:105) ≤
κ}. This is the fraction of the space of the parameters ruled out by each of the n interpolation
constraints.
Any ﬁxed parameter vector θ ∈ Sd−1 satisﬁes all constraints with probability roughly (1 − Φ(κ))n. One
would guess the problem becomes unsatisﬁable when this probability becomes exponentially small in
d, i.e., n log(1 − Φ(κ)) ≈ −Cd. This heuristic gives the rough order of the threshold in Eq. (7), namely
n/d ≈ C (cid:48)/Φ(κ) but misses the log |κ| factor.

Tractability. The model becomes more overparametrized as d/n increases (moving down in Figure 1), or
κ decreases (moving left in Figure 1). Our results on δalg(κ) show that, if the overparametrization is
large enough, the problem of ﬁnding a global empirical risk minimizer becomes indeed tractable.

We will prove this by using a very simple optimization algorithm, which replaces the original non-
convex optimization problem by a linear programming surrogate; see Section 3.2 and Section 4.2. We
expect a similar or potentially better behavior for gradient descent or stochastic gradient descent, as
suggested by numerical simulations of Section 5.2.

Better algorithms or fundamental barriers? Despite the last point, there is a large multiplicative gap,
of order log |κ| between the satisﬁability threshold δs(κ) and the lower bound on the algorithmic
threshold δalg(κ). In words, for most values of the overparametrization, solutions exist but our simple
linear programming algorithm is not able to ﬁnd them.

5

It is natural to ask whether better optimization algorithms can be constructed for this problem, or there
is a fundamental computational barrier and indeed δs(κ) (cid:54)= δalg(κ). Such gaps are indeed quite common
in discrete random CSPs [ACO08, COHH17, BH21]. We leave this question to future investigation.

Beyond the linear regime. Over the last few years, considerable attention has been devoted to the remark
that vanishing training error can be achieved generically in overparametrized nonlinear models [JGH18,
COB19]. In a nutshell, upon linearizing the model f ( · ; θ) around the initialization θ0, interpolating the
n datapoints amounts to solving the system of n linear equations: Df (X; θ0)(θ − θ0) = y − f (X; θ0).
(Here f (X; θ) ∈ Rn is the vector with entries f (xi; θ).) For a non-degenerate Jacobian Df (X; θ0),
this has a solution as soon as the number of parameters exceeds the sample size.

Specializing this generic argument to the negative perceptron, we deduce that interpolation is possible
for n < d. In other words, this argument implies that δs(κ) ≥ 1. Explicitly, for n < d, we can solve the
linear system yi(cid:104)xi, θ(cid:105) = 1 for all i ≤ n, and hence obtain a linear separator by rescaling θ.
Our results imply that (for negative κ) the interpolation threshold δs(κ) is in fact much larger. Also,
linearization arguments cannot capture the gap between satisﬁability and algorithmic threshold. The
reason is that they come with an explicit eﬃcient algorithm (gradient descent or gradient descent with
respect to the linearized risk).

1.3 Connection with random polytope geometry

Given a dataset X, y that is not linearly separable, the associated maximum margin is
κ∗(X, y) := sup (cid:8)κ ∈ R : ∃θ ∈ Sd−1 s.t. yi(cid:104)xi, θ(cid:105) ≥ κ ∀i ≤ n(cid:9) ,

This is of course related to the threshold δs(κ) introduced in Eq. (5). If δ < δs(κ) then κ∗(X, y) ≥ κ with
probability bounded away from zero as n, d → ∞, n/d = δ.
If δ > δs(κ) then κ∗(X, y) ≤ κ with high
probability.

By a simple change of variables, κ∗(X, y) is directly related to the radius of a certain polytope. Given
Z ∈ Rn×d, a matrix with rows zi, denote by Rd(EZ) the radius of the polytope deﬁned by the inequalities
(cid:104)zi, ξ(cid:105) ≤ 1:

Rd(EZ) := max (cid:8)(cid:107)ξ(cid:107)2 : ξ ∈ EZ

(cid:9) , EZ := (cid:8)ξ ∈ Rd : (cid:104)zi, ξ(cid:105) ≤ 1 ∀i ≤ n(cid:9) .

(9)

Then

κ∗(X, y) = −

1
Rd(E−y(cid:12)X )

,

where −y (cid:12) X is the matrix with rows −yix(cid:62)
i . Hence, ﬁnding a maximum negative margin linear classiﬁer is
equivalent to ﬁnding a maximum norm vector in a polytope (a non-convex problem). This can be contrasted
with the case of positive margin that is equivalent to ﬁnding a minimum norm vector in a polytope that
does not contain the origin (a convex problem).

As a consequence, our results have implications on the geometry of random polytopes. In particular, if
y, X are distributed according to either of the pure noise or linear signal models introduced above, then our
results imply that the radius satisﬁes the following with probability bounded away from zero:

Rd(EZ) =

√

1
2 log δ

+

α
2 log δ

+ o(cid:0)(2 log δ)−1(cid:1) .

(10)

Here Z := −y (cid:12) X and it is understood that α = 0 for the pure noise model. Note that, under the pure
noise model, the rows of Z are i.i.d. N(0, Id), hence corresponding to an isotropic random polytope. We also
point out that our actual estimates on δs(κ) yield indeed a more precise characterization of Rd(EZ) than
in Eq. (10). The best earlier result on Rd(EZ) (for the pure noise model) was proved in the recent paper
[BV21]: We refer to Section 4.4 for a comparison.

The rest of this paper is organized as follows. The next section reviews related work. Sections 3 and 4
present our results for the random labels models and the linear signal model. In Section 5, we discuss an
alternative algorithmic approach based on gradient descent. We then outline the proof techniques in Section
6 and discuss the general picture and future directions in Section 7.

6

2 Related work

For κ ≥ 0, the phase transition for the existence of κ-margin solutions has been well studied for several
decades. For purely random labels, the maximum number of data points n that can be classiﬁed correctly
is also known as the model memorization capacity. The model we study is sometimes referred to as the
‘spherical perceptron’, to emphasize the fact that θ can take values on a sphere.

With an elegant combinatorial argument, Tom Cover [Cov65] proved that n random patterns in d dimen-
sions can be linearly separated with high probability if n < 2d − o(d), and can not if n > 2d + o(d). In our
language, this implies that δs(κ = 0) = 2. For κ > 0, Elizabeth Gardner [Gar88] employed the physicists’
replica method to obtain the replica-symmetric prediction δRS(κ) for the phase transition threshold. Gard-
ner’s prediction was rigorously conﬁrmed in [ST03, Sto13a]. In fact, the analysis of [Sto13a] implies that
δs(κ) ≤ δRS(κ) also for negative κ.

Recent work has extended the above results, and determined the phase transition boundary δs(κ) for labels
yi dependent on a linear signal (cid:104)θ∗, xi(cid:105), and for non-isotropic Gaussian covariates [CS20, SC19, MRSY19].
Further, [CS20, SC19] considered the underparametrized regime δ > δs(κ) and studied parameter estimation
using logistic regression, while [MRSY19] characterizes the generalization error of max-margin classiﬁcation
within the over-parameterized regime δ < δs(κ).

The negative margin problem κ < 0 has been much less studied by mathematicians or statisticians. From
a technical point of view, it is more diﬃcult than the original spherical perceptron problem since one cannot
rely on convexity. Talagrand conjectured in [Tal10] that the replica symmetric formula δRS(κ) is an upper
bound for the phase transition δs(κ) for all κ ∈ R. Stojnic [Sto13a] used Gordon’s comparison inequality
to prove that this is is indeed the case. However, for κ suﬃciently negative, δs(κ) < δRS(κ) strictly as also
implied by our upper bound, cf. Fig. 1, due to the replica symmetry breaking (RSB) phenomenon. Also
note that Guerra’s interpolation techniques from spin glass theory [Gue03] cannot be applied to this case
because the model is not symmetric (it is akin to a ‘multi-species’ model [Pan15]).

Our upper bound is also based on Gordon’s inequality. However, following an approach introduced again
by Stojnic [Sto13b], we apply the inequality to an exponential of the cost function. We prove that this yields
an upper bound which captures the correct behavior at large negative κ.

To conclude the survey of mathematical results on the negative spherical perceptron, the recent paper
[AS20] develops algorithm to ﬁnd κ-margin solutions with κ < 0. This algorithm is based on the IAMP
(incremental approximate message passing) strategy of [Mon21, AMS21] and is guaranteed to succeed under
a certain no overlap gap condition on the order parameter. It is currently unknown for which values of κ
this condition holds. Finally [BV21] proves a lower bound on the inner radius of the convex hull of random
points in high dimension, a problem which relates via duality to the problem of Section 1.3. We provide
further comparison in Section 4.4.

In a parallel research eﬀort, the negative spherical perceptron model has recently attracted attention
within the physics community as a toy model for the jamming transition [FP16, FPS+17, FHU19, FSU19].
In particular [FP16] and [FPS+17] use the non-rigorous replica method from spin glass theory to derive an
exact formula for the phase transition δs(κ), in the case of purely random labels. The phase transition is
conjectured to take place in a full replica-symmetry-breaking phase. These authors also characterize the
limiting law of the margins yi(cid:104)xi, θ(cid:105) − κ as δ ↑ δs(κ). The paper [FHU19] generalizes this analysis to other
neural network models, with O(1) hidden units.

Our lower bound on δs(κ) is based on the “second moment method”, which has been broadly applied
to proving existence theorems in probability theory and computer science. Examples include the random
k-satisﬁability problem [AM02, AP03, AM06], and the Ising perceptron [DS19, APZ19, ALS21], where we
search for θ ∈ {−1, +1}d instead of the unit sphere. Unlike in these applications, we deal with a model with
continuous decision variables. For this reason, we cannot upper bound δs(κ) using the ﬁrst moment method.
As mentioned above, we develop instead an alternative approach based on Gordon’s comparison inequality.
Finally, our work is related to the question of determining the interpolation threshold (or memorization
capacity) of neural networks. Since the seminal work of Baum [Bau88], several papers have been devoted to
estimating the interpolation threshold of neural networks, or showing that interpolators can be constructed
eﬃciently [Sak92, Kow93, Dan19, Dan20, BELM20, MZ20]. However earlier work is either based on con-
structing special networks that are not typically produced by learning algorithms, or they apply to very high
overparametrization ratios (e.g. in the linear regime).

7

2.1 Notations

For any positive integer n, we let [n] = {1, 2, . . . , n}. For a scalar a, we write a+ = max{a, 0} and a− =
max{−a, 0}. We use (cid:107)u(cid:107) or (cid:107)u(cid:107)2 to denote the (cid:96)2 norm of a vector u, and we use (cid:107)M(cid:107)op to denote the
operator norm of a matrix M. We denote by Sd−1 the unit sphere in d dimensions.

We will use On(·) and on(·) for the standard big-O and small-o notation, where n is the asymptotic
variable. We occasionally write an (cid:29) bn if bn = on(an). We write ξ1(n) = on,P(ξ2(n)) if ξ1(n)/ξ2(n)
converges to 0 in probability. Additionally, ˘oκ(1) denotes a quantity that vanishes as κ → −∞, uniformly
in all other variables mentioned in the context. Hence, the meaning of ˘oκ(1) may change from line to line.
For example, if (ρ, κ) ∈ [−1, 1] × R are the variables of interest, and s : [−1, 1] × R → R is a function, then

s(ρ, κ) = ˘oκ(1) ⇐⇒ lim

κ→−∞

sup
ρ∈[−1,1]

|s(ρ, κ)| = 0.

Similarly, the notation ˘Oκ(1) is deﬁned via:

s(ρ, κ) = ˘Oκ(1) ⇐⇒ lim sup
κ→−∞

sup
ρ∈[−1,1]

|s(ρ, κ)| < +∞.

For two vectors u, v of the same dimension, we may write u ≥ v or u ≤ v to mean entry-wise inequality.
We always use Φ and φ to denote the c.d.f. and p.d.f. of standard normal distribution, respectively. We
write X ⊥ Y if X and Y are two independent random variables (or random vectors). Throughout this paper,
we assume the asymptotics limn→+∞(n/d) = δ ∈ (0, +∞).

3 The case of random labels

Throughout this section, we assume that the data {(xi, yi)}i≤n are i.i.d., with isotropic covariates xi ∼i.i.d.
N(0, Id) and random labels yi ∼i.i.d. Unif({+1, −1}) independent of xi.

To the best of our knowledge, the best available rigorous result in this context is the replica symmetric

upper bound in [Sto13a], which yields

δs(κ) ≤ δRS(κ) = E{(κ − G)2

+}−1 =

|κ|2
2Φ(κ)

(cid:0)1 + oκ(1)(cid:1) .

(11)

(Here expectation is with respect to G ∼ N(0, 1).) As demonstrated below, this upper bound has the wrong
behavior for large negative κ.

3.1 Existence of κ-margin classiﬁers

We begin by stating our lower bound.

Deﬁnition 1. For κ < 0, let c∗ = c∗(κ) > 0 be the unique positive solution of the equation

(Proof of existence and uniqueness is given in Lemma 1. (a) in the appendix.)

For q ∈ [−1, 1], deﬁne

c(1 − Φ(κ + c)) = φ(κ + c) .

Ψ(q) = − log Eq [exp(−c∗(κ)(G1 + G2))1 {G1 ≥ κ, G2 ≥ κ}] −

1
2δ

log (cid:0)1 − q2(cid:1) ,

(12)

where Eq denotes expectation with respect to (G1, G2)(cid:62) ∼ N

(cid:18)

0,

(cid:20)1
q

(cid:21)(cid:19)

q
1

. Finally, we deﬁne

δlb(κ) := sup

(cid:110)

δ > 0 s.t. Ψ(q) is uniquely minimized at q = 0 and Ψ(cid:48)(cid:48)(0) > 0

.

(13)

(cid:111)

Our ﬁrst theorem provides a lower bound on δs(κ).

8

Theorem 3.1. If δ < δlb(κ), then with probability bounded away from 0 there is a κ-margin solution, i.e.,

δlb(κ) ≤ δs(κ) .

(14)

Moreover, δlb(κ) = (1 + oκ(1))(log |κ|)/Φ(κ) as κ → −∞. Hence, for any ε > 0, there exists a κ = κ(ε) < 0,
such that for all κ < κ,

δ < (1 − ε)

log |κ|
Φ(κ)

⇒ lim inf
n→∞

P(ERM0 (cid:54)= ∅) > 0 .

Deﬁnition 2. For any κ < 0, deﬁne

(cid:40)

δub(κ) = inf

δ > 0 : ∃c > 0, s.t. ∀t > 0,

√

1
c2 + 4 + c

+

1
c

log

√

c2 + 4 + c
2

<

1
4t

−

δ
c

(cid:41)

log ψκ(−ct)

,

(15)

where, for t ≥ 0,

ψκ(−t) := E

(cid:110)

(cid:16)

exp

−t (κ − G)2
+

(cid:17) (cid:111)

, G ∼ N(0, 1).

(16)

We next state our upper bound on δs(κ). The ﬁrst part of this theorem was proved already in [Sto13b],
which however relied on numerical evaluations to argue that the resulting bound was superior to the replica
symmetric one.

Theorem 3.2. If δ > δub(κ), then with high probability there is no κ-margin solution, i.e.,

δub(κ) ≥ δs(κ) .

(17)

Moreover, δub(κ) = (1 + oκ(1))(log |κ|)/Φ(κ) as κ → −∞. Hence, for any ε > 0, there exists a κ = κ(ε) < 0,
such that for all κ < κ,

δ > (1 + ε)

log |κ|
Φ(κ)

⇒ lim
n→∞

P(ERM0 (cid:54)= ∅) = 0 .

Figure 1 reports the upper and lower bounds δub(κ), δlb(κ) as functions of κ. The proofs for Theorem 3.1

and Theorem 3.2 are deferred to Appendix A with outlines in Section 6.

3.2 A linear programming algorithm

As mentioned in Section 1.3, ﬁnding a κ-margin solution is equivalent to solving the following non-convex
optimization problem:

maximize (cid:107)θ(cid:107)2
2 ,
subject to yi(cid:104)xi, θ(cid:105) ≥ κ for all i ∈ [n],

(cid:107)θ(cid:107)2 ≤ 1 .

(18)

If the solution to this problem has unit norm, then it provides a κ-margin solution.

Imagine now trying to maximize this cost function, starting at a random initialization near the origin,
2 =
2). It is natural to try to maximize the linearized cost. We are therefore

for instance θ0 = εv with v ∼ Unif(Sd−1). Linearizing the objective around the initialization yields (cid:107)θ(cid:107)2
(cid:107)θ0(cid:107)2
led to the following algorithm:

2 + 2(cid:104)θ0, θ − θ0(cid:105) + O((cid:107)θ − θ0(cid:107)2

1. Draw v ∼ Unif(Sd−1).

2. Solve the following convex optimization problem

maximize (cid:104)v, θ(cid:105) ,
subject to yi(cid:104)xi, θ(cid:105) ≥ κ for all i ∈ [n],

(19)

(cid:107)θ(cid:107)2 ≤ 1.

9

In optimization language, this algorithm executes the ﬁrst step of Frank-Wolfe algorithm [FW+56].

We denote by (cid:98)θ the output of this algorithm.

(If the optimization problem (19) has more than one
maximizer, one of them can be selected arbitrarily.) Of course, if (cid:107)(cid:98)θ(cid:107)2 = 1, then (cid:98)θ is a κ-margin solution.
The next theorem establishes a phase transition boundary for this algorithm.

Theorem 3.3. Under the pure noise model, assume n, d → ∞ with n/d → δ. If

δ < δlin(κ) :=

1
Φ(κ)

,

(20)

then (cid:107)(cid:98)θ(cid:107)2 = 1 with high probability and therefore (cid:98)θ is, with high probability, a κ-margin solution. On the
other hand, if δ > δlin(κ) then, with high probability (cid:107)(cid:98)θ(cid:107)2 < 1.

In particular, for any constant ε ∈ (0, 1) there exists κ = κ(ε) < 0 such that for all κ < κ(ε) the following

holds. If

√

δ ≤ (1 − ε)

2π |κ| exp

(cid:19)

,

(cid:18) κ2
2

(21)

then (cid:98)θ is a κ-margin solution with high probability.

Since the algorithm described above can be implemented in polynomial time, the above result yields a

lower bound on the polynomial threshold: δalg(κ) ≥ δlin(κ) for all κ.

Remark 3.1. A slightly diﬀerent formulation of the same approach removes the norm constraint (cid:107)θ(cid:107)2 ≤ 1
in the convex program (19). Denote the solution of the resulting linear program by θlin. If (cid:107)θlin(cid:107)2 ≥ 1, then
a κ margin solution is given by (cid:98)θlin := θlin/(cid:107)θlin(cid:107)2.

It is easy to see that this version of the algorithm succeeds (i.e., (cid:107)θlin(cid:107)2 ≥ 1) if and only if the previous
does (i.e., (cid:107)(cid:98)θ(cid:107)2 = 1). Indeed, if (cid:107)(cid:98)θ(cid:107)2 = 1, then (cid:107)θlin(cid:107)2 ≥ 1 because (cid:98)θ is feasible for the linear program.
On the other hand, we show that (cid:107)θlin(cid:107)2 ≥ 1 implies (cid:107)(cid:98)θ(cid:107)2 ≥ 1. Assume by contradiction (cid:107)(cid:98)θ(cid:107)2 < 1. Then
deﬁning (cid:98)θt := (1 − t)(cid:98)θ + tθlin, there exists t∗ ∈ (0, 1] such that (cid:107)(cid:98)θt∗ (cid:107)2 = 1. This is a feasible point for (19),
thus leading to a contradiction.

4 Labels correlated with a linear signal

In this section we assume that the labels are correlated with an one-dimensional projection of the data. As
before, {(xi, yi)}i≤n are i.i.d. data, xi ∼ N(0, Id) and the labels yi ∈ {±1} satisfy

P (cid:0)yi = 1(cid:12)

(cid:12) (cid:104)xi, θ∗(cid:105)(cid:1) = 1 − P (cid:0)yi = −1(cid:12)

(cid:12) (cid:104)xi, θ∗(cid:105)(cid:1) = ϕ ((cid:104)xi, θ∗(cid:105)) ,

where θ∗ ∈ Sd−1 is the true signal, and ϕ : R → [0, 1] is a monotone increasing function. We make the
following assumption on the tail behavior of the link function ϕ.

Assumption 4.1 (Exponential tail). There exists an α > 0, such that as x → −∞,

1 + ϕ(x) − ϕ(−x)
2 exp(αx)

→ 1.

Remark 4.1. The above assumption holds for the logistic link function, i.e., ϕ(x) = (1 + e−αx)−1.

Throughout this section, we denote by (Y, G, W ) a vector of random variables with joint distribution:

(cid:26) G ∼ N(0, 1), P(Y = 1|G) = ϕ(G) = 1 − P(Y = −1|G), Y ∈ {+1, −1} ,

(Y, G) ⊥ W , W ∼ N(0, 1) .

(22)

10

4.1 Existence of κ-margin classiﬁers

We start with a deﬁnition that plays an important role in the phase transition lower bound.

Deﬁnition 3. We deﬁne auxiliary function δsec(ρ, κ, κt) on the set

{(ρ, κ, κt) : 0 < ρ < 1, 0 > ρκt > κ}

through the following steps:

1. For s ≥ κt, deﬁne κ(s) = (κ − ρs)/(cid:112)1 − ρ2. Then we know κ(s) < 0, hence by Lemma 1.(a) there

exists c(s) > 0 such that

2. For q ∈ [−1, 1], deﬁne

c(s) (1 − Φ(κ(s) + c(s))) = φ(κ(s) + c(s)).

e(q, s) = Eq [exp (−c(s)(G1 + G2)) 1 {G1 ≥ κ(s), G2 ≥ κ(s)}]

and

e(q) = 1 +

(cid:90) +∞

κt

pY G(s)

(cid:18) e(q, s)
e(0, s)

(cid:19)

− 1

ds,

where pY G denotes the density function of Y G.

(23)

(24)

(25)

3. Now set F (q) = − log(e(q)) and I(q) = − log(1 − q2)/2. Similarly as in Deﬁnition 1, let δsec(ρ, κ, κt) be
the supremum of all δ > 0 such that F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0, and F (q) + I(q)/δ achieves unique minimum
at q = 0.

Now we are in position to deﬁne δlb(κ; ϕ) as the supremum of all δ > 0, such that there exist parameters

ρ ∈ (0, 1), 0 > κ1 > κ2 > κ0 =

κ
(cid:112)1 − ρ2

satisfying the inequality below:



(cid:40)

(cid:16)

(cid:112)

> min

max

ρY G +

1 − ρ2W ≤ κ

(cid:17)



(cid:32)

, E



κ0 −

1
δ

P






P



+ inf
c≥0

(cid:32)

Y G ≥

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

E

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+

+

1
c2

ρY G
(cid:112)1 − ρ2

1

(cid:40)

E

Y G <










+
(cid:112)1 − ρ2
ρ

(cid:33)2

− W

(cid:32)

, δsec

ρ,

(cid:112)

1 − ρ2κ2,

(cid:33)−1

(cid:112)1 − ρ2
ρ

κ1

(cid:41) (cid:32)

κ1

κ0 −

ρY G
(cid:112)1 − ρ2

−

(cid:112)

1 + c2W

(cid:41)
.










(cid:33)2

+

The next theorem establishes that δlb(κ; ϕ) is indeed a lower bound on the satisﬁability threshold, and

establishes an approximation for this lower bound which holds for κ large and negative.

Theorem 4.1. Under the linear signal model, if δ < δlb(κ; ϕ), then with probability bounded away from 0,
there is a κ-margin solution, i.e., we have

Moreover, under Assumption 4.1, δlb(κ; ϕ) = (1 + oκ(1))eα|κ| log |κ|/(4Φ(κ)) as κ → −∞. Hence, for any
ε > 0, there exists a κ = κ(ε; ϕ) < 0, such that for all κ < κ, as long as

δs(κ) ≥ δlb(κ; ϕ) .

(26)

δ < (1 − ε)

eα|κ| log |κ|
4Φ(κ)

⇒ lim inf
n→∞

P(ERM0 (cid:54)= ∅) > 0 .

We next introduce and state our upper bound on the threshold δs(κ) in the linear signal model.

Deﬁnition 4. For any κ < 0 and ρ ∈ [−1, 1], set

(cid:40)

δub(κ, ρ; ϕ) := inf

δ > 0 : inf
c>0

(cid:32)

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

− inf
u>0

(cid:26) c
4u

−

δ
c

(cid:27)(cid:33)

log ψκ,ρ(−u)

(cid:41)

< 0

,

where

ψκ,ρ(−u) = E

(cid:20)

(cid:18)

exp

−u

(cid:16)

κ − ρY G −

(cid:112)

1 − ρ2W

(cid:19)(cid:21)

(cid:17)2

+

.

(27)

Then we deﬁne δub(κ; ϕ) = supρ∈[−1,1] δub(κ, ρ; ϕ).

11

Figure 2: Theoretical predictions of the phase transition thresholds and the linear programming lower bound
for labels correlated to a linear signal. Here the link function is logistic: ϕ(t) = (1 + e−t)−1. The phase
transition for the existence of κ-margin solution occurs in the region delimited by δlb(κ; ϕ) and δub(κ; ϕ)
(gray area).

Theorem 4.2. Under the linear signal model, if δ > δub(κ; ϕ), then with high probability there is no κ-margin
solution, i.e., we have

δs(κ) ≤ δub(κ; ϕ) .
(28)
Moreover, under Assumption 4.1, δub(κ; ϕ) = (1 + oκ(1))eα|κ| log |κ|/(4Φ(κ)) as κ → −∞. That is to say,
for any ε > 0, there exists a κ = κ(ε; ϕ) < 0, such that for all κ < κ,

δ > (1 + ε)

eα|κ| log |κ|
4Φ(κ)

⇒ lim
n→∞

P(ERM0 (cid:54)= ∅) = 0 .

The proofs of Theorem 4.1 and Theorem 4.2 are deferred, respectively, to Appendix C.1 and C.2.

4.2 Linear programming algorithm

Our bounds imply that the interpolation threshold δs(κ) is signiﬁcantly higher in the linear signal model
as compared to the case of purely random labels. Indeed the threshold changed from δs(κ) = exp{κ2/2 +
O(log |κ|)} to δs(κ) = exp{κ2/2 + α|κ| + O(log |κ|)}. On the other hand, the algorithm of Section 3.2, if
applied without modiﬁcations, has a similar threshold under the current model as for pure noise labels.
The underlying reason is that the random direction v is nearly orthogonal to the signal θ∗, and hence the
algorithm is insensitive to the change in data distribution.

In order to overcome this limitation, we introduce the following modiﬁcation of the algorithm of Section

3.2.

1. Compute

v :=

1
n

n
(cid:88)

i=1

yixi .

(29)

2. Solve the following convex optimization problem

maximize (cid:104)v, θ(cid:105) ,

12

Figure 3: Maximizing the function M (ρ, r) as deﬁned in (34) over ρ ∈ [−1, 1], r ∈ [0, 1] numerically gives
the maximizer (ρ∗, r∗). The heatmaps show the values of r∗ (left) and ρ∗ (right) under varying κ and δ.
Left: Yellow region indicates the regime where a κ-margin solution exists. Right: ρ∗ gives the asymptotic
correlation (cid:104)(cid:98)θ, θ∗(cid:105).

subject to yi(cid:104)xi, θ(cid:105) ≥ κ for all i ∈ [n],

(30)

(cid:107)θ(cid:107)2 ≤ 1.

As in the case of purely random labels, we could remove the norm constraint (cid:107)θ(cid:107)2 ≤ 1, and normalize the
solution of the resulting linear program.

In order to state our results for this algorithm, recall the triple of random variables (Y, G, W ) introduced
at the beginning of this section. It is useful to introduce the random variable Zρ,r whose distribution depends
on ρ ∈ [−1, +1] and r ≥ 0:

We also deﬁne the domain Ω≥ ⊆ R2, via:

Zρ,r = ρY G +

(cid:112)

1 − ρ2 rW − κ .

(31)

(32)

.

(cid:111)

(cid:110)

Ω≥ =

(ρ, r) ∈ [−1, 1] × R≥0 : (1 − ρ2)r2δ−1 ≥ E[Z 2

ρ,r; Zρ,r < 0]

We also denote by Ω> the analogous set in which the inequality is satisﬁed strictly.

Theorem 4.3. Let ϕ satisfy Assumption 4.1. For (ρ, r) ∈ Ω≥, let s∗(ρ, r) be the only non-negative solution
of the equation

and deﬁne

(1 − ρ2)r2δ−1 = E[max{s, −Zρ,r}2] ,

M (ρ, r) = E(cid:2)(cid:0)Zρ,r + s∗(ρ, r)(cid:1)

(cid:3) + κ.

+

Finally, for r0 > 0 deﬁne M∗(r0) = max{M (ρ, r) : (ρ, r) ∈ Ω≥, r ≤ r0} and
(cid:110)

δlin(κ; ϕ) := sup

δ : ∀ δ < δ : Ω> ∩ {r ≤ 1} (cid:54)= ∅, M∗(∞) > M∗(1)

(33)

(34)

(35)

.

(cid:111)

If δ < δlin(κ; ϕ), then the linear programming algorithm outputs (cid:98)θ such that (cid:107)(cid:98)θ(cid:107)2 = 1 with high probability.
Therefore (cid:98)θ is, with high probability, a κ-margin solution.

In particular, for any constant ε ∈ (0, 1) there exists κ = κ(ε; ϕ) < 0 such that for all κ < κ(ε; ϕ) the

following holds. If

δ ≤ (1 − ε)

|κ|eα|κ|
2Φ(κ)

,

(36)

then (cid:98)θ is a κ-margin solution with high probability.

13

-5  -4.5-4  -3.5-3  -2.5-2  -1.5-1  -0.51e+09 1e+08 1e+07 1e+06 10000010000 1000  100   10    00.10.20.30.40.50.60.70.80.9-5  -4.5-4  -3.5-3  -2.5-2  -1.5-1  -0.51e+09 1e+08 1e+07 1e+06 10000010000 1000  100   10    00.10.20.30.40.50.60.70.80.94.3 Asymptotic estimation error

What is the geometry of the set of empirical risk minimizers? In this section we explore one speciﬁc aspect
of this question, namely what are the possible distances between the signal θ∗ and θ ∈ ERM0(κ), and where
in that spectrum is the solution (cid:98)θ found by the linear programming algorithm.

We begin by bounding the interval of values of (cid:104)θ, θ∗(cid:105).

Theorem 4.4. Let Assumption 4.1 hold. Deﬁne ρmin, ρmax with −1 ≤ ρmin ≤ ρmax ≤ +1 via

ρmin := sup (cid:8)ρ ∈ [−1, 1] : δub(κ, ρ; ϕ) < δ , ∀ρ ∈ [−1, ρ](cid:9) ,
ρmax := inf (cid:8)ρ ∈ [−1, 1] : δub(κ, ρ; ϕ) < δ , ∀ρ ∈ [ρ, 1](cid:9) .

(37)

(38)

(If δub(κ, ρ; ϕ) < δ for all ρ ∈ [−1, 1], the statement below is empty and therefore the deﬁnition of ρmin, ρmax
is immaterial.)

Then we have, with probability converging to one as n, d → ∞, n/d → δ:

(cid:8)(cid:104)θ, θ∗(cid:105) : θ ∈ ERM0(κ)(cid:9) ⊆ [ρmin, ρmax] .

(39)

In words, all solutions θ ∈ ERM0(κ) lie in the annulus (cid:104)θ, θ∗(cid:105) ∈ [ρmin, ρmax]. The next theorem charac-

terizes δub for large negative κ. (Recall that the notation ˘oκ(1) was introduced in Section 2.1.)

Theorem 4.5. Let Assumption 4.1 hold. As κ → −∞, the following holds uniformly over ρ ∈ [−1, 1]:

δub(κ, ρ; ϕ) =

(1 + ˘oκ(1))D (cid:0)κ2(1 − ρ2)(cid:1)
(cid:16)
ρY G + (cid:112)1 − ρ2W < κ
P

(cid:17) ,

where D : R≥0 → R≥0 is deﬁned by:

(cid:40)

D(a) = inf

b ≥ 0 : ∃c > 0,

1
√
c2 + 4

+

1
c

c +

c +

log

√

c2 + 4
2

(cid:26) c
4ta

+

b
c

< inf
t>0

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:27)(cid:41)

.

The next theorem provides a simpler expression for ρmax below the linear programing threshold δlin(κ; ϕ)

which we introduced in the previous section.

Theorem 4.6. Let Assumption 4.1 hold. If δ < δlin(κ; ϕ), then with high probability ERM0(κ) is not empty,
and

ρmax ≤ 1 − (1 + ˘oκ(1))

.

(40)

E

(cid:104)
(κ − Y G)2
+

(cid:105)

δ
2

We ﬁnally pass to studying the behavior of the linear programming algorithm introduced in the previous
section. The next result provides a simple deterministic approximation for (cid:104)(cid:98)θ, θ∗(cid:105) which is accurate for large
negative κ.

Theorem 4.7. Let Assumption 4.1 hold. We recall the deﬁnition of M (ρ, r) in (34) and denote m = E[Y G].
Further recall that (cid:98)θ denotes the κ-margin solution found by the linear programming algorithm. Then, the
following hold:

(a) If (ρ∗, r∗) is the unique maximizer of M (ρ, r) over [−1, 1] × [0, 1], then (cid:104)(cid:98)θ, θ∗(cid:105)

p
−→ ρ∗ as n → ∞.

(b) Let δ = δ(κ) be such that limκ→−∞ δ(κ) = ∞ and condition (36) holds. Then, there exists a suﬃciently

negative κ = κ(α, m, ε) > 0 such that the following holds for all κ < κ:

P

lim
n→∞

E(κ) :=

(cid:16)(cid:12)
(cid:12)
(cid:12)(cid:104)(cid:98)θ, θ∗(cid:105) − (1 − E(κ))
δ
δ0(κ)

1
2m2δ

+

,

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ ˘oκ(1) · E(κ)
|κ|2eα|κ|
2Φ(κ)

δ0(κ) :=

= 1,

.

14

Figure 4: Asymptotic predictions of the estimation error among k-margin solutions θ ∈ ERM0(κ), as a
function of κ for δ = 1.515. Here we use the logistic link function ϕ(t) = (1 + e−t)−1. The estimation
error of any κ-margin solution lies in the gray region. The linear programming algorithm of Section 4.2
achieves estimation error reported as the blue curve (namely (cid:112)2(1 − ρ∗) for ρ∗ introduced in Theorem 4.7
(a)). Vertical dashed lines correspond to κlin(δ) := sup{κ : δ < δlin(κ; ϕ)} (left vertical line) and κub(δ) :=
sup{κ : δ < δub(κ; ϕ)} (right vertical line).

Figure 4 reports the predictions for the (cid:96)2 estimation error ∆ := (cid:107)(cid:98)θ − θ∗(cid:107)2. The relation between this

error and the alignment ρ = (cid:104)(cid:98)θ, θ∗(cid:105) is simply given by

∆ = (cid:112)2(1 − ρ) .

It might be useful to point out a couple of interesting features of Figure 4.

1. We can interpret κ as tuning how constrained or overparametrized is the model: as κ becomes more neg-
ative, the problem becomes less constrained or more overparametrized. We observe that —somewhat
counterintuitively— the estimation error decreases as the problem becomes less constrained.

2. The solutions selected by the linear programming algorithm appear to be close to the lower edge of
the band of possible estimation errors for θ ∈ ERM0(κ). Further study is warranted to understand
precisely how atypical these solutions are.

4.4 Polytope geometry

As anticipated in Section 1.3, our results have implications on the geometry of random polytopes, which we
spell out explicitly.

i , and denote by Rd(EZ) the radius
Corollary 4.1. Let Z := −y (cid:12) X ∈ Rn×d be the matrix with rows −yix(cid:62)
of the polytope EZ := {θ : Zθ ≤ 1}, as deﬁned in Eq. (9). Then we have, for any ε > 0, assuming n, d → ∞,
n/d → δ,

Pn,d(Rd(EZ) ≤ ρ+ε(δ)(cid:1) = 1 ,
Pn,d(Rd(EZ) ≥ ρ−ε(δ)(cid:1) > 0 ,

lim
n→∞
lim inf
n→∞

ρ±ε(δ) :=

√

1
2 log δ

+

α ± ε
2 log δ

.

15

(41)

(42)

(43)

(Here it is understood that α = 0 in the case of pure noise labels.)

Given the matrix Z ∈ Rn×d, we can consider a diﬀerent way of constructing a random polytope, by taking
i . We then deﬁne the inner radius as the maximum

the convex hull of the rows of Z, namely zi = −yix(cid:62)
radius of a ball that is contained in this convex hull:

rd(E ◦

Z) := max (cid:8)r ≥ 0 : Bd(0; r) ⊆ E ◦

Z

(cid:9) ,

E ◦
Z := conv(z1, . . . , zn) .

(44)

The notation E ◦
Z is due to the fact that conv(z1, . . . , zn) is dual to the polytope EZ deﬁned in Eq. (9). The
outer radius of EZ and inner radius of its dual are related through the following standard duality relation.

Proposition 4.1 ([GK92]). Assume Rd(EZ) < ∞. Then

rd(E ◦

Z)Rd(EZ) = 1 .

Proof. The proof is elementary but we sketch it for the reader’s convenience. We note that v (cid:54)∈ conv(z1, . . . , zn)
if and only if there exists ξ ∈ Rd such that (cid:104)ξ, v(cid:105) > (cid:104)ξ, zi(cid:105) for all i ≤ n. Since 0 ∈ conv(z1, . . . , zn), we have
(cid:104)ξ, v(cid:105) > 0, and hence can assume, without loss of generality, (cid:104)ξ, v(cid:105) = 1. Therefore
Z) = inf (cid:8)(cid:107)v(cid:107)2 : v (cid:54)∈ E ◦

rd(E ◦

Z

(cid:9)
= inf (cid:8)(cid:107)v(cid:107)2 : (cid:104)ξ, v(cid:105) = 1 , (cid:104)ξ, zi(cid:105) < 1 ∀i ≤ n(cid:9)
= inf (cid:8)1/(cid:107)ξ(cid:107)2 : (cid:104)ξ, zi(cid:105) < 1 ∀i ≤ n(cid:9) = Rd(EZ)−1 ,

(i)

where (i) is due to inf{(cid:107)v(cid:107)2 : (cid:104)ξ, v(cid:105) = 1} = 1/(cid:107)ξ(cid:107)2. This completes the proof.

As a consequence of this duality, our results on the threshold δs(κ) have direct implications on rd(E ◦

Z).

Informally, with probability bounded away from zero,

rd(E ◦

Z) = (cid:112)2 log δ − α + oδ(1) .

(45)

Recently, Baldi and Vershynin [BV21] considered the problem of bounding rd(E ◦
Z). For the pure noise case,
Z) ≥ (cid:112)2(1 − ε) log δ for any δ > C(ε). Our
they prove that, with probability at least 1 − exp(−n), rd(E ◦
results hold in an asymptotic setting, while the one of [BV21] is non-asymptotic. On the other hand, we
provide a more precise characterization, that applies also to the linear signal model, and a matching upper
bound.

Remark 4.2. Neither Corollary 4.1 nor its consequence Eq. (45) is as accurate as our main results in
Theorems 3.1, 3.2 (for the pure noise case) or Theorems 4.1, 4.2 (for the linear signal case). In particular,
these simpler formulas do not account for the prefactors polynomial in κ which separate between δlin(κ) and
δs(κ).

5 Gradient descent

Our algorithmic results are based on an extremely simple linear programming approach. An interesting
open question is to understand the behavior and relative advantages of other algorithms. In this section we
introduce a diﬀerentiable loss function, whose minimization leads to a maximum negative margin solution,
and which does not require any tuning.

5.1 A diﬀerentiable loss function without tuning parameters

In the case of linearly separable data, it is known that gradient descent with respect to the logistic loss
converges to a maximum margin solution [SHN+18, JT18]. This approach has the advantage of providing a
diﬀerentiable cost function without tuning parameters. Is it possible to achieve the same for non-separable
data (to achieve maximum negative margin)?

16

We are interested in ﬁnding θ satisfying yi(cid:104)θ, xi(cid:105) ≥ κ for all i ≤ n. Let (cid:96) : R → R≥0 be a monotone

decreasing function with limx→+∞ (cid:96)(x) = 0. We deﬁne the empirical risk function:

(cid:98)Rn,κ(θ) :=

1
n

n
(cid:88)

i=1

(cid:96)(cid:0)yi(cid:104)xi, θ(cid:105) − κ(cid:107)θ(cid:107)2

(cid:1),

and consider using gradient descent (GD) to minimize it. Fixing a learning rate η > 0 (to be determined
later), the GD iteration reads

θt+1 = θt − η∇ (cid:98)Rn,κ(θt) = θt −

η
n

n
(cid:88)

i=1

(cid:96)(cid:48)(cid:0)yi(cid:104)xi, θt(cid:105) − κ (cid:13)

(cid:13)θt(cid:13)
(cid:13)2

(cid:1)

(cid:32)

yixi − κ

(cid:33)

.

θt
(cid:13)θt(cid:13)
(cid:13)
(cid:13)2

(46)

The intuition behind our deﬁnition of (cid:98)Rn,κ is the following. Assume (cid:98)θ is a (κ + ε)-margin solution for some
ε > 0, then yi(cid:104)xi, (cid:98)θ(cid:105) − κ > 0, ∀i ∈ [n], and we have

lim
s→+∞

(cid:98)Rn,κ(s(cid:98)θ) = lim

s→+∞

1
n

n
(cid:88)

i=1

(cid:16)
(cid:96)

s(cid:0)yi(cid:104)xi, (cid:98)θ(cid:105) − κ(cid:1)(cid:17)

= 0 .

In words, (cid:98)Rn,κ is minimized at inﬁnity along the direction of (cid:98)θ. Therefore, it would be reasonable to guess
that if (cid:107)θt(cid:107)2 → ∞, then θt/(cid:107)θt(cid:107)2 should converge to a κ-margin solution. We will prove that this is indeed
the case under some additional conditions.

Deﬁnition 5. We say that (cid:96) has a tight exponential tail if there exist positive constants c, a, µ+, µ−, u+ and
u− such that

∀u > u+ : −(cid:96)(cid:48)(u) ≤ c (1 + exp (−µ+u)) e−au,
∀u > u− : −(cid:96)(cid:48)(u) ≥ c (1 − exp (−µ−u)) e−au.

Theorem 5.1. Let (cid:96) : R → R≥0 be positive, diﬀerentiable, monotonically decreasing to zero at +∞. Further
assume (cid:96)(cid:48) to be bounded and β-Lipschitz for some positive constant β. Finally, assume (cid:96) to have a tight
exponential tail.

There exists a constant M > 0, such that for all learning rate η < 2/M , as long as the gradient descent

iterates diverge to inﬁnity ((cid:13)

(cid:13)θt(cid:13)

(cid:13)2 → ∞), then we have

lim inf
t→∞

min
1≤i≤n

yi(cid:104)(cid:98)θ

t

, xi(cid:105) ≥ κ ,

t

(cid:98)θ

:=

θt
(cid:107)θt(cid:107)2

.

5.2 Numerical experiments

In this section, we illustrate the behavior of the linear programming (LP) algorithm, and of gradient descent
(GD) on (cid:98)Rn,κ(θ) via numerical simulations. We generate synthetic data from the pure noise model, namely
xi ∼i.i.d. N(0, Id), and without loss of generality we set yi = 1 for all i ∈ [n]. We run the gradient descent
iteration (46) with (cid:96)(x) = log(1 + e−x), random initialization θ0 ∼ Unif(Sd−1), and step size η = 0.05, and
terminate it as soon as the data achieves a margin κ, i.e.,

t

min
1≤i≤n

(cid:104)(cid:98)θ

, xi(cid:105) ≥ κ ,

t

(cid:98)θ

:=

θt
(cid:107)θt(cid:107)2

.

For the largest experiment, we replace GD by mini-batch stochastic gradient descent (SGD).

In the ﬁrst experiment, we ﬁx κ = −1.5, and d ∈ {50, 100, 200}. We choose various values of n with
δ = n/d ranging from below the linear programming threshold to the phase transition upper bound. For
each pair of (n, d) we run both GD and LP (cf. Eq. (19)) for 1000 independent realizations of the data
(y, X), and report in Fig. 5 the empirical probability of ﬁnding a κ-margin solution.

Figure 5 suggests the following remarks:

17

δlb

δlin

δub

Figure 5: Scatter plots of the empirical probability of ﬁnding a κ-margin interpolator using gradient descent
(GD) and linear programming (LP), as a function of δ. We ﬁx κ = −1.5 and choose d ∈ {50, 100, 200}.
The vertical solid lines represent δlb(κ), δlin(κ) and δub(κ) (from left to right) respectively. Note that for
κ = −1.5, we actually have δlb(κ) < δlin(κ).

1. For both algorithms we see that the probability of ﬁnding a κ-margin interpolator decreases from close
to one to close to zero as δ crosses a threshold. The transition region becomes steeper as d grows
larger. This ﬁnding conﬁrms Theorem 3.3, which predicts a sharp threshold for the LP algorithm. The
location of this threshold is also consistent with the theoretical prediction.

2. Figure 5 also suggests a phase transition for GD under proportional asymptotics. A proof of this

phenomenon is at this moment an open problem.

3. GD signiﬁcantly outperforms LP. Notice that for κ = −1.5, our bounds δlb(κ), δub(κ) are too wide to
separate the linear programming threshold δlin(κ) from the existence threshold δs(κ). However the GD
threshold appears to be strictly larger than δlin(κ), suggesting that such a separation exists.

We note that an analysis of gradient-based algorithms using techniques from physics was initiated in
[ABUZ18].

As for the second experiment, we select κ = −3, and d ∈ {25, 50, 100}. In this case we need to take
δ ≥ 1000 to be close to the satisﬁability phase transition, resulting in n of the order of 105. Computing
the full gradient is very slow in this case, and therefore we replace GD by mini-batch SGD with batch size
= 1000. As before, in Figure 6 we plot the empirical probability of ﬁnding a κ-margin interpolator using
both SGD and LP calculated from 400 independent realizations.

Note that for κ = −3, δlb(κ) > δlin(κ).

In other words, our bounds are precise enough to separate
the satisﬁability and linear programming threshold. However the empirical threshold for SGD seems to be
between δlb(κ) and δub(κ). Therefore we cannot exclude that SGD ﬁnds solutions with probability bounded
away from zero for all δ < δs(κ).

18

δlin

δlb

δub

Figure 6: Scatter plots of the empirical probability of ﬁnding a κ-margin interpolator using mini-batch
stochastic gradient descent (SGD) and linear programming (LP), as a function of δ. We ﬁx κ = −3 and
choose d ∈ {25, 50, 100}. The vertical solid lines represent δlin(κ), δlb(κ) and δub(κ) (from left to right)
respectively.

6 Proof ideas

In the previous sections we stated three main types of results: (i) Lower bounds on the interpolation
threshold δs(κ); (ii) Upper bounds on the interpolation threshold δs(κ); (iii) Characterizations of the linear
programming threshold δlin(κ). We brieﬂy describe the approach we use for each of these proofs. We will
mostly focus this informal overview on the case of pure noise. The linear signal model requires additional
technical work, and we will only mention diﬀerences in the proof when they are substantial.

Lower bounds on δs(κ). A standard approach in the context of random CSPs is to apply the second moment
method. Namely, one constructs a non-negative random variable Zn = Z(X) ≥ 0 such that Z > 0 if and
only if the problem has solutions. By Paley-Zygmund inequality, computing the ﬁrst two moments of Z
yields a lower bound on the probability that there exist solutions:

P(cid:0)ERM0(κ) (cid:54)= ∅(cid:1) ≥

E{Z}2
E{Z 2}

.

(47)

The simplest choice for Z in discrete settings is the number of solutions [AM06], and an obvious generalization
would be the volume of the set ERM0(κ). However, an explicit calculation shows that in this case E{Z 2} is
exponentially larger than E{Z}2 for any δ > 0.

We obtain a non-trivial bound by weighting the constraints yi(cid:104)xi, θ(cid:105) ≥ κ, and deﬁning

(cid:90)

Z :=

Sd−1

n
(cid:89)

i=1

f (yi(cid:104)xi, θ(cid:105)) µ(dθ) ,

(48)

where µ is the uniform probability distribution over Sd−1 and f : R → R is a non-negative measurable
function such that f (t) = 0 for t < κ. Weighted solution counts were used already in the context of discrete
CSPs, e.g. in [ANP05].

19

For the pure noise model, we use f (t) = e−ct1t≥κ and show that c = c(κ) can be chosen to ensure

E{Z 2} ≤ O(1) · E{Z}2.

For labels correlated with a linear signal, the application of the second moment becomes more challeng-
ing, because the quantity Z deﬁned above has larger ﬂuctuations, and its moment can be dominated by
exponentially rare realizations of the data {(yi, xi)}i≤n. In order to reduce this variability, we deﬁne Z via a
restricted integral over vectors θ such that (cid:104)θ, θ∗(cid:105) = ρ. Further, we condition over the values ui = yi(cid:104)xi, θ∗(cid:105).
Even after this conditioning, a direct application of the second moment method fails, because of a small
fraction of ‘bad’ samples (yi, xi). Instead, we apply the second moment method to verify that to show that
the margin condition can be satisﬁed on the ‘good’ samples, and deal with the bad samples separately.

Upper bounds on δs(κ). The simplest approach to obtain upper bounds in discrete CSPs is to let Z0 be
the number of solutions and show that (when the number of constraints per variables passes a threshold),
E{Z0} → 0. This implies P(Z0 > 0) → 0 by Markov inequality.

In the case of continuous decision variables, this method cannot be applied, as the volume can be arbi-

trarily small and yet nonzero. We consider instead the following minimax problem

ξn,κ = min
(cid:107)θ(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

1
√
d

λ(cid:62) (κ1 − Xθ) .

It is easy to see that

P(ERM0(κ) (cid:54)= ∅) = P (ξn,κ ≤ 0) ,

(49)

and it is therefore suﬃcient to upper bound the latter probability. Gordon’s Gaussian comparison inequality
[Gor85] implies

ξn,κ +

1
√
d

z (cid:23) ξlin

n,κ ,

ξlin
n,κ := min
(cid:107)θ(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

(cid:110) 1
√
d

λ(cid:62) (κ1 − (cid:107)θ(cid:107)2 h) +

(cid:107)λ(cid:107)2 g(cid:62)θ

(cid:111)

.

1
√
d

(50)

(51)

Here (cid:23) denotes stochastic domination3 and z ∼ N(0, 1), h ∼ N(0, In), g ∼ N(0, Id) are mutually independent
and independent of X. At this point, the standard approach is to estimate Eξlin
n,κ, and upper bound δs(κ)
by the inﬁmum of δ such that Eξlin
n,κ is positive bounded away from zero. In the present setting, this yields
the replica symmetric upper bound of [Sto13a], which is substantially suboptimal.

We follow a diﬀerent approach, ﬁrst appeared in [Sto13b], which is based on estimating the moment gener-

ating function of ξn,κ. Namely, we use Eq. (50) to deduce that, for any c ≥ 0, E{exp(−cξn,κ)}E{exp(−cz/
E{exp(−cξlin

n,κ)}, whence

√

d)} ≤

P(ERM0(κ) (cid:54)= ∅) ≤

E{exp(−cξlin
n,κ)}
√
E{exp(−cz/
d)}

.

The proof is obtained by estimating the right-hand side and optimizing this bound over c ≥ 0. Remarkably,
the exponential weighting method yields an upper bound that is signiﬁcantly tighter than more standard
applications of Gordon’s inequality.

Characterization of δlin(κ). The linear programming algorithm of Section 3.2 succeeds if and only if the
maximum is achieved at (cid:107)θ(cid:107)2 ≥ 1 (or (cid:107)θ(cid:107)2 = 1 in case the constraint (cid:107)θ(cid:107)2 ≤ 1 is imposed). This can be
determined by comparing the value of problem (19) to the value of the same problem in which the constraint
(cid:107)θ(cid:107)2 ≤ 1 is replaced by |θ(cid:107)2 ≤ 1 + ε. Since the underlying problem is convex, these values can be tightly
characterized by an application of Gordon’s inequality analogous to [TAH18, MRSY19]. The case of labels
correlated to a linear signal is analogous.

3Namely, X (cid:23) Y if E(ψ(X)) ≥ E(ψ(Y )) for any non-decreasing function ψ.

20

7 Conclusion

Given data (X, y) = {(xi, yi)}i≤n, xi ∈ Rd, yi ∈ {+1, −1} that are not linearly separable, the negative
perceptron attempts to ﬁnd a linear classiﬁer that minimizes the maximum distance of any misclassiﬁed
sample from the decision boundary. This leads to a non-convex empirical risk minimization problem, whose
global minimizers are κ-margin classiﬁers, with κ < 0.

We studied two simple models for the data (X, y): purely random labels, and labels correlated with a
linear signal. We focused on the high-dimensional regime n, d → ∞, n/d → δ. For each of these models,
we proved bounds on the maximum margin κs(δ) or, equivalently, on the maximum number of samples per
dimension δs(κ) such that a κ-margin solution exists for δ < δs(κ).

Our main result is that, despite non-convexity, very simple algorithms can ﬁnd κ margin solutions in a
large portion of the region in which such solutions exist. In particular, a single step of Frank-Wolfe succeeds
with high probability for n/d ≤ δlin(κ)(1 − ε), for a threshold δlin(κ) that we characterize. In other words
the non-convex learning problem becomes easy at suﬃcient overparametrization.

It is worth emphasizing that this phenomenon is very diﬀerent from the one arising in certain high-
dimensional statistics problems, whereby the problem becomes easy in a high signal-to-noise ratio regime.
Example of this have been studied in matrix completion [KMO10, GLM16], phase retrieval [CLS15, CC15,
SQW18], mixture of Gaussians [XHM16, DTZ17], and so on. In those setting, additional samples make the
problem even easier, and the basic mechanism is uniform concentration around a bowl-shaped population
risk that is uniquely minimized at the true signal [MBM18]. In contrast, in the present example, the problem
becomes easy because it is underconstrained and larger sample size can translate into a harder problem.

Our work leads to a number of interesting open questions.

In particular, it would be interesting to
obtain a more precise characterization of the existence phase transition δs(κ) and prove that it is a sharp
threshold. Also, it would be interesting to analyze gradient descent algorithms, which empirically seem to
have a superior behavior to the linear programming approach.

Finally, we used the negative perceptron mainly as a toy model for more complex non-convex empirical
risk minimization problems. It would be interesting to understand whether negative margin methods can be
useful from a statistical viewpoint.

Acknowledgements

This work was supported by NSF through award DMS-2031883 and from the Simons Foundation through
Award 814639 for the Collaboration on the Theoretical Foundations of Deep Learning. It was also supported
by the NSF grant CCF-2006489 and the ONR grant N00014-18-1-2729.

References

[ABUZ18] Elisabeth Agoritsas, Giulio Biroli, Pierfrancesco Urbani, and Francesco Zamponi. Out-of-
equilibrium dynamical mean-ﬁeld equations for the perceptron model. Journal of Physics A:
Mathematical and Theoretical, 51(8):085002, 2018.

[ACO08] Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase transitions.

In
2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 793–802. IEEE,
2008.

[ALS21]

Emmanuel Abbe, Shuangping Li, and Allan Sly. Proof of the contiguity conjecture and lognormal
limit for the symmetric perceptron. arXiv:2102.13069, 2021.

[AM02]

Dimitris Achlioptas and Cristopher Moore. The asymptotic order of the random k-sat threshold.
In The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.,
pages 779–788. IEEE, 2002.

[AM06]

Dimitris Achlioptas and Cristopher Moore. Random k-sat: Two moments suﬃce to cross a sharp
threshold. SIAM Journal on Computing, 36(3):740–762, 2006.

21

[AMS21] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Optimization of mean-ﬁeld spin glasses.

Annals of Probability, 2021. to appear.

[ANP05] Dimitris Achlioptas, Assaf Naor, and Yuval Peres. Rigorous location of phase transitions in hard

optimization problems. Nature, 435(7043):759–764, 2005.

[AP03]

Dimitris Achlioptas and Yuval Peres. The threshold for random k-sat is 2k (ln 2-o (k)).
In
Proceedings of the thirty-ﬁfth annual ACM symposium on Theory of computing, pages 223–231,
2003.

[APZ19]

Benjamin Aubin, Will Perkins, and Lenka Zdeborov´a. Storage capacity in symmetric binary
perceptrons. Journal of Physics A: Mathematical and Theoretical, 52(29):294003, 2019.

[AS20]

[Bau88]

Ahmed El Alaoui and Mark Sellke. Algorithmic pure states for the negative spherical perceptron.
arXiv:2010.15811, 2020.

Eric B Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193–215,
1988.

[BELM20] S´ebastien Bubeck, Ronen Eldan, Yin Tat Lee, and Dan Mikulincer. Network size and weights

size for memorization with two-layers neural networks. arXiv:2006.02855, 2020.

[BH21]

[Bir42]

Guy Bresler and Brice Huang. The algorithmic phase transition of random k-sat for low degree
polynomials. arXiv:2106.02129, 2021.

Zygmunt Wilhelm Birnbaum. An inequality for mill’s ratio. The Annals of Mathematical Statis-
tics, 13(2):245–246, 1942.

[BLLT20] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in

linear regression. Proceedings of the National Academy of Sciences, 2020.

[BMR21] Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical view-

point. Acta Numerica, 2021.

[BV21]

[CC15]

Pierre Baldi and Roman Vershynin. A theory of capacity and sparse neural encoding. Neural
Networks, 2021.

Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly
as easy as solving linear systems. In Advances in Neural Information Processing Systems, pages
739–747, 2015.

[CLS15]

Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow:
Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.

[COB19]

L´ena¨ıc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in diﬀerentiable program-
ming.
In Proceedings of the 33rd International Conference on Neural Information Processing
Systems, pages 2937–2947, 2019.

[COHH17] Amin Coja-Oghlan, Amir Haqshenas, and Samuel Hetterich. Walksat stalls well below satisﬁa-

bility. SIAM Journal on Discrete Mathematics, 31(2):1160–1173, 2017.

[Cov65]

[CS20]

[Dan19]

Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
IEEE transactions on electronic computers, (3):326–334,
applications in pattern recognition.
1965.

Emmanuel J Cand`es and Pragya Sur. The phase transition for the existence of the maximum
likelihood estimate in high-dimensional logistic regression. The Annals of Statistics, 48(1):27–42,
2020.

Amit Daniely.
parameterization. arXiv:1911.09873, 2019.

Neural networks

learning and memorization with (almost) no over-

22

[Dan20]

Amit Daniely. Memorizing gaussians with no over-parameterizaion via gradient decent on neural
networks. arXiv:2003.12895, 2020.

[DB81]

[DS19]

Nicolaas Govert De Bruijn. Asymptotic methods in analysis, volume 4. Courier Corporation,
1981.

Jian Ding and Nike Sun. Capacity lower bound for the ising perceptron. In Proceedings of the
51st Annual ACM SIGACT Symposium on Theory of Computing, pages 816–827, 2019.

[DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suﬃce for

mixtures of two gaussians. In Conference on Learning Theory, pages 704–710. PMLR, 2017.

[DZ10]

Amir Dembo and Ofer Zeitouni. Large Deviations Techniques and Applications. Springer Berlin
Heidelberg, 2010.

[FHU19]

Silvio Franz, Sungmin Hwang, and Pierfrancesco Urbani. Jamming in multilayer supervised
learning models. Physical review letters, 123(16):160602, 2019.

[FP16]

Silvio Franz and Giorgio Parisi. The simplest model of jamming. Journal of Physics A: Mathe-
matical and Theoretical, 49(14):145001, 2016.

[FPS+17] Silvio Franz, Giorgio Parisi, Maksim Sevelev, Pierfrancesco Urbani, and Francesco Zamponi. Uni-
versality of the sat-unsat (jamming) threshold in non-convex continuous constraint satisfaction
problems. 2017.

[FSU19]

Silvio Franz, Antonio Sclocchi, and Pierfrancesco Urbani. Critical jammed phase of the linear
perceptron. Physical review letters, 123(11):115702, 2019.

[FW+56] Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research

logistics quarterly, 3(1-2):95–110, 1956.

[Gar88]

[GK92]

Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A:
Mathematical and general, 21(1):257, 1988.

Peter Gritzmann and Victor Klee. Inner and outerj-radii of convex bodies in ﬁnite-dimensional
normed spaces. Discrete & Computational Geometry, 7(3):255–280, 1992.

[GLM16] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum.
In Proceedings of the 30th International Conference on Neural Information Processing Systems,
pages 2981–2989, 2016.

[Gor85]

[GU14]

[Gue03]

Yehoram Gordon. Some inequalities for gaussian processes and applications. Israel Journal of
Mathematics, 50(4):265–289, 1985.

Armengol Gasull and Frederic Utzet. Approximating Mills ratio. Journal of Mathematical Anal-
ysis and Applications, 420(2):1832–1853, 2014.

Francesco Guerra. Broken replica symmetry bounds in the mean ﬁeld spin glass model. Com-
munications in mathematical physics, 233(1):1–12, 2003.

[HMRT21] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-

dimensional ridgeless least squares interpolation. Annals of Statistics, 2021. to appear.

[JGH18] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: convergence and
generalization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pages 8580–8589, 2018.

[JT18]

Ziwei Ji and Matus Telgarsky.
arXiv:1803.07300, 2018.

Risk and parameter convergence of

logistic regression.

23

[KMO10] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few

entries. IEEE transactions on information theory, 56(6):2980–2998, 2010.

[Kow93]

Adam Kowalczyk. Counting function theorem for multi-layer networks. Advances in neural
information processing systems, 6:375–382, 1993.

[LR20]

Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can
generalize. Annals of Statistics, 48(3):1329–1347, 2020.

[MBM18] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses.

The Annals of Statistics, 46(6A):2747–2774, 2018.

[MM21]

L´eo Miolane and Andrea Montanari. The distribution of the lasso: Uniform control over sparse
balls and adaptive parameter tuning. The Annals of Statistics, 49(4):2313–2335, 2021.

[Mon21]

Andrea Montanari. Optimization of the sherrington–kirkpatrick hamiltonian. SIAM Journal on
Computing, (0):FOCS19–1, 2021.

[MRSY19] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of
max-margin linear classiﬁers: High-dimensional asymptotics in the overparametrized regime.
arXiv:1911.01544, 2019.

[MZ20]

Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks:
Memorization and generalization under lazy training. arXiv:2007.12826, 2020.

[NM94] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing.

Handbook of econometrics, 4:2111–2245, 1994.

[NTS15]

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:
On the role of implicit regularization in deep learning. In ICLR (Workshop), 2015.

[Pan15]

[Sak92]

Dmitry Panchenko. The free energy in a multi-species sherrington–kirkpatrick model. The Annals
of Probability, 43(6):3494–3513, 2015.

Akito Sakurai. nh-1 networks store no less n* h+ 1 examples, but sometimes no more.
In
[Proceedings 1992] IJCNN International Joint Conference on Neural Networks, volume 3, pages
936–941. IEEE, 1992.

[SC19]

Pragya Sur and Emmanuel J Cand`es. A modern maximum-likelihood theory for high-dimensional
logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516–14525, 2019.

[SHN+18] Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
19(1):2822–2878, 2018.

[Sio58]

Maurice Sion. On general minimax theorems. Paciﬁc Journal of mathematics, 8(1):171–176,
1958.

[SQW18]

Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of
Computational Mathematics, 18(5):1131–1198, 2018.

[ST03]

Mariya Shcherbina and Brunello Tirozzi. Rigorous solution of the Gardner problem. Communi-
cations in mathematical physics, 234(3):383–422, 2003.

[Sto13a] Mihailo Stojnic. Another look at the gardner problem. arXiv:1306.3979, 2013.

[Sto13b] Mihailo Stojnic. Negative spherical perceptron. arXiv:1306.3980, 2013.

[TAH18] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory, 64(8):5592–5628,
2018.

24

[Tal10]

Michel Talagrand. Mean ﬁeld models for spin glasses: Volume I: Basic examples, volume 54.
Springer Science & Business Media, 2010.

[TOH15] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A
precise analysis of the estimation error. Proceedings of Machine Learning Research, 40:1683–1709,
2015.

[Ver18]

Roman Vershynin. High-dimensional probability: An introduction with applications in data sci-
ence, volume 47. Cambridge University Press, 2018.

[XHM16]

Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures
of two gaussians. Advances in Neural Information Processing Systems, 29, 2016.

[ZBH+21] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021.

25

A κ-margin classiﬁers in the pure noise model:

Proofs of Theorems 3.1 and 3.2

A.1 Phase transition lower bound: Proof of Theorem 3.1

We ﬁrst state the following crucial lemma, whose proof will be deferred to Appendix A.3.

Lemma 1.

(a) For any κ < 0, there exists a unique c∗ = c∗(κ) > 0 satisfying c (1 − Φ(κ + c)) = φ(κ + c). Moreover,

c∗ = ˘Oκ(exp(−κ2/2)) as κ → −∞.

(b) Let κ and c∗ be as described in (a), set f (x) = exp(−c∗(κ)x)1{x ≥ κ}, and deﬁne

F (q) = − log Eq [f (G1)f (G2)] ,

I(q) = −

log (cid:0)1 − q2(cid:1)

1
2

(52)

as per Deﬁnition 1. We further deﬁne

e(q) = Eq[f (G1)f (G2)] = exp (−F (q)) ,

then it follows that e(cid:48)(0) = F (cid:48)(0) = 0, e(0) = 1 + ˘oκ(1), and e(1) − e(0) = (1 + ˘oκ(1))Φ(κ).

(c) For q ∈ [−1, 1], we have

e(cid:48)(q) =

=

(cid:18)

1
2π(cid:112)1 − q2
(cid:114) 2
π
1 + ˘oκ(1)
2π(cid:112)1 − q2

−

1 − Φ

(cid:18)(cid:114) 1 − q
1 + q
κ2
1 + q

−

(cid:18)

exp

(cid:18)

exp

−2κc∗ −

(cid:19)

κ2
1 + q

+ c2

∗ exp (cid:0)c2

∗(1 + q)(cid:1) Pq (min (G1, G2) ≥ κ + c∗(1 + q))

(κ + c∗(1 + q))

c∗ exp

(cid:19)(cid:19)

(cid:18)

c2
∗(1 + q) −

1
2

(κ + c∗(1 + q))2

(cid:19)

(53)

(cid:19)

+ ˘Oκ

(cid:0)exp(−κ2)(cid:1) .

Here, Pq denotes the probability distribution N

constant C ∈ (0, +∞) such that

(cid:18)

0,

(cid:20)1
q

(cid:21)(cid:19)

q
1

. Further, for any q ∈ [0, 1) there exists a

sup
u∈[−q,q]

|e(cid:48)(cid:48)(u)| ≤ |κ|C exp

(cid:18)

−

κ2
1 + q

(cid:19)

.

(54)

We will prove Theorem 3.1 via the second moment method. Let µ denote the uniform probability measure
on Sd−1, and f be as deﬁned in Lemma 1 (b). Then we know that f is supported on the interval [κ, +∞),
i.e., f (x) > 0 if and only if x ≥ κ. We deﬁne the random variable

(cid:90)

Z =

Sd−1

n
(cid:89)

i=1

f ((cid:104)θ, xi(cid:105)) µ(dθ),

then we know that Z > 0 if and only if ∃θ ∈ Sd−1, ∀1 ≤ i ≤ n, (cid:104)xi, θ(cid:105) ≥ κ. Therefore,

P (cid:0)∃θ ∈ Sd−1, ∀1 ≤ i ≤ n, (cid:104)xi, θ(cid:105) ≥ κ(cid:1) = P (Z > 0) .

Now, according to the Paley-Zygmund inequality,

P (Z > 0) = E [1 {Z > 0}] ≥

E [Z1 {Z > 0}]2
E [Z 2]

=

E [Z]2
E [Z 2]

,

(55)

(56)

where the intermediate bound follows from the Cauchy-Schwarz inequality. Our aim is to show that δ < δlb(κ)
implies E (cid:2)Z 2(cid:3) = Od(E [Z]2), then combining Eq. (55) and Eq. (56) yields the ﬁrst part of Theorem 3.1.

26

To this end we calculate and compare the ﬁrst and second moments of Z. Note that for any θ ∈ Sd−1,

(cid:104)θ, xi(cid:105) ∼i.i.d. N(0, 1), hence

(cid:90)

E[Z] =

Sd−1

(cid:34) n
(cid:89)

E

i=1

(cid:35)

f ((cid:104)θ, xi(cid:105))

µ(dθ) = E [f (G)]n = exp (n log E [f (G)]) , G ∼ N(0, 1).

(57)

To calculate E[Z 2], we ﬁrst write Z 2 as a double integral:

(cid:90)

Z 2 =

Sd−1×Sd−1

n
(cid:89)

i=1

f ((cid:104)θ1, xi(cid:105)) f ((cid:104)θ2, xi(cid:105)) µ(dθ1)µ (dθ2) .

Next, denote q = (cid:104)θ1, θ2(cid:105), we observe the following two useful facts:

1. For θ1, θ2 ∼i.i.d. Unif(Sd−1), the probability density function of q is given as

p(q) =

√

Γ (d/2)
πΓ ((d − 1)/2)

(cid:0)1 − q2(cid:1)(d−3)/2

1[−1,1](q).

2. For 1 ≤ i ≤ n,

((cid:104)θ1, xi(cid:105) , (cid:104)θ2, xi(cid:105))(cid:62) ∼i.i.d. N

(cid:18)

0,

(cid:20)1
q

(cid:21)(cid:19)

q
1

.

Therefore, it follows that

(cid:90)

E[Z 2] =

Sd−1×Sd−1

(cid:34) n
(cid:89)

E

i=1

f ((cid:104)θ1, xi(cid:105)) f ((cid:104)θ2, xi(cid:105))

µ(dθ1)µ (dθ2)

(cid:35)

=

(cid:90) 1

−1

Eq [f (G1)f (G2)]n p(q)dq = (1 + od(1))

(cid:90) 1

(cid:114)

−1

d
2π

Eq [f (G1)f (G2)]n (cid:0)1 − q2(cid:1)(d−3)/2

dq,

where Eq denotes the expectation taken under (G1, G2)(cid:62) ∼ N
Γ(d/2)/Γ((d − 1)/2) = (1 + od(1))(cid:112)d/2, which can be easily derived from Stirling’s formula:

0,

, and the last equality is due to

(cid:18)

(cid:20)1
q

(cid:21)(cid:19)

q
1

Γ(t + 1) = (1 + ot(1))

√

2πt

(cid:19)t

(cid:18) t
e

,

t → +∞.

According to Eq. (52), we can then write

E[Z 2] = (1 + od(1))

= (1 + od(1))

(cid:90) 1

(cid:114)

−1
(cid:90) 1

(cid:114)

−1

d
2π

d
2π

exp (−nF (q) − (d − 3)I(q)) dq

(cid:18)

(cid:18)

exp

−n

F (q) +

(cid:19)(cid:19)

I(q)

dq.

d − 3
n

Since limn→+∞ n/(d − 3) = δ < δlb(κ), for any δ(cid:48) such that δ < δ(cid:48) < δlb(κ), we have

E[Z 2] ≤ (1 + od(1))

(cid:90) 1

(cid:114)

−1

d
2π

(cid:18)

(cid:18)

exp

−n

F (q) +

(cid:19)(cid:19)

dq.

I(q)
δ(cid:48)

By Deﬁnition 1, F (q) + I(q)/δ(cid:48) is uniquely minimized at q = 0, and F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ(cid:48) > 0. Utilizing

Laplace’s method (Chap 4.2 of [DB81]), we obtain that

E[Z 2] ≤ (1 + od(1))

(cid:90) 1

(cid:114)

−1

d
2π

(cid:18)

(cid:18)

exp

−n

F (q) +

(cid:19)(cid:19)

dq

I(q)
δ(cid:48)

27

= (1 + od(1))

(cid:115)

2π
n (F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ(cid:48))

(cid:114)

d
2π

exp (−n (F (0) + I(0)/δ(cid:48)))

(cid:115)

(i)
= (1 + od(1))

1
δ (F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ(cid:48))

exp (−nF (0))

(cid:115)

(ii)
= (1 + od(1))

1
δ (F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ(cid:48))

exp (2n log E[f (G)]) , G ∼ N(0, 1),

(58)

where (i) is due to the fact that I(0) = 0, and (ii) is due to the deﬁnition of F (q). Now combining Eq. (57)
and Eq. (58) gives

lim inf
n→+∞

P(Z > 0) ≥ lim inf
n→+∞

(cid:115)

E[Z]2
E[Z 2]

≥

1
δ (F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ(cid:48))

> 0,

thus proving Eq. (14). In order to conclude the second part of Theorem 3.1, it suﬃces to show that if |κ| is
suﬃciently large, and

√

δ < (1 − ε)

2π|κ| log |κ| exp

(cid:19)

,

(cid:18) κ2
2

then F (q) + I(q)/δ is uniquely minimized at q = 0, and F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0. To verify these two conditions,
we proceed with the following three steps.
Step 1. Simplify the ﬁrst condition
Recall that F (q) = − log e(q), therefore the condition “F (q) + I(q)/δ is uniquely minimized at q = 0” is
equivalent to

∀q ∈ [−1, 1]\{0}, F (q) +

> F (0) +

I(0)
δ

= F (0)

⇐⇒ ∀q ∈ [−1, 1]\{0},

δ log

(i)
⇐= ∀q ∈ [−1, 1]\{0},

δ

< I(q)

< I(q)

I(q)
δ
(cid:18) e(q)
e(0)

(cid:19)

e(q) − e(0)
e(0)
(cid:90) q

δ
e(0)

0

⇐⇒ ∀q ∈ [−1, 1]\{0},

e(cid:48)(s)ds < I(q) = −

log (cid:0)1 − q2(cid:1) ,

1
2

(59)

where in (i) we use the inequality log x ≤ x − 1.
Step 2. Prove the second condition
Note that Eq. (54) implies e(cid:48)(cid:48)(0) ≤ |κ|C exp(−κ2), hence we have

|F (cid:48)(cid:48)(0)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

e(cid:48)(0)2 − e(cid:48)(cid:48)(0)e(0)
e(0)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(i)
=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

e(cid:48)(cid:48)(0)
e(0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ii)

≤ |κ|C exp (cid:0)−κ2(cid:1) ,

where (i) is due to e(cid:48)(0) = 0, (ii) is due to e(0) = 1+˘oκ(1) (Lemma 1 (b)). Now since 1/δ ≥ |κ|−C exp(−κ2/2),
I (cid:48)(cid:48)(0) = 1, it immediately follows that ∃M1 > 0, ∀κ < −M1, F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0.
Step 3. Prove the ﬁrst condition
To show Eq. (59), consider the following three cases:

(1) q ∈ [−1/2, 1/2]\{0}. Following Eq. (54), and using the facts e(0) = 1+ ˘oκ(1), e(cid:48)(0) = 0 from Lemma 1

(b), we obtain the estimate below:

e(cid:48)(s)ds = (1 + ˘oκ(1))δ

(cid:90) q

e(cid:48)(s)ds = (1 + ˘oκ(1))δ

(cid:90) q

(cid:90) s

ds

0

0

e(cid:48)(cid:48)(u)du

δ
e(0)

(cid:90) q

0

≤δ|κ|C exp

(cid:18)

−

κ2
1 + q

(cid:19) (cid:90) q

0

(cid:90) s

ds

0

du

√

≤(1 − ε)

2π|κ|C+1 log |κ| exp

0
(cid:18) κ2
2

(cid:19)

(cid:18)

exp

−

2κ2
3

(cid:19) q2
2

28

≤|κ|C exp

(cid:18)

−

κ2
6

(cid:19) q2
2

= ˘oκ(1)q2.

Since I(q) ≥ q2/2 for all q ∈ [−1, −1], we conclude that ∃M2 > 0, as if κ < −M2,

δ
e(0)

(cid:90) q

0

e(cid:48)(s)ds ≤ ˘oκ(1)q2 < I(q),

(cid:20)

q ∈

−

(cid:21)

1
2

,

1
2

(cid:15){0}.

(2) q ∈ [−1, −1/2]. In this case, with the aid of Eq. (53) one gets that

δ
e(0)

(cid:90) q

0

e(cid:48)(s)ds =(1 + ˘oκ(1))δ

(i)
≤Cδ

(cid:32)(cid:90) |q|

√

0

(cid:90) q

0

(cid:18) 1 + ˘oκ(1)
√
1 − s2
(cid:33)

2π

ds
1 − s2
(cid:18)

+ |q|

(cid:19)

κ2
2

≤C|κ| log |κ| exp

−

= ˘oκ(1),

(cid:18)

exp

−

(cid:19)

κ2
1 + s

+ ˘Oκ

(cid:0)exp(−κ2)(cid:1)

(cid:19)

ds

exp (cid:0)−κ2(cid:1) (ii)

≤ Cδ exp (cid:0)−κ2(cid:1)

where (i) holds since exp(−κ2/(1 + s)) ≤ exp(−κ2) when s ≤ 0, and (ii) follows from the fact

(cid:90) |q|

0

√

ds
1 − s2

≤

(cid:90) 1

0

√

ds
1 − s2

=

π
2

.

Therefore, we can ﬁnd some M3 > 0 such that when κ < −M3,

δ
e(0)

(cid:90) q

0

e(cid:48)(s)ds ≤ ˘oκ(1) < I

(cid:19)

(cid:18)

−

1
2

(cid:20)

≤ I(q),

q ∈

−1, −

(cid:21)

.

1
2

(3) q ∈ [1/2, 1]. On the one hand, we observe that Eq. (53) implies that e(cid:48)(q) ≥ 0 when |κ| is large

enough, and also note that e(1) − e(0) = (1 + ˘oκ(1))Φ(κ) (Lemma 1 (b)), thus leading to

δ
e(0)

(cid:90) q

0

e(cid:48)(s)ds ≤ δ

e(1) − e(0)
e(0)

≤(1 + ˘oκ(1))(1 − ε)

√

2π|κ| log |κ| exp

= (1 + ˘oκ(1))δΦ(κ)

(cid:19)

(cid:18) κ2
2

Φ(κ)

(i)
≤(1 + ˘oκ(1))(1 − ε) log |κ| <

(cid:16)

1 −

(cid:17)

ε
2

log |κ|

≤ −

1
2

log (cid:0)1 − q2(cid:1) = I(q),

if |κ| is suﬃciently large and 1 − q ≤ |κ|−2+ε/2. Here, (i) is due to the inequality Φ(κ) ≤ |κ|φ(κ). On the
other hand, if 1 − q ≥ |κ|−2+ε/2, then for any s ≤ q, we have

−

κ2
1 + s

≤ −

κ2
1 + q

= −

κ2
2

−

(1 − q)κ2
2(1 + q)

≤ −

κ2
2

−

|κ|ε
8

,

which further implies (use Eq. (53))

e(cid:48)(s) ≤

1 + ˘oκ(1)
1 − s2
2π

√

(cid:18)

exp

−

(cid:19)

κ2
2

−

|κ|ε
8

+ ˘Oκ

(cid:0)exp(−κ2)(cid:1) .

Hence it follows that

δ
e(0)

(cid:90) q

0

e(cid:48)(s)ds ≤(1 + ˘oκ(1))δ

(cid:18) 1 + ˘oκ(1)
2π
√

(cid:18)

exp

−

κ2
2

(cid:19) (cid:90) q

√

−

|κ|ε
8
(cid:18) κ2
2

0
(cid:19) (cid:18) 1
4

exp

ds
1 − s2
(cid:18)
κ2
2

−

+ ˘Oκ

(cid:0)exp(−κ2)(cid:1) q

(cid:19)

(cid:19)

−

|κ|ε
8

+ ˘Oκ

(cid:0)exp(−κ2)(cid:1)

(cid:19)

≤(1 + ˘oκ(1))(1 − ε)

2π|κ| log |κ| exp

29

≤|κ|C exp

(cid:19)

(cid:18)

−

|κ|ε
8

= ˘oκ(1) < −

1
2

log (cid:0)1 − q2(cid:1) = I(q),

q ∈

(cid:20) 1
2

, 1 −

|κ|−2+ε
2

(cid:21)

.

To conclude, there exists some M4 > 0, such that for all κ < −M4,
(cid:20) 1
2

e(cid:48)(s)ds < I(q),

δ
e(0)

q ∈

(cid:90) q

0

(cid:21)

, 1

.

Now we choose M = max1≤j≤4 Mj and κ = −M . Then for κ < κ, and

√

δ < (1 − ε)

2π|κ| log |κ| exp

(cid:19)

,

(cid:18) κ2
2

Eq. (59) will be valid and F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0. The conclusion of Theorem 3.1 follows naturally.

A.2 Phase transition upper bound: Proof of Theorem 3.2

To begin with, let us deﬁne the following random variable:

ξn,κ = min
(cid:107)θ(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

1
√
d

λ(cid:62) (κ1 − Xθ) .

Note that

(cid:18)

P (ξn,κ ≤ 0) = P

∃θ ∈ Sd−1, max

λ(cid:62) (κ1 − Xθ) ≤ 0

(cid:107)λ(cid:107)2=1,λ≥0
=P (cid:0)∃θ ∈ Sd−1, κ1 − Xθ ≤ 0(cid:1) = P (cid:0)∃θ ∈ Sd−1, ∀1 ≤ i ≤ n, (cid:104)xi, θ(cid:105) ≥ κ(cid:1) ,
hence it suﬃces to prove P (ξn,κ ≤ 0) → 0 under the assumption of Theorem 3.2. To this end we will use a
modiﬁed version of Gordon’s comparison theorem, namely Theorem 2.3 of [Gor85]. We state its adaption to
our case below for readers’ convenience:

1
√
d

(cid:19)

Theorem A.1 (Theorem 2.3 of [Gor85]). Assume ψ : R → R is an increasing function, then we have

E

≥E

(cid:20)

min
(cid:107)θ(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

ψ

(cid:20)

min
(cid:107)θ(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

ψ

(cid:18) 1
√
d
(cid:18) 1
√
d

λ(cid:62) (κ1 − Xθ) +

(cid:19)(cid:21)

(cid:107)λ(cid:107)2 (cid:107)θ(cid:107)2 z

1
√
d

λ(cid:62) (κ1 − (cid:107)θ(cid:107)2 h) +

(cid:107)λ(cid:107)2 g(cid:62)θ

(cid:19)(cid:21)

,

1
√
d

(60)

where g ∼ N(0, Id), h ∼ N(0, In) and z ∼ N(0, 1) are mutually independent, and further independent of the
standard Gaussian matrix X.

Now for some c > 0 which may depend on κ (to be determined later), if we choose ψ(x) = − exp(−cnx),

then Eq. (60) becomes

− E [exp (−cnξn,κ)] E

(cid:20)

(cid:18)

exp

−

(cid:19)(cid:21)

cnz
√
d

(cid:19)(cid:19)(cid:21)

= − E

(cid:20)

(cid:18)

exp

−cn

≥ − E

(cid:20)

(cid:18)

exp

−cn

(cid:18)

(cid:18)

ξn,κ +

min
(cid:107)θ(cid:107)2=1

z
√
d
1
√
d

= − E

(cid:20)

(cid:18)

exp

−

cn
√
d

min
(cid:107)θ(cid:107)2=1

(cid:19)(cid:21)

E

g(cid:62)θ

(cid:20)

g(cid:62)θ + max

(cid:19)(cid:19)(cid:21)

λ(cid:62) (κ1 − h)

1
√
d

(cid:19)(cid:21)

λ(cid:62) (κ1 − h)

,

(61)

max
(cid:107)λ(cid:107)2=1,λ≥0

(cid:107)λ(cid:107)2=1,λ≥0
(cid:18)
cn
√
d

exp

−

where the ﬁrst and last equality are due to the independence between g, h, z, X. Since

min
(cid:107)θ(cid:107)2=1

g(cid:62)θ = − (cid:107)g(cid:107)2 ,

max
(cid:107)λ(cid:107)2=1,λ≥0

λ(cid:62) (κ1 − h) =

30

(cid:40)(cid:13)

(cid:13)
(cid:13)2

(cid:13)(κ1 − h)+
max
1≤i≤n

, min1≤i≤n hi ≤ κ
(κ − hi) , min1≤i≤n hi > κ

,

it follows that

and

(cid:20)

(cid:18)

exp

−

E

(cid:20)

(cid:18)

exp

−

=E

(cid:20)

(cid:18)

exp

≤ E

(cid:124)

−

cn
√
d
cn
√
d
cn
√
d

(cid:20)

(cid:18)

exp

−

E

cn
√
d

min
(cid:107)θ(cid:107)2=1

(cid:19)(cid:21)

(cid:20)

exp

= E

g(cid:62)θ

(cid:18) cn
√
d

(cid:19)(cid:21)

,

(cid:107)g(cid:107)2

(cid:19)(cid:21)

λ(cid:62) (κ1 − h)

max
(cid:107)λ(cid:107)2=1,λ≥0

(cid:13)
(cid:13)(κ1 − h)+

(cid:13)
(cid:13)2

(cid:13)
(cid:13)2

(cid:13)
(cid:13)(κ1 − h)+
(cid:123)(cid:122)
I

(cid:19)

(cid:26)

1

(cid:19)(cid:21)

min
1≤i≤n
(cid:20)

(cid:18)

+ E

exp

−

(cid:125)

(cid:124)

cn
√
d

(cid:27)(cid:21)

hi ≤ κ

(cid:20)

(cid:18)

exp

−

+ E

cn
√
d

(cid:19)

max
1≤i≤n
(cid:26)

(cid:19)

(cid:26)

(cid:27)(cid:21)

(κ − hi)

1

min
1≤i≤n

hi > κ

(cid:27)(cid:21)

max
1≤i≤n

(κ − hi)

1

min
1≤i≤n

hi > κ

.

(62)

(cid:123)(cid:122)
II

(cid:125)

According to Markov’s inequality, we obtain that P(ξn,κ ≤ 0) ≤ E[exp(−cnξn,κ)]. Combining this estimate
with Eq. (60) and Eq. (61) yields

log P (ξn,κ ≤ 0) ≤
(cid:20)

(cid:18)

log E [exp (−cnξn,κ)]
(cid:18)
(cid:19)(cid:21)

(cid:20)

1
n

≤ −

+

≤ −

1
n

1
n
c2n
2d

+

1
n

1
n
cnz
√
d
cn
√
max
d
(cid:107)λ(cid:107)2=1,λ≥0
(cid:18) cn
√
d

(cid:107)g(cid:107)2

(cid:19)(cid:21)

log E

log E

exp

−

(cid:20)

(cid:18)

exp

−

+

1
n

log E

(cid:20)

exp

log E

exp

−

λ(cid:62) (κ1 − h)

(cid:19)(cid:21)

g(cid:62)θ

cn
√
d
(cid:19)(cid:21)

min
(cid:107)θ(cid:107)2=1

+

1
n

log(I + II).

(63)

In what follows we focus on the right-hand side of the above inequality, the argument is divided into three
parts.
Part 1. Find an upper bound for II
Note that − max1≤i≤n(κ − hi) = min1≤i≤n(hi − κ) has probability density

p(u) = n (1 − Φ (u + κ))n−1 φ (u + κ) ,

thus leading to

(cid:90) +∞

II =

n (1 − Φ (u + κ))n−1 φ (u + κ) exp

(cid:18)

exp

(n − 1) log (1 − Φ (u + κ)) +

(cid:18)

(cid:18)

exp

(n − 1)

log (1 − Φ(κ)) −

(cid:19)

(cid:19)

(cid:18) cnu
√
d
cnu
√
d
φ(κ)u
1 − Φ(κ)

0
(cid:90) +∞

0
(cid:90) +∞

0

=n

(i)
≤n

du

φ (u + κ) du

(cid:19)

+

(cid:19)

cnu
√
d

φ (u + κ) du

(64)

≤n (1 − Φ(κ))n−1

(cid:90) +∞

(cid:18)

exp

−

0

(cid:18) (n − 1)φ(κ)
1 − Φ(κ)

−

cn
√
d

(cid:19)

(cid:19)

u

φ (u + κ) du

(ii)
= od

(cid:16)

n (1 − Φ(κ))n−1(cid:17)

,

where (i) is due to the fact that u (cid:55)→ log(1 − Φ(u + κ)) is concave, and

d
du

log (1 − Φ(u + κ))

(cid:12)
(cid:12)
(cid:12)u=0

= −

φ(κ)
1 − Φ(κ)

,

(ii) is due to our assumption n/d → δ, which further implies that

(n − 1)φ(κ)
1 − Φ(κ)

−

cn
√
d

→ +∞ =⇒

(cid:90) +∞

(cid:18)

exp

−

0

(cid:18) (n − 1)φ(κ)
1 − Φ(κ)

−

cn
√
d

(cid:19)

(cid:19)

u

φ (u + κ) du → 0.

Part 2. Calculate the limit of other terms as n → +∞
We use the following consequence of Varadhan’s Integral lemma (Theorem 4.3.1 of [DZ10]):

31

Lemma 2. Let Xi be i.i.d. non-negative random variables, Sn = X1 + · · · + Xn. Assume ψ(t) = E[exp(tX1)]
is ﬁnite for some t > 0, then we have

lim
n→+∞

1
n

log E

(cid:104)
exp

(cid:16)

(cid:112)
c

(cid:17)(cid:105)

=

nSn






(cid:18) c2
4t
(cid:18) c2
4t

inf
t>0

sup
t<0

(cid:19)

+ log ψ(t)

, c > 0

(cid:19)

.

+ log ψ(t)

, c < 0

The proof of Lemma 2 will be provided in Appendix A.3. Since

E (cid:2)exp (cid:0)tg2

1

(cid:1)(cid:3) =

√

1
1 − 2t

,

0 < t <

1
2

,

it follows from the above lemma that

lim
n→+∞

1
n

(cid:20)

exp

log E

(cid:18) cn
√
d



(cid:19)(cid:21)

(cid:107)g(cid:107)2
(cid:118)
(cid:117)
(cid:117)
(cid:116)d

d
(cid:88)





g2
i







lim
d→+∞

1
d

log E

exp

cδ

inf
t>0

(cid:18) c2δ2
4t

√

c2δ + c

(cid:18)

√

+ log

i=1
(cid:19)(cid:19)

1
1 − 2t

(cid:32) √

1
δ

1
δ

=

=

=

c2δ2 + 4
4

−

1
δ

log

c2δ2 + 4 − cδ
2

(cid:33)

,

(65)

where the last equality just results from direct computation. Similarly, we have (I is deﬁned in Eq. (62))

lim
n→+∞

= lim

n→+∞

1
n

1
n

(cid:18) c2δ
4t

= sup
t<0

log (I) = lim

n→+∞




log E

exp

−c

(cid:20)

(cid:18)

exp

−

log E

1
n

cn
√
d

(cid:13)
(cid:13)(κ1 − h)+

(cid:13)
(cid:13)2

(cid:19)(cid:21)

√

(cid:118)
(cid:117)
(cid:117)
(cid:116)n

δ

n
(cid:88)





(κ − hi)2
+





(cid:19)

+ log ψκ(t)

= − inf
u>0

i=1
(cid:18) c2δ
4u

(cid:19)

− log ψκ(−u)

,

(66)

where ψκ(−u) = E[exp(−u(κ − h)2
Part 3. Put everything together
Note that Eq. (64) implies (II is deﬁned in Eq. (62))

+)], with h ∼ N(0, 1).

lim sup
n→+∞

≤ − inf
u>0

1
n
(cid:18) c2δ
4u

log (II) ≤ log (1 − Φ(κ)) = − lim

(cid:19)

− log ψκ(−u)

= lim

n→+∞

1
n

log (I) .

(cid:18) c2δ
4u

(cid:19)

− log ψκ(−u)

u→+∞

Combining this calculation with Eq. (66) leads to

lim
n→+∞

1
n

log(I + II) = lim

n→+∞

1
n

log (I) = − inf
u>0

(cid:18) c2δ
4u

(cid:19)

− log ψκ(−u)

.

(67)

We can now upper bound P(ξn,κ ≤ 0). Using Eq. (63), Eq. (65) and Eq. (67) we conclude that for all c > 0,

lim sup
n→+∞

1
n

log P (ξn,κ ≤ 0) ≤ −

c2δ
2

+

√

c2δ + c

c2δ2 + 4
4

−

1
δ

log

(cid:32) √

c2δ2 + 4 − cδ
2

(cid:33)

(cid:18) c2δ
4u

− inf
u>0

(cid:19)

− log ψκ(−u)

.

32

Therefore, if there exists c > 0 such that the right-hand side of the above inequality is negative, then
P(ξn,κ ≤ 0) would be exponentially small, and the ﬁrst part of Theorem 3.1 follows immediately. After
dividing both sides by c and making a change of variable c (cid:55)→ c/δ here, we see that it suﬃces to show

√

∃c > 0,

u=ct⇐⇒ ∃c > 0,

√

c2 + 4 − c
4

1
c2 + 4 + c

−

+

1
c

1
c

log

log

√

√

c2 + 4 − c
2

c2 + 4 + c
2

⇐⇒ ∃c > 0, s.t. ∀t > 0,

√

1
c2 + 4 + c

+

1
c

log

(cid:18) c
4u
(cid:18) 1
4t

(cid:19)

log ψκ(−u)

< 0

−

δ
c

−

(cid:19)

log ψκ(−ct)

< 0

δ
c

− inf
u>0

− inf
t>0
√

c2 + 4 + c
2

<

1
4t

−

δ
c

log ψκ(−ct),

(68)

where (by integration by part)

ψκ(−u) = E

(cid:104)

exp

(cid:16)

−u (κ − h)2
+

(cid:17)(cid:105)

= Φ (|κ|) +

exp (cid:0)−u|κ|2/(1 + 2u)(cid:1)
1 + 2u

√

(cid:18)

1 − Φ

(cid:18) |κ|
√

1 + 2u

(cid:19)(cid:19)

.

(69)

According to Deﬁnition 2, if δ > δu(κ), then Eq. (68) holds and we complete the proof of the ﬁrst part of
Theorem 3.2. Now we are in position to ﬁnish the proof. Assume |κ| is suﬃciently large, and

√

δ > (1 + ε)

2π|κ| log |κ| exp

(cid:19)

.

(cid:18) κ2
2

Choose η > 0 such that (1 + η)2 < 1 + ε, and choose c > 0 such that

c2
4(1 + η) log c

= |κ|2(1+η),

then for large |κ|, we must have log c = (1 + ˘oκ(1))(1 + η) log |κ| and

√

1
c2 + 4 + c

+

1
c

log

√

c2 + 4 + c
2

< (1 + η)

log c
c

.

Consider the following two situations:

(1) t ≤ c/(4(1 + η) log c), then we get that

1
4t

≥ (1 + η)

log c
c

>

√

1
c2 + 4 + c

+

1
c

log

√

c2 + 4 + c
2

,

therefore, Eq. (68) holds.

(2) t > c/(4(1 + η) log c). Based on Eq. (69), we obtain that

1 − ψκ

(cid:16)

−|κ|2(1+η)(cid:17)

=1 − Φ (|κ|) −

exp (cid:0)−|κ|2(1+η)|κ|2/ (cid:0)1 + 2|κ|2(1+η)(cid:1)(cid:1)
(cid:112)1 + 2|κ|2(1+η)
(cid:32)

(cid:32)

exp (cid:0)−|κ|2/2(cid:1)
|κ|1+η
exp (cid:0)−|κ|2/2(cid:1)
|κ|

(cid:33)

exp

−

|κ|2
2 (cid:0)1 + 2|κ|2(1+η)(cid:1)

(cid:33)(cid:33)

= (1 + ˘oκ(1))

exp(−|κ|2/2)
2π|κ|

√

.

=1 − Φ (|κ|) − ˘Oκ

(cid:32)

=1 − Φ (|κ|) − ˘oκ

(cid:32)

(cid:32)

1 − Φ

(cid:33)(cid:33)

|κ|
(cid:112)1 + 2|κ|2(1+η)

We also note that

1 − Φ(κ) ≤ ψκ(−u) ≤ 1 =⇒ 1 − ψκ(−u) = ˘oκ(1) =⇒ − log ψκ(−u) = (1 + ˘oκ(1))(1 − ψκ(−u)).

Now by our assumption, δ > (1 + ε)

√

2π|κ| log |κ| exp (cid:0)κ2/2(cid:1), thus leading to

−

δ
c

log ψκ(−ct) > −

(cid:18)

log ψκ

−

c2
4(1 + η) log c

(cid:19)

= −

δ
c

δ
c

log ψκ

(cid:16)

−|κ|2(1+η)(cid:17)

33

=(1 + ˘oκ(1))

(cid:16)

δ
c

1 − ψκ

(cid:16)

−|κ|2(1+η)(cid:17)(cid:17)

= (1 + ˘oκ(1))

δ
c

exp(−|κ|2/2)
2π|κ|

√

>(1 + ˘oκ(1))

(1 + ε) log |κ|
c

≥ (1 + ˘oκ(1))

(1 + ε) log c
(1 + η)c

,

where the last inequality is due to log c = (1 + ˘oκ(1))(1 + η) log |κ|. Since (1 + ε)/(1 + η) > 1 + η, for |κ|
large enough we know that the right-hand side of the above inequality is greater than

(1 + η)

log c
c

>

√

1
c2 + 4 + c

+

1
c

log

which implies Eq. (68).

In conclusion, there exists a κ < 0, such that if κ < κ and

√

c2 + 4 + c
2

,

√

δ > (1 + ε)

2π|κ| log |κ| exp

(cid:19)

,

(cid:18) κ2
2

then we have Eq. (68) and as a consequence, Eq. (17) holds.

A.3 Additional proofs

Proof of Lemma 1. (a): Denote the Mills ratio of standard normal distribution by R(u) = (1−Φ(u))/φ(u),
and deﬁne an auxiliary function Aκ(c) = c − 1/R(κ + c), then we have

A(cid:48)

κ(c) = 1 +

R(cid:48)(κ + c)
R(κ + c)2 = 1 +

(κ + c)R(κ + c) − 1
R(κ + c)2

=

R(κ + c)2 + (κ + c)R(κ + c) − 1
R(κ + c)2

,

since R(cid:48)(u) = uR(u) − 1. According to [Bir42], we have for all u ∈ R,

√

R(u) >

u2 + 4 − u
2

=⇒ R(u)2 + uR(u) − 1 > 0,

κ(c) > 0. Hence Aκ(c) is increasing. Note that Aκ(0) < 0. Choose c > |κ| + 1/|κ|, we

thus leading to A(cid:48)
deduce that

Aκ(c)

(i)
> c −

(κ + c)2 + 1
κ + c

=

c|κ| − κ2 − 1
κ + c

> 0,

where in (i) we use the Gaussian tail bound:

R(x) =

1 − Φ(x)
φ(x)

>

x
x2 + 1

.

The existence and uniqueness of c∗ is thus established. For suﬃciently large |κ|, one has
(cid:19)

(cid:18) 1
κ2
which implies c∗ ∈ (0, 1/κ2). Furthermore,

Aκ

=

1
κ2 − (1 + ˘oκ(1))φ(κ) > 0,

c∗ =

φ(κ + c∗)
1 − Φ(κ + c∗)

= (1 + ˘oκ(1))φ(κ) = ˘Oκ

(cid:18)

(cid:18)

exp

−

(cid:19)(cid:19)

.

κ2
2

(b) & (c): We ﬁrst derive the expression for e(cid:48)(q), namely Eq. (53). Let G, G(cid:48) ∼i.i.d. N(0, 1), then we have
(cid:104)
f (G)f (qG +

(cid:105)
1 − q2G(cid:48))

e(cid:48)(q) =

(cid:112)

E

d
Eq[f (G1)f (G2)] =
dq
(cid:34)

d
dq

(cid:32)

=E

f (G)f (cid:48)(qG +

1 − q2G(cid:48))

G −

(cid:112)

(cid:104)

(i)
=E

f (cid:48)(G)f (cid:48)(qG +

(cid:112)

1 − q2G(cid:48))

(cid:105)

+ qE

(cid:33)(cid:35)

G(cid:48)

q
(cid:112)1 − q2
(cid:104)
f (G)f (cid:48)(cid:48)(qG +

(cid:112)

(cid:105)
1 − q2G(cid:48))

(70)

(cid:104)

E

q
(cid:112)1 − q2
f (cid:48)(G)f (cid:48)(qG +

−

(cid:104)

=E

f (G)f (cid:48)(cid:48)(qG +

(cid:112)

1 − q2G(cid:48)) ·

(cid:112)

1 − q2

(cid:105)

(cid:112)

1 − q2G(cid:48))

(cid:105)

= Eq[f (cid:48)(G1)f (cid:48)(G2)],

34

where (i) is due to Stein’s identity. Note that f is weakly diﬀerentiable, with

f (cid:48)(x) = −c∗ exp(−c∗x)1{x ≥ κ} + exp(−c∗κ)δκ(x) = −c∗f (x) + exp(−c∗κ)δκ(x),

where δκ denotes the Dirac’s delta measure at x = κ. It then follows that

e(cid:48)(q) =c2
∗

Eq [f (G1)f (G2)] − 2c∗ exp(−c∗κ)Eq [f (G1)δκ(G2)] + exp(−2c∗κ)Eq [δκ(G1)δκ(G2)]

=c2

∗e(q) −

c∗ exp(−c∗κ)
π(cid:112)1 − q2

(cid:90) +∞

(cid:18)

exp

−

κ

1
2(1 − q2)

(x2 + κ2 − 2qκx) − c∗x

(cid:19)

dx +

exp(−2c∗κ)
2π(cid:112)1 − q2

(cid:18)

exp

−

(cid:19)

κ2
1 + q

:=T1 − T2 + T3.

As for the ﬁrst term, we notice that

e(q) =Eq [exp(−c∗(G1 + G2))1{min(G1, G2) ≥ κ}]

(cid:19)
(cid:0)x2 + y2 − 2qxy(cid:1) − c∗(x + y)

dxdy

(cid:90)

[κ,+∞)2

1
2π(cid:112)1 − q2
(cid:90)

(cid:18)

exp

−

1
2(1 − q2)
(cid:18)

=

(i)
=

=

(ii)
=

∗(1 + q))

∗(1 + q))

exp(c2
2π(cid:112)1 − q2
exp(c2
2π(cid:112)1 − q2
exp(c2
2π(cid:112)1 − q2

[κ,+∞)2

(cid:90)

[κ,+∞)2

(cid:90)

∗(1 + q))

1
2(1 − q2)
∗(1 + q)(cid:1) Pq (min (G1, G2) ≥ κ + c∗(1 + q)) ,

[κ+c∗(1+q),+∞)2

exp

−

(cid:18)

= exp (cid:0)c2

exp

−

(cid:18)

exp

−

−

(x + y)2
(x − y)2
4(1 + q)
4(1 − q)
(x + y + 2c∗(1 + q))2
4(1 + q)

− c∗(x + y) − c2

∗(1 + q)

(cid:19)

dxdy

(cid:19)

−

(x − y)2
4(1 − q)

(cid:0)x2 + y2 − 2qxy(cid:1)

dxdy

(cid:19)

dxdy

where in (i) and (ii) we use the identity

1
2(1 − q2)

(cid:0)x2 + y2 − 2qxy(cid:1) =

(x + y)2
4(1 + q)

+

(x − y)2
4(1 − q)

.

This proves that

T1 = c2

∗ exp (cid:0)c2

∗(1 + q)(cid:1) Pq (min (G1, G2) ≥ κ + c∗(1 + q)) .

Moreover, by direct calculation we obtain that

T2 =

c∗ exp(−c∗κ)
π(cid:112)1 − q2
(cid:18)

1 − Φ

(cid:114) 2
π

=

κ
(cid:18)(cid:114) 1 − q
1 + q

(cid:90) +∞

(cid:18)

exp

−

1
2(1 − q2)

(x2 + κ2 − 2qκx) − c∗x

(cid:19)

dx

(κ + c∗(1 + q))

c∗ exp

(cid:19)(cid:19)

(cid:18)

c2
∗(1 + q) −

1
2

(κ + c∗(1 + q))2

(cid:19)

,

thus proving Eq. (53). Now we have

e(cid:48)(0) =

exp(−2κc∗ − κ2) + c2

∗ exp(c2

∗) (1 − Φ(κ + c∗))2 −

∗) (c∗ (1 − Φ(κ + c∗)) − φ(κ + c∗))2 = 0,

1
2π
= exp(c2

(cid:114) 2
π

(1 − Φ(κ + c∗)) c∗ exp

(cid:18)

c2
∗ −

1
2

(cid:19)

(κ + c∗)2

and consequently F (cid:48)(0) = −e(cid:48)(0)/e(0) = 0. Next we prove e(0) = 1+ ˘oκ(1), and e(1)−e(0) = (1+ ˘oκ(1))Φ(κ).
It’s straightforward that

E[f (G)] =

= exp

(cid:19)

(cid:18) c2
∗
2

(cid:90) +∞

κ

1
√
2π

(cid:18)

exp

−c∗x −

(cid:19)

x2
2

dx = exp

(cid:18) c2
∗
2

(cid:19) (cid:90) +∞

κ

1
√
2π

(cid:18)

exp

−

(x + c∗)2
2

(cid:19)

dx

(1 − Φ(κ + c∗)) = exp(˘oκ(1))(1 + ˘oκ(1)) = 1 + ˘oκ(1),

35

since c∗ = ˘oκ(1). Hence we deduce that e(0) = E[f (G)]2 = 1 + ˘oκ(1). Similarly, we calculate

e(1) = E (cid:2)f (G)2(cid:3) =

(cid:90) +∞

κ

1
√
2π

(cid:18)

exp

−2c∗x −

(cid:19)

x2
2

dx = exp (cid:0)2c2

∗

(cid:1) (1 − Φ(κ + 2c∗)) ,

which implies that

e(1) − e(0) = (1 + ˘oκ(1))

e(1) − e(0)
e(0)

= (1 + ˘oκ(1)) log

(cid:19)

(cid:18) e(1)
e(0)

(cid:18)

exp(c2
∗)

1 − Φ(κ + 2c∗)
(1 − Φ(κ + c∗))2

(cid:19)

=(1 + ˘oκ(1)) log
=(1 + ˘oκ(1)) (cid:0)c2

=(1 + ˘oκ(1))

(i)
=(1 + ˘oκ(1))

∗ + log (1 − Φ(κ + 2c∗)) − 2 log (1 − Φ(κ + c∗))(cid:1)
(cid:17)
(cid:16) ˘Oκ(exp(−κ2)) − (1 + ˘oκ(1))Φ(κ + c∗) + 2(1 + ˘oκ(1))Φ(κ + 2c∗)
(cid:16) ˘Oκ(exp(−κ2)) + (1 + ˘oκ(1))Φ(κ)

= (1 + ˘oκ(1))Φ(κ),

(cid:17)

as desired. Here the approximation (i) is valid because c∗ = ˘oκ(1/κ). Hence, (b) is proved.

Now we turn to show Eq. (54), i.e., the uniform bound on e(cid:48)(cid:48)(q). Using Eq (53), we derive

e(cid:48)(cid:48)(q) =

exp

(cid:18)

−2κc∗ −

κ2
1 + q

(cid:19) (cid:18) q

1 − q2 +

κ2

(1 + q)2 + c2

∗(1 − 2q) −

(cid:19)

2κc∗
1 + q

1
2π(cid:112)1 − q2
∗ exp (cid:0)c2
+ c4
(cid:114) 2
(cid:18)
π

+

∗(1 + q)(cid:1) Pq (min (G1, G2) ≥ κ + c∗(1 + q))

1 − Φ

(cid:18)(cid:114) 1 − q
1 + q

(κ + c∗(1 + q))

(cid:19)(cid:19)

c2
∗(κ − c∗(1 − q)) exp

(cid:18)

c2
∗(1 + q) −

1
2

(κ + c∗(1 + q))2

(cid:19)

:=A(q) + B(q) + C(q).

Again noting that c∗ = ˘Oκ(exp(−κ2/2)), we get the following upper bounds:

sup
u∈[−q,q]

|A(u)| ≤

=

|B(u)| ≤c4

1
2π(cid:112)1 − q2
1 + ˘oκ(1)
2π(cid:112)1 − q2
∗ exp (cid:0)c2

(cid:18)

exp

−2κc∗ −

κ2
1 + q
(cid:19) (cid:18) q

(cid:19) (cid:18) q

1 − q2 +
κ2

(cid:18)

−

exp

κ2
1 + q
∗(1 + q)(cid:1) = (1 + ˘oκ(1))c4

1 − q2 +

(1 − q)2 + c2
∗ = ˘Oκ(exp(−2κ2)),

κ2

∗ −

(1 − q)2 + c2
2κc∗
1 − q

∗ −

(cid:19)

2κc∗
1 − q

(cid:19)

≤ |κ|C exp

(cid:18)

−

(cid:19)

,

κ2
1 + q

sup
u∈[−q,q]

sup
u∈[−q,q]

|C(u)| ≤

(cid:114) 2
π
(cid:114) 2
π

=

c2
∗(|κ| + c∗(1 + q)) exp

(cid:18)

c2
∗(1 + q) −

1
2

(κ + c∗(1 + q))2

(cid:19)

c2
∗(|κ| + c∗(1 + q))(1 + ˘oκ(1)) exp

(cid:19)

(cid:18)

−

κ2
2

≤ |κ|C ˘Oκ

(cid:18)

(cid:18)

exp

−

(cid:19)(cid:19)

,

3κ2
2

which immediately implies

We ﬁnish the proof here.

sup
u∈[−q,q]

|e(cid:48)(cid:48)(u)| ≤ |κ|C exp

(cid:18)

−

(cid:19)

.

κ2
1 + q

Proof of Lemma 2. This proof is based on the original Varadhan’s lemma, i.e., Theorem 4.3.1 in [DZ10].
According to Cramer’s theorem (Theorem 2.2.3 in [DZ10]), we know that Sn/n satisﬁes the LDP (Large
deviations principle) with the following convex rate function:

I(x) = sup
t∈R

{tx − log ψ(t)} .

36

Note that by our assumption, ψ(t0) < +∞ for some t0 > 0, and ψ(t) is increasing (X1 is non-negative),
therefore ψ(t) < +∞ on (−∞, t0), and Lemma 2.2.20 of [DZ10] implies I(x) is a good rate function. Now
we verify the condition of Varadhan’s lemma, i.e., there exists γ > 1 such that

lim sup
n→+∞

1
n

log E

(cid:104)
exp

(cid:16)

(cid:112)

γc

nSn

(cid:17)(cid:105)

< +∞.

The above inequality automatically holds when c ≤ 0. For c > 0, we have

lim sup
n→+∞

1
n

log E

(cid:104)

exp

(cid:16)

(cid:112)

γc

nSn

(cid:17)(cid:105) (i)

≤ lim sup
n→+∞

1
n

log E

(cid:20)

(cid:18)

exp

t0Sn +

(cid:19)(cid:21)

nγ2c2
4t0

= log ψ(t0) +

γ2c2
4t0

,

where (i) follows from AM-GM inequality. Hence, Varadhan’s lemma is applicable, and we obtain that

lim
n→+∞

1
n

log E

(cid:104)

(cid:16)

(cid:112)
c

exp

nSn

(cid:17)(cid:105)

= sup
x>0

√

(cid:8)c

x − I(x)(cid:9) .

It suﬃces to show the following relationship:

√

(cid:8)c

x − I(x)(cid:9) =

sup
x>0






(cid:18) c2
4t
(cid:18) c2
4t

inf
t>0

sup
t<0

(cid:19)

+ log ψ(t)

, for c > 0,

(cid:19)

+ log ψ(t)

, for c < 0.

To this end we ﬁrst consider the case when c < 0, note that c

√

x = supt<0{c2/(4t) + tx}, therefore

√

(cid:8)c

x − I(x)(cid:9) = sup

x>0

sup
x>0

(cid:26) c2
4t

sup
t<0

+ tx − I(x)

(cid:27)

= sup
t<0

sup
x>0

(cid:26) c2
4t

+ tx − I(x)

(cid:27)

(cid:26) c2
4t

= sup
t<0

(cid:27)

+ log ψ(t)

,

where the last equality is due to the property of Legendre transform. Now assume c > 0, it is well-known
that if x ≤ ψ(cid:48)(+∞)/ψ(+∞), then I(x) = txx − log ψ(tx), where tx satisﬁes (tx can be +∞)

ψ(cid:48)(tx)
ψ(tx)

= x ⇐⇒ tx =

(cid:19)−1

(cid:18) ψ(cid:48)
ψ

(x),

otherwise I(x) = +∞. Since x (cid:55)→ tx is a bijection from [0, +∞) to R, we obtain that

√

(cid:8)c

x − I(x)(cid:9) = sup

x>0

sup
x>0

(cid:115)

(cid:40)
c

ψ(cid:48)(tx)
ψ(tx)

− tx

ψ(cid:48)(tx)
ψ(tx)

(cid:41)

+ log ψ(tx)

= sup
t∈R

(cid:115)
(cid:40)
c

ψ(cid:48)(t)
ψ(t)

− t

ψ(cid:48)(t)
ψ(t)

(cid:41)

+ log ψ(t)

.

We calculate the derivative of the above function:

(cid:115)

(cid:32)
c

d
dt

ψ(cid:48)(t)
ψ(t)

− t

ψ(cid:48)(t)
ψ(t)

(cid:33)

(cid:115)

+ log ψ(t)

=

ψ(t)
ψ(cid:48)(t)

(cid:32)

c
2

(cid:115)

(cid:33)

ψ(cid:48)(t)
ψ(t)

d
dt

(cid:18) ψ(cid:48)(t)
ψ(t)

(cid:19)

.

− t

Since t(cid:112)ψ(cid:48)(t)/ψ(t) is increasing on (0, +∞), and approaches 0 and +∞ as t → 0 and t → +∞ respectively,
there exists a unique t∗ > 0 such that

Moreover, we have

c
2

= t∗

(cid:115)

ψ(cid:48)(t∗)
ψ(t∗)

t > (<)t∗ ⇐⇒

, and further

c
2

< (>)t∗

(cid:115)

ψ(cid:48)(t∗)
ψ(t∗)

.

(cid:40)

(cid:115)
c

sup
t∈R

ψ(cid:48)(t)
ψ(t)

− t

ψ(cid:48)(t)
ψ(t)

(cid:41)

+ log ψ(t)

37

(cid:115)

=c

ψ(cid:48)(t∗)
ψ(t∗)

− t∗ ψ(cid:48)(t∗)
ψ(t∗)

+ log ψ(t∗)

=c ·

(cid:17)2

c
2t∗ − t∗ ·
c2
4t∗ + log ψ(t∗)

(cid:16) c
2t∗
(i)
= inf
t>0

+ log ψ(t∗)
(cid:18) c2
4t

=

(cid:19)

+ log ψ(t)

,

where (i) just results from the fact that

d
dt

(cid:18) c2
4t

(cid:19)

+ log ψ(t)

= −

c2
4t2 +

ψ(cid:48)(t)
ψ(t)

=

1
t2

(cid:32)

(cid:115)
t

ψ(cid:48)(t)
ψ(t)

−

c
2

(cid:115)
(cid:33) (cid:32)
t

(cid:33)

ψ(cid:48)(t)
ψ(t)

+

c
2

is negative when t < t∗, and is positive when t > t∗. In conclusion, we have

√

(cid:8)c

sup
x>0

x − I(x)(cid:9) = inf

t>0

(cid:18) c2
4t

(cid:19)

+ log ψ(t)

.

This completes the proof.

B Linear programming algorithm in the pure noise model:

Proof of Theorem 3.3

Recall that we assume yi = 1 for all i without loss of generality. In this proof, we will also assume that ε1 > 0
is a suﬃciently small constant such that δΦ(κ/(1 + ε1)) < 1 if δ < 1/Φ(κ) holds and δΦ(κ/(1 − ε1)) > 1 if
δ > 1/Φ(κ) holds.

The optimization problem (19) can be rewritten into the max-min form.

(cid:40)

M = max
(cid:107)θ(cid:107)≤1

min
α≥0

(cid:104)θ, v(cid:105) +

(cid:41)

(cid:0)(cid:104)xi, θ(cid:105) − κ(cid:1)

αi

n
(cid:88)

i=1

= max
(cid:107)θ(cid:107)≤1

min
α≥0

{(cid:104)θ, v(cid:105) + (cid:104)α, Xθ(cid:105) − κ(cid:104)α, 1n(cid:105)}

We will apply Gordon’s theorem. To this end, we deﬁne

(cid:102)M = max
(cid:107)θ(cid:107)≤1

min
α≥0

(cid:110)

(cid:104)θ, v(cid:105) + (cid:107)θ(cid:107)(cid:104)α, g(cid:105) + (cid:107)α(cid:107)(cid:104)θ, h(cid:105) − κ(cid:104)α, 1n(cid:105)

(cid:111)
.

(71)

(72)

where g ∼ N(0, In) and h ∼ N(0, Id) are independent Gaussian vectors. We use a variant of Gordon’s
theorem due to [TOH15, Thm. 3] which is also used in [MRSY19].

Theorem B.1. Let C1 ∈ Rn and C2 ∈ Rd be two compact sets and let T : C1 × C2 → R be a continuous
function. Let X = (Xi,j) ∼i.i.d. N(0, 1) ∈ Rn×d, g ∼ N(0, In), and h ∼ N(0, Id) be independent vectors and
matrices. Deﬁne,

Q1(X) = min
w1∈C1
Q2(g, h) = min
w1∈C1

max
w2∈C2

w(cid:62)

1 Xw2 + T (w1, w2),

max
w2∈C2

(cid:107)w2(cid:107)g(cid:62)w1 + (cid:107)w1(cid:107)h(cid:62)w2 + T (w1, w2).

(73)

(74)

Then, for all t ∈ R,

(a) we have

(b) if further, C1 and C2 are convex and T is convex concave in (w1, w2), then

P (Q1(X) ≤ t) ≤ 2P (Q2(g, h) ≤ t) ;

P (Q1(X) ≥ t) ≤ 2P (Q2(g, h) ≥ t) .

38

When applying this theorem, we ﬁrst condition on v and then taking expectations in the end. A subtlety
is that the constraint α ≥ 0 does not produce a compact feasible region. We nevertheless obtain the desired
inequalities (75) below; proofs are deferred to the end of this section.

We apply this theorem twice: one with the original constraints in the optimization problem (19) and the
other with the norm constraint (cid:107)θ(cid:107) ≤ 1 replaced by (cid:107)θ(cid:107) ≤ 1 + ε1. The problem with the modiﬁed constraint
is only used for the proof.

Corollary B.1. Let M and (cid:102)M be deﬁned as in (71) and (72). We replace the constraint (cid:107)θ(cid:107) ≤ 1 by
(cid:107)θ(cid:107) ≤ 1 + ε1 and similarly deﬁne M (cid:48) and (cid:102)M (cid:48). Then, we have
(cid:40)P(cid:0)M ≤ t(cid:1) ≤ 2P(cid:0)
P(cid:0)M ≥ t(cid:1) ≤ 2P(cid:0)

(cid:40)P(cid:0)M (cid:48) ≤ t(cid:1) ≤ 2P(cid:0)
P(cid:0)M (cid:48) ≥ t(cid:1) ≤ 2P(cid:0)

(cid:102)M (cid:48) ≤ t(cid:1),
(cid:102)M (cid:48) ≥ t(cid:1).

(cid:102)M ≤ t(cid:1),
(cid:102)M ≥ t(cid:1).

(75)

In order to analyze (cid:102)M , ﬁrst we ﬁx some r1 ≥ 0 and minimize our objective over α ≥ 0 with (cid:107)α(cid:107) = r1.

Then we can rewrite (cid:102)M as

(cid:102)M = max
(cid:107)θ(cid:107)≤1

min
r1≥0

(cid:104)

(cid:104)θ, v(cid:105) + r1(cid:104)θ, h(cid:105) + min
(cid:107)α(cid:107)=r1
α≥0

(cid:8)(cid:104)α, (cid:107)θ(cid:107)g − κ1n(cid:105)(cid:9)(cid:105)
.

For any r ∈ [0, 1 + ε1], we deﬁne

(cid:102)Mr = max
(cid:107)θ(cid:107)=r

min
r1≥0

(cid:104)

(cid:104)θ, v(cid:105) + r1(cid:104)θ, h(cid:105) + min
(cid:107)α(cid:107)=r1
α≥0

(cid:8)(cid:104)α, (cid:107)θ(cid:107)g − κ1n(cid:105)(cid:9)(cid:105)

so that (cid:102)M = maxr∈[0,1] (cid:102)Mr and (cid:102)M (cid:48) = maxr∈[0,1+ε1] (cid:102)Mr under this notation. Notice that the following holds
with probability 1 − od(1): for all θ with ε(cid:48) ≤ (cid:107)θ(cid:107) ≤ 1 + ε1 where ε(cid:48) ∈ (0, 1) is any constant, there is at least
one negative element in the vector (cid:107)θ(cid:107)g − κ1n. In fact, we have

P (∃ i ∈ [n] such that ε(cid:48)gi − κ < 0) = 1 − (1 − Φ((ε(cid:48))−1κ))n → 1

as n → ∞.

For r ≥ ε(cid:48) and on this event, we have

{(cid:104)α, (cid:107)θ(cid:107)g − κ1n(cid:105)}

min
(cid:107)α(cid:107)=r1
α≥0

(i)
= min

(cid:107)α(cid:107)=r1
α≥0

(cid:8)(cid:10)α, − [(cid:107)θ(cid:107)g − κ1n]−

(cid:11)(cid:9)

(ii)
= −r1

(cid:13)
(cid:13) [(cid:107)θ(cid:107)g − κ1n]−

(cid:13)
(cid:13)

where (i) is because we always assign zero to αi if (cid:107)θ(cid:107)gi −κ is positive, and (ii) is due to the Cauchy-Schwarz
inequality (particularly the condition where equality holds). Therefore,

(cid:102)Mr

(i)
= max
(cid:107)θ(cid:107)≤r

min
r1≥0

(cid:110)

(cid:104)θ, v(cid:105) + r1(cid:104)θ, h(cid:105) − r1

(cid:13)
(cid:13) [rg − κ1n]−

(cid:111)

(cid:13)
(cid:13)

(ii)
= min
r1≥0

(iii)
= min
r1≥0

max
(cid:107)θ(cid:107)≤r
(cid:113)
(cid:110)
r

(cid:110)

(cid:104)θ, v(cid:105) + r1(cid:104)θ, h(cid:105) − r1

(cid:13)
(cid:13) [rg − κ1n]−

(cid:111)

(cid:13)
(cid:13)

1 + r2

1(cid:107)h(cid:107)2 − r1

(cid:13)
(cid:13) [rg − κ1n]−

(cid:111)

(cid:13)
(cid:13)

Here, in (i) we replaced the constraint (cid:107)θ(cid:107) = r by (cid:107)θ(cid:107) ≤ r since the maximum can be always attained at the
boundary, (ii) is because of Sion’s minimax theorem, and (iii) is because of the Cauchy-Schwarz inequality.
We then simplify the inner minimization problem using the following lemma.

Lemma 3. Suppose a, b ≥ 0. Then we have

(cid:110)

(cid:112)

1 + x2 − bx

(cid:111)

=

a

inf
x≥0

(cid:40)√

a2 − b2,

−∞,
(cid:40)

√

if a ≥ b
if a < b

(cid:110)

(cid:112)

1 + x2 − bx

(cid:111)

a

argmin
x≥0

=

b/
∞,

a2 − b2,

if a > b
if a ≤ b

39

Proof of Lemma 3. It is easy to check that the function f (x) = a

√

1 + x2 − bx is convex in x. Moreover,

f (cid:48)(x) =

√

ax
1 + x2

− b.

If a ≤ b, the function f (x) is monotone decreasing, so the inﬁmum is taken at ∞; if a > b, the function f (x)
has a unique minimizer at x such that f (cid:48)(x) = 0, which gives x = b/

a2 − b2.

√

Denote T (x) = x1{x ≥ 0} + (−∞)1{x < 0}. We apply this lemma (by viewing (cid:107)h(cid:107)r1 as x) and obtain

(cid:102)Mr = T

(cid:20)(cid:16)

r2 −

1
(cid:107)h(cid:107)2

(cid:13)
(cid:2)rg − κ1n
(cid:13)
(cid:13)

(cid:3)

−

2(cid:17)1/2(cid:21)
(cid:13)
(cid:13)
(cid:13)

.

Lemma 4. Recall that n/d → δ. Denote G ∼ N(0, 1). We have

sup
r∈[0,1+ε1]

(cid:12)
(cid:12)
(cid:12)

1
(cid:107)h(cid:107)2

(cid:13)
(cid:2)rg − κ1n
(cid:13)
(cid:13)

(cid:3)

−

(cid:13)
2
(cid:13)
(cid:13)

− δE

(cid:110)(cid:2)rG − κ(cid:3)2

−

(cid:111) (cid:12)
(cid:12)
(cid:12) = on,P(1).

Proof of Lemma 4. First, we use uniform law of large numbers (ULLN) [NM94][Lemma 2.4] to obtain

sup
r∈[0,1+ε1]

(cid:12)
(cid:12)
(cid:12)

1
n

(cid:13)
(cid:2)rg − κ1n
(cid:13)
(cid:13)

(cid:3)
−

(cid:13)
2
(cid:13)
(cid:13)

− E

(cid:110)(cid:2)rG − κ(cid:3)2

−

(cid:111) (cid:12)
(cid:12)
(cid:12) = on,P(1).

− ≤ (1 + ε1)2g2
(Note that [rgi − κ]2
(cid:13)
Denote f1(r) = n−1(cid:13)
2
(cid:2)rg − κ1n
(cid:13)
(cid:13)
(cid:13)
(cid:13)
and n

(cid:107)h(cid:107)2 = δ + od,P(1) and conclude that

(cid:3)
−

i holds for all r ∈ [0, 1] and E[g2
and f2(r) = E

(cid:110)(cid:2)rG − κ(cid:3)2

i ] < ∞, so we can apply the ULLN.)
(cid:111)
. We notice that supr∈[0,1+ε1] f2(r) ≤ E[G2]

−

sup
r∈[0,1+ε1]

(cid:12)
(cid:12)
(cid:12)

(cid:12)
n
(cid:12)
(cid:12) ≤
(cid:107)h(cid:107)2 f1(r) − δf2(r)

n
(cid:107)h(cid:107)2

sup
r∈[0,1+ε1]

(cid:12)f1(r) − f2(r)(cid:12)
(cid:12)

(cid:12) +

(cid:12)
(cid:12)
(cid:12)

n
(cid:107)h(cid:107)2 − δ

(cid:12)
(cid:12)
(cid:12)

sup
r∈[0,1+ε1]

f2(r) = od,P(1),

which ﬁnishes the proof.

Now let us deﬁne, for r ∈ [0, 1 + ε1],

F (r) = r2 − δE

(cid:110)(cid:2)rG − κ(cid:3)2

−

(cid:111)

.

Using this deﬁnition, we can express (cid:102)Mr as, for all r ∈ [ε(cid:48), 1 + ε1],

(cid:20)(cid:16)

(cid:102)Mr = T

F (r) + od,P(1)

(cid:17)1/2(cid:21)

.

Here, od,P(1) is uniform over all r.

Lemma 5. Consider the function F (r) deﬁned above. For all r ∈ (0, 1 + ε1), the function F (r) is diﬀeren-
tiable, and its derivative is given by

F (cid:48)(r) = 2r

(cid:16)

1 − δP(cid:0)rG − κ < 0(cid:1)(cid:17)

.

If δ < Φ(κ/(1 + ε1))−1, then F (cid:48)(r) > 0 and F (r) > 0 for all r ∈ (0, 1 + ε1]. If δ > Φ(κ/(1 − ε1))−1, then
there exists r1 ∈ (0, 1 − ε1) such that F (cid:48)(r) > 0 on r ∈ [0, r1) and F (cid:48)(r) < 0 on r ∈ (r1, 1)
Proof of Lemma 5. The diﬀerentiability of F (r) follows from the fact that (cid:2)rG − κ(cid:3)2
r and the dominated convergence theorem. Taking the derivative of F (r), we obtain
1 − δP(cid:0)rG − κ < 0(cid:1)(cid:17)
(cid:110)
G(cid:2)rG − κ(cid:3)

− is diﬀerentiable in

F (cid:48)(r) = 2r + 2δE

= 2r

(cid:111)

(cid:16)

−

where we used Stein’s identity in the second equality. Note that P(cid:0)rG − κ < 0(cid:1) = Φ(r−1κ) is monotone
increasing in r, where recall that Φ is the Gaussian CDF. Thus we have
F (cid:48)(r) > 2r(cid:0)1 − δΦ(κ/(1 + ε1))(cid:1) > 0
in the case δ < Φ(κ/(1 + ε1))−1. Since F (0) = 0, it follows that F (r) > 0. If δ > Φ(κ/(1 − ε1))−1, then
there the equation 1 − δP(cid:0)rG − κ < 0(cid:1) = 0 has a unique zero r1 ∈ (0, 1 − ε1).

40

This lemma states that maximizing F (r) requires r to be as large as possible. Thus, we obtain the

following result.

Proposition B.1. Suppose that δ < Φ(κ/(1 + ε1))−1 holds. Then, we have

(cid:102)Mr = (cid:112)F (1) + od,P(1),

max
r∈[0,1]

max
r∈[0,1+ε1]

(cid:102)Mr = (cid:112)F (1 + ε1) + od,P(1).

(76)

Consequently, with probability 1 − od(1), we have M (cid:48) > M .

Proof of Proposition B.1. First, we notice that (cid:102)Mr ≤ max(cid:107)θ(cid:107)=r(cid:104)θ, v(cid:105) = r from the deﬁnition of (cid:102)Mr
(choosing r1 = 0). This gives a simple bound maxr∈[0,ε(cid:48)] (cid:102)Mr ≤ ε(cid:48). We can choose ε(cid:48) so that ε(cid:48) < (cid:112)F (1).
From Lemma 5 we know that maxr∈[0,1] F (r) = F (1) > 0 so maxr∈[0,1] (cid:102)Mr = (cid:112)F (1) + od,P(1). The second
equality in (76) is obtained similarly. Let ε0 = (cid:112)F (1 + ε1) − (cid:112)F (1), which does not depend on d. Then
the following holds with probability 1 − od(1).

max
r∈[0,1+ε1]

Mr ≥ max

r∈[0,1+ε1]

(cid:102)Mr −

ε0
3

, max
r∈[0,1]

(cid:102)Mr ≥ max
r∈[0,1]

Mr −

ε0
3

,

max
r∈[0,1+ε1]

(cid:102)Mr > max
r∈[0,1]

(cid:102)Mr +

2ε0
3

.

The ﬁrst two inequalities are due to Corollary B.1 and the third is due to (76). Combining these inequalities
leads to M (cid:48) > M .

Proof of Theorem 3.3. First, we consider the case δΦ(κ/(1 + ε1)) < 1. Recall that (cid:98)θ is a maximizer to
the optimization problem (19) so by feasibility we must have yi(cid:104)xi, (cid:98)θ(cid:105) ≥ κ. Let (cid:98)θ
be any maximizer to the
modiﬁed optimization problem where the constraint (cid:107)θ(cid:107) ≤ 1 is replaced by (cid:107)θ(cid:107) ≤ 1 + ε1. It must also satisfy
yi(cid:104)xi, (cid:98)θ

(cid:105) ≥ κ. For any λ ∈ [0, 1], the interpolant

(cid:48)

(cid:48)

(cid:98)θλ := λ(cid:98)θ + (1 − λ)(cid:98)θ

(cid:48)

also satisﬁes yi(cid:104)xi, (cid:98)θλ(cid:105) ≥ κ by linearity. Proposition B.1 implies that the modiﬁed optimization problem
(cid:48)
, v(cid:105) > (cid:104)(cid:98)θ, v(cid:105). If (cid:107)(cid:98)θ(cid:107) < 1, then we can
has a strictly larger maximum, so we must have (cid:107)(cid:98)θ
choose appropriate λ ∈ (0, 1) such that (cid:107)(cid:98)θλ(cid:107) = 1 and (cid:104)(cid:98)θλ, v(cid:105) > (cid:104)(cid:98)θ, v(cid:105), which contradicts the deﬁnition of (cid:98)θ.
Therefore, we conclude that (cid:107)(cid:98)θ(cid:107) = 1, so the ﬁrst claim of the theorem is proved.

(cid:107) > 1 and (cid:104)(cid:98)θ

(cid:48)

Next, we consider the case δΦ(κ/(1 − ε1)) > 1.

In (71), we replace the constraint (cid:107)θ(cid:107) ≤ 1 by a
non-convex constraint 1 − ε1 ≤ (cid:107)θ(cid:107) ≤ 1 and denote the optimal values by M (cid:48)(cid:48). Lemma 5 implies that
maxr∈[0,1] (cid:102)Mr > maxr∈[1−ε1,1] (cid:102)M with probability 1 − od(1). Despite the non-convex constraint, we are still
able to apply Theorem B.1 (a) and obtain M > M (cid:48)(cid:48) with probability 1 − od(1). This implies that the optimal
solution (cid:98)θ must satisfy (cid:107)(cid:98)θ(cid:107) ≤ 1 − ε1 with probability 1 − od(1), thus proving the second claim.

The ﬁnal claim follows from the Gaussian tail probability asymptotics:

Φ(κ) =

1 + oκ(1)
√
2π |κ|

(cid:18)

exp

−

(cid:19)

.

κ2
2

Now the proof is complete.

Finally, we prove Corollary B.1 as claim before.

Proof of Corollary B.1. We will only prove the inequalities for M and (cid:102)M as the others can be derived
similarly. Let (τk)k≥1 be an increasing sequence with τk > 0 and limk→∞ τk = ∞. Let us deﬁne

Mk = max
(cid:107)θ(cid:107)≤1

(cid:102)Mk = max
(cid:107)θ(cid:107)≤1

min
α≥0
(cid:107)α(cid:107)≤τk

min
α≥0
(cid:107)α(cid:107)≤τk

{(cid:104)θ, v(cid:105) + (cid:104)α, Xθ(cid:105) − κ(cid:104)α, 1n(cid:105)}

(cid:110)

(cid:104)θ, v(cid:105) + (cid:107)θ(cid:107)(cid:104)α, g(cid:105) + (cid:107)α(cid:107)(cid:104)θ, h(cid:105) − κ(cid:104)α, 1n(cid:105)

(cid:111)
.

41

Note that we can ﬂip min and max by multiplying −1 on both sides, i.e.,

−Mk = min
(cid:107)θ(cid:107)≤1

max
α≥0
(cid:107)α(cid:107)≤τk

{(cid:104)θ, −v(cid:105) + (cid:104)α, −Xθ(cid:105) + κ(cid:104)α, 1n(cid:105)}

and − (cid:102)Mk has a similar expression. We can then apply Theorem B.1 and obtain, for every k ≥ 1 and t

P(Mk ≤ t) ≤ 2P( (cid:102)Mk ≤ t),

P(Mk ≥ t) ≤ 2P( (cid:102)Mk ≥ t).

It suﬃces to prove that limk→∞ Mk = M and limk→∞ (cid:102)Mk = (cid:102)M . Clearly M ≥ 0 and (cid:102)M ≥ 0 (because they
are lower bounded at θ = 0). By deﬁnition of Mk, we must have Mk ≥ M and Mk is non-increasing. It is
similar for (cid:102)Mk. So clearly we have limk Mk ≥ M and limk (cid:102)Mk ≥ (cid:102)M . Below we shall prove that limk Mk ≤ M
and limk (cid:102)Mk ≤ (cid:102)M .

Let θk be a maximizer associated with Mk. Without loss of generality we assume θk converges to a limit
θ∗ where (cid:107)θ∗(cid:107) ≤ 1, because otherwise we can pass to a subsequence. We claim that for every i, the sequence
((cid:104)xi, θk(cid:105) − κ)k≥1 does not have a negative accumulation point. Indeed, if it has a negative accumulation
point, then we could assign αi = −τk, αj = 0 (j (cid:54)= i), which would lead to limk Mk = −∞, which is a
contradiction. Thus (cid:104)xi, θ∗(cid:105) − κ ≥ 0 for every i and therefore M ≥ (cid:104)θ∗, v(cid:105). On the other hand, we have
Mk ≤ (cid:104)θk, v(cid:105), which implies limk Mk ≤ (cid:104)θ∗, v(cid:105) ≤ M .

Next, let (cid:101)θk be a maximizer associated with (cid:102)Mk. Without loss of generality we assume (cid:101)θk converges to

∗

∗

a limit (cid:101)θ

where (cid:107)(cid:101)θ

(cid:107) ≤ 1. We rewrite (cid:102)Mk into

(cid:102)Mk = max
(cid:107)θ(cid:107)≤1

min
α≥0
(cid:107)α(cid:107)≤τk

(cid:111)
(cid:110)
(cid:104)θ, v(cid:105) + (cid:10)α, (cid:107)θ(cid:107)g − κ1n(cid:105) + (cid:107)α(cid:107)(cid:104)θ, h(cid:105)
.

We claim that the sequence (ηk)k≥1 does not have a negative accumulation point, where

ηk = min

(cid:104)α, (cid:107)(cid:101)θk(cid:107)g − κ1n(cid:105) + (cid:104)(cid:101)θk, h(cid:105).

α≥0,(cid:107)α(cid:107)=1

Otherwise, we could choose α such that (cid:107)α(cid:107) = τk and then limk (cid:102)Mk = −∞, leading to a contradiction.
Thus, by inspecting (cid:101)θ

, we have

∗

∗

(cid:102)M ≥ (cid:104)(cid:101)θ

, v(cid:105) + (cid:107)α(cid:107) ·

(cid:16)

(cid:104)

α
(cid:107)α(cid:107)

∗

, (cid:107)(cid:101)θ

(cid:107)g − κ1n(cid:105) + (cid:104)(cid:101)θ

(cid:17)

∗

, h(cid:105)

≥ (cid:104)(cid:101)θ

∗

, v(cid:105).

On the other hand, we have (cid:102)Mk ≤ (cid:104)(cid:101)θk, v(cid:105), which implies limk (cid:102)Mk ≤ (cid:104)(cid:101)θ

∗

, v(cid:105) ≤ (cid:102)M .

C κ-margin classiﬁers in the linear signal model:

Proofs of Theorems 4.1, 4.2, 4.4, 4.5 and 4.6

We state below a useful lemma that characterizes the tail probability of a key random variable, whose proof
will be deferred to Appendix C.4.

Lemma 6. Let η0 ∈ (0, 0.1) be any constant. For t > 0, deﬁne

Aρ,t =






(cid:114) 2
π
(cid:114) 2
π
(cid:114) 2
π

1
t
1
t
1
t

(cid:16)

(cid:16)

(cid:16)

−

−

−

exp

exp

exp

t2
2
t2
2
t2
2

− αρt +

(1 − ρ2)α2
2

(cid:17)

, ρ ∈ [η0, 1]

(cid:17)

,

(cid:17)

· aρ,t,

ρ ∈ [−1, −η0]

ρ ∈ (−η0, η0)

where aρ,t is given by

aρ,t =

1
2(cid:112)2π(1 − ρ2)

(cid:90)

R

[ϕ(u − ρt) + 1 − ϕ(u + ρt)] exp

−

(cid:18)

(cid:19)

u2
2(1 − ρ2)

du,

(77)

42

and it satisﬁes

Then, we have

As a consequence,

1 + ˘ot(1)
2

min (cid:8)1, exp(−αη0t)(cid:9) ≤ aρ,t ≤ 1.

lim
t→∞

max
ρ∈[−1,1]

(cid:12)
(cid:12)A−1
(cid:12)

ρ,t

(cid:16)

P

ρY G +

(cid:112)

1 − ρ2 W < −t

(cid:17)

(cid:12)
(cid:12)
(cid:12) = 0.
− 1

lim
κ→−∞

max
ρ∈[−1,1]

(cid:12)
(cid:12)(2|κ|−2Aρ,|κ|)−1 E
(cid:12)

(cid:104)(cid:0)ρY G +

(cid:112)

1 − ρ2 W − κ(cid:1)2

−

(cid:105)

(cid:12)
(cid:12)
(cid:12) = 0.
− 1

C.1 Phase transition lower bound: Proof of Theorem 4.1

Without loss of generality, we can assume that the label yi only depends on the ﬁrst coordinate of xi. To
see this, let Pθ∗ = θ∗(θ∗)(cid:62) ∈ Rd×d be the orthogonal projection onto span{θ∗} and P⊥
θ∗ = Id − Pθ∗ be the
projection matrix onto the orthogonal complement of θ∗. Then we have the following decomposition:

yi(cid:104)xi, θ(cid:105) = yi(cid:104)θ, θ∗(cid:105)(cid:104)xi, θ∗(cid:105) + yi(cid:104)xi, P⊥

θ∗θ(cid:105) = yiρGi + yi

(cid:112)

1 − ρ2(cid:104)xi, θ(cid:105),

where Gi = (cid:104)xi, θ∗(cid:105), ρ = (cid:104)θ, θ∗(cid:105), and θ = P⊥
symmetric distribution, we can write

θ∗θ/(cid:107)P⊥

θ∗θ(cid:107) ∈ Sd−1. Since (yi, Gi)⊥(cid:104)xi, θ(cid:105) and (cid:104)xi, θ(cid:105) has a

yi(cid:104)xi, θ(cid:105) d= ρyiGi +

(cid:112)

1 − ρ2(cid:104)xi, θ(cid:105) d= ρyiGi +

(cid:112)

1 − ρ2(cid:104)zi, w(cid:105),

where w ∈ Sd−2, {(yi, Gi, zi)}1≤i≤n are i.i.d., each have joint distribution:

(yi, Gi)⊥zi, zi ∼ N(0, Id−1), Gi ∼ N(0, 1), P(yi = 1|Gi) = ϕ(Gi) = 1 − P(yi = −1|Gi).

Note that we can actually search for w with (cid:107)w(cid:107)2 ≥ 1, which will result a κ-margin classiﬁer θ lying
outside the unit ball. Since κ < 0, the orthogonal projection of θ onto Sd−1 is a κ-margin classiﬁer as well.
Consequently, we aim to show that under the conditions of Theorem 4.1, for properly chosen ρ ∈ [−1, 1],

(cid:16)

P

lim inf
n→+∞

∃(cid:107)w(cid:107)2 ≥ 1, s.t. ∀1 ≤ i ≤ n, ρyiGi +

(cid:112)

1 − ρ2(cid:104)zi, w(cid:105) ≥ κ

(cid:17)

> 0.

(78)

Since δ < δlb(κ; ϕ), by Deﬁnition 3 we know that (at least) one of the following assumptions holds:

Assumption C.1. There exists ρ ∈ (0, 1) such that

δ < max






(cid:16)

P

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)



(cid:32)

, E



κ0 −

ρY G
(cid:112)1 − ρ2

− W

(cid:33)2

+

−1










.

(79)

Assumption C.2. There exist parameters

ρ ∈ (0, 1), 0 > κ1 > κ2 > κ0 =

κ
(cid:112)1 − ρ2

, c ≥ 0,

such that

(cid:32)

>δsec

ρ,

1
δ

(cid:112)

1 − ρ2κ2,

(cid:112)1 − ρ2
ρ

κ1

(cid:33)−1

(cid:32)

+ P

Y G ≥

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

E

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+



(cid:40)

E

1

Y G <

+

1
c2

(cid:112)1 − ρ2
ρ

(cid:41) (cid:32)

κ1

κ0 −

ρY G
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W



(cid:33)2

 .

+

Lemma 7. Under Assumption C.1, Eq. (78) holds, which further implies Eq. (26).

(80)

(81)

43

The proof of Lemma 7 is deferred to Appendix C.4. Now assume C.2 and denote

y = (yi)1≤i≤n, G = (Gi)1≤i≤n, Z = (zi)(cid:62)

1≤i≤n,

then we have

ρy (cid:12) G +

(cid:112)

1 − ρ2Zw ≥ κ1 ⇐⇒

ρy (cid:12) G
(cid:112)1 − ρ2

+ Zw ≥

κ
(cid:112)1 − ρ2

1

⇐⇒ u + Zw ≥ κ01, where u =

ρy (cid:12) G
(cid:112)1 − ρ2

, κ0 =

κ
(cid:112)1 − ρ2

.

Let κ1, κ2 be as deﬁned in Assumption C.2. We further deﬁne the “good sample” as those satisfying ui ≥ κ1,
i.e.,

SG = {1 ≤ i ≤ n : ui ≥ κ1}, and uG = (ui)i∈SG , ZG = (zi)(cid:62)

i∈SG

.

Similarly, the set of “bad sample” is deﬁned as SB = [n]\SG, and uB = (ui)i∈SB , ZB = (zi)(cid:62)
convenience, we also denote

i∈SB

. For future

pB = P(u < κ1) = P

Y G <

(cid:32)

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

.

A major ingredient of our proof is the “second moment method” applied to the good sample, where a small
fraction of the columns of Z plays a special role. The moment calculation will be based on the randomness
in the remaining majority of columns of Z. According to Eq. (81), there exists a sequence of positive integers
{d0 = d0(n)}n∈N such that

lim
n→+∞

d0
n

(cid:32)

>P

Y G ≥

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

E

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+



(cid:40)

E

1

Y G <

+

1
c2

(cid:112)1 − ρ2
ρ

(cid:41) (cid:32)

κ1

κ0 −

ρY G
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W



(cid:33)2

 ,

+

and

lim
n→+∞

d − 1 − d0
n

(cid:32)

>δsec

ρ,

(cid:112)

1 − ρ2κ2,

(cid:112)1 − ρ2
ρ

κ1

(cid:33)−1

.

With this choice of d0, we partition

Z = (Z1, Z2) , where Z1 ∈ Rn×(d−1−d0), Z2 ∈ Rn×d0 .

(82)

(83)

Then Eq. (78) reduces to proving that with probability bounded away from 0, there exists w1 ∈ Rd−1−d0 ,
w2 ∈ Rd0, satisfying (cid:107)w1(cid:107)2 = 1, (cid:107)w2(cid:107)2 ≤ c, and
uG + ZG
uB + ZB

1 w1 + ZG
1 w1 + ZB

2 w2 ≥κ01,
2 w2 ≥κ01,

(84)

which is achieved by the following two lemmas:
Lemma 8. If (83) holds, then with probability bounded away from 0, there exists w1 ∈ Rd−1−d0, (cid:107)w1(cid:107)2 = 1,
such that

(85)
Lemma 9. Assume (82), for any ﬁxed w1 ∈ Rd−1−d0, (cid:107)w1(cid:107)2 = 1, with high probability there exists some
w2 ∈ Rd0, (cid:107)w2(cid:107)2 ≤ c, such that the following inequalities hold:

1 w1 ≥ κ21.

uG + ZG

ZG
ZB

2 w2 ≥(κ0 − κ2)1,
2 w2 ≥κ01 − uB − ZB

1 w1.

44

Note that in Lemma 9 we can assume that w1 obtained by Lemma 8 is ﬁxed, since w1 is only determined
by uG and ZG
1 , and is independent of other random variables. Hence, the validity of Lemma 9 is preserved
after conditioning on any given w1. Now, combining the results of the above lemmas yields Eq. (84), thus
proving the ﬁrst part of Theorem 4.1.

Lemma 10. For any ε > 0, there exists κ = κ(ε) < 0, such that for all κ < κ and

δ < (1 − ε)

√
π
√
2
2

|κ| log |κ| exp

(cid:18) κ2
2

(cid:19)

+ α|κ|

,

Assumption C.2 is true. Therefore, δ < δlb(κ; ϕ).

Applying Lemma 10 concludes the proof of Theorem 4.1. The proofs of Lemmas 8, 9 and 10 are deferred

to Appendix C.4.

C.2 Phase transition upper bound: Proof of Theorem 4.2

The ﬁrst part of Theorem 4.2 is a direct consequence of Theorem 4.4, and can be deduced by taking
J = [−1, 1] in the proof of Theorem 4.4. Now we focus on the second part. We claim that, if |κ| is
suﬃciently large, then with a proper choice of c (the same for all ρ ∈ [−1, 1]), and

δ > (1 + ε)

√
π
√
2
2

|κ| log |κ| exp

(cid:18) κ2
2

(cid:19)

+ α|κ|

,

the following inequality holds:

(cid:32)

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

sup
ρ∈[−1,1]

(cid:32)

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

− inf
u>0

(cid:26) c
4u

−

δ
c

(cid:27)(cid:33)

log ψκ,ρ(−u)

< 0,

(86)

which implies that for all ρ ∈ [−1, 1],

(cid:32)

inf
c>0

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

(cid:32)

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

− inf
u>0

(cid:26) c
4u

−

δ
c

(cid:27)(cid:33)

log ψκ,ρ(−u)

< 0.

(87)

Hence, δ ≥ δub(κ; ϕ) for all ρ ∈ [−1, 1], leading to δ(cid:48) ≥ δub(κ, ρ; ϕ) as long as

δ > δ(cid:48) > (1 + ε)

√
π
√
2
2

|κ| log |κ| exp

(cid:18) κ2
2

(cid:19)

+ α|κ|

.

As a consequence, we obtain that δ > δub(κ; ϕ). To conclude the proof, it suﬃces to prove the claim (86).
Let ν > 0 be such that (1 + ν)(1 + 3ν)/(1 − ν) < 1 + ε, choose c = |κ|1+ν, we ﬁrst focus on the term

Consider the following two cases:
(1) u ≤ |κ|2+ν, then we have

(cid:26) c
4u

−

δ
c

inf
u>0

(cid:27)

log ψκ,ρ(−u)

.

c
4u

−

δ
c

log ψκ,ρ(−u) ≥

c
4u

≥

1
4|κ|

.

(2) u > |κ|2+ν, then it follows that

c
4u

−

δ
c

log ψκ,ρ(−u) ≥ −

δ
c

log ψκ,ρ(−u) ≥ −

δ
c

log ψκ,ρ

(cid:0)−|κ|2+ν(cid:1) (i)

= (1 + ˘oκ(1))

(cid:0)1 − ψκ,ρ

(cid:0)−|κ|2+ν(cid:1)(cid:1) ,

δ
c

where the approximation (i) is valid, since Lemma 6 implies

0 ≤ 1 − ψκ,ρ

(cid:0)−|κ|2+ν(cid:1) ≤ P

(cid:16)

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

= ˘oκ(1).

45

Now we want to further simplify the right hand side of the above inequality. Note that

0 ≤ P
(cid:20)

=E

exp

(cid:16)

(cid:112)

ρY G +
(cid:18)
−|κ|2+ν (cid:16)

1 − ρ2W ≤ κ

κ − ρY G −

(cid:17)

(cid:112)

− (cid:0)1 − ψκ,ρ
(cid:17)2

1 − ρ2W

(cid:20)

(cid:18)

exp

=E

(cid:20)

+ E

exp

−|κ|2+ν (cid:16)
(cid:18)
−|κ|2+ν (cid:16)

κ − ρY G −

(cid:112)

1 − ρ2W

κ − ρY G −

(cid:112)

1 − ρ2W

(cid:18)

≤P

κ −

1
|κ|1+ν/4

< ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:0)−|κ|2+ν(cid:1)(cid:1)
(cid:19)

(cid:110)

1

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:111)(cid:21)

(cid:19)

(cid:26)

1

κ −

1
|κ|1+ν/4

< ρY G +

(cid:112)

(cid:27)(cid:21)

1 − ρ2W ≤ κ

(cid:19)

(cid:17)2

+

(cid:26)

1

ρY G +

(cid:112)

1 − ρ2W ≤ κ −

(cid:27)(cid:21)

1
|κ|1+ν/4

+ exp

(cid:16)

−|κ|ν/2(cid:17)

P

(cid:18)

ρY G +

(cid:112)

1 − ρ2W ≤ κ −

1
|κ|1+ν/4

(cid:19)

.

+
(cid:17)2

+

(cid:19)

From Lemma 6, we also know that uniformly for all ρ ∈ [−1, 1],

(cid:18)

ρY G +

P

(cid:112)

1 − ρ2W ≤ κ −

(cid:19)

1
|κ|1+ν/4

= (1 + ˘oκ(1))P

(cid:16)

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

,

thus leading to

(cid:16)

P

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

− (cid:0)1 − ψκ,ρ
(cid:16)

(cid:0)−|κ|2+ν(cid:1)(cid:1) = ˘oκ(1) · P

(cid:16)

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

=⇒ 1 − ψκ,ρ

=⇒

c
4u

−

δ
c

(cid:0)−|κ|2+ν(cid:1) = (1 + ˘oκ(1))P
(cid:16)

log ψκ,ρ(−u) ≥ (1 − ν)

P

δ
c

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

when |κ| is large enough. To summarize, we have

(cid:26) c
4u

−

δ
c

inf
u>0

(cid:27)

log ψκ,ρ(−u)

≥ min

(cid:26) 1
4|κ|

, (1 − ν)

(cid:16)

P

δ
c

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)(cid:27)

.

(88)

It will be useful to remember the fact that

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

(cid:32)

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

is an increasing function of (cid:112)1 − ρ2, which we will apply for several times later. To show Eq. (86), we
proceed with the following two parts:
Part 1. Since c = |κ|1+ν, for suﬃciently large |κ| we obtain that

(cid:32)

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

(cid:32)

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

−

1
4|κ|

(cid:33)

sup
ρ∈[−1,1]

≤

c +

1
√
c2 + 4

+

1
c

log

(cid:32)

c +

(cid:33)

√

c2 + 4
2

−

1
4|κ|

≤ (1 + ν)

log c
c

−

1
4|κ|

= (1 + ν)2 log |κ|

|κ|1+ν −

1
4|κ|

< 0.

(89)

Part 2. Also, when |κ| is large, using Lemma 6 implies that uniformly for all ρ < 1 − |κ|−1+ν,

(1 − ν)

(cid:16)

P

δ
c

ρY G +

(cid:112)

1 − ρ2W ≤ κ

(cid:17)

(1 + ε)(1 − ν)
2c

log |κ| exp (cid:0)α|κ|−1+ν|κ|(cid:1)

≥(1 + ˘oκ(1))
≥(1 + ν)2 log |κ|

c

≥

c +

1
√
c2 + 4

+

1
c

log

= (1 + ν)

log c
c
√

(cid:32)

c +

(cid:33)

c2 + 4
2

46

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

≥

+

1
c

(cid:32)

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

.

log

Similarly, if ρ ≥ 1 − |κ|−1+ν, one gets that (cid:112)1 − ρ2 ≤
uniformly for all such ρ and suﬃciently large |κ|:

√

2|κ|−1/2+ν/2, hence the following inequalities hold

+

1
c

log

(cid:32)

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
(cid:32)
√

≤

2|κ|−1/2+ν/2

√

√

≤

2|κ|−1/2+ν/2(1 + ν)

1
2|κ|(1+3ν)/2 + (cid:112)2|κ|1+3ν + 4
2|κ|(1+3ν)/2(cid:1)

log (cid:0)√
√

=

2|κ|(1+3ν)/2

(1 + ν)(1 + 3ν)
2

(1 + ˘oκ(1)) log |κ|
|κ|1+ν

+

√

1
2|κ|(1+3ν)/2

log

(cid:32) √

2|κ|(1+3ν)/2 + (cid:112)2|κ|1+3ν + 4
2

(cid:33)(cid:33)

(i)
<(1 + ˘oκ(1))

(1 + ε)(1 − ν)
2c

log |κ|

(ii)
≤ (1 − ν)

(cid:16)

P

δ
c

ρY G +

(cid:112)

1 − ρ2W ≤ κ

,

(cid:17)

where (i) follows from our assumption (1 + ν)(1 + 3ν)/(1 − ν) < 1 + ε, (ii) follows from Lemma 6. Now we
have proved that
(cid:32)

(cid:33)

(cid:32)

(cid:33)

sup
ρ∈[−1,1]

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

log

1
c

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

− (1 − ν)

ρY G +

1 − ρ2W ≤ κ

< 0,

(cid:112)

(cid:17)

(cid:16)

P

δ
c

combining this with equations (88) and (89) gives Eq. (86) and ﬁnishes proving Theorem 4.2.

C.3 Solution space geometry: Proofs of Theorems 4.4, 4.5 and 4.6

Proof of Theorem 4.4. Let J = [−1, ρmin] ∪ [ρmax, 1], then we know that δ > δub(κ, ρ; ϕ) for all ρ ∈ J.
As in the proof of Theorem 4.1, we may assume that θ∗ = (1, 0, · · · , 0)(cid:62) and write yi(cid:104)xi, θ(cid:105) as ρyiGi +
(cid:112)1 − ρ2(cid:104)zi, w(cid:105), where {(yi, Gi, zi)}1≤i≤n are i.i.d., each have joint distribution:

(yi, Gi)⊥zi, zi ∼ N(0, Id−1), Gi ∼ N(0, 1), P(yi = 1|Gi) = ϕ(Gi) = 1 − P(yi = −1|Gi).

Now for any ﬁxed ρ ∈ [−1, 1], deﬁne

ξn,κ,ρ = min

(cid:107)w(cid:107)2=1

max
(cid:107)λ(cid:107)2=1,λ≥0

λ(cid:62) (cid:16)

1
√
d

κ1 − ρy (cid:12) G −

(cid:112)

1 − ρ2Zw

(cid:17)

,

then we know that

∃θ ∈ Sd−1, (cid:104)θ, θ∗(cid:105) ∈ J, s.t. ∀1 ≤ i ≤ n, yi (cid:104)xi, θ(cid:105) ≥ κ ⇐⇒ ∃ρ ∈ J, s.t. ξn,κ,ρ ≤ 0.

Hence, it suﬃces to prove that

lim
n→+∞

P

(cid:18)

min
ρ∈J

(cid:19)

ξn,κ,ρ ≤ 0

= 0.

To this end, we proceed with the following two steps:
Step 1. Control the probability P(ξn,κ,ρ ≤ η) for η > 0.
The argument in this part mainly uses the modiﬁed Gordon’s inequality, i.e., Eq. (60), and is almost identical
to what appears in the proof of Theorem 3.2, so we will omit some technical details here. Set ψ(x) =
− exp(−cnx) for some c > 0 (may depend on κ and ρ), and combining Markov’s inequality with Eq. (61)
yields

P (ξn,κ,ρ ≤ η) ≤ exp (cnη) E [exp (−cnξn,κ,ρ)]

(cid:34)

(cid:32)

≤ exp (cnη) E

exp

−

cn(cid:112)1 − ρ2
√
d

z

(cid:33)(cid:35)−1

(cid:34)

(cid:32)

E

exp

cn(cid:112)1 − ρ2
√
d

(cid:107)g(cid:107)2

(cid:33)(cid:35)

47

(cid:20)

(cid:18)

exp

−

× E

cn
√
d

max
(cid:107)λ(cid:107)2=1,λ≥0

λ(cid:62) (cid:16)

κ1 − ρy (cid:12) G −

(cid:112)

1 − ρ2h

(cid:17)(cid:19)(cid:21)

,

where z ∼ N(0, 1), g ∼ N(0, Id−1), h ∼ N(0, In) are independent, and further independent of y (cid:12) G.

Now we calculate the asymptotics of the right hand side in the above inequality. First, similarly as before,

we apply Lemma 2 to obtain that

lim
n→+∞

1
n

(cid:34)

(cid:32)

log E

exp

cn(cid:112)1 − ρ2
√
d

(cid:107)g(cid:107)2

(cid:33)(cid:35)

(cid:32)

=

1
δ

c2δ2(1 − ρ2) + cδ(cid:112)1 − ρ2(cid:112)c2δ2(1 − ρ2) + 4
4

− log

(cid:32) (cid:112)c2δ2(1 − ρ2) + 4 − cδ(cid:112)1 − ρ2
2

(cid:33)(cid:33)

.

Next, with a slight modiﬁcation on the argument in the proof of Theorem 3.2 (conditioning on y (cid:12) G, since
it’s independent of h), we can show that

lim
n→+∞

1
n

(cid:20)

(cid:18)

exp

−

log E

cn
√
d

max
(cid:107)λ(cid:107)2=1,λ≥0

λ(cid:62) (cid:16)

κ1 − ρy (cid:12) G −

(cid:112)

1 − ρ2h

(cid:17)(cid:19)(cid:21)

= − inf
u>0

(cid:26) c2δ
4u

− log ψκ,ρ(−u)

,

(cid:27)

where for u > 0,

ψκ,ρ(−u) = E

(cid:20)

(cid:18)

exp

−u

(cid:16)

κ − ρY G −

(cid:112)

1 − ρ2W

(cid:19)(cid:21)

(cid:17)2

+

.

Therefore, after making a change of variable (c (cid:55)→ c/δ), it ﬁnally follows that

lim sup
n→+∞

−

log

1
δ
(cid:32)

=

c
δ

η +

−

cη
δ

log P (ξn,κ,ρ ≤ η) ≤

1
n
(cid:32) (cid:112)c2(1 − ρ2) + 4 − c(cid:112)1 − ρ2
2
(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

c2(1 − ρ2)
2δ
(cid:33)

− inf
u>0

+

1
δ
(cid:26) c2
4δu

c2(1 − ρ2) + c(cid:112)1 − ρ2(cid:112)c2(1 − ρ2) + 4
4

− log ψκ,ρ(−u)

(cid:27)

(cid:32)

1
c

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

(cid:33)

− inf
u>0

(cid:26) c
4u

−

δ
c

(cid:27)(cid:33)

log ψκ,ρ(−u)

.

Now we deﬁne for η > 0 and ρ ∈ [−1, 1]:

fδ(η, ρ) = inf
c>0

(cid:32)

η +

(cid:40)

c
δ

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

− inf
u>0

(cid:26) c
4u

−

δ
c

log ψκ,ρ(−u)

(cid:27)(cid:33)(cid:41)

,

then the above argument implies that

lim sup
n→+∞

1
n

log P (ξn,κ,ρ ≤ η) ≤ fδ(η, ρ).

Moreover, by deﬁnition of δub(κ, ρ; ϕ) we know that

δ > δub(κ, ρ; ϕ) =⇒ ∃ηρ > 0, fδ(ηρ, ρ) < 0,

therefore ∀ρ ∈ J, there exists a ηρ > 0 such that

lim sup
n→+∞

1
n

log P (ξn,κ,ρ ≤ ηρ) ≤ fδ(ηρ, ρ) < 0.

We will show that under the conditions of Theorem 4.4,

∃η > 0, s.t. sup
ρ∈J

fδ(η, ρ) < 0.

(90)

In order to prove (90), we introduce the following useful property:

Lemma 11. For any ﬁxed η > 0, fδ(η, ρ) is upper semicontinuous in ρ ∈ [−1, 1].

48

The proof of Lemma 11 is provided in Appendix C.4. Assuming its correctness, we are now in position
to prove Eq. (90) by contradiction. If for any η > 0, supρ∈J fδ(η, ρ) ≥ 0, then according to the compactness
of J and Lemma 11, there exists ρη ∈ J such that fδ(η, ρη) ≥ 0. Hence, we can ﬁnd a sequence of (ηn, ρn)
satisfying ηn → 0+, ρn ∈ J, and fδ(ηn, ρn) ≥ 0. Now again since J is compact, {ρn} has a limit point ρ ∈ J
(WLOG we may assume ρn → ρ, otherwise just recast the convergent subsequence as {ρn}), and we can ﬁnd
a ηρ > 0 such that fδ(ηρ, ρ) < 0, thus leading to

0 > fδ(ηρ, ρ)

(i)
≥ lim sup
n→+∞

fδ(ηρ, ρn)

(ii)
≥ lim sup
n→+∞

fδ(ηn, ρn) ≥ 0,

where (i) is due to Lemma 11, (ii) is due to ηn → 0+ and the fact that fδ(η, ρ) is an increasing function of
η for any ﬁxed ρ ∈ [−1, 1]. Therefore, a contradiction occurs and Eq. (90) follows.
Step 2. Covering argument.
By deﬁnition of ξn,κ,ρ, one has for any ρ1, ρ2 ∈ [−1, 1],

|ξn,κ,ρ1 − ξn,κ,ρ2| ≤ max
max
(cid:107)w(cid:107)2=1
(cid:107)λ(cid:107)2=1,λ≥0
1
√
d

|ρ2 − ρ1| (cid:107)y (cid:12) G(cid:107)2 +

1
√
d

1 − ρ2

(cid:113)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
d
(cid:113)

1 −

λ(cid:62)(ρ2 − ρ1)y (cid:12) G −

1 − ρ2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)Z(cid:107)op ,

(cid:18)(cid:113)

1
√
d

λ(cid:62)

1 − ρ2

1 −

(cid:113)

1 − ρ2
2

(cid:19)

Zw

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last line just follows from the deﬁnition of operator norm. According to Cramer’s theorem and
Theorem 4.4.5 of [Ver18], we know there exist constants C0, C1, C2 > 0, such that (note n/d → δ)

(cid:16)

(cid:110)

P

max

(cid:107)y (cid:12) G(cid:107)2 > C0

√

(cid:17)

d

, P

(cid:16)

(cid:107)Z(cid:107)op > C0

√

(cid:17)(cid:111)

d

< C1 exp(−C2n),

Now we choose a ﬁnite covering {ρ1, · · · , ρN } of J, satisfying that

∀ρ ∈ J, ∃1 ≤ i ≤ N, |ρ − ρi| +

(cid:112)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 − ρ2 −

(cid:113)

1 − ρ2
i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

<

η
C0

,

hence if ξn,κ,ρ ≤ 0, and max{(cid:107)y (cid:12) G(cid:107)2 , (cid:107)Z(cid:107)op} ≤ C0

As a consequence, we deduce that

√

d, then we must have ξn,κ,ρi ≤ η.

(cid:18)

P

(cid:19)

min
ρ∈J

ξn,κ,ρ ≤ 0

(cid:16)

≤P

max

(cid:110)

(cid:107)y (cid:12) G(cid:107)2 , (cid:107)Z(cid:107)op

(cid:111)

√

(cid:17)

d

+ P

> C0

(cid:18)

(cid:19)

min
1≤i≤N

ξn,κ,ρi ≤ η

≤2C1 exp(−C2n) + N max
1≤i≤N

P (ξn,κ,ρi ≤ η) → 0,

where max1≤i≤N P (ξn,κ,ρi ≤ η) → 0 follows from Eq. (90). This concludes the proof.

Proof of Theorem 4.5. To begin with, recall the deﬁnition of δub(κ; ϕ), which is the inﬁmum of all δ such
that ∃c > 0, such that

1
√
c2 + 4

+

1
c

c +

c +

log

√

c2 + 4
2

< inf
u>0

(cid:26)

c
4u(1 − ρ2)

−

δ
c

(cid:27)

log ψκ,ρ(−u)

.

Now we rescale by

it follows that

1 − ρ2 =

a
κ2 , u = tκ2, δ =

b
P(ρY G + (cid:112)1 − ρ2W < κ)

,

c
4u(1 − ρ2)

−

δ
c

(i)
=

c
4ta

+ (1 + ˘oκ(1))

b
c

log ψκ,ρ(−u) =

−

c
b
4ta
c
1 − ψκ,ρ(−tκ2)
P(ρY G + (cid:112)1 − ρ2W < κ)

,

log ψκ,ρ(−tκ2)
P(ρY G + (cid:112)1 − ρ2W < κ)

49

where (i) is due to log ψκ,ρ(−tκ2) = −(1 + ˘oκ(1))(1 − ψκ,ρ(−tκ2)), since 0 ≤ 1 − ψκ,ρ(−tκ2) ≤ P(ρY G +
(cid:112)1 − ρ2W < κ) = ˘oκ(1). Using integration by parts, we obtain that

1 − ψκ,ρ

(cid:0)−tκ2(cid:1) = 1 − E

(cid:20)

exp

(cid:18)
−tκ2 (cid:16)

κ − ρY G −

(cid:112)

1 − ρ2W

(cid:19)(cid:21)

(cid:17)2

+

(cid:18)

=1 −

1 −

(cid:90)

R

2tκ2(κ − x)+ exp (cid:0)−tκ2(κ − x)2

+

(cid:16)

(cid:1) P

ρY G +

(cid:112)

1 − ρ2W < x

(cid:19)

(cid:17)

dx

(cid:90) +∞

=

0

2tκ2x exp (cid:0)−tκ2x2(cid:1) P

(cid:16)

ρY G +

(cid:112)

1 − ρ2W < κ − x

(cid:17)

dx,

leading to

1 − ψκ,ρ(−tκ2)
P(ρY G + (cid:112)1 − ρ2W < κ)

=

(cid:90) +∞

0

2tκ2x exp (cid:0)−tκ2x2(cid:1)

(cid:16)

P

ρY G + (cid:112)1 − ρ2W < κ − x
(cid:16)
ρY G + (cid:112)1 − ρ2W < κ

P

(cid:17) dx

(cid:17)

(i)
=(1 + ˘oκ(1))

(ii)
= (1 + ˘oκ(1))

(cid:90) +∞

0
(cid:90) +∞

0

2tκ2x exp (cid:0)−tκ2x2(cid:1)

2ts exp (cid:0)−ts2(cid:1) κ2
κ2 + s

|κ|
|κ − x|
(cid:18)

exp

−

(cid:18)

exp

−

x2
2

(cid:19)

+ κx − αρ+x

dx

s2
2κ2 − s −

αρ+s
|κ|

(cid:19)

ds,

where ρ+ = max(ρ, 0), (i) follows from Lemma 6, and (ii) follows from the change of variable s = |κ|x. Now
we claim that uniformly for all t > 0 and ρ ∈ [−1, 1],

(cid:90) +∞

0

2ts exp (cid:0)−ts2(cid:1) κ2
κ2 + s

(cid:18)

exp

−

s2
2κ2 − s −

αρ+s
|κ|

(cid:19)

ds = (1 + ˘oκ(1))

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds,

which is equivalent to

lim
κ→−∞

sup
ρ∈[−1,1],t>0

(cid:82) +∞
0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2ts exp (cid:0)−ts2(cid:1) κ2
(cid:82) +∞
0

κ2+s exp

(cid:16)

− s2

2κ2 − s − αρ+s

|κ|

2ts exp (−ts2 − s) ds

(cid:17)

ds

(cid:12)
(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)
(cid:12)

= 0.

In what follows we prove the above claim. Consider the following two cases:

(1) 0 < t ≤ 1. We obtain that

2ts exp (cid:0)−ts2(cid:1) κ2
κ2 + s

(cid:18)

exp

−

2ts exp (cid:0)−ts2 − s(cid:1)

(cid:18) κ2

κ2 + s
(cid:18)

exp

(cid:90) +∞

0
(cid:90) +∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

0
(cid:90) +∞

0

≤2t

s exp (−s)

(i)
=2t˘oκ(1) = 2t˘oκ(1)

κ2
κ2 + s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) +∞

−

exp

s2
2κ2 −
s exp (cid:0)−s2 − s(cid:1) ds ≤ ˘oκ(1)

0

0

(cid:90) +∞

ds −

0
(cid:19)

(cid:12)
(cid:12)
ds
(cid:12)
(cid:12)

− 1

s2
2κ2 − s −
(cid:18)
s2
2κ2 −
(cid:19)
αρ+s
|κ|

−

(cid:19)

(cid:19)

αρ+s
|κ|
αρ+s
|κ|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) +∞

− 1

ds

2ts exp (cid:0)−ts2 − s(cid:1) ds,

(cid:12)
(cid:12)
2ts exp (cid:0)−ts2 − s(cid:1) ds
(cid:12)
(cid:12)

where (i) is a consequence of the Dominated Convergence Theorem.

(2) t > 1. Then making the change of variable x =

ts gives

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) +∞

0
(cid:90) +∞

0
(cid:90) +∞

0

=

≤

2ts exp (cid:0)−ts2(cid:1) κ2
κ2 + s
√
tκ2
tκ2 + x

2x exp (cid:0)−x2(cid:1)

√

exp

(cid:18)

−

(cid:18)

s2
2κ2 − s −
x2
2tκ2 −
(cid:18)

−

exp

exp
(cid:19) (cid:18) √
√

tκ2
tκ2 + x

(cid:18)

−x2 −

2x exp

x
√
t

(cid:19)

αρ+s
|κ|

ds −

(cid:90) +∞

0

(cid:12)
(cid:12)
2ts exp (cid:0)−ts2 − s(cid:1) ds
(cid:12)
(cid:12)

(cid:90) +∞

(cid:18)

−x2 −

2x exp

dx −

x
√
t

(cid:19)

dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−

x
√
t
x2
2tκ2 −

αρ+x
√
t|κ|
αρ+x
√
t|κ|

−

0
(cid:19)

(cid:19)

− 1

dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

50

(cid:90) +∞

≤

0

2x exp (cid:0)−x2(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

√

tκ2
tκ2 + x

(cid:18)

exp

−

x2
2tκ2 −

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dx

(cid:19)

αρ+x
√
t|κ|
(cid:90) +∞

(i)
= ˘oκ(1) = ˘oκ(1)

(cid:90) +∞

0

2x exp (cid:0)−x2 − x(cid:1) dx ≤ ˘oκ(1)

(cid:18)

−x2 −

2x exp

(cid:19)

x
√
t

dx

0

=˘oκ(1)

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds,

where (i) again results from the Dominated Convergence Theorem, and the fact that
Combining Case 1 and 2 immediately yields our claim, hence we deduce that

√

tκ → −∞.

c
4u(1 − ρ2)

−

δ
c

log ψκ,ρ(−u) =

c
4ta

+ (1 + ˘oκ(1))

b
c

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds.

Now according to the deﬁnition of D(a) in Theorem 4.5, we ﬁnally obtain that

δub(κ; ϕ) =

(1 + ˘oκ(1))D(a)
P(ρY G + (cid:112)1 − ρ2W < κ)

=

(1 + ˘oκ(1))D(κ2(1 − ρ2))
P(ρY G + (cid:112)1 − ρ2W < κ)

.

This concludes the proof.

Proof of Theorem 4.6. To begin with, we collect some important properties of D(a) in the following
proposition (proved in Appendix C.4), which allows us to determine the asymptotic behavior of δub(κ; ϕ)
explicitly within some regime.

Proposition C.1.

(a) D(a) is asymptotic to (log a)/2 as a → +∞. Consequently, when κ2(1 − ρ2) → +∞ we have

δub(κ; ϕ) = (1 + ˘oκ(1))

log(|κ|(cid:112)1 − ρ2)
P(ρY G + (cid:112)1 − ρ2W < κ)

.

(b) D(a) = a/2 when a ≤ 2. Therefore, if 1 − ρ2 ≤ 2/κ2, then we get that

δub(κ; ϕ) = (1 + ˘oκ(1))

κ2(1 − ρ2)
2P(ρY G + (cid:112)1 − ρ2W < κ)

.

Now we turn to the proof of Theorem 4.6. Note that D−1 is an increasing function, we ﬁrst show that

for any ε > 0, there exists a κ = κ(ε) < 0, such that as long as κ < κ, with high probability we have

ρmax ≤ 1 −

1
2κ2 D−1 ((1 − ε)δP(Y G < κ)) .

(91)

Assume that

ρ ≥ 1 −

1
2κ2 D−1 ((1 − ε)δP(Y G < κ)) ,

then we have the following estimate:

κ2(1 − ρ2) ≤ 2κ2(1 − ρ) ≤ D−1 ((1 − ε)δP(Y G < κ)) .

Using the conclusion of Theorem 4.5, it follows that

δub(κ, ρ; ϕ) =

(1 + ˘oκ(1))D(κ2(1 − ρ2))
P(ρY G + (cid:112)1 − ρ2W < κ)
(cid:18)

(i)
=(1 + ˘oκ(1))(1 − ε)δ exp

≤

(1 + ˘oκ(1))(1 − ε)δP(Y G < κ)
P(ρY G + (cid:112)1 − ρ2W < κ)

α(1 − ρ)κ −

(cid:19)

α2(1 − ρ2)
2

51

(cid:16)

≤

1 −

(cid:17)

ε
2

δ < δ

if |κ| is large, where in (i) we use Lemma 6. Applying Theorem 4.4 then yields Eq. (91).

To show Eq. (40), we ﬁrst notice that if δ < δlin(κ; ϕ), then using the asymptotics of δlin(κ; ϕ) from

Theorem 4.3 implies

(1 + ˘oκ(1))δP(Y G < κ) ≤ 1 + ˘oκ(1),

which further implies that with high probability,

ρmax ≤1 −

(i)
=1 −

1
2κ2 D−1 ((1 + ˘oκ(1))δP(Y G < κ))
(1 + ˘oκ(1))δP(Y G < κ)
|κ|2

(ii)
= 1 −

δ
2

(1 + ˘oκ(1))E

(cid:104)

(κ − Y G)2
+

(cid:105)

,

where (i) and (ii) both follow from Lemma 6. This concludes the proof.

C.4 Additional proofs

Proof of Lemma 6. Recall the ˘o notation we introduced before: for a function s(ρ, r), we write s(ρ, t) =
˘ot(1) to mean

For convenience, sometimes in this proof we may write ot(1) instead of ˘ot(1).

Step 1. First, we consider the case ρ ≥ η0. We calculate

lim
t→∞

max
ρ∈[−1,1]

|s(ρ, t)| = 0.

P

(cid:90)

=

(cid:16)

ρY G < −t −

(cid:112)

1 − ρ2 W

(cid:17)

(cid:16)

P

ρY G < −t −

(cid:112)

1 − ρ2 x

R

= ot(1) · exp

(cid:16)

−

t2(1 + ε0)2
2

(cid:17)

+

exp (cid:0) −

x2
2

(cid:1) dx

(cid:17) 1
√

2π

(cid:90)

(cid:16)

P

ρY G < −t −

(cid:112)

1 − ρ2 x

(cid:17) 1
√

2π

(cid:16)

−

exp

(cid:17)

x2
2

dx

|x|≤(1+ε0)t

where ε0 := ε0(η0) > 0 is suﬃciently small such that (cid:112)1 − η2
4 , and where we used the
Gaussian tail probability (Lemma 12) bound in the last expression. We denote (cid:101)t = t + (cid:112)1 − ρ2 x, which
satisﬁes (cid:101)t ≥ t − (cid:112)1 − ρ2(1 + ε0)t ≥ η2

0 (1 + ε0) < 1 − η2

0t/4. We observe that

0

P (cid:0)ρY G < −(cid:101)t(cid:1) =

+

=

(i)
=

(cid:17)

z2
2
z2
(cid:17)
2

dz

dz

(cid:16)

exp

1
√
2π

−

(cid:16)

exp

−

(cid:16)

(cid:90) −ρ−1

(cid:101)t

P(Y = 1|G = z)

−∞
(cid:90) ∞

ρ−1(cid:101)t
(cid:90) ∞

P(Y = −1|G = z)

1
√
2π
(cid:2)ϕ(−z) + 1 − ϕ(z)(cid:3) 1
√
2π
z2
2

(cid:90) ∞

exp

−

(cid:16)

ρ−1(cid:101)t
2(1 + ot(1))
2π

√

ρ−1(cid:101)t

exp

−

(cid:17)

z2
2

dz

− αz

(cid:17)

dz

(ii)
=

2(1 + ot(1))ρ
2π |(cid:101)t|

√

exp

(cid:16)

− (cid:101)t2

2ρ2 −

(cid:17)
α(cid:101)t
ρ

.

Here (i) is because by the tail probability assumption we have

(cid:12)1 − ϕ(z) + ϕ(−z) − 2e−αz(cid:12)
(cid:12)

(cid:12) ≤ βte−αz,

for z ≥ t

52

where βt = ot(1); (ii) is due to the Gaussian tail probability inequality (Lemma 12). Thus, we obtain

(cid:16)

P

ρY G < −t −

(cid:112)

1 − ρ2 W

(cid:17)

=

(1 + ˘oκ(1))ρ
π

(cid:90)

|x|≤(1+ε0)t

exp

(cid:16)

− (cid:101)t2
2ρ2 −

α(cid:101)t
ρ

−

x2
2

(cid:17)

1
|(cid:101)t|

dx+ot(1)·exp

(cid:16)

−

t2(1 + ε0)2
2

(cid:17)

.

We rearrange the exponent

− (cid:101)t2
2ρ2 −

α(cid:101)t
ρ

−

x2
2

= −

= −

= −

x2
2ρ2 −
(cid:16)
1
2ρ2
1
2ρ2

(cid:16)

(cid:112)

(cid:112)

α(cid:112)1 − ρ2
ρ

x −

t(cid:112)1 − ρ2
ρ2

x −

x +

1 − ρ2 t + αρ

1 − ρ2

(cid:112)

(cid:112)

t2
2ρ2 −
1
2ρ2
t2
2

+

−

(cid:17)2

(cid:17)2

x +

1 − ρ2 κ + αρ

1 − ρ2

− αρt +

αt
ρ
(cid:16)(cid:112)

1 − ρ2 t + αρ

(cid:112)

1 − ρ2

(cid:17)2

−

αt
ρ

−

t2
2ρ2

(1 − ρ2)α2
2

.

Note that (cid:112)1 − ρ2 t + αρ(cid:112)1 − ρ2 ≤ t when t is suﬃciently large, so denoting b = (cid:112)1 − ρ2 t + αρ(cid:112)1 − ρ2,
we have
(cid:90)

(cid:20)

(cid:21)

(cid:90)

(i)
=

I

(cid:90)

=

Iin
(cid:90)

|x|≤(1+ε0)t

−

exp

1
|t + (cid:112)1 − ρ2 x|
1
|ρ2t − αρ(1 − ρ2) + u(cid:112)1 − ρ2|
1
|ρ2t − αρ(1 − ρ2) + u(cid:112)1 − ρ2|
(cid:19)
u2
2ρ2

1
|ρ2t − ot(1) · ρ2t|
(cid:18)

exp

−

(cid:18)

(cid:90)

1
2ρ2 (x + b)2
(cid:18)
(cid:19)

exp

−

(cid:18)

u2
2ρ2
u2
2ρ2

exp

−

du + ot(1) ·

dx

du

(cid:19)

(cid:90)

du +

(cid:90)

Iout
(cid:18)

exp

exp

−

(cid:19)

u2
2ρ2

(cid:18)

du + ot

exp

(cid:16)

−

Iout
(cid:17)(cid:19)

t4/3
2ρ2

(ii)
=

(iii)
=

(iv)
=

Iin
1 + ot(1)
ρ2t

Iin
√

(1 + ot(1))

2π

,

ρt

1
|ρ2t − αρ(1 − ρ2) + u(cid:112)1 − ρ2|
u2
2ρ2

du

(cid:19)

−

(cid:18)

exp

−

(cid:19)

u2
2ρ2

du

where in (i) we used change of variable u = x + b, in (ii) we used

ρ2t − αρ(1 − ρ2) + u

(cid:112)

1 − ρ2 ≥ t − (1 − ε0)

(cid:112)

1 − ρ2 t ≥ η2

0t/4,

∀ u ∈ I, ρ ≥ η0

and in (iii) and (iv) we used the Gaussian tail probability (Lemma 12). Here we have denoted by the interval

I = [−(1 + ε0)t + b, (1 + ε0)t + b] ,

Iin =

(cid:104)

−t2/3, t2/3(cid:105)

,

Iout = I(cid:15) (cid:104)

−t2/3, t2/3(cid:105)

.

This leads to

(cid:16)

P

ρY G < −t −

(cid:112)

1 − ρ2 W

(cid:17)

=

(1 + ot(1))ρ
π

(cid:18)

exp

−

t2
2

− αρt +

(1 − ρ2)α2
2

(cid:19)

·

√

2π
ρt

,

which proves the ﬁrst part of the tail probability.

Step 2. Now we consider the case ρ ≤ −η0. Diﬀerent from before, in the calculation of P(ρY G < −(cid:101)t)

we will ﬂip the inequality direction. To be speciﬁc,

(cid:90) ∞

P(Y = 1|G = z)

P(Y = −1|G = z)

P (cid:0)ρY G < −(cid:101)t(cid:1) =

+

(i)
=

−ρ−1(cid:101)t
(cid:101)t

(cid:90) ρ−1

−∞
2(1 + ot(1))
2π

√

1
√
2π

(cid:16)

−

exp

(cid:17)

z2
2

dz

1
√
2π

(cid:16)

−

exp

(cid:17)

z2
2

dz

(cid:90) ∞

|ρ−1(cid:101)t|

(cid:16)

−

exp

(cid:17)

z2
2

dz

53

=

2(1 + ot(1))ρ
2π |(cid:101)t|

√

exp

(cid:16)

(cid:17)

− (cid:101)t2
2ρ2

where (i) is because by the tail assumption

ϕ(z) + 1 − ϕ(−z) = 2 − ot(1) · 2e−αz = 2 − ot(1),

∀ z ≥ |ρ−1

(cid:101)t|

Then, following a similar argument as in Step 1 (essentially replacing α by 0), we get

(cid:16)

P

ρY G < −t −

(cid:112)

1 − ρ2 W

(cid:17)

=

(1 + ot(1))ρ
π

(cid:18)

exp

−

(cid:19)

·

t2
2

√

2π
ρt

.

(92)

This proves the second part of the tail probability.

Step 3: Finally, let us consider the case |ρ| < η0. We use a diﬀerent way to do the conditional probability

calculation.

(cid:16)(cid:112)

1 − ρ2 W < −t − ρY G

(cid:17)

(cid:16)(cid:112)

P

(cid:16)(cid:112)

P

P

(cid:90)

R

(cid:90)

R

(cid:90)

R

=

=

+

1 − ρ2 W < −t − ρY G (cid:12)

(cid:12) G = x

(cid:17) 1
√

(cid:16)

−

exp

(cid:17)

x2
2

dx

1 − ρ2 W < −t − ρx

(cid:17)

ϕ(x)

1
√
2π

2π
(cid:16)

exp

−

(cid:16)(cid:112)

P

1 − ρ2 W < −t + ρx

(cid:17) (cid:2)1 − ϕ(x)(cid:3) 1
√
2π

exp

x2
2
(cid:16)

(cid:17)

dx

−

(cid:17)

x2
2

dx

(93)

(94)

We notice that

(cid:90)

P

|x|>1.5t

(cid:16)(cid:112)

1 − ρ2 W < −t − ρx

(cid:17)

ϕ(x)

1
√
2π

(cid:16)

−

exp

(cid:17)

x2
2

(cid:90)

dx <

|x|>1.5t

(cid:16)

−

exp

(cid:17)

x2
2

dx

which is at most ot(exp(−2.25t2/2)); and a similar upper bound holds if we replace −t − ρx by −t + ρx. If
|x| ≤ 1.5t, then −t − ρx < −0.5t so we use the Gaussian tail asymptotics (Lemma 12) to get

(cid:16)(cid:112)

P

1 − ρ2 W < −t − ρx

(cid:17)

=

(1 + ot(1))(cid:112)1 − ρ2
2π | − t − ρx|

√

(cid:16)

−

exp

(t + ρx)2
2(1 − ρ2)

(cid:17)

and thus

(cid:90)

(cid:16)(cid:112)

P

1 − ρ2 W < −t − ρx

(cid:17)

ϕ(x)

1
2π

(cid:16)

−

exp

(cid:17)

x2
2

dx

|x|≤1.5t

(1 + ot(1))(cid:112)1 − ρ2
2π
(1 + ot(1))(cid:112)1 − ρ2
2π
(1 + ot(1))(cid:112)1 − ρ2
2π

=

=

=

(cid:90)

|x|≤1.5t

1
| − t − ρx|

(cid:18)

ϕ(x) exp

−

(t + ρx)2
2(1 − ρ2)

−

(cid:19)

x2
2

dx

exp

exp

(cid:16)

(cid:16)

−

−

t2
2

t2
2

(cid:17) (cid:90)

|x|≤1.5t

1
|t + ρx|

(cid:18)

ϕ(x) exp

−

(cid:19)

dx

(x + ρt)2
2(1 − ρ2)
(cid:18)

(cid:17) (cid:90)

u∈J

1
(1 − ρ2)t + ρu

ϕ(u − ρt) exp

−

u2
2(1 − ρ2)

(cid:19)

du

where we used change of variable u = x + ρt in the last equality, and where we deﬁne the intervals

J = [−1.5t + ρt, 1.5t + ρt],

Jin =

(cid:104)

−t2/3, t2/3(cid:105)

,

Jout = J(cid:15) (cid:104)

−t2/3, t2/3(cid:105)

.

Note that (1 − ρ2)t + ρu ≥ t/2, so using the Gaussian tail probability inequality (Lemma 12), we have

(cid:90)

Jout

1
(1 − ρ2)t + ρu

(cid:18)

ϕ(u − ρt) exp

−

(cid:19)

u2
2(1 − ρ2)

(cid:18)

du = ot

exp

(cid:16)

−

(cid:17)(cid:19)

,

t4/3
2

54

(cid:90)

Jout

(cid:18)

ϕ(u − ρt) exp

−

(cid:19)

u2
2(1 − ρ2)

(cid:18)

du = ot

exp

(cid:16)

−

(cid:17)(cid:19)

.

t4/3
2

Thus we obtain

(cid:90)

P

(cid:16)(cid:112)

1 − ρ2 W < −t − ρx

(cid:17)

ϕ(x)

1
2π

(cid:16)

−

exp

(cid:17)

x2
2

dx

|x|≤1.5t
1 + ot(1)
2π(cid:112)1 − ρ2 t
1 + ot(1)
2π(cid:112)1 − ρ2 t

=

=

exp

exp

(cid:16)

(cid:16)

−

−

t2
2

t2
2

(cid:17) (cid:90)

Jin

ϕ(u − ρt) exp

(cid:16)

−

u2
2(1 − ρ2)

(cid:17)

(cid:18)

du + ot

exp

(cid:16)

−

t2 + t4/3
2

(cid:17)(cid:19)

(cid:17) (cid:90)

R

ϕ(u − ρt) exp

(cid:16)

−

u2
2(1 − ρ2)

(cid:17)

(cid:18)

du + ot

exp

(cid:16)

−

t2 + t4/3
2

(cid:17)(cid:19)

A similar term is obtained for (94), and thus we have

(cid:16)(cid:112)

1 − ρ2 W < −t − ρY G

P

(cid:17)

=

1 + ot(1)
2π(cid:112)1 − ρ2 t

(cid:16)

−

exp

(cid:18)

+ ot

exp

(cid:16)

−

t2 + t4/3
2

We claim that

t2
2
(cid:17)(cid:19)

(cid:17) (cid:90)

R

.

(cid:104)
ϕ(u − ρt) + 1 − ϕ(u + ρt)

(cid:105)

(cid:16)

−

exp

u2
2(1 − ρ2)

(cid:17)

du

(95)

(cid:110)
1, e−αη0t(cid:111)(cid:112)2π(1 − ρ2) ≤

min

(cid:90)

R

(cid:104)

(cid:105)
ϕ(u − ρt) + 1 − ϕ(u + ρt)

(cid:16)

−

exp

u2
2(1 − ρ2)

(cid:17)

du ≤ 2(cid:112)2π(1 − ρ2). (96)

In fact, it is obvious that ϕ(u − ρt) + 1 − ϕ(u + ρt) ≤ 2 so the second inequality follows. To see why the ﬁrst
inequality holds, we notice that if ρ ≤ 0, by monotonicity of ϕ we must have ϕ(u − ρt) + 1 − ϕ(u + ρt) ≥ 1.
And if ρ > 0, we notice

(cid:90)

(cid:104)

(cid:105)
ϕ(u − ρt) + 1 − ϕ(u + ρt)

(cid:16)

−

exp

u2
2(1 − ρ2)

(cid:17)

du

R
(cid:90) ∞

(cid:104)

=

≥

ϕ(u − ρt) + 1 − ϕ(u + ρt) + ϕ(−u − ρt) + 1 − ϕ(−u + ρt)

0
(cid:90) ∞

0

(cid:104)

(cid:105)
ϕ(u − ρt) + 1 − ϕ(−u + ρt)

(cid:16)

−

exp

u2
2(1 − ρ2)

(cid:17)

du

(cid:105)

(cid:16)

−

exp

u2
2(1 − ρ2)

(cid:17)

du

and by monotonicity,

ϕ(u − ρt) + 1 − ϕ(−u + ρt) ≥ ϕ(−η0t) + 1 − ϕ(η0t) = (1 + ot(1)) · 2e−η0αt.

Combining (95) and (96) yields the third part of the tail probability.

Step 4: We prove the “as a consequence” part. Using the identity E[X 2

tP(X > t) dt, we have

+] = 2 (cid:82) ∞

0

(cid:104)(cid:0)ρY G +
E

(cid:112)

1 − ρ2 W − κ(cid:1)2

−

(cid:90) ∞

(cid:105)

= 2

0

tP(cid:0)ρY G +

(cid:112)

1 − ρ2 W < κ − t(cid:1) dt = (1 + ˘oκ(1)) · 2

(cid:90) ∞

0

tAρ,|κ−t| dt.

For ρ ≥ η0,

(cid:90) ∞

2

0

tAρ,|κ−t| dt = 2

− αρ|κ| +

(cid:114) 2
π
(cid:114) 2
π
(cid:114) 2
π

(cid:18)

exp

−

1
|κ|

(cid:18)

(cid:18)

1
|κ|3 exp
1
|κ|3 exp

κ2
2
κ2
2
κ2
2

−

(i)
= 2

(ii)
= 2

−

− αρ|κ| +

(cid:19) (cid:90) ∞

0
(cid:19) (cid:90) ∞

0

(cid:19)

(1 − ρ2)α2
2
(1 − ρ2)α2
2
(1 − ρ2)α2
2

55

t
(t/|κ|) + 1

exp

u
(u/|κ|2) + 1

(cid:18)

−

t2
2
(cid:18)

exp

−

(cid:19)

− t|κ| − αρt

dt

u2
2κ2 − u −

αρ
|κ|

(cid:19)

du

− αρ|κ| +

· (1 + ˘oκ(1)) =

2(1 + ˘oκ(1))
|κ|2

Aρ,|κ|

where (i) is due to a change of variable u = t|κ| and (ii) is due to the dominated convergence theorem. For
ρ ≤ −η0, following a similar argument,

(cid:90) ∞

2

0

tAρ,|κ−t| dt = 2

(cid:114) 2
π

1
|κ|

(cid:18)

exp

−

κ2
2

(cid:19) (cid:90) ∞

0

t
(t/|κ|) + 1

(cid:18)

exp

−

t2
2

(cid:19)

− t|κ|

dt =

2(1 + ˘oκ(1))
|κ|2

Aρ,|κ|.

For ρ ∈ (−η0, η0), we deﬁne F (x) = ϕ(x) + 1 − ϕ(−x), so the formula (77) is simpliﬁed into

aρ,t =

=

1
2(cid:112)2π(1 − ρ2)
1
2(cid:112)2π(1 − ρ2)

(cid:90) ∞

0
(cid:90) ∞

0

(cid:2)ϕ(u − ρt) + 1 − ϕ(u + ρt) + ϕ(−u − ρt) + 1 − ϕ(−u + ρt)(cid:3) exp

(cid:18)

−

u2
2(1 − ρ2)

(cid:19)

du

(cid:2)F (u − ρt) + F (−u − ρt)(cid:3) exp

(cid:18)

−

u2
2(1 − ρ2)

(cid:19)

du

Using this and Fubini’s theorem, we derive

(cid:90) ∞

2

tAρ,|κ−t| dt

0

=

=

=

(cid:114) 2
1
(cid:112)2π(1 − ρ2)
π
(cid:114) 2
exp(−κ2/2)
(cid:112)2π(1 − ρ2)
π
(cid:114) 2
π

3
(cid:88)

exp(−κ2/2)
(cid:112)2π(1 − ρ2)

k=1

(cid:90) ∞

0
(cid:90) ∞

t
|t − κ|
(cid:18)

exp

−

0

(cid:18)

exp

−

u2
2(1 − ρ2)

(t − κ)2
2
(cid:19)

du

(cid:90) ∞

(cid:19)

dt

0

(cid:2)F (u − ρt − ρ|κ|) + F (−u − ρt − ρ|κ|)(cid:3) exp

(cid:18)

−

u2
2(1 − ρ2)

(cid:19)

du

(cid:90) ∞

0

t
|t − κ|

(cid:18)

exp

−

t2
2

(cid:19)

− t|κ|

(cid:2)F (u − ρt − ρ|κ|) + F (−u − ρt − ρ|κ|)(cid:3) dt

(cid:90) ∞

(cid:18)

exp

−

0

u2
2(1 − ρ2)

(cid:19)

(cid:90)

du

Jk

t
|t − κ|

(cid:18)

exp

−

t2
2

(cid:19)

− t|κ|

(cid:2)F (u − ρt − ρ|κ|) + F (−u − ρt − ρ|κ|)(cid:3) dt

=: I1 + I2 + I3

where

J1 = [0, |κ|−1/4],

J2 = [|κ|−1/4, |κ|1/4],

J3 = [|κ|1/4, +∞).

A straightforward bound gives I3 = ˘oκ(exp(−|κ|2/2 − |κ|5/4)). We claim that

sup
u∈R,|ρ|≤η0,t∈J1

(cid:12)
(cid:12)
(cid:12)

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

− 1

(cid:12)
(cid:12)
(cid:12) = ˘oκ(1),

sup
u∈R,|ρ|≤η0,t∈J2

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

= O(exp(|κ|1/4)).

Once this is proved, we simply the inner integrals and obtain

I1 =

=

I2 =

=

(cid:114) 2
π
(cid:114) 2
π
(cid:114) 2
π
(cid:114) 2
π

exp(−κ2/2)
(cid:112)2π(1 − ρ2)
(cid:18)

exp

−

exp(−κ2/2)
(cid:112)2π(1 − ρ2)
(cid:18)

exp

−

κ2
2

1 + ˘oκ(1)
|κ|3

(cid:90) ∞

0

(cid:2)F (u − ρ|κ|) + F (−u − ρ|κ|)(cid:3) exp

(cid:18)

−

u2
2(1 − ρ2)

(cid:19)

du

κ2
2

(cid:19) 1 + ˘oκ(1)
|κ|3

aρ,|κ| =

1 + ˘oκ(1)

|κ|2 Aρ,|κ|;

1 + ˘oκ(1)
|κ|3

(cid:90) ∞

0

(cid:2)F (u − ρ|κ|) + F (−u − ρ|κ|)(cid:3) exp

(cid:18)

−

u2
2(1 − ρ2)

(cid:19)

du · O

(cid:16)

(cid:16)

− |κ|3/4 + |κ|1/4(cid:17)(cid:17)

exp

(cid:19) ˘oκ(1)

|κ|3 aρ,|κ| =

˘oκ(1)
|κ|2 Aρ,|κ|.

The would lead to the conclusion. Thus it remains to prove the claim.

First consider the case t ∈ J1. By the tail assumption of ϕ, for any given ε0 > 0, we can ﬁnd an

M = M (ε0) > 0 such that |F (x) − 2e−αx| ≤ ε0e−αx for all x < −M . If u − ρ|κ| < −2M ,

|F (u − ρt − ρ|κ|) − F (u − ρ|κ|)| ≤ (2 + ε0)e−α(u−ρt−ρ|κ|) − (2 − ε0)e−α(u−ρ|κ|)

so

sup
u−ρ|κ|<−2M,t∈J1

(cid:12)
(cid:12)
(cid:12)

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

(cid:12)
(cid:12)
(cid:12) ≤ sup
− 1
t∈J1

(2 + ε0)eαρt − (2 − ε0)
2 − ε0

κ→−∞−→

2ε0
2 − ε0

56

If u − ρ|κ| > −2M , we use uniform continuity of F to obtain

sup
u−ρ|κ|≥−2M,t∈J1

(cid:12)
(cid:12)
(cid:12)

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

− 1

(cid:12)
(cid:12)
(cid:12)

κ→−∞−→ 0.

Since ε0 is arbitrary, this proves the ﬁrst half of the claim.

Next consider the case t ∈ J2. If u − ρ|κ| > −M , then the ratio is always bounded by O(2/F (−M )). If

u − ρ|κ| ≤ −M and u − ρ|κ| − ρt ≤ −M , then

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

≤

(2 + ε0) exp (cid:0)u − ρt − ρ|κ|(cid:1)

(2 − ε0) exp (cid:0)u − ρ|κ|(cid:1) =

(2 + ε0) exp(|κ|1/4)
2 − ε0

.

If u − ρ|κ| ≤ −M and u − ρ|κ| − ρt > −M , then

F (u − ρt − ρ|κ|)
F (u − ρ|κ|)

≤

2
(2 − ε0) exp (cid:0)u − ρ|κ|(cid:1) =

2 exp(ρt)
(2 − ε0) exp (cid:0)u − ρ|κ| − ρt(cid:1) ≤

2 exp(|κ|1/4)
(2 − ε0) exp (cid:0) − M (cid:1) .

This proves the second half of the claim, thus completing the proof.

Lemma 12. Let G ∼ N(0, 1) be a Gaussian variable. Then, for t > 0,

t
t2 + 1

e−t2/2 <

(cid:90) ∞

t

e−x2/2 dx <

e−t2/2.

1
t

Moreover,

E(cid:2)(G − t)+

(cid:3) =

1 + ot(1)
√
2π t2

e−t2/2,

E(cid:2)(G − t)2

+

(cid:3) =

2(1 + ot(1))
2π t3

√

e−t2/2.

Proof of Lemma 7. This proof is similar to that of Theorem 3.3. Let v ∼ Unif(Sd−1) be independent of
the (yi, Gi, zi)’s. For any c ≥ 1, consider the following convex optimization problem:

maximize (cid:104)v, w(cid:105) , subject to ρyiGi +

(cid:112)

1 − ρ2(cid:104)zi, w(cid:105) ≥ κ, ∀i ∈ [n], and (cid:107)w(cid:107)2 ≤ c.

(97)

Denote by Mn(c) the maximum of (97), then we can write

Mn(c) = max
(cid:107)w(cid:107)2≤c

min
α≥0

= max

(cid:107)w(cid:107)2≤c

min
α≥0

(cid:110)

(cid:104)v, w(cid:105) +

(cid:68)

α, ρy (cid:12) G +

(cid:112)

1 − ρ2Zw − κ1

(cid:69)(cid:111)

(cid:110)

(cid:104)v, w(cid:105) + (cid:104)α, ρy (cid:12) G − κ1(cid:105) +

(cid:112)

1 − ρ2 (cid:104)α, Zw(cid:105)

(cid:111)

.

As in the proof of Theorem 3.3, we can ﬁrst condition on the (yi, Gi)’s, then applying Gordon’s inequality
(Theorem B.1) reduces the analysis of Mn(c) to that of (cid:102)Mn(c), where

(cid:102)Mn(c) = max
(cid:107)w(cid:107)2≤c

min
α≥0

(cid:110)

(cid:104)v, w(cid:105) + (cid:104)α, ρy (cid:12) G − κ1(cid:105) +

(cid:112)

1 − ρ2 (cid:107)w(cid:107)2 (cid:104)α, h(cid:105) +

(cid:112)

1 − ρ2 (cid:107)α(cid:107)2 (cid:104)w, g(cid:105)

(cid:111)

,

where h ∼ N(0, In), g ∼ N(0, Id−1) are mutually independent, and further independent of other random
variables.

Proceeding similarly as in the proof of Theorem 3.3, we obtain that

(cid:102)Mn(c) = max
r∈[0,c]

F (r) + od,P(1),

where

F (r) =






(cid:115)

r2 − δ

1−ρ2 E

(cid:20)(cid:16)

κ − ρY G − (cid:112)1 − ρ2rW

(cid:21)
,

(cid:17)2

+

if r2(1 − ρ2) ≥ δE

(cid:20)(cid:16)

κ − ρY G − (cid:112)1 − ρ2rW

(cid:21)

(cid:17)2

+

,

−∞,

otherwise.

57

Note that by Stein’s identity, when F (r) > −∞, we have

d
dr

F (r)2 =

d
dr


r2 − δE



(cid:32)



κ
(cid:112)1 − ρ2

−

ρY G
(cid:112)1 − ρ2

− rW





(cid:33)2





+

(cid:32)

(cid:34)

(cid:32)

=2r

1 − δE

Φ

(cid:33)(cid:35)(cid:33)

κ − ρY G
r(cid:112)1 − ρ2
(cid:112)

(cid:16)

1 − δP

(cid:16)

=2r

ρY G +

1 − ρ2rW < κ

(cid:17)(cid:17)

.

Since the link function ϕ is monotone increasing, we know that the map r (cid:55)→ P(ρY G + (cid:112)1 − ρ2rW < κ) is
increasing. According to Assumption C.1, we have

δP(ρY G +

(cid:112)

1 − ρ2W < κ) <1,



(cid:32)

δE



κ
(cid:112)1 − ρ2

−

ρY G
(cid:112)1 − ρ2



(cid:33)2

− W

 <1,

+

it follows that

F (1) > −∞,

d
dr

F (r)2 > 0 whenever r ∈ [0, 1], F (r) > −∞.

This proves that there exists c > 1, such that

max
r∈[0,c]

F (r) > max
r∈[0,1]

F (r),

which further implies that with high probability,

(cid:102)Mn(c) > (cid:102)Mn(1) =⇒ Mn(c) > Mn(1).

Therefore, the maximizer of (97) satisﬁes (cid:107)w(cid:107)2 ≥ 1 with high probability. As a consequence, Eq. (78) holds
and we ﬁnish the proof of Lemma 7.

Proof of Lemma 8. To begin with, we rewrite Eq. (85) as

ρyiGi +

(cid:112)

1 − ρ2(cid:104)zi1, w1(cid:105) ≥

(cid:112)

1 − ρ2κ2, ∀i ∈ SG,

where w1 ∈ Sd−2−d0, and

i ∈ SG ⇐⇒ yiGi ≥

(cid:112)1 − ρ2κ1
ρ

.

It will be more convenient if we adjust our notation here: We recast zi1 as zi, w1 as w, d−d0 as d, (cid:112)1 − ρ2κ2
as κ, and (cid:112)1 − ρ2κ1/ρ as κt. Note that under this new notation, according to Eq. (83) we have

δ := lim

n→+∞

n
d

< δsec(ρ, κ, κt).

(98)

Now it suﬃces to show that with probability bounded away from 0, ∃w ∈ Sd−2 such that

ρyiGi +

(cid:112)

1 − ρ2(cid:104)zi, w(cid:105) ≥ κ, ∀i ∈ SG, i.e., yiGi ≥ κt.

To this end we apply the second moment method. As in the proof of Theorem 3.1, let µ denote the uniform
probability measure on Sd−2, and choose f : R2 → [0, +∞) to be

(cid:16)

f

ρs,

(cid:112)

1 − ρ2w

(cid:17)

=

1
(cid:112)e(0, s)

exp(−c(s)w)1{w ≥ κ(s)},

58

where κ(s), c(s) and e(q, s) are deﬁned in Deﬁnition 3. In particular,

Note that f is supported on {(x, y) ∈ R2 : x + y ≥ κ}. We further deﬁne the following random variable:

e(0, s) = E[exp(−c(s)W )1{W ≥ κ(s)}]2.

(99)

(cid:90)

Z =

(cid:89)

(cid:16)

f

Sd−2

i∈SG

ρyiGi,

(cid:112)

1 − ρ2(cid:104)zi, w(cid:105)

µ(dw),

(cid:17)

then we want to show that E[Z 2] = Od(E[Z]2). Below we calculate the ﬁrst and second moments of Z, we
omit some details here since the computation is almost the same as that in the proof of Theorem 3.1.

Let Wi ∼i.i.d. N(0, 1), and independent of the (yi, Gi)’s, then we have

(cid:34)

E[Z] =E

(cid:89)

(cid:16)

f

ρyiGi,

(cid:112)

1 − ρ2Wi

(cid:35)

(cid:17)

i∈SG

(cid:34) n
(cid:89)

(cid:16)

exp

=E

1 {yiGi ≥ κt} log f

(cid:16)

(cid:112)

ρyiGi,

1 − ρ2Wi

(cid:35)

(cid:17)(cid:17)

(cid:104)

i=1

exp

(cid:16)

=E

1 {Y G ≥ κt} log f

(cid:16)

(cid:112)

ρY G,

1 − ρ2W

(cid:17)(cid:17)(cid:105)n

.

Again, using the same kind of analysis as in the proof of Theorem 3.1, we obtain that

(cid:90)

E[Z 2] =

Sd−2×Sd−2

=

(cid:90) 1

−1

(cid:104)

p(q)Eq

(cid:34)

E

(cid:89)

(cid:16)

f

ρyiGi,

(cid:112)

1 − ρ2(cid:104)zi, w1(cid:105)

(cid:17)

(cid:16)

f

ρyiGi,

(cid:112)

1 − ρ2(cid:104)zi, w2(cid:105)

(cid:35)

(cid:17)

µ(dw1)µ(dw2)

i∈SG
(cid:16)

exp

1 {Y G ≥ κt} log

(cid:16)

(cid:16)

f

ρY G,

(cid:112)

1 − ρ2W1

(cid:16)

(cid:17)

f

ρY G,

(cid:112)

1 − ρ2W2

(cid:17)(cid:17)(cid:17)(cid:105)n

dq

= (1 + od(1))

(cid:90) 1

(cid:114)

−1

d
2π

(cid:0)1 − q2(cid:1)(d−3)/2

× Eq

(cid:104)
exp

(cid:16)

1 {Y G ≥ κt} log

(cid:16)

(cid:16)

f

ρY G,

(cid:112)

1 − ρ2W1

(cid:16)

(cid:17)

f

ρY G,

(cid:112)

1 − ρ2W2

(cid:17)(cid:17)(cid:17)(cid:105)n

dq,

where p(q) is the density of (cid:104)w1, w2(cid:105) introduced in the proof of Theorem 3.1. Notice that

(cid:104)
exp

(cid:16)

E

1 {Y G ≥ κt} log f

(cid:16)

(cid:112)

ρY G,

1 − ρ2W

(cid:17)(cid:17)(cid:105)

=1 +

=1 +

(cid:90) +∞

κt
(cid:90) +∞

κt

pY G(s)

(cid:16)

E

(cid:104)
f

(cid:16)

ρs,

(cid:112)

1 − ρ2W

(cid:17)(cid:105)

(cid:17)

− 1

ds

pY G(s)

(cid:32) E[exp(−c(s)W )1{W ≥ κ(s)}]
(cid:112)e(0, s)

(cid:33)

− 1

ds = 1,

where the last equality is due to Eq. (99). This implies E[Z] = 1. Recall that e(q), F (q) and I(q) are deﬁned
in Deﬁnition 3, a similar calculation then leads to

E[Z 2] = (1 + od(1))

(cid:90) 1

(cid:114)

−1

d
2π

exp (−nF (q) − (d − 3)I(q)) dq,

and

exp (−nF (0)) = e(0)n = 1.

Therefore, with the aid of Laplace’s method, it only suﬃces to verify that F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0, and that
F (q) + I(q)/δ is uniquely minimized at q = 0. However, this is just a direct consequence of Eq. (98) and the
deﬁnition of δsec(ρ, κ, κt), thus proving Lemma 8.

59

Proof of Lemma 9. For ﬁxed w1 ∈ Rd−1−d0, (cid:107)w1(cid:107)2 = 1, we know that ZB
z = ZB

1 w1, and deﬁne

1 w1 ∼i.i.d. N(0, 1). Denote

b =

(cid:20) (κ0 − κ2)1
κ01 − uB − z

(cid:21)

,

then we want to show that with high probability, ∃w2 ∈ Rd0, (cid:107)w2(cid:107)2 ≤ c such that Z2w2 ≥ b.

The proof strategy is similar to that of Theorem 3.3 and Lemma 7, so technical details will be omitted.

To be speciﬁc, let us consider the following random variable:

ξn,c =

1
√
d0

min
(cid:107)w2(cid:107)2≤c

max
λ≥0,(cid:107)λ(cid:107)2≤1

(cid:110)

λ(cid:62) (b − Z2w2)

(cid:111)

.

(100)

Then our desired result is equivalent to proving that ξn,c = 0 with high probability. Using Gordon’s
comparison inequality (Theorem B.1) and the Uniform Law of Large Numbers [NM94, Lemma 2.4], we
conclude that as n → +∞, ξn,c converges in probability to the following constant:

(cid:32)(cid:114)

min
0≤γ≤c

(cid:16)

pGE

(cid:104)
(κ0 − κ2 − γW )2
+

(cid:105)

δ0

(cid:104)

+ pBE

(κ0 − uB − z − γW )2
+

(cid:105)(cid:17)

(cid:33)

− γ

,

+

where pG = 1 − pB, W ∼ N(0, 1) is independent of uB and z, and

δ0 = lim

n→+∞

n
d0

.

Note that Eq. (82) implies

1 >δ0


pGE

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+

+

pB
c2



(cid:32)

E



κ0 −

ρY G
(cid:112)1 − ρ2

−

(cid:112)

1 + c2W

(cid:33)2

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y G <

(cid:112)1 − ρ2
ρ





κ1





=

=

δ0
c2
δ0
c2

(cid:18)

(cid:104)

pGE

(κ0 − κ2 − cW )2
+

(cid:105)

+ pBE

(cid:20)(cid:16)

κ0 − uB −

(cid:112)

1 + c2W

(cid:21)(cid:19)

(cid:17)2

(cid:16)

pGE

(cid:104)
(κ0 − κ2 − cW )2
+

(cid:105)

+ pBE

(cid:104)(cid:0)κ0 − uB − z − cW (cid:1)2

+

(cid:105)(cid:17)

+

,

where z ∼ N(0, 1) is independent of other random variables, thus leading to

(cid:32)(cid:114)

δ0

(cid:16)

pGE

(cid:104)

min
0≤γ≤c

(κ0 − κ2 − γW )2
+

(cid:105)

+ pBE

(cid:104)
(κ0 − uB − z − γW )2
+

(cid:105)(cid:17)

− γ

(cid:32)(cid:114)

≤

(cid:16)

pGE

(cid:104)

δ0

(κ0 − κ2 − cW )2
+

(cid:105)

+ pBE

(cid:104)
(κ0 − uB − z − cW )2
+

(cid:105)(cid:17)

(cid:33)

(cid:33)

+

− c

= 0.

+

This concludes the proof.

Proof of Lemma 10. To verify Assumption C.2, we discuss the choice of parameters ρ, κ1, κ2, c in the
following two parts:
Part 1. The truncated second moment method.
Let t, η > 0 be small constants independent of κ. Set

ρ = 1 −

t
|κ|

, κ2 = κ0 + c (|κ| + α + η) , κ1 = κ2 + M (κ),

where M (κ) = |κ|1/4. Then we have

(cid:112)

1 − ρ2κ2 − κ = ˘oκ(|κ|−1).

60

According to Lemma 6, it follows that

(cid:112)

1 − ρ2κ2

(cid:17)

(cid:16)

P

ρY G +

(cid:112)

=

=

1 + ˘oκ(1)
|κ + ˘oκ(|κ|−1)|
(cid:114) 2
1 + ˘oκ(1)
π
|κ|

1 − ρ2W <
(cid:114) 2
π
(cid:18)

exp

(cid:18)

−

κ2
2

exp

−

+ αρκ +

(κ + ˘oκ(|κ|−1))2
2
(1 − ρ2)α2
2

+ αρ(κ + ˘oκ(|κ|−1)) +

(cid:19)

=

1 + ˘oκ(1)
|κ|

(cid:114) 2
π

(cid:19)

(cid:19)

(1 − ρ2)α2
2
κ2
2

−

(cid:18)

exp

+ ακ + αt

.

Therefore, we can take t > 0 to be small enough so that

(cid:16)

1 −

√
π
√
2
2

(cid:17)

ε
2

|κ| log |κ| exp

(cid:18) κ2
2

(cid:19)

+ α|κ|

<

(cid:16)

1 −

(cid:17)

ε
4

(cid:112)1 − ρ2κ2
log (cid:12)
(cid:12)
ρY G + (cid:112)1 − ρ2W < (cid:112)1 − ρ2κ2

(cid:12)
(cid:12)

(cid:17) .

(cid:16)

2P

(101)

Additionally, we have

Next we aim to show that

ρ = 1 −

tκ
(cid:112)1 − ρ2κ2

(cid:12)
(cid:12)

,

(cid:12)
(cid:12)

tκ → t.

(cid:32)

δsec

ρ,

(cid:112)

1 − ρ2κ2,

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

≥

(cid:16)

1 −

(cid:17)

ε
4

(cid:16)

2P

log (cid:12)
(cid:12)

(cid:112)1 − ρ2κ2
ρY G + (cid:112)1 − ρ2W < (cid:112)1 − ρ2κ2

(cid:12)
(cid:12)

(cid:17) .

(102)

Under the new notation introduced in the proof of Lemma 8, we have

ρ = 1 −

tκ
|κ|

, tκ → t,

κt =

κ
ρ

+

(cid:112)1 − ρ2M (κ)
ρ

, M (κ) → +∞.

Recasting ε as 4ε, it suﬃces to prove that as long as

δ <

(1 − ε) log |κ|
ρY G + (cid:112)1 − ρ2W < κ

(cid:16)

(cid:17) ,

2P

(103)

then F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0, and that F (q) + I(q)/δ achieves unique minimum at q = 0. We proceed with the
following two steps:
Step 1. Verify the ﬁrst condition
In fact, the ﬁrst condition will be automatically implied by the second one. To see this we note that

e(cid:48)(0) =

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)

e(cid:48)(0, s)
e(0, s)

ds = 0,

where we use e(cid:48)(q, s) to denote the partial derivative of e(q, s) with respect to the ﬁrst variable q, and
e(cid:48)(0, s) = 0 is just a consequence of Lemma 1 (b). Therefore, F (cid:48)(0) + I (cid:48)(0)/δ = 0. Assume that for all δ
satisfying Eq. (103), F (q) + I(q)/δ is uniquely minimized at q = 0, then we must have F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ ≥ 0.
However, F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ is strictly decreasing in δ, so it must be the case that F (cid:48)(cid:48)(0) + I (cid:48)(cid:48)(0)/δ > 0 for all
δ satisfying Eq. (103).
Step 2. Verify the second condition
Similarly as before, it suﬃces to show that (note e(0) = 1 by Deﬁnition 3)

∀q ∈ [−1, 1]\{0}, δ log

(cid:19)

(cid:18) e(q)
e(0)

< I(q) ⇐= δ (e(q) − 1) < I(q)

⇐⇒ δ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)
e(0, s)

(e(q, s) − e(0, s)) ds < I(q)

61

⇐⇒ δ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du < I(q).

Consider the following three cases:

(1) q ∈ [−1/2, 1/2]\{0}. Since κ(s) ≤ −M (κ), Lemma 1 (b) tells us that e(0, s) = 1 + ˘oκ(1), and further

from Eq. (54) we know that

sup
u∈[−q,q]

|e(cid:48)(cid:48)(u, s)| ≤ |κ(s)|C exp

(cid:19)

(cid:18)

−

2κ(s)2
3

(cid:18)

(cid:18)

= ˘oκ

exp

−

3κ(s)2
5

(cid:19)(cid:19)

,

therefore it follows that (note e(cid:48)(0, s) = 0)

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du = δ

(cid:90) +∞
√

pY G(s)
e(0, s)

ds

(cid:90) q

(cid:90) u

du

0

0

e(cid:48)(cid:48)(v, s)dv

(ϕ(s) + 1 − ϕ(−s))

1−ρ2M (κ)

ρ

1−ρ2M (κ)

κ
ρ +
1
√
2π

ρ
(cid:18)

exp

−

s2
2

−

3κ(s)2
5

(cid:19)

ds

(cid:18)

exp

−

3κ2
5 + ρ2

(cid:19) (cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

(ϕ(s) + 1 − ϕ(−s))

(cid:32)

exp

−

1
√
2π

5 + ρ2
10(1 − ρ2)

(cid:18)

s −

6ρκ
5 + ρ2

(cid:19)2(cid:33)

ds

1−ρ2M (κ)

ρ
(cid:90) +∞
√

κ
ρ +

(cid:90) +∞
√

δ

κ
ρ +

(i)
= ˘oκ(1)δ

=˘oκ(1)δ

q2
2

q2
2

(ii)
≤ ˘oκ(1)δ

q2
2
(cid:32)

(cid:19) (cid:115)

(cid:18)

exp

−

3κ2
5 + ρ2

5(1 − ρ2)
5 + ρ2
(cid:33)

(cid:90)

×

R

(cid:32)(cid:115)

5(1 − ρ2)
5 + ρ2 x +

ϕ

6ρκ
5 + ρ2

(cid:32)

(cid:115)

+ 1 − ϕ

−

5(1 − ρ2)
5 + ρ2 x −

6ρκ
5 + ρ2

(cid:33)(cid:33)

1
√
2π

(cid:18)

exp

−

(cid:19)

x2
2

dx,

where (i) is due to the fact that pY G(s) = (ϕ(s) + 1 − ϕ(−s))φ(s), (ii) results from the change of variable

(cid:115)

x =

5 + ρ2
5(1 − ρ2)

(cid:18)

s −

(cid:19)

6ρκ
5 + ρ2

(cid:115)

⇐⇒ s =

5(1 − ρ2)
5 + ρ2 x +

6ρκ
5 + ρ2 .

Now, recall that Assumption 4.1 implies (note ρ = 1 − tκ/|κ| → 1)

(cid:18)(cid:113) 5(1−ρ2)

5+ρ2 x + 6ρκ

5+ρ2

ϕ

(cid:19)

(cid:18)

+ 1 − ϕ

−

(cid:113) 5(1−ρ2)

5+ρ2 x − 6ρκ

5+ρ2

hence applying Dominated Convergence Theorem yields that

2 exp

(cid:17)

(cid:16) 6αρκ
5+ρ2

(cid:19)

→ 1,

(cid:90) +∞
√

δ

κ
ρ +

1−ρ2M (κ)

ρ
(cid:18)

≤q2|κ|C exp

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du ≤ ˘oκ(1)δ

q2
2

(cid:18)

exp

−

(cid:19)

3κ2
5 + ρ2

2 exp

(cid:18) 6αρκ
5 + ρ2

(cid:19) (cid:115)

5(1 − ρ2)
5 + ρ2

−

1 − ρ2
2(5 + ρ2)

κ2 −

αρ(1 − ρ2)
5 + ρ2

(cid:19)

|κ|

≤ q2|κ|C exp (−Ctκ|κ|) = ˘oκ(1)q2 < I(q),

when |κ| is large enough and q ∈ [−1/2, 1/2]\{0}.

(2) q ∈ [−1, −1/2]. Then similarly as the previous case, we deduce from Eq. (53) that

(cid:90) +∞
√

δ

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du ≤ Cδ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s) exp (cid:0)−κ(s)2(cid:1) ds

(cid:18)

=Cδ exp

−

κ2
1 + ρ2

(cid:19) (cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

(cid:18)

≤Cδ exp

−

κ2
1 + ρ2

(cid:19) (cid:115)

1 − ρ2
1 + ρ2

(ϕ(s) + 1 − ϕ(−s))

(cid:32)

exp

−

1
√
2π

1 + ρ2
2(1 − ρ2)

(cid:18)

s −

2ρκ
1 + ρ2

(cid:19)2(cid:33)

ds

62

(cid:32)

(cid:32)(cid:115)

ϕ

(cid:90)

×

R

1 − ρ2
1 + ρ2 x +
(cid:115)

≤2C(1 + ˘oκ(1))δ

(cid:33)

2ρκ
1 + ρ2

(cid:32)

(cid:115)

+ 1 − ϕ

−

1 − ρ2
1 + ρ2 x −

2ρκ
1 + ρ2

(cid:33)(cid:33)

1
√
2π

(cid:18)

exp

−

(cid:19)

x2
2

dx

1 − ρ2
1 + ρ2 exp

(cid:18)

−

κ2
1 + ρ2 +

2αρκ
1 + ρ2

(cid:19)

≤ |κ|C exp

(cid:18)

−

1 − ρ2
2(1 + ρ2)

κ2 −

αρ(1 − ρ2)
1 + ρ2

(cid:19)

|κ|

≤|κ|C exp (−Ctκ|κ|) = ˘oκ(1) < I(q),

for suﬃciently large |κ| and q ∈ [−1, −1/2].

(3) q ∈ [1/2, 1]. From Lemma 1 (b) and (c) we know that for large |κ|, e(cid:48)(u, s) ≥ 0 when u ≥ 1/2, and

e(1, s) − e(0, s) = (1 + ˘oκ(1))Φ(κ(s)), therefore

(cid:90) +∞
√

δ

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du ≤ δ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s)
e(0, s)

(e(1, s) − e(0, s))ds

pY G(s)Φ(κ(s))ds = (1 + ˘oκ(1))δ

1−ρ2M (κ)

ρ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

(cid:32)

pY G(s)P

W ≤

(cid:33)

ds

κ − ρs
(cid:112)1 − ρ2

≤(1 + ˘oκ(1))δ

(cid:90) +∞
√

κ
ρ +
(cid:16)

=(1 + ˘oκ(1))δP

ρY G +

1 − ρ2W ≤ κ, ρY G ≥ κ +

1 − ρ2M (κ)

(cid:112)

(cid:17)

≤(1 + ˘oκ(1))δP

(cid:16)

ρY G +

1 − ρ2W ≤ κ

(cid:17)

<

(1 + ˘oκ(1))(1 − ε)
2

log |κ|

(cid:112)

(cid:112)

(cid:18) 1
2

<

−

(cid:19)

ε
4

log |κ| ≤ I(q) = −

log (cid:0)1 − q2(cid:1) ,

1
2

if 1 − q ≤ |κ|−1+ε/2. Now it remains to deal with the case when 1 − q > |κ|−1+ε/2, using Eq. (53) and a
similar argument as in the proof of Theorem 3.1, we obtain that

(cid:90) q

0

e(cid:48)(u, s)ds ≤ (1 + ˘oκ(1))

(cid:18)

=C exp

−

κ(s)2
2

−

0
(1 − q)κ(s)2
2(1 + q)

(cid:90) q

√

1
1 − u2
(cid:18)

2π
(cid:19)

≤ C exp

−

exp

(cid:19)

(cid:18)

−

κ(s)2
1 + u

(cid:18)

du ≤ C exp

−

(cid:19)

κ(s)2
1 + q

κ(s)2
2

−

κ(s)2
4|κ|1−ε/2

(cid:19)

= C exp (cid:0)−aκ(s)2(cid:1) ,

where we denote a = 1/2 + 1/(4|κ|1−ε/2). Similarly as the previous computations, it follows that

pY G(s)
e(0, s)

ds

(cid:90) q

0

e(cid:48)(u, s)du ≤ Cδ

(cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

pY G(s) exp (cid:0)−aκ(s)2(cid:1) ds

(cid:90) +∞
√

δ

κ
ρ +

1−ρ2M (κ)

ρ

(cid:18)

=Cδ exp

−

aκ2
1 + (2a − 1)ρ2

(cid:19) (cid:90) +∞
√

κ
ρ +

1−ρ2M (κ)

ρ

(ϕ(s) + 1 − ϕ(−s))

(cid:32)

× exp

−

1 + (2a − 1)ρ2
2(1 − ρ2)

(cid:18)

s −

2aρκ
1 + (2a − 1)ρ2

(cid:19)2(cid:33)

ds

(cid:18)

≤Cδ exp

−

aκ2
1 + (2a − 1)ρ2

(cid:19) (cid:115)

1 − ρ2
1 + (2a − 1)ρ2

1
√
2π

(cid:18)

exp

−

(cid:19)

x2
2

dx

(cid:90)

R

(cid:33)

(cid:32)

(cid:32)(cid:115)

×

ϕ

1 − ρ2

1 + (2a − 1)ρ2 x +
(cid:18)

≤2C(1 + ˘oκ(1))δ exp

−

≤|κ|C exp

≤|κ|C exp

(cid:16)

(cid:18)

(2a − 1)(1 − ρ2)
2(1 + (2a − 1)ρ2)

−
−Ctκ|κ|ε/2(cid:17)

2aρκ
1 + (2a − 1)ρ2

(cid:32)

(cid:115)

+ 1 − ϕ

−

aκ2
1 + (2a − 1)ρ2

(cid:19) (cid:115)

1 − ρ2

1 + (2a − 1)ρ2 exp

κ2 −

(2a − 1)αρ(1 − ρ2)
1 + (2a − 1)ρ2

(cid:19)

|κ|

= ˘oκ(1) < I(q),

63

(cid:33)(cid:33)

2aρκ
1 + (2a − 1)ρ2

1 − ρ2

1 + (2a − 1)ρ2 x −
(cid:19)
(cid:18)

2αaρκ
1 + (2a − 1)ρ2

when |κ| is large and q ∈ [1/2, 1]. Here the last inequality just results from the following quick calculation:

a =

1
2

+

1
4|κ|1−ε/2

=⇒

(2a − 1)(1 − ρ2)
2(1 + (2a − 1)ρ2)

κ2 ≥ C

1
|κ|1−ε/2

tκ
|κ|

κ2 = Ctκ|κ|ε/2.

To conclude, we have ﬁnished the veriﬁcation of the above two conditions, thus proving Eq. (102).

Combining this with Eq. (101) ﬁnally yields

(cid:32)

δ · δsec

ρ,

(cid:112)

1 − ρ2κ2,

(cid:33)−1

(cid:112)1 − ρ2
ρ

κ1

<

1 − ε
1 − ε/2

.

(104)

Part 2. The Gordon calculation.
Based on the choice of ρ, κ2 and κ1 in Part 1, we deduce that
(cid:112)1 − ρ2κ1
ρ

(cid:112)1 − ρ2M (κ)
ρ

(cid:112)1 − ρ2κ2
ρ

(cid:18) 1
|κ|

+ ˘oκ

κ
ρ

=

=

+

(cid:19)

+

(cid:112)1 − ρ2M (κ)
ρ

=

κ
ρ

+ ˘oκ(1) = κ − t + ˘oκ(1).

Hence, by Lemma 6, it follows that

δpB = δP(u < κ1) = δP

Y G <

(cid:32)

(cid:33)

(cid:112)1 − ρ2κ1
ρ

= δP (Y G < κ − t + ˘oκ(1)) ≤ C exp (−t|κ|) .

Moreover, if we choose c = 1/|κ|2, then

Now we focus on the second term on the right hand side of Eq. (81). We obtain the following estimate:

δpB
c2 ≤ C|κ|4 exp (−t|κ|) = ˘oκ(1).

(105)

(cid:32)

δP

Y G ≥

(i)
=δ

2(1 + ˘oκ(1))
(cid:0) κ0−κ2
c

(cid:1)2 Φ

(cid:33)

κ1

(cid:112)1 − ρ2
ρ
(cid:18) κ0 − κ2
c

E

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+

≤ δE

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+

(cid:19)

≤

Cδ
κ2 Φ (κ − α − η) ≤ C|κ|C exp (−η|κ|) = ˘oκ(1),

where (i) is due to Lemma 6. As for the third term, we write



(cid:32)

E



κ0 −

δ
c2

ρY G
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W

(cid:33)2

(cid:40)

1

Y G <

+

(cid:112)1 − ρ2κ1
ρ

(cid:41)


√

1−ρ2κ1
ρ

(cid:90)

−∞

=

δ
c2



(cid:32)

pY G(s)E



κ − ρs
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W



(cid:33)2

 ds.

+

Then it follows that

√

1−ρ2 κ1
ρ

(cid:90)

−∞



(cid:32)

pY G(s)E



κ − ρs
(cid:112)1 − ρ2



 pY G(s)E



(cid:112)

1 + c2W

−



(cid:33)2

 ds

+

(cid:32)

κ − ρs
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W



(cid:33)2

 ds

+



(cid:90) κ

ρ

=



√

1−ρ2 κ1
ρ

(cid:90)

+

−∞

κ
ρ

(cid:90) κ

ρ

(i)
≤C

−∞

(cid:18)

pY G(s)

1 +

(cid:19)

(κ − ρs)2
1 − ρ2

ds + C

√

1−ρ2 κ1
ρ

(cid:90)

κ
ρ

pY G(s)ds

64

=CpB +

C
1 − ρ2

(cid:104)

E

(κ − ρY G)2
+

(cid:105) (ii)

≤ CpB +

(cid:18)

≤C

pB +

1
(1 − ρ2)κ2

P

(cid:18)

Y G <

(cid:19)(cid:19)

≤

κ
ρ

Cρ2
1 − ρ2 (1 + ˘oκ(1))
CpB
(1 − ρ2)κ2 ,

(cid:18)

ρ2
κ2

P

Y G <

(cid:19)

κ
ρ

where (i) is due to the fact that E[(a −
(ii) results from Lemma 6:

√

1 + c2W )2

+] ≤ C(a2 + 1), and further ≤ C if a ≤ 0 (since c = ˘oκ(1)),

(cid:104)

E

(κ − ρY G)2
+

(cid:105)

= ρ2E

(cid:34)(cid:18) κ
ρ

(cid:35)

(cid:19)2

− Y G

= (1 + ˘oκ(1))

+

(cid:18)

Y G <

2ρ4
κ2

P

(cid:19)

.

κ
ρ

Therefore, we ﬁnally deduce that



(cid:32)

E



κ0 −

δ
c2

ρY G
(cid:112)1 − ρ2

(cid:112)

−

1 + c2W

(cid:33)2

(cid:40)

1

Y G <

+

(cid:112)1 − ρ2κ1
ρ

(cid:41)

 ≤

CδpB

c2(1 − ρ2)κ2 ≤ C|κ|C exp(−t|κ|),

where the last inequality follows from Eq. (105). Note that we already proved that


P

δ

(cid:32)

Y G ≥

(cid:112)1 − ρ2
ρ

(cid:33)

κ1

E

(cid:34)(cid:18) κ0 − κ2
c

− W

(cid:35)

(cid:19)2

+

+

E

1
c2

(cid:40)


1

Y G <

(cid:112)1 − ρ2
ρ

(cid:41) (cid:32)

κ1

κ0 −

ρY G
(cid:112)1 − ρ2

−

(cid:112)

1 + c2W

(cid:33)2






 = ˘oκ(1),

+

and further < ε/2 if |κ| is large enough. Combining this result with Eq. (104) immediately gives Eq. (81).

Proof of Lemma 11. Since the inﬁmum of an arbitrary collection of upper semicontinuous functions is
upper semicontinuous, we only need to show that for any c > 0,

(cid:32)

η +

c
δ

(cid:112)1 − ρ2
c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4

+

1
c

log

c(cid:112)1 − ρ2 + (cid:112)c2(1 − ρ2) + 4
2

− inf
u>0

(cid:26) c
4u

−

δ
c

(cid:27)(cid:33)

log ψκ,ρ(−u)

is upper semicontinuous in ρ. We will prove an even stronger statement, i.e.,

(cid:26) c
4u

inf
u>0

δ
c
is a continuous function of ρ ∈ [−1, 1]. Let pρ(s) denote the p.d.f. of ρY G + (cid:112)1 − ρ2W , then it’s not hard
to see that ∀ρ, ρ(cid:48) ∈ [−1, 1],
(cid:26) c
4u

| log ψκ,ρ(−u) − log ψκ,ρ(cid:48)(−u)|

log ψκ,ρ(cid:48)(−u)

log ψκ,ρ(−u)

log ψκ,ρ(−u)

(cid:26) c
4u

− inf
u>0

inf
u>0

δ
c

δ
c

−

−

≤

−

(cid:27)

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
u>0

(cid:27)

δ
c
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|ψκ,ρ(−u) − ψκ,ρ(cid:48)(−u)| =

δCκ
c

sup
u>0

(cid:90)

R

exp (cid:0)−u(κ − s)2

+

(cid:12)
(cid:12)
(cid:1) (pρ(s) − pρ(cid:48)(s)) ds
(cid:12)
(cid:12)

(cid:12)
(cid:12)pρ(s) − pρ(cid:48)(s)(cid:12)

(cid:12)ds =

δCκ
c

(cid:107)pρ − pρ(cid:48)(cid:107)L1(R) ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
δCκ
c
δCκ
c

(i)
≤

≤

sup
u>0
(cid:90)

R

where (i) is due to

inf
ρ∈[−1,1],u>0

ψκ,ρ(−u) = 1 − sup

ρ∈[−1,1]

(cid:16)

ρY G +

P

(cid:112)

1 − ρ2W < κ

(cid:17)

> 0,

it reduces to proving
and the fact that log x is Lipschitz-continuous on any [c, +∞), c > 0. Hence,
(cid:107)pρ − pρ(cid:48)(cid:107)L1(R) → 0 when ρ → ρ(cid:48), which according to Scheﬀe’s Lemma will be implied by the a.e. con-
vergence of pρ to pρ(cid:48). Below we show that if ϕ is continuous at both s and −s, then pρ(s) → pρ(cid:48)(s) as
ρ → ρ(cid:48). Consider the following two cases:

(1) ρ(cid:48) ∈ (−1, 1). Then we can assume ρ ∈ (−1, 1) and write

pρ(s) =

1
(cid:112)1 − ρ2

(cid:90)

R

pY G(x)φ

(cid:32)

s − ρx
(cid:112)1 − ρ2

(cid:33)

dx.

65

Based on the continuity of φ and the dominated convergence theorem, we obtain that

(cid:90)

R

lim
ρ→ρ(cid:48)

pY G(x)φ

(cid:32)

s − ρx
(cid:112)1 − ρ2

(cid:33)

dx =

(cid:90)

R

pY G(x)φ

(cid:32)

s − ρ(cid:48)x
(cid:112)1 − ρ(cid:48)2

(cid:33)

dx,

thus leading to limρ→ρ(cid:48) pρ(s) = pρ(cid:48)(s).

(2) ρ(cid:48) = ±1. In this case we have (suppose ρ (cid:54)= 0)

pρ(s) =

1
|ρ|

(cid:90)

R

pY G

(cid:32)

(cid:33)

s − (cid:112)1 − ρ2x
ρ

φ(x)dx.

According to our assumption, pY G is continuous at s/ρ(cid:48), therefore as ρ → ρ(cid:48) one has

(cid:32)

(cid:33)

s − (cid:112)1 − ρ2x
ρ

pY G

→ pY G

(cid:19)

(cid:18) s
ρ(cid:48)

= pρ(cid:48)(s),

and again using dominated convergence theorem yields that

lim
ρ→ρ(cid:48)

pρ(s) = pρ(cid:48)(s)

(cid:90)

R

φ(x)dx = pρ(cid:48)(s).

Now since ϕ is almost everywhere continuous, for a.e. s ∈ R both s and −s are continuity points of ϕ,

consequently we have limρ→ρ(cid:48) (cid:107)pρ − pρ(cid:48)(cid:107)L1(R) = 0, which concludes the proof of this lemma.

Proof of Proposition C.1. (a) lima→+∞ 2D(a)/ log a = 1. It suﬃces to prove that ∀ε > 0:

1. D(a) ≥ (1 − ε)(log a)/2 for large a > 0, i.e., ∀c > 0,

1
√
c2 + 4

+

1
c

c +

c +

log

√

c2 + 4
2

(cid:26) c
4ta

+

(1 − ε) log a
2c

≥ inf
t>0

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:27)

.

First, for suﬃciently large a > 0 and c ≥

√

a, we have

LHS ≥ (1 − ε)

log c
c

≥

(1 − ε) log a
2c

= lim

t→+∞

(cid:18) c
4ta

+

(1 − ε) log a
2c

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:19)

≥ RHS.

Next, if c <

√

a, then we have

RHS ≤ inf
t>0

(cid:26) c
4ta

+

(1 − ε)t log a
c

(cid:27)

(cid:114)

=

(1 − ε) log a
a

≤

for a large enough. Hence we have proved the desired result.

2. D(a) ≤ (1 + ε)(log a)/2 for large a > 0, i.e., ∃c > 0,

(1 − ε) log

√

a

√

a

≤ LHS

1
√
c2 + 4

+

1
c

c +

c +

log

√

c2 + 4
2

(cid:26) c
4ta

+

(1 + ε) log a
2c

< inf
t>0

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:27)

.

Proof of this inequality is similar to the argument in the proof of Theorem 3.2.

(b) D(a) = a/2 when a ≤ 2. Again we need to show the following two things for any ε > 0:

1. D(a) ≤ (1 + ε)a/2 for any a > 0, i.e., ∃c > 0,

1
√
c2 + 4

+

1
c

c +

c +

log

√

c2 + 4
2

(cid:26) c
4ta

+

(1 + ε)a
2c

< inf
t>0

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:27)

.

66

To this end, let η > 0 satisfy (1 + η)3 < 1 + ε, then we choose c > 0 small enough so that LHS < 1 + η,
and that

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds ≤

2c(1 + η)
a(1 + ε)

=⇒

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds ≥

2t
1 + η

.

Therefore, if

then we have

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds >

2c(1 + η)
a(1 + ε)

,

c
4ta

+

(1 + ε)a
2c

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds > 1 + η ≥ LHS.

Otherwise, it follows that

c
4ta

+

(1 + ε)a
2c

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds ≥

c
4ta

+

(1 + ε)at
c(1 + η)

≥

(cid:114) 1 + ε
1 + η

> 1 + η ≥ LHS.

Hence we have proved LHS < RHS for small c > 0.

2. D(a) ≥ a/2 for 0 < a ≤ 2, i.e., ∀c > 0,
√

1
√
c2 + 4

c +

+

log

1
c

c +

c2 + 4
2

(cid:26) c
4ta

+

a
2c

≥ inf
t>0

(cid:90) +∞

0

2ts exp (cid:0)−ts2 − s(cid:1) ds

(cid:27)

.

By taking derivatives we know that this inﬁmum is obtained when

c2
2a2 =

√

t

(cid:90) +∞

x2 exp

(cid:18)

−x2 −

0

(cid:19)

x
√
t

dx.

Note that the above equation uniquely determines t since the right hand side is increasing in t, now
we make a change of variable u = 1/

2t and integrate by part to get that

√

2c2
a2 + 1 =

(cid:0)u2 + 1(cid:1) (1 − Φ(u))
uφ(u)

:=

u2 + 1
u

R(u),

where R(u) denotes the Mills ratio of standard normal distribution, thus leading to

2ts exp (cid:0)−ts2 − s(cid:1) ds =

(cid:90) +∞

+

a
2c
0
(cid:18)(cid:18) u2 + 1

c
4ta
a
2c

=

R(u) − 1

(cid:19) u2
2

u

(cid:18) u2
2

c
a
(cid:19)

+

(cid:19)
a2
2c2 (1 − uR(u))
(cid:18) 1 − u2
2

a
2c

(1 − uR(u)) +

(cid:19)

.

1
2

+ 1 − uR(u)

=

Hence, it suﬃces to prove that ∀u > 0,

1 − u2
2

(1 − uR(u)) +

(cid:32)

1
2

≤

2
a

c
√
c2 + 4

c +

c +

+ log

√

c2 + 4
2

(cid:33)

,

where

c =

(cid:114)

a
√
2

u2 + 1
u

R(u) − 1.

Since the right hand side is a decreasing function of a for ﬁxed u > 0, we only need to prove this
inequality when a = 2. To this end we deﬁne

h(u) =

1 − u2
2

(1 − uR(u)) +

(cid:32)

1
2

−

c
√
c2 + 4

c +

c +

+ log

√

c2 + 4
2

(cid:33)

,

67

then simple calculation yields that

h(cid:48)(u) =

(cid:32)

1
2

−

u2c (cid:0)c +

2
√

c2 + 4(cid:1)

(cid:33)

(cid:0)(cid:0)u4 + 2u2 − 1(cid:1) R(u) − u(u2 + 1)(cid:1) .

According to Theorem 1 of [GU14], we know that

R(u) <

u2 + 2
u3 + 3u

<

u4 + 2u2 + 2
u(u2 + 1)(u2 + 2)

,

which implies that

(cid:0)u4 + 2u2 − 1(cid:1) R(u) − u(u2 + 1) < (cid:0)u4 + 2u2 − 1(cid:1) u2 + 2
u3 + 3u

− u(u2 + 1) = −

2
u3 + 3u

< 0,

and

(cid:115)

c =

2

(cid:18) u2 + 1
u

(cid:115)

(cid:19)

R(u) − 1

<

2

(cid:18) u4 + 2u2 + 2
u2(u2 + 2)

(cid:19)

− 1

=

2
u2 + 2

.

√

u

Therefore, we deduce that

u2c

(cid:16)

c +

(cid:112)

c2 + 4

(cid:18)

(cid:17)

√

< u2

2
u2 + 2
c2 + 4(cid:1) < 0 =⇒ h(cid:48)(u) > 0,

√

u

u

2
u2 + 2

=⇒

1
2

−

2
√

u2c (cid:0)c +

(cid:19)

+

2(u2 + 1)
√
u2 + 2
u

= 4

i.e., h(u) is increasing. Since c → 0 as u → +∞, we know that limu→+∞ h(u) = 0, and consequently
h(u) ≤ 0 for all u > 0.

Combing parts (a) and (b) concludes the proof.

D Linear programming algorithm in the linear signal model:

Proofs of Theorems 4.3 and 4.7

Let us consider a slightly generalized setup. Suppose we are given r0 > 0 and closed interval I ⊂ [−1, 1].
Denote P⊥ = Id − θ∗(θ∗)(cid:62) the projection matrix onto the orthogonal space of θ∗. Deﬁne

Θ =

(cid:110)

θ ∈ Rd : (cid:104)θ, θ∗(cid:105) ∈ I, (cid:107)P⊥θ(cid:107) ≤ r0

(cid:113)

M n = max

(cid:104)θ, v(cid:105) : θ ∈ Θ, yi(cid:104)xi, θ(cid:105) ≥ κ, ∀ i ∈ [n]

(cid:110)

1 − (cid:104)θ, θ∗(cid:105)2
(cid:111)
.

(cid:111)
,

(106)

(107)

In particular, if I = [−1, 1] and r0 = 1, then the maximization in (107) is exactly the linear programming
algorithm we introduced in Section 4.2. Recall that in (31) we deﬁned the random variable Z := Zρ,r (we
suppress the subscripts if no confusion arises). Deﬁne three disjoint sets

(cid:110)

(cid:110)

Ω>(I, r0) =

Ω<(I, r0) =

Ω=(I, r0) =

(ρ, r) ∈ I × [0, r0] : (1 − ρ2)r2δ−1 > E[Z 2; Z < 0]

(ρ, r) ∈ I × [0, r0] : (1 − ρ2)r2δ−1 < E[Z 2; Z < 0]

(cid:110)

(ρ, r) ∈ I × [0, r0] : (1 − ρ2)r2δ−1 = E[Z 2; Z < 0]

(cid:111)
.

(cid:111)
,
(cid:111)
,

We also denote Ω≥(I, r0) = Ω>(I, r0) ∪ Ω=(I, r0). Using the notation in (32), we have equivalence
Ω≥(I, r0) = Ω≥ ∩ {(ρ, r) : ρ ∈ I, r ≤ r0}.

We will prove the following crucial convergence result that serves as a cornerstone for proving Theorem 4.3

and 4.7.

68

Theorem D.1. Suppose that we are given I and r0 > 0 and that Ω>(I, r0) is nonempty.

(a) The set Ω≥(I, r0) is compact. For ρ ∈ I, r ∈ [0, r0], the equation

(1 − ρ2)r2δ−1 = E[max{s, −Z}2],

s.t.

s ≥ 0

(108)

has a unique solution if and only if (ρ, r) ∈ Ω≥(I, r0). Moreover, if (ρ, r) ∈ Ω≥(I, r0), then the solution
s∗ = s∗(ρ, r) is continuous in (ρ, r).

(b) The function

M (ρ, r) = E(cid:2)(Z + s)+

(cid:3) + κ.

is continuous in (ρ, r) ∈ Ω≥(I, r0). Let

M ∗ = max (cid:8)M (ρ, r) : (ρ, r) ∈ Ω≥(I, r0)(cid:9).

Then for any ε > 0 independent of n, d, we have

P(cid:0)M ∗ − ε ≤ M n ≤ M ∗ + ε(cid:1) = 1.

lim
n→∞

D.1 Reduction via Gordon’s comparison theorem

(109)

(110)

Let (cid:101)θ∗ ∈ Rd×(d−1) be the orthogonal complement of θ∗ (i.e., (θ∗, (cid:101)θ∗) is an orthogonal matrix). Recall that in
the beginning of Section C.1, we write θ into its projection to the space of θ∗ and its complement: denoting
ρ = (cid:104)θ, θ∗(cid:105), we obtain an one-to-one map θ ↔ (ρ, w) in the unit ball via the equivalence

(cid:112)

(111)
In particular, if I = [−1, 1] and r0 = 1, then Θ is simply the unit ball in Rd. By the deﬁnition of v, we have
n
(cid:88)

where w ∈ Rd−1, (cid:107)w(cid:107) ≤ 1.

1 − ρ2 (cid:101)θ∗w,

θ = ρ θ∗ +

n
(cid:88)

n
(cid:88)

(cid:104)θ, v(cid:105) = n−1

yi(cid:104)xi, θ(cid:105) d=

yiGi +

ρ
n

i=1

(cid:112)1 − ρ2
n

(cid:104)(cid:101)xi, w(cid:105).

i=1

Thus, we can rewrite M n as
n
(cid:88)

(cid:110)

M n = max
θ∈Θ

min
α≥0

(cid:104)θ, v(cid:105) +

i=1

(cid:0)yi(cid:104)xi, θ(cid:105) − κ(cid:1)(cid:111)

αi

i=1
n
(cid:88)

i=1

ρ
n

(cid:40)

(cid:110)

d= max
ρ∈I

max
(cid:107)w(cid:107)≤r0

min
α≥0

= max
ρ∈I

max
(cid:107)w(cid:107)≤r0

min
α≥0

=: max
ρ∈I

M ρ,n.

yiGi +

(cid:112)1 − ρ2
n

n
(cid:88)

(cid:104)(cid:101)xi, w(cid:105) + ρ

n
(cid:88)

i=1

αiyiGi +

(cid:112)

1 − ρ2

n
(cid:88)

αi(cid:104)(cid:101)xi, w(cid:105) − κ

ρ(cid:104)α + n−11n, y (cid:12) G(cid:105) +

1 − ρ2 (cid:104)α + n−11n, (cid:101)Xw(cid:105) − κ(cid:104)α, 1n(cid:105)

i=1
(cid:112)

i=1
(cid:111)

(112)

(cid:41)

n
(cid:88)

i=1

αi

(113)

Let us denote (cid:101)α = α + n−11n, which satisﬁes (cid:101)α ≥ n−11n. We will apply Gordon’s theorem. To this end,
we deﬁne

Mρ,n := max
(cid:107)w(cid:107)≤r0

min
(cid:101)α≥n−11n

= max
(cid:107)w(cid:107)≤r0

min
(cid:101)α≥n−11n

Mn := max
ρ∈I

Mρ,n.

(cid:112)

(cid:110)
ρ(cid:104) (cid:101)α, y (cid:12) G(cid:105) +
(cid:112)

(cid:110)
(cid:104) (cid:101)α, ρy (cid:12) G +

(cid:124)

1 − ρ2 (cid:107)w(cid:107)(cid:104) (cid:101)α, g(cid:105) +

(cid:112)

1 − ρ2 (cid:107)w(cid:107)g − κ1n
(cid:105) +
(cid:125)

(cid:123)(cid:122)
denoted by z

1 − ρ2 (cid:107) (cid:101)α(cid:107)(cid:104)w, h(cid:105) − κ(cid:104) (cid:101)α, 1n(cid:105) + κ
(cid:112)

(cid:111)
1 − ρ2 (cid:107) (cid:101)α(cid:107)(cid:104)w, h(cid:105) + κ

(cid:111)

(114)

where g ∼ N(0, In) and h ∼ N(0, Id−1) are independent Gaussian vectors.

By using a variant of Gordon’s theorem [MM21, Corollary G.1], we obtain the following result. A minor
technicality is that the constraints (cid:101)α ≥ n−11n do not produce a compact set. Its proof is deferred to the
appendix.
Lemma 13. For all t ∈ R, we have

P (cid:0)M n ≤ t(cid:1) ≤ 2P (Mn ≤ t) ,

P (cid:0)M n ≥ t(cid:1) ≤ 2P (Mn ≥ t) .

69

D.2 Convergence to the asymptotic limit: Proof of Theorem 4.3

First, let us slightly simplify the expression of Mρ,n in (114).

Lemma 14. We can express Mρ,n as

Mρ,n = max
r∈[0,r0]

min
(cid:101)α≥n−11n

(cid:110)
(cid:104) (cid:101)α, z(cid:105) +

(cid:112)

(cid:111)
1 − ρ2 r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107)

+ κ.

(115)

Proof of Lemma 14. We only need to show that for every ﬁxed r ∈ [0, r0], the following holds:

I0 := max
(cid:107)w(cid:107)=r

min
(cid:101)α≥n−11n

(cid:110)
(cid:104) (cid:101)α, z(cid:105) +

(cid:112)

1 − ρ2 (cid:107) (cid:101)α(cid:107)(cid:104)w, h(cid:105)

(cid:111)

= min

(cid:101)α≥n−11n

(cid:110)
(cid:104) (cid:101)α, z(cid:105) +

(cid:112)

(cid:111)
1 − ρ2 r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107)

=: I1. (116)

Note that z does not change on {w : (cid:107)w(cid:107) = r}. Thus, by the Cauchy-Schwarz inequality, for any w with
(cid:107)w(cid:107) = r and (cid:101)α with (cid:101)α ≥ n−11n, we have

(cid:112)

(cid:104) (cid:101)α, z(cid:105) +

1 − ρ2 (cid:107) (cid:101)α(cid:107)(cid:104)w, h(cid:105) ≤ (cid:104) (cid:101)α, z(cid:105) +

(cid:112)

1 − ρ2 r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107).

(117)

By ﬁrst taking the minimum over (cid:101)α and then taking the maximum over w, we obtain I0 ≤ I1. By choosing
w = rh/(cid:107)h(cid:107) in the maximization in I0, we obtain I0 ≥ I1. Therefore, I0 = I1 and this lemma is proved.

For given r ∈ [0, r0], the minimization problem in (115) is convex. We will solve this minimization

problem via the KKT conditions. To that end, we deﬁne

Mn(ρ, r) = min

(cid:101)α≥n−11n

(cid:110)
(cid:104) (cid:101)α, z(cid:105) +

(cid:112)

1 − ρ2 r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107)

(cid:111)

+ κ.

We allow Mn(ρ, r) = −∞ in this deﬁnition. We also assume without loss of generality that zi (cid:54)= 0 for all i
(this holds almost surely since zi is a continuous variable). For convenience, we denote

τn := τn(ρ, r) =

(cid:112)1 − ρ2 r(cid:107)h(cid:107)
√
n

,

τ := τ (ρ, t) =

(cid:112)1 − ρ2 r
√
δ

.

(118)

We will use Qn to denote the empirical distribution of {(yi, Gi, gi)}n
R3 given by

i=1, namely, the probability measure on

Qn =

1
n

n
(cid:88)

i=1

δ(yi,Gi,gi) .

Using this notation, we can write n−1 (cid:80)n
use Q∞ to denote the population measure of (Y, G, g) (so that EQ∞ = E).
Lemma 15. Let Q be any probability measure (which can be random, including the empirical measure Qn),
and let τQ ≥ 0 be a scalar associated with Q. Then, the following equation

i=1 ψ(zi) as EQn [ψ(Z)] for any function ψ. Sometimes, we will also

Q = EQ
τ 2

(cid:2) max{−Z, s}2(cid:3),

s.t.

s ≥ 0

has a unique solution sQ = sQ(ρ, r) > 0 if the inequality

Q > EQ
τ 2

(cid:2) max{−Z, 0}2(cid:3)

(119)

(120)

holds; if the reverse inequality

(121)
holds, then there is no solution to the equation (119). Moreover, if Q = Q∞, then (119) has a unique solution
s(ρ, r) ≥ 0 under the relaxed condition

Q < EQ
τ 2

(cid:2) max{−Z, 0}2(cid:3),

τ 2 ≥ E(cid:2) max{−Z, 0}2(cid:3),

(122)

under which s(ρ, r) is continuous in (ρ, r).

70

Proof of Lemma 15. Denote by ess inf Z the essential inﬁmum of Z, i.e., ess inf Z = sup{a : P(Z <
a) = 0}. Since max{−Z, s} is strictly increasing in s, the function F (ρ, r, s) := E(cid:2) max{−Z, s}(cid:3) is strictly
increasing for s ≥ ess inf Z and is constant for s < ess inf Z. Note that lims→∞ F (ρ, r, s) = ∞. Thus, under
the condition (120), there exists a solution to equation (119), and under the condition (121) there is no
solution.

Under the condition (120), we must have sQ > 0. Since F (ρ, r, sQ) > F (ρ, r, 0), we have sQ > 0 ≥ ess inf Z,

so sQ is the unique solution.

To prove the ‘moreover’ part, we derive

F (ρ, r, s) = 2E(cid:2) max{s, −Z}1{s > −Z}(cid:3) = 2sP(s > −Z) ≥ 0,

∂
∂s
∂2
∂s2 F (ρ, r, s) = 2P(s > −Z) + 2sp−Z(s),

where p−Z is the probability density function of −Z.

∂2
∂s2 F (ρ, r, s) ≥ 2 min

s≥0

min
s≥0

P(Z > −s) ≥ 2P(Z > 0).

By Lemma 23, we have P(Z > 0) ≥ 1/2, which gives ∂2
implies that F (ρ, r, s) is strictly increasing in s, so the solution to (119) is unique.
stronger inequality

∂s2 F (ρ, r, s) ≥ 1 , which gives ∂2

∂s2 F (ρ, r, s) ≥ 1. This
In fact, we have the

F (ρ, r, s) − F (ρ, r, s(cid:48)) ≥

(s − s(cid:48))2,

∀ s(cid:48) > s.

(123)

1
2

By the implicit function theorem, the solution s(ρ, r) is also continuous in (ρ, r).

Lemma 16.

(a) If the condition

τ 2
n >

1
n

n
(cid:88)

i=1

max{−zi, 0}2

holds, then the solution (cid:101)α and optimal objective value are given by

Mn(ρ, r) =

αi =

1
n

1
n

n
(cid:88)

(zi + sn)+ + κ,

i=1

(cid:26)

max

1,

(cid:27)

−zi
sn

where sn is the solution to equation (119) with Q = Qn.

(b) If the reverse inequality

holds, then M (ρ, r) = −∞.

τ 2
n <

1
n

n
(cid:88)

i=1

max{−zi, 0}2

(124)

(125)

(126)

(127)

(c) If, instead, the equality holds, then Mn(ρ, r) = n−1(cid:104)1n, z+(cid:105) + κ, and the optimization problem does not

have a ﬁnite minimizer.

For convenience, we will henceforth denote the unique solution of (119) by sn for the empirical measure
Qn. If the solution of (119) does not exist, we will denote sn = −∞. We also denote the unique solution
of (119) by s for the population measure Q∞.

71

Proof of Lemma 16. Step 1: First we prove that the inequality (124) guarantees the existence of a (ﬁnite)
minimizer, and reverse of the inequality produces M (ρ, r) = −∞.

By the Cauchy-Schwarz inequality,

(cid:104) (cid:101)α, z(cid:105) +

(cid:112)

1 − ρ2 r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107) = (cid:104) (cid:101)α, z+(cid:105) − (cid:104) (cid:101)α, z−(cid:105) +

√

n τn(cid:107) (cid:101)α(cid:107)
√

≥ (cid:104) (cid:101)α, z+(cid:105) − (cid:107) (cid:101)α(cid:107)(cid:107)z−(cid:107) +
(cid:104)1n, z+(cid:105) + (cid:107) (cid:101)α(cid:107) · (cid:0)√
≥

n τn(cid:107) (cid:101)α(cid:107)
n τn − (cid:107)z−(cid:107)(cid:1).

1
n

(128)

If the condition (124) holds, then the objective value goes to +∞ as (cid:107) (cid:101)α(cid:107) → ∞, so a ﬁnite minimizer is
guaranteed to exist. However, if
n τn < (cid:107)z−(cid:107), then we can ﬁnd (cid:101)α with (cid:107) (cid:101)α(cid:107) → ∞ such that the objective
value tends to −∞. The reason is the following.

√

We choose (cid:101)αi = n−1 if zi ≥ 0 and (cid:101)αi = (−zi)γ if zi < 0 where γ > 0 is to be determined. Then, the

objective value is

(cid:104)1n, z+(cid:105) − γ(cid:107)z−(cid:107)2 +

√

n τn

(cid:0)γ2(cid:107)z−(cid:107)2 + n−2k(cid:1)1/2

1
n

where k is the number of i such that zi ≥ 0. As γ → ∞, we have γ−1(cid:0)γ2(cid:107)z−(cid:107)2 + n−2k(cid:1)1/2

→ (cid:107)z−(cid:107). Thus,

−γ(cid:107)z−(cid:107)2 +

√

n τn

(cid:0)γ2(cid:107)z−(cid:107)2 + n−2k(cid:1)1/2

→ −∞,

as γ → ∞.

The objective value goes to −∞ as (cid:107) (cid:101)α(cid:107) → ∞, so Mn(ρ, r) = −∞.
√

Step 2: If we have equality

n τn = (cid:107)z−(cid:107), then the same choice of (cid:101)α leads to

−γ(cid:107)z−(cid:107)2 +

√

n τn

(cid:0)γ2(cid:107)z−(cid:107)2 + n−2k(cid:1)1/2

=

n−2k(cid:107)z−(cid:107)
γ(cid:107)z−(cid:107) + (cid:0)γ2(cid:107)z−(cid:107)2 + n−2k(cid:1)1/2

where we used the simple identity a − b = (a2 − b2)/(a + b). As γ → ∞, the above expression tends to 0,
so the objective value tends to n−1(cid:104)1n, z+(cid:105). Also, the lower bound (128) shows that the objective value is
at least n−1(cid:104)1n, z+(cid:105). The equality condition for the Cauchy-Schwarz inequality is (cid:101)αi = τ (zi)− for some τ ,
which is only possible if zi = 0 whenever it is nonnegative. But zi (cid:54)= 0 almost surely, so we must have a
strict inequality in (128). Therefore, there is no ﬁnite minimizer in this case.

Step 3: Now we suppose that the condition (124) holds. The KKT conditions (complementary slackness)

give

This gives

√

√






(cid:101)αi
(cid:107) (cid:101)α(cid:107)
(cid:101)αi
(cid:107) (cid:101)α(cid:107)

n τn + zi ≥ 0,

n τn + zi = 0,

if (cid:101)αi =

if (cid:101)αi >

1
n
1
n

,

.

(cid:101)αi = max

(cid:26)

√

−(

n τn)−1zi(cid:107) (cid:101)α(cid:107),

(cid:27)

.

1
n

The value of (cid:107) (cid:101)α(cid:107) can be obtained from solving

(cid:107) (cid:101)α(cid:107)2 =

(cid:26)

√

−(

max

n
(cid:88)

i=1

n τn)−1zi(cid:107) (cid:101)α(cid:107),

(cid:27)2

.

1
n

For convenience, we (cid:101)sn =

√

n τn

n(cid:107) (cid:101)α(cid:107) . Then, the solution (cid:101)α is given by
(cid:27) (cid:105)
1
n

(cid:101)αi =

−zi
sn

max

1,

(cid:26)

(cid:104)

and (cid:101)sn > 0 is determined by

τ 2
n =

1
n

n
(cid:88)

i=1

max{(cid:101)sn, −zi}2.

72

This (cid:101)sn is exactly the solution to equation (119), namely (cid:101)sn = sn.
The above analysis proves the expression (126). It also leads to

Mn(ρ, r) =

1
n

n
(cid:88)

i=1

(cid:26)

zi max

1,

(cid:27)

+

−zi
sn

τ 2
n
sn

+ κ.

(129)

After simpliﬁcation, we obtain

Mn(ρ, r) =

1
n

n
(cid:88)

(zi + sn)+ + κ,

i=1

which is exactly the expression for Mn(ρ, r) in (125).

Next, we establish a convergence result, which is key to the asymptotic limit theorem.

Lemma 17. For (ρ, r) ∈ Ω≥, recall that s∗ = s∗(ρ, r) ≥ 0 is the solution to the equation (108). For
(ρ, r) ∈ Ω<, let s∗(ρ, r) = 0. Recall the deﬁnition Z = Zρ,r in (31) and

(a) Let K ⊂ Ω> be any compact set. Then,

M (ρ, r) = E(cid:2)(Z + s∗)+

(cid:3) + κ.

(cid:12)
(cid:12)sn − s∗

(cid:12)
(cid:12) = on,P(1),

sup
(ρ,r)∈K

sup
(ρ,r)∈K

(cid:12)Mn(ρ, r) − M (ρ, r)(cid:12)
(cid:12)

(cid:12) = on,P(1).

(b) Let K ⊂ Ω< be any compact set. Then,

P(cid:0)sn = −∞, ∀ (ρ, r) ∈ K(cid:1) = 1 − on,P(1), P(cid:0)Mn(ρ, r) = −∞, ∀ (ρ, r) ∈ K(cid:1) = 1 − on,P(1).

(c) Let ε > 0 be any constant that does not depend on n, d, ρ, r. Then, There exists an open set U ⊃ Ω=

such that

P(cid:0)sn ∈ [s∗ − ε, s∗ + ε] ∪ {−∞}, ∀ (ρ, r) ∈ U (cid:1) = 1 − on(1),
P(cid:0)Mn(ρ, r) ∈ [−∞, E[Z+] + κ + ε], ∀ (ρ, r) ∈ U (cid:1) = 1 − on(1).

(Note that E[Z+] + κ = M (ρ, r) if (ρ, r) ∈ Ω=.)

Proof of Lemma 17. For convenience, we will use ‘with high probability’ or simply w.h.p. to refer to an
event that happens with probability 1 − on(1). To emphasize the limit, we will write s∞ and s∗ interchange-
ably. We notice that sn has a trivial upper bound: in fact, from (119), we always have

τ 2
n ≥

1
n

n
(cid:88)

i=1

n = s2
s2
n

τ 2 ≥ E[s2

∞] = s2

∞.

(130)

Since (cid:107)h(cid:107)2/n

p
→ δ−1, we get sn ≤ r0

(cid:112)2(1 − ρ2)δ−1 ≤

√

2r0 δ−1/2 w.h.p.. Now we deﬁne

(cid:101)Fn(ρ, r, s) =

1
n

n
(cid:88)

i=1

(zi + s)+, Fn(ρ, r, s) =

1
n

n
(cid:88)

i=1

max{s, −zi}2

(cid:101)F (ρ, r, s) = E[(Z + s)+], F (ρ, r, s) = E(cid:2) max{s, −Z}2(cid:3).

Denote the set A = {(ρ, r, s) : |ρ| ≤ 1, r ∈ [0, 1 + εb], s ∈ [0,
numbers [NM94, Lemma 2.4], we have

√

2δ−1]}. Then, by the uniform law of large

max
(ρ,r,s)∈A

(cid:12) (cid:101)Fn(ρ, r, s) − (cid:101)F (ρ, r, s)(cid:12)
(cid:12)

(cid:12) = on,P(1),

max
(ρ,r,s)∈A

(cid:12)Fn(ρ, r, s) − F (ρ, r, s)(cid:12)
(cid:12)

(cid:12) = on,P(1).

(131)

73

(a) Suppose that (ρ, r) ∈ K ⊂ Ω>. To prove sup(ρ,r)∈K

(cid:12)
(cid:12)sn − s∞

(cid:12)
(cid:12) = on,P(1), we note that

τ 2
n

p
→ τ 2,

1
n

n
(cid:88)

i=1

max{−zi, 0}2 p

→ E(cid:2)Z 2; Z < 0(cid:3).

Fix an arbitrary ε(cid:48) > 0 that is independent of n. We set s = sn in (131) and use the deﬁnition of sn to
obtain

n ≤ E(cid:2) max{sn, −Z}2(cid:3) + ε,
τ 2

∀ (ρ, r) ∈ K

w.h.p.. Since τ 2

n ≥ τ 2 − ε w.h.p., we get

Similarly,

τ 2 − ε(cid:48) ≤ E(cid:2) max{sn, −Z}2(cid:3) + ε(cid:48),

∀ (ρ, r) ∈ K.

τ 2 + ε(cid:48) ≥ E(cid:2) max{sn, −Z}2(cid:3) − ε(cid:48),

∀ (ρ, r) ∈ K.

(132)

(133)

The above two inequalities, namely (132)–(133), imply

max
(ρ,r)∈K

(cid:12)F (ρ, r, sn) − F (ρ, r, s∞)(cid:12)
(cid:12)

(cid:12) ≤ 2ε(cid:48).

By the inequality in (123), we get max(ρ,r)∈K |sn − s∞| ≤ 2

√

ε(cid:48). This proves max(ρ,r)∈K |sn − s∞| = on,P(1).

Next, we bound the diﬀerence | (cid:101)Fn(ρ, r, sn) − (cid:101)F (ρ, r, s∞)|. By the triangle inequality,

sup
(ρ,r)∈K

(cid:12) (cid:101)Fn(ρ, r, sn) − (cid:101)F (ρ, r, s∞)(cid:12)
(cid:12)

(cid:12) ≤ sup

(ρ,r)∈K

(cid:12) (cid:101)Fn(ρ, r, sn) − (cid:101)F (ρ, r, sn)(cid:12)
(cid:12)

(cid:12) + sup

(ρ,r)∈K

(cid:12) (cid:101)F (ρ, r, sn) − (cid:101)F (ρ, r, s∞)(cid:12)
(cid:12)
(cid:12)

√

2δ−1] w.h.p., the ﬁrst term on the right-hand side is on,P(1) due to (131). We will show that
Since sn ∈ [0,
the second term is also on,P(1). Using the inequality |a+ − b+| ≤ |a − b|, we have, for any ﬁxed constant
ε(cid:48) > 0,

(cid:12) (cid:101)F (ρ, r, sn) − (cid:101)F (ρ, r, s∞)(cid:12)
(cid:12)

(cid:12) ≤ E(cid:2)|sn − s∞|(cid:3) ≤ ε(cid:48)P(cid:0)|sn − s∞| ≤ ε(cid:48)) + (τn + τ )P(cid:0)|sn − s∞| > ε(cid:48)(cid:1)

where we used (130). Since max(ρ,r)∈K |sn − s∞| = on,P(1), τn

p
→ τ and ε(cid:48) is arbitrary, we obtain

(cid:12) (cid:101)F (ρ, r, sn) − (cid:101)F (ρ, r, s∞)(cid:12)
(cid:12)

(cid:12) = od,P(1).

sup
(ρ,r)∈K

This then proves sup(ρ,r)∈K |Mn(ρ, r) − M (ρ, r)| = on,P(1).
(b) Suppose K ⊂ Ω<. Our goal is to show that w.h.p.

for all (ρ, r) ∈ K, there is no solution sn to (119)
with the empirical measure. Since τ (ρ, r) and E[Z 2; Z < 0] are both continuous in (ρ, r), by compactness we
must have

for all (ρ, r) ∈ K
for certain ε(cid:48) > 0. We set s = 0 in (131) and get E[Z 2; Z < 0] ≤ n−1 (cid:80)n
w.h.p. Using τn

p
→ τ , we obtain, w.h.p.,

E[Z 2; Z < 0] > τ 2 + 2ε(cid:48),

i=1(−zi)2

+ + ε(cid:48) for all (ρ, r) ∈ K

τ 2
n <

1
n

n
(cid:88)

i=1

max{0, −zi}2 ≤

1
n

n
(cid:88)

i=1

max{s, −zi}2,

for all s ≥ 0.

Therefore, w.h.p., there is no nonnegative solution to sn for all (ρ, r) ∈ K.

(c) Since s∞(ρ, r) is continuous in (ρ, r) and Ω= is compact, we can ﬁnd an open set U ⊃ Ω= relative to

[−1, 1] × [0, 1 + εb] such that s∞ ≤ ε/2.

Suppose (ρ, r) ∈ U . By (strict) monotonicity of F (ρ, r, s) in s from the proof of Lemma 15, we have

τ 2 < E(cid:2) max{ε, −Z}2(cid:3).

74

Then, w.h.p., we also have

τ 2
n <

1
n

n
(cid:88)

i=1

(cid:2) max{ε, −zi}2(cid:3).

Since Fn(ρ, r, s) is increasing in s, we must have w.h.p., sn = −∞ (no solution) or sn ≤ ε, where the latter
implies |sn − s∞| ≤ ε.

Also, we note that if sn = −∞ then by deﬁnition Mn(ρ, r) = −∞, and if |sn − s∞| ≤ ε, then

Mn(ρ, r) =

1
n

n
(cid:88)

(zi + sn)+ + κ ≤

i=1

1
n

n
(cid:88)

(zi)+ + sn + κ ≤

i=1

1
n

n
(cid:88)

i=1

(zi)+ +

3
2

ε + κ.

Since sup(ρ,r)∈U
2ε with ε, we prove claim (c) in this lemma.

i=1(zi)+ −E[Z+](cid:12)

(cid:12)
(cid:12) 1
n

(cid:80)n

(cid:12) = on,P(1) by (131), we obtain Mn(ρ, r) ≤ E[Z+]+2ε+κ. By replacing

Proof of Theorem D.1. Note that Part (a) is already proved in Lemma 15. Below we prove Part (b).

Step 1. Note that Z is continuous in (ρ, r). We also proved that s(ρ, r) is continuous in (ρ, r). This
implies (by dominated convergence theorem) that M (ρ, r) is uniformly continuous in (ρ, r) ∈ Ω≥. By
Lemma 17 (c) and continuity of M (ρ, r), we can choose an open set U ⊂ I × [0, r0] such that the we have
the conclusions of Lemma 17 (c) and E[Z+] + κ ≤ supΩ≥(I,r0)∩U M (ρ, r) + ε.

Setting K1 = Ω≥(I, r0) ∩ U c, K2 = Ω≤(I, r0) ∩ U c, and apply Lemma 17 (a)(b)(c) and ﬁnd the following

inequalities hold with high probability.

Mn(ρ, r) ≤ M (ρ, r) + ε,
Mn(ρ, r) = −∞,
Mn(ρ, r) ≤ supΩ≥(I,r0)∩U M (ρ, r) + ε, ∀(ρ, r) ∈ U.

∀ (ρ, r) ∈ K1,
∀ (ρ, r) ∈ K2,

Combining all three inequalities, we get that with high probability,

sup
ρ∈I,r∈[0,r0]

Mn(ρ, r) ≤

max
(ρ,r)∈Ω≥(I,r0)

M (ρ, r) + ε,

which is exactly M n ≤ M ∗ + ε with high probability.

Step 2. Now suppose that (ρ∗, r∗) ∈ Ω≥(I, r0) is a maximizer. We claim that we can ﬁnd a sequence

(ρk, rk) ∈ Ω>(I, r0) such that

(ρk, rk) = (ρ∗, r∗).

lim
k→∞

(134)

Once this claim is proved, then by continuity of M (ρ, r), we can ﬁnd a suﬃciently large k such that

M (ρk, rk) ≥ M (ρ∗, r∗) − ε/2 = M ∗ − ε/2.

We can ﬁnd a compact set K ⊂ Ω>(I, r0) such that (ρk, rk) ∈ K. Applying Lemma 17 (a), we ﬁnd that
with high probability,

sup
ρ∈I,r∈[0,r0]

Mn(ρ, r) ≥ sup

Mn(ρ, r) ≥ sup

M (ρ, r) − ε/2 ≥ M ∗ − ε.

(ρ,r)∈K

(ρ,r)∈K

which is M n ≥ M ∗ − ε. Combining with the conclusions in Step 1, we will obtain the result in Part (b).

Step 3. To prove the claim (134), let us consider the continuous map M : (ρ, r) → (ρ, ω) where

ω = (cid:112)1 − ρ2 r. The image of Ω≥(I, r0) under M is denoted by (cid:101)Ω≥(I, r0), which is given by

(cid:101)Ω≥(I, r0) =

(cid:110)

(ρ, ω) : ρ ∈ I,

ω2
r2
0

+ ρ2 ≤ 1,

(cid:111)

≥ (cid:107)Z−(cid:107)L2

ω
√
δ

−](cid:3)1/2
where (cid:107)Z−(cid:107)L2 = (cid:2)E[Z 2
satisﬁes |ρ| (cid:54)= 1, the map M has a continuous inverse.

with Z = ρY G + ωW − κ similarly as before. Since any (ρ, r) ∈ Ω≥(I, r0)

75

We observe that (cid:107)Z−(cid:107)L2 is convex in (ρ, ω):

in fact, taking any (ρ1, ω1) and (ρ2, ω2) and denoting

Zk = ρkY G + ωkW − κ (k = 1, 2), for any λ ∈ [0, 1] we have
(cid:13)L2 ≥ (cid:13)
(cid:13)L2 + (1 − λ)(cid:13)
≥ (cid:13)

(cid:13) max{−Z1, 0}(cid:13)
λ(cid:13)

(cid:13) max{−Z1, 0}(cid:13)

(cid:13) max{−λZ1, 0} + max{−(1 − λ)Z2, 0}(cid:13)
(cid:13) max{−λZ1 − (1 − λ)Z2, 0}(cid:13)

(cid:13)L2

(cid:13)L2

where we used a− + b− ≥ (a + b)− in the second inequality. Hence, (cid:101)Ω≥ is a convex set. For any (ρ, ω) ∈
(cid:101)Ω≥ with ω/
δ > (cid:107)Z−(cid:107)L2 such that
limk→∞(ρk, ωk) = (ρ, ω). Using the map M−1 we then obtain the claim (134).

δ = (cid:107)Z−(cid:107)L2, we can thus ﬁnd a sequence (ρk, ωk) ∈ (cid:101)Ω≥ with ω/

√

√

Proof of Theorem 4.3: ﬁrst claim. By the deﬁnition of δlin, for given δ < δlin, we have Ω>([−1, 1], 1) (cid:54)=
∅, and M∗(1 + ε(cid:48)) > M∗(1) for certain ε(cid:48) > 0. That Ω>([−1, 1], 1) is nonempty implies that Ω>([−1, 1], 1 + ε(cid:48))
is also nonempty. So we can apply Theorem D.1 twice where we set I = [−1, 1], and r0 = 1 and r0 = 1 + ε(cid:48)
respectively. The two diﬀerent choices of r0 are associated with two optimization problems given in (D.1);
correspondingly, let Mn and M (cid:48)
n. (Recall
that (cid:98)θ deﬁned in the algorithm is associated with Mn.)

be a maximizer associated with M (cid:48)

n be the maximum. Also, let (cid:98)θ

(cid:48)

By Theorem D.1(b), we have Mn = M∗(1) + on,P(1) and M (cid:48)

M∗(1), we have M (cid:48)
any λ ∈ [0, 1], consider the interpolant

n > Mn with high probability, which also implies that (cid:107)(cid:98)θ

n = M∗(1 + ε(cid:48)) + on,P(1). Since M∗(1 + ε(cid:48)) >
(cid:107) > 1 with high probability. For

(cid:48)

(cid:98)θλ = λ(cid:98)θ + (1 − λ)(cid:98)θ

(cid:48)

.

By linearity, we must have yi(cid:104)(cid:98)θλ, v(cid:105) ≥ κ. If (cid:107)(cid:98)θ(cid:107) < 1, then we can choose appropriate λ ∈ (0, 1) such that
(cid:107)(cid:98)θλ(cid:107) = 1 and (cid:104)(cid:98)θλ, v(cid:105) > Mn, which contradicts the deﬁnition of Mn. Hence, we must have (cid:107)(cid:98)θ(cid:107) = 1 with
high probability. This proves that (cid:98)θ is a κ-margin solution with high probability.

D.3 Analysis of maximization of M (ρ, r): Proof of Theorem 4.3 continued

From the deﬁning equation (108) of s∗, it is easy to derive an upper bound

∗ ≤ E(cid:2) max{s∗, −Z}2(cid:3) = (1 − ρ2)r2δ−1,
s2

thus

0 ≤ s∗ ≤

(cid:112)

1 − ρ2 δ−1/2.

Denote

q(κ) =

(cid:114) 2
π

1
|κ|

(cid:18)

exp

−

κ2
2

(cid:19)

(cid:18)

− α|κ|

,

q+(κ) = q(κ) exp

α|κ| +

(cid:19)

.

α2
2

Also denote the set

Ω≥,ρ(I, r0) = (cid:8)r ∈ [0, r0] : (ρ, r) ∈ Ω≥(I, r0)(cid:9).

(135)

(136)

(137)

It is useful to derive a rough range of the maximizer ρ∗ when optimizing M (ρ, r). The following lemma

gives a lower bound.
Lemma 18. Assume that δ satisﬁes |κ|8 ≤ δ ≤ (1 − ε)(cid:112)π/2|κ| exp(κ2/2 + α|κ|). Then, there exists a
suﬃciently negative κ(α, m, ε) such that for all κ < κ the following holds. The particular choice (ρ, r) =
(1 − |κ|−2, 1) satisﬁes (1 − ρ2)r2δ−1 > E[Z 2; Z < 0], and thus

max
(ρ,r)∈Ω≥(I,r0)

M (ρ, r) ≥

1 −

(cid:18)

(cid:19)

1
|κ|2

m,

where m := E[Y G]

(138)

and consequently, if (ρ∗, r∗) maximizes M (ρ, r) over Ω≥(I, r0), then we must have ρ∗ ≥ 1 − (1 + ε/2)|κ|−2.

Proof of Lemma 18. First we prove that under the choice (ρ, r) = (1 − |κ|−2, 1), we have (1 − ρ2)r2δ−1 >
E[Z 2; Z < 0] for suﬃciently negative κ. Note

(1 − ρ2)r2δ−1 = (2 − |κ|−2)|κ|−2δ−1 = 2(1 + ˘oκ(1))|κ|−2δ−1.

76

The condition δ ≤ (1 − ε)(cid:112)π/2|κ| exp(κ2/2 + α|κ|) is equivalent to q(κ) ≤ (1 − ε)δ−1. So by Lemma 24,

E(cid:2)Z 2; Z < 0(cid:3) =

2(1 + ˘oκ(1))
|κ|2

q(κ) ≤

2(1 − ε) · (1 + ˘oκ(1))
|κ|2

δ−1.

This proves the ﬁrst claim. The bound (138) then follows from

M (ρ, r) = E[(Z + s∗)+] − E(cid:2)(Z + s∗)−

(cid:3) ≥ E[Z] = ρm.

Moreover, by the condition δ ≥ |κ|8 and the naive bound (135) on s∗, we have s∗ ≤ δ−1/2 ≤ |κ|−4. The
upper bound on (141) is further bounded by ˘oκ(1) · |κ|−2. Thus,

M (ρ∗, r∗) ≤ ρ∗m + 2|κ|−4 + ˘oκ(|κ|−2).

Combining this with the lower bound (138), we must have ρ∗ ≥ 1 − (1 + ε/2)|κ|−2 for suﬃciently negative
κ.

Let L2 := L2(Q∞) be the space of square integrable functions on the population measure Q∞. For

A ∈ L2(Q∞), ρ ∈ [−1, 1], and r ≥ 0, we deﬁne

Ψ(A, ρ, r) = E[AZ] +

r(cid:112)1 − ρ2
√
δ

(cid:107)A(cid:107)L2 + κ .

(139)

Below we make use of this deﬁnition to prove monotonicity of M (ρ, r) in r.
Proposition D.1. Suppose that either of the two holds: (1) δ satisﬁes |κ|8 ≤ δ ≤ (1−ε)(cid:112)π/2|κ| exp(κ2/2+
α|κ|) and ρ ≥ 1 − (1 + ε/2)|κ|−2, (2) δ ≤ |κ|8. Also suppose that Ω≥,ρ(I, r0) is nonempty. Then, there exists
a suﬃciently negative κ = κ(α, m, ε) such that the following holds for all κ < κ:

(a) Ψ is strictly convex in A and

M (ρ, r) = min

A∈L2,A≥1

(cid:8)Ψ(A, ρ, r)(cid:9)

and the unique minimizer A∗ := A∗(ρ, r) is given by A∗ = s−1
solution in (15) (with Q = Q∞).

∗ max{s, −Z} where recall s∗ is the

(b) There exists ε(cid:48) := ε(cid:48)(κ, δ) > 0 such that for all r ∈ (0, 1 + ε(cid:48)), we have

∂Ψ
∂r

(cid:12)
(cid:12)
(cid:12)A=A∗

= (1 − ρ2)r

(cid:104) 1
δ

− P(s∗ + Z < 0)

(cid:105)

> 0.

(c) The function M (ρ, r) is strictly increasing in r ∈ [0, 1 + ε(cid:48)].

(d) Consequently, together with Lemma 18, we have that M∗(1 + ε(cid:48)) > M∗(1).
Proof of Proposition D.1. First, we note that r(cid:112)1 − ρ2 (cid:54)= 0, because otherwise Ω≥ is empty.

(a) Since (cid:107)A(cid:107)L2 is strictly convex in A and E[AZ] is linear in A, Ψ must also be strictly convex in A.

Similar to the proof of Lemma 16, we apply the KKT conditions and ﬁnd

The norm (cid:107)A∗(cid:107)L2 satisﬁes

A∗ = max (cid:8) − τ −1Z(cid:107)A∗(cid:107)L2, 1(cid:9).

(cid:107)A∗(cid:107)L2 =

(cid:13)
(cid:13)

(cid:13) max (cid:8) − τ −1Z(cid:107)A∗(cid:107)L2, 1(cid:9)(cid:13)
(cid:13)
(cid:13)L2

.

Comparing this with the deﬁnition of s∗, we ﬁnd s∗ = τ /(cid:107)A∗(cid:107)L2 , and thus A∗ = max{s∗, −Z}/s∗. Using
this to simplify Ψ(A∗, ρ, r), we get Ψ(A∗, ρ, r) = E[(Z + s∗)+] + κ = M (ρ, r) as desired.

(b) We calculate

∂Ψ
∂r

=

(cid:112)

1 − ρ2 E[AW ] +

(cid:112)1 − ρ2
√
δ

(cid:107)A(cid:107)L2.

77

At the minimizer A = A∗, we use Stein’s identity and (cid:107)A∗(cid:107)L2 = τ /s∗ to obtain

∂Ψ
∂r

(cid:12)
(cid:12)
(cid:12)A=A∗

= (1 − ρ2) · (−r) · P(s∗ < −Z) +

(cid:112)1 − ρ2
√
δ
s∗

τ = (1 − ρ2)r

− P(s∗ + Z < 0)

(cid:105)
.

(cid:104) 1
δ

We claim that, under the conditions of this proposition, for all ρ ∈ [−1, 1],

> P(cid:0)ρY G +

(cid:112)

1 − ρ2 W < κ(cid:1) =: q0.

1
δ

Once this claim is proved, we can ﬁnd a suﬃciently small constant ε(cid:48) > 0 (due to continuity) such that

> P(cid:0)ρY G +

(cid:112)

1 − ρ2 rW < κ(cid:1),

for all ρ ∈ [−1, 1], r ∈ [0, 1 + ε(cid:48)],

1
δ

which will imply ∂Ψ
∂r
ε)(cid:112)π/2|κ| exp(κ2/2 + α|κ|) and ρ ≥ 1 − (1 + ε/2)|κ|−2. By Lemma 24,

> 0 as desired. To prove this claim, ﬁrst consider the case |κ|8 ≤ δ ≤ (1 −

(cid:12)
(cid:12)
(cid:12)A=A∗

q0 ≤ (cid:0)1 + ˘oκ(1)) · q(κ) ≤ (cid:0)1 + ˘oκ(1)) · (1 − ε) · δ−1 < δ−1.

For the case c0 ≤ δ < |κ|8,

q0 ≤ (1 + ˘oκ(1)) · q(κ) exp (cid:0)α|κ| + α2/2(cid:1),

Since δ−1 ≥ |κ|−8, which vanishes only polynomially as κ → −∞, we must have q0 < 1/δ as well.

(c) First we ﬁx ρ ∈ [−1, 1] and r ∈ [0, 1 + ε(cid:48)]. By the strict monotonicity from (b), we can ﬁnd ηr > 0

such that Ψ(A, ρ, r) > Ψ(A, ρ, r(cid:48)) at A = A∗(ρ, r) for all r(cid:48) ∈ (r − ηr, r). Thus, we derive

M (ρ, r) = Ψ(A∗(ρ, r), ρ, r) > Ψ(A∗(ρ, r), ρ, r(cid:48))
≥ Ψ(A∗(ρ, r(cid:48)), ρ, r(cid:48)) = M (ρ, r(cid:48)).

Similarly, we have M (ρ, r) < M (ρ, r(cid:48)) for all r(cid:48) ∈ (r, r + η(cid:48)
r > 0. Thus, M (ρ, r) is strictly increasing
in r on every compact set in (0, 1 + ε(cid:48)). By continuity, we conclude that M (ρ, r) is strictly increasing in r
on [0, 1 + ε(cid:48)].

r) where η(cid:48)

(d) By Lemma 18, the maximizer (ρ∗, r∗) ∈ Ω≥(I, r0) satisﬁes ρ∗ ≥ 1 − (1 + ε/2)|κ|−2 if |κ|8 ≤ δ ≤
(1 − ε)(cid:112)π/2|κ| exp(κ2/2 + α|κ|). By (c), we must have r∗ = 1; moreover, M (ρ∗, 1 + ε(cid:48)) > M (ρ∗, r∗). Thus,
we must have M∗(∞) ≥ M∗(1 + ε(cid:48)) > M∗(1).

Once this proposition is proved, it leads to Theorem 4.3.

Proof of Theorem 4.3: second claim. First, we note that under the condition (36), the set Ω>([−1, 1], 1)
is nonempty due to Lemma 18. Using ε(cid:48) deﬁned in Proposition D.1, we have M∗(∞) ≥ M∗(1). Thus,
δlin ≥ (1 − ε)(cid:112)π/2 |κ| exp(κ2/2 + α|κ|) and so the second claim is proved.

D.4 Estimation error: Proof of Theorem 4.7

We note that part (a) of Theorem 4.7 is a direct consequence of Theorem D.1 (by setting r0 = 1 and taking
I = [−1, ρ∗ − ε0] and I = [ρ∗ + ε0] for any small constant ε0 > 0). Below we prove part (b) of Theorem 4.7.
Recall the deﬁnitions of q(κ) and q+(κ) in (136). Let us start with approximating s∗(ρ, r) and M (ρ, r)

with simpler functions, as stated in the next two lemmas.

Lemma 19. Let ε > 0 be a constant, and recall the deﬁnition of s∗ = s∗(ρ, r) in (108). Then, there exists
a suﬃciently negative κ = κ(α, m, ε) such that for all κ < κ the following holds for all (ρ, r) ∈ Ω≥(I, 1).
0 ≤ s∗ − (cid:112)(1 − ρ2)r2δ−1 − E[Z 2; Z < 0] ≤ 3δ−1/2(cid:112)(1 − ρ2)q+(κ),

(140)

and

0 ≤ E[(Z + s∗)−] ≤

2
|κ|

q+(κ).

(141)

If, in addition, ρ ≥ 1 − (1 + ε/2)|κ|−2, then q+(κ) can be replaced by q(κ) from the above bound.

78

Proof of Lemma 19. In the proof, we will write s = s∗ for simplicity. We will use the results from
Lemma 24:

sup
ρ∈[−1,1],r∈[0,1]

P(Z < 0) ≤ (1 + oκ(1)) · q+(κ),

sup
ρ∈[1−(1+ε/2)|κ|−2,1],r∈[0,1]

P(Z < 0) ≤ (1 + oκ(1)) · q(κ).

We can rewrite the equation (108) as

s2P(Z ≥ −s) = (1 − ρ2)r2δ−1 − E[Z 2; Z < −s]

= (1 − ρ2)r2δ−1 − E[Z 2; Z < 0] + E[Z 2; −s ≤ Z < 0].

Note that the third term on the last expression is bounded by

0 ≤ E[Z 2; −s ≤ Z < 0] ≤ s2P(Z < 0)

where we used the naive bound on s in the last inequality. Thus, by nonnegativity of E[Z 2; Z < 0] and the
simple inequality

b for a, b ≥ 0, we have

a + b ≤

a +

√

√

√

s(cid:112)P(Z ≥ −s) ≥ (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0],
s(cid:112)P(Z ≥ −s) ≤ (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0] +

(cid:112)

1 − ρ2 δ−1/2(cid:112)P(Z < 0).

We also have

and thus

P(Z ≥ −s) = 1 − P(Z ≤ −s) ≥ 1 − P(Z ≤ 0).

1 ≤ (cid:2)P(Z ≥ −s)(cid:3)−1/2

≤ (cid:2)1 − P(Z < 0)(cid:3)−1/2

= 1 + (1 + ˘oκ(1)) · ξ(ρ, r, b−1κ)/2 ≤ 1 + q+(κ).

Thus for suﬃciently negative κ we derive
s ≥ (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0],
s ≤ (cid:0)1 + q+(κ)(cid:1) ·
≤ (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0] + q+(κ)
≤ (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0] + (cid:112)(1 − ρ2)q+(κ) δ−1/2 + 2(cid:112)(1 − ρ2)q+(κ) δ−1/2
= (cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0] + 3(cid:112)(1 − ρ2)q+(κ) δ−1/2

1 − ρ2 δ−1/2(cid:112)2q+(κ)
(cid:112)

(cid:16)(cid:112)(1 − ρ2) r2δ−1 − E[Z 2; Z < 0] +

1 − ρ2 δ−1/2 + (1 + q+(κ)) ·

(cid:112)

(cid:112)

(i)

(cid:17)

1 − ρ2δ−1/2(cid:112)2q+(κ)

where (i) is because q+(κ) ≤ (cid:112)q+(κ) and 1 + q+(κ) ≤
additionally, then by Lemma 24, we can replace q+(κ) with q(κ).

√

2 for suﬃciently negative κ. If ρ ≥ 1 − (1 + ε/2)|κ|−2

Hereafter we will focus on r = 1, since by Proposition D.1 implies that M (ρ, r) is always maximized at
r = 1 for suﬃciently negative κ. By Lemma 24 (d)(e), the expectation term E[Z 2; Z < 0] has an asymptotic
expression (as κ → −∞) if ρ ≥ 1 − (1 + ε/2)|κ|−2, namely

sup
ρ≥1−(1+ε/2)|κ|−2

(cid:12)
(cid:12)
(cid:12)

E(cid:2)(ρY G1 +

(cid:112)

1 − ρ2 G2 − b−1κ)2
−

(cid:3) −

(cid:12)
2
(cid:12)
|κ|2 q(κ)
(cid:12) ≤

2ακ
|κ|2 q(κ)

(142)

for certain ακ ≥ 0 and ακ = ˘oκ(1). Let (ρ∗, 1) be any maximizer of M (ρ, r).

Using the above inequality and Lemma 19, we can approximate the function M (ρ, r). Recall that m =

E[Y G]. Deﬁne functions M 0, M+, and M− as follows.

M 0(ρ) = ρm + (cid:112)(1 − ρ2)δ−1 − E[Z 2; Z < 0],

∀ ρ : (ρ, 1) ∈ Ω≥(I, 1),






(cid:115)

M 1

±(ρ) = ρm +

(1 − ρ2)δ−1 − (1 ± ακ)

M 1(ρ) = ρm +

(cid:112)

1 − ρ2 δ−1/2,

2
|κ|2 q(κ),

if δ ≥ |κ|8

if δ < |κ|8.

(143)

(144)

79

Let J+, J− ⊂ [−1, 1] be the subsets in which M+(ρ) and M−(ρ), respectively, are well deﬁned (i.e., the
expressions in the square root are nonnegative).

The next simple lemma states that M 1

±(ρ) and M 1(ρ) are strictly concave functions, and their maximizers

have explicit expressions.

Lemma 20. There exists a suﬃciently negative κ = κ(α, m, ε) such that for all κ < κ the following holds.

(a) Suppose that δ satisﬁes |κ|8 ≤ δ ≤ (1 − ε)q(κ)−1. Then M 1(ρ) is a strictly concave function for ρ. Its

unique maximizer is given by

(cid:16)

1 −

ρ =

2(1 ± ακ)δ
|κ|2

(cid:17)1/2

q(κ)

·

(cid:16) m2δ

(cid:17)1/2

1 + m2δ

,

or equivalently,

1 − ρ2 =

1
1 + m2δ

+

m2δ
1 + m2δ

·

2(1 ± ακ)δ
|κ|2

q(κ),

(145)

and it holds that ρ ≥ 1 − (1 + ε/2)|κ|−2.

(b) Suppose that δ satisﬁes c ≤ δ ≤ |κ|8. Then M 1(ρ) is a strictly concave function and its unique

maximizer is given by

or equivalently,

ρ =

(cid:16) m2δ

(cid:17)1/2

1 + m2δ

1 − ρ2 =

1
1 + m2δ

.

(146)

Proof of Lemma 20. First consider the scenario in (a). Taking the derivative of M 1

±(ρ), we get

d
dρ

M 1

±(ρ) = m −

ρδ−1

(cid:0)(1 − ρ2)δ−1 − (1 ± ακ) 2

|κ|2 q(κ)(cid:1)1/2

.

(147)

Taking a further derivative, we have

d2
dρ2 M 1

±(ρ) = −

δ−2(cid:2)1 − (1 ± ακ) 2δ
(cid:0)(1 − ρ2)δ−1 − (1 ± ακ) 2

|κ|2 q(κ)(cid:3)

|κ|2 q(κ)(cid:1)3/2

(148)

which is negative for suﬃciently negative κ. This proves that M 1
equation dM 1
is similar.

±(ρ) is strictly concave. Solving the
±(ρ)/dρ = 0 for ρ, we obtain the desired expression (145). The derivation for the scenario (b)

Lemma 21. Then there exists a suﬃciently negative κ = κ(α, m, ε) such that for all κ < κ the following
holds.

(a) Suppose that δ satisﬁes |κ|8 ≤ δ ≤ (1 − ε)q(κ)−1. Let ρ1 be the unique minimizer of M 1

−(ρ). Then

there exists some βκ ≥ 0 satisfying βκ = ˘oκ(1), such that

M 1

−(ρ1) − M 1

−(ρ∗) ≤

(cid:112)q(κ).

βκ
|κ|

(b) Suppose that δ satisﬁes c ≤ δ ≤ |κ|8. Let ρ1 be the unique minimizer of M 1(ρ). Then,

M 1(ρ1) − M 1(ρ∗) ≤

3
|κ|

(cid:112)q+(κ).

80

(149)

(150)

Proof of Lemma 21. Let (cid:101)ρ1 be the unique minimizer of M 1
+(ρ). By Lemma 20 and the inequality (142),
we have ((cid:101)ρ1, 1) ∈ Ω≥ with suﬃciently negative κ. By the deﬁnition of ρ∗, the optimality of ρ∗ implies
M (ρ∗, 1) ≥ M ((cid:101)ρ1, 1).

By Lemma 19, we can approximate the function

M (ρ, 1) = E[(Z + s∗)] + E[(Z + s∗)−] + κ = ρm + s∗ + E[(Z + s∗)−]

using M 0(ρ); that is, we write

M (ρ, 1) = M 0(ρ) + M res(ρ),

where |M res(ρ)| ≤ 3δ−1/2(cid:112)(1 − ρ2)q+(κ) + 2|κ|−1q+(κ)

and q+(κ) can be replaced by q(κ) if ρ ≥ 1 − (1 + ε/2)|κ|−2.

(a) Since M 1

+(ρ) ≤ M 0(ρ) ≤ M 1

−(ρ) for any ρ such that these functions are well deﬁned (namely, the

expression inside the square root is nonnegative), we derive

M 1

−(ρ∗) ≥ M 0(ρ∗) ≥ M (ρ∗, 1) − |M res(ρ∗)| ≥ M 0((cid:101)ρ1) − |M res(ρ∗)| − |M res((cid:101)ρ1)|

(i)
≥ M 1

−((cid:101)ρ1) −

(cid:112)4ακq(κ)
|κ|

− |M res(ρ∗)| − |M res((cid:101)ρ1)|

(ii)
≥ M 1

√

−((cid:101)ρ1) − (2

ακ + oκ(1))

(cid:112)q(κ)
|κ|
√

where in (i) we used the simple inequality |
ε/2)|κ|−2. Also, we have

√

√

a −

M 1

−((cid:101)ρ1) ≥ M 1

−(ρ1) − (4

√

ακ + oκ(1))

We conclude that

M 1

−(ρ∗) ≥ M 1

−(ρ1) − (6

√

ακ + oκ(1))

(cid:112)q(κ)
|κ|

.

(cid:112)q(κ)
|κ|

.

b| ≤

a − b, and in (ii) we used min{ρ∗, (cid:101)ρ1} ≥ 1 − (1 +

(b) By Lemma 24, we get supρ∈[−1,1] |M 1(ρ) − M 0(ρ)| ≤ (cid:112)E[Z 2; Z < 0] ≤ 1.1(cid:112)q+(κ)/|κ| for suﬃciently

negative κ, so

M 1(ρ∗) ≥ M 0(ρ∗) −

≥ M 1(ρ1) −

1.1(cid:112)q+(κ)
|κ|
2.2(cid:112)q+(κ)
|κ|

≥ M 0(ρ1) −

1.1(cid:112)q+(κ)
|κ|

− oκ(1)

(cid:112)q+(κ)
|κ|

,

− |M res(ρ∗)| − |M res((cid:101)ρ1)|

which completes the proof.

The above lemma allows us to bound the diﬀerence ρ∗ − ρ1 through the concave function M 1

−(ρ) (or

M 1(ρ)).

Lemma 22.

(a) Suppose that δ satisﬁes |κ|8 ≤ δ ≤ (1−ε)q(κ)−1. Then there exists some γκ ≥ 0 satisfying

γκ = ˘oκ(1) such that

(cid:12)ρ1 − ρ∗(cid:12)
(cid:12)

(cid:12) < γκ ·

(cid:16) 1
δ

+

δq(κ)
|κ|2

(cid:17)

.

(b) Suppose that δ satisﬁes c ≤ δ ≤ |κ|8. Then there exists some γκ ≥ 0 satisfying γκ = ˘oκ(1) such that

(cid:12)ρ1 − ρ∗(cid:12)
(cid:12)

(cid:12) <

γκ
δ

.

Proof of Lemma 22. In this proof, for simplicity we deﬁne

a := a(κ, δ) =

(cid:112)δq(κ)
|κ|

,

b := b(κ, δ) = 1 −

(1 − ακ)2δq(κ)
|κ|2

.

81

Note that b satisﬁes b ≤ 1 − |κ|−2 = 1 − oκ(1). Recall that the expression of d2
optimality of ρ1, for any ρ ∈ J− we have

dρ2 M 1

−(ρ) is given in (148). By

M 1

−(ρ1) − M 1

−(ρ∗) ≥ inf

ρ∈[ρ1,ρ∗]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d2
dρ2 M 1

−(ρ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

·

(ρ1 − ρ∗)2
2

.

(1 − ρ2) − (1 − ακ)

Since

we obtain

2δ
|κ|2 q(κ) ≤ (1 − ρ2
1
1 + m2δ
b
1 + m2δ

≤

=

1) − (1 − ακ)

(cid:16)

1 −

2(1 − ακ)δ
|κ|2

+ 2|ρ1 − ρ|,

2δ
|κ|2 q(κ) + 2|ρ1 − ρ|
(cid:17)

q(κ)

+ 2|ρ1 − ρ|

inf
ρ∈[ρ1,ρ∗]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d2
dρ2 M 1

−(ρ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

δ−1/2b
(cid:2)b(1 + m2δ)−1 + 2|ρ1 − ρ∗|(cid:3)3/2

.

Therefore, we derive

2(M 1

−(ρ1) − M 1

−(ρ∗)) ≥ δ−1/2bf (|ρ1 − ρ∗|),

where f (t) :=

t2
(cid:2)b(1 + m2δ)−1 + 2t(cid:3)3/2

.

Note that f (t) is strictly increasing in t, since

f (cid:48)(t) =

t(t + 2b(1 + m2δ)−1)
2(cid:2)b(1 + m2δ)−1 + 2t(cid:3)5/2

> 0.

By Lemma 21, we get

2βκδ1/2
b|κ|

(cid:112)q(κ) ≥ f (|ρ1 − ρ∗|) =⇒ |ρ1 − ρ∗| ≤ f −1

(cid:18) 2βκa
b

(cid:19)

.

Let γκ := β1/3

κ

so it satisﬁes γκ = oκ(1). We claim that

f (cid:0)γκ max{δ−1, a2}(cid:1) >

2βκa
b

,

(151)

which, once proved, implies |ρ1 − ρ∗| ≤ γκ max{δ−1, a2} and thus the conclusion in (a). To prove the
claim (151), observe that

f (cid:0)γκ max{δ−1, a2}(cid:1) ≥

κ max{δ−1, a2}2
γ2
max{(1 + oκ(1))(1 + m2δ)−1, γκa2}3/2
(1 − oκ(1))γ2

κm3 max{δ−1, a2}2

max{δ−1, γκa2}3/2
κ1{δ−1 ≥ γκa2}δ−1/2 + γ1/2
γ2

κ 1{δ−1 ≥ γκa2}a + γ1/2
γ5/2

(cid:17)
κ 1{δ−1 < γκa2}a
(cid:17)
κ 1{δ−1 < γκa2}a

≥
≥ (1 − oκ(1))m3(cid:16)
≥ (1 − oκ(1))m3(cid:16)
≥ 1 − oκ(1))m3γ5/2
κ a

which is smaller than 2βκa/b when κ is suﬃciently negative. This proves the claim and thus proves part (a).

For part (b) Following the same strategy, we ﬁrst derive

inf
ρ∈[ρ1,ρ∗]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d2
dρ2 M 1(ρ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

δ−1/2
(cid:2)(1 + m2δ)−1 + 2|ρ1 − ρ∗|(cid:3)3/2

82

and so we have

6δ1/2
|κ|

(cid:112)q(κ) ≥ 2δ1/2(cid:0)M 1(ρ1) − M 1(ρ∗)(cid:1) ≥

(ρ1 − ρ∗)2
(cid:2)(1 + m2δ)−1 + 2|ρ1 − ρ∗|(cid:3)3/2

.

Similarly, the function (cid:101)f (t) =
negative κ we have

t2

[(1+m2δ)−1+2t]3/2 is strictly increasing in t. Taking γκ = 1/|κ|, for suﬃciently

f (γκ/δ) ≥ (1 − oκ(1))γ2

κm3δ−1/2 ≥

6δ1/2
|κ|

(cid:112)q(κ).

This implies that |ρ1 − ρ∗| ≤ γκ/δ as desired.

Finally, we put the pieces together to prove Theorem 4.7.

Proof of Theorem 4.7. As discussed in the beginning of this subsection, we only need to prove part (b)
of this theorem. We set

ρmin = ρ1 − γκ

(cid:16) 1
δ

+

δq(κ)
|κ|2

(cid:17)

,

I1 = [−1, ρmin],

(cid:16) 1
δ

+

δq(κ)
|κ|2

(cid:17)

,

ρmax = ρ1 + γκ

I2 = [ρmax, 1]

where γκ is given by Lemma 22. We apply Theorem D.1 with r0 = 1, and we set I = I1, I = I2, and
I = [−1, 1] respectively. Theorem D.1(c) implies that the maximizer (cid:98)θ must satisfy (cid:104)(cid:98)θ, θ∗(cid:105) ∈ I \ (I1 ∩ I2)
w.h.p. Thus, w.h.p.,

(cid:104)(cid:98)θ, θ∗(cid:105) = ρ1 + oκ(1) ·

(cid:18) 1
δ

+

δq(κ)
|κ|2

(cid:19)

.

We note that

E(κ) = (1 + oκ(1)) ·

(cid:18) 1

2m2δ

+

δq(κ)
|κ|2

(cid:19)

.

For case (a), if limκ→−∞ δ(κ) = ∞, then we have

(cid:16)

1 −

ρ1 =

(1 + oκ(1))δ
|κ|2

(cid:17)

q(κ)

(cid:16)

·

1 −

(cid:17)

1 + oκ(1)
2m2δ

= 1 − (1 + oκ(1)) · E(κ).

Therefore, w.h.p.,

(cid:12)
(cid:12)
(cid:12)(cid:104)(cid:98)θ, θ∗(cid:105) − (1 − E(κ))

(cid:12)
(cid:12)
(cid:12) = oκ(1) · E(κ).

(152)

For case (b), we have E(κ) = (1 + oκ(1))/(m2δ) and ρ1 = 1 − (1 + oκ(1))/(2m2ρ). This gives the same high
probability bound as (152).

D.5 Additional proofs
Proof of Lemma 13. For convenience, we introduce u := (cid:112)1 − ρ2 w ∈ Rd−1. Consider the convex set

Denote u = (ρ, u). Deﬁne two Gaussian processes as

C = (cid:8)(ρ, u) : ρ ∈ I, r−2

0 (cid:107)u(cid:107)2 + ρ2 ≤ 1(cid:9).

(cid:101)X; u, (cid:101)α(cid:1) = ρ(cid:104) (cid:101)α, y (cid:12) G(cid:105) + (cid:104) (cid:101)α, (cid:101)Xu(cid:105) − b−1κ(cid:104) (cid:101)α, 1n(cid:105) + b−1κ,

(cid:0)
Q1
(cid:0)g, h; u, (cid:101)α(cid:1) = ρ(cid:104) (cid:101)α, y (cid:12) G(cid:105) + (cid:107)u(cid:107)(cid:104) (cid:101)α, g(cid:105) + (cid:107) (cid:101)α(cid:107)(cid:104)u, h(cid:105) − b−1κ(cid:104) (cid:101)α, 1n(cid:105) + b−1κ.

Q2

For every integer k ≥ 1, deﬁne

Ak = min

(cid:101)α≥n−11n
(cid:107) (cid:101)α(cid:107)≤k

max
u∈C

Q1

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1),

Ak = min

(cid:101)α≥n−11n
(cid:107) (cid:101)α(cid:107)≤k

max
u∈C

Q2

(cid:0)g, h; u, (cid:101)α(cid:1).

83

Note that both minimization and maximization above are deﬁned over compact and convex constraint sets.
This allows us to apply [MM21, Corollary G.1], which yields

P(cid:0)Ak ≤ t(cid:1) ≤ 2P(cid:0)Ak ≤ t(cid:1),

P(cid:0)Ak ≥ t(cid:1) ≤ 2P(cid:0)Ak ≥ t(cid:1),

for all t ∈ R.

We take k → ∞ in the both two inequalities (by the monotone convergence theorem), and obtain

(cid:18)

(cid:18)

P

P

min
(cid:101)α≥n−11n

max
u∈C

Q1

min
(cid:101)α≥n−11n

max
u∈C

Q1

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1) ≤ t

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1) ≥ t

(cid:19)

(cid:19)

(cid:18)

(cid:18)

≤ 2P

≤ 2P

min
(cid:101)α≥n−11n

max
u∈C

Q2

min
(cid:101)α≥n−11n

max
u∈C

Q2

(cid:19)
(cid:0)g, h; u, (cid:101)α(cid:1) ≤ t
(cid:19)
(cid:0)g, h; u, (cid:101)α(cid:1) ≥ t

,

.

By Sion’s minimax theorem [Sio58], we can exchange the min with max for Q1 and obtain

min
(cid:101)α≥n−11n

max
u∈C

Q1

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1) = max

u∈C

min
(cid:101)α≥n−11n

Q1

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1) = max

ρ∈I

max
(cid:107)w(cid:107)≤r0

min
(cid:101)α≥n−11n

Q1

(cid:0)

(cid:101)X; u, (cid:101)α(cid:1) = M n.

It suﬃces to prove that we can also exchange the min with max for Q2, namely

min
(cid:101)α≥n−11n

max
u∈C

Q2

(cid:0)g, h; u, (cid:101)α(cid:1) = max

u∈C

min
(cid:101)α≥n−11n

Q2

(cid:0)g, h; u, (cid:101)α(cid:1),

(153)

since the right-hand side above is exactly Mn. It is clear that given the norm (cid:107)u(cid:107) = r, we have

max
u:(cid:107)u(cid:107)=r

Q2(g, h; u, (cid:101)α) = ρ(cid:104) (cid:101)α, y (cid:12) G(cid:105) + r(cid:104) (cid:101)α, g(cid:105) + r(cid:107) (cid:101)α(cid:107)(cid:107)h(cid:107) − b−1κ(cid:104) (cid:101)α, 1n(cid:105) + b−1κ

(154)

=: Q2

(cid:0)g, h; ρ, r, (cid:101)α(cid:1).

The right-hand side of (154) is linear in (ρ, r) over the compact and convex set C(cid:48) := {(ρ, r) : ρ ∈ I, (r/r0)2 +
ρ2 ≤ 1}. Thus,

min
(cid:101)α≥n−11n

max
u∈C

Q2

(cid:0)g, h; u, (cid:101)α(cid:1) = min
= max
(ρ,r)∈C(cid:48)

(cid:101)α≥n−11n

max
(ρ,r)∈C(cid:48)

Q2

min
(cid:101)α≥n−11n

Q2

(cid:0)g, h; ρ, r, (cid:101)α(cid:1)
(cid:0)g, h; ρ, r, (cid:101)α(cid:1)

where the the second equality is due to Sion’s minimax theorem. In order to prove (153), it remains to show,
for every ρ ∈ I,

max
√

(cid:107)u(cid:107)≤

1−ρ2 r0

min
(cid:101)α≥n−11n

Q2

(cid:0)g, h; u, (cid:101)α(cid:1) =

max
√

0≤r≤

1−ρ2 r0

min
(cid:101)α≥n−11n

Q2

(cid:0)g, h; ρ, r, (cid:101)α(cid:1).

It suﬃces to show that for every ﬁxed r ∈ [0, (cid:112)1 − ρ2 r0],

max
(cid:107)u(cid:107)=r

min
(cid:101)α≥n−11n

Q2

(cid:0)g, h; u, (cid:101)α(cid:1) = min

(cid:101)α≥n−11n

Q2

(cid:0)g, h; ρ, r, (cid:101)α(cid:1).

This is a consequence of the Cauchy-Schwarz inequality: the maximizer is given by u = rh/(cid:107)h(cid:107) regardless
of (cid:101)α. (A similar argument has already appeared in the proof of Lemma 14.) Thus this lemma is proved.
Lemma 23. Recall that we deﬁned Zρ,r = ρY G + (cid:112)1 − ρ2 rW in (31). Then, we have P(Zρ,r > 0) ≥ 1/2.

Proof of Lemma 23. We denote the density function of a normal variable by φ and the density function
of Y G by pY G. First, we claim that pY G(u) ≥ pY G(−u) holds for u ≥ 0. In fact, for u ≥ 0,

pY G(u) = φ(u)ϕ(u) + φ(−u)(1 − ϕ(−u)) = φ(u)(cid:0)1 + ϕ(u) − ϕ(−u)(cid:1)

(i)

≥ φ(−u)(cid:0)1 + ϕ(−u) − ϕ(u)(cid:1)
= pY G(−u)

84

where in (i) we used symmetry of φ and monotonicity of ϕ. Now using the density pY G we write P(Zρ,r > 0)
as

P(Zρ,r > 0) =

(cid:90) ∞

0

P(cid:0)ρu +

(cid:112)

1 − ρ2 rW > 0(cid:1)pY G(u) + P(cid:0) − ρu +

(cid:112)

1 − ρ2 rW > 0(cid:1)pY G(−u) du.

We use a similar decomposition and W d= −W for P(Zρ,r < 0), and ﬁnd

P(Zρ,r > 0) − P(Zρ,r < 0) =

(cid:90) ∞

0

(cid:104)
P(cid:0)ρu +

(cid:112)

1 − ρ2 rW > 0(cid:1) − P(cid:0) − ρu +

(cid:112)

1 − ρ2 rW > 0(cid:1)(cid:105)(cid:0)pY G(u) − pY G(−u)(cid:1) du

which is nonnegative. This proves P(Zρ,r > 0) ≥ P(Zρ,r < 0), which implies P(Zρ,r > 0) ≥ 1/2 since
P(Zρ,r = 0) = 0.

For ρ ∈ [−1, 1], r ∈ [0, 1], t ≥ 0, let us denote by ξ(ρ, r, −t) and ζ(ρ, r)

ξ(ρ, r, −t) = P(cid:0)ρY G +
(cid:112)
ζ(ρ, r) = E(cid:2)(cid:0)ρY G +

1 − ρ2 rW < −t(cid:1),
(cid:3).

1 − ρ2 rW − κ(cid:1)2

(cid:112)

−

The probability ξ(ρ, r, −t) is related to Lemma 6 in the following way. Denote

ρ(cid:48) =

ρ
(cid:112)ρ2 + (1 − ρ2)r2

,

t(cid:48) =

t
(cid:112)ρ2 + (1 − ρ2)r2

.

Then we can rewrite ξ(ρ, r, −t) as

ξ(ρ, r, −t) = P

(cid:16)

ρ
(cid:112)ρ2 + (1 − ρ2)r2

Y G +

(cid:112)1 − ρ2 r
(cid:112)ρ2 + (1 − ρ2)r2

W <

−t
(cid:112)ρ2 + (1 − ρ2)r2

(cid:17)

= P(cid:0)ρ(cid:48)Y G + (cid:112)1 − (ρ(cid:48))2 W < −t(cid:48)(cid:1).

(155)

(156)

(157)

If t → ∞ (and thus t(cid:48) → ∞), we can apply the tail probability Lemma 6 to ﬁnd an asymptotic expression
for ξ(ρ, r, −t).

Lemma 24. Let c > 0 be any constant. Then, there exists a sequence (βκ) with βκ > 0, and limκ→−∞ βκ = 0
such that the following holds.

(a) For all ρ ∈ [−1, 1], r ∈ [0, 1], and t ≥ |κ|,

(1 − βκ) · q(−t(cid:48)) ≤ ξ(ρ, r, −t) ≤ (1 + βκ) · q(−t) exp

(cid:16)

αt +

(cid:17)

;

α2
2

(b) For ρ ∈ [1 − c|κ|−2, 1], r ∈ [0, 1], and 2|κ| ≤ t ≤ |κ|,

ξ(ρ, r, −t) ≤ (1 + βκ) · q(−t);

(c) For all ρ ∈ [−1, 1], r ∈ [0, 1],

(cid:90) ∞

0

ξ(ρ, r, κ − t) dt ≤ (1 + βκ) ·

(cid:16)

exp

α|κ| +

q(κ)
|κ|

(cid:17)

;

α2
2

(d) For all ρ ∈ [−1, 1],

(e) For all ρ ∈ [1 − c|κ|−2, 1],

ζ(ρ, 1) ≥

1 − βκ
|κ|2 q(κ);

ζ(ρ, 1) ≤

1 + βκ
|κ|2 q(κ);

85

Proof of Lemma 24. In diﬀerent parts of this proof, we may choose diﬀerent sequence (βκ) for proving
inequalities (because in the end we can always take the maximum).
(a) In the tail probability Lemma 6, we have

Aρ,t ≤

(cid:114) 2
π

1
t

(cid:16)

−

exp

t2
2

+

(cid:17)
.

α2
2

We use the relationship (157) and apply Lemma 6 to obtain

ξ(ρ, r, −t) = (1 + ot(1)) · Aρ(cid:48),t(cid:48) ≤

1 + ot(1)
t(cid:48)

(cid:114) 2
π

(cid:16)

−

exp

(t(cid:48))2
2

+

(cid:17)

.

α2
2

Since t ≤ t(cid:48), we must have

ξ(ρ, r, −t) ≤

1 + ot(1)
t

(cid:114) 2
π

(cid:16)

−

exp

t2
2

+

(cid:17)

α2
2

= (1 + ot(1)) · q(−t) exp

(cid:16)

αt +

(cid:17)

.

α2
2

Note that κ → −∞ necessarily implies t → ∞, so we obtain the upper bound. We minimizer Aρ,t over ρ
and ﬁnd

Aρ,t ≥

1
t

(cid:114) 2
π

(cid:16)

−

exp

(cid:17)

− αt

= q(−t).

t2
2

This gives the lower bound.
(b) If ρ ≥ 1 − c|κ|−2, then for suﬃciently negative κ we have ρ ≥ η0, so by Lemma 6,

ξ(ρ, r, −t) ≤

1 + ot(1)
t(cid:48)

(cid:114) 2
π

(cid:16)

−

exp

(t(cid:48))2
2

− αρ(cid:48)t(cid:48) +

α2(1 − (ρ(cid:48))2)
2

(cid:17)

.

Note that t(cid:48) ≥ t, ρ(cid:48) ≥ ρ, so

α2(1 − (ρ(cid:48))2)
2

≤

α2(1 − ρ2)
2

≤ α2(1 − ρ) ≤

cα2
|κ|2 = oκ(1).

ρ(cid:48)t ≥ ρt ≥ t −

ct
|κ|2 ≥ t − oκ(1).

Thus we get

ξ(ρ, r, −t) ≤

1 + oκ(1)
t

(cid:114) 2
π

(cid:16)

−

exp

(cid:17)

− αt

t2
2

= (cid:0)1 + oκ(1)(cid:1) · q(−t).

(c) We use the conclusion of (a) and Gaussian tail probability inequality (Lemma 12) to derive

(cid:90) ∞

0

ξ(ρ, r, κ − t) dt ≤ (1 + oκ(1)) ·

(cid:90) ∞

(cid:16)

−

exp

≤

1 + oκ(1)
|κ|

exp

|κ|
(cid:16)

−

|κ|2
2

+

du · exp

(cid:17)

(cid:16) α2
2

(cid:17)

(cid:17)

u2
2
α2
2

= (1 + oκ(1)) ·

(cid:16)

exp

α|κ| +

q(κ)
|κ|

(cid:17)

.

α2
2

(d) We can express ζ(ρ, 1) as

ζ(ρ, 1) =

(cid:90) ∞

0

2tξ(ρ, 1, κ − t) dt.

(158)

Note that with r = 1, we have t(cid:48) = t. Using the lower bound from (a), we derive

ζ(ρ, 1) ≥ (1 − βκ) ·

(cid:90) ∞

2tq(κ − t) dt

= (1 − βκ)

0

(cid:114) 2
π

(cid:90) ∞

·

0

2t
|κ| + t

(cid:18)

exp

−

(|κ| + t)2
2

(cid:19)

− α(t + |κ|)

dt

86

= 2(1 − βκ)

(cid:18)

exp

−

(cid:114) 2
π
(cid:114) 2
π

|κ|
b

|κ|2
2b2 − α
(cid:18)
|κ|2
2b2 − α

−

(cid:19)

0

·

|κ|
b

=

2b3(1 − βκ)
|κ|3

exp

(cid:90) ∞

(cid:19)

·

t
|κ| + t

(cid:18)

exp

−

t2
2

(cid:19)

− t|κ| − αt

dt

(cid:90) ∞

0

u

1 + b2u|κ|−2 exp

(cid:18)

−

b2u2
|κ|2 − u −

αbu
|κ|

(cid:19)

du

where we used change of variable u = t|κ| in the last equality. Since

lim
κ→−∞

(cid:90) ∞

0

u

1 + b2u|κ|−2 exp

(cid:18)

−

b2u2
|κ|2 − u −

αbu
|κ|

(cid:19)

du =

(cid:90) ∞

0

u exp(−u) du = 1

by the dominated convergence theorem. Thus,
(cid:114) 2
π

2(1 − oκ(1))
|κ|3

ζ(ρ, 1) ≥

(cid:18)

exp

−

|κ|2
2

(cid:19)

− α|κ|

=

2(1 − oκ(1))
|κ|

q(κ).

(e) We use the identity (158) and will show

ζ1 :=

ζ2 :=

(cid:90) |κ|

0
(cid:90) ∞

|κ|

2tξ(ρ, 1, κ − t) dt ≤

2tξ(ρ, 1, κ − t) dt ≤

2(1 + oκ(1))
|κ|

q(κ).

oκ(1)
|κ|

q(κ).

Using the upper bound in (b) and following a similar derivation as in (d), we have

(cid:18)

exp

−

ζ1 ≤

2b3(1 + βκ)
|κ|3

≤

2(1 + oκ(1))
|κ|3

(cid:114) 2
π
(cid:114) 2
π

|κ|2
2b2 − α
|κ|2
2

(cid:18)

exp

−

(cid:19)

=

− α|κ|

0
2(1 + oκ(1))
|κ|

q(κ).

(cid:90) b−2|κ|2

(cid:19)

·

|κ|
b

u

1 + b2u|κ|−2 exp

(cid:18)

−

b2u2
|κ|2 − u −

αbu
|κ|

(cid:19)

du

Moreover, because

2t
|κ| + t

(cid:18)

exp

−

(|κ| + t)2
2

(cid:19)

(cid:18)

− α(t + |κ|)

≤ 2 exp

−

(|κ| + t)2
2

(cid:19)

,

together with the Gaussian tail probability inequality (Lemma 12), we obtain

ζ2 ≤ (1 + βκ)

(cid:114) 2
π

(cid:90) ∞

(cid:18)

exp

−

2|κ|

(cid:19)

u2
2

du ≤ (1 + oκ(1))

(cid:114) 2
π

exp (cid:0)−2|κ|2(cid:1) =

oκ(1)
|κ|

q(κ).

This completes the proof.

E Gradient descent: Proof of Theorem 5.1

Proof of Theorem 5.1. Without loss of generality, we may assume yi = 1 and recast yixi as xi. To begin
with, we calculate the Hessian of (cid:98)Rn,κ(θ):

∇2 (cid:98)Rn,κ(θ) =

1
n

n
(cid:88)

i=1

(cid:96)(cid:48)(cid:48) ((cid:104)xi, θ(cid:105) − κ (cid:107)θ(cid:107)2)

(cid:18)

xi − κ

θ
(cid:107)θ(cid:107)2
(cid:32)

(cid:19) (cid:18)

xi − κ

(cid:19)(cid:62)

θ
(cid:107)θ(cid:107)2

−

κ
(cid:107)θ(cid:107)2

1
n

n
(cid:88)

i=1

(cid:96)(cid:48) ((cid:104)xi, θ(cid:105) − κ (cid:107)θ(cid:107)2)

Id −

(cid:33)

.

θθ(cid:62)
(cid:107)θ(cid:107)2
2

Hence, if (cid:107)θ(cid:107)2 ≥ r > 0, one gets the following estimate:
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)∇2 (cid:98)Rn,κ(θ)
(cid:13)
(cid:13)
(cid:13)op

θ
(cid:107)θ(cid:107)2

xi − κ

n
(cid:88)

(cid:19) (cid:18)

β
n

xi − κ

≤

(cid:18)

i=1

θ
(cid:107)θ(cid:107)2

(cid:19)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

+

|κ|
r

sup
u∈R

|(cid:96)(cid:48)(u)|

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Id −

θθ(cid:62)
(cid:107)θ(cid:107)2
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

87

(cid:32)

≤β

1
n

(cid:13)
(cid:13)X (cid:62)X
(cid:13)

(cid:13)
(cid:13)
(cid:13)op

+ 2

|κ|
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:33)

+ κ2

+

xi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

|κ|
r

sup
u∈R

|(cid:96)(cid:48)(u)|.

Now if M is chosen to be greater than the above quantity, then (cid:98)Rn,κ(θ) is M -smooth in Rd\Bd(0, r), where
Bd(0, r) is the ball of radius r in Rd. Assume η < 2/M and (cid:13)
(cid:13)2 ≥ r for large enough
t and therefore Lemma 10 from [SHN+18] tells us ∇ (cid:98)Rn,κ(θt) → 0 as t → ∞.
(cid:13)θt(cid:13)

(cid:13)2 → ∞, we have (cid:13)

Denote Rt = (cid:13)

(cid:13)2 , mit = (cid:104)θt/ (cid:13)

(cid:13)θt(cid:13)

(cid:13)θt(cid:13)

(cid:13)θt(cid:13)

(cid:13)2 , xi(cid:105) − κ, then we have
(cid:43)

, ∇ (cid:98)Rn,κ(θt)

= lim
t→∞

1
n

n
(cid:88)

i=1

(cid:42)

θt
(cid:13)θt(cid:13)
(cid:13)
(cid:13)2

lim
t→∞

(cid:96)(cid:48)(Rtmit)mit = 0.

Suppose there exists ε > 0 such that for some 1 ≤ i ≤ n, mitk ≤ −ε for a subsequence tk → ∞. It follows
that

0 = lim
tk→∞

n
(cid:88)

i=1
(cid:18)

(cid:96)(cid:48)(Rtk mitk )mitk = lim
tk→∞





(cid:88)

mitk ≥0

(cid:96)(cid:48)(Rtk mitk )mitk +

(cid:96)(cid:48)(Rtk mitk )mitk





(cid:88)

mitk <0

≥ lim sup
tk→∞

−ε(cid:96)(cid:48)(−Rtk ε) − n sup
m≥0

|m(cid:96)(cid:48)(Rtk m)|

= −ε lim

u→−∞

(cid:96)(cid:48)(u) > 0,

(cid:19) (i)

where (i) is due to our assumption on (cid:96)(x), Deﬁnition 5 and the fact Rtk → ∞. Therefore, a contradiction
occurs and we complete the proof.

88

