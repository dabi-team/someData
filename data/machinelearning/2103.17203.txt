2
2
0
2

b
e
F
8
1

]
L
M

.
t
a
t
s
[

2
v
3
0
2
7
1
.
3
0
1
2
:
v
i
X
r
a

Universal Prediction Band via Semi-Deﬁnite Programming

Tengyuan Liang ∗1

1University of Chicago, Booth School of Business

Abstract

We propose a computationally eﬃcient method to construct nonparametric, heteroscedastic
prediction bands for uncertainty quantiﬁcation, with or without any user-speciﬁed predictive
model. Our approach provides an alternative to the now-standard conformal prediction for
uncertainty quantiﬁcation, with novel theoretical insights and computational advantages. The
data-adaptive prediction band is universally applicable with minimal distributional assumptions,
has strong non-asymptotic coverage properties, and is easy to implement using standard convex
programs. Our approach can be viewed as a novel variance interpolation with conﬁdence and
further leverages techniques from semi-deﬁnite programming and sum-of-squares optimization.
Theoretical and numerical performances for the proposed approach for uncertainty quantiﬁcation
are analyzed.

Keywords— Uncertainty quantiﬁcation, heteroscedasticity, nonparametric prediction band, semi-deﬁnite

programming, sum-of-squares.

1 Introduction

A plausible criticism from the statistics community to modern machine learning is the lack of rigorous uncer-
tainty quantiﬁcation, with perhaps the exception in conformal prediction [Vovk et al., 2005, Lei et al., 2018,
Romano et al., 2019]. Instead, the machine learning community would argue that conventional uncertainty
quantiﬁcation based on idealized distributional assumptions may be too restrictive for real data. Neverthe-
less, without a doubt, uncertainty quantiﬁcation for predictive modeling is essential to statistics, learning
theory, and econometrics. This paper will resolve the above inference dilemma by introducing a new method
with provable uncertainty quantiﬁcation via semi-deﬁnite programming. This paper provides an alternative
approach to the now-standard conformal prediction for uncertainty quantiﬁcation, with novel theoretical
insights and computational advantages. The proposed method learns a data-adaptive, heteroscedastic pre-
diction band that is: (a) universally applicable without strong distributional assumptions, (b) with desirable
theoretical coverage with or without any user-speciﬁed predictive model, and (c) easy to implement via
standard convex programs (when used in conjunction with a wide range of positive-deﬁnite kernels).

Let (x, y)

R be the covariates and response data pair drawn from an unknown probability
∈ X ×
. There are plenty of regression or predictive models—denoted by m0(x)—that estimate
distribution
P
m(x) := E[y
x = x] suﬃciently well with ﬁnite data. However, to make downstream decisions reliable, a
|
m0(x)
y
with provable coverage is urgently needed.
good prediction band quantifying the uncertainty in
|
|
The prediction band is of particular relevance to complex machine learning models that construct m0(x) in
a less transparent way, such as deep neural networks and boosting machines. This paper makes progress
in ﬁlling in such a gap: we estimate a nonparametric, heteroscedastic prediction band (cid:98)PI(x) that enjoys

−

∗tengyuan.liang@chicagobooth.edu.

1

 
 
 
 
 
 
provable coverage with minimal data assumptions for any predictive model. Our approach can be viewed
as a novel variance interpolation with conﬁdence and leverages techniques from sum-of-squares relaxations
for nonparametric variance estimation. On a non-technical level, this paper enriches the toolbox of applied
researchers with a theoretically justiﬁed new methodology for uncertainty quantiﬁcation and visualization,
as in conformal prediction.

1.1 Semi-deﬁnite Programs and Prediction Bands

We introduce our procedure for constructing the predictive band in this section. Let K(
be a continuous symmetric and positive-deﬁnite kernel function. Given n data pairs
corresponding kernel matrix K
the following semi-deﬁnite program (SDP)

R
) :
,
X × X →
·
·
n
i=1 and the
(xi, yi)
}
{
Sn×n with Kij = K(xi, xj), our prediction band is constructed based on

∈

min
B
s.t.

Tr(KB)

Ki, BKi
(cid:104)
B
0

(cid:23)

(yi

−

(cid:105) ≥

m0(xi))2, i = 1, . . . , n

(1)

R
where the optimization variable B
Rn denotes the i-th column of the kernel matrix K.
is a given predictive model (user-speciﬁed), and Ki
∈
Given the estimated (cid:98)B, the prediction band, (cid:98)PI(x) that maps each x to an interval, can be constructed
accordingly

Sn×n is a symmetric positive semi-deﬁnite (PSD) matrix, m0(
·

X →

) :

∈

−
Kx, (cid:98)BKx
(cid:104)

(cid:98)PI(x, δ) :=

(cid:104)

m0(x)

(cid:112)

(1 + δ)

· (cid:98)v(x) , m0(x) +

(cid:112)

(1 + δ)

(cid:105)

,

· (cid:98)v(x)

x

∀

∈ X

,

where (cid:98)v(x) :=
and Kx := [K(x, x1), . . . , K(x, xn)](cid:62)

(cid:105)

,

Rn ,

(2)

∈

R being a scalar quantifying conﬁdence. δ can be later calibrated for exact coverage control, and
m0(xi). A few

with δ
may be set as 0 if n is large. Here (cid:98)v(x) estimates the variability in the “deviations” ei := yi
remarks on such deviations are in place.

−

∈

• First, ei’s can be computed based on any user-speciﬁed predictive model m0(x) that estimates the

conditional mean m0(x)

x = x], be it accurate or not.
|
• Second, in the absence of such a predictive model for the conditional mean, one can set m0(x)

≈

E[y

and learn a conditional second-moment function to assess uncertainty.

0

≡

• Last, as shown next in (3), in practice, one can simultaneously learn the conditional mean and variance
functions using a variant of the above SDP. Therefore, a pre-speciﬁed model m0(x) is not required.
Let K m and K v specify two kernel functions, corresponding to the conditional mean and variance func-
Sn×n denote empirical kernel matrices on ﬁnite data with size n. For any
0, the following convex SDP program constructs the prediction band and the conditional mean function

∈

tions respectively. Km, Kv
γ
simultaneously

≥

(cid:10)α, Kmα(cid:11) + Tr (cid:0)KvB(cid:1)

min
α,B

s.t.

γ

·
(cid:10)Kv
B

i , BKv
i
0

(cid:0)yi

(cid:11)

≥

−

(cid:10)Km

i , α(cid:11)(cid:1)2

, i = 1, . . . , n

(3)

Sn×n and α

∈

∈

Rn. Given the solution (cid:98)B and (cid:98)α, the (cid:98)PI(x) is

(cid:23)
where the optimization variables are B
constructed as

(cid:104)

(cid:112)

(cid:98)m(x)
(cid:98)PI(x, δ) :=
−
i , (cid:98)α(cid:11) and (cid:98)v(x) := (cid:10)Kv
where (cid:98)m(x) := (cid:10)Km

· (cid:98)v(x) , (cid:98)m(x) +
x, (cid:98)BKv
x

(1 + δ)

(1 + δ)

· (cid:98)v(x)

(cid:105)

,

x

∀

∈ X

,

(cid:112)

(cid:11) .

2

1.2 A Numerical Illustration

Before diving into the motivations behind the above SDPs (Sec. 1.3) and corresponding theory (Sec. 2), let
us ﬁrst visually illustrate the empirical performance of the constructed prediction bands on a toy numerical
example. A complete simulation study comparing our methods and conformal predictions will be deferred
to Section 3.1. Details of the data generating processes will be elaborated therein as well. The quick exercise
is to showcase that convex programs (1) and (3) are easy to implement using standard optimization toolkits
(say, CVX [Grant and Boyd, 2014]), and construct ﬂexible prediction bands with desired coverage properties.
As a motivating example, we try out the SDP (3), which simultaneously estimates the conditional mean and
variance functions. A minimal 10-line Python implementation is provided in Listing. 1.

(a)

(b)

Figure 1: From left to right: SLR, SDP1, and SDP2. For each plot, Blue dots denote training data {(xi, yi)}n
i=1, Blue line
denotes the estimated conditional mean (cid:98)m(x), and Blue band denotes the estimated prediction band (cid:98)PI(x). Red dots represent
the unknown test distribution, and Red line denotes the true conditional mean m(x) = E[y|x = x]. Here the training and test
data share the same conditional distribution y|x = x and thus m(x). The training and test data are shared in three plots. A
good coverage corresponds to when Blue band covers essentially all Red dots. Statistics are summarized in Table 1.

The ﬁrst example is a linear model with heteroscedastic error: the conditional mean m(x) being a
linear function and variance v(x) being a quadratic (with the conditional variance generated from a uniform
distribution). We generate a training dataset of size n = 40 and compare the coverage among three methods:
(a) SLR, simple linear regression, (b) SDP1, a SDP (3) with linear kernels K m and K v for both mean and
variance functions, and (c) SDP2, a SDP (3) with a linear K m and a (degree-3) polynomial kernel K v. The
coverage is compared on the same test dataset of size N = 800. Here γ = 0.1. See Fig. 1a for details and
Table. 1 for coverage statistics.

The second example is a non-linear, heteroscedastic error model: mean m(x) and variance v(x) functions
lying in a reproducing kernel Hilbert space (RKHS) with a radial basis function (rbf) kernel. Here n = 60
training samples and N = 800 test samples are generated. Three methods being compared are: (a) SLR,
(b) SDP1, rbf kernel for K m and linear kernel for K v, and (c) SDP2, rbf kernels for both K m and K v,
summarized in Fig. 1b and Table. 1. Here γ = 1.

These two numerical examples are minimal yet informative. In Fig. 1a, SLR misspends a wide prediction
bandwidth on data where the conditional variances are small yet fails to capture the large conditional variance
cases, resulting in the overall coverage of 86% and a median bandwidth of 8.21. SDP1/SDP2 re-distributes
the bandwidth budget leveraging the heteroscedastic nature and achieves an improved coverage 91%/94%,
with a smaller median bandwidth of 7.47/7.30. Such an eﬀect is even more pronounced in Fig. 1b. Observe
ﬁrst that in SDP2, the prediction band constructed by (3) almost perfectly contours the heteroscedastic
variances, thus achieving a > 99% prediction coverage with a merely 3.35 median bandwidth, in contrast
to SLR with a 96% coverage and a 4.80 bandwidth. Second, a better conditional variance estimate also

3

2.50.02.5x105051015y2.50.02.5x2.50.02.5x202x321012345y202x202xTable 1: Simulated examples

Coverage Median Len Average Len

MSE

Fig. 1a: linear m(x), quadratic v(x)

SLR
SDP1
SDP2

85.88%
91.13%
94.00%

8.2057
7.4689
7.2962

8.2658
0.6294
7.7173 0.1146
0.1720
8.3361

Fig. 1b: rbf m(x), rbf v(x)

SLR
SDP1
SDP2

96.13%
99.25%
99.50%

4.8048
4.4138
3.3488

0.2556
4.8185
4.6196
0.1916
3.7506 0.1670

improves performance in learning the conditional mean, as seen in the diﬀerences between Blue lines and
Red lines. The errors are also numerically summarized in the column “MSE” of Table 1. Leveraging the
heteroscedasticity in data, our prediction band distributes the bandwidth in a data-adaptive way, thus
improving the overall coverage.

1.3 Sum-of-Squares, Interpolation and Connections to Literature

The SDPs proposed in (1) and (3) are inspired by recent advancements in optimization and learning theory.
We will elaborate on the connections to related works and explain the innovations in our approach. We start
with some basic observations about the SDPs. First, when α is not an optimization variable, (3) recovers
(1). Second, the constraints set of (3) is always non-empty since α = 0, B = maxi

I is feasible.

Kv
i (cid:107)
(cid:107)

−2y2
i ·

Sum-of-Squares and Phase Retrieval As shown in Prop. 3, the inﬁnite-dimensional SDP with a
nuclear-norm minimization is equivalent to (3),

min
β∈Hm, A:Hv→Hv

s.t.

A
(cid:107)

(cid:63)
(cid:107)

γ

β

2
Hm +
· (cid:107)
(cid:107)
xi, Aφv
φv
(cid:104)
A
0

xi(cid:105)

(cid:23)

Hv

(cid:0)yi

φm
xi, β

Hm

(cid:105)

− (cid:104)

(cid:1)2

,

≥

i .

∀

H

H

m,

v denote two RKHSs where the conditional mean and variance functions reside. φxi ∈ H

is
Here
the feature map w.r.t. the Hilbert space
H is the Hilbert space inner-product. We call it the
·(cid:105)
inﬁnite-dimensional SDP since the optimization variables (β, A) are (function, operator) rather than ﬁnite-
dimensional (vector, matrix). A few remarks are in place. First, if the kernel K m is universal,
Hm is
(cid:105)
dense in L2 and hence can universally approximate all conditional mean function m(x). Second, as for the
m(x)(cid:1)2
conditional variance which has positivity constraints over a continuum x
,
we relax the positivity constraints using a sum-of-squares form

φm
(cid:104)
v(x) = (cid:0)y

with 0

x , β

∈ X

,
(cid:104)·

and

H

≤

−

0

x, Aφv
φv
x(cid:105)

≤ (cid:104)

Hv = (cid:0)y

m(x)(cid:1)2

, for some A

−

0 .

(cid:23)

(4)

It turns out that when K v is universal, the above sum-of-squares function can approximate all smooth,
positive functions [Feﬀerman and Phong, 1978, Bagnell and Farahmand, 2015, Marteau-Ferey et al., 2020],
thus explaining the name “universal” in the title. Remark that sum-of-squares optimization [Lasserre, 2001]
for nonparametric estimation has recently been considered; see [Bagnell and Farahmand, 2015, Marteau-Ferey
et al., 2020, Curmei and Hall, 2020]. The further relaxation changing from equality in (4) to inequality will be
discussed in the next paragraph. Last, the minimum nuclear norm objective translates to a particular form

4

of “minimum bandwidth” in the prediction band as v(x) =
bands that shelter the data, (3) aims to ﬁnd the one with minimum bandwidth.

x, Aφv
φv
x(cid:105)
(cid:104)

Hv . In language, for all prediction

The curious reader may wonder where the nuclear norm

(cid:63) arises from. The ﬁrst reason is conceptual:
(cid:107)
the nuclear norm is a relaxation for rank, and the procedure is to minimize the number of factors (rank)
that explain the variance. The second reason is a connection to phase retrieval: specify K m(x, x(cid:48))
0 and
K v(x, x(cid:48)) =
x = x), and force the inequality constraints to be equal, our SDP
(cid:105)
in (3) is equivalent to phase retrievel

(the linear kernel with φv

x, x(cid:48)

≡

(cid:104)

A
(cid:107)

min
A(cid:23)0 (cid:107)

A

(cid:63), s.t.
(cid:107)

(cid:104)

xi, Axi

(cid:105)

= y2
i ,

i = 1, . . . , n .

∀

Conceptually, the minimum nuclear norm procedure estimates the smallest number of factors that could
generate the variance.

∞

Min-norm Variance Interpolation with Conﬁdence Now we discuss the tuning parameter γ
∈
[0,
] and reveal the connection to the recent min-norm interpolation literature [Liang and Rakhlin, 2020,
Bartlett et al., 2020, 2021, Ghorbani et al., 2020, Montanari et al., 2020, Liang and Recht, 2021]. In the
0, (3) reduces to the familiar min-norm interpolation with kernel Km (whenever it has full rank,
limit of γ
→
since optimal B = 0)

min
α

s.t.

(cid:10)α, Kmα(cid:11)
0 = (cid:0)yi

−

(cid:10)Km

i , α(cid:11)(cid:1)2

,

i .

∀

In the limit of γ

, (3) reduces to (since optimal α = 0)

→ ∞

min
B
s.t.

Tr(KvB)

Kv
(cid:104)
B

i , BKv
0

i (cid:105) ≥

(cid:23)

y2
i ,

i .

∀

it trades oﬀ the conditional mean m(x) and
Now it is clear what the role of the tuning parameter γ is:
variance v(x) to explain the variability in y’s witnessed on the data. A small γ aims to use a complex mean
m(x) and a parsimonious variance v(x) to explain the overall variability, and vice versa.

From the above discussion, it is also clear that the SDP (3) can be viewed as a min-norm variance

interpolation with conﬁdence. Instead of having the typical equality constraints in interpolation

(cid:10)Kv

i , BKv
i

(cid:11) = (cid:0)yi

(cid:10)Km

i , α(cid:11)(cid:1)2

,

−

which violates the disciplined convex programming ruleset (due to the quadratic form on the RHS), we
further relax to inequality constraints to incorporate additional “conﬁdence” (and to make the problem
convex at the same time)

(cid:0)yi
As we shall see, the notion of conﬁdence in this variance interpolation is closely related to the notion of
margin in classiﬁcation [Bartlett et al., 1998, Liang and Sur, 2020].

i , α(cid:11)(cid:1)2

i , BKv
i

(cid:10)Km

(cid:10)Kv

≥

−

(cid:11)

.

Support Vector Regression We illustrate that minor modiﬁcations to our SDP formulation lead to
other problems, including support vector regression and kernel ridge regression. Specify the variance kernel
as the trivial one K v(x, x(cid:48)) = 1(x = x(cid:48)), then the decision variable B only matters in its diagonal component,
and our SDP (3) reduces to the kernel ridge regression

min
α

γ

· (cid:104)

α, Kmα

+

(cid:105)

n
(cid:88)

i=1

(cid:0)yi

−

(cid:10)Km

i , α(cid:11)(cid:1)2

.

5

Moreover, a slight modiﬁcation of (3) is to use the absolute deviation rather than the squared deviation in
the constraints, namely

In this case support vector regression is exactly our procedure with the speciﬁcation K v(x, x(cid:48)) = 1(x = x(cid:48)),

(cid:10)Kv

i , BKv
i

(cid:11)

(cid:12)
(cid:12)yi

−

≥

(cid:10)Km

i , α(cid:11)(cid:12)
(cid:12) .

min
α

γ

· (cid:104)

α, Kmα

+

(cid:105)

n
(cid:88)

i=1

(cid:12)
(cid:12)yi

−

(cid:10)Km

i , α(cid:11)(cid:12)
(cid:12) .

In summary, our SDP generalize beyond support vector machines, with the new non-trivial variance compo-
nent for rigorous uncertainty quantiﬁcation for heteroscedastic data.

1.4 Literature Review

There are increasingly many approaches proposed to address the uncertainty quantiﬁcation dilemma in
machine learning due to its signiﬁcance and centrality. However, very few methods are theoretically grounded
and universally applicable to the best of our knowledge. Many approaches are merely heuristics or data
visualization tools. This section divides related theoretical studies in the literature into two categories and
discusses how our method signiﬁcantly diﬀers from them and could potentially lead to a stronger theory.

Conformal Prediction Based on the exchangeability of data and a user-speciﬁed nonconformity mea-
sure, Vovk et al. [2005], Shafer and Vovk [2008] pioneered the ﬁeld of conformal prediction, which uses past
data to determine precise levels of conﬁdence in new predictions. To some extent, the elegant theory of
conformal prediction, motivated by online learning and sequential prediction, resolved the uncertainty quan-
tiﬁcation dilemma. The conformal prediction algorithm (see, for instance Shafer and Vovk [2008] Section
, and for each possibility, cal-
4.3) usually requires to enumerate over all the possibilities of z = (x, y)
culate n nonconformity measures via the leave-one-out method. Therefore, the total budget is n
,
× |Y| × |X |
which can be expensive for continuous y and multi-dimensional x. Much of the above computation can be
saved if additional information about the metric structure in x
can be leveraged. In contrast, our SDP
, and
approach constructs the prediction band over all the x’s at once, leverages the metric structure in
suﬀers at most a computational budget of n2. An additional key feature in our approach is in the coverage
theory established in Theorem 1: the prediction band has coverage probability > 95% on a new data point
n
(x, y), for 99.9999% dataset
i=1 of size n drawn from the same distribution. Such a distinction on
}
“conﬁdence” vs. “probability” is discussed extensively in Section 2.2 of Shafer and Vovk [2008].

(xi, yi)
{

∈ X × Y

∈ X

X

There has been a vast line of recent work on extending the conformal prediction idea further to address
the bottlenecks above in the regression setting. The body of work proliferates, and we certainly cannot do
justice here. Lei et al. [2018] alleviates the computational burden of the conformal prediction by introducing
the sample-splitting technique. Remarkably, theory on the bandwidth is also studied in Lei et al. [2018], thus
providing an angle to probe the statistical eﬃciency. Romano et al. [2019] studies the problem that existing
conformal methods can form nearly constant or weakly varying bandwidth and provide conservative results.
Romano et al. [2019] proposes conformalized quantile regression to address this issue. One shared feature of
our SDP approach is that the prediction band is fully adaptive to heteroscedasticity. Finally, we would like to
emphasize that conformal prediction constructs prediction bands in a numerical, black-box fashion without
a structural understanding of the variance function. In contrast, our SDP approach provides a transparent
and eﬃcient way of learning the variance function, a complementary contribution to the conformal literature.

Residual Subsampling and Quantile Regression An alternative approach for uncertainty quan-
tiﬁcation that leverages the metric structure in x
is to resample the residuals locally. Typically, this is
done by ﬁrst ﬁtting a predictive model m0(x), and deﬁning a local neighborhood around a new data x, then
subsampling the residuals for uncertainty quantiﬁcation via (conditional) quantiles. The validity of the above
approach crucially depends on how many “similar residuals” to pool information from. However, the curse

∈ X

6

of dimensionality comes in since data points are far from each other in high dimensions, posing challenges
in pooling the residuals. One can also use either the obtained residuals or the original responses y to ﬁt
a conditional quantile regression model [Koenker and Bassett Jr, 1978, Koenker and Hallock, 2001, Belloni
ξ(xi)(cid:1) where τ
and Chernozhukov, 2011, Belloni et al., 2019], (cid:98)ξτ (
(0, 1) is a
) := arg minξ
−
·
R is the estimated
R+ is the tilted absolute value function, and (cid:98)ξτ (
) : R
) :
quantile parameter, ρτ (
·
·
conditional quantile function. However, it is not guaranteed that over all x
, the estimated conditional
quantile function satisﬁes (cid:98)ξτ1(x) < (cid:98)ξτ2(x) for two quantiles τ1 < τ2. In other words, it is entirely possible
that for several x’s, the conditional prediction intervals are empty [Chernozhukov et al., 2010].

i=1 ρτ

X →

(cid:0)yi

∈ X

(cid:80)n

→

1
n

∈

2 Theory for Uncertainty Quantiﬁcation

In this section, we develop a theory for the coverage property of the prediction band constructed above,
under the mild assumption that the data are i.i.d. drawn with (x, y)
. To highlight the main arguments
in a simple form, let us consider the setting m0(x)
0. Otherwise, the same proof follows by replacing y
with y

m0(x). Deﬁne the corresponding prediction band, with a conﬁdence parameter δ
(cid:104)

(0, 1]

∼ P

≡

−

∈

(5)

(cid:98)PI(x, δ) =

(cid:112)

(1 + δ)

±

(cid:105)
· (cid:98)v(x)

.

We need the following assumptions before stating the theorem, where (x, y)
universal constant.
[S1] (Kernel and RKHS) The continuous symmetric kernel K is positive deﬁnite and satisﬁes supx∈X K(x, x)
N

and C > 0 denotes a

∼ P

satisfy λj(

)

Cj−τ , j

C. In addition, eigenvalues of the associated integral operator
for some constant τ > 1.

≤

∈

:

≤

T

H → H
T
(0, 1), ξ > 0 such that P (cid:2)y2 > ξ

∈

K(x, x)

·

x =

|

[S2] (Non-trivial uncertainty) There exist constants η

x(cid:3) > η holds for all x

.

∈ X

[S3] (Non-wild uncertainty) There exists a constant ω > 0 such that P (cid:2)y2 > t

K(x, x)(cid:3) < exp(

−

·

Ctω) for

all t

1.

≥

T

∞

) <

Discussion of Assumptions All the above assumptions are mild. The eigenvalue decay in [S1] is
(bounded trace integral operator). [S2] is also minimal, since it is only not
almost identical to Tr(
x = x. [S3] is the most stringent one, which requires the variability of
true when there is no variability in y
|
y to exhibit a certain tail-decay. For small ω
(0, 1), [S3] can be much milder than exponential tail-decay.
x = x satisﬁes [S3] with arbitrarily large ω or ω = 2, respectively. With some extra
Bounded or Gaussian y
work, [S3] can be relaxed to the case of a suﬃciently rapid polynomial tail-decay.
[S2] can be relaxed to
x = x(cid:3) = 1.
restricting only to x with P (cid:2)y2 > 0
Theorem 1. Deﬁne the objective value of the SDP in (1)

∈

|

|

(cid:100)Optn := min
B
s.t.

Tr(KB)

Ki, BKi
(cid:104)
B
0

(cid:23)

(cid:105) ≥

y2
i , i = 1, . . . , n .

Assume that [S1]-[S3] hold. For any δ
coverage guarantee holds,

∈

(0, 1], the following non-asymptotic, data-dependent prediction band

P
(x,y)∼P

(cid:2)y

(cid:98)PI(x, δ)(cid:3)

∈
≥
and (cid:100)Optn ≤
(xi, yi)
}
{

(cid:113) Cτ,ξ,η,ω·log(n)
n

1)

,

1

δ−1((cid:100)Optn ∨

−
(cid:2) log(n)(cid:3)cω ,

with probability at least 1
in [S1]-[S3].

−

n−10 on

n
i=1. Here the constants Cτ,ξ,η,ω, cω only depend on parameters

7

2.1 What does the Theorem Entail

A few remarks are in order before we sketch the proof of Theorem 1.

Coverage First, the above theorem says that the prediction band constructed using the SDP based on a
dataset of size n, will correctly cover a fresh data point (x, y)
drawn from the same distribution, with
a non-asymptotic coverage probability (on the new data x, y)

∼ P

(cid:113)

log3(n)
n

.

δ−1

1

−

(cid:98)PI(x)(cid:3) = 2.45
O(cid:63)(

With δ = 0.5, the bandwidth Length(cid:2)
(cid:98)v(x) is at a heteroscedastic level adaptive to x with
n ). Here O(cid:63) hides polylog factors. The coverage can
corresponding coverage probability at least 1
be arbitrary close to 1 with large n without the need of increasing δ, which is in clear distinction to the
conventional wisdom that coverage 1 can only be possible with an increasing δ regardless of n. Again,
n−10 of the datasets
we emphasize that the above coverage guarantee holds essentially on 99.9999%
i=1. In Section 2.2, we propose a rigorous calibration algorithm to choose δ(cid:63)(α) to achieve a constant
n
(xi, yi
{
}
coverage level 1

(0, 1).

(cid:112)
(cid:113) 1

(cid:28)

−

−

α

1

−

∈

Optimality If one wishes to obtain the classic 95% coverage probability, then choosing δ = O(cid:63)(
suﬃces, which translates to

(cid:113) 1
n )

Length(cid:2)

(cid:98)PI(x)(cid:3) = (cid:0)1 + O(cid:63)(

(cid:113) 1

n )(cid:1)

(cid:112)

(cid:98)v(x) .

·

Recall that in classic simple linear regression, the prediction interval is of length

(cid:114)

(cid:0)1 +

1

n + (x−¯x)2

Σi(xi−¯x)2

(cid:1)

·

3.92ˆs

(cid:113) Σi ˆe2

with ˆs =
ﬂuctuation seems to indicate the optimality of our Theorem (in terms of the dependence on n).

n−2 being the estimated residual standard error. The fact that (6) and (7) share the

i

(6)

(7)

(cid:113) 1
n

Data Adaptivity Curiously, the objective value of the convex optimization program quantiﬁes the
uncertainty of the prediction band: a smaller (cid:100)Optn implies (a) a better conﬁdence/coverage guarantee and
(b) a narrower prediction band overall. More importantly, the (cid:100)Optn can be calculated directly from data! We
ﬁnd such an optimization/inference interface exciting: the data-adaptive bound lets us know the coverage
guarantee speciﬁc to the current dataset. Put diﬀerently, the convex program constructs the prediction
band via its solution and at the same time, reveals the conﬁdence via its objective value. Since (cid:100)Optn is a
function of the dataset, our Theorem reveals which dataset allows for a better prediction band. Remark that
2
(cid:100)Optn =
(cid:63) is also a particular norm of the heteroscedastic variance function, quantiﬁed by the nuclear
(cid:107)
φx, Aφx
norm of the associated PSD operator (cid:98)A
H. Curiously, a simpler variance function
(cid:98)v(x) (with a small norm) will simultaneously result in a narrower band and better coverage. We emphasize
that the above discussion is in sharp contrast to the conventional wisdom that a narrow band usually leads
to poor coverage guarantees.

0 with (cid:98)v(x) =

(cid:107)(cid:98)v(
)
·

(cid:23)

(cid:104)

(cid:105)

2.2 Calibration and Coverage Control

One nice feature about conformal prediction is that it directly operates on a user-speciﬁed coverage level
(e.g., 95%), albeit the resulting procedure only achieves some form of coverage guarantee in the marginal
sense. In contrast, the coverage level guarantee of the current SDP approach in Theorem 1 is in an inequality

8

form with a mild dependence on the non-explicit universal constant Cγ,ξ,η,ω; however, the coverage is in a
n
stronger conditional sense conditioned on
i=1. The constant won’t signiﬁcantly aﬀect the coverage
}
guarantee in the large n setting; nevertheless, curious readers may wonder if more transparent control could
be achieved by tuning δ. This section provides a theoretically justiﬁed calibration procedure in choosing δ
(0, 1), as in conformal prediction.
(in the SDP band) to control coverage at a user-speciﬁed level 1
Such ﬁne calibration on δ can be helpful numerically in the moderate n and constant α setting (e.g., 5%).

(xi, yi)
{

−

∈

α

The calibration idea is based on sample-splitting. Split the samples into two parts, the training set
n
i=1 and the calibration set
(xi, yi)
. The
{
}
training set will be used to construct prediction band (cid:98)PI(
, δ). The calibration data set will be used to choose
·
δ to calibrate coverage.

m
j=1, with in total n + m data points drawn i.i.d. from

j, y(cid:48)
x(cid:48)
j}
{

P

Calibration Procedure Suppose that there exsits large enough constants ∆, L > 0 such that

P
(x,y)∼P

(cid:2)y

∈

(cid:98)PI(x, ∆)(cid:3) = 1 ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d
dδ

P
(x,y)∼P

(cid:2)y /
∈

(cid:12)
(cid:12)
(cid:98)PI(x, δ)(cid:3)
(cid:12)
(cid:12)

< L .

The calibration uses a dyadic search to select δ(cid:63)(α)
calibration is to ensure P(x,y)∼P

(cid:98)PI(x, δ(cid:63)(α))(cid:3)

(cid:2)y

1, ∆] with the set

[
−
α.

∈
1
−

j, y(cid:48)
x(cid:48)
j}

m
j=1. The goal of the

{

≥

∈

Algorithm 1: Calibration for Coverage Control

m
j=1, coverage level 1

α;

−

{

j, y(cid:48)
x(cid:48)
Data: Calibration set
j}
Result: Calibrated δ(cid:63)(α) ;
Initialize δ =
(cid:80)m
while 1
m
δ
←
end
return δ(cid:63)(α)

−
j=1
;

1;
1[y(cid:48)

(cid:98)PI(x(cid:48)

δ+∆
2

j /
∈

δ.

←

j, δ)] > 3

4 α, do

The following result derives the theoretical ground for the calibration procedure. We defer the proof to

Section A.2.

Lemma 2 (Calibration). Consider the calibration procedure in Algorithm 1 and L, ∆, α speciﬁed therein. If
the size of the calibration set m is large enough such that

(cid:114)

log (cid:0)(cid:100)log2

(cid:0) 2L(∆+1)
α

m

(cid:1)(cid:101)+1(cid:1)+10 log(m)

1
4 α ,

≤

then the calibrated δ(cid:63)(α) satisﬁes the coverage control

P
(x,y)∼P

(cid:2)y

∈

(cid:98)PI(x, δ(cid:63)(α))(cid:3)

1

−

≥

α ,

with probability at least 1

−

2m−10 on the calibration set

j, y(cid:48)
x(cid:48)
j}
{

m
j=1.

2.3 Intuition and Proof Sketch

We ﬁrst explain the key intuition before presenting the details of the proof sketch. The proof ﬁrst leverages a
representation theorem that relates the ﬁnite-dimensional (kernelized) SDP to an inﬁnite-dimensional SDP
to decouple the dependencies among xi’s. Next, we propose to use empirical process theory to analyze the
prediction coverage, inspired by the margin-based analysis originally done in analyzing classiﬁcation. Finally,

9

in controlling the uniform deviations between empirical and population coverage, we use properties of the
PSD operators and conditional quantile functions. The additional calibration procedure in ﬁne-tuning the
conﬁdence parameter δ(cid:63)(α) for a ﬁxed coverage level 1
(0, 1) also hinges on uniform deviation arguments.
−
Our analysis fundamentally relies on empirical process theory and is crucially diﬀerent from conformal
prediction analysis (based on exchangeability). Since the SDP provides a rigorous coverage guarantee as
conformal prediction, we hope the new proof idea opens new doors to study uncertainty quantiﬁcation.

∈

α

Now we sketch the proof of Theorem 1. Observe that by deﬁnition
(cid:2)1(y−2

(cid:98)PI(x, δ)(cid:3) = E

(LHS) := P

(cid:2)y /
∈

(x,y)∼P

(x,y)∼P

(cid:98)v(x) < 1

1+δ )(cid:3) .

Deﬁne a hinge function hδ(t) : t

max

(cid:55)→

and thus

1+δ
δ (1
−
{
1(t < 1
1+δ )

t), 0
}
hδ(t),

, we have

≤

R ,

t
∀

∈

(LHS)

E
(x,y)∼P

≤

(cid:2)hδ(y−2

(cid:98)v(x))(cid:3) .

(8)

φx
y ∈ H
to its nuclear-norm radius

lies in the RKHS, and A :
k :=

Deﬁne a real positive function (indexed by A) on the data z = (x, y), fA(z) : z

H . Here
is a PSD operator. Deﬁne a sequence of function spaces according
N and

for all k

2k

(cid:55)→

A

(cid:10) φx

y , A φx

y

(cid:11)

≤
With the Prop. 3 establishing the equivalence between the kernelized SDP and the inﬁnite-dimensional
k with

φxj , we know that y−2

A
(cid:107)
F
(cid:98)v(x) = f (cid:98)A(z). There exists a k

(cid:107)
≤
N such that f (cid:98)A ∈ F

i,j (cid:98)Bijφxi ⊗

fA :
{

.
1
}

0 :=

(cid:63)
(cid:107)

(cid:63)
(cid:107)

F

∈

∈

}

H → H
fA : 2k−1 <
{

SDP, (cid:98)A = (cid:80)
2k−1

≤

(cid:100)Optn < 2k, and thus we continue to bound
(cid:2)hδ
◦
f (cid:98)A(z)(cid:3)
◦
(cid:125)
(cid:123)(cid:122)
(i)

E
(x,y)∼P

(cid:98)E(cid:2)hδ
(cid:124)

(LHS)

≤

≤

f (cid:98)A(z)(cid:3)

+ sup
f ∈Fk
(cid:124)

(cid:0)E

(cid:98)E(cid:1) [hδ
−
(cid:123)(cid:122)
(ii)

f ]

.

◦

(cid:125)

For term (i), recall the optimality condition of (1),

which further implies hδ

Ki, (cid:98)BKi
(cid:105) ≥
(cid:104)
f (cid:98)A(zi) = 0 for all i = 1,

◦

· · ·

y2
i ⇔

f (cid:98)A(zi)

1

≥

, n. Therefore term (i) is zero.

For term (ii), we will use the high probability symmetrization in Prop. 4. Introduce i.i.d. Rademacher
k0 such that 2k0 = [log(n)]cω ,
variables
where we use the upper estimate on (cid:100)Optn obtained in Prop. 5, which is implied by Assumption [S3]. With
probability at leat 1

n
1 independent of the data. Note that we only need to consider k
}

t) on the data

2 exp(

(cid:15)i
{

≤

k0

zi

−

−

n
1 , uniformly over all k
}
n
(cid:88)

f (cid:1)(zi) + (iii)

(cid:0)hδ

(cid:15)i

≤

i=1

◦

{
1
n

E
{(cid:15)i}n
1

sup
f ∈Fk

Lip(hδ)

E
{(cid:15)i}n
1

·

sup
f ∈Fk

1
n

n
(cid:88)

i=1

(cid:15)if (zi) + (iii)

(ii)

2

2

·

·

≤

≤

= 2(1+δ)
δ

E
{(cid:15)i}n
1

sup
(cid:107)A(cid:107)(cid:63)≤2k

(cid:10) 1
n

n
(cid:88)

i=1

(cid:15)i

φxi
yi ⊗

φxi
yi

, A(cid:11) + (iii)

2(1+δ)
δ

≤

2k E
{(cid:15)i}n
1
(cid:124)

(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:15)i

φxi
yi ⊗

φxi
yi

(cid:13)
(cid:13)op

+(iii)

(cid:123)(cid:122)
(iv)

(cid:125)

10

where the last step follows from the duality between the nuclear norm and operator norm. Before getting into
the deviation term (iii) (originated by Prop. 4, formally upper bounded in (15)), ﬁrst recall 2k
1),
we know

2((cid:100)Optn ∨

≤

(ii)

≤

4(1+δ)
δ

((cid:100)Optn ∨

1)

·

(iv) + (iii) .

Similarly, by Prop. 4, the deviation term (iii) can be bounded by 6((cid:100)Optn ∨
k0 = cω log log(n).

(9)

1)

supx,y (cid:107)

·

φx
2
H ·
y (cid:107)

(cid:113) k0+t

n with

To bound the expected operator norm for the above random matrix, namely term (iv), we rely on matrix

Bernstein’s inequality plus a truncation technique. Observe that

E
(cid:15)

(cid:2)(cid:15) φx

y ⊗

(cid:3) = 0, and

φx
y

(cid:15) φx
(cid:107)

y ⊗

φx
y (cid:107)

op

sup
x,y (cid:107)

φx
y (cid:107)

≤

2
H a.s. ,

and that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:104)(cid:0)(cid:15)i

E
(cid:15)

φxi
yi ⊗

φxi
yi

(cid:1)2(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

(cid:0) sup
x,y (cid:107)

φx
2
H
y (cid:107)

(cid:1)2

·

≤

n .

t
n

n ∨

(cid:1) with probability 1

Naively applying the matrix Bernstein inequality, one would expect the term (iv) to behave like supx,y (cid:107)
(cid:0)(cid:113) t
(cid:15)i
{

φx
2
H·
y (cid:107)
n
1 . This is educative yet wrong, since dim(φ) is inﬁnity.
}
To make things rigorous, we rely on a truncation technique to look at a ﬁnite-dimensional version φ≤m
truncated at a level m = poly(n) to apply matrix Bernstein, and then estimate the remaining contribution
from φ>m

by the eigenvalue decay in Assumption [S1]. With details given in Prop. 6, we derive that

dim(φx)

t) on

exp(

−

−

x

·

x

(iv)

Cτ

sup
x,y (cid:107)

·

φx
2
H ·
y (cid:107)

≤

(cid:0)(cid:113) log(n)

n ∨

log(n)
n

(cid:1) .

(10)

The ﬁnal piece of the puzzle lies in the term sup(x,y)∈dom(P) (cid:107)

2
H, which appears in both the main
for all x, y, the above term is bounded. To
term (iv) and deviation term (iii).
resolve this issue, we rely on a conditional quantile technique. Introduce the conditional quantile function
x = x. Let’s only look at data (xi, yi)’s lying
Qy2|x=x(
|
·
in the region

R+ for the conditional random variable y2

It is not true that a.s.

) : [0, 1]

→

φx
y (cid:107)

Ω :=

(x, y)
{

|

y2 > Qy2|x=x(1

,

η)
}

−

and denote
any (cid:98)PI(x, δ)

P|

Ω as the conditional distribution of data (x, y) conditioning on the region Ω. Claim that for

P
(x,y)∼P

(cid:2)y /
∈

(cid:98)PI(x, δ)(cid:3)

P
(x,y)∼P|Ω

≤

(cid:2)y /
∈

(cid:98)PI(x, δ)(cid:3) .

This is based on two facts. First,

P (cid:2)y2 > (1 + δ)(cid:98)v(x)

x = x(cid:3)

|
P (cid:2)y2>(1+δ)(cid:98)v(x)∨Qy2|x=x(1−η) | x=x(cid:3)
P (cid:2)y2>Qy2|x=x(1−η) | x=x(cid:3)

≤
= P (cid:2)y2 > (1 + δ)(cid:98)v(x)

x = x, (x, y)

|

Ω(cid:3) .

∈

(11)

(12)

regardless of the ordering of Qy2|x=x(1
marginal distribution of x due to the quantile construction, namely
proves the above claim.

η) and (1 + δ)(cid:98)v(x). Second, conditioning on Ω does not change the
x. Marginalizing (12) over x
Ω
|

≡ P

x
P

−

11

The inequality (11) makes the analyses upper bounding (LHS) from (8)-(9) applicable, with the changes:
, and (b) (cid:98)E denoting average over data points inside Ω rather than the whole dataset.

(a)
With the conditioning on Ω, Assumption [S2] implies Qy2|x=x(1

Ω replacing

P|

P

η)

ξ

·

≥

−

K(x, x), and thus

sup
(x,y)∈dom(P|Ω) (cid:107)

φx
2
H ≤
y (cid:107)

K(x, x)
Qy2|x=x(1

−

ξ−1 .

η) ≤

(13)

Now, we only need to estimate the eﬀective sample size inside Ω to complete the analyses. By the quantile
construction, P(x,y)∼P

Ω(cid:3) = η, a simple Bernstein’s inequality asserts that

(cid:2)(x, y)

∈

with probability at least 1

exp(

cη

·
Finally, plug (13) into upper bounds on terms (iii) and (iv), with η

−

−

i : (xi, yi)

Ω

}|

∈

> η

2 ·

n

|{
n) on

zi
{

n
1 .
}

have proved that

(LHS)

≤

with probability at least 1

−

exp(

δ−1((cid:100)Optn ∨
+ ((cid:100)Optn ∨
1)
n)
cη

−

·

−

(cid:113) Cτ,ξ,η log(n)
1)
n
(cid:113) Cξ,η,ω(log log(n)+t)
n

2 exp(

t) on

n
1 .

zi
{

}

−

n replacing n in (10) and (9), we

2 ·

(main term)

(deviation term)

(14)

(15)

3 Numerical Studies

We now study the numerical performance of our procedure.

3.1 Empirical Example: Comparison to Conformal Prediction

This section compares our SDP methods to the conformal prediction methods, measuring the coverage and
statistical eﬃciency of the prediction bands. We compare ﬁve methods on two simulated data sets, including
(a) standard prediction band using simple linear regression (SLR), without accounting for heteroscedasticity,
as a baseline; (b) SDP prediction bands proposed in this paper, with two speciﬁcations of the kernels,
denoted as SDP1 and SDP2; (c) conformal prediction bands, including full conformal prediction (CF) and
split conformal prediction (SplitCF), see [Lei et al., 2018, Algorithms 1 and 2 respectively]. For each method,
we report the coverage probability, eﬃciency statistics (including median length and average length), and
ﬁnally, the mean squared error (MSE) of the estimated conditional mean function.

·

|

(cid:15)

{

∼

−

−

Unif([

x = x

(xi, yi)
}

We ﬁrst explain the two simulated data sets. Here the x

√3, √3]), and the conditional
(cid:112)v(x) where the independent error (cid:15) is either drawn from a standard normal N(0, 1)
∼
distribution y
√3, √3]) (Fig. 2b). The conditional mean function m(x) = 0. The
(Fig. 2a) or a uniform distribution Unif([
heteroscedastic variance function scales as v(x) = 1 + x + 4x2, depending on x. For each simulated data set,
n
we generate
i=1 i.i.d. from the above data generating process (DGP), with n = 50 as the training
data, marked by Blue dots. The test data set are drawn from the same DGP, with N = 500 marked by
Red dots. For the SDP1/SDP2, and SplitCF, an independent calibration data set with n = 50 is used. The
calibration set is used to choose δ in SDPs as in Algorithm 1, and the homoscedestic conformal bandwidth
as in SplitCF. We compare ﬁve methods that construct the prediction bands, illustrated by the Blue band,
on the Gaussian error data set in Fig. 2a, and Uniform error data set in Fig. 2b. For all methods, the desired
α = 95%. For the SDPs in (3) γ = 10. Table 2 summarizes the coverage, eﬃciency,
coverage is set at 1
−
and estimation error.

Nearly all methods achieve 95% desired coverage, with the only exception of SLR. The focus will be on
comparing eﬃciencies, namely, which method estimates a smaller, truly heteroscedastic band in achieving
the desired coverage. As seen visually in Fig. 2a-2b and numerically in Table 2, the two conformal methods,
SplitCF and CF, estimates a conservative, wide prediction band that is almost homoscedastic. In contrast,

12

(a)

(b)

Figure 2: From left to right: SLR, SDP1, SDP2, split conformal (SplitCF), and full conformal (CF). Same style as in
Fig. 1a-1b. Statistics are summarized in Table 2.

Table 2

Coverage Median Len Average Len MSE

x
∼
|
11.2272
6.5172
7.0025
9.3960
11.5152

x
|

∼
7.6882
7.9161
7.3064
11.5199
8.0808

Gaussian

11.2537
6.9019
7.6900
9.3960
11.5911

0.5554
0.0002
0.0272
0.5554
0.5554

Uniform

7.7077
8.1540
7.8175
11.5199
8.1651

0.4405
0.0000
0.0183
0.4405
0.4405

Fig. 2a: m(x) = 0, quadratic v(x), y

SLR
SDP1
SDP2
SplitConformal
Conformal

97.40%
92.80%
94.20%
96.00%
97.40%

Fig. 2b: m(x) = 0, quadratic v(x), y

SLR
SDP1
SDP2
SplitConformal
Conformal

89.60%
95.40%
96.60%
97.80%
92.80%

13

101x10.07.55.02.50.02.55.07.510.0ySLR101xSDP1101xSDP2101xSplitCF101xCF101x6420246ySLR101xSDP1101xSDP2101xSplitCF101xCFboth SDP approaches estimate desirable heteroscedastic bands that are on average much shorter, with the
closest (94.20% and 95.40%) to the desired 95% coverage. Finally, we would like to remark that all four
methods SLR, SDP1, SDP2 and SplitCF are eﬃcient to compute. Yet, the full conformal method CF
involves discretizing input space, which is computationally intensive. Our empirical results show that the
two conformal methods can be unnecessarily conservative and form bands of nearly constant width across x
[Romano et al., 2019].

3.2 Real Data Example: Fama-French Factors

In this section, we apply our method of constructing the prediction band to the celebrated three-factor
dataset created by Fama and French [1993]. We choose this dataset for three reasons: (a) ﬁnancial data
are known to suﬀer severe heteroscedasticity, (b) the factors are believed to be diﬀerent sources explaining
returns of diversiﬁed portfolios, thus when conditioned on one factor, the other factors should have large,
heteroscedastic conditional variability, and (c) the factors—Market, Size, and Value— correspond nicely to
our common sense about the ﬁnancial market for exploratory data analysis.

Let us ﬁrst explain the data in plain language. The dataset consists of yearly and monthly observations
of four variables from July 1926 to December 2020. The four variables are (a) Risk-free return rate (RF), the
one-month Treasury bill rate (i.e., interest rate), (b) Market factor (MKT), the excess return on the market
(i.e., market return minus interest rate), (c) Size factor (SMB, Small Minus Big), the average diﬀerence in
returns between small and big portfolios according to the market capitalization, and (d) Value factor (HML,
High Minus Low), the diﬀerence in returns between value and growth portfolios. We design two experiments,
one focusing on the prediction coverage and bandwidth, and the other on exploring the role of the tuning
parameter γ in trading oﬀ mean and variance.

The ﬁrst experiment aims to access the prediction coverage in the SDP (3), using MKT (as x) to predict
other variables (as y): RF and two other factors SMB and HML. Here we use yearly data (n = 94 from
1927-2020, shown as Blue dots) to construct the prediction bands, each illustrated in Fig. 3a-3c. As for the
test data, we use the standardized monthly data (N = 1134, normalized to zero mean and unit standard
deviation, shown as Red dots) as a surrogate for test (x, y) pairs. Namely, we match 12 test data to each
training data. We veriﬁed that after standardization, the histograms of yearly and monthly data match
nicely for all four variables. For each type of response variable, we run two SDPs with diﬀerent kernels. The
summary statistics about the coverage probability, median, and mean bandwidth are given in Table 3.

(a) RF

(b) SMB

(c) HML

Figure 3: From left to right: response variable y corresponds to RF, SMB and HML, with x being MKT. Blue dots denote
n = 94 training data {(xi, yi)}n
i=1, Blue line denotes the estimated conditional mean (cid:98)m(x), and Blue band denotes the estimated
prediction band (cid:98)PI(x). Red dots represent N = 1134 test data points.

We note a few observations regarding the empirical results. First, all models achieve desirable coverage
(all > 95%). Second, controlling for MKT, all other factors have signiﬁcant heteroscedastic error left unex-
plained. For the RF, a high MKT return implies a low expected RF interest, and more importantly, a small
variability, compared to the low MKT return case. For the size factor SMB, the conditional variability is
much larger when the MKT is high vs. low, so does the conditional expectation. The conditional variability

14

−202−2024RF−202−202−6−4−20246SMB−202−202−6−4−20246HML−202Table 3: Real data: Fama-French

Kernel

Coverage Median Len Average Len

RF
RF

lin m(x), quad v(x)
rbf m(x), quad v(x)

SMB lin m(x), quad v(x)
SMB rbf m(x), quad v(x)

HML
HML

lin m(x), quad v(x)
rbf m(x), quad v(x)

98.68%
98.59%

95.77%
97.53%

96.56%
97.27%

4.3616
4.5693

5.2560
5.5407

5.2822
4.9180

4.4358
4.6847

5.2798
5.4290

5.5556
5.3640

in SMB is roughly minimized when the market is signiﬁcantly below average. While for the value factor
HML, conditional variability is minimized when the market is slightly below its average.

The second experiment aims to verify the mean and variance trade-oﬀs by tuning the parameter γ,
discussed in Sec. 1.3. Here we use the monthly return data, and for each sub-experiment, we split the data
into (train, valid, test) parts. We train models with diﬀerent γ = 0.1, 1, 10 on the training data, then valid
their performances on the validation data. We ﬁnally evaluate the performance using the test data with the
cross-validated optimal γ (based on the validation data). A nice feature about this experiment is that, one
can visualize how the SDPs trade a complex/large conditional variance v(x) for a simple/small conditional
mean m(x) in explaining y

x = x as γ increases, illustrated by Fig. 4.
|

4 Summary

−

The current paper progresses to resolve the uncertainty quantiﬁcation dilemma faced by modern machine
learning models. There are two innovative viewpoints we are taking. First, rather than relying on idealized
parametric distributional assumptions on error y
m(x), we make minimal assumptions. Both the conditional
mean and variance functions are modeled nonparametrically and can universally approximate all functions.
It is worth noting that such ﬂexibility does not hinder computational feasibility due to the sum-of-squares
and convex relaxations. The computational complexity and statistical guarantee scale favorably with high-
dimensional covariates x. Second, rather than modeling the conditional mean only and giving up the variance
(Frequentist justiﬁcation, the conditional mean is assumed inside an RKHS, see [Caponnetto and De Vito,
2007, Liang and Rakhlin, 2020, Liang et al., 2020]), or modeling the conditional variance function only
(Bayesian justiﬁcation of kriging/Gaussian processes regression, the covariance function is speciﬁed by a
kernel, see [Handcock and Stein, 1993, Stein, 2005, 2012]) for the variability in data, we model both the
mean and the variance and prove strong, non-asymptotic Frequentist coverage guarantees. Such a modeling
advantage enables the uncertainty quantiﬁcation with or without any black-box predictive model, whether
accurate or not.

To conclude, our Theorem 1 established a strong, non-asymptotic coverage guarantee in the language
of Neyman, yet with two distinct new features. First, the coverage probability can go to 1 with a ﬁxed
conﬁdence parameter δ as long as the sample size n is large enough. Second, the data-adaptive quantity
(cid:100)Optn controls both the average bandwidth and the coverage guarantee of the prediction band (cid:98)PI(x). A small
objective value of the SDP makes the prediction band accurate and narrow simultaneously. Finally, our
procedure for constructing prediction bands can be viewed as a novel variance interpolation with conﬁdence
and further leverages techniques from semi-deﬁnite programming and sum-of-squares optimization. We
conducted simulated and real data experiments to validate the prediction interval’s numerical performance for
uncertainty quantiﬁcation. A minimal 10-line Python implementation is provided in Listing. 1 for interested
readers.

15

Figure 4: Cross-validate γ experiment. On the left is SMB as the response and on the right is HML. For each response
variable, we run two sub-experiments: the top row corresponds to a linear m(x) and a quadratic v(x), and the bottom row
corresponds to a degree-3 polynomial m(x) and a quadratic v(x). Each sub-ﬁgure corresponds to a speciﬁc γ, noted in its
title. The cv γ denotes the cross-validated optimal γ using the validation dataset. Here the (train, valid, test) dataset has size
proportional to 1:3:9, denoted by Blue dots, Red pluses, and Red dots respectively.

Acknowledgement

Liang acknowledges the generous support from the NSF Career award (DMS-2042473) and the George C. Tiao
faculty fellowship. Liang thanks Michael Stein, Ruey Tsay, and Vladimir Vovk for constructive comments.
Liang thanks the editor, associate editor, and three anonymous referees for the valuable suggestions that
improve the paper.

References

Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world. Springer

Science & Business Media, 2005.

Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-Free
Predictive Inference for Regression. Journal of the American Statistical Association, 113(523):1094–1111,
July 2018. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1307116.

Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized Quantile Regression. In Advances

in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming, version 2.1.

http://cvxr.com/cvx, March 2014.

C Feﬀerman and Duong Hong Phong. On positivity of pseudo-diﬀerential operators. Proceedings of the

National Academy of Sciences of the United States of America, 75(10):4673, 1978.

J Andrew Bagnell and Amir-massoud Farahmand. Learning positive functions in a hilbert space. In NIPS

Workshop on Optimization (OPT2015), 2015.

Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Non-parametric Models for Non-negative Func-

tions. arXiv:2007.03926, 2020.

Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on

optimization, 11(3):796–817, 2001.

16

−505SMBγ=0.1γ=1γ=10cvγ−2.50.02.5−505SMB−2.50.02.5−2.50.02.5−2.50.02.5−505HMLγ=0.1γ=1γ=10cvγ−2.50.02.5−505HML−2.50.02.5−2.50.02.5−2.50.02.5Mihaela Curmei and Georgina Hall. Shape-constrained regression using sum of squares polynomials. arXiv

preprint arXiv:2004.03853, 2020.

Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “Ridgeless” regression can generalize. The

Annals of Statistics, 48(3):1329–1347, 2020. doi: 10.1214/19-AOS1849.

Peter L. Bartlett, Philip M. Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear re-
gression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, December 2020. ISSN
0027-8424, 1091-6490. doi: 10.1073/pnas.1907378117.

Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: A statistical viewpoint.

arXiv:2103.09177, 2021.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural

networks in high dimension. arXiv:1904.12191, February 2020.

Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear

classiﬁers: High-dimensional asymptotics in the overparametrized regime. arXiv:1911.01544, 2020.

Tengyuan Liang and Benjamin Recht.

Interpolating classiﬁers make few mistakes.

arXiv preprint

arXiv:2101.11815, 2021.

Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E. Schapire. Boosting the margin: A new explanation
for the eﬀectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, October 1998. ISSN
0090-5364, 2168-8966. doi: 10.1214/aos/1024691352.

Tengyuan Liang and Pragya Sur. A precise high-dimensional asymptotic theory for boosting and min-l1-
norm interpolated classiﬁers. arXiv preprint arXiv:2002.01586, The Annals of Statistics, forthcoming,
2020.

Glenn Shafer and Vladimir Vovk. A Tutorial on Conformal Prediction. Journal of Machine Learning
Research, 9(12):371–421, 2008. ISSN 1533-7928. URL http://jmlr.org/papers/v9/shafer08a.html.

Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica:

journal of the Econometric

Society, pages 33–50, 1978.

Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):143–156,

2001.

Alexandre Belloni and Victor Chernozhukov. (cid:96)1-penalized quantile regression in high-dimensional sparse
models. The Annals of Statistics, 39(1):82 – 130, 2011. doi: 10.1214/10-AOS827. URL https://doi.
org/10.1214/10-AOS827.

Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov, and Iv´an Fern´andez-Val. Conditional quantile

processes based on series or many regressors. Journal of Econometrics, 213(1):4–29, 2019.

Victor Chernozhukov, Iv´an Fern´andez-Val, and Alfred Galichon. Quantile and probability curves without

crossing. Econometrica, 78(3):1093–1125, 2010.

Eugene F. Fama and Kenneth R. French. Common risk factors in the returns on stocks and bonds. Journal of
Financial Economics, 33(1):3–56, February 1993. ISSN 0304-405X. doi: 10.1016/0304-405X(93)90023-5.

Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foun-

dations of Computational Mathematics, 7(3):331–368, 2007.

17

Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants
and restricted lower isometry of kernels. In Proceedings of 33rd Conference on Learning Theory, volume
125, pages 2683–2711. PMLR, July 2020.

Mark S Handcock and Michael L Stein. A bayesian analysis of kriging. Technometrics, 35(4):403–410, 1993.

Michael L Stein. Space–time covariance functions. Journal of the American Statistical Association, 100(469):

310–321, 2005.

Michael L Stein. Interpolation of spatial data: some theory for kriging. Springer Science & Business Media,

2012.

18

A Appendix

A.1 Remaining Propositions

In this section, we collect the remaining propositions.

Proposition 3 (Representation). The kernelized version of the SDP as in (3) is equivalent to the following
inﬁnite-dimensional SDP

min
β∈Hm, A:Hv→Hv

s.t.

A
(cid:107)

(cid:63)
(cid:107)

γ

β

2
Hm +
(cid:107)
· (cid:107)
xi, Aφv
φv
(cid:104)
A
0

xi(cid:105)

(cid:23)

Hv

(cid:0)yi

φm
xi, β

Hm
(cid:105)

− (cid:104)

(cid:1)2

,

≥

i .

∀

Proof. Noticing that the solution to the inﬁnite-dimensional problem must lie in the span of data,
Rn. With
(cid:63), we can derive (3).
(cid:107)

namely A = (cid:80)
φv
xj with some PSD B
the above representation, plug in the inﬁnite-dimensional SDP and recall Tr(A) =
When β is not a decision variable, this representation theorem applies to (1). (cid:4)

xi with some α
A
(cid:107)

Sn×n, and β = (cid:80)

i,j Bijφv

i αiφm

xi ⊗

∈

∈

Proposition 4 (Symmetrization). Let
with probability at least 1

2 exp(

F
t) on

−

−

(cid:12)
(cid:12) E[f (z)]

sup
f ∈F

−

R, with supx∈Z |
be a class of functions f :
n
i=1 i.i.d. drawn from a distribution, we have
zi
{
}
(cid:98)E[f (z)](cid:12)
(cid:12)

(cid:15)if (zi) + 3M

n
(cid:88)

Z →

(cid:114)

2

.

E
(cid:15)

sup
f ∈F

·

1
n

t
2n

≤

i=1

f (z)

M . Then

| ≤

Proof. First, with McDiarmid’s inequality, we know w.h.p.

sup
f ∈F

(cid:12)
(cid:12) E[f (z)]

(cid:98)E[f (z)](cid:12)
(cid:12)

−

E
{zi}n
1

sup
f ∈F

≤

(cid:12)
(cid:12) E[f (z)]

−

(cid:98)E[f (z)](cid:12)

(cid:12) + M

(cid:114)

t
2n

.

Apply Gin´e-Zinn symmetrization to the ﬁrst term on the RHS, then apply McDiarmid’s inequality again,
we can establish the claim. See Liang and Rakhlin [2020] for details. (cid:4)

Proposition 5 (Objective value estimate). Under [S3], the following holds with probability at least 1

n−10,

−

Proof. Apply union bound on the tails given by [S3], with the choice t0 = [log(n)]cω we know

(cid:100)Optn ≤

[log(n)]cω .

P (cid:2)y2

i ≤
≤
In view of Prop. 3, the above certiﬁes that A := t0
which implies t0 being an upper bound on (cid:100)Optn. (cid:4)

∀

·

·

t0

K(xi, xi),

1

i

n(cid:3)

1

n

exp(

≥

≤
I lies in the feasibility set

−

−

·

Ctω
0 )

1

n−10.

≥
−
φxi, Aφxi(cid:105)

(cid:104)

H = t0

K(xi, xi)

·

y2
i ,

≥

Proposition 6 (Operator-norm estimate). Under [S1], for any

E
{(cid:15)i}n
1

(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:15)iφxi ⊗

φxi

(cid:13)
(cid:13)op ≤

Cτ

sup

x (cid:107)

·

φx

2
H ·
(cid:107)

xi
{

n
i=1, the following holds
}
(cid:0)(cid:113) log(n)

log(n)
n

(cid:1) .

n ∨

with each coordinate of φj
λj

Proof. Recall [S1], due to the Mercer’s theorem, one can represent φx as an inﬁnite-dimensional vector,
x corresponding to the eigenfunction of the integral operator, with j = 1, . . . ,
and
∞
2
H = 1
h
(cid:107)

Cj−τ . To bound the operator norm, recall the Rayleigh quotient form, for any h

with

∈ H

≤

(cid:107)

(cid:88)

(cid:10)h, (cid:0) 1

n

i

(cid:15)iφxi ⊗

(cid:1)h(cid:11)

φxi

H = 1

n

(cid:88)

i

(cid:15)i

φxi, h
(cid:105)
(cid:104)

2
H .

(16)

19

, h≤m
H =
Note
of φx. By Cauchy-Schwarz

φx, h
(cid:105)
(cid:104)

φ≤m
x
(cid:104)

H+

(cid:105)

φ>m
x
(cid:104)

, h>m

H where the superscript indicates a truncation on the coordinates
(cid:105)

Therefore LHS in (16) can be upper bounded by

φxi, h
(cid:105)
(cid:104)

2
H ≤

2
(cid:104)

φ≤m
xi

, h≤m

2
H + 2
(cid:105)

φ>m
xi
(cid:104)

, h>m

2
H .
(cid:105)

(cid:13)
2
(cid:13)

1
n

n
(cid:88)

i=1

(cid:15)iφ≤m

xi ⊗

φ≤m
xi

(cid:13)
(cid:13)op + 2 sup

2
φ>m
H .
xi (cid:107)

i (cid:107)

For the ﬁrst term, now we can apply the matrix Bernstein inequality. With probability 1
following upper bound on the ﬁrst term holds

−

2 exp(

−

t) the

sup

x (cid:107)

φx

(cid:0)(cid:113) log(m)+t

n

2
H ·
(cid:107)

log(m)+t
n

(cid:1) .

∨

Cj−τ with τ > 1, we know it is upper bounded
For the second term, recall the eigenvalue decay λj
by Cm−(τ −1). Choosing log(m) = Cτ log(n) with a constant large enough, we know the second term is
dominated by the ﬁrst term. By integrating the tail bound to obtain a bound on the expected value, we
complete the proof. (cid:4)

≤

A.2 Proof of the Calibration Lemma
Proof of Lemma 2. Deﬁne the Bernoulli random variables zj(δ) := 1[y(cid:48)
δ

1, ∆], we know by Hoeﬀding’s inequality that

[

j /
∈

∈

−

(cid:98)PI(x(cid:48)

j, δ)], then for any ﬁxed

(cid:12)
(cid:12)

P
(x,y)∼P

(cid:2)y /
∈

(cid:98)PI(x, δ)(cid:3)

1
m

−

m
(cid:88)

j=1

1[y(cid:48)

j /
∈

(cid:98)PI(x(cid:48)

(cid:12)
j, δ)]
(cid:12)

(cid:113) t
2m

≤

with probability 1

2 exp(

−

−

t) on

j, y(cid:48)
x(cid:48)
j}
{

m
j=1. Therefore, if we can identify a δ such that

1
m

m
(cid:88)

j=1

1[y(cid:48)

j /
∈

(cid:98)PI(x(cid:48)

j, δ)]

3
4 α ,

≤

and for some later speciﬁed choice of t that

(cid:113) t

1
4 α ,
(cid:98)PI(x, δ)(cid:3) and its empirical counterparts 1
m
j, δ)] are monotonic in δ. We claim that the Algorithm 1 will terminate with at most log2

the proof will complete. Observe that both P(x,y)∼P
(cid:98)PI(x(cid:48)
iterations (namely, disjoint choices of δ in the while loop). To prove this claim, we note that the algorithm
must terminate in the interval δ

2m ≤
(cid:2)y /
∈

(cid:80)m
1[y(cid:48)
j /
j=1
∈
(cid:1)
(cid:0) 2L(∆+1)
α

α
2L , ∆] since we know

[∆

∈

−

1
m

m
(cid:88)

j=1

1[y(cid:48)

j /
∈

(cid:98)PI(x(cid:48)

j, ∆

α
2L )]

P
(x,y)∼P

≤

−

(cid:2)y /
∈

(cid:98)PI(x, ∆

−

α

2L )(cid:3) +

(cid:113) t

2m ≤

L α

2L + 1

4 α

3
4 α ,

≤

(20)

by the mean value theorem and the upper bound on the Lipschitz constant. With the dyadic search, the
(cid:0) 2L(∆+1)
pre-determined dyadic grids of δ’s with the form
algorithm will terminate after at most
α
(cid:1)
(cid:0) 2L(∆+1)
1
1
Gdyadic :=
+
2k )∆
2k |
α
1(cid:1) + 10 log(m) and recall that m is large enough such that

. Therefore, take t = log (cid:0)

(cid:0) 2L(∆+1)
α

k = 0, 1, . . . ,

log2
(cid:100)

(1
{

log2

log2

(cid:101)}

−

−

(cid:1)

(cid:1)

(cid:100)

(cid:101)

(cid:101)

(cid:100)

(cid:114)

log (cid:0)(cid:100)log2

(cid:0) 2L(∆+1)
α

m

(cid:1)(cid:101)+1(cid:1)+10 log(m)

1
4 α ,

≤

20

(17)

(18)

(19)

then uniformly over the ﬁxed dyadic grid δ

Gdyadic,

∈

sup
δ∈Gdyadic

P
(x,y)∼P

(cid:2)y /
∈

(cid:98)PI(x, δ)(cid:3)

1
m

−

m
(cid:88)

j=1

1[y(cid:48)

j /
∈

(cid:98)PI(x(cid:48)

j, δ)]

1
4 α

≤

with probability at least 1

−

2m−10. It is easy to see that δ(cid:63)(α)

Gdyadic, and thus on the same event,

∈

P
(x,y)∼P

(cid:2)y /
∈

(cid:98)PI(x, δ(cid:63)(α))(cid:3)

1
m

≤

m
(cid:88)

j=1

1[y(cid:48)

j /
∈

(cid:4)

(cid:98)PI(x(cid:48)

j, δ(cid:63)(α))] + 1

4 α

α .

≤

(21)

(22)

A.3 Remaining Experimental Details

All experiments are conducted using the Python language. The minimal implementation is provided below

import cvxpy as cp

def sdpDual ( K1 , K2 , Y , n , gamma = 1 e1 ) :
# K1 kernel for conditional mean , 1 st moment
# K2 kernel for conditional variance , 2 nd moment
# Define and solve the CVXPY problem .

# Create a symmetric matrix variable \ hat { B }
hB = cp . Variable (( n , n ) , symmetric = True )
# Create a vector variable \ hat { a }
ha = cp . Variable ( n )

# PSD and inequality constraints

constraints = [ hB >> 0]
constraints += [

K2 [i ,:] @hB@K2 [i ,:] >=
cp . square ( Y [ i ] - K1 [i ,:] @ha ) for i in range ( n )

]
prob = cp . Problem ( cp . Minimize (

gamma * cp . quad_form ( ha , K1 ) + cp . trace ( K2@hB )

) , constraints )

# Solve the SDP
prob . solve ()
print ( " Optimal_Value " , prob . value )

return [ ha . value , hB . value ]

Listing 1: Minimal python code

21

