0
2
0
2

c
e
D
3
1

]
S
D
.
h
t
a
m

[

1
v
1
1
1
9
0
.
2
1
0
2
:
v
i
X
r
a

A Data Driven Method for Computing Quasipotentials

Bo Lin*1, Qianxiao Li†1,2, and Weiqing Ren‡1

1Department of Mathematics, National University of Singapore, Singapore 119076
2Institute of High Performance Computing, A*STAR, Singapore 138632

December 17, 2020

Abstract

The quasipotential is a natural generalization of the concept of energy functions to non-equilibrium systems. In the
analysis of rare events in stochastic dynamics, it plays a central role in characterizing the statistics of transition events
and the likely transition paths. However, computing the quasipotential is challenging, especially in high dimensional
dynamical systems where a global landscape is sought. Traditional methods based on the dynamic programming
principle or path space minimization tend to suffer from the curse of dimensionality. In this paper, we propose a simple
and efﬁcient machine learning method to resolve this problem. The key idea is to learn an orthogonal decomposition
of the vector ﬁeld that drives the dynamics, from which one can identify the quasipotential. We demonstrate on
various example systems that our method can effectively compute quasipotential landscapes without requiring spatial
discretization or solving path-space optimization problems. Moreover, the method is purely data driven in the sense
that only observed trajectories of the dynamics are required for the computation of the quasipotential. These properties
make it a promising method to enable the general application of quasipotential analysis to dynamical systems away
from equilibrium.

Keywords. Non-equilibrium Systems, Quasipotential, Machine Learning, Rare Events, Hamilton-Jacobi Equations

1 Introduction

Dynamical systems under the inﬂuence of random perturbations are widely used in scientiﬁc modelling, including nu-
cleation events during phase transitions, chemical reactions and biological networks. For these systems, understanding
the mechanism and statistics of transitions between stable states is of great interest, especially when the noise has very
small amplitude. According to large deviation theory [1], the transition dynamics become predictable in the small noise
limit, and is completely characterized by the quasipotential. The latter generalizes the notion of equilibrium potential
to non-equilibrium systems. Consequently, the quasipotential landscape gives an intuitive description of the essential
dynamical features of complex systems that are out of equilibrium [9, 10, 14, 15].

However, computing quasipotentials is a challenging problem, especially when the system is high dimensional, or
when a global landscape is sought. To date, there are two classes of methods for computing the quasipotential. The ﬁrst
type relies on the variational formulation of the quasipotential based on the Freidlin-Wentzell action functional [2, 4, 5].

*Electronic mail: E0046836@u.nus.edu
†Corresponding author. Electronic mail: qianxiao@nus.edu.sg
‡Corresponding author. Electronic mail: matrw@nus.edu.sg

1

 
 
 
 
 
 
Here, the value of the quasipotential with respect to two chosen points is computed based on the solution of a path-
space minimization problem. These methods have the advantage that they can handle high-dimensional systems, and
moreover, a most likely transition path is identiﬁed together with the computation. However, the key disadvantage is
the behavior of the quasipotential away from the chosen points (and a minimum action path connecting them) remains
unknown. In particular, computing a quasipotential landscape is prohibitively expensive using such methods. The
second class of methods is developed to compute the quasipotential on 2D or 3D meshes. These methods are based
on the dynamic programming principle. At each step, the estimated quasipotential values at selected spatial points are
updated by solving the associated Hamilton-Jacobi equation [6] or directly solving the action minimization problem
locally [7, 8, 16]. Contrasting with previous variational approaches in path space, these methods compute an entire
quasipotential landscape and do not require a priori information to select special points of interest. However, due to
the requirement of a discretization mesh, they are limited to low dimensional system, as the computational complexity
and cost grow exponentially over dimensions.

In practical applications, it is often the case that we need to analyze transition events or asymptotic occupational
probabilities in high dimensional spaces, e.g. applications in biological networks [10, 11, 12]. For such computations,
the quasipotential is a very useful object. Thus, it is of importance to develop a method that can effectively address
the previously mentioned limitations. In this paper, we introduce a machine learning based method for computing
quasipotential landscapes. The method is not only scalable to high dimensions, but also yields the entire quasipotential
landscape. Moreover, it has the advantage that no explicit dynamical models are required, and the quasipotential can
be constructed directly from sampled trajectory data. In fact, this method simultaneously learns the force ﬁeld of the
dynamical system from the trajectories. This makes the computation of quasipotential landscapes, thus the analysis of
rare events, for practical applications a much more tractable task.

The paper is organized as follows. We ﬁrst introduce some theoretical background in Section 2 and then propose
the machine learning based method in Section 3, including parameterization of the orthogonal decomposition of the
vector ﬁeld and the loss function. In Section 4, we illustrate the proposed method on several numerical examples.
Finally we draw the conclusions in Section 5.

2 Background

We consider a dynamical system driven by small white noise. Its evolution is described by the stochastic differential
equation

(1)
where f : Rd → Rd is a continuously differentiable vector ﬁeld, W is the standard Brownian motion and (cid:15) is a small
parameter, typically identiﬁed as a scaled temperature. For a given continuous path ϕ(t) ∈ Rd on the time interval
t ∈ [0, T ], the Freidlin-Wentzell action functional of the path associated with the system is deﬁned as

(cid:15)dW, x = (x1, . . . , xD) ∈ Rd,

dx = f (x)dt +

√

A[ϕ(·); T ] =

(cid:90) T

0

1
2

| ˙ϕ − f (ϕ)|2 dt.

(2)

Denote by x(cid:15)(t) the trajectory of the system (1) starting from ϕ(0). The Freidlin-Wentzell theory tells that for sufﬁ-
ciently small (cid:15), δ, the probability that x(cid:15)(t) stays in the neighborhood of the path ϕ(t) on the time interval [0, T ] can
be estimated by

P[ sup
0≤t≤T

|x(cid:15)(t) − ϕ(t)| < δ] ≈ exp(−

1
(cid:15)

A[ϕ(·); T ]).

(3)

We assume that the deterministic dynamical system ˙x = f (x) exhibits a ﬁnite number of stable equilibria or limit
cycles, such that almost every trajectory of the system is asymptotically convergent to those isolated attractors. Let A

2

be one of the attractors. The quasipotential at the state x with respect to the attractor A is deﬁned as

UA(x) = inf
T >0

inf
ϕ

A[ϕ(·); T ],

(4)

where the inﬁmum of the action functional is taken over all time horizon T > 0 and all absolutely continuous paths
ϕ connecting the attractor A and the state x, i.e. ϕ(0) ∈ A and ϕ(T ) = x. The quasipotential with respect to the
attractor A describes the difﬁculty of exiting the basin of A for the system (1) when the strength of noise (cid:15) is small.
According to the large deviation theory [1], the statistics of the escaping event from the attractor A can be estimated
using the quasipotential. For instance, the maximum likelihood path from A to another attractor is characterized by the
quasipotential - the tangent of the path is parallel to f + ∇UA along the path. Also, the expected exit time τ from the
attractor A is determined by the minimum of the quasipotential on the boundary of the basin of A: lim(cid:15)→0 (cid:15) log E [τ ] =
minx∈∂B(A) UA(x), where B(A) is the basin of the attractor A.

The central idea of our approach relies on an alternative characterization of the quasipotential through an orthogonal

decomposition of the vector ﬁeld. Suppose f can be decomposed as

f (x) = −∇V (x) + g(x), with ∇V (x)T g(x) = 0,

(5)

where the term −∇V (x) is referred to as the potential component of f (x) and g(x) as the rotational component. It is
proved in the following theorem that under certain conditions, 2V coincides with the quasipotential of system (1) up
to an additive constant.

Theorem 1. Suppose the vector ﬁeld f in the system (1) has the orthogonal decomposition (5) and V attains its strict
local minimum at a point or limit cycle, denoted by A. If there is a bounded domain D containing A such that

• V is continuously differentiable in D ∪ ∂D;

• V (x) > V (A) and ∇V (x) (cid:54)= 0 for all x ∈ D ∪ ∂D and x /∈ A,

then the quasipotential of the system (1) with respect to the attractor A in the set {x ∈ D ∪ ∂D : V (x) ≤
miny∈∂D V (y)} coincides with 2V (x) up to an additive constant.

Proof. See Ref. [1].

For the system with multiple attractors, each attractor corresponds to a local quasipotential. These local quasipo-
tential can be used to construct the global quasipotential [1, 9, 13]. The global quasipotential is related to the invariant
measure of the dynamical system when the noise is small: lim(cid:15)→0 (cid:15) log p∞(x) = −U (x), where p∞(x) is the steady-
state probability distribution of the system.

3 Methods

We construct the quasipotential based on the orthogonal decomposition (5), where the potential and rotational compo-
nents are parameterized by neural networks. For systems with multiple attractors, we use a single neural network for
the potential component and a single neural network for the rotational component across the whole domain of interest.
The local quasipotential with respect to each attractor can be obtained by conﬁning the parameterized function to the
corresponding basin of attraction.

Once we have a suitable parameterization of V and g, they can then be trained by minimizing a loss function over

the trajectory data from the deterministic system

˙x = f (x) = −∇V (x) + g(x).

(6)

The loss function is designed to reconstruct the dynamics of the original system (6) and to impose the orthogonality
condition between the potential and rotational components.

3

3.1 Parameterization of the Orthogonal Decomposition

The function V is parameterized by the sum of a neural network and a quadratic function,

Vθ(x) = ˆVθ(x) + |x|2,

(7)

where the activation function of the network ˆVθ is taken as the hyperbolic tangent function. The rotational component g
is parameterized by a neural network gθ with continuously differentiable activation (e.g. tanh(z) or ReLU2(z) [26]).
Therefore, the parameterized vector ﬁeld is

fθ(x) = −∇Vθ(x) + gθ(x).

(8)

The neural networks Vθ(x) and gθ(x) are constructed to obey the following properties:

(i) Vθ is real analytic;

(ii) Both Vθ and |∇Vθ| are radially unbounded, i.e. Vθ(x) → ∞ and |∇Vθ(x)| → ∞, as |x| → ∞;

(iii) gθ is continuously differentiable.

The following theorem shows that under the above three conditions, the set {x ∈ Rd : ∇Vθ(x) = 0} is bounded
and has Lebesgue measure zero in Rd, and the learned dynamics ˙x = fθ(x) is stable with respect to this set. Hence,
any dynamics parameterized as such enjoys good stability properties, and are suitable candidates to model physical
systems.

Theorem 2. Let f (x) = −∇V (x) + g(x) where ∇V (x)T g(x) = 0 and V , g satisfy the conditions (i),(ii),(iii). Then
any trajectory {x(t)}t≥0 of the system ˙x = f (x) approaches the bounded measure-zero set C := {x ∈ Rd : ∇V (x) =
0} as t → ∞, i.e.

lim
t→∞

inf
y∈C

|x(t) − y| = 0.

(9)

Proof. As V is real analytic, all partial derivatives of V are also real analytic. Since V is radially unbounded, the zero
sets of these partial derivatives are all measure-zero in Rd. Thus, the set C has measure of zero in Rd. Furthermore,
|∇V (x)| → ∞, as |x| → ∞, which implies that C is also bounded.
For any trajectory {x(t)}t≥0 of the system ˙x = f (x), we have

dV (x(t))
dt

= ∇V (x(t)) · f (x(t)) = −|∇V (x(t))|2 ≤ 0.

(10)

Therefore V is the Lyapunov function of this system and we have V (x(t)) ≤ V (x(0)), for all t. Furthermore, since V
is radially unbounded, the sub-level set

S0 = {x ∈ Rd : V (x) ≤ V (x(0))}

(11)

is bounded. The trajectory {x(t)}t≥0 is contained in the bounded set S0. By Lasalle’s theorem [17], the trajectory
{x(t)}t≥0 approaches the set C as t → ∞.

Incidentally, the data-driven nature of our method also gives a way to learn stable and interpretable dy-
Remark.
namical systems from trajectory data, as shown in Theorem 2. Up to this paper, a large amount of efforts have been
devoted to the various data-driven methods for system identiﬁcation in two main directions. One is to learn closed
form equations with some prior knowledge on the underlying mechanism. Related methods include Kronecker product

4

representations [18], sparse identiﬁcation of nonlinear dynamics [19], Gaussian processes [20] and PDE-net [21].
The other direction employs black box methods to learn a model with better accuracy in the prediction. These methods
exploit the expressive power of deep neural networks [22, 23], which could potentially learn more complicated models
of the nonlinear dynamical systems. However, stability and interpretability is not generally ensured. The attempts to
balance expressive power and physical relevance is investigated in [24, 25]. The current method falls into this category,
in that stability is ensured by construction, and subsequent ﬂexibility is introduced via neural network approximation.

3.2 Loss Function

Once we have parameterized Vθ and gθ, it remains to deﬁne a suitable loss function over the data in order to train them
to ensure reconstruction (f ≈ −∇Vθ + gθ) and orthogonality (∇V T

θ gθ ≈ 0).

The observation dataset X = {Xi(tj), Xi(tj + ∆t) : i = 1, . . . , N, j = 0, . . . , M − 1} contains N trajectories
of the deterministic system (6) where Xi(t) denotes the ith trajectory. Along each trajectory, 2M + 2 data points are
sampled at the times

where t0 < t1 < ... < tM and ∆t is a small time step. The loss function consists of two parts

t0, t0 + ∆t, t1, t1 + ∆t, ..., tM , tM + ∆t,

L = Ldyn + λLorth,

(12)

(13)

where Ldyn is to reconstruct the dynamics in (6), Lorth is to impose the orthogonality condition ∇Vθ(x)T gθ(x) = 0,
and λ is a parameter.

The term Ldyn depends on the difference between the learned dynamics and the observed trajectories,

Ldyn =

1
N (M + 1)

N
(cid:88)

M
(cid:88)

i=1

j=0

¯h (eij; δ1) ,

eij =

1
∆t

(I∆t[fθ; Xi(tj)] − Xi(tj + ∆t)) ,

(14)

where I∆t[fθ; Xi(tj)] is the state obtained by performing the numerical integration of the learned dynamics ˙x = fθ(x)
by one time step ∆t from the state Xi(tj), and ¯h(e; δ1) denotes the mean Huber loss of the vector e = (e1, . . . , ed)
with threshold δ1,

d
(cid:88)

1
d
(cid:26) 1

i=1

¯h(e; δ1) =

h(ei; δ1) =

h(ei; δ1),

2 e2
i ,
δ1|ei| − 1

|ei| < δ1,
1, otherwise.

2 δ2

The Huber loss reduces the dominating effect of large components in the vector e.

The orthogonality between ∇Vθ and gθ is imposed by the penalty term λLorth with

Lorth =

(cid:32)

1
S

S
(cid:88)

i=1

w

∇Vθ( ˜Xi)T gθ( ˜Xi)
|∇Vθ( ˜Xi)| · |gθ( ˜Xi)|

(cid:33)

; δ2

,

where w(y; δ2) = y2Iy>0 + δ2y2Iy<0, δ2 is a parameter and ˜X1, . . . , ˜XS are representative data points sampled from
X by using Algorithm 1. The representative data points are chosen such that each of them covers a ball of radius r and
no other representative data points lie inside this ball. This sampling procedure avoids the situation where points in the
trajectories are clumped together near attractors, where the orthogonality condition is difﬁcult to enforce numerically.

5

(15)

(16)

Algorithm 1 Sampling the representative dataset

Initialize the sets Y = X and ˜X = ∅
while Y (cid:54)= ∅ do

1: function GETXHAT(X, r)
2:
3:
4:
5:

Randomly select x ∈ Y and append x to the set ˜X
Delete all the points belonging to the ball Br(x) from Y

end while
6:
Return ˜X
7:
8: end function

4 Numerical Examples

We now illustrate using various numerical examples that the proposed method can efﬁciently compute the quasipoten-
tial and at the same time learn stable dynamics. Section 4.1 contains two ODE systems: one with two stable equilibrium
points and the other with a limit cycle. The quasipotentials are known in these two examples, and we use these ex-
act solutions to benchmark the numerical method. Section 4.2 is a biological system which models the reproduction
process of a budding yeast cell cycle. Section 4.3 contains two high-dimensional systems which are obtained from the
discretization of partial differential equations (PDEs).

In the examples, we generate trajectories by simulating the deterministic dynamics in Eq. (6) using the forth-order
Runge-Kutta method with the time step ∆t on the time interval [0, T ]. The initial states are randomly sampled from
certain distributions which will be speciﬁed in the examples. From these trajectories, we obtain the dataset X by
collecting the data points at the times tj = jm∆t and tj + ∆t where j = 0, 1, . . . , M and m is some positive integer.
The set of trajectories is split into three parts: 70% (training), 20% (validation) and 10% (test). The representative
datasets are sampled from these three datasets respectively using Algorithm 1 with various choices of the parameter r.
The parameters ∆t, T , m, r and the number of trajectories N are given in Table 1.

Table 1: Parameters in the numerical examples.

Example

N

∆t

T

5

5

m

10

10

2 × 103

10−2

2 × 103

10−2

1 × 104

10−2

50

100

1 × 104

10−3

2 × 104

10−4

2

2

20

200

r

δ1

0.1

0.05

0.1

0.2

0.2

1

1

1

1

1

λ

1

0.02

0.005

1

0.1

# nodes in each
hidden layer

50

50

100

100

200

1

2

3

4

5

The networks ˆVθ, gθ for the potential and rotational components in the parameterized vector ﬁeld (8) are both
taken as fully connected neural networks of 2 hidden layers with the same number of nodes in each hidden layer. The
nonlinear activation function in ˆVθ is tanh in all the examples, and the activation function in gθ is tanh in Examples

6

1-3 and ReLU2 in Examples 4-5. The input to the parameterized vector ﬁeld fθ is centered so that the centered data
points have mean-zero.

In the loss function, we use the second-order Runge-Kutta method as the numerical integrator I and set δ2 = 1
10 .
The two parameters δ1, λ are chosen so that the orthogonality error Lorth and the error of the predicted long-term
dynamics over the test dataset are both small. To quantify the accuracy of the predicted long-term dynamics, we solve
the learned dynamics

˙xθ = −∇Vθ(xθ) + gθ(xθ)

(17)

using the second-order Runge-Kutta method on the time interval [0, T ], and compare the solution with the original
dynamics:

(cid:113)(cid:80)M

(cid:15) =

j=1 |xθ(tj) − x(tj)|2
(cid:113)(cid:80)M
j=1 |x(tj)|2

,

(18)

where x(t) is the trajectory from the test dataset with the same initial sate as xθ(t).

The networks are trained with Adam optimizer [27] using mini-batches of size 5000, while the learning rate expo-

nentially decays over the training steps.

4.1 ODE systems with known quasipotentials

First, we consider two low-dimensional systems: one with two stable equilibrium points and the other with a limit
cycle. The quasipotentials are known in these two examples, and we use these exact quasipotentials to benchmark the
proposed method.

Example 1. We consider the following system in three-dimensional space [16],

dx
dt
dy
dt
dz
dt

= −2(x3 − x) − (y + z),

= −y + 2(x3 − x),

= −z + 2(x3 − x),

(19)

where the state of the system is x = (x, y, z)T . This system has two stable equilibrium points, one at xa = (−1, 0, 0)
and the other at xb = (1, 0, 0) and one unstable equilibrium point at xc = (0, 0, 0). In the basins of the two stable
equilibrium points, the quasipotential is known and given by

U (x, y, z) = (1 − x2)2 + y2 + z2.

(20)

We generate 2000 trajectories by solving the equations in (19) starting from initial states sampled from the uniform
distribution on the domain D = [−2, 2] × [−1.5, 1.5]2. Along each trajectory, we collect 100 data points. In total,
X contains 2 × 105 data points. Out of these data points, 8571 representative data points are used to impose the
orthogonality condition.

The test dataset contains 200 trajectories. To quantify the accuracy of the predicted long-term dynamics, we
solve the learned dynamics starting from the initial states of these trajectories and compute the error in (18) for each
trajectory. Fig. 1 (lower panel) shows the comparison of three trajectories of the learned dynamics with those of the
original dynamics in the test dataset. These errors have the mean 5.069×10−4 and the standard deviation 1.565×10−3.
The learned quasipotential is given by Uθ(x) = 2Vθ(x) − C, where the constant C is such that the minimum of
Uθ(x) on the domain D equals zero. Fig. 1 (upper panel) shows the comparison of Uθ(x) with the exact quasipotential

7

Figure 1 (Example 1): Upper Panel: Contour plots of the learned quasipotential Uθ (left) and exact quasipotential U
(middle) projected onto the xy plane with z = 0, and plot of the learned quasipotential along the line y = z = 0 (right).
Lower Panel: Comparison of trajectories of the learned dynamics and the original dynamics (19) from different initial
states.

in (20). To quantify the accuracy of the learned quasipotential, we compute the relative root mean square error (rRMSE)
and the relative mean absolute error (rMAE),

rRMSE =

(cid:113)(cid:80)L

i=1 (U (xi) − Uθ(xi))2
(cid:113)(cid:80)L

i=1 U 2(xi)

,

rMAE =

(cid:80)L

i=1 |U (xi) − Uθ(xi)|
i=1 |U (xi)|

(cid:80)L

,

(21)

where {xi}L
0.0037 and 0.0017, respectively. The errors are computed with L = 106.

i=1 are the grid points of the uniform mesh on D. The rRMSE and rMAE for the learned quasipotential are

Example 2. We consider the system with the quasipotential

U (x, y) =

(cid:18)

(x − a)2 + (x − a)(y − b) + (y − b)2 −

(cid:19)2

1
2

,

(x, y) ∈ R2,

(22)

where a, b are two parameters. The function U attains its local maximum at the point (a, b) and attains its minimum
on the ellipse

.

(23)

(cid:26)

(x, y) ∈ R2 : (x − a)2 + (x − a)(y − b) + (y − b)2 =

1
2

8

(cid:27)

Figure 2 (Example 2): Upper Panel: Contour plots of the learned quasipotential Uθ (left) and exact quasipotential U
(middle) and plot of the learned quasipotential along the line y = b (right). Lower Panel: Comparison of trajectories
of the learned dynamics and the original dynamics (24) from different initial states.

The dynamics for the system is governed by

dx
dt
dy
dt

= −

= −

1
2
1
2

∂U
∂x
∂U
∂y

(x, y) − 2 (x + 2y − a − 2b) ,

(x, y) + 2 (2x + y − 2a − b) ,

(24)

where the state of the system is x = (x, y)T . This dynamical system has a stable limit cycle on the ellipse in (23) and
an unstable equilibrium point at (a, b) inside the limit cycle.

We take a = 1, b = 2.5 and generate 2000 trajectories by solving the equations in (24) starting from initial states
sampled from the uniform distribution on the domain D = [−0.5, 2.5] × [1, 4]. Along each trajectory, we collect 100
data points. In total, X contains 2 × 105 data points. Out of these data points, 3712 representative data points are used
to impose the orthogonality condition.

Fig. 2 (lower panel) shows a comparison of one trajectory of the learned dynamics with that of the original dynamics
in the test dataset. The statistics (mean ± deviation) of the errors of 200 trajectories is 4.797 × 10−4 ± 2.923 × 10−4.
A comparison of the learned quasipotential with the exact quasipotential in (22) is shown in Fig. 2 (upper panel). The
rRMSE and rMAE for the learned quasipotential on the domain D are 0.0141 and 0.0090, respectively. The errors are
computed using Eqs. (21) with L = 104.

9

4.2 Biological system: budding yeast cell cycle

The previous two examples are toy problems where the exact quasipotential is known. Now, we test our method on a
more challenging problem where computing quasipotential landscapes using traditional methods may be very expen-
sive.

Example 3. We study the robustness of the reproduction process of a budding yeast cell cycle by constructing the
quasipotential [10]. The simpliﬁed network of yeast cell is composed of three modules: the G1/S module, the early
M module and the late M module. Based on the feedback of each module and the interactions between different
modules, the following dynamics has been proposed for the cell cycle

dx
dt
dy
dt
dz
dt

=

=

=

x2
1 + x2 − k1x − xy + a0,
j2
y2
2 + y2 − k2y − yz + ka1x,
j2
ksz2
3 + z2 − k3z − kizx + ka2y,
j2

(25)

where x, y, z represent the concentration of certain key regulators in the G1/S, early M and late M/G1 phase,
respectively. The values for the parameters j1,j2,j3,k1,k2,k3,ki,ks,ka1,ka2,a0 are taken from Ref. [10]. The dynamics
has a stable equilibrium state G1 approximately at (0, 0, zmax) where zmax = 4.342. The yeast cell cycle is termed a
robust process in [10], in the sense that most transition paths stay close to a particular pathway due to the dynamical
landscape. This pathway starts from the excited G1 state and ends at the stable G1 state by going through the S
phase approximately at (xmax, 0, 0) where xmax = 4.335 and the early M state approximately at (0, ymax, 0) where
ymax = 4.353.

We generate 104 trajectories by solving the equations in (25) starting from initial states sampled from the uniform

distribution on the set

{x = (x, y, z) ∈ [0, 5]3 : (cid:107)f (x)(cid:107)∞ < 5},
where the notation (cid:107)y(cid:107)∞ denotes the maximum of absolute values of the components in the vector y. The last condition
excludes states far away from the regions of interest corresponding to the transition events. Along each trajectory, we
collect 100 data points. In total, X contains 106 data points. Out of these data points, 8384 representative data points
are used to impose the orthogonality condition.

(26)

Fig. 3 (lower panel) shows a comparison of one trajectory of the learned dynamics and that of the original dynamics
in the test dataset. The statistics (mean ± deviation) of the errors of the 1000 trajectories is 0.161 ± 0.226. The cross-
sections of the learned quasipotential at z = 0 and x = 0 are shown in Fig. 3 (upper panel). The quasipotential
characterizes the robust process of the cell cycle, which agrees well with the result in Ref. [10] using the geometric
minimum action method. Moreover, notice that the quasipotential we compute can be evaluated at arbitrary points in
space (in the regions explored by the sampled data) and is not limited by any meshes, or choice of beginning and end
points for path-based methods.

4.3 High-dimensional systems: discretized PDEs

We next apply the proposed method to two high-dimensional systems which are obtained from the discretization of
PDEs. After discretization, the ﬁrst system is a gradient system in the 50-dimensional space with known quasipotential,
and the second one is a non-gradient system in the 40-dimensional space.

10

Figure 3 (Example 3): Upper Panel: Contour plots of the quasipotential projected onto the xy-plane with z = 0 (left)
and the yz-plane with x = 0 (right). Lower Panel: Comparison of one trajectory of the learned dynamics (left) and the
original dynamics in (25) (right).

Example 4. We consider the Ginzburg-Landau equation

with the boundary conditions u(0, t) = u(1, t) = 0 and the initial condition u(x, 0) = u0(x), where V (u) = 1
4 (1 −
u2)2 is the double-well potential and δ is a small parameter. The equation is a gradient ﬂow associated with the energy

ut = δuxx − δ−1V (cid:48)(u),

x ∈ [0, 1],

(27)

E[u] =

(cid:90) 1

0

(cid:18) 1
2

δu2

x + δ−1V (u)

(cid:19)

dx.

(28)

We partition the interval [0, 1] using I + 1 grid points x0,...,xI , where xi = ih and h = 1/I. Then we approximate

the spatial derivatives in Eq. (27) using the central ﬁnite difference and obtain the following system of ODEs

dui
dt

= δ

ui−1 − 2ui + ui+1
h2

− δ−1V (cid:48)(ui),

1 ≤ i ≤ I − 1,

(29)

with u0 = uI = 0 and the initial condition ui(0) = u0(xi) for 1 ≤ i ≤ I − 1, where ui denotes the approximate
solution at the grid point xi. The state of the system is denoted by u = (u1, . . . , uI−1). The ODE system is a gradient

11

Figure 4 (Example 4): Comparison of trajectories of the learned dynamics and the original dynamics (29) from different
initial states.

ﬂow associated with the energy

Eh[u] =

I
(cid:88)

i=1

1
2

δ

(cid:18) ui − ui−1
h

(cid:19)2

+ δ−1V (ui),

(30)

which is a discretization of the energy (28), up to the factor h. The dynamics (29) has two stable states at the two local
minima u± of the energy (30), which are shown in Fig. 4 (last column) for δ = 0.1. The quasipotential with respect to
the two stable states is

U (u) = 2Eh[u] + C

(31)

in the basins of attraction, where C is constant.

The number of discretization points is taken as I = 51. We generate 104 trajectories by solving the dynamics

in (29) starting from the initial states:

u0(x) =

a · ˜u(x)
maxy|˜u(y)|

,

(32)

where ˜u(x) = (cid:80)4
U (cid:0)0, 3
points, 76044 representative data points are used to impose the orthogonality condition.

k=1, a are drawn from the uniform distributions: uk ∼ U (−1, 1), a ∼
(cid:1). Along each trajectory, we collect 200 data points. In total, X contains 2 × 106 data points. Out of these data

k=1 ˆuk sin(kπx) and {ˆuk}4

2

Fig. 4 shows a comparison of two trajectories of the learned dynamics and those of the original dynamics in the test
dataset. The statistics (mean ± deviation) of the errors of the 1000 trajectories is 1.220 × 10−2 ± 7.734 × 10−2. To
assess the accuracy of the learned quasipotential, we compare Uθ and U in (31) along the minimum energy path (MEP)
from u− to u+. The MEP is computed using the string method [3]. The comparison is shown in Fig. 5, from which

12

Figure 5 (Example 4): Comparison of the learned and exact quasipotentials for the discretized Ginzburg-Landau equa-
tion along the MEP, where α is the normalized arc-length parameter along the MEP.

a good agreement can be observed. In particular, the learned quasipotential accurately captures the energy barrier
between the two stable states.

Example 5. We consider the dynamics of the Brusselator on the spatial interval [0, 1],

ut =

1
α

(cid:0)uxx + 1 + u2v − (1 + A)u(cid:1) ,

vt = vxx + Au − u2v,

(33)

with the Neumann boundary conditions ux(0, t) = ux(1, t) = 0, vx(0, t) = vx(1, t) = 0, and the initial condition
u(x, 0) = u0(x), v(x, 0) = v0(x), where α, A are parameters. We discretize the interval [0, 1] with grid points
x0, . . . , xI , where xi = ih and h = 1/I. Then we approximate the spatial derivatives in Eq. (33) using the central
ﬁnite difference and obtain the following system of ODEs

dui
dt
dvi
dt

=

=

(cid:18) ui−1 − 2ui + ui+1
h2

1
α
vi−1 − 2vi + vi+1
h2

+ Aui − u2

i vi,

+ 1 + u2

i vi − (1 + A)ui

(cid:19)

,

(34)

for 0 ≤ i ≤ I, with the Neumann boundary conditions imposed by u−1 = u1, uI+1 = uI−1, v−1 = v1, vI+1 = vI−1,
and the initial condition ui(0) = u0(xi), vi(0) = v0(xi) for 0 ≤ i ≤ I, where (ui, vi) denotes the solution of Eq. (33)
at xi. The state of the system is denoted by x = (u0, ..., uI , v0, ..., vI ). The dynamics has a stable state: ui = 1,
vi = A for 0 ≤ i ≤ I.

We take α = 0.1 and A = 0.5. The number of discretization points is taken as I = 19, so the discretized system is
in the 40-dimensional space. We generate 2 × 104 trajectories by solving the dynamics in (34) starting from the initial
states:

u0(x) =

a1 · ˜u(x)
maxy|˜u(y)|

a3 · ˜v(x)
maxy|˜v(y)|

+ a4,

(35)

+ a2,

v0(x) =

13

Figure 6 (Example 5): Upper Panel: Comparison of one trajectory of the learned dynamics and the original dynamics
in (34). Lower Panel: Contour plots of the quasipotential as a function of (ˆu0, ˆv0) for which the state is u(x) ≡ ˆu0,
v(x) ≡ ˆv0 (left), and as a function of (ˆu1, ˆv1) for which the state is u(x) = 1 + ˆu1 cos(πx), v(x) = 0.5 + ˆv1 cos(πx)
(right).

where ˜u(x) = (cid:80)4
the uniform distributions

k=0 ˆuk cos(kπx), ˜v(x) = (cid:80)4

k=0 ˆvk cos(kπx) and {ˆuk}4

k=0, {ˆvk}4

k=0, a1, a2, a3, a4 are drawn from

ˆuk ∼ U (−1, 1) , ˆvk ∼ U (−1, 1) , k = 0, . . . , 4,

(cid:18)

a1 ∼ U

0,

(cid:19)

1
2

, a2 ∼ U

(cid:18) 1
2

+ a1,

3
2

(cid:19)

(cid:18)

− a1

, a3 ∼ U

0,

(cid:19)

1
2

, a4 ∼ U (a3, 1 − a3) .

(36)

Along each trajectory, we collect 200 data points. In total, X contains 4 × 106 data points. Out of these data points,
26218 representative data points are used to impose the orthogonality condition.

Fig. 6 (upper panel) shows a comparison of one trajectory of the learned dynamics and that of the original dynamics
in the test dataset. The statistics (mean ± deviation) of the errors of the 2000 trajectories is 8.227 × 10−4 ± 6.741 ×
10−4. The quasipotential is shown in Fig. 6 (lower panel) as a function of (ˆu0, ˆv0), where (ˆu0, ˆv0) corresponds
to the state u(x) ≡ ˆu0, v(x) ≡ ˆv0 (left), and as a function of (ˆu1, ˆv1), where (ˆu1, ˆv1) corresponds to the state
u(x) = 1 + ˆu1 cos(πx), v(x) = 0.5 + ˆv1 cos(πx) (right). The numerical results agree well with those computed using
the minimum action method [2].

14

5 Conclusion

In this paper, we proposed a method for computing the quasipotential for dynamical systems and at the same time
learning the dynamics from the trajectory data. This method is based on learning an orthogonal decomposition of the
force ﬁeld into potential and rotational components, each parameterized by a neural network. The neural networks are
trained by minimizing a loss function composed of two parts: one is to reconstruct the dynamics and the other one is
to impose the orthogonality condition between the potential and rotational components. The quasipotential associated
with each attractor of the dynamical system can be obtained by conﬁning the potential component to the corresponding
basin of attraction. We successfully applied the method to various examples including systems with stable equilibrium
points, limit cycles and systems in high dimensions. The method is purely data driven in the sense that no explicit
form of the dynamical system is required; in fact, an explicit model for the dynamics is learned from the observed
trajectories in this method. To the best of our knowledge, this is the ﬁrst efﬁcient and accurate method that can be used
to map the landscape of the quasipotential in high dimensions.

After we obtain the quasipotential, we can compute other interesting objects associated with the dynamical system
perturbed by small noise. For example, we can identify the minimum action path between the attractor A and another
state. Using the fact that the tangent of the path is parallel to f + ∇UA along the minimum action path, the path can be
computed using the string method. The expected exit time from the basin of attraction can also be estimated using the
minimum value of the quasipotential on the boundary of the basin of attraction.

In the current work, we demonstrated the effectiveness of the proposed method using examples with different
features. In the future, we plan to apply the method to problems of practical interest such as dynamical systems in ﬂuid
mechanics and biological systems.

Acknowledgements

The work of Ren was supported in part by Singapore MOE AcRF grant R-146- 000-267-114, and the NSFC grant (No.
11871365). The work of QL was supported by the start-up grant at the National University of Singapore, under the
PYP programme.

References

[1] M. I. Freidlin and A. D. Wentzell, “Random Perturbations of Dynamical Systems,” 3rd Ed, Springer Press (2012).

[2] W. E, W. Ren, and E. Vanden-Eijnden, “Minimum action method for the study of rare events,” Commun. Pure

Appl. Math. 57, 637–656 (2004).

[3] W. E, W. Ren, and E. Vanden-Eijnden, “Simpliﬁed and improved string method for computing the minimum

energy paths in barrier-crossing events,” J. Chem. Phys. 126, 164103 (2007).

[4] X. Zhou, W. Ren, and W. E, “Adaptive minimum action method for the study of rare events,” J. Chem. Phys. 128,

104111 (2008).

[5] M. Heymann and E. Vanden-Eijnden, “The geometric minimum action method: A least action principle on the

space of curves,” Commun. Pure Appl. Math. 61, 1052-1117 (2008).

[6] M. K, Cameron, “Finding the quasipotential for nongradient SDEs,” Physica D 241, 1532-1550 (2012).

[7] D. Dahiya and M. Cameron, “Ordered line integral methods for computing the quasi-potential,” J. Scientiﬁc

Computing 75, 1351-1384 (2018).

15

[8] D. Dahiya and M. Cameron, “An ordered line integral method for computing the quasi-potential in the case of

variable anisotropic diffusion,” Physica D 382, 33-45 (2018).

[9] P. Zhou and T. Li, “Construction of the landscape for multi-stable systems: Potential landscape, quasipotential,

A-type integral and beyond,” J. Chem. Phys. 144, 094109 (2016).

[10] C. Lv, X. Li, F. Li, and T. Li, “Energy landscape reveals that the budding yeast cell cycle is a robust and adaptive

multi-stage process,” PLoS Comput. Biol. 11, e1004156 (2015).

[11] C. Li and G. Balazsi, “A landscape view on the interplay between EMT and cancer metastasis,” NPJ Syst. Biol.

Appl. 4, 1-9 (2018).

[12] J. Wang, C. Li, and E. Wang, “Potential and ﬂux landscapes quantify the stability and robustness of budding yeast

cell cycle network,” Proc. Natl. Acad. Sci. 107, 8195-8200 (2010).

[13] F. Bouchet, K. Gawedzki, and C. Nardini, “Perturbative calculation of quasi-potential in non-equilibrium diffu-

sions: a mean-ﬁeld example,” J. Stat. Phys. 163, 1157-1210 (2016).

[14] B. C. Nolting and K. C. Abbott, “Balls, cups, and quasi-potentials: quantifying stability in stochastic systems,”

Ecology 97, 850-864 (2016).

[15] C. Lv, X. Li, F. Li, and T. Li, “Constructing the energy landscape for genetic switching system driven by intrinsic

noise,” PLoS One 9, e88167 (2014).

[16] S. Yang, F. P. Samuel, and K. C. Maria, “Computing the quasipotential for nongradient SDEs in 3D,” J. Comput.

Phys. 379, 325-350 (2019).

[17] J. LaSalle, “Some extensions of Liapunov’s second method,” IRE Trans. Circuit Theory 7, 520-527 (1960).

[18] C. Yao and E. M. Bollt, “Modeling and nonlinear parameter estimation with Kronecker product representation

for coupled oscillators and spatiotemporal systems,” Physica D 227, 78-99 (2007).

[19] S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from data by sparse identiﬁcation

of nonlinear dynamical systems,” Proc. Natl. Acad. Sci. 113, 3932-3937 (2016).

[20] M. Raissi and G. E. Karniadakis, “Hidden physics models: Machine learning of nonlinear partial differential

equations,” J. Comput. Phys. 357, 125-141 (2018).

[21] Z. Long, Y. Lu, and B. Dong, “PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep

network,” J. Comput. Phys. 399, 108925 (2019).

[22] M. Raissi, and P. Perdikaris, and G. E. Karniadakis, “Multistep neural networks for data-driven discovery of

nonlinear dynamical systems,” arXiv preprint arXiv:1801.01236 (2018).

[23] M. Raissi, “Deep hidden physics models: Deep learning of nonlinear partial differential equations,” J. Mach.

Learn. Res. 19, 932-955 (2018).

[24] H. Yu, X. Tian, and Q. Li, “OnsagerNet: Learning Stable and Interpretable Dynamics using a Generalized On-

sager Principle,” arXiv preprint arXiv:2009.02327 (2020).

[25] G. Manek and J. Z. Kolter, “Learning stable deep dynamics models,” arXiv preprint arXiv:2001.06116 (2020).

16

[26] B. Li, S. Tang, and H. Yu, “Better approximations of high dimensional smooth functions by deep neural networks

with rectiﬁed power units,” arXiv preprint arXiv:1903.05858 (2019).

[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proceedings of International Confer-

ence on Learning Representations (ICLR), San Diego (2015).

17

