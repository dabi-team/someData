2
2
0
2

n
u
J

9
2

]

Y
S
.
s
s
e
e
[

2
v
5
1
9
7
0
.
6
0
2
2
:
v
i
X
r
a

Barrier Certiﬁed Safety Learning Control:
When Sum-of-Square Programming Meets Reinforcement Learning

Hejun Huang1, Zhenglong Li2, Dongkun Han1∗

Abstract— Safety guarantee is essential in many engineering
implementations. Reinforcement
learning provides a useful
learning
way to strengthen safety. However, reinforcement
algorithms cannot completely guarantee safety over realistic op-
erations. To address this issue, this work adopts control barrier
functions over reinforcement learning, and proposes a compen-
sated algorithm to completely maintain safety. Speciﬁcally, a
sum-of-squares programming has been exploited to search for
the optimal controller, and tune the learning hyperparameters
simultaneously. Thus, the control actions are pledged to be
always within the safe region. The effectiveness of proposed
method is demonstrated via an inverted pendulum model.
Compared to quadratic programming based reinforcement
learning methods, our sum-of-squares programming based
reinforcement learning has shown its superiority.

I. INTRODUCTION

Reinforcement learning (RL) is one of the most popular
methods for accomplishing the long-term objective of a
Markov-decision process [1]–[6]. This learning method aims
for an optimal policy that can maximize a long-term reward
in a given environment recursively. For the case of RL
policy gradient method [7]–[9], this reward-related learning
problem is traditionally addressed using gradient descent to
obtain superior control policies.

RL algorithms might generate desired results in some
academic trials, e.g., robotic experiments [10], [11] and
autonomous driving contexts [12], [13]. But for realistic
situations, how to completely guarantee the safety based on
these RL policies is still a problem. The intermediate policies
from the RL agent’s exploration and exploitation processes
may sometimes ruin the environment or the agent itself,
which is unacceptable for safety-critical systems. Moreover,
disturbances around the system would confuse the agent over
learning process, which make the control tasks more complex
and challenging.

To cope with the issues above, various methods are
proposed to specify dynamical safety during the learning
process. Initially, the region of attraction (ROA) from control
Lyapunov functions or Lyapunov-like function work [14],
[15] are used to certify the dynamical safety. Related and
useful work of ROA estimation are introduced in [16]–[19].
In 2017, [20] started to execute the RL’s exploration and
exploitation in ROA to maintain safety. Different from region
constraint, in 2018 [21], [22] proposed a shield framework
to select safe actions repeatedly. Inspired by this work, [23],

1Hejun Huang and Dongkun Han are with the Department of Mechanical

and Automation Engineering, The Chinese University of Hong Kong

2Zhenglong Li

is with the Department of Electrical and Electronic

Engineering, The University of Hong Kong

*Email: dkhan@mae.cuhk.edu.hk

[24] further combined the control barrier function (CBF) to
synthesis the RL-based controller. Note that, the CBF is not
new to identify safe regions in control theory, [25], [26] are
representative work to combine CBF in biped robot and [27]
explored its implementation of quadrotors, etc. Appropriate
region constraint for RL is important for building safe RL.
Hence, a reliable learning method is expected to construct a
controller inside the safe region over the process.

In this paper, we will consider a state-space model in the
polynomial form, including an unknown term that estimated
by Gaussian process. Then, we will apply Deep Deterministic
Policy Gradient (DDPG) algorithm to design a controller.
The generated control action will impact system dynam-
ics and further inﬂuence the sum-of-squares programming
(SOSP) solution of control barrier functions.

The main contributions of this work are: (1) Formulate
the framework to embed SOSP-based CBF into DDPG algo-
rithm; (2) compare the performance of quadratic program-
ming (QP)-based and SOSP-based controller with DDPG
algorithm; and (3) demonstrate the safety and efﬁciency of
DDPG-SOSP algorithm via an inverted pendulum model.

This paper is organized as follows, we ﬁrst present some
preliminaries about reinforcement learning, Gaussian pro-
cess and control barrier function in Section II. Then, in
Section III, we formulate the steps to compute an SOSP-
based controller and introduce a corresponding DDPG-SOSP
algorithm. Numerical examples are given for these cases in
Section IV, then we discuss the learning efﬁciency and safety
performance of QP-based and SOSP-based strategies, before
concluding this paper.

II. PRELIMINARY

A discrete-time nonlinear control-afﬁne system with state

st, st+1 ∈ S and control action at ∈ A is considered as:

st+1 = f (st) + g(st)at + d(st), ∀t ∈ [t, t],

(1)

where st, st+1 are ﬁnished within a bounded time t, while t
and t are constants.

Notation: From the inﬁnite-horizon discounted Markov
decision process (MDP) theory [28], let r : S × A → R de-
note the reward of the current state-action pair (st, at) in (1),
let P (st, st+1, at) = p(st+1|st, at) denote the probability
distribution to next state st+1, and let γ ∈ (0, 1) denote a dis-
count factor. Thus, a MDP tuple τ = (st, st+1, at, rt, Pt, γ)
establishes at each state t.

 
 
 
 
 
 
A. Reinforcement Learning

RL focuses on training an agent to map various situations
to the most valuable actions in a long-term process. During
the process, the agent will be inspired by a cumulative reward
r to ﬁnd the best policy π(a|s). Various novel RL algorithms
are proposed to select optimal π(a|s). For more details of
selecting π, we kindly refer interested readers to [1].

Given MDP tuple τ , the RL agent is selecting an optimal

policy π∗ by maximizing the expected reward J(π)

J(π) = Eτ [

∞
(cid:88)

γtr(st, at)].

(2)

t=0
Furthermore, the action value function Qπ can be generated
from τ and satisﬁes the Bellman equation as follows,
Qπ(st, at) = Est,at[rt + γEat+1[Qπ(st+1, at+1)]]

= Est+1,at+1,...[

∞
(cid:88)

γlr(st+l, at+l)].

(3)

l=0

In this paper, we will use DDPG algorithm, a typical
actor-critic and off-policy method, to handle the concerned
dynamics of (1). Let parameters θπ and θQ denote the neural
network of actor and critic, respectively. By interacting with
the environment repeatedly, the actor will generate an actor
policy π : S → A and be evaluated by the critic via
Qπ : S × A → R. These actions will be stored in replay
buffer to update the policy gradient w.r.t θπ, and the loss
function w.r.t θQ, once if the replay buffer is fulﬁlled.

However, the state-action of DDPG algorithm generates
without a complete safety guarantee, which is unacceptable
in safety critical situations. Inspired by the work of [23],
we propose a programming assist method to maintain safety
over the learning process in Section III.

B. Gaussian Process

Gaussian processes (GP) estimate the system and further
predict dynamics based on the prior data. A GP is a stochastic
process that construct a joint Gaussian distribution with
concerned states {s1, s2, . . . } ⊂ S. We use GP to estimate
the unknown term d in (1) during the system implementation.
Mixed with an independent Gaussian noise (0, σ2
n), the GP
model’s prior data {d0, d1, . . . , dk} can be computed from
dk = sk+1 − f − g · ak indirectly. Then, a high probability
statement of the posterior output ˆd : S → R generates as

d(s) ∼ N (md(s), σd(s)),

(4)

where the mean function md(s) and the variance function
σd(s) can describe the posterior distribution as

md(s) − kδσd(s) ≤ d(s) ≤ md(s) + kδσd(s),

(5)

with probability greater or equal to (1 − δ)k, δ ∈ (0, 1)
[29], where kδ is a parameter to design interval [(1 − δ)k, 1].
Therefore, by learning a ﬁnite number of data points, we can
estimate the unknown term d(s∗) of any query state s∗ ∈ S.
As the following lemma declaims in [30], [31], there exists
a polynomial mean function to approximate the term d via
GP, which can predict the output of state s∗ straightly here.

Lemma 1. Suppose we have access to k measurements of
d(s) in (1) that corrupted with Gaussian noise (0, σ2
n). If the
norm unknown term d(s) bounded, the following GP model
of d(s) can be established with polynomial mean function
md(s∗) and covariance function σ2
md(s∗) = ϕ(s∗)Tw,
d(s∗) = k(s∗, s∗) − kT
σ2

∗ (K + σ2

nI)−1k∗,

d(s∗),

(6)

within probability bounds [(1−δ)k, 1], where δ ∈ (0, 1), s∗ is
a query state, ϕ(s∗) is a monomial vector, w is a coefﬁcient
vector, [K](i,j) = k(si, sj) is a kernel Gramian matrix and
k∗ = [k(s1, s∗), k(s2, s∗), . . . , k(sk, s∗)]T.

Lemma 1 generates a polynomial expression of d(s)
within probabilistic range [(1 − δ)k, 1]. Meanwhile, the non-
linear term f (s) in (1) can be approximated by Chebyshev
interpolants Pk(s) and bounded remainder ξ(s) in certain
domain as f (s) = Pk(s) + ξ(s) [32], where k is the degree
of the polynomial Pk(x) [32]. Now we convert (1) as

st+1 = Pk(st) + g(st)at + dξ(st).

(7)

The polynomial (7) is equal to (1) with a new term dξ(st) =
d(st) + ξ(st). Consequently, we can obtain a polynomial
system within a probability range by learning dξ(st),

st+1 = Pk(st) + g(st)at + mdξ (st),

(8)

which will be used to synthesis the controller in Section III.

C. Control Barrier Function

The super-level set of the control barrier function (CBF)

h : S → R could validate an safety set C as

C = {st ∈ S : h(st) ≥ 0}.

(9)

Throughout this paper, we refer to C as a safe region and

U = S − C = {st ∈ S : h(st) < 0},

(10)

as unsafe regions of the dynamical system (8). Then, the
state st ∈ C will not enter into U by satisfying the forward
invariant constraint below,

∀st ∈ C : h(st) ≥ 0, ∆h(st, at) ≥ 0,

(11)

where ∆h(st, at) = h(st+1)−h(st). Before we demonstrate
the computation of h of the system (8), the Positivestellensatz
(P-satz) needs to be introduced ﬁrst [33].

Let P be the set of polynomials and P SOS be the set of sum
i (x), where

of squares polynomials, e.g., P (x) = (cid:80)k
pi(x) ∈ P and P (x) ∈ P SOS.
Lemma 2. For polynomials {ai}m
j=1 and p, deﬁne
a set B = {s ∈ S : {ai(s)}m
j=1 ≥ 0}. Let
B be compact. The condition ∀x ∈ S, p(s) ≥ 0 holds if the
following condition is satisﬁed,

i=1, {bj}n
i=1 = 0, {bj(s)}n

i=1 p2

(cid:26) ∃r1, . . . , rm ∈ P, s1, . . . , sn ∈ P SOS,
j=1 sjbj ∈ P SOS.

i=1 riai − (cid:80)n

p − (cid:80)m

(cid:3)

Lemma 2 points out that any strictly positive polynomial p
lies in the cone that generated by non-negative polynomials

Fig. 1. Workﬂow of the safe RL control with SOS program

j=1 in the set of B. Based on the deﬁnition of C and U,

{bj}n
P-satz will be adequately used in the safety veriﬁcation.

The sufﬁcient condition to establish a CBF is the existence
of a deterministic controller aCBF : S → A. Based on
Lemma 2, we can compute a polynomial controller aCBF
through SOS program to avoid entering into unsafe regions
µi(s) ∈ U, i = 1, 2, . . . , n at each step t.

Lemma 3. Provided a safe region C certiﬁed by the polyno-
mial barrier function h and some unsafe regions µi ∈ U, i ∈
Z+, in a polynomial system (8), if there exists a controller
aCBF that satisﬁes

a∗ = arg min (cid:107)aCBF (cid:107)2
st∈S,aCBF ∈A;L(st),Mi(st)∈P SOS ;
∆h(st, aCBF ) − L(st)h(st) ∈ P SOS,
− ∆h(st, aCBF ) − Mi(st)µi(st) ∈ P SOS,

s.t.

(12)

where L(st) and Mi(st) are SOS polynomials for i =
1, 2, . . . , n. Then, action a∗ is a minimum polynomial con-
troller that regulates the system (8) safely.

Proof. Let us suppose that there exists a CBF h that certiﬁed
a safe region C. Then, a deterministic input at = aCBF (st) is
needed to establish the necessary condition ∆h(st, aCBF ) ≥
0 in (11) of the system (8).

Since h is a polynomial function, if aCBF is also a poly-
nomial, ∆h will maintain in the polynomial form. According
to Lemma 2, we can formulate this non-negative condition
of ∆h as part of SOS constraint in the cone of h as

∆h(st, aCBF ) − L(st)h(st) ∈ P SOS,

(13)

where the auxiliary SOS polynomials L(st) is used to project
the related term into an non-negative compact domain C that
certiﬁed by the superlevel set of h(st).

We continue to leverage the P-satz again to drive the

dynamics of (8) far from unsafe areas µi(st) as follows,

−∆h(st, aCBF ) − µi(st)Mi(st) ∈ P SOS,

(14)

This lemma maintains the safety of polynomial system
(8) and minimizes the current control aCBF with a SOS
program. Multiple unsafe regions µi(st) are also considered
input a∗ in (12). Since Lemma 1
in solving the target
discussed the probabilistic relationship of (8) and (1), the
computed result of (12) of (1) can also be a feasible control
to regulate the true dynamics safely.

III. CBF GUIDING CONTROL WITH REINFORCEMENT
LEARNING

In the ﬁrst part of this section, we will further express
the computation steps of (12). In the second part, we will
go through the details about the SOSP-based CBF guiding
control with DDPG algorithm.

A. SOS Program Based CBF

Different from [23], our control aCBF is solved by SOS
program with polynomial barrier functions, rather than the
QP and linear barrier functions. In [23], they initially con-
structed QP with monotonous linear barrier constraints to
cope with RL. Obviously, a safe but more relaxed searching
area will enhance the RL training performance. We propose a
SOS program to compute a minimal control with polynomial
barrier function based on Lemma 3.

Lemma 3 declared that we can compute a minimal control
aCBF from an approximated polynomial system (8) directly.
However, it is hard to deﬁne the minimization of the convex
function aCBF in (12). The optimal control can be searched
by using the following 2 steps:

Step 1: Search an optimal auxiliary factor L(st) by

maximizing the scalar (cid:15)1,

L∗(st) = arg max
(cid:15)1∈R+,L(st)∈P SOS

(cid:15)1

s.t. ∆h(st, aCBF ) − L(st)h(st) − (cid:15)1 ∈ P SOS
− ∆h(st, aCBF ) − Mi(st)µi(st) ∈ P SOS,

(15)

where Mi(st) denote the corresponding SOS polynomials.
Guided by the optimization (12), we can solve a minimal
cost control to maintain system safety with (13) and (14),
which completes the proof.

where L(st) is an auxiliary factor to obtain a permissive
constraint for the existing action aCBF .

Step 2: Given L(st) from last step, search an optimal
control of aCBF with minimal control cost by minimizing

the scalar (cid:15)2,
a∗
t = arg min

(cid:15)2∈R+

(cid:15)2

s.t. ∆h(st, aCBF ) − L(st)h(st) − (cid:15)2 ∈ P SOS
− ∆h(st, aCBF ) − Mi(st)µi(st) ∈ P SOS.
Remark. The scalars (cid:15)1 and (cid:15)2 in (15) and (16) are used to
limit the magnitude of the coefﬁcients’ optimization of L(st)
and at, respectively.

(16)

The SOS programs above demonstrate the details of a
target controller computation of the system (8), and the
solution of (16) regulates the dynamical safety via a control
barrier function directly.

B. SOS Program Based CBF with Reinforcement Learning

The workﬂow of the CBF guiding DDPG control algo-
rithm is shown in Fig. 1, which is akin to the original
idea in [23] to push the agent’s action into a safe training
architecture. In Fig. 1, we list these ﬂowing elements into
brackets and highlight the corresponding input of each factor
computation.

We illustrate the core idea of CBF guiding RL control as,

at =aRL
θk

+ ¯ak + aCBF

t

,

(17)

where the ﬁnal action at consists of a RL-based controller
aRL
, a previous deployed CBF controller ¯aφ and a SOS
θk
program based CBF controller aCBF
. A clear distinction
of the subscript t and k is stated here: t denotes the step
number of each policy iteration and k denotes the policy
iteration number over the learning process.

t

the ﬁrst

i=0 aCBF
i

More speciﬁcally,

term in (17) is an action
generated from a standard DDPG algorithm with parameter
θ. The second term ¯ak = (cid:80)k−1
demotes a global
consideration of previous CBF controllers. Since it is im-
practical to compute the exact value of each CBF action
aCBF
at each step. Supported by the work of [23], we
i
approximate this term as ¯aφk ≈ ¯ak = (cid:80)k−1
, where
¯aφk denotes an output from the multilayer perceptron (MLP)
with hyperparameters φ. Then, we ﬁt the MLP and update
φk with (cid:80)k−1

) at each episode.

i=0 aCBF
i

, . . . , aRL
θi−1

(s, aRL
θ0

The third term aCBF

in (17) is a real-time value based on
the deterministic value st, aRL
(st) and ¯aφk (st). Although
θk
there exists an unknown term d in the system (1), we can still
solve the dynamical safety in the approximated polynomial
system (8). So, the optimal at in (17) can be computed by
the SOS program below with deterministic aRL
θk

and ¯aφk

i=0 aCBF
i
t

s.t.

a∗ = arg min (cid:107)ak(cid:107)2
st∈S,ak∈A;L(st),Mi(st)∈P SOS ;
∆h(st, ak) − L(st)h(st) ∈ P SOS,
− ∆h(st, ak) − Mi(st)µi(st) ∈ P SOS.
Thus, we can establish a controller by satisfying the solution
of (18) over the learning process.

(18)

Theorem 1. Given a barrier function h in (9), a partially
unknown dynamical system (1) and a corresponding approx-
imated system (8), while (8) is a stochastic statement of (1)

within the probabilistic range [(1 − δ)k, 1], suppose there
exists an action at satisfying (18) in the following form:

at(st) =aRL
θk

(st, at, st+1, rt) + ¯aφk (st,

k−1
(cid:88)

(st, aCBF
i

))

i=0

+ aCBF
t

(st, aRL
θk

, ¯aφk ).

(19)
Then, the controller (19) guarantees the system (1) inside
the safe region within the range of probability [(1 − δ)n, 1].

Proof. Regarding the system (8), this result follows directly
from Lemma 3 by solving the SOS program (18). The only
different part of SOS program (18) from the SOS program
(12) in Lemma 3 is the action ak here contains additional
deterministic value aRL
θk

(st) and ¯aφk (st).
Since the output of (18) can regulate the dynamics of the
model (8) in a safe region, while the approximated model (8)
is a stochastic statement of the original system (1) with the
probability greater or equal to (1 − δ)k. Then, the solution
of (18) can be regarded as a safe control to drive the system
(1) far from dangerous, which ends the proof.

We display an overview of the whole steps in the combina-
tion of the aforementioned factors over the learning process.
The detailed workﬂow is outlined in Algorithm 1

Algorithm 1: DDPG-SOSP
Input: Origin system (1); barrier function h.
Output: RL optimal policy π.

1 Preprocess (1) into (8).
2 Create the tuple ˆD to store {st, at, st+1, rt} in ˆD.
3 for k ∈ {1, 2, . . . , k} do
4

Execute SOSP (18) to obtain the real-time CBF
controller aCBF
and store it to train previous
CBF controller ¯aCBF
Actuate the agent with at in (19).
Construct ˆD and update θk and φk directly.

5

6

θk

k

.

7 return π.

IV. NUMERICAL EXAMPLE

A swing-up pendulum model from OpenAI Gym environ-

ment (Pendulum-v1) is used to verify the above theorems,

ml2 ¨θ = mgl sin(θ) + a.

(20)

,

(cid:21)

(cid:21)

(cid:20)

=

− g

(cid:20) ˙s1
˙s2

ml2 + d

Let s1 = θ, s2 = ˙θ. The dynamics of (20) are deﬁned as
s2
l sin (s1) + a
where d denotes unknown dynamics that generate by inaccu-
rate parameters ¯m = 1.4, ¯l = 1.4, as [23] introduced. When
we try to construct a polynomial system (8), the sin(s1) in
(21) will be approximated by Chebyshev interpolants within
s1 ∈ [−3, 3]. Then, with SOSOPT Toolbox [34], Mosek
solver [35] and Tensorﬂow, we implement Algorithm 1 in
the pendulum model.

(21)

Fig. 2. Comparison of the maximum absolute θ of different algorithms. The
dotted solid line, the dashed line, and the solid line denote the performance
of DDPG-ONLY, DDPG-QP and DDPG-SOSP, respectively The straight
line denotes the safe boundary |s1| = 1.

The related model parameters are given in Table 1.

Fig. 3. Comparison of the maximum absolute ˙θ of different algorithms.

TABLE 1: Swing-up Pendulum Model Parameters
Units
Model Parameter

Symbol

Value

Pendulum mass
Gravity
Pendulum length
Input torque
Pendulum Angle
Angle velocity
Angle acceleration

m
g
l
a
θ
˙θ
¨θ

1.0
10.0
1.0
[−15.0, 15.0]
[−1.00, 1.00]
[−60.0, 60.0]
—

kg
m/s2
m
N
rad
rad/s
rad/s2

We want to maintain the pendulum angle θ always in a safe
range [−1.0, 1.0] during the learning process. Three RL algo-
rithms are selected and compared in this example, including
the DDPG-ONLY algorithm [5], CBF-based DDPG-QP al-
gorithm [23] and CBF-based DDPG-SOSP algorithm (our
work). The codes of CBF-based DDPG-SOSP can be found
at the url: https://github.com/Wogwan/CCTA2022 SafeRL.

All of these algorithms are trained in 150 episodes and
each episode contains 200 steps. Each step is around 0.05s.
And the reward function r = θ2 + 0.1 ˙θ2 + 0.001a2 is deﬁned
such that the RL agent are expected to keep the pendulum
upright with minimal θ, ˙θ and ¨θ.

Fig. 2 compares the collected maximum |θ| at each
episode by using different algorithms. As a baseline algo-
rithm, DDPG-ONLY explored every state without any safety
considerations, while DDPG-QP sometimes violated the safe
state and DDPG-SOS completely kept in safe regions.

The historical maximum value of | ˙θ| of these algorithms is
compared and shown in Fig. 3. It is found that DDPG-SOS
is able to maintain the pendulum into a lower ˙θ easily than
others over episodes.

In Fig. 4, from the reward curve of these algorithms, it is
easy to observe the efﬁciency of different algorithms: DDPG-
SOS obtains comparatively lower average reward and thus a
better performance.

Fig. 5 shows the ﬁrst (2nd episode) and the ﬁnal (150th
episode) policy guided control performance between DDPG-

Fig. 4. Comparison of the accumulated reward of different algorithms.

QP and DDPG-SOSP algorithm. We observed 50 steps of
the pendulum angle to highlight the superiority of these two
algorithms. Both in Fig. 5(a) and (b), the ﬁnal policy can
reach a smoother control output with less time. Although
DDPG-SOSP takes a quicker and smoother action to stabilize
the pendulum than DDPG-QP, but the time consuming will
increase due to the dynamics regression and optimization
computation under uncertainty. Accordingly, the result of
DDPG-SOSP algorithm of model (20) is not only obviously
stabler than DDPG-QP, but also maintaining the RL algo-
rithm action’s safety strictly.

(a)

(b)

Fig. 5. The comparison of the 1st and the 150th policy performance of
the angle control task between (a) DDPG-QP and (b) DDPG-SOS.

V. CONCLUSION

In this paper, we consider a novel approach to guarantee
learning (RL) with sum-of-
the safety of reinforcement
squares programming (SOSP). The objective is to ﬁnd a
safe RL method toward a partial unknown nonlinear system
such that the RL agent’s action is always safe. One of the
typical RL algorithms, Deep Deterministic Policy Gradient
(DDPG) algorithm, cooperates with control barrier function
(CBF) in our work, where CBF is widely used to guarantee
the dynamical safety in control
theories. Therefore, we
leverage the SOSP to compute an optimal controller based
on CBF, and propose an algorithm to creatively embed this
computation over DDPG. Our numerical example shows
that SOSP in the inverted pendulum model obtains a better
control performance than the quadratic program work. It also
shows that the relaxations of SOSP can enable the agent to
generate safe actions more permissively.

For the future work, we will investigate how to incorporate
SOSP with other types of RL including value-based RL and
policy-based RL. Meanwhile, we check the possibility to
solve a SOSP-based RL that works in higher dimensional
cases.

REFERENCES

[1] R. S. Sutton, A. G. Barto, ”Reinforcement learning: An introduction,”

MIT press, 2018.

[2] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, “Bench-
marking deep reinforcement learning for continuous control,” in Proc.
Int. Conf. on Machine Learning, pp. 1329–1338, PMLR, 2016.
[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.

[4] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
et al., “Mastering the game of go with deep neural networks and tree
search,” Nature, vol. 529, no. 7587, pp. 484–489, 2016.

[5] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, D. Wierstra, “Continuous control with deep reinforcement
learning,” ArXiv:1509.02971, 2015.

[6] Li, Z., Cheng, X., Peng, X. B., Abbeel, P., Levine, S., Berseth,
G., Sreenath, K., ”Reinforcement Learning for Robust Parameterized
Locomotion Control of Bipedal Robots,” in Proc. Int. Conf. on
Robotics and Automation, pp. 2811–2817, 2021.

[7] J. Peters, S. Schaal, “Reinforcement learning of motor skills with
policy gradients,” Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.
[8] R. S. Sutton, et al. ”Policy gradient methods for reinforcement learn-
ing with function approximation,” Advances in Neural Information
Processing Systems, 1999.

[9] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller,
“Deterministic policy gradient algorithms,” in Proc. Int. Conf. on
Machine Learning, pp. 387–395, PMLR, 2014.

[10] J. Kober, J. A. Bagnell, J. Peters, “Reinforcement learning in robotics:
A survey,” Int. Journal of Robotics Research, vol. 32, no. 11, pp. 1238–
1274, 2013.

[11] P. Kormushev, S. Calinon, D. G. Caldwell, “Reinforcement learning
in robotics: Applications and real-world challenges,” Robotics, vol. 2,
no. 3, pp. 122–148, 2013.

[12] J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel,
J. Z. Kolter, D. Langer, O. Pink, V. Pratt, et al., “Towards fully au-
tonomous driving: Systems and algorithms,” IEEE Intelligent Vehicles
Symposium, pp. 163–168, 2011.

[13] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab,
S. Yogamani, P. P´erez, “Deep reinforcement learning for autonomous
driving: A survey,” IEEE Trans. on Intelligent Transportation Systems,
2021.

[14] L. Wang, D. Han, M. Egerstedt, “Permissive barrier certiﬁcates for
safe stabilization using sum-of-squares,” in Proc. Amer. Control Conf.,
pp. 585–590, 2018.

[15] D. Han, D. Panagou, “Robust multitask formation control via para-
metric Lyapunov-like barrier functions,” IEEE Trans. on Automatic
Control, vol. 64, no. 11, pp. 4439–4453, 2019.

[16] G. Chesi, ”Domain of attraction: analysis and control via SOS pro-
gramming”, Springer Science & Business Media, vol. 415, 2011.
[17] D. Han, G. Chesi, Y. S. Hung, “Robust consensus for a class of
uncertain multi-agent dynamical systems,” IEEE Trans. on Industrial
Informatics, vol. 9, no. 1, pp. 306–312, 2012.

[18] D. Han, G. Chesi, “Robust synchronization via homogeneous

parameter-dependent polynomial contraction matrix,” IEEE Trans. on
Circuits and Systems I: Regular Papers, vol. 61, no. 10, pp. 2931–
2940, 2014.

[19] D. Han, M. Althoff, “On estimating the robust domain of attraction
for uncertain non-polynomial systems: An LMI approach,” in Proc.
Conf. on Decision and Control, pp. 2176–2183, 2016.

[20] B. Felix, M. Turchetta, A. Schoellig, A. Krause, ”A Safe model-based
reinforcement learning with stability guarantees,” Advances in Neural
Information Processing Systems, pp. 909–919, 2018.

[21] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer, S. Niekum,
U. Topcu, “Safe reinforcement learning via shielding,” in AAAI Conf.
on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.

[22] C. Steven, N. Jansen, S. Junges, U. Topcu, ”Safe Reinforcement
Learning via Shielding for POMDPs,” ArXiv:2204.00755, 2022.
[23] R. Cheng, G. Orosz, R. M. Murray, J. W. Burdick, “End-to-end safe
reinforcement
learning through barrier functions for safety-critical
continuous control tasks,” in AAAI Conf. on Artiﬁcial Intelligence,
vol. 33, no. 1, 2019.

[24] R. Cheng, A. Verma, G. Orosz, S. Chaudhuri, Y. Yue, J. W. Burdick,
”Control Regularization for Reduced Variance Reinforcement Learn-
ing,” in Proc. Int. Conf. on Machine Learning. pp. 1141–1150, 2019.
[25] S. C. Hsu, X. Xu, A. D. Ames, “Control barrier function based
quadratic programs with application to bipedal robotic walking,” in
Proc. Amer. Control Conf., pp. 4542–4548, 2015.

[26] T. Akshay, Z. Jun, K. Sreenath. ”Safety-Critical Control and Plan-
ning for Obstacle Avoidance between Polytopes with Control Barrier
Functions,” in Proc. Int. Conf. on Robotics And Automation, accepted,
2022.

[27] L. Wang, E. A. Theodorou, M. Egerstedt, “Safe learning of quadrotor
dynamics using barrier certiﬁcates,” in Proc. Int. Conf. on Robotics
And Automation, pp. 2460–2465, 2018.

[28] P. Martin, ”Markov decision processes: discrete stochastic dynamic

programming”, John Wiley & Sons, 2014.

[29] A. Lederer, J. Umlauft, S. Hirche, “Uniform error bounds for Gaussian
process regression with application to safe control,” Advances in
Neural Information Processing Systems, pp. 659–669, 2019.

[30] H. Huang, D. Han, ”On Estimating the Probabilistic Region of
Attraction for Partially Unknown Nonlinear Systems: An Sum-of-
Squares Approach,” in Proc. Chinese Control and Decision Conf.,
accepted, 2022.

[31] D. Han, H. Huang, ”Sum-of-Squares Program and Safe Learning on
Maximizing the Region of Attraction of Partially Unknown Systems,”
in Proc. Asian Control Conf., 2022.

[32] L. N. Trefethen, ”Approximation Theory and Approximation Practice,”

SIAM, 2019.

[33] M. Putinar, “Positive Polynomials on Compact Semi-algebraic Sets,”
Indiana University Mathematics Journal, vol. 42, no. 3, pp. 969–984,
1993.

[34] P. Seiler, ”SOSOPT: A toolbox for polynomial optimization,”

ArXiv:1308.1889, 2013.

[35] M. ApS, ”Mosek optimization toolbox for Matlab,” User’s Guide And

Reference Manual, Ver. 4, 2019.

