0
2
0
2

p
e
S
1
1

]

G
L
.
s
c
[

2
v
2
7
6
8
0
.
5
0
0
2
:
v
i
X
r
a

Hyperbolic Distance Matrices

Puoya Tabaghi
University of Illinois at Urbana-Champaign
tabaghi2@illinois.edu

Ivan Dokmanić
University of Basel
ivan.dokmanic@unibas.ch

ABSTRACT
Hyperbolic space is a natural setting for mining and visualizing
data with hierarchical structure. In order to compute a hyperbolic
embedding from comparison or similarity information, one has to
solve a hyperbolic distance geometry problem. In this paper, we
propose a unified framework to compute hyperbolic embeddings
from an arbitrary mix of noisy metric and non-metric data. Our
algorithms are based on semidefinite programming and the notion
of a hyperbolic distance matrix, in many ways parallel to its famous
Euclidean counterpart. A central ingredient we put forward is a
semidefinite characterization of the hyperbolic Gramian—a matrix
of Lorentzian inner products. This characterization allows us to
formulate a semidefinite relaxation to efficiently compute hyper-
bolic embeddings in two stages: first, we complete and denoise the
observed hyperbolic distance matrix; second, we propose a spectral
factorization method to estimate the embedded points from the hy-
perbolic distance matrix. We show through numerical experiments
how the flexibility to mix metric and non-metric constraints allows
us to efficiently compute embeddings from arbitrary data.

CCS CONCEPTS
• Human-centered computing → Hyperbolic trees; • Com-
puting methodologies → Machine learning; • Networks;

KEYWORDS
distance geometry, hyperbolic space, semidefinite program, spectral
factorization

ACM Reference Format:
Puoya Tabaghi and Ivan Dokmanić. 2020. Hyperbolic Distance Matrices. In
Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD ’20), August 23–27, 2020, Virtual Event, CA, USA. ACM,
New York, NY, USA, 11 pages. https://doi.org/10.1145/3394486.3403224

1 INTRODUCTION
Hyperbolic space is roomy. It can embed hierarchical structures
uniformly and with arbitrarily low distortion [30, 45]. Euclidean
space cannot achieve comparably low distortion even using an
unbounded number of dimensions [33].

Embedding objects in hyperbolic spaces has found a myriad
applications in exploratory science, from visualizing hierarchical

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’20, August 23–27, 2020, Virtual Event, CA, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7998-4/20/08. . . $15.00
https://doi.org/10.1145/3394486.3403224

structures such as social networks and link prediction for symbolic
data [40, 53] to natural language processing [14, 31], brain networks
[8], gene ontologies [4] and recommender systems [10, 54].

Commonly in these applications, there is a tree-like data struc-
ture which encodes similarity between a number of entities. We
experimentally observe some relational information about the struc-
ture and the data mining task is to find a geometric representation
of the entities consistent with the experimental information. In
other words, the task is to compute an embedding. This concept
is closely related to the classical distance geometry problems and
multidimensional scaling (MDS) [29] in Euclidean spaces [16, 32].
The observations can be metric or non-metric. Metric observa-
tions convey (inexact) distances; for example, in internet distance
embedding a small subset of nodes with complete distance informa-
tion are used to estimate the remaining distances [47]. Non-metric
observations tell us which pairs of entities are closer and which are
further apart. The measure of closeness is typically derived from
domain knowledge; for example, word embedding algorithms aim
to relate semantically close words and their topics [36, 43].

In scientific applications it is desirable to compute good low-
dimensional hyperbolic embeddings. Insisting on low dimension
not only facilitates visualization, but also promotes simple expla-
nations of the phenomenon under study. However, in most works
that leverage hyperbolic geometry the embedding technique is not
the primary focus and the related computations are often ad hoc.
The situation is different in the Euclidean case, where the notions
of MDS, Euclidean distance matrices (EDMs) and their characteriza-
tion in terms of positive semidefinite Gram matrices play a central
role in the design and analysis of algorithms [3, 32].

In this paper, we focus on computing low-dimensional hyperbolic
embeddings. While there exists a strong link between Euclidean
geometry and positive (semi)definiteness, we prove that what we
call hyperbolic distance matrices (HDMs) can also be characterized
via semidefinite constraints. Unlike in the Euclidean case, the hyper-
bolic analogy of the Euclidean Gram matrix is a linear combination
of two rank-constrained semidefinite variables. Together with a
spectral factorization method to directly estimate the hyperbolic
points, this characterization gives rise to flexible embedding algo-
rithms which can handle diverse constraints and mix metric and
non-metric data.

1.1 Related Work
The usefulness of hyperbolic space stems from its ability to effi-
ciently represent the geometry of complex networks [5, 28]. Em-
bedding metric graphs with underlying hyperbolic geometry has
applications in word embedding [36, 43], geographic routing [27],
routing in dynamical graphs [12], odor embedding [61], internet net-
work embedding for delay estimation and server selection [7, 47], to
name a few. In the literature such problems are known as hyperbolic
multidimensional scaling [13].

 
 
 
 
 
 
There exist Riemann gradient-based approaches [11, 31, 40, 41]
which can be used to directly estimate such embeddings from metric
measurements [44]. We emphasize that these methods are iterative
and only guaranteed to return a locally optimal solution. On the
other hand, there exist one-shot methods to estimate hyperbolic
embeddings from a complete set of measured distances. The method
of Wilson et al. [55] is based on spectral factorization of an inner
product matrix (we refer to it as hyperbolic Gramian) that directly
minimizes a suitable stress. In this paper, we derive a semidefinite
relaxation to estimate the missing measurements and denoise the
distance matrix, and then follow it with the spectral embedding
algorithm.

Non-metric (or order) embedding has been proposed to learn
visual-semantic hierarchies from ordered input pairs by embed-
ding symbolic objects into a low-dimensional space [52]. In the
Euclidean case, stochastic triplet embeddings [50], crowd kernels
[49], and generalized non-metric MDS [1] are some well-known
order embedding algorithms. For embedding hierarchical struc-
tures, Ganea et al. [20] model order relations as a family of nested
geodesically convex cones. Zhou et. al. [61] show that odors can be
efficiently embedded in hyperbolic space provided that the similar-
ity between odors is based on the statistics of their co-occurrences
within natural mixtures.

1.2 Contributions
We summarize our main contributions as follows:

• Semidefinite characterization of HDMs: We introduce
HDMs as an elegant tool to formalize distance problems in
hyperbolic space; this is analogous to Euclidean distance
matrices (EDM). We derive a semidefinite characterization
of HDMs by studying the properties of hyperbolic Gram
matrices—matrices of Lorentzian (indefinite) inner products
of points in a hyperbolic space.

• A flexible algorithm for hyperbolic distance geometry
problems (HDGPs): We use the semidefinite characteriza-
tion to propose a flexible embedding algorithm based on
semidefinite programming. It allows us to seamlessly com-
bine metric and non-metric problems in one framework and
to handle a diverse set of constraints. The non-metric and
metric measurements are imputed as linear and quadratic
constraints.

• Spectral factorization and projection: We compute the
final hyperbolic embeddings with a simple, closed-form spec-
tral factorization method.1 We also propose a suboptimal
method to find a low-rank approximation of the hyperbolic
Gramian in the desired dimension.

1.3 Paper Organization
We first briefly review the analytical models of hyperbolic space
and formalize hyperbolic distance geometry problems (HDGPs) in
Section 2. Our framework is parallel with semidefinite approaches
for Euclidean distance problems as per Table 1. In the ’Loid model,

1After posting the first version of our manuscript we became aware that such a
one-shot spectral factorization technique was proposed at least as early as in [55]. The
same technique is also used by [26].

Table 1: Essential elements in semidefinite approach for dis-
tance problems, Euclidean versus hyperbolic space.

Euclidean

Hyperbolic

Euclidean Distance Matrix
Gramian
Semidefinite relaxation
to complete an EDM
Spectral factorization of a

Hyperbolic Distance Matrix
H-Gramian
Semidefinite relaxation
to complete an HDM
Spectral factorization of an

Gramian to estimate the points H-Gramian to estimate the points

we define hyperbolic distance matrices to compactly encode hy-
perbolic distance measurements. We show that an HDM can be
characterized in terms of the matrix of indefinite inner products,
the hyperbolic Gramian. In Section 3, we propose a semidefinite
representation of hyperbolic Gramians, and in turn HDMs. We cast
HDGPs as rank-constrained semidefinite programs, which are then
convexified by relaxing the rank constraints. We develop a spectral
method to find a sub-optimal low-rank approximation of the hy-
perbolic Gramian, to the correct embedding dimension. Lastly, we
propose a closed-form factorization method to estimate the embed-
ded points. This framework lets us tackle a variety of embedding
problems, as shown in Section 4, with real (odors) and synthetic
(random trees) data. The proofs of propositions and derivations of
proposed algorithms are given in the appendix and a summary of
used notations is given in Table 2.

Table 2: Summary of notations.

Symbol

[m]
[M ]2
as
x = [x0, . . . , xm−1]⊤
X = (xi, j )i ∈[m], j ∈[n]
X ⪰ 0
∥X ∥F
∥X ∥2
∥X ∥1, 2
EN [x ]
em ∈ RM
Pr (X )
1
0

Meaning
Short for {1, . . . , m }
Asymmetric pairs {(m, n) : m < n, m, n ∈ [M ]}
A vector in Rm
A matrix in Rm×n
A positive semidefinite (square) matrix
Frobenius norm of X
Operator norm of X
The ℓ2 norm of columns’ ℓ1 norms, (cid:13)
Empirical expectation of a random variable, N −1 (cid:205)N
The m-th standard basis vector in RM
The projection of X ⪰ 0 onto the span of its top r eigenvectors
All-one vector of appropriate dimension
All-zero vector of appropriate dimension

(cid:13)[ ∥x1 ∥1 , . . . , ∥xn ∥1]⊤(cid:13)
(cid:13)2

n=1 xn

2 HYPERBOLIC DISTANCE GEOMETRY

PROBLEMS

2.1 Hyperbolic Space
Hyperbolic space is a simply connected Riemannian manifold with
constant negative curvature [6, 9]. In comparison, Euclidean and
elliptic geometries are spaces with zero (flat) and constant positive
curvatures. There are five isometric models for hyperbolic space:
half-space (Hd ), Poincaré (interior of the disk) (Id ), jemisphere (Jd ),
Klein (Kd ), and ’Loid (Ld ) [9] (Figure 1). Each provides unique
insights into the properties of hyperbolic geometry.

In the machine learning community the most popular models of
hyperbolic geometry are Poincaré and ’Loid. We work in the ’Loid
model as it has a simple, tractable distance function. It lets us cast
the HDGP (formally defined in Section 2.2) as a rank-constrained

where H is the indefinite matrix given by (2).

The subtle difference between the Lorentz Gramian (defined for
points in Rd +1) and the H-Gramian (defined only on Ld ⊂ Rd +1)
will be important for the low-rank projection and the spectral
factorization steps in Section 3. The indefinite inner product (1)
also determines the distance between x, y ∈ Ld , as

d(x, y) = acosh(−[x, y]).

(4)

2.1.2 Poincaré Model. In the Poincaré model (Id ), the points reside
in the unit d-dimensional Euclidean ball. The distance between
x, y ∈ Id is given by

d(x, y) = acosh(cid:16)1 + 2

∥x − y∥2
(1 − ∥x ∥2)(1 − ∥y∥2)

(cid:17)

.

(5)

The isometry between the ’Loid and the Poincaré model, h : Ld →
Id is called the stereographic projection. For y = h(x), we have
yi = xi+1
x0 + 1 ,

(6)

x = h−1 (y) =

The inverse of stereographic projection is given by
(cid:21)

1
1 − ∥y∥2
The isometry between the ’Loid and Poincaré models makes them
equivalent in their embedding capabilities. However, the Poincaré
model facilitates visualization of the embedded points in a bounded
disk, whereas the ’Loid model is an unbounded space.

(cid:20) 1 + ∥y∥2
2y

(7)

.

2.2 Hyperbolic Distance Problems
In a metric hyperbolic distance problem, we want to find a point
set x1, . . . , xN ∈ Ld , such that

dmn = acosh (−[xm, xn ]) ,

for all (m, n) ∈ C,

for a subset of measured distances D = (cid:8)dmn : (m, n) ∈ C ⊆ [N ]2
(cid:9).
as
In many applications we have access to the true distances only
through an unknown non-linear map ˜dmn = ϕ(dmn ); examples are
connectivity strength of neurons [22] or odor co-ocurrence statis-
tics [61]. If all we know is that ϕ(·) is a monotonically increasing
function, then only the ordinal information has remained intact,
dkl ≤ dmn ⇔ ˜dkl ≤ ˜dmn .
This leads to non-metric problems in which the measurements are
in the form of binary comparisons [1].

Definition 4. For a set of binary distance comparisons of the
form dkl ≤ dmn , we define the set of ordinal distance measurements
as
O = (cid:8)(k, l, m, n) : dkl ≤ dmn, (k, l), (m, n) ∈ [N ]2

(cid:9) .

as

We are now in a position to give a unified definition of metric

and non-metric embedding problems in a hyperbolic space.

Problem 1. A hyperbolic distance geometry problem aims to find

x1, . . . , xN ∈ Ld , given

• a subset of pairwise distances D such that

dmn = d(xm, xn ),

for all dmn ∈ D

• and/or a subset of ordinal distances measurements O such that

d(xi1 , xi2 ) ≤ d(xi3 , xi4 ),

for all i ∈ O.

Figure 1: Models of hyperbolic space with level sets (colors)
illustrating isometries.

semidefinite program. Importantly, it also leads to a closed-form
embedding by a spectral method. For better visualization, however,
we map the final embedded points to the Poincaré model via the
stereographic projection, see Sections 2.1.2 and 4.

’Loid Model. Let x and y be vectors in Rd +1 with d ≥ 1. The

2.1.1
Lorentzian inner product of x and y is defined as

where

[x, y] = x ⊤Hy,

H =

(cid:18)−1
0

(cid:19)

0⊤
I

∈ R(d +1)×(d +1).

(1)

(2)

This is an indefinite inner product on Rd +1. The Lorentzian inner
product has almost all the properties of ordinary inner products,
except that

∥x ∥2
H

def= [x, x]

can be positive, zero, or negative. The vector space Rd +1 equipped
with the Lorentzian inner product (1) is called a Lorentzian (d + 1)-
space, and is denoted by R1,d . In a Lorentzian space we can define
notions similar to the Gram matrix, adjoint, and unitary matrices
known from Euclidean spaces as follows.

Definition 1 (H-adjoint [23]). The H-adjoint R[∗] of an arbi-

trary matrix R ∈ R(d +1)×(d +1) is characterized by
[Rx, y] = [x, R[∗]y], ∀x, y ∈ Rd +1

.

Equivalently,

R[∗] = H −1

R⊤H .

(3)

Definition 2 (H-unitary matrix [23]). An invertible matrix R

is called H-unitary if R[∗] = R−1

.

The ’Loid model of d-dimensional hyperbolic space is a Riemann-

ian manifold Ld = (Ld , (дx )x ), where
Ld = (cid:110)
x ∈ Rd +1 : ∥x ∥2
H
and дx = H is the Riemannian metric.

= −1, x0 > 0(cid:111)

Definition 3 (Lorentz Gramian, H-Gramian). Let the columns
of X = [x1, x2, · · · , xN ] be the positions of N points in Rd +1
(resp.
Ld ). We define their corresponding Lorentz Gramian (resp. H-Gramian)
as

G = (cid:0)[xi , xj ](cid:1)
= X ⊤HX

i, j ∈[N ]

where d(x, y) = acosh (−[x, y]) and i = (i1, i2, i3, i4).

We denote the complete sets of metric and non-metric measure-

ments by Dc and Oc .

3 HYPERBOLIC DISTANCE MATRICES
We now introduce hyperbolic distance matrices in analogy with Eu-
clidean distance matrices to compactly encode inter-point distances
of a set of points x1, . . . , xN ∈ Ld .

Definition 5. The hyperbolic distance matrix (HDM) correspond-

ing to the list of points X = [x1, . . . , xN ] ∈ (Ld )N is defined as
D = D(X ) = (cid:0)d(xi , xj )(cid:1)

i, j ∈[N ] .

The ij-th element of D(X ) is hyperbolic distance between xi and xj ,
given by d(xi , xj ) = acosh(−[xi , xj ]) and for all i, j ∈ [N ].

HDMs are characterized by Lorentzian inner products which
allows us to leverage the definition of an H-Gramian (Definition 3).
Given points x1, . . . , xN ∈ Ld , we compactly write the HDM corre-
sponding to G as

D = acosh[−G],

(8)

where acosh[·] is an elementwise acosh(·) operator.

We now state our first main result: a semidefinite character-
ization of H -Gramians. This is a key step in casting HDGPs as
rank-constrained semidefinite programs.

Proposition 1 (Semidefinite characterization of H-Gramian).

Let G be the hyperbolic Gram matrix for a set of points x1, · · · , xN ∈
Ld . Then,

G = G

+ − G−

where

+

, G− ⪰ 0,
G
+ ≤ d,
rank G
rank G− ≤ 1,
diag G = −1,
e⊤
i Gej ≤ −1, ∀i, j ∈ [N ].

Conversely, any matrix G ∈ RN ×N that satisfies the above conditions
is a hyperbolic Gramian for a set of N points in Ld .

The proof is given in Proposition 1.

3.1 Solving for the H-Gramians
While Problem 1 could be formalized directly in X domain, this
approach is unfavorable as the optimization domain, Ld , is a non-
convex set. What is more, the hyperbolic distances
(cid:1)

d(xm, xn ) = acosh (cid:0)−e⊤

mX ⊤HXen

(9)

are non-linear functions of X with an unbounded gradient [13].
Similar issues arise when computing embeddings in other spaces
such as Euclidean [16] or the space of polynomial trajectories [48].
A particularly effective strategy in the Euclidean case is the semidef-
inite relaxation which relies on the simple fact that the Euclidean
Gramian is positive semidefinite. We thus proceed by formulat-
ing a semidefinite relaxation for hyperbolic embeddings based on
Proposition 1.

Solving the HDGP involves two steps, summarized in Algo-

rithm 1:

(1) Complete and denoise the HDM via a semidefinite program;
(2) Compute an embedding of the clean HDM: we propose a

closed-form spectral factorization method.

Note that step (2) is independent of step (1): given accurate hy-
perbolic distances, spectral factorization will give the points that
reproduce them. However, since the semidefinite relaxation might
give a Gramian with a higher rank than desired, eigenvalue thresh-
olding in step (2) might move the points off of Ld . That is because
eigenvalue thresholding can violate the necessary condition for
the hyperbolic norm, ∥x ∥2
= −1, or diag G = −1 in Proposition 1.
H
We fix this by projecting each individual point to Ld . The spectral
factorization and the projection are summarized in Algorithms 2
and 3.

Let (cid:101)D be the measured noisy and incomplete HDM, with un-
known entries replaced by zeroes. We define the mask matrix
W = (wi j ) as

def=

wi j

(cid:40)1, for (i, j) ∈ C ∨ (j, i) ∈ C
0, otherwise.

.

This mask matrix lets us compute the loss only at those entries that
were actually measured. We use the semidefinite characterization
of hyperbolic Gramians in Proposition 1 to complete and denoise
the measured HDM, and eventually solve HDGP.

Although the set of hyperbolic Gramians for a given embedding
dimension is non-convex due to the rank constraints, discarding the
rank constraints results in a straightforward semidefinite relaxation.
However, if we convexify the problem by simply discarding the rank
constraints, then all pairs (G1, G2) ∈ (cid:8)(G
+ + P, G− + P) : P ⪰ 0(cid:9)
become a valid solution. On the other hand, since

rank G + P ≥ rank G for G, P ⪰ 0,
we can eliminate this ambiguity by promoting low-rank solutions
for G

+ and G−. While directly minimizing
+ + rank G−

rank G

(10)

is NP-hard [51], there exist many approaches to make (10) computa-
tionally tractable, such as trace norm minimization [38], iteratively
reweighted least squares minimization [19], or the log-det heuristic
[18] that minimizes the following smooth surrogate for (10):

log det(G

+ + δI ) + log det(G− + δI ),

Algorithm 1 HDGP algorithm
1: procedure HDGP((cid:101)D, (cid:101)O, d)
2:

Input: Incomplete and noisy distance matrix, (cid:101)D, and ordi-

nal measurements, (cid:101)O, and embedding dimension, d.

3:

4:

5:

6:

G = SDR((cid:101)D, (cid:101)O, d)
X = Embed(G, d)
For X = [x1, . . . , xN ] ∈

▷ Complete & denoise HDM.
▷ Embed points in Ld .

(cid:16)Ld (cid:17) N

, let

yn = h(xn ), ∀n ∈ [N ]

where h(·) is given by (6).

return Y = [y1, · · · , yN ] ∈

(cid:16)Id (cid:17) N

.

▷ Map the points to Id .

7: end procedure

Table 3: Examples of specialized HDGP objectives.

Cost function

Tr W

+
k G

+ + Tr W −

k G−

Tr G

Tr G

+ + Tr G− + (cid:205)
k pk ϵk
+ + Tr G− + λ ∥C ∥1,2

= I

Parameters
+
= I , W −
• W
k
k
+
+
+ δI )−1, W −
= (G
• W
k
k
k
+
+
), W −
= I − Pd (G
• W
k
k
k
= 1
• pk
• (cid:205)
k pk
(cid:13)
cosh[(cid:101)D] + G
(cid:13)
(cid:13)

= M, 0 ≤ pk ≤ 1
+ − G− + C

•

= (G−
k
= I − P1(G−
k

+ δI )−1
)

(cid:13)
(cid:13)
(cid:13)F

≤ ϵ

+
k G

= (G−
k

+ +Tr W −

where δ > 0 is a small regularization constant. This objective func-
+δI )−1
tion is linearized as C +Tr W
+ δI )−1, which can be iteratively minimized2. In our
and W −
k
numerical experiments we will uset he trace norm minimization
unless otherwise stated. Then, we enforce the data fidelity objec-
tives and the properties of the embeddings space (Proposition 1) in
the form of a variety of constraints.

k G− for W

= (G

+
k

+
k

Metric embedding: The quadratic constraint

(cid:13)
(cid:13)

(cid:13)W ◦ (cid:0)cosh[(cid:101)D] + G(cid:1)(cid:13)

2
(cid:13)
(cid:13)

F

≤ ϵ1

makes sure the hyperbolic Gramian, G, accurately reproduces the
given distance data.

Non-metric embedding: The ordinal measurement constraint

of

d(xi1 , xi2 ) ≤ d(xi3 , xi4 ),

is simply a linear constraint in form of
i1Gei2 − e⊤
where i ∈ O and i = (i1, i2, i3, i4). In practice, we replace this
constraint by Li (G) ≥ ϵ2 > 0 to avoid trivial solutions.

Li (G) = e⊤

i3Gei4 ≥ 0

Algorithm 2 Semidefinite relaxation for HDGP
1: procedure SDR((cid:101)D, (cid:101)O, d)
2:

Input: Incomplete and noisy distance matrix, (cid:101)D, and ordi-

nal measurements, (cid:101)O, and embedding dimension, d.

3:

4:

Let W be the measurement mask.
For small ϵ1, ϵ2 > 0, solve for G:

minimize

w.r.t

subject to

Tr G
+

+ + Tr G−
, G− ⪰ 0

G
+ − G−,
G = G
diag G = −1,
e⊤
i Gej ≤ −1,
(cid:13)W ◦ (cid:0)cosh[(cid:101)D] + G(cid:1)(cid:13)
(cid:13)
(cid:13)
Lk (G) ≥ ϵ2,

2
(cid:13)
(cid:13)

F

return G.

5:
6: end procedure

≤ ϵ1,

∀i, j ∈ [N ]

∀k ∈ (cid:101)O.

Applications

Low-rank hyperbolic embedding [17–19, 25]

Ordinal outlier removal [42, 46, 57],
Robust hierarchical embedding [34, 40]

Anomaly detection in weighted graphs [2] and networks [58]

’Loid model: The unit hyperbolic norm appears as a simple

linear constraint

diag G = −1,
which guarantees that the embedded points reside in sheets Ld ∪
−Ld . Finally, e⊤
i Gej ≤ −1 enforces all embedded points to belong
to the same hyperbolic sheet, i.e. xn ∈ Ld for all n ∈ [N ].

This framework can serve as a bedrock for multitude of other
data fidelity objectives. We can seamlessly incorporate outlier re-
moval schemes by introducing slack variables into the objective
function and constraints [42, 46, 57]. For example, the modified
objective function

Tr G

+ + Tr G− + (cid:213)

ϵk

k

can be minimized subject to Lk (G) + ϵk ≥ 0 and ϵk ≥ 0 as a means
of removing outlier comparisons (we allow some comparisons to
be violated; see Section 4.3 for an example).

We can similarly implement outlier detection in metric embed-
ding problems. As an example, we can adapt the outlier pursuit
algorithm [56]. Consider the measured H -Gramian of a point set
with a few outliers

ˆG = G + C + N
where G is outlier-free hyperbolic Gramian, C is a matrix with only
few non-zero columns and N represents the measurement noise.
Outlier pursuit aims to minimize a convex surrogate for

rank G + λ ∥C ∥0,c
is the number of non-zero columns of C; more details

2
ˆG − G − C(cid:13)
s.t. (cid:13)
(cid:13)
(cid:13)
F

≤ ϵ

where ∥C ∥0,c
and options are given in Table 3.

We note that scalability of semidefinite programs has been stud-
ied in a number of recent works [35], for example based on sketch-
ing [59, 60].

3.2 Low-rank Approximation of H-Gramians
From Proposition 1, it is clear that the rank of a hyperbolic Gramian
of points in Ld is at most d + 1. However, the H-Gramian estimated
by the semidefinite relaxation in Algorithm 2 does not necessarily
have the correct rank. Therefore, we want to find its best rank-(d +1)
approximation, namely ˆG, such that
2
2
(cid:13)G − X ⊤HX (cid:13)
(cid:13)
(cid:13)G − ˆG(cid:13)
(cid:13)
F .
(cid:13)
(cid:13)
F

(11)

=

inf
X ∈(Ld )N

2In practice, we choose a diminishing sequence of δk .

In Algorithm Algorithm 3 we propose a simple but suboptimal
procedure to solve this low-rank approximation problem. Unlike
iterative refinement algorithms based on optimization on manifolds

[25], our proposed method is one-shot. It is based on the spectral
factorization of the the estimated hyperbolic Gramian and involves
the following steps:

• Step 1: We find a set of points {zn } in Rd +1, whose Lorentz
Gramian best approximates G; See Definition 3 and lines 2 to
5 of Algorithm 3. In other words, we relax the optimization
domain of (11) from Ld to Rd +1,

2

(cid:13)G − X ⊤HX (cid:13)
(cid:13)
(cid:13)

Z = arg min
X ∈R(d +1)×N
• Step 2: We project each point zn onto Ld , i.e.
ˆX = arg min
X ∈(Ld )N

∥X − Z ∥2
F .

.

This gives us an approximate rank-(d+1) hyperbolic Gramian,
ˆG = ˆX ⊤H ˆX ; see Figure 2 and Appendix C.

The first step of low-rank approximation of a hyperbolic Gramian
G can be interpreted as finding the positions of points in Rd +1 (not
necessarily on Ld ) whose Lorentz Gramian best approximates G.

3.3 Spectral Factorization of H-Gramians
To finally compute the point locations, we describe a spectral fac-
torization method, proposed in [55] (cf. footnote 1), , to estimate
point positions from their Lorentz Gramian (line 5 of Algorithm 3).
This method exploits the fact that Lorentz Gramians have only one
non-positive eigenvalue (see Lemma 1 in the appendix) as detailed
in the following proposition.

Proposition 2. Let G be a hyperbolic Gramian for X ∈

(cid:16)Ld (cid:17) N
,
with eigenvalue decomposition G = U ΛU ⊤, and eigenvalues λ0 ≤
3
0 ≤ λ1 ≤ . . . ≤ λd .
Then, there exists an H -unitary matrix R such
that X = R|Λ|1/2
U .

Algorithm 3 Low-rank approximation and spectral factorization
of hyperbolic Gramian

1: procedure Embed(G, d)
2:

Input: Hyperbolic Gramian G, and embedding dimension

Let U ⊤ΛU be eigenvalue decomposition of G, where Λ =

d.

3:

4:

5:

6:

7:

diag (λ0, · · · , λN −1) such that
• λ0 = mini λi ,
• λi is the top i-th element of {λi } for i ∈ [N ] − 1.

Let Gd +1 = U ⊤

, where

d ΛdUd
= diag (cid:0)λ0, u(λ1), · · · , u(λd )(cid:1),

Λd
u(x) = max {x, 0}, and Ud
value matrix.

Z = R|Λd |1/2
For Z = [z1, . . . , zN ], let

U ⊤
d

, for any H-unitary matrix R.

xn = Project(zn ), ∀n ∈ [N ]

return X = [x1, . . . , xN ] ∈

(cid:16)Ld (cid:17) N

.

be the corresponding sliced eigen-

8: end procedure

3An H-Gramian is a Lorentz Gramian.

Figure 2: Projecting a point in Rd +1 (blue) to Ld (red).

The proof is given in Appendix D. Note that regardless of the
choice of R, X = R|Λ|1/2
U will reproduce G and thus the corre-
sponding distances. This is the rigid motion ambiguity familiar
from the Euclidean case [9]. If we start with an H -Gramian with a
wrong rank, we need to follow the spectral factorization by Step 2
where we project each point zn ∈ Rd +1 onto Ld . This heuristic is
suboptimal, but it is nevertheless appealing since it only requires a
single one-shot calculation as detailed in Appendix C.

4 EXPERIMENTAL RESULTS
In this section we numerically demonstrate different properties
of Algorithm 1 in solving HDGPs. In a general hyperbolic embed-
ding problem, we have a mix of metric and non-metric distance
measurements which can be noisy and incomplete. Code, data and
documentation to reproduce the experimental results are available
at https://github.com/puoya/hyperbolic-distance-matrices.

4.1 Missing Measurements
Missing measurements are a common problem in hyperbolic embed-
dings of concept hierarchies. For example, hyperbolic embeddings
of words based on Hearst-like patterns rely on co-occurrence prob-
abilities of word pairs in a corpus such as WordNet [37]. These
patterns are sparse since word pairs must be detected in the right
configuration [31]. In perceptual embedding problems, we ask indi-
viduals to rate pairwise similarities for a set of objects. It may be
difficult to collect and embed all pairwise comparisons in applica-
tions with large number of objects [1].

The proposed semidefinite relaxation gives a simple way to han-
dle missing measurements. The metric sampling density 0 ≤ S ≤ 1
of a measured HDM is the ratio of the number of missing mea-
surements to total number of pairwise distances, S = 1 − | D |
. We
| Dc |
want to find the probability p(S) of successful estimation given a
sampling density S. In practice, we fix the embedding dimension,
d, and the number of points, N , and randomly generate a point
. A trial is successful if we can solve the HDGP for
set, X ∈
noise-free measurements and a random mask W of a fixed size so
that the estimated hyperbolic Gramian has a small relative error,
erel( ˆG) =
≤ δ . We repeat for M trials, and em-
pirically estimate the success probability as ˆp(S) = Ms
where Ms
M
is the number of successful trials. We repeat the experiment for
different values of N and d, see Figure 3.

(cid:13)D(X )−acosh[− ˆG](cid:13)
(cid:13)
(cid:13)F
∥ D(X ) ∥ F

(cid:16)Ld (cid:17) N

Figure 3: Left and middle: The probability of δ -accurate estimation for metric sampling density S, M = 100, and δ = 10−2. Right:
The empirical error erel = EK [erel(X )] for ordinal sampling density S, d = 2, M = 50, and K = 10. In each bar, shading width
represents the empirical standard deviation of erel(X ).

For non-metric embedding applications, we want to have con-
sistent embedding for missing ordinal measurements. The ordinal
sampling density 0 ≤ S ≤ 1 of a randomly selected set of or-
dinal measurements is defined as S = 1 − | O |
. For a point set
| Oc |

(cid:16)Ld (cid:17) N

∥DO −EM [DO ] ∥ F
∥EM [DO ] ∥ F

, we define the average relative error of estimated

X ∈
HDMs as erel(X ) = EM
where D O is the estimated
HDM for ordinal measurements O, and empirical expectation is
with respect to the random ordinal set O. We repeat the experiment
(Figure 3). We can observe
for K different realizations of X ∈
that across different embedding dimensions, the maximum allowed
fraction of missing measurements for a consistent and accurate
estimation increases with the number of points.

(cid:16)Ld (cid:17) N

4.2 Weighted Tree Embedding
Tree-like hierarchical data occurs commonly in natural scenarios. In
this section, we want to compare the embedding quality of weighted
trees in hyperbolic and the baseline in Euclidean space.

We generate a random tree T with N nodes, maximum degree
of ∆(T ) = 3, and i.i.d. edge weights from unif(0, 1)4. Let DT be the
distance matrix for T , where the distance between each two nodes
is defined as the weight of the path joining them.

For the hyperbolic embedding, we apply Algorithm 2 with log-
det heuristic objective function to acquire a low-rank embedding.
On the other hand, Euclidean embedding of T is the solution to the
following semidefinite relaxation
(cid:13)
(cid:13)D◦2
G ⪰ 0
G1 = 0

T − K(G)(cid:13)
(cid:13)

subject to

minimize

w.r.t

(12)

2
F

experiment, we define the optimal embedding dimension as

d0 = min

(cid:26)
d ∈ N :

∥DN −1 − Dd ∥F
∥DN −1 − Dd +1 ∥F

(cid:27)

≥ 1 − δ

where Dn is the distance matrix for embedded points in Ln (or Rn ),
and δ = 10−3. This way, we accurately represent the estimated
distance matrix in a low dimensional space. Finally, we define the
relative (or normalized) error of embedding T in d0-dimensional
space as erel(T ) =
. We repeat the experiment for M
randomly generated trees T with a varying number of vertices
N . The hyperbolic embedding yields smaller average relative er-
ror EM [erel(T )] compared to Euclidean embedding, see Figure 4. It
should also noted that the hyperbolic embedding has a lower opti-
mal embedding dimension, even though the low-rank hyperbolic
Gramian approximation is sub-optimal.

(cid:13)
(cid:13)DT −Dd0
∥DT ∥ F

(cid:13)
(cid:13)F

4.3 Odor Embedding
In this section, we want to compare hyperbolic and Euclidean non-
metric embeddings of olfactory data following the work of Zhou et
al. [61]. We conduct identical experiments in each space, and com-
pare embedding quality of points from Algorithm 2 in hyperbolic
space to its semidefinite relaxation counterpart in Euclidean space,
namely generalized non-metric MDS [1].

We use an olfactory dataset comprising mono-molecular odor
concentrations measured from blueberries [21]. In this dataset,
there are N = 52 odors across the total of M = 164 fruit samples.
Like Zhou et al. [61], we begin by computing correlations between

where K(G) = −2G + diag(G)1⊤ + 1diag(G)⊤, and D◦2
is the en-
T
trywise square of DT . This semidefinite relaxation (SDR) yields a
minimum error embedding of T , since the embedded points can
reside in an arbitrary dimensional Euclidean space.

The embedding methods based on semidefinite relaxation are
generally accompanied by a projection step to account for the poten-
tially incorrect embedding dimension. For hyperbolic embedding
problems, this step is summarized in Algorithm 3, whereas it is
simply a singular value thresholding of the Gramian for Euclidean
problems. Note that the SDRs always find a (N − 1)-dimensional
embedding for a set of N points; see Algorithm 2 and (12). In this

4The most likely maximum degree for trees with N ≤ 25 [39].

Figure 4: Tree embedding in hyperbolic (red) and Euclidean
(green) space. Discrete distribution of optimal embedding di-
mension for M = 100, (a) and (b). Average, EM [erel(T )], and
standard deviation of embedding error, (c) and (d).

Table 4: Reconstruction accuracy of ordinal measurements
γd for different levels of allowable violation ζp .

Space

Hyperbolic

Euclidean

d = 2
76.06
76.52
76.43
73.44
73.27
73.12

d = 4
83.60
83.71
83.71
78.86
79.03
78.92

d = 6
86.87
86.94
86.92
82.23
82.65
82.51

d = 8
89.48
89.68
89.76
85.06
86.24
86.01

d = 10
91.03
91.16
91.21
88.67
88.98
89.02

ζ0
ζ0.5
ζ1
ζ0
ζ0.5
ζ1

in Table 4 that hyperbolic space better represent the structure of
olfactory data compared to Euclidean space of the same dimen-
sion. This is despite the fact that the number of measurements per
variable is in favor of Euclidean embedding, and that the low rank
approximation of hyperbolic Gramians is suboptimal. Moreover,
if we remove a small number of outliers we can produce more
accurate embeddings. These results corroborate the statistical anal-
ysis of Zhou et. al. [61] that aims to identify the geometry of the
olfactory space. 6

5 CONCLUSION
We introduced hyperbolic distance matrices, an analogy to Eu-
clidean distance matrices, to encode pairwise distances in the ’Loid
model of hyperbolic geometry. Same as in the Euclidean case, al-
though the definition of hyperbolic distance matrices is trivial,
analyzing their properties gives rise to powerful algorithms based
on semidefinite programming. We proposed a semidefinite relax-
ation which is essentially plug-and-play: it easily handles a variety
of metric and non-metric constraints, outlier removal, and missing
information and can serve as a template for different applications.
Finally, we proposed a closed-form spectral factorization algorithm
to estimate the point position from hyperbolic Gramians. Several
important questions are still left open, most notably the role of
the isometries in the ’Loid model and the related concepts such as
Procrustes analysis.

ACKNOWLEDGEMENT
We thank Lav Varshney for bringing our attention to hyperbolic
geometry and for the numerous discussions about the manuscript.

REFERENCES
[1] Sameer Agarwal, Josh Wills, Lawrence Cayton, Gert Lanckriet, David Kriegman,
and Serge Belongie. 2007. Generalized non-metric multidimensional scaling. In
Artificial Intelligence and Statistics. 11–18.

[2] Leman Akoglu, Mary McGlohon, and Christos Faloutsos. 2010. Oddball: Spotting
anomalies in weighted graphs. In Pacific-Asia Conference on Knowledge Discovery
and Data Mining. Springer, 410–421.

[3] Abdo Y Alfakih, Amir Khandani, and Henry Wolkowicz. 1999. Solving Euclidean
distance matrix completion problems via semidefinite programming. Computa-
tional Optimization and Applications 12, 1-3 (1999), 13–30.

[4] Michael Ashburner, Catherine A Ball, Judith A Blake, David Botstein, Heather
Butler, J Michael Cherry, Allan P Davis, Kara Dolinski, Selina S Dwight, Janan T
Eppig, et al. 2000. Gene ontology: tool for the unification of biology. Nature
Genetics 25, 1 (2000), 25.

[5] Dena Asta and Cosma Rohilla Shalizi. 2014. Geometric network comparison.

[6] Riccardo Benedetti and Carlo Petronio. 2012. Lectures on hyperbolic geometry.

arXiv preprint arXiv:1411.1350 (2014).

Springer Science & Business Media.

6Statistical analysis of Betti curve behavior of underlying clique topology [22].

Figure 5: Embedding of odors for different levels of allow-
able violated measurements ζp . Clusters with the matching
colors contain the same odors.

odor concentrations across samples [61]. The correlation coefficient
between two odors xi and xj is defined as

C(i, j) =

(xi − µxi
(cid:13)
(cid:13)xi − µxi
(M )
)⊤, x
n

1)⊤(xj − µx j
(cid:13)
1(cid:13)
(cid:13)xj − µx j
(cid:13)
(m)
where xn = (x
is the concentration of i-th
i
odor in m-th fruit sample, M is total number of fruit samples and
µxn

= 1
M
The goal is to find an embedding for odors y1, . . . , yN ∈ Id (or

(1)
n , . . . , x

m=1 x

1)
1(cid:13)
(cid:13)

(m)
n

(cid:205)M

.

Rd ) such that

d(yi1 , yi2 ) ≤ d(yi3 , yi4 ), (i1, i2, i3, i4) ∈ O,

where,

(cid:26)

O ⊆ Oc =

(i1, i2, i3, i4) ∈

(cid:17)2

(cid:16)

[N ]2
as

: C(i1, i2) ≥ C(i3, i4)

(cid:27)

.

The total number of distinct comparisons grows rapidly with the
number of points, namely |Oc | = 0.87 million. In this experiment,
we choose a random set of size |O| = 2K (cid:0)N
(cid:1) for K = 4 to have the
2
sampling density of S = 98.79%5, which brings the size of ordinal
measurements to |O| ≈ 104.

We ensure the embedded points do not collapse by imposing
the following minimum distance constraint d(xi , xj ) ≥ 1 for all
(i, j) ∈ [N ]2
as; this corresponds to a simple linear constraint in
the proposed formulation. An ideal order embedding accurately
reconstructs the missing comparisons. We calculate the percentage
= | (cid:98)Oc,d ∩
of correctly reconstructed distance comparisons as γd
Oc |/|Oc |, where (cid:98)Oc,d
is the complete ordinal set corresponding to
a d-dimensional embedding.

A simple regularization technique helps to remove outlier mea-
surements and improve the generalized accuracy of embedding
algorithms. We introduce the parameter ζp to permit SDR algo-
rithms to dismiss at most p-percent of measurements, namely
Lk (G) + ϵk ≥ ϵ2 and ϵk ≥ 0, ∀k ∈ O and (cid:213)

ϵk ≤ ζp

k

where ζp = p

100 |O |ϵ2.

In Figure 5, we show the embedded points in I2 and R2 with
different levels of allowable violated measurements. We can observe

5In hyperbolic embedding, this is the ratio of number of ordinal measurements to

number of variables, i.e. K = |O|
2(N
2 )

.

[7] Marián Boguná, Fragkiskos Papadopoulos, and Dmitri Krioukov. 2010. Sustaining
the Internet with hyperbolic mapping. Nature Communications 1 (2010), 62.
[8] Carlo Vittorio Cannistraci, Gregorio Alanis-Lobato, and Timothy Ravasi. 2013.
From link-prediction in brain connectomes and protein interactomes to the local-
community-paradigm in complex networks. Scientific Reports 3 (2013), 1613.
[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. 1997.

Hyperbolic geometry. Flavors of Geometry 31 (1997), 59–115.

[10] Benjamin Paul Chamberlain, Stephen R Hardwick, David R Wardrope, Fabon
Dzogang, Fabio Daolio, and Saúl Vargas. 2019. Scalable hyperbolic recommender
systems. arXiv preprint arXiv:1902.08648 (2019).

[11] Kenny Chowdhary and Tamara G Kolda. 2018. An improved hyperbolic embed-

ding algorithm. Journal of Complex Networks 6, 3 (2018), 321–341.

[12] Andrej Cvetkovski and Mark Crovella. 2009. Hyperbolic embedding and routing
for dynamic graphs. In IEEE International Conference on Computer Communica-
tions. IEEE, 1647–1655.

[13] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. 2018. Repre-
sentation tradeoffs for hyperbolic embeddings. Proceedings of Machine Learning
Research 80 (2018), 4460.

[14] Bhuwan Dhingra, Christopher J Shallue, Mohammad Norouzi, Andrew M Dai,
and George E Dahl. 2018. Embedding text in hyperbolic spaces. arXiv preprint
arXiv:1806.04313 (2018).

[15] Steven Diamond and Stephen Boyd. 2016. CVXPY: A Python-embedded modeling
language for convex optimization. The Journal of Machine Learning Research 17,
1 (2016), 2909–2913.

[16] Ivan Dokmanić, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. 2015. Euclidean
distance matrices: Essential theory, algorithms, and applications. IEEE Signal
Processing Magazine 32, 6 (2015), 12–30.

[17] Maryam Fazel. 2002. Matrix rank minimization with applications. (2002).
[18] Maryam Fazel, Haitham Hindi, and Stephen P Boyd. 2003. Log-det heuristic for
matrix rank minimization with applications to Hankel and Euclidean distance
matrices. In Proceedings of the 2003 American Control Conference, 2003., Vol. 3.
IEEE, 2156–2162.

[19] Massimo Fornasier, Holger Rauhut, and Rachel Ward. 2011. Low-rank matrix
recovery via iteratively reweighted least squares minimization. SIAM Journal on
Optimization 21, 4 (2011), 1614–1640.

[20] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. 2018. Hyper-
bolic entailment cones for learning hierarchical embeddings. arXiv preprint
arXiv:1804.01882 (2018).

[21] Jessica L Gilbert, Matthew J Guthart, Salvador A Gezan, Melissa Pisaroglo de
Carvalho, Michael L Schwieterman, Thomas A Colquhoun, Linda M Bartoshuk,
Charles A Sims, David G Clark, and James W Olmstead. 2015. Identifying breed-
ing priorities for blueberry flavor using biochemical, sensory, and genotype by
environment analyses. PLoS One 10, 9 (2015), e0138494.

[22] Chad Giusti, Eva Pastalkova, Carina Curto, and Vladimir Itskov. 2015. Clique
topology reveals intrinsic geometric structure in neural correlations. Proceedings
of the National Academy of Sciences 112, 44 (2015), 13455–13460.

[23] Israel Gohberg, Peter Lancaster, and Leiba Rodman. 1983. Matrices and indefinite

scalar products. (1983).

[24] Roger A Horn and Charles R Johnson. 2012. Matrix analysis. Cambridge Univer-

sity Press.

[25] Pratik Jawanpuria, Mayank Meghwanshi, and Bamdev Mishra. 2019. Low-rank ap-
proximations of hyperbolic embeddings. arXiv preprint arXiv:1903.07307 (2019).
[26] Martin Keller-Ressel and Stephanie Nargang. 2020. Hydra: a method for strain-
minimizing hyperbolic embedding of network-and distance-based data. Journal
of Complex Networks 8, 1 (2020), cnaa002.

[27] Robert Kleinberg. 2007. Geographic routing using hyperbolic space. In 26th IEEE
International Conference on Computer Communications. IEEE, 1902–1909.
[28] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and
Marián Boguná. 2010. Hyperbolic geometry of complex networks. Physical
Review E 82, 3 (2010), 036106.

[29] Joseph B Kruskal and Myron Wish. 1978. Multidimensional scaling. Number 11.

Sage.

[30] John Lamping and Ramana Rao. 1994. Laying out and visualizing large trees
using a hyperbolic space. In Proceedings of the 7th annual ACM symposium on
User interface software and technology. ACM, 13–14.

[31] Matt Le, Stephen Roller, Laetitia Papaxanthos, Douwe Kiela, and Maximilian
Nickel. 2019. Inferring concept hierarchies from text corpora via hyperbolic
embeddings. arXiv preprint arXiv:1902.00913 (2019).

[32] Leo Liberti, Carlile Lavor, Nelson Maculan, and Antonio Mucherino. 2014. Eu-
clidean distance geometry and applications. SIAM Rev. 56, 1 (2014), 3–69.
[33] Nathan Linial, Eran London, and Yuri Rabinovich. 1995. The geometry of graphs
and some of its algorithmic applications. Combinatorica 15, 2 (1995), 215–245.
[34] Ke Ma, Qianqian Xu, and Xiaochun Cao. 2019. Robust ordinal embedding from
contaminated relative comparisons. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 33. 7908–7915.

[35] Anirudha Majumdar, Georgina Hall, and Amir Ali Ahmadi. 2019. Recent scala-
bility improvements for semidefinite programming with applications in machine

Learning, control, and robotics. Annual Review of Control, Robotics, and Au-
tonomous Systems 3 (2019).

[36] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems. 3111–3119.

[37] George A Miller. 1998. WordNet: An electronic lexical database. MIT press.
[38] Bamdev Mishra, Gilles Meyer, Francis Bach, and Rodolphe Sepulchre. 2013. Low-
rank optimization with trace norm penalty. SIAM Journal on Optimization 23, 4
(2013), 2124–2149.

[39] John W Moon et al. 1968. On the maximum degree in a random tree. The Michigan

Mathematical Journal 15, 4 (1968), 429–432.

[40] Maximillian Nickel and Douwe Kiela. 2017. Poincaré embeddings for learning
hierarchical representations. In Advances in neural information processing systems.
6338–6347.

[41] Maximilian Nickel and Douwe Kiela. 2018. Learning continuous hierarchies in
the lorentz model of hyperbolic geometry. arXiv preprint arXiv:1806.03417 (2018).
[42] Carl Olsson, Anders Eriksson, and Richard Hartley. 2010. Outlier removal using
duality. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. IEEE, 1450–1457.

[43] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
Empirical Methods in Natural Language Processing (EMNLP). 1532–1543.
[44] Stephen Roller, Douwe Kiela, and Maximilian Nickel. 2018. Hearst patterns
revisited: Automatic hypernym detection from large text corpora. arXiv preprint
arXiv:1806.03191 (2018).

[45] Rik Sarkar. 2011. Low distortion delaunay embedding of trees in hyperbolic
plane. In International Symposium on Graph Drawing. Springer, 355–366.
[46] Yongduek Seo, Hyunjung Lee, and Sang Wook Lee. 2009. Outlier removal by
convex optimization for l-infinity approaches. In Pacific-Rim Symposium on Image
and Video Technology. Springer, 203–214.

[47] Yuval Shavitt and Tomer Tankel. 2008. Hyperbolic embedding of internet graph
for distance estimation and overlay construction. IEEE/ACM Transactions on
Networking 16, 1 (2008), 25–36.

[48] Puoya Tabaghi, Ivan Dokmanić, and Martin Vetterli. 2019. Kinetic Euclidean
distance matrices. IEEE Transactions on Signal Processing 68 (2019), 452–465.
[49] Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and Adam Tauman Kalai.
2011. Adaptively learning the crowd kernel. arXiv preprint arXiv:1105.1033
(2011).

[50] Laurens Van Der Maaten and Kilian Weinberger. 2012. Stochastic triplet em-
bedding. In 2012 IEEE International Workshop on Machine Learning for Signal
Processing. IEEE, 1–6.

[51] Lieven Vandenberghe and Stephen Boyd. 1996. Semidefinite programming. SIAM

Rev. 38, 1 (1996), 49–95.

[52] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Order-
embeddings of images and language. arXiv preprint arXiv:1511.06361 (2015).
[53] Kevin Verbeek and Subhash Suri. 2014. Metric embedding, hyperbolic space, and
social networks. In Proceedings of the thirtieth annual symposium on Computa-
tional Geometry. ACM, 501.

[54] Tran Dang Quang Vinh, Yi Tay, Shuai Zhang, Gao Cong, and Xiao-Li Li. 2018.
Hyperbolic recommender systems. arXiv preprint arXiv:1809.01703 (2018).
[55] Richard C Wilson, Edwin R Hancock, Elżbieta Pekalska, and Robert PW Duin.
2014. Spherical and hyperbolic embeddings of data. IEEE Transactions on Pattern
Analysis and Machine Intelligence 36, 11 (2014), 2255–2269.

[56] Huan Xu, Constantine Caramanis, and Sujay Sanghavi. 2010. Robust PCA via
outlier pursuit. In Advances in Neural Information Processing Systems. 2496–2504.
[57] Jin Yu, Anders Eriksson, Tat-Jun Chin, and David Suter. 2014. An adversarial
optimization approach to efficient outlier removal. Journal of Mathematical
Imaging and Vision 48, 3 (2014), 451–466.

[58] Wenchao Yu, Wei Cheng, Charu C Aggarwal, Kai Zhang, Haifeng Chen, and Wei
Wang. 2018. Netwalk: A flexible deep embedding approach for anomaly detection
in dynamic networks. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2672–2681.

[59] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher.
2019. Scalable semidefinite programming. arXiv preprint arXiv:1912.02949 (2019).
[60] Alp Yurtsever, Madeleine Udell, Joel A Tropp, and Volkan Cevher. 2017. Sketchy
decisions: Convex low-rank matrix optimization with optimal storage. arXiv
preprint arXiv:1702.06838 (2017).

[61] Yuansheng Zhou, Brian H Smith, and Tatyana O Sharpee. 2018. Hyperbolic
geometry of the olfactory space. Science Advances 4, 8 (2018), eaaq1458.

A PROOF OF PROPOSITION 1
A hyperbolic Gramian can be written as G = X ⊤HX for a X =
. Let us rewrite it as
[x1, . . . , xN ] ∈

(cid:16)Ld (cid:17) N

d
(cid:213)

G =

дiд⊤

i − д0д⊤
0

i=1
+ − G−

= G

is the (i + 1)-th row of X , G− = д0д⊤

i=1 дiд⊤
where д⊤
i
i
are positive semidefinite matrices. We have rank G− ≤ 1 and
rank G

0 and G

+ = (cid:205)d

+ ≤ d. On the other hand, we have
def= [xi , xj ]

e⊤
i Gej

= −x0,i x0, j +

d
(cid:213)

k =1

xk,i xk, j

(cid:13)
(cid:13)

(cid:113)

¯xj

(cid:13) ¯xj

1 + (cid:13)

(a)
= −

2 + ¯x ⊤
i

(b)
≤ −(1 + ¯x ⊤
i

1 + ∥ ¯xi ∥2(cid:113)
¯xj ) + ¯x ⊤
i
is the (k + 1)-th element of xi , ¯xi = (x1,i , . . . , xd,i )⊤,
where xk,i
and (a) is due to ∥xi ∥2
= −1, and (b) results from Cauchy-
H
Shwartz inequality. The equality holds for i = j, which yields the
diag G = −1 condition.

¯xj = −1.

2
(cid:13)
(cid:13)
H

= (cid:13)

(cid:13)xj

Conversely, let G = G

+ − G−, where G
+ ≤ d. Let us write G− = д0д⊤

and rank G
д0, . . . , дd ∈ RN . Then, we define

+, G− ⪰ 0, rank G− ≤ 1,
for

+ = (cid:205)d

i=1 дiд⊤
i

0 and G

def=

X

= [x1, · · · , xN ] ∈ R(d +1)×N .

д⊤
0
...
д⊤
d

















where xn ∈ Rd +1 for all n ∈ [N ]. By construction, we have
X ⊤HX = G, and

diag G = −1 ⇒ ∥xn ∥2
H

= −1, ∀n ∈ [N ].

Finally, e⊤
i Gej ≤ −1 guarantees that xn ∈ Ld for all n ∈ [N ]. We
prove the contrapositive statement. Let xi and xj belong to different
the hyperbolic sheets, e.g. xi ∈ Ld , xj ∈ −Ld . Then,

e⊤
i Gej

def= [xi , xj ]

= −x0,i x0, j +

d
(cid:213)

k=1

xk,i xk, j

(cid:113)

(a)
≥

1 + ∥ ¯xi ∥2(cid:113)

1 + (cid:13)

(cid:13) ¯xj

2

(cid:13)
(cid:13)

− ∥ ¯xi ∥ (cid:13)
(cid:13) ¯xj

(cid:13)
(cid:13) ≥ 0

where (a) is due to Cauchy-Shwartz inequality. This is in contra-
diction with e⊤
i Gej ≤ −1 condition. Therefore, {xn } belong to the
same hyperbolic sheet, namely Ld .

B DERIVATIONS FOR ALGORITHM 3

Theorem 1. Let G ∈ RN ×N be a hyperbolic Gramian, with eigen-

value decomposition

G = U ⊤ΛU ,
where Λ = diag (λ0, · · · , λN −1) such that

(13)

• λ0 = mini λi ,
• λi is the i-th top element of {λi } for i ∈ {1, · · · , d}

The best rank-(d + 1) Lorentz Gramian approximation of G, in ℓ2
sense, is given by

d ΛdUd
= diag [λ0, u(λ1), · · · , u(λd )], u(x) = max {x, 0}, and

where Λd
Ud ∈ R(d +1)×N is the corresponding sliced eigenvalue matrix.

Gd +1 = U ⊤

Proof. We begin by characterizing the eigenvalues of a Lorentz

Gramian.

Lemma 1. Let G ∈ RN ×N be a Lorentz Gramian of rank d + 1
with eigenvalues ψ0 ≤ · · · ≤ ψd . Then, ψ0 < 0, and ψi > 0, for
i ∈ {1, · · · , d }.

Proof. We write Lorentzian Gramian, G = (дi, j ), as G = X ⊤HX

where

X = [x1, · · · , xN ]

∈ R(d +1)×N .

def=

д⊤

0

...



д⊤

d

+ def= (cid:205)d








i=1 дiд⊤
i

+ − G− where G

Then, G = G
is a positive semi-
,
definite matrix of rank d and with eigenvalues 0 < γ1 ≤ · · · ≤ γd
and −G− def= −д0д⊤
0 is a negative definite matrix of rank 1, with
eigenvalue µ ≤ 0. From Weyl’s inequality [24], we have

µ + γ1 ≤ ψ0 ≤ µ + γd
where ψ0 is the smallest eigenvalue of G. Therefore, ψ0 can be non-
positive (negative if µ + γd < 0). For other eigenvalues of G, we
have

0 + γ1 ≤ ψi ≤ γd , for 1 ≤ i ≤ d.
Hence, ψi > 0 for i ∈ {1, · · · , d}. This is result is irrespective to the
order of eigenvalues.

Now, let us proveψ0 < 0. Suppose д0 ∈ S = span {дi : i ∈ {1, · · · , d}}.

Then,

д⊤
0
...
д⊤
d









< d + 1,

rank G = rank








which is a contradiction. Therefore, we write д0 = αt + βs where
s ∈ S, t ∈ S⊥ with ∥t ∥ = 1, α, β ∈ R and α (cid:44) 0. Then, we have
ψ0 ≤ t ⊤Gt
(a)
= −t ⊤д0д⊤
0 t
2
= −α
< 0
0 + (cid:205)d
i=1 дiд⊤
i

where (a) is due to G = −д0д⊤

and t ∈ S⊥.

□

Consider eigenvalue decomposition of G in eq. (13). Without

loss of generality, we assume
• λ0 = mini λi < 0,
• λi is the i-th top element of {λi } for i ∈ {1, · · · , d}.

By construction G = X ⊤HX and from diag G = −1 condition, we
have

(cid:213)

λi = −N .

Therefore, λ0 < 0. From Lemma 1, one eigenvalue of a Lorentz
Gramian is negative and the rest must be positive. Therefore, ˆG =

= diag {λ0, u(λ1), · · · , u(λd )} and
= [u0, · · · , ud ], is the best rank-(d + 1) Lorentz

with eigenvalues Λd

U ⊤
d ΛdUd
eigenvectors Ud
Gramian approximation to G, i.e.
2
(cid:13)
ˆG − G(cid:13)
2 =
(cid:13)
(cid:13)

inf
H : Lorentz Gram. of rank ≤d +1

∥H − G ∥2
2 .

□

Finally, a rank-(d + 1) Lorentz Gramian with eigenvalue decom-

position

can be decomposed as X = R|Λ|1/2
arbitrary H-unitary matrix and Gd +1 = X ⊤HX .

U ⊤
d

Gd +1 = Ud ΛdU ⊤
d
∈ R(d +1)×N where R is an

C Project : Rd → Ld

ˆx =

x(λ)






Algorithm 4 Projection from Rd +1 to Ld
1: procedure Project(x)
2:

For x ∈ Rd +1, let
x ∈ (cid:8)(x0, 0⊤)⊤ : x0 ≤ 2(cid:9) ,
(1, 0⊤)⊤
( 1
2 x0, ˆx1, · · · , ˆxd )⊤ x ∈ (cid:8)(x0, 0⊤)⊤ : x0 > 2(cid:9)

and for a ( ˆx1, · · · , ˆxd ) ∈ S,
otherwise and for λ :
∥x(λ)∥2
H

= −1.

x and

where x(λ) = (I + λH )−1

(cid:26)

S =

(x1, · · · , xd ) : x

2
1 + · · · + x

2
d

= −1 +

(cid:27)

1
4 x

2
0

.

return ˆx.

3:
4: end procedure

Proof. Let us reformulate the following projection problem

ˆx ∈ arg min

∥y − x ∥2

(14)

y ∈Ld
as unconstrained augmented Lagrangian minimization, i.e.

L(y, λ) = ∥y − x ∥2 + λ(y⊤Hy + 1).
The first order necessary condition for ˆx to be a (local) minimum
of eq. (14) is

(I + λ∗H ) ˆx = x

(15)

for a λ∗ ∈ R such that ˆx ∈ Ld .

λ∗ = −1: This happens when x = (x0, 0⊤)⊤ and x0 ≥ 2. Follow-
= −1, we have

ing from optimality condition of eq. (15) and ∥ ˆx ∥2
H
ˆx = ( 1

2 x0, ˆx1, · · · , ˆxd )⊤, where

1
4 x

ˆx

2
2
1 + · · · + ˆx
d
Therefore, ˆx could be any point on a (d − 1)-dimensional sphere on
Ld . For x = (x0, 0⊤)⊤ and x0 ≤ 2, we have ˆx = (1, 0⊤)⊤.

= −1 +

2
0 .

λ∗ = 1: This happens for x = (0, x1, · · · , xd )⊤. From optimality
1
2 xd ), where ˆx0 =

1
2 x1, · · · ,

condition of eq. (15), we have ˆx = ( ˆx0,
1
+ 4.
2

1 + · · · + x 2
x 2
d

(cid:113)

For non-degenerate cases of λ∗ (cid:44) ±1, we have

where λ∗ ∈

(cid:110)

λ : (cid:13)

(cid:13)(I + λH )−1

(1) λ∗ ∈ (−1, 1): First, we define

ˆx = (I + λ∗H )−1
2
x(cid:13)
(cid:13)
H

x,
= −1, ˆx0 ≥ 0(cid:111).

(16)

(cid:13)(I + λH )−1

f (λ) = (cid:13)
λ→1− f (λ) = −∞,
This is a monotonous function on (−1, 1), with lim
λ→−1+ f (λ) = +∞. Hence, f (λ) = −1 has a unique solution
and lim
λ∗ ∈ (−1, 1). Finally, ˆx is a local minima since the second order
sufficient condition

2
x(cid:13)
H .
(cid:13)

I + λ∗H ≻ 0
is satisfied for λ∗ ∈ (−1, 1). Lastly, from eq. (16), we have ˆx0x0 ≥ 0.
In other words, λ∗ ∈ [−1, 1] if and only if x is in the same half-space
as Ld , i.e. x0 ≥ 0.

(2) λ∗ ∈ (−∞, −1): Similarly, f (λ) is a continuous function in
λ→−∞ f (λ) = 0, and
λ→−1− f (λ) = +∞, lim

this interval with lim
its first order derivative

d
dλ

f (λ) = −

2
(1 − λ)3 x

2
0 −

2
(1 + λ)3

d
(cid:213)

i=1

2
i

x

has at most one zero. Therefore, f (λ) = −1 has at most two solu-
tions. The second order necessary condition for local minima is
v⊤(I + λ∗H )v ≥ 0 for all v ∈ T ˆx Ld , where

T ˆx Ld = (cid:110)

v ∈ Rd +1 : x ⊤(I + λ∗H )−1

Hv = 0(cid:111)

.

However, there exists a v ∈ T ˆx Ld where v = (0, ¯v⊤)⊤ which
violates the second order necessary condition, v⊤(I + λ∗H )v < 0.
Therefore, ˆx – even if it exists – is not a local minima.

(3) λ∗ ∈ (1, ∞): We can easily see that lim
λ→+∞ f (λ) = 0, and d

λ→1+ f (λ) = −∞,
d λ f (λ) = 0 has at most one solution in this
lim
interval. Therefore, f (λ) = −1 has exactly one solution. However,
we have ˆx0x0 ≤ 0 from eq. (16). In other words, λ∗ ∈ (1, ∞) if and
only if x is in the opposite half-space of Ld , i.e. x0 ≤ 0. Finally, ˆx is
the unique minima, since the projection of x (cid:60) S to the closed and
convex set of

S = (cid:8)x : x0 ≥ 0, ∥x ∥2

H ≤ −1(cid:9)

always exits and is unique.

□

D PROOF OUTLINE OF PROPOSITION 2
Let X = R|Λ|1/2

U ⊤. Then,
X ⊤HX = U |Λ|1/2
(a)
= U |Λ|1/2
(b)
= G

U ⊤

R⊤HR|Λ|1/2
H |Λ|1/2

U ⊤

where (a) is due to properties of H-unitary matrices, (b) from
|λ0|1/2(−1)|λ0|1/2 = λ0 for λ0 ≤ 0. Therefore X = R|Λ|1/2
U ⊤
is a hyperbolic spectral factor of G. Finally, the uniqueness of these
factors is due to fact that H -unitary operators fully characterize
isometries in the ’Loid model [9].

