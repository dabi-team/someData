2
2
0
2

b
e
F
5

]

C
O
.
h
t
a
m

[

3
v
2
4
8
1
0
.
1
1
1
2
:
v
i
X
r
a

Coordinate Linear Variance Reduction
for Generalized Linear Programming *

Chaobing Song, Cheuk Yin Lin, Stephen J. Wright, Jelena Diakonikolas
Computer Sciences, University of Wisconsin-Madison

February 8, 2022

Abstract

We study a class of generalized linear programs (GLP) in a large-scale setting, which includes simple,
possibly nonsmooth convex regularizer and simple convex set constraints. By reformulating (GLP) as an
equivalent convex-concave min-max problem, we show that the linear structure in the problem can be used to
design an efﬁcient, scalable ﬁrst-order algorithm, to which we give the name Coordinate Linear Variance
Reduction (CLVR; pronounced “clever”). CLVR yields improved complexity results for (GLP) that depend
on the max row norm of the linear constraint matrix in (GLP) rather than the spectral norm. When the
regularization terms and constraints are separable, CLVR admits an efﬁcient lazy update strategy that makes
its complexity bounds scale with the number of nonzero elements of the linear constraint matrix in (GLP)
rather than the matrix dimensions. On the other hand, for the special case of linear programs, by exploiting
sharpness, we propose a restart scheme for CLVR to obtain empirical linear convergence. Then we show that
Distributionally Robust Optimization (DRO) problems with ambiguity sets based on both f -divergence and
Wasserstein metrics can be reformulated as (GLPs) by introducing sparsely connected auxiliary variables.
We complement our theoretical guarantees with numerical experiments that verify our algorithm’s practical
effectiveness, in terms of wall-clock time and number of data passes.

1 Introduction

We study the following generalized linear program (GLP):

(cid:8)cT x + r(x) : Ax = b, x ∈ X (cid:9),

min
x

(GLP)

where x, c ∈ Rd, A ∈ Rn×d, b ∈ Rn, r : Rd → R is a convex regularizer that admits an efﬁciently computable
proximal operator, and X ⊆ Rd is a closed convex set with efﬁciently computable projection operator. When
X is the nonnegative orthant {x : xi ≥ 0, i ∈ [d]} and r ≡ 0, (GLP) reduces to the standard form of a linear
program (LP). When X is a convex cone and r ≡ 0, (GLP) reduces to a conic linear program. (GLP) has had a
signiﬁcant impact in traditional engineering disciplines such as transportation, energy, telecommunications, and
manufacturing. In modern data science, we observe the renaissance of (GLP) due to its modeling power in such
areas as reinforcement learning [De Farias and Van Roy, 2003], optimal transport [Villani, 2009], and neural
network veriﬁcation [Liu et al., 2020]. For traditional engineering disciplines with moderate scale or exploitable
sparsity, off-the-shelf interior point methods that form and factorize matrices in each iteration are often good
choices as practical solvers [Gurobi, 2020]. In data science applications, however, where the data are often dense

*CS (chaobing.song@wisc.edu) acknowledges support from the NSF award 2023239.

JD (jelena@cs.wisc.edu) and CYL
(cylin@cs.wisc.edu) acknowledge support from the NSF award 2007757. SJW (swright@cs.wisc.edu) acknowledges support from NSF
awards 1934612 and 2023239, Subcontract 8F-30039 from Argonne National Laboratory, and an AFOSR subcontract UTA20-001224
from UT-Austin. Part of this work was done while JD, CS, and SW were visiting the Simons Institute for the Theory of Computing.

1

 
 
 
 
 
 
or of extreme scale, the amount of computation and/or memory required by matrix factorization is prohibitive.
Thus, ﬁrst-order methods that avoid matrix factorizations are potentially appealing options. In this context,
because the presence of the linear equality constraint in (GLP) may complicate projection operations onto the
feasible set, we consider the following equivalent reformulation of (GLP) as a min-max problem involving the
Lagrangian:

min
x∈X ⊂Rd

max
y∈Rn

(cid:110)

L(x, y) = cT x + r(x) + yT Ax − yT b

(cid:111)
.

(PD-GLP)

In data science applications, both n and d can be very large. (PD-GLP) can be viewed as a structured
bilinearly coupled min-max problem, where the linearity of L(x, y) in the dual variable vector y is vital to our
algorithmic development.

1.1 Background

While the literature addressing (PD-GLP) is sparse — some special cases have been considered in Mangasarian
and Meyer [1979], Mangasarian [1984, 2004] — there has been signiﬁcant recent work on ﬁrst-order methods
for general bilinearly coupled convex-concave min-max problems. Deterministic ﬁrst-order methods include
the proximal point method (PPM) of Rockafellar [1976], the extragradient/mirror-prox method (EGM) of Kor-
pelevich [1976] and Nemirovski [2004], the primal-dual hybrid gradient (PDHG) method of Chambolle and
Pock [2011], and the alternating direction method of multipliers (ADMM) of Douglas and Rachford [1956]. All
these methods have per-iteration cost Θ(nnz(A) + n + d) and convergence rate 1/k, where nnz(A) denotes the
number of nonzero elements of A and k is the number of iterations.

For better scalability, stochastic counterparts of these methods have been proposed. Juditsky et al. [2011],
Ouyang et al. [2013], Bianchi [2016], Patrascu and Necoara [2017] have used “vanilla” stochastic gradients to
replace the full gradients of their deterministic counterparts. Carmon et al. [2019], Hamedani and Jalilzadeh
[2020], Alacaoglu and Malitsky [2021] have exploited the ﬁnite-sum structure of the interaction term (cid:104)y, Ax(cid:105)
involving both primal and dual variables to perform variance reduction. With a separability assumption for
the dual variables, Alacaoglu et al. [2017] and Chambolle et al. [2018] have combined incremental coordinate
approaches on the dual variables with an implicit variance reduction strategy on the primal variables. Recently,
under a separability assumption for dual variables, Song et al. [2021] have proposed a new incremental coordinate
method with an initialization step that requires a single access to the full data. This approach, known as variance
reduction via primal-dual accelerated dual averaging (VRPDA2), obtains the ﬁrst theoretical bounds that are better
than their deterministic counterparts in the class of incremental coordinate approaches. The VRPDA2 algorithm
serves as the main motivation for our approach.

It is of particular interest to design algorithms that scale with the number of nonzero elements in A for at
least two reasons: (i) the data matrix can be sparse; and (ii) when we consider simpliﬁed reformulations of
certain complicated models, we often need to introduce sparsely connected auxiliary variables. Nevertheless, the
randomized coordinate algorithms of Alacaoglu et al. [2017], Chambolle et al. [2018], Song et al. [2021] have
O(d) per-iteration cost regardless of the sparsity of A. To address this issue, Fercoq and Bianchi [2019], Latafat
et al. [2019] have proposed incremental primal-dual coordinate methods with per-iteration cost which scales
with the number of nonzero elements in the row from A that is used in each iteration, at the price of smaller step
size for dense A. Moreover, Alacaoglu et al. [2020] has proposed a random extrapolation approach that admits
both low per-iteration cost and large step size. Despite these developments, all these algorithms produce less
accurate iterates than the methods with O(d) per-iteration cost, thus degrading their worst-case complexity.

Finally, for the special case of LP, based on the positive Hoffman constant [Hoffman, 2003], Applegate
et al. [2021b] proved that the primal-dual formulation of LP exhibits a sharpness property, which bounds below
the growth of a normalized primal-dual gap introduced in the same work. Based on this sharpness property,
Applegate et al. [2021b] proposed a restart scheme for the deterministic ﬁrst-order methods discussed above to

2

obtain linear convergence. Applegate et al. [2021a] extended this restart strategy further using various heuristics
to improve practical performance.

1.2 Motivations

We sharpen the focus from general bilinearly coupled convex-concave min-max problems to (GLP) and its
primal-dual formulation (PD-GLP), because many complicated models can be reformulated as (GLP) and (GLP)
possesses additional structure that can be exploited in algorithm design. Our motivation for focusing on (GLP) is
to bridge the large gap between the well-studied stochastic variance reduced ﬁrst-order methods [Johnson and
Zhang, 2013, Allen-Zhu, 2017, Song et al., 2020, 2021] and the increasingly popular and complicated, yet highly
structured large-scale problems arising in distributionally robust optimization (DRO) [Wiesemann et al., 2014,
Shaﬁeezadeh Abadeh et al., 2015, Namkoong and Duchi, 2016, Esfahani and Kuhn, 2018, Hu et al., 2018, Duchi
and Namkoong, 2018, Li et al., 2019, Duchi et al., 2021, Yu et al., 2021]; see also a recent survey by Rahimian
and Mehrotra [2019] and references therein.

For DRO problems with ambiguity sets deﬁned by f -divergence [Namkoong and Duchi, 2016, Hu et al.,
2018, Levy et al., 2020], the original formulation is a nonbilinearly coupled convex-concave min-max problem.
Even the well constructed reformulation in Levy et al. [2020] does not admit unbiased stochastic gradients,
leading to complicated algorithms and analysis. For DRO problems with ambiguity sets deﬁned by Wasserstein
metric [Shaﬁeezadeh Abadeh et al., 2015, Esfahani and Kuhn, 2018, Li et al., 2019, Yu et al., 2021, Ho-Nguyen
and Wright, 2021], the original formulation is in general inﬁnite-dimensional. (Finite-dimensional reformulations
[Shaﬁeezadeh Abadeh et al., 2015, Esfahani and Kuhn, 2018] exist for special cases of logistic regression and
smooth convex losses.) Solvers that have been proposed for DRO with Wasserstein metric are either multiple-loop
deterministic ADMM [Li et al., 2019] or are designed for general convex-concave problems [Yu et al., 2021].
By introducing auxiliary variables with sparse connections, we show that DRO with ambiguity sets based on
both f -divergence and the Wasserstein metric can be reformulated as (GLP). Thus, complicated DRO problems
can be addressed by a simple, efﬁcient, and scalable algorithm for (GLP). Our algorithm for solving (GLP) and
the proposed reformulations of DRO are our main contributions.

1.3 Contributions

Algorithm. Motivated by VRPDA2 [Song et al., 2021], we propose a simple, efﬁcient, and scalable algorithm
for (PD-GLP). Our algorithm combines an incremental coordinate method with exploitation of the linear
structure for the dual variables in (PD-GLP) and the implicit variance reduction effect in the algorithm, so
we name it coordinate linear variance reduction (CLVR, pronounced “clever”). CLVR is a simpliﬁed variant
of VRPDA2 which is customized to the particular structure of (PD-GLP). By exploiting the fact that the max
problem is linear and unconstrained in the dual variable vector y ∈ Rn, we ﬁnd that the expensive initialization
step used in VRPDA2 is not needed and we can take simpler and larger steps. Meanwhile, in the structured
case in which A is sparse and the convex constraint set X and the regularizer r(x) are fully separable1, we
show that the dual averaging update in CLVR enables us to design an efﬁcient lazy update strategy for which
the per-iteration cost of CLVR scales with the number of nonzero elements of the selected row from A in each
iteration. Further, CLVR uses extrapolation on dual variables rather than on primal variables considered in
VRPDA2, which signiﬁcantly reduces implementation complexity of our lazy update strategy for structured
variants of (PD-GLP). To state our complexity results, we make Assumption 1.

Assumption 1. L := (cid:107)A(cid:107) and each row of A in (GLP) is normalized with Euclidean norm R.

As a comment, it is known that preprocessing in modern LP solvers [Gurobi, 2020] often ensures normalized
√
nR. The upper bound is achieved when all elements

rows/columns for data matrix. Observe that R ≤ L ≤

1We state the results here for the fully separable setting for convenience of comparison; however, our results are also applicable to the

block separable setting.

3

Table 1: Overall complexity and per-iteration cost for solving structured (PD-GLP). (“—”
indicates that the corresponding result does not exist or is unknown.)

Algorithm

PDHG
CP(2011)
SPDHG
CERS(2018)
EVR
AM (2021)
VRPDA2
SWD(2021)
CLVR
(This Paper)

General Convex
(Primal-Dual Gap)
O(cid:0) (nnz(A)+n+d)L
(cid:1)

(cid:15)

Strongly Convex
(Distance to Solution)
O(cid:0) (nnz(A)+n+d)L

√

(cid:1)

σ

(cid:15)

(cid:1)

O(cid:0) ndL

√

(cid:1)

σ

(cid:15)

O(cid:0) ndL
√

(cid:15)

O(cid:0)nnz(A) +

nnz(A)(n+d)nR

(cid:1)

—

(cid:15)

O(nd log min{ 1

(cid:15) , n} + ndR

(cid:15) ) O(nd log min{ 1

(cid:15) , n} + ndR
(cid:15) )

√

σ

Per-Iteration
Cost

O(nd)

O(d)

O(n + d)

O(d)

O( nnz(A)R

(cid:15)

)

O( nnz(A)R
σ

√

(cid:15)

)

O(nnz(row(A)))

of A have identical value. Although this is an extreme case, there exist ill-conditioned practical datasets
where we can expect signiﬁcant performance gains if the complexity can be reduced from O(L) to O(R). In
Table 1, we give the overall complexity bounds (total number of arithmetic operations) and the per-iteration
cost of a representative set of existing algorithms and our CLVR algorithm for solving structured (PD-GLP), i.e.,
X = X1 × · · · × Xd with Xi ∈ R (i ∈ [d]) and r(x) := (cid:80)d
i=1 r(xi). We further assume that for the stochastic
algorithms [Chambolle et al., 2018, Alacaoglu and Malitsky, 2021, Song et al., 2021] and our CLVR algorithm,
we draw one row of A per iteration uniformly at random to make the complexity results comparable. The general
convex setting corresponds to r(x) being general convex (σ = 0), while the strongly convex setting corresponds
to r(x) being σ-strongly convex (σ > 0).

As shown in Table 1, all the algorithms have the optimal dependence on (cid:15) [Ouyang and Xu, 2019], while
the dependences on the ambient dimensions n, d, the number of nonzero elements of A, nnz(A), and the
constants L and R are quite different. For both the general convex and strongly convex settings, CLVR is the
ﬁrst algorithm that has no dependence on the ambient dimensions d and n, instead depending on the number of
nonzero elements nnz(A). Moreover, the complexity of CLVR depends on the max row norm R rather than the
spectral norm L, and the per-iteration cost of CLVR only depends on the nonzero elements of the selected row
from A in each iteration, which can be far less than d.

By exploiting the linear structure again, we provide explicit guarantees for both the objective value and the
constraint satisfaction of (GLP). Further, the analysis of CLVR applies to the more general block-coordinate
update setting, which is better suited to modern parallel computing platforms.

Finally, following the restart strategy based on the normalized duality gap for LP introduced in Applegate
et al. [2021b], we propose a more straightforward strategy to restart our CLVR algorithm (as well as other iterative
algorithms for (PD-GLP)): Restart the algorithm every time the widely known LP metric of Andersen and
Andersen [2000] halves. Compared with the normalized duality gap, the LP metric can be computed more
efﬁciently and in a more straightforward fashion.

DRO Reformulations. When the loss function is convex, DRO problems with ambiguity sets based on f -
divergence [Namkoong and Duchi, 2016] or Wasserstein metric [Esfahani and Kuhn, 2018] are convex. However,
as both original formulations either have complicated constraints or are inﬁnite-dimensional, vanilla ﬁrst-order
methods are inapplicable.

For DRO with f -divergence, we show that by using convex conjugates and introducing auxiliary variables,
the problem can be reformulated in the form of (GLP). As a result, we do not have the issue of biased stochastic
gradients encountered in Levy et al. [2020] and CLVR can be applied. Even though the resulting problem has
larger dimensions, due to the sparseness of the introduced auxiliary variables and the lazy update strategy of
CLVR, it can be solved with complexity scaling only with the number of nonzero elements of the data matrix.
Due to being cast in the form of (GLP), the DRO problem can be solved with O(1/(cid:15)) iteration complexity with

4

our CLVR algorithm, while existing methods such as Levy et al. [2020] have O(1/(cid:15)2) iteration complexity, with
higher iteration cost because of the batch of samples needed to reduce bias. This improvement is enabled in part
by considering the primal-dual gap (rather than the primal gap considered in Levy et al. [2020]) and by allowing
the constraints to be approximately satisﬁed (see Corollary 1).

For DRO with Wasserstein metric, following the reformulation of Shaﬁeezadeh Abadeh et al. [2015, The-
orem 1], we show further that the problem can be cast in the form of (GLP). Compared with the existing
reformulations [Shaﬁeezadeh Abadeh et al., 2015, Esfahani and Kuhn, 2018, Li et al., 2019, Yu et al., 2021], our
reformulation can handle both smooth and nonsmooth convex loss functions. In fact, our reformulation can pro-
vide a more compact form for nonsmooth piecewise-linear convex loss functions (such as hinge loss). Moreover,
compared with algorithms customized to this problem [Li et al., 2019] and extragradient methods [Korpelevich,
1976, Nemirovski, 2004, Yu et al., 2021] for general convex-concave min-max problems, our CLVR method
attains the best-known iteration complexity and per-iteration cost, as shown in Table 1.

2 Notation and Preliminaries

For any positive integer p, we use [p] to denote {1, 2, . . . , p}. We assume that there is a given (disjoint) partition
of the set [n] into sets Sj, j ∈ [m], where |Sj| = nj > 0 and (cid:80)m
j=1 nj = n. For j ∈ [m], we use ASj
to denote
the submatrix of A with rows indexed by Sj and ySj
to denote the subvector of y indexed by Sj. We use 0d and
1d to denote the vectors with all ones and all zeros in d dimensions, respectively. Unless otherwise speciﬁed, we
use (cid:107) · (cid:107) to denote the Euclidean norm for vectors and the spectral norm for matrices. For a given proper convex
lower semi-continuous function f : R → R ∪ {+∞}, we deﬁne the convex conjugate in the standard way as
f ∗(y) = supx∈R{yx − f (x)} (so that f ∗∗ = f ). For a vector u, the inequality u ≥ 0 is applied element-wise.
For a convex function r(x), we use r(cid:48)(x) to denote an element of the subdifferential set ∂r(x). The proximal
operator of r(x) is deﬁned by:

proxr( ˆx) = arg min

x∈X

(cid:107)x − ˆx(cid:107)2 + r(x)

(cid:111)
.

(cid:110) 1
2

(1)

Furthermore, we make the following assumptions, which apply throughout the convergence analysis.

Assumption 2. (PD-GLP) admits at least one Nash point (x∗, y∗). W ∗ denotes the set of all Nash points
(x∗, y∗).

Due to the convex-concave property of (PD-GLP), W ∗ is a convex set in X × Rn.

Assumption 3. ˆL = maxj∈[m] (cid:107)ASj (cid:107) is available at the input, where (cid:107)ASj (cid:107) = max(cid:107)x(cid:107)≤1 (cid:107)ASj x(cid:107).

Note that ˆL can be obtained either via preprocessing of the data or by parameter tuning. By combining
maxj∈[m] |Sj|R.

Assumptions 1 and 3, it follows that R ≤ ˆL ≤

(cid:113)

Assumption 4. r(x) is σ-strongly convex (σ ≥ 0); that is, for all x1 and x2 in X and all r(cid:48)(x2) ∈ ∂r(x2), we
have

r(x1) ≥ r(x2) + (cid:104)r(cid:48)(x2), x1 − x2(cid:105) +

(cid:107)x1 − x2(cid:107)2.

σ
2

For convex-concave min-max problems, a common metric for measuring solution quality is the primal-dual

gap, which, for a feasible solution (x, y) of (PD-GLP), is deﬁned by

sup
(u,v)∈X ×Rn

{L(x, v) − L(u, y)}.

(2)

5

However, as the domain of v is unbounded, the primal-dual gap can be inﬁnite, which makes it a poor metric for
measuring the progress of algorithms. As a result, for measuring the progress of our algorithm, we consider the
following restricted primal-dual gap instead:

sup
(u,v)∈W

{L(x, v) − L(u, y)},

(3)

where W ⊂ X × Rn is a compact (i.e., closed and bounded) convex set. Such a restricted version of primal-dual
gap has also been used in existing literature [Nesterov, 2007].

3 The CLVR Algorithm

3.1 Algorithm and Analysis for General Formulation

Algorithm 1 speciﬁes CLVR for (PD-GLP) in the general setting. The algorithm alternates the full update for
xk in Step 4 (O(d) cost) with an incremental block coordinate update for yk in Steps 5 and 6 (with O(|Sjk |d)
cost for dense A). The cost of updating auxiliary vectors zk and qk is O(|Sjk |d) and O(d), respectively. In
essence, CLVR is a primal-dual coordinate method that uses a dual averaging update for xk, then updates the state
variables {qk} by a linear recursion, and computes xk from qk−1 via a proximal step without direct dependence
on xk−1. The output ˜xK is a convex combination of the iterates {xk}K
k=1, as is standard for primal-dual methods.
However, ˜yK is only an afﬁne (not convex) combination of {yk}K
k=0, as it involves the term −(m − 1)y0 (whose
coefﬁcient is negative) and some of the coefﬁcients mak − (m − 1)ak+1 multiplying yk for k ∈ {1, . . . K − 1}
may also be negative. An afﬁne combination still provides valid bounds because the dual variable vector y
appears linearly in (PD-GLP). Moreover, in Step 9, the term mak(zk − zk−1) serves to cancel certain errors
from the randomization of the update w.r.t. yk, thus playing a key role in implicit variance reduction.

Algorithm 1 Coordinate Linear Variance Reduction (CLVR)
1: Input: x0 ∈ X , y0 ∈ Rn, z0 = AT y0, γ > 0, ˆL > 0, σ ≥ 0, K, m, {S1, S2, . . . , Sm}.
2: a1 = A1 = 1
2 ˆLm
3: for k = 1, 2, . . . , K do
4:

, q0 = a1(z0 + c).

5:

6:

7:

8:

γ Akr(x0 − 1
xk = prox 1
Pick jk uniformly at random in [m].

γ qk−1).

(cid:40)

ySi
k =

ySi
k−1,
ySi
k−1 + γmak
√
1+σAk/γ
ak+1 =
2 ˆLm
zk = zk−1 + ASjk ,T (ySjk
qk = qk−1 + ak+1

(cid:0)ASixk − bSi(cid:1),

i (cid:54)= jk
i = jk

.

, Ak+1 = Ak + ak+1.
k − ySjk

k−1).
(cid:0)zk + c(cid:1) + mak(zk − zk−1).

9:
10: end for
11: return ˜xK = 1
AK

(cid:80)K

k=1 akxk, ˜yK = 1
AK

(cid:80)K

k=1(akyk + (m − 1)ak(yk − yk−1)).

Theorem 1 provides the convergence results for Algorithm 1. The proof is provided in Appendix B. In the

theorem (as in the algorithm), γ is a positive parameter that can be tuned.

Theorem 1. Let xk, yk, k = 1, 2, . . . , K, be iterates generated by Algorithm 1 and let ˜xk and ˜yk deﬁned by

˜xk =

1
Ak

k
(cid:88)

i=1

aixi,

˜yk =

1
Ak

k
(cid:88)

i=1

(aiyi + (m − 1)ai(yi − yi−1)),

(4)

for k = 1, 2, . . . , K. Let Wk ⊂ X × Rn, k = 1, 2, . . . , K be a sequence of compact convex sets such that

( ˜xk, ˜yk) ∈ Wk ⊂ W ⊂ X × Rn, where W is also convex and compact. Then the following bound holds:

6

(cid:105)
{L( ˜xk, v) − L(u, ˜yk)}

(cid:104)
E

≤

sup
(u,v)∈Wk
(cid:18)
(cid:104) γ
1
2
Ak

E

(cid:107) ˆu − x0(cid:107)2 +

(cid:107)ˆv − y0(cid:107)2(cid:105)

+

1
γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

(cid:107)y∗ − y0(cid:107)2

(cid:19)
,

1
2γ

where ( ˆu, ˆv) = arg sup(u,v)∈Wk

{L( ˜xk, v) − L(u, ˜yk)}. Furthermore,

E

(cid:104) γ + σAk
4

(cid:107)xk − x∗(cid:107)2 +

(cid:107)yk − y∗(cid:107)2(cid:105)

≤

1
2γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2.

Deﬁne K0 =

(cid:109)

(cid:108)

σ
18 ˆLmγ

. Then in the bounds above:

Ak ≥ max

(cid:26) k

2 ˆLm

(cid:16)

,

σ
(6 ˆLm)2γ

k − K0 + max

(cid:110)
3

(cid:113)

2 ˆLmγ/σ, 1

(cid:111)(cid:17)2(cid:27)
.

(5)

(6)

Observe that ( ˆu, ˆv) in the theorem statement exists because of compactness of Wk and our assumptions on
r(·). The parameter γ can be tuned to balance the relative weights of primal and dual initial quantities (cid:107)x∗ − x0(cid:107)
and (cid:107)y∗ − y0(cid:107) (or estimates of these quantities), which can signiﬁcantly inﬂuence practical performance of the
method.

In addition to the guarantee on the variational form, due to the linear structure, we can also provide explicit

guarantees for both the objective and the constraints in (GLP), summarized in the following corollary.

Corollary 1. In Algorithm 1, for all k ≥ 1, ˜xk satisﬁes

E[(cid:107)y∗(cid:107) · (cid:107)A ˜xk − b(cid:107)] ≤

|E[(cT ˜xk + r( ˜xk)) − (cT x∗ + r(x∗))]| ≤

where v = 2 (cid:107)y∗(cid:107)

(cid:107)A ˜xk−b(cid:107) (A ˜xk − b).

γ(cid:107)x∗ − x0(cid:107)2 + 1

γ(cid:107)x∗ − x0(cid:107)2 + 1

γ

2γ (cid:107)y∗ − y0(cid:107)2 + 1
Ak
2γ (cid:107)y∗ − y0(cid:107)2 + 1
Ak

γ

E[(cid:107)v − y0(cid:107)2]

E[(cid:107)v − y0(cid:107)2]

,

,

In CLVR, we allow for arbitrary (x0, y0) ∈ X × Rn. Nevertheless, by setting y = 0n, we can obtain
z0 = 0d at no cost — a useful strategy for large-scale problems since it avoids the (potentially expensive) single
matrix-vector multiplication w.r.t. A. On the other hand, direct computation of ˜yk can be expensive. However,
Dang and Lan [2015] showed that we only need to update the averaged vector in the coordinate block chosen for
that iteration. This strategy requires us to record the most recent update for each coordinate block and update it
only when it is selected again, which is tricky and and needs to be carefully implemented. We show next that by
introducing auxiliary variables that can be efﬁciently implemented, we can simplify the efﬁcient implementation
signiﬁcantly and make the complexity of CLVR independent of the ambient dimension n · d for sparse and
structured instances of (PD-GLP).

3.2 Lazy Update for Sparse and Structured (PD-GLP)

In Algorithm 1, for dense A, the O(|Sjk |d) cost of Steps 6 and 8 dominates the O(d) cost of Steps 4 and 9.
However, when A is sparse and |Sjk | is small, the cost of Steps 6 and 8 will be O(nnz(ASjk )), which may be
less than the O(d) cost of Steps 4 and 9. Using this observation, we show that the nature of the dual averaging
update enables us to propose an efﬁcient implementation whose complexity depends on nnz(A) rather than n · d.
Recall that we partition [n] into subsets {S1, S2, . . . , Sm} and use ASj (j ∈ [m]) to denote the jth row
that contain

, we use Cj ⊂ [d] to denote the indices of those columns of ASj

block of A. For each block ASj

7

at least one nonzero element. (Of course, {C1, . . . , Cm} is not in general a partition of [d] as different subsets
Sj may have nonzeros in the same columns.) We assume further that X and r are coordinate separable, that is,
X = X1 × · · · × Xd with Xi ⊂ R for all i and r(x) := (cid:80)d

i=1 r(xi) with xi ∈ Xi.

In Step 4 of Algorithm 1, the separability of X and r means that an update to one coordinate block of x —
say the Cjk block in the update from xk−1 to xk, which requires a projection and an application of the proximal
operator — does not inﬂuence other coordinates xi
k for i /∈ Cjk . Moreover, we can efﬁciently maintain an
implicit representation of qk, in terms of a newly introduced auxiliary vector rk; see Lemma 8 of Appendix B.2.
Similarly, to update ˜yk efﬁciently, we maintain an implicit representation of ˜yk via an auxiliary vector sk, as
shown in Lemma 9 of Appendix B.2.

, a2
AK

, . . . , aK
AK

It is generally not possible to maintain ˜xk efﬁciently since, in principle, all components of xk can change on
every iteration, and we wish to avoid the O(d) cost of evaluating every full xk. We seek instead to output a proxy
ˆxK for ˜xK from Algorithm 1 such that E[ ˆxK] = ˜xK. One possible choice is to pick an index k(cid:48) ∈ [K] from the
weighted discrete distribution ( a1
) (computing the scalar quantities ak and Ak for k ∈ [K] in
AK
advance), then setting ˆxK = xk(cid:48). A slightly more sophisticated strategy is to sample a predetermined number
(cid:98)K of vectors xk, k ∈ [K], and deﬁne ˆxK to be the simple average of these vectors. Once again, the indices
are chosen from the weighted discrete distribution ( a1
). Note that the total expected cost of the
AK
K iterations of the algorithm (excluding the full-vector updates) is O(Knnz(A)/m), while the total cost of
evaluating the (cid:98)K full vectors xk and accumulating them into ˆxK is O( (cid:98)Kd). Thus, for the full-step iterations not
to dominate the total cost, we can choose (cid:98)K to be O(Knnz(A)/(md)). Finally, we note that Theorem 1 is for
˜xk, not ˆxK. However, since E[ ˆxk] = ˜xk, we expect the convergence rates from the theorem to hold for ˆxK in
practice.

, . . . , aK
AK

, a2
AK

An implementation of Algorithm 1 that exploits the form of qk in Lemma 8 and ˜yk in Lemma 9, and evaluates
explicitly only those components of xk needed to perform the rest of the iteration is given as Algorithm 2. This
version also incorporates the strategy for obtaining ˆxK by sampling (cid:98)K iterates on which to evaluate the full
vector xk.

Due to the efﬁcient implementation in Algorithm 2, to attain an (cid:15)-accurate solution in terms of the primal-dual
gap in Theorem 1, we need O( nnz(A) ˆL
) FLOPS, which corresponds to O( ˆL
(cid:15) ) data passes. As a result, because a
smaller batch size leads to a smaller ˆL, we attain the best performance in terms of number of data passes when
the batch size is set to one. However, as modern computer architecture has particular design for vectorized
operations, lower runtime is obtained for a small batch size strictly larger than one (see Section 5).

(cid:15)

Remark 1. While we consider the case of fully coordinate separable r and X for simplicity, our lazy update
approach is also applicable to the coordinate block partitioning case in which X = X1 × · · · Xm with Xi ∈
Rdi (i ∈ [m], (cid:80)m
i=1 r(xi) with xi ∈ Xi. The difference is that for each coordinate
block ASj (j ∈ [m]), we overload Cj ⊂ [m] to denote the set of blocks in ASj where each coordinate block
contains at least one nonzero element.

i=1 di = d) and r(x) := (cid:80)m

Remark 2. Except for the dual averaging variant from Song et al. [2021], all stochastic primal-dual methods
[Zhang and Lin, 2015, Chambolle et al., 2018, Fercoq and Bianchi, 2019, Latafat et al., 2019, Alacaoglu et al.,
2020] are based on mirror descent-style updates in which xk depends directly on xk−1 in a nonlinear way when
proximal terms exist. As a result, it is not obvious (and maybe not possible) to design lazy update strategies for
these methods in full generality. Our lazy update strategy can be adapted to the VRPDA2 algorithm from Song
et al. [2021], but the implementation becomes more complicated due to the extrapolation steps on the primal
variables in VRPDA2.

Remark 3. Dual averaging has been shown to have signiﬁcant advantage in producing sparser iterates than
mirror descent in the context of online learning [Xiao, 2010, Lee and Wright, 2012]. It further leads to better
bounds in well-conditioned ﬁnite-sum optimization [Song et al., 2020]. In this work, we show that dual averaging
offers better ﬂexibility with sparse matrices than does mirror descent.

8

3.3 Restart Scheme

We now propose a ﬁxed restart strategy with a ﬁxed number of iterations per each restart epoch and discuss an
adaptive restart strategy for the special case of standard-form LP, which corresponds to (GLP) with r(x) ≡ 0
and X = {x : xi ≥ 0, i ∈ [d]}. We write

and the primal-dual form

min
x

cT x s. t. Ax = b, x ≥ 0d,

min
x≥0d

max
y∈Rn

(cid:110)

L(x, y) = cT x + yT Ax − yT b

(cid:111)
.

(LP)

(PD-LP)

This problem has a sharpness property that can be used to obtain linear convergence in ﬁrst-order methods [Apple-
gate et al., 2021b]. For convenience, in the following, we deﬁne w = (x, y), ˆw = ( ˆx, ˆy), ˜w = ( ˜x, ˜y) and w∗ =
(x∗, y∗). Meanwhile, for γ > 0, we denote the weighted norm (cid:107)w(cid:107)(γ) :=
2. Fur-
ther, we use W ∗ to denote the optimal solution set of the LP and deﬁne the distance to W ∗ by dist(w, W ∗)(γ) =
minw∗∈W ∗ (cid:107)w − w∗(cid:107)(γ). When γ = 1, (cid:107) · (cid:107)(γ) is the standard Euclidean norm. Then based on (PD-LP), we
can use the following classical LP metric2 to measure the progress of iterative algorithms for LP:

γ (cid:107)y − y∗(cid:107)2

γ(cid:107)x − x∗(cid:107)2

2 + 1

(cid:113)

LPMetric(x, y) =

(cid:107) max{−x, 0}(cid:107)2

2 + (cid:107)Ax − b(cid:107)2

2 + (cid:107) max{−AT y − c, 0}(cid:107)2

(cid:113)

2 + | max{cT x + bT y, 0}|2,
(7)

which can be explicitly and directly computed. For the Euclidean case (γ = 1), it is well-known [Hoffman,

2003] that there exists a Hoffman constant H1 such that

LPMetric(w) ≥ H1dist(w, W ∗)(1).

(8)

Using the equivalence of norms in ﬁnite dimensions, for general γ > 0, we can conclude that there exists another
constant Hγ (to which we refer as the generalized Hoffman’s constant) such that

LPMetric(w) ≥ Hγdist(w, W ∗)(γ).

(9)

Using Eq. (9) and the result from Theorem 1, we can then obtain the following bounds for distance and

LPMetric.

Theorem 2. Consider the CLVR algorithm applied to the standard-form LP problem (PD-LP), with input w0
and output ˜wk. Given γ > 0, deﬁne w∗ = arg minw∈W ∗ (cid:107)w0 − w(cid:107)(γ), and deﬁne C0 = γ + 1/γ + (
2 +
1)(cid:107)w0 − w∗(cid:107)(γ) + (cid:107)w∗(cid:107)(γ). Then for Hγ deﬁned as in (9), we have

√

(cid:104)(cid:113)

E

dist( ˜wk, W ∗)(γ)

(cid:105)

≤ 5

E(cid:2)(cid:112)

LPMetric( ˜wk)(cid:3) ≤ 5

(cid:115)

(cid:115)

ˆLmC0
Hγk

ˆLmC0
Hγk

(cid:113)

dist(w0, W ∗)(γ),

(cid:112)

LPMetric(w0).

As a result, by Theorem 2, if we know the values of ˆL, (cid:107)w∗(cid:107)(γ) and Hγ, then by setting k = 100 ˆLmC0
, we
can halve the square root of the distance and the LPMetric in expectation. Thus we can obtain linear convergence
if we restart the CLVR algorithm after a ﬁxed number of iterations. However, the values of (cid:107)w∗(cid:107)(γ) and Hγ are
often unknown and thus make this strategy unrealistic in practice.

Hγ k

2In our primal-dual reformulation (PD-LP), we dualize the constraint Ax = b by yT (Ax − b) instead of yT (b − Ax), so in our

LP metric, there exist a sign difference for y from the more common representation such as the one in Applegate et al. [2021b].

9

Compared with the above ﬁxed restart strategy, a natural strategy is to restart whenever the LPMetric halves
(summarized in Algorithm 4 in the appendix). Since LPMetric is easy to monitor and update, implementation
of this strategy is straightforward. However, bounding the number of iterations required to halve the metric
(in expectation or with high probability) seems nontrivial. What can be said based on Theorem 2, denoting by
K the number of iterations on CLVR between restarts, is that P[K > 50 ˆLmC0
] ≤ δ. This follows by Markov’s
δ2Hγ
(cid:113)
(cid:104)(cid:112)LPMetric( ˜wk) >
2

ˆLmC0
inequality, as P[K > k] = P
Hγ k . Although we use adaptive
restart in our experiments, we leave the exact bound for the number of iterations for future work. Finally, as an
independent and parallel work to ours, Lu and Yang [2021] proposed a high probability guarantee for adaptive
restart for stochastic extragradient-type methods.

(cid:113) LPMetric(w0)
2

≤ 5

(cid:105)

4 Application: DRO

Consider sample vectors {a1, a2, . . . , an} with labels {b1, b2, . . . , bn}, where bi ∈ {1, −1} (i ∈ [n]). The DRO
problem with f -divergence based ambiguity set is

min
x∈X

sup
p∈Pρ,n

n
(cid:88)

i=1

pig(biaT

i x),

(10)

n

i=1 pi = 1, pi ≥ 0 (i ∈ [n]), Df (p(cid:107)1/n) ≤ ρ

where Pρ,n = (cid:8)p ∈ Rn : (cid:80)n
loss function and Df is an f -divergence deﬁned by Df (p(cid:107)q) = (cid:80)
(cid:80)n

(cid:9) is the ambiguity set, g is a convex
i=1 qif (pi/qi) with p, q ∈ (cid:8)p ∈ Rn :
i=1 pi = 1, pi ≥ 0(cid:9) and f being a convex function [Namkoong and Duchi, 2016]. The formulation (10) is a
nonbilinearly coupled convex-concave min-max problem with constraint set Pρ,n for which efﬁcient projections
are not available in general. When g is a nonsmooth loss (e.g., the hinge loss), many well-known methods such
as the extragradient [Korpelevich, 1976, Nemirovski, 2004] cannot be used even if we could project onto Pρ,n
efﬁciently. However, by introducing auxiliary variables and additional linear constraints and simple convex
constraints, we can make the interacting term between primal and dual variables bilinear, as shown next. (See
Appendix C for a proof.)
Theorem 3. Let X be a compact convex set. Then the DRO problem in Eq. (10) is equivalent to the following
problem:

min
x,u,v,w,µ,q,γ

(cid:110)

γ +

ρµ1
n

+

1
n

n
(cid:88)

i=1

(cid:17)(cid:111)

µif ∗(cid:16) qi
µi

s. t. w + v −

− γ1n = 0n,

q
n
ui = biaT
i x,
µ1 = µ2 = · · · = µn,
g(ui) ≤ wi,
qi ∈ µi dom(f ∗),
vi ≥ 0, µi ≥ 0,
x ∈ X .

i ∈ [n]

i ∈ [n]

i ∈ [n]

i ∈ [n]

In Theorem 3, the domain of the one-dimensional convex function f ∗(·) is an interval such as [a, b], so that
(cid:1) is a simple
qi ∈ µi dom(f ∗) denotes the inequality µia ≤ qi ≤ µib. Since the perspective function µf ∗(cid:0) q
convex function of two variables, we can assume that the proximal operator for this function on the domain
{(µ, q) : q ∈ µ dom(f ∗), µ > 0} can be computed efﬁciently [Boyd and Vandenberghe, 2004]. Similarly, we
can assume that the constraint g(u) ≤ w admits an efﬁciently computable projection operator. As a result, the
formulation (10) can be solved by CLVR. When expressing (10) in the form (PD-GLP), the primal and dual

µ

10

(cid:15)

variable vectors have dimensions d + 1 + 4n and 3n − 1, respectively. However, according to Table 1, provided
that X is coordinate separable, the overall complexity of CLVR will only be O(cid:0) (nnz(A)+n)(R+1)

(cid:1).

The original DRO problem with Wasserstein metric based ambiguity set is an inﬁnite-dimensional nonbilin-

early coupled convex-concave min-max problem deﬁned by

min
w∈Rd

sup
P∈Pρ,κ

EP[g(baT w)],

(11)

where a ∈ Rd, b ∈ {1, −1}, P is a distribution on Rd × {1, −1}, g is a convex loss function and Pρ,κ is the
Wasserstein metric-based ambiguity set [Shaﬁeezadeh Abadeh et al., 2015]. Our reformulation for Eq. (11) can
be found in Appendix C.2.

5 Numerical Experiments and Discussion

Figure 1: Comparison of numerical performance in terms of number of data passes and wall-clock time.

We provide experimental evaluations of our algorithm for the reformulation of the DRO with Wasserstein
metric of (cid:96)1-norm (with κ = 0.1 and ρ = 10) and hinge loss. Our code is available at https://github.
com/ericlincc/Efficient-GLP. For the DRO with Wasserstein metric of (cid:96)1-norm, since the dual norm
of the (cid:96)1 norm is the (cid:96)∞ norm, the reformulation for this problem in Theorem 4 is a standard-form LP. For
the LP formulation, we compare our CLVR method with three representative methods: PDHG [Chambolle and
Pock, 2011], SPDHG [Chambolle et al., 2018] and PURE-CD [Alacaoglu et al., 2020]. For all algorithms we
use LPMetric (7) as the performance measure and use a restart strategy of successive halving of LPMetric
(Section 3.3) to obtain linear convergence. We implemented CLVR and other algorithms in Julia, optimizing all
implementations to the best of our ability. For SPDHG, whose per-iteration cost is at least O(d), we consider
a large batch size 50 to balance the effect of the O(d) cost and improve the overall efﬁciency. Meanwhile,
PURE-CD with block size 1 is already well suited to sparsity. For CLVR, we experiment with block sizes 1 and 10.
We conducted our experiments on LibSVM datasets a9a, gisette, and rcv1.binary, each with different
sparsity levels. For fair comparison in terms of wall-clock time, we implement all the algorithms in the Julia
programming language3 and run each algorithm using one CPU core, all the experiments are run on a Linux
machine with a second generation Intel Xeon Scalable Processor (Cascade Lake-SP) with 128 GB of RAM. As

3Julia is particularly designed for high performance numerical computation

11

the weight parameter γ (see Theorem 1) between primal and dual variables strongly inﬂuences the empirical
performance, we tune it for all datasets by trying the values {10−i} for i ∈ Z. We set the Lipschitz constant of
PDHG to be the largest singular value of the constraint matrix in the LP formulation. For PURE-CD and CLVR with
block size 1, because the rows of the matrix are normalized, we set the Lipschitz constant to 1. For CLVR with
block size 10 and SPDHG with block size 50, the Lipschitz constants are tuned to 3 and 9, respectively.

In Table 2, we list information about the three datasets and the corresponding matrices in the reformulations.
As we see, due to the sparse connectivity of auxiliary variables, all the matrices in reformulations are quite sparse.
As a commonly adopted preprocessing step for LP, we normalize each row of the matrix in the standard-form LP
with Euclidean norm one.

Table 2: The dimension and sparsity of the original datasets and the corresponding matrices in reformulations.

Dataset Original (d, n) #nonzeros / (d × n) Reformulated (d, n) #nonzeros / (d × n)

a9a

(123, 32561)
gisette (5000, 6000)
(47236, 20242)

rcv1

0.11
0.99
1.5 × 10−3

(130738, 97929)
(44002, 28000)
(269914, 155198)

9.6 × 10−5
4.9 × 10−2
8.8 × 10−5

Figure 1 compares the number of data passes and wall-clock time for various algorithms. For the number
of data passes (top three ﬁgures), CLVR with block size 1 and PURE-CD perform best on all three datasets.
CLVR with block size 10 and SPDHG with block size 50 have second-tier performance, while PDHG is worst.
For the CLVR algorithm, smaller block size corresponds to smaller ˆL in Assumption 3, which corresponds
to better complexity in terms of data passes by Theorem 1. Nevertheless, the consistency between empirical
performance and theoretical guarantee for SPDHG and PURE-CD deserves further research because, to date, they
have only been shown to have the same iteration complexity as PDHG. Empirically on a9a, CLVR with block
size 1 performs better than PURE-CD in terms of data passes. Note that all plots contain spikes due to the restart
strategy. At the beginning of each cycle, the value of LP metric increases signiﬁcantly, then decreases rapidly
thereafter.

In terms of wall-clock time (bottom three ﬁgures), because of different per-iteration costs of each algorithm
and instruction-level parallelism in modern processors [Hennessy and Patterson, 2011], the plots differ signiﬁ-
cantly from the plots for number of data passes. Even with block size 50, SPDHG spends the most wall-clock
time for one data pass and is the slowest on sparse datasets a9a and rcv1, but is faster than PDHG on the
dense dataset gisette. Meanwhile, while CLVR with block size 10 is not best in terms of data passes, it
remains fastest in terms of wall-clock time on all datasets due to cheaper per-iteration cost and instruction-level
parallelism. On rcv1, the per-iteration cost of PURE-CD is about 60% of that of CLVR with block size 1. Hence,
despite having similar performance in terms of data passes, PURE-CD is faster than CLVR with block size 1, but
is still slower than CLVR with block size 10.

Our CLVR algorithm is fastest in both the number of data passes and wall-clock time on these datasets. Since

it also has the best-known theoretical guarantee, we believe that CLVR is an appropriate “method of choice”.

Acknowledgement

Chaobing Song acknowledges very useful discussions with Ahmet Alacaoglu.

References

Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods. arXiv

preprint arXiv:2102.08352, 2021.

12

Ahmet Alacaoglu, Quoc Tran Dinh, Olivier Fercoq, and Volkan Cevher. Smooth primal-dual coordinate descent

algorithms for nonsmooth convex optimization. In Proc. NIPS’17, 2017.

Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher. On the convergence of stochastic primal-dual hybrid

gradient. arXiv preprint arXiv:1911.00799, 2019.

Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher. Random extrapolation for primal-dual coordinate descent.

In Proc. ICML’20, 2020.

Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proc. ACM

STOC’17, 2017.

Erling D Andersen and Knud D Andersen. The mosek interior point optimizer for linear programming: an
implementation of the homogeneous algorithm. In High performance optimization, pages 197–232. Springer,
2000.

David Applegate, Mateo Díaz, Oliver Hinder, Haihao Lu, Miles Lubin, Brendan O’Donoghue, and War-
ren Schudy. Practical large-scale linear programming using primal-dual hybrid gradient. arXiv preprint
arXiv:2106.04756, 2021a.

David Applegate, Oliver Hinder, Haihao Lu, and Miles Lubin. Faster ﬁrst-order primal-dual methods for linear

programming using restarts and sharpness. arXiv preprint arXiv:2105.12715, 2021b.

Pascal Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on Optimization,

26(4):2235–2260, 2016.

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Variance reduction for matrix games.

In

Proc. NeurIPS’19, 2019.

Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications

to imaging. Journal of mathematical imaging and vision, 40(1):120–145, 2011.

Antonin Chambolle, Matthias J Ehrhardt, Peter Richtárik, and Carola-Bibiane Schonlieb. Stochastic primal-dual
hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM Journal on Optimization,
28(4):2783–2808, 2018.

Cong D Dang and Guanghui Lan. Stochastic block mirror descent methods for nonsmooth and stochastic

optimization. SIAM Journal on Optimization, 25(2):856–881, 2015.

Daniela Pucci De Farias and Benjamin Van Roy. The linear programming approach to approximate dynamic

programming. Operations research, 51(6):850–865, 2003.

Jim Douglas and Henry H Rachford. On the numerical solution of heat conduction problems in two and three

space variables. Transactions of the American Mathematical Society, 82(2):421–439, 1956.

John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust

optimization. arXiv preprint arXiv:1810.08750, 2018.

John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized

empirical likelihood approach. Mathematics of Operations Research, 2021.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171
(1):115–166, 2018.

13

Olivier Fercoq and Pascal Bianchi. A coordinate-descent primal-dual algorithm with large step size and possibly

nonseparable functions. SIAM Journal on Optimization, 29(1):100–134, 2019.

Gurobi Optimizer Gurobi. Reference manual, gurobi optimization, 2020.

Erfan Yazdandoost Hamedani and Afrooz Jalilzadeh. A stochastic variance-reduced accelerated primal-dual

method for ﬁnite-sum saddle-point problems. arXiv preprint arXiv:2012.13456, 2020.

John L Hennessy and David A Patterson. Computer architecture: a quantitative approach. Elsevier, 2011.

N. Ho-Nguyen and S. J. Wright. Adversarial classiﬁcation via distributional robustness with Wasserstein

ambiguity. arXiv preprint arXiv:2005.13815, November 2021.

Alan J Hoffman. On approximate solutions of systems of linear inequalities. In Selected Papers Of Alan J

Hoffman: With Commentary, pages 174–176. World Scientiﬁc, 2003.

Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give

robust classiﬁers? In Proc. ICML’18, 2018.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Proc. NIPS’13, 2013.

Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic

mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.

G. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Ekonomika i Matem-

aticheskie Metody, 12(5):747–756 (in Russian; English translation in Matekon), 1976.

Puya Latafat, Nikolaos M Freris, and Panagiotis Patrinos. A new randomized block-coordinate primal-dual
proximal algorithm for distributed optimization. IEEE Transactions on Automatic Control, 64(10):4050–4065,
2019.

Sangkyun Lee and Stephen J. Wright. Manifold identiﬁcation in dual averaging for regularized stochastic online

learning. Journal of Machine Learning Research, 13(6), 2012.

Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust

optimization. In Proc. NeurIPS’20, 2020.

Jiajin Li, Sen Huang, and Anthony Man-Cho So. A ﬁrst-order algorithmic framework for wasserstein distribu-

tionally robust logistic regression. arXiv preprint arXiv:1910.12778, 2019.

Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher Strong, Clark Barrett, Mykel J Kochenderfer,
et al. Algorithms for verifying deep neural networks. Foundations and Trends® in Optimization, 4, 2020.

Haihao Lu and Jinwen Yang. Nearly optimal linear convergence of stochastic primal-dual methods for linear

programming. arXiv preprint arXiv:2111.05530, 2021.

OL Mangasarian. A newton method for linear programming. Journal of Optimization Theory and Applications,

121(1):1–18, 2004.

Olvi L Mangasarian. Normal solutions of linear programs. In Mathematical Programming at Oberwolfach II,

pages 206–216. Springer, 1984.

Olvi L Mangasarian and RR Meyer. Nonlinear perturbation of linear programs. SIAM Journal on Control and

Optimization, 17(6):745–752, 1979.

14

Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization

with f-divergences. In Proc. NIPS’16, 2016.

Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization, 15(1):229–251, 2004.

Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems.

Mathematical Programming, 109(2-3):319–344, 2007.

Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction method of multipliers.

In Proc. ICML’13, 2013.

Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of ﬁrst-order methods for convex-concave bilinear

saddle-point problems. Mathematical Programming, pages 1–35, 2019.

Andrei Patrascu and Ion Necoara. Nonasymptotic convergence of stochastic proximal point methods for

constrained convex optimization. Journal of Machine Learning Research, 18(1):7204–7245, 2017.

Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint

arXiv:1908.05659, 2019.

R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and

optimization, 14(5):877–898, 1976.

Soroosh Shaﬁeezadeh Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic

regression. In Proc. NIPS’15, 2015.

Alexander Shapiro. On duality theory of conic linear problems. In Semi-inﬁnite programming, pages 135–165.

Springer, 2001.

Chaobing Song, Yong Jiang, and Yi Ma. Variance reduction via accelerated dual averaging for ﬁnite-sum

optimization. In Proc. NeurIPS’20, 2020.

Chaobing Song, Stephen J Wright, and Jelena Diakonikolas. Variance reduction via primal-dual accelerated dual

averaging for nonsmooth convex ﬁnite-sums. In Proc. ICML’21, 2021.

Josef Stoer. Duality in nonlinear programming and the minimax theorem. Numerische Mathematik, 5(1):

371–379, 1963.

Cédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.

Wolfram Wiesemann, Daniel Kuhn, and Melvyn Sim. Distributionally robust convex optimization. Operations

Research, 62(6):1358–1376, 2014.

Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research, 11(Oct):2543–2596, 2010.

Yaodong Yu, Tianyi Lin, Eric Mazumdar, and Michael I Jordan. Fast distributionally robust learning with

variance reduced min-max optimization. arXiv preprint arXiv:2104.13326, 2021.

Yuchen Zhang and Xiao Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimiza-

tion. In Proc. ICML’15, 2015.

15

Coordinate Linear Variance Reduction
for Generalized Linear Programming

Appendix

Outline. The appendix of this paper is organized as follows:

• Section A contains the pseudo code for CLVR with lazy update.
• Section B provides the proofs for Section 3.
• Section C provides the proofs for Section 4.

A Efﬁcient Lazy Implementation of CLVR

Algorithm 2 Coordinate Linear Variance Reduction with Lazy Update (Lazy CLVR)

1: Input:

˜x0 = x0 = x−1 ∈ X , y0 ∈ Rn, z0 = AT y0, γ > 0, ˆL > 0, K, (cid:98)K, m, {S1, S2, . . . , Sm},

{C1, C2, . . . , Cm}.

, q0 = a1(z0 + c), r0 = 0d, s0 = 0n.

(cid:98)K} i.i.d. from [K] according to the distribution

(cid:110) a1
AK

, a2
AK

, . . . , aK
AK

(cid:111)
.

, Ak+1 = Ak + ak+1.

2: a0 = A0 = 0, a1 = A1 = 1
2 ˆLm
3: for k = 1, 2, . . . , K − 1 do
√

1+σAk/γ
2 ˆLm

ak+1 =

4:
5: end for
6: Choose indices {(cid:96)1, . . . , (cid:96)
7: for k = 1, 2, . . . , K do
8:

Pick jk uniformly at random in [m].
if k = (cid:96)i for some i = 1, 2, . . . , (cid:98)K then

qk−1 = Ak(c + zk−1) + rk−1.
γ Akr(x0 − 1
xk = prox 1
γ qk−1).

9:

10:

11:

12:

k−1) + rCjk
k−1.
γ qCjk
0 − 1
k−1).

else

14:

13:

15:

17:

16:

k−1 = Ak(cCjk + zCjk
qCjk
xCjk
γ Akr(xCjk
k = prox 1
end if
k = ySjk
ySjk
k = zCjk
zCjk
rCjk
k = rCjk
sSjk
k = sSjk
19:
20: end for
21: ˆxK = 1
i=1 x(cid:96)i.
(cid:98)K
22: ˜yK = yK + 1
sK.
AK
23: return ˆxK and ˜yK.

(cid:80) (cid:98)K

18:

(cid:0)ASjk ,Cjk xCjk

k−1 + γmak
k − ySjk
k−1 + (ASjk ,Cjk )T (ySjk
k−1 + (mak − Ak)(zCjk
k − zCjk
k−1 + ((m − 1)ak − Ak−1)(ySjk

k − bSjk (cid:1), ySi
k = zi
k−1), zi
k = ri
k−1), ri
k − ySjk
k−1), si

k−1 for all i (cid:54)= jk.

k−1 for all i /∈ Cjk ;
k−1 for all i /∈ Cjk ;

k = si

k−1 for all i /∈ Sjk ;

B Omitted Proofs from Section 3

B.1 Omitted Proofs from Section 3.1

We state a version of CLVR in Algorithm 3 that is convenient for the analysis, and is equivalent to Algorithm 1 in
the main body of the paper. Before analyzing the convergence of CLVR, we justify our claim of equivalence in

16

Proposition 1.

Proposition 1. The iterates of Algorithm 1 and 3 are equivalent.

Proof. To argue equivalence, we show that the iterates of Algorithm 1 and 3 solve the same optimization
problems. To avoid ambiguity, here we will use ˆxk, ˆyk to denote the iterates xk, yk in Algorithm 3, while we
retain the notation xk, yk for the iterates of Algorithm 1.

Let us ﬁrst start by writing an equivalent deﬁnition of xk in Algorithm 1. To do so, we ﬁrst unroll the
recursive deﬁnitions of zk and qk. We can observe that, since by deﬁnition yk and yk−1 only differ over the
coordinate block Sjk , we have

zk = z0 +

k
(cid:88)

i=1

AT (yi − yi−1) = AT yk.

On the other hand, using Eq. (12), the deﬁnition of qk implies

qk = Ak+1c + a1AT y0 +

k
(cid:88)

i=1

AT (cid:2)ai+1yi + mai(yi − yi−1)(cid:3)

(12)

(13)

xk = arg min

Using the deﬁnition of the proximal operator (see Eq. (1)) and the deﬁnition of xk in Step 4 of Algorithm 1, we
have

(cid:13)
1
(cid:13)
(cid:13)x − x0 +
2
(cid:13)
γ
(cid:13)
(cid:13)x − x0 +
2
Now let us consider the optimization problem that deﬁnes ˆxk. Assume for now that the deﬁnitions of yk and
ˆyk agree (we justify this claim below). Observe ﬁrst that the minimization problem deﬁning ˆxk is independent
of u, so by unrolling the recursion for φk, we have

(cid:13)
(cid:13)
(cid:13)
2(cid:27)
.

= arg min
x∈X

(cid:26) Ak
γ

Akr(x) +

1
γ
1
γ

r(x) +

qk−1

qk−1

(14)

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

x∈X

(cid:26)

(cid:40)

γ
2

(cid:40)

γ
2
(cid:110) γ
2

(cid:26)

ˆxk = arg min

x∈X

= arg min
x∈X

= arg min
x∈X

= arg min
x∈X

= xk.

(cid:107)x − x0(cid:107)2 + Akr(x) + a1

(cid:10)x, c + AT ¯y0

(cid:11) +

(cid:28)

ai

x, c + AT (cid:16)

yi−1 +

mai−1
ai

(yi−1 − yi−2)

(cid:17)(cid:29) (cid:41)

k
(cid:88)

i=2

(cid:107)x − x0(cid:107)2 + Akr(x) +

(cid:42)

x, Akc + a1AT y0 + AT (cid:16) k

(cid:88)

(cid:2)aiyi−1 + mai−1(yi−1 − yi−2)(cid:3)(cid:17)

(cid:43) (cid:41)

(cid:107)x − x0(cid:107)2 + Akr(x) + (cid:104)x, qk−1(cid:105)

(cid:111)

i=2

Akr(x) +

(cid:13)
(cid:13)
(cid:13)x − x0 +

γ
2

1
γ

qk−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

It remains to argue that the deﬁnitions of yk and ˆyk agree. First, observe that since the deﬁnitions of ψk and
ψk−1 differ only over block Sjk , we have ˆySj
k−1 for all j (cid:54)= jk. For j = jk, we have by unrolling the
recursive deﬁnition of ψk that

k = ˆySj

ˆySjk
k = arg min
ySjk ∈Rnjk

(cid:110) 1
2γ

(cid:107)ySjk − ySjk

0 (cid:107)2 −

k
(cid:88)

i=1

1
{Sji =Sjk }mai

(cid:68)

ySji , ASji xi − bSji (cid:69) (cid:111)

= ySjk

0 + γ

k
(cid:88)

i=1

{Sji =Sjk }mai(ASji xi − bSji )
1

= ˆySjk
= ySjk
k

k−1 + γmak(ASjk xk − bSjk )

,

17

as claimed.

Algorithm 3 Coordinate Linear Variance Reduction (Analysis Version)
1: Input: x0 = x−1 ∈ X , y0 = ¯y0 ∈ Rn, m, {S1, S2, . . . , Sm}, K, γ > 0, ˆL > 0.
2: φ0(·) = γ
3: a1 = A1 = 1
2 ˆLm
4: for k = 1, 2, 3, . . . , K do
5:

2 (cid:107) · −x0(cid:107)2, ψ0(·) = 1

2γ (cid:107) · −y0(cid:107)2.

.

xk = arg minx∈X {φk(x) = φk−1(x) + ak((cid:104)x − u, AT ¯yk−1 + c(cid:105) + r(x) − r(u))}.
Pick jk uniformly at random in [m].
yk = arg miny∈Rn{ψk(y) = ψk−1(y) + mak(−(cid:104)ySjk − vSjk , ASjk xk − bSjk (cid:105))}.
1+σAk/γ
ak+1 =
2 ˆLm
¯yk = yk + mak
ak+1

, Ak+1 = Ak + ak+1.

(yk − yk−1).

√

6:

7:

8:

(cid:80)K

k=1 akxk, ˜yK = 1
AK

(cid:80)K

k=1(akyk + (m − 1)ak(yk − yk−1)).

9:
10: end for
11: return ˜xK := 1
AK

In the following three lemmas, we bound φk(xk) and ψk(yk) below and above, which is then subsequently

used to bound the primal-dual gap in Theorem 1.

Lemma 1. For all steps of Algorithm 3 with k ≥ 1, we have, ∀(u, v) ∈ X × Rn,

φk(xk) ≤

ψk(yk) ≤

γ
(cid:107)u − x0(cid:107)2 −
2
1
2γ

(cid:107)v − y0(cid:107)2 −

γ + σAk
2
(cid:107)v − yk(cid:107)2.

1
2γ

(cid:107)u − xk(cid:107)2,

Proof. By the deﬁnitions of ψk(y) and φk(x) in Algorithm 3, it follows that, ∀k ≥ 1,

and

φk(x) =

ψk(y) =

k
(cid:88)

i=1

k
(cid:88)

i=1

ai((cid:104)x − u, AT ¯yi−1 + c(cid:105) + r(x) − r(u)) +

γ
2

(cid:107)x − x0(cid:107)2,

(cid:16)

ySji − vSji , ASji xi − bSji (cid:69) (cid:17)
(cid:68)

+

−

mai

1
2γ

(cid:107)y − y0(cid:107)2.

(15)

(16)

Observe that, as function of x, φk(x) is (γ+σAk)-strongly convex. As, by deﬁnition, xk = arg minx∈X φk(x),

it follows that

φk(u) ≥ φk(xk) +

γ + σAk
2

(cid:107)u − xk(cid:107)2.

(17)

Now, writing φk(u) explicitly and rearranging the last inequality, the stated bound on φk(xk) follows.

As a function of y, ψk(y) is 1/γ-strongly convex. The proof for the bound on ψk uses the same argument

and is omitted.

In the following proof, for k ≥ 1, let Fk denote the natural ﬁltration, containing all the randomness in the

(cid:33)

(cid:32) AS1
...
ASm

, and let A ¯Sj (j ∈ [m]) denote the matrix A

algorithm up to and including iteration k. Recall that A =

with its Sj block of rows replaced by a zero block.

18

For convenience, for k = 1, 2, . . . , we deﬁne

ˆyk = yk−1 + γmak(Axk − b).

(18)

Then from the deﬁnition of yk in Algorithm 1, we have E[yk − yk−1|Fk−1] = 1

m ( ˆyk − yk−1).

Motivated by Alacaoglu et al. [2019], we have the following result.

Lemma 2. Given the sequences {yk} in Algorithm 3 and { ˆyk} in Eq. (18), we deﬁne the sequence {ˇvk} by

ˇvk = (yk − yk−1) −

1
m

( ˆyk − yk−1).

Then for any v ∈ Rd that may be correlated with the randomness in {ˇvi}k

i=1, we have

(cid:104)
E

−

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105)

(cid:105)

≤ E

(cid:104) 1
2

(cid:107)y0 − v(cid:107)2 +

1
2

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

,

(19)

where the expectation is taken over all the randomness in the history.

Proof. First, we prove a bound for E
each ˇvi, we have

(cid:104) (cid:80)k

i=1 (cid:107)ˇvi(cid:107)2(cid:105)

. By the fact that E[yi − yi−1|Fi−1] = 1

m ( ˆyi − yi−1), for

(cid:104)
E[(cid:107)ˇvi(cid:107)2|Fi−1] = E

(cid:107)yi − yi−1(cid:107)2|Fi−1

(cid:105)

− E

(cid:104)(cid:13)
(cid:13)

1
m

(cid:0) ˆyi − yi−1

(cid:1)(cid:13)
2|Fi−1
(cid:13)

(cid:105)

≤ E[(cid:107)yi − yi−1(cid:107)2|Fi−1].

Taking expectation on all the randomness in the history, for E

(cid:104) (cid:80)k

i=1 (cid:107)ˇvi(cid:107)2(cid:105)

, we have

(cid:104) k
(cid:88)

E

(cid:107)ˇvi(cid:107)2(cid:105)

(cid:104) k
(cid:88)

= E

(cid:105)
E[(cid:107)ˇvi(cid:107)2|Fi−1]

(cid:104) k
(cid:88)

≤ E

E[(cid:107)yi − yi−1(cid:107)2|Fi−1]

(cid:105)

(cid:104) k
(cid:88)

= E

(cid:107)yi − yi−1(cid:107)2(cid:105)

.

(20)

i=1

i=1

i=1

i=1

Then to prove (19), we deﬁne the sequence { ˇyk}∞
expanding 1

2 (cid:107) ˇyk − v(cid:107)2,

k=0 by ˇy0 = y0 and ˇyk = ˇyk−1 − ˇvk for k = 1, 2, . . . . By

1
2

(cid:107) ˇyk − v(cid:107)2 =

=⇒ (cid:104)ˇvk, ˇyk−1 − v(cid:105) =

1
2
1
2

(cid:107) ˇyk−1 − v(cid:107)2 − (cid:104)ˇvk, ˇyk−1 − v(cid:105) +

(cid:107) ˇyk−1 −

1
2

(cid:107) ˇyk − v(cid:107)2 +

1
2

(cid:107)ˇvk(cid:107)2.

1
2

(cid:107)ˇvk(cid:107)2

By summing this expression and telescoping, we obtain

k
(cid:88)

i=1

(cid:104) ˇyi−1 − v, ˇvi(cid:105) =

≤

=

1
2

1
2

1
2

(cid:107) ˇy0 − v(cid:107)2 −

1
2

(cid:107) ˇyk − v(cid:107)2 +

k
(cid:88)

i=1

1
2

(cid:107)ˇvi(cid:107)2

(cid:107) ˇy0 − v(cid:107)2 +

(cid:107)y0 − v(cid:107)2 +

k
(cid:88)

i=1
k
(cid:88)

i=1

1
2

1
2

(cid:107)ˇvi(cid:107)2

(cid:107)ˇvi(cid:107)2,

(21)

19

It follows that

(cid:104)
E

−

k
(cid:88)

i=1

(cid:105)

(cid:104)v, ˇvi(cid:105)

(cid:104) k
(cid:88)

= E

i=1

(cid:104) ˇyi−1 − v, ˇvi(cid:105) −

(cid:105)

(cid:104) ˇyi−1, ˇvi(cid:105)

k
(cid:88)

i=1

≤ E

(cid:104) 1
2

≤ E

(cid:104) 1
2

(cid:107)y0 − v(cid:107)2 +

(cid:107)y0 − v(cid:107)2 +

k
(cid:88)

i=1

1
2

1
2

k
(cid:88)

i=1

(cid:107)ˇvi(cid:107)2 −

(cid:105)

(cid:104) ˇyi−1, ˇvi(cid:105)

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2 −

(cid:104) ˇyi−1, ˇvi(cid:105)

(cid:105)
.

k
(cid:88)

i=1

(22)

where the ﬁrst inequality is by Eq. (21) and the second inequality is by Eq. (20). To obtain the result (19), we
use the facts that E[ˇvi | Fi−1] = 0 and that ˇyi−1 ∈ Fi−1 to obtain E

(cid:104) (cid:80)k

= 0.

(cid:105)

i=1 (cid:104) ˇyi−1, ˇvi(cid:105)

We emphasize that Lemma 2 holds for any v that may be even correlated with the randomness in the

algorithm.

Lemma 3. For all steps of Algorithm 3 with k ≥ 1, we have for all (u, v) ∈ X × Rn that

φk(xk) ≥ φk−1(xk−1) +

γ + σAk−1
2

− ak

(cid:10)xk − u, AT (yk − yk−1)(cid:11) + ak

(cid:107)xk − xk−1(cid:107)2 + ak(r(xk) − r(u))
(cid:10)xk − u, AT yk + c(cid:11)

+ mak−1

(cid:16) (cid:10)xk−1 − u, AT (yk−1 − yk−2)(cid:11) + (cid:10)xk − xk−1, AT (yk−1 − yk−2)(cid:11) (cid:17)

,

ψk(yk) ≥ ψk−1(yk−1) +

1
2γ

(cid:107)yk − yk−1(cid:107)2 − ak(cid:104)Axk − b, yk − v(cid:105) − (m − 1)ak(cid:104)Axk − b, yk − yk−1(cid:105)

(cid:28)

+

1
γ

yk−1 − v, −(yk − yk−1) +

( ˆyk − yk−1)

(cid:29)

.

1
m

Proof. For the ﬁrst claim, we have from the deﬁnition of φk(xk), using that φk−1(xk−1) is (γ +σAk−1)-strongly
convex and minimized at xk−1, that

φk(xk) ≥ φk−1(xk−1) +

γ + σAk−1
2

(cid:107)xk − xk−1(cid:107)2

+ ak

(cid:0)(cid:104)xk − u, AT ¯yk−1 + c(cid:105) + r(xk) − r(u)(cid:1).

Meanwhile, by the deﬁnition of { ¯yk} (using y−1 = y0 for the case of k = 1), we have that

ak

= ak

(cid:11)

(cid:10)xk − u, AT ¯yk−1
(cid:28)
xk − u, AT (cid:16)
yk−1 +
(cid:10)xk − u, AT (yk − yk−1)(cid:11) + ak

mak−1
ak

= − ak

(yk−1 − yk−2)

(cid:17)(cid:29)

(cid:10)xk − u, AT yk
(cid:16) (cid:10)xk−1 − u, AT (yk−1 − yk−2)(cid:11) + (cid:10)xk − xk−1, AT (yk−1 − yk−2)(cid:11) (cid:17)

(cid:11)

+ mak−1

(23)

.

(24)

The claimed lower bound on φk(xk) follows when we combine (23) and (24).

For the second claim, we have by the deﬁnition of ψk that

ψk(yk) − ψk−1(yk−1) = ψk−1(yk) − ψk−1(yk−1) − mak

(cid:68)

k − vSjk , ASjk xk − bSjk (cid:69)
ySjk

.

(25)

20

To obtain the claimed lower bound on ψk, we proceed to bound the terms on the right-hand side in (25). First,
since ψk−1 is (1/γ)-strongly convex and minimized at yk−1, we have

ψk−1(yk) − ψk−1(yk−1) ≥

1
2γ

(cid:107)yk − yk−1(cid:107)2.

(26)

Second, by using the deﬁnition of ASjk and A
their Sjk components, we have

¯Sjk , and by using several times that yk−1 and yk differ only in

k − vSjk , ASjk xk − bSjk (cid:69)
(cid:68)
ySjk

−

= − (cid:104)yk − v, Axk − b(cid:105) +

(cid:68)

yk − v, A

¯Sjk xk − b

¯Sjk (cid:69)

= − (cid:104)yk − v, Axk − b(cid:105) +

= − (cid:104)yk − v, Axk − b(cid:105) +

(cid:28)

+

yk−1 − v, A

(cid:68)

¯Sjk xk − b

¯Sjk (cid:69)

yk−1 − v, A
m − 1
m
¯Sjk −
¯Sjk xk − b

m − 1
m

(cid:104)yk−1 − v, Axk − b(cid:105)
(cid:29)

(Axk − b)

1
m
(cid:68)

= −

(cid:104)yk − v, Axk − b(cid:105) +

(cid:104)yk−1 − yk, Axk − b(cid:105)

+

(cid:69)
k−1 − vSjk , −(ASjk xk − bSjk )
ySjk

+

1
m

(cid:104)yk−1 − v, Axk − b)(cid:105)

m − 1
m

= −

1
m
(cid:28)

(cid:104)yk − v, Axk − b(cid:105) +

m − 1
m

+

yk−1 − v, −

(cid:104)yk−1 − yk, Axk − b(cid:105)

1
γmak

(yk − yk−1) +

(cid:29)

( ˆyk − yk−1)

,

1
γm2ak

(27)

where in the last equality we used ySjk
k − ySjk
which both hold by deﬁnitions of ˆyk and yk.

k−1 = γmak(ASjk xk − bSjk ) and ˆyk − yk−1 = γmak(Axk − b),

Finally, combining (25)–(27), we have the bound on ψk(yk) from the statement of the lemma.

By combining the two lower bounds in Lemma 3, we obtain the following result.

Lemma 4. For any (u, v) ∈ X × Rn, the sum of ψk(yk) + φk(xk) can be bounded as follows: for all k ≥ 1,

φk(xk) + ψk(yk)

≥ φk−1(xk−1) + ψk−1(yk−1)

− mak
1
2γ

(cid:10)xk − u, AT (yk − yk−1)(cid:11) + mak−1
1
(cid:107)yk − yk−1(cid:107)2 −
4γ

(cid:107)yk−1 − yk−2(cid:107)2 + Qk,

+

(cid:10)xk−1 − u, AT (yk−1 − yk−2)(cid:11)

(28)

where

(cid:16)

Qk :=ak

r(xk) − r(u) − (cid:104)Au, yk(cid:105) + (cid:104)xk, AT v(cid:105) + (cid:104)xk − u, c(cid:105) + (cid:104)b, yk − v(cid:105)
(cid:17)

(cid:28)

− (m − 1)(cid:104)Au − b, yk − yk−1(cid:105)

+

yk−1 − v, −(yk − yk−1) +

1
γ

(cid:29)

( ˆyk − yk−1)

(29)

.

1
m

Proof. Before our proof, as A−1 and A0 are not used in Algorithm 3, without loss of generality, we set
A−1 = A0 = 0. Fix any (u, v) ∈ X × Rn. By combining the bounds on φk(xk) and ψk(yk) from Lemma 3,

21

we have ∀k ≥ 1 that

φk(xk) + ψk(yk)

≥ φk−1(xk−1) + ψk−1(yk−1)

(cid:10)xk − u, AT (yk − yk−1)(cid:11) + mak−1

− mak
+ Pk + Qk,

(cid:10)xk−1 − u, AT (yk−1 − yk−2)(cid:11)

(30)

where

Pk =

γ + σAk−1
2

(cid:107)xk − xk−1(cid:107)2 +

1
2γ

(cid:107)yk − yk−1(cid:107)2 + mak−1

(cid:10)xk − xk−1, AT (yk−1 − yk−2)(cid:11)

(31)

and Qk is deﬁned in Eq. (29).

To ﬁnd a lower bound on Pk we start by bounding the magnitude of the inner product term in (31). Recall

that yk−2 and yk−1 differ only on the coordinate block Sjk−1. We thus have:

|mak−1(cid:104)xk − xk−1, AT (yk−1 − yk−2)(cid:105)|

= |mak−1 (cid:104)ASjk−1 (xk − xk−1), ySjk−1
≤ mak−1(cid:107)ASjk−1 (xk − xk−1)(cid:107)(cid:107)ySjk−1
≤ (mak−1)2γ(cid:107)ASjk−1 (xk − xk−1)(cid:107)2 +

≤ (m ˆLak−1)2γ(cid:107)xk − xk−1(cid:107)2 +

= (m ˆLak−1)2γ(cid:107)xk − xk−1(cid:107)2 +

1
4γ
1
4γ

k−2

k−1 − ySjk−1
(cid:105)|
k−1 − ySjk−1
k−2 (cid:107)
1
k−1 − ySjk−1
(cid:107)ySjk−1
4γ
(cid:107)ySjk−1

k−1 − ySjk−1

k−2 (cid:107)2

k−2 (cid:107)2

(cid:107)yk−1 − yk−2(cid:107)2

(32)

where the second inequality is by Young’s inequality, and the third inequality is by Assumption 3 where
(cid:107)ASjk−1 (xk − xk−1)(cid:107) ≤ ˆL(cid:107)xk − xk−1(cid:107). With our setting ak−1 =
we have

1+σAk−1/γ
2m ˆL

1+σAk−2/γ
2m ˆL

, for all k ≥ 1,

√

√

≤

(m ˆLak−1)2γ =

γ + σAk−1
4

.

By substituting this equality into (32) and then combining with (31), we obtain

Pk ≥

γ + σAk−1
4

(cid:107)xk − xk−1(cid:107)2 +

1
2γ

(cid:107)yk − yk−1(cid:107)2 −

1
4γ

(cid:107)yk−1 − yk−2(cid:107)2

≥

1
2γ

(cid:107)yk − yk−1(cid:107)2 −

1
4γ

(cid:107)yk−1 − yk−2(cid:107)2.

We complete the proof by combining Eqs. (30), (31) and the bound Eq. (33) for Pk.

By telescoping the inequality in Lemma 4, and using Lemmas 1 and 2, we obtain the next result.

Lemma 5. For all (u, v) ∈ X × Rn, we have

Ak(L( ˜xk, v) − L(u, ˜yk)) +

γ + σAk
4

(cid:107)u − xk(cid:107)2 +

1
2γ

(cid:107)v − yk(cid:107)2

≤

γ
2

(cid:107)u − x0(cid:107)2 +

1
2γ

(cid:107)v − y0(cid:107)2 −

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2 +

1
γ

k
(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105) −

1
γ

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105) ,

where ˇvi is deﬁned in Lemma 2.

(33)

(34)

22

Proof. Telescoping the inequality in Lemma 4, we have

φk(xk) + ψk(yk) ≥ φ0(x0) + ψ0(y0)

− mak

(cid:10)xk − u, AT (yk − yk−1)(cid:11) + ma0

(cid:10)x0 − u, AT (y0 − y−1)(cid:11)

+

1
2γ

(cid:107)yk − yk−1(cid:107)2 −

1
4γ

(cid:107)y0 − y−1(cid:107)2 +

k−1
(cid:88)

(cid:107)yi − yi−1(cid:107)2 +

1
4γ

= − mak

(cid:10)xk − u, AT (yk − yk−1)(cid:11) +

1
4γ

+

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2 +

k
(cid:88)

i=1

Qi,

i=1
(cid:107)yk − yk−1(cid:107)2

k
(cid:88)

i=1

Qi

(35)

where the last equality is by the fact that ψ0(y0) = φ0(x0) = 0, a0 = 0, and our convention that y−1 := y0.

Then by the convexity of r(·) and the deﬁnition of {Qi}, we have

k
(cid:88)

i=1

Qi ≥ Ak(r( ˜xk) − r(u) − (cid:104)Au, ˜yk(cid:105) + (cid:104) ˜xk, AT v(cid:105) + (cid:104) ˜xk − u, c(cid:105) + (cid:104)b, ˜yk − v(cid:105))

+

1
γ

k
(cid:88)

i=1

(cid:28)

yi−1 − v, −(yi − yi−1) +

(cid:29)

( ˆyi − yi−1)

1
m

= Ak(L( ˜xk, v) − L(u, ˜yk)) +

1
γ

k
(cid:88)

i=1

(cid:104)yi−1 − v, −ˇvi(cid:105) .

(36)

i=1 aixi, ˜yk = 1
where ˜xk = 1
Ak
Ak
equality is by the deﬁnition of the Lagrangian L(·, ·) and {ˇvi} in Lemma 2.

i=1(aiyi + (m − 1)ai(yi − yi−1)) (as deﬁned in (4)) and the last

(cid:80)k

(cid:80)k

Then by combining Eqs. (35)-(36) and Lemma 1, we have

Ak(L( ˜xk, v) − L(u, ˜yk))

(cid:16)

≤

ψk(yk) + φk(xk) + mak

(cid:10)xk − u, AT (yk − yk−1)(cid:11) (cid:17)

−

1
4γ

(cid:107)yk − yk−1(cid:107)2 −

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2

+

1
γ

k
(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105) −

1
γ

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105)

≤

(cid:16) 1
2γ

(cid:107)v − y0(cid:107)2 −

1
2γ

(cid:107)v − yk(cid:107)2 +

γ
2

(cid:107)u − x0(cid:107)2 −

γ + σAk
2

(cid:107)u − xk(cid:107)2(cid:17)

+ mak

(cid:10)xk − u, AT (yk − yk−1)(cid:11) −

1
4γ

(cid:107)yk − yk−1(cid:107)2 −

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2

+

1
γ

k
(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105) −

1
γ

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105) .

Finally, we have from the fact that yk and yk−1 differ in only the Sjk components, Young’s inequality, and

23

the deﬁnition of ak that

|mak(cid:104)xk − u, AT (yk − yk−1)(cid:105)|

= |mak (cid:104)ASjk (xk − u), ySjk
≤ mak(cid:107)ASjk (xk − u)(cid:107)(cid:107)ySjk
≤ (mak)2γ(cid:107)ASjk (xk − u)(cid:107)2 +

k − ySjk
k − ySjk

≤ (m ˆLak)2γ(cid:107)xk − u(cid:107)2 +

= (m ˆLak)2γ(cid:107)xk − u(cid:107)2 +

k − ySjk

k−1(cid:107)2

k−1(cid:105)|
k−1(cid:107)
1
(cid:107)ySjk
4γ
k − ySjk
(cid:107)ySjk

k−1(cid:107)2

(cid:107)yk − yk−1(cid:107)2

1
4γ
1
4γ
1
4γ

=

≤

γ + σAk−1
4
γ + σAk
4

(cid:107)xk − u(cid:107)2 +

(cid:107)yk − yk−1(cid:107)2

(cid:107)xk − u(cid:107)2 +

1
4γ

(cid:107)yk − yk−1(cid:107)2

leading to

Ak(L( ˜xk, v) − L(u, ˜yk)) +

1
2γ

(cid:107)v − yk(cid:107)2 +

γ + σAk
4

(cid:107)u − xk(cid:107)2

≤

γ
2

(cid:107)u − x0(cid:107)2 +

1
2γ

(cid:107)v − y0(cid:107)2 −

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2

+

1
γ

k
(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105) −

1
γ

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105)

and completing the proof.

Lemma 6. Suppose that (x∗, y∗) is a Nash point for (PD-GLP). Then the iterates xk, yk from Algorithm 1
satisfy

E

(cid:104) γ + σAk
4

(cid:107)x∗ − xk(cid:107)2 +

1
2γ

(cid:107)y∗ − yk(cid:107)2 +

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

(37)

≤

γ
2

(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2,

where the expectation is w.r.t. all the randomness in the algorithm.

Proof. Note that the existence of a Nash point is assumed in Assumption 2. With (u, v) = (x∗, y∗), by the
deﬁnition of Nash equilibrium, we have

L( ˜xk, y∗) − L(x∗, ˜yk) = (L( ˜xk, y∗) − L(x∗, y∗)) − (L(x∗, ˜yk) − L(x∗, y∗)) ≥ 0.

(38)

By setting (u, v) = (x∗, y∗) in the result of Lemma 5, using (38) to eliminate the ﬁrst term on the left-hand
side of the inequality, and rearranging, we obtain

γ + σAk
4

(cid:107)x∗ − xk(cid:107)2 +

1
2γ

(cid:107)y∗ − yk(cid:107)2 +

1
4γ

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2

≤

γ
2

(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2 +

1
γ

k
(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105) −

1
γ

k
(cid:88)

i=1

(cid:104)v, ˇvi(cid:105) .

24

The result will follow when we show that the expectation (with respect to all the randomness) of the last two
terms on the right-hand side is zero. Since yi−1 ∈ Fi−1, we have

(cid:34) k

(cid:88)

E

i=1

(cid:35)

(cid:34)

(cid:104)yi−1, ˇvi(cid:105)

= E

E

(cid:34) k

(cid:88)

i=1

(cid:104)yi−1, ˇvi(cid:105)

(cid:12)
(cid:12)
(cid:12)Fi−1

(cid:35)(cid:35)

(cid:34) k

(cid:88)

(cid:35)
(cid:104)yi−1, E [ˇvi|Fi−1](cid:105)

= E

= 0,

(39)

i=1

which takes care of the second-last term. For any ﬁxed v ∈ Rn, we also have

E

(cid:20) 1
γ

k
(cid:88)

i=1

(cid:21)

= 0,

(cid:104)v, ˇvi(cid:105)

(40)

which takes care of the last term, and completes the proof.

Next we state a technical lemma, proved in an earlier paper, which provides the ﬁnal ingredient for the proof

of Theorem 1.

Lemma 7. [[Song et al., 2021]] Let {Ak}k≥0 be a sequence of nonnegative real numbers such that A0 = 0 and
Ak is deﬁned recursively via Ak = Ak−1 + (cid:112)c2
(cid:101).
Then

1 + c2Ak−1, where c1 > 0, and c2 ≥ 0. Deﬁne K0 = (cid:100) c2
9c1

Ak ≥






(cid:16)

c2
9
c1k,

k − K0 + max

(cid:110)
3

(cid:113) c1
c2

, 1

(cid:111)(cid:17)2

,

if c2 > 0 and k > K0,

otherwise.

We are now ready to prove our main result, which we restate here.

Theorem 1. Let xk, yk, k = 1, 2, . . . , K, be iterates generated by Algorithm 1 and let ˜xk and ˜yk deﬁned by

˜xk =

1
Ak

k
(cid:88)

i=1

aixi,

˜yk =

1
Ak

k
(cid:88)

i=1

(aiyi + (m − 1)ai(yi − yi−1)),

(4)

for k = 1, 2, . . . , K. Let Wk ⊂ X × Rn, k = 1, 2, . . . , K be a sequence of compact convex sets such that

( ˜xk, ˜yk) ∈ Wk ⊂ W ⊂ X × Rn, where W is also convex and compact. Then the following bound holds:

(cid:105)
{L( ˜xk, v) − L(u, ˜yk)}

(cid:104)
E

≤

sup
(u,v)∈Wk
(cid:18)
(cid:104) γ
1
2
Ak

E

(cid:107) ˆu − x0(cid:107)2 +

(cid:107)ˆv − y0(cid:107)2(cid:105)

+

1
γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

(cid:107)y∗ − y0(cid:107)2

(cid:19)
,

1
2γ

where ( ˆu, ˆv) = arg sup(u,v)∈Wk

{L( ˜xk, v) − L(u, ˜yk)}. Furthermore,

E

(cid:104) γ + σAk
4

(cid:107)xk − x∗(cid:107)2 +

(cid:107)yk − y∗(cid:107)2(cid:105)

≤

1
2γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2.

Deﬁne K0 =

(cid:109)

(cid:108)

σ
18 ˆLmγ

. Then in the bounds above:

Ak ≥ max

(cid:26) k

2 ˆLm

(cid:16)

,

σ
(6 ˆLm)2γ

k − K0 + max

(cid:110)
3

(cid:113)

2 ˆLmγ/σ, 1

(cid:111)(cid:17)2(cid:27)
.

(5)

(6)

Proof. To provide a guarantee in terms of primal-dual gap, we need to take the supremum on u, v of {L( ˜xk, v)−
L(u, ˜yk))} over Wk; we denote the arg sup by ( ˆu, ˆv). When we subsequently take the expectation, we have to
account for the fact that ˆv will be correlated with the randomness in the iteration history. We can however, use
(cid:104)
Lemmas 2 and 6 to give the upper bound on E

− (cid:80)k

(cid:105)
.

i=1 (cid:104)ˆv, ˇvi(cid:105)

25

From (34), using the fact that 1

2γ (cid:107)v − yk(cid:107)2 + γ+σAk

4

(cid:107)u − xk(cid:107)2 ≥ 0, we have

(cid:104)
E

Ak

sup
(u,v)∈Wk

(cid:105)
{L( ˜xk, v) − L(u, ˜yk)})

≤ E

(cid:104) γ
2

(cid:107) ˆu − x0(cid:107)2 +

(cid:107)y0 − ˆv(cid:107)2(cid:105)

−

1
2γ

1
4γ

(cid:104) k
(cid:88)

E

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

+

1
γ

(cid:104) k
(cid:88)

E

i=1

(cid:105)

(cid:104)yi−1, ˇvi(cid:105)

+

(cid:104)
E

−

1
γ

k
(cid:88)

i=1

(cid:105)

(cid:104)ˆv, ˇvi(cid:105)

≤ E

(cid:104) γ
2

(cid:107) ˆu − x0(cid:107)2 +

(cid:107)y0 − ˆv(cid:107)2(cid:105)

−

1
2γ

1
4γ

(cid:104) k
(cid:88)

E

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

+

1
γ

E

(cid:104) 1
2

(cid:107)y0 − ˆv(cid:107)2 +

1
2

k
(cid:88)

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

= E

(cid:104) γ
2

≤ E

(cid:104) γ
2

(cid:107) ˆu − x0(cid:107)2 +

(cid:107) ˆu − x0(cid:107)2 +

1
γ

1
γ

(cid:107)y0 − ˆv(cid:107)2(cid:105)

+

1
4γ

(cid:107)y0 − ˆv(cid:107)2(cid:105)

+

γ
2

(cid:104) k
(cid:88)

E

(cid:107)yi − yi−1(cid:107)2(cid:105)

i=1
(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2.

(41)

where the ﬁrst inequality is by Eq. (34), the second inequality is by Lemma 2 and (39), and the last inequality is
by Lemma 6. This proves the ﬁrst claim (5). The second claim (6) follows from Lemma 6 with the fact that
1
4γ

≥ 0. The ﬁnal claim concerning Ak follows from Lemma 7 when we set

i=1 (cid:107)yi − yi−1(cid:107)2(cid:105)

(cid:104) (cid:80)k
E

c1 =

1
2 ˆLm

,

c2 =

σ
(2 ˆLm)2γ

.

Corollary 1. In Algorithm 1, for all k ≥ 1, ˜xk satisﬁes

E[(cid:107)y∗(cid:107) · (cid:107)A ˜xk − b(cid:107)] ≤

|E[(cT ˜xk + r( ˜xk)) − (cT x∗ + r(x∗))]| ≤

where v = 2 (cid:107)y∗(cid:107)

(cid:107)A ˜xk−b(cid:107) (A ˜xk − b).

γ(cid:107)x∗ − x0(cid:107)2 + 1

γ(cid:107)x∗ − x0(cid:107)2 + 1

γ

2γ (cid:107)y∗ − y0(cid:107)2 + 1
Ak
2γ (cid:107)y∗ − y0(cid:107)2 + 1
Ak

γ

E[(cid:107)v − y0(cid:107)2]

E[(cid:107)v − y0(cid:107)2]

,

,

Proof. Assume that (cid:107)A ˜xk − b(cid:107) (cid:54)= 0, as otherwise the ﬁrst bound follows trivially. Let u = x∗ and v =
2(cid:107)y∗(cid:107)(A ˜xk−b)
(cid:107)A ˜xk−b(cid:107)

. Then we have

L( ˜xk, v) − L(u, ˜yk)

= (cT ˜xk + r( ˜xk) + 2(cid:107)y∗(cid:107)(cid:107)A ˜xk − b(cid:107)) − (cT x∗ + r(x∗) + ˜yT
= (cT ( ˜xk − x∗) + r( ˜xk) − r(x∗)) + 2(cid:107)y∗(cid:107)(cid:107)A ˜xk − b(cid:107).

k (Ax∗ − b))

(42)

For any ﬁxed u, and any v ∈ Rn possibly depending on the randomness in the algorithm, we have from

26

Lemma 5, taking expectation over all the randomness, that

AkE[L( ˜xk, v) − L(u, ˜yk)]

(cid:20)

≤ E

Ak(L( ˜xk, v) − L(u, ˜yk)) +

1
2γ

≤

γ
2

(cid:107)u − x0(cid:107)2 +

1
2γ

E[(cid:107)v − y0(cid:107)2] − E

(cid:104) 1
4γ

k
(cid:88)

i=1

+

1
γ

(cid:104) k
(cid:88)

E

i=1

(cid:104)yi−1, ˇvi(cid:105)

(cid:105)

−

1
γ

(cid:104) k
(cid:88)

E

(cid:104)v, ˇvi(cid:105)

(cid:105)

.

i=1

(cid:107)v − yk(cid:107)2 +

(cid:107)u − xk(cid:107)2

γ + σAk
4
(cid:107)yi − yi−1(cid:107)2(cid:105)

(cid:21)

(43)

Meanwhile, we have

−

1
4γ

(cid:104) k
(cid:88)

E

i=1

(cid:107)yi − yi−1(cid:107)2(cid:105)

−

1
γ

(cid:104) k
(cid:88)

E

(cid:105)

(cid:104)v, ˇvi(cid:105)

i=1

≤

≤

1
2γ

1
2γ

(cid:107)y0 − v(cid:107)2(cid:105)
(cid:104)
E

+

1
4γ

E[(cid:107)y0 − v(cid:107)2] +

γ
2

(cid:104) k
(cid:88)

E

(cid:107)yi − yi−1(cid:107)2(cid:105)

i=1
(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2,

where the ﬁrst inequality is by Lemma 2 and the second inequality is by Lemma 6. Since yi−1 ∈ Fi−1, we have
as in (39) that

1
γ

(cid:104) k
(cid:88)

E

i=1

(cid:104)yi−1, ˇvi(cid:105)

(cid:105)

= 0.

By combining Eq. (42)-(44) with u = x∗, we have

E(cid:2)(cT ( ˜xk − x∗) + r( ˜xk) − r(x∗)) + 2(cid:107)y∗(cid:107)(cid:107)A ˜xk − b(cid:107)(cid:3)
E[(cid:107)v − y0(cid:107)2] + 1
γ(cid:107)x∗ − x0(cid:107)2 + 1
γ

2γ (cid:107)y∗ − y0(cid:107)2

.

Ak

≤

By the KKT condition of (PD-GLP) and the optimality of (x∗, y∗), we have for all x ∈ X that

(cT x + r(x)) − (cT x∗ + r(x∗)) − (cid:104)y∗, Ax − b(cid:105) ≥ 0,

and thus

(cT x + r(x)) − (cT x∗ + r(x∗)) ≥ −(cid:107)y∗(cid:107)(cid:107)Ax − b(cid:107).

(44)

(45)

(46)

(47)

By combining Eq. (45) and Eq. (47), we have

E[(cid:107)y∗(cid:107) · (cid:107)A ˜xk − b(cid:107)] ≤

γ(cid:107)x∗ − x0(cid:107)2 + 1
γ

E[(cid:107)v − y0(cid:107)2] + 1

2γ (cid:107)y∗ − y0(cid:107)2

Ak

,

(48)

proving our ﬁrst claim. The second claim is obtained by combining Eqs. (45), (47), and (48).

27

B.2 Omitted Proof from Section 3.2

We need to compute explicitly only those components of qk−1 and xk that are needed to update yk.

Lemma 8. For {qk} deﬁned in Algorithm 1, we have

qk = Ak+1(c + zk) + rk,

k = 0, 1, 2, . . . ,

where r0 = 0 and for all k = 1, 2, . . .

rk = rk−1 + (mak − Ak)(zk − zk−1).

Proof. The proof is by induction. For k = 0, we have q0 = A1(c + z0) which is true by deﬁnition. Assuming
that the result holds for some index k, we show that it continues to hold for k + 1. We have from Step 9 of
Algorithm 1, then using the inductive assumption, that

qk+1 = ak+2(zk+1 + c) + mak+1(zk+1 − zk) + qk

= ak+2(zk+1 + c) + mak+1(zk+1 − zk) + Ak+1(c + zk) + rk
= Ak+2c + (Ak+2 − Ak+1)zk+1 + mak+1(zk+1 − zk) + Ak+1zk + rk
= Ak+2(c + zk+1) + (mak+1 − Ak+1)(zk+1 − zk) + rk
= Ak+2(c + zk+1) + rk+1,

as required.

This lemma indicates that we can reconstruct qk (or any subvector of qk that we need, on demand), provided
we maintain zk and rk. Note that the update from zk to zk+1 is sparse; these two vectors differ only in the
components corresponding to the block Cjk . To obtain rk+1 from rk, we need to add a scalar times the sparse
vector zk+1 − zk, so this update is also efﬁcient.

We can also maintain an implicit representation of the averaged vector ˜yk efﬁciently, as shown in the

following lemma. We defer the proofs of Lemmas 8 and 9 to Appendix B.

Lemma 9. For { ˜yk} deﬁned in (4), we have

˜yk = yk +

1
Ak

sk,

k = 1, 2, . . . ,

where s0 = 0 and for all k = 1, 2, . . . ,

sk = sk−1 + ((m − 1)ak − Ak−1)(yk − yk−1).

Proof. Recall that ˜yK (K ≥ 1) is deﬁned in Step 11 of Algorithm 1. The proof is by induction. For k = 1,
˜y1 = y1 + 1
(m − 1)a1(y1 − y0) = y1 + 1
s1 which is true by the deﬁnition of s1 and A0. Next, we assume
A1
A1
that the result holds for some k ≥ 2 and show that it continues to hold for k + 1. We have

Ak+1 ˜yk+1 =

k+1
(cid:88)

(aiyi + (m − 1)ai(yi − yi−1))

i=1

= Ak ˜yk + ak+1yk+1 + (m − 1)ak+1(yk+1 − yk)
= Akyk + sk + ak+1yk+1 + (m − 1)ak+1(yk+1 − yk)
= Ak(yk − yk+1 + yk+1) + sk + ak+1yk+1 + (m − 1)ak+1(yk+1 − yk)
= Ak+1yk+1 + sk + ((m − 1)ak+1 − Ak)(yk+1 − yk)
= Ak+1yk+1 + sk+1.

as required.

28

B.3 Omitted Proof from Section 3.3

The standard-form LP (PD-LP) is derived by setting r(x) ≡ 0 and X = {x ∈ Rd : xi ≥ 0, i ∈ [d]} in
(PD-GLP). Given any w ∈ X × Rn, we deﬁne a compact convex subset Wζ,γ(w) of X × Rn as follows:

Wζ,γ(w) = { ˆw ∈ X × Rn : (cid:107) ˆw − w(cid:107)(γ) ≤ ζ, ζ > 0, γ > 0},

(49)

where we have deﬁned (cid:107) · (cid:107)(γ) by (cid:107)w(cid:107)(γ) =

(cid:113)

γ(cid:107)x(cid:107)2 + 1

γ (cid:107)y(cid:107)2 in Section 3.3.

Lemma 10. Consider the standard-form LP (LP). Given τ > 0 and w ∈ X × Rn, if (cid:107)w(cid:107)(γ) ≤ τ and ζ ≤ τ ,
then

sup
ˆw∈Wζ,γ (w)

{L(x, ˆy) − L( ˆx, y)} ≥

ζ
γ + 1/γ + τ

LPMetric(w).

(50)

Proof. Let F (w) =

(cid:21)

(cid:20)AT y + c
b − Ax

. Then we have

ρζ,γ(w) :=

sup
ˆw∈Wζ,γ (w)

{L(x, ˆy) − L( ˆx, y)} =

sup
ˆw∈Wζ,γ (w)

F (w)T (w − ˆw) ≥ 0,

(51)

where the inequality follows from w ∈ Wζ,γ(w).

First, we prove

ρζ,γ(w) ≥

ζ(cid:107) max{−AT y − c, 0}(cid:107)2
γ

.

(52)

If (cid:107) max{−AT y − c, 0}(cid:107)2 > 0, for ˆw1 = ( ˆx1, ˆy1), let ˆx1 = x +
max{−AT y − c, 0} ∈
X and ˆy1 = y. Then we have (cid:107) ˆw1 − w(cid:107)(γ) = ζ and thus ˆw1 ∈ Wζ,γ(w). So, as ρζ,γ(w) ≥ F (w)T (w − ˆw1),
with the deﬁnition of ˆw1, Eq. (52) holds. If (cid:107) max{−AT y − c, 0}(cid:107)2 = 0, then Eq. (52) holds trivially.

ζ
γ(cid:107) max{−AT y−c, 0}(cid:107)2

Second, we prove

ρζ,γ(w) ≥ ζγ(cid:107)Ax − b(cid:107)2.

(53)

If (cid:107)Ax − b(cid:107)2 > 0, for ˆw2 = ( ˆx2, ˆy2), let ˆx2 = x and ˆy2 = y + ζγ
(cid:107) ˆw2 − w(cid:107)(γ) = ζ and thus ˆw2 ∈ Wζ,γ(w). Then as ρζ,γ(w) ≥ F (w)T (w − ˆw2), Eq. (53) holds.
(cid:107)Ax − b(cid:107)2 = 0, then Eq. (53) holds trivially.

(cid:107)Ax−b(cid:107) (Ax − b). Then we have
If

Third, we prove that

ρζ,γ(w) ≥

ζ
τ

max{cT x + bT y, 0}.

(54)

Note that the inequality holds trivially if cT x + bT y ≤ 0, so we assume that cT x + bT y > 0, and note
that F (w)T w = cT x + bT y > 0 in this case. This condition implies that w (cid:54)= 0, so we can deﬁne
ˆw3 = w − min (cid:8) ζ
(cid:107)w(cid:107)(γ) ≤ ζ. Meanwhile, we also have
ˆx3 ≥ x3 − x3 = 0, so that ˆw3 ∈ X × Rn. Thus, we have ˆw3 ∈ Wζ,γ(w). Then, with the assumptions ζ ≤ τ
and (cid:107)w(cid:107)(γ) ≤ τ, together with F (w)T w > 0, we have

, 1(cid:9)w. Then we have (cid:107) ˆw3 − w(cid:107)(γ) ≤ ζ

(cid:107)w(cid:107)(γ)

(cid:107)w(cid:107)(γ)

ρζ,γ(w) ≥ min

(cid:110) ζ

(cid:107)w(cid:107)(γ)

, 1

(cid:111)

F (w)T w ≥ min

(cid:110) ζ
τ

, 1

(cid:111)

F (w)T w =

ζ
τ

F (w)T w =

ζ
τ

(cT x + bT y),

(55)

completing the proof of the claim (54).

29

By combining Eqs. (52), (53) and (54), we obtain

(γ + 1/γ + τ )2ρ2

ζ,γ(w) ≥ ζ 2((cid:107) max{−AT y − c, 0}(cid:107)2
≥ ζ 2(LPMetric(w))2,

2 + (cid:107)Ax − b(cid:107)2

2 + max{cT x + bT y, 0}2)

(56)

from which the result follows.

Lemma 11. Consider the standard-form LP (LP), and let w∗ = (x∗, y∗) be a Nash point (that is, a solution of
the primal-dual form (PD-LP)). For a starting point w0, deﬁne ζ = (cid:107)w0 − w∗(cid:107)(γ) and choose γ > 0. Then for
all k = 1, 2, . . . , we have

E[(cid:107) ˜wk − w∗(cid:107)(γ)] ≤

√

2(cid:107)w0 − w∗(cid:107)(γ),

where the expectation is taken w.r.t. all the randomness up to iteration k. Further, for Wζ,γ( ˜wk) deﬁned as in
(49), we have

(cid:104)
E

sup
w∈Wζ,γ ( ˜wk)

{L( ˜xk, y) − L(y, ˜yk)}

(cid:105)

≤

25 ˆLm
k

(cid:107)w0 − w∗(cid:107)2

(γ).

(57)

Proof. By Theorem 1, we have

E

(cid:104) γ
4

(cid:107)x∗ − xk(cid:107)2 +

(cid:107)y∗ − yk(cid:107)2(cid:105)

≤

1
2γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

1
2γ

(cid:107)y∗ − y0(cid:107)2 =

1
2

(cid:107)w0 − w∗(cid:107)2

(γ),

by the deﬁnition of (cid:107) · (cid:107)(γ). Using this deﬁnition again, we have that the left-hand side in this expression is
bounded below by 1

4 (cid:107)wk − w∗(cid:107)2

(γ), so that

(cid:104)
E

By convexity of (cid:107) · (cid:107)2, we obtain

(cid:107)wk − w∗(cid:107)2

(γ)] ≤ 2(cid:107)w0 − w∗(cid:107)2

(γ),

∀k ≥ 1.

(58)

E[(cid:107) ˜wk − w∗(cid:107)2

(γ)] = E

(cid:104)(cid:13)
(cid:13)
(cid:13)

1
k

k
(cid:88)

(wi − w∗)

i=1

(cid:105)

(cid:13)
2
(cid:13)
(cid:13)

(γ)

≤

1
k

(cid:104) k
(cid:88)

E

i=1

(cid:107)wi − w∗(cid:107)2

(γ)

(cid:105)

≤ 2(cid:107)w0 − w∗(cid:107)2

(γ).

(59)

The ﬁrst claim of the lemma now follows by applying Jensen’s inequality to this bound.
γ(cid:107) ˆx − x0(cid:107)2 + 1

(cid:104)
For ˆw = ( ˆx, ˆy) ∈ Wζ,γ( ˜wk), we have the following bound on E

γ (cid:107) ˆy − y0(cid:107)2(cid:105)
:

(cid:104)
E

(cid:107) ˆy − y0(cid:107)2(cid:105)

1
γ

(γ)

γ(cid:107) ˆx − x0(cid:107)2 +
= E(cid:2)(cid:107)w0 − ˆw(cid:107)2
(cid:3)
= E(cid:2)(cid:107)w0 − w∗ + w∗ − ˜wk + ˜wk − ˆw(cid:107)2
≤ E(cid:2)((cid:107)w0 − w∗(cid:107)(γ) + (cid:107)w∗ − ˜wk(cid:107)(γ) + (cid:107) ˜wk − ˆw(cid:107)(γ))2(cid:3)
≤ E(cid:2)3((cid:107)w0 − w∗(cid:107)2
(γ))(cid:3)
(γ) + (cid:107)w∗ − ˜wk(cid:107)2
(γ) + (cid:107) ˜wk − ˆw(cid:107)2
≤ E(cid:2)3((cid:107)w0 − w∗(cid:107)2
(γ) + (cid:107)w0 − w∗(cid:107)2
(γ) + 2(cid:107)w∗ − w0(cid:107)2
= 12(cid:107)w0 − w∗(cid:107)2
(γ),

(γ)

(cid:3)

(γ))(cid:3)

(60)

where the ﬁrst inequality is by the triangle inequality, the second inequality is by the arithmetic inequality, the
third inequality is by Eq. (59) and our assumption ˆw ∈ Wζ,γ( ˜wk) with ζ = (cid:107)w0 − w∗(cid:107)(γ).

30

For the case of linear programming, the strong convexity constant is σ = 0, so that Ak = k
2 ˆLm

in CLVR.
Thus, by applying Theorem 1 with Wk = Wζ,γ( ˜wk) and ˆw = ( ˆx, ˆy) = arg supw=(x,y)∈Wζ,γ ( ˜wk){L( ˜xk, y) −
L(x, ˜yk)}, and using the deﬁnition of (cid:107) · (cid:107)(γ) and Eq. (60), we have

(cid:104)
E

(cid:105)
{L( ˜xk, y) − L(y, ˜yk)}

sup
w∈Wζ,γ ( ˜wk)

E

(cid:104) γ
2

(cid:107) ˆx − x0(cid:107)2 +

(cid:107) ˆy − y0(cid:107)2(cid:105)

+

1
γ

γ
2

(cid:107)x∗ − x0(cid:107)2 +

(cid:19)

(cid:107)y∗ − y0(cid:107)2

1
2γ

≤

≤

≤

≤

(cid:18)

(cid:16)

(cid:16)

2 ˆLm
k
2 ˆLm
k
2 ˆLm
k
25 ˆLm
k

E(cid:2)(cid:107)w0 − ˆw(cid:107)2

(γ)

(cid:3) +

12(cid:107)w0 − w∗(cid:107)2

(γ) +

(cid:107)w0 − w∗(cid:107)2

(γ)

(cid:17)

(cid:107)w0 − w∗(cid:107)2

(γ)

(cid:17)

1
2
1
2

(cid:107)w0 − w∗(cid:107)2

(γ).

(61)

Then with Theorem 1, Lemmas 10 and 11, we give our theorem for the ﬁxed restart strategy.

Theorem 2. Consider the CLVR algorithm applied to the standard-form LP problem (PD-LP), with input w0
and output ˜wk. Given γ > 0, deﬁne w∗ = arg minw∈W ∗ (cid:107)w0 − w(cid:107)(γ), and deﬁne C0 = γ + 1/γ + (
2 +
1)(cid:107)w0 − w∗(cid:107)(γ) + (cid:107)w∗(cid:107)(γ). Then for Hγ deﬁned as in (9), we have

√

(cid:104)(cid:113)

E

dist( ˜wk, W ∗)(γ)

(cid:105)

≤ 5

E(cid:2)(cid:112)

LPMetric( ˜wk)(cid:3) ≤ 5

(cid:115)

(cid:115)

ˆLmC0
Hγk

ˆLmC0
Hγk

(cid:113)

dist(w0, W ∗)(γ),

(cid:112)

LPMetric(w0).

Proof. Applying Lemma 10 with w = ˜wk, τ = (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ) and ζ = (cid:107)w0 − w∗(cid:107)(γ) ≤ τ , we
have

sup
ˆw∈Wζ,γ (w)

{L( ˜xk, ˆy) − L( ˆx, ˜yk)} ≥

(cid:107)w0 − w∗(cid:107)(γ)LPMetric( ˜wk)
γ + 1/γ + (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ)

.

(62)

As x2

y is jointly convex in x, y on the domain {(x, y) : x ∈ R, y > 0} [Boyd and Vandenberghe, 2004,
Example 3.18] using Jensen’s inequality, we have that E[ x2
. (In our case, this simply follows as x, y
depend on the same source of randomness.) Applying this inequality to (62) with x = (cid:112)LPMetric( ˜wk) and
y = γ + 1/γ + (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ), we obtain

y ] ≥ (E[x])2
E[y]

(cid:104)
E

sup
ˆw∈Wζ,γ ( ˜wk)

{L( ˜xk, ˆy) − L( ˆx, ˜yk)}

(cid:105)

≥ E

(cid:21)

(cid:20) (cid:107)w0 − w∗(cid:107)(γ)((cid:112)LPMetric( ˜wk))2
γ + 1/γ + (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ)
E(cid:2)(cid:112)LPMetric( ˜wk)(cid:3)(cid:17)2
(cid:107)w0 − w∗(cid:107)(γ)
(cid:3) .
E(cid:2)γ + 1/γ + (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ)

(cid:16)

≥

(63)

31

Using Lemma 11, we have

(cid:3)

E(cid:2)γ + 1/γ + (cid:107) ˜wk(cid:107)(γ) + (cid:107)w0 − w∗(cid:107)(γ)
= γ + 1/γ + (cid:107)w0 − w∗(cid:107)(γ) + E[(cid:107) ˜wk(cid:107)(γ)]
≤ γ + 1/γ + (cid:107)w0 − w∗(cid:107)(γ) + E[(cid:107)w∗(cid:107)(γ) + (cid:107) ˜wk − w∗(cid:107)(γ)]
2(cid:107)w0 − w∗(cid:107)(γ)
≤ γ + 1/γ + (cid:107)w0 − w∗(cid:107)(γ) + (cid:107)w∗(cid:107)(γ) +
= γ + 1/γ + (

2 + 1)(cid:107)w0 − w∗(cid:107)(γ) + (cid:107)w∗(cid:107)(γ).

√

√

(64)

By combining Eqs. (63) and (64) and using the deﬁnition of C0, we have

(cid:104)
E

sup
ˆw∈Wζ,γ ( ˜wk)

{L( ˜xk, ˆy) − L( ˆx, ˜yk)}

(cid:105)

≥

≥

(cid:107)w0 − w∗(cid:107)(γ)
C0
(cid:107)w0 − w∗(cid:107)(γ)
C0

(cid:16)

E(cid:2)(cid:112)

LPMetric( ˜wk)(cid:3)(cid:17)2

(cid:16)

E(cid:2)(cid:113)

Hγdist( ˜wk, W ∗)(γ)

(cid:3)(cid:17)2

,

(65)

where the last inequality is by the deﬁnition of Hoffman constant in Eq. (9). Meanwhile, by Lemma 11, we have:

(cid:104)
E

sup
ˆw∈Wζ,γ ( ˜wk)

{L( ˜xk, ˆy) − L( ˆx, ˜yk)}

(cid:105)

≤

25 ˆLm
k

(cid:107)w0 − w∗(cid:107)2

(γ).

Now, recalling that w∗ = arg minw∈W ∗ (cid:107)w0 − w(cid:107)(γ) and using Eq. (9), we have

(cid:107)w0 − w∗(cid:107)(γ) = dist(w0, W ∗)(γ) ≤

1
Hγ

LPMetric(w0).

By combining Eqs. (65)-(67), we have

(cid:104)(cid:113)

(cid:16)

1
C0

E

Hγdist( ˜wk, W ∗)(γ)

(cid:105)(cid:17)2

≤

≤

≤

(cid:16)

1
C0
25 ˆLm
k
25 ˆLm
Hγk

E(cid:2)(cid:112)

LPMetric( ˜wk)(cid:3)(cid:17)2

dist(w0, W ∗)(γ)

LPMetric(w0).

(66)

(67)

(68)

Both bounds follow from this chain of inequalities.

Proof. By the sharpness property, (cid:107) ˜wk − w∗(cid:107)(γ) ≤ LPMetric( ˜wk)
halves the LPMetric, we have LPMetric( ˜wk) ≤ 1
the claimed bound.

Hγ

. Since one call to CLVR between restarts
2 LPMetric(w0). Combining these two inequalities leads to

Finally, we summarize the adaptive restart strategy in Algorithm 4.

32

Algorithm 4 Lazy CLVR with Adaptive Restarts
1: Input: (cid:15) > 0, x0 ∈ X , y0 ∈ Rn, γ > 0, ˆL > 0, K, (cid:98)K, m, {S1, S2, . . . , Sm}, {C1, C2, . . . , Cm}
2: t = 0, x(0)
0 , y(0)
0 )
3: repeat

0 = y0, w(0) = (x(0)

0 = x0, y(0)

4:

Run Lazy CLVR (Algorithm 2) until LPMetric(w(t+1)) ≤ 1
( ˜x(t)
K , ˜y(t)
K are the output points of Lazy CLVR
t = t + 1

K ) and ˜x(t)

K , ˜y(t)

5:
6: until LPMetric(w(t)) ≤ (cid:15)
7: Return: w(t)

2 LPMetric(w(t)) where, w(t+1) = ˜w(t)

K =

C Omitted Proofs from Section 4

C.1 DRO with f -divergence-based Ambiguity Set

Theorem 3. Let X be a compact convex set. Then the DRO problem in Eq. (10) is equivalent to the following
problem:

min
x,u,v,w,µ,q,γ

(cid:110)

γ +

ρµ1
n

+

1
n

n
(cid:88)

i=1

(cid:17)(cid:111)

µif ∗(cid:16) qi
µi

s. t. w + v −

− γ1n = 0n,

q
n
ui = biaT
i x,
µ1 = µ2 = · · · = µn,
g(ui) ≤ wi,
qi ∈ µi dom(f ∗),
vi ≥ 0, µi ≥ 0,
x ∈ X .

i ∈ [n]

i ∈ [n]

i ∈ [n]

i ∈ [n]

Proof. Since the context is clear, to simplify the notation, in the following we use P to denote Pρ,n. First, using
Sion’s minimax theorem, we have that

min
x∈X

sup
p∈P

n
(cid:88)

i=1

pig(biaT

i x) = sup
p∈P

min
x∈X

n
(cid:88)

i=1

pig(biaT

i x).

Introducing auxiliary variables w and u, the problem is further equivalent to

sup
p∈P

min
x∈X ,w,u:

ui=biaT
i x,i∈[n],
g(ui)≤wi,i∈[n]

pT w ≡

min
x∈X ,w,u:

ui=biaT
i x,i∈[n],
g(ui)≤wi,i∈[n]

pT w,

sup
p∈P

where the last equivalence is by applying minimax equality, which holds due to compactness of P [Stoer, 1963].
Hence, we can conclude that

min
x∈X

sup
p∈P

n
(cid:88)

i=1

pig(biaT

i x) =

min
x∈X ,w,u:

ui=biaT
i x,i∈[n],
g(ui)≤wi,i∈[n]

pT w.

sup
p∈P

(69)

For a ﬁxed tuple (x, w, u), using Lagrange multipliers to enforce the constraints from P, we can further

33

write

sup
p∈P

pT w = − inf
p∈P

−pT w

= − inf
p∈Rn

(cid:16)

− pT w +

sup
v≥0,γ∈R,µ≥0

= sup
p∈Rn

inf
v≥0,γ∈R,µ≥0

(cid:18)

pT w + pT v − γ

(cid:16) n
(cid:88)

i=1

(cid:16)

− pT v + γ

(cid:16) n
(cid:88)

(cid:17)

pi − 1

(cid:16)

+ µ

Df (p(cid:107)1/n) −

(cid:17)(cid:17)(cid:17)

ρ
n

i=1
(cid:17)

pi − 1

− µ

(cid:16)

Df (p(cid:107)1/n) −

(cid:17)(cid:19)
.

ρ
n

Now, using the deﬁnitions of Df and the convex conjugate of f , we have

Df (p(cid:107)1/n) =

n
(cid:88)

i=1

1
n

f (npi) =

n
(cid:88)

i=1

1
n

sup
νi∈dom(f ∗)

(cid:0)npiνi − f ∗(νi)(cid:1).

(70)

As a result, we have

(cid:16)

−µ

inf
µ≥0

Df (p(cid:107)1/n) −

(cid:17)

ρ
n

= inf
µ≥0

−

µ
n

(cid:16) n
(cid:88)

i=1

=

inf
µ≥0,νi∈dom(f ∗),i∈[n]

sup
νi∈dom(f ∗),i∈[n]
(cid:16) n
(cid:88)

−

µ
n

i=1

(cid:17)
(cid:0)npiνi − f ∗(νi)(cid:1) − ρ

(cid:17)
(cid:0)npiνi − f ∗(νi)(cid:1) − ρ

=

inf
µ≥0,qi∈µ dom(f ∗),i∈[n]

1
n

n
(cid:88)

i=1

(cid:0) − piqi + µf ∗(cid:0) qi
nµ

(cid:1)(cid:1) +

µρ
n

=

inf
µ1=µ2=···=µn≥0,
qi∈µi dom(f ∗),i∈[n]

1
n

n
(cid:88)

i=1

(cid:0) − piqi + µif ∗(cid:0) qi
nµi

(cid:1)(cid:1) +

µ1ρ
n

,

where the ﬁrst equality is by Eq. (70), the third one is by the variable substitution qi = nµνi, and the last one
(cid:1)(i ∈ [n]) is a perspective function of f ∗
is by introducing µ1, µ2, . . . , µn to replace µ. Then each nµif ∗(cid:0) qi
nµi
[Boyd and Vandenberghe, 2004], which is jointly convex w.r.t. (µi, qi). Hence, we can conclude that

pT w

sup
p∈P

= sup
p∈Rn

inf
γ∈R,v≥0,
µ1=µ2=···=µn≥0,
qi∈µi dom(f ∗),i∈[n]

=

inf
γ∈R,v≥0,
µ1=µ2=···=µn≥0,
qi∈µi dom(f ∗),i∈[n]

sup
p∈Rn

(cid:18)

pT w + pT v − γ

(cid:18)

pT w + pT v − γ

(cid:16) n
(cid:88)

i=1

(cid:16) n
(cid:88)

i=1

(cid:17)

pi − 1

+

(cid:17)

pi − 1

+

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:0) − piqi + µif ∗(cid:0) qi
nµi

(cid:1)(cid:1) +

(cid:19)

µ1ρ
n

(cid:0) − piqi + µif ∗(cid:0) qi
nµi

(cid:1)(cid:1) +

(cid:19)

,

µ1ρ
n

where the last line is by strong duality. Thus, combining with Eq. (69), we conclude that the original DRO

34

problem with f -divergence based ambiguity set is equivalent to the following problem:

min
x,u,v,w,µ,q,γ

max
p∈Rn

(cid:110)

γ +

ρµ1
n

+

1
n

n
(cid:88)

i=1

µif ∗(cid:16) qi
µi

(cid:17)

+ pT (cid:16)

w + v −

(cid:17)(cid:111)

− γ1n

q
n

s. t. ui = biaT

i x,
µ1 = µ2 = · · · = µn,
g(ui) ≤ wi,
qi ∈ µi dom(f ∗),
vi ≥ 0, µi ≥ 0,
x ∈ X .

i ∈ [n]

i ∈ [n]

i ∈ [n]

i ∈ [n]

Finally, noticing that the maximization problem over p ∈ Rn enforces the equality constraint w +v − q
0n, we obtain

n −γ1n =

min
x,u,v,µ,q,γ

(cid:110)

γ +

ρµ1
n

+

1
n

n
(cid:88)

i=1

(cid:1)(cid:111)

µif ∗(cid:0) qi
µi

s. t. w + v −

− γ1n = 0n,

q
n
ui = biaT
i x,
µ1 = µ2 = · · · = µn,
g(ui) ≤ wi,
qi ∈ µi dom(f ∗),
vi ≥ 0, µi ≥ 0,
x ∈ X ,

i ∈ [n]

i ∈ [n]

i ∈ [n]

i ∈ [n]

as claimed.

Example: Conditional Value at Risk (CVaR) with Hinge Loss. As a speciﬁc example of an application of
Theorem 3, we consider CVaR at level α ∈ (0, 1), which leads to the optimization problem:

min
x∈Rd

sup
p≥0,1T
pi≤ 1

n p=1
αn (i∈[d])

n
(cid:88)

i=1

pig(biaT

i x),

(71)

where g(ui) = max{0, 1 − ui} is the hinge loss. Here the ambiguity set constraint reduces to simple bounds
pi ≤ 1
αn for i ∈ [n], so the reformulation based on the convex perspective function can be avoided altogether
and replaced by simple Lagrange multipliers for this linear constraint. In particular, in the proof of Theorem 3,
we can write

sup
p∈P

pT w = sup
p∈Rn

inf
v≥0,γ∈R,µ≥0

(cid:18)

pT w + pT v − γ

(cid:16) n
(cid:88)

i=1

pi − 1

(cid:17)

−

n
(cid:88)

i=1

(cid:16)

µi

pi −

(cid:17)(cid:19)

1
αn

(cid:18)

= sup
p∈Rn

inf
v≥0,γ∈R,µ≥0

=

inf
v≥0,γ∈R,µ≥0

sup
p∈Rn

(cid:17)(cid:19)

(cid:17)(cid:19)
.

µT 1n

1
αn

1
αn

γ + pT (w + v − γ1n − µ) +

µT 1n

(cid:18)

γ + pT (w + v − γ1n − µ) +

35

Following the argument from the proof of Theorem 3, and expressing wi ≥ g(ui) equivalently as the pair of
constraints wi ≥ 0 and wi ≥ 1 − ui, the problem reduces to

min
x,u,v,w,µ,γ

(cid:110)

γ +

(cid:111)

µT 1n

1
αn

s. t. w + v − γ1n − µ = 0n,
ui = biaT
i x,
wi ≥ 0, wi ≥ 1 − ui,
vi ≥ 0, µi ≥ 0,

i ∈ [n],

i ∈ [n],

i ∈ [n],

(72)

which is a linear program. To write it in the standard form, we further introduce slack variables s ∈ Rn, s ≥ 0,
to replace inequality constraints wi ≥ 1 − ui by si − ui − wi = −1. For implementation purposes, we deﬁne X
to be the set of simple non-negativity constraints (wi ≥ 0, si ≥ 0,, vi ≥ 0, µi, ≥ 0, ∀i ∈ [n]) from Eq. (72). The
problem then becomes

min
x,u,v,w,µ,s,γ

(cid:110)

γ +

(cid:111)

µT 1n

1
αn

s. t. w + v − γ1n − µ = 0n,

i x = 0,

ui − biaT
s − u − w = −1n,
w ≥ 0n, v ≥ 0n, µ ≥ 0n, s ≥ 0n.

i ∈ [n],

C.2 DRO with Wasserstein Metric-based Ambiguity Set

Deﬁnition 1. Let µ and ν be two probability distributions supported on Θ = Rd × {1, −1} and let Π(µ, ν)
denote the set of all joint distributions between µ and ν. Then the Wasserstein metric between µ and ν is deﬁned
by

W (µ, ν) = inf

π∈Π(µ,ν)

(cid:90)

Θ×Θ

ζ(ξ, ξ(cid:48))π(dξ, dξ(cid:48)),

(73)

where ξ, ξ(cid:48) ∈ Θ and ζ(·, ·) : Θ × Θ → R+ is a convex cost function deﬁned by

ζ((a, b), (a(cid:48), b(cid:48))) = (cid:107)a − a(cid:48)(cid:107) + κ|b − b(cid:48)|,

where (cid:107) · (cid:107) denotes a general norm and κ > 0 is used to balance the feature mismatch and label uncertainty.
Let Q = 1
i=1 δ(ai,bi), where δ(ai,bi) is the Dirac Delta function (or a point mass) at point (ai, bi). Then, the
n
Wasserstein metric based ambiguity set is deﬁned as

(cid:80)n

Assumption 5. R (cid:51) M = supθ∈dom(g∗) |θ|.

Pρ,κ =

(cid:110)

P : W (P, Q) ≤ ρ

(cid:111)
.

(74)

To obtain our main result in Theorem 12, we ﬁrst show the following auxiliary lemma that is subsequently
used in the proof of Theorem 4. A similar result for the special case of a logistic loss function can be found in
Shaﬁeezadeh Abadeh et al. [2015].

Lemma 12. Let (a(cid:48), b(cid:48)) ∈ Rd × {1, −1} be a given pair of data sample and label. Then, for every λ > 0, we
have

g(b(cid:48)aT w) − λ(cid:107)a − a(cid:48)(cid:107) =

sup
a∈Rd

(cid:40)

g(b(cid:48)a(cid:48)T w),
+∞,

if (cid:107)w(cid:107)∗ ≤ λ/M
otherwise

.

(75)

36

Proof. Since g is assumed to be proper, convex, and lower semicontinuous, by the Fenchel-Moreau theorem, it
is equal to its biconjugate. Applying this property, we have

{g(b(cid:48)aT w) − λ(cid:107)a − a(cid:48)(cid:107)}

(cid:8)θb(cid:48)aT w − g∗(θ) − λ(cid:107)a − a(cid:48)(cid:107)(cid:9)

(cid:8)θb(cid:48)(a − a(cid:48))T w + θb(cid:48)(a(cid:48))T w − g∗(θ) − λ(cid:107)a − a(cid:48)(cid:107).(cid:9)

sup
a∈Rd
sup
a∈Rd,
θ∈dom(g∗)

sup
a∈Rd,
θ∈dom(g∗)

=

=

Applying the change of variable v := a − a(cid:48), we further have

{g(b(cid:48)aT w) − λ(cid:107)a − a(cid:48)(cid:107)}

(cid:8)θb(cid:48)vT w + θb(cid:48)(a(cid:48))T w − g∗(θ) − λ(cid:107)v(cid:107)(cid:9)

sup
a∈Rd
sup
v∈Rd,
θ∈dom(g∗)

(cid:8)1{(cid:107)θb(cid:48)w(cid:107)∗≤λ} + θb(cid:48)(a(cid:48))T w − g∗(θ)(cid:9)

g(b(cid:48)(a(cid:48))T w),
+∞,

if supθ∈dom(g∗) (cid:107)θb(cid:48)w(cid:107)∗ ≤ λ
otherwise

,

=

=

=

sup
θ∈dom(g∗)
(cid:40)

where the second equality is by the convex conjugate of a norm (cid:107) · (cid:107) being equal to the convex indicator of the
unit ball w.r.t. the dual norm (cid:107) · (cid:107)∗. Finally, it remains to use that, by assumption, supθ∈dom(g∗) |θ| = M.

Then following Shaﬁeezadeh Abadeh et al. [2015, Theorem 1], we provide reformulations of the problem

from Eq. (11) that can be addressed by computationally efﬁcient solvers.

Theorem 4. The optimization problem from Eq. (11) is equivalent to:

min
w,λ,u,v,s,t

ρλ +

n
(cid:88)

si

1
n

i=1
i w,

s. t. ui = biaT
vi = −ui,
ti = 2κλ + si,
g(ui) ≤ si,
g(vi) ≤ ti,
(cid:107)w(cid:107)∗ ≤ λ/M.

i ∈ [n],

i ∈ [n],

i ∈ [n],

i ∈ [n],

i ∈ [n],

(76)

Proof. Let z = (a, b) ∈ Θ := Rd × {1, −1} and let hw(z) := g(baT w). Then by the deﬁnition of the
Wasserstein metric,

EP[g(baT w)] =

sup
P∈Pρ,κ

(cid:40)

supπ∈Π(P,ˆPn)
s. t.

(cid:82)
Θ hw(z)π(dz, Θ)
(cid:82)
Θ×Θ ζ(z, z(cid:48))π(dz, dz(cid:48)) ≤ ρ.

(77)

Assume that the conditional distribution of z given z(cid:48) = (ai, bi) is Pi, for all i ∈ [n]. Then, based on the
deﬁnition of Pn = 1
n

i=1 δ(ai,bi), we have

(cid:80)n

δ(ai,bi)Pi(dz).

(78)

π(dz, dz(cid:48)) =

1
n

n
(cid:88)

i=1

37

As a result, the problem from Eq. (77) is equivalent to

EP[g(baT w)] =

sup
P∈Pρ,κ




supPi
s. t.



(cid:80)n
(cid:80)n

i=1

i=1

(cid:82)
(cid:82)

Θ hw(z)Pi(dz)
Θ ζ(z, z(cid:48))Pi(dz) ≤ ρ

Pi(dz) = 1.

1
n
1
n
(cid:82)

Θ

(79)

Then substituting in z = (a, b), using that the domain of y is {1, −1}, and decomposing Pi into unnormalized
measures P±1(da) = Pi(da, {b = ±1}) supported on Rd, the RHS of Eq. (79) can be simpliﬁed to

n
(cid:88)

(cid:90)

Rd

(cid:90)

Rd

i=1
n
(cid:88)

i=1

(cid:0)hw(a, 1)Pi

1(da) + hw(a, −1)Pi

−1(da)(cid:1)

(cid:0)ζ((a, 1), (ai, bi))Pi

1(da) + ζ((a, −1), (ai, bi))Pi

−1(da)(cid:1) ≤ ρ

sup
Pi
±

s. t.

1
n

1
n
(cid:90)

(cid:0)Pi

1(da) + Pi

−1(da)(cid:1) = 1.

Rd

With the deﬁnition of the cost function ζ((a, b), (a(cid:48), b(cid:48))) = (cid:107)a − a(cid:48)(cid:107) + κ|b − b(cid:48)|, it follows that

1
n

1
n

=

n
(cid:88)

(cid:90)

i=1
(cid:90)

Rd

(cid:88)

Rd
bi=1
(cid:90)

+

1
n

(cid:88)

Rd

bi=−1

ζ((a, 1), (ai, bi))Pi

1(da) + ζ((a, −1), (ai, bi))Pi

−1(da)

[(cid:107)a − ai(cid:107)Pi

1(da) + (cid:107)a − ai(cid:107)Pi

−1(da) + 2κPi

−1(da)]

[(cid:107)a − ai(cid:107)Pi

−1(da) + (cid:107)a − ai(cid:107)Pi

1(da) + 2κPi

1(da)]

=

2κ
n

(cid:90)

(cid:16) (cid:88)

Rd

bi=1

Pi

−1(da) +

(cid:88)

Pi

(cid:17)
1(da)

+

bi=−1

1
n

(cid:90)

n
(cid:88)

Rd

i=1

Thus, we have

(cid:107)a − ai(cid:107)(Pi

−1(da) + Pi

1(da)).

(80)

EP[g(baT w)] =

sup
P∈Pρ,κ

1
n

sup
Pi

±1

n
(cid:88)

(cid:90)

i=1

s. t.

2κ
n

(cid:90)

Rd

Rd




(cid:0)hw(a, 1)Pi

1(da) + hw(a, −1)Pi

−1(da)(cid:1)



Pi

1(da)



(cid:88)

bi=−1

(cid:88)

Pi

−1(da) +

bi=1
(cid:90)

+

1
n

n
(cid:88)

Rd

i=1

(cid:107)a − ai(cid:107)(Pi

−1(da) + Pi

1(da)) ≤ ρ

(cid:90)

Rd

(cid:0)Pi

1(da) + Pi

−1(da)(cid:1) = 1.

The above problem w.r.t. Pi
Shapiro [2001, Proposition 3.4], we get the following equivalent dual formulation:

±1 is an inﬁnite-dimensional linear program with a ﬁnite number of constraints. By

EP[g(baT w)] =

sup
P∈Pρ,κ

hw(a, 1) − λ(cid:107)a − ai(cid:107) − λκ(1 − bi) ≤ si,

i ∈ [n]

hw(a, −1) − λ(cid:107)a − ai(cid:107) − λκ(1 + bi) ≤ si, i ∈ [n]











min
λ,si

ρλ +

1
n

n
(cid:88)

i=1

si

s. t. sup
a∈Rd
sup
a∈Rd
λ ≥ 0.

38

Then, recalling that hw(a, ±1) is short for g(±aT w), by Lemma 12, we have

hw(a, ±1) − λ(cid:107)a − ai(cid:107) =

sup
a∈Rd

Finally, the resulting reformulation is

(cid:40)

i w),

g(±aT
+∞,

min
w,λ,s

s. t.

ρλ +

n
(cid:88)

si

1
n

i=1
g(biaT
i w) ≤ si,
i w) − 2λκ ≤ si,
(cid:107)θw(cid:107)∗ ≤ λ.

g(−biaT
sup
θ∈dom(g∗)

if supθ∈dom(g∗) (cid:107)θw(cid:107)∗ ≤ λ
otherwise.

i ∈ [n],

i ∈ [n],

Finally, recalling that, by assumption, supθ∈dom(g∗) |θ| = M, it follows that the constraint supθ∈dom(g∗) (cid:107)θw(cid:107)∗ ≤
λ is equivalent to (cid:107)w(cid:107)∗ ≤ λ/M. Meanwhile, by introducing ui = biaT
i w, vi = −ui(i ∈ [n]) and si, ti(i ∈ [n],
we obtain Theorem 4.

In Theorem 4, when we assume that the conic constraints g(u) ≤ s and (cid:107)w(cid:107)∗ ≤ λ/M in Eq. (76) admit
efﬁcient proximal operators, we can formulate this problem as (PD-GLP) and apply CLVR. The resulting
complexity bounds are similar to those discussed above for the f -divergence formulation.

39

