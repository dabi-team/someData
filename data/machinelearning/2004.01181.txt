GraphChallenge.org
Sparse Deep Neural Network Performance

Jeremy Kepner1,2,3, Simon Alford2, Vijay Gadepally1,2,
Michael Jones1, Lauren Milechin4, Albert Reuther1, Ryan Robinett3, Sid Samsi1
1MIT Lincoln Laboratory Supercomputing Center, 2MIT Computer Science & AI Laboratory,
3MIT Mathematics Department, 4MIT Dept. of Earth, Atmospheric, & Planetary Sciences

0
2
0
2

r
p
A
6

]

G
L
.
s
c
[

2
v
1
8
1
1
0
.
4
0
0
2
:
v
i
X
r
a

Abstract—The MIT/IEEE/Amazon GraphChallenge.org en-
courages community approaches to developing new solutions for
analyzing graphs and sparse data. Sparse AI analytics present
unique scalability difﬁculties. The Sparse Deep Neural Network
(DNN) Challenge draws upon prior challenges from machine
learning, high performance computing, and visual analytics to
create a challenge that is reﬂective of emerging sparse AI systems.
The sparse DNN challenge is based on a mathematically well-
deﬁned DNN inference computation and can be implemented
in any programming environment. In 2019 several sparse DNN
challenge submissions were received from a wide range of authors
and organizations. This paper presents a performance analysis of
the best performers of these submissions. These submissions show
that their state-of-the-art sparse DNN execution time, TDNN, is a
strong function of the number of DNN operations performed,
Nop. The sparse DNN challenge provides a clear picture of
current sparse DNN systems and underscores the need for new
innovations to achieve high performance on very large sparse
DNNs.

I. INTRODUCTION

MIT/IEEE/Amazon GraphChallenge.org encourages com-
munity approaches to developing new solutions for analyzing
graphs and sparse data. GraphChallenge.org provides a well-
deﬁned community venue for stimulating research and high-
lighting innovations in graph and sparse data analysis software,
hardware, algorithms, and systems. The target audiences for
these challenges are individuals or teams that seek to highlight
their contributions to graph and sparse data analysis software,
hardware, algorithms, and/or systems.

As research in artiﬁcial neural networks progresses, the sizes
of state-of-the-art deep neural network (DNN) architectures
put increasing strain on the hardware needed to implement
them [1], [2]. In the interest of reduced storage and runtime
costs, much research over the past decade has focused on
the sparsiﬁcation of artiﬁcial neural networks [3]–[13]. In
the listed resources alone, the methodology of sparsiﬁcation
includes Hessian-based pruning [3], [4], Hebbian pruning [5],
matrix decomposition [9], and graph techniques [10]–[13]. The
sparse DNN challenge seeks to highlight innovations that are
applicable to emerging sparse AI and machine learning [14].

This material is based upon work supported by the Assistant Secretary of
Defense for Research and Engineering under Air Force Contract No. FA8702-
15-D-0001 and National Science Foundation CCF-1533644. Any opinions,
ﬁndings, conclusions or recommendations expressed in this material are those
of the author(s) and do not necessarily reﬂect the views of the Assistant
Secretary of Defense for Research and Engineering or the National Science
Foundation.

Challenges such as YOHO [15], MNIST [16], HPC Chal-
lenge [17], ImageNet [18] and VAST [19], [20] have played
important roles in driving progress in ﬁelds as diverse as
machine learning, high performance computing and visual
analytics. YOHO is the Linguistic Data Consortium database
for voice veriﬁcation systems and has been a critical enabler
of speech research. The MNIST database of handwritten
letters has been a bedrock of the computer vision research
community for two decades. HPC Challenge has been used by
the supercomputing community to benchmark and acceptance
test the largest systems in the world as well as stimulate
research on the new parallel programing environments. Im-
ageNet populated an image dataset according to the WordNet
hierarchy consisting of over 100,000 meaningful concepts
(called synonym sets or synsets) [18] with an average of 1000
images per synset and has become a critical enabler of vision
research. The VAST Challenge is an annual visual analytics
challenge that has been held every year since 2006; each
year, VAST offers a new topic and submissions are processed
like conference papers. The sparse DNN challenge seeks to
draw on the best of these challenges, but particularly the
VAST Challenge in order to highlight innovations across the
algorithms, software, hardware, and systems spectrum.

The focus on graph analytics allows the sparse DNN
challenge to also draw upon signiﬁcant work from the graph
benchmarking community. Scale is an important driver of
the Graph Challenge and graphs with billions to trillions of
edges are of keen interest. The Graph Challenge is designed
to work on arbitrary graphs drawn from both real-world
data sets and simulated data sets. Examples of real-world
data sets include the Stanford Large Network Dataset Collec-
tion (see http://snap.stanford.edu/data), the AWS Public Data
Sets (see aws.amazon.com/public-data-sets), and the Yahoo!
Webscope Datasets (see webscope.sandbox.yahoo.com). These
real-world data sets cover a wide range of applications and
data sizes. While real-world data sets have many contextual
beneﬁts, synthetic data sets allow the largest possible graphs to
be readily generated. Examples of synthetic data sets include
Graph500, Block Two-level Erdos-Renyi graph model (BTER)
[21], Kronecker Graphs [10], [22], [23], and Perfect Power
Law graphs [24]–[26]. The focus of the Graph Challenge is on
graph analytics. While parsing and formatting complex graph
data are necessary in any graph analysis system, these data sets
are made available to the community in a variety of pre-parsed

 
 
 
 
 
 
formats to minimize the amount of parsing and formatting
required by Graph Challenge participants. The public data
are available in a variety of formats, such as linked list, tab
separated, and labeled/unlabeled.

The Graph Challenge consists of a pre-challenge and three

full challenges

• Pre-challenge: PageRank pipeline [27]
• Static graph challenge: subgraph isomorphism [28]
• Streaming graph challenge: stochastic block partition [29]
• Sparse DNN challenge [14]

The static graph challenge is further broken down into triangle
counting and k-truss. The sparse DNN challenge is the focus of
this paper. The organization of this paper is as follow. First,
a recap of the sparse DNN challenge is provided. Next, an
overview is presented of the 2019 submissions. The core of
the paper is the section on the analysis of the submissions
that performed sparse DNN challenge. Based on this analysis,
these results are synthesized to provide a picture of the current
state-of-the-art.

II. DEEP NEURAL NETWORKS
Machine learning has been the foundation of artiﬁcial
intelligence since its inception [30]–[37]. Standard machine
learning applications include speech recognition [32], com-
puter vision [33], and even board games [34], [38].

Fig. 1. Typical network elements i and j showing connection weights w
(reproduced from [31])

Drawing inspiration from biological neurons to implement
machine learning was the topic of the ﬁrst paper presented
at the ﬁrst machine learning conference in 1955 [30], [31]
(see Figure 1). It was recognized very early on in the ﬁeld
that direct computational
training of neural networks was
computationally unfeasible with the computers that were avail-
able at that time [36]. The many-fold improvement in neural
network computation and theory has made it possible to create
neural networks capable of better-than-human performance in
a variety of domains [39]–[42]. The production of validated
data sets [43]–[45] and the power of graphic processing units
(GPUs) [46]–[49] have allowed the effective training of deep
neural networks (DNNs) with 100,000s of input features, N ,
and 100s of layers, L, that are capable of choosing from among
100,000s categories, M (see Figure 2).

The impressive performance of large DNNs provides mo-
tivation to explore even larger networks. However, increas-
ing N , L, and M each by a factor 10 results in a 1000-
fold increase in the memory required for a DNN. Because

Fig. 2. Four layer (L = 4) deep neural network architecture for categorizing
images. The input features y0 of an image are passed through a series of
network layers W(cid:96)=0,1,2,3, with bias terms b(cid:96)=0,1,2,3, that produce scores
for categories yL=4. (Figure adapted from [50])

of these memory constraints, trade-offs are currently being
made in terms of precision and accuracy to save storage and
computation [11], [51]–[53]. Thus, there is signiﬁcant interest
in exploring the effectiveness of sparse DNN representations
where many of the weight values are zero. As a comparison,
the human brain has approximately 86 billion neurons and
150 trillion synapses [54]. Its graph representation would
have approximately 2,000 edges per node, or a density of
2 × 103/86 × 109 = 0.000002%.

If a large fraction of the DNN weights can be set to zero,
storage and computation costs can be reduced proportionately
[6], [55]. The interest
limited to
their computational advantages. There has also been extensive
theoretical work exploring the potential neuromorphic and
algorithmic beneﬁts of sparsity [8], [56]–[59].

in sparse DNNs is not

The primary mathematical operation performed by a DNN
network is the inference, or forward propagation, step. Infer-
ence is executed repeatedly during training to determine both
the weight matrix W(cid:96) and the bias vectors b(cid:96) of the DNN.
The inference computation shown in Figure 2 is given by

y(cid:96)+1 = h(y(cid:96)W(cid:96) + b(cid:96))

where h() is a nonlinear function applied to each element
of the vector. The sparse DNN challenge uses the standard
graph community convention whereby W(i, j) (cid:54)= 0 implies a
connection between neuron i and neuron j. In this convention
y(cid:96) are row vectors and left matrix multiply is used to progress
through the network. Standard AI deﬁnitions can be used
by transposing all matrices and multiplying on the right. A
commonly used function is the rectiﬁed linear unit (ReLU)
given by

h(y) = max(y, 0)

86 1955 WESTERN JOINT COMPUTER CONFERENCE Generalization of Pattern Recognition in a Self-Organizing System* W. A. CLARKf AND B. G. FARLEYf Summary—A self-organizing system reported upon earlier is briefly described. Two further experiments to determine its proper-ties have been carried out. The first demonstrates that self-organiza-tion still takes place even if the input patterns are subjected to con-siderable random variation. The second experiment indicates that, after organization with the usual fixed patterns, the system classifies other input patterns statistically according to a simple preponderance criterion. Significance of this result as a generalization in pattern recognition is discussed. Some remarks are made on methods of simulation of such systems and their relation to computer design. DESCRIPTION OF SELF-ORGANIZING SYSTEM IN A PREVIOUS paper1 the authors described a sys-tem which organized itself from an initially random condition to a state in which discrimination of two different input patterns2 was accomplished. The be-havior of the system was simulated by means of a digital computer—the Memory Test Computer of Lincoln Laboratory. Briefly, the self-organizing system was composed of two parts. The first part received input patterns and transformed them into outputs, and the second part acted upon parameters of the first so as to modify the input-output transformation according to certain fixed criteria. These parts were termed the transformation and the modifier, respectively. The transformation is a randomly interconnected network of nonlinear elements, each element having a definite threshold for incoming excitation, below which no action occurs, and above which the element "fires." When an element fires, its threshold immediately rises effectively to infinity (it cannot be fired), and then, after a short fixed delay, falls exponentially back toward its quiescent value. Furthermore, at some short time after firing, an element transmits excitation to all other eler ments to which it is connected. The effectiveness of the excitation thus transmitted to a succeeding element is determined by a property of the particular connection known as its "weight." In general, there will be several incoming connections at any element, each having its individual weight as shown in Fig. 1. At the instant of transmission (which is the time of impulse arrival at the succeeding element), the appropriate weight is added to any excitation already present at the succeeding cell. * The research reported in this document was supported jointly by the Army, the Navy, and the Air Force under contract with the Massachusetts Institute of Technology. f Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, Mass. 1 B. G. Farley and W. A. Clark, "Simulation of self-organizing systems by digital computer," Trans. IRE, vol. PGIT-4, pp. 76-84; September, 1954. 2 In this paper, the word "pattern" is synonymous with "con-figuration." Thereafter the excitation decays exponentially to zero. If at any time this excitation exceeds the threshold of the succeeding element, the element performs its firing cycle and transmits its own excitations. Fig. 1—Typical network elements i and j showing connection weights w. A network such as the one described is suggestive of networks of the nerve cells, or neurons, of physiology, but since the details of neuron interaction are as yet un-certain, it cannot even be said that the networks are identical without some simplifications which are present. In the work mentioned, the network was activated and an output obtained in the following way. The net was divided arbitrarily into two groups, designated as input and output groups. The output group was further subdivided in two, and an output was defined at any instant by the difference in the number of elements fired in the two subgroups during the instant. This arrange-ment might be termed a push-pull output. The input group was also subdivided into two sub-groups, and two fixed input patterns were provided, usually designated as px and p2. Input pi consisted in adding a large excitation into all the input elements of one subgroup simultaneously and repetitively at a con-stant period, but doing nothing to the other subgroup. Input p2 was just the reverse. In this way output ac-tivity characteristic of the input pattern was obtained. It was now desired to provide a modifier acting upon parameters of the net so as to gradually reorganize it to obtain output activity of a previously specified charac-teristic, namely, that patterns pi and pi would always drive the output in previously specified directions. In our experiments, pi was made to drive the output in a negative direction, that is to say, pi causes more firing to take place on the average in the first output subgroup than in the second. In the case of p%, the situation was exactly reversed. This desired organization of the net was accomplished by means of varying the weights mentioned above in the following way. Examination is made of the change in output at every instant. If a change in a favorable direc-tion occurs (e.g. negative change in case pi is the input InputFeaturesOutputCategoriesEdgesObject PartsObjectsy0 W0b0 W1b1 W2b2 W3b3 y2 y3 y4 y1 Hidden LayersLayers
120
480
1920

Neurons
1024
3,932,160
15,728,640
62,914,560

Neurons
4096
15,728,640
62,914,560
251,658,240

Neurons
16384
62,914,560
251,658,240
1,006,632,960

Neurons
65536
251,658,240
1,006,632,960
4,026,531,840

TABLE I
TOTAL NUMBER OF CONNECTIONS = 32X(LAYERS)X(NEURONS) FOR
DIFFERENT LARGE SPARSE DNNS USED IN THE SPARSE DNN
CHALLENGE.

which sets values less that 0 to 0 and leaves other values
unchanged. For the Sparse DNN challenge, h() also has an
upper limit set to 32. When training a DNN, or performing
inference on many different inputs, it is usually necessary to
compute multiple y(cid:96) vectors at once in a batch that can be
denoted as the matrix Y(cid:96). In matrix form, the inference step
becomes

Y(cid:96)+1 = h(Y(cid:96)W(cid:96) + B(cid:96))

where B(cid:96) is a replication of b(cid:96) along columns given by

B(cid:96) = b(cid:96)|Y(cid:96)1|0

and 1 is a column array of 1’s, and | |0 is the zero norm.

III. NEURAL NETWORK DATA

Scale is an important driver of the Graph Challenge and
graphs with billions to trillions of edges are of keen interest.
Real sparse neural networks of this size are difﬁcult to obtain
from real data. Until such data is available, a reasonable ﬁrst
step is to simulate data with the desired network properties
with an emphasis on the difﬁcult part of the problem, in
this case: large sparse DNNs. The RadiX-Net synthetic sparse
DNN generator is used [60] to efﬁciently generate a wide
range of pre-determined DNNs all with 32 connections per
neuron. RadiX-Net produces DNNs with a number of de-
sirable properties, such as equal number of paths between
all inputs, outputs, and intermediate layers. The RadiX-Net
DNN generation algorithm uses mixed radices to generate
DNNs of speciﬁed connectedness which are then expanded
via Kronecker products into larger DNNs. The number of
connections (see Table I) in the resulting large sparse DNNs
are computed using the formula

Nc = 32 × L × N

IV. INPUT DATA SET

Executing the Sparse DNN Challenge requires input data
or feature vectors Y0. MNIST (Modiﬁed National Institute of
Standards and Technology) is a large database of handwritten
digits that is widely used for training and testing DNN image
processing systems [16]. MNIST consists of 60,000 28×28
pixel images. The Sparse DNN Challenge uses interpolated
sparse versions of this entire corpus as input (Figure 3). Each
28×28 pixel image is resized to 32×32 (1024 neurons), 64×64
(4096 neurons), 128×128 (16384 neurons), and 256×256
(65536 neurons). The resized images are thresholded so that
all values are either 0 or 1. The images are ﬂattened into a

single row to form a feature vector. The non-zero values are
written as triples to a .tsv ﬁle where each row corresponds to
a different image, each column is the non-zero pixel location
and the value is 1.

Fig. 3. MNIST data set consists of 60,000 handwritten digits [16]. (top)
Original 28×28 pixel images of four MNIST images. (bottom) 256×256
resampled thresholded versions of the same images.

V. SPARSE DNN CHALLENGE

The core of the Sparse DNN Challenge is timing DNN
inference using the provided DNNs on the provided MNIST
input data and verifying the output with the provided truth
categories. The complete process for performing the challenge
consists of the following steps

• Download from GraphChallenge.org: DNN weight ma-
trices W(cid:96), sparse MNIST input data Y0, and truth
categories

• Load a DNN and its corresponding input
• Create and set the appropriate sized bias vectors b(cid:96) from

the table

• Timed: Evaluate the DNN equation for all layers

Y(cid:96)+1 = h(Y(cid:96)W(cid:96) + B(cid:96))

• Timed: Identify the categories (rows) in ﬁnal matrix with

entries > 0

• Compare computed categories with truth categories to

check correctness

• Compute rate for the DNN: (# inputs) × (# connections)

/ time

• Report time and rate for each DNN measured
Submissions to the Sparse DNN Challenge are evaluated
on the overall innovations highlighted by the implementation
and two metrics: correctness and performance. Correctness
is evaluated by comparing the reported categories with the
ground truth categories provided. The performance of the
algorithm implementation is reported in terms of the following
metrics:

• Total number of non-zero connections in the given DNN:

This measures the amount of data processed

• Execution time: Total time required to perform DNN

inference.

• Rate: Measures the throughput of the implementation as
the ratio of the number of inputs (e.g., number of MNIST

images) times the number of connections in the DNN
divided by the execution time.

• Processor: Number and type of processors used in the

computation.

VI. COMMUNITY SUBMISSIONS

Graph Challenge has received a wide range of submissions
across all its various challenges that have included hundreds of
authors from over ﬁfty organizations. In 2019, twenty submis-
sions across all the challenges were selected for publication
[61]–[80]. Six of the published submissions provided sparse
DNN performance data for analysis [61]–[66].

The submissions implemented the sparse DNN challenge in
a comparable manner, resulting in over 60 distinct measure-
ments of sparse DNN execution time, TDNN. The number of
connections, Nc, in the graph describes the overall size of the
graph. The rate of operations processed in DNN inference is
given by

Rate = Nop/TDNN

where Nop = Nin × Nc = 60, 000 × 32 × L × N . Analyzing
and combining all the performance data from the submissions
can be done by ﬁtting a model to each submission and then
comparing the models. For each submission, TDNN vs Nop is
plotted on a log-log scale from which a model can be ﬁt to the
data by estimating the parameters N1 and β in the formula

TDNN = (Nop/N1)β

where N1 is the number operations that can be processed in 1
second. The sparse DNN execution time vs number of connec-
tions and corresponding model ﬁts are shown in Figures 4 and
5. The model ﬁts illustrate the strong dependence of TDNN on
Nop.

VII. PERFORMANCE ANALYSIS

The normalized parameters N1 and β, along with the largest
values of Nc, are shown in Table II for each submission.
Submissions with larger Nc, larger N1, and smaller β perform
best. The current state-of-the-art can be seen by plotting all the
model ﬁts TDNN together (see Figures 6 and 7). Combined,
these suggest that typical performance model for 2019 is

TDNN ≈ Nop/1011

Fig. 4. 2019 Champions and Innovation Award. Sparse DNN execution time
vs number of operations and corresponding model ﬁts for Bisson-Nvidia-2019
[61], Davis-TAMU-2019 [62], and Ellis-Sandia-2019 [63].

TABLE II
2019 Sparse DNN time model ﬁt coefﬁcients for TDNN = (Ne/N1)β for
large values of Ne.

Submission
Bisson-Nvidia-2019
Davis-TAMU-2019
Ellis-Sandia-2019

Ref
[61]
[62]
[63]
[64] Wang-UCDavis-2019
[65] Wang-PingAn-2019
[66] Mofrad-UPitt-2019

max Nc
4.0 × 109
4.0 × 109
4.0 × 109
1.0 × 109
1.0 × 109
4.0 × 109

N1
1 × 1013
1 × 1011
1.5 × 1011
2 × 1011
2 × 1011
5 × 1010

β
4/5
1
1
1
1.1
1

with the exception of [61], which produced the higher perfor-
mance given by

VIII. CONCLUSION

TDNN ≈ (Nop/1013)4/5

Given that this is the ﬁrst year of the sparse DNN challenge,
it would be expected that subsequent submissions will aim to
approach the higher performance demonstrated by [61].

The MIT/IEEE/Amazon GraphChallenge.org encourages
community approaches to developing new solutions for ana-
lyzing graphs and sparse data. Sparse AI analytics presents
unique scalability difﬁculties. The machine learning, high
performance computing, and visual analytics communities

10111012101310141015Nop, DNN Operations Performed10-1100101TDNN, DNN Time (seconds)Bisson-Nvidia-2019-DNNTDNN = (Ne/1013)4/510111012101310141015Nop, DNN Operations Performed100101102103104TDNN, DNN Time (seconds)Davis-TAMU-2019-DNNTDNN = Ne/101110111012101310141015Nop, DNN Operations Performed100101102103104TDNN, DNN Time (seconds)Ellis-Sandia-2019-DNNTDNN = Ne/1.5x1011Fig. 6. Model ﬁts of sparse DNN execution time vs number DNN operations
for selected Graph Challenge 2019 sparse DNN submissions.

Fig. 5. Graph Challenge 2019 Student Innovation Award, Finalist, and
Honorable Mention. Sparse DNN execution time vs number of operations and
corresponding model ﬁts for Wang-UCDavis-2019 [64], Wang-PingAn-2019
[65], and Mofrad-UPitt-2019 [66].

have wrestled with these difﬁculties for decades and devel-
oped methodologies for creating challenges to move these
communities forward. The sparse Deep Neural Network chal-
lenge draws upon prior challenges from machine learning,
high performance computing, and visual analytics to create
a challenge that is reﬂective of emerging sparse AI systems.
The sparse DNN challenge is a based on a mathematically
well-deﬁned DNN inference kernel and can be implemented in
any programming environment. In 2019 several sparse DNN
challenge submissions were received from a wide range of
authors and organizations. These submissions illustrate the
state-of-the-art sparse DNN execution time, TDNN, is a strong
function of the number of connections in the network, Nc.

Fig. 7. Model ﬁts of sparse DNN execution rate vs number DNN operations
for selected Graph Challenge 2019 sparse DNN submissions.

ACKNOWLEDGMENTS

The authors wish to acknowledge the following individuals
for their contributions and support: Alan Edelman, Charles
Leiserson, Steve Pritchard, Michael Wright, Bob Bond, Dave
Martinez, Sterling Foster, Paul Burkhardt, Victor Roytburd,
Trung Tran, along with William Arcand, David Bestor,
William Bergeron, Chansup Byun, Matthew Hubbell, Michael
Houle, Anna Klein, Peter Michaleas, Lauren Milechin, Julie
Mullen, Andrew Prout, Antonio Rosa, and Charles Yee.

REFERENCES

[1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1–9, June 2015.

1011101210131014Nop, DNN Operations Performed100101102103TDNN, DNN Time (seconds)Wang-UCDavis-2019-DNNTDNN = Ne/2x10111011101210131014Nop, DNN Operations Performed100101102103TDNN, DNN Time (seconds)Wang-PingAn-2019-DNNTDNN = (Ne/1011)1.110111012101310141015Nop, DNN Operations Performed100101102103104TDNN, DNN Time (seconds)Mofrad-UPitt-2019-DNNTDNN = Ne/5x101010111012101310141015Nop, DNN Operations Performed10-2100102104TDNN, DNN Time (seconds)10111012101310141015Nop, DNN Operations Performed101110121013DNN Operations per Second[2] J. Kepner, V. Gadepally, H. Jananthan, L. Milechin, and S. Samsi,
“Sparse deep neural network exact solutions,” in High Performance
Extreme Computing Conference (HPEC), IEEE, 2018.

[3] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in
Advances in neural information processing systems, pp. 598–605, 1990.
[4] B. Hassibi and D. G. Stork, “Second order derivatives for network
pruning: Optimal brain surgeon,” in Advances in neural information
processing systems, pp. 164–171, 1993.

[5] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from over-
ﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1,
pp. 1929–1958, 2014.

[6] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[7] S. Srinivas and R. V. Babu, “Data-free parameter pruning for deep neural

networks,” CoRR, vol. abs/1507.06149, 2015.

[8] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
neural network with pruning, trained quantization and huffman coding,”
CoRR, vol. abs/1510.00149, 2015.

[9] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Penksy, “Sparse
convolutional neural networks,” in 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 806–814, June 2015.
[10] J. Kepner and J. Gilbert, Graph Algorithms in the Language of Linear

Algebra. SIAM, 2011.

[11] J. Kepner, M. Kumar, J. Moreira, P. Pattnaik, M. Serrano, and H. Tufo,
“Enabling massive deep neural networks with the graphblas,” in High
Performance Extreme Computing Conference (HPEC), IEEE, 2017.
[12] M. Kumar, W. Horn, J. Kepner, J. Moreira, and P. Pattnaik, “Ibm power9
and cognitive computing,” IBM Journal of Research and Development,
2018.

[13] J. Kepner and H. Jananthan, Mathematics of big data: Spreadsheets,

databases, matrices, and graphs. MIT Press, 2018.

[14] J. Kepner, S. Alford, V. Gadepally, M. Jones, L. Milechin, R. Robi-
nett, and S. Samsi, “Sparse deep neural network graph challenge,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–7, Sep. 2019.

[15] J. P. Campbell, “Testing with the yoho cd-rom voice veriﬁcation corpus,”
in 1995 International Conference on Acoustics, Speech, and Signal
Processing, vol. 1, pp. 341–344 vol.1, May 1995.

[16] C. C. Y. LeCun and C. J. Burges, “The MNIST Database.” http://yann.
lecun.com/exdb/mnist/, 2017. [Online; accessed 01-January-2017].
[17] “HPC Challenge.” http://www.hpcchallenge.org, 2017. [Online; accessed

01-January-2017].

[18] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3,
pp. 211–252, 2015.

[19] K. A. Cook, G. Grinstein, and M. A. Whiting, “The VAST Challenge:
History, Scope, and Outcomes: An introduction to the Special Issue,”
Information Visualization, 13(4):301-312, Oct 2014.

[20] J. Scholtz, M. A. Whiting, C. Plaisant, and G. Grinstein, “A Reﬂection
on Seven Years of the VAST Challenge,” in Proceedings of the 2012
BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods
for Visualization, BELIV ’12, pp. 13:1–13:8, ACM, 2012.

[21] C. Seshadhri, T. G. Kolda, and A. Pinar, “Community structure and
scale-free collections of erd˝os-r´enyi graphs,” Physical Review E, vol. 85,
no. 5, p. 056109, 2012.

[22] G. Sanders, R. Pearce, T. La Fond, and J. Kepner, “On large-scale
graph generation with validation of diverse triangle statistics at edges
and vertices,” in 2018 IEEE International Parallel and Distributed
Processing Symposium Workshops (IPDPSW), pp. 287–296, May 2018.
[23] J. Kepner and R. Robinett, “Radix-net: Structured sparse matrices
for deep neural networks,” in 2019 IEEE International Parallel and
Distributed Processing Symposium Workshops (IPDPSW), pp. 268–274,
May 2019.

[24] J. Kepner, “Perfect power law graphs: Generation, sampling, construc-

tion and ﬁtting,” in SIAM Annual Meeting, 2012.

[25] V. Gadepally and J. Kepner, “Using a power law distribution to describe
big data,” in High Performance Extreme Computing Conference (HPEC),
IEEE, 2015.

[26] J. Kepner, S. Samsi, W. Arcand, D. Bestor, B. Bergeron, T. Davis,
V. Gadepally, M. Houle, M. Hubbell, H. Jananthan, M. Jones, A. Klein,
P. Michaleas, R. Pearce, L. Milechin, J. Mullen, A. Prout, A. Rosa,
G. Sanders, C. Yee, and A. Reuther, “Design, generation, and validation
of extreme scale power-law graphs,” in 2018 IEEE International Parallel
and Distributed Processing Symposium Workshops (IPDPSW), pp. 279–
286, May 2018.

[27] P. Dreher, C. Byun, C. Hill, V. Gadepally, B. Kuszmaul, and J. Kepner,
“Pagerank pipeline benchmark: Proposal for a holistic system bench-
mark for big-data platforms,” in Parallel and Distributed Processing
Symposium Workshops, 2016 IEEE International, pp. 929–937, IEEE,
2016.

[28] S. Samsi, V. Gadepally, M. Hurley, M. Jones, E. Kao, S. Mohindra,
P. Monticciolo, A. Reuther, S. Smith, W. Song, D. Staheli, and J. Kepner,
“Static graph challenge: Subgraph isomorphism,” in High Performance
Extreme Computing Conference (HPEC), IEEE, 2017.

[29] E. Kao, V. Gadepally, M. Hurley, M. Jones, J. Kepner, S. Mohindra,
P. Monticciolo, A. Reuther, S. Samsi, W. Song, D. Staheli, and S. Smith,
“Streaming Graph Challenge - Stochastic Block Partition,” in High
Performance Extreme Computing Conference (HPEC), IEEE, 2017.
[30] W. H. Ware, “Introduction to session on learning machines,” in Pro-
ceedings of the March 1-3, 1955, western joint computer conference,
pp. 85–85, ACM, 1955.

[31] W. A. Clark and B. G. Farley, “Generalization of pattern recognition
in a self-organizing system,” in Proceedings of the March 1-3, 1955,
western joint computer conference, pp. 86–91, ACM, 1955.

[32] O. G. Selfridge, “Pattern recognition and modern computers,” in Pro-
ceedings of the March 1-3, 1955, western joint computer conference,
pp. 91–93, ACM, 1955.

[33] G. Dinneen, “Programming pattern recognition,” in Proceedings of the
March 1-3, 1955, western joint computer conference, pp. 94–100, ACM,
1955.

[34] A. Newell, “The chess machine: an example of dealing with a complex
task by adaptation,” in Proceedings of the March 1-3, 1955, western
joint computer conference, pp. 101–108, ACM, 1955.

[35] J. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon, “A
proposal for the dartmouth summer research project on artiﬁcial intelli-
gence, august 31, 1955,” AI magazine, vol. 27, no. 4, p. 12, 2006.
[36] M. Minsky and O. G. Selfridge, “Learning in random nets,” in Informa-
tion theory : papers read at a symposium on information theory held at
the Royal Institution, London, August 29th to September 2nd, pp. 335–
347, Butterworths, London, 1960.

[37] M. Minsky, “Steps toward artiﬁcial intelligence,” Proceedings of the

IRE, vol. 49, no. 1, pp. 8–30, 1961.

[38] A. L. Samuel, “Some studies in machine learning using the game of
checkers,” IBM Journal of research and development, vol. 3, no. 3,
pp. 210–229, 1959.

[39] R. Lippmann, “An introduction to computing with neural nets,” IEEE

Assp magazine, vol. 4, no. 2, pp. 4–22, 1987.

[40] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker veriﬁcation
using adapted gaussian mixture models,” Digital signal processing,
vol. 10, no. 1-3, pp. 19–41, 2000.

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, pp. 1097–1105, 2012.

[42] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[43] J. P. Campbell, “Testing with the yoho cd-rom voice veriﬁcation corpus,”
in Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995
International Conference on, vol. 1, pp. 341–344, IEEE, 1995.

[44] Y. LeCun, C. Cortes, and C. J. Burges, “The mnist database of

handwritten digits,” 1998.

[45] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–
255, IEEE, 2009.

[46] M. Campbell, A. J. Hoane, and F.-h. Hsu, “Deep blue,” Artiﬁcial

intelligence, vol. 134, no. 1-2, pp. 57–83, 2002.

[47] M. P. McGraw-Herdeg, D. P. Enright, and B. S. Michel, “Benchmarking
the nvidia 8800gtx with the cuda development platform,” HPEC 2007
Proceedings, 2007.

[48] A. Kerr, D. Campbell, and M. Richards, “Gpu performance assessment

with the hpec challenge,” in HPEC Workshop 2008, 2008.

[70] A. Yaar, S. Rajamanickam, J. Berry, M. Wolf, J. S. Young, and . V.
atalyrek, “Linear algebra-based triangle counting via ﬁne-grained tasking
on heterogeneous environments : (update on static graph challenge),” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–4, Sep. 2019.

[71] L. Hoang, V. Jatala, X. Chen, U. Agarwal, R. Dathathri, G. Gill, and
K. Pingali, “Disttc: High performance distributed triangle counting,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–7, Sep. 2019.

[72] X. Wang, Z. Lin, C. Yang, and J. D. Owens, “Accelerating dnn inference
with graphblas and the gpu,” in 2019 IEEE High Performance Extreme
Computing Conference (HPEC), pp. 1–6, Sep. 2019.

[73] C. Gui, L. Zheng, P. Yao, X. Liao, and H. Jin, “Fast triangle counting on
gpu,” in 2019 IEEE High Performance Extreme Computing Conference
(HPEC), pp. 1–7, Sep. 2019.

[74] C. Pearson, M. Almasri, O. Anjum, V. S. Mailthody, Z. Qureshi, R. Nagi,
J. Xiong, and W. Hwu, “Update on triangle counting on gpu,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–7, Sep. 2019.

[75] M. Blanco, T. M. Low, and K. Kim, “Exploration of ﬁne-grained paral-
lelism for load balancing eager k-truss on gpu and cpu,” in 2019 IEEE
High Performance Extreme Computing Conference (HPEC), pp. 1–7,
Sep. 2019.

[76] S. Ghosh, M. Halappanavar, A. Tumeo, and A. Kalyanarainan, “Scaling
and quality of modularity optimization methods for graph clustering,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–6, Sep. 2019.

[77] X. Liu, J. S. Firoz, M. Zalewski, M. Halappanavar, K. J. Barker,
A. Lumsdaine, and A. H. Gebremedhin, “Distributed direction-
optimizing label propagation for community detection,” in 2019 IEEE
High Performance Extreme Computing Conference (HPEC), pp. 1–6,
Sep. 2019.

[78] M. Almasri, O. Anjum, C. Pearson, Z. Qureshi, V. S. Mailthody, R. Nagi,
J. Xiong, and W. Hwu, “Update on k-truss decomposition on gpu,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–7, Sep. 2019.

[79] F. Wanye, V. Gleyzer, and W. Feng, “Fast stochastic block partitioning
via sampling,” in 2019 IEEE High Performance Extreme Computing
Conference (HPEC), pp. 1–7, Sep. 2019.

[80] S. Huang, C. Pearson, R. Nagi, J. Xiong, D. Chen, and W. Hwu,
“Accelerating sparse deep neural networks on fpgas,” in 2019 IEEE
High Performance Extreme Computing Conference (HPEC), pp. 1–7,
Sep. 2019.

[49] E. A. Epstein, M. I. Schor, B. Iyer, A. Lally, E. W. Brown, and J. Cwik-
lik, “Making watson fast,” IBM Journal of Research and Development,
vol. 56, no. 3.4, pp. 15–1, 2012.

[50] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, “Convolutional deep
belief networks for scalable unsupervised learning of hierarchical repre-
sentations,” in Proceedings of the 26th annual international conference
on machine learning, pp. 609–616, ACM, 2009.

[51] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky, “Sparse
convolutional neural networks,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 806–814, 2015.
[52] A. Lavin and S. Gray, “Fast algorithms for convolutional neural net-
works,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4013–4021, 2016.

[53] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers, et al., “In-datacenter
performance analysis of a tensor processing unit,” arXiv preprint
arXiv:1704.04760, 2017.

[54] F. A. Azevedo, L. R. Carvalho, L. T. Grinberg, J. M. Farfel, R. E.
Ferretti, R. E. Leite, W. J. Filho, R. Lent, and S. Herculano-Houzel,
“Equal numbers of neuronal and nonneuronal cells make the human
brain an isometrically scaled-up primate brain,” The Journal of Com-
parative Neurology, vol. 513, no. 5, pp. 532–541, 2009.

[55] S. Shi and X. Chu, “Speeding up convolutional neural networks by ex-
ploiting the sparsity of rectiﬁer units,” arXiv preprint arXiv:1704.07724,
2017.

[56] H. Lee, C. Ekanadham, and A. Y. Ng, “Sparse deep belief net model for
visual area v2,” in Advances in neural information processing systems,
pp. 873–880, 2008.

[57] M. Ranzato, Y.-l. Boureau, and Y. L. Cun, “Sparse feature learning for
deep belief networks,” in Advances in neural information processing
systems, pp. 1185–1192, 2008.

[58] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural

networks.,” in Aistats, vol. 15, p. 275, 2011.

[59] D. Yu, F. Seide, G. Li, and L. Deng, “Exploiting sparseness in deep
neural networks for large vocabulary speech recognition,” in Acoustics,
Speech and Signal Processing (ICASSP), 2012 IEEE International
Conference on, pp. 4409–4412, IEEE, 2012.

[60] R. Robinett and J. Kepner, “Radix-net: Structured sparse matrices for
deep neural networks,” in Proceedings of the 2015 IEEE International
Parallel and Distributed Processing Symposium Workshop, IPDPSW
’19, IEEE Computer Society, 2019.

[61] M. Bisson and M. Fatica, “A gpu implementation of the sparse deep
neural network graph challenge,” in 2019 IEEE High Performance
Extreme Computing Conference (HPEC), pp. 1–8, Sep. 2019.

[62] T. A. Davis, M. Aznaveh, and S. Kolodziej, “Write quick, run fast:
Sparse deep neural network in 20 minutes of development time via
suitesparse:graphblas,” in 2019 IEEE High Performance Extreme Com-
puting Conference (HPEC), pp. 1–6, Sep. 2019.

[63] J. A. Ellis and S. Rajamanickam, “Scalable inference for sparse deep
neural networks using kokkos kernels,” in 2019 IEEE High Performance
Extreme Computing Conference (HPEC), pp. 1–7, Sep. 2019.

[64] X. Wang, Z. Lin, C. Yang, and J. D. Owens, “Accelerating dnn inference
with graphblas and the gpu,” in 2019 IEEE High Performance Extreme
Computing Conference (HPEC), pp. 1–6, Sep. 2019.

[65] J. Wang, Z. Huang, L. Kong, J. Xiao, P. Wang, L. Zhang, and C. Li,
“Performance of training sparse deep neural networks on gpus,” in
2019 IEEE High Performance Extreme Computing Conference (HPEC),
pp. 1–5, Sep. 2019.

[66] M. H. Mofrad, R. Melhem, Y. Ahmad, and M. Hammoud, “Mul-
tithreaded layer-wise training of sparse deep neural networks using
compressed sparse column,” in 2019 IEEE High Performance Extreme
Computing Conference (HPEC), pp. 1–6, Sep. 2019.

[67] S. Pandey, X. S. Li, A. Buluc, J. Xu, and H. Liu, “H-index: Hash-
indexing for parallel triangle counting on gpus,” in 2019 IEEE High
Performance Extreme Computing Conference (HPEC), pp. 1–7, Sep.
2019.

[68] R. Pearce, T. Steil, B. W. Priest, and G. Sanders, “One quadrillion
triangles queried on one million processors,” in 2019 IEEE High
Performance Extreme Computing Conference (HPEC), pp. 1–5, Sep.
2019.

[69] S. Acer, A. Yaar, S. Rajamanickam, M. Wolf, and . V. Catalyrek, “Scal-
able triangle counting on distributed-memory systems,” in 2019 IEEE
High Performance Extreme Computing Conference (HPEC), pp. 1–5,
Sep. 2019.

