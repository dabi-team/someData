Detecting and Quantifying Malicious Activity with Simulation-based Inference

1
2
0
2

t
c
O
7

]
L
M

.
t
a
t
s
[

2
v
3
8
4
2
0
.
0
1
1
2
:
v
i
X
r
a

Andrew Gambardella 1 Bogdan State 2 Naeemullah Khan 1 Leo Tsourides 3 Philip H. S. Torr 1
Atılım G ¨unes¸ Baydin 1

Abstract

We propose the use of probabilistic programming
techniques to tackle the malicious user identiﬁ-
cation problem in a recommendation algorithm.
Probabilistic programming provides numerous ad-
vantages over other techniques, including but not
limited to providing a disentangled representation
of how malicious users acted under a structured
model, as well as allowing for the quantiﬁcation
of damage caused by malicious users. We show
experiments in malicious user identiﬁcation us-
ing a model of regular and malicious users inter-
acting with a simple recommendation algorithm,
and provide a novel simulation-based measure for
quantifying the effects of a user or group of users
on its dynamics.

1. Introduction

In 1993 a famous New Yorker cartoon of a computer-
browsing canine dryly proclaimed, “on the Internet nobody
knows you’re a dog.” With hindsight this ur-meme has
proven prescient with respect to the problem of authentic-
ity on the Internet. That any one Internet user can have
identities that are both multitudinous and mutable formed
an important part of the network’s promise as a medium
for communication, self-expression and empowerment. But
ﬂexible identities also carry with them the risk of decep-
tion, with Internet-facilitated fraud coming to cast a dark
shadow over the luminous future originally envisaged by
techno-optimists (Friedman & Resnick, 2001).

Stakes increase dramatically when the problem of authen-
ticity meets the power of ranking algorithms, which are
responsible for fulﬁlling and deﬁning information retrieval
needs. Tricking an algorithm into honoring at face value
those features coming from a certain set of (malicious) users

1Department of Engineering Science, University of Oxford
2scie.nz, work done while at Facebook 3Facebook Inc.. Correspon-
dence to: Andrew Gambardella <gambs@robots.ox.ac.uk>.

ICML workshop on Socially Responsible Machine Learning, 38 th
International Conference on Machine Learning, 2021. Copyright
2021 by the author(s).

can result in disaster, with unsuspecting users being recom-
mended content, the consumption of which serves purely to
enrich a set of attackers rather than to fulﬁll users’ informa-
tion needs.

Ideally speaking, a good recommendations system should be
able to identify and remove malicious users before they can
disrupt the ranking system by a signiﬁcant margin. However,
to eliminate the risk of false positives a resilient ranking
system can use as much data as possible. So we have to
adjust the tradeoff between false positives and the damage a
set of malicious users can cause to a ranking system.

Bearing these limitations in mind, as a ﬁrst approximation,
it seems reasonable, for the sake of greater analytical clar-
ity, to divide the user base of an online social network into
the vast majority of organic users and a minority of attack
proﬁles. Seen in this way, discussions of inauthentic am-
pliﬁcation and coordinated inauthentic behavior ﬁt in with
earlier analyses of “shilling” in recommender systems (Si &
Li, 2020). Here, we study a popular class of shilling attacks
known as proﬁle injection attacks (Williams et al., 2007), in
which attackers add bogus accounts to a recommendation
system, and attempt to push the ratings of a certain subset of
products upward and others downward, while obfuscating
their intentions (Ricci et al., 2015).

In our case we are interested in asking the deceptively simple
question, how would ranking outcomes differ in the absence
of malicious users. We deﬁne malicious users here as those
users misrepresenting either their motives or their identity
for strategic gain involving the promotion or demotion of
units of content. The question is difﬁcult to answer because
of the multitudinous feedback loops potentially at work in
recommender systems, which mean that simply counting
the effects directly attributable to known malicious users is
insufﬁcient for giving an accurate picture of ranking out-
comes in the putative counterfactual universe in which no
malicious users existed.

Probabilistic programming (van de Meent et al., 2018) has
emerged as a principled means of dealing with complex
causal scenarios not unlike the issue discussed here, being
used in domains as diverse as lion behavior interpretation
(Dhir et al., 2017), spacecraft trajectories (Acciarini et al.,
2020) and high-energy physics (Baydin et al., 2019a;b).

 
 
 
 
 
 
Detecting and Quantifying Malicious Activity with Simulation-based Inference

It is our contention that probabilistic programming, and
simulation-based inference (Cranmer et al., 2020) in general,
can be used credibly to estimate the difference between
realized outcomes and the counter-factual scenario which
excludes malicious behavior. We support our assertion by
providing:

1. A proof-of-concept detection algorithm, validating that
malicious user identiﬁcation using simulation-based
inference techniques is possible using data from the
model. This is a necessary but insufﬁcient condition
for the computation of a counterfactual scenario.

2. A simulation-based counterfactual measure of inﬂu-
ence in a ranking algorithm, grounded in an informa-
tion theoretical view of the joint probability distribu-
tions of ranking outcomes in the presence, and absence,
of malicious users.

3. An illustration of how the measure could be applied
– we show that malicious users acting in coordination
have greater impact on social network dynamics than
those acting independently.

2. Related Work

The misuse of recommendation algorithms for adversarial
gain dates back to the Internet’s ﬁrst growth spurt as a com-
munication platform, and is inherently intertwined with the
history of digital spam, deﬁned as:

“the attempt to abuse of, or manipulate, a techno-
social system by producing and injecting unso-
licited, and/or undesired content aimed at steering
the behavior of humans or the system itself, at the
direct or indirect, immediate or long-term advan-
tage of the spammer(s).” (Ferrara, 2019)

Recommendation algorithms have been a key vector for the
ampliﬁcation of spam since the 1990s, when automated –
rather than curated – information retrieval ﬁrst became feasi-
ble in a consumer setting, thanks to Page et al.’s (1999) now-
famous development of the PageRank algorithm. Compared
to earlier proposals, PageRank notably provided a mecha-
nism which enforced algorithmic resiliency – a recursive
deﬁnition of popularity which protected against simplistic
attempts at faking site popularity for monetary gain through
the construction of hyperlinking rings (“spamdexing”).

PageRank became the algorithmic foundation for Google,
the dominant search engine of the past two decades.
Nonetheless, in what would become a common pattern in
the Internet industry, its original mechanisms proved only
partially effective against adaptive adversaries. The rise of
ranking also gave birth to an entire industry, search engine

optimization (SEO), dedicated to improving results against
the ranking algorithm, sometimes using adversarial “black
hat” methods (Malaga, 2010). This evolution, in turn, led to
subsequent changes to Google’s algorithms to improve their
resiliency against adversaries (McCullagh, 2011).

Adversarial attacks against recommendation algorithms
have become increasingly prominent recently, given the
importance of social media in shaping contentious news cy-
cles rife with misinformation, in particular during the course
of events such as the 2016 U.S. general elections (Allcott
& Gentzkow, 2017), or the 2018 Brazilian general elections
(Machado et al., 2019). Automated posting and engage-
ment, via “social bots” has been recognized as a particularly
important factor in the spread of disinformation on social
media (Ferrara et al., 2016; Arnaudo, 2017; Shao et al.,
2017; Cresci, 2020). The issue of automation is intertwined
with that of inauthenticity, with “coordinated inauthentic
behavior” (CIB) emerging as a distinct concept both among
academics (Giglietto et al., 2020) and industry practitioners
(Weedon et al., 2017).

The concept of CIB relies on the existence of “inauthentic
accounts,” distinguishable from authentic users. The notion
of authenticity carries with it a great deal of complexity
on the Internet. Our analysis as a result is scoped to those
platforms (such as Facebook or Twitter) which rely on the
explicit expectation of online identities consistent with of-
ﬂine personas. Even CIB itself should not be seen simply
through the binary view of malicious attackers and organic
users, as attackers may act to catalyze existing grievances
and ideologies present among audiences ripe for manipula-
tion (Starbird et al., 2019).

To ﬁght vote spam in user–item interactions, Bian et al.
(2008) proposed training ranking models using a method
based on simulated voting spam at training time. Alterna-
tively, Bhattacharjee & Goel (2007) have proposed creat-
ing incentives for power users (“connoisseurs”) to counter-
act the inﬂuence of spammers. More recently, Basat et al.
(2017) have proposed introducing noise into the ranking
function to account for distorted incentives leading to the
production of low-quality content (e.g., through link farm-
ing).

3. Methods

Our examination is meant to provide a minimal example of a
recommender system. We choose movie ranking as our set-
ting, given the canonical nature of the task, e.g., IMDb1 data
having a long history of use in the study of recommender
systems. This is admittedly a “toy” model, which does not
account for more complex designs (i.e., personalization), or
for the many issues that intervene in the deployment of rec-

1https://www.imdb.com/

Detecting and Quantifying Malicious Activity with Simulation-based Inference

ommender systems in the real world (model update cycles,
A/B testing, site outages, etc.). Formulating and studying
such a model allows us to focus on the derivation of core
concepts such as the inﬂuence metric (Section 4.2) in the
framework of probabilistic programming.

We created a model that represents several malicious users
attempting to game a recommendation algorithm modeled
as a simple ranking algorithm (without loss of generality,
users rating movies and being recommended new movies
to watch based on the current mean rating), provided in the
supplementary material. In this model, multiple users (some
malicious and some organic) are rating items, which are then
ranked and suggested to other users based on their ranking.
User tastes are modeled by real-valued variables νi ∈ [0, 1],
which determine which movies they would naturally like.
Similarly, movies have taste features µj ∈ [0, 1] which
denote something akin to their genre and in our model are
left ﬁxed. We model the rating function Rate(υi, µj) so
that user i will rate movie j higher the closer user taste
νi is to movie taste µj. The resulting ratings ρi,j in each
user–movie pair constitute the elements of the global rating
matrix R.

In this model the main latent variables we would like to
infer are the binary variables βi, denoting whether a given
user i is malicious or not, and τi, denoting the target movie
which user i would like to boost, if user i is malicious, i.e.,
if βi = 1. Probabilistic programming will allow us to con-
dition this model on a given rating matrix (for instance, one
that represents real-world movie ratings), and then ﬁnd em-
pirical distributions over the latent variables in the simulator
(µ, ν, β, τ ) consistent with the given rating matrix Robs. In
summary, for the purposes of identifying malicious users
and what they are trying to boost, we will obtain the pos-
terior distribution p(β, τ |Robs), while leaving µ and ν as
nuisance variables.

We implemented our model in PyProb (Baydin et al., 2019b),
a lightweight probabilistic programming library for stochas-
tic simulators. We obtain our posteriors using weighted
importance sampling (Kitagawa, 1996) which gives a poste-
rior in the form of weighted traces drawn from a proposal
distribution, which is in our case the unmodiﬁed stochastic
simulator. As our posterior is given to us in the form of
simulations conditioned on observed data, it is by nature
completely disentangled and interpretable, and will tell us
the goals of each of the malicious users (τi) in addition to
their identities (βi). Crucially, this Bayesian approach also
gives us principled uncertainty estimates associated with all
our predictions.

4. Experiments

4.1. Obtaining the Posterior and Identifying Malicious

Users

We found that obtaining a posterior over the identities of ma-
licious users, as well as their targets, was non-trivial, with
different inference engine families behaving considerably
differently. Markov-chain Monte Carlo (MCMC) (Metropo-
lis et al., 1953; Hastings, 1970; Wingate et al., 2011), despite
being the gold standard to converge to the correct posterior
given enough samples, performed extremely poorly. We
suspect that this is due to the variables of interest (the mali-
cious users and their targets) making up only a small portion
of the latent variables in the model, as well as being discrete
whereas the others are continuous. We observed that the
model as it is currently formulated was not a good ﬁt for the
single-site MCMC (Wingate et al., 2011; van de Meent et al.,
2018) inference engine implemented in PyProb, mainly be-
cause generating proposals where a single maliciousness
latent βi is ﬂipped lead to very low acceptance probabili-
ties due to the very abrupt nature of the resulting change in
the rating matrix (which needs to be compensated by corre-
sponding changes in some user taste latents υi that cannot
be achieved in a single-site MCMC algorithm), leading to
slow mixing and poor sample efﬁciency.

We found that weighted importance sampling (IS) (Kita-
gawa, 1996) performed better in practice than MCMC, and
used it to obtain posteriors conditioned on observed rat-
ings matrices. When using IS with 100,000 executions of
our model in which malicious users do not attempt to dis-
guise their activities (i.e., difﬁculty α = 0), the mean of
rating matrices in the posterior, i.e., the posterior predictive
p(R|Robs), appears to be a noisy version of the observed
ground truth rating matrix Robs, showing that the inference
scheme sampled a posterior distribution over simulation
runs in which the observed rating matrix is likely.

Much more interesting are scenarios in which malicious
users attempt to disguise their activities through obfuscated
attacks. We model this obfuscation by setting the difﬁculty
hyperparameter α = 0.3. This obfuscation introduces a
signiﬁcant amount of uncertainty into our results, which is
reﬂected in the detection of malicious users. Whereas in
the unambiguous case IS produced an empirical posterior
over malicious users and malicious target that matched with
the ground truth nearly exactly, in the ambiguous case we
see signiﬁcant uncertainty in the empirical posterior. The
results show that the most probable (0.85) explanation of
the observed rating matrix is that there are no malicious
users, although we also see non-negligible (0.15) probabil-
ity attached to the scenario where there is one malicious
user (which we know to be the ground truth for the observed
rating matrix). We also see that the mode of the posterior
distribution for the malicious users matches the ground truth

Detecting and Quantifying Malicious Activity with Simulation-based Inference

(Endres & Schindelin, 2003), the square root of the sym-
metric JS divergence (Dagan et al., 1997), computed as the
average of JS distances between the counterfactual and real
probability distributions over ratings, per entry in the rating
matrix. In a distribution over rating matrices p(R|·), each
entry in the matrix is a probability distribution (in the em-
pirical case, a histogram) over ratings for each user–movie
pair over a large number of simulator executions. Our mea-
sure for impact is then the average JS distance for each of
these histograms, between the realized and counterfactual
rating matrices. The JS distance is a valid metric between
probability distributions and always normalised to [0, 1],
making it an attractive choice to measure distances between
distributions over matrix entries.

We show results using our inﬂuence measure between coun-
terfactual and realised ratings matrices while varying the
amount of coordination between malicious users in Figure 1.
When malicious targets are drawn from a distribution with
a low standard deviation, malicious actors act in a more co-
ordinated fashion (as they are more likely to target the same
movie), which leads to a higher average distance between
the counterfactual and realized ratings distributions. As
the malicious target standard deviation increases, malicious
users act against each others’ interests, leading to a lower
overall impact on the dynamics of the model.

5. Discussion

We have suggested the use of probabilistic programming
techniques to both discover and measure the inﬂuence of
malicious users interacting with a ranking algorithm. We
base our choice of this method on its conceptual advantages
in modeling the full universe of possibilities deriving from
the complex interactions between users, content and ranking
algorithms. The use of simulation-based approaches in
this setting has been limited by practical concerns, until
recent advances in numerical computation – coupled with
the emergence of high-quality libraries for probabilistic
programming.

Probabilistic programming techniques also have some im-
portant additional advantages over other methods. They
provide for interpretable explanations as to, e.g., why a
user would be classiﬁed as malicious, and provide measures
of conﬁdence in their predictions. Furthermore, genera-
tive modeling of the entities and processes involved in this
setting allows us to make precise deﬁnitions of the core
concepts and to quantify key aspects such as user inﬂuence
and malicious user damage.

References

Acciarini, G., Pinto, F., Metz, S., Boufelja, S., Kaczmarek,
S., Merz, K., Martinez-Heras, J. A., Letizia, F., Bridges,

Figure 1. Measuring the effect on the rating matrix as the standard
deviation of the distribution from which we draw τ is increased.
Lower values of standard deviation (i.e., more coordination be-
tween malicious actors) results in a higher impact to the dynamics
of the ratings in the matrix. Results averaged over 10 different
seeds, with one standard deviation bounds shown.

(user 4), albeit with much lower probability (0.15) compared
with the unambiguous case (1.0). We also see that while
there is probability mass associated with the ground truth
value of the malicious target, this probability is quite low,
showing that the IS inference scheme has signiﬁcant uncer-
tainty in the identiﬁcation of the malicious target, given the
experimental setup and the number of traces (100,000) that
we ran during inference.

4.2. Using Counterfactuals to Quantify the Damage

Done by Malicious Users

In addition to giving disentangled and interpretable expla-
nations of the behaviour of users interacting with a ranking
algorithm, simulation-based inference also gives us the abil-
ity to measure the effects of users on the model’s dynamics.
We can quantify the amount of impact that users imposed
on the dynamics of an observed rating matrix by using a
slightly modiﬁed model, which allows us to disarm users
by nullifying the effect of their ratings. Comparing the dy-
namics reﬂected in the distributions both with and without
disarmed users gives us a counterfactual-based method of
quantifying their impact on the dynamics of the simula-
tor. Given a set of disarmed users γ, we ﬁnd the distance
between the posterior-predictive distributions p(R|Robs)
and p(R|Robs, γ) given observed data Robs, or between
the prior-predictive p(R) and p(R|γ) in the generic case
without an observed Robs.

Our chosen metric for measuring the impact that a disarmed
user or group of users has caused is the average JS distance

012345Coordination ( distribution standard deviation)4.9004.9254.9504.9755.0005.0255.050Damage (Distance)1e2Detecting and Quantifying Malicious Activity with Simulation-based Inference

C., and Baydin, A. G. Spacecraft Collision Risk Assess-
ment with Probabilistic Programming. In Third Workshop
on Machine Learning and the Physical Sciences (NeurIPS
2020), Vancouver, Canada, 2020.

Allcott, H. and Gentzkow, M. Social media and fake news
in the 2016 election. Journal of Economic Perspectives,
31(2):211–236, 2017. ISSN 08953309. doi: 10.1257/jep.
31.2.211.

Arnaudo, D. Computational Propaganda in Brazil: Social
Bots during Elections. Computational Propaganda Re-
search Project, 8:1–39, 2017.

Basat, R. B., Tennenholtz, M., and Kurland, O. A game the-
oretic analysis of the adversarial retrieval setting. Journal
of Artiﬁcial Intelligence Research, 60:1127–1164, 2017.
ISSN 10769757. doi: 10.1613/jair.5547.

Baydin, A. G., Heinrich, L., Bhimji, W., Gram-Hansen,
B., Louppe, G., Shao, L., Prabhat, P., Cranmer, K., and
Wood, F. Efﬁcient probabilistic inference in the quest
In Advances
for physics beyond the standard model.
in Neural Information Processing Systems, volume 33,
2019a.

Baydin, A. G., Shao, L., Bhimji, W., Heinrich, L., Meadows,
L., Liu, J., Munk, A., Naderiparizi, S., Gram-Hansen, B.,
Louppe, G., Ma, M., Zhao, X., Torr, P., Lee, V., Cranmer,
K., Prabhat, and Wood, F. Etalumis: Bringing proba-
bilistic programming to scientiﬁc simulators at scale. In
International Conference for High Performance Comput-
ing, Networking, Storage and Analysis, SC, 2019b. ISBN
9781450362290. doi: 10.1145/3295500.3356180.

Bhattacharjee, R. and Goel, A. Algorithms and incentives
for robust ranking. In Proceedings of the Annual ACM-
SIAM Symposium on Discrete Algorithms, volume 07-09-
Janu, pp. 425–433, 2007. ISBN 9780898716245.

Bian, J., Agichtein, E., Liu, Y., and Zha, H. A few bad
votes too many? Towards robust ranking in social media.
In AIRWeb 2008 - Proceedings of the 4th International
Workshop on Adversarial Information Retrieval on the
Web, pp. 53–60, 2008. ISBN 9781605581590. doi: 10.
1145/1451983.1451997.

Cranmer, K., Brehmer, J., and Louppe, G. The frontier of
simulation-based inference. Proceedings of the National
Academy of Sciences, 117(48):30055–30062, 2020. ISSN
0027-8424. doi: 10.1073/pnas.1912789117.

Cresci, S. Detecting malicious social bots: Story of a never-
ending clash. In Lecture Notes in Computer Science (in-
cluding subseries Lecture Notes in Artiﬁcial Intelligence
and Lecture Notes in Bioinformatics), volume 12021
LNCS, pp. 77–88. Springer, 2020. ISBN 9783030396268.
doi: 10.1007/978-3-030-39627-5 7.

Dagan, I., Lee, L., and Pereira, F. Similarity-based methods
for word sense disambiguation. In Proceedings of the
Thirty-Fifth Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the Euro-
pean Chapter of the Association for Computational Lin-
guistics, pp. 56–63, 1997. doi: 10.3115/979617.979625.

Dhir, N., Wood, F., V´ak´ar, M., Markham, A., Wijers, M.,
Trethowan, P., Du Preez, B., Loveridge, A., and MacDon-
ald, D. Interpreting lion behaviour with nonparametric
probabilistic programs. In Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), 2017.

Endres, D. M. and Schindelin, J. E. A new metric for prob-
ability distributions. IEEE Transactions on Information
Theory, 49(7):1858–1860, 2003. ISSN 00189448. doi:
10.1109/TIT.2003.813506.

Ferrara, E. The History of Digital Spam. Communications

of the ACM, 62(8):82–91, 2019.

Ferrara, E., Varol, O., Davis, C., Menczer, F., and Flammini,
A. The rise of social bots. Communications of the ACM,
ISSN 15577317. doi: 10.1145/
59(7):96–104, 2016.
2818717.

Friedman, E. J. and Resnick, P. The social cost of cheap
pseudonyms. Journal of Economics and Management
Strategy, 10(2):173–199, 2001. ISSN 10586407. doi:
10.1162/105864001300122476.

Giglietto, F., Righetti, N., Rossi, L., and Marino, G.

It
takes a village to manipulate the media: coordinated link
sharing behavior during 2018 and 2019 Italian elections.
Information Communication and Society, 23(6):867–891,
2020. ISSN 14684462. doi: 10.1080/1369118X.2020.
1739732.

Hastings, W. K. Monte Carlo Sampling Methods Using
Markov Chains and Their Applications. Biometrika, 57
(1):97, 1970. ISSN 00063444. doi: 10.2307/2334940.

Kitagawa, G. Monte Carlo Filter and Smoother for Non-
Gaussian Nonlinear State Space Models. Journal of
Computational and Graphical Statistics, 1996.
ISSN
10618600. doi: 10.2307/1390750.

Machado, C., Kira, B., Narayanan, V., Kollanyi, B., and
Howard, P. N. A study of misinformation in whatsapp
groups with a focus on the brazilian presidential elections.
In The Web Conference 2019 - Companion of the World
Wide Web Conference, WWW 2019, pp. 1013–1019, 2019.
ISBN 9781450366755. doi: 10.1145/3308560.3316738.

Malaga, R. A. Search Engine Optimization—Black and
In Advances in Computers,
White Hat Approaches.
volume 78, pp. 1–39. Elsevier, 2010. doi: 10.1016/
s0065-2458(10)78001-3.

Detecting and Quantifying Malicious Activity with Simulation-based Inference

McCullagh, D.

Testing Google’s Panda Algorithm:
CNET Analysis. https://www.cnet.com/news/
testing-googles-panda-algorithm-cnet-analysis/,
2011.
8301-31921_3-20054797-281.html.

http://news.cnet.com/

URL

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
Teller, A. H., and Teller, E. Equation of state calcula-
tions by fast computing machines. Journal of Chemical
Physics, 21(6):1087–1092, 1953. ISSN 00219606. doi:
10.1063/1.1699114.

Page, L., Brin, S., Motwani, R., and Winograd, T. The
PageRank Citation Ranking: Bringing Order to the Web.
World Wide Web Internet And Web Information Systems,
1998. ISSN 1752-0509. doi: 10.1.1.31.1768.

Ricci, F., Shapira, B., and Rokach, L. Recommender systems
handbook, Second edition. 2015. ISBN 9781489976376.
doi: 10.1007/978-1-4899-7637-6.

Shao, C., Ciampaglia, G. L., Varol, O., Yang, K., Flammini,
A., and Menczer, F. The spread of low-credibility content
by social bots. Nature communications, 9(1):1–9, 2017.

Si, M. and Li, Q. Shilling attacks against collaborative
recommender systems: a review. Artiﬁcial Intelligence
Review, 53(1):291–319, 2020.
ISSN 15737462. doi:
10.1007/s10462-018-9655-x.

Starbird, K., Arif, A., and Wilson, T. Disinformation as
collaborative work: Surfacing the participatory nature of
strategic information operations. Proceedings of the ACM
on Human-Computer Interaction, 3(CSCW):1–26, 2019.
ISSN 25730142. doi: 10.1145/3359229.

van de Meent, J.-W., Paige, B., Yang, H., and Wood, F.
An Introduction to Probabilistic Programming. arXiv
preprint, 2018.

Weedon, J., Nuland, W., and Stamos, A. Information Oper-

ations and Facebook, 2017. ISSN 1047-9651.

Williams, C. A., Mobasher, B., and Burke, R. Defending rec-
ommender systems: Detection of proﬁle injection attacks.
Service Oriented Computing and Applications, 2007.
ISSN 18632386. doi: 10.1007/s11761-007-0013-0.

Wingate, D., Stuhlm¨uller, A., and Goodman, N. D.
Lightweight implementations of probabilistic program-
In
ming languages via transformational compilation.
Journal of Machine Learning Research, volume 15, pp.
770–778, 2011.

Detecting and Quantifying Malicious Activity with Simulation-based Inference

A. Recommender Model

Algorithm 1 Probabilistic generative model of movie ratings based on the movie ranking setting, deﬁning the joint
distribution p(µ, υ, β, τ , R) = p(R|µ, υ, β, τ ) p(µ, υ, β, τ ), where p(R|·) is the likelihood and p(µ, υ, β, τ ) is the prior.
Given an observed rating matrix Robs, we would like to infer the posterior p(β, τ |Robs).

Nµ ∈ N: Number of movies
Nυ ∈ N: Number of users
pβ ∈ [0, 1]: Probability of maliciousness
ρβ ∈ [0, 1]: Mean malicious rating
ρσ ∈ R+: Rating standard deviation
τµ ∈ [0, Nµ]: Mean malicious target
τσ ∈ R+: Malicious target standard deviation
T ∈ N: Time steps
α ∈ [0, 1]: Difﬁculty

µ ← ﬁxed vector, components sampled only once as {µi ∼ Uniform(0, 1)}Nµ
i=1
υ ← {υi ∼ Uniform(0, 1)}Nυ
i=1
β ← {βi ∼ Bernoulli(pβ)}Nυ
i=1
τ ← {τi ∼ TruncatedNormal(τµ, τσ, 0, Nµ)}Nυ
i=1

(cid:46) Movies and taste features
(cid:46) Users and taste features
(cid:46) Maliciousness of users
(cid:46) Maliciousness targets (ignored for organic users)

R ← {ρi,j = 0}Nυ,Nµ
i=1,j=1
for t = 1, . . . , T do

1: procedure RATINGMODEL
2: Sample latents:
3:
4:
5:
6:
7: Simulate:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

if βi then

for i = 1, . . . , Nυ do

else

if τi is unrated then

j ← τi
ρ ← (1 − α) ∗ ρβ + α ∗ Rate(υi, µj)

j ← Pick(R)
ρ ← α ∗ Rate(υi, µj)

18:
19:
20:

21:

else

j ← Pick(R)
ρ ← Rate(υi, µj)

ρi,j ∼ TruncatedNormal(ρ, ρσ, 0, 1)

return R
22:
23: procedure PICK(R)
24: m ← {mj = 1
Nυ
25:
26:
27: procedure RATE(υi, µj)
return 1 − |υi − µj|
28:

j ∼ Categorical(Nµ, m)
return j

(cid:80)Nυ

i=1 ρi,j}Nµ

j=1

(cid:46) Rating matrix (Nυ rows, Nµ cols)

(cid:46) Malicious user

(cid:46) Pick malicious target
(cid:46) Rate to boost τi

(cid:46) Pick movie based on ranking
(cid:46) Rate lower than or equal to taste feature match
(cid:46) Organic user
(cid:46) Pick movie based on ranking
(cid:46) Rate according to taste feature match
(cid:46) Sample rating by user i for movie j
(cid:46) Return rating matrix

(cid:46) Mean movie ratings
(cid:46) Sample movie index based on mean ratings
(cid:46) Return picked movie index j/

(cid:46) Return rating

Detecting and Quantifying Malicious Activity with Simulation-based Inference

B. Conditioning Results, Unambiguous Problem

(a) Observed rating matrix Robs

(b) Empirical posterior p(β, τ , R|Robs)

Figure 2. (a) Rating matrix Robs that we observe as the input, and the corresponding ground truth values for user maliciousness β and
targets τ . (b) Empirical posterior distribution found by weighted IS in a scenario in which malicious users do not attempt to disguise their
activities (α = 0). We can see that the posterior predictive rating matrix, p(R|Robs) matches fairly closely with the ground truth Robs,
and that the posterior malicious users p(β, τ |Robs) match up with the ground truth in all posterior simulations. IS is able to discover both
the malicious user and its target with little uncertainty. Dashed vertical lines show ground truth values.

0123456789Movie0123456789User0.390.620.360.530.290.380.560.910.390.950.50.730.470.640.40.490.670.980.50.950.680.910.650.820.580.670.850.80.680.770.420.650.390.560.320.410.590.940.420.970000000000.980.450.680.420.590.350.440.620.970.4510.840.610.870.70.940.850.670.320.840.290.870.640.90.730.970.880.70.350.870.310.980.790.950.880.880.970.850.50.980.460.720.490.750.580.820.730.550.20.720.17Rating matrix0123456789MovieTaste0.610.380.640.470.710.620.440.090.610.0570123456789User0.00430.110.290.0270.470.060.770.740.590.89Taste0123456789User0000100000Malicious0123456789User-1-1-1-19-1-1-1-1-1Mal. target movie0123456789Movie0123456789User0.570.690.550.650.50.560.660.710.570.690.630.660.620.660.580.630.660.620.630.610.630.680.620.670.590.630.680.60.630.590.590.680.580.650.540.590.660.670.590.660.240.240.240.270.240.240.240.240.240.720.530.660.510.620.470.530.630.730.530.720.660.60.670.640.670.670.630.450.660.430.710.610.710.660.720.710.640.440.710.420.730.650.730.70.710.730.680.480.730.460.660.540.670.590.70.670.570.370.660.35Rating matrix (mean)0123456789MovieTaste (mean)0.610.380.640.470.710.620.440.090.610.0570123456789User0.220.360.390.280.330.160.680.70.620.82Taste (mean)0123456789User6.5e-051.9e-083e-111.4e-0813.4e-071.1e-071.7e-111.6e-171.6e-09Malicious (mean)0246810Num. malicious users0.00.51.002468User0.00.51.0Malicious0246810Detected malicious target0.00.51.0Detecting and Quantifying Malicious Activity with Simulation-based Inference

C. Conditioning Results, Ambiguous Problem

(a) Observed rating matrix Robs

(b) Empirical posterior p(β, τ , R|Robs)

Figure 3. (a) Rating matrix Robs that we observe as the input, sampled from a scenario in which malicious imitate non-malicious users
(α = 0.3). (b) Empirical posterior found by weighted IS. IS is able to discover this malicious user in about 15% of simulations making up
the empirical posterior, but cannot discover their target accurately. Dashed vertical lines show ground truth values.

0123456789Movie0123456789User0.390.620.360.530.290.380.560.910.390.950.50.730.470.640.40.490.670.980.50.950.680.910.650.820.580.670.850.80.680.770.420.650.390.560.320.410.590.940.420.970.430.450.420.50.380.430.480.310.430.780.450.680.420.590.350.440.620.970.4510.840.610.870.70.940.850.670.320.840.290.870.640.90.730.970.880.70.350.870.310.980.790.950.880.880.970.850.50.980.460.720.490.750.580.820.730.550.20.720.17Rating matrix0123456789MovieTaste0.610.380.640.470.710.620.440.090.610.0570123456789User0.00430.110.290.0270.470.060.770.740.590.89Taste0123456789User0000100000Malicious0123456789User-1-1-1-19-1-1-1-1-1Mal. target movie0123456789Movie0123456789User0.510.650.490.60.450.50.610.740.510.730.50.640.480.590.440.50.610.750.50.740.710.70.70.730.670.710.720.550.710.530.530.660.510.610.470.520.630.730.530.720.480.60.460.590.420.470.570.70.480.690.580.70.560.650.520.570.670.690.580.680.740.640.750.680.740.740.670.460.740.440.730.620.740.670.740.730.650.440.730.420.730.680.720.720.690.730.710.510.730.490.670.550.690.60.710.680.580.370.670.35Rating matrix (mean)0123456789MovieTaste (mean)0.610.380.640.470.710.620.440.090.610.0570123456789User0.130.110.50.160.130.240.650.690.570.8Taste (mean)0123456789User0.00530.000213.7e-050.000980.150.011e-051e-052.1e-050.00082Malicious (mean)0246810Num. malicious users0.00.502468User0.00.2Malicious0246810Detected malicious target0.00.5Detecting and Quantifying Malicious Activity with Simulation-based Inference

D. Algorithm with Disarmed Users

Algorithm 2 Variation of Algorithm 1, in which we can disarm a subset of users γ and quantify the effect (inﬂuence) of one
or more users in the rating distribution, e.g., by computing a distance between predictive distributions p(R) and p(R|γ) for
the prior, or p(R|Robs) and p(R|Robs, γ) for the posterior conditioned on a given rating matrix observation Robs.

Nµ ∈ N: Number of movies
Nυ ∈ N: Number of users
pβ ∈ [0, 1]: Probability of maliciousness
ρβ ∈ [0, 1]: Mean malicious rating
ρσ ∈ R+: Rating standard deviation
τµ ∈ [0, Nµ]: Mean malicious target
τσ ∈ R+: Malicious target standard deviation
T ∈ N: Time steps
α ∈ [0, 1]: Difﬁculty
γ : {γi ∈ 0, 1}Nυ

i=1: Disarmed users

µ ← ﬁxed vector, components sampled only once as {µi ∼ Uniform(0, 1)}Nµ
i=1
υ ← {υi ∼ Uniform(0, 1)}Nυ
i=1
β ← {βi ∼ Bernoulli(pβ)}Nυ
i=1
τ ← {τi ∼ TruncatedNormal(τµ, τσ, 0, Nµ)}Nυ
i=1

(cid:46) Movies and taste features
(cid:46) Users and taste features
(cid:46) Maliciousness of users
(cid:46) Maliciousness targets (ignored for organic users)

R ← {ρi,j = 0}Nυ,Nµ
i=1,j=1
for t = 1, . . . , T do

1: procedure RATINGMODEL
2: Sample latents:
3:
4:
5:
6:
7: Simulate:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

for i = 1, . . . , Nυ do
if not γi then
if βi then

else

if τi is unrated then

j ← τi
ρ ← (1 − α) ∗ ρβ + α ∗ Rate(υi, µj)

j ← Pick(R)
ρ ← α ∗ Rate(υi, µj)

19:
20:
21:

22:

else

j ← Pick(R)
ρ ← Rate(υi, µj)

ρi,j ∼ TruncatedNormal(ρ, ρσ, 0, 1)

return R
23:
24: procedure PICK(R)
25: m ← {mj = 1
Nυ
26:
27:
28: procedure RATE(υi, µj)
return 1 − |υi − µj|
29:

j ∼ Categorical(Nµ, m)
return j

(cid:80)Nυ

i=1 ρi,j}Nµ

j=1

(cid:46) Rating matrix (Nυ rows, Nµ cols)

(cid:46) Do not let user rate if disarmed
(cid:46) Malicious user

(cid:46) Pick malicious target
(cid:46) Rate to boost τi

(cid:46) Pick movie based on ranking
(cid:46) Rate lower than or equal to taste feature match
(cid:46) Organic user
(cid:46) Pick movie based on ranking
(cid:46) Rate according to taste feature match
(cid:46) Sample rating by user i for movie j
(cid:46) Return rating matrix

(cid:46) Mean movie ratings
(cid:46) Sample movie index based on mean ratings
(cid:46) Return picked movie index j/

(cid:46) Return rating

