0
2
0
2

t
c
O
6
2

]
E
M

.
t
a
t
s
[

1
v
2
5
4
3
1
.
0
1
0
2
:
v
i
X
r
a

BayCANN: Streamlining Bayesian Calibration with

Artiﬁcial Neural Network Metamodeling

Hawre Jalal1,* and Fernando Alarid-Escudero2,*

1Department of Health Policy and Management, University of
Pittsburgh, Graduate School of Public Health,Pittsburgh, PA, 15261
2Division of Public Administration, Center for Research and Teaching in
Economics (CIDE), Aguascalientes, Aguascalientes, Mexico, 20313

Abstract

Purpose: Bayesian calibration is theoretically superior to standard direct-search

algorithm because it can reveal the full joint posterior distribution of the calibrated

parameters. However, to date, Bayesian calibration has not been used often in

health decision sciences due to practical and computational burdens. In this

paper we propose to use artiﬁcial neural networks (ANN) as one solution to these

limitations.

Methods: Bayesian Calibration using Artiﬁcial Neural Networks (BayCANN)

involves (1) training an ANN metamodel on a sample of model inputs and outputs,

and (2) then calibrating the trained ANN metamodel instead of the full model in

a probabilistic programming language to obtain the posterior joint distribution of

the calibrated parameters. We demonstrate BayCANN by calibrating a natural

history model of colorectal cancer to adenoma prevalence and cancer incidence

data. In addition, we compare the eﬃciency and accuracy of BayCANN against

performing a Bayesian calibration directly on the simulation model using an

incremental mixture importance sampling (IMIS) algorithm.

Results: BayCANN was generally more accurate than IMIS in recovering

the ”true” parameter values. The ratio of the absolute ANN deviation from the

truth compared to IMIS for eight out of the nine calibrated parameters were less

than one indicating that BayCANN was more accurate than IMIS. In addition,

BayCANN took about 15 minutes total compared to the IMIS method which

took 80 minutes.

Conclusions: In our case study, BayCANN was more accurate than IMIS and

was ﬁve-folds faster. Because BayCANN does not depend on the structure of the

simulation model, it can be adapted to models of various levels of complexity

1

 
 
 
 
 
 
with minor changes to its structure. We provide BayCANN’s open-source implementation

in R.

Keywords— Calibration, Bayesian methods, metamodel, deep learning, artiﬁcial neural

networks

* Please address correspondences to either Hawre Jalal (hjalal@pitt.edu) or Fernando

Alarid-Escudero (fernando.alarid@cide.edu).

1 Background

Modelers and decision-makers often use mathematical or simulation models to simplify real-life

complexity and inform decisions, particularly those for which uncertainty is inherent. However,

some of the model parameters might be either unobserved or unobservable due to various

ﬁnancial, practical or ethical reasons. For example, a model that simulates the natural history

of cancer progression may lack an estimate for the rate at which an individual transitions

from a pre-symptomatic cancer state to becoming symptomatic. Although this rate might

not be directly observable, mathematical models have proven useful in reconstructing cancer

progression dynamics and estimating these unknown parameters using a technique commonly

referred to as calibration. Calibration involves modifying the model input parameters until

a desired output is obtained. Thus, researchers have successfully calibrated their models to

other rich data on cancer such as prevalence of precancerous lesions or cancer incidence that

are commonly produced from cancer models.[3, 69, 60]

Calibration has the potential of improving model structure, and recent guidelines recommend

that model calibration should be performed where there is existing data on outputs.[71, 11]

In addition, modelers are encouraged to report the uncertainty around calibrated parameters

and use these uncertainties in both deterministic and probabilistic sensitivity analyses.[11]

There are several calibration techniques with various levels of complexity. For example, Nelder-Mead

is a direct-search algorithms commonly used to calibrate models in health and medicine.

Nelder-Mead is a deterministic approach that searches the parameter space for good-ﬁtting

parameter values.[47] Although Nelder-Mead is generally reliable, it cannot produce parameter

distributions or directly inform correlations among the calibrated parameters. It is also not

guaranteed to produce a global optimal value because it might converge to a local optima.

Unlike the direct-search algorithms, Bayesian methods are naturally suited for calibration

because they can reveal the posterior joint and marginal distributions of the input parameters.[44]

However, Bayesian methods are rarely implemented in practice due to the technical and computational

challenges. Bayesian calibration often requires tens or hundreds of thousands of simulations

runs, and often requires a model written in a probabilistic programming language, such as

Stan[12] or Bayesian inference Using Gibbs Sampling (BUGS)[41]. We argue that the complexity

of these tasks and their potential computational demand have prohibited a wider adoption of

Bayesian calibration methods.

2

In this manuscript, we propose using metamodels to streamline Bayesian calibration. A metamodel

is a second model that can be used to approximate the model’s input-output relationship.[25]

We speciﬁcally propose to use deep artiﬁcial neural networks (ANN) given their ability to

map highly nonlinear relationship between sets of inputs and outputs. ANNs have been used

as metamodels of both stochastic and deterministic responses mainly for their computational

eﬃciency.[8, 5, 22, 13, 74] One of the ﬁrst implementation of ANN as metamodels was in

1992 for a scheduling simulation model.[53, 54] Since then, ANNs have been successfully

implemented as emulators of all sorts of discrete-event and continuous simulation models in

a wide variety of ﬁelds.[33, 63, 17, 15] ANNs have also been proposed as proxies for nonlinear

and simulation models[51, 42, 52]. An example of ANNs as metamodels is estimating the

mean and variance of patient time in emergency department visits.[32, 31] Nowadays ANNs

are widely popular as machine learning tools in artiﬁcial intelligence.[66] Deep learning using

ANNs are used for visual recognition in self-driving cars[46] and in classifying galaxies[16].

ANNs have been used for calibration of computationally expensive models, such as general

circulation and rainfall-runoﬀ models in climate science[28, 20], and other complex global

optimization techniques such as genetic algorithms.[70] Most algorithms use ANN mainly

for its computational eﬃciency relative on the simulation models upon which they are built.

In this paper, we illustrate that in addition to its computational eﬃciency, ANN can also

overcome practical challenges involved in representing the simulation model in probabilistic

programming languages. We refer to our approach as Bayesian calibration via artiﬁcial neural

networks, or BayCANN for short.

We demonstrate BayCANN by calibrating a realistic model of the natural history of colorectal

cancer (CRC). We compare the results of this approach to a full Bayesian calibration of the

original model using an incremental mixture importance sampling (IMIS) algorithm. We

provide the code in R and Stan for our application that researchers can use to calibrate their

own models.

2 Methods

We start this exposition with reviewing Bayes formula and showing that Bayes formula can

be applied directly to calibrate a model. We describe the computational burden of attempting

to apply Bayes formula in most realistic models, and how deep ANNs can be used to streamline

Bayesian calibration methods to calibrate these models. We illustrate this approach by calibrating

a natural history model of CRC. We also compare the performance of the BayCANN approach

to a traditional Bayesian calibration using IMIS.

3

2.1 Bayesian calibration

In its most basic form, Bayes formula states

p(θ|data) =

l(data|θ)p(θ)
p(data)

(1)

where θ is a set of model parameters, data is the observed data, and l is the likelihood. Because

the denominator is not a function of θ, we can rewrite Equation (1) as

p(θ|data) ∝ l(data|θ)p(θ).

(2)

Table 1 shows how each term in Equation 2 can be mapped to a component in a calibration

exercise. The prior distribution, p(θ), represents our uncertainty about the distribution of the

model parameters before calibrating the model. Modelers often use various forms of distributions

to describe this uncertainty, including beta or logit-normal distribution for probabilities,

gamma for rates, or log-normal distributions for rates or hazard ratios. Thus, a prior distribution

can be thought of as the uncertainty of the pre-calibrated model input parameters. For example,

a vague distribution can be represented by a uniform distribution where all the values are

equally likely within a deﬁned range. Essentially, Bayesian calibration updates this prior

distribution based on the observed target data.

The term p(θ|data) is often referred to as the posterior distribution, which represents the

updated distribution of θ after we observe some data. This is equivalent to the deﬁnition of

a calibrated parameter when the data are the calibration targets.

The likelihood function, l(data|θ), denotes how likely it is that the observed data arise from a

given data generation mechanism with a parameter set values θ. From a simulation modeling
perspective, l(data|θ) is equivalent to measuring the goodness of the model output ﬁt to the

calibration targets given a simulation model’s input parameter set θ. Thus, all components of

Bayes formula can be mapped to calibration exercises and this formula can be used to obtain

the calibrated parameter distributions (a.k.a. the posterior distributions).

While Bayesian calibration provides a seemingly better framework than other calibration

approaches, such as through direct-search algorithms, it is rarely used in practice due to

computational and practical limitations. In most cases, the computation and technical diﬃculties

preclude applying Bayesian calibration in most realistic models. The main challenge lies in
the complexity of applying Equation 2. Speciﬁcally, an analytical solution for p(θ|data) is

unlikely to exist for most realistic simulation models. Thus, specialized algorithms, such as

Markov-Chain Monte-Carlo (MCMC) will be necessary which can be both practically challenging

and computationally expensive.

4

2.2 Metamodels

To overcome the computational and practical challenges of Bayesian calibration, we propose

to use artiﬁcial neural network (ANN) metamodels. A metamodel is a second model that

approximates the relationship between the simulation model’s inputs and outputs (i.e., a

metamodel is a model of the model). [10, 38, 36, 35] A metamodel generally describes a simpler

relationship among the inputs and outputs than the underlying original simulation model.

Metamodels range from simple models, such as linear regressions to complex nonlinear models,

such as artiﬁcial neural networks (ANN). Although, linear regression models are the most

common form of metamodels[9, 8, 64, 19, 72, 6, 37, 25, 24], in this paper, we focus on ANN

because they are generally more ﬂexible than linear regression and are relatively simple to

implement in Stan or BUGS.

Metamodels are often used because they generally oﬀer vast reduction in computation time.[34,

18, 7, 72, 48, 9, 65, 58, 29] For example, a model that takes several hours or even days to

run can be approximated with a metamodel that may only take a few milliseconds to run.

This feature has been an attractive attribute of metamodels for many decades in engineering

and computer science, however, the use of metamodels has been limited in health decision

sciences.[25] Some examples of metamodels in health decision sciences have been Gaussian

processes to conduct probabilistic sensitivity analysis (PSA) of computationally expensive

simulation models, such as microsimulation models.[67, 14]

While shorter computation time is an important advantage of metamodeling that we will

exploit, it is not the only advantage of metamodels in Bayesian calibration. An additional

advantage of metamodels is that they can be generalized to models of high complexity, without

changing the structure of the metamodel itself. This is because they describe a simpler and

more structured relationship than the simulation model. Thus, the same metamodel structure

can easily be adapted to more sophisticated simulation models, and the same code in Stan or

BUGS can be adapted to more complicated calibration problems with no or minimal change

to the code itself.

2.2.1 ANN metamodels

Artiﬁcial neural networks (ANNs) are networks of nonlinear regressions that were originally

developed to mimic the neural signal processing in the brain and to model how the nervous

system processes complex information [43, 45, 59, 23, 49]. In the simplest form, an ANN

has three layers of neurons: an input layer, a hidden layer and an output layer. Figure 1

illustrates the basic structure of a four layer neural network with two hidden layers with 4

input nodes in the input layer, 4 nodes in the hidden layer, and 4 output nodes in the output

5

layer. The structure of this ANN can be represented by the following sets of equations

h = f (1) (cid:16)

z(1) = W (1)θ + b(1)
z(1)(cid:17)
z(2) = W (2)h + b(2)
z(2)(cid:17)
Y = f (2) (cid:16)

,

(3)

where θ is the simulation model inputs, Y is the model outputs to be compared to the calibrated
targets, and (W, b) = (cid:0)W (1), b(1), W (2), b(2)(cid:1) are the ANN coeﬃcients, such that W (1) are the
weights connecting the inputs θ with the neurons h in the hidden layer and W (2) represents
the weights connecting the neurons h in the hidden layer with the output Y , b(1) and b(2) are
biases connecting the inputs with the neurons in the hidden layer, and the hidden layer with
the outputs, respectively, and f (1) is the activation function, which is commonly implemented
as sigmoid or logistic functions, such that

f (1) (cid:16)

z(1)(cid:17)

=

1
1 + e−z(1) .

(4)

The function f (2) is often called a transfer function that transforms the results from the neurons
of the hidden layer into a working output. The transfer function can also be a sigmoid function
or simply a linear function. Thus, the z(1) and z(2) are the weighted sum of inputs from the
input layer and the hidden layer, respectively.

The ﬂexibility of an ANN can be increased by increasing the number of hidden layers and the

number of nodes in these layers. ANNs with more than one hidden layer are often referred

to as deep ANNs. For this reason, ANNs have recently witnessed signiﬁcant advances for

applications in machine learning, artiﬁcial intelligence and pattern recognition.[57] We use

TensorFlow(R) and the package keras in R to create ANN metamodels that approximate the

relationship between our model’s input parameters and outputs and estimate the coeﬃcients

β and W .[55, 26] We perform this estimation from a set of probabilistic samples using a Latin

hypercube sampling (LHS) design of experiment (DoE) to eﬃciently sample the input parameter

space. Once we obtain the ANN coeﬃcients, we perform the Bayesian calibration using the

ANN rather than the simulation model itself.

2.3 BayCANN algorithm summary

This section outlines the steps to conduct BayCANN.

1. Structure the simulation model such that it produces outputs corresponding to the

calibration targets. For example, if calibration data are in the form of disease incidence

or prevalence, make sure the model produces these outputs.

2. Obtain a dataset of parameter sets from a probabilistic sampling framework. This

could be done by conducting a Latin hypercube sampling (LHS) of model inputs’ prior

6

distributions.

3. Run the simulation model using all the parameter sets from the input samples from the

previous step and generate their corresponding simulation model outputs.

4. Train an ANN using a subset of the model inputs and the model outputs, and validate

it using the remaining simulation runs. Adjust the ANN’s structure to obtain an accurate

metamodel.

5. Obtain the calibration targets and, if available, a measure of uncertainty (e.g., standard

errors or sample size) from datasets, literature or subjective assessments.

6. Perform the Bayesian calibration by passing the ANN coeﬃcients, the prior input parameter

samples, and the targets data to the ANN framework in Stan. Stan then returns the

joint posterior distribution of the calibrated parameters.

The R code provided reﬂect this algorithm. In the case study below, we use BayCANN to

calibrate a colorectal cancer natural history model.

2.4 Case study: Natural history model of colorectal cancer

We use BayCANN to calibrate a state-transition model (STM) of the natural history of colorectal

cancer (CRC) implemented in R [26]. We refer to our model as CRCmodR. CRCmodR is a

discrete-time STM based on a model structure originally proposed by Wu et al., 2006 [73]

that has previously been used for testing other methods.[2, 21] Brieﬂy, CRCModR has 9

diﬀerent health states that include absence of the disease, small and large precancerous lesions

(i.e., adenomatous polyps) , and early and late preclinical and clinical cancer states by stage.

Figure 2 shows the state-transition diagram of the model. The progression between health

states follows a continuous-time age-dependent Markov process. There are two age-dependent

transition intensities (i.e., transition rates), λ1(a) and µ(a), that govern the age of onset of
adenomas and all-cause mortality, respectively. Following Wu’s original speciﬁcation [73], we

specify λ1(a) as a Weibull hazard such that

λ1(a) = lγaγ−1,

(5)

where l and γ are the scale and shape parameters of the Weibull hazard model, respectively.

The model simulates two adenoma categories: small (adenoma smaller than or equal to 1

cm in size) and large (adenoma larger than 1 cm in size). All adenomas start small and can

transition to the large size category at a constant annual rate λ2. Large adenomas may become
preclinical CRC at a constant annual rate λ3. Both, small and large adenomas may progress
to preclinical CRC, although most will not in an individual’s lifetime. Early preclinical cancers

progress to late stages at a constant annual rate λ4 and could become symptomatic at a
constant annual rate λ5. Late preclinical cancer could become symptomatic at a constant
annual rate λ6. After clinical detection, the model simulates the survival time to death from

7

early and late CRC using time-homogeneous mortality rates, λ7 and λ8, respectively. In total,
the model has nine health states: normal, small adenoma, large adenoma, preclinical early

CRC, preclinical late CRC, CRC death and other causes of death. The state-transition diagram

of the model is shown in Figure 2. The model simulates the natural history of CRC of a

hypothetical cohort of 50-year-old women in the US over a lifetime. The cohort starts the

simulation with a prevalence of adenoma of padeno, from which a proportion, psmall, correspond
to small adenomas and prevalence of preclinical early and late CRC of 0.12 and 0.08, respectively.

The simulated cohort in any state is at risk of all-cause mortality µ(a) which was obtained

from the US life tables.[4]

CRCmodR involves eleven parameters summarized in Table 2.[2]. Mortality rates from early

and late stages of CRC (λ7, λ8]) could be obtained from cancer population registries (e.g.,
SEER in the U.S.). Thus, we calibrate the model to the remaining nine parameters (padeno,
psmall,l,γ,λ2,λ3,λ4,λ5,λ6).

To obtain a “truth” that we can compare BayCANN against, we ﬁrst conducted a conﬁrmatory
simulation where we generated the targets for the base-case values in Table 2. We generated

four diﬀerent age-speciﬁc targets, including adenoma prevalence, proportion of small adenomas

and CRC incidence for early and late stages, which resemble commonly used calibration targets

for this type of models.[40] To generate the calibration targets, we ran CRCmodR as a microsimulation

[39] 100 times to produce diﬀerent adenoma-related and cancer incidence outputs using the

base-case values in Table 2. We then aggregated the results across all 100 outputs to compute

their mean and standard errors (SE). Diﬀerent calibration targets could have diﬀerent level

of uncertainty given the amount of data to compute their summary measures. Therefore,

to account for diﬀerent variations in the amount of data on diﬀerent calibration targets, we

simulated diﬀerent numbers of individuals for adenoma-related (N = 500) and cancer incidence

(N = 100, 000) individuals. Figure 3 shows the generated adenoma-related and cancer incidence

calibration targets aggregated over 100 diﬀerent runs using the parameter set in Table 2.

To create a deep ANN metamodel, we generated a DOE by sampling each of the nine parameters

from the ranges of the uniform distributions as shown in Table 2. We then ran the natural

history model and generated model outputs that represent the calibration targets at each

of the parameter sets of the DOE. We ran the model on 10,000 samples from the DOE. To

train the ANN, we divided the DOE dataset into a training subset (8,000 simulations) and

a validation subset (2000 simulations) that we used for cross validation. We deﬁne an ANN

with two hidden layers and 100 nodes per each hidden layer. Then, we evaluated the performance

of the ANN by cross validating the predicted values for the 36 outcomes against the observed

values from the validation datasets.

To calibrate the ANN, we adopted a Bayesian approach that allowed us to obtain a joint

posterior distribution that characterizes the uncertainty of both the calibration targets and

previous knowledge of the parameters of interest. The likelihood function was constructed by
assuming that the targets, yti, are normally distributed with mean φti and standard deviation

8

σti, where φti = M [θ] is the model-predicted output for each type of target t and age group
i at parameter set θ. We deﬁned uniform prior distributions for all θu based on previous
knowledge or nature of the parameters (Table 2).

To conduct the BayCANN, we implemented the deep ANN in Stan [12] which uses a guided

MCMC using gradient decent, referred to as Hamiltonian Monte-Carlo. Similarly, we used

the package rstan to conduct the Bayesian calibration in R. In the supplementary material,

we provide the R code for wider implementation of our algorithms. In addition, we compare

BayCANN against a full Bayesian calibration of the natural history model using the incremental

mixture importance sampling (IMIS) algorithm. The IMIS algorithm has been described

elsewhere[56], but brieﬂy, this algorithm reduces the computational burden of Bayesian calibration

by incrementally building a better importance sampling function based on Gaussian mixtures.

We compare BayCANN to the IMIS both in terms of accuracy and eﬃciency.

3 Results

We present the performance of the ANN in approximating the output of the simulation model,

and compare the joint posterior distribution of the simulation model parameters produced

from BayCANN against the IMIS approach. We compare both the BayCANN and IMIS

results recovering the “truths” - the parameter values we used to generate the calibration

targets.

3.1 Cross validation

In the cross-validation exercise, we split the DOE dataset into 80% training and 20% validation,

to ensure that the ANN can approximate the model outputs generated from inputs that were

not used in the training process. Figure 4 illustrates the results of this exercise. Each plot

represents one of the model outputs, where we compare the ANN’s output on the y-axis against

the model’s output on the x-axis. Each red dot represents one of the DOE validation sample

that was not used in the training. The ANN had a high prediction performance in approximating
the model outputs (R2 = 99.9%), indicating that the deep ANN is an accurate and reliable
metamodel of the simulation model within the parameter ranges.

3.2 Accuracy and eﬃciency of BayCANN versus IMIS

Figure 5 compares BayCANN against IMIS in recovering the true parameter values used to

generate the targets. The posterior distributions for both methods overlap and cover the

truth for all parameters in the case of the BayCANN method and most of the parameters

in the case of the IMIS. Table 3 compares the ANN against the IMIS method for recovering

the true parameter values. This table presents the mean of the calibrated parameters and

the absolute deviation from the truth. The ratio of the ANN deviation to the IMIS deviation

presents the relative performance of the two methods. This ratio of deviations is less than 1

9

for most parameters except for g which is slightly above one, indicating that the mean of the

ANN distribution performed better at recovering the true parameter value. For the majority

of the parameters (six out of nine) this ratio was less than 0.35, indicating a better accuracy

for BayCANN relative to IMIS.

In addition, BayCANN was ﬁve times faster than the IMIS. The IMIS algorithm took 80

minutes to run in a MacBook Pro Retina, 15-inch, Late 2013 with a 2.6 GHz Intel Core i7

processor with 4 cores and 16 megabytes of RAM. The Bayesian ANN took only 15 minutes

on the same computer; 5 minutes to produce 10,000 samples for the DOE dataset and 10

minutes to ﬁt the ANN and produce the joint posterior distributions.

4 Discussion

In this study, we propose BayCANN as a feasible and practical solution to the challenges

of Bayesian calibration in health decision science models. We compared the accuracy and

eﬃciency of BayCANN approach against the IMIS algorithm using a natural history model

of colorectal cancer. In our case study, BayCANN was both faster and overall more accurate

in recovering the true parameter values than the IMIS algorithm. We developed BayCANN

approach to be generalizable to models of various complexities, and we provide the open

source implementation in R and Stan to facilitate its wider adoption.

Bayesian calibration is superior to other forms of calibration (such as direct-search algorithms)

because it can reveal the joint posterior distribution of the calibrated parameters.[61] This

joint distribution will be informative in the case of non-identiﬁablity where calibration targets

are not suﬃcient to provide a unique solution to the calibrated parameters.[2] Non-identiﬁability

is often overlooked using standard non-Bayesian calibration approaches. In addition, Bayesian

calibration provides other practical advantages because the samples from the joint posterior

distribution can be used directly as inputs to probabilistic sensitivity analyses (PSA) of cost-eﬀectiveness

models and other models.

Despite its advantages, Bayesian calibration has rarely been used in calibrating models in

health decision sciences because of practical and computational burdens of its implementation.

The practical burdens involves adapting the simulation model to the probabilistic programming

languages or other algorithms that are often designed for simple regression-type models. The

computational burdens involves, the time it might take for algorithms such as Markov-chain

Monte-Carlo (MCMC) to converge especially given the complexity of simulation models.

These two factors are the main reasons for the limited adoption of Bayesian calibration in

practice.

We illustrated how ANNs can be used as a practical solution to these computational and

technical challenges. In this application we use an ANN to “learn” the relationship between

the model inputs and outputs using a training subset of the simulation runs. We used cross-validation

to avoid over-ﬁtting the ANN to the calibration targets.[30, 68, 27] The ANN was an eﬃcient

10

and accurate emulator of the natural history model because it predicted the outputs of our

simulation model with high accuracy and in a fraction of the time the original simulation

model required. Compared to linear regressions metamodels, ANN metamodels are more

powerful, require fewer assumptions and less precise information about the system to be modeled

(i.e., the degree of polynomials).[1, 50] These properties make it an attractive choice for Bayesian

calibration.

In addition to the computational and practical advantages of using ANNs, BayCANN may

have an additional advantage for representing models with ﬁrst-order Monte-Carlo noise from

individual-based state-transition models (iSTM). Traditionally, calibrating these models has

been especially challenging because of (1) the stochasticity of each simulation due to the

output of the simulation varying given the same set of input parameter values, and (2) the

extra computational burden involved in calibrating iSTM. Because BayCANN averages over a

set of simulations, it can account for the ﬁrst order Monte-Carlo noise.

Our approach has some limitations. First, accuracy - Because ANN’s are metamodels, they

may rarely achieve 100% precision compared to using the simulation model itself. In our

example, with a relatively simple ANN (only two hidden layers with 100 hidden nodes each),

we were able to achieve 99.9% accuracy. However, for other application, the accuracy of the

ANN might be lower. In addition, over-ﬁtting can be a serious problem with any metamodel

especially when the purpose of the metamodel is as sensitive as calibration. To reduce the

chance of overﬁtting, we cross-validated the model against a subset of the data, which is

a commonly used technique. We used 80% of the 10,000 PSA observations for training the

ANN, and 20% to validate the ANN. We visually inspected the degree of ﬁt for the simulation

output against those predicted by the ANN (Figure 4). Second, similar to any Bayesian model,

the choice of priors could be important. Fortunately, in simulation models, modelers often

make careful choices of their priors when they design their models and run PSA analyses.

Thus, depending on the ranges chosen, the best-ﬁtting parameters may be outside the simulated

ranges. Importantly, the joint posterior distribution can give insights into the parameter

ranges. For example, if a parameter is skewed heavily without a clear peak, that may indicate

that the parameter range needs to be shifted to cover values that may ﬁt better. This process

is usually iterative and may involve multiple steps or redeﬁning the parameter ranges and

recalibrating the model. Finally, there is no strict guideline for choosing the number of hidden

ANN layers or the number of nodes per layer. In this study, we chose an ANN with two hidden

layers and 100 nodes per layer. Adjusting these parameters and additional parameters of

the Bayesian calibration process can provide better calibration results. While determining

these values apriori can be challenging, we recommend modelers who wish to use BayCANN

to start with simple settings initially and gradually increase the complexity of the ANN to

accommodate their particular needs. We provide ﬂexible code in R and Stan to simplify these

tasks.

In summary, Bayesian calibration can reveal important insights into model parameter values

and produce outcomes that match observed data. BayCANN is one eﬀort to target the computational

11

and technical challenges of Bayesian calibration for complex models.

5 Acknowledgements

Dr. Jalal was supported by the Center for Disease Control and Prevention Contract No.

34150, and a grant from the National Institute on Drug Abuse of the National Institute of

Health under award no. K01DA048985. Dr Alarid-Escudero was supported by a grant from

the National Cancer Institute (U01-CA-253913-01) as part of the Cancer Intervention and

Surveillance Modeling Network (CISNET), the Gordon and Betty Moore Foundation, and

Open Society Foundations (OSF).

References

[1] Fasihul M. Alam, Ken R. McNaught, and Trevor J. Ringrose. A comparison of

experimental designs in the development of a neural network simulation metamodel.
Simulation Modelling Practice and Theory, 12:559–578, 2004.

[2] Fernando Alarid-Escudero, Richard F. MacLehose, Yadira Peralta, Karen M Kuntz,

and Eva A Enns. Nonidentiﬁability in Model Calibration and Implications for Medical
Decision Making. Medical Decision Making, 38(7):810–821, oct 2018.

[3] Fernando Alarid-Escudero, Richard F Maclehose, Yadira Peralta, Karen M Kuntz, and

Eva A Enns. Nonidentiﬁability in model calibration and implications for medical decision
making. Supplement. Medical Decision Making, 38(7):1–8, 2018.

[4] Elizabeth Arias. United States Life Tables, 2014. National Vital Statistics Reports, 62(7),

2014.

[5] Adedeji B. Badiru and David B. Sieger. Neural network as a simulation metamodel
in economic analysis of risky projects. European Journal of Operational Research,

105(1):130–142, 1998.

[6] Jerry Banks. Handbook of Simulation. John Wiley & Sons, Inc., 1998.

[7] Russell R Barton. Metamodels for simulation input-output relations.

In J.J. Swain,

D. Goldsman, R.C. Crain, and J.R. Wilson, editors, Winter Simulation Conference,

volume 9, pages 289–299, 1992.

[8] Russell R. Barton. Simulation optimization using metamodels. In Winter Simulation

Conference, number 2, pages 230–238. IEEE, 2009.

[9] Russell R. Barton and Martin Meckesheimer. Chapter 18 Metamodel-Based

Simulation Optimization. Handbooks in Operations Research and Management Science,

13(C):535–574, 2006.

12

[10] Robert W Blanning. The Sources and Uses of Sensitivity Information.

Interfaces,

4(4):32–38, 1974.

[11] Andrew H. Briggs, Milton C. Weinstein, Elisabeth A. L. Fenwick, Jonathan Karnon,

Mark J. Sculpher, and A. David Paltiel. Model Parameter Estimation and Uncertainty

Analysis: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force
Working Group-6. Medical Decision Making, 32(5):722–732, 9 2012.

[12] Bob Carpenter, Andrew Gelman, Matthew D Hoﬀman, Daniel Lee, Ben Goodrich,

Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan:
A probabilistic programming language. Journal of statistical software, 76(1), 2017.

[13] M Chambers. Process optimization via neural network metamodeling. 79(November

2000):0–7, 2002.

[14] Tiago M. de Carvalho, Eveline A.M. Heijnsdijk, Luc Coﬀeng, and Harry J. de Koning.

Evaluating Parameter Uncertainty in a Simulation Model of Cancer Using Emulators.
Medical Decision Making, 39(4):405–413, 2019.

[15] Eddy El Tabach, Laurent Lancelot, Isam Shahrour, and Yacoub Najjar. Use of artiﬁcial

neural network simulation metamodelling to assess groundwater contamination in a road
project. Mathematical and Computer Modelling, 45:766–776, 2007.

[16] SR Folkes, O Lahav, and SJ Maddox. An artiﬁcial neural network approach to the
classiﬁcation of galaxy spectra. Monthly Notices of the Royal Astronomical Society,

283(2):651–665, 1996.

[17] D.J. Fonseca, D.O. Navaresse, and G.P. Moynihan.

Simulation metamodeling

through artiﬁcial neural networks. Engineering Applications of Artiﬁcial Intelligence,

16(3):177–183, 2003.

[18] Linda Weiser Friedman and Israel Pressman. The Metamodel in Simulation Analysis:
Can It Be Trusted? Journal of the Operational Research Society, 39(10):939–948, 1988.

[19] Michael C. Fu. A Tutorial Review of Techniques for Simulation Optimization. In J. D.
Tew, S. Manivannan, D. A. Sadowski, and A. F. Seila, editors, Proceedings of the 1994
Winter Simulation Conference, page 8. IEEE, 1994.

[20] Tristan Hauser, Andrew Keats, and Lev Tarasov. Artiﬁcial neural network assisted

Bayesian calibration of climate models. Climate Dynamics, 39(1-2):137–154, 2012.

[21] Anna Heath, Natalia Kunst, Christopher Jackson, Mark Strong, Fernando

Alarid-Escudero, Jeremy D. Goldhaber-Fiebert, Gianluca Baio, Nicolas A. Menzies,

and Hawre Jalal. Calculating the Expected Value of Sample Information in Practice:
Considerations from 3 Case Studies. Medical Decision Making, 40(3):314–326, 2020.

13

[22] R.D. Hurrion. An example of simulation optimisation using a neural network metamodel:
Finding the optimum number of kanbans in a manufacturing system. The Journal of
Operational Research Society, 48(11):1105–1112, 1997.

[23] Anil K. Jain, Jianchang Mao, and K.M. Mohiuddin. Artiﬁcal Neural Networks: A

tutorial. Computer, 29(3):31–44, 1996.

[24] Hawre Jalal, Michale Boudreaux, Bryan Dowd, and Karen M. Kuntz. Measuring

Decision Sensitivity with Monte Carlo Simulation and Multinomial Logistic Regression

Metamodeling. 2015.

[25] Hawre Jalal, Bryan Dowd, Fran¸cois Sainfort, and Karen M. Kuntz. Linear regression
metamodeling as a tool to summarize and present simulation model results. Medical
Decision Making, 33(7):880–90, 2013.

[26] Hawre Jalal, Petros Pechlivanoglou, Eline Krijkamp, Fernando Alarid-Escudero, Eva A.

Enns, and M. G. Myriam Hunink. An Overview of R in Health Decision Sciences.
Medical Decision Making, 37(7):735–746, 2017.

[27] Fayiz Y Abu Khadra and Jaber E Abu Qudeiri. Comparison between neural network

and response surface metamodels based on D-optimal designs. International Journal of
Computational Materials Science and Surface Engineering, 5(2):85–101, 2013.

[28] Soon-thiam Khu, Dragan Savic, Yang Liu, Henrik Madsen, and Computer Science.

A fast Evolutionary-based Meta-Modelling Approach for the Calibration of a
In Trans. 2nd Biennial Meeting of the International

Rainfall-Runoﬀ Model.
Environmental Modelling and Software Society, iEMSs, pages 1–6, 2004.

[29] Andr´e I. Khuri and Siuli Mukhopadhyay. Response surface methodology. Wiley

Interdisciplinary Reviews: Computational Statistics, 2(2):128–149, 2010.

[30] Robert A. Kilmer, Alice E. Smith, and Larry J. Shuman. Neural networks as a

metamodeling technique for discrete event stochastic simulation. Proceedings of the
Artiﬁcial Neural Networks in Engineering Conference, 4(1):1141–1146, 1994.

[31] Robert A. Kilmer, Alice E. Smith, and Larry J. Shuman. An emergency department

simulation and a neural network metamodel. Journal of the Society for Health Systems,

5(3):63–79, 1997.

[32] Robert Allen Kilmer. Artiﬁcial neural network metamodels of stochastic computer

simulations. PhD thesis, Pittsburgh University, PA, 1994.

[33] Robert Allen Kilmer. Applications of Artiﬁcial Neural Networks to Combat Simulations.

Mathematical and Computer Modelling, 23(1/2):91–99, 1996.

[34] Jack P. C. Kleijnen. Regression Metamodels for Generalizing SImulation Results. IEEE

Transactions on Systems, Man, and Cybernetics, 9(2):93–96, 1979.

14

[35] Jack P. C. Kleijnen. Design and analysis of simulation experiments. Springer US, 2nd

edition, 2015.

[36] Jack P. C. Kleijnen, Susan M. Sanchez, Thomas W. Lucas, and Thomas M. Cioppa.

State-of-the-Art Review: A User’s Guide to the Brave New World of Designing
Simulation Experiments. INFORMS Journal on Computing, 17(3):263–289, 2005.

[37] Jack P.C. Kleijnen and Robert G. Sargent. A methodology for ﬁtting and validating
metamodels in simulation. European Journal of Operational Research, 120(1):14–29,

2000.

[38] J.P.C. Kleijnen. A Comment on Blanning’s ”Metamodel for Sensitivity Analysis: The

Regression Metamodel in Simulation”. Interfaces, 5(3):21–23, 1975.

[39] Eline M Krijkamp, Fernando Alarid-Escudero, Eva A Enns, Hawre J Jalal, MG Myriam

Hunink, and Petros Pechlivanoglou. Microsimulation modeling for health decision
sciences using r: a tutorial. Medical Decision Making, 38(3):400–422, 2018.

[40] Karen M Kuntz, Iris Lansdorp-Vogelaar, Carolyn M Rutter, Amy B Knudsen, Marjolein

van Ballegooijen, James E Savarino, Eric J Feuer, and Ann G Zauber. A systematic

comparison of microsimulation models of colorectal cancer: the role of assumptions about
adenoma progression. Medical Decision Making, 31:530–539, 2011.

[41] David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project:

Evolution, critique and future directions. Statistics in medicine, 28(25):3049–3067, 2009.

[42] Tom´aˇs Mareˇs and Aniˇcka Kuˇcerov´a. Artiﬁcial Neural Networks in Calibration of

Nonlinear Models. Cognitive Neuroscience of Human Systems: Work and Everyday Life,

2012.

[43] Egill M´asson and Yih-Jeou Wang. Introduction to computation and learning in artiﬁcial

neural networks. European Journal of Operational Research, 47(1):1–28, 1990.

[44] Nicolas A. Menzies, Djøra I. Soeteman, Ankur Pandya, and Jane J. Kim. Bayesian

Methods for Calibrating Health Policy Models: A Tutorial. PharmacoEconomics, pages

1–12, 2017.

[45] Editors D Michie, D J Spiegelhalter, and C C Taylor. Machine Learning , Neural and

Statistical Classiﬁcation. 1994.

[46] Anselme Ndikumana, Nguyen H Tran, Ki Tae Kim, Choong Seon Hong, et al. Deep

learning based caching for self-driving cars in multi-access edge computing.
Transactions on Intelligent Transportation Systems, 2020.

IEEE

[47] J.A. Nelder and R. Mead. A simplex method for function minimization. Computer

Journal, 7(4):308–313, 1965.

15

[48] A. O’Hagan, Marc C. Kennedy, and Jeremy E. Oakley. Uncertainty analysis and other
inference tools for complex computer codes. Bayesian Staistics 6, pages 503–524, 1999.

[49] Julian D Olden, Joshua J Lawler, and N LeRoy Poﬀ. Machine learning methods without
tears: a primer for ecologists. The Quarterly review of biology, 83(2):171–193, 2008.

[50] M Padgett and T A Roppel. Neural networks and simulation: modeling for applications.

Simulation, 58(5):295–304, 1992.

[51] Ricardo M. Paiva, Andr´e R. D. Carvalho, Curran Crawford, and Afzal Suleman.

Comparison of Surrogate Models in a Multidisciplinary Optimization Framework for
Wing Design. AIAA Journal, 48(5):995–1006, 2010.

[52] B. Pichler, R. Lackner, and H. a. Mang. Back analysis of model parameters in
International Journal for

geotechnical engineering by means of soft computing.
Numerical Methods in Engineering, 57(14):1943–1978, 2003.

[53] Henri Pierreval, Universitc Claude Bernard, Bd Novembre, and Villeurbanne Cedex.

Training a Neural Network by Simulation for Dispatching Problems. Proceedings of the
Third International Conference on Computer Integrated Manufacturing, 1992.,, pages

332–336, 1992.

[54] Henri Pierreval and R. C. Huntsinger. An investigation on neural network capabilities
as simulation metamodels. In Proceedings of the 1992 Summer Computer Simulation
Conference, pages 413–417, 1992.

[55] R Core Team. R: A Language and Environment for Statistical Computing, 2018.

[56] Adrian E Raftery and Le Bao. Estimating and projecting trends in hiv/aids generalized
epidemics using incremental mixture importance sampling. Biometrics, 66(4):1162–1173,

2010.

[57] Daniele Rav`ı, Charence Wong, Fani Deligianni, Melissa Berthelot, Javier Andreu-Perez,
Benny Lo, and Guang-Zhong Yang. Deep learning for health informatics. IEEE journal
of biomedical and health informatics, 21(1):4–21, 2016.

[58] Pedro M. Reis dos Santos and M. Isabel Reis dos Santos. Using subsystem linear
regression metamodels in stochastic simulation. European Journal of Operational
Research, 196(3):1031–1040, 2009.

[59] Raul Rojas. Statistics and Neural Networks.

In Neural Networks, chapter 9, pages

229–264. 1996.

[60] Carolyn M. Rutter, Diana L. Miglioretti, and James E. Savarino. Bayesian Calibration

of Microsimulation Models.

Journal of the American Statistical Association,

104(488):1338–1350, 2009.

16

[61] Carolyn M Rutter, Jonathan Ozik, Maria DeYoreo, Nicholson Collier, et al.

Microsimulation model calibration using incremental mixture approximate bayesian
computation. The Annals of Applied Statistics, 13(4):2189–2212, 2019.

[62] Carolyn M Rutter, Onchee Yu, and Diana L Miglioretti. A hierarchical non-homogenous
Poisson model for meta-analysis of adenoma counts. Statistics in Medicine, 26(1):98–109,

2007.

[63] Ihsan Sabuncuoglu and Souheyl Touhami. Simulation metamodelling with neural

networks: An experimental investigation. International Journal of Production Research,

40(11):2483–2505, 2002.

[64] Jerome Sacks, William J Welch, Toby J Mitchell, and Henry P Wynn. Design and

Analysis of Computer Experiments. Statistical Science, 4(4):409–423, 1989.

[65] Isabel R Santos and Pedro R Santos. Simulation metamodels for modeling output

distribution parameters. In Winter Simulation Conference, pages 910–918. IEEE, 2007.

[66] J¨urgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks,

61:85–117, 2015.

[67] M D Stevenson, J Oakley, and J B Chilcott. Gaussian Process Modeling in Conjunction

with Individual Patient Simulation Modeling: A Case Study Describing the Calculation
of Cost-Eﬀectiveness Ratios for the Treatment of Established Osteoporosis. Medical
Decision Making, 24(1):89–100, 2004.

[68] Anna Syberfeldt, Henrik Grimm, and Amos Ng. Design of Experiments for training

metamodels in simulation-based optimisation of manufacturing systems.
Proceedings of The 18th International Conference on Flexible Automation and Intelligent

In

Manufacturing (FAIM’08), number 46, 2008.

[69] Tazio Vanni, Jonathan Karnon, Jason Madan, Richard G White, W John Edmunds,

Anna M Foss, and Rosa Legood. Calibrating Models in Economic Evaluation: A
Seven-Step Approach. PharmacoEconomics, 29(1):35–49, 2011.

[70] Ling Wang. A hybrid genetic algorithm-neural network strategy for simulation

optimization. Applied Mathematics and Computation, 170(2):1329–1343, 2005.

[71] Milton C Weinstein, Bernie O’Brien, John Hornberger, Joseph Jackson, Magnus

Johannesson, Chris McCabe, and Bryan R Luce. Principles of good practice for decision

analytic modeling in health-care evaluation: report of the ISPOR Task Force on Good
Research Practices–Modeling Studies. Value in Health, 6(1):9–17, 2003.

[72] Linda Weiser Friedman. The Simulation Metamodel. Kluwer Academic Publishers,

Norwell, MA, 1996.

17

[73] Grace Hui-Min Wu, Yi-Ming Wang, Amy Ming-Fang Yen, Jau-Min Wong, Hsin-Chih

Lai, Jane Warwick, and Tony Hsiu-Hsi Chen. Cost-eﬀectiveness analysis of colorectal
cancer screening with stool DNA testing in intermediate-incidence countries. BMC
cancer, 6:136, 2006.

[74] Christopher W. Zobel and Kellie B. Keeling. Neural network-based simulation
metamodels for predicting probability distributions. Computers and Industrial
Engineering, 54(4):879–888, 2008.

18

6 Tables

Table 1: Contrasting Bayes formula with a calibration process.

Term

Bayesian Context

Calibration Context

p(θ)

p(θ|data)

Prior distribution of the model input parameters
θ
Posterior distribution of the model parameters θ
given observed data

l(data|θ)

Likelihood of the data given model parameters θ

Pre-calibrated model input parameters

Calibrated model parameters to target data

Goodness-of-ﬁt measure; how well the model
output ﬁts the target data given a particular
value of θ

19

Table 2: The parameters of the natural history model of colorectal cancer (CRC). The
base valeus are used to generate the calibration targets and the ranges of the uniform
distribution used as priors for the Bayesian calibration.

Parameter Description

Base value Calibrate? Source

Prior range

l
g
λ2
λ3
λ4
λ5
λ6
λ7
λ8
padeno
psmall

Scale parameter of Weibull hazard
Shape parameter of Weibull hazard
Small adenoma to large adenoma
Large adenoma to preclinical early CRC
Preclinical early to preclinical late CRC
Preclinical early to clinical early CRC
Preclinical late to clinical late CRC
CRC mortality in early stage
CRC mortality in late stage
Prevalence of adenoma at age 50
Proportion of small adenomas at age 50

2.86e-06
2.78
0.0346
0.0215
0.3697
0.2382
0.4852
0.0302
0.2099
0.27
0.71

Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
No
Yes
Yes

[73]
[73]
[73]
[73]
[73]
[73]
[73]
[73]
[73]
[62]
[73]

[2 × 10−6, 2 × 10−5]
[2.00, 4.00]
[0.01, 0.10]
[0.01, 0.04]
[0.20, 0.50]
[0.20, 0.30]
[0.30, 0.70]
-
-
[0.25, 0.35]
[0.38, 0.95]

20

Table 3: Comparing the accuracy of BayCANN against IMIS in recovering the true
parameter values as shown in Table 2
Parameter Truth

IMIS (mean) ANN (deviation)

ANN (mean)

IMIS (deviation) Deviation ratio

l
g
λ2
λ3
λ4
λ5
λ6
padeno
psmall

0.0000029
2.7790840
0.0346000
0.0215000
0.3697000
0.2382000
0.4852000
0.2700000
0.7100000

0.0000107
2.5249560
0.0336306
0.0210140
0.3699218
0.2343703
0.5137273
0.2689944
0.7078401

0.0000108
2.5293840
0.0381652
0.0197588
0.4026561
0.2528302
0.5231138
0.2669941
0.7008754

0.0000078
0.2541280
0.0009694
0.0004860
0.0002218
0.0038297
0.0285273
0.0010056
0.0021599

0.0000079
0.2497000
0.0035652
0.0017413
0.0329561
0.0146302
0.0379138
0.0030059
0.0091246

0.9861448
1.0177333
0.2719139
0.2791156
0.0067302
0.2617668
0.7524252
0.3345421
0.2367117

21

Figures

Figure 1: Diagram of general structure of a deep neural network with I inputs, two
hidden layers with J and K hidden nodes and O outputs.

22

θ1θ2...θIh11h12h13...h1Jh21h21h21h21h22h23...h2KY1Y2...YOFirsthiddenlayerSecondhiddenlayerInputlayerOutputlayerFigure 2: State-transition diagram of the natural history model of colorectal cancer.

23

CRCDeathClinicalEarlyCRCClinicalLateCRCPreclinicalEarlyCRCPreclinicalLateCRCLargeAdenomaSmallAdenomaNormalλ1(a)λ2λ3λ4λ5λ6λ7λ81Figure 3: Generated calibration targets and its 95% CI of a cohort of 500 and 100,000
simulated individuals for adenoma-related targets cancer incidence targets, respectively.
These distributions are from 100 diﬀerent runs using the same parameter set values in
each set of runs.

24

llllllllllllllllllllPr(1+ adenomas)Adenoma Size Small506070809010050607080901003040506070AgeProportion (%)llllllllllllllllCRC Incidence EarlyCRC Incidence Late5060708090506070809050100150200250AgeIncidence (per 100,000)Figure 4: Validation of the ﬁtted ANN on the validation dataset for 36 targets.
The x and y axes represent the scaled model outputs and scaled ANN predictions,
respectively.

25

Figure 5: Prior and Marginal posterior distributions of the calibrated parameters from
the IMIS and BayCANN methods. The vertical solid lines indicate the value of the
parameters used to generate the calibration targets. PDF: probability distribution
function; IMIS: incremental mixture importance sampling; BayCANN: Bayesian
calibration with artiﬁcial neural network metamodeling.

26

l6padenopsmalll3l4l5lgl20.30.40.50.60.70.260.280.300.320.340.40.50.60.70.80.90.0100.0150.0200.0250.0300.0350.0400.200.250.300.350.400.450.500.200.220.240.260.280.300.0e+005.0e−061.0e−051.5e−052.0e−052.02.53.03.54.00.020.040.060.080.10DistributionPriorPosterior IMISPosterior BayCANNFigure 6: BayCANN calibration results. The upper panel show adenoma targets and
lower panels show cancer incidence targets by stage. Calibration targets with their
95% conﬁdence intervals are shown in black. The colored curves show the posterior
model-predicted mean, and the shaded area shows the corresponding 95% posterior
model-predictive credible interval of the outcomes.

27

