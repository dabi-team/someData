Semi-asynchronous Hierarchical Federated Learning
for Cooperative Intelligent Transportation Systems

Qimei Chen, Zehua You, and Hao Jiang,

1

1
2
0
2

t
c
O
8
1

]

G
L
.
s
c
[

1
v
3
7
0
9
0
.
0
1
1
2
:
v
i
X
r
a

Abstract—Cooperative Intelligent Transport System (C-ITS) is a
promising network to provide safety, efﬁciency, sustainability, and
comfortable services for automated vehicles and road infrastruc-
tures by taking advantages from participants. However, the com-
ponents of C-ITS usually generate large amounts of data, which
makes it difﬁcult to explore data science. Currently, federated
learning has been proposed as an appealing approach to allow
users to cooperatively reap the beneﬁts from trained participants.
Therefore, in this paper, we propose a novel Semi-asynchronous
Hierarchical Federated Learning (SHFL) framework for C-ITS
that enables elastic edge to cloud model aggregation from data
sensing. We further formulate a joint edge node association and
resource allocation problem under the proposed SHFL frame-
work to prevent personalities of heterogeneous road vehicles
and achieve communication-efﬁciency. To deal with our proposed
Mixed integer nonlinear programming (MINLP) problem, we in-
troduce a distributed Alternating Direction Method of Multipliers
(ADMM)-Block Coordinate Update (BCU) algorithm. With this
algorithm, a tradeoff between training accuracy and transmission
latency has been derived. Numerical results demonstrate the
training
advantages of
overhead and model performance.

the proposed algorithm in terms of

Index Terms—C-ITS, federated learning, asynchronous, edge

association, resource allocation.

I. INTRODUCTION

With the rapid developments of automated vehicles and
Internet-of-Vehicles, Cooperative Intelligent Transport Sys-
tems (C-ITS) is expected to provide safety, trafﬁc efﬁciency,
and comfortable infotainment services for vehicles with an
accurate neighborhood view [1]. Speciﬁcally, the C-ITS com-
ponents including vehicles, road infrastructure units, personal
devices, and trafﬁc command centers can achieve additional
trafﬁc management and convenience with the help of enhanced
visions from surrounding participants [2].

Currently, both the Wireless Access in Vehicular Environ-
ments (WAVE) and the European Telecommunications Stan-
dards Institute (ETSI) standards have been proposed con-
secutively to deﬁne spectrum allocation,
trafﬁc parameters
setting, and protocol stack to enable Quality-of-Service (QoS)
and security procedure for C-ITS [3]. Several works have
also studied the technologies for C-ITS. The authors in [4]
introduce a choreography-based service composition platform
to accelerate the reuse-based development for a choreography-
based urban coordinative application. The authors in [5]
proposed a novel topological graph convolutional network to
predict the urban trafﬁc ﬂow and trafﬁc density.

Q. Chen, Z. You, and H. Jiang are with the School of Electronic In-
formation, Wuhan University, Wuhan 430072, China (e-mail: {chenqimei;
youzehua; jh}@whu.edu.cn)

The authors in [6] analyze the relationship among security,
QoS, and safety for vehicles in C-ITS. In addition, the authors
in [7] investigate service-oriented cooperation models and
mechanisms for autonomous vehicles in C-ITS. Nevertheless,
they have neglected to explore the large amounts of trafﬁc
data generated from transportation sensor networks, which
is essential for advanced C-ITS applications like connected
and autonomous vehicles, trafﬁc control and prediction, and
road safety. Although some attempts have been made to
utilize data science for C-ITS [8], there still exists various
stuff challenges to be solved, such as privacy protection,
computational complexity, and data heterogeneity.

With the signiﬁcant advance cloud computing, big data,
transportation is a trend
intelligent
and machine learning,
and requirement. As one of the most dominant distributed
machine learning technologies, Federated Learning (FL) has
been widely studied to deal with the data science recently
[9]–[11]. The FL technology allows participant devices to
collaboratively build a shard model while preserving privacy
data locally [12]. Particularly, the prevalent FL algorithm,
namely federated averaging, allows each device to train a
model locally with its own dataset, and then transmits the
model parameters to the central controller for a global aggre-
gation [13]. Intuitively, FL presents a great potential for C-
ITS to facilitate the large data management. However, directly
applying FL to C-ITS still faces three major deﬁciencies: 1)
limited wireless resources; 2) high latency; 3) obliterated data
diversity.

According to [18], [19], Federated Learning training at
Edge networks (FEL) has been regarded as a solution to
facilitate the above limitations through bringing model train-
ing closer to the data produced locally. Compared with the
conventional cloud centric FL approaches, the implementation
of FEL can provide high wireless resources utilization since
less information is required to be transmitted to the cloud.
In addition, FEL has a much lower transmission latency and
higher privacy than the conventional FL by making decisions
at the edge nodes. In [15], the authors develop an importance
aware joint data selection and resource allocation algorithm to
maximize the resource and learning efﬁciencies. Meanwhile,
the authors in [16] propose an adaptive federated learning
mechanism in resource constrained edge computing systems.
Along the FEL, the authors in [17] propose a novel Hierarchi-
cal Federated Edge Learning (HFEL) framework, where edge
servers deployed with base stations ﬁxedly and can upload
edge aggregation model to the cloud. The above HFEL enables
great potentials in low latency and energy efﬁciency.

However, the participant C-ITS devices usually have hetero-

 
 
 
 
 
 
geneous resources, which lead to non-independent-identically
distributed (non-iid) private data during the communication
[20], [21]. The existing federated learning methods mainly
utilize the synchronous model aggregation mechanism, where
the central server needs to wait for the slowest device to
complete the training in each communication round [14], [22].
With heterogeneous data, the transmission latency of each
synchronous model aggregation mechanism is unacceptable
for time-sensitive devices. In this way, several works have
proposed asynchronous model aggregation methods, where
only one participant device would update the global model
each time [24]–[26]. Nonetheless, the training round under
asynchronous methods is several
times than that of syn-
chronous methods. Moreover, due to the asynchrony, gradient
the
staleness may be difﬁcult
authors in [23] design an n-softsync aggregation model that
can signiﬁcantly reduce training time by combines the beneﬁts
of both synchronous and asynchronous aggregations.

to control [26]. Therefore,

Inspired by the above analyses, we aim to leverage a novel
Semi-asynchronous Hierarchical Federated Learning (SHFL)
framework that can provide safety, efﬁciency, sustainability,
and comfortable services to C-ITS. Speciﬁcally, the proposed
SHFL framework consists of both edge and cloud layers,
where each edge node aggregates all of homogeneous local
models and the cloud layer aggregates parts of heterogeneous
edge models. These selected nodes would update the global
model once the selected slowest node ﬁnishes training, which
combines the merits of both synchronous and asynchronous
aggregations. For further performance enhancement, we for-
mulate a joint edge node association and resource allocation
optimization problem to prevent heterogeneous edge node
personalities as well as ensure communication-efﬁcient of
the whole system. The objective function is a Mixed Integer
NonLinear Programming (MINLP) problem, which has been
solved by a distributed Alternating Direction Method of Mul-
tipliers (ADMM)-Block Coordinate Update (BCU) algorithm.
It is shown that the proposed algorithm can achieve near
optimal with low computational complexity. To the best of
our knowledge, this is the ﬁrst work that applying improved
FL into the C-ITS.

Overall, the main contributions of this work can be listed

as follows.

• We propose a novel SHFL framework by applying the
synchronous aggregation model for local-edge and the
semi-asynchronous aggregation model for edge-cloud to
provide safety, trafﬁc efﬁciency, and comfortable info-
tainment services for C-ITS.

• To reserve the personalities of heterogeneous edge nodes,
we introduce an elastic edge model update method based
on the distance between the global model and the edge
model.

• We formulate a joint edge node association and resource
allocation problem to achieve communication-efﬁciency
by achieving a tradeoff between training accuracy and
transmission latency. A distributed ADMM-BCU algo-
rithm has been used to solve the MINLP problem.

The rest of this paper is organized as follows. Section II

2

Fig. 1.

Illustration for the SHFL based C-ITS.

introduces the system model and the SHFL learning mecha-
nism. In Section III, we formulate the communication-efﬁcient
problem. A joint edge node association and resource allocation
strategy is presented in Section IV. Section V presents the
numerical results, followed by the conclusions in Section VI.

II. SYSTEM MODEL

In this work, we aim to design a novel SHFL framework for
C-ITS that contains three layers, namely the cloud layer, the
edge layer, and the local layer, as shown in Fig. 1. Here, we
consider the vehicle devices have heterogeneous data struc-
tures, namely the local datasets are non-iid. We let homoge-
neous devices with similar data size, network bandwidth, and
QoS gather in the same edge node. Hence, the edge nodes are
heterogeneous. A shared Deep Neural Network (DNN) model
is distributed over the local devices, which has been trained
collaboratively across the devices under their datasets. Dif-
ferent from conventional FLs, the proposed SHFL framework
allows devices train their data locally, homogeneous devices
report their computed parameters to the same edge node syn-
chronously, and heterogeneous edge nodes upload their models
to the cloud node semi-asynchronously, which can preserve
data privacy as well as improve communication efﬁciency. In
the proposed framework, we assume there has a set of K
edge nodes K = {1, ..., K}. Any edge node k consists of a
set of Nk local devices, denoted as Nk = {Lk,1, ..., Lk,Nk }.
Under edge node k ∈ K, local device n ∈ Nk owns a local
data set Dk,n = {(xj,k,n, yj,k,n) : j = 1, ..., |Dk,n|}, where
xj,k,n is the j-th input training data sample, yj,k,n is the j-
th corresponding output, and |Dk,n| denotes the cardinality
of the data set Dk,n. For simplicity, we assume the SHFL
algorithm with a single output. However, this work can be
extended to the multiple outputs case. In what follows, we
would introduce each part of the proposed SHFL framework
at the t-th iteration.

ITS control centerEdgeLocalLocalLocalCloudEdgeEdgeA. Edge Aggregation

The edge aggregation stage contains three processes, includ-
ing local model computation, local model transmission, and
edge model aggregation. In detail, local model ﬁrst trained
by local data, then local models respectively transmit to their
associated edge nodes for edge aggregation. The detailed
processes are as follows.

1) Local Model Computation: Without loss of generality,
we consider a supervised machine learning task on device
n ∈ Nk associated with edge node k ∈ K, which has a learn-
ing model of wk,n. We further deﬁne fn(xj,k,n, yj,k,n, wk,n)
as the loss function of data sample j that quantiﬁes the
prediction error between data sample xj,k,n and output yj,k,n.
In this work, we mainly focus on the logistic regression
model for the loss function, i.e., fn(xj,k,n, yj,k,n, wk,n) =
− log
j,k,nwk,n
. Hence, the loss func-
tion of device n ∈ Nk associated with edge node k ∈ K on
dataset Dk,n can be deﬁned as

−yj,k,nxT

1 + exp

(cid:17)(cid:17)

(cid:16)

(cid:16)

Fk,n(wk,n) =

1
|Dk,n|

|Dk,n|
(cid:88)

j=1

fk,n(xj,k,n, yj,k,n, wk,n),

(1)

∀k ∈ K, n ∈ Nk.

The local update model of device n ∈ Nk in edge node

k ∈ K can be achieved by

wt

k,n = wt−1

k,n − η∇Fk,n(wt−1

k,n ), ∀k ∈ K, n ∈ Nk,

(2)

where η is a predeﬁned learning rate.

Deﬁne Ck,n as the number of CPU cycles for local device
n ∈ Nk associated with edge node k ∈ K to process one
sample data. Assuming each sample data has the same size,
the total CPU cycles to run one local iteration is Ck,n|Dk,n|.
We further let fk,n be the computation frequency of device
n ∈ Nk in edge node k ∈ K. In this way, the related local
gradient calculation latency in one round can be formulated
as

T c
k,n =

Ck,n|Dk,n|
fk,n

, ∀k ∈ K, n ∈ Nk.

(3)

2) Local Model Transmission: We adopt the Orthogonal-
Frequency-Division Multiple Access (OFDMA) technique for
local uplink transmissions. Deﬁne Bk,n as the bandwidth
allocated to device n ∈ Nk. Therefore, we have (cid:80)Nk
n=1 Bk,n =
Bk, where Bk is the bandwidth allocated to edge node k ∈ K
for the transmission between edge node k ∈ K and the
associated local devices. Meanwhile, we have (cid:80)K
k=1 Bk ≤ Be,
where Be is the total bandwidth allocated for the communica-
tion between edge nodes to the local devices. Therefore, the
achievable local uplink data rate from device n ∈ Nk to edge
node k ∈ K can be formulated as
Pk,ngk,n
Bk,nN0

ru
k,n = Bk,n log2

, ∀k ∈ K, n ∈ Nk,

1 +

(4)

(cid:18)

(cid:19)

where Pk,n is the uplink transmission power of device n ∈ Nk
in edge node k ∈ K, gk,n denotes the channel gain between
local device n ∈ Nk and edge node k ∈ K, and N0 means the
noise power.

Similarly, the achievable downlink data rate for device n ∈

Nk associated with edge node k ∈ K can be expressed as

3

(cid:18)

(cid:19)

rd
k,n = Bk log2

Pkgk,n
BkN0
where Pk is the downlink transmission power of edge node
k ∈ K.

, ∀k ∈ K, n ∈ Nk,

1 +

(5)

In this work, we use the same training model for the
whole communication system. Therefore, the number of model
parameters in each level of model transfer has the same size.
Denote Z as the data size of the model parameter bits. The
local gradient upload latency of device n ∈ Nk in edge node
k ∈ K can be expressed as

T u
k,n =

Z
ru
k,n

=

(cid:16)

Z
1 + Pk,ngk,n
Bk,nN0

Bk,n log2

(cid:17), ∀k ∈ K, n ∈ Nk.

(6)
Correspondingly, the edge model download latency of de-

vice n ∈ Nk in edge node k ∈ K can be formulated as
Z
1 + Pkgk,n
BkN0

Z
rd
k,n

T d
k,n =

Bk log2

(cid:17), ∀k ∈ K, n ∈ Nk. (7)

=

(cid:16)

3) Edge Model Aggregation: In this work, each edge node
can receive the updated model parameters from its associated
homogeneous devices. Since the devices under one edge node
usually have a similar type, we adopt the synchronous aggrega-
tion method to average these updated models. It means that the
edge node would wait for the slowest node to complete training
in each round and collect all the connected devices’ updated
model parameters. Therefore,
the edge model aggregating
equation for edge node k ∈ K can be formulated as

wt

k =

(cid:80)Nk

n=1 |Dk,n|wt
|Dk|

k,n

, ∀k ∈ K,

(8)

where |Dk| = (cid:80)Nk
edge node k ∈ K.

n=1 |Dk,n| is the total number of data in

We omit edge model aggregation time due to its strong com-
puting capability. Hence, the computation and communication
latency between each edge k ∈ K and the related local devices
can be derived as
T edge
k

(cid:1) , ∀k ∈ K.

k,n + T d
k,n

k,n + T u

(cid:0)T c

(9)

= max
n∈Nk

B. Cloud Aggregation

Similarly, the cloud aggregation stage contains two pro-
cesses, i.e., edge model transmission and cloud model ag-
gregation. Particularly, the selected edge nodes upload their
updated model parameters to the cloud for aggregation. The
detailed processes are as follows.

1) Edge Model Transmission: Edge nodes would upload
their model parameters to the cloud after edge model aggre-
gations. To ensure uninterrupted transmission from edge to
cloud, we also adopt the OFDMA technique. Hence, the uplink
data rate for edge node k ∈ K can be expressed as
(cid:18)

(cid:19)

ru
c,k = Bc,k log2

1 +

, ∀k ∈ K,

(10)

Pc,kgc,k
Bc,kN0

4

asynchronous aggregation method, we can achieve a balance
between training accuracy and communication latency. The
semi-asynchronous aggregation method can be written as

wt

c = wt−1

c +

(cid:88)

k∈S t

|Dk|
k=1 |Dk|

(cid:80)K

(wt

k − wt−1

c

).

(15)

Also, we ignore the cloud model aggregation latency due
to its strong computing capability. Therefore, the cloud-edge
communication latency can be derived as

c,k + T d
Towards this end, the one-round latency for edge node k ∈

c,k, ∀k ∈ K.

T cloud
k

= T u

(16)

K is given by

Tk = T edge

k

+ T cloud
k

, ∀k ∈ K.

(17)

C. Edge Update Model

From Eq. (2), the local updated models are determined
by their own characteristics. Since the non-iid devices that
connected with one edge node have a similar characteristic,
the edge aggregation models are heterogeneous. Therefore, if
we directly use the cloud model to update the edge models,
the personalities among edge models would be eliminated.
Meanwhile, the accuracy of the cloud model would be de-
creased. Hence, we introduce a new edge update model based
on [28], which deﬁnes a weight distance formula to represent
the difference among different weight relatives as

dist (cid:0)wt

k, wt
c

(cid:1) =

||wt

k − wt
c||
||wt
c||

, ∀k ∈ K.

(18)

Intuitively,
model difference.

the larger of dist (wk, wc),

the greater of the

Typically, deep learning networks that consist of multiple
layers and each layer contains various amounts of weights
can be adopted here. For simplicity, we use a small dataset
to obtain the layer with the most obvious characteristics,
which has been denoted as L = {(cid:96)1, (cid:96)2, · · · }. Thereafter, we
introduce a parameter εk to measure the difference between
the cloud model and edge model k, which can be formulated
as

εk =

1
|L|

(cid:88)

(cid:96)∈L

(cid:16)

dist

wt,(cid:96)

k , wt,(cid:96)

c

(cid:17)

, ∀k ∈ K,

(19)

k and wt,(cid:96)
c

where wt,(cid:96)
edge model wt
cardinality of L.

represent the weight of the (cid:96)-th layer of
c. Meanwhile, |L| is the

k and cloud model wt

From Eq. (19), εk increases with dist (wk, wc). To keep
the personalities, the edge updated model can be derived by

wt

k ← εkwt

c + (1 − εk)wt

k, ∀k ∈ K.

(20)

D. Learning Procedure of the SHFL Model

Based on the deﬁnition of SHFL model, the training pro-
cedure of the SHFL model at the t-th iteration proceeds as
follows, which is also shown in Fig. 3.

1) Local model training and update: Devices in C-ITS train
their learning model and calculate their local gradient
as ∇Fk,n(wt
k,n), ∀k ∈ K, n ∈ Nk. After receiving

Fig. 2.

Illustration of the semi-asynchronous process.

where Bc,k is the bandwidth allocated to edge node k ∈ K
transmits to the cloud node, Pc,k is the uplink transmission
power of edge node k ∈ K to the cloud node, and gc,k denotes
the channel gain between edge node k ∈ K and the cloud node.
Correspondingly, the downlink data rate from the cloud

node to edge node k ∈ K can be formulated as

rd
c,k = Bc log2

(cid:18)

1 +

Pcgc,k
BcN0

(cid:19)

, ∀k ∈ K,

(11)

where Pc is the downlink transmission power of the cloud
node, Bc is the total bandwidth for the transmission between
the edge nodes and the cloud. As we would discuss later,
only parts of the edge nodes can be selected in each round.
Therefore, we have the constraint of

K
(cid:88)

k=1

αkBc,k ≤ Bc,

(12)

where αk ∈ {0, 1}. Here, αk = 1, ∀k ∈ K indicates edge node
k has been selected, and αk = 0, ∀k ∈ K otherwise.

In this way, the upload latency from edge node k ∈ K to

the cloud node can be written as
Z
1 + Pc,kgc,k
Bc,kN0

Bc,k log2

Z
ru
c,k

T u
c,k =

=

(cid:16)

(cid:17), ∀k ∈ K.

(13)

Similarly, the downlink latency from the cloud node to edge

node k ∈ K can be expressed as

T d
c,k =

Z
rd
c,k

=

(cid:16)

Z
1 + Pcgc,k
BcN0

Bc log2

(cid:17), ∀k ∈ K.

(14)

2) Cloud Model Aggregation: Since these edge nodes cor-
respond to heterogeneous local datasets, their model updated
periods various. If we adopt
the synchronous aggregation
model, the latency for faster training nodes is unacceptable.
On the contrary, the asynchronous method has shorter round
latency, however, it requires several times of training rounds
than the synchronous method. Therefore, in this work, we
propose a ﬂexible semi-asynchronous aggregation method by
combining the merits of both synchronous and asynchronous
methods. As shown in Fig. 2, the cloud node would select
|S t| = (cid:80)K
k=1 αk edge nodes with the fastest training round
for model aggregation, where the set of selected edge nodes
is denoted as S t. Slow nodes would wait for the next commu-
nication round to upload their models. Hence, under the semi-

E1EKt...②:Cloud model aggregation, and broadcast period;①②③③①①:Local training, local model uploading, edge model aggregation, and edge model update period;③:Edge model upload.E2E3wait data5

With out loss of generality, we leverage the norm of GNV to
present the importance, which can be written as

σt
k,n =

2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)gw,t
(cid:12)

k,n

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

, ∀k ∈ K, n ∈ Nk.

(22)

Since an edge node connects homogeneous local devices,
the GNVs among these local devices are approximately equal.
Moreover, local devices in one edge node also have similar
training duration, hence, all of these training models (GNV)
would be uploaded. In this way, the GNV of edge node k ∈ K
can be deﬁned as

σt
k =

Nk(cid:88)

n=1

σt
k,n, ∀k ∈ K.

(23)

On the contrary, the cloud node associates with heterogeneous
edge nodes, the GNVs among them various. Intuitively, edge
nodes with signiﬁcant gradients have more contributions on
model updating and convergence. Therefore, the cloud would
impactive edge nodes to upload their
preferentially select
information for cloud model aggregation. Then, the GNV of
the cloud model can be written as

σt =

K
(cid:88)

k=1

kσt
αt
k.

(24)

For easy of expression, we remove the iteration t in the
following.

Now, we are ready to describe the problem formulation. The
goal of this work is to maximize communication-efﬁcient via
joint edge node selection and resource allocation scheduling
for a SHFL based C-ITS network. To accelerate the learning
process, it is desirable to select more edge nodes with larger
data importance. However, to shorten the communication and
computation latency, it is better to upload as fewer edge nodes
as possible. As a result, the objective function that represents
the tradeoff between GNVs and transmission latency can be
formulated as

(cid:32)

min
α,Bk,n,Bc,k

−ρ

K
(cid:88)

k=1

subject to

αkσk + (1 − ρ) max
k∈K

αkTk

,

(25)

(cid:33)

K
(cid:88)

Nk(cid:88)

k=1

n=1

Bk,n ≤ Be,

K
(cid:88)

k=1

αkBc,k ≤ Bc,

αk ∈ {0, 1}, ∀k ∈ K,

(25a)

(25b)

(25c)

where Bc = B − Be, B is
the total bandwidth,
α = [α1, α2, · · · , αK]T, Bk,n = [Bk,1, Bk,2, · · · , Bk,Nk ]T,
Bc,k = [Bc,1, Bc,2, · · · , Bc,K]T, and ρ ∈ [0, 1] is the weight
factor that controls the tradeoff between data importance and
transmission latency.

Obviously, (25) is a MINLP problem, which is NP-hard. In
the following, we would introduce an ADMM-BCU method
to ﬁnd the joint edge node selection and resource allocation
strategy.

Fig. 3. Learning Procedure of the proposed SHFL model.

k, ∀k ∈ S t, devices in the selected edge nodes update

wt
their learning model based on Eq. (2).

2) Local model upload: Local devices upload their concrete
models to related edge nodes based on the local-edge
bandwidth allocation scheme.

3) Edge model aggregation: After receiving local models,
each edge node computes the average edge model based
on Eq. (8). Since device types among edge nodes are
heterogeneous, their edge model aggregations are semi-
asynchronous.

4) Edge node selection and resource management: Based
on the reports of edge nodes, the cloud node selects a
subset of edge nodes S t with the fastest training round
and determines the bandwidth allocation.

5) Selected edge model upload: The selected edge nodes
upload their aggregated models to the cloud node.
6) Cloud model aggregation and broadcast: The cloud
server aggregates the uploaded edge models, and then
broadcasts the current aggregated model wt
c to the
selected edge nodes.

7) Edge model update and broadcast: The selected edge
k to local related

servers broadcast the updated model wt
devices.

The procedure starts from t = 1 and repeats the above steps

until convergence.

III. PROBLEM FORMULATION
As discussed earlier, there exists a tradeoff between the
training accuracy and the transmission latency. Therefore, our
goal in this work is to ﬁnd a balance between them to provide
safety, efﬁciency, sustainability, and comfortable services for
the SHFL based C-ITS framework.

According to Eq. (2), the local model Gradient-Norm-Value
(GNV) inﬂuences the local model updating, which measures
the data importance. The GNV of local device n ∈ Nk in edge
node k ∈ K can be expressed as

gw,t
k,n = ∇Fk,n(wt
(cid:88)

k,n)

=

∂fk,n(xj,k,n, yj,k,n, wt
∂wt

k,n)

k,n

Dk,n

, ∀k ∈ K, n ∈ Nk.

(21)

Local DeviceLocal DeviceEdge Node1Local DeviceLocal DeviceEdge Node2Local DeviceLocal DeviceEdge NodeK............                         Upload timeUpload timeUpload timeEdge broadcastCloud broadcast Edge updateEdge upload Local trainingLocal uploadInformation uploadEdge node selection           IV. JOINT EDGE NODE SELECTION AND RESOURCE
ALLOCATION

As known to us, all of the steps in the learning procedure
are independent with the optimal scheduling decision. Denote
k,n) + T u
k = maxn∈Nk (T c
T
c,k, the original problem (25)
can be rewritten as

k,n + T u

(cid:48)

min
α,Bk,n,Bc,k

max
k∈K

(cid:40)

−ρ

K
(cid:88)

k=1

(cid:48)
αkσk + (1 − ρ)αkT
k

(cid:41)

,

(26)

subject to (25a), (25b), and (25c).

the

solve

we
To
denote X
and
ﬁrst
Y = maxk∈K(−ρ (cid:80)K
c,k)). By
applying the parametric method [29], (26) can be transformed
into

above min-max
(cid:16)
k,n + T u
T c
=
maxn∈Nk
k,n
k=1 αkσk + (1 − ρ)αk(X + T u

problem,
(cid:17)

6

(29) is decomposed into edge node selection and resource
allocation subproblems, which aim to solve the blocks of
{α, X, Y } and { ˜α, Bk,n, Bc,k}, respectively.

a) The optimal edge node selection {α, X, Y }: Under
the ﬁxed resource allocations block { (cid:101)α, Bk,n, Bc,k},
the
edge node selection optimization subproblem over the variable
block {α, X, Y } can be rewritten as

min
α,X,Y

Y +

1
2ν

K
(cid:88)

k=1

[αk (1 − ˜αk) + νλk]2

+

1
2ν

K
(cid:88)

(cid:16)

k=1

αk − ˜αk + ν ˜λk

(cid:17)2

,

(30)

subject to (25b), (27a), (27b), and (28c).

Obviously, (30) is a convex problem, which can be solved

min
α,Bk,n,Bc,k,X,Y

Y,

(27)

by standard tools, such as CVX.

subject to (25a), (25b), (25c), and

k,n + T u
T c

k,n ≤ X,

− ρ

K
(cid:88)

k=1

αkσk + (1 − ρ)αk(X + T u

c,k) ≤ Y.

(27a)

(27b)

Nevertheless, (27) is still a unsolvable MINLP problem. We
then introduce an auxiliary variable ˜α to deal with the binary
vector α [30]. Thereafter, (27) can be reformulated as

min
α,Bk,n,Bc,k,X,Y

Y,

subject to (25a), (25b), (25c), (27a), (27b), and

α − ˜α = 0,

αk(1 − ˜αk) = 0,

0 ≤ αk ≤ 1.

(28)

(28a)

(28b)

(28c)

Problem (26) is transferred as a convex problem with equal-
ity constraints now. Hereinafter, we introduce the Augmented
Lagrangian (AL) method to solve problem (28) through pe-
nalizing and dualizing the equality constraints (28a) and (28b)
as

In what follows, we provide the closed form expression of

the optimal edge node selection by introducing Lemma 1.

Lemma 1. The optimum edge node selection α∗ can be
expressed as

νMk + νκk

(cid:16)

α∗

k =

ρσk − (1 − ρ)
(1 − ˜αk)2 + 1

(cid:16)

X + T u
c,k

(cid:17)(cid:17)

, ∀k ∈ K,

(31)
where Mk = ψk − ξk − λk (1 − ˜αk) − ˜λk − φkBc,k + 1
ν ˜αk.
φ, ζ, κ, ψ, and ξ are Lagrangian multipliers correspond
to constraints (25b), (27a), (27b), and (28c), which can
be found by one-dimensional search methods based on the
complementary slackness conditions.

Proof. To ﬁnd the optimal αk, ∀k ∈ K, we apply for the
Lagrangian dual method, which can rearrange Eq. (30) with
respect to αk, ∀k ∈ K as

L(αk, φk, ζk, κk, ψk)

+

1
2ν

K
(cid:88)

k=1

[αk (1 − ˜αk) + νλk]2

Y

=

(cid:80)K

k=1 σkαk
K
(cid:88)

(cid:16)

+

1
2ν

αk − ˜αk + ν ˜λk

(cid:17)2

min
α, ˜α,Bk,n,Bc,k,X,Y

Y +

1
2ν

+

1
2ν

K
(cid:88)

[αk (1 − ˜αk) + νλk]2

k=1
K
(cid:88)

(cid:16)

k=1

αk − ˜αk + ν ˜λk

(cid:17)2

,

(29)

+ φk

+ ζk

+ κk

k=1
(cid:32) K
(cid:88)

(cid:33)

(32)

αkBc,k − Bc

− ψkαk − ξk (1 − αk)

k,n − X(cid:1)

(cid:0)T c
(cid:32)

k=1
k,n + T u
K
(cid:88)

−ρ

αkσk + (1 − ρ) αk

(cid:0)X + T u

c,k

(cid:1) − Y

(cid:33)

.

subject to (25a), (25b), (27a), (27b), and (28c).

Here, ν is the non-negative penalty parameter, and λ =
[λ1, λ2, ..., λK] and (cid:101)λ = [(cid:101)λ1, (cid:101)λ2, ..., (cid:101)λK] denote the dual
variable vectors correspond to constraints (28a) and (28b),
respectively.

Nevertheless, (29) is still a coupled problem due to the
multiply variables of Y , Bk,n, and Bc,k. Therefore, in this
work, we propose a distributed ADMM-BCU algorithm that
can iteratively approach a near optimal stable solution with low
computational complexity. Speciﬁcally, during each iteration,

k=1

Calculate the ﬁrst-order partial derivatives with respect to

αk, ∀k ∈ K, we derive that
∂L(αk, φk, ζk, κk, ψk)
∂αk

(cid:104)

1
ν

αk (1 − ˜αk)2 + αk − ˜αk

=
+ ˜λk + φkBc,k − ψk + ξk
+ κk

(cid:0)−ρσk + (1 − ρ) (cid:2)X + T u

(cid:3)(cid:1) , ∀k ∈ K.

c,k

(cid:105)

+ λk (1 − ˜αk)

(33)

= 0, we have

Setting ∂L(αk,µk,φk,ζk)
(cid:16)

∂αk

νMk + νκk

α∗

k =

ρσk − (1 − ρ)(X + T u
(1 − ˜αk)2 + 1

(cid:17)
c,k)

L ( ˜αk, Bk,n, Bc,k, βk, κk, ϕk, τk)

,

(34)

= Y +

1
2ν

K
(cid:88)

k=1

[αk (1 − ˜αk) + νλk]2

where Mk = ψk − ξk − λk (1 − ˜αk) − ˜λk − φkBc,k + 1
This ends the proof.

ν ˜αk.

+

1
2ν

+ βk

K
(cid:88)

(cid:16)

αk − ˜αk + ν ˜λk

(cid:17)2

k=1
(cid:32) K
(cid:88)

Nk(cid:88)

(cid:33)

Bk,n − Be

+ κk

7

(cid:33)

αkBc,k − Bc

(36)





(cid:32) K
(cid:88)

k=1

(cid:16)

Z
1 + Pk,ngk,n
Bk,nN0

(cid:17) +

Ck,nNk,n
fk,n

− X

Bk,n log2

k=1

n=1

+ ϕk





(cid:32)

+ τk

−ρ

αkσk

K
(cid:88)

k=1


+(1 − ρ)αk

X +

Bc,k log2





 − Y

 ,

(cid:17)

(cid:16)

Z
1 + Pc,kgc,k
Bc,kN0

where β, κ, ϕ, and τ are the Lagrangian multipliers corre-
sponding to constraints (25a), (25b), (35a), and (35b), respec-
tively.

By taking ∂L( ˜αk,Bk,n,Bc,k,βk,κk,ϕk,τk)
local-edge uplink bandwidth allocation B∗
by

∂Bk,n

= 0, the optimal
k,n can be derived

Z

ϕk

(cid:16)

(cid:16)

B∗

k,n log2

1 + Pk,ngk,n
B∗
k,nN0


Pk,ngk,n
(cid:16)
Pk,ngk,n + B∗

k,nN0

(cid:34)

(cid:32)

log2

1 +

(cid:33)

−

Pk,ngk,n
B∗
k,nN0

(cid:17)(cid:17)2

 = βk, ∀k ∈ K, n ∈ Nk.

(37)

(cid:17)

ln 2

From Lemma 1, we ﬁnd that the edge node selection is
mainly determined by the edge node importance σk and the
uplink transmission latency from edge k to the cloud T u
c,k.
Intuitively, the cloud preferentially selects the edge node with
either a larger edge node importance or a smaller uplink
transmission latency that can improve the communication-
efﬁciency.

b) The optimal bandwidth allocation { (cid:101)α, Bk,n, Bc,k}:
Similarly, under
the ﬁxed edge node selection block
{α, X, Y }, the resource allocation optimization subproblem
over the block { (cid:101)α, Bk,n, Bc,k} can be rearranged as

min
(cid:101)α,Bk,n,Bc,k

1
2ν

K
(cid:88)

k=1

[αk (1 − ˜αk) + νλk]2

+

1
2ν

K
(cid:88)

(cid:16)

k=1

αk − ˜αk + ν ˜λk

(cid:17)2

,

(35)

subject to (25a), (25b), and

(cid:16)

Z
1 + Pk,ngk,n
Bk,nN0

(cid:17) +

Ck,nNk,n
fk,n

Bk,n log2

≤ X,

(35a)

− ρ

K
(cid:88)

k=1

αkσk + (1 − ρ)αk·



X +

(cid:16)

Z
1 + Pc,kgc,k
Bc,kN0

Bc,k log2



 ≤ Y.

(cid:17)

(35b)

Also, it is easy to observe that (35) is a convex problem. For
ease of analyses, we write this problem under the Lagrangian
dual formulation, where (35) can be rearranged as

From Eq. (37), the optimal local-edge bandwidth allocation
B∗
k,n is mainly inﬂuenced by the related channel conditions
Pk,ngk,n
N0

.

Algorithm 1 Joint edge node selection and resource allocation
strategy.

1: Initialize the gradient norm value σk, ∀k ∈ K.
2: Set the minimum successive divergence threshold of the
objective function (cid:15)min and the maximum iteration number
Rmax.

3: Set the iteration number  = 0.
4: Initialize the auxiliary variables ˜α(), λ(), ˜λ().
5: While (cid:15)() ≥ (cid:15)min and  ≤ Rmax do
6:

Calculate the optimal device selection decision
according to (34).
Calculate the optimal bandwidth B∗()

k,n , B∗()

c,k accord-

α∗()
k

k

according to (41).

ing to (37) and (40).
Obtain ˜α∗()
Update λ() and ˜λ() according to
1
ν
1
ν

λ() = λ(−1) +

α∗(−1) (cid:16)

= ˜λ

(−1)

˜λ

+

()

(cid:16)

1 − ˜α∗(−1)(cid:17)

,

(38)

α∗(−1) − ˜α∗(−1)(cid:17)

.

(39)

7:

8:

9:

Update (cid:15)() according to (42).
Set  =  + 1.

10:
11:
12: End while

Alternatively, by taking ∂L( ˜αk,Bk,n,Bc,k,βk,κk,ϕk,τk)
the optimal edge-cloud uplink bandwidth allocation B∗
be obtained by

∂Bc,k

= 0,
c,k can

τk

(cid:16)

(cid:16)

B∗

c,k log2

(1 − ρ) Z

(cid:34)

(cid:32)

log2

1 +

(cid:33)

Pc,kgc,k
B∗
c,kN0

(cid:17)(cid:17)2

1 + Pc,kgc,k
B∗
c,kN0

 = κk, ∀k ∈ K, n ∈ Nk.

(cid:17)

−

(40)

Pc,kgc,k
Pc,kgc,k + B∗

(cid:16)

c,kN0

ln 2

Obviously, the optimal edge-cloud uplink bandwidth allocation
B∗

c,k has a similar rule with B∗
Thereafter, by setting ∂L( ˜αk,Bk,n,Bc,k,βk,κk,ϕk,τk)

= 0, we

k,n.

˜αk

can obtain the optimal auxiliary variable ˜α∗

k as

˜α∗

k =

αk (1 + αk + νλk) + ν ˜λk
1 + α2
k
The detailed procedure for the joint edge node selection and

, ∀k ∈ K.

(41)

resource allocation scheduling is presented in Algorithm 1.

In Algorithm 1, (cid:15)() means the successive divergence of
the objective function at the -th iteration [30], which can be
deﬁned as

(cid:15)() = F () − F (−1),
(cid:80)K

(42)
k=1 [αk (1 − ˜αk) + νλk]2 +

= Y + 1
2ν
(cid:17)2

αk − ˜αk + ν ˜λk

.

where F
(cid:16)
(cid:80)K

k=1

1
2ν

V. NUMERICAL RESULTS

In this section, we conduct experiments to evaluate the
theoretical analyses and test the performance of the proposed
algorithm.

8

A. Experiment Settings

CNN model settings: For exposition, we consider
the
learning task of training image classiﬁers, which are imple-
mented on a Convolutional Neural Network (CNN) model,
namely VGGNet 16 [31]. The corresponding training dataset
is CIFAR-10, which contains 50000 training images and 10000
testing images with 10 categories. To simulate the distributions
of heterogeneous data based mobile devices, all data samples
are ﬁrst sorted by digital labels, and then divided into 100
shards of size 500 and each local device is assigned with 5
shards. The batch size of each local device is set as 50 and
the average quantitative bit number of each parameter is set as
16 bits. In addition, we adopt the Stochastic Gradient Descent
(SGD) optimizer, and the learning rate for the CNN model is
set as 0.1. The computation frequency of each local device is
randomly set between 2 GHz to 4 GHz.

Wireless communication settings: We consider a hierarchical
SHFL communication network consists of one cloud node
and 10 edge nodes. Each edge node connects with two local
devices. Both edge nodes and local devices are uniformly
distributed under the coverage of the cloud node. The total
bandwidth is set as 20 MHz. Moreover, the uplink transmission
powers of each local device and edge node are set as 10 dBm
and 24 dBm, respectively. Also, the downlink transmission
powers of each edge node and the cloud node are set as
10 dBm and 24 dBm, respectively. Furthermore, we utilize
the transmission pass loss model of 128.1 + 37.6 log(d[km]).
Meanwhile, the noise power spectral density is set as N0 =
−174 dBm/Hz.

In the ADMM-BCU algorithm, we set the non-negative
penalty parameter ν as 1. The minimum successive divergence
threshold (cid:15)min is set as 10−4. In addition, the maximum
iteration number of ADMM-BCU algorithm is set as 200.

B. SHFL Performance

In this subsection, we present the convergence performance
of the proposed SHFL model. We ﬁrst introduce the following
baselines.

• Random selection: Under this circumstance, CNN is
implemented with random data selection, where both
5 and 8 edge nodes randomly selective conditions are
respectively considered.

• Full selection: Under this circumstance, CNN is imple-

mented by selecting all of the edge nodes.

• Normal edge update: Edge nodes directly use the broad-

cast cloud model as their updated model.

For simplicity, we assume the transmissions from the selected
edge nodes to the cloud node are uniformly allocated, totally
5 MHz. Meanwhile, the transmission bandwidths from local
devices to edge nodes are also set as the same, totally 15 MHz.
Moreover, we set the weighted factor ρ under the proposed
algorithm as 0.8. Fig. 4 shows the convergence performance
of the proposed CNN based SHFL model. From this ﬁgure,
we can ﬁnd that the VGG-16 network starts to converge at
about 70 communication rounds for both the random selection
scheme with 8 edge nodes, the Full selection scheme, and the
proposed scheme. However, the random selection scheme with

9

(a) Training accuracy.

(a) Training accuracy.

(b) Training loss.

(b) Training loss.

Fig. 4. Convergence performance of the proposed CNN model under different
algorithms.

Fig. 5. Performance inﬂuence from the edge update model.

5 edge nodes presents the worst convergence performance.
Intuitively, it is because the more devices to be selected, the
larger data information can be provided to the neural network,
and thus faster convergence. Moreover, due to the non-iid
datasets, each node has different contributions. Therefore,
the random selection scheme may play a side effect on the
whole model, leading to a decreasing model accuracy. Overall,
the proposed algorithm shows a near to the full selection
scheme convergence and accuracy, which can achieve better
performance than the baselines that would be discussed latter.
Fig. 5 presents the performance inﬂuence from the edge
update model. From this ﬁgure, we ﬁnd that either the training
accuracy or the training loss under the elastic edge update
model is better than that of the normal edge update model.
The ﬂuctuation of these curves are mainly due to the non-iid
data form. Therefore, we can conclude that the elastic edge
update model is signiﬁcant to keep the personalities of the
edge nodes.

C. The Scheduling Performance

In this subsection, we mainly verify the scheduling perfor-
mance of the proposed algorithm. In Fig. 6, we shows that the
proposed ADMM-BCU algorithm has a fast convergence and
a low computational complexity.

Fig. 7 illustrates that a tradeoff exists between data im-
portance and the transmission latency. The value of ρ starts

Fig. 6. Convergence performance of the proposed ADMM-BCU algorithm.

from 0.4 to 0.8 under the step of 0.05. This ﬁgure shows
that a large value of ρ leads to higher data importance and
longer transmission latency, and vise versa. Thus, the operators
can select a suitable value of ρ according to their speciﬁc
requirements.

In Fig. 8, we present the performance among the number
of selected edge nodes, data importance, and latency under
various weight factors. Fig. 8(a) shows the number of selected
edge nodes and total data importance in different weight
factors under various algorithms. From this subﬁgure,
the
number of selected edge nodes increases with the weight
factor ρ. When the value of weight factor ρ is small, i.e.,

0102030405060708090100The training period0.10.20.30.40.50.60.70.80.91Test accuracyRandom selection, 5 edge nodesRandom selection, 8 edge nodesFull selectionThe proposed algorithm,  = 0.80102030405060708090100The training period00.511.522.533.5Loss functionRandom selection, 5 edge nodesRandom selection, 8 edge nodesFull selectionThe proposed algorithm,  = 0.80102030405060708090100The training period 0.10.20.30.40.50.60.70.80.91Test accuracyElastic edge updateNormal edge update0102030405060708090100The training period00.511.522.533.5Loss functionElastic edge updateNormal edge update0510152025Iteration number12.51313.51414.51515.51616.51717.5Average latency (s)4648505254565860Total data importanceLatency, = 0.8Data importance, = 0.810

devices. Under this circumstance, we can enlarge the wireless
bandwidth by some resource management technologies.

VI. CONCLUSION

This work proposes a novel SHFL framework that consists
trafﬁc
of local, edge, and cloud nodes to provide safety,
efﬁciency, and comfortable infotainment services for C-ITS.
Speciﬁcally, homogeneous devices are allowed to associate
with one edge node. Therefore, we adopt the synchronous
aggregation model for edge nodes. On the contrary, for the
heterogeneous edge aggregation models, we introduce a semi-
asynchronous aggregation model for the cloud node, where
parts of the fastest training edge models can be uploaded at
each iteration. Moreover, we investigate an effective edge-
cloud update method to keep the personalities of the edge
nodes. For communication-efﬁciency, we propose a joint edge
node association and resource allocation strategy, which illus-
trates a tradeoff between training accuracy and transmission la-
tency. A distributed ADMM-BCU algorithm has been adopted
to solve the proposed optimal MINLP problem. Numerical
results show that our proposed scheme can accelerate the
training process and improve the performance for C-ITS.

REFERENCES

[1] L. Chen and C. Englund, “Cooperative ITS, EU standards to accelerate
cooperative mobility,” in Proc. Int. Conf. Connected Vehicles Expo
(ICCVE), pp. 681 - 686, Nov. 2014.

[2] F. Qu, F.-Y. Wang, and L. Yang, “Intelligent

transportation spaces:
Vehicles, trafﬁc, communications, and beyond,” IEEE Commun. Mag.,
vol. 48, no. 11, pp. 136 ¨C 142, Nov. 2010.

[3] E. B. Hamida, H. Noura, and W. Znaidi, “Security of cooperative in-
telligent transport systems: Standards, threats analysis and cryptographic
countermeasures,” Electronics, vol. 4, no. 3, pp. 380 - 423, 2015.
[4] M. Autili, L. Chen, C. Englund, C. Pompilio, and M. Tivoli, “Cooperative
intelligent transport systems: Choreography-based urban trafﬁc coordina-
tion,” IEEE Trans. Intell. Transp. Syst., vol. 22, no. 4, pp. 2088 - 2099,
Apr. 2021.

[5] H. Qiu, Q. Zheng, M. Msahli, G. Memmi, M. Qiu, and J. Lu, “Topological
Graph Convolutional Network-Based Urban Trafﬁc Flow and Density
Prediction,”IEEE Trans. on Intel. Trans. Sys., 2020

[6] M. A. Javed and E. B. Hamida, “On the interrelation of security, QoS,
and safety in cooperative ITS,” IEEE Trans. Intell. Transp. Syst., vol. 18,
no. 7, pp. 1943 - 1957, Jul. 2017.

[7] K. Zhang, A. Yang, H. Su, ”. L. Fortelle, K. Miao, and Y. Yao, “Service-
oriented cooperation models and mechanisms for heterogeneous driverless
vehicles at continuous static criticlal sections,” IEEE Trans. Intell. Transp.
Syst., vol. 18, no. 7, pp. 1867 - 1881, Jul. 2017.

[8] J. Zhang, F.-Y. Wang, K. Wang, W.-H. Lin, X. Xu, and C. Chen, “Data-
driven intelligent transportation systems: A survey,” IEEE Trans. Intell.
Transp. Syst., vol. 12, no. 4, pp. 1624 - 1639, Dec. 2011.

[9] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated
multi-task learning,” in Proc. Adv. Neural Inf. Process. Syst., pp. 4424 -
4434, 2017.

[10] H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, “Scheduling policies
for federated learning in wireless networks,” IEEE Trans. Commun.,
vol. 68, no. 1, pp. 317 - 333, Jan. 2020.

[11] J.-H. Ahn, O. Simeone, and J. Kang, “Wireless federated distillation
for distributed edge learning with heterogeneous data,” arXiv preprint
arXiv:1907.02745, 2019.

[12] J. Kone˘cn`y, H. B. McMahan, D. Ramage, and P. Richt´arik, “Federated
optimization: Distributed machine learning for on-device intelligence,”
arXiv preprint arXiv:1610.02527, 2016.

[13] H. B. McMahan, E. Moore,D. Ramage, S. Hampson, and B. Y. A. Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” Artif. Intell. Statist., vol. 54, pp. 1273 - 1282, Apr. 2017.

[14] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-
stein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for
mobile keyboard prediction,” arXiv preprint arXiv:1811.03604, 2018.

Fig. 7. Tradeoff between data importance and delay.

(a) Data importance and the number of selected edge nodes.

(b) Latency.

Fig. 8.
importance, and latency under different mechanisms.

Performance among the number of selected edge nodes, data

the associated edge nodes are small, the proposed algorithm
has a lower data importance than the random selection scheme.
With the increment of associated edge nodes, the circumstance
changes, which has been explained in Fig. 4. However, the
full selection scheme always has the highest value of data
importance at the cost of higher latency, which is shown in
Fig. 8(b). Fig. 8(b) shows the full selection scheme suffers
the highest latency, and the proposed algorithm has the lowest
latency after scheduling. Intuitively, the transmission latency
is much lower than the total latency, which means the data
the transmission latency
training time is huge. Moreover,
may not meet the requirements of ultra low latency C-ITS

11.411.611.81212.212.412.612.81313.2Average latency (s)40424446485052Total data importance : Ascending order = 0.8 = 0.40.40.450.50.550.60.650.70.750.83840424446485052545658Total data importance66.577.588.5Number of selected edge nodesFull selection data importanceRandom selection, 8 edge nodes data importanceThe proposed algorithm data importanceNumber of selected edge nodes0.40.450.50.550.60.650.70.750.81111.51212.51313.51414.515Average latency (s)303234363840424446Transmission latency (ms)Full selection (average latency)Random selection, 8 edge nodes (average latency)The proposed algorithm (average latency)Full selection (transmission latency)Random selection, 8 edge nodes (transmission latency)The proposed algorithm (transmission latency)11

[15] Y. He, J. Ren, G. Yu, and J. Yuan, “Importance-aware data selection and
resource allocation in federated edge learning system,” IEEE Trans. Vel.
Technol., vol. 69, no. 11, pp. 13593 - 13605, Nov. 2020.

[16] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He,
and K. Chan, “Adaptive federated learning in resource constrained edge
computing systems,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp.
1205 - 1221, Jun. 2019.

[17] S. Luo, X. Chen, Q. Wu, Z. Zhou, and S. Yu, “HFEL: Joint edge
association and resource allocation for cost-efﬁcient hierarchical federated
edge learning,” IEEE Trans. Wireless Commun., vol. 19, no. 10, pp. 6535
- 6548, Oct. 2020.

[18] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks:
A comprehensive survey,” IEEE Commun. Surveys Tuts., vol. 22, no. 3,
pp. 2031 - 2063, Third Quarter, 2020.

[19] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision
and challenges,” IEEE Internet Things J., vol. 3, no. 5, pp. 637 - 646,
Oct. 2016.

[20] S.

Itahara, T. Nishio, Y. Koda, M. Morikura,

and K. Ya-
mamoto, “Distillation-based semi-supervised federated learning for
communication-efﬁcient collaborative training with non-IID private data,”
IEEE Trans. Mobile Comput., early access, 2021.

[21] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng, and C.-T.
Chou, “Semisupervised distributed learning with non-IID data for AIoT
service platform,” IEEE Inter. Things J., vol. 7, no. 10, pp. 9266 - 9277,
Oct. 2020.

[22] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy
efﬁcient federated learning over wireless communication networks,” IEEE
Trans. Wireless Commun., vol. 20, no. 3, pp. 1938 - 1949, March 2021.
[23] Y. Gao, L. Liu, X. Zheng, C. Zhang, and H. Ma, “Federated sensing:
Edge-cloud elastic collaborative learning for intelligent sensing,” IEEE
Inter. Things J., early access, 2021.

[24] C. Xie, S. Koyejo, and I. Gupta, “Asynchronous federated optimization,”

arXiv preprint arXiv:1903.03934, 2019.

[25] M. Chen, B. Mao, and T. Ma, “Efﬁcient and robust asynchronous feder-
ated learning with stragglers,” in Submitted to International Conference
on Learning Representations, 2019.

[26] Y. Chen, Y. Ning, M. Slawski, and H. Rangwala, “Asynchronous online
federated learning for edge devices with non-IID data,” in Proc. IEEE
Inter. Confer. Big Data, 2020.

[27] M. R. Sprague, A. Jalalirad, M. Scavuzzo, C. Capota, M. Neun, L.
Do, and M. Kopp, “Asynchronous federated learning for geospatial
applications,” in Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, Springer, pp. 21 - 28, 2018.

[28] Y. Zhao, M. Li, N. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
learning with non-iid data.” arXiv preprint arXiv:1806.00582, 2018.
[29] S. Geisser and W. M. Johnson, Modes of Parametric Statistical Infer-

ence. Hoboken, NJ, USA: Wiley, 2006.

[30] R. Guo, Y. Cai, M. Zhao, Q. Shi, and B. Champagne, “Joint design of
beam selection and precoding matrices for mmWave MU-MIMO systems
relying on lens antenna arrays,” IEEE J. Sel. Topics Signal Process., vol.
12, no. 2, May 2018.

[31] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

large-scale image recognition,” arXiv preprint arXiv: 1409.1556, 2015.

