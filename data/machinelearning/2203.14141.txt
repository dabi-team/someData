Efﬁcient Global Robustness Certiﬁcation of Neural
Networks via Interleaving Twin-Network Encoding

Zhilu Wang∗, Chao Huang†, Qi Zhu∗
∗Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA
Emails: zhilu.wang@u.northwestern.edu, qzhu@northwestern.edu
†Department of Computer Science, University of Liverpool, Liverpool, UK
Email: chao.huang2@liverpool.ac.uk

2
2
0
2

r
a

M
6
2

]

G
L
.
s
c
[

1
v
1
4
1
4
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—The robustness of deep neural networks has received
signiﬁcant interest recently, especially when being deployed in
safety-critical systems, as it is important to analyze how sensitive
the model output
is under input perturbations. While most
previous works focused on the local robustness property around an
input sample, the studies of the global robustness property, which
bounds the maximum output change under perturbations over the
entire input space, are still lacking. In this work, we formulate
the global robustness certiﬁcation for neural networks with
ReLU activation functions as a mixed-integer linear programming
(MILP) problem, and present an efﬁcient approach to address it.
Our approach includes a novel interleaving twin-network encoding
scheme, where two copies of the neural network are encoded
side-by-side with extra interleaving dependencies added between
them, and an over-approximation algorithm leveraging relaxation
and reﬁnement techniques to reduce complexity. Experiments
demonstrate the timing efﬁciency of our work when compared
with previous global robustness certiﬁcation methods and the
tightness of our over-approximation. A case study of closed-loop
control safety veriﬁcation is conducted, and demonstrates the
importance and practicality of our approach for certifying the
global robustness of neural networks in safety-critical systems.

I. INTRODUCTION

Deep neural networks (DNNs) are being applied to a wide
range of tasks, such as perception, prediction, planning, control,
and general decision making. While DNNs often show signif-
icant advantages in performance over traditional model-based
methods, pressing concerns have been raised on the uncertain
behaviors of DNNs under varying inputs, especially for safety-
critical systems such as autonomous vehicles and robots. Some
of the well-trained DNNs are found to be vulnerable to a small
adversarial perturbation on their inputs [1]. The robustness
metric of DNNs is deﬁned to bound such uncertain behaviors
when the input is perturbed. Speciﬁcally, the robustness of a
DNN represents how much its output varies when its input has
a bounded perturbation, where the perturbation can be either
from random noises or due to malicious attacks.

Formal methods,

such as Satisﬁability Modulo Theo-
ries (SMT) [2], [3] and Mixed-Integer Linear Programming
(MILP) [4], have been used to either ﬁnd an adversarial ex-
ample or certify the robustness (i.e., proving no adversarial ex-
ample exists) around a given input sample, which is considered

This work is supported in part by NSF grants 1834701, 1839511, 1724341,

2038853, and ONR grant N00014-19-1-2496.

This paper is published on Design, Automation and Test in Europe Confer-

ence (DATE 2022).

as local robustness. The efﬁciency is improved by certifying a
conservative (sound but not complete) local robustness bound
via over-approximated output range analysis methods [5], [6].
In safety-critical systems with DNNs involved, it is important
to analyze the DNN robustness for evaluating system safety. As
local robustness is deﬁned locally to a given input sample, to
evaluate system safety, we will need to apply local robustness
certiﬁcation during runtime for each input sample that the
system encounters or may encounter. However, the complexity
of local robustness certiﬁcation, even with conservative over-
approximation,
is too high for runtime execution in most
practical systems. Moreover, sometimes it is required to ensure
system safety for future time horizon (e.g., to verify there is
no collision for the next 3 seconds). As future input samples
are unknown, local robustness certiﬁcation cannot be applied.
Instead, we could address system safety by considering
the problem of global robustness, which measures the worst-
case DNN output change against input perturbation for all
possible input samples. The deﬁnition of the global robustness
is introduced in [2], with a proposed SMT-based tool Reluplex
for certifying the exact global robustness. In [7], the global
robustness for classiﬁcation problems is formulated into MILP
to ﬁnd the maximum input perturbation that can preserve
the classiﬁcation result. In [8], an MILP-based exact global
robustness certiﬁcation method is proposed for logic ensemble
classiﬁer (a generalized-version of decision tree, not DNN).

A major challenge for exact global robustness certiﬁcation
is its complexity – while it can be run ofﬂine to facilitate
safety analysis (since it considers the entire input space), the
intractable even in ofﬂine
high complexity often makes it
computation. A number of methods have been proposed to
tackle the complexity, however they often lack the deterministic
guarantees needed for safety veriﬁcation. For instance, dataset-
based global robustness estimation approaches are proposed
in [9], [10], which conduct local robustness certiﬁcation for
each sample in the dataset and ﬁnd the worst case or the
average. As the dataset cannot cover all possible input samples,
the derived global robustness is just an estimation with no
deterministic guarantees. Sampling-based methods [11] ran-
domly sample the DNN inputs, evaluate the local robustness
of the random-sampled inputs, and provide a probabilistic
global robustness. Similarly, they cannot provide deterministic
guarantees. Region-based robustness analyses [11], [12] divide

 
 
 
 
 
 
Fig. 1. An example neural network for illustration.

the input space into regions, where all possible inputs in
each certiﬁed-robust region have the same classiﬁcation result.
However, the number of regions could be extremely large for
DNNs with high-dimensional inputs, making it hard to certify
all regions and provide deterministic guarantees.

In this work, we propose an efﬁcient certiﬁcation approach
to over-approximate the global robustness. The derived global
robustness is sound and deterministic, the over-approximation
is tight, and the approach can be used effectively for the purpose
of safety veriﬁcation. To achieve this, our approach introduces
a novel network encoding structure, namely interleaving twin-
network encoding, to compare two copies of the neural network
side-by-side under different
inputs, with extra interleaving
dependencies added between them to improve efﬁciency. Our
approach also includes over-approximation techniques based
on network decomposition and LP (linear programming) relax-
ation, to further reduce the computation complexity. To the best
of our knowledge, our approach is the ﬁrst global robustness
over-approximation method that certiﬁes the robustness among
the entire input domain with sound and deterministic guarantee.
Experiments show that our approach is much more efﬁcient
and scalable than the exact global robustness methods such as
Reluplex [2], with tight over-approximation – our approach can
certify DNNs with more than 5k neurons in 5 hours while the
exact methods cannot certify DNNs with more than 64 neurons
in 1 day. To demonstrate the importance of global robustness
and the practical application of our certiﬁcation approach, a
case study is conducted to verify the safety of a closed-loop
control system with a vision-based perception DNN in the loop.

II. EFFICIENT GLOBAL ROBUSTNESS CERTIFICATION

A. Problem Formulation

An n-layer neural network, formulated as F : Rm0 → Rmn
maps an m0-dimension input x(0) ∈ Rm0
into an mn-
dimension output x(n) ∈ Rmn . In the neural network, the
output of each layer i is denoted as x(i) ∈ Rmi with dimension
mi, and layer i maps the output from the previous layer x(i−1)
into its output by x(i) = f (i)(x(i−1)). f (i) is composed with
a linear transformation y(i) = W (i)x(i−1) + b(i) (e.g., fully-
connected layer, convolution layer, average pooling or nor-
malization), and (optionally) a ReLU activation function. The
linear transformation result is denoted as intermediate variable
y(i) ∈ Rmi . The layer output x(i) = relu(y(i)) = max(y(i), 0)
if it has ReLU activation function, and x(i) = y(i) otherwise.
An illustrating example is shown in Fig. 1, which is a 2-
layer neural network that maps 2-dimension input x(0) into
1-dimension output x(2) with a 2-neuron hidden layer. For
simplicity, the bias b(i) of each layer i is set to 0.

We consider the global robustness over the entire input
domain X of the neural network, i.e., ∀x(0) ∈ X. It measures
the worst-case output variation when there is a small input
perturbation for any possible input sample in the input domain
X. Moreover, we focus on the input perturbation that
is
bounded in the form of L∞ norm, and have the same global
robustness deﬁnition as [2], [13].

Deﬁnition 1 (Global Robustness). The j-th output of neural
network F is (δ, ε)-globally robust in the input domain X iff
∀x(0), ˆx(0) ∈ X, (cid:107)ˆx(0) − x(0)(cid:107)∞ ≤ δ =⇒ |ˆx(n)

j − x(n)

| ≤ ε

j

where x(n) = F (x(0)) and ˆx(n) = F (ˆx(0)).

In this work, we tackle the problem of how robust the neural
network is, i.e., how small ε can be for a given δ, formally as:

Problem 1. For a neural network F , given an input perturba-
tion bound δ, determine the minimal output variation bound ε
such that F is guaranteed to be (δ, ε)-globally robust.

The work in [2] proposes to solve the global robustness
problem by encoding two copies of the neural network side
by side, as illustrated in the left part of Fig. 2. The two
network copies take two separate inputs x(0) and ˆx(0) and
produce outputs x(n) and ˆx(n). As a perturbation of x(0), ˆx(0) is
restricted by the input perturbation ∆x(0) = ˆx(0) − x(0), where
(cid:107)∆x(0)(cid:107)∞ ≤ δ. The output variation bound ε is the bound of
the output distance ∆x(n) = ˆx(n) − x(n). Under this encoding,
Problem 1 can be formulated as an optimization problem:

ε := max |ˆx(n) − x(n)|

s.t.

ˆx(n) = F (ˆx(0)), x(n) = F (x(0))
ˆx(0), x(0) ∈ X, (cid:107)ˆx(0) − x(0)(cid:107)∞ < δ

(1)

Eq. (1) can be solved by mixed-integer linear programming
(MILP), where the neural network F can be decomposed into a
series of neuron-wise dependency. For the j-th neuron of layer
i with ReLU activation, we have:

y(i)
j = W (i)
j = W (i)
ˆy(i)

j xi−1 + b(i)
j ,
j ˆx(i−1) + b(i)
j ,
The max(x, 0) function for each ReLU activation can be
linearized by Big-M method [7] with the introduction of a new
integer (binary) variable to indication whether y < 0.

j = max(y(i)
x(i)
j = max(ˆy(i)
ˆx(i)

, 0)

, 0)

(2)

(3)

j

j

The complexity to solve this MILP is exponential to the
number of ReLU neurons, making it too complex for most
practical neural networks. In this work, to overcome this chal-
lenge, we present 1) a new interleaving twin-network encod-
ing (ITNE) scheme that adds extra interleaving dependencies
between the two copies of the neural network, and 2) two
approximation techniques leveraging the new encoding scheme,
namely network decomposition (ND) and LP relaxation (LPR),
to efﬁciently ﬁnd an over-approximated sub-optimal solution ¯ε
of the output variation bound ε, where ¯ε ≥ ε.

B. Interleaving Twin-Network Encoding

We call

the neural network encoding scheme introduced
in [2], shown in the left side of Fig. 2 with only input and output

𝑥10𝑥20𝑦(2)𝑥(2)𝑦11𝑦21𝑥11𝑥21ReLUReLUReLU1.01.0Fig. 2. Left: The basic twin-network encoding (BTNE) for global robust-
ness certiﬁcation. Right: the neuron-level interleaving twin-network encoding
(ITNE) built upon the basic structure, where the hidden layer neurons are
and ∆x(i)
connected between the two copies with distance variables ∆y(i)
j .

j

layers connected, the basic twin-network encoding (BTNE). In
contrast, the new network encoding scheme we designed to
enable the over-approximation techniques for global robustness
certiﬁcation is shown in the right side of Fig. 2 and called the
interleaving twin-network encoding (ITNE).

In ITNE, besides the connections between the input and
output layers, interleaving connections are added for all hidden
layer neurons between the two network copies. Speciﬁcally, for
and ∆x(i)
the j-th neuron of layer i, two variables, ∆y(i)
j , are
j
added to encode the distance of y(i)
and x(i)
j between the two
j
copies, where

∆x(i)

j = ˆx(i)

j − x(i)

j ) − relu(y(i)
j ).

∆y(i)

j = ˆy(i)
j = relu(y(i)

j − y(i)
j + ∆y(i)

,

j

These additional distance variables reﬂect how different the
two network copies are during the forward propagation. And
the non-linear mapping from ˆy(i)
is substituted by the
j
and ∆x(i)
mapping between ∆y(i)
j . These changes enable the
j
usage of the over-approximation techniques introduced below.

to ˆx(i)
j

C. Over-Approximation Techniques

Leveraging ITNE, we design two over-approximation tech-
niques, network decomposition (ND) and LP relaxation (LPR),
to improve the global robustness certiﬁcation efﬁciency. The
two techniques are inspired by the local robustness certiﬁcation
work in [6]. However, there are major differences between
global and local robustness certiﬁcation, given that the former
considers the entire input space X while the latter only consid-
ers a given input x(0) ∈ X, and our ND and LPR techniques
are speciﬁcally designed for global robustness.

a) ITNE-based network decomposition (ND): The main
idea of ND is to divide a neural network into sub-networks
and decompose the entire optimization problem into smaller
problems. The input of a sub-network is either the network
input or the output of other sub-networks. Given the range
of the input of a sub-network, the range of its output can

Fig. 3. Left: ReLU distance relation when y ≥ 0; Middle: ReLU distance
relation when y < 0; Right: LP-relaxation of ReLU distance relation. The
ReLU distance relation for ∀y ∈ R lays in the shadowed area. Within the
distance range ∆y ∈ [∆y, ∆y], ∆x is bounded by the lower and upper bounds.

be derived by solving an optimization problem of the sub-
network, which can then be used as the input range of the
next sub-network. As the complexity of MILP is exponential to
the neural network size, using ND can signiﬁcantly reduce the
complexity of the optimization problem. For global robustness
certiﬁcation, we also consider two copies of each sub-network.
However, instead of ﬁnding the output range of each sub-
network copy, we look for the range of one sub-network copy
and the range of the output distance, based on the ITNE.

More speciﬁcally, a w-layer sub-network with input x(i−w)
and output x(i)
(i ≥ w) is denoted as Fw(x(i)
j ). For any variable
j
v, its range is denoted as v = [v, v]. Under ITNE, for each
sub-network Fw(x(i)
j ), given the input range x(i−w) and input
distance range ∆x(i−w), we can derive the output range x(i)
j
and output distance range ∆x(i)
by solving the optimization
j
problem in Eq. (1) for this sub-network.

b) ITNE-based LP relaxation (LPR): The idea of LPR is
to relax the ReLU relation x = max(0, y) into linear constraints
given the range of y as [y, y]. When y ≤ 0 or y ≥ 0, the ReLU
relation degenerates to x = 0 or x = y. Otherwise, ReLU
relation can be relaxed by three linear inequations:

x ≥ 0,

x ≥ y,

x ≤

y(y − y)
y − y

(4)

The ReLU relations of a neuron in the two network copies
are formulated in Eqs. (2) and (3). In our work, instead of
relaxing the ReLU relations in two network copies separately,
we relax Eq. (2) by the original LPR in Eq. (4), and relax the
ReLU distance relation based on ITNE:

∆x = relu(y + ∆y) − relu(y).

(5)

For a given y, the relation between ∆x and ∆y is shown in
the ﬁrst two plots of Fig. 3. Thus, the shadowed area in the
third plot of Fig. 3 can cover all possible (∆x, ∆y) mappings
for ∀y ∈ R. Given ∆y ∈ [∆y, ∆y], the relation between ∆x
and ∆y can be bounded by the linear lower and upper bounds
as shown in the third plot of Fig. 3. Formally, the bound is

u(∆y − l)
u − l
where l = min(0, ∆y) and u = max(0, ∆y).

l(u − ∆y)
u − l

≤ ∆x ≤

,

(6)

D. Illustrating example

Consider the example neural network in Fig. 1. Assume
that the input perturbation bound as δ = 0.1 and the input

Δ𝑥𝑛=ො𝑥(𝑛)−𝑥𝑛𝑥0Δ𝑥0ො𝑥(0)ො𝑥(𝑛)𝑥𝑛𝑦𝑗𝑖ො𝑦𝑗𝑖Δ𝑦𝑗𝑖𝑥𝑗𝑖ො𝑥𝑗𝑖Δ𝑥𝑗𝑖Δ𝑥Δ𝑦−𝑦𝑦<0Δ𝑥Δ𝑦−𝑦𝑦≥0Δ𝑦Δ𝑦Δ𝑥Δ𝑦domain as x(0) ∈ [−1, 1]2. The example neural network can be
decomposed into three sub-networks:
1 = relu(y(1)
x(1)
x(1)
2 = relu(y(1)

1 ) = relu(x(0)
2 ) = relu(−0.5x(0)

1 + 0.5x(0)
2 )
1 + x(0)
2 )

x(2) = relu(y(2)) = relu(x(1)

1 − x(1)
2 ).

For local robustness, with input x(0) = [0, 0], the certiﬁcation
processes for different techniques are shown in Fig. 4. The exact
bound of ˆx(2) is derived by solving the MILP problem for the
entire network. ND solves the MILP problems of the ﬁrst two
sub-networks to derive the bound of ˆx(1) and then derive the
bound of ˆx(2) by solving the MILP problem of the third sub-
network. For LPR, given ˆy(1)
1 , ˆy(1)
2 , ˆy(2) ∈ [−0.15, 0.15], all
ReLU relations can be relaxed based on Eq. (4), and the bound
of ˆx(2) can be derived by solving the relaxed LP problem. As
we can see, ND and LPR can provide tight (1.2x and 1.15x)
over-approximations for local robustness.

For global robustness, under BTNE, there are no distance
variables for hidden neurons. Thus, ND and LPR can only be
applied to each individual network copy. ND will only ﬁnd the
range of x(1) and ˆx(1) and for the third sub-network, the dis-
tance information between two network copies is lost, resulting
in a 7.5x over-approximation of the range of ∆x(2). The LPR is
based on the bounds y(1), ˆy(1) ∈ [−1.5, 1.5]2, and y(2), ˆy(2) ∈
[−1.5, 1.5], resulting in a 10.9x over-approximation. On the
other hand, under ITNE, the ITNE-based ND can ﬁrst ﬁnd
the range of x(1) and ∆x(1) and then derive the range of
∆x(2), which is only 1.5x of the exact one. Given ∆y(1) ∈
[−0.15, 0.15]2 and ∆y(2) ∈ [−0.3, 0.3], the ITNE-based LPR
derives a tight 1.38x over-approximation of the range of ∆x(2).
This shows that combing ITNE with ND and LPR signiﬁcantly
improves the approximation tightness over BTNE.

E. Efﬁcient Global Robustness Over-Approximation Algorithm

and ∆y(i)
j

Finally, we design our global robustness certiﬁcation algo-
rithm by leveraging the ITNE encoding as well as the ND and
LPR techniques, as shown in Algorithm 1. Note that the MILP
problems for sub-networks from ND are relaxed by LPR. The
pre-assigned window size W is deﬁned as the desired depth
of sub-networks. For LPR, besides the range of the input and
distance, y(i−k) and ∆y(i−k) for ∀k ∈ [0, w − 1] are also
needed. To evaluate y(i)
, we can consider a w-layer
j
sub-network with y(i)
as the output (i.e., no ReLU activation
j
at the output layer), denoted as Fw(y(i)
j ). Formally, we have
two types of sub-network optimization problems: LpRelaxY
and LpRelaxX, which is respectively encoded (lines 7 and 9)
as ITNE of sub-network Fw(y(i)
j ) (decomposed
from network F in lines 6 and 9). Both problems require the
sub-network input range x(i−w), ∆x(i−w) and the ranges for
hidden neurons, i.e., y(i−k) and ∆y(i−k), ∀k ∈ [1, w − 1] as
, ∆y(i)
prerequisites, while LpRelaxX (line 11) also needs y(i)
,
j
j
which is derived from LpRelaxY (line 8). Therefore,
the
ranges need to be evaluated layer by layer, by treating these
hidden layer neurons as outputs of sub-networks Fw(y(i)
j ) and

j ) and Fw(x(i)

Fig. 4.
Illustrating example: The local and global robustness certiﬁcation
processes of exact MILP, network decomposition (ND) and LP relaxation
(LPR). For global robustness, the ND and LPR for both basic and interleaving
twin-network encoding (i.e., BTNE and ITNE) are illustrated.

Algorithm 1 Global Robustness Certiﬁcation Algorithm
Input: Neural Network F , window size W , reﬁne param r
Input: input domain X, input perturbation bound δ
Output: output variation bound ¯ε

1: x(0) = X
2: ∆x(i) = [−δ, δ]m0
3: for i = 1 : n do
4:
5:

w = max(i, W )
for j = 1 : mi do

6:

7:

8:

9:

10:

j

j

, w)

Fw(y(i)
j ) ← N etDecompose(F, y(i)
ey = InterleaveT winN etEncode(Fw(y(i)
(y(i)
, ∆y(i)
) ← LpRelaxY (ey, r)
j
Fw(x(i)
j ) ← N etDecompose(F, x(i)
ex = InterleaveT winN etEncode(Fw(x(i)
(x(i)
j ) ← LpRelaxX(ex, r)
end for

j , ∆x(i)

j , w)

j ))

j ))

11:
12:
13: end for
14: ¯ε ← max(|∆x(n)|, |∆x(n)|)

Fw(x(i)
layer, all ranges of previous layers have been derived.

j ). In such a way, when evaluating the ranges of certain

Selective Reﬁnement: While LPR can remove all integer
variables in the MILP formulation to reduce the complexity,
such extreme over-approximation may be too inaccurate. Thus,
we try to selectively reﬁne a limited number of neurons, by not
relaxing their ReLU relations. This is similar to the layer-level
reﬁnement idea in [6], but with a focus on global robustness.
Speciﬁcally, a score of LPR is evaluated for each neuron as the
worst-case inaccuracy, and the top r neurons will be reﬁned.
The score is deﬁned as the maximum distance between the
lower and upper bound of LPR, i.e., the score is −yy/(y − y)

Global Robustness𝑖=0𝑖=1𝑖=2ExactΔ𝑥(𝑖)−0.1,0.12−0.2,0.2𝑥(𝑖),ො𝑥(𝑖)−1,120,1.25Basic EncodingNDΔ𝑥(𝑖)−0.1,0.12−1.5,1.5𝑥(𝑖),ො𝑥(𝑖)−1,120,1.520,1.5LPRΔ𝑥(𝑖)−0.1,0.12−2.85,1.5𝑥(𝑖),ො𝑥(𝑖)−1,120,1.44InterleavingNDΔ𝑥(𝑖)−0.1,0.12−0.15,0.152−0.3,0.3𝑥(𝑖),ො𝑥(𝑖)−1,120,1.520,1.5LPRΔ𝑥(𝑖)−0.1,0.12−0.275,0.275𝑥(𝑖),ො𝑥(𝑖)−1,120,1.44MILPRelaxed LPMILPMILPRelaxedLPLocal Robustnessො𝑥(0)ො𝑥(1)ො𝑥(2)Exact−0.1,0.120,0.125ND−0.1,0.120,0.1520,0.15LPR−0.1,0.120,0.144MILPMILPMILPRelaxedLPTABLE I
NEURAL NETWORK SETTING AND EXPERIMENTAL RESULTS.

Dataset

Auto
MPG

Dataset
Auto
MPG

MNIST

ID
1
2
3
4
ID

5

6

7

8

Neurons
8
12
16
32
Neurons

64

1416

3872

5824

tR
2s
130s
8h
> 24h

tM
0.1s
0.2s
0.8s
74s

Layers

FC:3

Conv:1
FC:2
Conv:2
FC:2
Conv:3
FC:2

tour
0.3s
0.4s
1s
5s
tour

50s

4.8h

3.3h

3.5h

ε
0.0583
0.0527
0.0496
0.0481
ε

ε (ours)
0.0657
0.0722
0.0653
0.0673
ε (ours)

0.0452

0.0731

0.347
0.300
0.453
0.420
0.519
0.407

0.578
0.572
0.874
0.723
1.521
1.175

In Table I, ‘Layers’ are the type and number of layers; ‘Neurons’ is the total
number of hidden neurons; tR, tM , tour are the certiﬁcation time of exact
MILP, Reluplex, and our approach, respectively; ε, ε, ε are the exact (derived
by Reluplex or MILP), under-approximated (derived by projected gradient
descent (PGD) among dataset), and over-approximated (derived by our
approach) output variation bounds, respectively.

for Eq. (4), and max(|∆y|, |∆y|) for Eq. (6). The number of
neurons for reﬁnement r is a parameter that can be set in LPR.
Complexity Analysis: MILP’s complexity is polynomial to
the number of variables and exponential to the number of
integer variables. In our approach, for each MILP, the number
of variables is linear to the number of neurons in the sub-
network, and the number of integer variables is linear to the
number of neurons selected for reﬁnement. The number of
MILPs to solve is linear to the total number of neurons.

III. EXPERIMENTAL RESULTS

We ﬁrst evaluate our algorithm on various DNNs and com-
pare its results with exact global robustness (when available)
and an under-approximated global robustness. Then, we demon-
strate the application of our approach in a case study of safety
veriﬁcation for a vision-based robotic control system, and show
the importance of efﬁcient global robustness certiﬁcation for
safety-critical systems that involve neural networks.

We implement our efﬁcient global robustness certiﬁcation
algorithm in Python. All MILP/LP problems in the algorithm
are solved by Gurobi [14]. All the experiments are conducted
on a platform with a 4-core 3.6 GHz Intel Xeon CPU and 16
GB memory. All neural networks are modeled in Tensorﬂow.

A. Comparison with Other Methods

We conduct experiments on a set of DNNs with different
sizes. The DNNs are trained on two datasets: the Auto MPG
dataset [15] for vehicle fuel consumption prediction, and the
MNIST handwritten digits classiﬁcation dataset [16]. Besides
the fully-connected (FC) output layer, the Auto MPG DNNs
contain 2 FC hidden layers with the number of hidden neurons
ranging from 8 to 64. MNIST DNNs contain 1 to 3 convolu-
tional layers followed by 1 FC hidden layer, with 1472 to 7176
hidden neurons. We certify the output variation bound ε based
on the input perturbation bound δ, where δ = 0.001 for Auto
MPG DNNs and δ = 2/255 for MNIST DNNs.

The network settings and the comparison among different
approaches are shown in Table I. For our approach, window

size W = 2 for Auto MPG DNNs, and W = 3 for MNIST
DNNs. Half neurons are reﬁned in Auto MPG DNNs and 30
neurons in each layer are reﬁned in MNIST DNNs. We present
2 outputs of MNIST (out of 10) in Table I, and others show
similar differences between different methods.

For small networks (DNN-1 to DNN-5), we compare our
over-approximated output variation bound ε with the exact
bound ε solved by Reluplex [2] (latest version that is integrated
in the tool Marabou [17] is used) and the MILP encoding
in Eq. (1). We can see that the runtime of Reluplex tR and
MILP tM quickly increases with respect to neural network size.
None of them can get a solution within 24 hours for DNN-5
(with only 64 hidden neurons). From DNN-1 to DNN-4, our
algorithm can derive an over-approximated global robustness
with much slower runtime increase, while only have about 13%
to 40% over-approximation over the exact global robustness.

Starting from DNN-5, the two exact certiﬁcation methods
cannot ﬁnd a solution in reasonable time. There is also no
other work in the literature that can derive a sound and
deterministic global robustness. To assess how good our over-
approximated results are for larger networks, we evaluate an
under-approximation of the global robustness, inspired by [9].
Speciﬁcally, for each data sample in the dataset, we leverage
projected gradient descent (PGD) [18] to look for an adversarial
example in the input perturbation bound that maximizes the
output variation, and treat the maximal output variation among
the entire dataset as an under-approximated output variation
bound ε. The exact global robustness should be between the
under-approximation ε derived from this dataset-wise PGD, and
the over-approximation ε derived by our certiﬁcation algorithm.
The experiments from DNN-6 to DDN-8 demonstrate that our
method can provide meaningful over-approximation (less
than 3x of the under-approximation) for DNNs with more
than 5000 hidden neurons within 5 hours.

B. Case Study on Control Safety Veriﬁcation

For control systems that use neural networks for perception,
a critical and yet challenging question is whether the system
can remain safe when there is perturbation/disturbance to the
perception neural network input. In [13], the authors formulate
this as a design-time safety assurance problem based on global
robustness (although did not provide a solution), i.e., if a global
robustness bound (cid:15) can be derived for an input perturbation
bound δ, such (cid:15) can be viewed as the state estimation error
bound in control and leveraged for control safety veriﬁcation.
In this section, leveraging our approach for bounding global
robustness, we conduct a case study for safety veriﬁcation of
control systems that use neural networks for perception. We
consider an advanced cruise control (ACC) scenario where an
ego vehicle is following a reference vehicle. The ego vehicle
is equipped with an RGB camera, and a DNN is designed to
estimate the distance from the reference vehicle based on the
camera image. We assume that the captured image may be
slightly perturbed. A feedback controller takes the estimated
distance, along with the ego vehicle speed, to generate the
control input, which is the acceleration of the ego vehicle.

Method (FGSM) [21]. With more than 1000 minutes of simu-
lation, we ﬁnd that when the perturbation bound is δ = 2/255
as assumed, the distance estimation error ∆d never exceeds the
[−0.14, 0.14] bound and the system is always safe (as expected
since it was formally proven). When the perturbation bound δ is
increased to 5/255, ∆d sometimes exceeds the bound, although
unsafe system state is not observed. When δ is increased to
10/255, about 17% of the simulations encounter unsafe states.
This shows the impact of input perturbation on system safety
and the importance of our global robustness analysis.

IV. CONCLUSION
We present an efﬁcient certiﬁcation algorithm to provide
a sound and deterministic global robustness for neural net-
works with ReLU activation. Experiments demonstrate that
our approach is much more efﬁcient and scalable than exact
global robustness certiﬁcation approaches while providing tight
over-approximation, and a case study further demonstrates the
application of our approach in safety-critical systems. Our
future work will continue improving the approach’s efﬁciency,
possibly via parallelization on multi-core platforms, to enable
its usage for larger-size perception neural networks.

REFERENCES

[1] B. Biggio, I. Corona, D. Maiorca et al., “Evasion attacks against machine

learning at test time,” in ECML PKDD, 2013, pp. 387–402.

[2] G. Katz, C. Barrett, D. L. Dill et al., “Reluplex: An efﬁcient smt solver

for verifying deep neural networks,” in CAV, 2017, pp. 97–117.

[3] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety veriﬁcation

of deep neural networks,” in CAV, 2017, pp. 3–29.

[4] A. Lomuscio and L. Maganti, “An approach to reachability analysis for

feed-forward relu neural networks,” arXiv, 2017.

[5] G. Singh, T. Gehr, M. P¨uschel et al., “An abstract domain for certifying

neural networks,” Proc. ACM Program. Lang., vol. 3, Jan. 2019.

[6] C. Huang, J. Fan, X. Chen et al., “Divide and slide: Layer-wise reﬁnement
for output range analysis of deep neural networks,” TCAD, vol. 39, 2020.
[7] C.-H. Cheng, G. N¨uhrenberg, and H. Ruess, “Maximum resilience of

artiﬁcial neural networks,” in ATVA, 2017, pp. 251–268.

[8] Y. Chen, S. Wang, Y. Qin et al., “Learning security classiﬁers
with veriﬁed global robustness properties,” 2021. [Online]. Available:
https://arxiv.org/abs/2105.11363

[9] W. Ruan, M. Wu, Y. Sun et al., “Global robustness evaluation of deep
neural networks with provable guarantees for the hamming distance,” in
IJCAI, 2019.

[10] O. Bastani, Y. Ioannou, L. Lampropoulos et al., “Measuring neural net

robustness with constraints,” in NeurIPS, vol. 29, 2016.

[11] R. Mangal, A. V. Nori, and A. Orso, “Robustness of neural networks: A

probabilistic and practical approach,” in ICSE-NIER, 2019, pp. 93–96.

[12] D. Gopinath, G. Katz, C. S. P˘as˘areanu et al., “Deepsafe: A data-driven

approach for assessing robustness of neural networks,” in ATVA, 2018.

[13] Z. Wang, C. Huang, Y. Wang et al., “Bounding perception neural network

uncertainty for safe control of autonomous systems,” in DATE, 2021.

[14] Gurobi Optimization, LLC, “Gurobi Optimizer Reference Manual,”

2021. [Online]. Available: https://www.gurobi.com

[15] D. Dua and C. Graff, “UCI machine learning repository,” 2017. [Online].

Available: http://archive.ics.uci.edu/ml

[16] Y. Lecun, L. Bottou, Y. Bengio et al., “Gradient-based learning applied
to document recognition,” Proceedings of the IEEE, vol. 86, 1998.
[17] G. Katz, D. A. Huang, D. Ibeling et al., “The marabou framework for
veriﬁcation and analysis of deep neural networks,” in CAV, 2019.
[18] A. Madry, A. Makelov, and L. S. andothers, “Towards deep learning

models resistant to adversarial attacks,” in ICLR, 2018.

[19] Webots, “Commercial mobile robot simulation software.” [Online].

Available: http://www.cyberbotics.com

[20] C. Huang, S. Xu, Z. Wang et al., “Opportunistic intermittent control with
safety guarantees for autonomous systems,” in DAC, 2020, pp. 1–6.
[21] I. J. Goodfellow, J. Shlens et al., “Explaining and harnessing adversarial

examples,” in ICLR, 2015.

Fig. 5.
(a) Webots simulation for case study, where ego vehicle (left) follows
reference vehicle (right); (b) An example image captured by ego vehicle; (c)
Lower bound of DNN input space; (d) Upper bound of DNN input space.

We model this example in the tool Webots [19] (Fig. 5).
The ego vehicle is safe if distance d ∈ [0.5, 1.9] and speed
ve ∈ [0.1, 0.7]. The reference vehicle speed vr is randomly
adjusted within [0.2, 0.6]. The sampling period is 100ms. The
camera takes RGB images with resolution 24×48. A 5-layer (3
convolutional and 2 FC) DNN is trained with 100k pre-captured
images. The system dynamics is modeled as:

x[k+1] =

(cid:21)
(cid:20)1 −0.1
0

1

x[k]+

(cid:21)

(cid:20)−0.005
0.1

(cid:21)
(cid:20)1
0

u[k]+

w1[k]+w2[k]

where the system state x = [d − 1.2, ve − 0.4](cid:62) contains the
normalized distance and speed. w1 = 0.4 − vr is the external
disturbance, where vr is bounded within [0.2, 0.6] as previously
mentioned. w2 = [wd, wv](cid:62) is the inaccuracy of this system
dynamics model. The control input follows the feedback control
law u = K ˆx, where K = [0.3617, −0.8582]. ˆx is the estimated
system state, and the state estimation error is ∆x = ˆx − x.
The estimation error of ego vehicle speed ve is assumed as 0.
The bound of wd, wv are proﬁled based on simulations, and
bounded as |wd| ≤ 5e − 4 and |wv| ≤ 3e − 5. The estimation
error for distance contains two parts ∆d = ∆d1 + ∆d2, where
∆d1 is the error coming from the DNN model inaccuracy, while
∆d2 is the output variation caused by input perturbation.

The DNN model

inaccuracy ∆d1 is determined by the
worst-case model inaccuracy among the dataset and set as
|∆d1| ≤ 0.0730. The DNN output variation ∆d2 is the focus
of this study and can be bounded by our global robustness
certiﬁcation algorithm. Speciﬁcally, the input perturbation in
this study is bounded by δ = 2/255 (Fig. 5 illustrates the
upper and lower bound of each pixel for the input space). Then,
using our approach, we can derive a certiﬁed output variation
bound |∆d2| ≤ (cid:15) = 0.0568. Combined with ∆d1, we have
|∆d| ≤ 0.0730 + 0.0568 = 0.1298.

With bounds on ∆d and other variables, the vehicle control
safety can be veriﬁed based on the computation of control
invariant set, similarly as in [20]. Through such invariant
set based veriﬁcation, we ﬁnd that as long as the distance
estimation error ∆d is within [−0.14, 0.14], the system will
always be safe. Since our global robustness certiﬁcation bounds
∆d ≤ 0.1298, we can assert that our ACC system is safe with
this DNN design under the assumed perturbation bound.

We also deploy our DNN model in Webots and add adver-
sarial perturbation on the input images by Fast Gradient Sign

(a)(b)(c)(d)