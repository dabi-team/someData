Efï¬cient Global Robustness Certiï¬cation of Neural
Networks via Interleaving Twin-Network Encoding

Zhilu Wangâˆ—, Chao Huangâ€ , Qi Zhuâˆ—
âˆ—Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA
Emails: zhilu.wang@u.northwestern.edu, qzhu@northwestern.edu
â€ Department of Computer Science, University of Liverpool, Liverpool, UK
Email: chao.huang2@liverpool.ac.uk

2
2
0
2

r
a

M
6
2

]

G
L
.
s
c
[

1
v
1
4
1
4
1
.
3
0
2
2
:
v
i
X
r
a

Abstractâ€”The robustness of deep neural networks has received
signiï¬cant interest recently, especially when being deployed in
safety-critical systems, as it is important to analyze how sensitive
the model output
is under input perturbations. While most
previous works focused on the local robustness property around an
input sample, the studies of the global robustness property, which
bounds the maximum output change under perturbations over the
entire input space, are still lacking. In this work, we formulate
the global robustness certiï¬cation for neural networks with
ReLU activation functions as a mixed-integer linear programming
(MILP) problem, and present an efï¬cient approach to address it.
Our approach includes a novel interleaving twin-network encoding
scheme, where two copies of the neural network are encoded
side-by-side with extra interleaving dependencies added between
them, and an over-approximation algorithm leveraging relaxation
and reï¬nement techniques to reduce complexity. Experiments
demonstrate the timing efï¬ciency of our work when compared
with previous global robustness certiï¬cation methods and the
tightness of our over-approximation. A case study of closed-loop
control safety veriï¬cation is conducted, and demonstrates the
importance and practicality of our approach for certifying the
global robustness of neural networks in safety-critical systems.

I. INTRODUCTION

Deep neural networks (DNNs) are being applied to a wide
range of tasks, such as perception, prediction, planning, control,
and general decision making. While DNNs often show signif-
icant advantages in performance over traditional model-based
methods, pressing concerns have been raised on the uncertain
behaviors of DNNs under varying inputs, especially for safety-
critical systems such as autonomous vehicles and robots. Some
of the well-trained DNNs are found to be vulnerable to a small
adversarial perturbation on their inputs [1]. The robustness
metric of DNNs is deï¬ned to bound such uncertain behaviors
when the input is perturbed. Speciï¬cally, the robustness of a
DNN represents how much its output varies when its input has
a bounded perturbation, where the perturbation can be either
from random noises or due to malicious attacks.

Formal methods,

such as Satisï¬ability Modulo Theo-
ries (SMT) [2], [3] and Mixed-Integer Linear Programming
(MILP) [4], have been used to either ï¬nd an adversarial ex-
ample or certify the robustness (i.e., proving no adversarial ex-
ample exists) around a given input sample, which is considered

This work is supported in part by NSF grants 1834701, 1839511, 1724341,

2038853, and ONR grant N00014-19-1-2496.

This paper is published on Design, Automation and Test in Europe Confer-

ence (DATE 2022).

as local robustness. The efï¬ciency is improved by certifying a
conservative (sound but not complete) local robustness bound
via over-approximated output range analysis methods [5], [6].
In safety-critical systems with DNNs involved, it is important
to analyze the DNN robustness for evaluating system safety. As
local robustness is deï¬ned locally to a given input sample, to
evaluate system safety, we will need to apply local robustness
certiï¬cation during runtime for each input sample that the
system encounters or may encounter. However, the complexity
of local robustness certiï¬cation, even with conservative over-
approximation,
is too high for runtime execution in most
practical systems. Moreover, sometimes it is required to ensure
system safety for future time horizon (e.g., to verify there is
no collision for the next 3 seconds). As future input samples
are unknown, local robustness certiï¬cation cannot be applied.
Instead, we could address system safety by considering
the problem of global robustness, which measures the worst-
case DNN output change against input perturbation for all
possible input samples. The deï¬nition of the global robustness
is introduced in [2], with a proposed SMT-based tool Reluplex
for certifying the exact global robustness. In [7], the global
robustness for classiï¬cation problems is formulated into MILP
to ï¬nd the maximum input perturbation that can preserve
the classiï¬cation result. In [8], an MILP-based exact global
robustness certiï¬cation method is proposed for logic ensemble
classiï¬er (a generalized-version of decision tree, not DNN).

A major challenge for exact global robustness certiï¬cation
is its complexity â€“ while it can be run ofï¬‚ine to facilitate
safety analysis (since it considers the entire input space), the
intractable even in ofï¬‚ine
high complexity often makes it
computation. A number of methods have been proposed to
tackle the complexity, however they often lack the deterministic
guarantees needed for safety veriï¬cation. For instance, dataset-
based global robustness estimation approaches are proposed
in [9], [10], which conduct local robustness certiï¬cation for
each sample in the dataset and ï¬nd the worst case or the
average. As the dataset cannot cover all possible input samples,
the derived global robustness is just an estimation with no
deterministic guarantees. Sampling-based methods [11] ran-
domly sample the DNN inputs, evaluate the local robustness
of the random-sampled inputs, and provide a probabilistic
global robustness. Similarly, they cannot provide deterministic
guarantees. Region-based robustness analyses [11], [12] divide

 
 
 
 
 
 
Fig. 1. An example neural network for illustration.

the input space into regions, where all possible inputs in
each certiï¬ed-robust region have the same classiï¬cation result.
However, the number of regions could be extremely large for
DNNs with high-dimensional inputs, making it hard to certify
all regions and provide deterministic guarantees.

In this work, we propose an efï¬cient certiï¬cation approach
to over-approximate the global robustness. The derived global
robustness is sound and deterministic, the over-approximation
is tight, and the approach can be used effectively for the purpose
of safety veriï¬cation. To achieve this, our approach introduces
a novel network encoding structure, namely interleaving twin-
network encoding, to compare two copies of the neural network
side-by-side under different
inputs, with extra interleaving
dependencies added between them to improve efï¬ciency. Our
approach also includes over-approximation techniques based
on network decomposition and LP (linear programming) relax-
ation, to further reduce the computation complexity. To the best
of our knowledge, our approach is the ï¬rst global robustness
over-approximation method that certiï¬es the robustness among
the entire input domain with sound and deterministic guarantee.
Experiments show that our approach is much more efï¬cient
and scalable than the exact global robustness methods such as
Reluplex [2], with tight over-approximation â€“ our approach can
certify DNNs with more than 5k neurons in 5 hours while the
exact methods cannot certify DNNs with more than 64 neurons
in 1 day. To demonstrate the importance of global robustness
and the practical application of our certiï¬cation approach, a
case study is conducted to verify the safety of a closed-loop
control system with a vision-based perception DNN in the loop.

II. EFFICIENT GLOBAL ROBUSTNESS CERTIFICATION

A. Problem Formulation

An n-layer neural network, formulated as F : Rm0 â†’ Rmn
maps an m0-dimension input x(0) âˆˆ Rm0
into an mn-
dimension output x(n) âˆˆ Rmn . In the neural network, the
output of each layer i is denoted as x(i) âˆˆ Rmi with dimension
mi, and layer i maps the output from the previous layer x(iâˆ’1)
into its output by x(i) = f (i)(x(iâˆ’1)). f (i) is composed with
a linear transformation y(i) = W (i)x(iâˆ’1) + b(i) (e.g., fully-
connected layer, convolution layer, average pooling or nor-
malization), and (optionally) a ReLU activation function. The
linear transformation result is denoted as intermediate variable
y(i) âˆˆ Rmi . The layer output x(i) = relu(y(i)) = max(y(i), 0)
if it has ReLU activation function, and x(i) = y(i) otherwise.
An illustrating example is shown in Fig. 1, which is a 2-
layer neural network that maps 2-dimension input x(0) into
1-dimension output x(2) with a 2-neuron hidden layer. For
simplicity, the bias b(i) of each layer i is set to 0.

We consider the global robustness over the entire input
domain X of the neural network, i.e., âˆ€x(0) âˆˆ X. It measures
the worst-case output variation when there is a small input
perturbation for any possible input sample in the input domain
X. Moreover, we focus on the input perturbation that
is
bounded in the form of Lâˆ norm, and have the same global
robustness deï¬nition as [2], [13].

Deï¬nition 1 (Global Robustness). The j-th output of neural
network F is (Î´, Îµ)-globally robust in the input domain X iff
âˆ€x(0), Ë†x(0) âˆˆ X, (cid:107)Ë†x(0) âˆ’ x(0)(cid:107)âˆ â‰¤ Î´ =â‡’ |Ë†x(n)

j âˆ’ x(n)

| â‰¤ Îµ

j

where x(n) = F (x(0)) and Ë†x(n) = F (Ë†x(0)).

In this work, we tackle the problem of how robust the neural
network is, i.e., how small Îµ can be for a given Î´, formally as:

Problem 1. For a neural network F , given an input perturba-
tion bound Î´, determine the minimal output variation bound Îµ
such that F is guaranteed to be (Î´, Îµ)-globally robust.

The work in [2] proposes to solve the global robustness
problem by encoding two copies of the neural network side
by side, as illustrated in the left part of Fig. 2. The two
network copies take two separate inputs x(0) and Ë†x(0) and
produce outputs x(n) and Ë†x(n). As a perturbation of x(0), Ë†x(0) is
restricted by the input perturbation âˆ†x(0) = Ë†x(0) âˆ’ x(0), where
(cid:107)âˆ†x(0)(cid:107)âˆ â‰¤ Î´. The output variation bound Îµ is the bound of
the output distance âˆ†x(n) = Ë†x(n) âˆ’ x(n). Under this encoding,
Problem 1 can be formulated as an optimization problem:

Îµ := max |Ë†x(n) âˆ’ x(n)|

s.t.

Ë†x(n) = F (Ë†x(0)), x(n) = F (x(0))
Ë†x(0), x(0) âˆˆ X, (cid:107)Ë†x(0) âˆ’ x(0)(cid:107)âˆ < Î´

(1)

Eq. (1) can be solved by mixed-integer linear programming
(MILP), where the neural network F can be decomposed into a
series of neuron-wise dependency. For the j-th neuron of layer
i with ReLU activation, we have:

y(i)
j = W (i)
j = W (i)
Ë†y(i)

j xiâˆ’1 + b(i)
j ,
j Ë†x(iâˆ’1) + b(i)
j ,
The max(x, 0) function for each ReLU activation can be
linearized by Big-M method [7] with the introduction of a new
integer (binary) variable to indication whether y < 0.

j = max(y(i)
x(i)
j = max(Ë†y(i)
Ë†x(i)

, 0)

, 0)

(2)

(3)

j

j

The complexity to solve this MILP is exponential to the
number of ReLU neurons, making it too complex for most
practical neural networks. In this work, to overcome this chal-
lenge, we present 1) a new interleaving twin-network encod-
ing (ITNE) scheme that adds extra interleaving dependencies
between the two copies of the neural network, and 2) two
approximation techniques leveraging the new encoding scheme,
namely network decomposition (ND) and LP relaxation (LPR),
to efï¬ciently ï¬nd an over-approximated sub-optimal solution Â¯Îµ
of the output variation bound Îµ, where Â¯Îµ â‰¥ Îµ.

B. Interleaving Twin-Network Encoding

We call

the neural network encoding scheme introduced
in [2], shown in the left side of Fig. 2 with only input and output

ğ‘¥10ğ‘¥20ğ‘¦(2)ğ‘¥(2)ğ‘¦11ğ‘¦21ğ‘¥11ğ‘¥21ReLUReLUReLU1.01.0Fig. 2. Left: The basic twin-network encoding (BTNE) for global robust-
ness certiï¬cation. Right: the neuron-level interleaving twin-network encoding
(ITNE) built upon the basic structure, where the hidden layer neurons are
and âˆ†x(i)
connected between the two copies with distance variables âˆ†y(i)
j .

j

layers connected, the basic twin-network encoding (BTNE). In
contrast, the new network encoding scheme we designed to
enable the over-approximation techniques for global robustness
certiï¬cation is shown in the right side of Fig. 2 and called the
interleaving twin-network encoding (ITNE).

In ITNE, besides the connections between the input and
output layers, interleaving connections are added for all hidden
layer neurons between the two network copies. Speciï¬cally, for
and âˆ†x(i)
the j-th neuron of layer i, two variables, âˆ†y(i)
j , are
j
added to encode the distance of y(i)
and x(i)
j between the two
j
copies, where

âˆ†x(i)

j = Ë†x(i)

j âˆ’ x(i)

j ) âˆ’ relu(y(i)
j ).

âˆ†y(i)

j = Ë†y(i)
j = relu(y(i)

j âˆ’ y(i)
j + âˆ†y(i)

,

j

These additional distance variables reï¬‚ect how different the
two network copies are during the forward propagation. And
the non-linear mapping from Ë†y(i)
is substituted by the
j
and âˆ†x(i)
mapping between âˆ†y(i)
j . These changes enable the
j
usage of the over-approximation techniques introduced below.

to Ë†x(i)
j

C. Over-Approximation Techniques

Leveraging ITNE, we design two over-approximation tech-
niques, network decomposition (ND) and LP relaxation (LPR),
to improve the global robustness certiï¬cation efï¬ciency. The
two techniques are inspired by the local robustness certiï¬cation
work in [6]. However, there are major differences between
global and local robustness certiï¬cation, given that the former
considers the entire input space X while the latter only consid-
ers a given input x(0) âˆˆ X, and our ND and LPR techniques
are speciï¬cally designed for global robustness.

a) ITNE-based network decomposition (ND): The main
idea of ND is to divide a neural network into sub-networks
and decompose the entire optimization problem into smaller
problems. The input of a sub-network is either the network
input or the output of other sub-networks. Given the range
of the input of a sub-network, the range of its output can

Fig. 3. Left: ReLU distance relation when y â‰¥ 0; Middle: ReLU distance
relation when y < 0; Right: LP-relaxation of ReLU distance relation. The
ReLU distance relation for âˆ€y âˆˆ R lays in the shadowed area. Within the
distance range âˆ†y âˆˆ [âˆ†y, âˆ†y], âˆ†x is bounded by the lower and upper bounds.

be derived by solving an optimization problem of the sub-
network, which can then be used as the input range of the
next sub-network. As the complexity of MILP is exponential to
the neural network size, using ND can signiï¬cantly reduce the
complexity of the optimization problem. For global robustness
certiï¬cation, we also consider two copies of each sub-network.
However, instead of ï¬nding the output range of each sub-
network copy, we look for the range of one sub-network copy
and the range of the output distance, based on the ITNE.

More speciï¬cally, a w-layer sub-network with input x(iâˆ’w)
and output x(i)
(i â‰¥ w) is denoted as Fw(x(i)
j ). For any variable
j
v, its range is denoted as v = [v, v]. Under ITNE, for each
sub-network Fw(x(i)
j ), given the input range x(iâˆ’w) and input
distance range âˆ†x(iâˆ’w), we can derive the output range x(i)
j
and output distance range âˆ†x(i)
by solving the optimization
j
problem in Eq. (1) for this sub-network.

b) ITNE-based LP relaxation (LPR): The idea of LPR is
to relax the ReLU relation x = max(0, y) into linear constraints
given the range of y as [y, y]. When y â‰¤ 0 or y â‰¥ 0, the ReLU
relation degenerates to x = 0 or x = y. Otherwise, ReLU
relation can be relaxed by three linear inequations:

x â‰¥ 0,

x â‰¥ y,

x â‰¤

y(y âˆ’ y)
y âˆ’ y

(4)

The ReLU relations of a neuron in the two network copies
are formulated in Eqs. (2) and (3). In our work, instead of
relaxing the ReLU relations in two network copies separately,
we relax Eq. (2) by the original LPR in Eq. (4), and relax the
ReLU distance relation based on ITNE:

âˆ†x = relu(y + âˆ†y) âˆ’ relu(y).

(5)

For a given y, the relation between âˆ†x and âˆ†y is shown in
the ï¬rst two plots of Fig. 3. Thus, the shadowed area in the
third plot of Fig. 3 can cover all possible (âˆ†x, âˆ†y) mappings
for âˆ€y âˆˆ R. Given âˆ†y âˆˆ [âˆ†y, âˆ†y], the relation between âˆ†x
and âˆ†y can be bounded by the linear lower and upper bounds
as shown in the third plot of Fig. 3. Formally, the bound is

u(âˆ†y âˆ’ l)
u âˆ’ l
where l = min(0, âˆ†y) and u = max(0, âˆ†y).

l(u âˆ’ âˆ†y)
u âˆ’ l

â‰¤ âˆ†x â‰¤

,

(6)

D. Illustrating example

Consider the example neural network in Fig. 1. Assume
that the input perturbation bound as Î´ = 0.1 and the input

Î”ğ‘¥ğ‘›=à·œğ‘¥(ğ‘›)âˆ’ğ‘¥ğ‘›ğ‘¥0Î”ğ‘¥0à·œğ‘¥(0)à·œğ‘¥(ğ‘›)ğ‘¥ğ‘›ğ‘¦ğ‘—ğ‘–à·œğ‘¦ğ‘—ğ‘–Î”ğ‘¦ğ‘—ğ‘–ğ‘¥ğ‘—ğ‘–à·œğ‘¥ğ‘—ğ‘–Î”ğ‘¥ğ‘—ğ‘–Î”ğ‘¥Î”ğ‘¦âˆ’ğ‘¦ğ‘¦<0Î”ğ‘¥Î”ğ‘¦âˆ’ğ‘¦ğ‘¦â‰¥0Î”ğ‘¦Î”ğ‘¦Î”ğ‘¥Î”ğ‘¦domain as x(0) âˆˆ [âˆ’1, 1]2. The example neural network can be
decomposed into three sub-networks:
1 = relu(y(1)
x(1)
x(1)
2 = relu(y(1)

1 ) = relu(x(0)
2 ) = relu(âˆ’0.5x(0)

1 + 0.5x(0)
2 )
1 + x(0)
2 )

x(2) = relu(y(2)) = relu(x(1)

1 âˆ’ x(1)
2 ).

For local robustness, with input x(0) = [0, 0], the certiï¬cation
processes for different techniques are shown in Fig. 4. The exact
bound of Ë†x(2) is derived by solving the MILP problem for the
entire network. ND solves the MILP problems of the ï¬rst two
sub-networks to derive the bound of Ë†x(1) and then derive the
bound of Ë†x(2) by solving the MILP problem of the third sub-
network. For LPR, given Ë†y(1)
1 , Ë†y(1)
2 , Ë†y(2) âˆˆ [âˆ’0.15, 0.15], all
ReLU relations can be relaxed based on Eq. (4), and the bound
of Ë†x(2) can be derived by solving the relaxed LP problem. As
we can see, ND and LPR can provide tight (1.2x and 1.15x)
over-approximations for local robustness.

For global robustness, under BTNE, there are no distance
variables for hidden neurons. Thus, ND and LPR can only be
applied to each individual network copy. ND will only ï¬nd the
range of x(1) and Ë†x(1) and for the third sub-network, the dis-
tance information between two network copies is lost, resulting
in a 7.5x over-approximation of the range of âˆ†x(2). The LPR is
based on the bounds y(1), Ë†y(1) âˆˆ [âˆ’1.5, 1.5]2, and y(2), Ë†y(2) âˆˆ
[âˆ’1.5, 1.5], resulting in a 10.9x over-approximation. On the
other hand, under ITNE, the ITNE-based ND can ï¬rst ï¬nd
the range of x(1) and âˆ†x(1) and then derive the range of
âˆ†x(2), which is only 1.5x of the exact one. Given âˆ†y(1) âˆˆ
[âˆ’0.15, 0.15]2 and âˆ†y(2) âˆˆ [âˆ’0.3, 0.3], the ITNE-based LPR
derives a tight 1.38x over-approximation of the range of âˆ†x(2).
This shows that combing ITNE with ND and LPR signiï¬cantly
improves the approximation tightness over BTNE.

E. Efï¬cient Global Robustness Over-Approximation Algorithm

and âˆ†y(i)
j

Finally, we design our global robustness certiï¬cation algo-
rithm by leveraging the ITNE encoding as well as the ND and
LPR techniques, as shown in Algorithm 1. Note that the MILP
problems for sub-networks from ND are relaxed by LPR. The
pre-assigned window size W is deï¬ned as the desired depth
of sub-networks. For LPR, besides the range of the input and
distance, y(iâˆ’k) and âˆ†y(iâˆ’k) for âˆ€k âˆˆ [0, w âˆ’ 1] are also
needed. To evaluate y(i)
, we can consider a w-layer
j
sub-network with y(i)
as the output (i.e., no ReLU activation
j
at the output layer), denoted as Fw(y(i)
j ). Formally, we have
two types of sub-network optimization problems: LpRelaxY
and LpRelaxX, which is respectively encoded (lines 7 and 9)
as ITNE of sub-network Fw(y(i)
j ) (decomposed
from network F in lines 6 and 9). Both problems require the
sub-network input range x(iâˆ’w), âˆ†x(iâˆ’w) and the ranges for
hidden neurons, i.e., y(iâˆ’k) and âˆ†y(iâˆ’k), âˆ€k âˆˆ [1, w âˆ’ 1] as
, âˆ†y(i)
prerequisites, while LpRelaxX (line 11) also needs y(i)
,
j
j
which is derived from LpRelaxY (line 8). Therefore,
the
ranges need to be evaluated layer by layer, by treating these
hidden layer neurons as outputs of sub-networks Fw(y(i)
j ) and

j ) and Fw(x(i)

Fig. 4.
Illustrating example: The local and global robustness certiï¬cation
processes of exact MILP, network decomposition (ND) and LP relaxation
(LPR). For global robustness, the ND and LPR for both basic and interleaving
twin-network encoding (i.e., BTNE and ITNE) are illustrated.

Algorithm 1 Global Robustness Certiï¬cation Algorithm
Input: Neural Network F , window size W , reï¬ne param r
Input: input domain X, input perturbation bound Î´
Output: output variation bound Â¯Îµ

1: x(0) = X
2: âˆ†x(i) = [âˆ’Î´, Î´]m0
3: for i = 1 : n do
4:
5:

w = max(i, W )
for j = 1 : mi do

6:

7:

8:

9:

10:

j

j

, w)

Fw(y(i)
j ) â† N etDecompose(F, y(i)
ey = InterleaveT winN etEncode(Fw(y(i)
(y(i)
, âˆ†y(i)
) â† LpRelaxY (ey, r)
j
Fw(x(i)
j ) â† N etDecompose(F, x(i)
ex = InterleaveT winN etEncode(Fw(x(i)
(x(i)
j ) â† LpRelaxX(ex, r)
end for

j , âˆ†x(i)

j , w)

j ))

j ))

11:
12:
13: end for
14: Â¯Îµ â† max(|âˆ†x(n)|, |âˆ†x(n)|)

Fw(x(i)
layer, all ranges of previous layers have been derived.

j ). In such a way, when evaluating the ranges of certain

Selective Reï¬nement: While LPR can remove all integer
variables in the MILP formulation to reduce the complexity,
such extreme over-approximation may be too inaccurate. Thus,
we try to selectively reï¬ne a limited number of neurons, by not
relaxing their ReLU relations. This is similar to the layer-level
reï¬nement idea in [6], but with a focus on global robustness.
Speciï¬cally, a score of LPR is evaluated for each neuron as the
worst-case inaccuracy, and the top r neurons will be reï¬ned.
The score is deï¬ned as the maximum distance between the
lower and upper bound of LPR, i.e., the score is âˆ’yy/(y âˆ’ y)

Global Robustnessğ‘–=0ğ‘–=1ğ‘–=2ExactÎ”ğ‘¥(ğ‘–)âˆ’0.1,0.12âˆ’0.2,0.2ğ‘¥(ğ‘–),à·œğ‘¥(ğ‘–)âˆ’1,120,1.25Basic EncodingNDÎ”ğ‘¥(ğ‘–)âˆ’0.1,0.12âˆ’1.5,1.5ğ‘¥(ğ‘–),à·œğ‘¥(ğ‘–)âˆ’1,120,1.520,1.5LPRÎ”ğ‘¥(ğ‘–)âˆ’0.1,0.12âˆ’2.85,1.5ğ‘¥(ğ‘–),à·œğ‘¥(ğ‘–)âˆ’1,120,1.44InterleavingNDÎ”ğ‘¥(ğ‘–)âˆ’0.1,0.12âˆ’0.15,0.152âˆ’0.3,0.3ğ‘¥(ğ‘–),à·œğ‘¥(ğ‘–)âˆ’1,120,1.520,1.5LPRÎ”ğ‘¥(ğ‘–)âˆ’0.1,0.12âˆ’0.275,0.275ğ‘¥(ğ‘–),à·œğ‘¥(ğ‘–)âˆ’1,120,1.44MILPRelaxed LPMILPMILPRelaxedLPLocal Robustnessà·œğ‘¥(0)à·œğ‘¥(1)à·œğ‘¥(2)Exactâˆ’0.1,0.120,0.125NDâˆ’0.1,0.120,0.1520,0.15LPRâˆ’0.1,0.120,0.144MILPMILPMILPRelaxedLPTABLE I
NEURAL NETWORK SETTING AND EXPERIMENTAL RESULTS.

Dataset

Auto
MPG

Dataset
Auto
MPG

MNIST

ID
1
2
3
4
ID

5

6

7

8

Neurons
8
12
16
32
Neurons

64

1416

3872

5824

tR
2s
130s
8h
> 24h

tM
0.1s
0.2s
0.8s
74s

Layers

FC:3

Conv:1
FC:2
Conv:2
FC:2
Conv:3
FC:2

tour
0.3s
0.4s
1s
5s
tour

50s

4.8h

3.3h

3.5h

Îµ
0.0583
0.0527
0.0496
0.0481
Îµ

Îµ (ours)
0.0657
0.0722
0.0653
0.0673
Îµ (ours)

0.0452

0.0731

0.347
0.300
0.453
0.420
0.519
0.407

0.578
0.572
0.874
0.723
1.521
1.175

In Table I, â€˜Layersâ€™ are the type and number of layers; â€˜Neuronsâ€™ is the total
number of hidden neurons; tR, tM , tour are the certiï¬cation time of exact
MILP, Reluplex, and our approach, respectively; Îµ, Îµ, Îµ are the exact (derived
by Reluplex or MILP), under-approximated (derived by projected gradient
descent (PGD) among dataset), and over-approximated (derived by our
approach) output variation bounds, respectively.

for Eq. (4), and max(|âˆ†y|, |âˆ†y|) for Eq. (6). The number of
neurons for reï¬nement r is a parameter that can be set in LPR.
Complexity Analysis: MILPâ€™s complexity is polynomial to
the number of variables and exponential to the number of
integer variables. In our approach, for each MILP, the number
of variables is linear to the number of neurons in the sub-
network, and the number of integer variables is linear to the
number of neurons selected for reï¬nement. The number of
MILPs to solve is linear to the total number of neurons.

III. EXPERIMENTAL RESULTS

We ï¬rst evaluate our algorithm on various DNNs and com-
pare its results with exact global robustness (when available)
and an under-approximated global robustness. Then, we demon-
strate the application of our approach in a case study of safety
veriï¬cation for a vision-based robotic control system, and show
the importance of efï¬cient global robustness certiï¬cation for
safety-critical systems that involve neural networks.

We implement our efï¬cient global robustness certiï¬cation
algorithm in Python. All MILP/LP problems in the algorithm
are solved by Gurobi [14]. All the experiments are conducted
on a platform with a 4-core 3.6 GHz Intel Xeon CPU and 16
GB memory. All neural networks are modeled in Tensorï¬‚ow.

A. Comparison with Other Methods

We conduct experiments on a set of DNNs with different
sizes. The DNNs are trained on two datasets: the Auto MPG
dataset [15] for vehicle fuel consumption prediction, and the
MNIST handwritten digits classiï¬cation dataset [16]. Besides
the fully-connected (FC) output layer, the Auto MPG DNNs
contain 2 FC hidden layers with the number of hidden neurons
ranging from 8 to 64. MNIST DNNs contain 1 to 3 convolu-
tional layers followed by 1 FC hidden layer, with 1472 to 7176
hidden neurons. We certify the output variation bound Îµ based
on the input perturbation bound Î´, where Î´ = 0.001 for Auto
MPG DNNs and Î´ = 2/255 for MNIST DNNs.

The network settings and the comparison among different
approaches are shown in Table I. For our approach, window

size W = 2 for Auto MPG DNNs, and W = 3 for MNIST
DNNs. Half neurons are reï¬ned in Auto MPG DNNs and 30
neurons in each layer are reï¬ned in MNIST DNNs. We present
2 outputs of MNIST (out of 10) in Table I, and others show
similar differences between different methods.

For small networks (DNN-1 to DNN-5), we compare our
over-approximated output variation bound Îµ with the exact
bound Îµ solved by Reluplex [2] (latest version that is integrated
in the tool Marabou [17] is used) and the MILP encoding
in Eq. (1). We can see that the runtime of Reluplex tR and
MILP tM quickly increases with respect to neural network size.
None of them can get a solution within 24 hours for DNN-5
(with only 64 hidden neurons). From DNN-1 to DNN-4, our
algorithm can derive an over-approximated global robustness
with much slower runtime increase, while only have about 13%
to 40% over-approximation over the exact global robustness.

Starting from DNN-5, the two exact certiï¬cation methods
cannot ï¬nd a solution in reasonable time. There is also no
other work in the literature that can derive a sound and
deterministic global robustness. To assess how good our over-
approximated results are for larger networks, we evaluate an
under-approximation of the global robustness, inspired by [9].
Speciï¬cally, for each data sample in the dataset, we leverage
projected gradient descent (PGD) [18] to look for an adversarial
example in the input perturbation bound that maximizes the
output variation, and treat the maximal output variation among
the entire dataset as an under-approximated output variation
bound Îµ. The exact global robustness should be between the
under-approximation Îµ derived from this dataset-wise PGD, and
the over-approximation Îµ derived by our certiï¬cation algorithm.
The experiments from DNN-6 to DDN-8 demonstrate that our
method can provide meaningful over-approximation (less
than 3x of the under-approximation) for DNNs with more
than 5000 hidden neurons within 5 hours.

B. Case Study on Control Safety Veriï¬cation

For control systems that use neural networks for perception,
a critical and yet challenging question is whether the system
can remain safe when there is perturbation/disturbance to the
perception neural network input. In [13], the authors formulate
this as a design-time safety assurance problem based on global
robustness (although did not provide a solution), i.e., if a global
robustness bound (cid:15) can be derived for an input perturbation
bound Î´, such (cid:15) can be viewed as the state estimation error
bound in control and leveraged for control safety veriï¬cation.
In this section, leveraging our approach for bounding global
robustness, we conduct a case study for safety veriï¬cation of
control systems that use neural networks for perception. We
consider an advanced cruise control (ACC) scenario where an
ego vehicle is following a reference vehicle. The ego vehicle
is equipped with an RGB camera, and a DNN is designed to
estimate the distance from the reference vehicle based on the
camera image. We assume that the captured image may be
slightly perturbed. A feedback controller takes the estimated
distance, along with the ego vehicle speed, to generate the
control input, which is the acceleration of the ego vehicle.

Method (FGSM) [21]. With more than 1000 minutes of simu-
lation, we ï¬nd that when the perturbation bound is Î´ = 2/255
as assumed, the distance estimation error âˆ†d never exceeds the
[âˆ’0.14, 0.14] bound and the system is always safe (as expected
since it was formally proven). When the perturbation bound Î´ is
increased to 5/255, âˆ†d sometimes exceeds the bound, although
unsafe system state is not observed. When Î´ is increased to
10/255, about 17% of the simulations encounter unsafe states.
This shows the impact of input perturbation on system safety
and the importance of our global robustness analysis.

IV. CONCLUSION
We present an efï¬cient certiï¬cation algorithm to provide
a sound and deterministic global robustness for neural net-
works with ReLU activation. Experiments demonstrate that
our approach is much more efï¬cient and scalable than exact
global robustness certiï¬cation approaches while providing tight
over-approximation, and a case study further demonstrates the
application of our approach in safety-critical systems. Our
future work will continue improving the approachâ€™s efï¬ciency,
possibly via parallelization on multi-core platforms, to enable
its usage for larger-size perception neural networks.

REFERENCES

[1] B. Biggio, I. Corona, D. Maiorca et al., â€œEvasion attacks against machine

learning at test time,â€ in ECML PKDD, 2013, pp. 387â€“402.

[2] G. Katz, C. Barrett, D. L. Dill et al., â€œReluplex: An efï¬cient smt solver

for verifying deep neural networks,â€ in CAV, 2017, pp. 97â€“117.

[3] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, â€œSafety veriï¬cation

of deep neural networks,â€ in CAV, 2017, pp. 3â€“29.

[4] A. Lomuscio and L. Maganti, â€œAn approach to reachability analysis for

feed-forward relu neural networks,â€ arXiv, 2017.

[5] G. Singh, T. Gehr, M. PÂ¨uschel et al., â€œAn abstract domain for certifying

neural networks,â€ Proc. ACM Program. Lang., vol. 3, Jan. 2019.

[6] C. Huang, J. Fan, X. Chen et al., â€œDivide and slide: Layer-wise reï¬nement
for output range analysis of deep neural networks,â€ TCAD, vol. 39, 2020.
[7] C.-H. Cheng, G. NÂ¨uhrenberg, and H. Ruess, â€œMaximum resilience of

artiï¬cial neural networks,â€ in ATVA, 2017, pp. 251â€“268.

[8] Y. Chen, S. Wang, Y. Qin et al., â€œLearning security classiï¬ers
with veriï¬ed global robustness properties,â€ 2021. [Online]. Available:
https://arxiv.org/abs/2105.11363

[9] W. Ruan, M. Wu, Y. Sun et al., â€œGlobal robustness evaluation of deep
neural networks with provable guarantees for the hamming distance,â€ in
IJCAI, 2019.

[10] O. Bastani, Y. Ioannou, L. Lampropoulos et al., â€œMeasuring neural net

robustness with constraints,â€ in NeurIPS, vol. 29, 2016.

[11] R. Mangal, A. V. Nori, and A. Orso, â€œRobustness of neural networks: A

probabilistic and practical approach,â€ in ICSE-NIER, 2019, pp. 93â€“96.

[12] D. Gopinath, G. Katz, C. S. PË˜asË˜areanu et al., â€œDeepsafe: A data-driven

approach for assessing robustness of neural networks,â€ in ATVA, 2018.

[13] Z. Wang, C. Huang, Y. Wang et al., â€œBounding perception neural network

uncertainty for safe control of autonomous systems,â€ in DATE, 2021.

[14] Gurobi Optimization, LLC, â€œGurobi Optimizer Reference Manual,â€

2021. [Online]. Available: https://www.gurobi.com

[15] D. Dua and C. Graff, â€œUCI machine learning repository,â€ 2017. [Online].

Available: http://archive.ics.uci.edu/ml

[16] Y. Lecun, L. Bottou, Y. Bengio et al., â€œGradient-based learning applied
to document recognition,â€ Proceedings of the IEEE, vol. 86, 1998.
[17] G. Katz, D. A. Huang, D. Ibeling et al., â€œThe marabou framework for
veriï¬cation and analysis of deep neural networks,â€ in CAV, 2019.
[18] A. Madry, A. Makelov, and L. S. andothers, â€œTowards deep learning

models resistant to adversarial attacks,â€ in ICLR, 2018.

[19] Webots, â€œCommercial mobile robot simulation software.â€ [Online].

Available: http://www.cyberbotics.com

[20] C. Huang, S. Xu, Z. Wang et al., â€œOpportunistic intermittent control with
safety guarantees for autonomous systems,â€ in DAC, 2020, pp. 1â€“6.
[21] I. J. Goodfellow, J. Shlens et al., â€œExplaining and harnessing adversarial

examples,â€ in ICLR, 2015.

Fig. 5.
(a) Webots simulation for case study, where ego vehicle (left) follows
reference vehicle (right); (b) An example image captured by ego vehicle; (c)
Lower bound of DNN input space; (d) Upper bound of DNN input space.

We model this example in the tool Webots [19] (Fig. 5).
The ego vehicle is safe if distance d âˆˆ [0.5, 1.9] and speed
ve âˆˆ [0.1, 0.7]. The reference vehicle speed vr is randomly
adjusted within [0.2, 0.6]. The sampling period is 100ms. The
camera takes RGB images with resolution 24Ã—48. A 5-layer (3
convolutional and 2 FC) DNN is trained with 100k pre-captured
images. The system dynamics is modeled as:

x[k+1] =

(cid:21)
(cid:20)1 âˆ’0.1
0

1

x[k]+

(cid:21)

(cid:20)âˆ’0.005
0.1

(cid:21)
(cid:20)1
0

u[k]+

w1[k]+w2[k]

where the system state x = [d âˆ’ 1.2, ve âˆ’ 0.4](cid:62) contains the
normalized distance and speed. w1 = 0.4 âˆ’ vr is the external
disturbance, where vr is bounded within [0.2, 0.6] as previously
mentioned. w2 = [wd, wv](cid:62) is the inaccuracy of this system
dynamics model. The control input follows the feedback control
law u = K Ë†x, where K = [0.3617, âˆ’0.8582]. Ë†x is the estimated
system state, and the state estimation error is âˆ†x = Ë†x âˆ’ x.
The estimation error of ego vehicle speed ve is assumed as 0.
The bound of wd, wv are proï¬led based on simulations, and
bounded as |wd| â‰¤ 5e âˆ’ 4 and |wv| â‰¤ 3e âˆ’ 5. The estimation
error for distance contains two parts âˆ†d = âˆ†d1 + âˆ†d2, where
âˆ†d1 is the error coming from the DNN model inaccuracy, while
âˆ†d2 is the output variation caused by input perturbation.

The DNN model

inaccuracy âˆ†d1 is determined by the
worst-case model inaccuracy among the dataset and set as
|âˆ†d1| â‰¤ 0.0730. The DNN output variation âˆ†d2 is the focus
of this study and can be bounded by our global robustness
certiï¬cation algorithm. Speciï¬cally, the input perturbation in
this study is bounded by Î´ = 2/255 (Fig. 5 illustrates the
upper and lower bound of each pixel for the input space). Then,
using our approach, we can derive a certiï¬ed output variation
bound |âˆ†d2| â‰¤ (cid:15) = 0.0568. Combined with âˆ†d1, we have
|âˆ†d| â‰¤ 0.0730 + 0.0568 = 0.1298.

With bounds on âˆ†d and other variables, the vehicle control
safety can be veriï¬ed based on the computation of control
invariant set, similarly as in [20]. Through such invariant
set based veriï¬cation, we ï¬nd that as long as the distance
estimation error âˆ†d is within [âˆ’0.14, 0.14], the system will
always be safe. Since our global robustness certiï¬cation bounds
âˆ†d â‰¤ 0.1298, we can assert that our ACC system is safe with
this DNN design under the assumed perturbation bound.

We also deploy our DNN model in Webots and add adver-
sarial perturbation on the input images by Fast Gradient Sign

(a)(b)(c)(d)