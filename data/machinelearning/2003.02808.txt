0
2
0
2

r
a

M
5

]

G
L
.
s
c
[

1
v
8
0
8
2
0
.
3
0
0
2
:
v
i
X
r
a

Linear time dynamic programming for the exact path of optimal
models selected from a ﬁnite set

Toby Hocking, Joseph Vargovich

March 6, 2020

Abstract

Many learning algorithms are formulated in terms of ﬁnding model parameters which minimize a data-
ﬁtting loss function plus a regularizer. When the regularizer involves the (cid:96)0 pseudo-norm, the resulting
regularization path consists of a ﬁnite set of models. The fastest existing algorithm for computing the
breakpoints in the regularization path is quadratic in the number of models, so it scales poorly to high
dimensional problems. We provide new formal proofs that a dynamic programming algorithm can be
used to compute the breakpoints in linear time. Empirical results on changepoint detection problems
demonstrate the improved accuracy and speed relative to grid search and the previous quadratic time
algorithm.

1

Introduction

In this paper we propose a new algorithm related to the regularization path of machine learning problems
such as

where θ ∈ Rp is a vector of model parameters, the loss function L : Rp → R is typically convex, and λ ≥ 0 is
a penalty constant. The regularizer R : Rp → R+ is a non-convex function involving the (cid:96)0 pseudo-norm,

ˆθ(λ) = arg min

L(θ) + λR(θ),

θ

(1)

||θ||0 =

p
(cid:88)

j=1

I[θj (cid:54)= 0] ∈ Z+ = {0, 1, . . . , p}

(2)

which counts the number of non-zero entries of the θ parameter vector (I is the indicator function). Some
typical examples are given in Table 1, which includes best subset regression [Miller, 2002, Soussen et al., 2010],
optimal segmentation [Auger and Lawrence, 1989, Jackson et al., 2005], k-means clustering [MacQueen, 1967],
and low-rank matrix factorization [Huang and Wolkowicz, 2018]. For learning it is important to compute not
just a single model for one penalty λ, but also the full regularization path {ˆθ(λ)|λ ≥ 0} = {Θ1, Θ2, . . . , ΘN }.
The path is a ﬁnite set of N models, i.e. for any λ ≥ 0, we have ˆθ(λ) = Θk for some model size k ∈ {1, . . . , N }.
To simplify the presentation we limit our discussion to regularizers R(Θk) = k which are equal to model
size. More general regularizers, e.g. R(Θk) = rk for some sequence of increasing values r1 < · · · < rN , can
be handled using a straightforward modiﬁcation of our proposed algorithm.

One example is best subset regression, which seeks the best k features for a linear regression function. In
this problem there a total of p input features and therefore a set of N = p + 1 models in the regularization
path {Θ0, Θ1, . . . , Θp}. This non-convex problem is NP-hard, so the optimal regularization path can only be
computed for low-dimensional problems [Bertsimas et al., 2016]. For high-dimensional problems there are

1

 
 
 
 
 
 
Problem
Best Subset Regression
Optimal Segmentation
k-means Clustering
Matrix Factorization

Loss
||Xθ − y||2
2
||θ − y||2
2
||θM − X||F
||U Diag(θ)V T − X||F

Regularizer
||θ||0
||Dθ||0
||θT 1||0
||θ||0

Model complexity
features selected
segments/changepoints
cluster centers
rank

Table 1: Examples of machine learning problems which use the (cid:96)0 pseudo-norm as a regularizer.

various heuristic algorithms for computing approximate solutions, e.g. greedy forward/backward selection
[Mallat and Zhang, 1993, Davis et al., 1994, Miller, 2002, Schniter et al., 2009, Soussen et al., 2010], non-
smooth non-convex regularizers [Fan and Li, 2001, Zhang, 2010, Mazumder et al., 2011, van den Burg et al.,
2017], and (cid:96)1 regularization/LASSO [Tibshirani, 1996, Chen et al., 1998]. Each weight vector Θk ∈ Rp in
the optimal or approximate regularization path has k non-zero entries, R(Θk) = ||Θk||0 = k. The extreme
elements correspond to the ordinary least squares solution Θp with all features selected, and the completely
regularized solution Θ0 with no features selected.

Another example is optimal segmentation, which is the maximum likelihood model with k segments (k − 1
changepoints) for a sequential data set. In this problem there are p sequence data, and each element Θk
of the regularization path has k ∈ {1, . . . , p} distinct segments (k − 1 changes) along the sequence, i.e.
R(Θk) = 1 + ||DΘk||0 = k where D ∈ Rp−1×p is the matrix which returns the diﬀerence between adjacent
pairs of data in the sequence. Even though this problem is non-convex, an optimal solution can be computed
via dynamic programming algorithms that are log-linear in the number of data, and linear in the number
of models [Killick et al., 2012, Maidstone et al., 2016]. There are also several fast heuristic algorithms,
including binary segmentation [Scott and Knott, 1974, Truong et al., 2018], which computes an approximate
regularization path of N = p models in O(N log N ) time on average and O(N 2) in the worst case. The
optimal or approximate regularization path of N = p models {Θ1, Θ2, . . . , Θp} has extreme elements Θ1
with no changes (one common segment/parameter for the entire data sequence) and Θp with a change after
every data point (a diﬀerent segment/parameter for each data point).

In this context solving the penalized problem (1) for a given penalty λ ≥ 0 results in one of the solutions to
the corresponding constrained problem,

Lk = min

θ

L(θ), subject to R(θ) ≤ k,

(3)

where k ∈ Z+ is the model size (selected features, changepoints, clusters, etc). If the loss values L1 > · · · >
LN in a regularization path {Θ1, Θ2, . . . , ΘN } are known, they can be used to deﬁne the model selection
function

k∗
N (λ) = arg min
k∈{1,...,N }

.

Lk + λk
(cid:124) (cid:123)(cid:122) (cid:125)
fk(λ)

(4)

The model selection function (4) returns the (smallest) model complexity k which is optimal for a given
penalty parameter λ. In this paper we provide a new formal proof that dynamic programming can be used
to compute an exact representation of the model selection function k∗

N (λ) in linear O(N ) time.

1.1 Existing algorithms and related work

The model selection function k∗
N (λ) can be trivially evaluated for a single λ parameter in linear O(N ) time,
which yields the solution to (1) via ˆθ(λ) = Θk∗
N (λ). However for some learning algorithms we need an exact
representation of the model selection function for a full path of penalty λ values. Quadratic O(N 2) time
algorithms for computing the full path have been proposed for changepoint detection [Lavielle, 2005, Hocking
et al., 2013] and regression [Arlot and Massart, 2009], but these algorithms are too slow for high-dimensional
problems (e.g. full path of binary segmentation models for large data sets, see Section 4).

2

The algorithm we propose is similar to the “convex hull trick” which is informally described, without any
references to the machine learning literature, on a web page [PEGWiki, 2018]. The novelty of our paper with
respect to that previous work is (1) rigorous formal proofs of the linear time complexity and optimality, (2)
explaining the relevance to the machine learning literature, (3) detailed theoretical and empirical comparisons
with baseline algorithms.

A ﬁnal related work is the CROPS algorithm of Haynes et al. [2017], which also proposes an algorithm
that outputs an exact path of solutions for several penalty values. Both papers exploit the structure of the
piecewise linear model selection function which relates the constrained and penalized problems. The input
to our algorithm is a sequence of constrained models of sizes 1 to N , whereas the input to CROPS is an
interval of penalty values. In general the two algorithms output diﬀerent results (partial solution paths).
However in the special case when N = p models are input to our algorithm (all possible models) and the
interval (0, ∞) is input to CROPS, then the two algorithms return the same output (the full path).

1.2

Important deﬁnitions

In this section, we will reference several concepts that are related to our proposed algorithm. Changepoints
refer to a sudden deviation from previously recorded values within a given data set. Segments are used to
ﬁt each unique block data with an average value line. The model complexity (k) refers to the total amount
of unique segments present in a given segmentation model. A regularization path is the path of optimal
segmentation models selected for a given range of penalties. Breakpoints represent the penalty value in
which the algorithm switches from a previously selected model to a new one based on the evaluation of the
model selection function (4).

1.3 Contributions and organization

In this paper we propose a new dynamic programming algorithm for computing the model selection function
k∗
N (λ), and we prove that it computes an exact representation for all penalties λ ≥ 0 (Section 2). Our second
contribution is a theoretical analysis of the time complexity of our algorithm, which demonstrates that it is
linear O(N ) time in the worst case; we also provide a theoretical analysis of previous algorithms in terms of
the framework of this paper (Section 3). Our third contribution is an empirical study of time complexity in
several real and synthetic data sets, including a comparison with previous algorithms (Section 4). Our ﬁnal
contribution is an empirical study of the prediction accuracy in cross-validation experiments on supervised
changepoint detection problems (Section 5). The paper concludes with a discussion (Section 6).

2 Dynamic programming algorithm

We propose a dynamic programming algorithm for N decreasing loss values L1 > · · · > LN ; it computes an
exact representation of the k∗

N (λ) model selection function for all penalties λ ≥ 0.

2.1 Exact representation of a piecewise constant function using breakpoints

Our proposed algorithm recursively computes k∗
N from k∗
N −1, so is an instance of dynamic programming
[Bellman, 1961]. At each step/iteration t ∈ {1, . . . , N } of the algorithm, the algorithm stores a set of
Mt ∈ {1, . . . , t} selectable models,

1 = Kt,1 < Kt,2 < · · · < Kt,Mt = t.

(5)

3

The algorithm also stores a corresponding set of breakpoints,

∞ = bt,0 > bt,1 > · · · > bt,Mt = 0.

These two sets deﬁne for all t ≥ 1 a recursively computed model selection function,

Ft(λ) =






Kt,1

if λ ∈ (bt,1, bt,0)

...

Kt,Mt

if λ ∈ (bt,Mt, bt,Mt−1)

(6)

(7)

We prove later (Theorem 1) that the recursively computed function FN is identical to k∗
N , the desired model
selection function (4). The geometric interpretation of the models Kt,i ∈ Z and breakpoints bt,i ∈ R are
shown in Figure 1. Each breakpoint is a penalty value where the min cost (grey segments) changes from one
cost function to another (black lines).

2.2 Dynamic programming update rules

The algorithm starts at the ﬁrst iteration t = 1 by initializing M1 = 1 model with

K1,1 = 1, b1,1 = 0, b1,0 = ∞,

(8)

which is an exact representation of the ﬁrst model selection function F1. For all other iterations t > 1, the
algorithm begins by discarding any breakpoints which are no longer necessary, then adds one new breakpoint.
In particular it ﬁrst computes the index i corresponding to the largest model from the previous iteration
t − 1 which is still selected at iteration t,

It = max (cid:8)i ∈ {1, . . . , Mt−1} | c(t, i) < bt−1,i−1

(cid:9).

(9)

The new candidate breakpoint c(t, i) is the penalty for which the new cost function ft is equal to a previous
function fKt−1,i,

c(t, i) =

LKt−1,i − Lt
t − Kt−1,i

.

(10)

12

10

8

6

4

2

0

k
λ
+
k
L
=
)
λ
(
k
f

t
s
o
C

L1 = 7

L2 = 4

c(2, 1)
f1

f2

b2,2 = 0.0

t = 2

b2,0 = ∞

b2,1 = 3.0

t = 3, It = 1
c(3, 2)

b3,0 = ∞

b3,1 = 3.5

b2,1 removed

t = 3, It = 2

b2,1 kept
c(3, 2)

b3,0 = ∞

b3,1 = 3.0

K2,1 = 1
K2,2 = 2

L1 = 7

L2 = 4

f3

K3,1 = 1
K3,2 = 3

L1 = 7

L2 = 4
L3 = 2

0

2

4

6

L3 = 0

b3,2 = 0.0
4
2
0

6

Penalty λ

b3,2 = 2.0

K3,1 = 1
K3,2 = 2
K3,3 = 3

f3

b3,3 = 0.0

0

2

4

6

Figure 1: Two possible runs of the algorithm for N = 3 models. Left: at iteration t = 2, the cost function
f2(λ) is minimal (grey curve) for penalties λ < b2,1 = c(2, 1), and f1(λ) is minimal otherwise. Middle: at
iteration t = 3 a small L3 value results in c(3, 2) > b2,1 so we remove b2,1 and K2,2 = 2, because f2(λ) is
no longer minimal for any λ. Right: at iteration t = 3 a large L3 value results in c(3, 2) < b2,1 so we keep
b2,1 = b3,1 and store a new breakpoint c(3, 2) = b3,2.

4

A modiﬁcation to use more general regularizers involves replacing t − Kt−1,i with rt − rKt−1,i in the denom-
inator of equation (10).

We then compute the number of models selected at iteration t,

Mt = It + 1.

The algorithm stores the new set of models selected at iteration t,

Kt,i =

(cid:40)

Kt−1,i
t

for i ∈ {1, . . . , It}
for i = Mt.

The algorithm also stores the new breakpoint c(t, It) along with some of the previous breakpoints,

bt,i =






bt−1,i
c(t, It)
0

for i ∈ {0, . . . , It − 1}
for i = It
for i = Mt.

(11)

(12)

(13)

Once the selected models Kt,i and breakpoints bt,i have been recursively computed via (12–13), the model
selection function Ft is deﬁned using equation (7).

2.3 Demonstration of algorithm up to t = 3

In this section we provide two example runs of the algorithm. The initialization creates an exact represen-
tation of k∗
1 = F1, via one possible model K1,1 = 1 which is selected for all λ between the two breakpoints
b1,1 = 0 < ∞ = b1,0. The second iteration computes the candidate breakpoint c(2, 1) which is stored in the
new breakpoints b2,2 = 0 < c(2, 1) < ∞ = b2,0, along with M2 = 2 models K2,2 = 2 > 1 = K2,1 (Figure 1,
left).

At iteration t = 3 we ﬁrst compute the candidate c(3, 2) and compare it to the stored breakpoint b2,1.
If c(3, 2) ≥ b2,1 then I3 = 1 and so b2,1 is removed (Figure 1, middle). The candidate c(3, 2) is also
discarded; the new breakpoints b3,2 = 0 < c(3, 1) < ∞ = b3,0 are computed and stored with M3 = 2 models
K3,2 = 3 > 1 = K3,1. Otherwise, c(3, 2) < b2,1 implies I3 = 2 and the previous breakpoint b2,1 is kept along
with the candidate c(3, 2) (Figure 1, right). The breakpoints b3,3 = 0 < c(3, 2) < b2,1 < ∞ = b3,0 are stored
with M3 = 3 models K3,3 = 3 > 2 > 1 = K3,1.

2.4 Recursive update rules are optimal

Equations (5–13) deﬁne a dynamic programming algorithm, because the recursively computed FN is optimal
in the sense of equation (4), as demonstrated in the following theorem.
Theorem 1 (Update rules yield the optimal model selection function). The recursively computed function
FN (7) and the model selection function k∗

N (4) are identical.

Proof. The proof follows from equations (5–13) using induction on t. The base case is t = 1, for which
the initialization (8) of the recursively computed function implies F1(λ) = 1 for all λ ∈ (0, ∞). Because at
iteration t = 1 there is only one possible model, it is clear that F1(λ) = k∗

1(λ) for all λ.

The proof by induction now assumes that Ft−1(λ) = k∗

t−1(λ) for all λ; we will prove that the same is true

5

Algorithm
This paper, Algorithm 1

Best Worst
O(N )
O(N )
[Arlot and Massart, 2009, Hocking et al., 2013] O(N ) O(N 2)
O(N 2) O(N 2)

Always quadratic

Table 2: Summary of asymptotic time complexity in terms of number of input models, N .

for t. The recursive updates (12–13) imply that

Ft(λ) =



Kt−1,1


Kt−1,It

t

if λ ∈ (bt−1,1, bt−1,0)

...

if λ ∈ (c(t, It), bt−1,It−1)
if λ ∈ (0, c(t, It))

=

(cid:40)

Ft−1(λ)
t

if λ > c(t, It)
if λ < c(t, It)

(14)

(15)

We need to prove that the function above returns the model size k ∈ {1, . . . , t} with min cost fk(λ), for any
penalty λ. Equations (9–10) imply Kt−1,It = Ft−1[c(t, It)] is the min cost model at the penalty c(t, It) where
the new cost function ft equals the previous min cost function,

ft[c(t, It)] = fKt−1,It

[c(t, It)] =

min
k∈{1,...,t−1}

fk[c(t, It)].

(16)

Because ft(λ) = Lt + λt is a linear function with a larger slope than any of f1, . . . , ft−1, and a smaller
intercept Lt < Lt−1 < · · · , we therefore deduce that ft is less costly before c(t, It), and more costly after:

(cid:40)

ft(λ) < mink∈{1,...,t−1} fk(λ)
ft(λ) > mink∈{1,...,t−1} fk(λ)

for all λ < c(t, It)
for all λ > c(t, It)

(17)

Combining equations (15,17) and using the induction hypothesis completes the proof that Ft(λ) = k∗
arg mink∈{1,...,t} ft(λ) for all λ.

t (λ) =

3 Theoretical complexity analysis

In this section we propose pseudocode that eﬃciently implements the dynamic programming algorithm, and
provide a proof of worst case linear time complexity. We also provide a theoretical analysis of the previous
quadratic time algorithm in terms of the framework of this paper.

3.1 Proposed linear time algorithm

We propose Algorithm 1, which is pseudocode for equations (9–13). It recursively computes an exact repre-
sentation of the model selection function FN in terms of breakpoints b and selected models K.

It begins by initializing the model selection function F1 (line 3). Then for all t ∈ {2, . . . , N } it recusively
computes Ft from Ft−1. The ﬁrst step in the loop (line 5) is to call the Solve sub-routine, which computes
the number of selected models Mt and the new breakpoint λ = c(t, Mt − 1). The number of while loop
evaluations w[t] can optionally be stored in order to analyze empirical time complexity. The next step is to
store the new model t and new breakpoint λ (line 6), which completes the computation of Ft.

6

Algorithm 1 Dynamic programming for computing exact representation of model selection function

1: Input: Array of N real numbers L[1] > · · · > L[N ] (decreasing loss values).
2: Allocate: selected models K ∈ ZN , breakpoints b ∈ RN , while loop iterations w ∈ ZN
3: Initialize: number of models M ← 1, breakpoint b[1] ← ∞, selected model K[1] ← 1
4: for t = 2 to N do
5: M, λ, w[t] ← Solve(K, b, L, M, t)
6:
7: end for
8: Output: selected models K[1 : M ], breakpoints b[1 : M ], while loop iterations w[2 : N ]

b[M ] ← λ, K[M ] ← t // store a new breakpoint

In this paper we propose an amortized constant O(1) time implementation of the Solve sub-routine (Algo-
rithm 2). It computes It, Mt by solving the maximization in equation (9) using a linear search over possible
values of the model index i. It starts at the current number of selected models (line 2), and then repeatedly
tests the criterion from equation (9). If the current value of the model index i does not satisfy the condi-
tion of the while loop (line 3), then the model index is decremented to remove a breakpoint (line 4). The
number of while loop iterations wt (lines 2,4) can be optionally computed in order to analyze the empirical
time complexity of the algorithm. Even though Algorithm 2 is clearly O(M ) in the worst case, in the next
section we prove that it is amortized constant O(1) time when used in the context of Algorithm 1. Using
this sub-routine therefore results in an overall linear O(N ) time complexity for Algorithm 1, in the best and
worst case (Table 2).

Algorithm 2 Proposed Solve sub-routine
1: Input: selected model sizes K ∈ ZN , breakpoints b ∈ RN , loss values L ∈ RN , number of selected models

M ∈ Z, new model size t ∈ Z.

2: i ← M, wt ← 1
3: while λ ← (L[K[i]] − L[t])/(t − K[i]) ≥ b[i] do
i − −, wt + + // remove a breakpoint
4:
5: end while
6: Output: number of models i + 1, new breakpoint λ, number of while loop iterations wt

3.2 Previous quadratic algorithms

In this section we provide a detailed comparison with several previously proposed quadratic algorithms [Arlot
and Massart, 2009, Hocking et al., 2013]. In terms of the framework of this paper, these previous algorithms
can be interpreted as computing Kt,i, bt,i for t = N , without computing any of the solutions at the previous
iterations t < N . These other algorithms are therefore not performing dynamic programming. Whereas our
algorithm starts at the smallest model size and then updates the model selection function for larger sizes,
these other algorithms begin at the largest model size. In particular they start by initializing the largest
model KN,MN = N and the smallest breakpoint bN,MN = 0, then for all i ∈ {MN − 1, . . . , 1} they recursively
compute KN,i, bN,i from KN,i+1, bN,i+1. There are MN − 1 iterations of this recursive computation, and each
iteration considers KN,i+1 − 1 breakpoints. The overall algorithm is therefore O(N MN ); best case O(N ) is
when the number of selected models MN = 2 is small; worst case O(N 2) is when MN = N is large (Table 2).
Interestingly, the opposite is true of our algorithm (best case is when MN is large), as we prove in the next
section.

7

3.3 Proof of linear time and space complexity

The overall space complexity of Algorithm 1 is clearly O(N ), because up to N possible models/breakpoints
can be computed. The time complexity depends on the implementation of the Solve sub-routine (line 5).

The computation time of our proposed implementation of the Solve sub-routine (Algorithm 2) depends on
wt, the number of times the while condition is evaluated (line 3). In particular, the overall time complexity
of Algorithm 1 is linear in total number of times the while condition is checked,

WN =

N
(cid:88)

t=2

wt.

(18)

The following result proves that Algorithm 1 is overall O(N ) time, by bounding the total number of times
the while condition is checked.
Theorem 2 (Best and worst case time complexity). For any N inputs to Algorithm 1, the total number of
while loop iterations WN over all calls to Algorithm 2 is bounded: N − 1 ≤ WN ≤ 2N − 3.

Proof. The proof uses the fact that for all t ∈ {2, . . . , N }, we have

which follows from the deﬁnition of the number of while loop iterations wt (on line 4 of Algorithm 2, every
iteration decrements i, and therefore Mt). The total number of while loop iterations is thus

Mt = 2 + Mt−1 − wt,

(19)

WN =

N
(cid:88)

t=2

wt =

N
(cid:88)

t=2

2 + Mt−1 − Mt

= 2(N − 1) +

N
(cid:88)

Mt−1 −

= 2(N − 1) +

t=2

N −1
(cid:88)

t=1

Mt −

N
(cid:88)

t=2

Mt

N
(cid:88)

t=2

Mt

= 2N − 2 + M1 − MN
= 2N − 1 − MN .

(20)

(21)

(22)

(23)

(24)

The ﬁrst two equalities (20) follow from the deﬁnitions of the number of while loop iterations (18–19). The
next equalities come from distributing the sum (21), then re-writing the second term as a sum from t = 1 to
N − 1 (22). The last equalities come from subtracting the terms in the two sums (23), then using the fact
that M1 = 1 (24). The result is obtained using the fact that the number of selectable models is bounded,
2 ≤ MN ≤ N .

The best case of Algorithm 1, WN = N − 1 iterations, happens when the number of selected models is large,
MN = N ; the worst case WN = 2N − 3 iterations occurs when MN = 2. Because the total number of
iterations is always O(N ), the Solve sub-routine (Algorithm 2) is amortized constant O(1) time on average,
even though it is linear in the number of models O(M ) in the worst case.

4 Empirical complexity analysis

In this section we empirically examine the number of iterations of our algorithm, and show that it is overall
orders of magnitude faster than previous baselines.

8

s
n
o
i
t
a
r
e
t
i

f
o

r
e
b
m
u
n
=

N
W

)
e
l
a
c
s

r
a
e
n
i
l

,
y
t
i
x
e
l
p
m
o
c

e
m

i
t
(

1126

1000

750

500

250

1

Theoretical upper bound:

WN ≤ 2N − 3 iterations

1000 neuroblastoma

data sequences

Lt = N − t,

synthetic data achieving

upper bound

√

Lt = N −

Theoretical lower bound:

WN ≥ N − 1 iterations

t, synthetic data achieving lower bound

2

250

500

750

869

N = number of models (linear scale)

Figure 2: Empirical number of iterations WN of while loop in Algorithm 2 in synthetic loss values (violet
points) and loss values from optimal changepoint models of real neuroblastoma data (black points) are
linear O(N ) in the number of input models N , consistent with theoretical upper/lower bounds obtained in
Theorem 2 (grey lines).

Figure 3: The proposed exact linear time algorithm is orders of magnitude faster than the previous exact
quadratic time algorithm and naïve approximate grid search. Left: the two exact algorithms compute
the same result, but the proposed linear time algorithm is orders of magnitude faster, even when time to
compute loss values via binary segmentation (binseg) is included in the timing (lines/bands for mean/SD
over 5 timings). Right: when used on loss values from N = 287443 optimal changepoint models for one
genomic data sequence, the proposed exact linear time algorithm is always faster than approximate grid
search with at least 10 points (points/segments for mean/SD over 5 timings).

9

4.1 Empirical iteration counts are consistent with theoretical bounds

As discussed in Section 3.3, the time complexity of Algorithm 1 is linear WN , the total number of iterations of
the while loop in the Solve sub-routine. Here we demonstrate that the theoretical bounds on WN obtained
in Theorem 2 are consistent with the number of iterations obtained empirically in real and synthetic data.
First, we considered 1000 real cancer DNA copy number data sets of diﬀerent sizes p ∈ {2, . . . , 869} from
R package neuroblastoma. For each sequence data set z ∈ Rp we used the Pruned Dynamic Programming
Algorithm (PDPA) of Rigaill [2015] to compute a sequence of optimal changepoint models. For each number
of segments k ∈ {1, . . . , p} the optimal loss is

Lk = min
θ∈Rp

p
(cid:88)

j=1

(θj − zj)2

subject to ||Dθ||0 =

p−1
(cid:88)

j=1

I[θj (cid:54)= θj+1] = k − 1.

(25)

The PDPA returns a regularization path of N = p models, from k = 1 segment (no changepoints, θj = θj+1
for all j) to k = N = p segments (change after every data point, θj (cid:54)= θj+1 for all j). We used the resulting
loss values L1 > · · · > LN as input to Algorithm 1. We plotted the number of iterations WN as a function
of data set size N (black points in Figure 2), and observed that they always fall between the upper/lower
bounds from Theorem 2 (grey lines). These results provide empirical evidence that the time complexity of
our algorithm is linear O(N ) in real data.

Second, we considered two synthetic sequences of loss values, Lt = N − t for all t ∈ {1, . . . , N } (e.g.
t (e.g. L1 = 4 > 3.59 > 3.27 > 3 > 2.76 = L5 for
L1 = 4 > 3 > 2 > 1 > 0 = L5 for N = 5) and Lt = N −
N = 5). For these loss values we observed a number of iterations (violet points in Figure 2) that always falls
on the upper/lower bounds (grey lines), which indicates that these synthetic data achieve the worst/best case.
Overall these results provide a convincing empirical validation of our theoretical bounds from Theorem 2.

√

4.2 Empirical timings suggest orders of magnitude speedups

Our proposed algorithm takes as input a sequence of N loss values, which must be computed by some other
machine learning algorithm. In this section we therefore analyzed our algorithm in the context of a two-step
pipeline: (i) compute the N loss values, (ii) compute an exact representation of the model selection function.
The overall time complexity of the two-step pipeline is determined by the slower of the two steps. If the ﬁrst
step is at least quadratic, then the pipeline is as well (using either linear or quadratic time model selection
in the second step). However if the ﬁrst step is sub-quadratic, then we expect that our linear time algorithm
in the second step should result in speedups.

Simulated data for which proposed linear algorithm results in speedups over previous quadratic
algorithm. For the ﬁrst step we therefore use the log-linear binary segmentation algorithm, which inputs
a data sequence z ∈ Rp, and computes an approximate solution to (25). The binary segmentation algorithm
computes the full path of N = p models with corresponding loss values L1, . . . , LN in O(N log N ) time
on average [Scott and Knott, 1974, Truong et al., 2018]. For each data set size N ∈ {102, . . . , 105} we
generate synthetic data sequences zj = sin(j) + j/N , for all j ∈ {1, . . . , N }. Figure 3 (left) shows timings
of binary segmentation alone (binseg), exact model selection algorithms alone (linear, quadratic), and two-
step pipelines (binseg.linear, binseg.quadratic), on an Intel T2390 1.86GHz CPU. As expected, our proposed
linear time algorithm is orders of magnitude faster than the previous quadratic time algorithm (when run
alone, and also in the two-step pipeline). For example, for N = 105 data, the binseg.linear pipeline takes
about 3 seconds, whereas binseg.quadratic takes about 2 minutes. More generally, such timings are typical

10

for any data for which binary segmentation runs in log-linear time, and selected models MN increases with
the data set size N (second column of Figure 4, same as Figure 3 left). However, there are other kinds
of data for which our approach is no faster than the quadratic baseline (other columns of Figure 4). For
example, when binary segmentation runs in quadratic time, then our linear time model selection algorithm
oﬀers no speedups to the overall pipeline (third and fourth columns of Figure 4). Also, since the previous
(worst case quadratic) algorithm achieves its best case linear time complexity when the number of selected
models MN is small/constant, then our proposed algorithm oﬀers no speedups in this case (ﬁrst columns of
Figure 4). Overall, we have shown that for some data sets, our linear time algorithm provides substantial
speedups relative to the previous quadratic time algorithm.

Real data for which proposed linear algorithm is faster than grid search. Another baseline
algorithm for computing a representation of the model selection function is a naïve approximate grid search
over G penalties λ, which takes O(N G) time. We expected this baseline to perform poorly in the context
of large N and large G, so we performed timings on a large chipseq data set from the UCI repository
[Newman and Merz, 1998]. We ﬁrst computed a regularization path of N = 287, 443 optimal changepoint
models for a sequence of p = 1, 656, 457 data, and then performed timings of the model selection algorithms
on the resulting N loss values. We observed that our proposed linear time algorithm is always faster than
approximate grid search with at least 10 grid points (Figure 3, right). For example the approximate grid
search takes almost 2 minutes for N = 10, 000 grid points, whereas the proposed exact linear time algorithm
takes only 27 milliseconds. Overall these data indicate that the proposed linear time algorithm is indeed
faster than the two baselines in large data.

BinSeg: log-linear

BinSeg: log-linear

BinSeg: quadratic

BinSeg: quadratic

n.selected: few

n.selected: many

n.selected: few

n.selected: many

)
e
l
a
c
s

g
o
l

,
s
d
n
o
c
e
s
(

e
m

i
t

n
o
i
t
a
t
u
p
m
o
C

1e+00

1e-02

1e-04

1e+00

1e-02

1e-04

zj = j

binseg

zj = j + sin(j)

binseg

zj = 0

quadratic

quadratic

binseg

linear

binseg

binseg

linear

binseg

binseg

linear

binseg

quadratic

binseg

zj = sin(j)

binseg

zj = j

zj = j + sin(j)

zj = 0

zj = sin(j)

quadratic
linear

100

1000

10000

quadratic

linear

linear
quadratic

100

1000

100
N = number of simulated data (log scale)

10000

10000

1000

100

1000

10000

quadratic

binseg

linear

binseg

quadratic

linear

s
t
e
p
s
:

1
-
2

s
t
e
p
s
:

2

Figure 4: Binary segmentation (step 1) followed by exact model selection (step 2) was run on four synthetic
data sequences xi (panels from left to right). Both model selection algorithms (worst case quadratic, and
proposed worst case linear time) output the exact path of selected models. Bottom: when there are
few selected models (ﬁrst and third columns) the quadratic algorithm achieves its best case linear time
complexity; when there are many selected models (second and fourth columns) it achieves the worst case
quadratic time complexity. Top: total timings over both steps show that the linear time algorithm oﬀers
substantial speedups when binary segementation achieves its best-case log-linear time complexity, and there
are many selected models (second column).

11

5 Prediction accuracy in supervised changepoint problems

In this section we aim to demonstrate that the proposed exact algorithm results in more accurate predictions
than a naïve approximate grid search. To examine the accuracy of our algorithm, we consider several
supervised changepoint detection problems from the UCI chipseq data, which contain labels that indicate
presence/absence of changepoints in particular data subsets. Accurate changepoint detection in these data
is important in order to characterize active/inactive regions in the human epigenome.

Here we give a brief summary of the supervised learning framework for changepoint detection; for details see
[Hocking et al., 2013]. Each observation i is represented by a numeric data vector/sequence zi along with
a corresponding label set (cid:96)i. We compute a feature vector xi then learn a penalty function f (xi) = log λi
which results in a model ˆθ(λi). The goal is to learn a function f that results in minimal errors with respect to
the labels (cid:96)i in test data sequences. In this context there is a model selection function k∗
i which is speciﬁc to
each data sequence i, and is used in two places during the learning and prediction (bold arrows in Figure 5).
, yi) of optimal penalty values for each training data
First, it is used to compute the interval/output yi = (y
, yi) results in minimal label errors. Second, it is used
sequence i, such that predicting f (xi) = log λi ∈ (y
i
to compute the predicted model ˆθ(λi) given a predicted penalty λi. We learn a linear f by minimizing an
L1-regularized cost function [Hocking et al., 2013], using outputs yi computed by either our exact algorithm
or a naïve approximate grid search with a variable number of penalties λ.

i

We performed 4-fold cross-validation in ﬁve diﬀerent labeled data sets (panels in Figure 6). We observed
in each data set that it takes 10–100 penalties λ in the grid search to achieve the maximum number of
correctly predicted labels, which was also achieved by the proposed exact algorithm. Overall these data
provide empirical evidence that, in the context of supervised changepoint detection problems, using an exact
representation of the model selection function results in more accurate predictions than using an approximate
representation obtained via grid search.

6 Discussion and conclusions

For learning problems with (cid:96)0 regularization, we proposed a new dynamic programming algorithm for com-
puting an exact representation of the model selection function (4). By bounding the number of iterations,
we proved theoretically that the algorithm is linear time in the worst case. In real and synthetic data we
empirically validated these bounds, and showed that the proposed linear time algorithm is orders of mag-
nitude faster than two baselines. We used cross-validation in supervised changepoint detection problems to

Label set (cid:96)

Sequence z

k∗

Model ˆθ

k∗

Interval y

Features x

Penalty λ

i

Function f

Figure 5: Computation graph for supervised changepoint detection in labeled sequences i. Directed edges
start from inputs and end at outputs, e.g. interval y can be computed using labels (cid:96) and sequence z. Bold
edges indicate computations which use the model selection function k∗.

12

Figure 6: Prediction accuracy on held-out test data increases as a function of number of points used in
approximate grid search algorithm (red line/band); it takes 10–100 grid points to achieve the maximum
accuracy in each data set (panels), which is also achieved by the proposed linear time exact algorithm (black
point/error bar on right).

show that the exact representation provides more accurate predictions than the grid search approximation
baseline.

Our algorithm requires no special data structures and can be eﬃciently implemented using arrays in standard
C; our free software implementation is available at https://github.com/tdhock/penaltyLearning/. For
reproducibility we also provide the source code that we used to make the ﬁgures at https://github.com/
tdhock/changepoint-data-structure. For future work we would like to consider selecting models from a
partial set S ⊂ {1, . . . , N }, and develop an eﬃcient algorithm for updating an exact representation of the
corresponding model selection function.

References

S. Arlot and P. Massart. Data-driven Calibration of Penalties for Least-Squares Regression. Journal of

Machine Learning Research, 10:245–279, 2009.

I. Auger and C. Lawrence. Algorithms for the optimal identiﬁcation of segment neighborhoods. Bull Math

Biol, 51:39–54, 1989.

R. Bellman. On the approximation of curves by line segments using dynamic programming. Commun. ACM,

4(6):284–, June 1961.

D. Bertsimas, A. King, and R. Mazumder. Best subset selection via a modern optimization lens. The Annals

of Statistics, 44(2):813–852, 2016.

S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientitic

Computing, 20(1):33–61, 1998.

G. Davis, S. Mallat, and Z. Zhang. Adaptive time-frequency decompositions with matching pursuit. Wavelet

Applications, 402:402–413, 1994.

J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal

of the American Statistical Association, 96(456):1348–1360, 2001.

K. Haynes, I. A. Eckley, and P. Fearnhead. Computationally eﬃcient changepoint detection for a range of
penalties. Journal of Computational and Graphical Statistics, 26(1):134–143, 2 2017. ISSN 1061-8600.

13

S. Huang and H. Wolkowicz. Low-rank matrix completion using nuclear norm minimization and facial

reduction. Journal of Global Optimization, 72(1):5–26, Sep 2018.

B. Jackson, J. Scargle, D. Barnes, S. Arabhi, A. Alt, P. Gioumousis, E. Gwin, P. Sangtrakulcharoen, L. Tan,
and T. Tsai. An algorithm for optimal partitioning of data on an interval. IEEE Signal Process Lett, 12:
105–108, 2005.

R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear computational

cost. Journal of the American Statistical Association, 107(500):1590–1598, 2012.

M. Lavielle. Using penalized contrasts for the change-point problem. Signal Processing, 85(8):1501–1510,

2005.

J. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In Proc. of the

Fifth Berkeley Symp. on Math. Stat. and Prob., pages 281–297, 1967.

R. Maidstone, T. Hocking, G. Rigaill, and P. Fearnhead. On optimal multiple changepoint algorithms for

large data. Statistics and Computing, 2016.

S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal

Processing, 41(12):3397–3415, 1993.

R. Mazumder, J. Friedman, and T. Hastie. Sparsenet: Coordinate descent with nonconvex penalties. Journal

of the American Statistical Association, 106(495):1125–1138, 2011.

A. Miller. Subset selection in regression. Chapman and Hall, second edition, 2002.

C. B. D. Newman and C. Merz. UCI repository of machine learning databases, 1998.

PEGWiki. Convex hull trick. https://wcipeg.com/wiki/Convex_hull_trick, 2018.

T. Hocking, G. Rigaill, J.-P. Vert, and F. Bach. Learning sparse penalties for change-point detection using

max margin interval regression. In Proc. 30th ICML, pages 172–180, 2013.

G. Rigaill. A pruned dynamic programming algorithm to recover the best segmentations with 1 to kmax

change-points. Journal de la Société Française de la Statistique, 156(4), 2015.

P. Schniter, L. C. Potter, and J. Ziniel. Fast bayesian matching pursuit: Model uncertainty and parameter

estimation for sparse linear models, 2009.

A. Scott and M. Knott. A cluster analysis method for grouping means in the analysis of variance. Biometrics,

30:507–512, 1974.

C. Soussen, J. Idier, D. Brie, and J. Duan. From Bernoulli-Gaussian deconvolution to sparse signal restora-
tion. technical report, 34 pages, Jan. 2010. URL https://hal.archives-ouvertes.fr/hal-00443842.

R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58(1):267–288,

1996.

C. Truong, L. Oudre, and N. Vayatis. A review of change point detection methods. preprint arXiv:1801.00718,

2018.

G. J. J. van den Burg, P. J. F. Groenen, and A. Alfons. SparseStep: Approximating the Counting Norm for

Sparse Regularization. preprint arXiv:1701.06967, 2017.

C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38:

894–942, 2010.

14

