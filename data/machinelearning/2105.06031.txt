1
2
0
2

y
a
M
3
1

]
L
M

.
t
a
t
s
[

1
v
1
3
0
6
0
.
5
0
1
2
:
v
i
X
r
a

Joint Community Detection and Rotational
Synchronization via Semideï¬nite Programming

Yifeng Fanâˆ—

Yuehaw Khooâ€ 

Zhizhen Zhaoâˆ—

May 14, 2021

Abstract

In the presence of heterogeneous data, where randomly rotated objects fall into multiple
underlying categories, it is challenging to simultaneously classify them into clusters and syn-
chronize them based on pairwise relations. This gives rise to the joint problem of community
detection and synchronization. We propose a series of semideï¬nite relaxations, and prove their
exact recovery when extending the celebrated stochastic block model to this new setting where
both rotations and cluster identities are to be determined. Numerical experiments demonstrate
the eï¬ƒcacy of our proposed algorithms and conï¬rm our theoretical result which indicates a sharp
phase transition for exact recovery.

Keywords: Community detection, synchronization, semideï¬nite programming, rotation group,

stochastic block model

1

Introduction

In the presence of heterogeneous data, where randomly rotated objects fall into multiple underly-
ing categories, it is challenging to simultaneously synchronize the objects and classify them into
clusters. A motivating example is the 2D class averaging in cryo-electron microscopy single particle
reconstruction [17, 41, 49]. It aims to cluster images taken from similar viewing directions, rota-
tionally align and average those images in order to denoise the experimental data. Joint clustering
and synchronization is an emerging area that connects community detection [1, 16, 32, 2, 19, 20]
and synchronization [39, 7, 18]. Recently, several works discussed simultaneous classiï¬cation and
mapping (alignment) [4, 24] and proposed diï¬€erent models and algorithms.
In [4], the authors
addressed simultaneous permutation group synchronization and clustering via a spectral method
with rounding and used the consistency of the mapping for clustering. In [24], the authors noticed
that both rotational alignment and classiï¬cation are problems over compact groups and proposed
a harmonic analysis and semideï¬nite programming based approach for solving alignment and clas-
siï¬cation simultaneously.

In this paper, we consider joint community detection and rotational synchronization under a
speciï¬c probabilistic model, which extends the celebrated stochastic block model (SBM) [9, 10, 11,
15, 21, 22, 28, 29, 30, 31] to incorporate both the community structure and pairwise rotations.
In particular, we inherit the G(n, p, q)-SBM setting [26, 19, 2, 20] for the graph connection as
shown in Figure 1. Given a network of size n with K underlying disjoint communities, a random

âˆ—Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign,

IL.

â€ Department of Statistics, University of Chicago, Chicago, IL.

1

 
 
 
 
 
 
Figure 1: An illustration of our probabilistic model. We show a network with two underlying
communities in blue and orange respectively. Each vertex i is associated with a rotation Ri âˆˆ SO(d).
Edges between vertices within the same cluster (resp. across clusters) independently connected with
probability p (resp. q) are shown in solid lines (resp. dash lines). The pairwise alignment Rij between
vertex i and vertex j is observed on each edge (i, j).

graph is generated such that each pair of vertices are independently connected with probability p
(resp. q) if they all belong to the same cluster (resp. diï¬€erent clusters). In addition, each node i is
associated with a rotation Ri âˆˆ SO(d) which is assumed to be unknown, and a pairwise alignment
Rij is observed on each connected edge. The noisy observation Rij is generated according to the
probabilistic model considered in [41, 13, 14]. When i and j belong to the same cluster we observe
the clean measurement Rij = RiR(cid:62)
j . When they are in diï¬€erent clusters, Rij is assumed to be
uniformly drawn from SO(d). The model considered here is diï¬€erent from the probabilistic model
in [4].

For such probabilistic model, a naive two-stage approach for recovery is to (1) directly apply
existing methods for community detection under SBM, and (2) perform rotational synchronization
for each identiï¬ed community separately. However, such approach does not take advantage of
the cycle consistency of the alignments within each cluster and inconsistency of the alignments
across clusters, and thus the clustering result might be sub-optimal. Instead, we can exploit such
consistency of rotational alignments to further improve the identiï¬cation of communities. For
example, for three nodes i, j and k, their pairwise alignments satisfy RijRjkRki = Id if they belong
to the same cluster. In fact, the notion of cycle consistency has already been used in synchronization
problems [39, 33, 40, 14, 12, 38].

1.1 Our contributions

To incorporate the consistency of alignments in clustering, we formulate a series of optimization
problems in diï¬€erent settings which aim to simultaneously recover the community structure and
individual rotations. To solve these non-convex problems, we apply semideï¬nite relaxation to
obtain a polynomial time solutions. Each resulting semideï¬nite programming (SDP) is tailored
to accommodate diï¬€erent cluster structures with known or unknown cluster sizes. For the case of
two clusters, we provide dual certiï¬cates of the corresponding SDPs, and derive sharp exact recovery
conditions of the SDP. In particular, our analysis relies on two novel concentration inequalities for
random matrices: (1) Given a series of independent random matrices {Ri}n
i=1 uniformly drawn from
SO(d), we derive a sharp tail bound for the Frobenius norm of (cid:80)n
i=1 Ri at d = 2. (2) Given an
m Ã— n block matrix S âˆˆ RmdÃ—nd with independent blocks uniformly drawn from SO(d), we show
that the operator norm (cid:107)S(cid:107) âˆ¼
m, which can be regarded as an extension of the norm bound
of random matrices with independent entries [5, 47, 43].

n +

âˆš

âˆš

2

ğ‘…ğ‘–ğ‘…ğ‘—ğ‘…ğ‘™ğ‘–ğ‘—ğ‘™ğ‘…ğ‘–ğ‘—ğ‘…ğ‘—ğ‘™w.p.ğ‘w.p.ğ‘1.2 Organization

The rest of this paper is organized as follows. In Section 2 we formally deï¬ne our probabilistic model
and formulate non-convex optimization problems to solve for the cluster identities and rotations.
In Section 3 we propose SDP relaxations, and prove conditions for exact recovery for the case
when there are two underlying communities. Numerical results are given in Section 4 to evaluate
the proposed algorithm and demonstrate the predicted phase transition for exact recovery. We
conclude with some future directions in Section 5. For the clarity of the presentation, most the
proofs are provided in Appendix.

1.3 Notations

Throughout this paper we use the following notations: given a matrix X, its transpose is denoted
by X (cid:62). An m Ã— n matrix of all zeros is denoted by 0mÃ—n (or 0 for brevity). An n Ã— n identity
matrix is denoted by In. 1n âˆˆ Rn represents a vector of length n with all ones. The maximum and
minimum eigenvalues of X are denoted by Î»max(X) and Î»min(X) respectively. (cid:107)X(cid:107) and (cid:107)X(cid:107)F
denote the operator norm and the Frobenius norm of X respectively. For two matrices X and Y ,
their inner product is written as (cid:104)X, Y (cid:105) = Tr(X (cid:62)Y ). For two non-negative functions f (n) and
g(n), f (n) = O(g(n)) (resp. f (n) = â„¦(g(n))) denotes if there exists a constant C > 0 such that
f (n) â‰¤ Cg(n) (resp. f (n) â‰¥ Cg(n)) for all suï¬ƒciently large n, f (n) = o(g(n)) denotes for every
constant C > 0 it holds f (n) â‰¤ Cg(n) for all suï¬ƒciently large n. Speciï¬cally, given a matrix X of
size md Ã— nd, we usually treat it as an m Ã— n block matrix such that Xij âˆˆ RdÃ—d denotes its (i, j)-th
block, and the i-th block row (resp. j-th block column) of X is referred to as the sub-matrix that
contains Xij for all j = 1, . . . , n (resp. i = 1, . . . , m).

2 Our probabilistic model and problem formulation

We state our probabilistic model for joint community detection and rotation synchronization as
following: We assume that n agents in a network fall in K underlying communities and each agent i
is associated with a rotation matrix Ri âˆˆ SO(d). The size of the k-th cluster is denoted by mk, and
Ck denotes the set of nodes belonging to the k-th cluster. Each agent i has an underlying cluster
label C(i) âˆˆ {1, . . . , K}. A random graph G = (V, E) with node set V and edge set E is generated
from the underlying community structure and rotations of the agents such that each pair of nodes
(i, j) in the graph is connected with probability p if agents i and j are in the same cluster, i.e.,
C(i) = C(j), and with probability q if the agents are not in the same cluster, i.e., C(i) (cid:54)= C(j). In
addition, a relative rotational transformation Rij is observed on each edge (i, j) âˆˆ E. If C(i) = C(j),
we obtain Rij = RiR(cid:62)
j which equals to the ground truth pairwise alignment. while if C(i) (cid:54)= C(j),
we assume Rij is a random rotation uniformly drawn from SO(d). Given the above, we construct
an observation matrix A âˆˆ RndÃ—nd whose (i, j)-th block Aij âˆˆ RdÃ—d satisï¬es

Aij =

ï£±
ï£´ï£²

ï£´ï£³

RiR(cid:62)
with probability p and when C(i) = C(j),
j ,
Rij âˆ¼ Unif(SO(d)), with probability q and when C(i) (cid:54)= C(j),
0,

otherwise.

(1)

A can be viewed as an extension of the adjacency matrix such that each Aij is not a {0, 1}-valued
entry, but rather a matrix that indicates the connectivity as well as the observed rotation from j to

3

i. In addition, let us denote M âˆ— âˆˆ RndÃ—nd as the clean observation matrix where

M âˆ—

ij =

(cid:40)

RiR(cid:62)
0,

j , C(i) = C(j),

otherwise.

(2)

When p = 1 and q = 0, M âˆ— is identical with A. Furthermore, we deï¬ne V (k) âˆˆ RndÃ—d that indicates
the cluster membership and individual rotations for each cluster k, whose i-th block V (k)
âˆˆ RdÃ—d
satisï¬es

i

V (k)

i =

(cid:40)

Ri,
0,

i âˆˆ Ck,
otherwise.

(3)

Then it is clear to see that M âˆ— = (cid:80)K
k=1 V (k)(V (k))(cid:62). Speciï¬cally for the case of two clusters,
without loss of generality we can assume that the ï¬rst m1 nodes belong to C1 and the remaining m2
nodes belong to the other cluster C2. Then the ground truth M âˆ— deï¬ned in (2) has the following
form:

M âˆ— = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) =

.

(4)

(cid:33)

(cid:32)

(cid:102)M âˆ—
0
1
0 (cid:102)M âˆ—
2

where (cid:102)M âˆ—

1 âˆˆ Rm1dÃ—m1d and (cid:102)M âˆ—

2 âˆˆ Rm2dÃ—m2d.

Based on the model, we state our problem of joint community detection and synchronization as

the following optimization program:

max
Ë†R1,..., Ë†RnâˆˆSO(d), Ë†C1,..., Ë†CK

(cid:88)

(cid:68)

Aij, Ë†Ri Ë†R(cid:62)
j

(cid:69)

,

i,jâˆˆ Ë†Ck,âˆ€k

(5)

where Ë†Ck denotes the set of nodes identiï¬ed for the k-th cluster and Ë†Ri âˆˆ SO(d) denotes the rotation
identiï¬ed for node i. Diï¬€erent from community detection under SBM, (5) aims to determine { Ë†Ci}K
i=1
and { Ë†Ri}n
i=1 simultaneously such that the cluster membership relies on both the edge connections
and the consistency of alignments. By introducing M âˆˆ RndÃ—nd with its (i, j)-th block
(cid:40) Ë†Ri Ë†R(cid:62)
j ,
0,

Ë†C(i) = Ë†C(j),
otherwise,

Mij =

(6)

we can recast (5) as the following program:

max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M satisï¬es the form in (6).

Notably, if the size of each cluster is known, an additional constraint should be added as

max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M has the form in (6), and | Ë†Ck| = mk, k = 1, . . . , K.

(7)

(8)

Since directly solving (7) or (8) is clearly non-convex and computationally intractable, in the fol-
lowing section we propose semideï¬nite relaxation to eï¬ƒciently solve them.

4

3 Semideï¬nite relaxation

For simplicity, in Sections 3.1â€“3.3, we focus on the case of two clusters (i.e., K = 2). In particular,
we consider the following three scenarios: (1) two cluster sizes m1 = m2 = n/2 are equal and
known; (2) two cluster sizes m1, m2 are unequal and known; (3) two cluster sizes are unknown.
For each scenario, we develop its tailored SDP and provide corresponding performance guarantee.
In Section 3.4, we extend our approach to general cases with an arbitrary number of underlying
clusters.

3.1 Two equal-sized clusters with known cluster sizes

In this section, we study the case of two clusters where the cluster sizes m1 = m2 = m are equal and
known to us. In this case, the ground truth M âˆ— in (4) satisï¬es the following convex constraints:

1. M âˆ— is positive semi-deï¬nite, i.e. M âˆ— (cid:23) 0.

2. Each diagonal block of M âˆ— is an identity matrix, i.e. M âˆ—

ii = Id, i = 1, . . . , n.

3. The i-th block row of M âˆ— satisï¬es (cid:80)n

j=1 (cid:107)M âˆ—

ij(cid:107)F â‰¤ m

d, i = 1, . . . , n.

âˆš

Notably, instead of the last constraint, we want the equality (cid:80)n
d. However, the
norm equality constraint is non-convex and therefore a convex relaxation by replacing â€œ=â€ with
â€œâ‰¤â€ is needed. Based on these properties of M âˆ—, a semideï¬nite relaxation of (8) can be stated as

j=1 (cid:107)M âˆ—

ij(cid:107)F = m

âˆš

MSDP = arg max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

n
(cid:88)

j=1

âˆš

(cid:107)Mij(cid:107)F â‰¤ m

d,

i = 1, . . . , n.

(9)

As a result, (9) is a semideï¬nite program (SDP) which can be eï¬ƒciently solved by interior-point
methods with polynomial complexity [46]. We remark that in the context of community detection,
SDPs similar to (9) have been studied in [2, 19] on SBM with two equal-sized clusters. In their
problem, each Mij is a scalar and a convex equality constraint (cid:80)n
i=1 Mij = m can be imposed. In
contrast, we use a convex inequality constraint on (cid:80)n

j=1 (cid:107)Mij(cid:107)F.

To characterize the performance of (9), we identify the conditions on the model parameters

(p, q, n) such that (9) achieves exact recovery, i.e., MSDP =M âˆ— as follows.

Theorem 3.1. Let p = Î± log n/n and q = Î² log n/n. For suï¬ƒciently large n, (9) achieves exact
recovery with probability 1 âˆ’ nâˆ’â„¦(1) if

Î± âˆ’ (cid:112)2(cid:96)Î² log

(cid:19)

(cid:18) eÎ±
âˆš
2(cid:96)Î²

> 2, where

(cid:96) =

(cid:40)

2,
1,

when d > 2,
when d = 2.

(10)

Notice that the function f (Ï„ ) := Î±âˆ’Ï„ log(eÎ±/Ï„ ) monotonically decreases for Ï„ âˆˆ (0, Î±], therefore
Theorem 3.1 implies the SDP (9) achieves exact recovery if Î² is smaller than certain threshold
determined by Î±. Also, we remark that in (10), the diï¬€erence between the case for d = 2 and d > 2
lies in the diï¬€erent concentration inequalities of random rotation matrix used in the proof (see
Section 3.1.1 for detail). However, we conjecture the condition for d > 2 can be further improved
such that the constant factor (cid:96) can be smaller than 2. Although the condition in (10) is suï¬ƒcient

5

but may not be necessary, in experiments we show the threshold of (10) sharply characterizes the
empirical phase transition boundary for exact recovery when d = 2 (see Figure 3).

In the following section, we outline the key steps in the proof of Theorem 3.1 based on showing
the dual certiï¬cate of the ground truth MSDP = M âˆ— for (9). The complete proofs of the lemmas
in Section 3.1 are deferred to Appendix B.1.

3.1.1 The sketch of proof

Our proof consists of the following three steps:

Step 1: Derive KKT conditions, uniqueness and optimality of M âˆ—. The Lagrangian of
the optimization problem (9) is,

L(M , Î›, Z, Âµ) = âˆ’ (cid:104)A, M (cid:105) âˆ’ (cid:104)Î›, M (cid:105) âˆ’

n
(cid:88)

i=1

(cid:104)Zi, Mii âˆ’ Id(cid:105) âˆ’

n
(cid:88)

(cid:28)

âˆš

Âµi, m

d âˆ’

i=1

(cid:29)

,

(cid:107)Mij(cid:107)F

n
(cid:88)

j=1

where Î›, Z = diag ({Zi}n
M (cid:23) 0, Mii = Id and (cid:80)n
M = M âˆ— are listed as,

i=1) and Âµ = {Âµi}n

j=1 (cid:107)Mij(cid:107)F â‰¤ m

âˆš

i=1 are the dual variables associated with the constraints
d for i = 1, . . . , n. Then the KKT conditions at

â€¢ Stationarity:

âˆ’ A âˆ’ Î› âˆ’ diag({Zi}n

Î˜ = [Î˜ij]n

i,j=1 , Î˜ij =

i=1) + Î˜ + Î˜(cid:62) = 0,
ij/

âˆš

(cid:40)

d,

ÂµiM âˆ—
M âˆ—
ÂµiÎ±ij, (cid:107)Î±ij(cid:107)F â‰¤ 1, M âˆ—

ij (cid:54)= 0,
ij = 0.

(11)

(12)

â€¢ Comp. slackness:

(cid:104)Î›, M âˆ—(cid:105) = 0,

(cid:28)

âˆš

Âµi, m

d âˆ’

(cid:29)

(cid:13)
(cid:13)M âˆ—
ij

(cid:13)
(cid:13)F

n
(cid:88)

j=1

= 0, i = 1, . . . , n.

(13)

â€¢ Dual feasibility:

Î› (cid:23) 0, Âµi â‰¥ 0,

i = 1, . . . , n.

(14)

Notably, Î±ij in (12) can be chosen arbitrarily as long as (cid:107)Î±ij(cid:107)F â‰¤ 1, because the derivative of
(cid:107)Mij(cid:107)F is undeï¬ned at Mij = 0. Based on the KKT conditions, the following result establishes
the uniqueness and optimality of M âˆ—:

Lemma 3.2. Given M âˆ— deï¬ned in (2), suppose there exists dual variables Î›, Z, {Âµi}n
{Î±ij}(i,j):M âˆ—

ij =0 that satisfy the KKT conditions (11) - (14) as well as

i=1 and

N (Î›) = R(M âˆ—),

(15)

where N (Â·) and R(Â·) denote the null space and column space respectively. Then M âˆ— is the optimal
and unique solution to (9).

Step 2: Construct dual variables. Without loss of generality, we assume the rotation matrix
Ri = Id for each i. Then for any pair of (i, j) that C(i) = C(j), we have M âˆ—
ij = Id and Aij = rijId,
where rij âˆˆ {0, 1} is a Bernoulli random variable such that P{rij = 1} = p. Our next step is to ï¬nd
dual variables that satisfy the conditions in Lemma 3.2. To this end, inspired by [26] we make the
following guess for the dual variables.

Lemma 3.3. The following dual variables satisfy the KKT conditions (11) - (13) and Âµi â‰¥ 0, i =

6

1, . . . , n in (14):

Î˜ij =

âˆš

(cid:40)

ÂµiId/
(cid:101)Î±i,

d, C(i) = C(j),
C(i) (cid:54)= C(j),

Âµi = (cid:107) (cid:101)Î±i(cid:107)F,

(cid:101)Î±i =

Zi =

Î› = âˆ’A âˆ’ diag ({Zi}n

i=1) + Î˜ + Î˜(cid:62).

1
m
(cid:18) mÂµiâˆš
d

(cid:88)

Aij âˆ’

j:C(i)(cid:54)=C(j)

(cid:88)

+

s:C(s)=C(i)

1
2m2
(cid:18) Âµsâˆš
d

(cid:88)

(cid:88)

Asj,

j:C(i)(cid:54)=C(j)
(cid:19)(cid:19)

s:C(s)=C(i)

âˆ’ ris

Id,

When constructing the dual certiï¬cate, for ï¬xed i, j where C(i) (cid:54)= C(j), Î˜ij can have many
choices. Here we assume Î˜ij is identical for all j when i and j are not in the same cluster. This
choice allows us to determine all dual variables via the KKT conditions and (15). Now it remains
to check under what condition on (Î±, Î²) does Î› satisfy both Î› (cid:23) 0 and (15). To this end, we use
the following property for Î›.

Lemma 3.4. Given the dual variables in Lemma 3.3, Î› can be expressed as

Î› = (Ind âˆ’ Î ) (E[A] âˆ’ A âˆ’ Z + pInd)
(Ind âˆ’ Î ), where Î  =
(cid:125)

(cid:124)

(cid:123)(cid:122)
=: (cid:101)Î›

(cid:19)

(cid:18) 1
m

M âˆ—.

(16)

Then, Î› (cid:23) 0 and N (Î›) = R(M âˆ—) are satisï¬ed if (cid:101)Î› (cid:31) 0.

Step 3: Find the condition for (cid:101)Î› (cid:31) 0. Applying Weylâ€™s inequality [42] yields Î»min( (cid:101)Î›) â‰¥
Î»min(pInd âˆ’ Z) âˆ’ (cid:107)E[A] âˆ’ A(cid:107). Then for (cid:101)Î› (cid:31) 0, it suï¬ƒces to show

Î»min(pInd âˆ’ Z) > (cid:107)E[A] âˆ’ A(cid:107).

(17)

To this end, we have the following bound for Î»min(pInd âˆ’ Z).

Lemma 3.5. Let p = Î± log n/n, q = Î² log n/n. For n that is suï¬ƒciently large,

Î»min(pInd âˆ’ Z) â‰¥ min

i

xi âˆ’ max

i

yi âˆ’ max{(cid:15)1, (cid:15)2} + p

where

1. xi := (cid:80)

s:C(s)=C(i) ris, which satisï¬es

min
i

xi â‰¥

Ï„
2

log n with probability

1 âˆ’ n1âˆ’ 1

2 (Î±âˆ’Ï„ log( eÎ±

Ï„ )+o(1)),

for Ï„ âˆˆ [0, Î±) such that 1 âˆ’ 1
2

(cid:0)Î± âˆ’ Ï„ log (cid:0) eÎ±
Ï„

(cid:1) + o(1)(cid:1) < 0;

2. yi := mÂµiâˆš
d

, which satisï¬es

max
i

yi â‰¤

(cid:114)

(cid:96)(c + 1)Î²
2

log n + o(log n) with probability

1 âˆ’ nâˆ’c,

(cid:96) =

(cid:40)

2,
1,

d > 2,
d = 2;

3. (cid:15)1 := 1âˆš
d

(cid:80)

sâˆˆC1

Âµs and (cid:15)2 := 1âˆš
d

(cid:80)

sâˆˆC2

Âµs, which satisfy

max{(cid:15)1, (cid:15)2} â‰¤

2c
3

log n + o(log n) with probability

1 âˆ’ nâˆ’c.

7

In Lemma 3.5, xi âˆ¼ Binom(m, p) follows the binomial distribution, which is bounded by the
result in [19, Lemma 2]. For yi, it depends on the bound of Âµi, according to Lemma 3.3, this further
relies on bounding (cid:107) (cid:80)m
i=1 Ri(cid:107)F where {Ri}m
i=1 are i.i.d. random matrices uniformly drawn from
SO(d). When d > 2, (cid:107) (cid:80)m
i=1 Ri(cid:107)F is bounded by matrix Bernstein inequality [45], and when d = 2,
a sharper result is derived by explicitly computing the moments of (cid:107) (cid:80)m
i=1 Ri(cid:107)F and then apply
Markov inequality, which are detailed in Appendix A.2. For (cid:15)1 and (cid:15)2, by deï¬nition they depend
on the concentration of Âµi and are thus bounded by Bernsteinâ€™s inequality [47].

It remains to bound (cid:107)E[A]âˆ’A(cid:107). If A is an adjacency matrix where Aij âˆˆ {0, 1}, the upper bound
of (cid:107)E[A]âˆ’A(cid:107) has been studied in the context of SBM (e.g. [19, 25]). However, here Aij is a random
rotation matrix for C(i) (cid:54)= C(j) rather than being {0, 1}-valued. Therefore, in Appendix A.3, we
derive an upper bound of the operator norm of a random block matrix in Theorem A.7, which leads
us to the following result for (cid:107)E[A] âˆ’ A(cid:107).

Lemma 3.6. Let the two clusters be of size m1 and m2 (m1 + m2 = n), and p, q = â„¦(log n/n).
Then for A deï¬ned in (1), it satisï¬es

(cid:107)E[A] âˆ’ A(cid:107) â‰¤ c1(

âˆš

pm1 +

âˆš

pm2) + c2(

âˆš

qm1 +

âˆš

qm2) + O((cid:112)log n)

with probability 1 âˆ’ nâˆ’c for c > 0, where c1, c2 > 0 are some universal constants.

Proof of Theorem 3.1. By applying the union bound on mini xi, maxi yi and max{(cid:15)1, (cid:15)2} in Lemma 3.5,
we obtain

Î»min(pId âˆ’ Z) â‰¥

log n + o(log n),

(cid:114)

(cid:18) Ï„
2

âˆ’

(cid:96)(c + 1)Î²
2

âˆ’

2c
3

(cid:19)

with probability 1 âˆ’ nâˆ’â„¦(1), as long as the condition in Lemma 3.5 that

1 âˆ’

(cid:16)

1
2

Î± âˆ’ Ï„ log

(cid:17)(cid:17)

(cid:16) eÎ±
Ï„

< 0

(18)

(cid:113) (cid:96)(c+1)Î²
2

is satisï¬ed. Also, from Lemma 3.6, we have (cid:107)E[A] âˆ’ A(cid:107) = O(
log n) with high probability. This
implies, as n is suï¬ƒciently large, (17) holds if Î»min(pId âˆ’ Z) = â„¦(log n), that is equivalent to
Ï„
2 âˆ’
It
remains to ensure there exists a Ï„ âˆˆ (0, Î±] that satisï¬es (18). To this end, since the LHS of (18)
is monotonically increasing for Ï„ âˆˆ (0, Î±], by choosing Ï„ â†’
2(cid:96)Î² the condition (18) becomes (10),
which is the condition for exact recovery.

3 > 0. By taking c â†’ 0 to be suï¬ƒciently small, it reduces to Ï„ >

âˆ’ 2c

2(cid:96)Î².

âˆš

âˆš

âˆš

3.2 Two unequal-sized clusters with known cluster size

âˆš

In
Now we consider the case where the two cluster sizes m1, m2 are unequal and known to us.
this case, the ground truth M âˆ— still has the form in (4) and convex relaxations similar to (9) can
be applied. However, unlike the equal-sized case, the row-sum constraint satisï¬es (cid:80)n
j=1 (cid:107)Mij(cid:107)F =
d if i âˆˆ C2. Therefore a convex constraint can be imposed
m1
as (cid:80)n
d. So this imposes an upper-bound on the cluster sizes, based
on the size of the larger cluster. Besides, to incorporate the information of the smaller cluster of the
two, we consider an additional constraint on the sum of all blocks as (cid:80)n
d.

d if i âˆˆ C1 and (cid:80)n
j=1 (cid:107)Mij(cid:107)F â‰¤ max{m1, m2}

j=1 (cid:107)Mij(cid:107)F = m2

i,j=1 (cid:107)Mij(cid:107)F â‰¤ (m2

1 +m2
2)

âˆš

âˆš

âˆš

8

(20)

(21)

In this way, the resulting SDP is

MSDP = arg max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

n
(cid:88)

j=1

(cid:107)Mij(cid:107)F â‰¤ max{m1, m2}

âˆš

d,

âˆ€i = 1, . . . , n,

(19)

(cid:107)Mij(cid:107)F â‰¤ (cid:0)m2

1 + m2
2

(cid:1) âˆš

d.

n
(cid:88)

i,j=1

For (19), the condition for exact recovery is derived as the following.

Theorem 3.7. Let p = Î± log n/n, q = Î² log n/n. Suppose m1 = Ïn, m2 = (1 âˆ’ Ï)n for some
Ï âˆˆ (1/2, 1) such that m1 > m2. Let Ï„ âˆ—

1 , Ï„ âˆ—
(cid:19)

2 âˆˆ [0, Î±) be the roots of the equations
(cid:18) eÎ±
Ï„ âˆ—
2

1
1 âˆ’ Ï

, Î± âˆ’ Ï„ âˆ—

2 log

1
Ï

=

=

(cid:19)

.

(cid:18) eÎ±
Ï„ âˆ—
1

Î± âˆ’ Ï„ âˆ—

1 log

As n is suï¬ƒciently large, (19) achieves exact recovery with probability 1 âˆ’ nâˆ’â„¦(1) if

ï£±
ï£´ï£²

Î± >

ï£´ï£³

Î± >

1
1 âˆ’ Ï
1
1 âˆ’ Ï

,

,

Ï„ âˆ—
1
Î´2

> 1

and

Ï„ âˆ—
1
Î´1

(cid:26)

> max

1 âˆ’

(cid:27)

Ï„ âˆ—
2
2Î´2

,

1
2

when Î² = 0,

, when Î² > 0,

where Î´1 :=

(cid:16) 2âˆš

Ï + 1âˆš

1âˆ’Ï

(cid:17) âˆš

lÎ², Î´2 :=

(cid:16) 1âˆš

Ï + 1âˆš

1âˆ’Ï

(cid:17) âˆš

lÎ². (cid:96) = 2 when d > 2 and (cid:96) = 1 when d = 2.

Note that the function f (Ï„ ) := Î± âˆ’ Ï„ log (eÎ±/Ï„ ) monotonically decreases for Ï„ âˆˆ [0, Î±), and it
2 of
2 âˆˆ [0, Î±) when Î± > 1/(1 âˆ’ Ï). In the following section, we outline the key steps for

satisï¬es f (Î±) = 0 and limÏ„ â†’0 f (Ï„ ) = Î±. Therefore, there exists a unique pair of solutions Ï„ âˆ—
(20) for Ï„ âˆ—
proving Theorem 3.7 and leave the detail to Appendix B.2.

1 , Ï„ âˆ—

1 , Ï„ âˆ—

3.2.1 The sketch of proof

The steps for proving Theorem 3.7 are similar to those outlined in Section 3.1.1, while a consid-
erable amount of eï¬€ort is devoted to handle the imbalanced clusters and the additional constraint
(cid:80)n

(cid:1) âˆš

i,j=1 (cid:107)Mij(cid:107)F â‰¤ (cid:0)m2

1 + m2
2

d.

Step 1: Derive KKT conditions, uniqueness and optimality of M âˆ—. The Lagrangian of
(19) is given as

L = âˆ’ (cid:104)A, M (cid:105) âˆ’ (cid:104)Î›, M (cid:105) âˆ’

n
(cid:88)

i=1

(cid:104)Zi, Mii âˆ’ Id(cid:105) +

n
(cid:88)

(cid:28)

Âµi,

i=1

n
(cid:88)

j=1

(cid:107)Mij(cid:107)F âˆ’ m1

âˆš

(cid:29)

d

(cid:18) (cid:88)

+ Î½

i,j

(cid:107)Mij(cid:107)F âˆ’ (m2

1 + m2
2)

âˆš

(cid:19)
,

d

where Î½ is the dual variable associated with the extra constraint. Then we list the KKT conditions

9

of (19) when M = M âˆ— as

â€¢ Stationarity:

i=1) + Î˜ + Î˜(cid:62) = 0,
âˆ’ A âˆ’ Î› âˆ’ diag({Zi}n
(cid:40)
(Âµi + Î½)M âˆ—
M âˆ—
d,
(Âµi + Î½)Î±ij, (cid:107)Î±ij(cid:107)F â‰¤ 1, M âˆ—

i,j=1, Î˜ij =

Î˜ = [Î˜ij]n

ij/

âˆš

ij (cid:54)= 0,
ij = 0.

â€¢ Comp. slackness:

(cid:104)Î›, M âˆ—(cid:105) = 0,

Î½

(cid:18) (cid:88)

i,j

(cid:107)M âˆ—

ij(cid:107)F âˆ’ (m2

1 + m2
2)

âˆš

(cid:19)

d

= 0,

(cid:28)

âˆš

d âˆ’

Âµi, m1

(cid:29)

(cid:107)M âˆ—

ij(cid:107)F

n
(cid:88)

j=1

= 0,

i = 1, . . . , n,

â€¢ Dual feasibility:

Î› (cid:23) 0,

Î½ â‰¥ 0, Âµi â‰¥ 0,

i = 1, . . . , n.

Again, the following lemma establishes the uniqueness and optimality of M âˆ—:

(22)

(23)

(24)

(25)

(26)

Lemma 3.8. Given M âˆ— deï¬ned in (2), suppose there exists dual variables Î›, Z, {Âµi}n
and Î½ that satisfy the KKT conditions (22) - (26) as well as

i=1, {Î±ij}(i,j):M âˆ—

ij =0

N (Î›) = R(M âˆ—).

(27)

Then M âˆ— is the optimal and unique solution to (19).

Step 2: Construct dual variables. Again, assuming M âˆ—
that C(i) = C(j). we make the following guess of the dual variables.

ij = Id and Aij = rijId for any (i, j)

Lemma 3.9. With a constant Î³ âˆˆ [1/2, 1], the following forms of the dual variables satisfy (22) -
(25) and Î½ â‰¥ 0, Âµi â‰¥ 0, for i = 1, . . . , n in (26).

(cid:101)Î±ij =

Î˜ij =

1
m1
ï£±
ï£´ï£²

ï£´ï£³

(cid:88)

s1âˆˆC1

As1j +

âˆš

(Âµi + Î½)Id/
Î³ (cid:101)Î±ij,
(1 âˆ’ Î³) (cid:101)Î±(cid:62)
ji,

1
m2

(cid:88)

s2âˆˆC2

Ais2 âˆ’

1
m1m2

(cid:88)

(cid:88)

s1âˆˆC1

s2âˆˆC2

As1s2,

i âˆˆ C1, j âˆˆ C2,

d, C(i) = C(j),

ï£³

Âµi =

i âˆˆ C1, j âˆˆ C2,
i âˆˆ C2, j âˆˆ C1,
(cid:18) m1(Âµi + Î½)
ï£±
âˆš
ï£´ï£´ï£´ï£²
(cid:18) 2m2Î½
ï£´ï£´ï£´ï£³
âˆš
d

âˆ’

d

(cid:88)

sâˆˆC2

(cid:88)

+

sâˆˆC1
(cid:19)

ï£±
ï£²

0,

max{0, Î³ Â· max
jâˆˆC2

(cid:107) (cid:101)Î±ij(cid:107)F âˆ’ Î½},

i âˆˆ C1,

i âˆˆ C2,

(cid:18) Âµs + Î½
âˆš
d

(cid:19)(cid:19)

âˆ’ ris

Id,

i âˆˆ C1,

ris

Id,

i âˆˆ C2,

Î½ = (1 âˆ’ Î³) max

iâˆˆC2,jâˆˆC1

(cid:107) (cid:101)Î±ji(cid:107)F, Zi =

Î› = âˆ’A âˆ’ diag ({Zi}n

i=1) + Î˜ + Î˜(cid:62).

Furthermore, Î› can be simpliï¬ed as the following form.

Lemma 3.10. Given the dual variables in Lemma 3.9, Î› can be expressed as

Î› = (Ind âˆ’ Î ) (E[A] âˆ’ A âˆ’ Z + pInd)
(Ind âˆ’ Î ) where Î  :=
(cid:125)

(cid:124)

(cid:123)(cid:122)
=: (cid:101)Î›

(cid:32)

(cid:102)M âˆ—
1 /m1
0

(cid:33)

.

0
(cid:102)M âˆ—
2 /m2

Then, Î› (cid:23) 0 and N (Î›) = R(M âˆ—) are satisï¬ed if (cid:101)Î› (cid:31) 0.

10

Diï¬€erent from Step 2 in Section 3.1.1, here, we adopt a new way to construct Î˜ij in order to
obtain a threshold close to the empirical phase transition line by tuning Î³. The parameter Î³ controls
the values of Î½, Âµi, and the diagonal entries of Zi. As we shall see in the next step, by adjusting
Î³, we can balance the diagonal values of Zi for i âˆˆ C1 and i âˆˆ C2, which helps to ensure (cid:101)Î› (cid:31) 0.
We remark that our current construction of the dual variables may still be sub-optimal, which
indicates the condition in (21) is suï¬ƒcient but not necessary. Finding the optimal dual appears to
be challenging and is left for future work.

Step 3: Find the condition for (cid:101)Î› (cid:31) 0. Again, by using the same argument as in Section 3.1.1,
one can see that for exact recovery it suï¬ƒces to show Î»min(pInd âˆ’ Z) > (cid:107)E[A] âˆ’ A(cid:107). Here we get
the following bound for Î»min(pInd âˆ’ Z).

Lemma 3.11. Let p = Î± log n/n, q = Î² log n/n and Î³ âˆˆ [1/2, 1]. Suppose m1 = Ïn and m2 =
(1 âˆ’ Ï)n for some Ï âˆˆ (1/2, 1). Î´1 and Î´2 are deï¬ned in Theorem 3.7. We have

Î»min(pInd âˆ’ Z) â‰¥ min{Ï(Ï„1 âˆ’ max{Î³Î´1, Î´2}), (1 âˆ’ Ï)(Ï„2 âˆ’ 2(1 âˆ’ Î³)Î´2)} log n âˆ’ o(log n)

with probability 1 âˆ’ nâˆ’â„¦(1) for some Ï„1, Ï„2 âˆˆ [0, Î±] such that

(cid:18)

1 âˆ’ Ï

Î± âˆ’ Ï„1 log

(cid:19)(cid:19)

(cid:18) eÎ±
Ï„1

< 0,

1 âˆ’ (1 âˆ’ Ï)

Î± âˆ’ Ï„2 log

(cid:18)

(cid:19)(cid:19)

(cid:18) eÎ±
Ï„2

< 0.

(28)

Proof of Theorem 3.7. From Lemma 3.11, we get Î»min(pInd âˆ’ Z) = O(log n). From Lemma 3.6, we
have (cid:107)E[A] âˆ’ A(cid:107) = o(
log n). Therefore, as n is suï¬ƒciently large, the condition Î»min(pInd âˆ’ Z) >
(cid:107)E[A] âˆ’ A(cid:107) is satisï¬ed if Î»min(pInd âˆ’ Z) = â„¦(log n), which is equivalent to

âˆš

min{Ï(Ï„1 âˆ’ max{Î³Î´1, Î´2}), (1 âˆ’ Ï)(Ï„2 âˆ’ 2(1 âˆ’ Î³)Î´2)} > 0.

(29)

By deï¬ning Â¯Ï„1 := max{Î³Î´1, Î´2} and Â¯Ï„2 := 2(1 âˆ’ Î³)Î´2, (29) reduces to Ï„1 > Â¯Ï„1 and Ï„2 > Â¯Ï„2. Now
it remains to check if (28) holds for some Ï„1, Ï„2 âˆˆ [0, Î±). To this end, let Ï„ âˆ—
2 âˆˆ [0, Î±) be the
roots of the equations in (20) (Notice that such Ï„ âˆ—
2 exist only if Î± > 1/(1 âˆ’ Ï)). Since the
function f (Ï„ ) := Î± âˆ’ Ï„ log (eÎ±/Ï„ ) monotonically decreases for Ï„ âˆˆ [0, Î±), then (28) holds only if
1 and Ï„2 < Ï„ âˆ—
Ï„1 < Ï„ âˆ—
2 . Putting all these together, the condition for exact recovery reduces to
Î± > 1/(1 âˆ’ Ï), Â¯Ï„1 < Ï„1 < Ï„ âˆ—
2 . Furthermore, since Î³ âˆˆ [1/2, 1] is not speciï¬ed, it
suï¬ƒces to ensure Â¯Ï„1 < Ï„ âˆ—
2 for some Î³ âˆˆ [1/2, 1] such that the range of Ï„1, Ï„2 is valid.
This leads to the condition in (21).

1 and Â¯Ï„2 < Ï„2 < Ï„ âˆ—

1 and Â¯Ï„2 < Ï„ âˆ—

1 , Ï„ âˆ—

1 , Ï„ âˆ—

3.3 Two clusters with unknown cluster size

In real applications, we usually have no information on cluster sizes. This motivates us to seek
an alternative approach that does not rely on such prior knowledge. To this end, we consider a
normalized ground truth Â¯M âˆ—, whose (i, j)-th block is deï¬ned as

Â¯M âˆ—

ij =

(cid:40) 1

mC(i)
0,

RiR(cid:62)

j , C(i) = C(j),
otherwise.

(30)

Here mC(i) denotes the size of the cluster that i belongs to. By assumption that the ï¬rst m1 nodes
form C1 and the remaining m2 nodes form C2, we have

Â¯M âˆ— :=

1
m1

V (1)(V (1))(cid:62) +

1
m2

V (2)(V (2))(cid:62) =

11

(cid:32) 1

m1 (cid:102)M âˆ—
0

1

(cid:33)

,

(31)

0
m2 (cid:102)M âˆ—

1

2

where each diagonal block has been normalized by the corresponding cluster size. Then Â¯M âˆ— satisï¬es
the following structural properties:

1. Â¯M âˆ— is positive semi-deï¬nite, i.e. Â¯M âˆ— (cid:23) 0;
2. The sum of the diagonal blocks satisï¬es (cid:80)
3. The i-th block row of Â¯M âˆ— satisï¬es (cid:80)n

ij(cid:107)F â‰¤
Based on these, we formulate the SDP for solving Â¯M âˆ— as

j=1 (cid:107) Â¯M âˆ—

Â¯M âˆ—

i

ii = 2Id;
âˆš

d, for i = 1, . . . , n.

MSDP = arg max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0,

n
(cid:88)

i=1

Mii = 2Id,

âˆš

(cid:107)Mij(cid:107)F â‰¤

n
(cid:88)

j=1

d,

i = 1, . . . , n.

(32)

Due to the normalization factor, (32) diï¬€ers from the previous SDPs that it does not rely on the
cluster sizes, and exact recovery is achieved if the normalized Â¯M âˆ— is obtained. Besides, similar to
(19) a sum of all blocks constraint (cid:80)n
d could apply to (32), via the row sum
constraint that (cid:80)n
d for all i and is thus redundant. Notably, in the context of
community detection, similar normalization has been studied [34, 48, 35] when cluster sizes are
unknown, where the row-sum constraint is formulated as (cid:80)n
In contrast, we use
an inequality constraint for the sake of convexity. Here, we obtain the following exact recovery
guarantees for (32).

i,j=1 (cid:107)Mij(cid:107)F â‰¤ n

j=1 (cid:107)Mij(cid:107)F â‰¤

j=1 Mij = 1.

âˆš

âˆš

Theorem 3.12. Let p = Î± log n/n, q = Î² log n/n. Suppose m1 = Ïn and m2 = (1 âˆ’ Ï)n for some
Ï âˆˆ (0, 1). Ï„ âˆ—
2 are deï¬ned in (20). As n is suï¬ƒciently large, (32) achieves exact recovery i.e.
MSDP = Â¯M âˆ— with probability 1 âˆ’ nâˆ’â„¦(1) if
(cid:26)

1 , Ï„ âˆ—

(cid:27)

(cid:27)

< Î± â‰¤ min

2Ï„ âˆ—

1 , 2Ï„ âˆ—

2 , Ï„ âˆ—

1 + Ï„ âˆ—

2 âˆ’

(cid:18) 1
âˆš
Ï

+

âˆš

1
1 âˆ’ Ï

(cid:19)
(cid:112)(cid:96)Î²

,

(33)

max

(cid:26) 1
Ï

,

1
1 âˆ’ Ï

where (cid:96) = 2 when d > 2 and (cid:96) = 1 when d = 2.

The proof of Theorem 3.12 is deferred to Appendix B.3, as it follows a similar structure as the

one presented in Section 3.1.1, with a slight diï¬€erence on the construction of dual variables.

Rounding. In practice, we observe that an extra rounding procedure can improve the recovery of
M âˆ—. As an illustration, in Figure 2 we present an example with 2D rotation and two equal-sized
clusters. For the ease of presentation, we choose to use a complex number to represent each Â¯M âˆ—
ij
such that Â¯M âˆ—
ij| denotes the â€œamplitudeâ€ and Î¸ij âˆˆ [0, 2Ï€] represents the
rotation angle. Then we form two n Ã— n matrices | Â¯M âˆ—| and âˆ  Â¯M âˆ— that contain

ij|eÄ±Î¸ij , where | Â¯M âˆ—

ij = | Â¯M âˆ—

amplitude: | Â¯M âˆ—|ij := | Â¯M âˆ—

ij|,

and rotation angle âˆ  Â¯M âˆ—

ij := Î¸ij âˆ’ Ï€

(34)

respectively. A similar procedure is applied to A and MSDP. In Figures 2a and 2c, one can see
that |MSDP| (cid:54)= | Â¯M âˆ—| and thus exact recovery fails. However, they have the same support on the
two clusters. Moreover, in Figures 2e and 2g we observe âˆ MSDP = âˆ  Â¯M âˆ—, meaning the rotational
alignments are exactly recovered. These observations inspire us to design the following rounding
procedure. For each block of MSDP denoted by (MSDP)ij, we round it to

(Mround)ij =

ï£±
ï£²

ï£³

(MSDP)ij
(cid:107)(MSDP)ij(cid:107)F
0,

âˆš

Â·

d,

(cid:107)(MSDP)ij(cid:107)F â‰¥ (cid:15),

otherwise,

(35)

12

(a) | Â¯M âˆ—|

(b) |A|

(c) |MSDP|

(d) |Mround|

(e) âˆ  Â¯M âˆ—

(f) âˆ A

(g) âˆ MSDP

(h) âˆ Mround

Figure 2: An example of rounding when d = 2. We plot the matrix of amplitudes ((a)â€“(d)) and
rotation angles ((e)â€“(h)) deï¬ned in (34) for the ground truth Â¯M âˆ—, the observation matrix A, the
SDP result MSDP by (32) and the rounding result Mround.

where (cid:15) is some small value for avoiding numerical issues. That is, for each non-zero block of MSDP
d and keep its â€œphaseâ€ unchanged. Let
we normalize its â€œamplitudeâ€ (Frobenius norm) to be
Mround be the rounded solution based on all blocks of MSDP, then in Figures 2d and 2h we obtain
Mround = M âˆ— which is equal to the unnormalized ground truth in (2). Therefore, empirically, we
observe exact recovery after an appropriate rounding.

âˆš

3.4 SDPs for general cluster structures

The SDP formulation can be easily extended to general cases with an arbitrary number of clusters.
When cluster sizes are known, the convex relaxation introduced in (19) still applies to M âˆ— in
principle, and we can formulate an SDP relaxation as

MSDP = arg max
M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

(cid:88)

j

(cid:107)Mij(cid:107)F â‰¤ (max

i

âˆš

mi)

d,

i = 1, . . . , n,

(36)

n
(cid:88)

i,j=1

(cid:107)Mij(cid:107)F â‰¤

K
(cid:88)

âˆš

d

m2
k.

k=1

When the cluster sizes are unknown, an SDP similar to (32) can be designed as:

MSDP = max

M âˆˆRndÃ—nd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0,

(cid:88)

i

Mii = KId,

âˆš

(cid:107)Mij(cid:107)F â‰¤

(cid:88)

j

d,

i = 1, . . . , n,

(37)

which only requires the number of clusters K as a parameter. Moreover, a similar rounding pro-
cedure as in Section 3.3 can be used to improve the solution by (37). Finding the exact recovery
guarantee for both (36) and (37) is an interesting but challenging task, which is left for future work.

13

(a) n = 50

(b) n = 100

(c) n = 150

(d) n = 150, by [19, Eq. (5)]

(e) Comparison of two thresholds

Figure 3: Results on two equal-sized clusters with known cluster size. (a)â€“(c) Recovery error of
MSDP by (9) with diï¬€erent n. The thresholds in Theorem 3.1 for d = 2 and d > 2 are shown in
red solid curve and red dashed curve, respectively. (d) Recovery error by ï¬rst performing clustering
on graph scalar edge connection using the SDP in [19, Eq. (5)] and then synchronization, the exact
recovery threshold
2 (see [19, Theorem 2]) is shown in red. (e) Comparison of the
thresholds in Theorem 3.1 for d = 2 (blue) and [19, Theorem 2] (red), exact recovery is guaranteed
in the region below each threshold. The recovery error is deï¬ned in (38).

Î² =

Î± âˆ’

âˆš

âˆš

âˆš

4 Numerical results

This section is devoted to empirically investigating the performance of the SDPs in Section 3. In
Sections 4.1â€“4.3, we focus on experiments with two clusters and 2D rotations (d = 2). In Section 4.4,
we include results for more general settings, e.g., 3D rotations and the number of clusters is larger
than two. We generate the observation matrix A based on the model introduced in Section 2. Then
we evaluate the SDP solution by measuring the following error metric:

Error = log((cid:107)MSDP âˆ’ M âˆ—(cid:107)F).

(38)

When cluster sizes are unknown and the SDP in (32) is adopted, the ground truth M âˆ— in (38) should
be replaced by the normalized Â¯M âˆ— deï¬ned in (30). For the case of two clusters, we evaluate the
phase transition of the condition (cid:101)Î› (cid:31) 0 according to our guess of dual variables (e.g. in Lemmas 3.3
and 3.9) via the following metric:

Failure rate = 1 âˆ’ the rate that (cid:101)Î› (cid:31) 0 is satisï¬ed.

(39)

Failure rate = 0 means that our construction of the dual variables satisfy the optimality and unique-
ness condition for M âˆ— or Â¯M âˆ— (e.g. in Lemmas 3.2, 3.8 and B.1) and the exact recovery is guaranteed.
For each experiment, we average the result over 10 diï¬€erent realizations of the observation matrix
A.

14

)
0
4
,
0
6
(
=
)
2

m

,
1

m

(

)
0
2
,
0
8
(
=
)
2

m

,
1

m

(

(a) Error of MSDP

(b) Failure rate

Figure 4: Results on two unequal-sized clusters with known cluster sizes. (a) Recovery error of the
solution of (19); (b) The failure rate deï¬ned in (39) for (cid:101)Î› in Lemma 3.10. The threshold given in
Theorem 3.7 for d = 2 is shown in red.

4.1 Two equal-sized clusters

First, we consider two equal-sized clusters where the SDP in (9) is used for recovery. In Figures 3aâ€“
3c we plot the recovery error with varying Î± and Î² for n = 50, 100, 150. In each plot, the red solid
curve corresponds to the threshold of the condition in Theorem 3.1 for d = 2 . As we can see, (9)
achieves exact recovery within a wide range of Î± and Î². Also, the theoretical threshold closely ï¬ts
the empirical phase transition boundary, especially as n is large, which indicates the sharpness of
the condition. In contrast, the red dash curve corresponds to the threshold of the conditions given
in Theorem 3.1 for d > 2, which is slightly loose by a constant factor

âˆš

2.

In addition, we consider using the two-stage approach mentioned in the introduction that ï¬rst
applies existing methods for community detection and then performs synchronization for each com-
munity separately. We use the SDP proposed in [19, Eq. (5)] to detect the cluster membership,
which is proved to achieve the information-theoretical limit as n â†’ âˆ [2, 19]. As a result, Figure 4a
displays the corresponding recovery error with n = 150, which exhibits worse performance than
Figure 3c. In Figure 3e, we compare the theoretical thresholds for exact recovery in Theorem 3.1
for d = 2 and [19, Theorem 2]. It is clear to see that, by incorporating the additional pairwise
alignment information, our method outperforms the classical community detection method which
only exploits the edge connectivity.

4.2 Two unequal-sized clusters

Next, we consider two unequal-sized clusters where the cluster sizes (m1, m2) are given, and the
SDP in (19) is used for recovery. We simulate A under diï¬€erent settings of (m1, m2), and plot the
recovery error with varying Î± and Î² in Figure 4. By comparing the results for (m1, m2) = (60, 40)
and (m1, m2) = (80, 20), we ï¬nd that it gets more challenging to recover M âˆ— if the cluster sizes are

15

(a) (m1, m2) = (600, 400)

(b) (m1, m2) = (800, 200)

Figure 5: Results on two unequal-sized clusters with known cluster sizes, large n. We plot the failure
rate deï¬ned in (39) for (cid:101)Î› in Lemma 3.10. The threshold in Theorem 3.7 for d = 2 is shown in red.

more imbalanced. The red curve in each plot corresponds the threshold of the suï¬ƒcient conditions
given in Theorem 3.7 for d = 2. Clearly, there is a gap between the empirical phase transition
boundary and the theoretical threshold, which implies the condition in Theorem 3.7 may not be
necessary.

To understand why there exists such gap, we construct the dual variables according to our
construction in Lemma 3.9 for each realization of A, and in Figure 4b we plot the failure rate in
(39) for (cid:101)Î› deï¬ned in Lemma 3.10. As a result, we see that in some certain regime of Î± and Î²,
even though our dual construction does not satisfy such conditions, empirically we still observe
exact recovery of (19) in Figure 4a. This implies our guess of the dual variables in Lemma 3.9 is
sub-optimal. In addition, to further verify the sharpness of the conditions in Theorem 3.7. As n
is large, we test the failure rate under diï¬€erent setting of (m1, m2). As we can see in Figure 5,
the threshold of the condition in Theorem 3.7 sharply characterize the empirical phase transition
boundaries, which indicates the derived condition is tight based on our guessed dual variables.

4.3 Two clusters with unknown cluster sizes

Here we consider two clusters with unknown cluster sizes, where (32) is used for recovery. We
simulate A with diï¬€erent settings of (m1, m2).
In Figure 6a we plot the recovery error of the
SDP solutions. Also, we apply the rounding step (35) to further reï¬ne the SDP solutions and
plot the error after rounding in Figure 6b. As a result, one can see that the result after rounding
clearly exhibits certain improvement against the original one. In addition, to check whether our
construction of the dual variables in Lemma B.2 is optimal, in Figure 6c we show the failure rate in
(39) for (cid:101)Î› in Lemma B.3. Again, we see that there is certain regime of Î±, Î² where exact recovery
by (32) is possible (see Figure 6a), but the constructed dual certiï¬cate fails to satisfy the condition,
indicating a lack of optimality in the choice of dual variables.

While our guess of the dual variables is not optimal, as n is large we are able to sharply
characterize when such a dual certiï¬cate fails to satisfy the condition in Lemma B.2. This is
demonstrated in Figure 7, where we show good agreement between the predicted boundary in
Theorem 3.12 and the empirical boundary of the failure rate.

4.4 3D rotations and K = 3

Here we extend our experiments to SO(3) group transformation. In Figure 8 we plot the recovery
errors by SDPs on diï¬€erent scenarios under two clusters. Similar to the results for SO(2), we

16

)
0
5
,
0
5
(
=
)
2

m

,
1

m

(

)
0
2
,
0
8
(
=
)
2

m

,
1

m

(

(a) Error of MSDP

(b) Error of Mround

(c) Failure rate

Figure 6: Results on two clusters with unknown cluster sizes. (a) Recovery error of MSDP by (32);
(b) Recovery error of Mround by (35); (c) The failure rate deï¬ned in (39) for (cid:101)Î› in Lemma B.3.

(a) (m1, m2) = (600, 400)

(b) (m1, m2) = (800, 200)

Figure 7: Results on two clusters with unknown cluster sizes with large n. We plot the failure rate
deï¬ned in (39) for (cid:101)Î› in Lemma B.3. The threshold given in Theorem 3.12 for d = 2 is shown in
red.

observe sharp phase transitions in the SDP solutions. Speciï¬cally, in Figures 8a and 8b we display
the thresholds for exact recovery derived in Sections 3.1 and 3.2, respectively. One can see that
exact recovery is still possible even above the thresholds, which implies there is still some room
for improvement of our conditions. Also, we test the generalized SDPs proposed in (36) and (37),
under the setting of K = 3 and (m1, m2, m3) = (25, 25, 25). The plots of recovery error are shown in
Figure 9. As we can see, the proposed SDPs achieves exact recovery with sharp phase transitions.
In addition, the rounding step is eï¬€ective to further improve the SDP solutions in both cases.

17

(a) Equal, by (9)

(b) Unequal, by (19)

(c) Unknown, by (32)

(d) Rounding, by (35)

Figure 8: Results on SO(3) group transformation. We plot the recovery error under (a) two equal-
sized clusters, the thresholds in Theorem 3.1 for d = 2 and d > 2 are shown in red solid and red dash
curve, respectively; (b) two unequal-sized clusters, the thresholds in Theorems 3.7 for d = 2 and
d > 2 are shown in red solid and red dash curve, respectively; (c)â€“(d) two clusters with unknown
cluster sizes. We test under (m1, m2) = (25, 25) for (a), and (m1, m2) = (30, 20) for (b)â€“(d).

(a) MSDP by (36)

(b) MSDP by (37)

(c) Mround by (35)

Figure 9: Results on three clusters. We test under the setting (m1, m2, m3) = (25, 25, 25). We plot
the recovery error under (a) known cluster sizes, recovered by (36); (b)â€“(c) unknown cluster sizes,
recovered by (37) and rounded by (35) respectively.

5 Conclusion and open problems

In this paper, we propose to solve the community detection and rotational synchronization simulta-
neously via semideï¬nite programming. Compared with the existing SDP for community detection
under SBM, our proposed SDPs not only take into account the graph structure but also the ad-
ditional information on pairwise rotational alignments, which gives better clustering results.
In
particular, for the case of two clusters we obtain suï¬ƒcient conditions for the SDPs to achieve exact
In addition, we acquire
recovery, which characterize the empirical phase transition boundaries.
new concentration inequalities for the Frobenius norm of sum of random rotation matrices and the
operator norm of a random block matrix as interesting byproducts from our analysis.

There are several directions to be further explored. First, it is natural to expect that the results
obtained in this paper can be extended to a much more general family of groups, such as symmetric
group and orthogonal group. Also our analysis can be applied to other probabilistic models on
the pairwise relations, such as the additive Gaussian model considered in [27].
In addition, the
conditions for exact recovery by (19) and (32) in the case of unequal and unknown cluster sizes can
be improved by ï¬nding a more optimal construct of dual variables. For d > 2, current results can
be further improved with a Frobenius norm bound on the sum of random rotation matrices that
is tighter than the one given in Theorem A.3. Furthermore, we note that ï¬nding the information-

18

theoretical limit of exact recovery on the proposed probabilistic model is still an open problem.
Finally, although the SDP can be solved in polynomial time complexity, it is still computationally
expensive when the graph size is large, and one can consider spectral methods and message passing
algorithms in that situation.

A Important technical ingredients

This section is devoted to technical ingredients that support our proofs in Appendix B. Speciï¬cally,
we derive new matrix concentration inequalities for random rotation matrices in Appendix A.2, and
new upper bounds of the operator norm of random block matrices in Appendix A.3.

A.1 A tail bound of Bernoulli trials

Our analysis relies on the following tail bound of a Bernoulli trail.

Lemma A.1 ([19, Lemma 2]). Let X âˆ¼ Binom (m, Î± log n/n) for m âˆˆ N, Î± = O(1), where m = Ïn
for some Ï > 0. Let kn = Ï„ Ï log n for some Ï„ âˆˆ (0, Î±]. Then for a suï¬ƒciently large n,

P {X â‰¤ kn} = nâˆ’Ï(Î±âˆ’Ï„ log( eÎ±

Ï„ )+o(1)).

A.2 Concentration inequalities for random rotation matrices

Given a series of i.i.d. random matrices Xi âˆˆ RdÃ—d for i = 1, . . . m such that
(cid:40)

Xi =

Ri,
0,

with probability q,
otherwise,

(40)

where Ri is a random rotation matrix with det(Ri) = 1, uniformly drawn from SO(d) according
to Haar measure. Our analysis relies on bounding the Frobenius norm of Z = (cid:80)m
i=1 Xi, namely
(cid:107)Z(cid:107)F. In general, a fairly tight bound can be obtained from matrix concentration inequalities such
as matrix Bernstein [44, Theorem 1.6], which is presented as following:

Theorem A.2 (Matrix Bernstein, [44, Theorem 1.6]). Let X1, . . . , Xm âˆˆ RdÃ—d be independent,
centered random matrices and each one is uniformly bounded as

E[Xi] = 0 and

(cid:107)Xi(cid:107) â‰¤ L,

for i = 1, . . . , n.

Let Z =

n
(cid:80)
i=1

Xi and v(Z) denotes the matrix variance statistic of Z as

Then for any t > 0,

v(Z) = max{(cid:107)E(ZZ(cid:62))(cid:107), (cid:107)E(Z(cid:62)Z)(cid:107)}.

P {(cid:107)Z(cid:107) â‰¥ t} â‰¤ 2d exp

(cid:18) âˆ’t2/2

v(Z) + Lt/3

(cid:19)

.

As a result, by applying Theorem A.2 on Xi deï¬ned in (40) we get:

19

Theorem A.3. Let X1, . . . , Xm âˆˆ RdÃ—d be i.i.d. random matrices deï¬ned in (40) and Z =

m = O(n). Then for any t > 0,

P

(cid:110)

(cid:107)Z(cid:107)F â‰¥

âˆš

(cid:111)

dt

â‰¤ 2d exp

(cid:18) âˆ’t2/2

(cid:19)

qm + t/3

.

In other words,

(cid:107)Z(cid:107)F â‰¤ (cid:112)2qm(c log n + log 2d)

1
âˆš
d

(cid:32)(cid:115)

1 +

c log n + log 2d
18qm

+

(cid:115)

(cid:33)

c log n + log 2d
18qm

with probability 1 âˆ’ nâˆ’c for any c > 0.

m
(cid:80)
i=1

Xi,

(41)

(42)

Proof. By deï¬nition of the operator norm and Frobenius norm (cid:107)Z(cid:107) â‰¤ (cid:107)Z(cid:107)F â‰¤
it satisï¬es

âˆš

P{(cid:107)Z(cid:107)F â‰¥

dt} â‰¤ P {(cid:107)Z(cid:107)2 â‰¥ t} ,

âˆš

d(cid:107)Z(cid:107). Therefore,

which enables us to bound (cid:107)Z(cid:107)F via (cid:107)Z(cid:107)2 by Theorem A.2. To this end, for L deï¬ned in Theo-
rem A.2, we have (cid:107)Xi(cid:107) â‰¤ 1 then L = 1. For v(Z) we have (cid:107)E(Z(cid:62)Z)(cid:107) = (cid:13)
(cid:3)(cid:13)
(cid:13) =
(cid:13)
(cid:107)(cid:80)m
i=1 qId(cid:107) = qm, similarly (cid:107)E(ZZ(cid:62))(cid:107) = qm then it follows v(Z) = qm. This leads to (41).
Furthermore, by setting the RHS of (41) to be nâˆ’c for c > 0 and solve it we get (42).

E (cid:2)X (cid:62)

i Xi

(cid:80)m

i=1

In our analysis, we are interested in the case of m = Ïn, q = Î² log n/n for some Ï, Î² and n (cid:29) d.

Then (42) can be written as

(cid:107)Z(cid:107)F = (cid:112)2cÏÎ²

1
âˆš
d

(cid:18)(cid:114)

1 +

c
18ÏÎ²

+

(cid:114) c

(cid:19)

18ÏÎ²

log n â‰ˆ (cid:112)2cÏÎ² log n

(43)

where we use the approximation

â‰ˆ 1 since usually 18ÏÎ² (cid:29) c.

18ÏÎ²
In particular, when the transformation is a 2-dimensional rotation i.e. d = 2, we are able to

(cid:16)(cid:113)

1 + c

18ÏÎ² +

(cid:113) c

(cid:17)

obtain a result sharper than Theorem A.3 as following:

Theorem A.4. Under the setting of Theorem A.3 with d = 2, for a suï¬ƒciently large m

(cid:107)Z(cid:107)F â‰¤ (cid:112)cqm log n(1 + o(1))

1
âˆš
d

with probability 1 âˆ’ nâˆ’c for any c > 0.

Proof. To begin with, by deï¬nition of Xi in (40), we can rewrite Xi = riRi where ri is a Bernoulli
random variable such that P{ri = 1} = q. Moreover, when d = 2 each rotation matrix Ri can be

expressed as Ri =

for some Î¸i âˆˆ [0, 2Ï€). As a result,

(cid:18)cos Î¸i âˆ’ sin Î¸i
cos Î¸i

sin Î¸i

(cid:19)

1
d

(cid:107)Zi(cid:107)2

F =

(cid:32) m
(cid:88)

i=1

(cid:33)2

ri sin Î¸i

+

(cid:32) m
(cid:88)

i=1

(cid:33)2

ri cos Î¸i

=: xm.

We are going to bound 1âˆš
d
xm i.e. E[xk

(cid:107)Zi(cid:107)F in terms of xm. Our technique is to compute the k-th moment of

m] for some integer k â‰¤ m followed by Markovâ€™s inequality. First,

(cid:34) m
(cid:88)

E[xm] = E

m
(cid:88)

i=1

j=1

rirj (cos Î¸i cos Î¸j + sin Î¸i sin Î¸j)

=

(cid:35)

m
(cid:88)

m
(cid:88)

i=1

j=1

E [rirj] E [cos(Î¸i âˆ’ Î¸j)] .

20

Next, in general E[xk

m] is given as

(cid:105)

(cid:104)

E

xk
m

=

m
(cid:88)

m
(cid:88)

i1,...,ik=1

j1,...,jk=1

E [ri1rj1 Â· Â· Â· rik rjk ] E [cos(Î¸i1 âˆ’ Î¸j1) Â· Â· Â· cos(Î¸ik âˆ’ Î¸jk )] .

(44)

Let si âˆˆ {âˆ’1, 1} denotes a discrete variable for i = 1, . . . , k âˆ’ 1. Then applying the property
cos(Î±1) cos(Î±2) = 1
2 [cos(Î±1 + Î±2) + cos(Î±1 âˆ’ Î±2)] recursively for k âˆ’ 1 times yields

cos(Î¸i1 âˆ’ Î¸j1 ) Â· Â· Â· cos(Î¸ik âˆ’ Î¸jk ) =

1
2kâˆ’1

(cid:88)

cos (cid:0)(Î¸i1 âˆ’Î¸j1 )+s1(Î¸i2 âˆ’ Î¸j2 )+Â· Â· Â·+skâˆ’1(Î¸ik âˆ’Î¸jk )(cid:1) .

s1,...,skâˆ’1âˆˆ{âˆ’1,1}

Plugging this into (44) gives

(cid:104)

E

xk
m

(cid:105)

=

(cid:18) 1

(cid:19)

2kâˆ’1

(cid:88)

(cid:88)

(cid:88)

E (cid:2)ri1 rj1 Â· Â· Â· rik rjk

(cid:3) E (cid:2)cos (cid:0)Î¸i1 âˆ’Î¸j1 +Â· Â· Â·+Î´kâˆ’1(Î¸ik âˆ’Î¸jk )(cid:1)(cid:3)

s1,...,skâˆ’1âˆˆ{âˆ’1,1}

i1,...,ik

j1,...,jk

(a)
=

(cid:88)

(cid:88)

i1,...,ik

j1,...,jk

E (cid:2)ri1 rj1 Â· Â· Â· rik rjk

(cid:3) E (cid:2)cos (cid:0)Î¸i1 âˆ’ Î¸j1 + Î¸i2 âˆ’ Î¸j2 + Â· Â· Â· + Î¸ik âˆ’ Î¸jk

(cid:1)(cid:3) .

(45)

Here, (a) holds since (cid:80)
i1,...,ik

(cid:80)
j1,...,jk

E [cos (Î¸i1 âˆ’Î¸j1 +Â· Â· Â·+skâˆ’1(Î¸ik âˆ’Î¸jk ))] is identical for diï¬€erent choices

of {si}kâˆ’1
i=1 , and there are 2kâˆ’1 number of choices in total then the factor 1/2kâˆ’1 is cancelled by sum-
ming all of them, and we can focus on the case that s1 = s2 = Â· Â· Â· = skâˆ’1 = 1. To proceed, let us
denote Si = {i1, . . . , ik} and Sj = {j1, . . . , jk} as two multisets (a multiset is a variant of a set that
allows repeated elements) that contain all i and j respectively, then

E [cos (Î¸i1 âˆ’ Î¸j1 + Î¸i2 âˆ’ Î¸j2 + Â· Â· Â· + Î¸ik âˆ’ Î¸jk )] =

(cid:40)

1,
0,

Si = Sj,
otherwise.

This holds because the sum of the random angles equals 0 mod 2Ï€ if Si = Sj, otherwise it vanishes
by taking the expectation. Therefore, under Si = Sj, let ml denotes the multiplicity of element
l âˆˆ {1, . . . , m} that either Si or Sj contains, then it satisï¬es

E[ri1rj1 Â· Â· Â· rik rjk ]

(a)
= E[r2m1

1

r2m2
2

Â· Â· Â· r2mn

m ] =

m
(cid:89)

i=1

E[r2mi
i

]

(b)
= qÏ„ (m1,...mm)

where Ï„ (m1, . . . mm) denotes the number of non-zero elements in {m1, . . . , mm}, (a) holds because
of Si = Sj, and (b) comes from E[r2mi
] = q, âˆ€mi > 0. Given the above, in (45) the summation over
all i and j can be instead summing over m1, . . . , mm. That is,

i

E[xk

m] =

(cid:88)

Î (m1, . . . mm)qÏ„ (m1,...mm).

m1,...,mmâ‰¥0
m1+Â·Â·Â·+mm=k

(46)

Here, Î (m1, . . . mm) denotes the number of combinations of {i1, j1, . . . , ik, jk} that satisfy Si =
Sj with multiplicity {m1, . . . , mm}. To determine it, notice that given a ï¬xed {m1, . . . , mm}
there are (cid:0)
(cid:1) number of combinations for either Si or Sj that satisfy it. Then we have
Î (m1, . . . mm) = (cid:0)

. Plugging this into (46) gives

k
m1,m2,...,mm

(cid:1)2

k
m1,m2,...,mm

(cid:105)

(cid:104)

E

xk
m

=

(cid:88)

qÏ„ (m1,...mm)

m1,...,mmâ‰¥0
m1+Â·Â·Â·+mm=k

(cid:18)

k
m1, . . . , mm

(cid:19)2

=

k
(cid:88)

l=1

(cid:18)n
l

(cid:19)
ql (cid:88)

m1,...,mlâ‰¥1
m1+Â·Â·Â·+ml=k

(cid:18)

k
m1, . . . , ml

(cid:19)2

,

21

where the RHS holds by ï¬xing Ï„ (m1, . . . , mm) = l for l = 1, . . . , k. Furthermore,

(cid:105)

(cid:104)

E

xk
m

=

(cid:19)

(cid:18)n
k

qk (cid:88)

(cid:18)

k
m1, . . . , mk

(cid:19)2

+

kâˆ’1
(cid:88)

l=1

(cid:18)m
l

(cid:19)
ql (cid:88)

(cid:18)

k
m1, . . . , ml

(cid:19)2

m1,...,mlâ‰¥1
m1+Â·Â·Â·+ml=k

m1,...,mkâ‰¥1
m1+...+mk=k
(cid:124)

(cid:123)(cid:122)
=(k!)2 since m1 = Â· Â· Â· = mk = 1

(cid:125)

(cid:18)

=

m!
k!(m âˆ’ k)!

(cid:19)

â‰¤ c0k!qkmk

qk(k!)2 + O(qkâˆ’1mkâˆ’1) = k!qkmk + O(qkâˆ’1mkâˆ’1)

where the last inequality holds for any constant c0 > 1 with a suï¬ƒciently large m.

Now we are ready to bound xm by applying Markovâ€™s inequality as

P {xm â‰¥ t} = P

(cid:110)

m â‰¥ tk(cid:111)
xk

â‰¤ tâˆ’kE[xk

m] â‰¤ tâˆ’k Â· c0k!qkmk â‰¤

âˆš

(cid:18) (c0e

(cid:19)k

k)1/kkqm
et

where the last inequality uses the fact that k! â‰¤ ekk+ 1
âˆš

âˆš

2 eâˆ’k. If we set k = (cid:100)c log n(cid:101) and

t = (c0e

k)1/kkqm = e

1

k log(c0e

k)kqm = (1 + o(1))cqm log n,

then we have

P {xn â‰¥ t} â‰¤ n
By deï¬nition of xm, this further leads to 1âˆš
d
which completes the proof.

(cid:18)

1+log

âˆ’c

(cid:19)

t

âˆš

k)1/k kqm

= nâˆ’c.

(c0e
âˆš

(cid:107)Z(cid:107)F â‰¤

cqm log n(1 + o(1)) with probability 1 âˆ’ nâˆ’c,

1âˆš
d

Remark 1. When d = 2, we can interpret
(cid:107)Z(cid:107)F from a random walk perspective: Suppose a
random walk on a 2-D plane such that in each step, with probability q we take a unit-length step
towards a random direction uniformly drawn from [0, 2Ï€), and with probability 1 âˆ’ q we stay where
we are. As a result, 1âˆš
(cid:107)Z(cid:107)F can be interpreted as the distance that we travel within m steps where
d
ri indicates whether we move or not in the i-th step, and Î¸i stands for the corresponding direction.
In fact, such 2-D random walk with q = 1 has been originally studied by Rayleigh in [37, 36]. Let
pm(r) be the probability distribution of travelling a distance r with m steps, Rayleigh showed that
as m â†’ âˆ,

2r
m

eâˆ’r2/m.

pm(r) âˆ¼
âˆš

cm log n with probability 1 âˆ’ nâˆ’c for any c > 0. Im-
Therefore, asymptotically it satisï¬es r â‰¤
portantly, this bound agrees with the non-asymptotic result in Theorem A.4, which indicates the
sharpness of our result.

Lemma A.5. Let m1 = Ïn and m2 = (1 âˆ’ Ï)n with 0 < Ï < 1, and Z1, . . . , Zm1 âˆˆ RdÃ—d be
Xj is independently generated as in Theorem A.3. Let
i.i.d. random matrices where each Zi =

m2(cid:80)
j=1

(cid:15)i := (cid:107)Zi(cid:107)F
âˆš
d

m2

. Then, when p, q = O(log n/n)

m1(cid:88)

i=1

(cid:15)i â‰¤

2c
3

log n + Ï

(cid:114) qn
1 âˆ’ Ï

(cid:115)

+

2cqÏ log n
1 âˆ’ Ï

=

2c
3

log n + o(log n)

with probability 1 âˆ’ nâˆ’c.

22

Proof. By deï¬nition, each (cid:15)i is bounded by 1, then applying Bernsteinâ€™s inequality [47, Theorem
2.8.4] yields

(cid:40) m1(cid:88)

P

((cid:15)i âˆ’ E [(cid:15)i]) â‰¥ t

(cid:41)

(cid:18)

â‰¤ exp

âˆ’

(cid:19)

t2/2
Ïƒ2 + Kt/3

where K = 1 is the boundary, and

i=1

Ïƒ2 =

m1(cid:88)

i=1

(cid:0)E[(cid:15)2

i ] âˆ’ E[(cid:15)i]2(cid:1) â‰¤

m1(cid:88)

i=1

E[(cid:15)2

i ] =

m1(cid:88)

i=1

(cid:3)

E (cid:2)(cid:107)Zi(cid:107)2
m2
2d

F

=

m1(cid:88)

i=1

q
m2

=

qÏ
1 âˆ’ Ï

is the variance of the sum. Then, by letting the RHS of (47) equal to nâˆ’c for c > 0, we obtain

m1(cid:88)

i=1

((cid:15)i âˆ’ E [(cid:15)i]) â‰¤

Kc
3

log n +

(cid:115)(cid:18) Kc
3

(cid:19)2

log n

+ 2Ïƒ2c log n

â‰¤

2Kc
3

log n +

(cid:112)

2Ïƒ2c log n

with probability 1 âˆ’ nâˆ’c. Also, by Jensenâ€™s inequality the expectation of (cid:15)i is bounded as

E[(cid:15)i] =

1
âˆš
m2

d

E

(cid:20)(cid:113)

Tr(Z(cid:62)

i Zi)

(cid:21)

â‰¤

1
âˆš
m2

d

(cid:113)

E (cid:2)Tr(Z(cid:62)

i Zi)(cid:3) =

(cid:114) q

(1 âˆ’ Ï)n

.

(47)

(48)

Plugging this into (48) completes the proof.

A.3 Operator norm of random (block) matrices

The following existing result provides an upper bound on the spectrum of an ErdÂ¨osâ€“RÂ´enyi random
graph.

Theorem A.6 ([19, Theorem 5]). Let E âˆˆ RnÃ—n denotes the adjacency matrix of a graph generated
from the ErdÂ¨osâ€“RÂ´enyi model G(n, p), where the entries {Eij : i < j} are independent and {0, 1}-
valued. Assume that E[Eij] â‰¤ p where p = â„¦(c0 log n/n). Then for any c > 0, there exists c1 > 0
such that,

with probability 1 âˆ’ nâˆ’c.

(cid:107)E âˆ’ E[E](cid:107) â‰¤ c1

âˆš

np

Another important result is the bound the spectrum of a random block matrix with i.i.d random
blocks. That is, let S âˆˆ Rm1dÃ—m2d be an m1 Ã—m2 block random matrix where each block Sij âˆˆ RdÃ—d
is i.i.d. such that

Sij =

(cid:40)

Rij,
0,

with probability q,
otherwise.

(49)

Here, Rij is a random rotation matrix uniformly drawn from SO(d). Then we have

Theorem A.7. Let S âˆˆ Rm1dÃ—m2d be an m1 Ã— m2 random block matrix with i.i.d block Sij deï¬ned
in (49). Let m1, m2 = O(n). Then, for any c > 0, there exists c1, c2 > 0 such that
âˆš

âˆš

(cid:107)S(cid:107) â‰¤ c1(

qm1 +

qm2) + c2

(cid:112)log n.

with probability 1 âˆ’ nâˆ’c.

23

Proof. Our technique is based on the moment method which is broadly used in random matrix
theory (see e.g. [3, 43]). We start from bounding (cid:107)S(cid:107) by the trace of SS(cid:62) as

(cid:104)

(cid:107)S(cid:107)2k(cid:105)

E

= E

(cid:104)

(cid:107)(SS(cid:62))k(cid:107)

(cid:105)

â‰¤ E

(cid:104)
Tr

(cid:16)

(SS(cid:62))k(cid:17)(cid:105)

which holds for any k âˆˆ N. Then, E (cid:2)Tr (cid:0)(SS(cid:62))k(cid:1)(cid:3) can be expanded as

(cid:104)

E

Tr

(cid:16)

(SS(cid:62))k(cid:17)(cid:105)

=

(cid:88)

(cid:88)

(cid:104)

E

Tr

(cid:16)

i1,...,ikâˆˆ[m1]

j1,...,jkâˆˆ[m2]

Si1j1S(cid:62)

i2j1Si2j2S(cid:62)

i3j2 Â· Â· Â· Sikjk S(cid:62)
i1jk

(cid:17)(cid:105)

(50)

where [m1] := {1, . . . , m1} and [m2] := {1, . . . , m2}. Let G(U, V ) be a complete bipartite graph
with the set of nodes U = [m1] and V = [m2]. Then each Sij for i âˆˆ U, j âˆˆ V is associated with an
edge (i, j) on the graph G. Furthermore, the matrix product Si1j1S(cid:62)
can be treated
i2j1
as a walk that goes back and forth along G and follows the path

Â· Â· Â· Sikjk S(cid:62)
i1jk

i1 â†’ j1 â†’ i2 â†’ j2 â†’ Â· Â· Â· â†’ ik â†’ jk â†’ i1

which starts and ends at i1 as a cycle with 2k steps. With this in mind, let (i1, j1, . . . , ik, jk) denote
this walk, since E[Sk
ij] = 0 when k is odd, the sum in (50) can be restricted to even cycles on the
graph G where each distinct edge traversed by the walk should be visited by an even number of
times. Therefore, let W denote the set of all such walks, then
(cid:88)

(cid:17)(cid:105)

(cid:16)

(cid:16)

(cid:104)

E

Tr

(SS(cid:62))k(cid:17)(cid:105)

=

Si1j1S(cid:62)

i2j1Si2j2S(cid:62)

i3j2 Â· Â· Â· Sikjk S(cid:62)
i1jk

(cid:104)
Tr

E

(i1,j1,...,ik,jk)âˆˆW
(cid:88)

(a)
=

(cid:16)

(cid:104)
Tr

E

Ri1j1R(cid:62)

i2j1Â· Â· Â·Rikjk R(cid:62)
i1jk

(cid:17)(cid:105)

E[ri1j1ri2j1Â· Â· Â·rikjk ri1jk ]

(i1,j1,...,ik,jk)âˆˆW

(b)
â‰¤ d

(cid:88)

(i1,j1,...,ik,jk)âˆˆW

E[ri1j1ri2j1 Â· Â· Â· rikjk ri1jk ],

(51)

where (a) holds by rewriting Sij = rijRij for some Bernoulli random variable rij such that rij = 1
with probability q and rij = 0 otherwise, (b) holds because the product Ri1j1 Â· Â· Â· Ri1jk is a rotation
matrix, therefore (cid:107)Ri1j1R(cid:62)
(cid:107) = 1 and

i2j1

Â· Â· Â· Rikjk R(cid:62)
(cid:16)

(cid:104)

i1jk
Ri1j1R(cid:62)

Tr

E

i2j1 Â· Â· Â· Rikjk R(cid:62)
i1jk

(cid:17)(cid:105)

â‰¤ d.

(52)

To proceed, we introduce a series of i.i.d. symmetric random variables tij as
ï£±
ï£´ï£²

tij =

with probability q
2 ,
1,
âˆ’1, with probability q
2 ,
0,

with probability 1 âˆ’ q.

ï£´ï£³

for i âˆˆ [m1], j âˆˆ [m2]. As a result, tij has symmetric distribution such that E[tij]2kâˆ’1 = 0 and
E[tij]2k = q, âˆ€k âˆˆ N. Moreover, the summation in (51) satisï¬es

(cid:88)

E[ri1j1ri2j1 Â· Â· Â· rikjk ri1jk ] =

(cid:88)

E[ti1j1ti2j1 Â· Â· Â· tikjk ti1jk ]

(i1,j1,...,ik,jk)âˆˆW

(i1,j1,...,ik,jk)âˆˆW

(cid:88)

(cid:88)

(a)
=

E[ti1j1ti2j1 Â· Â· Â· tikjk ti1jk ]

i1,...,ikâˆˆ[m1]
(cid:16)

(cid:104)

= E

Tr

j1,...,jkâˆˆ[m2]

(T T (cid:62))k(cid:17)(cid:105)
(cid:107)T (cid:107)2k(cid:105)
(cid:104)

â‰¤ min{m1, m2}E

24

.

(53)

Here, T âˆˆ Rm1Ã—m2 is the random matrix whose (i, j)-th entry is tij, the equality (a) holds since any
walk (i1, j1, . . . , ik, jk) /âˆˆ W that visits some edge by odd times vanishes. Therefore, (53) enables us
to bound E (cid:2)Tr (cid:0)(SS(cid:62))k(cid:1)(cid:3) via E (cid:2)(cid:107)T (cid:107)2k(cid:3). To this end, by computing the following quantities:

Ïƒk,1 =

Ïƒk,2 =

(cid:32) m1(cid:88)

(cid:32) m2(cid:88)

i=1

j=1

(cid:32) m2(cid:88)

(cid:32) m1(cid:88)

j=1

i=1

E (cid:2)t2
ij

(cid:3)

E (cid:2)t2
ij

(cid:3)

(cid:33)k(cid:33)1/2k

(cid:33)k(cid:33)1/2k

= m1/2k
1

âˆš

m2q,

= m1/2k
2

âˆš

m1q,

Ïƒâˆ—
k =

(cid:32) m1(cid:88)

m2(cid:88)

i=1

j=1

(cid:33)1/2k

(cid:107)tij(cid:107)2k
âˆ

= (m1m2)1/2k.

Then applying [23, Theorem 4.9] yields the following bound on E (cid:2)(cid:107)T (cid:107)2k(cid:3):

(cid:107)T (cid:107)2k(cid:105)1/2k
(cid:104)

E

â‰¤ Ïƒk,1 + Ïƒk,2 + C
âˆš

â‰¤ n1/2k (

m1q +

âˆš

âˆš

kÏƒâˆ—

k â‰¤ m1/2k
âˆš

2

m2q) + C

kn1/k

âˆš

m1q + m1/2k

1

âˆš

m2q + C

âˆš

k(m1m2)1/2k

for some universal constant C > 0. Furthermore, by setting k = (cid:100)Î³ log n(cid:101) for some Î³ > 0,

(cid:107)T (cid:107)2k(cid:105)1/2k
(cid:104)

E

â‰¤ e1/2Î³ (

âˆš

m1q +

âˆš

m2q) + C(cid:48)(cid:112)log n

for some C(cid:48) > 0. Finally, by putting all the results together and using Markov inequality,

P{(cid:107)S(cid:107) â‰¥ t} = P{(cid:107)S(cid:107)2k â‰¥ t2k} â‰¤ tâˆ’2kE

(cid:16)

(cid:104)
Tr

(cid:32)

â‰¤ d min{m1, m2}

âˆš

e1/2Î³(

m1q +

(cid:33)2(cid:100)Î³ log n(cid:101)

log n

,

(SS(cid:62))k(cid:17)(cid:105)
âˆš

m2q) + C(cid:48)âˆš
t

for any t > 0. By setting Î³ = c
âˆš
t = c1(

m2q) + c2

m1q +

âˆš

âˆš

2 + 1, for any c > 0, we can identify some c1, c2 > 0 such that

log n and P{(cid:107)S(cid:107) â‰¥ t} â‰¤ nâˆ’c, which completes the proof.

Remark 2. It is worth noting that our result in Theorem A.7 is sharper than either the one by
using (cid:15)-net argument (e.g. [47, Theorem 4.6.1]) or the one by applying matrix Bernstein (e.g. [27,
Lemma 5.14]). If using (cid:15)-net argument, we get (cid:107)S(cid:107) â‰¤ c1
log n with high probability
âˆš
d in the leading term compared to our
for some c1, c2 > 0, which has an additional factor of
(cid:112)q max{m1, m2} (log nd + c2 log n) with high
result. If using matrix Bernstein, we obtain (cid:107)S(cid:107) â‰¤ c1
log nd compared to our result. These
probability for some c1, c2 > 0, which is loose by a factor
bounds are not adequate enough in our analysis and thus it is necessary to have a sharp result as
in Theorem A.7.

qnd + c2

âˆš

âˆš

âˆš

Proof of Lemma 3.6. Let us deï¬ne

(cid:32)

A =

(cid:33)

(cid:101)A11
(cid:101)A(cid:62)
12

(cid:101)A12
(cid:101)A22

, E [A] âˆ’ A =:

(cid:19)

(cid:18)S11 S12
S(cid:62)
12 S22

, Sin :=

(cid:19)

(cid:18)S11

0
0 S22

, Sout :=

(cid:18) 0 S12
S(cid:62)
0
12

(cid:19)

,

where the two diagonal blocks represent the two clusters, then by triangle inequality (cid:107)E[A] âˆ’ A(cid:107) =
(cid:107)Sin + Sout(cid:107) â‰¤ (cid:107)Sin(cid:107) + (cid:107)Sout(cid:107). Moreover, notice (cid:107)Sin(cid:107) = max {(cid:107)S11(cid:107), (cid:107)S22(cid:107)} â‰¤ (cid:107)S11(cid:107) + (cid:107)S22(cid:107) and
(cid:107)Sout(cid:107) = (cid:107)S12(cid:107), it holds

(cid:107)E[A] âˆ’ A(cid:107) â‰¤ (cid:107)S11(cid:107) + (cid:107)S22(cid:107) + (cid:107)S12(cid:107).

25

As a result, we can bound S11, S22 and S12 them separately. For S11, by deï¬nition S11 = E[ (cid:101)A11] âˆ’
(cid:101)A11. Then if we deï¬ne E1 âˆˆ Rm1Ã—m1 be the adjacency matrix of the subgraph on C1 which is
generated from the ErdÂ¨osâ€“RÂ´enyi model G(m1, p), (cid:101)A11 can be expressed as (cid:101)A11 = E1 âŠ— Id under the
assumption Ri = Id for i = 1, . . . , n, where âŠ— denotes the tensor product. Therefore, we have

âˆš

(cid:107)S11(cid:107) = (cid:107)E[ (cid:101)A11] âˆ’ (cid:101)A11(cid:107) = (cid:107)(E[E1] âˆ’ E1) âŠ— Id(cid:107) = (cid:107)E[E1] âˆ’ E1(cid:107).
This enables us to study (cid:107)S11(cid:107) via (cid:107)E[E1] âˆ’ E1(cid:107), which is bounded by Theorem A.6 as (cid:107)S11(cid:107) â‰¤
pm1 with probability 1 âˆ’ nâˆ’c for some c1, c > 0. (cid:107)S22(cid:107) is bounded in a similar manner and we
c1
do not repeat. For (cid:107)S12(cid:107), we apply Theorem A.7 and get (cid:107)S12(cid:107) â‰¤ c2(
log n)
with probability 1 âˆ’ nâˆ’c for some c2, c > 0. Combining these completes the proof.

qm2) + O(

qm1 +

âˆš

âˆš

âˆš

B Proof of main results

This section is devoted to the proofs of theorems in Section 3. In Sections B.1 and B.2 we provide
detailed proofs for the lemmas and theorems presented in Sections 3.1.1 and 3.2.1 respectively. In
Section B.3 we consider the case of two clusters with unknown cluster sizes.

B.1 Two equal-sized clusters with known cluster sizes

Proof of Lemma 3.2. The optimality is immediately established since (9) is convex then KKT con-
ditions are suï¬ƒcient for M âˆ— being optimal [8]. For the uniqueness, suppose there exists another
optimal solution (cid:102)M (cid:54)= M âˆ—, then the following holds:

(a)
= (cid:104)A, (cid:102)M âˆ’ M âˆ—(cid:105)

0

i=1), (cid:102)M âˆ’ M âˆ—(cid:105) + (cid:104)Î˜ + Î˜(cid:62), (cid:102)M âˆ’ M âˆ—(cid:105)

(b)
= âˆ’(cid:104)Î›, (cid:102)M âˆ’ M âˆ—(cid:105) âˆ’ (cid:104)diag({Zi}n
(c)
= âˆ’(cid:104)Î›, (cid:102)M âˆ’ M âˆ—(cid:105) + (cid:104)Î˜ + Î˜(cid:62), (cid:102)M âˆ’ M âˆ—(cid:105)
(d)
= âˆ’(cid:104)Î›, (cid:102)M (cid:105) + (cid:104)Î˜ + Î˜(cid:62), (cid:102)M âˆ’ M âˆ—(cid:105)
(e)
= âˆ’(cid:104)Î›, (cid:102)M (cid:105) + 2(cid:104)Î˜, (cid:102)M âˆ’ M âˆ—(cid:105),

(54)

where (a) holds since both (cid:102)M and M âˆ— are optimal and they should share the same primal value, i.e.,
(cid:104)A, (cid:102)M (cid:105) = (cid:104)A, M âˆ—(cid:105); (b) follows from (11); (c) comes from the constraint in (9) that M âˆ—
ii = (cid:102)Mii = Id,
for i = 1, . . . , n; (d) uses (cid:104)Î›, M âˆ—(cid:105) = 0 in (13); and (e) holds because both (cid:102)M and M âˆ— are
symmetric. To proceed, let us rewrite

(cid:104)Î˜, (cid:102)M âˆ’ M âˆ—(cid:105) = (cid:104)Î˜, (cid:102)M (cid:105) âˆ’ (cid:104)Î˜, M âˆ—(cid:105) =

(cid:88)

(cid:104)Î˜ij, (cid:102)Mij(cid:105) âˆ’

i,j

(cid:104)Î˜ij, M âˆ—

ij(cid:105).

(cid:88)

i,j

(55)

Then by plugging the explicit expression of Î˜ij in (12) into (55) we obtain
Âµiâˆš
d

(cid:104)Î˜ij, M âˆ—
ij(cid:105)

ij, M âˆ—

ij(cid:105) =

(cid:104)M âˆ—

Âµim

(a)
=

(cid:88)

(cid:88)

(cid:88)

(cid:88)

âˆš

d,

i

(cid:104)Î˜ij, (cid:102)Mij(cid:105)

(cid:88)

(b)
=

Âµi

i

j:C(j)=C(i)
(cid:18) (cid:88)

i,j

(cid:88)

i,j

i

j:C(j)=C(i)

(c)
â‰¤

(cid:88)

Âµi

(cid:18) (cid:88)

i

j:C(j)=C(i)

1
âˆš
d

1
âˆš
d

(cid:104)M âˆ—

ij, (cid:102)Mij(cid:105) +

(cid:88)

(cid:19)

(cid:104)Î±ij, (cid:102)Mij(cid:105)

j:C(j)(cid:54)=C(i)

(cid:107)M âˆ—

ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F +

(cid:88)

(cid:107)Î±ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F

j:C(j)(cid:54)=C(i)

(cid:19)

(56)

(d)
â‰¤

(cid:88)

Âµi

(cid:88)

(cid:107) (cid:102)Mij(cid:107)F

(e)
â‰¤

(cid:88)

âˆš

d

Âµim

i

j

i

26

where both (a) and (b) hold because M âˆ—
ij = 0, C(i) (cid:54)= C(j); (c) follows from Cauchy-Schwartz
inequality; (d) uses (cid:107)Î±ij(cid:107)F â‰¤ 1 in (12); (e) comes from (cid:80)
d, for i = 1, . . . , n in (9).
As a result, combining (55) and (56) yields (cid:104)Î˜, (cid:102)M âˆ’ M âˆ—(cid:105) â‰¤ 0. Plugging this back into (54) gives
(cid:104)Î›, (cid:102)M (cid:105) â‰¤ 0. On the other hand, from Î› (cid:23) 0 in (14) and (cid:102)M (cid:23) 0 in (9) we have (cid:104)Î›, (cid:102)M (cid:105) â‰¥ 0. Then
it holds that (cid:104)Î›, (cid:102)M (cid:105) = 0. With this in mind, combining the assumption N (Î›) = R(M âˆ—) with
M âˆ— = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) in (4), (cid:102)M satisï¬es

j (cid:107) (cid:102)Mij(cid:107)F â‰¤ m

âˆš

(cid:102)M = V (1)Î£1(V (1))(cid:62) + V (2)Î£2(V (2))(cid:62)

for some dÃ—d diagonal matrices Î£1, Î£2 (cid:23) 0. Furthermore, from the constraint in (9) that (cid:102)Mii = Id,
we obtain Î£1 = Î£2 = Id, then (cid:102)M = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) = M âˆ— which contradicts with the
assumption (cid:102)M (cid:54)= M âˆ—. Therefore, M âˆ— is the unique solution.

Proof of Lemma 3.3. Recall the assumption Ri = Id, i = 1, . . . , n. From (11) we get Î› = âˆ’A âˆ’
diag ({Zi}n

i=1) + Î˜ + Î˜(cid:62), plugging this into (15) yields

(cid:88)

j:C(j)=C(i)
(cid:88)

j:C(j)(cid:54)=C(i)

Î˜ij + Î˜(cid:62)

ji âˆ’ Aij = Zi,

i = 1, . . . , n

Î˜ij + Î˜(cid:62)

ji âˆ’ Aij = 0,

i = 1, . . . , n.

To determine Î˜ij, notice that (12) can be written as

Î˜ij =

(cid:40)

âˆš

d,

ij/

ÂµiM âˆ—
C(i) = C(j),
ÂµiÎ±ij, (cid:107)Î±ij(cid:107)F â‰¤ 1, C(i) (cid:54)= C(j),

(57)

(58)

(59)

where we use the fact M âˆ—
C(i) (cid:54)= C(j). Importantly, our ansatz of Î˜ij is of the following form:

ij (cid:54)= 0 if and only if C(i) = C(j). Then let us consider the case when

(60)

(61)

(62)

for some (cid:101)Î±i. To solve it, plugging (60) into (58) yields
(cid:88)

(cid:88)

Î˜ij = (cid:101)Î±i,

âˆ€j : C(j) (cid:54)= C(i),

(cid:101)Î±i =

1
m

Aij âˆ’

1
m

Ajs âˆ’

1
m

jâˆˆC2
(cid:88)

sâˆˆC1

jâˆˆC2
(cid:88)

sâˆˆC1

(cid:101)Î±(cid:62)
j ,

(cid:101)Î±(cid:62)
s ,

i âˆˆ C1,

j âˆˆ C2.

(cid:101)Î±j =

1
m

By combining (61) and (62) we get

(cid:101)Î±i âˆ’

(cid:101)Î±j âˆ’

1
m

1
m

(cid:88)

sâˆˆC1
(cid:88)

sâˆˆC2

(cid:101)Î±s =

(cid:101)Î±s =

1
m

1
m

(cid:88)

jâˆˆC2
(cid:88)

iâˆˆC1

Aij âˆ’

Aji âˆ’

1
m2

1
m2

(cid:88)

(cid:88)

sâˆˆC1
(cid:88)

jâˆˆC2
(cid:88)

sâˆˆC2

iâˆˆC1

Asj,

i âˆˆ C1,

Asi,

j âˆˆ C2.

Then it is natural to guess (cid:101)Î±i for i âˆˆ C1 and (cid:101)Î±j for j âˆˆ C2 has the form

(cid:101)Î±i =

1
m

(cid:88)

sâˆˆC2

Ais âˆ’ Î±(1),

i âˆˆ C1,

(cid:101)Î±j =

1
m

(cid:88)

sâˆˆC2

Ajs âˆ’ Î±(2),

j âˆˆ C2

27

for some Î±(1) and Î±(2). By further plugging this into (61) or (62) we obtain

Î±(1) + (Î±(2))(cid:62) =

1
m2

(cid:88)

(cid:88)

As1s2.

s1âˆˆC1

s2âˆˆC2

(cid:80)
By symmetry we guess Î±(1) = (Î±(2))(cid:62) = 1
As1s2, which leads to our guess of
2m2
(cid:101)Î±i given in Lemma 3.3. Now it remains to determine Âµi: From (59) and (60) we see that when
C(i) (cid:54)= C(j), (cid:107)Î˜ij(cid:107)F = Âµi(cid:107)Î±ij(cid:107)F = (cid:107) (cid:101)Î±i(cid:107)F. Also, due to the constraint in (12) that (cid:107)Î±ij(cid:107)F â‰¤ 1, Âµi
should satisfy Âµi â‰¥ (cid:107) (cid:101)Î±i(cid:107)F. Then we guess Âµi = (cid:107) (cid:101)Î±i(cid:107)F. As we shall see in Lemma 3.4, this is to
make Âµi as small as possible such that the eigenvalues of Î› are greater and the condition Î› (cid:23) 0
beneï¬ts from it. To complete the proof, plugging Î˜ij in (69) with M âˆ—
ij = Id into (57) yields the
form of Z.

s1âˆˆC1

s2âˆˆC2

(cid:80)

Proof of Lemma 3.4. Let us rewrite A, Î› and Î˜ into four blocks as
(cid:32)

(cid:33)

(cid:32)

A =

(cid:101)A11
(cid:101)A(cid:62)
12

(cid:101)A12
(cid:101)A22

, Î› =

, Î˜ =

(cid:19)

(cid:18)Î›11 Î›12
Î›21 Î›22

(cid:33)

(cid:101)Î˜11
(cid:101)Î˜21

(cid:101)Î˜12
(cid:101)Î˜22

(63)

where the two diagonal blocks represent C1 and C2 respectively. By assumption the two diagonal
i=1) âˆˆ RmdÃ—md and
blocks of M âˆ— in (4) satisfy (cid:102)M âˆ—
i=m+1) âˆˆ RmdÃ—md be two diagonal block matrices whose diagonal blocks are ÂµiId
(cid:101)Âµ2 = diag({ÂµiId}n
for i âˆˆ C1 and i âˆˆ C2 respectively. Then from Lemma 3.3 each block of Î˜ can be expressed as

m) âŠ— Id. Let (cid:101)Âµ1 = diag({ÂµiId}m

2 = (1m1(cid:62)

1 = (cid:102)M âˆ—

(cid:101)Î˜11 = (cid:101)Âµ1 (cid:102)M âˆ—
1âˆš
d

,

1
2m2 (cid:102)M âˆ—
Given the above, from Lemma 3.3, each block of Î› can be expressed as

2 (cid:101)A21 (cid:102)M âˆ—
1 ,

(cid:101)A21 (cid:102)M âˆ—

(cid:101)Î˜21 =

1
m

1 âˆ’

.

(cid:101)A12 (cid:102)M âˆ—

2 âˆ’

1
2m2 (cid:102)M âˆ—

1 (cid:101)A12 (cid:102)M âˆ—
2 ,

(cid:101)Î˜12 =

1
m
(cid:101)Î˜22 = (cid:101)Âµ2 (cid:102)M âˆ—
2âˆš
d

(cid:101)Î›12 = âˆ’ (cid:101)A12 + (cid:101)Î˜12 + (cid:101)Î˜(cid:62)

21 = âˆ’

(cid:101)Î›21 = âˆ’ (cid:101)A21 + (cid:101)Î˜21 + (cid:101)Î˜(cid:62)

12 = âˆ’

(cid:18)

Imd âˆ’

(cid:18)

Imd âˆ’

(cid:19)

(cid:19)

1
m
1
m

(cid:102)M âˆ—
1

(cid:102)M âˆ—
2

(cid:18)

(cid:101)A12

Imd âˆ’

(cid:18)

(cid:101)A21

Imd âˆ’

(cid:19)

(cid:19)

,

,

(cid:102)M âˆ—
2

(cid:102)M âˆ—
1

1
m
1
m

(cid:101)Î›11 = âˆ’ (cid:101)A11 âˆ’ diag({Zi}m
(cid:18)(cid:26)(cid:18) mÂµiâˆš
d

= âˆ’ (cid:101)A11 âˆ’ diag

i=1) +

1
âˆš

1 +

d (cid:101)Âµ1 (cid:102)M âˆ—
(cid:18) Âµsâˆš
d

(cid:88)

+

sâˆˆC1

1
(cid:102)M âˆ—
âˆš
d
(cid:19)(cid:19)

1 (cid:101)Âµ(cid:62)
(cid:27)m

1

(cid:19)

âˆ’ ris

Id

(cid:18)

=

Imd âˆ’

(cid:18)

(cid:101)Î›22 =

Imd âˆ’

(cid:19) (cid:18)

(cid:19) (cid:18)

1
m
1
m

(cid:102)M âˆ—
1

(cid:102)M âˆ—
2

âˆ’ (cid:101)A11 âˆ’ diag ({Zi}m

i=1)

âˆ’ (cid:101)A22 âˆ’ diag (cid:0){Zi}n

i=m+1

(cid:1)

Imd âˆ’

(cid:19)

.

(cid:102)M âˆ—
2

1
m

1
âˆš

d (cid:101)Âµ1 (cid:102)M âˆ—

1 +

1
âˆš
d

(cid:102)M âˆ—

1 (cid:101)Âµ(cid:62)

1

(cid:19)

,

(cid:102)M âˆ—
1

+

1
m

i=1

(cid:19) (cid:18)

Imd âˆ’

(cid:19) (cid:18)

Then, Î› can be simply expressed as

Î› = (Ind âˆ’ Î ) (âˆ’Z âˆ’ A) (Ind âˆ’ Î ).

(64)

To proceed, notice that the expectation of A is given as

E[A] =

(cid:32)E[ (cid:101)A11] E[ (cid:101)A12]
E[ (cid:101)A21] E[ (cid:101)A22]

(cid:33)

(cid:32)

=

p (cid:102)M âˆ—

1 âˆ’ pImd

0

0

p (cid:102)M âˆ—

2 âˆ’ pImd

(cid:33)

(cid:32)

=

p (cid:102)M âˆ—
1
0

(cid:33)

0
p (cid:102)M âˆ—
2

âˆ’ pInd,

28

where pInd comes from the convention Aii = 0, i = 1, . . . , n. Then one can see that

(Ind âˆ’ Î )(E[A] + pInd)(Ind âˆ’ Î ) = 0.

(65)

Therefore, by plugging (65) into (64) we obtain Î› = (Ind âˆ’ Î ) (cid:101)Î›(Ind âˆ’ Î ) with (cid:101)Î› deï¬ned in
Lemma 3.3. This yields two observations: (1) Î› (cid:23) 0 is satisï¬ed if (cid:101)Î› (cid:23) 0; (2) Ind âˆ’ Î  is a projection
matrix such that N (Ind âˆ’ Î ) = R(M âˆ—), which implies N (Î›) âŠ‡ R(M âˆ—). When (cid:101)Î› (cid:31) 0, for all
x /âˆˆ R(M âˆ—), Î›x (cid:54)= 0 since x /âˆˆ N (Ind âˆ’ Î ). Combining it with the fact that N (Î›) âŠ‡ R(M âˆ—), we
have N (Î›) = R(M âˆ—) if (cid:101)Î› (cid:31) 0. As a result, (cid:101)Î› (cid:31) 0 ensures both Î› (cid:23) 0 and N (Î›) = R(M âˆ—) are
satisï¬ed.

Proof of Lemma 3.5. According to Lemma 3.4, notice that Z is a diagonal matrix, then Î»min(pInd âˆ’
Z) turns out to be the smallest diagonal entry of pInd âˆ’ Z. This leads to

Î»min(pInd âˆ’ Z) = min

i

(cid:0)xi âˆ’ yi âˆ’ (cid:15)C(i)

(cid:1) + p â‰¥ min
i

xi âˆ’ max

i

yi âˆ’ max{(cid:15)1, (cid:15)2} + p.

with xi, yi, (cid:15)1, (cid:15)2 deï¬ned in Lemma 3.5, and we can bound them separately.

For xi, recall that ris is a Bernoulli random variable with P{ris = 1} = p. Then xi âˆ¼
Binom (m, p) that follows a binomial distribution. Applying the existing tail bound in Lemma A.1
with Ï = 1/2 and the union bound yields the result for mini xi.

For yi, by deï¬nition

yi =

â‰¤

m(cid:107) (cid:101)Î±i(cid:107)Fâˆš
d
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
âˆš
d

=

m
âˆš
d

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:88)

Aij

(cid:124)

j:C(j)(cid:54)=C(i)
(cid:123)(cid:122)
=:yi1

(cid:88)

+

j:C(j)(cid:54)=C(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

1
âˆš
2m

(cid:124)

Aij âˆ’

1
2m2

(cid:88)

(cid:88)

j:C(j)(cid:54)=C(i)

s:C(s)=C(i)

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

d

(cid:88)

(cid:88)

s:C(s)=C(i)

j:C(j)(cid:54)=C(i)
(cid:123)(cid:122)
=:yi2

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

= yi1 + yi2.

As a result, both yi1 and yi2 consist of (cid:107) (cid:80)
i Xi(cid:107) where each Xi âˆˆ RdÃ—d is i.i.d. and uniformly
distributed in SO(d). This can be bounded by matrix concentration inequalities such as matrix
Bernstein [45] in general. Therefore, by applying Theorem A.3 we obtain

yi1 â‰¤ (cid:112)2qm(c log n + log 2d)

(cid:32)(cid:115)

1 +

c log n + log 2d
18qm

+

(cid:115)

(cid:33)

c log n + log 2d
18qm

(66)

(a)

â‰ˆ (cid:112)cÎ² log n

with probability 1 âˆ’ nâˆ’c for any c > 0, where (a) comes from (43). Similarly, by applying The-
orem A.3 on yi2 we get yi2 = o(log n) with high probability. Combining the results above and
applying the union bound yields the result for maxi yi.

For (cid:15)1 and (cid:15)2, by symmetry they should have the same statistics, so let us focus on (cid:15)1, which

satisï¬es

(cid:15)1 =

1
âˆš
d

(cid:88)

sâˆˆC1

Âµs =

1
âˆš
d

(cid:88)

sâˆˆC1

(cid:107) (cid:101)Î±s(cid:107)F â‰¤

1
âˆš
m
(cid:124)

d

= (cid:15)(1)

1 + (cid:15)(2)
1 .

(cid:88)

(cid:88)

sâˆˆC1

jâˆˆC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:123)(cid:122)
=:(cid:15)(1)
1

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

+

1
âˆš
2m
(cid:124)

d

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

Asj

sâˆˆC1

jâˆˆC2
(cid:123)(cid:122)
=:(cid:15)(2)
1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

29

Then, by applying Lemma A.5 on (cid:15)(1)
1 = (3c/2) log n + o(log n) and
(cid:15)(2)
1 = yi2 = o(log n) with probability 1 âˆ’ nâˆ’c, this leads to the bound on (cid:15)1 and (cid:15)2. By further
applying the union bound we get the result on max{(cid:15)1, (cid:15)2}.

1 with m1 = m2 = m we get (cid:15)(1)

Speciï¬cally when d = 2, a tighter bound on yi is obtained by applying Theorem A.4 instead of
Theorem A.3 for bounding yi1 and yi2, which yields yi1 â‰¤ (cid:112)cÎ²/2 log n + o(log n) and yi2 = o(log n)
with probability 1 âˆ’ nâˆ’c. The remaining is the save as above.

B.2 Two unequal-sized clusters with known cluster sizes

Proof of Lemma 3.8. The proof is very similar to the one for Lemma 3.2, with a tiny diï¬€erence on
evaluating (cid:104)Î˜, (cid:102)M âˆ’ M âˆ—
ij(cid:105) in (56), where in this case Âµi is replaced by Âµi + Î½. Therefore we do not
repeat.

Proof of Lemma 3.9. First, we obtain Î› = âˆ’A âˆ’ diag({Zi}n
this into (27) yields

i=1) + Î˜ + Î˜(cid:62) from (22), then plugging

(cid:88)

j:C(j)=C(i)
(cid:88)

j:C(j)(cid:54)=C(i)

Î˜ij + Î˜(cid:62)

ji âˆ’ Aij = Zi,

i = 1, . . . , n

Î˜ij + Î˜(cid:62)

ji âˆ’ Aij = 0,

i = 1, . . . , n.

Next, to determine Î˜ij, (23) can be rewritten as

(cid:40)

Î˜ij =

C(i) = C(j),
(Âµi + Î½)Id/
(Âµi + Î½)Î±ij, (cid:107)Î±ij(cid:107)F â‰¤ 1, C(i) (cid:54)= C(j).

d,

âˆš

(67)

(68)

(69)

Then one can check that our guess of Î˜ij in Lemma 3.9 indeed satisï¬es (68) and (69). It remains
to determine Î½ and Âµi for i = 1, . . . , n: From (25) we get Âµi = 0, âˆ€i âˆˆ C2, since m1 > m2. Then
from (69), due to the constraint (cid:107)Î±ij(cid:107)F â‰¤ 1, Î½ and Âµi satisfy

Î½ + Âµi â‰¥ (cid:107)Î˜ij(cid:107)F,

âˆ€i âˆˆ C1

and Î½ â‰¥ (cid:107)Î˜ij(cid:107)F,

âˆ€i âˆˆ C2.

This leads to our guess of Î½ and Âµi in Lemma 3.9. Plugging this back into (67) we obtain the
expression of Z.

Proof of Lemma 3.10. We follow the same route in Lemma 3.4 by ï¬rst rewriting A, Î› and Î˜ into
four blocks as (63). Let (cid:101)Âµ1 := diag({ÂµiId}m1
i=1) âˆˆ Rm1dÃ—m1d. Then one can see that the four blocks
of Î˜ satisfy

(cid:101)Î˜11 =

(cid:101)Î˜12 + (cid:101)Î˜(cid:62)

1

( (cid:101)Âµ1 + Î½Im1d) (cid:102)M âˆ—
âˆš
d
(cid:102)M âˆ—

21 =

1
m1

1 (cid:101)A21 +

,

(cid:101)Î˜22 =

Î½ (cid:102)M âˆ—
2âˆš
d

,

1
m2

(cid:101)A12 (cid:102)M âˆ—

2 âˆ’

1
m1m2

(cid:102)M âˆ—

1 (cid:101)A12 (cid:102)M âˆ—
2 .

By plugging these into the four blocks of Î›, one can simplify Î› as (we leave the detail to readers
who are interested)

Î› = (Ind âˆ’ Î )(âˆ’Z âˆ’ A)(Ind âˆ’ Î ) = (Ind âˆ’ Î )(E[A] âˆ’ A âˆ’ Z + pInd)(Ind âˆ’ Î )

where we use (65) that (Ind âˆ’ Î ) (E[A] + pInd) (Ind âˆ’ Î ) = 0. As a result, by using the same
argument as in the proof of Lemma 3.4, one can see that (cid:101)Î› (cid:31) 0 ensures both Î› (cid:23) 0 and N (Î›) =
R(M âˆ—) are satisï¬ed.

30

Proof of Lemma 3.11. We ï¬rst deï¬ne the following quantities and provide their individual analysis.

Ïƒ1 := max
jâˆˆC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1

(cid:88)

s1âˆˆC1

As1j

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

Ïƒ2 := max
iâˆˆC1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

(cid:88)

s2âˆˆC1

Ïƒ :=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1m2

(cid:88)

(cid:88)

As1s2

s1âˆˆC1

s2âˆˆC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

, Ïƒ(i) :=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(cid:88)

s2âˆˆC2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

.

By applying Lemma A.3 with the approximation in (43) and the union bound, one can see that
with probability 1 âˆ’ nâˆ’c, Ïƒ1, Ïƒ2 and Ïƒ are bounded by

(cid:115)

Ïƒ1 â‰¤

2(1 + c)Î²d
Ï

(cid:18) log n
n

(cid:19)
,

(cid:115)

Ïƒ2 â‰¤

2(1 + c)Î²d
(1 âˆ’ Ï)

(cid:18) log n
n

(cid:19)
,

and Ïƒ = o(log n/n). Then, the dual variable Î½ given in Lemma 3.9 is bounded as

Î½

(a)
â‰¤ (1 âˆ’ Î³) max

iâˆˆC2,jâˆˆC1

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1

(cid:88)

s1âˆˆC1

Ais1

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(b)

= (1 âˆ’ Î³) (Ïƒ1 + Ïƒ2 + Ïƒ) â‰¤ (1 âˆ’ Î³)(cid:112)2(1 + c)Î²d

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
1
(cid:13)
(cid:13)
m1m2
(cid:13)
(cid:19)(cid:18) log n

(cid:88)

As2j

s2âˆˆC2
(cid:18) 1
âˆš
Ï

+

âˆš

1
1 âˆ’ Ï

(cid:88)

(cid:88)

As1s2

s1âˆˆC1
(cid:19)

+ o

s2âˆˆC2
(cid:18) log n
n

(cid:19)

n

(70)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(71)
with probability 1 âˆ’ nâˆ’c, where (a) holds by triangle inequality and (b) uses the fact Aij = A(cid:62)
ji.
Similarly, we can bound maxjâˆˆC2 (cid:107)Î˜ij(cid:107)F in Lemma 3.9 for any i âˆˆ C1 as

max
jâˆˆC2

(cid:107)Î˜ij(cid:107)F â‰¤ Î³

= Î³

(cid:18)

(cid:16)

(cid:88)

max
jâˆˆC2

(cid:13)
1
(cid:13)
(cid:13)
m1
(cid:13)
Ïƒ1 + Ïƒ + Ïƒ(i)(cid:17)

s1âˆˆC1

.

As1j

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(cid:88)

s2âˆˆC2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1m2

(cid:88)

(cid:88)

s1âˆˆC1

s2âˆˆC2

As1s2

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

Therefore, Âµi + Î½ given in Lemma 3.9 satisï¬es

Âµi + Î½ = max

(cid:26)

max
jâˆˆC2

(cid:27)

(cid:26)

(cid:107)Î˜ij(cid:107)F, Î½

â‰¤ max

Î³

â‰¤ max {Î³Ïƒ1, (1 âˆ’ Î³)(Ïƒ1 + Ïƒ2)} + Î³

(cid:16)

(cid:16)

Ïƒ1 + Ïƒ + Ïƒ(i)(cid:17)
Ïƒ + Ïƒ(i)(cid:17)

.

, (1 âˆ’ Î³) (Ïƒ1 + Ïƒ2 + Ïƒ)

(cid:27)

Now we are ready to bound Î»min(pInd âˆ’ Z). According to Lemma 3.9, by deï¬ning

(cid:15)1 := min
iâˆˆC1

(cid:20) (cid:88)

(cid:18)

ris âˆ’

sâˆˆC1

Âµs + Î½
âˆš
d

(cid:19)

âˆ’

m1(Âµi + Î½)
âˆš
d

(cid:21)
,

(cid:15)2 := min
iâˆˆC2

(cid:88)

sâˆˆC2

ris âˆ’

2m2Î½
âˆš
d

,

(72)

(73)

we have Î»min(pInd âˆ’ Z) = min{(cid:15)1, (cid:15)2} + p, and we can bound (cid:15)1, (cid:15)2 separately. To this end, let
xi := (cid:80)
s:C(s)=C(i) ris and it follows a Binomial distribution such that xi âˆ¼ Binom(m1, p) when
i âˆˆ C1 and xi âˆ¼ Binom(m2, p) when i âˆˆ C2. Then, by applying Lemma A.1 and the union bound
we obtain

(cid:26)

(cid:26)

P

P

min
iâˆˆC1

min
iâˆˆC2

(cid:27)

xi â‰¤ Ï„1Ï log n

= n

1âˆ’Ï

(cid:16)
Î±âˆ’Ï„1 log( eÎ±
Ï„1

(cid:17)

)+o(1)

,

xi â‰¤ Ï„2(1 âˆ’ Ï) log n

= n

(cid:27)

1âˆ’(1âˆ’Ï)

(cid:16)
Î±âˆ’Ï„2 log( eÎ±
Ï„2

(cid:17)

)+o(1)

.

(74)

(75)

31

for Ï„1, Ï„2 âˆˆ [0, Î±) which satisfy the conditions in (28). For (cid:15)1, we have

(cid:15)1 â‰¥ min
iâˆˆC1

(cid:88)

(cid:88)

ris

âˆ’

(cid:124)

sâˆˆC1
(cid:123)(cid:122)
=:(cid:15)11

(cid:125)

sâˆˆC1
(cid:124)

(cid:123)(cid:122)
=:(cid:15)12

(cid:125)

Âµs + Î½
âˆš
d

âˆ’

m1âˆš
d
(cid:124)

max
iâˆˆC1

(Âµi + Î½)

= (cid:15)11 âˆ’ (cid:15)12 âˆ’ (cid:15)13.

(cid:123)(cid:122)
=:(cid:15)13

(cid:125)

Then we can bound (cid:15)11, (cid:15)12 and (cid:15)13 separately. (cid:15)11 is bounded by (74). For (cid:15)12, by deï¬nition,

(a)
â‰¤

(cid:15)12

=

1
âˆš
d
m1âˆš
d

max {Î³Ïƒ1, (1 âˆ’ Î³)(Ïƒ1 + Ïƒ2)} + Î³

(cid:16)

Ïƒ + Ïƒ(i)(cid:17)

(cid:88)

iâˆˆC1

(max {Î³Ïƒ1, (1 âˆ’ Î³)(Ïƒ1 + Ïƒ2)} + Î³Ïƒ) +

(cid:88)

Ïƒ(i)

1
âˆš
d

iâˆˆC1
Â· Ï(cid:112)2(1 + c)Î² log n +

(b)
â‰¤ max

(cid:26) Î³
âˆš
Ï

, (1 âˆ’ Î³)

(cid:18) 1
âˆš
Ï

(cid:19)(cid:27)

+

âˆš

1
1 âˆ’ Ï

3Î³c
2

log n + o(log n)

with probability 1 âˆ’ nâˆ’c, where (a) uses the bound of Âµs + Î½ in (72), (b) comes from (70) for
bounding Ïƒ1 and Ïƒ2, and 1âˆš
Ïƒ(i) is bounded by Lemma A.5. Similarly, (cid:15)13 can be bounded
d
as

iâˆˆC1

(cid:80)

(cid:15)13 =

(a)
=

max
iâˆˆC1

m1âˆš
d
m1âˆš
d
(cid:18) 1
âˆš
Ï

â‰¤ Î³Ï

(cid:16)

max {Î³Ïƒ1, (1 âˆ’ Î³)(Ïƒ1 + Ïƒ2)} + Î³

(cid:16)

Ïƒ + Ïƒ(i)(cid:17)(cid:17)

(max {Î³(Ïƒ1 + Ïƒ2), (1 âˆ’ Î³)(Ïƒ1 + Ïƒ2)} + Î³Ïƒ)

+

âˆš

1
1 âˆ’ Ï

(cid:19)
(cid:112)2(1 + c)Î² log n + o(log n)

(b)
=

m1âˆš
d

(Î³(Ïƒ1 + Ïƒ2) + Î³Ïƒ)

with probability 1 âˆ’ nâˆ’c, where (a) comes from maxiâˆˆC1 Ïƒ(i) = Ïƒ1 and (b) holds since Î³ â‰¥ 1/2 by
assumption. Combining these individual results and applying the union bound yields

(cid:15)1 â‰¥ Ï„1Ï log n âˆ’ Ï max

(cid:26) 2Î³
âˆš
Ï

+

âˆš

Î³
1 âˆ’ Ï

,

1
âˆš
Ï

+

âˆš

1
1 âˆ’ Ï

(cid:27)

(cid:112)2(1 + c)Î² log nâˆ’

3Î³c
2

log nâˆ’o(log n)

with probability 1 âˆ’ nâˆ’â„¦(1). By further taking c â†’ 0 to be close to zero we get

(cid:15)1 â‰¥ Ï„1Ï log n âˆ’ Ï max

(cid:26) 2Î³
âˆš
Ï

+

âˆš

Î³
1 âˆ’ Ï

,

1
âˆš
Ï

+

âˆš

(cid:27)

1
1 âˆ’ Ï

(cid:112)(cid:96)Î² log n âˆ’ o(log n),

Similarly, by combining (75) with the bound of Î½ in (71) and let c â†’ 0, we get the bound for (cid:15)2 as

(cid:15)2 â‰¥ Ï„2(1 âˆ’ Ï) log n âˆ’ 2(1 âˆ’ Î³)(1 âˆ’ Ï)

(cid:18) 1
âˆš
Ï

+

âˆš

(cid:19)

1
1 âˆ’ Ï

(cid:112)(cid:96)Î² log n âˆ’ o(log n)

with probability 1 âˆ’ nâˆ’â„¦(1). Plugging these into Î»min(pInd âˆ’ Z) = min{(cid:15)1, (cid:15)2} + p completes the
proof.

Speciï¬cally when d = 2, Ïƒ1, Ïƒ2 and Ïƒ can be bounded by Theorem A.4 instead of Theorem A.3.

The remaining proof is same as above, therefore we do not repeat.

B.3 Two clusters with unknown cluster sizes

Now we consider the SDP proposed in (32). Note in this case, we use the normalized ground truth
Â¯M âˆ— deï¬ned in (31). The proof basically follows Section 3.1.1, and we will omit repetitive analysis.

32

Step 1: Derive KKT conditions, uniqueness and optimality of M âˆ—. The KKT conditions
of (32) when M = Â¯M âˆ— are given as

â€¢ Stationarity:

âˆ’ A âˆ’ Î› âˆ’ In âŠ— Z + Î˜ + Î˜(cid:62) = 0,
ij/(cid:107) Â¯M âˆ—
Âµi Â¯M âˆ—
ÂµiÎ±ij, (cid:107)Î±ij(cid:107)F â‰¤ 1,

i,j=1, Î˜ij =

Î˜ = [Î˜ij]n

ij(cid:107)F,

(cid:40)

Â¯M âˆ—
Â¯M âˆ—

ij (cid:54)= 0,
ij = 0.

(76)

(77)

â€¢ Comp. slackness:

(cid:10)Î›, Â¯M âˆ—(cid:11) = 0,

âˆš

(cid:28)

Âµi,

d âˆ’

(cid:29)

(cid:107) Â¯M âˆ—

ij(cid:107)F

n
(cid:88)

j=1

= 0,

i = 1, . . . , n.

(78)

â€¢ Dual feasibility:

Î› (cid:23) 0, Âµi â‰¥ 0,

i = 1, . . . , n.

(79)

Again, the following result establishes the uniqueness and optimality of Â¯M âˆ—:

Lemma B.1. Given Â¯M âˆ— deï¬ned in (30), suppose there exist dual variables Î›, Z, {Âµi}n
{Î±ij}(i,j):M âˆ—

ij =0 that satisfy the KKT conditions (76) - (79) as well as

i=1, and

Then M = Â¯M âˆ— is the optimal and unique solution to (32).

N (Î›) = R( Â¯M âˆ—).

(80)

Step 2: Construct dual variables. Again, we follow the assumption in Section 3.1.1 that
Ri = Id for each i and Aij = rijId for any j with C(j) = C(i). Then we have Â¯M âˆ—
ij = Id/m1 for
i, j âˆˆ C1 and Â¯M âˆ—

ij = Id/m2 for i, j âˆˆ C2. Also, let us deï¬ne

Ïƒi :=

(cid:88)

ris,

Ïƒ(1) :=

s:C(s)=C(i)

1
m1

(cid:88)

(cid:88)

s1âˆˆC1

s2âˆˆC1

rs1s2,

Ïƒ(2) :=

1
m2

(cid:88)

(cid:88)

rs1s2.

s1âˆˆC2

s2âˆˆC2

We make the following guess of dual variables, in particular, we adopt the assumption of Î˜ in
Lemma 3.9.

Lemma B.2. With a constant Î³ âˆˆ [0, 1], the dual variables with the following forms satisfy the
KKT conditions (76) - (78) and Âµi â‰¥ 0, i = 1, . . . , n in (79).

(cid:101)Î±ij =

1
m1

(cid:88)

As1j +

1
m2

(cid:88)

s2âˆˆC2

Ais2 âˆ’

1
m1m2

s1âˆˆC1

(cid:26) m1Î³
âˆš
d

Â¯Âµ1 = 2 max
iâˆˆC1
âˆš

(cid:18)

Ïƒi +

(cid:107) (cid:101)Î±ij(cid:107)F âˆ’Ïƒi

max
jâˆˆC2
Ïƒ(2)
d
max {Â¯Âµ1, Â¯Âµ2}
m1
2
2
âˆš
Ïƒ(1)
d
max {Â¯Âµ1, Â¯Âµ2}
m2
2
2
Ïƒ(1) + Ïƒ(2) + max {Â¯Âµ1, Â¯Âµ2}

Ïƒi +

Id,

+

+

(cid:18)

(cid:17)

ï£±
ï£´ï£´ï£²

ï£´ï£´ï£³
(cid:16)

(cid:19)
,

(cid:19)
,

Âµi =

Z =

(cid:88)

(cid:88)

As1s2,

i âˆˆ C1, j âˆˆ C2,

s1âˆˆC1

s2âˆˆC2

(cid:27)

âˆ’Ïƒ(2), Â¯Âµ2 = 2 max
iâˆˆC2

(cid:26) m2(1âˆ’Î³)
âˆš

d

max
jâˆˆC2
âˆš

(cid:107) (cid:101)Î±ji(cid:107)F âˆ’Ïƒi

(cid:27)

âˆ’Ïƒ(1),

ï£±
ï£´ï£²

d,

Î˜ij =

i âˆˆ C1,

ÂµiId/
Î³ (cid:101)Î±ij,
(1 âˆ’ Î³) (cid:101)Î±(cid:62)
ji,
Î› = âˆ’A âˆ’ In âŠ— Z + Î˜ + Î˜(cid:62).

i âˆˆ C2.

ï£´ï£³

C(i) = C(j),
i âˆˆ C1, j âˆˆ C2,
i âˆˆ C2, j âˆˆ C1,

Lemma B.3. Given the dual variables in Lemma B.2, Î› can be expressed as

Î› = (Ind âˆ’ Â¯M âˆ—)(E[A] âˆ’ A + (p âˆ’ (cid:15))Ind)(Ind âˆ’ Â¯M âˆ—),

(cid:15) := Ïƒ(1) + Ïƒ(2) + max {Â¯Âµ1, Â¯Âµ2}

Then, Î› (cid:23) 0 and N (Î›) = R( Â¯M âˆ—) are satisï¬ed if (cid:101)Î› (cid:31) 0.

33

Step 3: Find the condition for (cid:101)Î› (cid:31) 0. Again, by using the same argument as in Section 3.1.1
we get, for exact recovery it suï¬ƒces to ensure Î»min((p âˆ’ (cid:15))Ind) > (cid:107)E[A] âˆ’ A(cid:107). To this end, we have
the following bound for Î»min((p âˆ’ (cid:15))Ind).

Lemma B.4. Let p = Î± log n/n, q = Î² log n/n and Î´ :=
and (cid:96) = 1 for d = 2. Then Î»min((p âˆ’ (cid:15))Ind) satisï¬es

(cid:16) 1âˆš

Ï + 1âˆš

1âˆ’Ï

(cid:17) âˆš

(cid:96)Î², where (cid:96) = 2 for d > 2

Î»min((p âˆ’ (cid:15))Ind) â‰¥ min{Ï(2Ï„1 âˆ’ Î± âˆ’ 2Î³Î´), (1 âˆ’ Ï)(2Ï„2 âˆ’ Î± âˆ’ 2(1 âˆ’ Î³)Î´} log n + o(log n)

with probability 1 âˆ’ nâˆ’â„¦(1) for Ï„1, Ï„2 âˆˆ [0, Î±) such that

(cid:18)

1 âˆ’ Ï

Î± âˆ’ Ï„1 log

(cid:19)(cid:19)

(cid:18) eÎ±
Ï„1

< 0,

1 âˆ’ (1 âˆ’ Ï)

Î± âˆ’ Ï„2 log

(cid:18)

(cid:19)(cid:19)

(cid:18) eÎ±
Ï„2

< 0.

(81)

Proof of Theorem 3.12. From Lemma B.4 we have Î»min((pâˆ’(cid:15))Ind) = O(log n) with high probability.
âˆš
Also from Lemma 3.6 we get (cid:107)E[A] âˆ’ A(cid:107) = o(
log n) with high probability. This implies as n is
large, Î»min(pIndâˆ’Z) > (cid:107)E[A]âˆ’A(cid:107) satisï¬es as long as Î»min(pIndâˆ’Z) = â„¦(log n), which is equivalent
to

min{Ï(2Ï„1 âˆ’ Î± âˆ’ 2Î³Î´), (1 âˆ’ Ï)(2Ï„2 âˆ’ Î± âˆ’ 2(1 âˆ’ Î³)Î´} > 0.

(82)

This further reduces to Ï„1 > Î±/2 + Î³Î´ and Ï„2 > Î±/2 + (1 âˆ’ Î³)Î´. It remains to check (81) holds
for some Ï„1, Ï„2 âˆˆ [0, Î±). To this end, by using the same argument as in the proof of Theorem 3.7,
one can see that (81) holds as long as Ï„1 < Ï„ âˆ—
1 , Ï„2 < Ï„ âˆ—
2 deï¬ned in (20) (Notice that
Ï„ âˆ—
1 , Ï„ âˆ—
2 exist only if Î± > max {1/Ï, 1/(1 âˆ’ Ï)}). Therefore, putting all these together the condition
for exact recovery becomes

2 with Ï„ âˆ—

1 , Ï„ âˆ—

Î± > max

(cid:26) 1
Ï

,

(cid:27)

,

1
1 âˆ’ Ï

Î±
2

+ Î³Î´ < Ï„1 < Ï„ âˆ—
1

and

Î±
2

+ (1 âˆ’ Î³)Î´ < Ï„2 < Ï„ âˆ—
2 .

(83)

Furthermore, since Î³ âˆˆ [0, 1] is not speciï¬ed, it suï¬ƒces to ensure (83) holds for some Î³ âˆˆ [0, 1] such
that the range of Ï„1 and Ï„2 is valid. This leads to the conditions in (33).

B.3.1 Proof of the lemmas

Proof of Lemma B.1. The optimality of Â¯M âˆ— is immediately obtained since (32) is convex then KKT
conditions are suï¬ƒcient for Â¯M âˆ— being optimal [8]. For uniqueness, suppose (cid:102)M (cid:54)= Â¯M âˆ— is another
optimal solution. Similar to (54) in Section B.1, we have

0 = (cid:104)A, (cid:102)M âˆ’ Â¯M âˆ—(cid:105) = âˆ’(cid:104)Î›, (cid:102)M âˆ’ Â¯M âˆ—(cid:105) âˆ’ (cid:104)In âŠ— Z, (cid:102)M âˆ’ Â¯M âˆ—(cid:105) + (cid:104)Î˜ + Î˜(cid:62), (cid:102)M âˆ’ Â¯M âˆ—(cid:105)
(a)
= âˆ’(cid:104)Î›, (cid:102)M âˆ’ Â¯M âˆ—(cid:105) + (cid:104)Î˜ + Î˜(cid:62), (cid:102)M âˆ’ Â¯M âˆ—(cid:105)
= âˆ’(cid:104)Î›, (cid:102)M (cid:105) + 2(cid:104)Î˜, (cid:102)M âˆ’ Â¯M âˆ—(cid:105)

where (a) comes from (cid:80)n

i=1 Mii = 2Id in (32). To proceed, let us rewrite

(cid:104)Î˜, (cid:102)M âˆ’ Â¯M âˆ—(cid:105) = (cid:104)Î˜, (cid:102)M (cid:105) âˆ’ (cid:104)Î˜, Â¯M âˆ—(cid:105) =

(cid:88)

i,j

(cid:104)Î˜ij, (cid:102)Mij(cid:105) âˆ’

(cid:104)Î˜ij, Â¯M âˆ—

ij(cid:105).

(cid:88)

i,j

(84)

(85)

34

Furthermore, by plugging (77) into (85) we obtain

(cid:88)

i,j

(cid:88)

i,j

(cid:104)Î˜ij, Â¯M âˆ—

ij(cid:105) =

(cid:88)

(cid:88)

(cid:104)Î˜ij, (cid:102)Mij(cid:105) =

i

(cid:88)

i

j:C(j)=C(i)
(cid:18) (cid:88)

Âµi

Âµi
(cid:107) Â¯M âˆ—
ij(cid:107)F

(cid:104) Â¯M âˆ—

ij, Â¯M âˆ—

ij(cid:105) =

(cid:88)

âˆš

d,

Âµi

i

j:C(j)=C(i)

(cid:18) (cid:88)

(cid:88)

â‰¤

Âµi

i

j:C(j)=C(i)

1
(cid:107) Â¯M âˆ—
ij(cid:107)F

1
(cid:107) Â¯M âˆ—
ij(cid:107)F

(cid:104) Â¯M âˆ—

ij, (cid:102)Mij(cid:105) +

(cid:88)

(cid:19)

(cid:104)Î±ij, (cid:102)Mij(cid:105)

j:C(j)(cid:54)=C(i)

(cid:107) Â¯M âˆ—

ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F +

(cid:88)

(cid:107)Î±ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F

j:C(j)(cid:54)=C(i)

(cid:19)

(86)

(cid:88)

â‰¤

Âµi

(cid:88)

(cid:107) (cid:102)Mij(cid:107)F

(a)
â‰¤

(cid:88)

âˆš

d,

Âµi

i

j

i

âˆš

j (cid:107) (cid:102)Mij(cid:107)F â‰¤

where (a) holds since (cid:80)
d in (32). Then combining (85) and (86) results in (cid:104)Î˜, (cid:102)M âˆ’
M âˆ—(cid:105) â‰¤ 0, and plugging this back into (84) gives (cid:104)Î›, (cid:102)M (cid:105) â‰¤ 0. On the other hand, from Î› (cid:23) 0 in (79)
and (cid:102)M (cid:23) 0 in (32) we have (cid:104)Î›, (cid:102)M (cid:105) â‰¥ 0. Therefore we conclude (cid:104)Î›, (cid:102)M (cid:105) = 0 and (cid:104)Î˜, (cid:102)M âˆ’M âˆ—(cid:105) = 0.
To proceed, by N (Î›) = R( Â¯M âˆ—) and the deï¬nition of Â¯M âˆ— in (31), (cid:102)M has the following form:

(cid:102)M = V (1)Î£1(V (1))(cid:62) + V (2)Î£2(V (2))(cid:62),

(87)

for some diagonal Î£1, Î£2 (cid:23) 0. Now it remains to show Î£1 = Id/m1, Î£2 = Id/m2. To this end,
from (cid:104)Î˜, (cid:102)M âˆ’ M âˆ—(cid:105) = 0 we see (a) in (86) holds with equality, i.e. (cid:80)n
d, âˆ€i. Then
combining this with (87) yields the following for i âˆˆ C1:

j=1 (cid:107) (cid:102)Mij(cid:107)F =

âˆš

n
(cid:88)

j=1

(cid:107) (cid:102)Mij(cid:107)F

(a)
=

n
(cid:88)

j=1

(cid:107)RiÎ£1R(cid:62)

j (cid:107)F

(b)
= m1(cid:107)Î£1(cid:107)F =

âˆš

d, âˆ€i âˆˆ C1 â‡’ (cid:107)Î£1(cid:107)F =

âˆš

d/m1,

where both (a) and (b) follow from (87). Similarly for i âˆˆ C2 we can obtain (cid:107)Î£2(cid:107)F =
note for each diagonal block of (cid:102)M it satisï¬es

âˆš

d/m2. Next,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:102)Mii

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(a)
=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

RiÎ£1R(cid:62)

i +

(cid:88)

RiÎ£2R(cid:62)
i

iâˆˆC1
âˆš

(d)
=

d

(c)
= 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:102)Mii

iâˆˆC2
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(b)
â‰¤

(cid:88)

iâˆˆC1

(cid:13)
(cid:13)RiÎ£1R(cid:62)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)RiÎ£2R(cid:62)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)F

(cid:88)

iâˆˆC2

âˆš

where (a) follows from (87); (b) comes from triangle inequality; (c) holds because (cid:107)Î£1(cid:107)F =
and (cid:107)Î£1(cid:107)F =
and further RiÎ£1R(cid:62)
and Î£2 = Id/m2, which indicates (cid:102)M = Â¯M âˆ—.

d/m1
i=1 Mii = 2Id in (32). This implies (b) holds with equality,
i = Id/m2, âˆ€i âˆˆ C2. It follows Î£1 = Id/m1

i = Id/m1, âˆ€i âˆˆ C1 and RiÎ£2R(cid:62)

d/m2; (d) follows from (cid:80)n

âˆš

Proof of Lemma B.2. First, from (76) we get Î› = âˆ’A âˆ’ In âŠ— Z + Î˜ + Î˜(cid:62), plugging this into (80)
yields

(cid:18) ÂµiIdâˆš
d

+

ÂµsIdâˆš
d

(cid:19)

âˆ’ Ais

= Z,

i = 1, . . . , n,

(cid:16)

Î˜is + Î˜(cid:62)

si âˆ’ Ais

(cid:17)

= 0,

i = 1, . . . , n.

(cid:88)

s:C(s)=C(i)
(cid:88)

s:C(s)(cid:54)=C(i)

(88)

(89)

35

Then, by summing (88) over i âˆˆ C1 and i âˆˆ C2 separately we get the expression of Z as

Z =

(cid:18) 2
âˆš
d

(cid:88)

sâˆˆC1

Âµs âˆ’ Ïƒ(1)

(cid:19)

Id =

(cid:18) 2
âˆš
d

(cid:88)

sâˆˆC2

Âµs âˆ’ Ïƒ(2)

(cid:19)

Id,

Next, plugging (90) back into (88) yields expressions of Âµi as
âˆš

âˆš

âˆš

Âµi =

Ïƒi +

d
m1

(cid:18) 1
m1

âˆš

d
m2

Ïƒi +

Âµi =

(cid:124)

(cid:18) 1
m2

(cid:124)

(cid:88)

sâˆˆC1

(cid:88)

sâˆˆC2

d
m1

Ïƒ(1)

âˆš

d
m2

Ïƒ(2)

(cid:19)

=

=

(cid:125)

(cid:19)

(cid:125)

Âµs âˆ’

(cid:123)(cid:122)
=:Â¯Âµ1

Âµs âˆ’

(cid:123)(cid:122)
=:Â¯Âµ2

d
m1

Ïƒi + Â¯Âµ1,

i âˆˆ C1,

âˆš

d
m2

Ïƒi + Â¯Âµ2,

i âˆˆ C2.

Then plugging (91) into (90) yields

(cid:18)

Ïƒ(1) +

Z =

2m1âˆš
d

(cid:19)

Â¯Âµ1

Id =

(cid:18)

Ïƒ(2) +

(cid:19)

Â¯Âµ2

Id.

2m2âˆš
d

Importantly, to make the LHS and RHS above identical, Â¯Âµ1 and Â¯Âµ2 satisfy

âˆš

d
2m1

Â¯Âµ1 =

(Ïƒ(2) + Â¯Âµ),

Â¯Âµ2 =

âˆš

d
2m2

(Ïƒ(1) + Â¯Âµ)

(90)

(91)

(92)

(93)

for some Â¯Âµ. Plugging (93) into (91) and (90) yields the expressions of Z and Âµi in terms of Â¯Âµ, and
we restrict Î³ âˆˆ [0, 1] which ensures Âµi â‰¥ 0. To determine Â¯Âµ, we adopt the guess of Î˜ij and (cid:101)Î±ij in
Lemma 3.9. From (77) we have (cid:107)Î˜ij(cid:107)F = Âµi(cid:107)Î±ij(cid:107)F with (cid:107)Î±ij(cid:107)F â‰¤ 1, then for each Âµi it should
satisfy Âµi â‰¥ (cid:107)Î˜ij(cid:107)F for any j with C(j) (cid:54)= C(i). By further plugging the deï¬nition of Âµi we get
Â¯Âµ â‰¥ max {Â¯Âµ1, Â¯Âµ2} with Â¯Âµ1, Â¯Âµ2 deï¬ned in Lemma B.2, and we set Â¯Âµ = max {Â¯Âµ1, Â¯Âµ2}. This completes
our guess of dual variables.

Proof of Lemma B.3. We follow the same route as in Lemma 3.4 by ï¬rst rewriting A, Î› and Î˜ into
four blocks as (63). Also, let (cid:101)Âµ1 = diag({ÂµiId}m1
i=m1+1) âˆˆ
Rm2dÃ—m2d. Then the four blocks of Î˜ satisfy

i=1) âˆˆ Rm1dÃ—m1d and (cid:101)Âµ2 = diag({ÂµiId}n

(cid:101)Î˜11 = (cid:101)Âµ1 (cid:102)M âˆ—
1âˆš
d

,

(cid:101)Î˜12 + (cid:101)Î˜(cid:62)

21 =

1
m1

,

(cid:101)Î˜22 = (cid:101)Âµ2 (cid:102)M âˆ—
2âˆš
d
1
(cid:101)A12 (cid:102)M âˆ—
m2

(cid:102)M âˆ—

1 (cid:101)A21 +

2 âˆ’

1
m1m2

(cid:102)M âˆ—

1 (cid:101)A12 (cid:102)M âˆ—
2 .

The remaining proof is very similar to the one of Lemma 3.4, and we do not repeat.

Proof. According to Lemma B.3, Î»min((p âˆ’ (cid:15))Ind) satisï¬es

Î»min((p âˆ’ (cid:15))Ind) = p âˆ’ (cid:15) = p âˆ’ max{Â¯Âµ1, Â¯Âµ2} âˆ’ Ïƒ(1) âˆ’ Ïƒ(2)
(cid:26)

(cid:27)

(cid:26)

(cid:26)

= p+min

Ïƒi âˆ’

2 min
iâˆˆC1

(cid:124)

= p + min{(cid:15)1, (cid:15)2}.

m1Î³
âˆš
d

(cid:107) (cid:101)Î±ij(cid:107)F

max
jâˆˆC2
(cid:123)(cid:122)
=:(cid:15)1

âˆ’Ïƒ(1)

, 2 min
iâˆˆC2

Ïƒi âˆ’

m2(1âˆ’Î³)
âˆš
d

max
jâˆˆC1

(cid:107) (cid:101)Î±ji(cid:107)F

(cid:123)(cid:122)
=:(cid:15)2

(cid:27)

âˆ’Ïƒ(2)

(cid:27)

(cid:125)

(cid:125)

(cid:124)

36

Then we bound (cid:15)1 and (cid:15)2 separately as

(cid:15)1 â‰¥ 2 min
iâˆˆC1

Ïƒi âˆ’

2m1Î³
âˆš
d

max
iâˆˆC1,jâˆˆC2

(cid:107) (cid:101)Î±ij(cid:107)F âˆ’ Ïƒ(1),

(cid:15)2 â‰¥ 2 min
iâˆˆC2

Ïƒi âˆ’

2m2(1 âˆ’ Î³)
âˆš
d

max
iâˆˆC2,jâˆˆC1

(cid:107) (cid:101)Î±ji(cid:107)F âˆ’ Ïƒ(2).

For miniâˆˆC1 Ïƒi and miniâˆˆC2 Ïƒi, we can use the bounds derived in (74) and (75) respectively. For
maxiâˆˆC1,jâˆˆC2 (cid:107) (cid:101)Î±ij(cid:107)F, by using the result in (71) we obtain
1
1 âˆ’ Ï

(cid:107) (cid:101)Î±ij(cid:107)F â‰¤ (cid:112)2(1 + c)Î²

(cid:18) log n
n

max
iâˆˆC1,jâˆˆC2

(cid:18) 1
âˆš
Ï

(cid:19)(cid:18) log n

+ o

âˆš

+

(cid:19)

(cid:19)

n

with probability 1 âˆ’ nâˆ’c. For Ïƒ(1), since rij = rji for i, j âˆˆ C1, it satisï¬es

(cid:88)

(cid:88)

rs1s2 = 2

(cid:88)

rs1s2,

s1âˆˆC1

s2âˆˆC1

s1, s2âˆˆC1, s1<s2

and

(cid:80)
s1,s2âˆˆC1, s1<s2

rs1s2 follows a binomial distribution Binom (m1(m1 âˆ’ 1)/2, Î± log n/n). By apply-

ing Hoeï¬€dingâ€™s inequality [6] we obtain

(cid:40)

P

(cid:88)

s1,s2âˆˆC1, s1<s2

rs1s2 âˆ’

(m1 âˆ’ 1)Î±Ï log n
2

(cid:41)

(cid:18)

â‰¥ t

â‰¤ exp

âˆ’

(cid:19)

,

4t2
m2
1

for any t > 0. This further indicates Ïƒ(1) â‰¤ Î±Ï log n + o(log n) with high probability. Similarly we
can obtain Ïƒ(2) â‰¤ Î±(1 âˆ’ Ï) log n + o(log n) with high probability. By putting all the results together
and let c â†’ 0 to be close to zero, we get the bounds for (cid:15)1, (cid:15)2 and further Î»min((p âˆ’ (cid:15))Ind).

References

[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments.

The Journal of Machine Learning Research, 18(1):6446â€“6531, 2017.

[2] Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic

block model. IEEE Transactions on Information Theory, 62(1):471â€“487, 2015.

[3] Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices.

Cambridge university press, 2010.

[4] Chandrajit Bajaj, Tingran Gao, Zihang He, Qixing Huang, and Zhenxiao Liang. SMAC: simul-
taneous mapping and clustering using spectral decompositions. In International Conference on
Machine Learning, pages 324â€“333, 2018.

[5] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of
random matrices with independent entries. Annals of Probability, 44(4):2479â€“2506, 2016.

[6] StÂ´ephane Boucheron, GÂ´abor Lugosi, and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford university press, 2013.

[7] Nicolas Boumal. Nonconvex phase synchronization.

SIAM Journal on Optimization,

26(4):2355â€“2377, 2016.

37

[8] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge

university press, 2004.

[9] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka ZdeborovÂ´a. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Physical Review E, 84(6):066106, 2011.

[10] Patrick Doreian, Vladimir Batagelj, and Anuska Ferligoj. Generalized blockmodeling, volume 25.

Cambridge university press, 2005.

[11] Martin E. Dyer and Alan M. Frieze. The solution of some random NP-hard problems in

polynomial expected time. Journal of Algorithms, 10(4):451â€“489, 1989.

[12] Yifeng Fan, Tingran Gao, and Zhizhen Zhao. Unsupervised co-learning on G-manifolds across
In Advances in Neural Information Processing Systems, pages

irreducible representations.
9041â€“9053, 2019.

[13] Yifeng Fan, Tingran Gao, and Zhizhen Zhao. Representation theoretic patterns in multi-
Information and

frequency class averaging for three-dimensional cryo-electron microscopy.
Inference: A journal of IMA, page accepted, 2021.

[14] Yifeng Fan and Zhizhen Zhao. Multi-frequency vector diï¬€usion maps. In International Con-

ference on Machine Learning, pages 1843â€“1852. PMLR, 2019.

[15] Stephen E Fienberg, Michael M Meyer, and Stanley S Wasserman. Statistical analysis of
multiple sociometric relations. Journal of the american Statistical association, 80(389):51â€“67,
1985.

[16] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75â€“174, 2010.

[17] Joachim Frank. Three-Dimensional Electron Microscopy of Macromolecular Assemblies: Vi-
sualization of Biological Molecules in Their Native State. Oxford University Press, New York,
2nd edition, 2006.

[18] Tingran Gao and Zhizhen Zhao. Multi-frequency phase synchronization.

In International

Conference on Machine Learning, pages 2132â€“2141. PMLR, 2019.

[19] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via
semideï¬nite programming. IEEE Transactions on Information Theory, 62(5):2788â€“2797, 2016.

[20] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold
IEEE Transactions on Information Theory,

via semideï¬nite programming: Extensions.
62(10):5918â€“5937, 2016.

[21] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels:

First steps. Social networks, 5(2):109â€“137, 1983.

[22] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in

networks. Physical review E, 83(1):016107, 2011.

[23] Rafa(cid:32)l Lata(cid:32)la, Ramon van Handel, and Pierre Youssef. The dimension-free structure of nonho-

mogeneous random matrices. Inventiones mathematicae, 214(3):1031â€“1080, 2018.

38

[24] Roy R Lederman and Amit Singer. A representation theory perspective on simultaneous align-

ment and classiï¬cation. Applied and Computational Harmonic Analysis, 2019.

[25] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models.

The Annals of Statistics, 43(1):215â€“237, 2015.

[26] Xiaodong Li, Yudong Chen, Jiaming Xu, et al. Convex relaxation methods for community

detection. Statistical Science, 36(1):2â€“15, 2021.

[27] Shuyang Ling. Near-optimal performance bounds for orthogonal and permutation group syn-

chronization via spectral methods. arXiv preprint arXiv:2008.05341, 2020.

[28] Laurent MassouliÂ´e. Community detection thresholds and the weak ramanujan property. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 694â€“703,
2014.

[29] Frank McSherry. Spectral partitioning of random graphs. In Proceedings 42nd IEEE Symposium

on Foundations of Computer Science, pages 529â€“537. IEEE, 2001.

[30] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields, 162(3):431â€“461, 2015.

[31] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.

Combinatorica, 38(3):665â€“708, 2018.

[32] Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167â€“

256, 2003.

[33] Andy Nguyen, Mirela Ben-Chen, Katarzyna Welnicka, Yinyu Ye, and Leonidas Guibas. An
optimization approach to improving collections of shape maps. In Computer Graphics Forum,
volume 30, pages 1481â€“1491. Wiley Online Library, 2011.

[34] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideï¬nite program-

ming. SIAM journal on optimization, 18(1):186â€“205, 2007.

[35] Amelia Perry and Alexander S Wein. A semideï¬nite program for unbalanced multisection
In 2017 International Conference on Sampling Theory and

in the stochastic block model.
Applications (SampTA), pages 64â€“67. IEEE, 2017.

[36] John William Strutt Baron Rayleigh. The theory of sound, volume 2. Macmillan, 1896.

[37] Lord Rayleigh. XII. on the resultant of a large number of vibrations of the same pitch and of
arbitrary phase. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of
Science, 10(60):73â€“78, 1880.

[38] Yunpeng Shi and Gilad Lerman. Message passing least squares framework and its application

to rotation synchronization. arXiv preprint arXiv:2007.13638, 2020.

[39] Amit Singer. Angular synchronization by eigenvectors and semideï¬nite programming. Applied

and computational harmonic analysis, 30(1):20â€“36, 2011.

[40] Amit Singer and H-T Wu. Vector diï¬€usion maps and the connection laplacian. Communications

on pure and applied mathematics, 65(8):1067â€“1144, 2012.

39

[41] Amit Singer, Zhizhen Zhao, Yoel Shkolnisky, and Ronny Hadani. Viewing angle classiï¬cation
of cryo-electron microscopy images using eigenvectors. SIAM Journal on Imaging Sciences,
4(2):723â€“759, 2011.

[42] Gilbert W Stewart. Perturbation theory for the singular value decomposition. Technical report,

University of Maryland, College Park, 1998.

[43] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.

[44] Joel A... Tropp. User-friendly tail bounds for sums of random matrices. Foundations of com-

putational mathematics, 12(4):389â€“434, 2012.

[45] Joel A... Tropp. An introduction to matrix concentration inequalities. Foundations and

TrendsÂ® in Machine Learning, 8(1-2):1â€“230, 2015.

[46] Lieven Vandenberghe and Stephen Boyd. Semideï¬nite programming. SIAM review, 38(1):49â€“

95, 1996.

[47] Roman Vershynin. High-dimensional probability: An introduction with applications in data

science, volume 47. Cambridge university press, 2018.

[48] Bowei Yan, Purnamrita Sarkar, and Xiuyuan Cheng. Provable estimation of the number of
blocks in block models. In International Conference on Artiï¬cial Intelligence and Statistics,
pages 1185â€“1194. PMLR, 2018.

[49] Zhizhen Zhao and Amit Singer. Rotationally invariant image representation for viewing direc-

tion classiï¬cation in cryo-EM. Journal of structural biology, 186(1):153â€“166, 2014.

40

