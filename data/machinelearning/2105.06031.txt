1
2
0
2

y
a
M
3
1

]
L
M

.
t
a
t
s
[

1
v
1
3
0
6
0
.
5
0
1
2
:
v
i
X
r
a

Joint Community Detection and Rotational
Synchronization via SemideÔ¨Ånite Programming

Yifeng Fan‚àó

Yuehaw Khoo‚Ä†

Zhizhen Zhao‚àó

May 14, 2021

Abstract

In the presence of heterogeneous data, where randomly rotated objects fall into multiple
underlying categories, it is challenging to simultaneously classify them into clusters and syn-
chronize them based on pairwise relations. This gives rise to the joint problem of community
detection and synchronization. We propose a series of semideÔ¨Ånite relaxations, and prove their
exact recovery when extending the celebrated stochastic block model to this new setting where
both rotations and cluster identities are to be determined. Numerical experiments demonstrate
the eÔ¨Écacy of our proposed algorithms and conÔ¨Årm our theoretical result which indicates a sharp
phase transition for exact recovery.

Keywords: Community detection, synchronization, semideÔ¨Ånite programming, rotation group,

stochastic block model

1

Introduction

In the presence of heterogeneous data, where randomly rotated objects fall into multiple underly-
ing categories, it is challenging to simultaneously synchronize the objects and classify them into
clusters. A motivating example is the 2D class averaging in cryo-electron microscopy single particle
reconstruction [17, 41, 49]. It aims to cluster images taken from similar viewing directions, rota-
tionally align and average those images in order to denoise the experimental data. Joint clustering
and synchronization is an emerging area that connects community detection [1, 16, 32, 2, 19, 20]
and synchronization [39, 7, 18]. Recently, several works discussed simultaneous classiÔ¨Åcation and
mapping (alignment) [4, 24] and proposed diÔ¨Äerent models and algorithms.
In [4], the authors
addressed simultaneous permutation group synchronization and clustering via a spectral method
with rounding and used the consistency of the mapping for clustering. In [24], the authors noticed
that both rotational alignment and classiÔ¨Åcation are problems over compact groups and proposed
a harmonic analysis and semideÔ¨Ånite programming based approach for solving alignment and clas-
siÔ¨Åcation simultaneously.

In this paper, we consider joint community detection and rotational synchronization under a
speciÔ¨Åc probabilistic model, which extends the celebrated stochastic block model (SBM) [9, 10, 11,
15, 21, 22, 28, 29, 30, 31] to incorporate both the community structure and pairwise rotations.
In particular, we inherit the G(n, p, q)-SBM setting [26, 19, 2, 20] for the graph connection as
shown in Figure 1. Given a network of size n with K underlying disjoint communities, a random

‚àóDepartment of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign,

IL.

‚Ä†Department of Statistics, University of Chicago, Chicago, IL.

1

 
 
 
 
 
 
Figure 1: An illustration of our probabilistic model. We show a network with two underlying
communities in blue and orange respectively. Each vertex i is associated with a rotation Ri ‚àà SO(d).
Edges between vertices within the same cluster (resp. across clusters) independently connected with
probability p (resp. q) are shown in solid lines (resp. dash lines). The pairwise alignment Rij between
vertex i and vertex j is observed on each edge (i, j).

graph is generated such that each pair of vertices are independently connected with probability p
(resp. q) if they all belong to the same cluster (resp. diÔ¨Äerent clusters). In addition, each node i is
associated with a rotation Ri ‚àà SO(d) which is assumed to be unknown, and a pairwise alignment
Rij is observed on each connected edge. The noisy observation Rij is generated according to the
probabilistic model considered in [41, 13, 14]. When i and j belong to the same cluster we observe
the clean measurement Rij = RiR(cid:62)
j . When they are in diÔ¨Äerent clusters, Rij is assumed to be
uniformly drawn from SO(d). The model considered here is diÔ¨Äerent from the probabilistic model
in [4].

For such probabilistic model, a naive two-stage approach for recovery is to (1) directly apply
existing methods for community detection under SBM, and (2) perform rotational synchronization
for each identiÔ¨Åed community separately. However, such approach does not take advantage of
the cycle consistency of the alignments within each cluster and inconsistency of the alignments
across clusters, and thus the clustering result might be sub-optimal. Instead, we can exploit such
consistency of rotational alignments to further improve the identiÔ¨Åcation of communities. For
example, for three nodes i, j and k, their pairwise alignments satisfy RijRjkRki = Id if they belong
to the same cluster. In fact, the notion of cycle consistency has already been used in synchronization
problems [39, 33, 40, 14, 12, 38].

1.1 Our contributions

To incorporate the consistency of alignments in clustering, we formulate a series of optimization
problems in diÔ¨Äerent settings which aim to simultaneously recover the community structure and
individual rotations. To solve these non-convex problems, we apply semideÔ¨Ånite relaxation to
obtain a polynomial time solutions. Each resulting semideÔ¨Ånite programming (SDP) is tailored
to accommodate diÔ¨Äerent cluster structures with known or unknown cluster sizes. For the case of
two clusters, we provide dual certiÔ¨Åcates of the corresponding SDPs, and derive sharp exact recovery
conditions of the SDP. In particular, our analysis relies on two novel concentration inequalities for
random matrices: (1) Given a series of independent random matrices {Ri}n
i=1 uniformly drawn from
SO(d), we derive a sharp tail bound for the Frobenius norm of (cid:80)n
i=1 Ri at d = 2. (2) Given an
m √ó n block matrix S ‚àà Rmd√ónd with independent blocks uniformly drawn from SO(d), we show
that the operator norm (cid:107)S(cid:107) ‚àº
m, which can be regarded as an extension of the norm bound
of random matrices with independent entries [5, 47, 43].

n +

‚àö

‚àö

2

ùëÖùëñùëÖùëóùëÖùëôùëñùëóùëôùëÖùëñùëóùëÖùëóùëôw.p.ùëùw.p.ùëû1.2 Organization

The rest of this paper is organized as follows. In Section 2 we formally deÔ¨Åne our probabilistic model
and formulate non-convex optimization problems to solve for the cluster identities and rotations.
In Section 3 we propose SDP relaxations, and prove conditions for exact recovery for the case
when there are two underlying communities. Numerical results are given in Section 4 to evaluate
the proposed algorithm and demonstrate the predicted phase transition for exact recovery. We
conclude with some future directions in Section 5. For the clarity of the presentation, most the
proofs are provided in Appendix.

1.3 Notations

Throughout this paper we use the following notations: given a matrix X, its transpose is denoted
by X (cid:62). An m √ó n matrix of all zeros is denoted by 0m√ón (or 0 for brevity). An n √ó n identity
matrix is denoted by In. 1n ‚àà Rn represents a vector of length n with all ones. The maximum and
minimum eigenvalues of X are denoted by Œªmax(X) and Œªmin(X) respectively. (cid:107)X(cid:107) and (cid:107)X(cid:107)F
denote the operator norm and the Frobenius norm of X respectively. For two matrices X and Y ,
their inner product is written as (cid:104)X, Y (cid:105) = Tr(X (cid:62)Y ). For two non-negative functions f (n) and
g(n), f (n) = O(g(n)) (resp. f (n) = ‚Ñ¶(g(n))) denotes if there exists a constant C > 0 such that
f (n) ‚â§ Cg(n) (resp. f (n) ‚â• Cg(n)) for all suÔ¨Éciently large n, f (n) = o(g(n)) denotes for every
constant C > 0 it holds f (n) ‚â§ Cg(n) for all suÔ¨Éciently large n. SpeciÔ¨Åcally, given a matrix X of
size md √ó nd, we usually treat it as an m √ó n block matrix such that Xij ‚àà Rd√ód denotes its (i, j)-th
block, and the i-th block row (resp. j-th block column) of X is referred to as the sub-matrix that
contains Xij for all j = 1, . . . , n (resp. i = 1, . . . , m).

2 Our probabilistic model and problem formulation

We state our probabilistic model for joint community detection and rotation synchronization as
following: We assume that n agents in a network fall in K underlying communities and each agent i
is associated with a rotation matrix Ri ‚àà SO(d). The size of the k-th cluster is denoted by mk, and
Ck denotes the set of nodes belonging to the k-th cluster. Each agent i has an underlying cluster
label C(i) ‚àà {1, . . . , K}. A random graph G = (V, E) with node set V and edge set E is generated
from the underlying community structure and rotations of the agents such that each pair of nodes
(i, j) in the graph is connected with probability p if agents i and j are in the same cluster, i.e.,
C(i) = C(j), and with probability q if the agents are not in the same cluster, i.e., C(i) (cid:54)= C(j). In
addition, a relative rotational transformation Rij is observed on each edge (i, j) ‚àà E. If C(i) = C(j),
we obtain Rij = RiR(cid:62)
j which equals to the ground truth pairwise alignment. while if C(i) (cid:54)= C(j),
we assume Rij is a random rotation uniformly drawn from SO(d). Given the above, we construct
an observation matrix A ‚àà Rnd√ónd whose (i, j)-th block Aij ‚àà Rd√ód satisÔ¨Åes

Aij =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

RiR(cid:62)
with probability p and when C(i) = C(j),
j ,
Rij ‚àº Unif(SO(d)), with probability q and when C(i) (cid:54)= C(j),
0,

otherwise.

(1)

A can be viewed as an extension of the adjacency matrix such that each Aij is not a {0, 1}-valued
entry, but rather a matrix that indicates the connectivity as well as the observed rotation from j to

3

i. In addition, let us denote M ‚àó ‚àà Rnd√ónd as the clean observation matrix where

M ‚àó

ij =

(cid:40)

RiR(cid:62)
0,

j , C(i) = C(j),

otherwise.

(2)

When p = 1 and q = 0, M ‚àó is identical with A. Furthermore, we deÔ¨Åne V (k) ‚àà Rnd√ód that indicates
the cluster membership and individual rotations for each cluster k, whose i-th block V (k)
‚àà Rd√ód
satisÔ¨Åes

i

V (k)

i =

(cid:40)

Ri,
0,

i ‚àà Ck,
otherwise.

(3)

Then it is clear to see that M ‚àó = (cid:80)K
k=1 V (k)(V (k))(cid:62). SpeciÔ¨Åcally for the case of two clusters,
without loss of generality we can assume that the Ô¨Årst m1 nodes belong to C1 and the remaining m2
nodes belong to the other cluster C2. Then the ground truth M ‚àó deÔ¨Åned in (2) has the following
form:

M ‚àó = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) =

.

(4)

(cid:33)

(cid:32)

(cid:102)M ‚àó
0
1
0 (cid:102)M ‚àó
2

where (cid:102)M ‚àó

1 ‚àà Rm1d√óm1d and (cid:102)M ‚àó

2 ‚àà Rm2d√óm2d.

Based on the model, we state our problem of joint community detection and synchronization as

the following optimization program:

max
ÀÜR1,..., ÀÜRn‚ààSO(d), ÀÜC1,..., ÀÜCK

(cid:88)

(cid:68)

Aij, ÀÜRi ÀÜR(cid:62)
j

(cid:69)

,

i,j‚àà ÀÜCk,‚àÄk

(5)

where ÀÜCk denotes the set of nodes identiÔ¨Åed for the k-th cluster and ÀÜRi ‚àà SO(d) denotes the rotation
identiÔ¨Åed for node i. DiÔ¨Äerent from community detection under SBM, (5) aims to determine { ÀÜCi}K
i=1
and { ÀÜRi}n
i=1 simultaneously such that the cluster membership relies on both the edge connections
and the consistency of alignments. By introducing M ‚àà Rnd√ónd with its (i, j)-th block
(cid:40) ÀÜRi ÀÜR(cid:62)
j ,
0,

ÀÜC(i) = ÀÜC(j),
otherwise,

Mij =

(6)

we can recast (5) as the following program:

max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M satisÔ¨Åes the form in (6).

Notably, if the size of each cluster is known, an additional constraint should be added as

max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M has the form in (6), and | ÀÜCk| = mk, k = 1, . . . , K.

(7)

(8)

Since directly solving (7) or (8) is clearly non-convex and computationally intractable, in the fol-
lowing section we propose semideÔ¨Ånite relaxation to eÔ¨Éciently solve them.

4

3 SemideÔ¨Ånite relaxation

For simplicity, in Sections 3.1‚Äì3.3, we focus on the case of two clusters (i.e., K = 2). In particular,
we consider the following three scenarios: (1) two cluster sizes m1 = m2 = n/2 are equal and
known; (2) two cluster sizes m1, m2 are unequal and known; (3) two cluster sizes are unknown.
For each scenario, we develop its tailored SDP and provide corresponding performance guarantee.
In Section 3.4, we extend our approach to general cases with an arbitrary number of underlying
clusters.

3.1 Two equal-sized clusters with known cluster sizes

In this section, we study the case of two clusters where the cluster sizes m1 = m2 = m are equal and
known to us. In this case, the ground truth M ‚àó in (4) satisÔ¨Åes the following convex constraints:

1. M ‚àó is positive semi-deÔ¨Ånite, i.e. M ‚àó (cid:23) 0.

2. Each diagonal block of M ‚àó is an identity matrix, i.e. M ‚àó

ii = Id, i = 1, . . . , n.

3. The i-th block row of M ‚àó satisÔ¨Åes (cid:80)n

j=1 (cid:107)M ‚àó

ij(cid:107)F ‚â§ m

d, i = 1, . . . , n.

‚àö

Notably, instead of the last constraint, we want the equality (cid:80)n
d. However, the
norm equality constraint is non-convex and therefore a convex relaxation by replacing ‚Äú=‚Äù with
‚Äú‚â§‚Äù is needed. Based on these properties of M ‚àó, a semideÔ¨Ånite relaxation of (8) can be stated as

j=1 (cid:107)M ‚àó

ij(cid:107)F = m

‚àö

MSDP = arg max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

n
(cid:88)

j=1

‚àö

(cid:107)Mij(cid:107)F ‚â§ m

d,

i = 1, . . . , n.

(9)

As a result, (9) is a semideÔ¨Ånite program (SDP) which can be eÔ¨Éciently solved by interior-point
methods with polynomial complexity [46]. We remark that in the context of community detection,
SDPs similar to (9) have been studied in [2, 19] on SBM with two equal-sized clusters. In their
problem, each Mij is a scalar and a convex equality constraint (cid:80)n
i=1 Mij = m can be imposed. In
contrast, we use a convex inequality constraint on (cid:80)n

j=1 (cid:107)Mij(cid:107)F.

To characterize the performance of (9), we identify the conditions on the model parameters

(p, q, n) such that (9) achieves exact recovery, i.e., MSDP =M ‚àó as follows.

Theorem 3.1. Let p = Œ± log n/n and q = Œ≤ log n/n. For suÔ¨Éciently large n, (9) achieves exact
recovery with probability 1 ‚àí n‚àí‚Ñ¶(1) if

Œ± ‚àí (cid:112)2(cid:96)Œ≤ log

(cid:19)

(cid:18) eŒ±
‚àö
2(cid:96)Œ≤

> 2, where

(cid:96) =

(cid:40)

2,
1,

when d > 2,
when d = 2.

(10)

Notice that the function f (œÑ ) := Œ±‚àíœÑ log(eŒ±/œÑ ) monotonically decreases for œÑ ‚àà (0, Œ±], therefore
Theorem 3.1 implies the SDP (9) achieves exact recovery if Œ≤ is smaller than certain threshold
determined by Œ±. Also, we remark that in (10), the diÔ¨Äerence between the case for d = 2 and d > 2
lies in the diÔ¨Äerent concentration inequalities of random rotation matrix used in the proof (see
Section 3.1.1 for detail). However, we conjecture the condition for d > 2 can be further improved
such that the constant factor (cid:96) can be smaller than 2. Although the condition in (10) is suÔ¨Écient

5

but may not be necessary, in experiments we show the threshold of (10) sharply characterizes the
empirical phase transition boundary for exact recovery when d = 2 (see Figure 3).

In the following section, we outline the key steps in the proof of Theorem 3.1 based on showing
the dual certiÔ¨Åcate of the ground truth MSDP = M ‚àó for (9). The complete proofs of the lemmas
in Section 3.1 are deferred to Appendix B.1.

3.1.1 The sketch of proof

Our proof consists of the following three steps:

Step 1: Derive KKT conditions, uniqueness and optimality of M ‚àó. The Lagrangian of
the optimization problem (9) is,

L(M , Œõ, Z, ¬µ) = ‚àí (cid:104)A, M (cid:105) ‚àí (cid:104)Œõ, M (cid:105) ‚àí

n
(cid:88)

i=1

(cid:104)Zi, Mii ‚àí Id(cid:105) ‚àí

n
(cid:88)

(cid:28)

‚àö

¬µi, m

d ‚àí

i=1

(cid:29)

,

(cid:107)Mij(cid:107)F

n
(cid:88)

j=1

where Œõ, Z = diag ({Zi}n
M (cid:23) 0, Mii = Id and (cid:80)n
M = M ‚àó are listed as,

i=1) and ¬µ = {¬µi}n

j=1 (cid:107)Mij(cid:107)F ‚â§ m

‚àö

i=1 are the dual variables associated with the constraints
d for i = 1, . . . , n. Then the KKT conditions at

‚Ä¢ Stationarity:

‚àí A ‚àí Œõ ‚àí diag({Zi}n

Œò = [Œòij]n

i,j=1 , Œòij =

i=1) + Œò + Œò(cid:62) = 0,
ij/

‚àö

(cid:40)

d,

¬µiM ‚àó
M ‚àó
¬µiŒ±ij, (cid:107)Œ±ij(cid:107)F ‚â§ 1, M ‚àó

ij (cid:54)= 0,
ij = 0.

(11)

(12)

‚Ä¢ Comp. slackness:

(cid:104)Œõ, M ‚àó(cid:105) = 0,

(cid:28)

‚àö

¬µi, m

d ‚àí

(cid:29)

(cid:13)
(cid:13)M ‚àó
ij

(cid:13)
(cid:13)F

n
(cid:88)

j=1

= 0, i = 1, . . . , n.

(13)

‚Ä¢ Dual feasibility:

Œõ (cid:23) 0, ¬µi ‚â• 0,

i = 1, . . . , n.

(14)

Notably, Œ±ij in (12) can be chosen arbitrarily as long as (cid:107)Œ±ij(cid:107)F ‚â§ 1, because the derivative of
(cid:107)Mij(cid:107)F is undeÔ¨Åned at Mij = 0. Based on the KKT conditions, the following result establishes
the uniqueness and optimality of M ‚àó:

Lemma 3.2. Given M ‚àó deÔ¨Åned in (2), suppose there exists dual variables Œõ, Z, {¬µi}n
{Œ±ij}(i,j):M ‚àó

ij =0 that satisfy the KKT conditions (11) - (14) as well as

i=1 and

N (Œõ) = R(M ‚àó),

(15)

where N (¬∑) and R(¬∑) denote the null space and column space respectively. Then M ‚àó is the optimal
and unique solution to (9).

Step 2: Construct dual variables. Without loss of generality, we assume the rotation matrix
Ri = Id for each i. Then for any pair of (i, j) that C(i) = C(j), we have M ‚àó
ij = Id and Aij = rijId,
where rij ‚àà {0, 1} is a Bernoulli random variable such that P{rij = 1} = p. Our next step is to Ô¨Ånd
dual variables that satisfy the conditions in Lemma 3.2. To this end, inspired by [26] we make the
following guess for the dual variables.

Lemma 3.3. The following dual variables satisfy the KKT conditions (11) - (13) and ¬µi ‚â• 0, i =

6

1, . . . , n in (14):

Œòij =

‚àö

(cid:40)

¬µiId/
(cid:101)Œ±i,

d, C(i) = C(j),
C(i) (cid:54)= C(j),

¬µi = (cid:107) (cid:101)Œ±i(cid:107)F,

(cid:101)Œ±i =

Zi =

Œõ = ‚àíA ‚àí diag ({Zi}n

i=1) + Œò + Œò(cid:62).

1
m
(cid:18) m¬µi‚àö
d

(cid:88)

Aij ‚àí

j:C(i)(cid:54)=C(j)

(cid:88)

+

s:C(s)=C(i)

1
2m2
(cid:18) ¬µs‚àö
d

(cid:88)

(cid:88)

Asj,

j:C(i)(cid:54)=C(j)
(cid:19)(cid:19)

s:C(s)=C(i)

‚àí ris

Id,

When constructing the dual certiÔ¨Åcate, for Ô¨Åxed i, j where C(i) (cid:54)= C(j), Œòij can have many
choices. Here we assume Œòij is identical for all j when i and j are not in the same cluster. This
choice allows us to determine all dual variables via the KKT conditions and (15). Now it remains
to check under what condition on (Œ±, Œ≤) does Œõ satisfy both Œõ (cid:23) 0 and (15). To this end, we use
the following property for Œõ.

Lemma 3.4. Given the dual variables in Lemma 3.3, Œõ can be expressed as

Œõ = (Ind ‚àí Œ†) (E[A] ‚àí A ‚àí Z + pInd)
(Ind ‚àí Œ†), where Œ† =
(cid:125)

(cid:124)

(cid:123)(cid:122)
=: (cid:101)Œõ

(cid:19)

(cid:18) 1
m

M ‚àó.

(16)

Then, Œõ (cid:23) 0 and N (Œõ) = R(M ‚àó) are satisÔ¨Åed if (cid:101)Œõ (cid:31) 0.

Step 3: Find the condition for (cid:101)Œõ (cid:31) 0. Applying Weyl‚Äôs inequality [42] yields Œªmin( (cid:101)Œõ) ‚â•
Œªmin(pInd ‚àí Z) ‚àí (cid:107)E[A] ‚àí A(cid:107). Then for (cid:101)Œõ (cid:31) 0, it suÔ¨Éces to show

Œªmin(pInd ‚àí Z) > (cid:107)E[A] ‚àí A(cid:107).

(17)

To this end, we have the following bound for Œªmin(pInd ‚àí Z).

Lemma 3.5. Let p = Œ± log n/n, q = Œ≤ log n/n. For n that is suÔ¨Éciently large,

Œªmin(pInd ‚àí Z) ‚â• min

i

xi ‚àí max

i

yi ‚àí max{(cid:15)1, (cid:15)2} + p

where

1. xi := (cid:80)

s:C(s)=C(i) ris, which satisÔ¨Åes

min
i

xi ‚â•

œÑ
2

log n with probability

1 ‚àí n1‚àí 1

2 (Œ±‚àíœÑ log( eŒ±

œÑ )+o(1)),

for œÑ ‚àà [0, Œ±) such that 1 ‚àí 1
2

(cid:0)Œ± ‚àí œÑ log (cid:0) eŒ±
œÑ

(cid:1) + o(1)(cid:1) < 0;

2. yi := m¬µi‚àö
d

, which satisÔ¨Åes

max
i

yi ‚â§

(cid:114)

(cid:96)(c + 1)Œ≤
2

log n + o(log n) with probability

1 ‚àí n‚àíc,

(cid:96) =

(cid:40)

2,
1,

d > 2,
d = 2;

3. (cid:15)1 := 1‚àö
d

(cid:80)

s‚ààC1

¬µs and (cid:15)2 := 1‚àö
d

(cid:80)

s‚ààC2

¬µs, which satisfy

max{(cid:15)1, (cid:15)2} ‚â§

2c
3

log n + o(log n) with probability

1 ‚àí n‚àíc.

7

In Lemma 3.5, xi ‚àº Binom(m, p) follows the binomial distribution, which is bounded by the
result in [19, Lemma 2]. For yi, it depends on the bound of ¬µi, according to Lemma 3.3, this further
relies on bounding (cid:107) (cid:80)m
i=1 Ri(cid:107)F where {Ri}m
i=1 are i.i.d. random matrices uniformly drawn from
SO(d). When d > 2, (cid:107) (cid:80)m
i=1 Ri(cid:107)F is bounded by matrix Bernstein inequality [45], and when d = 2,
a sharper result is derived by explicitly computing the moments of (cid:107) (cid:80)m
i=1 Ri(cid:107)F and then apply
Markov inequality, which are detailed in Appendix A.2. For (cid:15)1 and (cid:15)2, by deÔ¨Ånition they depend
on the concentration of ¬µi and are thus bounded by Bernstein‚Äôs inequality [47].

It remains to bound (cid:107)E[A]‚àíA(cid:107). If A is an adjacency matrix where Aij ‚àà {0, 1}, the upper bound
of (cid:107)E[A]‚àíA(cid:107) has been studied in the context of SBM (e.g. [19, 25]). However, here Aij is a random
rotation matrix for C(i) (cid:54)= C(j) rather than being {0, 1}-valued. Therefore, in Appendix A.3, we
derive an upper bound of the operator norm of a random block matrix in Theorem A.7, which leads
us to the following result for (cid:107)E[A] ‚àí A(cid:107).

Lemma 3.6. Let the two clusters be of size m1 and m2 (m1 + m2 = n), and p, q = ‚Ñ¶(log n/n).
Then for A deÔ¨Åned in (1), it satisÔ¨Åes

(cid:107)E[A] ‚àí A(cid:107) ‚â§ c1(

‚àö

pm1 +

‚àö

pm2) + c2(

‚àö

qm1 +

‚àö

qm2) + O((cid:112)log n)

with probability 1 ‚àí n‚àíc for c > 0, where c1, c2 > 0 are some universal constants.

Proof of Theorem 3.1. By applying the union bound on mini xi, maxi yi and max{(cid:15)1, (cid:15)2} in Lemma 3.5,
we obtain

Œªmin(pId ‚àí Z) ‚â•

log n + o(log n),

(cid:114)

(cid:18) œÑ
2

‚àí

(cid:96)(c + 1)Œ≤
2

‚àí

2c
3

(cid:19)

with probability 1 ‚àí n‚àí‚Ñ¶(1), as long as the condition in Lemma 3.5 that

1 ‚àí

(cid:16)

1
2

Œ± ‚àí œÑ log

(cid:17)(cid:17)

(cid:16) eŒ±
œÑ

< 0

(18)

(cid:113) (cid:96)(c+1)Œ≤
2

is satisÔ¨Åed. Also, from Lemma 3.6, we have (cid:107)E[A] ‚àí A(cid:107) = O(
log n) with high probability. This
implies, as n is suÔ¨Éciently large, (17) holds if Œªmin(pId ‚àí Z) = ‚Ñ¶(log n), that is equivalent to
œÑ
2 ‚àí
It
remains to ensure there exists a œÑ ‚àà (0, Œ±] that satisÔ¨Åes (18). To this end, since the LHS of (18)
is monotonically increasing for œÑ ‚àà (0, Œ±], by choosing œÑ ‚Üí
2(cid:96)Œ≤ the condition (18) becomes (10),
which is the condition for exact recovery.

3 > 0. By taking c ‚Üí 0 to be suÔ¨Éciently small, it reduces to œÑ >

‚àí 2c

2(cid:96)Œ≤.

‚àö

‚àö

‚àö

3.2 Two unequal-sized clusters with known cluster size

‚àö

In
Now we consider the case where the two cluster sizes m1, m2 are unequal and known to us.
this case, the ground truth M ‚àó still has the form in (4) and convex relaxations similar to (9) can
be applied. However, unlike the equal-sized case, the row-sum constraint satisÔ¨Åes (cid:80)n
j=1 (cid:107)Mij(cid:107)F =
d if i ‚àà C2. Therefore a convex constraint can be imposed
m1
as (cid:80)n
d. So this imposes an upper-bound on the cluster sizes, based
on the size of the larger cluster. Besides, to incorporate the information of the smaller cluster of the
two, we consider an additional constraint on the sum of all blocks as (cid:80)n
d.

d if i ‚àà C1 and (cid:80)n
j=1 (cid:107)Mij(cid:107)F ‚â§ max{m1, m2}

j=1 (cid:107)Mij(cid:107)F = m2

i,j=1 (cid:107)Mij(cid:107)F ‚â§ (m2

1 +m2
2)

‚àö

‚àö

‚àö

8

(20)

(21)

In this way, the resulting SDP is

MSDP = arg max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

n
(cid:88)

j=1

(cid:107)Mij(cid:107)F ‚â§ max{m1, m2}

‚àö

d,

‚àÄi = 1, . . . , n,

(19)

(cid:107)Mij(cid:107)F ‚â§ (cid:0)m2

1 + m2
2

(cid:1) ‚àö

d.

n
(cid:88)

i,j=1

For (19), the condition for exact recovery is derived as the following.

Theorem 3.7. Let p = Œ± log n/n, q = Œ≤ log n/n. Suppose m1 = œÅn, m2 = (1 ‚àí œÅ)n for some
œÅ ‚àà (1/2, 1) such that m1 > m2. Let œÑ ‚àó

1 , œÑ ‚àó
(cid:19)

2 ‚àà [0, Œ±) be the roots of the equations
(cid:18) eŒ±
œÑ ‚àó
2

1
1 ‚àí œÅ

, Œ± ‚àí œÑ ‚àó

2 log

1
œÅ

=

=

(cid:19)

.

(cid:18) eŒ±
œÑ ‚àó
1

Œ± ‚àí œÑ ‚àó

1 log

As n is suÔ¨Éciently large, (19) achieves exact recovery with probability 1 ‚àí n‚àí‚Ñ¶(1) if

Ô£±
Ô£¥Ô£≤

Œ± >

Ô£¥Ô£≥

Œ± >

1
1 ‚àí œÅ
1
1 ‚àí œÅ

,

,

œÑ ‚àó
1
Œ¥2

> 1

and

œÑ ‚àó
1
Œ¥1

(cid:26)

> max

1 ‚àí

(cid:27)

œÑ ‚àó
2
2Œ¥2

,

1
2

when Œ≤ = 0,

, when Œ≤ > 0,

where Œ¥1 :=

(cid:16) 2‚àö

œÅ + 1‚àö

1‚àíœÅ

(cid:17) ‚àö

lŒ≤, Œ¥2 :=

(cid:16) 1‚àö

œÅ + 1‚àö

1‚àíœÅ

(cid:17) ‚àö

lŒ≤. (cid:96) = 2 when d > 2 and (cid:96) = 1 when d = 2.

Note that the function f (œÑ ) := Œ± ‚àí œÑ log (eŒ±/œÑ ) monotonically decreases for œÑ ‚àà [0, Œ±), and it
2 of
2 ‚àà [0, Œ±) when Œ± > 1/(1 ‚àí œÅ). In the following section, we outline the key steps for

satisÔ¨Åes f (Œ±) = 0 and limœÑ ‚Üí0 f (œÑ ) = Œ±. Therefore, there exists a unique pair of solutions œÑ ‚àó
(20) for œÑ ‚àó
proving Theorem 3.7 and leave the detail to Appendix B.2.

1 , œÑ ‚àó

1 , œÑ ‚àó

3.2.1 The sketch of proof

The steps for proving Theorem 3.7 are similar to those outlined in Section 3.1.1, while a consid-
erable amount of eÔ¨Äort is devoted to handle the imbalanced clusters and the additional constraint
(cid:80)n

(cid:1) ‚àö

i,j=1 (cid:107)Mij(cid:107)F ‚â§ (cid:0)m2

1 + m2
2

d.

Step 1: Derive KKT conditions, uniqueness and optimality of M ‚àó. The Lagrangian of
(19) is given as

L = ‚àí (cid:104)A, M (cid:105) ‚àí (cid:104)Œõ, M (cid:105) ‚àí

n
(cid:88)

i=1

(cid:104)Zi, Mii ‚àí Id(cid:105) +

n
(cid:88)

(cid:28)

¬µi,

i=1

n
(cid:88)

j=1

(cid:107)Mij(cid:107)F ‚àí m1

‚àö

(cid:29)

d

(cid:18) (cid:88)

+ ŒΩ

i,j

(cid:107)Mij(cid:107)F ‚àí (m2

1 + m2
2)

‚àö

(cid:19)
,

d

where ŒΩ is the dual variable associated with the extra constraint. Then we list the KKT conditions

9

of (19) when M = M ‚àó as

‚Ä¢ Stationarity:

i=1) + Œò + Œò(cid:62) = 0,
‚àí A ‚àí Œõ ‚àí diag({Zi}n
(cid:40)
(¬µi + ŒΩ)M ‚àó
M ‚àó
d,
(¬µi + ŒΩ)Œ±ij, (cid:107)Œ±ij(cid:107)F ‚â§ 1, M ‚àó

i,j=1, Œòij =

Œò = [Œòij]n

ij/

‚àö

ij (cid:54)= 0,
ij = 0.

‚Ä¢ Comp. slackness:

(cid:104)Œõ, M ‚àó(cid:105) = 0,

ŒΩ

(cid:18) (cid:88)

i,j

(cid:107)M ‚àó

ij(cid:107)F ‚àí (m2

1 + m2
2)

‚àö

(cid:19)

d

= 0,

(cid:28)

‚àö

d ‚àí

¬µi, m1

(cid:29)

(cid:107)M ‚àó

ij(cid:107)F

n
(cid:88)

j=1

= 0,

i = 1, . . . , n,

‚Ä¢ Dual feasibility:

Œõ (cid:23) 0,

ŒΩ ‚â• 0, ¬µi ‚â• 0,

i = 1, . . . , n.

Again, the following lemma establishes the uniqueness and optimality of M ‚àó:

(22)

(23)

(24)

(25)

(26)

Lemma 3.8. Given M ‚àó deÔ¨Åned in (2), suppose there exists dual variables Œõ, Z, {¬µi}n
and ŒΩ that satisfy the KKT conditions (22) - (26) as well as

i=1, {Œ±ij}(i,j):M ‚àó

ij =0

N (Œõ) = R(M ‚àó).

(27)

Then M ‚àó is the optimal and unique solution to (19).

Step 2: Construct dual variables. Again, assuming M ‚àó
that C(i) = C(j). we make the following guess of the dual variables.

ij = Id and Aij = rijId for any (i, j)

Lemma 3.9. With a constant Œ≥ ‚àà [1/2, 1], the following forms of the dual variables satisfy (22) -
(25) and ŒΩ ‚â• 0, ¬µi ‚â• 0, for i = 1, . . . , n in (26).

(cid:101)Œ±ij =

Œòij =

1
m1
Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

(cid:88)

s1‚ààC1

As1j +

‚àö

(¬µi + ŒΩ)Id/
Œ≥ (cid:101)Œ±ij,
(1 ‚àí Œ≥) (cid:101)Œ±(cid:62)
ji,

1
m2

(cid:88)

s2‚ààC2

Ais2 ‚àí

1
m1m2

(cid:88)

(cid:88)

s1‚ààC1

s2‚ààC2

As1s2,

i ‚àà C1, j ‚àà C2,

d, C(i) = C(j),

Ô£≥

¬µi =

i ‚àà C1, j ‚àà C2,
i ‚àà C2, j ‚àà C1,
(cid:18) m1(¬µi + ŒΩ)
Ô£±
‚àö
Ô£¥Ô£¥Ô£¥Ô£≤
(cid:18) 2m2ŒΩ
Ô£¥Ô£¥Ô£¥Ô£≥
‚àö
d

‚àí

d

(cid:88)

s‚ààC2

(cid:88)

+

s‚ààC1
(cid:19)

Ô£±
Ô£≤

0,

max{0, Œ≥ ¬∑ max
j‚ààC2

(cid:107) (cid:101)Œ±ij(cid:107)F ‚àí ŒΩ},

i ‚àà C1,

i ‚àà C2,

(cid:18) ¬µs + ŒΩ
‚àö
d

(cid:19)(cid:19)

‚àí ris

Id,

i ‚àà C1,

ris

Id,

i ‚àà C2,

ŒΩ = (1 ‚àí Œ≥) max

i‚ààC2,j‚ààC1

(cid:107) (cid:101)Œ±ji(cid:107)F, Zi =

Œõ = ‚àíA ‚àí diag ({Zi}n

i=1) + Œò + Œò(cid:62).

Furthermore, Œõ can be simpliÔ¨Åed as the following form.

Lemma 3.10. Given the dual variables in Lemma 3.9, Œõ can be expressed as

Œõ = (Ind ‚àí Œ†) (E[A] ‚àí A ‚àí Z + pInd)
(Ind ‚àí Œ†) where Œ† :=
(cid:125)

(cid:124)

(cid:123)(cid:122)
=: (cid:101)Œõ

(cid:32)

(cid:102)M ‚àó
1 /m1
0

(cid:33)

.

0
(cid:102)M ‚àó
2 /m2

Then, Œõ (cid:23) 0 and N (Œõ) = R(M ‚àó) are satisÔ¨Åed if (cid:101)Œõ (cid:31) 0.

10

DiÔ¨Äerent from Step 2 in Section 3.1.1, here, we adopt a new way to construct Œòij in order to
obtain a threshold close to the empirical phase transition line by tuning Œ≥. The parameter Œ≥ controls
the values of ŒΩ, ¬µi, and the diagonal entries of Zi. As we shall see in the next step, by adjusting
Œ≥, we can balance the diagonal values of Zi for i ‚àà C1 and i ‚àà C2, which helps to ensure (cid:101)Œõ (cid:31) 0.
We remark that our current construction of the dual variables may still be sub-optimal, which
indicates the condition in (21) is suÔ¨Écient but not necessary. Finding the optimal dual appears to
be challenging and is left for future work.

Step 3: Find the condition for (cid:101)Œõ (cid:31) 0. Again, by using the same argument as in Section 3.1.1,
one can see that for exact recovery it suÔ¨Éces to show Œªmin(pInd ‚àí Z) > (cid:107)E[A] ‚àí A(cid:107). Here we get
the following bound for Œªmin(pInd ‚àí Z).

Lemma 3.11. Let p = Œ± log n/n, q = Œ≤ log n/n and Œ≥ ‚àà [1/2, 1]. Suppose m1 = œÅn and m2 =
(1 ‚àí œÅ)n for some œÅ ‚àà (1/2, 1). Œ¥1 and Œ¥2 are deÔ¨Åned in Theorem 3.7. We have

Œªmin(pInd ‚àí Z) ‚â• min{œÅ(œÑ1 ‚àí max{Œ≥Œ¥1, Œ¥2}), (1 ‚àí œÅ)(œÑ2 ‚àí 2(1 ‚àí Œ≥)Œ¥2)} log n ‚àí o(log n)

with probability 1 ‚àí n‚àí‚Ñ¶(1) for some œÑ1, œÑ2 ‚àà [0, Œ±] such that

(cid:18)

1 ‚àí œÅ

Œ± ‚àí œÑ1 log

(cid:19)(cid:19)

(cid:18) eŒ±
œÑ1

< 0,

1 ‚àí (1 ‚àí œÅ)

Œ± ‚àí œÑ2 log

(cid:18)

(cid:19)(cid:19)

(cid:18) eŒ±
œÑ2

< 0.

(28)

Proof of Theorem 3.7. From Lemma 3.11, we get Œªmin(pInd ‚àí Z) = O(log n). From Lemma 3.6, we
have (cid:107)E[A] ‚àí A(cid:107) = o(
log n). Therefore, as n is suÔ¨Éciently large, the condition Œªmin(pInd ‚àí Z) >
(cid:107)E[A] ‚àí A(cid:107) is satisÔ¨Åed if Œªmin(pInd ‚àí Z) = ‚Ñ¶(log n), which is equivalent to

‚àö

min{œÅ(œÑ1 ‚àí max{Œ≥Œ¥1, Œ¥2}), (1 ‚àí œÅ)(œÑ2 ‚àí 2(1 ‚àí Œ≥)Œ¥2)} > 0.

(29)

By deÔ¨Åning ¬ØœÑ1 := max{Œ≥Œ¥1, Œ¥2} and ¬ØœÑ2 := 2(1 ‚àí Œ≥)Œ¥2, (29) reduces to œÑ1 > ¬ØœÑ1 and œÑ2 > ¬ØœÑ2. Now
it remains to check if (28) holds for some œÑ1, œÑ2 ‚àà [0, Œ±). To this end, let œÑ ‚àó
2 ‚àà [0, Œ±) be the
roots of the equations in (20) (Notice that such œÑ ‚àó
2 exist only if Œ± > 1/(1 ‚àí œÅ)). Since the
function f (œÑ ) := Œ± ‚àí œÑ log (eŒ±/œÑ ) monotonically decreases for œÑ ‚àà [0, Œ±), then (28) holds only if
1 and œÑ2 < œÑ ‚àó
œÑ1 < œÑ ‚àó
2 . Putting all these together, the condition for exact recovery reduces to
Œ± > 1/(1 ‚àí œÅ), ¬ØœÑ1 < œÑ1 < œÑ ‚àó
2 . Furthermore, since Œ≥ ‚àà [1/2, 1] is not speciÔ¨Åed, it
suÔ¨Éces to ensure ¬ØœÑ1 < œÑ ‚àó
2 for some Œ≥ ‚àà [1/2, 1] such that the range of œÑ1, œÑ2 is valid.
This leads to the condition in (21).

1 and ¬ØœÑ2 < œÑ2 < œÑ ‚àó

1 and ¬ØœÑ2 < œÑ ‚àó

1 , œÑ ‚àó

1 , œÑ ‚àó

3.3 Two clusters with unknown cluster size

In real applications, we usually have no information on cluster sizes. This motivates us to seek
an alternative approach that does not rely on such prior knowledge. To this end, we consider a
normalized ground truth ¬ØM ‚àó, whose (i, j)-th block is deÔ¨Åned as

¬ØM ‚àó

ij =

(cid:40) 1

mC(i)
0,

RiR(cid:62)

j , C(i) = C(j),
otherwise.

(30)

Here mC(i) denotes the size of the cluster that i belongs to. By assumption that the Ô¨Årst m1 nodes
form C1 and the remaining m2 nodes form C2, we have

¬ØM ‚àó :=

1
m1

V (1)(V (1))(cid:62) +

1
m2

V (2)(V (2))(cid:62) =

11

(cid:32) 1

m1 (cid:102)M ‚àó
0

1

(cid:33)

,

(31)

0
m2 (cid:102)M ‚àó

1

2

where each diagonal block has been normalized by the corresponding cluster size. Then ¬ØM ‚àó satisÔ¨Åes
the following structural properties:

1. ¬ØM ‚àó is positive semi-deÔ¨Ånite, i.e. ¬ØM ‚àó (cid:23) 0;
2. The sum of the diagonal blocks satisÔ¨Åes (cid:80)
3. The i-th block row of ¬ØM ‚àó satisÔ¨Åes (cid:80)n

ij(cid:107)F ‚â§
Based on these, we formulate the SDP for solving ¬ØM ‚àó as

j=1 (cid:107) ¬ØM ‚àó

¬ØM ‚àó

i

ii = 2Id;
‚àö

d, for i = 1, . . . , n.

MSDP = arg max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0,

n
(cid:88)

i=1

Mii = 2Id,

‚àö

(cid:107)Mij(cid:107)F ‚â§

n
(cid:88)

j=1

d,

i = 1, . . . , n.

(32)

Due to the normalization factor, (32) diÔ¨Äers from the previous SDPs that it does not rely on the
cluster sizes, and exact recovery is achieved if the normalized ¬ØM ‚àó is obtained. Besides, similar to
(19) a sum of all blocks constraint (cid:80)n
d could apply to (32), via the row sum
constraint that (cid:80)n
d for all i and is thus redundant. Notably, in the context of
community detection, similar normalization has been studied [34, 48, 35] when cluster sizes are
unknown, where the row-sum constraint is formulated as (cid:80)n
In contrast, we use
an inequality constraint for the sake of convexity. Here, we obtain the following exact recovery
guarantees for (32).

i,j=1 (cid:107)Mij(cid:107)F ‚â§ n

j=1 (cid:107)Mij(cid:107)F ‚â§

j=1 Mij = 1.

‚àö

‚àö

Theorem 3.12. Let p = Œ± log n/n, q = Œ≤ log n/n. Suppose m1 = œÅn and m2 = (1 ‚àí œÅ)n for some
œÅ ‚àà (0, 1). œÑ ‚àó
2 are deÔ¨Åned in (20). As n is suÔ¨Éciently large, (32) achieves exact recovery i.e.
MSDP = ¬ØM ‚àó with probability 1 ‚àí n‚àí‚Ñ¶(1) if
(cid:26)

1 , œÑ ‚àó

(cid:27)

(cid:27)

< Œ± ‚â§ min

2œÑ ‚àó

1 , 2œÑ ‚àó

2 , œÑ ‚àó

1 + œÑ ‚àó

2 ‚àí

(cid:18) 1
‚àö
œÅ

+

‚àö

1
1 ‚àí œÅ

(cid:19)
(cid:112)(cid:96)Œ≤

,

(33)

max

(cid:26) 1
œÅ

,

1
1 ‚àí œÅ

where (cid:96) = 2 when d > 2 and (cid:96) = 1 when d = 2.

The proof of Theorem 3.12 is deferred to Appendix B.3, as it follows a similar structure as the

one presented in Section 3.1.1, with a slight diÔ¨Äerence on the construction of dual variables.

Rounding. In practice, we observe that an extra rounding procedure can improve the recovery of
M ‚àó. As an illustration, in Figure 2 we present an example with 2D rotation and two equal-sized
clusters. For the ease of presentation, we choose to use a complex number to represent each ¬ØM ‚àó
ij
such that ¬ØM ‚àó
ij| denotes the ‚Äúamplitude‚Äù and Œ∏ij ‚àà [0, 2œÄ] represents the
rotation angle. Then we form two n √ó n matrices | ¬ØM ‚àó| and ‚à† ¬ØM ‚àó that contain

ij|eƒ±Œ∏ij , where | ¬ØM ‚àó

ij = | ¬ØM ‚àó

amplitude: | ¬ØM ‚àó|ij := | ¬ØM ‚àó

ij|,

and rotation angle ‚à† ¬ØM ‚àó

ij := Œ∏ij ‚àí œÄ

(34)

respectively. A similar procedure is applied to A and MSDP. In Figures 2a and 2c, one can see
that |MSDP| (cid:54)= | ¬ØM ‚àó| and thus exact recovery fails. However, they have the same support on the
two clusters. Moreover, in Figures 2e and 2g we observe ‚à†MSDP = ‚à† ¬ØM ‚àó, meaning the rotational
alignments are exactly recovered. These observations inspire us to design the following rounding
procedure. For each block of MSDP denoted by (MSDP)ij, we round it to

(Mround)ij =

Ô£±
Ô£≤

Ô£≥

(MSDP)ij
(cid:107)(MSDP)ij(cid:107)F
0,

‚àö

¬∑

d,

(cid:107)(MSDP)ij(cid:107)F ‚â• (cid:15),

otherwise,

(35)

12

(a) | ¬ØM ‚àó|

(b) |A|

(c) |MSDP|

(d) |Mround|

(e) ‚à† ¬ØM ‚àó

(f) ‚à†A

(g) ‚à†MSDP

(h) ‚à†Mround

Figure 2: An example of rounding when d = 2. We plot the matrix of amplitudes ((a)‚Äì(d)) and
rotation angles ((e)‚Äì(h)) deÔ¨Åned in (34) for the ground truth ¬ØM ‚àó, the observation matrix A, the
SDP result MSDP by (32) and the rounding result Mround.

where (cid:15) is some small value for avoiding numerical issues. That is, for each non-zero block of MSDP
d and keep its ‚Äúphase‚Äù unchanged. Let
we normalize its ‚Äúamplitude‚Äù (Frobenius norm) to be
Mround be the rounded solution based on all blocks of MSDP, then in Figures 2d and 2h we obtain
Mround = M ‚àó which is equal to the unnormalized ground truth in (2). Therefore, empirically, we
observe exact recovery after an appropriate rounding.

‚àö

3.4 SDPs for general cluster structures

The SDP formulation can be easily extended to general cases with an arbitrary number of clusters.
When cluster sizes are known, the convex relaxation introduced in (19) still applies to M ‚àó in
principle, and we can formulate an SDP relaxation as

MSDP = arg max
M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0, Mii = Id,

(cid:88)

j

(cid:107)Mij(cid:107)F ‚â§ (max

i

‚àö

mi)

d,

i = 1, . . . , n,

(36)

n
(cid:88)

i,j=1

(cid:107)Mij(cid:107)F ‚â§

K
(cid:88)

‚àö

d

m2
k.

k=1

When the cluster sizes are unknown, an SDP similar to (32) can be designed as:

MSDP = max

M ‚ààRnd√ónd

(cid:104)A, M (cid:105)

s.t. M (cid:23) 0,

(cid:88)

i

Mii = KId,

‚àö

(cid:107)Mij(cid:107)F ‚â§

(cid:88)

j

d,

i = 1, . . . , n,

(37)

which only requires the number of clusters K as a parameter. Moreover, a similar rounding pro-
cedure as in Section 3.3 can be used to improve the solution by (37). Finding the exact recovery
guarantee for both (36) and (37) is an interesting but challenging task, which is left for future work.

13

(a) n = 50

(b) n = 100

(c) n = 150

(d) n = 150, by [19, Eq. (5)]

(e) Comparison of two thresholds

Figure 3: Results on two equal-sized clusters with known cluster size. (a)‚Äì(c) Recovery error of
MSDP by (9) with diÔ¨Äerent n. The thresholds in Theorem 3.1 for d = 2 and d > 2 are shown in
red solid curve and red dashed curve, respectively. (d) Recovery error by Ô¨Årst performing clustering
on graph scalar edge connection using the SDP in [19, Eq. (5)] and then synchronization, the exact
recovery threshold
2 (see [19, Theorem 2]) is shown in red. (e) Comparison of the
thresholds in Theorem 3.1 for d = 2 (blue) and [19, Theorem 2] (red), exact recovery is guaranteed
in the region below each threshold. The recovery error is deÔ¨Åned in (38).

Œ≤ =

Œ± ‚àí

‚àö

‚àö

‚àö

4 Numerical results

This section is devoted to empirically investigating the performance of the SDPs in Section 3. In
Sections 4.1‚Äì4.3, we focus on experiments with two clusters and 2D rotations (d = 2). In Section 4.4,
we include results for more general settings, e.g., 3D rotations and the number of clusters is larger
than two. We generate the observation matrix A based on the model introduced in Section 2. Then
we evaluate the SDP solution by measuring the following error metric:

Error = log((cid:107)MSDP ‚àí M ‚àó(cid:107)F).

(38)

When cluster sizes are unknown and the SDP in (32) is adopted, the ground truth M ‚àó in (38) should
be replaced by the normalized ¬ØM ‚àó deÔ¨Åned in (30). For the case of two clusters, we evaluate the
phase transition of the condition (cid:101)Œõ (cid:31) 0 according to our guess of dual variables (e.g. in Lemmas 3.3
and 3.9) via the following metric:

Failure rate = 1 ‚àí the rate that (cid:101)Œõ (cid:31) 0 is satisÔ¨Åed.

(39)

Failure rate = 0 means that our construction of the dual variables satisfy the optimality and unique-
ness condition for M ‚àó or ¬ØM ‚àó (e.g. in Lemmas 3.2, 3.8 and B.1) and the exact recovery is guaranteed.
For each experiment, we average the result over 10 diÔ¨Äerent realizations of the observation matrix
A.

14

)
0
4
,
0
6
(
=
)
2

m

,
1

m

(

)
0
2
,
0
8
(
=
)
2

m

,
1

m

(

(a) Error of MSDP

(b) Failure rate

Figure 4: Results on two unequal-sized clusters with known cluster sizes. (a) Recovery error of the
solution of (19); (b) The failure rate deÔ¨Åned in (39) for (cid:101)Œõ in Lemma 3.10. The threshold given in
Theorem 3.7 for d = 2 is shown in red.

4.1 Two equal-sized clusters

First, we consider two equal-sized clusters where the SDP in (9) is used for recovery. In Figures 3a‚Äì
3c we plot the recovery error with varying Œ± and Œ≤ for n = 50, 100, 150. In each plot, the red solid
curve corresponds to the threshold of the condition in Theorem 3.1 for d = 2 . As we can see, (9)
achieves exact recovery within a wide range of Œ± and Œ≤. Also, the theoretical threshold closely Ô¨Åts
the empirical phase transition boundary, especially as n is large, which indicates the sharpness of
the condition. In contrast, the red dash curve corresponds to the threshold of the conditions given
in Theorem 3.1 for d > 2, which is slightly loose by a constant factor

‚àö

2.

In addition, we consider using the two-stage approach mentioned in the introduction that Ô¨Årst
applies existing methods for community detection and then performs synchronization for each com-
munity separately. We use the SDP proposed in [19, Eq. (5)] to detect the cluster membership,
which is proved to achieve the information-theoretical limit as n ‚Üí ‚àû [2, 19]. As a result, Figure 4a
displays the corresponding recovery error with n = 150, which exhibits worse performance than
Figure 3c. In Figure 3e, we compare the theoretical thresholds for exact recovery in Theorem 3.1
for d = 2 and [19, Theorem 2]. It is clear to see that, by incorporating the additional pairwise
alignment information, our method outperforms the classical community detection method which
only exploits the edge connectivity.

4.2 Two unequal-sized clusters

Next, we consider two unequal-sized clusters where the cluster sizes (m1, m2) are given, and the
SDP in (19) is used for recovery. We simulate A under diÔ¨Äerent settings of (m1, m2), and plot the
recovery error with varying Œ± and Œ≤ in Figure 4. By comparing the results for (m1, m2) = (60, 40)
and (m1, m2) = (80, 20), we Ô¨Ånd that it gets more challenging to recover M ‚àó if the cluster sizes are

15

(a) (m1, m2) = (600, 400)

(b) (m1, m2) = (800, 200)

Figure 5: Results on two unequal-sized clusters with known cluster sizes, large n. We plot the failure
rate deÔ¨Åned in (39) for (cid:101)Œõ in Lemma 3.10. The threshold in Theorem 3.7 for d = 2 is shown in red.

more imbalanced. The red curve in each plot corresponds the threshold of the suÔ¨Écient conditions
given in Theorem 3.7 for d = 2. Clearly, there is a gap between the empirical phase transition
boundary and the theoretical threshold, which implies the condition in Theorem 3.7 may not be
necessary.

To understand why there exists such gap, we construct the dual variables according to our
construction in Lemma 3.9 for each realization of A, and in Figure 4b we plot the failure rate in
(39) for (cid:101)Œõ deÔ¨Åned in Lemma 3.10. As a result, we see that in some certain regime of Œ± and Œ≤,
even though our dual construction does not satisfy such conditions, empirically we still observe
exact recovery of (19) in Figure 4a. This implies our guess of the dual variables in Lemma 3.9 is
sub-optimal. In addition, to further verify the sharpness of the conditions in Theorem 3.7. As n
is large, we test the failure rate under diÔ¨Äerent setting of (m1, m2). As we can see in Figure 5,
the threshold of the condition in Theorem 3.7 sharply characterize the empirical phase transition
boundaries, which indicates the derived condition is tight based on our guessed dual variables.

4.3 Two clusters with unknown cluster sizes

Here we consider two clusters with unknown cluster sizes, where (32) is used for recovery. We
simulate A with diÔ¨Äerent settings of (m1, m2).
In Figure 6a we plot the recovery error of the
SDP solutions. Also, we apply the rounding step (35) to further reÔ¨Åne the SDP solutions and
plot the error after rounding in Figure 6b. As a result, one can see that the result after rounding
clearly exhibits certain improvement against the original one. In addition, to check whether our
construction of the dual variables in Lemma B.2 is optimal, in Figure 6c we show the failure rate in
(39) for (cid:101)Œõ in Lemma B.3. Again, we see that there is certain regime of Œ±, Œ≤ where exact recovery
by (32) is possible (see Figure 6a), but the constructed dual certiÔ¨Åcate fails to satisfy the condition,
indicating a lack of optimality in the choice of dual variables.

While our guess of the dual variables is not optimal, as n is large we are able to sharply
characterize when such a dual certiÔ¨Åcate fails to satisfy the condition in Lemma B.2. This is
demonstrated in Figure 7, where we show good agreement between the predicted boundary in
Theorem 3.12 and the empirical boundary of the failure rate.

4.4 3D rotations and K = 3

Here we extend our experiments to SO(3) group transformation. In Figure 8 we plot the recovery
errors by SDPs on diÔ¨Äerent scenarios under two clusters. Similar to the results for SO(2), we

16

)
0
5
,
0
5
(
=
)
2

m

,
1

m

(

)
0
2
,
0
8
(
=
)
2

m

,
1

m

(

(a) Error of MSDP

(b) Error of Mround

(c) Failure rate

Figure 6: Results on two clusters with unknown cluster sizes. (a) Recovery error of MSDP by (32);
(b) Recovery error of Mround by (35); (c) The failure rate deÔ¨Åned in (39) for (cid:101)Œõ in Lemma B.3.

(a) (m1, m2) = (600, 400)

(b) (m1, m2) = (800, 200)

Figure 7: Results on two clusters with unknown cluster sizes with large n. We plot the failure rate
deÔ¨Åned in (39) for (cid:101)Œõ in Lemma B.3. The threshold given in Theorem 3.12 for d = 2 is shown in
red.

observe sharp phase transitions in the SDP solutions. SpeciÔ¨Åcally, in Figures 8a and 8b we display
the thresholds for exact recovery derived in Sections 3.1 and 3.2, respectively. One can see that
exact recovery is still possible even above the thresholds, which implies there is still some room
for improvement of our conditions. Also, we test the generalized SDPs proposed in (36) and (37),
under the setting of K = 3 and (m1, m2, m3) = (25, 25, 25). The plots of recovery error are shown in
Figure 9. As we can see, the proposed SDPs achieves exact recovery with sharp phase transitions.
In addition, the rounding step is eÔ¨Äective to further improve the SDP solutions in both cases.

17

(a) Equal, by (9)

(b) Unequal, by (19)

(c) Unknown, by (32)

(d) Rounding, by (35)

Figure 8: Results on SO(3) group transformation. We plot the recovery error under (a) two equal-
sized clusters, the thresholds in Theorem 3.1 for d = 2 and d > 2 are shown in red solid and red dash
curve, respectively; (b) two unequal-sized clusters, the thresholds in Theorems 3.7 for d = 2 and
d > 2 are shown in red solid and red dash curve, respectively; (c)‚Äì(d) two clusters with unknown
cluster sizes. We test under (m1, m2) = (25, 25) for (a), and (m1, m2) = (30, 20) for (b)‚Äì(d).

(a) MSDP by (36)

(b) MSDP by (37)

(c) Mround by (35)

Figure 9: Results on three clusters. We test under the setting (m1, m2, m3) = (25, 25, 25). We plot
the recovery error under (a) known cluster sizes, recovered by (36); (b)‚Äì(c) unknown cluster sizes,
recovered by (37) and rounded by (35) respectively.

5 Conclusion and open problems

In this paper, we propose to solve the community detection and rotational synchronization simulta-
neously via semideÔ¨Ånite programming. Compared with the existing SDP for community detection
under SBM, our proposed SDPs not only take into account the graph structure but also the ad-
ditional information on pairwise rotational alignments, which gives better clustering results.
In
particular, for the case of two clusters we obtain suÔ¨Écient conditions for the SDPs to achieve exact
In addition, we acquire
recovery, which characterize the empirical phase transition boundaries.
new concentration inequalities for the Frobenius norm of sum of random rotation matrices and the
operator norm of a random block matrix as interesting byproducts from our analysis.

There are several directions to be further explored. First, it is natural to expect that the results
obtained in this paper can be extended to a much more general family of groups, such as symmetric
group and orthogonal group. Also our analysis can be applied to other probabilistic models on
the pairwise relations, such as the additive Gaussian model considered in [27].
In addition, the
conditions for exact recovery by (19) and (32) in the case of unequal and unknown cluster sizes can
be improved by Ô¨Ånding a more optimal construct of dual variables. For d > 2, current results can
be further improved with a Frobenius norm bound on the sum of random rotation matrices that
is tighter than the one given in Theorem A.3. Furthermore, we note that Ô¨Ånding the information-

18

theoretical limit of exact recovery on the proposed probabilistic model is still an open problem.
Finally, although the SDP can be solved in polynomial time complexity, it is still computationally
expensive when the graph size is large, and one can consider spectral methods and message passing
algorithms in that situation.

A Important technical ingredients

This section is devoted to technical ingredients that support our proofs in Appendix B. SpeciÔ¨Åcally,
we derive new matrix concentration inequalities for random rotation matrices in Appendix A.2, and
new upper bounds of the operator norm of random block matrices in Appendix A.3.

A.1 A tail bound of Bernoulli trials

Our analysis relies on the following tail bound of a Bernoulli trail.

Lemma A.1 ([19, Lemma 2]). Let X ‚àº Binom (m, Œ± log n/n) for m ‚àà N, Œ± = O(1), where m = œÅn
for some œÅ > 0. Let kn = œÑ œÅ log n for some œÑ ‚àà (0, Œ±]. Then for a suÔ¨Éciently large n,

P {X ‚â§ kn} = n‚àíœÅ(Œ±‚àíœÑ log( eŒ±

œÑ )+o(1)).

A.2 Concentration inequalities for random rotation matrices

Given a series of i.i.d. random matrices Xi ‚àà Rd√ód for i = 1, . . . m such that
(cid:40)

Xi =

Ri,
0,

with probability q,
otherwise,

(40)

where Ri is a random rotation matrix with det(Ri) = 1, uniformly drawn from SO(d) according
to Haar measure. Our analysis relies on bounding the Frobenius norm of Z = (cid:80)m
i=1 Xi, namely
(cid:107)Z(cid:107)F. In general, a fairly tight bound can be obtained from matrix concentration inequalities such
as matrix Bernstein [44, Theorem 1.6], which is presented as following:

Theorem A.2 (Matrix Bernstein, [44, Theorem 1.6]). Let X1, . . . , Xm ‚àà Rd√ód be independent,
centered random matrices and each one is uniformly bounded as

E[Xi] = 0 and

(cid:107)Xi(cid:107) ‚â§ L,

for i = 1, . . . , n.

Let Z =

n
(cid:80)
i=1

Xi and v(Z) denotes the matrix variance statistic of Z as

Then for any t > 0,

v(Z) = max{(cid:107)E(ZZ(cid:62))(cid:107), (cid:107)E(Z(cid:62)Z)(cid:107)}.

P {(cid:107)Z(cid:107) ‚â• t} ‚â§ 2d exp

(cid:18) ‚àít2/2

v(Z) + Lt/3

(cid:19)

.

As a result, by applying Theorem A.2 on Xi deÔ¨Åned in (40) we get:

19

Theorem A.3. Let X1, . . . , Xm ‚àà Rd√ód be i.i.d. random matrices deÔ¨Åned in (40) and Z =

m = O(n). Then for any t > 0,

P

(cid:110)

(cid:107)Z(cid:107)F ‚â•

‚àö

(cid:111)

dt

‚â§ 2d exp

(cid:18) ‚àít2/2

(cid:19)

qm + t/3

.

In other words,

(cid:107)Z(cid:107)F ‚â§ (cid:112)2qm(c log n + log 2d)

1
‚àö
d

(cid:32)(cid:115)

1 +

c log n + log 2d
18qm

+

(cid:115)

(cid:33)

c log n + log 2d
18qm

with probability 1 ‚àí n‚àíc for any c > 0.

m
(cid:80)
i=1

Xi,

(41)

(42)

Proof. By deÔ¨Ånition of the operator norm and Frobenius norm (cid:107)Z(cid:107) ‚â§ (cid:107)Z(cid:107)F ‚â§
it satisÔ¨Åes

‚àö

P{(cid:107)Z(cid:107)F ‚â•

dt} ‚â§ P {(cid:107)Z(cid:107)2 ‚â• t} ,

‚àö

d(cid:107)Z(cid:107). Therefore,

which enables us to bound (cid:107)Z(cid:107)F via (cid:107)Z(cid:107)2 by Theorem A.2. To this end, for L deÔ¨Åned in Theo-
rem A.2, we have (cid:107)Xi(cid:107) ‚â§ 1 then L = 1. For v(Z) we have (cid:107)E(Z(cid:62)Z)(cid:107) = (cid:13)
(cid:3)(cid:13)
(cid:13) =
(cid:13)
(cid:107)(cid:80)m
i=1 qId(cid:107) = qm, similarly (cid:107)E(ZZ(cid:62))(cid:107) = qm then it follows v(Z) = qm. This leads to (41).
Furthermore, by setting the RHS of (41) to be n‚àíc for c > 0 and solve it we get (42).

E (cid:2)X (cid:62)

i Xi

(cid:80)m

i=1

In our analysis, we are interested in the case of m = œÅn, q = Œ≤ log n/n for some œÅ, Œ≤ and n (cid:29) d.

Then (42) can be written as

(cid:107)Z(cid:107)F = (cid:112)2cœÅŒ≤

1
‚àö
d

(cid:18)(cid:114)

1 +

c
18œÅŒ≤

+

(cid:114) c

(cid:19)

18œÅŒ≤

log n ‚âà (cid:112)2cœÅŒ≤ log n

(43)

where we use the approximation

‚âà 1 since usually 18œÅŒ≤ (cid:29) c.

18œÅŒ≤
In particular, when the transformation is a 2-dimensional rotation i.e. d = 2, we are able to

(cid:16)(cid:113)

1 + c

18œÅŒ≤ +

(cid:113) c

(cid:17)

obtain a result sharper than Theorem A.3 as following:

Theorem A.4. Under the setting of Theorem A.3 with d = 2, for a suÔ¨Éciently large m

(cid:107)Z(cid:107)F ‚â§ (cid:112)cqm log n(1 + o(1))

1
‚àö
d

with probability 1 ‚àí n‚àíc for any c > 0.

Proof. To begin with, by deÔ¨Ånition of Xi in (40), we can rewrite Xi = riRi where ri is a Bernoulli
random variable such that P{ri = 1} = q. Moreover, when d = 2 each rotation matrix Ri can be

expressed as Ri =

for some Œ∏i ‚àà [0, 2œÄ). As a result,

(cid:18)cos Œ∏i ‚àí sin Œ∏i
cos Œ∏i

sin Œ∏i

(cid:19)

1
d

(cid:107)Zi(cid:107)2

F =

(cid:32) m
(cid:88)

i=1

(cid:33)2

ri sin Œ∏i

+

(cid:32) m
(cid:88)

i=1

(cid:33)2

ri cos Œ∏i

=: xm.

We are going to bound 1‚àö
d
xm i.e. E[xk

(cid:107)Zi(cid:107)F in terms of xm. Our technique is to compute the k-th moment of

m] for some integer k ‚â§ m followed by Markov‚Äôs inequality. First,

(cid:34) m
(cid:88)

E[xm] = E

m
(cid:88)

i=1

j=1

rirj (cos Œ∏i cos Œ∏j + sin Œ∏i sin Œ∏j)

=

(cid:35)

m
(cid:88)

m
(cid:88)

i=1

j=1

E [rirj] E [cos(Œ∏i ‚àí Œ∏j)] .

20

Next, in general E[xk

m] is given as

(cid:105)

(cid:104)

E

xk
m

=

m
(cid:88)

m
(cid:88)

i1,...,ik=1

j1,...,jk=1

E [ri1rj1 ¬∑ ¬∑ ¬∑ rik rjk ] E [cos(Œ∏i1 ‚àí Œ∏j1) ¬∑ ¬∑ ¬∑ cos(Œ∏ik ‚àí Œ∏jk )] .

(44)

Let si ‚àà {‚àí1, 1} denotes a discrete variable for i = 1, . . . , k ‚àí 1. Then applying the property
cos(Œ±1) cos(Œ±2) = 1
2 [cos(Œ±1 + Œ±2) + cos(Œ±1 ‚àí Œ±2)] recursively for k ‚àí 1 times yields

cos(Œ∏i1 ‚àí Œ∏j1 ) ¬∑ ¬∑ ¬∑ cos(Œ∏ik ‚àí Œ∏jk ) =

1
2k‚àí1

(cid:88)

cos (cid:0)(Œ∏i1 ‚àíŒ∏j1 )+s1(Œ∏i2 ‚àí Œ∏j2 )+¬∑ ¬∑ ¬∑+sk‚àí1(Œ∏ik ‚àíŒ∏jk )(cid:1) .

s1,...,sk‚àí1‚àà{‚àí1,1}

Plugging this into (44) gives

(cid:104)

E

xk
m

(cid:105)

=

(cid:18) 1

(cid:19)

2k‚àí1

(cid:88)

(cid:88)

(cid:88)

E (cid:2)ri1 rj1 ¬∑ ¬∑ ¬∑ rik rjk

(cid:3) E (cid:2)cos (cid:0)Œ∏i1 ‚àíŒ∏j1 +¬∑ ¬∑ ¬∑+Œ¥k‚àí1(Œ∏ik ‚àíŒ∏jk )(cid:1)(cid:3)

s1,...,sk‚àí1‚àà{‚àí1,1}

i1,...,ik

j1,...,jk

(a)
=

(cid:88)

(cid:88)

i1,...,ik

j1,...,jk

E (cid:2)ri1 rj1 ¬∑ ¬∑ ¬∑ rik rjk

(cid:3) E (cid:2)cos (cid:0)Œ∏i1 ‚àí Œ∏j1 + Œ∏i2 ‚àí Œ∏j2 + ¬∑ ¬∑ ¬∑ + Œ∏ik ‚àí Œ∏jk

(cid:1)(cid:3) .

(45)

Here, (a) holds since (cid:80)
i1,...,ik

(cid:80)
j1,...,jk

E [cos (Œ∏i1 ‚àíŒ∏j1 +¬∑ ¬∑ ¬∑+sk‚àí1(Œ∏ik ‚àíŒ∏jk ))] is identical for diÔ¨Äerent choices

of {si}k‚àí1
i=1 , and there are 2k‚àí1 number of choices in total then the factor 1/2k‚àí1 is cancelled by sum-
ming all of them, and we can focus on the case that s1 = s2 = ¬∑ ¬∑ ¬∑ = sk‚àí1 = 1. To proceed, let us
denote Si = {i1, . . . , ik} and Sj = {j1, . . . , jk} as two multisets (a multiset is a variant of a set that
allows repeated elements) that contain all i and j respectively, then

E [cos (Œ∏i1 ‚àí Œ∏j1 + Œ∏i2 ‚àí Œ∏j2 + ¬∑ ¬∑ ¬∑ + Œ∏ik ‚àí Œ∏jk )] =

(cid:40)

1,
0,

Si = Sj,
otherwise.

This holds because the sum of the random angles equals 0 mod 2œÄ if Si = Sj, otherwise it vanishes
by taking the expectation. Therefore, under Si = Sj, let ml denotes the multiplicity of element
l ‚àà {1, . . . , m} that either Si or Sj contains, then it satisÔ¨Åes

E[ri1rj1 ¬∑ ¬∑ ¬∑ rik rjk ]

(a)
= E[r2m1

1

r2m2
2

¬∑ ¬∑ ¬∑ r2mn

m ] =

m
(cid:89)

i=1

E[r2mi
i

]

(b)
= qœÑ (m1,...mm)

where œÑ (m1, . . . mm) denotes the number of non-zero elements in {m1, . . . , mm}, (a) holds because
of Si = Sj, and (b) comes from E[r2mi
] = q, ‚àÄmi > 0. Given the above, in (45) the summation over
all i and j can be instead summing over m1, . . . , mm. That is,

i

E[xk

m] =

(cid:88)

Œ†(m1, . . . mm)qœÑ (m1,...mm).

m1,...,mm‚â•0
m1+¬∑¬∑¬∑+mm=k

(46)

Here, Œ†(m1, . . . mm) denotes the number of combinations of {i1, j1, . . . , ik, jk} that satisfy Si =
Sj with multiplicity {m1, . . . , mm}. To determine it, notice that given a Ô¨Åxed {m1, . . . , mm}
there are (cid:0)
(cid:1) number of combinations for either Si or Sj that satisfy it. Then we have
Œ†(m1, . . . mm) = (cid:0)

. Plugging this into (46) gives

k
m1,m2,...,mm

(cid:1)2

k
m1,m2,...,mm

(cid:105)

(cid:104)

E

xk
m

=

(cid:88)

qœÑ (m1,...mm)

m1,...,mm‚â•0
m1+¬∑¬∑¬∑+mm=k

(cid:18)

k
m1, . . . , mm

(cid:19)2

=

k
(cid:88)

l=1

(cid:18)n
l

(cid:19)
ql (cid:88)

m1,...,ml‚â•1
m1+¬∑¬∑¬∑+ml=k

(cid:18)

k
m1, . . . , ml

(cid:19)2

,

21

where the RHS holds by Ô¨Åxing œÑ (m1, . . . , mm) = l for l = 1, . . . , k. Furthermore,

(cid:105)

(cid:104)

E

xk
m

=

(cid:19)

(cid:18)n
k

qk (cid:88)

(cid:18)

k
m1, . . . , mk

(cid:19)2

+

k‚àí1
(cid:88)

l=1

(cid:18)m
l

(cid:19)
ql (cid:88)

(cid:18)

k
m1, . . . , ml

(cid:19)2

m1,...,ml‚â•1
m1+¬∑¬∑¬∑+ml=k

m1,...,mk‚â•1
m1+...+mk=k
(cid:124)

(cid:123)(cid:122)
=(k!)2 since m1 = ¬∑ ¬∑ ¬∑ = mk = 1

(cid:125)

(cid:18)

=

m!
k!(m ‚àí k)!

(cid:19)

‚â§ c0k!qkmk

qk(k!)2 + O(qk‚àí1mk‚àí1) = k!qkmk + O(qk‚àí1mk‚àí1)

where the last inequality holds for any constant c0 > 1 with a suÔ¨Éciently large m.

Now we are ready to bound xm by applying Markov‚Äôs inequality as

P {xm ‚â• t} = P

(cid:110)

m ‚â• tk(cid:111)
xk

‚â§ t‚àíkE[xk

m] ‚â§ t‚àík ¬∑ c0k!qkmk ‚â§

‚àö

(cid:18) (c0e

(cid:19)k

k)1/kkqm
et

where the last inequality uses the fact that k! ‚â§ ekk+ 1
‚àö

‚àö

2 e‚àík. If we set k = (cid:100)c log n(cid:101) and

t = (c0e

k)1/kkqm = e

1

k log(c0e

k)kqm = (1 + o(1))cqm log n,

then we have

P {xn ‚â• t} ‚â§ n
By deÔ¨Ånition of xm, this further leads to 1‚àö
d
which completes the proof.

(cid:18)

1+log

‚àíc

(cid:19)

t

‚àö

k)1/k kqm

= n‚àíc.

(c0e
‚àö

(cid:107)Z(cid:107)F ‚â§

cqm log n(1 + o(1)) with probability 1 ‚àí n‚àíc,

1‚àö
d

Remark 1. When d = 2, we can interpret
(cid:107)Z(cid:107)F from a random walk perspective: Suppose a
random walk on a 2-D plane such that in each step, with probability q we take a unit-length step
towards a random direction uniformly drawn from [0, 2œÄ), and with probability 1 ‚àí q we stay where
we are. As a result, 1‚àö
(cid:107)Z(cid:107)F can be interpreted as the distance that we travel within m steps where
d
ri indicates whether we move or not in the i-th step, and Œ∏i stands for the corresponding direction.
In fact, such 2-D random walk with q = 1 has been originally studied by Rayleigh in [37, 36]. Let
pm(r) be the probability distribution of travelling a distance r with m steps, Rayleigh showed that
as m ‚Üí ‚àû,

2r
m

e‚àír2/m.

pm(r) ‚àº
‚àö

cm log n with probability 1 ‚àí n‚àíc for any c > 0. Im-
Therefore, asymptotically it satisÔ¨Åes r ‚â§
portantly, this bound agrees with the non-asymptotic result in Theorem A.4, which indicates the
sharpness of our result.

Lemma A.5. Let m1 = œÅn and m2 = (1 ‚àí œÅ)n with 0 < œÅ < 1, and Z1, . . . , Zm1 ‚àà Rd√ód be
Xj is independently generated as in Theorem A.3. Let
i.i.d. random matrices where each Zi =

m2(cid:80)
j=1

(cid:15)i := (cid:107)Zi(cid:107)F
‚àö
d

m2

. Then, when p, q = O(log n/n)

m1(cid:88)

i=1

(cid:15)i ‚â§

2c
3

log n + œÅ

(cid:114) qn
1 ‚àí œÅ

(cid:115)

+

2cqœÅ log n
1 ‚àí œÅ

=

2c
3

log n + o(log n)

with probability 1 ‚àí n‚àíc.

22

Proof. By deÔ¨Ånition, each (cid:15)i is bounded by 1, then applying Bernstein‚Äôs inequality [47, Theorem
2.8.4] yields

(cid:40) m1(cid:88)

P

((cid:15)i ‚àí E [(cid:15)i]) ‚â• t

(cid:41)

(cid:18)

‚â§ exp

‚àí

(cid:19)

t2/2
œÉ2 + Kt/3

where K = 1 is the boundary, and

i=1

œÉ2 =

m1(cid:88)

i=1

(cid:0)E[(cid:15)2

i ] ‚àí E[(cid:15)i]2(cid:1) ‚â§

m1(cid:88)

i=1

E[(cid:15)2

i ] =

m1(cid:88)

i=1

(cid:3)

E (cid:2)(cid:107)Zi(cid:107)2
m2
2d

F

=

m1(cid:88)

i=1

q
m2

=

qœÅ
1 ‚àí œÅ

is the variance of the sum. Then, by letting the RHS of (47) equal to n‚àíc for c > 0, we obtain

m1(cid:88)

i=1

((cid:15)i ‚àí E [(cid:15)i]) ‚â§

Kc
3

log n +

(cid:115)(cid:18) Kc
3

(cid:19)2

log n

+ 2œÉ2c log n

‚â§

2Kc
3

log n +

(cid:112)

2œÉ2c log n

with probability 1 ‚àí n‚àíc. Also, by Jensen‚Äôs inequality the expectation of (cid:15)i is bounded as

E[(cid:15)i] =

1
‚àö
m2

d

E

(cid:20)(cid:113)

Tr(Z(cid:62)

i Zi)

(cid:21)

‚â§

1
‚àö
m2

d

(cid:113)

E (cid:2)Tr(Z(cid:62)

i Zi)(cid:3) =

(cid:114) q

(1 ‚àí œÅ)n

.

(47)

(48)

Plugging this into (48) completes the proof.

A.3 Operator norm of random (block) matrices

The following existing result provides an upper bound on the spectrum of an Erd¬®os‚ÄìR¬¥enyi random
graph.

Theorem A.6 ([19, Theorem 5]). Let E ‚àà Rn√ón denotes the adjacency matrix of a graph generated
from the Erd¬®os‚ÄìR¬¥enyi model G(n, p), where the entries {Eij : i < j} are independent and {0, 1}-
valued. Assume that E[Eij] ‚â§ p where p = ‚Ñ¶(c0 log n/n). Then for any c > 0, there exists c1 > 0
such that,

with probability 1 ‚àí n‚àíc.

(cid:107)E ‚àí E[E](cid:107) ‚â§ c1

‚àö

np

Another important result is the bound the spectrum of a random block matrix with i.i.d random
blocks. That is, let S ‚àà Rm1d√óm2d be an m1 √óm2 block random matrix where each block Sij ‚àà Rd√ód
is i.i.d. such that

Sij =

(cid:40)

Rij,
0,

with probability q,
otherwise.

(49)

Here, Rij is a random rotation matrix uniformly drawn from SO(d). Then we have

Theorem A.7. Let S ‚àà Rm1d√óm2d be an m1 √ó m2 random block matrix with i.i.d block Sij deÔ¨Åned
in (49). Let m1, m2 = O(n). Then, for any c > 0, there exists c1, c2 > 0 such that
‚àö

‚àö

(cid:107)S(cid:107) ‚â§ c1(

qm1 +

qm2) + c2

(cid:112)log n.

with probability 1 ‚àí n‚àíc.

23

Proof. Our technique is based on the moment method which is broadly used in random matrix
theory (see e.g. [3, 43]). We start from bounding (cid:107)S(cid:107) by the trace of SS(cid:62) as

(cid:104)

(cid:107)S(cid:107)2k(cid:105)

E

= E

(cid:104)

(cid:107)(SS(cid:62))k(cid:107)

(cid:105)

‚â§ E

(cid:104)
Tr

(cid:16)

(SS(cid:62))k(cid:17)(cid:105)

which holds for any k ‚àà N. Then, E (cid:2)Tr (cid:0)(SS(cid:62))k(cid:1)(cid:3) can be expanded as

(cid:104)

E

Tr

(cid:16)

(SS(cid:62))k(cid:17)(cid:105)

=

(cid:88)

(cid:88)

(cid:104)

E

Tr

(cid:16)

i1,...,ik‚àà[m1]

j1,...,jk‚àà[m2]

Si1j1S(cid:62)

i2j1Si2j2S(cid:62)

i3j2 ¬∑ ¬∑ ¬∑ Sikjk S(cid:62)
i1jk

(cid:17)(cid:105)

(50)

where [m1] := {1, . . . , m1} and [m2] := {1, . . . , m2}. Let G(U, V ) be a complete bipartite graph
with the set of nodes U = [m1] and V = [m2]. Then each Sij for i ‚àà U, j ‚àà V is associated with an
edge (i, j) on the graph G. Furthermore, the matrix product Si1j1S(cid:62)
can be treated
i2j1
as a walk that goes back and forth along G and follows the path

¬∑ ¬∑ ¬∑ Sikjk S(cid:62)
i1jk

i1 ‚Üí j1 ‚Üí i2 ‚Üí j2 ‚Üí ¬∑ ¬∑ ¬∑ ‚Üí ik ‚Üí jk ‚Üí i1

which starts and ends at i1 as a cycle with 2k steps. With this in mind, let (i1, j1, . . . , ik, jk) denote
this walk, since E[Sk
ij] = 0 when k is odd, the sum in (50) can be restricted to even cycles on the
graph G where each distinct edge traversed by the walk should be visited by an even number of
times. Therefore, let W denote the set of all such walks, then
(cid:88)

(cid:17)(cid:105)

(cid:16)

(cid:16)

(cid:104)

E

Tr

(SS(cid:62))k(cid:17)(cid:105)

=

Si1j1S(cid:62)

i2j1Si2j2S(cid:62)

i3j2 ¬∑ ¬∑ ¬∑ Sikjk S(cid:62)
i1jk

(cid:104)
Tr

E

(i1,j1,...,ik,jk)‚ààW
(cid:88)

(a)
=

(cid:16)

(cid:104)
Tr

E

Ri1j1R(cid:62)

i2j1¬∑ ¬∑ ¬∑Rikjk R(cid:62)
i1jk

(cid:17)(cid:105)

E[ri1j1ri2j1¬∑ ¬∑ ¬∑rikjk ri1jk ]

(i1,j1,...,ik,jk)‚ààW

(b)
‚â§ d

(cid:88)

(i1,j1,...,ik,jk)‚ààW

E[ri1j1ri2j1 ¬∑ ¬∑ ¬∑ rikjk ri1jk ],

(51)

where (a) holds by rewriting Sij = rijRij for some Bernoulli random variable rij such that rij = 1
with probability q and rij = 0 otherwise, (b) holds because the product Ri1j1 ¬∑ ¬∑ ¬∑ Ri1jk is a rotation
matrix, therefore (cid:107)Ri1j1R(cid:62)
(cid:107) = 1 and

i2j1

¬∑ ¬∑ ¬∑ Rikjk R(cid:62)
(cid:16)

(cid:104)

i1jk
Ri1j1R(cid:62)

Tr

E

i2j1 ¬∑ ¬∑ ¬∑ Rikjk R(cid:62)
i1jk

(cid:17)(cid:105)

‚â§ d.

(52)

To proceed, we introduce a series of i.i.d. symmetric random variables tij as
Ô£±
Ô£¥Ô£≤

tij =

with probability q
2 ,
1,
‚àí1, with probability q
2 ,
0,

with probability 1 ‚àí q.

Ô£¥Ô£≥

for i ‚àà [m1], j ‚àà [m2]. As a result, tij has symmetric distribution such that E[tij]2k‚àí1 = 0 and
E[tij]2k = q, ‚àÄk ‚àà N. Moreover, the summation in (51) satisÔ¨Åes

(cid:88)

E[ri1j1ri2j1 ¬∑ ¬∑ ¬∑ rikjk ri1jk ] =

(cid:88)

E[ti1j1ti2j1 ¬∑ ¬∑ ¬∑ tikjk ti1jk ]

(i1,j1,...,ik,jk)‚ààW

(i1,j1,...,ik,jk)‚ààW

(cid:88)

(cid:88)

(a)
=

E[ti1j1ti2j1 ¬∑ ¬∑ ¬∑ tikjk ti1jk ]

i1,...,ik‚àà[m1]
(cid:16)

(cid:104)

= E

Tr

j1,...,jk‚àà[m2]

(T T (cid:62))k(cid:17)(cid:105)
(cid:107)T (cid:107)2k(cid:105)
(cid:104)

‚â§ min{m1, m2}E

24

.

(53)

Here, T ‚àà Rm1√óm2 is the random matrix whose (i, j)-th entry is tij, the equality (a) holds since any
walk (i1, j1, . . . , ik, jk) /‚àà W that visits some edge by odd times vanishes. Therefore, (53) enables us
to bound E (cid:2)Tr (cid:0)(SS(cid:62))k(cid:1)(cid:3) via E (cid:2)(cid:107)T (cid:107)2k(cid:3). To this end, by computing the following quantities:

œÉk,1 =

œÉk,2 =

(cid:32) m1(cid:88)

(cid:32) m2(cid:88)

i=1

j=1

(cid:32) m2(cid:88)

(cid:32) m1(cid:88)

j=1

i=1

E (cid:2)t2
ij

(cid:3)

E (cid:2)t2
ij

(cid:3)

(cid:33)k(cid:33)1/2k

(cid:33)k(cid:33)1/2k

= m1/2k
1

‚àö

m2q,

= m1/2k
2

‚àö

m1q,

œÉ‚àó
k =

(cid:32) m1(cid:88)

m2(cid:88)

i=1

j=1

(cid:33)1/2k

(cid:107)tij(cid:107)2k
‚àû

= (m1m2)1/2k.

Then applying [23, Theorem 4.9] yields the following bound on E (cid:2)(cid:107)T (cid:107)2k(cid:3):

(cid:107)T (cid:107)2k(cid:105)1/2k
(cid:104)

E

‚â§ œÉk,1 + œÉk,2 + C
‚àö

‚â§ n1/2k (

m1q +

‚àö

‚àö

kœÉ‚àó

k ‚â§ m1/2k
‚àö

2

m2q) + C

kn1/k

‚àö

m1q + m1/2k

1

‚àö

m2q + C

‚àö

k(m1m2)1/2k

for some universal constant C > 0. Furthermore, by setting k = (cid:100)Œ≥ log n(cid:101) for some Œ≥ > 0,

(cid:107)T (cid:107)2k(cid:105)1/2k
(cid:104)

E

‚â§ e1/2Œ≥ (

‚àö

m1q +

‚àö

m2q) + C(cid:48)(cid:112)log n

for some C(cid:48) > 0. Finally, by putting all the results together and using Markov inequality,

P{(cid:107)S(cid:107) ‚â• t} = P{(cid:107)S(cid:107)2k ‚â• t2k} ‚â§ t‚àí2kE

(cid:16)

(cid:104)
Tr

(cid:32)

‚â§ d min{m1, m2}

‚àö

e1/2Œ≥(

m1q +

(cid:33)2(cid:100)Œ≥ log n(cid:101)

log n

,

(SS(cid:62))k(cid:17)(cid:105)
‚àö

m2q) + C(cid:48)‚àö
t

for any t > 0. By setting Œ≥ = c
‚àö
t = c1(

m2q) + c2

m1q +

‚àö

‚àö

2 + 1, for any c > 0, we can identify some c1, c2 > 0 such that

log n and P{(cid:107)S(cid:107) ‚â• t} ‚â§ n‚àíc, which completes the proof.

Remark 2. It is worth noting that our result in Theorem A.7 is sharper than either the one by
using (cid:15)-net argument (e.g. [47, Theorem 4.6.1]) or the one by applying matrix Bernstein (e.g. [27,
Lemma 5.14]). If using (cid:15)-net argument, we get (cid:107)S(cid:107) ‚â§ c1
log n with high probability
‚àö
d in the leading term compared to our
for some c1, c2 > 0, which has an additional factor of
(cid:112)q max{m1, m2} (log nd + c2 log n) with high
result. If using matrix Bernstein, we obtain (cid:107)S(cid:107) ‚â§ c1
log nd compared to our result. These
probability for some c1, c2 > 0, which is loose by a factor
bounds are not adequate enough in our analysis and thus it is necessary to have a sharp result as
in Theorem A.7.

qnd + c2

‚àö

‚àö

‚àö

Proof of Lemma 3.6. Let us deÔ¨Åne

(cid:32)

A =

(cid:33)

(cid:101)A11
(cid:101)A(cid:62)
12

(cid:101)A12
(cid:101)A22

, E [A] ‚àí A =:

(cid:19)

(cid:18)S11 S12
S(cid:62)
12 S22

, Sin :=

(cid:19)

(cid:18)S11

0
0 S22

, Sout :=

(cid:18) 0 S12
S(cid:62)
0
12

(cid:19)

,

where the two diagonal blocks represent the two clusters, then by triangle inequality (cid:107)E[A] ‚àí A(cid:107) =
(cid:107)Sin + Sout(cid:107) ‚â§ (cid:107)Sin(cid:107) + (cid:107)Sout(cid:107). Moreover, notice (cid:107)Sin(cid:107) = max {(cid:107)S11(cid:107), (cid:107)S22(cid:107)} ‚â§ (cid:107)S11(cid:107) + (cid:107)S22(cid:107) and
(cid:107)Sout(cid:107) = (cid:107)S12(cid:107), it holds

(cid:107)E[A] ‚àí A(cid:107) ‚â§ (cid:107)S11(cid:107) + (cid:107)S22(cid:107) + (cid:107)S12(cid:107).

25

As a result, we can bound S11, S22 and S12 them separately. For S11, by deÔ¨Ånition S11 = E[ (cid:101)A11] ‚àí
(cid:101)A11. Then if we deÔ¨Åne E1 ‚àà Rm1√óm1 be the adjacency matrix of the subgraph on C1 which is
generated from the Erd¬®os‚ÄìR¬¥enyi model G(m1, p), (cid:101)A11 can be expressed as (cid:101)A11 = E1 ‚äó Id under the
assumption Ri = Id for i = 1, . . . , n, where ‚äó denotes the tensor product. Therefore, we have

‚àö

(cid:107)S11(cid:107) = (cid:107)E[ (cid:101)A11] ‚àí (cid:101)A11(cid:107) = (cid:107)(E[E1] ‚àí E1) ‚äó Id(cid:107) = (cid:107)E[E1] ‚àí E1(cid:107).
This enables us to study (cid:107)S11(cid:107) via (cid:107)E[E1] ‚àí E1(cid:107), which is bounded by Theorem A.6 as (cid:107)S11(cid:107) ‚â§
pm1 with probability 1 ‚àí n‚àíc for some c1, c > 0. (cid:107)S22(cid:107) is bounded in a similar manner and we
c1
do not repeat. For (cid:107)S12(cid:107), we apply Theorem A.7 and get (cid:107)S12(cid:107) ‚â§ c2(
log n)
with probability 1 ‚àí n‚àíc for some c2, c > 0. Combining these completes the proof.

qm2) + O(

qm1 +

‚àö

‚àö

‚àö

B Proof of main results

This section is devoted to the proofs of theorems in Section 3. In Sections B.1 and B.2 we provide
detailed proofs for the lemmas and theorems presented in Sections 3.1.1 and 3.2.1 respectively. In
Section B.3 we consider the case of two clusters with unknown cluster sizes.

B.1 Two equal-sized clusters with known cluster sizes

Proof of Lemma 3.2. The optimality is immediately established since (9) is convex then KKT con-
ditions are suÔ¨Écient for M ‚àó being optimal [8]. For the uniqueness, suppose there exists another
optimal solution (cid:102)M (cid:54)= M ‚àó, then the following holds:

(a)
= (cid:104)A, (cid:102)M ‚àí M ‚àó(cid:105)

0

i=1), (cid:102)M ‚àí M ‚àó(cid:105) + (cid:104)Œò + Œò(cid:62), (cid:102)M ‚àí M ‚àó(cid:105)

(b)
= ‚àí(cid:104)Œõ, (cid:102)M ‚àí M ‚àó(cid:105) ‚àí (cid:104)diag({Zi}n
(c)
= ‚àí(cid:104)Œõ, (cid:102)M ‚àí M ‚àó(cid:105) + (cid:104)Œò + Œò(cid:62), (cid:102)M ‚àí M ‚àó(cid:105)
(d)
= ‚àí(cid:104)Œõ, (cid:102)M (cid:105) + (cid:104)Œò + Œò(cid:62), (cid:102)M ‚àí M ‚àó(cid:105)
(e)
= ‚àí(cid:104)Œõ, (cid:102)M (cid:105) + 2(cid:104)Œò, (cid:102)M ‚àí M ‚àó(cid:105),

(54)

where (a) holds since both (cid:102)M and M ‚àó are optimal and they should share the same primal value, i.e.,
(cid:104)A, (cid:102)M (cid:105) = (cid:104)A, M ‚àó(cid:105); (b) follows from (11); (c) comes from the constraint in (9) that M ‚àó
ii = (cid:102)Mii = Id,
for i = 1, . . . , n; (d) uses (cid:104)Œõ, M ‚àó(cid:105) = 0 in (13); and (e) holds because both (cid:102)M and M ‚àó are
symmetric. To proceed, let us rewrite

(cid:104)Œò, (cid:102)M ‚àí M ‚àó(cid:105) = (cid:104)Œò, (cid:102)M (cid:105) ‚àí (cid:104)Œò, M ‚àó(cid:105) =

(cid:88)

(cid:104)Œòij, (cid:102)Mij(cid:105) ‚àí

i,j

(cid:104)Œòij, M ‚àó

ij(cid:105).

(cid:88)

i,j

(55)

Then by plugging the explicit expression of Œòij in (12) into (55) we obtain
¬µi‚àö
d

(cid:104)Œòij, M ‚àó
ij(cid:105)

ij, M ‚àó

ij(cid:105) =

(cid:104)M ‚àó

¬µim

(a)
=

(cid:88)

(cid:88)

(cid:88)

(cid:88)

‚àö

d,

i

(cid:104)Œòij, (cid:102)Mij(cid:105)

(cid:88)

(b)
=

¬µi

i

j:C(j)=C(i)
(cid:18) (cid:88)

i,j

(cid:88)

i,j

i

j:C(j)=C(i)

(c)
‚â§

(cid:88)

¬µi

(cid:18) (cid:88)

i

j:C(j)=C(i)

1
‚àö
d

1
‚àö
d

(cid:104)M ‚àó

ij, (cid:102)Mij(cid:105) +

(cid:88)

(cid:19)

(cid:104)Œ±ij, (cid:102)Mij(cid:105)

j:C(j)(cid:54)=C(i)

(cid:107)M ‚àó

ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F +

(cid:88)

(cid:107)Œ±ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F

j:C(j)(cid:54)=C(i)

(cid:19)

(56)

(d)
‚â§

(cid:88)

¬µi

(cid:88)

(cid:107) (cid:102)Mij(cid:107)F

(e)
‚â§

(cid:88)

‚àö

d

¬µim

i

j

i

26

where both (a) and (b) hold because M ‚àó
ij = 0, C(i) (cid:54)= C(j); (c) follows from Cauchy-Schwartz
inequality; (d) uses (cid:107)Œ±ij(cid:107)F ‚â§ 1 in (12); (e) comes from (cid:80)
d, for i = 1, . . . , n in (9).
As a result, combining (55) and (56) yields (cid:104)Œò, (cid:102)M ‚àí M ‚àó(cid:105) ‚â§ 0. Plugging this back into (54) gives
(cid:104)Œõ, (cid:102)M (cid:105) ‚â§ 0. On the other hand, from Œõ (cid:23) 0 in (14) and (cid:102)M (cid:23) 0 in (9) we have (cid:104)Œõ, (cid:102)M (cid:105) ‚â• 0. Then
it holds that (cid:104)Œõ, (cid:102)M (cid:105) = 0. With this in mind, combining the assumption N (Œõ) = R(M ‚àó) with
M ‚àó = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) in (4), (cid:102)M satisÔ¨Åes

j (cid:107) (cid:102)Mij(cid:107)F ‚â§ m

‚àö

(cid:102)M = V (1)Œ£1(V (1))(cid:62) + V (2)Œ£2(V (2))(cid:62)

for some d√ód diagonal matrices Œ£1, Œ£2 (cid:23) 0. Furthermore, from the constraint in (9) that (cid:102)Mii = Id,
we obtain Œ£1 = Œ£2 = Id, then (cid:102)M = V (1)(V (1))(cid:62) + V (2)(V (2))(cid:62) = M ‚àó which contradicts with the
assumption (cid:102)M (cid:54)= M ‚àó. Therefore, M ‚àó is the unique solution.

Proof of Lemma 3.3. Recall the assumption Ri = Id, i = 1, . . . , n. From (11) we get Œõ = ‚àíA ‚àí
diag ({Zi}n

i=1) + Œò + Œò(cid:62), plugging this into (15) yields

(cid:88)

j:C(j)=C(i)
(cid:88)

j:C(j)(cid:54)=C(i)

Œòij + Œò(cid:62)

ji ‚àí Aij = Zi,

i = 1, . . . , n

Œòij + Œò(cid:62)

ji ‚àí Aij = 0,

i = 1, . . . , n.

To determine Œòij, notice that (12) can be written as

Œòij =

(cid:40)

‚àö

d,

ij/

¬µiM ‚àó
C(i) = C(j),
¬µiŒ±ij, (cid:107)Œ±ij(cid:107)F ‚â§ 1, C(i) (cid:54)= C(j),

(57)

(58)

(59)

where we use the fact M ‚àó
C(i) (cid:54)= C(j). Importantly, our ansatz of Œòij is of the following form:

ij (cid:54)= 0 if and only if C(i) = C(j). Then let us consider the case when

(60)

(61)

(62)

for some (cid:101)Œ±i. To solve it, plugging (60) into (58) yields
(cid:88)

(cid:88)

Œòij = (cid:101)Œ±i,

‚àÄj : C(j) (cid:54)= C(i),

(cid:101)Œ±i =

1
m

Aij ‚àí

1
m

Ajs ‚àí

1
m

j‚ààC2
(cid:88)

s‚ààC1

j‚ààC2
(cid:88)

s‚ààC1

(cid:101)Œ±(cid:62)
j ,

(cid:101)Œ±(cid:62)
s ,

i ‚àà C1,

j ‚àà C2.

(cid:101)Œ±j =

1
m

By combining (61) and (62) we get

(cid:101)Œ±i ‚àí

(cid:101)Œ±j ‚àí

1
m

1
m

(cid:88)

s‚ààC1
(cid:88)

s‚ààC2

(cid:101)Œ±s =

(cid:101)Œ±s =

1
m

1
m

(cid:88)

j‚ààC2
(cid:88)

i‚ààC1

Aij ‚àí

Aji ‚àí

1
m2

1
m2

(cid:88)

(cid:88)

s‚ààC1
(cid:88)

j‚ààC2
(cid:88)

s‚ààC2

i‚ààC1

Asj,

i ‚àà C1,

Asi,

j ‚àà C2.

Then it is natural to guess (cid:101)Œ±i for i ‚àà C1 and (cid:101)Œ±j for j ‚àà C2 has the form

(cid:101)Œ±i =

1
m

(cid:88)

s‚ààC2

Ais ‚àí Œ±(1),

i ‚àà C1,

(cid:101)Œ±j =

1
m

(cid:88)

s‚ààC2

Ajs ‚àí Œ±(2),

j ‚àà C2

27

for some Œ±(1) and Œ±(2). By further plugging this into (61) or (62) we obtain

Œ±(1) + (Œ±(2))(cid:62) =

1
m2

(cid:88)

(cid:88)

As1s2.

s1‚ààC1

s2‚ààC2

(cid:80)
By symmetry we guess Œ±(1) = (Œ±(2))(cid:62) = 1
As1s2, which leads to our guess of
2m2
(cid:101)Œ±i given in Lemma 3.3. Now it remains to determine ¬µi: From (59) and (60) we see that when
C(i) (cid:54)= C(j), (cid:107)Œòij(cid:107)F = ¬µi(cid:107)Œ±ij(cid:107)F = (cid:107) (cid:101)Œ±i(cid:107)F. Also, due to the constraint in (12) that (cid:107)Œ±ij(cid:107)F ‚â§ 1, ¬µi
should satisfy ¬µi ‚â• (cid:107) (cid:101)Œ±i(cid:107)F. Then we guess ¬µi = (cid:107) (cid:101)Œ±i(cid:107)F. As we shall see in Lemma 3.4, this is to
make ¬µi as small as possible such that the eigenvalues of Œõ are greater and the condition Œõ (cid:23) 0
beneÔ¨Åts from it. To complete the proof, plugging Œòij in (69) with M ‚àó
ij = Id into (57) yields the
form of Z.

s1‚ààC1

s2‚ààC2

(cid:80)

Proof of Lemma 3.4. Let us rewrite A, Œõ and Œò into four blocks as
(cid:32)

(cid:33)

(cid:32)

A =

(cid:101)A11
(cid:101)A(cid:62)
12

(cid:101)A12
(cid:101)A22

, Œõ =

, Œò =

(cid:19)

(cid:18)Œõ11 Œõ12
Œõ21 Œõ22

(cid:33)

(cid:101)Œò11
(cid:101)Œò21

(cid:101)Œò12
(cid:101)Œò22

(63)

where the two diagonal blocks represent C1 and C2 respectively. By assumption the two diagonal
i=1) ‚àà Rmd√ómd and
blocks of M ‚àó in (4) satisfy (cid:102)M ‚àó
i=m+1) ‚àà Rmd√ómd be two diagonal block matrices whose diagonal blocks are ¬µiId
(cid:101)¬µ2 = diag({¬µiId}n
for i ‚àà C1 and i ‚àà C2 respectively. Then from Lemma 3.3 each block of Œò can be expressed as

m) ‚äó Id. Let (cid:101)¬µ1 = diag({¬µiId}m

2 = (1m1(cid:62)

1 = (cid:102)M ‚àó

(cid:101)Œò11 = (cid:101)¬µ1 (cid:102)M ‚àó
1‚àö
d

,

1
2m2 (cid:102)M ‚àó
Given the above, from Lemma 3.3, each block of Œõ can be expressed as

2 (cid:101)A21 (cid:102)M ‚àó
1 ,

(cid:101)A21 (cid:102)M ‚àó

(cid:101)Œò21 =

1
m

1 ‚àí

.

(cid:101)A12 (cid:102)M ‚àó

2 ‚àí

1
2m2 (cid:102)M ‚àó

1 (cid:101)A12 (cid:102)M ‚àó
2 ,

(cid:101)Œò12 =

1
m
(cid:101)Œò22 = (cid:101)¬µ2 (cid:102)M ‚àó
2‚àö
d

(cid:101)Œõ12 = ‚àí (cid:101)A12 + (cid:101)Œò12 + (cid:101)Œò(cid:62)

21 = ‚àí

(cid:101)Œõ21 = ‚àí (cid:101)A21 + (cid:101)Œò21 + (cid:101)Œò(cid:62)

12 = ‚àí

(cid:18)

Imd ‚àí

(cid:18)

Imd ‚àí

(cid:19)

(cid:19)

1
m
1
m

(cid:102)M ‚àó
1

(cid:102)M ‚àó
2

(cid:18)

(cid:101)A12

Imd ‚àí

(cid:18)

(cid:101)A21

Imd ‚àí

(cid:19)

(cid:19)

,

,

(cid:102)M ‚àó
2

(cid:102)M ‚àó
1

1
m
1
m

(cid:101)Œõ11 = ‚àí (cid:101)A11 ‚àí diag({Zi}m
(cid:18)(cid:26)(cid:18) m¬µi‚àö
d

= ‚àí (cid:101)A11 ‚àí diag

i=1) +

1
‚àö

1 +

d (cid:101)¬µ1 (cid:102)M ‚àó
(cid:18) ¬µs‚àö
d

(cid:88)

+

s‚ààC1

1
(cid:102)M ‚àó
‚àö
d
(cid:19)(cid:19)

1 (cid:101)¬µ(cid:62)
(cid:27)m

1

(cid:19)

‚àí ris

Id

(cid:18)

=

Imd ‚àí

(cid:18)

(cid:101)Œõ22 =

Imd ‚àí

(cid:19) (cid:18)

(cid:19) (cid:18)

1
m
1
m

(cid:102)M ‚àó
1

(cid:102)M ‚àó
2

‚àí (cid:101)A11 ‚àí diag ({Zi}m

i=1)

‚àí (cid:101)A22 ‚àí diag (cid:0){Zi}n

i=m+1

(cid:1)

Imd ‚àí

(cid:19)

.

(cid:102)M ‚àó
2

1
m

1
‚àö

d (cid:101)¬µ1 (cid:102)M ‚àó

1 +

1
‚àö
d

(cid:102)M ‚àó

1 (cid:101)¬µ(cid:62)

1

(cid:19)

,

(cid:102)M ‚àó
1

+

1
m

i=1

(cid:19) (cid:18)

Imd ‚àí

(cid:19) (cid:18)

Then, Œõ can be simply expressed as

Œõ = (Ind ‚àí Œ†) (‚àíZ ‚àí A) (Ind ‚àí Œ†).

(64)

To proceed, notice that the expectation of A is given as

E[A] =

(cid:32)E[ (cid:101)A11] E[ (cid:101)A12]
E[ (cid:101)A21] E[ (cid:101)A22]

(cid:33)

(cid:32)

=

p (cid:102)M ‚àó

1 ‚àí pImd

0

0

p (cid:102)M ‚àó

2 ‚àí pImd

(cid:33)

(cid:32)

=

p (cid:102)M ‚àó
1
0

(cid:33)

0
p (cid:102)M ‚àó
2

‚àí pInd,

28

where pInd comes from the convention Aii = 0, i = 1, . . . , n. Then one can see that

(Ind ‚àí Œ†)(E[A] + pInd)(Ind ‚àí Œ†) = 0.

(65)

Therefore, by plugging (65) into (64) we obtain Œõ = (Ind ‚àí Œ†) (cid:101)Œõ(Ind ‚àí Œ†) with (cid:101)Œõ deÔ¨Åned in
Lemma 3.3. This yields two observations: (1) Œõ (cid:23) 0 is satisÔ¨Åed if (cid:101)Œõ (cid:23) 0; (2) Ind ‚àí Œ† is a projection
matrix such that N (Ind ‚àí Œ†) = R(M ‚àó), which implies N (Œõ) ‚äá R(M ‚àó). When (cid:101)Œõ (cid:31) 0, for all
x /‚àà R(M ‚àó), Œõx (cid:54)= 0 since x /‚àà N (Ind ‚àí Œ†). Combining it with the fact that N (Œõ) ‚äá R(M ‚àó), we
have N (Œõ) = R(M ‚àó) if (cid:101)Œõ (cid:31) 0. As a result, (cid:101)Œõ (cid:31) 0 ensures both Œõ (cid:23) 0 and N (Œõ) = R(M ‚àó) are
satisÔ¨Åed.

Proof of Lemma 3.5. According to Lemma 3.4, notice that Z is a diagonal matrix, then Œªmin(pInd ‚àí
Z) turns out to be the smallest diagonal entry of pInd ‚àí Z. This leads to

Œªmin(pInd ‚àí Z) = min

i

(cid:0)xi ‚àí yi ‚àí (cid:15)C(i)

(cid:1) + p ‚â• min
i

xi ‚àí max

i

yi ‚àí max{(cid:15)1, (cid:15)2} + p.

with xi, yi, (cid:15)1, (cid:15)2 deÔ¨Åned in Lemma 3.5, and we can bound them separately.

For xi, recall that ris is a Bernoulli random variable with P{ris = 1} = p. Then xi ‚àº
Binom (m, p) that follows a binomial distribution. Applying the existing tail bound in Lemma A.1
with œÅ = 1/2 and the union bound yields the result for mini xi.

For yi, by deÔ¨Ånition

yi =

‚â§

m(cid:107) (cid:101)Œ±i(cid:107)F‚àö
d
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
‚àö
d

=

m
‚àö
d

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:88)

Aij

(cid:124)

j:C(j)(cid:54)=C(i)
(cid:123)(cid:122)
=:yi1

(cid:88)

+

j:C(j)(cid:54)=C(i)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

1
‚àö
2m

(cid:124)

Aij ‚àí

1
2m2

(cid:88)

(cid:88)

j:C(j)(cid:54)=C(i)

s:C(s)=C(i)

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

d

(cid:88)

(cid:88)

s:C(s)=C(i)

j:C(j)(cid:54)=C(i)
(cid:123)(cid:122)
=:yi2

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

= yi1 + yi2.

As a result, both yi1 and yi2 consist of (cid:107) (cid:80)
i Xi(cid:107) where each Xi ‚àà Rd√ód is i.i.d. and uniformly
distributed in SO(d). This can be bounded by matrix concentration inequalities such as matrix
Bernstein [45] in general. Therefore, by applying Theorem A.3 we obtain

yi1 ‚â§ (cid:112)2qm(c log n + log 2d)

(cid:32)(cid:115)

1 +

c log n + log 2d
18qm

+

(cid:115)

(cid:33)

c log n + log 2d
18qm

(66)

(a)

‚âà (cid:112)cŒ≤ log n

with probability 1 ‚àí n‚àíc for any c > 0, where (a) comes from (43). Similarly, by applying The-
orem A.3 on yi2 we get yi2 = o(log n) with high probability. Combining the results above and
applying the union bound yields the result for maxi yi.

For (cid:15)1 and (cid:15)2, by symmetry they should have the same statistics, so let us focus on (cid:15)1, which

satisÔ¨Åes

(cid:15)1 =

1
‚àö
d

(cid:88)

s‚ààC1

¬µs =

1
‚àö
d

(cid:88)

s‚ààC1

(cid:107) (cid:101)Œ±s(cid:107)F ‚â§

1
‚àö
m
(cid:124)

d

= (cid:15)(1)

1 + (cid:15)(2)
1 .

(cid:88)

(cid:88)

s‚ààC1

j‚ààC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:123)(cid:122)
=:(cid:15)(1)
1

Asj

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

+

1
‚àö
2m
(cid:124)

d

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

Asj

s‚ààC1

j‚ààC2
(cid:123)(cid:122)
=:(cid:15)(2)
1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:125)

29

Then, by applying Lemma A.5 on (cid:15)(1)
1 = (3c/2) log n + o(log n) and
(cid:15)(2)
1 = yi2 = o(log n) with probability 1 ‚àí n‚àíc, this leads to the bound on (cid:15)1 and (cid:15)2. By further
applying the union bound we get the result on max{(cid:15)1, (cid:15)2}.

1 with m1 = m2 = m we get (cid:15)(1)

SpeciÔ¨Åcally when d = 2, a tighter bound on yi is obtained by applying Theorem A.4 instead of
Theorem A.3 for bounding yi1 and yi2, which yields yi1 ‚â§ (cid:112)cŒ≤/2 log n + o(log n) and yi2 = o(log n)
with probability 1 ‚àí n‚àíc. The remaining is the save as above.

B.2 Two unequal-sized clusters with known cluster sizes

Proof of Lemma 3.8. The proof is very similar to the one for Lemma 3.2, with a tiny diÔ¨Äerence on
evaluating (cid:104)Œò, (cid:102)M ‚àí M ‚àó
ij(cid:105) in (56), where in this case ¬µi is replaced by ¬µi + ŒΩ. Therefore we do not
repeat.

Proof of Lemma 3.9. First, we obtain Œõ = ‚àíA ‚àí diag({Zi}n
this into (27) yields

i=1) + Œò + Œò(cid:62) from (22), then plugging

(cid:88)

j:C(j)=C(i)
(cid:88)

j:C(j)(cid:54)=C(i)

Œòij + Œò(cid:62)

ji ‚àí Aij = Zi,

i = 1, . . . , n

Œòij + Œò(cid:62)

ji ‚àí Aij = 0,

i = 1, . . . , n.

Next, to determine Œòij, (23) can be rewritten as

(cid:40)

Œòij =

C(i) = C(j),
(¬µi + ŒΩ)Id/
(¬µi + ŒΩ)Œ±ij, (cid:107)Œ±ij(cid:107)F ‚â§ 1, C(i) (cid:54)= C(j).

d,

‚àö

(67)

(68)

(69)

Then one can check that our guess of Œòij in Lemma 3.9 indeed satisÔ¨Åes (68) and (69). It remains
to determine ŒΩ and ¬µi for i = 1, . . . , n: From (25) we get ¬µi = 0, ‚àÄi ‚àà C2, since m1 > m2. Then
from (69), due to the constraint (cid:107)Œ±ij(cid:107)F ‚â§ 1, ŒΩ and ¬µi satisfy

ŒΩ + ¬µi ‚â• (cid:107)Œòij(cid:107)F,

‚àÄi ‚àà C1

and ŒΩ ‚â• (cid:107)Œòij(cid:107)F,

‚àÄi ‚àà C2.

This leads to our guess of ŒΩ and ¬µi in Lemma 3.9. Plugging this back into (67) we obtain the
expression of Z.

Proof of Lemma 3.10. We follow the same route in Lemma 3.4 by Ô¨Årst rewriting A, Œõ and Œò into
four blocks as (63). Let (cid:101)¬µ1 := diag({¬µiId}m1
i=1) ‚àà Rm1d√óm1d. Then one can see that the four blocks
of Œò satisfy

(cid:101)Œò11 =

(cid:101)Œò12 + (cid:101)Œò(cid:62)

1

( (cid:101)¬µ1 + ŒΩIm1d) (cid:102)M ‚àó
‚àö
d
(cid:102)M ‚àó

21 =

1
m1

1 (cid:101)A21 +

,

(cid:101)Œò22 =

ŒΩ (cid:102)M ‚àó
2‚àö
d

,

1
m2

(cid:101)A12 (cid:102)M ‚àó

2 ‚àí

1
m1m2

(cid:102)M ‚àó

1 (cid:101)A12 (cid:102)M ‚àó
2 .

By plugging these into the four blocks of Œõ, one can simplify Œõ as (we leave the detail to readers
who are interested)

Œõ = (Ind ‚àí Œ†)(‚àíZ ‚àí A)(Ind ‚àí Œ†) = (Ind ‚àí Œ†)(E[A] ‚àí A ‚àí Z + pInd)(Ind ‚àí Œ†)

where we use (65) that (Ind ‚àí Œ†) (E[A] + pInd) (Ind ‚àí Œ†) = 0. As a result, by using the same
argument as in the proof of Lemma 3.4, one can see that (cid:101)Œõ (cid:31) 0 ensures both Œõ (cid:23) 0 and N (Œõ) =
R(M ‚àó) are satisÔ¨Åed.

30

Proof of Lemma 3.11. We Ô¨Årst deÔ¨Åne the following quantities and provide their individual analysis.

œÉ1 := max
j‚ààC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1

(cid:88)

s1‚ààC1

As1j

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

œÉ2 := max
i‚ààC1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

(cid:88)

s2‚ààC1

œÉ :=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1m2

(cid:88)

(cid:88)

As1s2

s1‚ààC1

s2‚ààC2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

, œÉ(i) :=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(cid:88)

s2‚ààC2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

.

By applying Lemma A.3 with the approximation in (43) and the union bound, one can see that
with probability 1 ‚àí n‚àíc, œÉ1, œÉ2 and œÉ are bounded by

(cid:115)

œÉ1 ‚â§

2(1 + c)Œ≤d
œÅ

(cid:18) log n
n

(cid:19)
,

(cid:115)

œÉ2 ‚â§

2(1 + c)Œ≤d
(1 ‚àí œÅ)

(cid:18) log n
n

(cid:19)
,

and œÉ = o(log n/n). Then, the dual variable ŒΩ given in Lemma 3.9 is bounded as

ŒΩ

(a)
‚â§ (1 ‚àí Œ≥) max

i‚ààC2,j‚ààC1

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1

(cid:88)

s1‚ààC1

Ais1

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(b)

= (1 ‚àí Œ≥) (œÉ1 + œÉ2 + œÉ) ‚â§ (1 ‚àí Œ≥)(cid:112)2(1 + c)Œ≤d

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
1
(cid:13)
(cid:13)
m1m2
(cid:13)
(cid:19)(cid:18) log n

(cid:88)

As2j

s2‚ààC2
(cid:18) 1
‚àö
œÅ

+

‚àö

1
1 ‚àí œÅ

(cid:88)

(cid:88)

As1s2

s1‚ààC1
(cid:19)

+ o

s2‚ààC2
(cid:18) log n
n

(cid:19)

n

(70)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(71)
with probability 1 ‚àí n‚àíc, where (a) holds by triangle inequality and (b) uses the fact Aij = A(cid:62)
ji.
Similarly, we can bound maxj‚ààC2 (cid:107)Œòij(cid:107)F in Lemma 3.9 for any i ‚àà C1 as

max
j‚ààC2

(cid:107)Œòij(cid:107)F ‚â§ Œ≥

= Œ≥

(cid:18)

(cid:16)

(cid:88)

max
j‚ààC2

(cid:13)
1
(cid:13)
(cid:13)
m1
(cid:13)
œÉ1 + œÉ + œÉ(i)(cid:17)

s1‚ààC1

.

As1j

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m2

(cid:88)

s2‚ààC2

Ais2

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m1m2

(cid:88)

(cid:88)

s1‚ààC1

s2‚ààC2

As1s2

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

Therefore, ¬µi + ŒΩ given in Lemma 3.9 satisÔ¨Åes

¬µi + ŒΩ = max

(cid:26)

max
j‚ààC2

(cid:27)

(cid:26)

(cid:107)Œòij(cid:107)F, ŒΩ

‚â§ max

Œ≥

‚â§ max {Œ≥œÉ1, (1 ‚àí Œ≥)(œÉ1 + œÉ2)} + Œ≥

(cid:16)

(cid:16)

œÉ1 + œÉ + œÉ(i)(cid:17)
œÉ + œÉ(i)(cid:17)

.

, (1 ‚àí Œ≥) (œÉ1 + œÉ2 + œÉ)

(cid:27)

Now we are ready to bound Œªmin(pInd ‚àí Z). According to Lemma 3.9, by deÔ¨Åning

(cid:15)1 := min
i‚ààC1

(cid:20) (cid:88)

(cid:18)

ris ‚àí

s‚ààC1

¬µs + ŒΩ
‚àö
d

(cid:19)

‚àí

m1(¬µi + ŒΩ)
‚àö
d

(cid:21)
,

(cid:15)2 := min
i‚ààC2

(cid:88)

s‚ààC2

ris ‚àí

2m2ŒΩ
‚àö
d

,

(72)

(73)

we have Œªmin(pInd ‚àí Z) = min{(cid:15)1, (cid:15)2} + p, and we can bound (cid:15)1, (cid:15)2 separately. To this end, let
xi := (cid:80)
s:C(s)=C(i) ris and it follows a Binomial distribution such that xi ‚àº Binom(m1, p) when
i ‚àà C1 and xi ‚àº Binom(m2, p) when i ‚àà C2. Then, by applying Lemma A.1 and the union bound
we obtain

(cid:26)

(cid:26)

P

P

min
i‚ààC1

min
i‚ààC2

(cid:27)

xi ‚â§ œÑ1œÅ log n

= n

1‚àíœÅ

(cid:16)
Œ±‚àíœÑ1 log( eŒ±
œÑ1

(cid:17)

)+o(1)

,

xi ‚â§ œÑ2(1 ‚àí œÅ) log n

= n

(cid:27)

1‚àí(1‚àíœÅ)

(cid:16)
Œ±‚àíœÑ2 log( eŒ±
œÑ2

(cid:17)

)+o(1)

.

(74)

(75)

31

for œÑ1, œÑ2 ‚àà [0, Œ±) which satisfy the conditions in (28). For (cid:15)1, we have

(cid:15)1 ‚â• min
i‚ààC1

(cid:88)

(cid:88)

ris

‚àí

(cid:124)

s‚ààC1
(cid:123)(cid:122)
=:(cid:15)11

(cid:125)

s‚ààC1
(cid:124)

(cid:123)(cid:122)
=:(cid:15)12

(cid:125)

¬µs + ŒΩ
‚àö
d

‚àí

m1‚àö
d
(cid:124)

max
i‚ààC1

(¬µi + ŒΩ)

= (cid:15)11 ‚àí (cid:15)12 ‚àí (cid:15)13.

(cid:123)(cid:122)
=:(cid:15)13

(cid:125)

Then we can bound (cid:15)11, (cid:15)12 and (cid:15)13 separately. (cid:15)11 is bounded by (74). For (cid:15)12, by deÔ¨Ånition,

(a)
‚â§

(cid:15)12

=

1
‚àö
d
m1‚àö
d

max {Œ≥œÉ1, (1 ‚àí Œ≥)(œÉ1 + œÉ2)} + Œ≥

(cid:16)

œÉ + œÉ(i)(cid:17)

(cid:88)

i‚ààC1

(max {Œ≥œÉ1, (1 ‚àí Œ≥)(œÉ1 + œÉ2)} + Œ≥œÉ) +

(cid:88)

œÉ(i)

1
‚àö
d

i‚ààC1
¬∑ œÅ(cid:112)2(1 + c)Œ≤ log n +

(b)
‚â§ max

(cid:26) Œ≥
‚àö
œÅ

, (1 ‚àí Œ≥)

(cid:18) 1
‚àö
œÅ

(cid:19)(cid:27)

+

‚àö

1
1 ‚àí œÅ

3Œ≥c
2

log n + o(log n)

with probability 1 ‚àí n‚àíc, where (a) uses the bound of ¬µs + ŒΩ in (72), (b) comes from (70) for
bounding œÉ1 and œÉ2, and 1‚àö
œÉ(i) is bounded by Lemma A.5. Similarly, (cid:15)13 can be bounded
d
as

i‚ààC1

(cid:80)

(cid:15)13 =

(a)
=

max
i‚ààC1

m1‚àö
d
m1‚àö
d
(cid:18) 1
‚àö
œÅ

‚â§ Œ≥œÅ

(cid:16)

max {Œ≥œÉ1, (1 ‚àí Œ≥)(œÉ1 + œÉ2)} + Œ≥

(cid:16)

œÉ + œÉ(i)(cid:17)(cid:17)

(max {Œ≥(œÉ1 + œÉ2), (1 ‚àí Œ≥)(œÉ1 + œÉ2)} + Œ≥œÉ)

+

‚àö

1
1 ‚àí œÅ

(cid:19)
(cid:112)2(1 + c)Œ≤ log n + o(log n)

(b)
=

m1‚àö
d

(Œ≥(œÉ1 + œÉ2) + Œ≥œÉ)

with probability 1 ‚àí n‚àíc, where (a) comes from maxi‚ààC1 œÉ(i) = œÉ1 and (b) holds since Œ≥ ‚â• 1/2 by
assumption. Combining these individual results and applying the union bound yields

(cid:15)1 ‚â• œÑ1œÅ log n ‚àí œÅ max

(cid:26) 2Œ≥
‚àö
œÅ

+

‚àö

Œ≥
1 ‚àí œÅ

,

1
‚àö
œÅ

+

‚àö

1
1 ‚àí œÅ

(cid:27)

(cid:112)2(1 + c)Œ≤ log n‚àí

3Œ≥c
2

log n‚àío(log n)

with probability 1 ‚àí n‚àí‚Ñ¶(1). By further taking c ‚Üí 0 to be close to zero we get

(cid:15)1 ‚â• œÑ1œÅ log n ‚àí œÅ max

(cid:26) 2Œ≥
‚àö
œÅ

+

‚àö

Œ≥
1 ‚àí œÅ

,

1
‚àö
œÅ

+

‚àö

(cid:27)

1
1 ‚àí œÅ

(cid:112)(cid:96)Œ≤ log n ‚àí o(log n),

Similarly, by combining (75) with the bound of ŒΩ in (71) and let c ‚Üí 0, we get the bound for (cid:15)2 as

(cid:15)2 ‚â• œÑ2(1 ‚àí œÅ) log n ‚àí 2(1 ‚àí Œ≥)(1 ‚àí œÅ)

(cid:18) 1
‚àö
œÅ

+

‚àö

(cid:19)

1
1 ‚àí œÅ

(cid:112)(cid:96)Œ≤ log n ‚àí o(log n)

with probability 1 ‚àí n‚àí‚Ñ¶(1). Plugging these into Œªmin(pInd ‚àí Z) = min{(cid:15)1, (cid:15)2} + p completes the
proof.

SpeciÔ¨Åcally when d = 2, œÉ1, œÉ2 and œÉ can be bounded by Theorem A.4 instead of Theorem A.3.

The remaining proof is same as above, therefore we do not repeat.

B.3 Two clusters with unknown cluster sizes

Now we consider the SDP proposed in (32). Note in this case, we use the normalized ground truth
¬ØM ‚àó deÔ¨Åned in (31). The proof basically follows Section 3.1.1, and we will omit repetitive analysis.

32

Step 1: Derive KKT conditions, uniqueness and optimality of M ‚àó. The KKT conditions
of (32) when M = ¬ØM ‚àó are given as

‚Ä¢ Stationarity:

‚àí A ‚àí Œõ ‚àí In ‚äó Z + Œò + Œò(cid:62) = 0,
ij/(cid:107) ¬ØM ‚àó
¬µi ¬ØM ‚àó
¬µiŒ±ij, (cid:107)Œ±ij(cid:107)F ‚â§ 1,

i,j=1, Œòij =

Œò = [Œòij]n

ij(cid:107)F,

(cid:40)

¬ØM ‚àó
¬ØM ‚àó

ij (cid:54)= 0,
ij = 0.

(76)

(77)

‚Ä¢ Comp. slackness:

(cid:10)Œõ, ¬ØM ‚àó(cid:11) = 0,

‚àö

(cid:28)

¬µi,

d ‚àí

(cid:29)

(cid:107) ¬ØM ‚àó

ij(cid:107)F

n
(cid:88)

j=1

= 0,

i = 1, . . . , n.

(78)

‚Ä¢ Dual feasibility:

Œõ (cid:23) 0, ¬µi ‚â• 0,

i = 1, . . . , n.

(79)

Again, the following result establishes the uniqueness and optimality of ¬ØM ‚àó:

Lemma B.1. Given ¬ØM ‚àó deÔ¨Åned in (30), suppose there exist dual variables Œõ, Z, {¬µi}n
{Œ±ij}(i,j):M ‚àó

ij =0 that satisfy the KKT conditions (76) - (79) as well as

i=1, and

Then M = ¬ØM ‚àó is the optimal and unique solution to (32).

N (Œõ) = R( ¬ØM ‚àó).

(80)

Step 2: Construct dual variables. Again, we follow the assumption in Section 3.1.1 that
Ri = Id for each i and Aij = rijId for any j with C(j) = C(i). Then we have ¬ØM ‚àó
ij = Id/m1 for
i, j ‚àà C1 and ¬ØM ‚àó

ij = Id/m2 for i, j ‚àà C2. Also, let us deÔ¨Åne

œÉi :=

(cid:88)

ris,

œÉ(1) :=

s:C(s)=C(i)

1
m1

(cid:88)

(cid:88)

s1‚ààC1

s2‚ààC1

rs1s2,

œÉ(2) :=

1
m2

(cid:88)

(cid:88)

rs1s2.

s1‚ààC2

s2‚ààC2

We make the following guess of dual variables, in particular, we adopt the assumption of Œò in
Lemma 3.9.

Lemma B.2. With a constant Œ≥ ‚àà [0, 1], the dual variables with the following forms satisfy the
KKT conditions (76) - (78) and ¬µi ‚â• 0, i = 1, . . . , n in (79).

(cid:101)Œ±ij =

1
m1

(cid:88)

As1j +

1
m2

(cid:88)

s2‚ààC2

Ais2 ‚àí

1
m1m2

s1‚ààC1

(cid:26) m1Œ≥
‚àö
d

¬Ø¬µ1 = 2 max
i‚ààC1
‚àö

(cid:18)

œÉi +

(cid:107) (cid:101)Œ±ij(cid:107)F ‚àíœÉi

max
j‚ààC2
œÉ(2)
d
max {¬Ø¬µ1, ¬Ø¬µ2}
m1
2
2
‚àö
œÉ(1)
d
max {¬Ø¬µ1, ¬Ø¬µ2}
m2
2
2
œÉ(1) + œÉ(2) + max {¬Ø¬µ1, ¬Ø¬µ2}

œÉi +

Id,

+

+

(cid:18)

(cid:17)

Ô£±
Ô£¥Ô£¥Ô£≤

Ô£¥Ô£¥Ô£≥
(cid:16)

(cid:19)
,

(cid:19)
,

¬µi =

Z =

(cid:88)

(cid:88)

As1s2,

i ‚àà C1, j ‚àà C2,

s1‚ààC1

s2‚ààC2

(cid:27)

‚àíœÉ(2), ¬Ø¬µ2 = 2 max
i‚ààC2

(cid:26) m2(1‚àíŒ≥)
‚àö

d

max
j‚ààC2
‚àö

(cid:107) (cid:101)Œ±ji(cid:107)F ‚àíœÉi

(cid:27)

‚àíœÉ(1),

Ô£±
Ô£¥Ô£≤

d,

Œòij =

i ‚àà C1,

¬µiId/
Œ≥ (cid:101)Œ±ij,
(1 ‚àí Œ≥) (cid:101)Œ±(cid:62)
ji,
Œõ = ‚àíA ‚àí In ‚äó Z + Œò + Œò(cid:62).

i ‚àà C2.

Ô£¥Ô£≥

C(i) = C(j),
i ‚àà C1, j ‚àà C2,
i ‚àà C2, j ‚àà C1,

Lemma B.3. Given the dual variables in Lemma B.2, Œõ can be expressed as

Œõ = (Ind ‚àí ¬ØM ‚àó)(E[A] ‚àí A + (p ‚àí (cid:15))Ind)(Ind ‚àí ¬ØM ‚àó),

(cid:15) := œÉ(1) + œÉ(2) + max {¬Ø¬µ1, ¬Ø¬µ2}

Then, Œõ (cid:23) 0 and N (Œõ) = R( ¬ØM ‚àó) are satisÔ¨Åed if (cid:101)Œõ (cid:31) 0.

33

Step 3: Find the condition for (cid:101)Œõ (cid:31) 0. Again, by using the same argument as in Section 3.1.1
we get, for exact recovery it suÔ¨Éces to ensure Œªmin((p ‚àí (cid:15))Ind) > (cid:107)E[A] ‚àí A(cid:107). To this end, we have
the following bound for Œªmin((p ‚àí (cid:15))Ind).

Lemma B.4. Let p = Œ± log n/n, q = Œ≤ log n/n and Œ¥ :=
and (cid:96) = 1 for d = 2. Then Œªmin((p ‚àí (cid:15))Ind) satisÔ¨Åes

(cid:16) 1‚àö

œÅ + 1‚àö

1‚àíœÅ

(cid:17) ‚àö

(cid:96)Œ≤, where (cid:96) = 2 for d > 2

Œªmin((p ‚àí (cid:15))Ind) ‚â• min{œÅ(2œÑ1 ‚àí Œ± ‚àí 2Œ≥Œ¥), (1 ‚àí œÅ)(2œÑ2 ‚àí Œ± ‚àí 2(1 ‚àí Œ≥)Œ¥} log n + o(log n)

with probability 1 ‚àí n‚àí‚Ñ¶(1) for œÑ1, œÑ2 ‚àà [0, Œ±) such that

(cid:18)

1 ‚àí œÅ

Œ± ‚àí œÑ1 log

(cid:19)(cid:19)

(cid:18) eŒ±
œÑ1

< 0,

1 ‚àí (1 ‚àí œÅ)

Œ± ‚àí œÑ2 log

(cid:18)

(cid:19)(cid:19)

(cid:18) eŒ±
œÑ2

< 0.

(81)

Proof of Theorem 3.12. From Lemma B.4 we have Œªmin((p‚àí(cid:15))Ind) = O(log n) with high probability.
‚àö
Also from Lemma 3.6 we get (cid:107)E[A] ‚àí A(cid:107) = o(
log n) with high probability. This implies as n is
large, Œªmin(pInd‚àíZ) > (cid:107)E[A]‚àíA(cid:107) satisÔ¨Åes as long as Œªmin(pInd‚àíZ) = ‚Ñ¶(log n), which is equivalent
to

min{œÅ(2œÑ1 ‚àí Œ± ‚àí 2Œ≥Œ¥), (1 ‚àí œÅ)(2œÑ2 ‚àí Œ± ‚àí 2(1 ‚àí Œ≥)Œ¥} > 0.

(82)

This further reduces to œÑ1 > Œ±/2 + Œ≥Œ¥ and œÑ2 > Œ±/2 + (1 ‚àí Œ≥)Œ¥. It remains to check (81) holds
for some œÑ1, œÑ2 ‚àà [0, Œ±). To this end, by using the same argument as in the proof of Theorem 3.7,
one can see that (81) holds as long as œÑ1 < œÑ ‚àó
1 , œÑ2 < œÑ ‚àó
2 deÔ¨Åned in (20) (Notice that
œÑ ‚àó
1 , œÑ ‚àó
2 exist only if Œ± > max {1/œÅ, 1/(1 ‚àí œÅ)}). Therefore, putting all these together the condition
for exact recovery becomes

2 with œÑ ‚àó

1 , œÑ ‚àó

Œ± > max

(cid:26) 1
œÅ

,

(cid:27)

,

1
1 ‚àí œÅ

Œ±
2

+ Œ≥Œ¥ < œÑ1 < œÑ ‚àó
1

and

Œ±
2

+ (1 ‚àí Œ≥)Œ¥ < œÑ2 < œÑ ‚àó
2 .

(83)

Furthermore, since Œ≥ ‚àà [0, 1] is not speciÔ¨Åed, it suÔ¨Éces to ensure (83) holds for some Œ≥ ‚àà [0, 1] such
that the range of œÑ1 and œÑ2 is valid. This leads to the conditions in (33).

B.3.1 Proof of the lemmas

Proof of Lemma B.1. The optimality of ¬ØM ‚àó is immediately obtained since (32) is convex then KKT
conditions are suÔ¨Écient for ¬ØM ‚àó being optimal [8]. For uniqueness, suppose (cid:102)M (cid:54)= ¬ØM ‚àó is another
optimal solution. Similar to (54) in Section B.1, we have

0 = (cid:104)A, (cid:102)M ‚àí ¬ØM ‚àó(cid:105) = ‚àí(cid:104)Œõ, (cid:102)M ‚àí ¬ØM ‚àó(cid:105) ‚àí (cid:104)In ‚äó Z, (cid:102)M ‚àí ¬ØM ‚àó(cid:105) + (cid:104)Œò + Œò(cid:62), (cid:102)M ‚àí ¬ØM ‚àó(cid:105)
(a)
= ‚àí(cid:104)Œõ, (cid:102)M ‚àí ¬ØM ‚àó(cid:105) + (cid:104)Œò + Œò(cid:62), (cid:102)M ‚àí ¬ØM ‚àó(cid:105)
= ‚àí(cid:104)Œõ, (cid:102)M (cid:105) + 2(cid:104)Œò, (cid:102)M ‚àí ¬ØM ‚àó(cid:105)

where (a) comes from (cid:80)n

i=1 Mii = 2Id in (32). To proceed, let us rewrite

(cid:104)Œò, (cid:102)M ‚àí ¬ØM ‚àó(cid:105) = (cid:104)Œò, (cid:102)M (cid:105) ‚àí (cid:104)Œò, ¬ØM ‚àó(cid:105) =

(cid:88)

i,j

(cid:104)Œòij, (cid:102)Mij(cid:105) ‚àí

(cid:104)Œòij, ¬ØM ‚àó

ij(cid:105).

(cid:88)

i,j

(84)

(85)

34

Furthermore, by plugging (77) into (85) we obtain

(cid:88)

i,j

(cid:88)

i,j

(cid:104)Œòij, ¬ØM ‚àó

ij(cid:105) =

(cid:88)

(cid:88)

(cid:104)Œòij, (cid:102)Mij(cid:105) =

i

(cid:88)

i

j:C(j)=C(i)
(cid:18) (cid:88)

¬µi

¬µi
(cid:107) ¬ØM ‚àó
ij(cid:107)F

(cid:104) ¬ØM ‚àó

ij, ¬ØM ‚àó

ij(cid:105) =

(cid:88)

‚àö

d,

¬µi

i

j:C(j)=C(i)

(cid:18) (cid:88)

(cid:88)

‚â§

¬µi

i

j:C(j)=C(i)

1
(cid:107) ¬ØM ‚àó
ij(cid:107)F

1
(cid:107) ¬ØM ‚àó
ij(cid:107)F

(cid:104) ¬ØM ‚àó

ij, (cid:102)Mij(cid:105) +

(cid:88)

(cid:19)

(cid:104)Œ±ij, (cid:102)Mij(cid:105)

j:C(j)(cid:54)=C(i)

(cid:107) ¬ØM ‚àó

ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F +

(cid:88)

(cid:107)Œ±ij(cid:107)F(cid:107) (cid:102)Mij(cid:107)F

j:C(j)(cid:54)=C(i)

(cid:19)

(86)

(cid:88)

‚â§

¬µi

(cid:88)

(cid:107) (cid:102)Mij(cid:107)F

(a)
‚â§

(cid:88)

‚àö

d,

¬µi

i

j

i

‚àö

j (cid:107) (cid:102)Mij(cid:107)F ‚â§

where (a) holds since (cid:80)
d in (32). Then combining (85) and (86) results in (cid:104)Œò, (cid:102)M ‚àí
M ‚àó(cid:105) ‚â§ 0, and plugging this back into (84) gives (cid:104)Œõ, (cid:102)M (cid:105) ‚â§ 0. On the other hand, from Œõ (cid:23) 0 in (79)
and (cid:102)M (cid:23) 0 in (32) we have (cid:104)Œõ, (cid:102)M (cid:105) ‚â• 0. Therefore we conclude (cid:104)Œõ, (cid:102)M (cid:105) = 0 and (cid:104)Œò, (cid:102)M ‚àíM ‚àó(cid:105) = 0.
To proceed, by N (Œõ) = R( ¬ØM ‚àó) and the deÔ¨Ånition of ¬ØM ‚àó in (31), (cid:102)M has the following form:

(cid:102)M = V (1)Œ£1(V (1))(cid:62) + V (2)Œ£2(V (2))(cid:62),

(87)

for some diagonal Œ£1, Œ£2 (cid:23) 0. Now it remains to show Œ£1 = Id/m1, Œ£2 = Id/m2. To this end,
from (cid:104)Œò, (cid:102)M ‚àí M ‚àó(cid:105) = 0 we see (a) in (86) holds with equality, i.e. (cid:80)n
d, ‚àÄi. Then
combining this with (87) yields the following for i ‚àà C1:

j=1 (cid:107) (cid:102)Mij(cid:107)F =

‚àö

n
(cid:88)

j=1

(cid:107) (cid:102)Mij(cid:107)F

(a)
=

n
(cid:88)

j=1

(cid:107)RiŒ£1R(cid:62)

j (cid:107)F

(b)
= m1(cid:107)Œ£1(cid:107)F =

‚àö

d, ‚àÄi ‚àà C1 ‚áí (cid:107)Œ£1(cid:107)F =

‚àö

d/m1,

where both (a) and (b) follow from (87). Similarly for i ‚àà C2 we can obtain (cid:107)Œ£2(cid:107)F =
note for each diagonal block of (cid:102)M it satisÔ¨Åes

‚àö

d/m2. Next,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:102)Mii

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(a)
=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

RiŒ£1R(cid:62)

i +

(cid:88)

RiŒ£2R(cid:62)
i

i‚ààC1
‚àö

(d)
=

d

(c)
= 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:102)Mii

i‚ààC2
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(b)
‚â§

(cid:88)

i‚ààC1

(cid:13)
(cid:13)RiŒ£1R(cid:62)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)RiŒ£2R(cid:62)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)F

(cid:88)

i‚ààC2

‚àö

where (a) follows from (87); (b) comes from triangle inequality; (c) holds because (cid:107)Œ£1(cid:107)F =
and (cid:107)Œ£1(cid:107)F =
and further RiŒ£1R(cid:62)
and Œ£2 = Id/m2, which indicates (cid:102)M = ¬ØM ‚àó.

d/m1
i=1 Mii = 2Id in (32). This implies (b) holds with equality,
i = Id/m2, ‚àÄi ‚àà C2. It follows Œ£1 = Id/m1

i = Id/m1, ‚àÄi ‚àà C1 and RiŒ£2R(cid:62)

d/m2; (d) follows from (cid:80)n

‚àö

Proof of Lemma B.2. First, from (76) we get Œõ = ‚àíA ‚àí In ‚äó Z + Œò + Œò(cid:62), plugging this into (80)
yields

(cid:18) ¬µiId‚àö
d

+

¬µsId‚àö
d

(cid:19)

‚àí Ais

= Z,

i = 1, . . . , n,

(cid:16)

Œòis + Œò(cid:62)

si ‚àí Ais

(cid:17)

= 0,

i = 1, . . . , n.

(cid:88)

s:C(s)=C(i)
(cid:88)

s:C(s)(cid:54)=C(i)

(88)

(89)

35

Then, by summing (88) over i ‚àà C1 and i ‚àà C2 separately we get the expression of Z as

Z =

(cid:18) 2
‚àö
d

(cid:88)

s‚ààC1

¬µs ‚àí œÉ(1)

(cid:19)

Id =

(cid:18) 2
‚àö
d

(cid:88)

s‚ààC2

¬µs ‚àí œÉ(2)

(cid:19)

Id,

Next, plugging (90) back into (88) yields expressions of ¬µi as
‚àö

‚àö

‚àö

¬µi =

œÉi +

d
m1

(cid:18) 1
m1

‚àö

d
m2

œÉi +

¬µi =

(cid:124)

(cid:18) 1
m2

(cid:124)

(cid:88)

s‚ààC1

(cid:88)

s‚ààC2

d
m1

œÉ(1)

‚àö

d
m2

œÉ(2)

(cid:19)

=

=

(cid:125)

(cid:19)

(cid:125)

¬µs ‚àí

(cid:123)(cid:122)
=:¬Ø¬µ1

¬µs ‚àí

(cid:123)(cid:122)
=:¬Ø¬µ2

d
m1

œÉi + ¬Ø¬µ1,

i ‚àà C1,

‚àö

d
m2

œÉi + ¬Ø¬µ2,

i ‚àà C2.

Then plugging (91) into (90) yields

(cid:18)

œÉ(1) +

Z =

2m1‚àö
d

(cid:19)

¬Ø¬µ1

Id =

(cid:18)

œÉ(2) +

(cid:19)

¬Ø¬µ2

Id.

2m2‚àö
d

Importantly, to make the LHS and RHS above identical, ¬Ø¬µ1 and ¬Ø¬µ2 satisfy

‚àö

d
2m1

¬Ø¬µ1 =

(œÉ(2) + ¬Ø¬µ),

¬Ø¬µ2 =

‚àö

d
2m2

(œÉ(1) + ¬Ø¬µ)

(90)

(91)

(92)

(93)

for some ¬Ø¬µ. Plugging (93) into (91) and (90) yields the expressions of Z and ¬µi in terms of ¬Ø¬µ, and
we restrict Œ≥ ‚àà [0, 1] which ensures ¬µi ‚â• 0. To determine ¬Ø¬µ, we adopt the guess of Œòij and (cid:101)Œ±ij in
Lemma 3.9. From (77) we have (cid:107)Œòij(cid:107)F = ¬µi(cid:107)Œ±ij(cid:107)F with (cid:107)Œ±ij(cid:107)F ‚â§ 1, then for each ¬µi it should
satisfy ¬µi ‚â• (cid:107)Œòij(cid:107)F for any j with C(j) (cid:54)= C(i). By further plugging the deÔ¨Ånition of ¬µi we get
¬Ø¬µ ‚â• max {¬Ø¬µ1, ¬Ø¬µ2} with ¬Ø¬µ1, ¬Ø¬µ2 deÔ¨Åned in Lemma B.2, and we set ¬Ø¬µ = max {¬Ø¬µ1, ¬Ø¬µ2}. This completes
our guess of dual variables.

Proof of Lemma B.3. We follow the same route as in Lemma 3.4 by Ô¨Årst rewriting A, Œõ and Œò into
four blocks as (63). Also, let (cid:101)¬µ1 = diag({¬µiId}m1
i=m1+1) ‚àà
Rm2d√óm2d. Then the four blocks of Œò satisfy

i=1) ‚àà Rm1d√óm1d and (cid:101)¬µ2 = diag({¬µiId}n

(cid:101)Œò11 = (cid:101)¬µ1 (cid:102)M ‚àó
1‚àö
d

,

(cid:101)Œò12 + (cid:101)Œò(cid:62)

21 =

1
m1

,

(cid:101)Œò22 = (cid:101)¬µ2 (cid:102)M ‚àó
2‚àö
d
1
(cid:101)A12 (cid:102)M ‚àó
m2

(cid:102)M ‚àó

1 (cid:101)A21 +

2 ‚àí

1
m1m2

(cid:102)M ‚àó

1 (cid:101)A12 (cid:102)M ‚àó
2 .

The remaining proof is very similar to the one of Lemma 3.4, and we do not repeat.

Proof. According to Lemma B.3, Œªmin((p ‚àí (cid:15))Ind) satisÔ¨Åes

Œªmin((p ‚àí (cid:15))Ind) = p ‚àí (cid:15) = p ‚àí max{¬Ø¬µ1, ¬Ø¬µ2} ‚àí œÉ(1) ‚àí œÉ(2)
(cid:26)

(cid:27)

(cid:26)

(cid:26)

= p+min

œÉi ‚àí

2 min
i‚ààC1

(cid:124)

= p + min{(cid:15)1, (cid:15)2}.

m1Œ≥
‚àö
d

(cid:107) (cid:101)Œ±ij(cid:107)F

max
j‚ààC2
(cid:123)(cid:122)
=:(cid:15)1

‚àíœÉ(1)

, 2 min
i‚ààC2

œÉi ‚àí

m2(1‚àíŒ≥)
‚àö
d

max
j‚ààC1

(cid:107) (cid:101)Œ±ji(cid:107)F

(cid:123)(cid:122)
=:(cid:15)2

(cid:27)

‚àíœÉ(2)

(cid:27)

(cid:125)

(cid:125)

(cid:124)

36

Then we bound (cid:15)1 and (cid:15)2 separately as

(cid:15)1 ‚â• 2 min
i‚ààC1

œÉi ‚àí

2m1Œ≥
‚àö
d

max
i‚ààC1,j‚ààC2

(cid:107) (cid:101)Œ±ij(cid:107)F ‚àí œÉ(1),

(cid:15)2 ‚â• 2 min
i‚ààC2

œÉi ‚àí

2m2(1 ‚àí Œ≥)
‚àö
d

max
i‚ààC2,j‚ààC1

(cid:107) (cid:101)Œ±ji(cid:107)F ‚àí œÉ(2).

For mini‚ààC1 œÉi and mini‚ààC2 œÉi, we can use the bounds derived in (74) and (75) respectively. For
maxi‚ààC1,j‚ààC2 (cid:107) (cid:101)Œ±ij(cid:107)F, by using the result in (71) we obtain
1
1 ‚àí œÅ

(cid:107) (cid:101)Œ±ij(cid:107)F ‚â§ (cid:112)2(1 + c)Œ≤

(cid:18) log n
n

max
i‚ààC1,j‚ààC2

(cid:18) 1
‚àö
œÅ

(cid:19)(cid:18) log n

+ o

‚àö

+

(cid:19)

(cid:19)

n

with probability 1 ‚àí n‚àíc. For œÉ(1), since rij = rji for i, j ‚àà C1, it satisÔ¨Åes

(cid:88)

(cid:88)

rs1s2 = 2

(cid:88)

rs1s2,

s1‚ààC1

s2‚ààC1

s1, s2‚ààC1, s1<s2

and

(cid:80)
s1,s2‚ààC1, s1<s2

rs1s2 follows a binomial distribution Binom (m1(m1 ‚àí 1)/2, Œ± log n/n). By apply-

ing HoeÔ¨Äding‚Äôs inequality [6] we obtain

(cid:40)

P

(cid:88)

s1,s2‚ààC1, s1<s2

rs1s2 ‚àí

(m1 ‚àí 1)Œ±œÅ log n
2

(cid:41)

(cid:18)

‚â• t

‚â§ exp

‚àí

(cid:19)

,

4t2
m2
1

for any t > 0. This further indicates œÉ(1) ‚â§ Œ±œÅ log n + o(log n) with high probability. Similarly we
can obtain œÉ(2) ‚â§ Œ±(1 ‚àí œÅ) log n + o(log n) with high probability. By putting all the results together
and let c ‚Üí 0 to be close to zero, we get the bounds for (cid:15)1, (cid:15)2 and further Œªmin((p ‚àí (cid:15))Ind).

References

[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments.

The Journal of Machine Learning Research, 18(1):6446‚Äì6531, 2017.

[2] Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic

block model. IEEE Transactions on Information Theory, 62(1):471‚Äì487, 2015.

[3] Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices.

Cambridge university press, 2010.

[4] Chandrajit Bajaj, Tingran Gao, Zihang He, Qixing Huang, and Zhenxiao Liang. SMAC: simul-
taneous mapping and clustering using spectral decompositions. In International Conference on
Machine Learning, pages 324‚Äì333, 2018.

[5] Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of
random matrices with independent entries. Annals of Probability, 44(4):2479‚Äì2506, 2016.

[6] St¬¥ephane Boucheron, G¬¥abor Lugosi, and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford university press, 2013.

[7] Nicolas Boumal. Nonconvex phase synchronization.

SIAM Journal on Optimization,

26(4):2355‚Äì2377, 2016.

37

[8] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge

university press, 2004.

[9] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov¬¥a. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Physical Review E, 84(6):066106, 2011.

[10] Patrick Doreian, Vladimir Batagelj, and Anuska Ferligoj. Generalized blockmodeling, volume 25.

Cambridge university press, 2005.

[11] Martin E. Dyer and Alan M. Frieze. The solution of some random NP-hard problems in

polynomial expected time. Journal of Algorithms, 10(4):451‚Äì489, 1989.

[12] Yifeng Fan, Tingran Gao, and Zhizhen Zhao. Unsupervised co-learning on G-manifolds across
In Advances in Neural Information Processing Systems, pages

irreducible representations.
9041‚Äì9053, 2019.

[13] Yifeng Fan, Tingran Gao, and Zhizhen Zhao. Representation theoretic patterns in multi-
Information and

frequency class averaging for three-dimensional cryo-electron microscopy.
Inference: A journal of IMA, page accepted, 2021.

[14] Yifeng Fan and Zhizhen Zhao. Multi-frequency vector diÔ¨Äusion maps. In International Con-

ference on Machine Learning, pages 1843‚Äì1852. PMLR, 2019.

[15] Stephen E Fienberg, Michael M Meyer, and Stanley S Wasserman. Statistical analysis of
multiple sociometric relations. Journal of the american Statistical association, 80(389):51‚Äì67,
1985.

[16] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75‚Äì174, 2010.

[17] Joachim Frank. Three-Dimensional Electron Microscopy of Macromolecular Assemblies: Vi-
sualization of Biological Molecules in Their Native State. Oxford University Press, New York,
2nd edition, 2006.

[18] Tingran Gao and Zhizhen Zhao. Multi-frequency phase synchronization.

In International

Conference on Machine Learning, pages 2132‚Äì2141. PMLR, 2019.

[19] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via
semideÔ¨Ånite programming. IEEE Transactions on Information Theory, 62(5):2788‚Äì2797, 2016.

[20] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold
IEEE Transactions on Information Theory,

via semideÔ¨Ånite programming: Extensions.
62(10):5918‚Äì5937, 2016.

[21] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels:

First steps. Social networks, 5(2):109‚Äì137, 1983.

[22] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in

networks. Physical review E, 83(1):016107, 2011.

[23] Rafa(cid:32)l Lata(cid:32)la, Ramon van Handel, and Pierre Youssef. The dimension-free structure of nonho-

mogeneous random matrices. Inventiones mathematicae, 214(3):1031‚Äì1080, 2018.

38

[24] Roy R Lederman and Amit Singer. A representation theory perspective on simultaneous align-

ment and classiÔ¨Åcation. Applied and Computational Harmonic Analysis, 2019.

[25] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models.

The Annals of Statistics, 43(1):215‚Äì237, 2015.

[26] Xiaodong Li, Yudong Chen, Jiaming Xu, et al. Convex relaxation methods for community

detection. Statistical Science, 36(1):2‚Äì15, 2021.

[27] Shuyang Ling. Near-optimal performance bounds for orthogonal and permutation group syn-

chronization via spectral methods. arXiv preprint arXiv:2008.05341, 2020.

[28] Laurent Massouli¬¥e. Community detection thresholds and the weak ramanujan property. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 694‚Äì703,
2014.

[29] Frank McSherry. Spectral partitioning of random graphs. In Proceedings 42nd IEEE Symposium

on Foundations of Computer Science, pages 529‚Äì537. IEEE, 2001.

[30] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields, 162(3):431‚Äì461, 2015.

[31] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.

Combinatorica, 38(3):665‚Äì708, 2018.

[32] Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167‚Äì

256, 2003.

[33] Andy Nguyen, Mirela Ben-Chen, Katarzyna Welnicka, Yinyu Ye, and Leonidas Guibas. An
optimization approach to improving collections of shape maps. In Computer Graphics Forum,
volume 30, pages 1481‚Äì1491. Wiley Online Library, 2011.

[34] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideÔ¨Ånite program-

ming. SIAM journal on optimization, 18(1):186‚Äì205, 2007.

[35] Amelia Perry and Alexander S Wein. A semideÔ¨Ånite program for unbalanced multisection
In 2017 International Conference on Sampling Theory and

in the stochastic block model.
Applications (SampTA), pages 64‚Äì67. IEEE, 2017.

[36] John William Strutt Baron Rayleigh. The theory of sound, volume 2. Macmillan, 1896.

[37] Lord Rayleigh. XII. on the resultant of a large number of vibrations of the same pitch and of
arbitrary phase. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of
Science, 10(60):73‚Äì78, 1880.

[38] Yunpeng Shi and Gilad Lerman. Message passing least squares framework and its application

to rotation synchronization. arXiv preprint arXiv:2007.13638, 2020.

[39] Amit Singer. Angular synchronization by eigenvectors and semideÔ¨Ånite programming. Applied

and computational harmonic analysis, 30(1):20‚Äì36, 2011.

[40] Amit Singer and H-T Wu. Vector diÔ¨Äusion maps and the connection laplacian. Communications

on pure and applied mathematics, 65(8):1067‚Äì1144, 2012.

39

[41] Amit Singer, Zhizhen Zhao, Yoel Shkolnisky, and Ronny Hadani. Viewing angle classiÔ¨Åcation
of cryo-electron microscopy images using eigenvectors. SIAM Journal on Imaging Sciences,
4(2):723‚Äì759, 2011.

[42] Gilbert W Stewart. Perturbation theory for the singular value decomposition. Technical report,

University of Maryland, College Park, 1998.

[43] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.

[44] Joel A... Tropp. User-friendly tail bounds for sums of random matrices. Foundations of com-

putational mathematics, 12(4):389‚Äì434, 2012.

[45] Joel A... Tropp. An introduction to matrix concentration inequalities. Foundations and

Trends¬Æ in Machine Learning, 8(1-2):1‚Äì230, 2015.

[46] Lieven Vandenberghe and Stephen Boyd. SemideÔ¨Ånite programming. SIAM review, 38(1):49‚Äì

95, 1996.

[47] Roman Vershynin. High-dimensional probability: An introduction with applications in data

science, volume 47. Cambridge university press, 2018.

[48] Bowei Yan, Purnamrita Sarkar, and Xiuyuan Cheng. Provable estimation of the number of
blocks in block models. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics,
pages 1185‚Äì1194. PMLR, 2018.

[49] Zhizhen Zhao and Amit Singer. Rotationally invariant image representation for viewing direc-

tion classiÔ¨Åcation in cryo-EM. Journal of structural biology, 186(1):153‚Äì166, 2014.

40

