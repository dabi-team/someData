Optimal to-do list gamiﬁcation

Jugoslav Stojcheski, Valkyrie Felso, & Falk Lieder
Rationality Enhancement Group
Max Planck Institute for Intelligent Systems
Tübingen, Baden-Württemberg, Germany

August 5, 2021

Abstract

What should I work on ﬁrst? What can wait until later? Which projects should I prioritize
and which tasks are not worth my time? These are challenging questions that many people
face every day. People’s intuitive strategy is to prioritize their immediate experience over the
long-term consequences. This leads to procrastination and the neglect of important long-term
projects in favor of seemingly urgent tasks that are less important. Optimal gamiﬁcation
strives to help people overcome these problems by incentivizing each task by a number of
points that communicates how valuable it is in the long-run. Unfortunately, computing the
optimal number of points with standard dynamic programming methods quickly becomes in-
tractable as the number of a person’s projects and the number of tasks required by each project
increase. Here, we introduce and evaluate a scalable method for identifying which tasks are
most important in the long run and incentivizing each task according to its long-term value.
Our method makes it possible to create to-do list gamiﬁcation apps that can handle the size
and complexity of people’s to-do lists in the real world.

Keywords: optimal gamiﬁcation; reward shaping; productivity; dynamic programming;

to-do lists; decision-support

1
2
0
2

g
u
A
3

]
I

A
.
s
c
[

2
v
8
2
2
5
0
.
8
0
0
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Contents

1 Introduction

2 Problem deﬁnition

2.1 Modeling working on projects as a discrete-time semi-Markov decision process . . .
2.1.1
State space S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Action space A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.3 Transition time F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.4 Transition dynamics T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.5 Reward function r . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.6 Optimal policy π⋆
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Maximizing the productivity of myopic workers by optimal gamiﬁcation . . . . . .

3 Solution

Incorporating inductive biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1
3.2 Parsing information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Solving SMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4

Incentivizing and scheduling tasks

4 API

5 Evaluations

5.1
Inspecting pseudo-rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Speed and reliability tests in diﬀerent scenarios . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
5.3 Comparison to non-hierarchical SMDP solving methods

6 Future work

References

Appendix

A1 API input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2 API output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

3
4
4
5
6
7
7
8
9

10
10
12
12
13

15

16
16
17
17

18

27

28
28
29

2

1 Introduction

Many people struggle with procrastination and setting priorities [13]. But prioritization is diﬃcult
and self-discipline is challenging. Decision support systems have been developed to support human
decision-making in very speciﬁc domains [1, 3, 4, 11, 12]. But, to date, there is no decision-support
system for deciding what to work on.

Procrastination takes a considerable toll on people’s lives, the economy and society at large.
It is often a consequence of people’s propensity to prioritize their immediate experiences over the
long-term consequences of their actions [13]. This is problematic when immediate rewards are
misaligned with long-term value. This can be addressed by optimal to-do-list gamiﬁcation [8]
which proposes adding incentives to tasks to help people overcome procrastination. However, so
far, this approach has been limited to small artiﬁcial laboratory paradigms in which people’s to-do
lists were restricted due to the exponentially-increasing computational cost with respect to the
number of tasks in the to-do list. Mitigating this problem so that to-do list gamiﬁcation can be
applied to real-world to-do lists was the major issue to be addressed.

In this text, we present an algorithmic solution that tackles the scalability issue. The key
idea is to leverage the natural structure of a to-do list composed of goals and tasks, and to use a
2-level hierarchical decomposition of a discrete-time semi-Markov decision process (SMDP; [5]). In
addition, we introduce inductive biases in the procedure of searching for an optimal solution, which
removes a large amount of unnecessary computations. We show empirically that this method is
drastically more scalable than other methods used previously for tackling this problem such as
Backward Induction [14] and Value Iteration [2]. Concretely, we provide results in terms of wall-
clock computational time for to-do lists with varying numbers of goals and tasks.

The main beneﬁt of introducing hierarchies in the procedure of solving SMDPs is the large
reduction of the state-space complexity. Moreover, the introduced inductive biases reduce the
the problem complexity by discarding sequences of actions that cannot possibly be part of an
optimal solution. These improvements lead to both a massive computational speed-up and low
memory requirements. This enables the gamiﬁcation application to process large to-do lists that
were previously intractable, which in turn allows users to make detailed plans for multiple goals
far into the future. We expect the performance of the algorithm presented here to be further
improved in future work by adding more layers of abstraction in the hierarchy of SMDP solvers
and by introducing additional inductive biases.

Furthermore, we developed an application programming interface (API) that takes in a to-do
list and outputs a gamiﬁed list of tasks for a desired workload. However, although there are no
known limitations to the algorithmic procedure, our API is currently unable to process some large
to-do lists due to a 30-second request timeout. We are conﬁdent that future work will be able to
address this issue by changing the software infrastructure. We provide details on this limitation in
Section 5.2.

This text is divided into the following sections: In Section 2, we describe the problem that we are
trying to solve, and we present a formal deﬁnition of the problem as a discrete-time semi-Markov
decision process. In Section 3, we describe two algorithmic tricks that allow us to solve this problem
more eﬃciently, the API components, their functionalities, and the way solutions are generated.
In Section 4, we give a brief introduction to the API that we have developed. In Section 5, we
provide a use case in which we inspect pseudo-rewards, evaluate the speed and reliability of the
API, and make a qualitative comparison between non-hierarchical and hierarchical SMDP solving
methods. Lastly, in Section 6 we outline directions for future work, and the Appendix provides
details on the API’s input and output.

2 Problem deﬁnition

The goal of to-do list gamiﬁcation is to maximize the user’s long-term productivity1 by proposing
daily task lists where each task is incentivized by a certain number of points. Users compose
hierarchical to-do lists comprised of two types of items: goals and tasks. Each goal contains a
deadline, a value estimate, and a list of tasks which contribute to the goal. Each task contains a
time estimate. Tasks may optionally include a deadline and scheduling tags (i.e. “do dates" and

1We deﬁne productivity as the amount of value that a person generates by completing a series of tasks within a

ﬁxed amount of time.

3

“do days"). In addition, users provide their desired workload in hours for a typical day (typical
day’s working hours) and for the day at hand (today’s working hours). The outputted gamiﬁed
daily schedule should contain all tasks that users indicated they wanted to work on today, as well
as additional tasks towards their goals, up to the desired daily workload. Further details on these
components of the to-do list are provided in the Appendix.

We formally deﬁne this problem in terms of a discrete-time semi-Markov decision process
(SMDP) in the sections that follow. We ﬁrst model working on projects as an SMDP in Sec-
tion 2.1. Then, we build on this model to formalize the problem of productivity maximization by
optimal gamiﬁcation in Section 2.2.

2.1 Modeling working on projects as a discrete-time semi-Markov deci-

sion process

The deﬁnition of a discrete-time semi-Markov decision process (SMDP) M = (S, A, F, T, r, γ)
comprises state space S, action space A, transition-time function F , transition dynamics T , reward
function r, and a discount factor γ. We model each level in a hierarchical to-do list as an SMDP.
For simplicity, we initially concentrate on a 2-level hierarchical decomposition, in which we have
one goal-level SMDP (M(goal)) and one task-level SMDP for each goal (M (g)
(task)). Since tasks which
have to be completed in the same period of time or which contribute to a sub-goal can be grouped
to form an SMDP on their own, adding intermediate layers of abstraction between the goal level
and the task level is also viable but is left for future work.

We provide a detailed description of these parameters in the following sections. Additionally, we
give a formal deﬁnition of the optimal policy in relation to the traditional deﬁnition of Q-function
in Section 2.1.6.

2.1.1 State space S

As mentioned in the introductory section (Section 1), the main beneﬁt of the hierarchical decom-
position of the problem is the huge reduction of the state-space cardinality. In this section, we
present an informal complexity analysis of this improvement by ﬁrst deﬁning the non-hierarchical
state space of the problem, and then deﬁning the hierarchical state space of the problem.

The state space of the original (non-hierarchical) SMDP - S - consists of all possible combina-
tions of completed and uncompleted tasks. Intuitively, since a task in a to-do list can be either
completed or not completed, we represent each instance s ∈ S of the state space as a binary vector
whose length corresponds to the total number of tasks in a to-do list. For example, if the total
number of tasks is n, then the state space S is deﬁned by an n-fold Cartesian product of the set
{0, 1}. Formally written, that is

S = {{0, 1}n}

where the i-th element of any binary vector s ∈ S is 1 if the task associated with that element is
completed and 0 otherwise.

According to this deﬁnition, the state space comprises all possible conﬁgurations of the binary
vector and its complexity is exponential with respect to the total number of tasks n. Formally, the
cardinality of the state space is exactly 2n. For a real-world to-do list with a non-trivial number of
tasks, this is an extremely large number of states, and operating on it demands enormous amounts
of computational power and memory.

In order to reduce the cardinality of the state space, we introduce a 2-level hierarchical decom-

position that decomposes the state space into mutually exclusive sets:

1. Goal-level state space S(goal), in which we keep track of goals’ completion.

2. Task-level state space S(task) for each goal, in which we keep track of tasks’ completion

within a goal.

The goal-level state space S(goal) consists of a binary vector of length |G|, where G is the set of

goals and |G| is the total number of goals in a to-do list.

S(goal) =

{0, 1}|G|

o

n

4

In a similar manner, we deﬁne the task-level state space S(task) for each goal. A task-level state
space for an arbitrary goal g ∈ G consists of a binary vector of length ng, where ng is the total
number of tasks associated with that goal.

S(g)
(task) = {{0, 1}ng}

On each level, there is only one initial state, i.e. the binary vector of completed and uncompleted
goals/tasks at present time. If there are no completed goals/tasks at that time, then the initial
state is represented by a vector of all zeros - 0, i.e.

(completed tasks) = 0 = (0, 0, . . . , 0)

Furthermore, all states in which all goals/tasks are completed are considered to be terminal states
as well as states that follow as a consequence of taking a special kind of action (i.e. slack-oﬀ action,
see Section 2.1.2). In the former case, the terminal state is represented by a vector of all ones - 1,
i.e.

(completed tasks) = 1 = (1, 1, . . . , 1)

In order to avoid that taking the last step toward achieving a goal could be penalized by a negative
pseudo-reward (see Equation 4), we introduce an additional state, a so-called goal-achieving state
- s†. The purpose of this special state is to separate the cost of action execution from the reward
for goal-achievement reward. The goal-achieving state can only be reached as a consequence of
automatic execution of an instantaneous action a† immediately after reaching the terminal state
in which all tasks are completed (i.e. 1).

We do a non-rigorous complexity analysis in order to show the state-space reduction obtained
as a consequence of the hierarchical decomposition in the following manner. Let ng be the total
number of tasks associated with goal g ∈ G. Assuming mutually-exclusive task-level state spaces,
the total number of tasks in a to-do list - n - can be written as

|G|

n =

ng

g=1
X
Motivated by real-world to-do lists, we assume that each to-do list is composed of at least two
goals (i.e.
|G| ≥ 2) and each goal has at least two tasks (i.e. ng ≥ 2, g = 1, . . . , |G|) associated
with it. In this case, the following strict inequality holds:

|S| = 2n = 2P|G|

g=1 ng =

2ng > 2|G| +

|G|

g=1
Y

|G|

g=1
X

2ng = |S(goal)| + |S(task)|

Moreover, the exact reduction of the state-space cardinality (as a percentage) can be computed as

1 −

2|G| +

|G|
g=1 2ng

|G|
g=1 2ng
P

!

· 100%

Q

2.1.2 Action space A

In a similar manner to the state space decomposition, we decompose the action space into two
mutually-exclusive sets, a goal-level action space (A(goal)) and task-level action space (A(g)
(task)) for
each goal g ∈ G.

The goal-level action space A(goal) consists of all goals in a to-do list as well as one slack-oﬀ

action a+.

A(goal) = {ag | g = 1, . . . , |G|} ∪ {a+}

The task-level action space A(g)

(task) for a goal g consists of all tasks within that goal as well as

one slack-oﬀ action a+.

A(g)

(task) =

a(g)
i

| i = 1, . . . , ng

∪ {a(g)
+ }

A slack-oﬀ action is a special kind of action whose purpose is to model the value of an
immediately-enjoyable action by providing an immediate positive reward after its execution. Al-
ternatively, it can be used by the policy to discard goals that are not worth attaining. However,

n

o

5

 
it is not explicitly a part of the to-do list, and the slack-oﬀ action is therefore not a part of the
state space on any level. This special kind of action can be executed at any time and its initial
execution triggers an inﬁnite sequence of slack-oﬀ -action executions since there exists no other
more-valuable action. At this point, the value of this action is arbitrarily chosen to be a small
positive constant, but future work will quantify how much people value their leisure time relative
to the value of attaining their goals. Moreover, the value of a slack-oﬀ action might be dependent
on the level at which it occurs and/or the goal that it is associated with.

Any goal-level action can be chosen and executed as long as there are uncompleted tasks as-
sociated with that goal. Alternatively, the slack-oﬀ action can be chosen. The set of all available
goal-level actions is completely determined by the history of task-level actions for each goal. For-
mally, the availability of the goal-level action ag is directly associated with the value of the g-th
entry in the binary vector s, i.e.

ng

sg =

a(g)
i

!

∨ a(g)
+

i=1
^

where the goal-state action ag is available if sg is false and is unavailable otherwise. Please note
that in order to simplify the notation, in the rest of this text, we will use st to denote a goal-level
state that occurs at time t ∈ Z≥0, and ˜s(g)
to denote a task-level state associated with goal g that
occurs at time t ∈ Z≥0.

t

Under the assumption that users always complete a task successfully once they start working
on it, we allow each task-level action (except for the slack-oﬀ action) to be performed at most
once. Therefore, for a goal g, the set of all available task-level actions (i.e. uncompleted tasks)
at time ˜t
depends on the history of actions (i.e. completed tasks) by that time
H (g)
(cid:16)

. Using set notation, we can formally write this as:

˜t ⊆ A(g)
Ω(g)
(cid:16)
(task)

˜t ⊆ A(g)

(task)

(cid:17)

(cid:17)

˜t = {a | a ∈ A(g)
Ω(g)

(task) ∧ a /∈ H (g)

˜t } ∪ {a(g)

+ } Ω(g)

˜t ∪ H (g)

˜t = A(g)

(task)

˜t ∩ H (g)
Ω(g)

˜t = {a(g)
+ }

Furthermore, we consider task-level action execution to be indivisible. This is analogous to a
user working on only one task at a time until that task is completely executed. In other words, the
agent is allowed to choose an action to perform next only if there is no other action in progress at
that time. After executing an action a, the action is marked as completed by changing the value of
the corresponding element in the task-level binary state vector from 0 to 1. As a consequence, the
SMDP moves ˜τ time units in the future according to the task time estimate (e.g. number of min-
utes), which is stochastically obtained by the transition-time function F (deﬁned in Section 2.1.3).
Additionally, if the executed action is not the slack-oﬀ action, we remove the task-level action
from the set of available task-level actions Ω(g) and we add it to the history of actions H(g). That
is, using formal set notation, we perform the following updates

˜t+˜τ = Ω(g)
Ω(g)

˜t \{a} H (g)

˜t+˜τ = H (g)

˜t ∪ {a}

Otherwise, once the slack-oﬀ action is chosen, the SMDP reaches a terminal state in which the
slack-oﬀ action is executed inﬁnitely many times.

Similar set operations apply analogously at the goal-level once a goal is completed. Brieﬂy, we
check whether the last completed task completes the goal which it contributes to and we change
the corresponding value of the goal-level binary state vector from 0 to 1 if that is the case.

2.1.3 Transition time F

The formalization of a problem within the framework of an SMDP requires a deﬁnition of a
transition-time function F . Here, F (τ |st, a) is the probability that the time at which the agent
has to make the next decision occurs in exactly τ time units, as a consequence of executing action
a in state s at time t. We chose a transition-time function that can model the cognitive bias
known as the planning fallacy, in which people underestimate the time required to complete a
task. Kahneman and Tversky [6] describe this bias as such: "Scientists and writers, for example,
are notoriously prone to underestimate the time required to complete a project, even when they
have considerable experience of past failures to live up to planned schedules... It frequently occurs
even when underestimation of duration or cost is actually penalized."

6

 
Since people have unreliable time estimates and our SMDPs are decision processes with discrete
time steps, we model the number of time units required for action completion to follow a zero-
truncated Poisson probability distribution2 with adjusted mean value and variance. We formally
deﬁne the transition-time function as

F (τ |st, a) := Poisson>0(τ ; ˜k) =

˜kτ e−˜k
τ !(1 − e−˜k)

where ˜k = cpf · k, k is the discrete amount of time units required to complete action a in state s
at time t, and cpf ∈ R>0 is a planning-fallacy constant that adjusts the distribution parameter. In
lack of knowledge about the exact value of the planning-fallacy constant (cpf), we follow King and
Wilson [7] and we initially set its value to 1.39. Obtaining better estimates for this value based on
real-world data is left for future work.

2.1.4 Transition dynamics T

In the previous sections, we presented two assumptions related to the task completion: (1) users
complete a task with certainty once they start working on it, and (2) the time estimate for task
completion is stochastic. Under these assumptions, the transition dynamics from a current state s
at time t to a next state s′ after executing an action a is deterministic in completion, but stochastic
in duration. In other words, the transition probability T (st, a, s′
t+τ ) is completely dependent on
the probability of completing an action in exactly τ time units, which can be formally written as

T (st, a, s′

t+τ ) = Pr(s′

t+τ |st, a) ∼ F (τ |st, a)

∀s ∈ S(g)

(task)

∀a ∈ Ω(g)

(task)

t ∈ Z≥0

g = 1, . . . , |G|

where τ ∼ F (τ |st, a) is the transition-time function that determines the time needed to complete
a chosen action a in state s at time t, and s′ is the state that follows as a consequence of executing
action a in state s. Formally, if an action a is represented by the i-th bit of the binary state vector
s, the binary vector of the next state s′ can be written as s′ = ei ∨ s, where ei is a one-hot vector
with a value of 1 only at its i-th position, and ∨ represents the “or” operation of two binary vectors.
This operation is applicable on both the goal and task levels as long as the procedures of goal and
task completion described in Section 2.1.2 are being completely followed.

A special case of the transition dynamics at task level occurs after reaching the terminal state
(1) in which all tasks have been completed. There, the process transits to a goal-achieving state s†
after instantaneously executing the action a† in 0 time steps, that is T (1, a†, s†) = Pr(s†|1, a†) = 1.

2.1.5 Reward function r

Formally, we deﬁne the goal-level reward function r(goal)(st, a, s′
t+τ ) from a current goal-level state
s at time t to a next goal-level state s′ at time t + τ after performing a goal-level action a ∈ Ω(goal)
that takes τ time units for execution to be

R(a+) · (1 − γ)−1
τ −1

if the goal-level slack-oﬀ action was chosen

r(goal)(st, a, s′

(task)(˜s˜t, π(g)
r(g)

t+τ ) = 


where a+ represents the goal-level slack-oﬀ action, γ ∈ (0, 1] is a discount factor, and r(g)
is a task-level reward function of goal g.
The task-level reward function r(g)

(task)(˜s˜t), ˜s′

˜t=0
X

˜t′)

if any other goal-level action was chosen

(task)(˜s˜t, ˜a, ˜s′

˜t+˜τ ) for a goal g from a current task-level state ˜s

(task)(˜s˜t, ˜a, ˜s˜t′)

at time ˜t to a next task-level state ˜s′ at time ˜t + ˜τ after performing a task-level action ˜a ∈ Ω(g)
that takes ˜τ time units for execution is deﬁned as

(task)

R(˜a(g)

+ ) · (1 − γ)−1

if the task-level slack-oﬀ action associated with goal
g was chosen

˜τ −1

−λ(g)

γk

if a task-level action with duration ˜τ was chosen

r(g)
(task)(˜s˜t, ˜a, ˜s′

˜t+˜τ ) =

k=0
X

if the terminal task-level state has been reached,
i.e. ˜s˜t = 1(g), ˜a = a†, and ˜s′
2Also known as conditional Poisson distribution, positive Poisson distribution.

R(g) · Π(βg)

˜t+˜τ = s†






7

0.1
0.25
0.5
0.75
1.0
10.0
1000.0

1000

800

d
r
a
w
e
R

600

400

200

0

0.1
0.25
0.5
0.75
1.0
10.0
1000.0

300

250

200

d
r
a
w
e
R

150

100

50

0

0

100

200

300

400

500

0

50

100

150

200

Time

Time

Figure 1: Penalized rewards (vertical axis) for multiple values of β (legend) as a function of time
(horizontal axis).

where λ(g) ∈ R>0 models the value that reﬂects the cost of a person’s time and mental eﬀort to
work on goal g, and ˜a(g)

+ is the slack-oﬀ action associated with goal g.

Furthermore, we deﬁne the reward function R(g) as the reward for accomplishing3 goal g. This
reward function returns the goal value if executing the next task-level action ˜a leads to completion
of the goal. In addition, we deﬁne Π(βg) to be the penalty function for a goal g ∈ G. The value
of the penalty function discounts the goal reward proportionally to the time by which deadlines
associated with that goal are missed and it can be formally written as Π(βg) = (1 + βg)−1. Here,
ng
i=1 ψ · ∆ti is a weighted sum of penalties for tasks whose deadlines were missed, where
βg =
ψ ∈ R≥0 is the penalty rate (per time unit) and ∆ti is the number of time units by which the
deadline was missed. It acts as a constant that regulates the steepness of the curve of goal function
R(g). An example of this eﬀect is graphically represented in Figure 1 for the diﬀerent values of β
and a goal value of 1000.

P

According to the deﬁnition of these reward functions, an immediate negative reward is obtained
for completing each task. Conversely, an immediate positive reward is obtained after the goal-
accomplishing task-level state has been reached or a slack-oﬀ action has been chosen for execution.
These deﬁnitions of the reward functions are directly related to the inability to see long-term
consequences of immediate actions in real life. This poses the problem of misalignment between
immediate and long-term rewards which may cause procrastination. We tackle this real-life issue
by aligning immediate rewards with future value using reward-shaping functions [10] (deﬁned in
Section 2.2).

2.1.6 Optimal policy π⋆

First of all, we deﬁne the action-value function Q(level) in a traditional manner. Its deﬁnition is
exactly the same on both levels, but we explicitly write all expressions for clarity. The goal-level
action-value function is deﬁned as

Q(goal)(st, a) =

T (st, a, s′

t+τ )

s′∈S(goal)
X

(cid:20)

r(goal)(st, a, s′

t+τ ) + γτ max
a′∈Ωt+τ

Q(goal)(s′

t+τ , a′)
(cid:21)

whereas the task-level action-value function for a goal g is deﬁned as

Q(g)

(task)(˜s˜t, ˜a) =

X˜s′∈S (g)

(task) X˜τ ∼F (·|˜s˜t,˜a)

T (˜s˜t, ˜a, ˜s′

˜t+˜τ )



r(g)
(task)(˜s˜t, ˜a, ˜s′

˜t+˜τ ) + γ ˜τ max
˜a′∈Ω(g)
˜t+ ˜τ

Q(g)

(task)(˜s′

˜t+˜τ , ˜a′)




3A goal is accomplished iﬀ each task related to that goal has been completed.



8

"

t=t0
X

∞





˜t=˜t0
X

In both equations, T (·, ·, ·) is the probability of transition (as deﬁned in Section 2.1.4), τ is the
time needed to complete a goal-level action a, ˜τ is the time needed to complete a task-level action
˜a, and γ ∈ (0, 1] is a discount factor which can be interpreted as the probability that the decision
maker can continue to act and gather more rewards when they arrive in state s′. Setting γ to a
value less than 1 thereby captures the possibility that the episode described by the SMDP can end
early so that future rewards might become unavailable.

The optimal policy π⋆

(level) is an objective function that maximizes the expected sum of dis-
counted future rewards. Here, we extend the deﬁnition of an optimal policy to be time-dependent,
since the value of the available actions depends not only on the current state s of the SMDP, but
also on the time t at which the decision on the next action is made. The goal-level optimal policy
is deﬁned as

∞

π⋆
(goal)(st0 ) = arg max

π

E

γt · r(goal)(st, π(goal)(st), st+τ )
#

whereas the task-level optimal policy for a goal g ∈ G is deﬁned as

π(g)⋆
(task)(˜s˜t0 ) = arg max

π

E

γ ˜t · r(g)

(task)(˜s˜t, π(g)

(task)(˜s˜t), ˜s˜t+˜τ )



where t0 and ˜t0 represent the current time of the SMDP on a goal level and task level, respectively.
In other words, the objective is to ﬁnd a function that maximizes the cumulative future reward
on both levels by choosing an appropriate action a in any given state s at time t. Written in
terms of the action-value function Q, the goal-level optimal policy for a given goal-level state s
and current time t0 is deﬁned as



π⋆
(goal)(st0 ) = arg max
a∈Ωt

Q⋆

(goal)(s, a)

Similarly, the task-level optimal policy for a goal g at a given task-level state ˜s and current time
˜t0 is deﬁned as

π(g)⋆
(task)(˜s˜t0 ) = arg max
˜a∈Ω(g)

˜t

Q(g)⋆

(task)(˜s, ˜a)

Under the assumption that goal values are non-negative, the maximum potential value of cu-
mulative rewards is obtained when all goal and task deadlines in the to-do list are attained. We
discuss this observation in details in Section 3.1.

2.2 Maximizing the productivity of myopic workers by optimal gamiﬁ-

cation

The SMDP deﬁned above allows us to formalize a person’s productivity from time t1 to time t2 by

P(st1, st2 ) =

V ⋆(st2 ) − V ⋆(st1 )
t2 − t1

,

(1)

where st1 and st2 describes the sets of tasks that the person had completed by time t1 and time t2
respectively.

Following [8], we model people as myopic bounded agents who generally choose tasks based
on the diﬀerence between their immediate reward minus the subjective cost of working on a task,
which we model as the product of the task’s unpleasantness and its duration. That is, we assume
that people select tasks according to the greedy policy

πgreedy(s) = arg max

a

E [r(s, a, s′)] .

(2)

Under this assumption, the problem of maximizing people’s productivity by optimal gamiﬁca-

tion can be formalized as computing optimal incentives f ⋆(s, a) so that

∀s : π⋆(s) ∈ arg max

a

{f ⋆(s, a) + E [r(s, a, s′)]} .

(3)

Lieder et al. [8] proved that this can be achieved by setting f (s, a) to the optimal incentives

f ⋆(s, a) = E [V ⋆(s′)|s, a] − V ⋆(s)
Q⋆(s′, a′) − max

= max
a′

˜a

Q⋆(s, ˜a)

(4)

(5)

9

Figure 2: Complete pipeline. Inputs: Unincentivized to-do list is a complete list of goals and
tasks accompanied by all the relevant information such as goal values, time estimates, deadlines
etc.; Workload represents information on the desired amount of typical daily and today’s working
hours. API: Parsing information is the procedure of parsing the provided to-do list information
as text entries in a JSON dictionary; Solve SMDPs is the procedure that solves the hierarchical
structure of SMDPs; Incentive assignment is the procedure of assigning incentives to tasks; Task
scheduling is the procedure that proposes a task list schedule. Output: Incentivized to-do list is
a gamiﬁed to-do list shown to the user.

To help people choose the most valuable task, their daily to-do list should include the tasks
π⋆(st), π⋆(st+1), · · · that the optimal policy would choose on that particular day (where t is the
ﬁrst time step of the person’s work day). Unfortunately, naive computation of f ⋆ is intractable for
real-world applications. The goal of this text is to present a scalable algorithm for approximate
computation of optimal incentives f ⋆ for real-world applications of productivity apps.

3 Solution

We start this section by introducing inductive biases (Section 3.1) that lead to a drastic reduction
in the problem complexity, tackling the problem of intractable computations for real-world to-do
lists. Then, in the following sections, we describe the pipeline for computing an optimal gamiﬁed
to-do list. That is, we describe the API components, their functionalities and the procedure for
generating a solution.
In Section 3.2, we give a brief description of the procedure that parses
user’s input. Next, in Section 3.3, we describe the procedure that constructs the hierarchy of goal-
and task-level SMDPs and the relations between them, as well as the algorithmic procedure that
solves them. Lastly, in Section 3.4 we describe the procedure that assigns task incentives and the
scheduling procedure that proposes a list of the most-valuable set of tasks to a user. An illustration
of the complete procedure is given in Figure 2.

3.1 Incorporating inductive biases

In order to further reduce the problem complexity, we introduce a set of assumptions that restrict
the potential solution space by removing parts of it that are certainly sub-optimal. These kind of
assumptions are formally known as inductive/learning biases [9]. We ﬁrst describe an inductive
bias in cases where to-do lists have no task deadlines. We then provide a proof that the inductive
bias eliminates computations that cannot be part of an optimal solution. Finally, we extend the
inductive bias to cases in which to-do lists contain task deadlines. As a consequence, the problem of
to-do list gamiﬁcation becomes solvable in a reasonable amount of time (see Section 5 for details).
The inductive biases that we incorporate in the procedure of solving task-level SMDPs with

constant non-zero loss rate λ > 0 (i.e. unit cost of working on a task) are:

10

1. If no task has a deadline (or not attaining one is not penalized), then any sequence of task

execution is an optimal one.

2. Tasks with deadlines should be executed according to the proximity of the deadlines in order

to minimize the total penalty for not attaining those deadlines (if any).

We support the claim for the ﬁrst inductive bias via the following example. Let a goal g ∈ G
have 3 tasks with time estimates {1, 2, 3} such that none of these actions correspond to the slack-oﬀ
action. Concretely, we show that the value of executing the sequence of actions a1 → a2 → a3 is
equivalent to the value of executing a3 → a2 → a1 for arbitrary values of γ and λ.

Q(s0, a1) = r(s0, a1, s1) + γ1Q(s1, a2, s3)

= r(s0, a1, s1) + γ1

r(s1, a2, s3) + γ2Q(s3, a3, s6)

(cid:18)

(cid:19)

= r(s0, a1, s1) + γ1

r(s1, a2, s3) + γ2

r(s3, a3, s6) + γ3R(g) · Π(βg)

(cid:18)

= −λγ0 + γ1

− λ(γ0 + γ1) + γ2

(cid:0)

(cid:19)
(cid:1)
− λ(γ0 + γ1 + γ2) + γ3R(g) · Π(βg)

= −λ(γ0 + γ1 + γ2 + γ3 + γ4 + γ5) + γ6R(g) · Π(βg)

(cid:0)

(cid:18)

(cid:19)

(cid:1)

= −λ(γ0 + γ1 + γ2) + γ3

− λ(γ0 + γ1) + γ2

− λγ0 + γ1R(g) · Π(βg)

(cid:18)

= r(s0, a3, s3) + γ3

r(s3, a2, s5) + γ2

(cid:18)

(cid:0)

= r(s0, a3, s3) + γ3

r(s3, a2, s5) + γ2Q(s5, a1)

(cid:0)

r(s5, a1, s6) + γ1R(g) · Π(βg)

(cid:19)

(cid:1)

(cid:19)
(cid:1)

(cid:18)

= r(s0, a3, s3) + γ3Q(s3, a2, s5)
= Q(s0, a3)
= Q⋆(s0, ·)

(cid:19)

where st denotes the state s at time t and aτ denotes an action a with duration τ . The equality
above holds for all permuted sequences of action execution ai1 → ai2 → ai3 and generalization of
this observation for higher number of actions and varying time estimates applies by induction.

As a consequence of the ﬁrst inductive bias, we are able to compute the Q-value for all sequences
of action execution by evaluating only one (arbitrarily-chosen) sequence of actions. Given this
inductive bias, the aim of solving task-level SMDPs becomes trivial since all sequences of actions
are optimal ones.

However, once task deadlines come into play, the solution loses its triviality. Executing an
arbitrary task at each time step does not guarantee attainability of task deadlines and thus op-
timality. Luckily, although this introduces an overhead expense in the procedure of computing
an optimal task-level policy, dealing with it is not overly complicated. In this setting, where at
least one task has a deadline, the objective is to minimize the penalty induced by not attaining
a deadline in order to maximize the Q-value for a particular sequence of actions. Therefore, an
optimal sequence of actions is one that executes tasks according to the temporal proximity of their
deadlines. Consequently, the optimal policy at each step has to greedily choose the task with the
next upcoming deadline (while breaking ties randomly, if any). Finally, the computation of optimal
task-level policy requires two steps: (1) sort task-level actions in an increasing order according to
their deadlines (for all goals separately); (2) greedily execute actions in that order. We present a
complete algorithmic solution in Section 3.3.

Exact task incentivization requires computing Q-values for all uncompleted task-level actions at
the present state. Doing this in a brute-force manner (i.e. without incorporating inductive biases)
corresponds to exhaustive exploration of the state space and yields a computational time complexity
of O(2n), where n is the total number of tasks on the to-do list. Incorporating inductive biases
reduces the problem complexity from exponential – O(2n) – to quadratic – O(n2). Additionally,
sorting tasks according to task deadlines takes at most O(n log n) computational time, which is
negligible compared to the other terms in the problem complexity.

11

Figure 3: Diﬀerent methods of parsing hierarchical structures.

Figure 4: Algorithmic procedure.

3.2 Parsing information

The parsing component of the API converts an unincentivized to-do list (accompanied with addi-
tional information such as current tasks in that day’s schedule) to information that can generate
a hierarchy of SMDPs and solve them. The information goes through the following parsing sub-
components before it reaches the algorithmic procedure:

1. Parsing hierarchical structure. Hierarchical to-do lists have goals on the main level and
tasks on the sub-levels. Here, we provide two possible methods for parsing the hierarchical
structure of a to-do list (see Figure 3).

• Flattening by taking into account the internal structure of the hierarchy.

• Discarding the internal structure by taking into account only the tasks that have no

sub-tasks (i.e. leaf nodes in the to-do-list tree).

2. Parsing goals and tasks. This sub-component parses goal and task descriptions in order

to extract information provided in them such as values, time estimates etc.

3. Parsing scheduling tags. This sub-component parses information on the desired “do"
days/dates for each task on a to-do list. This information is necessary so that the API knows
when to propose a given tasks, as well as how much working time is available for each day
in the week.

4. Parsing deadlines. Here, the API uses the previously-parsed information on today’s work-
load and the typical day’s workload in order to calculate the available working time (in
minutes) until the speciﬁed deadline for each task that has one.

Once the parsing procedure is done, the API checks whether all goal values and the average goal
values (calculated as a fraction of the goal value over its total time estimate) are within predeﬁned
ranges. If both conditions are satisﬁed then the parsed information is forwarded to the procedure
that generates and solves SMDPs. Otherwise, the user is asked to modify the inputted to-do list.

3.3 Solving SMDPs

In this section, we present an algorithmic procedure that eﬃciently solves a hierarchy of SMDPs.
In general, the complete procedure is divided into four major methods which are executed in a
hierarchical manner. Here, we give a brief description of their functionalities. A more detailed
description of the procedure is provided in a form of a pseudo-code in Algorithms 1, 2, 3, and 4.
A graphical representation of the complete procedure is provided in Figure 4.

12

Initially, the solve_to_do_list method (Algorithm 1) is called. This method initializes the
global parameters (i.e. discount factor γ, loss rate λ, penalty rate ψ), incorporates the induc-
tive biases deﬁned in Section 3.1 by generating sorted lists of tasks and initiates the method for
computing goal-level Q-values for each possible goal ordering (i.e. solve_next_goals). Once the
goal-level Q-values are computed, the method computes the optimal policy of the goal-level SMDP
π⋆
(goal), collects all task-level actions, updates their Q-value for the initial task-level state ˜st0 with
the Q-value of the succeeding optimal goal, and computes task incentives at initial time t0.

The solve_next_goals method (Algorithm 2) is initiated by the solve_to_do_list method
and provided with the information of the current goal-level state s, current goal-level time t and
the set of uncompleted goals UG (which is initially the complete set of goals G). This method
computes the Q-values for all possible goal orderings (i.e. goal-level state-time pairs st) by solving
each individual goal via the solve_goal method in a recursive manner. Since the method computes
Q-values for all possible permutations of goal orderings, its computational time is O(|G|!), where
|G| is the total number of goals in a to-do list.

The solve_goal method (Algorithm 3) initiates the recursive procedure of computing task-level
Q-values for a given goal g at a given initial time ˜t0. Its purpose is to to compute expected future
rewards for all task-level actions that might appear on the user’s schedule for today (i.e. at present
time ˜t0 = 0) given the inductive biases deﬁned in Section 3.1. The computation of the task-level
Q-values is done completely within the solve_next_tasks method. Once the task-level Q-values
for the given goal are computed, the method computes the optimal task-level policy π⋆(g)
(task). The
computational cost of this method is O(˜n), where ˜n is the total number of tasks within the goal.
The solve_next_tasks method (Algorithm 4) is initiated by the solve_goal method and
provided with the information of the current task-level state ˜s, current task-level time ˜t, sorted list
of tasks q, index that keeps track of the solved tasks in the list i, as well as an optional task-level
action ˜a to be executed next. This method computes the Q-value for a path in the restricted (by
the inductive biases) set of possible paths. It starts by searching for the next uncompleted action
in the sorted list of actions. If such a task exists, it makes the necessary state and time transitions
and it makes a recursive call from the next state and time. If no such task exists, this absence
implies that a terminal state has been reached, in which case the procedure stores the (potentially
penalized) goal reward as a Q-value for that state and time. The computational cost of this method
is O(˜nb), where ˜n is the total number of tasks within the goal and b is a branching factor encoding
the number of possible time transitions.

3.4 Incentivizing and scheduling tasks

The aim of this work is to develop a gamiﬁed to-do list app that helps people overcome the
motivational obstacles that result from the misalignment between the immediate cost of work and
its long-term beneﬁts. For that matter, we leverage the method of optimal gamiﬁcation presented
in Section 2.2 to compute pseudo-rewards f (st, a, s′
t′ ) for completing task a in state s at time t
leading to state s′ at time t′. We compute those point values by combining the pseudo-rewards
computed by the optimal reward shaping method by Ng et al.
[10] with the original reward
r(st, a, s′

t′) for performing the task, that is

f (st, a) = f ′(s, a) + E [r(st, a, s′

t′)]

where f ′(s, a) = m · f ⋆(st, a) + b is an optimality-preserving linear function of the optimal pseudo-
rewards f ⋆(s, a) deﬁned in Equation 4. Adding the rewards r to the pseudo-rewards is also opti-
mality preserving because this is equivalent to multiplying the reward function by 2 before adding
the optimality preserving pseudo-rewards. Concretely, we use the following linear transformation
in order to ensure that the sum of all pseudo-rewards equals the sum of goal values. To break
ties between multiple optimal tasks by preferring longer tasks over shorter ones, we set the scaling
parameter m = 1.1 and we derive the value for the bias parameter b in the following manner:

13

Algorithm: solve_to_do_list(s, t0, G, γ, λ, ψ)
Input: Initial state s
Initial time t0
Set of goals G
Discount factor γ
Loss rate λ
Penalty rate ψ

Set γ, λ, ψ as global variables
foreach goal g ∈ G do

Sort tasks by deadlines and store them in q(g)

end

solve_next_goals(s, t0, G) (Solve to-do list recursively)
π⋆
(goal)(st0) ← arg max
a∈Ωt0

(goal)(st0 , a) (Store optimal policy)

Q⋆

A ← {} (Initialize collection of task-level actions)
foreach goal g ∈ G do

Combine task Q-values with the Q-value of the next optimal goal
foreach ˜a ∈ A do

(task)(˜st0 , ˜a) ← Q(g)⋆
Q(g)⋆

(task)(˜st0, ˜a) + max
a′∈Ωt′

Q⋆

(goal)(s′

t′ , a′)

end
A ← A ∪ A(g)

(task) (Update collection of tasks)

end

Compute incentives r′(·, ·, ·) for all tasks A at t0

Algorithm 1: Pseudo-code for the method that initiates computing goal-level Q-values,
and computes the goal-level optimal policy and goal-level pseudo-rewards.

Algorithm: solve_next_goals(s, t, UG)
Input: Current state s;
Current time t;
Set of uncompleted goals UG;

if UG 6= ∅ then

foreach uncompleted goal g ∈ UG do
τ ← (Get goal time estimate)
t′ ← t + τ (Move τ time steps in the future)
s′
t′ ← (Move to next state by taking action a in state st)

if Q⋆

(goal)(st, g) not computed then

solve_goal(g, t)
solve_next_goals(s′, t′, UG − {g}) (Make a recursive call )

end

Q⋆

(goal)(st, g) ← r(st, g, s′
π⋆
(goal)(s′

t′ ) ← arg max
a′∈Ωt′

Q⋆

t′) + γτ max
a′∈Ωt′
Q⋆
(goal)(s′
t′ , a′) (Store optimal policy)

(goal)(s′

t′ , a′) (Update Q-value)

end

else

Q⋆

(goal)(st, ⊥) ← 0 (Initialize terminal state)

end

Algorithm 2: Pseudo-code for the method that computes goal-level Q-values.

14

Algorithm: solve_goal(g, ˜t0)
Input: Goal g;

Initial time ˜t0;

˜s ← (Initialize task-level state)
q(g) ← (Get list of sorted tasks by deadlines)

if

t = 0 then
foreach ˜a ∈ q do

solve_next_tasks(˜s, ˜t0, q(g), 0, ˜a)

end

else

solve_next_tasks(˜s, ˜t0, q(g), 0, /)

end

π⋆(g)
(task)(˜s˜t0 ) ← arg max
˜a∈Ω(g)
˜t0

Q⋆(g)

(task)(˜s˜t0 , ˜a)

Algorithm 3: Pseudo-code for the method that initiates computing task-level Q-values,
and computes task-level optimal policy and task-level pseudo-rewards for a given goal.

(m · f ⋆(s0, a) + b + E [r(s0, a, s′

t′ )]) =

R(g)

a
X
f ⋆(s0, a) +

m ·

b +

E [r(s0, a, s′

t′)] =

a
X

f ⋆(s0, a) + n · b +

a
X

m ·

a
X

a
X

a
X

E [r(s0, a, s′

t′)] =

g∈G R(g) − m ·

b =

P

a f ⋆(s0, a) −
n

a

P

P

g∈G
X

g∈G
X

R(g)

R(g)

g∈G
X

E [r(s0, a, s′

t′)]

Once the pseudo-rewards are computed, the scheduling procedure acquires the most-valuable
set of tasks that do not exceed the desired user workload as well as other tasks to be scheduled for
the present day, rounds their incentives to a pre-deﬁned number of decimals, and proposes them
as next tasks to be executed.

4 API

The API that we have developed can serve as a back-end to any to-do list gamiﬁcation application.
It is available online at https://github.com/RationalityEnhancementGroup/todolistAPI.
Details on how to communicate with the API can be found in the README.md ﬁle there, and we
brieﬂy describe the communication in the following paragraphs and the Appendix.

In our speciﬁc case, we coupled the API with a research version of the productivity appli-
cation named CompliceX4, which has been developed especially for to-do-list gamiﬁcation. The
communication between the gamiﬁcation application and the API occurs in the following manner:

1. The gamiﬁcation application sends a POST request with a speciﬁc URL to the API hosted
on Heroku5, which provides information about the to-do list in JSON format. Details on the
format expected as input are provided in Section A1.

2. After the POST request is received, the API parses the provided to-do-list information,
computes task incentives, proposes a daily schedule of incentivized tasks, and sends this
information back to the gamiﬁcation application in JSON format. Details on the output that
the API returns are provided in Section A2.

4https://reg.complicex.science/
5https://safe-retreat-20317.herokuapp.com/

15

Algorithm: get_next_task(˜s, ˜t, q, i, ˜a)
Input: Current state ˜s;
Current time ˜t;
List of sorted tasks by deadline q;
Index for the list of sorted tasks by deadlines i;
Next action ˜a (optional);

if ˜a is not provided then
while i < length(q) do

˜a ← (Get the next (i-th) task from q)
if ˜a is completed then

i ← i + 1

else

break

end

end

end

if ∃ uncompleted task ˜a then
d ← get task time deadline
T ← (Get all potential time transitions and corresponding probabilities)
Q(g)

(task)(˜s˜t, ˜a) ← 0 (Initialize Q-value)

foreach (Transition time ˜τ , Transition probability ˜p) ∈ T do

˜t′ ← ˜t + ˜τ (Move ˜τ time steps in the future)
˜s′
˜t′ ← (Move to next state by taking action ˜a in state ˜s˜t)
r(˜s˜t, ˜a, ˜s′
βg ← βg + I(˜t′ > d) · (˜t′ − d) · ψ (Update penalty if missed deadline)

˜τ −1
k=0 γkλ (Calculate total loss for action ˜a)

˜t′) ← −

P

get_next_task(˜s′, ˜t′, q, i, /)

(task)(˜s˜t, ˜a) ← Q(g)
Q(g)
π(g)
(task)( ˜s′

˜t′ ) ← arg max

(task)(˜s˜t, ˜a) + ˜p ·
Q(g)
(task)( ˜s′

r(˜s˜t, ˜a, ˜s′
h
˜t′ , ˜a′)

˜a′

˜t′) + γ ˜τ · max
˜a′

Q(g)

(task)(˜s′

˜t′ , ˜a′)
i

end

else

Q(g)

(task)(˜s˜t, ˜⊥) ← R(g) · Π(βg) (Initialize terminal state)

end

Algorithm 4: Pseudo-code for the method for computing task-level Q-values recursively.

Additionally, CompliceX communicates with a productivity application named WorkFlowy6,
but we omit describing their communication since the API communicates exclusively with Com-
pliceX. Examples of input and output in JSON format can be found in the examples/use_cases
folder of the project’s GitHub repository.

5 Evaluations

5.1 Inspecting pseudo-rewards

In order to ensure that the computed task incentives are meaningful in real-world applications, we
assess them according to the following criteria:

• An optimal task is assigned the maximum number of points.

6We recommend opusfluxus as a NodeJS wrapper for WorkFlowy. The source code is available on GitHub -

https://github.com/malcolmocean/opusﬂuxus

16

• Incentives of optimal tasks at current time are as high as or higher than what they were in

the previous time step.

• The number of points and their diﬀerences across tasks motivate the selection of one task

over others.

To illustrate this, we provide a use case of a student’s to-do list with 3 long-term goals. Figure 5
shows an unincentivized to-do list with all details about its goals and tasks. Figure 6 shows an
incentivized to-do list, in which the ﬁrst 4 tasks are scheduled for the current day according to
the desired workload, as well as updated incentive values after the next optimal task has been
completed. For this use case example, our method used 2 possible task durations and scaled the
user-provided time estimates by a planning fallacy constant of 1.39. As a result, we show the scaled
time estimates to the user in order to make the user aware of potential discrepancies between the
estimated time and the real-world task-execution time. Moreover, the incentives are computed by
optimal gamiﬁcation and shifted such that they convey the long-term value of performing the task
(as deﬁned in Section 3.4). The incentives are scaled to the closest integer and their broad range
ensures that the incentives are scaled up with the mental eﬀort needed to execute these tasks.

5.2 Speed and reliability tests in diﬀerent scenarios

To show that the API is scalable in various real-world scenarios, we deployed the API on Heroku and
tested its speed and reliability as performance measures. We measure reliability by the proportion
of trials in which the API responds without throwing a timeout error. The default timeout for a
request on Heroku is 30 seconds. Since we want users to be able to interact with our API in real
time, we set 28 seconds as an upper limit for the API to process a request. Concretely, we tested
the API for for various number of daily working hours (i.e. 8, 12, 16), number of goals (i.e. 1
to 10), number of tasks per goal (i.e. 10 to 250), and number of possible task durations per task
(i.e. 1 and 2). For simplicity, we ﬁxed the average task time duration to be 15 minutes, which
corresponds to 32, 48 and 64 daily tasks for 8, 12, and 16 daily working hours, respectively.

Results from the speed and reliability tests in the case of only 1 potential task duration for
8, 12, and 16 daily working hours are provided in Figures 7, 9, 11, 13, 15, and 17. According to
them, we expect the API to support most of the real-world to-do lists (e.g. 5 goals wtih 150 tasks,
8 goals with 100 tasks etc.) Unfortunately, we cannot make the same statement in the case of 2
potential task durations. The results presented in Figures 8, 10, 12, 14, 16, and 18 show that the
API can support no more that 6 goals with 10 tasks per goal.

In conclusion, we expect the API to be scalable for most to-do lists encountered in real-world
scenarios when a single possible task duration is taken into account. Future work will be directed
towards improving the scalability and reliability of the algorithm for multiple possible task du-
rations and will report detailed information on the structure of real-world to-do lists, number of
goals, number of tasks, proportion of task deadlines etc.

5.3 Comparison to non-hierarchical SMDP solving methods

The hierarchical SMDP method is optimal in cases where to-do lists have no task deadlines.
However, there might exist discrepancies in cases where non-atomic goal execution is required
in order to meet task deadlines. We illustrate this observation via a simple example.

Let there be two goals with two tasks each.

• Goal 1 (G1) | Value: 500

– Task 1 (G1-T1) | Time estimate: 1 | Deadline: 1

– Task 2 (G1-T2) | Time estimate: 3 | Deadline: 6

• Goal 2 (G2) | Value: 500

– Task 1 (G2-T1) | Time estimate: 2 | Deadline: 3

– Task 2 (G2-T2) | Time estimate: 4 | Deadline: 10

The optimal solution obtained by a non-hierarchical SMDP is to execute the following sequence of
actions: G1-T1 → G2-T1 → G1-T2 → G2-T2, but policy found by the hierarchical SMDP does not

17

match the optimal solution. The policy obtained by the hierarchical SMDP chooses and atomically
executes the most-valuable goal by trading oﬀ between the loss incurred by not attaining deadlines
and the goal values.

Theoretically, the number of mismatches in the sequence of actions in the worst case is O(|G|·˜n),
where |G| is the total number of goals and ˜n is the highest number of tasks within a goal in a to-do
list. This occurs when tasks have to be executed sequentially (e.g. G1-T1 → G2-T1 → ... →
GN-T1 → G1-T2 → ... → GN-T2 → ...). Furthermore, it holds in general that the value diﬀerence
between the hierarchical and non-hierarchical SMDP method is equivalent to the loss incurred by
missing task deadlines that are not detected as attainable. However, we believe that the hierarchical
decomposition of the solution method does not introduce large deviations from optimality since
the task incentives are updated after every task completion. We leave investigating this issue in
real-world to-do lists for future work.

Regarding the execution time, we found that in comparison with other algorithms, the hierar-
chical SMDP method clearly outperforms alternative methods such as Backward Induction (BI)
and Value Iteration (VI). A visual representation of this observation is shown in Figures 19 and 20,
where the BI and VI algorithms perform worse even for a small amount of tasks (up to 16) with
a single possible task duration. Concretely, while the Backward Induction algorithm struggles to
solve a to-do list with 16 tasks (runtime of about 193 seconds), the hierarchical SMDP algorithm
is able to solve 10 to-do lists with a total number of 800 tasks in comparable time (about 191
seconds).

6 Future work

We consider multiple potential ways to improve the API in order to make its functionality even
closer to real-world demands. On one hand, usability enhancements will include supporting tasks
that contribute to multiple goals simultaneously as well as multi-level hierarchical to-do lists. Fur-
thermore, we will support dependencies between tasks imposed by users in order to form sequences
of tasks that have to be executed in a particular order. Additionally, we will allow users to specify
desired workload for each weekday. On the other hand, algorithmic enhancements will include
support for multiple possible task durations in order to model multiple real-life situations while
retaining (or even decreasing) computational cost. Decreasing the time complexity is of the highest
priority. One possible future improvement would be to produce fast responses after making minor
changes in the input information even if the changes may modify the solution.

Acknowledgment

This work was supported by grant number 1757269 from the National Science Foundation.

18

1) Learn mathematical foundations of machine learning (deadline: 2021-04-30;

value: 1000)

• Lectures

• Read lecture notes for the next lecture (time est: 3 hours; “do” days: Wednesdays)
• Attend lecture (time est: 2 hours; “do” days: Thursdays)

• Weekly assignments

• Solve exercises (time est: 3 hours; “do” days: Mondays)
• Write down solutions in LATEX(time est: 1 hour; “do” days: Mondays)
• Submit solutions (time est: 30 minutes; “do” days: Thursdays)

• Final exam (time est: 2 hours; “do” date: 2021-02-20)

• Everything else (time est: 60 hours)

2) Take part in the seminar on causal inference (deadline: 2020-09-30; value: 500)

• Prepare for the next session

• Read Spohn’s “Causation: An Alternative” (time est: 4 hours; “do” day: Wednes-

day)

• Presentation

• Read Hájek’s “Interpretations of Probability” (time est: 2 hours; “do” day: today)
• Compose slides for presentation (time est: 2 hours; deadline: 2020-09-14)
• Send presentation (time est: 30 minutes; deadline: 2020-09-21)
• Practice presentation (time est: 10 hours; deadline: 2020-09-28)
• Presentation day (time est: 2 hours; deadline: 2020-09-28)

• Everything else (time est: 20 hours)

3) Pursue a master’s degree in machine learning (deadline: 2021-09-30; value: 5000)

• Summer semester 2020

• Read exam regulations (time est: 1 hour; “do” day: today)
• Register for exams (time est: 2 hours; deadline: 2020-08-30)

• Winter semester 2020

• Choose courses for the next semester (time est: 4 hours; deadline: 2020-10-31)

• Summer semester 2021

• Explore potential topics for master thesis (time est: 50 hours; deadline: 2021-03-31)
• Write master thesis (time est: 400 hours)
• Prepare master thesis defense (time est: 50 hours)
• Defend master thesis (time est: 2 hours; “do” date: 2021-09-30)

+ Today’s working hours: 10 hours

+ Typical day’s working hours: 10 hours

Figure 5: Unincentivized to-do list.

19

To do

Today

1) Solve exercises (takes about 4 hours and 11 minutes)
1) Write solutions in LaTeX (takes about 1 hour and 24 minutes)
2) Read Hájek’s “Interpretations of Probability” (takes about 2 hours and 47 minutes)
3) Read exam regulations (takes about 1 hour and 24 minutes)

Complete

1st task




y

To do

Today

1) Solve exercises (takes about 4 hours and 11 minutes)
1) Write solutions in LaTeX (takes about 1 hour and 24 minutes)
2) Read Hájek’s “Interpretations of Probability” (takes about 2 hours and 47 minutes)
3) Read exam regulations (takes about 1 hour and 24 minutes)

Figure 6: Incentivized to-do list generated on 2020-08-03 (Monday) at 08:00. URL parameters:
smdp/mdp/30/14/inf/0/inf/0/inf/false/0/max/0.999999/0.1/2/1.39/0.0001/0.01/tree/u123/getTasksForToday

Points

686
684
683
619

Points

684
683
619

.

Time (seconds)

4.01

10.72

11.42

16.10

1.57

2.96

4.31

8.53

9.95

21.34

22.77

1.09

2.26

2.63

5.42

9.62

11.69

18.16

0.72

1.77

2.42

3.35

6.27

7.23

16.09

19.10

0.42

1.24

1.43

2.31

3.44

4.73

7.83

13.86

24.38

0.22

0.63

0.70

0.92

1.48

2.88

5.42

6.20

15.02

25.09

0.09

0.15

0.27

0.25

0.46

0.87

1.21

2.31

5.27

11.30

0.01

0.06

0.05

0.05

0.12

0.29

0.44

1.11

2.30

3.37

1

2

3

4

5

6

7

8

9

10

Number of goals

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Figure 7: Speed tests for scenario: 8 working hours, 15 minutes average task time estimate, 1
possible task duration.

20

 
 
 
 
l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time (seconds)

1.08

2.32

5.50

7.70

11.69

22.99

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 8: Speed tests for scenario: 8 working hours, 15 minutes average task time estimate, 2
possible task durations.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Time-outs (proportion)

0.00

0.00

0.00

0.00

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.20

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.60

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 9: Reliability tests for scenario: 8 working hours, 15 minutes average task time estimate, 1
possible task duration.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time-outs (proportion)

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

1.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 10: Reliability tests for scenario: 8 working hours, 15 minutes average task time estimate,
2 possible task durations.

21

 
 
 
 
 
 
 
 
 
 
 
 
l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Time (seconds)

6.44

14.01

14.25

24.86

1.88

4.18

5.68

9.49

14.80

21.76

25.81

1.42

3.83

4.01

7.26

12.73

15.15

17.78

1.02

2.23

2.96

4.76

11.43

9.16

16.75

21.16

0.68

2.12

2.16

3.74

5.07

5.34

9.54

16.20

26.04

0.39

0.61

1.10

1.42

2.09

4.38

5.99

8.29

15.77

25.50

0.08

0.16

0.30

0.25

0.48

0.89

1.35

2.30

4.78

10.04

0.01

0.03

0.04

0.06

0.22

0.34

0.47

0.91

1.86

3.63

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 11: Speed tests for scenario: 12 working hours, 15 minutes average task time estimate, 1
possible task duration.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time (seconds)

0.93

2.69

6.19

6.96

11.46

21.61

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 12: Speed tests for scenario: 12 working hours, 15 minutes average task time estimate, 2
possible task durations.

22

 
 
 
 
 
 
 
 
l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Time-outs (proportion)

0.00

0.00

0.00

0.60

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.80

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.20

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 13: Reliability tests for scenario: 12 working hours, 15 minutes average task time estimate,
1 possible task duration.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time-outs (proportion)

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

1.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 14: Reliability tests for scenario: 12 working hours, 15 minutes average task time estimate,
2 possible task durations.

23

 
 
 
 
 
 
 
 
l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Time (seconds)

8.55

17.36

19.77

2.52

5.45

7.75

14.84

21.72

22.35

1.99

5.27

4.60

15.04

17.17

18.32

21.50

1.23

2.84

3.80

5.21

11.69

12.30

20.78

26.67

0.68

2.21

3.00

3.44

5.89

7.12

9.87

22.85

0.38

0.75

1.09

1.16

2.23

3.31

5.90

9.05

14.78

0.07

0.19

0.31

0.27

0.68

0.85

1.30

2.28

5.22

10.46

0.01

0.04

0.05

0.06

0.12

0.30

0.38

0.82

2.13

3.39

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 15: Speed tests for scenario: 16 working hours, 15 minutes average task time estimate, 1
possible task duration.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time (seconds)

0.90

3.11

4.48

6.44

14.93

23.29

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 16: Speed tests for scenario: 16 working hours, 15 minutes average task time estimate, 2
possible task durations.

24

 
 
 
 
 
 
 
 
l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

0
5
2

0
5
1

5
2
1

0
0
1

5
7

0
5

5
2

0
1

Time-outs (proportion)

0.00

0.00

0.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.20

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.60

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.40

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 17: Reliability tests for scenario: 16 working hours, 15 minutes average task time estimate,
1 possible task duration.

l

a
o
g

r
e
p

s
k
s
a
t

f
o

r
e
b
m
u
N

5
2

0
1

Time-outs (proportion)

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

1.00

0.00

0.00

0.00

0.00

0.00

0.00

1.00

1.00

1.00

1.00

1

2

3

4

5

6

7

8

9

10

Number of goals

Figure 18: Reliability tests for scenario: 16 working hours, 15 minutes average task time estimate,
2 possible task durations.

25

 
 
 
 
 
 
 
 
m
h
t
i
r
o
g
A

l

P
D
M
S

I

B

I

V

0.004

0.006

0.010

0.016

0.004

0.132

5.765

193.075

0.011

0.809

42.033

1455.824

4

8
Total number of tasks

12

16

Figure 19: Speed test comparison of various solving methods (SMDP; Backward Induction - BI;
Value Iteration - VI) as a function of total number of the tasks with a single possible task duration.

SMDP
Backward Induction
Value Iteration

)
s
d
n
o
c
e
s
(

e
m
T

i

1400

1200

1000

800

600

400

200

0

4

6

8

10
Total number of tasks

12

14

16

Figure 20: Speed test comparison of various solving methods (SMDP; Backward Induction - BI;
Value Iteration - VI) as a function of total number of the tasks with a single possible task duration.

26

 
References

[1] Yossi Aviv and Amit Pazgal. A partially observed markov decision process for dynamic pricing.

Management science, 51(9):1400–1416, 2005.

[2] Richard Bellman. A markovian decision process. Journal of mathematics and mechanics,

pages 679–684, 1957.

[3] Shalabh Bhatnagar, Emmanuel Fernández-Gaucherand, Michael C Fu, Ying He, and Steven I
Marcus. A markov decision process model for capacity expansion and allocation. In Proceedings
of the 38th IEEE Conference on Decision and Control (Cat. No. 99CH36304), volume 2, pages
1380–1385. IEEE, 1999.

[4] Adam Maria Gadomski, Sandro Bologna, Giovanni Di Costanzo, Anna Perini, and Marco
Schaerf. Towards intelligent decision support systems for emergency managers: the ida ap-
proach. International Journal of Risk Assessment and Management, 2(3-4):224–242, 2001.

[5] Ronald A Howard. Semi-markovian decision-processes. Bulletin of the International Statistical

Institute, 40(2):625–652, 1963.

[6] Daniel Kahneman and Amos Tversky. Intuitive prediction: Biases and corrective procedures.

Technical report, Decisions and Designs Inc Mclean Va, 1977.

[7] William R King and Talmadge A Wilson. Subjective time estimates in critical path plan-

ning—a preliminary analysis. Management Science, 13(5):307–320, 1967.

[8] Falk Lieder, Owen X Chen, Paul M Krueger, and Thomas L Griﬃths. Cognitive prostheses

for goal achievement. Nature human behaviour, 3(10):1096–1106, 2019.

[9] Tom M Mitchell. The need for biases in learning generalizations. Department of Computer

Science, Laboratory for Computer Science Research . . . , 1980.

[10] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transfor-
mations: Theory and application to reward shaping. In ICML, volume 99, pages 278–287,
1999.

[11] Luiz Guilherme Nadal Nunes, Solon Venancio de Carvalho, and Rita de Cássia Meneses Ro-
drigues. Markov decision process applied to the control of hospital elective admissions. Arti-
ﬁcial intelligence in medicine, 47(2):159–171, 2009.

[12] Haili Song, C-C Liu, Jacques Lawarrée, and Robert W Dahlgren. Optimal electricity supply
IEEE transactions on power systems, 15(2):618–624,

bidding by markov decision process.
2000.

[13] Piers Steel. The nature of procrastination: A meta-analytic and theoretical review of

quintessential self-regulatory failure. Psychological bulletin, 133(1):65, 2007.

[14] Ernst Zermelo. Über eine anwendung der mengenlehre auf die theorie des schachspiels. In
Proceedings of the ﬁfth international congress of mathematicians, volume 2, pages 501–504.
II, Cambridge UP, Cambridge, 1913.

27

Appendix

A1 API input

The API receives a POST request with a speciﬁc URL and body in JSON format as input from
the gamiﬁcation application. The general pattern of the URL is as follows:
http://<server>/api/<compulsoryParameters>/<additionalParameters>/tree/<userID>/<functionName> .

• <server> : DNS or IP address of the server.

• <compulsoryParameters> : Parameters that are independent of the incentivizing method.

• <additionalParameters> : Parameters that are related to the incentivizing method.

• <userID> : A unique user identiﬁcation code.

• <functionName> : The functionality that the API should provide.

Please note that you must set up a MongoDB database in order to be able to store the information
generated by the API, and to add the parameter as a conﬁguration variable so that it can be
accessed in the main block in the ﬁle app.py . For details on each of these items, please refer to
the README.md ﬁle of the repository.

Additionally, the request must contain a body in JSON format with the following information:

• currentIntentionsList : List of tasks that have already been scheduled. Each item in

this list represents a scheduled task and it has to contain the following information:

– _c : Goal code/number.

– _id : Unique identiﬁcation code of the scheduled task.

– d : Whether the scheduled task has been completed or not.

– nvm : Whether the scheduled task has been marked to be completed at some other

time.

– t : Title of the scheduled task.

– vd : Value of the scheduled task.

• projects : Tree of goals and their respective tasks. Each item (goal or task) is composed

of the following information:

– id : Unique identiﬁcation code of the item.

– nm : Title of the item.

– lm : Time stamp of item’s last modiﬁcation.

– cp : Time stamp of item’s completion.

– ch : Sub-items of the current item.

• timezoneOffsetMinutes : Time diﬀerence in minutes between user’s time zone and UTC.

• today_hours : Number of hours that a user would like to work on the current day (today).

• typical_hours : Number of hours that a user would like to work on a typical day.

• userkey : Unique identiﬁcation code of the user.

• updated : Time stamp of the last modiﬁcation of the items in the projects tree.

Each to-do-list item (i.e. goal or task) title follows patterns that encode all the necessary

information. The following list describes these patterns in detail:

28

• #CG<N>_<goal_name> deﬁnes a goal name, where <N> is the number of the goal and

<goal_name> is the actual goal name speciﬁed by the user.

• ==<value> deﬁnes a value of a goal/tasks, where <value> ∈ Z≥0.

• DUE:<YYYY-MM-DD> <HH:mm> deﬁnes a deadline, where <YYYY-MM-DD> deﬁnes a date ac-
cording to the ISO format and <HH:mm> deﬁnes a 24-hours day time. If <HH:mm> is not
provided, then 23:59 is taken as a default day-time value.

• ∼∼<time_estimate> <time_unit> deﬁnes a time estimate for a task, where <time_estimate> min

∈ N corresponds to the number of minutes or <time_estimate> h ∈ R>0 corresponds to
the amount of hours.

• #HOURS_TYPICAL ==<hours> deﬁnes the total number of hours per day, i.e. the amount of

hours ∈ (0, 24] that a user wants to work on a typical day.

• #HOURS_TODAY ==<hours> deﬁnes the total number of hours for today, i.e. the amount of

hours ∈ (0, 24] that a user wants to work today.

• Scheduling tags that users can accompany to their tasks:

– #daily represents a task that is repetitive on a daily basis.

– #future represents a task that has to be scheduled at some point in the future, but

not at the moment.

– #today represents a task that has to be scheduled today.

– #<weekday> represents a task that has to be scheduled on a speciﬁc weekday (where
weekday is a day from Monday to Sunday). If this task is repetitive on a weekly basis,
a plural suﬃx is appended to the same tag, i.e. #<weekday>s .

– #weekdays represents a repetitive task that has to be scheduled on each working day

(from Monday to Friday).

– #weekends represents a repetitive task that has to be scheduled during weekends (Sat-

urday and Sunday).

– #YYYY-MM-DD represents a task that has to be scheduled on a speciﬁc day according to

the ISO standard (year-month-day).

A2 API output

After generating incentives for each task in a to-do-list, the API selects a subset of them and it
proposes an incentivized daily schedule as output. The output is a list of dictionaries in JSON
format and it contains the following information for each task in the list:

• id : Unique identiﬁcation code of the task.

• nm : Human-readable name of the task.

• lm : Time stamp of task’s last modiﬁcation.

• est : Time estimate of the task.

• parentId : Unique identiﬁcation code of the goal which the task belongs to.

• pcp : Whether the parent node (i.e. goal) has been completed.

• val : Generated incentive for the task.

29

