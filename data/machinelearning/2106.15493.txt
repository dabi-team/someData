Generalized Power Method for Generalized
Orthogonal Procrustes Problem: Global
Convergence and Optimization Landscape Analysis

Shuyang Ling∗

June 30, 2021

Abstract

1
2
0
2

n
u
J

9
2

]
T
I
.
s
c
[

1
v
3
9
4
5
1
.
6
0
1
2
:
v
i
X
r
a

Given a set of multiple point clouds, how to ﬁnd the rigid transformations (rotation,
reﬂection, and shifting) such that these point clouds are well aligned? This problem, known
as the generalized orthogonal Procrustes problem (GOPP), plays a fundamental role in sev-
eral scientiﬁc disciplines including statistics, imaging science and computer vision. Despite
its tremendous practical importance, it is still a challenging computational problem due to
the inherent nonconvexity. In this paper, we study the semideﬁnite programming (SDP)
relaxation of the generalized orthogonal Procrustes problems and prove that the tightness
of the SDP relaxation holds, i.e., the SDP estimator exactly equals the least squares esti-
mator, if the signal-to-noise ratio (SNR) is relatively large. We also prove that an eﬃcient
generalized power method with a proper initialization enjoys global linear convergence to
the least squares estimator. In addition, we analyze the Burer-Monteiro factorization and
show the corresponding optimization landscape is free of spurious local optima if the SNR
is large. This explains why ﬁrst-order Riemannian gradient methods with random initial-
izations usually produce a satisfactory solution despite the nonconvexity. One highlight of
our work is that the theoretical guarantees are purely algebraic and do not require any
assumptions on the statistical property of the noise. Our results partially resolve one open
problem posed in [Bandeira, Khoo, Singer, 2014] on the tightness of the SDP relaxation in
solving the generalized orthogonal Procrustes problem. Numerical simulations are provided
to complement our theoretical analysis.

1 Introduction

Given a set of multiple point clouds, how to ﬁnd the rigid transformations (rotation, reﬂection,
and shift) such that these point clouds are well aligned? More precisely, we are interested in
recovering an underlying point cloud A ∈ Rd×m of m points in Rd, unknown orthogonal matrices
Oi, and shift parameters µi from its n copies of noisy point clouds {Ai}n

i=1:

Ai = Oi(A − µi1(cid:62)

m) + ∆i, 1 ≤ i ≤ n,

where {∆i}n
i=1 is the noise and 1m is a vector of m × 1 with all entries equal to 1. This problem,
known as rigid point clouds registration and also generalized orthogonal Procrustes problem
(GOPP), has found numerous applications in computer vision [42] (multi-view point clouds
registration), statistics [25, 26, 45], shape analysis [49], cryo-electron microscopy [18, 43, 47, 46],
and robotics [44].

Given its tremendous importance in applications, there have been many eﬀorts in solving
the GOPP. One common approach is to ﬁnd the least squares estimator by minimizing a highly
nonconvex objective function. In fact, it is generally an NP-hard problem to obtain the globally
optimal least squares estimator. While local search, spectral methods, convex and nonconvex

∗New York University Shanghai (Email: sl3635@nyu.edu). This work is supported by National Natural Science
Foundation of China (NSFC) via Grant No.12001372 and Shanghai Municipal Education Commission (SMEC)
via Grant 0920000112.

1

 
 
 
 
 
 
approaches are developed to tackle this highly nonconvex program, several theoretical problems
still remain open. In [7], Bandeira, Khoo, and Singer conjectured that a threshold exists such
that if the signal-to-noise ratio (SNR) is above this threshold, the semideﬁnite programming
(SDP) relaxation of the GOPP succeeds in recovering the globally optimal least squares esti-
mator despite its seeming NP-hardness. On the other side, even though the SDP relaxation
has been observed to produce a tight solution sometimes, solving a large-scale SDP is quite
computationally expensive. Therefore, we usually prefer using eﬃcient ﬁrst-order nonconvex
approaches to get an approximate solution to the SDPs at the risk of potentially getting stuck at
spurious local optima. However, there is a huge gap between the theory and practice: the empir-
ical studies indicate that simple ﬁrst-order optimization algorithms are usually quite successful
and do not often suﬀer much from the nonconvexity.

In this work, we will focus on solving the GOPP via convex and nonconvex optimization.

In particular, we would like to answer two core questions:

When does SDP relaxation recover the least squares estimator?

The ﬁrst question is known as the tightness of the SDP relaxation, i.e., the SDP relaxation
sometimes produces the globally optimal least squares estimator. The answer will partially
resolve an open problem in [7].
The other question is:

Can we design an eﬃcient iterative method to recover the least squares estimator exactly?

This question is of great practical importance as it explains why simple ﬁrst-order approaches
are empirically successful in spite of the nonconvexity.

The answers to both questions are related to the signal-to-noise (SNR) ratio: simply speak-
ing, if the SNR is large (i.e., the signal is stronger than the noise), one can ensure the tightness
of the SDP relaxation and moreover it is provably guaranteed to recover the least squares
estimator eﬃciently.

1.1 Related works

Given the extensive literature on the generalized orthogonal Procrustes problem, we are unable
to provide a comprehensive literature review; instead, we will brieﬂy review the works which
are relevant to ours. For a comprehensive discussion on the Procrustes problem, we refer the
interested readers to an excellent book [26].

Orthogonal Procrustes problem is often used in the point clouds registration: given two (or a
few) point clouds, we aim to ﬁnd an optimal rigid transformation and labelings of the two point
sets such that they are well aligned [8, 21, 29, 38]. This is often known as the simultaneous pose
and correspondence matching problem or the Procrustes matching problem. Throughout our
discussion, we will only focus on its subproblem in which the labelings are given and it suﬃces
to ﬁnd the rigid transformation. When n = 2, i.e., there are two point clouds (or two data
matrices), the generalized orthogonal Procrustes problem can be easily solved by performing
singular value decomposition [45]. However, things become much more complicated [25] when
n ≥ 3: in order to ﬁnd the least squares estimator, one needs to ﬁnd the global maximizer to
the following optimization program with orthogonality constraints [41, 48]:

max
Oi∈O(d)

(cid:88)

i<j

Tr(O(cid:62)

i CijOj)

(1.1)

where {Cij}i,j ∈ Rd×d are known and only depend on the given data, and O(d) denotes the set
of all d × d orthogonal matrices. It is a well-known NP-hard problem even if d = 1, which is
closely related to the graph max-cut problem. Therefore, it becomes highly necessary to develop
alternative methods to approximate the global solution to this nonconvex program.

Convex relaxation has proven to be one powerful approach to solve the generalized or-
thogonal Procrustes problem [9, 6, 18, 40, 41, 47, 48]. One popular convex relaxation of (1.1)

2

generalizes the famous Goemans-Williamson relaxation [24] of the graph max-cut. After solving
the convex relaxation, one needs to round the solution to orthogonal matrices. This relaxation
plus rounding strategy has enjoyed remarkable successes and can produce satisfactory approxi-
mate solutions [6, 40]. On the other hand, as pointed out in [7], the SDP relaxation sometimes
directly produces the globally optimal solution to (1.1), i.e., the rounding procedure is not even
necessary. This is often referred to as the tightness of the SDP relaxation: the SDP recovers the
least squares estimator exactly. Similar phenomena have been observed in a series of applications
arising from signal processing and machine learning such as matrix completion [17], blind de-
convolution [3], phase retrieval [15], multi-reference alignment [5], and community detection [1]
in which the optimal solution to the SDP relaxation is exactly the ground truth solution. How-
ever, unlike these aforementioned examples, the least squares estimator usually do not match
the ground truth signal in the generalized orthogonal Procrustes problem. The similar issue
also arises in angular synchronization [4, 60], orthogonal group synchronization [33, 31, 47, 55],
and orthogonal trace-sum maximization [59]. This has created many diﬃculties in analyzing the
tightness of convex relaxation which requires signiﬁcantly diﬀerent techniques from the previous
examples [1, 3, 5, 15, 17].

As convex relaxation usually suﬀers from expensive computational costs, one would deﬁnitely
prefer running an eﬃcient algorithm which hopefully produces an estimator of the same quality
as that from convex methods. In practice, the ﬁrst-order optimization methods [2, 9, 22, 39, 58]
have been quite popular in handling medium/large-scale convex programs. The major theo-
retical question regarding their performance is: why ﬁrst-order methods work well empirically
even though the optimization program is highly nonconvex.
In the past few years, we have
seen signiﬁcant progresses in understanding the performance of many nonconvex approaches
in signal processing and machine learning problems. Roughly speaking, there are two lines of
works regarding the theoretical advances of nonconvex optimization approaches: (a) Find an
initialization smartly and show that global convergence of the algorithm is guaranteed with
this carefully-chosen initialization [16, 19, 28, 37]; (b) Showing that the optimization landscape
is benign, meaning there are no spurious local minima [11, 12, 33, 50, 51, 54] and all local min-
ima are global minima. For the generalized orthogonal Procrustes problem, we will focus on the
generalized power method (GPM) since the multiple point clouds registration problem is closely
related to the group synchronization problem. The GPM was ﬁrst proposed and analyzed in
joint alignment problem [19], angular synchronization [10, 35, 60], and was later extended to
orthogonal group synchronization [36, 31] and community detection under the stochastic block
model [56]. The core question here is to answer when the generalized power method recovers the
least squares estimator which is an NP-hard problem in general. The work [60] by Zhong and
Boumal analyzed the global convergence of the GPM to the maximum likelihood estimator in
angular synchronization. Later on, [36] studied the convergence of the GPM in the orthogonal
group (and its subgroup) synchronization and provided a uniﬁed analysis on the error bound for
the iterates. The analysis of the GPM algorithm combined with spectral initialization in [60]
has also been extended to the orthogonal group synchronization with additive Gaussian noise
in [32, 31].

Regarding the optimization landscape, many works have shown that seemingly nonconvex
objective function is not as “scary” as expected [11, 50, 51]: the landscape is usually benign and
contains only one local optimum that is also global. Examples include phase retrieval [51], phase
synchronization and community detection [11, 34], orthogonal group synchronization [33], ma-
trix completion [23] and dictionary learning [50]. Our work will mainly discuss the optimization
landscape associated with the Burer-Monteiro (BM) factorization approach in solving (1.1). In
the past few years, the BM approach [13, 14] has been shown to have no spurious local minima
for a class of the SDPs (including the SDP relaxation of (1.1)) if the degree of freedom of the
search space is suﬃciently large, namely greater than the square root of the number of the
constraints [12, 54]. However, there is always a tradeoﬀ for the BM factorization: increasing the
dimension of the search space leads to a smaller risk of getting stuck at local minima but also
higher computational costs per iteration. Therefore, one key question is to understand what the
optimal search space should be in this tradeoﬀ, which will be one main topic of our manuscript.

3

Our contribution is multifold. We give a deterministic suﬃcient condition which guarantees
the tightness of semideﬁnite programming relaxation for the GOPP. This partially solves the
open problem proposed in [7]. It is well worth noting that the condition is completely determin-
istic and works for any noise, including random noise as a special case. Also we show that the
convenient generalized power method with a proper spectral initialization produces the globally
optimal solution which matches the optimal SDP estimator and the least squares estimator.
Finally, we show that the optimization landscape associated with the Burer-Monteiro factoriza-
tion is benign as long as the noise is suﬃciently small and the degree of freedom in the search
space is slightly larger than the that of the original program. This sheds light on why ﬁrst-order
gradient methods with random initializations usually have the global convergence. Finally, we
note that the tightness of the SDP relaxation, the global convergence of the GPM, and the opti-
mization landscape of the Burer-Monteiro factorization all depend the signal-to-noise ratio. In
particular, we will mainly restrict our attention to the high SNR regime, i.e., the signal strength
is stronger than the noise. The more challenging low SNR scenario is beyond the scope of this
manuscript. For this, we may refer the readers to a recent work [43] which takes advantage of
the invariant structures to recover the signals in the presence of high noise.

1.2 Organization of our paper

Our paper will proceed as follows.
In Section 2, we will introduce the basics of the gener-
alized orthogonal Procrustes problem and several optimization formulations including convex
relaxation, generalized power method, and the Burer-Monteiro approach. Section 3.1 will fo-
cus on the main theoretical results in this paper and Section 3.2 provides simulation results to
complement our theoretical analysis. The proof will be presented in Section 4.

1.3 Notations

We denote vectors and matrices by boldface letters x and X respectively. For a given matrix X,
X (cid:62) is the transpose of X and X (cid:23) 0 means X is symmetric and positive semideﬁnite. Let In be
the identity matrix of size n×n. For two matrices X and Y of the same size, their inner product
is (cid:104)X, Y (cid:105) = Tr(X (cid:62)Y ) = (cid:80)
i,j XijYij. Let (cid:107)X(cid:107) be the operator norm, (cid:107)X(cid:107)∗ be the nuclear
norm, and (cid:107)X(cid:107)F be the Frobenius norm. We denote the ith largest and the smallest singular
value of X by σi(X) and σmin(X) respectively. We let Jn and 1n be the n × n matrix and n × 1
vector with all entries equal to 1 respectively, and O(d) := {O ∈ Rd×d : O(cid:62)O = OO(cid:62) = Id} is
the set of all d × d orthogonal matrices. For a non-negative function f (x), we write f (x) (cid:46) g(x)
and f (x) = O(g(x)) if there exists a positive constant C0 such that f (x) ≤ C0g(x) for all x.

2 Preliminaries

Let A = [a1, · · · , am] ∈ Rd×m be a point cloud of m samples in Rd with m ≥ d + 1. Suppose
we observe n noisy copies {A}n

i=1 of A with unknown rigid transformations:

Ai = Oi(A − µi1(cid:62)

m) + ∆i, 1 ≤ i ≤ n,

where µi ∈ Rd is the unknown shift and Oi is an unknown orthogonal matrix for the ith point
cloud. The point cloud registration problem asks to recover the unknown orthogonal matrix
Oi (rotation plus reﬂection), the unknown shift µi, and the underlying point cloud A from n
noisy point clouds {Ai}n
i=1 so that these point clouds are aligned as much as possible. This is
also known as the generalized orthogonal Procrustes problem.

One of the most common approaches is to ﬁnd the least squares estimator by minimizing

the following objective function over O, µi, and A:

f (O, µ, A) =

n
(cid:88)

i=1

(cid:107)Oi(A − µi1(cid:62)

m) − Ai(cid:107)2

F =

n
(cid:88)

i=1

(cid:107)A − µi1(cid:62)

m − O(cid:62)

i Ai(cid:107)2
F

4

where O(cid:62) = [O(cid:62)

1 , · · · , O(cid:62)

n ] ∈ Rd×nd is an element in O(d)⊗n.

It is easy to see that the objective function is convex while the orthogonality constraint
on Oi is nonconvex. In fact, recovering the unknown shift is rather simple given Oi while the
major diﬃculty arises from estimating the unknown orthogonal matrix Oi. We will ﬁrst brieﬂy
discuss the shift parameter estimation and then proceed to the estimation of Oi.

2.1 Estimating the unknown means

Let P = m−1Jm = m−11m1(cid:62)
µi1(cid:62)
optima for (A, µ) are derived from

m − O(cid:62)

m be a rank-1 projection matrix and we decompose each A −
i Ai onto the subspace spanned by P and its complement. For each ﬁxed Oi, the

f (O, µ, A) =

n
(cid:88)

i=1

(cid:107)(A − O(cid:62)

i Ai)(Im − P )(cid:107)2

F +

n
(cid:88)

i=1

(cid:107)(A − µi1(cid:62)

m − O(cid:62)

i Ai)P (cid:107)2
F .

Here the minimizer for each µi is independent from one another:

(cid:107)(A − µi1(cid:62)

m − O(cid:62)

i Ai)P (cid:107)2

F = m−1(cid:107)(A − µi1(cid:62)
= m−1(cid:107)(A − O(cid:62)

m − O(cid:62)

i Ai)1m(cid:107)2
F
i Ai)1m − mµi(cid:107)2
F .

which gives the optimal estimator of µi:

(cid:98)µi =

1
m

(A − O(cid:62)

i Ai)1m.

Plugging (cid:98)µi back into f (O, µ, A), we have

f (O, A) =

n
(cid:88)

i=1

(cid:107)(A − O(cid:62)

i Ai)(Im − P )(cid:107)2
F .

The global minimizer for A is

(cid:98)A =

1
n

n
(cid:88)

j=1

O(cid:62)

j Aj(Im − P ).

Substituting (cid:98)A into f (O, A) gives

f (O) =

n
(cid:88)

i=1





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

O(cid:62)

j Aj − O(cid:62)

i Ai



(cid:13)
2
(cid:13)
(cid:13)
 (Im − P )
(cid:13)
(cid:13)
(cid:13)
F

.

By expanding f (O), we notice that it suﬃces to maximize the cross term in f (O):

max
O∈O(d)⊗n

n
(cid:88)

n
(cid:88)

(cid:68)

i=1

j=1

O(cid:62)

i Ai(Im − P ), O(cid:62)

j Aj(Im − P )

(cid:69)

or equivalently

max
O∈O(d)⊗n

n
(cid:88)

n
(cid:88)

(cid:68)

i=1

j=1

Ai(Im − P )A(cid:62)

j , OiO(cid:62)
j

(cid:69)

.

Note that each Ai(Im − P ) is essentially the ith centered point cloud and thus Ai(Im − P )A(cid:62)
j
is the “cross covariance” between two centered point clouds.

5

2.2 Estimation of rotation matrix via SDP relaxation

From the discussion above, we can see that it suﬃces to consider the following model:

Ai = OiA + ∆i ∈ Rd×m,

where A ∈ Rd×m is the data matrix, Oi is an unknown orthogonal matrix, and ∆i is the noise
matrix. Without loss of generality, we can assume the ground truth rotation is Oi = Id. To
write this model in a more compact form, we let

D :=






A1
...
An




 = ZA + ∆ where Z :=






Id
...
Id


 ∈ Rnd×d, ∆ :=







∆1
...
∆n


 ∈ Rnd×m.


(2.1)

The least squares estimation aims to ﬁnd the global maximizer to the following program:

max
Si∈O(d)

(cid:88)

i,j

(cid:104)S(cid:62)

i Ai, S(cid:62)

j Aj(cid:105) = max

S∈O(d)⊗n

(cid:104)C, SS(cid:62)(cid:105)

(P)

1 , · · · , S(cid:62)

n ] ∈ Rd×nd and C = DD(cid:62) (or in block form Cij = AiA(cid:62)

where S(cid:62) = [S(cid:62)
j ) is the data
matrix. However, solving (P) to get the globally optimal least squares estimator is a well-known
NP-hard problem in general. Therefore, one may not hope to ﬁnd the exact global maximizer
without additional assumptions. Among many existing approaches, semideﬁnite programming
(SDP) relaxation is one of the most powerful methods.

One common SDP relaxation uses the fact that G = SS(cid:62) is positive semideﬁnite and also

its diagonal block is Id. Using these two observations gives a convex program:

max (cid:104)C, G(cid:105)

s.t. G (cid:23) 0, Gii = Id,

(SDP)

which is a generalization of the famous Goemans-Williamson relaxation for graph Max-Cut
problem [24]. Despite that this SDP relaxation is solvable with a polynomial-time algorithm,
the main question is whether this SDP produces a solution close to the least squares estimator.
In particular, we are interested in the tightness of this SDP relaxation: when is the optimal
solution of (SDP) equal to that of the original program (P)?

2.3 Generalized power method (GPM)

it is natural to ask: can we
Due to the expensive computational costs in solving (SDP),
develop an alternative eﬃcient approach to ﬁnd the least squares estimator with provable
theoretical guarantees? Inspired by the recent progresses of nonconvex approaches in solv-
ing angular/orthogonal group synchronization, we employ the projected generalized power
method [19, 31, 36, 60] to solve this nonconvex optimization program (P). The algorithm follows
the classical two-step procedure: (a) starting with an initialization S0 via spectral method and
(b) updating the iterate St by

St+1 = Pn(CSt)
where CSt ∈ Rnd×d and Pn(·) : Rnd×d → O(d)⊗n maps each d × d block to a d × d orthogonal
matrix by

(2.2)

P(X) := argminR∈O(d) (cid:107)R − X(cid:107)2
F
whose global minimizer is R = U V (cid:62) where U and V are the left and right singular vectors of
X ∈ Rd×d respectively. The GPM is summarized in Algorithm 1.

(2.3)

Generalized power method is much more eﬃcient than the convex relaxation. However, this
two-step approach is essentially nonconvex and has the risk of getting stuck at stationary points
or local optima. On the other hand, the output of the GPM is usually quite satisfactory and is
even equal to the solution to (SDP). Therefore, we would like to theoretically justify why this
eﬃcient nonconvex approach is successful, especially in the regime of high signal-to-noise ratio
(SNR). We defer the main result to the Section 3.

6

Algorithm 1 Generalized power methods for the GOPP
1: Compute the top d left singular vectors U ∈ Rnd×d of D in (2.1) with U (cid:62)U = Id.
2: Compute P(Ui) for all 1 ≤ i ≤ n where Ui is the ith block of U .
3: Initialize S0 = Pn(U ) ∈ Rnd×d.
4: St+1 = Pn(CSt),
t = 0, 1, · · ·
5: Stop when the iteration stabilizes.

2.4 Optimization landscape of the Burer-Monteiro (BM) factorization

The generalized power method works well empirically to solve the generalized Procrustes prob-
lem if the noise is small compared with the signal. More surprisingly, even without spectral
initialization, the power method still recovers a tight solution. To partially explain this phe-
nomenon, we investigate the performance of the Burer-Monteiro approach. More precisely, we
consider the general signal-plus-noise model

Cij = Π + (cid:101)∆ij

(2.4)

where (cid:101)∆ij is the noise and Π (cid:31) 0 is the signal. This model include our scenario as a special
case if we set Cij = AiA(cid:62)

j and Ai = A + ∆i,

Π = AA(cid:62),

(cid:101)∆ij = A∆(cid:62)

j + ∆iA(cid:62) + ∆i∆(cid:62)
j .

The Burer-Monteiro approach [13, 14, 9] replaces the orthogonality constraints of each Oi

in (P) by an element on the Stiefel manifold [2, 22]:

max
Si∈St(d,p)

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:104)Cij, SiS(cid:62)
j (cid:105)

(BM)

where

St(d, p) := {Si ∈ Rd×p : SiS(cid:62)

i = Id}, p ≥ d.

In particular, if p = d, (BM) equals (P) and if p = nd, (BM) is exactly the SDP relaxation (SDP).
In other words, this family of optimization programs (BM) with diﬀerent p is an “interpolation”
between (P) and (SDP).

This function (BM) is nonconvex (more speciﬁcally, it is non-concave; but we still call it
nonconvex). Despite its nonconvexity, running gradient methods on the Stiefel manifold work
well in practice even with a random initialization. As a result, it is an intriguing question
whether the landscape of (BM) is benign, i.e., there are no spurious local maxima in (BM).
Moreover, we are also interested in whether the global maximizer to (BM) equals that of (P)
and (SDP).

3 Main results

3.1 Main theorems

Before we introduce the main theorem, we ﬁrst get an intuitive idea of when it is possible
to ﬁnd the globally optimal solution to (P). The data matrix C can be rewritten into the
signal-plus-noise form, by using the notation in (2.1),

)(ZA + ∆)(cid:62) = ZAA(cid:62)Z(cid:62) + (cid:101)∆
C = (ZA + ∆
(cid:124)
(cid:125)

(cid:123)(cid:122)
D

where

(cid:101)∆ := (∆A(cid:62)Z(cid:62) + ZA∆(cid:62)) + ∆∆(cid:62).

(3.1)

(3.2)

7

We would expect that if the signal strength is stronger than that of the noise, the GPM will
produce a meaningful solution, i.e.,

(cid:107)∆(cid:107) ≤

√

n(cid:107)A(cid:107).

(3.3)

Our result is close to this ideal bound with additional terms involving condition number and
possibly a dimension factor. Let κ be the condition number of A, i.e., κ = σmax(A)/σmin(A).
Now we present our ﬁrst main result:

Theorem 3.1 (Tightness of the SDP relaxation). Under

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:107)A(cid:107)
192κ4 =

σmin(A)
192κ3 ,

(3.4)

the SDP relaxation is tight and recovers the unique globally optimal solution to (P) (modulo a
global rotation and reﬂection).

The signiﬁcance of Theorem 3.1 is that: this suﬃcient condition (3.4) is completely deter-
ministic and works for any type of noise. We may interpret our result as follows: as long as the
noise for each point cloud is small compared with the data, the (SDP) relaxation produces the
globally optimal solution to (P), namely (P) = (SDP). In other words, in the high SNR regime,
solving a simple convex program ﬁnds the least squares estimator to the generalized Procrustes
problem. It is important to note that the globally optimal solution is not necessarily equal to
the ground truth orthogonal matrices {Oi}n
i=1. The assumption (3.4) is stronger than the ideal
bound (3.3) since we require the noise of every point cloud is weaker than the signal besides the
presence of the condition number κ, which may be improved by using a more reﬁned analysis.
In [7], Bandeira, Khoo, and Singer posed a conjecture for the generalized orthogonal Pro-
crustes problem: under certain statistical model, the SDP relaxation is tight as long as the noise
is smaller than some threshold. If we plug the statistical model (i.e., the data are uniformly
sampled in a hypercube with Gaussian noise) in [7] into the assumption in Theorem 3.1, we
arrive at the following conclusion, resolving the open problem posed in [7].

Theorem 3.2 (Tightness of the SDP under statistical models). Let d > 1, n > 2, and
m ≥ d + 1, and A ∈ Rd×m represent m random points in Rd with ai ∈ Unif[−1, 1]d, and {Oi}n
be any sequence of orthogonal transformations. Suppose we observe the corrupted data:

i=1

Ai = OiA + σWi,

1 ≤ i ≤ n,

where Wi ∈ Rd×m is a Gaussian random matrix.
√

Then if

σ (cid:46)

√

√

d +

m
m +

√

2 log n

(3.5)

the SDP relaxation recovers the maximum likelihood estimator with high probability.

√

√

m and

The proof of Theorem 3.2 is straightforward as we only need to estimate σmin(A) and
max1≤i≤n (cid:107)Wi(cid:107) which are bounded by
2 log n respectively with high
probability. Then invoking Theorem 3.1 immediately ﬁnishes the proof. The details can be
found in Section 4.1.4. However, the current theory is not entirely complete since we do not
discuss whether the bound on σ is optimal. The numerical experiments indicate that the most
likely scaling should be σ = O((cid:112)m/d) instead of O(
2 log n)) when n and
m are both large with d ﬁxed. While our analysis does not rely on the statistical properties of
the noise, we believe using it will deﬁnitely improve the bound on σ.

m +

m +

m/(

d +

d +

√

√

√

√

√

√

Next, we proceed to the global convergence of the GPM. In fact, due to the nature of our
proof, the tightness of the SDP is a by-product of the convergence analysis of the generalized
power method.

8

Theorem 3.3 (Convergence of the GPM). Let (cid:15) be any constant satisfying (cid:15) ≤ 1/(16κ2
Under

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:15)σmin(A)

24κ2 ≤

σmin(A)
√
384κ4
d

,

√

d).

(3.6)

the generalized power method with spectral initialization recovers the globally optimal solution
to the SDP relaxation with linear convergence

dF (St, S∞) ≤ ρtdF (S0, S∞)
where S∞ and S∞(S∞)(cid:62) are the unique globally optimal solution to (P) and (SDP) respectively,
and

(cid:18)

√

ρ = 4κ2

2(cid:15)

d +

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

(cid:19)

< 1.

Theorem 3.3 renders an eﬃcient algorithm to produce the globally optimal solution to (P)
and (SDP), as long as we have picked up an initialization suﬃciently close to the ground truth.
For a concrete example, we let A be the uniform data and ∆i = σWi with Gaussian random
matrix Wi in Theorem 3.2, then (3.6) in Theorem 3.3 becomes

σ (cid:46)

√

√

m +

d(
√

√

√

√
√

m
d + 2

√

log n)

m +

d + 2

where max1≤i≤n (cid:107)∆i(cid:107) (cid:46)
m). The condition
d. This gap mainly arises
is less tight than (3.5) in Theorem 3.2 by an additional factor of
from the initialization step and the analysis in the generalized power method. The proof of
Theorem 3.3 uses the well-known Banach contraction mapping theorem by showing the GPM
is essentially a contraction mapping in a small neighborhood around the ground truth. The
technical details are provided in Section 4.1.

log n and σmin(A) is of order O(

√

√

Finally, we will discuss the optimization landscape of (BM). The main theorem states that
if the noise is small, then the objective function in (BM) has no spurious local maxima. Before
introducing the theorem, we need to deﬁne a rescaled noise matrix associated to the model
in (2.4) and (3.2),

(cid:101)∆Π

ij = Π−1 (cid:101)∆ij,

Π = AA(cid:62).

(3.7)

The equivalent matrix form is

(cid:101)∆Π = blkdiag (cid:0)Π−1, · · · , Π−1(cid:1)

(cid:101)∆ ∈ Rnd×nd

where blkdiag (cid:0)Π−1, · · · , Π−1(cid:1) is a block-diagonal matrix whose diagonal blocks are all Π−1.

Theorem 3.4 (Optimization landscape of the Burer-Monteiro factorization). Con-
sider the optimization program (BM) with p ≥ 2d + 1. The optimization landscape of (BM) is
benign, i.e., there is a unique second-order critical point S ∈ St(d, p)⊗n of (BM) which is also
the global maximizer, under the model Cij = AiA(cid:62)
j

if

max

(cid:111)
(cid:110)
(cid:107) Trd( (cid:101)∆Π)(cid:107), (cid:107) (cid:101)∆Π(cid:107)

≤

n(p − 2d)
√

8κ(p + d)

,

d

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

σmin(A)
12κ

,

where [Trd( (cid:101)∆Π)]ij = Tr(Π−1 (cid:101)∆ij) is the partial trace of (cid:101)∆Π and Π = AA(cid:62) is given in (3.7).
Moreover, this critical point S ∈ St(d, p)⊗n is rank-d and SS(cid:62) is the unique global maximizer
to (SDP).

Theorem 3.4 justiﬁes why nonconvex approaches do not suﬀer from the local maxima in (BM)
as there are no spurious local maxima in (BM) in the high SNR regime. Moreover, the global
maximizer is unique and also equal to the global maximizer to (P) since it is rank-d. This
bound is completely deterministic and can be used to certify whether the landscape is benign
for a given noise level, at the cost of only doubling the dimension of the search space in (P)
2nd for the
by setting p > 2d. The bound is signiﬁcantly better than the general bound p ≈
Burer-Monteiro factorization to be free of spurious local optima [12, 54]. In particular, if we
apply Theorem 3.4 to the signal-plus-normal-error model, we immediately have Theorem 3.5.

√

9

Theorem 3.5. Consider the normal error model,

Ai = A + σWi ∈ Rd×m,

1 ≤ i ≤ n,

where Wi ∈ Rd×m is a Gaussian random matrix with n ≥ d. The optimization landscape
of (BM) is benign if

σ (cid:46) p − 2d
p + d

√

κ2d1/2(

σmin(A)
√
d +

m + 2

√

log n)

(3.8)

where p ≥ 2d + 1.

Here are a few remarks: if each sample in A is drawn uniformly from [−1, 1]d and p = 3d,
then (3.8) is very similar to the condition in Theorem 3.3 but losing a factor of
d compared
with Theorem 3.1. The additional assumption n ≥ d is quite mild especially in computer vision
and imaging science where d = 3 and the number of cloud points n is much larger than d.

√

The proofs of Theorem 3.4 and 3.5 are given in Section 4.2. Our result regarding the Burer-
Monteiro approach generalizes the author’s earlier work [33] in the sense that [33] considers
a special case Π = Id. The main proof idea is quite similar and borrows several supporting
lemmas from [33].

3.2 Numerics

In this section, we provide numerical experiments to complement our theoretical analysis. The
goal is to identify the regime of σ under which the tightness of (SDP) holds in solving (P) under
certain statistical models. We will run the generalized power method with random initialization
S0. Once the iterates {St} stabilize, we verify its global optimality via convex analysis. More
precisely, the algorithm stops if

(cid:107)St+1(St+1)(cid:62) − St(St)(cid:62)(cid:107)F ≤ 10−6

or reaches the maximum number of iterations. From convex analysis, it is well known that SS(cid:62)
is the unique global maximizer to the SDP relaxation if

(cid:107)(Λ − C)S(cid:107) < 10−6,

λd+1(Λ − C) > 0

where Λ is a symmetric block-diagonal matrix whose ith block equals Λii = (cid:80)n
and λd+1(Λ − C) denotes the (d + 1)th smallest eigenvalue of Λ − C.

j=1 CijSjS(cid:62)
i

The ﬁrst experiment assumes that the data aj ∼ Unif[−1, 1]d, 1 ≤ j ≤ m, and there are in

total n point clouds. We consider the following model

Ai = OiA + σWi, 1 ≤ i ≤ n,

where Wi ∈ Rd×m is a Gaussian random matrix. Due to the rotation invariance of normal noise,
we simply assume Oi = Id, 1 ≤ i ≤ n. The population covariance matrix of aj ∈ Unif[−1, 1]d
is Id/3. For each diﬀerent set of parameter n, m, and σ, we run 20 experiments and count
how many instances have the tightness of the SDP (i.e., the global convergence of the GPM).
Figure 1 implies that if

σ < κ

,

κ ≤ 0.3,

(cid:114) m
d

then the global convergence of the GPM to the optimal SDP solution holds empirically. The
factor (cid:112)m/d comes from the comparison between the signal and noise strength: the signal
nm for m > d in (cid:107) · (cid:107) while the noise part σ∆ is
strength of ZA is approximately of order
√
roughly σ(cid:107)∆(cid:107) (cid:46) σ(
m) where ∆ is an nd × m Gaussian random matrix consisting of
√
n, (cid:112)m/d}. In the setup of
i=1. Thus we have a high SNR σ(cid:107)∆(cid:107) ≤ (cid:107)ZA(cid:107) if σ (cid:46) min{
{Wi}n
√
our experiment, the parameter (n, m, d) is in the regime (cid:112)m/d (cid:28)
n. This may explain why
the phase transition occurs at σ = O((cid:112)m/d) in this experiment.

nd +

√

√

10

Figure 1: Uniform point clouds with normal noise. Left ﬁgure: m = 25 is ﬁxed and n varies
from 100 to 1000; right ﬁgure: n = 250 is ﬁxed and m is between 25 and 500.

The second experiment assumes the data are sampled from normal distributions, i.e., each
aj ∼ N (0, Id), 1 ≤ j ≤ m. We let κ range from 0.05 to 0.5 and the phase transition occurs
approximately at κ ≈ 0.25. Despite there is some tiny diﬀerence on the speciﬁc choice of κ
due to the diﬀerent covariance matrix from the uniform case, we can see that the scaling for
the noise level is still approximately of order (cid:112)m/d to ensure the global convergence of the
GPM for (cid:112)m/d (cid:46)
n. The numerical experiments imply that the bounds in Theorem 3.1
and 3.2 are suboptimal. This is mainly due to the fact that our theoretical analysis do not
take full advantage of the statistical noise. We believe incorporating the noise distribution into
the analysis [60, 31] would yield a near-optimal bound for the tightness of the SDP and the
convergence of the GPM. We leave those for the future work.

√

Figure 2: Gaussian point clouds with Gaussian noise. Left ﬁgure: m = 25 is ﬁxed and n varies
from 100 to 1000; right ﬁgure: n = 250 is ﬁxed and m is between 25 and 500.

4 Proofs

This section is organized as follows: Section 4.1 provides the tightness of the SDP (Theorem 3.1
and 3.2) and the convergence of the GPM (Theorem 3.3), and Section 4.2 is devoted to the
landscape analysis of the Burer-Monteiro factorization (Theorem 3.4).

Before we proceed to the main proof, we ﬁrst introduce a few notations. Note that the value
of (P) and (BM) remains the same if we multiply S ∈ St(d, p)⊗n by another orthogonal matrix
from the right side1. We remove this ambiguity by using the following distance for two elements

1If p = d, St(d, p) = O(d).

11

1502503504505506507508509500.050.10.150.20.250.30.350.40.450.500.10.20.30.40.50.60.70.80.91501001502002503003504004505000.050.10.150.20.250.30.350.40.450.500.10.20.30.40.50.60.70.80.911502503504505506507508509500.050.10.150.20.250.30.350.40.450.500.10.20.30.40.50.60.70.80.91501001502002503003504004505000.050.10.150.20.250.30.350.40.450.500.10.20.30.40.50.60.70.80.91X and Y ∈ Rnd×p by

dF (X, Y ) := min

Q∈O(p)

(cid:107)X − Y Q(cid:107)F

(4.1)

where the minimizer Q = P(Y (cid:62)X) is given in (2.3). One can verify (4.1) is indeed a distance
and satisﬁes triangle inequality. The distance dF (·, ·) can be computed exactly by using

F (X, Y ) = (cid:107)X(cid:107)2
d2

F + (cid:107)Y (cid:107)2

F − 2(cid:107)X (cid:62)Y (cid:107)∗

(4.2)

where (cid:107) · (cid:107)∗ denotes the matrix nuclear norm, i.e., the sum of singular values. In particular, if
X and Y are both in St(d, p)⊗n, then

F (X, Y ) = 2nd − 2(cid:107)X (cid:62)Y (cid:107)∗
d2

(4.3)

To measure how close a given element S ∈ St(d, p)⊗n is to the fully “synchronized” ground
truth state, i.e., ZQ ∈ St(d, p)⊗n for some Q ∈ St(d, p), we deﬁne an (cid:15)-neighborhood around
ZQ in St(d, p)⊗n ⊂ Rnd×p:

(cid:40)

N(cid:15) :=

S ∈ St(d, p)⊗n : min

Q∈St(d,p)

(cid:107)S − ZQ(cid:107)F = min

Q∈St(d,p)

n
(cid:88)

i=1

(cid:113)

(cid:107)Si − Q(cid:107)2

F ≤ (cid:15)

√

(cid:41)

nd

.

(4.4)

In the later discussion, we set p = d in the convergence analysis of the generalized power
method and the tightness of the SDP, and p > d for the landscape analysis of the Burer-Monteiro
factorization (BM).

Another important ingredient of our proof is the optimality condition of (SDP). This
result provides a simple suﬃcient condition to certify whether a candidate solution is a global
maximizer to (P) and (SDP).

Theorem 4.1. The matrix X = SS(cid:62) with S ∈ St(d, p)⊗n is a globally optimal solution
to (SDP) if there exists a block-diagonal matrix Λ = blkdiag(Λ11, · · · , Λnn) ∈ Rnd×nd with
Λii ∈ Rd×d such that

CS = ΛS, Λ − C (cid:23) 0.

(4.5)

Moreover, if Λ − C is of rank (n − 1)d, then X is the unique global maximizer, and S is rank-d
and equals the global maximizer to (P).

This optimality condition can be found in several places including [9, Theorem 3.4], [33,
Proposition 5.1] and [44, Theorem 7]. The derivation of Theorem 4.1 follows from the standard
routine of duality theory in convex optimization.

4.1 Convergence of the GPM and the tightness of the SDP

The main idea of the proof follows from [60, 31] by showing the iterates from the GPM with
a suﬃciently good initialization actually converge to the SDP estimator linearly. Unlike the
scenarios in [60, 31], the GPM here is in fact a local contraction mapping in N(cid:15). This technical
diﬀerence signiﬁcantly simpliﬁes the proof.

Deﬁne

L :=

C
n(cid:107)A(cid:107)2 =

1
n(cid:107)A(cid:107)2

(cid:16)

(cid:17)
ZAA(cid:62)Z(cid:62) + (cid:101)∆

(4.6)

where C, Z, and (cid:101)∆ are deﬁned in (2.1), (3.1), and (3.2). Under the assumptions of Theorem 3.1,
(cid:101)∆ satisﬁes

(cid:107) (cid:101)∆(cid:107) ≤ 2
≤ 3

√

√

n(cid:107)∆(cid:107)(cid:107)A(cid:107) + (cid:107)∆(cid:107)2
n(cid:107)A(cid:107)(cid:107)∆(cid:107) ≤ 6n(cid:107)A(cid:107) max
1≤i≤n

(cid:107)∆i(cid:107)

(4.7)

√

√

where (cid:107)∆(cid:107) ≤
strength (cid:107) (cid:101)∆(cid:107) is weaker than the signal strength (cid:107)ZAA(cid:62)Z(cid:62)(cid:107) = n(cid:107)A(cid:107)2.

n max1≤i≤n (cid:107)∆i(cid:107) ≤

n(cid:107)A(cid:107). In other words, Theorem 3.1 implies the noise

12

In the analysis of the generalized power method, we set

(cid:15) ≤

1
16κ2

√

,

d

κ =

σmax(A)
σmin(A)

,

p = d

in (4.4). We will see that N(cid:15) is a basin of attraction, i.e., if the initialization is located in N(cid:15),
then the iterates generated from the GPM will stay in N(cid:15) and converge to the global optima.
The main idea of the proof is straightforward: we will ﬁrst prove the following two important

claims.

Claim (a) Pn ◦ L is a contraction mapping on N(cid:15);

Claim (b) Pn ◦ L maps N(cid:15) to itself.

With these two claims, the Banach ﬁxed-point theorem implies that there exists a unique
ﬁxed point S∞ within N(cid:15) and the iterates converge to S∞ linearly. Moreover, we can justify
that this ﬁxed-point S∞ also satisﬁes the optimality condition for the SDP relaxation which
guarantees the tightness of the SDP relaxation. We will justify Claim (a) and (b) in the
Section 4.1.1 and 4.1.2 respectively. Now we formally present the main result on the convergence
of the GPM.

Theorem 4.2 (Convergence of the GPM). If an initialization S0 is located in N(cid:15) and the
assumption (3.4), i.e.,

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

σmin(A)
192κ3

(4.8)

holds, then St converges to a unique ﬁxed point S∞ linearly

dF (St+1, S∞) ≤ ρtdF (S0, S∞),

∀t ≥ 0

where

ρ = 4κ2

(cid:18)

√

2(cid:15)

d +

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

(cid:19)

< 1,

(cid:15) ≤

1
16κ2

√

.

d

Moreover, S∞(S∞)(cid:62) is the unique global maximizer to (SDP).

Theorem 4.2 will be proven in Section 4.1.3. Note that Theorem 3.1 immediately follows from
Theorem 4.2 if we set S0 = Z. Regarding Theorem 3.3, it suﬃces to construct an initialization
in N(cid:15) and then apply Theorem 4.2. The following proposition guarantees that the spectral
initialization in Algorithm 1 will produce an initial guess S0 in N(cid:15).

Proposition 4.3 (Spectral initialization). Under

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:15)σmin(A)
24κ2

,

(cid:15) ≤

1
16κ2

√

,

d

the spectral initialization produces S0 ∈ N(cid:15).

We will discuss the proof of Proposition 4.3 and Theorem 3.2 in Section 4.1.4.

4.1.1 Contraction mapping on N(cid:15)

Proposition 4.4 (Contraction mapping). For X and Y ∈ N(cid:15) ⊆ O(d)⊗n, it holds that

dF (Pn(LX), Pn(LY )) ≤ 4κ2

(cid:18)

√

2(cid:15)

d +

3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

(cid:124)

(cid:123)(cid:122)
ρ

dF (X, Y ).

(cid:19)

(cid:125)

Suppose

then

√
(cid:15)

d ≤

1
16κ2 ,

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:107)A(cid:107)
48κ2 ,

(4.9)

dF (Pn(LX), Pn(LY )) ≤ ρdF (X, Y ),

ρ < 1.

13

The proof of Proposition 4.4 follows from showing that both L and Pn are contraction

mappings.

Lemma 4.5. For any X and Y in N(cid:15) ⊆ O(d)⊗n, we have
(cid:19)

(cid:18)

√

dF (LX, LY ) ≤

2(cid:15)

d +

dF (X, Y ).

3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

Proof: Let Q be the orthogonal matrix which makes dF (X, Y ) = (cid:107)X − Y Q(cid:107)F , i.e., Q =
P(Y (cid:62)X).

(cid:107)L(X − Y Q)(cid:107)F ≤

≤

≤

1

n(cid:107)A(cid:107)2 (cid:107) (cid:101)∆(X − Y Q)(cid:107)F

1

(cid:107)Z(cid:62)(X − Y Q)(cid:107)F +

n(cid:107)A(cid:107)2 (cid:107)ZAA(cid:62)Z(cid:62)(X − Y Q)(cid:107)F +
1
√
n
1
√
n

(cid:107)Z(cid:62)(X − Y Q)(cid:107)F +

n(cid:107)A(cid:107)2 (cid:107) (cid:101)∆(cid:107)(cid:107)X − Y Q(cid:107)F
3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

(cid:107)X − Y Q(cid:107)F

1

where the noise matrix satisﬁes (cid:107) (cid:101)∆(cid:107) ≤ 3

√

n(cid:107)∆(cid:107)(cid:107)A(cid:107) in (4.7). The ﬁrst term is bounded by

(cid:107)Z(cid:62)(X − Y Q)(cid:107)F ≤ (cid:107)(Y − ZQY )(cid:62)(X − Y Q)(cid:107)F + (cid:107)Y (cid:62)(X − Y Q)(cid:107)F

Use QY = P(Z(cid:62)Y ) ≤ (cid:15)

nd · dF (X, Y ) + (cid:107)Y (cid:62)X − nQ(cid:107)F

Use (4.10)

≤ (cid:15)

nd · dF (X, Y ) +
√

nd · dF (X, Y )

≤ 2(cid:15)

1
2

(cid:107)X − Y Q(cid:107)2
F

√

√

where dF (X, Y ) = minQ∈O(d) (cid:107)X − Y Q(cid:107)F ≤ dF (X, Z) + dF (Y , Z) ≤ 2(cid:15)

√

(cid:107)Y (cid:62)X − nQ(cid:107)F ≤

1
2

(cid:107)X − Y Q(cid:107)2
F

nd and

(4.10)

for any X and Y ∈ O(d)⊗n where Q := P(Y (cid:62)X). The inequality (4.10) follows from

(cid:107)Y (cid:62)X − nQ(cid:107)2

F =

d
(cid:88)

(n − σi(Y (cid:62)X))2

(σi(Y (cid:62)X) ≤ n)

≤

i=1
(cid:32) d

(cid:88)

i=1

(cid:33)2

(n − σi(Y (cid:62)X))

Use (4.3)

(cid:16)

=

nd − (cid:107)Y (cid:62)X(cid:107)∗

(cid:17)2

=

(cid:18) 1
2

(cid:107)X − Y Q(cid:107)2
F

(cid:19)2

.

Thus

dF (LX, LY ) ≤ (cid:107)L(X − Y Q)(cid:107)F ≤

(cid:18)

√

2(cid:15)

d +

(cid:19)

3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

dF (X, Y ).

Lemma 4.6. For two invertible matrices X and Y in Rd×d,

and

(cid:107)P(X) − P(Y )(cid:107) ≤

2(cid:107)X − Y (cid:107)
σmin(X) + σmin(Y )

(cid:107)P(X) − P(Y )(cid:107)F ≤

4(cid:107)X − Y (cid:107)F
σmin(X) + σmin(Y )

.

The proof uses Davis-Kahan theorem for eigenvector perturbation [20]. This perturbation
bound can be found in [31, Lemma 4.7] and [30, Theorem 1]. It seems that to prove Pn ◦ L is a
contraction, we only need to combine Lemma 4.5 with Lemma 4.6. The only missing ingredient
is: whether each block of [LS]i has a strictly positive smallest singular value for 1 ≤ i ≤ n so
that it is invertible and we can use Lemma 4.6.

14

Lemma 4.7. Suppose S ∈ N(cid:15), then

σmin([LS]i) ≥

1
κ2

(cid:18)

1 −

(cid:19)

(cid:15)2d
2

−

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

, 1 ≤ i ≤ n,

and it holds

σmin([LS]i) ≥

1
2κ2

if

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:107)A(cid:107)
12κ2 ,

√
(cid:15)

d ≤

1
16κ2 .

Proof: Note that (4.10) and (4.3) imply

σmin(Z(cid:62)S) ≥ (cid:0)1 − (cid:15)2d/2(cid:1) n

for any S ∈ N(cid:15) since (cid:107)Z(cid:62)S − nP(Z(cid:62)S)(cid:107) ≤ (cid:15)2nd/2. For each block, it holds that

(4.11)

(4.12)

[LS]i =

(cid:16)

1
n(cid:107)A(cid:107)2

AA(cid:62)Z(cid:62)S + (cid:101)∆(cid:62)

i S

(cid:17)

where (cid:101)∆i = ∆A(cid:62) + ZA∆(cid:62)

i + ∆∆(cid:62)
i

is the ith block column of (cid:101)∆.

We ﬁrst provide an upper bound of (cid:107) (cid:101)∆(cid:62)

i S(cid:107):

(cid:107) (cid:101)∆(cid:62)

i S(cid:107) ≤ (cid:107)∆iA(cid:62)Z(cid:62)S + (A + ∆i)∆(cid:62)S(cid:107)

(Use (cid:107)∆i(cid:107) ≤ (cid:107)A(cid:107))

≤ (cid:107)∆i(cid:107)(cid:107)A(cid:107)(cid:107)Z(cid:62)S(cid:107) + ((cid:107)A(cid:107) + (cid:107)∆i(cid:107))(cid:107)∆(cid:62)S(cid:107)
2(cid:107)∆(cid:107)
√
n

(cid:107)∆i(cid:107) +

≤ n(cid:107)A(cid:107)

(cid:19)

(cid:18)

≤ 3n(cid:107)A(cid:107) max
1≤i≤n

(cid:107)∆i(cid:107)

(4.13)

where

(cid:107)Z(cid:62)S(cid:107) ≤ n,

(cid:107)∆(cid:62)S(cid:107) ≤

√

n(cid:107)∆(cid:107).

For the lower bound of σmin([LS]i), it holds that

σmin([LS]i) ≥

≥

≥

(cid:18)

(cid:19)

1
nκ2 σmin(Z(cid:62)S) −
(cid:15)2d
1
κ2
2
(cid:15)2d
1
κ2
2

1 −

1 −

−

−

(cid:19)

(cid:18)

1

n(cid:107)A(cid:107)2 (cid:107) (cid:101)∆(cid:62)
1

i S(cid:107)

n(cid:107)A(cid:107)2 · 3n(cid:107)A(cid:107) max
3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

1≤i≤n

.

(cid:107)∆i(cid:107)

The proof of Proposition 4.4 follows from combing all the results above.

Proof of Proposition 4.4. For any X and Y in N(cid:15), Lemma 4.5 implies σmin([LX]i) and
σmin([LY ]i) are greater than 1/2κ2 in (4.11) under the assumption in Theorem 3.1. Let Q be
the orthogonal matrix which minimizes (cid:107)LX − LY Q(cid:107)F . We have

dF (Pn(LX), Pn(LY )) ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

(cid:107)P([LX]i) − P([LY ]i)Q(cid:107)2
F

(Lemma 4.6)

≤ 4

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:107)[LX]i − [LY ]iQ(cid:107)2
F
(σmin([LX]i) + σmin([LY ]i))2

(Lemma 4.5)

≤ 4κ2dF (LX, LY )
√

(cid:18)

≤ 4κ2

2(cid:15)

d +

3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

(cid:19)

dF (X, Y ).

15

4.1.2 Pn ◦ L maps N(cid:15) to itself

This section is devoted to proving that Pn(LS) stays in N(cid:15) for any S ∈ N(cid:15).

Proposition 4.8. Under

√
(cid:15)

d ≤

1
16κ2 ,

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

√
(cid:15)

d(cid:107)A(cid:107)
12κ2

(4.14)

then we have Pn(LS) ∈ N(cid:15) for any S ∈ N(cid:15).

Proof: Suppose S ∈ N(cid:15), then we want to show that Pn(LS) ∈ N(cid:15). For any S ∈ N(cid:15), we have

σmin([LS]i) ≥

1
2κ2 ,

1

(cid:107)A(cid:107)2 σmin

(cid:16)

AA(cid:62)(cid:17)

≥

1
κ2

(4.15)

following from Lemma 4.7 whose assumptions hold under (4.14).
Let Q = P(Z(cid:62)S) be the minimizer to minQ∈O(d) (cid:107)S − ZQ(cid:107)F .

dF (Pn(LS), Z) = dF (Pn(LS), Pn((cid:107)A(cid:107)−2ZAA(cid:62)))

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:107)P([LS]i) − P((cid:107)A(cid:107)−2AA(cid:62))Q(cid:107)2
F

(cid:118)
(cid:117)
(cid:117)
(cid:116)

·

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1

4
2κ2 + 1
κ2
(cid:13)
8κ2
(cid:13)
(cid:13)
3
(cid:13)

LS −

1

i=1

[LS]i −

(cid:107)A(cid:107)2 AA(cid:62)Q
(cid:13)
1
(cid:13)
(cid:107)A(cid:107)2 ZAA(cid:62)Q
(cid:13)
(cid:13)F

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

≤

≤

≤

which follows from Lemma 4.6 and (4.15).

Note that

(4.16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

1
(cid:107)A(cid:107)2 AA(cid:62)Q
n(cid:107)A(cid:107)2 (cid:107) (cid:101)∆(cid:62)

1

i S(cid:107)F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

[LS]i −

1

n(cid:107)A(cid:107)2 AA(cid:62)Q

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

Use (4.13)

Use (4.10)

Use S ∈ N(cid:15)

=

≤

≤

≤

≤

≤

1

1

i S) −

(cid:13)
(cid:13)
n(cid:107)A(cid:107)2 (AA(cid:62)Z(cid:62)S + (cid:101)∆(cid:62)
(cid:13)
(cid:13)
1
n(cid:107)A(cid:107)2 (cid:107)AA(cid:62)(Z(cid:62)S − nQ)(cid:107)F +
1
(cid:107)Z(cid:62)S − nQ(cid:107)F +
n
1
2n
(cid:15)2d
2
√
d
(cid:15)
32κ2 +

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

√
d
(cid:15)
4κ2 =

(cid:107)S − ZQ(cid:107)2

d
9(cid:15)
32κ2

F +

√

+

n(cid:107)A(cid:107)2 · 3n(cid:107)A(cid:107) max
3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

1≤i≤n

(cid:107)∆i(cid:107)

provided that (cid:15)

√

d < 1

√

16κ2 and max1≤i≤n (cid:107)∆i(cid:107) ≤ (cid:15)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

8κ2
3

LS −

1

·

(cid:13)
(cid:13)
n(cid:107)A(cid:107)2 ZAA(cid:62)Q
(cid:13)
(cid:13)F

d(cid:107)A(cid:107)
12κ2

dF (Pn(LS), Z) ≤

. Combining it with (4.16), we have

≤

8κ2
3

·

√
√
9(cid:15)
nd
32κ2 < (cid:15)

nd

which implies Pn(LS) ∈ N(cid:15).

4.1.3 Proof of Theorem 4.2: Convergence analysis of the GPM

In this section, we show that the ﬁxed point is a global maximizer to the SDP relaxation.

16

Theorem 4.9. Banach contraction principle, [27, Theorem 1.1] Let (X , d) be a complete
metric space and T : X → X be a contraction map, i.e.,

d(T (x), T (y)) ≤ ρd(x, y),

∀x, y ∈ X

for an absolute positive constant ρ < 1. Then there is a unique ﬁxed point u and T n(x) converges
to u as n → ∞ for each x ∈ X with

d(T n(x), u) ≤ ρnd(x, u).

Proof of Theorem 4.2. We divide the proof into three steps.

Step 1: Existence of the ﬁxed point and linear convergence. Let X = N(cid:15) =
{X ∈ O(d)⊗n : dF (X, Z) ≤ (cid:15)
nd} and we equip it with the distance function dF (·, ·) in (4.1).
Apparently, this is a complete and compact metric space since N(cid:15) is a bounded and closed
subset in Rnd×d. Here we let the mapping T :

√

T (X) = Pn(LX).

√

Under (4.8), we set (cid:15) = 1/(16κ2
d); then (4.14) in Proposition 4.8 and (4.9) in Proposition 4.4
hold immediately. Proposition 4.8 ensures that T maps N(cid:15) to N(cid:15) and Proposition 4.4 implies
that T : N(cid:15) → N(cid:15) is a contraction mapping. As a result, Theorem 4.9 guarantees that the
generalized power method with an initial value S0 ∈ N(cid:15) will ﬁnally converge to a unique ﬁxed
point satisfying

Pn(LS∞) = Pn(CS∞) = S∞Q, S∞ ∈ N(cid:15)

for some Q ∈ O(d) where

dF (Pn(CS∞), S∞) = (cid:107)(Pn(CS∞) − S∞Q(cid:107)F = 0.

Moreover, the convergence rate satisﬁes

dF (St, S∞) ≤ ρtdF (S0, S∞)

where Proposition 4.4 gives

ρ = 4κ2

(cid:18)

√

2(cid:15)

d +

3(cid:107)∆(cid:107)
√
n(cid:107)A(cid:107)

(cid:19)

≤ 4κ2

(cid:18)

√

2(cid:15)

d +

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

(cid:19)

< 1.

Step 2: Proof of Pn(LS∞) = S∞. Next, we will show that Q is in fact equal to Id. Note

that Lemma 4.7 implies σmin([LS∞]i) > 0 for S∞ ∈ N(cid:15). For each block of LS∞, we get

P([LS∞]i) = P([CS∞]i) = ([CS∞]i[CS∞](cid:62)

i )−1/2[CS∞]i = S∞

i Q, 1 ≤ i ≤ n,

where

σmin([CS∞]i) = n(cid:107)A(cid:107)2σmin([LS∞]i) > 0, P([LS∞]i) = P([CS∞]i)

follow from (4.6). As a result, we have

[CS∞]i = ([CS∞]i[CS∞](cid:62)

i )1/2S∞

i Q, 1 ≤ i ≤ n.

Now multiplying both sides of (4.17) by (S∞

i )(cid:62) and summing over 1 ≤ i ≤ n give

n
(cid:88)

i=1

[S∞
i

](cid:62)[CS∞]i = (S∞)(cid:62)CS∞ = (S∞)(cid:62)ZAA(cid:62)Z(cid:62)S∞ + (S∞)(cid:62) (cid:101)∆S∞

n
(cid:88)

(S∞

i )(cid:62)([CS∞]i[CS∞](cid:62)

i )1/2S∞

i Q

=

where C is deﬁned in (2.4).

i=1

17

(4.17)

(4.18)

The smallest eigenvalue of (S∞)(cid:62)CS∞ is lower bounded by

λmin((S∞)(cid:62)CS∞) ≥ σ2
≥ σ2

Use (4.7)

min(A) · (S∞)(cid:62)ZZ(cid:62)S∞ − n(cid:107) (cid:101)∆(cid:107)
min(Z(cid:62)S∞) − 3n2 max
min(A) · σ2
1≤i≤n

(cid:107)∆i(cid:107)(cid:107)A(cid:107)

Use (4.12)

≥ σ2

min(A)

(cid:18)

1 −

(cid:19)2

(cid:15)2d
2

n2 − 3n2 max
1≤i≤n

(cid:107)∆i(cid:107)(cid:107)A(cid:107) > 0

provided that

max
1≤i≤n

(cid:107)∆i(cid:107) <

(cid:107)A(cid:107)
3κ2

(cid:18)

1 −

(cid:19)2

(cid:15)2d
2

,

(cid:15) ≤

1
16κ2

√

.

d

Note (S∞)(cid:62)CS∞ is positive deﬁnite and so is (cid:80)n

in (4.18).
Therefore, Q must be Id according to (4.18) since the only positive semideﬁnite orthogonal
matrix is Id. Now we can see S∞ is the ﬁxed point to the generalized power iteration which
satisﬁes Pn(CS∞) = S∞.

i )(cid:62)([CS∞]i[CS∞](cid:62)

i )1/2S∞
i

i=1(S∞

Step 3: Global optimality of S∞(S∞)(cid:62) in (SDP). Let Λ be a block-diagonal matrix

whose ith block Λii equals ([CS∞]i[CS∞](cid:62)

i )1/2. Since S∞ is a ﬁxed point, it holds

CS∞ = ΛS∞

following from (4.17). It suﬃces to show λd+1(Λ − C) > 0, i.e., the (d + 1)th smallest eigenvalue
of Λ − C is strictly positive, to guarantee that S∞(S∞)(cid:62) is the unique global maximizer
to (SDP) since Λ − C has d zero eigenvalues whose corresponding eigenspace is spanned by the
columns of S∞.

Let u ∈ Rnd whose ith block is ui be any unit vector perpendicular to all columns of S∞

and Q∞ := argminQ∈O(d) (cid:107)S∞ − ZQ(cid:107)F . Now we estimate λd+1(Λ − C) by

u(cid:62)(Λ − C)u =

(u(cid:62)S∞ = 0)

(Lemma 4.7 and (4.6))

≥

≥

n
(cid:88)

i=1
n
(cid:88)

i ([CS∞]i[CS∞](cid:62)
u(cid:62)

i )1/2ui − u(cid:62)(ZAA(cid:62)Z(cid:62) + (cid:101)∆)u

σmin([CS∞]i)(cid:107)ui(cid:107)2 − (cid:107)A(cid:62)Q∞(S∞ − ZQ∞)(cid:62)u(cid:107)2 − (cid:107) (cid:101)∆(cid:107)

i=1
(cid:18) 1
κ2

(cid:18)

1 −

(cid:19)

(cid:15)2d
2

−

3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

(cid:19)

· n(cid:107)A(cid:107)2

(cid:16)

−

(cid:107)A(cid:107)2(cid:107)S∞ − ZQ∞(cid:107)2

(cid:17)
F + (cid:107) (cid:101)∆(cid:107)

(S∞ ∈ N(cid:15))

(cid:18)

≥

1 −

(cid:18)

≥

1 −

(cid:19)

(cid:15)2d
2
3κ2(cid:15)2d
2

nσ2

min(A) − (cid:15)2nd(cid:107)A(cid:107)2 − 6n(cid:107)A(cid:107) max
1≤i≤n
(cid:19)

nσ2

min(A) − 6n(cid:107)A(cid:107) max
1≤i≤n

(cid:107)∆i(cid:107).

(cid:107)∆i(cid:107)

Under (4.8) in Theorem 3.1, we have

u(cid:62)(Λ − C)u ≥

(cid:18)
√
(cid:15)

d ≤

(cid:19)

1
16κ2

1 −

(cid:18)

(cid:18)

≥

1 −

(cid:19)

3κ2(cid:15)2d
2
3
512κ2 −

nσ2

min(A) −

nσ2

min(A)
32κ2 > 0

(cid:19)

1
32κ2

nσ2

min(A) > 0 =⇒ λd+1(Λ − C) > 0.

Finally, we can see S∞(S∞)(cid:62) is the unique global maximizer to the SDP relaxation, following
directly from Theorem 4.1.

18

4.1.4 Proof of Proposition 4.3 (Initialization) and Theorem 3.2

Note that the key to the success of the generalized power method is to ensure that the initial
value S0 is located in the basin of attraction N(cid:15) deﬁned in (4.4) with (cid:15) ≤ 1/(16κ2
d). For the
tightness of the SDP, we can simply choose the ground truth rotation matrix Z ∈ O(d)⊗n as an
initial guess and then apply the Banach contraction mapping theorem to show the existence of
a ﬁxed point for the power method. In practice, we need to ﬁnd an algorithm to produce such
an initialization. The strategy is quite simple: we compute the top d left singular vectors U of
D = ZA + ∆ and perform a rounding procedure for each block of the left singular vectors to
get the initial value S0.

√

The proof relies on the Davis-Kahan theorem for singular vector perturbation [20, 57].

Theorem 4.10 (Davis-Kahan theorem). Let B and (cid:101)B be two matrices of size RN ×M with
min{N, M } ≥ d. Suppose

where σd(·) denotes the dth largest singular value, then

σd(B) ≥ α + δ,

σd+1( (cid:101)B) ≤ α,

1
2

min
R∈O(d)

(cid:107)V

(cid:101)B − VBR(cid:107) ≤

(cid:13)
(cid:13)(IN − VBV (cid:62)
(cid:13)

B )V

(cid:101)B

(cid:13)
(cid:13)
(cid:13) ≤

(cid:107) (cid:101)B − B(cid:107)
δ

where VB and V
A similar bound also holds for the right singular vectors.

(cid:101)B ∈ RN ×d are the top d normalized left singular vectors of B and (cid:101)B respectively.

Now we present the proof of Proposition 4.3.

Proof of Proposition 4.3. Consider the data matrix D := ZA + ∆ ∈ Rnd×m. From now on,
we assume

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

(cid:15)σmin(A)
24κ2

,

(cid:15) =

1
16κ2

√

.

d

(4.19)

Denote (Σ, U , V ) as the top d singular values and left/right singular vectors of D where Σ ∈
Rd×d is a diagonal matrix, U ∈ Rnd×d with U (cid:62)U = Id, and V ∈ Rm×d with V (cid:62)V = Id. It
simply holds

U = DV Σ−1.

Let A = U0Σ0V (cid:62)
vectors of A. We will approximate U by using “one-step power method”:

0 ∈ Rd×m be the SVD of A and V0 ∈ Rm×d consists of top d right singular

U − DV0RΣ−1 = D(V − V0R)Σ−1

where R is an orthogonal matrix to be decided. Next, we will show that each block Ui of U
is well approximated by that of DV0RΣ−1. We start with estimating (cid:107)V − V0R(cid:107) using the
Davis-Kahan theorem.

Using Weyl’s inequality, we have

(cid:107)D − ZA(cid:107) ≤ (cid:107)∆(cid:107) =⇒ |σi(D) −

nσi(A)| ≤ (cid:107)∆(cid:107), 1 ≤ i ≤ d

√

⇐⇒ (

nσmin(A) − (cid:107)∆(cid:107))Id (cid:22) Σ (cid:22) (

n(cid:107)A(cid:107) + (cid:107)∆(cid:107))Id

√

√

where Σ = diag(σ1(D), · · · , σd(D)) (cid:31) 0.

Applying the Davis-Kahan theorem to D and ZA implies that

min
R∈O(d)

(cid:107)V − V0R(cid:107) ≤

√

2(cid:107)∆(cid:107)
nσmin(A) − (cid:107)∆(cid:107)

.

19

(4.20)

(4.21)

As a result, the ith block Ui of U is approximated by [DV0RΣ−1]i with approximation error
controlled by

(cid:107)Ui − [DV0RΣ−1]i(cid:107) ≤ (cid:107)(A + ∆i)(V − V0R)Σ−1(cid:107)

Use (4.20) and (4.21)

≤ 2(cid:107)A(cid:107) ·

√

2(cid:107)∆(cid:107)
nσmin(A) − (cid:107)∆(cid:107)
n(cid:107)A(cid:107) max1≤i≤n (cid:107)∆i(cid:107)
nσ2
min(A)
max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

·

√

4.5

4.5κ2
√
n

≤

=

·

√

1
nσmin(A) − (cid:107)∆(cid:107)

where (cid:107)∆(cid:107) ≤ 0.05

√

nσmin(A) follows from (4.19). Therefore,

(cid:107)Ui − AV0RΣ−1(cid:107) ≤ (cid:107)Ui − (A + ∆i)V0RΣ−1(cid:107) + (cid:107)∆iV0RΣ−1(cid:107)

≤

=

4.5κ2
√
n
6κ2
√
n

·

·

max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

.

+ max
1≤i≤n

(cid:107)∆i(cid:107) ·

√

1.5
nσmin(A)

Now applying Lemma 4.6 gives:

(cid:107)P(Ui) − P(AV0RΣ−1)(cid:107) ≤

2
σmin(AV0RΣ−1)
6κ2
√
n

n ·

√

·

≤ 4κ

· (cid:107)Ui − AV0RΣ−1(cid:107)

max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

=

24κ3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)

where

σmin(AV0RΣ−1) ≥ σmin(U0Σ0RΣ−1)
≥ σmin(Σ0)σmin(Σ−1)

Use (4.20)

≥ σmin(A) ·

1
n(cid:107)A(cid:107)

√

2

=

1
√
2κ

.

n

To ensure dF (S0, Z) ≤ (cid:15)

√

nd, it suﬃces to have

(cid:107)P(Ui) − P(AV0RΣ−1)(cid:107) ≤

=

24κ3 max1≤i≤n (cid:107)∆i(cid:107)
(cid:107)A(cid:107)
24κ2 max1≤i≤n (cid:107)∆i(cid:107)
σmin(A)

≤ (cid:15),

(cid:15) ≤

1
16κ2

√

d

which is guaranteed by (4.19). Under this assumption, we have

dF (S0, Z) ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:107)P(Ui) − P(AV0RΣ−1)(cid:107)2
F

(cid:118)
(cid:117)
(cid:117)
(cid:116)d

≤

n
(cid:88)

i=1

(cid:107)P(Ui) − P(AV0RΣ−1)(cid:107)2 ≤ (cid:15)

√

nd

where P(AV0RΣ−1) ∈ Rd×d is an orthogonal matrix.

Proof of Theorem 3.2. Suppose ai ∼Unif[−1, 1]d. Then its sample covariance matrix

(cid:98)Σm =

1
m

AA(cid:62) =

1
m

m
(cid:88)

i=1

aia(cid:62)
i

20

converges to Id in operator norm. In particular, Theorem 6.5 in [53] implies that

(cid:107) (cid:98)Σm − Id(cid:107) ≤ c1

(cid:32)(cid:114)

(cid:33)

+ δ

d
m

+

d
m

with probability at least 1 − c2 exp(−c3m min{δ, δ2}) where cj, 1 ≤ j ≤ 3 are absolute con-
stants. Here we may set δ = (cid:112)(d + log m)/m and m/ log m (cid:38) d. Then the data matrix
A = [a1, · · · , am] ∈ Rd×m has a conditional number of order κ = O(1) and σmin(A) = O(
m)
with high probability.

√

For each noise matrix ∆i = σWi, we know that

√

(cid:107)Wi(cid:107) (cid:46) C(

√

d +

m + (cid:112)2 log n), 1 ≤ i ≤ n,

holds with probability at least 1 − 2n−1 which follows from [52, Theorem 4.4.5]. As a result, we
have the tightness of the SDP relaxation if

σ (cid:46)

√

d +

√

m
m +

√

√

2 log n

which follows from Theorem 3.1.

4.2 Proof of Theorem 3.4: Optimization landscape analysis

The landscape analysis of the Burer-Monteiro factorization follows from a modiﬁcation of the
author’s previous work [33] which focuses on the orthogonal group synchronization. We start
with citing a few key lemmas regarding the calculation of the ﬁrst and second order necessary
condition derived from [33]. The strategy is to ﬁrst use the necessary condition for local opti-
mality to show that all the second order critical points (SOCPs) are close to the ground truth
orthogonal matrices; then we apply Theorem 4.1 to certify that under the assumption of Theo-
rem 3.4, it exists only one SOCP of rank d and it is equal to the global maximizer of (SDP). The
major technical diﬀerence from [33] is on establishing the proximity condition for the second
order critical points in Proposition 4.12, which requires a diﬀerent rounding procedure to justify
that every SOCP is well aligned with the ground truth.

Lemma 4.11 ([33, Lemma 5.2 and 5.9]). For the objective function deﬁned in (BM), S ∈
St(d, p)⊗n is an SOCP if it satisﬁes the ﬁrst order necessary condition

n
(cid:88)

j=1

CijSj −

1
2

n
(cid:88)

j=1

and the second-order condition

(SiS(cid:62)

j Cji + CijSjS(cid:62)

i )Si = 0

n
(cid:88)

(cid:104)Λii, ˙Si ˙S(cid:62)

i (cid:105) ≥

n
(cid:88)

n
(cid:88)

(cid:104)Cij, ˙Si ˙S(cid:62)

j (cid:105), Λii (cid:23) 0

(4.22)

(4.23)

i=1

i=1

j=1

(cid:80)n

where Λii = 1
2
tangent plane ˙SiS(cid:62)

j=1(CijSjS(cid:62)
i + Si ˙S(cid:62)

i = 0.

i + SiS(cid:62)

j Cji) is symmetric and ˙Si is an arbitrary element on the

The proof of Lemma 4.11 can be found directly in [33, Lemma 5.2 and Lemma 5.9], which
is quite straightforward by using the calculus on Stiefel manifold [2, 22]. The next proposition
is crucial in analyzing the optimization landscape of (BM). It essentially says that any SOCP
is quite close to the ground truth signal and also their distance is controlled by the operator
norm of the rescaled noise matrix (cid:101)∆Π in (3.7) and its partial trace.

21

Proposition 4.12. For any second order critical point S ∈ St(d, p)⊗n and p ≥ 2d + 1, it holds
that

dF (S, Z) := min
√
√

(2 +

=

Q∈St(d,p)

(cid:107)S − ZQ(cid:107)F ≤ δ
√

d(p + d)

5)
n(p − 2d)

(cid:107) Trd( (cid:101)∆Π)(cid:107) ∨ (cid:107) (cid:101)∆Π(cid:107)

(cid:114)

d
n

(cid:107) (cid:101)∆Π(cid:107)

where (cid:101)∆Π is deﬁned in (3.7),

δ =

√

(2 +

5)(p + d)γ

p − 2d

, γ =

(cid:107) Trd( (cid:101)∆Π)(cid:107)
(cid:107) (cid:101)∆Π(cid:107)

∨ 1

and Trd( (cid:101)∆Π) := [Tr(Π−1 (cid:101)∆ij)]1≤i,j≤n ∈ Rn×n is the partial trace of (cid:101)∆Π.

The proof of Proposition 4.12 relies on the following result in [33]. In fact, with the following
proposition, it suﬃces to show that any SOCP in (BM) satisfy (4.24) and (4.25) for ¯∆ = (cid:101)∆Π,
and then invoking Proposition 4.13 immediately gives Proposition 4.12.
Proposition 4.13. [33, Proposition 5.7] Suppose S ∈ St(d, p)⊗n satisﬁes

(p − d)(cid:107)Z(cid:62)S(cid:107)2

F ≥ (p − 2d)n2d + (cid:107)SS(cid:62)(cid:107)2
F d
n
(cid:88)

n
(cid:88)

+ (p − d)(cid:104) ¯∆, ZZ(cid:62) − SS(cid:62)(cid:105) +

((cid:107)SiS(cid:62)

j (cid:107)2

F − d) Tr( ¯∆ij)

and

for any matrix ¯∆, then

i=1

j=1

(cid:107)Z(cid:62)S(cid:107)2

F − (cid:107)SS(cid:62)(cid:107)2

F ≤

1
n

(cid:107) ¯∆S(cid:107)2
F

dF (S, Z) ≤ δ

(cid:114)

d
n

(cid:107) ¯∆(cid:107) =

√

(2 +

5)(p + d)

p − 2d

(cid:114)

d
n

Trd( ¯∆)(cid:107) ∨ (cid:107) ¯∆(cid:107)

where

√

(2 +

5)(p + d)γ

p − 2d

δ =

, γ =

(cid:107) Trd( ¯∆)(cid:107)
(cid:107) ¯∆(cid:107)

∨ 1

and Trd( ¯∆) = [Tr( ¯∆ij)]1≤i,j≤n is the partial trace of ¯∆.

(4.24)

(4.25)

(4.26)

To make the argument more self-contained, we brieﬂy discuss the proof idea of Proposi-

tion 4.13 below.

Proof of Proposition 4.12. Step 1: Proof of (4.24). We ﬁrst use the second order nec-
essary condition in Lemma 4.11 to show that all local maximizers are aligned with Z. Let
˙Si = Π− 1
i Si) where Φ ∈ Rd×p is a Gaussian random matrix. It is easy to verify that
˙Si is an element on the tangent space of St(d, p) at Si. Then

2 Φ(Ip − S(cid:62)

E ˙Si ˙S(cid:62)

j =

(cid:40)

(p − d)Π−1,
(p − 2d + (cid:107)SiS(cid:62)

j (cid:107)2

F )Π−1,

i = j,
i (cid:54)= j.

Plugging them into the second necessary condition (4.23) results in

n
(cid:88)

(cid:104)Λii, ˙Si ˙S(cid:62)

i (cid:105) =

E

i=1

p − d
2

n
(cid:88)

n
(cid:88)

(cid:104)CijSjS(cid:62)

i + SiS(cid:62)

j Cji, Π−1(cid:105)

i=1
n
(cid:88)

j=1
n
(cid:88)

= (p − d)

(cid:104)SiS(cid:62)

j , Π−1(Π + (cid:101)∆ij)(cid:105)

i=1
n
(cid:88)

j=1
n
(cid:88)

(cid:104)SiS(cid:62)

j , Id + Π−1 (cid:101)∆ij(cid:105)

j=1

i=1
(cid:16)

(cid:107)Z(cid:62)S(cid:107)2

(cid:17)
F + (cid:104) (cid:101)∆Π, SS(cid:62)(cid:105)

= (p − d)

= (p − d)

22

where Cij = Π + (cid:101)∆ij and (cid:101)∆Π is deﬁned in (3.7), i.e., (cid:101)∆Π

ij = Π−1 (cid:101)∆ij.

For the right hand side of (4.23), we have

n
(cid:88)

E

n
(cid:88)

(cid:104)Cij, ˙Si ˙S(cid:62)

j (cid:105) =

i=1

j=1

=

n
(cid:88)

i=1
n
(cid:88)

n
(cid:88)

(p − 2d + (cid:107)SiS(cid:62)

j (cid:107)2

F )(cid:104)Cij, Π−1(cid:105)

j=1
n
(cid:88)

(p − 2d + (cid:107)SiS(cid:62)

j (cid:107)2

F ) Tr(Id + Π−1 (cid:101)∆ij)

i=1

j=1

= (p − 2d)n2d + (cid:107)SS(cid:62)(cid:107)2

F d

+ (p − 2d)(cid:104)ZZ(cid:62), (cid:101)∆Π(cid:105) +

n
(cid:88)

n
(cid:88)

i=1

j=1

As a result, it holds that

(cid:107)SiS(cid:62)

j (cid:107)2

F Tr( (cid:101)∆Π

ij).

(p − d)(cid:107)Z(cid:62)S(cid:107)2

F ≥ (p − 2d)n2d + (cid:107)SS(cid:62)(cid:107)2
n
(cid:88)

F d
n
(cid:88)

+ (p − d)(cid:104) (cid:101)∆Π, ZZ(cid:62) − SS(cid:62)(cid:105) +

((cid:107)SiS(cid:62)

j (cid:107)2

F − d) Tr( (cid:101)∆Π
ij)

which follows from (4.23).

Step 2: Proof of (4.25). We will use the ﬁrst-order necessary condition (4.22) to enhance

the lower bound of (cid:107)Z(cid:62)S(cid:107)F . The ﬁrst order necessary condition is equivalent to

i=1

j=1

n
(cid:88)

j=1

(Π + (cid:101)∆ij)Sj(Ip − S(cid:62)

i Si) = 0 ⇐⇒

n
(cid:88)

(Id + Π−1 (cid:101)∆ij)Sj(Ip − S(cid:62)

i Si) = 0

j=1

where Cij = Π + (cid:101)∆ij. Separating the signal from the noise leads to

Z(cid:62)S(Ip − S(cid:62)

i Si) = −

= −

n
(cid:88)

j=1
n
(cid:88)

j=1

Π−1 (cid:101)∆ijSj(Ip − S(cid:62)

i Si)

(cid:101)∆Π

ijSj(Ip − S(cid:62)

i Si), 1 ≤ i ≤ n.

Taking the Frobenius norm and summing both sides over 1 ≤ i ≤ n gives

n(cid:107)Z(cid:62)S(cid:107)2

F − (cid:104)ZZ(cid:62), SS(cid:62)SS(cid:62)(cid:105) ≤

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

j=1

(cid:101)∆Π

ijSj

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

= (cid:107) (cid:101)∆ΠS(cid:107)2
F .

Note that (cid:104)ZZ(cid:62), SS(cid:62)SS(cid:62)(cid:105) ≤ (cid:107)ZZ(cid:62)(cid:107) · (cid:107)SS(cid:62)SS(cid:62)(cid:107)∗ = n(cid:107)SS(cid:62)(cid:107)2

F . Therefore,

(cid:107)Z(cid:62)S(cid:107)2

F − (cid:107)SS(cid:62)(cid:107)2

F ≤

1
n

(cid:107) (cid:101)∆ΠS(cid:107)2
F

Step 3: Essentially, we already have established (4.24) and (4.25) with ¯∆ = (cid:101)∆Π in the
ﬁrst two steps. Now applying Proposition 4.13 immediately gives Proposition 4.12. To make
the presentation more self-contained, we brieﬂy explain how Proposition 4.13 is established.

In fact, combining the ﬁrst two steps above gives rise to

(p − 2d)(n2d − (cid:107)Z(cid:62)S(cid:107)2

F )

≤ d((cid:107)Z(cid:62)S(cid:107)2

(cid:124)

F − (cid:107)SS(cid:62)(cid:107)2
F )
(cid:123)(cid:122)
(cid:125)
T1

n
(cid:88)

n
(cid:88)

−

j=1

i=1
(cid:124)

((cid:107)SiS(cid:62)

j (cid:107)2

F − d) Tr( (cid:101)∆Π
ij)

(cid:125)

(cid:123)(cid:122)
T2

23

+(p − d) (cid:104) (cid:101)∆Π, SS(cid:62) − ZZ(cid:62)(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
T3

.

The question now reduces to estimating the error terms T1, T2, and T3. In the proof of [33,
Proposition 5.7], one can establish that

T1 ≤

d
n
T2 ≤ 4d
T3 ≤ 2(cid:107) (cid:101)∆Π(cid:107)

(cid:107) (cid:101)∆Π(cid:107)2 · (cid:107)S(cid:107)2
√

F = d2(cid:107) (cid:101)∆Π(cid:107)2,
nd(cid:107) Trd( (cid:101)∆Π)(cid:107)dF (S, Z),
nd · dF (S, Z),

√

where T1 follows from Step 2.

Therefore, we have

(p − 2d)(n2d − (cid:107)Z(cid:62)S(cid:107)2

F ) ≤ d2(cid:107) (cid:101)∆Π(cid:107)2 + 2

√

nd(p − d + 2γd)(cid:107) (cid:101)∆Π(cid:107) · dF (S, Z)

where

Now note that

γ =

(cid:107) Trd( ¯∆)(cid:107)
(cid:107) ¯∆(cid:107)

∨ 1 := max

(cid:26) (cid:107) Trd( ¯∆)(cid:107)
(cid:107) ¯∆(cid:107)

(cid:27)

, 1

d2
F (S, Z) = min

Q∈St(d,p)

(cid:107)S − ZQ(cid:107)2

F = 2nd − 2(cid:107)Z(cid:62)S(cid:107)∗ ≤ 2nd −

2
n

· (cid:107)Z(cid:62)S(cid:107)2
F

where (cid:107)Z(cid:62)S(cid:107) ≤ n. Thus
(p − 2d)nd2

F (S, Z)

2

≤ d2(cid:107) (cid:101)∆Π(cid:107)2 + 2

√

nd(p − d + 2γd)(cid:107) (cid:101)∆Π(cid:107) · dF (S, Z).

Then solving this quadratic inequality immediately yields an upper bound of dF (S, Z).

Proof of Theorem 3.4. It suﬃces to verify the optimality condition holds for the SOCP ac-
cording to Theorem 4.1. Note that the ﬁrst order critical point in Lemma 4.11 satisﬁes:

where Λ = blkdiag(Λ11, · · · , Λnn) and

(Λ − C)S = 0

Λii =

1
2

n
(cid:88)

j=1

(CijSjS(cid:62)

i + SiS(cid:62)

j Cji) =

n
(cid:88)

j=1

CijSjS(cid:62)
i .

Therefore, we only need to show Λ − C (cid:23) 0 and λd+1(Λ − C) > 0. Suppose S ∈ St(d, p)⊗n
is an SOCP, then we know Λii (cid:23) 0 from (4.23) in Lemma 4.11. The smallest eigenvalue of Λii
equals its smallest singular value and satisﬁes

λmin(Λii) = σmin





n
(cid:88)



CijSjS(cid:62)
i

 .

Note that (4.22) implies

n
(cid:88)

CijSj(Ip − S(cid:62)

i Si) = 0 ⇐⇒

j=1

n
(cid:88)

n
(cid:88)

CijSj =

CijSjS(cid:62)

i Si.

j=1

j=1
Using the fact that Si ∈ St(d, p) indicates that (cid:80)n
j=1 CijSjS(cid:62)
nonnegative singular values. As a result, it holds

j=1

i and (cid:80)n

j=1 CijSj share the same

λmin(Λii) = σmin



CijSj







n
(cid:88)

j=1

(Wely’s inequality)
(Π (cid:31) 0, rank(Z(cid:62)S) = d)

(cid:16)

(cid:17)

= σmin

ΠZ(cid:62)S + (cid:101)∆(cid:62)
i S
≥ σmin(ΠZ(cid:62)S) − (cid:107) (cid:101)∆(cid:62)
≥ σmin(Π)σmin(Z(cid:62)S) − (cid:107) (cid:101)∆(cid:62)
i S(cid:107)
(cid:19)

i S(cid:107)

(cid:18)

2δ2d
n

(cid:107) (cid:101)∆Π(cid:107)2

− (cid:107) (cid:101)∆(cid:62)

i S(cid:107)

≥ σmin(Π)

n −

24

where the last inequality follows from

dF (S, Z) =

(cid:113)

2nd − 2(cid:107)Z(cid:62)S(cid:107)∗ ≤ δ

(cid:114)

d
n

(cid:107) (cid:101)∆Π(cid:107) =⇒ σmin(Z(cid:62)S) ≥ n −

2δ2d
n

(cid:107) (cid:101)∆Π(cid:107)2

and δ is given in (4.26). Now we will show that Λ − C (cid:23) 0 where

Λ − C = Λ − ZΠZ(cid:62) − (cid:101)∆.

Note that S is in the null space of Λ − C; it suﬃces to show that u(cid:62)(Λ − C)u ≥ 0 for u ⊥ S,
i.e., S(cid:62)u = 0.
We have

u(cid:62)ZΠZ(cid:62)u ≤ (cid:107)Π(cid:107) · (cid:107)Z(cid:62)u(cid:107)2

= (cid:107)Π(cid:107) · (cid:107)Q(cid:62)Z(cid:62)u(cid:107)2
= (cid:107)Π(cid:107) · (cid:107)(S − ZQ)(cid:62)u(cid:107)2
≤ (cid:107)Π(cid:107) · (cid:107)S − ZQ(cid:107)2

≤

δ2d
n

(cid:107) (cid:101)∆Π(cid:107)2(cid:107)Π(cid:107)

where Q = argminQ∈St(d,p) (cid:107)S − ZQ(cid:107)F .

Therefore, we have

u(cid:62)(Λ − C)u ≥ min
1≤i≤n

λmin(Λii) − u(cid:62)(ZΠZ(cid:62) + (cid:101)∆)u

(cid:18)

≥ σmin(Π)

n −

(cid:19)

(cid:107) (cid:101)∆Π(cid:107)2

δ2d
2n

− max
1≤i≤n

(cid:107) (cid:101)∆(cid:62)

i S(cid:107) −

δ2d
n

(cid:107) (cid:101)∆Π(cid:107)2(cid:107)Π(cid:107) − (cid:107) (cid:101)∆(cid:107) > 0

(4.27)

where κ(Π) = σmax(Π)/σmin(Π) is the condition number of Π. The condition (4.27) ensures
the global optimality of SS(cid:62) in the SDP.

To guarantee the uniqueness of S as the global maximizer, we need to control the rank of

Λ − C:

rank(Λ − C) ≥ rank(Λ − (cid:101)∆) − rank(ZΠZ(cid:62)) = nd − d.

due to the subadditivity of rank where the rank(Λ − (cid:101)∆) = nd follows from

min
1≤i≤n

λmin(Λii) > (cid:107) (cid:101)∆(cid:107) =⇒ Λ − (cid:101)∆ (cid:31) 0

which is guaranteed by (4.27). This implies the rank of S is d, and S and SS(cid:62) are the unique
global maximizer to (P) and (SDP) respectively.

Now we apply (4.27) to the special case when Cij = AiA(cid:62)

j where Ai = A + ∆i. Note that

(4.13) implies

and

max
1≤i≤n

(cid:107) (cid:101)∆(cid:62)

i S(cid:107) ≤ 3n(cid:107)A(cid:107) max
1≤i≤n

(cid:107)∆i(cid:107)

(cid:107) (cid:101)∆Π(cid:107) ≤ (cid:107)Π−1(cid:107)(cid:107) (cid:101)∆(cid:107) ≤ σ−2

min(A) · 3

√

n(cid:107)A(cid:107) · (cid:107)∆(cid:107)

where Π = AA(cid:62), (cid:101)∆ij = A∆(cid:62)
estimations into the optimality condition (4.27), we have

j + ∆iA(cid:62) + ∆i∆(cid:62)

j , and (cid:107) (cid:101)∆(cid:107) ≤ 3

√

n(cid:107)A(cid:107)(cid:107)∆(cid:107). Substituting these

n >

δ2d(cid:107) (cid:101)∆Π(cid:107)2
n

(cid:18) 1
2

(cid:19)

+ κ2

+

1
σmin(Π)

(cid:18)

max
1≤i≤n

25

(cid:107) (cid:101)∆(cid:62)

i S(cid:107) + (cid:107) (cid:101)∆(cid:107)

(cid:19)

where κ = (cid:107)A(cid:107)/σmin(A) and κ2 = (cid:107)Π(cid:107)/σmin(Π). Equivalently,

(cid:32)

(2 +

√

5)(p + d)

p − 2d

(cid:114)

d
n

n >

(cid:107) Trd( (cid:101)∆Π)(cid:107) ∨ (cid:107) (cid:101)∆Π(cid:107)

(cid:33)2 (cid:18) 1
2

(cid:19)

+

+ κ2

1
σmin(Π)

(cid:18)

max
1≤i≤n

where δ is given in Proposition 4.13.

Using the bound on (cid:107) (cid:101)∆(cid:107) and (cid:107) (cid:101)∆(cid:62)

i S(cid:107), we have

(cid:107) (cid:101)∆(cid:62)

i S(cid:107) + (cid:107) (cid:101)∆(cid:107)

(cid:19)

n >

d
n

√

(cid:32)

(2 +

5)(p + d)

p − 2d

(cid:107) Trd( (cid:101)∆Π)(cid:107) ∨ (cid:107) (cid:101)∆Π(cid:107)

(cid:33)2 (cid:18) 1
2

(cid:19)

+

+ κ2

6n(cid:107)A(cid:107) max1≤i≤n (cid:107)∆i(cid:107)
σ2
min(A)

√

where (cid:107)∆(cid:107) ≤
optimization landscape is benign:

n max1≤i≤n (cid:107)∆i(cid:107). In conclusion, if the following inequalities are true, then the

(cid:107) Trd( (cid:101)∆Π)(cid:107) ∨ (cid:107) (cid:101)∆Π(cid:107) ≤

n(p − 2d)
√

8κ(p + d)

,

d

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

σmin(A)
12κ

.

Finally, Theorem 3.5 follows from applying Theorem 3.4 to the data with Gaussian noise.

Our analysis will use the following important fact about Gaussian random matrix.

Theorem 4.14. [52, Theorem 4.4.5] For any X ∈ Rn×m random matrix whose entries are
i.i.d. standard normal random variables. For any t > 0, it holds

with probability at least 1 − 2 exp(−t2).

(cid:107)X(cid:107) (cid:46)

√

n +

√

m + t

Proof of Theorem 3.5. The proof of Theorem 3.5 is quite straightforward: it suﬃces to plug
Ai = A + σWi into the assumptions of Theorem 3.4. Under the normal error model,

Π = AA(cid:62),

(cid:101)∆ij = σ(AW (cid:62)

j + WiA(cid:62) + σWiW (cid:62)

j ).

Now we need to estimate the operator norm of the partial trace Trd( (cid:101)∆Π) ∈ Rn×n. Deﬁne

ξi :=

(cid:104)(AA(cid:62))−1A, Wi(cid:105)
(cid:112)Tr((AA(cid:62))−1)

∼ N (0, 1), wi := vec(Wi) ∈ Rmd, (cid:102)W = (cid:2)w1, · · · , wn

(cid:3) ∈ Rmd×n

where Wi is a d × m Gaussian random matrix and wi is the vectorization of Wi. Then the
(i, j)-entry of Trd( (cid:101)∆Π) equals

[Trd( (cid:101)∆Π)]ij = (cid:104)(AA(cid:62))−1, (cid:101)∆ij(cid:105)

= (cid:104)(AA(cid:62))−1, σ(AW (cid:62)

j + WiA(cid:62)) + σ2WiW (cid:62)
j (cid:105)

(cid:113)

Tr[(AA(cid:62))−1](ξj + ξi) + σ2(cid:104)(AA(cid:62))−1, WiW (cid:62)
j (cid:105)

(cid:113)

Tr[(AA(cid:62))−1](ξj + ξi) + σ2(cid:104)Im ⊗ (AA(cid:62))−1wi, wj(cid:105).

= σ

= σ

Writing the partial trace into the matrix form gives

Trd( (cid:101)∆Π) = σ

(cid:113)

Tr[(AA(cid:62))−1](ξ1(cid:62)

n + 1nξ(cid:62)) + σ2 (cid:102)W (cid:62)(Im ⊗ (AA(cid:62))−1) (cid:102)W

and

(cid:107) Trd( (cid:101)∆Π)(cid:107) ≤ 2σ

(cid:113)

n Tr[(AA(cid:62))−1](cid:107)ξ(cid:107) + σ2(cid:107)(AA(cid:62))−1(cid:107)(cid:107) (cid:102)W (cid:107)2

where ξ = [ξ1, · · · , ξn](cid:62).

26

Theorem 4.4.5 in [52] implies that
√

(cid:107)ξ(cid:107) (cid:46)

n,

(cid:107) (cid:102)W (cid:107) (cid:46)

√

√

n +

md.

As a result, it holds that

(cid:107) Trd( (cid:101)∆Π)(cid:107) (cid:46) 2nσ

(cid:113)

√

Tr[(AA(cid:62))−1] + σ2(cid:107)(AA(cid:62))−1(cid:107) · (

√

√

md)2

n +

≤

2nσ
d
σmin(A)

+

nσ2(1 + (cid:112)md/n)2
σ2
min(A)

with high probability.

We proceed to estimate (cid:107) (cid:101)∆Π(cid:107):

(cid:107) (cid:101)∆Π(cid:107) ≤ (cid:107)(AA(cid:62))−1(cid:107)(cid:107) (cid:101)∆(cid:107) ≤

(cid:46)

=

1
σ2
min(A)
1
σ2
min(A)
√

m.

where (cid:107)W (cid:107) (cid:46)

√

nd +

1
σ2
min(A)
√
√

m)(2

√

n(cid:107)A(cid:107)(cid:107)W (cid:107) + σ2(cid:107)W (cid:107)2)

· (2σ

√

n(cid:107)A(cid:107) + σ(

nd +

√

m))

√

· σ(

nd +

√

· σ(

d + (cid:112)m/n)(2(cid:107)A(cid:107) + σ(

√

d + (cid:112)m/n))n

Under the assumption of Theorem 3.5

σ (cid:46) p − 2d
p + d

·

√

√

d(

κ2

σmin(A)
√

d +

m + 2

√

,

log n)

we immediately have max1≤i≤n (cid:107)∆i(cid:107) ≤ σmin(A)
trace, it holds that

12κ

. For the operator norm of (cid:98)∆Π and its partial

√
2

d

σmin(A)

·

√

σmin(A)
√
d +
d(

√

κ2

(1 + (cid:112)md/n)2
σ2
min(A)
(cid:19)

·

(cid:33)

σ2
min(A)
√
√
d +

κ4d(

m)2

+

m)
√

√

2
d +

+

√

m)

(1 +
√

m)2
√

κ4d(

d +

m)2

(cid:107) Trd( (cid:101)∆Π)(cid:107) (cid:46) n(p − 2d)

p + d

(Use d ≤ n) ≤

n(p − 2d)
p + d
(cid:46) n(p − 2d)
p + d

and

(cid:32)

(cid:18)

κ2(
1
√
κ2

d

·

· (cid:107)A(cid:107)

(cid:16)√

d + (cid:112)m/n

(cid:17)

n ·

p − 2d
p + d

·

√

σmin(A)
√
d +
d(

√

κ2

m)

(cid:107) (cid:101)∆Π(cid:107) (cid:46)

1
σ2
min(A)
(cid:46) n(p − 2d)
p + d

·

1
√

κ

.

d

This conﬁrms

(cid:107) Trd( (cid:101)∆Π)(cid:107) ∨ (cid:107) (cid:101)∆Π(cid:107) ≤

n(p − 2d)
√

8κ(p + d)

,

d

max
1≤i≤n

(cid:107)∆i(cid:107) ≤

σmin(A)
12κ

holds with high probability in Theorem 3.4 where the second inequality directly follows from (3.8).

References

[1] E. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from
censored edge measurements: Phase transition and eﬃcient recovery. IEEE Transactions
on Network Science and Engineering, 1(1):10–22, 2014.

27

[2] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.

Princeton University Press, 2009.

[3] A. Ahmed, B. Recht, and J. Romberg. Blind deconvolution using convex programming.

IEEE Transactions on Information Theory, 60(3):1711–1732, 2013.

[4] A. S. Bandeira, N. Boumal, and A. Singer. Tightness of the maximum likelihood semideﬁ-
nite relaxation for angular synchronization. Mathematical Programming, 163(1-2):145–167,
2017.

[5] A. S. Bandeira, M. Charikar, A. Singer, and A. Zhu. Multireference alignment using
semideﬁnite programming. In Proceedings of the 5th Conference on Innovations in Theo-
retical Computer Science, pages 459–470, 2014.

[6] A. S. Bandeira, C. Kennedy, and A. Singer. Approximating the little Grothendieck problem
over the orthogonal and unitary groups. Mathematical Programming, 160(1):433–475, 2016.

[7] A. S. Bandeira, Y. Khoo, and A. Singer. Open problem: Tightness of maximum likelihood
In Conference on Learning Theory, pages 1265–1267. PMLR,

semideﬁnite relaxations.
2014.

[8] P. J. Besl and N. D. McKay. Method for registration of 3D shapes. In Sensor Fusion IV:
Control Paradigms and Data Structures, volume 1611, pages 586–606. International Society
for Optics and Photonics, 1992.

[9] N. Boumal. A Riemannian low-rank method for optimization over semideﬁnite matrices

with block-diagonal constraints. arXiv preprint arXiv:1506.00575, 2015.

[10] N. Boumal. Nonconvex phase synchronization. SIAM Journal on Optimization, 26(4):2355–

2377, 2016.

[11] N. Boumal, V. Voroninski, and A. Bandeira. The non-convex Burer-Monteiro approach
works on smooth semideﬁnite programs. In Advances in Neural Information Processing
Systems, pages 2757–2765, 2016.

[12] N. Boumal, V. Voroninski, and A. S. Bandeira. Deterministic guarantees for burer-monteiro
factorizations of smooth semideﬁnite programs. Communications on Pure and Applied
Mathematics, 73(3):581–608, 2020.

[13] S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.

[14] S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semideﬁnite

programming. Mathematical Programming, 103(3):427–444, 2005.

[15] E. J. Cand`es, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix

completion. SIAM Review, 57(2):225–251, 2015.

[16] E. J. Cand`es, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger ﬂow: Theory
and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.

[17] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics, 9(6):717–772, 2009.

[18] K. N. Chaudhury, Y. Khoo, and A. Singer. Global registration of multiple point clouds
using semideﬁnite programming. SIAM Journal on Optimization, 25(1):468–501, 2015.

[19] Y. Chen and E. J. Cand`es. The projected power method: An eﬃcient algorithm for joint
alignment from pairwise diﬀerences. Communications on Pure and Applied Mathematics,
71(8):1648–1714, 2018.

28

[20] C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation III. SIAM

Journal on Numerical Analysis, 7(1):1–46, 1970.

[21] N. Dym and Y. Lipman. Exact recovery with symmetries for Procrustes matching. SIAM

Journal on Optimization, 27(3):1513–1530, 2017.

[22] A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogonality
constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303–353, 1998.

[23] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Pro-
ceedings of the 30th International Conference on Neural Information Processing Systems,
pages 2981–2989, 2016.

[24] M. X. Goemans and D. P. Williamson.

Improved approximation algorithms for maxi-
mum cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM
(JACM), 42(6):1115–1145, 1995.

[25] J. C. Gower. Generalized Procrustes analysis. Psychometrika, 40(1):33–51, 1975.

[26] J. C. Gower and G. B. Dijksterhuis. Procrustes Problems, volume 30. Oxford University

Press on Demand, 2004.

[27] A. Granas and J. Dugundji. Fixed Point Theory. Springer, 2003.

[28] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE

Transactions on Information Theory, 56(6):2980–2998, 2010.

[29] Y. Khoo and A. Kapoor. Non-iterative rigid 2D/3D point-set registration using semideﬁnite

programming. IEEE Transactions on Image Processing, 25(7):2956–2970, 2016.

[30] R.-C. Li. New perturbation bounds for the unitary polar factor. SIAM Journal on Matrix

Analysis and Applications, 16(1):327–332, 1995.

[31] S. Ling. Improved performance guarantees for orthogonal group synchronization via gen-

eralized power method. arXiv preprint arXiv:2012.00470, 2020.

[32] S. Ling. Near-optimal performance bounds for orthogonal and permutation group synchro-

nization via spectral methods. arXiv preprint arXiv:2008.05341, 2020.

[33] S. Ling. Solving orthogonal group synchronization via convex and low-rank optimization:

Tightness and landscape analysis. arXiv preprint arXiv:2006.00902, 2020.

[34] S. Ling, R. Xu, and A. S. Bandeira. On the landscape of synchronization networks: A
perspective from nonconvex optimization. SIAM Journal on Optimization, 29(3):1879–
1907, 2019.

[35] H. Liu, M.-C. Yue, and A. Man-Cho So. On the estimation performance and conver-
gence rate of the generalized power method for phase synchronization. SIAM Journal on
Optimization, 27(4):2426–2446, 2017.

[36] H. Liu, M.-C. Yue, and A. M.-C. So. A uniﬁed approach to synchronization problems over

subgroups of the orthogonal group. arXiv preprint arXiv:2009.07514, 2020.

[37] C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit regularization in nonconvex statistical
estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics, 20:451–632, 2020.

[38] H. Maron, N. Dym, I. Kezurer, S. Kovalsky, and Y. Lipman. Point registration via eﬃcient

convex relaxation. ACM Transactions on Graphics (TOG), 35(4):1–12, 2016.

29

[39] N. J. Mitra, N. Gelfand, H. Pottmann, and L. Guibas. Registration of point cloud data
from a geometric optimization perspective. In Proceedings of the 2004 Eurographics/ACM
SIGGRAPH Symposium on Geometry Processing, pages 22–31, 2004.

[40] A. Naor, O. Regev, and T. Vidick. Eﬃcient rounding for the noncommutative Grothendieck
inequality. In Proceedings of the Forty-ﬁfth Annual ACM Symposium on Theory of Com-
puting, pages 71–80, 2013.

[41] A. Nemirovski. Sums of random symmetric matrices and quadratic optimization under

orthogonality constraints. Mathematical Programming, 109(2):283–317, 2007.

[42] O. Ozyesil, V. Voroninski, R. Basri, and A. Singer. A survey of structure from motion.

Acta Numerica, pages 305–364, 2017.

[43] T. Pumir, A. Singer, and N. Boumal. The generalized orthogonal Procrustes problem in

the high noise regime. Information and Inference: A Journal of the IMA, 2021.

[44] D. M. Rosen, L. Carlone, A. S. Bandeira, and J. J. Leonard. SE-Sync: A certiﬁably correct
algorithm for synchronization over the special Euclidean group. The International Journal
of Robotics Research, 38(2-3):95–125, 2019.

[45] P. H. Sch¨onemann. A generalized solution of the orthogonal Procrustes problem. Psy-

chometrika, 31(1):1–10, 1966.

[46] A. Singer et al. Mathematics for cryo-electron microscopy. Proceedings of the International

Congress of Mathematicians (ICM), 3:3981–4000, 2018.

[47] A. Singer and Y. Shkolnisky. Three-dimensional structure determination from common
lines in cryo-em by eigenvectors and semideﬁnite programming. SIAM Journal on Imaging
Sciences, 4(2):543–572, 2011.

[48] A. M.-C. So. Moment inequalities for sums of random matrices and their applications in

optimization. Mathematical Programming, 130(1):125–151, 2011.

[49] M. B. Stegmann and D. D. Gomez. A brief introduction to statistical shape analysis.
Informatics and Mathematical Modelling, Technical University of Denmark, DTU, 15(11),
2002.

[50] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview
IEEE Transactions on Information Theory, 63(2):853–884,

and the geometric picture.
2016.

[51] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Foundations of

Computational Mathematics, 18(5):1131–1198, 2018.

[52] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data

Science, volume 47. Cambridge University Press, 2018.

[53] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48.

Cambridge University Press, 2019.

[54] I. Waldspurger and A. Waters. Rank optimality for the Burer–Monteiro factorization.

SIAM Journal on Optimization, 30(3):2577–2602, 2020.

[55] L. Wang and A. Singer. Exact and stable recovery of rotations for robust synchronization.

Information and Inference: A Journal of the IMA, 2(2):145–193, 2013.

[56] P. Wang, H. Liu, Z. Zhou, and A. M.-C. So. Optimal non-convex exact recovery in stochastic

block model via projected power method. arXiv preprint arXiv:2106.05644, 2021.

30

[57] P.-˚A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT

Numerical Mathematics, 12(1):99–111, 1972.

[58] Z. Wen and W. Yin. A feasible method for optimization with orthogonality constraints.

Mathematical Programming, 142(1):397–434, 2013.

[59] T. Zhang. Tightness of the semideﬁnite relaxation for orthogonal trace-sum maximization.

arXiv preprint arXiv:1911.08700, 2019.

[60] Y. Zhong and N. Boumal. Near-optimal bounds for phase synchronization. SIAM Journal

on Optimization, 28(2):989–1016, 2018.

31

