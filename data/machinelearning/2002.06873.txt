Springer Nature 2021 LATEX template

œÄVAE: a stochastic process prior for Bayesian deep learning
with MCMC

Swapnil Mishra1,2*‚Ä†, Seth Flaxman3‚Ä†, Tresnia Berah4, Harrison Zhu4, Mikko
Pakkanen4 and Samir Bhatt1,2‚Ä†

1*MRC Centre for Global Infectious Disease Analysis, Jameel Institute for Disease and
Emergency Analytics, School of Public Health, Imperial College London, London, UK.
2Section of Epidemiology, Department of Public Health, University of Copenhagen,
Copenhagen, Denmark.
3Department of Computer Science, University of Oxford, Oxford, UK.
4Department of Mathematics, Imperial College London, London, UK.

*Corresponding author(s). E-mail(s): s.mishra@imperial.ac.uk;
‚Ä†These authors contributed equally to this work.

Abstract

Stochastic processes provide a mathematically elegant way to model complex data. In theory, they
provide Ô¨Çexible priors over function classes that can encode a wide range of interesting assumptions.
However, in practice eÔ¨Écient inference by optimisation or marginalisation is diÔ¨Écult, a problem
further exacerbated with big data and high dimensional input spaces. We propose a novel varia-
tional autoencoder (VAE) called the prior encoding variational autoencoder (œÄVAE). œÄVAE is a
new continuous stochastic process. We use œÄVAE to learn low dimensional embeddings of function
classes by combining a trainable feature mapping with generative model using a VAE. We show
that our framework can accurately learn expressive function classes such as Gaussian processes,
but also properties of functions such as their integrals. For popular tasks, such as spatial interpo-
lation, œÄVAE achieves state-of-the-art performance both in terms of accuracy and computational
eÔ¨Éciency. Perhaps most usefully, we demonstrate an elegant and scalable means of performing fully
Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.

Keywords: Bayesian inference, MCMC, VAE, Spatio-temporal

2
2
0
2

p
e
S
3
1

]

G
L
.
s
c
[

6
v
3
7
8
6
0
.
2
0
0
2
:
v
i
X
r
a

1 Introduction

A central task in machine learning is to specify
a function or set of functions that best gener-
alises to new data. Stochastic processes (Pavliotis,
2014; Ross, 1996) provide a mathematically ele-
gant way to deÔ¨Åne a class of functions, where each
element from a stochastic process is a (usually
inÔ¨Ånite) collection of random variables. Popular

examples of stochastic processes in computational
statistics and machine learning are Gaussian pro-
cesses (Rasmussen & Williams, 2006), Dirichlet
processes (Antoniak, 1974),
log-Gaussian Cox
processes (M√∏ller, Syversveen, & Waagepetersen,
1998), Hawkes processes (Hawkes, 1971), Mondrian
processes (Roy & Teh, 2009) and Gauss-Markov
processes (Lindgren, Rue, & Lindstr√∂m, 2011).
Many of these processes are intimately connected

1

 
 
 
 
 
 
2

œÄVAE

Springer Nature 2021 LATEX template

with popular techniques in deep learning, for
example, both the inÔ¨Ånite width limit of a sin-
gle layer neural network and the evolution of
a deep neural network by gradient descent are
Gaussian processes (Jacot, Gabriel, & Hongler,
2018; R. Neal, 1996). However, while stochastic
processes have many favourable properties, they
are often cumbersome to work with in practice.
For example, inference and prediction using a
Gaussian process requires matrix inversions that
scale cubicly with data size, log-Gaussian Cox
processes require the evaluation of an intractable
integral and Markov processes are often highly
correlated. Bayesian inference can be even more
challenging due to complex high dimensional pos-
terior topologies. Gold standard evaluation of
posterior expectations is done by Markov Chain
Monte Carlo (MCMC) sampling, but high auto-
correlation, narrow typical sets (Betancourt, Byrne,
Livingstone, & Girolami, 2017) and poor scalabil-
ity have prevented use in big data and complex
model settings. A plethora of approximation algo-
rithms exist (Blundell, Cornebise, Kavukcuoglu,
& Wierstra, 2015; Lakshminarayanan, Pritzel, &
Blundell, 2017; Minka, 2001; Ritter, Botev, &
Barber, 2018; Welling & Teh, 2011), but few actu-
ally yield accurate posterior estimates (HoÔ¨Äman,
Blei, Wang, & Paisley, 2013; Huggins, Kasprzak,
Campbell, & Broderick, 2019; J. Yao, Pan, Ghosh,
& Doshi-Velez, 2019; Y. Yao, Vehtari, Simp-
son, & Gelman, 2018). In this paper, rather
than relying on approximate Bayesian inference
to solve complex models, we extend variational
autoencoders (VAE) (Kingma & Welling, 2014;
Rezende, Mohamed, & Wierstra, 2014) to develop
portable models that can work with state-of-the-art
Bayesian MCMC software such as Stan (Car-
penter et al., 2017). Inference on the resulting
models is tractable and yields accurate posterior
expectations and uncertainty.

An autoencoder (Hinton & Salakhutdinov,
2006) is a model comprised of two component net-
works. The encoder e : X ‚Üí Z encodes inputs
from space X into a latent space Z of lower dimen-
sion than X . The decoder d : Z ‚Üí X decodes
latent codes in Z to reconstruct the input. The
parameters of e and d are learned through the
minimisation of a reconstruction loss on a training
dataset. A VAE extends the autoencoder into a
generative model (Kingma & Welling, 2014). In a
VAE, the latent space Z is given a distribution,

such as standard normal, and a variational approxi-
mation to the posterior is estimated. In a variety of
applications, VAEs do a superb job reconstructing
training datasets and enable the generation of new
data: samples from the latent space are decoded to
generate synthetic data (Kingma & Welling, 2019).
In this paper we propose a novel use of VAEs: we
learn low-dimensional representations of samples
from a given function class (e.g. sample paths from
a Gaussian process prior). We then use the result-
ing low dimensional representation and the decoder
to perform Bayesian inference.

One key beneÔ¨Åt of this approach is that we
decouple the prior from inference to encode arbi-
trarily complex prior function classes, without
needing to calculate any data likelihoods. A second
key beneÔ¨Åt is that when inference is performed,
our sampler operates in a low dimensional, uncor-
related latent space which greatly aids eÔ¨Éciency
and computation, as demonstrated in the spatial
statistics setting in PriorVAE (Semenova et al.,
2022). One limitation of this approach (and of
PriorVAE) is that we are restricted to encoding
Ô¨Ånite-dimensional priors, because VAEs are not
stochastic processes. To overcome this limitation,
we take as inspiration the Karhunen-Lo√®ve decom-
position of a stochastic process as a random linear
combination of basis functions and introduce a
new VAE called the prior encoding VAE (œÄVAE).
œÄVAE is a valid stochastic process by construction,
it is capable of learning a set of basis functions,
and it incorporates a VAE, enabling simulation
and highly eÔ¨Äective fully Bayesian inference.

We employ a two step approach: Ô¨Årst, we encode
the prior using our novel architecture; second we
use the learnt basis and decoder network‚Äîa new
stochastic process in its own right‚Äîas a prior,
combining it with a likelihood in a fully Bayesian
modeling framework, and use MCMC to Ô¨Åt our
model and infer the posterior. We believe our frame-
work‚Äôs novel decoupling into two stages is critically
important for many complex scenarios, because
we do not need to compromise in terms of either
the expressiveness of deep learning or accurately
characterizing the posterior using fully Bayesian
inference.

We thus avoid some of the drawbacks of other
Bayesian deep learning approaches which rely
solely on variational inference, and the drawbacks

Springer Nature 2021 LATEX template

œÄVAE

3

of standard MCMC methods for stochastic pro-
cesses which are ineÔ¨Écient and suÔ¨Äer from poor
convergence.

Taken together, our work is an important
advance in the Ô¨Åeld of Bayesian deep learning,
providing a practical framework combining the
expressive capability of deep neural networks to
encode stochastic processes with the eÔ¨Äectiveness
of fully Bayesian and highly eÔ¨Écient gradient-
based MCMC inference to Ô¨Åt to data while fully
characterizing uncertainty.

Once a œÄVAE is trained and deÔ¨Åned, the com-
plexity of the decoder scales linearly in the size of
the largest hidden layer. Additionally, because the
latent variables are penalised via the KL term from
deviating from a standard normal distribution, the
latent space is approximately uncorrelated, leading
to high eÔ¨Äective sample sizes in MCMC sampling.
The main contributions of this paper are:

‚Ä¢ We apply the generative framework of VAEs to
perform full Bayesian inference. We Ô¨Årst encode
priors in training and then, given new data,
perform inference on the latent representation
while keeping the trained decoder Ô¨Åxed.

‚Ä¢ We propose a new generative model, œÄVAE,
that generalizes VAEs to be able to learn priors
over both functions and properties of functions.
We show that œÄVAE is a valid (and novel)
stochastic process by construction.

‚Ä¢ We show the performance of œÄVAE on a range
of simulated and real data, and show that
œÄVAE achieves state-of-the-art performance in
a spatial interpolation task.

The rest of this paper is structured as follows.
Section 2 details the proposed framework and the
generative model along with toy Ô¨Åtting examples.
The experiments on large real world datasets are
outlined in Section 3. We discuss our Ô¨Åndings and
conclude in Section 4.

2 Methods

2.1 Variational Autoencoders

(VAEs)

A standard VAE has three components:

1. an encoder network e(x, Œ≥) which encodes inputs

x ‚àà X using learnable parameters Œ≥,

2. random variables z for the latent subspace,

3. a decoder network d(z, œà) which decodes latent
embeddings z using learnable parameters œà.

In the simplest case we are given inputs x ‚àà Rd =
X such as a Ô¨Çattened image or discrete time series.
The encoder e(x, Œ≥) and decoder d(z, œà) are fully
connected neural networks (though they could
include convolution or recurrent layer). The output
of the encoder network are vectors of mean and
standard deviation parameters z¬µ and zsd. These
vectors can thus be used to deÔ¨Åne the random
variable Z for the latent space:

[z¬µ, zsd](cid:62) = e(x, Œ≥)

Z ‚àº N (z¬µ, z2
sd

I)

(1)

(2)

For random variable Z, the decoder network
reconstructs the input by producing ÀÜx:

ÀÜx = d(Z, œà)

(3)

To train a VAE, a variational approximation is

used to estimate the posterior distribution

p(Z | x, Œ≥, œà) ‚àù p(x | Z, Œ≥, œà) √ó p(Z)

The variational approximation greatly simpliÔ¨Åes
inference by turning a marginalisation problem into
an optimisation problem. Following (Kingma &
Ba, 2014), the optimal parameters for the encoder
and decoder are found by maximising the evidence
lower bound:

(cid:20)

EZ

arg max
Œ≥,œà

log p (x | Z, Œ≥, œà)

‚àí KL (Z (cid:107) N (0, I))

(cid:21)

(4)

The Ô¨Årst term in Eq. (4) is the likelihood quan-
tifying how well ÀÜx matches x. In practice we can
simply adopt the mean squared error loss directly,
referred to as the reconstruction loss, without tak-
ing a probabilistic perspective. The second term is
a Kullback-Leibler divergence to ensure that Z is
as similar as possible to the prior distribution, a
standard normal. Again, this second term can be
speciÔ¨Åed directly without the evidence lower bound
derivation: we view the KL-divergence as a regular-
ization penalty to ensure that the latent parameters
are approximately uncorrelated by penalizing how
far they deviate from N (0, I).

4

œÄVAE

Springer Nature 2021 LATEX template

Once training is complete, we Ô¨Åx œà, and use
the decoder as a generative model. To simplify
subsequent notation we refer to a fully trained
decoder as d(z). Generating a new sample is simple:
Ô¨Årst draw a random variable Z ‚àº N (0, I) and
then apply the decoder, which is a deterministic
transformation to obtain d(Z). We see immediately
that d(Z) is itself a random variable. In the next
section, we will use this generative model as a
prior in a Bayesian framework by linking it to a
likelihood to obtain a posterior.

2.2 VAEs for Bayesian inference

VAEs have been typically used in the literature to
create or learn a generative model of observed data
(Kingma & Welling, 2014), such as images. (Semen-
ova et al., 2022) introduced a novel application of
VAEs in a Bayesian inference setting, using a two
stage approach that is closely related to ours. In
brief, in the Ô¨Årst stage, a VAE is trained to encode
and decode a large dataset of vectors consisting of
samples drawn from a speciÔ¨Åed prior p(Œ∏) over ran-
dom vectors. In the second stage, the original prior
is replaced with the approximate prior: Œ∏ := d(Z)
where Z ‚àº N (0, I).

To see how this works in a Bayesian infer-
ence setting, consider a likelihood p(y | Œ∏) linking
the parameter Œ∏ to data y. Bayes‚Äô rule gives the
unnormalized posterior:

p(Œ∏ | y) ‚àù p(y | Œ∏) √ó p(Œ∏)

(5)

The trained decoder serves as a drop-in replace-
ment for the original prior class in a Bayesian
setting:

p(Z | y, d) ‚àù p(y | d(Z)) √ó p(Z) .

(6)

The implementation within a probabilistic pro-
gramming language is very straightforward: a
standard normal prior and deterministic function
(the decoder) are all that is needed.

It is useful to contrast the inference task from
Eq. (6) to a Bayesian neural network (BNN)
(R. Neal, 1996) or Gaussian process in primal form
(Rahimi & Recht, 2008). In a BNN with parame-
ters œâ and hyperparameters Œª, the unnormalised
posterior would be

p(œâ, Œª | y) ‚àù p(y | œâ, Œª) √ó p(œâ | Œª) √ó p(Œª) .

(7)

The key diÔ¨Äerence between Eq. (7) and Eq. (6) is
the term p(œâ | Œª). The dimension of œâ is typically
huge, sometimes in the millions, and is conditional
on Œª, whereas in Eq. (6) the latent dimension
of Z is typically small (< 50), uncorrelated and
unconditioned. Full batch MCMC training is typ-
ically prohibitive for BNNs due to large datasets
and the high-dimensionality of œâ, but approximate
Bayesian inference algorithms tend to poorly cap-
ture the complex posterior (J. Yao et al., 2019;
Y. Yao et al., 2018). Additionally, œâ tends to be
highly correlated, making eÔ¨Écient MCMC nearly
impossible. Finally, as the dimension and depth
increases, the posterior distribution suÔ¨Äers from
complex multimodality, and concentration to a
narrow typical set (Betancourt et al., 2017). By
contrast, oÔ¨Ä-the-shelf MCMC methods are very
eÔ¨Äective for equation (6) because the prior space
they need to explore is as simple as it could be:
a standard normal distribution, while the com-
plexity of the model lives within the deterministic
(and diÔ¨Äerentiable) decoder. In a challenging spa-
tial statistics setting, (Semenova et al., 2022) used
this approach and achieved MCMC eÔ¨Äective sam-
ple sizes exceeding actual sample sizes, due to the
incredible eÔ¨Éciency of the MCMC sampler.

An example of using VAEs to perform infer-
ence is shown in Figure 1 where we train a VAE
with latent dimensionality 10 on samples drawn
from a zero mean Gaussian process with RBF
kernel (K(Œ¥) = e‚àíŒ¥2/82
) observed on the grid
0, 0.01, 0.02, . . . , 1.0. In Figure 1 we closely recover
the true function and correctly estimate the data
noise parameter. Our MCMC samples showed vir-
tually no autocorrelation, and all diagnostic checks
were excellent (see Appendix). Solving the equiva-
lent problem using a Gaussian process prior would
not only be considerably more expensive (O(n3))
but correlations in the parameter space would com-
plicate MCMC sampling and necessitate very long
chains to achieve even modest eÔ¨Äective sample
sizes.

This example demonstrates the promise that
VAEs hold to improve Bayesian inference by
encoding function classes in a two stage process.
While this simple example proved useful in some
settings (Semenova et al., 2022), inference and
prediction is not possible at new input locations,
because a VAE is not a stochastic process. As
described above, a VAE provides a novel prior over
random vectors. Below, we take the next step by

Springer Nature 2021 LATEX template

œÄVAE

5

(a)

(b)

Fig. 1: Learning functions with VAE: (a) Prior samples from a VAE trained on Gaussian process samples
(b) we Ô¨Åt our VAE model to data drawn from a GP (blue) plus noise (black points). The posterior mean
of our model is in red with the 95% epistemic credible intervals shown in purple.

introducing œÄVAE, a new stochastic process capa-
ble of approximating useful and widely used priors
over function classes, such as Gaussian processes.

2.3 Encoding stochastic processes

with œÄVAE

To create a model with the ability to perform
inference on a wide range of problems we have to
ensure that it is a valid stochastic process. Pre-
vious attempts in deep learning in this direction
have been inspired by the Kolmogorov Extension
Theorem and have focused on extending from a
Ô¨Ånite-dimensional distribution to a stochastic pro-
cess. SpeciÔ¨Åcally, (Garnelo et al., 2018) introduced
an aggregation step (typically an average) to cre-
ate an order invariate global distribution. However,
as noted by (Kim et al., 2019), this can lead to
underÔ¨Åtting.

by

We take a diÔ¨Äerent approach with œÄVAE,
inspired
the Karhunen-Lo√®ve Expan-
sion (Karhunen, 1947; Loeve, 1948). Recall that a
centered stochastic process f (s) can be written as
an inÔ¨Ånite sum:

f (s) =

‚àû
(cid:88)

j=1

Œ≤jœÜj(s)

(8)

for pairwise uncorrelated random variables Œ≤j
and continuous real-valued functions forming an
orthonormal basis œÜj(s). The random Œ≤j‚Äôs provide
a linear combination of a Ô¨Åxed set of basis func-
tions, œÜj. This perspective has a long history in
neural networks, cf. radial basis function networks.
What if we consider a trainable, deep learn-
ing parameterization of Eq. (8) as inspiration? We
need to learn deterministic basis functions while

allowing the Œ≤j‚Äôs to be random. Let Œ¶(s) be a fea-
ture mapping with weights w, i.e. a feed-forward
neural network architecture over the input space,
representing the basis functions. Let Œ≤ be a vector
of weights on the basis functions, so f (s) = Œ≤(cid:62)Œ¶(s).
We use a VAE architecture to encode and decode
Œ≤, meaning we maintain the random variable per-
spective and at the same time learn a Ô¨Çexible
low-dimensional non-linear generative model.

How can we specify and train this model? As
with the VAE in the previous section, œÄVAE is
trained on draws from a prior. Our goal is to encode
a stochastic process prior Œ†, so we consider i =
1, . . . , N function realizations denoted fi(s). Each
fi(s) is an inÔ¨Ånite dimensional object, a function
deÔ¨Åned for all s, so we further assume that we
are given a Ô¨Ånite set of Ki observation locations.
We set Ki = K for simplicity of implementation
i.e. the number of evaluations for each function is
constant across all draws i. We denote the observed
values as yk
i ). The training dataset thus
consists of N sets of K observation locations and
function values:

i := fi(sk

{(s1

i , y1

i ) . . . , (sK

i , yK

i )}N
i=1

Note that the set of K observation locations varies
across the N realizations.

We now return to the architecture of œÄVAE
(Fig. 2). The feature mapping Œ¶(s) is shared across
all i = 1, . . . , N function draws, so it consists of a
feedforward neural network and is parameterized
by a set of global parameters w which must be
learned. However, a particular random realization
fi(s) is represented by a random vector Œ≤i, for
which we use a VAE architecture. We note the
following non-standard setup: Œ≤i is a learnable

0.00.20.40.60.81.0210120.00.20.40.60.81.01012TruePosterior meanObservations95% Credible Interval6

œÄVAE

Springer Nature 2021 LATEX template

parameter of our model, but it is also the input to
the encoder of the VAE. The decoder attempts to
reconstruct Œ≤i with an output ÀÜŒ≤i. We denote the
encoder and decoder as:

[z¬µ, zsd](cid:62) = e(Œ≤, Œ≥)

Z ‚àº N (z¬µ, z2
sd
ÀÜŒ≤ = d(Z, œà)

I)

(9)

(10)

(11)

We are now ready to express the loss, which
combines the two parts of the network, summing
across all observations. Rather than deriving an
evidence lower bound, we proceed directly to spec-
ify a loss function, in three parts. In the Ô¨Årst, we
use MSE to check the Ô¨Åt of the Œ≤i‚Äôs and Œ¶ to the
data:

Loss 1 :

1
N K

(cid:88)

i,k

(yk

i ‚àí Œ≤(cid:62)

i Œ¶(sk

i ))2

In the second, we use MSE to check the Ô¨Åt of the
reconstructed ÀÜŒ≤i‚Äôs and Œ¶ to the data:

Loss 2 :

1
N K

(cid:88)

i,k

(yk

i ‚àí ÀÜŒ≤(cid:62)

i Œ¶(sk

i ))2

existing stochastic processes: we can obtain a Gaus-
sian process with kernel k(¬∑, ¬∑) using a single-layer
linear VAE for Œ≤ (meaning the Œ≤s are simply stan-
dard normals) and setting Œ¶(s) = L(cid:62)s for L the
Cholesky decomposition of the Gram matrix K
where Kij = k(si, sj).

In contrast to a standard VAE encoder that
takes as input the data to be encoded, œÄVAE
Ô¨Årst transforms input data (locations) to a higher
dimensional feature space via Œ¶, and then con-
nects this feature space to outputs, y, through a
linear mapping, Œ≤. The œÄVAE decoder takes out-
puts from the encoder, and attempts to recreate
Œ≤ from a lower dimensional probabilistic embed-
ding. This re-creation, ÀÜŒ≤, is then used as a linear
mapping with the same Œ¶ to get a reconstruction
of the outputs y. It is crucial to note that a sin-
gle global Œ≤ vector is not learnt. Instead, for each
function i = 1, . . . , N a Œ≤i is learnt.

In terms of number of parameters, we need to
learn w, Œ≥, œà, Œ≤1, . . . , Œ≤N . While this may seem like
a huge computational task, K is typically quite
small (< 200) and so learning can be relatively
quick (dominated by matrix multiplication of hid-
den layers). Algorithm 1 in the Appendix presents
the step-by-step process of training œÄVAE.

We also require the standard variational loss:

2.3.1 Simulation and Inference with

KL (Z (cid:107) N (0, I))

Note that we do not consider reconstruction loss
(cid:107)Œ≤i ‚àí ÀÜŒ≤i(cid:107)2 because in practice this did not improve
training.

To provide more intuition: the feature map Œ¶(s)
transforms each observed location to a Ô¨Åxed fea-
ture space that is shared for all locations across
all functions. Œ¶(s) could be an explicit feature
representation for an RKHS (e.g. an RBF net-
work or a random Fourier feature basis (Rahimi
& Recht, 2008)), a neural network of arbitrary
construction or, as we use in the examples in
this paper, a combination of both. Following this
transformation, a linear basis Œ≤ (which we obtain
from a non-linear decoder network) is used to pre-
dict function evaluations at an arbitrary location.
The intuition behind these two transformations
is to learn the association between locations and
observations while allowing for randomness‚ÄîŒ¶
provides the correlation structure over space and
Œ≤ the randomness. Explicit choices can lead to

œÄVAE

Given a trained embedding Œ¶(¬∑) and trained
decoder d(z), we can use œÄVAE as a generative
model to simulate sample paths f as follows. A
single function f is obtained by Ô¨Årst drawing
Z ‚àº N (0, I) and deÔ¨Åning f (s) := d(Z)(cid:62)Œ¶(s). For
a Ô¨Åxed Z, f (s) is a deterministic function‚Äîa sam-
ple path from œÄVAE deÔ¨Åned for all s. Varying Z
produces diÔ¨Äerent sample paths. Computationally,
f can be eÔ¨Éciently evaluated at any arbitrary loca-
tion s using matrix algebra: f (s) = d(Z)(cid:62)Œ¶(s).
We remark that the stochastic process perspec-
tive is readily apparent: for a random variable Z,
d(Z)(cid:62)Œ¶(s) is a random variable deÔ¨Åned on the
same probability space for all s.

Algorithm 3 in the Appendix presents the step-

by-step process for simulation with œÄVAE.

œÄVAE can be used for inference on new data
pairs (sj, yj), where the unnormalised posterior
distribution is

p(Z | d, yj, sj, Œ¶) ‚àù p(yj | d, sj, Z, Œ¶)p(Z)

(12)

Springer Nature 2021 LATEX template

œÄVAE

7

Fig. 2: Schematic description of end-to-end trainig procedure for œÄVAE including the reconstruction loss.
Dashed arrows contribute to the loss, blue circles are reconstructions, and grey boxes are functions.

with likelihood p(yj | d, sj, Z, Œ¶) and prior p(Z).
MCMC can be used to eÔ¨Éciently obtain sam-
ples from the posterior distribution over Z using
Equation (12). An implementation in probabilistic
programming languages such as Stan (Carpenter
et al., 2017) is very straightforward.

The posterior predictive distribution of yj at a

location sj is given by:

p(yj | d, sj, Œ¶) =
(cid:90)

p(yj | d, sj, Œ¶, Z)p(Z | d, yj, sj, Œ¶)dZ

(13)

While equations Eqs. (12)-(13) are written for
a single location sj, we can extend them to any
arbitrary collection of locations without loss of
generality, a necessary condition for œÄVAE to be a
valid stochastic process. Further, the distinguishing
diÔ¨Äerence between Eq. (6) and Eqs. (12)-(13) is
conditioning on input locations and Œ¶. It is Œ¶ that
ensures œÄVAE is a valid stochastic process. We
formally prove this below.

Algorithm 2 in the Appendix presents the step-

by-step process for inference with œÄVAE.

2.3.2 œÄVAE is a stochastic process

Claim. œÄVAE is a stochastic process.
Recall that, mathematically, a stochastic pro-
cess is deÔ¨Åned as a collection {f (s) : s ‚àà S}, where
f (s) for each location s ‚àà S is a random vari-
able on a common probability space (‚Ñ¶, F, P ), see,
e.g., Pavliotis (2014, DeÔ¨Ånition 1.1). This tech-
nical requirement is necessary to ensure that for
any locations s1, . . . , sn ‚àà S, the random variables
f (s1), . . . , f (sn) have a well-deÔ¨Åned joint distri-
bution. Subsequently, it also ensures consistency.
Namely, writing fi := f (si) and integrating fn out,
we get

p(f1, . . . , fn‚àí1) =

(cid:90)

fn

p(f1, . . . , fn)dfn.

Proof. For œÄVAE, we have f (¬∑) := d(Z)Œ¶(¬∑),
where Z is a multivariate Gaussian random vari-
able, hence deÔ¨Åned on some probability space
(‚Ñ¶, F, P ). Since d and Œ¶ are deterministic (measur-
able) functions, it follows that f (si) := d(Z)Œ¶(si)
for any i = 1, . . . , n, is a random variable on
(‚Ñ¶, F, P ), whereby {f (s) : s ‚àà S} is a stochastic
process. (cid:4)

We remark here that œÄVAE is a new stochastic
process. If œÄVAE is trained on samples from a zero

Training œÄVAE{fi}Ni=1(ski)Kk=1EncoderÃÇŒ≤iŒ≤ie(‚ãÖ,Œ≥)ŒºzœÉzd(‚ãÖ,œà)DecoderEmbeddingœÄVAE Loss = Loss 1  + Loss 2  + Regularisation LossLoss 1=[yki‚àíŒ≤TiŒ¶(ski)]2:MSE from approximating         by ykiŒ≤TiŒ¶(ski)Loss 2=yki‚àíÃÇŒ≤TiŒ¶(ski)]2:MSE from approximating         by ÃÇŒ≤TiŒ¶(ski)ykiRegLoss=DKL(N(Œºz,œÉz)||N(0,ùïÄ))Reconstructed yki‚âàÃÇŒ≤TiŒ¶(ski)yki‚âàŒ≤TiŒ¶(ski)K locationsFunction  evaluationsyki=fi(ski)N functions]Œ¶()ski‚ãÖTraining data8

œÄVAE

Springer Nature 2021 LATEX template

mean Gaussian process with a squared exponen-
tial covariance function, and similarly choose Œ¶ to
have the same covariance function, and d is linear,
then œÄVAE will be a Gaussian process. But for a
non-positive deÔ¨Ånite Œ¶ and / or non-linear d, even
if œÄVAE is trained on samples from a Gaussian pro-
cess, it will not truly be a Gaussian process, but
some other stochastic process which approximates
a Gaussian process. We do not know the theoret-
ical conditions under which œÄVAE will perform
better or worse than existing classes of stochastic
processes; its general construction means that the-
oretical results will be challenging to prove in full
generality. We demonstrate below that in practice,
œÄVAE performs very well.

2.4 Examples

We Ô¨Årst demonstrate the utility of our proposed
œÄVAE model by Ô¨Åtting the simulated 1-D regres-
sion problem introduced in (Hern√°ndez-Lobato &
Adams, 2015). The training points for the dataset
are created by uniform sampling of 20 inputs, x,
between (‚àí4, 4). The corresponding output is set
as y ‚àº N (x3, 9). We Ô¨Åt two diÔ¨Äerent variants of
œÄVAE, representing two diÔ¨Äerent prior classes of
functions. The Ô¨Årst prior produces cubic monotonic
functions and the second prior is a GP with an
RBF kernel and a two layer neural network. We
generated 104 diÔ¨Äerent function draws from both
priors to train the respective œÄVAE. One important
consideration in œÄVAE is to chose a suÔ¨Éciently
expressive Œ¶, we used a RBF layer (see Appendix
D) with trainable centres coupled with two layer
neural network with 20 hidden units each. We com-
pare our results against 20,000 Hamiltonian Monte
Carlo (HMC) samples (R.M. Neal, 1993) imple-
mented using Stan (Carpenter et al., 2017). Details
of the implementation for all the models can be
found in the Appendix.

Figure 3(a) presents results for œÄVAE with a
cubic prior, Figure 3(b) with an RBF prior, and
Figure 3(c) for standard Gaussian processes Ô¨Åt-
ting using an RBF kernel. The mean absolute
error (MAE) for all three methods are presented
in Table 1. Both, the mean estimates and the
uncertainty from œÄVAE variants, are closer, and
more constrained than the ones using Gaussian pro-
cesses with HMC. Importantly, œÄVAE with cubic
prior not only produces better point estimates
but is able to capture better uncertainty bounds.

Method

Test MAE

œÄVAE (cubic functions)
œÄVAE (Gaussian process with RBF kernel)
Gaussian process with RBF kernel

10.47
33.15
67.37

Table 1: Test results of Ô¨Åtting to a cubic function
with noise y ‚àº N (x3, 9).

We note that œÄVAE does not exactly replicate an
RBF Gaussian process, but does retain the main
qualitative features inherent to GPs - such as the
concentration of the posterior where there is data.
Despite œÄVAE ostensibly learning an RBF function
class, diÔ¨Äerences are to be expected from the VAE
low dimensional embedding. This simple example
demonstrates that œÄVAE can be used to incorpo-
rate domain knowledge about the functions being
modelled.

In many scenarios, learning just the mapping
of inputs to outputs is not suÔ¨Écient as other func-
tional properties are required to perform useful
(interesting) analysis. For example, using point pro-
cesses requires knowing the underlying intensity
function, however, to perform inference we need to
calculate the integral of that intensity function too.
Calculating this integral, even in known analytical
form, is very expensive. Hence, in order to circum-
vent the issue, we use œÄVAE to learn both function
values and its integral for the observed events.
Figure 4 shows œÄVAE prediction for both the inten-
sity and integral of a simulated 1-D log-Gaussian
Cox Process (LGCP).

In order to train œÄVAE to learn from the func-
tion space of 1-D LGCP functions, we Ô¨Årst create a
training set by drawing 10,000 diÔ¨Äerent samples of
the intensity function using an RBF kernel for 1-D
LGCP. For each of the drawn intensity function,
we choose an appropriate time horizon to sample
80 observed events (locations) from the intensity
function. œÄVAE is trained on the sampled 80 loca-
tions with their corresponding intensity and the
integral. œÄVAE therefore outputs both the instan-
taneous intensity and the integral of the intensity.
The implementation details can be seen in the
Appendix. For testing, we Ô¨Årst draw a new intensity
function (1-D LGCP) using the same mechanism
used in training and sample 100 events (locations).
As seen in Figure 4 our estimated intensity is very
close to true intensity and even the estimated inte-
gral is close to the true integral. This example

Springer Nature 2021 LATEX template

œÄVAE

9

(a)

(b)

(c)

Fig. 3: Fitting to a cubic function with noise y ‚àº N (x3, 9). (a) œÄVAE trained on a class of cubic functions,
(b) œÄVAE trained on samples from a Gaussian process with RBF kernel and (c) is a Gaussian process
with RBF kernel. All methods use Hamiltonian Markov Chain Monte Carlo for posterior inference.

(a)

(b)

Fig. 4: Inferring the intensity of a log-Gaussian Cox Process. (a) compares the posterior distribution of
the intensity estimated by œÄVAE to the true intensity function on train and test data. (b) compares the
posterior mean of the cumulative integral over time estimated by œÄVAE to the true cumulative integral
on train and test data.

shows that the œÄVAE approach can be used to
learn not only function evaluations but properties
of functions.

3 Results

Here we show applications of œÄVAE on three real
world datasets. In our Ô¨Årst example we use œÄVAE
to predict the deviation in land surface tempera-
ture in East Africa (Ton et al., 2018). We have the
deviation in land surface temperatures for ‚àº89,000
locations across East Africa. Our training data
consisted of 6,000 uniformly sampled locations.
Temperature was predicted using only the spatial
locations as inputs. Figure 5 and Table 2 shows
the results of the ground truth (a), our œÄVAE (b),
a full rank Gaussian process with Mat√©rn kernel
(Gardner, Pleiss, Bindel, Weinberger, & Wilson,

2018) (c), and low rank Gauss Markov random
Ô¨Åeld (GMRF) (a widely used approach in the Ô¨Åeld
of geostatistics) with 1, 046 ( 1
6 th of the training
size) basis functions (Lindgren et al., 2011; Rue,
Martino, & Chopin, 2009) (d). We train our œÄVAE
model on 107 functions draws from 2-D GP with
small lengthscales between 10‚àí5 to 2. Œ¶ was set to
be a Mat√©rn layer ( see Appendix D) with 1,000
centres followed by a two layer neural network of
100 hidden units in each layer. The latent dimen-
sion of œÄVAE was set to 20. As seen in Figure 5,
œÄVAE is able to capture small scale features and
produces a far better reconstruction than the both
full and low rank GP and despite having a much
smaller latent dimension of 20 vs 6,000 (full) vs
1,046 (low). The testing error for œÄVAE is substan-
tially better than the full rank GP which leads to
the question, why does œÄVAE perform so much

64202462001000100200300TruePostreior meanObservations95% Credible Interval64202462001000100200642024620010001002000102030405060Time Points050100150200250300IntensityTrue IntensityEstimated IntensityObserved LocationsTrain DataTest Data0102030405060Time Points0200040006000800010000Integral IntensityIntegral True IntensityIntegral Estimated IntensityTrain DataTest Data10

œÄVAE

Springer Nature 2021 LATEX template

(a)

(b)

(c)

(d)

Fig. 5: Deviation in land surface temperature for East Africa trained on 6000 random uniformly chosen
locations (Ton et al., 2018). Plots: (a) the data, (b) our œÄVAE approach (testing MSE: 0.38), (c) a full
rank GP with Mat√©rn 3
2 kernel (testing MSE: 2.47), and (d) a low rank SPDE approximation with 1046
basis functions (Lindgren et al., 2011) and a Mat√©rn 3
2 kernel (testing MSE: 4.36). œÄVAE not only has
substantially lower test error, it captures Ô¨Åne scale features much better than Gaussian processes or neural
processes.

Method

Test MSE

Method

RMSE

NLL

Full rank GP
œÄVAE
low rank GMRF (basis = 1046)

2.47
0.38
4.36

Table 2: Test results for œÄVAE, a full rank GP,
and low rank GMRF on land surface temperature
for East Africa trained on 6000 random uniformly
chosen locations (Ton et al., 2018).

Full rank GP
œÄVAE
SGPR (m = 512)
SVGP (m = 1024)

0.099
0.112
0.273
0.268

-0.258
0.006
0.087
0.236

Table 3: Test results for œÄVAE, a full rank GP
and approximate algorithms SGPR and SVGP on
Kin40K.

better than a GP, despite being trained on samples
from a GP? One possible reason is that the extra
hidden layers in Œ¶ create a much richer structure
that could capture elements of non-stationarity
(Ton et al., 2018). Alternatively, the ability to use
state-of-the-art MCMC and estimate a reliable
posterior expectation might create resilience to
overÔ¨Åtting. The training/testing error for œÄVAE
is 0.07/0.38, while the full rank GP is 0.002/2.47.
Therefore the training error is 37 times smaller in
the GP, but the testing error is only 6 times smaller
in œÄVAE suggesting that, despite marginalisation,
the GP is still overÔ¨Åtting.
3

the
Kin40K (Schwaighofer & Tresp, 2003) dataset
to state-of-the-art full and approximate GPs,
with results taken from (Wang et al., 2019). The
objective was to predict the distance of a robotic
arm from the target given the position of all 8
links present on the robotic arm. In total we have
40,000 samples which are divided randomly into

compares

œÄVAE

Table

on

2
3 training samples and 1
3 test samples. We train
œÄVAE on N = 107 functions drawn from an 8-D
GP, observed at K = 200 locations, where each
of the 8 dimensions had values drawn uniformly
from the range (‚àí2, 2) and lengthscale varied
between 10‚àí3 and 10. Once œÄVAE was trained on
the prior function we use it to infer the posterior
distribution for the training examples in Kin40K.
Table 3 shows results for RMSE and negative
log-likelihood (NLL) of œÄVAE against various GP
methods on test samples. The full rank GP results
reported in (Wang et al., 2019) are better than
those from œÄVAE, but we are competitive, and far
better than the approximate GP methods. We
also note that the exact GP is estimated via max-
imising the log marginal likelihood in closed form,
while œÄVAE performs full Bayesian inference; all
posterior checks yielded excellent convergence
measured via ÀÜR and eÔ¨Äective samples sizes. Cal-
ibration was checked using posterior predictive
intervals. For visual diagnostics see the Appendix.

Springer Nature 2021 LATEX template

œÄVAE

11

10% pixels

20% pixels

30% pixels

(a) Observation

(b) Observation

(c) Observation

(d) Sample 1

(e) Sample 1

(f) Sample 1

(g) Sample 2

(h) Sample 2

(i) Sample 2

(j) Sample 3

(k) Sample 3

(l) Sample 3

Fig. 6: MNIST reconstruction after observing only 10, 20 or 30% of pixels from original data.

Finally, we apply œÄVAE to the task of recon-
structing MNIST digits using a subset of pixels
from each image. Similar to the earlier tempera-
ture prediction task, image completion can also be
seen as a regression task in 2-D. The regression
task is to predict the intensity of pixels given the
pixel locations. We Ô¨Årst train neural processes on
full MNIST digits from the training split of the
dataset, whereas œÄVAE is trained on N = 106 func-
tions drawn from a 2-D GP. The latent dimension

of œÄVAE is set to be 40. As with previous examples,
the decoder and encoder networks are made up of
two layer neural networks. The hidden units for the
encoder are 256 and 128 for the Ô¨Årst and second
layer respectively, and the reverse for decoder.

Once we have trained œÄVAE we now use images
from the test set for prediction. Images in the
testing set are sampled in such a way that only 10,
20 or 30% of pixel values are observed. Inference
is performed with œÄVAE to predict the intensity

12

œÄVAE

Springer Nature 2021 LATEX template

at all other pixel locations using Eq. (13). As seen
from Figure 6, the performance of œÄVAE increases
with increase in pixel locations available during
prediction but still even with 10% pixels our model
is able to learn a decent approximation of the image.
The uncertainty in prediction can be seen from
the diÔ¨Äerent samples produced by the model for
the same data. As the number of given locations
increases, the variance between samples decreases
with quality of the image also increasing. Note that
results from neural processes, as seen in Figure C4,
look better than from œÄVAE. Neural processes
performed better in the MNIST case because they
were speciÔ¨Åcally trained on full MNIST digits from
the training dataset, whereas piVAE was trained
on the more general prior class of 2D GPs.

4 Discussion and Conclusion

In this paper we have proposed a novel VAE for-
mulation of a stochastic process, with the ability
to learn function classes and properties of func-
tions. Our œÄVAEs typically have a small (5-50) ,
uncorrelated latent dimension of parameters, so
Bayesian inference with MCMC is straightforward
and highly eÔ¨Äective at successfully exploring the
posterior distribution. This accurate estimation
of uncertainty is essential in many areas such as
medical decision-making.

œÄVAE combines the power of deep learning
to create high capacity function classes, while
ensuring tractable inference using fully Bayesian
MCMC approaches. Our 1-D example in Figure
3 demonstrates that an exciting use of œÄVAE is
to incorporate domain knowledge about the prob-
lem. Monotonicity or complicated dynamics can be
encoded directly into the prior (Caterini, Doucet,
& Sejdinovic, 2018) on which œÄVAE is trained.
Our log-Gaussian Cox Process example shows that
not only functions can be modelled, but also prop-
erties of functions such as integrals. Perhaps the
most surprising result is the performance of œÄVAE
on spatial interpolation. Despite being trained on
samples from a Gaussian process, œÄVAE substan-
tially outperforms a full rank GP. We conjecture
this is due to the more complex structure of the
feature representation Œ¶ and due to a resilience to
overÔ¨Åtting.

There are costs to using œÄVAE, especially the
large upfront cost in training. For complex priors,

training could take days or weeks and will invari-
ably require the heuristics and parameter searches
inherent in applied deep learning to achieve a good
performance. However, once trained, a œÄVAE net-
work is applicable on a wide range of problems,
with the Bayesian inference MCMC step taking
seconds or minutes.

Future work should investigate the performance
of œÄVAE on higher dimensional settings (input
spaces > 10). Other stochastic processes, such as
Dirichlet processes, should also be considered.

Declarations

Funding

SB acknowledge the The Novo Nordisk Young
Investigator Award (NNF20OC0059309) which also
supports SM. SB also acknowledges the Danish
National Research Foundation Chair grant, The
Schmidt Polymath Award and The NIHR Health
Protection Research Unit (HPRU) in Modelling
and Health Economics. SM and SB acknowledge
funding from the MRC Centre for Global Infec-
tious Disease Analysis (reference MR/R015600/1)
and Community Jameel. SF acknowledges the
EPSRC (EP/V002910/2) and the Imperial College
COVID-19 Research Fund.

ConÔ¨Çict of interest/Competing
interests

NA

Ethics approval

NA

Consent to participate

NA

Consent for publication

NA

Availability of data and materials

All data used in the paper is available at https://
github.com/MLGlobalHealth/pi-vae.

Springer Nature 2021 LATEX template

œÄVAE

13

Code availability

is

Code
MLGlobalHealth/pi-vae.

available

at

https://github.com/

References

Antoniak, C.E.

(1974). Mixtures of dirich-
let processes with applications to bayesian
nonparametric problems. The annals of
statistics, 1152‚Äì1174.

Betancourt, M., Byrne, S., Livingstone, S., Giro-
lami, M. (2017). The geometric foundations
of Hamiltonian Monte Carlo. Bernoulli .

10.3150/16-BEJ810

Blundell, C., Cornebise, J., Kavukcuoglu, K., Wier-
stra, D. (2015). Weight uncertainty in neural
networks. 32nd international conference on
machine learning, icml 2015.

Garnelo, M., Rosenbaum, D., Maddison, C.J.,
Ramalho, T., Saxton, D., Shanahan, M., . . .
Eslami, S.M.
(2018). Conditional neural
processes. 35th international conference on
machine learning, icml 2018.

Hawkes, A.G.

(1971).

Spectra of some self-
exciting and mutually exciting point pro-
cesses. Biometrika, 58 (1), 83‚Äì90.

10.1093/biomet/58.1.83

Hern√°ndez-Lobato, J.M., & Adams, R.

(2015).
Probabilistic backpropagation for scalable
learning of bayesian neural networks. Inter-
national conference on machine learning (pp.
1861‚Äì1869).

Hinton, G.E., & Salakhutdinov, R.R.

(2006).
Reducing the dimensionality of data with
neural networks. science, 313 (5786), 504‚Äì
507.

Broomhead, D.S., & Lowe, D.

(1988, March).
Radial Basis Functions, Multi-Variable
Functional
and Adap-
Interpolation
tive Networks. DTIC . Retrieved from
https://apps.dtic.mil/sti/citations/ADA196234

HoÔ¨Äman, M.D., Blei, D.M., Wang, C., Paisley,
J. (2013). Stochastic variational inference.
The Journal of Machine Learning Research,
14 (1), 1303‚Äì1347.

Carpenter, B., Gelman, A., HoÔ¨Äman, M.D., Lee, D.,
Goodrich, B., Betancourt, M., . . . Riddell, A.
(2017). Stan : A Probabilistic Programming
Language. Journal of Statistical Software,
76 (1), 1‚Äì32.

10.18637/jss.v076.i01

Caterini, A.L., Doucet, A., Sejdinovic, D.
(2018). Hamiltonian variational auto-encoder.
Advances in neural information processing
systems.

Gardner, J.R., Pleiss, G., Bindel, D., Weinberger,
K.Q., Wilson, A.G.
(2018). Gpytorch:
Blackbox matrix-matrix Gaussian process
inference with GPU acceleration. Advances
in neural information processing systems.

Huggins, J.H., Kasprzak, M., Campbell, T., Brod-
erick, T. (2019). Practical posterior error
bounds from variational objectives. arXiv
preprint arXiv:1910.04102 .

Jacot, A., Gabriel, F., Hongler, C. (2018). Neural
tangent kernel: Convergence and generaliza-
tion in neural networks. Advances in neural
information processing systems.

Karhunen, K.

(1947). On linear methods in
probability theory. Annales academiae scien-
tiarum fennicae, ser. al (Vol. 37, pp. 3‚Äì79).

Kim, H., Mnih, A., Schwarz, J., Garnelo, M.,
Eslami, S.M.A., Rosenbaum, D., . . . Teh,
Y.W. (2019). Attentive Neural Processes.
CoRR, abs/1901.0 .

14

œÄVAE

Springer Nature 2021 LATEX template

Kingma, D.P., & Ba, J.

(2014, 12). Adam: A
Method for Stochastic Optimization. arXiv
preprint arXiv:1412.6980 .

https://doi.org/10.1145/2983323.2983812
10.1145/2983323.2983812

Kingma, D.P., & Welling, M.

(2014). Auto-
Encoding Variational Bayes (VAE, reparam-
eterization trick). ICLR 2014 .

M√∏ller, J., Syversveen, A.R., Waagepetersen, R.P.
(1998). Log gaussian cox processes. Scandi-
navian Journal of Statistics, 25 (3), 451-482.
https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-
9469.00115

10.1111/1467-9469.00115

Kingma, D.P., & Welling, M. (2019). An intro-
duction to variational autoencoders. Foun-
dations and Trends¬Æ in Machine Learning,
12 (4), 307‚Äì392.

Neal, R.

(1996). Bayesian Learning for Neu-
LECTURE NOTES IN
ral Networks.
STATISTICS -NEW YORK- SPRINGER
VERLAG-.

Lakshminarayanan, B., Pritzel, A., Blundell, C.
(2017).
Simple and scalable predictive
uncertainty estimation using deep ensembles.
Advances in neural information processing
systems.

Lindgren, F., Rue, H., Lindstr√∂m, J. (2011). An
explicit link between Gaussian Ô¨Åelds and
Gaussian Markov random Ô¨Åelds: the stochas-
tic partial diÔ¨Äerential equation approach.
Journal of
the Royal Statistical Society:
Series B (Statistical Methodology), 73 (4),
423‚Äì498.

10.1111/j.1467-9868.2011.00777.x

Loeve, M. (1948). Functions aleatoires du second
ordre. Processus stochastique et mouvement
Brownien, 366‚Äì420.

Minka, T.P. (2001). Expectation propagation for
approximate bayesian inference. Proceedings
of the seventeenth conference on uncertainty
in artiÔ¨Åcial intelligence (p. 362‚Äì369). San
Francisco, CA, USA: Morgan Kaufmann
Publishers Inc.

Mishra, S., Rizoiu, M.-A., Xie, L. (2016). Fea-
ture driven and point process approaches
Proceed-
for popularity prediction.
the 25th acm international on
ings of
knowl-
conference
edge management (p. 1069‚Äì1078). New
York, NY, USA: Association for Com-
Retrieved from
puting Machinery.

on information and

Neal, R.M. (1993). Probabilistic inference using
markov chain monte carlo methods. Depart-
ment of Computer Science, University of
Toronto Toronto, Ontario, Canada.

Park, J., & Sandberg, I.W. (1991, June). Universal
Approximation Using Radial-Basis-Function
Networks. Neural Comput., 3 (2), 246‚Äì257.

10.1162/neco.1991.3.2.246

Paszke, A., Gross, S., Massa, F., Lerer, A.,
Bradbury, J., Chanan, G.,
others
(2019). Pytorch: An imperative style, high-
performance deep learning library. Advances
information processing systems
in neural
(pp. 8024‚Äì8035).

. . .

Pavliotis, G.A. (2014). Stochastic processes and
applications: diÔ¨Äusion processes, the fokker-
planck and langevin equations (Vol. 60).
Springer.

Rahimi, A., & Recht, B. (2008). Random features
for large-scale kernel machines. Advances in
neural information processing systems (pp.
1177‚Äì1184).

Rasmussen, C.E., & Williams, C.K.I.

Gaussian processes
ing.
http://www.worldcat.org/oclc/61285753

for machine

MIT Press.

(2006).
learn-
Retrieved from

Rezende, D.J., Mohamed, S., Wierstra, D. (2014).
Stochastic backpropagation and approximate

systems (pp. 14622‚Äì14632).

Welling, M., & Teh, Y.W.

(2011). Bayesian
learning via stochastic gradient langevin
dynamics. Proceedings of the 28th interna-
tional conference on machine learning, icml
2011.

Yao, J., Pan, W., Ghosh, S., Doshi-Velez, F.
(2019). Quality of uncertainty quantiÔ¨Åcation
for bayesian neural network inference. arXiv
preprint arXiv:1906.09686 .

Yao, Y., Vehtari, A., Simpson, D., Gelman, A.
(2018). Yes, but did it work?: Evaluat-
ing variational inference. 35th international
conference on machine learning, icml 2018.

Springer Nature 2021 LATEX template

œÄVAE

15

inference in deep generative models. Interna-
tional conference on machine learning (pp.
1278‚Äì1286).

Ritter, H., Botev, A., Barber, D. (2018). A scalable
laplace approximation for neural networks.
6th international conference on learning rep-
resentations,
iclr 2018 - conference track
proceedings.

Ross, S.M. (1996). Stochastic processes (Vol. 2).

John Wiley & Sons.

Roy, D.M., & Teh, Y.W. (2009). The Mondrian
process. Advances in neural information pro-
cessing systems 21 - proceedings of the 2008
conference.

Rue, H., Martino, S., Chopin, N.

(2009, 4).
Approximate Bayesian inference for latent
Gaussian models by using integrated nested
Journal of the
Laplace approximations.
Royal Statistical Society. Series B: Statisti-
cal Methodology, 71 (2), 319‚Äì392.

10.1111/j.1467-9868.2008.00700.x

Schwaighofer, A., & Tresp, V. (2003). Transduc-
tive and inductive methods for approximate
gaussian process regression. Advances in
neural information processing systems (pp.
977‚Äì984).

Semenova, E., Xu, Y., Howes, A., Rashid, T.,
Bhatt, S., Mishra, S., Flaxman, S. (2022).
Priorvae: encoding spatial priors with vari-
ational autoencoders for small-area estima-
tion. Journal of the Royal Society Interface,
19 (191), 20220094.

Ton, J.-F., Flaxman, S., Sejdinovic, D., Bhatt, S.
(2018). Spatial mapping with Gaussian pro-
cesses and nonstationary Fourier features.
Spatial Statistics.

10.1016/j.spasta.2018.02.002

Wang, K., Pleiss, G., Gardner, J., Tyree, S., Wein-
berger, K.Q., Wilson, A.G. (2019). Exact
gaussian processes on a million data points.
Advances in neural information processing

Springer Nature 2021 LATEX template

16

œÄVAE

Appendix A MCMC diagnostics

Fig. A1: MCMC diagnostics for VAE inference presented in Figure 1: (a) and (b) shows the values for ÀÜR

Nef f
N

and
the draws from the posterior predictive distribution.

for all parameters inferred with Stan. (c) shows the true distribution of observations along with

Figure A1 presents the MCMC diagnostics for the 1-D GP function learning example shown in Figure 1.
Both ÀÜR and eÔ¨Äective sample size for all the inferred parameters (latent dimension of the VAE and noise in
the observation) are well behaved with ÀÜR ‚â§ 1.01 (Figure A1(a)) and eÔ¨Äective sample size greater than 1
(Figure A1(b)). Furthermore, even the draws from the posterior predictive distribution very well capture
the true distribution in observations as shown in Figure A1(c).

Springer Nature 2021 LATEX template

œÄVAE

17

Fig. A2: MCMC diagnostics for œÄVAE inference presented in Table 3: (a) and (b) shows the values for ÀÜR

Nef f
N

and
the draws from the posterior predictive distribution.

for all parameters inferred with Stan. (c) shows the true distribution of observations along with

Figure A2 presents the MCMC diagnostics for the kin40K dataset with œÄVAE as shown in Table 3.
Both ÀÜR and eÔ¨Äective sample size for all the inferred parameters (latent dimension of the VAE and noise in
the observation) are well behaved with ÀÜR ‚â§ 1.01 (Figure A2(a)) and eÔ¨Äective sample size greater than 0.5
(Figure A2(b)). Furthermore, the draws from the posterior predictive distribution are shown against the
true distribution in observations as shown in Figure A2(c).

Figure A3 presents the MCMC calibration plots for the posterior. Both the marginal predictive check

and leave one out predictive intervals plots demonstrate that our posterior is well calibrated.

Springer Nature 2021 LATEX template

18

œÄVAE

Fig. A3: MCMC calibration for œÄVAE inference presented in Table 3: (a) shows the marginal predictive
check using a leave one out probability integral transform and (b) shows the leave one out predictive
intervals compared to observations.

0.000.250.500.751.000.000.250.500.751.00UniformLOO‚àíPIT‚àí3‚àí2‚àí10120255075100Data point (index)yrepySpringer Nature 2021 LATEX template

œÄVAE

19

Appendix B Algorithm

œÄVAE proceeds in two stages. In the Ô¨Årst stage (Algorithm 1) we train œÄVAE, using a very large set of
draws from a pre-speciÔ¨Åed prior class. In the second stage (Algorithm 2) we use the trained œÄVAE from
Algorithm 1 as a prior, combine this with data using a likelihood, and perform inference. MCMC for
Bayesian inference or optimization with a loss function are alternative approaches to learn the best Z
to explain the data. While these two algorithms are all that is needed to apply œÄVAE, for completeness
Algorithm 3 shows how one can use a trained œÄVAE, which encodes a stochastic process, to sample
realisations from this stochastic process.

Algorithm 1 Prior Training for œÄVAE (stage 1)

1: Simulate draws from N functions evaluated at K points to create input data consisting of location,

function value pairs: {(s1

i , y1

i ), . . . , (sK

i , yK

i )}N

i=1

2: repeat
3:

for each function i = 1, . . . , N do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

for each location k = 1, . . . , K do
transform locations: Œ¶(sk
i )
inner product with a linear basis:
i Œ¶(sk
i )

i,1 ‚Üê Œ≤T
ÀÜyk
end for
append loss1: loss1 ‚Üê M SE(ÀÜyi,1, yi)
encode Œ≤i with VAE:

I)

[z¬µ, zsd](cid:62) = e(Œ≤i, Œ≥)
reparameterize for Z: Z ‚àº N (z¬µ, z2
sd
decode with VAE, ÀÜŒ≤i : ÀÜŒ≤i = d(Z, œà)
for each location k = 1, . . . , K do
transform locations: Œ¶(sk
i )
inner product with decoded ÀÜŒ≤i:
i Œ¶(sk
i )

i,2 = ÀÜŒ≤(cid:62)
ÀÜyk
end for
append loss2: loss2 ‚Üê M SE(ÀÜyi,2, yi)
minimize loss1 + loss2 + KL (Z(cid:107)N (0, I))

to get Œ¶, Œ≤i, Œ≥, œà

end for

18:
19: until termination criterion satisÔ¨Åed (epochs)

Springer Nature 2021 LATEX template

20

œÄVAE

Algorithm 2 Inference from œÄVAE (stage 2)

Require: Trained decoder d (œà Ô¨Åxed) and Œ¶ (learnt from Algorithm 1)
1: Input: J observations consist of location, function value pairs: {(sj, yj)}J
2: Goal: infer latent function with parameters Z.
3: Sample Z: Z ‚àº N (0, I)
4: decode with VAE to get Œ≤ := d(Z, œà)
5: for each location j do
6:

transform locations: Œ¶(sj)
inner product with decoder Œ≤: ÀÜyj ‚Üê Œ≤T Œ¶(sj)

j=1.

7:
8: end for
9: Perform Bayesian inference with MCMC for Z to obtain a set of draws from the posterior distribution:

p(Z | d, y1, s1, . . . , yJ , sJ , Œ¶) ‚àù p(y1, . . . , yJ | d, s1, . . . , sJ , Z, Œ¶)p(Z)

= p(y1, . . . , yJ | ÀÜy1, . . . , ÀÜyJ , Z)p(Z)

10: (alternative to step 9:) minimize an expected loss (e.g. gradient decent with mean squared error):

arg minZ

(cid:80)J

j=1 (cid:107)ÀÜyj - yj(cid:107)2.

Algorithm 3 Sampling from œÄVAE

Require: Trained decoder d (œà Ô¨Åxed) and Œ¶ (learnt from Algorithm 1)

1: Input: Locations s1, . . . , sJ where we want to evaluate the sampled function.
2: Sample Z: Z ‚àº N (0, I)
3: decode with VAE to get : Œ≤ := d(Z, œà)
4: for each location j do
5:

transform locations: Œ¶(sj)
inner product with Œ≤: ÀÜyj ‚Üê Œ≤T Œ¶(sj)

6:
7: end for

Appendix C MNIST Example

Figure C4 below is the MNIST example referenced in the main text for neural processes.

Appendix D Implementation Details

All models were implemented with PyTorch (Paszke et al., 2019) in Python. For Bayesian inference
Stan (Carpenter et al., 2017) was used. For training while using a Ô¨Åxed grid, when not mentioned in main
text, in each dimension was on the range -1 to 1. Our experiments ran on a workstation with two NVIDIA
GeForce RTX 2080 Ti cards.

RBF and Mat√©rn layers: RBF and Mat√©rn layers are implemented as a variant of the original RBF
networks as described in (Broomhead & Lowe, 1988; Park & Sandberg, 1991). In our setting we deÔ¨Åne a
set of C trainable centers, which act as Ô¨Åxed points. Now for each input location we calculate a RBF or
Mat√©rn Kernel for all the Ô¨Åxed points. These calculated kernels are weighted for each Ô¨Åxed center and
then summed over to create a scalar output for each location. We can describe the layer as follows:-

Œ¶(s) =

c
(cid:88)

i=1

Œ±iœÅ ((cid:107)s ‚àí ci(cid:107))

Springer Nature 2021 LATEX template

œÄVAE

21

10% pixels

20% pixels

30% pixels

(a) Observation

(b) Observation

(c) Observation

(d) Sample 1

(e) Sample 1

(f) Sample 1

(g) Sample 2

(h) Sample 2

(i) Sample 2

(j) Sample 3

(k) Sample 3

(l) Sample 3

Fig. C4: MNIST reconstruction using neural processes after observing only 10, 20 or 30% of pixels from
original data.

where s is an input location (point), Œ±i is the weight for each center ci, and œÅ ((cid:107)x ‚àí ci(cid:107)) is RBF or

Mat√©rn kernel for RBF and Mat√©rn layer respectively.

LGCP simulation example

We study the function space of 1-D LGCP realisations. We deÔ¨Åne a nonnegative intensity function at any
time t as Œª(t) : T ‚Üí R+. The number of events in an interval [t1, t2] within some time period, y[t1,t2], is
distributed Poisson via the following Bayesian hierarchical model:

Z(t) ‚àº GP(0, k)

Springer Nature 2021 LATEX template

22

œÄVAE

Œª(t) = Œ≥ ¬∑ exp(Z(t))
(cid:18)(cid:90) t2

y[t1,t2] ‚àº Poisson

t1

(cid:19)

Œª(t)dt

(D1)

where Œ≥ is a constant event rate, set to 5 in our experiments.

To train œÄVAE on functions from an LGCP, we draw 10,000 samples from Eq (D1), assuming k is an
RBF kernel/covariance function with form e‚àíœÉx2
, with inverse lengthscale œÉ chosen randomly from the
set {8, 16, 32, 64}. We then choose an observation window suÔ¨Éciently large to ensure that 80 events are
observed. This approach is meant to simulate the situation in which we observe a point process until a
certain number of events have occurrence, at which point we conduct inference (Mishra, Rizoiu, & Xie,
2016).

Given the set of 80 √ó 10, 000 events, we train œÄVAE with their corresponding intensity and integral
of the intensity over the corresponding observation window. The integral is calculated numerically. We
concatenate the integral of the intensity at the end with the intensity itself (value of the function evaluated
the speciÔ¨Åc location). Note, in this setup we have Œ≤ setup as a 2 ‚àí D vector, Ô¨Årst value corresponding to
the intensity and second to the integral of the intensity. The task for œÄVAE is to simultaneously learn both
the instantaneous intensity and the integral of the intensity. At testing, we expand the number of events
(and hence the time horizon) to 100, and compare the intensity and integral of œÄVAE compared to the true
LGCP. As seen in Figure 4, in this extrapolation, our estimated intensity is very close to the true intensity
and even the estimated integral is close to the true (numerically calculated) integral. This example shows
that the œÄVAE approach can be used to learn not only function evaluations but properties of functions.

