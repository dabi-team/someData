Towards ML Engineering: A Brief History 
Of TensorFlow Extended (TFX) 

Konstantinos (Gus) Katsiapis, Abhijit Karmarkar, Ahmet Altay, Aleksandr Zaks, Neoklis Polyzo-
tis, Anusha Ramesh, Ben Mathes, Gautam Vasudevan, Irene Giannoumis, Jarek Wilkiewicz, Jiri 
Simsa, Justin Hong, Mitch Trott, Noé Lutz, Pavel A. Dournov, Robert Crowe, Sarah Sirajuddin, 
Tris Brian Warkentin, Zhitao Li 

Google Inc. 

{katsiapis, awk, altay, azaks, npolyzotis, anusharamesh, benmathes, 
gvasudevan, irenegi, jarekw, jsimsa, hongjustin, trott, noelutz, 
dournov, robertcrowe, ssirajuddin, triswarkentin, zhitaoli}@google.com 

Abstract 

Software  Engineering,  as  a  discipline,  has  ma-
tured  over  the  past  5+  decades.  The  modern 
world  heavily  depends  on  it,  so  the  increased 
maturity  of  Software  Engineering  was  an  even-
tuality.  Practices  like  testing  and  reliable  tech-
nologies  help  make  Software  Engineering  reli-
able  enough  to  build  industries  upon.  Mean-
while,  Machine  Learning  (ML)  has  also  grown 
over the past 2+ decades. ML is used more and 
more  for  research,  experimentation  and  produc-
tion  workloads.  ML  now  commonly  powers 
widely-used products integral to our lives. 

But  ML  Engineering,  as  a  discipline,  has  not 
widely  matured  as  much  as  its  Software  Engi-
neering  ancestor.  Can  we  take  what  we  have 
learned and help the nascent field of applied ML 
evolve  into  ML  Engineering  the  way  Program-
ming evolved into Software Engineering [1]? 

In  this  article  we  will  give  a  whirlwind  tour  of 
Sibyl  [2]  and  TensorFlow  Extended  (TFX)  [3], 
two  successive  end-to-end  (E2E)  ML  platforms 
at  Alphabet.  We  will  share  the  lessons  learned 
from over a decade of applied ML built on these 
platforms,  explain  both  their  similarities  and 
their differences, and expand on the shifts (both 

mental and technical) that helped us on our jour-
ney.  In  addition,  we  will  highlight  some  of  the 
capabilities  of TFX  that  help  realize  several  as-
pects of ML Engineering. We argue that in order 
to unlock the gains ML can bring, organizations 
should  advance  the  maturity  of  their  ML  teams 
by  investing  in  robust  ML  infrastructure  and 
promoting  ML  Engineering  education.  We  also 
recommend that before focusing on cutting-edge 
ML modeling techniques, product leaders should 
invest  more  time  in  adopting  interoperable  ML 
platforms for their organizations. In closing, we 
will also share a glimpse into the future of TFX. 

Where We Are Coming 
From 

Applied ML has been an integral part of Google 
products and services over the last decade, and is 
becoming  more  so  over  time.  We  discovered 
early  on  from  our  endeavors  to  apply  ML  in 
production that while ML algorithms are impor-
tant, they are usually insufficient in realizing the  
successful application of ML in a product [4]. In 
particular,  E2E  ML  platforms,  which  help  with 
all aspects of the ML lifecycle, are usually need-

1

ed to both accelerate ML adoption and make its 
use durable and sustainable. 

Sibyl (2007 - 2020) 

E2E  ML  platforms  are  not  a  new  thing  at 
Google. Sibyl [2], founded in 2007, was a plat-
form  that  enabled  massive-scale  ML,  catered  to 
production use. Sibyl offered a decent amount of 
modeling  flexibility  on  top  of  “wide”  models 
(linear, logistic, poisson regression and later fac-
torization machines [5]) coupled with non-linear 
transformations and customizable loss functions 
and  regularization  [6].  Importantly,  Sibyl  also 
offered tools for several aspects of the ML work-
flow  including  Data  Ingestion,  Data  Analysis 
and  Validation,  Training  (of  course),  Model 
Analysis, and Training-Serving Skew Detection. 
All  these  were  packaged  as  a  single  integrated 
product  that  allowed  for  iterative  experimenta-
tion. This holistic product offering, coupled with 
the  Sibyl  team’s  user  focus,  rendered  Sibyl  to, 
once  upon  a  time,  be  one  of  the  most  widely 
used  E2E  ML  platforms  at  Google.  Sibyl  has 
since  been  decommissioned.  It  was  in  produc-
tion  for  ~14  years,  and  the  vast  majority  of  its 
workloads migrated to TFX. 

TFX (2017 -  ?) 

While several of us were still working on Sibyl, 
a  notable  revolution  was  happening  in  the  ML 
algorithms  fields  with  the  popularization  of 
Deep  Learning  (DL).  In  2015,  Google  publicly 
released TensorFlow [7] (which was itself a suc-
cessor  to  a  previous  system  called  DistBelief 
[8]). Since its inception, TensorFlow supported a 
variety of applications with a focus on DL train-
ing  and  inference.  Its  flexible  programming 
model allowed it to be used for a lot more than 
DL  and  its  popularity  in  both  research  and  pro-
duction positioned it as the lingua franca for au-
thoring  ML  algorithms.  While  TensorFlow  of-
fered flexibility, it lacked a complete end-to-end 
production system. On the other hand, Sibyl had 
robust  end-to-end  capabilities,  but  lacked  flexi-

bility. It became apparent that we needed an E2E 
ML platform for TensorFlow in order to acceler-
ate ML at Google; in 2017, nearly a decade after 
the  birth  of  Sibyl,  we  launched  TFX  within 
Google. TFX is now the most widely used, gen-
eral  purpose  E2E  ML  platform  at Alphabet,  in-
cluding Google. 

In the 3 years since its launch, TFX has enabled 
Alphabet  to  realize  what  might  be  described  as 
“industrial-scale”  ML:  TFX  is  used  by  thou-
sands  of  users  within  Alphabet,  and  it  powers 
hundreds  of  popular Alphabet  products,  includ-
ing Cloud AI services on Google Cloud Platform 
(GCP). On any given day there are thousands of 
TFX  pipelines  running,  which  are  processing 
exabytes  of  data  and  producing  tens  of  thou-
sands  of  models,  which  in  turn  are  performing 
hundreds  of  millions  of  inferences  per  second. 
TFX’s widespread adoption helps Alphabet real-
ize the flow of research into production and en-
ables  very  diverse  use  cases  for  both  direct  and 
indirect  TFX  users.  This  widespread  adoption 
also  enables  teams  to  focus  on  model  develop-
ment  rather  than  ML  platform  development,  al-
lowing  ML  to  be  more  easily  used  in  novel 
product  areas,  and  creating  a  virtuous  cycle  of 
ML platform evolution from ML applications. 

The  popularity  and  impact  of  TensorFlow  [9] 
within  and  outside  of  Alphabet,  the  popularity 
and impact of TFX within Alphabet, and the re-
ality that equivalents of ML engineering will be 
needed  by  organizations  and  individuals  every-
where in the world, felt like something we could 
not  ignore.  That  led  us  to  publicly  describe  the 
design  and  initial  deployment  of  TFX  within 
Google [10] and to, step by step, make more of 
our learnings and our technology publicly avail-
able (including open source), while we continue 
building more of each. We were able to accom-
plish  this  in  part  because,  like  Sibyl, TFX  built 
upon  robust  infrastructural  dependencies.  For 
example,  Sibyl  made  heavy  use  of  MapReduce 
[11]  and  its  successor  Flume  [12]  for  its  dis-
tributed  data  processing,  and  now  TFX  heavily 

2

uses their portable successor, Apache Beam [13], 
for the same. 

Applied ML 

Following  in TensorFlow’s  footsteps,  the  public 
TFX offering [3] was released in early 2019 and 
widely  adopted  in  under  a  year  across  environ-
ments  including  on-premises  and  GCP  with 
Cloud AI  Platform  Pipelines  [14].  Some  of  our 
partners have also publicly shared their use cases 
powered by TFX [15], including how it radically 
improved their applied ML velocity [16]. 

Lessons From Our 10+ 
Year Journey Of ML Plat-
form Evolution 

Though the journey of ML Platform(s) evolution 
at Google has been a long and exciting one, we 
expect  that  the  majority  of  excitement  is  yet  to 
come! To that end, we want to share a summary 
of  our  learnings,  some  of  which  were  more 
painfully  gained  than  others.  The  learnings  fall 
into  two  categories,  namely  what  remained  the 
same  as  part  of  the  evolution,  but  also  what 
changed,  and  why!  We  present  the  learnings  in 
the  context  of  two  successive  platforms,  Sibyl 
and TFX, though we believe them to be widely 
applicable. 

What Remains The Same And 
Why 

The areas discussed in this section capture a few 
examples of things that seem enduring and pass 
the test of time. As such, we expect these to also 
remain  applicable  in  the  future,  across  different 
incarnations  of  ML  platforms  and  frameworks. 
We look at these from both an applied ML per-
spective and an infrastructure perspective. 

The Rules Of Machine Learning 
Successfully  applying  ML  to  a  product  is  very 
much  a  discipline.  It  involves  a  steep  learning 
curve and necessitates some mental model shifts 
(or  perhaps  augmentations).  To  make  this  chal-
lenging task easier, we have publicly shared The 
Rules  of  Machine  Learning  [17,  18].  These  are 
rules  that  represent  learnings  from  iteratively 
applying ML to a lot of products at Google. No-
tably,  the  adoption  of  ML  in  Google  products 
illustrates a common evolution: 

● Start  with  simple  rules  and  heuristics, 
and  generate  data  to  learn  from;  this 
journey  usually  starts  from  the  serving 
side [17]. 

● Move  to  simple  ML  (i.e.,  simple  mod-
els) and realize large gains; this is usual-
ly the entry point for introduction of ML 
pipelines [17]. 

● Move  to  ML  with  more  features  and 
more advanced models to realize decent 
gains [17]. 

● Move  to  state-of-the-art  ML,  manage 
refinement  and  complexity  (for  solu-
tions  to  the  problems  that  are  worth  it), 
and realize small gains [17]. 

● Apply  the  above  launch-and-iterate  cy-
cle  to  more  aspects  of  products  and  to 
solve  more  problems,  bearing  in  mind 
return  on  investment  (and  diminishing 
returns). 

We have found The Rules of Machine Learning 
to be steadfast across platforms and time and we 
hope they end up being as valuable to others as 
they have been to us and our users. In particular, 
we believe that following the rules will help oth-
ers be better at the discipline of ML engineering, 
including  helping  them  avoid  the  mistakes  that 
we and our users have made in the past. TFX is 
an attempt to codify these rules, quite literally, in 
code. We  hope  to  benefit  ourselves  but  also  ac-
celerate ML, done well, for the entire industry. 

3

The Discipline Of ML Engineering 
In  developing  The  Rules  of  Machine  Learning, 
we  realized  that  the  discipline  for  building  ro-
bust systems where the core logic is produced by 
complex processes involving both code and data 
requires  additional  scrutiny  beyond  that  which 
software  engineering  provides. As  such,  we  de-
fine  ML  Engineering  as  a  superset  of  the  disci-
pline of software engineering designed to handle 
the unique complexities of the practical applica-
tion of ML. 

Attempting to summarize the totality of the dis-
cipline  of  ML  engineering  would  be  somewhat 
difficult, if not impossible, especially given how 
our  understanding  of  it  is  still  limited,  and  the 
discipline itself continues to evolve. We do take 
solace in the following though: 

● The  limited  understanding  we  do  have 
seems  to  be  enduring  across  platforms 
and time. 

● Analogy can be a powerful tool, so sev-
eral aspects of the better understood dis-
cipline  of  software  engineering  have 
helped us draw parallels of how ML en-
gineering  could  evolve  from  ML  pro-
gramming,  much  like  how  software  en-
gineering  evolved  from  programming 
[1]. 

An  early  realization  we  had  was  the  following: 
artifacts  are  first  class  citizens  in  ML,  on  par 
with  the  processes  that  produce  and  consume 
them.  

This realization affected the implementation and 
evolution of Sibyl; it was entrenched in TFX by 
the time we publicly wrote about it [10] and was 
ultimately  generalized  and  formalized  in  ML 
Metadata [19], now powering TFX.  

Below  we  present  fundamental  elements  of  ML 
engineering, some examples of ML artifacts and 
their first class citizenship, and make an attempt 

to  draw  analogies  with  software  engineering 
where possible. 

Data 
Similarly to how code is at the heart of software, 
data  is  at  the  heart  of  ML.  Data  management 
represents  serious  challenges  in  production  ML 
[20].  Perhaps  the  simplest  analogy  would  be  to 
think about what constitutes a unit test for data. 
Unit  tests  verify  expectations  on  how  code 
should  behave,  by  testing  the  contracts  of  the 
pertinent  code  and  instilling  trustworthiness  in 
said  contracts.  Similarly,  setting  explicit  expec-
tations  on  the  form  of  the  data  (including  its 
schema,  invariants  and  value  distributions),  and 
checking  that  the  data  agrees  with  implicit  ex-
pectations  embedded  in  the  training  code  can, 
more  so  together,  make  the  data  trustworthy 
enough  to  train  models  on  [21].  Though  unit 
tests  can  be  exhaustive  and  verify  strong  con-
tracts, data contracts are in general a lot weaker 
even if they are necessary. Though unit tests can 
be  exhaustively  consumed  and  verified  by  hu-
mans, data can usually be meaningful to humans 
only in summarized fashion. 

Just as code repositories and version control are 
pillars  for  managing  code  evolution  in  software 
engineering,  systems  for  managing  data  evolu-
tion  and  understanding  are  pillars  of  ML  engi-
neering. 

TFX’s  ExampleGen  [22],  StatisticsGen  [22], 
SchemaGen  [22]  and  ExampleValidator  [22] 
components help one treat data as first class citi-
zens,  by  enabling  data  management,  analysis 
and  validation  in  (continuous)  ML  pipelines 
[23]. 

Models 
Similarly  to  how  a  software  engineer  produces 
code that is compiled into programs, an ML en-
gineer  produces  data  and  code  which  is  “com-
piled”  into  ML  programs,  more  commonly 
known as models. These two kinds of programs 
are  however  very  different  in  nature.  Though 
programs that come out of software usually have 

4

strong contracts, models have much weaker con-
tracts. These weak contracts are usually statisti-
cal in nature and as such only verifiable in some 
summarized form (such as a model having suffi-
cient accuracy on a subset of labeled data). This 
is not at all surprising since models are the prod-
uct of code and data, and the latter itself doesn’t 
have strong contracts and is also only digestible 
in summarized form.  

Just  as  code  and  data  evolve  over  time,  models 
also  evolve  over  time.  However,  model  evolu-
tion  is  more  complicated  than  the  evolution  of 
its constituent code and data. For example, high 
test coverage (with fuzzing) can give good con-
fidence  in  both  the  correctness  and  the  correct 
evolution of a piece of code, but out-of-distribu-
tion  and  counterfactual  yet  realistic  data  for 
model  evaluation  can  be  notoriously  difficult  to 
produce. 

In  the  same  way  that  putting  together  multiple 
programs  in  a  system  necessitates  integration 
testing which is a pillar of software engineering, 
putting together code and data necessitates end-
to-end  model  validation  and  understanding  [24] 
which is a pillar of ML engineering. 

TFX’s  Evaluator  [22]  and  InfraValidator  [22] 
components  provide  validation  and  understand-
ing  of  models,  treating  them  as  first  class  citi-
zens of ML engineering.  

Mergeable Fragments 
Similarly  to  how  a  software  engineer  merges 
together  pre-existing  libraries  (or  systems)  with 
their code in order to build useful programs, an 
ML  engineer  merges  together  code  fragments, 
data  fragments,  analysis  fragments  and  model 
fragments  on  a  regular  basis  in  order  to  build 
useful  ML  pipelines.  A  notable  difference  be-
tween software engineering and ML engineering 
is that even when the code is fixed for the latter, 
data  is  usually  volatile  for  it  (e.g.  new  data  ar-
rives  on  a  regular  basis)  and  as  such  the  down-
stream  artifacts  need  to  be  produced  frequently 
and efficiently. For example, a new version of a 

model  usually  needs  to  be  produced  if  any  part 
of its input data has changed. As such, it is im-
portant for ML pipelines to produce artifacts that 
are  mergeable.  For  example,  a  summary  of  sta-
tistics from one dataset should be easily merge-
able  with  that  of  another  dataset  such  that  it  is 
easy  to  summarize  the  statistics  of  the  union  of 
the  two  datasets.  Similarly,  it  should  be  easy  to 
transfer  the  learnings  of  one  model  to  another 
model in general, and the learnings of a previous 
version  of  a  model  to  the  next  version  of  the 
same model in particular. 

There  is  however  a  catch,  which  relates  to  the 
previous discussion regarding the equivalents of 
test  coverage  for  models.  Merging  new  frag-
ments into a model could necessitate creation of 
novel  out-of-distribution  and  counterfactual 
evaluation  data,  contributing  to  the  difficulty  of 
(efficient)  model  evolution,  thus  rendering  it  a 
lot harder than pure code evolution. 

TFX’s ExampleGen [22], Transform [22], Train-
er  [22]  and  Tuner  [22]  components,  together 
with  TensorFlow  Hub  [25],  help  one  treat  arti-
facts  as  first  class  citizens  by  enabling  produc-
tion and consumption of mergeable fragments in 
workflows  that  perform  data  caching,  analyzer 
caching, warmstarting and transfer learning. 

Artifact Lineage 
Despite all the advanced methodology and tool-
ing that exists for software engineering, the pro-
grams and systems that are built invariably need 
to  be  debugged.  The  same  holds  for  ML  pro-
grams, but debugging them is notoriously harder 
because  non-proximal  effects  are  a  lot  more 
prevalent  for  ML  programs  due  to  the  plethora 
of  artifacts  involved. A  model  might  be  inaccu-
rate due to bad artifacts from several sources of 
error,  including  flaws  in  the  code,  the  learning 
algorithm, the training data, the serving path, or 
the serving data, to name a few.  Much like how 
stack  traces  are  invaluable  for  identifying  root 
causes  of  defects  in  software  programs,  the  lin-
eage  of  all  artifacts  produced  and  consumed  by 
an ML pipeline is invaluable for identifying root 

5

causes of defects in ML models. Additionally, by 
knowing  which  downstream  artifacts  were  pro-
duced from a problematic artifact, we can identi-
fy all impacted systems and users and take miti-
gating actions. 

TFX’s use of ML Metadata (MLMD) [19] helps 
treat  artifacts  as  first  class  citizens.  MLMD  en-
ables  advanced  cataloging  and  querying  of 
metadata  and  lineage  associated  with  artifacts 
which  can  together  increase  the  confidence  of 
sharing artifacts even outside the boundaries of a 
pipeline.  MLMD  also  helps  with  advanced  de-
bugging and, when coupled with the underlying 
data  storage  layer,  forms  the  foundation  of 
TFX’s ML compliance mechanisms. 

Continuous Learning And Unlearning 
ML  production  pipelines  operate  in  a  dynamic 
environment: 

● New data can arrive continuously. 
● The modeling code can change, particu-
larly in the early stages of model devel-
opment. 

● The  surrounding  infrastructure  can 
change, e.g., a new version of some un-
derlying (ML) library. 

When changes happen, a pipeline needs to react, 
often by rerunning its steps in the new environ-
ment. This dynamicity increases the importance 
of  provenance  tracking  in  order  to  facilitate  de-
bugging  and  root-cause  analysis.  As  a  simple 
example,  to  debug  a  model  failure,  it  is  neces-
sary  to  know  not  only  which  data  was  used  to 
train  the  model,  but  also  the  versions  of  the 
modeling  code  and  any  surrounding  in-
frastructure. 

ML  pipelines  must  also  support  low-friction 
mechanisms  to  handle  these  changes.  Consider 
for  example  the  arrival  of  new  data,  which  ne-
cessitates retraining the model. This is a natural 
requirement  in  rapidly  changing  environments, 
like  recommender  systems  or  adversarial  sys-
tems. Requiring the user to manually retrain the 

model can be unrealistic, given that the data can 
arrive at a regular and frequent rate. Instead, we 
can employ automation by way of   “continuous 
training”,  where  the  pipeline  detects  the  pres-
ence  of  new  data  and  automatically  schedules 
the  generation  of  updated  models.  In  turn,  this 
functionality  requires  automatically:  orchestrat-
ing  work  based  on  the  presence  of  artifacts  (in-
cluding  data),  recovering  from  intermittent  fail-
ures, and catching up to real-time when recover-
ing  [26].  It  is  common  for  ML  pipelines  to  run 
for  years  ingesting  code  and  data,  continuously 
producing  models  that  make  predictions  that 
inform decisions. 

Another example of a low-friction mechanism is 
support for “backfilling” an ML pipeline. In this 
case,  the  user  might  need  to  rerun  the  pipeline 
on  existing  artifacts  but  using  updated  versions 
of the components, such as rerunning the trainer 
on existing data using a new version of the mod-
eling  code/library. Another  use  of  backfilling  is 
rerunning the pipeline with new versions of ex-
isting data, say, to fix an error in the data. These 
backfills  are  orthogonal  to  continuous  training 
and can be used together. For instance, the user 
can  manually  trigger  a  rerun  of  the  trainer,  and 
the  generated  model  artifact  can  then  automati-
cally trigger model evaluation and validation. 

TFX was built from the ground up in a way that 
enables  continuous  learning  (and  unlearning) 
which  fundamentally  shaped  its  design.  At  the 
same  time,  these  advanced  capabilities  also  al-
low it to be used in a “one-shot”, discontinuous, 
fashion. In fact, within Alphabet, both modes of 
deployment  are  widely  used.  Moreover,  TFX 
also  supports  different  types  of  backfill  opera-
tions to enable fine-grained interventions during 
normal pipeline execution. 

Even  though  the  public  TFX  offering  [22] 
doesn’t  yet  offer  continuous  ML  pipelines,  we 
are  actively  working  on  making  our  existing 
technology portable so that it can be made pub-
licly available [e.g 27]. 

6

Infrastructure 

Building On The Shoulders Of Giants 
Realizing  ambitious  goals  necessitates  building 
on  top  of  solid  foundations,  collaborating  with 
others  and  leveraging  each  other's  work.  TFX 
reuses many of Sibyl's system designs, hardened 
over a decade of Sibyl’s production ML experi-
ence. Additionally,  TFX  incorporates  new  tech-
nologies  in  areas  where  robust  standards 
emerged: 

● Similarly  to  how  Sibyl  built  its  algo-
rithms and workflows on top of MapRe-
duce,  TFX  leverages  both  TensorFlow 
[9]  and  Apache  Beam  [13]  for  its  dis-
tributed  training  and  data  processing 
workflows. 

● Similarly  to  how  Sibyl  was  columnar, 
TFX adopted Apache Arrow [28] as the 
columnar  in-memory  representation  for 
its compute-intensive libraries. 

Taking  dependencies  where  robust  standards 
have emerged has allowed TFX and its users to 
achieve seamless performance and scalability. It 
also enables TFX to focus its energy on building 
the deltas of what is needed for applied ML, as 
opposed to re-implementing difficult-to-get-right 
technology.  Some  of  our  dependencies,  like 
Kubeflow  Pipelines  [29]  or  Apache  Airflow 
[30],  are  selected  by  TFX’s  users  themselves 
when  the  value  /  features  they  get  from  them 
outweigh the costs that the additional dependen-
cies entail. 

Taking  dependencies  unfortunately  incurs  costs. 
We have found that taking dependencies requires 
effort  that  is  super-linear  to  the  number  of  de-
pendencies. Said costs are often absorbed by us 
and our sister teams but can (and sometimes do) 
leak to our users, usually in the form of conflict-
ing  (version)  dependencies  or  incompatibilities 
between environments and dependencies. 

Interoperability And Positive Externalities 
ML platforms do not operate in a vacuum. They 
instead  operate  within  the  context  of  a  bigger 
system or infrastructure, connecting to data pro-
ducing  sources  upstream  and  model  consuming 
sinks downstream, which in turn frequently pro-
duce the data that feeds the ML platform, there-
by  closing  the  loop.  Strong  adoption  of  a  plat-
form  usually  necessitates  interoperability  with 
other important technologies in its environment. 

● Similarly  to  how  Sibyl  interoperated 
with  Google’s Ads  technology  stack  for 
data  ingestion  and  model  serving,  TFX 
offers  a  plethora  of  connectors  for  data 
ingestion  and  allows  serving  the  pro-
duced  model  in  multiple  deployment 
environments and devices. 

● Similarly  to  how  Sibyl  interoperated 
with  Google’s  compute  stack,  TFX 
leverages Apache  Beam  [13]  to  execute 
on Apache Flink [31] and Apache Spark 
[32] clusters as well as serverless offer-
ings like Google Cloud Dataflow [33]. 
● TFX  built  an  orchestration  abstraction 
on top of MLMD [19] and provides or-
chestration  options  on  top  of  Apache 
Airflow [22], Apache Beam [22], Kube-
flow Pipelines [22] as well as the primi-
tives  to  integrate  with  one’s  custom  or-
chestrator.  MLMD  itself  works  with 
several  relational  databases  like  SQLite 
[34] and MySQL [35]. 

Interoperability  necessitates  some  amount  of 
abstraction  and  standardization  and  usually  en-
ables  sum-greater-than-its-parts  effects.  TFX  is 
both a beneficiary and a benefactor of the posi-
tive externalities created by said interoperability, 
both  within  and  outside  of  Alphabet.  TFX’s 
users are also beneficiaries of the interoperabili-
ty  as  they  can  more  easily  deploy  and  use TFX 
on top of their existing installed base.  

Interoperability  also  comes  with  costs.  The 
combination  of  multiple  technology  stacks  can 

7

lead  to  an  exponential  number  of  distinct  de-
ployment configurations. While we test some of 
the  distinct  deployment  configurations  end-to-
end and at-scale, like for example TFX on GCP 
[14],  we  have  neither  the  expertise  nor  the  re-
sources to do so for the combinatorial explosion 
of all possible deployment options. We thus en-
courage  the  community  to  work  with  us  on  the 
deployment  configurations  that  are  most  useful 
for them [36]. 

What Is Different And Why 

The areas discussed in this section capture a few 
examples of things that needed to change in or-
der for our ML platform to adapt to a new reality 
and as such remain useful and impactful. 

Environment And Device Portability 

Sibyl was a massive scale ML platform designed 
to  be  deployed  on  Google’s  large-scale  cluster, 
namely  Borg  [37].  This  made  sense  as  applied 
ML at Google was, originally, primarily used in 
products that were widely used. As ML expertise 
grew across the world, and ML could be applied 
to more use cases (large and small) across envi-
ronments both within and outside of Google, the 
need for portability gradually but surely became 
a hard constraint. 

● While  Sibyl  ran  only  on  Google’s  data-
centers,  TFX  runs  on  laptops,  worksta-
tions,  servers,  datacenters,  and  public 
Clouds. In particular, when TFX runs on 
Google’s  Cloud  [38],  it  leverages  au-
tomation  and  optimizations  offered  by 
GCP  Services,  enabled  by  Google’s 
unique infrastructure. 

● While  Sibyl  ran  only  on  CPUs,  TFX 
leverages TensorFlow to run on different 
kinds  of  hardware  including  CPUs, 
GPUs and Google’s TPUs [39, 40]. 
● While  Sibyl’s  models  ran  on  servers, 
TFX  leverages  TensorFlow  to  produce 
models  that  run  on  laptops,  worksta-
tions,  and  servers  via TensorFlow  Serv-

ing  [22,  41]  and Apache  Beam  [22],  on 
mobile  and  IoT  devices  via TensorFlow 
Lite  [42],  and  on  browsers  via  Tensor-
Flow JS [43]. 

TFX’s portability enabled it to be used in a very 
diverse set of environments and devices, in order 
to  solve  problems  from  small  scale  to  massive 
scale. 

Unfortunately,  portability  comes  with  costs. We 
have found that maintaining a portable core with 
environment-specific  and  device-specific  spe-
cialization  requires  effort  that  is  super-linear  to 
the number of environments / devices. Said costs 
are however largely absorbed by us and our sis-
ter  teams  and  as  such  are  frequently  not  visible 
to our users. 

Modularity And Layering 

Even  though  Sibyl’s  offering  as  an  integrated 
product  was  immensely  valuable,  its  structure 
and  interface  were  somewhat  monolithic,  limit-
ing  it  to  a  specific  set  of  “direct”  users  who 
would  have  to  adopt  it  wholesale.  In  contrast, 
TFX evolved to be a modular and layered archi-
tecture,  and  became  more  so  over  time  as  part-
nerships  with  other  teams  and  products  grew. 
Notable layers (with examples) in TFX include: 

Layer

Examples

ML Services

Pipelines 
(of compos-
able Compo-
nents)

● Cloud AutoML [44] 
● Cloud Recommen-
dations AI [45] 
● Cloud AI Platform 

[14, 22, 46] 

● Cloud Dataflow [33, 

47] 

● Cloud BigQuery 

[48]

● TensorFlow Extend-
ed (TFX) [3, 22, 49]

8

Binaries

● TensorFlow Serving 
(TFS) [41, 50]

Libraries

● TensorFlow Data 

Validation (TFDV) 
[51] 

● TensorFlow Trans-
form (TFT) [52] 
● TensorFlow Hub 
(TFH) [53] 

● TensorFlow Model 
Analysis (TFMA) 
[54] 

● TFX Basic Shared 
Libraries (TFX_B-
SL) [55] 
● ML Metadata 
(MLMD) [56]

TFX’s layered architecture enables it to be used 
by  a  very  diverse  set  of  users  whether  that’s 
piecemeal  via  its  libraries,  wholesale  via  its 
pipelines  (with  or  without  the  pertinent 
services), or in a fashion that’s completely obliv-
ious  to  the  end  users  (e.g.  by  them  using  ML 
services which TFX powers under the hood)! 

Unfortunately,  layering  comes  with  costs.  We 
have  found  that  maintaining  multiple  publicly 
accessible  layers  of  our  product  requires  effort 
that  is  roughly  linear  to  the  number  of  layers. 
Said  costs  occasionally  leak  to  our  users  in  the 
form  of  confusion  regarding  what  layer  makes 
the most sense for them to use. 

Multi-faceted Flexibility 

Even though Sibyl was more flexible in terms of 
modeling  capabilities  compared  to  available  al-
ternatives  at  the  time,  aspects  of  its  flexibility 
across  several  parts  of  the  ML  workflow  fell 
short of Google’s needs for accelerating ML for 
novel use cases, which led to the development of 
TFX [22]. 

● While  Sibyl  only  offered  specific  kinds 
of  data  analysis,  TFX’s  StatisticGen 

component [22] offers more built-in ca-
pabilities  and  the  ability  to  realize  cus-
tom analyses, via TensorFlow Data Val-
idation [22]. 

● While  Sibyl  only  offered  transforma-
tions  that  were  pure  composable  map-
pers,  TFX’s  Transform  component  [22] 
offers  more  mappers,  custom  mappers, 
more  analyzers,  custom  analyzers,  as 
well  as  arbitrarily  composed  (custom) 
mappers  and  (custom)  analyzers,  via 
TensorFlow Transform [22]. 

● While  Sibyl  only  offered  “wide”  mod-
els,  TFX’s  Trainer  component  [22]  of-
fers  any  model  that  can  be  realized  on 
top of TensorFlow [57], including mod-
els  that  can  be  shared  and  can  transfer-
learn, via TensorFlow Hub [25]. 

● While Sibyl only offered automatic fea-
ture  crossing  (a.k.a.  feature  conjunc-
tions)  on  top  of  “wide”  models,  TFX’s 
Tuner  component  [22]  allows  for  arbi-
trary  hyper  parameter  optimization 
based on state of the art algorithms. 
● While  Sibyl  only  offered  specific  kinds 
of  model  analysis,  TFX’s  Evaluator 
component  [22]  offers  more  built-in 
metrics,  custom  metrics,  confidence  in-
tervals  and  fairness  indicators  [58],  via 
TensorFlow Model Analysis [22]. 

● While  Sibyl’s  pipeline  topology  was 
fixed  (albeit  somewhat  customizable), 
TFX’s SDK allows one to create custom 
(optionally  containerized)  components 
[22]  and  use  them  together  with  stan-
dard  components  [22]  in  a  flexible  and 
fully  customizable  pipeline  topology 
[22]. 

The  increase  of  flexibility  in  all  these  dimen-
sions  enabled  improved  experimentation,  wider 
reach,  more  use  cases,  as  well  as  accelerated 
flow from research to production.  

Flexibility does not come without costs. A more 
flexible system is one that is harder to get right 
in the first place as well as harder for us to main-

9

tain and to evolve as developers of the ML plat-
form. Users may also have to manage increased 
complexity as they take advantage of this flexi-
bility. Furthermore, we might not be able to offer 
as  strong  of  a  support  story  on  top  of  an  ML 
platform that is Turing complete. 

Where We Are Going 

Armed  with  the  knowledge  of  the  past,  we 
present a glimpse of what we plan for the future 
of TFX as of 2020, based on our roadmap [59], 
requests  for  comments  (RFCs)  [60],  and  contri-
bution  guidelines  [36].  We  will  continue  our 
work  on  enabling  ML  Engineering  in  order  to 
democratize  applied  ML,  and  help  everyone 
practice  responsible  AI  [61]  and  apply  it  in  a 
fashion  that  upholds  Google’s  AI  Principles 
[62]. 

Drive  Interoperability  And  Stan-
dards 

In order to meet the demand for the burgeoning 
variety of ML solutions, we will continue to in-
crease  our  technology’s  interoperability.  Our 
work on interoperability and standards as well as 
open-sourcing  more  of  our  technology,  reflects 
our principle to “be socially beneficial” as well 
as  to  “be  made  available  for  uses  that  accord 
with  these  principles”  by  making  it  easier  for 
everyone  to  follow  these  practices.  As  part  of 
this  mission,  we  will  empower  the  industry  to 
build  advanced  ML  systems  by  open-sourcing 
more  of  our  technology,  and  by  standardizing 
ML  artifacts  and  metadata.  Some  select  exam-
ples of this work include: 

● TFX Standardized Inputs [63]. 
● Advanced  TFX  DSL  semantics  [64], 

Data Model and IR [65]. 

● Standardization  of  ML  artifacts  and 

metadata. 

● Standardization of distributed workloads 
on heterogeneous runtime environments. 
● Inference  on  distributed  and  streaming 

models. 

● Improvements  to  interoperability  with 
mobile and edge ML deployments. 
● Improvements  for  ML  framework  inter-

operability and artifact sharing. 

Increase Automation 

Automation  is  the  backbone  of  reliable  produc-
tion  systems,  and  TFX  is  heavily  invested  in 
improving and expanding its use of automation. 
Our  work  in  increased  automation  reflects  our 
principles of helping make ML deployments “be 
built and tested for safety” and “avoid creating 
or  reinforcing  unfair  bias”.  Some  upcoming 
efforts  include  a  TFX  Pipeline  testing  frame-
work,  automated  model  improvement  in  the 
TFX Tuner [66], auto-detecting surprising model 
behavior on multidimensional slices, facilitating 
automatic  production  of  Model  Cards  [67,  68] 
and  improving  our  training-serving  skew  detec-
tion capabilities. TFX on GCP will also continue 
driving  requirements  for  new  (and  will  better 
make use of existing) advanced automation fea-
tures of pertinent services. 

Improve ML Understanding 

ML understanding is an important aspect of de-
ploying  production  ML,  and  TFX  is  well  posi-
tioned  to  provide  significant  gains  in  this  field. 
Our  work  on  improving  ML  understanding  re-
flects  our  principles  to  help  “avoid  creating  or 
reinforcing  unfair  bias”  and  help  make  ML  de-
ployments  “be  accountable  to  people”.  Critical 
to understanding is to be able to track the lineage 
of  artifacts  used  to  produce  a  model,  an  area 
TFX will continue to invest in. Improvements to 
TFX  technologies  like  struct2tensor  [69]  will 
further  enable  training,  serving,  and  analyzing 
models on structured data, thus allowing reason-
ing  about  models  closer  to  the  original  data  se-
mantics. We also plan to utilize TFX as a vehicle 
to expand support for fairness evaluation, reme-
diation, and documentation. 

10

Uphold High Standards And Best 
Practices 

As a vehicle for amplification of ML technology, 
TFX must continue to “uphold high standards of 
scientific  excellence”  and  promote  best  prac-
tices. The team will continue publishing scientif-
ic papers and conducting public outreach via our 
existing  channels,  as  well  as  offer  educational 
courses  in  partnership  with  established  institu-
tions.  We  will  also  improve  trust  in  our  model 
analysis  tools  using  integrated  uncertainty  mea-
sures by, for example, enabling scalable compu-
tation of confidence intervals for model metrics, 
and  we  will  improve  our  training-serving  skew 
detection  capabilities.  It’s  also  critical  for  re-
search  and  production  to  be  able  to  have  repro-
ducible  ML  artifacts,  enabled  by  our  work  in 
precise  provenance  tracking  for  auditing  and 
reproducing  models. Also  key  is  reproducibility 
of  measurements,  driven  by  efforts  like  Ni-
troML,  which  will  provide  tooling  for  bench-
marking AutoML pipelines [70]. 

Given that several of the areas where we expand 
our  technology  are  new  to  us,  we  will  make  an 
effort  to  distinguish  the  battle-tested  from  the 
experimental aspects of our technology, in order 
to enable our users to confidently choose the set 
of capabilities that meet their desires and needs. 

Improve Tooling 

Despite TFX  providing  tools  for  aspects  of  ML 
engineering  and  several  phases  of  the  ML  life-
cycle,  we  believe  this  is  still  a  nascent  area. 
While improving tooling is a natural fit for TFX, 
it  also  reflects  our  principle  of  helping  ML  de-
ployments “be made available for uses that ac-
cord  with  these  principles”,  “supporting  scien-
tific excellence,” and being “built and tested for 
safety” . 

One area of improvement is applying ML to the 
data  itself,  be  it  through  sensing  anomalies  or 
finding  patterns  in  data  or  enriching  data  with 

predictions from ML models. Making it easy to 
enrich  large  volumes  of  data  (especially  critical 
streaming  data  used  for  low-latency,  high  vol-
ume  actions)  has  always  been  a  challenge. 
Bringing  TFX  capabilities  into  data  processing 
frameworks  is  our  first  step  here.  We  have  al-
ready  made  it  possible  to  enrich  streaming 
events with labels or make predictions in Apache 
Beam  and,  by  extension,  Cloud  Dataflow.  We 
plan to follow this work by leveraging pre-built 
models  (served  out  of  Cloud  AI  Pipelines  and 
TensorFlow Serving) to make adding a new field 
in  a  distributed  dataset  representing  predictions 
from streams of models trivially easy. 

Furthermore, while there are many tools for de-
tecting,  discovering,  and  auditing  ML  work-
flows, there is still a need for automated (or as-
sisted)  mitigation  of  discovered  issues,  and  we 
will invest in this area. For example, proactively 
predicting  which  pipeline  runs  won’t  result  in 
better  models  based  on  the  currently-executing 
pipeline,  perhaps  even  before  training,  can  sig-
nificantly  reduce  time  and  resources  spent  on 
creating poor models. 

A Joint Journey 

Building TFX and exploring the fundamentals of 
ML  engineering  was  the  cumulative  effort  of 
many  people  over  many  years. As  we  continue 
to make strides and further develop this field, it’s 
important  we  recognize  the  collaborative  effort 
of those who got us here.  

Of course, it will take many more collaborations 
to drive the future of this field, and as such, we 
invite  you  to  join  us  on  this  journey  “Towards 
ML Engineering”! 

The TFX Team 

The TFX project is realized via collaboration of 
multiple  organizations  within  Google.  Different 
organizations usually focus on different technol-
ogy  and  product  layers,  though  there  is  a  lot  of 

11

overlap on the portable parts of our technology. 
Overall we consider ourselves a single team and 
below we present an alphabetically sorted list of 
current TFX team members who are contributors 
to  the  ideation,  research,  design,  implementa-
tion,  execution,  deployment,  management,  and 
advocacy (to name a few) aspects of TFX; they 
continue  to  inspire,  help,  teach,  and  challenge 
each other to advance our field: 

Abhijit  Karmarkar,  Adam  Wood,  Aleksandr  Zaks, 
Alina  Shinkarsky,  Neoklis  Polyzotis,  Amy  Jang,  Amy 
McDonald  Sandjideh,  Amy  Skerry-Ryan,  Andrew 
Audibert,  Andrew  Brown,  Andy  Lou,  Anh  Tuan 
Nguyen,  Anirudh  Sriram,  Anna  Ukhanova,  Anusha 
Ramesh,  Archana  Jain,  Arun  Venkatesan,  Ashley 
Oldacre,  Baishun  Wu,  Ben  Mathes,  Billy  Lamberta, 
Chandni  Shah,  Chansoo  Lee,  Chao  Xie,  Charles 
Chen,  Chi  Chen,  Chloe  Chao,  Christer  Leusner, 
Christina  Greer,  Christina  Sorokin,  Chuan  Yu  Foo, 
CK  Luk,  Connie  Huang,  Daisy  Wong,  David  Small-
ing, David Zats, Dayeong Lee, Dhruvesh Talati, Doo-
jin  Park,  Elias  Moradi,  Emily  Caveness,  Eric  John-
son, Evan Rosen, Florian Feldhaus, Gal Oshri, Gau-
tam Vasudevan, Gene Huang, Goutham Bhat, Guanx-
in  Qiao,  Gus  Katsiapis,  Gus  Martins,  Haiming  Bao, 
Huanming Fang, Hui Miao, Hyeonji Lee, Ian Nappi-
er,  Ihor  Indyk,  Irene  Giannoumis,  Jae  Chung,  Jan 
Pfeifer, Jarek Wilkiewicz, Jason Mayes, Jay Shi, Jiayi 
Zhao,  Jingyu  Shao,  Jiri  Simsa,  Jiyong  Jung,  Joana 
Carrasqueira,  Jocelyn  Becker,  Joe  Liedtke,  Jongbin 
Park,  Jordan  Grimstad,  Josh  Gordon,  Josh  Yellin, 
Jungshik  Jang,  Juram  Park,  Justin  Hong,  Karmel 
Allison,  Kemal  El  Moujahid,  Kenneth  Yang,  Khanh 
LeViet,  Kostik  Shtoyk,  Lance  Strait,  Laurence  Mo-
roney,  Li  Lao,  Liam  Crawford,  Magnus  Hyttsten, 
Makoto  Uchida,  Manasi  Joshi,  Mani  Varadarajan, 
Marcus  Chang,  Mark  Daoust,  Martin  Wicke,  Megha 
Malpani,  Mehadi  Hassen,  Melissa  Tang,  Mia  Roh, 
Mig  Gerard,  Mike  Dreves,  Mike  Liang,  Mingming 
Liu,  Mingsheng  Hong,  Mitch  Trott,  Muyang  Yu, 
Naveen  Kumar,  Ning  Niu,  Noah  Hadfield-Menell, 
Noé  Lutz,  Nomi  Felidae,  Olga  Wichrowska,  Paige 
Bailey,  Paul  Suganthan,  Pavel  Dournov,  Pedram 
Pejman,  Peter  Brandt,  Priya  Gupta,  Quentin  de 
Laroussilhe,  Rachel  Lim,  Rajagopal  Anantha-
narayanan,  Rene  van  de  Veerdonk,  Robert  Crowe, 
Romina Datta, Ron Yang, Rose Liu, Ruoyu Liu, Sagi 
Perel,  Sai  Ganesh  Bandiatmakuri,  Sandeep  Gupta, 
Sanjana  Woonna,  Sanjay  Kumar  Chotakur,  Sarah 

Sirajuddin,  Sheryl  Luo,  Shivam  Jindal,  Shohini 
Ghosh,  Sina  Chavoshi,  Sydney  Lin,  Tanya  Grunina, 
Thea  Lamkin,  Tianhao  Qiu,  Tim  Davis,  Tris 
Warkentin,  Varshaa  Naganathan,  Vilobh  Meshram, 
Volodya  Shtenovych,  Wei  Wei,  Wolff  Dobson, 
Woohyun  Han,  Xiaodan  Song,  Yash  Katariya,  Yifan 
Mai,  Yiming  Zhang,  Yuewei  Na,  Zhitao  Li,  Zhuo 
Peng, Zhuoshu Li, Ziqi Huang, Zoey Sun, Zohar Ya-
hav 

Thank you, all! 

The TFX Team … Extended 

Beyond  the  current  TFX  team  members,  there 
have  been  many  collaborators  both  within  and 
outside of Alphabet whose discussions, technol-
ogy, as well as direct and indirect contributions, 
have  materially  influenced  our  journey.  Below 
we  present  an  alphabetically  sorted  list  of  these 
collaborators: 

Abdulrahman Salem, Ahmet Altay, Ajay Gopinathan, 
Alexandre Passos, Alexey Volkov, Anand Iyer, Andrew 
Bernard,  Andrew  Pritchard,  Chary  Aasuri,  Chenkai 
Kuang, Chenyu Zhao, Chiu Yuen Koo, Chris Harris, 
Chris  Olston,  Christine  Robson,  Clemens  Mewald, 
Corinna Cortes, Craig Chambers, Cyril Bortolato, D. 
Sculley,  Daniel  Duckworth,  Daniel  Golovin,  David 
Soergel,  Denis  Baylor,  Derek  Murray,  Devi  Krishna, 
Ed  Chi,  Fangwei  Li,  Farhana  Bandukwala,  Gal  Eli-
dan,  Gary  Holt,  George  Roumpos,  Glen  Anderson, 
Greg  Steuck,  Grzegorz  Czajkowski,  Haakan  Younes, 
Heng-Tze Cheng, Hossein Attar, Hubert Pham, Hus-
sein Mehanna, Irene Cai, James L. Pine, James Pine, 
James Wu, Jeffrey Hetherly, Jelena Pjesivac-Grbovic, 
Jeremiah  Harmsen,  Jessie  Zhu,  Jiaxiao  Zheng,  Joe 
Lee, Jordan Soyke, Josh Cai, Judah Jacobson, Kaan 
Ege  Ozgun,  Kenny  Song,  Kester  Tong,  Kevin  Haas, 
Kevin Serafini, Kiril Gorovoy, Kostik Steuck, Kristen 
LeFevre,  Kyle  Weaver,  Kym  Hines,  Lana  Webb, 
Lichan  Hong,  Lukasz  Lew,  Mark  Omernick,  Martin 
Zinkevich,  Matthieu  Monsch,  Michel  Adar,  Michelle 
Tsai, Mike Gunter, Ming Zhong, Mohamed Hammad, 
Mona  Attariyan,  Mustafa  Ispir,  Neda  Mirian, 
Nicholas Edelman, Noah Fiedel, Panagiotis Voulgar-
is,  Paul  Yang,  Peter  Dolan,  Pushkar  Joshi,  Rajat 
Monga,  Raz  Mathias,  Reiner  Pope,  Rezsa  Farahani, 
Robert  Bradshaw,  Roberto  Bayardo,  Rohan  Khot, 
Salem  Haykal,  Sam  McVeety,  Sammy  Leong,  Samuel 

12

Ieong,  Shahar  Jamshy,  Slaven  Bilac,  Sol  Ma,  Stan 
Jedrus,  Steffen  Rendle,  Steven  Hemingray,  Steven 
Ross, Steven Whang, Sudip Roy, Sukriti Ramesh, Su-
san  Shannon,  Tal  Shaked,  Tushar  Chandra,  Tyler 
Akidau,  Venkat  Basker,  Vic  Liu,  Vinu  Rajashekhar, 
Xin Zhang, Yan Zhu, Yaxin Liu, Younghee Kwon, Yury 
Bychenkov, Zhenyu Tan 

Thank you, all! 

References 

[1] Titus Winters, Tom Manshreck, and Hyrum 
Wright, Software Engineering at Google. 
O’Reilly Media, 2020. 

[2] “Inside Sibyl, Google’s Massively Parallel 
Machine Learning Platform.” https://www.-
datanami.com/2014/07/17/inside-sibyl-googles-
massively-parallel-machine-learning-platform/ 
(accessed: Sep. 27, 2020). 

[3] “TensorFlow Extended (TFX) | ML Produc-
tion Pipelines.” https://www.tensorflow.org/tfx 
(accessed: Sep. 27, 2020). 

[4] D. Sculley, Gary Holt, Daniel Golovin, Eu-
gene Davydov, Todd Phillips, Dietmar Ebner, 
Vinay Chaudhary, and Michael Young. Machine 
learning: The high interest credit card of techni-
cal debt. In SE4ML: Software Engineering for 
Machine Learning (NeurIPS 2014 Workshop), 
2014. 

[5] S. Rendle, Factorization Machines. 2010 
IEEE International Conference on Data Mining, 
Sydney, NSW, 2010, pp. 995-1000, doi: 
10.1109/ICDM.2010.127. 

[6] “Sibyl: A system for large scale supervised 
machine learning.” https://users.soe.ucsc.edu/
~niejiazhong/slides/chandra.pdf (accessed: Sep. 
27, 2020). 

[7] Martin Abadi, Paul Barham, Jianmin Chen, 
Zhifeng Chen, Andy Davis, Jeffrey Dean, 
Matthieu Devin, Sanjay Ghemawat, Geoffrey 
Irving, Michael Isard, Manjunath Kudlur, Josh 
Levenberg, Rajat Monga, Sherry Moore, Derek 
G. Murray, Benoit Steiner, Paul Tucker, Vijay 
Vasudevan, Pete Warden, Martin Wicke, Yuan 
Yu, and Xiaoqiang Zheng. Tensorflow: A system 
for large-scale machine learning. In 12th 
USENIX Symposium on Operating Systems De-
sign and Implementation (OSDI 16), pages 265--
283, 2016.  

[8] Jeffrey Dean, Greg Corrado, Rajat Monga, 
Kai Chen, Matthieu Devin, Mark Mao, Marcau-
relio Ranzato, Andrew Senior, Paul Tucker, Ke 
Yang, Quoc V. Le, and Andrew Y. Ng. Large 
scale distributed deep networks. In F. Pereira, C. 
J. C. Burges, L. Bottou, and K. Q. Weinberger, 
editors, Advances in Neural Information Pro-
cessing Systems 25, pages 1223-1231. Curran 
Associates, Inc., 2012. 

[9] “TensorFlow.org.” https://www.tensor-
flow.org/ (accessed: Sep. 27, 2020). 

[10] Denis Baylor, Eric Breck, Heng-Tze Cheng, 
Noah Fiedel, Chuan Yu Foo, Zakaria Haque, 
Salem Haykal, Mustafa Ispir, Vihan Jain, Levent 
Koc, Chiu Yuen Koo, Lukasz Lew, Clemens 
Mewald, Akshay Naresh Modi, Neoklis Polyzo-
tis, Sukriti Ramesh, Sudip Roy, Steven Euijong 
Whang, Martin Wicke, Jarek Wilkiewicz, Xin 
Zhang, and Martin Zinkevich. TFX: A tensor-
flow-based production-scale machine learning 
platform. In Proceedings of the 23rd ACM 
SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD '17, 
page 1387–1395, New York, NY, USA, 2017. 
Association for Computing Machinery. http://
dx.doi.org/10.1145/3097983.3098021 

[11] Jeffrey Dean and Sanjay Ghemawat. 
Mapreduce: Simplified data processing on large 
clusters. In OSDI'04: Sixth Symposium on Oper-
ating System Design and Implementation, pages 
137--150, San Francisco, CA, 2004. 

13

[12] Craig Chambers, Ashish Raniwala, Frances 
Perry, Stephen Adams, Robert Henry, Robert 
Bradshaw, and Nathan. Flumejava: Easy, effi-
cient data-parallel pipelines. In ACM SIGPLAN 
Conference on Programming Language Design 
and Implementation (PLDI), pages 363-375, 2 
Penn Plaza, Suite 701 New York, NY 
10121-0701, 2010. 

[13] “Apache Beam.” https://beam.apache.org/ 
(accessed: Sep. 27, 2020). 

[14] “Introducing Cloud AI Platform Pipelines | 
Google Cloud Blog.” https://cloud.google.com/
blog/products/ai-machine-learning/introducing-
cloud-ai-platform-pipelines (accessed: Sep. 27, 
2020). 

[15] “TFX blog.” https://blog.tensorflow.org/
search?label=TFX (accessed: Sep. 27, 2020). 

[16] “The Winding Road to Better Machine 
Learning Infrastructure Through Tensorflow Ex-
tended and Kubeflow” https://engineering.atspo-
tify.com/2019/12/13/the-winding-road-to-better-
machine-learning-infrastructure-through-tensor-
flow-extended-and-kubeflow/ (accessed: Sep. 
27, 2020). 

[17] “Rules of Machine Learning: | ML Univer-
sal Guides” https://developers.google.com/ma-
chine-learning/guides/rules-of-ml (accessed: 
Sep. 27, 2020). 

[18] Rules of ML - YouTube. Accessed: Sep. 27, 
2020. [Online Video]. Available: https://www.y-
outube.com/watch?v=VfcY0edoSLU. 

[19] “ML Metadata | TFX | TensorFlow.” https://
www.tensorflow.org/tfx/guide/mlmd (accessed: 
Sep. 27, 2020). 

[20] Neoklis Polyzotis, Martin A. Zinkevich, 
Steven Whang, and Sudip Roy. Data manage-
ment challenges in production machine learning. 
In Proceedings of the 2017 ACM International 

Conference on Management of Data, pages 
1723-1726, New York, NY, USA, 2017. 

[21] Eric Breck, Marty Zinkevich, Neoklis Poly-
zotis, Steven Whang, and Sudip Roy. Data vali-
dation for machine learning. In Proceedings of 
SysML, 2019. 

[22] “The TFX User Guide | TensorFlow.” 
https://www.tensorflow.org/tfx/guide (accessed: 
Sep. 27, 2020). 

[23] Emily Caveness, Marty Zinkevich, Neoklis 
Polyzotis, Paul Suganthan, Sudip Roy, and Zhuo 
Peng. Tensorflow data validation: Data analysis 
and validation in continuous ml pipelines. In 
SIGMOD, 2020. 

[24] Neoklis Polyzotis, Steven Whang, Tim Klas 
Kraska, and Yeounoh Chung. Slice finder: Au-
tomated data slicing for model validation. In 
Proceedings of the IEEE Int' Conf. on Data En-
gineering (ICDE), 2019, 2019.  

[25] “TensorFlow Hub.” https://www.tensor-
flow.org/hub (accessed: Sep. 27, 2020). 

[26] Denis M. Baylor, Kevin Haas, Konstantinos 
Katsiapis, Sammy W Leong, Rose Liu, Clemens 
Mewald, Hui Miao, Neoklis Polyzotis, Mitch 
Trott, and Marty Zinkevich. Continuous training 
for production ml in the TensorFlow Extended 
(TFX) platform. In Proceedings of USENIX 
OpML 2019, 2019. 

[27] “RFC: TFX Advanced DSL semantics · 
Issue #253” https://github.com/tensorflow/
community/pull/253/files (accessed: Sep. 27, 
2020). 

[28] “Apache Arrow.” https://arrow.apache.org/  
(accessed: Sep. 27, 2020). 

[29] “Overview of Kubeflow Pipelines.” https://
www.kubeflow.org/docs/pipelines/overview/
pipelines-overview/ (accessed: Sep. 27, 2020). 

14

[30] “Apache Airflow.” https://airflow.a-
pache.org/ (accessed: Sep. 27, 2020). 

[31] “Apache Flink: Stateful Computations over 
Data Streams.” https://flink.apache.org/ (ac-
cessed: Sep. 27, 2020). 

[32] “Apache Spark™ - Unified Analytics En-
gine for Big Data.” https://spark.apache.org/ 
(accessed: Sep. 27, 2020). 

[33] “Dataflow | Google Cloud.” https://cloud.-
google.com/dataflow (accessed: Sep. 27, 2020). 

[34] “SQLite Home Page.” https://
www.sqlite.org/ (accessed: Sep. 27, 2020). 

[35] “MySQL.” https://www.mysql.com/ (ac-
cessed: Sep. 27, 2020). 

Noah Fiedel, Sukriti Ramesh, and Vinu Ra-
jashekhar. Tensorflow-Serving: Flexible, high-
performance ML serving. In Workshop on ML 
Systems at NeurIPS 2017, 2017. 

[42] “TensorFlow Lite guide.” https://www.ten-
sorflow.org/lite/guide (accessed: Sep. 27, 2020). 

[43] “Get Started | TensorFlow.js.” https://
www.tensorflow.org/js/tutorials (accessed: Sep. 
27, 2020). 

[44] “Cloud AutoML - Custom Machine Learn-
ing Models | Google Cloud.” https://cloud.-
google.com/automl (accessed: Sep. 27, 2020). 

[45] “Recommendations AI - Google Cloud.” 
https://cloud.google.com/recommendations (ac-
cessed: Sep. 27, 2020). 

[36] “How to Contribute.” https://github.com/
tensorflow/tfx/blob/master/CONTRIBUT-
ING.md (accessed: Sep. 27, 2020). 

[46] “AI Platform | Google Cloud.” https://
cloud.google.com/ai-platform (accessed: Sep. 
27, 2020). 

[37] Abhishek Verma, Luis Pedrosa, Madhukar 
R. Korupolu, David Oppenheimer, Eric Tune, 
and John Wilkes. Large-scale cluster manage-
ment at Google with Borg. In Proceedings of the 
European Conference on Computer Systems 
(EuroSys), Bordeaux, France, 2015. 

[38] “Google Cloud: Cloud Computing 
Services.” https://cloud.google.com/ (accessed: 
Sep. 27, 2020). 

[39] “Cloud TPU | Google Cloud.” https://
cloud.google.com/tpu (accessed: Sep. 27, 2020). 

[40] Norman P. Jouppi, Doe Hyun Yoon, George 
Kurian, Sheng Li, Nishant Patil, James Laudon, 
Cliff Young, and David Patterson. A domain-
specific supercomputer for training deep neural 
networks. Commun. ACM, 63(7):67–78, June 
2020. https://dl.acm.org/doi/10.1145/3360307 

[41] Christopher Olston, Fangwei Li, Jeremiah 
Harmsen, Jordan Soyke, Kiril Gorovoy, Li Lao, 

[47] “TensorFlow Extended (TFX): Using 
Apache Beam for large scale data processing.” 
https://blog.tensorflow.org/2020/03/tensorflow-
extended-tfx-using-apache-beam-large-scale-
data-processing.html (accessed: Sep. 27, 2020). 

[48] “BigQuery: Cloud Data Warehouse | 
Google Cloud.” https://cloud.google.com/big-
query (accessed: Sep. 27, 2020). 

[49] “tensorflow/tfx - GitHub.” https://github.-
com/tensorflow/tfx (accessed: Sep. 27, 2020). 

[50] “tensorflow/serving - GitHub.” https://
github.com/tensorflow/serving (accessed: Sep. 
27, 2020). 

[51] “tensorflow/data-validation - GitHub.” 
https://github.com/tensorflow/data-validation 
(accessed: Sep. 27, 2020). 

15

[52] “tensorflow/transform - GitHub.” https://
github.com/tensorflow/transform  (accessed: 
Sep. 27, 2020). 

[53] “tensorflow/hub - GitHub.” https://github.-
com/tensorflow/hub (accessed: Sep. 27, 2020). 

[54] “tensorflow/model-analysis - GitHub.” 
https://github.com/tensorflow/model-analysis 
(accessed: Sep. 27, 2020). 

[55] “tensorflow/tfx-bsl - GitHub.” https://
github.com/tensorflow/tfx-bsl (accessed: Sep. 
27, 2020). 

[56] “google/ml-metadata - GitHub.” https://
github.com/google/ml-metadata (accessed: Sep. 
27, 2020). 

[57] “Guide | TensorFlow Core.” https://
www.tensorflow.org/guide (accessed: Sep. 27, 
2020). 

[58] “Fairness Indicators | TensorFlow.” https://
www.tensorflow.org/tfx/fairness_indicators (ac-
cessed: Sep. 27, 2020). 

[59] “TFX Roadmap · GitHub.” https://github.-
com/tensorflow/tfx/blob/master/ROADMAP.md 
(accessed: Sep. 27, 2020). 

[60] “TFX RFCs - GitHub. ” https://github.com/
tensorflow/tfx/blob/master/RFCs.md (accessed: 
Sep. 27, 2020). 

[61] “Responsible AI | TensorFlow.” https://
www.tensorflow.org/resources/responsible-ai 
(accessed: Sep. 27, 2020). 

[62] “Our Principles – Google AI.” https://ai.-
google/principles/ (accessed: Sep. 27, 2020). 

[63] “RFC: Standardized TFX Inputs” https://
github.com/tensorflow/community/blob/master/
rfcs/20191017-tfx-standardized-inputs.md (ac-
cessed: Sep. 27, 2020). 

[64] “RFC: TFX Advanced DSL semantics” 
https://github.com/tensorflow/community/pull/
253/files/102bc5f3beb6ff756d147e7cbf88ff-
b5422a9f5c (accessed: Sep. 27, 2020). 

[65] “RFC: TFX DSL Data Model and IR” 
https://github.com/tensorflow/community/pull/
271 (accessed: Sep. 27, 2020). 

[66] “RFC: TFX Tuner Component” https://
github.com/tensorflow/community/blob/master/
rfcs/20200420-tfx-tuner-component.md (ac-
cessed: Sep. 27, 2020). 

[67] “Google Cloud Model Cards.” https://mod-
elcards.withgoogle.com/ (accessed: Sep. 27, 
2020). 

[68] “Introducing the Model Card Toolkit for 
Easier Model Transparency Reporting - Google 
AI Blog.” http://ai.googleblog.com/2020/07/in-
troducing-model-card-toolkit-for.html (accessed: 
Sep. 27, 2020). 

[69] “struct2tensor - GitHub.” https://github.-
com/google/struct2tensor (accessed: Sep. 27, 
2020). 

[70] NitroML: Accelerate AutoML Research. 
(Jul. 21, 2020). Accessed: Sep. 27, 2020. [Online 
Video]. Available: https://www.youtube.com/
watch?v=SagSL38Kx0Q. 

16

