Decoupled Data Based Approach for Learning to Control Nonlinear
Dynamical Systems

Ran Wang1, Karthikeya Parunandi1, Dan Yu2, Dileep Kalathil3, Suman Chakravorty1

9
1
0
2

r
p
A
7
1

]

G
L
.
s
c
[

1
v
1
6
3
8
0
.
4
0
9
1
:
v
i
X
r
a

Abstract— This paper addresses the problem of learning
the optimal control policy for a nonlinear stochastic dynam-
ical system with continuous state space, continuous action
space and unknown dynamics. This class of problems are
typically addressed in stochastic adaptive control and rein-
forcement learning literature using model-based and model-
free approaches respectively. Both methods rely on solving a
dynamic programming problem, either directly or indirectly,
for ﬁnding the optimal closed loop control policy. The inherent
‘curse of dimensionality’ associated with dynamic programming
method makes these approaches also computationally difﬁcult.
This paper proposes a novel decoupled data-based control
(D2C) algorithm that addresses this problem using a decou-
pled, ‘open loop - closed loop’, approach. First, an open-
loop deterministic trajectory optimization problem is solved
using a black-box simulation model of the dynamical system.
Then, a closed loop control
is developed around this open
loop trajectory by linearization of the dynamics about this
nominal trajectory. By virtue of linearization, a linear quadratic
regulator based algorithm can be used for this closed loop
control. We show that the performance of D2C algorithm
is approximately optimal. Moreover, simulation performance
suggests signiﬁcant reduction in training time compared to
other state of the art algorithms.

I. INTRODUCTION

Controlling an unknown dynamical system adaptively has
a rich history in control literature [1] [2]. These classical lit-
erature provides rigorous analysis about the asymptotic per-
formance and stability of the closed loop system. Classical
adaptive control literature mainly focuses on non-stochastic
systems [3] [4]. Stochastic adaptive control literature mostly
addresses tractable models like linear quadratic regulator
(LQR) where Riccati equation based closed form expressions
are available for the optimal control law. Optimal control
of an unknown nonlinear dynamical system with continuous
state space and continuous action space is a signiﬁcantly
more challenging problem. Even with a known model, com-
puting an optimal control law requires solving a dynamic
programming problem. The ‘curse of dimensionality’ as-
sociated with dynamic programming makes solving such
problems computationally intractable, except under special
structural assumptions on the underlying system. Learning to

1R. Wang, K. Parunandi

Department
Texas,
USA.
schakrav}@tamu.edu

the
of Aerospace Engineering, Texas A&M University,
{rwang0417@tamu.edu, s.parunandi,

and S. Chakravorty

are with

2D.Yu is with the College of Astronautics, Nanjing University
China.

Astronautics,

Aeronautics

Nanjing,

210016,

of
yudan@nuaa.edu.cn

and

3D. Kalathil

is with

Computer
Engineering,
dileep.kalathil@tamu.edu

the Department
Texas A&M University,

of

Electrical
and
Texas, USA.

control problems where the model of the system is unknown
also suffer from this computational complexity issues, in
addition to the usual
identiﬁability problems in adaptive
control.

Last few years have seen signiﬁcant progresses in deep
neural netwoks based reinforcement learning approaches for
controlling unknown dynamical systems, with applications
in many areas like playing games [5], locomotion [6] and
robotic hand manipulation [7]. A number of new algo-
rithms that show promising performance are proposed [8]
[9] [10] and various improvements and innovations have
been continuously developed. However, despite excellent
performance on a number of tasks, reinforcement learning
(RL) is still considered very data intensive. The training time
for such algorithms are typically really large. Moreover, high
variance and reproducibility issues on the performance are
also reported [11]. While there have been some attempts to
improve the sample efﬁciency [12], a systematic approach is
still lacking.

In this work, we propose a novel decoupled data based
control (D2C) algorithm for learning to control an unknown
nonlinear dynamical system. Our approach introduces a rig-
orous decoupling of the open loop (planning) problem from
the closed loop (feedback control) problem. This decoupling
allows us to come up with a highly sample efﬁcient approach
to solve the problem in a completely data based fashion.
Our approach proceeds in two steps: (i) ﬁrst, we optimize
the nominal open loop trajectory of the system using a
blackbox simulation model, (ii) then we identify the linear
system governing perturbations from the nominal trajectory
using random input-output perturbation data, and design an
LQR controller for this linearized system. We show that the
performance of D2C algorithm is approximately optimal, in
the sense that the decoupled design is near optimal to second
order in a suitably deﬁned noise parameter. Moreover, simu-
lation performance suggests signiﬁcant reduction in training
time compared to other state of the art algorithms.

Related works: The solution approaches to the problem
of controlling an unknown dynamical systems can be divided
into two broad classes, model-based methods and model-free
methods.

In the model-based methods, many techniques [13] rely
on a discretization of the underlying state and action space,
and hence, run into the curse of dimensionality, the fact that
the computational complexity grows exponentially with the
dimension of the state space of the problem. The most com-
putationally efﬁcient among these techniques are trajectory-
based methods such as differential dynamic programming

 
 
 
 
 
 
(DDP) [14] [15] which linearizes the dynamics and the cost-
to-go function around a given nominal trajectory, and designs
a local feedback controller using DP. The iterative linear
quadratic Gaussian (ILQG) [16] [17], which is closely related
to DDP, considers the ﬁrst order expansion of the dynamics
(in DDP, a second order expansion is considered), and
designs the feedback controller using Riccati-like equations,
and is shown to be computationally more efﬁcient. In both
approaches, the control policy is executed to compute a
new nominal trajectory, and the procedure is repeated until
convergence.

Model-free methods, more popularly known as approx-
imate dynamic programming [18] [19] or reinforcement
learning (RL) methods [20], seek to improve the control
policy by repeated interactions with the environment, while
observing the system’s responses. The repeated interactions,
or learning trials, allow these algorithms to compute the
solution of the dynamic programming problem (optimal
value/Q-value function or optimal policy) without explicitly
constructing the model of the unknown dynamical system.
Standard RL algorithms are broadly divided into value-based
methods, like Q-learning, and policy-based methods, like
policy gradient algorithms. Recently, function approximation
using deep neural networks has signiﬁcantly improved the
performance of reinforcement learning algorithm, leading
to a growing class of literature on ‘deep reinforcement
learning’. Despite the success, the amount of samples and
training time required still seem prohibitive.

Our Contributions: Rather than directly ﬁnding the
closed loop control law which requires solving a dynamic
programming problem, our approach addresses the original
stochastic control problem in a ’decoupled open loop -
closed loop’ fashion. In this approach: i) we solve an open
loop deterministic optimization problem to obtain an optimal
nominal trajectory in a model-free fashion, and then ii) we
design a closed loop controller for the resulting linearized
time-varying system around the optimal nominal trajectory,
in a model-based fashion. This ‘divide and conquer’ strategy
can be shown to be extremely effective. In this context,
our major contributions are: 1) we show a near optimal
parametrization of the feedback policy in terms of an open
loop control sequence, and a linear feedback control law,
2) we show rigorously that the open loop and closed loop
learning can be decoupled, which 3) results in the D2C
algorithm that is highly data efﬁcient when compared to
state of the art RL techniques. This paper is a rigorous
extension of our preliminary work [21], [22], in particular,
it includes a stronger decoupling result, and an extensive
empirical evaluation of the D2C algorithm with state of the
art RL implementations on standard benchmark problems.

The rest of the paper is organized as follows. In Section
II, the basic problem formulation is outlined. In Section III,
a decoupling result which solves the MDP in a “decoupled
open loop-closed loop ” fashion is brieﬂy summarized. In
Section IV, we propose a decoupled data based control
algorithm, with discussions of implementation problems. In
Section V, we test the proposed approach using four typical

benchmarking examples with comparisons to a state of the
art RL technique.

II. PROBLEM FORMULATION

Consider the following discrete time nonlinear stochastic

dynamical system:

xt+1 = h(xt, ut, wt),

(1)

where xt ∈ Rnx, ut ∈ Rnu are the state measurement and
control vector at time k, respectively. The process noise wt
is assumed as zero-mean, uncorrelated Gaussian white noise,
with covariance W .

The optimal stochastic control problem is to ﬁnd the
1, πo
T −1} such that the

the control policy πo = {πo
expected cumulative cost is minimized, i.e.,

2, · · · , πo

πo = arg min
π
(cid:34)T −1
(cid:88)

˜J π(x) = Eπ

t=1

˜J π(x), where,

c(xt, ut) + cT (xT )|x1 = x

,

(2)

(cid:35)

ut = πt(xt), c(·, ·) is the instantaneous cost function, and
cT (·) is the terminal cost function. In the following, we
assume that the initial state x1 is ﬁxed, and denote ˜J π(x)
simply as ˜J π.

If the function h(·, ·, ·) is known exactly, then the optimal
control law πo can be computed using dynamic program-
ming method. However, as noted before, this can be often
computationally intractable. Moreover, when h is unknown,
designing an optimal closed loop control law is a much
more challenging problem. In the following, we propose a
data based decoupled approach for solving (2) when h is
unknown.

III. A NEAR OPTIMAL DECOUPLING PRINCIPLE

We ﬁrst outline a near-optimal decoupling principle in
stochastic optimal control that paves the way for the D2C
algorithm described in Section IV.

We make the following assumptions for the simplicity of
illustration. We assume that the dynamics given in (1) can
be written in the form

xt+1 = f (xt) + Bt(ut + (cid:15)wt),

(3)

where (cid:15) < 1 is a small parameter. We also assume that the
instantaneous cost c(·, ·) has the following simple form,

c(x, u) = l(x) +

1
2

u(cid:48)Ru.

(4)

We emphasis that these assumptions, quadratic control cost
and afﬁne in control dynamics, are purely for the simplicity
of treatment. These assumptions can be omitted at the cost
of increased notational complexity.

A. Linearization w.r.t. Nominal Trajectory

Consider a noiseless version of the system dynamics given
by (3). We denote the “nominal” state trajectory as ¯xt and
the “nominal” control as ¯ut where ut = πt(xt), where π =
(πt)T −1
t=1 is a given control policy. The resulting dynamics
without noise is given by ¯xt+1 = f (¯xt) + Bt ¯ut.

Assuming that f (·) and πt(·) are sufﬁciently smooth, we
trajectory.

can linearize the dynamics about
Denoting δxt = xt − ¯xt, δut = ut − ¯ut, we can express,

the nominal

δxt+1 = Atδxt + Btδut + St(δxt) + (cid:15)wt,

(5)

δut = Ktδxt + ˜St(δxt),
∂x |¯xt, Kt = ∂πt

(6)
∂x |¯xt , and St(·), ˜St(·) are
where At = ∂f
second and higher order terms in the respective expansions.
Similarly, we can linearize the instantaneous cost c(xt, ut)
about the nominal values (¯xt, ¯ut) as,

c(xt, ut) = l(¯xt) + Ltδxt + Ht(δxt)+

1
2

tR¯ut + δu(cid:48)
¯u(cid:48)

tR¯ut + δu(cid:48)

tRδut,

cT (xT ) = cT (¯xT ) + CT δxT + HT (δxT ),

(7)

(8)

∂x |¯xt, CT = ∂cT

where Lt = ∂l
∂x |¯xt, and Ht(·) and HT (·) are
second and higher order terms in the respective expansions.
Using (5) and (6), we can write the closed loop dynamics

of the trajectory (δxt)T

δxt+1 = (At + BtKt)
(cid:125)

(cid:124)

(cid:123)(cid:122)
¯At

t=1 as,
δxt + {Bt ˜St(δxt) + St(δxt)}
(cid:125)

(cid:124)

(cid:123)(cid:122)
¯St(δxt)

+(cid:15)wt,

(9)
where ¯At represents the linear part of the closed loop systems
and the term ¯St(.) represents the second and higher order
terms in the closed loop system. Similarly, the closed loop
incremental cost given in (7) can be expressed as
1
2
(cid:123)(cid:122)
¯ct

¯u(cid:48)
tR¯ut}
(cid:125)
+ (Ktδxt + ˜St(δxt))(cid:48)R(Ktδxt + ˜St(δxt))
(cid:123)(cid:122)
(cid:125)
¯Ht(δxt)

+ [Lt + ¯u(cid:48)
(cid:123)(cid:122)
¯Ct

c(xt, ut) = {l(¯xt) +

tRKt]
(cid:125)

δxt

(cid:124)

(cid:124)

(cid:124)

Therefore, the cumulative cost of any given closed loop
t=1 can be expressed as,

trajectory (xt, ut)T

J π =

=

T −1
(cid:88)

t=1
T
(cid:88)

t=1

c(xt, ut = πt(xt)) + cT (xT )

¯ct +

T
(cid:88)

t=1

¯Ctδxt +

T
(cid:88)

t=1

¯Ht(δxt),

(11)

where ¯cT = cT (¯xT ), ¯CT = CT .

We ﬁrst show the following result.
Lemma 1: The state perturbation equation
δxt+1 = ¯Atδxt + ¯St(δxt) + (cid:15)wt

given in (9) can be equivalently characterized as
t + ¯¯St, δxl

t+1 = ¯Atδxl

δxt = δxl

t + (cid:15)wt

where ¯¯St is an O((cid:15)2) function that depends on the entire
noise history {w0, w1, · · · wt} and δxl
t evolves according to
the linear closed loop system.
Proof is omitted due to page limits. Detailed proof is given
in [23].

Using (12) in (11), we can obtain the cumulative cost of

any given closed loop trajectory as,

J π =

T
(cid:88)

¯ct

+

T
(cid:88)

¯Ctδxl
t

+

t=1
(cid:124) (cid:123)(cid:122) (cid:125)
¯J π

t=1
(cid:124)

(cid:123)(cid:122)
δJ π
1

(cid:125)

T
(cid:88)

t=1
(cid:124)

¯Ht(δxt) + ¯Ct

¯¯St

.

(13)

(cid:123)(cid:122)
δJ π
2

(cid:125)

Now, we show the following important result.
Proposition 1:

˜J π = E[J π] = ¯J π + O((cid:15)2),
Var(J π) = Var(δJ π
1 )
(cid:125)

+O((cid:15)4).

(cid:124)

(cid:123)(cid:122)
O((cid:15)2)
Proof: From (13), we get,

˜J π = E[J π] = E[ ¯J π + δJ π

= ¯J π + E[δJ π

1 + δJ π
2 ],
2 ] = ¯J π + O((cid:15)2),

(14)

The ﬁrst equality in the last line of the equations before
follows from the fact that E[δxl
t] = 0, since its the linear
part of the state perturbation driven by white noise and by
deﬁnition δxl
1 = 0.The second equality follows form the fact
that δJ π

2 is an O((cid:15)2) function. Now,

Var(J π) = E[J π − ˜J π]2
0 − δ ˜J π
2 − ¯J π
2 ]2
1 + δJ π
2 ) + 2E[δJ π
1 δJ π
2 ].

0 + δJ π
1 ) + Var(δJ π

= E[ ¯J π
= Var(δJ π

(15)

2 is O((cid:15)2), Var(δJ π

2 ) is an O((cid:15)4) function. It can
Since δJ π
be shown that E[δJ π
1 δJ π
2 ] is O((cid:15)4) as well (proof is given
[23]). Finally Var(δJ π
1 ) is an O((cid:15)2) function because δxl is
an O((cid:15)) function. Combining these, we will get the desired
result.

The following observations can now be made from Propo-

Remark 1 (Expected cost-to-go): Recall that ut = πt(xt)
= ¯ut + Ktδxt + ˜St(δxt). However, note that due to Propo-
sition 1, the expected cost-to-go, ˜J π, is determined almost
solely (within O((cid:15)2)) by the nominal control action sequence
¯ut. In other words, the linear and higher order feedback terms
have only O((cid:15)2) effect on the expected cost-to-go function.
Remark 2 (Variance of cost-to-go): Given the nominal
control action ¯ut, the variance of the cost-to-go, which is
O((cid:15)2), is determined overwhelmingly (within O((cid:15)4)) by the
linear feedback term Ktδxt, i.e., by the variance of the linear
perturbation of the cost-to-go, δJ π
1 , under the linear closed
loop system δxl

t+1 = (At + BtKt)δxl

t + (cid:15)wt.

B. Decoupled Approach for Closed Loop Control

Proposition 1 and the remarks above suggest that an open
loop control super imposed with a closed loop control for
the perturbed linear system may be approximately optimal.
We delineate this idea below.

(12)

.

(10)

sition 1.

Open Loop Design. First, we design an optimal (open
t for the noiseless system. More

loop) control sequence ¯u∗
precisely,

Approximate Closed Loop Problem. We solve the fol-
lowing LQR problem for suitably deﬁned cost function
weighting factors Qt, Rt:

T −1
(cid:88)

c(¯xt, ˜ut) + cT (¯xT ),

(16)

(¯u∗

t )T −1

t=1 = arg min
(˜ut)T −1
t=1

t=1
¯xt+1 = f (¯xt) + Bt ˜ut.

We will discuss the details of this open loop design in Section
IV.

Closed Loop Design. We ﬁnd the optimal feedback gain
t such that the variance of the linear closed loop system
t ), from the open loop design

K ∗
around the nominal path, (¯xt, ¯u∗
above, is minimized.

(K ∗

t )T −1

t=1 = arg min

(Kt)T −1
t=1

Var(δJ π

1 ),

δJ π

1 =

T
(cid:88)

t=1

¯Ctxl
t,

δxl

t+1 = (At + BtKt)δxl

t + (cid:15)wt.

(17)

min
(δut)T

t=1

T −1
(cid:88)

E[

t=1

δx(cid:48)

tQtδxt + δu(cid:48)

tRtδut + δx(cid:48)

T QT δxt],

δxt+1 = Atδxt + Btδut + (cid:15)wt.

(19)

The solution to the above problem furnishes us a feedback
gan ˆK ∗
t which we can use in the place of the true variance
minimizing gain K ∗
t .

Remark 3: Proposition 1 states that the expected cost-to-
go of the problem is dominated by the nominal cost-to-go.
Therefore, even an open loop policy consisting of simply
the nominal control action is within O((cid:15)2) of the optimal
expected cost-to-go. However,
the plan with the optimal
feedback gain K ∗
is strictly better than the open loop plan
t
in that it has a lower variance in terms of the cost to go.
Furthermore, solving the approximate closed loop problem
using the surrogate LQR problem, we can expect a lower
variance of the cost-to-go function as well.

We now characterize the approximate closed loop policy
below.

IV. DECOUPLED DATA BASED CONTROL (D2C)
ALGORITHM

Proposition 2: Construct a closed loop policy

t (xt) = ¯u∗
π∗

t + K ∗

t δxt,

(18)

t is the solution of the open loop problem (16), and
t is the solution of the closed loop problem (17). Let πo

where ¯u∗
K ∗
be the optimal closed loop policy. Then,

| ˜J π∗ − ˜J πo

| = O((cid:15)2).

Furthermore, among all policies with nominal control action
t , the variance of the cost-to-go under policy π∗
¯u∗
t , is within
O((cid:15)4) of the variance of the policy with the minimum
variance.

Proof: We have

˜J π∗

− ˜J πo

= ˜J π∗
≤ ˜J π∗

− ¯J π∗
− ¯J π∗

+ ¯J π∗
+ ¯J πo

− ˜J πo
− ˜J πo

The inequality above is due the fact that ¯J π∗
, by
deﬁnition of π∗. Now, using Proposition 1, we have that
− ¯J πo
| ˜J π∗
| = O((cid:15)2). Also,
by deﬁnition, we have ˜J πo
. Then, from the above
inequality, we get

| = O((cid:15)2), and | ˜J πo

≤ ¯J πo

≤ ˜J π∗

− ¯J π∗

| ˜J π∗

− ˜J πo

| ≤ | ˜J π∗

− ¯J π∗

| + | ¯J πo

− ˜J πo

| = O((cid:15)2)

A similar argument holds for the variance as well.

Unfortunately, there is no standard solution to the closed
loop problem (17) due to the non additive nature of the
cost function Var(δJ π
1 ). Therefore, we solve a standard
LQR problem as a surrogate, and the effect is again one
of reducing the variance of the cost-to-go by reducing the
variance of the closed loop trajectories.

In this section, we propose a novel decoupled data based
control (D2C) algorithm formalizing the ideas proposed in
Section III. First, a noiseless open-loop optimization prob-
lem is solved to ﬁnd a nominal optimal trajectory. Then
a linearized closed-loop controller is designed around this
nominal trajectory, such that, with existence of stochastic
perturbations, the state stays close to the optimal open-loop
trajectory. The three-step framework to solve the stochastic
feedback control problem may be summarized as follows.

• Solve the open loop optimization problem using gradi-
ent decent with a black box simulation model of the
dynamics.

• Linearize the system around the nominal open loop
optimal
trajectory, and identify the linearized time-
varying system from input-output experiment data using
a suitable system identiﬁcation algorithm.

• Design an LQR controller which results in an optimal
linear control policy around the nominal trajectory.
In the following section, we discuss each of the above steps.

A. Open Loop Trajectory Optimization

A ﬁrst order gradient descent based algorithm is proposed
here for solving the open loop optimization problem given
in (16), where the underlying dynamic model is used as
a blackbox, and the necessary gradients are found from
a sequence of input perturbation experiment data using
standard least square.

Denote the initial guess of the control sequence as U (0) =
t=1, and the corresponding states X (0) = {¯x(0)
t }T
t=1.

{¯u(0)
The control policy is updated iteratively via

t }T

U (n+1) = U (n) − α∇U ¯J|X (n),U (n),

(20)

t }T

where U (n) = {¯u(n)
t=1 denotes the control sequence in the
nth iteration, X (n) = {¯x(n)
t=1 denotes the corresponding
states, and α is the step size parameter. As ¯J|X (n),U (n) is the
expected cumulative cost under control sequence U (n) and
corresponding states X (n), the gradient vector is deﬁned as

t }T

∇U ¯J|X (n),U (n) =

(cid:16) ∂ ¯J
∂u1

∂ ¯J
∂u1

· · ·

(cid:17)

∂ ¯J
∂uT

|X (n),U (n) ,

(21)

which is the gradient of the expected cumulative cost w.r.t the
control sequence after n iterations. The following paragraph
elaborates on how to estimate the above gradient.

Let us deﬁne a rollout to be an episode in the simulation
that starts from the initial settings to the end of the horizon
with a control sequence. For each iteration, multiple rollouts
are conducted sequentially with both the expected cumulative
cost and the gradient vector updated iteratively after each
rollout. During one iteration for the control sequence, the
expected cumulative cost is calculated as

¯J|j+1

X (n),U (n) = (1 −

1
j

) ¯J|j

X (n),U (n) +

1
j

(J|X j,(n),U j,(n) ),

(22)

X (n),U (n)

where j denotes the jth rollout within the current iteration
process of control sequence. ¯J|j
is the expected
cumulative cost after j rollouts while J|X j,(n),U j,(n) de-
notes the cost of the jth rollout under control sequence
U j,(n) and corresponding states X j,(n). Note that U j,(n) =
{¯u(n)
}T
t=1 is the zero-mean, i.i.d
t
Gaussian noise added as perturbation to the control sequence
U (n).

t=1 where {δuj,(n)
}T

t +δuj,(n)

t

Then the gradient vector is calculated in a similar sequen-

tial manner as

∇U ¯J|j+1

X (n),U (n) = (1 −

1
j

)∇U ¯J|j

X (n),U (n) +

1
jσδu

(J|X j,(n),U j,(n) − ¯J|j+1

X (n),U (n))(U j,(n) − U (n)),

(23)

where σδu is the variance of the control perturbation and
∇U ¯J|j+1
X (n),U (n) denotes the gradient vector after j rollouts.
Note that after each rollout, both the expected cumulative
cost and the gradient vector are updated. The rollout number
m in one iteration for the control sequence is decided by
the convergence of both the expected cumulative cost and
the gradient vector. After m rollouts, the control sequence
is updated by equation (20) in which ∇U ¯J|X (n),U (n)
is
estimated by ∇U ¯J|m
X (n),U (n) . Keep doing this until the cost
converges and the optimized nominal control sequence is
{¯u∗

t }T
Higher order approaches other than gradient descent are
possible. However, for a general system, the gradient descent
approach is easy to implement. Also it is memory efﬁcient
and highly amenable to parallelization as a result of our
sequential approach.

t=1 = {¯u(N −1)

}T
t=1.

t

B. Linear Time-Varying System Identiﬁcation

Closed loop control design speciﬁed in (17) or the approx-
imate closed loop control design speciﬁed in (19) requires

the knowledge of the parameters At, Bt, 1 ≤ t ≤ T,
of the perturbed linear system. We propose a linear time
variant (LTV) system identiﬁcation procedure to estimate
these parameters.

First start from perturbed linear system given by equation
(19). Using only ﬁrst order information and estimate the
system parameters At, Bt with the following form

δxt+1 = ˆAtδxt + ˆBtδut,

rewrite with augmented matrix

δxt+1 = [ ˆAt | ˆBt]

(cid:21)

(cid:20)δxt
δut

,

(24)

(25)

Now write out their components for each iteration in vector
form as,

t+1 · · · δxN −1
t+1δx1
Y = [δx0
t+1 ],
(cid:20)δx0
δxN −1
δx1
· · ·
t
t
t
δuN −1
δu1
δu0
· · ·
t
t
t
Y = [ ˆAt | ˆBt]X,

X =

(cid:21)

,

(26)

where N is the total iteration number. δxn
t+1 denotes the
output state deviation , δxn
t denotes the input state perturba-
tions and δun
t denotes the input control perturbations at time
t of the nth iteration. All the perturbations are zero-mean,
i.i.d, Gaussian random vectors whose covariance matrix is
σI where I is the identity matrix and σ is a scalar. Note that
here one iteration only has one rollout.

The next step is to apply the perturbed control {¯u∗
t }T

t +
t=1 to the system and collect input-output experiment

δun
data in a blackbox simulation manner.

Using the least square method ˆAt and ˆBt can be calculated

in the following procedure

Y X (cid:48) = [ ˆAt | ˆBt]XX (cid:48),

(27)

As the perturbations are zero-mean, i.i.d, Gaussian random
noise, XX (cid:48) = σI. Then

[ ˆAt | ˆBt] =

1
σ

Y X (cid:48)

=

1
σ

[δx0

t+1δx1

t+1 · · · δxN −1
t+1 ]








t )(cid:48)
t )(cid:48)

(δx0
(δx1
...
(δxN −1
t








t )(cid:48)
t )(cid:48)

(δu0
(δu1
...
(δuN −1
t

)(cid:48)

)(cid:48)

(28)

The calculation procedure can also be done in a sequential
way similar to the update of the gradient vector in the open-
loop optimization algorithm. Therefore it is highly amenable
to parallelization and memory efﬁcient.

C. Closed Loop Control Design

Given the parameter estimate of the perturbed linear
system, we solve the closed loop control problem given in
(19). This is a standard LQR problem. By solving the Riccati
equation, we can get the closed-loop optimal feedback gain
K ∗
t . The details of the design is standard and is omitted here.

t }T

t=1, {¯x∗

t=1) using

Algorithm 1: D2C Algorithm
1) Solve the deterministic open-loop optimization
problem for optimal open loop nominal control
sequence and trajectory ({¯u∗
t }T
gradient descent method (Section IV-A).
2) Identify the LTV system ( ˆAt, ˆBt) via least square
estimation (Section IV-B).
3) Solve the Riccati equations using estimated LTV
system equation for feedback gain {K ∗
4) Set t = 1, given initial state x1 = ¯x∗
deviation δx1 = 0.
while t ≤ T do

t }T
t=1.
1 and state

t + K ∗

ut = ¯u∗
t δxt,
xt+1 = f (xt) + Bt(ut + (cid:15)wt),
δxt+1 = xt+1 − ¯x∗

t+1

(29)

t = t + 1.

end while

D. D2C Algorithm: Summary

The Decoupled Data Based Control (D2C) Algorithm is

summarized in Algorithm 1.

V. SIMULATION RESULTS
In this section, we compare the D2C approach with the
well-known deep reinforcement learning algorithm - Deep
Deterministic Policy Gradient (DDPG) [24]. For the com-
parison, we evaluate both the methods in the following three
aspects:

• Data efﬁciency in training - the amount of data sampling

and storage required to achieve a desired task.

• Robustness to noise - the deviation from the predeﬁned
task due to random noise in process in the testing stage.
• Ease of training - the challenges involved in training

with either of the data-based approaches.

A. Tasks and Implementation

We tested our method with four benchmark tasks, all
implemented in MuJoCo simulator [25]: Inverted pendulum,
Cart-pole, 3-link swimmer and 6-link swimmer [26]. Each
of the systems and their tasks are brieﬂy deﬁned as follows:
a) Inverted pendulum: A swing-up task of this 2D

system from its downright initial position is considered.

b) Cart-pole: The state of a 4D under-actuated cart-
pole comprises of the angle of the pole, cart’s horizontal
position and their rates. Within a given horizon, the task is
to swing-up the pole and balance it at the middle of the rail
by applying a horizontal force on the cart.

c) 3-link Swimmer: The 3-link swimmer model has 5
degrees of freedom and together with their rates, the system
is described by 10 state variables. The task is to solve the
planning and control problem from a given initial state to the
goal position located at the center of the ball. Controls can
only be applied in the form of torques to two joints. Hence,
it is under-actuated by 3 DOF.

d) 6-link Swimmer: The task with a 6-link swimmer
model is similar to that deﬁned in the 3-link case. However,
with 6 links, it has 8 degrees of freedom and hence, 16 state
variables, controlled by 5 joint motors.

D2C implementation is done in three stages corresponding
to those mentioned in the previous section and ‘MuJoCo
Pro 150’ is used as the environment for simulating the
blackbox model. An off-the-shelf implementation of DDPG
provided by Keras-RL [27] library has been customized for
our simulations. It may be noted that the structure of the
reward function is formulated to optimize the performance of
DDPG and hence, different from that used in D2C. However,
the episode length (or horizon) and the time-discretization
step is held identical. Although continuous RL algorithms
such as DDPG learn the policy and thereby interpolating
it
to the entire state space, we consider the task-based
experiments in a ﬁnite limited time-frame window approach.
Training is performed until
the neural networks’ loss is
converged. Hence, though the episodic reward does seem to
converge earlier, that itself may not indicate that the policy
is converged.

For fair comparison, ‘Episodic reward/cost fraction’ is
considered with both methods. It is deﬁned as the fraction
of reward obtained in an episode during training w.r.t the
nominal episodic reward (converged reward). Note that the
words ’reward’ and ’cost’ are being used interchangeably due
to their different notions in optimal control and RL literature
respectively, though they achieve the same objective. For
simplicity, one is considered the negative of the other.

B. Performance Comparison

Data-efﬁciency: As mentioned above, an efﬁcient training
is one that requires minimal data sampling in order to
achieve the same behavior. One way of measuring it is to
collate the times taken for the episodic cost (or reward) to
converge during training. Plots in Fig. 1 show the training
process with both methods on the systems considered. Table
I delineates the times taken for training respectively. As the
system identiﬁcation and feedback gain calculation in case
of D2C take only a small portion of time, the total time
comparison in (Table I) shows that D2C learns the optimal
policy substantially faster than DDPG and hence, has a better
data efﬁciency.

Robustness to noise: As with canonical off-policy RL
algorithms, DDPG requires that an exploration noise be
added to the policy, during training. Given that the training
adapts the policy to various levels of noise, combined with
hours of intense training and a nonlinear policy output, it is
expected that it is more robust towards noise as is evident
from Figs. 2 (c) and 2 (d). However, from plots in Figs. 2 (a)
and (b), it is evident that in some systems the performance of
D2C is on par with or better than DDPG. It may also be noted
that the error variance in D2C method increases abruptly
when the noise level is higher than a threshold and drives
the system too far away from the nominal trajectory that the
LQR controller cannot ﬁx it. This could be considered as a
drawback for D2C. However, it must be noted that the range

(a) Inverted Pendulum - D2C

(b) Cart-Pole - D2C

(c) 3-link Swimmer - D2C

(d) 6-link Swimmer - D2C

(e) Inverted Pendulum - DDPG

(f) Cart-Pole - DDPG

(g) 3-link Swimmer - DDPG

(h) 6-link Swimmer - DDPG

Fig. 1: Episodic reward fraction vs time taken during training

(a) Inverted Pendulum

(b) Cart-Pole

(c) 3-link Swimmer

(d) 6-link Swimmer

Fig. 2: Terminal MSE vs noise level during testing

of noise levels (up until 100 % of the maximum control
signal) that we are considering here is far beyond what is
typically encountered in practical scenarios. Hence, even in
swimmer examples, the performance of D2C is tolerable to
a reasonable extent of noise in the system.

Ease of training: The ease of training is often an ignored
topic in analyzing a reinforcement learning (RL) algorithm.
By this, we mean the challenges associated with its imple-
mentation. As with many RL algorithms that involve neural
networks, DDPG has no guarantees for policy convergence.
As a result,
the implementation often involves tuning a
number of hyper-parameters and a careful reward shaping in
a trial and error fashion, which is even more time-consuming
given that their successful implementation already involves
signiﬁcant time and computational resources. Reproducibility
[11] is still a major challenge that RL is yet to overcome.
On the other hand, training in our approach is predictable
and very reliable.

To elucidate the ease of training from an empirical per-
spective, the exploration noise that is required for training
in DDPG mandates the system to operate with a shorter

time-step than a threshold, beyond which the simulation fails
due to an unbearable magnitude of control actions into the
system. For this, we train both the swimmers in one such case
(with ∆t = 0.01 sec) till it fails and execute the intermediate
policy. Fig. 3 shows the plot in the testing-stage with both
methods. It is evident from the terminal state mean-squared
error at zero noise level that the nominal trajectory of DDPG
is incomplete and its policy failed to reach the goal. The
effect is more pronounced in the higher-dimensional 6-link
swimmer system (Fig. 3b), where the DDPG’s policy can be
deemed to be downright broken. Note, from Table I, that the
systems have been trained with DDPG for a time that is more
than thrice with the 3-link swimmer and 4 times with the 6-
link swimmer. On the other hand, under same conditions, the
seamless training of D2C results in a working policy with
even greater data-efﬁciency.

VI. CONCLUSIONS

In this paper, we proposed a near-optimal control algo-
rithm under fully observed conditions and showed that our
method is able to scale-up to higher dimensional state-space
without any knowledge about the system model. Due to

TABLE I: Simulation parameters and training outcomes

System

Steps per
episode

Inverted
Pendulum
Cart pole
3-link
Swimmer
6-link
Swimmer

30

30
1600
800
1500
900

Time-
step
(in sec.)

0.1

0.1
0.005
0.01
0.006
0.01

Training time (in sec.)

D2C

Open-
loop
12.9

15.0
7861.0
4001.0
9489.3
3585.4

Closed-
loop
< 0.1

DDPG

2261.15

1.33
13.1
4.6
26.5
16.4

6306.7
38833.64
13280.7*
88160
15797.2*

(a) 3-link swimmer

(b) 6-link swimmer

Fig. 3: D2C vs DDPG at ∆t = 0.01s

sequential calculation used in the open-loop optimization and
the system identiﬁcation, D2C is highly memory efﬁcient and
also convenient for parallelization. We tested its performance
and juxtaposed them with a state-of-the-art deep RL tech-
nique - DDPG. From the results, our method has conspicuous
advantages over DDPG in terms of data efﬁciency and ease
of training. The robustness of D2C is also better in some
cases, but has scope for further development by employing
more sophisticated feedback methods and ensuring that the
data efﬁciency is not compromised. We also believe further
drastic reduction in the planning time can be achieved by
parallelization and a more sophisticated parametrization of
the open loop problem.

It is evident from the simulations that methods such as
D2C are able to achieve their goals accurately whereas
DDPG consumes inordinate amount of time in ‘ﬁne-tuning’
their behavior towards the goal. However, we also note that,
by doing this, DDPG is tentatively exploring over the entire
state-space and can result in a better generic policy. Another
drawback with D2C over canonical RL algorithms is that
the cost could be stuck in a local minimum, whereas DDPG
due to the nature of stochastic gradient descent in training
neural networks, can potentially reach the globally optimal
solution. Nevertheless, we hope that our approach signiﬁes
the potential of decoupling based approaches such as D2C in
a reinforcement learning paradigm and recognizes the need
for more hybrid approaches that complement the merits of
each.

REFERENCES

[1] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identiﬁ-

cation, and adaptive control. SIAM, 2015, vol. 75.

[2] P. A. Ioannou and J. Sun, Robust adaptive control. Courier Corpo-

ration, 2012.

[3] K. J. ˚Astr¨om and B. Wittenmark, Adaptive control. Courier Corpo-

ration, 2013.

[4] S. Sastry and M. Bodson, Adaptive control: stability, convergence and

robustness. Courier Corporation, 2011.

[5] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural
networks and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.
[6] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforce-
ment learning,” arXiv preprint arXiv:1509.02971, 2015.

[7] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training
of deep visuomotor policies,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 1334–1373, 2016.

[8] W. Yuhuai, M. Elman, L. Shun, G. Roger, and B. Jimmy, “Scalable
trust-region method for deep reinforcement learning using kronecker-
factored approximation,” arXiv:1708.05144, 2017.

[9] S. John, L. Sergey, M. Philipp, J. Michael I., and A. Pieter, “Trust

region policy optimization,” arXiv:1502.05477, 2017.

[10] S. John, W. Filip, D. Prafulla, R. Alec, and K. Oleg, “Proximal policy

optimization algorithms,” arXiv:1707.06347, 2017.

[11] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and
learning that matters,” in Thirty-

D. Meger, “Deep reinforcement
Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[12] S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine, “Q-
prop: Sample-efﬁcient policy gradient with an off-policy critic,” arXiv
preprint arXiv:1611.02247, 2016.

[13] M. Falcone, “Recent Results in the Approximation of Nonlinear Op-
timal Control Problems,” in Large-Scale Scientiﬁc Computing LSSC,
2013.

[14] D. Jacobsen and D. Mayne, Differential Dynamic Programming.

Elsevier, 1970.

[15] E. Theoddorou, Y. Tassa, and E. Todorov, “Stochastic Differential
Dynamic Programming,” in Proceedings of American Control Con-
ference, 2010.

[16] E. Todorov and W. Li, “A generalized iterative LQG method for
locally-optimal feedback control of constrained nonlinear stochastic
systems,” in Proceedings of American Control Conference, 2005, pp.
300 – 306.

[17] W. Li and E. Todorov, “Iterative linearization methods for approxi-
mately optimal control and estimation of non-linear stochastic system,”
International Journal of Control, vol. 80, no. 9, pp. 1439–1453, 2007.
[18] W. B. Powell, Approximate Dynamic Programming: Solving the curses

of dimensionality.

John Wiley & Sons, 2007.

[19] D. Bertsekas, Dynamic programming and optimal control. Athena

scientiﬁc Belmont, MA, 2012, vol. 2.

[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[21] D. Yu, M. Raﬁeisakhaei, and S. Chakravorty, “Stochastic Feedback
Control of Systems with Unknown Nonlinear Dynamics,” in 56th
IEEE Conference on Decision and Control(CDC), 2017.

[22] M. Raﬁeisakhaei, S. Chakravorty, and P. R. Kumar, “A Near-Optimal
Separation Principle for Nonlinear Stochastic Systems Arising in
Robotic Path Planning and Control,” in 56th IEEE Conference on
Decision and Control(CDC), 2017.

[23] R. Wang, K. Parunandi, D. Yu, D. Kalathil, and S. Chakravorty,
“Decoupled data based approach for learning to control nonlinear
dynamical systems,” Tech. Report. Available at https://goo.gl/pd3pRU,
March, 2019.

[24] T. Lillicrap et al., “Continuous control with deep reinforcement

learning,” in Proc. ICLR, 2016.

[25] T. Emanuel, E. Tom, and Y. Tassa, “Mujoco: A physics engine for
model-based control,” IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, pp. 5026–5033, 2012.

[26] T. Yuval and et al., “Deepmind control suite,” arXiv:1801.00690, 2018.
[27] M. Plappert, “keras-rl,” https://github.com/keras-rl/keras-rl, 2016.

APPENDIX

for suitably deﬁned ¯qT,τ . Therefore:

A. Proof of Lemma 1

Proof: We proceed by induction. The ﬁrst general
instance of the recursion occurs at t = 3. It can be shown
that:
δx3 = ( ¯A2 ¯A1((cid:15)w0) + ¯A2((cid:15)w1) + (cid:15)w2)
(cid:125)

+

(cid:124)

(cid:123)(cid:122)
δxl
3

Break δxl
{ ¯A2 ¯S1((cid:15)w0) + ¯S2( ¯A1((cid:15)w0) + ¯S2( ¯A1((cid:15)w0) + (cid:15)w1 + ¯S1((cid:15)w0))}
.
follows that:
(cid:125)
(cid:123)(cid:122)
(cid:124)
¯¯S3

δJ1δJ2 =

(cid:88)

t,τ

C x

τ (δxl

τ )¯qT,t(δxl

t)2 + O((cid:15)4).

Taking expectations on both sides:

E[δJ1δJ2] =

(cid:88)

t,τ

C x

τ ¯qT,tE[δxl

τ (δxl

t)2] + O((cid:15)4).

(36)

(37)

t = (δxl

t − δxl

τ ) + δxl

τ , assuming τ < t. Then, it

E[δxl

τ (δxl

t)2] = E[δxl

τ (δxl

t − δxl
+2E[(δxl

τ )2] + E[(δxl
τ )(δxl
t − δxl
= E[(δxl

τ )3]
τ )2]
τ )3],

(38)

where the ﬁrst and last terms in the ﬁrst equality drop out due
τ ) from δxl
to the independence of the increment (δxl
τ ,
and the fact that E[δxl
τ ] = 0. Since
δxl
t + (cid:15)wt,
it may again be shown that:

t − δxl
τ is the state of the linear system δxt+1 = ¯Atδxl

t − δxl
τ ] = 0 and E[δxl

E[δxl

τ ]3 =

(cid:88)

s1,s2,s3

Φτ,s1Φτ,s2 Φτ,s3 E[ws1 ws2 ws3 ],

(39)

where Φt,τ represents the state transitions operator between
times τ and t, and follows from the closed loop dynamics.
Now, due to the independence of the noise terms wt, it
follows that E[ws1ws2ws3] = 0 regardless of s1, s2, s3.
An analogous argument as above can be repeated for the case
when τ > t. Therefore, it follows that E[δJ1δJ2] = O((cid:15)4).

(30)
Noting that ¯S1(.) and ¯S2(.) are second and higher order
terms, it follows that ¯¯S3 is O((cid:15)2).
Suppose now that δxt = δxl
δxt+1 = ¯At+1(δxl
= ( ¯At+1δxl
t + (cid:15)wt)
(cid:123)(cid:122)
(cid:125)
δxl
t+1

t + ¯¯St) + (cid:15)wt + ¯St+1(δxt),
¯¯St + ¯St+1(δxt)}
+ { ¯At+1
.
(cid:125)
(cid:124)

t + ¯¯St where ¯¯St is O((cid:15)2). Then:

(cid:123)(cid:122)
¯¯St+1

(31)

(cid:124)

Noting that ¯St+1 is O((cid:15)2) and that ¯¯St+1 is O((cid:15)2) by
assumption, the result follows.

B. Lemma 2

Lemma 2: Let δJ π

1 , δJ π

E[δJ1δJ2] is an O((cid:15)4) function.

2 be as deﬁned in (13). Then,

t=0

Proof:

1 and δJ π
t δxl
t=0 cx

In the following, we suppress the explicit de-
pendence on π for δJ π
2 for notational convenience.
Recall that δJ1 = (cid:80)T
t, and δJ2 = (cid:80)T
¯Ht(δxt) +
¯¯St. For notational convenience, let us consider the scalar
cx
t
case, the vector case follows readily at the expense of more
elaborate notation. Let us ﬁrst consider ¯¯S2. We have that
¯¯S2 = ¯A2 ¯S1((cid:15)w0) + ¯S2( ¯A1((cid:15)w0) + (cid:15)w1 + ¯S1((cid:15)w0)). Then, it
follows that:
¯¯S2 = ¯A2 ¯S(2)
where ¯S(2)
represents the coefﬁcient of the second order
term in the expansion of ¯St. A similar observation holds for
H2(δx2) in that:

2 ( ¯A1(cid:15)w0 + (cid:15)w1)2 + O((cid:15)3), (32)

1 ((cid:15)w0)2 + ¯S(2)

t

¯H2(δx2) = ¯H (2)

2 ( ¯A1((cid:15)w0) + (cid:15)w1)2 + O((cid:15)3),

(33)

t

where ¯H (2)
expansion of ¯Ht. Note that (cid:15)w0 = δxl
δxl

is the coefﬁcient of the second order term in the
1 and ¯A1((cid:15)w0)+(cid:15)w1 =

2. Therefore, it follows that we may write:

¯Ht(δxt) + C x
t

¯¯St =

t−1
(cid:88)

τ =0

qt,τ (δxl

τ )2 + O((cid:15)3),

(34)

for suitably deﬁned coefﬁcients qt,τ . Therefore, it follows
that

T
(cid:88)

t=1

¯Ht(δxt) + C x
t

¯¯St

¯qT,τ (δxl

τ )2 + O((cid:15)3),

(35)

δJ2 =

=

T
(cid:88)

τ =0

