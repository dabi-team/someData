2
2
0
2

b
e
F
5
2

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

2
v
4
6
6
1
1
.
2
0
2
2
:
v
i
X
r
a

Under consideration for publication in J. Fluid Mech.

1

Comparative analysis of machine learning methods
for active ﬂow control

Fabio Pino1†, Lorenzo Schena1, Jean Rabault2,
Alexander Kuhnle3, and Miguel A. Mendez1

1von Karman Institute for Fluid Dynamics, EA Department, Sint Genesius Rode, Belgium
2Norwegian Meteorological Institute, Oslo, Norway
3Dep. of Computer Science and Technology, University of Cambridge, Cambridge, UK

Machine learning frameworks such as Genetic Programming (GP) and Reinforcement Learning
(RL) are gaining popularity in ﬂow control. This work presents a comparative analysis of the
two, bench-marking some of their most representative algorithms against global optimization
techniques such as Bayesian Optimization (BO) and Lipschitz global optimization (LIPO). First,
we review the general framework of the ﬂow control problem, linking optimal control theory with
model-free machine learning methods. Then, we test the control algorithms on three test cases.
These are (1) the stabilization of a nonlinear dynamical system featuring frequency cross-talk, (2)
the wave cancellation from a Burgers’ ﬂow and (3) the drag reduction in a cylinder wake ﬂow.
Although the control of these problems has been tackled in the recent literature with one method
or the other, we present a comprehensive comparison to illustrate their differences in exploration
versus exploitation and their balance between ‘model capacity’ in the control law deﬁnition versus
‘required complexity’. We believe that such a comparison opens the path towards hybridization of
the various methods, and we offer some perspective on their future development in the literature
of ﬂow control problems.

Key words: Optimal Flow control and Machine Learning, Bayesian Optimization, LIPO Opti-
mization, Genetic Programming, Reinforcement Learning

1. Introduction

The multidisciplinary nature of active ﬂow control has attracted interests from many research
areas for a long time (Gunzburger 2002; Wang & Feng 2018; el Hak 2000; Bewley 2001) and
its scientiﬁc and technological relevance have ever growing proportions (Brunton & Noack
2015; Noack et al. 2022; Bewley 2001). Indeed, the ability to interact and manipulate a ﬂuid
system to improve its engineering beneﬁts is essential in countless problems and applications,
including laminar to turbulent transition (Schlichting Deceased; Lin 2002), drag reduction (el Hak
2000; Wang & Feng 2018), stability of combustion systems (Lang et al. 1987), ﬂight mechanics
(Longuski et al. 2014), wind energy (Apata & Oyedokun 2020; Munters & Meyers 2018), and
aeroacoustic noise control (Collis et al. 2002; Kim et al. 2014), to name just a few.

The continuous development of computational and experimental tools, together with the advent
of data-driven methods from the ongoing machine learning revolution, are reshaping tools and
methods in the ﬁeld (Noack et al. 2022; Noack 2019). Nevertheless, the quest for reconciling
terminology and methods from the machine learning and the control theory community has a long
history (see Bersini & Gorrini (1996) and Sutton et al. (1992)) and it is still ongoing, as described

† Email address for correspondence: fabio.pino@vki.ac.be

 
 
 
 
 
 
2

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

in the recent review by Recht (2019) and Nian et al. (2020). This article aims at reviewing some
recent machine learning algorithms for ﬂow control, presenting a uniﬁed framework that highlights
differences and similarities amidst various techniques. We hope that such a generalization opens
the path towards hybrid approaches.

In its most abstract formulation, the (ﬂow) control problem is essentially a functional opti-
mization problem constrained by the (ﬂuid) systems’ dynamics (Stengel 1994; Kirk 2004). As
further discussed in Section 2, the goal is to ﬁnd a control function that minimizes (or maximizes)
a cost (or reward) functional which measures the controller performances (e.g. drag or noise
reduction). Following Wiener’s metaphors (Wiener 1948), active control methods can be classiﬁed
as white, grey or black depending on how much knowledge about the system is used to solve the
optimization: the whiter the approach, the more the control relies on the analytical description of
the system to be controlled.

In a white box method, the equations governing the system to be controlled are known
and amenable to analytical treatment. This enables tools from optimal control theory such as
Pontryagin’s principle or the Hamilton-Jacobi-Bellman optimality condition (Stengel 1994; Kirk
2004). The literature of optimal control theory further classiﬁes a control technique as direct
or indirect (Rao 2009; Biral et al. 2016) depending on whether the necessary conditions for
optimality is used in its continuous form. Leveraging on the analytical description of the system,
calculus of variation and Pontryagin’s principle, these conditions render as a set of differential
equations with given initial and ﬁnal conditions, i.e. a two-point boundary value problem (TPBVP).
Indirect methods exploit this TPBVP in various ways, either by seeking a numerical solution
via shooting or collocation methods or to compute gradients of the cost function with respect
to the control parameters, as in adjoint-based optimal control (Carnarius et al. 2010; Bewley
et al. 2001; Kim et al. 2014; Delport et al. 2011; Nita et al. 2016; Meliga et al. 2010; Flinois &
Colonius 2015). Direct methods (Böhme & Frank 2017) translate the optimal control problem
into a discrete constrained optimization or nonlinear programming problem (Kraft 1985; Bazaraa
et al. 2005).

Signiﬁcant simpliﬁcations are possible if three conditions are met: (1) the system is linear (2)
the control law to be derived is linear with respect to the state and (3) the performance measure is
quadratic. In this case, the principles of optimal control lead to the well known Linear Quadratic
Regulator (LQR) (Stengel 1994; Kim & Bewley 2007; Sipp & Schmid 2016) and the optimal
control law is analytically available. The LQR is arguably the most common tool also for nonlinear
systems, provided that these are close enough to a ﬁxed point around which the controller seeks to
keep the system. An extensive literature review of linear control theory for ﬂuid ﬂows is provided
by Brunton & Noack (2015). Tools from linear control theory have often been combined with
tools from system identiﬁcation (Brunton 2022), which seeks to identify the best linear model for
a dynamical system by using input-output data (Proctor et al. 2016; Brunton 2009; Rowley 2005).
These approaches are "grey", in the sense that a "surrogate" model is used to derive the optimal
control law.

Within the direct methods, a radically different perspective –central to this work– is provided
by "black-box" or "model-free" methods. In these approaches, only input-output relations are
used in the search for the optimal control law. This is a radical change of paradigm compared with
the analytical-based control strategies such as the adjoint method, as knowledge of the system
is gathered by only interacting with it. The popularity of these heuristic approaches has been
growing considerably in the last decade, promoted by the popularization of machine learning
tools, the development of new algorithms, and the increase in available computational power.

Machine learning (Abu-Mostafa et al. 2012; Mitchell 1997; Vladimir Cherkassky 2008; Brunton
et al. 2020) is a subset of Artiﬁcial Intelligence which combines optimization and statistics to
"learn" (i.e. calibrate) models from data (i.e. experience). These models can be general enough to
describe any (nonlinear) function without requiring prior knowledge and can be encoded in various

Comparative analysis of machine learning methods for active ﬂow control

3

forms: examples are parametric models such as Radial Basis Function (RBFs, see Fasshauer
(2007)) expansions or Artiﬁcial Neural Networks (ANNs, see Goodfellow et al. (2016)), or tree
structures of analytic expressions such as in Genetic Programming (GP Koza (1994)). The process
by which these models are "ﬁtted" to (or "learned" from) data is an optimization in one of its
many forms (Sun et al. 2019): continuous or discrete, global or local, stochastic or deterministic.
Within the ﬂow control literature, at the time of writing, the two most prominent model-free
control techniques from the machine learning literature are Genetic Programming (Koza 1994)
and Reinforcement Learning (Sutton & Barto 2018). Both are reviewed in this article.

Genetic Programming (GP) is an evolutionary computational technique developed by Koza
(Koza 1994) as a new paradigm for automatic programming and machine learning (Banzhaf
et al. 1997; Vanneschi & Poli 2012). GP optimizes both the structure and the parameters of a
model, which is usually constructed as recursive trees of predeﬁned functions connected through
mathematical operations. The use of GP for ﬂow control has been pioneered and popularized by
Noack and coworkers (Noack 2019; Duriez et al. 2017). Successful examples on experimental
problems include the drag reduction past bluff bodies (Li et al. 2017), shear ﬂow separation
control (Gautier et al. 2015; Debien et al. 2016; Benard et al. 2016) and many more, as reviewed
by Noack (2019). More recent extensions of this "Machine Learning Control" (MLC) approach,
combining genetic algorithms with the down-hill simplex method, have been proposed by Li et al.
(2019) and Maceda et al. (2021).

Reinforcement Learning (RL) is one of the three main machine learning paradigms and
encompasses learning algorithms collecting data "online", in a trial and error process. In Deep RL
(DRL), the control law (known as policy) is modelled by an ANN which needs to be trained. The
use of an ANN to parametrize control laws has a long history (see Lee et al. (1997)), but their
application to ﬂow control, leveraging on RL algorithms, is at its infancy (see also Li & Zhang
(2021) for a recent review). The landscape of RL is vast and grows at a remarkable pace, fostered
by the recent success in strategy board games (Silver et al. 2016, 2018), video games (Szita
2012), robotics (Kober & Peters 2014), language processing Luketina et al. (2019) and more. In
the literature of ﬂow control, RL has been pioneered by Komoutsakos and coworkers (Gazzola
et al. 2014; Verma et al. 2018) (see also Garnier et al. (2021) and Rabault & Kuhnle (2022) for
more literature). The ﬁrst applications of RL in ﬂuid mechanics were focused on the study of
collective behavior of swimmers (Wang & Feng 2018; Verma et al. 2018; Novati et al. 2017;
Novati & Koumoutsakos 2019; Novati et al. 2019), while the ﬁrst applications for ﬂow control
were presented by Pivot et al. (2017) and by Rabault et al. (2019, 2020); Rabault & Kuhnle (2019).
A similar ﬂow control problem has been solved numerically and experimentally via RL by Fan
et al. (2020). Bucci et al. (2019) showcased the use of RL to control chaotic systems such as the
one-dimensional Kuramoto–Sivashinsky equation; Beintema et al. (2020) used it to control heat
transport in a two-dimensional Rayleigh-Bénard systems while Belus et al. (2019) used RL to
control the interface of unsteady liquid ﬁlms. Ongoing efforts in the use of DRL for ﬂow control
are focused with increasing the complexity and nonlinearity of the problems treated, either by
increasing the Reynolds number (see Ren et al. (2021)), or by considering more challenging and
realistic conﬁgurations (Vinuesa et al. 2022).

In this article, we consider the Deep Deterministic Policy Gradient (DDPG)(Lillicrap et al.
2015) as a representative deterministic RL algorithm. This is introduced in Section 3.3, and the
results obtained for one of the investigated test cases are compared with those obtained by Tang
et al. (2020) using a stochastic RL approach, namely the Proximal Policy Optimization (PPO)
Schulman et al. (2017). It is worth noticing that the boundaries between GP and RL are more
methodological than conceptual: as highlighted by Banzhaf et al. (1997), many ﬁtness functions
in GP can lead to RL systems. Yet, GP and RL are often presented as different frameworks.

This work puts GP and RL in a global control framework and benchmarks their performances
against simpler black-box optimization methods. Within this category, we include model-free

4

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

control methods in which the control action is predeﬁned and prescribed by a few parameters (e.g
a simple linear controller), and the model learning is driven by global black-box optimization.
This approach, using Genetic Algorithms, has a long history (Fleming & Fonseca 1993). However,
we here focus on more sample efﬁcient alternatives: the Bayesian Optimization (BO) and the
LiPschitz global Optimization technique (LIPO). Both are described in Section 3.1.

The BO is arguably the most popular "surrogate-based", derivative-free, global optimization
tool, popularized by Jones et al. (1998) and their Efﬁcient Global Optimization (EGO) algorithm.
In its most classic form (Forrester et al. 2008; Archetti & Candelieri 2019), the BO uses a Gaussian
process (Rasmussen & Williams 2005) for regression of the cost function under evaluation and an
acquisition function to decide where to sample next. This method has been used by Mahfoze et al.
(2019) for reducing the skin-friction drag in a turbulent boundary layer and by Blanchard et al.
(2022) for reducing the drag in the ﬂuidic pinball and for enhancing mixing in a turbulent jet.

The LIPO algorithm is a more recent global optimization strategy proposed by Malherbe &
Vayatis (2017). This is a sequential procedure to optimize a function under the only assumption
that it has a ﬁnite Lipschitz constant. Since this method has virtually no hyper-parameters involved,
variants of the LIPO are becoming increasingly popular in hyper-parameter calibration of machine
learning algorithms (Ahmed et al. 2020), but to the authors’ knowledge it has never been tested
on ﬂow control applications.

All of the aforementioned algorithms are analyzed on three test cases of different dimensions
and complexity. The ﬁrst test case is the 0D model proposed by Duriez et al. (2017) as the
simplest dynamical system reproducing the frequency cross-talk encountered in many turbulent
ﬂows. The second test case is the control of nonlinear travelling waves described by the 1D
Burgers’ equation. This test case is representative of the challenges involved in the control of
advection-diffusion problems. Moreover, recent works on Koopman analysis by Page & Kerswell
(2018) and Balabane et al. (2021) have provided a complete analytical linear decomposition of
the Burgers’ ﬂow and might render this test case well suited to test the full arsenal of possible
"white-box" control methods. Finally, the last selected test case is arguably the most well known
benchmark in ﬂow control: the drag attenuation in the ﬂow past a cylinder. This problem has
been tackled by nearly the full spectra of control methods in the literature, including reduced
order models and linear control (Seidel et al. 2008; Bergmann et al. 2005; Park et al. 1994),
resolvent-based feedback control (Jin et al. 2020), reinforcement learning via stochastic (Rabault
et al. 2019) and deterministic algorithms (Fan et al. 2020), and reinforcement learning assisted by
stability analysis (Li et al. 2017).

In the literature on machine learning methods for ﬂow control, both the ﬁrst and the last test
cases have been tackled by either GP or RL. We here benchmark both methods on the same
test cases against classic black-box optimization. Emphasis is given to the different precautions
these algorithms require, the number of necessary interactions with the environment, the different
approaches to balance exploration and exploitation, and the differences (or similarities) in the
derived control laws. The remaining of the article is structured as follows. Section 2 recalls the
conceptual transition from optimal control theory to machine learning control. Section 3 brieﬂy
recalls the machine learning algorithm analyzed in this work, while Section 4 describes the
introduced test cases. Results are collected in Section 5 while conclusions and outlooks are given
in Section 6.

Comparative analysis of machine learning methods for active ﬂow control

5

2. From optimal control to machine learning

This section brieﬂy presents the framework linking optimal control, reinforcement learning and
global optimization methods. We refer the reader the literature (Sutton & Barto 2018; Evans 1983;
Stengel 1994; Kirk 2004) for more details. We introduce the optimal control problem in 2.1 and
we focus on model-free learning based control in 2.2.

2.1. Methods from Optimal Control Theory
An optimal control problem consists in ﬁnding a control action a(t), within a feasible set A ,
which optimizes a functional measuring our ability to keep a ﬂow (plant in control theory and
environment in reinforcement learning) close to the desired states or conditions. The functional is
usually a cost to minimize in control theory and a payoff to maximize in reinforcement learning.
We focus on the second and denote the reward function as R(a). The optimization is constrained
by the plant/environment’s dynamic:

max
A
a(t)
∈

s.t.

R(a) = φ (s(T )) +

(cid:90) T

0

L (s(τ), a(τ), τ) dτ,

(cid:40)

˙s(t) = f(s(t), a(t),t)
s(0) = s0

(0, T ]

t

∈

,

(2.1)

Rna

where f : Rns
is the system’s state vector evolving along a trajectory, a
controller in control theory and an agent in reinforcement learning.

→

×

∈

Rns is the vector ﬁeld in the phase space of the dynamical system, s

Rns
Rna is the action/actuation, taken by a

∈

The functional R(a) comprises a running cost (or Lagrangian) L : Rna

R, which
Rns
accounts for the system’s states evolution, and a terminal cost (or Mayer term) φ : Rns
R, which
depends on the ﬁnal state condition. Optimal control problems with this cost functional form are
known as Bolza problem (Stengel 1994; Evans 1983; Kirk 2004).

→
→

×

The agent/controller selects the action/actuation from a control law or policy π : Rns

a(t) = π(s(t),t)

Rna .

∈

Note that the system might not be fully observable, and hence the policy would generally
ns. However, as we later focus on

Rno , with no

depend on some observation y = O(s(t))
black-box methods, this clariﬁcation is unessential.

∈

(cid:28)

Identifying the best policy a∗(t) = π ∗(s(t),t) is the goal of optimal control theory and
reinforcement learning alike. When the agent/controller acts according to π ∗, the state evolves
along the optimal trajectory s∗(t). Using calculus of variation, the necessary condition for
optimality is the minimum of the augmented reward functional RA(a) which adjoints the equality
f(s(t), a(t),t) = 0 using a vector of Lagrangian multipliers λ
constraint ˙s(t)

Rn:

−

∈

The vector λ
to perturbation of the trajectory s(t), i.e λ =

(2.3)
Rn is also known as the adjoint vector or costate and represent the cost sensitivity
∂sH (s, a(t), λ (t))T .

−

∈

RA(a) = R(a) + λ T (cid:2)˙s(t)

f(s(t), a(t),t)(cid:3) .

It can be shown (Stengel 1994; Kirk 2004; Evans 1983) that the optimal control law/policy is

the one that maximizes the Hamiltonian H (s, a(t), λ (t)) of the system, deﬁned as:

−

i.e.:

H := L (s(t), a(t),t) + λ (t)T (˙s(t)

f(s(t), a(t),t)),

−

(2.4)

Rna:

→

(2.2)

6

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

a∗(t) = arg max

(cid:8)H (s, a, λ (t))(cid:9) .

(2.5)

A

a

∈

The costate vector in the Hamiltonian satisﬁes the following system of ordinary differential

equations:

(cid:40) ˙λ (t) =
∂sfT λ (t)
−
−
λ (T ) = ∂sφ (s(T ))T

∂sL (s, a(t),t)T

(0, T ]

t

∈

,

(2.6)

having used the short hand notation ∂x for the partial derivatives ∂ /∂x, so that ∂sL is the gradient
of the Lagrangian over the action vector and ∂sf is the Jacobian of the dynamical system’s ﬂow.
Equation (2.5), together with (2.6), state the Pontryagin principle (Stengel 1994; Evans 1983;
Kirk 2004). This deﬁnes the necessary condition for optimality of the control law/policy and is
the backbone of adjoint-based control methods.

More speciﬁcally, the gradient ∂aL and the Jacobian ∂sf can be computed once the system and
the cost function are deﬁned. Then, the system (2.6) can be integrated backward in time from the
terminal state λ (T ) for any ‘tentative’ control action a(t). Given a(t) and λ (t), it is possible to
compute the gradient of the Hamiltonian with respect to the control action:

∂aH (s, a, λ ) = ∂aL (s(t), a(t),t) + λ T ∂af(s(t), a(t),t) .
This information can then be used to improve the selected action in a gradient-based manner.
For example, assuming that the action is given by a parametric form a = π(s; w), depending on a
set of weights w

Rnw , an adjoint-based gradient ascent step can be

(2.7)

∈

∈

w

←

w + α ∂wH (s, a(w), λ ) ,

(2.8)
R a suitable learning rate and ∂wH = ∂aH ∂wa the gradient of the Hamiltonian with
with α
respect to the weights. Having access to the analytic evaluation of the cost function’s gradient
enable also more sophisticated Quasi-Newton methods (see for instance Nita et al. (2016)). The
power of adjoint-based control is in its independence from the control vector dimension na or
the number of parameters nw: for an actuation law a(t), the cost to evaluate the gradient ∂aH is
the cost of one ‘forward’ integration of the system to evaluate the states s(t) plus one ‘backward’
integration of the ODE (2.6) to evaluate the costate λ (t).

A different approach, providing both a necessary and sufﬁcient condition for optimality, is
dynamic programming. This approach is based on the deﬁnition of a value function which
measures the quality of a given state s(t) according to a policy π(s) in (2.2). In other terms, this
function gives the minimal cost/maximum reward achievable by a controller/agent following π,
from s(t) to s(T ):

V π (t, s(t)) = φ (s(T )) +

L (s(t), a(t),t)dt .

(cid:90) T

(2.9)

t
The central result in dynamic programming is that the optimal policy π ∗, namely the one
leading to V ∗ := V π∗ = supV π for all states, satisﬁes the Hamilton–Jacobi–Bellman (HJB) partial
differential equation:

(cid:110)

∂tV ∗(s,t) + max
A

a
∈

L (s, a,t) + ∂sV ∗(s,t)f(s, a,t)

= 0 ,

(2.10)

(cid:111)

starting from the terminal condition V ∗(T ) = φr(T ).

After solving the HJB and hence obtaining the optimal value function, the optimal policy is

found indirectly as the result of a maximization process:

Comparative analysis of machine learning methods for active ﬂow control

7

a∗(t) = arg max

L (s, a,t) + ∂sV ∗(s,t)f(s, a,t)

.

(2.11)

(cid:111)

(cid:110)

A

a

∈

Besides the difﬁculties in solving the HJB, the main problem of this approach is that every
time step requires an expensive maximization to deﬁne the best action. Moreover, the numerical
integration of (2.10) requires the discretization of the state space, which scales exponentially
with ns. This limit is known as the curse of dimensionality and prevents the application of
dynamic programming to many ﬂow control problems of interest even if the dynamical system
is analytically tractable and its states are fully observable. The tools presented in this section
provides the foundation to the model-free approaches discussed next.

2.2. Model Free Control and Machine Learning

In the context of machine learning control, the equations governing the system are unknown,
and the controller is designed relying only on input/output relations. As shown in Figure 1, the
dynamics of the system f(s, a,t), as well as the performance measures R(a), are embedded into an
unknown “black-box function” and the algorithm looks for an optimal control/policy through an
iterative trial and error process.

We uniformly discretize the time interval t

[0, T ] in (2.1) as tk = k∆t, leading to N = T /∆t + 1
1 and we introduce the notation sk = s(tk). Although it is impossible
points indexed as k = 0, . . . N
to predict the response of the system to an action, i.e. sk+1 = f(sk, aπ
k , k), it is possible to collect
a sequence of states S :=
.
}
Moreover, one reward with L (sk, ak, k) each state-action pair as a function of time (index). In an
episodic approach, each set of N interactions with the system is an episode, at the end of which it
is possible to compute a discrete version of the performance measures R(a) in (2.1):

while taking a sequence of actions Aπ :=

s1, s2 . . . sN
{

a1, a2 . . . aN

−

∈

}

{

k , k) .

L (sk, aπ

R(Aπ ) = φ (sN) +

k , k) = γ kr(sk, aπ

N
1
−
∑
k=0
In the RL literature, this is known as cumulative reward and the Lagrangian takes the form
L (sk, aπ
[0, 1] is a discount factor which prioritize immediate
over future rewards. In a deterministic policy, the sequence of actions is taken from a parametric
function which we denote aπ = π(s; w). In a stochastic policy, the parametric function outputs
the parameters of the distribution (e.g mean and standard deviation in a Gaussian) from which
Rnw is unknown and must be learned.
the actions will be sampled. In both cases, the vector w
By mapping the action space A onto a ﬁnite weight space w
Rnw, the variational problem of
maximizing R(a(t)) reduces to the optimization problem of maximizing R(w).

k ) = γ krπ

k , where γ

(2.12)

∈

∈

∈

A machine learning-based controller (see Figure 1 is thus an optimizer of R(w) which proceeds
iteratively while interacting with the environment. At every iteration n, the update wn+1 hopefully
leads to better performances than the previous, i.e. R(wn) < R(wn+1). Although there is no
guarantee that the optimal control law/policy can be found, and we have no means to attack neither
(2.7) nor (2.10), it is possible to give a value to a state s under the policy π and hence have the
discrete version of (2.9):

Vπ (st ) =φ (sN) +

N
∑
k=t

Lr(sk, aπ

k , k) = φ (sN) +

N
∑
k=t

γ k

t rk = rk + γVπ (st+1) .
−

(2.13)

One thus has R(Aπ ) = Vπ (s1). Moreover, one might occasionally let the environment/controller
k . Then, it is worth deﬁning the value of a

act off-policy and thus take an action ak rather than aπ
state-action pair (st , at ) under policy π as:

8

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 1: General setting for a machine learning-based control problem: the learning algorithm
improves the agent/control performances by interacting with the environment/plant. The
agent/controller acts according to a parametric law and receives a feedback on its actions. The
control problem is an optimization of the weights to maximize a function of actions and states.

Qπ (st , at ) =φr(sN) + r(sk, ak) +

N
∑
k=t+1
= r(sk, ak) + γVπ (st+1) .
This function, known as Q-function, measures the reward that can be obtained if a ‘de-tour’ from
the current policy is taken at time k, the action ak is taken instead of aπ
k , and the policy π is
followed afterwards. It is therefore possible to deﬁne the advantage of such exploration as

Lr(sk, aπ

(2.14)

k , k)

Aπ (st , at ) = r(st , at )

r(sk, aπ

k) = Qπ (st , at )

Vπ (st ) .

(2.15)

−

−

If the optimal policy is followed, the advantage is zero. It is worth noticing that all the deﬁnitions
(2.13), (2.14),(2.15) assume that the system is deterministic. This is uncommon in the literature
of RL, where the environment is usually treated as stochastic and modelled as a Markov Decision
Process (MDP) (Sutton & Barto 2018). We brieﬂy reconsider the stochastic approach in the
description of the DDPG in section 3.3. For a deterministic system, one has

R(w) = V π (s0) = Q(s0, aπ

0 ) .

(2.16)

Control methods based on machine learning differ in the form of the policy parametrization
a = π(st , w), and the use of data and function approximators to compensate for the lack of solid
theoretical tools such as (2.4) and (2.11).

Some machine learning tools ‘imitates’ (2.4) and –borrowing from the RL literature– could be
classiﬁed as actor-only. These methods focus on the policy parametrization and seek to maximize
f(s(t), a(t),t) = 0 in (2.1), or, in a sense,
R(A(w)) regardless of the physical constraints ˙s(t)
taking λ = 0 in (2.3). Actor-only methods are essentially optimizers coupled with a (possibly
adaptive) policy parametrization. Like all optimizers, these can be global and gradient-free or
local and gradient-based, and might use surrogate functions to approximate the function R(w).
Within this last category, we consider the BO and LIPO in section 3.1. Among the gradient-free,
actor-only methods, the most classic approach is the GP, brieﬂy described in Section 3.2.

−

performance measure:  Controller/Agent action LearningMethod states   Environment/PlantComparative analysis of machine learning methods for active ﬂow control

9

The main difference between black box optimization and reinforcement learning is that the
second seeks to approximate the value function Q in (2.16). The interest in such approximation
depends on whether the algorithm is actor-only or critic-only. An actor-only RL approach focuses
on learning Q(st , at ) to enable the computation of the gradient ∂wR = ∂aQ∂wa. This can then be
used for gradient based optimization. The most classic example of this category is REINFORCE
(Williams 1992; Sutton & Barto 2018). A critic-only RL focuses on learning Q(st , at ) to then
A Qπ (s, a). The most classic example of this category, also
take actions such that at = arg maxa
∈
known as ‘off-policy’ methods, is deep Q learning (Mnih et al. 2013). Modern RL algorithms are
actor-critic, i.e. combine both approaches. The DDPG algorithm implemented in this work falls
within this category and is described in Section 3.3.

3. Implemented Algorithms

3.1. Optimization via BO and LIPO

∈

∈

⊆

W

Let the policy be described by a predeﬁned function a = π(st , wπ )

Rna , deﬁned by a set of
Rnw. This function could be a linear mapping, a user-deﬁned nonlinear function,
weights wπ
or any other parametric model from machine learning (r.g. RBFs or ANNs). We assume in this
O(10)). For
section that this model depends on a relatively small number of parameters (say nw
a given set of weights, the policy deﬁnes the behaviour of the agent/controller and the cumulative
reward at each episode can be computed from (2.12). In a stochastic system, the cumulative reward
varies even if the same set of weights is considered: in this case, we shall repeat the evaluation
of a set of weights multiple times and average the results. In both cases, the problem is now an
optimization, in which every episode gives an evaluation of a "black-box" function R(w) (or its
expectation over a number of episodes). Because this function might be expensive, we focus on
efﬁcient optimizers. Other methods are illustrated by Maceda et al. (2018).

∼

3.1.1. Bayesian Optimization (BO)

{

∗}

The ﬁrst ingredient of BO is a regression tool that derives a surrogate model of the function
to optimize. Among several options, the classic BO uses a Gaussian Process (GPr) as surrogate
function. Let W∗ :=
the
associated cumulative rewards. The GPr offers a probabilistic model that computes the probability
of a certain reward given the observations (W∗, R∗), i.e. p(R(w)
W∗, R∗). In a GPr, this is
|
R∗, W∗) = N (µ, Σ ) ,
p(R(w)
|

tested weights and R∗ :=

be a set of n
∗

w1, w2 . . . wn

R1, R2 . . . Rn

(3.1)

where N denotes a multivariate Gaussian distribution with mean µ and covariance function Σ .
In a Bayesian framework, eq (3.1) is interpreted as a posterior distribution, conditioned to the
observations (W∗, R∗). A Gaussian process is a distribution over functions whose smoothness is
deﬁned by the covariance function, computed using a kernel function. Given a set of data (w∗, R∗),
this allows for building a continuous function to estimate both the reward of a possible candidate
and the uncertainties associated with it.

∗}

{

We are interested in evaluating (3.1) on a set of nE new samples W :=

and let
us denote as R :=
the possible outcomes (treated as random variables). Assuming
that the possible candidate solutions belong to the same Gaussian process (usually assumed to
have zero mean (Rasmussen 2003)) as the observed data (W∗, R∗), we have:

w1, w2 . . . wnE }

R1, R2 . . . RnE }

{

{

where K
kernel function.

∗∗

= κ(W∗, W∗)

Rn

n
∗×

∗, K

∈

∗

(cid:19)

(cid:18)R∗
R

(cid:19)(cid:19)

,

(cid:18)

0,

N

(cid:18)K
∗∗
KT
∼
∗
= κ(W, W∗)

K
∗
K
Rne

∈

n
∗, K = κ(W, W)
×

(3.2)

RnE

nE and κ a
×

∈

10

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

The prediction in (3.1) can be build using standard rules for conditioning multivariate Gaussian,

and the functions µ and Σ in (3.1) becomes a vector µ

and a matrix Σ

:

∗

∗

µ

Σ

∗

∗

= KT
∗
= K

1
R R∗
K−
∈
KT
1
R K
K−
∗

−

∗ ∈

RnE

RnE

nE ,
×

(3.3)

(3.4)

+ σ 2

RI, with σ 2

∗∗

where KR = K

R the expected variance in the sampled data and I the identity
matrix of appropriate size. The main advantage of BO is that the function approximation is
sequential, and new predictions improve the approximation of the reward function (i.e. the
surrogate model) episode after episode. This makes the GPr- based BO one of the most popular
black-box optimization methods for expensive cost functions. Nevertheless, many variants exist
and are continuously proposed in the growing ﬁeld of Hyper-parameter Optimization (HPO) (see
Bergstra et al. (2013)). Moreover, in perspective towards possible future hybridization, it is worth
noticing that Gaussian processes implemented in classic BO are generic function approximators
(like ANNs) and could be used, for example, to attempt approximating the value functions in
(2.13) and (2.14). On thus enters the realm of Bayesian Reinforcement learning (Ghavamzadeh
et al. 2015), which, to the authors’ knowledge, is fairly unexplored in ﬂuid dynamics.

The second ingredient of BO is a function suggesting where to sample next. Many variants exist
(Frazier 2018), each providing their exploration/exploitation balance. In BO, the exploration seeks
to sample in regions of large uncertainty, while exploration seeks to sample at the best location
according to the current function approximation. The most classic function, used in this study, is
the expected improvement, deﬁned as (Rasmussen 2003)

(cid:40)

EI(x) =

(∆
0
R(w+) and w+ = arg maxw

−

ξ )Φ(Z) + σ (w)φ (Z)

if σ (w) > 0
if σ (x) = 0

(3.5)

with ∆ = µ(w)
distribution (CDF), φ (Z) the probability density (PDF) of a standard Gaussian and

˜R(w) the best sample so far, Φ(Z) the cumulative

−

Z =

(cid:40) ∆

ξ
−
σ (w)
0

if σ (w) > 0
if σ (w) = 0

.

(3.6)

Eq (3.5) balances the desire to sample in regions where µ(w) is closer to R(w+) (hence large
and positive ∆ ) versus sampling in regions where σ (w) is large. The parameter ξ sets a threshold
over the minimal expected improvement that justiﬁes the exploration.

Finally, the method requires the deﬁnition of the kernel function and its hyper-parameters, as
well as an estimate of σy. In this work, the GPr based BO was implemented using the Python API
scikit-optimize (Head et al. 2020). The selected kernel function was a Mater kernel with ν = 5/2
(see Chapter 4 from Rasmussen (2003)) which reads:

κ(x, x(cid:48)) = κ(r) = 1 +

5r2
3l2 exp
where r =
2 and l the length scale of the process. In general, the performances of the BO
strongly depends on the choice of these parameters and their estimation implies knowledge of the
cost function, particularly its smoothness or the possible presence of noise. If the computational
cost of evaluating an episode is not too large, it is worth running a ﬁrst exploratory analysis and
test the Gaussian process regression on the collected data with various parameters.

√5r
l

√5r
l

x(cid:48)||

(3.7)

x
||

−

−

+

,

Algorithm 22 report the main steps of the Bayesian Optimization through Gaussian Process.
Lines (1-9) deﬁnes the GPr predictor function, which takes in input the sampled points W∗, the

Comparative analysis of machine learning methods for active ﬂow control

11

and its variance Σ

associated cumulative rewards R∗, the testing points W, and the Kernel function κ in eq. (3.7).
This outputs the mean value of the prediction µ
. The algorithm starts with the
∗
initialization of the simulated weights W∗ and rewards R∗ buffers (line 10 and 11). Prior to start
the optimization, 10 random weights W0 are tested (line 12 and 13). Within the optimization loop,
at each iteration, 1000 random points are passed to the GPr predictor, which is also fed with the
weight and rewards buffers (line 16 and 17). It provides the associated mean reward value and
variance for each weight combination. This information is then passed to an acquisition function
(line 17) which outputs a set of values A associated to the weights W+. The acquisition function
is then optimized by an optimizer (line 19). Finally, the best weights are tested in the environment
(line 20) and the buffers updated (line 21 and 22).

∗

Algorithm 1 Bayesian Optimization using GPr, adapted from Rasmussen (2003) and Pedregosa
et al. (2011)

function PREDICTOR(W∗, R∗, W, κ)

K

−

∗ ←

vT v

κ(W, W)

←
∗∗ ←
←

Compute K
κ(W∗, W∗)
Compute K
Compute KR
I
K
+ σw
∗∗
Compute Cholesky decomposition L
LT L−
Compute α
←
1
Compute v
LK−
←
return mean µ

1R∗

Kα and variance Σ

∗

KR

←

∗ ←

end function
Initialize weight buffer W∗ as null
Initialize function buffer R∗ as null
Initialize a set of 10 random weights W0
Collect reward from simulation R0
Add rewards and weights to buffers R∗ ←
for k in (1,N) do
Select 1000 random points W+
Evaluate points (µ
∗
Compute (A,W +)
←
wk
argmin
w†

, Σ
ACQFUNCTION((µ
∗

ACQFUNCTION(w†)

←

←

←

)

∗

R(W0)

Collect reward from simulation Rk
Add result to buffers R∗ ←

←
Rk and W∗ ←

R(wk)
wk

end for

R0 and W∗ ←

W0

PREDICTOR(W∗, R∗, W+, κ)
, Σ

))

∗

3.1.2. LIPschitz global Optimization (LIPO)

Like BO, LIPO relies on a surrogate model to select the next sampling points (Malherbe & Vay-
atis 2017). However, LIPO’s surrogate function is the much simpler upper bound approximation
U(w) of the cost function R(w) (Ahmed et al. 2020). In the DLIB implementation by King (2009),
used in this work, this is given by:

U(w) = min
i=1...t

(cid:113)

(cid:0)R(wi) +

σi + (w

wi)T K(w

−

wi)(cid:1),

−

(3.8)

where wi are the sampled parameters, σi are coefﬁcients which account for discontinuities and
stochasticity in the objective function, and K is a diagonal matrix that contains the Lipschitz
constants ki for the different dimensions of the input vector. We recall that a function R(w) : W

⊆

12

Rnw

→

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

R is a Lipschitz function if there exists a constant C such that:

R(w1)

R(w2)
(cid:107)

(cid:54) C

w1
(cid:107)

w2

,

w1, w2

W ,

(cid:107)

−

where

(cid:107)
is the Euclidean norm on Rnw. The Lipshitz constant k of R(w) is the smallest C that
satisﬁes the above condition (Davidson & Donsig 2009). In other terms, this is an estimate of
the largest possible slope of the function R(w). The values of K and σi are found by solving the
optimization problem:

(cid:107)·(cid:107)

−

∈

∀

(3.9)

σ 2
i

(cid:107)

K

F + 106
2

t
∑
min
(cid:107)
K,σ
i=1
s.t. U(wi) (cid:62) R(wi),
[1
∈
i, j

i
∀

σi (cid:62) 0,
Ki, j (cid:62) 0,
K =

∀
k1, k2,
{

∈

i
∀
t]
· · ·
[1
· · ·
∈
, knw}
,

[1

t]

· · ·

d]

(3.10)

where 106 is a penalty factor and

· · ·
F is the Frobenius norm.
To compensate for the poor convergence of LIPO in the area around local optima, the algorithm
alternates between a global and a local search. If the iteration number is even, it selects the new
weights by means of the maximum upper bounding position (MaxLIPO):

(cid:107)·(cid:107)

wk+1 = arg max

w

(U(w)),

(3.11)

otherwise, it relies on a Trust Region (TR) method (Powell 2006) based on a quadratic approxima-
tion of R(w) around the best weights obtained so far w∗, i.e:

wk+1 = arg max

w
s.t.

m(w;w∗)
(cid:125)(cid:124)
(cid:122)
(cid:16)
w∗ + g(w∗)T w +

1
2

wk+1
||

||

< d(w∗)

(cid:123)
(cid:17)

wT H(w∗)w

(3.12)

where g(w∗) is the approximation of the gradient at w∗ (g(w∗)
approximation of the Hessian matrix (H(w∗))i j
region. If the TR-method converges to a local optimum with an accuracy smaller than ε:

∇R(w∗))), H(w∗) is the
and d(w∗) is the radius of the trust

∂ 2R(w∗)
∂ wi∂ w j

≈

≈

the optimization goes on with the global search method, until it ﬁnds a better optimum.

R(wk)
|

−

R(w∗)
|

< ε,

wk,

∀

(3.13)

R

The algorithm 2 reports the keys steps of the MaxLIPO+TR method. First, a function
GLOBALSEARCH function is deﬁned (line 1). This performs a random global search of the
parametric space if the random number selected from S =
is smaller than p
(line3), otherwise it proceeds with MaxLIPO. In our case p = 0.02, hence the random search is
almost negligible. The upper and lower bound (U, L) of the search space are deﬁned in line 10. A
buffer object is initialized as empty (line 11). This structure logs the weight wi and their relative
reward R(wi) along the optimization. Within the learning loop (line 17), the second and third
weights are selected randomly (line 19). Then, if the iteration number k is even, the algorithm
select the next weights via GLOBALSEARCH (line 23), else it relies on the local optimization
method (line 31). If the local optimizer reaches an optimum within an accuracy of ε (line 33),
the algorithm continues exclusively with GLOBALSEARCH. At the end of each iteration, both the
local and the global models are updated with the new weights wk+1 (line 38 and 39).

0 (cid:54) x (cid:54) 1
}

x
{

∈

|

Comparative analysis of machine learning methods for active ﬂow control

13

Algorithm 2 MaxLIPO + TR (Adapted from King (2009))

(w0, R(w0))

∈
Select weights w base on MaxLIPO (Eq.(3.11))

←

←

if x

else

else

False

if k < 3 then

R S > p then

Select weights w randomly

Select weights wk randomly
Evaluate reward function R(wk)

end if
Evaluate reward function R(w)
return (w, R(w))

1: function GLOABALSEARCH
2:
3:
4:
5:
6:
7:
8:
9: end function
10: Deﬁne upper U and lower L weights’ bounds
11: Initialize buffer structure W as empty
12: Initialize weights as w0 = (U + L)/2
13: Evaluate reward function R(w0)
14: Initialize the best weight and reward (w∗, R∗)
15: Add weights and reward to the buffer W(w0, R(w0))
16: Initialize ﬂag
17: for k in (1,Ne-1) do
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46: end for
47: EndFor

if ﬂag = True then
wk, R(wk)
if R(wk) > R∗ then
False

end if
if R(wk) > R∗ then
Update (w∗, R∗)

R(wk)
R∗|
−
|
Set ﬂag
True
←
continue

if k mod 2 = 0 then
wk, R(wk)

< ε (Eq.(3.13)) then

GLOABALSEARCH()

GLOABALSEARCH()

(wk, R(wk))

Set ﬂag

end if

end if

end if

end if

else

else

←

←

←

←

end if
Update upper bound U(w) with wk(Eq.(3.8))
Update TR (m(w; w∗) Eq.(3.12))

Select weights wk based on TR (Eq.(3.12))
Evaluate reward function R(wk)
if

14

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 2: Syntax tree representation of the function 2x sin(x) + sin(x) + 3. This tree has a root ’+’
and a depth of two. The nodes are denoted with orange circles while the last entries are leafs.

3.2. Genetic Programming

In a Genetic Programming (GP) approach to optimal control, the policy a = π(s; w) is encoded
in the form of a syntax tree. The optimal parameters w∗ are not scalar quantities but lists
of numbers and functions which can include arithmetic operations, mathematical functions,
Boolean operations, conditional operations or iterative operations. An example of a syntax tree
representation of a function is shown in Figure 2. A tree (or program in GP terminology) is
composed of a root that branches out into nodes (containing functions or operations) throughout
various levels. The number of levels deﬁnes the depth of the tree, and the last nodes are called
terminals or leaves. These contain the input variables or constants. Any combination of branches
below the root is called sub-tree and can generate a tree if the node becomes a root.

Syntax trees allow encoding complex functions by growing into large structures. The ﬂexible
nature of GP allows this architecture to adapt during the training: the user does not provide a
parametrization of the policy but a primitive set, i.e. the pool of allowed functions, the maximum
depth of the tree, and set the parameters of the training algorithm. Then, the GP operates on
a population of possible candidate solutions (individuals) and evolves it over various steps
(generations) using genetic operations in the search for the optimal tree. Classic operations include
elitism, replication, cross-over and mutations, as in Genetic Algorithm Optimization (Haupt &
Ellen Haupt 2004).

The implementation of GP in this work was carried out in the Distributed Evolutionary
Algorithms in Python (DEAP) (Fortin et al. 2012) framework. This is an open-source Python
library allowing for the implementation of various evolutionary strategies. In this work, we focus
on the (µ, λ ) algorithm, which we here brieﬂy describe for completeness. The reader is referred
to the literature (Banzhaf et al. 1997; Vanneschi & Poli 2012; Kober & Peters 2014; Duriez et al.
2017) for many other possible implementations of this method.

We used a primitive set of four elementary operations (+,

) and four functions
(exp, log, sin, cos). The initial population of individuals varied between nI = 30 and nI = 50
candidates depending on the test case and the maximum depth tree was set to 17. In all test cases,
the population was initialized using a classic "half-half" approach, whereby half the population
is initialized with the full method and the rest with the growth method. In the full method, the
trees are generated with a predeﬁned depth and then ﬁlled randomly with nodes and leafs. In the
growth method, trees are randomly ﬁlled from the roots: because nodes ﬁlled with variables or
constant are terminals, this approach generates trees of variable depth.

, /,

−

×

Algorithm 3 shows the relevant steps of the learning process. First, an initial population
of random individuals (i.e. candidate control policies) is generated and evaluated (lines 1 and

Comparative analysis of machine learning methods for active ﬂow control

15

2) individually. An episode is run for each different tree structure. The population, with their
respective rewards (according to eq.2.12), is used to generate a set of λ offspring individuals. The
potential parents are selected via tournament, where new individuals are generated cross-over
(line 9), mutation (line 12) and replication (line 15): each of the new member of the population has
a probability pc, pm and pr to arise from any of these three operations, hence pc + pm + pr = 1.
The implemented cross-over strategy is the simplest (and oldest) one-point cross-over. In this
technique, two randomly chosen parents are ﬁrst broken around one randomly selected cross-over
point, generating two trees and two subtrees. Then, the offspring is created by replacing the subtree
rooted in the ﬁrst parent with the subtree rooted at the cross-over point of the second parent. Of the
two sons, only one is considered in the offspring and the other is discarded. The mutation strategy
is a one-point mutation, in which a random node (sampled with from a uniform distribution) is
replaced with any other possible node from the primitive set. The replication strategy consists in
the direct cloning of one randomly selected parent to the next generation (asexual reproduction).
The key difference between the implemented (µ, λ ) algorithm over the alternative (µ + λ ) is
in the tournament selection process: in the ﬁrst, the tournament only involves the offspring (λ ),
while in the second also parents could be re-selected.

The new population is created by selecting the best individuals, based on the obtained reward,

among the old population B(i

1) and the offspring ˜B (line 19).
−

∈

←

(0, 1)

Select random number ζ
if ζ < pc then

Random sample two individuals (am,an) from B(i
Compute offspring individual ˜ai
Mate(am,an)

Initialize offspring population ˜B with λ individuals as empty.
for t in (1,λ ) do

Algorithm 3 GP (µ + λ )-ES (Adapted from Beyer & Schwefel (2002))
1: Initialize population B(0) with µ random individuals ai.
(wi, R(wi))
2: Evaluate ﬁtness ai
3: for i in (1,Ne) do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end for

end for
Evaluate ﬁtness of mated and mutated ˜ai
Update population B(i)

Random sample an individual (am) from B(i
Compute offspring individual ˜ai

Random sample an individual (am) from B(i
Compute offspring individual ˜ai

else if ζ < (pc + pm) then

←
Select(B(i), ˜B,µ)

Mutate(am)

(wi, R(wi))

end if

else

am

←

←

←

←

1)

1)

1)

−

−

−

3.3. Reinforcement Learning via DDPG

The Deep Deterministic Policy gradient (DDPG) by Lillicrap et al. (2015) is an off-policy
actor-critic algorithm using an ANN to learn the policy and an ANN to learn the Q function. In
what follows, we call Π - network the ﬁrst (i.e. the actor) and Q-network the second (i.e. the critic).
The DDP combines the DPG by Silver et al. (2014) and the Deep Q learning (DQN) by Mnih
et al. (2013, 2015). The algorithm has evolved into more complex versions such as the Twin
Delayed DDPG (Fujimoto et al. 2018), but in this work we focus on the basic implementation.

16

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

The policy encoded in the Π network is deterministic and acts according to the set of weights
and biases wπ , i.e. a = π(st , wπ ). The environment is assumed to be stochastic and modelled
as a Markov Desition Process. Therefore, (2.14) must be modiﬁed to introduce an expectation
operator:

E

(cid:2)r(st , at ) + γQπ (st+1, aπ

Qπ (st , at ) = Est ,st+1∼
(3.14)
where the policy is intertwined in the action state relation, i.e. Qπ (st+1, at+1) = Qπ (st+1, aπ (st+1))
and having used the shorthand notation aπ
t+1 = π(st+1, wπ ). Because the expectation operator
in (3.14) solely depends on the environment (E in the expectation operator), it is possible to
decouple the problem of learning the policy π from the problem of learning the function Qπ (st , at ).
Concretely, let Q(st , at ; wQ) denote the prediction of Q function by the Q network, deﬁned with
weights and biases wQ and let T denote a set of N transitions (st , at , st+1, rt+1) collected through
(any) policy. The performances of the Q-network can be measured as

t+1)(cid:3) ,

JQ(wQ) = Est ,at ,rt

T

∼

(cid:104)(cid:16)

Q(cid:0)st , at ; wQ)

(cid:17)2(cid:105)

,

yt

−

(3.15)

where

yt = r(st , at ) + γQ(st+1, at+1; wQ) .
Equation (3.15) measures how closely the prediction of the Q network satisﬁes the discrete
Bellman equation (2.14). The training of the Q network can be carried out using standard stochastic
gradient descent methods using the back-propagation algorithm (Kelley 1960) to evaluate the
gradient ∂wQJQ.

(3.16)

The training of the Q-network gives the off-policy ﬂavour to the DDPG because it is carried
out with an exploratory policy that might largely differ from the policy that will be ﬁnally
selected. Nevertheless, because the training of the Q-network is notoriously unstable, Mnih et al.
(2013, 2015) introduced the use of a replay buffer to leverage accumulated experience (previous
transitions) and a target network to under-relax the update of the weights during the training. Both
the computation of the cost function in (3.15) and its gradient are performed over a random batch
of transitions T in the replay buffer R.

The DDPG combines the Q-network prediction with a policy gradient approach to train the
Π -network. This is inherited from the DPG by Silver et al. (2014), who have shown that, given

(3.17)
E,at
the expected return from the initial condition, the gradient with respect to the weights in the Π
network is:

∼

∼

π

Jπ (wπ ) = Est

(cid:2)(r(st , at ))(cid:3)

∂wπ Jπ = Est

(3.18)
Both ∂aQ(st , at ; wQ) and ∂wπ a(st ; wπ ) can be evaluated via back-propagation, on the Q network
and the Π network respectively. The main extension of DDPG over DPG is the use of DQN for
the estimation of the Q function.

(cid:2)∂aQ(st , at ; wQ) ∂wπ a(st ; wπ )(cid:3) .

E,at

∼

∼

π

In this work, we implement the DDPG using KERAS API in PYTHON with three minor
modiﬁcations to the original algorithm. The ﬁrst is a clear separation between the exploration and
the exploitation phases. In particular, we introduce a number of exploratory episodes nEx < nE p
and the action is computed as

a(st ) = a(st ; wπ ) + η(ep)E (t; θ , σ 2) ,
(3.19)
where E (t; θ , σ ) is an exploratory random process characterized by a mean θ and variance σ 2.

Comparative analysis of machine learning methods for active ﬂow control

17

Figure 3: ANN Architecture of the DDPG implementation analyzed in this work. The illustrated
architecture is the one used for the test case in section 4.3. During the exploration phase, the
two networks are essentially decoupled by the presence of the stochastic term E that leads to
exploration of the action space.

This could be the time-correlated (Uhlenbeck & Ornstein 1930) noise or white noise, depending
on the test case at hand (see Sec. 4). The transition from exploration to exploitation is governed
by the parameter η ep, which is taken as η(ep) = 1 if ep < nEx where dep
nEx if ep > nEx. This
−
decaying term for ep > nE p progressively reduces the exploration and the coefﬁcient d controls
how rapidly this is done.

The second modiﬁcation is in the selection of the transitions from the replay buffer R that are
used to compute the gradient ∂wQJQ. While the original implementation selects these randomly,
we implement a simple version of the prioritized experience replay from Schaul et al. (2018). The
idea is to prioritize, while sampling from the replay buffer, those transitions which have lead to the
largest improvement in the network performances. These can be measured in terms of Temporal
Difference Error (or TD-Error):

δ = rt + γQ(si+1, aπ

t+1; wQ)
Intuitively, this quantity measures how much a transition was unexpected. As discussed by
Schaul et al. (2018), it can be shown that this increases the learning rate of the algorithm towards
the steepest gradients ∂wQJQ, and helps overcoming local minima. Ranking the transitions in
decreasing order of TD error δ , the sampling is performed following a triangular distribution
which assigns the highest probability p(n) to the transition with the largest TD error δ .

Q(si, ai; wQ) .

(3.20)

−

The third modiﬁcation, extensively discussed in previous works on reinforcement learning for
ﬂow control (Rabault & Kuhnle 2019; Tang et al. 2020; Rabault et al. 2020), is the implementation
of a moving average of the actions. In other words, an action is performed for K consecutive
interactions with the environment, which in our work occur at every simulation’s time step.

We illustrate the neural network architecture employed in this work in Figure 3. The scheme
in Figure shows how the Π network and the Q network are interconnected: intermediate layers
map the current state and the action (output by the Π network) to the core of the Q network.
For plotting purposes, the number of neurons in the ﬁgure is much smaller than the one actually
used and indicated in the ﬁgure. The Π network has two hidden layers with 128 neurons each,
while the input and output depends on the test cases considered (see Sec. 4). Similarly, the Q
network has two hidden layers with 128 neurons each and intermediate layers as shown in the
ﬁgure. During the exploration phase, the presence of the stochastic term in the action selection
decouples the two networks.

18

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

We recall the main steps of the DDPG algorithm in algorithm 4. After random initialization
of the weights in both network and the initialization of the replay buffer (lines 1-3), the loop
over episodes and time steps proceeds as follows. The agent begins from an initial state (line 5),
which is simply the ﬁnal state of the system from the previous episode or the last state from the
uncontrolled dynamics. In other words, none of the investigated environments has a terminal state
and no re-inizialitation is performed.

Within each episode, at each time step, the DDPG takes actions (lines 7-12) following (3.19)
(line 8) or copying the previous action (line 10). After storing the transition in the replay buffer
(lines 13), these are ranked based on the associated TD error δ (line 14). This is used to sample
a batch of N transitions following a triangular distribution favouring the transitions with the
highest δ . The transitions are used to compute the cost functions JQ(wQ) and Jπ (wπ ) and their
gradients ∂wQJ(wπ ), ∂wπ J(wπ ) and thus update the weights following a gradient ascent (lines 17
and 19). This operation is performed on the ’current networks’ (deﬁned by the weights wπ and
wQ). However, the computation of the critic losses JQ is performed with the prediction yt from
the target networks (deﬁned by the weights wπ(cid:48) and wQ(cid:48)). The targets are under-relaxed updates
of the network weights computed at the end of each episode (lines 21-22).

The reader should notice that, differently from the other optimization-based approaches, the

update of the policy is performed at each time step and not at the end of the episode.

Algorithm 4 DDPG (Adapted from Lillicrap et al. (2015))

2: Initialize targets wQ(cid:48)

Initialize Q(s, a; wq) and π(s; wπ ) with random wq and wπ .
wQ and wπ(cid:48)
Initialize replay Buffer R as empty.

wπ .

←

←

4: for ep in (1,nE ) do

6:

8:

10:

12:

14:

16:

18:

20:

22:

Observe initial state s0
for t in (1,T) do

if t = 1 or mod(t, K) = 0 then

at = a(st ; wπ ) + η(ep)N (t; θ , σ )

else

at = at

1
−

end if
Execute at , get rt and observe st+1
Store the transitions (st , at , rt , st+1) in R
Rank the transition by TD error δ
Select N transitions in R, favouring the highest δ
Compute yt = rt + γQ(cid:48)(st , π(st , wπ(cid:48)))
Compute JQ = E(yt
Update wQ
←
Compute Jπ (wπ(cid:48)) and ∂wπ(cid:48)
wπ + αq∂wπ(cid:48)
Update wπ
←
Update targets in Q: wQ(cid:48)
Update targets in π: wπ(cid:48)

−
wQ + αq∂wQJQ
Jπ
Jπ
τwQ(cid:48) + (1
τwπ(cid:48) + (1

τ)wQ(cid:48)
τ)wπ(cid:48)

Q(st , π(st , wπ ))) and ∂wQJQ

←
←

−
−

end for

24: end for

In our implementation, we used the Adam optimizer for training the ANN’s with a learning
3 for the actor and the critic, respectively. The discount factor was set
rate of 10−
3. For what concerns the neural
to γ = 0.99 and the soft-target update parameters is τ = 5
networks architecture, the hidden layers used the rectiﬁed non-linear activation function, while

3 and 2

10−

10−

·

·

Comparative analysis of machine learning methods for active ﬂow control

19

the actor output was bounded relying on a hyperbolic tangent (tanh). The actor’s network was
nsx256x256xna, where ns is the number of states and na is the number of actions expected by the
environment. Finally, the critic’s network concatenates two networks. The ﬁrst, from the action
taken by the agent composed as nax64. The states are elaborated in two layers: nsx32x64. These
are concatenated and expanded by means of two layers 256x256x1 - where the output is the value
estimated.

4. Test Cases

4.1. A 0D Frequency Cross-Talk Problem

The ﬁrst selected test case is a system of nonlinear ODEs reproducing one of the main features
of turbulent ﬂows: the frequency cross-talk. This control problem was proposed and extensively
analysed by Duriez et al. (2017). It essentially consists in stabilizing two coupled oscillators which
describe the time evolution of four leading Proper Orthogonal Decomposition (POD) modes of
the ﬂow past a cylinder. The model is known as generalized mean ﬁeld model (Dirk et al. 2009)
and it was used to describe the stabilizing effect of low frequency forcing on the wave ﬂow past a
bluff body (Aleksic et al. 2010; Pastoor et al. 2008).

The set of ODEs which describes the evolution of the four POD modes (s(t) =

[s1(t), s2(t), s3(t), s4(t)]T ), where (s1, s2) and (s3, s4) are the ﬁrst and second oscillator, reads:

where a is the forcing vector with a single scalar component interacting with the second oscillator
(i.e., a = [0, 0, 0, a]T ) and the matrix F(s) is given by:

˙s = F(s) s + a,

(4.1)

F(s) =







σ (s)
1
0
0

1
−
σ (s)
0
0

0
0
0.1
10

−

and σ models the coupling between the two oscillators:







,

0
0
10
0.1

−
−

where E1 and E2 are the energy of the ﬁrst and the second oscillator given by:

σ (s) = 0.1

E1

−

−

E2,

E1 = s2

1 + s2

2 E2 = s2

3 + s2
4.

(4.2)

(4.3)

(4.4)

This nonlinear link is the essence of the frequency cross-talk and challenges linear control methods
based on linearization of the dynamical system.

The initial conditions are set to a(0) = [0.01, 0, 0, 0]T . Without actuation, the system reaches a
‘slow’ limit cycle involving the ﬁrst oscillator (a1, a2) while the second vanishing ((a3, a4)
0).
The evolution of the oscillator (a1, a2) with no actuation is shown in Figure 4a. Figure 4b shows
the time evolution of σ , which σ

0 as the system naturally reaches the limit cycle.

→

The governing equations (Eq.4.1) were solved using scipy’s package odeint with a time step of
∆ t = π/50. This time step is smaller than the one by Duriez et al. (2017) (∆ t = π/10), as we
observed this had an impact on the training performances (aliasing in LIPO and BO optimization).
Finally, the actions are clipped to the range ak
[
−
The actuators’ goal is to bring to rest the ﬁrst oscillator, leveraging on the non-linear connection
between the two oscillators while using the least possible actuation. In this respect, the optimal
control law, similarly to Duriez et al. (2017), is the one that minimizes the cost function:

1, 1].

∈

→

20

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

(a)

(b)

Figure 4: Evolution of the oscillator (a1, a2) (a) of the variable σ (4.3) (b) in the 0D test case in
absence of actuation (b = 0). As σ
0, the system naturally evolves towards a ‘slow’ limit cycle.

≈

J = Ja + γ Jb = s2

1 + s2
2 + α a2
60π
(cid:90)

1
40π

20π

where

f (t) =

f (t(cid:48))dt(cid:48),

.

(4.5)

where α is a coefﬁcient set to penalize large actuations and is set to 10−
The time interval of an episode is here set to t

2.
[20π, 60π], thus much shorter than the one
used by Duriez et al. (2017). This duration was considered sufﬁcient, as it allows the system to
reach the limit cycle and to observe approximately 20 periods of the slow oscillator. To reproduce
the same cost function in a reinforcement learning framework, we rewrite (4.5) as a cumulative
reward, replacing the integral mean with the arithmetic average and setting:

∈

J =

1
nt

nt
1
−
∑
k=0

1k + s2
s2

2k + αa2

k =

nt
1
−
∑
k=0

−

rt =

R,

−

(4.6)

with rt the environment’s reward at each time step.

For the BO and LIPO optimizers, the control law is deﬁned as a quadratic form of the four

system’s states:

(4.7)
R20 and it
with gw
collects all the entries in gw and Hw. For later reference, the labelling of the weights is as follows:

ws + sT Hws,
R4x4. The weight vectors associated to this policy is thus w

π(s; w) := gT

R4 and Hw

∈

∈

∈

gk =







w1
w2
w3
w4







and Hk =







w5 w9 w13 w17
w6 w10 w14 w18
w7 w11 w15 w19
w8 w12 w16 w20







.

(4.8)

Both LIPO and BO seek for the optimal weights in the range [-3,3]. BO was set up with
a Matern kernel (see (3.7)) with a smoothness parameter ν = 1.5, a length scale of 0.01, an
acquisition function based on the expected improvement and an exploitation-exploration (see
(3.5)) trade-off parameter ξ of 0.1. All the simulations were run for 100 episodes except for the

Comparative analysis of machine learning methods for active ﬂow control

21

· · ·

GP, which was set with an initial population of 30 individuals (µ = 30), an offspring size of 60
(λ = 60) and it was trained for 20 generations.

In the DDPG implementation, we scale the reward stored in the replay buffer (rRB

) with
the environment reward values obtained until then, which are stored in a dynamical vector
rlog = [r1, r2,

, rt ]:

t

−

10

rRB
t =

rt
¯rlog
std(rlog) + 1e
where ¯rlog is the mean value and std(rlog) is the standard deviation. The stored reward are then
ordered based on their temporal difference (see (3.20)), promoting experiences with high error.
The experiences are collected with an exploration strategy structured into three parts: a heavy
explorative phase (until episode 30) during which the noise is clipped in the range [-0.8,0.8] with
η = 1 (see (3.19)). An off-policy exploration phase (between episode 30 and 55) with a noise
signal clipped in the range [-0.25,0.25] with a magnitude of 0.25 and a pure exploitation phase
where the noise is almost negligible. As explorative signal, we used a white noise with a standard
deviation of 0.5.

(4.9)

−

4.2. Control of the viscous Burgers’s equation
We consider Burger’s equation because it offers a simple 1D problem combining nonlinear

advection and diffusion. The problem set is:

∂t u + u∂xu = ν∂xxu + f (x,t) + c(x,t),

u(x, 0) = u0

∂xu(0,t) = ∂xu(L,t) = 0

(4.10)

(0, L)

where (x,t)
(0, T ] with L = 20 and T = 15 is the episode length, ν = 0.9 is the kinematic
viscosity, f (x,t) is a disturbance term and c(x,t) is the control function. These are both Gaussian
functions in space, modulated by a time varying amplitude:

×

∈

f (x,t) = A sin (2π f pt)
c(x,t) = a(t)

N (x

·

−

x f , σ ),

N (x
·
xa, σ ),

−

(4.11)

(4.12)

taking A = 100 and f p = 0.5 for the disturbance’s amplitude and frequencies and denoting as
a(t) the action function provided by the controller. The disturbance and the controller action
are centered at x f = 6.6 and xc = 13.2 respectively and have σ = 0.2. The uncontrolled system
produces a set of nonlinear waves propagating in both directions at approximately constant
velocities. The objective of the controller is to neutralize the waves downstream the control
location, i.e. for x > xc. The controller is informed by three observations, located at x = 8, 9, 10.
The action is then computed as

a(t) = w0 u(8,t) + w1 u(9,t) + w2 u(10,t) .
The controller’s performance is measured by the reward function:

r(t) =

(cid:16)
(cid:96)2(ut )Ωr + α

−

a(t)2(cid:17)

·

(4.13)

(4.14)

R

x
{

15.4 (cid:54) x (cid:54) 16.4
}
|

)Ωr is the Euclidean norm of the displacement ut at time step t over a portion of the
where (cid:96)2(
·
called reward area, α is a penalty coefﬁcient and at is the
domain Ωr =
value of the control action selected by the controller. The cumulative reward is computed with
a discount factor γ = 1while the penalty in the actions was set to α = 100. Figure 5 shows the
evolution of the uncontrolled system in a contour plot in the space-time domain, recalling the
location of perturbation, action, observation and reward area.

∈

22

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 5: Contour plot of the spatio-temporal evolution of u in (4.10) for the uncontrolled problem,
i.e c(x,t) = 0.. The perturbation is centered at x = 6.6 (red continuous line) while the control law
is centered at x = 13.2. The dashed black lines visualize the location of the observation points
while the region within the white dash-dotted line is used to evaluate the controller performance.

Eq.(4.10) was solved using Crank–Nicolson’s method. The Neumann boundary conditions are
enforced using ghost cells, and the system is solved at each time step via the banded matrix solver
solve_banded from the python library scipy. The mesh consists of nx = 1000 points and the time
stepping is ∆t = 0.01, thus leading to nt = 1500 steps per episode.

Both LIPO and BO optimizers operate within the bounds [-0.1, 0.1] for the weights to avoid
saturation in the control action. The overall set-up of the BO is the same as the one used in the
0D test case. For the GP, the selected evolutionary strategy is (µ + λ ), with the initial population
of 10 individuals µ = 10 and an offspring λ = 20. The DDPG agent set-up relies on the same
reward normalization and buffer prioritization presented for the previous test case. However, the
trade-off between exploration and exploitation was handled differently: the random noise term in
(3.19) is set to zero every N = 3 episodes to prioritize exploitation. This noise term was taken as
an Ornstein-Uhlenbeck, time-correlated noise with θ = 0.15 and dt = 1e
3 and its contribution
was clipped in [-0.3, 0.3].

−

4.3. Control of the von Kármán street behind a 2D cylinder
The third test case is controlling the 2D viscous and incompressible ﬂow past a cylinder in
a channel. The ﬂow past a cylinder is a classic benchmark for bluff body wakes (Zhang et al.
1995; Noack et al. 2003), exhibiting a supercritical Hopf bifurcation leading to the well known
von Kármán vortex street. The cylinder wake conﬁguration within a narrow channel has been
extensively used for CFD benchmark purposes (Schäfer et al. 1996) and as a test case for ﬂow
control techniques (Rabault et al. 2019; Tang et al. 2020; Li & Zhang 2021).

We consider the same control problem as in Tang et al. (2020), sketched in Figure 6. The
computational domain is a rectangle of width L and height H, with a cylinder of diameter
D = 0.01m located slightly off the symmetric plane of the channel (cf. Fig. 6). This asymmetry
triggers the development of vortex shedding.

The reference system is located at the center of the cylinder. At the inlet (x =

velocity proﬁle is imposed:

2D), a parabolic

−

uinlet = −

(cid:16)

y2

4Um
H2

0.1Dy

4.2D2(cid:17)
,

−
where Um = 1, 5m/s. This leads to a Reynolds number of Re = UD/ν = 400 using the mean inlet
4m2/s. It is
velocity U = 2/3Um as a reference and taking a kinematic viscosity of ν = 2.5e
worth noticing that this is much higher than Re = 100 considered by Jin et al. (2020), who deﬁnes
the Reynolds number based on the maximum velocity.

−

−

(4.15)

Comparative analysis of machine learning methods for active ﬂow control

23

Figure 6: Geometry and observations probes for the 2D von Kármán street control test case. The
256 observations used by Tang et al. (2020) are shown with black markers. These are organized
in three concentric circles (diameters 1 + 0.002/D, 1 + 0.02D and 1 + 0.05D) around the cylinder
and three grids (horizontal spacing c1 = 0.025/D, c2 = 0.05/D and c3 = 0.1/D). All the grids
have the same vertical distance between adjacent points (c4 = 0.05/D). The ﬁve observations
used in this work (red markers) have coordinates s1(0,
1) and s4(1, 1)
and s5(1, 0). Each probe samples the pressure ﬁeld.

1.5), s2(0, 1.5), s3(1,

−

−

The computational domain is discretized with an unstructured mesh reﬁned around the cylinder,
and the incompressible Navier-Stokes equations are solved using the incremental pressure
correction scheme (IPCS) method in the FEniCS platform (Alnæs et al. 2015). The mesh consists
of 25865 elements and simulation time step is set to ∆t = 1e
4[s] to respect the CFL condition.
The reader is referred to Tang et al. (2020) for more details on the numerical set-up and the mesh
convergence analysis.

−

In the control problem, every episode is initialized from a snapshot that has reached a developed
shedding condition. This was computed by running the simulation without control for T = 0.91s
= 3T ∗, where T ∗ = 0.303s is the vortex shedding period. We computed T ∗ by analysing the
period between consecutive pressure peaks observed by probe s5 of an uncontrolled simulation.
The result is the same as the one found by Tang et al. (2020), who performed a Discrete Fourier
Transform (DFT) of the drag coefﬁcient.

The instantaneous drag and lift on the cylinder are calculated via the surface integrals:

(cid:90)

FD =

(σ

n)

·

·

ex dS,

(cid:90)

FL =

(σ

n)

·

·

ey dS,

(4.16)

where S is the cylinder surface, σ is the Cauchy stress tensor, n is the unit vector normal to the
cylinder surface, ex and ey are the unit vectors of the x and y axes respectively. The drag and lift
coefﬁcient are calculated as CD = 2FD/(ρ ¯U 2D) and CL = 2FL/(ρ ¯U 2D) respectively.

The control action consists in injecting/removing ﬂuid from four synthetic jets positioned on the
cylinder boundary as shown in Figure 7. The jets are symmetric with respect to the horizontal and
vertical axes. These are located at θ = 75o, 105o, 255o, 285o and have the same width ∆ θ = 15o.
The velocity proﬁle in each of the jets is taken as:

u jet (θ ) =

π
∆ θ D

Q∗i cos

(cid:16) π
∆ θ

(θ

−

(cid:17)

θi)

(4.17)

where θi is the radial position of the i-th jet and Q∗i is the imposed ﬂow rate. Eq (4.17) respects
the non-slip boundary conditions at the walls. To ensure a zero-net mass injection at every time
i Qi the mean value of the four
step, the ﬂow rates are mean shifted as Q∗i = Qi
ﬂow rates.

¯Q with ¯Q = 1

4 ∑4

−

xy2D2.1DH2D20DLR(cid:31)d1(cid:31)d2(cid:31)d310◦c1c2c3c4s1s2s3s4s524

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 7: Location of the four control jets for the 2D von Kármán street control test case. These
are located at θ = 75o, 105o, 255o, 285o and have width ∆ θ = 15o. The velocity proﬁle is deﬁned
as in (4.17), with ﬂow rate deﬁned by the controller and shifted to have zero-net mass ﬂow.

The ﬂow rates in the four nozzle constitute the action vector, i.e. a = [Q1, Q2, Q3, Q4]T in the
formalism of Section 2. To avoid abrupt changes in the boundary conditions, the control action is
kept constant for a period of Tc = 10∆t = 1e
3[s]. This is thus equivalent to having a moving
average ﬁltering of the controller actions with impulse response of length N = 10. The frequency
modulation of such a ﬁlter is

−

H(ω) =

1
10

(cid:12)
(cid:12)
(cid:12)

sin(5ω)
sin(ω/2)

(cid:12)
(cid:12)
(cid:12)

(4.18)

with ω = 2π f / fs. The ﬁrst zero of the ﬁlter is located at ω = 2π/5, thus f = fs/5 = 2000Hz,
while the attenuation at the shedding frequency is negligible. Therefore, this ﬁltering allows the
controller to act freely within the range of frequencies of interest to the control problem, while
preventing abrupt changes that might compromise the stability of the numerical solver. Each
episode has a duration of T = 0.91s, corresponding to 2.73 periods of the vortex shedding in
uncontrolled conditions. This allows having 91 interactions per episode (i.e. 33 interactions per
vortex shedding period.

The actions are linked to the pressure measurements (observations of the ﬂow) in various
locations. In the original environment by Tang et al. (2020), 256 probes were used, similarly to
Rabault et al. (2019). The locations of these probes are shown in Figure 6 using black markers. In
this work, we reduce the set of probes to ns = 5. A similar conﬁguration was analyzed by Rabault
et al. (2019) although using different locations. In particular, we kept the probes s1 and s2 at the
same x coordinate, but we moved them further away from the cylinder wall to reduce the impact
of the injection on the sensing area. Moreover, we slightly move the sensors s3, s4, s5 downstream
in regions where the vortex shedding is more strongly present.

The locations used in this work are recalled in Figure 6. The state vector, in the formalism
of Section 2, is thus the set of pressure at the probe locations, s = [p1, p2, p3, p4, p5]T . For the
optimal control strategy identiﬁed via the BO and LIPO algorithms in Section 3.1.1 and 3.1.2, a
linear control law is assumed, hence a = Ws, with the 20 weight coefﬁcients labelled as follows







=







Q1
Q2
Q3
Q4







w2
w7

w4
w5
w3
w1
w9 w10
w8
w6
w11 w12 w13 w14 w15
w16 w17 w18 w19 w20























p1
p2
p3
p4
p5

.

(4.19)

It is worth noticing the zero-net mass condition enforced by removing the average ﬂow rate

xy∆θθ∆θQ2Q1Q3Q4∆θ∆θComparative analysis of machine learning methods for active ﬂow control

25

from each action could be easily imposed by constraining all columns of W to add up to zero.
For example, setting the symmetry w1 =
Q3 and
w11, w6 =
Q4) allows for halving the dimensionality of the problem, thus signiﬁcantly improving the
Q2 =
optimization performances. Nevertheless, one has inﬁnite ways of embedding the zero-net mass
condition. We do not impose any and let the control problem act in R20.

w16, etc. (leading to Q1 =

−

−

−

−

Finally, the instantaneous reward rt is deﬁned as

(4.20)

rt =

FD
(cid:104)

T
(cid:105)

α

FL

T
(cid:105)

where

|(cid:104)
Tc is the moving average over Tc = 10∆t and α is the usual penalization parameter
set to α = 0.2. This penalization term prevents the control strategies from relying on the high lift
ﬂow conﬁgurations Rabault et al. (2019). The cumulative reward was given with γ = 1.

(cid:104)•(cid:105)

−

|

th interaction was scaled with a factor χ = 2e

The search space for the optimal weights in LIPO and BO was bounded to [-1, 1]. Moreover,
the action resulting from the linear combination of such weights with the states collected in the
i
3, to avoid numerical instabilities. The BO
−
settings are the same as in the previous test-cases, exception made for the smoothness parameter ν
that was reduced to ν = 1.5. On the GP side, the evolutionary strategy applied was the eaSimple’s
(Back & Michalewicz 2000) implementation in Deap - with hard-coded elitism to preserve the
best individuals. It is noted how one population per each jet was used, to retain generality in the
description performed by GP and not to misrepresent its proposed solution.

−

Finally, the DDPG agent was trained using the same exploration policy of the Burgers’ test-case,
alternating 20 exploratory episodes with the η = 1 and 45 exploitative episodes with η = 0 (c.f eq
(3.19)). During the exploratory phase, an episode with η = 0 is taken every N = 4 episodes and
the policy weights are saved. We used the Ornstein-Uhlenbeck time correlated noise with θ = 0.1
and dt = 1e

2 in eq. (3.19), clipped in [-0.5, 0.5].

−

5. Results and Discussions

We present here the outcomes of the different control algorithms in terms of learning curves
and control actions for the three investigate test cases. Given the heuristic nature of these control
strategies, we ran several training sessions for each, using different seeding values for the random
number generator. We deﬁne as learning curve, the upper bound of the cumulative reward R(w)
in (2.12) obtained at each episode within the various training sessions. Moreover, we deﬁne as
learning variance, the variance of the global reward between the various training sessions at each
episode. We considered ten training sessions for all environments and for all control strategies. In
the episode counting shown in the learning curves and learning variance, it is worth recalling that
the BO learning initially performs 10 explorative iterations. For the GP, each iteration involves np
episodes, with np the number of individuals in the population.

All the training details, i.e. the optimal weights found by the optimizers or the best tree found

by the GP, are reported in the appendix.

5.1. The 0D Frequency Cross-talk problem

We here report on the results for the four algorithms for the 0D problem in Section 4.1. All
implemented methods found strategies capable of solving the control problem, bringing to rest the
ﬁrst oscillator (s1, s2) while exiting the second (s3, s4). Table 2 collects the ﬁnal best cumulative
reward for each control method together with the conﬁdence interval, deﬁned as 1.96 time the
standard deviation within the various training sessions.

The control law found by the GP yields the highest reward and the highest variance. Figures 8a

and 8b show the learning curve and learning variance for the various methods.

The learning curve for the GP is initially ﬂat because the best reward from the best individuals

26

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

3
10−
·
Best
Reward

LIPO

BO

GP

DDPG

-8.96

0.75 -9.41

±

1.33 -2.77

±

1.49 -2.98

±

1.37

±

Table 1: Mean optimal cost function (bold) and conﬁdence interval (over 10 training sessions with
different random number generator seeds) obtained ad the end of the training for the 0D frequency
cross-talk control problem.

(a) Learning curve

(b) Learning curve variance

Figure 8: Comparison of the learning curves (a) and their variances (b) for different machine
learning methods for the 0D test case (Sec. 4.1).

Figure 9: Orbit of the second oscillator (s3, s4) in the 0D control problem governed by Eq.(4.1))
(left column of Fig.2) in the last part of the episode (from 194s to 200s). The coloured curves
corresponds to the four control methods.

100101102103ep10−210−1-R(w)BOLIPOGPDDPG100101102103ep10−310−2σRBOLIPOGPDDPG−2−1012s3−2−1012s4LIPOBOGPDDPGComparative analysis of machine learning methods for active ﬂow control

27

Figure 10: Weights of the control action for the 0D control problem in (10). The coloured bars
represent a standard deviation around the mean value found by LIPO and BO.

of each generation is taken after all individuals have been tested. Considering that the starting
population consists of 30 individuals, this shows that approximately three generations are needed
before signiﬁcant improvements are evident. In its simple implementation considered here, the
distinctive feature of the GP is the lack of a programmatic explorative phase: exploration proceeds
only through the genetic operations, and their repartition does not change over the episodes. This
leads to a relatively constant (and signiﬁcant) reward variance over the episodes. Possible variants
to the implemented algorithms could be the reduction of the explorative operations (e.g. mutation)
after various iterations (see, for example Mendez et al. (2021)). Nevertheless, the extensive
exploration of the function space, aided by the large room for manoeuvre provided by the tree
formalism, is arguably the main reason for the success of the method, which indeed ﬁnds the
control law with the best cumulative reward (at the expense of a much larger number of episodes).
In the case of the DDPG, the steep improvement in the learning curve in the ﬁrst 30 episodes
might be surprising, recalling that in this phase the algorithm is still in its heavy exploratory phase
(see Sec. 3.3). This trend is explained by the interplay of two factors: (1) we are showing the upper
bound of the cumulative reward and (2) the effectiveness of a random search in the early training
phase. Indeed, improvements over a (bad) initial choice are easily achieved by the stochastic
search, while smarter updates are needed as the performances improve. This result highlights
the importance of the stochastic contribution in (3.19), and its adaptation during the training to
balance exploration and exploitation.

The learning behaviour of BO and LIPO is similar. Both have high variance in the early stages,
as the surrogate model of the reward function is inaccurate. But both manage to obtain non-
negligible improvements over the initial choice while acting randomly. The reader should notice
that the variance of the LIPO at the ﬁrst episode is 0 for all trainings because the initial points are
always taken in the middle of the parameter space, as reported in Algorithm 2. For both methods,
the learning curve steepens once the surrogate models become more accurate but reach a plateau
that has surprisingly low variance after the tenth episode. This behaviour could be explained by
the difﬁculty of both the LIPO and GPr models in representing the reward function.

Comparing the different control strategies identiﬁed by the four methods, the main difference
reside in the settling times and energy consumption. Fig.11 shows the evolution of s1 and s2 from
the initial conditions to the controlled conﬁguration for each method.

As shown in Eq.(4.5), the cost function accounts mainly for the stabilization of the ﬁrst oscillator
and the penalization of too strong actions. In this respect, the better overall performance of the GP

w1w2w3w4w5w6w7w8w9w10w11w12w13w14w15w16w17w18w19w20−2−10123Values0DcontrolfunctionparametersLIPOBO28

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez
LIPO

BO

GP

DDPG

Table 2: Evolution of the best control function a (continuous blue line with squares), the energy of
the ﬁrst oscillator (continuous red line with triangles) and the energy of the second one (dashed
red line), for the different control methods. The ﬁgures on the left report the early stage of the
simulation, until the onset of a limit cycle condition, and those on the right the ﬁnal time steps.

is also visible in the transitory phase of the ﬁrst oscillator, shown in Fig.2, and in the evolution
of the control action. These are shown in Table 2 for all the investigated algorithms. For each
algorithm, the ﬁgure on the left shows the action policy and the energy E1 (continuous red line
with triangles) and E2 (dashed red line) (see equation (4.4)) of the two oscillators in the time span

626568707275788082t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy194195196197198199200t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy626568707275788082t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy194195196197198199200t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy626568707275788082t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy194195196197198199200t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergy626568707275788082t−1.00−0.75−0.50−0.250.000.250.500.751.00aactionr·r◦01234oscillatorenergy194195196197198199200t−1.00−0.75−0.50−0.250.000.250.500.751.00a01234oscillatorenergyComparative analysis of machine learning methods for active ﬂow control

29

Figure 11: Evolution of the states s1 and s2, associated with the unstable oscillator, obtained using
the optimal control action provided by the different machine learning methods.

t = 62
−
time span t = 194

−

82, i.e. during the early stages of the control. The ﬁgure on the right shows a zoom in the

200, once the system has reached a steady (controlled) state.

The control actions by LIPO and BO are qualitatively similar and results in small oscillation in
the energy of the oscillator. Both sustain the second oscillator with periodic actions that saturates.
The periodicity is in this case enforced by the simple quadratic law that these algorithms are called
to optimize. The differences in the two strategies can be well visualized by the different choice of
weights (cf. equation (4.8)), which are shown in Figure 10. While the BO systematically gives
considerable importance to the weight w10, which governs the quadratic response to the state s2,
the LIPO favors a more uniform choice of weighs, resulting in a limited saturation of the action
and less variance. The action saturation clearly highlight the limits of the proposed quadratic
control law. Both LIPO and BO give a large important to the weight w4 because this is useful
in the initial transitory to quickly energize the second oscillator. However, this term becomes a
burden once the ﬁrst oscillator is stabilized and forces the controller to over-react. Interestingly,
simplifying the control law to the essential terms

a = s1w1 + s4w2 + s1s4w3,

(5.1)

allows the LIPO to identify a control law with comparable performances in less than ﬁve iterations.
The GP and the DDPG use their larger model capacity to propose laws that are far more
complex and more effective. The GP selects an impulsive control (also reported by Duriez et al.
(2017)) while the DDPG proposes a periodic forcing. The impulsive strategy of the GP performs
better than the DDPG (according to the metrics in 4.5) because it consumes less energy. This can
be also shown by plotting the orbits of the second oscillator under the action of the four controller,
as done in Figure 9. Indeed, an impulsive control is hardly described by a continuous function and
this is evident from the complexity of the policy found by the GP, which reads:

a = (cid:0) log (s2 + s4)+ee(s4)(cid:17)

+

sin (cid:0) sin (cid:0) tanh (cid:0) log (cid:0)

e(s2
2−

(cid:0) tanh(sin (s1)

s2)

s2s4

(cid:1)(cid:1)(cid:1)(cid:1)

−
The best GP control strategy consists of two main terms. The ﬁrst depends on s2 and s4 and
the second takes all the states at the denominator and only s2 at the numerator. This allows to
moderate the control efforts once the ﬁrst oscillator is stabilized.

−

−

−

·

sin (cid:0) log(s2)(cid:1)
(cid:1)
s2
3)
s3

62.565.067.570.072.5t−0.2−0.10.00.10.20.3s1646668707274t−0.10−0.050.000.050.100.150.200.250.30s2LIPOBOGPDDPG30

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

103
·
Best
Reward

LIPO

BO

GP

DDPG

-7.26

0.93 -7.10

±

0.32 -12.06

±

12.25 -6.88

±

0.58

±

Table 3: Same as table 1 but for the control of nonlinear waves in the viscous Burger’s equation.

(a) Learning curve

(b) Learning curve variance

Figure 12: Comparison of the learning curves (a) and their variances (b) for different machine
learning methods for the 0D test case (Sec. 4.1).

5.2. Viscous Burgers’ equation test case

We now present the results of the viscous Burgers’ test case (cf Sec.4.2). As for the previous test
case, Table 3 collects the ﬁnal best cumulative reward for each control method together with the
conﬁdence interval, while ﬁgures 12a and 12b show the learning curve and the learning variance
over ten training sessions. The DDPG achieved the best performance, with low variance, whereas
the GP performed worse in both maximum reward and variance. LIPO and BO give comparable
results. For the LIPO, the learning variance initially, as the algorithm randomly selects the second
and third episodes’ weights.

For this test case, the GPr-based surrogate model of the reward function used by the BO proves
to be particularly successful in approximating the expected cumulative reward. This yields steep
improvements in the cumulative rewards from the ﬁrst iterations (recalling that the BO runs ten
exploratory iterations to build its ﬁrst surrogate model, which are not included in the learning
curve). On the other hand, the GP does not proﬁt from the relatively simple functional at hand
and exhibits the usual stair-like learning curve since 20 were run with an initial population of 10
individuals.

The control laws found by BO and LIPO have similar weights (with differences of the order
O(10−
2)), although the BO has much lower variance among the training sessions. Figure 13
shows the best control law derived by the four controller, together with the forcing term. These
ﬁgures should be analysed together with table 4 which shows the spatio-temporal evolution of the
variable u(x,t) under the action of the best control law derived by the four algorithms.

The linear control laws of BO and LIPO are characterized by two main periods, one that seeks

100101102ep104105-R(w)BOLIPOGPDDPG100101102ep103104105σRBOLIPODDPGGPComparative analysis of machine learning methods for active ﬂow control

31

Figure 13: Comparison of the control actions derived by the four machine learning methods. The
action for each control methods are shown in blue (left axis) while the curves in dashed red show
the evolution of the introduced perturbation (cf. (4.11)).

to cancel the incoming wave and the second that seek to compensate the upward propagation
of the control action, due to the viscous dissipation. This mechanism is clearly revealed in the
spatio-temporal plots in Figure 4 for the BO and LIPO while it is moderate in the problem
controlled via GP and absent in the case of the DDPG control. This mechanism challenges the
closed-loop approach by the simple linear law used by the LIPO and BO because actions are fed
back into the observations after a certain time. An open-loop strategy (for example introducing
a constant term in the policy) could signiﬁcantly ease the control task in this problem, but this
option is unavailable to the LIPO and BO’s linear policy used in this test. The same is true for the
GP, which in this simple implementation did not have ephemeral random constant allowing for
decoupling the action form the state by producing a constant action. Hinging on the nonlinearities,
the GP manages nevertheless to mitigate this effects through a fairly complex action (see Table
12b) which is periodic in line with the forcing. Nevertheless towards the end of the episode, the
periodicity is broken by the waves produced by the controller itself: the optimal solution analyzed

−0.2−0.10.00.1aLIPO−0.2−0.10.00.1aBO−0.2−0.10.00.1aGP02468101214t−0.2−0.10.00.1aDDPG−1.0−0.50.00.51.0f−1.0−0.50.00.51.0f−1.0−0.50.00.51.0f−1.0−0.50.00.51.0f32

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Table 4: Contour plot of the spatio-temporal evolution of u in (46) using the best control action of
the different methods. The perturbation is centred at x = 6.6 (red continuous line) while the control
law is centred at x = 13.2. The dashed black lines visualize the location of the observation points,
while the region within the white dash-dotted line is used to evaluate the controller performance

LIPO

BO

GP

DDPG

Best
Reward

6.53

0.34 6.43

±

1.22 7.14

±

0.86 5.58

±

2.62

±

Table 5: Same as table 1 but for the von Kármán street control problem.

in this case is strongly dependent on the duration of the episode and the difﬁculties in dealing with
the feedback of the action explains the poor performance of the GP.

The DDPG, hinging on non-linearities and the bias terms in the ANN, allows for breaking
this feedback mechanics by providing an open-loop contribution such as a constant action. This
strategies is particularly successful for this problem, as it allows avoiding upstream travelling (see
Table 4) and leveraging more on the viscous dissipation of the system.

Nevertheless, despite the large differences in the control method and the different performances,
all methods succeeds in cancelling the travelling waves. An animation of the system controlled by
the four methods is provided in the supplemental material.

5.3. von Kármán street control test case

We begin the analysis of this test case with an investigation on the performances of the RL
agent trained by Tang et al. (2020) using the Proximal Policy Optimization (PPO) on the same
control problem. As recalled in section 4.3, these authors used 236 probes, located as shown in
Figure 6, and a policy a = f (s; w) represented by an ANN with three layers with 256 neurons
each. Such a complex parametric function gives a large model capacity, and it is thus natural to
analyse whether the trained agent leverage this potential model complexity.

To this end, we perform a linear regression of the policy identiﬁed by the ANN. Given a

R4
R236 the state vector collecting information from all probes, we seek the

∈

the action vector and s

∈

Comparative analysis of machine learning methods for active ﬂow control

33

Figure 14: Scatter plot of the sensor locations, coloured by the norm of the weights
w1 j, w2 j, w3 j, w4 j that link the observation at state j with the action vector a = [a1, a2, a3, a4]
in the linear regression of the policy by Tang et al. (2020)

(a) Learning curve

(b) Learning curve variance

Figure 15: Comparison of the learning curves (a) and their variances (b) for different machine
learning methods for the von Kármán street control problem

236 the matrix of weights of the linear policy.
R4
best linear law of the form a = Ws, with W
×
Let w j denote the j-th raw of W, hence the set of weights that linearly map the state s to the
action a j, i.e. the ﬂow rate in the one of the fourth injections. One thus has a j = wT

∈

j s.

To perform the regression, we produce a dataset of n
∗

= 400 samples of the control law, by
interrogating the ANN agent trained by Tang et al. (2020). Denoting as s∗i the evolution of the
state i and as a∗j the vector of actions proposed by the agent the 400 samples, the linear ﬁt of the
control action is the solution of a linear least square problem, which using Ridge regression yields:

236 is the matrix collecting the 400 samples for the 236
where S = [s∗1, s∗2, . . . s∗236]
×
observations along its columns, I is the identity matrix of appropriate size and α is a regularization

∈

a∗j = Sw j
R400

→

w j = (ST S + αI)−

1ST a∗j

(5.2)

020406080100120140160180Averagedcoeﬃcientmagnitude100101102103ep1234567-R(w)BOLIPOGPDDPG100101102103ep100σRBOLIPODDPGGP34

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 16: Evolution of the jets’ ﬂow rates(left) and the drag around the cylinder(right) for the
best control action found by the different machine learning methods.

term. In this regression, the parameter α is taken running a K=5 fold validation and looking for
the minima of the out-of sample error.

||

= 1e

a∗j −
||

The second reason is that analysing the weights wi, j

The result of this exercise is illuminating for two reasons. The ﬁrst reason is that the residuals
Sw j
in the solution of (5.2) have a norm of
5. This means that despite the large
model capacity available to the ANN, the RL by Tang et al. (2020) is acting as a linear controller.
W, in the linearized policy a j = Ws,
allows for quickly identifying which of the sensors is more important in the action selection
process. The result, in the form of a coloured scatter-plot, is shown in Figure 14. The markers are
placed at the sensor location and coloured by the sum ∑i w2
i, j for each of the j-th sensors. This
result shows that only a small fraction of all the sensors play a role in the action selection. In
particular, the two most important ones are placed on the rear part of the cylinder and have much
larger weights than all the others.

−

∈

In the light of this result with the benchmark RL agent, it becomes particularly interesting to
perform the same analysis of the control action proposed by DDPG and GP, since BO and the
LIPO use a linear law by construction. Figure 15a and 15b show the learning curves and learning

−0.0050.0000.0050.010qLIPO−0.0050.0000.0050.010qBO−0.0050.0000.0050.010qGP0.06.513.019.526.032.539.045.552.058.565.071.578.084.591.0t−0.0050.0000.0050.010qDDPGq1q2q3q4Cd1.52.02.53.03.54.0Cd1.52.02.53.03.54.0Cd1.52.02.53.03.54.0Cd1.52.02.53.03.54.0CdComparative analysis of machine learning methods for active ﬂow control

35

DDPG

GP

Table 6: Comparison of the optimal actions of the DDPG and GP (x axis) with their linearized
version (y axis) for the four jets, the red line is the bisector of the ﬁrst and third quadrant.

variance as a function of the episodes, while table 5 collects the results for the four methods in
terms of the best reward and conﬁdence interval as done for the previous test cases.

The BO and the LIPO reached an average reward of 6.43 (with the best performances of the
BO hitting 7.07) in 80 episodes while the PPO agent trained by Tang et al. (2020) required 800 to
reach a reward of 6.21, although they mainly aimed at achieving a robust policy across a range of
Reynolds numbers. In a problem that can be well controlled by a linear policy using 5 sensors, the
use of an ANN-based policy leveraging 236 sensors increases the dimensionality of the problem
without beneﬁting from the available model capacity.

Genetic Programming had the best mean control performance with a remarkably small variance.
LIPO had the lowest standard deviation due to its mainly deterministic research strategy, which
selects only two random coefﬁcients at the second and the third optimization steps.

The large exploration by the GP, on the other hand, requires more than 300 episodes to attain
a better mean value than the other methods. LIPO and BO had similar trends, with an almost
constant rate of improvement. This suggests that the surrogate models used in the regression are
particularly effective in approximating the expected cumulative reward.

The DDPG follows a similar trend but slightly worse performances and larger variance. The
large model capacity of the ANN, combined with the initial exploratory phase, tend to set the
DDPG on a bad initial condition. However, this is not noticeable from the standard deviation
since it is fairly low for the DDPG, which infer that the learning in the different random seeds is
similar. Moreover, the DDPG presents a steep variance reduction around episode 20 due to the
explorative phase’s end and the exploitative start. The other methods have a lower variance which
also presents peaks and valleys, but they do not have any particular trend.

Despite the low dispersion overall variance in the reward, the BO and LIPO ﬁnds largely
different weights for the linear control functions, as shown in Fig.17. This implies that fairly
different strategies leads to comparable rewards. In general, the identiﬁed linear law seeks to
compensate the momentum deﬁcit due to the vortex shedding by injecting momentum with the
jets on the opposite side. For example, in the case of q4, the LIPO gave higher weight to the
observations s2, s3 and s5 which lay on the upper half-plane. The same occurs in terms of q3 with

−10a1−1.0−0.50.0a1l−101a2−1.0−0.50.00.51.0a2l−10a3−1.5−1.0−0.50.00.5a3l012a4−0.50.00.51.01.52.0a4l468a1×10−345678a1l×10−3−2.50.02.5a2×10−4−202a2l×10−4−6−303a3×10−3−6−4−2024a3l×10−3−6−3036a4×10−3−5.0−2.50.02.55.07.5a4l×10−336

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

Figure 17: Weights of control action for the von Kármán street control problem, given by a linear
combination of the system’s states for the four ﬂow rates. The coloured bars represent a standard
deviation around the mean value found by LIPO and BO with ten random number generator seeds.

s2 and s4. To suggests that the a vortex passage in the upper part triggers the activation of the jets
on the bottom part and the opposite is true in the other half-plane. Particularly interesting is the
fact that neither the average weights nor their variance is symmetric with respect to the half plane.
Figure 16 show the time evolution of the four actions (ﬂow rates) and (line red, the evolution
of the instantaneous drag coefﬁcient. Probably due to the short duration of the episode, none of
the controllers identiﬁes a symmetric control law. LIPO and BO, despite the different weights’
distribution, ﬁnd an almost identical linear combination. They both produce a small ﬂow rate
for the second jet and larger ﬂow rates for the ﬁrst, both in the initial transitory and in the ﬁnal
stages. As the shedding is reduced and the drag coefﬁcient drops, all ﬂow rates tends to a constant
injection for both BO and LIPO, while the GP keep continuous pulsations in both q4 and q3 (with
opposite signs). Once again, it should be remarked that these three controllers are not allowed to
act in an open loop, contrary to the DDPG.

All the control methods leads to satisfactory performances, with a mitigation of the von Kármán
street and a reduction of the drag coefﬁcient, also visible by the increased size of the recirculation
bubble in the wake. The mean ﬂow and standard deviation for the baseline and for the best strategy
identiﬁed by the four techniques is shown in Figure 7. It is worth highlighting that DDPG is the
only method without a symmetric distribution of the mean velocity ﬁeld behind the cylinder. The
vortex suppression is even more evident comparing the standard deviation of the mean velocity
ﬁeld with the baseline case.

Finally, we close this section by evaluating the degree of nonlinearity in the control laws derived
by the GP and the DDPG. As performed for the PPO agent by Tang et al. (2020) at the opening
of this section, we perform a linear regression with respect to the evolution of the states. The
results are shown in Table 6, which compares the action taken by the DDPG (ﬁrst row) and the GP
(second row), in the abscissa, with the linearized actions, in the ordinate, for the four injections.
None of the four injections produced by the DDPG agent can be linearized and the open-loop
behaviour (constant action regardless of the states) is visible. Interestingly, the action taken by the
GP on the fourth jet is almost linear.

w1w2w3w4w5Weights−1.00−0.75−0.50−0.250.000.250.500.751.00Valuesq1BOLIPOw6w7w8w9w10q2w11w12w13w14w15q3w16w17w18w19w20q4Comparative analysis of machine learning methods for active ﬂow control

37

Mean Value

Standard Deviation

Baseline

LIPO

BO

GP

DDPG

Table 7: Final pressure and velocity magnitude ﬁelds using the best control action found by the
different methods.

6. Conclusions and outlooks

We presented a general mathematical framework linking machine learning-based control
techniques and optimal control. The ﬁrst category comprises methods based on ’black-box
optimization’ such as Bayesian Optimization (BO) and Lipschitz Global Optimization (LIPO),
methods based on tree expression programming such as Genetic Programming (GP), and methods
from reinforcement learning such as the Deep Deterministic Policy Gradient (DDPG).

We introduced the mathematical background for each method, in addition we illustrated
their algorithmic implementation. Following the deﬁnition by Mitchell (1997), these algorithms
automatically improve at a task (controlling a system) according to a performance measure (a
reward function) with experience (i.e. data, collected via trial and errors from the environment).
In its most classic formulation, the solution to a control problem is black-box optimization. The
function to optimize measures the controller performance over a set of iterations that we call
episodes. Therefore, training a controller algorithm requires (1) a function approximation to
express the ‘policy’ or ‘actuation law’ linking the current state of the system to the action to take
and (2) an optimizer that improves the function approximation episode after episode.

In the Bayesian Optimization and LIPO, the function approximator for the policy is deﬁned as
a priori. In this work, we consider linear or quadratic controllers, but any function approximator
could have been used instead (e.g. RBF or ANN). These optimizers build a surrogate model of
the performance measure and adapt this model episode by episode. In Genetic Programming,
the function approximator is an expression tree, and the optimization is carried out using classic
evolutionary algorithms. In Reinforcement Learning (RL), particularly in the DDPG algorithm

38

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

implemented in this work, the function approximation is an ANN, and the optimizer is a stochastic
(batch) gradient-based optimization. In this optimization, the gradient of the cumulative reward is
computed using a surrogate model of the Q-function, i.e. the function mapping the value of each
state-action pair, using a second ANN.

In the machine learning terminology, we say that the function approximators available to the
GP and the DDPG have a larger ‘model capacity’ than those we used for the BO and the LIPO
(linear or quadratics). This allows these algorithms to identify nonlinear control laws that are
difﬁcult to cast in the form of a prescribed parametric function. On the other hand, the larger
capacity requires many learning parameters (branches and leaves in the tree expressions of the GP
and weights in the ANN of the DDPG), leading to optimization problems linked to performance
improvement and possible local minima. This study shows that the additional complexity does
not always pay off and is sometimes unnecessary if not harmful.

We compared the ‘learning’ performances of these four algorithms on three control problems
of growing complexity and dimensionality: (1) the stabilization of a nonlinear 0D oscillator, (2)
the cancellation of nonlinear waves in the burgers’ equation in 1D, and (3) the drag reduction in
the ﬂow past a cylinder in laminar conditions. The successful control of these systems highlighted
the strengths and weaknesses of each method, although, all algorithms identify valuable control
laws in the three systems.

The GP achieve the best performances on both the stabilization of the 0D system and the control
of the cylinder wake, while the DDPG gives the best performances on the control of nonlinear
waves in the Burgers’ equation. However, the GP has the poorest sample efﬁciency in all the
investigated problems, thus requiring a larger number of interactions with the system, and has the
highest learning variance, meaning that repeating the training leads to vastly different results. This
behaviour is inherent to the population-based and evolutionary optimization algorithm, which has
the main merit of escaping local minima in problems characterized by complex functionals. These
features paid off in the 0D problem, for which the GP derives an effective impulsive policy, but
are ineffective in the control of nonlinear waves in the Burgers’ equation, characterized by a much
simpler reward functional.

On the other side of the spectra in terms of sample efﬁciency are the black box optimizers such
as LIPO and BO. Their performance is strictly dependent on the effectiveness of the predetermined
policy parametrization to optimize. In the case of the 0D control problem, the quadratic policy
is, in its simplicity, less effective than the complex policy derived by GP and DDPG. However,
in the nonlinear wave cancellation problem, the only limit was the lack of a constant (open-
loop) term. For the problem of drag reduction in the cylinder ﬂow, the linear policy was rather
satisfactory. To the point that it was shown that the PPO policy by Tang et al. (2020) has, in fact,
derived a linear policy. The DDPG implemek was trained using 5 sensors (instead of 236) and
reached a performance comparable to the PPO by Tang et al. (2020) in 80 episodes (instead of
800). Nevertheless, although the policy derived by the DDPG is nonlinear, its performances are
comparable to the linear laws derived by BO and LIPO. Yet, the policy by the DDPG is based on
an ANN parametrized by 68361 parameters (4 fully connected layers with 5 neurons in the ﬁrst,
256 in the second and third and 4 in the output) while the linear laws used by BO and LIPO only
depend on 5 parameters.

We believe that this work has shed some light (or open some paths) on two main aspects of the
machine-learning-based control problem: (1) the contrast between the generality of the function
approximator for the policy and the number of episodes required to obtain good control actions;
(2) the need for tailoring the model complexity to control task at hand and the possibility of having
a modular approach in the construction of the optimal control law. The resolution of both aspects
resides in the hybridization of the investigated methods.

Concerning the choice of the function approximator (or more generally the policy parametriza-
tion, or the ’hypothesis set’ in the machine learning terminology), both ANN and expression

Comparative analysis of machine learning methods for active ﬂow control

39

offer large modelling capacities, with the seconds often outperforming the ﬁrst in the author’s
experience. Intermediate solutions such as RBFs or Gaussian processes can provide a valid
compromise between model capacity and dimensionality of their parameter space. They should
be explored more in the ﬁeld of ﬂow control.

Finally, concerning the dilemma model complexity versus task complexity, a possible solution
could be increasing the complexity modularly. For example, one could limit the function space in
the GP by ﬁrst taking linear functions and then enlarging it modularly, adding more primitives.
Or, in a hybrid formalism, one could ﬁrst train a linear or polynomial controller (e.g. via LIPO or
BO) and then use it to pre-train models of larger complexity (e.g. ANNs or expression trees) in a
supervised fashion, or to assist their training with the environment (for instance by inﬂating the
replay buffer of the DDPG with transitions learned by the BO/LIPO models).

This is the essence of ’behavioural cloning’, in which a ﬁrst agent (called ’demonstrator’) trains
a second one (called ’imitator’) ofﬂine so that the second does not start from scratch. This is
unexplored territory in ﬂow control and, of course, opens the question of how much the supervised
training phase should last and whether the pupil could ever surpass the master.

Appendix A. From Hamilton-Jocobi-Bellman to the adjoint differential equation

Starting from the HJB equation in (2.10):

(cid:110)

∂tV ∗(s,t) + max
A

a
∈

L (s, a,t) + ∂sV (s,t)f(s, a,t)

= 0 ,

(A 1)

(cid:111)

assume a∗(t) is the optimal control function. This leads to:

∂tV ∗(s,t) + L (s, a∗,t) + ∂sV (s,t)f(s, a∗,t) = 0 .

Differentiating both sides with respect to the state vector s:

∂s∂tV ∗(s,t) + ∂sL (s, a∗,t) + ∂s

(cid:105)
(cid:104)
∂sV (s,t)f(s, a∗,t)

= 0,

and using the product rule, yields:

∂tsV ∗(s,t) + ∂sL (s, a∗,t) + ∂s2V (s,t)f(s, a∗,t)+
+∂sV (s,t)∂sf(s, a∗,t) = 0 .

Rearranging terms:

∂tsV ∗(s,t) + ∂s2V (s,t)f(s, a∗,t) =

=

∂sL (s, a∗,t)

−

−

∂sV (s,t)∂sf(s, a∗,t),

(cid:105)
(cid:104)
∂sV (s,t)

∂t

=∂tsV ∗(s,t) + ∂s2V (s,t)˙s

=∂tsV ∗(s,t) + ∂s2V (s,t)f(s, a∗,t) .

where:

Then:

(A 2)

(A 3)

(A 4)

(A 5)

(A 6)

(A 7)

(A 8)

(cid:105)
(cid:104)
∂sV (s,t)

∂t

=

∂sL (s, a∗,t)

−

−

∂sV (s,t)∂sf(s, a∗,t) .

(A 9)

Noticing that this is true for any s that solves the HJB equation, it must also be true for the

optimal trajectory s∗ associated with the optimal control action a∗

40

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

(cid:105)
(cid:104)
∂sV (s∗,t)

∂t

=

∂sL (s∗, a∗,t)

−

−

∂sV (s∗,t)∂sf(s∗, a∗,t),

(A 10)

which yields the classical result of the max principle presented in Eq.(2.6)

∂t λ (t) =

∂sL (s∗, a∗,t)

−

−

∂sV (s∗,t)
(cid:123)(cid:122)
(cid:125)
(cid:124)
λ (t)

∂sf(s∗, a∗,t),

(A 11)

somewhat magically the λ (t) no longer depends on the states s as it did in the HJB, so the

right-hand side is in fact the partial derivative of the Hamiltonian with respect to s.

Appendix B. Weights identiﬁed by the BO and LIPO

The tables below collects the weights for the linear and nonlinear policies identiﬁed by LIPO
and BO for the three investigated control problems. The reported value represents the mean of
ten optimization with different random conditions and the uncertainty is taken as the standard
deviation.

Comparative analysis of machine learning methods for active ﬂow control

41

0
1
w

9
w

8
w

7
w

6
w

5
w

4
w

3
w

2
w

1
w

7
5
1

.

4
0
1
±

.

2
5

.

1
-

1
0

.

±

9
3
2

.

6
6
1
±

.

3
2
0

.

2
0

.

±

2
1
2

.

4
2
.
0
-

4
1
±
.
2

±

0

6
6
.
2

5
5
.
1
±

3
0
.
0
-

7
.
0

±

5
2
.
2

1
6
.
1
±

5
.
0

0

±

7
4
.
2

6
.
0

6
3
.
1
±
4
.
0
-

±

9
3
.
0

7
7
.
±
0

±

2

3
7
.
2

7
6
.
1

6
2
.
±
1

±

1

4
9
.
1

8
2
.
2

6
2
.
0

8
5
.
1
±
1
.
0
-

±

3
2
.
2

3
8
.
1
±

3
1
.
1

2
.
0

±

0
2
w

9
1
w

8
1
w

7
1
w

6
1
w

5
1
w

4
1
w

3
1
w

2
1
w

1
1
w

2
7
2

.

9
1
2
±

.

7
5

.

0
-

3
0

.

±

8
9
1

.

3
7
1
±

.

7
1

.

0
-

3

.

0
-

±

6
2
2

.

5
2
.
1
±

5
0
.
1

8
.
1

±

6
5
.
2

2
1
.
0
-

5
.
1
±
5
.
0

±

6
2
.
2

9
1
.
0

8
.
0
±
6
.
0
-

±

4
1
.
2

5
4
.
1
±

8
5
.
0
-

1
.
0
-

±

5
2
.
2

9
6
.
1
±

7
2
.
0

4
.
0

±

6
.
1

5
7
.
±
0

2
3
.
0
-

8
.
0
-

±

3
4
.
2

6
2
.
±
1

7
.
0
-

0

±

3
.
2

2
7
.
±
1

8
1
.
0
-

2
.
0

±

0
1
w

9
w

8
w

7
w

6
w

5
w

4
w

3
w

2
w

1
w

s
t
n
e
i
c
ﬁ
f
e
o
c
w
a
l

l
o
r
t
n
o
c

k
c
a
b
d
e
e
f

’
s
r
e
g
r
u
B
e
h
t

f
o

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

f
l
a
h

d
n
a

e
u
l
a
v
n
a
e

M

:
9
e
l
b
a
T

3
w

2
w

1
w

s
t
n
e
i
c
ﬁ
f
e
o
c
w
a
l

l
o
r
t
n
o
c

k
c
a
b
d
e
e
f

D
0

e
h
t

f
o

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

f
l
a
h

d
n
a

e
u
l
a
v
n
a
e

M

:
8
e
l
b
a
T

)
2
0
.
0

)
0
0
.
0
±

±

(
3
0
.
0
-

(
3
0
.
0
-

)
3
0
.
0

)
1
0
.
0
±

±

(
3
0
.
0

(
2
0
.
0

)
1
0
.
0

)
0
0
.
0
±

±

(
2
0
.
0
-

(
2
0
.
0
-

O
P
I
L

O
B

0
7
0

.

.

9
6
0
±

1
2

.

0
-

6
4
0

.

±

7
4
0

.

9
5
0
±

.

4
6

.

0
-

7
0

.

±

6
7
0

.

7
4
.
0
-

.

6
5
0
±
3
6
.
0
-

±

1
7
.
0

8
3
.
0
-

3
8
.
0
±
1
1
.
0

±

0
8
.
0

0
3
.
0

9
8
.
0
±
6
3
.
0

±

4
8
.
0

3
2
.
0

0
6
.
0
±
6
6
.
0

±

2
6
.
0

0
4
.
0

5
5
.
0
±
7
6
.
0

±

4
7
.
0

2
1
.
0
-

2
7
.
0
±
7
3
.
0
-

±

7
6
.
0

8
4
.
0
-

2
3
.
0
±
3
7
.
0
-

±

9
6
.
0

9
2
.
0
-

0
4
.
0
±
7
6
.
0
-

±

0
2
w

9
1
w

8
1
w

7
1
w

6
1
w

5
1
w

4
1
w

3
1
w

2
1
w

1
1
w

0
0

.

0
1

.

.

0
9
0
±
1
4
0

.

±

2
3
0

.

6
7
0

.

.

6
7
0
±
1
1
0

.

±

9
3
0

.

3
5
.
0
-

.

7
7
0
±
5
4
.
0
-

±

±

4
0
.
0

9
9
.
0
-

4
8
.
0
±
2
4
.
0
-

5
6
.
0

2
2
.
0
-

2
6
.
0
±
6
4
.
0
-

±

0
8
.
0

1
1
.
0
-

5
8
.
0
±
6
2
.
0
-

±

5
5
.
0

7
4
.
0

4
6
.
0
±
3
5
.
0

±

2
7
.
0

5
1
.
0
-

5
8
.
0
±
3
2
.
0
-

±

±

2
.
0

9
8
.
0

7
.
0
±
4
5
.
0

7
6
.
0

8
3
.
0

2
6
.
0
±
6
6
.
0

±

s
t
n
e
i
c
ﬁ
f
e
o
c
w
a
l

l
o
r
t
n
o
c

k
c
a
b
d
e
e
f

t
e
e
r
t
s

x
e
t
r
o
v

n
a
m
r
a
K
n
o
v

e
h
t

f
o

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

f
l
a
h
d
n
a

e
u
l
a
v

n
a
e

M

:
0
1
e
l
b
a
T

O
P
I
L

O
B

O
P
I
L

O
B

O
P
I
L

O
B

O
P
I
L

O
B

42

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

R E F E R E N C E S

ABU-MOSTAFA, YASER S., MAGDON-ISMAIL, MALIK & LIN, HSUAN-TIEN 2012 Learning from Data.

AMLBook.

AHMED, MOHAMED OSAMA, VASWANI, SHARAN & SCHMIDT, MARK 2020 Combining bayesian

optimization and lipschitz optimization. Machine Learning 109 (1), 79–102.

ALEKSIC, KATARINA, LUCHTENBURG, MARK, KING, RUDIBERT, NOACK, BERND & PFEIFER, JENS
2010 Robust nonlinear control versus linear model predictive control of a bluff body wake. In 5th
Flow Control Conference. American Institute of Aeronautics and Astronautics.

ALNÆS, MARTIN, BLECHTA, JAN, HAKE, JOHAN, JOHANSSON, AUGUST, KEHLET, BENJAMIN, LOGG,
ANDERS, RICHARDSON, CHRIS, RING, JOHANNES, ROGNES, MARIE E & WELLS, GARTH N
2015 The fenics project version 1.5. Archive of Numerical Software 3 (100).

APATA, O & OYEDOKUN, DTO 2020 An overview of control techniques for wind turbine systems. Scientiﬁc

African p. e00566.

ARCHETTI, FRANCESCO & CANDELIERI, ANTONIO 2019 Bayesian optimization and data science.

Springer.

BACK, FOGEL & MICHALEWICZ 2000 Evolutionary Computation 1 : Basic Algorithms and Operators.
BALABANE, MIKHAEL, MENDEZ, MIGUEL ALFONSO & NAJEM, SARA 2021 Koopman operator for

burgers's equation. Physical Review Fluids 6 (6).

BANZHAF, WOLFGANG, NORDIN, PETER & KELLER, ROBERT E. 1997 Genetic Programming: An

Introduction. MORGAN KAUFMANN PUBL INC.

BAZARAA, MOKHTAR S., SHERALI, HANIF D. & SHETTY, C. M. 2005 Nonlinear Programming. John

Wiley & Sons, Inc.

BEINTEMA, GERBEN, CORBETTA, ALESSANDRO, BIFERALE, LUCA & TOSCHI, FEDERICO 2020
Controlling rayleigh–bénard convection via reinforcement learning. Journal of Turbulence 21 (9-10),
585–605.

BELUS, VINCENT, RABAULT, JEAN, VIQUERAT, JONATHAN, CHE, ZHIZHAO, HACHEM, ELIE &
REGLADE, ULYSSE 2019 Exploiting locality and physical invariants to design effective deep
reinforcement learning control of the unstable falling liquid ﬁlm. arXiv preprint arXiv:1910.07788 .
BENARD, N., PONS-PRATS, J., PERIAUX, J., BUGEDA, G., BRAUD, P., BONNET, J. P. & MOREAU, E.
2016 Turbulent separated shear ﬂow control by surface plasma actuator: experimental optimization
by genetic algorithm approach. Experiments in Fluids 57 (2).

BERGMANN, MICHEL, CORDIER, LAURENT & BRANCHER, JEAN-PIERRE 2005 Optimal rotary control of
the cylinder wake using proper orthogonal decomposition reduced-order model. Physics of Fluids
17 (9), 097101.

BERGSTRA, J., YAMINS, DANIEL & COX, D. 2013 Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures. In ICML.

BERSINI, H. & GORRINI, V. 1996 Three connectionist implementations of dynamic programming for

optimal control: a preliminary comparative analysis. IEEE Comput. Soc. Press.

BEWLEY, THOMAS R 2001 Flow control: new challenges for a new renaissance. Progress in Aerospace

sciences 37 (1), 21–58.

BEWLEY, THOMAS R., MOIN, PARVIZ & TEMAM, ROGER 2001 DNS-based predictive control of
turbulence: an optimal benchmark for feedback algorithms. Journal of Fluid Mechanics 447, 179–225.
BEYER, HANS-GEORG & SCHWEFEL, HANS-PAUL 2002 Evolution strategies - a comprehensive

introduction. Natural Computing 1, 3–52.

BIRAL, FRANCESCO, BERTOLAZZI, ENRICO & BOSETTI, PAOLO 2016 Notes on numerical methods for

solving optimal control problems. IEEJ Journal of Industry Applications 5 (2), 154–166.

BLANCHARD, ANTOINE B, CORNEJO MACEDA, GUY Y, FAN, DEWEI, LI, YIQING, ZHOU, YU, NOACK,
BERND R & SAPSIS, THEMISTOKLIS P 2022 Bayesian optimization for active ﬂow control. Acta
Mechanica Sinica pp. 1–13.

BÖHME, THOMAS J & FRANK, BENJAMIN 2017 Direct methods for optimal control. In Hybrid Systems,

Optimal Control and Hybrid Vehicles, pp. 233–273. Springer.

BRUNTON, S. 2009 Methods for System Identiﬁcation. AIAA.
BRUNTON, S. 2022 Methods for System Identiﬁcation. Cambridge University Press.
BRUNTON, STEVEN L. & NOACK, BERND R. 2015 Closed-loop turbulence control: Progress and challenges.

Applied Mechanics Reviews 67 (5).

Comparative analysis of machine learning methods for active ﬂow control

43

BRUNTON, STEVEN L, NOACK, BERND R & KOUMOUTSAKOS, PETROS 2020 Machine learning for ﬂuid

mechanics. Annual Review of Fluid Mechanics 52, 477–508.

BUCCI, MICHELE ALESSANDRO, SEMERARO, ONOFRIO, ALLAUZEN, ALEXANDRE, WISNIEWSKI,
GUILLAUME, CORDIER, LAURENT & MATHELIN, LIONEL 2019 Control of chaotic systems by
deep reinforcement learning. Proceedings of the Royal Society A 475 (2231), 20190351.

CARNARIUS, ANGELO, THIELE, FRANK, OEZKAYA, EMRE & GAUGER, NICOLAS R 2010 Adjoint

approaches for optimal ﬂow control. In 5th Flow Control Conference, p. 5088.

COLLIS, S.S., GHAYOUR, K. & HEINKENSCHLOSS, M. 2002 Optimal control of aeroacoustic noise
generated by cylinder vortex interaction. International Journal of Aeroacoustics 1 (2), 97–114.
DAVIDSON, KENNETH R & DONSIG, ALLAN P 2009 Real analysis and applications: theory in practice.

Springer Science & Business Media, pg. 70.

DEBIEN, ANTOINE, VON KRBEK, KAI A. F. F., MAZELLIER, NICOLAS, DURIEZ, THOMAS, CORDIER,
LAURENT, NOACK, BERND R., ABEL, MARKUS W. & KOURTA, AZEDDINE 2016 Closed-loop
separation control over a sharp edge ramp using genetic programming. Experiments in Fluids 57 (3).
DELPORT, S., BAELMANS, M. & MEYERS, J. 2011 Maximizing dissipation in a turbulent shear ﬂow by

optimal control of its initial state. Physics of Fluids 23 (4), 045105.

DIRK, M. LUCHTENBURG, GÜNTHER, BERT, NOACK, BERND R., KING, RUDIBERT & TADMOR,
GILEAD 2009 A generalized mean-ﬁeld model of the natural and high-frequency actuated ﬂow around
a high-lift conﬁguration. Journal of Fluid Mechanics 623, 283–316.

DURIEZ, THOMAS, BRUNTON, STEVEN L & NOACK, BERND R 2017 Machine learning control-taming

nonlinear dynamics and turbulence. Springer.

EVANS, LAWRENCE C. 1983 An introduction to mathematical optimal control theory, lecture notes.
FAN, DIXIA, YANG, LIU, WANG, ZHICHENG, TRIANTAFYLLOU, MICHAEL S. & KARNIADAKIS,
GEORGE EM 2020 Reinforcement learning for bluff body active ﬂow control in experiments and
simulations. Proceedings of the National Academy of Sciences 117 (42), 26091–26098.

FASSHAUER, GREGORY E 2007 Meshfree Approximation Methods with Matlab. WORLD SCIENTIFIC.
FLEMING, PETER J & FONSECA, CARLOS M 1993 Genetic algorithms in control systems engineering.

IFAC Proceedings Volumes 26 (2), 605–612.

FLINOIS, THIBAULT L. B. & COLONIUS, TIM 2015 Optimal control of circular cylinder wakes using long

control horizons. Physics of Fluids 27 (8), 087105.

FORRESTER, ALEXANDER I. J., SÓBESTER, ANDRÁS & KEANE, ANDY J. 2008 Engineering Design via

Surrogate Modelling. Wiley.

FORTIN, FÉLIX-ANTOINE, DE RAINVILLE, FRANÇOIS-MICHEL, GARDNER, MARC-ANDRÉ, PARIZEAU,
MARC & GAGNÉ, CHRISTIAN 2012 DEAP: Evolutionary algorithms made easy. Journal of Machine
Learning Research 13, 2171–2175.

FRAZIER, PETER I. 2018 A tutorial on bayesian optimization , arXiv: http://arxiv.org/abs/1807.02811v1.
FUJIMOTO, SCOTT, VAN HOOF, HERKE & MEGER, DAVID 2018 Addressing function approximation error

in actor-critic methods , arXiv: http://arxiv.org/abs/1802.09477v3.

GARNIER, PAUL, VIQUERAT, JONATHAN, RABAULT, JEAN, LARCHER, AURÉLIEN, KUHNLE,
ALEXANDER & HACHEM, ELIE 2021 A review on deep reinforcement learning for ﬂuid mechanics.
Computers & Fluids 225, 104973.

GAUTIER, N., AIDER, J.-L., DURIEZ, T., NOACK, B. R., SEGOND, M. & ABEL, M. 2015 Closed-loop

separation control using machine learning. Journal of Fluid Mechanics 770, 442–457.

GAZZOLA, MATTIA, HEJAZIALHOSSEINI, BABAK & KOUMOUTSAKOS, PETROS 2014 Reinforcement
learning and wavelet adapted vortex methods for simulations of self-propelled swimmers. SIAM
Journal on Scientiﬁc Computing 36 (3), B622–B639.

GHAVAMZADEH, MOHAMMED, MANNOR, SHIE, PINEAU, JOELLE & TAMAR, AVIV 2015 Convex
optimization: Algorithms and complexity. Foundations and Trends® in Machine Learning 8 (5-6),
359–483.

GOODFELLOW, IAN, BENGIO, YOSHUA & COURVILLE, AARON 2016 Deep Learning. the MIT Press.
GUNZBURGER, MAX D. 2002 Perspectives in Flow Control and Optimization. Society for Industrial and

Applied Mathematics.

EL HAK, MOHAMED GAD 2000 Flow Control. Cambridge University Press.
HAUPT, RANDY L & ELLEN HAUPT, SUE 2004 Practical genetic algorithms .
HEAD, TIM, KUMAR, MANOJ, NAHRSTAEDT, HOLGER, LOUPPE, GILLES & SHCHERBATYI, IAROSLAV

2020 scikit-optimize/scikit-optimize.

44

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

JIN, BO, ILLINGWORTH, SIMON J. & SANDBERG, RICHARD D. 2020 Feedback control of vortex shedding

using a resolvent-based modelling approach. Journal of Fluid Mechanics 897.

JONES, DONALD R., SCHONLAU, MATTHIAS & WELCH, WILLIAM J. 1998 Journal of Global

Optimization 13 (4), 455–492.

KELLEY, HENRY J 1960 Gradient theory of optimal ﬂight paths. Ars Journal 30 (10), 947–954.
KIM, JOHN & BEWLEY, THOMAS R. 2007 A linear systems approach to ﬂow control. Annual Review of

Fluid Mechanics 39 (1), 383–417.

KIM, JEONGLAE, BODONY, DANIEL J. & FREUND, JONATHAN B. 2014 Adjoint-based control of loud

events in a turbulent jet. Journal of Fluid Mechanics 741, 28–59.

KING, DAVIS E. 2009 Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research 10,

1755–1758.

KIRK, DONALD E 2004 Optimal control theory: an introduction. Courier Corporation.
KOBER, JENS & PETERS, JAN 2014 Reinforcement learning in robotics: A survey. In Springer Tracts in

Advanced Robotics, pp. 9–67. Springer International Publishing.

KOZA, JOHNR. 1994 Genetic programming as a means for programming computers by natural selection.

Statistics and Computing 4 (2).

KRAFT, DIETER 1985 On converting optimal control problems into nonlinear programming problems. In

Computational Mathematical Programming, pp. 261–280. Springer Berlin Heidelberg.

LANG, WALTER, POINSOT, THIERRY & CANDEL, SEBASTIEN 1987 Active control of combustion

instability. Combustion and Flame 70 (3), 281–289.

LEE, CHANGHOON, KIM, JOHN, BABCOCK, DAVID & GOODMAN, RODNEY 1997 Application of neural

networks to turbulence control for drag reduction. Physics of Fluids 9 (6), 1740–1747.

LI, JICHAO & ZHANG, MENGQI 2021 Reinforcement-learning-based control of conﬁned cylinder wakes

with stability analyses. Journal of Fluid Mechanics 932.

LI, RUIYING, NOACK, BERND R., CORDIER, LAURENT, BORÉE, JACQUES & HARAMBAT, FABIEN 2017
Drag reduction of a car model by linear genetic programming control. Experiments in Fluids 58 (8).
LI, YIQING, CUI, WENSHI, JIA, QING, LI, QILIANG, YANG, ZHIGANG, MORZY ´NSKI, MAREK & NOACK,
BERND R. 2019 Explorative gradient method for active drag reduction of the ﬂuidic pinball and
slanted ahmed body , arXiv: http://arxiv.org/abs/1905.12036v2.

LILLICRAP, TIMOTHY P., HUNT, JONATHAN J., PRITZEL, ALEXANDER, HEESS, NICOLAS, EREZ,
TOM, TASSA, YUVAL, SILVER, DAVID & WIERSTRA, DAAN 2015 Continuous control with deep
reinforcement learning , arXiv: http://arxiv.org/abs/1509.02971v6.

LIN, JOHN C 2002 Review of research on low-proﬁle vortex generators to control boundary-layer separation.

Progress in Aerospace Sciences 38 (4-5), 389–420.

LONGUSKI, JAMES M, GUZMÁN, JOSÉ J. & PRUSSING, JOHN E. 2014 Optimal Control with Aerospace

Applications. Springer New York.

LUKETINA, JELENA, NARDELLI, NANTAS, FARQUHAR, GREGORY, FOERSTER, JAKOB, ANDREAS,
JACOB, GREFENSTETTE, EDWARD, WHITESON, SHIMON & ROCKTÄSCHEL, TIM 2019 A survey
of reinforcement learning informed by natural language , arXiv: http://arxiv.org/abs/1906.03926v1.
MACEDA, GUY Y. CORNEJO, LI, YIQING, LUSSEYRAN, FRANÇOIS, MORZY ´NSKI, MAREK & NOACK,
BERND R. 2021 Stabilization of the ﬂuidic pinball with gradient-enriched machine learning control.
Journal of Fluid Mechanics 917.

MACEDA, GUY YOSLAN CORNEJO, NOACK, BERND R, LUSSEYRAN, FRANÇOIS, MORZYNSKI, MAREK,
PASTUR, LUC & DENG, NAN 2018 Taming the ﬂuidic pinball with artiﬁcial intelligence control. In
European Fluid Mechanics Conference.

MAHFOZE, OA, MOODY, A, WYNN, A, WHALLEY, RD & LAIZET, S 2019 Reducing the skin-friction
drag of a turbulent boundary-layer ﬂow with low-amplitude wall-normal blowing within a bayesian
optimization framework. Physical Review Fluids 4 (9), 094601.

MALHERBE, CÉDRIC & VAYATIS, NICOLAS 2017 Global optimization of lipschitz functions. In

International Conference on Machine Learning, pp. 2314–2323. PMLR.

MELIGA, PHILIPPE, SIPP, DENIS & CHOMAZ, JEAN-MARC 2010 Open-loop control of compressible

afterbody ﬂows using adjoint methods. Physics of Fluids 22 (5), 054109.

MENDEZ, FRANCISCO JOSÉ, PASCULLI, ANTONIO, MENDEZ, MIGUEL ALFONSO & SCIARRA, NICOLA
2021 Calibration of a hypoplastic model using genetic algorithms. Acta Geotechnica 16 (7), 2031–
2047.

MITCHELL, TOM 1997 Machine Learning. New York: McGraw-Hill.
MNIH, VOLODYMYR, KAVUKCUOGLU, KORAY, SILVER, DAVID, GRAVES, ALEX, ANTONOGLOU,

Comparative analysis of machine learning methods for active ﬂow control

45

IOANNIS, WIERSTRA, DAAN & RIEDMILLER, MARTIN 2013 Playing atari with deep reinforcement
learning , arXiv: http://arxiv.org/abs/1312.5602v1.

MNIH, VOLODYMYR, KAVUKCUOGLU, KORAY, SILVER, DAVID, RUSU, ANDREI A., VENESS, JOEL,
BELLEMARE, MARC G., GRAVES, ALEX, RIEDMILLER, MARTIN, FIDJELAND, ANDREAS K.,
OSTROVSKI, GEORG, PETERSEN, STIG, BEATTIE, CHARLES, SADIK, AMIR, ANTONOGLOU,
IOANNIS, KING, HELEN, KUMARAN, DHARSHAN, WIERSTRA, DAAN, LEGG, SHANE &
HASSABIS, DEMIS 2015 Human-level control through deep reinforcement learning. Nature
518 (7540), 529–533.

MUNTERS, WIM & MEYERS, JOHAN 2018 Dynamic strategies for yaw and induction control of wind farms

based on large-eddy simulation and optimization. Energies 11 (1), 177.

NIAN, RUI, LIU, JINFENG & HUANG, BIAO 2020 A review on reinforcement learning: Introduction and

applications in industrial process control 139, 106886.

NITA, C., VANDEWALLE, S. & MEYERS, J. 2016 On the efﬁciency of gradient based optimization
algorithms for DNS-based optimal control in a turbulent channel ﬂow. Computers & Fluids 125,
11–24.

NOACK, BERND R. 2019 Closed-loop turbulence control-from human to machine learning (and retour). In
Proceedings of the 4th Symposium on Fluid Structure-Sound Interactions and Control (FSSIC) (ed.
Zhou, Y., Kimura, M., Peng, G. Lucey, A.D., Huang & L.), pp. 23–32. Springer.

NOACK, BERND R., AFANASIEV, KONSTANTIN, MORZY ´NSKI, MAREK, TADMOR, GILEAD & THIELE,
FRANK 2003 A hierarchy of low-dimensional models for the transient and post-transient cylinder
wake. Journal of Fluid Mechanics 497, 335–363.

NOACK, B. R., CORNEJO MACEDA, G.Y. & LUSSEYRAN, F. 2022 Machine Learning for Turbulence

Control. Cambridge University Press.

NOVATI, GUIDO & KOUMOUTSAKOS, PETROS 2019 Remember and forget for experience replay. In

Proceedings of the 36th International Conference on Machine Learning.

NOVATI, GUIDO, MAHADEVAN, L. & KOUMOUTSAKOS, PETROS 2019 Controlled gliding and perching

through deep-reinforcement-learning. Phys. Rev. Fluids 4 (9).

NOVATI, GUIDO, VERMA, SIDDHARTHA, ALEXEEV, DMITRY, ROSSINELLI, DIEGO, VAN REES, WIM M
& KOUMOUTSAKOS, PETROS 2017 Synchronisation through learning for two self-propelled
swimmers. Bioinspir. Biomim. 12 (3), 036001.

PAGE, JACOB & KERSWELL, RICH R. 2018 Koopman analysis of burgers equation. Physical Review Fluids

3 (7).

PARK, D. S., LADD, D. M. & HENDRICKS, E. W. 1994 Feedback control of von kármán vortex shedding

behind a circular cylinder at low reynolds numbers. Physics of Fluids 6 (7), 2390–2405.

PASTOOR, MARK, HENNING, LARS, NOACK, BERND R., KING, RUDIBERT & TADMOR, GILED 2008
Feedback shear layer control for bluff body drag reduction. Journal of Fluid Mechanics 608, 161–196.
PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V., THIRION, B., GRISEL, O., BLONDEL,
M., PRETTENHOFER, P., WEISS, R., DUBOURG, V., VANDERPLAS, J., PASSOS, A., COURNAPEAU,
D., BRUCHER, M., PERROT, M. & DUCHESNAY, E. 2011 Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research 12, 2825–2830.

PIVOT, CHARLES, CORDIER, LAURENT & MATHELIN, LIONEL 2017 A continuous reinforcement learning
strategy for closed-loop control in ﬂuid dynamics. In 35th AIAA Applied Aerodynamics Conference.
American Institute of Aeronautics and Astronautics.

POWELL, MICHAEL JD 2006 The newuoa software for unconstrained optimization without derivatives. In

Large-scale nonlinear optimization, pp. 255–297. Springer.

PROCTOR, JOSHUA L., BRUNTON, STEVEN L. & KUTZ, J. NATHAN 2016 Dynamic mode decomposition

with control. SIAM Journal on Applied Dynamical Systems 15 (1), 142–161.

RABAULT, JEAN, KUCHTA, MIROSLAV, JENSEN, ATLE, RÉGLADE, ULYSSE & CERARDI, NICOLAS 2019
Artiﬁcial neural networks trained through deep reinforcement learning discover control strategies for
active ﬂow control. Journal of ﬂuid mechanics 865, 281–302.

RABAULT, JEAN & KUHNLE, ALEXANDER 2019 Accelerating deep reinforcement learning strategies of

ﬂow control through a multi-environment approach. Physics of Fluids 31 (9), 094105.

RABAULT, J. & KUHNLE, A. 2022 Deep Reinforcement Learning applied to Active Flow Controll.

Cambridge University Press.

RABAULT, JEAN, REN, FENG, ZHANG, WEI, TANG, HUI & XU, HUI 2020 Deep reinforcement learning in
ﬂuid mechanics: A promising method for both active ﬂow control and shape optimization. Journal of
Hydrodynamics 32 (2), 234–246.

46

F. Pino, L. Schena, J. Rabault, A. Kuhnle and M.A. Mendez

RAO, ANIL V 2009 A survey of numerical methods for optimal control. Adv Astronaut Sci. 135 (1), 497–528.
RASMUSSEN, CARL EDWARD 2003 Gaussian processes in machine learning. In Summer school on machine

learning, pp. 63–71. Springer.

RASMUSSEN, CARL EDWARD & WILLIAMS, CHRISTOPHER K. I. 2005 Gaussian Processes for Machine

Learning. MIT Press Ltd.

RECHT, BENJAMIN 2019 A tour of reinforcement learning: The view from continuous control 2 (1), 253–279.
REN, FENG, RABAULT, JEAN & TANG, HUI 2021 Applying deep reinforcement learning to active ﬂow

control in weakly turbulent conditions. Physics of Fluids 33 (3), 037121.

ROWLEY, C. W. 2005 Model reduction for ﬂuids, using balanced proper orthogonal decomposition.

International Journal of Bifurcation and Chaos 15 (03), 997–1013.

SCHÄFER, MICHAEL, TUREK, STEFAN, DURST, FRANZ, KRAUSE, EGON & RANNACHER, ROLF 1996
Benchmark computations of laminar ﬂow around a cylinder. In Flow simulation with high-performance
computers II, pp. 547–566. Springer.

SCHAUL, TOM, QUAN, JOHN, ANTONOGLOU, IOANNIS & SILVER, DAVID 2018 Prioritized experience

replay , arXiv: http://arxiv.org/abs/1511.05952v4.

SCHLICHTING (DECEASED), HERMANN & GERSTEN, KLAUS 2017 Boundary–Layer Control

(Suction/Blowing), pp. 291–320. Berlin, Heidelberg: Springer Berlin Heidelberg.

SCHULMAN, JOHN, WOLSKI, FILIP, DHARIWAL, PRAFULLA, RADFORD, ALEC & KLIMOV, OLEG 2017

Proximal policy optimization algorithms , arXiv: http://arxiv.org/abs/1707.06347v2.

SEIDEL, J, SIEGEL, S, FAGLEY, C, COHEN, K & MCLAUGHLIN, T 2008 Feedback control of a circular
cylinder wake. Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace
Engineering 223 (4), 379–392.

SILVER, DAVID, HUANG, AJA, MADDISON, CHRIS J., GUEZ, ARTHUR, SIFRE, LAURENT,
VAN DEN DRIESSCHE, GEORGE, SCHRITTWIESER,
IOANNIS,
PANNEERSHELVAM, VEDA, LANCTOT, MARC, DIELEMAN, SANDER, GREWE, DOMINIK, NHAM,
JOHN, KALCHBRENNER, NAL, SUTSKEVER, ILYA, LILLICRAP, TIMOTHY, LEACH, MADELEINE,
KAVUKCUOGLU, KORAY, GRAEPEL, THORE & HASSABIS, DEMIS 2016 Mastering the game of go
with deep neural networks and tree search. Nature 529 (7587), 484–489.

JULIAN, ANTONOGLOU,

SILVER, DAVID, HUBERT, THOMAS, SCHRITTWIESER, JULIAN, ANTONOGLOU, IOANNIS, LAI,
MATTHEW, GUEZ, ARTHUR, LANCTOT, MARC, SIFRE, LAURENT, KUMARAN, DHARSHAN,
GRAEPEL, THORE, LILLICRAP, TIMOTHY, SIMONYAN, KAREN & HASSABIS, DEMIS 2018 A
general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science
362 (6419), 1140–1144.

SILVER, DAVID, LEVER, GUY, HEESS, NICOLAS, DEGRIS, THOMAS, WIERSTRA, DAAN & RIEDMILLER,
MARTIN 2014 Deterministic policy gradient algorithms. In Proceedings of the 31st International
Conference on International Conference on Machine Learning - Volume 32, p. I–387–I–395.
JMLR.org.

SIPP, DENIS & SCHMID, PETER J. 2016 Linear closed-loop control of ﬂuid instabilities and noise-induced

perturbations: A review of approaches and tools1. Applied Mechanics Reviews 68 (2).

STENGEL, ROBERT F 1994 Optimal control and estimation. Courier Corporation.
SUN, SHILIANG, CAO, ZEHUI, ZHU, HAN & ZHAO, JING 2019 A survey of optimization methods from a

machine learning perspective , arXiv: http://arxiv.org/abs/1906.06821v2.

SUTTON, R.S., BARTON, A.G. & WILLIAMS, R.J. 1992 Reinforcement learning is direct adaptive optimal

control 12 (2), 19–22.

SUTTON, RICHARD S & BARTO, ANDREW G 2018 Reinforcement learning: An introduction. MIT press.
SZITA, ISTVÁN 2012 Reinforcement learning in games. In Adaptation, Learning, and Optimization, pp.

539–577. Springer Berlin Heidelberg.

TANG, HONGWEI, RABAULT, JEAN, KUHNLE, ALEXANDER, WANG, YAN & WANG, TONGGUANG 2020
Robust active ﬂow control over a range of Reynolds numbers using an artiﬁcial neural network trained
through deep reinforcement learning. Physics of Fluids 32 (5), 053605, arXiv: 2004.12417.
UHLENBECK, G. E. & ORNSTEIN, L. S. 1930 On the theory of the brownian motion. Physical Review

36 (5), 823–841.

VANNESCHI, LEONARDO & POLI, RICCARDO 2012 Genetic Programming — Introduction, Applications,

Theory and Open Issues, pp. 709–739. Berlin, Heidelberg: Springer Berlin Heidelberg.

VERMA, SIDDHARTHA, NOVATI, GUIDO & KOUMOUTSAKOS, PETROS 2018 Efﬁcient collective swimming
by harnessing vortices through deep reinforcement learning. Proceedings of the National Academy of
Sciences 115 (23), 5849–5854.

Comparative analysis of machine learning methods for active ﬂow control

47

VINUESA, RICARDO, LEHMKUHL, ORIOL, LOZANO-DURÁN, ADRIAN & RABAULT, JEAN 2022 Flow

control in wings and discovery of novel approaches via deep reinforcement learning. Fluids 7 (2).

VLADIMIR CHERKASSKY, FILIP M. MULIER 2008 Learning from Data. John Wiley & Sons.
WANG, JINJUN & FENG, LIHAO 2018 Flow Control Techniques and Applications. Cambridge University

Press.

WIENER, N. 1948 Cybernetics: or the Control and Communication in the Animal and the Machine. MIT

Press, Boston.

WILLIAMS, RONALD J. 1992 Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine Learning 8 (3-4), 229–256.

ZHANG, HONG-QUAN, FEY, UWE, NOACK, BERND R., KÖNIG, MICHAEL & ECKELMANN, HELMUT

1995 On the transition of the cylinder wake. Physics of Fluids 7 (4), 779–794.

