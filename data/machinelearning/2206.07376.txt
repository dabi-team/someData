2
2
0
2

t
c
O
6
1

]

G
L
.
s
c
[

2
v
6
7
3
7
0
.
6
0
2
2
:
v
i
X
r
a

MSV Policy Optimization via Risk-Averse RL

Mean-Semivariance Policy Optimization via Risk-Averse
Reinforcement Learning

Xiaoteng Ma
Department of Automation, Tsinghua University,
Beijing, 100086, P. R. China

Shuai Ma
School of Business, Sun Yat-sen University,
Guangzhou, 510275, P. R. China

Li Xia
(Corresponding author)
School of Business, Sun Yat-sen University,
Guangzhou, 510275, P. R. China

Qianchuan Zhao
Department of Automation, Tsinghua University,
Beijing, 100086, P. R. China

ma-xt17@mails.tsinghua.edu.cn

mash35@mail.sysu.edu.cn

xiali5@sysu.edu.cn

zhaoqc@tsinghua.edu.cn

Abstract

Keeping risk under control is often more crucial than maximizing expected reward in
real-world decision-making situations, such as ï¬nance, robotics, autonomous driving, etc.
The most natural choice of risk measures is variance, while it penalizes the upside volatility
as much as the downside part. Instead, the (downside) semivariance, which captures negative
deviation of a random variable under its mean, is more suitable for risk-averse proposes.
This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement
learning w.r.t. steady reward distribution. Since semivariance is time-inconsistent and
does not satisfy the standard Bellman equation, the traditional dynamic programming
methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to
Perturbation Analysis (PA) theory and establish the performance diï¬€erence formula for
MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of
RL problems with a policy-dependent reward function. Further, we propose two on-policy
algorithms based on the policy gradient theory and the trust region method. Finally, we
conduct diverse experiments from simple bandit problems to continuous control tasks in
MuJoCo, which demonstrate the eï¬€ectiveness of our proposed methods.

1. Introduction

Reinforcement learning (RL) has shown great promise in solving complex decision problems,
such as Go (Silver et al., 2017), video games (Berner et al., 2019; Vinyals et al., 2019)
and dexterous robotic control (Nagabandi et al., 2020). Learning by trial and error, RL
enables an agent to maximize its accumulated expected rewards through the interaction
with a simulator. However, RL deployment in real-world scenarios is still challenging and
unreliable (GarcÄ±a & FernÃ¡ndez, 2015; Dulac-Arnold et al., 2019). One of the reasons is
that real decision-makers need to consider multi-objective functions. The desired policy

1

 
 
 
 
 
 
Ma, Ma, Xia, & Zhao

Figure 1: A toy example illustrates the eï¬€ect of MSV. We refer the policy going left as l and
the other as r. Two policies have the same average return Î·l = Î·r = 0 and the same variance
Î¶ l = Î¶ r = 2. However, since the semivariance Î¶ l
âˆ’ = 2/3, the policy going right
has a smaller (downside) semivariance. It shows that MSV enables to avoid extreme costs
compared with MV.

âˆ’ = 4/3 > Î¶ r

should perform well for broader metrics, not just for expectation. That raises the demand of
risk-sensitive learning, which aims at balancing the return and risk in face of uncertainty.

The risk-sensitive decision-making has been widely studied beyond the scope of RL,
which can be traced back to the mean-variance (MV) optimization theory established by
Markowitz (Markowitz, 1952). Variance, which captures the ï¬‚uctuation and concentration of
random variables, is a natural choice of the risk measure. As Markowitz only considers the
single-period problem, many studies focus on extending the results to multi-period scenarios,
from stochastic control (Li & Ng, 2000) to Markov decision process (Sobel, 1982; Filar
et al., 1989). However, the variance of a multi-period problem depends on the average
value of the whole process. It breaks the essential property of dynamic programmingâ€”time-
consistency, and makes it hard to design model-free learning algorithms under the standard
RL framework. Developing an eï¬ƒcient algorithm to optimize MV is still an ongoing topic
in the RL community (Xie et al., 2018; Bisi et al., 2020; Xia, 2020; Zhang et al., 2021; Ma
et al., 2022b, 2022a).

While MV analysis is the most widely applied risk-return analysis in practice, variance
metric is questionable as a risk measure. As a measure of volatility, variance penalizes
upside deviations from the mean as much as downside deviations. It could be problematic
as the upside deviation comes from the higher return which is desirable. In general, the
outcome distributions in the real world are often asymmetrical, such as the ones in the stock
market (Estrada, 2007; Bollerslev et al., 2020), suggesting that we should control the â€œgoodâ€
and â€œbadâ€ volatility separately. Hence, Markowitz (1959) presents the mean-semivariance
(MSV) as an alternative measure, which only penalizes the â€œbadâ€ volatility, performing as
a downside risk indicator. Even if the distribution is symmetrical, optimizing MSV is at
least eï¬€ective as optimizing MV. To better illustrate the diï¬€erence between variance and
semivariance, we construct a simple MDP example shown in Figure 1. The two policies
result in two reward distributions symmetrically, for which variances are indistinguishable.
However, the policy going right is preferred since it results in a lower semivariance.

Though MSV is a more plausible measure of risk, optimizing MSV is even more com-
plicated than MV. It inherits time-inconsistency from variance and introduces a truncation

2

Dept. of Automation, Tsinghua University411âˆ’1âˆ’12âˆ’2ğ‘ 1ğ‘ 2ğ‘ 0ğ‘ 3ğ‘ 4MSV Policy Optimization via Risk-Averse RL

Algorithm 1 The framework of MSV optimization

Initialize policy as Âµ
repeat

Evaluate Âµ and get Î· (cf. Equation 1) and Î·âˆ’ (cf. Equation 12)
Set reward function as g = (1 + 2Î²Î·âˆ’)r
Âµ

POLICY_UPDATE(Âµ, g)

Î·)2
âˆ’

Î²(r

âˆ’

âˆ’

â†

until Âµ converges

function of mean, making the analysis non-trivial. Due to the complexity of this objective,
existing works consider a subset of problems restricted with a ï¬xed mean (Wei, 2019) or
heuristic algorithms for MSV (Yan et al., 2007; Zhang et al., 2012; Liu & Zhang, 2015; Chen
et al., 2019). To the best of our knowledge, there are currently no relevant studies on MSV
in the RL literature.

In this paper, we aim to ï¬ll the gap of the previous study on the single-period MSV
problem and extend the static methods to online RL algorithms. To achieve that, we resort to
Perturbation Analysis (PA) theory (Cao, 2007) (also called the sensitivity-based optimization
theory or the relative optimization theory) for Markov systems, which lays the basis of many
eï¬ƒcient RL methods, such as TRPO (Schulman et al., 2015), CPO (Achiam et al., 2017)
and MBPO (Janner et al., 2019). The contributions of our work are threefold. Firstly,
instead of constructing a Bellman operator, we establish the MSV performance diï¬€erence
formula of two policies (see Section 4 for details). The result indicates that the performance
diï¬€erence can be decomposed into two parts: the improvement corresponding to a reward
function depending on the current policy and the average performance change from the
current to the updated one. Second, we iteratively optimize MSV by considering the shift
in mean locally and constructing a surrogate reward function. The framework is shown in
Algorithm 1. Under this framework, we develop two algorithms based on the policy gradient
theory and the trust region method, respectively. We show that optimizing the surrogate
reward function in the trust region has a similar performance lower bound with the standard
TRPO, which guarantees the monotonic improvement if the trust region is tight. Finally, we
conduct diverse experiments to examine the eï¬€ectiveness of our proposed methods, including
a bandit problem, a tabular portfolio management problem, and robotic control tasks based
on MuJoCo. The results demonstrate that the proposed algorithms successfully improve the
performance under the criterion of MSV, which is better than standard RL from a risk-averse
perspective.

2. Related Work

Below we brieï¬‚y review the literature about optimization of MSV and other risk measures.

2.1 Mean-Semivariance

MSV is ï¬rst introduced by Markowitz (1959) as an alternative to MV. Thereafter, many
researchers study portfolio selection problems by employing the semivariance as the risk
measure (Markowitz et al., 1993; Hogan & Warren, 1974; Choobineh & Branting, 1986;
Briec & Kerstens, 2009), most of which are limited to the single-period problem. Due

3

Ma, Ma, Xia, & Zhao

to the complexity of MSV, previous studies on MSV in multi-period problems resort to
heuristic methods, such as fuzzy systems and genetic algorithms (Yan et al., 2007; Zhang
et al., 2012; Liu & Zhang, 2015; Chen et al., 2019). Wei (2019) studies a special case of
MSV in the continuous-time MDP, where the mean of the discounted total cost is equal to
a given function. Another stream of researches (Tamar et al., 2016; Shapiro et al., 2021)
study semideviation instead of semivariance. As standard deviation is an alternative to
variance, semideviation is considered as an alternative to semivariance. The main beneï¬t of
mean-semideviation (MSD) is that it satisï¬es the property â€œcoherent,â€ and hence it can be
written in a Bellman form (RuszczyÅ„ski, 2010). However, the additional square operation
makes optimizing MSD with a data-driven approach non-trivial. We leave the optimization of
MSD in RL as future work. Furthermore, maximizing the upside semivariance could improve
the exploration ability (Mavrin et al., 2019; Ma et al., 2020; Zhou et al., 2020), showing the
potential of MSV from an opposite perspective.

2.2 Mean-Variance

Since MSV is highly related to MV, in this part, we summarize the works on MV in Markov
decision processes (MDPs) and RL. Based on the deï¬nition of variance in the framework
of MDPs, the existing studies on variance can be broadly divided into two categories. One
stream of works (Sobel, 1982; Castro et al., 2012; Prashanth & Ghavamzadeh, 2016; Xie
et al., 2018) concern the variance of total return R = (cid:80)âˆ
t=0 Î³trt under the initial state
distribution, i.e., VÏ€0(R) where Î³ is the discount factor, Ï€0 is the initial state distribution
and rt is the reward at the stage t. This deï¬nition concerns the risk of total rewards at the
ï¬nal stage, while we are more concerned about long-term volatility in practical problems.
Hence, the long-run variance (Filar et al., 1989; Chung, 1994; Gosavi, 2014; Xia, 2016, 2020;
Bisi et al., 2020; Zhang et al., 2021), also known as the steady-state variance, is proposed to
describe the variance of the steady reward distribution. The long-run variance is deï¬ned by
(cf. Equation 3), where Î·Âµ is the long-run average
limT â†’âˆ
of policy Âµ. Since the average reward Î·Âµ depends on the current policy, it breaks the
time-consistency. To handle this problem, Xia (2016, 2020) derives a variance performance
diï¬€erence formula with PA and proposes a policy iteration algorithm which is guaranteed to
converge to a local optimum. In this paper, we adopt similar deï¬nition of Xiaâ€™s work and
extend the formulation from MV to MSV.

t=0 (r(st, at)

(cid:104)(cid:80)T âˆ’1

Î·Âµ)2(cid:105)

EÏ€0,Âµ

âˆ’

1
T

2.3 Other Risk Measures

Besides the MV and MSV, other risk measures capture diï¬€erent features of the return
distribution. A classical risk measure in optimal control is exponential utility (Howard &
Matheson, 1972; Borkar & Meyn, 2002; Fei et al., 2020). The exponential utility enjoys a
product form of the Bellman equation. Therefore the corresponding value-based algorithms
such as Q-learning are well-developed. While the exponential Bellman equation is elegant in
theory, it poses some computational problems as the exponential values are often too large to
be numerically calculated. Another famous risk measure is Conditional Value at Risk (CVaR),
deï¬ned as the average value under the Î±-quantile. Many existing methods (Nemirovski &
Shapiro, 2007; Chow & Ghavamzadeh, 2014; Tamar et al., 2015; Chow et al., 2015, 2017)
optimize CVaR as the objective or constraints. The main diï¬€erence between CVaR and

4

MSV Policy Optimization via Risk-Averse RL

MSV is that CVaR puts even weights for the events under a certain threshold, while the
importance of the extreme values on the concerned side increases quadratically in MSV. We
refer to Delage et al.â€™s work (2019) for more discussion on the connection of diï¬€erent risk
measures.

3. Preliminaries

S

In this paper, we focus on the inï¬nite-horizon discrete-time MDP as
where
denotes the state space,
denotes a bounded reward function and P :
Ï€0
S
are ergodic. Let Âµ :
S (cid:55)â†’
randomized policy space.

,
,
, r, P, Ï€0
(cid:105)
A
(cid:104)S
Rmax, Rmax]
[
âˆ’
) is the transition matrix and
S Ã— A (cid:55)â†’
S
) denotes the initial state distribution. We assume that all the involved MDPs
) denote a Markovian randomized policy and Î  denote the

denotes the action space, r :

M
S Ã— A (cid:55)â†’

âˆ†(

âˆ†(

âˆ†(

A

A

=

âˆˆ

We are interested in the long-run average reward

Î·Âµ := lim
T â†’âˆ

1
T

EÏ€0,Âµ

(cid:35)

r(st, at)

,

(cid:34)T âˆ’1
(cid:88)

t=0

(1)

where EÏ€0,Âµ stands for the expectation with s0
that Î·Âµ is independent of Ï€0 when T
is convenient to rephrase the long-run average reward as

Ï€0, at

â†’ âˆ

st, at). Note
st), st+1
. With Ï€ denoting the steady state distribution, it

P (

Âµ(

Â· |

Â· |

âˆ¼

âˆ¼

âˆ¼

The variance and semivariance w.r.t. Âµ are deï¬ned by

Î·Âµ := Esâˆ¼Ï€,aâˆ¼Âµ [r(s, a)] .

Î¶ Âµ := lim
T â†’âˆ

Î¶ Âµ
âˆ’ := lim
T â†’âˆ

1
T

1
T

EÏ€0,Âµ

EÏ€0,Âµ

(cid:34)T âˆ’1
(cid:88)

t=0
(cid:34)T âˆ’1
(cid:88)

t=0

(r(st, at)

(r(st, at)

(cid:35)

Î·Âµ)2

,

(cid:35)

Î·Âµ)2
âˆ’

,

âˆ’

âˆ’

(2)

(3)

(4)

where (
)âˆ’ := min
{
Â·

0,

. In this paper, we focus on the mean-semivariance criterion,

Â·}

Î¾Âµ
âˆ’ := Î·Âµ

Î²Î¶ Âµ
âˆ’,

âˆ’

where Î²
when mean-variance criterion is mentioned, we mean Î¾Âµ := Î·Âµ

0 is the parameter for the trade-oï¬€ between mean and semivariance. Analogously,

Î²Î¶ Âµ.

â‰¥

We further respectively deï¬ne state-value function, action-value function, and advantage

âˆ’

function for average reward as

V Âµ
Î· (s) := EÂµ

(cid:34) âˆ
(cid:88)

(r(st, at)

QÂµ

Î· (s, a) := EÂµ

t=0
(cid:34) âˆ
(cid:88)

(r(st, at)

(cid:35)

s0 = s

,

(cid:35)

s0 = s, a0 = a

,

Î·Âµ)

Î·Âµ)

|

|

âˆ’

âˆ’

AÂµ

Î· (s, a) := QÂµ

t=0
Î· (s, a)

V Âµ
Î· (s).

âˆ’

5

Ma, Ma, Xia, & Zhao

Similarly, the value functions for semivariance are deï¬ned as

V Âµ
Î¶âˆ’

(s) := EÂµ

QÂµ
Î¶âˆ’

(s, a) := EÂµ

(cid:34) âˆ
(cid:88)

t=0
(cid:34) âˆ
(cid:88)

AÂµ
Î¶âˆ’

(s, a) := QÂµ
Î¶âˆ’

t=0
(s, a)

(cid:0)(r(st, at)

Î·Âµ)2

âˆ’ âˆ’

âˆ’

(cid:1)

Î¶ Âµ
âˆ’

(cid:0)(r(st, at)

Î·Âµ)2

âˆ’ âˆ’

âˆ’

(cid:1)

Î¶ Âµ
âˆ’

(cid:35)

s0 = s

,

(cid:35)

s0 = s, a0 = a

,

|

|

V Âµ
Î¶âˆ’

(s).

âˆ’

For notation simplicity, we will omit the superscript â€œÂµâ€ when the context is clear, e.g.,
the average rewards Î·Âµ, Î·Âµ(cid:48) are written as Î·, Î·(cid:48) instead. When r is mentioned, we omit (s, a)
and use r in short.

Before our analysis of MSV, we brieï¬‚y review the average-reward policy gradient theorem

and the trust region theorem.

Theorem 1 (Average-Reward Policy Gradient by Sutton & Barto, 2018). For a policy Âµ
parameterized by Î¸, we have

Î¸Î· = Esâˆ¼Ï€,aâˆ¼Âµ[

âˆ‡

Î¸ log Âµ(a

|

s)AÂµ

Î· (s, a)].

âˆ‡

Theorem 2 (Average-Reward Trust Region Policy Optimization by Zhang & Ross, 2021;
Ma et al., 2021). Consider the following problem,

max

Âµ(ÂµÎ¸),

ÂµÎ¸ L
s.t. Esâˆ¼Ï€DTV(ÂµÎ¸(

s)

Âµ(

Â· |

(cid:107)

Â· |

s))

â‰¤

(cid:15)Âµ,

where

Âµ(ÂµÎ¸) := Esâˆ¼Ï€,aâˆ¼ÂµÎ¸

(cid:2)AÂµ

Î· (s, a)(cid:3) .

L

Denote Âµ(cid:48) as the solution of the above problem. The following bound holds:

Î·(cid:48)

âˆ’

where (cid:15)Î· = maxs

Eaâˆ¼Âµ(cid:48)[AÂµ
|

Î· (s, a)]
|

4. Perturbation Analysis

Î·

Âµ(Âµ(cid:48))

2(Îº(cid:48)

1)(cid:15)Î·(cid:15)Âµ,

âˆ’

â‰¥ L

âˆ’
and Îº(cid:48) is Kemenyâ€™s constant under Âµ(cid:48).

(5)

(6)

(7)

In this section, we derive the MSV performance diï¬€erence formula (MSVPDF), where the
core conceptâ€”performance diï¬€erence formulaâ€”comes from the PA for Markov systems, also
called the sensitivity-based optimization theory. With the aid of MSVPDF, we obtain the
necessary optimality condition for the MSV problem. It also lays the basis for developing
optimization algorithms (see Section 5), such as the policy gradient method and the trust
region method. For readers unfamiliar with PA, we provide a brief review of the theory in
Appendix A.

6

MSV Policy Optimization via Risk-Averse RL

4.1 Performance Diï¬€erence Formula

MSVPDF is formally stated as below.

Theorem 3. For any two policies Âµ, Âµ(cid:48)

Î , we have

âˆˆ
Î²AÂµ
Î¶âˆ’

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’ = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

Î· (s, a)

âˆ’

(s, a)]

âˆ’

Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[(r

Î·(cid:48))2

âˆ’ âˆ’

âˆ’

(r

âˆ’

Î·)2
âˆ’]

(8)

Proof. To decompose the policy performance with the policy-dependent reward, we ï¬rst
introduce a pseudo mean Î». We analyze the policy diï¬€erence with the pseudo mean and
corresponding pseudo reward function, and then turn into the true mean by letting Î» = Î·.
With a pseudo mean Î», we transform the original problem into a standard MDP with

reward function

We obtain a pseudo mean-semivariance objective by optimizing this pseudo reward-

f (s, a) := r

Î²(r

âˆ’

âˆ’

Î»)2
âˆ’.

(9)

function,

By deï¬nition, we have

Î¾Î»,âˆ’ := Î¾Âµ

Î»,âˆ’ = Esâˆ¼Ï€,aâˆ¼Âµ [f (s, a)] .

Î¾âˆ’

âˆ’

Î¾Î»,âˆ’ = Esâˆ¼Ï€,aâˆ¼Âµ

(cid:2)r

Î²(r

Î·)2

âˆ’ âˆ’

âˆ’

âˆ’

f (s, a)(cid:3) .

Since the pseudo reward is independent of the policy, we can write its performance diï¬€erence
formula directly (Cao, 2007, Chapter 2):

Î¾(cid:48)
Î»,âˆ’ âˆ’

Î¾Î»,âˆ’ = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

f (s, a)],

(10)

where AÂµ
Equation 10, we can derive the performance diï¬€erence formula of Î¾âˆ’ as

f (s, a) is the pseudo advantage with f as the reward function. With the aid of

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’ = (Î¾(cid:48)

Î»,âˆ’ âˆ’
= Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ
Î²Esâˆ¼Ï€,aâˆ¼Âµ

Î¾Î»,âˆ’) + (Î¾(cid:48)
f (s, a)]
(cid:2)(r

Î¾(cid:48)
Î»,âˆ’) + (Î¾Î»,âˆ’
Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)
Î·)2
âˆ’

âˆ’ âˆ’
âˆ’
Î»)2
âˆ’ âˆ’

(r

âˆ’

âˆ’
(cid:2)(r
(cid:3) .

âˆ’

âˆ’

Î¾âˆ’)

Î·(cid:48))2

âˆ’ âˆ’

âˆ’

(r

âˆ’

(cid:3)

Î»)2
âˆ’

Finally, by setting Î» = Î·, we arrive at

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’ = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

f (s, a)]

Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

(cid:2)(r

Î·(cid:48))2

âˆ’ âˆ’

âˆ’

(r

âˆ’

Î·)2
âˆ’

(cid:3) ,

âˆ’

which is the same as Equation 8 if we explicitly calculate the advantage function with reward
function f and Î» = Î·.

The MSVPDF in Equation 8 or Equation 11 claims that the MSV improvement can be
separated into two parts. The ï¬rst term in Equation 11 is a standard MDP with f as the
reward function, and the second term is caused by the perturbation of the mean. It clearly
quantiï¬es the diï¬ƒculty of solving the MSV problem, i.e., the policy-dependent reward function
breaks down the time-consistent nature of MDPs. Meanwhile, it also shows us the standard
MDP algorithm such as policy iteration (PI) is unavailable. A PI-like algorithm may be
eï¬ƒcient in improving the ï¬rst term, but the sign of the remaining term (dependent on Î·(cid:48)) is
unpredictable. It suggests that we need novel tools to guarantee the policy improvement.

7

Ma, Ma, Xia, & Zhao

4.2 Performance Derivative Formula

While Equation 11 describes the performance diï¬€erence between any two policies, we still need
the local structure of the MSV problem to guide the direction of optimization. Following the
line of the last part, we present the MSV performance derivative formula in this subsection,
which describes the performance derivative at Âµ towards another policy Âµ(cid:48).

Theorem 4. Given any two policies Âµ, Âµ(cid:48)

Î , we consider a mixed policy ÂµÎ½,

âˆˆ

ÂµÎ½(a

|

s) = (1

Î½)Âµ(a

|

âˆ’

s) + Î½Âµ(cid:48)(a

s),

|

where the action follows Âµ with probability 1
We have

âˆ’

Î½, and follows Âµ(cid:48) with probability Î½ for Î½

[0, 1].

âˆˆ

dÎ¾âˆ’
dÎ½

= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[(1 + 2Î²Î·âˆ’)AÂµ

Î· (s, a)

Î²AÂµ
Î¶âˆ’

(s, a)].

âˆ’

Proof. From MSVPDF, we obtain the diï¬€erence for Âµ, ÂµÎ½,

Î¾Î½
âˆ’ âˆ’

Î¾âˆ’ = Esâˆ¼Ï€Î½ ,aâˆ¼ÂµÎ½ [AÂµ

f (s, a)]

Î²Esâˆ¼Ï€Î½ ,aâˆ¼ÂµÎ½ (cid:2)(r

Î·Î½)2

âˆ’ âˆ’

(r

âˆ’

âˆ’

Î·)2
âˆ’

(cid:3) ,

âˆ’

where Î·Î½ := Î·ÂµÎ½ . Taking the derivative w.r.t. Î½ and letting Î½
0, we obtain the performance
derivative formula. To simplify the derivation, we denote the terms on the right hand side as

â†’

h1(Î½) = Esâˆ¼Ï€Î½ ,aâˆ¼ÂµÎ½ [AÂµ
h2(Î½) = Esâˆ¼Ï€Î½ ,aâˆ¼ÂµÎ½ (cid:2)(r

f (s, a)],
Î·Î½)2

Î·)2
âˆ’

(cid:3) .

(r

âˆ’

âˆ’ âˆ’

âˆ’

Then Î¾Î½

âˆ’ âˆ’

Î¾âˆ’ = h1(Î½)

âˆ’

Î²h2(Î½). Speciï¬cally, we have

h1(Î½) = Esâˆ¼Ï€Î½ [(1

= Î½Esâˆ¼Ï€Î½ ,aâˆ¼Âµ(cid:48)[AÂµ

âˆ’

Î½)Eaâˆ¼Âµ[AÂµ
f (s, a)],

f (s, a)] + Î½Eaâˆ¼Âµ(cid:48)[AÂµ

f (s, a)]]

where the last equality follows that Eaâˆ¼Âµ[AÂµ

f (s, a)] = 0. Since limÎ½â†’0 Ï€Î½ = Ï€, we obtain

dh1
dÎ½

= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[AÂµ

f (s, a)].

Next, we diï¬€erentiate (r

Î·)2
âˆ’

,

âˆ’

d(r

Î·)2
âˆ’

âˆ’
dÎ½

= 2(r

âˆ’

d(r

Î·Î½)âˆ’

Î·)âˆ’

âˆ’
dÎ½
Î·)âˆ’1(r < Î·)

2(r

âˆ’

âˆ’

(i)
=

(ii)
=

2(r

âˆ’

âˆ’

Î·)âˆ’

dÎ·
dÎ½

,

8

dÎ·
dÎ½

(11)

MSV Policy Optimization via Risk-Averse RL

where (i) follows d(x)âˆ’
we have

dx = 1(x < 0), and (ii) comes from (r

Î·)âˆ’1(r < Î·) = (r

Î·)âˆ’. Thus,

âˆ’

âˆ’

dh2
dÎ½

= lim
Î½â†’0

1
Î½

(cid:88)

Ï€Î½(s)

(cid:88)

ÂµÎ½(a

s

a

|

s) (cid:2)(r

Î·Î½)2

âˆ’ âˆ’

(r

(cid:3)

Î·)2
âˆ’

âˆ’
Î·Î½)2

âˆ’
Î·)2
âˆ’

(cid:88)

Ï€Î½(s)

(cid:88)

ÂµÎ½(a

= lim
Î½â†’0

(r

âˆ’

s)

|

(r

âˆ’

âˆ’ âˆ’
Î½

s

Ï€(s)

Ï€(s)

a

Âµ(a

Âµ(a

(cid:88)

a
(cid:88)

a

s)

s)

|

|

(cid:88)

s
(cid:88)

s

d(r

Î·)2
âˆ’

âˆ’
dÎ½

(cid:20)

2(r

âˆ’

âˆ’

Î·)âˆ’

(cid:21)

dÎ·
dÎ½

=

=

=

2Î·âˆ’

âˆ’

dÎ·
dÎ½

.

Here we deï¬ne the semimean Î·âˆ’ as

Î·âˆ’ := Î·Âµ

âˆ’ = Esâˆ¼Ï€,aâˆ¼Âµ[(r

Î·)âˆ’],

âˆ’

(12)

which is the downside expectation of rewards under Ï€. From the standard result of PA (Cao,
2007, Chapter 2), we have

Putting the above relationships together, we obtain

dÎ·
dÎ½

= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[AÂµ

Î· (s, a)].

dÎ¾âˆ’
dÎ½

Î²

=

dh1
dÎ½ âˆ’

dh2
dÎ½
= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[AÂµ
= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[AÂµ
= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[(1 + 2Î²Î·âˆ’)AÂµ

dÎ·
f (s, a)] + 2Î²Î·âˆ’
dÎ½
f (s, a) + 2Î²Î·âˆ’AÂµ
Î· (s, a)]
Î²AÂµ
Î· (s, a)
Î¶âˆ’

âˆ’

(s, a)].

The above equality indicates that the performance derivative is related to another reward

function w.r.t. f (cf. Equation 9):

g(s, a) := f (s, a) + 2Î²Î·âˆ’r

= (1 + 2Î²Î·âˆ’)r

Î²(r

âˆ’

âˆ’

Î·)2
âˆ’,

and the derivative formula can be written as

dÎ¾âˆ’
dÎ½

= Esâˆ¼Ï€,aâˆ¼Âµ(cid:48)[AÂµ

g (s, a)],

(13)

(14)

(15)

where AÂµ

g (s, a) is the advantage function w.r.t. g.

With the performance derivative formula, we deï¬ne the local optimum for MSV and

present the necessary condition for MSV optimality.

9

Ma, Ma, Xia, & Zhao

Deï¬nition 1. For a policy Âµ,
say Âµ is a local optimum in the mixed policy space.

âˆˆ

Â¯Î½

âˆƒ

(0, 1) and we always have Î¾Âµ

âˆ’ â‰¥

Î¾Î½
âˆ’,

Î½
âˆ€

âˆˆ

(0, Â¯Î½), then we

Theorem 5. The optimal policy of MSV can be found in the deterministic policy space, and
satisï¬es the necessary condition

Âµâˆ—(a

|

(cid:18)

s) = Î´

a

argmax
bâˆˆA

âˆˆ

(cid:19)

Aâˆ—

g(s, b)

,

which implies that Aâˆ—

g(s, a)

0,

s
âˆ€

â‰¤

âˆˆ S

âˆˆ A

, a

. Here Î´ denotes the Dirac delta function.

Proof. The theorem is a direct result of the derivative formula. The (local) optimality implies
that if Âµ is a local optimum, we always have dÎ¾âˆ’
0 for any direction in the policy space.
Assuming there is a contradiction, where for a state s there exists Âµ(a
s) = Î´(a = a(cid:48)) for
any a(cid:48) /
g (s, b), we can always ï¬nd a better policy in the mixed policy space along
âˆˆ
the derivative direction.

argmaxb AÂµ

dÎ½ â‰¤

|

5. Optimization and Algorithms

In this section, we propose two approaches to optimize MSV with the parameterized policy.
We ï¬rstly extend the policy gradient method to MSV with the pseudo reward function (cf.
Equation 13) in Section 4. Following the same idea, we propose a trust region method to
solve the MSV problem, and prove the the lower bound for its performance improvement.
The two approaches together establish an iterative framework to solve the MSV problem.

5.1 MSV Policy Gradient Method

Policy gradient theorem is an essential foundation of modern deep RL algorithms, such as
Actor-Critic methods. Here we consider the policy Âµ parameterized by Î¸
Î˜, which can
be implemented with any diï¬€erentiable function. We ï¬rst give the MSV Policy Gradient
(MSVPG) theory formally as follows.

âˆˆ

Theorem 6. For a policy Âµ parameterized by Î¸, we have

Î¸Î¾âˆ’ = Esâˆ¼Ï€,aâˆ¼Âµ[

âˆ‡

Î¸ log Âµ(a

|

s)AÂµ

g (s, a)].

âˆ‡

(16)

The policy gradient for MSV can be easily proved by PA, which follows the same lines of
derivative formula. For the readers from the DRL community, we also provide an alternative
proof based on (Sutton & Barto, 2018) in the appendix.

Proof. Consider two policies Âµ, Âµ(cid:48) parameterized by Î¸, Î¸(cid:48) respectively. Their performance
diï¬€erence is given as

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’ = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

f (s, a)]

Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

(cid:2)(r

Î·(cid:48))2

âˆ’ âˆ’

âˆ’

(r

âˆ’

Î·)2
âˆ’

(cid:3) .

âˆ’

Let denote âˆ†Î¸ = Î¸(cid:48)
above equation

âˆ’

Î¸. Similar to the derivation in Section 4.2, we denote the terms of

h1(âˆ†Î¸) = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ
(cid:2)(r
h2(âˆ†Î¸) = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

f (s, a)],
Î·(cid:48))2

Î·)2
âˆ’

(cid:3) .

(r

âˆ’

âˆ’ âˆ’

âˆ’

10

MSV Policy Optimization via Risk-Averse RL

We take the limit of Î¾(cid:48)

âˆ’ âˆ’

Î¾âˆ’ by letting Î¸(cid:48)

Î¸.

â†’

Î¸h1 = lim
âˆ†Î¸â†’0

âˆ‡

(cid:88)

Ï€(cid:48)(s)

(cid:88)

(cid:104)
Âµ(cid:48)(a

s)AÂµ

f (s, a)

(cid:105)

|

1
âˆ†Î¸

(cid:88)

s

(i)
= lim
âˆ†Î¸â†’0
(cid:88)

=

Ï€(s)

s

(cid:88)
a âˆ‡
(cid:104)

âˆ‡

s

(cid:88)

Ï€(cid:48)(s)

a
Âµ(cid:48)(a

|

s)

âˆ’
âˆ†Î¸

Âµ(a

s)

|

AÂµ

f (s, a)

a

Î¸Âµ(a

|

s)AÂµ

f (s, a)

(ii)
= Esâˆ¼Ï€,aâˆ¼Âµ

Î¸ log Âµ(a

s)AÂµ

(cid:105)
f (s, a)

,

|

where (i) follows Eaâˆ¼Âµ[AÂµ

f (s, a)] = 0 and (ii) comes from

Similar to the derivation in Equation 11, we have

Î¸ log Âµ(a

âˆ‡

|

s) = âˆ‡

Î¸Âµ(a
Âµ(a

s)

|
s)

.

|

Î¸h2 = lim
âˆ†Î¸â†’0

âˆ‡

(cid:88)

Ï€(cid:48)(s)

(cid:88)

Âµ(cid:48)(a

s

a

(r

âˆ’

Î·(cid:48))2

âˆ’ âˆ’
âˆ†Î¸

s)

|

Î·)2
âˆ’

(r

âˆ’

=

=

=

=

(cid:88)

s
(cid:88)

s
(cid:88)

Ï€(s)

Ï€(s)

Ï€(s)

(cid:88)

a
(cid:88)

a
(cid:88)

Âµ(a

Âµ(a

Âµ(a

s
2Î·âˆ’

âˆ’

âˆ‡

a
Î¸Î·.

|

|

|

s) lim
âˆ†Î¸â†’0

(r

âˆ’

Î·(cid:48))2

âˆ’ âˆ’
âˆ†Î¸

Î·)2
âˆ’

(r

âˆ’

Î¸(r

s)

âˆ‡

âˆ’

Î·)2
âˆ’

s)[

2(r

âˆ’

Î·)âˆ’

Î¸Î·]

âˆ‡

âˆ’

Since

Î¸Î· = Esâˆ¼Ï€,aâˆ¼Âµ [

the gradient of Î¾âˆ’

âˆ‡

Î¸ log Âµ(a

|

s)AÂµ

Î· (s, a)], we combine the results together and give

âˆ‡

Î¸Î¾âˆ’ =

âˆ‡

âˆ‡

Î²
Î¸h1
âˆ’
âˆ‡
(cid:104)
= Esâˆ¼Ï€,aâˆ¼Âµ

Î¸h2

Î¸ log Âµ(a

âˆ‡

(cid:104)

(cid:2)

Î¸ log Âµ(a

âˆ‡

Î¸ log Âµ(a

âˆ‡

s)AÂµ

s)AÂµ
s)AÂµ

Î¸Î·

+ 2Î²Î·âˆ’

(cid:105)
f (s, a)
âˆ‡
(cid:105)
f (s, a) + 2Î²Î·âˆ’AÂµ
Î· (s, a)
g (s, a)(cid:3) .

|

|

|

= Esâˆ¼Ï€,aâˆ¼Âµ
= Esâˆ¼Ï€,aâˆ¼Âµ

Here we present an Actor-Critic algorithm based on MSVPG, which is named MSVAC (see
Algorithm 2). In addition to the parameterized policy, we maintain another parameterized
function VÏ† as the value function. Then, the advantage function is estimated with the
generalized advantage estimation (GAE) (Schulman et al., 2016). Typically, we have

Ë†Ag(sn, an) =

N âˆ’1
(cid:88)

t=n

Î»tâˆ’n (g(st, at)

Ë†g + VÏ†(st)

âˆ’

âˆ’

VÏ†(st+1)) ,

(17)

11

Ma, Ma, Xia, & Zhao

Algorithm 2 MSVAC
Input: Î±, Î², K, N
1: Initialize the policy with Î¸ and the value with Ï† randomly.
2: Set Ë†Î· = 0, Ë†Î·âˆ’ = 0, Ë†Î¶âˆ’ = 0.
3: for k = 1, 2,
4:

, K do

Â· Â· Â·

(1

â†

(cid:80)N âˆ’1

âˆ’
(1
(1

(sn, an, rn, sn+1)
}
{

Execute policy ÂµÎ¸ for N times to collect
Update Ë†Î·
Î±)Ë†Î· + Î± 1
N
Update Ë†Î·âˆ’
Î±)Ë†Î·âˆ’ + Î± 1
N
Update Ë†Î¶âˆ’
Î±)Ë†Î¶âˆ’ + Î± 1
N
Compute g(sn, an) with Equation 13 at all timesteps and Ë†g.
Compute Ë†Ag(sn, an) with Equation 17 at all timesteps.
Update the Î¸ with Equation 16.
Update the Ï† with Equation 18.

n=0 rn.
(cid:80)N âˆ’1
(cid:80)N âˆ’1

n=0 (rn
n=0 (rn

Ë†Î·)âˆ’.
.
Ë†Î·)2
âˆ’

â†
â†

âˆ’
âˆ’

âˆ’
âˆ’

5:

6:

7:

8:

9:

10:

11:
12: end for

N âˆ’1
n=0

.

Î² Ë†Î¶âˆ’ is
where Î» is the hyper-parameter to trade-oï¬€ bias and variance, and Ë†g = (1 + 2Î² Ë†Î·âˆ’)Ë†Î·
the estimation of average surrogate reward function. With Ë†Vn = VÏ†(sn) + Ë†Ag(sn, an) as the
target value, we update the value function with

âˆ’

V (Ï†) :=

L

1
2N

N âˆ’1
(cid:88)

(VÏ†(sn)

n=0

Ë†Vn)2.

âˆ’

(18)

5.2 MSV Trust Region Method

While PG has a concise form, it often suï¬€ers from the diï¬ƒculty of selecting step-sizes and
the sensitivity to initial points in practice, especially when it works with neural networks. To
address these drawbacks, trust region method (Schulman et al., 2015) is proposed to solve a
surrogate problem in a local trust region and perform an approximate policy iteration.

5.2.1 Monotonic Improvement Guarantee

We extend the idea of trust region in the standard MDP into MSV, and propose the MSV
Trust Region Policy Optimization (MSVTRPO) method. In MSVTRPO, we iteratively solve
the problem as below

max

Âµ
g (ÂµÎ¸)
ÂµÎ¸ L
s.t. Esâˆ¼Ï€DTV(ÂµÎ¸(

Â· |

s)

Âµ(

Â· |

(cid:107)

s))

â‰¤

(cid:15)Âµ,

(19)

where

Âµ
g (ÂµÎ¸) := Esâˆ¼Ï€,aâˆ¼ÂµÎ¸

(cid:2)AÂµ

g (s, a)(cid:3) .

L

Remark 1. The trust region method updates the policy via the direction of maximum deriva-
tive (cf. the performance derivative formula in Equation 15), constrained in the proximity

12

MSV Policy Optimization via Risk-Averse RL

policy space with the T V -divergence. In contrast, the standard policy iteration scheme updates
the policy via the same direction without constraint, which breaks the monotonic improvement
for MSV.

Next, we will show that MSVTRPO enjoys an analogous performance improvement
0, the lower bound is dominated by

bound. When the trust region is tight enough, i.e., (cid:15)Âµ
the ï¬rst order term.

â†’

To complete the proof, we need following lemma to bound the state-action distributions.
For a policy Âµ, we denote the steady state-action distribution as Ï(s, a) := Ï€(s)Âµ(s, a). Then
we have:

Lemma 1. For any two policies Âµ, Âµ(cid:48)
distributions Ï, Ï(cid:48) is bounded by

âˆˆ

Î , the diï¬€erence of their steady state-action

Ï(cid:48)

(cid:107)

1

Ï
(cid:107)

âˆ’

â‰¤

2Îº(cid:48)(cid:15)Âµ.

Proof.

Ï(cid:48)

(cid:107)

Ï
(cid:107)

âˆ’

1 =

â‰¤

=

â‰¤

(cid:88)
s,a |
(cid:88)
s,a |
(cid:88)
s |
2 (cid:0)(Îº(cid:48)

Ï€(cid:48)(s)Âµ(cid:48)(a

Ï€(s)Âµ(a

s)

|

|

Ï€(cid:48)(s)Âµ(cid:48)(a

Ï€(s)Âµ(cid:48)(a

s)

s)

|

|

âˆ’

âˆ’

Ï€(cid:48)(s)

Ï€(s)
|

âˆ’

+

1)(cid:15)Âµ + (cid:15)Âµ

âˆ’

(cid:88)

Ï€(s)

s

(cid:1) = 2Îº(cid:48)(cid:15)Âµ,

|

s)

|
(cid:88)
a |

+

Ï€(s)Âµ(cid:48)(a

(cid:88)
s,a |

s)

|

âˆ’

Ï€(s)Âµ(a

s)
|

|

Âµ(cid:48)(a

s)

|

âˆ’

Âµ(a

s)

|

|

where the last inequality follows that
appendix shown by Ma et al., 2021).

(cid:107)

Ï€(cid:48)(s)

Ï€(s)
1
(cid:107)

âˆ’

â‰¤

2(Îº(cid:48)

âˆ’

1)(cid:15)Âµ (see proposition 2 in

Theorem 7. Let Âµ(cid:48) be the solution to the problem deï¬ned by Equation 19. We have

where (cid:15)g = maxs

Î¾(cid:48)

âˆ’
Eaâˆ¼Âµ(cid:48)[AÂµ
|

Î¾

g (Âµ(cid:48))
Âµ
â‰¥ L
g (s, a)]
|

2(Îº(cid:48)

1)(cid:15)g(cid:15)Âµ

12Î²(Îº(cid:48))2R2

max(cid:15)2
Âµ,

âˆ’

âˆ’

âˆ’
and Îº(cid:48) is Kemenyâ€™s constant under Âµ(cid:48).

Proof. Again, we start our analysis from MSVPDF. Based on Equation 11, we have

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’ = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ
= Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ
Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)
= Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

f (s, a)]
âˆ’
f (s, a) + 2Î²Î·âˆ’AÂµ
(cid:2)(r
Î·(cid:48))2
(r
âˆ’ âˆ’
âˆ’
âˆ’
2Î²Î·âˆ’(Î·(cid:48)
g (s, a)]

(cid:2)(r
Î· (s, a)]
âˆ’
(cid:3)
Î·)2
âˆ’
Î·)

âˆ’

âˆ’
where the last equation follows the diï¬€erence formula of average reward,

âˆ’

âˆ’

âˆ’

Î²Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

(cid:2)(r

Î·(cid:48))2

âˆ’ âˆ’

Î·(cid:48))2

(r
âˆ’
Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[2Î²Î·âˆ’AÂµ

âˆ’ âˆ’

âˆ’

Î·)2
âˆ’

(cid:3)

Î· (s, a)]

Î·(cid:48)

âˆ’

Î· = Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

Î· (s, a)].

The result indicates that the diï¬€erence can be separated into two parts: the improvement
by optimizing the surrogate problem (the ï¬rst term), and the discrepancy by the change of

13

Î·)2
âˆ’

(cid:3) ,

(r

âˆ’

(20)

Ma, Ma, Xia, & Zhao

Î· (the rest terms). The insight of our proof is to show that the ï¬rst term dominates the
diï¬€erence and the rest terms can be ignored in a tight trust region.

The ï¬rst term can be tackled with the standard trust region method. With the lower

bound of average trust region method in Equation 7, we have

Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)[AÂµ

g (s, a)]

g (Âµ(cid:48))
Âµ

âˆ’ L

2(Îº(cid:48)

â‰¥ âˆ’

1)(cid:15)g(cid:15)Âµ.

âˆ’

(21)

Now, we need to bound the rest terms. We have

(cid:2)(r

Î·(cid:48))2

2Î·âˆ’(Î·(cid:48)
Î·) + Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)
âˆ’ âˆ’
âˆ’
âˆ’
âˆ’
Î·)âˆ’(Î·(cid:48)
= Esâˆ¼Ï€,aâˆ¼Âµ[2(r
Î·)] + Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)
âˆ’
(cid:2)(r
Î·(cid:48))2
= Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)
(r
âˆ’ âˆ’
âˆ’
Î·)(cid:0)Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)(r
2(Î·(cid:48)

Î·)2
âˆ’
Î·)âˆ’

âˆ’ + 2(r

(r

âˆ’

âˆ’
Esâˆ¼Ï€,aâˆ¼Âµ(r

(cid:3)
Î·)2
âˆ’
(cid:2)(r
âˆ’
Î·)âˆ’(Î·(cid:48)

Î·(cid:48))2

âˆ’ âˆ’
Î·)(cid:3)
(cid:1).

âˆ’
Î·)âˆ’

âˆ’

(cid:3)

Î·)2
âˆ’

(r

âˆ’

Î·). Considering all potential cases for

âˆ’

âˆ’

âˆ’

Denote h := (r(cid:48)
âˆ’ âˆ’
the relationship between Î·, Î·(cid:48) and h, we have

Î·(cid:48))2

Î·)2

(r(cid:48)

âˆ’

âˆ’

âˆ’

âˆ’
âˆ’ + 2(r(cid:48)

âˆ’
Î·)âˆ’(Î·(cid:48)

â€¢ If r

max

Î·, Î·(cid:48)

â‰¥
{
â€¢ If r < min
Î·, Î·(cid:48)
{

}

, h = 0.

}
, h = (r

âˆ’
Î·(cid:48))2

âˆ’
(Î·(cid:48)

âˆ’
Î·)2.

â€¢ If Î·

â‰¤

r < Î·(cid:48), h = (r

â€¢ If Î·(cid:48)
(r

âˆ’
âˆ’
r < Î·, we denote c0 = r
Î·)(Î·(cid:48)

Î·(cid:48)
âˆ’
1 + 2c0c1
âˆ’
Synthesizing the above results, we conclude 0

â‰¤
Î·)2 + 2(r

Î·) = c2

âˆ’

âˆ’

âˆ’

â‰¤

â‰¥
â‰¤
h

Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)

(cid:2)(r

Î·(cid:48))2

âˆ’ âˆ’
With Lemma 1, we obtain that

âˆ’

(r

âˆ’

Î·(cid:48))2

(r

Î·)2 + 2(r

Î·)(Î·(cid:48)

âˆ’

âˆ’

Î·) = (Î·(cid:48)

Î·)2.

âˆ’

0 and c1 = Î·
(c0 + c1)2 = (Î·(cid:48)

r > 0. We have h =
Î·)2.

âˆ’
âˆ’
Î·)2. Thus we have

Î·)(cid:3)

âˆ’

(Î·(cid:48)

âˆ’

â‰¤

Î·)2.

(22)

(Î·(cid:48)

â‰¤

â‰¤
âˆ’ + 2(r

Î·)2

âˆ’
Î·)âˆ’(Î·(cid:48)

âˆ’

Î·(cid:48)
|

Î·

|

âˆ’

=

Ï(cid:48)r
|

âˆ’

Ïr

Ï(cid:48)

| â‰¤ (cid:107)

1Rmax
Ï
(cid:107)

âˆ’

â‰¤

2Îº(cid:48)(cid:15)ÂµRmax,

where the ï¬rst inequality follows the HÃ¶lderâ€™s inequality. Similarly, we have

Î·)âˆ’
Ï(r

âˆ’

Esâˆ¼Ï€,aâˆ¼Âµ(r
Î·)âˆ’

âˆ’

|

Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)(r
âˆ’
|
Ï(cid:48)(r
Î·)âˆ’
=
âˆ’
|
âˆ’
Ï(cid:48)
1Rmax
Ï
âˆ’
(cid:107)
2Îº(cid:48)(cid:15)ÂµRmax

â‰¤ (cid:107)

â‰¤

Î·)âˆ’

|

âˆ’

(23)

(24)

(25)

(26)

where Equation 25 comes from that 0
into Equation 22 and combining with Equation 21, we arrive at

Î·)âˆ’

(r

â‰¤

âˆ’

â‰¤

2Rmax. Substituting the previous results

Î¾(cid:48)
âˆ’ âˆ’

Î¾âˆ’

g (Âµ(cid:48))
Âµ
â‰¥ L
Î·(cid:48)
2Î²
|
g (Âµ(cid:48))
Âµ
â‰¥ L

âˆ’

âˆ’

âˆ’

âˆ’

2(Îº
Î²
1)(cid:15)g(cid:15)Âµ
âˆ’
âˆ’
(cid:12)
(cid:12)Esâˆ¼Ï€(cid:48),aâˆ¼Âµ(cid:48)(r
1)(cid:15)g(cid:15)Âµ

Î·
|
2(Îº(cid:48)

Î·)2

(Î·(cid:48)
âˆ’
|
Î·)âˆ’
âˆ’
12Î²(Îº(cid:48))2R2

âˆ’

|
Esâˆ¼Ï€,aâˆ¼Âµ(r
max(cid:15)2
Âµ.

âˆ’

âˆ’

Î·)âˆ’

(cid:12)
(cid:12)

âˆ’

14

MSV Policy Optimization via Risk-Averse RL

Algorithm 3 MSVPO
Input: Î±, Î², K, N, M
1: Initialize the policy with Î¸ and the value with Ï† randomly.
2: Set Ë†Î· = 0, Ë†Î·âˆ’ = 0, Ë†Î¶âˆ’ = 0.
3: for k = 1, 2,
4:

, K do

Â· Â· Â·

(1

â†

â†
â†

(cid:80)N âˆ’1

âˆ’
(1
(1

n=0 rn.
(cid:80)N âˆ’1
(cid:80)N âˆ’1

(sn, an, rn, sn+1)
}
{

Î±)Ë†Î· + Î± 1
N
Î±)Ë†Î·âˆ’ + Î± 1
N
Î±)Ë†Î¶âˆ’ + Î± 1
N

Execute policy ÂµÎ¸ for N times to collect
Ë†Î·
Ë†Î·âˆ’
n=0 (rn
Ë†Î¶âˆ’
n=0 (rn
Compute g(sn, an) with Equation 13 at all timesteps and Ë†g.
Compute Ë†Ag(sn, an) with Equation 17 at all timesteps.
Update the Î¸ with equation 27 for M epochs.
Update the Ï† with Equation 18 for M epochs.

Ë†Î·)âˆ’.
.
Ë†Î·)2
âˆ’

âˆ’
âˆ’

âˆ’
âˆ’

5:

6:

7:

8:

9:

10:

11:
12: end for

N âˆ’1
n=0

.

5.2.2 Implementation details

In the end of this subsection, we address some implementation issues of MSVTRPO. First of
all, in practice, we replace the TV-divergence with KL-divergence as most of trust region
methods do. Since DTV(p
q)/2, the theoretical results are still applicable
(cid:107)
for the practical algorithms.

DKL(p

(cid:112)

q)

â‰¤

(cid:107)

In the tabular case, where the state and action spaces are ï¬nite and discrete, it is enough
to parameterize the policy tabularly. The previous analysis of TRPO (Abdolmaleki et al.,
2018) shows that Equation 19 enjoys a closed form solution:

Âµ(cid:48)(

Â· |

s, a)

Âµ(

Â· |

âˆ

s, a) exp

(cid:18) AÂµ

g (s, a)
Ï…âˆ—

(cid:19)

,

where Ï…âˆ— can be obtained by solving the dual problem

min

Ï… L

(Ï…) := Ï…(cid:15)Âµ + Ï…

(cid:88)

s

Ï€(s) log

(cid:88)

a

Âµ (a

|

s) exp

(cid:18) AÂµ

g (s, a)
Ï…

(cid:19)

.

With a known MDP, we name this iterative procedure as MSV Trust Region Policy Iteration
(MSVTRPI). As aforementioned in Section 4, PI is not available for MSV. Nevertheless, we
can do MSVTRPI as an alternative. When (cid:15)Âµ
, it degrades to the standard PI without
the monotonic improvement guarantee.

â†’ âˆ

In the model-free case with large state and action spaces, we recommend solving the
surrogate loss proposed by PPO (Schulman et al., 2017), for its stable performance and fast
computing with neural networks. Formally, instead of optimizing the problem in Equation 19,
we maximizing the clipping objective

CLIP
Âµ

(Î¸) :=

L

1
N

N âˆ’1
(cid:88)

n=0

(cid:104)
min

(cid:16)

Ï‰n(Î¸) Ë†Ag(sn, an), clip(Ï‰n(Î¸), 1

âˆ’

Îµ, 1 + Îµ) Ë†Ag(sn, an)

(cid:17)(cid:105)

,

(27)

where Ï‰n(Î¸) = ÂµÎ¸(an|sn)
is the importance sampling ratio. Since we consider the long-run
Âµ(an|sn)
average performance in this paper, GAE is not applicable directly. Thus, we adopt the average

15

Ma, Ma, Xia, & Zhao

value constraint (AVC) proposed by (Ma et al., 2021) to stabilize the value learning. The
full algorithm, named by MSV Policy Optimization (MSVPO) is presented in Algorithm 3.

6. Experiments

In the previous sections, we analyze the properties of MSV problem and ï¬nd that it can
be soloved iteratively optimizing a surrogate reward function g (c.f. Equation 13). We also
propose two methods to solve the MSV problem in the parameterized policy space.

To validate the eï¬€ectiveness of our proposed methods in solving MSV problem, we conduct

a series of experiments to answer the corresponding questions:

â€¢ Is the MSV really optimized by the the surrogate reward function g? Speciï¬cally, what

is the diï¬€erence from optimizing g instead of f ?

â€¢ What is the diï¬€erence between the MV (Xia, 2020) and MSV criteria?

â€¢ Does the proposed algorithms work well with the current deep RL algorithms?

6.1 Bandit Problem

(a) Reward distributions.

(b) Polices paths.

Figure 2: The bandit problem. (a) Reward distributions in the bandit problem. (b) Polices
paths in the bandit problem. The paths are shown in the logarithmic parameter space.

We start from a simple bandit problem. In this problem, there are three actions with only
a single state. Diï¬€erent actions result in diï¬€erent rewards following the distributions shown
in Figure 2(a). Speciï¬cally, we have r0 sampled from a shifted LogNormal(0, 1) distribution,
of which the mean is shifted to zero. If we choice a1, we will obtain r1
N (0, 22). Otherwise,
we will have r2
N (1, 32). Obviously, we have three diï¬€erent risk preference actions. When
we ï¬x Î² = 1 in MV and MSV, the agent should always choice a0 if it optimizes the MSV
criterion, and choice a1 if it optimizes the MV criterion. The a2 has the highest outcome,
which is preferred by risk-neutral agents.

âˆ¼

âˆ¼

We compare three diï¬€erent agents, which optimize diï¬€erent reward functions. The ï¬rst
(cf. Equation 13), which is the derived reward
(cf. Equation 9),
(r

one optimizes g = (1 + 2Î·âˆ’)r
âˆ’
function with Î² = 1 in this work. The second one optimizes f = r

Î·)2
âˆ’

(r

âˆ’

Î·)2
âˆ’

âˆ’

âˆ’

16

âˆ’4âˆ’2024Reward0.00.20.40.6PDFr0r1r2âˆ’6âˆ’4âˆ’20logÂµ(a0)âˆ’4âˆ’20logÂµ(a1)(1+2Î·âˆ’)râˆ’(râˆ’Î·)2âˆ’râˆ’(râˆ’Î·)2âˆ’râˆ’(râˆ’Î·)2MSV Policy Optimization via Risk-Averse RL

Figure 3: Comparison of MSVTRPI and MVPI in the portfolio management problem. The
normalized MSV means Î² is doubled in comparison.

which is the Monte-Carlo return of MSV. We further consider a third agent which optimizes
Î·)2 (Xia, 2020), an MV objective to illustrate the diï¬€erence of MSV and MV problems.
r
All the agents use policy gradient with a parameterized policy initialized as a uniform one.

(r

âˆ’

âˆ’

To visualize the learning process, we plot the curves in the logarithmic parameter space,
as shown in Figure 2(b). Since (cid:80)
i Âµ(ai) = 1, Âµ(a2) is ignored in the ï¬gure. As expected,
the learning curve of the ï¬rst agent (blue solid curve) approaches (0,
), meaning that
it always chooses a0 ï¬nally. Similarly, the third agent (green dotted curve) also chooses a1
correspondingly. Interestingly, the second agent (red dashed curve), which optimizes the
Monte-Carlo return of MSV, ï¬nally converges to choose a2. The result tells us optimizing the
reward f = r
cannot optimize the MSV objective even in such a simple problem.
This reï¬‚ects the most essential diï¬€erence between the optimization of policy-dependent
reward and other problems. As discussed in Section 4, to optimize a problem with a policy-
dependent reward function, we must consider the perturbation of the mean, at least in MSV
problems.

Î·)2
âˆ’

âˆ’âˆ

Î²(r

âˆ’

âˆ’

6.2 Portfolio Management

In this part, we compare the performances of MSV- and MV-optimal polices in a portfolio
management problem. We need to manage two independent assets and cash. At the stage t,
the gain of the i-th asset is denoted by xi,t
, which transits according
âˆˆ {âˆ’
to a transition probability matrix (described in Appendix). The action space is deï¬ned
(cid:80)
, where wi,t is the weight of
as
âˆˆ {
}}
current portfolio on the i-th asset. Let w0,t = 1
w2,t denote the partition of cash
in current portfolio and x0 denote the return of cash. The reward function is deï¬ned as

0, 0.2, . . . , 1
w1,t

0.1, . . . , 0.5
}

(w1,t, w2,t)
{

i=1,2 wi,t

1, wi,t

0.2,

A

=

â‰¤

âˆ’

âˆ’

âˆ’

|

17

102030Î²0.100.15ValueMean102030Î²0.0050.010Variance102030Î²0.0020.0040.006Semivariance102030Î²0.000.050.100.15ValueMean-Var102030Î²0.050.100.15Mean-SemivarMVMSVMSV(normalized)Ma, Ma, Xia, & Zhao

Figure 4: Reward distribution in the portfolio management problem. The policy optimizing
MSV achieves Î· = 0.168, Î¶ = 0.014, Î¶âˆ’ = 0.006. As a comparison, the policy optimizing MV
achieves Î· = 0.073, Î¶ = 0.002, Î¶âˆ’ = 0.001.

âˆ’

(cid:80)

wi,t

wi,tâˆ’1

i=1,2 |

rt = w0,tx0 + (cid:80)
i=1,2 wi,txi,t
is deï¬ned as st = (x1,t, x2,t, w0,t, w1,t). Hence,

c, where c is the transition cost. The state
|
= 21 and
|A|
For the MSV, we optimize the policy with the MSV trust region policy iteration
(MSVTRPI) (see Section 5.2 for details), which aims to maximize Î¾Âµ
. We param-
eterize the policy in the softmax form as ÂµÎ¸(a
R|S||A| are
|
the â€œlogic valuesâ€. For MV, we optimize the policy with the mean-variance policy iteration
(MVPI) proposed by Xia (2020), which maximizes Î¾Âµ = Î·Âµ

Î²Î¶ Âµ
âˆ’
s) := softmax(Î¸(s, a)), where Î¸

= 1344.

âˆ’ = Î·Âµ

Î²Î¶ Âµ.

|S|

âˆ’

âˆ’

âˆˆ

We change the risk preference parameter Î² and compare the MSVTRPI and MVPI. We
depict the result in Figure 3, showing that with a ï¬xed Î², optimizing MSV always results
in a larger return than that of MV. Besides, MV is more sensitive than MSV in terms of
Î², meaning that a small change of Î² will lead to a quick drop in both the return and risk.
To better compare MSV and MV, we also show the â€œnormalizedâ€ results of MSV, where we
double Î² to provide the same penalty strength as MV. The result shows the normalized MSV
also outperforms MV in terms of the average reward, illustrating that MSV is more plausible
than MV. We demonstrate the reward distributions in Figure 4 with Î² = 10. It shows that
MSV maintains high returns while avoiding large losses. In contrast, optimizing MV may be
too conservative, as the upside rewards cause more volatility in this problem.

âˆ’

6.3 Robotic Control

To demonstrate the eï¬€ectiveness of our proposed method in more general problem setups,
we implement a â€œdeepâ€ variant algorithm named mean-semivariance policy optimization
(MSVPO), which is based on the recent developed method APO (Ma et al., 2021) for
average-reward RL problems.

We evaluate MSVPO in the continuous control benchmark MuJoCo (Todorov et al.,
2012) with OpenAI gym (Brockman et al., 2016) as the interface. Since the original setup of
MuJoCo is not suitable for the long-run average setting, we slightly modify the experimental
protocol. In most of MuJoCo tasks, the agent will be terminated if it reaches any unsafe

18

âˆ’0.2âˆ’0.10.00.10.20.30.40.50.000.020.040.060.080.10ProbabilityMeanMean-VarMean-SemivarMSV Policy Optimization via Risk-Averse RL

Figure 5: Training curves of Walker2d with noise. Each curve is averaged over 10 random
seeds and shaded by the standard deviation.

state, such as falling down. In that cases, we will reset the system and add an extra cost to
the terminal state. Diï¬€erent from other works focusing on the average episode returns, we
are interested in the long-run average and semivariance of the steady reward distribution.
To further increase the risk in the test scenarios, we add some noise to the agent outputs,
i.e., the real action taken by the environment is at + (cid:15), where (cid:15)
N (0, Ïƒ2). We call Ïƒ as the
noise level of the modiï¬ed MuJoCo tasks.

âˆ¼

We evaluate MSVPO with diï¬€erent Î²â€™s in the noisy Walker2d with diï¬€erent noise levels.
When the agent falls, we penalize it with an extra cost -10 and reset the system. As shown
in the Figure 5, the choice of diï¬€erent Î²â€™s achieves the trade-oï¬€ between the average and
semivariance. In the noiseless environment (noise level = 0), we interestingly ï¬nd that
risk-averse policy (Î² = 0.1) achieves competitive average reward with lower semivariance. It
indicates that in complex scenes, optimizing a risk-averse metric may generate more robust
policies with better performances comparing with a risk-neutral one.

To better understand the performance diï¬€erence with diï¬€erent risk preference polices, we
visualize the reward distributions of typical agents in Figure 6, where each agent of noise
level 0.1 is evaluated for 1000 steps. We can see that risk-averse polices successfully avoid
the unsafe states. Meanwhile, the agent uses smaller steps forward with the risk parameter
Î² increasing. Instead, the risk-neutral agent tends to take the risk of falling for larger gains.

19

0.02.55.07.510.0012345Reward AverageNoise Level=0.00.02.55.07.510.001234Noise Level=0.1=0.0=0.1=0.3=1.00.02.55.07.510.001234Noise Level=0.20.02.55.07.510.0Million Steps0123456Reward Semivariance0.02.55.07.510.0Million Steps01234560.02.55.07.510.0Million Steps0123456Ma, Ma, Xia, & Zhao

Figure 6: Reward distribution of Walker2d with noise.

7. Conclusion

This paper discusses how to optimize the mean-semivariance criterion for the steady reward
of MDPs and RL, which is an alternative risk measure of mean-variance. The semivariance
is a more reasonable measure than the variance in general scenarios, as it only penalizes
the downside risk. We utilize PA theory to derive the performance diï¬€erence formula and
optimize MSV with data-driven approaches. We develop two algorithms for MSV based on
PA theory, following the policy gradient theory and the trust region theory, respectively. We
also demonstrate the eï¬€ectiveness of the proposed algorithms in diï¬€erent problems, showing
the risk-averse performance of MSV policy. We point out that the application of the proposed
two-stage optimization framework for risk measures is not limited for MSV. We hope our
work can promote the applications of data-driven approaches in risk-sensitive environments
of MDPs and RL.

Acknowledgments

This work is funded by the National Natural Science Foundation of China (No. U1813216,
62192751, 61425027, 62073346, 11931018, U1811462), the National Key Research and Devel-
opment Project of China under Grant 2017YFC0704100 and Grant 2016YFB0901900, in part
by the 111 International Collaboration Program of China under Grant BP2018006, BNRist
Program (BNR2019TD01009), the National Innovation Center of High Speed Train R&D
project (CX/KJ-2020-0006), the Guangdong Province Key Laboratory of Computational
Science at the Sun Yat-Sen University (2020B1212060032) and the Guangdong Basic and
Applied Basic Research Foundation (2021A1515011984).

20

64202468Reward0.000.010.020.030.04Probability=0.0=0.1=0.3=1.0MSV Policy Optimization via Risk-Averse RL

Appendix A. Brief Review of Perturbation Analysis theory

|

Consider an ergodic MDP with transition matrix P (induced by some policy Âµ), where
s) is the transition probability from s to s(cid:48). We also consider a corresponding reward
P (s(cid:48)
function r, where r(s) is the reward expectation at s. We are interested in the average
performance Î· = Ï€r, where Ï€ denotes the steady state distribution. The Perturbation
Analysis (PA) theory (Cao, 2007) captures how the performance changes if the policy (or
system parameters P and r) has perturbations.

Theorem 8 (Performance diï¬€erence formula). For two ergodic MDPs with P and P (cid:48), we
have

Î· = Ï€[(P (cid:48)

Î·(cid:48)

âˆ’

âˆ’

P )V + r(cid:48)

r],

âˆ’

where V is the value function (called potential function in PA) for the system with P .

The value function satisï¬es the Poisson equation (I

identity matrix and e is the unit vector.

âˆ’

P )g + Î·e = r, where I denotes the

Theorem 9 (Performance derivative formula). Consider another MDP with P Î½ = P + âˆ†P =
(1

Î½)P + Î½P (cid:48) and rÎ½ = r + Î½âˆ†r = (1

Î½)r + Î½r(cid:48). We have

âˆ’

âˆ’

dÎ·
dÎ½

(cid:12)
(cid:12)
(cid:12)
(cid:12)Î½=0

= Ï€[(âˆ†P )V + âˆ†r].

Appendix B. Alternative Proof of MSVPG

This proof follows the similar derivation of Sutton and Barto (2018, Chapter 13). We
ï¬rst derive the policy gradient of Î¶âˆ’, and give the complete form of MSV gradient by

Î¸Î¾âˆ’ =

Î¸Î·

âˆ‡

Î²

âˆ‡

âˆ’

âˆ‡

Î¸Î¶âˆ’. Taking the gradient of V Âµ
Î¶âˆ’

for any arbitrary s

, we have

âˆˆ S

s)QÂµ
Î¶âˆ’

(s, a)

(cid:105)

Âµ(a

|

Î¸V Âµ
Î¶âˆ’

(s)
(cid:104) (cid:88)

âˆ‡
=

Î¸

âˆ‡
(cid:88)

a

(cid:104)

âˆ‡

âˆ‡

âˆ‡

a
(cid:88)

(cid:104)

a
(cid:88)

(cid:104)

a

=

=

=

Î¸Âµ(a

Î¸Âµ(a

Î¸Âµ(a

s)QÂµ
Î¶âˆ’

s)QÂµ
Î¶âˆ’

s)QÂµ
Î¶âˆ’

|

|

|

(s, a) + Âµ(a

(s, a) + Âµ(a

(s, a) + Âµ(a

|

|

|

Î¸QÂµ
Î¶âˆ’

s)

âˆ‡

(cid:105)
(s, a)

(cid:88)

P (cid:0)s(cid:48)

s, a(cid:1) (cid:0)(r

s)

âˆ‡

Î¸

(cid:88)

s)

s(cid:48)

|
s, a)(cid:0)

s(cid:48)
P (s(cid:48)

|

Î·)2

âˆ’ âˆ’

âˆ’

Î¶âˆ’ + V Âµ
Î¶âˆ’

(cid:0)s(cid:48)(cid:1)(cid:1) (cid:105)

2(r

Î·)âˆ’

Î¸Î·

âˆ‡

âˆ’

âˆ’

âˆ’ âˆ‡

Î¸Î¶âˆ’ +

(cid:0)s(cid:48)(cid:1)(cid:1) (cid:105)
.

Î¸V Âµ
Î¶âˆ’

âˆ‡

Rephrasing the equation above, we obtain

Î¸Î¶âˆ’ =
(cid:104)

âˆ‡
(cid:88)

Î¸Âµ(a

âˆ‡

a

s)QÂµ
Î¶âˆ’

|

(s, a) + Âµ(a

s)

|

(cid:88)

P (cid:0)s(cid:48)

s(cid:48)

s, a(cid:1) (cid:16)

|

Î¸V Âµ
Î¶âˆ’

(cid:0)s(cid:48)(cid:1)

âˆ‡

2(r

Î·)âˆ’

Î¸Î·

âˆ‡

âˆ’

âˆ’

(cid:17) (cid:105)

Î¸V Âµ
Î¶âˆ’

(s).

âˆ’ âˆ‡

21

Ma, Ma, Xia, & Zhao

Taking the expectation under Ï€ for both sides, we have

Ï€(s)

Î¸V Âµ
Î¶âˆ’

(s)

Î¸Î¶âˆ’
(cid:88)

âˆ‡
=

(cid:88)

(cid:104)

Ï€(s)

=

Ï€(s)

s
(cid:88)

âˆ’
s
(cid:88)

s
(cid:88)

âˆ’

s

âˆ‡
(cid:88)
a âˆ‡
(cid:88)

a

Ï€(s)

Î¸Âµ(a

âˆ‡

|

s)QÂµ
Î¶âˆ’

(s, a) + Âµ(a

s)

|

(cid:88)

P (cid:0)s(cid:48)

s(cid:48)

s, a(cid:1) (cid:0)

âˆ‡

|

Î¸V Âµ
Î¶âˆ’

(cid:0)s(cid:48)(cid:1)

2(r

Î·)âˆ’

âˆ‡

âˆ’

âˆ’

Î¸Î·(cid:1) (cid:105)

a

Î¸Âµ(a

s)QÂµ
Î¶âˆ’

|

(s, a) +

(cid:88)

(cid:88)

Ï€(s)

s(cid:48)

s

(cid:88)

a

Âµ(a

|

s)P (cid:0)s(cid:48)

s, a(cid:1)

Î¸V Âµ
Î¶âˆ’

âˆ‡

|

(cid:0)s(cid:48)(cid:1)

Âµ(a

s)

|

(cid:88)

s(cid:48)

2(r

Î·)âˆ’

Î¸Î·

âˆ‡

âˆ’

âˆ’

(cid:88)

s

Ï€(s)

Î¸V Âµ
Î¶âˆ’

âˆ‡

(s).

(28)

By the deï¬nitions of Ï€ and Î·âˆ’, we have

Ï€ (cid:0)s(cid:48)(cid:1) =

Î·âˆ’ =

(cid:88)

s
(cid:88)

s

Ï€(s)

Ï€(s)

(cid:88)

a
(cid:88)

a

Âµ(a

Âµ(a

|

|

s)P (cid:0)s(cid:48)

s, a(cid:1) ,

|

(cid:88)

(r

s)

s(cid:48)

Î·)âˆ’.

âˆ’

Substituting into the Equation 28, we have

Î¸V Âµ
Î¶âˆ’

(cid:0)s(cid:48)(cid:1)

âˆ‡

2Î·âˆ’

Î¸Î·

âˆ‡

âˆ’

âˆ’

(cid:88)

s

Ï€(s)

âˆ‡

Î¸V Âµ
Î¶âˆ’

(s)

Î¸Î¶âˆ’ =

âˆ‡

=

=

=

(cid:88)

s
(cid:88)

s
(cid:88)

s
(cid:88)

s

Ï€(s)

Ï€(s)

Ï€(s)

Ï€(s)

(cid:88)
a âˆ‡
(cid:88)
a âˆ‡
(cid:88)
a âˆ‡
(cid:88)
a âˆ‡

Î¸Âµ(a

Î¸Âµ(a

Î¸Âµ(a

Î¸Âµ(a

|

|

|

|

s)QÂµ
Î¶âˆ’

(s, a) +

(cid:88)

Ï€ (cid:0)s(cid:48)(cid:1)

s)QÂµ
Î¶âˆ’

(s, a)

s)QÂµ
Î¶âˆ’

(s, a)

âˆ’

âˆ’

(cid:104)
QÂµ
Î¶âˆ’

s)

(s, a)

âˆ’

s(cid:48)

2Î·âˆ’

2Î·âˆ’

Î¸Î·

âˆ‡
(cid:88)

s
2Î·âˆ’QÂµ

Î¸Âµ(a

|

s)QÂµ

Î· (s, a)

Ï€(s)

Î· (s, a)

(cid:88)
a âˆ‡
(cid:105)
.

Finally, applying the trick

log Âµ =

âˆ‡

âˆ‡

Âµ/Âµ, we have

Î¸Î¶âˆ’ = Esâˆ¼Ï€,aâˆ¼Âµ

âˆ‡

(cid:104)
QÂµ
Î¶âˆ’

(s, a)

âˆ’

2Î·âˆ’QÂµ

Î· (s, a)

(cid:105)
.

Thus, the MSVPG is given by

Î¸Î¾

âˆ‡

âˆ’

= Esâˆ¼Ï€,aâˆ¼Âµ

(cid:104)

(1 + 2Î·âˆ’)QÂµ

Î· (s, a)

Î²QÂµ
Î¶âˆ’

(cid:105)
(s, a)

.

âˆ’

Appendix C. Experiment Details

C.1 The Setup of Portfolio Management Problem

The return of cash x0 = 0.01. The transition cost c = 0.05.

22

MSV Policy Optimization via Risk-Averse RL

Table 1: The transition matrix of asset 1

x1
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.09
0.05
0.04
0.04
0.00
0.07
0.02
0.03

0.05
0.02
0.03
0.04
0.02
0.02
0.04
0.03

0.25
0.33
0.26
0.20
0.16
0.16
0.14
0.09

0.24
0.22
0.24
0.28
0.24
0.19
0.19
0.19

0.18
0.17
0.18
0.26
0.27
0.25
0.18
0.23

0.05
0.09
0.07
0.08
0.11
0.14
0.20
0.15

0.10
0.06
0.12
0.03
0.15
0.12
0.17
0.14

0.04
0.06
0.06
0.07
0.05
0.05
0.06
0.14

Table 2: The transition matrix of asset 2

x2
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.13
0.06
0.01
0.06
0.02
0.04
0.10
0.01

0.10
0.11
0.06
0.06
0.04
0.07
0.11
0.10

0.08
0.09
0.12
0.12
0.09
0.11
0.13
0.30

0.09
0.12
0.15
0.15
0.24
0.20
0.16
0.21

0.20
0.17
0.25
0.22
0.23
0.26
0.17
0.16

0.36
0.37
0.35
0.34
0.32
0.27
0.20
0.16

0.02
0.04
0.02
0.01
0.04
0.03
0.04
0.00

0.02
0.04
0.04
0.04
0.02
0.02
0.09
0.06

23

Ma, Ma, Xia, & Zhao

Appendix D. Hyper-parameters of MSVPO

Hyper-parameter

Network learning rate Î²
Network hidden sizes
Activation function
Optimizer
Batch size
Gradient Clipping
Clipping parameter Îµ
Optimization Epochs M
GAE parameter Î»
Average Value Constraint Coeï¬ƒcient in APO (Ma et al., 2021) Î½

Value

3e-4
[64, 64]
Tanh
Adam
256
10
0.2
10
0.95
0.3

Table 3: Hyper-parameters sheet

References

Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Riedmiller, M. A.
(2018). Maximum a posteriori policy optimisation. In International Conference on
Learning Representations.

Achiam, J., Held, D., Tamar, A., & Abbeel, P. (2017). Constrained policy optimization. In

International Conference on Machine Learning, Vol. 70, pp. 22â€“31.

Berner, C., Brockman, G., Chan, B., Cheung, V., DÄ™biak, P., Dennison, C., Farhi, D., Fischer,
Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement
learning. ArXiv preprint, abs/1912.06680.

Bisi, L., Sabbioni, L., Vittori, E., Papini, M., & Restelli, M. (2020). Risk-averse trust region
optimization for reward-volatility reduction. In International Joint Conference on
Artiï¬cial Intelligence, pp. 4583â€“4589.

Bollerslev, T., Li, S. Z., & Zhao, B. (2020). Good volatility, bad volatility, and the cross section
of stock returns. Journal of Financial and Quantitative Analysis, 55 (3), 751â€“781.
Borkar, V. S., & Meyn, S. P. (2002). Risk-sensitive optimal control for Markov decision
processes with monotone cost. Mathematics of Operations Research, 27 (1), 192â€“209.
Briec, W., & Kerstens, K. (2009). Multi-horizon markowitz portfolio performance appraisals:

A general approach. Omega, 37 (1), 50â€“62.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba,

W. (2016). OpenAI Gym. ArXiv preprint, abs/1606.01540.

Cao, X.-R. (2007). Stochastic Learning and Optimization: A Sensitivity-Based Approach.

Springer.

Castro, D. D., Tamar, A., & Mannor, S. (2012). Policy gradients with variance related risk

criteria. In International Conference on Machine Learning, pp. 1651â€“1658.

24

MSV Policy Optimization via Risk-Averse RL

Chen, W., Li, D., Lu, S., & Liu, W. (2019). Multi-period meanâ€“semivariance portfolio
optimization based on uncertain measure. Soft Computing, 23 (15), 6231â€“6247.
Choobineh, F., & Branting, D. (1986). A simple approximation for semivariance. European

Journal of Operational Research, 27 (3), 364â€“370.

Chow, Y., & Ghavamzadeh, M. (2014). Algorithms for CVaR optimization in MDPs. In

Advances in Neural Information Processing Systems, pp. 3509â€“3517.

Chow, Y., Ghavamzadeh, M., Janson, L., & Pavone, M. (2017). Risk-constrained reinforcement
learning with percentile risk criteria. Journal of Machine Learning Research, 18 (1),
6070â€“6120.

Chow, Y., Tamar, A., Mannor, S., & Pavone, M. (2015). Risk-sensitive and robust decision-
making: a CVaR optimization approach. In Advances in Neural Information Processing
Systems, pp. 1522â€“1530.

Chung, K.-J. (1994). Mean-variance tradeoï¬€s in an undiscounted MDP: the unichain case.

Operations Research, 42 (1), 184â€“188.

Delage, E., Kuhn, D., & Wiesemann, W. (2019). â€œDiceâ€-sionâ€“making under uncertainty:

When can a random decision reduce risk?. Management Science, 65 (7), 3282â€“3301.

Dulac-Arnold, G., Mankowitz, D., & Hester, T. (2019). Challenges of real-world reinforcement

learning. ArXiv preprint, abs/1904.12901.

Estrada, J. (2007). Mean-semivariance behavior: Downside risk and capital asset pricing.

International Review of Economics & Finance, 16 (2), 169â€“185.

Fei, Y., Yang, Z., Chen, Y., Wang, Z., & Xie, Q. (2020). Risk-sensitive reinforcement
learning: Near-optimal risk-sample tradeoï¬€ in regret. In Advances in Neural Information
Processing Systems, Vol. 33, pp. 22384â€“22395.

Filar, J. A., Kallenberg, L. C., & Lee, H.-M. (1989). Variance-penalized Markov decision

processes. Mathematics of Operations Research, 14 (1), 147â€“161.

GarcÄ±a, J., & FernÃ¡ndez, F. (2015). A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research, 16 (1), 1437â€“1480.

Gosavi, A. (2014). Variance-penalized Markov decision processes: Dynamic programming and
reinforcement learning techniques. International Journal of General Systems, 43 (6),
649â€“669.

Hogan, W. W., & Warren, J. M. (1974). Toward the development of an equilibrium capital-
market model based on semivariance. Journal of Financial and Quantitative Analysis,
9 (1), 1â€“11.

Howard, R. A., & Matheson, J. E. (1972). Risk-sensitive Markov decision processes. Man-

agement science, 18 (7), 356â€“369.

Janner, M., Fu, J., Zhang, M., & Levine, S. (2019). When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems, pp.
12498â€“12509.

Li, D., & Ng, W.-L. (2000). Optimal dynamic portfolio selection: Multiperiod mean-variance

formulation. Mathematical Finance, 10 (3), 387â€“406.

25

Ma, Ma, Xia, & Zhao

Liu, Y.-J., & Zhang, W.-G. (2015). A multi-period fuzzy portfolio optimization model with
minimum transaction lots. European Journal of Operational Research, 242 (3), 933â€“941.

Ma, S., Ma, X., & Xia, L. (2022a). An optimistic value iteration for meanâ€“variance optimiza-
tion in discounted markov decision processes. Results in Control and Optimization, 8,
100165.

Ma, S., Ma, X., & Xia, L. (2022b). A uniï¬ed algorithm framework for mean-variance
optimization in discounted Markov decision processes. ArXiv preprint, abs/2201.05737.

Ma, X., Tang, X., Xia, L., Yang, J., & Zhao, Q. (2021). Average-reward reinforcement
learning with trust region methods. In International Joint Conference on Artiï¬cial
Intelligence, pp. 2797â€“2803.

Ma, X., Xia, L., Zhou, Z., Yang, J., & Zhao, Q. (2020). Dsac: distributional soft actor critic

for risk-sensitive reinforcement learning. ArXiv preprint, abs/2004.14547.

Markowitz, H., Todd, P., Xu, G., & Yamane, Y. (1993). Computation of mean-semivariance
eï¬ƒcient sets by the critical line algorithm. Annals of Operations Research, 45 (1),
307â€“317.

Markowitz, H. M. (1952). Portfolio selection. Journal of Finance, 7, 77â€“91.

Markowitz, H. M. (1959). Portfolio Selection: Eï¬ƒcient Diversiï¬cation of Investments. John

Wiley & Sons, New York.

Mavrin, B., Yao, H., Kong, L., Wu, K., & Yu, Y. (2019). Distributional reinforcement learning
for eï¬ƒcient exploration. In International Conference on Machine Learning, Vol. 97, pp.
4424â€“4434.

Nagabandi, A., Konolige, K., Levine, S., & Kumar, V. (2020). Deep dynamics models for

learning dexterous manipulation. In Conference on Robot Learning, pp. 1101â€“1112.

Nemirovski, A., & Shapiro, A. (2007). Convex approximations of chance constrained programs.

SIAM Journal on Optimization, 17 (4), 969â€“996.

Prashanth, L., & Ghavamzadeh, M. (2016). Variance-constrained actor-critic algorithms for

discounted and average reward MDPs. Machine Learning, 105 (3), 367â€“417.

RuszczyÅ„ski, A. (2010). Risk-averse dynamic programming for Markov decision processes.

Mathematical programming, 125 (2), 235â€“261.

Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., & Moritz, P. (2015). Trust region policy
optimization. In International Conference on Machine Learning, Vol. 37, pp. 1889â€“1897.

Schulman, J., Moritz, P., Levine, S., Jordan, M. I., & Abbeel, P. (2016). High-dimensional
continuous control using generalized advantage estimation. In International Conference
on Learning Representations.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy

optimization algorithms. ArXiv preprint, abs/1707.06347.

Shapiro, A., Dentcheva, D., & Ruszczynski, A. (2021). Lectures on stochastic programming:

modeling and theory. SIAM.

26

MSV Policy Optimization via Risk-Averse RL

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human
knowledge. Nature, 550 (7676), 354â€“359.

Sobel, M. J. (1982). The variance of discounted Markov decision processes. Journal of

Applied Probability, 19 (4), 794â€“802.

Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT press.
Tamar, A., Chow, Y., Ghavamzadeh, M., & Mannor, S. (2016). Sequential decision making
with coherent risk. IEEE Transactions on Automatic Control, 62 (7), 3323â€“3338.
Tamar, A., Glassner, Y., & Mannor, S. (2015). Optimizing the CVaR via sampling. In
Proceedings of the Twenty-Ninth AAAI Conference on Artiï¬cial Intelligence, pp. 2993â€“
2999.

Todorov, E., Erez, T., & Tassa, Y. (2012). Mujoco: A physics engine for model-based
control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.
5026â€“5033.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,
D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft
ii using multi-agent reinforcement learning. Nature, 575 (7782), 350â€“354.

Wei, Q. (2019). Meanâ€“semivariance optimality for continuous-time Markov decision processes.

Systems & Control Letters, 125, 67â€“74.

Xia, L. (2016). Optimization of Markov decision processes under the variance criterion.

Automatica, 73, 269â€“278.

Xia, L. (2020). Risk-sensitive Markov decision processes with combined metrics of mean and

variance. Production and Operations Management, 29 (12), 2808â€“2827.

Xie, T., Liu, B., Xu, Y., Ghavamzadeh, M., Chow, Y., Lyu, D., & Yoon, D. (2018). A block
coordinate ascent algorithm for mean-variance optimization. In Advances in Neural
Information Processing Systems, Vol. 31, pp. 1073â€“1083.

Yan, W., Miao, R., & Li, S. (2007). Multi-period semi-variance portfolio selection: Model
and numerical solution. Applied Mathematics and Computation, 194 (1), 128â€“134.
Zhang, S., Liu, B., & Whiteson, S. (2021). Mean-variance policy iteration for risk-averse
reinforcement learning. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence,
Vol. 35, pp. 10905â€“10913.

Zhang, W.-G., Liu, Y.-J., & Xu, W.-J. (2012). A possibilistic mean-semivariance-entropy
model for multi-period portfolio selection with transaction costs. European Journal of
Operational Research, 222 (2), 341â€“349.

Zhang, Y., & Ross, K. W. (2021). On-policy deep reinforcement learning for the average-
In International Conference on Machine Learning, Vol. 139, pp.

reward criterion.
12535â€“12545.

Zhou, F., Wang, J., & Feng, X. (2020). Non-crossing quantile regression for distributional
reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 33,
pp. 15909â€“15919.

27

