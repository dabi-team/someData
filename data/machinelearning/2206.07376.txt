2
2
0
2

t
c
O
6
1

]

G
L
.
s
c
[

2
v
6
7
3
7
0
.
6
0
2
2
:
v
i
X
r
a

MSV Policy Optimization via Risk-Averse RL

Mean-Semivariance Policy Optimization via Risk-Averse
Reinforcement Learning

Xiaoteng Ma
Department of Automation, Tsinghua University,
Beijing, 100086, P. R. China

Shuai Ma
School of Business, Sun Yat-sen University,
Guangzhou, 510275, P. R. China

Li Xia
(Corresponding author)
School of Business, Sun Yat-sen University,
Guangzhou, 510275, P. R. China

Qianchuan Zhao
Department of Automation, Tsinghua University,
Beijing, 100086, P. R. China

ma-xt17@mails.tsinghua.edu.cn

mash35@mail.sysu.edu.cn

xiali5@sysu.edu.cn

zhaoqc@tsinghua.edu.cn

Abstract

Keeping risk under control is often more crucial than maximizing expected reward in
real-world decision-making situations, such as Ô¨Ånance, robotics, autonomous driving, etc.
The most natural choice of risk measures is variance, while it penalizes the upside volatility
as much as the downside part. Instead, the (downside) semivariance, which captures negative
deviation of a random variable under its mean, is more suitable for risk-averse proposes.
This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement
learning w.r.t. steady reward distribution. Since semivariance is time-inconsistent and
does not satisfy the standard Bellman equation, the traditional dynamic programming
methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to
Perturbation Analysis (PA) theory and establish the performance diÔ¨Äerence formula for
MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of
RL problems with a policy-dependent reward function. Further, we propose two on-policy
algorithms based on the policy gradient theory and the trust region method. Finally, we
conduct diverse experiments from simple bandit problems to continuous control tasks in
MuJoCo, which demonstrate the eÔ¨Äectiveness of our proposed methods.

1. Introduction

Reinforcement learning (RL) has shown great promise in solving complex decision problems,
such as Go (Silver et al., 2017), video games (Berner et al., 2019; Vinyals et al., 2019)
and dexterous robotic control (Nagabandi et al., 2020). Learning by trial and error, RL
enables an agent to maximize its accumulated expected rewards through the interaction
with a simulator. However, RL deployment in real-world scenarios is still challenging and
unreliable (Garcƒ±a & Fern√°ndez, 2015; Dulac-Arnold et al., 2019). One of the reasons is
that real decision-makers need to consider multi-objective functions. The desired policy

1

 
 
 
 
 
 
Ma, Ma, Xia, & Zhao

Figure 1: A toy example illustrates the eÔ¨Äect of MSV. We refer the policy going left as l and
the other as r. Two policies have the same average return Œ∑l = Œ∑r = 0 and the same variance
Œ∂ l = Œ∂ r = 2. However, since the semivariance Œ∂ l
‚àí = 2/3, the policy going right
has a smaller (downside) semivariance. It shows that MSV enables to avoid extreme costs
compared with MV.

‚àí = 4/3 > Œ∂ r

should perform well for broader metrics, not just for expectation. That raises the demand of
risk-sensitive learning, which aims at balancing the return and risk in face of uncertainty.

The risk-sensitive decision-making has been widely studied beyond the scope of RL,
which can be traced back to the mean-variance (MV) optimization theory established by
Markowitz (Markowitz, 1952). Variance, which captures the Ô¨Çuctuation and concentration of
random variables, is a natural choice of the risk measure. As Markowitz only considers the
single-period problem, many studies focus on extending the results to multi-period scenarios,
from stochastic control (Li & Ng, 2000) to Markov decision process (Sobel, 1982; Filar
et al., 1989). However, the variance of a multi-period problem depends on the average
value of the whole process. It breaks the essential property of dynamic programming‚Äîtime-
consistency, and makes it hard to design model-free learning algorithms under the standard
RL framework. Developing an eÔ¨Écient algorithm to optimize MV is still an ongoing topic
in the RL community (Xie et al., 2018; Bisi et al., 2020; Xia, 2020; Zhang et al., 2021; Ma
et al., 2022b, 2022a).

While MV analysis is the most widely applied risk-return analysis in practice, variance
metric is questionable as a risk measure. As a measure of volatility, variance penalizes
upside deviations from the mean as much as downside deviations. It could be problematic
as the upside deviation comes from the higher return which is desirable. In general, the
outcome distributions in the real world are often asymmetrical, such as the ones in the stock
market (Estrada, 2007; Bollerslev et al., 2020), suggesting that we should control the ‚Äúgood‚Äù
and ‚Äúbad‚Äù volatility separately. Hence, Markowitz (1959) presents the mean-semivariance
(MSV) as an alternative measure, which only penalizes the ‚Äúbad‚Äù volatility, performing as
a downside risk indicator. Even if the distribution is symmetrical, optimizing MSV is at
least eÔ¨Äective as optimizing MV. To better illustrate the diÔ¨Äerence between variance and
semivariance, we construct a simple MDP example shown in Figure 1. The two policies
result in two reward distributions symmetrically, for which variances are indistinguishable.
However, the policy going right is preferred since it results in a lower semivariance.

Though MSV is a more plausible measure of risk, optimizing MSV is even more com-
plicated than MV. It inherits time-inconsistency from variance and introduces a truncation

2

Dept. of Automation, Tsinghua University411‚àí1‚àí12‚àí2ùë†1ùë†2ùë†0ùë†3ùë†4MSV Policy Optimization via Risk-Averse RL

Algorithm 1 The framework of MSV optimization

Initialize policy as ¬µ
repeat

Evaluate ¬µ and get Œ∑ (cf. Equation 1) and Œ∑‚àí (cf. Equation 12)
Set reward function as g = (1 + 2Œ≤Œ∑‚àí)r
¬µ

POLICY_UPDATE(¬µ, g)

Œ∑)2
‚àí

Œ≤(r

‚àí

‚àí

‚Üê

until ¬µ converges

function of mean, making the analysis non-trivial. Due to the complexity of this objective,
existing works consider a subset of problems restricted with a Ô¨Åxed mean (Wei, 2019) or
heuristic algorithms for MSV (Yan et al., 2007; Zhang et al., 2012; Liu & Zhang, 2015; Chen
et al., 2019). To the best of our knowledge, there are currently no relevant studies on MSV
in the RL literature.

In this paper, we aim to Ô¨Åll the gap of the previous study on the single-period MSV
problem and extend the static methods to online RL algorithms. To achieve that, we resort to
Perturbation Analysis (PA) theory (Cao, 2007) (also called the sensitivity-based optimization
theory or the relative optimization theory) for Markov systems, which lays the basis of many
eÔ¨Écient RL methods, such as TRPO (Schulman et al., 2015), CPO (Achiam et al., 2017)
and MBPO (Janner et al., 2019). The contributions of our work are threefold. Firstly,
instead of constructing a Bellman operator, we establish the MSV performance diÔ¨Äerence
formula of two policies (see Section 4 for details). The result indicates that the performance
diÔ¨Äerence can be decomposed into two parts: the improvement corresponding to a reward
function depending on the current policy and the average performance change from the
current to the updated one. Second, we iteratively optimize MSV by considering the shift
in mean locally and constructing a surrogate reward function. The framework is shown in
Algorithm 1. Under this framework, we develop two algorithms based on the policy gradient
theory and the trust region method, respectively. We show that optimizing the surrogate
reward function in the trust region has a similar performance lower bound with the standard
TRPO, which guarantees the monotonic improvement if the trust region is tight. Finally, we
conduct diverse experiments to examine the eÔ¨Äectiveness of our proposed methods, including
a bandit problem, a tabular portfolio management problem, and robotic control tasks based
on MuJoCo. The results demonstrate that the proposed algorithms successfully improve the
performance under the criterion of MSV, which is better than standard RL from a risk-averse
perspective.

2. Related Work

Below we brieÔ¨Çy review the literature about optimization of MSV and other risk measures.

2.1 Mean-Semivariance

MSV is Ô¨Årst introduced by Markowitz (1959) as an alternative to MV. Thereafter, many
researchers study portfolio selection problems by employing the semivariance as the risk
measure (Markowitz et al., 1993; Hogan & Warren, 1974; Choobineh & Branting, 1986;
Briec & Kerstens, 2009), most of which are limited to the single-period problem. Due

3

Ma, Ma, Xia, & Zhao

to the complexity of MSV, previous studies on MSV in multi-period problems resort to
heuristic methods, such as fuzzy systems and genetic algorithms (Yan et al., 2007; Zhang
et al., 2012; Liu & Zhang, 2015; Chen et al., 2019). Wei (2019) studies a special case of
MSV in the continuous-time MDP, where the mean of the discounted total cost is equal to
a given function. Another stream of researches (Tamar et al., 2016; Shapiro et al., 2021)
study semideviation instead of semivariance. As standard deviation is an alternative to
variance, semideviation is considered as an alternative to semivariance. The main beneÔ¨Åt of
mean-semideviation (MSD) is that it satisÔ¨Åes the property ‚Äúcoherent,‚Äù and hence it can be
written in a Bellman form (Ruszczy≈Ñski, 2010). However, the additional square operation
makes optimizing MSD with a data-driven approach non-trivial. We leave the optimization of
MSD in RL as future work. Furthermore, maximizing the upside semivariance could improve
the exploration ability (Mavrin et al., 2019; Ma et al., 2020; Zhou et al., 2020), showing the
potential of MSV from an opposite perspective.

2.2 Mean-Variance

Since MSV is highly related to MV, in this part, we summarize the works on MV in Markov
decision processes (MDPs) and RL. Based on the deÔ¨Ånition of variance in the framework
of MDPs, the existing studies on variance can be broadly divided into two categories. One
stream of works (Sobel, 1982; Castro et al., 2012; Prashanth & Ghavamzadeh, 2016; Xie
et al., 2018) concern the variance of total return R = (cid:80)‚àû
t=0 Œ≥trt under the initial state
distribution, i.e., VœÄ0(R) where Œ≥ is the discount factor, œÄ0 is the initial state distribution
and rt is the reward at the stage t. This deÔ¨Ånition concerns the risk of total rewards at the
Ô¨Ånal stage, while we are more concerned about long-term volatility in practical problems.
Hence, the long-run variance (Filar et al., 1989; Chung, 1994; Gosavi, 2014; Xia, 2016, 2020;
Bisi et al., 2020; Zhang et al., 2021), also known as the steady-state variance, is proposed to
describe the variance of the steady reward distribution. The long-run variance is deÔ¨Åned by
(cf. Equation 3), where Œ∑¬µ is the long-run average
limT ‚Üí‚àû
of policy ¬µ. Since the average reward Œ∑¬µ depends on the current policy, it breaks the
time-consistency. To handle this problem, Xia (2016, 2020) derives a variance performance
diÔ¨Äerence formula with PA and proposes a policy iteration algorithm which is guaranteed to
converge to a local optimum. In this paper, we adopt similar deÔ¨Ånition of Xia‚Äôs work and
extend the formulation from MV to MSV.

t=0 (r(st, at)

(cid:104)(cid:80)T ‚àí1

Œ∑¬µ)2(cid:105)

EœÄ0,¬µ

‚àí

1
T

2.3 Other Risk Measures

Besides the MV and MSV, other risk measures capture diÔ¨Äerent features of the return
distribution. A classical risk measure in optimal control is exponential utility (Howard &
Matheson, 1972; Borkar & Meyn, 2002; Fei et al., 2020). The exponential utility enjoys a
product form of the Bellman equation. Therefore the corresponding value-based algorithms
such as Q-learning are well-developed. While the exponential Bellman equation is elegant in
theory, it poses some computational problems as the exponential values are often too large to
be numerically calculated. Another famous risk measure is Conditional Value at Risk (CVaR),
deÔ¨Åned as the average value under the Œ±-quantile. Many existing methods (Nemirovski &
Shapiro, 2007; Chow & Ghavamzadeh, 2014; Tamar et al., 2015; Chow et al., 2015, 2017)
optimize CVaR as the objective or constraints. The main diÔ¨Äerence between CVaR and

4

MSV Policy Optimization via Risk-Averse RL

MSV is that CVaR puts even weights for the events under a certain threshold, while the
importance of the extreme values on the concerned side increases quadratically in MSV. We
refer to Delage et al.‚Äôs work (2019) for more discussion on the connection of diÔ¨Äerent risk
measures.

3. Preliminaries

S

In this paper, we focus on the inÔ¨Ånite-horizon discrete-time MDP as
where
denotes the state space,
denotes a bounded reward function and P :
œÄ0
S
are ergodic. Let ¬µ :
S (cid:55)‚Üí
randomized policy space.

,
,
, r, P, œÄ0
(cid:105)
A
(cid:104)S
Rmax, Rmax]
[
‚àí
) is the transition matrix and
S √ó A (cid:55)‚Üí
S
) denotes the initial state distribution. We assume that all the involved MDPs
) denote a Markovian randomized policy and Œ† denote the

denotes the action space, r :

M
S √ó A (cid:55)‚Üí

‚àÜ(

‚àÜ(

‚àÜ(

A

A

=

‚àà

We are interested in the long-run average reward

Œ∑¬µ := lim
T ‚Üí‚àû

1
T

EœÄ0,¬µ

(cid:35)

r(st, at)

,

(cid:34)T ‚àí1
(cid:88)

t=0

(1)

where EœÄ0,¬µ stands for the expectation with s0
that Œ∑¬µ is independent of œÄ0 when T
is convenient to rephrase the long-run average reward as

œÄ0, at

‚Üí ‚àû

st, at). Note
st), st+1
. With œÄ denoting the steady state distribution, it

P (

¬µ(

¬∑ |

¬∑ |

‚àº

‚àº

‚àº

The variance and semivariance w.r.t. ¬µ are deÔ¨Åned by

Œ∑¬µ := Es‚àºœÄ,a‚àº¬µ [r(s, a)] .

Œ∂ ¬µ := lim
T ‚Üí‚àû

Œ∂ ¬µ
‚àí := lim
T ‚Üí‚àû

1
T

1
T

EœÄ0,¬µ

EœÄ0,¬µ

(cid:34)T ‚àí1
(cid:88)

t=0
(cid:34)T ‚àí1
(cid:88)

t=0

(r(st, at)

(r(st, at)

(cid:35)

Œ∑¬µ)2

,

(cid:35)

Œ∑¬µ)2
‚àí

,

‚àí

‚àí

(2)

(3)

(4)

where (
)‚àí := min
{
¬∑

0,

. In this paper, we focus on the mean-semivariance criterion,

¬∑}

Œæ¬µ
‚àí := Œ∑¬µ

Œ≤Œ∂ ¬µ
‚àí,

‚àí

where Œ≤
when mean-variance criterion is mentioned, we mean Œæ¬µ := Œ∑¬µ

0 is the parameter for the trade-oÔ¨Ä between mean and semivariance. Analogously,

Œ≤Œ∂ ¬µ.

‚â•

We further respectively deÔ¨Åne state-value function, action-value function, and advantage

‚àí

function for average reward as

V ¬µ
Œ∑ (s) := E¬µ

(cid:34) ‚àû
(cid:88)

(r(st, at)

Q¬µ

Œ∑ (s, a) := E¬µ

t=0
(cid:34) ‚àû
(cid:88)

(r(st, at)

(cid:35)

s0 = s

,

(cid:35)

s0 = s, a0 = a

,

Œ∑¬µ)

Œ∑¬µ)

|

|

‚àí

‚àí

A¬µ

Œ∑ (s, a) := Q¬µ

t=0
Œ∑ (s, a)

V ¬µ
Œ∑ (s).

‚àí

5

Ma, Ma, Xia, & Zhao

Similarly, the value functions for semivariance are deÔ¨Åned as

V ¬µ
Œ∂‚àí

(s) := E¬µ

Q¬µ
Œ∂‚àí

(s, a) := E¬µ

(cid:34) ‚àû
(cid:88)

t=0
(cid:34) ‚àû
(cid:88)

A¬µ
Œ∂‚àí

(s, a) := Q¬µ
Œ∂‚àí

t=0
(s, a)

(cid:0)(r(st, at)

Œ∑¬µ)2

‚àí ‚àí

‚àí

(cid:1)

Œ∂ ¬µ
‚àí

(cid:0)(r(st, at)

Œ∑¬µ)2

‚àí ‚àí

‚àí

(cid:1)

Œ∂ ¬µ
‚àí

(cid:35)

s0 = s

,

(cid:35)

s0 = s, a0 = a

,

|

|

V ¬µ
Œ∂‚àí

(s).

‚àí

For notation simplicity, we will omit the superscript ‚Äú¬µ‚Äù when the context is clear, e.g.,
the average rewards Œ∑¬µ, Œ∑¬µ(cid:48) are written as Œ∑, Œ∑(cid:48) instead. When r is mentioned, we omit (s, a)
and use r in short.

Before our analysis of MSV, we brieÔ¨Çy review the average-reward policy gradient theorem

and the trust region theorem.

Theorem 1 (Average-Reward Policy Gradient by Sutton & Barto, 2018). For a policy ¬µ
parameterized by Œ∏, we have

Œ∏Œ∑ = Es‚àºœÄ,a‚àº¬µ[

‚àá

Œ∏ log ¬µ(a

|

s)A¬µ

Œ∑ (s, a)].

‚àá

Theorem 2 (Average-Reward Trust Region Policy Optimization by Zhang & Ross, 2021;
Ma et al., 2021). Consider the following problem,

max

¬µ(¬µŒ∏),

¬µŒ∏ L
s.t. Es‚àºœÄDTV(¬µŒ∏(

s)

¬µ(

¬∑ |

(cid:107)

¬∑ |

s))

‚â§

(cid:15)¬µ,

where

¬µ(¬µŒ∏) := Es‚àºœÄ,a‚àº¬µŒ∏

(cid:2)A¬µ

Œ∑ (s, a)(cid:3) .

L

Denote ¬µ(cid:48) as the solution of the above problem. The following bound holds:

Œ∑(cid:48)

‚àí

where (cid:15)Œ∑ = maxs

Ea‚àº¬µ(cid:48)[A¬µ
|

Œ∑ (s, a)]
|

4. Perturbation Analysis

Œ∑

¬µ(¬µ(cid:48))

2(Œ∫(cid:48)

1)(cid:15)Œ∑(cid:15)¬µ,

‚àí

‚â• L

‚àí
and Œ∫(cid:48) is Kemeny‚Äôs constant under ¬µ(cid:48).

(5)

(6)

(7)

In this section, we derive the MSV performance diÔ¨Äerence formula (MSVPDF), where the
core concept‚Äîperformance diÔ¨Äerence formula‚Äîcomes from the PA for Markov systems, also
called the sensitivity-based optimization theory. With the aid of MSVPDF, we obtain the
necessary optimality condition for the MSV problem. It also lays the basis for developing
optimization algorithms (see Section 5), such as the policy gradient method and the trust
region method. For readers unfamiliar with PA, we provide a brief review of the theory in
Appendix A.

6

MSV Policy Optimization via Risk-Averse RL

4.1 Performance DiÔ¨Äerence Formula

MSVPDF is formally stated as below.

Theorem 3. For any two policies ¬µ, ¬µ(cid:48)

Œ†, we have

‚àà
Œ≤A¬µ
Œ∂‚àí

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

Œ∑ (s, a)

‚àí

(s, a)]

‚àí

Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[(r

Œ∑(cid:48))2

‚àí ‚àí

‚àí

(r

‚àí

Œ∑)2
‚àí]

(8)

Proof. To decompose the policy performance with the policy-dependent reward, we Ô¨Årst
introduce a pseudo mean Œª. We analyze the policy diÔ¨Äerence with the pseudo mean and
corresponding pseudo reward function, and then turn into the true mean by letting Œª = Œ∑.
With a pseudo mean Œª, we transform the original problem into a standard MDP with

reward function

We obtain a pseudo mean-semivariance objective by optimizing this pseudo reward-

f (s, a) := r

Œ≤(r

‚àí

‚àí

Œª)2
‚àí.

(9)

function,

By deÔ¨Ånition, we have

ŒæŒª,‚àí := Œæ¬µ

Œª,‚àí = Es‚àºœÄ,a‚àº¬µ [f (s, a)] .

Œæ‚àí

‚àí

ŒæŒª,‚àí = Es‚àºœÄ,a‚àº¬µ

(cid:2)r

Œ≤(r

Œ∑)2

‚àí ‚àí

‚àí

‚àí

f (s, a)(cid:3) .

Since the pseudo reward is independent of the policy, we can write its performance diÔ¨Äerence
formula directly (Cao, 2007, Chapter 2):

Œæ(cid:48)
Œª,‚àí ‚àí

ŒæŒª,‚àí = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

f (s, a)],

(10)

where A¬µ
Equation 10, we can derive the performance diÔ¨Äerence formula of Œæ‚àí as

f (s, a) is the pseudo advantage with f as the reward function. With the aid of

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí = (Œæ(cid:48)

Œª,‚àí ‚àí
= Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ
Œ≤Es‚àºœÄ,a‚àº¬µ

ŒæŒª,‚àí) + (Œæ(cid:48)
f (s, a)]
(cid:2)(r

Œæ(cid:48)
Œª,‚àí) + (ŒæŒª,‚àí
Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)
Œ∑)2
‚àí

‚àí ‚àí
‚àí
Œª)2
‚àí ‚àí

(r

‚àí

‚àí
(cid:2)(r
(cid:3) .

‚àí

‚àí

Œæ‚àí)

Œ∑(cid:48))2

‚àí ‚àí

‚àí

(r

‚àí

(cid:3)

Œª)2
‚àí

Finally, by setting Œª = Œ∑, we arrive at

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

f (s, a)]

Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

(cid:2)(r

Œ∑(cid:48))2

‚àí ‚àí

‚àí

(r

‚àí

Œ∑)2
‚àí

(cid:3) ,

‚àí

which is the same as Equation 8 if we explicitly calculate the advantage function with reward
function f and Œª = Œ∑.

The MSVPDF in Equation 8 or Equation 11 claims that the MSV improvement can be
separated into two parts. The Ô¨Årst term in Equation 11 is a standard MDP with f as the
reward function, and the second term is caused by the perturbation of the mean. It clearly
quantiÔ¨Åes the diÔ¨Éculty of solving the MSV problem, i.e., the policy-dependent reward function
breaks down the time-consistent nature of MDPs. Meanwhile, it also shows us the standard
MDP algorithm such as policy iteration (PI) is unavailable. A PI-like algorithm may be
eÔ¨Écient in improving the Ô¨Årst term, but the sign of the remaining term (dependent on Œ∑(cid:48)) is
unpredictable. It suggests that we need novel tools to guarantee the policy improvement.

7

Ma, Ma, Xia, & Zhao

4.2 Performance Derivative Formula

While Equation 11 describes the performance diÔ¨Äerence between any two policies, we still need
the local structure of the MSV problem to guide the direction of optimization. Following the
line of the last part, we present the MSV performance derivative formula in this subsection,
which describes the performance derivative at ¬µ towards another policy ¬µ(cid:48).

Theorem 4. Given any two policies ¬µ, ¬µ(cid:48)

Œ†, we consider a mixed policy ¬µŒΩ,

‚àà

¬µŒΩ(a

|

s) = (1

ŒΩ)¬µ(a

|

‚àí

s) + ŒΩ¬µ(cid:48)(a

s),

|

where the action follows ¬µ with probability 1
We have

‚àí

ŒΩ, and follows ¬µ(cid:48) with probability ŒΩ for ŒΩ

[0, 1].

‚àà

dŒæ‚àí
dŒΩ

= Es‚àºœÄ,a‚àº¬µ(cid:48)[(1 + 2Œ≤Œ∑‚àí)A¬µ

Œ∑ (s, a)

Œ≤A¬µ
Œ∂‚àí

(s, a)].

‚àí

Proof. From MSVPDF, we obtain the diÔ¨Äerence for ¬µ, ¬µŒΩ,

ŒæŒΩ
‚àí ‚àí

Œæ‚àí = Es‚àºœÄŒΩ ,a‚àº¬µŒΩ [A¬µ

f (s, a)]

Œ≤Es‚àºœÄŒΩ ,a‚àº¬µŒΩ (cid:2)(r

Œ∑ŒΩ)2

‚àí ‚àí

(r

‚àí

‚àí

Œ∑)2
‚àí

(cid:3) ,

‚àí

where Œ∑ŒΩ := Œ∑¬µŒΩ . Taking the derivative w.r.t. ŒΩ and letting ŒΩ
0, we obtain the performance
derivative formula. To simplify the derivation, we denote the terms on the right hand side as

‚Üí

h1(ŒΩ) = Es‚àºœÄŒΩ ,a‚àº¬µŒΩ [A¬µ
h2(ŒΩ) = Es‚àºœÄŒΩ ,a‚àº¬µŒΩ (cid:2)(r

f (s, a)],
Œ∑ŒΩ)2

Œ∑)2
‚àí

(cid:3) .

(r

‚àí

‚àí ‚àí

‚àí

Then ŒæŒΩ

‚àí ‚àí

Œæ‚àí = h1(ŒΩ)

‚àí

Œ≤h2(ŒΩ). SpeciÔ¨Åcally, we have

h1(ŒΩ) = Es‚àºœÄŒΩ [(1

= ŒΩEs‚àºœÄŒΩ ,a‚àº¬µ(cid:48)[A¬µ

‚àí

ŒΩ)Ea‚àº¬µ[A¬µ
f (s, a)],

f (s, a)] + ŒΩEa‚àº¬µ(cid:48)[A¬µ

f (s, a)]]

where the last equality follows that Ea‚àº¬µ[A¬µ

f (s, a)] = 0. Since limŒΩ‚Üí0 œÄŒΩ = œÄ, we obtain

dh1
dŒΩ

= Es‚àºœÄ,a‚àº¬µ(cid:48)[A¬µ

f (s, a)].

Next, we diÔ¨Äerentiate (r

Œ∑)2
‚àí

,

‚àí

d(r

Œ∑)2
‚àí

‚àí
dŒΩ

= 2(r

‚àí

d(r

Œ∑ŒΩ)‚àí

Œ∑)‚àí

‚àí
dŒΩ
Œ∑)‚àí1(r < Œ∑)

2(r

‚àí

‚àí

(i)
=

(ii)
=

2(r

‚àí

‚àí

Œ∑)‚àí

dŒ∑
dŒΩ

,

8

dŒ∑
dŒΩ

(11)

MSV Policy Optimization via Risk-Averse RL

where (i) follows d(x)‚àí
we have

dx = 1(x < 0), and (ii) comes from (r

Œ∑)‚àí1(r < Œ∑) = (r

Œ∑)‚àí. Thus,

‚àí

‚àí

dh2
dŒΩ

= lim
ŒΩ‚Üí0

1
ŒΩ

(cid:88)

œÄŒΩ(s)

(cid:88)

¬µŒΩ(a

s

a

|

s) (cid:2)(r

Œ∑ŒΩ)2

‚àí ‚àí

(r

(cid:3)

Œ∑)2
‚àí

‚àí
Œ∑ŒΩ)2

‚àí
Œ∑)2
‚àí

(cid:88)

œÄŒΩ(s)

(cid:88)

¬µŒΩ(a

= lim
ŒΩ‚Üí0

(r

‚àí

s)

|

(r

‚àí

‚àí ‚àí
ŒΩ

s

œÄ(s)

œÄ(s)

a

¬µ(a

¬µ(a

(cid:88)

a
(cid:88)

a

s)

s)

|

|

(cid:88)

s
(cid:88)

s

d(r

Œ∑)2
‚àí

‚àí
dŒΩ

(cid:20)

2(r

‚àí

‚àí

Œ∑)‚àí

(cid:21)

dŒ∑
dŒΩ

=

=

=

2Œ∑‚àí

‚àí

dŒ∑
dŒΩ

.

Here we deÔ¨Åne the semimean Œ∑‚àí as

Œ∑‚àí := Œ∑¬µ

‚àí = Es‚àºœÄ,a‚àº¬µ[(r

Œ∑)‚àí],

‚àí

(12)

which is the downside expectation of rewards under œÄ. From the standard result of PA (Cao,
2007, Chapter 2), we have

Putting the above relationships together, we obtain

dŒ∑
dŒΩ

= Es‚àºœÄ,a‚àº¬µ(cid:48)[A¬µ

Œ∑ (s, a)].

dŒæ‚àí
dŒΩ

Œ≤

=

dh1
dŒΩ ‚àí

dh2
dŒΩ
= Es‚àºœÄ,a‚àº¬µ(cid:48)[A¬µ
= Es‚àºœÄ,a‚àº¬µ(cid:48)[A¬µ
= Es‚àºœÄ,a‚àº¬µ(cid:48)[(1 + 2Œ≤Œ∑‚àí)A¬µ

dŒ∑
f (s, a)] + 2Œ≤Œ∑‚àí
dŒΩ
f (s, a) + 2Œ≤Œ∑‚àíA¬µ
Œ∑ (s, a)]
Œ≤A¬µ
Œ∑ (s, a)
Œ∂‚àí

‚àí

(s, a)].

The above equality indicates that the performance derivative is related to another reward

function w.r.t. f (cf. Equation 9):

g(s, a) := f (s, a) + 2Œ≤Œ∑‚àír

= (1 + 2Œ≤Œ∑‚àí)r

Œ≤(r

‚àí

‚àí

Œ∑)2
‚àí,

and the derivative formula can be written as

dŒæ‚àí
dŒΩ

= Es‚àºœÄ,a‚àº¬µ(cid:48)[A¬µ

g (s, a)],

(13)

(14)

(15)

where A¬µ

g (s, a) is the advantage function w.r.t. g.

With the performance derivative formula, we deÔ¨Åne the local optimum for MSV and

present the necessary condition for MSV optimality.

9

Ma, Ma, Xia, & Zhao

DeÔ¨Ånition 1. For a policy ¬µ,
say ¬µ is a local optimum in the mixed policy space.

‚àà

¬ØŒΩ

‚àÉ

(0, 1) and we always have Œæ¬µ

‚àí ‚â•

ŒæŒΩ
‚àí,

ŒΩ
‚àÄ

‚àà

(0, ¬ØŒΩ), then we

Theorem 5. The optimal policy of MSV can be found in the deterministic policy space, and
satisÔ¨Åes the necessary condition

¬µ‚àó(a

|

(cid:18)

s) = Œ¥

a

argmax
b‚ààA

‚àà

(cid:19)

A‚àó

g(s, b)

,

which implies that A‚àó

g(s, a)

0,

s
‚àÄ

‚â§

‚àà S

‚àà A

, a

. Here Œ¥ denotes the Dirac delta function.

Proof. The theorem is a direct result of the derivative formula. The (local) optimality implies
that if ¬µ is a local optimum, we always have dŒæ‚àí
0 for any direction in the policy space.
Assuming there is a contradiction, where for a state s there exists ¬µ(a
s) = Œ¥(a = a(cid:48)) for
any a(cid:48) /
g (s, b), we can always Ô¨Ånd a better policy in the mixed policy space along
‚àà
the derivative direction.

argmaxb A¬µ

dŒΩ ‚â§

|

5. Optimization and Algorithms

In this section, we propose two approaches to optimize MSV with the parameterized policy.
We Ô¨Årstly extend the policy gradient method to MSV with the pseudo reward function (cf.
Equation 13) in Section 4. Following the same idea, we propose a trust region method to
solve the MSV problem, and prove the the lower bound for its performance improvement.
The two approaches together establish an iterative framework to solve the MSV problem.

5.1 MSV Policy Gradient Method

Policy gradient theorem is an essential foundation of modern deep RL algorithms, such as
Actor-Critic methods. Here we consider the policy ¬µ parameterized by Œ∏
Œò, which can
be implemented with any diÔ¨Äerentiable function. We Ô¨Årst give the MSV Policy Gradient
(MSVPG) theory formally as follows.

‚àà

Theorem 6. For a policy ¬µ parameterized by Œ∏, we have

Œ∏Œæ‚àí = Es‚àºœÄ,a‚àº¬µ[

‚àá

Œ∏ log ¬µ(a

|

s)A¬µ

g (s, a)].

‚àá

(16)

The policy gradient for MSV can be easily proved by PA, which follows the same lines of
derivative formula. For the readers from the DRL community, we also provide an alternative
proof based on (Sutton & Barto, 2018) in the appendix.

Proof. Consider two policies ¬µ, ¬µ(cid:48) parameterized by Œ∏, Œ∏(cid:48) respectively. Their performance
diÔ¨Äerence is given as

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

f (s, a)]

Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

(cid:2)(r

Œ∑(cid:48))2

‚àí ‚àí

‚àí

(r

‚àí

Œ∑)2
‚àí

(cid:3) .

‚àí

Let denote ‚àÜŒ∏ = Œ∏(cid:48)
above equation

‚àí

Œ∏. Similar to the derivation in Section 4.2, we denote the terms of

h1(‚àÜŒ∏) = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ
(cid:2)(r
h2(‚àÜŒ∏) = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

f (s, a)],
Œ∑(cid:48))2

Œ∑)2
‚àí

(cid:3) .

(r

‚àí

‚àí ‚àí

‚àí

10

MSV Policy Optimization via Risk-Averse RL

We take the limit of Œæ(cid:48)

‚àí ‚àí

Œæ‚àí by letting Œ∏(cid:48)

Œ∏.

‚Üí

Œ∏h1 = lim
‚àÜŒ∏‚Üí0

‚àá

(cid:88)

œÄ(cid:48)(s)

(cid:88)

(cid:104)
¬µ(cid:48)(a

s)A¬µ

f (s, a)

(cid:105)

|

1
‚àÜŒ∏

(cid:88)

s

(i)
= lim
‚àÜŒ∏‚Üí0
(cid:88)

=

œÄ(s)

s

(cid:88)
a ‚àá
(cid:104)

‚àá

s

(cid:88)

œÄ(cid:48)(s)

a
¬µ(cid:48)(a

|

s)

‚àí
‚àÜŒ∏

¬µ(a

s)

|

A¬µ

f (s, a)

a

Œ∏¬µ(a

|

s)A¬µ

f (s, a)

(ii)
= Es‚àºœÄ,a‚àº¬µ

Œ∏ log ¬µ(a

s)A¬µ

(cid:105)
f (s, a)

,

|

where (i) follows Ea‚àº¬µ[A¬µ

f (s, a)] = 0 and (ii) comes from

Similar to the derivation in Equation 11, we have

Œ∏ log ¬µ(a

‚àá

|

s) = ‚àá

Œ∏¬µ(a
¬µ(a

s)

|
s)

.

|

Œ∏h2 = lim
‚àÜŒ∏‚Üí0

‚àá

(cid:88)

œÄ(cid:48)(s)

(cid:88)

¬µ(cid:48)(a

s

a

(r

‚àí

Œ∑(cid:48))2

‚àí ‚àí
‚àÜŒ∏

s)

|

Œ∑)2
‚àí

(r

‚àí

=

=

=

=

(cid:88)

s
(cid:88)

s
(cid:88)

œÄ(s)

œÄ(s)

œÄ(s)

(cid:88)

a
(cid:88)

a
(cid:88)

¬µ(a

¬µ(a

¬µ(a

s
2Œ∑‚àí

‚àí

‚àá

a
Œ∏Œ∑.

|

|

|

s) lim
‚àÜŒ∏‚Üí0

(r

‚àí

Œ∑(cid:48))2

‚àí ‚àí
‚àÜŒ∏

Œ∑)2
‚àí

(r

‚àí

Œ∏(r

s)

‚àá

‚àí

Œ∑)2
‚àí

s)[

2(r

‚àí

Œ∑)‚àí

Œ∏Œ∑]

‚àá

‚àí

Since

Œ∏Œ∑ = Es‚àºœÄ,a‚àº¬µ [

the gradient of Œæ‚àí

‚àá

Œ∏ log ¬µ(a

|

s)A¬µ

Œ∑ (s, a)], we combine the results together and give

‚àá

Œ∏Œæ‚àí =

‚àá

‚àá

Œ≤
Œ∏h1
‚àí
‚àá
(cid:104)
= Es‚àºœÄ,a‚àº¬µ

Œ∏h2

Œ∏ log ¬µ(a

‚àá

(cid:104)

(cid:2)

Œ∏ log ¬µ(a

‚àá

Œ∏ log ¬µ(a

‚àá

s)A¬µ

s)A¬µ
s)A¬µ

Œ∏Œ∑

+ 2Œ≤Œ∑‚àí

(cid:105)
f (s, a)
‚àá
(cid:105)
f (s, a) + 2Œ≤Œ∑‚àíA¬µ
Œ∑ (s, a)
g (s, a)(cid:3) .

|

|

|

= Es‚àºœÄ,a‚àº¬µ
= Es‚àºœÄ,a‚àº¬µ

Here we present an Actor-Critic algorithm based on MSVPG, which is named MSVAC (see
Algorithm 2). In addition to the parameterized policy, we maintain another parameterized
function VœÜ as the value function. Then, the advantage function is estimated with the
generalized advantage estimation (GAE) (Schulman et al., 2016). Typically, we have

ÀÜAg(sn, an) =

N ‚àí1
(cid:88)

t=n

Œªt‚àín (g(st, at)

ÀÜg + VœÜ(st)

‚àí

‚àí

VœÜ(st+1)) ,

(17)

11

Ma, Ma, Xia, & Zhao

Algorithm 2 MSVAC
Input: Œ±, Œ≤, K, N
1: Initialize the policy with Œ∏ and the value with œÜ randomly.
2: Set ÀÜŒ∑ = 0, ÀÜŒ∑‚àí = 0, ÀÜŒ∂‚àí = 0.
3: for k = 1, 2,
4:

, K do

¬∑ ¬∑ ¬∑

(1

‚Üê

(cid:80)N ‚àí1

‚àí
(1
(1

(sn, an, rn, sn+1)
}
{

Execute policy ¬µŒ∏ for N times to collect
Update ÀÜŒ∑
Œ±)ÀÜŒ∑ + Œ± 1
N
Update ÀÜŒ∑‚àí
Œ±)ÀÜŒ∑‚àí + Œ± 1
N
Update ÀÜŒ∂‚àí
Œ±)ÀÜŒ∂‚àí + Œ± 1
N
Compute g(sn, an) with Equation 13 at all timesteps and ÀÜg.
Compute ÀÜAg(sn, an) with Equation 17 at all timesteps.
Update the Œ∏ with Equation 16.
Update the œÜ with Equation 18.

n=0 rn.
(cid:80)N ‚àí1
(cid:80)N ‚àí1

n=0 (rn
n=0 (rn

ÀÜŒ∑)‚àí.
.
ÀÜŒ∑)2
‚àí

‚Üê
‚Üê

‚àí
‚àí

‚àí
‚àí

5:

6:

7:

8:

9:

10:

11:
12: end for

N ‚àí1
n=0

.

Œ≤ ÀÜŒ∂‚àí is
where Œª is the hyper-parameter to trade-oÔ¨Ä bias and variance, and ÀÜg = (1 + 2Œ≤ ÀÜŒ∑‚àí)ÀÜŒ∑
the estimation of average surrogate reward function. With ÀÜVn = VœÜ(sn) + ÀÜAg(sn, an) as the
target value, we update the value function with

‚àí

V (œÜ) :=

L

1
2N

N ‚àí1
(cid:88)

(VœÜ(sn)

n=0

ÀÜVn)2.

‚àí

(18)

5.2 MSV Trust Region Method

While PG has a concise form, it often suÔ¨Äers from the diÔ¨Éculty of selecting step-sizes and
the sensitivity to initial points in practice, especially when it works with neural networks. To
address these drawbacks, trust region method (Schulman et al., 2015) is proposed to solve a
surrogate problem in a local trust region and perform an approximate policy iteration.

5.2.1 Monotonic Improvement Guarantee

We extend the idea of trust region in the standard MDP into MSV, and propose the MSV
Trust Region Policy Optimization (MSVTRPO) method. In MSVTRPO, we iteratively solve
the problem as below

max

¬µ
g (¬µŒ∏)
¬µŒ∏ L
s.t. Es‚àºœÄDTV(¬µŒ∏(

¬∑ |

s)

¬µ(

¬∑ |

(cid:107)

s))

‚â§

(cid:15)¬µ,

(19)

where

¬µ
g (¬µŒ∏) := Es‚àºœÄ,a‚àº¬µŒ∏

(cid:2)A¬µ

g (s, a)(cid:3) .

L

Remark 1. The trust region method updates the policy via the direction of maximum deriva-
tive (cf. the performance derivative formula in Equation 15), constrained in the proximity

12

MSV Policy Optimization via Risk-Averse RL

policy space with the T V -divergence. In contrast, the standard policy iteration scheme updates
the policy via the same direction without constraint, which breaks the monotonic improvement
for MSV.

Next, we will show that MSVTRPO enjoys an analogous performance improvement
0, the lower bound is dominated by

bound. When the trust region is tight enough, i.e., (cid:15)¬µ
the Ô¨Årst order term.

‚Üí

To complete the proof, we need following lemma to bound the state-action distributions.
For a policy ¬µ, we denote the steady state-action distribution as œÅ(s, a) := œÄ(s)¬µ(s, a). Then
we have:

Lemma 1. For any two policies ¬µ, ¬µ(cid:48)
distributions œÅ, œÅ(cid:48) is bounded by

‚àà

Œ†, the diÔ¨Äerence of their steady state-action

œÅ(cid:48)

(cid:107)

1

œÅ
(cid:107)

‚àí

‚â§

2Œ∫(cid:48)(cid:15)¬µ.

Proof.

œÅ(cid:48)

(cid:107)

œÅ
(cid:107)

‚àí

1 =

‚â§

=

‚â§

(cid:88)
s,a |
(cid:88)
s,a |
(cid:88)
s |
2 (cid:0)(Œ∫(cid:48)

œÄ(cid:48)(s)¬µ(cid:48)(a

œÄ(s)¬µ(a

s)

|

|

œÄ(cid:48)(s)¬µ(cid:48)(a

œÄ(s)¬µ(cid:48)(a

s)

s)

|

|

‚àí

‚àí

œÄ(cid:48)(s)

œÄ(s)
|

‚àí

+

1)(cid:15)¬µ + (cid:15)¬µ

‚àí

(cid:88)

œÄ(s)

s

(cid:1) = 2Œ∫(cid:48)(cid:15)¬µ,

|

s)

|
(cid:88)
a |

+

œÄ(s)¬µ(cid:48)(a

(cid:88)
s,a |

s)

|

‚àí

œÄ(s)¬µ(a

s)
|

|

¬µ(cid:48)(a

s)

|

‚àí

¬µ(a

s)

|

|

where the last inequality follows that
appendix shown by Ma et al., 2021).

(cid:107)

œÄ(cid:48)(s)

œÄ(s)
1
(cid:107)

‚àí

‚â§

2(Œ∫(cid:48)

‚àí

1)(cid:15)¬µ (see proposition 2 in

Theorem 7. Let ¬µ(cid:48) be the solution to the problem deÔ¨Åned by Equation 19. We have

where (cid:15)g = maxs

Œæ(cid:48)

‚àí
Ea‚àº¬µ(cid:48)[A¬µ
|

Œæ

g (¬µ(cid:48))
¬µ
‚â• L
g (s, a)]
|

2(Œ∫(cid:48)

1)(cid:15)g(cid:15)¬µ

12Œ≤(Œ∫(cid:48))2R2

max(cid:15)2
¬µ,

‚àí

‚àí

‚àí
and Œ∫(cid:48) is Kemeny‚Äôs constant under ¬µ(cid:48).

Proof. Again, we start our analysis from MSVPDF. Based on Equation 11, we have

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ
= Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ
Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)
= Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

f (s, a)]
‚àí
f (s, a) + 2Œ≤Œ∑‚àíA¬µ
(cid:2)(r
Œ∑(cid:48))2
(r
‚àí ‚àí
‚àí
‚àí
2Œ≤Œ∑‚àí(Œ∑(cid:48)
g (s, a)]

(cid:2)(r
Œ∑ (s, a)]
‚àí
(cid:3)
Œ∑)2
‚àí
Œ∑)

‚àí

‚àí
where the last equation follows the diÔ¨Äerence formula of average reward,

‚àí

‚àí

‚àí

Œ≤Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

(cid:2)(r

Œ∑(cid:48))2

‚àí ‚àí

Œ∑(cid:48))2

(r
‚àí
Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[2Œ≤Œ∑‚àíA¬µ

‚àí ‚àí

‚àí

Œ∑)2
‚àí

(cid:3)

Œ∑ (s, a)]

Œ∑(cid:48)

‚àí

Œ∑ = Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

Œ∑ (s, a)].

The result indicates that the diÔ¨Äerence can be separated into two parts: the improvement
by optimizing the surrogate problem (the Ô¨Årst term), and the discrepancy by the change of

13

Œ∑)2
‚àí

(cid:3) ,

(r

‚àí

(20)

Ma, Ma, Xia, & Zhao

Œ∑ (the rest terms). The insight of our proof is to show that the Ô¨Årst term dominates the
diÔ¨Äerence and the rest terms can be ignored in a tight trust region.

The Ô¨Årst term can be tackled with the standard trust region method. With the lower

bound of average trust region method in Equation 7, we have

Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)[A¬µ

g (s, a)]

g (¬µ(cid:48))
¬µ

‚àí L

2(Œ∫(cid:48)

‚â• ‚àí

1)(cid:15)g(cid:15)¬µ.

‚àí

(21)

Now, we need to bound the rest terms. We have

(cid:2)(r

Œ∑(cid:48))2

2Œ∑‚àí(Œ∑(cid:48)
Œ∑) + Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)
‚àí ‚àí
‚àí
‚àí
‚àí
Œ∑)‚àí(Œ∑(cid:48)
= Es‚àºœÄ,a‚àº¬µ[2(r
Œ∑)] + Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)
‚àí
(cid:2)(r
Œ∑(cid:48))2
= Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)
(r
‚àí ‚àí
‚àí
Œ∑)(cid:0)Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)(r
2(Œ∑(cid:48)

Œ∑)2
‚àí
Œ∑)‚àí

‚àí + 2(r

(r

‚àí

‚àí
Es‚àºœÄ,a‚àº¬µ(r

(cid:3)
Œ∑)2
‚àí
(cid:2)(r
‚àí
Œ∑)‚àí(Œ∑(cid:48)

Œ∑(cid:48))2

‚àí ‚àí
Œ∑)(cid:3)
(cid:1).

‚àí
Œ∑)‚àí

‚àí

(cid:3)

Œ∑)2
‚àí

(r

‚àí

Œ∑). Considering all potential cases for

‚àí

‚àí

‚àí

Denote h := (r(cid:48)
‚àí ‚àí
the relationship between Œ∑, Œ∑(cid:48) and h, we have

Œ∑(cid:48))2

Œ∑)2

(r(cid:48)

‚àí

‚àí

‚àí

‚àí
‚àí + 2(r(cid:48)

‚àí
Œ∑)‚àí(Œ∑(cid:48)

‚Ä¢ If r

max

Œ∑, Œ∑(cid:48)

‚â•
{
‚Ä¢ If r < min
Œ∑, Œ∑(cid:48)
{

}

, h = 0.

}
, h = (r

‚àí
Œ∑(cid:48))2

‚àí
(Œ∑(cid:48)

‚àí
Œ∑)2.

‚Ä¢ If Œ∑

‚â§

r < Œ∑(cid:48), h = (r

‚Ä¢ If Œ∑(cid:48)
(r

‚àí
‚àí
r < Œ∑, we denote c0 = r
Œ∑)(Œ∑(cid:48)

Œ∑(cid:48)
‚àí
1 + 2c0c1
‚àí
Synthesizing the above results, we conclude 0

‚â§
Œ∑)2 + 2(r

Œ∑) = c2

‚àí

‚àí

‚àí

‚â§

‚â•
‚â§
h

Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)

(cid:2)(r

Œ∑(cid:48))2

‚àí ‚àí
With Lemma 1, we obtain that

‚àí

(r

‚àí

Œ∑(cid:48))2

(r

Œ∑)2 + 2(r

Œ∑)(Œ∑(cid:48)

‚àí

‚àí

Œ∑) = (Œ∑(cid:48)

Œ∑)2.

‚àí

0 and c1 = Œ∑
(c0 + c1)2 = (Œ∑(cid:48)

r > 0. We have h =
Œ∑)2.

‚àí
‚àí
Œ∑)2. Thus we have

Œ∑)(cid:3)

‚àí

(Œ∑(cid:48)

‚àí

‚â§

Œ∑)2.

(22)

(Œ∑(cid:48)

‚â§

‚â§
‚àí + 2(r

Œ∑)2

‚àí
Œ∑)‚àí(Œ∑(cid:48)

‚àí

Œ∑(cid:48)
|

Œ∑

|

‚àí

=

œÅ(cid:48)r
|

‚àí

œÅr

œÅ(cid:48)

| ‚â§ (cid:107)

1Rmax
œÅ
(cid:107)

‚àí

‚â§

2Œ∫(cid:48)(cid:15)¬µRmax,

where the Ô¨Årst inequality follows the H√∂lder‚Äôs inequality. Similarly, we have

Œ∑)‚àí
œÅ(r

‚àí

Es‚àºœÄ,a‚àº¬µ(r
Œ∑)‚àí

‚àí

|

Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)(r
‚àí
|
œÅ(cid:48)(r
Œ∑)‚àí
=
‚àí
|
‚àí
œÅ(cid:48)
1Rmax
œÅ
‚àí
(cid:107)
2Œ∫(cid:48)(cid:15)¬µRmax

‚â§ (cid:107)

‚â§

Œ∑)‚àí

|

‚àí

(23)

(24)

(25)

(26)

where Equation 25 comes from that 0
into Equation 22 and combining with Equation 21, we arrive at

Œ∑)‚àí

(r

‚â§

‚àí

‚â§

2Rmax. Substituting the previous results

Œæ(cid:48)
‚àí ‚àí

Œæ‚àí

g (¬µ(cid:48))
¬µ
‚â• L
Œ∑(cid:48)
2Œ≤
|
g (¬µ(cid:48))
¬µ
‚â• L

‚àí

‚àí

‚àí

‚àí

2(Œ∫
Œ≤
1)(cid:15)g(cid:15)¬µ
‚àí
‚àí
(cid:12)
(cid:12)Es‚àºœÄ(cid:48),a‚àº¬µ(cid:48)(r
1)(cid:15)g(cid:15)¬µ

Œ∑
|
2(Œ∫(cid:48)

Œ∑)2

(Œ∑(cid:48)
‚àí
|
Œ∑)‚àí
‚àí
12Œ≤(Œ∫(cid:48))2R2

‚àí

|
Es‚àºœÄ,a‚àº¬µ(r
max(cid:15)2
¬µ.

‚àí

‚àí

Œ∑)‚àí

(cid:12)
(cid:12)

‚àí

14

MSV Policy Optimization via Risk-Averse RL

Algorithm 3 MSVPO
Input: Œ±, Œ≤, K, N, M
1: Initialize the policy with Œ∏ and the value with œÜ randomly.
2: Set ÀÜŒ∑ = 0, ÀÜŒ∑‚àí = 0, ÀÜŒ∂‚àí = 0.
3: for k = 1, 2,
4:

, K do

¬∑ ¬∑ ¬∑

(1

‚Üê

‚Üê
‚Üê

(cid:80)N ‚àí1

‚àí
(1
(1

n=0 rn.
(cid:80)N ‚àí1
(cid:80)N ‚àí1

(sn, an, rn, sn+1)
}
{

Œ±)ÀÜŒ∑ + Œ± 1
N
Œ±)ÀÜŒ∑‚àí + Œ± 1
N
Œ±)ÀÜŒ∂‚àí + Œ± 1
N

Execute policy ¬µŒ∏ for N times to collect
ÀÜŒ∑
ÀÜŒ∑‚àí
n=0 (rn
ÀÜŒ∂‚àí
n=0 (rn
Compute g(sn, an) with Equation 13 at all timesteps and ÀÜg.
Compute ÀÜAg(sn, an) with Equation 17 at all timesteps.
Update the Œ∏ with equation 27 for M epochs.
Update the œÜ with Equation 18 for M epochs.

ÀÜŒ∑)‚àí.
.
ÀÜŒ∑)2
‚àí

‚àí
‚àí

‚àí
‚àí

5:

6:

7:

8:

9:

10:

11:
12: end for

N ‚àí1
n=0

.

5.2.2 Implementation details

In the end of this subsection, we address some implementation issues of MSVTRPO. First of
all, in practice, we replace the TV-divergence with KL-divergence as most of trust region
methods do. Since DTV(p
q)/2, the theoretical results are still applicable
(cid:107)
for the practical algorithms.

DKL(p

(cid:112)

q)

‚â§

(cid:107)

In the tabular case, where the state and action spaces are Ô¨Ånite and discrete, it is enough
to parameterize the policy tabularly. The previous analysis of TRPO (Abdolmaleki et al.,
2018) shows that Equation 19 enjoys a closed form solution:

¬µ(cid:48)(

¬∑ |

s, a)

¬µ(

¬∑ |

‚àù

s, a) exp

(cid:18) A¬µ

g (s, a)
œÖ‚àó

(cid:19)

,

where œÖ‚àó can be obtained by solving the dual problem

min

œÖ L

(œÖ) := œÖ(cid:15)¬µ + œÖ

(cid:88)

s

œÄ(s) log

(cid:88)

a

¬µ (a

|

s) exp

(cid:18) A¬µ

g (s, a)
œÖ

(cid:19)

.

With a known MDP, we name this iterative procedure as MSV Trust Region Policy Iteration
(MSVTRPI). As aforementioned in Section 4, PI is not available for MSV. Nevertheless, we
can do MSVTRPI as an alternative. When (cid:15)¬µ
, it degrades to the standard PI without
the monotonic improvement guarantee.

‚Üí ‚àû

In the model-free case with large state and action spaces, we recommend solving the
surrogate loss proposed by PPO (Schulman et al., 2017), for its stable performance and fast
computing with neural networks. Formally, instead of optimizing the problem in Equation 19,
we maximizing the clipping objective

CLIP
¬µ

(Œ∏) :=

L

1
N

N ‚àí1
(cid:88)

n=0

(cid:104)
min

(cid:16)

œân(Œ∏) ÀÜAg(sn, an), clip(œân(Œ∏), 1

‚àí

Œµ, 1 + Œµ) ÀÜAg(sn, an)

(cid:17)(cid:105)

,

(27)

where œân(Œ∏) = ¬µŒ∏(an|sn)
is the importance sampling ratio. Since we consider the long-run
¬µ(an|sn)
average performance in this paper, GAE is not applicable directly. Thus, we adopt the average

15

Ma, Ma, Xia, & Zhao

value constraint (AVC) proposed by (Ma et al., 2021) to stabilize the value learning. The
full algorithm, named by MSV Policy Optimization (MSVPO) is presented in Algorithm 3.

6. Experiments

In the previous sections, we analyze the properties of MSV problem and Ô¨Ånd that it can
be soloved iteratively optimizing a surrogate reward function g (c.f. Equation 13). We also
propose two methods to solve the MSV problem in the parameterized policy space.

To validate the eÔ¨Äectiveness of our proposed methods in solving MSV problem, we conduct

a series of experiments to answer the corresponding questions:

‚Ä¢ Is the MSV really optimized by the the surrogate reward function g? SpeciÔ¨Åcally, what

is the diÔ¨Äerence from optimizing g instead of f ?

‚Ä¢ What is the diÔ¨Äerence between the MV (Xia, 2020) and MSV criteria?

‚Ä¢ Does the proposed algorithms work well with the current deep RL algorithms?

6.1 Bandit Problem

(a) Reward distributions.

(b) Polices paths.

Figure 2: The bandit problem. (a) Reward distributions in the bandit problem. (b) Polices
paths in the bandit problem. The paths are shown in the logarithmic parameter space.

We start from a simple bandit problem. In this problem, there are three actions with only
a single state. DiÔ¨Äerent actions result in diÔ¨Äerent rewards following the distributions shown
in Figure 2(a). SpeciÔ¨Åcally, we have r0 sampled from a shifted LogNormal(0, 1) distribution,
of which the mean is shifted to zero. If we choice a1, we will obtain r1
N (0, 22). Otherwise,
we will have r2
N (1, 32). Obviously, we have three diÔ¨Äerent risk preference actions. When
we Ô¨Åx Œ≤ = 1 in MV and MSV, the agent should always choice a0 if it optimizes the MSV
criterion, and choice a1 if it optimizes the MV criterion. The a2 has the highest outcome,
which is preferred by risk-neutral agents.

‚àº

‚àº

We compare three diÔ¨Äerent agents, which optimize diÔ¨Äerent reward functions. The Ô¨Årst
(cf. Equation 13), which is the derived reward
(cf. Equation 9),
(r

one optimizes g = (1 + 2Œ∑‚àí)r
‚àí
function with Œ≤ = 1 in this work. The second one optimizes f = r

Œ∑)2
‚àí

(r

‚àí

Œ∑)2
‚àí

‚àí

‚àí

16

‚àí4‚àí2024Reward0.00.20.40.6PDFr0r1r2‚àí6‚àí4‚àí20log¬µ(a0)‚àí4‚àí20log¬µ(a1)(1+2Œ∑‚àí)r‚àí(r‚àíŒ∑)2‚àír‚àí(r‚àíŒ∑)2‚àír‚àí(r‚àíŒ∑)2MSV Policy Optimization via Risk-Averse RL

Figure 3: Comparison of MSVTRPI and MVPI in the portfolio management problem. The
normalized MSV means Œ≤ is doubled in comparison.

which is the Monte-Carlo return of MSV. We further consider a third agent which optimizes
Œ∑)2 (Xia, 2020), an MV objective to illustrate the diÔ¨Äerence of MSV and MV problems.
r
All the agents use policy gradient with a parameterized policy initialized as a uniform one.

(r

‚àí

‚àí

To visualize the learning process, we plot the curves in the logarithmic parameter space,
as shown in Figure 2(b). Since (cid:80)
i ¬µ(ai) = 1, ¬µ(a2) is ignored in the Ô¨Ågure. As expected,
the learning curve of the Ô¨Årst agent (blue solid curve) approaches (0,
), meaning that
it always chooses a0 Ô¨Ånally. Similarly, the third agent (green dotted curve) also chooses a1
correspondingly. Interestingly, the second agent (red dashed curve), which optimizes the
Monte-Carlo return of MSV, Ô¨Ånally converges to choose a2. The result tells us optimizing the
reward f = r
cannot optimize the MSV objective even in such a simple problem.
This reÔ¨Çects the most essential diÔ¨Äerence between the optimization of policy-dependent
reward and other problems. As discussed in Section 4, to optimize a problem with a policy-
dependent reward function, we must consider the perturbation of the mean, at least in MSV
problems.

Œ∑)2
‚àí

‚àí‚àû

Œ≤(r

‚àí

‚àí

6.2 Portfolio Management

In this part, we compare the performances of MSV- and MV-optimal polices in a portfolio
management problem. We need to manage two independent assets and cash. At the stage t,
the gain of the i-th asset is denoted by xi,t
, which transits according
‚àà {‚àí
to a transition probability matrix (described in Appendix). The action space is deÔ¨Åned
(cid:80)
, where wi,t is the weight of
as
‚àà {
}}
current portfolio on the i-th asset. Let w0,t = 1
w2,t denote the partition of cash
in current portfolio and x0 denote the return of cash. The reward function is deÔ¨Åned as

0, 0.2, . . . , 1
w1,t

0.1, . . . , 0.5
}

(w1,t, w2,t)
{

i=1,2 wi,t

1, wi,t

0.2,

A

=

‚â§

‚àí

‚àí

‚àí

|

17

102030Œ≤0.100.15ValueMean102030Œ≤0.0050.010Variance102030Œ≤0.0020.0040.006Semivariance102030Œ≤0.000.050.100.15ValueMean-Var102030Œ≤0.050.100.15Mean-SemivarMVMSVMSV(normalized)Ma, Ma, Xia, & Zhao

Figure 4: Reward distribution in the portfolio management problem. The policy optimizing
MSV achieves Œ∑ = 0.168, Œ∂ = 0.014, Œ∂‚àí = 0.006. As a comparison, the policy optimizing MV
achieves Œ∑ = 0.073, Œ∂ = 0.002, Œ∂‚àí = 0.001.

‚àí

(cid:80)

wi,t

wi,t‚àí1

i=1,2 |

rt = w0,tx0 + (cid:80)
i=1,2 wi,txi,t
is deÔ¨Åned as st = (x1,t, x2,t, w0,t, w1,t). Hence,

c, where c is the transition cost. The state
|
= 21 and
|A|
For the MSV, we optimize the policy with the MSV trust region policy iteration
(MSVTRPI) (see Section 5.2 for details), which aims to maximize Œæ¬µ
. We param-
eterize the policy in the softmax form as ¬µŒ∏(a
R|S||A| are
|
the ‚Äúlogic values‚Äù. For MV, we optimize the policy with the mean-variance policy iteration
(MVPI) proposed by Xia (2020), which maximizes Œæ¬µ = Œ∑¬µ

Œ≤Œ∂ ¬µ
‚àí
s) := softmax(Œ∏(s, a)), where Œ∏

= 1344.

‚àí = Œ∑¬µ

Œ≤Œ∂ ¬µ.

|S|

‚àí

‚àí

‚àà

We change the risk preference parameter Œ≤ and compare the MSVTRPI and MVPI. We
depict the result in Figure 3, showing that with a Ô¨Åxed Œ≤, optimizing MSV always results
in a larger return than that of MV. Besides, MV is more sensitive than MSV in terms of
Œ≤, meaning that a small change of Œ≤ will lead to a quick drop in both the return and risk.
To better compare MSV and MV, we also show the ‚Äúnormalized‚Äù results of MSV, where we
double Œ≤ to provide the same penalty strength as MV. The result shows the normalized MSV
also outperforms MV in terms of the average reward, illustrating that MSV is more plausible
than MV. We demonstrate the reward distributions in Figure 4 with Œ≤ = 10. It shows that
MSV maintains high returns while avoiding large losses. In contrast, optimizing MV may be
too conservative, as the upside rewards cause more volatility in this problem.

‚àí

6.3 Robotic Control

To demonstrate the eÔ¨Äectiveness of our proposed method in more general problem setups,
we implement a ‚Äúdeep‚Äù variant algorithm named mean-semivariance policy optimization
(MSVPO), which is based on the recent developed method APO (Ma et al., 2021) for
average-reward RL problems.

We evaluate MSVPO in the continuous control benchmark MuJoCo (Todorov et al.,
2012) with OpenAI gym (Brockman et al., 2016) as the interface. Since the original setup of
MuJoCo is not suitable for the long-run average setting, we slightly modify the experimental
protocol. In most of MuJoCo tasks, the agent will be terminated if it reaches any unsafe

18

‚àí0.2‚àí0.10.00.10.20.30.40.50.000.020.040.060.080.10ProbabilityMeanMean-VarMean-SemivarMSV Policy Optimization via Risk-Averse RL

Figure 5: Training curves of Walker2d with noise. Each curve is averaged over 10 random
seeds and shaded by the standard deviation.

state, such as falling down. In that cases, we will reset the system and add an extra cost to
the terminal state. DiÔ¨Äerent from other works focusing on the average episode returns, we
are interested in the long-run average and semivariance of the steady reward distribution.
To further increase the risk in the test scenarios, we add some noise to the agent outputs,
i.e., the real action taken by the environment is at + (cid:15), where (cid:15)
N (0, œÉ2). We call œÉ as the
noise level of the modiÔ¨Åed MuJoCo tasks.

‚àº

We evaluate MSVPO with diÔ¨Äerent Œ≤‚Äôs in the noisy Walker2d with diÔ¨Äerent noise levels.
When the agent falls, we penalize it with an extra cost -10 and reset the system. As shown
in the Figure 5, the choice of diÔ¨Äerent Œ≤‚Äôs achieves the trade-oÔ¨Ä between the average and
semivariance. In the noiseless environment (noise level = 0), we interestingly Ô¨Ånd that
risk-averse policy (Œ≤ = 0.1) achieves competitive average reward with lower semivariance. It
indicates that in complex scenes, optimizing a risk-averse metric may generate more robust
policies with better performances comparing with a risk-neutral one.

To better understand the performance diÔ¨Äerence with diÔ¨Äerent risk preference polices, we
visualize the reward distributions of typical agents in Figure 6, where each agent of noise
level 0.1 is evaluated for 1000 steps. We can see that risk-averse polices successfully avoid
the unsafe states. Meanwhile, the agent uses smaller steps forward with the risk parameter
Œ≤ increasing. Instead, the risk-neutral agent tends to take the risk of falling for larger gains.

19

0.02.55.07.510.0012345Reward AverageNoise Level=0.00.02.55.07.510.001234Noise Level=0.1=0.0=0.1=0.3=1.00.02.55.07.510.001234Noise Level=0.20.02.55.07.510.0Million Steps0123456Reward Semivariance0.02.55.07.510.0Million Steps01234560.02.55.07.510.0Million Steps0123456Ma, Ma, Xia, & Zhao

Figure 6: Reward distribution of Walker2d with noise.

7. Conclusion

This paper discusses how to optimize the mean-semivariance criterion for the steady reward
of MDPs and RL, which is an alternative risk measure of mean-variance. The semivariance
is a more reasonable measure than the variance in general scenarios, as it only penalizes
the downside risk. We utilize PA theory to derive the performance diÔ¨Äerence formula and
optimize MSV with data-driven approaches. We develop two algorithms for MSV based on
PA theory, following the policy gradient theory and the trust region theory, respectively. We
also demonstrate the eÔ¨Äectiveness of the proposed algorithms in diÔ¨Äerent problems, showing
the risk-averse performance of MSV policy. We point out that the application of the proposed
two-stage optimization framework for risk measures is not limited for MSV. We hope our
work can promote the applications of data-driven approaches in risk-sensitive environments
of MDPs and RL.

Acknowledgments

This work is funded by the National Natural Science Foundation of China (No. U1813216,
62192751, 61425027, 62073346, 11931018, U1811462), the National Key Research and Devel-
opment Project of China under Grant 2017YFC0704100 and Grant 2016YFB0901900, in part
by the 111 International Collaboration Program of China under Grant BP2018006, BNRist
Program (BNR2019TD01009), the National Innovation Center of High Speed Train R&D
project (CX/KJ-2020-0006), the Guangdong Province Key Laboratory of Computational
Science at the Sun Yat-Sen University (2020B1212060032) and the Guangdong Basic and
Applied Basic Research Foundation (2021A1515011984).

20

64202468Reward0.000.010.020.030.04Probability=0.0=0.1=0.3=1.0MSV Policy Optimization via Risk-Averse RL

Appendix A. Brief Review of Perturbation Analysis theory

|

Consider an ergodic MDP with transition matrix P (induced by some policy ¬µ), where
s) is the transition probability from s to s(cid:48). We also consider a corresponding reward
P (s(cid:48)
function r, where r(s) is the reward expectation at s. We are interested in the average
performance Œ∑ = œÄr, where œÄ denotes the steady state distribution. The Perturbation
Analysis (PA) theory (Cao, 2007) captures how the performance changes if the policy (or
system parameters P and r) has perturbations.

Theorem 8 (Performance diÔ¨Äerence formula). For two ergodic MDPs with P and P (cid:48), we
have

Œ∑ = œÄ[(P (cid:48)

Œ∑(cid:48)

‚àí

‚àí

P )V + r(cid:48)

r],

‚àí

where V is the value function (called potential function in PA) for the system with P .

The value function satisÔ¨Åes the Poisson equation (I

identity matrix and e is the unit vector.

‚àí

P )g + Œ∑e = r, where I denotes the

Theorem 9 (Performance derivative formula). Consider another MDP with P ŒΩ = P + ‚àÜP =
(1

ŒΩ)P + ŒΩP (cid:48) and rŒΩ = r + ŒΩ‚àÜr = (1

ŒΩ)r + ŒΩr(cid:48). We have

‚àí

‚àí

dŒ∑
dŒΩ

(cid:12)
(cid:12)
(cid:12)
(cid:12)ŒΩ=0

= œÄ[(‚àÜP )V + ‚àÜr].

Appendix B. Alternative Proof of MSVPG

This proof follows the similar derivation of Sutton and Barto (2018, Chapter 13). We
Ô¨Årst derive the policy gradient of Œ∂‚àí, and give the complete form of MSV gradient by

Œ∏Œæ‚àí =

Œ∏Œ∑

‚àá

Œ≤

‚àá

‚àí

‚àá

Œ∏Œ∂‚àí. Taking the gradient of V ¬µ
Œ∂‚àí

for any arbitrary s

, we have

‚àà S

s)Q¬µ
Œ∂‚àí

(s, a)

(cid:105)

¬µ(a

|

Œ∏V ¬µ
Œ∂‚àí

(s)
(cid:104) (cid:88)

‚àá
=

Œ∏

‚àá
(cid:88)

a

(cid:104)

‚àá

‚àá

‚àá

a
(cid:88)

(cid:104)

a
(cid:88)

(cid:104)

a

=

=

=

Œ∏¬µ(a

Œ∏¬µ(a

Œ∏¬µ(a

s)Q¬µ
Œ∂‚àí

s)Q¬µ
Œ∂‚àí

s)Q¬µ
Œ∂‚àí

|

|

|

(s, a) + ¬µ(a

(s, a) + ¬µ(a

(s, a) + ¬µ(a

|

|

|

Œ∏Q¬µ
Œ∂‚àí

s)

‚àá

(cid:105)
(s, a)

(cid:88)

P (cid:0)s(cid:48)

s, a(cid:1) (cid:0)(r

s)

‚àá

Œ∏

(cid:88)

s)

s(cid:48)

|
s, a)(cid:0)

s(cid:48)
P (s(cid:48)

|

Œ∑)2

‚àí ‚àí

‚àí

Œ∂‚àí + V ¬µ
Œ∂‚àí

(cid:0)s(cid:48)(cid:1)(cid:1) (cid:105)

2(r

Œ∑)‚àí

Œ∏Œ∑

‚àá

‚àí

‚àí

‚àí ‚àá

Œ∏Œ∂‚àí +

(cid:0)s(cid:48)(cid:1)(cid:1) (cid:105)
.

Œ∏V ¬µ
Œ∂‚àí

‚àá

Rephrasing the equation above, we obtain

Œ∏Œ∂‚àí =
(cid:104)

‚àá
(cid:88)

Œ∏¬µ(a

‚àá

a

s)Q¬µ
Œ∂‚àí

|

(s, a) + ¬µ(a

s)

|

(cid:88)

P (cid:0)s(cid:48)

s(cid:48)

s, a(cid:1) (cid:16)

|

Œ∏V ¬µ
Œ∂‚àí

(cid:0)s(cid:48)(cid:1)

‚àá

2(r

Œ∑)‚àí

Œ∏Œ∑

‚àá

‚àí

‚àí

(cid:17) (cid:105)

Œ∏V ¬µ
Œ∂‚àí

(s).

‚àí ‚àá

21

Ma, Ma, Xia, & Zhao

Taking the expectation under œÄ for both sides, we have

œÄ(s)

Œ∏V ¬µ
Œ∂‚àí

(s)

Œ∏Œ∂‚àí
(cid:88)

‚àá
=

(cid:88)

(cid:104)

œÄ(s)

=

œÄ(s)

s
(cid:88)

‚àí
s
(cid:88)

s
(cid:88)

‚àí

s

‚àá
(cid:88)
a ‚àá
(cid:88)

a

œÄ(s)

Œ∏¬µ(a

‚àá

|

s)Q¬µ
Œ∂‚àí

(s, a) + ¬µ(a

s)

|

(cid:88)

P (cid:0)s(cid:48)

s(cid:48)

s, a(cid:1) (cid:0)

‚àá

|

Œ∏V ¬µ
Œ∂‚àí

(cid:0)s(cid:48)(cid:1)

2(r

Œ∑)‚àí

‚àá

‚àí

‚àí

Œ∏Œ∑(cid:1) (cid:105)

a

Œ∏¬µ(a

s)Q¬µ
Œ∂‚àí

|

(s, a) +

(cid:88)

(cid:88)

œÄ(s)

s(cid:48)

s

(cid:88)

a

¬µ(a

|

s)P (cid:0)s(cid:48)

s, a(cid:1)

Œ∏V ¬µ
Œ∂‚àí

‚àá

|

(cid:0)s(cid:48)(cid:1)

¬µ(a

s)

|

(cid:88)

s(cid:48)

2(r

Œ∑)‚àí

Œ∏Œ∑

‚àá

‚àí

‚àí

(cid:88)

s

œÄ(s)

Œ∏V ¬µ
Œ∂‚àí

‚àá

(s).

(28)

By the deÔ¨Ånitions of œÄ and Œ∑‚àí, we have

œÄ (cid:0)s(cid:48)(cid:1) =

Œ∑‚àí =

(cid:88)

s
(cid:88)

s

œÄ(s)

œÄ(s)

(cid:88)

a
(cid:88)

a

¬µ(a

¬µ(a

|

|

s)P (cid:0)s(cid:48)

s, a(cid:1) ,

|

(cid:88)

(r

s)

s(cid:48)

Œ∑)‚àí.

‚àí

Substituting into the Equation 28, we have

Œ∏V ¬µ
Œ∂‚àí

(cid:0)s(cid:48)(cid:1)

‚àá

2Œ∑‚àí

Œ∏Œ∑

‚àá

‚àí

‚àí

(cid:88)

s

œÄ(s)

‚àá

Œ∏V ¬µ
Œ∂‚àí

(s)

Œ∏Œ∂‚àí =

‚àá

=

=

=

(cid:88)

s
(cid:88)

s
(cid:88)

s
(cid:88)

s

œÄ(s)

œÄ(s)

œÄ(s)

œÄ(s)

(cid:88)
a ‚àá
(cid:88)
a ‚àá
(cid:88)
a ‚àá
(cid:88)
a ‚àá

Œ∏¬µ(a

Œ∏¬µ(a

Œ∏¬µ(a

Œ∏¬µ(a

|

|

|

|

s)Q¬µ
Œ∂‚àí

(s, a) +

(cid:88)

œÄ (cid:0)s(cid:48)(cid:1)

s)Q¬µ
Œ∂‚àí

(s, a)

s)Q¬µ
Œ∂‚àí

(s, a)

‚àí

‚àí

(cid:104)
Q¬µ
Œ∂‚àí

s)

(s, a)

‚àí

s(cid:48)

2Œ∑‚àí

2Œ∑‚àí

Œ∏Œ∑

‚àá
(cid:88)

s
2Œ∑‚àíQ¬µ

Œ∏¬µ(a

|

s)Q¬µ

Œ∑ (s, a)

œÄ(s)

Œ∑ (s, a)

(cid:88)
a ‚àá
(cid:105)
.

Finally, applying the trick

log ¬µ =

‚àá

‚àá

¬µ/¬µ, we have

Œ∏Œ∂‚àí = Es‚àºœÄ,a‚àº¬µ

‚àá

(cid:104)
Q¬µ
Œ∂‚àí

(s, a)

‚àí

2Œ∑‚àíQ¬µ

Œ∑ (s, a)

(cid:105)
.

Thus, the MSVPG is given by

Œ∏Œæ

‚àá

‚àí

= Es‚àºœÄ,a‚àº¬µ

(cid:104)

(1 + 2Œ∑‚àí)Q¬µ

Œ∑ (s, a)

Œ≤Q¬µ
Œ∂‚àí

(cid:105)
(s, a)

.

‚àí

Appendix C. Experiment Details

C.1 The Setup of Portfolio Management Problem

The return of cash x0 = 0.01. The transition cost c = 0.05.

22

MSV Policy Optimization via Risk-Averse RL

Table 1: The transition matrix of asset 1

x1
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.09
0.05
0.04
0.04
0.00
0.07
0.02
0.03

0.05
0.02
0.03
0.04
0.02
0.02
0.04
0.03

0.25
0.33
0.26
0.20
0.16
0.16
0.14
0.09

0.24
0.22
0.24
0.28
0.24
0.19
0.19
0.19

0.18
0.17
0.18
0.26
0.27
0.25
0.18
0.23

0.05
0.09
0.07
0.08
0.11
0.14
0.20
0.15

0.10
0.06
0.12
0.03
0.15
0.12
0.17
0.14

0.04
0.06
0.06
0.07
0.05
0.05
0.06
0.14

Table 2: The transition matrix of asset 2

x2
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5

-0.2

-0.1

0

0.1

0.2

0.3

0.4

0.5

0.13
0.06
0.01
0.06
0.02
0.04
0.10
0.01

0.10
0.11
0.06
0.06
0.04
0.07
0.11
0.10

0.08
0.09
0.12
0.12
0.09
0.11
0.13
0.30

0.09
0.12
0.15
0.15
0.24
0.20
0.16
0.21

0.20
0.17
0.25
0.22
0.23
0.26
0.17
0.16

0.36
0.37
0.35
0.34
0.32
0.27
0.20
0.16

0.02
0.04
0.02
0.01
0.04
0.03
0.04
0.00

0.02
0.04
0.04
0.04
0.02
0.02
0.09
0.06

23

Ma, Ma, Xia, & Zhao

Appendix D. Hyper-parameters of MSVPO

Hyper-parameter

Network learning rate Œ≤
Network hidden sizes
Activation function
Optimizer
Batch size
Gradient Clipping
Clipping parameter Œµ
Optimization Epochs M
GAE parameter Œª
Average Value Constraint CoeÔ¨Écient in APO (Ma et al., 2021) ŒΩ

Value

3e-4
[64, 64]
Tanh
Adam
256
10
0.2
10
0.95
0.3

Table 3: Hyper-parameters sheet

References

Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Riedmiller, M. A.
(2018). Maximum a posteriori policy optimisation. In International Conference on
Learning Representations.

Achiam, J., Held, D., Tamar, A., & Abbeel, P. (2017). Constrained policy optimization. In

International Conference on Machine Learning, Vol. 70, pp. 22‚Äì31.

Berner, C., Brockman, G., Chan, B., Cheung, V., Dƒôbiak, P., Dennison, C., Farhi, D., Fischer,
Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement
learning. ArXiv preprint, abs/1912.06680.

Bisi, L., Sabbioni, L., Vittori, E., Papini, M., & Restelli, M. (2020). Risk-averse trust region
optimization for reward-volatility reduction. In International Joint Conference on
ArtiÔ¨Åcial Intelligence, pp. 4583‚Äì4589.

Bollerslev, T., Li, S. Z., & Zhao, B. (2020). Good volatility, bad volatility, and the cross section
of stock returns. Journal of Financial and Quantitative Analysis, 55 (3), 751‚Äì781.
Borkar, V. S., & Meyn, S. P. (2002). Risk-sensitive optimal control for Markov decision
processes with monotone cost. Mathematics of Operations Research, 27 (1), 192‚Äì209.
Briec, W., & Kerstens, K. (2009). Multi-horizon markowitz portfolio performance appraisals:

A general approach. Omega, 37 (1), 50‚Äì62.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba,

W. (2016). OpenAI Gym. ArXiv preprint, abs/1606.01540.

Cao, X.-R. (2007). Stochastic Learning and Optimization: A Sensitivity-Based Approach.

Springer.

Castro, D. D., Tamar, A., & Mannor, S. (2012). Policy gradients with variance related risk

criteria. In International Conference on Machine Learning, pp. 1651‚Äì1658.

24

MSV Policy Optimization via Risk-Averse RL

Chen, W., Li, D., Lu, S., & Liu, W. (2019). Multi-period mean‚Äìsemivariance portfolio
optimization based on uncertain measure. Soft Computing, 23 (15), 6231‚Äì6247.
Choobineh, F., & Branting, D. (1986). A simple approximation for semivariance. European

Journal of Operational Research, 27 (3), 364‚Äì370.

Chow, Y., & Ghavamzadeh, M. (2014). Algorithms for CVaR optimization in MDPs. In

Advances in Neural Information Processing Systems, pp. 3509‚Äì3517.

Chow, Y., Ghavamzadeh, M., Janson, L., & Pavone, M. (2017). Risk-constrained reinforcement
learning with percentile risk criteria. Journal of Machine Learning Research, 18 (1),
6070‚Äì6120.

Chow, Y., Tamar, A., Mannor, S., & Pavone, M. (2015). Risk-sensitive and robust decision-
making: a CVaR optimization approach. In Advances in Neural Information Processing
Systems, pp. 1522‚Äì1530.

Chung, K.-J. (1994). Mean-variance tradeoÔ¨Äs in an undiscounted MDP: the unichain case.

Operations Research, 42 (1), 184‚Äì188.

Delage, E., Kuhn, D., & Wiesemann, W. (2019). ‚ÄúDice‚Äù-sion‚Äìmaking under uncertainty:

When can a random decision reduce risk?. Management Science, 65 (7), 3282‚Äì3301.

Dulac-Arnold, G., Mankowitz, D., & Hester, T. (2019). Challenges of real-world reinforcement

learning. ArXiv preprint, abs/1904.12901.

Estrada, J. (2007). Mean-semivariance behavior: Downside risk and capital asset pricing.

International Review of Economics & Finance, 16 (2), 169‚Äì185.

Fei, Y., Yang, Z., Chen, Y., Wang, Z., & Xie, Q. (2020). Risk-sensitive reinforcement
learning: Near-optimal risk-sample tradeoÔ¨Ä in regret. In Advances in Neural Information
Processing Systems, Vol. 33, pp. 22384‚Äì22395.

Filar, J. A., Kallenberg, L. C., & Lee, H.-M. (1989). Variance-penalized Markov decision

processes. Mathematics of Operations Research, 14 (1), 147‚Äì161.

Garcƒ±a, J., & Fern√°ndez, F. (2015). A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research, 16 (1), 1437‚Äì1480.

Gosavi, A. (2014). Variance-penalized Markov decision processes: Dynamic programming and
reinforcement learning techniques. International Journal of General Systems, 43 (6),
649‚Äì669.

Hogan, W. W., & Warren, J. M. (1974). Toward the development of an equilibrium capital-
market model based on semivariance. Journal of Financial and Quantitative Analysis,
9 (1), 1‚Äì11.

Howard, R. A., & Matheson, J. E. (1972). Risk-sensitive Markov decision processes. Man-

agement science, 18 (7), 356‚Äì369.

Janner, M., Fu, J., Zhang, M., & Levine, S. (2019). When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems, pp.
12498‚Äì12509.

Li, D., & Ng, W.-L. (2000). Optimal dynamic portfolio selection: Multiperiod mean-variance

formulation. Mathematical Finance, 10 (3), 387‚Äì406.

25

Ma, Ma, Xia, & Zhao

Liu, Y.-J., & Zhang, W.-G. (2015). A multi-period fuzzy portfolio optimization model with
minimum transaction lots. European Journal of Operational Research, 242 (3), 933‚Äì941.

Ma, S., Ma, X., & Xia, L. (2022a). An optimistic value iteration for mean‚Äìvariance optimiza-
tion in discounted markov decision processes. Results in Control and Optimization, 8,
100165.

Ma, S., Ma, X., & Xia, L. (2022b). A uniÔ¨Åed algorithm framework for mean-variance
optimization in discounted Markov decision processes. ArXiv preprint, abs/2201.05737.

Ma, X., Tang, X., Xia, L., Yang, J., & Zhao, Q. (2021). Average-reward reinforcement
learning with trust region methods. In International Joint Conference on ArtiÔ¨Åcial
Intelligence, pp. 2797‚Äì2803.

Ma, X., Xia, L., Zhou, Z., Yang, J., & Zhao, Q. (2020). Dsac: distributional soft actor critic

for risk-sensitive reinforcement learning. ArXiv preprint, abs/2004.14547.

Markowitz, H., Todd, P., Xu, G., & Yamane, Y. (1993). Computation of mean-semivariance
eÔ¨Écient sets by the critical line algorithm. Annals of Operations Research, 45 (1),
307‚Äì317.

Markowitz, H. M. (1952). Portfolio selection. Journal of Finance, 7, 77‚Äì91.

Markowitz, H. M. (1959). Portfolio Selection: EÔ¨Écient DiversiÔ¨Åcation of Investments. John

Wiley & Sons, New York.

Mavrin, B., Yao, H., Kong, L., Wu, K., & Yu, Y. (2019). Distributional reinforcement learning
for eÔ¨Écient exploration. In International Conference on Machine Learning, Vol. 97, pp.
4424‚Äì4434.

Nagabandi, A., Konolige, K., Levine, S., & Kumar, V. (2020). Deep dynamics models for

learning dexterous manipulation. In Conference on Robot Learning, pp. 1101‚Äì1112.

Nemirovski, A., & Shapiro, A. (2007). Convex approximations of chance constrained programs.

SIAM Journal on Optimization, 17 (4), 969‚Äì996.

Prashanth, L., & Ghavamzadeh, M. (2016). Variance-constrained actor-critic algorithms for

discounted and average reward MDPs. Machine Learning, 105 (3), 367‚Äì417.

Ruszczy≈Ñski, A. (2010). Risk-averse dynamic programming for Markov decision processes.

Mathematical programming, 125 (2), 235‚Äì261.

Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., & Moritz, P. (2015). Trust region policy
optimization. In International Conference on Machine Learning, Vol. 37, pp. 1889‚Äì1897.

Schulman, J., Moritz, P., Levine, S., Jordan, M. I., & Abbeel, P. (2016). High-dimensional
continuous control using generalized advantage estimation. In International Conference
on Learning Representations.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy

optimization algorithms. ArXiv preprint, abs/1707.06347.

Shapiro, A., Dentcheva, D., & Ruszczynski, A. (2021). Lectures on stochastic programming:

modeling and theory. SIAM.

26

MSV Policy Optimization via Risk-Averse RL

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human
knowledge. Nature, 550 (7676), 354‚Äì359.

Sobel, M. J. (1982). The variance of discounted Markov decision processes. Journal of

Applied Probability, 19 (4), 794‚Äì802.

Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT press.
Tamar, A., Chow, Y., Ghavamzadeh, M., & Mannor, S. (2016). Sequential decision making
with coherent risk. IEEE Transactions on Automatic Control, 62 (7), 3323‚Äì3338.
Tamar, A., Glassner, Y., & Mannor, S. (2015). Optimizing the CVaR via sampling. In
Proceedings of the Twenty-Ninth AAAI Conference on ArtiÔ¨Åcial Intelligence, pp. 2993‚Äì
2999.

Todorov, E., Erez, T., & Tassa, Y. (2012). Mujoco: A physics engine for model-based
control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.
5026‚Äì5033.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,
D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft
ii using multi-agent reinforcement learning. Nature, 575 (7782), 350‚Äì354.

Wei, Q. (2019). Mean‚Äìsemivariance optimality for continuous-time Markov decision processes.

Systems & Control Letters, 125, 67‚Äì74.

Xia, L. (2016). Optimization of Markov decision processes under the variance criterion.

Automatica, 73, 269‚Äì278.

Xia, L. (2020). Risk-sensitive Markov decision processes with combined metrics of mean and

variance. Production and Operations Management, 29 (12), 2808‚Äì2827.

Xie, T., Liu, B., Xu, Y., Ghavamzadeh, M., Chow, Y., Lyu, D., & Yoon, D. (2018). A block
coordinate ascent algorithm for mean-variance optimization. In Advances in Neural
Information Processing Systems, Vol. 31, pp. 1073‚Äì1083.

Yan, W., Miao, R., & Li, S. (2007). Multi-period semi-variance portfolio selection: Model
and numerical solution. Applied Mathematics and Computation, 194 (1), 128‚Äì134.
Zhang, S., Liu, B., & Whiteson, S. (2021). Mean-variance policy iteration for risk-averse
reinforcement learning. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,
Vol. 35, pp. 10905‚Äì10913.

Zhang, W.-G., Liu, Y.-J., & Xu, W.-J. (2012). A possibilistic mean-semivariance-entropy
model for multi-period portfolio selection with transaction costs. European Journal of
Operational Research, 222 (2), 341‚Äì349.

Zhang, Y., & Ross, K. W. (2021). On-policy deep reinforcement learning for the average-
In International Conference on Machine Learning, Vol. 139, pp.

reward criterion.
12535‚Äì12545.

Zhou, F., Wang, J., & Feng, X. (2020). Non-crossing quantile regression for distributional
reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 33,
pp. 15909‚Äì15919.

27

