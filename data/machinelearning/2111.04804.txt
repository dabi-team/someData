1
2
0
2

v
o
N
8

]
S
D
.
s
c
[

1
v
4
0
8
4
0
.
1
1
1
2
:
v
i
X
r
a

Approximating Fair Clustering with Cascaded Norm Objectives

Eden Chlamt´aˇc∗

Yury Makarychev†

Ali Vakilian‡

Abstract

We introduce the (p, q)-Fair Clustering problem. In this problem, we are given a set of points
P and a collection of diﬀerent weight functions W . We would like to ﬁnd a clustering which
minimizes the ℓq-norm of the vector over W of the ℓp-norms of the weighted distances of points
in P from the centers. This generalizes various clustering problems, including Socially Fair
k-Median and k-Means, and is closely connected to other problems such as Densest k-Subgraph
and Min k-Union.

We utilize convex programming techniques to approximate the (p, q)-Fair Clustering problem
q, we get an O(k(p−q)/(2pq)), which nearly matches a
for diﬀerent values of p and q. When p
kΩ((p−q)/(pq)) lower bound based on conjectured hardness of Min k-Union and other problems.
p, we get an approximation which is independent of the size of the input for bounded
When q
p, q, and also matches the recent O((log n/(log log n))1/p)-approximation for (p,
)-Fair Clus-
tering by Makarychev and Vakilian (COLT 2021).

∞

≥

≥

1

Introduction

Clustering is one of the fundamental problems in various areas including theoretical computer
science, machine learning and operations research. Due to its broad range of applications, diﬀerent
researchers have considered diﬀerent cluster models. A typical model in theoretical computer science
is the centroid model which contains the classic k-Median, k-Means, k-Center and more generally,
k-Clustering with ℓp-norm objective. In k-Clustering with ℓp-norm objective, we are given a set of
points in a metric space and the goal is to ﬁnd a set of k centers C so as to minimize the total ℓp
distance of points to C. These problems are all known to be NP-hard and admit various eﬃcient
approximation algorithms [Gonzalez, 1985, Hochbaum and Shmoys, 1985, Charikar et al., 2002,
Kanungo et al., 2004, Gupta and Tangwongsan, 2008, Li and Svensson, 2016, Ahmadian et al.,
2020].

Recently, clustering has been widely studied under fairness constraints to prevent or alleviate
the bias and discrimination in the solution constructed by the existing algorithms [Chierichetti
et al., 2017, Ahmadian et al., 2019, Bera et al., 2019, Bercea et al., 2019, Backurs et al., 2019,
Schmidt et al., 2019, Huang et al., 2019, Kleindessner et al., 2019, Chen et al., 2019, Micha and
Shah, 2020, Jung et al., 2020, Mahabadi and Vakilian, 2020, Kleindessner et al., 2020, Brubach
et al., 2020]. In particular, the notion we consider in this paper is inspired by the notion of Socially
Fair Clustering introduced by Abbasi et al. [2021], Ghadiri et al. [2021]. In Socially Fair Clustering,
points in the input belong to diﬀerent groups and the goal is to pick k centers so as to minimize

∗Department of Computer Science, Ben-Gurion University. The work was done while the author was visiting and

supported in part by TTIC. Email: chlamtac@cs.bgu.ac.il

†Toyota Technological Institute at Chicago (TTIC). Supported by NSF awards CCF-1718820, CCF-1955173, and

CCF-1934843. Email: yury@ttic.edu

‡Toyota Technological Institute at Chicago (TTIC). Supported by NSF award CCF-1934843. Email: vakil-

ian@ttic.edu

1

 
 
 
 
 
 
the maximum clustering cost incurred to any of the diﬀerent (demographic) groups in the input.
We note that the objective of Socially Fair Clustering was also previously examined by Anthony
et al. [2010] in the context of Robust k-Clustering where a set of possible scenarios (i.e., a set of
clients) are given in the input and the goal is to ﬁnd a set of centers which is a good solution for
all scenarios.

Here, we consider a generalization of both Socially Fair Clustering and Robust Clustering which
is denoted as (p, q)-Fair Clustering and is formally deﬁned as below. Besides the fact that (p, q)-Fair
Clustering generalizes various known problems, the new formulation is a generalization of the classic
k-Clustering problem and understanding the complexity of the problem under diﬀerent values of p
and q is of interest in itself.

Deﬁnition 1.1 ((p, q)-Fair Clustering). An instance
consists of a metric space ([m], d) on
m points, n diﬀerent groups of points which are represented by non-negative weight functions
w1, . . . , wn (for each i
[n],
∈
the ℓp-cost of group i w.r.t. a set of centers C

R≥0) and a target number of centers k. For each i

[m] is deﬁned as

[n], wi : [m]

→

∈

I

⊆

m

1/p

costI

p (C, wi) =

wi(j)

·

d(j, C)p

,



(1)





Xj=1



where d(j, C) = minj′∈C d(j, j′). Moreover, the (ℓp, ℓq)-cost of a set of k centers C
⊆
as the ℓq-norm of the vector consisting the ℓp-cost of the groups w.r.t. C is deﬁned as

[m] is deﬁned

costI

p,q(C) =

n

Xi=1

costp(C, wi)q

!

1/q

n

m

q/p

1/q

= 

Xi=1








Xj=1

wi(j)

·

d(j, C)p





(2)


[m] so as to minimize costI




p,q(C).

In (p, q)-Fair Clustering, the goal is to ﬁnd a set of k centers C

⊆

Remark 1.2. In the above deﬁnition, we may omit the superscript
relative to which we compute the cost when it is clear from the context.

I

indicating the instance

∞

By setting q =

, (p, q)-Fair Clustering captures Socially Fair Clustering with ℓp-cost. More-
over, compared to Socially Fair Clustering, the formulation of (p, q)-Fair Clustering allows for a
“relaxed” way of enforcing the fairness requirement. In particular, by varying the value of q from
p to
, the objective interpolates between the objective of classic k-Clustering with ℓp-cost and
that of Socially Fair Clustering with ℓp-cost.1 Depending on how accurate the group membership
information is, the user can set the value of q accordingly and aim for a clustering with a “rea-
sonable” fairness constraint w.r.t. the extracted group membership information: the more accurate
the group membership information gets, the higher value can be assigned to q.

∞

We remark that the special case of (

, 1)-Fair Clustering was previously studied by Anthony
et al. [2010] under the name of Stochastic k-Center where the intuition is that we have a potential
set of (client) scenarios and know that one of them is likely to happen but do not know which
one. Anthony et al. proved that Stochastic k-Center is as hard to approximate as the Densest
, 1)-Fair Clustering with 0-1 weight functions can be seen to be
k-Subgraph.

In particular, (

∞

∞

1We remark that in Socially Fair Clustering with ℓp-cost as deﬁned by Makarychev and Vakilian [2021], the goal
is to minimize the maximum over all groups i of costp(C, wi)p. However, in (p, ∞)-Fair Clustering the goal is to
minimize the maximum over all groups i of costp(C, wi). Note that an α-approximation algorithm for (p, ∞)-Fair
Clustering implies an αp-approximation guarantee for Socially Fair Clustering with ℓp-cost and vice versa.

2

 
equivalent to the Min s-Union problem (a generalization of Densest k-Subgraph) [Chlamt´aˇc et al.,
2018, 2017], in which we are given a collection of m sets and an integer s
[m] and the goal is to
choose s sets from the input whose union has minimum cardinality. In addition to the hardness
result for Stochastic k-Center, Anthony et al. also provided an O(log m)-approximation for the
Stochastic k-Median problem which is equivalent to (1, 1)-Fair Clustering with arbitrary weight
functions w1,

, wn.

∈

In a diﬀerent direction, Goyal and Jaiswal [2021] designed constant factor approximation FPT

· · ·

algorithms for Socially Fair k-Median and k-Means and provided hardness results as well.

Remark 1.3. Chakrabarty and Swamy [2019] observed that a slightly modiﬁed variant of standard
existing algorithms for k-Median (e.g., Charikar et al. [2002]) can be applied to obtain a constant
factor approximation algorithm for the more general (weighted) k-Clustering with ℓp-cost (for
p
)). This can also be achieved by applying the algorithm of [Makarychev and Vakilian,
[1,
2021] for (p,

)-Fair Clustering when the number of groups is one.

∞

∈

∞

1.1 Our Results and Techniques.

In this paper, we design approximation algorithms for (p, q)-Fair Clustering for all values of p, q
[1,

). The following theorems are our main contributions in this work.

∞

∈

q). In the regime p

q, there exists a polynomial time algorithm that computes

Theorem 1.4 (p
p−q
2pq

an O

k

≥

(cid:16)

(cid:17)

-approximation for (p, q)-Fair Clustering.

≥

This approximation may be nearly optimal in the following sense. As we show in Appendix B,
assuming certain hardness conjectures for Min s-Union and related problems, it is in fact not
possible to get an algorithm for (p, q)-Fair Clustering in the regime p > q with an approximation
factor less than mΩ((p−q)/(pq)) (note that k
m). See Theorem B.2 for details. Moreover, when
k

m our algorithm achieves a much better approximation which is independent of m.

≤

≪

Theorem 1.5 (p

an O

q
ln (1+q/p)

≤
1/p

(cid:18)(cid:16)

(cid:17)

q). In the regime p

q, there exists a polynomial time algorithm that computes

≤

-approximation for (p, q)-Fair Clustering.

(cid:19)

Note that (p, p)-Fair Clustering (i.e., p = q) with 0-1 weight functions is equivalent to the
classic k-Clustering with ℓp-cost. In this setting, Theorem 1.5 gives a constant factor approximation
which is essentially optimal. This bound implies an O
)-Fair
Clustering2, which matches the recent approximation algorithm of [Makarychev and Vakilian, 2021]
and the hardness result of [Bhattacharya et al., 2014]. Thus, for any value of p, the approximation
guarantee of Theorem 1.5 smoothly interpolates between the optimal approximation bounds for
the previously studied special cases of q =

and q = p.
Finally, we remark that for any p, q it is possible to get an O(n|1/p−1/q|) approximation by
approximating the outer ℓq-norm in the formulation of (p, q)-Fair Clustering with ℓp-norm and
solving the obtained instance of (p, p)-Fair Clustering with one of the existing constant factor
approximation algorithms of k-Clustering with ℓp-cost.

( log n
log log n )1/p
(cid:16)

-approximation for (p,

∞

∞

(cid:17)

Overview of our algorithms. At a high-level, our approach is to solve a convex programming
relaxation of (p, q)-Fair Clustering and then round the fractional solution to get an approximate
integral solution. However, even coming up with an eﬃcient convex program of the problems is

2This is the case, since the (ℓp, ℓ∞)-cost objective is equal to the (ℓp, ℓlog n)-cost objective up to a constant factor.

3

≥

non-trivial and introducing such a relaxation is one of our contributions in this paper. While a
generalization of the standard LP relaxations for clustering problems (e.g., k-Median [Charikar
et al., 2002]) results in a natural convex programming relaxation of (p, q)-Fair Clustering when p
≤
q, it is less clear how to even come up with a valid convex program relaxation of (p, q)-Fair Clustering
q. In particular, the natural constraint we need to add is not convex
for the other scenario, p
q regime, we cannot use the natural convex relaxation, or even the stronger
when p > q. In the p
)-Fair Clustering used by Makarychev and
relaxation which generalizes the LP-relaxation for (p,
∞
Vakilian, since even the latter has a simple Ω(n(q−p)/q2
) integrality gap construction. To overcome
this polynomial lower-bound, we use the round-or-cut framework (see Carr et al. [2000], An et al.
[2017], Li [2017], Chakrabarty and Negahbani [2019]): we have an exponential family of constraints
with a separation oracle based on our rounding algorithm; we add a constraint from the family
only if the rounding algorithm detects that it is violated. These constraints are needed to bound
diﬀerent moments of the cost function applied to a set of clusters generated by the ﬁrst step of our
rounding.

≤

Next, we employ a slightly modiﬁed version of a reduction technique introduced by Charikar
et al. [2002] which yields a simpliﬁed instance of the same problem along with a corresponding
convex programming solution. After performing the reduction on the input instance and solution
to the corresponding convex program, we will get a sparsiﬁed instance (i.e., the number of points
becomes O(k)) along with an adjusted feasible solution that together satisfy several useful properties
which relate to our convex programming relaxations and are crucial for our rounding algorithm.
More details on the reduction and the properties guaranteed by it are provided in Section 3.

Finally, we perform a rounding procedure on the sparsiﬁed instance and obtain an approximate
integral solution. By the properties of the reduction, using the output integral solution for the
sparsiﬁed instance, we can construct an integral solution of the original instance without increasing
the cost by more than a constant factor. We remark that the analysis of our rounding algorithm
crucially relies on the properties guaranteed by the “non-standard” constraints we added to the
relaxations of the problem in both p

q regimes.

q and p

≤

≥

1.2 Paper Organization.

We start with providing the convex programming relaxations of (p, q)-Fair Clustering in Section 2.
In Section 3, we concisely state the properties of the reduction by [Charikar et al., 2002] that is
used in our rounding algorithm – its proof is deferred to Appendix A. Then, in Section 4, we
describe our rounding algorithm and in Sections 5 and 6 we analyze the approximation guarantee
of the rounding algorithm in the regimes p
q respectively. Finally, in Appendix B, we
q and p
explore the connection of (p, q)-Fair Clustering to Min s-Union and prove polynomial hardness of
approximation for the problem assuming standard hardness conjectures for Min s-Union.

≥

≤

2 Convex Programming Relaxations

We will use somewhat diﬀerent convex programming relaxations in the two parameter regimes,
q. However, the relaxations for both regimes will use the following common
when p
assignment/clustering polytope, which is often used for problems such as k-Median or Facility

q and p

≥

≤

4

Location:

k
P
cluster :=

(x, y)
{

[0, 1]m×m
m

∈

×

[0, 1]m satisfying the following

}

xjℓ = 1

Xℓ=1
xjℓ ≤

m

yℓ := xℓℓ

yj ≤

k

Xj=1
xjℓ ≥

0

j

∀

∈

[m]

(3)

j

∀

∈

[m], ℓ

[m]

j
\ {

}

∈

(4)

(5)

j, ℓ

∀

∈

[m]

(6)

In the intended solution, xjℓ is a 0-1 variable which is 1 if and only if ℓ is a center and j is

assigned to a cluster centered at ℓ. The variable yj = xjj is 1 if and only if j is a center.

2.1 Convex Relaxation for the Case p

q.

≥
Our algorithm will use the following convex program. We think of B as a ﬁxed parameter of the
convex program, which we can use in a binary search.

min B
n

s.t.

Bq

zi ≤

Xi=1
(x, y)

k
P
cluster

∈

m

m

zi ≥ 



Xj=1

wi(j)





Xj′=1

d(j, j′)qxjj′








p/q

q/p



[n]

i

∀

∈

(7)

(8)

(9)

The variable zi represents the cost incurred by group i, raised to the q. The natural constraint

to express the connection between zi and the clustering variables xjj′ would be

Xj′=1
However, this is not a convex constraint in the p > q regime, so we further relax this connection
and write Constraint (9) instead.

Xj=1



.

(10)

m

m

wi(j)

d(j, j′)pxjj′

q/p



zi ≥ 


2.2 Convex Relaxation for the Case p

q.

≤
To motivate our relaxation, ﬁrst consider the natural convex relaxation which is identical to the
relaxation in the previous section but with Constraint (9) replaced by the more natural Con-
straint (10) above (which is convex in the p
q regime). Consider even a simple case where all
distances are 1, and the points can be partitioned into sets (J1, J2) such that yj = 1 for every
J1 there is exactly one j′
ε, and
j
∈
xjj′′ = 0 for all j′′
. Also suppose every weight function wi is an indicator function for some
}
= t (see Figure 1). Note that in this case, the natural cost constraints
Pi|
set Pi ⊆
|

∈
j, j′
J1 of cardinality

J2 such that xjj′ = ε and yj = xjj = 1

J2, and for every j

6∈ {

≤

−

∈

5

1 − ε

j

J1

xjj′ = ε

d = 1

j′

J2

Figure 1: In this example, the natural convex program has a large integrality gap.

d(j, j′)p
would give us zi ≥
≥
would be natural to apply the following randomized rounding:

t)q/p and B

ε)q/p = (ε

j∈Pi

(

·

·

1/q

n
i=1 zi

n1/q

(ε

·

·

≥

t)1/p. It

P
J1 to center j′ independently with probability yjj′ = ε, and otherwise make

(cid:16)P

(cid:17)

• assign each j
j a center;

∈

• make each j

∈

J2 a center (since yj = 1).

Denote the obtained set of centers by C. Let us see what the cost of group i w.r.t. C is. We

have

E

costp(C, wi)p

= E

d(j, C)p

= εt.

h

i

hXj∈Pi

i
q/p

Now, we are interested in the expectation E
bound the expectation of costp,q(C)q (see formula (2)). Observe that if ε

, as out ultimate goal is to upper
t = Ω(log n), then

d(j, C)p

j∈Pi

(cid:17)

i

h(cid:16)P

d(j, C)p is concentrated around the mean and

·

j∈Pi

P

m

E

d(j, C)p

h(cid:16)

Xj∈Pi

q/p

∼

(cid:17)

i

(εt)q/p = O(zi),

as desired. However, if εt = o(1), then

m
j=1 d(j, C)p is close to a Poisson random variable with

m
j=1 d(j, C)p

P

q/p

is at least

∼

εt, which is much greater than

rate εt. Thus, the expectation of
(εt)q/p (in this regime). Thus,

(cid:16)P

n

m

d(j, C)p

E



h

Xi=1(cid:16)

Xj=1


(q−p)/(pq)

(cid:17)

q/p

(cid:17)

i





1/q

& n1/q(εt)1/q,

which is a factor of

1
ε·t

larger than our bound for B.

(cid:1)

(cid:0)

To overcome this potentially polynomially-large gap, we need to introduce diﬀerent constraints
to handle diﬀerent moments in the randomized rounding (we will use Lata la’s inequality in the
analysis, which basically allows us to handle the two cases above separately). For instance, to
decrease the integrality gap and improve the performance of the rounding w.r.t. the relaxation in
the above example when εt = o(1), we may want to add constraints of the form

zi ≥

m

Xj=1

(1

−

yj)wi(j)q/pd(j, [m]

j)q.

\

6

d(j, [m] \ U )

j

U

Figure 2: To compute voli(U ), we ﬁnd the distance from every point j in U to the closest point
outside of U .

Vℓ}
{

However, it is not enough to introduce such constraints for the original points, since we apply our
randomized rounding to an instance produced by the reduction of Charikar et al. Loosely speaking,
the reduction partitions the set of points [m] into O(k) initial clusters
; it is guaranteed that
there is a k-clustering that for each ℓ, assigns all points in Vℓ to the same center, and has cost
at most a constant times greater than the optimum clustering. In our algorithm, for every ℓ, we
will ﬁnd one center that serves all points in Vℓ using randomized rounding. For this approach to
work, we need to introduce new constraints that depend on the partition
. The challenge is
returned by the reduction depend on the convex program solution, so we do not
that sets
know them when we solve the convex program. To deal with this problem, we introduce a family
of exponentially-many constraints for all possible choices of sets
. Since we only need these
constraints to hold for the set of clusters arising in the reduction and not all possible collections
of clusters, we will check these constraints with a “rounding separation oracle.” That is, we will
only check that the constraints hold for the set of clusters arising in the reduction, and if they do
not, we will use a separating hyperplane to continue solving the convex program with the Ellipsoid
Method. In order to describe our non-standard constraints, we introduce the following deﬁnition
(see Figure 2).

Vℓ}
{

Vℓ}
{

Vℓ}
{

⊂

[m]. Let voli(U ) =

Deﬁnition 2.1. Consider a set of points U

j∈U wi(j)d(j, [m]
We brieﬂy discuss the motivation for this deﬁnition. Consider a set of points U

[m] (later
this set will be one of the clusters returned by the reduction). Assume that our algorithm opens a
m
set of centers C. By Deﬁnition 1.1, costp(C, wi)p =
j=1 wi(j)d(j, C)p. We want to lower bound
the contribution of points in U to costp(C, wi)p assuming that there are no centers from C in U .
U ) for
We show that this contribution is at least voli(U ). Indeed, observe that d(j, C)
every j

U . Therefore,

U , since C

d(j, [m]

[m]

P

P

⊂

≥

\

U )p.

\

∈

⊆

\
wi(j)d(j, C)p

Xj∈U

wi(j)d(j, [m]

\

U )p = voli(U ).

≥

Xj∈U

(11)

In the following claim we generalize this inequality for the case where we have many disjoint sets.

Claim 2.2. Consider a collection (Vℓ)ℓ∈Λ of pairwise disjoint subsets of [m], indexed by some set
[n], the following lower
of indices Λ. Let C
bounds on costp(C, wi) hold.

[m] be a set of centers. Then for every group i

⊆

∈

costp(C, wi)p

costp(C, wi)q

≥

≥

Xℓ∈Λ:Vℓ∩C=∅

Xℓ∈Λ:Vℓ∩C=∅

7

voli(Vℓ)

and

voli(Vℓ)q/p if p

q.

≤

(12)

(13)

Xℓ∈Λ
Vℓ|

Proof. Using Deﬁnition 1.1, inequality (11), and that sets Vℓ are pairwise disjoint, we get,

costp(C, wi)p =

wi(j)d(j, C)p

m

≥

wi(j)d(j, C)p

by (11)

≥

voli(Vℓ),

Xj=1

Xℓ∈Λ:Vℓ∩C=∅
as required. We obtain inequality (13) from inequality (12) by applying the inequality
(note that p

q) to the vector a = (voli(Vℓ))ℓ∈Λ:Vℓ∩C=∅.

Xℓ∈Λ:Vℓ∩C=∅ Xj∈Vℓ

a
k

a
k1 ≥ k

kq/p

≤

We rewrite inequalities (12) and (13) as follows.

q/p

costp(C, wi)q

costp(C, wi)q

max (0, 1

C

− |

∩

≥  

Xℓ∈Λ

≥

max (0, 1

C

− |

∩

Vℓ|

) voli(Vℓ)

Vℓ|
) (voli(Vℓ))q/p .

!

,

(14)

(15)

Note that the term max (0, 1
our convex relaxation, we will use the following notation for disjoint collections of sets:

) equals 1 if C

and equals 0 otherwise. To state

Vℓ =

− |

C

∩

∩

∅

Π(m) :=

(Λ, (Vℓ)ℓ∈Λ)
{

|

Λ

⊆

.
[m] and (Vℓ)ℓ∈Λ are disjoint subsets of [m]
}

We are now ready to present our relaxation for the case p

q.

≤

min B

s.t. (x, y)

k
P
cluster

n

∈
zi ≤

Bq

Xi=1
zi ≥

zi ≥

zi ≥

m

m

wi(j)

d(j, j′)pxjj′

(cid:18)

Xj=1

Xj′=1

q/p

(cid:19)

max

0, 1

−

(cid:18)Xℓ∈Λ

max

Xℓ∈Λ

(cid:0)
0, 1

−

(cid:0)

yj

Xj∈Vℓ
yj

·

Xj∈Vℓ

(cid:1)

q/p

voli(Vℓ)

·

(cid:19)
(cid:1)
voli(Vℓ)q/p

(16)

(17)

(18)

[n]

i

∀

∈

i

i

∀

∀

∈

∈

[n],

[n],

∀

∀

(Λ, (Vℓ)ℓ∈Λ)

(Λ, (Vℓ)ℓ∈Λ)

∈

∈

Π(m)

(19)

Π(m)

(20)

It follows from inequalities (14) and (15) that this is a valid relaxation.

As noted in the following remark, it will be easy to check Constraints (19) and (20) once the

set of clusters is ﬁxed.

Remark 2.3. The correctness of the reduction of Charikar et al. does not require Constraints (19)
and (20), so they do not need to be checked before applying the reduction. The reduction will yield a
set of centers K along with the corresponding Voronoi cells (Vℓ)ℓ∈K . To check that Constraints (19)
and (20) hold for these sets for any Λ

K, it suﬃces to check that they hold for Λ =

K

ℓ
{

∈

|

⊆

j∈Vℓ

.
yj < 1
}

P
Remark 2.4. Let us brieﬂy compare our relaxation with the one used by Makarychev and Vakilian
)-relaxation
for solving (p,

)-Fair Clustering (let us call the latter the (p,

)-relaxation). The (p,

∞

∞

∞

8

has Constraints (16)–(18) and additional non-standard constraints. These non-standard constraints
are essentially equivalent to Constraints (19) for a very special family of (Λ, (Vℓ)ℓ∈Λ):
= 1 and
set Vℓ is a ball around some point in [m] (note that Constraints (19) and (20) are equivalent when
)-relaxation has
Λ
|
|
polynomially-many constraints. However, if we simply adapted this relaxation to the (p, q)-Fair
Clustering problem, we would get a relaxation with a polynomially large integrality gap.

= 1). Since there are polynomially many diﬀerent balls in ([m], d), the (p,

Λ
|
|

∞

3 Reduction `a la Charikar et al.

We will use a slight modiﬁcation of the reduction, which was used by Charikar et al. to get the
ﬁrst constant factor approximation algorithm for k-Median. We use the same reduction to solve
the (p, q)-Fair Clustering problem in both regimes, p

q and p

p.

We will refer to a not-necessarily-optimal solution (x, y, z) that satisﬁes Constraints (7)–(9)
q as a fractional clustering solution (when

q and Constraints (16)–(18) but when p

≥

≤

≤

q, the solution might not satisfy Constraints (19) and (20).

when p
p

≤

≥

Theorem 3.1. There is a polynomial-time reduction that given an instance
tering, a fractional clustering solution (x, y, z) of value B, and a parameter γ
k
1−γ with weights w′
instance
ℓ

′ on a subset of points K

K), and a fractional clustering solution (x′, y′, z′) such that the following properties hold.

of (p, q)-Fair Clus-
(0, 1/2) returns an
[n] and

i(ℓ) (where i

[m] of size

K
|

I
∈

| ≤

⊂

∈

I

∈
1. The cost of (x′, y′, z′) is at most twice that of solution (x, y, z).

2. Let σ(ℓ)

K be the closest point to ℓ

K other than ℓ itself. The solution (x′, y′, z′) assigns

each point ℓ

K only to centers ℓ and σ(ℓ). Speciﬁcally, for all ℓ

∈

∈

∈

K:

∈

ℓℓ = y′
x′
ℓ;

x′
ℓσ(ℓ) = 1

y′
ℓ;

−

Further, x′

ℓσ(ℓ) ≤

γ.

x′
ℓj = 0 for j /

.
ℓ, σ(ℓ)
}

∈ {

3. Let

Vℓ}ℓ∈K be the Voronoi partition of [m] induced by the set of centers K (Voronoi sites).
{

Then, for every ℓ, y′

ℓ = min

1,

yj

.

j∈Vℓ

(cid:17)
4. The costs of combinatorial solutions for

P

(cid:16)

centers L

⊆

I
[m] there is a set of centers L′

I
K of cost

⊆

and

′ are related as follows: For every set of

costI ′

p,q(L′)

≤

2costI

p,q(L) +

4
γ1/ν

B,

(21)

and moreover for any set of centers L

K, the cost of L as a solution for

is bounded by

I

⊆

costI

p,q(L)

≤

costI ′

p,q(L) +

2
γ1/ν

B.

(22)

5. If Λ

K is a set of centers such that σ(ℓ)
is bounded by

w.r.t. instance

⊆

I

Λ for every ℓ

6∈

∈

Λ, then the cost of centers K

Λ

\

costI

p,q(K

Λ)

\

≤

6
γ1/ν

B + 2

n

Xi=1  

Xℓ∈Λ





voli(Vℓ)

!

q/p

1/q

.





9

Charikar et al. presented this reduction and proved that it satisﬁes properties 1–3 and (22). As
far as we know, Property (21) has not been explicitly stated or used before, but its proof is similar
to that of Property (22). Finally, Property 5 is new – we need it to use our new CP Constraints (19)
and (20). We will also use the following observation from [Charikar et al., 2002].

Observation 3.2. We can eﬃciently ﬁnd a partition (K1, K2) of K such that σ(K1)
σ(K2)

K1, and

K2,

⊆

⊆

x′
ℓσ(ℓ) ≥

K
|

| −
2

k

.

Xℓ∈K1

For completeness, we provide proofs of Theorem 3.1 and Observation 3.2 in Appendix A.

4 A Randomized Rounding

In this section, we describe two steps that will be used in the rounding algorithms for both parameter
regimes. All our algorithms will begin by applying the reduction from Theorem 3.1 for γ = 1/5.
Let K be the set of points obtained by applying this reduction, let (x′, y′, z′) be the corresponding
convex programming solution, as described in the theorem, and let K1 be as in Observation 3.2.
k, then the current centers already give a constant factor approximation (since
Note that if
their cost in the new instance is 0, and so by Property (22) in Theorem 3.1, the cost in the original
instance is at most 2B/γ1/ν
K in
> k below. Consider the
the original instance only reducing the cost). Thus, we assume that
following rounding:

10B (and we can simply add

k centers to K from [m]

K
|

K
|

K
|

| ≤

| −

≤

\

|

• Let

and note by Observation 3.2 that

K ′ =

K1

ℓ
(cid:26)

∈

x′
ℓσ(ℓ) ≥

K
k
| −
|
K1| (cid:27)
4
|

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x′
ℓσ(ℓ) ≥

Xℓ∈K ′

Xℓ∈K1

x′
ℓσ(ℓ) −

K
k
|
| −
K1|
4
|

K1 \
· |

K ′

| ≥

k

K
|

| −
2

k

K
|

| −
4

−

≥

K
|

| −
4

k

.

• Let L = K ′. Independently close (remove from L) every center ℓ

(recall that x′

ℓσ(ℓ) ≤

γ = 1/5 by item 2 of Theorem 3.1).

K ′ with probability 5x′

ℓσ(ℓ)

∈

• If

L
|

|

< k, reopen (add to L) an arbitrary collection of k

L

− |

|

centers at no additional cost.

The expected number of centers that we close in the second step is 5

5
k).
K
4 (
|
By a Chernoﬀ bound this number is at least
k is bounded by
| −
some suﬃciently large constant, in which case we can enumerate over all possible solutions to get
a constant factor approximation). While the analysis of the cost of this rounding depends on the
speciﬁc parameter regime we are in, we introduce the following notation which we will use in both
K ′, we deﬁne random variables specifying the per-center
cases. For every group i and center ℓ
∈
and total costs incurred to every group i

k centers w.h.p. (unless
P

[n] by this rounding:

ℓ∈K ′ x′
K
|

ℓσ(ℓ) ≥

K
|

| −

| −

∈

Ziℓ :=

(cid:26)

voli(Vℓ)
0

if we close center ℓ,
otherwise,

and

Zi =

Ziℓ.

(23)

Xℓ∈K ′

10

For the simpler analysis in Section 5, it will suﬃce to analyze the cost in the new instance

(produced by the reduction from Theorem 3.1), for which we deﬁne the following variables:

′

I

Z ′

iℓ :=

(cid:26)

i(ℓ)d(ℓ, σ(ℓ))p

w′
0

if we close center ℓ,
otherwise,

and

Z ′

i =

Z ′

iℓ.

(24)

Xℓ∈K ′

Claim 4.1. The (ℓp, ℓq)-cost in

I

of the clustering found by randomized rounding is bounded by3

30B + 2

min




1/q

n

Xi=1

Z q/p
i

!

, 10B +

(Z ′

i)q/p

n

Xi=1

1/q

!




Proof. Note that if we close a center ℓ then we do not close σ(ℓ), since (i) we only close centers ℓ
in K ′
K2. Therefore, we can apply item 5 of Theorem 3.1 with
Λ = K

K1 and (ii) if ℓ
L (the set of centers we closed). We get,


K ′ then σ(ℓ)



∈

∈

⊆
\

costI

p,q(L)

6
γ1/ν

≤

B + 2



n

Xi=1  

Xℓ /∈L



q/p

1/q

voli(Vℓ)

!



=

6
γ1/ν

B + 2

1/q

.

Z q/p
i

!

n

Xi=1

where ν = min(p, q), γ = 1/5, and thus 6/γ1ν

30. Also, the cost of L w.r.t. instance



≤

n



Xi=1  

Xℓ∈K

w′

i(ℓ)d(ℓ, L)p

q/p

1/q

!



n

= 



Xi=1

Xℓ∈K\L

q/p

1/q

w′

i(ℓ)d(ℓ, σ(ℓ))p





=


Here, we used that d(ℓ, L) = 0 if ℓ is not closed and d(ℓ, L) = d(ℓ, σ(ℓ)), otherwise. By Theorem 3.1,
item 4, the cost of L w.r.t. instance







is







I

costI

p,q(L)

2
γ1/ν

≤

B +

(Z ′

i)q/p

1/q

!

≤

n

Xi=1

10B +

(Z ′

i)q/p

1/q

.

!

n

Xi=1

5 Approximation Algorithm for the Case p

q

≥

In this section, we present our rounding algorithm for the convex program from Section 2.1.

k

We start out by applying the reduction described in Theorem 3.1. We now consider two cases.
√k, then we apply the randomized rounding from Section 4. Otherwise, we re-weight
If
K
|
the points and run an approximation algorithm for k-Clustering with ℓq-norm objective. Let us
ﬁrst analyze the the performance of randomized rounding when p

| −

≥

q.

≥

Lemma 5.1. The expected cost of the randomized rounding is at most O

B

3Note that terms 30B and 10B only add a constant to the approximation factor of the randomized rounding
scheme, since B is at most the cost of the optimal clustering. Thus, the main challenge will be to upper bound either

(cid:16)

K
(k/(
|

·

| −

p−q
pq

k))

.

(cid:17)

′ is

I

n

q/p

Z ′
i

Xi=1

(cid:0)

(cid:1)

1/q

.

!

1/q

n

i=1 Zq/p
i (cid:17)

(cid:16)P

or (cid:16)P

n

i=1(Z′

i)q/p

1/q

.

(cid:17)

11

 
 
 
 
 
 
Proof. By Claim 4.1, it suﬃces to bound the expectation of
By Jensen’s inequality, and linearity of expectation, we have

(cid:0)P

n

1/q

n

1/q

n

(cid:1)

1/q

n

E



(Z ′

i)q/p

!



E

≤  

"

(Z ′

i)q/p

#!

Xi=1

Xi=1



Let us now bound the expectation E[Z ′

=

E

(Z ′

i)q/p

Xi=1

h

!

i

≤  

Xi=1

1/q

!

q/p

E

Z ′
i

(cid:2)

(cid:3)

(25)

i]. First, recall that by deﬁnition of K ′ and Theorem 3.1,

n
i=1(Z ′

i)q/p

1/q

. Let us do that now.

for every ℓ

∈

K ′, we have

From the deﬁnition of Z ′

K
k
| −
|
K1|
4
|

x′
ℓσ(ℓ) ≥

| −
−
i, using that (x′, y′, z′) satisﬁes Constraint (9) by Theorem 3.1, we have

| −
5k

≥

≥

= |

.

(26)

K
|
4k/(1

k
γ)

k

K
| −
|
K
4
|
|

K

k

E[Z ′

i] =

E

Z ′
iℓ

= 5

x′
ℓσ(ℓ)w′

i(ℓ)d(ℓ, σ(ℓ))p

Xℓ∈K ′

Xℓ∈K ′

(cid:3)

(cid:2)
5k

p−q
q

5

≤

= 5

K
|

(cid:18)

| −

k

(cid:19)

5k

K
|

(cid:18)

k

p−q
q

Xℓ∈K ′
(z′

i)p/q.

| −
Plugging this bound back into (25), we get

(cid:19)

(x′

ℓσ(ℓ))p/qw′

i(ℓ)d(ℓ, σ(ℓ))p

by (26)

by Constraint (9)

n

(Z ′

i)q/p

Xi=1

1/q

!





E





5q/p

≤  

5k

p−q
p

K
|

(cid:18)

| −

k

(cid:19)

1/q

z′
i

!

= 51/p

5k

p−q
pq

K
|

(cid:18)

| −

k

(cid:19)

n

·

(
Xi=1

i)1/q,
z′

By Constraint (7) and Theorem 3.1 (

n
i=1 z′

i)1/q

≤

2B. We get

n

P
i)q/p

(Z ′

Xi=1

1/q

10

!



≤



E





5k

p−q
pq

K
|

(cid:18)

| −

k

(cid:19)

B,

·

as required.

Thus, as mentioned earlier, the randomized rounding indeed gives the desired approximation
√k. Let us see a diﬀerent rounding algorithm, which gives the desired guarantee
√k. In this rounding, we deﬁne a new weight function ˆw : K

R≥0 as follows:

k
k

when
when

K
|
K
|

| −
| −

≥
≤

→

ˆw(ℓ) :=

w′

i(ℓ)q/p.

n

Xi=1

), k-Clustering with ℓq-cost can be approximated up to a constant

Recall that for any q

[1,

∈

∞

factor (see Remark 1.3). Our rounding algorithm in this case is simple:

• Apply a constant-factor approximation for k-Clustering with ℓq-cost to the current input (on

K) with new weights ˆw and return the set of centers L chosen by this algorithm.

12

 
 
 
 
Let us analyze the approximation guarantee. Recall that the new instance (obtained by the
reduction from Theorem 3.1) has optimum value B′ = O(γ−1/q)(B∗ + B) = O(B∗), where B∗ is the
optimum value of the original instance. Then for the above algorithm applied to the new instance,
we have the following guarantee.

Lemma 5.2. The cost costI ′

p,q(L) is at most O

B′

p−q
pq

k)

.

K
(
|

·

| −

(cid:16)
Proof. Since we use a constant-factor approximation for the ℓq objective, it suﬃces to show that
for every set ˆK

K of k centers, the following values

(cid:17)

⊂

• the (ℓp, ℓq)-cost of ˆK w.r.t. the original weights w′

i (from the reduction of Theorem 3.1), and

• the ℓq-cost of ˆK w.r.t. the new weights ˆwi

K
are within a factor of (
|

| −

k)

p−q

pq of each other. Indeed, let ˆK

K be any set of k centers. Then

⊆

costI ′

p,q( ˆK) =

n


Xi=1  


n

Xℓ∈K

w′

i(ℓ)d(ℓ, ˆK)p

q/p

1/q

!



1/q



i(ℓ)q/pd(ℓ, ˆK)q
w′

!

since

k · kp/q ≤ k · k1

Xi=1 Xℓ∈K
n

≤  

=

(cid:18)Xℓ∈K  

Xi=1

w′

i(ℓ)q/p

!

d(ℓ, ˆK)q

1/q

,

(cid:19)

ˆw(ℓ)

which is exactly the ℓq objective of our new instance applied to this set of centers.

}
On the other hand, again for any set ˆK

{z

|

K of k centers, we can bound the q-norm objective

⊆

of the instance with weights ˆw as follows

ˆw(ℓ)d(ℓ, ˆK)q

(cid:18)Xℓ∈K

1
q

=

(cid:19)

=





Xℓ∈K\ ˆK
n

1
q

ˆw(ℓ)d(ℓ, ˆK)q

=







i(ℓ)q/pd(ℓ, ˆK)q
w′



n

w′

i(ℓ)q/pd(ℓ, ˆK)q

Xi=1

Xℓ∈K\ ˆK

1
q

1
q









Xi=1 Xℓ∈K\ ˆK

by H¨older

n

≤ 


ˆK

K
|

\

Xi=1





p−q
pq

|

1
q

q
p





p−q
p

p
p−q

1

Xℓ∈K\ ˆK

n









Xℓ∈K\ ˆK

w′

i(ℓ)d(ℓ, ˆK)p

1
q

q
p






Xi=1  


Xℓ∈K

i(ℓ)d(ℓ, ˆK)p
w′

K
= (
|

| −

!







p−q
pq costI ′

k)

p,q( ˆK),

=

and the proof follows.

Proof of Theorem 1.4. We solve the convex programming relaxation for the problem and then apply
the reduction from Theorem 3.1. If the set of points K found by the reduction is of cardinality

13

|

K

− |

| ≤

k, we add k

points from [m] and return the resulting set, which as noted earlier
K
|
k > √k, we run the randomized rounding procedure, which
has cost O(B). Otherwise, if
| −
by Lemma 5.1 yields an O(k(p−q)/(2pq))-approximation. Finally, if 0 <
√k, we run a
constant-factor approximation for the ℓq norm k-Clustering with weights ˆw and obtain a solution
′ and B∗ be the (ℓp, ℓq)-cost of the
L. Let B∗
optimal solution for

I ′ be the (ℓp, ℓq)-cost of the optimal solution for

K
|

K
|

| −

≤

I

k

I

. By Lemma 5.2,
costI ′

p,q(L)

O(k(p−q)/(2pq))B∗

I ′.

≤

By Theorem 3.1, item 4:

costp,q(L)

≤
(22)

≤
(21)

≤

costI ′

p,q(L) + 10B

O(k(p−q)/(2pq))B∗

I ′ + 10B

O(k(p−q)/(2pq))(2B∗ + 20B) + 10B = O(k(p−q)/(2pq))B∗,

here we used that B
required.

≤

B∗. We conclude that the algorithm gives an O(k

p−q
pq ) approximation, as

6 Approximation Algorithm for the Case p

q

≤

In this section, we upper bound the cost of the solution produced by the rounding procedure. In
the analysis, we will use Lata la’s inequality.

Theorem 6.1 (Lata la [1997], Corollary 3). There exists a universal constant M such that if
Z1, . . . , ZN are independent non-negative random variables and α

1, then

E

N

Zi

h(cid:16)

Xi=1

1/α

α

!

(cid:17)

i

M α
ln(1 + α)

≤

max

Xi=1
(cid:16)

N

E[Zi],

≥
N

1/α

E[Z α
i ]

.

(cid:17)

(cid:16)

Xi=1
M q
p ln(1+q/p)

(cid:17)
q/p

. Then,

We will use Lata la’s inequality with α = q/p. Denote Mpq =

N

q/p

N

E

Zi

E[Zi]
Xi=1
(cid:17)
The algorithm for (p, q)-Fair Clustering in the case p
q simply solves the convex problem, applies
the reduction from Theorem 3.1, and then runs the randomized rounding procedure. We denote
the obtained set of centers by L. Now we are ready to prove the main result of this section.

Xi=1

Xi=1

Mpq

(27)

(cid:16)(cid:16)

h(cid:16)

+

≤

≤

(cid:17)

(cid:17)

i

]

.

(cid:16)

N

q/p

(cid:17)
E[Z q/p
i

Lemma 6.2. The rounding procedure outputs a solution for

in expectation.

of cost at most O

I

q
ln(1+q/p)

1/p

B

(cid:17)

(cid:19)

(cid:18)(cid:16)

Proof. We use random variables Ziℓ deﬁned in (23). By Claim 4.1, the cost of the solution for

′ found by the rounding procedure is
instance
cost using Lata la’s inequality (27). For every i

I

n
i=1

ℓ∈K ′ Ziℓ

q/p

1/q

[m], we have
(cid:16)P
∈

(cid:0)P

(cid:17)

(cid:1)

. We upper bound this

E

q/p

Ziℓ

h(cid:16) Xℓ∈K ′

(cid:17)

i

Mpq

≤

E

Ziℓ

q/p

+

(cid:16)

(cid:2) Xℓ∈K ′

(cid:3)(cid:17)

Xℓ∈K ′

E[Z q/p
iℓ

]
!

14

 
 
Now,

n

E

q/p

Ziℓ

h

Xi=1(cid:16) Xℓ∈K ′

Using that E[Ziℓ] = 5x′

ℓσ(ℓ) ·
n

Mpq

≤

i

(cid:17)
vol(Vℓ), we get

n

E
Xi=1(cid:16)

n

q/p

+

Ziℓ

(cid:2) Xℓ∈K ′

(cid:3)(cid:17)

Xi=1 Xℓ∈K ′

E[Z q/p
iℓ

]
!

E

Ziℓ

Xi=1(cid:16)

(cid:2) Xℓ∈K ′

n

5q/p

q/p

≤

(cid:3)(cid:17)
ℓσ(ℓ) = 1

Xi=1(cid:16) Xℓ∈K ′
y′
ℓ = max(0, 1

−

x′
ℓσ(ℓ) ·

voli(Vℓ)

q/p

.

(cid:17)

−

j∈Vℓ

yj), so from Constraint (19)

By items 2 and 3 of Theorem 3.1, x′
we get

We also have,

n

n

E

Ziℓ

q/p

Xi=1(cid:16)

(cid:2) Xℓ∈K ′

(cid:3)(cid:17)

P
5q/pBq.

5q/p

n

Xi=1

zi ≤

≤

n

E

Z q/p
iℓ

5q/p

≤

x′
ℓσ(ℓ) ·

voli(Vℓ)q/p.

Xi=1 Xℓ∈K ′

(cid:2)

(cid:3)

Xi=1 Xℓ∈K ′

And by the same argument as above, but using Constraint (20), we also get

n

E

Z q/p
iℓ

n

5q/p

≤

5q/pBq.

zi ≤

Xi=1 Xℓ∈K ′
We conclude that the expected cost of the clustering is at most,

Xi=1

(cid:3)

(cid:2)

n

E

"

Xi=1(cid:16) Xℓ∈K ′
(cid:16)

Ziℓ

q/p

1/q

Jensen’s
inequality

(cid:17)

(cid:17)

#

≤ 



n

E

Xi=1  
h

Xℓ∈K ′

q/p

1/q

Ziℓ

!

i





O

M 1/q
pq
(cid:16)

B = O

(cid:17)

(cid:16)(cid:16)

q
ln(1 + q/p)

1/p

B.

(cid:17)

(cid:17)

≤

Proof of Theorem 1.5. We solve the convex programming relaxation for the problem, apply the
reduction, and run the randomized rounding procedure. This gives us a solution of cost at most

O

q
ln(1+q/p)

1/p

B in expectation.

(cid:18)(cid:16)

(cid:17)
Acknowledgments

(cid:19)

The authors would like to thank the anonymous reviewers for their helpful suggestions on improving
the notation and presentation of these results.

References

M. Abbasi, A. Bhaskara, and S. Venkatasubramanian. Fair clustering via equitable group represen-
tations. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages
504–514, 2021.

15

 
S. Ahmadian, A. Epasto, R. Kumar, and M. Mahdian. Clustering without over-representation. In
Proceedings of the SIGKDD International Conference on Knowledge Discovery & Data Mining,
pages 267–275, 2019.

S. Ahmadian, A. Norouzi-Fard, O. Svensson, and J. Ward. Better guarantees for k-means and

euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, 49(4), 2020.

H.-C. An, M. Singh, and O. Svensson. LP-based algorithms for capacitated facility location. SIAM

Journal on Computing, 46(1):272–306, 2017.

B. Anthony, V. Goyal, A. Gupta, and V. Nagarajan. A plant location guide for the unsure:
Approximation algorithms for min-max location problems. Mathematics of Operations Research,
35(1):79–101, 2010.

A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering.

In Proceedings of the International Conference on Machine Learning, pages 405–413, 2019.

S. Bera, D. Chakrabarty, N. Flores, and M. Negahbani. Fair algorithms for clustering. In Advances

in Neural Information Processing Systems, pages 4955–4966, 2019.

I. O. Bercea, M. Groß, S. Khuller, A. Kumar, C. R¨osner, D. R. Schmidt, and M. Schmidt. On
In Approximation, Randomization, and Combinatorial

the cost of essentially fair clusterings.
Optimization. Algorithms and Techniques, 2019.

A. Bhaskara, M. Charikar, V. Guruswami, A. Vijayaraghavan, and Y. Zhou. Polynomial integrality
In Proceedings of the Symposium on

gaps for strong sdp relaxations of densest k-subgraph.
Discrete Algorithms, pages 388–405, 2012.

S. Bhattacharya, P. Chalermsook, K. Mehlhorn, and A. Neumann. New approximability results
for the robust k-median problem. In Scandinavian Workshop on Algorithm Theory, pages 50–61,
2014.

B. Brubach, D. Chakrabarti, J. Dickerson, S. Khuller, A. Srinivasan, and L. Tsepenekas. A pairwise
fair and community-preserving approach to k-center clustering. In Proceedings of the Interna-
tional Conference on Machine Learning, pages 1178–1189, 2020.

R. D. Carr, L. K. Fleischer, V. J. Leung, and C. A. Phillips. Strengthening integrality gaps for
capacitated network design and covering problems. In Proceedings of the Symposium on Discrete
algorithms, pages 106–115, 2000.

D. Chakrabarty and M. Negahbani. Generalized center problems with outliers. ACM Transactions

on Algorithms (TALG), 15(3):1–14, 2019.

D. Chakrabarty and C. Swamy. Approximation algorithms for minimum norm and ordered opti-
mization problems. In Proceedings of the Symposium on Theory of Computing, pages 126–137,
2019.

M. Charikar, S. Guha, ´E. Tardos, and D. B. Shmoys. A constant-factor approximation algorithm
for the k-median problem. Journal of Computer and System Sciences, 65(1):129–149, 2002.

X. Chen, B. Fain, L. Lyu, and K. Munagala. Proportionally fair clustering. In Proceedings of the

International Conference on Machine Learning, pages 1032–1041, 2019.

16

F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. In

Advances in Neural Information Processing Systems, pages 5036–5044, 2017.

E. Chlamt´aˇc, M. Dinitz, and Y. Makarychev. Minimizing the union: Tight approximations for
small set bipartite vertex expansion. In Proceedings of the Symposium on Discrete Algorithms,
pages 881–899, 2017.

E. Chlamt´aˇc, M. Dinitz, C. Konrad, G. Kortsarz, and G. Rabanca. The densest k-subhypergraph

problem. SIAM Journal on Discrete Mathematics, 32(2):1458–1477, 2018.

E. Chlamt´aˇc and P. Manurangsi. Sherali-Adams Integrality Gaps Matching the Log-Density Thresh-
old. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech-
niques, volume 116, pages 10:1–10:19, 2018.

M. Ghadiri, S. Samadi, and S. Vempala. Socially fair k-means clustering. In Proceedings of the

Conference on Fairness, Accountability, and Transparency, pages 438–448, 2021.

T. F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical computer

science, 38:293–306, 1985.

D. Goyal and R. Jaiswal. FPT approximation for socially fair clustering.

arXiv preprint

arXiv:2106.06755, 2021.

A. Gupta and K. Tangwongsan. Simpler analyses of local search algorithms for facility location.

arXiv preprint arXiv:0809.2554, 2008.

D. S. Hochbaum and D. B. Shmoys. A best possible heuristic for the k-center problem. Mathematics

of operations research, 10(2):180–184, 1985.

L. Huang, S. Jiang, and N. Vishnoi. Coresets for clustering with fairness constraints. In Proceedings

of the Conference on Neural Information Processing Systems, 2019.

C. Jung, S. Kannan, and N. Lutz. A center in your neighborhood: Fairness in facility location. In
Proceedings of the Symposium on Foundations of Responsible Computing, page 5:1–5:15, 2020.

T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu. A
local search approximation algorithm for k-means clustering. Computational Geometry, 28(2-3):
89–112, 2004.

M. Kleindessner, P. Awasthi, and J. Morgenstern. Fair k-center clustering for data summarization.
In Proceedings of the International Conference on Machine Learning, pages 3448–3457, 2019.

M. Kleindessner, P. Awasthi, and J. Morgenstern. A notion of individual fairness for clustering.

arXiv preprint arXiv:2006.04960, 2020.

R. Lata la. Estimation of moments of sums of independent real random variables. The Annals of

Probability, 25(3):1502–1513, 1997.

S. Li. On uniform capacitated k-median beyond the natural LP relaxation. ACM Transactions on

Algorithms (TALG), 13(2):1–18, 2017.

S. Li and O. Svensson. Approximating k-median via pseudo-approximation. SIAM Journal on

Computing, 45(2):530–547, 2016.

17

S. Mahabadi and A. Vakilian. Individual fairness for k-clustering. In Proceedings of the International

Conference on Machine Learning, pages 6586–6596, 2020.

Y. Makarychev and A. Vakilian. Approximation algorithms for socially fair clustering. In Proceed-

ings of the Conference on Learning Theory, pages 3246–3264. PMLR, 2021.

E. Micha and N. Shah. Proportionally fair clustering revisited. In Proceedings of the International

Colloquium on Automata, Languages, and Programming, pages 85:1–85:16, 2020.

M. Schmidt, C. Schwiegelshohn, and C. Sohler. Fair coresets and streaming algorithms for fair k-
means. In Proceedings of the International Workshop on Approximation and Online Algorithms,
pages 232–251, 2019.

A Proof of Theorem 3.1 and Observation 3.2

Proof. (Proof of Theorem 3.1.) Let ν = min(p, q). We deﬁne the Convex Program (CP) cost of
point j

[m] as

∈

C(j, x)

C(j) =

≡



xjj′d(j, j′)ν



1/ν

.

(28)

Xj′∈[m]
We sort all points according to the value of C(j). Renaming the points if necessary, we may assume
that





C(1)

C(m).

≤ · · · ≤
Now we choose a subset of points K and assign each point to exactly one point in K. Initially,
K = ∅ and all points are unassigned. Then we process points one by one, starting with 1 and
ending with m. When we process point j, we perform the following steps if j has not been assigned
to any vertex j′ yet.

• Add j to K and assign j to j.

• For every unassigned j′ > j, if d(j, j′)

2

γ1/ν C(j′), assign j′ to j.

≤

After all the points are processed, each of them is assigned to some ℓ

assigned to themselves). For ℓ

K, let

∈

• Uℓ be the set of points assigned to ℓ.

K (some points are

∈

• Vℓ be the set of points that are closer to ℓ than to any other point in K (we break ties

arbitrarily).

• σ(ℓ) be the closest point to ℓ in K other than ℓ itself. We break ties arbitrarily but consistently;

then the set of edges (ℓ, σ(ℓ)) forms a forest.

For a set of points A

⊆

[m], denote wi(A) =

j∈A wi(j). Deﬁne new weights w′

ℓ for ℓ

K by

∈

w′

i(ℓ) = wi(Uℓ) =

P

wi(j).

Xj∈Uℓ

18

We obtained the desired instance
we deﬁne the CP solution.

I

′ of (p, q)-Fair Clustering on K with weights

w′
{

i(j)

}j∈K . Now

y′
ℓ = min(1,

yj)

ℓℓ = y′
x′
ℓ;

x′
ℓσ(ℓ) = 1
−
i = 2qzi
z′

y′
ℓ;

Xj∈Vℓ

x′
ℓj = 0

for j /

ℓ, σ(ℓ)
}

∈ {

We verify that all the required properties hold.

We verify that (x′, y′, z′) is a fractional clustering solution and item 1 holds. From the
deﬁnition of (x′, y′, z′), it follows right away that Constraints (3), (4), (5), and (6) are satisﬁed. We
need to check that Constraints (9) and (18). We rewrite CP Constraints (9) and (18) uniformly
using notation C(j).

Note that for all j

zp/q
i ≥

Xj∈[m]

Uℓ, j

≥

∈
wi(j)C(j)p =

Now consider ℓ
∈
d(ℓ, ℓ′). Therefore,

K and j /
∈

wi(j)C(j)p

q/p

.



(29)

zi ≥ 


Xj

ℓ and therefore C(ℓ)



C(j). We have,

wi(j)C(ℓ)p =

w′

i(ℓ)C(ℓ)p.

Xℓ∈K Xj∈Uℓ
Vℓ. Let us say j

Xℓ∈K Xj∈Uℓ
Vℓ′. Then d(ℓ, j)

Xℓ∈K

d(ℓ′, j) and d(ℓ, j) + d(ℓ′, j)

≥

≥

≤
wi(j)C(j)p

≥

∈
d(ℓ, ℓ′)/2

≥
here we used that σ(ℓ) is the closest to ℓ point in K other than ℓ itself. We have,

≥

d(ℓ, j)

d(ℓ, σ(ℓ))/2

(30)

C(ℓ)ν =

d(ℓ, j)ν xℓj ≥

Xj∈[m]

d(ℓ, j)ν xℓj ≥

Xj /∈Vℓ

d(ℓ, σ(ℓ))
2

ν

(cid:19)

xℓj =

d(ℓ, σ(ℓ))
2

(cid:18)

ν

xℓj.

(cid:19)

Xj /∈Vℓ

Xj /∈Vℓ (cid:18)

If y′

ℓ < 1 then

xℓj = 1

If y′

ℓ = 1, then

j /∈Vℓ

P

Xj /∈Vℓ
xℓj ≥

0 = 1

−

−

xℓj ≥

1

−

yj = 1

ℓ = x′
y′

ℓσ(ℓ).

−

Xj∈Vℓ
ℓ = x′
y′

Xj∈Vℓ
ℓσ(ℓ). In either case,

xℓj ≥

1

−

ℓ = x′
y′

ℓσ(ℓ).

Xj /∈Vℓ

(31)

Similarly to (28), deﬁne C ′(ℓ, x′)

C ′(ℓ) =

ℓ′∈K x′

ℓℓ′d(ℓ, ℓ′)ν

1/ν = d(ℓ, σ(ℓ))

≡

(x′

ℓσ(ℓ))1/ν . Then,

·

C(ℓ)ν

(cid:0)P
1
2ν d(ℓ, σ(ℓ))ν x′
C ′(ℓ)/2 and hence zp/q

≥

ℓσ(ℓ) =

(cid:1)
1
2ν C ′(ℓ)ν.

1
2p

ℓ∈K w′

i(ℓ)C ′(ℓ)p. Therefore,

i ≥

We conclude that C(ℓ)

≥

(z′

i)p/q = 2p

zp/q
i ≥

·

2p

1
2p

·

P

w′

i(ℓ)C ′(ℓ)p =

Xℓ∈K

19

w′

i(ℓ)C ′(ℓ)p,

Xℓ∈K

as required by CP Constraints (9) and (18). Note that since z′
is at most twice that of (x, y, z).

i = 2qzi, cost B′ of solution (x′, y′, z′)

Now we verify that item 2 holds and
K
|
y′
their deﬁnitions. First, we show that 1
ℓ ≤
get

−

k/(1
| ≤
γ. By (31), 1

−

γ). Formulas for x′ and y′ follow from
xℓj. Applying (30), we

1

y′
ℓ ≤

−

Xj /∈Vℓ

xℓj ≤ P

xℓjd(ℓ, j)ν
j /∈Vℓ
(d(ℓ, σ(ℓ))/2)ν ≤

−

j /∈Vℓ

y′
ℓ ≤
P
(2C(ℓ))ν
d(ℓ, σ(ℓ))ν .

Now, both points ℓ and σ(ℓ) are in K. Therefore, neither ℓ was assigned to σ(ℓ) nor σ(ℓ) was
2C(ℓ)
assigned to ℓ. This means that d(ℓ, σ(ℓ)) > 2
γ1/ν max(C(ℓ), C(σ(ℓ)))
γ1/ν . We conclude that
γ) follows from the following inequality
K
1
|
k

γ, as required. Finally, the bound
.
|

y′
ℓ ≤
ℓ∈K y′

K
γ)
|

k/(1

−
≥

ℓ ≥

| ≤

(1

≥

−

−
It is immediate that item 3 holds.

P

Now we verify that item 4 holds. We ﬁrst prove inequality (21). Consider a set of centers
[m]. Note that points in L are not necessarily in K. So L is not necessarily a valid set of
L
′. However, we can think of L as a set of Steiner centers and then compute
centers for instance
the cost of L with respect to instance

′. Formally, we write

⊂

I

I

n


Xi=1  


Xℓ∈K

w′

i(ℓ)

·

d(ℓ, L)p

!

q/p

1/q





n

= 




Xi=1

n

≤ 


R deﬁned by

Xi=1

Consider function

: Rm

→

k · k

n

m

d(ℓ, L)p

q/p

1/q










(d(ℓ, j) + d(j, L))p

wi(j)

wi(j)

·

·









Xℓ∈K Xj∈Uℓ

Xℓ∈K Xj∈Uℓ

q/p

1/q










q/p

1/q

n

q/p

1/q

v
k

k

= 

Xi=1




wi(j)

p

vj|
· |









Xj=1



= 




Xi=1








Xℓ∈K Xj∈Uℓ

wi(j)

p

vj|
· |





.






Rm to (wi(1)1/pv1, . . . , wi(j)1/pvj, . . . , wi(m)1/pvm). Note
is a seminorm on Rm. In particular,

. Therefore,

∈

Let Λi be a linear map that sends v
that

=

Λ1v
k

kp, . . . ,

Λnv
k

v
k

kp

k

q

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

n






Xi=1





wi(j)

·

(d(ℓ, j) + d(j, L))p

Xℓ∈K Xj∈Uℓ

n

≤ 



Xi=1





Xℓ∈K Xj∈Uℓ

wi(j)

·

d(ℓ, j)p









Xℓ∈K Xj∈Uℓ

wi(j)

·

d(j, L)p





Xi=1




The second term on the right is simply the cost of L with respect to instance
costI

p,q(L). Now, we upper bound the ﬁrst term. Since every j

I

Uℓ is assigned to ℓ, d(ℓ, j)

and thus equals

∈

≤

q/p

1/q






k · k

q/p

1/q










q/p

1/q

n



+ 




20

q/p

1/q










q/p

1/q



2
γ1/ν C(j). Thus the ﬁrst term is upper bounded by

n





Xi=1

Xℓ∈K Xj∈Uℓ


We conclude that




wi(j)

2
γ1/ν

·

(cid:16)

p

C(j)

(cid:17)

by (29)

≤

2
γ1/ν  

n

1/q

zi

!

Xi=1

2
γ1/ν

≤

B.

n

Xi=1  

Xℓ∈K





w′

i(ℓ)

·

d(ℓ, L)p

!



≤

costI

p,q(L) +

2
γ1/ν

B.

Now we construct a proper solution L′ for K. For every point in j
point ℓ in K (breaking ties arbitrarily) and add it to L′. It is immediate that for every ℓ
d(ℓ, L′)

L, we choose a closest to j
K,

2d(ℓ, L). Thus,

∈

∈

≤

costI ′

p,q(L′) =

n

(cid:16)

Xi=1(cid:16)Xℓ∈K

w′

i(ℓ)

·

d(ℓ, L′)p

q/p

1/q

2

costI

p,q(L) +

≤

2
γ1/ν

B

.

(cid:17)

(cid:17)

(cid:16)
′ of cost at most 2costI

(cid:17)
p,q(L) + 4
γ1/ν B.

As required, we constructed a feasible solution for

I
Now we prove that inequality (22) holds. The proof is almost identical to that of inequality (21).

n

costI

p,q(L) =

(cid:16)

Xi=1 (cid:16) Xj∈[m]
n

wi(j)

·

d(j, L)p

q/p

1/q

=

n

(cid:17)

(cid:17)

(cid:16)

Xi=1 (cid:16) Xℓ∈K Xj∈Uℓ
1/q

q/p

wi(j)

·

d(j, L)p

q/p

1/q

(cid:17)

(cid:17)

≤

≤

(cid:16)

Xi=1 (cid:16) Xℓ∈K Xj∈Uℓ
n

(cid:16)

Xi=1 (cid:16) Xℓ∈K Xj∈Uℓ

wi(j)

wi(j)

·

·

(d(ℓ, j) + d(ℓ, L))p

d(ℓ, j)p

q/p

1/q

+

(cid:17)

(cid:17)

n

wi(j)

(cid:17)

(cid:16)

Xi=1 (cid:16) Xℓ∈K Xj∈Uℓ
(cid:17)
γ1/ν B; the second term equals costI ′

2

q/p

1/q

d(ℓ, L)p

·

(cid:17)

(cid:17)

p,q(L). Therefore,

As we showed above, the ﬁrst term is at most

costI

p,q(L)

≤

costI ′

p,q(L) +

2
γ1/ν

B.

Now we verify that item 5 holds. Take a point ℓ
point to j outside Vℓ and let ℓ′ be the closest point in K to it (so j′

Λ and point j

∈

Vℓ. Let j′ be the closest

∈
Vℓ′). Then we have

∈

d(j, σ(ℓ))

since σ(ℓ)

Λ

d(j, K

Λ)

\

≤

≤

≤

≤

d(j, ℓ) + d(ℓ, σ(ℓ))
d(j, ℓ) + d(ℓ, ℓ′)
d(j, ℓ) + 2d(j′, ℓ)
d(j, ℓ) + 2d(j′, j) + 2(j, ℓ)

≤
= 3d(j, ℓ) + 2d(j, j′) = 3d(j, K) + 2d(j, [m]

by triangle inequality

6∈

by choice of σ(ℓ)

by (30)

by triangle inequality

Vℓ)

\

Of course, if ℓ

have

K

\

∈

Λ and j

∈

Vℓ, then d(j, K

\

Λ) = d(j, K). Thus, for any center ℓ

K we

∈

d(j, K

Λ)

\

≤

3d(j, K) + 2

1ℓ∈Λ ·

·

d(j, [m]

Vℓ).

\

21

Using that

k · k

is a seminorm as in the proof of item 4, we get

costI

p,q(K

n

m

Λ) =

Xi=1(cid:16)
(cid:16)
m

Xj=1

n

\

3

wi(j)

·

d(j, K

Λ)p

\

q/p

1/q

(cid:17)

(cid:17)
n

wi(j)

·

d(j, K)p

n

Xj=1

≤

Xi=1(cid:16)
(cid:16)
= 3costI

p,q(K) + 2

q/p

1/q

(cid:17)

(cid:17)

voli(Vℓ)

+ 2

Xi=1(cid:16)Xℓ∈Λ Xj∈Vℓ
(cid:16)
1/q

q/p

wi(j)

·

d(j, [m]

Vℓ)p

\

q/p

1/q

(cid:17)

(cid:17)

By item 4, the cost of K w.r.t.
instance

′, which is 0, plus 2γ−1/ν B. We conclude that

instance

I

I

(cid:16)

Xi=1(cid:16)Xℓ∈Λ

(cid:17)

(cid:17)
, that is, costI

p,q(K), is at most the cost of K w.r.t.

costp,q(K

Λ)

\

≤

6γ−1/ν B + 2
(cid:16)

n

voli(Vℓ)

q/p

1/q

.

Xi=1(cid:16)Xℓ∈Λ

(cid:17)

(cid:17)

Proof. (Proof of Observation 3.2.) Recall that σ(ℓ) is the closest to ℓ point in K other than ℓ itself.
We break ties arbitrarily but consistently so that the set of edges (ℓ, σ(ℓ)) forms a forest on K.
We choose an arbitrary root in every tree in the forest (K, (ℓ, σ(ℓ)). Then we let K1 be the set
of vertices of odd depth and K2 be the set of vertices of even depth. Clearly, σ(K1)
K2 and
σ(K2)

K1. Now,

⊆

⊆

x′
ℓσ(ℓ) =

Xℓ∈K

Xℓ∈K

(1

−

yℓ) =

K
|

| −

yℓ =

K
|

| −

k.

Xℓ∈K
. We are done if the former inequality

x′
x′
Therefore,
ℓσ(ℓ) ≥
ℓσ(ℓ) ≥
holds. Otherwise, we simply swap K1 and K2.

ℓ∈K1

ℓ∈K2

or

|K|−k
2

|K|−k
2

P

P

B Connection to Min s-Union and Conjectured Hardness

Let us see how known hardness conjectures for other problems imply polynomial hardness of ap-
proximation for (p, q)-Fair Clustering when q < p

Ω(1).

In the Min s-Union problem, we are gives a collection of m sets S1, . . . , Sm ⊆
J
|

[n], and a
= s as to
parameter s
minimize the cardinality of their union
. It has been conjectured (see Chlamt´aˇc et al.
[2017]) that the following distinguishing problem, slightly rephrased here, is hard (and implies
polynomial hardness of approximation for Min s-Union):

[m], and wish to ﬁnd a collection J
j∈J Sj|

[m] of (indices of) sets of size

S

⊆

∈

|

|

−

Conjecture B.1 (Dense versus Random Conjecture). For any constants 0 < ε < δ < 1, it is hard
to distinguish between the following cases when s

≤
• Random: n = mε and each of the sets S1, . . . , Sm is sampled independently by selecting

√m:

10 ln m elements in [n] independently at random.

• Dense: n = 10mε ln m, and an adversary may select the sets, with an additional guarantee
= (10s ln m)δ, and for every
J
|
|
I (that is, there are s input sets contained in a single set of cardinality ˜O(sδ)).

that there exist sets J
j

[n] such that

[m] and I

= s and

I
|

⊆

⊆

|

J, Sj ⊆

∈

22

This conjecture is supported by Sherali-Adams integrality gaps (cf. Chlamt´aˇc et al. [2017],
Chlamt´aˇc and Manurangsi [2018]) and is related to similar conjectures for related problems, and
in particular to the Projection Games Conjecture. Bhaskara et al. [2012] show a somewhat weaker
(but still polynomial) lower bound for this problem is also supported by a matching integrality gap
in the Sum of Squares hierarchy.

A straightforward reduction gives the following:

1
Theorem B.2. For any 1
2 , if the Dense versus
Random Conjecture holds for δ, ε, then (p, q)-Fair Clustering is hard to approximate to within less
than mε(1−δ)(p−q)/(pq).

p and constants 0 < ε < δ < 1, where ε

≤

≤

≤

q

Proof. Let us see a reduction from Min s-Union with these parameters to (p, q)-Fair Clustering,
and analyze the optimum clustering value in both cases.

Given an instance (S1, . . . , Sm, s) of Min s-Union, construct a clustering instance as follows:
Identify the points [m] with the sets S1, . . . , Sm and let the distance between any two points be 1.
For all i
s.
In the context of the reduction, let us consider instances for which s = mε(

[m], let wi(j) = 1i∈Sj . Finally, let the target number of centers be k = m

[n] and j

√m).

−

∈

∈

Consider ﬁrst the random case. By a Chernoﬀ bound, w.h.p., for any set J
= s = mε, we have

m of cardinality

⊆

≤

J
|

|

[j∈J
Thus, for any set of centers K of cardinality

= k, the cost of K is bounded from below by

|

mε ln n.

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

Sj(cid:12)
(cid:12)
(cid:12)
(cid:12)
K
(cid:12)
(cid:12)
|

n

m





Xj=1

q/p

1/q

1i∈Sj d(j, K)p





1/q



q/p


Sj}|

!

j
|{

∈

[m]

K

i

|

∈

\

1

i∈Sj∈[m]\K Sj

1/q

!

1/q

1
2

≥

(cid:18)

mε ln m

1/q

.

(cid:19)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[j∈[m]\K

Sj(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

costp,q(K) = 

Xi=1
n




Xi=1
n

=

≥  

Xi=1

On the other hand, consider the dense case. Let J be the “dense” set as deﬁned in the conjecture,

23

 
and let K = [m]

\

J. Then as before, the cost of these centers is bounded from above by

costp,q(K) =

1/q

j
|{

J

i

|

∈

∈

q/p

Sj}|

!

n

Xi=1





=

=

j
|{

J

i

|

∈

∈

q/p

Sj}|

Xi∈Sj∈J Sj

1/q

j
|{

J

i

|

∈

∈

q/p

Sj}|

!

Xi∈I

1/q





I
|

(q−p)/p
|

≤ 



Xi∈I

j
|{

J

i

|

∈

∈

Sj}|!

1/p

since

j

∀

J : Sj ⊆

I

∈

by H¨older

q/p

1/q





I
≤ |

(q−p)/(pq)
|



Xj∈J
(10s ln m)δ(p−q)/(pq)(10s ln m)1/p



≤
= (10mε ln m)(δ(p−q)+q)/(pq).

Sj|
|


Thus, if the conjecture holds for these parameters, then (p, q)-Fair Clustering cannot be ap-
proximated to within less than the ratio between these bounds (the random optimum divided by
the dense optimum), which is at least

20−1/q(10mε ln m)(1/q)−(δ(p−q)+q)/(pq) = 20−1/q(10mε ln m)(1−δ)(p−q)/(pq)

mε(1−δ)(p−q)/(pq).

≥

While not every setting of 0 < ε < δ < 1 with ε

1
2 in the conjecture is supported by the
same evidence (the Sum of Squares result, for instance, gives a much smaller polynomial gap), it is
worth noting that any setting yields a lower bound of mΩ((p−q)/(pq)) for (p, q)-Fair Clustering, and
in particular, if ε and δ are arbitrarily close to 1
2 , as is permissible in Conjecture B.1, then we get
4 −η) p−q
a lower bound of m( 1

for any constant η > 0.

≤

pq

24

 
 
 
