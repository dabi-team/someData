Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies

Giulia Zarpellon,1* Jason Jo,2, 3 Andrea Lodi,1 Yoshua Bengio2, 3
1Polytechnique Montr´eal
2Mila
3Universit´e de Montr´eal
giulia.zarpellon@polymtl.ca, jason.jo.research@gmail.com, andrea.lodi@polymtl.ca, yoshua.bengio@mila.quebec

1
2
0
2

n
u
J

2

]

G
L
.
s
c
[

4
v
0
2
1
5
0
.
2
0
0
2
:
v
i
X
r
a

Abstract

Branch and Bound (B&B) is the exact tree search method
typically used to solve Mixed-Integer Linear Programming
problems (MILPs). Learning branching policies for MILP has
become an active research area, with most works proposing
to imitate the strong branching rule and specialize it to dis-
tinct classes of problems. We aim instead at learning a policy
that generalizes across heterogeneous MILPs: our main hy-
pothesis is that parameterizing the state of the B&B search
tree can aid this type of generalization. We propose a novel
imitation learning framework, and introduce new input fea-
tures and architectures to represent branching. Experiments
on MILP benchmark instances clearly show the advantages
of incorporating an explicit parameterization of the state of
the search tree to modulate the branching decisions, in terms
of both higher accuracy and smaller B&B trees. The resulting
policies signiﬁcantly outperform the current state-of-the-art
method for “learning to branch” by effectively allowing gen-
eralization to generic unseen instances.

1

Introduction

Many problems arising from transportation, healthcare, en-
ergy and logistics can be formulated as Mixed-Integer Lin-
ear Programming (MILP) problems, i.e., optimization prob-
lems in which some decision variables represent discrete or
indivisible choices. A MILP is written as

{cT x : Ax ≥ b, x ≥ 0, xi ∈ Z ∀i ∈ I},

(1)

min
x

where A ∈ Rm×n, b ∈ Rm, c, x ∈ Rn and I ⊆ {1, . . . , n}
is the set of indices of variables that are required to be inte-
gral, while the other ones can be real-valued. Note that one
can consider a MILP as deﬁned by (c, A, b, I); we do not as-
sume any special combinatorial structure on the parameters
c, A, b. While MILPs are in general N P-hard, MILP solvers
underwent dramatic improvements over the last decades
(Lodi 2009; Achterberg and Wunderling 2013) and now
achieve high-performance on a wide range of problems.
The fundamental component of any modern MILP solver
is Branch and Bound (B&B) (Land and Doig 1960), an ex-
act tree search method. Following a divide-and-conquer ap-
proach, B&B partitions the search space by branching on

*Corresponding author. Code, data and supplementary materi-
als can be found at: https://github.com/ds4dm/branch-search-trees
Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

variables’ values and smartly uses bounds from problem re-
laxations to prune unpromising regions from the tree. The
B&B algorithm actually relies on expertly-crafted heuris-
tic rules for its two most fundamental decisions: branching
variable selection (BVS) and node selection. In particular,
BVS is a crucial factor for B&B’s success (Achterberg and
Wunderling 2013), and will be the main focus of the present
article.

Understanding why B&B works has been called “one of
the mysteries of computational complexity theory” (Lipton
and Regan 2012), and there currently is no mathematical the-
ory of branching; to the best of our knowledge, the only at-
tempt in formalizing BVS is the recent work of Le Bodic
and Nemhauser. One central reason why B&B is difﬁcult to
formalize resides in its inherent exponential nature: millions
of BVS decisions could be needed to solve a MILP, and a
single bad one could result in a doubled tree size and no
improvement in the search. Such a complex and data-rich
setting, paired with a lack of formal understanding, makes
B&B an appealing ground for machine learning (ML) tech-
niques, which have lately been thriving in discrete optimiza-
tion (Bengio, Lodi, and Prouvost 2018). In particular, there
has been substantial effort towards “learning to branch”,
i.e., in using ML methods to learn BVS policies (Lodi and
Zarpellon 2017). Up to now, most works in this area focused
on learning branching policies by supervision or imitation
of strong branching (SB), a valid but expensive heuristic
scheme (see Sections 2 and 5). The latest and state-of-the-
art contribution to “learning to branch” (Gasse et al. 2019)
frames BVS as a classiﬁcation problem on SB expert de-
cisions, and employs a graph-convolutional neural network
(GCNN) to represent MILPs via their variable-constraint
structure. The resulting branching policies improve on the
solver by specializing SB to different classes of synthetic
problems, and the attained generalization ability is to simi-
lar MILP instances (within the same class), possibly larger
in formulation size.

The present work seeks a different type of generalization
for a branching policy, namely a systematic generalization
across heterogeneous MILPs, i.e., across problems not be-
longing to the same combinatorial class, without any restric-
tion on the formulation’s structure and size. To achieve this
goal, we parameterize BVS in terms of B&B search trees.
On the one hand, information about the state of the B&B

 
 
 
 
 
 
tree – abundant yet mostly unexploited by MILP solvers –
was already shown to be useful to learn resolution patterns
shared across general MILPs (Fischetti, Lodi, and Zarpel-
lon 2019). On the other hand, the state of the search tree
ought to have a central role in BVS – which ultimately de-
cides how the tree is expanded and hence how the search
itself proceeds. In practice, B&B continually interacts with
other algorithmic components of the solver to effectively
search the decision tree, and some algorithmic decisions
may be triggered depending on which phase the optimiza-
tion is in (Berthold, Hendel, and Koch 2017). In a highly
integrated framework, a branching variable should thus be
selected among the candidates based on its role in the search
and its various components. Indeed, state-of-the-art heuris-
tic branching schemes employ properties of the tree to make
BVS decisions, and the B&B method equipped with such
branching rules has proven to be successful across widely
heterogeneous instances.

Motivated by these considerations, our main hypothesis
is that MILPs share a higher order structure in the space of
B&B search trees, and parameterized BVS policies should
learn in this representational space. We setup a novel learn-
ing framework to investigate this idea. First of all, there is
no natural input representation of this underlying space. Our
ﬁrst contribution is to craft input features of the variables
that are candidates for branching: we aim at representing
their roles in the search and its dynamic evolution. The di-
mensionality of such descriptions naturally changes with the
number of candidates at every BVS step. The deep neural
network (DNN) architecture that we propose learns a base-
line branching policy (NoTree) from the candidate variables’
representations and effectively deals with varying input di-
mensions. Taking this idea further, we suggest that an ex-
plicit representation of the state of the search tree should
condition the branching criteria, in order for it to ﬂexibly
adapt to the tree evolution. We contribute such tree-state pa-
rameterization, and incorporate it to the baseline architec-
ture to provide context over the candidate variables at each
given branching step. In the resulting policy (TreeGate) the
tree state acts as a control mechanism to drive a top-down
modulation (speciﬁcally, feature gating) of the highly muta-
ble space of candidate variables representations. By training
in our hand-crafted input space, the signal-to-noise ratio of
the high-level branching structure shared amongst general
MILPs is effectively increased, enabling our TreeGate pol-
icy to rapidly infer these latent factors and dynamically com-
pose them via top-down modulation. In this sense, we learn
branching from parameterizations of B&B search trees that
are shared among general MILPs. To the best of our knowl-
edge, the present work is the ﬁrst attempt in the “learning to
branch” literature to represent B&B search trees for branch-
ing, and to establish such a broad generalization paradigm
covering many classes of MILPs.

We perform imitation learning (IL) experiments on a
curated dataset of heterogeneous instances from standard
MILP benchmarks. We employ as expert rule relpscost,
the default branching scheme of the optimization solver
SCIP (Gleixner et al. 2018), to which our framework is inte-
grated. Machine learning experimental results clearly show

the advantage of the policy employing the tree state (Tree-
Gate) over the baseline one (NoTree), the former achieving
a 19% improvement in test accuracy. When plugged in the
solver, both learned policies compare well with state-of-the-
art branching rules. The evaluation of the trained policies
in the solver also supports our idea that representing B&B
search trees enables learning to branch across generic MILP
instances: over test instances, the best TreeGate policy ex-
plores on average trees with 27% less nodes than the best
NoTree one. In contrast, the GCNN framework of Gasse
et al. that we use as benchmark does not appear to be able
to attain such broad generalization goal: often the GCNN
models fail to solve heterogeneous test instances, explor-
ing search trees that are considerably bigger than those we
obtain. The comparison thus remarks the advantage of our
fundamentally new architectural paradigm – of represent-
ing candidates’ role in the search and using a tree-context to
modulate BVS – which without training in a class-speciﬁc
manner nor focusing on constraints structure effectively al-
lows learning across generic MILPs.

2 Background
Simply put, the B&B algorithm iteratively partitions the so-
lution space of a MILP (1) into sub-problems, which are
mapped to nodes of a binary decision tree. At each node,
integrality requirements for variables in I are dropped, and
a linear programming (LP) (continuous) relaxation of the
problem is solved to provide a valid lower bound to the op-
timal value of (1). When the solution x∗ of a node LP re-
laxation violates the integrality of some variables in I, that
node is further partitioned into two children by branching
i /∈ Z}
on a fractional variable. Formally, C = {i ∈ I : x∗
deﬁnes the index set of candidate variables for branching
at that node. The BVS problem consists in selecting a vari-
able j ∈ C in order to branch on it, i.e., create child nodes
according to the split

j (cid:101).

xj ≤ (cid:98)x∗

j (cid:99) ∨ xj ≥ (cid:100)x∗

(2)
Child nodes inherit a lower bound estimate from their par-
ent, while (2) ensures x∗ is removed from their solution
spaces. After extending the tree, the algorithm moves on to
select a new open node, i.e., a leaf yet to be explored (node
selection): a new relaxation is solved, and new branchings
happen. When x∗ satisﬁes integrality requirements, then it
is actually feasible for (1), and its value provides a valid
upper bound to the optimal one. Maintaining global up-
per and lower bounds allows one to prune large portions
of the search space. During the search, ﬁnal leaf nodes are
created in three possible ways: by integrality, when the re-
laxed solution is feasible for (1); by infeasibility of the sub-
problem; by bounds, when the comparison of the node’s
lower bound to the global upper one proves that its sub-tree
is not worth exploring. An optimality certiﬁcate is reached
when the global bounds converge. See (Wolsey 1998; Lodi
2009) for details on B&B and its combination with other
components of a MILP solver.

Branching rules Usually, candidates are evaluated with
respect to some scoring function, and j is chosen for branch-

ing as the (or a) score-maximizing variable (Achterberg,
Koch, and Martin 2005). The most used criterion in BVS
measures variables depending on the improvement of the
lower bound in their (prospective) child nodes. The strong
branching (SB) rule (Applegate et al. 1995) explicitly com-
putes bound gains for C. The procedure is expensive, but ex-
perimentally realizes trees with the least number of nodes.
Instead, pseudo-cost (PC) (Benichou et al. 1971) maintains
a history of variables’ branchings, averaging past improve-
ments to get a proxy for the expected gain. Fast in evalua-
tion, PC can behave badly due to uninitialization, so com-
binations of SB with PC have been developed. In reliability
branching, SB is performed until PC scores for a variable
are deemed reliable proxies of bound improvements. In hy-
brid branching (Achterberg and Berthold 2009), PC scores
are combined with other ones measuring the variables’ role
on inference and conﬂict clauses. Many other scoring crite-
ria have been proposed, and some of them are surveyed in
Lodi and Zarpellon from a ML perspective.

State-of-the-art branching rules can in fact be interpreted
as mechanisms to score variables based on their effective-
ness in different search components. While hybrid branch-
ing explicitly combines ﬁve scores reﬂecting variables’ be-
haviors in different search tasks, the evaluation performed
by SB and PC can also be seen as a measure of how effec-
tive a variable is – in the single task of improving the bound
from one parent node to its children. Besides, one can as-
sume that the importance of different search functionalities
should change dynamically during the tree exploration. 1 In
this sense, our approach aims at learning a branching rule
that takes into account variables’ roles in the search and the
tree evolution itself to perform a more ﬂexible BVS, adapted
to the search stages.

3 Parameterizing B&B Search Trees
The central idea of our framework is to learn BVS by means
of parameterizing the underlying space of B&B search trees.
We believe this space can represent the complexity and the
dynamism of branching in a way that is shared across hetero-
geneous problems. However, there are no natural parameter-
ization of BVS or B&B search trees. To this end, our con-
tribution is two-fold: 1) we propose hand-crafted input fea-
tures to describe candidate variables in terms of their roles
in the B&B process, and explicitly encode a “tree state” to
provide a richer context to variable selection; 2) we design
novel DNN architectures to integrate these inputs and learn
BVS policies.

3.1 Hand-crafted Input Features
At each branching step t, we represent the set of vari-
ables that are candidates for branching by an input matrix
Ct ∈ R25×|Ct|. To capture the multiple roles of a variable
throughout the search, we describe each candidate xj, j ∈ Ct
in terms of its bounds and solution value in the current sub-
problem. We also feature statistics of a variable’s participa-
tion in various search components and in past branchings.

1Indeed, a “dynamic factor” adjusts weights in the default

branching scheme of SCIP (relpscost).

In particular, the scores that are used in the SCIP default
hybrid-branching formula are part of Ct.

Additionally, we create a separate parameterization
Tree t ∈ R61 to describe the state of the search tree. We
record information of the current node in terms of depth and
bound quality. We also consider the growth rate and the com-
position of the tree, the evolution of global bounds, aggre-
gated variables’ scores, statistics on feasible solutions and
on bound estimates and depths of open nodes.

All features are designed to capture the dynamics of the
B&B process linked to BVS decisions, and are efﬁciently
gathered through a customized version of PySCIPOpt (Ma-
her et al. 2016). Note that {Ct, Tree t} are deﬁned in a way
that is not explicitly dependent on the parameters of each
instance (c, A, b, I). Even though Ct naturally changes its
dimensionality at each BVS step t depending on the highly
variable Ct, the ﬁxed lengths of the vectors enable training
among branching sets of different sizes (see 3.2). The rep-
resentations evolve with the search: t-SNE plots (van der
Maaten and Hinton 2008) in Figures 1(a) and 1(b) synthesize
the evolution of Tree t throughout the B&B search, for two
different MILP instances. The pictures clearly show the high
heterogeneity of the branching data across different search
stages. A detailed description of the hand-crafted input fea-
tures is reported in the supplementary material (SM).

3.2 Architectures to Model Branching

We use parameterizations Ct as inputs for a baseline DNN
architecture (NoTree). Referring to Figure 2, the 25-feature
input of a candidate variable is ﬁrst embedded into a repre-
sentation with hidden size h; subsequently, multiple layers
reduce the dimensionality from h to an inﬁmum INF by
halving it at each step. The vector of length INF is then
compressed by global average pooling into a single scalar.
The |Ct| dimension of Ct is conceived (and implemented)
as a “batch dimension”: this makes it possible to handle
branching sets of varying sizes, still allowing the parame-
ters of the nets to be shared across problems. Ultimately, a
softmax layer yields a probability distribution over the can-
didate set Ct, according to which a variable is selected for
branching.

We incorporate the tree-state input to the baseline archi-
tecture to provide a search-based context over the muta-
ble branching sets. Practically, Tree t is embedded in a se-
ries of subsequent layers with hidden size h. The output
of a ﬁnal sigmoid activation is g ∈ [0, 1]H , where H =
h + h/2 + · · · + INF denotes the total number of units of the
NoTree layers. Separate chunks of g are used to modulate
by feature gating the representations of NoTree: [g1, . . . , gh]
controls features at the ﬁrst embedding, [gh+1, . . . , gh+h/2]
acts at the second layer, . . . , and so on, until exhausting
[gH−INF , . . . , gH ] with the last layer prior the average pool-
ing. In other words, g is used as a control mechanism on vari-
ables parameterization, gating their features via a learned
tree-based signal. The resulting network (TreeGate) models
the high-level idea that a branching scheme should adapt to
the tree evolution, with variables’ selection criteria dynami-
cally changing throughout the tree search.

(a)

(b)

(c)

Figure 1: Evolution of Tree t throughout B&B as synthesized by t-SNE plots (perplexity=5), for instances (a) eil33-2 and (b)
seymour1. (c) Histogram of |Ct| in train, validation and test data.

3.4 Systematic Generalization
Our aim is to explore systematic generalization in learn-
ing to branch. We measure systematic generalization by
evaluating how well trained policies perform on never-seen
heterogeneous MILP instances. To begin, we remark that
relpscost is a sophisticated ensemble of expertly-crafted
heuristic rules that are dependent on high-level branching
factors. Due to the systematic generalization failures cur-
rently plaguing even state-of-the-art DNNs (Lake and Ba-
roni 2017; Johnson et al. 2017; Goodfellow, Shlens, and
Szegedy 2015), we do not believe that current learning al-
gorithms are capable of inferring these high-level branching
factors from the raw MILP formulation data alone. Instead,
by opting to train BVS policies in our hand-crafted input
feature space: (1) the signal-to-noise ratio of the underly-
ing branching factors is effectively increased, and (2) the
likelihood of our models overﬁtting to superﬁcial regular-
ities is vastly decreased as this tree-oriented feature space
is abstracted away from instance or class speciﬁc peculiari-
ties of the MILP formulation. However, in order to achieve
systematic generalization, inference of high-level latent fac-
tors must be paired with a corresponding composition mech-
anism. Here, the top-down neuro-modulatory prior of our
TreeGate model represents a powerful mechanism for dy-
namically composing the tree state and the candidate vari-
able states together. In summary, we hypothesize that BVS
policies trained in our framework are able to better infer and
compose latent higher order branching factors, which in turn
enables ﬂexible generalization across heterogeneous MILP
instances.

4 Experiments
Our experiments are designed to carefully measure the in-
ductive bias of BVS learning frameworks. We echo the sen-
timent of Melis, Koˇcisk´y, and Blunsom that merely scal-
ing up a dataset is insufﬁcient to explore the issue of sys-
tematic generalization in deep learning models, and we in-
stead choose to curate a controlled dataset with the following
properties:

- Train/test split consisting of heterogeneous MILP in-

Figure 2: Candidate variables input Ct is processed by
NoTree layers (in blue) to select a variable for branching.
For the TreeGate model, the Tree t input is ﬁrst embedded
and then utilized in gating layers (in orange) on the candi-
dates’ representations.

3.3

Imitation Learning

We train our BVS policies via imitation learning, specif-
ically behavioral cloning (Pomerleau 1991). Our expert
branching scheme is SCIP default, relpscost, i.e., a re-
liability version of hybrid branching in which SB and PC
scores are complemented with other ones reﬂecting the can-
didates’ role in the search; relpscost is a more realistic
expert (nobody uses SB in practice), and the most suited in
our context, given the emphasis we put on the search tree.
One of the main challenges of learning to imitate a com-
plex BVS expert policy is that it is only possible to partially
observe the true state of the solver. In our learning frame-
work, we approximate the solver state with our parameter-
ized B&B search tree state xt = {Ct, Tree t} at branching
step t. For each MILP instance, we roll-out SCIP to gather
a collection of pairs D = (xt, yt)N
t=1 where yt ∈ Ct is the
branching decision made by relpscost at branching step
t. Our policies πθ are trained to minimize the cross-entropy
categorical loss function:

L(θ) = −

1
N

(cid:88)

(x,y)∈D

log πθ(y | x).

(3)

stances. This rules out the possibility of BVS policies
learning superﬁcial class-speciﬁc branching rules and in-
stead forces them to infer higher order branching factors
shared across MILPs.

- A restricted set of training instances. While limiting the
sample complexity of the training set poses a signiﬁcant
learning challenge, the learning framework with the best
inductive bias will be the one that best learns to infer and
compose higher order branching factors from such a dif-
ﬁcult training set.

While we train our BVS policies to imitate the SCIP
default policy relpscost, our objective is not to out-
perform relpscost. Indeed, expert BVS policies like
relpscost are tuned over thousands of solvers’ propri-
etary instances: comprehensively improving on them is a
very hard task – impossible to guarantee in our purely-
research experimental setting – and should thus not be the
only yardstick to determine the validity (and practicality) of
a learned policy. Instead, we focus on evaluating how ef-
ﬁciently BVS learning frameworks can learn to infer and
compose higher order branching factors by measuring gen-
eralization from a controlled training set to heterogeneous
MILP test instances.

MILP dataset and solver setting
In general, randomly-
generated generic MILPs are too easy to be of interest; be-
sides, public MILP libraries only contain few hundreds of
instances, not all viable for our setting, and a careful dataset
curation is thus needed. Comparisons of branching policies
become clearer when the explored trees are manageable in
size and the problems can be consistently solved to optimal-
ity. Thus we select 27 heterogeneous problems from real-
world MILP benchmark libraries (Bixby et al. 1998; Koch
et al. 2011; Gleixner et al. 2019; Mittelmann 2020), focus-
ing on instances whose tree exploration is on average rela-
tively contained (in the tens/hundreds of thousands nodes,
max.) and whose optimal value is known. We partition our
selection into 19 train and 8 test problems, which are listed
in Table 1(a) (see SM for more details).

We use SCIP 6.0.1. Modifying the solver conﬁguration is
common practice in BVS literature (Linderoth and Savels-
bergh 1999), especially in a proof-of-concept setting in
which our work is positioned. To reduce the effects of the
other solver’s components on BVS, we work with a conﬁg-
uration speciﬁcally designed to fairly compare the perfor-
mance of branching rules (Gamrath and Schubert 2018). In
particular, we disable all primal heuristics and for each prob-
lem we provide the known optimal solution value as cutoff.
We also enforce a time-limit of 1h. Further details on the
solver parameters and hardware settings are reported in the
SM.

Data collection and split We collect IL training data from
SCIP roll-outs, gathering inputs xt = {Ct, Tree t} and cor-
responding relpscost branching decisions (labels) yt ∈
Ct. Given that each branching decision gives rise to a single
data-point (xt, yt), and that the search trees of the selected

TRAIN: air04, air05, dcmulti, eil33-2, istanbul-no-cutoff,
lseu, misc03, neos20, neos21, neos-476283,
l152lav,
rmatr100-p5,
neos648910, pp08aCUTS,
sp150x300d, stein27, swath1, vpm2

rmatr100-p10,

TEST: map18, mine-166-5, neos11, neos18, ns1830653,
nu25-pr12, rail507, seymour1
(a)

Total

(s, k) pairs

Train
Valid.
Test

85,533
14,413
28,307

{0, 1, 2, 3} × {0, 1, 5, 10, 15}
{4} × {0, 1, 5, 10, 15}
{0, 1, 2, 3, 4} × {0}

(b)

Table 1: (a) List of MILP instances in train and test sets.
(b) For train, validation and test set splits we report the to-
tal number of data-points and the seed-k pairs (s, k) from
which they are obtained.

Policy

NT
TG

h / d / LR
128 / – / 0.001
64 / 5 / 0.01

Test acc (@5) Val acc (@5)

64.02 (88.51)
83.70 (95.83)

77.69 (95.88)
84.33 (96.60)

GCNN

– / – / –

15.28 (44.16)

19.28 (38.44)

Table 2: Selected NoTree (NT) and TreeGate (TG) models
with corresponding hyper-parameters, and test and valida-
tion accuracy. For GCNN we report average scores across 5
seeds; validation means use the best scores observed during
training.

MILP instances are not extremely big, one needs to augment
the data. We proceed in two ways.

i. We exploit MILPs performance variability (Lodi and Tra-
montani 2013a), and obtain perturbed searches of the
same instance by setting solver’s random seeds s ∈
{0, ..., 4} to control variables’ permutations.

ii. We diversify B&B explorations by running the branch-
ing scheme random for the ﬁrst k nodes, before switch-
ing to SCIP default rule and starting data collection. The
motivation behind this type of augmentation is to gather
input states that are unlikely to be observed by an ex-
pert rule (He, Daume III, and Eisner 2014). We select
k ∈ {0, 1, 5, 10, 15}, where k = 0 corresponds to a run
without random branching. We apply this type of augmen-
tation to train instances only.

One can quantify MILP variability by computing the coef-
ﬁcient of variation of the performance measurements (Koch
et al. 2011); we report such scores and measure the effect of
k initial random branchings in the SM. Overall, both (i) and
(ii) appear effective to diversify our dataset. The ﬁnal com-
position of train, validation and test sets is summarized in
Table 1(b). Train and validation data come from the same in-
stances; the test set contains samples from separate MILPs,
using only type (i) augmentations.

An important measure to analyze the dataset is given by

the size of the candidate sets (i.e., the varying dimension-
ality of the Ct inputs) contained in each split. Figure 1(c)
shows histograms for |Ct| in each subset. While in train and
validation the candidate set sizes are mostly concentrated in
the [0, 50] range, the test set has a very different distribution
of |Ct|, and in particular one with a longer tail (over 300).
In this sense, the test instances present never-seen branch-
ing data gathered from heterogeneous MILPs, and we test
the generalization of our policies to entirely unknown and
larger branching sets.

IL optimization and GCNN benchmark We train both
IL policies using ADAM (Kingma and Ba 2015) with de-
fault β1 = 0.9, β2 = 0.999, and weight decay 1 × 10−5.
Our hyper-parameter search spans: learning rate LR ∈
{0.01, 0.001, 0.0001}, hidden size h ∈ {32, 64, 128, 256},
and depth d ∈ {2, 3, 5}. The factor by which units of NoTree
are reduced is 2, and we ﬁx INF = 8. We use PyTorch
(Paszke et al. 2019) to train the models for 40 epochs, reduc-
ing LR by a factor of 10 at epochs 20 and 30. To benchmark
our results, we also train the GCNN framework of Gasse
et al. on our MILP dataset. Data collection and experiments
are carried out as in Gasse et al., with full SB as expert, but
we ﬁx the solver setting as discussed above.

4.1 Results

In our context, standard IL metrics are informative yet
incomplete measures of performance for a learned BVS
model, and one also cares about assessing the policies’ be-
haviors when plugged in the solver environment. This is why
in order to determine the best NoTree and TreeGate policies
we take into account both types of evaluations. We ﬁrst se-
lect few policies based on their test accuracy score; next, we
specify them as custom branching rules in SCIP and per-
form full roll-outs on the entire MILP dataset, over ﬁve ran-
dom seeds (i.e., 135 evaluations each). To summarize the
policies’ performance in the solver, we compute the shifted
geometric mean (with a shift of 100) of the total number of
nodes, over the 135 B&B executions (ALL), and restricted
to TRAIN and TEST instances.

Both types of metrics are extensively reported in the SM,
together with the policies’ hyper-parameters. Incorporating
an explicit parameterization of the state of the search tree to
modulate BVS clearly aids generalization: the advantage of
TreeGate over NoTree is evident in all metrics, and across
multiple trained policies. What we observe is that best test
accuracy does not necessarily translate into best solver per-
formance. We select as best policies those yielding the best
nodes average over the entire dataset (Table 2). In the case
of TreeGate, the best model corresponds to that realizing the
best top-1 test accuracy (83.70%), and brings a 19% (resp.
7%) improvement over the NoTree policy, in top-1 (resp.
top-5) test accuracy. The GCNN models (trained, tested and
evaluated over 5 seeds, as in Gasse et al.) struggle to ﬁt data
from heterogeneous instances: their average top-1 (resp. top-
5) test accuracy is only 15.28% (resp. 44.16%), and across
ALL instances they explore around three times the number
of nodes needed by our policies. Note that GCNN is a mem-

ory intensive model, and we had to drastically reduce the
batch size parameter to avoid memory issues when using our
instances. Learning curves and further details on training dy-
namics and test results can be found in the SM.

In solver evaluations, NoTree and TreeGate are also com-
pared to SCIP default branching scheme relpscost, PC
branching pscost and a random one. For relpscost
we also compute the fair number of nodes (Gamrath and
Schubert 2018), which accounts for those nodes that are pro-
cessed as side-effects of SB-like explorations, speciﬁcally
looking at domain reduction and cutoffs counts. In other
words, the fair number distinguishes tree-size reductions due
to better branching from those obtained by SB side-effects.
For rules that do not involve any SB, the fair number and the
usual nodes’ count coincide. The selected solver parametric
setting (the same used for data collection and GCNN bench-
mark) allows a meaningful computation of the fair number
of nodes, and a honest comparison of branching schemes.

Both NoTree and TreeGate policies are able to solve all
instances within the 1h time-limit, like relpscost. In
contrast, GCNN hits the limit on 7 instances (24 times in to-
tal), while random does so on 4 instances (17 times in total)
and pscost on one instance only (neos18), a single time.
Table 3 reports the nodes’ means for every test instance over
ﬁve runs (see SM for complete instance-speciﬁc results), as
well as measures aggregated over train and test sets, and the
entire dataset. In aggregation, TreeGate is always better than
NoTree, the former exploring on average trees with 14.9%
less nodes. This gap becomes more pronounced when mea-
sured over test instances only (27%), indicating the advan-
tage of TreeGate over NoTree when exploring unseen data.
Results are less clear-cut from an instance-wise perspective,
with neither policy emerging as an absolute winner, though
the reductions in tree sizes achieved by TreeGate are overall
more pronounced. While the multiple time-limits of GCNN
hinder a proper comparison in terms of explored nodes, re-
sults clearly indicate that the difﬁculties of GCNN exacerbate
over never-seen, heterogeneous test instances.

Our policies also compare well

to other branching
rules: both NoTree and TreeGate are substantially better
than random across all instances, and always better than
pscost in aggregated measures. Only on one training in-
stance both policies are much worse than pscost (neos-
476283); in the test set, GCNN appears competitive with our
models only on neos11. As expected, relpscost still re-
alizes the smallest trees, but on 11 (out of 27) instances at
least one among NoTree and TreeGate explores less nodes
than the relpscost fair number. In general, our policies
realize tree sizes comparable to the SCIP ones, when SB side
effects are taken into account.

5 Related Work

Among the ﬁrst attempts in “learning to branch”, Al-
varez, Louveaux, and Wehenkel perform regression to learn
proxies of SB scores. Instead, Khalil et al. propose to
learn the ranking associated with such scores, and train
instance-speciﬁc models (that are not end-to-end policies)
via SVMrank. Also Hansknecht, Joormann, and Stiller treat

Instance

NoTree

TreeGate

GCNN

random

pscost

relpscost (fair)

ALL
TRAIN
TEST

map18
mine-166-5
neos11
neos18
ns1830653
nu25-pr12
rail507
seymour1

1241.79
834.40
3068.96

457.89
3438.44
3326.32
15611.63
6422.37
357.00
9623.05
3202.20

1056.79
759.94
2239.47

575.92
4996.48
3223.46
10373.80
5812.03
86.80
3779.05
1646.82

*3660.32
*1391.41
*33713.63

*3907.64
*233142.25
1642.07
40794.74
*22931.45
*45982.34
*75663.48
*319046.04

*6580.79
*2516.04
*61828.29

11655.33
*389437.62
29949.69
228715.62
288489.30
1658.41
*80575.84
*167725.65

*1471.61
884.37
*4674.34

1025.74
4190.41
4728.49
*133437.40
12307.90
342.47
4259.98
3521.47

286.15 (719.20)
182.27 (558.34)
712.77 (1276.76)

270.25 (441.18)
175.10 (600.22)
2618.27 (5468.05)
2439.29 (5774.36)
3489.07 (4311.84)
21.39 (105.61)
543.39 (859.37)
866.32 (1096.67)

Table 3: Total number of nodes explored by learned and SCIP policies for test instances and aggregated over sets, in shifted
geometric means over 5 runs on seeds {0, . . . , 4}. We mark with * the cases in which time-limits were hit. For relpscost,
we also compute the fair number of nodes.

BVS as a ranking problem, and specialize their models to
the combinatorial class of time-dependent traveling sales-
man problems. More recently, the work of Balcan et al.
learns mixtures of existing branching schemes for different
classes of synthetic problems, focusing on sample complex-
ity guarantees. In Di Liberto et al., a portfolio approach to
BVS is explored. Similarly to us, Gasse et al. frames BVS
as classiﬁcation of SB-expert branching decisions and em-
ploys a GCNN model to learn branching. Proposed features
in Gasse et al. (as in Alvarez, Louveaux, and Wehenkel;
Khalil et al.) focus on static, parameters-dependent proper-
ties of MILPs and node LP relaxations, whereas our repre-
sentations aim at capturing the temporality and dynamism
of BVS. Although their resulting policies are specializations
of SB that appear to effectively capture structural character-
istics of some classes of combinatorial optimization prob-
lems, and are able to generalize to larger formulations from
the same distribution, we showed how such policies fail to
attain a broader generalization paradigm.

Still concerning the B&B framework, He, Daume III, and
Eisner employ IL to learn a heuristic class-speciﬁc node se-
lection policy; Song et al. propose instead a retrospective
approach on IL. A reinforcement learning (RL) approach for
node selection can be found in Sabharwal, Samulowitz, and
Reddy, where a Multi-Armed Bandit is used to model the
tree search.

Feature gating has a long and successful history in ma-
chine learning (see Makkuva et al.), ranging from LSTMs
(Hochreiter and Schmidhuber 1997) to GRUs (Chung et al.
2014). The idea of using a tree state to drive a feature gat-
ing of the branching variables is an example of top-down
modulation, which has been shown to perform well in other
deep learning applications (Shrivastava et al. 2016; Lin et al.
2016; Son and Mishra 2018). With respect to learning across
non-static action spaces, the most similar to our work is
Chandak et al., in the continual learning setting. Unlike the
traditional Markov Decision Process formulation of RL, the
input to our policies is not a generic state but rather includes
a parameterized hand-crafted representation of the available
actions, thus continual learning is not a relevant concern for
our framework. Other works from the RL setting learn repre-

sentations of static action spaces (Dulac-Arnold et al. 2015;
Chandak et al. 2019a), while in contrast the action space of
BVS changes dynamically with |Ct|.

6 Conclusions and Future Directions

Branching variable selection is a crucial factor in B&B suc-
cess, and we setup a novel imitation learning framework to
address it. We sought to learn branching policies that gen-
eralize across heterogeneous MILPs, regardless of the in-
stances’ structure and formulation size. In doing so, we un-
dertook a step towards a broader type of generalization. The
novelty of our approach is relevant for both the ML and the
MILP worlds. On the one hand, we developed parameter-
izations of the candidate variables and of the search trees,
and designed a DNN architecture that handles candidate
sets of varying size. On the other hand, the data encoded
in our Tree t parameterization is not currently exploited by
state-of-the-art MILP solvers, but we showed that this type
of information could indeed help in adapting the branching
criteria to different search dynamics. Our results on MILP
benchmark instances clearly demonstrated the advantage of
incorporating a search-tree context to modulate BVS and aid
generalization to heterogeneous problems, in terms of both
better test accuracy and smaller explored B&B trees. The
comparison with the GCNN setup of Gasse et al. reinforced
our conclusions: experiments showcased the inability of the
GCNN paradigm alone to generalize to new instances for
which no analogs were available during training. One crucial
step towards improving over state-of-the-art solvers is pre-
cisely that of being able to generalize across heterogeneous
problems, and our work is the ﬁrst paper in the literature
attaining this target.

There surely are additional improvements to be gained by
continuing to experiment with IL methods for branching,
and also by exploring innovative RL settings. Indeed, the
idea and the beneﬁts of using an explicit parameterization of
B&B search trees – which we demonstrated in the IL setup
– could be expanded even more in the RL one, for both state
representations and the design of branching rewards.

Acknowledgements
We would like to thank Ambros Gleixner, Gerald Gamrath,
Laurent Charlin, Didier Ch´etelat, Maxime Gasse, Antoine
Prouvost, Leo Henri and S´ebastien Lachapelle for helpful
discussions on the branching framework. We also thank
Compute Canada for compute resources. This work was sup-
ported by CIFAR and IVADO. We also thank the anonymous
reviewers for their helpful feedback.

References
Achterberg, T. 2007. Constraint Integer Programming. Ph.D. the-
sis.
Achterberg, T.; and Berthold, T. 2009. Hybrid Branching, 309–
311. Springer, Berlin, Heidelberg. doi:10.1007/978-3-642-01929-
6 23.
Achterberg, T.; Koch, T.; and Martin, A. 2005. Branching rules
revisited. Oper Res Lett 33(1): 42–54. doi:10.1016/j.orl.2004.04.
002.
Achterberg, T.; and Wunderling, R. 2013. Mixed Integer Program-
ming: Analyzing 12 Years of Progress, 449–481. Berlin, Heidel-
berg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-38189-
8 18.
Alvarez, M. A.; Louveaux, Q.; and Wehenkel, L. 2017. A Ma-
IN-
chine Learning-Based Approximation of Strong Branching.
FORMS Journal on Computing 29(1): 185–195. doi:10.1287/ijoc.
2016.0723.
Applegate, D.; Bixby, R.; Chvatal, V.; and Cook, B. 1995. Finding
Cuts in the TSP (A Preliminary Report). Technical report.
Balcan, M.-F.; Dick, T.; Sandholm, T.; and Vitercik, E. 2018.
In International Conference on Machine
Learning to Branch.
Learning, 344–353.
Bengio, Y.; Lodi, A.; and Prouvost, A. 2018. Machine Learning for
Combinatorial Optimization: a Methodological Tour d’Horizon.
arXiv:1811.06128 .
Benichou, M.; Gauthier, J.; Girodet, P.; and Hentges, G. 1971. Ex-
periments in mixed-integer programming. Math Program 1: 76–94.
Berthold, T.; Hendel, G.; and Koch, T. 2017. From feasibility to
improvement to proof: three phases of solving mixed-integer pro-
grams. Optimization Methods and Software 33(3): 499 – 517. doi:
10.1080/10556788.2017.1392519.
Bixby, R. E.; Ceria, S.; McZeal, C. M.; and Savelsbergh, M. W.
1998. An updated mixed-integer programming library: MIPLIB 3.
Technical Report TR98-03. URL http://miplib2010.zib.de/miplib3/
miplib.html. Accessed: 2019.
Chandak, Y.; Theocharous, G.; Kostas, J.; Jordan, S.; and Thomas,
P. 2019a. Learning Action Representations for Reinforcement
Learning. In Chaudhuri, K.; and Salakhutdinov, R., eds., Proceed-
ings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, 941–
950. PMLR.
Chandak, Y.; Theocharous, G.; Nota, C.; and Thomas, P. S.
2019b. Lifelong Learning with a Changing Action Set. CoRR
abs/1906.01770.
Chung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y. 2014. Empiri-
cal Evaluation of Gated Recurrent Neural Networks on Sequence
Modeling. arXiv:1412.3555 .
Di Liberto, G.; Kadioglu, S.; Leo, K.; and Malitsky, Y. 2016.
DASH: Dynamic Approach for Switching Heuristics. Eur J Oper
Res 248(3): 943–953. doi:10.1016/j.ejor.2015.08.018.

Dulac-Arnold, G.; Evans, R.; Sunehag, P.; and Coppin, B. 2015.
Reinforcement Learning in Large Discrete Action Spaces. CoRR
abs/1512.07679.
Fischetti, M.; Lodi, A.; and Zarpellon, G. 2019. Learning MILP
Resolution Outcomes Before Reaching Time-Limit. In Rousseau,
L.-M.; and Stergiou, K., eds., Integration of Constraint Program-
ming, Artiﬁcial Intelligence, and Operations Research, 275–291.
Cham: Springer International Publishing.
Gamrath, G.; and Schubert, C. 2018. Measuring the Impact of
Branching Rules for Mixed-Integer Programming. In Kliewer, N.;
Ehmke, J. F.; and Bornd¨orfer, R., eds., Operations Research Pro-
ceedings 2017, 165–170. Cham: Springer International Publishing.
Gasse, M.; Chetelat, D.; Ferroni, N.; Charlin, L.; and Lodi, A.
2019. Exact Combinatorial Optimization with Graph Convolu-
tional Neural Networks. In Wallach, H.; Larochelle, H.; Beygelz-
imer, A.; d’Alch´e Buc, F.; Fox, E.; and Garnett, R., eds., Advances
in Neural Information Processing Systems 32, 15554–15566. Cur-
ran Associates, Inc.
Gleixner, A.; Bastubbe, M.; Eiﬂer, L.; Gally, T.; Gamrath, G.;
Gottwald, R. L.; Hendel, G.; Hojny, C.; Koch, T.; L¨ubbecke, M.;
Maher, S. J.; Miltenberger, M.; M¨uller, B.; Pfetsch, M.; Puchert,
C.; Rehfeldt, D.; Schl¨osser, F.; Schubert, C.; Serrano, F.; Shinano,
Y.; Viernickel, J. M.; Walter, M.; Wegscheider, F.; Witt, J. T.; and
Witzig, J. 2018. The SCIP Optimization Suite 6.0. Technical Re-
port 18-26, ZIB, Takustr. 7, 14195 Berlin.
Gleixner, A.; Hendel, G.; Gamrath, G.; Achterberg, T.; Bastubbe,
M.; Berthold, T.; Christophel, P. M.; Jarck, K.; Koch, T.; Lin-
deroth, J.; L¨ubbecke, M.; Mittelmann, H. D.; Ozyurt, D.; Ralphs,
T. K.; Salvagnin, D.; and Shinano, Y. 2019. MIPLIB 2017: Data-
Driven Compilation of the 6th Mixed-Integer Programming Li-
brary. URL http://www.optimization-online.org/DB HTML/2019/
07/7285.html. Accessed: 2019.
Goodfellow, I.; Shlens, J.; and Szegedy, C. 2015. Explaining and
Harnessing Adversarial Examples. In International Conference on
Learning Representations.
Hansknecht, C.; Joormann, I.; and Stiller, S. 2018. Cuts, Primal
Heuristics, and Learning to Branch for the Time-Dependent Trav-
eling Salesman Problem. arXiv:1805.01415 .
He, H.; Daume III, H.; and Eisner, J. M. 2014. Learning to Search
in Branch and Bound Algorithms. In Ghahramani, Z.; Welling, M.;
Cortes, C.; Lawrence, N. D.; and Weinberger, K. Q., eds., Advances
in Neural Information Processing Systems 27, 3293–3301. Curran
Associates, Inc.
Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term mem-
ory. Neural computation 9(8): 1735–1780.
Ioffe, S.; and Szegedy, C. 2015. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate Shift. In
Bach, F.; and Blei, D., eds., Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of
Machine Learning Research, 448–456. PMLR.
Johnson, J.; Hariharan, B.; van der Maaten, L.; Fei-Fei, L.; Zit-
nick, C. L.; and Girshick, R. 2017. CLEVR: A Diagnostic Dataset
for Compositional Language and Elementary Visual Reasoning. In
CVPR.
Khalil, E. B.; Bodic, P. L.; Song, L.; Nemhauser, G.; and Dilkina,
B. 2016. Learning to Branch in Mixed Integer Programming. In
Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, AAAI’16, 724–731. AAAI Press.
Kingma, D. P.; and Ba, J. 2015. Adam: A method for stochastic
optimization. In International Conference on Learning Represen-
tations (ICLR).

Koch, T.; Achterberg, T.; Andersen, E.; Bastert, O.; Berthold, T.;
Bixby, R. E.; Danna, E.; Gamrath, G.; Gleixner, A. M.; Heinz, S.;
Lodi, A.; Mittelmann, H.; Ralphs, T.; Salvagnin, D.; Steffy, D. E.;
and Wolter, K. 2011. MIPLIB 2010. Mathematical Programming
Computation 3(2): 103. doi:10.1007/s12532-011-0025-9.

Lake, B. M.; and Baroni, M. 2017. Still not systematic after all
these years: On the compositional skills of sequence-to-sequence
recurrent networks. CoRR abs/1711.00350.

Land, A. H.; and Doig, A. G. 1960. An Automatic Method of Solv-
ing Discrete Programming Problems. Econometrica 28(3): 497–
520.

Le Bodic, P.; and Nemhauser, G. 2017. An abstract model for
branching and its application to mixed integer programming. Math
Program 1–37. doi:10.1007/s10107-016-1101-8.

Lin, T.; Doll´ar, P.; Girshick, R. B.; He, K.; Hariharan, B.; and Be-
longie, S. J. 2016. Feature Pyramid Networks for Object Detection.
CoRR abs/1612.03144.

Linderoth, J. T.; and Savelsbergh, M. W. P. 1999. A Compu-
tational Study of Search Strategies for Mixed Integer Program-
INFORMS Journal on Computing 11(2): 173–187. doi:
ming.
10.1287/ijoc.11.2.173.

Lipton, R. J.; and Regan, R. W. 2012. Branch And Bound—Why
Does It Work? https://rjlipton.wordpress.com/2012/12/19/branch-
and-bound-why-does-it-work/. Accessed: 2019.

Lodi, A. 2009. Mixed integer programming computation.
In
J¨unger, M.; Liebling, T.; Naddef, D.; Nemhauser, G.; Pulleyblank,
W.; Reinelt, G.; Rinaldi, G.; and Wolsey, L., eds., 50 Years of In-
teger Programming 1958-2008, 619–645. Springer Berlin Heidel-
berg.

Lodi, A.; and Tramontani, A. 2013a. Performance Variability in
Mixed-Integer Programming, chapter 1, 1–12.
INFORMS. doi:
10.1287/educ.2013.0112.

Lodi, A.; and Tramontani, A. 2013b. Performance Variability
in Mixed-Integer Programming, chapter Chapter 1, 1–12. doi:
10.1287/educ.2013.0112.

Lodi, A.; and Zarpellon, G. 2017. On learning and branching: a
survey. TOP 25(2): 207–236. doi:10.1007/s11750-017-0451-6.

Maher, S.; Miltenberger, M.; Pedroso, J. P.; Rehfeldt, D.; Schwarz,
R.; and Serrano, F. 2016. PySCIPOpt: Mathematical Programming
in Python with the SCIP Optimization Suite.
In Greuel, G.-M.;
Koch, T.; Paule, P.; and Sommese, A., eds., Mathematical Software
– ICMS 2016, 301–307. Cham: Springer International Publishing.
ISBN 978-3-319-42432-3.

Makkuva, A. V.; Oh, S.; Kannan, S.; and Viswanath, P. 2019.
Learning in Gated Neural Networks. CoRR abs/1906.02777.

Melis, G.; Koˇcisk´y, T.; and Blunsom, P. 2020. Mogriﬁer LSTM. In
International Conference on Learning Representations.

Mittelmann, H. D. 2020. MILPlib. http://plato.asu.edu/ftp/milp/.
Accessed 2019.

Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan,
G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison,
A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chil-
amkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; and Chintala, S. 2019.
PyTorch: An Imperative Style, High-Performance Deep Learning
Library. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d’ Alch´e-
Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Infor-
mation Processing Systems 32, 8024–8035. Curran Associates, Inc.

Pomerleau, D. A. 1991. Efﬁcient Training of Artiﬁcial Neural Net-
works for Autonomous Navigation. Neural Computation 3(1): 88–
97.

Code for the relpscost branching rule
relpscost. 2019.
in SCIP. https://scip.zib.de/doc-6.0.0/html/branch relpscost 8c
source.php#l00524. Accessed: 2019.

Sabharwal, A.; Samulowitz, H.; and Reddy, C. 2012. Guiding
Combinatorial Optimization with UCT. In Beldiceanu, N.; Jussien,
N.; and Pinson, ´E., eds., Integration of AI and OR Techniques in
Contraint Programming for Combinatorial Optimzation Problems:
9th International Conference, CPAIOR 2012, Nantes, France, May
28 – June 1, 2012. Proceedings, Lecture Notes in Computer Sci-
ence, 356–361. Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:10.1007/978-3-642-29828-8 23.

Shrivastava, A.; Sukthankar, R.; Malik, J.; and Gupta, A. 2016. Be-
yond Skip Connections: Top-Down Modulation for Object Detec-
tion. CoRR abs/1612.06851.

Son, J.; and Mishra, A. K. 2018. ExGate: Externally Controlled
Gating for Feature-based Attention in Artiﬁcial Neural Networks.
CoRR abs/1811.03403.

Song, J.; Lanka, R.; Zhao, A.; Bhatnagar, A.; Yue, Y.; and Ono,
M. 2018. Learning to Search via Retrospective Imitation. arXiv
preprint 1804.00846 .

van der Maaten, L.; and Hinton, G. 2008. Visualizing High-
Dimensional Data Using t-SNE. Journal of Machine Learning Re-
search 9(nov): 2579–2605. Pagination: 27.

Wolsey, L. A. 1998. Integer programming. New York, NY, USA:
Wiley-Interscience.

Wu, Y.; and He, K. 2018. Group Normalization. In The European
Conference on Computer Vision (ECCV).

A Dataset curation
To curate a dataset of heterogeneous MILP instances, we con-
sider the standard benchmark libraries MIPLIB 3, 2010 and 2017
(Bixby et al. 1998; Koch et al. 2011; Gleixner et al. 2019), to-
gether with the collection of Mittelmann. We assess the problems
by analyzing B&B roll-outs of SCIP with its default branching rule
(relpscost) and a random one, enforcing a time limit of 1h in
the same solver setting used for our experiments (see Appendix C).
We focus on instances whose tree exploration is on average rela-
tively contained (in the tens/hundreds of thousands nodes, maxi-
mum) and whose optimal value is known. This choice is primarily
motivated by the need of ensuring a fair comparison among branch-
ing policies in terms of tree size, which is more easily achieved
when roll-outs do not hit the time-limit. We also remove problems
that are solved at the root node (i.e., those for which no branching
was performed).

Final training and test sets comprise 19 and 8 instances, respec-
tively, for a total of 27 problems. They are summarized in Table 4,
where we report their size, the number of binary/integer/continuous
variables, the number of constraints, their membership in the
train/test split and their library of origin. The constraints of each
problem are of different types and give rise to various structures.

B Hand-crafted input features
Hand-crafted input features for candidate variables (Ct) and tree
state (Tree t) are reported in Table 10. To ease their reading, we
present them subdivided in groups, and synthetically describe them
by the SCIP API functions with which they are computed. We make

Table 4: The curated MILP dataset. For each instance we report: the number of variables (Vars) and their types (binary, integers
and continuous), the number of constraints (Conss), the membership in the train/test split and the library of origin.

Name

Vars

Types (bin - int - cont)

air04
air05
dcmulti
eil33-2
istanbul-no-cutoff
l152lav
lseu
misc03
neos20
neos21
neos-476283
neos648910
pp08aCUTS
rmatr100-p10
rmatr100-p5
sp150x300d
stein27
swath1
vpm2

map18
mine-166-5
neos11
neos18
ns1830653
nu25-pr12
rail507
seymour1

8904
7195
548
4516
5282
1989
89
160
1165
614
11915
814
240
7359
8784
600
27
6805
378

164547
830
1220
3312
1629
5868
63019
1372

8904 - 0 - 0
7195 - 0 - 0
75 - 0 - 473
4516 - 0 - 0
30 - 0 - 5252
1989 - 0 - 0
89 - 0 - 0
159 - 0 - 1
937 - 30 - 198
613 - 0 - 1
5588 - 0 - 6327
748 - 0 - 66
64 - 0 - 176
100 - 0 - 7259
100 - 0 - 8684
300 - 0 - 300
27 - 0 - 0
2306 - 0 - 4499
168 - 0 - 210

146 - 0 - 164401
830 - 0 - 0
900 - 0 - 320
3312 - 0 - 0
1458 - 0 - 171
5832 - 36 - 0
63009 - 0 - 10
451 - 0 - 921

Conss

823
426
290
32
20346
97
28
96
2446
1085
10015
1491
246
7260
8685
450
118
884
234

328818
8429
2706
11402
2932
2313
509
4944

Set

Library

MIPLIB 3
train
MIPLIB 3
train
train
MIPLIB 3
train MIPLIB 2010
train MIPLIB 2017
MIPLIB 3
train
MIPLIB 3
train
MIPLIB 3
train
MILPLib
train
train
MILPLib
train MIPLIB 2010
MILPLib
train
train
MIPLIB 3
train MIPLIB 2010
train MIPLIB 2010
train MIPLIB 2017
train
MIPLIB 3
train MIPLIB 2017
MIPLIB 3
train

test MIPLIB 2010
test MIPLIB 2010
test
MILPLib
test MIPLIB 2010
test MIPLIB 2010
test MIPLIB 2017
test MIPLIB 2010
test MIPLIB 2017

use of different functions to normalize and compare the solver in-
puts.

To compute the branching scores si of a candidate variable
i ∈ Ct, with respect to an average score savg , we use the formula
implemented in SCIP relpscost (relpscost):

varScore(si, savg ) = 1 −

(cid:18)

1
1 + si/ max{savg , 0.1}

(cid:19)

.

As in (Achterberg 2007), we normalize inputs that naturally span
different ranges by the following:

gNormMax(x) = max

(cid:26) x

x + 1

(cid:27)

, 0.1

.

To compare commensurable quantities (e.g., upper and lower
bounds), we compute measures of relative distance and relative po-
sition:

relDist(x, y) =

(cid:40)

0

|x−y|
max{|x|,|y|,1 × 10−10}

, if xy < 0
, else

relPos(z, x, y) =

|x − z|
|x − y|

.

We also make use of usual statistical functions such as min, max,
mean, standard deviation std and 25-75% quantile values (de-
noted in Table 10 as q1 and q3, respectively).

Further information on each feature can be gathered by search-
ing the SCIP online documentation at https://scip.zib.de/doc-6.0.1/
html/.

C Solver setting and hardware
Regarding the MILP solver parametric setting, we use SCIP 6.0.1
and set a time-limit of 1h on all B&B evaluations. We leave on pre-
solve routines and cuts separation (as in default mode), while dis-
abling all primal heuristics and reoptimization (also off at default).
To control SB side-effects and properly compute the fair number
of nodes (Gamrath and Schubert 2018), we additionally turn off SB
conﬂict analysis and the use of probing bounds identiﬁed during SB
evaluations. We also disable feasibility checking of LP solutions
found during SB with propagation, and always trigger the reeval-
uation of SB values. Finally, the known optimal solution value is
provided as cutoff to each model, and a random seed determines
variables’ permutations. Parameters are summarized in Table 5.

To benchmark the GCNN model of Gasse et al., we do not use
the original parametric setting of Gasse et al. but the one summa-
rized above. For the rest, data collection and experiments are exe-
cuted as in the original paper, with full SB used as expert rule. Note
that a time-limit of 10 minutes is enforced in data-collection runs.
To train and test GCNN, we had to reduce the batch size parameter
from 32 to 4 in training, and from 128 to 16 in test, in order to avoid
memory issues, as our MILP dataset contains bigger instances than
those used in Gasse et al.. Finally, the state buffer was deactivated
at evaluation time due to the presence of cuts in our solver setting.
For the IL experiments, we used the following hardware: Two
Intel Core(TM) i7-6850K CPU @ 3.60GHz, 16GB RAM and an
NVIDIA TITAN Xp 12GB GPU. Evaluations of SCIP branching
rules ran on dual Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz,
equipped with 512GB of RAM. The entire benchmark of GCNN
was executed on Intel(R) Xeon(R) Gold 6126 CPU @ 2.60GHz
and an Nvidia Tesla V100 GPU.

improvement over the NoTree policy, in top-1 (resp. top-5) test ac-
curacy. Additionally, we present plots of the optimization dynamics
for the selected NoTree and TreeGate policies. Figure 3 shows the
training loss curves, while Figure 4 depicts top-1 and top-5 vali-
dation accuracy curves. In general, we see that the TreeGate pol-
icy enjoys a better conditioned optimization. Note however that for
top-5 validation accuracy the two policies are quite close.

GCNN Detailed training, validation and test metrics for GCNN
are reported in Table 8.

Instability of batch-norm As observed in Figures 3 and 4,
optimization dynamics for NoTree seem to be of a much slower na-
ture than those of TreeGate. One common option to speed up train-
ing is to use batch normalization (BN) (Ioffe and Szegedy 2015). In
our architectures for branching, one may view the cardinality of the
candidate sets |Ct| as a batch dimension. When learning to branch
across heterogeneous MILPs, such batch dimension can (and will)
vary by orders of magnitude. Practically, our dataset has |Ct| vary-
ing from < 10 candidates to over 300. To this end, BN has been
shown to struggle in the small-batch setting (Wu and He 2018),
and in general we were unsure of the reliability of BN with such
variable batch-sizes.

Indeed, in our initial trials with BN we observed highly unreli-
able performance. Two troubling outcomes emerge when using BN
in our NoTree policies: 1) the validation accuracy varies wildly, as
shown in Figure 5, or 2) the NoTree+BN policy exhibits a stable
validation accuracy curve, but would time-limit on train instances,
i.e., would perform poorly in terms of solver performance. In par-
ticular, case 2) happened for a NoTree+BN policy with hidden size
h = 64 and LR = 0.001, reaching the 1h time-limit on train
instance neos-476283, over all ﬁve runs (on different seeds); the
geometric mean of explored nodes was 66170.66. We remark that
in our non-BN experiments, all of our trained policies (both Tree-
Gate and NoTree) managed to solve all the train instances without
even coming close to time-limiting. Moreover, none of our training
and validation curves ever remotely resemble those in Figure 5(b).
For these reasons we opted for a more streamlined presentation
of our results, without BN in the current framework. We leave it for
future work to analyze the relationship between the nature of local
minima in the IL optimization landscape and solver performance.

SCIP evaluation Instance-speciﬁc details of SCIP evaluations
of all policies are reported in Table 9. Results are less clear-cut from
an instance-wise perspective, with neither policy emerging as an
absolute winner. Nonetheless, TreeGate is at least 10% (resp. 25%)
better than NoTree on 10 (resp. 8) instances, while the opposite
only happens 6 (resp. 3) times. In this sense, the reductions in tree
sizes achieved by TreeGate are overall more pronounced.

Table 5: SCIP parametric setting.

limits/time = 3600
presolving/maxrounds = -1
separating/maxrounds = -1
separating/maxroundsroot = -1
heuristics/*/freq = -1
reoptimization/enable = False
conﬂict/usesb = False
branching/fullstrong/probingbounds = False
branching/relpscost/probingbounds = False
branching/checksol = False
branching/fullstrong/reevalage = 0
model.setObjlimit(cutoff value)
randomization/permutevars = True
randomization/permutationseed = scip seed

D Data augmentation
To augment our dataset, we (i) run MILP instances with different
random seeds to exploit performance variability (Lodi and Tramon-
tani 2013b), and (ii) perform k random branchings at the top of the
tree, before switching to the default SCIP branching rule and col-
lect data. To quantify the effects of such operations in diversifying
the search trees, we compute coefﬁcients of variations of perfor-
mance measurements (Koch et al. 2011). In particular, assuming
performance measurements nl, l = 1 . . . L are available, we com-
pute the variability score VS as

VS =

L
(cid:80)L
l=1 nl

(cid:118)
(cid:117)
(cid:117)
(cid:116)

L
(cid:88)

(cid:32)

l=1

nl −

(cid:80)L

l=1 nl
L

(cid:33)2

.

(4)

Table 6 reports such coefﬁcients for all instances, using as perfor-
mance measures the number of nodes explored in the ﬁve runs from
(i). The observed coefﬁcients range in [0.03, 1.70]: the majority of
the instances presents a variability of at least 0.20, conﬁrming (i)
as an effective way of diversifying our dataset. Similarly, we re-
port the shifted geometric means of the number of nodes over the
ﬁve runs for each k ∈ {0, 1, 5, 10, 15}, and additionally compute
the variability of those means, across different k’s (VS k). Gener-
ally, the size of the explored trees grows with k, i.e., initial ran-
dom branchings affect the nodes’ count for worse – though the op-
posite can also happen in few cases. The coefﬁcients of variation
of the nodes shifted geometric means across different k’s range in
[0.07, 0.79] in the training set, so (ii) also appears effective for data
augmentation. Overall, both (i) and (ii) appear effective ways of di-
versifying our dataset.

E IL optimization dynamics
Best policies We report hyper-parameters and performance de-
tails of the best learned NoTree and TreeGate policies in Table 7.
The top-1 test accuracy averages at 65.90 ± 1.6 for the NoTree
models, while TreeGate ones score at 83.08 ± 0.86; the gap in
validation accuracy is also signiﬁcant. In terms of B&B roll-outs,
NoTree models explore on average 1336.12±73.32 nodes, against
the 1126.97 ± 46.85 of TreeGate ones. What we observe is that
best test accuracy does not necessarily translate into best solver
performance. The NoTree policy with the best solver performance
exhibits an approximately 4% gap from the optimal top-1 test accu-
racy model, but an improvement over 7% in solver performance. In
the case of TreeGate, the best model corresponds to that realizing
the best top-1 test accuracy (83.70%), and brings a 19% (resp. 7%)

Table 6: Variability scores VS are reported for relpscost, and are computed using the 5 runs with k = 0 (i.e., SCIP default
runs, over seeds {0, . . . , 4}). Total number of nodes explored by data collection runs with k random branchings, in shifted
geometric means over 5 runs is also reported. Finally, VS k is the coefﬁcient of variation of the ﬁve means, across different k’s.

Instance

air04
air05
dcmulti
eil33-2
istanbul-no-cutoff
l152lav
lseu
misc03
neos20
neos21
neos648910
neos-476283
pp08aCUTS
rmatr100-p5
rmatr100-p10
sp150x300d
stein27
swath1
vpm2

map18
mine-166-5
neos11
neos18
ns1830653
nu25-pr12
rail507
seymour1

Set

train
train
train
train
train
train
train
train
train
train
train
train
train
train
train
train
train
train
train

test
test
test
test
test
test
test
test

VS

0.20
0.26
0.21
0.69
0.11
0.36
0.43
0.38
1.22
0.15
0.60
0.48
0.31
0.04
0.03
1.70
0.42
0.53
0.19

0.09
0.82
0.30
0.53
0.09
1.18
0.08
0.07

k = 0

k = 1

k = 5

k = 10

k = 15 VS k

8.19
60.25
9.38
583.34
242.39
10.14
148.99
12.11
200.26
668.44
39.83
204.88
69.66
411.93
806.35
182.22
926.82
298.58
199.46

270.25
175.10
2618.27
2439.29
3489.07
21.39
543.39
866.32

12.02
61.07
13.75
648.47
234.01
16.54
152.65
10.59
282.68
771.77
48.16
219.58
80.39
419.21
799.24
462.45
1062.69
280.49
180.93

309.77
70.77
3114.62
2747.77
3913.58
16.97
562.09
1174.18

46.57
115.94
27.53
492.95
271.35
29.31
154.16
13.80
557.15
898.79
65.84
384.86
92.60
451.01
860.60
484.55
1098.41
230.12
275.57

401.79
642.33
3488.40
4061.40
4091.59
56.04
854.76
1825.04

85.35
196.27
34.99
531.37
279.38
55.51
182.55
22.59
434.03
1110.82
41.05
480.37
69.94
461.83
933.80
483.89
1162.57
256.84
273.33

447.34
942.63
2898.41
4655.59
4839.39
101.34
1207.15
2739.45

119.49
274.44
45.00
441.24
311.39
61.14
177.35
31.80
944.75
1158.07
59.72
715.78
76.43
494.09
965.07
439.69
1154.01
267.55
316.82

489.85
1619.75
2659.96
5714.05
4772.73
119.05
1196.33
3313.87

0.79
0.59
0.50
0.13
0.10
0.59
0.09
0.44
0.54
0.21
0.20
0.47
0.11
0.07
0.08
0.28
0.08
0.09
0.20

0.21
0.81
0.11
0.31
0.12
0.66
0.33
0.47

Table 7: Best trained NoTree and TreeGate models. For each policy, we report the corresponding hyper-parameters, top-1 and
top-5 test and validation accuracy scores, and shifted geometric means of B&B nodes over ALL, TRAIN and TEST instances.
Policies selected as best ones are boldfaced.

Policy

NoTree

TreeGate

h / d / LR
32 / – / 0.0001
64 / – / 0.0001
128 / – / 0.0001
128 / – / 0.001
256 / – / 0.0001

64 / 5 / 0.01
256 / 2 / 0.001
32 / 3 / 0.01
128 / 5 / 0.001

Test acc@1 (@5) Val acc@1 (@5)

ALL

68.37 (91.43)
67.05 (89.18)
65.44 (90.21)
64.02 (88.51)
64.59 (90.13)

83.70 (95.83)
83.69 (95.18)
83.31 (95.72)
81.61 (95.81)

75.40 (95.23)
76.45 (95.11)
76.77 (95.66)
77.69 (95.88)
77.29 (96.08)

84.33 (96.60)
84.10 (96.42)
84.02 (96.50)
84.96 (96.74)

1341.72
1363.73
1454.20
1241.79
1279.18

1056.79
1135.28
1188.48
1127.31

TRAIN

859.17
847.63
875.19
834.40
731.16

759.94
822.80
809.18
771.60

TEST

3695.04
4010.65
4601.72
3068.96
4491.64

2239.47
2369.35
2849.28
2666.73

Table 8: Training and test details of the ﬁve GCNN models. All training phases ﬁnished by early stopping, after 20 epochs
without improvement.

Seed

# epochs

Best valid. loss

acc@1

acc@5

Test acc@1

Test acc@5

0
1
2
3
4

142
206
143
211
269

4.472
4.455
4.472
4.465
4.455

0.192
0.197
0.186
0.189
0.200

0.383
0.393
0.378
0.383
0.385

0.1327
0.1577
0.1575
0.2206
0.0954

0.3828
0.4738
0.5068
0.5004
0.3443

(a)

(b)

Figure 3: (a) Train and (b) validation loss curves for the best NoTree (orange) and TreeGate (blue) policies.

(a)

(b)

Figure 4: (a) Validation top-1 and (b) top-5 accuracy plots for the best NoTree (orange) and TreeGate (blue) policies.

Figure 5: (a) Train loss and (b) validation top-1 Accuracy for NoTree+BN policy with hidden size h = 64, LR = 0.001.

(a)

(b)

Table 9: Total number of nodes explored by learned and SCIP policies for train and test instances, in shifted geometric means
over 5 runs on seeds {0, . . . , 4}. We mark with * the cases in which time-limits were hit. For relpscost, we also compute
the fair number of nodes. Aggregated measures are over the entire dataset (ALL), as well as over TRAIN and TEST sets.

Instance

ALL
TRAIN
TEST

air04
air05
dcmulti
eil33-2
istanbul-no-cutoff
l152lav
lseu
misc03
neos20
neos21
neos648910
neos-476283
pp08aCUTS
rmatr100-p5
rmatr100-p10
sp150x300d
stein27
swath1
vpm2

map18
mine-166-5
neos11
neos18
ns1830653
nu25-pr12
rail507
seymour1

NoTree

TreeGate

GCNN

random

pscost

relpscost (fair)

1241.79
834.40
3068.96

645.99
789.70
203.53
7780.85
447.26
621.82
372.67
241.40
2062.23
1401.84
140.05
13759.59
267.86
443.35
908.27
868.60
1371.44
1173.14
589.03

457.89
3438.44
3326.32
15611.63
6422.37
357.00
9623.05
3202.20

1056.79
759.94
2239.47

536.07
516.06
187.49
8767.27
543.71
687.91
396.71
158.39
1962.95
1319.73
175.82
6356.81
293.74
460.48
906.04
785.27
1146.79
1165.39
440.74

*3660.32
*1391.41
*33713.63

1249.11
3318.40
160.02
24458.14
446.93
1649.27
316.37
957.10
507.95
770.97
162.24
*21077.30
327.33
747.91
1169.40
50004.09
3093.62
1690.07
313.71

*6580.79
*2516.04
*61828.29

6677.96
12685.83
599.12
12502.02
1085.16
6800.06
396.73
118.37
10049.15
7016.55
1763.05
*94411.77
337.76
1802.38
4950.77
1413.64
1378.91
1429.21
594.62

*1471.61
884.37
*4674.34

777.65
1158.89
122.39
8337.63
613.68
964.53
375.31
151.07
2730.01
1501.54
1519.01
2072.84
271.92
451.71
894.65
991.52
1322.36
1107.52
546.45

286.15 (719.20)
182.27 (558.34)
712.77 (1276.76)

8.19 (114.39)
60.25 (277.22)
9.38 (68.30)
583.34 (9668.71)
242.39 (328.25)
10.14 (250.04)
148.99 (389.88)
12.11 (294.11)
200.26 (612.75)
668.44 (1455.29)
39.83 (166.53)
204.88 (744.65)
69.66 (350.21)
411.93 (785.15)
806.35 (1214.76)
182.22 (300.42)
926.82 (1111.25)
298.58 (2485.63)
199.46 (463.12)

575.92
4996.48
3223.46
10373.80
5812.03
86.80
3779.05
1646.82

*3907.64
*233142.25
1642.07
40794.74
*22931.45
*45982.34
*75663.48
*319046.04

11655.33
*389437.62
29949.69
228715.62
288489.30
1658.41
*80575.84
*167725.65

1025.74
4190.41
4728.49
*133437.40
12307.90
342.47
4259.98
3521.47

270.25 (441.18)
175.10 (600.22)
2618.27 (5468.05)
2439.29 (5774.36)
3489.07 (4311.84)
21.39 (105.61)
543.39 (859.37)
866.32 (1096.67)

Table 10: Description of the hand-crafted input features. Features are doubled [x2] when they are computed for both upward
and downward branching directions. For features about open nodes, open lbs denotes the list of lower bound estimates of the
open nodes, while open ds the list of depths across open nodes. Table continues on the next page.

Group description (#)

Feature formula (SCIP API)

Candidate state [Ct]i, i ∈ Ct

General solution (2)

Branchings depth (2)
Branching scores (5)

PC stats (6)

Implications (2)
Cliques (2)
Cutoffs (2)
Conﬂict length (2)
Inferences (2)

Search tree state Tree t

Current node (8)

SCIPvarGetLPSol
SCIPvarGetAvgSol
1 - (SCIPvarGetAvgBranchdepthCurrentRun / SCIPgetMaxDepth) [x2]
varScore(SCIPgetVarConﬂictScore, SCIPgetAvgConﬂictScore)
varScore(SCIPgetVarConﬂictlengthScore, SCIPgetAvgConﬂictlengthScore)
varScore(SCIPgetVarAvgInferenceScore, SCIPgetAvgInferenceScore)
varScore(SCIPgetVarAvgCutoffScore, SCIPgetAvgCutoffScore)
varScore(SCIPgetVarPseudocostScore, SCIPgetAvgPseudocostScore)
SCIPgetVarPseudocostCountCurrentRun / SCIPgetPseudocostCount [x2]
SCIPgetVarPseudocostCountCurrentRun / SCIPvarGetNBranchingsCurrentRun [x2]
SCIPgetVarPseudocostCountCurrentRun / branch count [x2]
SCIPvarGetNImpls [x2]
SCIPvarGetNCliques / SCIPgetNCliques [x2]
gNormMax(SCIPgetVarAvgCutoffsCurrentRun) [x2]
gNormMax(SCIPgetVarAvgConﬂictlengthCurrentRun) [x2]
gNormMax(SCIPgetVarAvgInferencesCurrentRun) [x2]

SCIPnodeGetDepth / SCIPgetMaxDepth
SCIPgetPlungeDepth / SCIPnodeGetDepth
relDist(SCIPgetLowerbound, SCIPgetLPObjval)

Group description (#)

Feature formula (SCIP API)

Table 10 – continued from previous page

Nodes and leaves (8)

Depth and backtracks (4)

LP iterations (4)

Gap (4)

Bounds and solutions (5)

Average scores (12)

Open nodes bounds (12)

Open nodes depths (4)

relDist(SCIPgetLowerboundRoot, SCIPgetLPObjval)
relDist(SCIPgetUpperbound, SCIPgetLPObjval)
relPos(SCIPgetLPObjval, SCIPgetUpperbound, SCIPgetLowerbound)
len(getLPBranchCands) / getNDiscreteVars
nboundchgs / SCIPgetNVars
SCIPgetNObjlimLeaves / nleaves
SCIPgetNInfeasibleLeaves / nleaves
SCIPgetNFeasibleLeaves / nleaves
(SCIPgetNInfeasibleLeaves + 1) / (SCIPgetNObjlimLeaves + 1)
SCIPgetNNodesLeft / SCIPgetNNodes
nleaves / SCIPgetNNodes
ninternalnodes / SCIPgetNNodes
SCIPgetNNodes / ncreatednodes
nactivatednodes / SCIPgetNNodes
ndeactivatednodes / SCIPgetNNodes
SCIPgetPlungeDepth / SCIPgetMaxDepth
SCIPgetNBacktracks / SCIPgetNNodes
log(SCIPgetNLPIterations / SCIPgetNNodes)
log(SCIPgetNLPs / SCIPgetNNodes)
SCIPgetNNodes / SCIPgetNLPs
SCIPgetNNodeLPs / SCIPgetNLPs
log(primaldualintegral)
SCIPgetGap / lastsolgap
SCIPgetGap / ﬁrstsolgap
lastsolgap / ﬁrstsolgap
relDist(SCIPgetLowerboundRoot, SCIPgetLowerbound)
relDist(SCIPgetLowerboundRoot, SCIPgetAvgLowerbound)
relDist(SCIPgetUpperbound, SCIPgetLowerbound)
SCIPisPrimalboundSol
nnodesbeforeﬁrst / SCIPgetNNodes
gNormMax(SCIPgetAvgConﬂictScore)
gNormMax(SCIPgetAvgConﬂictlengthScore)
gNormMax(SCIPgetAvgInferenceScore)
gNormMax(SCIPgetAvgCutoffScore)
gNormMax(SCIPgetAvgPseudocostScore)
gNormMax(SCIPgetAvgCutoffs) [x2]
gNormMax(SCIPgetAvgInferences) [x2]
gNormMax(SCIPgetPseudocostVariance) [x2]
gNormMax(SCIPgetNConﬂictConssApplied)
len(open lbs at {min, max}) / nopen [x2]
relDist(SCIPgetLowerbound, max(open lbs))
relDist(min(open lbs), max(open lbs))
relDist(min(open lbs), SCIPgetUpperbound)
relDist(max(open lbs), SCIPgetUpperbound)
relPos(mean(open lbs), SCIPgetUpperbound, SCIPgetLowerbound)
relPos(min(open lbs), SCIPgetUpperbound, SCIPgetLowerbound)
relPos(max(open lbs), SCIPgetUpperbound, SCIPgetLowerbound)
relDist(q1(open lbs), q3(open lbs))
std(open lbs) / mean(open lbs)
(q3(open lbs) - q1(open lbs)) / (q3(open lbs) + q1(open lbs))
mean(open ds) / SCIPgetMaxDepth
relDist(q1(open ds), q3(open ds))
std(open ds) / mean(open ds)
(q3(open ds) - q1(open ds)) / (q3(open ds) + q1(open ds))

