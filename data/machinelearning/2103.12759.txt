Stochastic Optimal Control via Hilbert Space Embeddings of Distributions

Adam J. Thorpe, Student Member, IEEE, Meeko M. K. Oishi, Senior Member, IEEE

1
2
0
2

r
a

M
3
2

]

C
O
.
h
t
a
m

[

1
v
9
5
7
2
1
.
3
0
1
2
:
v
i
X
r
a

Abstract— Kernel embeddings of distributions have recently
gained signiﬁcant attention in the machine learning community
as a data-driven technique for representing probability distribu-
tions. Broadly, these techniques enable efﬁcient computation of
expectations by representing integral operators as elements in
a reproducing kernel Hilbert space. We apply these techniques
to the area of stochastic optimal control theory and present a
method to compute approximately optimal policies for stochas-
tic systems with arbitrary disturbances. Our approach reduces
the optimization problem to a linear program, which can
easily be solved via the Lagrangian dual, without resorting to
gradient-based optimization algorithms. We focus on discrete-
time dynamic programming, and demonstrate our proposed
approach on a linear regulation problem, and on a nonlinear
target tracking problem. This approach is broadly applicable
to a wide variety of optimal control problems, and provides
a means of working with stochastic systems in a data-driven
setting.

I. INTRODUCTION

Stochastic systems are ubiquitous, however most methods
for control of stochastic systems are reliant upon accurate
modeling not only of the dynamics, but also of the stochastic
processes of the system. As autonomous systems become
commonplace, and direct human interaction with auton-
omy become more pervasive, presumptions of linearity and
Gaussian stochasticity become questionable, as they could
lead to control solutions that are confusing, non-intuitive,
or simply incorrect. Robust solutions that work well when
uncertainty is bounded by known values may be excessively
conservative, and cannot accommodate long-tail phenomena.
In contrast, data-driven approaches do not rely upon any prior
assumptions on the dynamics or stochasticity of the system.
Data-driven approaches have garnered considerable interest
recently, due to the capabilities of learning algorithms to
handle systems with nonlinear dynamics and unknown dis-
turbances.

This material is based upon work supported by the National Science
Foundation under NSF Grant Number CNS-1836900. Any opinions, ﬁnd-
ings, and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views of the National
Science Foundation. The NASA University Leadership initiative (Grant
#80NSSC20M0163) provided funds to assist the authors with their research,
but this article solely reﬂects the opinions and conclusions of its authors
and not any NASA entity. This research was supported in part by the
Laboratory Directed Research and Development program at Sandia National
Laboratories, a multimission laboratory managed and operated by National
Technology and Engineering Solutions of Sandia, LLC., a wholly owned
subsidiary of Honeywell International, Inc., for the U.S. Department of
Energy’s National Nuclear Security Administration under contract DE-NA-
0003525. The views expressed in this article do not necessarily represent the
views of the U.S. Department of Energy or the United States Government.
A. Thorpe and M. Oishi are with Electrical & Computer Eng., University

of New Mexico, Abq., NM. Email: {ajthor,oishi}@unm.edu.

We propose a method for data-driven controller synthesis
based on conditional distribution embeddings [1], a non-
parametric learning technique that uses a sample of system
observations to construct a model of the stochastic system
dynamics as an element in a high-dimensional Hilbert space
of functions known as a reproducing kernel Hilbert space.
These techniques leverage functional analysis and statistical
learning theory to empirically estimate the stochastic kernel
using data. As a nonparametric technique, kernel methods
are inherently data-driven, and do not rely upon prior as-
sumptions placed upon the data or exploit system structure.
These techniques have been applied to Markov models [2],
partially-observable systems [3, 4], and more recently, to
robust optimization approaches [5, 6]. Furthermore, these
techniques admit ﬁnite sample bounds which show conver-
gence in probability as the number of samples tend to inﬁnity
[1]. A Hilbert space framework is particularly well-suited to
stochastic optimal control problems [7], primarily because
Hilbert spaces are a generalization of inner product spaces
to an inﬁnite-dimensional setting, meaning they encompass
many optimization problems of interest (note that Rn is a
Hilbert space).

The use of kernel methods for policy synthesis is well-
motivated in literature, especially in the area of reinforcement
learning (RL) [8]. Methods have been developed to optimize
a policy in an RKHS via functional gradient descent [9],
[10]. Other approaches rely upon value iteration, approxi-
mating the value function as an intermediate step in order to
compute an optimal control input [2]. However, most of these
approaches face signiﬁcant computational challenges due to
the sampling schemes used by RL, the need for knowledge
of a gradient, or reliance upon iterative numerical methods.
Some progress has been made to alleviate these issues, for
example using stochastic factorization [11].

Our main contribution is a data-driven algorithm for com-
puting approximately optimal policies for arbitrary discrete-
time stochastic dynamical systems. The novelty of our ap-
proach is the use of conditional distribution embeddings to
formulate an optimal control problem as a linear program
within a reproducing kernel Hilbert space, which can be
solved efﬁciently via the Lagrangian dual. Our approach is
model-free, since it relies only upon data collected from
prior observations of the system execution, meaning that
it is amenable to systems with arbitrary disturbances and
nonlinear dynamics. Because we formulate the optimal con-
trol problem as a linear program, we do not rely upon
gradient-based algorithms to compute an optimal solution,
and thus do not impose a speciﬁc structure on the policy for
the purpose of computing a functional gradient. The main

 
 
 
 
 
 
difﬁculty associated with our approach is the dependence
of the computational complexity on sample size (generally
O(M 3)), as with all kernel based approaches. This arises
from the presence of a matrix inverse operation and the
large number of observations needed to fully characterize the
stochasticity of a system. Fortunately, numerous approaches
to reducing the computational burden of kernel methods
have been explored, such as [12, 13], which use Fourier
transforms and Gaussian matrix approximations to reduce
the computational complexity to log-linear time.

The paper is structured as follows: In section II, we
deﬁne the problem and describe the preliminary theory of
embedding distributions in reproducing kernel Hilbert spaces
in section III. We then present our method in section IV to
compute the optimal policy and present an extension of our
proposed approach to solve dynamic programming problems
over a ﬁnite time horizon. In section V, we demonstrate
our proposed approach on a simple stochastic integrator
system for the purpose of validation against a known result,
tracking problem using nonlinear,
and then on a target
nonholonomic vehicle dynamics. Concluding remarks are
presented in section VI.

II. PRELIMINARIES

We use the following notation throughout: Let E be
an arbitrary nonempty space, and denote the σ-algebra on
E by E. If E is a topological space [14], the σ-algebra
generated by the set of all open subsets of E is called the
Borel σ-algebra, denoted by B(E). Let (Ω, F , P) denote
a probability space, where F is the σ-algebra on Ω and
P : F → [0, 1] is a probability measure on the measurable
space (Ω, F ). A measurable function X : Ω → E is called
a random variable taking values in (E, E). The image of
P under X, P(X −1A), A ∈ E is called the distribution of
X. Let T be an arbitrary set, and for each t ∈ T , let Xt
be a random variable. The collection of random variables
{Xt : t ∈ T } on (Ω, F ) is a stochastic process. We deﬁne
a stochastic kernel according to [14].

Deﬁnition 1 (Stochastic Kernel). Let (E, E) and (F, F ) be
measurable spaces with σ-algebras E and F , respectively.
A stochastic kernel is a map κ : F × E → [0, 1], where:
1) x 7→ κ(B | x) is E-measurable for all B ∈ F ; 2) B 7→
κ(B | x) is a probability measure on (F, F ) for all x ∈ E.

A. System Model

Consider a Markov control process, which is deﬁned in

[15] as a 3-tuple, (X , U, Q), consisting of:

• A Borel space X ⊆ Rn called the state space;
• A compact Borel space U ⊂ Rm called the control

space; and

• A stochastic kernel Q : B(X ) × X × U → [0, 1]
that assigns a probability measure Q(· | x, u) to each
(x, u) ∈ X × U on the measurable space (X , B(X )).
The system evolves from an initial condition x0 ∈ X ,
which may be chosen from an initial distribution P0 on X ,
over a ﬁnite time horizon t = 0, 1, . . . , N , N ∈ N+. As

the system evolves, the control actions u0, u1, . . . , uN −1 are
chosen from a Markov control policy π.

Deﬁnition 2 (Markov Policy, [16, Deﬁnition 8.2]). A Markov
policy π is a sequence π = {π0, π1, . . . πN −1} of univer-
sally measurable stochastic kernels, where for each t =
0, 1, . . . , N −1, the stochastic kernel πt : B(U)×X → [0, 1]
assigns a probability measure πt(· | x) to every x ∈ X on
the measurable space (U, B(U)).

B. Problem Formulation

We assume that the stochastic kernel Q is unknown, but
that a sample of observations of the system evolution is
available.

Assumption 1. We assume that Q is unknown, but that a
i=1 of size M ∈ N+ is available,
sample S = {(xi, ui, xi
′ ∼ Q(· | xi, ui) and ui is selected randomly from
where xi
the set of admissible control inputs.

′)}M

Consider an arbitrary cost function c : X → R, which
we assume is a continuous, bounded functional that lies in a
Hilbert space of functions H . At any time instant t, we seek
to minimize c by selecting the distribution πt on (U, B(U))
which minimizes the following unconstrained minimization
problem:

min
πt

Jt(π) =

ZU ZX

c(y)Q(dy | x, v)πt(dv | x)

(1)

The primary difﬁculty in solving (1) is that without knowl-
edge of Q, the integral in (1) is intractable. Thus, we seek to
form an approximate optimization problem by approximating
the integral in (1) using a sample S taken i.i.d. from Q as an
element in a Hilbert space of functions. By optimizing the
approximate problem, we obtain an approximate solution.
Thus, we additionally seek to ensure that the approximate
optimization problem converges in probability to the true
optimization problem as the sample size increases.

According to [16], in most cases, the optimal Markov
policy for a system can be viewed as nonrandomized, or
deterministic, meaning the stochastic kernel assigns a prob-
ability measure with mass one at a single element in U to
each x ∈ X . According to [15, 16], the set of nonrandomized
policies is a subset of the set of all randomized policies,
meaning we can search among the class of randomized
policies in Hilbert space to ﬁnd an optimal policy which
minimizes (1).

III. EMBEDDING STOCHASTIC KERNELS IN AN RKHS
Let H be a Hilbert space of functions of the form X → R

with inner product h·, ·iH and the induced norm k·kH .
Deﬁnition 3 (RKHS, [17]). A Hilbert space H is a repro-
ducing kernel Hilbert space (RKHS) if there exists a positive
deﬁnite [18, Deﬁnition 4.12] kernel function k that satisﬁes
the following properties:

k(x, ·) ∈ H ,
f (x) = hf, k(x, ·)iH ,

∀x ∈ X
∀f ∈ H , x ∈ X

(2)
(3)

where (3) is known as the reproducing property, and for any
x, x′ ∈ X , we denote k(x, ·) ∈ H as a function on X such
that x′ 7→ k(x, x′).

Remark 1. Alternatively, by the Moore-Aronszajn theorem
[17], we can deﬁne an RKHS by ﬁrst specifying a kernel k
and obtain a corresponding RKHS as the closure of the span
of kernel functions.

Given (x, u) ∈ X × U, let Q(· | x, u) be a conditional
probability measure on X . According to [1], if the following
sufﬁcient condition holds:

where β(x, u) ∈ RM is a vector of real-valued coefﬁcients
that depends on the conditioning variables x and u. The
problem in (9) admits a closed-form solution, given by:
ˆm(x, u) = Φ⊤(ΨΨ⊤ + λM I)−1Ψk(x, ·)k(u, ·)

(11)

where Φ and Ψ are called feature vectors, with elements
′, ·) and Ψi = k(xi, ·)k(ui, ·), respec-
given by Φi = k(xi
tively. For simplicity, we denote W = (ΨΨ⊤ + λM I)−1
and let β(x, u) = W Ψk(x, ·)k(u, ·), such that ˆm(x, u) =
Φ⊤β(x, u). Using ˆm(x, u), we can approximate the expec-
tation with respect to Q(· | x, u) for any f ∈ H as:

k(y, y)Q(dy | x, u) < ∞

ZX p

(4)

hf, ˆm(x, u)iH ≈

ZX

f (y)Q(dy | x, u)

(12)

then there exists an element m(x, u) ∈ H called a condi-
tional distribution embedding, where

m(x, u) :=

ZX

k(y, ·)Q(dy | x, u)

(5)

By the reproducing property of k in H , for any f ∈ H ,
we can evaluate the integral with respect to Q(· | x, u) as an
inner product with the embedding m(x, u):

hf, m(x, u)iH =

f,

(cid:28)

ZX

k(y, ·)Q(dy | x, u)

(cid:29)H

=

=

ZX

ZX

hf, k(y, ·)iH Q(dy | x, u)

f (y)Q(dy | x, u)

(6)

(7)

(8)

Intuitively, the element m ∈ H corresponds to the dynamics
of the system at the point (x, u). In other words, if the
integral exists, then we can embed the integral operator with
respect to the probability measure in H and evaluate the
integral via the reproducing property of k in H .

However, in a data-driven setting, the stochastic kernel
Q is unknown, which means the embedding m(x, u) is also
unknown. Instead, we can empirically estimate the stochastic
kernel using a sample of observations taken from Q.

A. Empirical Embeddings Using Observations

′)}M

Consider a sample S = {(xi, ui, xi

i=1 of size M ∈
N+, taken i.i.d. from Q, where xi and ui are taken randomly
from the state and control spaces X and U, respectively, and
′ ∼ Q(· | x, u). As shown in [19], we can compute an
xi
empirical estimate ˆm of m as the solution to a regularized
least-squares problem, given by:

min
ˆm

1
M

M

Xi=1

kk(xi

′, ·) − ˆm(xi, ui)k2

H + λk ˆmk2

Q (9)

where λ > 0 is the regularization parameter and Q is a
vector-valued RKHS [19]. As shown in [19, 20], by the
representer theorem, the solution ˆm to (9) is unique and has
the following form:

ˆm(x, u) =

M

Xi=1

βi(x, u)k(xi

′, ·)

(10)

Further, if the kernel function k is universal [21], then
the embedding is injective, meaning there exists a unique
representation of the distribution in H . In short, a universal
kernel k allows us to approximate any arbitrary real-valued
function using (10) arbitrarily well as the number of samples
tends to inﬁnity. A commonly used kernel function which
satisﬁes this property is the Gaussian kernel k(x, x′) =
exp(−kx − x′k2
2/2σ2), σ > 0. Additionally, the estimate
ˆm converges in probability to the true embedding m as the
number of samples M tends to inﬁnity and λ → 0 [1, 22].
This means the estimate ˆm is a consistent estimator of the
true embedding, and the integral of a function f ∈ H with
respect to Q converges in probability to the true result as the
sample size increases.

IV. POLICY OPTIMIZATION IN HILBERT SPACE
Consider the problem in (1) where c ∈ H . As shown in
[1], if the sufﬁcient condition in (4) holds, then there exists
a conditional distribution embedding m(x, u) such that for
any c ∈ H ,

c(y)Q(dy | x, u) = hc, m(x, u)iH

(13)

ZX

This allows us to evaluate the expected cost at a particular
(x, u) ∈ X × U as an inner product in Hilbert space. Let π
be a Markov policy as in Deﬁnition 2. Taking the integral
of (13) with respect to the Markov policy π, we obtain the
objective function Jt(π) in (1). By linearity of the integral
and the inner product, we can rewrite the objective using the
inner product in (13) to obtain:

Jt(π) =

c,

(cid:28)

ZU

m(x, v)πt(dv | x)

(cid:29)H

(14)

where the integral term on the right hand side of (14) can
be interpreted as a representation in H of the closed-loop
dynamics under a policy π.

However, the integral in (14) is intractable, since accord-
ing to Assumption 1,
the stochastic kernel Q (and thus
the embedding m) is unknown. Instead, we compute an
empirical estimate ˆm of m using a sample S taken i.i.d.
from Q. Recall from (11) that the empirical estimate has the
form ˆm(x, u) = Φ⊤W Ψk(x, ·)k(u, ·). We then substitute the
estimate for the true embedding to approximate the integral

in (14).

m(x, v)πt(dv | x) ≈

ˆm(x, v)πt(dv | x)

ZU

ZU

= Φ⊤W Ψk(x, ·)

ZU

k(v, ·)πt(dv | x)

(15)

Recall that the policy is a collection of stochastic kernels in-
dexed by time, which means that at a given time t, the policy
can be represented by a conditional distribution embedding.
Thus, it is natural to consider the policy at a particular time
as a collection of elements in an RKHS parameterized by x ∈
X , which admits a representation in terms of ﬁnite support.
Let {˜uj}P
j=1 be a collection of admissible control inputs. We
propose the following representation for the policy πt:

P

ˆpt(x) =

αj(x)k(˜uj , ·)

(16)

Xj=1

the Lagrangian dual. Let ν ∈ R be a dual variable, and for
simplicity, let C(x) = c⊤W Ψk(x, ·)Υ⊤. The dual problem
is given by:

max − ν

(20a)

s.t. − 1ν (cid:22) C(x)⊤
where 1 is a vector of all ones. From [23, §4], (20) has an
optimal solution, given by mini{Ci(x)⊤}, which means the
optimal solution α(x)∗ to (19) is a vector of all zeros, except
αi(x)∗ = 1. In other words, we choose the control input that
corresponds to the minimal value of C(x).

(20b)

A. Application to Approximate Dynamic Programming

Many optimal control problems can be formulated as
dynamic programs. Consider the following problem with an
additive cost, in which we seek a policy π that minimizes
the following optimization problem [16]:

N −1

Xi=0

gt(xt, ut)

(cid:21)

(21)

where α(x) ∈ RP is a vector of real-valued coefﬁcients that
depends on x ∈ X . Using (16), we can approximate (15) as:

min
π

JN (π) = Eπ

gN (xN ) +

(cid:20)

ZU

Φ⊤W Ψk(x, ·)

k(v, ·)πt(dv | x) ≈ Φ⊤W Ψk(x, ·)Υ⊤α(x)
(17)
where Υ is a feature vector with elements Υj = k(˜uj, ·).
Thus, we can approximate the objective function Jt(π) by:

ZU ZX

c(y)Q(dy | x, v)πt(dv | x) ≈ c⊤W Ψk(x, ·)Υ⊤α(x)
(18)
′). Thus,
where c is a vector with elements ci = c(xi
we form an approximation of the objective in (1), which
converges in probability to the true optimization problem
as the sample sizes M and P increase [1]. Now, instead
of minimizing over the distribution πt, we can view the
approximate optimization problem as ﬁnding α(x) ∈ RP
which minimizes (18). However, minimizing α(x) in (18)
is unbounded below, which makes the problem unsolvable.
As such, additional constraints are required to ensure that
the problem admits a feasible solution. Note that intuitively,
ˆpt(x) is an approximation of the distribution πt(· | x) at
time t. Because of this, we can view the coefﬁcients α(x)
as a vector of probabilities which weight
the nonlinear
transformations of the control inputs. Thus, we place ad-
ditional constraints on the coefﬁcients α(x), and form the
approximate optimization problem, constraining the values
of α(x) such that they are non-negative and sum to one:

min
α(x)∈RP

c⊤W Ψk(x, ·)Υ⊤α(x)

P

s.t.

αj(x) = 1

Xj=1
0 (cid:22) α(x)

(19a)

(19b)

(19c)

Since α(x) is an indirect weighting on the inputs that
depends on the state x, we interpret α(x) as a probability
weighting of the control inputs ˜uj. Note that (19) is a linear
program in standard form [23], and that we can solve (19) via

where N ∈ N+ is the time horizon, π is the control policy,
gN is the terminal cost for ending in state xN , gt is the
cost at time t of taking action ut ∼ πt(· | xt) while in
state xt, and the expectation is uniquely determined by the
initial distribution P0 and the Markov policy π (see [16,
Deﬁnition 8.3] for more details). The problem in (21) can
be rewritten via the Chapman-Kolmogorov identity and the
Markov property as a sequence of sub-problems, where the
problem is solved backward in time via backward recursion
[16]. We deﬁne the value functions Vt : X → R for all
t = 0, 1, . . . , N − 1 as:

πt ZU ZX

Vt(x) = max

g(x, v) + Vt+1(y)Q(dy | x, v)πt(dv | x)
(22)
initialized with VN (xN ) = gN (xN ). Then the solution to
(21) is equivalent to solving a sequence of sub-problems
given by (22), and iteratively substituting the solutions into
the subsequent value function.

We can apply (19) in this context to solve for the optimal
control policy when the dynamics and stochasticity are not
known, but a sample S is available. In this case, we solve
(22) at each time step t using (19). By approximating and
recursively substituting the solution to (22) into the subse-
quent value function, we obtain an approximately optimal
control policy π∗ ≈ arg minπ JN (π) which approximately
minimizes the cost in (21).

This means we can compute the approximately optimal
policy for a problem without exploiting knowledge of the
system dynamics or the structure of the disturbance. By
solving for the approximately optimal policy using (19), we
avoid intractable integrals in the stochastic optimal control
problem and can compute the policy as a linear operation in
a Hilbert space of functions. Additionally, this approach is
largely agnostic to the dimensionality of the system, since the
system dimensionality only directly affects the computation
of the kernel function. For example, the Gaussian kernel

scales linearly as the system dimensionality is increased.
However, higher-dimensional systems typically require a
larger sample size in order to fully characterize the dynamics
of the system, which can be computationally prohibitive if
the sample size is large.

V. NUMERICAL RESULTS

We demonstrate our approach on a 2-D discrete-time
stochastic integrator system for the purpose of veriﬁcation,
and on a target tracking problem with nonholonomic vehicle
dynamics to demonstrate the utility of the approach. For all
problems, we used a Gaussian kernel k(x, x′) = exp(−kx −
x′k2
2/2σ2). Following [1], we chose the regularization pa-
rameter to be λ = 1/M 2, where M is the sample size,
as the default parameter for our calculations. In practice,
the parameters σ and λ are chosen via cross-validation,
where σ is selected according to the relative “spacing”
of the observations, and λ is a “smoothness” parameter
chosen such that λ → 0 as M → ∞. A more detailed
discussion of parameter selection is outside the scope of the
current work (see [1, 24] for more information). Numerical
experiments were performed in Matlab on an AWS cloud
computing instance, and computation times were obtained
using Matlab’s Performance Testing Framework.

Code to reproduce the analysis and all ﬁgures is provided

at: github.com/unm-hscl/ajthor-CDC2021.

A. Double Integrator System

We consider the problem of regulation for a system whose
dynamics are governed by a stochastic 2-D discrete time
stochastic integrator system, without any knowledge of the
dynamics or the stochastic processes. That is, we seek a
distribution π which minimizes the following optimization
problem:

min
π

ZU ZX

c(y)Q(dy | x, v)π(dv | x)

(23)

where Q is a representation of the unknown system dynamics
as a stochastic kernel. For the purpose of comparison, we
chose the cost function c : X → R to be the norm function:

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

Fig. 1. Vector ﬁeld showing the optimal closed-loop dynamics of a 2-D
discrete-time integrator system under an optimal control strategy computed
via CVX (blue). The vector ﬁeld of the closed-loop dynamics of a stochastic
integrator system with a Gaussian disturbance computed using our proposed
algorithm (orange).

0.5

0

-0.5

0.5

0

-0.5

-0.5

0

0.5

-0.5

0

0.5

Fig. 2.
(Left) Vector ﬁeld of the approximately optimal closed-loop
dynamics of a 2-D stochastic integrator system with a Beta disturbance,
where the approximately optimal policy is computed using our proposed
approach. (Right) Vector ﬁeld of the approximately optimal closed-loop
system with an exponential disturbance.

c(x) = kxk2

given by:

(24)

which serves to drive the system to the origin. The dynamics
for a 2-D discrete time stochastic integrator system with
sampling time Ts are given by:

xt+1 =

1 Ts
0

1 (cid:21)

(cid:20)

xt +

T 2

s /2
Ts (cid:21)

(cid:20)

ut + wt

(25)

where xt ∈ X is the state, ut ∈ U is the control input, which
we specify to lie within the bounds ut ∈ [−1, 1], and w is
a stochastic process, comprised of the random variables wt
on the measurable space (Rp, B(Rp)). We consider three
distributions for the disturbance: 1) A Gaussian distribution
wt ∼ N (0, Σ), Σ = 0.01I; 2) A beta distribution wt ∼
0.1Beta(α, β), with a probability density function (PDF)

f (x | α, β) =

Γ(α + β)
Γ(α)Γ(β)

xα−1(1 − x)β−1

(26)

We consider a sample S = {(xi, ui, xi

where Γ is the Gamma function and shape parameters
α = 2, β = 0.5; and 3) An exponential distribution wt ∼
0.01Exp(α), with α = 3 and PDF f (x | α) = α exp(−αx).
i=1 of observa-
tions of size M = 1600 taken i.i.d. from Q, a representation
of (25) as a Markov control process. The states xi ∈ X
were selected uniformly in the range xi ∈ [−1, 1] × [−1, 1],
the control inputs ui ∈ U were chosen in the range ui ∈
[−1.1, 1.1], and the resulting states were generated according
′ ∼ Q(· | xi, ui). We then presumed no knowledge of
to xi
the system dynamics or the structure of the disturbance for

′)}M

0.3

0.2

0.1

0

0

0.01

0.005

0

0

Gaussian Disturbance
Beta Disturbance
Exponential Disturbance

2000

4000

6000

8000

10

5

0

0

10

20

30

40

50

60

Gaussian Disturbance
Beta Disturbance
Exponential Disturbance

Fig. 4. Mean computation time of the optimal control solution, computed
using CVX [25] (blue) vs. the mean computation time of the kernel based
algorithm (orange) as a function of the number of evaluation points R. The
sample size used to construct the estimate ˆm is M = 1600, and the number
of admissible control inputs is P = 100.

2000

4000

6000

8000

(Top) Maximum error of the control

Fig. 3.
inputs computed via
our proposed method vs. the optimal control inputs computed via CVX.
(Bottom) Mean error of the control inputs. The mean error is less than
0.005 when M > 1000.

the purpose of computing the approximately optimal control
inputs using our proposed method.

Using S, we then computed an estimate ˆm according
to (11), which can be viewed as an empirical estimate
of the system dynamics. We used a bandwidth parameter
σ = 1 for our calculations, which was determined by cross-
validation. We then chose a collection of admissible control
inputs {˜uℓ}L
ℓ=1, L = 100, in the range ˜uℓ ∈ [−1, 1] to
compute the estimator ˆp in (16). We then selected R = 25
evaluation points {xj}R
j=1, chosen uniformly in the region
[−1, 1] × [−1, 1] from which to compute the approximately
optimal control inputs.

In order to demonstrate the effectiveness of the method,
we computed the optimal control inputs using CVX [25]
from the evaluation points {xj}R
j=1 using the determinis-
tic dynamics. Once we computed the optimal inputs, we
propagate the dynamics forward in time using the optimal
inputs to obtain the state at the next time instant. We then
plotted the vector ﬁeld of the closed-loop dynamics under the
optimal control input in Figure 1 (blue). We then computed
the approximately optimal control inputs using (19) using the
sample S taken from the stochastic dynamics to minimize
the cost c at each point xj over a single time step. Using
the computed inputs, we then plotted the vector ﬁeld of
the closed-loop dynamics in Figure 1 (orange) to compare
against the optimal control inputs computed via CVX.

We can see in Figure 1 that the control inputs selected by
the algorithm are close to the optimal control inputs obtained
by CVX, especially further away from the origin, where the
computed control inputs coincide almost exactly with the
optimal control inputs. Closer to the origin, we see that the

6

4

2

0

0

2000

4000

6000

8000

10000

Fig. 5. Mean computation time of the kernel based algorithm (blue) as
a function of the sample size M used to construct the approximation. The
number of evaluation points is R = 10, and the number of admissible
control inputs is P = 100. The computation time increases exponentially
as the sample size increases.

algorithm deviates slightly from the optimal control inputs,
which we anticipate is due largely to the randomness of the
state observations.

We then generated a new sample S of the system in (25)
affected by a disturbance with a beta distribution, and then
from (25) affected by a disturbance with an exponential
distribution and computed the optimal control inputs using
our proposed method. The vector ﬁelds of the closed-loop
dynamics for these cases are shown in Figure 2. We can
see that the algorithm computes an approximately optimal
controller, despite the state observations being affected by a
non-Gaussian disturbance.

Note that the quality of the approximation obtained from
our method depends on the sample sizes M . As the number
of observations in the sample increase, the approximately
optimal control inputs converge to the actual optimal con-
trol inputs. In order to demonstrate this, we computed the
maximum and mean squared Euclidean error between the
control inputs computed via our method and the optimal
control inputs computed via CVX for varying sample sizes
M ∈ [100, 8000] in order to characterize the performance
of the algorithm. The results are shown in Figure 3. We
can see that the error of the approximately optimal control
inputs decreases quickly as the sample size increases, and
that the mean error is approximately less than 0.005 when the

sample size is greater than M > 2000. However, we can also
see that the quality of the approximation does not improve
signiﬁcantly as the sample size increases, which is due to the
asymptotic convergence of the estimate ˆm to the true em-
bedding m [1]. This presents a tradeoff between computation
time and numerical accuracy, especially since the complexity
scales exponentially as the sample size increases.

The computation times for both approaches are shown in
Figure 4 as a function of the evaluation points up to R = 51.
We can see from Figure 4 that the computation times for the
CVX optimization method increases roughly linearly as the
number of evaluation points increases, since the optimization
problem needs to solve for each point independently. The
computation time for our proposed method is also roughly
linear in the number of evaluation points, but is dominated
primarily by the computation time required for the matrix
inversion, which increases exponentially with the sample size
M , and is generally O(M 3) [1].

This is demonstrated empirically in Figure 5, where we
compute the mean computation time as a function of the
sample size M . We can see that as the sample size increases,
the computation time increases exponentially. However, as
mentioned earlier, the computation time of the kernel based
approach can also be improved using existing speedup tech-
niques [12, 13].

This also illustrates the computational advantage of our
proposed method, since the quality of the approximation ob-
tained via kernel methods has a mean error of roughly 0.005
with a sample size of M = 1600, but is able to compute the
optimal control inputs an order of magnitude faster than the
optimal solution via CVX for multiple evaluation points.

B. Nonholonomic Vehicle

We consider the problem of target tracking for a system
with nonholonomic vehicle dynamics as deﬁned in [26],
modiﬁed such that it has a minimum forward velocity. The
dynamics with sampling time Ts are given by:

˙x1 = (u1 + Vmin) sin(x3) + w
˙x2 = (u1 + Vmin) cos(x3) + w
˙x3 = u2 + w

(27)

where Vmin = 0.1 is the minimum, constant forward velocity,
[x1, x2, x3] ∈ X ⊆ R3 are the states, [u1, u2]⊤ ∈ U ∈ R2 are
the control inputs, and w ∼ N (0, Σ) is a random variable on
the measurable space (Rp, B(Rp)), where Σ = 0.1I. We de-
ﬁne a target trajectory, moving from xinit = [−1, −1, π/4]⊤
to xﬁnal = [1, 1, π/4]⊤ (shown in black in Fig. 6 and Fig. 7),
and deﬁne the cost function such that the goal is to minimize
the squared Euclidean distance from the system’s position to
the target trajectory’s position at each time step.
′)}M
We consider a sample S = {(xi, ui, xi

i=1 of size
M = 1600 taken i.i.d. from Q, a representation of (27) as a
Markov control process. The states xi were drawn uniformly
in the range [−1.1, 1.1] × [−1.1, 1.1] × [−6, 6], the control
inputs ui were drawn uniformly in the range [−0.1, 1.2] ×
[−10.1, 10.1], and then xi

′ drawn from Q(· | xi, ui).

1

0.5

0

-0.5

-1

-1

-0.5

0

0.5

1

Fig. 6. Trajectory computed using our proposed method (orange) which
tracks the target trajectory (black). The control actions are computed forward
in time, with the system selecting the approximately optimal control action
at each time step.

1

0.5

0

-0.5

-1

-1

-0.5

0

0.5

1

Fig. 7. Trajectory computed using our proposed method (orange) which
tracks the target
trajectory (black). The control actions are computed
backward in time using dynamic programming. Note that the trajectory
more closely follows the target using dynamic programming.

We then computed an estimate ˆm according to (11) and
used a bandwidth parameter of σ = 3 for the kernel function,
determined by cross-validation. We then selected a collection
{˜uℓ}L
ℓ=1 of L = 231 admissible control inputs within the
range [0, 1] × [−10, 10] to construct ˆp as in (16). We choose
an initial condition x0 = [−0.8, 0, π]⊤, and evolve the
system forward in time via (27) over a time horizon N = 20,
computing an approximately optimal control action at each
time step using our proposed method. The resulting trajectory
is plotted in Figure 6 (orange), and the computation time
over the time horizon was approximately 0.538 seconds. As
expected, we can see that the control actions selected from
our proposed method drive the system to closely follow the
target trajectory.

We then computed the optimal controller via dynamic
programming in order to compare against the forward in

[11] A. M. S. Barreto, D. Precup, and J. Pineau, “Practical kernel-based
learning,” J. Mach. Learn. Res., vol. 17, no. 1, p.

reinforcement
2372–2441, 2016.

[12] A. Rahimi and B. Recht, “Random features for large-scale kernel
machines,” in Proc. Int. Conf. on Neural Inf. Process. Syst., 2007,
p. 1177–1184.

[13] Q. Le, T. Sarl´os, and A. Smola, “Fastfood: Approximating kernel
expansions in loglinear time,” in Proc. Int. Conf. on Mach. Learn.
- Volume 28, 2013, p. III–244–III–252.

[14] E. C¸ inlar, Probability and Stochastics. Springer, 2011.
[15] M. Puterman, Markov Decision Processes: Discrete Stochastic Dy-

namic Programming.

John Wiley & Sons, 2005.

[16] D. Bertsekas and S. Shreve, Stochastic optimal control: the discrete

time case. Elsevier, 1978.

[17] N. Aronszajn, “Theory of reproducing kernels,” Trans. of the Amer.

Math. Soc., vol. 68, no. 3, pp. 337–404, 1950.

[18] I. Steinwart and A. Christmann, Support vector machines. Springer,

2008.

[19] S. Gr¨unew¨alder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton,
and M. Pontil, “Conditional mean embeddings as regressors,” in Proc.
Int. Conf. on Mach. Learn., 2012, p. 1803–1810.

[20] C. Micchelli and M. Pontil, “On learning vector-valued functions,”

Neural Comput., vol. 17, no. 1, p. 177–204, 2005.

[21] C. Micchelli, Y. Xu, and H. Zhang, “Universal kernels,” J. Mach.

Learn. Res., vol. 7, p. 2651–2667, 2006.

[22] L. Song, A. Gretton, and C. Guestrin, “Nonparametric tree graphical
models,” in Proc. Int. Conf. on Artif. Intell. and Statist., vol. 9, 2010,
pp. 765–772.

[23] S. Boyd, S. Boyd, and L. Vandenberghe, Convex optimization. Cam-

bridge University Press, 2004.

[24] A. Caponnetto and E. De Vito, “Optimal rates for the regularized
least-squares algorithm,” Foundations of Computational Mathematics,
vol. 7, no. 3, pp. 331–368, 2007.

[25] M. Grant and S. Boyd, “CVX: Matlab software for disciplined convex

programming, version 2.1,” 2014.

[26] H.-T. Chiang, N. Malone, K. Lesser, M. Oishi, and L. Tapia, “Path-
guided artiﬁcial potential ﬁelds with stochastic reachable sets for
motion planning in highly dynamic environments,” in IEEE Int. Conf.
on Robot. and Automation, 2015, pp. 2347–2354.

time approach. Unlike the previous approach, in which the
control actions are selected in a greedy fashion, the dynamic
programming approach computes the optimal control actions
backward in time by iteratively optimizing a sequence of
value functions (22), and then selecting the control actions
at each time step which have the highest value. We use
the same tracking trajectory as before, as well as the same
initial condition in order to compare the performance of
the two approaches. The resulting trajectory is shown in
Figure 7 (orange). The computation time for the dynamic
programming solution was approximately 6.448 seconds.

As expected, we can see that the trajectories obtained
from the two approaches both follow the target trajectory,
but the dynamic programming solution follows the trajectory
better over the entire time horizon. This is because the
value functions take into account the future actions of the
system in order to minimize the total cost. This shows that
our algorithm is able to select the approximately optimal
control actions at each time step for a nonlinear system
either forward in time or backward in time via dynamic
programming, using only sample information taken from
observations of the system evolution.

VI. CONCLUSIONS & FUTURE WORK

In this paper, we have presented a novel method for
computing the optimal policy for discrete-time dynamic
programming problems using observations taken from a
stochastic system under an arbitrary disturbance. Our method
is model-free and largely agnostic to the cost function
used. We have demonstrated our proposed method on a
discrete time stochastic double integrator system and on a
nonholonomic vehicle target tracking problem. We plan to
explore further theoretical extensions of this method to other
classes of stochastic control problems, and to constrained
optimal control problems.

REFERENCES

[1] L. Song, J. Huang, A. Smola, and K. Fukumizu, “Hilbert space
embeddings of conditional distributions with applications to dynamical
systems,” in Proc. Int. Conf. on Mach. Learn., 2009, p. 961–968.
[2] S. Gr¨unew¨alder, G. Lever, L. Baldassarre, M. Pontil, and A. Gretton,
“Modelling transition dynamics in MDPs with RKHS embeddings,”
in Proc. Int. Conf. on Mach. Learn., 2012, p. 1603–1610.

[3] Y. Nishiyama, A. Boularias, A. Gretton, and K. Fukumizu, “Hilbert
space embeddings of POMDPs,” in Proc. Conf. on Uncertainty in
Artif. Intell., 2012, p. 644–653.

[4] L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola, “Hilbert
space embeddings of hidden Markov models,” in Proc. Int. Conf. on
Mach. Learn., 2010, p. 991–998.

[5] J.-J. Zhu, W. Jitkrittum, M. Diehl, and B. Sch¨olkopf, “Kernel dis-
tributionally robust optimization,” ArXiv Preprint ArXiv:2006.06981,
2020.

[6] J.-J. Zhu, B. Sch¨olkopf, and M. Diehl, “A kernel mean embedding
approach to reducing conservativeness in stochastic programming and
control,” in Learn. for Dynamics and Ctrl., 2020, pp. 915–923.

[7] D. Luenberger, Optimization by Vector Space Methods.

John Wiley

& Sons, 1997.

[8] D. Ormoneit and S. Sen, “Kernel-based reinforcement learning,” Mach.

Learn., vol. 49, no. 2–3, p. 161–178, 2002.

[9] J. Bagnell and J. Schneider, “Policy search in kernel Hilbert space,”

2003.

[10] G. Lever and R. Stafford, “Modelling Policies in MDPs in Repro-
ducing Kernel Hilbert Space,” in Proc. Int. Conf. on Artif. Intell. and
Statist., vol. 38, 2015, pp. 590–598.

