1
2
0
2

c
e
D
8
2

]

G
L
.
s
c
[

1
v
0
3
4
5
1
.
2
1
1
2
:
v
i
X
r
a

Robustness and risk management via distributional dynamic programming

Robustness and risk management via distributional dynamic
programming

Mastane Achab
Universitat Pompeu Fabra
Barcelona, Spain

Gergely Neu
Universitat Pompeu Fabra
Barcelona, Spain

mastane.achab@gmail.com

gergely.neu@gmail.com

Abstract

In dynamic programming (DP) and reinforcement learning (RL), an agent learns to act
optimally in terms of expected long-term return by sequentially interacting with its envi-
ronment modeled by a Markov decision process (MDP). More generally in distributional
reinforcement learning (DRL), the focus is on the whole distribution of the return, not
just its expectation. Although DRL-based methods produced state-of-the-art performance
in RL with function approximation, they involve additional quantities (compared to the
non-distributional setting) that are still not well understood. As a ﬁrst contribution, we
introduce a new class of distributional operators, together with a practical DP algorithm
for policy evaluation, that come with a robust MDP interpretation. Indeed, our approach
reformulates through an augmented state space where each state is split into a worst-case
substate and a best-case substate, whose values are maximized by safe and risky policies
respectively. Finally, we derive distributional operators and DP algorithms solving a new
control task: How to distinguish safe from risky optimal actions in order to break ties in
the space of optimal policies?

Keywords:
age value-at-risk, coherent risk measure, linear programming

robust Markov decision process, distributional reinforcement learning, aver-

1. Introduction

This paper is concerned with robust sequential decision making in an uncertain environment,
modeled by a Markov decision process (MDP). In the classical setting, the decision maker
is looking for a strategy (or “policy”) that is optimal in terms of a risk-neutral objective
function. Typically, this objective is the expected value of some random cumulative return,
accounting for both immediate and future rewards. The dynamic programming (DP) ap-
proach comes with practical algorithms to evaluate and optimize such objective functions,
given the knowledge of the environment’s dynamics (see Puterman, 2014). DP is a pop-
ular framework for many applications ranging from computer programming to economics
and inventory management: we refer to Bertsekas et al. (2000) for an overview. Reinforce-
ment learning (RL) aims at solving the same problem as DP when the dynamical model
is unknown: the learner only observes trajectories sampled from the MDP (see Sutton and
Barto, 2018).

An inherent feature of standard dynamic programming procedures and most RL algo-
rithms is their risk-neutrality, meaning that they do not diﬀerentiate between strategies

1

 
 
 
 
 
 
Achab and Neu

with the same expected return but diﬀerent levels of risk (for instance, diﬀerent variances).
As a vanilla example, getting 0 with probability (w.p.) 1 is safer than getting +1 w.p. 1/2
and −1 w.p. 1/2, though both scenarios are equivalent in expectation. For this reason,
standard DP and RL methods need to be adjusted to take risk into account, for instance
by considering alternative objective functions such as mean minus variance in Mannor and
Tsitsiklis (2011), conditional value-at-risk (“CVaR” in short) in Osogami (2012) and Chow
et al. (2015), or Chernoﬀ functionals in Moldovan and Abbeel (2012).

A more general approach, ﬁrst proposed by Morimura et al. (2010) and Morimura et al.
(2012), is to handle the whole distribution of the long-term return in a dynamic program-
ming framework, not just its expectation or some risk measure. This approach is ﬁttingly
called distributional and is the main focus of our work. Recently, Bellemare et al. (2017) in-
troduced the distributional reinforcement learning (DRL) framework and proposed the C51
algorithm, achieving state-of-the-art performance in playing video games on the Atari 2600
benchmark (Bellemare et al., 2013). Rowland et al. (2018) analyzed C51 with the Cram´er
distance and Bellemare et al. (2019) described another DRL algorithm that approximates
distributions using the same metric. Many other DRL algorithms were proposed such as
QR-DQN (Dabney et al., 2018b) and IQN (Dabney et al., 2018a) both based on quantile re-
gression or ER-DQN in Rowland et al. (2019) based on expectile estimation. Most of these
DRL approaches rely on summarizing distributions by N ≥ 1 atoms Q1(x, a), . . . , QN (x, a)
instead of the single state-action value function Q(x, a) in classic RL or DP. Although there
are empirical evidence of the regularizing eﬀect of learning several atoms in a function ap-
proximation setup (Lyle et al., 2019), ﬁnding an intuitive explanation for these quantities
is still an open problem to the best of our knowledge. Hence, it seems natural to ask the
following question: “Is there any meaningful interpretation of these atoms?”. The answer
provided by this paper is “Yes, a robust MDP interpretation!”.

The robust MDP framework is a seemingly unrelated way of dealing with uncertainties in
sequential decision making, with the main idea being the optimization of a worst-case objec-
tive function subject to an uncertainty set over the true environment parameters (Iyengar,
2005, Nilim and El Ghaoui, 2005). In this work, we show that the usual notion of robustness
in MDPs can be directly derived from the distributional Bellman operator combined with
a carefully chosen projection to a family of distributions involving only two atoms: this
is our main contribution. Additionally, we show that once all optimal policies have been
identiﬁed and isolated from suboptimal ones (by solving classical DP), our methodology
allows a further discrimination among the space of optimal policies, by distinguishing safe
from risky optimal actions.

The rest of the paper is organized as follows. After providing the necessary technical
background in Section 2, we introduce our framework for robust distributional dynamic
programming in Section 3 and give an interpretation of the resulting value functions from
the perspective of risk-measure theory in Section 4. Finally, in Section 5, we propose
dynamic programming methods for tiebreaking in the space of optimal policies to favor safe
or risky policies, and provide some numerical illustration to our results in Section 6. The
paper is concluded with Section 7.

Notations. Throughout the paper, we denote by 1 the all-ones vector (the dimensionality
will always be clear from the context). We let Pb(R) be the set of probability measures on R

2

Robustness and risk management via distributional dynamic programming

with bounded support, and P(E ) the set of probability mass functions on any countable set
E , whose cardinality is denoted by |E |. The support of any discrete distribution q ∈ P(E )
is: Support(q) = {y ∈ E : q(y) > 0}. The cumulative distribution function (CDF) of
a real-valued random variable Z is the mapping F (z) = P(Z ≤ z) (∀z ∈ R), and we
denote its generalized inverse distribution function (a.k.a. quantile function) by F −1 : τ ∈
(0, 1) (cid:55)→ inf{z ∈ R, F (z) ≥ τ }1. For any probability measure ν ∈ Pb(R) and measurable
function f : R → R, the pushforward measure ν ◦ f −1 is deﬁned for any Borel set A ⊆ R
by ν ◦ f −1(A) = ν({z ∈ R : f (z) ∈ A}).
In this work, we will only encounter the
aﬃne case fr0,γ(z) = r0 + γz (with r0 ∈ R, γ ∈ [0, 1)) for which ν ◦ f −1
r0,γ ∈ Pb(R) and
ν ◦ f −1
: z ∈ A}) if γ (cid:54)= 0, or ν ◦ f −1
r0,γ = δr0 is the Dirac measure at r0 if
γ = 0. Lastly, for two probability measures ν, ν(cid:48) in Pb(R), ν (cid:28) ν(cid:48) means that ν is absolutely
continuous with respect to ν(cid:48) (i.e. ν(cid:48)(A) = 0 ⇒ ν(A) = 0) and dν
dν(cid:48) is the Radon-Nikodym
derivative.

r0,γ(A) = ν({ z−r0
γ

2. Background

This section presents basic technical background on Markov decision processes, robust
MDPs, and distributional dynamic programming with 2-Wasserstein projections.

2.1 Markov decision process

In this article, we study one of the most fundamental models for sequential decision-making
problems: discounted Markov decision processes (MDPs) with ﬁnite state and action spaces.
Here we only describe the most essential elements of this framework and refer to the classic
textbook of Puterman (2014) for details. A Markov decision process is described by the
tuple (X , A, P, r, γ) with ﬁnite state space X , ﬁnite action space A, transition kernel P :
X × A → P(X ), reward function r : X × A × X → R and discount factor 0 ≤ γ < 1.
An MDP describes a sequential process where in each round of interaction, the decision-
making agent chooses an action a ∈ A while the environment occupies some state x ∈ X ,
then the next state X1 is sampled from the distribution P (·|x, a) ∈ P(X ) and the agent
gets the reward r(x, a, X1). A stationary Markovian policy π : X → P(A) maps any state
x to a distribution over the actions π(·|x) ∈ P(A). We denote by Π the set of stationary
Markovian policies. The two major classes of problems in an MDP are the following.

The policy evaluation task:
the goal is to assess the quality of a policy π in terms of
expected return, through its state-action value function Qπ deﬁned for all (x, a) ∈ X × A
as follows,

Qπ(x, a) = E

(cid:34) ∞
(cid:88)

t=0

γtr(Xt, At, Xt+1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

X0 = x, A0 = a

,

where Xt+1 ∼ P (·|Xt, At) and At+1 ∼ π(·|Xt+1). The Bellman equation veriﬁed by Qπ is

Qπ(x, a) =

(cid:88)

P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) (cid:0)r(x, a, x(cid:48)) + γQπ(x(cid:48), a(cid:48))(cid:1) .

(x(cid:48),a(cid:48))∈X ×A

1. We will often express the expectation of Z with F −1: Lemma 3 (in Appendix A) recalls the classic

formula E[Z] = (cid:82) 1

τ =0 F −1(τ )dτ .

3

Achab and Neu

Figure 1: Markov decision process with two states X = {x1, x2} and two actions A =
{a1, a2}. In this example, the reward function does not depend on the next state: r(x, a, ·) ≡
r(x, a).

The corresponding value function is V π(x) = (cid:80)

a π(a|x)Qπ(x, a).

The control task: ﬁnd an optimal policy π∗ that simultaneously maximizes the values
across all states,

V π∗

(x) = sup
π∈Π

V π(x) =: V ∗(x)

and Qπ∗

(x, a) = sup
π∈Π

Qπ(x, a) =: Q∗(x, a) ,

where Q∗ satisﬁes the Bellman optimality equation

Q∗(x, a) =

P (x(cid:48)|x, a)

(cid:88)

x(cid:48)

(cid:18)

r(x, a, x(cid:48)) + γ max

a(cid:48)

Q∗(x(cid:48), a(cid:48))

(cid:19)

.

Moreover V ∗(x) = maxa Q∗(x, a), where the maximizing actions include the support of any
optimal policy, in any state:

π∗ is optimal

if and only if

∀x, Support(π∗(·|x)) ⊆ A∗(x) := argmax

Q∗(x, a) .

a

Equivalently, Qπ (resp. Q∗) can be seen as the unique ﬁxed point of the Bellman op-
erator (resp. Bellman optimality operator ), which is a γ-contraction2 in supremum norm3
(Bertsekas and Tsitsiklis, 1996). The dynamic programming (DP) approach consists in re-
cursively applying these contractive operators until convergence to their ﬁxed points (see
Bellman, 1966).
In Section 2.3, we recall how the same idea can be extended to entire
probability distributions, not just expected values.

2.2 Robust MDPs

Sometimes, there may be uncertainty in the transition probabilities, for instance when they
are estimated from noisy observations, or when the state transitions can be inﬂuenced by an

2. A function mapping a metric space to itself is called a γ-contraction if it is Lipschitz continuous with

Lipschitz constant γ < 1.

3. The supremum norm of any function h = (h1, . . . , hk)

: X × A → Rk

is

||h||∞ =

sup(x,a,i)∈X ×A×{1,...,k} |hi(x, a)|.

4

State x1State x2P(x2|x1,a1)=0P(x1|x2,a1)=0P(x1|x1,a1)=1P(x1|x1,a2)=0.5r(x1,a1)=1r(x2,a1)=2P(x2|x1,a2)=0.5P(x1|x2,a2)=0.5r(x1,a2)=0.5r(x2,a2)=2.5P(x2|x2,a1)=1P(x2|x2,a2)=0.5Robustness and risk management via distributional dynamic programming

external adversary. The robust MDP framework models this situation with an uncertainty
set Υ consisting of a family of candidate transition kernels P . The worst-case value function
of a policy π with respect to this set is deﬁned for each state s as

V π
worst(s) = inf
P ∈Υ

V π
P (s) ,

(1)

where V π
state s0, the goal is to ﬁnd a robust optimal policy π∗ satisfying

P denotes the value function in the MDP with kernel P . Then, given an initial

V π∗
worst(s0) ≥ V π

worst(s0)

for all π .

This setting has been extensively studied in the literature under various assumptions for
the uncertainty set. In Iyengar (2005) and Nilim and El Ghaoui (2005), the uncertainty set
Υ is a Cartesian product over state-action pairs:
Υ =×

Υs,a = {P = (P (·|s, a))s,a : P (·|s, a) ∈ Υs,a}.

s,a

In other words, each component P (·|s, a) can be chosen independently among the set Υs,a
in the inﬁmum in Eq. (1). This is the (s, a)-rectangularity assumption, under which there
exists a robust optimal policy that is stationary, Markovian and deterministic, that can be
computed by robust dynamic programming. Similarly, the s-rectangularity assumption is
considered in Wiesemann et al. (2013):

Υ =×

s

Υs = {P = (P (·|s, ·))s : P (·|s, ·) ∈ Υs}.

Under this weaker assumption, there is a stationary Markovian robust optimal policy, but
(unfortunately) it may not be deterministic. More general assumptions have also been
studied. Factor matrix uncertainty sets along with the so-called “r-rectangularity” struc-
ture are investigated in Goyal and Grand-Clement (2018). This alternative hypothesis is
proved to generalize the (s, a)-rectangulariy condition, and to produce as well a determin-
istic robust optimal policy. In Mannor et al. (2016), the authors consider a generalization
of s-rectangularity, called k-rectangularity. In section 4, we show that our distributional
approach reformulates as a robust MDP outside of any of the aforementioned rectangulariy
assumptions. Further in section 5, our setting leads to a deterministic robust optimal policy.
But ﬁrst, we need to recall the deﬁnition of the distributional Bellman operator, which is
the main tool to handle distributions in MDPs.

2.3 The distributional Bellman operator

The distributional Bellman operator (DBO) was introduced in Morimura et al. (2010) and
Morimura et al. (2012) for CDFs, in Bellemare et al. (2017) with random variables; here
we recall its formulation based on pushforward measures from Rowland et al. (2018). On
a high level, the DBO takes as input a distribution function µ ∈ Pb(R)X ×A that models
the collection of return distributions indexed by state-action pairs, and returns another
distribution function T πµ corresponding to the distribution of returns after being pushed
through the transition dynamics. The more formal deﬁnition is the following:

5

Achab and Neu

Deﬁnition 1 (Distributional Bellman operator). Let π ∈ Π. The distributional
Bellman operator T π : Pb(R)X ×A → Pb(R)X ×A is deﬁned for any distribution function
µ = (µ(x,a))x,a by

(T πµ)(x,a) =

(cid:88)

(x(cid:48),a(cid:48))∈X ×A

P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) · µ(x(cid:48),a(cid:48)) ◦ f −1

r(x,a,x(cid:48)),γ

,

for all (x, a) ∈ X × A .

In particular, the DBO is mixture-linear: for all µ1, µ2 and 0 ≤ λ ≤ 1,

T π(λµ1 + (1 − λ)µ2) = λT πµ1 + (1 − λ)T πµ2.

It was proved in Bellemare et al. (2017) that T π is a γ-contraction in the maximal p-
Wasserstein metric4

(cid:102)Wp(µ1, µ2) = max

(x,a)∈X ×A

Wp(µ(x,a)
1

, µ(x,a)
2

)

at any order p ∈ [1, +∞]. Akin to the “non-distributional” case, we know from Banach’s
ﬁxed point theorem that iterating the distributional Bellman operation, namely T π ◦ · · · ◦
T πµ (starting from an arbitrary initial µ), deﬁnes a sequence that converges exponentially
fast to the unique ﬁxed point µπ = T πµπ. This equality is called the distributional Bellman
equation. The ﬁxed point µπ = (µ(x,a)
)x,a is the collection of the probability laws of the
returns:

π

for all (x, a), µ(x,a)

π = Law

(cid:32) ∞
(cid:88)

t=0

γtr(Xt, At, Xt+1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

X0 = x, A0 = a ; π

.

(2)

Nevertheless, it may be hard in practice to compute T πµ, as it requires to deal with general
distributions. In the example below, we focus on a basic family of probability distributions:
the atomic distributions, over which the DBO is stable.

Example 1 (“Atomic fission”). Let µ = (µ(x,a))x,a be an atomic distribution function
with

N
(cid:88)

µ(x,a) =

αi(x, a)δQi(x,a) ,

where N ≥ 1, α1(x, a), . . . , αN (x, a) ≥ 0, α1(x, a) + · · · + αN (x, a) = 1. Then for any (x, a),

i=1

(T πµ)(x,a) =

(cid:88)

P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48))

(x(cid:48),a(cid:48))∈X ×A

N
(cid:88)

i=1

αi(x(cid:48), a(cid:48))δr(x,a,x(cid:48))+γQi(x(cid:48),a(cid:48)) ,

which is still an atomic distribution, but with up to |X ||A| times more particles.

Motivated by Example 1, several distributional RL algorithms are based on atomic
distributions: they apply T π—which multiplies the number of particles by a factor up to

4. We recall that the p-Wasserstein distance (p ≥ 1) between two probability distributions ν1, ν2 on R with
p , and for p = ∞ as W∞(ν1, ν2) =

(τ ) − F −1

(cid:16)(cid:82) 1

p dτ

(cid:17) 1

(τ )(cid:12)
(cid:12)

2

(cid:12)
(cid:12)F −1
1

τ =0

CDFs F1, F2 is deﬁned as Wp(ν1, ν2) =
(τ ) − F −1
supτ ∈(0,1) |F −1

(τ )|.

2

1

6

Robustness and risk management via distributional dynamic programming

Figure 2: Illustration of Example 1 in the MDP in Figure 1, with discount factor γ = 1
2 ,
for the stochastic policy π(a|x) = 1
2 for any x ∈ {x1, x2}, a ∈ {a1, a2}. For k ≥ 0, the
distribution function µk = (T π)kµ0 is obtained by applying k times the DBO to the initial
atomic distributions µ(x,a)

= 0.3δ−1 + 0.7δ1 (for all x, a).

0

7

1.000.750.500.250.000.250.500.751.00atoms0.00.10.20.30.40.50.60.7probabilitiesinitial atomic distribution: (x1,a2)00.60.81.01.21.41.61.82.0atoms0.000.050.100.150.200.25probabilitiessecond iteration: (x1,a2)21.001.251.501.752.002.252.502.75atoms0.000.010.020.030.040.050.060.070.08probabilitiesfourth iteration: (x1,a2)4Achab and Neu

|X ||A|—followed by a projection to bring the number of particles back to a ﬁxed budget
N .
Projected operators. In Dabney et al. (2018b), every distribution (T πµ)(x,a) with CDF
Fx,a is projected to a discrete distribution with uniform probabilities over N ≥ 1 evenly
spread quantiles:

1
N

N
(cid:88)

i=1

δQi(x,a) with Qi(x, a) = F −1
x,a

(cid:19)

(cid:18) 2i − 1
2N

.

This speciﬁc choice comes from minimizing the 1 -Wasserstein distance between (T πµ)(x,a)
and such average of Dirac measures. Notice that in the monoatomic case N = 1, this
approach summarizes an entire distribution by a single scalar: the median F −1
x,a (1/2). Inter-
estingly, this suggests that these projected operators are not an appropriate generalization
of the classical Bellman operators that involve the expectation of the return instead of the
median.

This issue can be addressed by using the 2 -Wasserstein distance for projection:

W2((T πµ)(x,a), δQ(x,a))2 =

(cid:90) 1

τ =0

(F −1

x,a (τ ) − Q(x, a))2dτ .

τ =0 F −1

Indeed, this W2-error is a quadratic function in Q(x, a), whose minimum is attained at the
mean value Q(x, a) = (cid:82) 1
Z∼(T πµ)(x,a)[Z] (by Lemma 3). Thus, combining
the distributional Bellman operators with a 2-Wasserstein projection correctly recovers the
classical DP operators in the special monoatomic case N = 1. More formally, it is easy
to check that averaging after applying the DBO to a collection δQ = (δQ(x,a))x,a of Dirac
measures, is nothing but the usual policy evaluation update:

x,a (τ )dτ = E

E

Z∼(T πδQ)(x,a) [Z] =

(cid:88)

x(cid:48),a(cid:48)

P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) (cid:0)r(x, a, x(cid:48)) + γQ(x(cid:48), a(cid:48))(cid:1) .

This simple observation motivated W2-projections in Achab (2020) (see chapter VII therein),
who derived multiatomic variants of the Temporal-Diﬀerence and Q-learning algorithms. As
shall be seen in the next section, this choice leads to a natural extension of non-distributional
DP with closed-form updates even for more than one atom.

3. Risk-sensitive distributional dynamic programming with

2-Wasserstein projections

We now turn to describing our main contribution: a framework for risk-sensitive dynamic
programming using W2-projected distributional Bellman operators, arising as a special case
of the distributional DP framework described in the previous section using projections
onto the set of diatomic distributions with ﬁxed non-uniform weights. As we will show,
projection to this set corresponds to calculating the well-studied coherent risk measure of
average value-at-risk or “AVaR”5 (see Rockafellar et al., 2000, Rockafellar and Uryasev,
2002 and Acerbi and Tasche, 2002). We start with the formal deﬁnitions below.

5. AVaR and CVaR are two diﬀerent names for the same quantity (Chun et al., 2012).

8

Robustness and risk management via distributional dynamic programming

Deﬁnition 2 (Average value-at-risk). Let 0 < α < 1 and ν ∈ Pb(R) with CDF F .
The left and right AVaRs of ν at respective levels α and 1 − α are deﬁned as:

AVaRleft

α (ν) =

1
α

(cid:90) α

τ =0

F −1(τ )dτ

and AVaRright

1−α (ν) =

1
1 − α

(cid:90) 1

τ =α

F −1(τ )dτ .

Deﬁnition 3 (Diatomic distribution function). Given α ∈ (0, 1) and some bidimen-
sional function Q = (Q1, Q2) : X × A → R2, we call “diatomic distribution function” and
denote Dα,Q = (D(x,a)

α,Q )(x,a)∈X ×A the following collection of diatomic distributions:

D(x,a)

α,Q = αδQ1(x,a) + (1 − α)δQ2(x,a)

,

for all (x, a) .

In other words, we associate a mixture D(x,a)

α,Q of two Dirac masses to each state-action
pair (x, a). As a comparison, this doubles the space complexity of the usual DP framework
that uses a single action-value function Q(x, a) in place of Q(x, a) = (Q1(x, a), Q2(x, a)).
Although we focus on distributions made of two atoms, most of the techniques used in this
paper easily extend to any number of atoms.

In order to respect our diatomic constraint, we apply successively the distributional
Bellman operator and a projection onto the space of diatomic distribution functions. More
precisely, every time the operator T π is applied to some Dα,Q, we use the 2-Wasserstein-
projection to approximate T πDα,Q by another collection of distributions in the same family
Dα,Q(cid:48). The following lemmas give a tractable method for evaluating the resulting operator.

Lemma 1 (From W2-projection to AVaR). Let 0 < α < 1 and ν ∈ Pb(R). Then,
2) ∈ R2 minimizing the 2-Wasserstein approximation error
there exists a unique couple (θ∗
minθ1≤θ2 W2(ν, αδθ1 + (1 − α)δθ2) between ν and any diatomic proxy. In addition, this best
diatomic approximation is given by the left and right AVaRs of ν, at levels α and 1 − α
respectively:

1, θ∗

1 = AVaRleft
θ∗

α (ν)

and

2 = AVaRright
θ∗

1−α (ν) .

Proof For any θ = (θ1, θ2) ∈ R2 with θ1 ≤ θ2, ﬁrst notice that the CDF Fα,θ and the
quantile function F −1
α,θ of the diatomic distribution Dα,θ = αδθ1 + (1 − α)δθ2 are given by:

∀y ∈ R, Fα,θ(y) = αI{θ1 ≤ y} + (1 − α)I{θ2 ≤ y}

and

∀τ ∈ (0, 1), F −1

α,θ (τ ) = I {0 < τ ≤ α} θ1 + I {α < τ ≤ 1} θ2 .

By deﬁnition of the 2-Wasserstein distance, we have:

W2(ν, Dα,θ)2 =

(cid:90) 1

τ =0

(F −1(τ )−F −1

α,θ (τ ))2dτ =

(cid:90) α

τ =0

(F −1(τ )−θ1)2dτ +

(cid:90) 1

τ =α

(F −1(τ )−θ2)2dτ,

where the ﬁrst term (which only depends on θ1) rewrites:

(cid:90) α

τ =0

(F −1(τ ) − θ1)2dτ =

(cid:90) α

F −1(τ )2dτ − 2θ1

(cid:90) α

F −1(τ )dτ + αθ2
1,

τ =0
τ =0 F −1(τ )dτ . Similarly, the second term is minimal if

τ =0

which is minimized for θ1 = (1/α) (cid:82) α
and only if θ2 = (1 − α)−1 (cid:82) 1

τ =α F −1(τ )dτ , which concludes the proof.

9

Achab and Neu

The W2-projection in Lemma 1 appears as a natural and canonical choice: it is simply an
orthogonal projection in the L2 space of quantile functions. In the following, we will apply
it entrywise, i.e. by computing the left and right AVaRs for each of the |X ||A| distributions
ν = (T πDα,Q)(x,a) contained in T πDα,Q.

Key observation: discrete AVaR. Although the AVaR may not be easy to compute
for general distributions, luckily our approach only requires it for discrete distributions.
Indeed,

(T πDα,Q)(x,a) =

(cid:88)

P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) (cid:0)αδr(x,a,x(cid:48))+γQ1(x(cid:48),a(cid:48)) + (1 − α)δr(x,a,x(cid:48))+γQ2(x(cid:48),a(cid:48))

(cid:1)

(x(cid:48),a(cid:48))∈X ×A

(3)
is simply a weighted average of 2|X ||A| Dirac masses. Plus, the AVaR of a discrete distri-
bution comes with a closed-form expression provided below.

Lemma 2 (AVaR of a discrete distribution). Let 0 < α < 1 and ν be a discrete
distribution:

M
(cid:88)

ν =

pjδvj ,

with M ≥ 1, p1, . . . , pM ≥ 0, p1 + · · · + pM = 1 and sorted values v1 ≤ · · · ≤ vM .

(i) Closed-form expression: the left and right AVaRs of ν at respective levels α and 1 − α

j=1

are

AVaRleft

α (ν) =





max

0 , min

pj , α −

1
α

M
(cid:88)

j=1





pj(cid:48)



 · vj







(cid:88)

j(cid:48)≤j−1


and AVaRright

1−α (ν) =

1
1 − α

M
(cid:88)

j=1

max

0 , min

pj ,

(cid:88)

j(cid:48)≤j

pj(cid:48) − α



 · vj

,

with the empty sum convention (cid:80)

j(cid:48)≤0 pj(cid:48) = 0 .

(ii) Dual representation: denoting v = (v1, . . . , vM ) and p = (p1, . . . , pM ),

AVaRleft

α (ν) =

1
α

inf
λ

(cid:104)λ, v(cid:105)

and AVaRright

1−α (ν) =

1
1 − α

(cid:104)p − λ, v(cid:105)

sup
λ

,

where the inﬁmum and supremum both range over weights λ = (λ1, . . . , λM ) such that

(cid:40)

∀i,
(cid:80)M

0 ≤ λi ≤ pi

i=1 λi = α .

(i) Closed-form expression. First denote pj = (cid:80)j

j(cid:48)=1 pj for each j ∈ {1, . . . , M },
Proof
and p0 = 0. Then, the CDF F and quantile function F −1 of the discrete distribution ν are:

∀y ∈ R, F (y) =

M
(cid:88)

j=1

pjI{vj ≤ y}

and

∀τ ∈ (0, 1), F −1(τ ) =

M
(cid:88)

j=1

I (cid:8)pj−1 < τ ≤ pj

(cid:9) vj ,

10

Robustness and risk management via distributional dynamic programming

which are both step functions. Hence, the right AVaR of ν at level 1 − α simply writes:

AVaRright

1−α (ν) =

(cid:90) 1

τ =α

F −1(τ )dτ =

M
(cid:88)

j=1

Length (cid:0)[α, 1] ∩ [pj−1, pj](cid:1) vj.

Observing that the length of the intersection of two intervals [a, b] and [c, d] is equal to

Length ([a, b] ∩ [c, d]) = max(0 , min(b, d) − max(a, c))

concludes the proof.
(ii) Dual representation. If α ∈ [0, p1], then inf λ(cid:104)λ, v(cid:105) = αv1. For j ≥ 2, if α ∈ [pj−1, pj],
then obviously inf λ(cid:104)λ, v(cid:105) = (α − pj−1)vj + (cid:80)j−1
i=1 pivi. All these diﬀerent cases coincide with
the expression provided in (i) for the left AVaR. We conclude the proof by observing that
α (ν) + (1 − α)AVaRright
αAVaRleft
1−α (ν) = (cid:104)p, v(cid:105) and that both the inﬁmum and the supremum
in (ii) are attained at the same λ.

Figure 3 depicts an application of Lemma 2: the left (resp. right) AVaR is obtained by
computing the signed area delimited by the staircase curve of the quantile function on the
segment [0, α] (resp. [α, 1]). The reason why we have the closed-form formula in Lemma
2-(i) is because this area is just the sum of the areas of rectangles.

3.1 Diatomic policy evaluation

Using Lemma 1, our distributional DP method can be formulated with operators that
map functions (Q1, Q2) ∈ R2|X ||A| to (Q(cid:48)
2) in the same space. We introduce below the
diatomic Bellman operator for the policy evaluation task.

1, Q(cid:48)

Deﬁnition 4 (Diatomic Bellman operator). Let α ∈ (0, 1). Given a stationary
α : (R2)X ×A → (R2)X ×A is deﬁned
Markovian policy π, the diatomic Bellman operator T π
for all Q = (Q1, Q2) : X × A → R2 by: T π
1, Q(cid:48)
2) such that for each pair (x, a),
α

Q = (Q(cid:48)

Q(cid:48)

1(x, a) = AVaRleft

α ((T πDα,Q)(x,a))

and Q(cid:48)

2(x, a) = AVaRright

1−α ((T πDα,Q)(x,a)) .

We point out that the update rule (Q(cid:48)

1, Q(cid:48)
2) in Deﬁnition 4 can be expressed explicitly
as a function of (Q1, Q2) by combining Eq. (3) with Lemma 2-(i). More precisely, one shall
set M = 2|X ||A| and v1 ≤ · · · ≤ vM the sorted values (r(x, a, x(cid:48)) + γQ1(x(cid:48), a(cid:48)) , r(x, a, x(cid:48)) +
γQ2(x(cid:48), a(cid:48)))x(cid:48),a(cid:48) with the corresponding probabilities pj = βP (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) (β ∈ {α, 1 −
α}). A detailed description of this practical sorting procedure is provided in Algorithm 1.
Next is a list of some important properties that are satisﬁed by our new operator T π
α .
α is not aﬃne, which is in contrast

In particular, the property (iii) and its proof show that T π
with the classical Bellman operators that are aﬃne.

Proposition 1 (Properties of T π
α
the two inequalities f (cid:48) ≥ f and g(cid:48) ≤ g. The following properties are veriﬁed.

). Let α ∈ (0, 1), π ∈ Π and denote by (f (cid:48), g(cid:48)) ≷ (f, g)

(i) Monotonicity: if Q ≥ (cid:101)Q, then T π
α

Q ≥ T π

α (cid:101)Q.

11

Achab and Neu

Figure 3: Left and right average value-at-risk of a discrete distribution. Graphic illustration
of Lemma 2 for ν = 0.2(δ−5 + 2δ−1 + δ4 + δ8) (with piecewise constant quantile function
F −1) and AVaR level α = 0.7. The total shaded (signed) area on the left is equal to −1;
the rightmost one to 2.

Algorithm 1 Sorted Policy Evaluation (SPE), single iteration.
Parameters: policy π ∈ Π, number of particles M = 2|X ||A|, level α ∈ (0, 1), (α1, α2) =

(α, 1 − α)

Input: double Q-function Q = (Q1, Q2)
1: for each state-action pair (x, a) ∈ X × A do
2:

probability-particle pairs:

(pj, vj)M

j=1 ← (αiP (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)), r(x, a, x(cid:48)) + γQi(x(cid:48), a(cid:48)))(x(cid:48),a(cid:48),i)∈X ×A×{1,2}

3:

4:

5:

particle sorting: vσ(1) ≤ · · · ≤ vσ(M ) with σ an “argsort” permutation
reordering: (pj, vj) ← (pσ(j), vσ(j)) for j = 1 . . . M
left AVaR: Q(cid:48)

(cid:80)M

(cid:17)(cid:17)

(cid:16)

(cid:16)

j(cid:48)≤j−1 pj(cid:48)

1(x, a) ← 1
α
2(x, a) ← 1
1−α

j=1 max
(cid:80)M

0 , min
(cid:16)

pj , α − (cid:80)
pj , (cid:80)

(cid:16)

j=1 max

0 , min

j(cid:48)≤j pj(cid:48) − α

· vj

(cid:17)(cid:17)

· vj

right AVaR: Q(cid:48)

6:
7: end for
Output: next double Q-function T π
α

Q = (Q(cid:48)

1, Q(cid:48)
2)

12

0.00.20.60.81.00.7quantile levels  51480quantiles  F1()AVaRleft()=1(1)AVaRright1()=2Robustness and risk management via distributional dynamic programming

SPE Algorithm 1
(|X ||A|)2 · log (|X ||A|)

(cid:17)

(cid:16)

O

Safe/Risky SVI Algorithm 2 Classic value iteration

O (cid:0)|X |2|A| · log (|X |)(cid:1)

O (cid:0)|X |2|A|(cid:1)

Table 1: Time complexity per iteration. For a deterministic policy, the time complextity of
SPE reduces to O (cid:0)|X |2|A| · log (|X |)(cid:1). The |X ||A| sorting steps induce the multiplicative
logarithmic terms for Algorithms 1 and 2. Obviously, if the reward function does not depend
on the next state (i.e. r(x, a, ·) ≡ r(x, a) for all x, a), these terms can be suppressed by
sorting (Qi(x(cid:48), a(cid:48)))x(cid:48),a(cid:48),i just once, then discounting by γ and shifting by r(x, a) for any x, a.

(ii) Distributivity: for any c ∈ R, T π

α (Q + c1) = T π
α

Q + γc1.

(iii) Concavity/convexity: for 0 ≤ λ ≤ 1, T π

α (λQ + (1 − λ) (cid:101)Q) ≷ λT π
α

Q + (1 − λ)T π

α (cid:101)Q.

(iv) γ-Contraction in sup norm: ||T π
α

Q − T π

α (cid:101)Q||∞ ≤ γ||Q − (cid:101)Q||∞.

(v) Fixed point: there exists a unique ﬁxed point Qπ = T π
α

Qπ, with Qπ = (Qπ

1 , Qπ

2 ).

(vi) Averaging property: αQπ

1 (x, a) + (1 − α)Qπ

2 (x, a) = Qπ(x, a).

(vii) Relative order: Qπ

1 (x, a) ≤ Qπ(x, a) ≤ Qπ

2 (x, a).

Proof (i) Monotonicity. Q = (Q1, Q2) ≥ (cid:101)Q = ( (cid:101)Q1, (cid:101)Q2) means that for all (x, a), Q1(x, a) ≥
(cid:101)Q1(x, a) and Q2(x, a) ≥ (cid:101)Q2(x, a). The monotonicity property follows from combining Eq.
(3) with the dual representation Lemma 2-(ii).
(ii) Distributivity. Fix a pair (x, a) and c ∈ R. The quantile function of (T πDα,Q+c1)(x,a)
is obtained by shifting that of (T πDα,Q)(x,a) by γc. The result follows by linearity of the
Lebesgue integral.
(iii) Concavity/convexity. Denoting (Q(cid:48)

Q, it holds from Lemma 2-(ii) that

1, Q(cid:48)

2) = T π
α

Q(cid:48)

1(x, a) = AVaRleft

α ((T πDα,Q)(x,a))

is a concave piecewise linear function of Q. For the same reason, Q(cid:48)
piecewise linear function of Q.
(iv) Contraction. Given that T π is a γ-contraction in (cid:102)W∞ (Lemma 3 in Bellemare et al.,
2017), it is enough to prove that the W2-projection is a non-expansion in W∞. Let (ν, ν(cid:48)) ∈
Pb(R)2 with respective CDFs F, G. Then,

2(x, a) is a convex

(cid:16)

W∞

αδAVaRleft

α (ν) + (1 − α)δAVaRright

= max
(cid:18) 1
α

(cid:16)(cid:12)
(cid:12)AVaRleft
(cid:12)
(cid:12)
(cid:90) α
(cid:12)
(cid:12)
(cid:12)

τ =0

= max

α (ν) − AVaRleft

(F −1(τ ) − G−1(τ ))dτ

1−α (ν) , αδAVaRleft
(cid:12)
α (ν(cid:48))
(cid:12)
(cid:12) ,
(cid:12)
(cid:12)
(cid:12)
(cid:12)

α (ν(cid:48)) + (1 − α)δAVaRright
(cid:12)
1−α (ν) − AVaRright
(cid:12)AVaRright
(cid:12)
(cid:12)
(cid:90) 1
1
(cid:12)
(cid:12)
1 − α
(cid:12)
≤ sup

,

(F −1(τ ) − G−1(τ ))dτ

τ =α
|F −1(τ ) − G−1(τ )| = W∞(ν, ν(cid:48)),

(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

1−α (ν(cid:48))
1−α (ν(cid:48))

(cid:12)
(cid:17)
(cid:12)
(cid:12)

where we used a triangular inequality in the last step.

τ ∈(0,1)

13

Achab and Neu

(v) Fixed point. Consequence of (iii) combined with Banach’s ﬁxed point theorem and the
completeness of R2|X ||A|.
(vi) Average. By Lemma 3: for any distribution ν ∈ Pb(R) with CDF F ,

αAVaRleft

α (ν) + (1 − α)AVaRright

1−α (ν) =

(cid:90) 1

τ =0

F −1(τ )dτ = EY ∼ν[Y ].

It follows for ν = (T πDα,Qπ )(x,a) that αQπ
(vii) Relative order. As quantile functions are always non-decreasing,
α (ν) ≤ AVaRright

1 +(1−α)Qπ

for any (α, β) ∈ (0, 1]2 ,

AVaRleft

(ν)

β

2 solves the classical Bellman equation.

from which the proof follows for ν = (T πDα,Qπ )(x,a) and β = 1 − α (and from Lemma 3,
the expected value of ν is equal to AVaRleft

1 (ν) = AVaRright

(ν)).

1

In brief, our approach comes with two state-action value functions Qπ

2 (x, a),
instead of just Qπ(x, a). Indeed from Proposition 1-(v), we know that our W2-projected
operator has a unique ﬁxed point Qπ = (Qπ
1 , Qπ
2 ). Plus, properties such as (vi) and (vii)
indicate that this method extends the expected case. Still, so far we have not yet provided
any meaningful interpretation of Qπ
2 . The next section shows that these quantities
are related to some notion of robustness induced by the policy π. By analogy with the
AVaR, we call Qπ
2 (x, a)
the right Bellman average value-at-risk (right BAVaR), for a given risk level α ∈ (0, 1).

1 (x, a) the left Bellman average value-at-risk (left BAVaR) and Qπ

1 (x, a) and Qπ

1 and Qπ

4. Robustness and risk-awareness

The purpose of this section is to interpret the left BAVaR Qπ
Qπ

2 (x, a) from the double perspective of robustness and risk-measure theory.

1 (x, a) and the right BAVaR

• Our ﬁrst interpretation bridges the gap with the robust MDP framework: Qπ

1 (x, a)
and Qπ
2 (x, a) are respectively “worst-case” and “best-case” Q-functions, in a latent
augmented MDP with twice more states. To the best of our knowledge, this constitutes
the ﬁrst formal link between the distributional Bellman operator and robust MDPs.

• Our second point states that −(1 − γ)Qπ

1 (x, a) and (1 − γ)Qπ

2 (x, a) are both coherent

risk measures.

4.1 Robust MDP with double state space

The purpose of this ﬁrst interpretation is to unveil the link existing between our distribu-
tional approach and robust MDPs, in the special case of α-coherent policies.
Deﬁnition 5 (α-coherent policy). For α ∈ (0, 1), we deﬁne the set Πα ⊆ Π of α-
coherent policies π that satisfy the following condition:

∀(x, a, b) ∈ X ×A2

,

(a, b) ∈ Support(π(·|x))2 =⇒ Qπ(x, a) = Qπ(x, b) =: (V π

1 (x), V π

2 (x)) .

All deterministic policies belong to Πα, as well as the safe/risky policies that are de-
rived in section 5. For such α-coherent policies, the BAVaR factorizes to a robust MDP
formulation.

14

Robustness and risk management via distributional dynamic programming

Augmented state space. Let π ∈ Πα be an α-coherent policy. We want to show that
V π
1 and V π
2 can respectively be interpreted as worst-case and best-case value functions in
an augmented MDP, where each state x ∈ X is split into two distinct substates x and x.
We denote the augmented state space by:

X =

(cid:91)

x∈X

{x, x},

(4)

which has double size |X| = 2|X |. We consider a decision-maker that only observes his
current state x in the original state space, not the latent substate (either x or x) in the
augmented one. For that reason, it is natural to extend the reward function r and the policy
π to the augmented state space without distinguishing the substates:
for any transition
(x, a, x(cid:48)) ∈ X × A × X , and corresponding substates s ∈ {x, x}, s(cid:48) ∈ {x(cid:48), x(cid:48)},

r(s, a, s(cid:48)) = r(x, a, x(cid:48))

and

π(·|s) = π(·|x) .

(5)

Reﬁned dynamics.
In the augmented MDP, we constrain the transition probabilities
to be consistent with the transition kernel P of the original MDP: this characterizes the
following dichotomous uncertainty set.

Deﬁnition 6 (Dichotomous uncertainty set). Given the original transition kernel P
and α ∈ (0, 1), the dichotomous uncertainty set Υα is the set of transition kernels P (in
the augmented MDP with double state space X) verifying: for all x, a, x(cid:48),






αP (x(cid:48)|x, a) + (1 − α)P (x(cid:48)|x, a) = αP (x(cid:48)|x, a)
αP (x(cid:48)|x, a) + (1 − α)P (x(cid:48)|x, a) = (1 − α)P (x(cid:48)|x, a)
P (x(cid:48)|x, a) ≥ α

1−α P (x(cid:48)|x, a) .

The inequality constraint in Deﬁnition 6 ensures that, starting from substate x, the
next substates with the same mode x(cid:48) are visited in priority compared to those with
diﬀerent mode x(cid:48). All together, these constraints also imply the symmetric inequality:
P (x(cid:48)|x, a) ≥ 1−α
α P (x(cid:48)|x, a). Interestingly, our new uncertainty set Υα does not fulﬁl any of
the rectangularity assumptions that have been considered in the literature (see subsection
2.2). In Deﬁnition 6, we highlight that Υα (cid:54)= Υ1−α. Still, the two sets are symmetric: Υ1−α
is obtained from Υα by permuting the roles of x and x for each x ∈ X . We are now ready
to state our main result.

Theorem 1 (Robust MDP interpretation). Let α ∈ (0, 1) and π ∈ Πα be an α-
coherent policy. Let V π
P be the value function of π in the augmented MDP (see Eq. (5))
with transition kernel P . Then, V π
2 are respectively worst-case and best-case value
functions:

1 and V π

∀x ∈ X ,

V π
1 (x) = inf
P ∈Υα

V π
P (x)

and

V π
2 (x) = sup
P ∈Υα

V π
P (x),

where the inﬁmum and supremum are attained at the same kernel(s) P (cid:63).

15

Achab and Neu

Proof Fix (x, a) and denote by Fx,a the CDF of (T πDα,Qπ )(x,a). Using the dual represen-
tation of the left AVaR in Lemma 2-(ii),

1
α

(cid:90) α

τ =0

F −1

x,a (τ )dτ =

1
α

inf
λ1,λ2

(cid:88)

x(cid:48),a(cid:48)

λ1(x(cid:48), a(cid:48))(r(x, a, x(cid:48))+γQπ

1 (x(cid:48), a(cid:48)))+λ2(x(cid:48), a(cid:48))(r(x, a, x(cid:48))+γQπ

2 (x(cid:48), a(cid:48))),

where the inﬁmum ranges over functions (λ1, λ2) : X × A → [0, 1]2 such that for all (x(cid:48), a(cid:48)),

• 0 ≤ λ1(x(cid:48), a(cid:48)) ≤ αP (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) ,

• 0 ≤ λ2(x(cid:48), a(cid:48)) ≤ (1 − α)P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) ,
• (cid:80)

x(cid:48),a(cid:48) λ1(x(cid:48), a(cid:48)) + λ2(x(cid:48), a(cid:48)) = α .

Under the additional assumption π ∈ Πα, any Qπ
Support(π(·|x(cid:48))) is equal to V π

1 (x(cid:48), a(cid:48)) (resp. Qπ

2 (x(cid:48), a(cid:48))) with a(cid:48) ∈

2 (x(cid:48))), which simpliﬁes the expression to:

1
α

(cid:90) α

τ =0

F −1

x,a (τ )dτ = inf
λ1,λ2

1 (x(cid:48)) (resp. V π
(cid:34)

(cid:88)

(cid:88)

(cid:35)
λ1(x(cid:48), a(cid:48))

1
α

x(cid:48)

(cid:124)

(r(x, a, x(cid:48)) + γV π

1 (x(cid:48)))

a(cid:48)

(cid:123)(cid:122)
P (x(cid:48)|x,a)

(cid:125)

1
α

(cid:34)

(cid:124)

+

(cid:88)

(cid:35)
λ2(x(cid:48), a(cid:48))

(r(x, a, x(cid:48)) + γV π

2 (x(cid:48))).

(6)

a(cid:48)

(cid:123)(cid:122)
P (x(cid:48)|x,a)

(cid:125)

Symmetrically for the right AVaR,

1
1 − α

(cid:90) 1

τ =α

F −1

x,a (τ )dτ = sup
λ1,λ2

(cid:88)

x(cid:48)

(cid:34)

(cid:124)

1
1 − α

(cid:88)

a(cid:48)

αP (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) − λ1(x(cid:48), a(cid:48))

(cid:123)(cid:122)
P (x(cid:48)|x,a)

(r(x, a, x(cid:48))+γV π

1 (x(cid:48)))

(cid:35)

(cid:125)

+

(cid:34)

(cid:124)

1
1 − α

(cid:88)

a(cid:48)

(cid:35)
(1 − α)P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) − λ2(x(cid:48), a(cid:48))

(r(x, a, x(cid:48)) + γV π

2 (x(cid:48))).

(7)

(cid:123)(cid:122)
P (x(cid:48)|x,a)

(cid:125)

Hence,

V π
1 (x) = inf
P ∈Υα

and

V π
2 (x) = sup
P ∈Υα

(cid:88)

a

(cid:88)

a

π(a|x)

π(a|x)

(cid:88)

x(cid:48)

(cid:88)

x(cid:48)

P (x(cid:48)|x, a)(r(x, a, x(cid:48))+γV π

1 (x(cid:48)))+P (x(cid:48)|x, a)(r(x, a, x(cid:48))+γV π

2 (x(cid:48)))

P (x(cid:48)|x, a)(r(x, a, x(cid:48))+γV π

1 (x(cid:48)))+P (x(cid:48)|x, a)(r(x, a, x(cid:48))+γV π

2 (x(cid:48))).

From equations (6) and (7), the inﬁmum and the supremum are clearly attained at the same
(λ(cid:63)
1 ≤ V π
P the non-distributional
Bellman operator in the augmented MDP with kernel P , we just proved that

2 . Denoting by T π

2) verifying λ(cid:63)

2 because V π

1 ≥ α

1−α λ(cid:63)

1, λ(cid:63)

1 (x) = V π
V π

P (cid:63)(x) = inf
P ∈Υα

(T π

P V π

P (cid:63))(x)

2 (x) = V π
V π

P (cid:63)(x) = sup
P ∈Υα

and

16

(T π

P V π

P (cid:63))(x) ,

Robustness and risk management via distributional dynamic programming

where P (cid:63) ∈ Υα is characterized by (λ(cid:63)

1, λ(cid:63)

2).

To conclude the proof, it remains to prove by induction that for any k ∈ N, the following

properties hold for all x ∈ X :

(a) averaging property:

∀P ∈ Υα , α((T π

P )kV π

P (cid:63))(x) + (1 − α)((T π

P )kV π

P (cid:63))(x) = V π(x) ,

(b) monotonicity:

inf
P ∈Υα

((T π

P )kV π

P (cid:63))(x) ≥ V π

P (cid:63)(x) .

The result then follows from taking the limit k → +∞.
Base case k = 0. The monotonicity property (b) trivially holds, while (a) derives from
Proposition 1-(vi).
Induction step: assume that the induction hypothesis is true for some k ≥ 0.
(a) Averaging property. Let us prove that the averaging property is true for k + 1:

α((T π

P )k+1V π
(cid:88)
π(a|x)

P (cid:63))(x) + (1 − α)((T π

P (cid:63))(x)

P )k+1V π
(cid:0)αP (x(cid:48)|x, a) + (1 − α)P (x(cid:48)|x, a)(cid:1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
αP (x(cid:48)|x,a)

(cid:88)

x(cid:48)

a

=

(cid:16)

·

r(x, a, x(cid:48)) + γ((T π

P )kV π

P (cid:63))(x(cid:48))

(cid:17)

+ (cid:0)αP (x(cid:48)|x, a) + (1 − α)P (x(cid:48)|x, a)(cid:1)
(cid:125)
(cid:123)(cid:122)
(1−α)P (x(cid:48)|x,a)

(cid:124)

(cid:16)

·

r(x, a, x(cid:48)) + γ((T π

P )kV π

(cid:17)
P (cid:63))(x(cid:48))

(cid:88)

=

π(a|x)

(cid:88)

a

x(cid:48)

(cid:32)

P (x(cid:48)|x, a)

r(x, a, x(cid:48))+γ

(cid:16)

(cid:124)

(b) Monotonicity. We have,

α((T π

P )kV π

P (cid:63))(x(cid:48)) + (1 − α)((T π
(cid:123)(cid:122)
V π(x(cid:48))

P )kV π

(cid:17)
P (cid:63))(x(cid:48))

(cid:33)

= V π(x).

(cid:125)

inf
P ∈Υα

((T π

P )k+1V π

P (cid:63))(x) = inf
P ∈Υα

P (x(cid:48)|x, a)

(cid:16)

π(a|x)

(cid:88)

x(cid:48)

(cid:88)

a
(cid:32)

r(x, a, x(cid:48)) + γ((T π

P )kV π

(cid:17)
P (cid:63))(x(cid:48))

(cid:33)

+ P (x(cid:48)|x, a)

r(x, a, x(cid:48)) + γ

((T π
(cid:124)

P )kV π
(cid:123)(cid:122)
V π (x(cid:48))−α((T π
P
1−α

P (cid:63))(x(cid:48))
(cid:125)
)k V π
P (cid:63) )(x(cid:48))

= inf

P ∈Υα

(cid:88)

a

π(a|x)

(cid:88)

x(cid:48)

(cid:18)

+ γ

P (x(cid:48)|x, a) −

P (x(cid:48)|x, a)

α
1 − α

(cid:0)P (x(cid:48)|x, a) + P (x(cid:48)|x, a)(cid:1) r(x, a, x(cid:48))

(cid:19)

((T π
(cid:124)

P (cid:63))(x(cid:48))
P )kV π
(cid:123)(cid:122)
(cid:125)
≥V π
P (cid:63) (x(cid:48))

+

γ
1 − α

P (x(cid:48)|x, a)V π(x(cid:48))

≥ inf

P ∈Υα

(cid:88)

a

π(a|x)

(cid:88)

x(cid:48)

P (x(cid:48)|x, a) (cid:0)r(x, a, x(cid:48)) + γV π

(cid:32)
P (cid:63)(x(cid:48))(cid:1)+P (x(cid:48)|x, a)

r(x, a, x(cid:48))+γ

V π(x(cid:48)) − αV π

P (cid:63)(x(cid:48))

(cid:33)

(cid:124)

1 − α
(cid:123)(cid:122)
V π
P (cid:63) (x(cid:48))

(cid:125)

(T π

P V π

P (cid:63))(x) = V π

P (cid:63)(x).

= inf

P ∈Υα

17

Achab and Neu

1 and V π

Theorem 1 shows that V π

2 merge into a value function in a state-augmented
MDP such that V π
1 contains worst-case values. Similar results can be found in the risk-
sensitive MDP literature. In Chow et al. (2015), risk-sensitive MDPs with CVaR objective
are linked to robust MDPs where the state space is augmented with a continuous state
causing computational issues. The relation between CVaR and robustness has been also
investigated in Osogami (2012) at the price of augmenting the state space with the time
step variable. In contrast in Theorem 1, we only double the number of states, which makes
our approach more tractable.

Characterization of P (cid:63). A careful look at the proof of Theorem 1, combined with
Lemma 2, reveals the exact expression of the kernel P (cid:63) ∈ Υα attaining both the inf and
the sup:

for s ∈ {x, x}, P (cid:63)(·|s, a) = Pσ(cid:63)

x,a(·|s, a) ,

where Pσ is deﬁned in Deﬁnition 9 (in Appendix B) for a generic permutation σ. Here,
σ(cid:63)
x,a : X → {1, ..., |X|} is a permutation that sorts the |X| = 2|X | particles


r(x, a, x(cid:48)) + γ V π


1 (x(cid:48))
(cid:124) (cid:123)(cid:122) (cid:125)
=:V π(x(cid:48))

in non-decreasing order:

, r(x, a, x(cid:48)) + γ V π






2 (x(cid:48))
(cid:124) (cid:123)(cid:122) (cid:125)
=:V π(x(cid:48))

x(cid:48)∈X

r(x, a, σ(cid:63)−1

x,a (1)) + γV π(σ(cid:63)−1

x,a (1)) ≤ · · · ≤ r(x, a, σ(cid:63)−1

x,a (|X|)) + γV π(σ(cid:63)−1

x,a (|X|)).

Example 2 In the MDP in Figure 1 (with γ = 1
π(a2|x2) = 1 and level α = 1
2 ,

2 ), for the deterministic policy π(a2|x1) =






V π
1 (x1) = 1.5
V π
2 (x1) = 2.5
V π
1 (x2) = 3.5
V π
2 (x2) = 4.5

and






P (cid:63)(x1|x1, a2) = P (cid:63)(x1|x1, a2) = 0.5
P (cid:63)(x2|x1, a2) = P (cid:63)(x2|x1, a2) = 0.5
P (cid:63)(x1|x2, a2) = P (cid:63)(x1|x2, a2) = 0.5
P (cid:63)(x2|x2, a2) = P (cid:63)(x2|x2, a2) = 0.5

,

and P (cid:63)(·|·, a1) can be chosen arbitrarily as long as P (cid:63) ∈ Υα.

Bridging the BAVaR-AVaR gap.
In general, the BAVaRs are not equal to the AVaRs
of the distributional return. A consequence of Theorem 1 is that the BAVaRs are more
concentrated than the AVaRs around their common mean, namely the value function.

Corollary 1 (BAVaR vs. AVaR). Consider V π
2 (x) for a given level α ∈ (0, 1),
an α-coherent policy π ∈ Πα and a state x ∈ X . Then for any action a ∈ Support(π(·|x)),

1 (x) and V π

AVaRleft

α (µ(x,a)
π

) ≤ V π

1 (x)

and

2 (x) ≤ AVaRright
V π

1−α (µ(x,a)
π

) .

18

Robustness and risk management via distributional dynamic programming

Proof From F¨ollmer and Schied (2008), the left AVaR at level α of the distribution µ(x,a)
admits the dual expression:

π

AVaRleft

α (µ(x,a)
π

) =

ν(cid:28)µ(x,a)
π

inf
:

dµ

dν
(x,a)
π

EZ∼ν[Z].

≤ 1
α

(8)

On the other side, from Theorem 1, V π
1 (x) also writes as an inﬁmum of expected values.
However, the distributions ν need to satisfy more constraints than in Eq. (8), due to the
geometry of the uncertainty set Υα. Indeed, with evident notations,

where the ﬁxed point distributions in the augmented MDP verify:

V π
1 (x) = inf
P ∈Υα

E

Z∼µ(x,a)
π,P

[Z],

∀P ∈ Υα, αµ(x,a)

π,P + (1 − α)µ(x,a)

π,P = µ(x,a)

π

.

Hence, µ(x,a)

π,P satisﬁes the constraints in Eq. (8):

µ(x,a)
π,P (cid:28) µ(x,a)

π

and

dµ(x,a)
π,P
dµ(x,a)
π

≤

1
α

,

which implies AVaRleft
replaced by sup, and α by 1 − α.

α (µ(x,a)

π

) ≤ V π

1 (x). The other half of the proof is similar, with inf

We recall that the whole approach developed in this paper relies on the successive ap-
plications of the distributional Bellman operator (DBO) T π followed by the W2-projection.
Similarly, one could derive k-step operators obtained by applying k times the DBO (instead
of just once) before projecting. Obviously, by taking k arbitrarily large, one can make the
gap between the resulting “k-step BAVaRs” and the true AVaRs arbitrarily small.

4.2 The BAVaR coherent risk measure

We now expose our second interpretation stating that the negative left BAVaR and the
right BAVaR are coherent risk measures. This claim is good news: indeed, a risk measure
is termed coherent if it satisﬁes a set of properties that are desirable in a wide range of
applications including ﬁnancial ones (Artzner et al., 1999).

Corollary 2 (Coherent risk measure). Let α ∈ (0, 1), π ∈ Πα and x ∈ X .

(i) The quantity −(1 − γ)V π

1 (x), seen as a function of the reward function r ∈ RX ×A×X ,

is a coherent risk measure.

(ii) The quantity (1 − γ)V π

2 (x), seen as a function of the negated reward function −r ∈

RX ×A×X , is a coherent risk measure.

Proof (ii) Right BAVaR. Let us prove that (1 − γ)V π
2 (x) is a coherent risk measure. Here,
V π
2 (x) is seen a function of the opposite reward function −r. For clarity, we denote it by
2 (x; −r) for a given reward function r ∈ RX ×A×X . We need to prove each of the following
V π
four properties (see Artzner et al., 1999).

19

Achab and Neu

(a) Translation invariance:

∀β ∈ R,

(1 − γ)V π

2 (x; −r + β1) = (1 − γ)V π

2 (x; −r) − β.

(b) Sub-additivity: for all reward functions r1, r2,

(1 − γ)V π

2 (x; −r1 − r2) ≤ (1 − γ)V π

2 (x; −r1) + (1 − γ)V π

2 (x; −r2).

(c) Positive homogeneity:

∀β ≥ 0,

(d) Monotonicity:

if − r1 ≤ −r2,

(1 − γ)V π

2 (x; −βr) = β(1 − γ)V π

2 (x; −r).

then (1 − γ)V π

2 (x; −r1) ≥ (1 − γ)V π

2 (x; −r2).

All four properties easily follow from Theorem 1, by writing V π
functions.
(i) Left BAVaR. The proof is similar to (ii), except that we parametrize V π
function, namely V π

1 (x; r).

2 (x) as a supremum of value

1 (x) by the reward

This result bridges the gap between our distributional point of view in section 3 and the
concept of coherent risk measure. The ﬁxed point object Qπ = (Qπ
2 ) now appears as an
appealing objective for risk-aware purpose in MDPs. Equipped with Theorem 1, we follow
in the next section the robust MDP paradigm: maximize the worst-case value function V π
1
to obtain a safe policy.

1 , Qπ

5. Safe or risky control

We now turn to describing a dynamic programming framework for risk-aware control prob-
lems derived from our distributional MDP perspective introduced in the previous sections.
By leveraging the robustness insights developed earlier, it makes sense to either

• look for a safe policy that maximizes V π

1 and minimizes V π
2 ,

• or rather for a risky policy that minimizes V π

1 and maximizes V π
2 .

While there is a degree of symmetry to these tasks, it is easy to see that there are fundamen-
tal diﬀerences between them: while risky control aims to solve a relatively straightforward
maximization problem, the safe control objective has a more intricate “max-min” nature.
In what follows, we study both objectives under a speciﬁc simplifying assumption on the
underlying MDP that allows an eﬀective and (relatively) symmetric treatment of both cases.

20

Robustness and risk management via distributional dynamic programming

5.1 Breaking the optimality ties

We provide the dynamic programming toolbox for solving these two control tasks for a very
special type of MDP, that we call balanced MDP, where the expected returns of all policies
are equal.

Assumption 1 (Balanced MDP). A Markov decision process is said “balanced” if A∗(x) =
A for every state x ∈ X . In other words, all policies are optimal in terms of their expected
return:

∀π ∈ Π , Qπ = Q∗

or equivalently

∀(x, a) ∈ X × A , Q∗(x, a) = V ∗(x) .

Under such assumption, there is a clear trade-oﬀ between Qπ

1 and Qπ
2 .

Indeed from

Proposition 1-(vi), their average has to remain constant:

∀π ∈ Π , αQπ

1 + (1 − α)Qπ

2 = Q∗ .

Hence, maximizing one of these two quantities necessarily means minimizing the other one.
Of course, any MDP can be reduced to a balanced MDP by 1) ﬁrst, identifying the set of
optimal actions A∗(x) in each state x (classic control problem) and 2) then, ﬁltering the
action space to only allow optimal actions.

Example 3 The MDP in Figure 1 combined with the discout factor γ = 0.5 is a balanced
MDP. In section 6, we run experiments based on this MDP.

5.2 Safe and risky operators

In balanced MDPs, we necessarily have

Qπ

2 =

Q∗ − αQπ
1
1 − α

,

for any policy π .

In other words, Qπ
restrict our attention to Qπ

2 is completely characterized by Qπ

1 (and vice-versa): hence, we can

1 to deﬁne our risk-aware operators more concisely.

Deﬁnition 7 (Safe & risky Bellman operators). Consider a balanced MDP and let
α ∈ (0, 1).

(i) The safe Bellman operator T safe

: RX ×A → RX ×A is deﬁned for all Q1 : X × A → R

by: T safe

α Q1 = Q(cid:48)

α
1 such that for any x, a,

Q(cid:48)

1(x, a) = AVaRleft
α

(cid:32)

(cid:88)

x(cid:48)

(cid:32)

(cid:33)(cid:33)

P (x(cid:48)|x, a)

αδr(x,a,x(cid:48))+γ maxa(cid:48) Q1(x(cid:48),a(cid:48))+(1−α)δr(x,a,x(cid:48))+γ mina(cid:48) Q2(x(cid:48),a(cid:48))

,

where Q2(x(cid:48), a(cid:48)) = V ∗(x(cid:48))−αQ1(x(cid:48),a(cid:48))

1−α

.

21

Achab and Neu

(ii) The risky Bellman operator T risky

: RX ×A → RX ×A is deﬁned for all Q1 : X × A → R

by: T risky

α Q1 = Q(cid:48)

α
1 such that for any x, a,

Q(cid:48)

1(x, a) = AVaRleft
α

(cid:32)

(cid:88)

x(cid:48)

(cid:32)

(cid:33)(cid:33)

P (x(cid:48)|x, a)

αδr(x,a,x(cid:48))+γ mina(cid:48) Q1(x(cid:48),a(cid:48))+(1−α)δr(x,a,x(cid:48))+γ maxa(cid:48) Q2(x(cid:48),a(cid:48))

,

where Q2(x(cid:48), a(cid:48)) = V ∗(x(cid:48))−αQ1(x(cid:48),a(cid:48))

1−α

.

As for the policy evaluation operator, these two control operators can be easily imple-
mented through a sorting step: see Algorithm 2. Moreover, they satisfy the properties listed
below.

Algorithm 2 Safe/Risky Sorted Value Iteration (Safe/Risky SVI), single itera-
tion.
Parameters: mode ∈ {safe, risky}, optimal value function V ∗, number of particles M =

2|X |, level α ∈ (0, 1), (α1, α2) = (α, 1 − α)

Input: Q-function Q1
1: if mode = safe then
2:

for each state x ∈ X do

3:

4:

8:

9:

ﬁrst value function: V1(x) ← maxa Q1(x, a)
second value function: V2(x) ← mina

V ∗(x)−αQ1(x,a)
1−α

end for

5:
6: else if mode = risky then
7:

for each state x ∈ X do

ﬁrst value function: V1(x) ← mina Q1(x, a)
second value function: V2(x) ← maxa

V ∗(x)−αQ1(x,a)
1−α

end for

10:
11: end if
12: for each state-action pair (x, a) ∈ X × A do
13:

probability-particle pairs:

(pj, vj)M

j=1 ← (αiP (x(cid:48)|x, a), r(x, a, x(cid:48)) + γVi(x(cid:48)))(x(cid:48),i)∈X ×{1,2}

14:

15:

particle sorting: vσ(1) ≤ · · · ≤ vσ(M ) with σ an “argsort” permutation
reordering: (pj, vj) ← (pσ(j), vσ(j)) for j = 1 . . . M
left AVaR: Q(cid:48)

pj , α − (cid:80)

0 , min

(cid:80)M

(cid:17)(cid:17)

(cid:16)

(cid:16)

j(cid:48)≤j−1 pj(cid:48)

j=1 max

· vj

16:
17: end for
Output: next Q-function T mode

1(x, a) ← 1
α

α

Q1 = Q(cid:48)
1

Proposition 2 (Properties of T safe
(0, 1), mode ∈ {safe, risky}. The following properties hold.

and T risky
α

α

). Assume a balanced MDP, let α ∈

(i) Concavity of the risky operator: if Q1 ≤ Q∗ and (cid:101)Q1 ≤ Q∗, then for any 0 ≤ λ ≤ 1,

T risky
α

(λQ1 + (1 − λ) (cid:101)Q1) ≥ λT risky

α Q1 + (1 − λ)T risky

α

(cid:101)Q1.

22

Robustness and risk management via distributional dynamic programming

(ii) γ-Contraction in sup norm: ||T mode

α Q1 − T mode

α

(cid:101)Q1||∞ ≤ γ||Q1 − (cid:101)Q1||∞.

(iii) Fixed point: there exists a unique ﬁxed point Qmode

1

(iv) Safe optimality: for all (x, a) ∈ X × A ,

= T mode

α Qmode

1

.

Qsafe
1

(x, a) = sup

π

Qπ

1 (x, a)

or equivalently Qsafe

2

(x, a) = inf
π

Qπ

2 (x, a) ,

where Qsafe

2

(x, a) := (V ∗(x) − αQsafe

1

(x, a))/(1 − α).

(v) Risky optimality: for all (x, a) ∈ X × A ,

Qrisky
1

(x, a) = inf
π

Qπ

1 (x, a)

or equivalently Qrisky

2

(x, a) = sup

π

Qπ

2 (x, a) ,

where Qrisky

2

(x, a) := (V ∗(x) − αQrisky

1

(x, a))/(1 − α).

Proof (i) Concavity. The assumption Q1 ≤ Q∗ is equivalent to Q1 ≤ Q2 with Q2(x, a) =
V ∗(x)−αQ1(x,a)
α Q1 and using the dual representation of the left
1−α

. Then, given Q(cid:48)

1 = T risky

AVaR from Lemma 2-(ii),

Q(cid:48)

1(x, a) =

1
α

inf
λ1,λ2

(cid:88)

x(cid:48)

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

α
1 − α

λ2(x(cid:48))) min
a(cid:48)

Q1(x(cid:48), a(cid:48))+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48)),

where the inﬁmum is necessarily attained for λ1 ≥ α
concave piecewise linear function for Q1 ≤ Q∗.
(ii) Contraction. Let us consider the safe case: mode = safe. Given a pair (x, a), by the
dual representation of the left AVaR in Lemma 2-(ii),

1−α λ2. We deduce that Q(cid:48)

1(x, a) is a

(T safe

α Q1)(x, a) =

1
α

inf
λ1,λ2

(cid:88)

x(cid:48)

λ1(x(cid:48))(r(x, a, x(cid:48))+γ max

a(cid:48)

Q1(x(cid:48), a(cid:48)))+λ2(x(cid:48))(r(x, a, x(cid:48))+γ min
a(cid:48)

V ∗(x(cid:48)) − αQ1(x(cid:48), a(cid:48))
1 − α

)

=

1
α

inf
λ1,λ2

(cid:88)

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

x(cid:48)

α
1 − α

λ2(x(cid:48))) max

a(cid:48)

Q1(x(cid:48), a(cid:48))+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48)),

where the inﬁmum ranges over functions (λ1, λ2) : X → [0, 1]2 such that for all x(cid:48),

• 0 ≤ λ1(x(cid:48)) ≤ αP (x(cid:48)|x, a) ,

• 0 ≤ λ2(x(cid:48)) ≤ (1 − α)P (x(cid:48)|x, a) ,
• (cid:80)

x(cid:48) λ1(x(cid:48)) + λ2(x(cid:48)) = α .

Hence, by successive applications of the triangular inequality,

(cid:12)
(cid:12)(T safe
(cid:12)

α Q1)(x, a) − (T safe

α

(cid:101)Q1)(x, a)

(cid:12)
(cid:12)
(cid:12) ≤

γ||Q1 − (cid:101)Q1||∞
α

sup
λ1,λ2

(cid:88)

x(cid:48)

23

(cid:12)
(cid:12)
λ1(x(cid:48)) −
(cid:12)
(cid:12)
(cid:124)

α
1 − α
(cid:123)(cid:122)
≤αP (x(cid:48)|x,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

λ2(x(cid:48))

≤ γ||Q1− (cid:101)Q1||∞.

Achab and Neu

The risky case is analogous.
(iii) Fixed point. Consequence of (i) combined with Banach’s ﬁxed point theorem and the
completeness of R|X ||A|.
(iv) Safe optimality. Fix a policy π. Let us prove by induction that for any k ∈ N∗: for all
(x, a) ∈ X × A,

(a) relative order:

(b) monotonicity:

((T safe
α

)kQπ

1 )(x, a) ≤ V ∗(x) ,

((T safe
α

)kQπ

1 )(x, a) ≥ Qπ

1 (x, a) .

Taking the limit k → +∞ will then conclude the proof.
Base case k = 1.
(b) Monotonicity. As Qπ
attained for λ1 ≥ α
1−α λ2:

2 = (Q∗ −αQπ

1 ≤ Qπ

1 )/(1−α), then the inﬁmum below is necessarily

1 )(x, a) =

(T safe

α Qπ
(cid:88)

inf
λ1,λ2

x(cid:48)

1
α

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

α
1 − α

λ2(x(cid:48))) max

a(cid:48)

Qπ

1 (x(cid:48), a(cid:48))+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48))

≥

1
α

inf
(cid:101)λ1,(cid:101)λ2

(cid:88)

x(cid:48),a(cid:48)

((cid:101)λ1(x(cid:48), a(cid:48))+(cid:101)λ2(x(cid:48), a(cid:48)))r(x, a, x(cid:48))+γ((cid:101)λ1(x(cid:48), a(cid:48))−

α
1 − α

(cid:101)λ2(x(cid:48), a(cid:48)))Qπ

1 (x(cid:48), a(cid:48))+γ

(cid:101)λ2(x(cid:48), a(cid:48))
1 − α

V ∗(x(cid:48))

= AVaRleft

α ((T πDα,Qπ )(x,a)) = Qπ

1 (x, a),

(9)

where the second inﬁmum ranges over functions (cid:101)λ1 ≥ α

1−α (cid:101)λ2 such that for all x(cid:48), a(cid:48),

• 0 ≤ (cid:101)λ1(x(cid:48), a(cid:48)) ≤ αP (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) ,
• 0 ≤ (cid:101)λ2(x(cid:48), a(cid:48)) ≤ (1 − α)P (x(cid:48)|x, a)π(a(cid:48)|x(cid:48)) ,
• (cid:80)

x(cid:48),a(cid:48) (cid:101)λ1(x(cid:48), a(cid:48)) + (cid:101)λ2(x(cid:48), a(cid:48)) = α .

(a) Relative order. From the ﬁrst inﬁmum in Eq.
Qπ

1 ≤ Q∗ ≡ V ∗,

(9) for λ1 ≥ α

1−α λ2, and using that

(T safe

α Qπ

1 )(x, a) ≤

1
α

=

inf
λ1,λ2

(cid:88)

x(cid:48)

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

α
1 − α

λ2(x(cid:48)))V ∗(x(cid:48))+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48))

1
α

inf
λ1,λ2

(cid:88)

(λ1(x(cid:48)) + λ2(x(cid:48)))(r(x, a, x(cid:48)) + γV ∗(x(cid:48)))

x(cid:48)

≤

(cid:88)

x(cid:48)

P (x(cid:48)|x, a)(r(x, a, x(cid:48)) + γV ∗(x(cid:48))) = Q∗(x, a) = V ∗(x) ,

where the last inequality is obtained by choosing the “risk-neutral” weight functions (λ1, λ2) =
(λ◦

2) deﬁned for all x(cid:48) by

1, λ◦

(cid:40)

1(x(cid:48)) = α2P (x(cid:48)|x, a)
λ◦
2(x(cid:48)) = α(1 − α)P (x(cid:48)|x, a) .
λ◦

24

Robustness and risk management via distributional dynamic programming

Induction step: assume that the induction hypothesis is true for some k ≥ 1.
(a) Relative order. Let us prove that the inequality holds for k + 1:

((T safe
α

)k+1Qπ

1 )(x, a) =

1
α

inf
λ1,λ2

(cid:88)

x(cid:48)

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

α
1 − α

λ2(x(cid:48))) max

((T safe
α

)kQπ

1 )(x(cid:48), a(cid:48))
(cid:125)

(cid:123)(cid:122)
≤V ∗(x(cid:48))

a(cid:48)

(cid:124)

+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48))

≤

1
α

inf
λ1,λ2

(cid:88)

(λ1(x(cid:48)) + λ2(x(cid:48)))(r(x, a, x(cid:48)) + γV ∗(x(cid:48)))

x(cid:48)

≤

(cid:88)

x(cid:48)

P (x(cid:48)|x, a)(r(x, a, x(cid:48)) + γV ∗(x(cid:48))) = Q∗(x, a) = V ∗(x) .

(b) Monotonicity. We have,

((T safe
α

)k+1Qπ

1 )(x, a) =

1
α

inf
λ1,λ2

(cid:88)

x(cid:48)

(λ1(x(cid:48))+λ2(x(cid:48)))r(x, a, x(cid:48))+γ(λ1(x(cid:48))−

α
1 − α

λ2(x(cid:48))) max

a(cid:48)

((T safe
α
(cid:124)

1 )(x(cid:48), a(cid:48))
(cid:125)

)kQπ
(cid:123)(cid:122)
≥Qπ
1 (x(cid:48),a(cid:48))
1 )(x, a) ≥ Qπ

1 (x, a) .

≥ (T safe

α Qπ

+γ

λ2(x(cid:48))
1 − α

V ∗(x(cid:48))

(v) Risky optimality. The proof is similar to the safe case (iii).

Basically, Proposition 2 says that the safe and risky Bellman operators enjoy properties
In
allow to identify the safest and riskiest actions

that are similar to the ones veriﬁed by the classical Bellman optimality operators.
particular, their ﬁxed points Qsafe
and policies.

and Qrisky

1

1

5.3 Safest and riskiest policies

From Proposition 2, it is natural to deﬁne the set of the safest/riskiest policies as follows:

Πsafe
α

:= {π ∈ Π : Qπ = (Qsafe

1

, Qsafe
2

)}

and Πrisky

α

:= {π ∈ Π : Qπ = (Qrisky

1

, Qrisky
2

)} .

The following corollary claims that these sets are non-empty and simply characterized by
the ﬁxed points: this is an immediate consequence of Proposition 2-(iii).

Corollary 3 (Safest & riskiest actions). Consider a balanced MDP and α ∈ (0, 1).

(i) Safest policies: π ∈ Πsafe

α

if and only if in each state x ∈ X ,

Support(π(·|x)) ⊆ Asafe

α (x) := argmax

a

Qsafe
1

(x, a) = argmin

a

Qsafe
2

(x, a).

(ii) Riskiest policies: π ∈ Πrisky

α

if and only if in each state x ∈ X ,

Support(π(·|x)) ⊆ Arisky

α

(x) := argmin

a

25

Qrisky
1

(x, a) = argmax

a

Qrisky
2

(x, a).

Achab and Neu

Thereby in a balanced MDP, the safest (resp. riskiest) actions/policies can be identiﬁed
by computing the ﬁxed point of the safe (resp. risky) Bellman operator. This is analogous
to the set of optimal actions A∗(x) = argmaxa Q∗(x, a) in the classic control problem. From
Corollary 3, there always exist safe and risky policies that are deterministic. Plus, notice
that all these safest and riskiest policies are α-coherent (Πsafe
α ⊆ Πα), which means
they come with the robust MDP interpretation of Theorem 1.

α ∪ Πrisky

6. Numerical illustrations

In this section, we test our Algorithms 1 and 2 in a practical example, for the risk level
α = 1
2 . We run our experiments in the two-states two-actions MDP from Figure 1 combined
with the discount factor γ = 1
2 , which constitutes a balanced MDP. Indeed, the Q-function

(cid:40)

Q∗(x1, a1) = Q∗(x1, a2) = 2
Q∗(x2, a1) = Q∗(x2, a2) = 4 ,

solves the Bellman optimality equation






2 maxa Q∗(x1, a)

Q∗(x1, a1) = 1 + 1
2 + 1
Q∗(x1, a2) = 1
2
Q∗(x2, a1) = 2 + 1
2 maxa Q∗(x2, a)
2 + 1
Q∗(x2, a2) = 5

2

(cid:0) 1
2 maxa Q∗(x1, a) + 1

2 maxa Q∗(x2, a)(cid:1)

(cid:0) 1
2 maxa Q∗(x1, a) + 1

2 maxa Q∗(x2, a)(cid:1) .

6.1 Evaluation of a policy

Consider the policy π picking uniformly at random the two actions in any of the two states:

π(a|x) =

1
2

for all (x, a) ∈ {x1, x2} × {a1, a2}.

For the policy evaluation task, we run 20 iterations of the SPE algorithm, starting from
Q1(x, a) and Q2(x, a) initialized (arbitrarily) at zero. Figure 4 displays the plots across
iterations, showing quick convergence to the ﬁxed point (Qπ

1 , Qπ

2 ).

6.2 Finding safe and risky policies

For the safe (resp. risky) control task, we run 20 iterations of the Safe SVI (resp. Risky
SVI) algorithm, starting from Q1(x, a) initialized at zero.
In both cases, we see quick
convergence to the ﬁxed points.
(·, a1) =
Qsafe
(·, a1), which conﬁrms that the corresponding policy is the safest one that always takes
2
the action a1, thus producing a deterministic discounted return. In Figure 6, the risky ﬁxed
point satisﬁes Qrisky
2 (·), where π is the policy from
Example 2 that always takes the riskiest action a2. Hence, our two control algorithms work
as expected: they quickly converge to the desired ﬁxed points, from which the safe or risky
(deterministic) policies can be extracted.

In Figure 5, the safe ﬁxed point satisﬁes Qsafe

1 (·) and Qrisky

(·, a2) = V π

(·, a2) = V π

1

2

1

26

Robustness and risk management via distributional dynamic programming

Figure 4: Evaluation of the policy π by successive iterations of the SPE algorithm 1.

Figure 5: Safe control by successive iterations of the Safe SVI algorithm 2.

Figure 6: Risky control by successive iterations of the Risky SVI algorithm 2.

27

05101520iterations0.00.51.01.52.02.5valuesstate x1Q1(x1,a1)Q2(x1,a1)Q1(x1,a2)Q2(x1,a2)05101520iterations01234valuesstate x2Q1(x2,a1)Q2(x2,a1)Q1(x2,a2)Q2(x2,a2)05101520iterations0.00.51.01.52.02.53.03.54.0valuesstate x1Q1(x1,a1)Q2(x1,a1)=V*(x1)Q1(x1,a1)1Q1(x1,a2)Q2(x1,a2)=V*(x1)Q1(x1,a2)105101520iterations012345678valuesstate x2Q1(x2,a1)Q2(x2,a1)=V*(x2)Q1(x2,a1)1Q1(x2,a2)Q2(x2,a2)=V*(x2)Q1(x2,a2)105101520iterations0.00.51.01.52.02.53.03.54.0valuesstate x1Q1(x1,a1)Q2(x1,a1)=V*(x1)Q1(x1,a1)1Q1(x1,a2)Q2(x1,a2)=V*(x1)Q1(x1,a2)105101520iterations012345678valuesstate x2Q1(x2,a1)Q2(x2,a1)=V*(x2)Q1(x2,a1)1Q1(x2,a2)Q2(x2,a2)=V*(x2)Q1(x2,a2)1Achab and Neu

7. Conclusion

In this paper, we showed that the distributional perspective in MDPs can be leveraged
to deﬁne new robust control tasks in the exact tabular setting. Our approach allows to
distinguish safe from risky policies among the space of optimal policies. In other words, we
ﬁrst require the classic control problem to be solved, so that all suboptimal actions can be
identiﬁed and removed from the action space, before we can apply our method. This strong
“balanced MDP” requirement constitutes the main limitation of our work. Future lines
of research include relaxing this assumption (e.g.
in the safe case, by just requiring the
condition argmaxa Q1(x, a) = argmina Q2(x, a)) or ﬁnding a natural class of MDPs with
such structure. Future work could as well investigate risk-seeking algorithms (based on
the linear programming formulation provided in Appendix B) in a reinforcement learning
setting with an agent only observing empirical transitions.

28

Robustness and risk management via distributional dynamic programming

Appendix A. A technical prerequisite

We recall the classic quantile representation of the expected value, that is used several times
throughout the paper.

Lemma 3 (Expectation by quantiles). Let Z be a real-valued random variable with
CDF F and quantile function F −1. Then,

E[Z] =

(cid:90) 1

τ =0

F −1(τ )dτ.

Proof As any CDF is non-decreasing and right continuous (see, e.g., Billingsley, 2013), we
have for all (τ, z) ∈ (0, 1) × R:

F −1(τ ) ≤ z ⇐⇒ τ ≤ F (z).

Then, denoting by U a uniformly distributed random variable over [0, 1],

P{F −1(U ) ≤ z} = P{U ≤ F (z)} = F (z),

which shows that the random variable F −1(U ) has the same distribution as Z. Hence,

E[Z] = E[F −1(U )] =

(cid:90) 1

τ =0

F −1(τ )dτ.

Appendix B. A linear programming formulation of risky control

Here, we show that the risky control task in a balanced MDP can be written as a linear
program (LP). From Lemma 2, we can get a closed-form expression of the left AVaR ap-
pearing in the ﬁxed point equation of the risky Bellman operator. Given (x, a), we need to
sort the |X| = 2|X | particles

(cid:18)

r(x, a, x(cid:48)) + γ min
a(cid:48)

Qrisky
1

(x(cid:48), a(cid:48)) , r(x, a, x(cid:48)) + γ max

a(cid:48)

Qrisky
2

(x(cid:48), a(cid:48))

(cid:19)

.

x(cid:48)∈X

≤ Qrisky
As Qrisky
2
1
transition kernels.

, we restrict our attention to the following constrained permutations and

Deﬁnition 8 (Constrained permutations). Let us denote by S(X) the set of permu-
tations σ : X → {1, ..., |X|} that verify σ(x) < σ(x) for all x ∈ X .

It can be shown by induction on |X | = 1

2 |X| that the cardinality of the set S(X) is

|S(X)| =

|X|!
2|X |

.

We refer to Achab et al. (2019) (and references therein) for a related analysis of constrained
permutations in a statistical learning context. With a slight abuse of notation, we denote
for all x, a, x(cid:48),

P (x(cid:48)|x, a) := αP (x(cid:48)|x, a)

and

P (x(cid:48)|x, a) := (1 − α)P (x(cid:48)|x, a) .

29

Achab and Neu

Deﬁnition 9 (Permutation kernels). For any constrained permutation σ ∈ S(X), we
deﬁne the transition kernel Pσ ∈ Υα as follows: for all (x, a) ∈ X × A, s(cid:48) ∈ X,

Pσ(s(cid:48)|x, a) =

1
α



max

0 , min


P (s(cid:48)|x, a) , α −

(cid:88)





P (s(cid:48)(cid:48)|x, a)



 ,

and Pσ(s(cid:48)|x, a) =

s(cid:48)(cid:48):σ(s(cid:48)(cid:48))≤σ(s(cid:48))−1



max

0 , min


P (s(cid:48)|x, a) ,

1
1 − α

(cid:88)

P (s(cid:48)(cid:48)|x, a) − α



 .





s(cid:48)(cid:48):σ(s(cid:48)(cid:48))≤σ(s(cid:48))

Primal & dual LPs. For an initial state distribution ν0 = (ν0(x))x > 0 over X , we deﬁne
the primal problem as:

maximize
V1,V

(1 − γ)(cid:104)ν0, V1(cid:105)

subject to:

V1(x) ≤

(cid:88)

s(cid:48)∈X

∀x ∈ X , ∀a ∈ A∗(x), ∀σ ∈ S(X),
Pσ(s(cid:48)|x, a) (cid:0)r(x, a, s(cid:48)) + γV (s(cid:48))(cid:1) ,

V (x) = V1(x) ,

and

V (x) =

V ∗(x) − αV1(x)
1 − α

.

(10)

Lemma 4

(i) The unique solution (V risky

1

, V risky) of the primal problem (10) is given by:

V risky
1

(x) = min

a

Qrisky
1

(x, a) .

(ii) The dual of problem (10) is:

minimize
p≥0

(cid:88)

x,a,σ

p(x, a, σ)·

(cid:32)

(cid:88)

s(cid:48)∈X

Pσ(s(cid:48)|x, a)r(x, a, s(cid:48)) +

(cid:33)

Pσ(x(cid:48)|x, a)V ∗(x(cid:48))

γ
1 − α

(cid:88)

x(cid:48)∈X

(cid:88)

a,σ

p(x, a, σ) = (1−γ)ν0(x)+γ

(iii) Strong duality holds.

∀x ∈ X ,

Pσ(x|x(cid:48), a(cid:48)) −

subject to:
(cid:18)

(cid:88)

x(cid:48),a(cid:48),σ

(cid:19)

Pσ(x|x(cid:48), a(cid:48))

α
1 − α

p(x(cid:48), a(cid:48), σ) .

(11)

Proof
(i) Primal LP. This follows from the properties of the risky Bellman operator (Proposition
2) combined with Lemma 2 in Nilim and El Ghaoui (2005).

30

Robustness and risk management via distributional dynamic programming

(ii) Dual LP. The Lagrangian function of the LP (10) is:

L(V1, V , p, λ1, λ2) = −(1−γ)(cid:104)ν0, V1(cid:105)+

(cid:88)

x,a,σ

(cid:34)

p(x, a, σ)

V1(x) −

(cid:88)

s(cid:48)∈X

Pσ(s(cid:48)|x, a) (cid:0)r(x, a, s(cid:48)) + γV (s(cid:48))(cid:1)

(cid:35)

(cid:88)

+

x

λ1(x) [V (x) − V1(x)] +

(cid:20)
V (x) −

λ2(x)

(cid:88)

x

Pσ(s(cid:48)|x, a)r(x, a, s(cid:48)) −

(cid:21)

V ∗(x) − αV1(x)
1 − α
V ∗
1 − α

λ2,

(cid:29)

(cid:28)

= −

p(x, a, σ)

(cid:88)

s(cid:48)

(cid:88)

x,a,σ
(cid:34)

(cid:88)

+

x

V1(x)

−(1 − γ)ν0(x) +



(cid:88)

a,σ

p(x, a, σ) − λ1(x) +

(cid:35)

λ2(x)

α
1 − α

(cid:88)

+

V (x)

−γ

(cid:88)

Pσ(x|x(cid:48), a(cid:48))p(x(cid:48), a(cid:48), σ) + λ1(x)





x

x(cid:48),a(cid:48),σ



(cid:88)

+

V (x)

−γ

(cid:88)

x

x(cid:48),a(cid:48),σ

Pσ(x|x(cid:48), a(cid:48))p(x(cid:48), a(cid:48), σ) + λ2(x)

 .

(12)



The result follows by setting the gradients of L with respect to V1 and V to zero.
(iii) Strong duality. It can be proved by Slater’s condition, see Boyd et al. (2004).

References

Carlo Acerbi and Dirk Tasche. On the coherence of expected shortfall. Journal of Banking

& Finance, 26(7):1487–1503, 2002.

Mastane Achab. Ranking and risk-aware reinforcement learning. PhD thesis, Institut poly-

technique de Paris, 2020.

Mastane Achab, Anna Korba, and Stephan Cl´emen¸con. Dimensionality reduction and
In Algorithmic Learning Theory,

(bucket) ranking: a mass transportation approach.
pages 64–93. PMLR, 2019.

Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures

of risk. Mathematical ﬁnance, 9(3):203–228, 1999.

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence
Research, 47:253–279, 2013.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on rein-
forcement learning. In International Conference on Machine Learning, pages 449–458.
PMLR, 2017.

31

Achab and Neu

Marc G Bellemare, Nicolas Le Roux, Pablo Samuel Castro, and Subhodeep Moitra. Dis-
tributional reinforcement learning with linear function approximation. In The 22nd In-
ternational Conference on Artiﬁcial Intelligence and Statistics, pages 2203–2211. PMLR,
2019.

Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena

Scientiﬁc Belmont, MA, 1996.

Dimitri P Bertsekas et al. Dynamic programming and optimal control: Vol. 1. Athena

scientiﬁc Belmont, 2000.

Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.

Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cam-

bridge university press, 2004.

Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust
decision-making: a cvar optimization approach. arXiv preprint arXiv:1506.02188, 2015.

So Yeon Chun, Alexander Shapiro, and Stan Uryasev. Conditional value-at-risk and average
value-at-risk: Estimation and asymptotics. Operations Research, 60(4):739–756, 2012.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos. Implicit quantile networks

for distributional reinforcement learning. arXiv preprint arXiv:1806.06923, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distributional reinforce-
ment learning with quantile regression. In Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, volume 32, 2018b.

Hans F¨ollmer and Alexander Schied. Convex and coherent risk measures. Encyclopedia of

Quantitative Finance, pages 355–363, 2008.

Vineet Goyal and Julien Grand-Clement. Robust markov decision process: Beyond rectan-

gularity. arXiv preprint arXiv:1811.00215, 2018.

Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30

(2):257–280, 2005.

Clare Lyle, Marc G Bellemare, and Pablo Samuel Castro. A comparative analysis of ex-
pected and distributional reinforcement learning. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 33, pages 4504–4511, 2019.

Shie Mannor and John Tsitsiklis. Mean-variance optimization in markov decision processes.

arXiv preprint arXiv:1104.5601, 2011.

Shie Mannor, Oﬁr Mebel, and Huan Xu. Robust mdps with k-rectangular uncertainty.

Mathematics of Operations Research, 41(4):1484–1509, 2016.

32

Robustness and risk management via distributional dynamic programming

Teodor Mihai Moldovan and Pieter Abbeel. Risk aversion in markov decision processes via

near optimal chernoﬀ bounds. In NIPS, pages 3140–3148, 2012.

Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki
Tanaka. Nonparametric return distribution approximation for reinforcement learning. In
ICML, 2010.

Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki
Tanaka. Parametric return density estimation for reinforcement learning. arXiv preprint
arXiv:1203.3497, 2012.

Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with

uncertain transition matrices. Operations Research, 53(5):780–798, 2005.

Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. Advances

in Neural Information Processing Systems, 25:233–241, 2012.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons, 2014.

R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss

distributions. Journal of banking & ﬁnance, 26(7):1443–1471, 2002.

R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk.

Journal of risk, 2:21–42, 2000.

Mark Rowland, Marc G Bellemare, Will Dabney, R´emi Munos, and Yee Whye Teh. An anal-
ysis of categorical distributional reinforcement learning. arXiv preprint arXiv:1802.08163,
2018.

Mark Rowland, Robert Dadashi, Saurabh Kumar, R´emi Munos, Marc G Bellemare, and
Will Dabney. Statistics and samples in distributional reinforcement learning. arXiv
preprint arXiv:1902.08102, 2019.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT

press, 2018.

Wolfram Wiesemann, Daniel Kuhn, and Ber¸c Rustem. Robust markov decision processes.

Mathematics of Operations Research, 38(1):153–183, 2013.

33

