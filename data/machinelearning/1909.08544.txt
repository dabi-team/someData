TOP (invited survey, to appear in 2020, Issue 2)

Distance Geometry and Data Science1

Leo Liberti1

1 LIX CNRS, ´Ecole Polytechnique, Institut Polytechnique de Paris, 91128 Palaiseau, France

Email:liberti@lix.polytechnique.fr

September 19, 2019

Dedicated to the memory of Mariano Bellasio (1943-2019).

Abstract

Data are often represented as graphs. Many common tasks in data science are based on distances
between entities. While some data science methodologies natively take graphs as their input, there
are many more that take their input in vectorial form. In this survey we discuss the fundamental
problem of mapping graphs to vectors, and its relation with mathematical programming. We discuss
applications, solution methods, dimensional reduction techniques and some of their limits. We then
present an application of some of these ideas to neural networks, showing that distance geometry
techniques can give competitive performance with respect to more traditional graph-to-vector map-
pings.
Keywords: Euclidean distance, Isometric embedding, Random projection, Mathematical Program-
ming, Machine Learning, Artiﬁcial Neural Networks.

Contents

1 Introduction

2 Mathematical Programming

2.1 Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 Reformulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Distance Geometry

3.1 The distance geometry problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Number of solutions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4

4

4

5

5

6

7

7

8

3.4 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

1This research was partly funded by the European Union’s Horizon 2020 research and innovation programme under the

Marie Sklodowska-Curie grant agreement n. 764759 ETN “MINOA”.

9
1
0
2

p
e
S
8
1

]

G
L
.
s
c
[

1
v
4
4
5
8
0
.
9
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
1 INTRODUCTION

4 Representing data by graphs

4.1 Processes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 Abductive inference

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Common data science tasks

5.1 Clustering on vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 Clustering on graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Robust solution methods for the DGP

6.1 Mathematical programming based methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Fast high-dimensional methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Dimensional reduction techniques

7.1 Principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2 Barvinok’s naive algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.3 Random projections

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Distance instability

8.1 Statement of the result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2 Related results

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3 The proof

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4

In practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 An application to neural networks

9.1 Performance measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.2 A Natural Language Processing task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.3 The ANN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.4 Training sets

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5 Computational comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Conclusion

1

Introduction

2

10

10

11

12

13

15

16

19

22

22

27

30

31

32

35

38

39

40

41

42

42

43

43

44

45

46

47

This survey is about the application of Distance Geometry (DG) techniques to problems in Data Science
(DS). More speciﬁcally, data are often represented as graphs, and many methodologies in data science
require vectors as input. We look at the fundamental problem in DG, namely that of reconstructing
vertex positions from given edge lengths, in view of using its solution methods in order to produce vector
input for further data processing.

The organization of this survey is based on a “storyline”. In summary, we want to exhibit alternative
competitive methods for mapping graphs to vectors in order to analyse graphs using Machine Learning
(ML) methodologies requiring vectorial input. This storyline will take us through fairly diﬀerent subﬁelds

1 INTRODUCTION

3

of mathematics, Operations Research (OR) and computer science. This survey does not provide exhaus-
tive literature reviews in all these ﬁelds. Its purpose (and usefulness) rests in communicating the main
idea sketched above, rather than serving as a reference for a ﬁeld of knowledge. It is nonetheless a survey
because, limited to the scope of its purpose, it aims at being informative and also partly educational,
rather than just giving the minimal notions required to support its goal.

Here is a more detailed account of our storyline. We ﬁrst introduce DG, some of its history, its
fundamental problem and its applications. Then we motivate the use of graph representations for several
types of data. Next, we discuss some of the most common tasks in data science (e.g. classiﬁcation,
clustering) and the related methodologies (unsupervised and supervised learning). We introduce some
robust and eﬃcient algorithms used for embedding general graphs in vector spaces. We present some
dimensional reduction operations, which are techniques for replacing sets X of high-dimensional vectors
by lower-dimensional ones X (cid:48), so that some of the properties of X are preserved at least approximately
in X (cid:48). We discuss the instability of distances on randomly generated vectors and its impact on distance-
based algorithms. Finally, we present an application of much of the foregoing theory: we train an Artiﬁcial
Neural Network (ANN) on many training sets, so as to learn several given clusterings on sentences in
natural language. Some training sets are generated using traditional methods, namely incidence vectors
of short sequences of consecutive words in the corpus dictionary. Other training sets are generated by
representing sentences by graphs and then using a DG method to encode these graphs into vectors. It
turns out that some of the DG-generated training sets have competitive performances with the traditional
methods. While the empirical evidence is too limited to support any general conclusion, it might invite
more research on this topic.

The survey is interspersed with eight theorems with proofs. Aside from Thm. 8.1 about distance
instability, the proof of which is taken almost verbatim from the original source [27], the proofs from
the other theorems are not taken from any source. This does not mean that the theorems and their
proofs are actually original. The theorems are usually quite easy to prove. Their proofs are reasonably
short, and, we hope, easy to follow. There are several reasons for the presence of these theorems in this
survey: (a) we have not found them stated and proved clearly anywhere else, and we wish we had during
our research work (Thm. 3.1-6.2); (b) their proofs showcase some point we deem important about the
underlying theory (Thm. 7.2-8.1); (c) they give some indication of the proof techniques involved in the
overarching ﬁeld (Thm. 7.1-7.2); (d) they justify a precise mathematical statement for which we found no
citation (Thm. 6.3). While there may be some original mathematical results in this survey, e.g. Eq. (35)
and the corresponding Thm. 6.3 (though something similar might be found in Henry Wolkowicz’ work)
as well as the computational comparison in Sect. 7.3.2, we believe that the only truly original part is the
application of DG techniques to constructing training sets of ANNs in Sect. 9. Sect. 4, about representing
data by graphs, may also contain some new ideas to Mathematical Programming (MP) readers, although
everything we wrote can be easily reconstructed from existing literature, though some of which might
perhaps be rather exotic to MP readership.

The rest of this paper is organized as follows. In Sect. 2 we give a brief introduction to the ﬁeld of
MP, considered as a formal language for optimization. In Sect. 3 we introduce the ﬁeld of DG. In Sect. 4
we give details on how to represent four types of data as graphs. In Sect. 5 we introduce methods for
clustering on vectors as well as directly on graphs. In Sect. 6 we present many methods for realizing
graphs in Euclidean spaces, most of which are based on MP. In Sect. 7 we present some dimensional
reduction techniques.
In Sect. 8 we discuss the distance instability phenomenon, which may have a
serious negative inpact on distance-based algorithms. In Sect. 9 we present a case-in-point application of
natural language clustering by means of an ANN, and discuss how the DG techniques can help construct
the input part of the training set.

2 MATHEMATICAL PROGRAMMING

4

2 Mathematical Programming

Many of the methods discussed in this survey are optimization methods. Speciﬁcally, they belong to
MP, which is a ﬁeld of optimization sciences and OR. While most of the readers of this paper should be
familiar with MP, the interpretation we give to this term is more formal than most other treatments, and
we therefore discuss it in this section.

2.1 Syntax

MP is a formal language for describing optimization problems. The valid sentences of this language are the
MP formulations. Each formulation consist of an array p of parameter symbols (which encode the problem
input), an array x of n decision variable symbols (which will contain the solution), an objective function
f (p, x) with an optimization direction (either min or max), a set of explicit constraints gi(p, x) ≤ 0 for all
i ≤ m, and some implicit constraints, which impose that x should belong to some implicitly described set
X. For example, some of the variables should take integer values, or should belong to the non-negative
orthant, or to a positive semideﬁnite (psd) cone. The typical MP formulation is as follows:

optx

f (p, x)

∀i ≤ m gi(p, x) ≤ 0

x ∈ X.






(1)

It is customary to deﬁne MP formulations over explicitly closed feasible sets, in order to prevent issues
with feasible formulations which have inﬁma or suprema but no optima. This prevents the use of strict
inequality symbols in the MP language.

2.2 Taxonomy

MP formulations are classiﬁed according to syntactical properties. We list the most important classes:

• if f, gi are linear in x and X is the whole space, Eq. (1) is a Linear Program (LP);

• if f, gi are linear in x and X = {0, 1}n, Eq. (1) is a Binary Linear Program (BLP);

• if f, gi are linear in x and X is the whole space intersected with an integer lattice, Eq. (1) is a

Mixed-Integer Linear Program (MILP);

• if f is a quadratic form in x, gi are linear in x, and X is the whole space, Eq. (1) is a Quadratic

Program (QP); if f is convex, then it is a convex QP (cQP);

• if f is linear in x and gi are quadratic forms in x, and X is the whole space or a polyhedron, Eq. (1)
is a Quadratically Constrained Program (QCP); if gi are convex, it is a convex QCP (cQCP);

• if f and gi are quadratic forms in x, and X is the whole space or a polyhedron, Eq. (1) is a
Quadratically Constrained Quadratic Program (QCQP); if f, gi are convex, it is a convex QCQP
(cQCQP);

• if f, gi are (possibly) nonlinear functions in x, and X is the whole space or a polyhedron, Eq. (1) is

a Nonlinear Program (NLP); if f, gi are convex, it is a convex NLP (cNLP);

• if x is a symmetric matrix of decision variables, f, gi are linear, and X is the set of all psd matrices,

Eq. (1) is a Semideﬁnite Program (SDP);

• if we impose some integrality constraints on any decision variable on formulations from the classes
QP, QCQP, NLP, SDP, we obtain their respective mixed-integer variants MIQP, MIQCQP, MINLP,
MISDP.

2 MATHEMATICAL PROGRAMMING

5

This taxonomy is by no means complete (see [108, §3.2] and [191]).

2.3 Semantics

As in all formal languages, sentences are given a meaning by replacing variable symbols with other
mathematical entities. In the case of MP, its semantics is assigned by an algorithm, called solver, which
looks for a numerical solution x∗ ∈ Rn having some optimality properties and satisfying the constraints.
For example, BLPs such as Eq. (19) can be solved by the CPLEX solver [88]. This allows users to solve
optimization problems just by “modelling” them (i.e. describing them as a MP formulation) instead of
having to invent a speciﬁc solution algorithm. As a formal descriptive language, MP was shown to be
Turing-complete [110, 121].

2.4 Reformulations

It is always the case that inﬁnitely many formulations have the same semantics: this can be seen in a
number of trivial ways, such as e.g. multiplying some constraint gi ≤ 0 by any positive scalar in Eq. (1).
This will produce an uncountable number of diﬀerent formulations with the same feasible and optimal
set.

Less trivially, this property is precious insofar as solvers perform more or less eﬃciently on diﬀerent
(but semantically equivalent) formulations. More generally, a symbolic transformation on an MP formu-
lation for which one can provide some guarantees on the consequent changes on the feasible or optimal
set is called a reformulation [108, 112, 111].

Three types of reformulation guarantees will appear in this survey:

• the exact reformulation: the optima of the reformulated problem can be mapped easily back to

those of the original problem;

• the relaxation: the optimal objective function value of the reformulated problem provides a bound
(in the optimization direction) on the optimal objective function value of the original problem;

• the approximating reformulation: a sequence of formulations based on a parameter which also
appears in a “guarantee statement” (e.g. an inequality providing a bound on the optimal objective
function value of the original problem); when the parameter tends to inﬁnity, the guarantee proves
that formulations in the sequence get closer and closer to an exact reformulation or to a relaxation.

Reformulations are only useful when they can be solved more eﬃciently than the original problem.
Exact reformulations are important because the optima of the original formulation can be retrieved easily.
Relaxations are important in order to evaluate the quality of solutions of heuristic methods which provide
solutions without any optimality guarantee; moreover, they are crucial in Branch-and-Bound (BB) type
solvers (such as e.g. CPLEX). Approximating reformulations are important to devise approximate solution
methods for MP problems.

There are some trivial exact reformulations which guarantee that Eq. (1) is much more general than
it would appear at ﬁrst sight: for example, inequality constraints can be turned into equality constraints
by the addition of slack or surplus variables; equality constraints can be turned to inequality constraints
by listing the constraint twice, once with ≤ sense and once with ≥ sense; minimization can be turned to
maximization by the equation min f = − max −f [112, §3.2].

3 DISTANCE GEOMETRY

6

2.4.1 Linearization

We note two easy, but very important types of reformulations.

• The linearization consists in identifying a nonlinear term t(x) appearing in f or gi, replacing it with

an added variable yt, and then adjoining the deﬁning constraint yt = t(x) to the formulation.

• The constraint relaxation consists in removing a constraint: since this means that the feasible region

becomes larger, the optima may only improve. Thus, relaxing constraints yields a relaxation.

These two reformulation techniques are often used in sequence: one identiﬁes problematic nonlinear
terms, linearizes them, and then relaxes the deﬁning constraints. Carrying this out recursively for every
term in an NLP [132], and only relaxing the nonlinear deﬁning constraints yields an LP relaxation of an
NLP [169, 172, 23].

3 Distance Geometry

DG refers to a foundation of geometry based on the concept of distances instead of those of points and
lines (Euclid) or point coordinates (Descartes). The axiomatic foundations of DG were ﬁrst laid out in
full generality by Menger [135], and later organized and systematized by Blumenthal [31]. A metric space
is a triplet (X, D, d), where X is an abstract set, D ⊆ R+, and d is a binary relation d : X × X → D obeying
the metric axioms:

1. ∀x, y ∈ X d(x, y) = 0 ↔ x = y (identity);

2. ∀x, y ∈ X d(x, y) = d(y, x) (symmetry);

3. ∀x, y, z ∈ X d(x, y) + d(y, z) ≥ d(x, z) (triangle inequality).

Based on these notions, one can deﬁne sequences and limits (calculus), as well as open and closed sets
(topology). For any triplet x, y, z of distinct elements in X, y is between x and z if d(x, y)+d(y, z) = d(x, z).
This notion of metric betweenness can be used to characterize convexity: a subset Y ⊆ X is metrically
convex if, for any two points x, z ∈ Y, there is at least one point y ∈ Y between x and z. The fundamental
notion of invariance in metric spaces is that of congruence: two metric spaces X, Y are congruent if there
is a mapping µ : X → Y such that for all x, y ∈ X we have d(x, y) = d(µ(x), µ(y)).

The word “isometric” is often used as a synonym of “congruent” in many contexts, e.g. with isometric
embeddings (Sect. 6.2.2). In this survey, we mostly use “isometric” in relation to mappings from graphs to
sets of vectors such that the weights of the edges are the same as the length of the segments between the
vectors corresponding to the adjacent vertices. In other words, “isometric” is mostly used for partially
deﬁned metric spaces — only the distances corresponding to the graph edges are considered.

While a systematization of the axioms of DG were formulated in the twentieth century, DG is pervasive
throughout the history of mathematics, starting with Heron’s theorem (computing the area of a triangle
given the side lengths) [68], going on to Euler’s conjecture on the rigidity of (combinatorial) polyhedra
[85], Cauchy’s creative proof of Euler’s conjecture for strictly convex polyhedra [43], Cayley’s theorem for
inferring point positions from determinants of distance matrices [44], Maxwell’s analysis of the stiﬀness
of frames [131], Henneberg’s investigations on rigidity of structures [84], G¨odel’s ﬁxed point theorem for
showing that a tetrahedron with nonzero volume can be embedded isometrically (with geodetic distances)
on the surface of a sphere [78], Menger’s systematization of DG [136], yielding, in particular, the concept
of the Cayley-Menger determinant (an extension of Heron’s theorem to any dimension, which was used
in many proofs of DG theorems), up to Connelly’s disproof of Euler’s conjecture [50]. A fuller account of
many of these achievements is given in [114]. An extension of G¨odel’s theorem on the sphere embedding
in any ﬁnite dimension appears in [123].

3 DISTANCE GEOMETRY

7

3.1 The distance geometry problem

Before the widespread use of computers, the main applied problem of DG was to congruently embed
ﬁnite metric spaces in some vector space. The ﬁrst mention of the need for isometric embeddings using
only a partial set of distances probably appeared in [195]. This need arose from wireless sensor networks:
by estimating a set of distances for pairs of sensors which are close enough to establish peer-to-peer
communication, is it possible to recover the position for all sensors in the network? Note that (a)
distances can be recovered from peer-to-peer communicating pairs by monitoring the amount of battery
required to exchange data; and (b) the positions for the sensors are in RK, with K = 2 (usually) or
K = 3 (sometimes).

Thus we can formulate the main problem in DG.

Distance Geometry Problem (DGP): given an integer K > 0 and a simple undirected
graph G = (V, E) with an edge weight function d : E → R+, determine whether there exists
a realization x : V → RK such that:

∀{u, v} ∈ E (cid:107)x(u) − x(v)(cid:107) = d(u, v).

(2)

We let n = |V | and m = |E| in the following.

We can re-state the DGP as follows: given a weighted graph G and the dimension K of a vector
space, draw G in RK so that each edge is drawn as a straight segment of length equal to its weight.
We remark that the realization x, deﬁned as a function, is usually represented as an n × K matrix
x = (xuk | u ∈ V ∧ k ≤ K), which may also be seen as an element of RnK.

Notationally, we usually write xu, xv and duv.

If the norm used in Eq. (2) is (cid:96)2, then the above

equation is usually squared, so it becomes a multivariate polynomial of degree two:

∀{u, v} ∈ E (cid:107)xu − xv(cid:107)2

2 = d2

uv.

(3)

While most of the distances in this paper will be Euclidean, we shall also mention the so-called linearizable
norms [52], i.e. (cid:96)1 and (cid:96)∞, because they can be described using linear forms. We also remark that the
input of the DGP can also be represented by a partial n × n distance matrix D where only the entries
duv corresponding to {u, v} ∈ E are speciﬁed.

Many more notions about the DGP can be found in [118, 115].

3.2 Number of solutions

A DGP instance may have no solutions if the given distances do not deﬁne a metric, a ﬁnite number of
solutions if the graph is rigid, or uncountably many solutions if the graph is ﬂexible.

Restricted to the (cid:96)2 norm, there are several diﬀerent notions of rigidity. We only deﬁne the simplest,
which is easiest to explain intuitively:
if we consider the graph as a representation of a joint-and-bar
framework, a graph is ﬂexible if the framework can move (excluding translations and rotations) and rigid
otherwise. The formal deﬁnition of rigidity of a graph G = (V, E) involves: (a) a mapping D from a
realization x ∈ RnK to the partial distance matrix

D(x) = ((cid:107)xu − xv(cid:107) | {u, v} ∈ E);

and (b) the completion K(G) of G, deﬁned as the complete graph on V . We want to say that G is
rigid if, were we to move x ever so slightly (excluding translations and rotations), D(x) would also vary
accordingly. We formalize this idea indirectly: a graph is rigid if the realizations in a neighbourhood χ of
x corresponding to changes in D(x) are equal to those in the neighbourhood ¯χ of a realization ¯x of K(G)

3 DISTANCE GEOMETRY

8

[115, Ch. 7]. We note that realizations ¯x ∈ ¯χ correspond to small variations in D(K(G)): this deﬁnition
makes sense because K(G) is a complete graph, which implies that its distance matrix has no variable
components that can change, and hence ¯χ may only contain congruences.

We obtain the following formal characterization of rigidity [16]:

D−1(D(x)) ∩ χ = D−1(D(¯x)) ∩ ¯χ.

(4)

Uniqueness of solution (modulo congruences) is sometimes a necessary feature in applications. Many
diﬀerent suﬃcient conditions to uniqueness have been found [118, §4.1.1]. By way of example as concerns
the number of DGP solutions in graphs, a complete graph has at most one solution modulo congruences,
as remarked above. It was proved in [116] that protein backbone graphs have a realization set having
power of two cardinality with probability 1. As shown in Fig. 1 (bottom row), a cycle graph on four
vertices has uncountably many solutions.

Figure 1: Instances with one, two, and uncountably many realizations.

On the other hand, the remaining possibility of an inﬁnite but countably many realizations of a DGP
instance cannot happen, as shown in Thm. 3.1. This result is a corollary of a well-known theorem of
Milnor’s. It was noted informally in [118, p. 27] without details; we provide a proof here.

3.1 Theorem
No DGP instance may have an inﬁnite but countable number of solutions.

Proof. Eq. (3) is a system of m quadratic equations associated with the instance graph G. Let X ⊆ RnK
be the variety associated to Eq. (3). Now suppose X is countable: then no connected component of
X may contain uncountably many elements. By the notion of connectedness, this implies that every
connected component is an isolated point in X. If X is countable, it must contain a countable numbers
of connected components. By [140], the number of connected components of X is ﬁnite; in particular,
it is bounded by O(3nK). Hence the number of connected components of X is ﬁnite. Since each is an
(cid:50)
isolated point, i.e. a single realization of G, |X| is ﬁnite.

3.3 Applications

The DGP is an inverse problem with many applications to science and engineering.

3 DISTANCE GEOMETRY

9

3.3.1 Engineering

When K = 1 a typical application is that of clock synchronization [168]. Network protocols for wireless
sensor networks are designed so as to save power in communication. When synchronization and battery
usage are key, the peer-to-peer communications needed to exchange the timestamp can be limited to the
exchange of a single scalar, i.e. the time (or phase) diﬀerence. The problem is then to retrieve the absolute
times of all of the clocks, given some of the phase diﬀerences. This is equivalent to a DGP on the time
line, i.e. in a single dimension. We already sketched above the problem of Sensor Network Localization
(SNL) in K ∈ {2, 3} dimensions. In K = 3 we also have the problem of controlling ﬂeets of Underwater
Autonomous Vehicles (UAV), which requires the localization of each UAV in real-time [17, 171].

3.3.2 Science

An altogether diﬀerent application in K = 3 is the determination of protein structure from Nuclear
Magnetic Resonance (NMR) experiments [193]: proteins are composed of a linear backbone and some
side-chains. The backbone determines a total order on the backbone atoms, by which follow some
properties of the protein backbone graph. Namely, the distances from vertex i to vertices i − 1 and i − 2
in the order are known almost exactly because of chemical information, and the distance between vertex
i and vertex i − 3 is known approximately because of NMR output. Moreover, some other distances (with
longer index diﬀerence) may also be known because of NMR — typically, when the protein folds and two
atoms from diﬀerent folds happen to be close to each other. If we suppose all of these distances are known
exactly, we obtain a subclass of DGP which is called Discretizable Molecular DGP (DMDGP).
The structure of the graph of a DMDGP instance is such that vertex i is adjacent to its three immediate
predecessors in the order: this yields a graph which consists of a sequence of embedded cliques on 4
vertices, the edges of which are called discretization edges, with possibly some extra edges called pruning
edges.

If we had to realize this graph with K = 2, we could use trilateration [67]: given three points in the
plane, compute the position of a fourth point at known distance from the three given points. Trilateration
gives rise to a system of equations which has either no solution (if the distance values are not a metric)
or a unique solution, since three distances in two dimensions are enough to disambiguate translations,
rotations and reﬂections. Due to the speciﬁc nature of the DMDGP graph structure, it would suﬃce to
know the positions of the ﬁrst three vertices in the order to be able to recursively compute the positions of
all other vertices. With K = 3, however, there remains one degree of freedom which yields an uncertainty:
the reﬂection.

We can still devise a combinatorial algorithm which, instead of ﬁnding a unique solution in n − K
trilateration steps, is endowed with back-tracking over reﬂections. Thus, the DMDGP can be solved
completely (meaning that all incongruent solutions can be found) in worst-case exponential time by
using the Branch-and-Prune (BP) algorithm [117]. The DMDGP has other very interesting symmetry
properties [122], which allow for an a priori computation of its number of solutions [116], as well as for
generating all of the incongruent solutions from any one of them [145]; moreover, it turns out that BP is
Fixed-Parameter Tractable (FPT) on the DMDGP [119].

3.3.3 Machine Learning

So far, we have only listed applications where K is ﬁxed. The focus of this survey, however, is a case
where K may vary:
if we need to map graphs to vectors in view of preprocessing the input of a ML
methodology, we may choose a dimension K appropriate to the methodology and application at hand.
See Sect. 9 for an example.

4 REPRESENTING DATA BY GRAPHS

10

3.4 Complexity

3.4.1 Membership in NP

The DGP is clearly a decision problem, and one may ask whether it is in NP. As stated above, with
real number input in the edge weight function, it is clear that it is not, since the Turing computation
model cannot be applied. We therefore consider its rational equivalent, where d : E → Q+, and ask the
same question. It turns out that, for K > 1, we do not know whether the DGP is in NP: the issue is
that the solutions of sets of quadratic polynomials over Q may well be algebraic irrational. We therefore
have the problem of establishing that a realization matrix x with algebraic component veriﬁes Eq. (3)
in polynomial time. While some compact representations of algebraic numbers exist [110, §2.3], it is not
known how to employ them in the polynomial time veriﬁcation of Eq. (3). Negative results for the most
basic representations of algebraic numbers were derived in [22].

On the other hand, it is known that the DGP is in NP for K = 1: as this case reduces to realizing
graphs on a single real line, the fact that all of the given distances are in Q means that the distance
between any two points on the line is rational: therefore, if one point is rational, then all the others can
be obtained as sums and diﬀerences of this one point and a set of rational values, which implies that
there is always a rational realization. Naturally, verifying whether a rational realization veriﬁes Eq. (3)
can be carried out in polynomial time.

3.4.2 NP-hardness

It was proved in [162] that the DGP is NP-hard, even for K = 1 (reduction from Partition to the DGP
on simple cycle graphs, see a detailed proof in [115, §2.4.2]), and hence actually NP-complete for K = 1.
In the same paper [162], with more complicated gadgets it was also shown that the DGP is NP-hard for
each ﬁxed K and with edge weights restricted to taking values in {1, 2} (reduction from 3sat).

A sketch of an adaptation of the reduction to cycle graphs is given in [196] for DMDGP graphs,
showing that they are an NP-hard subclass of the DGP. A full proof following a similar idea can be
found in [104].

4 Representing data by graphs

It may be obvious to most readers that data can be naturally represented by graphs. This is immediately
evident whenever data represent similarities or dissimilarities between entities in a vertex set V . In this
section we make this intuition more explicit for a number of other relevant cases.

4.1 Processes

The description of a process, be it chemical, electric/electronic, mechanical, computational, logical or
otherwise, is practically always based on a directed graph, or digraph, G = (N, A). The set of nodes N
represents the various stages of the process, while the arcs in A represent transitions between stages.

Formalizations of this concept may possibly be ﬁrst ascribed to the organization of knowledge proposed
by Aristotle into genera and diﬀerences, commonly represented with a tree (a class of digraphs). While no
graphical representation of this tree ever came to us from Aristotelian times, the commentator Porphyry
of Tyre (3rd century AD) did refer to a representation which was actually drawn as a tree (at least
since the 10th century [178]). Many interesting images can be found in last-tree.scottbot.net/
illustrations/, see e.g. Fig. 2.

4 REPRESENTING DATA BY GRAPHS

11

Figure 2: A tree diagram from F. Bacon’s Advancement of Learning, Oxford 1640.

A general treatment of process diagrams in mechanical engineering is given in [76]. Bipartite graphs
with two node classes representing operations and materials have been used in process network synthesis
in chemical engineering [74]. Circuit diagrams are a necessary design tool for any electrical and electronic
circuit [167]. Software ﬂowcharts (i.e. graphical description of computer programs) have been used in the
design of software so pervasively that one of the most important results in computer science, namely the
B¨ohm-Jacopini’s theorem on the expressiveness of universal computer languages, is based on a formaliza-
tion of the concept of ﬂowchart [32]. The American National Standards Institute (ANSI) set standards
for ﬂowcharts and their symbols in the 1960s. The International Organization for Standardization (ISO)
adopted the ANSI symbols in 1970 [187]. The cyclomatic number |E| − |V | + 1 of a graph, namely the
size of a cycle basis of the cycle space, was adopted as a measure of process graph complexity very early
(see [149, 60, 38, 13] and [100, §2.3.4.1]).

An evalution of ﬂowcharts to process design is the Uniﬁed Modelling Language (UML) [147], which
was mainly conceived to aid the design of software-based systems, but was soon extended to much more
general processes. With respect to ﬂowcharts, UML also models interactions between software systems
and hardware systems, as well as with system users and stakeholders. When it is applied to software,
UML is a semi-formal language, in the sense that it can automatically produce a set of header ﬁles with
the description of classes and other objects, ready for code development in a variety of programming
languages [109].

4.2 Text

One of the foremost issues in linguistics is the formalization of the rules of grammar in natural languages.
On the one hand, text is scanned linearly, word by word. On the other hand, the sense of a sentence
becomes apparent only when sentences are organized as trees [46]. This is immediately evident in the
computer parsing of formal languages, with a “lexer” which carries out the linear scanning, and a “parser”

4 REPRESENTING DATA BY GRAPHS

12

which organizes the lexical tokens in a parsing tree [107]. The situation is much more complicated for
natural languages, where no rule of grammar is ever absolute, and any proposal for overarching principles
has so many exceptions that it is hard to argue in their favor [143].

The study of natural languages is usually split into syntax (how the sentence is organized), semantics
(the sense conveyed by the sentence) and pragmatics (how the context when the sentence is uttered
inﬂuences the meaning, and the impact that the uttered sentence has on the context itself) [144]. The
current situation is that we have been able to formalize rules for natural language syntax (namely turning
a linear text string into a parsing tree) fairly well, using probabilistic parsers [127] as well as supervised
ML [49]. We are still far from being able to successfully formalize semantics. Semiotics suggested many
ways to assign semantics to sentences [66], but none of these is immediately and easily implementable as
a computer program.

Two particularly promising suggestions are the organization of knowledge into an evolving encyclo-
pedia, and the representation of the sense of words in a “space” with “semantic axes” (e.g. “good/bad”,
“white/black”, “left/right”. . . ). The ﬁrst suggestion yielded organized corpora such as WordNet [139],
which is a tree representation of words, synonyms and their semantical relations, not unrelated to a
Porphyrian tree (Sect. 4.1). There is still a long way to go before the second is successfully implemented,
but we see in the Google Word Vectors [138] the start of a promising path (although even easy semantical
interpretations, such as analogies, are apparently not so well reﬂected in these word vectors, despite the
publicity [98]).

For pragmatics, the situation is even more dire; some suggestions for representing knowledge and

cognition w.r.t. the state of the world are given in [141]. See [185] for more information.

Insofar as graphs are concerned, syntax is organized into tree graphs, and semantics is often organized

in corpora that are also trees, or directed acyclic graphs (DAGs), e.g. WordNet and similar.

4.2.1 Graph-of-words

In Sect. 9 we consider a graph representation of sentences known as the graph-of-words [159]. Given
a sentence s represented as a sequence of words s = (s1, . . . , sm), an n-gram is a subsequence of n
consecutive words of s. Each sentence obviously has at most (m − n + 1) n-grams. In a graph-of-words
G = (V, E) of order n, V is the set of words in s; two words have an edge only if they appear in the same
n-gram; the weight of the edge is equal to the number of n-grams in which the two words appear. This
graph may also be enriched with semantic relations between the words, obtained e.g. from WordNet.

4.3 Databases

The most common form of data collection is a database; among the existing database representations,
one of the most popular is the tabular form used in spreadsheets and relational databases.

A table is a rectangular array A with n rows (the records) and m columns (the features), which is
(possibly only partially) ﬁlled with values. Speciﬁcally, each feature column must have values of the same
type (when present). If Arf is ﬁlled with a value, we denote this def(r, f ), for each record index r and
feature index f . We can represent this array via a bipartite graph B = (R, F, E) where R is the set of
records, F is the set of features, and there is an edge {r, f } ∈ E if the (r, f )-th component Arf of A is
ﬁlled. A label function (cid:96) assigns the value Arf to the edge {r, f }. While this is an edge-labelled graph, the
labels (i.e. the contents of A) may not always be interpretable as edge weights — so this representation
is not yet what we are looking for.

We now assume that there is a symmetric function df : A·,f × A·,f → R+ deﬁned over elements of the
column A·,f : since all elements in a column have the same type, such functions can always be deﬁned in
practice. We note that df is undeﬁned whenever one of the two arguments is not ﬁlled with a value. We

4 REPRESENTING DATA BY GRAPHS

can then deﬁne a composite function d : R × R → R+ as follows:

∀r (cid:54)= s ∈ R d(r, s) =






(cid:80)

df (Arf , Asf )

f ∈F
def(r,f )∧def(s,f )
undeﬁned if ∃f ∈ F (¬def(r, f ) ∨ ¬def(s, f )).

Next, we deﬁne a graph G = (R, E(cid:48)) over the records R, where

E(cid:48) = { {r, s} | r (cid:54)= s ∈ R ∧ d(r, s) is deﬁned},

13

(5)

weighted by the function d : E(cid:48) → R+ deﬁned in Eq. (5). We call G the database distance graph.
Analysing this graph yields insights about record distributions, similarity and diﬀerences.

4.4 Abductive inference

According to [65], there are three main modes of rational thought, corresponding to three diﬀerent
permutations of the concepts “hypothesis” (call this H), “prediction” (call this P), “observation” (call this
O). Each of the three permutations singles out a pair of concepts and a remaining concept. Speciﬁcally:

1. deduction: H ∧ P → O;

2. (scientiﬁc) induction: O ∧ P → H;

3. abduction: H ∧ O → P.

Take for example the most famous syllogism about Socrates being mortal:

• H: “all humans are mortal”;

• P: “Socrates is human”;

• O: “Socrates is mortal”.

The syllogism is an example of deduction: we are given H and P, and deduce O. Note also that deduction
is related to modus ponens: if we call A the class of all humans and B the class of all mortals, and let
s be the constant denoting Socrates, the syllogism can be restated as (∀x A(x) ⊆ B(x) ∧ A(s)) → B(s).
Deduction infers truths (propositional logic) or provable sentences (ﬁrst-order and higher-order logic),
and is mostly used by logicians and mathematicians.

Scientiﬁc induction2 exploits observations and veriﬁes predictions in order to derive a general hypoth-
esis: if a large quantity of predictions is veriﬁed, a general hypothesis can be formulated. In other words,
given O and P we infer H. Scientiﬁc induction can never provide proofs in suﬃciently expressive logical
universes, no matter the amount of observations and veriﬁed predictions. Any false prediction, however,
disproves the hypothesis [154]. Scientiﬁc induction is about causality; it is mostly used by physicists and
other natural scientists.

Abduction [63] infers educates guesses about a likely state of a known universe from observed facts:

given H and O, we infer P. According to [133],

Deductions lead from rules and cases to facts — the conclusions. Inductions lead toward truth, with
less assurance, from cases and facts, toward rules as generalizations, valid for bound cases, not for
accidents. Abductions, the apagoge of Aristotle, lead from rules and facts to the hypothesis that the
fact is a case under the rule.

2Not to be confused with mathematical induction.

4 REPRESENTING DATA BY GRAPHS

14

According to [65] it can be traced back to Peirce [151], who cited Aristotle as a source. The author of [156]
argues that the precise Aristotelian source cited by Peirce fails to make a valid reference to abduction;
however, he also concedes that there are some forms of abduction foreshadowed by Aristotle in the texts
where he deﬁnes deﬁnitions.

Let us see an example of abduction. Sherlock Holmes is called on a crime scene where Socrates lies
dead on his bed. After much evidence is collected and a full-scale investigation is launched, Holmes
ponders some possible hypotheses:
for example, all rocks are dead. The prediction that is logically
consistent with this hypothesis and the observation that Socrates is dead would be that Socrates is a
rock. After some unsuccessful tests using Socrates’ remains as a rock, Holmes eliminates this possibility.
After a few more untenable suggestions by Dr. Watson, Holmes considers the hypothesis that all humans
are mortal. The logically consistent prediction is that Socrates is a man, which, in a dazzling display of
investigating abilities, Holmes ﬁnds it to be exactly the case. Thus Holmes brilliantly solves the mystery,
while Lestrade was just about ready to give up in despair. Abduction is about plausibility; it is the most
common type of human inference.

Abduction is also the basis of learning: after witnessing a set of facts, and postulating hypotheses
which link them together, we are able to make predictions about the future. Abductions also can, and
in fact often turn out to, be wrong, e.g.:

• H: all beans in the bag are white;

• O: there is a white bean next to the bag;

• P: the bean was in the bag.

The white bean next to the bag, however, might have been placed there before the bag was even in sight.
With this last example, we note that abductions are inferences often used in statistics. For an observation
O, a set H of hypotheses and a set of possible predictions P, we must evaluate

∀H ∈ H, P ∈ P pHP = P(O | O,H abduce P),

and then choose the pair (H,P) having largest probability pHP (see a simpliﬁed example in Fig. 3).

bag of white beans→bean was in bag

0.3

white bean ﬁeld closeby→bean came from ﬁeld

white bean beside bag

0.25

0.1

farmer market yesterday→bean came from market

0

.

1

5

0

.

2

kid was playing with beans→kid lost a bean

UFOs fueled with beans→bean clearly a UFO sign

Figure 3: Evaluating probabilities in abduction. From left to right, observation O abduces the inference
H→P.

When more than one observation is collected, one can also compare distributions to make more
plausible predictions, see Fig. 4. Abduction appears close to the kind of analysis often required by data
scientists.

5 COMMON DATA SCIENCE TASKS

15

0.01

bag of white beans→bean was in bag

0 . 3

white bean beside bag

0.25

0.01

white bean ﬁeld closeby→bean came from ﬁeld

0

.

1

0

0.49

.

1

farmer market yesterday→bean came from market

5

red bean beside bag

0.29

kid was playing with beans→kid lost a bean

0

.

2

0.2

UFOs fueled with beans→bean clearly a UFO sign

Figure 4: Probability distributions over abduction inferences assigned to observations.

4.4.1 The abduction graph

We now propose a protocol for modelling good predictions from data, by means of an abduction graph.
We consider:

• a set O of observations O;

• a set I ⊆ H × P of abductive premises, namely pairs (H, P).

First, we note that diﬀerent elements of I might be logically incompatible (e.g. there may be contradictory
sets of hypotheses or predictions). We must therefore extract a large set of logically compatible subsets
of I. Consider the relation ∼ on I with h ∼ k meaning that h, k ∈ I are logically compatible. This
deﬁnes a graph (I, ∼). We then ﬁnd the largest (or at least large enough) clique ¯I in (I, ∼).

Next, we deﬁne probability distributions pO on ¯I for each O ∈ O. We let E = {{O, O(cid:48)} | δ(pO, pO(cid:48)
) ≤
δ0}, where δ evaluates dissimilarities between probability distributions, e.g. δ could be the Kullback-
Leibler (KL) divergence [101], and δ0 a given threshold. Thus E deﬁnes a relation on O if pO, pO(cid:48)
are
suﬃciently similar. We can ﬁnally deﬁne the graph F = (O, E), with edges weighted by δ.

If we think of Sherlock Holmes again, the abduction graph encodes sets of clues compatible with the

most likely consistent explanations.

5 Common data science tasks

DS refers to practically every task or problem deﬁned over large amounts of data. Even problems in P,
and sometimes even those for which there exist linear time algorithms, may take too long when confronted
with huge-scale instances. We are not going to concern ourselves here with evaluation problems (such as
computing means, variances, higher-order moments or other statistical measures), which are the realm
of statistics, but rather with decision problems. In particular, it appears that a very common family
of decision problems solved on large masses of data are those that help people make sense of the data
themselves: in other words, classiﬁcation and clustering.

There is no real functional distinction between the two, as both aim at partitioning the data into a
relatively small number of subsets. However, “classiﬁcation” usually refers to the problem of assigning
class labels to data elements, while “clustering” indicates a classiﬁcation based on the concept of similarity
or distance, meaning that similar data elements should be in the same class. This diﬀerence is usually
more evident in the algorithmic description: classiﬁcation methods tend to exploit information inherent

5 COMMON DATA SCIENCE TASKS

16

to elements, while clustering methods consider information relative to pairs of elements. In the rest of this
paper, we shall adopt a functional view, and simply refer to “clustering” to indicate both classiﬁcation
and clustering.

Given a set P of n entities and some pairwise similarity function δ : P × P → R+, clustering aims at
ﬁnding a set of k subsets C1, . . . , Ck ⊆ P such that each cluster contains as many similar entities, and as
few dissimilar entities, as possible. Cluster analysis — as a ﬁeld — grew out of statistics in the course
of the second half of the 20th century, encouraged by the advances in computing power. But some early
forms of cluster analysis may also be attributed to earlier scientists (e.g. Aristotle, Buﬀon, Cuvier, Linn´e
[83]).

We note that “clustering on graphs” may refer to two separate tasks.

A. Cluster the vertices of a given graph.

B. Cluster the graphs in a given set.

Both may arise depending on the application at hand. The proposed DG techniques for realizing graphs
into vector spaces apply to both of these tasks (see Sect. 9.4.2).

As mentioned above, this paper focuses on transforming graphs into vectors so as to be able to use
vector-based methods for classiﬁcation and clustering. We shall ﬁrst survey some of these methods.
We shall then mention some methods for classifying/clustering graphs directly (i.e. without needing to
transform them into vectors ﬁrst).

5.1 Clustering on vectors

Methods for classiﬁcation and clustering on vectors are usually seen as part of ML. They are partitioned
into unsupervised and supervised learning methods. The former are usually based on some similarity or
dissimilarity measure deﬁned over pairs of elements. The latter require a training set, which they exploit
in order to ﬁnd a set of optimal parameter values for a parametrized “model” of the data.

5.1.1 The k-means algorithm

The k-means algorithm is a well-known heuristic for solving the following problem [12].

Minimum Sum-of-Squares Clustering (MSSC). Given an integer k > 0 and a set P ⊂ Rm
of n vectors, ﬁnd a set C = {C1, . . . , Ck} of subsets of P such that the function

is minimum, where

f (C) =

(cid:88)

(cid:88)

j≤k

x∈Cj

(cid:107)x − centroid(Cj)(cid:107)2
2

centroid(Cj) =

1
|Cj|

(cid:88)

x.

x∈Cj

(6)

(7)

It is interesting to note that the MSSC problem can also be seen as a discrete analogue of the problem
of partitioning a body into smaller bodies having minimum sum of moments of inertia [170].

The k-means algorithm improves a given initial clustering C by means of the two following operations:

1. compute centroids cj = centroid(Cj) for each j ≤ k;

5 COMMON DATA SCIENCE TASKS

17

2. for any pair of clusters Ch, Cj ∈ C and any point x ∈ Ch, if x is closer to cj than to ch, move x

from Ch to Cj.

These two operations are repeated until the clustering C no longer changes. Since the only decision
operation (i.e. operation 2) is eﬀective only if it decreases f (C), it follows that k-means is a local descent
In particular, this very simple analysis oﬀers no guarantee on the approximation of the
algorithm.
objective function. For more information on the k-means algorithm, see [30].

k-means is an unsupervised learning technique [92], insofar as it does not rest on a data model with
parameters to be estimated prior to actually ﬁnding clusters. Moreover, the number “k” of clusters must
be known a priori.

5.1.2 Artiﬁcial Neural Networks

An ANN is a parametrized model for representing an unknown function. Like all such models, it needs
data in order to estimate suitable values for the parameters: this puts ANNs in the category of supervised
ML. An ANN consists of two MP formulations deﬁned over a graph and a training set.

An ANN is formally deﬁned as a triplet N = (G, T, φ), where:

• G = (V, A) is a directed graph, with a node weight function b : V → R (threshold at a node), and
an edge weight function w : A → R (weight on an arc); moreover, a subset I ⊂ V of input nodes
with |I| = n and a subset O ⊂ V of output nodes with |O| = k are given in G;

• T = (X, Y ) is the training set, where X ⊂ Rn (input set), Y ⊂ Rk (output set), and |X| = |Y |;

• φ : R → R is the activation function (many common activation functions map injectively into [0, 1]).

The two MP formulations assigned to an ANN describe the training problem and the evaluation problem.
In the training problem, appropriate values for b, w are found using T .
In the evaluation problem, a
given input vector in Rn (usually not part of the input training set X) is mapped to an output vector in
Rk. The training problem decides values for the ANN parameters when seen as a model for an unknown
function mapping the training input X to the training output Y . After the model is trained, it can be
evaluated on new (unseen) input.

In the following, we use standard notation on graphs. For a node i ∈ V we let N −(i) = {j ∈ V | (j, i) ∈
A} be the inward star and N +(i) = {j ∈ V | (i, j) ∈ A} be the outward star of i. For undirected graphs
G = (V, E), we let N (i) = {j ∈ V | {i, j} ∈ E} be the star of i. Moreover, for a tensor si1,...,ir , where
ij ∈ Ij for each j ≤ r, we denote a slice of s, deﬁned by subsets Jj ⊆ Ij for some j ≤ r, by s[J1] · · · [Jr].

We discuss the evaluation phase ﬁrst. Given values for w, b and an input vector x ∈ Rn, we decide a

node weight function u over V as follows:

uI = x

∀j ∈ V (cid:114) I uj = φ(cid:0) (cid:88)

wijui + bj

(cid:1).

i∈N −(j)

(8)

(9)

We remark that Eq. (9) is not an optimization but a decision problem. Nonetheless, it is a MP formulation
(formally with zero objective function). After solving Eq. (9), one retrieves in particular u[O], which
correspond to an output vector in u[0] = y ∈ Rk. When G is acyclic, this decision problem reduces to a
simple computation, which “propagates” the values of u from the input nodes and forward through the
network until they reach the output nodes. If G is not acyclic, diﬀerent solution methods must be used
[14, 71, 80].

5 COMMON DATA SCIENCE TASKS

18

The training problem is given in Eq. (10). We let N be the index set for the training pairs (x, y) in
T (we recall that |X| = |Y |), and introduce a 2-dimensional tensor v of decision variables indexed by N
and V .

dist(v[N ][O], Y )

min
w,b,v
∀t ∈ N
∀t ∈ N, j ∈ V (cid:114) I

vt[I] = X
vtj = φj

(cid:0) (cid:80)

i∈N −(j)

wijvti + bj

(cid:1),



(10)




where dist(A, B) is a dissimilarity function taking dimensionally consistent tensor arguments A, B, which
becomes closer to zero as A and B get closer. The solution of the training problem yields optimal values
w∗, b∗ for the arc weights and node biases.

The training problem is in general a nonconvex optimization problem (because of the products between
w and v, and of the φ functions occurring in equations), which may have multiple global optima: ﬁnding
them with state-of-the-art methods might require exponential time. For speciﬁc types of graphs and
choices of objective function dist(·, ·), the training problem may turn out to be convex. For example, if
G is a DAG, V = I ˙∪O, the induced subgraphs G[I] and G[O] are empty (i.e. they have no arcs), the
activation functions are all sigmoids φ(z) = (1 + exp(−z))−1, and dist(·, ·) is the negative logarithm of
the likelihood functions

(cid:89)

φ(w(cid:62)xt + bi)yt(1 − φ(w(cid:62)xt + bi))1−yt

t∈N

summed over all output nodes i ∈ O, then it can be shown that the training problem is convex [95, 166].

In contemporary treatments of ANNs, the underlying graph G is almost always assumed to be a
DAG. In modern Application Programming Interfaces (API), the acyclicity of G is enforced by recursively
replacing vtj with the corresponding expression in φ(·).

Most algorithms usually solve Eq. (10) only locally and approximately. Usually, they employ a tech-
nique called Stochastic Gradient Descent (SGD) [35]. This is a form of gradient descent where, at each
iteration, the gradient of a multivariate function is estimated by partial gradients with respect to a
randomly chosen subset of variables [142, p. 100].

The functional deﬁnition of an optimum for the training problem Eq. 10 is poorly understood, as
ﬁnding precise local (or global) optima is considered “overﬁtting”.
In other words, global or almost
global optima of Eq. (10) lead to evaluations which are possibly perfect for pairs in the training set,
but unsatisfactory for yet unseen input. Currently, ﬁnding “good” optima of ANN training problems
is mostly based on experience, although a considerable eﬀort is under way in order to reach a sound
deﬁnition of optimum [58, 197, 81, 47].

The main reason why ANNs are so popular today is that they have proven hugely successful at
image recognition [80], and also extremely good at accomplishing other tasks, including natural language
processing [49]. Many eﬃcient applications of ANNs to complex tasks involve interconnected networks
of ANNs of many diﬀerent types [26].

ANNs originated from an attempt to simulate neuronal activity in the brain: should the attempt
prove successful, it would realize the old human dream of endowing a machine with human intelligence
[24]. While ANNs today display higher precision than humans in some image recognition tasks, they
may also be easily fooled by a few appropriately positioned pixels of diﬀerent colors, which places the
realization of “human machine intelligence” still rather far in the future — or even unreachable, e.g. if
Penrose’s hypothesis of quantum activity in the brain inﬂuencing intelligence at a macroscopic level holds
[152]. For more information about ANNs, see [164, 80].

5 COMMON DATA SCIENCE TASKS

19

5.2 Clustering on graphs

While we argue in this paper that DG techniques allow the use of vector clustering methods to graph
clustering, there also exist methods for clustering on graphs directly. We discuss two of them, both
applicable to the task of clustering vertices of a given graph (Task A on p. 16).

5.2.1 Spectral clustering

Consider a connected graph G = (V, E) with an edge weight function w : E → R+. Let A be the
adjacency matrix of G, with Aij = wij for all {i, j} ∈ E, and Aij = 0 otherwise. Let ∆ be the diagonal
weighted degree matrix of G, with ∆ii = (cid:80)
j(cid:54)=i Aij and ∆ij = 0 for all i (cid:54)= j. The Laplacian of G is
deﬁned as L = ∆ − A.

Spectral clustering aims at ﬁnding a minimum balanced cut U ⊂ V in G by looking at the spectrum
of the Laplacian of G. For now, we give the word “balanced” only an informal meaning: it indicates the
fact that we would like clusters to have approximately the same cardinality (we shall be more precise
below). Removing the cutset δ(U ) (i.e. the set of edges between U and V (cid:114) U ) from G yields a two-way
partitioning of V . If |δ(U )| is minimum over all possible cuts U , then the two sets U, V (cid:114) U should both
intuitively induce subgraphs G[U ] and G[V (cid:114) U ] having more edges than those in δ(U ). In other words,
the criterion we are interested in maximizes the intra-cluster edges of the subgraphs of G induced by the
cluster while minimizing the inter-cluster edges of the corresponding cutsets.

We remark that each of the two partitions can be recursively partitioned again. A recursive clustering
by two-way partitioning is a general methodology which is part of a family of hierarchical clustering
methods [163]. So the scope of this section is not limited to generating two clusters only.

For simplicity, we only discuss the case with unit edge weights, although the generalization to general
weights is not problematic. Thus, ∆ii is the degree of vertex i ∈ V . We model a balanced partition
{B, C} corresponding to a minimum cut by means of decision variables xi = 1 if i ∈ B and xi = −1 if
(cid:80)
i ∈ C, for each i ≤ n, with n = |V |. Then f (x) = 1
{i,j}∈E(xi − xj)2 counts the number of intercluster
4
edges between B and C. We have:

4f (x) =

(cid:88)

{i,j}∈E

(x2

i + x2

j ) − 2

(cid:88)

xixj =

(cid:88)

(cid:88)

2 −

xiaijxj =

{i,j}∈E
xidixi − x(cid:62)Ax = x(cid:62)(∆ − A)x = x(cid:62)Lx,

{i,j}∈E

i,j≤n

= 2|E| − x(cid:62)Ax =

(cid:88)

i≤n

whence f (x) = 1

4 x(cid:62)Lx. We can therefore obtain cuts with minimum |δ(B)| by minimizing f (x).

We can now give a more precise meaning to the requirement that partitions are balanced: we require

that x must satisfy the constraint

(cid:88)

i≤n

xi = 0.

(11)

Obviously, Eq. (11) only ensures equal cardinality partitions on graphs having an even number of vertices.
However, we relax the integrality constraints x ∈ {−1, 1}n to x ∈ [−1, 1]n, so (cid:80)
i≤n xi = 0 is applicable
to any graph. With this relaxation, the values of x might be fractional. We shall deal with this issue by
rounding them to {−1, 1} after obtaining the solution. We also note that the constraint

holds for x ∈ {−1, 1}n, and so it provides a strengthening of the continuous relaxation to x ∈ [−1, 1]n.
We therefore obtain a relaxed formulation of the minimum balanced two-way partitioning problem as

x(cid:62)x = (cid:107)x(cid:107)2

2 = n

(12)

5 COMMON DATA SCIENCE TASKS

follows:

min
x∈[−1,1]n
s.t.

1

4 x(cid:62)Lx

1(cid:62)x = 0
(cid:107)x(cid:107)2
2 = n.






20

(13)

We remark that, by construction, L is a diagonally dominant (dd) symmetric matrix with non-negative
diagonal, namely it satisﬁes

∀i ≤ n Lii ≥

|Lij|

(14)

(cid:88)

j(cid:54)=i

(in fact, L satisﬁes Eq. (14) at equality). Since all dd matrices are also psd [186], f (x) is a convex
function. This means that Eq. (13) is a cQP, which can be solved at global optimality in polynomial time
[175].

By [69], there is another polynomial time method for solving Eq. (14), which is generally more eﬃ-
cient than solving a cQP in polynomial time using a Nonlinear Programming (NLP) solver. This method
concerns the second-smallest eigenvalue of L (called algebraic connectivity) and its corresponding eigen-
vector. Let λ1 ≤ λ2 ≤ · · · ≤ λn be the ordered eigenvalues of L and u1, . . . , un be the corresponding
eigenvectors, normalized so that (cid:107)ui(cid:107)2
2 = n for all i ≤ n. It is known that u1 = 1, λ1 = 0 and, if G is
connected, λ2 > 0 [137, 33]. By the deﬁnition of eigenvalue and eigenvector, we have

∀i ≤ n Lui = λiui ⇒ ui

(cid:62)Lui = λiui

(cid:62)ui = λi(cid:107)ui(cid:107)2

2 = λin.

(15)

Because of the orthogonality of the eigenvectors, if i ≥ 2 we have uiu1 = 0, which implies u21 = 0
(i.e. u2 satisﬁes Eq. (11)). We recall that eigenvectors are normalized so that (cid:107)ui(cid:107)2
2 = n for all i ≤ n (in
particular, u2 satisﬁes Eq. (12)). By Eq. (15), since λ1 = 0, λ2 yields the smallest nontrivial objective
function value n

4 λ2 with solution ¯x = u2, which is therefore a solution of Eq. (13).

5.1 Theorem
The eigenvector u2 corresponding to the second smallest eigenvalue λ2 of the graph Laplacian L is an
optimal solution to Eq. (13).

Proof. Since the eigenvectors u1, . . . , un are an orthogonal basis of Rn, we can express an optimal solution
as ¯x = (cid:80)

i ciui. Thus,

x(cid:62)Lx =

(cid:88)

i,j

cicju(cid:62)

i Quj =

(cid:88)

i,j

cicjλju(cid:62)

i uj = n

c2
i λi.

(cid:88)

i>1

(16)

The last equality in Eq. (16) follows because Lui = λiui for all i ≤ n, u(cid:62)
i uj = 0 for each i (cid:54)= j, and
λ1 = 0. Since u1 = 1 and by eigenvector orthogonality, letting 1(cid:62) ¯x = 0 yields c1 = 0. Lastly, requiring
(cid:107)¯x(cid:107)2 = n, again by eigenvector orthogonality, yields
(cid:13)
(cid:13)

cicj(cid:104)ui, uj(cid:105)

2 = (cid:10) (cid:88)
(cid:13)
2
(cid:13)

(cid:11) =

ciui,

cjuj

ciui

(cid:88)

(cid:88)

(cid:88)

i>1

=

j>1

(cid:88)

i>1
i (cid:107)ui(cid:107)2
c2

2 = n

i>1

i,j>1

c2
i = n.

(cid:88)

i>1

(17)

After replacing c2

i by yi in Eq. (16)-(17), we can reformulate Eq. (13) as
yi = 1 ∧ y ≥ 0(cid:9),

n min (cid:8) (cid:88)

λiyi |

(cid:88)

which is equivalent to ﬁnding the convex combination of λ2, . . . , λn with smallest value. Since λ2 ≤ λi
for all i > 2, the smallest value is achieved at y2 = 1 and yi = 0 for all i > 2. Hence ¯x = u2 as claimed.
(cid:50)

i>1

i>1

Normally, the components of ¯x obtained this way are not in {−1, 1}. We round ¯xi to its closest value
in {−1, 1}, breaking ties in such a way as to keep the bisection balanced. We then obtain a practically
eﬃcient approximation of the minimum balanced cut.

5 COMMON DATA SCIENCE TASKS

21

5.2.2 Modularity clustering

Modularity, ﬁrst introduced in [146], is a measure for evaluating the quality of a clustering of vertices in a
graph G = (V, E) with a weight function w : E → R+ on the edges. We let n = |V | and m = |E|. Given
a vertex clustering C = (C1, . . . , Ck), where each Ci ⊆ V , Ci ∩ Cj = ∅ for each i (cid:54)= j, and (cid:83)
i Ci = V , the
modularity of C is the proportion of edges in E that fall within a cluster minus the expected proportion
of the same quantity if edges were distributed at random while keeping the vertex degrees constant. This
deﬁnition is not so easy to understand, so we shall assume for simplicity that wuv = 1 for all {u, v} ∈ E
and wuv = 0 otherwise. We give a more formal deﬁnition of modularity, and comment on its construction.

The “fraction of the edges that fall within a cluster” is

1
m

(cid:88)

(cid:88)

i≤k

u,v∈Ck
{u,v}∈E

1 =

1
2m

(cid:88)

wuv

i≤k
(u,v)∈(Ck )2

where wuv = wvu turns out to be the (u, v)-th component of the n × n symmetric incidence matrix of
the edge set E in V × V — thus we divide by 2m rather than m in the right hand side (RHS) of the
above equation. The “same quantity if edges were distributed at random while keeping the vertex degrees
constant” is the probability that a pair of vertices u, v belongs to the edge set of a random graph on V .
If we were computing this probability over random graphs sampled uniformly over all graphs on V with
m edges, this probability would be 1/m; but since we only want to consider graphs with the same degree
sequence as G, the probability is |N (u)| |N (v)|
[106]. Here is an informal explanation: given vertices u, v,
there are ku = |N (u)| “half-edges” out of u, and kv = |N (v)| out of v, which could come together to form
an edge between u and v (over a total of 2m “half-edges”). Thus we obtain a modularity

2m

µ(C) =

1
2m

(cid:88)

(u,v)∈C2
C∈C

(wuv − kukv/(2m))

for the clustering C.

We now introduce binary variables xuv which have value 1 if u, v ∈ V are in the same cluster, and 0

otherwise. This allows us to rewrite the modularity as:

µ(x) =

1
2m

=

1
m

u<v∈V

(cid:88)

(wuv − kukv/(2m))xuv

u(cid:54)=v∈V
(cid:88)

(wuv − kukv/(2m))xuv.

(18)

Following [10], we can reformulate the modularity maximization problem to a clique partitioning problem
with the following formulation:

max
∀1 ≤ i < j < k ≤ n
∀1 ≤ i < j < k ≤ n
∀1 ≤ i < j < k ≤ n
∀1 ≤ i < j ≤ n

µ(x)

xij + xjk − xik ≤ 1
xij − xjk + xik ≤ 1
−xij + xjk + xik ≤ 1

xij ∈ {0, 1},






(19)

which is a BLP formulation. The weighted variant of this problem yields a formulation like Eq. (19)
where w are the edge weights and ku = (cid:80)
{u,v}∈E wuv for all v (cid:54)= u in V . Another variant for graphs
including loops and multiple edges is described in [40]. We note that, by Eq. (19), maximizing modularity
does not require the number of clusters to be known a priori.

There is a large literature about modularity maximization and its solution methods: for a survey, see
[72, §VI]. Solution methods based on MP are of particular interest to the topics of this survey. A BLP
formulation similar to Eq. (19) was proposed in [39]. Another BLP formulation with diﬀerent sets of

6 ROBUST SOLUTION METHODS FOR THE DGP

22

decision variables (requiring the number of clusters to be known a priori) was proposed in [194]. Some
column generation approaches, which scale better in size w.r.t. previous formulations, were proposed in
[10]. Some MP based heuristics are discussed in [41, 42, 11].

6 Robust solution methods for the DGP

In this section we discuss some solution methods for the DGP which can be extended to deal with cases
where distances are uncertain, noisy or wrong. Most of the methods we present are based on MP. We also
discuss a diﬀerent (non-MP based) class of methods in Sect. 6.2, in view of their computational eﬃciency.

6.1 Mathematical programming based methods

DGP solution methods based on MP are robust to noisy or wrong data because MP allows for: (a)
modiﬁcation of the objective and constraints; (b) adjoining of side constraints. Moreover, although
we do not review these here, there are MP-based methodologies for ensuring robustness of solutions
[25], probabilistic constraints [153], and scenario-based stochasticity [29], which can be applied to the
formulations in this section.

6.1.1 Unconstrained quartic formulation

A system of equations such as Eq. (3) is itself a MP formulation with objective function identically equal
to zero, and X = RnK. It therefore belongs to the QCP class. In practice, solvers for this class perform
rather poorly when given Eq. (3) as input [103]. Much better performances can be obtained by solving
the following unconstrained formulation:

(cid:88)

min

(cid:0)(cid:107)xu − xv(cid:107)2

2 − d2
uv

(cid:1)2

.

{u,v}∈E

(20)

We note that Eq. (20) consists in the minimization of a polynomial of degree four. It belongs to the class
of nonconvex NLP formulations. In general, this is an NP-hard class [110], which is not surprising, as it
formulates the DGP which is itself an NP-hard problem. Very good empirical results can be obtained
on the DGP by solving Eq. (20) with a local NLP solver (such as e.g. IPOPT [48] or SNOPT [77]) from
a good starting point [103]. This is the reason why Eq. (20) is very important: it can be used to “reﬁne”
solutions obtained with other methods, as it suﬃces to let such solutions be starting points given to a
local solver acting on Eq. (20).

Even if the distances duv are noisy or wrong, optimizing Eq. (20) can yield good approximate real-
uv] for each edge

izations. If the uncertainty on the distance values is modelled using an interval [dL
{u, v}, the following function [120] can be optimized instead of Eq. (20):

uv, dU

(cid:88)

min

{u,v}∈E

(cid:0) max(0, (dL

uv)2 − (cid:107)xu − xv(cid:107)2

2) + max(0, (cid:107)xu − xv(cid:107)2

2 − (dU

uv)2)(cid:1).

(21)

The DGP variant where distances are intervals instead of values is known as the interval DGP (iDGP)
[79, 105].

Note that Eq. (21) involves binary max functions with two arguments. Relatively few MP user
interfaces/solvers would accept this function. To overcome this issue, we linearize (see Sect. 2.4.1) the

6 ROBUST SOLUTION METHODS FOR THE DGP

two max terms by two sets of added decision variables y, z, and obtain

min (cid:80)

(yuv + zuv)

{u,v}∈E

∀{u, v} ∈ E
∀{u, v} ∈ E

(cid:107)xu − xv(cid:107)2
(cid:107)xu − xv(cid:107)2

2 ≥ (dL
2 ≤ (dU

uv)2 − yuv
uv)2 + zuv

y, z ≥ 0,

23

(22)





which follows from Eq. (21) because of the objective function direction, and because a ≥ max(b, c) is
equivalent to a ≥ b ∧ a ≥ c. We note that Eq. (22) is no longer an unconstrained quartic, however, but a
QCP. It expresses a minimization of penalty variables to the quadratic inequality system

∀{u, v} ∈ E (dL

uv)2 ≤ (cid:107)xu − xv(cid:107)2

2 ≤ (dU

uv)2.

(23)

We also note that many local NLP solvers take very arbitrary functions in input (such as functions
expressed by computer code), so the reformulation Eq. (22) may be unnecessary when only locally optimal
solutions of Eq. (21) are needed.

6.1.2 Constrained quadratic formulations

We propose two formulations in this section. The ﬁrst is derived directly from Eq. (3):

min

(cid:80)

{u,v}∈E

s2
uv

(cid:41)

∀{u, v} ∈ E (cid:107)xu − xv(cid:107)2

2 = d2

uv + suv.

(24)

We note that Eq. (24) is a QCQP formulation. Similarly to Eq. (22) it uses additional variables to
penalize feasibility errors w.r.t. (3). Diﬀerently from Eq. (22), however, it removes the need for two
separate variables to model slack and surplus errors. Instead, suv is unconstrained, and can therefore
take any value. The objective, however, minimizes the sum of the squares of the components of s. In
practice, Eq. (24) performs much better than Eq. (3); on average, the performance is comparable to that
of Eq. (20). We remark that Eq. (24) has a convex objective function but nonconvex constraints.

The second formulation we propose is an exact reformulation of Eq. (20). First, we replace the

minimization of squared errors by absolute values, yielding

(cid:88)

min

(cid:12)
(cid:12)(cid:107)xu − xv(cid:107)2

2 − d2
uv

(cid:12)
(cid:12),

{u,v}∈E

which clearly has the same set of global optima as Eq. (20). We then rewrite this similarly to Eq. (22)
as follows:

min (cid:80)

(yuv + zuv)

{u,v}∈E

∀{u, v} ∈ E
∀{u, v} ∈ E

(cid:107)xu − xv(cid:107)2
(cid:107)xu − xv(cid:107)2

2 ≥ d2
2 ≤ d2
y, z ≥ 0,

uv − yuv
uv + zuv





which, again, does not change the global optima. Next, we note that we can ﬁx zuv = 0 without changing
global optima, since they all have the property that zuv = 0. Now we replace yuv in the objective function
by d2
2, which we can do without changing the optima since the ﬁrst set of constraints reads
2. We can discard the constant d2
yuv ≥ d2
uv from the objective, since adding constants to
the objective does not change optima, and change min −f to − max f , yielding:

uv − (cid:107)xu − xv(cid:107)2

uv − (cid:107)xu − xv(cid:107)2

max (cid:80)

{u,v}∈E

(cid:107)xu − xv(cid:107)2
2

(cid:41)

∀{u, v} ∈ E

(cid:107)xu − xv(cid:107)2

2 ≤ d2

uv,

(25)

which is a QCQP known as the “push-and-pull” formulation of the DGP, since the constraints ensure
that xu, xv are pushed closer together, while the objective attempts to pull them apart [134, §2.2.1].

6 ROBUST SOLUTION METHODS FOR THE DGP

24

Contrariwise to Eq. (24), Eq. (25) has a nonconvex (in fact, concave) objective function and convex
constraints. Empirically, this often turns out to be somewhat easier than tackling the reverse situation.
The theoretical justiﬁcation is that ﬁnding a feasible solution in a nonconvex set is a hard task in general,
whereas ﬁnding local optima of a nonconvex function in a convex set is tractable: the same cannot be
said for global optima, but in practice one is often satisﬁed with “good” local optima.

6.1.3 Semideﬁnite programming

SDP is linear optimization over the cone of psd matrices, which is convex: if A, B are two psd matrices,
C = αA + (1 − α)B is psd for α ∈ [0, 1]. Suppose there is x ∈ Rn such that x(cid:62)Cx < 0. Then
αx(cid:62)Ax + (1 − α)x(cid:62)Bx < 0, so 0 ≤ αx(cid:62)Ax < −(1 − α)x(cid:62)Bx ≤ 0, i.e. 0 < 0, which is a contradiction,
hence C is also psd, as claimed. Therefore, SDP is a subclass of cNLP.

The SDP formulation we propose is a relaxation of Eq. (3). First, we write (cid:107)xu − xv(cid:107)2

2 = (cid:104)xu, xu(cid:105) +

(cid:104)xv, xv(cid:105) − 2(cid:104)xu, xv(cid:105). Then we linearize all of the scalar products by means of additional variables Xuv:

∀{u, v} ∈ E Xuu + Xvv − 2Xuv = d2
uv

X = xx(cid:62).

We note that X = xx(cid:62) constitutes the whole set of deﬁning constraints Xuv = (cid:104)xu, xv(cid:105) (for each u, v ≤ n)
introduced by the linearization procedure (Sect. 2.4.1).

The relaxation we envisage does not entirely drop the deﬁning constraints, as in Sect. 2.4.1. Instead,
it relaxes them from X − xx(cid:62) = 0 to X − xx(cid:62) (cid:23) 0. In other words, instead of requiring that all of the
eigenvalues of the matrix X − xx(cid:62) are zero, we simply require that they should be ≥ 0. Moreover, since
the original variables x do not appear anywhere else, we can simply require X (cid:23) 0, obtaining:

∀{u, v} ∈ E Xuu + Xvv − 2Xuv = d2
uv
X (cid:23) 0.

(cid:27)

(26)

The SDP relaxation in Eq. (26) has the property that it provides a solution ¯X, which is an n × n
symmetric matrix. Spectral decomposition of ¯X yields P ΛP (cid:62), where P is a matrix of eigenvectors and
Λ = diag(λ) where λ is a vector of eigenvalues of ¯X. Since ¯X is psd, λ ≥ 0, which means that
Λ is a
real matrix. Therefore, by setting Y = P

Λ we have that

√

√

Y Y (cid:62) = (P

√

√

(cid:62)

Λ)

= P

√

√

Λ

Λ)(P

ΛP (cid:62) = P ΛP (cid:62) = ¯X,

which implies that ¯X is the Gram matrix of Y . Thus we can take Y to be a realization satisfying Eq. (3).
The only issue is that Y , as an n × n matrix, is a realization in n dimensions rather than K. Naturally,
rk(Y ) = rk( ¯X) need not be equal to n, but could be lower; in fact, in order to ﬁnd a realization of
the given graph, we would like to ﬁnd a solution ¯X with rank at most K. Imposing this constraint is
equivalent to asking that X = xx(cid:62) (which have been relaxed in Eq. (26)).

We note that Eq. (26) is a pure feasibility problem. Every SDP solver, however, also accepts an
objective function as input. In absence of a “natural” objective in a pure feasibility problem, we can
devise one to heuristically direct the search towards parts of the psd cone which we believe might contain
“good” solutions. A popular choice is

min tr(X) = min tr(P ΛP (cid:62)) = min tr(P P (cid:62)Λ) =

= min tr(P P −1Λ) = min λ1 + · · · + λn,

where tr is the trace, the ﬁrst equality follows by spectral decomposition (with P a matrix of eigenvectors
and Λ a diagonal matrix of eigenvalues of X), the second by commutativity of matrix products under
the trace, the third by orthogonality of eigenvectors, and the last by deﬁnition of trace. This aims at
minimizing the sum of the eigenvalues of X, hoping this will decrease the rank of ¯X.

6 ROBUST SOLUTION METHODS FOR THE DGP

25

For the DGP applied to protein conformation (Sect. 3.3.2), the objective function

(cid:88)

min

{u,v}∈E

(Xuu + Xvv − 2Xuv)

was empirically found to be a good choice [62, §2.1]. More (unpublished) experimentation showed that
the scalarization of the two objectives:

(cid:88)

min

{u,v}∈E

(Xuu + Xvv − 2Xuv) + γtr(X),

(27)

with γ in the range O(10−2)-O(10−3), is a good objective function for solving Eq. (26) when it is applied
to protein conformation.

In the majority of cases, solving SDP relaxations does not yield solution matrices with rank K, even
with objective functions such as Eq. (27). We discuss methods for constructing an approximate rank K
realization from ¯X in Sect. 7.

SDP is one of those problems which is not known to be in P (nor NP-complete) in the Turing machine
model. It is, however, known that SDPs can be solved in polynomial time up to a desired error tolerance
(cid:15) > 0, with the complexity depending on 1
(cid:15) as well as the instance size. Currently, however, the main
issue with SDP is technological: state-of-the art solvers do not scale all that well with size. One of the
reasons is that K is usually ﬁxed (and small) with respect to n, so the while the original problem has O(n)
variables, the SDP relaxation has O(n2). Another reason is that the Interior Point Method (IPM), which
often features as a “state of the art” SDP solver, has a relatively high computational complexity [155]: a
“big oh” notation estimate of O(max(m, n)mn2.5) is given in Bubeck’s blog at ORFE, Princeton.3

6.1.4 Diagonally dominant programming

In order to address the size limitations of SDP, we employ some interesting linear approximations of the
psd cone proposed in [126, 5]. An n × n real symmetric matrix X is diagonally dominant (dd) if

∀i ≤ n

(cid:88)

j(cid:54)=i

|Xij| ≤ Xii.

(28)

As remarked in Sect. 5.2.1, it is well known that every dd matrix is also psd, while the converse may not
hold. Speciﬁcally, the set of dd matrices form a sub-cone of the cone of psd matrices [18].

The interest of dd matrices is that, by linearization of the absolute value terms, Eq. (28) can be

reformulated so it becomes linear: we introduce an added matrix T of decision variables, then write:

∀i ≤ n

(cid:88)

j(cid:54)=i

Tij ≤ Xii

−T ≤ X ≤ T,

(29)

(30)

which are linear constraints equivalent to Eq. (28) [5, Thm. 10]. One can see this easily whenever X ≥ 0
or X ≤ 0. Note that

∀i ≤ n Xii ≥

∀i ≤ n Xii ≥

(cid:88)

j(cid:54)=i
(cid:88)

j(cid:54)=i

Tij ≥

Tij ≥

(cid:88)

j(cid:54)=i
(cid:88)

j(cid:54)=i

Xij

−Xij

follow directly from Eq. (29)-(30). Now one of the RHSs is equal to (cid:80)
j(cid:54)=i |Xij|, which implies Eq. (28).
For the general case, the argument uses the extreme points of Eq. (29)-(30) and elimination of T by
projection.

3blogs.princeton.edu/imabandit/2013/02/19/orf523-ipms-for-lps-and-sdps/

6 ROBUST SOLUTION METHODS FOR THE DGP

We can now approximate Eq. (26) by the pure feasibility LP:

∀{u, v} ∈ E Xuu + Xvv − 2Xuv = d2
uv
Tij ≤ Xii

∀i ≤ n

(cid:80)
j(cid:54)=i

−T ≤ X ≤ T,

26

(31)






which we call a diagonally dominant program (DDP). As in Eq. (26), we do not explicitly give an objective
function, since it depends on the application. Since the DDP in Eq. (31) is an inner approximation of
the corresponding SDP in Eq. (26), the DDP feasible set is a subset of that of the SDP. This situation
yields both an advantage and a disadvantage: any solution ˜X of the DDP is psd, and can be obtained
at a smaller computational cost; however, the DDP might be infeasible even if the corresponding SDP is
feasible (see Fig. 5, left). In order to decrease the risk of infeasibility of Eq. (31), we relax the equation

Figure 5: On the left, the DDP is infeasible even if the SDP is not; on the right, a relaxed set of constraints
makes the DDP feasible.

constraints to inequality, and impose an objective as in the push-and-pull formulation Eq. (25):

max (cid:80)

{u,v}∈E

(Xuu + Xvv − 2Xuv)

∀{u, v} ∈ E
∀i ≤ n

Xuu + Xvv − 2Xuv ≤ d2
uv
(cid:80)
Tij ≤ Xii
j(cid:54)=i

−T ≤ X ≤ T.






(32)

This makes the DDP feasible set larger, which means it is more likely to be feasible (see Fig. 5, right).
Eq. (32) was successfully tested on protein graphs in [62].

If C is any cone in Rn, the dual cone C ∗ is deﬁned as:

C ∗ = {y ∈ Rn | ∀x ∈ C (cid:104)x, y(cid:105) ≥ 0}.

Note that the dual cone contains the set of vectors making a non-obtuse angle with all of the vectors in
the original (primal) cone. We can exploit the dual dd cone in order to provide another DDP formulation
for the DGP which turns out to be an outer approximation. Outer approximations have symmetric
advantages and disadvantages w.r.t. the inner ones: if the original SDP is feasible, than the outer DDP
approximation is also feasible (but the DDP may be feasible even if the SDP is not); however, the solution
˜X we obtain from the DDP need not be a psd matrix. Some computational experience related to [161]
showed that it often happens that more or less half of the eigenvalues of ˜X are negative.

We now turn to the actual DDP formulation related to the dual dd cone. A cone C of n × n real

symmetric matrices is ﬁnitely generated by a set X of matrices if:

∀X ∈ C ∃δ ∈ R|X |

+ X =

δxxx(cid:62).

(cid:88)

x∈X

It turns out [18] that the dd cone is ﬁnitely generated by

Xdd = {ei | i ≤ n} ∪ {ei ± ej | i < j ≤ n},
where e1, . . . , en is the standard orthogonal basis of Rn. This is proved in [18] by showing that the
following rank-one matrices are extreme rays of the dd cone:

6 ROBUST SOLUTION METHODS FOR THE DGP

27

• Eii = diag(ei), where ei = (0, . . . , 0, 1i, 0, . . . , 0)(cid:62);

• E+

ij has a minor

(cid:18) 1ii
1ji

1ij
1jj

(cid:19)

and is zero elsewhere;

• E−

ij has a minor

(cid:18) 1ii −1ij
1jj

−1ji

(cid:19)

and is zero elsewhere,

and, moreover, that the extreme rays are generated by the standard basis vectors as follows:
∀i ≤ n Eii = eie(cid:62)
i

∀i < j ≤ n E+

∀i < j ≤ n E−

ij = (ei + ej)(ei + ej)(cid:62)
ij = (ei − ej)(ei − ej)(cid:62).

This observation allowed Ahmadi and his co-authors to write the DDP formulation Eq. (32) in terms of
the extreme rays Eii, E±

ij [5], and also to deﬁne a column generation algorithms over them [4].

If a matrix cone is ﬁnitely generated, the dual cone has the same property. Let Sn be the set of real

symmetric n × n matrices; for A, B ∈ Sn we deﬁne an inner product (cid:104)A, B(cid:105) = A • B (cid:44) tr(AB(cid:62)).

6.1 Theorem
Assume C is ﬁnitely generated by X . Then C ∗ is also ﬁnitely generated. Speciﬁcally, C ∗ = {Y ∈
Sn | ∀x ∈ X (Y • xx(cid:62) ≥ 0)}.

x∈X δxxx(cid:62)}.

+ X = (cid:80)
Proof. By assumption, C = {X ∈ Sn | ∃δ ∈ R|X |
(⇒) Let Y ∈ Sn be such that, for each x ∈ X , we have Y • xx(cid:62) ≥ 0. We are going to show that Y ∈ C ∗,
which, by deﬁnition, consists of all matrices Y such that for all X ∈ C, Y • X ≥ 0. Note that, for
all X ∈ C, we have X = (cid:80)
x δxY • xx(cid:62) ≥ 0 (by
deﬁnition of Y ), whence Y ∈ C ∗.
(⇐) Suppose Z ∈ C ∗ (cid:114) {Y | ∀x ∈ X (Y • xx(cid:62) ≥ 0)}. Then there is X (cid:48) ⊂ X such that for any x ∈ X (cid:48) we
have Z • xx(cid:62) < 0. Consider any Y = (cid:80)
x∈X (cid:48) δxZ • xx(cid:62) < 0,
(cid:50)
so Z (cid:54)∈ C ∗, which is a contradiction. Therefore C ∗ = {Y | ∀x ∈ X (Y • xx(cid:62) ≥ 0)} as claimed.

x∈X δxxx(cid:62) (by ﬁnite generation). Hence Y • X = (cid:80)

x∈X (cid:48) δxxx(cid:62) ∈ C with δ ≥ 0. Then Z • Y = (cid:80)

We are going to exploit Thm. 6.1 in order to derive an explicit formulation of the following DDP

formulation based on the dual cone C ∗

dd of the dd cone Cdd ﬁnitely generated by Xdd:

∀{u, v} ∈ E Xuu + Xvv − 2Xuv = d2
uv
X ∈ C ∗
dd.

(cid:27)

We remark that X • vv(cid:62) = v(cid:62)Xv for each v ∈ Rn. By Thm. 6.1, X ∈ C ∗
Xdd v(cid:62)Xv ≥ 0. We obtain the following LP formulation:

dd can be restated as ∀v ∈

max (cid:80)

{u,v}∈E

(Xuu + Xvv − 2Xuv)

∀{u, v} ∈ E
∀v ∈ Xdd

Xuu + Xvv − 2Xuv = d2
uv
v(cid:62)Xv ≥ 0.






(33)

With respect to the primal DDP, the dual DDP formulation in Eq. (33) provides a very tight bound
to the objective function value of the push-and-pull SDP formulation Eq. (25). On the other hand, the
solution ¯X is usually far from being a psd matrix.

6.2 Fast high-dimensional methods

In Sect. 6.1 we surveyed methods based on MP, which are very ﬂexible, insofar as they can accommodate
side constraints and noisy data, but computationally demanding. In this section we discuss two very fast,
yet robust, methods for embeddings graphs in Euclidean spaces.

6 ROBUST SOLUTION METHODS FOR THE DGP

28

6.2.1

Incidence vectors

The simplest, and most naive methods for mapping graphs into vectors are given by exploiting various
incidence information in the graph structure. By contrast, the resulting embeddings are unrelated to
Eq. (3).

Given a simple graph G = (V, E) with |V | = n, |E| = m and edge weight function w : E → R+, we
present two approaches: one which outputs an n × n matrix, and one which outputs a single vector in
RK with K = 1

2 n(n − 1).

1. For each u ∈ V , let xu = (xuv | v ∈ V ) ∈ Rn be the incidence vector of N (u) on V , i.e.:

∀u ∈ V

xuv =

(cid:26) wuv
0

if {u, v} ∈ E
otherwise.

2. Let K = 1

2 n(n − 1), and xE = (xe | e ∈ E) ∈ RK be the incidence vector of the edge set E into the

set {{i, j} | i < j ≤ n}, i.e.:

xe =

(cid:26) we
0

if e ∈ E
otherwise.

Both embeddings can be obtained in O(n2) time. Both embeddings are very high dimensional. So that
they may be useful in practice, it is necessary to post-process them using dimensional reduction techniques
(see Sect. 7).

6.2.2 The universal isometric embedding

This method, also called Fr´echet embedding, is remarkable in that it maps any ﬁnite metric space congru-
ently into a set of vectors in the (cid:96)∞ norm [102, §6]. No other norm allows exact congruent embeddings in
vector spaces [130]. The Fr´echet embedding provided the foundational idea for several other probabilistic
approximate embeddings in various other norms and dimensions [36, 125].

6.2 Theorem
Given any ﬁnite metric space (X, d), where |X| = n and d is a distance function deﬁned on X, there
exists an embedding ρ : X → Rn such that (ρ(X), (cid:96)∞) is congruent to (X, d).

This theorem is surprising because of its generality in conjunction with the exactness of the result:
it
works on any (ﬁnite) metric space. The “magic hat” out of which we shall pull the vectors in ρ(X) is
simply the only piece of data we are given, namely the distance matrix of X. More precisely the i-th
element of X is mapped to the vector corresponding to the i-th column of the distance matrix. Proof. Let
D(X) be the distance matrix of (X, d), namely Dij(X) = (d(xi, xj)) where X = {x1, . . . , xn}. We denote
d(xi, xj) = dij for brevity. For any j ≤ n we let ρ(xj) = δj, where δj is the j-th column of D(X). We
have to show that (cid:107)ρ(xi) − ρ(xj)(cid:107)∞ = dij for each i < j ≤ n. By deﬁnition of the (cid:96)∞ norm, for each
i < j ≤ n we have

(cid:107)ρ(xi) − ρ(xj)(cid:107)∞ = (cid:107)δi − δj(cid:107)∞ = max
k≤n

|δik − δjk| = max
k≤n

|dik − djk|.

(∗)

By the triangular inequality on (X, d), for i < j ≤ n and k ≤ n we have:

dik ≤ dij + djk ∧ djk ≤ dij + dik
⇒ dik − djk ≤ dij ∧ djk − dik ≤ dij
⇒ |dik − djk| ≤ dij;

6 ROBUST SOLUTION METHODS FOR THE DGP

29

since these inequalities are valid for each k, by (∗) we have:

(cid:107)ρ(xi) − ρ(xj)(cid:107)∞ ≤ max

k

dij = dij,

(†)

where the last equality follows because dij does not depend on k. Now we note that the maximum of
|dik − djk| over k must exceed the value of the same expression when either of the terms dik or djk is
zero, i.e. when k ∈ {i, j}, since, when k = i, then |dik − djk| = |dii − dji| = dij, and the same holds when
k = j. Hence,

By (∗), (†) and (‡), we ﬁnally have:

max
k≤n

|dik − djk| ≥ dij.

(‡)

∀i < j ≤ n (cid:107)ρ(xi) − ρ(xj)(cid:107)∞ = dij

as claimed.

(cid:50)

We remark that Thm. 6.2 is only applicable when D(X) is a distance matrix, which corresponds to
the case of a graph G edge-weighed by d being a complete graph. We address the more general case
of any (connected) simple graph G = (V, E), corresponding to a partially deﬁned distance matrix, by
completing the matrix using the shortest path metric (this distance matrix completion method was used
for the Isomap heuristic, see [173, 113] and Sect. 7.1.1):

∀{i, j} (cid:54)∈ E dij = shortest path lengthG(i, j).

(34)

In practice, we can compute the lengths of all shortest paths in G by using the Floyd-Warshall algorithm,
which runs in O(n3) time (but in practice it is very fast).

This method yields a realization of G in (cid:96)n

∞, which is a high-dimensional embedding. It is necessary

to post-process it using dimensional reduction techniques (see Sect. 7).

6.2.3 Multidimensional scaling

The literature on Multidimensional Scaling (MDS) is extensive [51, 34], and many variants exist. The
basic version, called classic MDS, aims at ﬁnding an approximate realization of a partial distance matrix.
In other words, it is a heuristic solution method for the

Euclidean Distance Matrix Completion Problem (EDMCP). Given a simple undi-
rected graph G = (V, E) with an edge weight function w : E → R+, determine whether there
exists an integer K > 0 and a realization x : V → RK such that Eq. (3) holds.

The diﬀerence between EDMCP and DGP may appear diminutive, but it is in fact very important.
In the DGP the integer K is part of the input, whereas in the EDMCP it is part of the output. This has
a large eﬀect on worst-case complexity: while the DGP is NP-hard even when only an ε-approximate
realization is sought [162, §5], ε-approximate realizations of EDMCPs can be found in polynomial time
by solving an SDP [7]. Consider the following matrix:

∆(E, d) =

(cid:26) w2
ij
dij

if {i, j} ∈ E
otherwise,

where d = (dij | {i, j} (cid:54)∈ E) is a vector of decision variables, and J = In − 1
vector. Then the following formulation is valid for the EDMCP:

n 11(cid:62), with 1 being the all-one

1 • T

min
d,T,G
−T ≤ G + 1
G (cid:23) 0,

2 J ∆(E, d) J ≤ T






(35)

where 1 is the n × n all-one matrix.

7 DIMENSIONAL REDUCTION TECHNIQUES

30

6.3 Theorem
The SDP in Eq. (35) correctly models the EDMCP.

By “correctly models” we mean that the solution of the EDMCP can be obtained in polynomial time
from the solution of the SDP in Eq. (35). Proof. First, we remark that, given a realization x : V → Rn,
its Gram matrix is G = xx(cid:62), and its squared Euclidean distance matrix (EDM) is

D2 = ((cid:107)xu − xv(cid:107)2

2 | u ≤ n ∧ v ≤ n) ∈ Rn×n.

Next, we recall that

G = In −

1
2

JD2J

(36)

by [57] (after [165] — see [114, §7] for a direct proof). Now we note that minimizing 1 • T subject to
−T ≤ G + 1

2 J∆(E, d)J ≤ T is an exact reformulation of

min
G,d

(cid:107)G − (−1/2)J∆(E, d)J)(cid:107)1,

(∗)

since 1 • T = (cid:80)

i,j Tij, and T is used to “sandwich” the argument of the (cid:96)1 norm in (∗).

We also recall another basic fact of linear algebra: a matrix is Gram if and only if it is psd: hence,
requiring G (cid:23) 0 forces G to be a Gram matrix. Consequently, if the optimal objective function value of
Eq. (35) is zero with corresponding solution d∗, T ∗, G∗, then tr(T ∗) = 0 ⇒ T ∗ = 0 ⇒ (∗). Moreover, G∗
is a Gram matrix, so ∆(E, d∗) is its corresponding EDM. Lastly, the realization x∗ corresponding to the
Gram matrix G∗ can be obtained by spectral decomposition of G∗ = P ΛP (cid:62), which yields x∗ = P
Λ:
this implies that the EDMCP instance is YES. Otherwise T ∗ (cid:54)= 0, which means that the EDMCP instance
(cid:50)
is NO (otherwise there would be a contradiction on tr(T ∗) > 0 being optimal).

√

The practically useful corollary to Thm. (6.3) is that solving Eq. (35) provides an approximate solution

x∗ even if ∆(E, d) cannot be completed to an EDM.

Classic MDS is an eﬃcient heuristic method for ﬁnding an approximate realization of a partial distance

matrix ∆(E, d). It works as follows:

1. complete ∆(E, d) to an approximate EDM ˜D2 using the shortest-path metric (Eq. (34));

2. let ˜G = In − 1

n J ˜D2J;

3. let P ˜ΛP (cid:62) be the spectral decomposition of ˜G;

4. if ˜Λ ≥ 0 then, by Eq. (36), ˜D2 is a EDM, with corresponding (exact) realization ˜x = P

√

Λ;

5. otherwise, let Λ+ = diag((max(λ, 0) | λ ∈ Λ)): then ˜x = P

˜D2.

√

Λ+ is an approximate realization of

Note that both Eq.(35) and classic MDS determine K as part of the output, i.e. K is the rank of the
realization (respectively x∗ and ˜x).

7 Dimensional reduction techniques

Dimensional reduction techniques reduce the dimensionality of a set of vectors according to diﬀerent
criteria, which may be heuristic, or give some (possibly probabilistic) guarantee of keeping some quantity
approximately invariant. They are necessary in order to make many of the methods in Sect. 6 useful in
practice.

7 DIMENSIONAL REDUCTION TECHNIQUES

31

7.1 Principal component analysis

Principal Component Analysis (PCA) is one of the foremost dimensional reduction techniques.
ascribed to Harold Hotelling4 [87].

It is

Consider an n×m matrix X consisting of n data row vectors in Rm, and let K < m be a given integer.
We want to ﬁnd a change of coordinates for X such that the ﬁrst component has largest variance over
the transformed vectors, the second component has second-largest variance, and so on, until the K-th
component. The other components can be neglected, as the variance of the data in those directions is
low.

The usual geometric interpretation of PCA is to take the smallest enclosing ellipsoid E for X: then the
required coordinate change maps component 1 to the line parallel to the largest radius of E, component
2 to the line parallel to the second-largest radius of E, and so on until component K (see Fig. 6). The

Figure 6: Geometric interpretation of PCA (image from [188]).

statistical interpretation of PCA looks for the change of coordinates which makes the data vectors be
uncorrelated in their components. Fig. 6 should give an intuitive idea about why this interpretation
corresponds with the ellipsoid of the geometric interpretation. The cartesian coordinates in Fig. 6 are
certainly correlated, while the rotated coordinates look far less correlated. The zero correlation situation
corresponds to a perfect ellipsoid. An ellipsoid is described by the equation (cid:80)
= 1, which has
no mixed terms xixj contributing to correlation. Both interpretations are well (and formally) argued in
[180, §2.1].

(cid:0) xj
rj

j≤n

(cid:1)2

The interpretation we give here is motivated by DG, and related to MDS (Sect. 6.2.3). PCA can be
seen as a modiﬁcation of MDS which only takes into account the K (nonnegative) principal components.
Instead of Λ+ (step 5 of the MDS algorithm), PCA uses a diﬀerent diagonal matrix Λpca: the i-th diagonal
component is

(cid:26) max(Λii, 0)

Λpca

ii =

0

if i ≤ K
otherwise,

(37)

where P ΛP (cid:62) is the spectral decomposition of ˜G. In this interpretation, when given a partial distance
matrix and the integer K as input, PCA can be used as an approximate solution method for the DGP.

4A young and unknown George Dantzig had just ﬁnished his presentation of LP to an audience of “big shots”, including
Koopmans and Von Neumann. Harold Hotelling raised his hand, and stated: “but we all know that the world is nonlinear!”,
thereby obliterating the simplex method as a mathematical curiosity. Luckily, Von Neumann answered on Dantzigs behalf
and in his defence [54].

7 DIMENSIONAL REDUCTION TECHNIQUES

32

On the other hand, the PCA algorithm is most usually considered as a method for dimensionality

reduction, so it has a data matrix X and an integer K as input. It is as follows:

1. let ˜G = XX (cid:62) be the n × n Gram matrix of the data matrix X;

2. let P ˜ΛP (cid:62) be the spectral decomposition of ˜G;

3. return ˜x = P

√

Λpca.

Then ˜x is an n × K matrix, where K < n. The i-th row vector in ˜x is a dimensionally reduced
representation of the i-th row vector in X.

There is an extensive literature on PCA, ranging over many research papers, dedicated monographs

and textbooks [188, 94, 180]. Among the variants and extensions, see [59, 160, 56, 8, 61].

7.1.1

Isomap

One of the most interesting applications of PCA is possibly the Isomap algorithm [173], already mentioned
above in Sect. 6.2.2, which is able to use PCA in order to perform a nonlinear dimensional reduction
from the original dimension m to a given target dimension K, as follows.

1. Form a connected graph H = (V, E) with the column indices 1, . . . , n of X as vertex set V : determine
a threshold value τ such that, for each column vector xi in X (for i ≤ n), and for each xj in X
such that (cid:107)xi − xj(cid:107)2 ≤ τ , the edge {i, j} is in the edge set E; the graph H should be as sparse as
possible but also connected.

2. Complete H using the shortest path metric (Eq. (34)).

3. Use PCA in the MDS interpretation mentioned above: interpret the completion of (V, E) as a metric
space, construct its (approximate) EDM ˜D, compute the corresponding (approximate) Gram matrix
˜G, compute the spectral decomposition of ˜G, replace its diagonal eigenvalue matrix Λ as in Eq. (37),
and return the corresponding K-dimensional vectors.

Intuitively, Isomap works well because in many practical situations where a set X of points in Rm are
close to a (lower) K-dimensional manifold, the shortest path metric is likely to be a better estimation of
the Euclidean distance in RK than the Euclidean distance in Rm, see [173, Fig. 3].

7.2 Barvinok’s naive algorithm

By Eq. (26), we can solve an SDP relaxation of the DGP and obtain an n×n psd matrix solution ¯X which,
in general, will not have rank K (i.e., it will not yield an n × K realization matrix, but rather an n × n
one). In this section we shall derive a dimensionality reduction algorithm to obtain an approximation of
¯X which has the correct rank K.

7.2.1 Quadratic Programming feasibility

Barvinok’s naive algorithm [20, §5.3] is a probabilistic algorithm which can ﬁnd an approximate vector
solution x(cid:48) ∈ Rn to a system of quadratic equations

∀i ≤ m x(cid:62)Qix = ai,

(38)

7 DIMENSIONAL REDUCTION TECHNIQUES

33

where the Qi are n × n symmetric matrices, a ∈ Rm, x ∈ Rn, and m is polynomial in n. The analysis of
this algorithm provides a probabilistic bound on the maximum distance that x(cid:48) can have from the set of
solutions of Eq. (38). Thereafter, one can run a local NLP solver with x(cid:48) as a starting point, and obtain
a hopefully good (approximate) solution to Eq. (38). We note that this algorithm is still not immediately
applicable to the our setting where K could be diﬀerent from 1: we shall address this issue in Sect. 7.2.4.

Barvinok’s naive algorithm solves an SDP relaxation of Eq. (38), and then retrieves a certain random-

ized vector from the solution:

1. form the SDP relaxation

of Eq. (38) and solve it to obtain ¯X ∈ Rn×n;

∀i ≤ m (Qi • X = ai) ∧ X (cid:23) 0

(39)

2. let T =

¯X, which is a real matrix since ¯X (cid:23) 0 (T can be obtained by spectral decomposition,

√

i.e. ¯X = P ΛP (cid:62) and T = P

Λ);

√

3. let y be a vector sampled from the multivariate normal distribution Nn(0, 1);

4. compute and return x(cid:48) = T y.

The analysis provided in [20] shows that ∃c > 0 and an integer n0 ∈ N such that ∀n ≥ n0

(cid:18)

∀i ≤ m dist(x(cid:48), Xi) ≤ c

(cid:113)

(cid:107) ¯X(cid:107)2 ln n

(cid:19)

≥ 0.9.

P

(40)

In Eq. (40), P(·) denotes the probability of an event,

dist(b, B) = inf
β∈B

(cid:107)b − β(cid:107)2

is the Euclidean distance between the point b and the set B, and c is a constant that only depends on
logn m. We note that the term (cid:112)(cid:107) ¯X(cid:107)2 in Eq. (40) arises from T being a factor of ¯X. We note also that
0.9 follows from assigning some arbitrary value to some parameter — i.e. 0.9 can be increased as long as
the problem size is large enough.

For cases of Eq. (38) where one of the quadratic equations is (cid:107)x(cid:107)2

2 = 1 (namely, the solutions of
Eq. (38) must belong to the unit sphere), it is noted in [20, Eg. 5.5] that, if ¯X is “suﬃciently generic”,
(cid:112) ¯X2 ln n → 0 as
then it can be argued that (cid:107) ¯X(cid:107)2 = O(1/n), which implies that the bounding function c
n → ∞. This, in turn, means that x(cid:48) converges towards a feasible solution of the original problem in the
limit.

7.2.2 Concentration of measure

The term ln n in Eq. (40) arises from a phenomenon of high-dimensional geometry called “concentration
of measure”.

We recall that a function f : X → R is Lipschitz if there is a constant M > 0 s.t. for any x, y ∈ X we
have |f (x) − f (y)| < M (cid:107)x − y(cid:107)2. A measure space (X , µ) has the concentration of measure property if
for any Lipschitz function f , there are constants C, c > 0 such that:

∀ε > 0 P(|f (x) − Eµ(f )| > ε | x ∈ X ) ≤ C e−cε2

(41)

where Eµ(f ) = (cid:82)
X f (x)dµ. In other words, X has measure concentration if for any Lipschitz function
f , its discrepancy from its mean value is small with arbitrarily high probability. It turns out that the
Euclidean space Rn with the Gaussian density measure φ(x) = (2π)n/2e−(cid:107)x(cid:107)2
2/2 has measure concentration
[21, §5.3].

7 DIMENSIONAL REDUCTION TECHNIQUES

34

Measure concentration is interesting in view of applications since, given any large enough closed subset

A of X , its ε-neighbourhood

A(ε) = {x ∈ X | dist(x, A) ≤ ε}

(42)

contains almost the whole measure of X . More precisely, if (X , µ) has measure concentration and A ⊂ X
is closed, for any p ∈ (0, 1) there is a ε0(p) > 0 such that [124, Prop. 2]:

∀ε ≥ ε0(p) µ(A(ε)) > 1 − p.

(43)

Eq. (43) is useful for applications because it deﬁnes a way to analyse probabilistic algorithms. For a
random point sampled in (X , µ) that happens to be in A on average, Eq. (43) ensures that it is unlikely
that it should be far from A. This can be used to bound errors, as Barvinok did with his naive algorithm.
Concentration of measure is fundamental in data science, insofar as it may provide algorithmic analyses
to the eﬀect that some approximation errors decrease in function of the increasing instance size.

7.2.3 Analysis of Barvinok’s algorithm

We sketch the main lines of the analysis of Barvinok’s algorithm (see [19, Thm. 5.4] or [124, §3.2] for a
more detailed proof). We let X = Rn and µ(x) = φ(x) be the Gaussian density measure. It is easy to
show that

for each i ≤ m, and, from this, that given the factorization ¯X = T T (cid:62), that

Eµ(x(cid:62)Qix | x ∈ X ) = tr(Qi)

Eµ(x(cid:62)T (cid:62)Qi T x | x ∈ X ) = tr(T (cid:62)Qi T ) = tr(Qi ¯X) = Qi • ¯X = ai.

This shows that, for any y ∼ Nn(0, 1), the average of y(cid:62)T (cid:62)Qi T y is ai.

The analysis then goes on to show that, for some y ∼ Nn(0, 1), it is unlikely that y(cid:62)T (cid:62)Qi T y should
be far from ai. It achieves this result by deﬁning the sets A+
i = {x ∈
Rn | x(cid:62)Qix ≤ ai}, and their respective neighbourhoods A+
i (ε), A−
i (ε). Using a technical lemma [124,
i (ε) and A−
Lemma 4] it is possible to apply Eq. (43) to A+
i (ε) to argue for concentration of measure.
Applying the union bound it can be shown that their intersection Ai(ε) is the neighbourhood of Ai =
{x ∈ Rn | x(cid:62)Qix = ai}. Another application of the union bound to all the sets Ai(ε) yields the result
[124, Thm. 5].

i = {x ∈ Rn | x(cid:62)Qix ≥ ai}, A−

We note that concentration of measure proofs often have this structure: (a) prove that a certain event
holds on average; (b) prove that the discrepancy from average gets smaller and/or more unlikely with
increasing size. Usually proving (a) is easier than proving (b).

7.2.4 Applicability to the DGP

The issue with trying to apply Barvinok’s naive algorithm to the DGP is that we should always assume
K = 1 by Eq. (38). To circumvent this issue, we might represent an n × K realization matrix as a vector
in RnK by stacking its columns (or concatenating its rows). This, on the other hand, would require
solving SDPs with nK × nK matrices, which is prohibitive because of size.

Luckily, Barvinok’s naive algorithm can be very easily extended to arbitrary values of K. We replace

Step 3 by:

3b. let y be an n × K matrix sampled from Nn×K(0, 1).

The corresponding analysis needs some technical changes [124], but the overall structure is the same as
the case K = 1. The obtained bound replaces

ln n in Eq. (40) with

ln nK.

√

√

7 DIMENSIONAL REDUCTION TECHNIQUES

35

In the DGP case, the special structure of the matrices Qi (for i ranging over the edge set E) makes
it possible to remove the factor K, so we retrieve the exact bound of Eq. (40). As noted in Sect. 7.2.1,
if the DGP instance is on a sphere [123], this means that x(cid:48) = T y converges to an exact realization with
probability 1 in the limit of n → ∞. Similar bounds to Eq. (40) were also derived for the iDGP case
[124].

Barvinok also described concentration of measure based techniques for ﬁnding low-ranking solutions
solutions of the SDP in Eq. (39) (see [19] and [21, §6.2]), but these do not allow the user to specify an
arbitrary rank K, so they only apply to the EDMCP.

7.3 Random projections

Random projections are another dimensionality reduction technique exploiting high-dimensional geometry
properties and, in particular, the concentration of measure phenomenon (Sect. 7.2.2). They are more
general than Barvinok’s naive algorithm (Sect. 7.2) in that they apply to sets of vectors in some high-
dimensional Euclidean space Rn (with n (cid:29) 1). These sets are usually ﬁnite and growing polynomially
with instance sizes [176], but they may also be inﬁnite [192], in which case the technical name used is
subspace embeddings.

7.3.1 The Johnson-Lindenstrauss Lemma

The foremost result in RPs is the celebrated Johnson-Lindenstrauss Lemma (JLL) [93]. For a set of
ε2 ln (cid:96)) and a mapping f : X → Rk such
vectors X ⊂ Rn with |X | = (cid:96), and an ε ∈ (0, 1) there is a k = O( 1
that:

∀x, y ∈ X (1 − ε)(cid:107)x − y(cid:107)2 ≤ (cid:107)f (x) − f (y)(cid:107)2 ≤ (1 + ε)(cid:107)x − y(cid:107)2.

(44)

The proof of this result [93, Lemma 1] is probabilistic, and show that an f satisfying Eq. (44) exists with
some nonzero probability.

Later and more modern proofs (e.g. [55]) clearly point out that f can be a linear operator represented
by a k × n matrix T , each component of which can be sampled from a subgaussian distribution. This
term refers to a random variable V for which there are constants C, c s.t. for each t > 0 we have

P(|V| > t) ≤ C e−ct2

.

In particular, the Gaussian distribution is also subgaussian. Then the probability that a randomly
sampled T satisﬁes Eq. (44) can be shown to exceed 1/(cid:96). The union bound then provides an estimate on
the number of samplings of T necessary to guarantee Eq. (44) with a desired probability.

Some remarks are in order.

1. Computationally Eq. (44) is applied to some given data as follows: given a set X of (cid:96) vectors in Rn
and some error tolerance ε ∈ (0, 1), ﬁnd an appropriate k = O( 1
ε2 ln (cid:96)), construct the k × n RP T
by sampling each of its components from N(0, 1√
), then deﬁne the set T X = {T x | x ∈ X}. By
k
the JLL, T X is approximately congruent to X in the sense of Eq. (44); however, T X ⊂ Rk whereas
X ⊂ Rn, and, typically, k (cid:28) n.

2. The computation of an appropriate k would appear to require an estimation of the constant in the
expression O( 1
ε2 ln (cid:96)). Values computed theoretically are often so large as to make the technique
useless in practice. As far as we know, this constant has only been done empirically in some cases
[177], ending up with an estimation of the constant at 1.8 (which is the value we employed in most
of our experiments).

7 DIMENSIONAL REDUCTION TECHNIQUES

36

3. The term 1√
k

is the standard deviation of the normal distribution from which the components of T
must be sampled. It corresponds to a scaling of the vectors in T X induced by the loss in dimensions
(see Thm. 7.1).

4. In the expression O( 1

ε2 ln (cid:96)), the logarithmic term is the one that counts for analysis purposes, but
in practice ε−2 can be large. Our advice is to take ε ∈ (0.1, 0.2) and then ﬁne-tune ε according to
results.

5. Surprisingly, the target dimension k is independent of the original dimension n.

6. Even if the data in X is sparse, T X ends up being dense. Diﬀerent classes of sparse RPs have been
investigated [2, 96] in order to tackle this issue. A simple algorithm [53, §5.1] consists in initializing
1√
T as the k × n zero matrix, and then only ﬁll components using samples from N(0,
kp ) with some
given probability p. The value of p corresponds to the density of T . In general, and empirically, it
appears that the larger n and (cid:96) are, the sparser T can be.

7. Obviously, a Euclidean space of dimension k can embed at most k orthogonal vectors. An easy,
but surprising corollary of the JLL is that as many as O(2k) approximately orthogonal vectors can
ﬁt in Rk. This follows by [182, Prop. 1] applied to the standard basis S = {e1, . . . , en} of Rn: we
obtain ∀i < j ≤ n (−ε ≤ (cid:104)T ei, T ej(cid:105) − eiej ≤ ε), which implies |(cid:104)T ei, T ej(cid:105)| ≤ ε with T S ⊂ Rk and
k = O(ln n). Therefore T S is a set of O(2k) almost orthogonal vectors in Rk, as claimed.

8. Typical applications of RPs arise in clustering databases of large ﬁles (e.g. e-mails, images, songs,
videos), performing basic tasks in ML (e.g. k-means [37], k-nearest neighbors (k-NN) [91], robust
learning [15] and more [89]), and approximating large MP formulations (e.g. LP, QP, see Sect. 7.3.3).

9. The JLL seems to suggest that most of the information encoded by the congruence of a set of
vectors can be maintained up to an ε tolerance in much smaller dimensional spaces. This is not
true for sets of vectors in low dimensions. For example, with n ∈ {2, 3} a few attempts immediately
show that RPs yield sets of projected vectors which are necessarily incongruent with the original
vectors.

In this paper, we do not give a complete proof of the JLL, since many diﬀerent ones have already been
provided in research articles [93, 55, 90, 6, 96, 129, 9] and textbooks [176, 130, 97, 179]. We only prove
the ﬁrst part of the proof, namely the easy result that RPs preserve norms on average. This provides an
explanation for the variance 1/k of the distribution from which the components of T are sampled.

7.1 Theorem
Let T be a k × n RP sampled from N(0, 1√
k

), and u ∈ Rn; then E((cid:107)T u(cid:107)2

2) = (cid:107)u(cid:107)2
2.

Proof. We prove the claim for (cid:107)u(cid:107)2 = 1; the result will follow by scaling. For each i ≤ k we deﬁne
vi = (cid:80)

j≤n Tijuj. Then E(vi) = E(cid:0) (cid:80)

j≤m E(Tij)uj = 0. Moreover,

j≤m Tijuj

(cid:1) = (cid:80)

Var(vi) =

(cid:88)

j≤m

Var(Tijuj) =

(cid:88)

j≤m

Var(Tij)u2

j =

u2
j
k

(cid:88)

j≤m

=

1
k

(cid:107)u(cid:107)2 =

1
k

.

Now, 1

k = Var(vi) = E(v2

i − (E(vi))2) = E(v2

i − 0) = E(v2

i ). Hence

E((cid:107)T u(cid:107)2) = E((cid:107)v(cid:107)2) = E(cid:0) (cid:88)

(cid:1) =

v2
i

i≤k

E(v2

i ) =

(cid:88)

i≤k

1
k

(cid:88)

i≤k

= 1,

as claimed.

(cid:50)

7 DIMENSIONAL REDUCTION TECHNIQUES

37

7.3.2 Approximating the identity

If T is a k × n RP where k = O(ε−2 ln n), both T T (cid:62) and T (cid:62)T have some relation with the identity
matrices Ik and In. This is a lesser known phenomenon, so it is worth discussing it here in some detail.

We look at T T (cid:62) ﬁrst. By [198, Cor. 7] for any (cid:15) ∈ (0, 1

2 ) we have

(cid:107)

1
n

T T (cid:62) − Ik(cid:107)2 ≤ ε

with probability at least 1 − δ as long as n ≥ (k+1) ln(2k/δ)

C ε2

, where C ≥ 1

4 is a constant.

In Table 1 we give values of (cid:107)s T T (cid:62) − Id(cid:107)2 for s ∈ {1/n, 1/d, 1}, n ∈ {1000, 2000, . . . , 10000} and
d = (cid:100)ln(n)/(cid:15)2(cid:101) where (cid:15) = 0.15. It is clear that the error decreases as the size increases only in the case

s

1e3
1/n 9.72
5e1
1/d
2e5
1

2e3
7.53
1e2
4e5

3e3
6.55
1.5e2
6e5

4e3
5.85
2e2
8e5

5e3
5.36
2.5e2
1e6

n

e3
5.01
3e2
1.2e6

7e3
4.71
3.5e2
1.4e6

8e3
4.44
3.9e2
1.6e6

9e3
4.26
4.4e2
1.8e6

1e4
4.09
4.8e2
2e6

Table 1: Values of (cid:107)sT T (cid:62) − Id(cid:107) in function of s, n.

s = 1

n . This seems to indicate that the scaling is a key parameter in approximating the identity.

Let us now consider the product T (cid:62)T . It turns out that, for each ﬁxed vector x not depending on T ,

the matrix T (cid:62)T behaves like the identity w.r.t. x.

7.2 Theorem
Given any ﬁxed x ∈ Rn, (cid:15) ∈ (0, 1) and a RP P ∈ Rd×n, there is a universal constant C such that

− 1ε ≤ T (cid:62)T x − x ≤ 1ε.

(45)

with probability at least 1 − 4eC(cid:15)2d.

Proof. By deﬁnition, for each i ≤ n we have xi = (cid:104)ei, x(cid:105), where ei is the i-th unit coordinate vector. By
elementary linear algebra we have (cid:104)ei, T (cid:62)T x(cid:105) = (cid:104)T ei, T x(cid:105). By [53, Lemma 3.1], for i ≤ n we have

with arbitrarily high probability, which implies the result.

(cid:50)

(cid:104)ei, x(cid:105) − (cid:15)(cid:107)x(cid:107)2 ≤ (cid:104)T ei, T x(cid:105) ≤ (cid:104)ei, x(cid:105) + (cid:15)(cid:107)x(cid:107)

One might be tempted to infer from Thm. 7.2 that T (cid:62)T “behaves like the identity matrix” (indepen-

dently of x). This is generally false: Thm. 7.2 only holds for a given (ﬁxed) x.

In fact, since T is a k × n matrix with k < n, T (cid:62)T is a square symmetric psd n × n matrix with rank
k, hence n − k of its eigenvalues are zero — and the nonzero eigenvalues need not have value one. On the
other hand, T (cid:62)T looks very much like a slightly perturbed identity, on average, as shown in Table 2.

7.3.3 Using RPs in MP

Random projections have mostly been applied to probabilistic approximation algorithms. By randomly
projecting their (vector) input, one can execute algorithms with lower-dimensional vector more eﬃciently.
The approximation guarantee is usually derived from the JLL or similar results.

8 DISTANCE INSTABILITY

38

n
500
1000
1500
2000
2500
3000
3500
4000

diagonal
1.00085
1.00069
0.99991
1.00194
0.99920
0.99986
1.00044
0.99693

oﬀ-diag
0.00014
0.00008
-0.00006
0.00005
-0.00004
-0.00000
0.00000
0.00000

Table 2: Average values of diagonal and oﬀ-diagonal components of T (cid:62)T in function of n, where T is a
k × n RP with k = O(ε−2 ln n) and (cid:15) = 0.15.

A line of research about applying RPs to MP fromulations has been started in [183, 182, 181, 53].
Whichever algorithm one may choose in order to solve the MP, the RP properties guarantee an approxi-
mation on optimality and/or feasibility. Thus, this approach leads to stronger/more robust results with
respect to applying RPs to algorithmic input.

Linear and integer feasibility problems (i.e. LP and MILP formulations without objective function)
are investigated in [183] from a purely theoretical points of view. The eﬀect of RPs on LPs (with nonzero
objective) are investigated in [182], both theoretically and computationally. Speciﬁcally, the randomly
projected LP formulation is shown to have bounded feasibility error and an approximation guarantee on
optimality. The computational results suggest that the range of practical application of this technique
starts with relatively small LPs (thousands of variables/constraints). In both [183, 182] we start from a
(MI)LP in standard form

P ≡ min{c(cid:62)x | Ax = b ∧ x ≥ 0 ∧ x ∈ X}
(where X = Rn or Zn respectively), and obtain a randomly projected formulation under the RP T ∼
Nn×k(0, 1√
k

) with the form

T P ≡ min{c(cid:62)x | T Ax = T b ∧ x ≥ 0 ∧ x ∈ X},

i.e. T reduces the number of constraints in P to O(ln n), which can therefore be solved more eﬃciently.

The RP technique in [181, 53] is diﬀerent, insofar as it targets the number of variables. In [53] we

consider a QP of the form:

Q ≡ max{x(cid:62)Qx + c(cid:62)x | Ax ≤ b},

where Q is n × n, c ∈ Rn, A is m × n, and b ∈ Rm, x ∈ Rn. This is projected as follows:

T Q ≡ max{u(cid:62) ¯Qx + ¯c(cid:62)u | ¯Au ≤ b},

where ¯Q = T QT (cid:62) is k × k, ¯A = AT (cid:62) is m × k, ¯c = T c is in Rk, and u ∈ Rk. In [181] we consider a QCQP
Q(cid:48) like Q but subject to a ball constraint (cid:107)x(cid:107)2 ≤ 1. In the projected problem T Q(cid:48), this is replaced by
a ball constraint (cid:107)u(cid:107)2 ≤ 1. Both [53, 181] are both theoretical and computational. In both cases, the
number of variables of the projected problem is O(ln n).

In applying RPs to MPs, one solves the smaller projected problems in order to obtain an answer
concerning the corresponding original problems.
In most cases one has to devise a way to retrieve a
solution for the original problem using the solution of the projected problem. This may be easy or
diﬃcult depending on the structure of the formulation and the nature of the RP.

8 Distance instability

Most of the models and methods in this survey are based on the concept of distance: usually Euclidean,
occasionally with other norms. The k-means algorithm (Sect. 5.1.1) is heavily based on Euclidean dis-
tances in Step 2 (p. 17), where the reassignment of a point to a cluster is carried out based on proximity:

8 DISTANCE INSTABILITY

39

in particular, one way to implement Step 2 is to solve a 1-nearest neighbor problem. The training of an
ANN (Sect. 5.1.2) repeatedly solves a minimum distance subproblem in Eq. (10). In spectral clustering
(Sect. 5.2.1) we have a Euclidean norm constraint in Eq. (12). All DGP solution methods (Sect. 6),
with the exception of incidence vectors (Sect. 6.2.1), are concerned with distances by deﬁnition. PCA
(Sect. 7.1), in its interpretation of a modiﬁed MDS, can be seen as another solution method for the DGP.
Barvinok’s naive algorithm (Sect. 7.2) is a dimensional reduction method for SDPs the analysis of which
is based on a distance bound; moreover, it was successfully applied to the DGP [124]. The RP-based
methods discussed in Sect. 7.3 have all been derived from the JLL (Sect. 7.3.1), which is a statement
about the Euclidean distance. We also note that the focus of this survey is on typical DS problems, which
are usually high-dimensional.

It is therefore absolutely essential that all of these methods should be able to take robust decisions
based on comparing distance values computed on pairs of high-dimensional vectors. It turns out, however,
that smallest and largest distances Dmin, Dmax of a random point Z ∈ Rn to a set of random points
X1, . . . , X(cid:96) ⊂ Rn are almost equal (and hence, diﬃcult to compare) as n → ∞ under some reasonable
conditions. This holds for any distribution used to sample Z, Xi. This result, ﬁrst presented in [27] and
subsequently discussed in a number of papers [86, 3, 73, 64, 157, 128, 70], appears to jeopardize all of
the material presented in this survey, and much more beyond. The phenomenon leading to the result is
known as distance instability and concentration of distances.

8.1 Statement of the result

Let us look at the exact statement of the distance instability result.

First, we note that the points Z, X1, . . . , X(cid:96) are not given points in Rn but rather multivariate ran-
dom variables with n components, so distance instability is a purely statistical statement rather than a
geometric one. We consider

Z = (Z1, . . . , Zn)

∀i ≤ (cid:96) Xi = (Xi1, . . . , Xin),

where Z1, . . . , Zn are random variables with distribution D1; X11, . . . , X(cid:96)n are random variables with
distribution D2; and all of these random variables are independently distributed.

Secondly, Dmin, Dmax are functions of random variables:

Dmin = min{dist(Z, Xi) | i ≤ (cid:96)}
Dmax = max{dist(Z, Xi) | i ≤ (cid:96)},

(46)

(47)

and are therefore random variables themselves. In the above, dist denotes a function mapping pairs of
points in Rn to a non-negative real number, which makes distance instability a very general phenomenon.
Speciﬁcally, dist need not be a distance at all.

1 , Dm

2 , Dm

min, Dm

Third, we now label every symbol with an index m, which will be used to compute limits for m → ∞:
max, distm. We shall see that the proof of the distance instability result is
Z m, X m, Dm
wholly syntactical: its steps are very simple and follow from basic statistical results. In particular, we
can see m as an abstract parameter under which we shall take limits, and the proof will hold. Since the
proof holds independently of the value of n, it also holds if we assume that m = n, i.e. if we give m the
interpretation of dimensionality of the Euclidean space embedding the points. While this assumption is
not necessary for the proof to hold, it may simplify its understanding: m = n makes the proof somewhat
less general, but it gives the above indexing a more concrete meaning. Speciﬁcally, Z, X, D, D, dist are
points, distributions, extreme distance values and a distance function in dimension m, and the limit
m → ∞ is a limit taken on increasing dimension.

Fourth, the “reasonable conditions” referred to above for the distance instability result to hold are

8 DISTANCE INSTABILITY

that there is a constant p > 0 such that

∃i ≤ (cid:96)

lim
m→∞

Var

(cid:18) (dist(Z m, X m
E((dist(Z m, X m

i ))p
i ))p)

(cid:19)

= 0.

A few remarks on Eq. (48) are in order.

40

(48)

(a) The existential quantiﬁer simply encodes the fact that the Xi are all identically distributed, so a
statement involving variance and expectation of quantities depending on the Xi random variables
holds for all i ≤ (cid:96) if it holds for just one Xi.

(b) The constant p simply gives more generality to the result, but plays no role whatsoever in the proof;

it can be used in order to simplify computations when dist is an (cid:96)p norm.

(c) The fraction term in Eq. (48) measures a spread relative to an expectation. Requiring that the limit
of this relative spread goes to zero for increasing dimensions looks like an asymptotic concentra-
tion requirement (hence the alternative name “distance concentration” for the distance instability
phenomenon). Considering the eﬀect of concentration of measure phenomena in high dimensions
(Sect. 7.2.2), distance instability might now appear somewhat less surprising.

With these premises, we can state the distance instability result.

8.1 Theorem
min and Dm
If Dm

max are as in Eq. (46)-(47) and satisfy Eq. (48), then, for any ε > 0, we have

lim
m→∞

P (Dm

max ≤ (1 + ε)Dm

min) = 1.

(49)

Thm. 8.1 basically states that closest and farthest neighbors of Z are indistinguishable up to an ε. If
the closest and farthest are indistinguishable, trying to discriminate between the closest and the second
closest neighbors of a given point might well be hopeless due to ﬂoating point errors (note that this
discrimination occurs at each iteration of the well known k-means algorithm). This is why distance
instability is sometimes cited as a reason for convergence issues in k-means [75].

8.2 Related results

In [27], several scenarios are analyzed to see where distance instability occurs — even if some of the
requirement of distance instability are relaxed [27, §3.5] — and where it does not [27, §4]. Among
the cases where distance instability does not apply, we ﬁnd the case where the data points X are well
separated and the case where the dimensionality is implicitly low. Among the cases where it does apply,
we ﬁnd k-NN: in their experiments, the authors of [27] ﬁnd that k-NN becomes unstable already in the
range n ∈ {10, 20} dimensions. Obviously, the instability of k-NN propagates to any algorithm using
k-NN, such as k-means.

Among later studies, [86] proposes an alternative deﬁnition of dist where high-dimensional points are
projected into lower dimensional spaces. In [86], the authors study the impact of distance instability
on diﬀerent (cid:96)p norms, and concludes that smallest values of p lead to more stable norms; in particular,
quasinorms with 0 < p < 1 are considered. Some counterexamples are given against a generalization
of this claim for quasinorms in [73]. In [64], the converse of Thm. 8.1 is proved, namely that Eq. (48)
follows from Eq. (49): from this fact, the authors ﬁnd practically relevant cases where Eq. (48) is not
veriﬁed, and propose them as “good” examples of where k-means can help. In [128], the authors propose
multiplicative functions dist and show that they are robust w.r.t. distance instability. In [157], distance
instability is related to “hubness”, i.e. the number of times a point appears among the k nearest neighbors
of other points. In [70], an empirical study is provided which shows how to show an appropriate (cid:96)p norm
that should avoid distance instability w.r.t. hubness.

8 DISTANCE INSTABILITY

41

8.3 The proof

The proof of the instability theorem can be found in [27]. We repeat it here to demonstrate the fact
that it is “syntactical”: every step follows from the previous ones by simple logical inference. There is
no appeal to any results other than convergence in probability, Slutsky’s theorem, and a simple corollary
as shown below. The proof does not pass from object language to meta-language, nor does it require
exotic interpretations of symbols in complicated contexts. Although one may ﬁnd this result surprising,
there appears to be no reason to doubt it, and no complication in the proof warranting sophisticated
interpretations. The only point worth re-stating is that this is a result about probability distributions,
not about actual instances of real data.

8.2 Lemma
Let {Bm | m ∈ N} be a sequence of of random variables with ﬁnite variance. Assume that limm→∞ E(Bm) =
b and that limm→∞ Var(Bm) = 0. Then

∀ε > 0 lim
m→∞

P((cid:107)Bm − b(cid:107) ≤ ε) = 1.

(50)

A random variable sequence satisfying Eq. (50) is said to converge in probability to b. This is denoted
Bm →P b.

8.3 Lemma (Slutsky’s theorem [190])
Let {Bm | m ∈ N} be a sequence of random variables, and g : R → R be a continuous function.
Bm →P b and g(b) exists, then g(Bm) →P g(b).

If

8.4 Corollary
If {Am | m ∈ N} and {Bm | m ∈ N} are sequences of random variables such that Am →P a and
Bm →P b (cid:54)= 0, then Am

Bm →P

a
b .

Proof of Thm. 8.1. Let µm = E((dm(Z m, X m
identically distributed.

i ))p). We note that µm is independent of i since all X m

i are

We claim Vm = (dm(Zm,Xm

i ))p

µm

→P 1:

• we have E(Vm) = 1 since it is a random variable over its mean: hence, trivially, limm E(Vm) = 1;

• by the hypothesis of the theorem (Eq. (48)), limm Var(Vm) = 0;

• by Lemma 8.2, Vm →P 1, which establishes the claim.

Now, let Vm = (Vm | i ≤ (cid:96)). By the claim above, we have Vm →P 1. Now by Lemma 8.3 we obtain
min(Vm) →P min(1) = 1 and, similarly, max(Vm) →P 1. By Cor. 8.4, max(Vm)

min(Vm) →P 1. Therefore,

By deﬁnition of convergence in probability, we have

Dm
max
Dm
min

=

µm max(Vm)
µm min(Vm)

→P 1.

∀ε > 0

lim
m→∞

P(|Dm

max/Dm

min − 1| ≤ ε) = 1.

Moreover, since P(Dm

max ≥ Dm

min) = 1, we have

P(Dm

max ≤ (1 + ε)Dm

min) = P(Dm

max/Dm

min − 1 ≤ ε) = P(|Dm

max/Dm

min − 1| ≤ ε) = 1.

The result follows by taking the limit as m → ∞.

9 AN APPLICATION TO NEURAL NETWORKS

42

8.4 In practice

In Fig. 7, we show how ε (Eq. (49)) varies with increasing dimension n (recall we assume m = n) between
1 and 10000. It is clear that ε decreases very rapidly towards zero, and then reaches its asymptotic value

Figure 7: Plots of ε versus n for the uniform distribution on [0, 1] (left), N(0, 1) (center), the exponential
distribution with parameter 1 (right).

more slowly. On the other hand, ε is the distortion between minimum and maximum distance values;
most algorithms need to discriminate between smallest and second smallest distance values.

Most of the papers listed in Sect. 8.2 include empirical tests which illustrate the impact and limits of

the distance instability phenomenon.

9 An application to neural networks

In this last section we ﬁnally show how several concepts explained in this survey can be used conjunctively.
We shall consider a natural language processing task (Sect. 4) where we cluster some sentences (Sect. 5)
using an ANN (Sect. 5.1.2) with diﬀerent training sets T = (X, Y ). We compare ANN performances
depending on the training set used.

The input set X is a vector representation of the input sentences. The output set Y is a vectorial rep-

9 AN APPLICATION TO NEURAL NETWORKS

43

resentation of cluster labels: we experiment with (a) clusterings obtained by running k-means (Sect. 5.1.1)
on the input sets, and (b) a clustering found by a modularity maximization heuristic (Sect. 5.2.2). All
of these clusterings are considered “ground truth” sets Y we would like our ANN to learn to associate to
various types of input vector sets X representing the sentences. The sentences to be clustered are ﬁrst
transformed into graphs (Sect. 4.2), and then into vectors (Sect. 6), which then undergo dimensionality
reduction (Sect. 7).

Our goal is to compare the results obtained by the same ANN with diﬀerent vector representations
for the same text: most notably, the comparison will confront how well or poorly input vector sets can
predict the ground truth outputs. We will focus speciﬁcally on a comparison of the well-known incidence
vectors (Sect. 6.2.1) embeddings w.r.t. the newly proposed DGP methods we surveyed in Sect. 6.

In our implementations, all our code was developed using Python 3 [158].

9.1 Performance measure

We are going to measure the performance quality of the error of an ANN, which is based on a comparison
of its output with the ground truth that the ANN is supposed to learn. Using the notation of Sect. 5.1.2,
if the ANN output for a given input x ∈ Rn consists of a vector y ∈ Rk, and if the ground truth
corresponding to x is z ∈ Rk, then we deﬁne the error as the loss function:

loss(y, z) = (cid:107)y − z(cid:107)2.

(51)

An ANN N = (G, T, φ) is usually evaluated over many (input,output) pairs. Let ˆX ⊂ Rn and ˆY ⊂ Rk be,
respectively, a set of input vectors and the corresponding set of output vectors evaluated by the trained
ANN. Let ˆZ be a set of ground truth vectors corresponding to ˆX, and assume | ˆX| = | ˆY | = | ˆZ| = q. The
cumulative loss measure evaluated on the test set ( ˆX, ˆZ) is then

loss(N ) =

loss(y, z).

1
q

(cid:88)

y∈ ˆY
z∈ ˆZ

(52)

9.2 A Natural Language Processing task

Clustering of sentences in a text is a common task in Natural Language Processing. We considered “On
the duty of civil disobedience” by H.D. Thoreau [174, 184]. This text is stored in an ASCII ﬁle walden.txt
which can be obtained from archive.org. The ﬁle is 661146 bytes long, organized in 10108 lines and
116608 words. The text was parsed into sentences using basic methods from NLTK [28] under Python
3. Common words, stopwords, punctuation and unusual characters were removed. After “cleaning”, the
text was reduced to 4083 sentences over a set of 11431 “signiﬁcant” words (see Sect. 9.2.1).

As mentioned above, we want to train our ANN to learn diﬀerent types of clusterings:

• (k-means) obtained by running the the k-means unsupervised clustering algorithm (Sect. 5.1.1)

over the diﬀerent vector representations of the sentences in the text;

• (sentence graph) obtained by running a modularity clustering heuristic (Sect. 5.2.2) on a graph

representation of the sentences in the document (see Sect. 9.2.2).

These clusterings are used as ground truths, and provide the output part of the training sets to be used
by the ANN, as well as of the test sets for measuring purposes (Sect. 9.1). See Sect. 9.4.1 for more
information on the construction of these clusterings.

9 AN APPLICATION TO NEURAL NETWORKS

44

9.2.1 Selecting the sentences

We constructed two sets of sentences.

• The large sentence set. Each sentence in walden.txt was mapped to an incidence vector of
3-grams in {0, 1}48087, i.e. a dictionary of 48087 3-grams over the text.
In other words, 48087
3-grams were found in the text, then each sentence was mapped to a vector having 1 at component
i iﬀ the i-th 3-gram was present in the sentence. Since some sentences had fewer than 3 signiﬁcant
words, only 3940 sentences remained in the sentence set S, which was therefore represented as a
3940 × 48087 matrix ¯S with components in {0, 1}.

• The small sentence set. It turns out that most of the 3-grams in the set S only appear a single
time. We selected a subset S(cid:48) ⊂ S of sentences having 3-grams appearing in at least two sentences.
It turns out that |S(cid:48)| = 245, and the total number of 3-grams appearing more than once is 160. S(cid:48)
is therefore naturally represented as a 245 × 160 matrix ¯S(cid:48) with components in {0, 1}.

We constructed training sets (Sect. 9.4) for each of these two sets.

9.2.2 Construction of a sentence graph

In this section we describe the method used to construct a sentence graph Gs = (S, E) from the text,
which is used to produce a ground truth for the (sentence graph) type. Gs is then clustered using the
greedy modularity clustering heuristic in the Python library networkX [82].

Each sentence in the text is encoded into a weighted graph-of-word (see Sect. 4.2.1) over 3-grams,
with edges {u, v} weighted by the number cuv of 3-grams where the two words u, v appear. The union
of the graph-of-words for the sentences (contracting repeated words to a single vertex) yields a weighted
graph-of-word Gw for the whole text.

The graph Gw = (W, F ) is then “projected” onto the set S of sentences as follows. We deﬁne the
logical proposition P (u, v, s, t) to mean (u ∈ s ∧ v ∈ t) ∨ (v ∈ s ∧ u ∈ t) for words u, v and sentences s, t.
The edge set E of Gs is then deﬁned by the following implication:

∀{u, v} ∈ F, s, t ∈ S

P (u, v, s, t) → {s, t} ∈ E.

In other words, s, t form an edge in E if two words u, v in s, t (respectively) or t, s form an edge in F .
For each edge {s, t} ∈ E, the weight wst is given by:

wst =

(cid:88)

cuv,

{u,v}∈F
P (u,v,s,t)

with edge weights meaning similarity.

9.3 The ANN

We consider a very simple ANN N = (G, T, φ). In the terminology of Sect. 5.1.2, the underlying digraph
G = (V, A) is tripartite with V = V1 ˙∪V2 ˙∪V3. The “input layer” V1 has n nodes, where n is the dimen-
sionality of the input vector set X. The “output layer” V3 has a single node. The “hidden layer” V2 has a
constant number of nodes (20 in our experiments). The training set T is discussed in Sect. 9.4. We adopt
the piecewise-linear mapping known as rectiﬁed linear unit (ReLu) [189] for the activation functions φ in
V2, and a traditional sigmoid function for the single node in V3. Both types of activation functions map
into [0, 1].

9 AN APPLICATION TO NEURAL NETWORKS

45

We implemented N using the Python library keras [45], which is a high-level API running over
TensorFlow [1]. The default conﬁguration was chosen for all layers. We used the Adam solver [99] in
order to train the network. Each training set was split in three parts: 35% of the vectors were used
for training, 35% for validation (a training phase used for deciding values of any model parameter aside
from v, b, w, if any exist, and/or for deciding when to stop the training phase), and 30% for testing. The
performance of the ANN is measured using the loss function in Eq. (52).

9.4 Training sets

Our goal is to compare training sets T = (X, Y ) where the vectors in X were constructed in a variety of
ways, and the vectors in Y were obtained by running k-means (Sect. 5.1.1) on the vectors in X.

In particular, we consider input sets X(σ, µ, ρ) where:

• σ ∈ Σ = {S(cid:48), S} is the sentence set: σ = S(cid:48) corresponds to the small set with 245 sentences, σ = S

corresponds to the large set with 3940 sentences;

• µ ∈ M = {inc, uie, qrt, sdp} is the method used to map sentences to vectors: inc are the incidence
vectors (Sect. 6.2.1), uie is the universal isometric embedding (Sect. 6.2.2), qrt is the unconstrained
quartic (Sect. 6.1.1), sdp is the SDP (Sect. 6.1.3);

• ρ ∈ R = {pca, rp} is the dimensional reduction method used: pca is PCA (Sect. 7.1), rp are RPs

(Sect. 7.3).

The methods in M were all implemented using Python 3 with some well known external libraries
(e.g. numpy, scipy). Speciﬁcally, qrt was implemented using the Ipopt [48] NLP solver, and sdp was
implemented using the SCS [148] SDP solver. As for the dimensional reduction methods in R, the
PCA implementation of choice was the probabilistic PCA algorithm implemented in the Python library
scikit-learn [150]. The RPs we chose were the simplest: each component of the RP matrices was
sampled from an appropriately scaled zero-mean Gaussian distribution (Thm. 7.1).

9.4.1 The output set

The output set Y should naturally contain discrete values, namely the labels of the h clusters {1, 2, . . . , h}
in the ground truth clusterings. We map these values to scalars in [0, 1] (or, according to Sect. 5.1.2, to
k-dimensional vectors with k = 1) as follows. We divide the range [0, 1] into h − 1 equal sub-intervals of
length 1/(h − 1), and hence h discrete values in [0, 1]. Then we assign labels to sub-intervals endpoints:
label j is mapped to (j − 1)/(h − 1) (for 1 ≤ j ≤ h).

As mentioned above, we consider two types of output sets:

• (k-means) for each input set X(σ, µ, ρ) we obtained an output set Y (σ, µ, ρ) using k-means
(Sect. 5.1.1) implementation in scikit-learn [150] on the vectors in X, for each sentence set
σ ∈ Σ, method µ ∈ M , and dimensional reduction method ρ ∈ R;

• (sentence graph) for each sentence set σ ∈ Σ we constructed a sentence graph as detailed in

Sect. 9.2.2.

9.4.2 Realizations to vectors

The inc method (Sect. 6.2.1) is the only one (in our benchmark) that can natively map sentences of
various lengths into vectors all having the same number of components.

9 AN APPLICATION TO NEURAL NETWORKS

46

For all other methods in M (cid:114) {inc}, we loop over sentences (in small/large sets S(cid:48), S). For each
sentence we construct its graph-of-words (Sect. 4.2.1). We then realize it in some arbitrary dimensional
Euclidean space RK (speciﬁcally, we chose K = 10) using uie, qrt, sdp. At this point, we are confronted
with the following diﬃculty: a realization of a graph G with p vertices in RK is a p × K matrix, and we
have as many graphs G as we have sentences, with p varying over the number of unique words in the
sentences (i.e. the cardinalities of the vertex sets of the graphs-of-words).

In order to reduce all of these diﬀerently-sized realizations to vectors having the same dimension, we
follow the following procedure. Given realizations {xi ∈ Rpi×K | i ∈ σ}, where σ is the set of sentences
(for σ ∈ Σ) and xi realizes the graph-of-word of sentence i ∈ σ,

1. we stack the columns of xi so as to obtain a single vector ˆxi ∈ RpiK for each i ∈ σ;

2. we let ˆn = maxi piK be the maximum dimensionality of the stacked realizations;

3. we pad every realization vector ˆxi shorter than ˆn with zeros to achieve dimension ˆn for stacked

realization vectors;

4. we form the s × ˆn matrix ˆX having ˆxi as its i-th row (for i ∈ σ and with s = |σ|);

5. we reduce the dimensionality of ˆX to an s × n matrix X with pca or rp.

9.5 Computational comparison

We discuss the details of our training sets, a validation test, and the comparison tests.

9.5.1 Training set statistics

In Table 3 we report the dimensionalities of the vectors in the input parts X(σ, µ, ρ) of the training sets,
as well as the number of clusters in the output sets Y (σ, µ, ρ) of the (k-means) class. We recall that

µ

ρ
pca
rp
original

pca
rp

inc
3
100
160

4
4

|σ| = 3940

Dimensionality of input vectors
|σ| = 245
qrt
uie
244
159
248
248
1140
1140

sdp
200
248
1140
Number of clusters to learn
3
3

inc
3
373
48087

11
7

6
5

3
3

uie
10
373
1460

qrt
400
373
1460

sdp
400
373
1460

8
9

9
16

14
14

Table 3: Training set statistics for X(σ, µ, ρ) and corresponding output sets in the (k-means) class.

the number of clusters was found with k-means in the scikit-learn implementation. The choice of
‘k’ corresponds to the smallest number of clusters giving a nontrivial clustering (with “trivial” meaning
having a cluster of zero cardinality, or too close to zero relative to the set size, only possibly allowing
some outlier clusters with a single element). Some more remarks follow.

• For ρ = pca we employed the smallest dimension such that the residual variance in the neglected
components was almost zero; this ranges from 3 to 244 in Table 3. For the two cases where the
dimensionality reduction was set to 400 (qrt and sdp in the large sentence set S), the residual
variance was nonzero.

10 CONCLUSION

47

• It is interesting that for µ = uie we have higher projected dimensionality (248) in the small set S(cid:48)
than in the large set S (10): this depends on the fact that the large set has more easily distinguish-
able clusters (8 found by k-means) than the small set (only 3 found by k-means). The dimension
of X(inc, pca, S) is smaller (3) than that of X(uie, pca, S) (10) even though the original number of
dimensions of the former (48087) vastly exceeds that of the latter (1460) for the same reason.

• The training sets X(σ, inc, pca) are the smallest-dimensional ones (for σ ∈ {S(cid:48), S}): they are also
“degenerate”, in the sense that the vectors in a given clusters are all equal; the co-occurrence
patterns of the incidence vectors conveyed relatively little information to this vectorial sentence
representation.

• The RP-based dimensionality reduction method yields the same dimensionality (373) of X(µ, rp, S)
for µ ∈ M . This occurs because the target dimensionality in RP depends on the number of vectors,
which is the same for all methods (3940), rather than on the number of dimensions (see Sect. 7.3).

There is one output set in the (sentence graph) class for each σ ∈ Σ. For σ = S(cid:48) we have |V | = 245,
|E| = 28519, and 230 clusters, with the ﬁrst 5 clusters having 6, 5, 4, 3, 2 elements, and the rest having
a single element. For σ = S we have |V | = 3940, |E| = 7173633, and 3402 clusters, with the ﬁrst 10
clusters having 161, 115, 62, 38, 34, 29, 19, 16, 14, 11 elements, and the rest having fewer than 10 elements.

9.5.2 Comparison tests

We ﬁrst report the comparative results of the ANN on

T = (X(σ, µ1, ρ1), Y (σ, µ2, ρ2))

for σ ∈ Σ, µ1, µ2 ∈ M , ρ1, ρ2 ∈ R. The sums in the rightmost columns of Table 4 are only carried out on
terms obtained with an input vector generation method µ1 diﬀerent from the method µ2 used to obtain
the ground truth clustering via k-means (since we want to compare methods). The results corresponding
to cases where µ1 = µ2 are emphasized in italics in the table. The best performance sums are emphasized
in boldface, and the worst are shown in grey.

According to Table 4, for the small sentence set the best method is inc, but qrt and sdp are not far
behind; the only really imprecise method is uie. For the large sentence set the best method is qrt, with
sdp not far behind; both inc, uie are imprecise.

In Table 5, which has a similar format as Table 4, we report results on training sets

¯T = (X(σ, µ, ρ), ¯Y (σ))

for σ ∈ Σ, µ ∈ M , ρ ∈ R, where ¯Y (σ) are output sets of the (sentence graph) class. For the small
set, inc is the best method (independently of ρ), with (µ = sdp, ρ = pca) following very closely, and,
in general, sdp and qrt still being acceptable; uie is the most imprecise method. For the large set inc is
againt the best method, with (µ = sdp, ρ = rp) following closely. While the other method do not excel,
the performance diﬀerence between all methods is less remarkable than with the small set.

10 Conclusion

We have surveyed some of the concepts and methodologies of distance geometry which are used in data
science. More speciﬁcally, we have looked at algorithms (mostly based on mathematical programming)
for representing graphs as vectors as a pre-processing step to performing some machine learning task
requiring vectorial input.

10 CONCLUSION

48

s
t
u
p
n

i

t
e
s

i

g
n
n
i
a
r
T

Training set outputs

inc
pca

inc
rp

uie
pca

uie
rp

0.061

0.042

0.059

0.013

qrt
pca
245
0.094

qrt
rp

sdp
pca

sdp
rp

sum
µ(cid:48) (cid:54)= µ

0.108

0.064

0.025

0.363

0.005

0.010

0.055

0.015

0.104

0.109

0.065

0.025

0.271

0.052

0.070

0.169

0.233

0.201

0.127

0.111

0.093

0.026

0.094

0.076

0.191

0.236

0.079

0.117

0.082

0.057

0.067

0.068

0.105

0.059

0.047

0.084

0.133

0.053

0.162

0.073

0.071

0.095

0.087

0.055

0.106

0.063

0.067

0.022

0.106

0.135

0.058

0.034

0.095

0.065

0.093

0.021

0.052

0.013

0.068

0.027

0.103
3940
0.106

0.139

0.074

0.018

0.164

0.079

0.161

0.001

0.000

0.067

0.028

0.106

0.167

0.080

0.159

0.063

0.022

0.020

0.016

0.124

0.201

0.070

0.127

0.061

0.023

0.024

0.023

0.131

0.190

0.072

0.126

0.063

0.062

0.022

0.024

0.36

0.023

0.038

0.218

0.047

0.025

0.120

0.035

0.079

0.076

0.159

0.164

0.063

0.021

0.023

0.024

0.126

0.195

0.033

0.149

0.059

0.021

0.025

0.024

0.121

0.176

0.083

0.037

0.373

0.995

0.976

0.459

0.387

0.499

0.516

0.605

0.607

0.607

0.603

0.382

0.398

0.452

0.426

µ
ρ
|σ|

inc
pca
inc
rp
uie
pca
uie
rp
qrt
pca
qrt
rp
sdp
pca
sdp
rp

|σ|

inc
pca
inc
rp
uie
pca
uie
rp
qrt
pca
qrt
rp
sdp
pca
sdp
rp

Table 4: Comparison tests on output sets of (k-means) class.

µ
ρ
|σ|

|σ|

s
t
u
p
n

i

i

g
n
n
i
a
r
T

inc
pca

inc
rp

Training set outputs
qrt
uie
uie
pca
rp
pca

245

qrt
rp

sdp
pca

sdp
rp

0.107

0.108

0.196

0.184

0.129

0.151

0.109

0.122

0.097

0.098

0.124

0.119

0.136

0.113

0.114

0.106

3940

Table 5: Comparison tests on output sets of (sentence graph) class.

We started with brief introductions to mathematical programming and distance geometry. We then
showed some ways to represent data by graphs, and introduced clustering on vectors and graphs. Follow-
ing, we surveyed robust algorithms for realizing weighted graphs in Euclidean spaces, where the robustness
is with respect to errors or noise in the input data. It turns out that most of these algorithms are based
on mathematical programming. Since some of these algorithms output high-dimensional vectors and/or
high-rank matrices, we also surveyed some dimensional reduction techniques. We also discussed a result
about the instability of distances with respect to randomly generated points.

The guiding idea in this survey is that, when one is confronted with clustering on graphs, then distance
geometry allows the use of many supervised and unsupervised clustering techniques based on vectors. To
demonstrate the applicability of this idea, we showed that vectorial representations of graphs obtained
using distance geometry oﬀer competitive performances when training an artiﬁcial neural network. While
we do not think that our limited empirical analysis allows any deﬁnite conclusion, we hope that it will
entice more research in this area.

REFERENCES

Acknowledgements

49

I am grateful to J.J. Salazar, the Editor-in-Chief of TOP, for inviting me to write this survey. This
work would not have been possible without the numerous co-authors with whom I pursued my investi-
gations in distance geometry, among which I will single out the longest-standing: C. Lavor, N. Maculan,
I have ﬁrst heard of concentration of measure as I passed by D. Malioutov’s oﬃce at
A. Mucherino.
the T.J. Watson IBM Research laboratory: the door was open, the Johnson-Lindenstrauss lemma was
mentioned, and I could not refrain from interrupting the conversation and asking for clariﬁcation, as I
thought it must be a mistake; incredibly, it was not, and I am grateful to Dmitry Malioutov for hosting
the conversation I eavesdropped on. I very thankful to the co-authors who helped me investigate random
projections, in particular P.L. Poirion and K. Vu, without whom none of our papers would have been
possible. I learned about the existence of the distance instability result thanks to N. Gayraud, who sug-
gested it to me as I expressed puzzlement at the poor quality of k-means clusterings during my talk. I am
very grateful to S. Khalife and M. Escobar for reading the manuscript and making insightful comments.

References

[1] M. Abadi and et al. TensorFlow: Large-scale machine learning on heterogeneous systems. http:

//tensorflow.org/, 2015. Software available from tensorﬂow.org.

[2] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins.

Journal of Computer and System Sciences, 66:671–687, 2003.

[3] C. Aggarwal, A. Hinneburg, and D. Keim. On the surprising behavior of distance metrics in high
dimensional space. In J. Van den Bussche and V. Vianu, editors, Proceedings of ICDT, volume
1973 of LNCS, pages 420–434, Berlin, 2001. Springer.

[4] A. Ahmadi, R. Jungers, P. Parrilo, and M. Roozbehani. Joint spectral radius and path-complete

graph Lyapunov functions. SIAM Journal on Optimization and Control, to appear.

[5] A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: More tractable alternatives to
sum of squares and semideﬁnite optimization. SIAM Journal on Applied Algebra and Geometry,
3(2):193–230, 2019.

[6] N. Ailon and B. Chazelle. Approximate nearest neighbors and fast Johnson-Lindenstrauss lemma.
In Proceedings of the Symposium on the Theory Of Computing, volume 06 of STOC, Seattle, 2006.
ACM.

[7] A. Alfakih, A. Khandani, and H. Wolkowicz. Solving Euclidean distance matrix completion prob-
lems via semideﬁnite programming. Computational Optimization and Applications, 12:13–30, 1999.

[8] G. Allen. Sparse higher-order principal components analysis. In N. Lawrence and M. Girolami, edi-
tors, Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 22
of Proceedings of Machine Learning Research, pages 27–36, La Palma, 2012. PMLR.

[9] Z. Allen-Zhu, R. Gelashvili, S. Micali, and N. Shavit. Sparse sign-consistent Johnson-Lindenstrauss
matrices: Compression with neuroscience-based constraints. Proceedings of the National Academy
of Sciences, 111(47):16872–16876, 2014.

[10] D. Aloise, S. Caﬁeri, G. Caporossi, P. Hansen, S. Perron, and L. Liberti. Column generation
algorithms for exact modularity maximization in networks. Physical Review E, 82(4):046112, 2010.

[11] D. Aloise, G. Caporossi, P. Hansen, L. Liberti, S. Perron, and M. Ruiz. Modularity maximization
in networks by variable neighbourhood search. In D. Bader, P. Sanders, and D. Wagner, editors,
Graph Partitioning and Graph Clustering, volume 588 of Contemporary Mathematics, pages 113–
127. American Mathematical Society, Providence, RI, 2013.

REFERENCES

50

[12] D. Aloise, P. Hansen, and L. Liberti. An improved column generation algorithm for minimum

sum-of-squares clustering. Mathematical Programming A, 131:195–220, 2012.

[13] E. Amaldi, L. Liberti, F. Maﬃoli, and N. Maculan. Edge-swapping algorithms for the minimum
fundamental cycle basis problem. Mathematical Methods of Operations Research, 69:205–223, 2009.

[14] J. Anderson. An introduction to neural networks. MIT Press, Cambridge, MA, 1995.

[15] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random

projection. Machine Learning, 63:161–182, 2006.

[16] L. Asimow and B. Roth. The rigidity of graphs. Transactions of the American Mathematical

Society, 245:279–289, 1978.

[17] A. Bahr, J. Leonard, and M. Fallon. Cooperative localization for autonomous underwater vehicles.

International Journal of Robotics Research, 28(6):714–728, 2009.

[18] G. Barker and D. Carlson. Cones of diagonally dominant matrices. Paciﬁc Journal of Mathematics,

57(1):15–32, 1975.

[19] A. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete

and Computational Geometry, 13:189–202, 1995.

[20] A. Barvinok. Measure concentration in optimization. Mathematical Programming, 79:33–53, 1997.

[21] A. Barvinok. A Course in Convexity. Number 54 in Graduate Studies in Mathematics. American

Mathematical Society, Providence, RI, 2002.

[22] N. Beeker, S. Gaubert, C. Glusa, and L. Liberti. Is the distance geometry problem in NP? In
A. Mucherino, C. Lavor, L. Liberti, and N. Maculan, editors, Distance Geometry: Theory, Methods,
and Applications, pages 85–94. Springer, New York, 2013.

[23] P. Belotti, J. Lee, L. Liberti, F. Margot, and A. W¨achter. Branching and bounds tightening
techniques for non-convex MINLP. Optimization Methods and Software, 24(4):597–634, 2009.

[24] Eleazar ben Judah of Worms. Sodei razayya, XII-XIII Century. [Earliest account of how to create

a Golem].

[25] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press,

Princeton, NJ, 2009.

[26] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep net-
works. In Advances in Neural Information Processing Systems, volume 19 of NIPS, pages 153–160,
Cambridge, MA, 2007. MIT Press.

[27] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. When is “nearest neighbor” meaningful?
In C. Beeri and P. Buneman, editors, Proceedings of ICDT, volume 1540 of LNCS, pages 217–235,
Heidelberg, 1998. Springer.

[28] S. Bird, E. Klein, and E. Loper. Natural Language Processing with Python. O’Reilly, Cambridge,

2009.

[29] J. Birge and F. Louveaux. Introduction to Stochastic Programming. Springer, New York, 2011.

[30] J. Bl¨omer, C. Lammersen, M. Schmidt, and C. Sohler. Theoretical analysis of the k-means algo-
rithm: A survey. In L. Kliemann and P. Sanders, editors, Algorithm Engineering, volume 9220 of
LNCS, pages 81–116, Cham, 2016. Springer.

[31] L. Blumenthal. Theory and Applications of Distance Geometry. Oxford University Press, Oxford,

1953.

REFERENCES

51

[32] C. B¨ohm and G. Jacopini. Flow diagrams, turing machines and languages with only two formation

rules. Communications of the ACM, 9(5):366–371, 1966.

[33] B. Bollob´as. Modern Graph Theory. Springer, New York, 1998.

[34] I. Borg and P. Groenen. Modern Multidimensional Scaling. Springer, New York, second edition,

2010.

[35] L. Bottou. Stochastic gradient descent tricks. In G. Montavon and et al., editors, Neural Networks:

Tricks of the trade, volume 7700 of LNCS, pages 421–436, Berlin, 2012. Springer.

[36] J. Bourgain. On Lipschitz embeddings of ﬁnite metric spaces in Hilbert space. Israel Journal of

Mathematics, 52(1-2):46–52, 1985.

[37] C. Boutsidis, A. Zouzias, and P. Drineas. Random projections for k-means clustering. In Advances
in Neural Information Processing Systems, NIPS, pages 298–306, La Jolla, 2010. NIPS Foundation.

[38] A. Brambilla and A. Premoli. Rigorous event-driven (red) analysis of large-scale nonlinear rc
circuits. IEEE Transactions on Circuits and Systems–I: Fundamental Theory and Applications,
48(8):938–946, August 2001.

[39] U. Brandes, D. Delling, M. Gaertler, R. G¨orke, M. Hoefer, Z. Nikoloski, and D. Wagner. On
modularity clustering. IEEE Transactions on Knowledge and Data Engineering, 20(2):172–188,
2008.

[40] S. Caﬁeri, P. Hansen, and L. Liberti. Loops and multiple edges in modularity maximization of

networks. Physical Review E, 81(4):46102, 2010.

[41] S. Caﬁeri, P. Hansen, and L. Liberti. Locally optimal heuristic for modularity maximization of

networks. Physical Review E, 83(056105):1–8, 2011.

[42] S. Caﬁeri, P. Hansen, and L. Liberti. Improving heuristics for network modularity maximization

using an exact algorithm. Discrete Applied Mathematics, 163:65–72, 2014.

[43] A.-L. Cauchy. Sur les polygones et les poly`edres. Journal de l’ ´Ecole Polytechnique, 16(9):87–99,

1813.

[44] A. Cayley. A theorem in the geometry of position. Cambridge Mathematical Journal, II:267–271,

1841.

[45] F. Chollet and et al. Keras. https://keras.io, 2015.

[46] N. Chomsky. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA, 1965.

[47] A. Choromanska, M. Henaﬀ, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of
multilayer networks. In Proceedings of the International Conference on Artiﬁcial Intelligence and
Statistics, volume 18 of AISTATS, San Diego, 2015. JMLR.

[48] COIN-OR. Introduction to IPOPT: A tutorial for downloading, installing, and using IPOPT, 2006.

[49] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural Language
Processing (almost) from scratch. Journal of Machine Learning Research, 12:2461–2505, 2011.

[50] R. Connelly. A counterexample to the rigidity conjecture for polyhedra. Publications Math´ematiques

de l’IHES, 47:333–338, 1978.

[51] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall, Boca Raton, 2001.

[52] C. D’Ambrosio and L. Liberti. Distance geometry in linearizable norms. In F. Nielsen and F. Bar-
baresco, editors, Geometric Science of Information, volume 10589 of LNCS, pages 830–838, Berlin,
2017. Springer.

REFERENCES

52

[53] C. D’Ambrosio, L. Liberti, P.-L. Poirion, and K. Vu. Random projections for quadratic program-

ming. Technical Report 2019-7-7322, Optimization Online, 2019.

[54] G. Dantzig. Reminiscences about the origins of linear programming. In A. Bachem, M. Gr¨otschel,
and B. Korte, editors, Mathematical Programming: the state of the art. Springer, Berlin, 1983.

[55] S. Dasgupta and A. Gupta. An elementary proof of a theorem by Johnson and Lindenstrauss.

Random Structures and Algorithms, 22:60–65, 2002.

[56] A. D’Aspremont, F. Bach, and L. El Ghaoui. Approximation bounds for sparse principal component

analysis. Mathematical Programming B, 148:89–110, 2014.

[57] J. Dattorro. Convex Optimization and Euclidean Distance Geometry. M(cid:15)βoo, Palo Alto, 2015.

[58] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking
In Advances in Neural

the saddle point problem in high-dimensional non-convex optimization.
Information Processing Systems, NIPS, pages 2933–2941, La Jolla, 2014. NIPS Foundation.

[59] P. Demartines and J. H´erault. Curvilinear component analysis: A self-organizing neural network

for nonlinear mapping of data sets. IEEE Transactions on Neural Networks, 8(1):148–154, 1997.

[60] N. Deo, G.M. Prabhu, and M.S. Krishnamoorthy. Algorithms for generating fundamental cycles in

a graph. ACM Transactions on Mathematical Software, 8(1):26–42, March 1982.

[61] S. Dey, R. Mazumder, M. Molinaro, and G. Wang. Sparse principal component analysis and its

(cid:96)1-relaxation. Technical Report 1712.00800v1, arXiv, 2017.

[62] G. Dias and L. Liberti. Diagonally dominant programming in distance geometry. In R. Cerulli,
S. Fujishige, and R. Mahjoub, editors, International Symposium in Combinatorial Optimization,
volume 9849 of LNCS, pages 225–236, New York, 2016. Springer.

[63] I. Douven. Abduction. In E. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics

Research Lab, Stanford University, summer 2017 edition, 2017.

[64] R. Durrant and A. Kab´an. When is ‘nearest neighbour’ meaningful: A converse theorem and

implications. Journal of Complexity, 25:385–397, 2009.

[65] U. Eco. Horns, hooves, insteps. Some hypotheses on three kinds of abduction. In U. Eco and T. Se-
beok, editors, Dupin, Holmes, Peirce. The Sign of Three. Indiana University Press, Bloomington,
1983.

[66] U. Eco. Semiotics and the Philosophy of Language. Indiana University Press, Bloomington, IN,

1984.

[67] T. Eren, D. Goldenberg, W. Whiteley, Y. Yang, A. Morse, B. Anderson, and P. Belhumeur. Rigidity,

computation, and randomization in network localization. IEEE, pages 2673–2684, 2004.

[68] L. Euler. Continuatio fragmentorum ex adversariis mathematicis depromptorum: II Geometria,
97. In P. Fuss and N. Fuss, editors, Opera postuma mathematica et physica anno 1844 detecta,
volume I, pages 494–496. Eggers & C., Petropolis, 1862.

[69] M. Fiedler. Algebraic connectivity of graphs. Czechoslovak Mathematical Journal, 23(2):298–305,

1973.

[70] A. Flexer and D. Schnitzer. Choosing (cid:96)p norms in high-dimensional spaces based on hub analysis.

Neurocomputing, 169:281–287, 2015.

[71] D. Floreano. Manuale sulle Reti Neurali. Il Mulino, Bologna, 1996.

[72] S. Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75–174, 2010.

REFERENCES

53

[73] D. Fran¸cois, V. Wertz, and M. Verleysen. The concentration of fractional distances. IEEE Trans-

actions on Knowledge and Data Engineering, 19(7):873–886, 2007.

[74] F. Friedler, Y. Huang, and L. Fan. Combinatorial algorithms for process synthesis. Computers and

Chemical Engineering, 16(1):313–320, 1992.

[75] N. Gayraud. Public remark, 2017. During the workshop Le Monde des Math´ematiques Industrielles

at INRIA Sophia-Antipolis (MOMI17).

[76] F. Gilbreth and L. Gilbreth. Process charts: First steps in ﬁnding the one best way to do work.
In Proceedings of the Annual Meeting, New York, Dec. 1921. American Society of Mechanical
Engineers.

[77] P.E. Gill. User’s guide for SNOPT version 7.2. Systems Optimization Laboratory, Stanford Uni-

versity, California, 2006.

[78] K. G¨odel. On the isometric embeddability of quadruples of points of r3 in the surface of a sphere.
In S. Feferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van Heijenoort, editors, Kurt
G¨odel: Collected Works, vol. I, pages (1933b) 276–279. Oxford University Press, Oxford, 1986.

[79] D. Gon¸calves, A. Mucherino, C. Lavor, and L. Liberti. Recent advances on the interval distance

geometry problem. Journal of Global Optimization, accepted.

[80] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA, 2016.

[81] B. Haeﬀele and R. Vidal. Global optimality in neural network training.

In Proceedings of the
conference in Computer Vision and Pattern Recognition, CVPR, pages 4390–4398, Piscataway,
2017. IEEE.

[82] A. Hagberg, D. Schult, and P. Swart. Exploring network structure, dynamics, and function using
NetworkX. In G. Varoquaux, T. Vaught, and J. Millman, editors, Proceedings of the 7th Python in
Science Conference (SciPy2008), pages 11–15, Pasadena, CA, 2008.

[83] P. Hansen and B. Jaumard. Cluster analysis and mathematical programming. Mathematical Pro-

gramming, 79:191–215, 1997.

[84] L. Henneberg. Die Graphische Statik der starren Systeme. Teubner, Leipzig, 1911.

[85] Heron. Metrica, volume I. Alexandria, ∼50AD.

[86] A. Hinneburg, C. Aggarwal, and D. Keim. What is the nearest neighbor in high dimensional spaces?
In Proceedings of the Conference on Very Large Databases, volume 26 of VLDB, pages 506–515,
San Francisco, 2000. Morgan Kaufman.

[87] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of

Educational Psychology, 24(6):417–441, 1933.

[88] IBM. ILOG CPLEX 12.8 User’s Manual. IBM, 2017.

[89] P. Indyk. Algorithmic applications of low-distortion geometric embeddings.

In Foundations of

Computer Science, volume 42 of FOCS, pages 10–33, Washington, DC, 2001. IEEE.

[90] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the Symposium on the Theory Of Computing, volume 30 of STOC, pages
604–613, New York, 1998. ACM.

[91] P. Indyk and A. Naor. Nearest neighbor preserving embeddings. ACM Transactions on Algorithms,

3(3):Art. 31, 2007.

[92] A. Jain, M. Murty, and P. Flynn. Data clustering: a review. ACM Computing Surveys, 31(3):264–

323, 1999.

REFERENCES

54

[93] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.

In
G. Hedlund, editor, Conference in Modern Analysis and Probability, volume 26 of Contemporary
Mathematics, pages 189–206, Providence, RI, 1984. American Mathematical Society.

[94] I. Jolliﬀe. Principal Component Analysis. Springer, Berlin, 2nd edition, 2010.

[95] M. Jordan. Why the logistic function? a tutorial discussion probabilities and neural networks.

Technical Report Computational Cognitive Science TR 9503, MIT, 1995.

[96] D. Kane and J. Nelson. Sparser Johnson-Lindenstrauss transforms. Journal of the ACM, 61(1):4,

2014.

[97] I. Kantor, J. Matouˇsek, and R. ˇS´amal. Mathematics++: Selected topics beyond the basic courses.
Number 75 in Student Mathematical Library. American Mathematical Society, Providence, RI,
2015.

[98] S. Khalife, L. Liberti, and M. Vazirgiannis. Geometry and analogies: a study and propagation
method for word representation. In Statistical Language and Speech Processing, volume 7 of SLSP,
2019.

[99] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of ICLR, San

Diego, 2015.

[100] D.E. Knuth. The Art of Computer Programming, Part I: Fundamental Algorithms. Addison-Wesley,

Reading, MA, 3rd edition, 1997.

[101] S. Kullback and R. Leibler. On information and suﬃciency. Annals of Mathematical Statistics,

22(1):79–86, 1951.

[102] C. Kuratowski. Quelques probl`emes concernant les espaces m´etriques non-s´eparables. Fundamenta

Mathematicæ, 25:534–545, 1935.

[103] C. Lavor, L. Liberti, and N. Maculan. Computational experience with the molecular distance
In J. Pint´er, editor, Global Optimization: Scientiﬁc and Engineering Case

geometry problem.
Studies, pages 213–225. Springer, Berlin, 2006.

[104] C. Lavor, L. Liberti, N. Maculan, and A. Mucherino. The discretizable molecular distance geometry

problem. Computational Optimization and Applications, 52:115–146, 2012.

[105] C. Lavor, L. Liberti, and A. Mucherino. The interval Branch-and-Prune algorithm for the discretiz-
able molecular distance geometry problem with inexact distances. Journal of Global Optimization,
56:855–871, 2013.

[106] S. Lehmann and L. Hansen. Deterministic modularity optimization. European Physical Journal B,

60:83–88, 2007.

[107] R. Levine, T. Mason, and D. Brown. Lex and Yacc. O’Reilly, Cambridge, second edition, 1995.

[108] L. Liberti. Reformulations in mathematical programming: Deﬁnitions and systematics. RAIRO-

RO, 43(1):55–86, 2009.

[109] L. Liberti.

Software modelling and architecture:

Exercises.

Ecole Polytechnique,

www.lix.polytechnique.fr/~liberti/swarchex.pdf, 2010.

[110] L. Liberti. Undecidability and hardness in mixed-integer nonlinear programming. RAIRO-

Operations Research, 53:81–109, 2019.

[111] L. Liberti, S. Caﬁeri, and D. Savourey. Reformulation optimization software engine. In K. Fukuda,
J. van der Hoeven, M. Joswig, and N. Takayama, editors, Mathematical Software, volume 6327 of
LNCS, pages 303–314, New York, 2010. Springer.

REFERENCES

55

[112] L. Liberti, S. Caﬁeri, and F. Tarissan. Reformulations in mathematical programming: A com-
In A. Abraham, A.-E. Hassanien, P. Siarry, and A. Engelbrecht, editors,
putational approach.
Foundations of Computational Intelligence Vol. 3, number 203 in Studies in Computational Intel-
ligence, pages 153–234. Springer, Berlin, 2009.

[113] L. Liberti and C. D’Ambrosio. The Isomap algorithm in distance geometry.

In C. Iliopoulos,
S. Pissis, S. Puglisi, and R. Raman, editors, Proceedings of 16th International Symposium on Exper-
imental Algorithms (SEA), volume 75 of LIPICS, pages 5:1–5:13, Schloss Dagstuhl, 2017. Dagstuhl
Publishing.

[114] L. Liberti and C. Lavor. Six mathematical gems in the history of distance geometry. International

Transactions in Operational Research, 23:897–920, 2016.

[115] L. Liberti and C. Lavor. Euclidean Distance Geometry: An Introduction. Springer, New York,

2017.

[116] L. Liberti, C. Lavor, J. Alencar, and G. Abud. Counting the number of solutions of kDMDGP
In F. Nielsen and F. Barbaresco, editors, Geometric Science of Information, volume

instances.
8085 of LNCS, pages 224–230, New York, 2013. Springer.

[117] L. Liberti, C. Lavor, and N. Maculan. A branch-and-prune algorithm for the molecular distance

geometry problem. International Transactions in Operational Research, 15:1–17, 2008.

[118] L. Liberti, C. Lavor, N. Maculan, and A. Mucherino. Euclidean distance geometry and applications.

SIAM Review, 56(1):3–69, 2014.

[119] L. Liberti, C. Lavor, and A. Mucherino. The discretizable molecular distance geometry problem
seems easier on proteins. In A. Mucherino, C. Lavor, L. Liberti, and N. Maculan, editors, Distance
Geometry: Theory, Methods, and Applications, pages 47–60. Springer, New York, 2013.

[120] L. Liberti, C. Lavor, A. Mucherino, and N. Maculan. Molecular distance geometry methods: from
continuous to discrete. International Transactions in Operational Research, 18:33–51, 2010.

[121] L. Liberti and F. Marinelli. Mathematical programming: Turing completeness and applications to

software analysis. Journal of Combinatorial Optimization, 28(1):82–104, 2014.

[122] L. Liberti, B. Masson, C. Lavor, J. Lee, and A. Mucherino. On the number of realizations of
certain Henneberg graphs arising in protein conformation. Discrete Applied Mathematics, 165:213–
232, 2014.

[123] L. Liberti, G. Swirszcz, and C. Lavor. Distance geometry on the sphere. In J. Akiyama and et al.,

editors, JCDCG2, volume 9943 of LNCS, pages 204–215, New York, 2016. Springer.

[124] L. Liberti and K. Vu. Barvinok’s naive algorithm in distance geometry. Operations Research Letters,

46:476–481, 2018.

[125] N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and some of its algorithmic

applications. Combinatorica, 15(2):215–245, 1995.

[126] A. Majumdar, A. Ahmadi, and R. Tedrake. Control and veriﬁcation of high-dimensional systems
In Conference on Decision and Control, volume 53, pages

with dsos and sdsos programming.
394–401, Piscataway, 2014. IEEE.

[127] C. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. MIT Press,

Cambridge, MA, 1999.

[128] J. Mansouri and M. Khademi. Multiplicative distance: a method to alleviate distance instability

for high-dimensional data. Knowledge and Information Systems, 45:783–805, 2015.

[129] J. Matouˇsek. On variants of the Johnson-Lindenstrauss lemma. Random Structures and Algorithms,

33:142–156, 2008.

REFERENCES

56

[130] J. Matouˇsek. Lecture notes on metric embeddings. Technical report, ETH Z¨urich, 2013.

[131] J. Maxwell. On the calculation of the equilibrium and stiﬀness of frames. Philosophical Magazine,

27(182):294–299, 1864.

[132] G.P. McCormick. Computability of global solutions to factorable nonconvex programs: Part I —

Convex underestimating problems. Mathematical Programming, 10:146–175, 1976.

[133] W. McCulloch. What is a number, that a man may know it, and a man, that he may know a

number? General Semantics Bulletin, 26-27:7–18, 1961.

[134] L. Mencarelli, Y. Sahraoui, and L. Liberti. A multiplicative weights update algorithm for MINLP.

EURO Journal on Computational Optimization, 5:31–86, 2017.

[135] K. Menger. Untersuchungen ¨uber allgemeine Metrik. Mathematische Annalen, 100:75–163, 1928.

[136] K. Menger. New foundation of Euclidean geometry. American Journal of Mathematics, 53(4):721–

745, 1931.

[137] R. Merris. Laplacian matrices of graphs: A survey. Linear Algebra and its Applications, 198:143–

176, 1994.

[138] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words
and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26 of NIPS,
pages 3111–3119, La Jolla, 2013. NIPS Foundation.

[139] G. Miller. Wordnet: A lexical database for English. Communications of the ACM, 38(11):39–41,

1995.

[140] J. Milnor. On the Betti numbers of real varieties. Proceedings of the American Mathematical

Society, 15:275–280, 1964.

[141] M. Minsky. The society of mind. Simon & Schuster, New York, 1986.

[142] A. Moitra. Algorithmic aspects of Machine Learning. Cambridge University Press, Cambridge,

2018.

[143] A. Moro. The boundaries of Babel. MIT Press, Cambridge, MA, 2008.

[144] C. Morris. Signs, Language and Behavior. Prentice-Hall, New York, 1946.

[145] A. Mucherino, C. Lavor, and L. Liberti. Exploiting symmetry properties of the discretizable
molecular distance geometry problem. Journal of Bioinformatics and Computational Biology,
10:1242009(1–15), 2012.

[146] M. Newman and M. Girvan. Finding and evaluating community structure in networks. Physical

Review E, 69:026113, 2004.

[147] Object Management Group. Uniﬁed modelling language: Superstructure, v. 2.0. Technical Report

formal/05-07-04, OMG, 2005.

[148] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Operator splitting for conic optimization via
homogeneous self-dual embedding. Journal of Optimization Theory and Applications, 169(3):1042–
1068, 2016.

[149] K. Paton. An algorithm for ﬁnding a fundamental set of cycles of a graph. Communications of the

ACM, 12(9):514–518, 1969.

REFERENCES

57

[150] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011.

[151] C. Peirce.

Illustrations of the logic of science, part 6:

Induction, Deduction, and Hypothesis.

Popular Science Monthly, 13:470–482, 1878.

[152] R. Penrose. The emperor’s new mind. Penguin, New York, 1989.

[153] A. Pfeﬀer. Practical Probabilistic Programming. Manning Publications, Shelter Island, NY, 2016.

[154] K. Popper. The Logic of Scientiﬁc Discovery. Hutchinson, London, 1968.

[155] F. Potra and S. Wright. Interior-point methods. Journal of Computational and Applied Mathemat-

ics, 124:281–302, 2000.

[156] G. Proni. Is there abduction in Aristotle? Peirce, Eco, and some further remarks. Ocula, 17:1–14,

2016.

[157] M. Radovanovi´c, A. Nanopoulos, and M. Ivanovi´c. Hubs in space: Popular nearest neighbors in

high-dimensional data. Journal of Machine Learning Research, 11:2487–2531, 2010.

[158] G. Van Rossum and et al. Python Language Reference, version 3. Python Software Foundation,

2019.

[159] F. Rousseau and M. Vazirgiannis. Graph-of-word and TW-IDF: new approach to ad hoc IR. In

Proceedings of CIKM, New York, 2013. ACM.

[160] M. Saerens, F. Fouss, L. Yen, and P. Dupont. The principal components analysis of a graph, and its
relationships to spectral clustering. In J.-F. Boulicaut, F. Esposito, F. Giannotti, and D. Pedreschi,
editors, Proceedings of the European Conference in Machine Learning (ECML), volume 3201 of
LNAI, pages 371–383, Berlin, 2004. Springer.

[161] E. Salgado, A. Scozzari, F. Tardella, and L. Liberti. Alternating current optimal power ﬂow with
generator selection. In J. Lee, G. Rinaldi, and R. Mahjoub, editors, Combinatorial Optimization
(Proceedings of ISCO 2018), volume 10856 of LNCS, pages 364–375, 2018.

[162] J. Saxe. Embeddability of weighted graphs in k-space is strongly NP-hard. Proceedings of 17th

Allerton Conference in Communications, Control and Computing, pages 480–489, 1979.

[163] S. Schaeﬀer. Graph clustering. Computer Science Review, 1:27–64, 2007.

[164] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117,

2015. Published online 2014; based on TR arXiv:1404.7828 [cs.NE].

[165] I. Schoenberg. Remarks to Maurice Fr´echet’s article “Sur la d´eﬁnition axiomatique d’une classe
d’espaces distanci´es vectoriellement applicable sur l’espace de Hilbert”. Annals of Mathematics,
36(3):724–732, 1935.

[166] M. Schumacher, R. Roßner, and W. Vach. Neural networks and logistic regression: Part I. Com-

putational Statistics & Data Analysis, 21:661–682, 1996.

[167] S. Seshu and M.B. Reed. Linear Graphs and Electrical Networks. Addison-Wesley, Reading, MA,

1961.

[168] A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Applied and

Computational Harmonic Analysis, 30:20–36, 2011.

[169] E. Smith and C. Pantelides. A symbolic reformulation/spatial branch-and-bound algorithm for the
global optimisation of nonconvex MINLPs. Computers & Chemical Engineering, 23:457–478, 1999.

REFERENCES

58

[170] H. Steinhaus. Sur la division des corps mat´eriels en parties. Bulletin de l’Acad´emie Polonaise des

Sciences Cl. III, 4(12):801–804, 1956.

[171] P. Tabaghi, I. Dokmani´c, and M. Vetterli. On the move: Localization with kinetic Euclidean dis-
tance matrices. In International Conference on Acoustics, Speech and Signal Processing (ICASSP),
Piscataway, 2019. IEEE.

[172] M. Tawarmalani and N.V. Sahinidis. Global optimization of mixed integer nonlinear programs: A

theoretical and computational study. Mathematical Programming, 99:563–591, 2004.

[173] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimen-

sionality reduction. Science, 290:2319–2322, 2000.

[174] H. Thoreau. Resistance to civil government. In E. Peabody, editor, Æsthetic papers. J. Wilson,

Boston, MA, 1849.

[175] S.A. Vavasis. Nonlinear Optimization: Complexity Issues. Oxford University Press, Oxford, 1991.

[176] S. Vempala. The Random Projection Method. Number 65 in DIMACS Series in Discrete Mathe-
matics and Theoretical Computer Science. American Mathematical Society, Providence, RI, 2004.

[177] S. Venkatasubramanian and Q. Wang. The Johnson-Lindenstrauss transform: An empirical study.
In Algorithm Engineering and Experiments, volume 13 of ALENEX, pages 164–173, Providence,
RI, 2011. SIAM.

[178] A. Verboon. The medieval tree of Porphyry: An organic structure of logic.

In A. Worm and
P. Salonis, editors, The Tree. Symbol, Allegory and Structural Device in Medieval Art and Thought,
volume 20 of International Medieval Research, pages 83–101, Turnhout, 2014. Brepols.

[179] R. Vershynin. High-dimensional geometry. Cambridge University Press, Cambridge, 2018.

[180] R. Vidal, Y. Ma, and S. Sastry. Generalized Principal Component Analysis. Springer, New York,

2016.

[181] K. Vu, P.-L. Poirion, C. D’Ambrosio, and L. Liberti. Random projections for quadratic programs
In A. Lodi and et al., editors, Integer Programming and Combinatorial

over a Euclidean ball.
Optimization (IPCO), volume 11480 of LNCS, pages 442–452, New York, 2019. Springer.

[182] K. Vu, P.-L. Poirion, and L. Liberti. Random projections for linear programming. Mathematics of

Operations Research, 43(4):1051–1071, 2018.

[183] K. Vu, P.-L. Poirion, and L. Liberti. Gaussian random projections for euclidean membership

problems. Discrete Applied Mathematics, 253:93–102, 2019.

[184] Wikipedia. Civil disobedience (thoreau), 2019. [Online; accessed 190804].

[185] Wikipedia. Computational pragmatics, 2019. [Online; accessed 190802].

[186] Wikipedia. Diagonally dominant matrix, 2019. [Online; accessed 190716].

[187] Wikipedia. Flowchart, 2019. [Online; accessed 190802].

[188] Wikipedia. Principal component analysis, 2019. [Online; accessed 190726].

[189] Wikipedia. Rectiﬁer (neurl networks), 2019. [Online; accessed 190807].

[190] Wikipedia. Slutsky’s theorem, 2019. [Online; accessed 190802].

[191] H.P. Williams. Model Building in Mathematical Programming. Wiley, Chichester, 4th edition, 1999.

[192] D. Woodruﬀ. Sketching as a tool for linear algebra. Foundations and Trends in Theoretical Com-

puter Science, 10(1-2):1–157, 2014.

REFERENCES

59

[193] K. W¨uthrich. Protein structure determination in solution by nuclear magnetic resonance spec-

troscopy. Science, 243:45–50, 1989.

[194] G. Xu, S. Tsoka, and L. Papageorgiou. Finding community structures in complex networks using

mixed integer optimisation. European Physical Journal B, 60:231–239, 2007.

[195] Y. Yemini. The positioning problem — a draft of an intermediate summary.

In Proceedings of
the Conference on Distributed Sensor Networks, pages 137–145, Pittsburgh, 1978. Carnegie-Mellon
University.

[196] Y. Yemini. Some theoretical aspects of position-location problems.

In Proceedings of the 20th
Annual Symposium on the Foundations of Computer Science, pages 1–8, Piscataway, 1979. IEEE.

[197] C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks.

In
Proceedings of the 6th International Conference on Learning Representations, La Jolla, CA, 2018.
ICLR.

[198] L. Zhang, M. Mahdavi, R. Jin, T. Yang, and S. Zhu. Recovering the optimal solution by dual
random projection. In S. Shalev-Shwartz and I. Steinwart, editors, Conference on Learning Theory
(COLT), volume 30 of Proceedings of Machine Learning Research, pages 135–157. jmlr.org, 2013.

