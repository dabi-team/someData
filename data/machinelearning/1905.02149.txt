9
1
0
2

y
a
M
8
2

]
S
D
.
s
c
[

2
v
9
4
1
2
0
.
5
0
9
1
:
v
i
X
r
a

Eﬃcient Second-Order Shape-Constrained
Function Fitting∗

David Durfee†

Yu Gao†

Anup B. Rao‡

Sebastian Wild§

May 30, 2019

We give an algorithm to compute a one-dimensional shape-constrained function
that best ﬁts given data in weighted-L∞ norm. We give a single algorithm that
works for a variety of commonly studied shape constraints including monotonic-
ity, Lipschitz-continuity and convexity, and more generally, any shape constraint
expressible by bounds on ﬁrst- and/or second-order diﬀerences. Our algorithm
computes an approximation with additive error ε in O(cid:0)n log U
(cid:1) time, where U
ε
captures the range of input values. We also give a simple greedy algorithm that
runs in O(n) time for the special case of unweighted L∞ convex regression. These
are the ﬁrst (near-)linear-time algorithms for second-order-constrained function
ﬁtting. To achieve these results, we use a novel geometric interpretation of the
underlying dynamic programming problem. We further show that a generalization
of the corresponding problems to directed acyclic graphs (DAGs) is as diﬃcult as
linear programming.

1. Introduction

We consider the fundamental problem of ﬁnding a function f that approximates a given set of
data points (x1, y1), . . . , (xn, yn) in the plane with smallest possible error, i.e., f (xi) shall be
close to yi (formalized below), subject to shape constraints on the allowable functions f , such
as being increasing and/or concave. More speciﬁcally, we present a new algorithm that can
handle arbitrary constraints on the (discrete) ﬁrst- and second-order derivatives of f .

When we only require f to be weakly increasing, the problem is known as isotonic regression,
a classic problem in statistics; (see, e.g., [13] for history and applications). It has more recently
also found uses in machine learning [17, 16, 12].

In certain applications, further shape restrictions are integral part of the model: For
example, microeconomic theory suggests that production functions are weakly increasing and
concave (modeling diminishing marginal returns); similar reasoning applies to utility functions.
Restricting f to functions with bounded derivative (Lipschitz-continuous functions) is desirable
to avoid overﬁtting [16]. All these shape restrictions can be expressed by inequalities for

∗The ﬁrst author is supported in part by National Science Foundation Grant 1718533. The last author is
supported by the Natural Sciences and Engineering Research Council of Canada and the Canada Research
Chairs Programme.

†Georgia Institute of Technology
‡Adobe Research
§University of Waterloo

·

anuprao @ adobe.com

· {

wild @ uwaterloo.ca

ddurfee,ygao380

@ gatech.edu

}

·

 
 
 
 
 
 
2

Eﬃcient Second-Order Shape-Constrained Function Fitting

ﬁrst and second derivatives of f ; their discretized equivalents are hence amenable to our new
method. Shape restrictions that we cannot directly handle are studied in [28] (f is piecewise
constant and the number of breakpoints is to be minimized) and [26] (unimodal f ). For a more
comprehensive survey of shape-constrained function-ﬁtting problems and their applications,
see [14, §1]. Motivated by these applications, the problems have been studied in statistics (as a
form of nonparametric regression), investigating, e.g., their consistency as estimators and their
rate of convergence [13, 14, 4].

While fast algorithms for isotonic-regression variants have been designed [27], both [22]
and [3] list shape constraints beyond monotonicity as important challenges. For example,
ﬁtting (multidimensional) convex functions is mostly done via quadratic or linear programming
solvers [24]. In his PhD thesis, Balzs writes that current “methods are computationally too
expensive for practical use, [so] their analysis is used for the design of a heuristic training
algorithm which is empirically evaluated” [4, p. 1].

This lack of eﬃcient algorithms motivated the present work. Despite a few limitations
discussed below (implying that we do not yet solve Balzs’ problem), we give the ﬁrst near-linear-
time algorithms for any function-ﬁtting problem with second-order shape constraints (such
as convexity). We use dynamic programming (DP) with a novel geometric encoding for the
“states”. Simpler versions of such geometric DP variants were used for isotonic regression [25]
and are well-known in the competitive programming community; incorporating second-order
constraints eﬃciently is our main innovation.

Problem deﬁnition. Given the vectors x = (x1, . . . , xn) ∈ Rn and y ∈ Rn, an error norm
d and shape constraints (formalized below), compute f = (f1, . . . , fn) satisfying the shape
constraints with minimal d(f , y), i.e., we represent f via its values fi = f (xi) at the given points.
i |xi −yi|p(cid:1)1/p; least squares (p = 2) dominate in statistics,
d is usually an Lp norm, d(x, y) = (cid:0)(cid:80)
but more general error functions have been studied for isotonic regression [23, 19, 22, 3]. We
will consider the weighted L∞ norm, i.e., d(f , y) = maxi∈[n] wi|fi − yi|, where [n] = {1, . . . , n}
and w ∈ Rn

≥0 is a given vector of weights.

Since we are dealing with discretized functions (a vector f ), restrictions for derivatives f (cid:48)

and f (cid:48)(cid:48) have to be discretized, as well. We deﬁne local slope and curvature as

f (cid:48)
i =

fi − fi−1
xi − xi−1

,

(i ∈ [2..n]),

and f (cid:48)(cid:48)

i =

f (cid:48)
i − f (cid:48)
i−1
xi − xi−1

,

(i ∈ [3..n]);

the shape constraints are then given in the form of vectors f (cid:48)−, f (cid:48)+, f (cid:48)(cid:48)−, f (cid:48)(cid:48)+ of bounds for
the ﬁrst- and second-order diﬀerences, i.e., we deﬁne the set of feasible answers as F = (cid:8)f ∈
Rn (cid:12)
(cid:12) f (cid:48)− ≤ f (cid:48) ≤ f (cid:48)+ ∧ f (cid:48)(cid:48)− ≤ f (cid:48)(cid:48) ≤ f (cid:48)(cid:48)+(cid:9) where inequalities on vectors mean the inequality on
all components. The weighted-L∞ function-ﬁtting problem with second-order shape constraints
is then to ﬁnd

f ∗ = arg min

(cid:18)

f ∈F

max
i

wi · |fi − yi|

(cid:19)
.

(1)

Often, we only need a lower resp. upper bound; we can achieve that by allowing −∞ and +∞
entries in f (cid:48)±
. For example, setting f (cid:48)(cid:48)− = 0, f (cid:48)− = f (cid:48)(cid:48)− = +∞ and f (cid:48)− = −∞, we
can enforce a convex function/vector. We also consider the decision-version of the problem:
given a bound L, decide if there is an f ∈ F with maxi wi|fi − yi| ≤ L, and if so, report one.

and f (cid:48)(cid:48)±

i

i

Contributions. Our main result is a single O(n)-time algorithm for the decision problem
of function ﬁtting with second-order constraints; see Theorem 1.2 for the precise statement.

1. Introduction

3

With binary search, this readily yields an additive ε-approximation for (1), and thus weighted
L∞ isotonic regression, convex regression and Lipschitz convex regression, in O(cid:0)n log U
(cid:1) time
(Theorem 1.4), where U = (maxi wi) · (maxi yi − mini yi). In the appendix, we give a simple
greedy algorithm (see Theorem A.1) for unweighted (w = 1) L∞ convex regression that runs in
O(n) time. Finally, we show that a generalization of the problem to DAGs (where the applied
ﬁrst- and second-order diﬀerence constraints are restricted by the graph), is as hard as linear
programming, see Appendix D.

ε

Related work. Stout [27] surveys algorithms for various versions of isotonic regression; they
achieve near-linear or even linear time for many error metrics. He also considers the gener-
alization to any partial order (instead of the total order corresponding to weakly increasing
functions). A related task is to ﬁt a piecewise-constant function (with a prescribed number of
jumps) to given data. [9, 10] solve this problem for L∞ in optimal O(n log n) time. Since the
geometric constraints are much easier than in our case, a simple greedy algorithm suﬃces to
solve the decision version.

For more restricted shapes, less is known.

[26] gives a O(n log n) solution for unimodal
regression. [1] gives an O(n log n) algorithm for unweighted L2 Lipschitz isotonic regression
and a O(n poly(log n)) time algorithm for Lipschitz unimodal regression. [24] describes (mul-
tidimensional) L2 convex regression algorithms based quadratic programming. Feﬀerman [8]
studied a closely related problem of smooth interpolation of data in Euclidean space minimizing
a certain norm deﬁned on the derivatives of the function. His setup is much more general, but
his algorithm cannot ﬁnd arbitrarily good interpolations (ε is ﬁxed for the algorithm). All fast
algorithms above consider classes deﬁned by constraints on the ﬁrst derivative only, not the
second derivative as needed for convexity. To our knowledge, the fastest prior solution for any
convex regression problem is solving a linear program, which will imply super-linear time.

We use a geometric interpretation of dynamic-programming states and represent them
implicitly. The work closest in spirit to ours is a recent article by Rote [25]; establishing the
transformation of states is much more complicated in the presently studied problem, though.
Implicitly representing a series of more complicated objects using data structures has been used
in geometric and graph algorithms, such as multiple-source shortest paths [18] and shortest
paths in polygons [5, 21, 7]. The only other work (we know of) that interprets dynamic
programming geometrically is [28].

There is a rich literature on methods for speeding up dynamic programming [29, 30, 6,
11]. They involve a variety of powerful techniques such as monotonicity of transition points,
quadrangle inequalities, and Monge matrix searching [2], many of which have found applications
in other settings. The focus of these methods is to reduce the (average) number of transitions
that a state is involved in, often from O(n) to O(1). Therefore, their running times are lower
bounded by the number of states in the dynamic programs.

1.1. Results

We formally state our theorem for the decision problem here; results for shape-constrained
function ﬁtting are obtained as corollaries. For our algorithm, the discrete derivatives (as deﬁned
above) are inconvenient because they involve the x-distance between points. We therefore
normalize all x-distances to 1 (s. t. xi = i); for the second-order constraints, this normalization
makes the introduction of an additional parameter necessary, the scaling factors αi (see below).

Deﬁnition 1.1 (1st/2nd-diﬀ-constrained vectors): Let n-dimensional vectors x− ≤ x+
(value bounds), y− ≤ y+ (diﬀerence bounds), z− ≤ z+ (second-order diﬀerence bounds), and

4

Eﬃcient Second-Order Shape-Constrained Function Fitting

α > 0 be given. We deﬁne S ⊂ Rn to be the set of all b ∈ Rn that satisfy the following
constraints:

∀i ∈ [1..n] x−
i ≤ bi ≤ x+
i
y−
i ≤ bi − bi−1 ≤ y+
∀i ∈ [2..n]
i
z−
i ≤ (bi − bi−1) − αi(bi−1 − bi−2) ≤ z+
i

∀i ∈ [3..n]

(value constraints)

(ﬁrst-order constr.)

(second-order constr.)

Moreover, we consider the “truncated problems” Sk, where Sk is the set of all b ∈ Rn that
satisfy the constraints up to k (instead of n).

A visualization of an example is shown in Figure 1. We can encode an instance (x, y, f (cid:48)±, f (cid:48)(cid:48)±)
of the decision version of the weighted-L∞ function-ﬁtting problem with second-order constraints
as 1st/2nd-diﬀ-constrained vectors by setting

x±
i = yi ± L/wi,
z±
i = f (cid:48)(cid:48)± · (xi − xi−1)2,

y±
i = f (cid:48)± · (xi − xi−1),
xi − xi−1
xi−1 − xi−2

αi =

.

So, our goal is to eﬃciently compute some b ∈ S or determine that S = ∅. Our core technical
result is a linear-time algorithm for this task:

Theorem 1.2 (1st/2nd-diﬀ-constrained decision): With the notation of Deﬁnition 1.1,
in O(n) time, we can compute b ∈ S or determine that S = ∅.

Section 2 will be devoted to the proof. To simplify the presentation, we will assume throughout
that x+, x−, y+, y−, z+, z− are bounded.1 For the optimization version of the problem,
Equation (1), we consider approximate solutions in the following sense.

Deﬁnition 1.3 (ε-approximation): We call f ∈ F an ε-approximate solution to the
weighted L∞ function-ﬁtting problem if it satisﬁes

max
i

wi|fi − yi| ≤ min
g∈F

max
i

wi|gi − yi|

+ ε.

(cid:18)

(cid:19)

By a simple binary search on L, we can ﬁnd approximate solutions.

Theorem 1.4 (Main result): There exists an algorithm that computes an ε-approximate
solution to the weighted-L∞ convex regression problem that runs in O(n log U
ε ) time, where
U = (maxi wi)(maxi yi − mini yi). The same holds true for isotonic regression, Lipschitz isotonic
regression, convex isotonic regression.

Proof: We will argue for the case of convex regression here, other cases are similar. Abbreviate
L(f ) = maxi wi|fi − yi|. For a given L, the decision version of convex regression can be solved
in O(n) time using Theorem 1.2. That is, in O(n) time, we can either ﬁnd f ∈ F such that
L(f ) ≤ L or conclude that for all f ∈ F, L(f ) > L. If we know an L0 for which there exists
f ∈ F with L(f ) ≤ L0, then we can do a binary search for Lc in [0, L0]. We can easily ﬁnd such
an L0 for the convex case: Let f = min yj be constant (hence convex). For this f , we have
L(f ) ≤ (maxj wj)(maxj yj −minj yj). Therefore, we can take L0 = (maxj wj)(maxj yj −minj yj)
(cid:3)
and the result immediately follows.

1Some problems are stated with

values, but we can always replace unbounded values in the algorithms

with an (input-speciﬁc) suﬃciently large ﬁnite number.

±∞

1. Introduction

5

Figure 1: Exemplary input for the 1st/2nd-diﬀ-constrained decision problem with α = 1. Value
constraints are illustrated as blue bars. First-order constraints are shown as green circles,
indicating the allowable incoming angles/slopes; the green dot and the circle can be
moved up and down within the blue range. Finally, second-order constraints are given
as red triangles, in which the minimal and maximal allowable change in slope is shown
(dotted red), based oﬀ an exemplary incoming slope (dashed red). The thin dotted line
shows b = (1.7, 1.2, 2.2, 2.8, 3.3) ∈ S.
Below the visualization of the instance, we show the set of pairs (bi, bi − bi−1) for
b ∈ Si, i.e., solutions of the truncated problem; the speciﬁc solution is shown as a dot.
These sets are the feasibility polygons Pi (deﬁned in Section 2.1) that play a vital role
in our algorithm. Given all Pi, one can easily construct a solution backwards, starting
from any point in P5.

12345ix−1=1.5x+1=3x−2=0.4x+2=2x−3=1.7x+3=2.2x−4=2.5x+4=4x−5=2x+5=3.3y−2=−.5y+2=1y−3=0y+3=1y−4=−1y+4=10y−5=−1y+5=0.5z−3=−.5z+3=1.5z−4=−.8z+4=−.2z−5=−.5z+5=.5∆=0.5∆∆=−0.3∆=0.8∆∆=−0.2∆=1∆∆=1.3∆=−0.3(1,−0.5)(2,−0.5)(2,0.5)P2(1.7,0)(2,0)(2.2,0.2)(2.2,1)(2,1)(1.7,0.7)P3(2.5,0.3)(3,0.8)(2.8,0.8)(2.5,.65)P4(2.3,−0.2)(3.3,0.3)(3.3,0.5)(3,0.5)P56

Eﬃcient Second-Order Shape-Constrained Function Fitting

We note that for the speciﬁc case of unweighted convex function ﬁtting, there is a simpler
linear-time greedy algorithm; we give more details on that in Appendix A. This algorithm was
the initial motivation for studying this problem and for the geometric approach we use. For
more general settings, in particular second-order diﬀerences that are allowed to be both positive
and negative, the greedy approach does not work; our generic algorithm, however, is almost as
simple and eﬃcient.

2. First- and second-order diﬀerence-constrained vectors

In this section, we present our main algorithm and prove Theorem 1.2. In Section 2.1, we give an
overview and introduce the feasibility polygons Pi. Section 2.2 shows how Pi can be inductively
computed from Pi−1 via a geometric transformation. We ﬁnally show how this transformation
can be computed eﬃciently, culminating in the proof of Theorem 1.2, in Section 2.3. Two
proofs are deferred to Appendix B and C.

2.1. Overview of the algorithm

Recall that the problem we want to solve, in order to prove Theorem 1.2, is ﬁnding a feasible
point b in S from Deﬁnition 1.1. Our algorithm will use dynamic programming (DP) where each
state is associated with the feasible bi in the truncated problem. We will iteratively determine
all bi such that bi is the ith entry of some b ∈ Si.

Feasible bi have to respect the ﬁrst- and second-order diﬀerence constraints. To check
those, we also need to know the possible pairs (bi−1, bi−2) of (i − 1)th and (i − 2)th entries
for some b ∈ Si−1, so the states have to maintain more information than the bi alone. It will
be instrumental to rewrite this pair as (bi−1, bi−1 − bi−2), the combination of valid values bi−1
and valid slopes at which we entered bi−1 for a solution in Si−1. From that, we can determine
the valid slopes at which we can leave bi−1 using our shape constraints. We thus deﬁne the
feasibility polygons

Pi = (cid:8)(x, y) (cid:12)

(cid:12) ∃b ∈ Si : x = bi ∧ y = bi − bi−1

(cid:9)

(2)

for i = 2, . . . , n. See Figure 1 for an example. We view each point in Pi as a “state” in our
DP algorithm, and our goal becomes to eﬃciently compute Pi from Pi−1. The key observation
is that each Pi is indeed an O(n)-vertex convex polygon, and we only need an eﬃcient way
to compute the vertices of Pi from those of Pi−1. This needs a clever representation, though,
since all vertices can change when going from Pi−1 to Pi. A closer look reveals that we can
represent the vertex transformations implicitly, without actually updating each vertex, and we
can combine subsequent transformations into a single one. More speciﬁcally, if we consider the
boundary of Pi−1, the transformation to Pi consists of two steps: (1) a linear transformation
for the upper and lower hull of Pi−1, and (2) a truncation of the resulting polygon by vertical
and horizontal lines (i.e., an intersection of the polygon and a half-plane).

The ﬁrst step requires a more involved proof and uses that all line segments of Pi have
weakly positive slope (“+sloped”, formally deﬁned below). Implicitly computing the ﬁrst
transformation as we move between Pi is straightforward, only requiring a composition of linear
operations (a diﬀerent one, though, for upper and lower hull). We can apply the cumulative
transformation whenever we need to access a vertex.

The second step is conceptually simpler, but more diﬃcult to implement eﬃciently, as we
have to determine where a line cuts the polygon in amortized constant time. For this operation,
we separately store the vertices of the upper and lower hull of Pi in two arrays, sorted by

2. First- and second-order diﬀerence-constrained vectors

7

increasing x-coordinate; since Pi is +sloped, y-values are also increasing. A linear search for
intersections has overall O(n) cost since we can charge individual searches to deleted vertices.
Finally, if Pn (cid:54)= ∅, we compute a feasible vector b backwards, starting from any point in
Pn. Since we do not explicitly store the Pi, this requires successively “undoing” all operations
(going from Pi back to Pi−1); see Appendix C for details.

2.2. Transformation from state Pi−1 to Pi
We ﬁrst deﬁne the structural property “+sloped” that our method relies on.

Deﬁnition 2.1 (+sloped): We say a polygon P ⊆ R2 with vertices v1, . . . , vk is +sloped if
slope(vi, vj) ≥ 0 for all edges (vi, vj) of P . Here, the slope between two points v1 = (x1, y1),
v2 = (x2, y2) ∈ R2 is deﬁned as slope(v1, v2) = y2−y1
, when x1 (cid:54)= x2, and slope(v1, v2) = ∞,
x2−x1
otherwise.

We will now show that Pi can be computed by applying a simple geometric transformation to
Pi−1. In passing, we will prove (by induction on i) that all Pi are +sloped. For the base case,
note that P2 = {(b2, b2 − b1) | x−
1 ∧ x−
2 }, which is
an intersection of 6 half-planes. The slopes of the deﬁning inequalities are all non-negative or
inﬁnite, so P2 is +sloped.

2 ≤ b2 − b1 ≤ y+

1 ≤ b1 ≤ x+

2 ≤ b2 ≤ x+

2 ∧ y−

Let us now assume that Pi−1, i ≥ 3, is +sloped; we will consider the transformation from
Pi−1 to Pi and show that it preserves this property. We begin by separating the transformation
from Pi−1 to Pi into two main steps.

Step 1: Second-order constraint only. For the ﬁrst step, we ignore the value and ﬁrst-order
constraints at index i. This will yield a convex polygon, P (z)
, that contains Pi; in Step 2, we
will add the other constraints at i to obtain Pi itself.
Deﬁnition 2.2 (P (z)
lem with x−
polygon Pi of this modiﬁed problem (considering only the zi constraints at i).

: 2nd-order-only polygons): For a ﬁxed i, consider the modiﬁed prob-
, as the

i = ∞. Deﬁne the second-order-only polygon, P (z)

i = −∞ and x+

i , y+

i , y−

i

i

i

The statement of the following lemma is very simple observation, but allows us to compute
P (z)
from Pi−1 with an explicit geometric construction, (whereas such seemed not obvious for
i
the original feasibility polygons).

i

Lemma 2.3 (P (z)
i = (cid:8)(x + αiy + z, αiy + z) | (x, y) ∈ Pi−1, z ∈ [z−
P (z)
Proof: The only constraint at i is z−
i . We rewrite this
as (a) a constraint for bi − bi−1, using that bi−1 − bi−2 is the y-coordinate in Pi−1, and (b) a
(cid:3)
constraint for bi, using that, additionally, bi−1 is the x-coordinate in Pi−1.

: scaled, sheared and shifted Pi−1):
i ](cid:9).
i ≤ (bi − bi−1) − αi(bi−1 − bi−2) ≤ z+

i , z+

Once we have computed this polygon P (z)
, computing Pi is easy: adding the constraints
x−
i ≤ x ≤ x+
i with two horizontal and vertical lines.
We give a visual representation of the mapping on an example in Figure 2. We break the above
mapping into two simpler stages:

i requires only cutting P (z)

i ≤ y ≤ y+

i and y−

i

i

: sheared and shifted P αi

Corollary 2.4 (P (z)
Setting P αi
i = (cid:8)(x + y + z, y + z) | (x, y) ∈ P αi
P (z)
We note that scaling the y-coordinate by αi preserves the +sloped-property:

i−1 = {(x, αiy) | (x, y) ∈ Pi−1}, we have
i−1, z ∈ [z−

i−1):
i ](cid:9).

i , z+

8

Eﬃcient Second-Order Shape-Constrained Function Fitting

Figure 2: The transformation from P3 to P4 for the example instance of Figure 1. Upper and

lower hull are shown separately in green resp. red.

Lemma 2.5: Let α ≥ 0. If P is +sloped, so is P α = {(x, αy) | (x, y ∈ P )}.

Proof: Scaling the y-coordinates will preserve all of the vertices of P , and also scale the slope
of each vertex pair by α ≥ 0. So, P α is +sloped.
(cid:3)

That leaves us with the core of the transformation, from P αi
. Intuitively, it can be
viewed as sliding P αi
i ] and taking the union
thereof, (see Figure 2). To compute the result of this operation, we split the boundary into
upper and lower hull.

i−1 along the line x = y by any amount z ∈ [z−

i−1 to P (z)
i
i , z+

Deﬁnition 2.6 (Upper/lower hull): Let P be a convex polygon with vertex set V . We
deﬁne the upper hull (vertices) resp. lower hull (vertices) of P as

u-hull(P ) = (cid:8)ui = (xi, yi) ∈ V (cid:12)
l-hull(P ) = (cid:8)ui = (xi, yi) ∈ V (cid:12)

(cid:12) (cid:64)(xi, y) ∈ P : y > yi
(cid:12) (cid:64)(xi, y) ∈ P : y < yi

(cid:9)

(cid:9)

Unless speciﬁed otherwise, hull vertices are ordered by increasing x-coordinate.

Note that a vertex can be in both hulls. Moreover, the leftmost vertices in u-hull(P ) and
l-hull(P ) always have the same x-coordinate, similarly for the rightmost vertices. As proved in
Lemma 2.3, each point in P αi
i−1 is mapped to a line-segment with slope 1; we give this mapping
a name.

(1.7,0)(2,0)(2.2,0.2)(2.2,1)(2,1)(1.7,0.7)P3scaley(1.7,0)(2,0)(2.2,0.2)(2.2,1)(2,1)(1.7,0.7)Pα43horizontalshear(1.7,0)(2,0)(2.4,0.2)(3.2,1)(3,1)(2.4,0.7)slidealongx=yz−4=−0.8z+4=−0.2(0.9,−0.8)(1.2,−0.8)(1.6,−0.6)(3,0.8)(2.8,0.8)(2.2,0.5)P(z)4(2.5,0.3)(3,0.8)(2.8,0.8)(2.5,0.65)P4x−4=2.5x+4=4y−4=−1y+4=10intersectw/halfplanes2. First- and second-order diﬀerence-constrained vectors

9

Deﬁnition 2.7 (2nd-order P transform): Let fi((x, y)) be the line-segment {(x + y + z, y +
z) | z ∈ [z−
i ((x, y)) = (x + y + z−
i ((x, y)) = (x + y +
z+
i , y + z+

i ]} and denote by f −
i , z+
i ) the two endpoints of fi((x, y)).

i ) and f +

i , y + z−

(x,y)∈S f ((x, y)) for the element-wise application of f to a set S of points.

We write f (S) = (cid:83)

result from transforming the upper hull of P αi

i . The next lemma formally establishes that applying f +
i

i−1 by f +
resp. f −
i

i and the lower hull
to the hulls of P αi
i−1

The vertices of P (z)
by f −
correctly computes P (z)

i

i

, (again, compare Figure 2).

i (vur )}, where vll (lower-left) and vur (upper-right) are the ﬁrst vertex of l-hull(P αi

via hulls): If P αi

i−1 is +sloped, then P (z)

i

i (u-hull(P αi

i−1)) and l-hull(P (z)

i

) = f −

is +sloped
i (l-hull(Pi−1)) ∪
i−1) and

Lemma 2.8 (From P αi
and u-hull(P (z)
) = {f −
{f +
the last vertex of u-hull(P αi

i−1 to P (z)
i (vll )} ∪ f +

i

i

i−1), respectively.

We defer the formal proof to Appendix B. Intuitively, since each point in P αi
i−1 is mapped to a
line-segment with slope 1 in P (z)
i−1 along the line x = y. Note
here that we could allow z−
i would instead
map to the ray centered at (x, x + y) and either pointed upwards or downwards with slope 1.
The full transformation from Pi−1 to P (z)

, P (z)
i
i = −∞ and/or z+

i = ∞, where the functions f −

is obtained by sliding P αi

can now be stated as:

i , f +

i

Lemma 2.9 (Pi−1 to P (z)
for ∗ ∈ {−, +}. If Pi−1 is +sloped, then P (z)

i

i

is +sloped with

be the function f ∗,αi

i

(x, y) = (x + αiy + z∗

i , αiy + z∗
i )

i
): Let f ∗,αi

u-hull(P (z)
l-hull(P (z)

i

i

i
) = (cid:8)f −,αi
) = f −,αi

i

i

(vll )(cid:9) ∪ f +,αi
(l-hull(Pi−1)) ∪ (cid:8)f +,αi

(u-hull(Pi−1))
(vur )(cid:9)

i

i

with vll and vur the lower-left resp. upper-right vertex of Pi−1.

Proof: This follows immediately from Corollary 2.4 and Lemmas 2.5 and 2.8.

(cid:3)

Step 2: Truncating by value and slope. To complete the transformation, we need to add
the constraints x−
i ≤ bi − bi−1 ≤ y+
. This is equivalent to cutting
our polygon with two vertical and horizontal planes. The following lemma shows that this
preserves the +sloped-property.

i ≤ bi ≤ x+

i and y−

i to P (z)

i

Lemma 2.10 (# new vertices): If Pi−1 is +sloped with k vertices, then Pi is either empty
or +sloped with at most k + 6 vertices.

It follows that over the course of the algorithm, only O(n) vertices are added in total. This will
be instrumental for analyzing the running time.

i

Proof: We know that P (z)
is +sloped, and it follows easily from the deﬁnition that cutting
by horizontal and vertical planes will preserve this property. Furthermore, note that cutting a
convex polygon will increase the total number of vertices by at most one. We added at most 2
vertices to Pi−1 to obtain P (z)
i , y ≤ y−
i ,
and y ≥ y+
i , i.e., two horizontal and vertical planes. Each adds at most one vertex, giving the
(cid:3)
desired upper bound.

by the inequalities x ≤ x+

. We then cut P (z)

i , x ≥ x−

i

i

10

Eﬃcient Second-Order Shape-Constrained Function Fitting

2.3. Algorithm

A direct implementation of the transformation of Lemma 2.9 yields a “brute force” algorithm
that maintains all vertices of Pi and checks if Pn is empty; (the running time would be quadratic).
It works as follows:

1. [Init]: Compute the vertices of P2.

2. [Compute Pi]: For i = 3, . . . , n, do the following:

resp. f −
i

2.1. At step i, scale the y-coordinate of each vertex by αi.
2.2. Apply f +
i
2.3. Add the new vertex to u-hull and l-hull, as per Lemma 2.9.
2.4. Delete all the vertices outside [x−

i , y+
add the vertices created by intersecting with [x−

to each vertex, depending on which hull it is in.

i ] × [y−

i , x+

i ] × [y−

i , y+
i ].

i ] and
i , x+

3. [Compute b]:

If Pn (cid:54)= ∅, compute (b1, . . . , bn) by backtracing.

i or f −

Observe that Lemma 2.9 applies the same linear function (multiplication of y-coordinate by
αi and f +
i ) to all vertices in u-hull resp. l-hull. So, we do not need to modify every
vertex each time; instead, we can store – separately for u-hull and l-hull – the composition
of the linear transformations as a matrix. Whenever we access a vertex, we take the unmodiﬁed
vertex and apply the cumulative transformation in O(1) time.

At each step, after applying the linear transformations, by Lemma 2.9 we also need to copy
the leftmost vertex of l-hull, add it to the left of u-hull and copy the rightmost vertex of
u-hull and add it to the right of l-hull. To add these vertices, we simply apply the inverse
of each respective cumulative transformation such that all stored vertices require the same
transformation. This will also take O(1) time.

i

i

i , or y ≥ y+

Since all the slopes of P (z)

. Depending on the constraint we are adding, (x ≤ x+

are non-negative (+sloped) and we keep vertices sorted by
x-coordinate, the truncation by a horizontal or vertical plane can only remove a preﬁx or suﬃx
from u-hull and l-hull of P (z)
i , x ≥ x−
i ,
y ≤ y−
i ), we start at the rightmost or leftmost vertex of the u-hull and l-hull, and
continue until we ﬁnd the intersection with the cutting plane. We remove all visited vertices.
This could take O(n) time in any single iteration, but the total cost over all iterations is
O(n) since we start with O(1) vertices and add O(n) vertices throughout the entire procedure
(by Lemma 2.10). This allows us to use two deques (double-ended queues), represented as
arrays, to store the vertices of u-hull and l-hull. Putting this all together gives the linear
time algorithm for the decision problem “S = ∅?”.

To compute an actual solution when S (cid:54)= ∅, we compute bn, . . . , b1, in this order. From
the last Pn, we can ﬁnd a feasible bn (the x-coordinate of any point in Pn). Then, we retrace
the steps of our algorithm through speciﬁc points in each Pi. Since intermediate Pi were only
implicitly represented, we have to recover Pi by “undoing” the algorithm’s operations in reverse
order; this is possible in overall time O(n) by remembering the operations from the forward
phase. The details on the backtracing step are deferred to Appendix C, where we also present
the ﬁnal algorithm.

3. Conclusion

In this article, we presented a linear-time dynamic-programming algorithm to decide whether
there is a vector b that lies (componentwise) between given upper and lower bounds and

References

11

additionally satisﬁes inequalities on its ﬁrst- and second-order (successive) diﬀerences. This
method can be used to approximate weighted-L∞ shape-restricted function-ﬁtting problems,
where the shape restrictions are given as bounds on ﬁrst- and/or second-order diﬀerences (local
slope and curvature).

This is a ﬁrst step towards much sought-after eﬃcient methods for more general convex
regression tasks. A main limitation of our approach is the restriction to one-dimensional
problems. We show in Appendix D that a natural extension of the problem studied here
to directed acyclic graphs is already as hard as linear programming, leaving little hope for
an eﬃcient generic solution. This is in sharp contrast to isotonic regression, where similar
extensions to arbitrary partial orders do have eﬃcient algorithms (for L∞) [27]. This might
also be bad news for multidimensional regression with second-order constraints, since higher
dimensions entail, among other complications, a non-total order over the inputs.

A second limitation is the L∞ error metric, which might not be adequate for all applications.
We leave the question whether similarly eﬃcient methods are also possible for other metrics
for future work. A further extension to study is convex unimodal regression; here, ﬁnding the
maximum is part of the ﬁtting problem, and so not directly possible with our presented method.

Acknowledgments

We thank Richard Peng, Sushant Sachdeva, and Danny Sleator for insightful discussions, and
our anonymous referees for further relevant references and insightful comments that signiﬁcantly
improved the presentation.

References

[1] Pankaj K. Agarwal, Jeﬀ M. Phillips, and Bardia Sadri. Lipschitz unimodal and isotonic
regression on paths and trees. In LATIN 2010: Theoretical Informatics, pages 384–396.
Springer Berlin Heidelberg, 2010. doi:10.1007/978-3-642-12200-2\_34.

[2] Alok Aggarwal, Maria M. Klawe, Shlomo Moran, Peter Shor, and Robert Wilber. Geometric
applications of a matrix-searching algorithm. Algorithmica, 2(1-4):195–208, November
1987. doi:10.1007/bf01840359.

[3] Francis Bach. Eﬃcient algorithms for non-convex isotonic regression through submodular
optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 1–10.
Curran Associates, Inc., 2018.

[4] G´abor Bal´azs. Convex Regression: Theory, Practice, and Applications. PhD thesis, 2016.

doi:10.7939/R3T43J98B.

[5] Bernard Chazelle. A theorem on polygon cutting with applications. In Symposium on
Foundations of Computer Science (SFCS), pages 339–349. IEEE, 1982. doi:10.1109/
SFCS.1982.58.

[6] D. Eppstein, Z. Galil, and R. Giancarlo. Speeding up dynamic programming. In Symposium
on Foundations of Computer Science (SFCS). IEEE, 1988. doi:10.1109/sfcs.1988.
21965.

12

References

[7] Jeﬀ Erickson.
topology.
shortest-homotopic-paths.pdf.

Shortest homotopic paths, 2009. Lecture notes for computational
URL: http://jeffe.cs.illinois.edu/teaching/comptop/2009/notes/

[8] C. Feﬀerman. Smooth interpolation of data by eﬃcient algorithms. In Excursions in
Harmonic Analysis, Volume 1, pages 71–84. Birkh¨auser Boston, November 2012. doi:
10.1007/978-0-8176-8376-4\_4.

[9] Herv´e Fournier and Antoine Vigneron. Fitting a step function to a point set. Algorithmica,

60(1):95–109, July 2009. doi:10.1007/s00453-009-9342-z.

[10] Herv´e Fournier and Antoine Vigneron. A deterministic algorithm for ﬁtting a step function
to a weighted point-set. Information Processing Letters, 113(3):51–54, February 2013.
doi:10.1016/j.ipl.2012.11.003.

[11] Zvi Galil and Raﬀaele Giancarlo. Speeding up dynamic programming with applications
to molecular biology. Theoretical Computer Science, 64(1):107–118, apr 1989. doi:
10.1016/0304-3975(89)90101-1.

[12] Ravi Sastry Ganti, Laura Balzano, and Rebecca Willett. Matrix completion under
monotonic single index models. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages
1873–1881. Curran Associates, Inc., 2015.

[13] Piet Groeneboom and Geurt Jongbloed. Nonparametric estimation under shape constraints,

volume 38. Cambridge University Press, 2014.

[14] Adityanand Guntuboyina and Bodhisattva Sen. Nonparametric shape-restricted regression.

Statistical Science, 33(4):568–594, November 2018. doi:10.1214/18-sts665.

[15] Alon Itai. Two-commodity ﬂow. Journal of the ACM (JACM), 25(4):596–611, 1978.

[16] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Eﬃcient learning of
generalized linear and single index models with isotonic regression. In J. Shawe-Taylor,
R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural
Information Processing Systems 24, pages 927–935. Curran Associates, Inc., 2011.

[17] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic

regression. In Annual Conference on Learning Theory (COLT), 2009.

[18] Philip N Klein. Multiple-source shortest paths in planar graphs. In Symposium on Discrete

Algorithms (SODA), pages 146–155. SIAM, 2005.

[19] Rasmus Kyng, Anup Rao, and Sushant Sachdeva. Fast, provable algorithms for isotonic
regression in all lp-norms.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages
2719–2727. Curran Associates, Inc., 2015.

[20] Rasmus Kyng and Peng Zhang. Hardness results for structured linear systems.

In
Symposium on Foundations of Computer Science (FOCS), pages 684–695, 2017. Available
at: https://arxiv.org/abs/1705.02944.

[21] Der-Tsai Lee and Franco P. Preparata. Euclidean shortest paths in the presence of
rectilinear barriers. Networks, 14(3):393–410, 1984. doi:10.1002/net.3230140304.

References

13

[22] Cong Han Lim. An eﬃcient pruning algorithm for robust isotonic regression. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances
in Neural Information Processing Systems 31, pages 219–229. Curran Associates, Inc.,
2018.

[23] Ronny Luss and Saharon Rosset. Generalized isotonic regression. Journal of Computational
and Graphical Statistics, 23(1):192–210, January 2014. doi:10.1080/10618600.2012.
741550.

[24] Rahul Mazumder, Arkopal Choudhury, Garud Iyengar, and Bodhisattva Sen. A com-
putational framework for multivariate convex regression and its variants. Journal of
the American Statistical Association, pages 1–14, January 2018. doi:10.1080/01621459.
2017.1407771.

[25] G¨unter Rote. Isotonic regression by dynamic programming. In Jeremy T. Fineman and
Michael Mitzenmacher, editors, Symposium on Simplicity in Algorithms (SOSA 2019),
volume 69 of OASIcs, pages 1:1–1:18. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
2018. doi:10.4230/OASIcs.SOSA.2019.1.

[26] Quentin F. Stout. Unimodal regression via preﬁx isotonic regression. Computational
Statistics & Data Analysis, 53(2):289–297, December 2008. doi:10.1016/j.csda.2008.
08.005.

[27] Quentin F. Stout. Fastest isotonic regression algorithms, 2014. URL: http://web.eecs.

umich.edu/~qstout/IsoRegAlg.pdf.

[28] Charalampos E. Tsourakakis, Richard Peng, Maria A. Tsiarli, Gary L. Miller, and Russell
Schwartz. Approximation algorithms for speeding up dynamic programming and denoising
aCGH data. Journal of Experimental Algorithmics, 16:1.1, May 2011. doi:10.1145/
1963190.2063517.

[29] F. Frances Yao. Eﬃcient dynamic programming using quadrangle inequalities. In Sympo-
sium on Theory of Computing (STOC). ACM Press, 1980. doi:10.1145/800141.804691.

[30] F. Frances Yao. Speed-up in dynamic programming. SIAM Journal on Algebraic Discrete

Methods, 3(4):532–540, December 1982. doi:10.1137/0603055.

14

Appendix

References

A. Simple greedy algorithm for convex regression

In this appendix, we give details on a simpler algorithm for the special case of unweighted
convex function ﬁtting.

Theorem A.1: There exists an algorithm for the unweighted L∞ convex regression that runs
in O(n) time.

Proof: We consider the following problem. Given an n-dimensional vector a, and parameter
∆ ≥ 0, ﬁnd a convex vector b such that (cid:107)b − a(cid:107)∞ ≤ ∆, if such a vector exists.

This clearly ﬁts under our parameters of Deﬁnition 1.1 by setting x− = a − ∆, x+ = a + ∆,
both y− and y+ to be unbounded, and z− = 0, z+ = ∞, along with α = 1. A binary search
on ∆ gives a O(n log U

ε ) algorithm.

However, this can also be solved by considering the set of points (i, ai + ∆) for all i, and
taking the lower hull,2 H(∆), such that for each point (i, hi) in this lower hull we set bi = hi.
We claim that the minimum possible ∆ such that bi ≥ ai − ∆ is exactly the answer to this
problem. If (i, ai + ∆) is a vertex of the convex hull, bi = ai + ∆ is always at least ai − ∆.
Otherwise, let (j, aj + ∆), (k, ak + ∆) be two vertices of H such that j < i < k. We have

bi ≥ ai − ∆

⇐⇒ aj + ∆ +

ak − aj
k − j

(i − j) ≥ ai − ∆

⇐⇒ ∆ ≥

(cid:18)

1
2

ai − aj +

(cid:19)

(ak − aj)

i − j
k − j

If ∆ violates this for some i, j, k, then it is impossible to ﬁt a convex function through the
intervals [(j, aj − ∆), (j, aj + ∆)], [(i, ai − ∆), (i, ai + ∆)], and [(k, ak − ∆), (k, ak + ∆)].

Conversely, if ∆ satisﬁes all of such constraints, bi ≥ ai − ∆ for all 1 ≤ i ≤ n, then bi cannot
be greater than ai + ∆ as that would violate H being the convex lower hull of (i, ai + ∆). Thus,
(b1, . . . , bn) is a possible solution.

It takes O(n) time to compute the lower convex hull and O(n) time to calculate the minimum
(cid:3)

∆. Thus, this algorithm solves L∞ convex regression in O(n) time.

The above method can also be adapted for inputs with x-values that are non-uniformly spaced.
However, it does not directly generalize to weighted L∞ regression: moving points up by wi · ∆
can lead to diﬀerent lower hulls for diﬀerent values of ∆.

2The lower hull of a set of points is the subset of vertices (xi, yi) of the convex hull, where yi is the minimal

y-coordinate of all points with the x-coordinate xi in the convex hull; see also Deﬁnition 2.6.

B. Proof of Lemma 2.8

15

B. Proof of Lemma 2.8

i−1))}∪{f −

i (l-hull(P αi

i (l-hull(P αi

i (u-hull(P αi

The proof of Lemma 2.8 will be separated into two stages. First, we show that the polygon deﬁned
by {f +
i−1))} and
lower-hull {f −
i−1) and vur
is the last vertex of u-hull(P αi
i−1). Furthermore, this polygon will have slopes between vertices
in [0, 1]. This property will then allow us to show that P (z)
is equivalent to the convex hull of
the vertices, which implies the claim.
In order to show that the P (z)

i (vur )}, where vll is the ﬁrst vertex of l-hull(P αi

has all slopes between 0 and 1, we consider how f −

i−1))} has upper-hull {f −

i (u-hull(P αi

i (vll ), f +

i−1)), f +

i

i and f +
i

i

aﬀect slopes.

Lemma B.1 (Bounded slopes): If P is +sloped, then for any connected vertices vj, vk ∈ V ,
any i, and ∗ ∈ {−, +}, we have

0 ≤ slope(f ∗

i (vj), f ∗

i (vk)) ≤ 1

and for any connected vertices vj, vk, vl ∈ V , if slope(vj, vk) < slope(vk, vl), then

slope(f ∗

i (vj), f ∗

i (vk)) < slope(f ∗

i (vk), f ∗

i (vl))

Proof: We ﬁrst write the slope function explicitly to obtain

slope(f ∗

i (vj), f ∗

i (vk)) =

(yj + z∗
(xj + yj + z∗

i ) − (yk + z∗
i )
i ) − (xk + yk + z∗
i )
i (vj), f ∗

=

yj − yk
(xj − xk) + (yj − yk)

.

This implies that if slope(vj, vk) = ∞ then slope(f ∗
then slope(f ∗

i (vk)) = 0. Furthermore, this gives the identity

i (vj), f ∗

i (vk)) = 1, and if slope(vj, vk) = 0

slope(f ∗

i (vj), f ∗

i (vk))−1 = slope(vj, vk)−1 + 1

when slope(vj, vk) ∈ (0, ∞). Combined with the fact that all slopes are non-negative, this gives
(cid:3)
both of our desired inequalities.

The ﬁrst inequality of the lemma above will allow us to show that all of the slopes between
vertices are bounded, and the second implies that each of the vertices remains a vertex, giving
the following corollary.

i−1 is +sloped, then the convex hull P of V = f +

Corollary B.2 (Hulls by elementwise transformation):
If P αi
u-hull(P ) = {f −
where vll
vertex of u-hull(P αi
slope(vj, vk) ≤ 1.

i−1)) has
i (l-hull(P αi
i (vur )},
i−1) and vur is the last (upper-right)
i−1). Furthermore, for any connected vertices vj, vk in P , we have 0 ≤

i (u-hull(P αi
is the ﬁrst (lower-left) vertex of l-hull(P αi

i (u-hull(P αi
i−1))} and l-hull(P ) = {f −

i (l-hull(P αi
i−1)), f +

i−1)) ∪ f −

i (vll ), f +

Proof: By construction, the ﬁrst and last vertices of u-hull(P ) and l-hull(P ) are the same.
Let vu1 be the ﬁrst vertex of u-hull(P αi
i−1), which gives two possibilities, either (1): vu1 = vll ,
or (2) slope(vu1, vll ) = ∞. For case (1) it is easy to see that slope(f +
i (vll )) = 1,
and for case (2), we showed in the proof of Lemma B.1 that slope(vu1, vll ) = ∞ im-
plies slope(f +
i (vll )) = 1 gives
i (vll )) = 1. Furthermore the slopes between all vertices in u-hull(P αi
slope(f +
i−1)
are less than ∞ by Deﬁnition 2.6, and therefore less than 1 under the transformation by
Lemma B.1. Along with the second inequality of Lemma B.1, this implies that u-hull(P )
makes up a concave function from f −

i (vll )) = 1, which combined with slope(f +

i (vu1), f +

i (vu1), f −

i (vu1), f −

i (vll ), f −

i (vll ) to f +

i (vur ).

16

References

By symmetric reasoning we see that l-hull(P ) makes up a convex function from
i (vur ). Additionally, the second inequality states that every element in
i−1)), f +
i−1))} must be a vertex. Accordingly,
(cid:3)

i (vll ) to f +
f −
i (l-hull(P αi
i (vur )} and {f −
{f −
P must be a convex polygon with all slopes between 0 and 1.

i (u-hull(P αi

i (vll ), f +

i

We now have ﬁxed upper and lower hulls of a polygon, and we use the representation as the
convex hull its vertices, along with the bounded-slope property, to show that this polygon is in
fact equal to P (z)
. In particular, all the slopes being bounded by 1 will be critical here because
each point (x, y) ∈ P αi
i , y + z+
i ),
which has slope 1. If we then consider (x, y) to be in the upper hull, if the slopes of our new
upper-hull for P (z)
i ) would lie outside of this
hull. Our bounded slopes prevent this, though, and lead to the following lemma.
Lemma B.3: Let P αi

i were greater than 1, the point (x + y + z−

i−1 maps to a line segment from (x + y + z−

i ) to (x + y + z+

i , y + z−

i , y + z−

i−1 be +sloped and let P be the convex hull of
V = (cid:8)f +
i (l-hull(P αi

i (u-hull(P αi

i−1))(cid:9) ∪ (cid:8)f −

i−1))(cid:9).

Then P = P (z)

i

.

Proof: We show both inclusions.

• P ⊆ P (z)

.

i

By deﬁnition of P , any point u ∈ P , can be written as a convex combination

(cid:88)

pj((xj + yj, yj) + (z∗

i , z∗

i )),

(xj ,yj )∈V (P αi
−

i

1)

where the sum is over the vertices (xj, yj) of P αi
z = (cid:80) pjz∗
know each (xj, yj) is a vertex in P αi
(x + y + x, y + z) ∈ P (z)

i−1, ∗ ∈ {−, +}, and (cid:80) pj = 1. We set
i ]. Furthermore set x = (cid:80) pjxi and y = (cid:80) pjyj. We
i−1, implying

i−1, so by convexity (x, y) must be in P αi

i ; clearly, z ∈ [z−

by Corollary 2.4.

i , z+

i

• P (z)

i ⊆ P .

i , z+

i , z+

i , yv + z+

i with (x, y) ∈ P αi

i ) ∈ P , so we must have z = z−
i .

i−1 is +sloped and fi is monotone, f −
i (vur ) dominates (xv + yv + z−

i−1 and
i ], but (x + y + z, y + z) /∈ P . By deﬁnition and assumption, both P and P αi
i−1
i−1 such that (xv + yv + z, yv + z) /∈
i } such that
i−1). By deﬁnition of

Assume towards a contradiction there were (x + y + z, y + z) ∈ P (z)
z ∈ [z−
are convex, so there must be a vertex (xv, yv) of P αi
P . Furthermore, by convexity of P , there must also exist z ∈ {z−
(xv + yv + z, yv + z) /∈ P . Assume w.l.o.g. that (xv, yv) ∈ u-hull(P αi
P , we have (xv + yv + z+
Since P αi
i , yv + z−
and similarly, f +
the upper hull lies above the line segment from from f −
has slope at most 1. But the slope between (xv +yv +z−
is exactly 1, so (xv + yv + z−
Finally, (xv + yv + z−
i , yv + z−
there would exist (xv, y) ∈ P αi
u-hull(P αi
and because the x-coordinate of (xv + yv + z−
of P , we have (xv + yv + z−

i ) also cannot lie below l-hull(P ) because otherwise
i−1 that lies above (xv, yv), contradicting (xv, yv) being in
i−1). Because the upper hull and lower hull combine to the convex polygon P
i , yv + z−
i ) is within the range of x-coordinate
(cid:3)
i ) ∈ P , a contradiction.

i , yv +z−
i ),
i ). Furthermore, by Corollary B.2
i (vll ) to (xv + yv + z+
i ) and
i , yv +z+
i , yv +z−
i )

i (vll ) is dominated3 by (xv +yv +z−

i ) cannot lie above the upper hull.

i ) and (xv +yv +z+

i , yv + z−

i , yv + z−

i , yv + z+

With this, we ﬁnish the proof of our lemma.

Proof of Lemma 2.8: Follows directly from Corollary B.2 and Lemma B.3.

(cid:3)

3(x1, y1) is said to dominate (x2, y2) if x1

x2 and y1

y2.

≥

≥

C. Complete algorithm

17

C. Complete algorithm

In this appendix, we give detailed pseudocode for our entire algorithm. We also discuss the
details on the backtracing step, i.e., computing an actual solution b ∈ S from the (implicitly
represented) feasibility polygons P2, . . . , Pn. The ﬁnal procedure is shown in Algorithm 1.

C.1. Implicitly computing the Pi

The main ideas have been described in Section 2.3. We represent points in homogeneous
coordinates, i.e., (x, y) becomes the column vector (x, y, 1)T . That allows our transformation
to be represented as a single matrix, and we can compose them by multiplying the matrices.
We store the current matrix in Algorithm 1 in Su (for the upper hull) and Sv for the lower
hull. u and v denote the deques storing the (untransformed) points of u-hull and l-hull in
homogeneous coordinates and in sorted order.

To compute Pi from Pi−1 (Step 2), we update the transformation matrices and add the
new points to the hull (following Lemma 2.9). After that (line 9), u and v represent P (z)
.
To implement the intersection with the half planes corresponding to the value and ﬁrst-order
constraints at i, we separately cut upper and lower hull with all four boundaries. Since we
store upper and lower hull separately, vertical line segments are not explicitly represented in
either hull, which requires some care in cutting with horizontal lines. We therefore use the
following strategy –it is illustrated on an example in Figure 3: We ﬁrst cut with the left and
right boundaries (the value constraints), then transform our representation temporarily to
left and right hulls (lines 17–18), which can easily handle cutting by horizontal line segments.
Cutting is always implemented as a linear scan of u resp. v, during which all vertices outside
the constraint halfplane are removed. Then we add a new vertex at the intersection of the last
segment with the constraint. (We remember the last removed vertex r for doing so.)

i

(4, 2)

(2, 1)

(1.95, 0.95)

(1.5, 0.5)

(1.5,

0.5)

−

(b)

(0,

1)

−

(1,

1)

−

(a)

(1.95, 0.4)

(1.5, 0.4)

(1.5, 0.4)

(1.95, 0.4)

(1.95,

0.05)

−

(1.95,

0.05)

−

(1.5,

0.3)

−

(1.7,

0.3)

−

(1.5,

0.3)

−

(1.7,

(1.95,

0.05)

−

0.3)

−

(c)

(d)

(e)

(f)

Figure 3: Example for lines 10–30 of Algorithm 1. (a) The polygon P (z)
i = 1.5 and x+

(after line 9). (b) After
i
vertical cuts at x−
i = 1.95. (c) After adding the vertical line segments
(line 18). (d) After horizontal cuts at y−
i = −0.4 (line 26); u and v
represent left and right hull of the correct polygon now, but we have to transform them
back to upper and lower hull. For that, we store the LL and UR vertices. (e) After
deletion of vertices with same x-coordinate (line 28); neither vertical, nor horizontal
line segments are represented. (f) After adding the stored LL and UR vertices (line 30),
we obtain the ﬁnal upper and lower hulls.

i = −0.3 and y+

18

References

Algorithm 1: 1st/2nd-Diﬀ-Constrained Decision Algorithm

Input: Vectors x− ≤ x+, y− ≤ y+, z− ≤ z+, α ≥ 0
Output: Some b ∈ S, or infeasible if S = ∅.

(homogeneous coordinates).

1 Note: We represent vertex (x, y) by real vector (x, y, 1)T ,
2 [Step 1: Init]
3 u ← deque with vertices of upper hull of P2 (sorted by x-coordinates);
4 v ← deque with vertices of lower hull of P2 (sorted by x-coordinates);
5 Su ← I3; Sv ← I3 ;
6 [Step 2: Compute Pi]
7 for i ← 3 to n do
1 αi
0 αi
0
0

1 αi
0 αi
0
0

z−
i
z−
i
1

z+
i
z+
i
1

 · Su;

 · Sv;

Su ←

Sv ←













8

/* init maps to the identity matrix I3 in R3×3 */

/* Update maps */

/* Add new LL / UR vertex to hulls after transformation
u.push front(cid:0)(Su)−1 · Sv · v.front()(cid:1); v.push back (cid:0)(Sv)−1 · Su · u.back ()(cid:1);
for c ∈ {u, v} do /* Cut left and right boundary

· (r − q)(cid:1)(cid:1)(cid:9);

r ← null ; while c.size() ≥ 1 ∧ (Sc · c.front())x < x−
if c.empty() then return infeasible;
if r (cid:54)= null then (cid:8)q ← Sc · c.front(); c.push front(cid:0)(Sc)−1 · (cid:0)q + qx−x−
r ← null ; while c.size() ≥ 1 ∧ (Sc · c.back ())x > x+
if c.empty() then return infeasible;
if r (cid:54)= null then (cid:8)q ← Sc · c.front(); c.push front(cid:0)(Sc)−1 · (cid:0)q + x+

i
qx−rx

i do r ← Sc · c.pop back ();

i do r ← Sc · c.pop front();

i −qx
rx−qx

· (r − q)(cid:1)(cid:1)(cid:9);
/* Temporarily add vertices for vertical line segments (simplifies cuts)
if (Su · u.front())y > (Sv · v.front())y then u.push front(cid:0)(Su)−1 · Sv · v.front()(cid:1);
if (Su · u.back ())y > (Sv · v.back ())y then v.push back (cid:0)(Sv)−1 · Su · u.back ()(cid:1);
for c ∈ {u, v} do /* Cut upper and lower boundary
r ← null ; while c.size() ≥ 1 ∧ (Sc · c.front())y < y−
if c.empty() then return infeasible;
if r (cid:54)= null then (cid:8)q ← Sc · c.front(); c.push front(cid:0)(Sc)−1 · (cid:0)q + qy−y−
r ← null ; while c.size() ≥ 1 ∧ (Sc · c.back ())y > y+
if c.empty() then return infeasible;
if r (cid:54)= null then (cid:8)q ← Sc · c.front(); c.push front(cid:0)(Sc)−1 · (cid:0)q + y+
ll c ← Sc · c.front(); ur c ← Sc · c.back ();
/* Remove generated duplicate nodes and vertical segments
/* (sndFront/sndBack denote the second / second-to-last elements)
while c.size() ≥ 2 ∧ (Sc · c.front())x = (Sc · c.sndFront())x do c.pop front();
while c.size() ≥ 2 ∧ (Sc · c.back ())x = (Sc · c.sndBack ())x do c.pop back ();

i −qy
ry−qy
/* Store current LL/UR for later */
*/
*/

i do r ← Sc · c.pop front();

i do r ← Sc · c.pop back ();

· (r − q)(cid:1)(cid:1)(cid:9);

· (r − q)(cid:1)(cid:1)(cid:9);

i
qy−ry

*/

*/

*/

*/

*/

/* Add stored LL/UR vertices if horizontal segments missing
(cid:1);
if (Sv · v.front())x > (Su · u.front())x then v.push front(cid:0)(Sv)−1 · ll u
(cid:1);
if (Su · u.back ())x < (Sv · v.back ())x then u.push back (cid:0)(Su)−1 · ur v

30
31 [Step 3: Compute b]
32 (x, y) ← Su · u.back (); bn ← x; p ← index of the last element of u;
33 for i ← n to 3 do

Revert u, v, Su, Sv to the previous stage;
x(cid:48) ← x − y;
while px < x(cid:48) do p++;
Use up and up−1 (if exists) to compute ym ← max{y(cid:48) | (x(cid:48), y(cid:48)) ∈ Pi−1};
if y ≥ αiym + z−
i )/αi);
bi−1 ← x;

i then (x, y) ← (x(cid:48), ym) else (x, y) ← (x(cid:48), (y − z−

39
40 b1 ← x − y;
41 return (b1, . . . , bn);

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

34

35

36

37

38

C. Complete algorithm

C.2. Backtracing

19

i+1, z+

Suppose we have computed Pn as described above, and then partially backtraced through a
sequence of feasible points. We are now at (bi+1, bi+1 − bi) in Pi+1. Since (bi+1, bi+1 − bi) = (x +
y +z, y +z), z ∈ [z−
i+1] for some (unknown) (x, y) = (bi, αi+1(bi −bi−1)) ∈ Pi, we can recover
x = bi from (bi+1, bi+1 − bi) by subtracting the two coordinates of (bi+1, bi+1 − bi). To recover
y, suppose we can ﬁnd ymax = max{y | (bi, y) ∈ P αi+1
} is
an interval, the following lemma allow us to ﬁnd bi − bi−1.
Lemma C.1 (back 1 step): Let fi+1(x, y) = {(x + y + z, y + z) | z ∈ [z−
(bi+1, bi+1 − bi) ∈ fi+1((bi, ymax)) or (bi+1, bi+1 − bi) = (bi + y + z−

i+1]}. Either
i+1) for some y < ymax.
Intuitively, a vertical line segment L inside Pi is mapped to a line-segment with slope 1 in Pi+1,
because the line segments the points in L are mapped to lie all on the same line (overlapping
with each other).

} eﬃciently. Since {y | (bi, y) ∈ P αi+1

i+1, y + z−

i+1, z+

i

i

Proof: If (bi+1, bi+1 − bi) (cid:54)∈ fi+1(bi, ymax), by the maximality of ymax, bi+1 − bi < ymax + z−
i+1.
Since there exists (bi, y(cid:48)) such that (bi+1, bi+1 − bi) ∈ fi+1(bi, y(cid:48)), (bi + y(cid:48) + z, y(cid:48) + z) =
(bi+1, bi+1 − bi) for some z ∈ [z−
i+1). Then (bi+1, bi+1 − bi) =
(bi +(y(cid:48) +z −z−
i+1 < ymax.
The lemma is proven by letting y be y(cid:48) + z − z−
(cid:3)

i+1]. Consider fi+1(bi, y + z − z−
i+1)+z−

i+1). Since bi+1 −bi < ymax +z−

i+1, z+
i+1, (y(cid:48) +z −z−

i+1, y(cid:48) +z −z−

i+1)+z−

i+1.

In the former case of Lemma C.1, we can take (x, ymax) as (bi, αi+1(bi − bi−1)). In the latter
case, we can take (bi, (bi+1 − bi) − z−

i+1) as (bi, αi+1(bi − bi−1)).
Since ymax is the y-coordinate of the intersection of u-hull(Pi) and the vertical line (bi, ·),
to compute ymax, we want to ﬁnd two vertices in u-hull(Pi), (xl, yl) and (xr, yr), such that
xl ≤ bi ≤ xr. (bi, ymax) is just the intersection of the line segment between (xl, yl) and (xr, yr)
and the vertical line (bi, ·). The following lemma shows how to ﬁnd (xl, yl) and (xr, yr) eﬃciently
using an amortized constant-time algorithm.

Lemma C.2 (Computing ymax): Suppose (xl, yl) and (xr, yr) are two vertices
in
u-hull(Pi), and some point (bi, y) ∈ Pi satisﬁes xl ≤ bi ≤ xr. Let (x(cid:48), y(cid:48)) be some
point in Pi+1 with (x(cid:48), y(cid:48)) ∈ fi+1(bi, αi+1y). Then x(cid:48) ≤ (f +
i+1(xr, αi+1yr))x, where (·)x means
taking the x-coordinate of a point and (·)y takes the y-coordinate.
Proof: Assume towards a contradiction that x(cid:48) > (f +
xr = (f +
αi+1y + k ≤ αi+1y + z+

i+1(xr, αi+1yr))x. Since x(cid:48) − y(cid:48) = bi ≤
i+1(xr, αi+1yr))y. But y(cid:48) =
(cid:3)

i+1(xr, αi+1yr))y, we have y(cid:48) > (f +

i+1(xr, αi+1yr))x − (f +

i+1(xr, αi+1yr))y. Contradiction.

i+1 ≤ αi+1yr + z+

i+1 = (f +

The amortized constant-time algorithm to retrieve (bn, . . . , b1) depends on the implementation
of the deques. Since we will add n vertices to the deques during the whole algorithm, the
(textbook) ﬁxed-size array-based implementation suﬃces; we recall it to ﬁx notation. A deque
d is represented by array A and two indices pl, pr. pl is the index of the ﬁrst element of d and
pr is the index of the last element. If we want to add an element e to the left of the deque,
the two operations pl ← pl − 1, A[pl] = e suﬃce. Similarly, we can add/pop elements from
left/right. During our algorithm, pl (resp. pr) can move to the left (resp. right) by at most n
positions, so A can be an array of length 2n + O(1). If we store the vertices of P2 in the middle
of A initially, we never exceed the boundaries of A when running the algorithm.

Deﬁnition C.3 (Position): We deﬁne pos i(x(cid:48)) as the smallest index (in the array representing
deque u) of a vertex of u-hull(Pi(·)) with x-coordinate at least x(cid:48).

Note that adding or removing elements does not change the vertex at a given index (unless
that vertex itself is removed).

20

References

Lemma C.4 (Monotonicity of positions):
pos i(bi) ≥ pos i+1(x(cid:48)) for some (x(cid:48), y(cid:48)) ∈ fi+1(bi, αi+1y).
i+1(xr, αi+1yr))x. So f +

Proof: By Lemma C.2, x(cid:48) ≤ (f +
And since our algorithm stores f +
posi(bi).

i+1(xr, αi+1yr) is stored after posi+1(x(cid:48)).
i+1(xr, αi+1yr) at the same place as (xr, yr), posi+1(x(cid:48)) ≤
(cid:3)

Lemma C.4 allows us to ﬁnd pos i(z) by moving a pointer monotonically to the right. Thus, we
can retrieve bn, . . . , b1 in order by unrolling our linear algorithm for the decision problem and
moving the pointer pos i(z). This process takes O(n) time overall.

C.3. Analysis

We conclude with the proof of our main theorem.

Proof of Theorem 1.2: The correctness of Algorithm 1 follows from the preceding discussions:
By Lemma 2.9, the iterative transformations compute the Pi as deﬁned in (2), and S (cid:54)= ∅ iﬀ
Pn (cid:54)= ∅. Moreover, Lemma C.1 shows that, when S (cid:54)= ∅, Step 3 computes a valid b ∈ S. It
remains to analyze the running time.

• Step 1 takes O(1) time since the vertices of P2 are a subset of the (at most) 12 intersection
2 , x−
1 ), (x−
2 −
2 and y ≤ y+
2 .)

points of the deﬁning lines. (P2 is the trapezoid spanned by (x−
1 ), (x+
x−

1 ), intersected with the halfspaces y ≥ y−

2 − x+

2 − x−

2 − x+

1 ), (x+

2 , x−

2 , x+

2 , x+

• Step 2. The operations inside the loops are all constant-time and the outer loop runs
O(n) times. Moreover, the inner while-loops all remove a node from a deque, so their
total cost over all iterations of the for-loop is O(n), too: We start with O(1) vertices
and adding at most O(n) vertices throughout the entire procedure (Lemma 2.10), so we
cannot remove more than O(n) vertices.

• Step 3. All operations except for the ﬁrst line inside the for-loop take constant time. The
inner while-loop runs for overall O(n) iterations, since p only moves right and we add
O(n) vertices in total.

It remains to implement the ﬁrst line of the loop body in O(n) overall time. To be able
to undo the changes to u, v, Su, Sv, we keep a log for each instruction executed in Step 2,
so that we can undo their changes here (in the opposite order). Since Step 2 runs in O(n)
total time, the rollback also runs in O(n) time.

Since all three steps run in linear time, so does the whole algorithm.

(cid:3)

D. Generalization to DAGs is hard

In this appendix, we will give a natural generalization of Deﬁnition 1.1 to arbitrary DAGs
and investigate its complexity. Our original setting with diﬀerences of adjacent indices only
corresponds to a directed-path graph.

In light of rather general results for isotonic regression, the path setting might appear quite
restrictive; we will argue here why these conditions probably cannot be relaxed much further if
we want an O(n) time algorithm.

Deﬁnition D.1: Suppose we are given a directed acyclic graph G = (V, E) with m = |E|
edges and mp number of length two directed paths in G , n-dimensional vectors x− ≤ x+, m

D. Generalization to DAGs is hard

21

dimensional vector y− ≤ y+, and mp dimensional vectors z− ≤ z+ and α ≥ 0. We deﬁne SG to
be the set of all n-dimensional vectors b such that x−
ij for all
edges (i, j) ∈ E, and z−
ijk for all pairs of edges (i, j), (j, k) ∈ E.

ijk ≤ (bk − bj) − αijk(bj − bi) ≤ z+

ij ≤ bj −bi ≤ y+

i ≤ bi ≤ x+
i

for all i, y−

In contrast to Theorem 1.2, we show that determining if SG if empty or not is as hard as

solving linear programs.

Theorem D.2: With notation as in Deﬁnition D.1, if we can determine SG is empty or not in
time f (n + m + mp), then we can determine feasibility of any set of linear constraints deﬁned
by s bounded integer coeﬃcients in c1f (c2s log M )) time, where c1 and c2 are two constants
and the absolute value of each coeﬃcient in the linear constraints is no more than M .

Our reduction to prove Theorem D.2 is closely motivated by the hardness of isotropic total
variation from [20], as well as subsequent works on extending such hardness results to positive
linear programs. Compared to these results though, it sidesteps linear systems, and is a more
direct invocation of the completeness of 2-commodity ﬂow linear programs from [15].

We ﬁrst consider a more restricted class of problems than Deﬁnition D.1 allows (where all

the α’s in Deﬁnition D.1 are set to be 1). Formally we deﬁne the problem as:

Deﬁnition D.3: A generalized second-order constrained feasibility problem is deﬁned by
variables b1 . . . bn, combined with a set of m constraints parameterized by

1. Upper and lower bounds on the variables x−

i and x+
i .

2. Upper and lower bounds on the ﬁrst order diﬀerences y−

i and y+

i and corresponding

indices pi < qi.

3. Upper and lower bounds on the second order diﬀerences z−

i and z+

i and corresponding

indices ri < si < ti

and constraints

Value Constraints: x−

i ≤ bi ≤ x+
i

First Order Constraints: y−

i ≤ bqi − bpi ≤ y+
i

Second Order Constraints: z−

i ≤ (bti − bsi) − (bsi − bri) ≤ z+
i .

The goal is to decide whether there exists b1, . . . , bn that satisfy all these constraints simultane-
ously.

Proof of Theorem D.2: It is easy to see that the problem deﬁned in Deﬁnition D.3 is a
special case of the problem in Deﬁnition D.1. This is obtained by forming a DAG with edges
(pi, qi), (ri, si), (si, ti) for all pi, qi, ri, si, ti. We will prove that a general linear programming
feasibility problem with s polynomially-bounded integer coeﬃcients can be expressed as a
second-order-constrained feasibility problem (Deﬁnition D.3). In particular, we will show that a
feasibility of a set of linear constraints containing at most s non-zero coeﬃcients whose absolute
values are integers no more than M can be reduced to O(s log M ) value, ﬁrst order and second
order constraints as in Deﬁnition D.3.

Note that the second constraint in Deﬁnition D.3 is the same as

i ≤ 2bqi − bri − bpi ≤ z+
z−
i .

22

References

In particular, it allows us to create constraints of the form

2bqi = bpi + bri.

We will now show how we can restate a feasibility of a set of general linear constraints can be
expressed as a second order constrained feasibility problem as in Deﬁnition D.1. The main idea
will be clear when we consider a linear constraint of the form

bi1 + bi2 + . . . bik ≤ ci,

with k a power of 2, and i1 < i2 < . . . < ik in increasing order. To express this in terms of
second order constraints, we can introduce new variables

i1 < i12 < i2
i3 < i34 < i4
. . .

and use bi12 to represent the sum of bi1 and bi2 and so on. Repeating this halves the value of k,
but aggregates the whole sum into a single variable. Therefore, we can express the above linear
constraint as one value constraint

and k − 1 second order constraints

bi12...k ≤ x+

12...k := ci

bi12 = bi1 + bi2, . . . , bi(k

1)k = bik

−

1 + bik , . . . , bi1...k = bi1...k/2 + bik/2+1...k .

−

In case k is not a power of 2, we can add dummy variables whose values we restrict to zero
using the value constraints. This process uses at most k value constraints. So we have shown
that we can express any linear constraint of the form bi1 + bi2 + . . . bik ≤ ci in terms of O(k)
second order constraints and O(k) value constraints.

Now consider the case with both positive and negative values in the linear constraint

bi1 ± . . . ± bik ≤ ci.

We can aggregate the sums of the variables with positive coeﬃcients and negative coeﬃcients
separately, and let us denote the resulting variables by bpos, bneg. We can now bound the
diﬀerence using a ﬁrst order constraint of the form

bpos − bneg ≤ ci.

This results in additional O(1) ﬁrst order constraints for each linear constraint.

Finally, when the coeﬃcients are arbitrary integers, we can do pairing based on the binary
representation. The second order constraint and value constraint allows us to create constrains
of the form

0 ≤ 2bi − b0 − bj ≤ 0

0 ≤ b0 ≤ 0

which are equivalent to

bj = 2bi.
So we can introduce new variables dkj representing 2kbj for any 1 ≤ k ≤ c where c is a constant.
Thus, given any linear constraint in k variables with integer coeﬃcients that are bounded by

D. Generalization to DAGs is hard

23

M , we ﬁrst represent each coeﬃcient by its binary representation, increasing the number of
non-zero coeﬃcients by O(log M ) times and creating O(k log M ) second order constraints and
value constraints. Then all the coeﬃcients in the linear constraints are +1 or −1 and we can
use the reduction above. In summary, we can solve any linear programming feasibility problem
with O(s) non-zero coeﬃcients which are integers bounded by M by a generalized second-order
constrained feasibility problem of O(s log M ) constraints. This together with our assumption
of an algorithm solving generalized second-order constrained feasibility problem in f (·) time
(cid:3)
prove the theorem.

