0
2
0
2

n
a
J

3

]

C
D
.
s
c
[

2
v
4
6
1
0
0
.
1
0
0
2
:
v
i
X
r
a

AIR – A Light-Weight Yet High-Performance Dataflow
Engine based on Asynchronous Iterative Routing
Vinu E. Venugopal
Martin Theobald
Samira Chaychi
Amal Tawakuli
{vinu.venugopal,martin.theobald,samira.chaychi,amal.tawakuli}@uni.lu
Faculty of Science, Technology and Communication
University of Luxembourg, Belval, Luxembourg

ABSTRACT
Distributed Stream Processing Systems (DSPSs) are among
the currently most emerging topics in data management,
with applications ranging from real-time event monitoring
to processing complex dataflow programs and big data ana-
lytics. The major market players in this domain are clearly
represented by Apache Spark and Flink, which provide a va-
riety of frontend APIs for SQL, statistical inference, machine
learning, stream processing, and many others. Yet rather few
details are reported on the integration of these engines into
the underlying High-Performance Computing (HPC) infras-
tructure and the communication protocols they use. Spark
and Flink, for example, are implemented in Java and still rely
on a dedicated master node for managing their control flow
among the worker nodes in a compute cluster.

In this paper, we describe the architecture of our AIR en-
gine, which is designed from scratch in C++ using the Mes-
sage Passing Interface (MPI), pthreads for multithreading,
and is directly deployed on top of a common HPC workload
manager such as SLURM. AIR implements a light-weight,
dynamic sharding protocol (referred to as “Asynchronous
Iterative Routing”), which facilitates a direct and asynchro-
nous communication among all client nodes and thereby
completely avoids the overhead induced by the control flow
with a master node that may otherwise form a performance
bottleneck. Our experiments over a variety of benchmark
settings confirm that AIR outperforms Spark and Flink in
terms of latency and throughput by a factor of up to 15;
moreover, we demonstrate that AIR scales out much better
than existing DSPSs to clusters consisting of up to 8 nodes
and 224 cores.

KEYWORDS
Distributed stream processing systems; Dataflow program-
ming; Stateful windowed operators; Asynchronous routing

1 INTRODUCTION
With the recent advent of Industry 4.0 and Internet-of-Things
(IoT) applications on a broad basis, the scalable processing
and real-time analysis of streaming data has reached an un-
precedented attention both in academia and industry. Catch-
ing up with the vastly growing rate at which data streams
are produced already today poses a major—if not intangible—
challenge to classical database architectures and is there-
fore increasingly replaced by distributed analytical platforms
based on Apache Hadoop [35], Spark [37], Storm [31] and
Kafka [27]. This new generation of tools is designed to pro-
cess data streams in a flexible, scalable, fast and resilient man-
ner, but still is largely limited to splitting an incoming data
stream into batches and to then synchronously execute their
analytical workflows over these data batches. To overcome
the limitations of this iterative form of bulk-synchronous pro-
cessing (BSP), asynchronous stream-processing (ASP) engines
such as Apache Flink [9], Samza [28] and Naiad [22, 26] have
recently emerged and are meanwhile increasingly being fos-
tered both by open-source communities, such as the Apache
Software Foundation (ASF), and by a new line of companies
like Databricks and dataArtisans.

What all of the aforementioned platforms (both BSP and
ASP) still have in common, though, is their rather static
master-client architecture, which still dates back to the origi-
nal design of Hadoop. Given the inherent deviances of dis-
tributed computations (due to communication and network
delays, scheduling algorithms, time spent on processing, seri-
alization/deserialization, etc.), the performance of platforms
built on such a master-client architecture is still bound by
hidden synchronization barriers and the constant need of
state exchange (and hence communication overhead) be-
tween the master and the worker nodes [33]. To tackle this
prevalent bottleneck of existing master-client architectures,
we present a novel ASP architecture that is completely client-
client based. Our engine, called AIR, is based on a novel
communication protocol among the worker nodes, which

 
 
 
 
 
 
we refer to “Asynchronous Iterative Routing”, to process
one or more incoming data streams in a completely asyn-
chronous manner. AIR has been designed from scratch in
C++ and is purely based on the Message Passing Interface
(MPI), which provides a low-level and highly efficient layer
of communication among the workers. It, therefore, does
not inherit any legacy design of an existing platform and
can freely be adapted to our needs. It is neither restricted to
the rather static, receiver-initiated message passing strategy
employed by Hadoop and Flink [13] nor to the actor-based
Akka API used by Spark [24], but allows for a highly multi-
threaded communication among the worker nodes directly
via channels. AIR is available as open-source release at our
GitLab repository1.

Figure 1: Extended Yahoo Streaming Benchmark (YSB)

Benchmark Setting: Figure 1 depicts an extended version
of the Yahoo Streaming Benchmark (YSB) [17], in which we
compute the ratio among “Click” and “View” events of ads
belonging to various campaigns over a given sliding window
(e.g., 10 seconds). As opposed to the original YSB setting, which
uses only a single sliding-window aggregation, we first split the
generated events into two “Click” and “View” streams, which
are then statically joined with their respective campaign ids
and processed by two sliding-window aggregations separately.
A third sliding-window join then computes the ratio among the
two aggregated event streams over each window. AIR outper-
forms Spark by a factor of 4.3 and Flink by a factor of 3.4 at a
maximum sustainable throughput of 269 million events per sec-
ond (corresponding to 36.58 GB/sec.) on a cluster of 8 compute
nodes and an overall amount of 224 cores in this setting.

1.1 AIR Overview & Contributions
Our contributions are a new, highly multithreaded, distributed
and asynchronous dataflow processing model, along with an
implementation of the AIR framework, which allows for a
direct HPC deployment based on any underlying MPI library
(such as MPICH or OpenMPI) and workload manager (such
as SLURM or OpenStack).
• Master-less architecture: the overhead of maintaining
a dedicated master node, which periodically polls the

1https://gitlab.uni.lu/mtheobald/AIR

2

worker nodes for their current state and workload, is com-
pletely avoided by a dynamic sharding protocol called
“Asynchronous Iterative Routing”.

• Globally asynchronous transformation operators:

stateless operators, such as Map, Split and Filter, pro-
cess their tasks in a completely asynchronous manner
both (1) across the communication channels per rank as
well as (2) across the ranks within a cluster.

• Locally asynchronous sliding-window operators:

stateful operators, such as Reduce, Join, and Aggregate,
use a combination of (1) asynchronous local preprocess-
ing and (2) synchronous global processing, which thus
globally need to synchronize only per sliding window (to
guarantee correctness) but overall remain asynchronous
across different windows.

• Multithreaded channel processing: communication

channels among ranks are implemented on top of the MPI
APIs in a highly multithreaded manner, thus contributing
to an increased core utilization per dataflow operator.
• Pipelining: for stateless operators, such as Map, Split
and Filter, MPI communication may optionally be re-
duced by a direct form of pipelining messages between a
sending thread’s outgoing message queue and a receiving
thread’s incoming message queue.

2 BACKGROUND & PRELIMINARIES
Before we introduce the detailed architecture of AIR, we
briefly review the main design differences among the ma-
jor DSPS platforms, which are most notably represented
by Apache Flink and Spark as open-source engines. While
Flink (as the successor of Stratosphere [4]) has been designed
from scratch for stream processing, the roots of Spark clearly
are in batch processing. Only as of Spark 2.3, a notion of
“continuous streams” [29] has been introduced to Spark’s
principal architecture. We therefore still list Spark as a bulk-
synchronous architecture, although the exact boundaries
among the two paradigms are increasingly vanishing. In ad-
dition to the DSPSs listed in the following two sections, there
exist a plethora of more systems [1, 5, 7, 8, 14, 19, 21, 30, 32],
some of which are specifically designed to exploit HPC-style
multi-core architectures [3, 23, 25].

2.1 Bulk-Synchronous Processing (BSP)
BSP engines divide a potentially unbounded stream of input
data into manageable batch sizes before performing their
logical operations on these batches. BSP systems therefore
need to synchronize their computations between each two
subsequent batches. Also, sliding-window operators, such
as aggregations or joins, are aligned with the batch sizes,
such that the consideration of time-stamps and out-of-order
events within these operators are not a principal concern.

Apache Storm is a real-time DSPS written in Java and Clo-
jure [31], which follows a classical master-client architecture.
Once the topology of the application is defined in the form
of a DAG of operators, the master specifies the computation
pipeline and schedules the tasks to the workers. In addition,
the master has to coordinate each worker process by consid-
ering this static topology specification. The worker nodes
monitor their processes and periodically send their current
state to the master by using a so-called “heartbeat” protocol.

Apache Spark is a major platform for distributed compu-
tations (including stream processing) written in Java and
Scala [37]. Distributed computations are based on Resilient
Distributed Datasets (RDDs), which essentially are distributed
in-memory data structures. Spark Streaming is a major ex-
tension of the core Spark APIs, in which an input stream is
treated as a sequence of micro-batches, then called a DStream
(i.e., a sequence of individual RDDs) [29]. Also Spark follows
a master-client architecture, where a driver program is re-
sponsible for creating the Spark Context and for scheduling
tasks across the worker nodes (then called “task managers”).
The Spark Context connects to the underlying cluster man-
ager which is responsible for resource allocation between
applications. Once connected, Spark executes tasks within
the task managers, which perform processing and data stor-
age (largely equivalent to Storm workers), and communicate
their results back to the Spark Context.

Thrill is a C++ prototype of a general-purpose BSP archi-
tecture developed at KIT (Karlsruhe) [6]. Similarly to Spark,
Thrill’s data model uses Distributed Immutable Arrays (DIAs),
in which operations are only permitted to the array as a
whole. DIAs act as abstract entities flowing between the op-
erations, thus allowing for optimizations such as pipelining
or chaining. Thrill provides an SPMD (Single Program, Mul-
tiple Data) execution model, similar to MPI, where the same
program is executed on different machines in parallel.

2.2 Asynchronous Stream Processing (ASP)
Recently, ASP engines have been designed in the spirit of
a so-called kappa-architecture [18], in which the granular-
ity of an incoming data stream is largely independent of the
granularity of the analytical tasks performed over the stream.
Although, also here, an incoming data stream is usually dis-
cretized into conveniently sized messages with respect to the
underlying network protocol (such as TCP-IP), the aggrega-
tions performed over a sliding window of incoming messages
are largely decoupled from the actual message sizes. With an
ASP architecture, the consideration of individual event-times
and the handling of out-of-order events plays a key role for
the correct execution of sliding-window operators, such as
aggregations or joins.

Apache Flink is a hybrid processing platform written in
Java and Scala [9]. The core of Flink is a stream processing
engine, thus allowing for batch processing as a special case.
Like most DSPSs, Flink follows a master-client architecture.
Upon receiving a job, the job manager (i.e., the master) gener-
ates the corresponding tasks and assigns them to the workers.
In addition, the job manager maintains the state of all exe-
cutions and the status of all worker nodes. Task Managers
(i.e., the workers) perform tasks assigned by the job manager
and exchange information with other workers when needed.
Each task manager provides a number of processing slots to
the cluster which are then used to perform tasks in parallel.
The abstraction for streams in Flink is called DataStream,
which is a sequence of partially ordered records.
Apache Samza [28] a distributed stream processing sys-
tem that supports stateful operators and adopts a unified
design for both real-time stream as well as batch processing
along the given dataflow structure. The overall framework
is tightly coupled with the Apache Kafka messaging system.
While Kafka can be used by many stream processing sys-
tems, Samza is designed specifically to take advantage of
Kafka’s infrastructure regarding fault tolerance, buffering,
and persistency.
Naiad [22, 26] is a distributed high-performance dataflow
framework developed at Microsoft Research. The core of
Naiad is implemented in C#. It follows a master-client ar-
chitecture where workers exchange messages locally using
shared memory, and remotely using a TCP connection. It
follows a new computational model called “timely dataflow”,
which extends traditional incremental computations to allow
arbitrarily nested iterations. Moreover, it enriches dataflow
computations with timestamps that represent logical points
in the computations which provide the basis for an efficient,
lightweight coordination mechanism.
The Dataflow [2] and Millwheel [1] frameworks by Google,
finally, introduce a number of seminal features such as the
“sessionization” of streaming events that belong to a same
operational unit (such as a user session) and a “watermarking”
technique to handle out-of-order events that fall into a same
sliding window.

2.3 HPC Resource Allocation
User access and resource management in an HPC cluster
usually are facilitated by a workload manager like Moab,
Torque, OpenStack or SLURM. In the following, we briefly
review the key concepts of SLURM, which is a default open-
source solution deployed on many clusters.

SLURM2 is a workload and task management middle-layer
for Unix/Linux-based cluster deployments. SLURM users

2https://slurm.schedmd.com

3

Figure 2: Event-time latency calculation of windowed events

primarily issue jobs via the salloc, sbatch and srun com-
mands. The former two commands create a resource allo-
cation, while the latter performs the actual task allocation
to then run a job in multiple, distributed tasks. Typically,
salloc allocates the requested resources for just a single
job, while sbatch is used to submit an entire batch of jobs
(e.g., a shell script of jobs) to be executed under the same
resource allocation. The key parameters (amongst various
others) used to control both salloc/sbatch and srun are
summarized below:
• -N: number of compute nodes
• -n: number of tasks per node
• -c: number of CPUs per task (for multithreading)
• -O: share (i.e., “overcommit”) CPUs among tasks
By default, srun inherits the pertinent arguments of salloc
and/or sbatch and then runs a job in as many tasks as spec-
ified by the -n argument. This default behavior of srun may
be overridden by the -O option, where the -n parameter of
srun may be greater than given by the actual resource al-
location. This allows more tasks to be launched on a single
allocation, thus sharing their CPUs equally, which is partic-
ularly effective when tasks are internally multithreaded (as
it is the case for AIR).

By controlling the above parameters, one may thus grad-
ually move from a centralized, single-threaded execution
(e.g., using srun -N1 -n1 -c1 with a single node, task
and CPU) to a distributed and multithreaded execution (e.g.,
using srun -N2 -n4 -c4 with 2 nodes, running 2×4=8
tasks and utilizing 4 CPUs per task). In “overcommit” mode
(e.g., using sbatch -N2 -n4 and then srun -n16 -O) mul-
tiple tasks (2 per node in this case) execute concurrently
on a single resource allocation. SLURM thus allows for a
very fine-grained configuration of user sessions and, once

granted, also guarantees exclusive access to the requested
resources, which makes it an ideal testbed for systematic
scale-out tests.

2.4 Performance Metrics
Stream Processing Systems (SPSs) are typically evaluated
based on two metrics, namely throughput and latency [17].
Event-Time. Two notions of timestamps are commonly used
in SPSs: event-time and processing-time. Since we rely on in-
tegrated data generators (rather than on an external message
brokers such as Kafka), we resort to considering only one
notion of timestamps in the remainder of this paper, namely
the wall-clock time at which an event is first ingested into the
SPS, and we refer to this as the event-time of the respective
event that has been ingested. Using this notion of the event-
time thus excludes delays that may arise due to the usage
of external data sources or intermediate message brokers.
Conversely, the event-time of a windowed event is defined as
the maximum event-time of all events that contributed to
that windowed event.
Latency. The event-time latency of a windowed event is de-
fined as the difference between the wall-clock time at which
the output event is released by the operator and its event-
time. Figure 2 shows the event-time latency calculation of a
windowed aggregation, where the input is a set of key-value
pairs which each are assigned a timestamp when they are
first ingested into the SPS. During the windowed aggregation
phase, the sum of the values corresponding to a specific key
within a sliding window of 10 seconds is calculated. Once
all values corresponding to a key within this window have
completed the aggregation, their result is released as a new
windowed event whose timestamp is set to the maximum
event-time of all events that contributed to it.

4

Throughput. The throughput of an SPS is defined as the
number of events processed by the system per time unit
(typically one second). Thus, to measure the performance of
an SPS, it is key to determine the maximum throughput it can
handle without exhibiting an undue increase in the queueing
(or even dropping) of messages which may in turn lead to a
substantial increase in latencies (or even incorrect results)—
an effect referred to as “backpressure” in [20]. This maximum
throughput value then is called the sustainable throughput
for that particular deployment [15] (see Section 5.2 for how
we determined this value for the systems we compare in the
experiments).

Clock Synchronization. Accurately measuring and com-
paring event-times in a distributed setting requires a care-
ful synchronization among the local clocks of all compute
nodes. We use the MPI_Wtime method of MPI to obtain the
wall-clock times of the local compute nodes, which are syn-
chronized by an NTP server with a guaranteed deviation of
less than 1 ms.

3 AIR ARCHITECTURE
The core of AIR is implemented in ANSI C++ using only
the Message Passing Interface (MPI) and POSIX Threads
(pthreads) APIs, two long-standing libraries for inter-process
communication and multithreading, respectively, as addi-
tional libraries. AIR can thus be deployed on any workload
manger with a pre-installed MPI package and requires no fur-
ther dependencies. As shown in Figure 3, one AIR instance
directly corresponds to one task obtained from the SLURM
resource allocation. Parallelism is achieved by (1) allocating
multiple worker nodes in a cluster, (2) assigning multiple
tasks per node, and (3) assigning multiple CPUs to each task.
While the former two are facilitated by initializing a sepa-
rate MPI process (and corresponding AIR instance) for each
task, the latter is facilitated via the pthreads library and a
respective multithreaded implementation of AIR’s dataflow
operators. Once instantiated, the AIR instances run in a com-
pletely autonomous manner and thereby do not need to rely
on a centralized master node that controls the worker nodes.

3.1 Physical Architecture
3.1.1 Cluster Deployment (SLURM). To use AIR, developers
write a dataflow program that implements the high-level con-
trol flow of their application in C++, thereby inheriting the
features of the core classes (such as Dataflow and Vertex,
described in detail in Section 3.2) of AIR. Based on a given
SLURM resource allocation, N × n separate AIR instances are
launched via srun, where N is the number of allocated nodes
and n is the number of allocated tasks per node. Internally,
AIR thus seamlessly integrates with the SLURM environment
via the MPI APIs, whose principal parameter, namely the

Figure 3: Overview of AIR and its HPC deployment

worldSize, is directly inherited as the number N × n of AIR
instances that have been initialized via SLURM. As shown
in Figure 3, SLURM has performed a number of task alloca-
tions such that each of the N physical compute node runs
n AIR instances with exclusive access to c CPU cores each.
As outlined in the previous section, the “overcommit” (-O)
option may further be utilized to also grant shared access to
these resources among multiple AIR instances.

Inter-Process Communication (MPI). AIR by default
3.1.2
uses the MPICH3 C/C++ libraries but can also dynamically
be linked with other MPI libraries such as OpenMPI or MVA-
PICH. Via MPI, each AIR instance has a holistic view of the
entire set of allocated tasks and can thus reroute messages
from itself to any other instance without depending on a
centralized master node.
Initialization. Two static MPI functions, MPI_Comm_size
and MPI_Comm_rank, return the worldSize (i.e., the overall
number of available tasks across all compute nodes) and the
rank (i.e., the unique index of the current task), respectively.
As shown in Algorithm 1, both parameters are used to initial-
ize a Dataflow class which represents the main entry point
to the AIR core libraries. The worldSize conforms to the com-
bination of -N and -n parameters of SLURM specified when
launching the AIR binary via srun (e.g., issuing srun -N2
-n4 -c4 ./AIR would result in a worldSize of 8 where each
task has exclusive access to 4 CPU cores).
Communication. MPI provides two blocking methods, MPI
_Send and MPI_Recv, for sending and receiving binary mes-
sage blocks among ranks in the network. Both allow for spec-
ifying the target and source ranks as well as a unique tag
that needs to be matched among a sent and received message.

3https://www.mpich.org

5

We remark that MPI provides also two non-blocking counter-
parts, MPI_Isend and MPI_Irecv, which provide their own
internal threading mechanism to asynchronously send and
receive messages. For AIR, we however do not rely on the
latter two, since we wrap MPI_Send and MPI_Recv into our
own multithreaded communication protocol which allows us
to precisely control the queuing and processing of incoming
and outgoing messages.
Channels. All pairs of ranks (including pairs of same ranks)
may communicate among each other by exchanging mes-
sages. A basic channel is a pair of ⟨sourceRank, targetRank⟩
indices, in which each channel has a unique sender and re-
ceiver. Channels are not directly available in MPI, but a simi-
lar form of determinism in the communication is achieved
by encoding the channel information into the message tag
used by the MPI_Send and MPI_Recv calls. In the case of a
DAG-structured dataflow model, in which multiple operators
run both in a distributed and parallel manner per rank, the
notion of a channel needs to be extended by including also
the source and target ids of the operators that exchange mes-
sages among each other. We therefore define the 32-bit mes-
sage tag associated with each incoming and outgoing mes-
sage as sourceRank ≪ 24 + sourceOp ≪ 16 + targetRank ≪
8 + targetOp.

Intra-Process Multithreading (pthreads). AIR employs
3.1.3
multithreading to control (i) the queuing of incoming mes-
sages, (ii) the processing of the received messages, and (iii)
the sending of outgoing messages at each dataflow opera-
tor and communication channel individually (see also Algo-
rithm 2 of the Vertex class). Specifically, pthread_create
and pthread_join are used to invoke and join (i.e., pro-
vide a synchronized termination) of these threads. More-
over, for internal thread synchronization, we rely on the
pthread_mutex_lock, pthread_cond_wait and pthread_
mutex_unlock methods to take a lock on a mutex variable,
wait for a signal being sent to a conditional variable, and
release the lock on a mutex variable, respectively. The lat-
ter three methods are used to implement producer-consumer
patterns between (i-ii) the thread receiving the incoming
messages and the thread-processing these messages, and be-
tween (ii-iii) the processing thread and the thread sending
the outgoing messages at each channel.

3.2 Logical Architecture
3.2.1 Dataflow Programs. AIR adopts the notion of dataflow
programs [16] in which a DAG-structured execution plan
of logical data transformations is compiled into a physical
execution plan of stream-processing operators. Formally, a
dataflow is a multi-rooted, directed acyclic graph (DAG) in
which the vertices represent the transformations we wish
to execute against one or more incoming data streams with

6

respect to the topology of the DAG. The roots of the DAG
are so-called sources, which implement C/C++ interfaces to
the streaming data sources (e.g., to message brokers such
as Kafka or to other web services via CURL), large files, or
custom data generators. Conversely, the leaves of the DAG
are so-called sinks, which usually write the aggregated results
of the dataflow transformations into a persistent storage
(such as Redis or output files), or which in turn provide
streaming access to other SDPSs. Internal vertices of the
DAG represent either unary (such as Map, Reduce) or binary
(such as Join) transformations that each connect to either
one or two input streams and produce one or more output
streams. The logical architecture of AIR is captured via the
pseudocode provided in Algorithms 1–4, which we describe
in detail in the following steps.

Dataflow. The Dataflow class depicted in Algorithm 1 serves
as the main entry point to create a number of concurrent
AIR instances. After a SLURM session has been established
with the desired number of nodes, tasks per node, and CPU
cores per task, one instance of the Dataflow class is created
per SLURM task. This is indicated by the main method in
Algorithm 1, which is merely parameterized by the number
of desired AIR tasks. Next, each Dataflow instance reads the
current worldSize and rank parameters from the MPI envi-
ronment and thereby runs each AIR task in a shared-nothing
manner. Each Dataflow instance has access to the topology
of the entire dataflow program by its vertices internal vec-
tor of interlinked dataflow operators (subclasses of Vertex).
The streamProcess method then in parallel invokes the
three types of threads by calling the listeningThread, pro-
cessingThread and sendingThread thread entry points—
one per vertex instance and communication channel.

Vertex. The Vertex class depicted in Algorithm 2 imple-
ments the operators involved in the dataflow program and
thereby also facilitates the communication among these op-
erators in a distributed deployment of AIR. According to the
topology of the dataflow DAG, each vertex is initialized with
indegree × worldSize many incoming channels (i.e., one per
incoming vertex and rank) and outdegree × worldSize many
outgoing channels (i.e., one per outgoing vertex and rank).
The respective inChannels and outChannels vectors thus
represent mechanisms both for thread synchronization and
the queueing of incoming and outgoing messages. The actual
synchronization and communication logic is inherited and
thus identical for all subclasses of Vertex. Correspondingly,
one listening and processing thread is created per incoming
channel, while one sending thread is created per outgoing
channel (as indicated by the respective methods of Vertex).
The threads and respective message queues are synchronized
via producer-consumer patterns to decouple the message
passing from their processing as much as possible.

Algorithm 1: Dataflow pseudocode of AIR

Algorithm 2: Vertex pseudocode of AIR

void main(int #tasks) {

// Initialize one instance of Dataflow per task
for i from 1 to #tasks in parallel do {
Dataflow dataflow = new Dataflow();
dataflow.streamProcess(); }

}

class Dataflow {

// Basic MPI parameters

int worldSize, rank;

// Vector holding the dataflow operators
vector<Vertex> vertices;

// Initialize MPI environment at current task
Dataflow() {

MPI_Init_();
MPI_Comm_size(worldSize);
MPI_Comm_rank(rank);

}

// Main entry point to invoke stream processing
void streamProcess() {

// Invoke listener, processing & sender threads per vertex
for each vertex in vertices in parallel do {

vertex.startThreads(); }

}

}

Listening Thread. The listening thread of each incoming
channel uses the blocking method MPI_Recv and fills the
inChannel message queue with one incoming message per
iteration of the embracing while loop. The received message
must match the message tag computed from the channel
information that each thread processes (see Section 3.1.2).
Upon having received a message, the listening thread notifies
the processing thread at the same channel and immediately
turns into the next iteration to receive a new message.
Processing Thread. Once notified by the listening thread,
the processing thread pops each incoming message from its
assigned inChannel and calls the process stub, which must
be overwritten by each subclass of Vertex with the intended
logic of that subclass (see Section 4 for various operator spec-
ifications). We remark that process has access to both the
incoming message as well as to an entire vector outMessages
of outgoing messages. In the final step of each processing
iteration, the new outgoing messages are placed into the
outgoing message queues associated with the outChannels
vector, and the respective sending threads are notified.
Sending Thread. Once notified by the processing thread,
the sending thread pops each message from its assigned
outChannel queue and uses the blocking method MPI_Send
to send the outgoing message to the rank associated with
this channel. After the message has been successfully sent,
the message can also safely be deleted by this thread.

class Vertex {

// Alive flag for threads
bool ALIVE = false;

// Channels for thread synchronization & MPI protocol
vector<Channel> inChannels;
vector<Channel> outChannels;

void startThreads() {

ALIVE = true;

// Invoke listener & processing thread per incoming channel
for each inChannel in inChannels in parallel do {
pthread_create( listeningThread(inChannel) );
pthread_create( processingThread(inChannel) ); }

// Invoke sender thread per outgoing channel
for each outChannel in outChannels in parallel do {
pthread_create( sendingThread(outChannel) ); }

}

void listeningThread(inChannel) {

while (ALIVE) {

inMessage = new Message();
MPI_Recv(inMessage);
inChannel.push_back(inMessage);
inChannel.notify(); }

}

void processingThread(inChannel) {

while (ALIVE) {

inChannel.cond_wait();
inMessage = inChannel.pop();

// Vector of outgoing messages, one per outgoing channel
vector<Message> outMessages;

// Actual process method will be overwritten by subclass
process(inMessage, outMessages);

// Asynchronous iterative routing of outgoing messages
for i from 1 to outChannels.size in parallel do {

outChannel[i].push_back(outMessage[i]);
outChannel[i].notify(); }

outMessages.clear();
delete inMessage; }

}

void sendingThread(outChannel) {

while (ALIVE) {

outChannel.cond_wait();
outMessage = outChannel.pop();
MPI_Send(outMessage);
delete outMessage; }

}

}

All threads remain alive until some external termination
signal is sent (also via messages), which indicates that the
execution of AIR should be completed.

3.3 Asynchronous Iterative Routing
As outlined in the previous subsection, the processing thread
at each incoming channel has access to an entire vector of
outgoing message queues—one for each outgoing channel.

7

Each processing thread therefore is able to “cross” channels
and thereby dynamically reroute messages to all ranks, in-
cluding itself, in the MPI environment (or tasks in SLURM).
Since all threads asynchronously receive, process and send
messages, we refer to this technique as “Asynchronous It-
erative Routing” (AIR). The exact routing strategy is both
workload- and application-dependent; a simple Map transfor-
mation may simply reroute data elements according to a
random sharding technique such as value % worldSize, while
a Split operation will have to separate the data values and
thus route each selected value to a different branch in the
dataflow DAG (compare to Figure 1 from the Introduction).
A Join operation, finally, needs to take the respective join
keys into account to make sure that tuples with the same join
keys are indeed routed to the same ranks. The exact routing
strategy thus is part of the process method implemented by
each subclass of Vertex.

3.4 Pipelining
Once one or more incoming data streams have been evenly
routed to all ranks, most data transformation operators such
as Map or StaticJoin do not actually need to perform rerout-
ing to guarantee a correct execution of the dataflow program.
Pipelining thus is a global flag associated with each opera-
tor in the dataflow DAG. At each rank, this flag enables
an operator to directly place an outgoing message from its
outChannel outgoing message queue into the inChannel
incoming message queue of the next operator in the DAG’s
topology. Thus, pipelining is able to circumvent MPI com-
munication in order to directly exchange messages between
operators within a same rank, which may substantially ac-
celerate performance when network communication is not
needed to actually redistribute data.

4 DATAFLOW OPERATORS
4.1 Data-Transformation Operators
Data-transformation operators, such as Map, Filter, Split,
StaticJoin, etc., take one event at a time as input and per-
form an associated transformation function on each of these
events individually. These one-to-one transformations thus
process events independently of each other and therefore do
not require synchronization. Also timestamps can be ignored
inside the logic of these operators and are merely forwarded
to the sliding-window operators. Therefore, we refer to these
operators as stateless, i.e., they do not need to maintain any
context information about the sliding window to which each
of the processed events belongs.

Map. The Map class is a subclass of Vertex and therefore
inherits all of the latter’s mechanisms for communication
via channels and for thread synchronization. It is initialized

Algorithm 3: Map pseudocode of AIR

class Map : Vertex {

// Function associated with this vertex subclass
Function function;

// Initialize Map with actual function to calculate
Map (function) {

this->function = function;

}

void process(inMessage, outMessages) {

// Deserialized partition for current message
Partition partition = deserialize(inMessage);

// Transform all events inside this partition
for each event in partition do {

event.value = function.calculate(event.value); }

// Serialize partition & send messages
outMessages = serialize(partition);

}

}

Algorithm 4: Reduce pseudocode of AIR

class Reduce : Vertex {

// Aggregate function associated with this vertex subclass
AggregateFunction function;

// Map from event-times to partitions
Map windowedPartitions;

// Window size associated with this operator
int windowSize;

// Initialize Map with actual function to calculate
Reduce (function, windowSize) {
this->function = function;
this->windowSize = windowSize;
this->windowedPartitions = new Map();

}

void process(inMessage, outMessages) {

// Deserialized partition for current message
Partition partition = deserialize(inMessage);

// Get windowed partition of each event or create new partition
for each event in partition do {

wId = event.timestamp / windowSize;
wPartition = windowedPartitions.getOrElse(wId,

new Partition());

function.combine(wPartition, event.value); }

// Serialize & send each complete windowed partition
for each wPartition in windowedPartitions do {

if wPartition.isComplete

outMessages = serialize(partition); }

}

}

with a dedicated function that implements the actual calcu-
lation associated with Map. The pseudocode for Map is given
in Algorithm 3. Since Map is a one-to-one transformation
(e.g., applying “times-two” as transformation function), each

8

Figure 4: Map with an in- and outdegree of 1

Figure 5: Split with an outdegree of 2

element of a deserialized data partition that is obtained from
an incoming message is applied to that function at a time
(or in parallel). The result of each calculation is in turn se-
rialized back into the outgoing messages (one per outgoing
channel), which are then queued and sent by the respective
sender threads in an asynchronous manner—the key idea
for what we refer to “Asynchronous Iterative Routing” (see
Section 3.3). A typical Map operator takes the value associ-
ated with an event and applies a random sharding strategy
by serializing the event into the outgoing message with in-
dex value % worldSize. If pipelining is enabled, however, all
values are directly serialized into the outgoing message with
index rank at the current rank and directly copied into the
incoming message queue of the subsequent operator accord-
ing to the topology of the dataflow program. Figure 4 depicts
the internal architecture of the Map operator, where li , pi
and si indicate listening, processing and sending threads,
respectively (w corresponds to the worldSize parameter)

Filter. This is a special case of Map, where the function
associated with the operator takes a deserialized data ele-
ment as input and returns a Boolean value. Based on the
Boolean value returned, the processing thread determines if
that data element should be serialized or not into the outgo-
ing messages. Filter behaves analogously to Map in terms
of routing and pipelining.

Split. Also this operator is similar to Map. However, here
events are distributed according to two or more splitting
conditions. For routing, the index of an event consisting of
a ⟨key, value⟩ pair in the outgoing message vector is com-
puted as index(key) × worldSize + value % worldSize, where
index(key) denotes the index of a splitting condition among
the operator’s successors in the topology of the dataflow
program. The architecture of the Split operator with an
outdegree of 2 is depicted in Figure 5.

9

StaticJoin. This operator joins each incoming event with
a static map (e.g., read from an external file that is shared
with all worker nodes) which is kept in the main-memory of
all worker nodes. The augmented events are then serialized
into the outgoing messages, where an analogous strategy
regarding routing and pipelining as for Map is used.

4.2 Sliding-Window Operators
Sliding-window operators, such as Reduce, Aggregation,
Join, etc., group multiple events, namely those that fall into
the same sliding window according to their timestamps, in or-
der to perform their associated aggregation or join operation.
Since, especially in an ASP architecture, sliding windows can-
not be assumed to be aligned with the batch or message sizes,
these operators require synchronization by the respective
window ids. For the same reason, we refer to these operators
as stateful, i.e., they need to maintain an in-memory data
structure (usually a hash map) across all processing threads
inside each operator, which stores intermediate results that
fall into a same window. Consequently, multithreaded access
to this data structure needs to be synchronized via a mutex
to guarantee correct results.
Reduce. The Reduce class is a subclass of Vertex and there-
fore inherits all of the latter’s mechanisms for communi-
cation via channels and for thread synchronization. It is
initialized with an AggregateFunction that implements the
actual calculation associated with Reduce. The pseudocode
for Reduce is given in Algorithm 4. Since Reduce is a many-
to-one aggregation (e.g., applying “sum” as aggregation),
multiple deserialized elements obtained from the incoming
messages need to be merged into a single result. Moreover,
if elements of a sliding window are spread across multiple
messages (which is typically the case in an asynchronous
setting), Reduce may only release the results of each sliding
window once it knows that all elements of that window have

been aggregated. The result of each window is in turn seri-
alized into one of the outgoing messages (one per outgoing
channel). The routing and pipelining of outgoing events is
again analogous to Map.
Aggregation. This operator is similar to Reduce except that
events consist of ⟨key, value⟩ pairs, and the aggregate func-
tion is performed to all values that fall under the same key.
Aggregation uses an analogous strategy regarding routing
and pipelining as Reduce, however using keys instead of
values.

Figure 6: Join with an indegree of 2

Join. The Join operator implements an equi-join among
two streams of incoming events with compatible ⟨key, value⟩
pairs. An analogous strategy regarding routing and pipelin-
ing as for Reduce and Aggregation is used also here. Fig-
ure 6 depicts the internal architecture of the Join operator
with an indegree of 2.

4.3 Local vs. Global Aggregations
For stateful sliding-window operators, such as Reduce, Aggre-
gation or Join, it is generally beneficial to first perform a
local pre-aggregation among events consisting of ⟨key, value⟩
pairs, during which the events are first rerouted among all
ranks according to their keys in order to improve parallelism.
In a subsequent global aggregation step, the pre-aggregated
events are then once more rerouted based on their window
ids (calculated from their timestamps) to fully aggregate and
synchronize these events with respect to the sliding windows
they belong to. For commutative and associative aggregation
functions, such as sum, this form of pre-aggregation does
not affect the results but helps to increase performance. In-
ternally, the pre- and full-aggregation steps are thus imple-
mented as two stateful operators, which each employ two
levels of nested hash maps. In the outer map, the keys are the
window ids; while in the inner maps, the keys are the key
entries of the respective events. This form of nested hashing

is outlined in the process method of Reduce (Algorithm 4),
where first a wId is computed from the timestamp of an event,
whereupon the event is then merged and aggregated into
the respective partition of events that fall into this window.

5 EXPERIMENTS
The experiments we run with AIR in comparison to Spark
and Flink are three-fold. First, we begin by determining the
sustainable throughput under the given HPC setup for all
systems under investigation: AIR, Spark and Flink, which re-
quires many repeated runs at different throughput levels. For
AIR, we need to repeat these runs for each choice of the -N,
-n and -c parameters, while for Spark and Flink the best setup
is usually given by allocating all resources for a given num-
ber of nodes (thus only the -N parameter is relevant for these
systems). Second, based on the sustainable throughput val-
ues we obtained from the first step, we perform a systematic
scale-out test of AIR on our HPC cluster under various task-
allocation parameters. Third, we compare the sustainable
throughput rates of the three systems under three different
use-cases, namely (1) a Simple Windowed-Aggregation (SWA),
(2) the original version of the Yahoo Streaming Benchmark
(YSB) [11], and (3) the extended version of YSB (called YSB*)
as it is depicted in Figure 1.

5.1 General Setup & Benchmarks
Each node of our HPC cluster is equipped with two 2.6 GHz
Intel Xeon Gold 6132 CPUs, 28 cores and 128GB RAM. All
nodes run CentOS Linux (v7) and are managed by SLURM
(v19.05.3). The network bandwidth is 100Gb/s (Infiniband
EDR). The benchmark implementations of AIR are complied
with GCC (v6.4.0) with -O3 optimization. The only additional
libraries are Intel MPI (v18.0.1) and the default POSIX Thread
API of GCC. We use Apache Flink (v1.6.1) and Apache Spark
Streaming (v2.3.2) as our baseline DSPSs. We have disabled
the checkpointing options in these systems to minimize their
additional overheads. The Scala (v2.11.12) implementations
of the YSB benchmarks run on top of JDK v1.8.0. All scripts
used for Spark and Flink are also available via our GitLab
repository4.

Simple Windowed-Aggregation (SWA). The first bench-
5.1.1
mark consists of an in-memory data generator and a single
windowed aggregation. We reuse the same data generator
as the one implemented for YSB (see next subsection) to
generate variable length events (on an average 136 bytes) at
various throughput rates. The windowed aggregation then
groups events over a 10-second sliding window and counts
the number of events produced by the generator. For Spark

4https://gitlab.uni.lu/vvenugopal/streambenchmarks

10

and Flink, we rely on their default grouping and aggrega-
tion operators (such as groupBy(.), keyBy(.), agg(.)) for
streams. The data generator is directly implemented in the
native runtime of all systems (Scala in the case of Spark and
Flink). Just like any other operator, it runs in a distributed
manner and generates the input events in multiple tasks in
parallel. In AIR, this dataflow contains 3 operators, namely
Generator, Aggregation plus a final sink node that writes
the results produced by all ranks into a single file.

5.1.2 Yahoo Streaming Benchmark (YSB). The original Ya-
hoo Streaming Benchmark [11] simulates a simple advertise-
ment-based analytics pipeline, where a stream of ad events is
consumed from a message broker, and the application com-
putes a sliding-window aggregation of ads being mapped uni-
formly across 100 distinct campaigns that have been "viewed"
by a user. In recent streaming benchmark designs (as pro-
posed by the Flink and Spark communities) [17, 36], to ac-
curately measure the performance of the DSPSs, distributed
in-memory data generators are utilized instead of external
message brokers such as Apache Kafka [34]. Therefore, we
employ the modified YSB implementation given in [12] to
avoid potential bottlenecks due to external message brokers
like Kafka or Redis for storage. This guarantees that the
data-generation rate is able to actually keep up with the
data-ingestion rate of the streaming system up to very high
throughputs and at the same time eliminates latencies due
to these external components. In AIR, the YSB dataflow con-
tains 5 operators, namely Generator, Filter, StaticJoin,
Aggregation, and again the sink node to store the results.

5.1.3 Yahoo Streaming Benchmark (YSB*). As mentioned in
the previous subsection, the original YSB follows a simple
pipeline, where we have only one windowed aggregation,
while the join from ads to campaigns remains a static map-
ping based on an in-memory table that is shared among all
worker nodes. To make the use-case more challenging (with-
out affecting the basic benchmark setup), we modified the
YSB dataflow as shown in Figure 1. In this modified form
of YSB, called YSB*, events are first split into two streams:
one corresponding to “Click” events and another one cor-
responding to “View” events. The two event streams then
each are transformed in a similar manner as in YSB. The
counts of view- and click-events per campaign are calculated
by two independent windowed aggregations. Further, we
introduce a third windowed operator, which joins the cor-
responding events produced by the previous aggregations
to finally determine the ratio of click- and view-events per
campaign.

In this extended use-case, the major challenge consists
of performing two windowed operations in parallel and to
then join their results by a third windowed operation: first
compute the counts of click- and view-events individually,

11

then compute their ratios. The streaming API of Flink has a
Join function for combining two streams and also supports
multiple window-based operations in a sequence. However,
Spark (as of its recent 2.4 version) does not support multiple
window-based operations. Therefore, we have utilized the
multiple simultaneous aggregation5 feature in the structured-
streaming API to implement YSB* in Spark [10]. In AIR, this
use-case is implemented using the 9 operators shown in
Figure 1) (just the final sink node that writes the results from
all ranks into a single file is not shown in the figure). Most
of these operators have the same functionality as we saw
in the YSB use-case already—except for the Join which is a
new kind of a windowed operator that comes into play here.

5.2 Sustainable Throughput
To determine the sustainable throughput (ST) for a given clus-
ter deployment, we run each system with a low throughput
first and gradually increase the throughput until the sys-
tem shows back-pressure. Back-pressure in these systems is
identified based on the following observations:
• For Spark and AIR, the system is said to be in back-pressure
if a sudden raise in (average) event-time latency is ob-
served while increasing the input throughput beyond a
sustainable range. The green lines in Figures 7 and 8 depict
the ST values for Spark and AIR, respectively.

• For Flink, it is observed that—unlike in Spark and AIR—
the (average) event-time latency does not raise abruptly
when the throughput goes beyond the sustainable value.
Instead, we see a sudden decline in the count of processed
windows. Therefore, in addition to the latency value, the
ST is determined by considering the window count as well.
Figure 9 shows the correlation of latency with the count
of processed windows under different throughput values.

The sustainable throughput is specific to each workload and
cluster deployment of a system. We ran the YSB benchmark
on N = 2 nodes for 300 seconds, thus processing 30 sliding
windows of 10 seconds each to compute the average latency
among the aggregated results. For Spark, while increasing
the throughput from 5M to 12M events per second, we can
observe a sudden rise in the average latency from 877 ms
to 5,939 ms, which steadily keeps increasing from there on.
For AIR (using N = 2, n = 2 and c = 4), we made a similar
observation when the throughput was around 24M with a
latency of 1,423 ms. Flink remains more stable with respect
to average latency. However, the number of windows pro-
cessed suddenly drops to 786 from 2,999 at a throughput of
13.5M events per second, which indicates that data is actually
dropped.

5Performing multiple aggregation operations simultaneously on a single
time-window based group than performing those aggregations on multiple
consecutive time-window based groups

Figure 7: ST for Spark on 2 nodes

Figure 8: ST for AIR on 2 nodes

·104

)
.
s

m

(
y
c
n
e
t
a
l

.

g
v
a

4

2

0

·104

)
.
s

m

(
y
c
n
e
t
a
l

.

g
v
a

4

2

0

10

0
throughput (in 106 events/sec.)

30

20

40

10

0
throughput (in 106 events/sec.)

20

30

40

Figure 9: ST for Flink on 2 nodes

8,000
6,000
4,000
2,000
0

)
.
s

m

(
y
c
n
e
t
a
l

.

g
v
a

0

50

100

150

4,000
3,000
2,000
1,000
0

d
e
s
s
e
c
o
r
p
s
w
o
d
n
i
w

throughput (in 106 events/sec.)

5.3 Benchmark Setting I: Different AIR

Configurations

Setup. This setting exclusively investigates the behaviour of
AIR under different configurations and workloads. Depend-
ing on whether pipelining (P) and overcommit (O) are en-
abled or not, we have four variants of AIR, namely AIR, AIR-
P, AIR-O and AIR-OP. These four variants of AIR are evalu-
ated against the three use-cases described in Sections 5.1.1–
5.1.3 under each choice of SLURM’s -N, -n and -c parameters.
First, we study the impact of the -n parameter (the number
of tasks) by varying it from 1 to 16 and by keeping both -c
and -N fixed to 1. Second, the impact of the number of CPU
cores per task, -c, is studied by varying it from 1 to 28 and by
fixing both n and -N to 1. Third, to find the best combination
of -n and c for a single node, we performed our experiments
with various combinations of these two parameters, thus
dividing the 28 cores of a compute node among the tasks.
After having determined the best combination of -n and -c
that yields the highest ST value for a single node, we used
this combination for allocating similar resources on -N com-
pute nodes and thereby determined the ST of the system for
multiple compute nodes.
Results. As shown in Figures 10 (a)–(c), by varying the setup
from 1 task per node to 16 tasks on a single node and with 1
CPU core per task, ST increases by a factor of nearly 9.5 for
all the four variants of AIR. The same pattern is observed
across all the three use-cases. The plots also show that the
overcommit option has no significant impact when the task
allocations have only a single core assigned to them.

Figures 10 (d)–(f) show the impact of the number of CPU
cores per task allocation. By keeping the number of tasks per
node constantly at 1, we increase the number of CPU cores
from 1 to 28. For AIR and AIR-P, it is observed that beyond
a particular number of cores, increasing the number of cores
has no major impact on the performance. That is, supplying
even more cores to a single AIR instance does not increase
ST. It thus becomes evident that the key to further improve is

12

to initialize more AIR instances, each associated with a their
optimal amount of CPU cores. For choosing such a setup, we
have two options, either using a large number of small (in
terms of CPU cores) task allocations at the SLURM level and
mapping one AIR instance to each such task allocations, or
having fewer task allocations on which we can then allocate
multiple AIR instances in overcommit mode. The former
option of having a large number of task allocations limits
the number of AIR instances to the number of available CPU
cores per compute node. In our case, it is typically beneficial
to have more than 28 parallel AIR instances per node since
not all cores are constantly utilized at 100%. In addition, for
complex use-cases involving lots of asynchronous operators
(each running several threads corresponding to the channels
they serve), it is generally not beneficial to allocate just one
CPU core per task allocation to yield the best performance.
Figures 10 (g)–(i) provide another insight into CPU utiliza-
tion for 4 tasks and 1 node. We could consider 4 combinations
of -n and -c to effectively divide the CPU cores for perform-
ing task allocations: (n = 1, c = 28), (n = 2, c = 14), (n = 4,
c = 7) and (n = 7, c = 4). The first two combinations could
clearly exploit the overcommit option (AIR-O and AIR-OP)
to deploy more parallel AIR instances per task allocation.
Figures 10 (d)–(f) show that when the overcommit option is
enabled, the performance improves nearly 6 times. However,
the number of parallel AIR instances on a single task alloca-
tion using overcommit cannot be supported beyond a certain
number (in our case it was 6) per task allocation. To have
more parallel instances deployed on a given node, we thus
have to increase the number of task allocations per node
while still making sure that each AIR instances is assigned
to the best number of CPU cores. In our experiment, the
combination (n = 4, c = 7), gave us the best ST values for all
the three use-cases—refer to plots (g)–(i).

In Figures 10 (j)–(l), finally, we keep c = 7 and n = 4
fixed and vary the number of nodes via -N from 1 to 8. It
is observed that all the 4 modes of AIR are scaling out very

Figure 10: ST values (in 106 events/sec.) of various AIR modes and under different HPC deployments for the 3
use-cases: SWA, YSB and YSB*

(a) SWA (c=1, N=1)

(b) YSB (c=1, N=1)

(c) YSB* (c=1, N=1)

AIR

AIR-O

AIR-P

AIR-OP

1

1

1

t
u
p
h
g
u
o
r
h
t

e
l
b
n
i
a
t
s
u
s

t
u
p
h
g
u
o
r
h
t

e
l
b
n
i
a
t
s
u
s

t
u
p
h
g
u
o
r
h
t

e
l
b
n
i
a
t
s
u
s

t
u
p
h
g
u
o
r
h
t

e
l
b
n
i
a
t
s
u
s

80

60

40

20

0

60

50

40

30

20

10

140

120

100

80

60

40

20

0

350

300

250

200

150

100

50

50

40

30

20

10

0

16

1

4

8
n (no. of tasks per node)

12

40

30

20

10

0

16

1

4

8
n (no. of tasks per node)

12

4

8
n (no. of tasks per node)

12

(d) SWA (n=1, N=1)

(e) YSB (n=1, N=1)

(f) YSB* (n=1, N=1)

60

50

40

30

20

10

0

28

1

60

50

40

30

20

10

0

28

1

8

12

4
24
c (no. of CPUS per task)

20

16

8

12

4
24
c (no. of CPUS per task)

16

20

8

12

4
24
c (no. of CPUS per task)

16

20

(g) SWA (n=4, N=1)

(h) YSB (n=4, N=1)

(i) YSB* (n=4, N=1)

100

80

60

40

20

7

1

100

80

60

40

20

7

1

3

2
4
6
c (no. of CPUS per task)

5

3

2
4
6
c (no. of CPUS per task)

5

3

2
4
6
c (no. of CPUS per task)

5

16

28

7

(j) SWA (c=7, n=4)

(k) YSB (c=7, n=4)

(l) YSB* (c=7, n=4)

300

250

200

150

100

50

7

8

1

2

250

200

150

100

50

7

8

1

2

5

4

3
6
N (no. of Nodes)

13

4

5

3
6
N (no. of Nodes)

7

8

1

2

4

5

3
6
N (no. of Nodes)

Figure 11: ST values (in 106 events/sec.) of AIR, Spark and Flink over multiple nodes: SWA, YSB and YSB*
(i) SWA

300

200

100

0

300

200

100

0

200

t
u
p
h
g
u
o
r
h
t

e
l
b
a
n

i
a
t
s
u
s

t
u
p
h
g
u
o
r
h
t

e
l
b
a
n
i
a
t
s
u
s

t
u
p
h
g
u
o
r
h
t

e
l
b
a
n
i
a
t
s
u
s

AIR-OP

SPARK

FLINK

175.2

129.2

203

229

55.5

48

61.13

60.5

73.57

63.5

74.28

69.75

75

72.9

1

2

3

4
5
N (no. of Nodes)

(ii) YSB

264

292.5

320

344

76

83.5

94.5

75.9

112

76.78

6

7

8

193

153

93

16

5.96

1

12.5

21.5

2

38.5

27.5

3

239

261

272.5

292

311

61.75

39

66

49.52

4
5
N (no. of Nodes)

(iii) YSB*

187

211

75

52

6

63

7

89.5

98

77.62

8

237.8

257.8

269

100

84.5

0

6.8

8

1

129

143

10.52

13.5

22.1

28.5

41.75

32.35

56

46

61

51

69.5

59.6

78

62.3

2

3

4
5
N (no. of Nodes)

6

7

8

well for the three use-cases. As expected, AIR-O and AIR-OP
exhibit the best ST values for all the given node counts.
Impact of Pipelining. On a single compute node, enabling
pipelining (P) while not considering the overcommit (O) op-
tion achieves up to 23% (on avg. 13.3%) higher ST values
under the SWA use-case. On the other hand, using both
pipelining and overcommit increases performance by up to
68% (on avg. 48%). Across all 8 nodes, we have very similar
observations. In SWA, pipelining can be employed only at
the Generator operator, where the messages can be directly
copied to the succeeding Aggregation operator. In the case
of YSB, for a single node, the pipelining option can optimize
the message passing at three operators: Generator, Filter
and Static-Join, which results in up to 17% (on avg. 9%)
improvement in ST, whereas enabling also overcommit in-
creases the performance by up to another 55% (on avg. 38%).

While running on 8 compute nodes, ST further improves
by to up to 69% (on avg. 61%) and up to 27% (on avg. 17%),
respectively. In the case of YSB*, enabling pipelining in its
4 stateless operators results in an increase of ST by up to
9% (on avg. 7.6%), whereas enabling overcommit in addition
increases performance by up to 52% (on avg. 41%). While
running on 8 compute nodes, ST increases by up to 65% (on
avg. 61%) for pipelining and by up to 8% (on avg. 4%) for both
pipelining and overcommit.

5.4 Benchmark Setting II: Systems under

Comparison

Setup. Finally, we compare the ST of AIR under its best
parameter setting on a given number of nodes N with the
ST of Spark and Flink under the same numbers of nodes. In

14

the case of Spark, we deploy N task-managers (each with 28
cores except one with 26 cores) and 1 master (2 cores) on a
configuration of N cluster nodes. Then we vary the number
of executors within each task-manager and its parallelism
parameters and report the highest ST we can measure. Simi-
larly, for Flink, we deploy N task-managers (workers) and
1 job-manager (master) on N cluster nodes, and vary the
number of slots within each task-manager to determine the
highest ST.
Results. The results of this setting are depicted in Figure 11.
Here, AIR outperforms Spark by a factor of up to 4.5 and
Flink by a factor of up to 3.6 for the SWA use-case. Simi-
larly, for the more complex YSB setting, AIR performs up to
15 times better than Spark and up to 5.8 times better than
Flink. For the even more complicated YSB* use-case, AIR
outperforms Spark by up to a factor of 12 and Flink by up
to a factor of 9 in terms of ST. We generally observe that
Spark loses performances relatively to Flink and AIR when
the dataflows become more complex, i.e., when more sliding-
window operators are involved, which seems to be a legacy
of the BSP architecture of Spark. We finally highlight that we
are able to process 269 million events per second in this set-
ting, which corresponds to a plain data throughput of 36.58
GB per second for an average event size of 136 bytes, when
using 8 compute nodes and 224 cores for the YSB* use-case.

6 CONCLUSIONS
As highlighted recently [38], the gap between the real-world
throughput of complex engines such as Spark or Flink and the
theoretical upper bound given by the main-memory band-
width of a single (centralized) machine still is about two
orders of magnitude. We believe that, with the design of AIR,
we found a good compromise for a light-weight, reduced
design of a DSPS that exhibits good performance and scales
well also to larger cluster deployments. The direct integra-
tion into SLURM via the MPI libraries makes AIR an ideal
backend for common HPC deployments. We intend to extend
the capabilities of AIR in particular with respect to its practi-
cal usability and thus develop more user-friendly frontends.
As for future research topics, we also plan to investigate
new mechanisms to include more explicit forms of workload
balancing and fault tolerance directly into the master-less
architecture of AIR.

7 ACKNOWLEDGEMENT
This project is funded by the University of Luxembourg,
Luxembourg. We would like to express our gratitude to Mr.
Sadi Nasib, M. Sc. student, University of Luxembourg, for his
assistance in setup in the benchmarking environment and to
Mr. Jeyhun Karimov, Ph.D. student, Technical University of
Berlin, for his valuable suggestions on configuring the Spark

benchmark. We also thank the HPC team of the University
of Luxembourg for their timely help and support.

REFERENCES
[1] Tyler Akidau, Alex Balikov, Kaya Bekiroğlu, Slava Chernyak, Josh
Haberman, Reuven Lax, Sam McVeety, Daniel Mills, Paul Nordstrom,
and Sam Whittle. 2013. MillWheel: Fault-tolerant Stream Processing
at Internet Scale. PVLDB 6, 11 (2013), 1033–1044.

[2] Tyler Akidau, Robert Bradshaw, Craig Chambers, Slava Chernyak,
Rafael J. FernÃąndez-Moctezuma, Reuven Lax, Sam McVeety, Daniel
Mills, Frances Perry, Eric Schmidt, and Sam Whittle. 2015. The
Dataflow Model: A Practical Approach to Balancing Correctness, La-
tency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Pro-
cessing. PVLDB 8 (2015), 1792–1803.

[3] Marco Aldinucci, Massimo Torquati, and Massimiliano Meneghin.
2009. FastFlow: Efficient Parallel Streaming Applications on Multi-core.
CoRR abs/0909.1187 (2009).

[4] Alexander Alexandrov, Rico Bergmann, Stephan Ewen, Johann-
Christoph Freytag, Fabian Hueske, Arvid Heise, Odej Kao, Marcus
Leich, Ulf Leser, Volker Markl, Felix Naumann, Mathias Peters, Astrid
Rheinländer, Matthias J. Sax, Sebastian Schelter, Mareike Höger, Kostas
Tzoumas, and Daniel Warneke. 2014. The Stratosphere Platform for
Big Data Analytics. The VLDB Journal 23, 6 (Dec. 2014), 939–964.
[5] Arvind Arasu, Brian Babcock, Shivnath Babu, Mayur Datar, Keith
Ito, Itaru Nishizawa, Justin Rosenstein, and Jennifer Widom. 2003.
STREAM: The Stanford Stream Data Manager (Demonstration De-
scription). In SIGMOD. 665–665.

[6] Timo Bingmann, Michael Axtmann, Emanuel Jöbstl, Sebastian Lamm,
Huyen Chau Nguyen, Alexander Noe, Sebastian Schlag, Matthias
Thrill: High-
Stumpp, Tobias Sturm, and Peter Sanders. 2006.
performance algorithmic distributed batch data processing with C++.
In BigData. 172–183.

[7] Thilina Buddhika and Shrideep Pallickara. 2016. NEPTUNE: Real Time
Stream Processing for Internet of Things and Sensing Environments.
IPDPS (2016), 1143–1152.

[8] Frank J Cangialosi, Yanif Ahmad, Magdalena Balazinska, Ugur
Cetintemel, Mitch Cherniack, Jeong-Hyon Hwang, Wolfgang Lindner,
Anurag S Maskey, Alexander Rasin, Esther Ryvkina, Nesime Tatbul,
Ying Xing, and Stan Zdonik. 2005. The Design of the Borealis Stream
Processing Engine. In CIDR.

[9] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl,
Seif Haridi, and Kostas Tzoumas. 2015. Apache Flink™: Stream and
Batch Processing in a Single Engine. IEEE Data Eng. Bull. 38, 4 (2015),
28–38.

[10] Bill Chambers and Jules Damji. Last Accessed 22/08/2019. Blog post:
Benchmarking Structured Streaming on Databricks Runtime Against
State-of-the-Art Streaming Systems.
https://databricks.com/blog/
2017/10/11/benchmarking-structured-streaming-on-databricks-
runtime-against-state-of-the-art-streaming-systems.html

[11] S. Chintapalli, D. Dagit, B. Evans, R. Farivar, T. Graves, M. Holderbaugh,
Z. Liu, K. Nusbaum, K. Patil, B. J. Peng, and P. Poulosky. 2016. Bench-
marking Streaming Computation Engines: Storm, Flink and Spark
Streaming. In IPDPSW. 1789–1792.

[12] Databricks. Last Accessed 22/08/2019. Running the Yahoo Benchmark
on Databricks. https://databricks.github.io/benchmarks/structured-
streaming-yahoo-benchmark/index.html

[13] Stephan Ewen. Last Accessed 27/12/2019. Flink Internals.

https:

//cwiki.apache.org/confluence/display/FLINK/Flink+Internals
[14] Bugra Gedik, Henrique Andrade, Kun-Lung Wu, Philip S. Yu, and
Myungcheol Doo. 2008. SPADE: The System’s Declarative Stream
Processing Engine. In SIGMOD. 1123–1134.

15

[15] Shigeru Imai, Stacy Patterson, and Carlos A. Varela. 2017. Maximum
Sustainable Throughput Prediction for Data Stream Processing over
Public Clouds. In Proceedings of the 17th IEEE/ACM International Sym-
posium on Cluster, Cloud and Grid Computing (CCGrid ’17). 504–513.
[16] Wesley M. Johnston, J. R. Paul Hanna, and Richard J. Millar. 2004.
Advances in Dataflow Programming Languages. ACM Comput. Surv.
36, 1 (2004), 1–34.

[17] Jeyhun Karimov, Tilmann Rabl, Asterios Katsifodimos, Roman
Samarev, Henri Heiskanen, and Volker Markl. 2018. Benchmarking
Distributed Stream Data Processing Systems. In ICDE. 1507–1518.
[18] Jay Kreps. Last Accessed 17/10/2019. Questioning the Lambda Ar-
chitecture. https://www.oreilly.com/radar/questioning-the-lambda-
architecture/

[19] Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli,
Christopher Kellogg, Sailesh Mittal, Jignesh M. Patel, Karthik Ra-
masamy, and Siddarth Taneja. 2015. Twitter Heron: Stream Processing
at Scale. In SIGMOD. 239–250.

[20] Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli,
Christopher Kellogg, Sailesh Mittal, Jignesh M. Patel, Karthik Ra-
masamy, and Siddarth Taneja. 2015. Twitter Heron: Stream Processing
at Scale. In SIGMOD 2015. 239–250.

[21] Wei Lin, Haochuan Fan, Zhengping Qian, Junwei Xu, Sen Yang, Jingren
Zhou, and Lidong Zhou. 2016. STREAMSCOPE: Continuous Reliable
Distributed Processing of Big Data Streams. In NSDI. 439–453.
[22] Frank McSherry, Derek Gordon Murray, Rebecca Isaacs, and Michael

Isard. 2013. Differential Dataflow. In CIDR.

[23] Hongyu Miao, Heejin Park, Myeongjae Jeon, Gennady Pekhimenko,
Kathryn S. McKinley, and Felix Xiaozhu Lin. 2017. StreamBox: Modern
Stream Processing on a Multicore Machine. In USENIX 2017. 617–629.
[24] Pietro Michiardi. Last Accessed 27/12/2019. Apache Spark Inter-
nals. http://cds.iisc.ac.in/wp-content/uploads/DS256.2017.L17.Spark_
.Execution.pdf

[25] Claudia Misale, Maurizio Drocco, Guy Tremblay, Alberto R. Martinelli,
and Marco Aldinucci. 2018. PiCo: High-performance data analytics
pipelines in modern C++. Future Generation Comp. Syst. 87 (2018),
392–403.

[26] Derek G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul
Barham, and Martín Abadi. 2013. Naiad: A Timely Dataflow System.
In SOSP. 439–455.

[27] Neha Narkhede, Gwen Shapira, and Todd Palino. 2017. Kafka: The
Definitive Guide Real-Time Data and Stream Processing at Scale. O’Reilly
Media, Inc.

[28] Shadi A. Noghabi, Kartik Paramasivam, Yi Pan, Navina Ramesh, Jon
Bringhurst, Indranil Gupta, and Roy H. Campbell. 2017. Samza: Stateful
Scalable Stream Processing at LinkedIn. PVLDB 10, 12 (2017), 1634–
1645.

[29] Apache Spark. Last Accessed 27/08/2019. Spark 2.4.3: Structured
Streaming Programming Guide. https://spark.apache.org/docs/latest/
structured-streaming-programming-guide.html#output-modes
[30] William Thies, Michal Karczmarek, and Saman P. Amarasinghe. 2002.
StreamIt: A Language for Streaming Applications. In CC. 179–196.
[31] Ankit Toshniwal, Siddarth Taneja, Amit Shukla, Karthik Ramasamy,
Jignesh M. Patel, Sanjeev Kulkarni, Jason Jackson, Krishna Gade,
Maosong Fu, Jake Donham, Nikunj Bhagat, Sailesh Mittal, and Dmitriy
Ryaboy. 2014. Storm@Twitter. In SIGMOD 2014. 147–156.

[32] Shivaram Venkataraman, Aurojit Panda, Kay Ousterhout, Michael
Armbrust, Ali Ghodsi, Michael J. Franklin, Benjamin Recht, and Ion
Stoica. 2017. Drizzle: Fast and Adaptable Stream Processing at Scale.
In SOSP 2017. 374–389.

[33] Abhishek Verma, Brian Cho, Nicolas Zea, Indranil Gupta, and Roy H.
Campbell. 2013. Breaking the MapReduce Stage Barrier. Cluster Com-
puting 16, 1 (2013), 191–206.

[34] Guozhang Wang, Joel Koshy, Sriram Subramanian, Kartik Paramasi-
vam, Mammad Zadeh, Neha Narkhede, Jun Rao, Jay Kreps, and Joe
Stein. 2015. Building a Replicated Logging System with Apache Kafka.
PVLDB 8, 12 (2015), 1654–1655.

[35] Tom White. 2009. Hadoop: The Definitive Guide (1st ed.). O’Reilly

Media, Inc.

[36] Burak Yavuz. 2017. Last Accessed 22/08/2019. Blog post: Arbi-
trary Stateful Processing in Apache SparkâĂŹs Structured Stream-
ing.
https://databricks.com/blog/2017/10/17/arbitrary-stateful-
processing-in-apache-sparks-structured-streaming.html

[37] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave,
Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and
Ion Stoica. 2012. Resilient Distributed Datasets: A Fault-tolerant Ab-
straction for In-memory Cluster Computing. In NSDI 2012. 2–2.
[38] Steffen Zeuch, Sebastian Breß, Tilmann Rabl, Bonaventura Del Monte,
Jeyhun Karimov, Clemens Lutz, Manuel Renz, Jonas Traub, and Volker
Markl. 2019. Analyzing Efficient Stream Processing on Modern Hard-
ware. PVLDB 12, 5 (2019), 516–530.

16

