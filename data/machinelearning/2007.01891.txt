0
2
0
2

l
u
J

3

]

G
L
.
s
c
[

1
v
1
9
8
1
0
.
7
0
0
2
:
v
i
X
r
a

A Unifying View of Optimism in Episodic Reinforcement Learning

Gergely Neu
Universitat Pompeu Fabra
Barcelona, Spain
gergely.neu@gmail.com

Ciara Pike-Burke∗
Imperial College London
London, UK
c.pikeburke@gmail.com

July 7, 2020

Abstract

The principle of “optimism in the face of uncertainty” underpins many theoretically successful rein-
forcement learning algorithms. In this paper we provide a general framework for designing, analyzing
and implementing such algorithms in the episodic reinforcement learning problem. This framework is
built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs
an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algo-
rithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic
algorithms beneﬁting from a cleaner probabilistic analysis while value-optimistic algorithms are easier
to implement and thus more practical. With the framework developed in this paper, we show that it is
possible to get the best of both worlds by providing a class of algorithms which have a computationally
eﬃcient dynamic-programming implementation and also a simple probabilistic analysis. Besides being
able to capture many existing algorithms in the tabular setting, our framework can also address large-
scale problems under realizable function approximation, where it enables a simple model-based analysis
of some recently proposed methods.

1

Introduction

Reinforcement learning (RL) is a key framework for sequential decision-making under uncertainty [43, 44].
In an RL problem, a learning agent interacts with a reactive environment by taking a series of actions. Each
action provides the agent with some reward, but also takes them to a new state which determines their future
rewards. The aim of the agent is to pick actions to maximize their total reward in the long run. The learning
problem is typically modeled by a Markov Decision Process (MDP, [38]) where the agent does not know the
rewards or transition probabilities. Dealing with this lack of knowledge is a crucial challenge in reinforcement
learning: the agent must maximize their rewards while simultaneously learning about the environment. One
class of algorithms that have been successful at balancing this exploration versus exploitation trade-oﬀ are
optimistic reinforcement learning algorithms. In this paper, we provide a new framework for studying these
algorithms.

Optimistic algorithms are built upon the principle of “optimism in the face of uncertainty” (OFU). They
operate by maintaining a set of statistically plausible models of the world, and selecting actions to maximize
the returns in the best plausible world. Such algorithms were ﬁrst studied in the context of multi-armed
bandit problems [28, 2, 14, 5, 29], and went on to inspire numerous algorithms for reinforcement learning. A
closer look at the literature reveals two main approaches to incorporate optimism in RL. In the ﬁrst, optimism
is introduced through estimates of the MDP: these approaches build a set of plausible MDPs by constructing
conﬁdence bounds around the empirical transition and reward functions, and select the policy that generates
the highest total expected reward in the best feasible MDP. We refer to this family of methods as model-
optimistic. Examples of model-optimistic methods include RMAX [13, 26, 45] and UCRL2 [4, 23, 42]. While

∗This work was done while CPB was at Universitat Pompeu Fabra and Barcelona Graduate School of Economics.

1

 
 
 
 
 
 
conceptually appealing, model-optimistic methods tend to be diﬃcult to implement due to the complexity
of jointly optimizing over models and policies. Another approach to incorporating optimism into RL is to
construct optimistic upper bounds on the optimal value functions which are (informally) the total expected
reward of the optimal policy in the true MDP. The optimistic policy greedily picks actions to maximize the
optimistic values. We refer to this class of methods as value-optimistic. Examples of algorithms in this class
are MBIE-EB [42], UCB-VI [6] and UBEV [16]. These algorithms compute the optimistic value functions
via dynamic programming (cf. 9), making them computationally eﬃcient and compatible with empirically
successful RL algorithms that are typically based on value functions. One downside of these approaches is
that their probabilistic analysis is often excessively complex.

While these two approaches may look very diﬀerent on the surface, we show in this paper that there
is in fact a very strong connection between them. Our ﬁrst contribution is to show that the optimization
problems associated with these two problems exhibit strong duality. This implies that that for every model-
optimistic approach, there exists an equivalent value-optimistic approach. This bridges the gap between
the conceptually simple model-optimistic approaches and the computationally eﬃcient value-optimistic ap-
proaches. This result enables us to develop a general framework for designing, analyzing and implementing
optimistic algorithms in the episodic reinforcement learning problem. Our framework is broad enough to
capture many existing algorithms for tabular MDPs, and for these we provide a simple analysis and com-
putationally eﬃcient implementation. The framework can also be extended to incorporate realizable linear
function approximation, where it leads to a new model-based analysis of two value-optimistic algorithms.
Our analysis involves constructing a new model-optimistic formulation for factored linear MDPs which may
be of independent interest.

A

and

are the ﬁnite sets of states and actions with S =

S
x, a) of reaching state x′
|

2 Background on Markov Decision Processes
Finite-horizon episodic MDPs. A ﬁnite episodic Markov decision process (MDP) is a tuple (
S
where
, H is the (ﬁxed) episode length
and α is the initial state distribution. The transition functions, P =
}h,x,a, give the probability
x, a)
Ph(x′
at stage h of an episode,
∈ S
and the reward function, r :
[0, 1], assigns a reward to each state-action pair. For simplicity, we
assume r is known and deterministic1, and each episode t begins from state x1,t ∼
α. If no further structure
is assumed, we call the MDP tabular. We deﬁne a stationary policy π :
as a mapping from states
S → A
to actions, and a nonstationary policy as a collection π =
H
h=1 of stationary policies for each stage h of
an episode, and note that these are suﬃcient for maximizing reward in an episode. We denote by Pπ [
] and
·
] a probability or expectation with respect to the distribution of state-action sequences under policy π
Eπ [
·
.
in the MDP, and let [H] =
S × A

|A|
Ph(
{
·|
from state x

after playing action a

∈ S
S × A →

1, . . . , H

πh}

, A =

∈ A

and

|S|

A

=

Z

}

{

{

,

, H, α, P, r)

Value functions and dynamic programming. For any policy π, we deﬁne the value function at each
and stage h as the expected total reward from running policy π from that point on:
state x

∈ S

H

h (x) = Eπ
V π

rl(xl, πl(xl))
(cid:12)
Xl=h
(cid:12)
(cid:12)
We denote by π∗ an optimal policy satisfying V π∗
(cid:12)
h (x) = maxπ V π
h (x) for all x
h (x) = V π∗
value function by V ∗
V ∗
1 (x1). We deﬁne the (optimal) action-value function for each x, a, h as

xh = x
(cid:21)

(cid:20)

.

[H], and the optimal
h (x). The total expected reward of π∗ in an episode starting from state x1 is

∈ S

, h

∈

H

Qπ

h(x, a) = Eπ

rl(xl, πl(xl))
(cid:12)
(cid:12)
(cid:12)
1The extension to unknown rewards is fairly straightforward using upper conﬁdence bounds on r
(cid:12)

xh = x, ah = a

h(x, a) = max

Xl=h

and

(cid:20)

(cid:21)

π

Q∗

Qπ

h(x, a).

2

It is easily shown that the value functions satisfy the Bellman equations for all x, a, h:
V ∗
H+1(x) = 0

h(x, a),

Q∗

V π
H+1(x) = 0

V ∗
h (x) = max
a∈A

h(x, π(x)),

V π
h (x) = Qπ
Qπ
h(x, a) = rh(x, a) +

Ph(y

x, a)V π
|

h+1(y) and

Q∗

h(x, a) = rh(x, a) +

Xy∈S

Ph(y

x, a)V ∗
|

h+1(y).

Xy∈S

In a ﬁxed MDP, an optimal policy can be found by solving the above system of equations by backward
recursion through the stages H, H

1, . . . , 1, a method known as dynamic programming [8, 22, 9].

−

Optimal control in MDPs by linear programming. A key technical tool underlying our results is a
classic linear-programming (LP) formulation for solving MDPs [32, 18, 19]. To state this formulation, we
S transition matrix Ph,a for each
will represent value functions by S-dimensional vectors and deﬁne the S
x, a)V (x′). Then, the following LP can be
h, a, acting on a value function V as (Ph,aV ) (x) =
|
seen to be equivalent to the Bellman optimality equations:

x′ Ph,a(x′

×

minimize
V

V1(x1)

a
ra + Ph,aVh+1 ∀

, h

∈ A

∈

[H],

(1)

where the inequality is to be understood to hold entrywise. Deﬁning the vector qh,a = (qh(x1, a), . . . , qh(xS, a))T,
the dual of the above LP is given as

P
subject to
Vh ≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)

maximize

q∈Q(x1) h

subject to

qh,a, rai
(cid:12)
(cid:12)
a q1(x, a) = I
(cid:12)
P
(cid:12)
{

a qh+1,a =
x = x1}

a P T

x
a,hqh,a ∀
0

∈ S

, h

∈
, h

RS×A×H

:

+

q

{

∈

Q

P
, qh(x, a)

P

(x1) =

. Feasible points of
for
∈
the above LP can be interpreted as occupancy measures. For a ﬁxed policy π, the occupancy measure qπ
of policy π at the state-action pair x, a is deﬁned as qπ
h (x, a) = Pπ [xh = x, ah = a]. It can be shown that
(x1) and the constraint in (2). Each feasible q
the set of occupancy measures is uniquely characterized by
induces a stochastic policy πq deﬁned as πq
if the denominator is nonzero, and deﬁned
arbitrarily otherwise. The optimal solution q∗ to the LP in (2) can be shown to induce an optimal policy
P
π∗ which satisﬁes the Bellman optimality equations. For proofs and further details of this formulation, see
Puterman [38].

Q
qh(x,a)
a′∈A qh(x,a′)

x) =
|

[H]
}

(x, a)

h(a

∈ Z

≥

∀

[H],

(2)

Linear function approximation in MDPs.
In most practical problems, the state space is too large to
use the above results and it is common to work with parameterized estimates of the quantities of interest.
We focus on the classic idea of linear function approximation to represent the action-value functions as linear
Rd
functions of some ﬁxed d-dimensional feature map ϕ :
for some θh,a ∈
for each action a and stage h. To avoid technicalities, we assume that the state space
is still ﬁnite, although
d feature matrix Φ with its xthrow being ϕT(x), and
potentially very large. This allows us to deﬁne the
represent the action-value function as Qh,a = Φθh,a. We make the following assumption:

θh,a, ϕ(x)
i
h

Rd, so Qθ

h(x, a) =

S →

S ×

S

Assumption 1 (Factored linear MDP [49, 37, 25]). For each action a and stage h, there exists a d
× S
matrix Mh,a and a vector ρa such that the transition matrix can be written as Ph,a = ΦMh,a, and the reward
function as ra = Φρa. Furthermore, the rows of Mh,a, mh,a(x), satisfy
CP for all (x, a, h), ρ
satisﬁes

R for some positive constants CP , Cr, R.

mh,a(x)
k

Cr, and

k1 ≤

ϕ(x)

ρak2 ≤

k

k

k2 ≤

h (x, a) =

As shown by Jin et al. [25], this assumption implies that for every policy π, there exists a θπ such
that Qπ
θπ
. We now show that factored linear MDPs also enjoy a strong dual realizability
h,a, ϕ(x)
i
h
weight matrix for each action a such that ΦTWh,aΦ is
property. Let Wh,a be an arbitrary symmetric
full rank, and notice that, due to the realizability of the action-value functions, the optimal value functions
can be written as the solution to the following LP:

S × S

minimize
V,θ

V1(x1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

subject to

θh,a = (ΦTWaΦ)−1 ΦTWh,a (ra + Ph,aVh+1)
Vh ≥

Φθh,a

h
a

∀
∀

[H],
, h

∈
∈ A

∈

[H].

3

Under Assumption 1, this LP is feasible and has a ﬁnite solution. It also holds that parameter vectors θh,a
are independent of the choice of the weight matrix Wh,a. The dual of this LP can be written as

H

subject to

maximize
q∈Q(x1),ω

Φωh,a, rai (cid:12)
h
(cid:12)
(cid:12)
(cid:12)
Due to the boundedness and feasibility of the primal LP, the dual is also feasible and bounded. Moreover,
(cid:12)
(cid:12)
any vector q that is feasible for (3) is also a feasible solution to the full LP (2), since

a qh+1,a =
ΦTqh,a = ΦTWh,aΦωh,a
P

h
h,aWh,aΦωh,a ∀
a
∀

a
Xh=1 X

[H]
, h

∈
∈ A

[H]

(3)

P

∈

a P T

qh+1,a =

P T

h,aΦWh,aωh,a =

Mh,aΦTWh,aΦωa,h =

Mh,aΦTqh,a =

Ph,aqh,a.

a
X

a
X

a
X

a
X

a
X

Thus, for factored linear MDPs, the set of occupancy measures is exactly characterized by the constraints
in (3). To the best of our knowledge, these LP formulations and results are novel and may have other uses
beyond the setting of factored linear MDPs. For instance, MDPs exhibiting zero inherent Bellman error [52]
can be also seen to yield a feasible and ﬁnite solution for both LPs, although the above dual realizability
property is not guaranteed to hold for all occupancy measures.

3 Regret Minimization in Episodic Reinforcement Learning

We consider algorithms that sequentially interact with a ﬁxed but unknown MDP over K episodes. In each
episode, t, the algorithm selects a policy πt with the aim of maximizing the cumulative reward in that
episode. We assume that the learner has no prior knowledge of the transition function, and can only learn
about the MDP through interaction. The performance is measured in terms of the regret,

K

RT =

(V ∗

1 (x1,t)

πk
V
1

(x1,t))

−

t=1
X

e

e

∈

≤

α is the initial state in episode t.

S is the maximal number of reachable states from any (x, a)

where T = KH is the total number of rounds and x1,t ∼
In tabular MDPs, the lower bound on the regret is Ω(H√SAT ) [23, 34, 24]2.Most optimistic algorithms
are either model-optimistic or value-optimistic. Some notable model-optimistic approaches are UCRL2 [23]
O(S√H 3AT ), and KL-UCRL [20, 46] and UCRL2-B [21], which have
and REGAL [7] which have regret
O(H√SΓAT ) where Γ
regret
and stage
h
[H]. These algorithms diﬀer predominantly in the choice of distance and concentration bounds deﬁning
the set of feasible transition functions. Value-optimistic approaches often enjoy low regret at a cost of a
O(√H 5SAT ), and
more complex analysis. Some examples of these include UBEV [16] which has regret
O(H√SAT ), matching the lower bound. We note that optimism has also been
UCB-VI [6] which has regret
used in the model free setting (e.g. [24]), and that other non-optimistic approaches have also been successful
[35, 3]). Other related works include [53, 39] which also use occupancy
at regret minimization (see e.g.
measures, [47] where optimistic linear programs are used, and [31, 46] which exploit duality in speciﬁc cases.
For factored linear MDPs, all optimistic algorithms we are aware of are value-based, without a clear
O(√d3H 3T ), while
model-based interpretation: LSVI-UCB [25] uses dynamic programming and has regret
Eleanor [52] has regret
O(Hd√T ) but requires solving a complex optimization problem in each episode.
The UC-MatrixRL algorithm [48] considers a diﬀerent problem with two feature maps but is model-based
with regret

O(H 2d√T ). Non-optimistic approaches include [40, 51].

∈ Z

e

e

e

e

4 Optimism in Tabular Reinforcement Learning

e

We now present our main contribution: a general framework for designing, analyzing and implementing
optimistic RL algorithms in episodic tabular MDPs. Our framework naturally extends the LPs in (1) and (2)

2The extra √H due to having a diﬀerent Ph per stage. We use

4

) to denote order up to logarithmic terms.

O(
·

e

to account for uncertainty about the transition function. We use conﬁdence intervals for the transition
functions to express uncertainty in the space of occupancy measures and maximize the expected reward
over this set. Our key result shows that the dual of this optimization problem can be written in dynamic-
programming form with added exploration bonuses, the size of which are determined by the shape of the
primal conﬁdence sets.

We deﬁne the uncertainty sets using conﬁdence intervals around a reference transition function

P . For a

divergence measure D(p, p′) between probability distributions p, p′, deﬁne the conﬁdence sets

=

P

∆ : D

Ph(

x, a),

Ph(

x, a)

ǫ(x, a)

(x, a)

·|

·|

≤

∀

∈ S × A

∈

P

∈

n

(cid:17)
where ∆ is the set of valid transition functions. We assume that the divergence measure D is jointly convex
0, D(αp, αp′) =
in its arguments so that
αD(p, p′). Note that the distance
for any norm and all f -divergences satisfy these conditions [30].
Using

p′
k
, we modify (2) to get the optimistic primal optimization problem,

is convex, and that D is positive homogeneous so for any α

≥

−

P

o

(cid:16)

b

e

e

p

k

P

b

[H]

,

(4)

, h

maximize
q∈Q(x1)

P ∈∆

H

h

Xh=1

qh,a, r

subject to

a qh+1,a =
x, a),
Ph(

·|

P T

h,aqh,a
x, a)

a
Ph(
P
·|
e

(cid:17)

i (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

D
P

(cid:16)

ǫ(x, a)

≤

[H]

h

∈
(x, a)

∀

∀

, h

∈

∈ Z

[H]

(5)

e

∈ P

We pick ǫ such that P

with high probability. In this case, the above optimization problem returns an
b
“optimistic” occupancy measure with higher expected reward than the true optimal policy. Unfortunately,
the optimization problem in (5) is not convex due to the bilinear constraint qh+1,a =
h,aqh,a. Our main
result below shows that it is still possible to obtain an equivalent value-optimistic formulation via Lagrangian
duality and an appropriate reparametrization. We make use of the conjugate of the divergence D deﬁned
for any function z, distribution p′ and threshold ǫ as

P T

P

e

e

a

D∗ (z

ǫ, p′) = max
|

p∈∆ {h

z, p

−

p′

D(p, p′)

i|

.

ǫ

}

≤

Proposition 1. Let CBh(x, a) = D∗(Vh+1|
The optimization problem in (5) can be equivalently written as

ǫh(x, a),

Ph(

·|

x, a)) and denote its vector representation by CBh,a.

minimize
V

V1(x1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

b

subject to
ra +
Vh ≥

a
Ph,aVh+1 + CBh,a ∀
b

, h

∈ A

∈

[H]

(6)

Proof sketch. The full proof is in Appendix A.1. Here we outline the key ideas. To show strong duality,
we reparameterize the problem as follows: deﬁne Jh(x, a, x′) =
Ph(x′
x, a)qh(x, a) and note that due to
|
homogeneity of D, the constraint on
ǫh(x, a)qh(x, a),
Ph(
),
·
It is straightforward to verify the Slater condition for the resulting convex
which is convex in q and J.
program, and thus strong duality holds for both parametrizations.

P is equivalent to D(Jh(x, a,
e

x, a)qh(x, a))

≤

·|

b

e

Letting

(q,

L

P ; V ) be the Lagrangian of (5) and using the non-negativity of q, the maximum of (5) is

min
V

e
max
q≥0
P ∈P

e

(q,

L

P ; V ) = min

V

max
q≥0

e

(cid:26) Xx,a,h

+

max
Ph(·|x,a)∈Ph(x,a)

y

(cid:18) X

Ph(y

b
x, a)
|

y (cid:16)
X

e

b

qh(x, a)

Ph(y

x, a)Vh+1(y) + r(x, a)
|

−

Vh(x)

Ph(y

−

x, a)
|

(cid:17)

Vh+1(y)

.
(cid:19)(cid:27)

(7)

p =

Then, letting
Ph(
·|
can be written as max
p∈∆{h
−
into (7) and use standard techniques to get the dual from the Lagrangian.

x, a), and using the deﬁnition of D and D∗, the inner maximum
p). We then substitute this

= D∗(Vh+1|

ǫh(x, a)
}

ǫh(x, a),

x, a)),

·|
; D(

p(x′) =
e
p

p)

p,

≤

b

Vh+1,
e

b

Ph(
p
i
e
b

e

e

e

b

5

b

This result enables us to establish a number of important properties of the optimal solutions of the
optimistic optimization problem (5). The following two propositions (proved in in Appendix A.2) highlight
that optimal solutions to (5) are optimistic, bounded, and can be found by a dynamic-programming pro-
cedure. This implies that any model-optimistic algorithm that solves (5) in each episode is equivalent to
value-optimistic algorithm using an appropriate choice of exploration bonuses.
Proposition 2. Let V + be the optimal solution to (6) and CB+
ǫ(x, a),
optimal policy π+ extracted from any optimal solution q+ of the primal LP in (5) satisﬁes

h (x, a) = D∗(V +

Ph). Then, the

h+1|

h (x) = r(x, π+
V +

h (x)) + CB+

h (x, π+

h (x)) +

Ph(y

x, π+
|

h (x))V +

h+1(y)

x

∀

, h

∈ S

∈

Xy∈S
Proposition 3. If the true transition function P satisﬁes the constraint in Equation (5), the optimal solution
V + of the dual LP satisﬁes V ∗

h + 1 for all x

H

b

.

h (x)

≤

V +
h (x)

≤

−

∈ S

b
[H].

(8)

4.1 Regret bounds for optimistic algorithms

Pt in (4) using some divergence
We consider algorithms that, in each episode t, deﬁne the conﬁdence sets
x, a) = Nh,t(x,a,x′)
. Here Nh,t(x, a, x′) is the
measure D and the reference model
, a
|
∈ A
total number of times that we have played action a from state x in stage h and landed in state x′ up to the
. In episode t, the algorithm follows the
beginning of episode t, and Nh,t(x, a) = max
optimistic policy πt extracted from the solution of the primal optimistic problem in (5), or equivalently, the
optimistic dynamic programming procedure in (6). The following theorem establishes a regret guarantee of
the resulting algorithm:

x′ Nh,t(x, a, x′), 1

Nh,t(x,a) ∀

Ph,t(x′

x, x′

∈ S

P

b

}

{

Theorem 4. On the event

K
t=1{

∩

P

∈ Pt}

K

H

, the regret is bounded with probability at least 1

δ as

−

CBh,t(xh,t, πh,t(xh,t)) + CB−

h,t(xh,t, πt(xh,t))

+ H

2T log(1/δ)

RT ≤

where CB−

t=1
X
h,t(x, a) = D∗(

Xh=1 (cid:18)
V +
h+1,t|

−

ǫh,t(x, a),

Ph,t) and CBh,t(x, a) = D∗(V +

(cid:19)

h+1,t|

p
ǫh,t(x, a),

Ph,t).

The proof is in Appendix A.3. While similar results are commonly used in the analysis of value-based
algorithms [6, 16], the merit of Theorem 4 is that it is derived from a model-optimistic perspective, and thus
cleanly separates the probabilistic and algebraic parts of the regret analysis. Indeed, proving the probabilistic
statement that P is in the conﬁdence set is very simple in the primal space where our constraints are speciﬁed.
Once this is established, the regret can be bounded in terms of the dual exploration bonuses. This simplicity
of analysis is to be contrasted with the analyses of other value-optimistic methods that often interleave
probabilistic and algebraic steps in a complex manner.

b

b

Inﬂating the exploration bonus. The downside of the optimistic dynamic-programming algorithm de-
rived above is that the exploration bonuses may sometimes be diﬃcult to calculate explicitly. Luckily, it is
easy to show that the regret guarantees are preserved if we replace the bonuses by an easily-computed upper
bound. This is helpful for instance when D is deﬁned as D(p, p′) =
, whence the conjugate can
k
∗ satisfying
be simply bounded by the dual norm
D†
[0, H], and obtain an optimistic value function by the
ǫ,
|
following dynamic-programming procedure:

k∗. Formally, we can consider an inﬂated conjugate D†

P ) for every function f :

D∗(f

ǫ′,
|

S →

p
k

∗(f

P )

p′

−

≥

V

k

b

b
V †
h (x) = max

a

(cid:26)
.

min

H

(cid:26)

−

h + 1, r(x, a) +

Ph(

x, a)V †
·|

h+1 + D†

∗(V †

h+1|

ǫ′(x, a),

Ph)

,
(cid:27)(cid:27)

(9)

x

H+1(x) = 0

with V †
b
Proposition 3 to show they are bounded. The resulting value-estimates then satisfy V ∗
≤
V †
1 (x1) with high probability, so we can bound the regret of this algorithm in the following theorem, whose
proof is in Appendix A.4:

In this case, we need to clip the value functions since we can no longer use

V +
1 (x1)

1 (x1)

∈ S

≤

b

∀

6

Algorithm
UCRL2 [23]:

UCRL2B [21]:

KL-UCRL3:

χ2-UCRL4

P

Distance D(p,

p)

p
p
k1
−
k
p(x))2
(p(x)−
b
maxx
x p(x) log p(x)

+

p(x)

p(x)

p(x)
b
x(
b
b
p(y))2
(p(y)−
P
b
p(y)

x
b
P

b

b

ǫ
S/N

1/N

p

Conjugate D†
ǫ

∗(V
span (V )

p)

ǫ,
|

·
p(x)

ǫ

x

b
pV

|

V (x)
|

−
p(y)))

Regret
S√H 3AT
H√SΓAT

V(V ) HS√AT
b

b

HS√AT

−

p(x))

S/N

p
P
(ǫ + (1

S/N

q

−
b

y
V(V )
P
ǫ
b
q

b

as the base measure to avoid division by 0, for UCRL2, we use

Table 1: Various algorithms in our framework. For all algorithms except UCRL2, we use
max 1,Nh,t(x,a,y)
Nh,t(x,a)

h,t(y
x, a). We denote
|
)2. The third column gives scaling of the conﬁdence width in terms of S and
V(V ) =
i
the number of sample transitions N . The fourth column gives a tractable upper bound on the value of the
conjugate. The last column gives the the regret bound derived from Theorem 5 (up to logarithmic factors)
b
with exploration bonus deﬁned from the inﬂated conjugate and the smallest value of ǫ that guarantees

x, a) =
|

p(x) (V (x)

P (y

p, V

− h

P

b

b

b

b

x

P +

K
t=1{

P

w.h.p.

∈ Pt}
∩
Theorem 5. Let D†
ǫ′,
∗(f
|
h,t(x, a) = D†
and, CB†
∗(V †
Ph,t). Then, on the event
δ, the policy returned by the procedure in (9) incurs regret
than 1

P ) be an upper bound on D∗(f

ǫ′
h,t(x, a),

h+1,t|
b

ǫ,
|

b

−

P ) and D∗(

f
−
K
P
t=1{

∩

P ) for every f :

[0, H],
, with probability greater

S →

ǫ,
|
∈ Pt}
b

b
CB†

h,t(xh,t, πh,t(xh,t)) + 4H

RT ≤

2

K

H

t=1
X

Xh=1

2T log(1/δ).

p

Examples. Theorems 4 and 5 show that the key quantities governing the size of the regret are the conjugate
distance and the conﬁdence width ǫ. This explicitly quantiﬁes the impact of the choice of primal conﬁdence
set. We provide some example choices of the divergences along with their conjugates, the best known
conﬁdence widths, and the resulting regret bounds in Table 4.1, with derivations in Appendix A.5. Many
of these correspond to existing methods for which our framework suggests their ﬁrst dynamic-programming
implementation in the original state space
, rather than the extended state-space which was traditionally
used [23, 20, 31]. More generally, our framework captures any algorithm that deﬁnes conﬁdence sets in
terms of a norm or f -divergence, along with many others. It may also be possible to derive model-optimistic
forms of value-optimistic methods, however, in this case care needs to be taken to show that the primal
conﬁdence sets are valid. For example, a variant of UCB-VI [6] can be derived from the divergence measure
h+1.
, but the probabilistic analysis here is complicated due to the dependence between
P
h

P and V +

P , V +

h+1i

−

S

b

5 Optimism with realizable linear function approximation

b

We now extend our framework to factored linear MDPs, where all currently known algorithms are value-
optimistic. We provide the ﬁrst model-optimistic formulation by modeling uncertainty about the MDP in
the primal LP involving occupancy measures in (3). All proofs are in Appendix B.

A key challenge in this setting is that the uncertainty can no longer be expressed using distance metrics in
the state space, since this could lead to trivially large conﬁdence sets5. Instead, we deﬁne conﬁdence sets in
terms of a distance that takes the linear structure into account. These are centered around a reference model
Mh,a. We consider reference models implicitly
P deﬁned for each h, a as
I{ah,k=a}ϕ(xh,k)ϕT(xh,k) + λI
deﬁned by the LSTD algorithm [12, 27, 36]. In episode t, let Σh,a,t =
b
c
3In the original KL-UCRL algorithm, [20, 46] consider the reverse KL-divergence. This also ﬁts into our framework. See

Mh,a for some d

Ph,a = Φ

matrix

t
k=1

× S

c

b

P

Appendix A.5.3 for details.

4[31] also use a χ2-divergence but require
P (x) > p0 for some p0 if
5E.g., for the total variation distance, concentration bounds scale with √S which is potentially unbounded.

P (x) > 0 making

non-convex.

P

e

e

7

for some λ
episode t is deﬁned for each action a as

≥

0, and ex be the unit vector in RS corresponding to state x. Then, our reference model in

Mh,a,t = Σ−1

h,a,t−1

c

I{ah,k=a}ϕ(xh,k)exh+1,k .

(10)

t−1

Xk=1

I{ah,k=a}exh,k eT

xh,k , so that

t
k=1
λI. We establish the following important technical result:

Finally, the weight matrix in the LP formulation (3) is chosen as Wh,a,t =
ΦTWh,a,tΦ = Σh,a,t −
Proposition 6. Consider the reference model
for any ﬁxed function g :

Mh,a,t with

Ph,a,t = Φ

P

S →

[
−

H, H], the following holds with probability at least 1
c

b

−

Mh,a,t deﬁned in Equation (10). Then,
δ:

Mh,a,t −
(cid:13)
(cid:0)
(cid:13)

Mh,a,t

g

H

d log

+ CP H√λd.

The proof is based on the fact that for a ﬁxed g,

s
g is essentially a vector-valued martingale.
Mh,a,t−
Our main contribution in this setting is to use this result to identify two distinct ways of deriving tight
(cid:0)
conﬁdence sets that incorporate optimism into (3). Both approaches use the optimistic parametric Bellman
(OPB) equations with some exploration bonus CBh,t(x, a) (deﬁned later):

(cid:18)
Mh,a,t

c

c

(cid:19)

(cid:1)

(cid:1)

Σh,a,t−1 ≤
(cid:13)
(cid:13)

c
1 + tR2/λ
δ

h,a,t = ρa + Σ−1
θ+

h,a,t−1

I{ah,k=a}ϕ(xh,k)V +

h+1,t (xh+1,k)

t−1

Xk=1

(11)

V +
h,t(x) = max

a

Φθ+

h,a,t

(x) + CBh,t(x, a)

Both bonuses we derive can be upper-bounded by CB†
for some C(d) > 0.
Then, one can apply a variant Theorem 5 to bound the regret of both algorithms in terms of the sum of
these inﬂated exploration bonuses, amounting to a total regret of

h,t(x, a) = C(d)

O(C(d)√dHT ).

kΣ−1

ϕ(x)

h,a,t−1

k

n(cid:16)

(cid:17)

o

5.1 Optimism in state space through local conﬁdence sets

e

Our ﬁrst approach models the uncertainty locally in each state-action pair x, a using some distance metric
D between transition functions. We consider the following optimization problem:

maximize
q∈Q(x1),ω

H
h=1

Wh,aΦωa,h, rai

a h

P

P

subject to

a qh+1,a =

Ph,aWh,aΦωh,a

a

ΦTqh,1 = ΦTWh,aΦωh,a
P
D
x, a)
x, a),

Ph(

P
e
Ph(
·|

·|

(cid:16)

ǫh(x, a)

≤

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[H]
, h

h
a

∈
∈ A
(x, a)

∀
∀

∀

∈

, h

∈ Z

[H]

∈

[H]

(12)

As in the tabular case, (12) can be reparametrized so that the constraint set is convex, allowing us to appeal
e
to Lagrangian duality to get an equivalent formulation as shown in the following proposition.

b

Proposition 7. The optimization problem (12) is equivalent to solving the optimistic Bellman equations (11)
with the exploration bonus deﬁned as CBh(x, a) = D∗(V +

ǫh(x, a),

x, a)).

Ph(

h+1|

·|

Taking the form of V +

h into account, in episode t, we deﬁne our conﬁdence sets as in (4) with

b

D

Ph,t(

x, a),

Ph,t(

x, a)

= sup

·|

·|

Ph,t(x′

x, a)
|

−

Ph,t(x′

x, a)
|

g(x′)

(13)

and

Ph,a,t = Φ

(cid:16)
Mh,a,t where
equations (11). For any choice of ǫt, CBh,t(x, a)
ǫh,t(x, a). The following theorem bounds the regret for an appropriate choice of ǫt

Vh+1,t is the set of value functions that can be produced by solving the OPB
b
h,t(x, a) =

b
ǫh,t(x, a), so one can simply use the bonus CB†

c

≤

(cid:16)

(cid:17)

(cid:17)

e

e

b

g∈Vh+1,t

Xx′

8

Theorem 8. The choice ǫh,t(x, a) = C
P is feasible for (12) in every episode t with probability 1
exploration bonus CB†

−
h,t(x, a) = ǫh,t(x, a) has regret bounded by

with C =

kΣ−1

ϕ(x)

h,a,t−1

k

O(Hd) guarantees that the transition model

δ. The resulting optimistic algorithm with
e
O(√H 3d3T ).

This algorithm coincides with the LSVI-UCB method of [25] and our performance guarantee matches

theirs. The advantage of our result is a simpler analysis allowed by our model-optimistic perspective.

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

5.2 Optimism in feature space through global constraints
Our second approach exploits the structure of the reference model (10), and constrains
Pt using the distance metric suggested by Proposition 6 as
conditions on
Mh,a,

Ma. We deﬁne

Mh,a) = sup

Mh,a

ǫh,a

D(

f

e

f

f ∈Vh+1

Mh,a −
(cid:13)
(cid:0)
(cid:13)
f

c

(cid:1)

(cid:13)
(cid:13)

Σh,a ≤

for

Vh+1 as in (13) and some ǫh,a > 0. We then consider the following optimization problem:
subject to

f

c

H

maximize
q∈Q(x1),

ω,

M

a
Xh=1 X

Wh,aΦωh,a, rai
h

a

a qh+1,a =

P T
h
h,aWh,aΦωh,a ∀
ΦTqh,a = ΦTWh,aΦωh,a
a
P
P
∀
e
a
ǫh,a
Mh,a)
D(
∀

Mh,a,

≤

[H]
, h
, h

∈
∈ A
∈ A

∈
∈

[H]
[H].

(14)

(15)

Pa through global

e

Unfortunately, directly constraining M leads to an optimization problem that, unlike in the other settings,
c
M , the
M ) = V +
1 (x1) where V + solves the OPB equations (11) with CBh(x, a) =
, then, we can re-write (32) as maximizing
M
=
ǫ
Vh+1
≤
. Exploiting this we provide a more tractable version of the optimization problem, and
(cid:1)
∈ M
c

f
cannot easily be re-written as an convex problem exhibiting strong duality. Nevertheless, for a ﬁxed
value of (32) is equivalent to G(
. Let
Mh,a
ϕ(x),
G(
(cid:11)
(cid:10)
bound the regret of the resulting algorithm, below:

Mh,a−
M
M ) over
(cid:0)
f

Rd×S : D(

M
f

M )

M ,

c

f

f

f

∈

{

}

f

f

Theorem 9. Deﬁne the function G′(B) = V +
exploration bonus CBh(x, a) =
[K]. Then, maxB∈Bt G′(B)

ϕ(x), Bh,ai
h
max
≥

t
M∈Mt
corresponding to the optimal solutions B†
t has regret bounded by
e

Bt =

G(

f

∈

(cid:8)

1 (x1) with V + the solution of the OPB equations (11) with
Bh,akΣh,a,t−1 ≤
and let
for all episodes
k

ǫh,a,t

B :

M ) and the optimistic algorithm with exploration bonuses

(cid:9)

O(d√H 3T ).

The algorithm suggested in this theorem essentially coincides with the Eleanor method proposed re-
cently in [52], and our guarantees match theirs under our realizability assumption. Our model-based per-
spective suggests that the problem of implementing Eleanor is inherently hard: the form of the primal
optimization problem reveals that G′(B) is a convex function of B, and thus its maximization over a convex
set is intractable in general. Note that the celebrated LinUCB algorithm for linear bandits must solve the a
similar convex maximization problem [15, 1]. As in linear bandits, it remains an open question to get regret
O(Hd√T ) with a computationally eﬃcient algorithm.

e

6 Conclusion
e

We have provided a new framework unifying model-optimistic and value-optimistic approaches for episodic
reinforcement learning, thus demonstrating that many desirable features are enjoyed by both approaches. In
the tabular setting, we provided improved implementations and analyses of a general class of model-optimistic
algorithms. While these results demonstrate the strength and ﬂexibility of the model-based perspective, our
regret bounds feature an additional factor of √S on top of the minimax optimal bounds, which has been
eliminated by value-optimistic methods [6, 16]. However, our bounds for factored linear MDPs match the
best existing results, which gives us hope that model-based approaches may also eventually prove to be
optimal in the tabular case. Finally, we note that it is straightforward to extend our framework for inﬁnite-
horizon MDPs, although we leave the challenge of analyzing the regret of the resulting algorithms for future
work.

9

References

[1] Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits. In

Advances in Neural Information Processing Systems, pages 2312–2320, 2011.

[2] R. Agrawal. Sample mean based index policies with O(logn) regret for the multi-armed bandit problem.

Advances in Applied Probability, 27:1054–1078, 1995.

[3] S. Agrawal and R. Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret

bounds. In Advances in Neural Information Processing Systems, pages 1184–1194, 2017.

[4] P. Auer and R. Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. In

Advances in Neural Information Processing Systems, pages 49–56, 2007.

[5] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning Journal, 47(2-3):235–256, 2002.

[6] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In Interna-

tional Conference on Machine Learning-Volume 70, pages 263–272. JMLR. org, 2017.

[7] P. L. Bartlett and A. Tewari. REGAL: a regularization based algorithm for reinforcement learning
in weakly communicating MDPs. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 35–42,
2009.

[8] R. Bellman. Dynamic Programming. Princeton University Press, Princeton, New Jersey, 1957.

[9] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, Belmont,

MA, 3 edition, 2007.

[10] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A Nonasymptotic Theory of

Independence. Oxford University Press, 2013.

[11] S. Boyd, S. P. Boyd, and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[12] S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine

Learning, 22:33–57, 1996.

[13] R. I. Brafman and M. Tennenholtz. R-MAX - a general polynomial time algorithm for near-optimal

reinforcement learning. Journal of Machine Learning Research, 3:213–231, 2002.

[14] A. Burnetas and M. Katehakis. Optimal adaptive policies for sequential allocation problems. Advances

in Applied Mathematics, 17:122–142, 1996.

[15] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In R. A.
Servedio and T. Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki,
Finland, July 9-12, 2008, pages 355–366. Omnipress, 2008.

[16] C. Dann, T. Lattimore, and E. Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic
reinforcement learning. In Advances in Neural Information Processing Systems 30, pages 5713–5723.
2017.

[17] C. Dann, L. Li, W. Wei, and E. Brunskill. Policy certiﬁcates: Towards accountable reinforcement

learning. In International Conference on Machine Learning, pages 1507–1516, 2019.

[18] G. de Ghellinck. Les problèmes de décisions séquentielles. Cahiers du Centre d’Études de Recherche

Opérationnelle, 2:161–179, 1960.

[19] E. V. Denardo. On linear programming in a Markov decision problem. Management Science, 16(5):

281–288, 1970.

10

[20] S. Filippi, O. Cappé, and A. Garivier. Optimism in reinforcement learning and kullback-leibler diver-
gence. In Allerton Conference on Communication, Control, and Computing (Allerton), pages 115–122.
IEEE, 2010.

[21] R. Fruit, M. Pirotta,

and A. Lazaric.

Improved

analysis

of UCRL2B,

2019.

https://rlgammazero.github.io/docs/ucrl2b_improved.pdf.

[22] R. A. Howard. Dynamic Programming and Markov Processes. The MIT Press, Cambridge, MA, 1960.

[23] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of

Machine Learning Research, 11(Apr):1563–1600, 2010.

[24] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan. Is Q-learning provably eﬃcient? In Advances in

Neural Information Processing Systems, pages 4863–4873, 2018.

[25] C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably eﬃcient reinforcement learning with linear

function approximation. arXiv preprint arXiv:1907.05388, 2019.

[26] S. Kakade. On the sample complexity of reinforcement learning. PhD thesis, Gatsby Computational

Neuroscience Unit, University College London, 2003.

[27] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning research, 4

(Dec):1107–1149, 2003.

[28] T. L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in Applied

Mathematics, 6:4–22, 1985.

[29] T. Lattimore and Cs. Szepesvári. Bandit algorithms. book draft, 2019.

[30] F. Liese and I. Vajda. On divergences and informations in statistics and information theory. IEEE

Transactions on Information Theory, 52(10):4394–4412, 2006.

[31] O.-A. Maillard, T. A. Mann, and S. Mannor. “How hard is my MDP?” The distribution-norm to the

rescue". In Advances in Neural Information Processing Systems, pages 1835–1843, 2014.

[32] A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259–267, 1960.

[33] A. Maurer and M. Pontil. Empirical bernstein bounds and sample variance penalization. In Conference

on Learning Theory, 2009.

[34] I. Osband and B. Van Roy. On lower bounds for regret in reinforcement learning. arXiv preprint

arXiv:1608.02732, 2016.

[35] I. Osband, D. Russo, and B. Van Roy. (more) eﬃcient reinforcement learning via posterior sampling.

In Advances in Neural Information Processing Systems, pages 3003–3011, 2013.

[36] R. Parr, L. Li, G. Taylor, C. Painter-Wakeﬁeld, and M. L. Littman. An analysis of linear models,
linear value-function approximation, and feature selection for reinforcement learning. In International
Conference on Machine Learning, pages 752–759, 2008.

[37] B. Á. Pires and Cs. Szepesvári. Policy error bounds for model-based reinforcement learning with factored

linear models. In Conference on Learning Theory, pages 121–151, 2016.

[38] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-

Interscience, April 1994.

[39] A. Rosenberg and Y. Mansour. Online convex optimization in adversarial Markov decision processes.

In International Conference on Machine Learning, pages 5478–5486, 2019.

11

[40] D. Russo. Worst-case regret bounds for exploration via randomized value functions. In Advances in

Neural Information Processing Systems, pages 14410–14420, 2019.

[41] M. Simchowitz and K. G. Jamieson. Non-asymptotic gap-dependent regret bounds for tabular MDPs.

In Advances in Neural Information Processing Systems, pages 1151–1160, 2019.

[42] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for Markov decision

processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.

[43] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. 2nd edition. 2018.

[44] Cs. Szepesvári. Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning. Morgan & Claypool Publishers, 2010.

[45] I. Szita and Cs. Szepesvári. Model-based reinforcement learning with nearly tight exploration complexity

bounds. pages 1031–1038.

[46] M. S. Talebi and O.-A. Maillard. Variance-aware regret bounds for undiscounted reinforcement learning

in MDPs. In Algorithmic Learning Theory, pages 770–805, 2018.

[47] A. Tewari and P. L. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible

MDPs. In Advances in Neural Information Processing Systems, pages 1505–1512, 2008.

[48] L. F. Yang and M. Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and regret

bound. arXiv preprint arXiv:1905.10389, 2019.

[49] H. Yao, Cs. Szepesvári, B. Pires, and X. Zhang. Pseudo-MDPs and factored linear action models. 10

2014. doi: 10.1109/ADPRL.2014.7010633.

[50] A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without
domain knowledge using value function bounds. In International Conference on Machine Learning, pages
7304–7312, 2019.

[51] A. Zanette, D. Brandfonbrener, M. Pirotta, and A. Lazaric. Frequentist regret bounds for randomized

least-squares value iteration. In Artiﬁcial Intelligence and Statistics, 2020.

[52] A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low

inherent bellman error. arXiv preprint arXiv:2003.00153, 2020.

[53] A. Zimin and G. Neu. Online learning in episodic Markovian decision processes by relative entropy

policy search. In Advances in neural information processing systems, pages 1583–1591, 2013.

12

A Proofs of Results for Tabular Setting

Appendix

We prove here the results of Section 4. For ease of exposition, we restate the results before proving them.
For convenience, we introduce the conﬁdence set for every state x

and stage h

, action a

[H],

Ph(x, a) =

Ph(

x, a)

∆ : D

Ph(

x, a),

Ph(

x, a)

·|

∈

·|

and note that

P

if

Ph(

x, a)

·|

∈ P

The following lemma will be useful in several of the proofs.

n
e
e
∈ Ph(x, a) for all x, a, h

(cid:16)

∈ S

·|

b

∈ A

∈

≤

ǫ(x, a)

o

(16)

(cid:17)

e

e

Lemma 10. The primal and dual optimization problems in (5) and (6) exhibit strong duality. Consequently
the Karush-Kuhn-Tucker (KKT) conditions hold, and in particular, complementary slackness holds.

Proof. We ﬁrst show that the optimization problem in (5) exhibits strong duality. For this, it is helpful to con-
sider a reparameterization where we introduce the variables Jh(x, a, x′) =
x, a)qh(x, a), so that the non-
|
convex constraint D
≤
ǫh(x, a)qh(x, a), which is convex in J and q. The two constraints are clearly equivalent due to positive
homogeneity of D. This implies that the optimization problem in (5) can be equivalently written as

ǫ(x, a) can be rewritten as D
e

x, a)qh(x, a)

Jh(x, a,

Ph(x′

x, a),

x, a)

Ph(

Ph(

Ph(

),
·

≤

·|

·|

·|

(cid:0)

(cid:1)

(cid:1)

(cid:0)

b

b

e

maximize
q∈Q(x1),J

Xx,a,h

qh(x, a)r(x, a)

Subject to

qh(x, a) =

Jh−1(x′, a′, x)

a
X
D

Jh(x, a,

Xx′,a′
Ph(

·|

),
·

x, a)qh(x, a)

(cid:16)

Jh(x, a, x′) = qh(x, a)
b

(cid:17)

Xx′
Jh(x, a, x′)

0

≥

ǫh(x, a)qh(x, a)

≤

(17)

x

∀

∈ S

, h

(x, a)

∀

(x, a)

∀

, h

, h

∈ Z

∈ Z

[H]

[H]

[H]

∈

∈

∈

x, x′
∀

, a

∈ S

, h

∈ A

∈

[H].

In this formulation, there is only one non-linear constraint, and by our assumption that D is convex in both
x, a)qh(x, a) satisﬁes
of its arguments, this constraint is convex in J and q. Moreover,
|
this constraint for any qh(x, a), and in particular, if qh(x, a) is the occupancy measure induced by any policy
π in the MDP with transition function
P , then qh(x, a) and Jh(x, a, a′) are feasible solutions to the primal.
Hence, the Slater conditions are satisﬁed, and thus the optimization problem exhibits strong duality (see e.g.
[11]). We can then write the dual of the optimization problem in (17) as

Jh(x, a, x′) =

Ph(x′

b

b

b

max
(q,M)∈C1

min
V,γ

(cid:26) Xx,a,h

qh(x, a)(Vh(x)

−

γh(x, a) + r(x, a) + V1(x1)

(18)

+

Jh(x, a, x′)(Vh+1(x′) + γh(x, a))

,

Xx,a,x′,h

(cid:27)

where
reparameterization to rewrite this in terms of

q, J : D(Jh(x, a,

C1 =

x, a)qh(x, a))

Ph(

),
·

·|

≤
Ph(x′

(cid:8)

b

x, a)
ǫh(x, a)qh(x, a) (
x, a) = Jh(x, a, x′)/qh(x, a), noting that
|

. Then, we can use the reverse
x, a) is a

Ph(

(cid:9)

∀

·|

e

e

13

valid probability density by constraints on J, q. We get,

qh(x, a)(

−

Vh(x)

−

γh(x, a) + r(x, a)) + V1(x1)

max
P ∈P
q,

min
V,γ

(cid:26) Xx,a,h

e

+

Ph(x′

x, a)qh(x, a)(Vh+1(x′) + γh(x, a))
|

(cid:27)

Xx,a,x′,h
Vh(x) + r(x, a) +

e

Ph(x′

x, a)Vh+1(x′)
|

(cid:19)

+ V1(x1)

,

(19)

(cid:27)

= max
P ∈P
q,

min
V

qh(x, a)

(cid:18)

−

(cid:26) Xx,a,h
Ph(

Xx′
(
∀

e
x, a, h)

P

e
∈

where

∆ : D(

P
Ph(y

, and the last equality follows since
=
x, a) = 1. This is the Lagrangian dual form of the original optimization problem we considered.
|
Let OBJ(a) denote the objective function of the optimization problem in equation (a). It then follows that,
P

ǫh(x, a)

x, a))

x, a),

Ph(

≤

(cid:9)

(cid:8)

b

e

e

·|

·|

y

e

OBJ(5) = OBJ(17) = OBJ(18) = OBJ(19)

and so strong duality holds for the problem in (5). Thus, by standard results (e.g., [11, Section 5.5.3]), we
P +, V +), the optimal solutions to the primal and
conclude that the KKT conditions are satisﬁed by (q+,
dual. As a consequence, complementary slackness also holds. This concludes the proof.

A.1 Duality Result

e

Proposition 1. Let CBh(x, a) = D∗(Vh+1|
The optimization problem in (5) can be equivalently written as

ǫh(x, a),

Ph(

·|

x, a)) and denote its vector representation by CBh,a.

Proof. It will be helpful to write the primal optimization problem as

b

subject to
ra +
Vh ≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)

a
Ph,aVh+1 + CBh,a ∀
b

minimize
V

V1(x1)

qh(x, a)r(x, a)

, h

∈ A

∈

[H]

(6)

maximize
q∈Q(x1), ˜P ,κ

Subject to

Xx,a,h

qh(x, a) =

Xx′,a′
a
X
κh(x, a, x′) = ˜Ph(x′

Ph(

x, a),

Ph(

·|

·|
κh(x, a, x′) = 0
b

b

D

(cid:16)

Xx′

Xx′,a′

x, a)
|
x, a)

Ph(x′

x, a)
|

ǫh(x, a)
b

−

≤

(cid:17)

ˆPh(x
x′, a′)qh(x′, a′) +
|

κh(x′, a′, x)qh(x′, a′)

x

∀

∈ S

, h

[H]

[H]

[H]

∈

∈

∈

, h

∈ A

, h

∈ Z

x, x′
∀

, a

∈ S
(x, a)

∀
(x, a)

∀

, h

∈

∈ Z

[H].

By Lemma 10, we know that this problem exhibits strong duality. We then consider the partial Lagrangian
of the above problem without the constraints on

P , which yields

(q, κ; V ) =

L

qh(x, a)

Xx,a,h

y

(cid:18) X

ˆPh(y

x, a)Vh+1(y) +
|

e

y
X

κh(x, a, y)Vh+1(y) + r(x, a)

−

Vh(x)

+ V1(x1)

(cid:19)

deﬁned in (4), we know that the optimal value of the objective function of the primal optimization

For
problem is given by the Lagrangian relaxation,

P

min
V

max

q≥0,κ,

P ∈P L

(q, κ; V ).

e

14

To proceed, we ﬁx a V and consider the inner maximization problem. By deﬁnition of κh(x, a, x′) =
Ph(x′

Ph(x′

x, a)
|

−
max
b
P ∈P L

q≥0,κ,

x, a)), we can write
|
(q, κ; V )

e

= max
e
q≥0,κ,

P ∈P

qh(x, a)

Xx,a,h

y

(cid:18) X

Ph(y

x, a)Vh+1(y) +
|

b

y
X

κh(x, a, y)Vh+1(y) + r(x, a)

−

Vh(x)

(cid:19)

+ V1(x1)

= max
q≥0

e

Xx,a,h

qh(x, a)

y

(cid:18) X

= max
q≥0

qh(x, a)

Xx,a,h

y

(cid:18) X

b

Ph(y

x, a)Vh+1(y) +
|

max
κh(x,a,·)

Ph(·|x,a)∈Ph(x,a) X

y

κ(x, a, y)Vh+1(y) + r(x, a)

e

−

Vh(x)

+ V1(x1)

(cid:19)

Ph(y

x, a)Vh+1(y) + D∗(Vh+1|
|

ǫh(x, a),

Ph(

x, a)) + r(x, a)

·|

−

Vh(x)

(cid:19)

b

b

+ V1(x1),

(20)

where
follows from the deﬁnition of the conjugate D∗:

Ph(x, a) is the set in (16). The second equality crucially uses that qh(x, a)

≥

0 and the last equality

max

κh(x, a, y)Vh+1(y)

κh(x,a,·),

Ph(·|x,a))∈Ph(x,a)

y
X

= max

e

Ph(·|x,a)∈∆ (cid:26)

Ph(

x, a)

·|

−

·|

Ph(

x, a), Vh+1

; D(

Ph(

x, a),

Ph(

x, a))

·|

·|

ǫh(x, a)
}

≤

= D∗(Vh+1|

e

(cid:10)
e
ǫh(x, a),

Ph(

b
x, a)).

·|

(cid:11)

e

b

We then optimize the expression in (20) with respect to q and V using an adaptation of techniques
let g(V ) =
P . Then, deﬁne ηh(x, a) =

used for establishing LP duality between the original problems (1) and (2). Speciﬁcally,
maxq L
Ph(y

(q; V ) and note that by (20), the Lagrangian no longer depends on κ or
Ph(

x, a)) + r(x, a)

ǫh(x, a),

b

x, a)Vh+1(y) + D∗(Vh+1|
|

·|

−

Vh(x) for all x, a, h and observe that
e

y

P

b

g(V ) = V1(x1) + max

q

b

qh(x, a)ηh(x, a) =

V1(x1)

(

∞

if ηh(x, a)
otherwise.

0

≤

x, a, h

∀

Xx,a,h

Thus, we can then write the dual optimization problem of minimizing g(V ) with respect to V as

minimize
V

V1(x1)

Subject to Vh(x)

≥

This proves the proposition.

r(x, a) +

Ph(y

y
X

b

x, a)Vh+1(y) + D∗(Vh+1|
|

ǫh(x, a),

Ph(

x, a)).

·|

b

A.2 Properties of the Optimal Solutions

In this section we prove Propositions 2 and 3. In order to prove Proposition 2, we ﬁrst need the following
result which gives the form of the optimal solution to the dual in Equation (6).

15

Lemma 11. The solution to the dual in (6) is given by

V +
h (x) = max
a∈A

(cid:26)

r(x, a) + CBh(x, a) +

Ph(y

x, a)V ∗
|

h+1(y)

(cid:27)

(21)

Xy∈S

b
x, a))).

where we use the notation CBh(x, a) = D∗(Vh+1|
Proof. The structure of the constraints on Vh(x) in (6) and the deﬁnition of CBh(x, a) mean that V +
can be determined using only the values of V +
l
induction on h = H, . . . , 1. For the base case, when h = H, the constraint in the dual is

h (x)
h + 1. Hence, we can prove the result by backwards

ǫh(x, a),

for l

Ph(

≥

b

·|

VH (x)

r(x, a) + CBH (x, a)

x

, a

.
∈ A

≥
In order to minimize VH (x), we set V +
H (x) = maxa∈A{
. Now assume that
for stage h + 1, the optimal value of V +
h+1(x) is given by (21). Then, when considering stage h, we wish to set
V +
h (x) as small as possible. By the inductive hypothesis, we know it is optimal to set Vh+1(x) = V +
h+1(x), and
we know that CBh(x, a) has been deﬁned using only terms from stage h + 1 and is minimal. Consequently,
the RHS of the constraint in (6) is minimized for any (x, a, h) by setting Vh+1 = V +
h+1. This means that the
minimal value of Vh is given by (21). Hence the result holds for all h = 1, . . . , H, and so considering h = 1
and initial state x1, we can conclude that V + is the optimal solution to the LP in (6).

r(x, a) + CBH (x, a)
}

for all x

∈ S

∈ S

∀

We now prove Proposition 2.

Proposition 2. Let V + be the optimal solution to (6) and CB+
ǫ(x, a),
optimal policy π+ extracted from any optimal solution q+ of the primal LP in (5) satisﬁes

h (x, a) = D∗(V +

h+1|

Ph). Then, the

V +
h (x) = r(x, π+

h (x)) + CB+

h (x, π+

h (x)) +

Ph(y

x, π+
|

h (x))V +

h+1(y)

b
[H].

x

∀

∈ S

, h

∈

Proof. By Lemma 11, we know that the optimal solution to the dual in (6) is given by

Xy∈S

b

V +
h (x) = max
a∈A

r(x, a) + CBh(x, a) +

Ph(y

(cid:26)

Xy∈S

x, a)V +
|

h+1(y)

.
(cid:27)

b

We then proceed by considering the case where the right hand side of the expression in (22) has a unique

maximizer. In this case, let

a∗
h(x) = arg max

r(x, a) + CBh(x, a) +

a∈A (cid:26)

Ph(y

x, a)V +
|

h+1(y)

(cid:27)

.

Xy∈S

b

Since a∗
h(x) is the unique maximizer of this expression, it follows that, for a ﬁxed x, h, the constraint in (6)
is only binding for one a
h(x). By Lemma 10, we know that complementary slackness holds
for this problem. Then, using complementary slackness, it follows that only one of the primal variables is
non-zero. In particular, for a ﬁxed state x and stage h, q+
. Consequently,
π+(x) = a∗
h(x) and so the policy induced by q+, π+, will only have non-zero probability of playing the action
which maximize the right hand side of (22).

h (x, a) = 0 for all a

, namely a∗

h(x), x′

= a∗

∈ A

∈ S

and h

We now consider the case where there are multiple maximizers of the right hand side of (22). Let
h (x) denote the m maximizers. By a similar argument to the previous case, we know that for
[m]. Then, by
∈
[m], and so the only non-zero
[m]. The action chosen from state x by policy
h (x, a) > 0 by properties of the relationship between occupancy

h(x), . . . , am
a1
a ﬁxed x
∈ S
∈
complementary slackness, it follows that q+
values of q+
h (x, a) can occur for a = ai
π+ must be one of the actions for which q+
measures and policies. Hence, π+(x) = ai

[H], the constraint in (6) is only binding for a = ai
i (x) for i

[m], and so equation (8) must hold.

h (x, a) = 0 for all a

h(x) for some i

h(x) for some i

= a∗

∈

∈

h(x) for some i

∈

16

(8)

(22)

6
6
Proposition 3. If the true transition function P satisﬁes the constraint in Equation (5), the optimal solution
V + of the dual LP satisﬁes V ∗

h + 1 for all x

H

.

h (x)

V +
h (x)

≤
Proof. We begin by proving that if P

≤

−
, then V ∗
h (x)

∈ P

V +
h (x).

≤

∈ S

Let q∗ be the occupancy measure corresponding to the optimal policy π∗ under P . Then, if P

, then

∈ P

P must feasible for the primal in (5), and so it must be the case that

H

x,a
X

Xh=1

r(x, a)q∗

h(x, a)

H

≤

x,a
X

Xh=1

r(x, a)q+

h (x, a),

where q+ is the optimal solution to the LP in (5). Considering the LHS of this expression, and the fact that
q∗ is the occupancy of the optimal policy π∗ under the true transition function, it follows that

Hence, when P

,

∈ P

H

x,a
X

Xh=1

r(x, a)q∗

h(x, a) = E
(cid:20)

H

Xh=1

r(Xh, π∗(Xh))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X1 = x1

= V ∗

1 (x1)

(cid:21)

V ∗
1 (x1)

≤

H

x,a
X

Xh=1

r(x, a)q+

h (x, a) = V +

1 (x1).

for the initial state x1, where we have used the fact that the value of the optimal objective functions are
equal due to strong duality (Lemma 10).
In order to prove the result for x

= 1, we consider modiﬁed linear programs deﬁned by
starting the problem at stage h with all prior mass in state x. In this case, deﬁne the initial state as xh = x,
the we write the modiﬁed primal optimization problem as

= x1 and h

maximize
P ∈∆
q∈Q(x),

Subject to

e

H

qlx, a)r(x, a)

x,a
Xl=h X

ql(x, a) =

Xa∈A
D

(cid:16)

e

Pl(

x, a),

·|

Xx′∈S,a′∈A
x, a)
Pl(

·|

(cid:17)

(23)

x′a′)ql−1(x′, a′)
Pl(x
|

x

∀

∈ S

, l = h + 1, . . . , H

e
≤

ǫl(x, a),

(x, a)

∀

∈ S

, l = h + 1, . . . , H

Q

(x) has been modiﬁed to account for the new initial state. Observe that this problem is analogous
where
to the primal optimization problem in (5), and hence we can apply the same techniques as used to prove
Proposition 1 to show that the dual can be written as

b

minimize
V

Vh(x)

subject to Vl(x)

≥

r(x, a) + CBl(x, a) +

Xy∈S

Pl(y

x, a)Vl+1(y)
|

(x, a)

∀

, l

∈ S × A

∈

[h : H].

b

(24)

·|

Pl(

ǫl(x, a),

x, a)). Analyzing this dual shows that for l = h, . . . , H and x

,
where CBl(x, a) = D∗(Vl+1|
∈ S
the constraints on Vh(x) here are the same as those in the full dual in (6). This means that the dual in
(6) can be broken down per stage and the optimal solution can be found by a dynamic programming style
algorithm. In particular, the optimal solution V +
h (x) in the complete dual in (6) is given by the optimal
value of the objective function in the optimization problem in (24). Note that strong duality also applies in
this modiﬁed problem since the technique used to prove this in Lemma 10 also applies here. We therefore
know that V +
l (x, a)r(x, a) where ˜q+ is the optimal solution to the modiﬁed LP in (23).
On the event that P is in the conﬁdence set, the occupancy measure ˜q∗ deﬁned by the optimal policy π∗

h (x) =

l=h ˜q+

x,a

b

H

P

P

17

6
6
and P starting from state x in stage h must be a feasible solution to the LP in (23). Consequently, by the
same argument as before,

H

V ∗
h (x) =

r(x, a)˜q∗

l (x, a)

x,a
X

Xl=h

H

≤

x,a
X

Xl=h

r(x, a)˜q+

l (x, a) = V +

h (x),

thus proving the ﬁrst inequality in the statement of the proposition for all (x, a)

, h = 1, . . . , H.

∈

∈

≤

−

H

, h

, h

∈ S

We now show that V +

h (x)

h + 1 for all x

[H]. The proof is similar to the previous case
∈ S
and again relies on building a new MDP from each state x in stage h and considering the dual. In particular,
[H], in the dual LP in (24), we see that that the optimal solution to the objective
for any x
function has value V +
l (x, a)r(x, a),
h (x). By strong duality, this must have the same value as
the optimal value of the objective function of the primal optimization problem in (23) started at x in stage
h. The optimal solution ˜q+ must be a valid occupancy measure since by the primal constraints ql(x, a)
0
x,a ql(x, a) = 1 for all l = h + 1, . . . , H by
and
Lemma 12. From this it follows that ˜q+
, l = h, . . . , H so combining this with the fact
P
h + 1, and so
that r(x, a)
V +
H
h,t(x)

[0, 1]
h + 1 and the result holds.

(x, a)
, it must be the case that

It also follows that
1,
l (x, a)

x,a qh(x, a) = 1 are satisﬁed.

l (a, x)r(x, a)

l=h ˜q+

l=h ˜q+

(x, a)

P
∈ Z

∈ Z

P

P

H

x,a

x,a

≤

≥

−

≤

∀

∀

H

H

∈ Z

P

P

∈
−

≤

Lemma 12. For any feasible solution q to the primal problem in (5), it must hold that
for all h

[H].

∈

x,a qh(x, a) = 1

P

Proof. The proof follows by induction on h. For the base case, when h = 1,

q1(x, a) =

q1(x1, a) = 1

x,a
X
x = x1}
by the constraint
for h + 1. By the ﬂow constraint (ﬁrst constraint in (5)), for any feasible

a q1(x, a) = I
{

for all x

a
X

∈ S

. Now assume the result holds for h, and we prove it
,

P

P

qh+1(x, a) =

∈ P

x′, a′)qh(x′, a′)
Ph(x
|

=

e
qh(x′, a′) = 1
Xx′,a′

x,a
X

x (cid:18) Xx′,a′
X
x′, a′) = 1. Thus the result holds for all h = 1, . . . , H.
Ph(x
|

(cid:19)

e

since

x

P

A.3 Regret Bounds

e

In this section, we bound the regret of any algorithm that ﬁts into our framework.

Theorem 4. On the event

K
t=1{

∩

P

∈ Pt}

, the regret is bounded with probability at least 1

δ as

−

RT ≤

where CB−

t=1
X
h,t(x, a) = D∗(

Xh=1 (cid:18)
V +
h+1,t|

−

K

H

CBh,t(xh,t, πh,t(xh,t)) + CB−

h,t(xh,t, πt(xh,t))

+ H

2T log(1/δ)

ǫh,t(x, a),

Ph,t) and CBh,t(x, a) = D∗(V +

(cid:19)

h+1,t|

p
ǫh,t(x, a),

Ph,t).

Proof. The proof is similar to standard proofs of regret for episodic reinforcement learning algorithms (e.g.
[6, 23]) but uses Proposition 3 to simplify the probabilistic analysis and the deﬁnition of the conﬁdence sets
V πt
to simplify the algebraic analysis. For the proof, for any h, t, deﬁne ∆h,t(xh,t) = V +
h (xh,t). Then
using the optimistic result from Proposition 3, on the event

, we can write the regret as

h,t(xh,t)

−

P

b

b

K
t=1{

∩

∈ Pt}

K

RT =

(V ∗

1 (x1,t)

t=1
X

V πt
1 (x1,t))

−

K

≤

t=1
X

(V +

1,t(x1,t)

−

V πt
1,t (x1,t)) =

∆1,t(x1,t).

K

t=1
X

18

Then, for a ﬁxed h, t, we consider ∆h,t(xh,t) and show that this can be bounded in terms of ∆h+1,t(xh+1,t),
some conﬁdence terms and some martingales. In particular, using the Bellman equations and the dynamic
programming formulation, we can write

V πt
h (xh,t)

=

Ph,t(

∆h,t(xh,t) = V +

h,t(xh,t)
−
xh,t, ah,t), V +
·|
xh,t, ah,t), V +
·|
= ∆h+1,t(xh+1,t) +

Ph,t(
(cid:10)
b

=

(cid:10)

h+1,t

Ph,t(

h+1,t

b

+ r(xh,t, ah,t) + CBh,t(xh,t, ah,t)
xh,t, ah,t), V πt
h+1
·|
V +
h+1,t(xh+1,t)
xh,t, ah,t), V πt
h+1
·|

Ph(
(cid:11)
−
xh,t, ah,t), V +
(cid:11)
·|
+ V πt
h+1(xh+1,t)
xh,t, ah,t)

Ph(
(cid:11)
xh,t, ah,t), V +
·|

−
Ph(
(cid:10)

h+1,t

h+1,t

−

−

·|

(cid:11)

(cid:10)

b
Ph,t(

= ∆h+1,t(xh+1,t) +
where in the last equality, ζπ

(cid:10)

(cid:10)

Ph(

xh,t, ah,t), V πt
h+1
·|

−
+ CBh,t(xh,t, ah,t)

(cid:10)

r(xh,t, ah,t)

−

(cid:11)

+ CBh,t(xh,t, ah,t)

+ ζπ
(cid:11)

h+1,t + CBh,t(xh,t, ah,t)

h+1,t is a martingale diﬀerence sequence deﬁned by
V πt
h+1

V +
h+1,t(xh+1,t)

b
Ph(

(cid:11)

−

V πt
h+1(xh+1,t)

.

−

(cid:11)

(cid:0)

(cid:1)

ζπ
h+1,t =
Then observe that on the event P

(cid:10)
xh,t, ah,t)

Ph,t(

·|

h+1,t −

xh,t, ah,t), V +
·|
∈ Pt,
xh,t, ah,t), V +
·|
xh,t, ah,t)

Ph(

−
Ph,t(

h+1,t

(cid:10)
b
≤

max
P ∈Ph(xh,t,ah,t)

·|

Ph(

xh,t, ah,t), V +
(cid:11)
·|

h+1,t

−

≤

max
e
P ∈∆ (cid:26)
(cid:10)

Ph,t(

·|

e

= max

P ∈∆ (cid:26)
(cid:10)

e

e

(cid:10)
xh,t, ah,t)

b

e

Ph(

xh,t, ah,t), V +
·|

h+1,t

−

(cid:11)

:

b

e

(cid:11)
xh,t, ah,t),

D(

Ph(

·|

Ph,t(

xh,t, ah,t))

·|

Ph(

xh,t, ah,t)

Ph,t(

−

·|

·|

e
xh,t, ah,t),

V +
h+1,t

b
:

−

b

D(

Ph(

·|

(cid:11)
Ph,t(
xh,t, ah,t),

xh,t, ah,t))

·|

≤

≤

ǫh,t(xh,t, ah,t)

(cid:27)

ǫh,t(xh,t, ah,t)

(cid:27)

V +
= D∗(
h+1,t|
−
= CB−
h,t(xh,t, ah,t)

ǫh,t(xh,t, aht),

b

Pt(

xh,t, ah,t))
e

·|

b

This gives a recursive expression for ∆h,t(xh,t),
∆h+1,t(xh+1,t) + ζπ

∆h,t(xh,t)

≤

h+1,t + CBh,t(xh,t, ah,t) + CB−

h,t(xh,t, ah,t)

Recursing over h = 1, . . . , H, we see that,

∆1,t(x1,t)

since ∆H+1,t(x) = 0.

H

≤

Xh=1

CBh,t(xh,t, πt(xh,t)) +

H

Xh=1

CB−

h,t(xh,t, πt(xh,t)) +

ζπ
h+1,t

H

Xh=1

By Azuma-Hoeﬀdings inequality, it follows that

K

H

ζπ
h+1,t ≤

H

2T log(1/δ)

t=1
X

Xh=1

p

−
K

H

with probability greater than 1

Consequently, with probability greater than 1

−

δ, since the sequence has increments bounded in [
δ, we can bound the regret by,

−

H, H].

K

H

RT ≤

t=1
X
thus giving the result.

Xh=1

CBh,t(xh,t, πt(xh,t)) + +

CB−

h,t(xh,t, πt(xh,t)) + H

2T log(1/δ)

t=1
X

Xh=1

19

p

A.4 Upper bounding the exploration bonus
We now prove the regret bound, when we use an upper bound D†
below result that shows that the optimistic value function V † in equation 9 is indeed optimistic.

∗ on the conjugate D∗. We ﬁrst need the

Lemma 13. On the event P

, it holds that V ∗

1 (x1)

∈ P

V †
1 (x1).

≤

Proof. We consider the dual optimization problem,

minimize
V

V1(x1)

subject to Vh(x)

Vh(x)

r(x, a) +

y
X

h + 1

H

−

≥

≤

Ph(y

x, a)Vh+1(y) + D∗
|

b

Vh+1

ǫh(x, a),

Ph(

x, a)

·|

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

b

(25)

(26)

which is the dual from Proposition 1, where we have added the additional constraint (26). Note that adding
this additional constraint will not eﬀect the value of the optimal solution since by Proposition 3, we know
that V +(x)

h + 1 for all h = 1, . . . , H, x

H

≤
By deﬁnition of D†

−

.
∈ S
∗, it follows that for any Vh+1,

r(x, a) +

Ph(y

x, a)Vh+1(y) + D∗
|

Vh+1

ǫh(x, a),

Ph(

x, a)

·|

y
X
min

b
H

≤

−

(cid:26)

h + 1, r(x, a) +

Ph(y

(cid:16)

(cid:12)
(cid:12)
b
(cid:12)
x, a)Vh+1(y) + D†
∗
|

(cid:17)

Vh+1

ǫ′
h(x, a),

(cid:16)
since all the original feasible solutions in stage h + 1 must satisfy Vh+1(x)
the constraint in (25) by

b

(cid:12)
(cid:12)
(cid:12)
H

−

≤

y
X

Ph(

x, a)

·|

(cid:17) (cid:27)

b

h. Therefore, we can replace

Vh(x)

≥

min

H

(cid:26)

−

h + 1, r(x, a) +

y
X

b

P (y

x, a)Vh+1(y) + D†
∗
|

Vh+1

ǫ′
h(x, a),

P (

x, a)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

·|

b

(cid:17) (cid:27)

knowing that this will only increase the optimal value of the objective function. Since we know that by
V +
Proposition 3, that the optimal solution to the original dual optimization problem satisﬁes V ∗
1 (x1)
on the event P
1 (x1,t) the optimal solution of
the modiﬁed dual. Note also that the solution to the modiﬁed dual problem will take the form given in (9)
by an argument similar to Lemma 11.

, it must also be the case that V ∗

1 (x1,t) for V †
V †

1 (x1,t)

1 (x1)

∈ P

≤

≤

Theorem 5. Let D†
ǫ′,
∗(f
|
h,t(x, a) = D†
and, CB†
∗(V †
Ph,t). Then, on the event
δ, the policy returned by the procedure in (9) incurs regret
than 1

P ) be an upper bound on D∗(f

ǫ′
h,t(x, a),

h+1,t|
b

ǫ,
|

b

P ) and D∗(

f
−
K
P
t=1{

∩

P ) for every f :

[0, H],
, with probability greater

S →

ǫ,
|
∈ Pt}
b

−

b
CB†

h,t(xh,t, πh,t(xh,t)) + 4H

RT ≤

2

K

H

t=1
X

Xh=1

2T log(1/δ).

p

Proof. Given the result in Lemma 13, we know that V † is optimistic so the proof proceeds similarly to the
case where CBh,t(x, a) is computed exactly. In particular, let ∆†

V π(xh,t), then,

h,t(xh,t) = V †(xh,t)

−

K

RT =

(V ∗

1 (x1,t)

t=1
X

V πt
1 (x1,t))

−

≤

K

(V †

1,t(x1,t)

t=1
X

V πt
1,t (x1,t)) =

−

K

≤

t=1
X

∆†

h,t(xh,t)

and, observe that by the same argument as Theorem 4,

∆†

h,t(xh,t) = ∆†

h+1,t(xh+1,t) +

P (

xh,t, ah,t)

·|

−

P (

xh,t, ah,t), V †
·|

h+1,t

+ ζ†

h+1,t + CB†

h,t(xh,t, ah,t)

(cid:10)

b

20

(cid:11)

h+1,t is the martingale diﬀerence sequence ζ†

where ζ†
V πt
h+1(xh+1,t)). Then, on the event P

,

h+1,t =

∈ P

P (

xh,t, ah,t, V †
·|

h+1,t −

(cid:10)

V πt
h+1

−

(cid:11)

(V †

h+1,t(xh+1,t)

−

Pt(

xh,t, ah,t)

·|

−

P (

xh,t, ah,t), V †
·|

h+1,t

(cid:10)

b

≤

max
P ∈∆ (cid:26)

Pt(

xh,t, ah,t)

·|

(cid:11)
P , V †
h+1,t

: D(

P ,

−

Pt(

xh,t, ah,t))

·|

≤

ǫh,t(xh,t, ah,t)

(cid:27)

(cid:10)
b
P

−

e
xh,t, ah,t),

Pt(

·|

(cid:11)

V †
h+1,t

−

e
: D(

b
P ,

b
Pt(

xh,t, ah,t), ǫh,t(xh,t, aht))

(cid:11)

= max
e
P ∈∆ (cid:26)

= D∗(
e
D†

−
∗(V †

≤

(cid:10)
V †
e
h+1,t|

h+1,t|

·|
xh,t, ah,t), ǫ′
Pt(
b
·|

e
b
CB†

≤

h,t(xh,t, aht))

h,t(xh,t, ah,t)

Pt(

xh,t, ah,t))

·|

≤

ǫ(xh,t, ah,t)

(cid:27)

by deﬁnition of the upper bound CB†

b
Using this, we can recurse over h = 1, . . . , H to get,

h,t(x, a).

∆†

1,t(x1,t)

2

≤

H

Xh=1

CB†

h,t(xh,t, ah,t) +

ζ†
h+1,t

H

Xh=1

so summing this over all episodes t = 1, . . . , K and using Azuma’s inequality to bound the sum of the
martingales gives the result.

A.5 Further Details of Examples

Here we present additional results and explanations to show that many algorithms ﬁt into our framework.
The main purpose of this section is to demonstrate the use of our general results for constructing conﬁdence
sets and calculating the corresponding exploration bonuses, as well as bounding the regret. We do not aim to
improve over state-of-the-art results or obtain tight constants, but we do note that several of the exploration
bonuses we derive are data-dependent in a way that may possibly enable tight problem-dependent regret
bounds. We refer to the works of Dann et al. [17], Zanette and Brunskill [50], Simchowitz and Jamieson [41]
that demonstrate the power of data-dependent exploration bonuses for achieving such guarantees.

In several calculations below, we will use the following simple result to bound the sum of the exploration

bonuses:

K

H

t=1
X

Xh=1 s

1
Nh,t(xh,t, ah,t)

=

K

H

xh,t = x, ah,t = a

I
{

Xx∈S,a∈A

t=1
X
H

Xh=1
Nh,K (x,a)

=

≤

Xx∈S,a∈A
2√HSAT

Xh=1

n=1 r
X

1
n ≤

1
Nh,t(xh,t, ah,t)

}s

H

2

Nh,K(x, a)

Xx∈S,a∈A

Xh=1

q

(27)

x∈S,a∈A,h∈[H] Nh,K(x, a) =

where the last inequality follows due to the Cauchy–Schwarz inequality and the fact that
HK = T . We also use the modiﬁed empirical transition probability deﬁned for any states x, x′
a

[H] and episode t

, stage h

[K] as

P

∈ A

∈

∈

P +
h,t(x′

x, a) =
|

max

{

1, Nh,t(x, a, x′)
}
Nh,t(x, a)

and note that this only diﬀers from

b
Ph,t(x′

x, a) if Nh,t(x, a, x′) = 0. Consequently,
|

P +
h,t(x′
|

x, a)
|

−

Ph,t(x′

b
x, a)
|
|

=

b

b

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max

{

1, Nh,t(x, a, x′)
}
Nh,t(x, a)

Nh,t(x, a, x′)
Nh,t(x, a)

−

21

1
Nh,t(x, a)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, action

∈ S

(28)

(29)

P to avoid
In several cases, we deﬁne the primal conﬁdence sets using
division by 0. Note that doing this results in dual formulations that involve
P . However,
since we are still optimizing over the space of probability distributions in the primal, it holds that the optimal
value of the dual objective will still be bounded by H. We can also use Equation (29) to bound the empirical
variance of any function z :

P + as the reference model rather than

P + rather than

[0, H] under

P +,

b

b

b

b

S →

V+(z) =

P +(y)(z(y)

b

b
P (y)(z(y)

y
X

2

≤

y
X

b

P +, z

− h

)2
i

b
≤

b
P , z

− h

)2 +
i

b

y
X

(cid:18)
b
2HS + 2( HS
N )2
N

P +(y)

2(z(y)

P , z

− h

)2 + 2(
i

HS
N

)2

(cid:19)

+

b
H 2S2
N 2 ≤

V(z) +

2

2HS
N

+

3H 2S2
N 2

.

(30)

b

A.5.1 Total variation distance

We start with the classic choice of the ℓ1 distance D(p, p′) =
algorithm of Jaksch et al. [23]. Deﬁning the conﬁdence sets used in episode t as

p
k

p′

−

k1 which underlies the seminal UCRL2

Pt =

for

ǫh,t(x, a) =

P

∆ :

Ph(

x, a)

Pt(

x, a)

·|

−

·|

∈

ǫh,t(x, a)

1 ≤

(x, a)

∀

, h

∈

∈ Z

[H]

(cid:27)

(cid:26)

s

(cid:13)
(cid:13)
(cid:13) e
e
2S log(2SAT /δ)
Nh,t(x, a)

b

(cid:13)
(cid:13)
(cid:13)

we know that P
distance is,

∈ Pt for all t = 1, . . . , K with probability greater than 1

−

δ [23]. Then, the conjugate

D∗(f

ǫ,
|

P ) = max
P ∈∆

P , f

P

−

ǫ

1 ≤

= min
λ∈R

max
P ≥0

P

P , f

−

−

λ1

P

b

min
λ∈R

≤

(cid:26)

(cid:10)
max
P ∈RS

P

P

(cid:12)
(cid:13)
(cid:12)
(cid:11)
(cid:13)
(cid:12)
(cid:13)
P , f
(cid:12)

−

−

−

(cid:13)
(cid:13)
b
λ1
(cid:13)

b
P

(cid:26)
(cid:10)

b

(cid:12)
(cid:12)
(cid:11)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)

(cid:27)

−

P

P

1 ≤

ǫ

(cid:27)

(cid:13)
(cid:13)
(cid:13)

b

(cid:26)
(cid:10)
ǫ min

b
f
λ∈R k

≤

(cid:12)
(cid:13)
(cid:12)
(cid:11)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
k∞ ≤

λ1

−

P

−

1 ≤

ǫ

(cid:27)

(cid:13)
(cid:13)
(cid:13)

b
ǫ sp(f )/2

where we have deﬁned λ as the Lagrange multiplier of the constraint
that the dual norm of the ℓ1 norm is the ℓ∞ norm and, denoted by sp(f ) = maxx f (x)
−
P
of f . Noting that a similar result holds for D∗(
exploration bonus

P ), we can deﬁne D†

x P (x) = 1 =

P (x), used the fact
minx f (x) the span
b
P ) = ǫ sp(f )/2, and use the

ǫ,
|

∗(f

P

f

x

−

ǫ,
|
h,t(x, a) = ǫh,t(x, a)sp(V †

b

CB†

h+1,t)/2

b

Since we are clipping V +
H. Applying Theorem 5
and using the bound of Equation (27) to bound the sum of the exploration bonuses shows that the regret
O(S√AH 3T ). This recovers the classic UCRL2 guarantees that can be
of this algorithm is bounded by
deduced from the work of [23].

h + 1], we can bound sp(V †
h )

h to be in the range [0, H

≤

−

A.5.2 Variance-weighted ℓ∞ norm

e

We can get tighter bounds by using the empirical Bernstein inequality [33] to constrain the transition function.
as the reference model in the primal conﬁdence sets. The constraints
Here, we use
considered here are related to those used in the UCRL2B algorithm of Fruit et al. [21]. Speciﬁcally, we
can apply the empirical Bernstein inequality to show that the following bound holds for all x, a, x′, h, t with

P + = max{1,Nh,t(x,a,y)}

Nh,t(x,a)

b

22

probability at least 1

δ:

−
Ph(x′

x, a)
|

−

P +
h,t(x′

(cid:12)
(cid:12)
(cid:12) b

x, a)
|

≤

(cid:12)
(cid:12)
Ph,t(x′
(cid:12)

2

(cid:12)
(cid:12)
(cid:12) b
x, a)
|

Ph,t(x′

x, a)
|

−

Ph(x′

x, a)
|

+

P +
h,t(x′

x, a)
|

−

Ph,t(x′

x, a)
|

Ph(x′

1

−

x, a)
|

Nh,t(x, a)
b

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12) b
log(HS2AT /δ)

(cid:12)
(cid:12)
(cid:12)

b

(cid:12)
(cid:12)
(cid:12)
7 log(HS2AT /δ)
3Nh,t(x, a)

+

+

1
Nh,t(x, a)

x, a) log(HS2AT /δ)
|
Nh,t(x, a)

x, a) log(HS2AT /δ)
|
Nh,t(x, a)

+

+

7 log(HS2AT /δ)
3Nh,t(x, a)

+

1
Nh,t(x, a)

7 log(HS2AT /δ)
3Nh,t(x, a)

+

1
Nh,t(x, a)

≤ v
u
u
t

≤ s

b
Ph,t(x′

2

b
P +
h,t(x′

2

≤ s

b

6 log(HS2AT /δ)

s

≤

P +
h,t(x′
x, a)
|
Nh,t(x, a)
b

The last inequality follows from the deﬁnition of the reference model that guarantees that Nh,t(x, a)
max

Nh,t(x, a, y), 1

{
In what follows, we will state a conﬁdence set inspired by the above result using the divergence measure

h,t(y

} ≥

x, a) =
|

1.

b

P +

, which is easily seen to be positive homogeneous and convex in both P and

P +) = maxx

D(P,
P +. Deﬁning ǫh,t(x, a) = 36 log2(HS2AT /δ)

P +(x)

(P (x)−

P +(x))2

Nh,t(x,a)

b

b

b

, we deﬁne the conﬁdence sets used in episode t as

b

Ph(
·|

x, a) =

(

Ph(

x, a)

·|

∆ : max

y

∈

P (y
(

e

x, a)
|
P +

P +

h,t(y
−
x, a)
h,t(y
|
b

x, a))2
|

ǫh,t(x, a)

)

≤

and
1

−

=

∩x,a,h{Ph(
·|

x, a)
}

P
δ.
The corresponding conjugate distance can be expressed by deﬁning λ as the Lagrange multiplier of the

∈ P

. By the above argument, we know that P

with probability greater than

b

constraint

x P (x) = 1 and writing

P
P +) = max
P ∈∆

D∗(f

ǫ,
|

b

= min
λ∈R

(cid:26)
(cid:10)
max
P ≥0

P +, f

P

−

max
x∈S

(cid:12)
(cid:12)
(cid:11)
(cid:12)
P +, f
(cid:12)

b
P

(cid:26)
(cid:10)

−

(P (x)

P +(x))2

−
P +(x)

b

ǫ

≤

(cid:27)

λ1

−

(cid:11)

λ(
b
−

x
X

P +(x)

−

1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
P +(x)
(

b
λ

b

(cid:26)
(cid:10)

min
|λ|≤H+ SH
N

max
P ∈RS

≤

P +, f

P

−

λ1

−

−

min
|λ|≤H+ SH
N

(f (x)

−

x (cid:12)
X
(cid:12)
(cid:12)
P +(x)
(cid:12)

f (x)
|

−

≤

≤

√ǫ

x q
X

b

b
λ)

(cid:11)

P +(x)

√ǫ +

q
b
P +f
|

+

(cid:12)
(cid:12)
2SH
(cid:12)
(cid:12)
N

.

b

max
x∈S

P (x)
|

P (x)
|

−
P +(x)
b

√ǫ

≤

(cid:27)

q
P (x))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b
1
N

x
X

b
H +

(cid:18)

SH
N

−

(cid:19)

P (x)
|

b
max
x∈S

P (x)
|

−
P +(x)
b

q

√ǫ

≤

(cid:27)

The same technique can be used to bound D∗(
P +f

+ 2SH

N and write the inﬂated exploration bonus in the form

ǫ,
|

−

f

P ), so we can deﬁne D†

∗(f

P ) = √ǫ

ǫ,
|

|

b

b

CB†

h,t(x, a) =

ǫh,t(x, a)

P +

h,t(y

V †
h+1(y)
x, a)
|
|

−

h,tV †
P +

h+1|

q

y q
X

b

b

23

b

+

2SH
Nh,t(x, a)

.

b

x

P

P +(x)

q

b

f (x)
|

−

By Theorem 5, we know that in order to bound the regret of this algorithm, we need to be able to bound
the sum of these exploration bonuses. For this, note that by the Cauchy–Schwarz inequality, and a similar
argument to (30),

ǫh,t(x, a)

P +

h,t(y

x, a)
|

V †
h+1(y)
|

−

h,tV †
P +

h+1|

q

y q
X

b

ǫh,t(x, a)

q

≤

Ph,t(y

y q
X

b

b
V †
h+1(y)
x, a)
|
|

Ph,tV †

h+1|

−

+ SH

ǫh,t(x, a)
Nh,t(x, a)

s

2 +

(cid:18)

s

H
SNh,t(x, a)

(cid:19)

b
V †
h+1(y)
|

−

x, a)
|

Ph,tV †

h+1|

+ 3SH

ǫh,t(x, a)
Nh,t(x, a)

s

P +

h,t(y

b
h+1(y)

x, a)(V †
|

−

Ph,tV †

h+1)2 + 3SH

ǫh,t(x, a)
Nh,t(x, a)

s

=

ǫh,t(x, a)

Ph,t(y

q

Xy:P (y)>0 q

b

ǫh,t(x, a)

Γh(x, a)

q

s

≤

≤

ǫh,t(x, a)Γ

q

b

Xy:P (y)>0

b
h+1) + 3SH

Vh,t(V †

b

ǫh,t(x, a)
Nh,t(x, a)

s

where Γh(x, a) is the number of next states which can be reached from state x after playing action a in stage
Vh,t is
h with positive probability, and Γ is a uniform upper bound on Γh(x, a) that holds for all x, a, and
the empirical variance using all data from stage h up to episode t. In order to bound CB†

h,t(xh,t, ah,t)

K
t=1

H
h=1(

ǫh,t(xh,t, ah,t)Γ

Vh,t(V †

h+1,t) + 3SH

≤
ǫh,t(x,a)
Nh,t(x,a) ), we use the Cauchy–Schwarz inequality and

b

techniques similar to Lemma 10 in [6] or Lemma 5 in [21] to show that
P

P

q

q

b

K

H

t=1
X

Xh=1

CB†

h,t(xh,t, ah,t)

C1√ΓL

v
u
u
t

K

H

t=1
X

Xh=1

1
Nh,t(xh,t, ah,t)

K

H

SA log(T )

t=1
X

Xh=1

b
h+1) + C2H 2

Vh(V πt

K

H

Vh,t(V †

h+1,t) + C4SH√L

K

H

t=1
X

Xh=1

1
Nh,t(xh,t, ah,t)

T log(T )

+ C4SH√LSA log(T )

(cid:18)

t=1
X

Xh=1

p

(cid:19)

HT + C3H 2√T L + C2H 2

T log(T )

+ C4SH√LSA log(T )

SA log(T )

≤

≤

≤

C1√ΓL

v
u
u
t

C1√ΓL

s

=

O(H√ΓSAT )

(cid:18)

p

(cid:19)

for some constants C1, C2, C3, C4 > 0, L = log(HS2AT /δ) and Vh the variance under Ph, where the
penultimate inequality follows from [6] and the last inequality holds for S3A
T Γ. This recovers the regret
bounds of Fruit et al. [21].

≤

e

A.5.3 Relative entropy

Inspired by the KL-UCRL algorithm of Filippi et al. [20], we also consider the relative entropy (or Kullback–
Leibler divergence, KL divergence) between
P as a divergence measure. The relative entropy between
two discrete probability distributions p and q is deﬁned as

P and

b
D(p, q) =

e

p(x) log

x
X

24

p(x)
q(x)

,

provided that p(x) = 0 holds whenever q(x) = 0. Being an f -divergence, the KL divergence satisﬁes
the conditions necessary for our analysis: positive homogeneous and jointly convex in its arguments (p, q).
However, it is not symmetric in its arguments, which suggests that it can be used for deﬁning conﬁdence
sets in two diﬀerent ways, corresponding to the ordering of P and
P . We describe the conﬁdence sets and
the resulting exploration bonuses below.

b

Forward KL-Divergence. We ﬁrst consider constraining the divergence D(P,

To address the issue that the empirical transition probabilities
the divergence with respect to
entropy to account for the fact that
follows, we consider the following divergence measure:

b

.
(cid:19)
, we deﬁne
P + (as deﬁned in equation (28)) and use the so-called unnormalized relative
b
P + may not be a valid probability distribution. Speciﬁcally, in what

P (y) may be zero for some y

y P (y) log

P ) =

∈ S

P (y)

P

(cid:18)

b

b

P (y)

b

D(P,

P ) =

P (y) log

y
X

b

P (y)
P +(y) (cid:19)

(cid:18)

b

+

P +(y)
(

P (y)).

−

y
X

b

The following concentration result will be helpful for the construction of the conﬁdence sets.

Lemma 14. With probability greater than 1
pair (x, a),

−

δ, it holds that for every episode t, stage h and state-action

D(Ph(x, a),

P +

h,t(x, a))

18S log(HSAT /δ)
Nh,t(x, a)

≤

b
Proof. We consider a ﬁxed h, t, x, a, and for ease of notation remove the dependence of P,
With probability greater than 1
HT SA , it follows that

δ

P on h, t, x, a.

−

+

P +(y)
(

P (y) log

P (y)
P +(y) (cid:19)

(cid:18)

y
X

b

P (y))

−

P (y)

(cid:18)

≤

y
X

P (y)
P +(y) −

1

+

(cid:19)

P +(y)
(

−

y
X

(Since log(x)
b

b
P (y))

x

−

≤

1 for x > 0)

y
X

=

=

y
X

y
X

2

2

≤

≤

y
X

y
X

b
P 2(y)

(P (y)

P +(y)

P (y)
−
P +(y)

b

P +(y))2

b
−
P +(y)
b
P (y))2

b
P +(y)
(

+

y
X

b

P (y))

−

(P (y)
b
−
P +(y)
b

P (y)
(

P +(y))2

+ 2

y
X

−
P +(y)
b

2

b
P (y) log(HS2AT /δ)/N + 6 log2(HS2AT /δ)/N 2
P +(y)

b

b

b

(By Bernstein’s inequality and (29))

1
P +(y)

N 2

+ 2

y
X

b

18S log(HS2AT /δ)
N

≤

b

where the last inequality follows since by deﬁnition
probability greater than 1
with probability greater than 1

1. Since this holds for each h, t, x, a with
HT SA , by the union bound, it follows that it holds simultaneously for all h, t, x, a

≥

−

δ.

δ

b

P +(y)N

−

Given the above result, we deﬁne our conﬁdence set as

Ph,t(
·|

x, a) =

Ph(

x, a)

·|

(cid:26)

e

∈

∆
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xx′

Ph(x′

x, a) log
|

e

25

Ph(x′
x, a)
|
P +
x, a) ≤
h,t(x′
|
e

ǫh,t(x, a)

(cid:27)

b

for

ǫh,t(x, a) =

CS log(HSAT /δ)
Nh,t(x, a)

for some constant C > 0. Using the notation KL(p, q) =
KL divergence, the conjugate of the above divergence can be written as

y p(y) log(p(y)/q(y)) to denote the normalized

D∗(z

ǫ,
|

P +) = max
P ∈∆

b

= min
λ≥0
e

= min
λ≥0

= min
λ≥0

= min
λ≥0

P

ǫ

≤

z,

P

P +

D(

P ,

P +)

−

e

z,

b
P

nD

z,

nD

z,

e
P

e
P

nD
max
P ∈∆

max
e
P ∈∆

max
e
P ∈∆

E(cid:12)
(cid:12)
P +
(cid:12)

−

P +
b

P +
b

−

−

b
λ

λ

λ

(cid:16)

(cid:16)

e
−

E

−

E

−

nD

λ log

(cid:16)
E
e
P +(x′)ez(x)/λ

b

o
P +)
P ,

D(

ǫ

−

P +) +
b
e
P ,
KL(

KL(

e
P ,

P +)
b

D
ǫ′

−

(cid:17)o
1,

P +

b

(cid:17)o

b

e
P +(x′)z(x′) + λǫ′

e
(cid:26)
P +

Xx′

b

−

Xx′

b

(cid:27)

ǫ

−

−

P

E

e

(cid:17)o

where we deﬁned ǫ′ = ǫ + 1
and used the well-known Donsker–Varadhan variatonal formula (see,
e.g., [10, Corollary 4.15]) in the last line. Thus, the exploration bonus can be eﬃciently calculated by a
line-search procedure to ﬁnd the λ minimizing the expression above.
b

− h

1,

i

A more tractable bound on the exploration bonus can be provided by noting that, for a vector z with

z

k

k∞ ≤

H, we have

D∗(z

ǫ,
|

P +) = min
λ≥0

b

= min
λ≥0

P +(y)z(y) + λǫ′

(cid:27)

−

y
X

λ log

P +(y)ez(y)/λ

(cid:26)

y
X

λ log

b
P +(y)e(z(y)−h

b
P +,zi)/λ + λǫ′

(cid:26)

y
X
λ log

b

b
P +(y)e(z(y)−h

(cid:27)
P +,zi)/λ + λǫ′

min
λ∈[0,H]

≤

min
λ∈[0,H]

≤

(cid:26)

(cid:26)

1
λ

2

ǫ′

≤

y
X
b
P +(y)

y
X

b
P +(y)(z(y)

b
P +, z

− h

z(y)

(cid:16)

P +, z

− h

b
)2 = 2
i

(cid:27)

+ λǫ′

(cid:27)

2

i
(cid:17)

ǫ′

V+(z)

q

y
X
where we used the inequality λ log E+[eX/λ]
long as
V+(z) to denote the variance of z under

X
|

| ≤

s

b
≤

P +(x)x that holds as
λ holds almost surely, and the result in Equation (29) several times. We also use the notation

E+[X] + 1
λ

x

b
E+[X 2] for E+[X] =

b

P +. Thus, deﬁning

P

b

b

ǫ′
h,t(x, a) = ǫh,t(x, a) +

y
X

b

P +(y
b

x, a)
|

−

1

≤

ǫh,t(x, a) +

S

Γ

−
Nt,h(x, a)

=

O

(cid:18)

S
Nh,t(x, a)

,
(cid:19)

the exploration bonus can be bounded as CBh,t(x, a)
argument yields the same bound for CB−
N + 3H2S2

V(z) + 2HS

h,t(x, a).

V+(z)

2

≤

q

b

2

By (29),

and so the exploration bonus can be bounded in the same way
as in the case of variance-weighted ℓ∞ constraints, plus some lower order terms that scale with 1/N . The
sum of these lower order terms can be straightforwardly bounded by a simple adaptation of the calculations
in Equation (27). Overall, the sum of the conﬁdence bounds can be bounded as

N 2

≤

b

b

ǫ′
h,t(x, a)

V+

h,t(V +

h+1,t), and using an identical

e

K

H

(CBh,t(xh,t, ah,t) + CB−

h,t(xh,t, ah,t))

t=1
X

Xh=1

C1HS√AT + C2H 2S2A log T

≤

26

for some C1, C2 = O(log(HSAT /δ)). Hence the regret can be bounded by

O(HS√AT ).

Reverse KL-Divergence. We now consider deﬁning conﬁdence sets in terms of the second argument of
the KL divergence, corresponding the the original KL-UCRL algorithm proposed by Filippi et al. [20], Talebi
and Maillard [46]. Speciﬁcally, deﬁne,

e

Ph,t(
·|

x, a) =

for

ǫh,t(x, a) =

Ph(

x, a)

·|

∈

(cid:26)
CS log(HSAT /δ)
Nh,t(x, a)

Xx′
.

e

∆
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ph(x′

x, a) log
|

b

Ph(x′
x, a)
|
x, a) ≤
Ph,t(x′
|
b

ǫh,t(x, a)

(cid:27)

e

for some constant C > 0. As shown by Filippi et al. [20], for an appropriate choice of C, this conﬁdence set
is guaranteed to capture the true transition function in all episodes with probability greater than 1

δ.

The conjugate of this distance for a ﬁxed x, a can be bounded as

−

D∗(z

ǫ,
|

P ) = max
P ∈∆

z,

P

P

D(

P ,

P )

ǫ

≤

nD
e
max
P ∈∆ (cid:26) D
max
e
P ∈∆ (cid:26) D

−

b
P

z,

E(cid:12)
(cid:12)
(cid:12)
−

e
P

z,

−

e

P

b
P

E

E

b

o
P ,

e

b
λ(D(
−

P )

−

ǫ)

e
λ(1/2
k

P

b
−

P

k

−

e

b

b

= min
e
λ≥0

min
λ≥0

≤

≤

sp(z)√2ǫ
e

(cid:27)
2
1 −

ǫ)

(cid:27)

(By Pinsker’s inequality)

where the last inequality follows by an argument similar to the results for the total variation distance in
Section A.5.1 using the fact that the dual of the ℓ1 norm is the ℓ∞ norm.

Similarly, it can be shown that D∗(

sp(z)√2ǫ. Therefore, we deﬁne the conﬁdence bounds,

P )

z

ǫ,
|

≤

−
h,t(x, a) = sp(V †
b

CB†

h+1,t)

2ǫh,t(x, a).

By Theorem 5, we know the regret can be bounded in terms of the sum of these conﬁdence bounds. Conse-
quently, using equation 27, we see that,

q

K

H

t=1
X

Xh=1

CB†

h,t(xh,t, ah,t)

≤

HS√2HAT log(HSAT /δ).

O(S√H 3AT ). This matches the regret bound in Filippi et al. [20].
Hence the regret can be bounded by
Using an alternative analysis essentially corresponding to a tighter bound on the conjugate distance, Talebi
h (x, a))T ) for KL-UCRL where
and Maillard [46] were able to prove a regret bound of
Vh−1(V ∗
1. We conjecture that
O(H√ΓSAT ) by combining the techniques of Talebi and Maillard
it is possible to obtain a regret bound of
[46] and Azar et al. [6].

S
h after playing action a from state s in stage h
q

h (x, a)) is the variance of V ∗

Vh−1(V ∗

h,x,a

O(

P

−

e

e

A.5.4 χ2-divergence

e

We can also use the Pearson χ2-divergence to deﬁne the primal conﬁdence sets in (5). Speciﬁcally, we
consider the distance

D(P,

P +) =

y
X

b

(P (y)

P +(y))2

,

−
P +(y)
b

b

27

for

P + deﬁned as in equation (28) and note that similar results hold for the distance D(P,

P ) =

P 2(y)−

P 2(y)

y

P (y)

.

We will use
b
[33], we see that with probability greater than 1

P + as the reference model for the primal conﬁdence sets. Using the empirical Bernstein inequality

P

b

δ, for all episodes t, a

, x

∈ A

, h

∈ S

[H],

b

b
∈

b

D(Ph(

x, a),

·|

P +

h,t(
·|

x, a)) =

(Ph(y

y
X

x, a)
|
P +

−
h,t(y

−
P +

h,t(y
x, a)
|
b

x, a))2
|

b

2

y
X

y (cid:18)

X

(Ph(y

x, a)
|
P +

−
h,t(y

Ph,t(y
x, a)
|
b
x, a)(1
b
|

x, a))2
b
|

+ 2

Ph(y
(

y
X

b

x, a)
|
P +

−
h,t(y

P +

x, a))2
|

h,t(y
x, a)
|
b
49 log2(HS2AT /δ)

2

2

Ph,t(y

Ph,t(y

−
Nh,t(x, a)

x, a)) log(HS2AT /δ)
b
|
P +
x, a)
h,t(y
|
b
b
P +
x, a) log(HS2AT /δ)
h,t(y
|
Nh,t(x, a)

49 log2(HS2AT /δ)
9Nh,t(x, a)

P +

+

+

h,t(y

b
x, a)
|

9N 2

h,t(x, a)

P +

h,t(y

x, a) (cid:19)
|

+

2S
b
Nh,t(x, a)

(cid:19)

+

2S
Nh,t(x, a)

b

y (cid:18)
X
11S log2(HS2AT /δ)
b
Nh,t(x, a)

≤

≤

≤

≤

where the second to last inequality follows since Nh,t(x, a)
P +

x, a) = max
|
x, a). We can then deﬁne the conﬁdence sets as
|

x, a)
|

Ph,t(y

h,t(y

h,t(y

≥

P +

b

1, Nh,t(x, a, y)

} ≥

{

1, and

b

b

x, a) =

Ph,t(
·|

(cid:26)

Ph(

x, a),

P +

h,t(
·|

x, a))

≤

ǫh,t(x, a)

for ǫh,t(x, a) =

(cid:27)

11S log2(HS2AT /δ)
Nh,t(x, a)

.

Furthermore, the conjugate D∗(V

b

P +) can be written as follows:

Ph ∈
e

D(
∆
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

·|

ǫ,
|

−

D∗(V

ǫ,
|

P +) = max
P ∈∆

P

b
P +, V

: D(P,

P +)

b

= min
λ∈R

= min
λ∈R

(cid:26)
(cid:10)
max
P ≥0

max
P

b
P

(cid:26)
(cid:10)

P

(cid:11)
P +, V

b
P +, V

−

−

b
λ1

−

−

(cid:11)

λ1

−

−

ǫ

≤

(cid:27)

λ(

P +(y)

y
X

b
P +(y)
(

λ

(cid:11)
b
P +(y)(V (y)

y
X
λ)2 +

−

b
H +

(cid:18)

b

SH
N

(cid:19)

1
N

(cid:26)
(cid:10)
ǫ
N s

= min

λ≤H+ SH

V+(V ) +

≤

ǫ
q

y
X
b
2SH
N

P

1) :

(cid:13)
(cid:13)
(cid:13)
(cid:13)
P (y)) :

−

−

p

2

P +

−
2 ≤
P + (cid:13)
b
(cid:13)
(cid:13)
P +
P
(cid:13)
−
b
P + (cid:13)
b
(cid:13)
(cid:13)
(cid:13)

p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

b

ǫ

(cid:27)

2 ≤

√ǫ

(cid:27)

b

h,t(x, a) can be upper-bounded by for CB†

where we have used properties of the dual of the weighted ℓ2 norm. Therefore, both CBh,t(x, a) and
CB−
h+1,t) and we can apply Theo-
rem 5 to show that the regret is bounded by the sum of these exploration bonuses. Following the same steps
P + in (30), this eventually leads to a regret
as in Section A.5.2 and using the bound on the variance under
bound of

O(HS√AT ).

h,t(x, a) =

h,t(V +

ǫh,t(x, a)

V+

q

b

e

It is interesting to note that Maillard et al. [31] considered similar conﬁdence sets using a reverse χ2-
divergence deﬁned as D(p, q) =
. Using this distance with a feasible conﬁdence set would ﬁt
into our framework. However, for their regret analysis, Maillard et al. [31] impose the additional constraint
that for all x′ such that
x, a) > p0 for some positive p0.
|

P
x, a) > 0, it must also hold that
|

q2(y)−p2(y)
p(y)

Ph,t(x′

Ph,t(x′

b

y

e

e

28

non-convex6 and thus their eventual approach does not
Unfortunately, this constraint makes the set
entirely ﬁt into our framework. Finally, we note that the bounds of Maillard et al. [31] replace a factor of S
appearing in our bounds by 1/p0, which may in an inferior bound when p0 is small. Overall, we believe that
the Pearson χ2-divergence we propose in this section can remove this limitation of the analysis of Maillard
et al. [31] while also retaining the strong problem-dependent character of their bounds.

P

B Results for Linear Function Approximation

In this section, we provide proofs of the results in the linear function approximation setting. Throughout
the analysis, we will use the notation

Ct(δ) = 2H

d log (1 + tR2/λ) + log(1/δ) + CP H√λd

where CP is such that
all x

. We also deﬁne the event

mh,a(x)

k1 ≤

k

∈ S

CP for every row mh,a(x) of Mh,a and R is such that

p

ϕ(x)

k2 ≤

k

R for

We start by proving our key concentration result that will be used for deriving our conﬁdence sets.
c

Eh,a,t(g, δ) =

Mh,a −

Mh,a,t

g

Σh,a,t−1 ≤

Ct(δ)

.

(cid:27)

(cid:26)(cid:13)
(cid:16)
(cid:13)
(cid:13)

(cid:17)

(cid:13)
(cid:13)
(cid:13)

Proposition 15. Consider the reference model
for any a
at least 1

Ph,a,t = Φ
[H], episode t and any ﬁxed function g :

, h

∈

b

Mh,a,t with
[
−

S →
c

c

Mh,a,t deﬁned in Equation (10). Then,
H, H], the following holds with probability

∈ A
δ:
−

Proof. We start by rewriting
c

Mh,a −
(cid:16)

(cid:13)
(cid:13)
(cid:13)

Mh,a,t

g

2H

d log (1 + tR2/λ) + log(1/δ) + CP H√λd.

p

Σh,a,t−1 ≤

(cid:13)
(cid:13)
(cid:13)

(cid:17)

Mh,a −

Mh,a,t

g

(cid:13)
(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
and proceed by using the deﬁnitions of

c

(cid:17)

=

Σh,a,t−1

Σh,a,t−1

Mh,a −
(cid:13)
(cid:16)
(cid:13)
(cid:13)
Mh,a,t, Σh,a,t−1 and Wh,a,t−1 to see that

Mh,a,t−1

(cid:13)
(cid:13)
(cid:13)

c

(cid:17)

g

,

Σ−1

h,a,t−1

Σh,a,t−1

Mh,a −
(cid:0)

Mh,a,t

g = ΦTWh,a,t−1ΦMh,ag + λMh,ag

c

(cid:1)

c

Σh,a,t−1Σ−1

h,a,t−1

−

I{ah,k=a}ϕ(xh,k)g (xh+1,k)

t−1

Xk=1

= ΦTWh,a,t−1Ph,ag

I{ah,k=a}ϕ(xh,k)g (xh+1,k) + λMh,ag

t−1

=

I{ah,k=a}

Xk=1

·|

Ph(

xh,k, ah,k), g

·|

g(xh+1,k)

ϕ(xh,k) + λMh,ag.

−

(cid:0)(cid:10)

(cid:11)

(cid:1)

The ﬁrst term on the right-hand side is a vector-valued martingale for an appropriately chosen ﬁltration,
since

so the sum of these terms can be bounded by appealing to Theorem 1 of Abbasi-Yadkori et al. [1] as
(cid:11)

(cid:2) (cid:10)

(cid:3)

E

Ph(

xh,k, ah,k), g

g(xh+1,k)

xh,k, ah,k

= 0,

−

I{ah,k=a}

t−1

(cid:13)
Xk=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ph(

xh,k, ah,k), g

·|

(cid:0)(cid:10)

g(xh+1,k)

(cid:1)

−

(cid:11)

(cid:12)
(cid:12)

ϕT(xh,k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2H

≤

Σ−1

h,a,t−1

d log (1 + tR2/λ) + log(1/δ).

6To see this, consider ˜p and ˜p′ satisfying the constraints, which diﬀer only in x where ˜p(x) = p0 and ˜p′(x) = 0. Then,

p

nontrivial convex combinations of ˜p, ˜p′ no longer satisfy the constraints.

t−1

−

Xk=1

29

The proof is concluded by applying the bound

h,a,t−1 ≤
where in the last step we used the assumption that

λMh,ag
k

kΣ−1

√λ

Mh,ag

k

k ≤

CP H√λd,

mh,a(x)
k

k1 ≤

CP and

g

k

k∞ ≤

H.

The following simple result will also be useful in bounding the sum of exploration bonuses and thus the

regret of the two algorithms:

Lemma 16. For any h

∈

[H],

K

Proof. The claim is directly proved by the following simple calculations:

Xa∈A

t=1
X

(cid:13)
(cid:13)

(cid:13)
(cid:13)

p

I{ah,t=a}ϕ(xh,t)

Σ−1

h,a,t−1 ≤

2

dAK log (1 + KR2/λ).

K

I{ah,t=a}ϕ(xh,t)

Xa∈A

t=1
X

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Σ−1

h,a,t−1 ≤ v
u
u
t
K

2

≤

s

a
X

t=1
X

log

K

K

I{ah,t=a}v
u
u
t

det (Σh,a,K)
det (λI)

a
X

t=1
X

≤

I{ah,t=a}ϕ(xh,t)

2
Σ−1

h,a,t

(cid:13)
(cid:13)
2

(cid:13)
(cid:13)

KdA log (1 + KR2/λ),

(cid:18)
where the ﬁrst inequality is Cauchy–Schwarz and the second one follows from Lemma 11 of Abbasi-Yadkori
et al. [1].

a
X

p

(cid:19)

Finally, the following result will be useful to bound the scale of the esimated model

Mh,a,t with probability

1:

Lemma 17. Consider the reference model
[
any B > 0 and any ﬁxed function g :
−
b

S →

Mh,a,tg

tBR
λ

≤

and

Ph,a,t = Φ

Mh,a,t with

c
Mh,a,t deﬁned in Equation (10). Then, for

B, B], the following statements hold with probability 1:

Mh,a −

c
Mh,a,t

g

c

Σh,a,t−1 ≤

λ−1/2tBR + λ1/2BCP .

(cid:16)
Proof. The ﬁrst statement is proven by straightforward calculations, using the deﬁnition of

c

(cid:17)

(cid:13)
(cid:13)
(cid:13) c

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Mh,a,tg

=

(cid:13)
(cid:13)
(cid:13) c

(cid:13)
(cid:13)
(cid:13)

≤

h,a,t−1

h,a,t−1

Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1

Xk=1

I{ah,k=a}ϕ(xh,k)g (xh+1,k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1

I{ah,k=a}ϕ(xh,k)g (xh+1,k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk=1

(cid:13)
(cid:13)
(cid:13)

op (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Mh,a,t:

c

B
λ

≤

t−1

Xk=1

ϕ(xh,k)

k ≤

k

tBR
λ

,

where the second inequality uses that the operator norm of Σ−1
As for the second inequality, we proceed as in the proof of Proposition 15 and recall that

h,a,t−1 is at most λ−1, and the triangle inequality.

Σh,a,t−1

Mh,a −
(cid:16)

t−1

Mh,a,t

g =

I{ah,k=a}

Xk=1
The norm of the above is clearly bounded by tBR + λBCP . Thus, we have

c

(cid:0)(cid:10)

(cid:17)

(cid:11)

(cid:1)

Ph(

xh,k, ah,k), g

g(xh+1,k)

ϕ(xh,k) + λMh,ag.

·|

−

Mh,a −

Mh,a,t

g

(cid:17)

c

(cid:16)

(cid:13)
(cid:13)
(cid:13)

Σh,a,t−1

(cid:13)
(cid:13)
(cid:13)

=

Σh,a,t−1

Mh,a,tg

Mh,a −
(cid:16)

c
Σh,a,t−1

Mh,a,t

g

Σ−1

h,a,t−1

(cid:17)(cid:13)
(cid:13)
Mh,a −
(cid:13)

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)

(tBR + λBCP ) = λ−1/2tBR + λ1/2BCP .

c

(cid:13)
(cid:13)
Σ−1/2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
√λ

≤

≤

h,a,t−1

op

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

This concludes the proof.

30

B.1 Optimism in state space through local conﬁdence sets

This section presents our approach for factored linear MDPs with local conﬁdence sets, which can be seen
to lead to conﬁdence bonuses in the state space. We ﬁrst state some structural results that will justify our
algorithmic approach, explain our algorithm in more detail, and then present the performance guarantees.

We recall that our approach is based on solving the following optimization problem:

H

maximize
P
q∈Q(x1),ω,

subject to
e

Wh,a,t−1Φωh,a, rai
h

a
Xh=1 X

qh+1,a =

Ph,aWh,a,t−1Φωh,a

a
X
ΦTqh,a = ΦTWh,a,t−1Φωh,a

a
X

e

D

Ph(

x, a),

Ph,t(

x, a)

ǫh,t(x, a)

≤

·|

(cid:17)

·|

(cid:16)

a

∀

a

∀

, h = 1, . . . , H

∈ A

, h = 1, . . . , H

∈ A

(x, a),

∀

where D is an arbitrary divergence that is positive homogeneous and convex in its arguments. The following
structural result shows that this optimization problem can be equivalently written in a dual form that is
essentially identical to the optimistic Bellman equations derived in Section 4 for the tabular setting.

b

e

Proposition 18. The optimization problem above is equivalent to solving the optimistic Bellman equa-
tions (11) with the exploration bonus deﬁned as

CBh(x, a) = D∗

ǫh(x, a),
(cid:12)
The proof follows from a similar reparametrization as used in the proof of Proposition 1 that makes the
(cid:12)
(cid:12)
optimization problem convex, thus enabling us to establish strong duality. To maintain readability, we defer
the proof to Appendix B.3.1. Consequently, the properties stated in Propositions 2 and 3 can also be shown
in a straightforward fashion.

x, a)

Ph(

(cid:17)

b

·|

.

V +
h+1
(cid:16)

Our results are based on using the divergence measure

D

Ph,t(

x, a),

Ph,t(

x, a)

·|

·|

= sup

g∈Vh+1,t

Ph,t(

x, a)

·|

Ph,t(

x, a), g

·|

−

,

(cid:16)

e

(cid:17)

D

e

whose conjugate can be directly upper-bounded by ǫh,t. Since the structural results established above directly
imply that Theorem 4 continues to hold, we can easily derive a practical and eﬀective algorithm by simply
using ǫh,t as the exploration bonuses. Speciﬁcally, we will consider an algorithm that calculates an optimistic
value function and a corresponding policy by solving the OPB equations (11) via dynamic programming,
with the conﬁdence bonuses chosen as

b

b

E

for some αh,t. The shape of this conﬁdence set is directly motivated by the following simple corollary of our
general concentration result in Lemma 15:

CB†

h,t(x, a) = αh,t k

ϕ(x)

kΣ−1

h,a,t−1

Lemma 19. Fix h, a and consider the reference model
Then, for any ﬁxed function g :
Eh,a,t(g, δ):

S →

[
−

Proof. The proof is immediate using the deﬁnition of the event
ity:

b

D

Ph(

x, a)

Ph,t(

x, a), g

Ct(δ)

·|

−

·|

b

≤

E

c
ϕ(x)

kΣ−1

c
.

h,a,t−1

k
Eh,a,t(g, δ) and the Cauchy–Schwarz inequal-

Mh,a,t deﬁned in Equation (10).
H, H], the following holds simultaneously for all x under event

Mh,a,t with

Ph,a,t = Φ

Ph(

x, a)

Ph,t(

x, a), g

=

ϕ(x),

·|

−

·|

Mh,a −

Mh,a,t

g

D

b

E
(cid:10)
ϕ(x)
≤ k

(cid:0)
h,a,t−1

kΣ−1

Mh,a −
c

(cid:1)
Mh,a,t

(cid:11)

g

(cid:17)

c

(cid:16)

(cid:13)
(cid:13)
(cid:13)

Σh,a,t−1 ≤

(cid:13)
(cid:13)
(cid:13)

Ct(δ)

k

31

ϕ(x)

kΣ−1

h,a,t−1

.

The main challenge in the analysis will be to show that there exists an appropriate choice of αh,t that
guarantees that the above result holds uniformly over the value-function class
Vh+1,t used in the deﬁnition of
the conﬁdence sets. We note that the resulting algorithm is essentially identical to the LSVI-UCB algorithm
proposed and analyzed by Jin et al. [25], and we will accordingly refer to it by this name (that stands for
“least-squares value iteration with upper conﬁdence bounds”).

B.1.1 Regret Bound

In this section we prove the regret bound of Theorem 8, whose precise statement is as follows:

Theorem 20. With probability greater than 1

−

δ, the regret of LSVI-UCB with the choice λ = 1 and

αh,t = α = 2H

d log (1 + KR2) + log(HA/δ) + dA

log(1 + 4HK 2R2) + d log(1 + 4R3K 3)

can be bounded as

q

+ CP

H√d + 1

+ 1

(cid:0)

(cid:16)

(cid:17)

RT =

O(A√H 3d3T ).

(cid:1)

We note that the statement of the theorem is trivial when α > K so we will suppose that the contrary
e
holds throughout the analysis. The proof is a straightforward application of Theorem 4: given that P
,
∈ P
the regret is bounded by the sum of exploration bonuses, which itself can be easily bounded using Lemma 16.
Thus, the main challenge is to show that the transition model lies in the conﬁdence set. To prove this, we
observe that, thanks to the choice of exploration bonus, the class of value functions
Vh+1,t produced by the
algorithm is composed of functions of the form

V +
t,h(x) = min

H

h, max

−

ϕ(x), θt,a,hi
h
n

n

+ α

k

ϕ(x)

kΣ−1

t,a,h

,

oo

and the covering number of this class is relatively small. We formalize this in the following proposition,
which takes care of the probabilistic part of the analysis:

Proposition 21. Consider the reference model
Mh,a,t deﬁned in Equation (10). Then,
Ph,a,t = Φ
for the choice of α in Theorem 20, the following holds simultaneously for all x, a, h, t, with probability at least
1

Mh,a,t with

δ:

−

sup
V ∈Vh+1,t

D

Ph(

x, a)

·|

−

b
Ph,t(

c
x, a), V

·|

b

α

k

≤

E

c
ϕ(x)

kΣ−1

h,a,t−1

.

The proof of this statement is rather technical and borrows some elements of the analysis of Jin et al.
[25]—we delegate the proof to Appendix B.3.2. Thus, we now have all the necessary ingredients to conclude
the proof of Theorem 20. Indeed, since Proposition 21 guarantees that the true model P is always in the
δ, and using the optimistic property of our algorithm that follows from
conﬁdence set with probability 1
Proposition 18, we can appeal to Theorem 5 to bound the regret in terms of the sum of exploration bonuses.
This in turn can be bounded by using Lemma 16 as follows:

−

H

K

Xh=1

t=1
X

CB†

h,t(xh,t, ah,t)

≤

≤

H

K

I{ah,t=a}ϕ(xh,t)

a
Xh=1 X
2αH

t=1
X
dAK log (1 + KR2/λ) = 2α

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Σ−1

h,a,t−1

αh,a,t

HdAT log (1 + KR2/λ).

The proof is concluded by observing that α =

p

p

O(Hd√A).

e

32

B.2 Optimism in feature space through global constraints

We now present our approach based on global conﬁdence sets for the transition model
M that lead to an
algorithm using exploration bonuses that can be expressed in the feature space. The main idea behind the
algorithm is deﬁning in each episode t, the conﬁdence set

M satisfying

f

D(

Mh,a,

Mh,a,t) = sup

f ∈Vh+1

f
Σh,a,t−1 ≤

ǫh,a,t

Mt of models
f
Mh,a,t

c

(cid:1)

(cid:13)
(cid:13)

Mh,a −
(cid:13)
(cid:0)
f
(cid:13)

for an appropriate choice of ǫh,a,t, and deﬁning the function

f

c

Gt(

M ) = max

q∈Q(x1),ω

f

subject to

H

a
Xh=1 X

Wh,a,t−1Φωh,a, rai
h

(31)

qh−1,a =

M T

h,aΦTWh,a,t−1Φωh,a

a
X
ΦTqh,a = ΦTWh,a,t−1Φωh,a

a
X

f

a

∀

a

∀

, h = 1, . . . , H

∈ A

, h = 1, . . . , H.

∈ A

1 (x1). As
Clearly, if the true model M is in the conﬁdence set
phrased above, this optimization problem is intractable due to the large number of variables and constraints.
Our algorithm addresses this challenge by converting the above problem into a more tractable one that
retains the optimistic property. In particular, our algorithm solves the parametric OPB equations (11) with
conﬁdence bonuses deﬁned as

Mt, we have max

M∈Mt

Gt(

M )

f

≥

e

Gt(M ) = V ∗

CB†

h,t(x, a) =

ϕ(x), B†

h,a,t

for a vector B†
:
ǫh,a,t

h,a,t ∈

Rd chosen to maximize the following function over the convex set

(cid:11)

(cid:10)

Bt =

B :

Bh,akΣh,a,t−1 ≤
k

(cid:8)

(cid:9)

H

Wh,a,t−1Φωh,a, ra + ΦBh,ai
h

(32)

G′

t(B) = max

q∈Q(x1),ω

subject to

a
Xh=1 X

qh−1,a =

M T

h,a,tΦTWh,a,t−1Φωh,a

a
X
ΦTqh,a = ΦTWh,a,t−1Φωh,a

a
X

c

a

∀

a

∀

, h = 1, . . . , H

∈ A

, h = 1, . . . , H.

∈ A

This deﬁnition is easily seen to be equivalent to the one given in the statement of Theorem 9 through basic
LP duality (cf. Section 2). Our analysis will take advantage of the fact that our exploration bonuses are
linear in the feature representation, which eventually yields value functions of the following form:

V †
h,t(x) = min

H

h + 1, max

a

−

n

for some θ†
The algorithm is justiﬁed by the following property:

Rd, which implies that the class of functions

h,a,t ∈

ϕ(x), θ†

h,a,t

,

(33)

(cid:11)o

(cid:10)
Vh+1,t is simpler than in the case LSVI-UCB.

k

ǫh,a,t

Proposition 22. For any episode t, let the functions Gt and G′
. Then, maxB∈Bt G′
Bh,akΣh,a,t−1 ≤
Proof. Let us ﬁx a model
Zt ∈ Bt due to the deﬁnition of
the OPB equations (11) deﬁning them. Indeed, for a ﬁxed
standard LP duality as exposed in Section 2. To express Gt(
f

≥
∈ Mt, introduce the notation Zh,a,t =
f

t(B)

M ∈Mt

max

Gt(

f

M

(cid:9)

e

M ).

Vh+1,t, and notice that
Mh,a,t
M ) and G′
t(B) through
Mt. The proof relies on expressing the values of Gt(
(cid:1)
c
M ) can be expressed through
M ), let Ut stand for the value function deﬁned

Mh,a −
(cid:0)
f
M , the value of Gt(

f

t be deﬁned as above and let

Bt =

B :

(cid:8)

f

f

33

through the system of equations

θh,a,t = ρa +

Mh,aUh+1,t = ρa +

Mh,a −
Mh,a,tUh+1,t,
(cid:0)
f

Mh,a,t

Uh+1,t +

Mh,a,tUh+1,t

(cid:1)

c

c

= ρa + Zh,a,t +

Uh+1,t(x) = max

a

f
ϕ(x), θh+1,a,ti
h

c

that have to be satisﬁed for all x, a, h. Then, it is easy to see that Gt(
be understood as the solution of the OPB equations (11) with exploration bonus CBh,t(x, a) =
On the other hand, G′

M ) = U1,t(x1). Notice that this can
.
ϕ(x), Zh,a,ti
h
t is deﬁned through the system of equations

t(B) can be expressed as U ′

1,t(x1) with U ′

f

θ′
h,a,t = ρa +
U ′
h,t(x) = max

Mh,a,tU ′
ϕ(x), θ′
f
(cid:10)
t(Z) and, using Z

a

h+1,t
h,a,t + Bh,a,t

.

M ) = G′

f
Notably, the above proposition ensures that the value function V †

It is then easy to verify that Gt(
concludes the proof since the inequality must hold for any model

(cid:11)
∈ Bt, that G′
∈ Mt.
t arising from the OPB equations (11)
with bonus CB†
V ∗
1 (x1,t). This
M )
enables us to apply the general regret bound of Theorem 5 to establish a performance guarantee for the
resulting algorithm. We provide this analysis in the next section.

is optimistic in the sense that V †

maxB∈Bt G′

t(B). This

h,t(x, a) =

ϕ(x), B†

1,t(x1,t)

t(Z)

Gt(

h,a,t

f

M

≥

≥

≤

(cid:11)

(cid:10)

f

From the above formulation, it is readily apparent that, since G′ is a maximum of linear functions, it is a
convex function of B, and thus maximizing it over a convex set is potentially still very challenging. We note
that this optimization problem is essentially identical to the one faced by the seminal LinUCB algorithm for
linear bandits [15, 1], which is known to be computationally intractable for general decision sets. This is to be
contrasted with the algorithms described in previous parts of this paper, which are eﬃciently implementable
through dynamic programming. Indeed, despite being of a similar form, the simplicity of these previous
methods stem from the local nature of their conﬁdence sets which was seen to lead to exploration bonuses
that can be set independently for each state and computed via dynamic programming. This is no longer
possible for the exploration bonuses used in this section, which are set through a global parameter vector B.
Intuitively, this prevents the application of dynamic-programming methodology which heavily relies on the
ability of breaking down an optimization problem into a set of local optimization problems (often referred
to as the “principle of optimality” in this context [9]).
It remains an open problem to ﬁnd an eﬃcient
implementation of this method.

It is interesting to note that our algorithm essentially coincides with the Eleanor method proposed
very recently by Zanette et al. [52], up to minor diﬀerences. Their analysis is more general than ours as
they considered the signiﬁcantly harder case of learning with misspeciﬁed linear models that our analysis
doesn’t account for. Nevertheless, our analysis is substantially simpliﬁed by our model-based perspective
that sheds new light on the algorithm. In particular, while Zanette et al. [52] do not provide a substantial
discussion of the computational challenges associated with Eleanor, our formulation clearly highlights the
convexity of the objective function optimized by the algorithm and the relation with LinUCB. We believe
that our model-based perspective can provide further insights into this challenging problem in the future,
and particularly that it will remain useful when analyzing misspeciﬁed linear models.

B.2.1 Regret bound

We now prove our main result regarding the algorithm: the regret bound claimed in Theorem 9. In particular,
the detailed statement of this result is as follows:

Theorem 23. With probability greater than 1

δ, the regret of our algorithm with λ = 1 for

−

ǫh,a,t = ǫ =2H

d log (1 + KR2) + dA log(1 + 4K 2HR3) + log (HA/δ)

+ λ1/2
p

CP √d + 1 + CP

(cid:16)

(cid:17)

34

satisﬁes

RT =

O(dA√H 3T ).

The key idea of the analysis is to use Proposition 22 to establish the optimistic property of the algorithm
and use Theorem 5 to bound the regret by the sum of exploration bonuses. The only remaining challenge
is to prove that, with high probability, the true model lies in the conﬁdence sets speciﬁed in Equation (14).
The following proposition guarantees that this is indeed true:

e

Proposition 24. Consider the reference model
Mh,a,t deﬁned in Equation (10). Then,
Ph,a,t = Φ
for the choice of ǫ in Theorem 23, the following holds simultaneously for all a, h, t, with probability at least
1

Mh,a,t with

δ:

−

sup
f ∈Vh+1,t

b
Mh,a −

c
f

Mh,a,t

Σh,a,t−1 ≤

c
ǫh,a,t.

(cid:13)
(cid:0)
(cid:13)

(cid:1)
The proof relies on a covering argument similar to the one we used for proving Proposition 21, exploiting
the fact that the value function class
Vh+1,t is composed of slightly simpler functions. The proof is deferred
to Appendix B.3.4. Thus, we can conclude the proof of Theorem 23 as follows. Taking advantage of the
fact that the algorithm follows the optimal policy corresponding to the solution of the OPB equations (11),
we can use the general guarantee of Theorem 5 and bound the regret of the algorithm as the sum of the
exploration bonuses. Noticing that the bonuses can be upper-bounded as

c

(cid:13)
(cid:13)

CB†

h,t(x, a) =

ϕ(x), B†

h,a,t

≤ k

ϕ(xh,t)

kΣ−1

h,a,t−1

B†

h,a,t

Σh,a,t−1 ≤ k

ϕ(xh,t)

kΣ−1

h,a,t−1

ǫh,a,t,

(cid:10)
where the last step follows from the fact that B†
appealing to Lemma 16:

(cid:11)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

h,a,t ∈ Bt, the sum of conﬁdence bonuses can be bounded by

H

K

Xh=1

t=1
X

CB†

h,t(xh,t, ah,t)

≤

≤

H

K

I{ah,t=a}ϕ(xh,t)

Σ−1

h,a,t−1

ǫh,a,t

a
Xh=1 X

t=1
X

(cid:13)
(cid:13)

2ǫH

dAK log

s

(cid:13)
(cid:13)
1 + KR2/λ
δ

(cid:19)

(cid:18)

= 2ǫ

HdAT log

s

1 + KR2/λ
δ

.
(cid:19)

(cid:18)

Setting λ = 1 and noticing that ǫ =

O(H√dA) concludes the proof of Theorem 23.

B.3 Technical proofs

e

B.3.1 Proof of Proposition 18

We ﬁrst note that, since
problem can be rewritten as

Ph,a = Φ

Mh,a and using the second constraint, the ﬁrst constraint in the optimization

b

c

qh+1,a =

Mh,aΦWh,aΦωh,a +

a
X

a
X

c

Ph,a

qh,a.

a (cid:16)
X

Ph,a −
e

b

(cid:17)

Using this, we use a similar argument to Lemma 10 to show that strong duality and the KKT conditions
Ph(x′
hold. We reparameterize by deﬁning Jh(x, a, x′) = qh(x, a)
x, a) and observe that the last constraint in
|
x′ Jh(x, a, x′))
x′ Jh(x, a, x′) which is convex
(12) is can be written as D(Jh(x, a,
in J. It can also be easily observed that the ﬁrst two constraints, and the objective are linear in q, J, ω.
Thus strong duality holds, and the optimal value of the reparameterized optimization problem is equal to
the optimal value of the corresponding Lagrangian dual problem. As in the proof of Lemma 10, by using
the reverse reparameterization, we can see that the value of the Lagrangian of the modiﬁed problem is equal
to that of the original problem in (12). Hence, strong duality holds for (12). It then follows that the KKT
conditions also hold for this problem.

ǫh(x, a)

x, a)

Ph(

),
·

P

P

≤

e

b

·|

35

Given strong duality, we can ﬁnd the dual of the problem in (12) by considering the Lagrangian. The
partial Lagrangian of the optimization problem without the last constraint of the primal can be written as

(q, κ, ω; V, θ) =

L

Wh,aΦωh,a, ra +

Xh,a D
+

Xx,a,h

qh(x, a)

(cid:18)

Ph,aVh+1 −
b
(Φθh,a) (x) +

Φθh,a

E

κh(x, a, y)Vh+1(y)

−

Vh(x)

+ V1(x1),

(34)

(cid:19)

y
X

for κh(x, a, y) =

Ph(y

x, a)
|

−

Ph(y

x, a). Then, by strong duality, the optimal value of the primal is equal to
|

e

b

min
V,θ

max

q≥0,

P ∈P

ω,κ

(q, κ, ω; V, θ).

L

Observing that qh(x, a)
x, a)
over

Ph(

≥
∈ Ph(x, a). We get,

·|

0 and using the deﬁnition of κh(x, a,

e

), we can consider the inner maximization
·

e

max
Ph(·|x,a)∈Ph(x,a)

y
X

Ph(y
(

x, a)
|

−

Ph(y

x, a))Vh+1(y) = D∗(Vh+1|
|

P , ǫ)

b
by deﬁnition of the conjugate. Substituting this back into (34), we can ﬁnd the dual from this Lagrangian
by a similar technique to Proposition 1. In particular, observe that the objective function will be given by
, it must
V1(x1). To deﬁne the constraints, note that if maxω

Wh,aΦωh,a, ra +

Φθh,a

<

b

e

e

Wh,aΦ, ra +

x, a))

−

Φθh,a

Ph,aVh+1 −
Vh(x)) <
∞
b

= 0, and likewise if maxq>0
P
, it must be the case that (Φθh,a) (x) + D∗(Vh+1|

P

E

ǫh,a,

Ph(

x, a))

·|

−

Ph,aVh+1 −
b

∞

x,a,h qh(x, a)((Φθh,a) (x) +

E

h,a

D

be the case that
ǫh,a,
D∗(Vh+1|
0.
Vh(x)
≤

Ph(

D
·|

Thus the dual optimization problem can be written,

b

b

(x, a)

∀

, h

∈

∈ Z

[H]

a

∀

, h

∈ A

∈

[H].

minimize
V

V1(x1)

Subject to Vh(x)

≥

(Φθh,a) (x) + D∗

Vh+1

(ΦWh,aΦ) θh,a = ΦTWh,a

Ph(

x, a)

·|

ǫh(x, a),
(cid:12)
(cid:12)
Ph,aVh+1
(cid:12)

b
(cid:17)

(cid:16)

ra +

(cid:16)

(cid:17)

(cid:17)

·|

It is easily seen that the solution to this can be found by solving the optimistic parametric Bellman

b

equations in (11) with CBh,t(x, a) = D∗

Vh+1,t

ǫh,t(x, a),

Ph,t(

x, a)

via backwards recursion.

(cid:16)
B.3.2 The proof of Proposition 21

(cid:12)
(cid:12)
(cid:12)

b

The proof follows from a construction proposed by Jin et al. [25]: it relies on taking a union bound over an
appropriately chosen covering of the class of value functions in stage h + 1 that can be ever produced by
solving the optimistic Bellman equations (11). For this purpose, we need the following technical result that
bounds the covering number of this set:

Lemma 25. Let
supx∈S |

V (x)

−

, ε) be the ε-covering number of the set
(
V
N
V ′(x)
. Then, for any stage h = 1, . . . , H and episode t,
|
log(

V

Ad log(1 + 4tHR/(λε)) + d2A log(1 + 4Rα/(λε2))

with respect to the distance

V

k

V ′

k∞ =

−

Vh+1,t, ε))
(
R

N
ϕ(x)

k2 ≤

∀

≤
x

k

where R is such that
[H], t

[K].

, h

A

∈

∈

, λ is such that the minimum eigenvalue, λmin(Σh,a,t)

λ

a

∀

∈

≥

∈ S

36

−

V

.

E

e

Mh,a,t

g

(cid:17)

c

(cid:13)
(cid:13)
(cid:13)

e

Σh,a,t

The proof of Lemma 25 is similar to that of Lemma D.6 of [25], and exploits that the class

Vh+1,t
is parametrized smoothly by θ and Σ. We relegate the proof to Appendix B.3.3. As for the proof of
Proposition 21, let us ﬁx any h, a, ε > 0 and any V
Vh+1,t deﬁned
V
in Lemma 25 such that

∈ Vh+1,t, and let

V be in the ε-covering of

ε. Then, we have

V

e

k

−

k∞ ≤

D

Ph(

x, a)

·|

−

=

D

Ph,t(
e

x, a), V

·|
x, a)

Ph(
b

·|

E
Ph,t(

·|

−

The second term can be bounded by introducing the notation

g = V

b

x, a),

V

+

Ph(

x, a)

·|

E

D

e

Ph,t(

x, a), V

·|

b
V and writing

−

−

Ph(

x, a)

·|

−

D

Ph,t(

x, a),

·|

b

g

=

E

e

D
ε

≤

ϕ(x),

Mh,a,t

Mh −
(cid:17)
λ−1/2tR + λ1/2CP

c

(cid:16)

g

e
≤ k

E
ϕ(x)

ϕ(x)

e
kΣ−1

h,a,t

,

kΣ−1

h,a,t

Mh −
(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

e
k

(cid:17)

where we used Lemma 17 with B = ε in the last step. As for the ﬁrst term, we use a union bound over all
Nε and setting δ′ = δ/HA,
V in the ε-covering of
δ′, we have
we can see that for any
e

Vh+1,t and Lemma 19. Denoting the covering number as
V in the ε-covering, with probability greater than 1
−
Nε),

Ct(δ′/

kΣ−1

Ph,t(

x, a),

ϕ(x)

x, a)

Ph(

≤ k

−

h,a,t

V

e

·|

·|

which can be further bounded as

b

e

D

E

Ct(δ′/

Nε)

2H

2H

≤

≤

CP H√λd = 2H

d log (1 + tR2/λ) + log (

−
d log (1 + tR2/λ) + log(1/δ′) + dA log(1 + 4HtR/(λε)) + d2A log(1 + 4Rα/(λε2))

p

Nε/δ′)

p

d log (1 + tR2/λ) + log(1/δ′) + dA log(1 + 4Ht2R2/λ2) + d2A log(1 + 4R3Kt2/λ3),

where we set ε = λ/(tR) and used the condition α
have

p

≤

K in the last step. With the same choice of ε, we also

Noticing that the sum of the two latter terms is bounded by α and taking a union bound over all h, a
concludes the proof.

ε

λ−1/2tR + λ1/2CP
(cid:16)

(cid:17)

λ1/2 (1 + CP ) .

≤

B.3.3 The proof of Lemma 25

We ﬁrst note that, due to the deﬁnition of the parameter vectors θ+
tions (11) with
H, we have

V +
h+1,t

∞ ≤

h,a,t as the solution of the OPB equa-

(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ+
h,a,t

tHR
λ

≤

def
= β,

where the inequality follows from Lemma 17. To preserve clarity of writing, we omit explicit references to t
below. By design of the algorithm, we can see that the value functions can be written with the help of the
function Uh,θ,Σ deﬁned as

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Uh,θ,Σ(x) = min

H

(cid:26)

h + 1, max
a∈A

−

n

ϕ(x), θh,ai
h

+ α
k

for some α > 0. Indeed, the class of value functions can be written as

ϕ(x)

kΣ−1

h,a

o(cid:27)

Vh =

(cid:26)

Uh,θ,Σ : max

a k

θh,ak ≤

β, max

a

37

Σ−1
h,a
(cid:13)
(cid:13)
(cid:13)

op ≤

(cid:13)
(cid:13)
(cid:13)

1/λ

.

(cid:27)

We show below that Uh,θ,Σ is a smooth function of the parameters θh,a and Σ−1

Vh. Indeed, letting Vh = Uh,θ,Σ and

h,a, which will allow us to
for

Vh = U

h,

θ,

Σ

prove a tight bound on the covering number of the class

an arbitrary set of parameters θ, Σ,

θ,

Σ, we have

e

e

e

Vh −

k

Vhk∞ = sup
x∈S (cid:12)
(cid:12)
(cid:12)
e
(cid:12)

min

e
H

{

e
−

h + 1, max

a∈A {

ϕ(x)Tθh,a + α
k

ϕ(x)

kΣ−1

h,a }}

min

H

{

−

−

h + 1, max

a∈A {

ϕ(x)T

θh,a + α
k

ϕ(x)
k

Σ−1

h,a }}

ϕ(x)

kΣ−1

h,a } −

ϕ(x)T

max
a∈A {

e
θh,a + α
k

ϕ(x)
k

e
Σ−1

h,a }

ϕ(x)

kΣ−1

h,a −

ϕ(x)T

e
ϕ(x)
θh,a + α
k
k

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ϕ(x)Tθh,a + α
k

ϕ(x)Tθh,a + α
k

max
a∈A {

sup
x∈S (cid:12)
(cid:12)
(cid:12)
sup
(cid:12)
x∈S,a∈A (cid:12)
(cid:12)
(cid:12)
ϕ(x)T(θh,a −
sup
(cid:12)
x∈S,a∈A (cid:12)
(cid:12)
(cid:12)
θh,a −
R
sup
(cid:12)
a∈A
θh,a −

sup
a∈A

R

k

k

≤

≤

≤

≤

≤

Σ−1
h,a (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θh,a) +

e
ϕ(x)T(αΣ−1
h,a −

α

Σ−1

h,a)ϕ(x)
e

q
αΣ−1

Σ−1

k

e

α

R

h,a −

e
a∈A

h,akop

θh,ak2 + sup
θh,ak2 + sup
e
e
kop to denote the operator norm and
A

Σ−1
e
h,akF

Rα
k

h,a −

Σ−1

a∈A

e

k

k2 ≤

R and we have used

ϕ(x)
since
k
a matrix A.

kF the Frobenius norm of
A
Rd, supa∈A k
We then note that the ε/2-covering number of the set Θ =
β
is
}
bounded by (1+4β/ε)Ad, and that ε/2-covering number of the set Γ =
ΣakF ≤
is bounded by (1 + 4/(λε2))d2A. These results follow due to the standard fact that the ε-covering num-
1/λ
}
ber of a ball in Rd with radius R > 0 with ℓ2 distance is bounded by (1 + 2R/ε)d, and that Θ and Γ are
(dA)-dimensional and (d2A)-dimensional, respectively.

k
(θa)a∈A : θa ∈

θak2 ≤
Rd×d, supa∈A k

(Σa)a∈A : Σa ∈

{

{

From the above discussion, we can conclude that for any Vh ∈ Vh, there is a

the ε/2-covering of Θh, and

Σh in the ε/2-covering of Γh such that,

Vh parameterized by

θh in

e

e

Vhk∞ ≤
By rescaling of the covering numbers, we can see that the logarithm of the ε-covering number of
e
bounded by

Rε/2 + Rαε/2.

Vh −

e

k

Vh can be

log(

N

Vh, ε))
(

≤

≤

(Θh, ε/(2R))) + log(

log(
Ad log(1 + 4βR/ε) + d2A log(1 + 4Rα/(λε2)).

(Γh, ε/(2αR))

N

N

Substituting in β = tHR
λ

gives the result.

B.3.4 The proof of Proposition 24

The proof is similar to that of Proposition 21, in that it also relies on a covering argument to prove uniform
convergence over the set of potential value functions. The following technical result bounds the covering
number of this set:

Lemma 26. Let
supx∈S |

V (x)

−

, ε) be the ε-covering number of some set
(
V
N
V ′(x)
. Then, for any stage h = 1, . . . , H and episode t,
|

V

with respect to the distance

V

k

V ′

k∞ =

−

log(

N

Vh+1,t, ε))
(

≤

dA log(1 + 4tHR2/(ελ)).

38

To reduce clutter, we defer the proof to Appendix B.3.5. To proceed, we ﬁx h, a, ε > 0 and an arbitrary
ε. Then, by the triangle
∈ Vh+1,t, and consider a

V in the covering deﬁned above such that

∞ ≤

−

V

V

V
inequality, we have

Mh,a −
(cid:13)
(cid:0)
(cid:13)

Mh,a,t

V

e

Σh,a,t−1 ≤

c

(cid:1)

(cid:13)
(cid:13)

≤

Mh,a,t

V

Σh,a,t−1

+

Mh,a,t
c

(cid:1)
V
e

(cid:13)
(cid:13)

Σh,a,t−1

(cid:13)
(cid:0)
+ ε
(cid:13)

Mh,a −
(cid:13)
(cid:0)
Mh,a −
(cid:13)
(cid:13)
(cid:0)
(cid:13)

(cid:13)
(cid:13)
Mh,a,t

(cid:13)
(cid:13)
e
Mh,a −
λ−1/2tR + λ1/2CP
c
(cid:16)

(cid:1)(cid:0)

V

−

(cid:17)

V

Σh,a,t−1

(cid:1)(cid:13)
(cid:13)

,
e

(cid:1)
where we used Lemma 17 with B = ε in the last step. Setting δ′ = δ/(HA), the ﬁrst term can be bounded
e
V is in the covering, and using the union bound to show
with probability at least 1
that for every such

c
δ′ by exploiting that

V , we simultaneously have

−

(cid:13)
(cid:13)

V

Mh,a,t
e
d log (1 + tR2/λ) + dA log(1 + 4tHR2/(ελ)) + log (1/δ′) + CP H√λd
p

d log (1 + tR2/λ) + log (

Nε) = 2H

Σh,a,t−1 ≤

(cid:1)

Ct(δ′/

Nε/δ′) + CP H√λd

(cid:13)
(cid:13)

e

e

Mh,a −
(cid:13)
(cid:0)
2H
(cid:13)

≤

c
p

Putting the two bounds together and setting ε = λ/(tR) gives

Mh,a −
(cid:13)
(cid:0)
(cid:13)

Mh,a,t

V

Σh,a,t−1 ≤

c

(cid:1)

(cid:13)
(cid:13)

p
+ λ1/2

CP H√d + 1 + CP
(cid:16)

(cid:17)

.

2H

d log (1 + tR2/λ) + dA log(1 + 4t2HR3/λ2) + log (HA/δ)

This is clearly upper-bounded by the chosen value of ǫ. Taking a union bound over all h, a concludes the
proof.

B.3.5 The proof of Lemma 26

The proof is similar to that of Lemma 25, although simpler due to the simpler form of the value functions
in this case. We stary by noting that, due to the deﬁnition of the parameter vectors θ+
h,a,t as the solution of
the OPB equations (11) with

H, we have

V +
h+1,t

∞ ≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ+
h,a,t

tHR
λ

≤

def
= β,

where the inequality follows from Lemma 17. Given the deﬁnition of the algorithm, it is easy to see that the
value functions can be with the help of the function Uh,θ deﬁned as

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Uh,θ(x) = min

H

(cid:26)

h + 1, max

a∈A h

−

ϕ(x), θh,ai
(cid:27)

,

in the form Vh,t = Uh,θ for some θ with norm bounded by β. Thus, the set of value functions can be written
as

We show below that U is a smooth function of θ, which will allow us to prove a tight bound on the covering
number of the class

Vh. Indeed, this can be seen by

Vh =

{

Uh,θ :

θ

k

k ≤

.

β

}

Uh,θ −

k

Uh,θ′

k∞ ≤

≤

sup
x∈S (cid:12)
(cid:12)
R max
(cid:12)
(cid:12)
a

max
a∈A h

ϕ(x), θh,ai −
θ′
h,a

.

max
a∈A

ϕ(x), θ′

h,a

(cid:10)

(cid:11)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
x∈S

max
a∈A

≤

ϕ(x), θh,a −

θ′
h,a

(cid:12)
(cid:10)
(cid:12)

(cid:11)(cid:12)
(cid:12)

θh,a −
(cid:13)
(cid:13)

(cid:13)
(cid:13)

is bounded by
Thus, the ε/2-covering number of the set Θ =
(1 + 4β/ε)Ad, which follows from the standard fact that the ε-covering number of a ball in Rd with radius

(θa)a∈A : θa ∈

θak2 ≤

β

{

}

Rd, supa∈A k

39

c > 0 in terms of the ℓ2 distance is bounded by (1 + 2c/ε)d. Thus, we have that for any Vh ∈ Vh, there exists
Vh parameterized by
a

θh in the ε/2-covering of Θh such that,

e

Vh −
By rescaling of the covering numbers, we can see that the logarithm of the ε-covering number of
bounded by

Vhk∞ ≤
e

Rε/2.

e

k

Vh can be

log(

N

Vh, ε))
(

≤

log(

N

(Θh, ε/(2R)))

≤

dA log(1 + 4βR/ε),

giving the result.

40

