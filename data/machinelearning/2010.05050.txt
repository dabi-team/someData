Symbolic Parallel Adaptive Importance Sampling for
Probabilistic Program Analysis

Yicheng Luo∗
University College London
Imperial College London
United Kingdom
yicheng.luo.20@ucl.ac.uk

Antonio Filieri
Imperial College London
United Kingdom
a.filieri@imperial.ac.uk

Yuan Zhou
Artificial Intelligence
Research Center, DII
China
yuaanzhou@outlook.com

1
2
0
2

n
u
J

7
1

]

G
L
.
s
c
[

2
v
0
5
0
5
0
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT
Probabilistic software analysis aims at quantifying the probability
of a target event occurring during the execution of a program pro-
cessing uncertain incoming data or written itself using probabilis-
tic programming constructs. Recent techniques combine symbolic
execution with model counting or solution space quantification
methods to obtain accurate estimates of the occurrence probability
of rare target events, such as failures in a mission-critical system.
However, they face several scalability and applicability limitations
when analyzing software processing with high-dimensional and
correlated multivariate input distributions.

In this paper, we present SYMbolic Parallel Adaptive Importance
Sampling (SYMPAIS), a new inference method tailored to analyze
path conditions generated from the symbolic execution of programs
with high-dimensional, correlated input distributions. SYMPAIS
combines results from importance sampling and constraint solving
to produce accurate estimates of the satisfaction probability for
a broad class of constraints that cannot be analyzed by current
solution space quantification methods. We demonstrate SYMPAIS’s
generality and performance compared with state-of-the-art alter-
natives on a set of problems from different application domains.

CCS CONCEPTS
• Mathematics of computing → Metropolis-Hastings algo-
rithm; • Software and its engineering → Software verifica-
tion and validation.

KEYWORDS
symbolic execution, probabilistic analysis, probabilistic program-
ming, importance sampling, Markov chain Monte Carlo

1 INTRODUCTION
Probabilistic software analysis methods extend classic static analy-
sis techniques to consider the effects of probabilistic uncertainty,
whether explicitly embedded within the code – as in probabilistic
programs – or externalized in a probabilistic input distribution [15].
Analogously to their classic counterparts, these analyses aim at
inferring the probability of a target event to occur during execution,
e.g. reaching a program state or triggering an exception.

For the probabilistic analysis of programs written in a general-
purpose programming language, probabilistic symbolic execution
(PSE) [17, 20, 30] exploits established symbolic execution engines
for the language – e.g. [10, 40] – to extract constraints on proba-
bilistic input or program variables that lead to the occurrence of

∗Work done when the author is with Imperial College London

the target event. The probability of satisfying any such constraints
is then computed via model counting [3, 17] or inferred via solu-
tion space quantification methods [5, 6], depending on the types
of the variable and the characteristic of the constraints, and the
probability distributions. Variations of PSE include incomplete anal-
yses inferring probability bounds from a finite sample of program
paths executed symbolically [18], methods for non-deterministic
programs [30] and data structures [16], with applications to reli-
ability [17], security [41], and performance analysis [12]. While
PSE can solve more general inference problem, the overhead of
symbolic execution is typically justified when the probability of
the target event is very low (rare event) or high accuracy standards
are required, e.g. for the certification purposes of safety-critical
systems.

A core element of PSE is to compute the probability of certain
variables satisfying a constraint under the given input probability
distribution. In this paper, we focus on estimating the probability
of satisfying numerical constraints over floating-point variables.
For limited classes of constraints and input distributions, analytical
solutions or numerical integration can be computed [9, 19]. How-
ever, these methods become inapplicable for more complex classes
of constraints or intractable for high-dimensional problems.

Monte Carlo methods provide a more general and scalable al-
ternative for these estimation problems. These methods estimate
the probability of constraint satisfaction by drawing samples from
the input distribution and estimating the satisfaction probability as
the ratio of samples that satisfy the constraints. Nonetheless, while
theoretically insensitive to the dimensionality of problems, care
must be taken to apply direct Monte Carlo methods in quantifying
the probability of rare events, i.e., when the probability of satisfying
required constraints is extremely small.

To improve the accuracy of the estimation in the presence of
low satisfaction probability, recent work [5, 6] uses interval con-
straint propagation and branch-and-bound techniques to partition
the input domain of a program into sub-regions that contain only,
no, or in part solutions to a constraint. This step analytically elimi-
nates uncertainty about the regions containing only or no solutions,
requiring estimation to be performed only for the remaining re-
gions. The local estimates computed within these regions are then
composed using a stratified sampling scheme: the probability mass
from the input distribution enclosed within each region serves as
the weight of the local estimate, effectively bounding the uncer-
tainty that it propagates through the composition. However, the
performance of this method degrades exponentially with the di-
mensionality of the input domain, and it requires an analytical form
for the cumulative distribution function of the input distribution to

 
 
 
 
 
 
compute the probability mass enclosed within each region. Since
the cumulative distribution function of most correlated distribu-
tions is not expressible in analytical form, the numerical programs
that can be currently analyzed with PSE are restricted to those
with independent inputs. In this paper, we propose SYMbolic Par-
allel Adaptive Importance Sampling (SYMPAIS), a new inference
method for the estimation of the satisfaction probability of numeri-
cal constraints that exploits adaptive importance sampling to allow
the analysis of programs processing high-dimensional, correlated
inputs. SYMPAIS does not require the computability of the input
cumulative density functions, overcoming the limitations of cur-
rent state-of-the-art alternatives relying on stratified sampling. We
further incorporate results from constraint solving and interval
constraint propagation to optimize the accuracy and convergence
rate of the inference process, allowing it to scale to handle higher-
dimensional and more general input distributions. We implemented
SYMPAIS in a Python prototype and evaluated its performance on
a set of benchmark problems from different domains.

2 BACKGROUND
This section recalls program analysis and mathematical results
required to ground our contribution and details the limitations of
the state of the art we aim to tackle.

2.1 Probabilistic Symbolic Execution
Probabilistic symbolic execution (PSE) [20] is a static analysis tech-
nique aiming at quantifying the probability of a target event occur-
ring during execution. It uses a symbolic execution engine to extract
conditions on the values of inputs or specified random variables
that lead to the occurrence of the target event. It then computes the
probability of such constraints being satisfied given a probability
distribution over the inputs or specified random variables. These
constraints are called path conditions because they uniquely identify
the execution path induced by an input satisfying them [28].

1 // Probabilistic profile
2 altitude ::= Gaussian (8000 , 100) ;
3 obstacle_x , obstacle_y ::= Gaussian (
4

[ -2 , -2] ,
[[0.2 , 0.1] , [0.1 , 0.2]]) ;

5
6 // Program
7 if ( altitude <= 9000) { ...
8

if ( Math . pow ( obstacle_x , 2) +

9

10

Math . pow ( obstacle_y , 2) <= 1) {
callSupervisor () ;

...}

11
12 } else { callSupervisor () ; }

Listing 1: Example code snippet for an example safety
monitor of an autopilot navigation system.

Consider the simplified example in Listing 1, adapted from [6]
using a Java-like syntax and hypothetical random distributions
for the input variables. The snippet represents part of the safety
controller for a flying vehicle whose purpose is to detect environ-
mental conditions – excessive altitude or collision distance of an
obstacle – that may compromise the crew’s safety and call for a
supervisor’s intervention. The purpose of the analysis is to esti-
mate the probability of invoking callSupervisor at any point

Yicheng Luo, Antonio Filieri, and Yuan Zhou

in the code. Safety-critical applications may require this proba-
bility to be very small (e.g. < 10−7) and to be estimated with
high accuracy. The symbolic execution of the snippet, where ran-
dom variables are marked as symbolic, would return the follow-
ing two path conditions (PCs), corresponding to the possible in-
vocations of callSupervisor: 𝑃𝐶0: altitude > 9000; and 𝑃𝐶1:
altitude ≤ 9000 ∧ pow(obstacle_x, 2) + pow(obstacle_y) ≤ 1.
The probability of satisfying a path condition 𝑃𝐶 can be com-
puted based on the distributions assigned to the symbolic variables
as in Equation (2) (for simplicity, in the remainder of the paper we
assume a probability distribution is specified for every symbolic
variable or vector of symbolic variables):

𝑝𝑃𝐶 := Pr(𝑥 |= 𝑃𝐶) =

∫

𝒙

1𝑃𝐶 (𝒙)𝑝 (𝒙) 𝑑𝒙

≈

1
𝑁

𝑁
∑︁

𝑖=1

1𝑃𝐶 (𝒙 (𝑖) ) =: ˆ𝑝𝐷𝑀𝐶, where 𝒙 (𝑖) ∼ 𝑝 (𝒙)

(1)

(2)

where 1𝑃𝐶 (𝒙) denotes the indicator function, which returns 1 if
𝒙 |= 𝑃𝐶, that is 𝒙 satisfies 𝑃𝐶, and 0 otherwise. For clarity, we
will use ¯𝑝 (𝒙) to denote the truncated distribution satisfying the
constraints, i.e., , ¯𝑝 (𝒙) := 1𝑃𝐶 (𝒙)𝑝 (𝒙).

Because analytical solutions to the integral are in general in-
tractable or infeasible, Monte Carlo methods are used to approxi-
mate 𝑝𝑃𝐶 , as formalized in Equation (1). When the samples 𝒙 (𝑖) are
generated independently from their distribution 𝑝 (𝒙), Equation (2)
describes a direct Monte Carlo (DMC) integration (also referred to as
hit-or-miss), which is an unbiased estimate of the desired probability
and its variance ˆ𝑝𝐷𝑀𝐶 (1 − ˆ𝑝𝐷𝑀𝐶 )/𝑁 is a measure of the estimator
convergence, which can be used to compute a probabilistic accu-
racy bound – i.e., the probability of the estimate deviating from
the actual (unknown) probability by more than a positive accuracy
𝜖 > 0 [46].

Since the path conditions are disjoint [28] (i.e., 𝒙 |= 𝑃𝐶𝑖 ∧ 𝒙 |=
𝑃𝐶 𝑗 ⇒ 𝑖 = 𝑗), an unbiased estimator for the probability of the
target event to occur through any execution path is ˆ𝑝𝑃𝐶 = (cid:205)𝑖 ˆ𝑝𝑃𝐶𝑖
over all the 𝑃𝐶𝑖 reaching the target event.

Specialized model counting or solution space quantification
methods to solve the integral in Equation (2) for PSE application
have been proposed for linear integer constraints [17], arbitrary
numerical constraints [5, 6], string constraints [3], bounded data
structures [16]. In this work, we focus on the probabilistic analysis
of program processing numerical random variables.

2.2 Compositional Solution Space

Quantification

Borges et al. [6] proposed a compositional Monte Carlo method to
estimate the probability of satisfying a path condition over nonlin-
ear numerical constraints with arbitrary small estimation variance
– we will refer to this method as qCoral. The integrand function
in Equation (2) is an indicator function returning 1 for variable
assignments satisfying a path condition 𝑃𝐶, and 0 otherwise. Such
function is typically ill-conditioned for standard quadrature meth-
ods [42] and may suffer from the curse of dimensionality when
the number of symbolic variables grows; the ill-conditioning and
discontinuities of the integrand may also lead to high-variance for
Monte Carlo estimators, and particular care should be placed when

Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

dealing with low-probability constraints. qCoral combines insights
from program analysis, interval constraint propagation, and strati-
fied sampling to mitigate the complexity of the integration problem
and reduce the variance of its estimates.
Constraint slicing and compositionality. As already recalled,
the path conditions of a program are mutually exclusive. Therefore
the probability estimates of a set of path conditions leading to a
target event can be added algebraically – the mean of the sum being
the sum of the means, while the variance of the sum can be bounded
from the variance of the individual summands [6]. A second level
of compositionality is achieved in qCoral within individual path
conditions via constraint slicing. A path condition is the conjunction
of atomic constraints on the symbolic variables. Two variables
depend directly on each other if they appear in an atomic constraint.
The reflexive and transitive closure of this dependency relation
induces a partition of the atomic constraints that groups together
all and only the constraints predicating on (transitively) dependent
variables [17].

Because each group of independent constraints predicate on a
separate subset of the program variables, its satisfaction probability
can be estimated independently from the other groups. The satis-
faction probability of the path conditions is then computed using
the product rule to compose the estimates of each independent
group [47]. Besides enabling independent estimation processes to
run in parallel, constraint slicing can potentially reduce a high-
dimensional integration to the composition of low-dimensional
ones – on independent subsets of the symbolic variables, in turn
leading to shorter estimation time and higher accuracy for a fixed
sampling budget [6].
Interval constraint propagation and stratified sampling. To
further reduce the variance of the probability estimates of each
independent constraint, qCoral uses interval constraint propaga-
tion and branch-and-bound methods [22] to find a disjoint union
of n-dimensional boxes that reliably encloses all the solutions of a
constraint – where 𝑛 is the number of variables in the constraint.
Regions of the input domain outside the boxes are guaranteed to
contain no solutions of the constraint (1· (·) = 0). A box is classi-
fied as either an inner box, which contains only solutions, or an
outer box, which may contain both solutions and non-solutions.
Boxes are formally the conjunction of interval constraints bound-
ing each of the 𝑛 variables between a lower and an upper bound:
(cid:211)𝑛

𝑖=0
Because the boxes are disjoint, the probabilities of satisfying a
constraint 𝐶 from values sampled from each box can be composed
via stratified sampling as the weighted sum of the local estimates,
weighted by the cumulative probability mass enclosed within the
corresponding box [39]. However, since the inner boxes contain
only solutions, the probability of satisfying 𝐶 from values sampled
from an inner box is always 1 – no actual sampling required and,
consequently, no estimation variance to propagate. Sampling and
variance propagation is instead required only for the outer boxes,
as per Equation (3):

𝑙𝑏𝑖 ≤ 𝑥𝑖 ≤ 𝑢𝑏𝑖 .

¯𝑥𝐶 =

|𝑂 |
∑︁

𝑖=1

𝑝 (𝑂𝑖 ) ¯𝑥𝐶∧𝑂𝑖 +

|𝐼 |
∑︁

𝑗=1

𝑝 (𝐼𝑖 ), ¯𝑣𝐶 =

|𝑂 |
∑︁

𝑖=1

𝑝 (𝑂𝑖 )2 ¯𝑣𝐶∧𝑂𝑖 ,

(3)

where 𝑂 and 𝐼 are the sets of outer and inner boxes, respectively.
𝑝 (·) is the cumulative probability mass in a box, and ¯𝑥𝑐 and ¯𝑣𝑐
represent the mean and variance of the direct Monte Carlo estimates
for constraint 𝑐. For independent input variables (as assumed in
qCoral), the cumulative probability mass enclosed in a box is the
product of CDF(𝑢𝑏𝑖 ) − CDF(𝑙𝑏𝑖 ) for all the variables 𝑥𝑖 defining
the box. Sampling from within a box is possible if the distribution
of a variable 𝑥𝑖 can be truncated within the interval [𝑙𝑏𝑖, 𝑢𝑏𝑖 ].

Figure 1: Left: solution space of 𝑥 2 + 𝑦2 ≤ 1. Right: inner and
outer boxes produced by RealPaver, in pink and gray, respec-
tively.

Stratified sampling with interval constraint propagation can lead
to a significant variance reduction in the aggregated estimate by
reducing the uncertainty only to the regions of the domain enclosed
within the outer boxes, potentially avoiding sampling from large
regions of the domain that can be analytically determined as includ-
ing only or no solutions. Because boxes can be iteratively refined
up to arbitrary accuracy, there is a trade-off between the target size
(and, consequently, weight) of the boxes and their number (since
each outer box requires a Monte Carlo estimation process).
Example. Consider the constraint 𝑥 2 + 𝑦2 ≤ 1 from the example
in Listing 1 (variable names abbreviated). Performing interval con-
straint propagation with RealPaver [22] – an interval constraint
solver supporting conjunctive, nonlinear inequality constraints
used in qCoral – with the initial input domain 𝑥, 𝑦 ∈ [−2, 2], we
obtained the outer and inner boxes depicted in Figure 1 in gray and
pink, respectively.

In Figure 1, a large region of the domain falls outside the boxes
since it contains no solutions. Hence, the probability of satisfying
the constraint for values in this region is 0. Similarly, the probability
of satisfying the constraint with inputs from an inner box is 1.
Therefore, uncertainty is bounded within the outer boxes, and
estimation proceeds sampling from their truncated distributions
and aggregating the result via stratified sampling.

2.3 Limitations of qCoral
qCoral can produce scalable and accurate estimates for the satis-
faction probability for constraints that 1) have low dimensionality
or can be reduced to low-dimensional subproblems via constraints
slicing, 2) are amenable to scalable and effective interval constraint
propagation, and 3) whose input distribution have CDFs in analyti-
cal form and allows efficient sampling from their truncated distribu-
tions. These constraints typically do not hold for high-dimensional
and correlated input distributions.

−202x−202y−202x−202Constraint slicing assumes that all the inputs are probabilistically
independent, with dependencies among variables arising only from
computational operations (e.g. if (𝑥 + 𝑦 > 0). . . ). Support for
correlated variables requires changing the dependency relation
to include also all correlated variable pairs. This may reduce the
effectiveness of constraint slicing in reducing the dimensionality
of the integration problems.
Interval constraint propagation contributes to reducing estima-
tion variance by pruning out large portions of the input domain
that do not contain solutions of a constraint and producing small-
size outer boxes to bound the variance propagated from in-box
local estimates. However, the complexity of this procedure grows
exponentially with the dimensionality of the problem, rendering it
ineffective when, after constraint slicing, the number of variables
appearing in an independent constraint is still large, e.g. due to cor-
related inputs that cannot be separate. The effectiveness of interval
constraint propagation for nonlinear, non-convex constraints also
varies significantly for different formulations of the constraint (e.g.
𝑥 2 vs. 𝑥 ×𝑥) and may require manual tuning for optimal results [22].
Stratified sampling requires analytical solutions of the input
CDFs, as well as the ability to sample from truncated distributions.
Both requirements are generally unsatisfiable for correlated input
variables, whose CDF cannot be computed in closed form. The lack
of an analytical CDF would require a separate Monte Carlo esti-
mation problem to quantify the probability mass enclosed within
each box and an analysis of how the corresponding uncertainty
propagates through the stratified sampling and the composition
operators of qCoral. Additionally, sampling from a truncated dis-
tribution typically relies on the computation of both the CDF and
the inverse CDF of the original distribution, which is inefficient
without an analytical form of these functions.

In summary, the main variance reduction strategies of qCoral
based on interval constraint propagation and stratified sampling
are not applicable for all but trivial correlated input distributions.
Constraint slicing can be extended with probabilistic dependencies
among input variables, but this results in smaller dimensionality
reduction, with exponential impact on interval constraint propa-
gation even when the CDFs of correlated inputs can be computed
analytically.

2.4 Importance Sampling
The indicator function in Equation (2) return 1 only within the
regions of the input domain satisfying a constraint (e.g. only within
the circle in Figure 1). When this region encloses only a small
probability mass, direct Monte Carlo methods sampling from the
input distribution 𝑝 (𝒙) may struggle to generate enough samples
that satisfy the constraint, and therefore fail to estimate the quantity
of interest 𝑝𝑃𝐶 . We discussed before how qCoral uses interval
constraint propagation and stratified sampling to prune out regions
of the domain that contain no solutions, sampling within narrower
boxes containing a larger portion of solutions.

An alternative method to improve statistical inference in this
problem is importance sampling (IS). Instead of sampling from the
input distribution 𝑝 (𝒙), IS generates samples from a different pro-
posal distribution – 𝑞(𝒙) – that overweighs the important regions
of the domain, i.e., the regions containing solutions in our case.

Yicheng Luo, Antonio Filieri, and Yuan Zhou

Because the samples are generated from a different distribution
than 𝑝 (𝒙), the computed statistics need to be re-normalized as
in Equation (4):
∫

∫

𝑝𝑃𝐶 :=

1𝑃𝐶 (𝒙)𝑝 (𝒙) 𝑑𝒙 =

𝒙

𝑁
∑︁

𝑖=1

≈

1
𝑁

1𝑃𝐶 (𝒙 (𝑖) )𝑝 (𝒙 (𝑖) )
𝑞(𝒙 (𝑖) )

1𝑃𝐶 (𝒙)𝑝 (𝒙)
𝑞(𝒙)

𝒙

𝑞(𝒙) 𝑑𝒙

(4)

=: ˆ𝑝𝐼𝑆, where 𝒙 (𝑖) ∼ 𝑞(𝒙).

(5)

While any distribution 𝑞(𝒙) > 0 over the entire domain guaran-
tees the estimate will eventually converge to the correct value, an
optimal choice of 𝑞(𝒙) determines the convergence rate of the pro-
cess and its practical efficiency. In our context of estimating the
probability of satisfying path conditions 𝑃𝐶, the optimal proposal
distribution 𝑞∗ (𝒙) is exactly the truncated, normalized distribution
𝑝 (𝒙) satisfying 𝑃𝐶,

𝑞∗ (𝒙) =

1
𝑝𝑃𝐶

𝑝 (𝒙)1𝑃𝐶 (𝒙).

In general, it is infeasible to sample from 𝑞∗ (𝒙) as it requires the
calculation of 𝑝𝑃𝐶 which is exactly our target. Fortunately, as we
will demonstrate in Section 3.1, a proposal distribution found via
adaptive refinement can allow us to achieve near-optimal perfor-
mance.

In this paper, we propose a new inference method to estimate the
satisfaction probability of numerical constraints on high-dimensional,
correlated input distributions. Our method does not require analyt-
ical CDFs and can replace qCoral’s variance reduction strategies
to analyze constraints where these are not applicable. Our method
combines results from constraint solving and adaptive estimation to
produce near-optimal proposal distributions aiming at computing
high-accuracy estimates suitable for the analysis of low-probability
constraints.

3 SYMPAIS: SYMBOLIC PARALLEL ADAPTIVE

IMPORTANCE SAMPLING

In this section, we introduce our new solution space quantifica-
tion method for probabilistic program analysis: SYMbolic Parallel
Adaptive Importance Sampling (SYMPAIS).

Figure 2: Overview of SYMPAIS.

Figure 2 gives an overview of SYMPAIS’s workflow. Following
the probabilistic symbolic execution approach, the path conditions
leading to the occurrence of a target event are extracted using a
symbolic execution engine. For simplicity, we assume that each
(vector of) symbolic variables is associated with a probability distri-
bution, either provided explicitly by the user or extracted from the
code, where it has been specified via convenient random generators.

ProgramPCsPCsPCsp(x)Symbolic Execution + assumptionsInferencePath conditionsProbabilistic profileConstraint Solverx1,…,xNMCMCq(x)pPCSymbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

Independent variables are associated with univariate probability
distributions. Vectors of correlated variables are associated with
multivariate probability distributions. For example, in Listing 1,
altitude is associated with a univariate Gaussian distribution with
location 8000 and scale 500, while < obstacle_x, obstacle_y >
are distributed as a bivariate Gaussian with location [−2, −2] and
covariance matrix [[0.1, 0.1], [0.1, 0.2]]. The path conditions are
assumed to have been sliced as in [6, 17], where the dependency
relation is augmented with pairwise dependency between the cor-
related variables, besides the dependencies induced by the program
control and data flows. The probability of satisfying each indepen-
dent constraint is quantified in the inference phase, which is the
focus of this work.

⊲ SymEx(P), domain knowledge

Algorithm 1 Symbolic Parallel Interacting Markov Adaptive Sam-
pling (SYMPAIS)
1: Given 𝐶, 𝑝 (𝒙)
2: 𝑝 (𝒙) ← 𝑝 (𝒙)1𝐶 (𝒙)
3: 𝒙init ← 𝑚𝑜𝑑𝑒𝑙𝑠 (𝐶)
4: Initialize the proposal distribution 𝑞𝑛,0 for 𝑛 = 1, . . . , 𝑁 .
5: for 𝑡 ← 1, . . . ,𝑇 do
6:

Update 𝑁 proposal distributions 𝑞1:𝑁 ,𝑡 using MCMC
Draw 𝑀 samples from each proposal distribution,

⊲ Run PI-MAIS for 𝑇 iterations

⊲ Constraint solver

7:

8:

(𝑚)
𝑛,𝑡 ∼ 𝑞𝑛,𝑡 (𝒙),

𝒙

for 𝑚 = 1 : 𝑀, and 𝑛 = 1 : 𝑁 .

Compute importance-sampling weights,

𝑤 (𝑚)

𝑛,𝑡 ←

𝑝 (𝒙

(𝑚)
𝑛,𝑡 )
𝑞 𝑗,𝑡 (𝒙

1
𝑁

(cid:205)𝑁

𝑗=1

(𝑚)
𝑛,𝑡 )

9: end for
10: Obtain PI-MAIS esitmate,
𝑇
∑︁

ˆ𝑝PIMAIS ←

1
𝑇 · 𝑁 · 𝑀

ˆ𝑣 2
PIMAIS ←

1
𝑇 · 𝑁 · 𝑀

11: return ˆ𝑝PIMAIS, ˆ𝑣 2

PIMAIS

𝑁
∑︁

𝑀
∑︁

𝑤 (𝑚)
𝑛,𝑡 ,

𝑡 =1
𝑇
∑︁

𝑛=1
𝑁
∑︁

𝑚=1
𝑀
∑︁

𝑡 =1

𝑛=1

𝑚=1

(𝑤 2(𝑚)

𝑛,𝑡 − ˆ𝑝PIMAIS)2

(6)

(7)

(8)

(9)

Due to the wide range of possible constraint forms (e.g. linear, non-
linear, non-convex) and of different types of distributions, optimal
proposal distributions cannot be obtained analytically. It is instead
approximated via a hierarchical probability distribution whose pa-
rameters are iteratively refined via a Markov Chain Monte Carlo
(MCMC) algorithm [35, 36] to best approximate the intractable op-
timal proposal. MCMC algorithms generate sequences of samples
that, when the process converges to its steady state, the samples are
distributed according to a target distribution whose analytical form
may be unknown or from which it is not possible or intractably
complex to sample directly. The MCMC samples can thus be used
to iteratively estimate the parameters of the proposal distribution
towards approximating the optimal distribution. The algorithm
returns the estimate of the satisfaction probability, as well as the
estimator variance. The latter may be used to reason about the
dispersion of the estimate, e.g., constructing confidence intervals to
decide if more sampling is desirable. Notice however that, similarly
to qCoral [6], the estimator variance is centered around the esti-
mate (Formula (9) in Algorithm 1). This requires enough samples
to have been collected for the estimate to stabilize first in order for
the variance to represent the estimator dispersion around it.

In the remaining of the section (Section 3.1), we will formally
define the adaptive importance sampling strategy of SYMPAIS and
the MCMC methods it adopts for the adaptive refinement of the pro-
posal distribution. Results from constraint solving will be brought
in to mitigate the complexity of the estimation process and acceler-
ate its convergence. A set of optimizations to improve the practical
performance of the methods will be discussed in Section 3.2, while
implementation details and an experimental evaluation will be
reported in Section 4.
Running example. To illustrate the different features of SYMPAIS,
we will use a 3-dimensional, nonlinear and non-convex constraint
– torus –, which is defined in Equation (10):
√︃

𝑥 2 + 𝑦2 − 𝑅)2 + 𝑧2 ≤ 𝑟 2,

(10)
with the constant parameters 𝑅 = 3, 𝑟 = 1. At first, we will associate
to each of the three variables an independent univariate Gaussian
distribution: i.e., 𝑥, 𝑦, 𝑧 ∼ N (0, 0.5). We will later generalize the
method to correlated inputs.

(

The main steps of SYMPAIS are summarized in Algorithm 1.
SYMPAIS takes as input a constraint 𝐶, which may be a path con-
dition of 𝑃 or an independent portion of the path condition after
constraint slicing, 𝐶 is assumed to be the conjunction of inequalities
(or equalities) on numerical functions of the inputs. In addition to
the constraint 𝐶, SYMPAIS requires specifying a probability dis-
tribution 𝑝 (𝒙) over the symbolic variables in the program. Such
distribution can be provided by the user or specified in the code
via convenient random generators. For simplicity, we refer to the
probability distribution over all of the symbolic variables as the
input distribution.
Overview. The core part of SYMPAIS is the adaptive importance
sampling process implemented in the for loop at Line 5. The goal of
this process is to iteratively refine an importance sampling proposal
that maximizes sample efficiency, i.e., it is very likely to generate
sample points within the solution space of the input constraint 𝐶.

3.1 SYMPAIS Adaptive Importance Sampling
As recalled in Section 2.4, importance sampling (IS) methods aim at
constructing a proposal distribution 𝑞(𝒙) that increases the likeli-
hood of generating samples that satisfy a constraint 𝐶. This allows
us to focus the estimation problem to the regions of the input do-
main that satisfy 𝐶, while avoiding the need to find a stratification of
the input domain and computing the inverse CDFs for the distribu-
tion truncation that prevent the use of qCoral for high-dimensional
and correlated input distributions.

The choice of a proposal distribution 𝑞(𝒙) largely affects the
efficiency of IS. However, it is usually difficult to obtain an ana-
lytical form for the theoretically optimal 𝑞∗ (𝒙) or to sample from.
Instead, we use an adaptive scheme to iteratively refine a proposal
distribution to approximate 𝑞∗ (𝒙).

3.1.1 Adaptive proposal refinement. To construct and refine the
IS proposal distribution, SYMPAIS adapts the parallel interacting

Yicheng Luo, Antonio Filieri, and Yuan Zhou

value 𝒙. The latter requires the 𝑝 (𝑥) to be differentiable and exploits
the gradient information to achieve higher efficiency in many cases,
especially for higher dimensional problems. For space reason, in
the remainder of this section we will mostly focus on RWMH while
additional details on our HMC implementation are provided in [32].
Random-Walk Metropolis-Hasting for adaptation.

(cid:17)

(cid:16)

MCMC methods generate a sequence of samples where each sam-
ple is via a probabilistic transition from its predecessor. Random-
Walk Metropolis-Hasting (RWMH) [35, 39] is an MCMC algorithm
where the next sample 𝒙 ′ is generated from its predecessor 𝒙 from
a proposal distribution (or proposal kernel) 𝜅 (𝒙 ′|𝒙). The newly
proposed sample 𝒙 ′ is accepted and added to the sequence ran-
domly with probability 𝛼 = min
, otherwise the
new sample is rejected and 𝒙 is retained.

1, 𝑝 (𝒙′)𝜅 (𝒙 |𝒙′)
𝑝 (𝒙)𝜅 (𝒙′ |𝒙)

For RWMH within SYMPAIS, we use 𝜅 (𝒙 ′|𝒙) = N (𝒙 ′; 𝒙, Σ), i.e.,
the next candidate sample is generated by adding a white Gaussian
noise with covariance Σ to the current sample 𝒙. After the genera-
tion process converges at steady state, a value 𝒙 should appear in
the sequence with a frequency proportional to its probability in the
target distribution 𝑝 (𝒙). Because 𝑝 (𝒙) is zero outside the solution
space of the constraint 𝐶, all the samples that do not satisfy 𝐶 will
be rejected. High rejection rate slows down the convergence and
can be mitigated by tuning Σ, using a different proposal kernel [39],
or switching to more sophisticated methods to generate the next
sample, such as a Hamiltonian proposal.

SYMPAIS estimation process. Each of the parallel MCMC
3.1.2
processes used to refine the importance sampling proposal requires
an initial value 𝒙𝑖𝑛𝑖𝑡 to start from. In theory, any point from the
input domain can be chosen to start the MCMC processes. How-
ever, principled choices of 𝒙𝑖𝑛𝑖𝑡 can speed up the converge of the
Markov chain to a steady state and reduce the sample rejection
rate. The choice of the initial points is particularly important when
the constrained distribution 𝑝 (𝒙) is multimodal – i.e., its density
function has two or more peaks –, either because the original input
distribution 𝑝 (𝒙) is itself multimodal or because the restriction to
the solution space of 𝐶 induces multiple modes in 𝑝 (𝒙). We will
discuss the problem of multiple modes and SYMPAIS’s mitigation
strategies in Section 3.2 while focus here on SYMPAIS’s use of
constraint solving to initialize the MCMC processes.

In statistical inference literature, the initial sample of an MCMC
process is typically randomly assigned by a value within the input
domain. However, if the satisfaction probability of the constraint
𝐶 is small, randomly generating a value of 𝒙 that satisfies the con-
straint may require a large number of attempts. Instead, we use
a constraint solver (Z3 [14] in this work) to generate one or more
models for the constraint 𝐶 to seed the MCMC processes.

Figure 4 demonstrates visually the evolution of the proposal
distribution through the iterations of the SYMPAIS loop (Line 5)
towards the optimal proposal distribution depicted in Figure 3. We
use a projection of the torus constraint on the x-y plane again as
an example. At the beginning (𝑡 = 0), the process is initialized with
a solution produced by Z3. The importance sampling proposal dis-
tribution is concentrated around that point, where darker shadows
of blue represent higher probability density. At iteration 𝑡 = 10, the
proposal distribution translated towards the inner border of the
solution space, where the constrained input distribution 𝑝 (𝒙) has

Figure 3: Left: input distribution 𝑝 (𝒙) for torus projected on
the x-y plane. Right: the optimal importance sampling pro-
posal 𝑞∗ (𝒙). The intensity of the blue shadowing is propor-
tional to the probability density. The solution space lays be-
tween the dashed circles.

Markov adaptive sampling (PI-MAIS) schema defined in [34]. The
proposal distribution in PI-MAIS is a hierarchical model parame-
terized by 𝑁 sub-proposals 𝑞1, . . . , 𝑞𝑁 . To sample from the proposal
distribution, we first choose a sub proposal 𝑞𝑖 and then draw sam-
ples 𝒙 from 𝑞𝑖 . Together, the sub-proposals form a mixture distribu-
tion. PI-MAIS adapts the sub-proposals to the target distribution by
running parallel-chain MCMC (Line 6) so that it can form efficient
proposal distributions for target distributions that are multimodal
and non-linear1.

In this paper, we use sub-proposals parameterized by Gaussian
distributions 𝑞𝑖 (𝒙) = N (𝒙; 𝝁𝑖, Σ). The probability density function
(PDF) for the proposal distribution is then given by a Gaussian:
𝑁
∑︁

𝑞(·) =

1
𝑁

N (·, 𝝁𝑖, Σ),

𝑖=1
(cid:9)𝑁
where the mean vectors (cid:8)𝝁𝑖
𝑖=1 are adapted by running 𝑁 parallel
MCMC samplers so that the proposal distribution approximates
more accurately 𝑞∗ (𝒙). At each step 𝑡, a sampler produces a set of
samples {𝒙𝑛,𝑡 }. The proposal distribution at step 𝑡 is:

𝑞𝑡 (·) =

1
𝑁

𝑁
∑︁

𝑛=1

𝑞𝑛,𝑡 (·) =

1
𝑁

𝑁
∑︁

𝑛=1

N (·|𝒙𝑛,𝑡 , Σ).

When the refinement process stabilizes, the estimate for the prob-
ability of satisfying the constraint 𝐶 given the input distribution
𝑝 (𝒙) (i.e., the solution of the integration problem in Equation (1))
is:

ˆ𝑝𝑃𝐼 𝑀𝐴𝐼𝑆 ≈

1
𝑇

1
𝑁

1
𝑀

𝑇
∑︁

𝑁
∑︁

𝑀
∑︁

𝑡 =1

𝑛=1

𝑚=1

𝑤𝑡,𝑛,𝑚, 𝑤𝑡,𝑛,𝑚 =

𝑝 (𝒙

𝑞𝑡 (𝒙

(𝑚)
𝑛,𝑡 )
(𝑚)
𝑛,𝑡 )

,

(𝑚)
𝑛,𝑡 are samples drawn from 𝑞𝑛,𝑡 (𝒙). Please refer to [34]

where 𝒙
for the convergence proofs of the PI-MAIS scheme.

To update the proposal distribution, we implemented two MCMC
samplers in SYMPAIS: random-walk Metropolis-Hastings (RWMH)
and Hamiltonian Monte Carlo (HMC). The former provides a gen-
eral procedure that only requires the ability to evaluate the density
of the constrained input distribution ¯𝑝 (𝒙) = 𝑝 (𝒙)1𝐶 for a given

1We provide an executable notebook with more details on PI-MAIS in the open-source
code [31].

−505x−505yp(x)−505xq∗(x)Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

of 𝑝 (𝒙), which behaves as an attractor for the MCMC processes,
requiring a longer time to converge to covering all the modes.
Example. Figure 5 shows how the optimal proposal distribution
𝑞∗ (𝒙) for a unimodal, correlated input distribution 𝑝 (𝒙) – we used
a Student’s T distribution with 2 degrees of freedom for the plot
– degenerates into a bimodal optimal proposal when constrained
within the solution space of 𝐶 (torus constraint projected to the x-y
plane). An MCMC process initialized in the neighborhood of one
of the mode may take a long time before “jumping” in the neigh-
borhood of the other mode, having to traverse a low probability
path across the non-convex solution space.

Figure 4: Graphical illustration of learning the adaptive pro-
posal in SYMPAIS.

.

a higher density. The white dots represent the samples 𝒙𝑛,𝑡 from
the MCMC processes that are also used to refine the mean vectors
of the importance sampling proposal 𝑞𝑛,𝑡 . Proceeding through the
iterative refinement, at iteration 𝑡 = 1000 the proposal distribution
approximates the optimal proposal very closely. The red line in the
rightmost subfigure shows a trajectory – a sequence of values –
generated by one of the MCMC processes, which touches portions
of the solution space approximately proportionally to their density
in the optimal distribution.

The accurate approximation of the optimal proposal distribution
allows SYMPAIS to effectively sweep the solution space of 𝐶 and
estimate its satisfaction probability.
Correlated input distributions. The adaptive importance sam-
pling strategy, as well as the MCMC processes described in this
section, do not require the input distributions to be independent.
Correlated distributions (such as the bivariate Gaussian in List-
ing 1) can be seamlessly processed by SYMPAIS. The requirement
for RWMC is the ability to evaluate the PDF of the distribution,
while HMC requires its differentiability. Computing the CDF (and
its inverse) as required for qCoral’s stratified sampling does in-
stead involve an integration problem that usually has no analytical
solution and requires a separate Monte Carlo integration. SYMPAIS
thus complements qCoral to allow the probabilistic analysis of a
broader range of programs. We will demonstrate applications of
SYMPAIS to correlated input distributions in Section 4.

3.2 Optimizations
The target distribution of SYMPAIS is 𝑝 (𝒙) = 𝑝 (𝒙)1𝑃𝐶 (𝒙), where
the indicator function zeroes the input distribution’s density outside
the solution space of 𝐶. However, the MCMC processes are not
aware of the geometry or location of the solution space of 𝐶. The
volume and shape of the solution space may affect the rejection rate
of the processes – how often the random walk reaches non-solution
points – and may induce multiple modes in 𝑝 (𝒙) even if 𝑝 (𝒙) is
unimodal. Intuitively, each mode is a peak in the density function

Figure 5: Left: unimodal correlated input distribution 𝑝 (𝒙).
Right: optimal bi-modal proposal distribution 𝑞∗ (𝒙).

While a complete characterization of 𝐶’s geometry is intractable,
in this section, we propose three heuristic optimizations that may
mitigate the impact of ill geometries of the solution space on SYM-
PAIS adaptive importance sampling.

3.2.1 Diverse initial solutions. For an effective importance sam-
pling, the adaptive proposal distribution should capture all the
modes of 𝑞∗ (𝒙). Running multiple MCMC processes in parallel in-
creases the chances of at least any of them covering each mode. In
the statistical inference literature, each chain is typically initialized
with an independent random sample from the input distribution
to maximize the chances of reaching all the modes on the whole.
However, as discussed before, if the satisfaction probability of 𝐶 is
small, it is unlikely to randomly generate valid solutions and even
less likely to also cover multiple modes.

Initializing all the chains with a feasible solution generated by
the constraint solver may result in the MCMC processes explor-
ing, in a finite time, only the mode closest to the initial solution.
We observed this phenomenon in particular for RWMH, but mul-
timodal distributions require longer convergence times also with
HMC. The top row in Figure 6 shows the evolution of the adaptive
importance sampling proposal 𝑞(𝒙) of SYMPAIS initialized with a
single solution from the constraint solver. This time, the optimal
proposal distribution 𝑞∗ (𝒙) is the one on the right-hand side of
Figure 5. After 𝑡 = 100 iterations, 𝑞(𝒙) still fails to converge to
𝑞∗ (𝒙), with most of the samples still generated around one of the
two modalities – which have instead the same density in 𝑞∗ (𝒙).

To mitigate this problem, SYMPAIS tries to generate multiple
diverse solutions of 𝐶 to increase the probability of obtaining at
least one in the neighborhood of each mode. We explored two
different approaches for this purpose. In principle, initial solutions
diversity can be achieved using an optimizing solver where each

−505yt=0t=10−505x−505yt=100−505xt=1000−505x−505yp(x)−505xq∗(x)Yicheng Luo, Antonio Filieri, and Yuan Zhou

When the 𝑁 parallel MCMC processes are initialized with solutions
taken uniformly at random from those generated by constraint
solver, many of these solutions are likely to be far from the modes,
therefore requiring longer warmup of the Markov chains to move
towards the modes. This can be observed in the wide spread of the
proposal 𝑞(𝒙) for 𝑡 = 0, 10 in the middle row of Figure 6.

To reduce the number of samples used for warmup, we sam-
ple the initial solutions proportionally to their likelihood in the
input distribution. Let {𝒙𝑖 }𝐹
𝑖=1 be the initial solutions found by the
constraint solver. We sample 𝑁 initial points {𝒙 ′

𝑖=1 from

𝑞(𝒙 ′) =

𝐹
∑︁

𝑖=1

𝑤𝑖 𝛿𝒙𝑖 (𝒙 ′), 𝑤𝑖 = 𝑝 (𝒙𝑖 )/

𝑝 (𝒙 𝑗 ),

𝑖 }𝑁
𝐹
∑︁

𝑗=1

Figure 6: Convergence of the adaptive proposal distribution
with different initialization strategies.

solution is chosen to maximize the distance from all the previous
ones. This method is general and flexible (e.g. it allows customizing
the distance function based on domain-specific information), but
computationally heavy for non-linear, non-convex constraints.

An alternative, more scalable method relies on interval constraint
propagation and branch and bound algorithms to single out regions
of the input domain that satisfy 𝐶. In our implementation, we use
RealPaver’s depth-first search mode with a coarse accuracy [22]
(10−2 in this example, but the configuration can be tuned based on
the length of the domain dimensions) to generate several boxes that
contain solutions of 𝐶. This mode differs from the standard paving
used in qCoral because it does not require the computed boxes
enclosing all the solutions, but potentially only a subset of them,
making it more scalable even for higher-dimensional problems. For
each box, SYMPAIS calculates the center and, if it satisfies 𝐶, adds
it to the MCMC initialization points.

An example of the SYMPAIS adaptive proposal distribution using
this heuristic is shown in the middle row of Figure 6. The initial
solutions cover, with different concentrations, several parts of the
solution space of 𝐶. While the initial iterations result in a very
spread proposal 𝑞(𝒙) – contrary to the highly concentrated optimal
proposal – the diversity of the initial points allows the adaptive
refinement to converge to covering both optimal proposal’s modes.
While the heuristics of using depth-first interval constraint prop-
agation may fail to produce solutions at the center of its boxes, or
to refine the boxes enough for it to happen, when applicable, it is
an efficient alternative to solving heavier optimization problems.

3.2.2 Re-sampling. The diversification of the initial solutions de-
scribed in the previous section uses only information about the
constraint to generate diverse initial points. However, the con-
straint solver is not aware of the underlying input distribution and
may generate many solutions that are far from the modes of 𝑞∗ (𝒙).

to be the initial states for the 𝑁 parallel MCMC chains.

The initial solutions seeding the MCMC chains now reflect both
the location of the solutions space – from the constraint solver –
and the distribution of the input probability across the solution
space – from the resampling. An example of the effects on the
convergence speed of SYMPAIS adaptation is shown in the bottom
row of Figure 6.

3.2.3 Truncated kernel for RWMH. The proposal kernel for RWMH
defined in the previous section generates the next sample by adding
Gaussian noise to the current one: 𝜅 (𝒙 ′|𝒙) = N (𝒙 ′; 𝒙, Σ). While
arbitrarily concentrated around 𝒙 by the value of Σ, this kernel may
propose new samples far away from the solution space of 𝐶, which
would then be rejected.

A way to reduce the rejection rate is to replace the kernel with
a Gaussian noise truncated within a smaller region of the input
domain that contains the solution space of 𝐶. We obtain this re-
gion in the form of the smallest n-dimensional box that contains 𝐶.
Such a box can be efficiently computed when an interval contrac-
tor function is defined for 𝐶 [2]. Efficient interval contractors are
implemented for a broad class of numerical constraints, e.g. [21],
and can be used to compute the smallest box B that encloses the
solutions space of 𝐶. The Gaussian proposal kernel of RWMH can
then also be replaced by the truncated Gaussian proposal kernel
𝜅 B (𝒙 ′|𝒙) = NB (𝒙 ′; 𝒙, Σ) to increase the probability of generate
candidate samples 𝒙 ′ that are still solutions of 𝐶.

Notice that this optimization does not require us to truncate the
input distribution 𝑝 (𝒙), as required by qCoral. Instead, it truncates
the uncorrelated Gaussian distribution of the proposal kernel of
RWMH, which can be done efficiently. Notably, the rejection rate
does not go to zero because B may be a coarse bounding of the
solution space of 𝐶, which may also be non-convex. Nonetheless, it
usually increases the probability of sampling solutions of 𝐶.

4 EVALUATION
In this section, we report an experimental evaluation of SYMPAIS.
We include direct Monte Carlo (DMC) estimation as a baseline,
qCoral for the uncorrelated input distributions, and SYMPAIS and
SYMPAIS-H where we configure SYMPAIS to use the RWMH and
the HMC algorithms for the MCMC samples, respectively. We in-
clude two geometrical microbenchmarks to expose the features
of SYMPAIS and six benchmarks from path conditions extracted
from a ReLU neural network and subjects from qCoral. Because the

−505SingleSolutionyt=0t=10t=100−505DiverseSolutiony−505x−505Re-sampley−505x−505xSymbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

Figure 7: RAE and convergence rates for spheres of different
dimensionality.

dependability of Monte Carlo estimators’ variance depends on the
convergence of their estimates (cf. Section 3, overview), we use the
relative absolute error (RAE) to compare the estimates against the
ground truth, instead of performing statistical tests on the variance
that are particularly challenging with the rare events considered in
this paper. Details about the experimental environment and set-up
are available in the extended version [32]. Code is available at [31].

4.1 Geometrical Microbenchmarks
Sphere. The first constraint we consider the d-dimensional
4.1.1
sphere: 𝐶 := (cid:8)∥𝒙 − 𝒄 ∥2 ≤ 1(cid:9), where 𝒙 ∈ [−10, 10]𝑑 ∩R𝑑 is the input
domain, and 𝒄 ∈ R𝑑 is the center of the sphere. We use 𝑝 (𝒙) =
N (0, 𝐼 ) – i.e., uncorrelated Gaussian – as the input distribution
and set 𝒄 = 1. Despite its simplicity, this problem illustrates the
challenges faced by direct Monte Carlo methods as well as qCoral
in high-dimensionality problems where 𝐶’s satisfaction probability
is small.

Specifically, as 𝑑 increases, the probability of the event happening
decreases, which makes estimation by DMC increasingly challeng-
ing. Moreover, the increase in 𝑑 also leads to coarser paving of
𝑑-dimensional boxes, which reduces the effectiveness of variance
reduction via stratified sampling.

The RAE results are illustrated in Figure 7 (left). As expected:
DMC achieves the worst performance throughout all tests. For low-
dimensional problems (𝑑 ≤ 4), qCoral is the most efficient, while
its performance deteriorates significantly when the 𝑑 increases
and RealPaver fails to prune out large portions of the domain that
contain no solutions. SYMPAIS’s performance is comparable to
qCoral in low dimensions, but up to one order of magnitude more
accurate when the dimensionality grows (𝑑 ≥ 8).

Figure 7 (right) shows the convergence rate of RAE for different
methods over sample size for 𝑑 = 8. SYMPAIS achieves the final
RAE of DMC with < 5% of the sampling budget and the final RAE
of qCoral with < 10%. SIMPAIS-H only marginally outperforms
SYMPAIS for 8 ≤ 𝑑 ≤ 10. The improvement in sample efficiency
becomes more significant for 𝑑 > 8.

4.1.2 Torus. Torus is a three-dimensional constraint introduced in
Section 3 as a running example. We evaluate the different methods
for both independent and correlated inputs.
Independent inputs. We first consider the uncorrelated input dis-
tribution 𝑝 (𝒙) = N (0, 0.5I) with input domain 𝒙 ∈ [−5, 5]3 ∩ R3.

Figure 8 (left) shows the RAE performance of the four methods.
While performing marginally better than the baseline DMC, qCoral
achieve poor performance on this non-convex subject because Re-
alPaver fails to effectively prune out the inner empty region of the
domain within the torus, effectively reducing qCoral to a DMC
sampling over most of the input domain.

RealPaver can be fine-tuned for a torus constraints by using
different consistency configurations (see [22] for instructions on
the matter). However, this may require human ingenuity to select
and tune the correct settings. Finally, we observed the performance
of RealPaver varies for equivalent formulations of the constraint
(e.g., 𝑥 2 vs 𝑥 × 𝑥 or reformulating the constraint without 𝑠𝑞𝑟𝑡 (·)).
We conjecture that using different interval constraint propagation
algorithms or clever simplifications of the constraint may improve
the performance of qCoral for this problem. Both variations of
SYMPAIS achieve an order of magnitude lower RAE.

Figure 8: RAE comparison for torus.

Correlated inputs. Consider the correlated distribution

𝑝 (𝑥, 𝑦, 𝑧) = T2 (0, 0.5)N (𝑥, 0.5)N (𝑥, 0.5),

where 𝑇2 denotes a Student’s T distribution with 2 degrees of free-
dom. Similarly to the situation illustrated in Figure 5, the distribu-
tion constrained within the solution space of torus is bi-modal. In
this case, the input distribution is correlated, with 𝑦 and 𝑧 proba-
bilistically dependent on 𝑥.

Correlated and potentially multimodal input distributions are
commonly used to describe real-world inputs arising from physical
phenomena. More recently, the success of deep learning has encour-
aged incorporating deep neural networks for generative modeling
of high-dimensional data distributions [29, 43, 44], trained from
observed data. For these distributions, the PDFs are often tractable,
while CDFs often are not. This in turn makes the stratified sampling
and truncations of the input distribution intractable for qCoral.

On the other hand, SYMPAIS can handle these problems because
it only requires evaluating the PDF of the input distribution, not
its CDF. Since qCoral cannot handle correlated inputs, Figure 8
(right) shows how both SYMPAIS and SYMPAIS-H outperforms
the baseline DMC by nearly one order of magnitude. As discussed
in Section 3.2, seeding the MCMC processes with re-sampled di-
verse solutions from the constraint solver improves SYMPAIS(-H)’s
convergence for multimodal distributions as in this experiment.

345678910d10−410−310−210−1RAEDMCqCoralSYMPAISSYMPAIS-H0.00.51.0no.ofsamples(×106)10−410−310−210−1DMCqCoralSYMPAISSYMPAIS-H0.000.050.10RAEIndependentDMCSYMPAISSYMPAIS-H0.000.050.10CorrelatedTorusYicheng Luo, Antonio Filieri, and Yuan Zhou

qCoral as well. However, neural networks tend to generate high-
dimensional problems because they establish control dependencies
among all their inputs, which prevents effective constraint slicing.
Figure 9 reports the RAE achieved by the different estimation
methods for each of the six randomly sampled activation patterns.
For this experiment, we used a single initial solution computed by
Z3 to initialize the MCMC chains of SYMPAIS and SYMPAIS-H. Be-
ing the conjunction of ReLU activations, the constraints produced
by ACAS Xu are convex (intersection of half-planes) and do not
induce multiple modes on the constrained input distribution. Al-
ready using a single initial solution, SYMPAIS converged to better
estimates than DMC and qCoral with the same sampling budget.

4.3 volComp
Finally, in this section we experiment with a set of constraints from
the benchmark volComp [45], also used to evaluate qCoral [5, 6].
We picked the first five path conditions for each of the subjects
named in Figure 10 from the public qCoral replication package.
Because most of the input variables in these subjects are compu-
tationally independent, constraint slicing would reduce to con-
straints with dimensionality < 3; we instead skip slicing and evalu-
ate the original constraints having between 5 and 25 variables. The
constraints are linear, with convex solution spaces. In this situa-
tion, RealPaver can produce a tight approximation of the solution
space, with significant benefits for qCoral’s stratified sampling ef-
ficiency. The input CDF can be computed analytically (independent
truncated Gaussian from qCoral’s replication package “normal”).
Among the different experiments in [6], these subjects represent a
sweet spot for the stratified sampling method of qCoral and are
included here as SYMPAIS’s worst-case comparison scenario.

Our current implementation of the HMC kernel proposal does
not support JIT-compilable truncated distributions. Thus we run
only SYMPAIS with RWMH for this set of subjects.

Figure 10 shows the RAE of the different methods. The ground
truth is computed with Mathematica. For all the subjects in this
experiment, except cart-12, 99% of the input domain enclosed
within RealPaver’s boxes contains only solutions of the constraint.
For all the subjects, except framingham-0, qCoral and SYMPAIS
produce comparable RAE. A deeper inspection of framingham-0
showed that most of the constraint is effectively the intersection of
boxes, which are identified as inner boxes by RealPaver and require
no further sampling for probability estimation (Equation (3)). The
experiment demonstrates that while the adaptive importance sam-
pling strategy of SYMPAIS is designed to estimate the satisfaction
probability of high-dimensional constraints with multimodal, corre-
lated input distributions, it can match the performance of stratified
sampling for most simpler problems where stratified sampling can
be applied. The analysis of the same constraints with correlated
inputs would instead not be possible with stratified sampling.

5 RELATED WORK
Probabilistic symbolic execution [20] relies on symbolic execution
to extract the path conditions characterizing the inputs that lead to
the occurrence of a target event; the probability of satisfying the
path condition constraints given an input distribution is then quanti-
fied using model counting or solution space quantification methods.

Figure 9: RAE comparison for ACAS Xu activation patterns.

4.2 ACAS Xu
ACAS Xu [25] is a benchmark neural network implementing a
safety-critical collision avoidance system for unmanned aircraft
control. Its inputs are readings from a set of sensors, including:
distance from the other vehicle, angle of the other vehicles rela-
tive to ownship direction, heading angle of other vehicle, speed
of ownship, and speed of the other vehicle. The outputs of the
networks are either clear-of-conflict – no risk of collision between
ownship and the other vehicle – or one of four possible collision
avoidance maneuvers the ownship can take to avoid a collision.
The US Federal Aviation Administration is experimenting with an
implementation of ACAS Xu to evaluate its safety for replacing the
current rule-based system [13].

This subject is has been used to benchmark several verification
methods (e.g. [26]), including performing a probabilistic robustness
analysis that computes bounds on the probability of the network
producing inconsistent decisions for small perturbations on the
inputs [13]. A central component of the analysis method in [13]
consists of computing reliable bounds for the probability of satisfy-
ing a constraint that corresponds to a unique activation pattern of
the network when white noise is added to an initial sensor reading.
For this experiment, we extract the constraints corresponding
to six random activation patterns and estimate their satisfaction
probability with different methods.

Consider a neural network with one hidden layer of 𝑚 neurons
that receives input 𝒙 ∈ R𝑑 . The neural network computes the
output as

𝑇
1 a + b1,
z = W0𝒙 + b0, a = ReLU(z), y = W

where ReLU is the Rectified Linear Unit defined as ReLU(𝒙) =
max(0, 𝒙) evaluated component-wise on 𝒙. W0, W1, b0 and b1 are
the pre-trained weights of the neural network. A hidden unit a𝑖 is
active if the constraint z𝑖 ≥ 0 is satisfied and inactive otherwise
(i.e., z𝑖 < 0). An activation pattern is defined as the conjunction
of the activation constraints of the hidden units {a𝑖 }𝑚
𝑖=1. We select
the network with one hidden layer of five neurons for analysis
(https://bit.ly/3fjAlOW). The selected network generates 32 pos-
sible combinations of activation patterns and we select randomly
six activation patterns for analysis. We use N (0, 1) to model the
distribution of each input dimension 𝑥𝑖 of the neural network and
additionally impose a domain of [−100, 100] ∩R for each dimension.
The bounded input and independent constraints allow the use of

012345PathConstraintID10−310−210−1RAEACASXuDMCqCoralSYMPAISSYMPAIS-HSymbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

Figure 10: RAE achieved by the different methods on volComp subjects.

supporting integer input variables that cannot be analyzed with
our current algorithms.

ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for useful com-
ments and feedback. Yicheng Luo is supported by UKRI AI Cen-
tre for Doctoral Training in Foundational Artificial Intelligence
(EP/S021566/1) and the UCL Overseas Research Scholarship (UCL-
ORS). Part of the experiments has been enabled by an equipment
grant from the UK National Cyber Security Centre (NCSC) awarded
to Antonio Filieri. Yuan Zhou is partially supported by the National
Natural Science Foundation of China (NSFC).

PSE has been applied in several domains, including reliability [17]
security [8, 33] and performance analyses [12], with variations
implemented for nondeterministic [30] and probabilistic [33] pro-
grams. Quantification methods have been proposed for uniform or
discretized distributions over linear integer constraints [17], string
constraints [3], bounded data structured [16], and numerical con-
straints over continuous input distributions [5, 6].

In the probabilistic programming literature, Chaganty et al. [11]
proposed breaking a probabilistic program with branches and loops
into small programs focused on only some execution paths and
use pre-image analysis to perform efficient importance sampling.
Differently from Chaganty et al. [11], we use PI-MAIS and MCMC
processes to further adapt the proposal distributions for the anal-
ysis of individual path conditions. Nori et al. [38] similarly uses
the idea of pre-image analysis to design a proposal distribution
that generates samples that are less likely to be rejected in MCMC.
These analyses complement our approach and can potentially be
incorporated to improve our MCMC scheme. Recent work by Zhou
et al. [48] motivates the decomposition into subproblems by con-
sidering universal probabilistic programs with stochastic support,
i.e., depending on the values of the samples, the program may
take on different control-flow paths, and the number of random
variables evaluated along each path varies as a result. This makes
designing a proposal distribution for efficient MCMC difficult. Zhou
et al. [48] approaches this issue by decomposing the problem into
small straight-line programs (SLPs) for which the support is fixed
and posterior inference is tractable. However, differently from PSE
approaches, SLPs are execution paths sampled via a specialized
MCMC algorithm, which adds an additional degree of uncertainty
to the results of probabilistic analysis and is not suitable for the
analysis of rare events.

6 CONCLUSIONS AND FUTURE WORK
We introduced SYMPAIS, a new inference method for estimat-
ing the satisfaction probability of numerical constraints on high-
dimensional, correlated input distributions. SYMPAIS combines
a sample-efficient importance sampling scheme with constraint
solvers to extend the applicability of probabilistic symbolic execu-
tion to a broader class of programs processing correlated inputs
that cannot be analyzed with existing methods. While we currently
implemented only RWMH and HMC kernel as adaptive proposals,
SYMPAIS can be extended with additional kernels to improve its
performance on different classes of constraints. Finally, it is also
worth investigating the integration of kernels and parametric im-
portance sampling proposals for discrete distributions, aiming at

0123410−510−310−1RAEcart-1201234carton-5-001234PathConstraintIDckd-epi-001234ckd-epi-simple-001234framingham-0DMCqCoralSYMPAISAPPENDIX
A EXPERIMENTAL SETTINGS DETAILS
Implementation. We implemented SYMPAIS in Python 3.8 using
Google’s JAX framework [7] to implement the mathematical pro-
cedures and generate random samples. JAX provides automated
differentiation, which we use for HMC, and allows for just-in-time
compile the numerical routines of SYMPAIS for faster execution,
mainly due to the use of its automatic vectorization features.
Execution environment. All the experiments have been run on
an AMD EPYC 7401P CPU with 448Gb of memory. However, SYM-
PAIS was limited to using only two cores and never exceeded 4Gb
of memory consumption in our experiments.
Configuration. The RWMH kernel is configured with scale 𝜎 =
1.0. The HMC trajectory is simulated for 𝑇 = 20 steps, with each
time step duration 𝜖 = 0.1; we use standard leapfrog steps to gen-
erate the Hamiltonian trajectory [36] (mathematical formulation
in the Appendix). We use 𝑁 = 100 parallel MCMC chains. Each
chain is warmed up with 500 samples – i.e., 500 samples are used to
bring the chain near convergence and discarded; only the samples
from the 501 − 𝑡ℎ on are actually used. Overall, the 100 chains
take 50, 000 samples to warm up. For the RWMH kernel, we also
implement adaptation of the scale parameter following in scheme
used by PyMC3 (https://bit.ly/2HrpHJK).

For the adaptive importance sampling distribution 𝑞𝑛,𝑡 (·), we
use N (·|𝒙𝑛, 𝑡, 0.5Λ) where Λ is the covariance of the input distri-
bution 𝑝 (𝒙). For the input distributions where the covariance of
𝑝 (𝒙) is analytically intractable, SYMPAIS uses the sample variance
(estimated from 100 samples). We draw 𝑀 = 5 samples from each
sub-proposal 𝑞𝑛,𝑡 during each iteration of the adaptation loop. Real-
Paver is configured with accuracy 0.1 to generate the initial inputs
of the MCMC chains and the truncated kernel proposal bounds
(min/max over RealPaver’s boxes vertices). RealPaver for paving in
qCoral computes up to 1024 boxes.
Experimental settings. We allow each of the four methods the
same budget of 106 samples. For SYMPAIS, this includes the samples
used to warm up the MCMC chains and to estimate the covariance
of the input distribution when not given. RealPaver executions
for qCoral and SYMPAIS are not time-limited. However, all the
experiments completed their execution within 2 minutes for the
geometrical microbenchmarks and 5 minutes for the other subjects.
The compilation time of JAX varies between ˜20 seconds and ˜25
minutes; reimplementing SYMPAIS with a compiled language might
make the compilation time more predictable.

Each experiment is repeated 20 times unless otherwise speci-
fied. To compare the estimates of the different methods, we use
the relative absolute error (RAE) against a ground truth value com-
puted with Mathematica (accuracy and precision goals = 15) or
via a large direct Monte Carlo samples (20 · 108) for the geometri-
cal microbenchmarks. The RAE is preferred to the absolute error
because normalizing by the ground truth probability allows a ho-
mogenous comparison over constraints with different satisfaction
probabilities.

Yicheng Luo, Antonio Filieri, and Yuan Zhou

B HAMILTONIAN MONTE CARLO

ESTIMATORS

The Hamiltonian Markov Chain Monte Carlo (HMC) algorithm [4,
36] uses derivatives of the target distribution and auxiliary variables
to generate candidate samples more efficiently than RWMH when
𝑝 (𝒙) is differentiable.

HMC generates the next sample 𝒙 ′ by simulating the movement
of a particle pushed in a random direction starting from the location
𝒙 across the surface defined by the probability density function
of 𝑝 (𝒙). Intuitively, regions where 𝑝 (𝒙) is larger are at a “lower
altitude” on the surface and are thus more likely to attract the
particle. The random direction and energy of the push can allow
the particle to reach any point of the surface. The position of the
particle after a predefined fixed time 𝑇 will be the new candidate
sample 𝒙 ′. A mathematical formulation of the HMC process im-
plemented in SYMPAIS is reported in the Appendix, while for a
broader presentation of the method, the reader is referred to [4].

To sample from the target distrbution of 𝒙, HMC augments the
original probability distribution 𝑝 (𝒙) with auxiliary momentum
variables 𝝆 and considers the joint density 𝑝 (𝒙, 𝝆) = 𝑝 (𝝆)𝑝 (𝒙)
where 𝑝 (𝝆 | 𝒙) can be chosen as N (0, I), I being the identity matrix
with size |𝒙 |. Define the Hamiltonian 𝐻 as

𝐻 (𝒙, 𝝆) = − log 𝑝 (𝒙, 𝝆)

= − log 𝑝 (𝝆) − log 𝑝 (𝒙)
= 𝐾 (𝝆) + 𝑈 (𝒙)
where 𝐾 (𝝆) = − log 𝑝 (𝝆) is called the kinetic energy and 𝑈 (𝒙) =
− log 𝑝 (𝒙) is called the potential energy.

A proposal 𝒙 ′ is made by simulating the Hamiltonian dynamics
𝑑𝝆
𝑑𝑡

= −∇𝒙𝑈 (𝒙),

= ∇𝝆𝐾 (𝝆),

𝑑𝒙
𝑑𝑡

which can be discretized and solved numerically by a symplectic
integrator such as the Leapfrog method for 𝑇leapfrog steps with a
step size of 𝜖

𝝆𝑡 +𝜖/2 = 𝝆𝑡 − (𝜖/2)∇𝒙𝑈 (𝒙𝑡 ),
𝒙𝑡 +𝜖 = 𝒙𝑡 + 𝜖∇𝝆𝐾 (𝝆𝑡 +𝜖/2),
𝝆𝑡 +𝜖 = 𝝆𝑡 +𝜖/2 − (𝜖/2)∇𝒙𝑈 (𝒙𝑡 +𝜖 )

for 𝑡 ∈ {1, . . . ,𝑇leapfrog}.

Choice of hyperparameters parameters such as the step size 𝜖
and the number of Leapfrog steps 𝑇leapfrog affects the efficiency of
HMC methods. The No-U-Turn Sampler [24] is an extension to the
HMC method which mitigates the effect of hyperparameter settings
on HMC methods.

While potentially more efficient than RWMH, especially for
higher-dimensional problems, – i.e., fewer candidate samples are
rejected – HMC requires the target density function to be differ-
entiable, which makes it less general than RWMH. In particular, it
cannot be applied for input distribution with discrete parameters.
Improvements to the original HMC algorithm and extensions exist
to handle this limitation [37]. Because SYMPAIS’s target distribu-
tion is ¯𝑝 (𝒙) = 𝑝 (𝒙)1𝐶 , instead of the original input distribution
𝑝 (𝒙), the probability density is not differentiable at the boundary of
the solution space of the constraint 𝐶, and simulations crossing that
boundary have to be rejected. Extensions to HMC specialized for

Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis

sampling from constrained spaces have been proposed for several
classes of constraints and will be investigated for future extensions
of SYMPAIS [1, 4, 36, 37].

C ADDITIONAL DISCUSSION
Handling broader families of 𝑝 (𝒙). Since SYMPAIS does not re-
quire CDF of 𝑝 (𝒙) to be tractable, we only require that the PDF can
be evaluated. This allows us to handle a broader family of distribu-
tions compared to qCoral. This is a trade-off between efficiency and
generality. By assuming that the truncated CDF can be evaluated.
qCoral can perform additional pruning that further eliminates prob-
ability masses where 1𝑃𝐶 (𝒙) = 0. While stratified sampling can
be used to reduce variance [27], most of the variance reduction in
qCoral comes from pruning out irrelevant solution space instead of
stratification. SYMPAIS, on the other hand, relaxes the assumption
of having tractable truncated CDFs but instead reduces variance
with IS. The interval contraction optimization allows us to reduce
variance compared to AIS in the original space, but this can still
produce a larger truncated proposal compared to qCoral. However,
IS may provide additional variance reduction to compensate for
the inefficiency in the stratified sampler in qCoral. In this case,
SYMPAIS complements qCoral in situations where the stratifica-
tion strategy is inefficient. When the truncated CDF is available,
which method works better is likely problem-specific. In this case,
the most practical solution is probably a combination of both. One
can first do a preliminary run with a smaller number of samples
to identify which method would work better and exploit the more
efficient method.
Future improvements Improvements to the MCMC algorithm
can improve SYMPAIS’s efficiency. In particular, it would benefit
from improvements in MCMC that can explore multimodal poste-
rior efficiently. Note that, however, the MCMC does not have to
be perfect. For the PI-MAIS adaptation scheme, the crucial thing is
that the parallel chains should capture the modes in 𝑞∗ (𝒙) in a way
the modes are relatively weighted in 𝑞∗ (𝒙). This may explain why
we do not observe improvements in performance when using the
HMC kernel. Importance sampling can be dangerous if the adapted
𝑞 misses the modes of 𝑞∗; this is partly mitigated in SYMPAIS, which
uses an adaptive mixture proposal that can approximate multimodal
posteriors. Additionally, we plan to further improve the robustness
of SYMPAIS against multimodal distributions adding supplemen-
tary diagnostics (e.g. Effective sample sizes (ESS)) and defensive
importance sampling [23, 39].

REFERENCES
[1] Hadi Mohasel Afshar and Justin Domke. 2015. Reflection, Refraction, and Hamil-
tonian Monte Carlo. In Proceedings of the 28th International Conference on Neural
Information Processing Systems - Volume 2 (NIPS’15). MIT Press, 3007–3015.
[2] Ignacio Araya, Gilles Trombettoni, and Bertrand Neveu. 2012. A Contractor Based
on Convex Interval Taylor. In Integration of AI and OR Techniques in Contraint
Programming for Combinatorial Optimzation Problems (Lecture Notes in Computer
Science), Nicolas Beldiceanu, Narendra Jussien, and Éric Pinson (Eds.). Springer,
1–16. https://doi.org/10.1007/978-3-642-29828-8_1

[3] Abdulbaki Aydin, William Eiers, Lucas Bang, Tegan Brennan, Miroslav Gavrilov,
Tevfik Bultan, and Fang Yu. 2018. Parameterized Model Counting for String
and Numeric Constraints. In Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. ACM, 400–410. https://doi.org/10.1145/3236024.3236064
[4] Michael Betancourt. 2018. A Conceptual Introduction to Hamiltonian Monte

Carlo. arXiv:1701.02434 [stat] (July 2018). arXiv:1701.02434 [stat]

[5] Mateus Borges, Antonio Filieri, Marcelo D’Amorim, and Corina S. Păsăreanu. 2015.
Iterative Distribution-Aware Sampling for Probabilistic Symbolic Execution. In
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering.
ACM, 866–877. https://doi.org/10.1145/2786805.2786832

[6] Mateus Borges, Antonio Filieri, Marcelo d’Amorim, Corina S. Păsăreanu, and
Willem Visser. 2014. Compositional Solution Space Quantification for Prob-
abilistic Software Analysis. In Proceedings of the 35th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation. ACM, 123–132.
https://doi.org/10.1145/2594291.2594329

[7] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris
Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. 2018. JAX: Composable Transformations of
Python+NumPy Programs. https://github.com/google/jax

[8] Tegan Brennan, Seemanta Saha, Tevfik Bultan, and Corina S. Păsăreanu. 2018.
Symbolic Path Cost Analysis for Side-Channel Detection. In Proceedings of the
27th ACM SIGSOFT International Symposium on Software Testing and Analysis.
ACM, 27–37. https://doi.org/10.1145/3213846.3213867

[9] Benno Büeler, Andreas Enge, and Komei Fukuda. 2000. Exact Volume Com-
putation for Polytopes: A Practical Study. In Polytopes — Combinatorics and
Computation, Gil Kalai and Günter M. Ziegler (Eds.). Birkhäuser Basel, 131–154.
https://doi.org/10.1007/978-3-0348-8438-9_6

[10] Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
In Proceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDI’08). USENIX Association, 209–224.

[11] Arun Chaganty, Aditya Nori, and Sriram Rajamani. 2013. Efficiently Sampling
Probabilistic Programs via Program Analysis. In Proceedings of the Sixteenth
International Conference on Artificial Intelligence and Statistics (Proceedings of
Machine Learning Research, Vol. 31), Carlos M. Carvalho and Pradeep Ravikumar
(Eds.). PMLR, 153–160.

[12] Bihuan Chen, Yang Liu, and Wei Le. 2016. Generating Performance Distributions
via Probabilistic Symbolic Execution. In Proceedings of the 38th International Con-
ference on Software Engineering. ACM, 49–60. https://doi.org/10.1145/2884781.
2884794

[13] Hayes Converse, Antonio Filieri, Divya Gopinath, and Corina S. Pasareanu.
2020. Probabilistic Symbolic Analysis of Neural Networks. In 2020 IEEE 31st
International Symposium on Software Reliability Engineering (ISSRE). IEEE, 148–
159. https://doi.org/10.1109/ISSRE5003.2020.00023

[14] Leonardo de Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver. In
Tools and Algorithms for the Construction and Analysis of Systems (Lecture Notes in
Computer Science), C. R. Ramakrishnan and Jakob Rehof (Eds.). Springer, 337–340.
https://doi.org/10.1007/978-3-540-78800-3_24

[15] Matthew B. Dwyer, Antonio Filieri, Jaco Geldenhuys, Mitchell Gerrard, Corina S.
Păsăreanu, and Willem Visser. 2017. Probabilistic Program Analysis. In Grand
Timely Topics in Software Engineering, Jácome Cunha, João P. Fernandes, Ralf Läm-
mel, João Saraiva, and Vadim Zaytsev (Eds.). Vol. 10223. Springer International
Publishing, 1–25. https://doi.org/10.1007/978-3-319-60074-1_1

[16] Antonio Filieri, Marcelo F. Frias, Corina S. Păsăreanu, and Willem Visser. 2015.
Model Counting for Complex Data Structures. In Model Checking Software (Lec-
ture Notes in Computer Science), Bernd Fischer and Jaco Geldenhuys (Eds.).
Springer International Publishing, 222–241. https://doi.org/10.1007/978-3-319-
23404-5_15

[17] Antonio Filieri, Corina S. Pasareanu, and Willem Visser. 2013. Reliability Anal-
ysis in Symbolic PathFinder. In 2013 35th International Conference on Software
Engineering (ICSE). IEEE, 622–631. https://doi.org/10.1109/ICSE.2013.6606608

[18] Antonio Filieri, Corina S. Păsăreanu, Willem Visser, and Jaco Geldenhuys. 2014.
Statistical Symbolic Execution with Informed Sampling. In Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
ACM, 437–448. https://doi.org/10.1145/2635868.2635899

[19] Timon Gehr, Sasa Misailovic, and Martin Vechev. 2016. PSI: Exact Symbolic
Inference for Probabilistic Programs. In Computer Aided Verification, Swarat

Yicheng Luo, Antonio Filieri, and Yuan Zhou

[43] Danilo Rezende and Shakir Mohamed. 2015. Variational Inference with Nor-
malizing Flows. In Proceedings of the 32nd International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 37), Francis Bach and
David Blei (Eds.). PMLR, 1530–1538.

[44] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic
Backpropagation and Approximate Inference in Deep Generative Models. In
Proceedings of the 31st International Conference on Machine Learning (Proceedings
of Machine Learning Research, Vol. 32), Eric P. Xing and Tony Jebara (Eds.). PMLR,
1278–1286.

[45] Sriram Sankaranarayanan, Aleksandar Chakarov, and Sumit Gulwani. 2013. Static
Analysis for Probabilistic Programs: Inferring Whole Program Properties from
Finitely Many Paths. In Proceedings of the 34th ACM SIGPLAN Conference on
Programming Language Design and Implementation - PLDI ’13. ACM Press, 447.
https://doi.org/10.1145/2491956.2462179

[46] John G. Saw, Mark C.K. Yang, and Tse Chin Mo. 1984. Chebyshev Inequality
with Estimated Mean and Variance. The American Statistician 38, 2 (May 1984),
130–132. https://doi.org/10.1080/00031305.1984.10483182
[47] Jun Shao, S Fienberg, and I Olkin. 2008. Mathematical Statistics.
[48] Yuan Zhou, Hongseok Yang, Yee Whye Teh, and Tom Rainforth. 2020. Divide,
Conquer, and Combine: A New Inference Strategy for Probabilistic Programs
with Stochastic Support. In Proceedings of the 37th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé
III and Aarti Singh (Eds.). PMLR, 11534–11545.

Chaudhuri and Azadeh Farzan (Eds.). Vol. 9779. Springer International Publishing,
62–83. https://doi.org/10.1007/978-3-319-41528-4_4

[20] Jaco Geldenhuys, Matthew B. Dwyer, and Willem Visser. 2012. Probabilistic
Symbolic Execution. In Proceedings of the 2012 International Symposium on Soft-
ware Testing and Analysis - ISSTA 2012. ACM Press, 166. https://doi.org/10.1145/
2338965.2336773

[21] Alexandre Goldsztejn and Gilles Chabert. 2021. ibex-lib. Retrieved Jun 4, 2021

from http://www.ibex-lib.org

[22] Laurent Granvilliers and Frédéric Benhamou. 2006. Algorithm 852: RealPaver:
An Interval Solver Using Constraint Satisfaction Techniques. ACM Trans. Math.
Software 32, 1 (March 2006), 138–156. https://doi.org/10.1145/1132973.1132980
[23] Tim Hesterberg. 1995. Weighted Average Importance Sampling and Defensive
Mixture Distributions. Technometrics 37, 2 (1995), 185–194. https://doi.org/10.
2307/1269620

[24] Matthew D. Hoffman and Andrew Gelman. 2014. The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of
Machine Learning Research 15, 1 (Jan. 2014), 1593–1623.

[25] Kyle D. Julian, Jessica Lopez, Jeffrey S. Brush, Michael P. Owen, and Mykel J.
Kochenderfer. 2016. Policy Compression for Aircraft Collision Avoidance Systems.
In 2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC). IEEE, 1–10.
https://doi.org/10.1109/DASC.2016.7778091

[26] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer.
2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In
Computer Aided Verification (Lecture Notes in Computer Science), Rupak Majumdar
and Viktor Kunčak (Eds.). Springer International Publishing, 97–117. https:
//doi.org/10.1007/978-3-319-63387-9_5

[27] M. Keramat and R. Kielbasa. 1998. A Study of Stratified Sampling in Variance
Reduction Techniques for Parametric Yield Estimation. IEEE Transactions on
Circuits and Systems II: Analog and Digital Signal Processing 45, 5 (May 1998),
575–583. https://doi.org/10.1109/82.673639

[28] James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM

19, 7 (July 1976), 385–394. https://doi.org/10.1145/360248.360252

[29] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes..
In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings.

[30] Kasper Luckow, Corina S. Păsăreanu, Matthew B. Dwyer, Antonio Filieri, and
Willem Visser. 2014. Exact and Approximate Probabilistic Symbolic Execution
for Nondeterministic Programs. In Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering. ACM, 575–586. https://doi.org/
10.1145/2642937.2643011

[31] Yicheng Luo and Antonio Filieri. 2021. SYMPAIS Implementation. https://github.

com/ethanluoyc/sympais.

[32] Yicheng Luo, Antonio Filieri, and Yuan Zhou. 2021.

Symbolic Parallel
(2021).

Adaptive Importance Sampling for Probabilistic Program Analysis.
arXiv:2010.05050 [cs.LG]

[33] Pasquale Malacaria, Mhr Khouzani, Corina S. Pasareanu, Quoc-Sang Phan, and
Kasper Luckow. 2018. Symbolic Side-Channel Analysis for Probabilistic Programs.
In 2018 IEEE 31st Computer Security Foundations Symposium (CSF). IEEE, 313–327.
https://doi.org/10.1109/CSF.2018.00030

[34] L. Martino, V. Elvira, D. Luengo, and J. Corander. 2017. Layered Adaptive
Importance Sampling. Statistics and Computing 27, 3 (May 2017), 599–623.
https://doi.org/10.1007/s11222-016-9642-5

[35] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H.
Teller, and Edward Teller. 1953. Equation of State Calculations by Fast Computing
Machines. The Journal of Chemical Physics 21, 6 (June 1953), 1087–1092. https:
//doi.org/10.1063/1.1699114

[36] Radford Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of Markov
Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng
(Eds.). Vol. 20116022. Chapman and Hall/CRC. https://doi.org/10.1201/b10905-6
[37] Akihiko Nishimura, David B Dunson, and Jianfeng Lu. 2020. Discontinuous
Hamiltonian Monte Carlo for Discrete Parameters and Discontinuous Likelihoods.
Biometrika 107, 2 (June 2020), 365–380. https://doi.org/10.1093/biomet/asz083

[38] Aditya V. Nori, Chung-Kil Hur, Sriram K. Rajamani, and Selva Samuel. 2014. R2:
An Efficient MCMC Sampler for Probabilistic Programs. In Proceedings of the
Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI’14). AAAI Press,
2476–2482.

[39] Art B. Owen. 2013. Monte Carlo Theory, Methods and Examples. https://statweb.

stanford.edu/~owen/mc/

[40] Corina S. Păsăreanu and Neha Rungta. 2010. Symbolic PathFinder: Symbolic
Execution of Java Bytecode. In Proceedings of the IEEE/ACM International Con-
ference on Automated Software Engineering - ASE ’10. ACM Press, 179. https:
//doi.org/10.1145/1858996.1859035

[41] Quoc-Sang Phan, Lucas Bang, Corina S. Pasareanu, Pasquale Malacaria, and
Tevfik Bultan. 2017. Synthesis of Adaptive Side-Channel Attacks. In 2017 IEEE
30th Computer Security Foundations Symposium (CSF). IEEE, 328–342. https:
//doi.org/10.1109/CSF.2017.8

[42] Alfio Quarteroni, Riccardo Sacco, and Fausto Saleri. 2007. Numerical Mathematics

(second ed.). Springer-Verlag. https://doi.org/10.1007/b98885

