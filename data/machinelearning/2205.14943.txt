2
2
0
2

l
u
J

7

]
L
P
.
s
c
[

3
v
3
4
9
4
1
.
5
0
2
2
:
v
i
X
r
a

Data-driven Numerical Invariant Synthesis with
Automatic Generation of Attributes⋆

Ahmed Bouajjani1, Wael-Amine Boutglay1,2, and Peter Habermehl1

1 Universit´e Paris Cit´e, IRIF, Paris, France
{abou,boutglay,haberm}@irif.fr
2 Mohammed VI Polytechnic University, Ben Guerir, Morocco

Abstract. We propose a data-driven algorithm for numerical invariant
synthesis and veriﬁcation. The algorithm is based on the ICE-DT schema
for learning decision trees from samples of positive and negative states
and implications corresponding to program transitions. The main issue
we address is the discovery of relevant attributes to be used in the learn-
ing process of numerical invariants. We deﬁne a method for solving this
problem guided by the data sample. It is based on the construction of a
separator that covers positive states and excludes negative ones, consis-
tent with the implications. The separator is constructed using an abstract
domain representation of convex sets. The generalization mechanism of
the decision tree learning from the constraints of the separator allows the
inference of general invariants, accurate enough for proving the targeted
property. We implemented our algorithm and showed its eﬃciency.

Keywords: Invariant synthesis, Data-driven program veriﬁcation.

1

Introduction

Invariant synthesis for program safety veriﬁcation is a highly challenging prob-
lem. Many approaches exist for tackling this problem, including abstract inter-
pretation, CEGAR-based symbolic reachability, property-directed reachability
(PDR), etc. [7,13,4,5,2,9,16,18]. While those approaches are applicable to large
classes of programs, they may have scalability limitations and fail to infer cer-
tain types of invariants, such as disjunctive invariants. Emerging data-driven
approaches, following the active learning paradigm with various machine learn-
ing techniques, have shown their ability to solve eﬃciently complex instances
of the invariant synthesis problem [14,15,11,29,19,30,25]. These approaches are
based on the iterative interaction between a learner inferring candidate invari-
ants from a data sample, i.e., a set of data classiﬁed either as positive examples,
known to be reachable from the initial states and that therefore must be included
in any solution, or negative examples, known to be predecessors of states violat-
ing the safety property and that therefore cannot be included in any solution,
and a teacher checking the validity of the proposed solutions and providing coun-
terexamples as feedback in case of non-validity. One such data-driven approach

⋆ This work was supported in part by the french ANR project AdeCoDS.

 
 
 
 
 
 
2

A. Bouajjani et al.

is ICE [14] which has shown promising results with its instantiation ICE-DT [15]
that uses decision trees for the learning component. ICE is a learning approach
tailored for invariant synthesis, where the feedback provided by the teacher can
be, in addition to positive and negative examples, implications of the form p → q
expressing the fact that if p is in a solution, then necessarily q should also be
included in the solution since there is a transition in the program from p to q.

The strength of data-driven approaches is the generalization mechanisms of
their learning components, allowing them to ﬁnd relevant abstractions from a
number of examples without exploring the whole state space of the program. In
the case of ICE-DT, this is done by a sophisticated construction of decision trees
classifying correctly the known positive and negative examples at some point,
and taking into account the information provided by the implications. These
decision trees, where the tested attributes are predicates on the variables of the
program, are interpreted as formulas corresponding to candidate invariants.

However, to apply data-driven methods such as ICE-DT, one needs to have
a pool of attributes that are potentially relevant for the construction of the
invariant. This is actually a crucial issue. In ICE-DT, as well as in most data-
driven methods, ﬁnding the predicates involved in the invariant construction
is based on systematic enumeration of formulas according to some pre-deﬁned
templates or grammars. For instance, in the case of numerical programs, the
considered patterns are some special types of linear constraints, and candidate
attributes are generated by enumerating all possible values for the coeﬃcients
under some ﬁxed bound. While such a brute-force enumeration can be eﬀective
in many cases, it represents, in general, an obstacle for both scalability and
ﬁnding suﬃciently accurate inductive invariants in complex cases.

In this paper, we provide an algorithmic method for eﬃcient generation of
attributes for data-driven invariant synthesis for numerical programs manipulat-
ing integer variables. While enumerative approaches are purely syntactic and do
not take into account the data sample, our method is guided by it. We show that
this method, when integrated in the ICE-DT schema, leads to a new invariant
synthesis algorithm outperforming state-of-the-art methods and tools.

Our method for attributes discovery is based on, given an ICE data sample,
computing a separator of it as a union of convex sets i.e., (1) it covers all the
positive examples, (2) it does not contain any negative example, and (3) it is
consistent with the implications (for every p → q in the sample, if the separator
contains p, then it should also contain q). Then, the set of attributes generated is
the set of all constraints deﬁning the separator. However, as for a given sample
there might be several possible separators, a question is which separators to
consider. Our approach is guided by two requirements: (1) we need to avoid big
pools of attributes in order to reduce the complexity of the invariant construction
process, and (2) we need to avoid having in the pool constraints that are (visibly)
unnecessary, e.g. separating positive examples in a region without any negative
ones. Therefore, we consider separators that satisfy the property that, whenever
they contain two convex sets, it is impossible to take their convex union (smallest
convex set containing the union) without including a negative example.

Data-driven Numerical Invariant Synthesis

3

To represent and manipulate algorithmically convex sets, we consider ab-
stract domains, e.g., intervals, octagons, and polyhedra, as they are deﬁned in
the abstract interpretation framework and implemented in tools such as APRON
[17]. These domains correspond to particular classes of convex sets, deﬁned by
speciﬁc types of linear constraints. In these domains, the union operation is
naturally over-approximated by the join operation that computes the best over-
approximation of the union in the considered class of convex sets. Then, con-
structing separators as explained above can be done by iterative application of
the join operation while it does not include negative examples.

Then, this method for generating candidate attributes can be integrated into
the ICE-DT schema: in each iteration of ICE loop, given a sample, the learner (1)
generates a set of candidate attributes from a separator of the sample, (2) builds
a decision tree from these attributes and proposes it as a candidate invariant
to the teacher. Then, the teacher (1) checks that the proposed solution is an
inductive invariant, and if it is not (2) provides a counterexample to the learner,
extending the sample that will be used in the next iteration.

Here a question might be asked: why do we need to construct a decision tree
from the constraints of the separator and do not propose directly the formula
deﬁning the separator as a candidate invariant to the teacher. The answer is
that the decision tree construction is crucial for generalization. Indeed, given a
sample, the constructed separator might be too specialized to that sample and
does not provide a useful inductive invariant (except for some simple cases). For
instance, the constructed separator is a union of bounded convex sets (polytopes),
while invariants are very often unbounded convex sets (polyhedra). The eﬀect
of using decision trees, in this case, is to select the relevant constraints and
discard the unnecessary bounds, leading very quickly to an unbounded solution
that is general enough to be an inductive invariant. Without this generalization
mechanisms, the ICE loop will not terminate in such (quite common) cases.

The integration of our method can be made tighter and more eﬃcient by
making the process of building separators incremental along the ICE iterations:
at each step, after the extension of the sample by the teacher, instead of con-
structing a separator of the new sample from scratch, the parts of previously
computed separators not aﬀected by the last extension of the sample are reused.
We have implemented our algorithm and carried out experiments on the
SyGuS-Comp’19 benchmarks. Our method solves signiﬁcantly more cases than
the tools LoopInvGen [25,24], CVC4 [1,26], and Spacer [18], as well as our im-
plementation of the original ICE-DT [15] algorithm (with template-based enu-
meration of attributes), with very competitive time performances.
Related work. Many learning-based approaches for the veriﬁcation of numer-
ical programs have been developed recently. One of the earliest approaches is
Daikon [10]. Given a pool of formulas, it computes likely invariants from program
executions. Later approaches were developed for the synthesis of sound invari-
ants, for example [29] iteratively generates a set of reachable and bad states and
classiﬁes them with a combination of half-spaces computed using SVM. In [28],
the problem is reformulated as learning geometric concepts in machine learning.

4

A. Bouajjani et al.

The ﬁrst instantiation of the ICE framework was based on a constraint solver
[14]. Later on, it was instantiated using the decision trees learning algorithm [15].
Both those instantiations require a ﬁxed template for the invariants or the for-
mulas appearing in them. LoopInvGen enumerates predicates on-demand using
the approach introduced in [25]. This is extended to a mechanism with hybrid
enumeration of several domains or grammars [24]. Continuous logic networks
were also used to tackle the problem in CLN2INV [27]. Code2Inv [30], the ﬁrst
approach to introduce general deep learning methods to program veriﬁcation,
uses a graph neural network to capture the program structure and reinforcement
learning to guide the search heuristic of a particular domain.

The learning approach of ICE and ICE-DT has been generalized to solve
problems given as constrained horn clauses (CHC) in Horn-ICE [11] and HoICE
[3]. Outside the ICE framework, [32] proposed a learning approach for solving
CHC using decision trees and SVM for the synthesis of candidate predicates from
a set of reachable and bad states of the program. The limitation of the non-ICE-
based approach is that when the invariant is not inductive, the program has to
be rerun, forward and backward, to generate more reachable and bad states.

In more theoretical work, an abstract learning framework for synthesis, in-
troduced in [20], incorporates the principle of CEGIS (counterexample-guided
inductive synthesis). A study of overﬁtting in invariant synthesis was conducted
in [24]. ICE was compared with IC3/PDR in terms of complexity in [12]. A
generalization of ICE with relative inductiveness [31] can implement IC3/PDR
following the paradigm of active learning with a learner and a teacher.

Automatic invariant synthesis and veriﬁcation has been addressed by many
other techniques based on exploring and computing various types of abstract rep-
resentations of reachable states (e.g., [7,13,4,5,2,9,16,18]). Notice that, although
we use abstract domains for representation and manipulation of convex sets, our
strategy for exploring the set of potential invariants is diﬀerent from the ones
used typically in abstract interpretation analysis algorithms [7].

2 Safety veriﬁcation using learning of invariants

This section presents the approach we use for solving the safety veriﬁcation prob-
lem. It is built upon the ICE framework [14] and in particular its instantiation
with the learning of decision trees [15]. We ﬁrst deﬁne the veriﬁcation problem.

2.1 Linear Constraints and Safety Veriﬁcation

Let X be a set of variables. Linear formulas over X are boolean combinations
n
of linear constraints of the form P
i=1 aixi ≤ b where the xi’s are variables in
X, the ai’s are integer constants, and b ∈ Z ∪ {+∞}. We use linear formulas to
reason symbolically about programs with integer variables. Assume we have a
program with a set of variables V and let n = |V |. A state of the program is a
vector of integers in Zn. Primed versions of these variables are used to encode
the transition relation T of the program: for each v ∈ V , we consider a variable

Data-driven Numerical Invariant Synthesis

5

v′ to represent the value of v after the transition. Let V ′ be the set of primed
variables, and consider linear formulas over V ∪ V ′ to deﬁne the relation T .

The safety veriﬁcation problem consists in, given a set of safe states Good ,
deciding whether, starting from a set of initial states Init, all the reachable states
by iterative application of T are in Good . Dually, this is equivalent to decide if
starting from Init, it is possible to reach a state in Bad which is the set of unsafe
states (the complement of Good ). Assuming that the sets Init and Good can be
deﬁned using linear formulas, the safety veriﬁcation problem amounts to ﬁnd an
adequate inductive invariant I, such that the three following formulas are valid:

Init (V ) ⇒ I(V )

I (V ) ⇒ Good (V )

I(V ) ∧ T (V, V ′) ⇒ I(V ′)

(1)
(2)

(3)

We are looking for inductive invariants which can be expressed as a linear
formula. In that case, the validity of the three formulas is decidable and can be
checked with a standard SMT solver.

2.2 The ICE learning framework

ICE [14] follows the active learning paradigm to learn adequate inductive invari-
ants of a given program and a given safety property. It consists of an iteratively
communicating learner and a teacher (see Algorithm 1).

Input : A transition system and a property: (Init, T, Good)
Output: An adequate invariant or error
1 initialize ICE-sample S = (S+, S−, S→);
2 while true do
3

J ← Learn(S);
(success, counterexample) ← is inductive(J);
if success then return J ;
else

S ← update(S, counterexample);
if contradictory(S) then return error;

Algorithm 1: The main loop of ICE.

4

5

6

7

8

In each iteration, in line 3, the learner, which does not know anything about
the program, synthesizes a candidate invariant (as a formula over the program
variables) from a sample S (containing information about program states) which
is enriched during the learning process. Contrary to other learning methods, the
sample S not only contains a set of positive states S+ which should satisfy the
invariant, and a set of negative states S− which should not satisfy the invariant,
but it contains also a set of implications S→ of the form s → s′ meaning that
if s satisﬁes the invariant, then s′ should satisfy it as well (because there is a

6

A. Bouajjani et al.

transition from s to s′ in the transition relation of the program). Therefore, an
ICE-sample S is a triple (S+, S−, S→), where to account for the information
contained in implications, it is imposed additionally that

∀s → s′ ∈ S→ :

if s ∈ S+, then s′ ∈ S+, and if s′ ∈ S−, then s ∈ S−

(4)

The sample is initially empty (or containing some states whose status, positive or
negative, is known). It is assumed that a candidate invariant J proposed by the
learner is consistent with the sample, i.e. states in S+ satisfy the invariant J, the
states in S− falsify it, and for implications s → s′ ∈ S→ it is not the case that s
satisﬁes J but not s′. Given a candidate invariant J provided by the learner in
line 3, the teacher who knows the transition relation T , checks if J is an inductive
invariant in line 4; if yes, the process stops, an invariant has been found; otherwise
a counterexample is provided and used in line 7 to update the sample for the
next iteration. The teacher checks the three conditions an inductive invariant
must satisfy (see Section 2.1). If (1) is violated the counterexample is a state s
which should be in the invariant because it is in Init . Therefore s is added to
S+. If (2) is violated the counterexample is a state s which should not be in
the invariant because it is not in Good and s is added to S−. If (3) is violated
the counterexample is an implication s → s′ where if s is in the invariant, s′
should also be in it. Therefore s → s′ is added to S→. In all three cases, the
sample is updated to satisfy property 4. If this leads to a contradictory sample,
i.e. S+ ∩ S− 6= ∅, the program is incorrect and an error is returned. Notice that
obviously, in general, the loop is not guaranteed to terminate.

2.3 ICE-DT: Invariant Learning using Decision Trees

In [15], the ICE learning framework is instantiated with a learn method, which
extends classical decision tree learning algorithms with the handling of impli-
cations. In the context of invariant synthesis, decision trees are used to classify
points from a universe, which is the set of program states. They are binary trees
whose inner nodes are labeled by predicates from a set of attributes and whose
leaves are either + or −. Attributes are (atomic) formulas over the variables
of the program. They can be seen as boolean functions that the decision tree
learning algorithm will compose to construct a classiﬁer of the given ICE sample.
In our case of numerical programs manipulating integer variables, attributes are
linear inequalities. Then, a decision tree can be seen naturally as a quantiﬁer-free
formula over program variables.

The main idea of the ICE-DT learner (see Algorithm 2) is as follows. Ini-
tially, the learner ﬁxes a set of attributes (possibly empty) which is kept in a
global variable and updated in successive executions of Learn(S). In line 2,
given a sample, the learner checks whether the current set of attributes is suf-
ﬁcient to produce a decision tree corresponding to a formula consistent with
the sample. If the check is successful the sample S is changed to SAttr taking
into account information gathered during the check (see below for the details of
sufficient(Attributes, S)). If the check fails new attributes are generated with

Data-driven Numerical Invariant Synthesis

7

Input : An ICE sample S = (S+, S−, S→)
Output: A formula
Global : Attributes initialized with InitialAttributes

1 Proc Learn(S)
2

(success, SAttr) ← sufficient(Attributes, S);
while ¬success do

3

4

5

6

Attributes ← generateAttributes(Attributes, S);
(success, SAttr) ← sufficient(Attributes, S);

return tree to f ormula(Construct-Tree(SAttr, Attributes))

Algorithm 2: The ICE-DT learner Learn(S) procedure.

generateAttributes(Attributes, S) until success. Then, a decision tree is con-
structed in line 6 from the sample SAttr by Construct-Tree(SAttr, Attributes)
which we present below (Algorithm 3). It is transformed into a formula and re-
turned as a potential invariant. Notice that in the main ICE loop of Algorithm 1
the teacher then checks if this invariant is inductive or not. If not, the original
sample S is updated and in the next iteration the learner checks if the attributes
are still suﬃcient for the updated sample. If not, the learner generates new at-
tributes and proceeds with constructing another decision tree and so on.

An important question is how to choose InitialAttributes and how to gener-
ate new attributes when needed. In [15], the set InitialAttributes is for example
the set of octagons over program variables with absolute values of constants
bounded by c ∈ N. If these attributes are not suﬃcient to classify the sample,
then new attributes are generated simply by increasing the bound c by 1. We
use a diﬀerent method described in detail in Section 4. We now describe how a
decision tree can be constructed from an ICE sample and a set of attributes.
Decision tree learning algorithms. The well-known standard decision tree
learning algorithms like ID3 [22] take as an input a sample containing points
marked as positive or negative of some universe and a ﬁxed set Attributes.
They construct a decision tree by choosing as the root an attribute, splitting
the sample in two (one with all points satisfying the attribute and one with the
other points) and recursively constructing trees for the two subsamples. At each
step the attribute maximizing the information gain computed using the entropy
of subsamples is chosen. Intuitively this means that at each step, the attribute
which separates the “best” positive and negative points is chosen. In the context
of veriﬁcation, exact classiﬁcation is needed, and therefore, all points in a leaf
must be classiﬁed in a way consistent with the sample.

In [15] this idea is extended to handle also implications which is essential for
an ICE learner. The basic algorithm to construct a tree (given as Algorithm 3
below) gets as input an ICE sample S = (S+, S−, S→) and a set of Attributes
and produces a decision tree consistent with the sample, which means that each
point in S+ (resp. S−) is classiﬁed as positive (resp. negative) and for each
implication (s, s′) ∈ S→ it is not the case that s is classiﬁed as positive and s′
as negative. The initial sample S is supposed to be consistent.

8

A. Bouajjani et al.

Input : An ICE sample S = (S+, S−, S→) and a set of Attributes.
Output: A tree

1 Proc Construct-Tree(S, Attributes)
2
Set G (partial mapping of end-
points of impl. to {Positive, Negative}) to empty ;

3

4

Let U nclass be the set of all end-points of implications in S→;
Compute the implication closure of G w.r.t. S;
return DecisionTreeICE(hS+, S−, U nclassi, Attributes);

5
6 Proc DecisionTreeICE(Examples = hP os, N eg, U nclassi, Attributes)
7

Move all points of U nclass classiﬁed as Positive (resp. Negative) to P os
(resp. N eg);
if N eg = ∅ then

Mark all points of U nclass in G as Positive;
Compute the implication closure of G w.r.t. S;
return Leaf(+);

else if P os = ∅ then

Mark all points of U nclass in G as Negative;
Compute the implication closure of G w.r.t. S;
return Leaf(−);

else

a ← choose(Attributes, Examples);
Divide Examples into two: Examplesa with all points satisfying a and

Examples¬a the others;
Tlef t ← DecisionTreeICE(Examplesa, Attributes \ {a});
Tright ← DecisionTreeICE(Examples¬a, Attributes \ {a});
return T ree(a, Tlef t, Tright);

Algorithm 3: The ICE-DT decision-tree learning procedures.

8

9

10

11

12

13

14

15

16

17

18

19

20

21

The learner is similar to the classical decision tree learning algorithms. How-
ever, it has to take care of implications. To this end, the learner also considers
the set of points appearing as end-points in the implications but not in S+ and
S−. These points are considered in the beginning as unclassiﬁed, and the learner
will either mark them Positive or Negative during the construction as follows:
if in the construction of the tree a subsample is reached containing only positive
(resp. negative) points and unclassiﬁed points (lines 8 and 12 resp.), all these
points are classiﬁed as positive (resp. negative). To make sure that implications
are still consistent, the implication closure with the newly classiﬁed points is
computed and stored in the global variable G, a (partial mapping) of end-points
in S→ to {Positive, Negative}. The implication closure of G w.r.t. S is deﬁned
as: If G(s) = Positive or s ∈ S+ and (s, s′) ∈ S→ then also G(s′) = Positive.
If G(s′) = Negative or s′ ∈ S− and (s, s′) ∈ S→ then also G(s) = Negative.
The set Attributes is such that a consistent decision tree will always be
found, i.e. the set Attributes in line 17 is never empty (see below). An attribute
in a node is chosen with choose(Attributes, Examples) returning an attribute

Data-driven Numerical Invariant Synthesis

9

a ∈ Attributes with the highest gain according to Examples. We do not give
the details of this function. In [15] several gain functions are deﬁned extending
the classical gain function based on entropy with the treatment of implications.
We use the one which penalizes cutting implications (like ICE-DT-penalty).
Checking if the set of attributes is suﬃcient. Here we show how the
function sufficient(Attributes, S) of Algorithm 2 is implemented in [15]. Two
states s and s′ are considered equivalent (denoted by ≡Attributes), if they satisfy
the same attributes of Attributes. One has to make sure that two equivalent
states are never classiﬁed in diﬀerent ways by the tree construction algorithm.
This is done by the following procedure: For any two states s, s′ with s ≡Attributes
s′ which appear in the sample (as positive or negative or end-points of the
implications) two implications s → s′ and s′ → s are added to S→ of S.

Then, the implication closure of the sample is computed starting from an
empty mapping G (all end-points are initially unclassiﬁed). If during the com-
putation of the implication closure one end-point is classiﬁed as both Positive
and Negative, then sufficient(Attributes, S) returns (f alse, S) else it returns
(true, SAttr) where SAttr is obtained from S = (S+, S−, S→) by adding to S+
the end-points of implications classiﬁed as Positive and to S− the end-points
classiﬁed as Negative.

In [15] it is shown that this guarantees in general that a tree consistent with
the sample will always be constructed regardless of the order in which attributes
are chosen. We illustrate now the ICE-DT learner on a simple example.

Example 1. Let S = (S+, S−, S→) be a sample (illustrated in Fig. 1) with two-
dimensional states (variables x and y): S+ = {(1, 1), (1, 4), (3, 1), (5, 1), (5, 4),
(6, 1), (6, 4)}, S− = {(4, 1), (4, 2), (4, 3), (4, 4)}, S→ = {(2, 2) → (2, 3), (0, 2) →
(4, 0)}. We suppose that Attributes = {x ≥ 1, x ≤ 3, y ≥ 1, y ≤ 4, x ≥ 5, x ≤ 6}
is given. In Section 4 we show how to obtain this set from the sample. The
learner ﬁrst checks that the set Attributes is suﬃcient to construct a formula
consistent with S. The check succeeds and we have among others that (2, 2) and
(2, 3) and the surrounding positive states on the left are all equivalent w.r.t.
≡Attributes. Therefore after adding implications (which we omit for clarity in the
following) and the computation of the implication closure both (2, 2) and (2, 3)
are added to S+. Then, the construction of the tree is started with Examples
containing 9 positive, 4 negative and 2 unclassiﬁed states. Depending on the
gain function an attribute is chosen. Here, it is x ≥ 5, since it separates all the
positive states on the right from the rest and does not cut the implication. The
set Examples is split into the states satisfying x ≥ 5 and those which don’t :
Examplesx≥5 and Examplesx<5. Examplesx≥5 contains only positive states
{(5, 1), (5, 4), (6, 1), (6, 4)} and the branch is ﬁnished whereas Examplesx<5 con-
tains the remaining positive, negative and unclassiﬁed states and the construc-
tion continues. The attribute x ≤ 3 is chosen and Examplesx<5 split in two.
Examplesx<5∧x≤3 contains the positive states {(1, 1), (1, 4), (3, 1), (2, 2), (2, 3)}
and one unclassiﬁed state (0, 2). Therefore, the algorithm marks (0, 2) as positive
and as there is an implication (0, 2) → (4, 0), the state (4, 0) is marked positive
as well and a leaf node is returned. The other branch Examplesx<5∧x>3 now

10

A. Bouajjani et al.

contains negative states {(4, 1), (4, 2), (4, 3), (4, 4)} and a positive state (4, 0).
Therefore another attribute is needed. Finally, the algorithm returns a tree cor-
responding to the formula x ≥ 5 ∨ (x < 5 ∧ x ≤ 3) ∨ (x < 5 ∧ x > 3 ∧ y < 1).

3 Linear Formulas as Abstract Objects

Algorithm 2 requires a set of attributes as input. In section 4, we show how to
generate these attributes from the sample. For that purpose, we use numerical
abstract domains to represent and manipulate algorithmically sets of integer
vectors representing program states. We consider standard numerical domains
deﬁned in [6,8,21] and implemented in tools such as APRON [17]: Intervals,
Octagons, and Polyhedra.

Given a set of n variables X and a linear formula ϕ over X, let [[ϕ]] ⊆ Zn be
the set of all integer points satisfying the formula. Now, a subset of Zn is called

– an interval, iﬀ it is equal to [[ϕ]] where ϕ is a conjunction of constraints of

the form α ≤ x ≤ β, where x ∈ X, α ∈ Z ∪ {−∞} and β ∈ Z ∪ {+∞}.

– an octagon, iﬀ it is equal to [[ϕ]] where ϕ is a conjunction of constraints of

the form ± x ± y ≤ α where x, y ∈ X and α ∈ Z ∪ {+∞}.

– a polyhedra, iﬀ it is equal to [[ϕ]] where ϕ is a conjunction of linear constraints
n
i=1 aixi ≤ b where X = {x1, . . . , xn} and for every i, ai ∈ Z,

of the form P
and b ∈ Z ∪ {+∞}.

Now, we can deﬁne several abstract domains as complete lattices Atype
X , ⊑, ⊔, ⊓, ⊥, ⊤i, where type is either int, oct or poly and Dint
X the set of polyhedra.

X is the set of octagons and Dpoly

hDtype
intervals, Doct

X =
X is the set of

The relation ⊑ is set inclusion. The binary operation ⊔ (resp. ⊓) is the join
(resp. meet) operation that deﬁnes the smallest (resp. greatest) element in DX
that contains (resp. contained in) the union (resp. the intersection) of the two
composed elements. Finally ⊥ (resp. ⊤) corresponds to the empty set (resp. Zn).
We suppose that we have a function Form type (d) which given an element
d ⊆ Zn of the lattice provides us a formula ϕ of the corresponding type such
that [[ϕ]] = d. There are many ways to describe the set d with a formula ϕ.
Therefore the function Form type (d) depends on the particular implementation
of the abstract domains. We furthermore deﬁne Constr type (d) to be the set of
linear constraints of Form type (d).

We drop the superscript type from all preceding deﬁnitions, when it is clear

from the context or when we deﬁne notions for all types.

All singleton subsets of Zn are elements of the lattices and for example, if
p = (x = 1, y = 2), then, for the domains of Intervals, Octagons, and Polyhedra
as implemented in APRON we have: Constr int ({p}) = {x ≤ 1, x ≥ 1, y ≤ 2, y ≥
2}, Constr oct ({p}) = {x ≥ 1, x ≤ 1, y − x ≥ 1, x + y ≥ 3, y ≥ 2, y ≤ 2, x + y ≤
3, x − y ≥ −1} and Constr poly ({p}) = {x = 1, y = 2}.

Notice, that in APRON while equality constraints are used in the Polyhedra
domain, these constraints are not explicit in the Interval and Octagon domains.

Data-driven Numerical Invariant Synthesis

11

y

y

y

y

x

x

x

x

(a) An ICE sample

(b) Intervals (int)

(c) Octagons (oct) (d) Polyhedra (poly)

Fig. 1. An ICE sample and its separators using diﬀerent abstract domains.

An important fact about the three domains mentioned above is that, each
element of the lattice is the intersection of a convex subset of Qn with Zn. To
be able to reason about integer points from nonconvex sets, we will use in the
next section sets of sets.

4 Generating Attributes from Sample Separators

We deﬁne in this section algorithms for generating a set of attributes that can
be used for constructing decision trees representing candidate invariants. Given
an ICE sample, these algorithms are based on constructing separators of the two
sets of positive and negative states that are consistent with the implications in
the sample. These separators are sets of intervals, octagons or polyhedra. The
set of all constraints that deﬁne these sets are collected as a set of attributes.

4.1 Abstract Sample Separators

Let S = (S+, S−, S→) be an ICE sample, and let AX = hDX , ⊑, ⊔, ⊓, ⊥, ⊤i be an
abstract domain. Intuitively, a separator has sets containing all positive states,
not containing any negative state and is consistent with implications. Formally,
an AX -separator of S is a set S ∈ 2DX such that ∀p ∈ S+. ∃d ∈ S. p ∈ d and
∀p ∈ S−. ∀d ∈ S. p 6∈ d and ∀p → q ∈ S→. ∀d ∈ S. (p ∈ d =⇒ (∃d′ ∈ S. q ∈ d′)).
Given a set of positive states S+, we deﬁne the basic separator Sbasic as
{{p} | p ∈ S+} where each state is alone in its set. Our method for generat-
ing attributes for the learning process is based on computing a special type of
separators called join-maximal. An AX -separator S is join-maximal if is not pos-
sible to take the join of two of its elements without including a negative state:
∀d1, d2 ∈ S. d1 6= d2 =⇒ (∃n ∈ S−. n ∈ d1 ⊔ d2).

Example 2. Let us consider again the ICE sample S given in Example 1. Fig.
1 shows the borders of join-maximal AX -separators for S for diﬀerent abstract
domains (Intervals int, Octagons oct, and Polyhedra poly).

Remark 1. An ICE sample may have multiple join-maximal separators as Fig. 2
shows for the polyhedra domain. The method presented in the next section
computes one of them non-deterministically.

12

A. Bouajjani et al.

y

y

x

x

(a)

(b)

Fig. 2. Diﬀerent join-maximal separators for a same sample.

4.2 Computing a Join-Maximal Abstract Separator

We present in this section a basic algorithm for computing a join-maximal AX -
separator for a given sample S. Computing such a separator can be done iter-
atively starting from Sbasic, and at each step, choosing two elements d1 and d2
in the current separator such that d1 ⊔ d2 does not contain a negative state in
S− (This can be checked using the meet operation ⊓), and replacing d1 and d2
by d1 ⊔ d2. Then, if any element of the separator contains the source p of an
implication p → q, which means that p is considered now as a positive state,
then since q must also be considered as positive, the element {q} must be added
to the separator if q is not already in some element of the current separator.
When no new join operations (without including negative states) can be done,
the obtained set is necessarily a join-maximal AX -separator of S. This procedure
corresponds to Algorithm 4.

Input : An ICE sample S = (S+, S−, S→) and an abstract domain

AX = hDX , ⊑, ⊔, ⊓, ⊥, ⊤i.
Output: S a join-maximal AX -separator of S.

1 Proc constructSeparator(S, AX)
2

S ← Sbasic (* = {{s} | s ∈ S+} *) ;
while true do

3

4

5

6

7

8

if ∃a, b ∈ S. a 6= b ∧ ∀n ∈ S−. n /∈ a ⊔ b then

S ← (S \ {a, b}) ∪ {a ⊔ b} ;
while ∃p → q ∈ S→. ∃d ∈ S. p ∈ d ∧ ∀d′ ∈ S. q /∈ d′ do

S ← S ∪ {{q}} ;

else break;

Algorithm 4: Computing a join-maximal AX -separator.

Notice that instead of starting with the basic separator Sbasic deﬁned as above
one can start with any separator Sinit ⊇ Sbasic whose additional sets contain only
states which are known to be positive (for example the initial states).

Example 3. Consider again the sample S of Example 2. We show how the sep-
arators of S in Fig. 1 are constructed using Algorithm 4. The algorithm starts

d6

d7

y

d3

y

d3

d1

d2 d4

(a)

d5

x

d4

d5

j1

x

Data-driven Numerical Invariant Synthesis

13

y

y

d6

d7

d6

d7

d6

d7

d8

j2

d4

d5

x

d4

d5

j3

x

(b)

(c)

(d)

Fig. 3. The ﬁrst iterations of Algorithm 4 on the sample S of Fig. 1

from the basic separator Sbasic where every positive state in S is alone (Fig.
3(a)). It picks two elements in that separator, e.g. {d1} and {d2}. As their join
does not include negative states, {d1} and {d2} are replaced by j1 = {d1} ⊔ {d2}
to get a new separator (Fig. 3(b)). Then, depending on the considered domain,
diﬀerent separators are obtained. For Intervals, the join of j1 and {d3} leads to
the separator in Fig. 1(a). Notice that both ends of the implication (2, 2) → (2, 3)
are included in j1 ⊔ {d3}. In the case of Octagons, the join of j1 and {d3} is the
set on the left of Fig. 1(b). Again, both ends of the implication (2, 2) → (2, 3) are
included in j1⊔{d3}. In the case of Polyhedra, j2 = j1⊔{d3} is the triangle shown
in Fig. 3(c). Since (2, 2) is included in j2 but not (2, 3), the element {(2, 3)} is
added to the separator, leading to the separator represented in Fig. 3(c). In the
next iteration, j2 is joined with {d8} leading to the separator shown in Fig. 3(d).
Finally, a similar iteration of join operations leads to the rectangle including the
four points, and this leads to the join-maximal separator of Fig. 1.

Remark 2. In the best case Algorithm 4 performs |S+| join and |S+|(|S−|+|S→|)
meet operations (all pairs of points can be joined and all left end-points of im-
plications are not in the new joined convex sets). In the worst case, it performs
O(cid:0)(|S+| + |S→|)2(cid:1) join and O(cid:0)(|S+| + |S→|)2(|S−| + |S→|)(cid:1) meet operations
(at most |S−| + |S→| meets are needed to check if two sets can be joined and
implications might add new points to S+). The cost of meet and join depends
on the used abstract domain; it is polynomial for intervals and octagons, and ex-
ponential for polyhedra, in the number of variables. Algorithm 4 is not designed
to compute a join-max separator with a minimal number of convex sets as this
would require a potentially exponential number of meet and join operations.

4.3 Integrating Separator Computation in ICE-DT

We use the computation of a join-maximal separator to provide an instance of the
function generateAttributes of ICE-DT in Algorithm 2. Given a sample S,
let S be the AX -separator of S computed by constructSeparator(S, AX )
deﬁned by Algorithm 4. We consider the set InitialAttributes containing all the
predicates that constitute the speciﬁcation (Init and Good ) and those that ap-
pear in the programs (as tests in the conditional statements and while loops).
Then, we deﬁne: generateAttributes(S) = InitialAttributes∪Sd∈S Constr (d)

14

A. Bouajjani et al.

Remark 3. Several convex sets of the separator S might generate the same con-
straint and the set of attributes generated in this way might contain attributes
which partition the state space in the same way (e.g. x ≤ 0 and x ≥ 1, equivalent
to x > 0 over the integers). We keep only one of them. The number of attributes
generated is at most linear in the number of positive states in the sample S.

1 int j, k, t;
2 assume(j = 2 ∧ k = 0);
3 while true do
4
5 assert(k = 0 ∨ j = 2k + 2);

if t = 0 then j ← j + 4 else j ← j + 2; k ← k + 1;

Fig. 4. Example program

Notice that our function generateAttributes(S), contrary to the one used
in the original ICE-DT (Algorithm 2), does not expand a set of existing at-
tributes, and therefore it only need the sample S as argument. In fact, with
our method for computing attributes, the ICE-DT schema can be simpliﬁed:
the while loop in Algorithm 2 can be replaced by one single initial test on the
condition of success. Indeed, each time the learner is called, it checks whether
the set of attributes computed for the previous sample is suﬃcient to build a
separator for the new sample. Only when it is not suﬃcient that the generation
of a separator is performed. Then, the call of the sufficient function afterward
is needed to extend the sample so that the construction of a decision tree can
be done (see explanation in Section 2.3), but it will necessarily succeed since in
our case the set of attributes deﬁnes by construction a separator of the sample.

Example 4. Consider the program in Fig. 4 whose set of variables is X = {j, k, t}.
We use Polyhedra. First, starting from an empty ICE-sample, regardless of the
attributes, the learner proposes true as an invariant and (5, 1, 0) is returned as
a negative counterexample. Then, it proposes f alse and (2, 0, 0) is returned as
a positive counterexample.

Now, Algorithm 4 is called to compute a separator for S = (S+ = {(2, 0, 0)},
S− = {(5, 1, 0)}, S→ = ∅). Here, we use initially a separator Sinit containing
the set of states satisfying the initial condition j = 2 ∧ k = 0 denoted by d1 in
addition to d0 where d0 = {(2, 0, 0)}. Since d0 ⊆ d1, the algorithm returns the
join-maximal separator S = {d1} with Constrpoly(d1) = {j = 2, k = 0}.

Using constraints from S as attributes, the learner constructs the candidate
invariant k = 0. Then, the teacher provides an implication counterexample
(0, 0, 1) → (2, 1, 1). Now, without computing another separator (as the one it
has is suﬃcient for the new sample), the learner proposes j = 2 ∧ k = 0 as
an invariant, and the implication counterexample (2, 0, 1) → (4, 1, 1) is returned
(and since (2, 0, 1) is an initial state, (4, 1, 1) is also considered positive).

Data-driven Numerical Invariant Synthesis

15

Then, Algorithm 4 is called again to construct a separator for the sample S =
(S+ = {(2, 0, 0), (4, 1, 1)}, S− = {(5, 1, 0)}, S→ = {(0, 0, 1) → (2, 1, 1), (2, 0, 1) →
(4, 1, 1)}). Starting from a separator Sinit = {d0, d1, d2} with d2 = {(4, 1, 1)} it
returns the join-maximal separator

S = {d3}

Constrpoly(d3) = {2k + 2 = j, j ≤ 4, j ≥ 2}

Based on this separator, the learner proposes 2k + 2 = j, (2, 0, 0) → (6, 0, 0) is
given as a counterexample (and then, since (2, 0, 0) is in S+, (6, 0, 0) is considered
positive). Then, from Sinit = {d0, d1, d2, d4} with d4 = {(6, 0, 0)} a new separator
S is constructed

S = {d5}

Constrpoly(d5) = {j + 2k ≤ 6, k ≥ 0, j ≥ 2k + 2}

leading to a new candidate invariant: j + 2k ≤ 6 ∧ j ≥ 2k + 2. The teacher
returns at this point the negative state (0, −2, 0). The attributes of S are still
suﬃcient to construct a decision tree for the sample. Then, the learner proposes
j + 2k ≤ 6 ∧ k ≥ 0 ∧ j ≥ 2k + 2, and the teacher returns the counterexample
(3, 0, 1) → (5, 1, 1) (and since (5, 1, 1) is a negative state, (3, 0, 1) is considered
negative). The current sample S is now (S+ = {(2, 0, 0), (4, 1, 1), (6, 0, 0)}, S− =
{(5, 1, 0), (5, 1, 1), (3, 0, 1), (0, −2, 0)}, S→ = {(0, 0, 1) → (2, 1, 1), (2, 0, 1) →
(4, 1, 1), (2, 0, 0) → (6, 0, 0), (3, 0, 1) → (5, 1, 1)}).

Then, from Sinit = {d0, d1, d2, d4}, a join-maximal separator is constructed

S = {d3, d4}

Constrpoly(d4) = {j = 6, t = 0, k = 0}

Some iterations later, using only the attributes of the last S, the learner generates
the inductive invariant (t = 0 ∧ 2 ≤ j ∧ k = 0) ∨ (t 6= 0 ∧ 2 ≤ j ∧ 2k + 2 = j)

4.4 Computing Separators Incrementally

Algorithm 4 of Section 4.2 always starts from the initial separator, regardless
of what has been done in the previous iterations of the ICE learning process.
Here, we present an incremental approach to exploit the fact that adding a
counterexample to the sample may modify the separator only locally allowing
parts of separators computed in previous iterations to be reused. The basic idea
is to store the history of the separator computation along the ICE iterations,
and update it according to the new counterexamples discovered at each step.

The Algorithm. We use an abstract stack data structure to represent the
history of separators. Along the iterations of the ICE learning algorithm, an
increasing sequence of samples Si’s is considered (at each iteration it is enriched
by the new counterexample provided by the teacher). Then, at each step i, a
join-maximal separator Si of the sample Si is computed and stored in the stack.
Notice that at a given step i, separators of index j < i are not necessarily
separators of Si since they may not cover all positive points of Si. Therefore,
we introduce the following notion: a partial AX -separator of a sample S is a set
S ∈ 2DX such that ∀p ∈ S−. ∀d ∈ S. p /∈ d.

Now, to compute the separator Si, we start from one of the partial separators
in the stack, the most recent one that is not aﬀected by the last update of the
sample. When the sample at step i is extended with positive states, Si can
be computed directly from Si−1. However, when the sample is extended with

16

A. Bouajjani et al.

negative states, this might require reconsidering several previous steps since some
of the elements (convex sets) of their separators might contain states that are
(discovered now to be) negative. In that case, we must return to the step of
the greatest index j < i (i.e., the last step before i) such that Sj is a partial
separator of Si (i.e., the new knowledge about the negative states does not
aﬀect the computed separation at step j). By the fact that the sequence of
samples is increasing, it is indeed correct to consider the biggest j < i satisfying
the property above. Therefore, the separator Si is computed starting from Sj
augmented with all the positive states in S+

i \ S+
j .

This leads to Algorithm 5. We use in its description a stack P supplied with
the usual operations: P.head() returns the top element of the stack, P.pop()
removes and returns the top element of the stack, and P.push(e) inserts an
element e at the top position of the stack. A reﬁnd version of Algorithm 5 is
presented in Section 4.5 where the backtracking phase is made more eﬀective:
We attach information to each join-created object in order to track its join-
predecessors (objects involved in its creation) in the stack.

Global

: P = {∅} a stack of partial separators.

1 Proc constructSeparatorInc(Si = (S+

i , S−

i , S→

i ), AX)

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

// backtracking
while true do
if ∃n ∈ S−
P.pop();
else break;

i . ∃d ∈ P.head(). n ∈ d then

// expansion
S ← P.head();
add ← {p ∈ S+
i
d′ ∧ ∀d′′ ∈ S. q /∈ d′′};
while ∃s ∈ add do

| ∀d ∈ S. p /∈ d} ∪ {q | ∃p → q ∈ S→
i

. ∃d′ ∈ S. p ∈

add ← add \ {s};
if ∃d ∈ S. ∀n ∈ S−
let o = d ⊔ {s};
S ← (S \ {d}) ∪ {o};
for p → q ∈ S→
i

i . n /∈ d ⊔ {s} then

s.t. p ∈ o ∧ ∀d′ ∈ S. q /∈ d′ do

add ← add ∪ {q}

else

S ← S ∪ {{s}};
for p → q ∈ S→
i

s.t. p = s ∧ ∀d′ ∈ S. q /∈ d′ do

add ← add ∪ {q}

P.push(S);
return S;

Algorithm 5: Incremental computation of an AX -separator of a sample S.

Data-driven Numerical Invariant Synthesis

17

Integration to ICE-DT. The function constructSeparatorInc can be in-
tegrated to the ICE-DT algorithm just as the function constructSeparator
in Section 4.3, by using it to implement the function generateAttributes of the
learner. But this time, the learner is more eﬃcient in computing the separator
from which the attributes are extracted.

Example 5. Consider again the program in Fig. 4 of Example 4. The two ﬁrst it-
erations are similar to the ones described in Example 4. Then, the obtained
sample is S = (S+ = {(2, 0, 0)}, S− = {(5, 1, 0)}, S→ = ∅). Starting from
the empty separator, Algorithm 5 computes the separator S1 = {d1} where
Constrpoly(d1) = {j = 2, k = 0}. Then, the learner proceeds as in the previous
example to get the sample S = (S+ = {(2, 0, 0), (4, 1, 1)}, S+ = {(5, 1, 0)}, S→ =
{(0, 0, 1) → (2, 1, 1), (2, 0, 1) → (4, 1, 1)}). To build a separator of S, Algorithm 5
starts from S1 and produces S2 = {d3} where d3 = d1 ⊔ {(4, 1, 1)}.

Similarly, when the counterexample (2, 0, 0) → (6, 0, 0) is obtained, the algo-
rithm starts directly from S2 to produce S3 = {d5} where d5 = d3 ⊔ {(6, 0, 0)}.
After two more iterations, the sample is the same as S′ in Example 4. At
this point, S3 cannot be used to construct a separator for S since d5 includes
the negative state (3, 0, 1). Then, the algorithm removes S3 from the stack. It
checks that S2 is a partial separator of S, which is indeed the case. Then, it
constructs a new separator S4 based on S2 by expanding it with the counterex-
amples received after the construction of S2 (the negative state (0, −2, 0) and
the implications (2, 0, 0) → (6, 0, 0) and (3, 0, 1) → (5, 1, 1)): S4 = {d3, d6} where
Constrpoly(d6) = {t = 0, k = 0, j = 6}. The rest of the execution proceeds as
with Algorithm 4. Here, the advantages of the incremental method are: (1) while
positive examples are added the separators are simply expanded, and (2) when
a negative example at step 4 is added, only one join operation has to be undone.

4.5 A Reﬁned Version of the Incremental Algorithm

Algorithm 5 includes a backtracking phase where it searches for the most re-
cent partial separator of S in the stack. To make this backtracking phase more
eﬀective, we attach some information to the separators stored in the stack and
to their elements. We associate with each newly created object an index that
corresponds to the number of the current iteration. Assume the algorithm is
at step i, and that a negative state n is discovered such that n ∈ d for some
d ∈ Si−1. Then, knowing the index of d, say j with j < i − 1, allows to jump
directly to Sj−1 in the backtracking process. If the index of d is i − 1, tjhis means
that d = d1 ⊔ d2 for some d1 and d2 in Si−2. Then, if one of these two objects
contains n, then it is possible to jump back to step just before its creation using
its index, and continue the backtracking process from there. So, in order to do
this, we need to associate also with each object the set of its parents, i.e., objects
that were joined for its creation, when they exist (since an object that be created
initially as a singleton from a program state).

In the following, a node N (∈ N , the set of nodes) is a structure with an
abstract object (object(N )), an index (index(N )), and a set of nodes parents(N )

18

A. Bouajjani et al.

Global

1 Proc constructSeparatorInc(Si = (S+

: P = {(∅, 0)} a stack of indexed forests.
i , S⇒

i , S−

i

, AX))

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

// backtracking
while true do
if ∃n ∈ S−

i . ∃N ∈ f orest(P.head()). n ∈ object(N ) then

tmp ← N ;
while n ∈ object(tmp) do

if ∃N ′ ∈ parents(tmp). n ∈ object(N ′) then

tmp ← N ′
else break ;

while index(P.head()) ≥ index(tmp) do P.pop() ;

else break;

// expansion
F ← f orest(P.head());
add ← {p ∈ S+
i
F. p ∈ object(N ′) ∧ ∀N ′′ ∈ F. q /∈ object(N ′′)};
while ∃s ∈ add do

| ∀N ∈ F. p /∈ object(N )} ∪ {q | ∃p → q ∈ S⇒
i

. ∃N ′ ∈

add ← add \ {s};
if ∃N ∈ F s.t. ∀n ∈ S−

i . n /∈ object(N ) ⊔ {s} then

let o = object(N ) ⊔ {s};
F ← (F \ {N }) ∪ {(o, i, {N, ({s}, i, ∅)})};
for p → q ∈ S⇒
i

s.t. p ∈ o ∧ ∀N ′ ∈ F. q /∈ object(N ′) do

add ← add ∪ {q}

else

F ← F ∪ {({s}, i, ∅)};
for p → q ∈ S⇒
i

s.t. p = s ∧ ∀N ′ ∈ F. q /∈ object(N ′) do

add ← add ∪ {q}

P.push((F, i));
return Separator(F);

Algorithm 6: A reﬁnement of Algorithm 5.

corresponding to its parents (or empty). Sets of nodes are called forests. Let
F = 2N be the set of forests. We use forests to represent sets of nodes such that
the set of their objects are separators. For a forest F ∈ F , let Separator(F) =
{object (N ) | N ∈ F}.

The algorithm uses a stack of indexed forests: a pair of a forest with the index
of the step at which it has been stored. This allows to express the backward jumps
during the backtracking phase.

5 Experiments

We have implemented the prototype tool NIS (Numerical Invariant Synthesizer)
using our method for attribute synthesis with the ICE-DT schema. NIS written
in C++ is conﬁgurable with an abstract domain for the manipulation of abstract
objects. It uses Z3 [23] for SMT queries and APRON’s [17] abstract domains.

Data-driven Numerical Invariant Synthesis

19

Tool

Solved
safe unsafe

Total

ICE-DT

111
LoopInvGen 130
129
118

CVC4
Spacer
NIS(int)
NIS(oct)
NIS(poly)
NIS(VB)

106
122
137

143

11
8
-
18

17
14
17

17

122
138
129
136

123
136
154

160

NIS(int)
NIS(oct)
NIS(poly)

NIS(int) NIS(oct) NIS(poly)
7
-
18

-
13
31

2
5
-

Fig. 5. Benchmark results and comparison of NIS wrt. diﬀerent abstract domains.

We compare our implementation with ICE-DT3, LoopInvGen, CVC4, and
Spacer4. LoopInvGen is a data-driven invariant inference tool based on a syn-
tactic enumeration of candidate predicates [25,24]. It is written in OCaml and
uses Z3 as an SMT solver. CVC4 uses an enumerative refutation-based approach
[1,26]. It is written in C++ and it includes an SMT solver. Spacer is a PDR-based
CHC solver [18], written in C++ and integrated in Z3.

The evaluation was done on 164 linear integer arithmetic (LIA) programs5
from SyGuS-Comp’19. They have a number of variables ranging from 2 to 10.
The experiments were carried out using a timeout of 1800s (30 minutes) for each
example. They were conducted on a machine with 4 CPUs Intel(R) Xeon(R)
2,13GHz, 16 cores, and 128 Go RAM running Linux CentOS 7.9.

Figure 5 shows the number of safe and unsafe solved programs by each tool.
The instance of our approach using the Polyhedral abstract domain solves 154
programs out of 164, and the virtual best of our approach with the three abstract
domains Intervals, Octagons, and Polyhedra, solves 160 programs out of 164.
Two of the remaining examples require handling quantiﬁers, which cannot be
done with the current implementation. The two others have not been solved with
any of the four tools we considered.

These results show that globally our approach is powerful and is able to solve
a signiﬁcant number of cases that are not solvable by other tools. Interestingly,
using diﬀerent abstract domains leads to incomparable performances: although
with polyhedra more cases are solvable, there are some cases that are uniquely

3 The original ICE-DT tool [15] does not support programs in the SyGuS format. Here
we use our own implementation of ICE-DT. It shares with NIS all the components
(teacher, decision tree learning algorithm with implications) except that attribute
discovery is enumerative.

4 Spacer does not support programs in the SyGuS format; a wrapper is written in
C++ that converts a SyGuS program to a CHC problem and supplies it to Spacer
via the Z3 FixedPoint API.

5 Other programs from SyGuS-Comp’19 have not been taken into account in our
evaluations as they are boolean programs with integer variables for encoding nonde-
terminism or artiﬁcial programs augmented with useless variables and statements.

20

A. Bouajjani et al.

solvable with intervals or octagons. Also, while operations on intervals and oc-
tagons have a lower complexity than on polyhedra, this is compensated with the
fact that polyhedra are more expressive. Indeed, their expressiveness allows in
many cases to ﬁnd quickly invariants for which a less expressive domain requires
much more iterations to be learned. Figure 5 shows the number of programs that
can be solved using a particular abstact domain but not with another. Polyhedra
are globally superior, but the three domains are complementary.

Compared to the other tools, the bottleneck of ICE-DT and also of Loop-
InvGen is the number of predicates that are generated using enumeration. Our
approach avoids the explosion of the size of the attribute pool by guiding their
discovery with the data sample, and reducing the size (by replacing objects by
their join) of the computed separators from which constraints are extracted. Con-
cerning CVC4, it uses enumerative refutation techniques, which are also subject
to an explosion problem. Moreover, CVC4 does not allow to solve the cases of
unsafe programs. The performances of Spacer depend on the ability to general-
ize the set of predecessors computed using the model-based projection and the
interpolants used for separation from bad states in the context of IC3/PDR.
While this is done eﬃciently in general, there are cases where this process can
lead to fastidious computations while our technique can be much faster using a
small number of join operations of positive states.

The scatter plots shown in Fig. 6 compare the execution times of our ap-
proach using Polyhedra abstract domain NIS(poly) with LoopInvGen, CVC4
and Spacer. (A timeout of 1800s is used for each example.) They show that
NIS(poly) is in general faster than both LoopInvGen and CVC4, and that it
has comparable performances in terms of execution time with Spacer. We have
also compared the original ICE-DT, based on enumerative attribute generation
using octagonal templates (as in [15]) with NIS(oct). The comparison shows
that our tool is signiﬁcantly faster (see the bottom right subﬁgure of Fig. 6).

6 Conclusion

We have deﬁned an eﬃcient method for generating relevant predicates for the
learning process of numerical invariants. The approach is guided by the data
sample built during the process and is based on constructing a separator of the
sample. The construction consists of an iterative application of join operations
in numerical abstract domains in order to cover positive states without including
negative ones. Our method is tightly integrated to the ICE-DT schema, leading
to an eﬃcient data-driven invariant synthesis and veriﬁcation algorithm.

Future work includes several directions. First, alternative methods for con-
structing separators should be investigated in order to reduce the size of the pool
of attributes along the learning process while increasing their potential relevance.
Another issue to investigate is the control of the counterexamples provided by
the teacher since they play an important role in the learning process. In our
current implementation, their choice is totally dependent on the SMT solver
used for implementing the teacher. Finally, we intend to extend this approach to

Data-driven Numerical Invariant Synthesis

21

TO

60

1

s

n

i

)
y
l
o
p
(
S
I

N

e
m
T

i

0

0

TO

60

1

s

n

i

)
y
l
o
p
(
S
I

N

e
m
T

i

1

60

TO

Time LoopInvGen in s

TO

60

1

s

n

i

)
y
l
o
p
(
S
I

N

e
m
T

i

0

0

TO

60

1

s

n

i

)
t
c
o
(
S
I

N

e
m
T

i

1
Time CVC4 in s

60

TO

0

0

1
Time Spacer in s

60

TO

0

0

1

Time ICE-DT in s

60

TO

Fig. 6. Runtime of NIS(poly) vs. LoopInvGen, CVC4, and Spacer,
and NIS(oct) vs. ICE-DT.

other types of programs, in particular to programs with other data types, and
programs with more general control structures such as procedural programs.

References

1. Barrett, C.W., Conway, C.L., Deters, M., Hadarean, L., Jovanovic, D., King, T.,
Reynolds, A., Tinelli, C.: CVC4. In: CAV 2011. Lecture Notes in Computer Science,
vol. 6806, pp. 171–177. Springer (2011)

2. Bradley, A.R.: SAT-Based Model Checking without Unrolling. In: VMCAI 2011.

Lecture Notes in Computer Science, vol. 6538, pp. 70–87. Springer (2011)

3. Champion, A., Kobayashi, N., Sato, R.: Hoice: An ice-based non-linear horn clause
solver. In: APLAS 2018. Lecture Notes in Computer Science, vol. 11275, pp. 146–
156. Springer (2018)

4. Clarke, E.M., Grumberg, O., Jha, S., Lu, Y., Veith, H.: Counterexample-guided
Abstraction Reﬁnement for Symbolic Model Checking. J. ACM 50(5), 752–794
(2003)

22

A. Bouajjani et al.

5. Col´on, M., Sankaranarayanan, S., Sipma, H.: Linear Invariant Generation using
Non-linear Constraint Solving. In: CAV 2003. Lecture Notes in Computer Science,
vol. 2725, pp. 420–432. Springer (2003)

6. Cousot, P., Cousot, R.: Static Determination of Dynamic Properties of Programs.
In: Proceedings of the Second International Symposium on Programming. pp. 106–
130. Dunod (1976)

7. Cousot, P., Cousot, R.: Abstract Interpretation: A Uniﬁed Lattice Model for Static
Analysis of Programs by Construction or Approximation of Fixpoints. In: POPL
1977. pp. 238–252. ACM (1977)

8. Cousot, P., Halbwachs, N.: Automatic Discovery of Linear Restraints Among Vari-

ables of a Program. In: POPL 1978. pp. 84–96. ACM Press (1978)

9. E´en, N., Mishchenko, A., Brayton, R.K.: Eﬃcient Implementation of Property

Directed Reachability. In: FMCAD 2011. pp. 125–134. FMCAD Inc. (2011)
10. Ernst, M.D., Perkins, J.H., Guo, P.J., McCamant, S., Pacheco, C., Tschantz, M.S.,
Xiao, C.: The daikon system for dynamic detection of likely invariants. Sci. Com-
put. Program. 69(1-3), 35–45 (2007)

11. Ezudheen, P., Neider, D., D’Souza, D., Garg, P., Madhusudan, P.: Horn-ICE
Learning for Synthesizing Invariants and Contracts. Proc. ACM Program. Lang.
2(OOPSLA), 131:1–131:25 (2018)

12. Feldman, Y.M.Y., Immerman, N., Sagiv, M., Shoham, S.: Complexity and informa-
tion in invariant inference. Proc. ACM Program. Lang. 4(POPL), 5:1–5:29 (2020)
13. Flanagan, C., Leino, K.R.M.: Houdini, an Annotation Assistant for ESC/Java. In:
FME 2001. Lecture Notes in Computer Science, vol. 2021, pp. 500–517. Springer
(2001)

14. Garg, P., L¨oding, C., Madhusudan, P., Neider, D.: ICE: A Robust Framework for
Learning Invariants. In: CAV 2014. Lecture Notes in Computer Science, vol. 8559,
pp. 69–87. Springer (2014)

15. Garg, P., Neider, D., Madhusudan, P., Roth, D.: Learning Invariants using Decision
Trees and Implication Counterexamples. In: POPL 2016. pp. 499–512. ACM (2016)
16. Hoder, K., Bjørner, N.: Generalized Property Directed Reachability. In: SAT 2012.
Lecture Notes in Computer Science, vol. 7317, pp. 157–171. Springer (2012)
17. Jeannet, B., Min´e, A.: Apron: A Library of Numerical Abstract Domains for Static
Analysis. In: CAV 2009. Lecture Notes in Computer Science, vol. 5643, pp. 661–
667. Springer (2009)

18. Komuravelli, A., Gurﬁnkel, A., Chaki, S.: SMT-Based Model Checking for Recur-
sive Programs. In: CAV 2014. Lecture Notes in Computer Science, vol. 8559, pp.
17–34. Springer (2014)

19. Li, J., Sun, J., Li, L., Le, Q.L., Lin, S.: Automatic Loop-invariant Generation
and Reﬁnement through Selective Sampling. In: ASE 2017. pp. 782–792. IEEE
Computer Society (2017)

20. L¨oding, C., Madhusudan, P., Neider, D.: Abstract learning frameworks for synthe-
sis. In: TACAS 2016. Lecture Notes in Computer Science, vol. 9636, pp. 167–185.
Springer (2016)

21. Min´e, A.: The Octagon Abstract Domain. High. Order Symb. Comput. 19(1),

31–100 (2006)

22. Mitchell, T.M.: Machine Learning, International Edition. McGraw-Hill Series in

Computer Science, McGraw-Hill (1997)

23. de Moura, L.M., Bjørner, N.: Z3: an eﬃcient SMT solver. In: TACAS 2008. Lecture

Notes in Computer Science, vol. 4963, pp. 337–340. Springer (2008)

Data-driven Numerical Invariant Synthesis

23

24. Padhi, S., Millstein, T.D., Nori, A.V., Sharma, R.: Overﬁtting in Synthesis: Theory
and Practice. In: CAV 2019. Lecture Notes in Computer Science, vol. 11561, pp.
315–334. Springer (2019)

25. Padhi, S., Sharma, R., Millstein, T.D.: Data-driven Precondition Inference with

Learned Features. In: PLDI 2016. pp. 42–56. ACM (2016)

26. Reynolds, A., Barbosa, H., N¨otzli, A., Barrett, C.W., Tinelli, C.: cvc4sy: Smart
and Fast Term Enumeration for Syntax-Guided Synthesis. In: CAV 2019. Lecture
Notes in Computer Science, vol. 11562, pp. 74–83. Springer (2019)

27. Ryan, G., Wong, J., Yao, J., Gu, R., Jana, S.: CLN2INV: learning loop invariants

with continuous logic networks. In: ICLR 2020. OpenReview.net (2020)

28. Sharma, R., Gupta, S., Hariharan, B., Aiken, A., Nori, A.V.: Veriﬁcation as
learning geometric concepts. In: SAS 2013. Lecture Notes in Computer Science,
vol. 7935, pp. 388–411. Springer (2013)

29. Sharma, R., Nori, A.V., Aiken, A.: CAV 2012. Lecture Notes in Computer Science,

vol. 7358, pp. 71–87. Springer (2012)

30. Si, X., Naik, A., Dai, H., Naik, M., Song, L.: Code2Inv: A Deep Learning Frame-
work for Program Veriﬁcation. In: CAV 2022. Lecture Notes in Computer Science,
vol. 12225, pp. 151–164. Springer (2020)

31. Vizel, Y., Gurﬁnkel, A., Shoham, S., Malik, S.: IC3 - ﬂipping the E in ICE. In: VM-
CAI 2017. Lecture Notes in Computer Science, vol. 10145, pp. 521–538. Springer
(2017)

32. Zhu, H., Magill, S., Jagannathan, S.: A data-driven CHC solver. In: PLDI 2018.

pp. 707–721. ACM (2018)

