2
2
0
2

n
a
J

7

]

G
L
.
s
c
[

1
v
7
8
4
2
0
.
1
0
2
2
:
v
i
X
r
a

Sparse PCA on ﬁxed-rank matrices ∗

Alberto Del Pia †

January 10, 2022

Abstract

Sparse PCA is the optimization problem obtained from PCA by adding a sparsity
constraint on the principal components. Sparse PCA is NP-hard and hard to approx-
imate even in the single-component case. In this paper we settle the computational
complexity of sparse PCA with respect to the rank of the covariance matrix. We show
that, if the rank of the covariance matrix is a ﬁxed value, then there is an algorithm
that solves sparse PCA to global optimality, whose running time is polynomial in the
number of features. We also prove a similar result for the version of sparse PCA which
requires the principal components to have disjoint supports.

Key words: principal component analysis; sparsity; polynomial-time algorithm; global

optimum; constant-rank quadratic function

1

Introduction

Principal component analysis is one of the oldest and most popular dimensionality re-
duction techniques and it is used in a wide array of scientiﬁc disciplines.
In principal
component analysis, we are given a positive integer d and an n × m data matrix Q, where
each column represents an independent sample from data population, and each row gives a
particular kind of feature. Our task is to ﬁnd d linear combinations of the n features, called
principal components, that correspond to directions of maximal variance in the data. The
d principal components typically explain most of the variance present in the data, even if
the number d is chosen to be much lower than the number of features n in the original
dataset. Typically, principal component analysis is formulated in terms of the covariance
matrix, which is the n × n positive semideﬁnite matrix K := 1/m · (Q − E[Q])(Q − E[Q])T.
Formally, in principal component analysis we are given an n × n positive semideﬁnite
matrix K, a positive integer d smaller than n, and we seek an optimal solution to the
optimization problem

max
X∈Rn×d, X TX=Id

trace(X TKX),

(PCA)

∗This work is supported by ONR grant N00014-19-1-2322. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reﬂect the
views of the Oﬃce of Naval Research.

†Department of Industrial and Systems Engineering & Wisconsin Institute for Discovery, University of

Wisconsin-Madison, Madison, WI, USA. E-mail: delpia@wisc.edu.

1

 
 
 
 
 
 
where Id denotes the d × d identity matrix. The d principal components correspond to the
d columns of an optimal solution X. It is well-known that PCA can be eﬃciently solved.
In fact, an optimal solution is the matrix X whose columns are the d eigenvectors of K
corresponding to the largest d eigenvalues. This optimal solution to PCA can be found in
O(n3) time by computing an eigenvalue decomposition of K. We refer the reader to [29]
for an introduction to principal component analysis.

1.1 Sparse PCA

A potential disadvantage of PCA is that the principal components are usually linear com-
binations of all features. This often makes the derived principal components diﬃcult to
interpret. Sparse principal component analysis overcomes this disadvantage by requir-
ing the principal components to be linear combinations of just a few features. A direct
consequence is that sparse principal component analysis generally provides higher data
interpretability as well as better generalization error [7, 18, 16, 35, 6]. A natural formu-
lation of sparse principal component analysis is obtained by adding to PCA a sparsity
constraint on the principal components. Formally, in sparse principal component analysis
we are given an n × n positive semideﬁnite matrix K, positive integers d, s smaller than
n, and we seek an optimal solution to the optimization problem

max
X∈Rn×d
X TX=Id, |supp(X)|≤s

trace(X TKX),

(SPCA)

where supp(X) denotes the index set of the nonzero rows of the matrix X. Throughout
this paper we will often discuss the special cases of PCA and SPCA with d = 1. We
refer to these cases, where we only seek one principal component, as the single-component
cases.

SPCA is NP-hard and hard to approximate [8, 24] even in the single component case.
Successful approaches for SPCA include replacing the (cid:96)0-norm constraint with an (cid:96)1-norm
constraint or (cid:96)1 penalty [18, 35, 32], branch-and bound [25, 4], semideﬁnite programming
[11, 9, 34, 10], and convex integer programming [12]. A number of other specialized
algorithms have been proposed in, e.g., [30, 17, 19, 6, 1, 33, 27]. Only few of these papers
directly deal with the general version of SPCA as deﬁned in this paper [6]. In fact, most
known algorithms are based on an iterative approach where the principal components are
estimated in a one-at-a-time fashion with some sort of deﬂation step between iterations
[23].

The main challenge in solving SPCA to global optimality lies in identifying an optimal
support of SPCA among all the (cid:0)n
(cid:1) index sets of cardinality s, where an optimal support of
s
SPCA is deﬁned as an index set S∗ ⊆ {1, . . . , n} of cardinality s such that supp(X ∗) ⊆ S∗
for an optimal solution X ∗ to SPCA. Asteris et al. [2] show that, in the single-component
case, it is possible to design an algorithm that identiﬁes O(nr) candidate supports in
O(nr+1) time, where r denotes the rank of the matrix K, among which lies an optimal
support. Therefore, if one considers matrices K whose rank r is a ﬁxed value, both the
number of candidate supports constructed and the running time of the algorithm are
polynomial in n. In this paper, we conﬁrm that ﬁxing the rank r of K is key in solving

2

SPCA in polynomial time, and not just in the single-component case, but for any number
d of principal components. Next, we formally state our ﬁrst main result.

Theorem 1. There is an algorithm that ﬁnds an optimal solution to SPCA in time

(cid:16)

(cid:17)
nmin{d,r}(r2+r)(min{d, r}nr2 + n log n)

,

O

where r denotes the rank of the input matrix K. In particular, the algorithm constructs
O(nmin{d,r}(r2+r)) candidate supports among which lies an optimal support.

If the rank r of K is a ﬁxed value, then both the number of candidate supports
constructed and the running time of the algorithm are polynomial in n. Theorem 1
constitutes the ﬁrst polynomial-time algorithm for SPCA, for any ﬁxed value of r. We
remark that the running time exponential dependence on r is expected, since SPCA is
NP-hard in its full generality. The proof of Theorem 1 is given in Section 3.

1.2 Sparse PCA with disjoint supports

In this paper, we study also sparse principal component analysis with disjoint supports,
which is a diﬀerent version of sparse principal component analysis which has been con-
sidered in the literature (see, e.g., [3]). Also in this model each principal component is a
linear combination of at most s features, but here no feature can be used by two diﬀerent
principal components. Given a matrix X, we denote by xi its ith column. Furthermore,
for a nonnegative integer d, we let [d] := {1, . . . , d}. With this notation, we can denote by
X the set of feasible matrices

X := {X ∈ Rn×d : |supp(xi)| ≤ s, (cid:107)xi(cid:107)2 = 1, ∀i ∈ [d],

supp(xi) ∩ supp(xi(cid:48)) = ∅, ∀i (cid:54)= i(cid:48) ∈ [d]},

Formally, in sparse principal component analysis with disjoint supports we are given an
n × n positive semideﬁnite matrix K, positive integers d, s smaller than n, and we seek an
optimal solution to the optimization problem

trace(X TKX).

max
X∈X

(SPCA-DS)

We remark that single-component SPCA is also a special case of SPCA-DS, obtained by
setting d = 1. Therefore, also SPCA-DS is NP-hard and hard to approximate.

Similarly to SPCA, the main diﬃculty in SPCA-DS consists in ﬁnding an optimal
support of SPCA-DS among all the O(nds) families of d index sets of cardinality at most
s, where an optimal support of SPCA-DS is deﬁned as a family of index sets {S∗
i }i∈[d] with
i ) ⊆ S∗
i ⊆ [n], |S∗
S∗
i ,
∀i ∈ [d], for an optimal solution X ∗ to SPCA-DS. Our second main result, stated below,
implies that we can construct O((dn)d2(r2+r)/2) candidate supports, among which lies an
optimal one.

i(cid:48) = ∅, ∀i (cid:54)= i(cid:48) ∈ [d], and such that supp(x∗

i | ≤ s, ∀i ∈ [d], S∗

i ∩ S∗

Theorem 2. There is an algorithm that ﬁnds an optimal solution to SPCA-DS in time

(cid:16)

(cid:17)
(dn)d2(r2+r)/2(dnr2 + d3n5 log n)

,

O

3

where r denotes the rank of the input matrix K. In particular, the algorithm constructs
O((dn)d2(r2+r)/2) candidate supports, among which lies an optimal support.

If r and d are ﬁxed values, then both the number of candidate supports constructed
and the running time of the algorithm are polynomial in n. Theorem 2 then yields the
ﬁrst polynomial-time algorithm for SPCA-DS, for any ﬁxed values of r and d. To the best
of our knowledge, the only other algorithm for SPCA-DS with theoretical guarantees is
given in [3], where the authors propose an algorithm that ﬁnds an (cid:15)-approximate solution
with running time polynomial in n and 1/(cid:15), provided that r and d are ﬁxed. The proof of
Theorem 2 can be found in Section 5.

1.3 Techniques

We brieﬂy explain the main techniques used in our two algorithms. To simplify the
exposition, we assume that r is a ﬁxed value in SPCA, and that both r and d are ﬁxed in
SPCA-DS.

The ﬁrst technique that we introduce is a dimensionality reduction approach which
allows us, in both problems, to replace our original matrix X of variables with a new
matrix Y of variables which has the advantage of having only a ﬁxed number of entries.
This approach can be seen as a multi-component generalization of the auxiliary unit
vector technique [22, 31, 26, 21, 20, 2], and has strong connections with procedures used
in principal component analysis when the original dimensionality n of the data is much
larger than the number of data vectors (see Section 23.1.1 in [29]).

The next technique is a tool from discrete geometry known as the hyperplane arrange-
ment theorem. A set H of p hyperplanes in a q-dimensional Euclidean space determines
a partition of the space called the arrangement of H. The hyperplane arrangement theo-
rem states that this arrangement consists of O(pq) full-dimensional polyhedra and can be
constructed in time O(pq). For more details, we refer the reader to [13], and in particular
to Theorem 3.3 therein. In both our algorithms, this theorem is employed to partition an
extended version of the space of variables Y in a polynomial number of polyhedra. Each
one will correspond to a candidate support that we construct, and at least one of them
will be optimal to the problem.

Finally, in the proof of Theorem 2, we reduce a restricted version of SPCA-DS to a
maximum-proﬁt integer circulation problem. This allows us to make use of the optimality
conditions for this problem and of the strongly polynomial-time algorithm by Goldberg and
Tarjan [14, 15]. First, the optimality conditions are exploited to obtain the arrangement
discussed above. Next, for each polyhedron in the arrangement, we select a vector in its
interior and apply Goldberg and Tarjan’s algorithm to the corresponding instance. The
output of the algorithm allows us to obtain the candidate support {Si}i∈[d] associated with
the polyhedron.

1.4 Computational complexity and practical applicability of our algo-

rithms

We remark that we do not expect that a direct implementation of our algorithms will lead
to practical algorithms for solving SPCA and SPCA-DS. Rather, our results demonstrate

4

that these problems are eﬃciently solvable from a theoretical point of view in the settings
considered. This is important, because once a problem is shown to be eﬃciently solvable,
usually practical algorithms follow (see, e.g., [5]).

We remark that our analysis of the algorithms can be improved in several ways to
obtain marginally better running times. For example, the hyperplane arrangement the-
orem is always used with a set H of p hyperplanes that pass through the origin in a q-
dimensional Euclidean space. In this special case, it is known that the arrangement consists
of O((p − 1)q−1) full-dimensional polyhedra and can be constructed in time O((p − 1)q−1).

2 A useful lemma

Before proving our main results, we present a lemma that uses standard eigenvalue argu-
ments. This lemma plays a crucial role in the dimensionality reduction performed by both
our algorithms. In particular, it implies that the optimal value of a PCA problem with an
input matrix of ﬁxed rank can be obtained by solving a diﬀerent PCA problem with an
input matrix of ﬁxed dimensions. In this paper, we denote by (cid:107)·(cid:107)F the Frobenius norm.
Lemma 1. Let M be an s × r matrix, let d be a positive integer, and let d(cid:48) := min{d, r}.
Then

max
X∈Rs×d
X TX=Id

(cid:107)M TX(cid:107)2

F = max

X∈Rs×d(cid:48)
X TX=Id(cid:48)

(cid:107)M TX(cid:107)2

F = max
Y ∈Rr×d(cid:48)
Y TY =Id(cid:48)

(cid:107)M Y (cid:107)2
F .

Proof. Denote by λj, for j ∈ [s], the eigenvalues of the s × s positive semideﬁnite matrix
M M T, and assume without loss of generality that λ1 ≥ λ2 ≥ · · · ≥ λs ≥ 0. Then

max
X∈Rs×d
X TX=Id

(cid:107)M TX(cid:107)2

F = max
X∈Rs×d
X TX=Id

trace(X TM M TX) =

d
(cid:88)

j=1

λj,

(1)

where in the ﬁrst equality we used the deﬁnition of Frobenius norm and the second is well
known (see, e.g., [29]).

Symmetrically, we obtain

max
X∈Rs×d(cid:48)
X TX=Id(cid:48)

(cid:107)M TX(cid:107)2

F = max

X∈Rs×d(cid:48)
X TX=Id(cid:48)

trace(X TM M TX) =

d(cid:48)
(cid:88)

j=1

λj.

(2)

Since the nonzero eigenvalues of M M T are at most rank(M M T) = rank(M ) ≤ r, we have
(cid:80)d
j=1 λj. Thus (1) and (2) coincide and we have shown the ﬁrst equality in

j=1 λj = (cid:80)d(cid:48)

the statement of the lemma.

Denote by µk, for k ∈ [r], the eigenvalues of the r × r positive semideﬁnite matrix
M TM , and assume without loss of generality that µ1 ≥ µ2 ≥ · · · ≥ µr ≥ 0. Similarly to
our previous derivations, we have

max
Y ∈Rr×d(cid:48)
Y TY =Id(cid:48)

(cid:107)M Y (cid:107)2

F = max
Y ∈Rr×d(cid:48)
Y TY =Id(cid:48)

trace(Y TM TM Y ) =

d(cid:48)
(cid:88)

k=1

µk.

(3)

5

Since the nonzero eigenvalues of M M T and M TM are the same, we have (cid:80)d(cid:48)
(cid:80)d(cid:48)

j=1 λj =
k=1 µk. Thus (2) and (3) coincide and we have shown the second equality in the state-

ment of the lemma.

3 Proof of Theorem 1

Consider SPCA where the input matrix K ∈ Rn×n has rank r. Since the matrix K is
positive semideﬁnite, it is well known that we can compute an n × r matrix R such that
K = RRT in O(n3) time, for instance using the Cholesky decomposition with complete
pivoting. Using the deﬁnition of Frobenius norm, SPCA takes the form

max
X∈Rn×d
X TX=Id, |supp(X)|≤s

(cid:107)RTX(cid:107)2
F .

(4)

We introduce some notation that will be used in this proof. For j ∈ [n], we denote
by Rj the jth row of R. Similarly, for S ⊆ [n], RS denotes the |S| × r submatrix of R
containing only the rows indexed by S. We also denote by d(cid:48) := min{d, r}.

As discussed in Section 1.1, the main diﬃculty in solving Problem (4) consists in
ﬁnding an optimal support S∗ of Problem (4). In fact, once S∗ is determined, an optimal
solution X ∗ to Problem (4) can be obtained by setting to zero the rows of X ∗ with indices
not in S∗, while the other rows of X ∗ can be obtained by solving the optimization problem

(cid:107)RT

S∗X(cid:107)2
F .

max
X∈Rs×d
X TX=Id

(5)

This is a PCA problem with an s×s input matrix. In particular, the input matrix RS∗RT
S∗
can be constructed in O(s2r) time and an optimal solution can be found in O(s3) time.
Based on this discussion, in the remainder of the proof it suﬃces to ﬁnd an optimal support
S∗ of Problem (4).

The next claim uses Lemma 1 to replace our matrix of variables X ∈ Rn×d in Prob-
lem (4) with an r × d(cid:48) matrix of variables, that we denote by Y . In the claim we consider
the following two optimization problems:

max
S⊆[n]
|S|=s

max
X∈Rs×d
X TX=Id

(cid:107)(RS)TX(cid:107)2
F ,

max
S⊆[n]
|S|=s

max
Y ∈Rr×d(cid:48)
Y TY =Id(cid:48)

(cid:107)RSY (cid:107)2
F .

(6)

(7)

We say that S∗ is an optimal support of Problem (6) if there exists X ∗ such that (S∗, X ∗)
is an optimal solution to Problem (6). Similarly, we say that S∗ is an optimal support of
Problem (7) if there exists Y ∗ such that (S∗, Y ∗) is an optimal solution to Problem (7).

Claim 1. The optimal supports of Problems (4), (6), (7) coincide.

6

Proof of claim. Lemma 1, applied with M := RS, implies that the optimal supports of
Problems (6) and (7) coincide. Thus we only need to show that the optimal supports of
Problems (4) and (6) coincide. To do so, it suﬃces to prove the following two statements:
(i) For every feasible solution (S, X) to Problem (6) with objective function value γ,
there is a feasible solution ˜X to Problem (4) with objective function value γ such that
supp( ˜X) ⊆ S; (ii) For every feasible solution ˜X to Problem (4) with objective function
value γ, there is a feasible solution (S, X) to Problem (6) with objective function value γ
such that supp( ˜X) ⊆ S.

(i). Let (S, X) be a feasible solution to Problem (6) with objective function value γ.
Let ˜X ∈ Rn×d be obtained from X by adding zero rows corresponding to the indices not
in S. Then ˜X is a feasible solution to Problem (4) with objective function value γ such
that supp( ˜X) ⊆ S.

(ii). Let ˜X be a feasible solution to Problem (4) with objective function value γ. Let
S be a subset of [n] of cardinality s containing supp( ˜X), and let X be obtained from ˜X
by dropping the (zero) rows with indices not in S. Then (S, X) is a feasible solution to
Problem (6) with objective function value γ such that supp( ˜X) ⊆ S.
(cid:5)

Due to Claim 1, in the rest of the proof our goal will be ﬁnding an optimal support of
Problem (7). Next, we deﬁne a restricted version of Problem (7), where we ﬁx the matrix
of variables Y ∈ Rr×d(cid:48)

:

(cid:107)RSY (cid:107)2
F .

max
S⊆[n]
|S|=s

We denote this restricted problem by RST(Y ). The next claim gives a simple characteri-
zation of the optimal solutions to Problem RST(Y ).

Claim 2. Let Y ∈ Rr×d(cid:48) be given. Then S∗ is an optimal solution to Problem RST(Y ) if
and only if S∗ ⊆ [n], |S∗| = s, and (cid:107)RjY (cid:107)2

2, ∀j ∈ S∗, ∀j(cid:48) ∈ [n] \ S∗.

2 ≥ (cid:107)Rj(cid:48)Y (cid:107)2

Proof of claim. This claim follows trivially by writing Problem RST(Y ) in the form

(cid:88)

j∈S

max
S⊆[n]
|S|=s

(cid:107)RjY (cid:107)2
2.

(cid:5)

Claim 2 implies that in order to ﬁnd an optimal solution to Problem RST(Y ), it is
suﬃcient to order all values (cid:107)RjY (cid:107)2
2, for j ∈ [n]. Therefore, our next task is to partition
all matrices Y ∈ Rr×d(cid:48)
based on the order of the values (cid:107)RjY (cid:107)2
2, for every j ∈ [n], that
they yield. Each (cid:107)RjY (cid:107)2
2, for j ∈ [n], is a quadratic polynomial in the entries of Y and
every monomial is a constant times the product of two variables in the same column of
Y , i.e., ykiyk(cid:48)i, for k, k(cid:48) ∈ [r], i ∈ [d(cid:48)]. Since we wish to obtain a polyhedral partition,
we introduce a new space of variables that allows us to write each (cid:107)RjY (cid:107)2
2, for j ∈ [n],
as a linear function. Formally, we deﬁne the space E that contains one variable for each
ykiyk(cid:48)i, for k, k(cid:48) ∈ [r], i ∈ [d(cid:48)]. The dimension of the space E is therefore d(cid:48) · (r2 + r)/2.

7

Note that, for each Y ∈ Rr×d(cid:48)
, there exists a unique corresponding point in E, that we
denote by ext(Y ), obtained by computing all the products ykiyk(cid:48)i, for k, k(cid:48) ∈ [r], i ∈ [d(cid:48)].
For each j ∈ [n], we can now write in time O(d(cid:48)r2) a linear function (cid:96)j : E → R such that
(cid:96)j(ext(Y )) = (cid:107)RjY (cid:107)2

2 for every matrix Y ∈ Rr×d(cid:48)

.

Claim 3. There exist a ﬁnite index set T of cardinality O(nd(cid:48)(r2+r)), full-dimensional
polyhedra P t ⊆ E, for t ∈ T , that cover E, and index sets St, for t ∈ T , with the following
property: For every t ∈ T , and for every Y such that ext(Y ) ∈ P t, St is an optimal solution
to Problem RST(Y ). The polyhedra P t, for t ∈ T , can be constructed in O(nd(cid:48)(r2+r)) time.
Furthermore, for each t ∈ T , St can be computed in O(d(cid:48)nr2 + n log n) time.

Proof of claim. For every two distinct indices j, j(cid:48) ∈ [n], the hyperplane

Hj,j(cid:48) := {z ∈ E : (cid:96)j(z) = (cid:96)j(cid:48)(z)}

(8)

partitions all points z ∈ E based on which of the two values (cid:96)j(z) and (cid:96)j(cid:48)(z) is larger. By
considering the hyperplane Hj,j(cid:48) for all distinct pairs of indices j, j(cid:48) ∈ [n], we obtain a set
H of (n2 − n)/2 ≤ n2 hyperplanes in E. By the hyperplane arrangement theorem, the
arrangement of H consists of O((n2)dim E ) = O(nd(cid:48)(r2+r)) full-dimensional polyhedra, and
can be constructed in O(nd(cid:48)(r2+r)) time. We denote by P t, for t ∈ T , the polyhedra in the
arrangement, where T is a ﬁnite index set of cardinality O(nd(cid:48)(r2+r)). From the deﬁnition
of the hyperplanes (8) we have that, if for some t ∈ T there exists a vector zt ∈ P t that
satisﬁes (cid:96)j(zt) > (cid:96)j(cid:48)(zt) for two distinct indices j, j(cid:48) ∈ [n], then every vector z ∈ P t must
satisfy (cid:96)j(z) ≥ (cid:96)j(cid:48)(z).

Next, we explain how the index sets St, for t ∈ T , are constructed. To do so, we ﬁx one
polyhedron P t, for some t ∈ T , until the end of the proof of the claim. The hyperplane
arrangement theorem also returns explicitly a vector zt in the interior of P t [13]. We then
compute (cid:96)j(zt) for every j ∈ [n] in time O(d(cid:48)nr2). Since zt is in the interior of P t, in time
O(n log n) we can ﬁnd an ordering jt

n of the indices 1, . . . , n such that

2, . . . , jt

1, jt

(cid:96)jt

1

(zt) > (cid:96)jt

2

(zt) > · · · > (cid:96)jt

n(zt).

From the property of the polyhedra in the arrangement we have that, for every z with
z ∈ P t,

In particular, for every Y with ext(Y ) ∈ P t, we have

(cid:96)jt

1

(z) ≥ (cid:96)jt

2

(z) ≥ · · · ≥ (cid:96)jt

n(z).

(cid:96)jt

1

(ext(Y )) ≥ (cid:96)jt

2

(ext(Y )) ≥ · · · ≥ (cid:96)jt

n(ext(Y )),

thus

(cid:107)Rjt

1

Y (cid:107)2

2 ≥ (cid:107)Rjt

2

Y (cid:107)2

2 ≥ · · · ≥ (cid:107)Rjt

nY (cid:107)2
2.

Claim 2 then implies that for each Y such that ext(Y ) ∈ P t, the set St := {jt
is an optimal solution to Problem RST(Y ).

1, jt

2, . . . , jt
s}
(cid:5)

8

Let S be the family of all index sets St obtained in Claim 3, namely

S := {St}t∈T .

Claim 4. The family S contains an optimal support of Problem (7).

Let (S∗, Y ∗) be an optimal solution to Problem (7). Then S∗ is an
Proof of claim.
optimal solution to the restricted Problem RST(Y ∗). Let P t, for t ∈ T , be a polyhedron
such that ext(Y ∗) ∈ P t, and let St ∈ S be the corresponding index set. From Claim 3,
St is an optimal solution to Problem RST(Y ∗). This implies that the solution (St, Y ∗) is
(cid:5)
also optimal to Problem (7).

Claim 4 implies that, in order to ﬁnd an optimal support of Problem (7), it suﬃces to

solve the |T | optimization problems

(cid:107)RStY (cid:107)2
F

∀t ∈ T.

(9)

max
Y ∈Rr×d(cid:48)
Y TY =Id(cid:48)

In fact, an index set St, for t ∈ T , which yields the maximum optimal value among
Problems (9) is then an optimal support of Problem (7). Each Problem (9) is a PCA
problem with an r × r input matrix.
StRSt can be
constructed in O(sr2) time and an optimal solution can be found in O(r3) time. This
completes the description of the algorithm and the proof of its correctness.

In particular, the input matrix RT

Next, we analyze the total running time of the algorithm presented. The matrix R
is computed in O(n3) time, the linear functions (cid:96)j, for j ∈ [n], are obtained in O(d(cid:48)nr2)
time, the polyhedra P t, for t ∈ T , are constructed O(|T |) time, the sets St, for t ∈ T ,
are computed in O(|T |(d(cid:48)nr2 + n log n)) time, the |T | PCA Problems (9) are solved in
O(|T |(sr2 + r3)) time, and the PCA Problem (5) is solved in O(s2r + s3) time. The total
running time is therefore

O (cid:0)|T |(d(cid:48)nr2 + n log n)(cid:1) = O

(cid:16)

(cid:17)
nd(cid:48)(r2+r)(d(cid:48)nr2 + n log n)

.

This concludes the proof of Theorem 1.

4 The maximum-proﬁt integer circulation problem

In the proof of Theorem 2 we will consider the maximum-proﬁt integer circulation problem.
Hence, before proceeding with the proof, we give a brief overview of this problem and we
present optimality conditions and a strongly polynomial-time algorithm to solve it.

Let D = (V, A) be a directed graph. A vector f ∈ RA is called a circulation if
f (δin(v)) = f (δout(v)) for each vertex v ∈ V , where δin(v) = {wv ∈ A} and δout(v) =
{vw ∈ A}. A circulation f is said to be integer if f has all integer entries. In a maximum-
proﬁt integer circulation problem we are given a directed graph D = (V, A), arc capacities
+, and arc proﬁts p ∈ QA. We say that f is a feasible circulation if f is an
u ∈ ZA
integer circulation in the directed graph D subject to 0 ≤ f ≤ u. The proﬁt of a feasible

9

circulation f is pTf . The goal of the maximum-proﬁt integer circulation problems is that
of ﬁnding an optimal circulation, which is a feasible circulation of maximum proﬁt. We
refer the reader to Chapters 11 and 12 in [28] for a thorough presentation of circulations
problems. We refer the reader to the same book [28] for standard graph theory deﬁnitions
including that of directed circuit and undirected circuit.

To state the optimality conditions for a maximum-proﬁt integer circulation problem,
it will be useful to consider the residual directed graph Df = (V, Af ) of a circulation f ,
where

Af := {a : a ∈ A, fa < ua} ∪ {a← : a ∈ A, fa > 0}.

Here a← := wu if a = uw. For a directed circuit C in Df , we deﬁne χC ∈ {0, ±1}A by:

χC
a :=

if C traverses a,


1

−1 if C traverses a←,

0

if C traverses neither a nor a←.

We then deﬁne, for every directed circuit C in Df its proﬁt as

p(C) =

χC
a pa.

(cid:88)

a∈A

We are now ready to state the optimality conditions, which follow, for example, from

Theorem 12.1 in [28].

Proposition 1. A feasible circulation f is optimal if and only if each directed circuit in
Df has nonpositive proﬁt.

The above optimality conditions are at the basis of Goldberg and Tarjan’s strongly
polynomial-time algorithm to solve the maximum-proﬁt integer circulation problem [14,
15]. We refer the reader to Section 12.3 in [28] for a description of the algorithm.

Proposition 2 (Corollary 12.2a in [28]). An optimal circulation can be found in O(|V |2|A|3 log |V |)
time.

5 Proof of Theorem 2

Consider SPCA-DS where the input matrix K ∈ Rn×n has rank r. Since the matrix K is
positive semideﬁnite, we can compute an n × r matrix R such that K = RRT in O(n3)
time, for example using the Cholesky decomposition with complete pivoting. The objective
i=1(cid:107)RTxi(cid:107)2
function of SPCA-DS can then be written as trace(X TKX) = (cid:107)RTX(cid:107)2
2
and SPCA-DS takes the form

F = (cid:80)d

max
X∈X

d
(cid:88)

(cid:107)RTxi(cid:107)2
2.

i=1

10

(10)

In this proof we use some of the notation introduced in the proof of Theorem 1.
Namely, for j ∈ [n], Rj denotes the jth row of R and, for S ⊆ [n], RS denotes the |S| × r
submatrix of R containing only the rows indexed by S.

As discussed in Section 1.2, the main diﬃculty in solving Problem (10) consists in
i }i∈[d] is determined,
i with
i can be obtained by solving the optimization

i , for i ∈ [d], can be obtained by setting to zero the entries of x∗

i }i∈[d] of Problem (10). In fact, once {S∗

i , while the other entries of x∗

ﬁnding an optimal support {S∗
each optimal vector x∗
indices not in S∗
problem

(cid:107)RT
S∗
i

xi(cid:107)2
2.

max
xi∈R|S∗
i |
(cid:107)xi(cid:107)2=1

(11)

i RT
S∗
i

This is a single-component PCA problem with an input matrix of dimension at most s × s.
can be constructed in O(s2r) time and an optimal
In particular, the input matrix RS∗
solution can be found in O(s3) time. Based on this discussion, in the remainder of the
proof it suﬃces to ﬁnd an optimal support {S∗

i }i∈[d] of Problem (10).

The next claim uses Lemma 1 to replace each vector of variables xi ∈ Rn in Prob-
lem (10) with a vector of variables yi ∈ Rr. In the claim we consider the following two
optimization problems:

max
Si⊆[n],|Si|≤s, ∀i∈[d]
Si∩Si(cid:48) =∅, ∀i(cid:54)=i(cid:48)∈[d]

max
xi∈R|Si|,(cid:107)xi(cid:107)2=1,
∀i∈[d]

d
(cid:88)

(cid:107)(RSi)Txi(cid:107)2
2,

i=1

max
Si⊆[n],|Si|≤s, ∀i∈[d]
Si∩Si(cid:48) =∅, ∀i(cid:54)=i(cid:48)∈[d]

max
yi∈Rr,(cid:107)yi(cid:107)2=1,
∀i∈[d]

d
(cid:88)

(cid:107)RSiyi(cid:107)2
2.

i=1

(12)

(13)

We say that {S∗
such that {(S∗
{S∗
{(S∗

i }i∈[d] is an optimal support of Problem (12) if there exist x∗

i , for i ∈ [d],
i )}i∈[d] is an optimal solution to Problem (12). Similarly, we say that
i , for i ∈ [d], such that

i }i∈[d] is an optimal support of Problem (13) if there exist y∗
i , y∗

i )}i∈[d] is an optimal solution to Problem (13).

i , x∗

Claim 5. The optimal supports of Problems (10), (12), (13) coincide.

Proof of claim. Lemma 1, applied d times with M := RSi, for i ∈ [d], implies that the
optimal supports of Problems (12) and (13) coincide. Thus we only need to show that
the optimal supports of Problems (10) and (12) coincide. To do so, it suﬃces to prove
the following two statements: (i) For every feasible solution {(Si, xi)}i∈[d] to Problem (12)
with objective function value γ, there is a feasible solution {˜xi}i∈[d] to Problem (10) with
objective function value γ such that supp(˜xi) ⊆ Si ∀i ∈ [d]; (ii) For every feasible solution
{˜xi}i∈[d] to Problem (10) with objective function value γ, there is a feasible solution
{(Si, xi)}i∈[d] to Problem (12) with objective function value γ such that supp(˜xi) ⊆ Si
∀i ∈ [d].

11

(i). Let {(Si, xi)}i∈[d] be a feasible solution to Problem (12) with objective function
value γ. For each i ∈ [d], let ˜xi ∈ Rn be obtained from xi by adding zero entries corre-
sponding to the indices not in Si. Then {˜xi}i∈[d] is a feasible solution to Problem (10)
with objective function value γ such that supp(˜xi) ⊆ Si ∀i ∈ [d].

(ii). Let {˜xi}i∈[d] be a feasible solution to Problem (10) with objective function value
γ. Let Si := supp(˜xi), for every i ∈ [d]. Let xi be obtained from ˜xi by dropping the (zero)
entries with indices not in Si. Then {(Si, xi)}i∈[d] is a feasible solution to Problem (12)
(cid:5)
with objective function value γ such that supp(˜xi) ⊆ Si ∀i ∈ [d].

Due to Claim 5, in the rest of the proof our goal will be ﬁnding an optimal support of

Problem (13).

5.1 The restricted problem

In this section we study the restricted version of Problem (13) obtained by ﬁxing the d vec-
tors of variables yi ∈ Rr, for i ∈ [d]. We denote this restricted problem by RST({yi}i∈[d]),
and formally deﬁne it as

max
Si⊆[n],|Si|≤s, ∀i∈[d]
Si∩Si(cid:48) =∅, ∀i(cid:54)=i(cid:48)∈[d]

d
(cid:88)

i=1

(cid:107)RSiyi(cid:107)2
2.

Our next goal is to provide a characterization of the optimal solutions to Problem RST({yi}i∈[d])
based on a maximum-proﬁt integer circulation problem. We refer the reader to Section 4
for a brief introduction to the maximum-proﬁt integer circulation problem.

In the remainder of the proof, we denote by D = (V, A) the directed graph with
vertices V = U ∪ W ∪ {t}, where U = {u1, . . . , ud}, W = {w1, . . . , wn}, and with arcs
A = A0 ∪ AU ∪ AW , where A0 = {uiwj
: i ∈ [d]},
AW = {wjt : j ∈ [n]}. The directed graph D is depicted in Figure 1. We deﬁne arc

: i ∈ [d], j ∈ [n]}, AU = {tui

Figure 1: The directed graph D = (V, A) considered in Section 5.1.

capacities u ∈ ZA

+ as ua := 1 if a ∈ A0 ∪ AW , ua := s if a ∈ AU . We also deﬁne arc

12

u1bu2budb...bw1bwn......bw2tbproﬁts p ∈ QA by pa := (Rjyi)2 if a = uiwj ∈ A0, pa := 0 if a ∈ AU ∪ AW . We then
deﬁne Problem CRC({yi}i∈[d]) as the maximum-proﬁt integer circulation problem on the
directed graph D = (V, A), with arc capacities u and arc proﬁts p. We remark that in
Problem CRC({yi}i∈[d]), only the arc proﬁts depend on {yi}i∈[d]. The next claim provides
a characterization of the optimal solutions to Problem RST({yi}i∈[d]) in terms of optimal
circulations to Problem CRC({yi}i∈[d]).

Claim 6. Let {yi}i∈[d] be given. Then {S∗
if and only if S∗
Problem CRC({yi}i∈[d]).

i := {j ∈ [n] : f ∗

i }i∈[d] is an optimal solution to Problem RST({yi}i∈[d])

uiwj = 1}, for i ∈ [d], where f ∗ is an optimal circulation to

Proof of claim. To prove the claim, it suﬃces to prove the following two statements:
(i) For every feasible solution {Si}i∈[d] to Problem RST({yi}i∈[d]) with objective function
value γ, there is a feasible circulation f to CRC({yi}i∈[d]) with proﬁt γ such that Si =
{j ∈ [n] : fuiwj = 1} ∀i ∈ [d]; (ii) For every feasible circulation f to CRC({yi}i∈[d]) with
proﬁt γ, the solution {Si}i∈[d] deﬁned by Si := {j ∈ [n] : fuiwj = 1} ∀i ∈ [d], is feasible
to Problem RST({yi}i∈[d]) and has objective function value γ. In the following, we ﬁrst
discuss the mapping between solutions and circulations in (i) and (ii), and then we discuss
the correspondence of objective function values and proﬁts in both (i) and (ii).

(i). Let {Si}i∈[d] be a feasible solution to Problem RST({yi}i∈[d]), i.e., Si ⊆ [n],
|Si| ≤ s, ∀i ∈ [d], and Si ∩ Si(cid:48) = ∅, ∀i (cid:54)= i(cid:48) ∈ [d]. For every pair i, j such that j ∈ Si, deﬁne
fuiwj := 1, fwj t := 1, and set fa := 0 for every other a ∈ A0 ∪ AW . For every i ∈ [d], deﬁne
ftui := |Si|. It can be easily checked that f is a feasible circulation to CRC({yi}i∈[d]) such
that Si = {j ∈ [n] : fuiwj = 1} ∀i ∈ [d].

(ii). Viceversa, let f be a feasible circulation to CRC({yi}i∈[d]). Since ua = 1 for every
a ∈ A0, we have fa ∈ {0, 1} for every a ∈ A0. For every i ∈ [d], deﬁne Si := {j ∈ [n] :
fuiwj = 1}. ua = 1 for every a ∈ AW implies that no j ∈ [n] is in more than one set Si,
thus Si ∩ Si(cid:48) = ∅, ∀i (cid:54)= i(cid:48) ∈ [d]. Since ua = s for every a ∈ AU , we also have |Si| ≤ s for
every i ∈ [d]. Therefore, {Si}i∈[d] is a feasible solution to Problem RST({yi}i∈[d]).

The claim follows since objective function values and proﬁts coincide in both mappings

(i) and (ii):

pTf =

d
(cid:88)

n
(cid:88)

i=1

j=1

puiwj fuiwj =

d
(cid:88)

(cid:88)

puiwj =

d
(cid:88)

(cid:88)

(Rjyi)2 =

i=1

j∈Si

i=1

j∈Si

d
(cid:88)

i=1

(cid:107)RSiyi(cid:107)2
2.

(cid:5)

5.2 A polynomial arrangement

Claim 6 implies that in order to ﬁnd an optimal solution to Problem RST({yi}i∈[d]), it is
suﬃcient to ﬁnd an optimal circulation to Problem CRC({yi}i∈[d]). Thus we now focus
on the latter problem. The optimality conditions stated in Proposition 1 imply that in
order to understand an optimal circulation to Problem CRC({yi}i∈[d]), it is important to
understand the sign of the proﬁts of all directed circuits in Df , for any feasible circulation
f . Note that any directed circuit C in Df , for a feasible circulation f , gives an undirected

13

circuit C(cid:48) in D. For an undirected circuit C(cid:48) in D, we deﬁne χC(cid:48) ∈ {0, ±1}A by:

χC(cid:48)
a :=


1

−1

0

if C(cid:48) traverses a forward,
if C(cid:48) traverses a backward,
if C(cid:48) does not traverse a.

We then deﬁne, for every undirected circuit C(cid:48) in D, its proﬁt as

p(C(cid:48)) =

χC(cid:48)
a pa.

(cid:88)

a∈A

In this way we obtain that, if a directed circuit C in Df , for some feasible circulation
f , gives the undirected circuit C(cid:48) in D, then we have p(C) = p(C(cid:48)). From the above
discussion, in order to understand the sign of the proﬁts of all directed circuits in Df , for
any feasible circulation f , it suﬃces to understand the signs of the proﬁts of all undirected
circuits in D. From now on, we denote by C the set of undirected circuits in D. The
structure of the directed graph D implies that each undirected circuit in C can contain at
most d vertices in W . Thus we obtain |C| = O((dn)d).

Our next task is to partition the dr-dimensional space of all d vectors {yi}i∈[d], where
each yi is in Rr, based on the sign of the values p(C(cid:48)), for every C(cid:48) ∈ C, that they yield.
Each p(C(cid:48)), for C(cid:48) ∈ C, can be written as a linear function of arc proﬁts

p(C(cid:48)) =

(cid:88)

a∈A

χC(cid:48)
a pa =

(cid:88)

χC(cid:48)
uiwj puiwj .

uiwj ∈A0

Each arc proﬁt puiwj = (Rjyi)2, for i ∈ [d], j ∈ [n], is a quadratic polynomial in the entries
of the vector yi, and every monomial is a constant times the product of two variables in the
vector yi, i.e., (yi)k(yi)k(cid:48), for k, k(cid:48) ∈ [r]. Since we wish to obtain a polyhedral partition, we
introduce a new space of variables that allows us to write each p(C(cid:48)), for C(cid:48) ∈ C, as a linear
function. Formally, we deﬁne the space E that contains one variable for each (yi)k(yi)k(cid:48),
for i ∈ [d], k, k(cid:48) ∈ [r]. The dimension of the space E is therefore d·(r2 +r)/2. Note that, for
every d vectors {yi}i∈[d], where each yi is in Rr, there exists a unique corresponding point
in E, that we denote by ext({yi}i∈[d]), obtained by computing all the products (yi)k(yi)k(cid:48),
for i ∈ [d], k, k(cid:48) ∈ [r]. For each arc uiwj, i ∈ [d], j ∈ [n], we can now write in time O(r2) a
linear function (cid:96)uiwj : E → R such that (cid:96)uiwj (ext({yi}i∈[d])) = (Rjyi)2 for every {yi}i∈[d].
As a consequence, for each C(cid:48) ∈ C, we can write a linear function (cid:96)C(cid:48) : E → R such that
(cid:96)C(cid:48)(ext({yi}i∈[d])) = p(C(cid:48)) for every {yi}i∈[d]. Note that all these linear functions can be
constructed in time O(dnr2 + dr2|C|) = O(dnr2 + dd+1r2nd).

Claim 7. There exist a ﬁnite index set T of cardinality O((dn)d2(r2+r)/2), full-dimensional
polyhedra P t ⊆ E, for t ∈ T , that cover E, and index sets {St
i }i∈[d], for t ∈ T , with the
following property: For every t ∈ T , and for every {yi}i∈[d] such that ext({yi}i∈[d]) ∈ P t,
i }i∈[d] is an optimal solution to Problem RST({yi}i∈[d]). The polyhedra P t, for t ∈ T ,
{St
can be constructed in O((dn)d2(r2+r)/2)) time. Furthermore, for each t ∈ T , {St
i }i∈[d] can
be computed in O(dnr2 + d3n5 log n) time.

14

Proof of claim. For every C(cid:48) ∈ C, the hyperplane

HC(cid:48) := {z ∈ E : (cid:96)C(cid:48)(z) = 0}

(14)

partitions all points z ∈ E based on the sign of (cid:96)C(cid:48)(z). By considering the hyperplane HC(cid:48)
for all C(cid:48) ∈ C, we obtain a set H of |C| = O((dn)d) hyperplanes in E. By the hyperplane
arrangement theorem, the arrangement of H consists of O((dn)d·dim E ) = O((dn)d2(r2+r)/2)
full-dimensional polyhedra, and can be constructed in O((dn)d2(r2+r)/2) time. We denote
by P t, for t ∈ T , the polyhedra in the arrangement, where T is a ﬁnite index set of
cardinality O((dn)d2(r2+r)/2). From the deﬁnition of the hyperplanes (14) we have that,
if for some t ∈ T there exists a vector zt ∈ P t that satisﬁes (cid:96)C(cid:48)(zt) < 0 for some C(cid:48) ∈ C,
then every vector z ∈ P t must satisfy (cid:96)C(cid:48)(z) ≤ 0.
Next, we explain how the index sets {St

i }i∈[d], for t ∈ T , are constructed. To do so,
we ﬁx one polyhedron P t, for some t ∈ T , until the end of the proof of the claim. Due
to Claim 6, it suﬃces to show that we can construct a circulation f that is an optimal
circulation to every Problem CRC({yi}i∈[d]) for all {yi}i∈[d] with ext({yi}i∈[d]) ∈ P t. To
obtain this optimal circulation we will use a vector zt in the interior of P t, which is returned
explicitly by the hyperplane arrangement theorem [13]. Then, we deﬁne Problem CRC(zt)
as the problem obtained from Problem CRC({yi}i∈[d]) for any {yi}i∈[d] with ext({yi}i∈[d]) ∈
P t, by replacing the arc proﬁts with the one induced by zt. Precisely, Problem CRC(zt) is
the maximum-proﬁt integer circulation problem on the directed graph D = (V, A) deﬁned
+ deﬁned in Section 5.1, and arc proﬁts pt ∈ QA
in Section 5.1, with arc capacities u ∈ ZA
deﬁned by pt
a := 0 if a ∈ AU ∪ AW . Note that these arc
proﬁts can be computed in time O(dnr2).

a := (cid:96)uiwj (zt) if a = uiwj ∈ A0, pt

From Proposition 2, an optimal circulation f ∗ to Problem CRC(zt) can be found
in O(|V |2|A|3 log |V |) time. Since |V | = O(n) and |A| = O(dn), we can obtain f ∗
in O(d3n5 log n) time. We now show that f ∗ is an optimal circulation to every Prob-
lem CRC({yi}i∈[d]) for all {yi}i∈[d] with ext({yi}i∈[d]) ∈ P t. So we ﬁx an arbitrary {¯yi}i∈[d]
with ext({¯yi}i∈[d]) ∈ P t. In the remainder of the proof we will denote by pt the proﬁts in
Problem CRC(zt) and by ¯p the proﬁts in Problem CRC({¯yi}i∈[d]). Since f ∗ is a feasible
circulation to Problem CRC(zt), it is also a feasible circulation to Problem CRC({¯yi}i∈[d]).
This is because the two problems share the same directed graph and the same arc capaci-
ties. Furthermore, the residual directed graph Df ∗ is the same in both problems. From the
optimality conditions stated in Proposition 1, we know that pt(C) ≤ 0 for every directed
circuit C in Df ∗. From the deﬁnition of the hyperplanes (14) and the fact that zt is in
the interior of P t, we obtain that pt(C) < 0 for every directed circuit C in Df ∗. Since
ext({¯yi}i∈[d]) ∈ P t, we then have ¯p(C) ≤ 0 for every directed circuit C in Df ∗. Again from
the optimality conditions in Proposition 1, we obtain that f ∗ is an optimal circulation to
Problem CRC({¯yi}i∈[d]). We have thereby shown that f ∗ is an optimal circulation to every
Problem CRC({yi}i∈[d]) for all {yi}i∈[d] with ext({yi}i∈[d]) ∈ P t. An optimal solution to all
Problems RST({yi}i∈[d]) for all {yi}i∈[d] with ext({yi}i∈[d]) ∈ P t can then be obtained as
described in Claim 6. The total running time to compute {St
i }i∈[d] is O(dnr2 + d3n5 log n)
(cid:5)

Let S be the family of all index sets {St
S := {{St

i }i∈[d]}t∈T .

i }i∈[d] obtained in Claim 7, namely

15

Claim 8. The family S contains an optimal support of Problem (13).

i , y∗

i )}i∈[d] be an optimal solution to Problem (13). Then {S∗

Proof of claim. Let {(S∗
is an optimal solution to the restricted Problem RST({y∗
a polyhedron such that ext({y∗
index sets. From Claim 7, {St
This implies that the solution {St

i }i∈[d]) ∈ P t, and let {St
i }i∈[d] is an optimal solution to Problem RST({y∗
i }i∈[d] is also optimal to Problem (13).

i }i∈[d]
i }i∈[d]). Let P t, for t ∈ T , be
i }i∈[d] ∈ S be the corresponding
i }i∈[d]).
(cid:5)

i , y∗

Claim 8 implies that, in order to ﬁnd an optimal support of Problem (13), it suﬃces

to solve the |T | optimization problems

max
yi∈Rr,(cid:107)yi(cid:107)2=1,
∀i∈[d]

d
(cid:88)

(cid:107)RSt

i

i=1

yi(cid:107)2
2

∀t ∈ T.

(15)

In fact, a {St
i }i∈[d], for t ∈ T , which yields the maximum optimal value among Prob-
lems (15) is then an optimal support of Problem (13). Each Problem (15) can be decom-
posed into the d optimization problems

(cid:107)RSt

i

yi(cid:107)2
2

max
yi∈Rr
(cid:107)yi(cid:107)2=1

∀i ∈ [d].

(16)

Each Problem (16) is a single-component PCA problem with an r × r input matrix. In
can be constructed in O(sr2) time and an optimal
particular, the input matrix RT
St
i
solution can be found in O(r3) time. This completes the description of the algorithm and
the proof of its correctness.

RSt

i

Next, we analyze the total running time of the algorithm presented. The matrix
R is computed in O(n3) time, the linear functions (cid:96)C(cid:48), for C(cid:48) ∈ C, are constructed in
O(dnr2 + dd+1r2nd) time, the polyhedra P t, for t ∈ T , are constructed O(|T |) time, the
i }i∈[d], for t ∈ T , are computed in O(|T |(dnr2 + d3n5 log n)) time, the |T |d PCA
sets {St
Problems (16) are solved in O(|T |d(sr2 + r3)) time, and the d PCA Problems (11) are
solved in O(d(s2r + s3)) time. The total running time is therefore

O (cid:0)|T |(dnr2 + d3n5 log n)(cid:1) = O

(cid:16)

(cid:17)
(dn)d2(r2+r)/2(dnr2 + d3n5 log n)

.

This concludes the proof of Theorem 2.

References

[1] Asteris, M., Papailiopoulos, D., Karystinos, G.: Sparse principal component of a

rank-deﬁcient matrix. In: Proceedings of ISIT (2011)

[2] Asteris, M., Papailiopoulos, D., Karystinos, G.: The sparse principal component of
a constant-rank matrix. IEEE Transactions on Information Theory pp. 2281–2290
(2014)

16

[3] Asteris, M., Papailiopoulos, D., Kyrillidis, A., Dimakis, A.: Sparse PCA via bipartite

matchings. In: Proceedings of NIPS (2015)

[4] Berk, L., Bertsimas, D.: Certiﬁably optimal sparse principal component analysis.

Mathematical Programming Computation 11, 381–420 (2019)

[5] Bertsimas, D., Tsitsiklis, J.: Introduction to Linear Optimization. Athena Scientiﬁc,

Belmont, MA (1997)

[6] Boutsidis, C., Drineas, P., Magdon-Ismail, M.: Sparse features for PCA-like linear

regression. In: Proceedings of NIPS, pp. 2285–2293 (2011)

[7] Cadima, J., Jolliﬀe, I.: Loading and correlations in the interpretation of principle

compenents. Journal of Applied Statistics 22(2), 203–214 (1995)

[8] Chan, S., Papailiopoulos, D., Rubinstein, A.: On the worst-case approximability of

sparse PCA. Proceedings of COLT (2016)

[9] d’Aspremont, A., Bach, F., Ghaoui, L.: Optimal solutions for sparse principal com-
ponent analysis. The Journal of Machine Learning Research 9, 1269–1294 (2008)

[10] d’Aspremont, A., Bach, F., Ghaoui, L.: Approximation bounds for sparse principal
component analysis. Mathematical Programming, Series B pp. 89–110 (2014)

[11] d’Aspremont, A., El Ghaoui, L., Jordan, M., Lanckriet, G.: A direct formulation for

sparse PCA using semideﬁnite programming. SIAM review 49(3), 434–448 (2007)

[12] Dey, S., Mazumder, R., Wang, G.: A convex integer programming approach for

optimal sparse PCA. arXiv preprint arXiv:1810.09062 (2018)

[13] Edelsbrunner, H., O’Rourke, J., Seidel, R.: Constructing arrangements of lines and
hyperplanes with applications. SIAM Journal on Computing 15(2), 341–363 (1986)

[14] Goldberg, A., Tarjan, R.: Finding minimum-cost circulations by canceling negative

cycles. In: Proceedings of STOC, pp. 388–397 (1988)

[15] Goldberg, A., Tarjan, R.: Finding minimum-cost circulations by canceling negative
cycles. Journal of the Association for Computing Machinery 36, 873–886 (1989)

[16] Hastie, T., Tibshirani, R., Wainwright, M.: Statistical learning with sparsity. CRC

press (2015)

[17] He, Y., Monteiro, R., Park, H.: An eﬃcient algorithm for rank-1 sparse PCA. working

paper (2010)

[18] Jolliﬀe, I., Trendaﬁlov, N., Uddin, M.: A modiﬁed principal component technique
based on the lasso. Journal of Computational and Graphical Statistics 12(3), 531–
547 (2003)

17

[19] Journ´ee, M., Nesterov, Y., Richt´arik, P., Sepulchre, R.: Generalized power method
for sparse principal component analysis. The Journal of Machine Learning Research
11, 517–553 (2010)

[20] Karystinos, G., Liavas, A.: Eﬃcient computation of the binary vector that maximizes
a rank-deﬁcient quadratic form. IEEE Transactions on Information Theory 56(7),
3581–3593 (2010)

[21] Karystinos, G., Pados, D.: Rank-2-optimal adaptive design of binary spreading codes.

IEEE Transactions on Information Theory 53(9), 3075–3080 (2007)

[22] Mackenthun, K.: A fast algorithm for multiple-symbol diﬀerential detection of MPSK.

IEEE Transactions on Communications 42(2/3/4), 1471–1474 (1994)

[23] Mackey, L.: Deﬂation methods for sparse PCA. In: Proceedings of NIPS, vol. 21, pp.

1017–1024 (2009)

[24] Magdon-Ismail, M.: NP-hardness and inapproximability of sparse PCA. Information

Processing Letters pp. 35–38 (2017)

[25] Moghaddam, B., Weiss, Y., Avidan, S.: Spectral bounds for sparse PCA: Exact and

greedy algorithms. In: Proceedings of NIPS, vol. 18, p. 915 (2006)

[26] Motedayen, I., Krishnamoorthy, A., Anastasopoulos, A.: Optimal

joint detec-
tion/estimation in fading channels with polynomial complexity. IEEE Transactions
on Information Theory 53(1), 209–223 (2007)

[27] Papailiopoulos, D., Dimakis, A., Korokythakis, S.: Sparse PCA through low-rank

approximations. In: Proceedings of ICML (2013)

[28] Schrijver, A.: Combinatorial Optimization. Polyhedra and Eﬃciency. Springer-

Verlag, Berlin (2003)

[29] Shalev-Shwartz, S., Ben-David, S.: Understanding Machine Learning. Cambridge

University Press (2014)

[30] Sigg, C., Buhmann, J.: Expectation-maximization for sparse and non-negative PCA.

In: Proceedings of ICML, pp. 960–967 (2008)

[31] Sweldens, W.: Fast block noncoherent decoding. IEEE Communications Letters 5(4),

132–134 (2001)

[32] Vu, V., Lei, J.: Minimax rates of estimation for sparse PCA in high dimensions. In:

Proceedings of AIStats, pp. 1278–1286 (2012)

[33] Yuan, X., Zhang, T.: Truncated power method for sparse eigenvalue problems. Jour-

nal of Machine Learning Research 14, 899–925 (2013)

[34] Zhang, Y., d’Aspremont, A., L., G.: Sparse PCA: Convex relaxations, algorithms and
applications. In: Handbook on Semideﬁnite, Conic and Polynomial Optimization, pp.
915–940. Springer (2012)

18

[35] Zou, H., Hastie, T., Tibshirani, R.: Sparse principal component analysis. Journal of

computational and graphical statistics 15(2), 265–286 (2006)

19

