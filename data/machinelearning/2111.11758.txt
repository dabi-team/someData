Understanding the impact of data distribution
on Q-learning with function approximation

2
2
0
2

n
u
J

5
2

]

G
L
.
s
c
[

2
v
8
5
7
1
1
.
1
1
1
2
:
v
i
X
r
a

Pedro P. Santos1,2

Francisco S. Melo1,2

Alberto Sardinha1,2

Diogo S. Carvalho1,2

1INESC-ID.
2Instituto Superior Técnico, University of Lisbon.

Abstract

In this work, we study the interplay between the
data distribution and Q-learning-based algorithms
with function approximation. We provide a uni-
ﬁed theoretical and empirical analysis as to why
different properties of the data distribution can con-
tribute to regulate sources of algorithmic instability.
First, we study theoretical bounds on the perfor-
mance of approximate dynamic programming al-
gorithms. Second, we provide a novel four-state
MDP that highlights the impact of the data distribu-
tion in the performance of a Q-learning algorithm
with function approximation, both in online and
ofﬂine settings. Finally, we experimentally assess
the impact of the data distribution properties in
the performance of ofﬂine Q-learning-based algo-
rithms. Our results emphasize the key role played
by data distribution properties while regulating al-
gorithmic stability. We provide a systematic and
comprehensive study of the topic by connecting
different lines of research, as well as validating and
extending previous results, both theoretically and
empirically.

1 INTRODUCTION

Recent years witnessed signiﬁcant progress in solving chal-
lenging problems across various domains using reinforce-
ment learning (RL) [Mnih et al., 2015, Silver et al., 2017,
Lillicrap et al., 2016]. Q-learning algorithms with function
approximation are among the most used methods. However,
extending of Q-learning convergence guarantees to function
approximation settings is hard, especially for the case of
large capacity approximators, and several works analyze the
unstable behavior of such algorithms both experimentally
[van Hasselt et al., 2018, Fu et al., 2019] and theoretically
[Zhang et al., 2021, Carvalho et al., 2020].

However, only a few works focus their attention on the study
of the interplay between the data distribution, i.e., the distri-
bution used to sample experience or the distribution induced
by a dataset of transitions, and the outcome of the learning
process [Kumar et al., 2020a, Fu et al., 2019]. Such research
direction is the main focus of this work and comprises an
important object of study since previous articles identiﬁed
it as a potential source of instability [Kumar et al., 2020a].

We center our study around the following research question:
which data distributions lead to improved algorithmic stabil-
ity and performance? Hence, we investigate how different
data distribution properties inﬂuence performance in the
context of off-policy RL algorithms with function approxi-
mation. We primarily focus our attention on ofﬂine control
RL settings [Levine et al., 2020] with discrete action spaces,
in which an RL algorithm aims to learn reward-maximizing
behavior using previously collected data without additional
interaction with the environment. Nevertheless, our conclu-
sions are also relevant in online RL settings, particularly for
algorithms that rely on large-scale replay buffering.

This work contributes to a deeper understanding regarding
the inﬂuence of the data distribution properties in the perfor-
mance of approximate dynamic programming (ADP) meth-
ods. We provide a systematic and comprehensive study of
the topic by connecting different lines of research, as well as
validating and extending previous results, both theoretically
and empirically. In particular, we ﬁrst review theoretical
bounds on the performance of ADP methods. We provide
new interpretations of currently proposed bounds, highlight
the close relationship between different properties of the
data distribution and the tightness of the bounds, and moti-
vate high entropy distributions from a game-theoretical point
of view in the form of a novel theorem. Second, we propose
a new four-state MDP that illustrates how the data distribu-
tion can impact algorithmic performance both in online and
ofﬂine RL settings. Finally, we experimentally assess the
impact of the data distribution in the performance of ofﬂine
Q-learning-based algorithms with function approximation
under a diverse set of environments.

Preliminary work. Correspondence to: Pedro P. Santos <pedro.pinto.santos@tecnico.ulisboa.pt>.

 
 
 
 
 
 
2 BACKGROUND

pseudocode of a generic Q-learning algorithm with function
approximation can be found in Appendix A.

,

(

S

S

∈

P

A

(
S

∈ A

, p, p0, r, γ), where

S
S × A →

denotes the action space, p :

In RL [Sutton and Barto, 2018], the agent-environment
interaction is modeled as an MDP, formally deﬁned as a
denotes the state space,
tuple (
) is
(
S
S × A → P
) being
S
) is the initial
, p0 ∈ P
R is the reward func-
(0, 1) is a discount factor. At each step,

A
the state transition probability function with
the set of distributions on
state distribution, r :
tion, and γ
the agent observes the state of the environment st
∈ S
and chooses an action at
. Depending on the chosen
action, the environment evolves to state st+1 ∈ S
with
probability p(st+1|
st, at), and the agent receives a reward
rt with expectation given by r(st, at). A policy, π(a
s) :
|
), is a mapping encoding the preferences of the
(
S → P
A
agent. We denote by P π the
matrix with elements
|S| × |S|
P π(st, st+1) = Ea
st, a)]. A trajectory,
st) [p(st+1|
∼
), comprises a sequence of states
τ = (s0, a0, ..., s
, a
and actions. The probability of trajectory τ under policy
π is given by ρπ(τ ) = p0(s0) (cid:81)∞t=0 π(at
st, at).
The discounted reward objective can be written as J(π) =
ρπ [(cid:80)∞t=0 γtr(st, at)] . The objective of the agent is to
Eτ
ﬁnd an optimal policy π∗ that maximizes the above objec-
tive function, i.e., J(π∗)
π. RL algorithms usu-
ally involve the estimation of the optimal Q-function, Q∗,
satisfying the Bellman optimality equation: Q∗(st, at) =
r(st, at) + γEst+1

Q∗(st+1, at+1)(cid:3) .

st)p(st+1|

(cid:2)maxat+1

J(π),

π(a
|

≥

∞

∞

∀

∼

p

|

∼

∈A

Q-learning [Watkins and Dayan, 1992] allows an agent
to learn directly from raw experience in an online,
incremental fashion, by estimating optimal Q-values
from observed trajectories using the temporal dif-
ference (TD) update rule: Q(st, at)
Q(st, at) +
Q(st, at)(cid:3) . Con-
α (cid:2)rt + γ maxat+1
Q(st+1, at+1)
vergence to the optimal policy is guaranteed if all
state-action pairs are visited inﬁnitely often and the learning
rate α is appropriately decayed. Usually, in order to
space, an (cid:15)-greedy policy
adequately explore the
is used. An (cid:15)-greedy policy chooses, most of the time, an
action that has maximal Q-value for the current state, but
with probability (cid:15) selects a random action instead.

S × A

←

∈A

−

In ADP, Q-values are approximated by a differentiable
function Qφ, where φ denotes the learnable parameters
of the model. ADP algorithms, such as the well-known
deep Q-network (DQN) algorithm [Mnih et al., 2015], usu-
ally interleave two phases: (i) a sampling phase, where pa-
rameters φ are kept ﬁxed and a behavior policy (e.g., (cid:15)-
greedy policy) is used to collect transitions (st, at, rt, st+1)
and store them into a replay buffer (denoted by
); and
(ii) an update phase, where transitions are sampled from
φ =
L
B
Qφ(st, at))2],
E
is minimized, where φ− denotes the parameters of the target
network Qφ− , a periodic copy of the behavior network. The

and used to update parameters φ such that
[(rt + γ maxat+1

Qφ− (st+1, at+1)

∈A

−

B

B

Ofﬂine RL [Levine et al., 2020] aims at ﬁnding an opti-
mal policy with respect to J(π) using a static dataset of
experience. The fundamental problem of ofﬂine RL is distri-
butional shift: out-of-distribution samples lead to algorith-
mic instabilities and performance loss, both at training and
deployment time. The conservative Q-learning (CQL) [Ku-
mar et al., 2020b] algorithm is an ofﬂine RL algorithm that
aims to estimate the optimal Q-function using ADP tech-
niques, while mitigating the impact of distributional shift.
Precisely, the algorithm avoids the overestimation of out-
of-distribution actions by considering an additional conser-
ν[Qφ(s, a)],
vative penalty term of the type
+
0 ,
yielding the global objective
c, k
R
which the algorithm aims to minimize. Distribution ν(a
s)
|
is chosen adversarially such that it selects overestimated
Q-values with high probability, e.g., by maximizing

,a
∼
∼B
φ + k
L

L
CQL =

c = Es

L

L

∈

c.

L

3 RELATED WORK

There are different lines of research closely related to our
work. We start by referring early studies that analyze the
unstable behavior of off-policy learning algorithms and the
harmful learning dynamics that can lead to the divergence
of the function parameters [Kolter, 2011, Tsitsiklis and van
Roy, 1996, Tsitsiklis and Van Roy, 1997, Baird, 1995]. Dif-
ferent works provide examples that highlight the unstable be-
havior of ADP methods [Baird, 1995, Tsitsiklis and van Roy,
1996, Kolter, 2011]. Kolter [2011] provides an example that
highlights the dependence of the off-policy distribution on
the approximation error of the algorithm. In Sec. 4.2, we
propose a novel four-state MDP that highlights the impact
of the data distribution in the performance of ADP meth-
ods. We further explore how off-policy algorithms can be
affected by data distribution changes, under diverse settings.
Unlike previous works, we consider both ofﬂine settings
comprising static data distributions, and online settings in
which data distributions are induced by replay buffers.

Another line of research related to our study involves works
that analyze error propagation in ADP methods, deriving
error bounds on the performance of approximate policy itera-
tion [Kakade and Langford, 2002, Munos, 2003] and approx-
imate value iteration [Munos, 2005, Munos and Szepesvári,
2008] algorithms. Munos [2003] provides error bounds for
approximate policy iteration using quadratic norms, as well
as bounds on the error between the performance of the
policies induced by the value iteration algorithm and the
optimal policy as a function of weighted Lp-norms of the
approximation errors [Munos, 2005]. Munos and Szepesvári
[2008] develop a theoretical analysis of the performance of
sampling-based ﬁtted value iteration, providing ﬁnite-time
bounds on the performance of the algorithm. Yang et al.
[2019] establish algorithmic and statistical rates of conver-

gence for the iterative sequence of Q-functions obtained by
the DQN algorithm. Chen and Jiang [2019] further improve
the bounds of the previous studies, while considering an
ofﬂine RL setting. Common to all these works is the depen-
dence of the derived bound on a data distribution-related
concentrability coefﬁcient. In this work, we review concen-
trability coefﬁcients in Sec. 4.1, providing new insights and
a novel motivation for the use of high entropy data distri-
butions through the lens of robust optimization. We also
connect our experimental results, presented in Sec. 5, with
theoretical results from the aforementioned articles.

Our study is also related with recent works that investigate
the stability of deep RL methods [van Hasselt et al., 2018,
Fu et al., 2019, Kumar et al., 2019, 2020a, Liu et al., 2018,
Zhang et al., 2021], as well as the development of RL meth-
ods speciﬁcally suited for ofﬂine settings [Agarwal et al.,
2019, Mandlekar et al., 2021, Levine et al., 2020]. Kumar
et al. [2020a] identify that Q-learning-related methods can
exhibit pathological interactions between the data distribu-
tion and the policy being learnt, leading to potential insta-
bility. Fu et al. [2019] investigate how different components
of DQN play a role in the emergence of the deadly triad. In
particular, the authors assess the performance of DQN with
different sampling distributions, ﬁnding that higher entropy
distributions tend to perform better. Agarwal et al. [2019]
provide a set of ablation studies that highlight the impact
of the dataset size and diversity in ofﬂine learning settings.
In this work, we provide a study of the impact of different
data distribution properties in the performance of ofﬂine
RL algorithms in Sec. 5. We validate and extend previous
results, as well as connect our experimental ﬁndings with
theoretical results. As opposed to previous works, we pro-
vide a systematic study of the topic with thorough attention
to detail, featuring experimental setups explicitly designed
to rigorously study the impact of the data distribution in the
stability of the ofﬂine learning algorithms.

Also related to our work, ofﬂine RL datasets have been re-
cently proposed [Fu et al., 2020, Gülçehre et al., 2020, Qin
et al., 2021] in an attempt to benchmark progress in ofﬂine
RL. Such datasets, which are usually collected by running
an exploratory policy on the environment or by storing the
contents of the replay buffer at a given point during training,
usually differ in properties such as the data diversity and
size. In this work, we chose not to use such datasets in order
to have complete control over the dataset generation proce-
dure, which allows us to rigorously control different metrics
of the datasets and systematically compare our experimen-
tal results. Nevertheless, our results are representative of a
diverse set of discrete action-space control tasks.

Finally, and concurrently to our work, Schweighofer et al.
[2021] study the impact of dataset characteristics on ofﬂine
RL. Precisely, the authors study the inﬂuence of the average
dataset return and state-action coverage on the performance
of different RL algorithms while controlling the dataset

generation procedure. Despite some similarity with some
of the experiments in Sec. 5, we present a much broader
picture regarding the impact of the data distribution in the
stability of general off-policy RL algorithms, from both
theoretical and experimental point-of-views. Additionally,
the experimental methodology carried out by both works
differs in several aspects, such as the calculation of the
dataset metrics used for the presentation of the experimental
results or the types of environments used.

4 THE DATA DISTRIBUTION MATTERS

In this section, we investigate how the data distribution
can impact the performance of Q-learning-based algorithms
with function approximation, theoretically and empirically.
We consider both online and ofﬂine learning settings.

4.1 THEORETICAL BOUNDS

As discussed in the previous section, different studies ana-
lyze error propagation in ADP [Munos, 2003, 2005, Munos
and Szepesvári, 2008, Yang et al., 2019, Chen and Jiang,
2019]. Importantly, the tightness of the error upper bounds
derived by the aforementioned works depends on different
sources of error, one of which consists of a concentrabil-
ity coefﬁcient C that is dependent on the properties of the
data/sampling distribution. The lower the concentrability
coefﬁcient C, the tighter the bound.

Munos [2003] introduces the ﬁrst version of this data-
dependent concentrability coefﬁcient, C1, which corre-
sponds to a bound on the density of the transition probability
function. Speciﬁcally, C1 = sups
,
,a
||∞
)1 denotes the sampling distribution. Munos
where µ
[2005] introduces a different concentrability coefﬁcient, C2,
related to the discounted average concentrability of future-
states on the MDP. Speciﬁcally,

s, a)/µ

∈A ||

∈ P

p(

∈S

S

·|

(

C2 = (1

γ)2 ∞(cid:88)

m=1

−

mγm

1c(m),

−

(1)

c(m) = sup

π1,...,πm

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρP π1P π2 ...P πm
µ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(2)

∞

(
S

∈ P

), and ρ reﬂects the importance of various re-
µ, ρ
gions of the state space and is selected by the practitioner.
Munos [2005] and Munos and Szepesvári [2008] note that
the assumption introduced by C1 is stronger than the as-
sumption introduced by (1), which expresses some smooth-
ness property of the future state distribution with respect to
µ for an initial distribution ρ.

Farahmand et al. [2010] and Yang et al. [2019] further
improve the aforementioned error bounds, replacing the

1The discussion generalizes for µ ∈ P(S × A).

supremum-norm in (2) with a weighted norm

4.1.2 Motivating Maximum Entropy Distributions

c(m) = sup

π1,...,πm

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρP π1P π2...P πm
µ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2,µ

.

(3)

We let C3 denote the coefﬁcient deﬁned by (1) and (3). Other
works [Antos et al., 2008, Lazaric et al., 2012, 2016, Xie and
Jiang, 2020] use concentrability coefﬁcients similar to those
deﬁned above to derive upper bounds on the performance
of various algorithms.

Unfortunately, although the concentrability coefﬁcients
above attempt to provide a closed way to quantify distribu-
tional shift, they have limited interpretability. Speciﬁcally, it
is hard to infer from the aforementioned coefﬁcients which
exact sampling distributions should be used. For example, if
we consider coefﬁcients C2 and C3, even if we know which
parts of the state space are relevant (according to the distri-
bution ρ in (2) and (3)), the computation of the coefﬁcient in
(1) still depends on the complex interactions between ρ and
the dynamics of the MDP under any possible policy. What
can be concluded is that the concentrability coefﬁcient will
depend on all states that can be reached by any policy when
the starting state distribution is given by ρ. However, it is
not obvious which exact target distribution we should aim at
when selecting the sampling distribution µ, especially when
we do not know the underlying MDP or which regions of
the state space are of interest. In the face of such uncertainty,
previous works suggest that high coverage of the state space
is desirable, assuming upper-bounded concentrability co-
efﬁcients [Munos and Szepesvári, 2008, Farahmand et al.,
2010, Chen and Jiang, 2019, Yang et al., 2019].

More recently, different works study the impact of the data
distribution on ofﬂine policy evaluation with linear function
approximation [Duan and Wang, 2020, Wang et al., 2020].
Wang et al. [2020] show that good data coverage is not sufﬁ-
cient to sample-efﬁciently perform ofﬂine policy evaluation
and that signiﬁcantly stronger assumptions on distributional
shift may be needed.

In the next sections, we provide a new interpretation of C3
as an f -divergence and give a new motivation for the use of
maximum entropy sampling distributions.

4.1.1 Concentrability Coefﬁcient as an f -divergence

Letting β = ρP π1P π2...P πm , we can rewrite (3) as

(cid:13)
(cid:13)
(cid:13)
(cid:13)

β
µ

(cid:13)
(cid:13)
(cid:13)
(cid:13)2,µ

=

=

(cid:32)

(cid:34)(cid:18) β(s, a)
µ(s, a)

E(s,a)

µ

∼

(cid:19)2(cid:35)(cid:33)1/2

(cid:113)

f (β

D

µ) + 1 =

||

(cid:112)

χ2(β

µ) + 1,

||

for f (x) = x2
−
D
χ2 the chi-square divergence.

1, where

f denotes the f -divergence and

||

D

f (β

We start our analysis by noting that optimizing (3) over the
distribution µ is hard due to the fact that, as previously noted,
we actually want to minimize
µ) with respect to a
large set of different β distributions (due to the supremum in
(3), as well as the summation in (1)). Furthermore, we usu-
ally do not have access to the transition probability function.
Therefore, we analyze the problem of picking an optimal µ
distribution as a robust optimization problem. Speciﬁcally,
we formulate a minimax objective where the minimizing
µ) and the
player aims at choosing µ to minimize
D
maximizing player chooses β to maximize
µ). We
D
get the following result (proof in Appendix B).

||
f (β

f (β

||

Theorem 4.1. Let
distributions over

) represent the set of probability
+
)
(
0
S × A

. Let also Lµ :

→

R

P

such that Lµ(β) =

. The solution µ to

(

S ×A
P
S × A
(cid:13)
(cid:13)
2
β
(cid:13)
(cid:13)
(cid:13)
(cid:13)
µ

2,µ

argmin
(

)
S×A

∈P

µ

β

max
(

)
S×A

∈P

Lµ(β)

(4)

is the maximum entropy distribution over

.

S × A

As stated in Theorem 4.1, the maximum entropy distribution,
equivalent to the uniform distribution, is the solution to the
robust optimization problem. This result provides a theoreti-
cal justiﬁcation for the beneﬁts of using high entropy sam-
pling distributions, as suggested by previous works [Kakade
and Langford, 2002, Munos, 2003]: uniform distributions
ensure coverage over the state-action space, contributing to
keep concentrability coefﬁcients low.

4.2 FOUR-STATE MDP

In this section, we study how the data distribution can in-
ﬂuence the performance of a Q-learning algorithm with
function approximation, under a four-state MDP (Fig. 1).
We show that the data distribution can signiﬁcantly inﬂuence
the quality of the resulting policies and affect the stability of
the learning algorithm. Due to space constraints, we focus
our discussion on the main conclusions and refer to Ap-
pendix B for an in-depth discussion, including additional
experiments and insights. This section shows that the data
distribution can play an active role regulating algorithmic
stability.

We focus our attention to non-terminal states s1 and s2.
In state s1 the correct action is a1, whereas in state s2
the correct action is a2. We set γ = 1. We consider a
linear function approximator Qw(st, at) = wTφ(st, at),
where φ is a feature mapping, deﬁned as φ(s1, a1) =
[1, 0, 0]T, φ(s1, a2) = [0, 1, 0]T, φ(s2, a1) = [α, 0, 0]T,
and φ(s2, a2) = [0, 0, 1]T, with α
[1, 3/2). As can
be seen, the capacity of the function approximator is lim-
ited and there exists a correlation between Qw(s1, a1) and
Qw(s2, a1). This will be key to the results that follow.

∈

{

a1, a2}

s1, s2, s3, s4}
{

and
Figure 1: Four-state MDP, with states
actions
. State s1 is the initial state and states s3
and s4 are terminal (absorbing) states. All actions are de-
terministic except for the state-action pair (s1, a1), where
s1, a1) = 0.01. The reward
p(s3|
10, r(s2, a1) =
function is r(s1, a1) = 100, r(s1, a2) =
35 and r(s2, a2) = 30.

s1, a1) = 0.99 and p(s2|

−

−

Figure 2: The number of correct actions at states s1 and s2
for different data distributions (α = 1.25).

4.2.1 Ofﬂine Learning

S × A

We consider an ofﬂine RL setting and denote by µ the distri-
bution over the
space induced by a static dataset of
transitions. We focus our attention on probabilities µ(s1, a1)
and µ(s2, a1), since these are the probabilities associated
with the two partially correlated state-action pairs. Fig.
2 displays the inﬂuence of the data distribution, namely
the proportion between µ(s1, a1) and µ(s2, a1), on the
number of correct actions yielded by the learned policy.
0.5, we
We identify three regimes: (i) when µ(s1, a1)
learn the optimal policy; (ii) if µ(s1, a1) < (
0.48) or
0.65), the policy is only cor-
(
≈
rect at one of the states; (iii) if µ(s1, a1) > (
0.65), the
policy is wrong at both states. The results above show that,
due to the limited power and correlation between features,
the data distribution impacts performance as the number of
correct actions is directly dependent on the properties of the
data distribution. As our results show, due to bootstrapping,
it is possible that under certain data distributions neither
action is correct.

0.52) < µ(s1, a1) < (

≈
≈

≈

≈

(a) Number of correct actions.

(b) Q-values mean error.

Figure 3: Experiments for different exploratory policies with
an inﬁnitely-sized replay buffer.

4.2.2 Online Learning with Unlimited Replay

Instead of considering a ﬁxed µ distribution, we now con-
sider a setting where µ is dynamically induced by a replay
buffer obtained using (cid:15)-greedy exploration. Figure 3 shows
the results when α = 1.2, under: (i) an (cid:15)-greedy policy with
(cid:15) = 1.0; and (ii) an (cid:15)-greedy policy with (cid:15) = 0.05. We
consider a replay buffer with unlimited capacity. We use an
uniform synthetic data distribution as baseline. As seen in
Fig. 3, the baseline outperforms all other data distributions,
as expected. Regarding the (cid:15)-greedy policy with (cid:15) = 1.0,
the agent is only able to pick the correct action at state s1,
featuring a higher average Q-value error in comparison to
the baseline. This is due to the fact that the stationary dis-
tribution of the MDP under the fully exploratory policy is
too far from the uniform distribution to retrieve the opti-
mal policy. Finally, for the (cid:15)-greedy policy with (cid:15) = 0.05,
the performance of the agent further deteriorates. Such ex-
ploratory policy induces oscillations in the Q-values (Fig. 3
(b)), which eventually damp out as learning progresses. The
oscillations are due to an undesirable interplay between the
features and the data distribution: exploitation may cause
abrupt changes in the data distribution and hinder learning.

4.2.3 Online Learning with Limited Replay Capacity

Finally, we consider an experimental setting where the re-
play buffer has limited capacity and study the impact of its

s1a1s2a1a2a2s3s4r=100r=−10r=−35r=30r=0r=01.00.80.60.40.20.0µ(s2,a1)0.00.20.40.60.81.0µ(s1,a1)012#correctactionsα=1.2502500500075001000012500150001750020000Episode012#correctactions(cid:15)=0.05(cid:15)=1.0Uniform02500500075001000012500150001750020000Episode0.000.050.100.150.200.250.30MeanQ-valueserror(cid:15)=0.05(cid:15)=1.0Uniformvironments: the grid 1 and grid 2 environments consist
of standard tabular environments with highly uncorrelated
state features, the multi-path environment is a hard explo-
ration environment, and the pendulum, mountaincar and
cartpole environments are benchmarking environments fea-
turing a continuous state-space domain. All reported val-
ues are calculated by aggregating the results of different
training runs. The description of the experimental environ-
ments and the experimental methodology, as well as the
complete results can be found in Appendix C. The devel-
oped software can be found here. We also provide an interac-
tive dashboard with all our experimental results at https:
//rl-data-distribution.herokuapp.com/.

In this section, we denote by µ the data distribution over
state-action pairs induced by a static dataset of transitions.
We consider two types of ofﬂine datasets: (i) (cid:15)-greedy
datasets, generated by running an (cid:15)-optimal policy on the
MDP, i.e., a policy that is (cid:15)-greedy with respect to the
[0, 1]; and (ii) Boltzmann(t)
optimal Q-values with (cid:15)
datasets, generated by running a Boltzmann policy with re-
spect to the optimal Q-values with temperature coefﬁcient
10, 10]. Additionally, we artiﬁcially force, for some
t
of the generated datasets, that they have full coverage over
the
space. We do this by running an additional pro-
cedure that enforces that each state-action pair appears at
least once in the dataset.

S × A

[
−

∈

∈

Two aspects are worth mentioning. First, in all environments,
the sampling error is low due to the highly deterministic na-
ture of the underlying MDPs. Thus, a single next-state sam-
ple is sufﬁcient to correctly evaluate the Bellman optimality
operator. Second, the function approximator has enough
capacity to correctly represent the optimal Q-function, a
property known as realizability [Chen and Jiang, 2019].

5.1 HIGH ENTROPY IS BENEFICIAL

H

We start our analysis by studying the impact of the dataset
distribution entropy,
(µ), on the performance of the ofﬂine
RL algorithms. Figure 5 displays the obtained experimental
results. As can be seen, under all environments and for
both ofﬂine algorithms, high entropy distributions tend to
achieve increased rewards. In other words, distributions with
an entropy close to that of the uniform distribution appear
to be well suited to be used in ofﬂine learning settings. Such
observation is inline with the discussion drawn in Sec. 4.1:
high entropy distributions contribute to keep concentrability
coefﬁcients low and, thus, mitigate algorithmic instabilities.

Importantly, we do not claim that high entropy distributions
are the only distributions suitable to be used. As can be
seen in Fig. 5, certain lower-entropy distributions also per-
form well. In the next sections, we investigate which other
properties of the distribution are of beneﬁt to ofﬂine RL.

Figure 4: Q-values error under the ((cid:15) = 0.05)-greedy policy
for different replay capacities.

size in the stability of the algorithm. Figure 4 displays the
results obtained with the (cid:15)-greedy policy with (cid:15) = 0.05,
while varying the capacity of the replay buffer. As can be
seen, as the replay buffer size increases the oscillations in
the Q-values errors are smaller. The undesirable interplay
previously observed under the inﬁnitely-sized replay buffer
repeats. However, the smaller the replay buffer capacity, the
more the data distribution induced by the contents of the re-
play buffer is affected by changes to the current exploratory
policy, i.e., exploitation leads to more abrupt changes in the
data distribution, which, in turn, drive abrupt changes to
the Q-values. For the inﬁnitely-sized replay buffer the am-
plitude of the oscillations is dampened because previously
stored experience contributes to make the data distribution
more stationary, not as easily achieved by smaller buffers.

4.2.4 Discussion

We presented a set of experiments using a four-state MDP
that show how the data distribution can inﬂuence the per-
formance of the resulting policies and the stability of the
learning algorithm. First, we showed that, under an ofﬂine
RL setting, the retrieved number of correct actions is di-
rectly dependent on the properties of the data distribution
due to an undesirable correlation between features. Second,
not only the quality of the retrieved policies depends on the
data collection mechanism, but also an undesirable interplay
between the data distribution and the function approximator
can arise: exploitation can lead to abrupt changes in the data
distribution and hinder learning. Finally, we showed that the
replay buffer size can also affect the learning dynamics.

5 EMPIRICALLY ASSESSING THE

IMPACT OF THE DATA
DISTRIBUTION IN OFFLINE RL

In the present section, we experimentally assess the impact
of different data distribution properties on the performance
of the ofﬂine DQN and CQL algorithms. We evaluate the
performance of the RL algorithms under six different en-

0500010000150002000025000300003500040000Episode0.000.050.100.150.200.250.30MeanQ-valueserrorReplaybuﬀersize=∞Replaybuﬀersize=50000Replaybuﬀersize=10000line with the discussion presented in Sec. 4.1. One could
argue that we are only interested in correctly estimating the
Q-values along an optimal trajectory, however, due to the
bootstrapped nature of the updates, error in the estimation of
the Q-values for adjacent states can erroneously affect the
estimation of the Q-values along the optimal trajectory. This
reasoning is suggested by concentrability coefﬁcients. If we
consider distribution ρ from (2) or (3) to be the uniform
distribution over the states of the optimal trajectory and zero
otherwise, we can see that the concentrability coefﬁcient
given by (1) still depends on other states than those of the
optimal trajectory. Precisely, the coefﬁcient depends on all
the states that can be reached by any policy when the start-
ing state is sampled according to ρ (the importance of each
state geometrically decays depending on their distance to
the optimal trajectory). Therefore, in order to keep the con-
centrability coefﬁcient low it is important that such states
are present in the dataset. On the other hand, CQL is still
able to robustly learn using high quality trajectories indepen-
dently of the data coverage because of its pessimistic nature.
Since the algorithm penalizes the Q-values for actions that
are underrepresented in the dataset, the error for adjacent
states is not propagated in the execution of the algorithm.

In this section, we considered datasets that are, in general,
aligned and close to that induced by optimal policies. What
happens if our data is collected by arbitrary policies? Ofﬂine
learning under such settings can be harder. We investigate
the impact of the trajectory quality in the next section.

5.3 CLOSENESS TO OPTIMAL POLICY

MATTERS

Now, we investigate how ofﬂine agents can be affected by
the quality of the trajectories contained in the dataset. More
precisely, we study how the statistical distance between
distribution µ and the distribution induced by one of the
optimal policies of the MDP, dπ∗ , affects ofﬂine learning.

The obtained experimental results are portrayed in Fig. 7.
We consider a wide spectrum of behavior policies, from
optimal to anti-optimal policies, as well as from fully ex-
ploitatory to fully exploratory policies. As can be seen, as
the statistical distance between distribution µ and the closest
distribution induced by one of the optimal policies increases,
the lower the rewards obtained, irrespectively of the algo-
rithm. We can also observe an increase in obtained rewards
when dataset coverage is enforced (Fig. 7 (b)) in comparison
to when dataset coverage is not enforced (Fig. 7 (a)).

At ﬁrst sight, our results appear intuitive if we focus on Fig.
7 (a), where dataset coverage is not enforced: if the policy
used to collect the dataset is not good enough, it will fail
to collect trajectories rich in rewards, key to learn reward-
maximizing behavior. As an example, if the policy used to
collect the data is highly exploratory, the agent will likely

Figure 5: Average normalized rollouts reward for datasets
with different normalized entropies.

5.2 DATASET COVERAGE MATTERS

We now study the impact of the dataset coverage, i.e., the
diversity of the transitions in the dataset, in the performance
of the ofﬂine agents. In order to keep the discussion concise,
in this section we focus our attention on (cid:15)-greedy datasets,
and refer to Appendix C for the complete results.

We start by focusing our attention on the ofﬂine DQN al-
gorithm. Figure 6 (a) displays the obtained experimental
results. As can be seen, DQN struggles to achieve optimal
rewards for low (cid:15) values, i.e., even though the algorithm
is provided with optimal or near-optimal trajectories, it is
unable to steadily learn under such setting. However, as (cid:15)
increases the performance of the algorithm increases, even-
tually decaying again for high (cid:15) values. Such results suggest
that a certain degree of data coverage is required by DQN to
robustly learn in an ofﬂine manner, despite being provided
with high quality data (rich in rewards). On the other hand,
the decay in performance for highly exploratory policies
under some environments can be explained by the fact that
such policies induce trajectories that are poor in reward
(this is further explored in the next section). Figure 6 (b)
displays the obtained experimental results under the exact
same datasets, except that we enforce coverage over the
space. We note a substantial improvement in the
S × A
performance of DQN across all environments, supporting
our hypothesis that data coverage plays an important role
regulating the stability of ofﬂine RL algorithms.

The CQL algorithm appears to perform more robustly than
DQN. Particularly, as seen in Fig. 6 (a), CQL is able to
robustly learn with low (cid:15) values, i.e., using optimal or near
optimal trajectories that feature low coverage. Additionally,
no substantial performance gain is observed under the ofﬂine
CQL agent by enforcing dataset coverage (Fig. 6 (b)).

The ﬁnding that data coverage appears to play an impor-
tant role regulating the performance of DQN, even when
considering high quality, near optimal trajectories, is in-

0.00.51.0Norm.rewardGrid1Grid2Multi-path0.00.51.0H(µ)/H(U)0.00.51.0Norm.rewardPendulum0.00.51.0H(µ)/H(U)Mountaincar0.00.51.0H(µ)/H(U)CartpoleDQNCQL(a) Dataset coverage not enforced.

(a) Dataset coverage not enforced.

(b) Dataset coverage enforced.

(b) Dataset coverage enforced.

Figure 6: Average normalized rollouts reward under (cid:15)-
greedy datasets for different (cid:15) values.

Figure 7: Average normalized rollouts reward; the x-axis
encodes the statistical distance between µ and the closest
distribution induced by one of the optimal policies, dπ∗ .

not reach high rewarding states and the learning signal may
be too weak to learn an optimal policy in an ofﬂine manner.

S × A

However, the results displayed in Fig. 7 (b), i.e., the ex-
perimental setting in which dataset coverage is enforced,
reveal a rather less intuitive ﬁnding: despite the fact that
all datasets feature full coverage over the
space, if
the statistical distance between the two distributions is high,
we observe a deterioration in algorithmic performance. In
other words, despite the fact that the datasets contain all
the information that can be retrieved from the environment
(including transitions rich in reward), ofﬂine learning can
still be hard if the behavior policy is too distant from the
optimal policy. Such observation can possibly be explained
by the fact that distributions far from the optimal policy
prevent the propagation of information, namely Q-values,
during the execution of the ofﬂine RL algorithm.

Given the experimental results presented in this section, we
hypothesize that it is important for the data distribution to
be aligned with that of optimal policies, not only to ensure
that trajectories are rich in reward, but also to mitigate algo-
rithmic instabilities. Our experimental results suggest that
the assumption of a bounded concentrability coefﬁcient, as
discussed in Sec. 4.1, may not be enough to robustly learn
in an ofﬂine manner and that more stringent assumptions on
the data distribution are required. Wang et al. [2020] reach

a similar conclusion, from a theoretical perspective.

5.4 DISCUSSION

This section experimentally assessed the impact of different
data distribution properties in the performance of ofﬂine
Q-learning algorithms with function approximation, show-
ing that the data distribution greatly impacts algorithmic
performance. Our results suggest that: (i) high entropy data
distributions are well-suited for learning in an ofﬂine man-
ner; and (ii) a certain degree of data diversity (data coverage)
and data quality (closeness to distributions induced by opti-
mal policies) are jointly desirable for robust ofﬂine learning.

Finally, as seen in Fig. 7, the performance of DQN and CQL
is dependent on the exact experimental setting, namely on
whether coverage is enforced or not. When dataset coverage
is not enforced (Fig. 7 (a)), CQL outperforms DQN, espe-
cially for distributions close to that of optimal policies (as
can also be seen in Fig. 6 (a)). However, when coverage is
enforced (Fig. 7 (b)), DQN outperforms CQL, especially for
distributions that are more distant to that of optimal policies,
as can be seen in Fig. 7 (b). This is due to the fact that both
algorithms balance the tradeoff between optimism and pes-
simism in different ways. DQN is very optimistic and fails
under low-coverage settings, since it propagates erroneous

0.00.51.0Norm.rewardGrid1Grid2Multi-path0.00.51.0(cid:15)0.00.51.0Norm.rewardPendulum0.00.51.0(cid:15)Mountaincar0.00.51.0(cid:15)CartpoleDQNCQL0.00.51.0Norm.rewardGrid1Grid2Multi-path0.00.51.0(cid:15)0.00.51.0Norm.rewardPendulum0.00.51.0(cid:15)Mountaincar0.00.51.0(cid:15)CartpoleDQNCQL5100.00.51.0Norm.rewardGrid15100.00.51.0Grid25100.00.51.0Multi-path510minπ∗Df(dπ∗||µ)0.51.0Norm.rewardPendulum2.55.07.5minπ∗Df(dπ∗||µ)0.00.51.0Mountaincar246minπ∗Df(dπ∗||µ)0.00.51.0CartpoleDQNCQL2.55.07.50.00.51.0Norm.rewardGrid12.55.07.50.00.51.0Grid22.55.07.50.00.51.0Multi-path2.55.07.5minπ∗Df(dπ∗||µ)0.51.0Norm.rewardPendulum246minπ∗Df(dπ∗||µ)0.00.51.0Mountaincar24minπ∗Df(dπ∗||µ)0.51.0CartpoleDQNCQLQ-values during the execution of the algorithm. However,
due to its optimistic nature, it outperforms CQL when cov-
erage is enforced, taking advantage of information that is
underrepresented in the dataset. CQL, on the other hand,
outperforms DQN under low coverage settings since, due to
its pessimistic nature, prevents the propagation of erroneous
Q-values. However, when valuable but underrepresented
information is present in the dataset, the pessimism of CQL
prevents learning, and CQL is outperformed by DQN.

6 CONCLUSION

In this work, we investigate the interplay between the data
distribution and Q-learning-based algorithms with function
approximation. We analyze how different properties of the
data distribution affect performance in both online and of-
ﬂine RL settings. We show, both theoretically and empiri-
cally, that: (i) high entropy data distributions contribute to
mitigate sources of algorithmic instability; and (ii) different
properties of the data distribution inﬂuence the performance
of RL methods with function approximation. We provide
a thorough experimental assessment of the performance
of both DQN and CQL algorithms under several types of
ofﬂine datasets, connecting our experimental results with
theoretical ﬁndings of previous works.

The experimental results presented herein provide useful
insights for the development of improved data processing
techniques for ofﬂine RL, which should be valuable for
future research. For example, our results suggest that maxi-
mum entropy exploration methods [Hazan et al., 2018] can
be well suited for the construction of datasets for ofﬂine
RL, naïve dataset concatenation can lead to deterioration
in performance, and that, by simply reweighting or discard-
ing of training data, it is possible to substantially improve
performance of ofﬂine RL algorithms.

References

Martin Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,
Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion
Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,
Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Van-
houcke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015.

Rishabh Agarwal, Dale Schuurmans, and Mohammad

Norouzi. An optimistic perspective on ofﬂine reinforce-
ment learning. CoRR, abs/1907.04543, 2019.

Andras Antos, Csaba Szepesvari, and Rémi Munos. Learn-
ing near-optimal policies with Bellman-residual mini-
mization based ﬁtted policy iteration and a single sample
path. Machine Learning, 71:89–129, 2008.

Leemon Baird. Residual algorithms: Reinforcement learn-
ing with function approximation. In In Proceedings of the
Twelfth International Conference on Machine Learning,
pages 30–37. Morgan Kaufmann, 1995.

Diogo Carvalho, Francisco S Melo, and Pedro Santos. A
new convergent variant of q-learning with linear function
approximation. Advances in Neural Information Process-
ing Systems, 33:19412–19421, 2020.

Jinglin Chen and Nan Jiang.

Information-theoretic con-
siderations in batch reinforcement learning. CoRR,
abs/1905.00360, 2019.

Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy
evaluation with linear function approximation. CoRR,
abs/2002.09516, 2020.

Amir-massoud Farahmand, Csaba Szepesvári, and Rémi
Munos. Error propagation for approximate policy and
In J. Lafferty, C. Williams, J. Shawe-
value iteration.
Taylor, R. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems, volume 23,
2010.

Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
Diagnosing bottlenecks in deep q-learning algorithms.
CoRR, abs/1902.10250, 2019.

Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and
Sergey Levine. D4RL: datasets for deep data-driven rein-
forcement learning. CoRR, abs/2004.07219, 2020.

Çaglar Gülçehre, Ziyu Wang, Alexander Novikov, Tom Le
Paine, Sergio Gómez Colmenarejo, Konrad Zolna,
Rishabh Agarwal, Josh Merel, Daniel J. Mankowitz,
Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mo-
hammad Norouzi, Matt Hoffman, Oﬁr Nachum, George
Tucker, Nicolas Heess, and Nando de Freitas. RL un-
plugged: Benchmarks for ofﬂine reinforcement learning.
CoRR, abs/2006.13888, 2020.

Elad Hazan, Sham M. Kakade, Karan Singh, and Abby Van
Soest. Provably efﬁcient maximum entropy exploration.
CoRR, abs/1812.02690, 2018.

Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel
Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas
Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli,
Sarah Henderson, Alex Novikov, Sergio Gomez Col-
menarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine,

Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Fre-
itas. Acme: A research framework for distributed rein-
forcement learning, 2020.

Sham Kakade and John Langford. Approximately optimal
approximate reinforcement learning. In In Proc. 19TH
International Conference on Machine Learning, pages
267–274, 2002.

J. Kolter. The ﬁxed points of off-policy td. In J. Shawe-
Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Wein-
berger, editors, Advances in Neural Information Process-
ing Systems, volume 24. Curran Associates, Inc., 2011.

Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
Stabilizing off-policy q-learning via bootstrapping error
reduction. CoRR, abs/1906.00949, 2019.

Aviral Kumar, Abhishek Gupta, and Sergey Levine. Dis-
cor: Corrective feedback in reinforcement learning via
distribution correction. CoRR, abs/2003.07305, 2020a.

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. Conservative q-learning for ofﬂine reinforcement
learning. CoRR, abs/2006.04779, 2020b.

Alessandro Lazaric, Mohammad Ghavamzadeh, and Rémi
Munos. Finite-sample analysis of least-squares policy
iteration. Journal of Machine Learning Research, 13(98):
3041–3074, 2012.

Alessandro Lazaric, Mohammad Ghavamzadeh, and Rémi
Munos. Analysis of classiﬁcation-based policy iteration
algorithms. Journal of Machine Learning Research, 17
(19):1–30, 2016.

Sergey Levine, Aviral Kumar, George Tucker, and Justin
Fu. Ofﬂine reinforcement learning: Tutorial, review, and
perspectives on open problems. CoRR, abs/2005.01643,
2020.

F. Liese and I. Vajda. On divergences and informations
in statistics and information theory. IEEE Transactions
on Information Theory, 52(10):4394–4412, 2006. doi:
10.1109/TIT.2006.881731.

T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and Daan Wierstra. Continuous control with
deep reinforcement learning. CoRR, abs/1509.02971,
2016.

Vincent Liu, Raksha Kumaraswamy, Lei Le, and Martha
White. The utility of sparse representations for control in
reinforcement learning. CoRR, abs/1811.06626, 2018.

Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiri-
any, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio
Savarese, Yuke Zhu, and Roberto Martín-Martín. What
matters in learning from ofﬂine human demonstrations
for robot manipulation. CoRR, abs/2108.03298, 2021.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Mar-
tin Riedmiller. Playing atari with deep reinforcement
learning. Nature, 518(7540):529–533, 2015.

Rémi Munos. Error bounds for approximate policy itera-
tion. In International Conference on Machine Learning,
volume 3, pages 560–567, 2003.

Rémi Munos. Error bounds for approximate value iteration.
In AAAI Conference on Artiﬁcial Intelligence, pages 1006–
1011, 2005.

Rémi Munos and Csaba Szepesvári. Finite-time bounds
for ﬁtted value iteration. Journal of Machine Learning
Research, 9(27):815–857, 2008.

Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu,
Shengkai Huang, Zewen Li, Weinan Zhang, and Yang Yu.
Neorl: A near real-world benchmark for ofﬂine reinforce-
ment learning. CoRR, abs/2102.00714, 2021.

Kajetan Schweighofer, Markus Hofmarcher, Marius-
Constantin Dinu, Philipp Renz, Angela Bitto-Nemling,
Vihang P. Patil, and Sepp Hochreiter. Understanding the
effects of dataset characteristics on ofﬂine reinforcement
learning. CoRR, abs/2111.04714, 2021.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis
Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,
Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen,
Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den
Driessche, Thore Graepel, and Demis Hassabis. Master-
ing the game of go without human knowledge. Nature,
550(7676):354–359, 2017.

Richard Sutton and Andrew Barto. Reinforcement Learning:
An Introduction. The MIT Press, second edition, 2018.

J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-
difference learning with function approximation. IEEE
Transactions on Automatic Control, 42(5):674–690, 1997.
doi: 10.1109/9.580874.

John N. Tsitsiklis and Benjamin van Roy. Feature-based
methods for large scale dynamic programming. Machine
Learning, 22(1):59–94, Mar 1996. ISSN 1573-0565. doi:
10.1007/BF00114724.

Hado van Hasselt, Yotam Doron, Florian Strub, Matteo
Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep
reinforcement learning and the deadly triad. CoRR,
abs/1812.02648, 2018.

Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What
are the statistical limits of ofﬂine RL with linear function
approximation? CoRR, abs/2010.11895, 2020.

Christopher Watkins and Peter Dayan. Q-learning. Machine
Learning, 8(3):279–292, 1992. ISSN 1573-0565. doi:
10.1007/BF00992698.

Tengyang Xie and Nan Jiang. Batch value-function approx-
imation with only realizability. CoRR, abs/2008.04990,
2020.

Zhuoran Yang, Yuchen Xie, and Zhaoran Wang. A theoreti-
cal analysis of deep q-learning. CoRR, abs/1901.00137,
2019.

Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson.
Breaking the deadly triad with a target network. CoRR,
abs/2101.08862, 2021.

A SUPPLEMENTARY MATERIALS FOR SECTION 2

Below, we present the pseudocode of a generic Q-learning algorithm with function approximation.

D

p0

do

K
{

for sampling step in

Algorithm 1 Generic Q-learning algorithm with function approximation.
1: initialize φ0
2: initialize s0 ∼
3: initialize
=
∅
4: for iteration k in
5:
6:
7:
8:
9:
10:
11:
12:

(st, at, r(st, at), st+1)
}

st, at)

D ∪ {

πk(at

S
{

do

}

}

st)
|
p(st+1|

at
∼
st+1 ∼
=
D
end for
φk,0 = φk
for gradient step g in
(si
sample batch
{
φk,g
φk,g+1 ←
end for
φk+1 = φk,G

−

do
G
{
}
t, ai
t, si
t, ri
from
t+1)
D
}
(cid:0)Qφk,g (si
(cid:80)
t, ai
t)
α
i

φk,g

∇

13:
14:
15:
16: end for

(cid:46) Sample from behavior policy (e.g., (cid:15)-greedy).

(cid:46) Append to replay memory.

(cid:46) Sample from replay memory.

(ri

t + γ maxat+1 Qφk (si

t+1, at+1))(cid:1)2

−

B SUPPLEMENTARY MATERIALS FOR SECTION 4

This section gives additional details with respect to Section 4.

B.1 SUPPLEMENTARY MATERIALS FOR SECTION 4.1

As noted in Section 4.1.1, concentrability coefﬁcient C3, as deﬁned by (1) and (3), is equivalent to an f -divergence between
a given distribution β and the sampling distribution µ. As stated, that is the case when f (x) = x2

1, since

−

(cid:19)2(cid:35)(cid:33)1/2

(cid:19)2(cid:35)

(cid:33)1/2

1 + 1

−

(cid:33)1/2

(cid:35)
1

+ 1

−

(cid:19)2

(cid:19)(cid:21)

(cid:19)1/2

+ 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

β
µ

(cid:13)
(cid:13)
(cid:13)
(cid:13)2,µ

=

=

=

=

=

=

(cid:32)

E(s,a)

∼

(cid:32)

E(s,a)

∼

(cid:32)

E(s,a)

∼

µ

µ

µ

(cid:18)

E(s,a)

µ

∼

(cid:34)(cid:18) β(s, a)
µ(s, a)
(cid:34)(cid:18) β(s, a)
µ(s, a)
(cid:34)(cid:18) β(s, a)
µ(s, a)
(cid:18) β(s, a)
µ(s, a)

(cid:20)
f

(cid:113)

(cid:112)

f (β

D
χ2(β

µ) + 1

||
µ) + 1.

||

As the last equality states, the f -divergence above corresponds to a particular type of divergence known as the χ2-divergence
[Liese and Vajda, 2006].

Below, we prove Theorem 4.1. We reproduce the theorem’s statement before the proof for ease of consultation.

Theorem B.1. Let

,

(
S

P

) represent the set of probability distributions over

A

.Let also Lµ :

(

P

S × A

)

→

+
0 such

R

S × A

that Lµ(β) =

(cid:13)
(cid:13)
(cid:13)

β
µ

(cid:13)
2
(cid:13)
(cid:13)

2,µ

. The solution µ to

argmin
(

)
S×A

∈P

µ

max
(
S×A

∈P

)

β

Lµ(β)

(5)

is the uniform distribution over the state-action space.

Proof. Suppose that, as discussed, the maximizing player (adversary) has access to a distribution µ, chosen by the minimizing
player. The adversary’s objective is therefore to maximize Lµ over the probability simplex. We begin to show what is the
solution of such maximization. We start by noting that Lµ, being a norm, is a convex real function. Lµ is also continuous.
) is a compact set, since it is both closed and bounded. Under the three
Additionally, the probability simplex
conditions just mentioned, Bauer’s Maximum Principle guarantees that the solution of the maximization lies on the subset
of extreme points of the admissible region. Such set is, in the case of the probability simplex, the subset of probability
distributions with a singleton support set. Equivalently, the adversary is to choose a distribution β such that, for some pair
(s, a), β(s, a) = 1 and is zero otherwise. Formally, we can write

(
S × A

P

Lµ(β) =

(cid:88)

µ(s, a)

(cid:19)2

(cid:18) β(s, a)
µ(s, a)

=

(s,a)

∈S×A
(cid:88)

(s,a)

∈S×A

µ(s, a)−

1β(s, a)2.

Therefore,

β

max
(

)
S×A

∈P

Lµ(β) = max

(s,a)

∈S×A

µ(s, a)−

1.

Finally, the minimizing player’s best choice of µ is the one minimizing the quantity above. We note that

if µ is the uniform distribution and

otherwise. The conclusion follows.

max

(s,a)

∈S×A

max

(s,a)

∈S×A

µ(s, a)−

1 =

|S × A|

µ(s, a)−

1 >

|S × A|

(6)

(7)

(8)

(9)

(10)

Figure 8 displays the relationship between the expected entropy of the sampling distribution µ and: (i) the coverage of
different datasets constructed using µ; and (ii) the mean χ2-divergence to all other distributions. The plots are computed
using randomly sampled distributions, which we sample from different Dirichlet distributions. We control the expected
entropy of the resulting distributions by varying the Dirichlet parameter α. As seen in Fig. 8 (a), irrespectively of the dataset
size, higher entropy distributions lead to datasets featuring higher coverage over the distribution support. As seen in Fig. 8
(b), higher entropy distributions yield, on average, a lower χ2-divergence to all other sampled distributions in comparison to
lower entropy distributions.

(a) Dataset coverage.

(b) Distance to all other distributions.

Figure 8: The relationship between the expected entropy of the distribution µ and: (i) the dataset coverage; and (ii) the
χ2-divergence to all other distributions.

2.53.03.54.04.5E[H(µ)]0.20.40.60.81.0Coverage(%)size(D)=1000size(D)=5000size(D)=10000size(D)=200002.53.03.54.04.5E[H(µ)]101102103104χ2(·||µ)Mean95%meanC.I.B.2 SUPPLEMENTARY MATERIALS FOR SECTION 4.2

In this section we study how the data distribution can inﬂuence the performance of Q-learning based RL algorithms with
function approximation, under a four-state MDP (Fig. 9). The main objective of this section is to show that the data
distribution plays an active role regulating algorithmic stability. We show that the data distribution can signiﬁcantly inﬂuence
the quality of the resulting policies and affect the stability of the learning algorithm. We consider both online and ofﬂine RL
settings. We ﬁnish this section by summarizing the key insights and providing a discussion on how the example presented
here, as well as the respective ﬁndings, can generalize into bigger and more realistic MDPs.

All experimental results are averaged over 6 independent runs.

Figure 9: Four-state MDP, comprising states
. State s1 is the initial state and states s3
and s4 are the terminal absorbing states. All actions lead to deterministic transitions, except for the state-action pair (s1, a1),
10,
where p(s3|
r(s2, a1) =

a1, a2}
{
s1, a1) = 0.01. The reward function is deﬁned as r(s1, a1) = 100, r(s1, a2) =

s1, s2, s3, s4}
{

and actions

−

s1, a1) = 0.99 and p(s2|
35 and r(s2, a2) = 30.
−

We focus our attention on non-terminal states s1 and s2 of the MDP, whose optimal Q-function is

Q∗ =

a1
100.3
35

(cid:20)
s1
s2 −

a2
20
30

(cid:21)

.

At state s1 the correct action is a1, whereas at state s2 the correct action is a2. We set γ = 1.
We consider a linear function approximator Qw(st, at) = wTφ(st, at), where w = [w1, w2, w3]T denotes a weight column
vector and φ is a feature mapping, deﬁned as (φ(s1, a1) = [1, 0, 0]T, φ(s1, a2) = [0, 1, 0]T, φ(s2, a1) = [α, 0, 0]T, and
φ(s2, a2) = [0, 0, 1]T)

Qw =

(cid:20)

a2
a1
s1
w2
w1
s2 αw1 w3

(cid:21)

,

where α
[1, 3/2). The optimal policy is retrieved if w1 > w2 and αw1 < w3. As can be seen, due to the choice of
feature mapping, the capacity of the function approximator is limited and there exists a correlation in the features between
Qw(s1, a1) and Qw(s2, a1). This will be key to the results that follow.

∈

B.2.1 Ofﬂine Oracle Version

We start by assuming that we have access to an oracle providing us with the exact optimal Q-function and write the loss of
the function approximator as

(w) = E(st,at)

µ

∼

L

(cid:104)(cid:0)wTφ(st, at)

−

Q∗(st, at)(cid:1)2(cid:105)

,

s1a1s2a1a2a2s3s4r=100r=−10r=−35r=30r=0r=0where µ can be interpreted as the probability distribution over state-action pairs induced by a static dataset of transitions or a
generative model of the environment. The optimal weight vector is given by

w∗1 =

µ(s1, a1)Q∗(s1, a1) + αµ(s2, a1)Q∗(s2, a1)
µ(s1, a1) + α2µ(s2, a1)

, w∗2 = Q∗(s1, a2), w∗3 = Q∗(s2, a2),

(11)

(s, a). In the results that follow, we focus our attention on the proportion between probabilites
as long as µ(s, a) > 0,
∀
µ(s2, a1) and µ(s1, a1), thus setting µ(s2, a1) = 1
µ(s1, a1). Figure 10 (a) displays the inﬂuence of the data distribution,
namely the proportion between µ(s1, a1) and µ(s2, a1), on the learned policy when α = 5/4. As can be seen, for the present
0.5. Precisely, as aforementioned, the optimal
MDP, the optimal policy is only attained if µ(s1, a1)
policy is retrieved if w∗1 > w∗2 and αw∗1 < w∗3. Such inequalities are veriﬁed when

0.5 and µ(s2, a1)

−

≈

≈

α(20α + 35)

80.3 + 35α + 20α2 < µ(s1, a1) <
0.48) < µ(s1, a1) < (

(

≈

65α2 + 100.3α

30

−

0.51).

≈

65α2

0.5, the optimal policy will not be retrieved. As an example,
However, as can be seen in the ﬁgure, if µ(s1, a1) is not
48.8, w∗2 = 20 and w∗3 = 30. Thus, the policy is correct at state s1
if µ(s1, a1) = 0.7 and µ(s1, a2) = 0.3, we have w∗1 ≈
since w∗1 > w∗2, but wrong at state s2 since condition αw∗1 < w∗3 is not veriﬁed. We observe a somewhat similar trend for
[1, 3/2), as can be seen in Figure 10 (b): the retrieval of the optimal policy is dependent on µ(s1, a1) and µ(s2, a1).
α

≈

∈

The results above show that, due to the limited approximation power and correlation between features, the data distribution
impacts the performance of the resulting policies, even for the presently considered oracle-based algorithm (with access to
the true Q∗ function). The number of retrieved correct actions is directly dependent on the properties of the data distribution.

(a) α =

1.25

.
}

{

(b) α =

.
1.0, 1.1, 1.2, 1.3, 1.4, 1.5
}
{

Figure 10: The number of correct actions at states s1 and s2 of the four-state MDP for different data distributions. The
number of correct actions is calculated using Eqs. 11 for different µ(s1, a1) and µ(s1, a2) proportions.

B.2.2 Ofﬂine Temporal Difference Version

Naturally, practical algorithms do not have access to the unknown target, Q∗. A common choice of replacement is to use the
one-step temporal difference target. In such cases, the loss function becomes

(w) = E(st,at,st+1)

∼

L

(cid:34)(cid:18)

µ

r(st, at) + max
∈A

at+1

wTφ(st+1, at+1)

−

(cid:19)2(cid:35)

wTφ(st, at)

.

If we neglect the inﬂuence of the weight vector in the target (semi-gradient), the update equation for the weight vector is
given by

wt+1 = wt

∂

η

(w)

L
∂w

−

= wt + ηE(st,at,st+1)

∼

(cid:20)(cid:18)

µ

r(st, at) + max
∈A

at+1

wT

t φ(st+1, at+1)

−

wT

t φ(st, at)

(cid:19)

(cid:21)

φ(st, at)

.

1.00.80.60.40.20.0µ(s2,a1)0.00.20.40.60.81.0µ(s1,a1)012#correctactionsα=1.251.00.80.60.40.20.0µ(s2,a1)0.00.20.40.60.81.0µ(s1,a1)012#correctactionsα=1.0α=1.1α=1.2α=1.3α=1.4α=1.5where η denotes the learning rate. Component-wisely,

w1,t+1 = w1,t + ηµ(s1, a1) (r(s1, a1) + p(s2|

a1, a1) max
{

+ ηµ(s2, a1) (r(s2, a1)
= w1,t + ηµ(s1, a1) (r(s1, a1) + p(s2|
+ ηµ(s2, a1) (r(s2, a1)

αw1,t) α,

αw1,t) α

−

−

w2,t+1 = w2,t + ηµ(s1, a2) (r(s1, a2) + max
w2,t + η (r(s1, a2) + max

αw1,t, w3,t
{
w3,t)
w3,t+1 = w3,t + ηµ(s2, a2) (r(s2, a2)
w3,t) .
w3,t + η (r(s2, a2)

−

∝

{

∝

−

Qw(s2, a1), Qw(s2, a2)

} −

w1,t)

αw1,t, w3,t

w1,t)

} −

a1, a1) max
{

Qw(s2, a1), Qw(s2, a2)

w2,t) ,

} −

w2,t)

} −

(12)

Figure 11 (a) displays the inﬂuence of the data distribution, namely the proportion between µ(s1, a1) and µ(s2, a1), on the
0.5, we retrieve the optimal policy
retrieved policy. As can be seen, we identify three regimes: (i) whenever µ(s1, a1)
(i.e., the actions are correct at both states s1 and s2); (ii) if µ(s1, a1) < (
0.65), the
0.48) or (
0.52) < µ(s1, a1) < (
policy is only correct at one of the states; (iii) if µ(s1, a1) > (

≈
≈
0.65), the policy is wrong at both states.

≈

≈

≈

Akin to the oracle version (Eqs. 11), the data distribution plays a key role in the performance of the resulting policies.
However, for the present temporal difference target, the impact of the data distribution is enhanced due to the dependence of
the target on the weight vector. As an example, if µ(s1, a1) = 0.7 and µ(s1, a2) = 0.3, we have w∗1 ≈
51.3 and
w∗3 = 30. As µ(s1, a1) increases, not only w1 increases (similarly to the oracle version), but also w2 wrongly increases.
This is due to the fact that the target used in the estimation of w2 depends on w1 (Eqs. 12).

49.0, w∗2 ≈

(a) α =

1.25

.
}

{

(b) α =

.
1.0, 1.1, 1.2, 1.3, 1.4, 1.5
}
{

Figure 11: The number of correct actions at states s1 and s2 of the four-state MDP for different data distributions. The
number of correct actions is calculated by ﬁnding the ﬁxed-point of Eqs. 12 for different µ(s1, a1) and µ(s1, a2) proportions.

B.2.3 Online Temporal Difference Version with Unlimited Replay Capacity

Reinforcement learning algorithms usually collect data in an online fashion, by using an exploratory policy such as an
(cid:15)-greedy policy with respect to the current Q-values estimates. We now focus on such setting, showing that similar problems
arise for the previously presented MDP and set of features. Instead of considering a ﬁxed µ distribution, we now consider a
setting where the µ distribution is dynamically induced by a replay buffer. We focus our attention on the commonly used
(cid:15)-greedy exploration.

Figures 12 and 13 display the experimental results for the four-state MDP when α = 1.2, under two different exploratory
policies: (i) a fully exploratory policy, i.e., an (cid:15)-greedy policy with (cid:15) = 1.0; and (ii) an (cid:15)-greedy policy with (cid:15) = 0.05. Under
the current experimental setting, we consider a replay buffer size big enough such that we never discard old transitions. We
use an uniform synthetic data distribution, i.e., µ(s1, a1) = µ(s1, a2) = µ(s2, a1) = µ(s2, a2) = 0.25, as baseline.

With respect to the uniform baseline, as can be seen in Fig. 12, it outperforms all other data distributions, yielding two
correct actions, higher reward, and lower Q-values error. This is expected, as previously discussed, since under this synthetic
distribution we have that µ(s1, a1) = µ(s2, a1).

Regarding the fully exploratory policy, i.e., the (cid:15)-greedy policy with (cid:15) = 1.0, the agent is only able to pick the correct
action at state s1, featuring a lower reward and higher average Q-value error in comparison to the uniform distribution

1.00.80.60.40.20.0µ(s2,a1)0.00.20.40.60.81.0µ(s1,a1)012#correctactionsα=1.251.00.80.60.40.20.0µ(s2,a1)0.00.20.40.60.81.0µ(s1,a1)012#correctactionsα=1.0α=1.1α=1.2α=1.3α=1.4α=1.5baseline. This is due to the fact that the stationary distribution of the MDP under the fully exploratory policy is too far
from the uniform distribution to retrieve the optimal policy. As seen in Fig. 13 (b), under the stationary distribution, we
0.05. If we normalize the µ distribution accounting only for µ(s1, a1) and µ(s2, a1)
have µ(s1, a1)
0.1 and µ(s2, a1)
probabilities we have that µ(s1, a1)
0.3. As seen in Fig. 11, under such proportion of µ(s1, a1) and
0.7 and µ(s2, a1)
µ(s2, a1) probabilities, we only yield one correct action, as veriﬁed by the experiment.

≈
≈

≈

≈

Finally, for the (cid:15)-greedy policy with (cid:15) = 0.05, the performance of the agent further deteriorates, as displayed in Fig. 12.
As can be seen in Fig. 12 (c) and Fig. 14, such exploratory policy induces oscillations in the Q-values, which eventually
damp out as learning progresses. The oscillations arise due to an undesirable interplay between the features of the function
approximator and the data distribution: exploitation leads to changes in distribution µ that, in turn, drive changes in the
weight vector w (which implicitly regulates the action selection mechanism). The oscillations end up being dampened by
the fact that the replay buffer contributes to stabilize the data distribution in such a way that exploitation will not lead to
such big changes in the µ distribution. As µ approaches its stationary distribution (since the replay buffer is sufﬁciently
large), we have that Qw(s1, a1)
Qw(s1, a2) and Qw(s2, a1) > Qw(s2, a2), as can be seen in Fig. 14 (b). We reach an
undesirable solution: the agent cannot distinguish which is the best action at state s1. Thus, it keeps alternating between
actions a1 and a2 at state s1, as seen in Fig. 12 (a).

≈

Detailed description of the interplay between the features of the function approximator and the data distribution under
the (cid:15)-greedy exploratory policy with (cid:15) = 0.05: (i) in early episodes, exploitation leads to an increase in the probability of
sampling state-action pair (s1, a1) from the replay buffer since we estimate that Qw(s1, a1) > Qw(s1, a2) (Fig. 14 (b)). This
is clearly seen in Fig. 13 (c), as µ(s1, a1) features a steep increase in probability during early training; (ii) the increase in
probability µ(s1, a1), as well as decrease in probability µ(s2, a1), drives an increase in weight w1, as previously discussed;
(iii) due to the fact that the target used in the estimation of w2, associated with Qw(s1, a2), depends on w1 (Eqs. 12), the
increase in w1 will also drive an increase in w2. However, the increase in weight w2 is slower than the increase in weight
w1 because the pair (s1, a2) is underrepresented in the replay buffer, i.e., the learning of Qw(s1, a2) occurs at a much
slower pace than that of Qw(s1, a1). Nevertheless, happens that, when a certain threshold is surpassed, we wrongly estimate
Qw(s1, a1) < Qw(s1, a2), as can be seen in Fig. 14 (b) around episode 2 500. This leads to a drop in the obtained reward
and number of correct actions, as seen in Fig. 12, due to the fact that action a2 is wrongly taken at state s1; (iv) since we
now wrongly estimate Qw(s1, a1) < Qw(s1, a2), exploitation leads to an increase in probabilities µ(s1, a2) and µ(s2, a1),
driving weight w1 down, until µ(s1, a1) is low enough such that we correctly estimate Qw(s1, a1) > Qw(s1, a2) again.
As can be seen in Fig. 13 (c), after an initial increase, we observe a decrease in probability µ(s1, a1), as well as increase
in probability µ(s2, a1), between episodes 2500 and 5000; The increase in probability µ(s2, a1) is driven by the fact that
Qw(s2, a1) > Qw(s2, a2) always veriﬁes (Fig. 14 (b)); (v) the described interplay repeats again. However, the oscillation is
now dampened by the fact that the replay buffer is already partially-ﬁlled with previous experience.

(a) Number of correct actions.

(b) Greedy policy reward.

(c) Q-values mean error.

Figure 12: Four-state MDP experiments for different exploratory policies with an inﬁnitely-sized replay buffer.

B.2.4 Online Temporal Difference Version with Limited Replay Capacity

Lastly, we consider an experimental setting where the replay buffer has limited capacity. Figures 15 and 16 display the
experimental results obtained with the (cid:15)-greedy exploratory policy with (cid:15) = 0.05 by varying the size/capacity of the replay
buffer. As can be seen in Fig. 16, as the replay buffer size increases, the amplitude of the oscillations in the µ distribution
gets smaller. Moreover, as the replay buffer size increases, the oscillations in the Q-values and Q-values errors are smaller,
as seen in Fig. 15. Given the previous discussion, the obtained results are expected. The undesirable interplay between
the function approximator and the data distribution repeats as previously discussed for the inﬁnitely-sized replay buffer.

02500500075001000012500150001750020000Episode012#correctactions(cid:15)=0.05(cid:15)=1.0Uniform02500500075001000012500150001750020000Episode−40−20020406080100Reward(cid:15)=0.05(cid:15)=1.0Uniform02500500075001000012500150001750020000Episode0.000.050.100.150.200.250.30MeanQ-valueserror(cid:15)=0.05(cid:15)=1.0Uniform(a) Uniform.

(b) (cid:15) = 1.0.

(c) (cid:15) = 0.05.

Figure 13: Four-state MDP experiments for different exploratory policies with an inﬁnitely-sized replay buffer. The plots
display the data distribution probabilities µ(s1, a1) and µ(s2, a1), as induced by the contents of the replay buffer, throughout
episodes.

(a) Weights.

(b) Q-values.

Figure 14: Four-state MDP experiments for the (cid:15)-greedy exploratory policy with (cid:15) = 0.05 and an inﬁnitely-sized replay
buffer. The plots display the estimated weights and Q-values throughout episodes.

However, as the replay buffer gets smaller, the more the data distribution induced by the contents of the replay buffer is
affected by changes to the current exploratory policy, in this case the (cid:15)-greedy exploratory policy. Therefore, for smaller
replay buffers, exploitation leads to more steep changes in µ(s1, a1) and µ(s2, a1) probabilities, as seen in Fig. 16 (a). Such
changes in the µ distribution drive abrupt changes in weights w1 and w2, as well as estimated Q-values, as seen in Fig.
15 (a). Similarly to the inﬁnitely-sized replay buffer, the agent keeps alternating between phases where it estimates that
Qw(s1, a1) > Qw(s1, a2) and phases where it estimates the opposite. However, the period at which phases switch is rather
longer for smaller replay buffer sizes. Whereas for the inﬁnitely-sized replay buffer the amplitude of the oscillations is
dampened due to the fact that previously stored experience contributes to make the data distribution more stationary, this is
not as easily achieved by smaller replay buffers. As our results suggest, the size of the replay buffer inﬂuences the stability
of the data distribution, which can, in turn, affect the stability of the learning algorithm and quality of the resulting policies.

B.2.5 Discussion

In summary, this manuscript presented a set of experiments under a four-state MDP that show how the data distribution
can greatly inﬂuence the performance of the resulting policies and the stability of the learning algorithm. First, we showed
that, under ofﬂine RL settings, the number of correct actions is directly dependent on the properties of the data distribution
due to an undesirable correlation between features. Second, under online RL settings, not only the quality of the retrieved
policies depends on the data collection mechanism, but also an undesirable interplay between the data distribution and the
function approximator can arise: exploitation can lead to abrupt changes in the data distribution, thus hindering learning.
Additionally, we showed that the replay buffer size can also affect the learning stability.

Above all, the results presented emphasize the key role played by the data distribution in the context of off-policy RL
algorithms with function approximation. Despite the fact that we study a four-state MDP, we argue that the example here
presented can generalize into more realistic settings. In the ﬁrst place, it is possible to construct an MDP such that the
learning dynamics under a function approximator with state-dependent features are equivalent to the previously discussed

02500500075001000012500150001750020000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)02500500075001000012500150001750020000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)02500500075001000012500150001750020000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)02500500075001000012500150001750020000Episode020406080Weightvaluew1w2w302500500075001000012500150001750020000Episode020406080100Q-valueQw(s1,a1)Qw(s1,a2)Qw(s2,a1)Qw(s2,a2)(a) Replay buffer size = 10 000.

(b) Replay buffer size = 50 000.

(c) Replay buffer size =

.

∞

Figure 15: Four-state MDP experiments for the (cid:15)-greedy exploratory policy with (cid:15) = 0.05 under different replay buffer sizes.
The plots display the estimated Q-values throughout episodes.

(a) Replay buffer size = 10 000.

(b) Replay buffer size = 50 000.

(c) Replay buffer size =

.

∞

Figure 16: Four-state MDP experiments for the (cid:15)-greedy exploratory policy with (cid:15) = 0.05 under different replay buffer sizes.
The plots display the data distribution probabilities µ(s1, a1) and µ(s2, a1), as induced by the contents of the replay buffer,
throughout episodes.

approximator with (state, action)-dependent features. Second, we can abstract the example here presented by hypothesizing
that states s1 and s3 can correspond to states along an optimal trajectory of a larger MDP, and state s2 to a state outside the
optimal trajectory. If the features of the states are wrongly correlated (as in the example, we have a correlation between the
features of states s1 and s2), exploitation along the optimal trajectory can deteriorate the Q-values for states outside the
optimal trajectory (as in the example, state s3), which can give rise to algorithmic instabilites.

C SUPPLEMENTARY MATERIALS FOR SECTION 5

This section gives additional details with respect to Section 5. The section is divided as follows:

• subsection C.1: Experimental environments.

• subsection C.2: Experimental methodology.

• subsection C.3: Implementation details and algorithms hyperparameters.

• subsection C.4: Complete experimental results.

C.1 EXPERIMENTAL ENVIRONMENTS

Below, we detail the experimental environments used in this work.

C.1.1 Grid 1 and Grid 2 Environments

The grid 1 environment comprises a tabular 8
8 grid, as seen in Fig. 17 (a). The agent starts in the lower left corner (’S’
square) and aims to reach the upper right corner (’G’ square) as fast as possible. The grid 2 environment, as seen in Fig. 17

×

0500010000150002000025000300003500040000Episode020406080100120Q-valueQw(s1,a1)Qw(s1,a2)Qw(s2,a1)Qw(s2,a2)0500010000150002000025000300003500040000Episode020406080100Q-valueQw(s1,a1)Qw(s1,a2)Qw(s2,a1)Qw(s2,a2)0500010000150002000025000300003500040000Episode020406080100Q-valueQw(s1,a1)Qw(s1,a2)Qw(s2,a1)Qw(s2,a2)0500010000150002000025000300003500040000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)0500010000150002000025000300003500040000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)0500010000150002000025000300003500040000Episode0.0000.0250.0500.0750.1000.1250.1500.1750.200Probabilityµ(s1,a1)µ(s2,a1)(b), comprises a tabular 8
square) and aims to reach the upper right square (’G’ square) as fast as possible.

8 grid, with walls in the middle of the environment. The agent starts in the lower left square (’S’

×

Common to both environments, the episodes have a ﬁxed length of 50 timesteps. The action set is {up, down, right, left,
stay}. No diagonal movements are allowed. All actions lead to deterministic state transitions except for against-wall actions
(the environments are virtually delimited by four walls); in such cases the agent has a 0.01 probability of moving to an
adjacent square and with the remainder probability stays at the same square. The reward at each timestep is one if the agent
is at the goal state and zero otherwise. The underlying MDPs comprise (8

5 = 320 state-action pairs.

8)

×

×

Regarding the state features, each state is mapped to a 8-dimensional vector. Each entry of the vector is drawn, independently,
1, 1]. The state features are pre-computed during initialization and kept constant throughout
from an uniform distribution in [
training. We set the discount factor γ to 0.9.

−

(a) Grid 1 environment.

(b) Grid 2 environment.

Figure 17: Illustration of the grid environments.

C.1.2 Multi-path Environment

The multi-path environment (k, l), displayed in Fig. 18, comprises k parallel paths of length l. The agent starts in the
left-most state and aims to reach one of the k absorbing states with positive reward on the right, while avoiding to fall into
the absorbing state with zero reward. In other words, the agent aims at following one of the k paths all the way until the
positive-rewarding state without falling into the absorbing zero reward state.

{

1, 2, 3, ..., k

. All actions lead to deterministic state transitions except for the action taken in the initial
The action set is
state; at the initial state, the agent’s action succeeds with probability 1
p and with probability p, the agent will randomly
enter the ﬁrst state of one of the k paths. By default we set p = 0.01 The reward is one if the agent is at one of the k ﬁnal
absorbing states of the path and zero everywhere else. The episode length is 10 timesteps. By default, we set k = 5 and
l = 5. Therefore, the underlying MDP comprises (k

k = 135 state-action pairs.

l + 2)

−

}

×

×

Regarding the state features, each state is mapped to a 4-dimensional vector. Each entry of the vector is drawn from an
uniform distribution in [
1, 1]. The state features are pre-computed during initialization and kept constant throughout
training. We set the discount factor γ to 0.9.

−

C.1.3 Pendulum, Cartpole and Mountaincar Environments

The pendulum, cartpole and mountaincar environments are based on the OpenAI gym environments implementations. For
all environments, we set the discount factor γ to 0.99. For the pendulum environment, the action space is discretized into 5
equally-spaced discrete actions.

For all environment, and only with the purpose of calculating different properties of the datasets (such as coverage, or
entropy), we discretized the state-space. We used an histogram-like discretization with equally spaced bins. For both the
pendulum and mountaincar environments we discretized each dimension of the state-space using 50 bins; for the cartpole
environment we used 20 bins per state-space dimension.

SGSGFigure 18: Illustration of the multi-path (k, l) environment.

C.2 EXPERIMENTAL METHODOLOGY

In order to reliably compare the different algorithms used in this work, we followed a rigorous experimental evaluation
methodology. Under each experimental conﬁguration, we perform 4 training runs. Each training run is punctually evaluated
during training, as well as at the end of training, by running 5 evaluation rollouts. Each rollout comprises a full-length
5) performance indicators per train run, thus obtaining 4 performance samples.
episode. We then average the resulting (4
We focus our attention on two main types of performance indicators: (i) the reward obtained under evaluation rollouts; and
(ii) the Q-values errors, calculated using a value-iteration or Q-learning solution for the same MDP;

×

C.3

IMPLEMENTATION DETAILS AND HYPERPARAMETERS

We developed our software in a Python environment, using the following additional open-source frameworks: ACME
[Hoffman et al., 2020], and Tensorﬂow [Abadi et al., 2015]. Table 1 presents the default hyperparameters for the ofﬂine
DQN and CQL algorithms used in the experiments presented in Section 5.

Table 1: Algorithms hyperparameters used under Section 5 experiments.

Hyperparameter
Optimizer
Learning rate
Discount factor (γ)
Dataset size
Batch size
Target update period
Num. learning steps
Network layers
Alpha (CQL param.)

Value(s)
Adam
1e-3
0.9
50 000
100
1 000
100 000
[20,40,20]
1.0

Hyperparameter
Optimizer
Learning rate
Discount factor (γ)
Dataset size
Batch size
Target update period
Num. learning steps
Network layers
Alpha (CQL param.)

Value(s)
Adam
1e-3
0.99
200 000
100
1 000
200 000
[64,128,64]
1.0

Hyperparameter
Optimizer
Learning rate
Discount factor (γ)
Dataset size
Batch size
Target update period
Num. learning steps
Network layers
Alpha (CQL param.)

Value(s)
Adam
1e-3
0.99
1 000 000
100
1 000
500 000
[64,128,64]
1.0

(a) Grid 1, 2 and multi-path envs.

(b) Pendulum and mountaincar envs.

(c) Cartpole env.

C.4 COMPLETE EXPERIMENTAL RESULTS

complete

The
rl-data-distribution.herokuapp.com/.

experimental

can be

results

found in the

following interactive dashboard: https://

r=0r=0r=0kr=0...r=0r=1...r=0r=1lkr=0r=0kkkr=0r=0r=0