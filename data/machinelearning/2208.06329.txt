2
2
0
2

g
u
A
2
1

]
L
P
.
s
c
[

1
v
9
2
3
6
0
.
8
0
2
2
:
v
i
X
r
a

Multi-Model Probabilistic Programming

Ryan Bernstein

Columbia University
ryan.bernstein@columbia.edu

August 15, 2022

Abstract

Probabilistic programming makes it easy to represent a probabilis-
tic model as a program. Building an individual model, however, is only
one step of probabilistic modeling. The broader challenge of probabilis-
tic modeling is in understanding and navigating spaces of alternative
models. There is currently no good way to represent these spaces of
alternative models, despite their central role.

We present an extension of probabilistic programming that lets each
program represent a network of interrelated probabilistic models. We
give a formal semantics for these multi-model probabilistic programs, a
collection of eﬃcient algorithms for network-of-model operations, and
an example implementation built on top of the popular probabilistic
programming language Stan.

This network-of-models representation opens many doors, including
search and automation in model-space, tracking and communication of
model development, and explicit modeler degrees of freedom to mit-
igate issues like p-hacking. We demonstrate automatic model search
and model development tracking using our Stan implementation, and
we propose many more possible applications.

1

 
 
 
 
 
 
Contents

1 Introduction

. . . . . . . . . . . . . . . . . . .
1.1 Uses of networks of models
1.2 Representing networks of models
. . . . . . . . . . . . . . . .
1.3 Swappable modules in Stan . . . . . . . . . . . . . . . . . . .

2 Related work

2.1 Comparison to ML-like module systems

. . . . . . . . . . . .

3 Background: Stan

3.1 Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Eﬀects and Scope . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Program validity . . . . . . . . . . . . . . . . . . . . . . . . .

4 Modular Stan syntax

5 Modular Stan semantics

5.1 Basic operations on programs . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
5.2 Structural constraints
5.3 Module signatures and semantic constraints . . . . . . . . . .
5.3.1 Module Signatures . . . . . . . . . . . . . . . . . . . .
Semantic constraints . . . . . . . . . . . . . . . . . . .
5.3.2
5.4 Program validity . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Selections . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Semantic domain . . . . . . . . . . . . . . . . . . . . . . . . .

6 Algorithms

6.1 Concretize . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.1
ApplyImpl . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.2 Correctness of ApplyImpl . . . . . . . . . . . . . . . .
6.1.3
ApplyImpls . . . . . . . . . . . . . . . . . . . . . . . .
6.1.4 Correctness of ApplyImpls . . . . . . . . . . . . . . .
6.1.5
Concretize . . . . . . . . . . . . . . . . . . . . . . . .
6.1.6 Correctness of Concretize . . . . . . . . . . . . . . .
6.1.7 User speciﬁcation of a selection . . . . . . . . . . . . .
6.2 ModelGraph . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2.1 Naive versions . . . . . . . . . . . . . . . . . . . . . . .
6.2.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
6.2.3 Proofs of correctness and completeness . . . . . . . . .
6.2.4 Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . .

4
4
7
8

11
12

13
14
16
16

16

19
19
21
21
21
23
23
24
25

25
25
26
27
31
31
32
32
32
33
33
35
36
42

2

6.3 ModelNeighbors . . . . . . . . . . . . . . . . . . . . . . . . .
6.3.1 Proofs of correctness and completeness . . . . . . . . .
6.3.2 Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . .

7 Additional features
7.1 Append blocks
. . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Module ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Macros

8.1 Collection holes . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2
Indexed holes . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 Hole instances and hole copies . . . . . . . . . . . . . . . . . .
8.3.1 Hole instances . . . . . . . . . . . . . . . . . . . . . . .
8.3.2 Hole copies
. . . . . . . . . . . . . . . . . . . . . . . .
8.3.3 Ranged hole instances and copies . . . . . . . . . . . .
8.4 Multi-ranges and range exponents . . . . . . . . . . . . . . . .
8.5 Hole products and hole exponents . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
8.6 Example application of macros

9 Example case studies

9.1
9.2

9.3

Interactive web interface . . . . . . . . . . . . . . . . . . . . .
“Golf” case study: Modular Stan for ease and clarity of devel-
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
opment
“Birthday” case study: Modular Stan as a platform for au-
tomation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Future work

43
43
45

46
46
47

49
49
51
51
52
53
53
53
53
54

57
58

59

61

64

3

1

Introduction

A probabilistic program is a program that represents a probabilistic model.
Probabilistic programming suggests an analogy between software engineering
and probabilistic modeling: developing a model is like developing a program.
However, there is a key diﬀerence: while there are only ever a handful of rel-
evant versions of a program in development, there are often a multitude of
alternative probabilistic models that remain relevant throughout develop-
ment, communication and validation. Probabilistic programming systems
currently ignore this multiplicity.

We present an extension of probabilistic programming that lets users
encode networks of models, graphs of models connected by similarity. Just
as probabilistic programs let users represent and query probabilistic mod-
els, these multi-model probabilistic programs let users represent and query
networks of probabilistic models.

1.1 Uses of networks of models

We focus on four categories of use cases for networks of probabilistic models:

1. Automation in model-space

Networks of models allow us to deploy automation in model-space.
For example, algorithms can search for models or neighborhoods in
the network that optimize a real-valued model scoring function. We
demonstrate a greedy graph search in section 9.3 and more advanced
search methods are discussed in section 10. Other examples of automa-
tion on networks include:

• Stacking methods seek an optimal weighted subset of the models
to combine into a high-performing ensemble or “stacked” model
[1].

• Projection prediction methods search for the “simplest” model
that reproduces the predictions of a gold standard model or en-
semble [2].

2. Understanding the problem and its solution space.

Statisticians often use multiple related models to gain insight into their
problem and solution space [3]. Examples include:

• Plotting alternative models by evaluation metrics to understand

trade-oﬀs such as model complexity vs. accuracy [4].

4

• Comparing diagnostic samples, such as posterior predictive sam-

ples, to understand the impact of model decisions [5].

• Applying “multiverse” methods, which seek to quantify modeling
uncertainty by sampling from a whole set of plausible models [6].

3. Tracking and communicating the branching path of develop-

ment.
Probabilistic model development is an inherently iterative and branch-
ing process: models are improved by a cycle of criticism and adjustment
[3, 7]. Development follows an often-backtracking path through model
space; a path that model developers often have diﬃculty managing
[8, 9, 10].

When the relevant region of model space is explicitly represented as
a network of models, the path of development can also be explicitly
documented. This documentation is useful for managing development
[10], for third parties interested in learning from or extending the work,
and, as discussed in case 4, as context for third-party auditors assessing
the quality of the ﬁnal model.

Standard version control tools like Git can serve this use case to some
extent, but as the case study in section 9.2 shows, the network of mod-
els abstraction earns more than version control:
it keeps all relevant
models available and tracks their semantic relationships. Two surveys
of data scientists, one by Guo and one by Kery et al., both report that
version control is not typically used for managing exploratory data
analysis [8, 10].

4. Making modeler degrees of freedom explicit.

P -hacking [11] and the Garden of Forking Paths [12] are issues in statis-
tics with a common root cause called modeler degrees of freedom [13]:
when a modeling task, such as the analysis of scientiﬁc data, includes
modeling decisions with multiple justiﬁable solutions, the modeler can,
intentionally or not, tune their solutions to cherry-pick a desirable
model, such as a model with a “signiﬁcant” p-value.
There are two ways in which explicit networks of models can alle-
viate the issues of modeler degrees of freedom. The ﬁrst way is to
aid in reporting the set of analyses that were done. Wigboldus and
Dotsch, discussing questionable research practices, argue: “the poten-
tially (highly) questionable part of your actions as a researcher is not

5

that you engage in all kinds of exploratory analyses. Instead, the ques-
tionable part is not reporting truthfully and explicitly the exploratory
nature of these analyses” [14]. When researchers explicitly report their
path of exploration through the network of models, their degrees of
freedom become transparent.

The second way is to automate sensitivity analysis. Sensitivity analy-
ses aim to “assess whether altering any of the assumptions made leads
to diﬀerent ﬁnal interpretations or conclusions” [15]. They are broadly
recommended for scientiﬁc reports such as clinical trials [16].
In a
survey of solutions to researcher degrees of freedom, Rubin ﬁnds that
sensitivity analyses “provide an eﬀective solution to the p-hacking prob-
lem”, and also the Forking Paths problem in some cases1 [17]. Explicit
networks of models let both researchers and third-party auditors au-
tomate sensitivity analysis by a simple procedure: for each conclusion
drawn from a ﬁnal model, check the extent to which that conclusion is
also drawn by the model’s neighbors. When each neighbor diﬀers by
one modeling decision, the analysis tells us the modeling decisions on
which the conclusions depend.

Some of these use cases, like automated model search and visualizations
of model space, are achieved in practice only by those with suﬃcient time and
expertise to implement ad-hoc methods with hand-enumerated sets of mod-
els. Other use cases, like explicit modeler degrees of freedom and automated
sensitivity analysis, are rarely ever achieved in practice despite being valu-
able in theory. We argue that all of these use cases could become convenient
and routine if we had a standard representation of networks of models.

We note that each of the four use cases above has a natural deﬁnition

and utility for drawing edges between models:

• For use case 1 (automation), edges should be between the most similar
models: the network is then more analogous to a continuous and dif-
ferentiable space, and methods like greedy graph search more closely
approximate gradient descent.

• For use case 2 (solution space mapping), again edges should be between
the most semantically similar models; edges then provide a more con-
sistent sense of distance and orientation.

1Rubin distinguishes between result-biased and result-neutral Forking Paths, and ﬁnds

that sensitivity analysis is not necessarily suﬃcient for result-neutral cases.

6

• For use case 3 (tracking and communicating development), edges should
bridge sequential versions of a model. Authors and auditors then can
cleanly trace model development, with each of its decision points and
model transformations, as a tree within the network.

• For use case 4 (explicit degrees of freedom), edges should be between
model pairs that are that are one “decision” apart; then each edge is
like a step in the Garden of Forking Paths.

Ideally, a standard construction of model networks should be compatible

with all of these use cases.

1.2 Representing networks of models

To support all of the above use cases, representations of networks of models
should support the following operations eﬃciently:

1. Explicitly generate the network of models (if practical).

2. Given a node, generate its set of neighbors. This operation
becomes necessary when the network is too large to practically generate
in its entirety; for instance, when we are searching through a large
model space.

3. Given a node, generate its corresponding probabilistic pro-

gram.

In addition, we would like our representation to be:

1. Easy to read and understand. Especially for cases 3 and 4, clarity

is the priority.

2. Simple to write. Probabilistic modelers may not be expert program-

mers.

3. Scalable to many models.

If a representation is too redundant,

large numbers of models become cumbersome.

4. Standardized.

A standard format enables a stable network-of-models API to easily
build general model-space tools.

7

One obvious choice for representing multiple models is a directory of
relevant probabilistic programs. According to surveys done by Guo and
Kery et al., this is the typical solution among data scientists [8, 10].

While appealing for their simplicity, ﬁle collections are a poor solution
because they become uninterruptible, unmanageable, and memory intensive
as the number of models grows large, and because they discard the semantic
relationships between models. Kery et al. also report that data scientists
have issues with ﬁle naming, keeping track of the relationships between ﬁles,
and maintaining a mental map of their code [10].

Another possible choice of representation is a program in a general-
purpose language that generates the set of probabilistic programs. This
approach is ﬂexible and scalable, but its ﬂexibility also makes it prohibitively
diﬃcult to write, understand, and standardize.

Our proposed representation is a middle ground: a meta-programming
feature to augment existing probabilistic programming languages, so that
meta-programs, which we call multi-model programs, represent networks of
probabilistic models. We argue that our meta-programming approach is
nearly as ﬂexible as general program generation and easier to write and
understand than the directory-of-programs approach for nontrivial examples.
We call our meta-programming feature swappable modules.

1.3 Swappable modules in Stan

To demonstrate swappable modules, we use Stan as our example host lan-
guage, mainly because Stan is popular, performant, and has a clear, estab-
lished semantics [18, 19]. Stan is also a highly structured and restrictive
language, so by adding a swappable modules to Stan, we are demonstrating
an usually diﬃcult case that can easily be transferred to other languages.

We refer to the Stan language augmented with swappable modules as
modular Stan. We have built a prototype compiler [20] and interactive visu-
alization website [21] for modular Stan.

Figure 1 shows an abstracted modular Stan program and two data struc-

tures derived from it.

A modular Stan program is made of two parts. The ﬁrst part is the base.
The base is like a Stan program, except that its code can contain holes,
which are syntactically similar to function calls but are more ﬂexible. The
second part is a list of module implementations. Each module implementa-
tion describes a new way to “ﬁll” a particular hole. More than one module
implementation be speciﬁed to “ﬁll” the same hole. Module implementations
can themselves contain holes.

8

Figure 1: The correspondence between a modular Stan program and a net-
work of models. Holes are represented as empty shapes. Modules are rep-
resented as ﬁlled shapes, with the same shape as the hole that they ﬁll and
a unique color. (a) and (b) represent a modular Stan program in its two
parts, the base (a) and the module implementations (b). (c) represents the
relationships between the base, holes and modules implementations.
(d)
represents the model graph, where each node is a model corresponding to a
valid selection of modules, and each edge is labeled with the hole by which
its endpoints diﬀer. (e) shows the Stan program corresponding to a node; it
is synthesized from the selected modules of the node.

9

We can visualize the holes and implementations, with their implementation-

ﬁlls-hole and code-contains-hole relationships, as a rooted directed acyclic
graph of base, hole and module implementation nodes, where the root repre-
sents the program’s base, each base and module implementation node points
to the holes it contains, and each hole node points to the module implemen-
tations that ﬁll it. We refer to this graph as the module graph of a program.
The module graph is not necessarily a tree because multiple pieces of code
can contain the same hole.

To build a valid Stan program out of a modular Stan program, we need
to select one module implementation to ﬁll each hole in its base, and one
implementation to ﬁll each hole in those implementations, and so on, until
there are no holes. A minimal set of module implementations that leaves no
empty holes is called a valid selection. Given a modular Stan program and
a valid selection, we can produce a valid Stan program by ﬁlling each hole
with the given module implementation.

A modular Stan program can therefore represent many Stan programs:
one for each valid selection.
If we consider each of these Stan programs
to be a node, we ﬁnd a natural network structure: draw an edge between
two Stan program nodes when their selections conﬂict by only one choice.
Figure 1 shows such a graph with correspondence of each single node to
a Stan program and thereby to a probabilistic model. We argue that this
graph construction is a powerful way to represent networks of models.

Holes and module implementations are like function calls and function

deﬁnitions, but with important diﬀerences:

• Selecting a module implementation can add model parameters and
have other changes that are not local to their call site. Stan requires
the set of parameters to be ﬁxed at compile time; therefore modules
are selected before Stan’s compile time.

• More than one module implementation can be speciﬁed to ﬁll a hole,

like function overloading but with identical type signatures.

• Because module implementation code is essentially inlined statically,

holes can appear in places where function calls cannot.

By deﬁning a module system for Stan, we are also incidentally letting
users write their normal Stan programs in a more modular way. Modularity
of this level is a feature that is both important for probabilistic modeling
workﬂow [3] and conspicuously absent from probabilistic programming lan-
guages like Stan [22].

10

Probabilistic programming languages augmented with swappable mod-

ules meet all of our earlier criteria to represent networks of models:

• They are easy to read and write, because only one abstraction (the
module) and a small amount of syntax is introduced on top of the host
language, and the module abstraction already ﬁts naturally into the
probabilistic modeling workﬂow [3].

• They are scalable, because the combinatory nature of implementation
selection can deﬁne many models with little code. The language ex-
tensions in section 7 further increase that expressiveness.

• They are standardized, because compilers enforce well-deﬁned lan-

guages.

• The required network-building, neighbor-ﬁnding and model-selection

operations can be implemented eﬃciently.

In addition, the deﬁnition of edges as between models that diﬀer by one

choice module is compatible with all four uses cases’ deﬁnitions of edges.

In this paper we introduce the syntax, semantics, and operation algo-
rithms of a swappable module system for Stan. For each algorithm we pro-
vide proofs of completeness, correctness and eﬃciency.

We also introduce language extensions and macros to conveniently ex-
press common patterns of model variation in terms of our module system.
We also introduce a macro system to concisely express common patterns of
model variation and large, complex model families.

We demonstrate some of the beneﬁts of multi-model probabilistic pro-

grams with two brief but real-world data science case studies.

2 Related work

The ’network of models’ is an established concept in statistics and machine
learning, but explicit and general network of models abstractions are not
supported within probabilistic programming languages or other statistical
software. For instance, multi-model methods like ensemble methods and
model search are common in machine learning, but are typically ad-hoc in
that they do not start with a declarative network topology for their set of
model. The closest method that we are aware of is perhaps grid search,
which searches a pre-deﬁned grid of model hyperparameters.

11

There are existing probabilistic programming systems which allow users
to deﬁne their programs by combining probabilistic subcomponents. For
example, Prophet [23] allows users to combine subcomponents into time-
series models. This eﬀect could also be achieved in embedded probabilistic
programming languages whose host languages have suﬃciently expressive
module systems. We are not aware of any other module systems specialized
to encapsulate ﬂexible components of structured probabilistic programs.

Kery et al. developed a tool for code editors, Variolite, to support ex-
ploratory data science by tracking alternative snippets of code in version
control. Variolite lets developers write, visualize and manage iterative and
branching versions of data science pipelines in a similar way to our proposal.
Variolite addresses what we call use case 3 (development tracking). Variolite
diﬀers from our system primarily in that it is a tool for code editors rather
than a metaprogramming feature, it does not produce a network of models,
and because it is language agnostic, it only supports swapping out regions
of code rather than more general semantic units.

There are existing systems that allow users to specify a program’s com-
ponents at compile-time. Backpack [24] is a build-system tool for the Haskell
ecosystem that lets users swap out external software libraries that implement
a common interface. Much of work done by Yang to introduce Backpack as
a mixin linker also applies to our swappable module system.
In practice,
the C preprocessor is often used for this purpose; it can include or exclude
sections of code depending on user ﬂags or system environment properties.
These systems are not commonly applied to probabilistic programs or to
study networks of programs.

2.1 Comparison to ML-like module systems

Our swappable module system bears some resemblance to ML-like module
systems [25, 26]. We ﬁnd OCaml to be a helpful comparison point [27]. The
hole and module approach can be understood in the language of ML-like
modules:

Each hole in a program, which is to say each unique hole identiﬁer refer-
enced in a program, can be thought of as declaring a module signature and
a module-valued variable of that signature. Each statement and expression
referencing a hole is like a reference to a ﬁeld of that hole’s corresponding
variable. A hole’s variable may take the value of any module that “imple-
ments” it. A hole’s signature is inferred from its usage and implementations.
The value assigned to a hole’s variable may either be speciﬁed outside of
the program or left non-deterministic. Blocks of code that contain holes

12

can then be thought of as ML functors in that their holes are like implicit
module-valued arguments.

However, unlike ML-like modules:

1. Modules are not applied at any point in the program. Rather, the
programs’ semantic domain is the set of programs generated by any
combination of module applications, as though module applications
were non-deterministic; then the user can determine the modules to
apply in an optional mode of compilation called concretization that we
discuss in section 6.1.

2. When a module is assigned to a hole, or selected, there can be global
eﬀects on the semantics of the program, because the module may add
code to Stan’s top-level blocks. For instance, a module may introduce
a new model parameter, which changes the domain over which the pro-
gram deﬁnes a joint distribution. This eﬀect is especially noteworthy
in languages like Stan in which model parameters are ﬁxed at compile
time, because it implies that module application must happen before
compile time.

We are not aware of prior examples of inferred implicit module signatures,
non-deterministic functor application semantics, or module application with
non-local eﬀects on the resulting program.

3 Background: Stan

Stan is our example host language, so we give a brief overview of Stan pro-
grams and of Stan’s syntax in this section.

Like all probabilistic programs, Stan programs represent probabilistic
models. Stan programs are C-like, imperative, and are written as a sequence
of top-level blocks. Here is a simple example of a Stan program:

data {

int N;
vector[N] x;

}
parameters {
real mu;
real sigma;

}
model {

13

mu ~ normal(0, 1);
sigma ~ lognormal(0, 1);
x ~ normal(mu, sigma);

}

We see three of Stan’s blocks: data, parameters, and model. The data
block declares the observed variables and the parameters block declares the
unobserved variables. The model block deﬁne the log-density of the joint
distribution of the observed and unobserved variables. Each ~ statement im-
plicitly increments a variable target that represents the value of the overall
log-density function; for instance, mu ~ normal(0, 1); could be rewritten
target += normal_lpdf(mu, 0, 1);.

The above program represents the probabilistic model:

µ „ N ormalp0, 1q
σ „ LogN ormalp0, 1q
x „ N ormalpµ, σq

where x is an observed variable. When the program is compiled and executed
given data for x, it should produce samples from the posterior distributions
P pmu|xq and P psigma|xq.

For our purposes, it is suﬃciently precise to take the semantic domain
, to be a joint distribution P pd, θq of the variables
of a Stan program p,
(cid:75)
declared in the data block, d, and parameters block, θ, of p, from which
can be inferred a posterior distribution P pθ | dq.

p
(cid:74)

3.1 Syntax

Stan programs are organized into blocks of statements that describe diﬀerent
aspects of a probabilistic model. Each is an ordered subset of blocks:

STAN_PROG: FUNCTIONS?

DATA?
TRANSFORMED_DATA?
PARAMETERS?
TRANSFORMED_PARAMETERS?
MODEL?
GENERATED_QUANTITIES?

Here ? indicates that the block may or may not be present.

Below is the syntax of the data block:

14

DATA: data { STMT_DECL;* }
STMT_DECL: TYPE identifier
TYPE: int | real | vector | matrix | ...

Here, identifier stands in for valid Stan variables names. The * symbols
are Kleene stars to indicate an element that can be repeated or absent.

The parameters block is similar2 :

PARAMETERS: parameters { STMT_DECL;* }

The model block is more ﬂexible:

MODEL: model { STMT_LPDF;* }
STMT_LPDF: STMT_BASIC

| identifier ~ identifier( EXPR,* );
| target+= EXPR;

STMT_BASIC: STMT_DECL | STMT_ASSIGNMENT | STMT_FOR

| STMT_IFELSE | STMT_FUNCTION_APPLICATION | ...

Here, target is a reserved variable in Stan that represents the accumulated
log-value of the density function.

STMT_BASIC and EXPR closely resemble C-like languages, so we omit their

details here.

Stan allows user-deﬁned function declared in the functions block:

FUNCTIONS: functions { FUNC_DECL* }
FUNC_DECL: RET_TYPE identifier ((TYPE identifier),*) { STMT_FUNC;* }
STMT_FUNC: STMT_BASIC | return EXPR
RET_TYPE: TYPE | void

There are three more blocks:

TRANSFORMED_DATA: transformed data { STMT_BASIC;* }
TRANSFORMED_PARAMETERS: transformed parameters { STMT_LPDF;* }
GENERATED_QUANTITIES: generated quantities { STMT_BASIC;* }

The transformed data and transformed parameters blocks let users
deﬁne transformed versions of the observed and hidden variables in a way
that works eﬃciently with the inference process. The generated quantities
block lets users deﬁne output quantities calculated from the samples of the
parameters.

2Strictly speaking, we should not allow discrete types like int to be declared as

parameters.

15

3.2 Eﬀects and Scope

Let Block be the set of Stan block types, tdata, parameters, model, . . . u.

Stan statements and expressions are sometimes allowed to be impure in
particular ways depending which block contains that code. We call those im-
purities eﬀects. We say that when code uses the random number generator,
it has the RNG eﬀect, and when it increments the program’s density function
with ~ or target+= statements, it has the LPDF eﬀect. The set of eﬀects
Eff is then tRNG, LPDFu. Stan’s speciﬁcation implicitly deﬁnes a mapping
effects from some block P Block to the set of eﬀects allowed within that
block, effectspblockq Ă Eff.

In Stan programs, the declarations that a statement may reference de-
pends on the statement’s block and the declaration’s block. For instance,
code in a transformed data block can reference top-level declarations in a
data block but not in a parameters block. To know whether it is valid to
inserting new statements into a given block, therefore, we need to know to
which blocks’ declarations that statement is allowed to refer. Stan’s speci-
ﬁcation implicitly deﬁnes a mapping scope from some block P Block to the
set of blocks whose top-level declared variables statements that block may
reference, scopepblockq Ă Block.

3.3 Program validity

We deﬁne validStan to denote whether a program is valid, or roughly whether
we expect it to compile. Let validStanpSP q for a Stan program SP if and
only if all of the following are true:

1. Eﬀects and scopes of code are available in their block.

2. SP typechecks as Stan code.

4 Modular Stan syntax

Below is an example modular Stan program:

data {

int N;
vector[N] x;

}
model {

x ~ normal(Mean(), Stddev());

16

}

module "standard" Mean() {

return 0;

}

module "standard" Stddev() {

return 1;

}

module "normal" Mean() {

parameters {
real mu;

}
mu ~ normal(0, 1);
return mu;

}

module "lognormal" Stddev() {

parameters {

real<lower=0> sigma;

}
sigma ~ lognormal(0, StddevInformative());
return sigma;

}

module "yes" StddevInformative() {

return 1;

}

module "no" StddevInformative() {

return 100;

}

In the above program, the base is made up of the data and model blocks,
Mean, Stddev and StddevInformative are holes, and each block starting
with the keyword module are the module implementations.

The above modular Stan program has similar structure to the exam-
ple in ﬁg. 1. It also includes the example program section 3 as one of its
nodes (where Mean is ﬁlled by normal, Stddev is ﬁlled by lognormal, and

17

StddevInformative is ﬁlled by yes).

Modular Stan makes two additions to Stan’s syntax: Holes and module

implementations.

Holes are statements or expressions that are syntactically similar to func-
tion applications. We deﬁne variants of the Stan syntax rules that are
allowed to include holes. For each Stan grammar rule RULE that can di-
rectly or indirectly contain a STMT_BASIC, STMT_LPDF, or EXPR, we deﬁne a
new rule RULE_M that replaces those rules with the following STMT_BASIC_M,
STMT_LPDF_M, and EXPR_M rules, respectively.

STMT_BASIC_M: hole_identifier(EXPR_M,*);

| STMT_DECL | STMT_ASSIGNMENT_M | STMT_FOR_M
| STMT_IFELSE_M | STMT_FUNCTION_APPLICATION_M | ...

STMT_LPDF_M: identifier ~ hole_identifier(EXPR_M,*);

| identifier ~ identifier( EXPR_M,* );
| target+= EXPR_M;
| STMT_BASIC_M

EXPR_M: hole_identifier( EXPR_M,* ) | ...

We use hole_identifier to stand in for valid hole names.

Module implementations are reminiscent of function deﬁnitions, and ap-

pear at the top level alongside blocks:

MODULE_IMPLEMENTATION_M:

module "impl_identifier" hole_identifier((TYPE identifier,)*) {

PARAMETERS?
STMT_LPDF_M;*
return EXPR_M;*

}

We use impl_identifier to stand in for valid implementation names, while
module is a new keyword.

A modular Stan program is then:

MODULAR_STAN_PROG: STAN_PROG_M

MODULE_IMPLEMENTATION_M*

We make two small additions to the syntax and capabilities of modular
Stan in section 7, but this base syntax is suﬃcient to introduce its semantics
and algorithms.

18

5 Modular Stan semantics

5.1 Basic operations on programs

We assume that there is some parsing procedure Parse such that, for all
strings F of the language deﬁned by the syntax in section 4, P “ ParsepF q,
where P is some reasonable representation of F that we refer to loosely as a
“modular Stan program”.

While the actual representation of a program P is an implementation
detail, we can think of P as eﬀectively a pair P “ pPbase, implspP qq, where
Pbase represents the Stan-like base of P and implspP q is the set of all module
implementations deﬁned in P . P also implicitly includes the set of all holes
referenced by Pbase and implspP q.

We likewise are not concerned with the representations details of imple-

mentations or holes, we only need to deﬁne operations on them.

Below are the basic operations we use to interact with programs P , im-

plementations i, holes h, sets I of implementations, and sets H of holes:

implsphq is the set of implementations that implement a hole h.

implspP q is the set of all implementations deﬁned in P .

implspHq “

Ť

hPH implsphq.

holespiq is the set of holes referenced in the deﬁnition of an implementation

i.

holespPbaseq is the set holes referenced in the base of P .

holespIq “

Ť

iPI holespiq.

holespP q “ holespPbaseq Y holespimplspP qq.

parpiq is the hole that the implementation i implements, also called the

parent of i.
Ť

parspIq “

iPI parspiq.

parspP q “ parspimplspP qq.

The above operations are speciﬁc to the context of a program P . Since
the intended P is usually clear, we only give the operation a subscript when
disambiguation is necessary.

19

We note that par and impls operations are like inverses, so:

i P implsphq ô h P parspiq

h P parspIq ô Di P I s.t. i P implsphq

i P implspHq ô Dh P H s.t. h “ parpiq

It is also be useful to note that, for all I1 Ă I2:

holespI1q Ă holespI2q

implspI1q Ă implspI2q
parspI1q Ă parspI2q
We also need to query certain syntactic elements of the code. Some of these
operations are not fully detailed in the interest of brevity.

We call locations or spans sites. We deﬁne a HoleSite as some data
structure that captures the syntactic information of a hole called within
code.

These are the operations on HoleSites hs, programs P , implementations

i, and blocks b:

sitespiq or sitespPbaseq is the set of hole sites in the code of i or Pbase, so

that |sitespcq| ě |holespcq|.

sitespP q is the set of hole sites in all of the code of P .

sitepbq is the site of the start of the code of block b.

blockphsq is the block that contains hs, if any.

holephsq is the hole that is called at hs.

scopepiq is the set of blocks whose top-level declarations i references.

ef f ectspiq Ă Eff is the set of effects whose top-level declarations i refer-

ences.

We can syntactically break down implementations i into a triple, pibody,

ireturn, iparametersq, so that:
ibody is the sequence of statements that makes of the code of the implemen-

tation,

ireturn is the expression returned, if any,
iparameters is the sequence of declarations of parameters made by the imple-

mentation.

20

5.2 Structural constraints

Not all modular Stan programs that can be parsed are valid; we also impose
certain structural and semantic constraints.
Input programs that do not
meet the constraints are be rejected by the compiler.

Below are the structural constraints on a modular program P :

1. The dependency graph of modules is acyclic:

For any graph G, let N pGq be the set of nodes and EpGq be the
set of edges. Let the module dependency graph M DGpP q be the di-
rected graph with nodes N pM DGpP qq “ implspP qYtPbaseu and edges
EpM DGpP qq “ t i1 Ñ i2 | i1 P implspP q Y tPbaseu, i2 P implspP q, parpi2q P holespi1q u.
We require that this graph is acyclic. We also refer to this property as
AcyclicpP q.

2. Every hole has an implementation. @h P holespP q Y holespPbaseq,

implsphq ‰ H.

3. Hole identiﬁers are unique and (hole identiﬁer, implementation identi-

ﬁer) pairs are unique.

When a program P meets these constraints, we say validstructurepP q.

5.3 Module signatures and semantic constraints

5.3.1 Module Signatures

We would like to be able to guarantee that any concrete Stan program gen-
erated from a modular Stan program P is valid. In order to do that, we need
to understand the type, scope, and eﬀect implications ﬁlling holes with their
implementations.

To that end, we attempt to infer a signature for every hole in P . A
signature is like a function type plus extra information. When no signature
can be inferred for a hole, we reject the program as invalid. Signatures let
us specify semantic constraints on input programs, and are also be useful for
generating Stan programs in section 6.1.

A signature s is a tuple:

s “ psarg´types, sret´types, sef f ects, sscopeq

sarg´types speciﬁes the argument types of a hole. sarg´types is a sequence

of Stan types, like the TYPE syntactic element introduced in section 3.

21

sret´type speciﬁes the return type of a hole. sret´type is a Stan return

type, like the RET_TYPE syntactic element introduced in section 3.

sef f ects refers to the set of eﬀects that a hole’s implementation may have.
sscope refers to the scope of non-local variables that a hole’s implementa-

tion may reference.

1. Module Signature Inference We deﬁne a procedure for either inferring
the signature of a hole given the type determinations at each of the
hole’s call sites and all of the hole’s implementations, or rejecting the
program as invalid.
We rely on an operation ReturnType that infers the type of the ex-
pression returned by implementation code, if any, given that the types
of all other expressions in that code are available. Type annotating
is a standard operation for compilers of typed languages including the
Stan compiler [28]. We use the following interface:

ReturnTypeparguments, iq

where arguments is the collection of variable names and types available
within the code of the implementation i.
We visit each hole in topological order by dependency, such that when
we visit a hole h, for all i P implsphq, all of the holes h1 P holespiq have
already been visited. This ordering is always possible because of the
AcyclicpP q property.

signaturephq “

`

argtypespimplsphq0q,
ReturnTypepargtypespimplsphq0q, implsphq0q,

ď

ď

ef f ectspibodyq Y

signatureph1qef f ects,

iPimplsphq
ď

scopepibodyq Y

h1Pholespiq
ď

signatureph1qscope

˘

iPimplsphq

h1Pholespiq

implsphq0 is an arbitrary element of implsphq; implsphq are never
empty by structural constraint 2.
This way, sarg´types is assigned to the argument types of any of its
implementations, sret´types is assigned to the type that can be inferred
from any of its implementations with the return types of dependent
holes, and sef f ects sscope are the unions of the eﬀects and scopes re-
quired by any of the hole’s implementations or descendants.

22

5.3.2 Semantic constraints

We give a set of semantic constraints on programs in terms of signatures:

1. Implementations match signature argtypes. @i P implspP q, iargtypes “

signaturepparpiqqargtypes

2. Implementations match signature rettype. @i P implspP q, irettype “

signaturepparpiqqrettype

3. Eﬀects and scopes of holes are available in their block or module sig-

nature. @i P implspP q, @st P sitespiq:

(a) signaturepstholeqef f ects Ă signaturepparpiqqef f ects
(b) signaturepstholeqscope Ă signaturepparpiqqscope

@st P sitespPbaseq:

(a) signaturepstholeqef f ects Ă ef f ectspblockq
(b) signaturepstholeqscope Ă scopepblockq

4. Eﬀects and scopes of code are available in their block or module sig-

nature.

5. Pbase would typecheck under Stan if all holes h were function calls
to function signatures with signaturephqargtypes parameter types and
signaturephqrettype return type.

6. The body of each implementation i would typecheck under Stan if it
were the body of a function with signaturepparpiqqargtypes parame-
ter types and signaturepparpiqqrettype return type, and if the module-
deﬁned parameters iparameters were included as model parameters.

When a program P meets these constraints, we say validsemanticspP q.
Constraint 6 deﬁnes the scope available to code within modules:

local
variables, module arguments, module-deﬁned global variables (such as pa-
rameters), and base-deﬁned global variables (such as parameters).

5.4 Program validity

We say that a modular Stan program P is valid if P meets both the struc-
tural constrains in section 5.2 and the semantic constraints in section 5.3.2:
validpP q “ validstructurepP q ^ validsemanticspP q.

23

5.5 Selections

Selections are subsets of the implementations in a modular program. We
call them “selections” because subsets of implementations are “selected” as
components to build a concrete Stan model. When a selection speciﬁes a
Stan program and has no extra implementations, the selection is valid.

Next we give formal criteria to recognize valid selections, validP pIq. We
show in section 6.1 that selections satisfying validP pIq are exactly those
needed to deﬁne concrete Stan programs. We ﬁrst need to deﬁne siblings,
an intersection-like operation on selections. siblingspI1, I2q is the set of pairs
of implementations across two sets that are diﬀerent but share a parent:

siblingspI1, I2q “ t pi1, i2q | i1 P I1, i2 P I2, i1 ‰ i2, parpi1q “ parpi2q u

We note a useful property:

I1 Ă I2 ùñ @I 1, siblingspI1, I 1q Ă siblingspI2, I 1q

The property validP pIq is true if and only if all of the following three

criteria are met:

1. The selection only includes implementations that are in the program:

I Ă implspP q

2. Every hole in the program base and the selection has an implementa-
tion in the selection, and no extra implementations are included:

implspIq “ holespPbaseq Y holespIq

3. The selection does not include any pair of implementations that im-
plement the same hole, as these would be contradictory deﬁnitions:

siblingspI, Iq “ H

The following are convenient restatements of the above properties 2 and

3:

1. Each hole found in the program base and the selection has exactly one

implementation in the selection:

@h P holespPbaseq Y holespIq, |t i | i P I, parpiq “ h u| “ 1

2. @h P parspIq, h P holespIq or h P holespPbaseq.

24

5.6 Semantic domain

We are now equipped to formally deﬁne our high-level operations 1 to 3 and
the semantic domain.

ConcretizepP, Iq is the concrete Stan program that results from includ-
ing each implementation from the set valid selection I into the base of the
valid modular program P . We consider an implementation of Concretize
correct if for all modular programs P and selections I such that validpP q
and validP pIq:

1. validSTANpConcretizepP, Iqq

2. ConcretizepP, Iq includes the same set of Stan statements in Pbase and

I.

ModelGraphpP q is the graph of all valid selections of P , connected if they

disagree on implementation of one hole.

N pModelGraphpP qq “ t I | I Ă implspP q, validP pIq u
EpModelGraphpP qq “ t pI1, I2q | I1, I2 P N pModelGraphpP qq, |siblingspI1, I2q| “ 1 u

ModelNeighborspP, Iq is the set of all selections that share an edge with I
in the model graph: ModelNeighborspP, Iq “ t I 1 | pI, I 1q P EpModelGraphpP qq u
We can give a formal semantics of a valid modular program P in these
terms:
has the same graph structure as ModelGraph(P), but with the
selection-valued nodes replaced by the corresponding probabilistic models:

P
(cid:74)

(cid:75)

P
N p
(cid:74)

q “ t
(cid:75)

Concretize(P, I)
(cid:74)

(cid:75)
In this way, a valid modular program P represents the graph of all prob-
abilistic models that can be produced by recombination of modules, with
connections between models that diﬀer by only one choice of module.

| I P N pModelGraphpP qq u

6 Algorithms

6.1 Concretize

In this section we develop a function ConcretizepP, Iq: the concrete Stan
program that is derived from a modular Stan program P by applying the
implementations of a valid selection set I.

We build Concretize by careful use of function inlining as a subroutine.

We use the following simpliﬁed interface:

25

InlineFunction(Program, CallSite, Stmts, Params)
InlineFunction(Program, CallSite, Stmts, Params, Return)

Here, Program is the whole program to be updated, CallSite is a data
structure indicating the span of code to be replaced, Stmts are a list of state-
ments that make up the body of the function, Params is a list of the function’s
parameters, and Return, when present, is return expression (modules can
only have zero or one return statements). When we use InlineFunction,
we take Program to be a modular Stan program, CallSite to be a HoleSite,
Stmts to be STMT_FUNC_M+, Params to be STMT_DECL+, and Return to be
EXPR_M.

InlineFunction is an operation standard in most optimizing compilers,

including the Stan compiler [29].

InlineFunction assumes the following preconditions:

1. Program typechecks in the scope of CallSite.

2. CallSite supplies arguments matching Params.

3. The function represented by Stmts, Params and Return typechecks.

Then we assume the following properties of Program’ = InlineFunction(Program,

CallSite, Stmts, Params, Return):

1. References to Params in Stmts and Return are replaced by argument

expression of CallSite.

2. Stmts are inserted in order before CallSite in the same scope as

CallSite.

3. When Return is provided, CallSite is replaced by Return, which does

not change the expression type.

4. Program’ typechecks wherever Program typechecks.

6.1.1

ApplyImpl

We deﬁne an operation ApplyImplpP, iq, the result of taking a valid modular
Stan program P and “applying” an implementation i P implspP q, or using
i to “ﬁll” all instances of the hole parpiq. The hole that i ﬁlls, parpiq, no
longer appears in the resulting program.

26

ApplyImpl(P, i):

sites = \st*{ site}{site \in HoleSites(p), site_{hole} = par(impl) }
P’ := P
for site in sites:

P’ := InlineFunction(P’, site, i_{body}, signature(st_{hole})_{arg-types}, i_{return})

return InlineFunction(P’, site(\texttt{parameters}), {}, i_{parameters})

ApplyImpl does not update module signatures.

6.1.2 Correctness of ApplyImpl

Lemma 6.1 (ApplyImplpP, iq replaces the hole ﬁlled by i with i’s holes). For
all i1 P implspP q Y tPbaseu, holesApplyImplpP,iqpi1q “ holesP pi1q Y holesP piq ´
tparpiqu if parpiq P holesP pi1q and holesP pi1q otherwise.

Proof. If parpiq R holesP pi1q, ApplyImpl does not make any replacements in
i1, so holesApplyImplpP,iqpi1q “ holesP pi1q.

Otherwise, ApplyImplpP, iq is made up of code from i and i1, so:

holesApplyImplpP,iqpi1q Ă holesP piq Y holesP pi1q

“ pholesP piq X tparpiquq Y pholesP piq ´ tparpiquq

Y pholesP pi1q X tparpiquq Y pholesP pi1q ´ tparpiquq

holesP piq X tparpiqu “ H, because otherwise i Ñ i P EpM DGpP qq,

which would violate AcyclicpP q and validpP q.

ApplyImpl replaces all h P holesP pi1q X tparpiqu with ibody, so h R

holesApplyImplpP,iqpi1q.

Therefore:

holesApplyImplpP,iqpi1q Ă pholesP piq ´ tparpiquq Y pholesP pi1q ´ tparpiquq

ApplyImpl does not remove any holesP pi1q ´ tparpiqu, so holesP pi1q ´

ApplyImpl inserts holesP piq ´ tparpiqu as part of ibody, so holesP piq ´

tparpiqu Ă holesApplyImplpP,iqpi1q.

tparpiqu Ă holesApplyImplpP,iqpi1q.

Therefore:

holesApplyImplpP,iqpi1q “pholesP piq ´ tparpiquq Y pholesP pi1q ´ tparpiquq

“holesP piq Y holesP pi1q ´ tparpiqu

27

Lemma 6.2 (ApplyImplpP, iq replaces the hole ﬁlled by i with i’s holes).
For all sites st P sitespP q such that sthole “ parpiq, the code ibody re-
placing st in ApplyImplpP, iq has ef f ectspibodyq Ă signaturepparpiqqef f ects,
scopepibodyq Ă signaturepparpiqqscope, and for all holes h P holespiq, signaturephqef f ects
Ă signaturepparpiqqef f ects and signaturephqscope Ă signaturepparpiqqscope.

Proof. By deﬁnition of signature:

signaturepparpiqqef f ects Ă ef f ectspibodyq Y

ď

h1Pholespiq

signatureph1qef f ects

and

signaturepparpiqqscope Ă scopepibodyq Y

ď

h1Pholespiq

signatureph1qscope

Lemma 6.3 (Preconditions of
InlineFunction are met). When validpP q,
the preconditions of the InlineFunction calls in ApplyImpl(P, i) are met.

Proof. Consider the ﬁrst call:

InlineF unctionpP, site, ibody, signaturepstholeqarg´types, ireturnq

1. If site is in the base of P , then the P typechecks in the scope of site
by semantic constraint 5, otherwise it must be in an implementation
body, in which case it typechecks by semantic constraint 6.

2. We know site looks like a function call because of the deﬁnition of
HoleSites. Its arguments must match signaturepsiteholeqarg´types by
semantic constraint 5.

3. Implied by semantic constraint 6.

On subsequent calls, P 1 is the output of a previous InlineFunction,
which by induction meets its preconditions. P 1 typechecks by the same
argument as P because of property 4. Both of the other preconditions hold
by the same reasoning as the initial call.

Consider the ﬁnal call:

InlineF unctionpP 1, sitepparametersq, tu, iparametersq

28

1. P 1 typechecks at sitepparametersq because P does by semantic con-

straint 5 and P 1 typechecks wherever P does.

2. sitepparametersq is a location rather than a hole site and so does not

supply any arguments, which matches {}.

3. Params and Return are empty, and Stmts can only be a list of STMT_DECL

and so typechecks.

Theorem 6.1 (ApplyImpl produces valid modular Stan programs). validpP q, i P
P ùñ validpApplyImplpP, iqq

Proof. Structural constraints:

1. The dependency graph of modules is acyclic.

Suppose there is a cycle C in M DGpApplyImplpP, iqqq.
Let i0 Ñ i1 be an edge in C; then parpi1q P holesApplyImplpP,iqpi0q.
By Lemma 6.1, either parpi1q P holesP pi0q, in which case i0 Ñ i1 P
EpM DGpP qq; or parpiq P holespi0q and parpi1q P holesP pi0q, in which
case i0 Ñ i, i Ñ i1 P EpM DGpP qq.
In either case, for all edges i0rightarrowi1 P C, there exists a path
from i0 to i1 in M DGpP q, so there exists a cycle in M DGpP q. As that
would contradict validpP q, there is no cycle C in ApplyImplpP, iq.

2. Every hole has an implementation.

ApplyImpl does not remove any implementations from or add any new
holes to P , so any holes without implementations would also be without
implementations in P , which would violate validpP q.

3. Hole identiﬁers are unique and (hole identiﬁer, implementation iden-

tiﬁer) pairs are unique.
ApplyImpl does not change any hole implementation identiﬁer names,
so any collisions would exist in P and violate validpP q.

Semantic constraints:

1. Implementations match signature argtypes.

ApplyImpl does not change signatures or implementation argument
types.

29

2. Implementations match signature rettype. @i1 P implspP q, i1

rettype “

signaturepparpi1qqrettype.
If ireturn is a hole expression of parpiq, then it is replaced by ApplyImplpP, iq.
By Lemma 6.3, property 3 of InlineFunction for the replacement, so
the type of ireturn is not changed.
If the type of ireturn depends on a hole expression of parpiq, by the
same reasoning, the type does not change. Otherwise, it is unaﬀected
by ApplyImpl.

3. Eﬀects and scopes of holes are available in their block or module signa-

ture.
Suppose Dst P sitespApplyImplpP, iqq so that st violates one of the
conditions.

If st P sitespP q, since block and signature are unaﬀected by ApplyImpl,
then st would violate validpP q. Therefore, st R sitespP q and must have
been part of a replacement of parpiq by ibody.
By Lemma 6.2, we have signaturepstholeqef f ects Ă signaturepparpiqqef f ects
and signaturepstholeqscope Ă signaturepparpiqqscope. Since the scope
and eﬀects of sthole are subsets of the scope and eﬀects of parpiq, and
parpiq passes the four conditions, sthole must also pass the four condi-
tions.

4. Eﬀects and scopes of code are available in their block or module signa-

ture.
By identical reasoning to the previous case, for any code in a block
or module implementation, the code either existed in P , in which case
it must be valid, or it replaced parpiq, in which case by Lemma 6.2 it
requires only a subset of the scope and eﬀects of parpiq, and is therefore
also valid.

5. Pbase would typecheck under Stan if all holes h were function calls
to function signatures with signaturephqargtypes parameter types and
signaturephqrettype return type.
By Lemma 6.3, property 4 of InlineFunction applies for all updates
to P in ApplyImpl. Since Pbase typechecks, ApplyImplpP, iqbase also
typechecks.

6. The body of each implementation i would typecheck under Stan if it
were the body of a function signaturepparpiqqargtypes parameter types

30

and signaturepparpiqqrettype return type, and if the module-deﬁned pa-
rameters iparameters were included as model parameters.
By Lemma 6.3, property 4 of InlineFunction applies for all updates
to ibody for any i P implspP q in ApplyImpl. Since ibody typechecks in
P for all i P implspP q, ibody also typechecks in ApplyImplpP, iq.

6.1.3

ApplyImpls

Let ApplyImplspP, Iq be the result of applying ApplyImplpP, iq for each i P I.
Formally, if we let I “ă i1, . . . iN ą, P0 “ P , and Pj “ ApplyImplpPj´1, ijq
for j P r1, N s, then ApplyImplspP, Iq “ PN .

6.1.4 Correctness of ApplyImpls

Theorem 6.2 (ApplyImplspP, Iq adds I’s holes and removes the holes ﬁlled
by I). For all i1 P implspP q Y tPbaseu, holesApplyImplspP,Iqpi1q Ă holesP pi1q Y
holesP pIq ´ parspIq

Proof. Let Ij “ă i1, . . . ij ą so that IN “ I. Note that ApplyImplspP, Ijq “
Pj.

We prove the following by induction on j:

holesPj pi1q Ă holesP pi1q Y holesP pIjq ´ parspIjq

For j “ 0: holesP0pi1q Ă holesP pi1q Y holesP0pIq ´ parspIq “ holesP pi1q Y

H ´ H “ holesP0pi1q.

For j ą 0: By Lemma 6.1, holesPj pi1q “ holesApplyImplpPj´1,ij qpi1q Ă

holesPj´1pi1q Y holesPj´1pijq ´ parpijq.

By induction:
holesPj pi1q Ă pholesP pi1q Y holesP pIj´1q ´ parspIj´1qq Y holesPj´1pijq ´

parpijq.

parspIj´1q:

Again by induction, we can use holesPj´1pijq Ă holesP pijqYholesP pIj´1q´

holesPj pi1q Ă pholesP pi1q Y holesP pIj´1q ´ parspIj´1qq Y pholesP pijq Y

holesP pIj´1q ´ parspIj´1qq ´ parpijq.

holesPj pi1q Ă holesP pi1q Y pholesP pIj´1q Y holesP pijq Y holesP pIj´1qq ´

pparspIj´1q Y parpijq.

holesPj pi1q Ă holesP pi1q Y holesP pIjq ´ parpIjq.

31

6.1.5

Concretize

We deﬁne Concretize as ConcretizepP, Iq “ ApplyImplspP, Iqbase when I
is a valid selection.

6.1.6 Correctness of Concretize

Lemma 6.4 (ConcretizepP, Iq has no holes). For validpP q and validP pIq,
holespConcretizepP, Iqq “ H.

Proof. By theorem 6.2, holespApplyImplspP, Iqbaseq Ă holespPbaseqYholespIq´
parspIq. By validP pIq, holespPbaseq Y holespIq “ parspIq. Therefore,
holespApplyImplspP, Iqbaseq Ă H.

Theorem 6.3 (ConcretizepP, Iq is a valid Stan program). validpP q and
validP pIq implies validStanpConcretizepP, Iqq.

Proof. By theorem 6.1 we have validpApplyImplspP, Iqq.

By Lemma 6.4, we have holespConcretizepP, Iqq “ H. Therefore, SP “

ConcretizepP, Iq is a concrete Stan program.

We can match the criteria of validStanpSP q in section 3.3:

1. Eﬀects and scopes of code are available in their block.

Implied by

validpApplyImplspP, Iqq, semantic constraint 4.

2. SP typechecks as Stan code. Implied by validpApplyImplspP, Iqq, se-

mantic constraint 5.

6.1.7 User speciﬁcation of a selection

In order for a user to call Concretize, they need to provide a “selection”.

There is one more piece of syntax to deﬁne, though it does not appear in
programs themselves: a selection string. A selection string is a string that
references a set of module implementations within a given program.

SELECTION: hole_identifier:impl_identifier,+

Selection strings can be used to narrow down and index into the set of

programs represented by a modular program.

32

6.2 ModelGraph

In this section we present an eﬃcient algorithm to implement the ModelGraph
operation.

6.2.1 Naive versions

To demonstrate what we mean by eﬃcient, consider a naive construction of
the model graph given a modular program P :

N pNaiveModelGraphpP qq “

$
&
% closepPbase, Iq

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

ą

I P

implsphq, validP pclosepPbase, Iqq

hPholesppq

,
.

-

EpNaiveModelGraphpP qq “

!

ˇ
ˇ
ˇ I1, I2 P N pNaiveModelGraphpP qq, |siblingspI1, I2q| “ 1

)

pI1, I2q

Here, closepi, Iq is the subset of I that is reachable from i by travers-
ing edges in the module graph through I. Thus closepPbase, Iq eﬀectively
removes redundant implementations from I. NaiveModelGraph works by
enumerating all possible combinations of implementations and model pairs
and then ﬁltering out the invalid options.

EpNaiveModelGraphpP qq is exponentially ineﬃcient when P is ﬁg. 2a,
because it would consider 2N candidate neighbors for each node, where in
reality each node has only N ´ 1 neighbors.

N pNaiveModelGraphpP qq is exponentially ineﬃcient when P is ﬁg. 2b,
because it would consider 2N candidate nodes, where in reality there are
only N nodes.

Another approach would be a recursive construction. If the module graph
is a tree, then the network of the sub-graph descending from each hole node
is the graph join of the networks of the sub-graphs descending from its im-
plementations, and the network of the sub-graph descending from each im-
plementation node is the graph Cartesian product of the networks of the
sub-graphs descending from its holes. The issue with this approach is how
to handle situations like ﬁgs. 2c and 2d: the module graph is not always
a tree, so hole and implementation nodes will sometimes combine networks
with overlapping information. It is possible to modify the graph join and
Cartesian product operations to handle the overlap correctly, but not without

33

(a)

(b)

(c)

(d)

Figure 2: Pathological module graphs. Round nodes represent the base and
module implementations while rectangular nodes represent holes.

34

adding complexity and ineﬃciency that scales with the number recombina-
tion nodes.

6.2.2 Algorithm

Our algorithm builds up a the model graph’s node and edge sets simulta-
neously by “visiting” each of the program’s holes in turn. The algorithm
tracks the set of model and edge “preﬁxes”: partial selections that only con-
tain implementations for the holes visited thus far. It also tracks the set of
holes required so far by each preﬁx. When the algorithm “visits” a hole, it
“expands” the set of preﬁxes that require that hole to reﬂect its set of imple-
mentations. To ensure that no “preﬁx” discovers that it requires a hole after
that hole has been visited, and to avoid other complex edge cases, holes are
visited in a topological order of dependency.

expandpN, E, hq “
´ ď

expand-nodepH, I, hq,

pH,IqPN
ď

pH,IqPN

new-edgespH, I, hq Y

expand-edgepH1, I1, H2, I2, hq

ď

¯

pH1,I1,H2,I2qPE

expand-nodepH, I, hq “

#

t pH Y holespiq, I Y tiuq | i P implsphq u
tpH, Iqu

if h P H
otherwise

$
’&

’%

H

$
’’’&
’’’%

new-edgespH, I, hq “
"

pH Y holespi1q, I Y ti1u, H Y holespi2q, I Y ti2uq

ˇ
ˇ
ˇ
ˇ pi1, i2q P

ˆ

implsphq
2

˙ *

if h P H

otherwise

expand-edgepH1, I1, H2, I2, hq “

tpH1, I1, H2, I2qu
t pH1 Y holespiq, I1 Y tiu, H2, I2q | i P implsphq u
t pH1, I1, H2 Y holespiq, I2 Y tiuq | i P implsphq u
t pH1 Y holespiq, I1 Y tiu, H2 Y holespiq, I2 Y tiuq | i P implsphq u

if h R H1, h R H2
if h P H1, h R H2
if h R H1, h P H2
if h P H1, h P H2

35

Let GDeppP q be the graph of dependencies between holes of a program
P so that N pGDeppP qq “ holespP q and h1 Ñ h2 P EpGDeppP qq if and only
if Di P implsph1q s.t. h2 P holespiq. Let (cid:126)H be a topological ordering of GpP q.

Let pN0, E0q “ ppholespPbaseq, Hq, Hq.
Let pNj, Ejq “ expandpNj´1, Ej´1, hjq, @j P r1, | (cid:126)H|s.
We show that t I | pH, Iq P N| (cid:126)H| u “ N pM odelGraphpP qq and t pI1, I2q | pH1, I1, H2, I2q P

E| (cid:126)H| u “ EpM odelGraphpP qq.

6.2.3 Proofs of correctness and completeness

Throughout this section, we assume that P is a modular program such that
validstructurepP q.

Lemma 6.5 ( (cid:126)H is ordered by dependency). There exists a topological or-
dering (cid:126)H “ topo pGDeppP qq and @j P r1, | (cid:126)H|s, @i P implspP q s.t. (cid:126)Hj P
holespiq, parpiq P (cid:126)H1:j´1 or parpiq P holespPbaseq.

Proof. The graph GDep must be acyclic, because otherwise the holes of P
would have cyclic dependencies and P would not be valid. Therefore GDep
has at least one topological order (cid:126)H.

For any such j and i, we must have parpiq Ñ (cid:126)Hj P EpGDeppP qq, by
deﬁnition of GDep. Then either parpiq P holespPbaseq and we are done, or
parpiq P holespP q, and so parpiq P (cid:126)H, so (cid:126)Hk for some k. By the deﬁnition
of a topological ordering, k ă j. Therefore, parpiq P (cid:126)H1:j´1.

Lemma 6.6 (Model preﬁxes have complete preﬁx hole sets). @j P r1, | (cid:126)H|s, @pHj´1, Ij´1q P
Nj´1, let N I “ t I 1 | I 1 P N pM odelGraphpP qq, Ij´1 Ă I 1 u.
For all I 1 in N I , parspI 1q X (cid:126)H1:j “ Hj´1 X (cid:126)H1:j.

Proof. This is a proof by induction on j.

We will simultaneously prove the following property, which we refer to

as “consistency,” by induction for each j:

I 1 X implsp (cid:126)H1:j´1q “ Ij´1 X implsp (cid:126)H1:j´1q

For j “ 1:
Since N0 “ tHu, Ij´1 “ H, so N I “ N pM odelGraphpP qq.
If (cid:126)H1 P parspI 1q, then by validP pI 1q, (cid:126)H1 P holespPbaseq, since if Di P
implspI 1q s.t. (cid:126)H1 P holespiq, then by Lemma 6.5, parpiq P (cid:126)H1:0 “ H. Then
since holespPbaseq Ă parspI 1q and H0 “ holespPbaseq, parspI 1q X (cid:126)H1:1 “
H0 X (cid:126)H1:1.

36

If (cid:126)H1 R parspI 1q, then (cid:126)H1 R holespPbaseq “ H0, so parspI 1q X (cid:126)H1:1 “

H0 X (cid:126)H1:1 “ H. Therefore the lemma holds for j “ 1.

Trivially, consistency holds for j “ 1 because Eh P (cid:126)H1:0.
For j ą 1:
Since pHj´1, Ij´1q P Nj´1 and j ě 2, DpHj´2, Ij´2q P Nj´2 such that

pHj´1, Ij´1q P expand-nodepHj´2, Ij´2, (cid:126)Hj´1q.

If (cid:126)Hj´1 P Hj´2, then by induction of the lemma, (cid:126)Hj´1 P parspI 1q, so
I 1 X implsp (cid:126)Hj´1q “ ti1u for some i1. It also follows from the deﬁnition of
expand-node that, for some i P implsp (cid:126)Hj´1q, Ij´1 “ Ij´2 Y tiu, and since
Ij´2 X implsp (cid:126)Hj´1q “ H, Ij´1 X implsp (cid:126)Hj´1q “ tiu. Since Ij´1 Ă I 1, i P I 1.

If i{neqi1, then since parpiq “ parpi1q, pi, i1q P siblingspI 1, I 1q, but validP pI 1q

implies siblingspI 1, I 1q “ H, so i “ i1.

Thus, if (cid:126)Hj´1 P Hj´2, then I 1Ximplsp (cid:126)Hj´1q “ Ij´1Ximplsp (cid:126)Hj´1q “ tiu.
If (cid:126)Hj´1 R Hj´2, then (cid:126)Hj´1 R parspI 1q and Ij´1 “ Ij´2, so I 1Ximplsp (cid:126)Hj´1q “
Ij´1 X implsp (cid:126)Hj´1q “ H.

Starting from induction on consistency:

I 1 X implsp (cid:126)H1:j´2q “ Ij´2 X implsp (cid:126)H1:j´2q

pI 1Ximplsp (cid:126)H1:j´2qqYpI 1Ximplsp (cid:126)Hj´1qq “ pIj´2Ximplsp (cid:126)H1:j´2qqYpIj´1Yimplsp (cid:126)Hj´1qq
Because Ij´2 “ Ij´1 X implsp (cid:126)H1:j´2q:

pI 1Ximplsp (cid:126)H1:j´2qqYpI 1Ximplsp (cid:126)Hj´1qq “ pIj´1Ximplsp (cid:126)H1:j´2qqYpIj´1Yimplsp (cid:126)Hj´1qq

I 1 X implsp (cid:126)H1:j´1q “ Ij´1 X implsp (cid:126)H1:j´1q

Thus, consistency holds.
If (cid:126)Hj P holespPbaseq, then the lemma is true by the same reasoning as

the j “ 1 case.

Suppose (cid:126)Hj P Hj´1 and (cid:126)Hj R holespPbaseq. By the construction of
Hj´1 and the deﬁnition of expand-node, Di P Ij´1 such that (cid:126)Hj P holespiq.
Ij´1 Ă I 1, i P I 1, so (cid:126)Hj “ parpiq P parspI 1q.

Suppose (cid:126)Hj P parspI 1q and (cid:126)Hj R holespPbaseq. By validP pI 1q and Lemma 6.5,

there exists some k ă j such that parpiq “ (cid:126)Hk and i P implsp (cid:126)Hkq. By con-
sistency, I 1 X implsp (cid:126)H1:kq “ Ik X implsp (cid:126)H1:kq, so i P Ik Ă Ij´1. By the
deﬁnition of expand-nodepHk´1, Ik´1, (cid:126)Hkq, i P Ik ùñ holespiq Ă Hk, so
(cid:126)Hj P Hk Ă Hj´1.

The following lemma shows that a model preﬁx can be expanded with

any implementation on its horizon and remain a valid model preﬁx.

37

Lemma 6.7 (Model preﬁxes can be expanded with any implementation).
@j P r1, | (cid:126)H|s, @I P N pM odelGraphpP qq, @h P holespIXimplsp (cid:126)H1:jqYholespPbaseqq´
(cid:126)H1:j, @i P implsphq, DI 1 P N pM odelGraphpP qq s.t. I Ximplsp (cid:126)H1:jqYtiu Ă I 1.
Proof. We will construct a selection set that satisﬁes the property by modi-
fying I.

Let I1 “ I ´ implsphq Y tiu. Let I2 “ I1 Y t anypimplsphqq | h P
holespP q ´ parspI2q u, where anypsq is an arbitrary element of a set. The
sets implsphq are never empty by validpP q. Then, parspI2q “ holespP q and
I X implsp (cid:126)H1:jq Y tiu Ă I2.

Let f pIq “ t i | i P I, parpiq P holespIq Y holespPbaseq u. Let I3 be the
ﬁxed point applying f to I2. We know f converges to some I3 because
|I| is ﬁnite, cannot go below 0, and decreases monotonically under f until
convergence.

I3 meets the criteria for validP pI3q:

1. I3 Ă implspP q: I3 Ă I2 Ă implspholespP qq Y I Y tiu Ă implspP q by

validP pIq.

2. By I3 “ f pI3q we know parspI3q Ă holespI3q Y holespPbaseq. Since
parspI2q “ holespP q Ą holespI3q Y holespPbaseq and f does not remove
any h P holespI3q Y holespPbaseq, we have parspI3q “ holespI3q Y
holespPbaseq.

3. siblingspI3, I3q “ H, because only one i P implsphq for each h R

parspIq is added to I in the construction of I1 and I2.

By construction, I X implsp (cid:126)H1:jq Y tiu Ă I3.

Lemma 6.8 (Nj represents exactly the model preﬁxes). @j P r0, | (cid:126)H|s, t I | pH, Iq P
Nj u “ t I X implsp (cid:126)H1:jq | I P N pM odelGraphpP qq u.
Proof. This is a proof by induction on j.

For j “ 0, t I | pH, Iq P N0 u “ tHu “ t IXH | I P N pM odelGraphpP qq u.
For j ě 1:
Suppose DI P t I X implsp (cid:126)H1:jq | I P N pM odelGraphpP qq u such that
I R t I | pH, Iq P Nj u. Let I 1 “ I X implsp (cid:126)H1:j´1q. By induction, I 1 P
t I | pH, Iq P Nj´1 u, so DH 1 s.t. pH 1, I 1q P Nj´1.

If (cid:126)Hj P H 1, then by Lemma 6.6, (cid:126)Hj P parspIq, so Di P I s.t. i P implsp (cid:126)Hjq.
By the deﬁnition of expand-nodepH 1, I 1, (cid:126)Hjq, @i1 P implsp (cid:126)Hjq, I 1 Y ti1u P
t I | pH, Iq P Nj u, so I 1 Y tiu “ I P t I | pH, Iq P Nj u, which contradicts the
construction of I.

38

If (cid:126)Hj R H 1, then by Lemma 6.6, (cid:126)Hj R parspIq, so I 1 “ I. By the deﬁnition
of expand-nodepH 1, I 1, (cid:126)Hjq, pH 1, I 1q P Nj, so I P t I | pH, Iq P Nj u, which
again contradicts the construction of I.

Therefore, there is no such I, and t IXimplsp (cid:126)H1:jq | I P N pM odelGraphpP qq u Ă

t I | pH, Iq P Nj u.

Suppose DI P t I | pH, Iq P Nj u such that I R t I X implsp (cid:126)H1:jq | I P
N pM odelGraphpP qq u. By the deﬁnition of Nj, DpH 1, I 1q P Nj´1 s.t. pH, Iq P
expand-nodepH 1, I 1, (cid:126)Hjq. By induction, I 1 P t I X implsp (cid:126)H1:j´1q | I P
N pM odelGraphpP qq u, and therefore DI ˚ P N pM odelGraphpP qq s.t. I 1 Ă I ˚.
If (cid:126)Hj R H 1, then by the deﬁnition of expand-node, I “ I 1, and by
Lemma 6.6, (cid:126)Hj R parspI ˚q; so I “ I 1 “ I ˚ X implsp (cid:126)H1:j´1q “ I ˚ X
implsp (cid:126)H1:jq, so I P t I X implsp (cid:126)H1:jq | I P N pM odelGraphpP qq u.

If (cid:126)Hj P H 1, then by the deﬁnition of expand-node, Di P implsp (cid:126)Hjq s.t. I “

I 1 Y tiu, and by Lemma 6.6, (cid:126)Hj P parspI ˚q.

Because (cid:126)Hj P holespI ˚ X implsp (cid:126)H1:j´1qq ´ (cid:126)H1:j´1, by Lemma 6.7, DI i P
N pM odelGraphpP qq s.t. I X implspH1:j´1q Y tiu Ă I i, so I 1 Y tiu “ I Ă I i,
so I P t I X implsp (cid:126)H1:jq | I P N pM odelGraphpP qq u.

Therefore, t I | pH, Iq P Nj u Ă t IXimplsp (cid:126)H1:jq | I P N pM odelGraphpP qq u.

is correct and complete). t I | pH, Iq P N| (cid:126)H| u “

Theorem 6.4 (N| (cid:126)H|
N pM odelGraphpP qq.
Proof. By Lemma 6.8, t I | pH, Iq P N| (cid:126)H| u “ t I X implsp (cid:126)H1:| (cid:126)H|q | I P
N pM odelGraphpP qq u, and because (cid:126)H1:| (cid:126)H| “ parspP q, t IXimplsp (cid:126)H1:| (cid:126)H|q | I P
N pM odelGraphpP qq u “ N pM odelGraphpP qq.

Lemma 6.9 (Edge preﬁx endpoints are model preﬁxes). @j P r0, | (cid:126)H|s, @pH1,
I1, H2, I2q P Ej, pH1, I1q P Nj and pH2, I2q P Nj.
Proof. This is a proof by induction on j.

For j “ 0, the lemma is trivially satisﬁed because E0 “ H.
For j ě 1:
By deﬁnition of expand, pH1, I1, H2, I2q came either from new-edges

expand-edge.

If pH1, I1, H2, I2q P new-edgespH 1, I 1, (cid:126)Hjq for some pH 1, I 1q P Nj´1, then
(cid:126)Hj P H 1 and pH1, I1q “ pH 1 Y holespi1q, I 1 Y ti1uq, pH2, I2q “ pH 1 Y
holespi2q, I 1 Y ti2uq, so pH1, I1q, pH2, I2q P expand-nodepH 1, I 1, (cid:126)Hjq Ă Nj.
1, H 1

If pH1, I1, H2, I2q P expand-edgepH 1
1q, pH 1

Ej´1, then by induction pH 1

2, (cid:126)Hjq for some pH 1
If (cid:126)Hj P H 1
1

1, H 1
2q P Nj´1.

2, I 1
, then Di P

1, I 1
2, I 1

1, I 1

2, I 1

1, I 1

2q P

39

1q s.t. pH1, I1q “ pH 1

implspH 1
Nj. If (cid:126)Hj R H 1
1, I 1
1
The same reasoning applies for pH2, I2q.

, then pH1, I1q “ pH 1

1Yholespi1q, I 1

1Yti1uq P expand-nodepH 1

1q P expand-nodepH 1

1, I 1

1, (cid:126)Hjq Ă

1, I 1
1, (cid:126)Hjq Ă Nj.

Lemma 6.10 (Edge preﬁx endpoints share one pair of siblings). @j P
r0, | (cid:126)H|s, @pH1, I1, H2, I2q P Ej, |siblingspI1, I2q| “ 1.

Proof. This is a proof by induction on j.

For j “ 0, the lemma is trivially satisﬁed because E0 “ H.
For j ě 1:
By deﬁnition of expand, pH1, I1, H2, I2q came either from new-edges

expand-edge.

˘
`
implsp (cid:126)Hj q
2

If pH1, I1, H2, I2q P new-edgespH 1, I 1, (cid:126)Hjq for some pH 1, I 1q P Nj´1, then
such that I1 “ I 1 Y ti1u, I2 “ I 1 Y ti2u, parpi1q “
Dpi1, i2q P
parpi2q “ (cid:126)Hj, so pi1, i2q P siblingspI1, I2q. Since I1 and I2 are otherwise
identical, siblingspI1, I2q can have no other elements.

2, (cid:126)Hjq for some pH 1
If pH1, I1, H2, I2q P expand-edgepH 1
1, I 1
2, I 1
1, H 1
2q “ tpi1, i2qu for some i1, i2. Since
1, I 1
2 Y tiuq for some i P implsp (cid:126)Hjq, I1 and

Ej´1, then by induction siblingspI 1
pI1, I2q is either pI 1
1 Y tiu, I 1
I2 cannot have gained or lost any siblings, so siblingspI1, I2q “ tpi1, i2qu.

2q or pI 1

1, H 1

1, I 1

1, I 1

2, I 1

2q P

If

1, I 1

1 ´ I 1
2

Lemma 6.11 (Model preﬁxes without siblings are equal). @I1, I2 P N pM odelGraphpP qq,
@j P r1, | (cid:126)H|s, let I 1
siblingspI1, I2q “ H, then I 1

2 “ I2 X implsp (cid:126)H1:jq.

2q “ H, (cid:126)Hk R parspI 1

2q, so (cid:126)Hk R holespPbaseq, so Di1 P I 1

1 “ I1 X implsp (cid:126)H1:jq and I 1
1 “ I 1
2.
so that parpiq “ (cid:126)Hk with minimal k.
Proof. Let i be the element of I 1
2q. Since validP pI1q and
and siblingspI 1
Since i R I 1
2
1q, (cid:126)Hk P holespPbaseqYholespI 1q. Since validP pI2q, holespPbaseq Ă
(cid:126)Hk P parspI 1
1 s.t. (cid:126)Hj P holespi1q By the topo-
parspI 1
logical sorting of (cid:126)H, parpi1q “ (cid:126)Hm for some m ă k. Since k is minimal in
2q,
1 ´ I 1
I 1
2
but (cid:126)Hk R parspI 1
is empty. The same reasoning applies
to show I 1

1 ´ I 1
2
.
1 “ I 1
2
Lemma 6.12 (Ej is a complete set of edge preﬁxes). @j P r1, | (cid:126)H|s, t
implsp (cid:126)H1:jq, I2Ximplsp (cid:126)H1:jq
tpi1, i2qu, parpi1q P (cid:126)H1:j u Ă t pI1, I2q | pH1, I1, H2, I2q P Ej u.

, so i1 P I 1
2
2q. Therefore I 1
is empty, so I 1

. Then (cid:126)Hk P holespi1q Ă holespI 1

| I1, I2 P N pM odelGraphpP qq, siblingspI1, I2q “

2q Ă parspI 1

`

I1 X

, i1 R I 1

1 ´ I 1
2

2 ´ I 1
1

˘

Proof. This is a proof by induction on j.

`

˘

For j “ 0, t

I1XH, I2XH

| I1, I2 P N pM odelGraphpP qq, siblingspI1, I2q “

tpi1, i2qu, parpi1q P H u “ H Ă E0 “ H.

40

For j ě 1:
We consider some pI1, I2q P t

`

˘
I1Ximplsp (cid:126)H1:jq, I2Ximplsp (cid:126)H1:jq

| I1, I2 P
N pM odelGraphpP qq, siblingspI1, I2q “ tpi1, i2qu, parpi1q P (cid:126)H1:j u and show
that pI1, I2q P t pI1, I2q | pH1, I1, H2, I2q P Ej u. Let tpi1, i2qu “ siblingspI1, I2q.
If parpi1q “ parpi2q “ (cid:126)Hj, then since siblingspI1 ´ i1, I2 ´ i2q “ H,
by Lemma 6.11, I1 ´ i1 “ I2 ´ i2. Since I1 ´ i1 Ă implsp (cid:126)H1:j´1q, by
Lemma 6.8, DH 1 s.t. pH 1, I1 ´ i1q P Nj´1, therefore new-edgespH 1, I1 ´
i1, (cid:126)Hjq Ă Ej, and because (cid:126)Hj P parspI1q implies (cid:126)Hj P H 1 and pi1, i2q P
˘
`
implsp (cid:126)Hj q
, pH 1 Y holespi1q, I1 ´ i1 Y i1 “ I1, H 1 Y holespi2q, I1 ´ i1 Y i2 “
2
I2q P new-edgespH 1, I1 ´ i1, (cid:126)Hjq.

2, I 1

1, I 1

1, H 1
2 “ I2 X implsp (cid:126)H1:j´1qq.

If parpi1q “ parpi2q ‰ (cid:126)Hj, then by induction, pI1 X implsp (cid:126)H1:j´1q, I2 X
2q P

implsp (cid:126)H1:j´1qq P t pI1, I2q | pH1, I1, H2, I2q P Ej´1 u, so DpH 1
1 “ I1 X implsp (cid:126)H1:j´1q and I 1
Ej´1 such that I 1
and (cid:126)Hj R H 1
If (cid:126)Hj R H 1
2
1
, and by the deﬁnition of expand-edge, pH 1

, then by Lemma 6.9 and Lemma 6.6, I1 “ I 1
,
1
1, I1, H 1
1, I 1
2, I2q P

I2 “ I 1
2
expand-edgepH 1
1, H 1
1, I 1
If (cid:126)Hj P H 1
and (cid:126)Hj R H 1
1
2
, and @i1 P implsp (cid:126)Hjq, pH 1
and I2 “ I 1
2
pH 1
1 Y holespi1q, I1, H 1
larly, if (cid:126)Hj R H 1
1
2Ytiuq “ pH 1
holespiq, I 1
Ej.

, then Di P implsp (cid:126)Hjq such that I1 “ I 1
1 Y tiu
1 Y ti1u, H 1
1 Y holespi1q, I 1
2, I 1
2q “
2, (cid:126)Hjq Ă Ej. Simi-
2, I 1
1, H 1
1, I 1
2 Y
2, (cid:126)Hjq Ă
1, H 1

1, I 1
and (cid:126)Hj P H 1
, then I1 “ I 1
1, I2 “ I 1
2
2Yholespiq, I2q P expand-edgepH 1
1, I1, H 1

2 Y tiu and pH 1
1, I 1

2, I2q P expand-edgepH 1

2, (cid:126)Hjq Ă Ej.

1, H 1
2, I 1

2q “ pH 1

1, H 1

2, I 1

2, I 1

, then since parpi1q ‰ (cid:126)Hj, Di P implsp (cid:126)Hjq such
2 Y tiu, and @i1 P implsp (cid:126)Hjq, pH 1
1 Y holespiq, I 1
1 Y
2 Y holespiq, I2q P

1 Y holespiq, I1 Y tiu, H 1

If (cid:126)Hj P H 1
1

and (cid:126)Hj P H 1
2
1 Y tiu and I2 “ I 1

that I1 “ I 1
tiu, H 1
expand-edgepH 1

2 Y holespiq, I 1
1, I 1
Theorem 6.5 (E| (cid:126)H|
E| (cid:126)H| u “ EpM odelGraphpP qq.

2, I 1

2 Y tiuq “ pH 1
1, H 1

2, (cid:126)Hjq Ă Ej.

is the correct and complete edge set). t pI1, I2q | pH1, I1, H2, I2q P

Proof. @pH1, I1, H2, I2q P E| (cid:126)H|
N pM odelGraphpP qq by theorem 6.4, and also |siblingspI1, I2q| “ 1 by Lemma 6.10.
Therefore pI1, I2q P EpM odelGraphpP qq, so t pI1, I2q | pH1, I1, H2, I2q P
E| (cid:126)H| u Ă EpM odelGraphpP qq.

by Lemma 6.9, so I1, I2 P

, I1, I2 P N| (cid:126)H|

41

Because (cid:126)H1:| (cid:126)H| “ parspP q,

$
’&

’%

`

I1 X implsp (cid:126)H1:| (cid:126)H|q, I2 X implsp (cid:126)H1:| (cid:126)H|q

˘

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

I1, I2 P N pM odelGraphpP qq,
siblingspI1, I2q “ tpi1, i2qu,
parpi1q P (cid:126)H1:| (cid:126)H|

,
/.

/-

“ t pI1, I2q | I1, I2 P N pM odelGraphpP qq, |siblingspI1, I2q| “ 1 u
“ EpM odelGraphpP qq

So by Lemma 6.12, EpM odelGraphpP qq Ă

pI1, I2q

!

ˇ
ˇ
ˇ pH1, I1, H2, I2q P E| (cid:126)H|

)

6.2.4 Eﬃciency

For a modular program P , let N “ |N pM odelGraphpP qq| and E “ |EpM odelGraphpP qq|.
Index the graph nodes as t ni | i P r1 . . . N s u “ N pM odelGraphpP qq and
the edges as t pei,1, ei,2q | i P r1 . . . Es u “ EpM odelGraphpP qq. Let H “
|holespP q|.
ř

The minimum size possible of the explicit representation of M odelGraphpP q
jPr1...Es |ej,1| ` |ej,2| identiﬁers.

iPr1...N s |ni| `

is then

ř

ř

ř

iPr1...N s |ni|q.

1. Runtime eﬃciency. Consider the selection set insertion operations
I Ytiu in expand-node. Since implementations are never removed from
any selection sets, and selection sets are never removed from model
preﬁx sets, the number of such insertions must be Op
Consider the hole union operations H Yholespiq in expand-node. Since
each h P holespiq is eventually replaced by exactly one implementation,
the amortized cost is again Op
The same arguments apply to the edge operations in new-edges and
expand-edge: the number of implementation set insertions must be
jPr1...Es |ej,1| ` |ej,2|q, and the amortized cost of each hole union
Op
operation is also Op
The set inclusion checks h P H result in a small constant overhead
for expand-node, new-edges and expand-edge. Since expand is called
H times with at most N nodes and E edges, the number of inclusion
checks is OpH ˚ N ` H ˚ Eq.
The resulting runtime complexity is then OpH ˚ N ` H ˚ N q.

jPr1...Es |ej,1| ` |ej,2|q.

iPr1...N s |ni|q.

ř

ř

42

2. Space eﬃciency. Because the nodes Nj and edges Ej are the entire

program state, |Nj| ď |N pM odelGraphpP qq| and |Ej| ď |EpM odelGraphpP qq|,
and the elements of Nj and Ej are less than or equal to the ele-
ments of N pM odelGraphpP qq and EpM odelGraphpP qq, the program
state never exceeds the size of the output, so space complexity is
Θp

jPr1...Es |ej,1| ` |ej,2|q.

iPr1...N s |ni| `

ř

ř

3. Notes on eﬃciency. Since N (cid:126)H

does not depend on any Ej, when
we only calculate the graph nodes we have a runtime complexity of
OpH ˚ N q and space complexity of Op
This method can eﬃciently handle the extreme cases in ﬁg. 2. We have
no no asymptotic time or space overhead for the example ﬁg. 2b, and
for ﬁg. 2a, we have no space overhead but an asymptotic time overhead
of OpH ˚ Hq set inclusion checks.

iPr1...N s |ni|q.

ř

6.3 ModelNeighbors

Let N eipI, P q be a shorthand for our previously deﬁned ModelNeighbors:,
N eipI, P q “ ModelNeighborspP, Iq “ t I 1 | pI, I 1q P EpModelGraphpP qq u.
Let LimitpI, P q be equal to a modular program P , except that, for all holes
h P parspP q:

implsLimitpI,P qphq “

#

implsP phq X I
implsP phq

if h P parspIq
otherwise

We can compute N ei eﬃciently with Limit and our previously deﬁned
M odelGraph algorithm:

ď

ď

N eipI, P q “

iPI

i1Pimplspparpiqq´tiu

N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq

6.3.1 Proofs of correctness and completeness

Lemma 6.13 (Limit(I, P) is a valid modular program). validstructurepP q,
validP pIq implies validstructurepLimitpP, Iqq.

Proof. Requirements 1 and 3 are implied by validstructurepP q and implspLimitpP, Iqq Ă
implspP q.

Requirement 2 is @h P holespP q Y holespPbaseq, implsLimitpP,Iqphq ‰
if h R parspIq, then implsLimitpP,Iqphq “ implsP phq,

H. For every h,

43

which must be non-empty given validstructurepP q.
If h P parspIq, then
implsLimitpP,Iqphq “ implsP phq X I, which must have one element by the
deﬁnition of validP pIq.

Lemma 6.14 (Neighbors of I are valid under LimitpI ´tiuYti1u, P q.). @I 1 P
N pM odelGraphpP qq such that siblingspI, I 1q “ tpi, i1qu, validLimitpI´tiuYti1u,P qpI 1q.

Proof.

´ P I 1 such that i1

1. I 1 Ă implspLimitpI ´ tiu Y ti1u, P qq: Suppose Di1

implspLimitpI ´tiuYti1u, P qq. It must be that parpi1
ti1uq, because otherwise implsLimitpI´tiuYti1u,P qpi1
the deﬁnition of Limit. Then Di´ P I ´tiuYti1u s.t. parpi´q “ parpi1
so pi´, i1
Since i1 P I 1, i´ ‰ i1, so i´ P I 1. Then pi1
contradicts siblingspI, I 1q “ tpi, i1qu, so there is no such i1
´

´q ‰ pi, i1q.
´, i´q P siblingspI, I 1q, which

´q P siblingspI ´ tiu Y ti1u, I 1q and pi´, i1

´q “ implsP pi1

´ R
´q P parspI ´tiuY
´q by
´q,

.

2. parspI 1q “ holespI 1q Y holespLimitpI ´ tiu Y ti1u, P qbaseq follows from
validP pI 1q and Lemma 6.13 because Limit does not modify base, holes
or pars.

3. siblingspI 1, I 1q “ H by validP pI 1q.

Lemma 6.15 (Selections that are valid under LimitpI ´ tiu Y ti1u, P q are
neighbors of I.). @I 1 P N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq, validP pI 1q
and siblingspI, I 1q “ tpi, i1qu.

Proof. Because implspLimitpI ´tiuYti1u, P qq Ă implspP q and Lemma 6.13,
validLimitpI´tiuYti1u,P qpI 1q implies each requirement of validP pI 1q.

Suppose siblingspI, I 1q “ H. Because I, I 1 P N pM odelGraphpP qq, by

Lemma 6.11, I “ I 1. Then, i P I ùñ i P I 1. However, by validLimitpI´tiuYti1u,P qpI 1q,
I 1 Ă implspLimitpI ´ tiu Y ti1u, P qq S i, which is a contradiction. Therefore
siblingspI, I 1q ‰ H.

Let pi2, i1

2q. If parpi1
2q P siblingspI, I 1q, so parpi2q “ parpi1
then by the deﬁnition of Limit, implsLimitpI´tiuYti1u,P qpparpi1
i2 “ i1
2
implsLimitpI´tiuYti1u,P qpparpi1
siblingspI, I 1q “ tpi, i1qu.

2q ‰ parpiq,
2qq “ ti2u, so
2q “ parpiq, then
2q must be pi, i1q. Therefore,

2q P siblingspI, I 1q. If parpi1

, which contradicts pi2, i1

2qq “ ti1u, so pi2, i1

44

Theorem 6.6 (The models of LimitpI ´ tiu Y ti1u, P q are exactly the neigh-
bors of I with siblingspI, I 1q “ ti, i1u, so we can union over possible siblings).

ď

ď

N eipI, P q “

iPI

i1Pimplspparpiqq´tiu

.

N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq

Proof. By Lemmas 6.14 and 6.15, t I | I P N pM odelGraphpP qq, siblingspI, I 1q “
tpi, i1qu u “ N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq.

ď

ď

N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq

iPI

i1Pimplspparpiqq´tiu
ď

ď

“

iPI

i1Pimplspparpiqq´tiu

t I | I P N pM odelGraphpP qq, siblingspI, I 1q “ tpi, i1qu u

“t I | I P N pM odelGraphpP qq, |siblingspI, I 1q| “ 1 u
“N eipI, P q

6.3.2 Eﬃciency

Since we are using the M odelGraph implementation from section 6.2.2, the
runtime complexity of N(ModelGraph(P)) is OpH ˚ N q for H “ holespP q
and N “ |N pM odelGraphpP qq|.

Our runtime complexity is then:

OpN eipI, P qq “Op

ď

ď

N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqqq

iPI
ÿ

i1Pimplspparpiqq´tiu
ÿ

iPI
ÿ

i1Pimplspparpiqq´tiu
ÿ

“Op

“Op

N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqqq

H|N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq|q

iPI

i1Pimplspparpiqq´tiu
ÿ

ÿ

“OpH

iPI

i1Pimplspparpiqq´tiu

“OpH|N eipI, P q|q

|N pM odelGraphpLimitpI ´ tiu Y ti1u, P qqq|q

Our space complexity is the size of the output.

45

7 Additional features

This section presents two extensions of the module system described in sec-
tions 4 and 5, giving users more expressive power to build their desired
networks of models.

7.1 Append blocks

In the base syntax, we allow modules to deﬁne a parameters block that upon
concretization is appended onto the base parameters block. In the same way,
we can allow modules to add on to each of the other Stan blocks (except for
data, which is ﬁxed for the whole network of models). The extended syntax
generalizes MODULE_IMPLEMENTATION_M:

MODULE_IMPLEMENTATION_M:

module "impl_identifier" hole_identifier((TYPE identifier,)*) {

FUNCTIONS_M?
TRANSFORMED_DATA_M?
TRANSFORMED_PARAMETERS_M?
PARAMETERS?
MODEL_M?
GENERATED_QUANTITIES_M?
STMT_LPDF_M;*
return EXPR_M;?

}

We refer to these module blocks as append blocks because they are ap-

pended to the end of their corresponding Stan blocks upon concretization.

The rules for scope and available eﬀects are the same for within-module
blocks as they are for the corresponding Stan blocks as described in sec-
tion 3.2. For example, variables deﬁned in a module transformed parameters
block can be referenced inside of that module’s model block but not its
transformed data block. Module arguments are not available inside ap-
pend blocks.

ApplyImpl is updated to include InlineFunction calls for each append

block in the same way that it is used for parameters.

Concretization is still correct:

• Scope is preserved: For any statement in a module that could reference
a global variable g in module append block B, g will still be a valid
reference after concretization because B must also be included. For any

46

statement S in a module append block that references a global variable
h in its corresponding base block, h will still be a valid reference after
concretization because M will be inserted after the declarations of the
base block.

• Type correctness and eﬀect correctness are preserved because appends
are statements and must have the same eﬀect requirements as the
blocks into which they are inserted, as usual.

• All other arguments about statement order, well-typedness, and lack

of cycles are unaﬀected by this change.

Since the inclusion of append blocks has no aﬀect on the module depen-
dency graph, this change does not change the correctness of the network of
models or neighbor algorithms.

Use cases of append blocks include:

• Deﬁning functions for use in the module and as arguments to its de-

scendants.

• Deﬁning transformed data or transformed parameters that are used in

the module.

• Adding a prior distribution to the model block for each parameter
deﬁned the module, without requiring that the module signature have
the LPDF eﬀect.

• Emitting the likelihood of the data under a distribution deﬁned the
module as a generated quantity (as in the case study in section 9.3).

7.2 Module ﬁelds

Sometimes, two or more behaviors only make sense to include together.

For example, suppose we want to deﬁne a change of variables transfor-
mation. We need to deﬁne a transform function and corresponding inverse
function. Suppose we want to abstract the transformation into a hole so that
it can be swapped out. If we were to deﬁne separate holes for Transform
and Inverse, we may end up with implementations may not match, be-
cause they can be selected independently. We would rather have one hole,
Transformation, that packages compatible transform and inverse imple-
mentations together.

We can achieve this coupling by letting modules contain multiple named
behaviors call ﬁelds. With ﬁelds, we would deﬁned one hole, Transformation,

47

with two ﬁelds, forward and reverse, referenced in code as Transformation.forward
and Transformation.reverse.

Fields of a module also share the append blocks (and therefore global

scope) of a module.

The syntax for associated behavior introduces a second, optional variant
of MODULE_IMPLEMENATION_M where arguments are moved into ﬁeld declara-
tions:

MODULE_IMPLEMENTATION_M: module "impl_identifier" hole_identifier {

FUNCTIONS_M?
TRANSFORMED_DATA_M?
TRANSFORMED_PARAMETERS_M?
PARAMETERS?
MODEL_M?
GENERATED_QUANTITIES_M?

field field_identifier?(TYPE identifier*) {

STMT_LPDF_M;*
return EXPR_M;*

} +

}

We use impl_identifier to stand in for valid ﬁeld names, while field is a
new keyword.

Each of a hole’s ﬁelds must be implemented by a corresponding ﬁeld
block and implementations as though it were a new hole. A ﬁeld fi in hole
h is referenced as h.fi(..), except when the ﬁeld identiﬁer fi is empty, in
which case the reference is h(..).

Each of a hole’s ﬁelds is treated like a separate hole for the purposes of
concretization, and they have no diﬀerences in terms of the correctness of the
resulting Stan program. Holes with ﬁelds are treated like holes without ﬁelds
for the purpose of building the module graph, model graph and neighbor sets;
an implementation has a dependency on a hole h if any of its ﬁelds depends
on h.

Coupling behavior into the ﬁelds of a module is only one constraint that
we could impose on co-selection of implementations. We discuss a more
general constraint logic in section 10.

48

8 Macros

While the module system described so far is in theory ﬂexible enough to
describe arbitrarily complex networks of models, it may sometimes be too
verbose to be practical.

For instance, suppose we want to build a regression model, but we don’t
know which of N features to include. Our model space of interest then
consists of one model for each subsets of features. Since we want to choose
inclusion or exclusion for each feature, we will need at least N holes, each
with two implementations, so we will need to write 2N implementations.
This is cumbersome and repetitive for large N .

To make the language more expressive, we deﬁne a series of “macros”
as shorthand ways of generating larger modular programs. Each macro is
translated by the compiler back into the basic module language, so we can
then use the same algorithms deﬁned in section 6.

Table 1 is a summary of the macros described in the following sections.

Name
Collection hole
Indexed hole
Hole instance
Hole copy
Ranged versions

Multi-ranges
Range exponent
Hole product
Hole exponent

Syntax

Description
Select a subset of implementations rather than one
Copy implementations of H for each index in range
Copy a hole; same selection but new parameters
Copy a hole; independent selection
Apply the macro for each index in range,

H+
H[i..j]
H<j>
H«j»
H<i..j>
H«i..j»
i..j,k..l,..
(i..j)^e
H1*H2(..)(..) New hole with implementations implspH1q ˆ implspH2q
H^j

Same as a range, but for each index combination
Same as a range, but e indices without replacement

Same as a hole product but without replacement

collecting an array of results

Table 1: Summary of macros

The expansion of each macro transforms the program’s module graph
in some way, it may synthesize new modules with new (Stan) code, and it
may deﬁne a translation of selection strings (section 6.1.7) to apply to the
post-expansion module graph.

8.1 Collection holes

A “collection hole” is a hole that can be ﬁlled with any number of implemen-
tations, rather than exactly one.

49

Consider our regression model example where we want to use a subset of
N features. For each feature, we need a hole with one implementation that
includes the feature and another that does not. Then, we need to collect
all of the holes into an array to pass into the regression. The “collection
hole” macro automates this pattern, so that the user can instead write one
“collection hole” with N implementations instead of N holes and 2N imple-
mentations.

Collection holes are identiﬁed by a + at the end of a hole identiﬁer. The
value of a non-void collection hole is an array containing the values of the
selected implementations in an undeﬁned order.

Figure 3 shows how expansion of collection holes modiﬁes a program’s

module graph.

Figure 3: Collection hole module graph transformation. A collection hole H+
produces a modiﬁed hole that lets a user select which implementations to
include (yes) or exclude (no), and then concatenates the results (H_merge).

For each implementation i of h+, the implementation yes is the same as
i but wraps its result in a singleton array, while no does nothing and returns
an empty list. The implementation merge_h returns a concatenation of the
arrays returned by each new hole.

Users can then select a subset of implementations in their selection
strings, as in ..h:[i_1,i_2,..]... If a collection hole h has a set of im-
plementations C, then a selection string sbef ore, h:[I],saf ter for some list of
implementations I Ă C is translated to sbef ore, h:merge_h,
iPI h_i:yes,
Ť

iPC´I h_i:no, saf ter.
The network produced by a collection hole includes an edge between two

Ť

nodes if they diﬀer by exactly one inclusion3.

3Like a Hamming graph.

50

8.2

Indexed holes

An “indexed hole” is a hole that can generate additional implementations.

Indexed holes are identiﬁed by adding a range [i..j], where i and j
non-negative integer literals, to at the end of a hole identiﬁer. Implemen-
tations of indexed holes accept an index as an extra argument, denoted in
brackets at the end of the hole identiﬁer in the their deﬁnition, such as in
{..}. Here, j is the index, and can be used as an
module "i" h[j](..)
integer literal within the module, because it will be replaced with by each
integer in the range upon macro expansion. In this way, each written module
implementation serves as a template for generating more.

Figure 4 shows how expansion of indexed holes modiﬁes a program’s

module graph.

Figure 4:
H[1..N] produces N copies of its implementation.

Indexed hole module graph transformation. An indexed hole

Users can then specify an indexed implementation to ﬁll the indexed hole,

as in ..h:i[5]...

8.3 Hole instances and hole copies

So far, the design of holes eﬀectively assumes that they represent decisions
about individual subcomponents of a model. For example, when a parameter
is deﬁned within a hole, it is only ever translates to single parameter in the
resulting program.
It may be instead that a hole should be repeated in
multiple places.

For example, suppose a hole h represents a model of a storm cloud. What

if we have data from two storm clouds? There are three possibilities:

1. We only want one copy of h, and data from both clouds will be used

to estimate the parameters in h.

51

2. We want two copies of h, one to model each cloud, with identical

implemenations but separate parameters.

3. We want two copies of h, one to model each cloud, but which may have

diﬀerent implementations.

Case 1 corresponds to the basic semantics: users can call hpq multiple
times, but all references to h are ﬁlled with the same implementation, and the
append blocks of h’s implementation are only added once, so all references
use the same parameters.

Case 2 is the motivation for a macro called hole instances.
Case 3 is the motivation for a macro called hole copies.

8.3.1 Hole instances

Hole instances are identiﬁed by the syntax hole_identifier<j>, where j is
a non-negative integer literal. Hole instances are transformed in the module
tree in the following way:

Figure 5 shows how expansion of hole instances modiﬁes a program’s

module graph.

Figure 5: Hole instance module graph transformation. When some code pi
refers to H<i>, a new hole called H<i> is created with copies of the imple-
mentations of H.

Here, h[1] . . . h[N] are identical copies of h except that their local and
global variables are given unique names. This way, there are N copies of the
implemenation of h, each refering to its own set of new global variables such
as parameters. Each j found in a reference hole_identifier<j> produces
a new instance.

Users specify selections in the same way as if h were not copied: ..,h:i,...

A selection string sbef ore, h:i, saf ter is translated into sbef ore,

jP1...N hj:i, saf ter.

Optionally, module implementations can accept an index as an extra
argument, denoted in angle brackets at the end of the hole identiﬁer in
{..}. When that module
their deﬁnition, such as in module "i" h<j>(..)

Ť

52

implementation is used as a hole instance, the variable j can be used as an
integer literal. This way, j can specify hole instances within the module,
copying deeper into the module graph.

8.3.2 Hole copies

Hole copies are identiﬁed by the syntax hole_identifier«j», where j is
a non-negative integer literal. Hole copies behave the same way as hole
instances, except that implementations must be selected independently for
each generated hole h«j», as in: ..,h«1»:i1, . . . , h«N»:iN,...

8.3.3 Ranged hole instances and copies

To generate many hole instances or copies automatically, h[1:N](..) is
} and
translated into a Stan array { h[1](..), h[2](..), .., h[N](..)
}.
h<1:N>(..) is translated into { h<1>(..), h<2>(..), .., h<N>(..)

8.4 Multi-ranges and range exponents

Macros that can have ranges, namedly H[i..j], H<i..j>, and H«i..j», can
also accept multi-ranges. Multi-ranges are the same as ranges except that
they produce one result per combination of their ranges. For example, a
1..3,1..5 produces (1,1),(1,2),..(3,5).
Implementations that accept
indices as extra arguments, such as h<j>, must then accept h<i,j>.

Ranges exponents come in three variants. R^n is equivalent to a multi-
range with R repeated n times, for example, (1..3)^2 is equivalent to 1..3,1..3.
R^Pn is like R^n except that it gives ordered permutations without replace-
ment, for example (1..3)^P3 does not include (1,1), (2,2), or (3,3). R^Cn
is like R^Pn except that it gives unordered combinations without replacement,
for example (1..3)^C3 does not include (2,1), (3,1), or (3,2).

Multi-ranges and range exponents make it easier to generate holes and

implementations that represent combinations.

8.5 Hole products and hole exponents

A hole product H1*H2(.., ..) is a hole that combines the implementations
of the holes H1 and H2. For each i1 P implspH1q and i2 P implspH2q, pi1, i2q
is an implementation of H1 ˆ H2 that returns a tuple of the results of i1 and
i2 The arguments lists of H1 and H2 are concatenated together to make the
argument list of H1*H2.

53

Figure 6 shows how expansion of hole products modiﬁes a program’s

module graph.

Figure 6: Hole product expansion shown as a module graph transformation.
When some code p refers to a product H1*H2, a new hole is created with the
cartesian product of H1 and H2’s implementations.

Hole exponents are analogous to range exponents from section 8.4. H^n
is equivalent to the product of H with itself n times. H^Pn gives the or-
dered n-permuations of implspHq without replacement, while H^Cn gives the
unordered n-combinations of implspHq without replacement.

8.6 Example application of macros

Recall the regression example in which we want to include some subset of
N variables in our model. Suppose N “ 100. To encode this model space
in the module system requires 2N “ 200 module implementations. Using
an indexed collection hole, we can reduce this to one handwritten module
implementation.

data {

int N;
matrix[100, N] x;
vector[N] y;

}
parameters {

real sigma;

}
model {

y ~ normal(sum(Feature[1..100]+(x)), sigma);

}

module "f" Feature[n](x) {

parameters {

54

real theta;

}
return theta*x[n,:];

}

This program represents a family of regression models on y given the fea-
tures x, each including a diﬀerent subset of features. y is modeled with a nor-
mal distribution centered on the sum of the subset of features, Feature[1..100]+(x),
which is an indexed collection hole: the range [1..100] copies the Feature
module implementation for n=1 to 100, and the + indicates that each imple-
mentation is either included or excluded from the result. An individual Stan
program can be generated from this family by supplying a selection string,
such as: Feature:[1,2,3], which includes only the ﬁrst three features.

Now suppose we also want our regression to include some subset of 2- and
3-way interactions between variables, such as x[3]*x[9] or x[4]*x[10]*x[99].
To encode this model space requires 2˚p100`
q “ 333500 module
implementations. We can use macros to reduce this to two or three handwrit-
ten module implementations. We will show two alternative implementations.

˘
100
3

˘
100
2

`

`

`

The ﬁrst way makes use of range exponents:

data {

int N;
matrix[100, N] x;
vector[N] y;

}
parameters {

real sigma;

}
model {

y ~ normal(sum( Feature[1..100]+(x) )

+ sum( FeaturePair[(1..100)^C2]+(x) ),
+ sum( FeatureTriplet[(1..100)^C3]+(x) ),

}

sigma);

module "f" Feature[n](x) {

parameters {

real theta;

}
return theta * x[n];

55

}

module "fp" FeaturePair[n, m](x) {

parameters {

real theta;

}
return theta * x[n] .* x[m];

}

module "ft" FeatureTriplet[n, m, p](x) {

parameters {

real theta;

}
return theta * x[n] .* x[m] .* x[p];

}

This program uses indexed collection holes with range exponentials to collect
and sum a subset of the features, feature pairs, and feature triplets. An indi-
vidual Stan program can be generated by supplying a selection string such as:
Feature:[1,2,3], FeaturePair:[(1,2),(1,4)], FeatureTriplet:[(1,2,3),(4,10,99)].

The second way makes use of hole products and exponents:

data {

int N;
matrix[100, N] x;
vector[N] y;

}
parameters {

real sigma;

}
model {

vector[N] total = rep_vector(0, 100);
for ((t, r) in Theta*Col[1..100]+()) {

total += t * r;

}
for ((t, r1, r2) in Theta*Col[1..100]^C2+()) {

total += t * r1 .* r2;

}
for ((t, r1, r2, r3) in Theta*Col[1..100]^C3+()) {

total += t * r1 .* r2 .* r3;

}

56

y ~ normal(total, sigma);

}

module "t" Theta() {

parameters {

real theta;

}
return theta;

}

module "r" Col[n]() {

return x[n];

}

By taking a product Theta*Col[1..100], we are generating an implemen-
tation of Col for each index 1 to 100, and then producing a new parameter
for each of those implementations. By taking a product of Theta with the
exponent Col[1..100]^2, we are producing a new parameter for each pair
of indices. Each of these products returns an array of tuples of values which
must then be multiplied and summed into the total vector. The selec-
tion strings for this program look like: Theta*Col:[(t,1),(t,2),(t,3)],
Theta*Col^2:[(t,1,2),(t,1,4)], Theta*Col^3:[(t,1,2,3),(t,4,10,99)].
These programs have « 333500 module implementations and represent
networks with 2100`p100
3 q « 1050000 models. How can we use such large
programs and spaces? By never explicitly representing them. Macros instan-
tiate synthetic modules lazily, only when they are selected. Large networks
can be explored eﬃently by only enumerating neighbors with ModelNeighbors;
in this case, the network diameter and branching factor are a more managable
166750.

2 q`p100

9 Example case studies

In this section we present two small but real-world probabilistic modeling
case studies that we have translated into the modular Stan language, and we
discuss their beneﬁts. These case studies showcase only two of the motivating
use cases listed in section 1; the rest we for future work.

In addition, we present a web interface that can be used to follow along

with our two examples.

57

9.1

Interactive web interface

We have built a prototype web interface for development and interactive
visualizations of modular Stan [21]. Figure 7 shows its interface.

Figure 7: A labeled screenshot of the prototype web interface for modular
Stan.

Users can write a modular Stan program or load an example program
and compile it at (a). When they do, interactive visualizations are produced:
(b) the module graph and (c) the model graph.

The page keeps track of a module selection set. Users can modify the
selection by: selecting or deselecting implementations the module graph (b),
selecting complete models in the model graph (c), editing the selection string
directly (d), or selecting a previously labeled model (d). When the selection
is modiﬁed, the model graph (c) highlights nodes compatible with that se-
lection, and when the selection is valid, the corresponding concrete Stan
program is displayed (e) and labels and notes associated with that program
can be edited (f). The set of model labels and notes can be saved and loaded
as a text ﬁle.

Users can also bookmark and annotate nodes in model graph. Annota-
tions can be saved and loaded as ﬁles separate from the modular Stan ﬁle,
in a format that maps between the model’s unique selection set and model
labels and annotations.

Interactive versions of the following two case studies can be found at [34]

58

and [31].

9.2

“Golf ” case study: Modular Stan for ease and clarity of
development

This section gives a basic demonstration of how modular Stan can cleanly
support and express a typical model development workﬂow, as an example
of application 3.

The “Golf” case study [32] follows the development of a Bayesian statis-
tical model for describing the probability that a professional golfer will sink
a shot given their distance from the hole.

We represent the modeling process as a single modular Stan program.
The base of the program is the part that remains constant throughout de-
velopment:

data {

int J;
vector[J] x;
int n[J];
int y[J];

// Number of distances
// Distances
// Number of shots at each distance
// Number of successful shots at each distance

}
model {

y ~ NSuccesses(n, PSuccess(x));

}

The data block describes J distances, where the jth distance is x[j] feet,
and y[j] shots out of n[j] were successful.

The model block describes an abstracted modeling approach: we model
the number of successes y as being drawn from some distribution NSuccesses
parameterized by the number of attempts n and the probability of success
PSuccess, which itself is a function of the distance x. NSuccesses and
PSuccess are holes.

A natural distribution to choose for NSuccesses is the binomial distri-

bution. We express this as a module:

module "binomial" NSuccesses(y | n, p) {

y ~ binomial(n, p);

}

A simple way to take a real value like x to a probability is the logit

function, so we choose to explore that option ﬁrst for PSuccess:

59

module "logistic" PSuccess(x) {

parameters {
real a;
real b;

}
return logit(a + b*x);

}

If we stop here, we have only one choice of implementation for each of
our holes, so our modular Stan program deﬁnes only one valid Stan program:

data {

int J;
vector[J] x;
int n[J];
int y[J];

// Number of distances
// Distances
// Number of shots at each distance
// Number of successful shots at each distance

}
parameters {
real a;
real b;

}
model {

y ~ binomial(n, logit(a + b*x));

}

This program implement logistic regression, which is the ﬁrst model explored
in the case study.

The next step in the probabilistic workﬂow or Box’s loop is to criticize
our model by applying it to the data. We ﬁnd that the model ﬁt is lacking
and the parameters a and b have no obvious physical interpretation.

This criticism motivates us to try a more sophisticated, more physically
realistic implementation for PSuccess.
If we suppose that a shot will be
successful if its trajectory angle is suﬃciently precise, and we suppose that
the angle is normally distributed, then we can write our model in terms of
angle variance:

// A shot’s angle is good if the center of the ball would roll
// over the hole.
module "angle_success" PSuccess(x) {

parameters {

60

(a)

(b)

real sigma_angle;

}
real r = (1.68 / 2) / 12; // ball radius
real R = (4.25 / 2) / 12; // hole radius
vector[J] threshold_angle = asin((R-r) ./ x);
vector[J] p_angle = 2*Phi(threshold_angle / sigma_angle) - 1;
return p_angle;

}

We ﬁnd this model has superior ﬁt and interpretability and we continue

iterating by adding on modules in this fashion.

The completed representation of the case study can be found with our
source code [33] or at the web interface along with visualizations [34]. Here
it is presented by the web interface:

Though this is a small example, we already see some beneﬁts to clarity:

• There is only a single, minimal source ﬁle;

• We have a standard, integrated way to draw the development path and
document the decision-making evidence and rationale at each step;

• The modular organization makes the solution space easier to under-

stand and extend.

9.3

“Birthday” case study: Modular Stan as a platform for
automation

This section gives an example of how the network of models provides a plat-
form for automation (use case 1). We demonstrate constructing a network,

61

deﬁning an evaluation metric, and performing a simple graph search. We
use a moderately sized network of models from a case study of incremental
model improvements. This approach would also apply to other multiple-
model contexts, such as feature selection and symbolic regression.

The “Birthday” case study [35, 36] follows the development of a statistical
model of the number of babies born in the US on a given day, given birth data
from 1969-1988. The authors used a time-series Gaussian process approach.
Like many probabilistic modeling case studies and publications, the Birth-
day case study presents a series of models that diﬀer by incremental varia-
tions. The authors explicitly evaluate nine models, but the model variations
they present implicitly deﬁne a much larger set of models that it would be
reasonable to explore: what if a diﬀerent combination of variations were ap-
plied, or in a diﬀerent way? To feasibly explore that larger set of models,
we need automation, and for automation, we need an explicit representation
of the model space, like the network of models oﬀered by a modular Stan
program.

We start by translating the case study into a single modular Stan pro-
gram. The translation reduces the number of lines of code from 1098 to 270
while increasing the number of models represented from nine to 120.

The translation process is largely mechanical, and involves encapsulating
the variation between the given models into modules. For example, model
2 adds a days-of-the-week Gaussian process component onto on model 1,
model 3 adds a long-term-trend component onto model 2, etc. Each of these
variations becomes one or more module in the modular Stan program. The
full translation of the modular program can be found with our source code
[30] or at the web interface along with visualizations [31] 4.

Figure 9 shows the model graph produced by the modular program.
To automatically search these 120 models for high-quality options, we
must ﬁrst deﬁne “quality” by choosing a model-scoring metric. One reason-
able approach is to measure a model’s predictive accuracy by computing
its Expected Log-Posterior Density (ELPD). ELDP approximates the leave-
one-out prediction accuracy of a model for a dataset [37].

While ELPD is relatively eﬃcient, it could still take a long time to ac-
curately compute ELPD for every model in our network. Our goal, then,
is to ﬁnd high-quality models with as few ELPD evaluations as possible.
Here we take the simplest approach, a greedy graph search, and leave more

4Our modular Stan program would have been signiﬁcantly more concise if we used the
collection hole feature described in section 8.1, but they were not implemented in our
prototype compiler at the time of writing.

62

Figure 9: The model graph corresponding to the Birthday problem modular
Stan program. Each node represents a model and each edge represents one
swapped-out module.

63

sophisticated search methods for future work.

Our greedy graph search loops over the following steps, given an arbitrary

starting point:

1. Score the neighbors of the current model.

2. Move to the highest scoring model seen so far, or if that is the current

model, return it.

This search algorithm is not guaranteed to be optimal; it is analogous to a
gradient descent of a (likely non-convex) space. Intuitively, the closer the
maximal-neighbor operation is to a gradient, the more eﬃcient the search
will be, so we can expect that the semantically minimal changes represented
by the edges in the model graph provide exactly the topology of models
closest to a smooth continuous space.

The search algorithm is implemented as a short Python script that uses
the prototype compiler’s implementation of the ModelNeighbors algorithm
described in section 6.3. Its source code is available online [20].

Starting from the case study’s ﬁrst model, the greedy graph search fol-

lowed the path shown in ﬁg. 10:

The search performed 47 ELPD evaluations. The search agreed with
the case study authors’ ﬁnal model, conﬁrming that it has (at least locally)
optimal predictive performance.

While greedy maximization of ELPD is a naive statistical workﬂow and
shouldn’t be blindly trusted it to give a ﬁnal model, it is at least useful for
ﬁnding promising neighborhoods, especially for large model spaces.

10 Future work

As discussed in section 1.1, there are many motivating use cases for the
network of models. We hope to demonstrate and build tooling for more of
these use cases, using modular Stan as a foundation:

• Model search. Section 9.3 gave a simple algorithm for greedy search
that only used the network topology. We hypothesize that more eﬃ-
cient search methods could leverage the module structure, treat search
as an exploration/exploitation problem, or utilize methods from the
symbolic regression literature.

• Model space navigation tools. Model developers must decipher promis-
ing directions in which to iterate. If we could annotate the edges be-

64

Figure 10: The red annotations show the ELPD scores for the assessed
models. The search algorithm visited nodes along the red arrow path from
starting at [START] and terminating at [GOAL].

65

tween models with meaningful joint metrics, we could provide develop-
ers with direction. We could also use model statistics or edge weights
to embed network-space into visualizations.

• Multi-model ensemble methods. As mentioned in section 1.1, multi-
model ensemble methods like stacking and multiverse analysis could
be applied to multi-model programs.

• Sensitivity analysis. To automate sensitivity analysis given a model
and some metric of a model’s results, like a p-value, we can check the
extent to which the model’s results diﬀer from its neighbors.

In addition, we hope to expand the capabilities of the swappable module
system.

• Explicit model signatures. Thought we believe that our module sig-
nature inference scheme is more beginner-friendly, explicit signatures
would make module reuse across applications easier.

• Implementation co-selection logic. While features like module ﬁelds al-
low users to encode the constraint that a set of implementations should
always be selected together, there is no reason that user constraints on
selection sets should not be arbitrarily complex, the end point being a
predicate logic.

66

References

[1] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using stacking to av-
erage bayesian predictive distributions (with discussion). Bayesian Analysis, 13:917–
1003, 2018. doi: 10.1214/17-BA1091. URL https://doi.org/10.1214/17-BA1091.

[2] Juho Piironen and Aki Vehtari. Comparison of bayesian predictive methods for
model selection. Statistics and Computing, 27:711–735, 5 2017.
ISSN 15731375.
doi: 10.1007/S11222-016-9649-Y/FIGURES/12. URL https://link.springer.
com/article/10.1007/s11222-016-9649-y.

[3] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Car-
penter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and
Martin Modrák. Bayesian workﬂow. 11 2020. doi: 10.48550/arxiv.2011.01808. URL
https://arxiv.org/abs/2011.01808v1.

[4] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation
using leave-one-out cross-validation and waic *. 2016. URL https://github.com/
stan-dev/loo.

[5] Comparing posterior predictions.

URL https://cran.r-project.org/web/

packages/bayesnec/vignettes/example4.html.

[6] Sara Steegen, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel. Increasing
transparency through a multiverse analysis. Perspectives on psychological science :
a journal of the Association for Psychological Science, 11:702–712, 9 2016.
ISSN
1745-6924. doi: 10.1177/1745691616658637. URL https://pubmed.ncbi.nlm.nih.
gov/27694465/.

[7] George E. P. Box. Science and statistics. Journal of the American Statistical Asso-

ciation, 71:791, 12 1976. ISSN 01621459. doi: 10.2307/2286841.

[8] Philip Jia Guo. Software tools to facilitate research programming, 5 2012. URL

http://purl.stanford.edu/mb510fs4943.

[9] Kayur Patel, James Fogarty, James A Landay, and Beverly Harrison. Investigating

statistical machine learning as a tool for software development. 2008.

[10] Mary Beth Kery, Amber Horvath, and Brad Myers. Variolite: Supporting exploratory
programming by data scientists. Conference on Human Factors in Computing Sys-
tems - Proceedings, 2017-May:1265–1276, 5 2017. doi: 10.1145/3025453.3025626.

[11] Ronald L. Wasserstein and Nicole A. Lazar. The asa statement on p-values: Context,
process, and purpose. https://doi.org/10.1080/00031305.2016.1154108, 70:129–133,
4 2016. ISSN 15372731. doi: 10.1080/00031305.2016.1154108.

[12] Andrew Gelman and E. Loken. The garden of forking paths: Why multiple compar-
isons can be a problem, even when there is no "ﬁshing expedition" or "p-hacking"
and the research hypothesis was posited ahead of time. 2019.

67

[13] Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn. False-positive psychology:
Undisclosed ﬂexibility in data collection and analysis allows presenting anything as
signiﬁcant. Psychological Science, 22:1359–1366, 10 2011.
ISSN 14679280. doi:
10.1177/0956797611417632. URL https://journals.sagepub.com/doi/10.1177/
0956797611417632.

[14] Daniel H J Wigboldus and Ron Dotsch.

Encourage playing with data and
discourage questionable reporting practices.
Psychometrika, 81:27–32, 2016.
doi: 10.1007/s11336-015-9445-1. URL https://groups.google.com/forum/#!msg/
spsp-discuss/6oP7QwsfBXM/bZzsx8AOafQJ.

[15] Jean-François F Viel, Dominique Pobel, and André Carré. Incidence of leukaemia
in young people around the la hague nuclear waste reprocessing plant: a sensitivity
analysis. Statistics in medicine, 14:2459–2472, 1995. ISSN 0277-6715. doi: 10.1002/
SIM.4780142114. URL https://pubmed.ncbi.nlm.nih.gov/8711281/.

[16] Lehana Thabane, Lawrence Mbuagbaw, Shiyuan Zhang, Zainab Samaan, Maura
Marcucci, Chenglin Ye, Marroon Thabane, Lora Giangregorio, Brittany Dennis,
Daisy Kosa, Victoria Borg Debono, Rejane Dillenburg, Vincent Fruci, Monica Ba-
wor, Juneyoung Lee, George Wells, and Charles H. Goldsmith. A tutorial on sensi-
tivity analyses in clinical trials: The what, why, when and how. BMC Medical Re-
search Methodology, 13:1–12, 7 2013. ISSN 14712288. doi: 10.1186/1471-2288-13-92/
TABLES/2. URL https://bmcmedresmethodol.biomedcentral.com/articles/10.
1186/1471-2288-13-92.

[17] Mark Rubin. An evaluation of four solutions to the forking paths problem: Adjusted
alpha, preregistration, sensitivity analyses, and abandoning the neyman-pearson ap-
proach. Review of General Psychology, 21:321–329, 2017. doi: 10.1037/gpr0000135.

[18] Stan Development Team. Stan reference manual version 2.29, . URL https://

mc-stan.org/docs/2_29/reference-manual/index.html.

[19] Maria I. Gorinova, Andrew D. Gordon, and Charles Sutton. Probabilistic pro-
gramming with densities in slicstan: Eﬃcient, ﬂexible, and deterministic. Proceed-
ings of the ACM on Programming Languages, 3:30, 1 2019.
ISSN 24751421. doi:
10.1145/3290348.

[20] Ryan Bernstein. rybern/mstan: "swappable module" compiler for the stan proba-

bilistic programming language., . URL https://github.com/rybern/mstan.

[21] Ryan Bernstein. Modular stan interface, . URL http://ryanbe.me/modular-stan.

html.

[22] Bob Carpenter. What do we need from a ppl to support bayesian workﬂow?
ProbProg, 10 2021. URL https://statmodeling.stat.columbia.edu/wp-content/
uploads/2021/10/probprog2021.pdf.

[23] Sean J Taylor and Benjamin Letham. Forecasting at scale. 9 2017. ISSN 2167-9843.
doi: 10.7287/PEERJ.PREPRINTS.3190V2. URL https://peerj.com/preprints/
3190.

68

[24] Edward Z Yang. Backpack: Towards practical mix-in linking in haskell, 6 2017. URL

http://purl.stanford.edu/zf998bz8716.

[25] R. (Robin) Milner, Mads. Tofte, and Robert Harper. The deﬁnition of standard ml.

page 101, 1990.

[26] Derek Dreyer. Understanding and evolving the ml module system, 5 2005.

[27] The ocaml manual - the module system. URL https://v2.ocaml.org/manual/

moduleexamples.html.

[28] Stan Development Team.

stan-dev/stanc3,
URL https://github.com/stan-dev/stanc3/blob/master/src/frontend/

stanc3/typechecker.mli at master :

.
Typechecker.mli.

[29] Stan Development Team.

.
https://mc-stan.org/docs/2_29/stan-users-guide/optimization.html#
function-inlining.

34.5 optimization | stan user’s guide,

URL

[30] Ryan Bernstein. mstan/birthday.m.stan at master : rybern/mstan, . URL https:
//github.com/rybern/mstan/blob/master/examples/birthday/birthday.m.stan.

[31] Ryan Bernstein. Modular stan interface (birthday example), . URL http://ryanbe.

me/modular-stan.html?example=birthday.

[32] Andrew Gelman and Deborah Nolan. A probability model for golf putting. Teaching

Statistics, 24:93–95, 2002. ISSN 14679639. doi: 10.1111/1467-9639.00097.

[33] Ryan Bernstein. mstan/golf.m.stan at master : rybern/mstan, . URL https://

github.com/rybern/mstan/blob/master/examples/golf.m.stan.

[34] Ryan Bernstein. Modular stan interface (golf example), . URL http://ryanbe.me/

modular-stan.html?example=golf.

[35] Andrew Gelman, Aki Vehtari, Daniel Simpson, John B Carlin, Hal S Stern, David B
Dunson, and Donald B Rubin. casestudies/birthdays at master : avehtari/casestud-
ies. URL https://github.com/avehtari/casestudies/tree/master/Birthdays.

[36] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and
Donald B Rubin. Bayesian Data Analysis. Chapman & Hall/CRC, 3rd edition,
2013.

[37] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation
using leave-one-out cross-validation and waic. Statistics and Computing 2016 27:5,
27:1413–1432, 8 2016.
ISSN 1573-1375. doi: 10.1007/S11222-016-9696-4. URL
https://link.springer.com/article/10.1007/s11222-016-9696-4.

69

