Noname manuscript No.
(will be inserted by the editor)

Complexity of Stochastic Dual Dynamic Programming

Guanghui Lan

2
2
0
2

b
e
F
3
2

]

C
O
.
h
t
a
m

[

8
v
2
0
7
7
0
.
2
1
9
1
:
v
i
X
r
a

Submitted: December 16, 2019; Revised: May 27, 2020, July 31, 2020, November 2, 2021.

Dedicated to Professor Alexander Shapiro on the occasion of his 70th birthday for his profound
contributions to stochastic optimization.

Abstract Stochastic dual dynamic programming is a cutting plane type algorithm
for multi-stage stochastic optimization originated about 30 years ago. In spite of
its popularity in practice, there does not exist any analysis on the convergence
rates of this method. In this paper, we ﬁrst establish the number of iterations,
i.e., iteration complexity, required by a basic dual dynamic programming method
for solving single-scenario multi-stage optimization problems, by introducing novel
mathematical tools including the saturation of search points. We then reﬁne these
basic tools and establish the iteration complexity for an explorative dual dynamic
programing method proposed herein and the classic stochastic dual dynamic pro-
gramming method for solving more general multi-stage stochastic optimization
problems under the standard stage-wise independence assumption. Our results in-
dicate that the complexity of some deterministic variants of these methods mildly
increases with the number of stages T , in fact linearly dependent on T for dis-
counted problems. Therefore, they are eﬃcient for strategic decision making which
involves a large number of stages, but with a relatively small number of decision
variables in each stage. Without explicitly discretizing the state and action spaces,
these methods might also be pertinent to the related reinforcement learning and
stochastic control areas.

This research was partially supported by the NSF grant 1953199 and NIFA grant 2020-67021-
31526.

H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Tech-
nology, Atlanta, GA, 30332. (email: george.lan@isye.gatech.edu).

Address(es) of author(s) should be given

 
 
 
 
 
 
2

1 Introduction

Guanghui Lan

In this paper, we are interested in solving the following stochastic dynamic opti-
mization problem

H1(x1, c1)+λE

min
X1
x1∈

min
X2(x1)

x2∈
(cid:20)

H2(x2, c2) + λE

+ λE[

min
XT (xT −1)

xT

∈

HT (xT , cT )]

,

(1.1)

(cid:21)
(cid:3)

· · ·

(cid:2)

with feasible sets Xt given by

Xt(xt

−

1)

X1 :=

x

∈
Xt(xt
(cid:8)

x

∈

≡
:=

¯X1
⊆
1, ξt)
−
¯Xt

⊆

Rn1 : A1x1 = b1, Φ1(x1, p1)

0

,

≤

(1.2)

Rnt : Atx = Btxt
−

(cid:9)
1 + bt, Φt(x, pt)

Qtxt

−

1

.

≤

(1.3)

(cid:8)

(cid:9)

∈

⊂

→

Rmt, and Qt : Rnt−1

, ct) are closed convex objective func-
Here T denotes the number of stages, Ht(
·
Rnt are closed convex sets, λ
tions, ¯Xt
(0, 1] denotes the discounting fac-
tor, At : Rnt
Rmt , Bt : Rnt−1
Rpt are linear
→
, pt) : Rnt
R, i = 1, . . . , pt are closed convex constraint
mappings, and Φt,i(
·
functions. Moreover, ξ1 := (A1, b1, B1, p1, c1) is a given deterministic vector, and
ξt := (At, bt, Bt, Qt, pt, ct), t = 2, . . . , T , are the random vectors at stage t. In par-
ticular, if Ht are aﬃne, Xt are polyhedral and Φt do not exist, then problem (1.1)
reduces to the well-known multi-stage stochastic linear programming problem (see,
e.g., [5, 28]). The incorporation of the nonlinear (but convex) objective functions
Ht and constraints Φt allows us to model a much wider class of problems.

→

→

In spite of its wide applicability, multi-stage stochastic optimization remains
highly challenging to solve. As shown by Nemirovski and Shapiro [29] and Shapiro [26],
the number of scenarios of ξt, t = 2, . . . , T , required to solve problem (1.1) has to
increase exponentially with T . In particular, if the number of stages T = 3, the
(1/ǫ4) in general.
total number of samples (a.k.a. scenarios) should be of order
There exist many algorithms for solving multi-stage stochastic optimization prob-
lems (e.g., [21, 24, 11]), but quite often without guarantees provided on their rate
of convergence. More recently, Lan and Zhou [17] developed a dynamic stochas-
tic approximation method for multi-stage stochastic optimization by generalizing
stochastic gradient descent methods, and show that this algorithm can achieve this
optimal sampling and iteration complexity bound for solving general multi-stage
stochastic optimization problems with T = 3. The complexity of this method de-
pends mildly on the problem dimensions, but increases exponentially with respect
to T . As a result, this type of method is suggested for solving some operational
decision-making problems, which involve a large number of decision variables but
only a small number of stages.

O

In practice, we often encounter strategic decision making problems which span
a long horizon and thus require a large number of stages T . In this situation, a
crucial simpliﬁcation that has been explored to solve problem (1.1) more eﬃciently
is to assume the stage-wise independence. In other words, we make the assumption
that the random variables ξt, t = 2, . . . , T , are mutually independent of each other.
Under this assumption, we can write problem (1.1) equivalently as

minx1∈

H1(x1, c1) + λV2(x1)

X1 {

,
}

(1.4)

Complexity of Stochastic Dual Dynamic Programming

where the value factions Vt, t = 2, . . . , T , are recursively deﬁned by

Vt(xt
t(xt

V

−

−

:= E[
1)
1, ξt) := minxt

V

t(xt

1, ξt)],
−
Xt(xt−1){

∈

Ht(xt, ct) + λVt+1(xt)

,
}

and

VT +1(xT ) = 0.

3

(1.5)

(1.6)

Furthermore, as pointed out by Shapiro [27], one can generate a relatively small
(i.e., Nt) number of samples for each ξt and deﬁne the so-called sample average ap-
proximation (SAA) problem by replacing the expectation in (1.5) with the average
over the generated samples (see Section 4 for more details).

Under the aforementioned stage-wise independence assumption, a widely-used
method for solving the SAA problem is the stochastic dual dynamic programming
(SDDP) algorithm. SDDP is an approximate cutting plane method, ﬁrst presented
by Birge [4] and Pereira and Pinto [21] and later studied by Shapiro [27], Philpott
et. al. [22], Donohue and Birge [6], Hindsberger [12], Kozm´ık and Morton [14],
Guigues [10] and Zou et. al. [30], among many others. SDDP has been applied
to solve problems arising from many diﬀerent ﬁelds such as hydro-thermal plan-
ning [9, 30] and bio-chemical process control [2]. Each iteration of this algorithm
contains two phases. In the forward phase, feasible solutions at each stage will
be generated starting from the ﬁrst stage based on the cutting plane models for
the value functions built in the previous iteration. Then in the backward phase,
the cutting plane models for the value functions of each stage will be updated
starting from the last stage. While the cost per iteration of the SDDP method
only linearly depends on the number of stages, it remains unknown what is the
number of iterations required by the SDDP method to achieve a certain accurate
solution of problem (1.4). Existing proofs of convergence of SDDP are based on
the assumption that the procedure passes through every possible scenario many
times [27, 19, 8]. Of course when the number of scenarios, although ﬁnite, is astro-
nomically large this is not very realistic. In addition, such analysis does not reveal
the dependence of the eﬃciency of SDDP on various parameters, e.g., number of
stages, target accuracy, Lipschitz constants, and diameter of feasible sets etc.

It is well-known that when the number of stages T = 2, SDDP reduces to the
classic Kelley’s cutting plane method [13]. As shown in Nesterov [20], the number
of iterations required by Kelley’s cutting plane method could depend exponentially
on the dimension of the problem even for a static optimization problem inevitably.
Therefore, this type of method is not recommended for solving large-scale opti-
mization problems. However, it turns out that the global cutting plane models are
critically important for multi-stage optimization especially if the number of stages
is large and one does not know the structure of optimal policies. In these cases
we need to understand the eﬃciency of these cutting plane methods in order to
identify not only problem classes amenable for these techniques, but also possibly
to inspire new ideas to solve these problems more eﬃciently.

This paper intends to close the aforementioned gap in our understanding about
cutting plane methods for multi-stage stochastic optimization. Our main contri-
butions mainly exist in the following several aspects. Firstly, we start with a dual
dynamic programming (DDP) method for solving dynamic convex optimization
problem with a single scenario. This simpliﬁcation allows us to build a few es-
sential mathematical notions and tools for the analysis of cutting plane methods.

4

Guanghui Lan

More speciﬁcally, we introduce the notion of saturated and distinguishable search
points. Using this notion, we show that each iteration of DDP will either ﬁnd a new
saturated and distinguishable search point, or compute an approximate solution
for the original problem. As a consequence, we establish the total number of iter-
ations required by the DDP method for solving the single-scenario problem. More
speciﬁcally, we show that the iteration complexity of DDP only mildly increases
w.r.t. the number of stages T , in fact linearly dependent on T for many problems,
especially those with a discounting factor λ < 1. The dependence of DDP on other
problem parameters has also been thoroughly studied. We also demonstrate that
one can terminate DDP based on some easily computable upper and lower bounds
on the optimal value.

Secondly, motivated by the analysis of the DDP method, we propose a new
explorative dual dynamic programming (EDDP) for solving the SAA problem of
multi-stage stochastic optimization in (1.4). When solving the SAA problem, we
have to choose one out of Nt possible feasible solutions in the forward phase, and
each one of them corresponds to a random realization of ξt. In EDDP, we choose
a feasible solution in an aggressive manner by selecting the most distinguishable
search point among the saturated ones in each stage. As a result, we show that the
number of iterations required by EDDP for solving the SAA problem is the same
as that of DDP for solving the single-scenario problem. However, to implement
EDDP we need to maintain the set of saturated search points explicitly.

Thirdly, we show that the SDDP method can be viewed as a randomized
version of the EDDP algorithm by choosing the aforementioned feasible solution
at each stage t randomly from the Nt possible selections. Since this algorithm is
stochastic, we establish the expected number of iterations required by SDDP to
compute an approximate feasible policy for solving the SAA problem. In particular
the iteration complexity of SDDP is worse than that of DDP and EDDP by a factor
of ¯N := N2
1, which increases exponentially w.r.t. T . However, it may
still have mild dependence on T for the low accuracy region (see Section 5 for more
discussions). Moreover, we show that the probability of having large deviation
from this expected iteration complexity decays exponentially fast. In addition, we
establish the convergence of the gap between a stochastic upper bound and lower
bound on the optimal value, and show how we can possibly use these bounds to
terminate the algorithm.

N3 . . . NT

×

−

To the best of our knowledge, all the aforementioned complexity results, as
well as the analysis techniques, are new for cutting plane methods for multi-stage
stochastic optimization.

This paper is organized as follows. In Section 2, we present some preliminary
results on the basic cutting plane methods for solving static convex optimization
problems. In Section 3, we present the DDP method for single-scenario problems
and establish its convergence properties. Section 4 is devoted to the EDDP method
for solving the SAA problem for multi-stage stochastic optimization. In Section
5, we establish the complexity of the SDDP method. Finally, some concluding
remarks are made in Section 6.

Complexity of Stochastic Dual Dynamic Programming

5

2 Preliminary: Kelley’s cutting plane methods

In this section, we brieﬂy review the basic cutting plane method and establish its
complexity bound. Consider the convex programming problem of

f(x),

min
X
x
∈

(2.1)

where X
convex function. Moreover, we assume that f is Lipschitz continuous s.t.

Rn is a convex compact set and f : X

R is a sub-diﬀerentiable

→

⊆

f(x)
|

−

f(y)

M

x

k

y

,
k

∀

−

x, y

∈

X.

| ≤

(2.2)

Algorithm 1 formally describes Kelley’s cutting plane method for solving (2.1).
The essential construct in this algorithm is the cutting plane model f (x), which
always underestimates f(x) for any x
X. Given the current search point xk, this
method ﬁrst updates the model function f and then minimizes it to compute the
new search point xk+1. It terminates if the gap between the upper bound (ubk)
and lower bound (lbk) falls within the prescribed target accuracy ǫ. As a result,
an ǫ-solution ¯x
ǫ will be found whenever the algorithm
stops.

X s.t. f(¯x)

f(x∗)

≤

−

∈

∈

Algorithm 1 Basic cutting plane method
Input: initial points x1 and target accuracy ǫ.
Set f
for k = 1, 2, . . . , do

(x) = −∞ and ub0 = +∞.

0

k

(x) = max{f

(x), f (xk) + hf ′(xk), x − xki}.

Set f
Set xk+1 ∈ Argminx∈X f (x).
Set lbk = f (xk+1) and ubk = min{ubk−1, f (xk+1)}.
if ubk − lbk ≤ ǫ then

k−1

terminate.

end if

end for

We establish the complexity, i.e., the number of iterations required to have a

gap lower than ǫ, of the cutting plane method in Proposition 1.

ǫ/M for any i =
norm and
k · k
Rn is contained in a box with side length bounded by l. Then the complexity of

Proposition 1 Unless Algorithm 1 stops, we have
1, . . . , k. Moreover, suppose that the norm
X
the basic cutting plane method can be bounded by

in (2.2) is given by the l

xk+1 −
k

k ≥

xi

⊂

∞

lM
ǫ + 1

n

.

(2.3)

Proof. Note that f k(x) = maxi=1,...,k f(xi) +
tinuous with constant M. Moreover, we have f k(x)
f(xi) = f k(xi) for any i = 1, . . . , k + 1. Hence,

h

≤

(cid:0)

(cid:1)

f ′(xi), x

xi

i
f(x) for any x

is Lipschitz con-
X and

−

∈

f k(xk+1) = min
X

x

∈

f k(x)

≤

min
X
x
∈

f(x) = f ∗.

6

Guanghui Lan

Using this observation, we have

f(xi)

lbk ≤

ubk −
Since ubk−
from this observation.

−
lbk > ǫ, we must have

lbk = f k(xi)

xi
k

xk+1k

−

−

lbk = f k(xi)

f k(xk+1)

−

≤
> ǫ/M. (2.3) then follows immediately

−

M

xi
k

.
xk+1k

Even though the complexity bound (2.3) of the cutting plane method has
not been explicitly established before, construction of this proof was used in
Ruszczy´nski [25]. Moreover, as pointed out in [20] the exponential dependence
of such complexity bound on the dimension n does not seem to be improvable in
general. It is worth noting that the cutting plane algorithm does not explicitly
depend on the selection of the norm even though the bound in (2.3) is obtained
under the assumption that X sits inside an l

box.

∞

3 Dual dynamic programming for single-scenario problems

In this section, we focus on a dynamic version of the cutting plane method ap-
plied to solve a class of deterministic dynamic convex optimization problems, i.e.,
multi-stage optimization problems with a single scenario. This dual dynamic pro-
gramming (DDP) method, which can be viewed as SDDP with one scenario, will
serve as a starting point for studying the more general dual dynamic programming
methods in later two sections. Moreover, this method may inspire some interests
in its own right.

More speciﬁcally, we consider the following dynamic convex programming

x1∈
where the value functions vt(

X1 {

f ∗ := min

f1(x1) := h1(x1) + λv2(x1)

,

}

), t = 2, . . . , T + 1, are deﬁned recursively by
·

vt(xt

−

1) :=

xt

∈

vT +1(xT )

0,

≡

min
Xt(xt−1) {

ft(xt) := ht(xt) + λvt+1(xt)

,

}

with convex feasible sets Xt(xt

−

1) given by

(3.1)

(3.2)

(3.3)

1 + bt, φt(x)

Qtxt

1

.

(3.4)

Xt(xt

1) :=

x

¯Xt

Rnt : Atx = Btxt
−

−

−

∈

⊆

1, λ

(0, 1] denotes the discounting factor, At : Rnt

≤
Rnt are closed convex sets independent of
(cid:8)
Similarly to problem (1.1), here ¯Xt
Rmt,
xt
∈
and Qt : Rnt−1
R,
i = 1, . . . , pt, are closed convex functions. Thus, we can view problem (3.1) as a
single-scenario multi-stage optimization problem in the form of (1.1), by assuming
, ct) and
ξt = (At, bt, Bt, Qt, pt, ct) to be deterministic, and setting ht(
·
φt(

⊂
→
Rpt are linear mappings, and ht : ¯Xt

→
R and φt,i : ¯Xt

Rmt , Bt : Rnt−1

) = Ht(
·

→

→

→

(cid:9)

−

) = Φt(
·
Throughout this section, we denote

, pt).
·

t the eﬀective feasible region of each

X

period t deﬁned recursively by

t :=

X

X1,

(

x

∪

∈X

t−1Xt(x),

t = 1,
2.
t

≥

(3.5)

Complexity of Stochastic Dual Dynamic Programming

7

Observe that
Conv(
Aﬀ(

X
t) :

y

X

t). Moreover, letting Aﬀ(
, we use
ǫ
}

k ≤

k

X

t is not necessarily convex and its convex hull is denoted by

t) be the aﬃne hull of

X

t and

X

t(ǫ) :=

B

y

{

∈

t(ǫ) :=

t +

X

t(ǫ)

B

X

to denote

t together with its surrounding neighborhood.

X

In order to develop a cutting plane algorithm for solving problem (3.1), we
need to make a few assumptions and discuss a few quantities that characterize the
problem.

Assumption 1 For any t

1, there exists Dt

0 s.t.

≥

≥

xt
k

−

x′t

k ≤

Dt,

xt, x′t

∀

t,

t

∀

≥

1.

∈ X

(3.6)

The quantity Dt provides a bound on the “diameter” of the eﬀective feasible
t. Clearly, Assumption 1 holds if the convex sets ¯Xt are compact, since by

region
deﬁnition we have

X

t

X

⊆

Conv(

t)

¯Xt,

t

1.

X

⊆
1, there exists ¯ǫt

∀

≥

(0, +

) s.t.

∞
t(¯ǫt) and rint(Xt+1(x))

∈

Assumption 2 For any t

≥

ht(x) < +

∞

,

x

∀

∈ X

=

,
∅

x

∀

∈ X

t(¯ǫt),

(3.7)

where rint(

) denotes the relative interior of a convex set.
·

Assumption 2 describes certain regularity conditions of problem (3.1). Speciﬁcally,
the two conditions in (3.7) imply that ht and vt+1 are ﬁnitely valued in
t(ǫt).
The second relation in (3.7) also implies the Slater condition of the feasible sets in
(3.4) and thus the existence of optimal dual solutions to deﬁne the cutting plane
models for problem (3.1). Here the relative interior is required due to the nonlin-
earity of the constraint functions in (3.4) and we can replace rint(Xt+1(x)) with
Xt+1(x) if the latter is polyhedral. Conditions of these types have been referred
to as extended relatively complete recourse, which is less stringent than imposing
complete recourse with ¯ǫ = +

in the second relation in (3.7) (see [8]).

X

∞

In view of Assumption 2, the objective functions ft, as given by the summation
t(¯ǫt). In addition, by Assumptions 1
X
t is bounded. Hence the convex functions ft must be Lipschitz continuous
X
t (see, e.g., Section 2.2.4 of [15]). We explicitly state the Lipschitz constants

of ht and λvt+1, must be ﬁnitely valued in
the set
over
of ft below since they will be used in the convergence analysis our algorithm.

X

Assumption 3 For any t

1, there exists Mt

≥

ft(xt)
|

−

ft(x′t)

Mt

xt
k

−

| ≤

≥
x′t

0 s.t.

,
k

xt, x′t

∀

t.

∈ X

(3.8)

≡

We are now ready to describe a dual dynamic programming method for solv-
ing problem (3.1) (see Algorithm 2). For notational convenience, we assume that
X1(xk
0)

X1 for any iteration k

1.

We now make a few observations about the above DDP method. Firstly, in
T ) sequen-
1 for the ﬁrst stage. In this phase we utilize the cutting plane
) in order to approxi-
·
) at stage t, because we do not have a convenient
·
2, . . . , xk
T ) is a feasible policy

the forward phase our goal is to compute a new policy (xk
tially starting from xk
model vk
1
t+1 (
−
mate the objective function ft(
expression for the value function vt+1(

) as a surrogate for the value function vt+1(
·

2, . . . , xk

1, xk

1, xk

≥

). Since (xk
·

6
8

Guanghui Lan

Algorithm 2 Dual dynamic programming (DDP) for single-scenario problems
T +1 = 0, and ub0
1: Set v0
2: for k = 1, 2, . . . , do
3:

t (x) = −∞, t = 2, . . . , T , v0

t = +∞, t = 1, . . . , T .

for t = 1, 2, . . . , T do

⊲ Forward phase.

xk
t ∈ Argmin

f k−1
t

(x) := ht(x) + λvk−1

t+1 (x) : x ∈ Xt(xk

t−1)

.

(3.9)

4:
5:
6:
7:
8:

end for
Set ubk

1 = min{ubk−1

1

n

,

T

t=1 λt−1ht(xk

t )}.

P
Set vk
for t = T, T − 1, . . . , 2 do

T +1 = 0.

o

⊲ Backward phase.

.

(3.10)

(˜vk

f k
t

t (xk
˜vk
t−1) = min
t )′(xk
t−1) = [Bt, Qt]yk
vk−1
vk
t (x) = max
t

n

(x) := ht(x) + λvk

t+1(x) : x ∈ Xt(xk

t−1)

t , where yk
t (xk
(x), ˜vk

t is the optimal dual multiplier of (3.10).
t−1), x − xk
t−1) + h(˜vk

t )′(xk

t−1i

.

o

(3.11)

end for

9:
10: end for

n

o

T

t=1 λt
by deﬁnition,
of problem (3.1), and accordingly, ubk
policy we found so far.

1ht(xk

P

−

t ) gives us an upper bound on the optimal value f ∗
1 gives us the value associated with the best

−

Secondly, given the new generated policy (xk
1, xk
2, . . . , xk
T ), our goal in the back-
ward phase is to update the cutting plane models vk
) to vk
1
), in order to pro-
t (
(
t
·
·
). More speciﬁcally, by Assumption 2,
vide a possibly tighter approximation of vt(
·
the feasible region of Xt(xk
1) of the subproblem in (3.10) has a nonempty relative
t
−
interior. Hence the function value ˜vk
t are
well-deﬁned, and they deﬁne a supporting hyperplane for the approximate value
function ˜vk
1)).
t (
Using all these supporting hyperplanes of ˜vk
t that have been generated so far, we
R, which underestimates the original
deﬁne a cutting plane model vk
t
value function vt(

) deﬁned in (3.10) (after replacing xk
t
·
−

: Rnt
) as shown in the following result.
·

1) and the associated vector [Bt, Qt]yk

1 with any x

t (xk
t
−

1(¯ǫt

∈ X

t
−

→

−

Lemma 1 For any k

1,

1

vk
t

−

(x)

≤

≥
vk
t (x)
f k
t

−

˜vk
t (x)

≤
1
(x)

≤
f k
t (x)

vt(x),

x

∀
ft(x),

∈ X
x

1(¯ǫt

t
−

1), t = 2, . . . , T,

−
t(¯ǫt), t = 1, . . . , T.

≤

≤

∀

∈ X

(3.12)

(3.13)

1

−

Proof. First observe that the inequalities in (3.13) follow directly from (3.12)
t+1(x) due
t in (3.2) and (3.9), respectively. Moreover, the ﬁrst

by using the facts that ft(x) = ht(x) + λvt+1(x) and f k
to the deﬁnitions of ft and f k
relation vk
(x)
t

vk
t (x) follows directly from (3.11).

t (x) = ht(x) + λvk

Second, we observe that the functions ˜vk

1)
due to Assumption 2 and will show that the remaining inequalities in (3.12), i.e.,
vk
t (x)
1), hold by using induction backwards for
−
t = T, . . . , 1 at any iteration k. Let us ﬁrst consider t = T . Note that vk
T +1 = 0
and thus by comparing the deﬁnitions of vT (x) and ˜vk
T (x) in (3.2) and (3.10), we

t and vt are well-deﬁned over

˜vk
t (x)

vt(x),

∈ X

1(¯ǫt

1(¯ǫt

t
−

t
−

≤

≤

≤

X

x

∀

−

Complexity of Stochastic Dual Dynamic Programming

9

T (x) = vT (x). Moreover, by deﬁnition ˜vk

have ˜vk
is a supporting hyperplane of ˜vk
the deﬁnition of vk
T (x), we have

T (x) at xk
T

−

T (xk
T

T )′(xk
T
1i
1. Combining these observations with

1) +

1), x

(˜vk

xk
T

−

−

−

−

h

h
˜vk
t (x)

vk
T (x)

T (xk
˜vk
T

−

≤

1) +

(˜vk

T )′(xk
T

1), x

−

−

xk
T

1i ≤

−

˜vk
T (x) = vT (x).

(3.14)

≤

≤

vt(x) for some 0
vt(x) in the the deﬁnitions of vt
vt

t (x)

t (x)

Now assume that vk
hypothesis of vk
(3.10), we conclude that ˜vk
1(x)
t
−
is a subgradient of ˜vk
) at xk
t
t
·
−
−
1)′(xk
1(xk
t
t
−
−

1(x) = ˜vk
t
−

(˜vk
t
−

2) +

vk
t
−

1(

≤

−

h

≤

t
≤
1(x) and ˜vk
t
−
−

T . Using the induction
1(x) in (3.2) and
2)

1(x). Moreover, by deﬁnition (˜vk
t
≤
−
2. Combining these relations, we conclude

1)′(xk
t
−

2), x

xk
t
−

−

2i ≤

˜vk
t
−

1(x)

vt

−

≤

1(x).

(3.15)

In order to establish the complexity of Algorithm 2, we need to show that the

approximation functions f k
t (

) are Lipschitz continuous on
·

t.

X

Lemma 2 For any t

≥
f k
t (xt)
|

−

1, there exists M t ≥
f k
t (x′t)

xt

M tk

| ≤

−

0 s.t.

x′t

,
k

∀

xt, x′t

t

∈ X

∀

k

≥

1.

(3.16)

Proof. Note that by Assumption 2, for any x

t(¯ǫ), the feasible region of
Xt+1(x) has a nonempty relative interior, hence for any i = 1, . . . , k, the func-
tion values ˜vi
t) and the associated vectors [Bt+1, Qt+1]yi
t+1 are well-deﬁned.
Therefore, the piecewise linear function vk

t+1(xi

∈ X

t+1(x) given by

vk
t+1(x) = max
i=1,...,k

t+1(xi
˜vi

t) +

h

[Bt+1, Qt+1]yi

t+1, x

xi
t

i

−

is well-deﬁned and sub-diﬀerentiable. This observation, in view of the convexity of
ht and Assumption 2, then implies that f k
t (x) is sub-diﬀerentiable
on
t. Note that for
t )′ on
X
any x

t (x) = ht(x)+λvk
t. We now provide a bound for the subgradients (f k

t, we have

t(¯ǫ) and x0

X

∈ X

∈ X
t )′(x0), x

(f k

h

x0

−

i ≤

f k
t (x)

−

f k
t (x0)

≤

f(x)

−

f 1
t (x0),

(3.17)

where the last inequality follows from (3.13). Letting
notes the conjugate norm of
have

k · k∗
and setting x = x0 + ¯ǫ(f k

k · k

:= max
, x
x
1h·
k
k≤
(f k
t )′(x0)
t )′(x0)/
k

k∗

de-
i
, we

(f k
¯ǫ
k

t )′(x0)

k∗ ≤

f(x)

f 1
t (x0)

−

≤

max
(¯ǫ)
x
∈X

f(x)

−

min
x
∈X

f 1
t (x),

which implies that

(f k
k

t )′(x0)

k∗ ≤

1
¯ǫ [ max
t(¯ǫ)
x
∈X

f(x)

−

min
x
t
∈X

f 1
t (x)],

x0

∀

t.

∈ X

The result in (3.16) then follows directly from the above inequality, the bounded-
ness of

(¯ǫ), and the fact that

t and hence

X

X
f k
t (x′t)

f k
t (xt)
|

−

max

(f k

t )′(xt)

,

k∗

(f k
k

t )′(x′t)

xt

k∗}k

x′t

k

−

{k

| ≤

10

Guanghui Lan

due to the convexity of f t and the Cauchy Schwarz inequality.

We now add some discussions about the Lipschitz continuity of f k

t obtained in
Lemma 2. Firstly, it might be interesting to establish some relationship between
the Lipschitz constants M t and Mt for f k
t and ft, respectively. Under certain
circumstances we can provide such a relationship. In particular, let us suppose
that

f k
t (x0)
It then follows from the above assumption and (3.17) that

f k
t (x0) + ¯ǫ.

ft(x0)

≤

≤

(3.18)

(f k

t )′(x0), x

h

x0

−

i ≤

f(x)

−

f(x0) + ¯ǫ.

Setting x = x0 + ¯ǫf ′(x0)/
f ′(x0)
k

k∗

, we conclude

(f k
¯ǫ
k

t )′(x0)

k∗ ≤

f(x)

−

f(x0) + ¯ǫ

M

x

k

−

≤

x0

k

+ ¯ǫ

≤

¯ǫM + ¯ǫ,

which implies that

f ′(x0)
k

k∗ ≤

Mt + 1 and M t ≤

Mt + 1.

(3.19)

Note however that the above relationship does not necessarily hold for a situation
more general than (3.18).

Secondly, while it is relatively easy to understand how the discounting factor
λ impacts the Lipschitz constants Mt for the objective functions ft over diﬀerent
stages, its impact on the Lipschitz constants M t for the approximation functions
f k
is more complicated since we do not know how the Lagrange multipliers yk
t
t
changes w.r.t. λ. On the other hand, the discounting factor does play a role in
compensating the approximation errors accumulated over diﬀerent stages for the
DDP method. Since we cannot quantify precisely such a compensation by simply
scaling the Lipschitz constants Mt and M t, we decide to incorporate explicitly the
discounting factor λ into our problem formulation, as well as the analysis of our
algorithms. We will see that to incorporate λ just makes some calculations, but
not the major development of the analysis, more complicated. One can certainly
assume that λ = 1 in order to see the basic idea of our convergence analysis.

In order to establish the complexity of DDP, we need to introduce an important

notion as follows.

Deﬁnition 1 We say that a search point xk

t gets ǫt-saturated at iteration k if

vt+1(xk
t )

t+1(xk
vk
t )

−

ǫt.

≤

(3.20)

In view of the above deﬁnition and (3.12), for any ǫt-saturated point xk

t we

must have

t+1(xk
vk
t )

≤

vt+1(xk
t )

≤

t+1(xk
vk

t ) + ǫt.

(3.21)

In other words, vk
by ǫt. By (3.12), we also have vk

t+1 will be a tight approximation of vt+1 at xk
t+1(xk
t ) for any k′
t )

vk′
t+1(xk

t with error bounded
k, and hence

vt+1(xk
t )

vk′
t+1(xk
t )

−

≤

≤
vt+1(xk
t )

≥

t+1(xk
vk
t )

−

ǫt.

≤

Complexity of Stochastic Dual Dynamic Programming

11

This implies that once a point xk
functions vk′
by ǫt for any iteration k′

k.

t+1 will also be a tight approximation of vt+1 at xk

t becomes ǫt-saturated at the k-th iteration, the
t with error bounded

≥

Below we describe some basic properties about the saturation of the search

points.

Lemma 3 Any search point xk
T
saturated for any k

1.

≥

1 generated for the (T

−

−

1)-th stage must be 0-

Proof. Note that by (3.12), we have vk

T (xk
T

1)

v(xk
T

1). Moreover, by (3.11),

−

T (xk
vk
T

1)

−

≥

˜vk
T (xk
T

−

≤
−
1) = v(xk
T

1)

−

where the last equality follows from the fact that vk
vT (x) and ˜vk
which, in view of (3.20), implies that xk
T

T (x) in (3.2) and (3.10). Therefore we must have vk

1 is 0-saturated.

T +1 = 0 and the deﬁnitions of
1),

1) = v(xk
T

T (xk
T

−

−

−

We now state a crucial observation for DDP that relates the saturation of
search points across two consecutive stages. More speciﬁcally, the following result
shows that if one search point xj
t at stage t has been ǫt-saturated at iteration j,
and a new search point generated at a later iteration k is close to xj
t , then a search
point in the previous stage t
1-saturated with an appropriately
chosen value for ǫt

1 will get ǫt

−

−

1.

−

Proposition 2 Suppose that the search point xk
enough to xj
t generated in a previous iteration 1

j < k, i.e.,

≤

t generated at the k-th iteration is close

xk
t
k

xj
t k ≤

δt

−
). Also assume that the search point xj

t is ǫt-saturated, i.e.,

vt+1(xj
t )

t+1(xj
vj
t )

−

ǫt.

≤

for some δt

[0, +

∞

∈

Then we have

ft(xk
t )

f k
t

1

−

−

(xk

t ) = λ[vt+1(xk
t )
ǫt

≤

−

1

t+1 (xk
vk
−

t )]

−

1 := (Mt + M t)δt + λǫt.

In addition, for any t

2, we have

≥

vt(xk
t
−

1)

−

t (xk
vk
t
−

1)

ǫt

−

1

≤

and hence the search point xk
t
−

1 will get ǫt

−

1-saturated at iteration k.

Proof. By the deﬁnitions of ft and f k
t

1

−

in (3.2) and (3.9) , we have

ft(x)

f k
t

1

−

−

(x) = λ[vt+1(x)

vk
1
t+1 (x)],
−

x

∀

∈

Xt(xk
t
−

1)

−

(3.22)

(3.23)

(3.24)

(3.25)

12

Guanghui Lan

and hence ﬁrst identity in (3.24) holds. It follows from the deﬁnition of xk
and the ﬁrst relation in (3.13) that

t in (3.9)

ft(xk
t )

min
Xt(xk

t−1)

−

x

∈

f k
t

1

(x) = ft(xk
t )

−

ft(xk
t )

≤

f k
t

1

(xk
t )

−

t (xk
f j
t ).

−

−

(3.26)

Now by (3.8) and (3.16), we have

ft(xk
t )
|

−

ft(xj
t )

k ≤

Mt

xk
t
k

xj
t k

−

and

t (xk
f j
t )
|

−

t (xj
f j
t )

| ≤

xk
t

M tk

xj
.
t k

−

In addition, by (3.23) and the deﬁnition ft and f j

t , we have

ft(xj
t )

−

t (xj
f j

t ) = λ[vt+1(xj
t )

vj
t+1(xj

t )]

−

λǫt.

≤

Combining the previous observations and (3.22), we have

ft(xk
t )

f k
t

1

(xk
t )

−

−

≤

≤

≤

−

[ft(xk
t )

ft(xj
xk
(Mt + M t)
t
k
(Mt + M t)δt + λǫt = ǫt

t )] + [ft(xj
t )
xj
+ λǫt
t k

−

1,

−

−

t (xj
f j

t )] + [f j

t (xj
t )

t (xk
f j
t )]

−

(3.27)

where the last equality follows from the deﬁnition of ǫt
shown the inequality in (3.24).

−

1 in (3.24). Thus we have

We will now show that the search point xt
−
k

1
1-saturated at iteration k. Note that xk

also be ǫt
−
stage problem and hence that the function value ft(xk
optimal value vt(xk
1). Using this observation, we have
t
−

1 must
in the preceding stage t
t is a feasible solution for the t-th
t ) must be greater than the

−

vt(xk
t
−

t (xk
vk
t
−
Moreover, using the deﬁnitions of ˜vk
t (xk
t
−
relations in (3.12) and the fact that vk
t+1(x)

1)

1)

−

ft(xk
t )

t (xk
vk
t
−

1).

−
≤
t (xk
1) and vk
t
−
vk
1
t+1 (x) due to (3.13), we have
−

1) in (3.10) and (3.11), the

(3.28)

t (xk
vk
t
−

1), ˜vk

t (xk
t
−

1)

}

≥
(xk
t
−

1

−

vk
1) = max
t
{
t (xk
= ˜vk
1)
t
−
f k
t (x) : x
n
f k
1
t
n
(xk
1

= min

≥
= f k
t

min

t ),

−

−

∈

(x) : x

1)

Xt(xk
t
−
o
Xt(xk
1)
t
−

∈

o

(3.29)

where the last identity follows from the deﬁnition of xk
(3.28) and (3.29), we have

t in (3.9). Putting together

vt(xk
t
−

1)

t (xk
vk
t
−

1)

−

ft(xk
t )

f k
t

1

(xk
t )

−

−

1,

ǫt

−

≤

≤

(3.30)

Complexity of Stochastic Dual Dynamic Programming

13

where the last inequality follows from (3.27). The above inequality then implies
that xk
t
−

1-saturated at the k-th iteration.

1 gets ǫt

−

) are not directly computable since they depend
Observe that the functions ft(
·
). The following result relates the notion of
on the exact value functions vt+1(
·
t ) and
saturation to the gap between a computable upper bound
the lower bound f k
1) on the optimal value f ∗, under the assumption that the
1
concluding inequality (3.24) obtained in Proposition 2 holds for all the stages, i.e.,
λ[vt+1(xk
t )

t = 1, . . . , T .

t=1 λt

1ht(xk

(xk

vk
t+1 (xk
−

t )]

P

1,

ǫt

−

−

T

1

1

−

≤

−

∀

Lemma 4 Suppose that at some iteration k

1, we have

λ[vt+1(xk
t )

−

for any t = 1, . . . , T . Then we have

≥
vk
t+1 (xk
1
−

t )]

1,

ǫt

−

≤

T

t=1 λt

−

1ht(xk
t )

f k
1

1

(xk
1)

−

−

≤

T

t=1 λt

−

P
Proof. By the deﬁnition of f k
1

1

−

(xk

1) in (3.9), we have

P

(3.31)

(3.32)

1ǫt

−

1.

f k
1

1

−

(xk

1) = h1(xk

1) + λvk
2

1

(xk

1),

−

which together with our assumption in (3.31) imply that

h1(xk

1) + λvk
2

1

(xk
1)

−

f k
1

−

1

(xk

1) = λ[vk
2

1

(xk
1)

−

−

vk
2

1

(xk

1)]

−

−

ǫ0.

≤

(3.33)

Moreover, it follows from (3.9) and (3.13) that

ht(xk

t ) + λvk

1

t+1 (xk
−

t ) = min

f k
t

1

−

(x) : x

which, in view of our assumption

min

≤

n

f(x) : x

n

∈

Xt(xk
t
−

∈
Xt(xk
t
−

1)

o

1)

o
= vt(xk
t
−

1),

λ[vt+1(xk
t )

−

then implies that

1

vk
t+1 (xk
−

t )]

1,

ǫt

−

≤

ht(xk

t ) + λvt+1(xk
t )

vt(xk
t
−

≤

1) + ǫt

−

1

(3.34)

for any t = 2, . . . , T . Multiplying λt
ming them up with the inequalities in (3.33), and using the fact that vT +1(xk
we have

1 to both side of the above inequalities, sum-
T ) = 0,

−

T

t=1 λt

−

1ht(xk
t )

f k
1

1

(xk
1)

−

−

≤

T

t=1 λt

−

1ǫt

−

1.

P

P

In the sequel, we use Sk
t

to denote the set of ǫt-saturated search points at
stage t that have been generated by the algorithm before the k-th iteration. Using
these sets, we now deﬁne the notion of distinguishable search points as follows.

−

1

14

Guanghui Lan

Deﬁnition 2 We say that a search point xk

t at stage t is δt-distinguishable if

t (xk
gk

t ) > δt,

(3.35)

where gk

t (x) denotes the distance between x to the set Sk
t

−

1

given by

gk
t (x) =

mins
∈
0,

(

k−1
t

S

s
k

−

x

,
k

t < T,

o.w.

Below we show that each iteration of the DDP method will either ﬁnd an ǫ0-
solution of problem (3.1), or ﬁnd a new ǫt-saturated and δt-distinguishable search
point at some stage t by properly specifying δt and ǫt for t = 0, . . . , T

1.

−

Proposition 3 Assume that δt
denote

[0, +

∞

∈

) for t = 1, . . . , T are given. Also let us

ǫt :=

0,

(

2

T
τ =t [(Mτ +1 + M τ +1)δτ +1λτ
−

t],

−

t = T
T
t

≤

−

−

1,
2.

(3.36)

Then, every iteration k of the DDP method will either generate a δt-distinguishable
and ǫt-saturated search point xk
t at some stage t = 1, . . . , T , or ﬁnd a feasible policy
(xk

T ) of problem (3.1) such that

1, . . . , xk

P

f1(xk
1)
f ∗
−
(xk
f k
1
1)
1

−

−

≤

≤

1ht(xk
t )

T

t=1 λt

−

ǫ0,
T

t=1 λt

−

1ǫt

−

1.

(3.37)

(3.38)

P

Proof. First note that the deﬁnition of ǫt is computed according to the recursion
1 = (Mt + M t)δt + λǫt (see (3.24)) and the assumption that ǫT
1 = 0. Next,
ǫt
observe that exactly one of the following T cases will happen at the k-th iteration
of the DDP method.

−

−

P

t (xk
t )

Case 1: gk
δt,
Case t, t = 2, . . . , T
Case T : gk
T

≤
1(xk
T

1
t
T
∀
≤
≤
i (xk
1: gk
i )
−
1.
1) > δT

1;
δi,

−
≤

t

∀

≤

i

≤

T

−

1, and gk
t
−

1(xk
t
−

1) > δt

−

1;

−

−

−
We start with the ﬁrst case. In this case, we have gk

t (xk
t )
≤
t must be close to an existing ǫt-saturated point xjt
t

δt,
1
t
∀
for some jt

≤

T
k

≤
≤

−
−

1.
1

Hence, xk
s.t.

xk
t
k

xjt
t k ≤

−

δt,

1

∀

≤

t

≤

T

−

1.

(3.39)

It then follows from the above relation (with t = 1), (3.24), and the fact f ∗
f k
1

1) that

(xk

−

1

≥

f1(xk
1)

f ∗

−

≤

f1(xk
1)

f k
1

−

1

(xk

1) = λ[v2(xk
1)

−

vk
2

1

(xk

1)]

−

−

ǫ0.

≤

Moreover, we conclude from (3.24) and (3.39) that

λ[vt+1(xk
t )

−

1

t+1 (xk
vk
−

t )]

ǫt

−

1,

1

∀

≤

t

≤

T

≤

−

1.

(3.40)

(3.41)

Hence, the assumptions in Lemma 4 hold and the result in (3.38) immediately
follows.

We now examine the t-th case for any 2
1(xk
1 and thus xk
t
t
−
−

1) > δt

1 is δt

−

−

t

2. In these cases, we have
1-distinguishable. In addition, we have

≤

≤

−

T

gk
t
−

Complexity of Stochastic Dual Dynamic Programming

15

δt. As a result, xk
k

gk
t (xk
t )
with jt
t (xk
vk
t
−
For the T -th case, we have gk
T

≤
≤
1)

−
ǫt

≤

−

Also by Lemma 3, xk
T
and ǫT

1-saturated (with ǫT

−

t must be close to an existing ǫt-saturated point xjt
t

1. This observation, in view of (3.25), then implies that vt(xk
t
−
1. Hence xk
1-saturated.
t
−
−
1 is δt-distinguishable.
1-distinguishable

1-distinguishable and ǫt
−
1 and hence xk
1) > δT
T
−
1 is δT

−
1 will get 0-saturated. Therefore, xk
T

1 is both δt
1(xk
T

1)

−

−

−

−

−

Combining all these cases together, we conclude that every DDP iteration will
either generate a δt-distinguishable and ǫt-saturated search point at some stage
t = 1, . . . , T , or ﬁnd a feasible policy of problem (3.1) satisfying (3.37) and (3.38).

1 = 0).

−

−

It is worth noting that each DDP iteration can possibly generate more than
one δt-distinguishable and ǫt-saturated points. For example, for the t-case in the
the above proof of Proposition 3, we pointed out that xk
1-distinguishable
t
−
−
1-saturated. Some other search point xk
and ǫt
2 in the preceding
i with i
≤
stages might also become δi-distinguishable and ǫi-saturated even though there
are no such guarantees.

1 is δt
t

−

−

We are now ready to establish the complexity of the DDP method. For the sake
of simplicity, we will ﬁx the norm
norm to deﬁne the distances and
to be an l
Lipschitz constants at each stage t. It should be noted, however, that the DDP
norm is
method itself does not really depend on the selection of norms. The l
chosen because it will help us to count the number of search points needed in each
stage to guarantee the convergence of the algorithm.

k · k

∞

∞

Theorem 1 Suppose that the norm used to deﬁne the bound on Dt in (3.6) is the l
∞
) are given and that ǫt are deﬁned in (3.36). Then
norm. Also assume that δt
the number of iterations performed by the DDP method to ﬁnd a solution satisfying
(3.37) and (3.38) can be bounded by

[0, +

∞

∈

T
1
−
t=1

Dt
δt + 1

nt

+ 1.

(3.42)

(cid:16)
P
D, max
In particular, If nt
n, Dt
Mt, M t} ≤
{
then the DDP method will ﬁnd a feasible policy (xk

≤

≤

(cid:17)

M and δt = ǫ for all t = 1, . . . , T ,
1, . . . , xk
T ) of problem (3.1) s.t.

T

t=1 λt

−

1ht(xk
t )

f1(xk
1)
f k
1

−

−

f ∗
−
(xk
1
1)

2M min
{
2M min
{

≤

≤

P
within at most

iterations.

1)

D
ǫ + 1

n

+ 1

(T

−

(cid:0)

(cid:1)

1

−

1

(1

−

ǫ,

1
λ , T
−
}
λ)2 , T (T
1
−
2

1)

ǫ

}

(3.43)

(3.44)

(3.45)

Proof. Let us count the total number of possible search points for saturation
before a solution satisfying (3.37) and (3.38) is found. Using (3.35) and the as-
sumption the eﬀective feasible region for each stage t is inside a box with side
length Dt (c.f., (3.6)), we can see that the number of possible δt-distingushable
search points for saturation at each stage is given by

Nt :=

Dt
δt + 1

nt

.

(cid:16)

(cid:17)

16

Guanghui Lan

This observation together with Proposition 3 then imply that the total number
T
of iterations performed by DDP will be bounded by
t=1 Nt + 1 and hence by
−
(3.42).

1

Now suppose that nt

n, Dt

≤

D, max
Mt, M t} ≤
{

≤

P

t = 1, . . . , T . We ﬁrst provide a bound on ǫt deﬁned in (3.36). For 0
we have

M and δt = ǫ for all
2,

T

t

≤

≤

−

ǫt =

T

2

τ =t λτ

−

t[(Mτ +1 + M τ +1)δτ +1]

−
T
τ =t λτ
−

2

= 2M
P

tǫ
−
λT −t−1
λ
1
−
λ , T

−

P

2M min
{
2M min
{

≤

≤

1

1

−

1

−

and as a result,

, T

t

−

t

−
1

−
ǫ,

}

1

ǫ
}

(3.46)

T
t=1 λt
−

1ǫt

−

1 =

P

≤

≤

2

T
t=0 λtǫt
−
t
λ , T

2M
P

2M

P

T
t=1 λt
−
T

−

−

1ǫt
1 =
2
t=0 [λt min
1
P
1
{
−
λt
T
2
t=0 min
λ , T
−
1
{
λ)2 , T (T

−
2

−

1

−
1)

1

]ǫ
}

−
t

−

−
1

ǫ
}

2M min
{
Using these bounds in (3.37) and (3.38), we obtain relations (3.43) and (3.44).
Moreover, the iteration complexity bound in (3.45) follows directly from (3.42).

(3.47)

P

≤

ǫ.

(1

−

}

X

We now add some remarks about the results obtained in Theorem 1.
Firstly, similar to the basic cutting plane method, the bound in (3.42) has an
exponential dependence on nt. However, since the algorithm itself does not require
us to explicitly discretize the decision variables in Rnt , the complexity bound
actually depends on the dimension of the aﬃne space spanned by eﬀective feasible
region

t deﬁned in (3.5), which can be smaller than the nominal dimension nt.

−

≤

f ∗

Secondly, it is interesting to examine the dependence of the complexity bound
in (3.45) on the number of stages T . In particular, if the discounting factor λ < 1,
the number of iterations required to ﬁnd an ǫ-solution of problem (3.1), i.e., a
point ¯x1 s.t. f1(¯x1)
ǫ only linearly depends on T . When the discounting
factor λ = 1, we can see that T also appears in the termination criterions (3.43)
and (3.44). As a result, the number of iterations required to ﬁnd an ǫ-solution of
problem(3.1) will depend on T n. The discounting factor provides a mechanism to
compensate the errors accumulated from approximating the value function vt+1
by vk

t+1 starting from t = T
Thirdly, while the termination criterion in (3.43) cannot be veriﬁed since the
function value f1 and f ∗ are not easily computable, the gap between the upper
and lower bound in the l.h.s. of (3.44) can be computed as we run the algorithm.
It should be noted that the dependence on T for these two criterions are slightly
diﬀerent especially when the discounting factor λ = 1 (see the r.h.s. of (3.43) and
(3.44)).

1 to t = 1.

−

4 Explorative dual dynamic programming

In this section, we generalize the DDP method for solving the multi-stage stochas-
tic optimization problems which have potentially an exponential number of scenar-

Complexity of Stochastic Dual Dynamic Programming

17

ios. As discussed in Section 1, we assume that we can sample from the probability
distribution Pt of the random vector ξt, t = 2, . . . , T . A sample average approxi-
mation (SAA) of the original problem (1.1) is constructed by replacing the true
distribution of ξt = (At, bt, Bt, Qt, pt, ct) with the empirical distribution PNt based
on a random sample

˜ξti = ( ˜Ati, ˜bti, ˜Bti, ˜Qti, ˜pti, ˜cti), i = 1, . . . , Nt

from the distribution Pt of size Nt. Consequently the probability distribution P2

PT of the random process ξ2, . . . , ξT is replaced by PN2 × · · · ×

×
PNT . Under
· · · ×
the stage-wise independence assumption of Pt and hence PNT , it has been shown
in [27] that under mild regularity assumptions we can approximate problem (1.4)
by the SAA problem deﬁned as

F ∗ := minx1∈

X1 {

F11(x1) := H1(x1, c1) + λV2(x1)

,
}

where the value factions Vt, t = 2, . . . , T , are recursively deﬁned by

Vt(xt
−
νti(xt

1) := 1
Nt
1) := minxt
P

−

Nt
i=1 νti(xt
1),
Xt(xt−1, ˜ξti){

−

∈

Fti(xt) := Ht(xt, ˜cti) + λVt+1(xt)

and

VT +1(xT ) = 0.

(4.1)

(4.2)

(4.3)

,
}

We will focus on how to solve the SAA problem in (4.1). The essential diﬀerence
between this problem and the single-scenario problem in (3.1) is that each stage t
involves Nt (rather than one) subproblems. As a consequence, when determining
the search point xk
t at each stage t in the forward phase, we need to choose one
out of Nt feasible solutions and each one of them corresponds to a realization ˜ξti of
the random variables. In this section, we will present a deterministic dual dynamic
programming method which chooses the feasible solution in the forward phase in
an aggressive manner, while in next section, we will discuss a stochastic approach
in which the feasible solution in the forward phase will be chosen randomly. As
we will see, the former approach will exhibit better iteration complexity while the
latter one is easier to implement. We start with the deterministic approach also
because the analysis for the latter stochastic method is built on the one for the
deterministic approach.

Let

ti be the eﬀective feasible region for the i-th subproblem in stage t, and
¯
t be the eﬀective feasible region all the subproblems in stage t, respectively, given
X
by

X

ti :=

X

X1,

(

∪x

∈

¯
X

t−1

Xt(x, ˜ξti),

t = 1,
2,
t

≥

and

Observe that ¯
X
hull of ¯
B
X

t and

X1,

¯
t :=
X

t = 1,
2.
t

(

i=1,...,Nt X
∪
t is not necessarily convex. Moreover, letting Aﬀ( ¯
X
t(ǫ) :=

≥

y

ti,

{

∈

y

Aﬀ( ¯
t) :
X
k
k ≤
t(ǫ) := ¯
¯
X
X

, we use
ǫ
}
t +

t(ǫ)

B

to denote ¯
X

t together with its small surrounding neighborhood.
We make the following assumptions throughout this section.

t) be the aﬃne

18

Guanghui Lan

Assumption 4 For any t

1, there exists Dt

0 s.t.

≥

≥

xt
k

−

x′t

k ≤

Dt,

xt, x′t

∀

¯
t,
X

∈

t

∀

≥

1.

(4.4)

With a little abuse of notation, we still use Dt as in the previous section
t. Clearly, Assump-

to bound the “diameter” of the eﬀective feasible region ¯
X
tion 4 holds if the convex sets ¯Xt are compact, since by deﬁnition we have ¯
t
X
Conv( ¯
X

¯Xt,

1.

t)

⊆

⊆

≥

∀

t

Assumption 5 For any t

1, there exists ¯ǫt

(0, +

) s.t.

≥
Ht(x, ˜cti) < +

,

∀

∞

=

,
∅

x

∀

∈

∈
x

∞
¯
t(¯ǫt),
∈
X
¯
t(¯ǫt),
X

∀

rint

Xt+1(x, ˜ξ(t+1)i)

i = 1, . . . , Nt,

∀

i = 1, . . . , Nt+1,

(4.5)

(4.6)

where rint(

(cid:1)
) denotes the relative interior of a convex set.
·

(cid:0)

Assumption 5 describes certain regularity conditions of problem (4.1). Specif-
ically, the conditions in (4.5) and (4.6) imply that Ht(x, ˜cti) and Vt+1 are ﬁnitely
ti. The second relation in (3.7) also implies
valued in a small neighborhood of
the Slater condition of the feasible sets in (4.2) and thus the existence of opti-
mal dual solutions to deﬁne the cutting plane models for problem (4.1). Here the
relative interior is required due to the nonlinearity of the constraint functions in
with Xt+1(x, ˜ξ(t+1)i) if the latter
(4.2) and we can replace rint
is polyhedral.

Xt+1(x, ˜ξ(t+1)i)

X

(cid:0)

(cid:1)

In view of Assumption 5, the objective functions Fti must be Lipschitz continu-
ous over
ti(¯ǫt). We explicitly state the Lipschitz constants of Fti below since they
will be used in the convergence analysis our algorithms. For the sake of notation
convenience, we still use Mt to denote the Lipschitz constants for Fti.

X

Assumption 6 For any t

≥

1 and i = 1, . . . , Nt, there exists Mt

Fti(xt)
|

−

Fti(x′t)

Mt

xt
k

x′t

,
k

−

| ≤

xt, x′t

∀

∈ X

0 s.t.

≥
ti.

(4.7)

We now formally state the explorative dual dynamic programming (EDDP)
method as shown in Algorithm 3. A distinctive feature of EDDP is that it maintains
a set of saturated search points Sk
t for each stage t. Similar to Deﬁnition 1, we say
that a search point xk
t generated by the EDDP method is ǫt-saturated at iteration
k if

Vt+1(xk
t )
Moreover, similar to Deﬁnition 2, we say an ǫt-saturated search point xk
t is δt-distinguishable if

t+1(xk
t )

V k

ǫt.

≤

−

(4.14)

t at stage

xk
t
k

xj
t k

> δt

−
for all other ǫt-saturated search points xj
by the algorithm. Equivalently, an ǫt-saturated search point xk
if

t that have been generated for stage t so far
t is δt-distinguishable

gk
t (xk

t ) > δt.

(4.15)

t (xk

t ) (c.f., (4.9)) denotes the distance between xk

Here gk
, i.e., the
set of currently saturated search points in stage t. Similar to the DDP method, sat-
. More precisely, the
uration is deﬁned for two given related sequences
}

t to the set Sk
t

δt
{

ǫt
{

and

−

}

1

6
Complexity of Stochastic Dual Dynamic Programming

19

t (x) = −∞, t = 2, . . . , T , V k

Algorithm 3 Explorative dual dynamic programming (EDDP)
1: Set V 0
2: for k = 1, 2, . . . , do
3:
4:

T +1(x) = 0, k ≥ 1, and S0

for i = 1, 2, . . . , Nt do

for t = 1, . . . , T do

t = ∅, t = 1, . . . , T .

⊲ Forward phase.

˜xk
ti ∈ Argminx∈Xt(xk

F k−1
ti

(x) := Ht(x, ˜cti) + λV k−1

t+1 (x)

.

gk
t (˜xk

ti) =

mins∈S
0,

(

k−1
t

t−1, ˜ξti)
ks − ˜xk

n
tik,

t < T,

o.w.

o

(4.8)

(4.9)

end for
Choose xk

t from {˜xk

ti} such that gk

t (xk

t ) = max

i=1,...,Nt

end for

if gk

1 (xk

1 ) ≤ δ0 then Terminate.

for t = T, T − 1, . . . , 2 do

if gk

t (xk
Set Sk

t ) ≤ δt then
t−1 = Sk−1

end if
for i = 1, . . . , Nt do

t−1 ∪ {xk

t−1}.

t (˜xk
gk

ti).

⊲ Backward phase.

ti(xk
˜νk

t−1) =

min
x∈Xt(xk

t−1, ˜ξti)

(˜νk

ti)′(xk

t−1) = [ ˜Bti, ˜Qti]yk

n
ti, where yk
dual multipliers of (4.10).

ti is the optimal

F k

ti(x) := Ht(x, ˜cti) + λV k

t+1(x)

end for

˜V k
t = 1
Nt

Nt
j=1 ˜νk

tj(xk
V k−1
t

t−1), ( ˜V k
(x), ˜V k

t )′ = 1
Nt
t + h( ˜V k

t )′, x − xk
P

t−1i

.

Nt
j=1(˜νk

tj)′(xk

t−1).

V k

t (x) = max

P

o

.

(4.10)

(4.11)

(4.12)

(4.13)

5:
6:

7:

8:

9:
10:
11:
12:
13:

14:
15:

end for

16:
17: end for

n

o

proposed algorithm takes
from

δt
{
) saturated points.
}

δt
{

as an initial argument and ends with

(derived

ǫt
{

}

}

t (˜xk

In the forward phase of EDDP, for each stage t, we solve Nt subproblems
as shown in (4.8) to compute the search points ˜xk
ti, i = 1, . . . , Nt. For each ˜xk
ti,
we further compute the quantity gk
ti) in (4.9), i.e., the distance between ˜xk
ti
and the set Sk
1
of currently saturated search points in stage t. Then we will
−
t
choose from ˜xk
ti, i = 1, . . . , Nt, the one with the largest value of gk
ti) as xk
t ,
i.e., gk
t (xk
ti). We can break the ties arbitrarily (or randomly
to be consistent with the algorithm in the next section). The search point xk
t is
deemed to be saturated if gk
t ) is small enough, therefore so is the case for ˜xk
ti
for all i. As a consequence, the point xk
1 must also be saturated and can be
t
−
added to Sk
1. We call the sequence (xk
1, . . . , xk
T ) a forward path at iteration k,
t
−
since it is the trajectory generated in the forward phase for one particular scenario

t ) = maxi=1,...,Nt gk

t (xk

t (˜xk

t (˜xk

20

Guanghui Lan

of the data process ˜ξti. In view of the above discussion, the EDDP method always
chooses the most “distinguishable” forward path to encourage exploration in an
aggressive manner (See Line 6 of Algorithm 3). This also explains the origin of the
name EDDP.

The backward phase of EDDP is similar to the DDP in Algorithm 2 with
the following diﬀerences. First, we need to update the set Sk
for the saturated
t
search points. Second, the computation of the cutting plane model also requires
the solutions of Nt subproblems in (4.10).

The following result is similar to Lemma 1 for the DDP method.

Lemma 5 For any k

1,

V k
t

−

1

(x)

V k
t (x)
1

≤

≤
F k
ti

≥
1
Nt
F k

Nt

j=1 ˜νk

tj(x)

Vt(x),

x

∀

∈

¯
t
X
−

1(¯ǫt

−

1), t = 2, . . . , T, (4.16)

−

(x)

¯
t(¯ǫt), t = 1, . . . , T, i = 1, . . . , Nt. (4.17)
X
Proof. The proof is similar to that of Lemma 1. The major diﬀerence exists in

Fti(x),

ti(x)
P

≤

≤

∈

∀

≤
x

that (3.15) will be replaced by

V k
t
−

1(x) = 1

Nt−1

1
Nt−1

P

≤
= Vt

−

˜ν1
(t

Nt−1
j=1
h
Nt−1
j=1 ˜νk
(t

1)j(xk
t
−

−
1)j(x)

≤

−

2) +

h

1
Nt−1

(˜νk
(t
−
Nt−1
j=1 νk
(t

1)j)′(xk
t
−
1)j(x)

2), x

−

xk
t
−

2i

−

i

1(x),
P

P

and hence we skip the details.

In order to establish the complexity of the EDDP Algorithm, we need to show
that the approximation functions F k
ti. For con-
) are Lipschitz continuous on
·
venience, we still use M t to denote the Lipschitz constants for F k
ti. We skip its
proof since it is similar to that of Lemma 2 after replacing Assumption 2 with
Assumption 5.

ti(

X

Lemma 6 For any t

≥
F k
ti(x′t)

1 and i = 1, . . . , Nt, there exists M t ≥
¯
t(¯ǫt)
,
X
k

M tk

xt, x′t

| ≤

x′t

xt

−

∈

∀

∀

0 s.t.

F k
|

ti(xt)

−

1.

k

≥

(4.18)

Below we describe some basic properties about the saturation of search points.

Lemma 7 Any search point xk
T
be 0-saturated for any k

1.

≥

1 generated for the (T

−

−

1)-th stage in EDDP must

Proof. Note that by (4.16), we have V k

T (xk
T

1)

−

≤

V (xk
T

−

1). Moreover, by (4.13),

Nt

V k

T (xk
T

1)

−

≥

1
NT

tj(xk
˜νk
t
−

1) +

(˜νk

tj)′(xk
t
−

h

1), ˜ξtj), xk
T

1 −

−

xk
t
−

1i

j=1 h
X
Nt
j=1 ˜νk
1)

= 1
NT
= V (xk
P
T
−

tj(xk
t
−

1) = 1
NT

Nt
j=1 νtj(xk
t
−

1)

P

i

where the second-to-last equality follows from the fact that vk
deﬁnitions of νT j(x) and ˜νk

T +1 = 0 and the
T j(x) in (4.2) and (4.10). Therefore we must have

Complexity of Stochastic Dual Dynamic Programming

21

V k

T (xk
T

−

1) = V (xk
T

−

1), which, in view of (4.14), implies that xk

T is 0-saturated.

We now generalize the result in Proposition 2 for the DDP method to relate
the saturation of search points across two consecutive stages in the EDDP method.

Proposition 4 Assume that δt
∈
deﬁned recursively according to (3.24) for some given ǫT
deﬁned in (4.9) and assume that xk

) for t = 1, . . . , T are given and that ǫt are
) be
·

1 > 0. Also let gk
t (

t is chosen such that

[0, +

∞

−

t (xk
gk

t ) = max

i=1,...,Nt

t (˜xk
gk

ti).

a) If gk

t (xk
t )

≤

δt, t = 2, . . . , T

1, then we have

−
(˜xk
1

F k
ti

−

Fti(˜xk
ti)

Moreover, for any T

−

≥

ti) = λ[Vt+1(˜xk
ti)

1

V k

t+1 (˜xk
−

ti)]

−

1.

ǫt

−

≤

(4.19)

2, we have

Vt(xk
t
−

1)

V k

t (xk
t
−

1)

−

1.

ǫt

−

≤

(4.20)

where ǫt

1 is deﬁned (3.24).

b) Sk

−
t , t = 1, . . . , T

−

by the algorithm up to the k-th iteration.

1, contains all the ǫt-saturated search points at stage t generated

Proof. We prove the results by induction. First note that by (4.9) we have
T ) = 0. Moreover, by Lemma 7, any search point xk
1 will be 0-saturated
T
1. Moreover, in view of Line 11
1 contains all the 0-saturated search

−

−

1 = 0 for t = T
−
T (xk
T ) = 0, Sk
T
1 and hence part b) holds for t = T
δt for the t-th stage for some t

−

gk
T (xk
and hence part a) holds with ǫT
of Algorithm 3 and the fact gk
point obtained for stage T
−
Now assume that gk
t (xk
t )

−
T

1.

1. In view of

−

≤

this assumption and the deﬁnition of xk

t , we have

≤

t (˜xk
gk

ti) = min
k−1
s
t

S

∈

s
k

−

˜xk
tik ≤

δt

for any i = 1, . . . , Nt. Note that we must have Sk
t
+

. Hence, there exists xji

−
for some ji < k

−

1

1

Sk
t

t ∈

∞

Vt+1(xji
t )

xji
˜xk
tik ≤
t −
k
V ji
t+1(xji
t )

≤

−

since otherwise gk

t (˜xk

ti) =

∅
1 such that

=

−

δt,

ǫt,

(4.21)

(4.22)

for any t = 1, . . . , Nt.

Observe that by the deﬁnition fo xk

ti in (4.8) and the ﬁrst relation in (4.17),

we have

Fti(˜xk
ti)

min
Xti(xk

t−1)

−

x

∈

1

F k
ti

−

(x) = Fti(˜xk
ti)

Fti(˜xk
ti)

≤

−

−

Moreover, by (4.7) and (4.18), we have

1

F k
ti

−

(˜xk
ti)

F ji

ti (˜xk

ti).

(4.23)

Fti(˜xk
ti)
|

−

Fti(xji
t )

Mt

˜xk
ti −
k

xji
t k

| ≤

and

ti (˜xk
ti)

F ji
|

−

F ji
ti (xji
t )

M tk

˜xk
ti −

xji
.
t k

| ≤

6
22

Guanghui Lan

In addition, it follows from the deﬁnitions of Fti and F k
(4.22) that

ti (c.f. (4.2) and (4.8)) and

Fti(xji
t )

−

F ji
ti (xji

t ) = λ[Vt+1(xji
t )

−

V ji

t+1(xji

t )]

λǫt.

≤

Combining the previous observations and (4.22), we have

1

F k
ti

Fti(˜xk
ti)
−
[Fti(˜xk
ti)

(˜xk
ti)
−
t )] + [Fti(xji
Fti(xji
t )
−
xji
˜xk
(Mt + M t)
ti −
t k
k
(Mt + M t)δt + λǫt = ǫt

+ λǫt

1,

≤

≤

≤

−

F ji
ti (xji

t )] + [F ji

ti (xji
t )

−

F ji
ti (˜xk

ti)]

−

(4.24)

where the last inequality follows from the deﬁnition of ǫt
result, in view of the deﬁnitions of Fti and F k
ti, then implies (4.19).
We will now show that the search point xk
t
−

1-saturated at iteration k. Note that ˜xk

also be ǫt
t-th stage problem and hence that the function value Fti(˜xk
the optimal value νti(xk
t
−

1 in the preceding stage t

1 must
ti are feasible solutions for the
ti) must be greater than

−

1) deﬁned in (4.2). Using this observation, we have

1 in (3.24). The above

−

−

Vt(xk
t
−

1)

V k

t (xk
t
−

−

1) = 1
Nt
1
Nt

1)

Nt

i=1 νti(xk
t
−
i=1 Fti(˜xk
ti)

Nt

P

1)

−
V k

t (xk
V k
t
−
t (xk
1).
t
−

≤
P
Moreover, using the deﬁnitions of V k
t (xk
ti(xk
1) and ˜νk
t
t
−
−
V k
relations in (4.16) and the fact that V k
1
t+1 (x) due to (4.17), we have
t+1(x)
−

1) in (4.13) and (4.10), the

−

(4.25)

V k

t (xk
t
−

1

V k
1) = max
t
{
Nt
i=1 ˜νk
= 1
Nt

−

Nt

i=1 ˜νk

ti(xk
t
−

1)

}

P

≥

1), 1
Nt

(xk
t
−
ti(xk
t
−
F k

1)

P

Nt
i=1 min

ti(x) : x

∈

1

(x) : x

−

1, ˜ξtj)

Xt(xk
t
−
Xt(xk
t
−

∈

o
1, ˜ξtj)

o

(4.26)

Nt
i=1 min
Nt
i=1 F k

ti

−

n

F k
ti
n
(˜xk
1

ti),

= 1
Nt

1
Nt

≥
= 1
Nt

P

P

P

where the last identity follows from the deﬁnition of xk
(4.25) and (4.26), we have

t in (4.8). Putting together

Vt(xk
t
−

1)

−

V k

t (xk
t
−

1)

≤

≤

1
Nt

Nt

i=1[Fti(˜xk
ti)

1

F k
ti

−

(˜xk

ti)]

−

ǫt

−

1,
P

(4.27)

where the last inequality follows from (4.24). The above inequality then implies
that xk
1 gets saturated at the k-th iteration. Moreover, the point xk
1 will be
t
t
−
−
added into the set Sk
1 in view of the deﬁnition in Line 11 of Algorithm 3. We
t
−
have thus shown both part a) and part b).

Diﬀerent from the DDP method, we do not have a convenient way to compute
an exact upper bound on the optimal value for the general multi-stage stochastic
optimization problem. However, we can use gk
1) as a termination criterion for

1 (xk

Complexity of Stochastic Dual Dynamic Programming

23

the EDDP method. Indeed, using (4.24) (with t = 1 and i = 1) and the fact that
N1 = 1, we conclude that if gk

δ1, then we must have

1 (xk
1)

F11(xk
1)

F ∗

−

≤

1

F k

11 (xk
1)
−

−

ǫ0.

≤

(4.28)

≤
F11(xk
1)

It is worth noting that one can possibly provide a stochastic upper bound on F ∗
for solving multi-stage stochastic optimization problems. We will discuss this idea
further in Section 5.

Below we show that each iteration of the EDDP method will either ﬁnd an ǫ0-
solution of problem (4.1), or ﬁnd a new ǫt-saturated and δt-distinguishable search
point at some stage t.

Proposition 5 Assume that δt
), t = 1, . . . , T , are given. Also let ǫt, t =
∈
0, . . . , T , be deﬁned in (3.36). Then any iteration k of the EDDP method will either
generate a new ǫt-saturated and δt-distinguishable search point xk
t at some stage t =
1, . . . , T , or ﬁnd a feasible solution xk

1 of problem (4.1) such that

[0, +

∞

F11(xk
1)

F ∗

−

≤

ǫ0.

(4.29)

Proof. Similar to the proof of Proposition 3, we consider the following T cases

that will happen at the k-th iteration of the EDDP method.

t (xk
t )

Case 1: gk
δt,
Case t, t = 2, . . . , T
Case T : gk
T

≤
1(xk
T

1
T
t
∀
≤
≤
i (xk
1: gk
i )
−
1.
1) > δT

−

−

−

1;
δi,

−
≤

t

∀

≤

i

≤

T

−

1, and gk
t
−

1(xk
t
−

1) > δt

−

1;

−

1 (xk
1)

1) > δt

1(xk
t
−

For the ﬁrst case, it follows from the assumption gk
δ1 and (4.28) that xk
1
≤
must be an ǫ0-solution of problem (4.1). Now let us consider the t-th case for any
2. Since gk
1, the search point xk
t = 2, . . . , T
1 is δt-distinguishable.
t
t
−
−
Moreover, we conclude from the assumption gk
t (xk
δt and Proposition 4.a)
t )
that the point xk
1-saturated. Hence, the search point xk
1 must be ǫt
1 is δt-
t
t
−
−
1-saturated for the t-th case, t = 2, . . . , T
distinguishable and ǫt
1. Finally for the
−
T -th case, xk
1-distinguishable by assumption. Moreover, by Lemma 7,
1 is δT
T
−
xk
1-distinguishable
1 in the (T
T
−
and ǫT

1-saturated. The result then follows by putting all these cases together.

1)-stage will get 0-saturated. Hence xk
T

1 is δT

≤

−

−

−

−

−

−

−

−

We are now ready to establish the complexity of the EDDP method. For the
norm to deﬁne the distances

to be an l

sake of simplicity, we will ﬁx the norm
and Lipschitz constants at each stage t.

k·k

∞

Theorem 2 Suppose that the norm used to deﬁne the bound on Dt in (4.4) is the l
∞
) are given and that ǫt are deﬁned in (3.36). Then
norm. Also assume that δt
the number of iterations performed by the EDDP method to ﬁnd a solution satisfying

[0, +

∞

∈

can be bounded by ¯K + 1, where

F11(xk
1)

F ∗

ǫ0

≤

−

¯K :=

T
1
−
t=1

Dt
δt + 1

nt

.

P

(cid:16)

(cid:17)

(4.30)

(4.31)

24

Guanghui Lan

In particular, If nt
then the EDDP method will ﬁnd a solution xk

D, max
Mt, M t} ≤
{

n, Dt

≤

≤

1 of problem (4.1) s.t.

M and δt = ǫ for all t = 1, . . . , T ,

F11(xk
1)

F ∗

2M min
{

≤

−

1

−

1

λ , T

ǫ,

1

}

−

within at most ¯Kǫ + 1 iterations with

¯Kǫ := (T

1)

−

D
ǫ + 1

n

.

(4.32)

(4.33)

(cid:0)
Proof. Let us count the total number of possible search points for saturation
before an ǫ-optimal policy of problem (4.1) is found. Using (4.15) and the assump-
tion the feasible region for each stage t is inside a box with side length Dt (c.f.,
(4.4)), we can see that the number of possible search points for saturation at each
stage is given by

(cid:1)

Dt
δt + 1

nt

.

(cid:16)

(cid:17)

As a consequence, the total number of iterations that EDDP will perform be-
fore ﬁnding an ǫ0-optimal policy will be bounded by ¯K + 1. If nt
D,
max
M and δt = ǫ for all t = 1, . . . , T , we can obtain (4.32) by using
Mt, M t} ≤
{
the bound (3.46) for ǫ0 in (4.30). Moreover, the bound in (4.33) follows directly
from (4.31).

n, Dt

≤

≤

We now add some remarks about the results obtained in Theorem 2 for the
EDDP method. First, comparing with the DDP method for single-scenario prob-
lems, we can see that these two algorithms exhibit similar iteration complexity.
However, the DDP method provides some guarantees on an easily computable gap
between the upper and lower bound. On the other hand, we can terminate the
EDDP method by using the quantity gk
1 . Second, the EDDP method requires us
to maintain the set of saturated search points Sk
t and explicitly use the selected
norm
t . In the next section, we will discuss a stochastic dual
dynamic programming method which can address some of these issues associated
with EDDP, by sacriﬁcing a bit on the iteration complexity bound in terms of its
dependence on the number of scenarios Nt. Third, similar to the DDP method, we
can replace nt in the complexity bound of the EDDP method with the dimension
of the eﬀective region ¯
X

to compute gk

t in (4.31).

k · k

5 Stochastic dual dynamic programming

In this section, we still consider the SAA problem (4.1) for multi-stage stochas-
tic optimization and suppose that Assumptions 4, 5 and 6 hold throughout this
section. Our goal is to establish the iteration complexity of the stochastic dual
dynamic programming (SDDP) for solving this problem.

As mentioned in the previous section, when dealing with multiple scenarios in
each stage t, we need to select xk
t from ˜xti, i = 1, . . . , Nt, deﬁned in (4.8), where ˜xti
corresponds to a particular realization ˜ξti, i = 1, . . . , Nt. While the EDDP method
chooses xk
t in an aggressive manner by selecting the most “distinguishable” search
points, SDDP will select xk

t from ˜xti, i = 1, . . . , Nt, in a randomized manner.

The SDDP method is formally described in Algorithm 4. This method still con-
sists of the forward phase and backward phase similarly to the DDP and EDDP

Complexity of Stochastic Dual Dynamic Programming

25

t as being randomly chosen from ˜xk

methods. On one hand, we can view DDP as a special case of SDDP with Nt = 1,
t = 1, . . . , T . On the other hand, there exist a few essential diﬀerences between
SDDP in Algorithm 4 and EDDP in Algorithm 3. First, in the forward phase of
SDDP, we randomly pick up an index it and solve problem (5.1) to update xk
t .
Equivalently, one can view xk
ti, i = 1, . . . , Nt,
deﬁned in (4.8) for the EDDP method. Note that we do not need to compute ˜xk
ti
= it, even though they will be used in the analysis of the SDDP method.
for i
Hence, the computation of the forward path (xk
T ) in SDDP is less expen-
sive than that in EDDP. Second, in SDDP we do not need to maintain the set
of saturated search points and thus the algorithmic scheme is much simpliﬁed.
However, without these sets, we will not be able to compute the quantities gk
t as
in Algorithm 3 and thus cannot perform a rigorous termination test as in EDDP.
We will discuss later in this section how to provide a statistical upper bound by
running the forward phase a few times.

1, . . . , xk

t (x) = −∞, t = 2, . . . , T , V k

Algorithm 4 Stochastic dual dynamic programming (SDDP)
1: Set V 0
2: for k = 1, 2, . . . , do
3:
4:
5:

for t = 1, . . . , T do
Pick up it ≡ ik
Set

t from {1, 2, . . . , Nt} uniformly randomly.

T +1(x) = 0, k ≥ 1.

⊲ Forward phase.

xk
t ∈ Argminx∈Xt(xk

t−1, ˜ξtit )

6:

end for

F k−1

tit (x) := Ht(x, ˜ctit ) + λV k−1

t+1 (x)

.

(5.1)

n

o

⊲ Backward phase.

for t = T, T − 1, . . . , 2 do
for i = 1, . . . , Nt do
ti(xk

t−1) and (˜νk

Set ˜νk

7:
8:
9:
10:
11:
12:
13: end for

end for
Update V k

end for

ti)′(xk

t−1) according to (4.10) and (4.11).

t (x) according to (4.12) and (4.13).

As mentioned earlier, our goal in this section is to solve the SAA problem in
(4.1) instead of the original problem in (1.1). Hence the randomness for the SDDP
method in Algorithm 4 comes from the i.i.d. random selection variable ik
t only. The
statistical analysis to relate the SAA problem in (4.1) and the original problem
in (1.1) has been extensively studied especially under the stage-wise independence
assumption (e.g. [27]). The separation of these two problems allows us to greatly
simplify the analysis of SDDP.

Whenever the iteration index k is clear from the context, we use the short-hand

notation it

ik
t . We also use the notation
T , i2

1, . . . , i2

≡
i[k,t] :=

1, . . . , i1
i1
{

T , . . . , . . . , ik

1

1

, . . . , ik
T

−

−

1

, ik

1, . . . , ik
t

}

to denote the sequence of random selection variables generated up to stage t at
the k-th iteration. The notions i[k,0] and i[k
1,T ] will be used interchangeably. We
use
Ik,t to denote the sigma-algebra generated by i[k,t]. It should be noted that fo
any iteration k
1 = 1 since the number of scenarios N1 = 1. In
other words, ik

1 is alway deterministic for any k

1, we must have ik

1.

≥

−

≥

6
26

Guanghui Lan

The complexity analysis of SDDP still relies on the concept of saturation.
xt :
−
{
1
for some ji < k

the set of saturated points in stage t, i.e., Sk
t
ǫt, for some j

Let us denote Sk
−
t
t+1(xt)
Vt+1(xt)
to denote the closest point to ˜xk

. We still use xji
t
}
ti from the saturated points Sk
t

1

−

, i.e.,

V j

:=

−

−

−

≤

≤

1

k

1

1

xji
t ∈
Vt+1(xji
t )

Argmins
S
∈
V ji

k−1
t

s
−
k
t+1(xji
t )

−

˜xk
,
tik
ǫt.

≤

(5.2)

(5.3)

In SDDP, we will explore the average distance between ˜xk
as follows:

ti to the set Sk
t

1

−

deﬁned

˜gk
t := 1
Nt

Nt
i=1 k

˜xk
ti −

xji
.
t k

(5.4)

P

1 (see (4.8)) and hence on i[k,t

Note that the search point xk
depends on xk
t
−
points Sk
only depends on i[k
t
−
the previous iteration. Hence, ˜gk
t is measurable w.r.t.
of the random selection variable ik

t is a function of i[k,t] and hence is also random. ˜xk
ti
1]. Moreover, the set of saturated
1,T ] since it is deﬁned in the backward phase of
1, but it is independent

Ik,t
−
t for the current stage t at the k-th iteration.

−

−

1

Lemma 8 below summarizes some important properties about ˜gk
t .

Lemma 8 Let δt
have

∈

[0, +

∞

) be given and ǫt be deﬁned in (3.24). If ˜gk

t ≤

δt, then we

1
Nt

Nt

i=1[Fti(˜xk
ti)

−

1

F k
ti

−

(˜xk

ti)] = λ
Nt

Nt

i=1[Vt+1(˜xk
ti)

−

P
Moreover, for t

2 we have

≥

P

Vt(xk
t
−

1)

−

V k

t (xk
t
−

1)

1.

ǫt

−

≤

1

V k

t+1 (˜xk
−

ti)]

1.

ǫt

−

≤

(5.5)

(5.6)

Proof. First note the second inequality in (4.24) still holds since it does not

depend on the selection of xk

t . Hence we have

Fti(˜xk
ti)

−

1

F k
ti

−

(˜xk
ti)

≤

(Mt + M t)

˜xk
ti −
k

xji
t k

+ λǫt.

Summing up the above inequalities, we can see that

1
Nt

Nt

i=1[Fti(˜xk
ti)

1

F k
ti

−

(˜xk

ti)]

−

P

Nt
i=1 k

˜xk
ti −

xji
t k

+ λǫt

(Mt + M t) 1
Nt
≤
= (Mt + M t)˜gk
1,

ǫt

≤

−

t + λǫt
P

which together with the deﬁnitions of Fti and F k
ti
(5.6) follows from (4.27) and (5.5).

−

1

then imply (5.5). Moreover,

Similar to the previous section, we use

t (xk
gk

t ) :=

mins
∈
0,

(

k−1
t

S

s
k

−

xk
,
t k

t < T,

o.w.

to measure the distance between xk
is a random variable dependent on xk

t and the set of saturated points. Clearly, gk

t and hence measurable w.r.t.

t (xk
t )
Ik,t. We say

Complexity of Stochastic Dual Dynamic Programming

27

V t+1(xk
t )

−

≤

ǫt. Moreover, xk
t

is said to be

is ǫt-saturated if V k

that xk
t
δt-distinguishable if gk
The quantitates ˜gk

t ) > δt.

t (xk
t and ˜gk

t+1(xk
t )

t+1 deﬁned in (5.4) provide us a way to check whether
xk
t is δt-distinguishable and ǫt-saturated. More speciﬁcally, If ˜gk
t > δt for some stage
ik,
t < T at iteration k, then there must exist an index i∗t ≡
s.t.
1, . . . , Nt
∗t ∈ {
˜xk
t ) > δt (since otherwise ˜gk
t (˜xk
x
ti∗
ti∗
t ≤
t −
k
both ˜gk
t and i∗t are measurable w.r.t.
Ik,t
conditioning on
Ik,t
xk
t = ˜xk
by the law of total probability, Prob
ti∗
{
the conditional probability of

δt). Note that
t . Therefore,
t = i∗t is 1/Nt, and consequently
= 1/Nt. Moreover, we can see that

1 the probability of having ik

1 but independent of the ik

> δ or equivalently gk

ji∗
t
t k

t }

−

−

}

t (xk
gk
Prob
{

t ) > δt

˜gk
t > δt
|

}

=

Nt
gk
t (˜xk
1
Nt Prob
i=1
{
t (˜xk
gk
1
t ) > δt
Nt Prob
P
ti∗
{
≥
= 1
Nt .

˜gk
ti) > δt
t > δt
|
˜gk
t > δt
|

}

}

(5.7)

t will be ǫt-saturated.

In other words, if ˜gk
distinguishable. If, in addition, ˜gk
V k
t+1(xk
t )

t+1 ≤
ǫt and hence xk

V t+1(xk
t )

−

≤

t > δt, then with probability at least 1/Nt, xk

t will be δt-
δt+1, then in view of Lemma 8, we have

While EDDP can ﬁnd at least one new saturated and distinguishable search
point in every iteration, SDDP can only guarantee so in probability as shown in
the following result. We use the random variable qk to denote whether there exists
such a point among any stages at iteration k. Clearly, qk is measurable w.r.t.
Ik,T .
Lemma 9 Assume that δt
), t = 1, . . . , T , are given. Also let ǫt, t = 0, . . . , T ,
be deﬁned in (3.36). The probability of ﬁnding a new δt-distinguishable and ǫt-saturated
and search point at the k-iteration of SDDP can be bounded by

[0, +

∞

∈

Prob

qk = 1
{

} ≥

1
¯N (1

˜gk
Prob
i ≤
{

−

δi, i = 1, . . . , T

1

),
}

−

where

¯N :=

T
1
i=2 Ni.

−

(5.8)

(5.9)

}

A

−

Prob

δi, i = 1, . . . , T

t > δt for some t = 1, . . . , T

Proof. Let A denote that the event that that ˜gk
= 1

Q
1.
˜gk
. Assume that the
Clearly we have Prob
i ≤
}
{
{
event A happens. Let S denote the set of sample paths, i.e., selection of T i.i.d.
uniformly sample indices, where there exists at least one index with ˜gk
t > δt. Clearly
T
t=2 Nt, and each sample path occurs with equal probability. We
we have
−
will show that there exists at least one sample path in S that generates and selects
an ǫt-saturated and δt-distinguishable search point. Let us consider the following
cases.
a) There exists a sample path in S such that ˜gk
T

1 > δT

S
|

| ≤

Q

−

−

1

1

exists at least one search point xk
T
−
1 is ǫT
every search point in stage T

1. In this case, there
1, since

1,i) > δT

−
1(xk
T

−

−

1,i such that gk
T
−
1-saturated, we are done.
1 > δT

−

−

−

b) Amongst all sample paths, no path will have ˜gk
T

1. Consider the set of
t > δt. There exists at least one search
ti) > δt. At least 1/Nt fraction of these sample paths
ti as the search point. Now, one of the following two cases must

sample paths with a stage t such that ˜gk
point xk
will select xk
occur upon selecting xk

ti such that gk

t (xk

−

−

xk
ti:

t ←

28

Guanghui Lan

b1) The sample path will have ˜gk

ǫt-saturaged. Since we have already shown xk
are done.

t+1 ≤

δt+1. Then, by Lemma 8, xk

t will be
t is also δt-distinguishable, we

b2) The sample path will have ˜gk

t+1 > δt+1. Repeat the same argument with
t = t + 1. By the assumption, this incremental argument must terminate
since we cannot have a sample path with ˜gk
T

1 > δT

1.

−

−

In both cases, we have shown the existence of a sample path that generates and
selects an ǫt-saturated and δt-distinguishable search point. Therefore, we have

qk = 1
Prob
{

A
|

} ≥

1
S

|

| ≥

1

N
−
t=2

1
Nt = 1
¯N ,

from which the result immediately follows.

Q

In view of Lemma 9, one of the following three diﬀerent cases will happen for
δt for all t = 1, . . . , T
1. The probability of this
; (b) A new ǫt-saturated and
δi, i = 1, . . . , T
}

each SDDP iteration: (a) ˜gk
t ≤
˜gk
case is denoted by Prob
i ≤
{
δt-distinguishable search point will be generated with probability at least

−

−

1

1
¯N (1

−

Prob

˜gk
i ≤
{

δi, i = 1, . . . , T

1

);
}

−

and (c) none of the above situation will happen, implying that this particular
SDDP iteration is not productive.

Observe that if for some iteration k, we have ˜gk

t ≤

δt for all t = 1, . . . , T

1.

−

Then by Lemma 8 (with t = 1), we have

F11(xk
1)

F ∗

−

≤

F11(xk
1)

−

Moreover, we have

1

F k

11 (xk
1)
−

ǫ0.

≤

(5.10)

λ
Nt

Nt

i=1[Vt+1(˜xk
ti)

−

1

V k

t+1 (˜xk
−

ti)]

ǫt

−

1

≤

for all t = 1, . . . , T . This observation together with the fact that xk
chosen from ˜xk
V k

ti, i = 1, . . . , Nt, then imply that the expectation of Vt+1(xk
t )

t ) conditionally on i[k,t

t is randomly

1]:

t+1 (xk
−

−

1

P

−

E[Vt+1(xk
t )

−

1

V k

t+1 (xk
t )
−

|Ik,t

−

1] = λ
Nt
ǫt

≤

−

Nt

i=1[Vt+1(˜xk
ti)
1, t = 1, . . . , T.
P

−

1

V k

t+1 (˜xk
−

ti)]

(5.11)

Similar in spirit to Lemma 4, the following result relates the above notion of
saturation to the gap between a stochastic upper bound and lower bound on the
optimal value of problem (4.1).

Lemma 10 Suppose that the relations in (5.11) hold for some iteration k
we have

≥

1. Then

T
t=1 λt
−

1E[Ht(xk

t , ˜ctit )

1]

|Ik,t

−

−

P

E[F k

1

11 (xk
1)
−

1,T ]

|Ik

−

≤

T
t=1 λt
−

1ǫt

−

1.

(5.12)

P

Complexity of Stochastic Dual Dynamic Programming

29

Proof. Note that we have N1 = 1. By the deﬁnition of xk

1 in (5.1) and our

assumption in (5.11), we have

1, ˜ct1) + λV2(xk
1)

E[H1(xk
−
1, ˜ct1) + λV2(xk
= E[H1(xk
1)
(xk
V k
= λE[V2(xk
1)
1)
2
λǫ1.

−

−

1

≤

F k

1

11 (xk
1)
−
1,T ]

1,T ]
|Ik
−
E[H1(xk

−

|Ik
|Ik

−

−
1,T ]

1, ˜ct1) + λV k
2

1

(xk
1)

−

1,T ]

|Ik

−

(5.13)

Now consider the t-th stage for any t

2. By the deﬁnition of xk

t in (5.1), we have

Ht(xk

t , ˜ctit ) + λV k

1

t+1 (xk
−

≥
Ht(x, ˜ctit ) + λV k
t+1 (x) : x
t ) = min
−
{
Ht(x, ˜ctit) + λVt+1(x) : x
min
≤
{
= νtit (xk
t
−

1)

1

1)

Xt(xk
t
∈
−
Xt(xk
1)
t
−

}

∈

}

for any t
and using our assumption λE[Vt+1(xk
t )

2. Taking conditional expectation on both sides of the above inequality
t+1 (xk
1
t )
−

1, we then have

V k

1]

≥

ǫt

−

≤

−

E[Ht(xk

t , ˜ctit ) + λVt+1(xk
t )

1]

|Ik,t

−

−

|Ik,t
E[νtit(xk
t
≤
−
= E[Vt(xk
1)
t
−
= E[Vt(xk
t
−

1)

1)

|Ik,t
|Ik,t
|Ik,t

−

−

1] + ǫt

−
1] + ǫt

1

−

−

1

1,

2] + ǫt

−

where the ﬁrst identity follows from the deﬁnition of Vt and the selection of it, and
the second identity follows from the fact that xk
t . Multiplying
t
−
λt
1 to both side of the above inequalities, summing them up with the inequalities
in (5.13), and using the fact that VT +1(xk

1 is independent of ik

T ) = 0, we have

−

T
t=1 λt
−

1E[Ht(xk

t , ˜ctit )

1]

|Ik,t

−

−

P

E[F k

1

11 (xk
1)
−

1,T ]

|Ik

−

≤

T
t=1 λt
−

1ǫt

−

1.

P

We also need to use the following well-known result for the martingale diﬀerence

sequence when establishing the iteration complexity of SDDP.

Lemma 11 Let ξ[t] ≡ {
ξ1, ξ2, . . . , ξt
ζt = ζt(ξ[t]) be deterministic Borel functions of ξ[t] such that E
and E
1
{

ζ2
t /σ2
ξ[t−1] [exp
]
t }
{

be a sequence of iid random variables, and
ξ[t−1] [ζt] = 0 a.s.

a.s., where σt > 0 are deterministic. Then

exp

≤

}

}

|

|

λ

∀

≥

0 : Prob

N
t=1 ζt > λ

N
t=1 σ2
t

(cid:26)

P

qP

exp

λ2/3

.
}

{−

≤

(cid:27)

and

λ

∀

≥

0 : Prob

N
t=1 ζt <

λ

N
t=1 σ2
t

−

exp

λ2/3

.
}

{−

q
Proof. The proof of (5.14) can be found, e.g., Lemma 2 in [16]. In addition,

P

P

(cid:26)

≤

(cid:27)

(5.15) follows from (5.14) by replacing ζt with

ζt.

−

We are now ready to establish the complexity of SDDP.

(5.14)

(5.15)

30

Guanghui Lan

Theorem 3 Suppose that the norm used to deﬁne the bound Dt in (4.4) is the l
norm. Also assume that δt
number of iterations performed by SDDP before it ﬁnds a forward path (xk
deﬁned in (5.1) for problem (4.1) s.t.

∞
) and ǫt are deﬁned in (3.36). Let K denote the
1, . . . , xk
T )

[0, +

∞

∈

T

t=1 λt

−

1E[Ht(xk

t , ˜ctit )

1]

|Ik,t

−

−

E[F k

11 (xk
1)
−

F11(xk
1)
1

F ∗

−

1,T ]

−

≤

≤

ǫ0,
T

t=1 λt

−

(5.16)

1ǫt

−

1.

(5.17)

|Ik

P

Then we have E[K]
respectively. In addition, for any α

≤

¯K ¯N + 2, where ¯K and ¯N are deﬁned in (4.31) and (5.9),

P

1, we have

≥
α ¯K ¯N + 1

Prob
{

K

≥

(α

1)2 ¯K 2

−
2α ¯N

exp

} ≤

−

(cid:16)

.

(cid:17)

(5.18)

Proof. First note that if ˜gk

1, then (5.16) and (5.17)
must hold in view of the discussions after Lemma 9 (c.f. (5.10) and (5.11)) and
Lemma 10. Therefore, the event ˜gk
1 will not happen for
any 1
= 0 for
all 1

˜gk
1. In other words, we have Prob
t ≤
{
1, which, in view of (5.8), implies that for any 1

δt, t = 1, . . . , T
k

δt for all t = 1, . . . , T

δt for all t = 1, . . . , T

t ≤

t ≤

≤
k

1,

K

K

−

−

≤

k

1
}
K
−

−
≤

≤

≤

≤

−
−

qk = 1
{
Moreover, observe that we must have

Prob

1
¯N .

} ≥

(5.19)

2

K

k=1 qk

−

¯K,

(5.20)

≤
since otherwise the algorithm has generated totally ¯K ǫt-saturated and δt-distinguishable
search points during the ﬁrst K
iterations (i.e., (5.16) and (5.17) must hold due to ˜gK
t
Taking expectation on both sides of (5.20), we have

P
2 iterations, and thus must terminate at the K

δt for all t = 1, . . . , T

1
−
1).

−

≤

−

−

1

¯K

≥

EK [E[

2

K

k=1 qk

−

K]]
|

≥

EK [ K

2
¯N ] =
−

E[K]

2
¯N ,
−

implying that E[K]

P
¯N ¯K + 2.

≤

Now we need to bound the probability that the algorithm does not terminate

in α ¯N ¯K + 1 iterations for α

1. Observe that

K

Prob
{

Prob
{
P
α ¯N ¯K + 1 must imply that
since K
margingale-diﬀerence sequence, and E[exp((qk)2)]

} ≤

≥

≥

α ¯N ¯K
k=1 qk < ¯K. Note that qk
1. Hence we have

−

α ¯N ¯K
k=1 qk < ¯K

,
}

(5.21)

E[qk] is a

≥
α ¯N ¯K + 1

P
α ¯N ¯K
k=1 qk < α ¯K
α ¯N ¯K
k=1 qk

≤
λ√α ¯N ¯K
−
α ¯N ¯K
k=1

}
E[qk]

Prob

{
Prob
P
{
λ2/2),
P

exp(

−

≤

≤

≤
λ > 0,

P

∀

λ√α ¯N ¯K

}

−

(5.22)

where the ﬁrst inequality follows from the fact that E[qk]
and thus
Setting

1/ ¯N, k = 1, . . . , α ¯N ¯K,
α ¯K, and the second inequality follows from Lemma 11.

α ¯N ¯K
k=1

E[qk]

≥

≥

P

λ = (α

1) ¯K

−
√α ¯N

Complexity of Stochastic Dual Dynamic Programming

31

in the above relation, we then conclude that

Prob
{
P
Combining (5.21) and (5.23), we then conclude that

exp

} ≤

−

(cid:16)

α ¯N ¯K
k=1 qk < ¯K

(α

1)2 ¯K 2

−
2α ¯N

.

(cid:17)

(5.23)

Prob
{

K

≥

α ¯N ¯K + 1

exp

} ≤

−

(cid:16)

(α

1)2 ¯K 2

−
2α ¯N

,

α

∀

≥

1.

(cid:17)

We have the following immediate consequence of Theorem 3.

Corollary 1 Suppose that nt
M and δt = ǫ for all
≤
≤
t = 1, . . . , T . Let K denote the number of iterations performed by the SDDP method
before it ﬁnds a forward path (xk
1, . . . , xk

D, max
Mt, M t} ≤
{
T ) of problem (4.1) s.t.

n, Dt

F11(xk
1)
−
T
t=1 λt

−

F ∗
≤
1E[Ht(xk

P

≤

2M min
{

−

λ , T

1

1

2M min
{
−
t , ˜ctit )
|Ik,t
λ)2 , T (T
1

(1

−

−
2

1]

−

−
1)

ǫ.

}

1
ǫ,
}
E[F k
11 (xk
1)
−

1

1,T ]

|Ik

−

(5.24)

(5.25)

Then we have E[K]
respectively. In addition, for any α

≤

¯Kǫ ¯N + 2, where ¯Kǫ and ¯N is deﬁned in (4.33) and (5.9),

1, we have

≥
α ¯Kǫ ¯N + 1

Prob

K

{

≥

exp

} ≤

−

(cid:16)

(α

1)2 ¯K 2
ǫ

−
2α ¯N

.

(cid:17)

Proof. The relations in (5.24) and (5.25) follow by using the bound (3.46) for
1 in (5.17), respectively.
directly follows from

ǫ0 in (5.16) and by using the bound (3.47) for
Moreover, the bounds on E[K] and Prob
Theorem 3 by replacing ¯K with ¯Kǫ.

T
t=1 ǫt
α ¯Kǫ ¯N + 1
P

K

≥

−

{

}

We now add a few remarks about the results obtained in Theorem 3 and
Corollary 1. Firstly, since SDDP is a randomized algorithm, we provide bounds
on the expected number of iterations required to ﬁnd an approximate solution
of problem (4.1). We also show that the probability of having large deviations
from these expected bounds for SDDP decays exponentially fast. Secondly, the
complexity bounds for the SDDP method is ¯N times worse than those in Theorem 2
for the EDDP method, even though the dependence on other parameters, including
n and ǫ, remains the same. Thirdly, similar to DDP and EDDP, the complexity of
SDDP actually depends the dimension of the eﬀective feasible region ¯
t in (4.31),
X
which can be smaller than nt.

Remark 1 It should be noted that although the complexity of SDDP is worse than
those for DDP and EDDP, its performance in earlier phase of the algorithm should
be similar to that of DDP. Intuitively, for earlier iterations, the tolerance parameter
δt are large. As long as δt are large enough so that the solutions ˜xk
ti are contained
within a ball with diameter roughly in the order of δt, one can choose any point
randomly from ˜xk
t . In this case, SDDP will perform similarly to DDP and
EDDP. This may explain why SDDP exhibits good practical performance for low
accuracy region. For high accuracy region, the new EDDP algorithm seems to be
a much better choice in terms of its theoretical complexity. In practice, it might

ti as xk

32

Guanghui Lan

make sense to run SDDP in earlier phases (due to its simplicity), and then switch
to EDDP to achieve higher accuracy.

As shown in Theorem 3 and Corollary 1, we can show the convergence of the
gap between a stochastic upper bound on F11(xk
t , ˜ctit ),
and the lower bound F k
11 (xk
1), generated by the SDDP method. In order to ob-
−
tain a statistically more reliable upper bound, we can run the forward phase L
1
times in each iteration. In particular, we can replace the forward phase in Algo-
rithm 4 with the one shown in Algorithm 5. We can then compute the average
and estimated standard deviation of ubk over these L runs of the forward phase.

1), given by

1Ht(xk

t=1 λt

P

≥

−

T

1

Algorithm 5 Forward phase with upper bound estimation
1: for l = 1, . . . , L do
Set ˜Fl = 0.
2:
for t = 1, . . . , T do
3:
4:
5:
6:
7:
8: end for
9: Set ubk = ubk/L.

Pick up it from {1, 2, . . . , Nt} uniformly randomly.
t according to (5.1) and ˜Fl = ˜Fl + λt−1Ht(xk
Set xk

end for
Set ubk = ubk + ˜Fl.

t , ˜ctit ).

⊲ Forward phase.

It should be noted, however, that the convergence of the SDDP method only
requires L = 1. To choose L > 1 helps to properly terminate the algorithm by
providing a statistically more accurate upper bound. Moreover, since each run of
the forward phase will generate a forward path, we can use these L forward paths
to run the backward phases in parallel to accelerate the convergence of SDDP.
Following a similar analysis to the basic version of SDDP, we can show that the
number of iterations required by the above variant of SDDP will be L times smaller
than the one for Algorithm 4, but each iteration is computationally more expensive
or requires more computing resources for parallel processing.

6 Conclusion

In this paper, we establish the complexity of a few cutting plane algorithms, includ-
ing DDP, EDDP and SDDP, for solving dynamic convex optimization problems.
These methods build up piecewise linear functions to approximate the value func-
tions through the backward phase and generate feasible policies in the forward
phase by utilizing these cutting plane models. For the ﬁrst time in the literature,
we establish the total number of iterations required to run these forward and back-
ward phases in order to compute a certain accurate solution. Our results reveal
that these methods have a mild dependence on the number of stages T .

It is worth noting that in our current analysis we assume that all the sub-
problems in the forward and backward phases are solved exactly. However, we can
possibly extend the basic analysis to the case when these subproblems are solved
inexactly as long as the errors are small enough. Moreover, we did not make any
assumptions on how the subproblems are solved. As a result, it is possible to extend
our complexity results to multi-stage stochastic binary (or integer) programming

Complexity of Stochastic Dual Dynamic Programming

33

problems (see, e.g., [30]). In addition, the major analysis for SDDP presented in
this paper does not rely on the convexity, but the Lipschitz continuity of the value
functions and their lower approximations. Hence, it seems to be possible to adapt
our analysis for SDDP-type methods with nonconvex approximations for the value
functions [23, 1].

We have discussed a few diﬀerent ways to terminate DDP, EDDP and SDDP.
More speciﬁcally, DDP can be terminated by calculating the gap between the up-
per and lower bounds, and EDDP is a variant of SDDP with rigorous termination
based on the saturation of search points, whereas SDDP is usually terminated
by resorting to statistically valid upper bounds coupled with the lower bounds
obtained from the cutting plane models. Recently an important line of research
has been developed to design SDDP-like methods with more reliable and eﬃcient
termination criterions (see, e.g., [7, 3, 18]). It will be interesting to study the com-
plexity of these new methods in the future.

Reference

1. S. Ahmed, F. G. Cabral, and B. F. P. d. Costa. Stochastic lipschitz dynamic programming,

2019.

2. H. Bao, Z. Zhou, G. Kotsalis, G. Lan, and Z. Tong. Lignin valorization process control un-
der feedstock uncertainty through a dynamic stochastic programming approach. Reaction
Chemistry & Engineering, 4:1740–1747, 2019.

3. R. Baucke, A. Downward, and G. Zakeri. A deterministic algorithm for solving multi-
stage stochastic programming problems. Technical report, The University of Auckland,
70 Symonds Street, Grafton, Auckland. July 2017, 2017.

4. J.R. Birge. Decomposition and partitioning methods for multistage stochastic linear pro-

grams. Operations Research, 33(5):989–1007, 1985.

5. J.R. Birge and F.V. Louveaux. Introduction to Stochastic Programming. Springer, New

York, 1997.

6. C.J.Donohue and J.R. Birge. The abridged nested decomposition method for multistage
stochastic linear programs with relatively complete recourse. Algorithmic Operations Re-
search, 1(1), 2006.

7. A. Georghiou, A. Tsoukalas, and W. Wiesemann. Robust dual dynamic programming.

Operations Research, 67(3):813–830, 2019.

8. P. Girardeau, V. Leclere, and A. B. Philpott. On the convergence of decomposition meth-
ods for multistage stochastic convex programs. Mathematics of Operations Research,
40:130–145, 2015.

9. V. Guigues. Sddp for some interstage dependent risk-averse problems and application
to hydro-thermal planning. Computational Optimization and Applications, 57:167–203,
2014.

10. V. Guigues.

Inexact cuts in deterministic and stochastic dual dynamic programming

applied to linear optimization problems, 2018.

11. J.L. Higle and S. Sen. Stochastic decomposition: An algorithm for two-stage linear pro-

grams with recourse. Mathematics of Operations Research, 16:650–669, 1991.

12. M. Hindsberger and AB Philpott. Resa: A method for solving multistage stochastic linear

programs. Journal of Applied Operational Research, 6(1):2–15, 2014.

13. J.E. Kelley. The cutting plane method for solving convex programs. Journal of the SIAM,

8:703–712, 1960.

14. V. Kozm´ık and D.P. Morton. Evaluating policies in risk-averse multi-stage stochastic

programming. Mathematical Programming, 152(1-2):275–300, 2015.

15. G. Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer

Nature, Switzerland AG, 2020.

16. G. Lan, A. S. Nemirovski, and A. Shapiro. Validation analysis of mirror descent stochastic

approximation method. Mathematical Programming, 134:425–458, 2012.

17. G. Lan and Z. Zhou. Dynamic stochastic approximation for multi-stage stochastic opti-
mization. Manuscript, Georgia Institute of Technology, 2017. Mathematical Programming,
under minor revision.

34

Guanghui Lan

18. V. Lecl`ere, P. Carpentier, J.P. Chancelier, A. Lenoir, and F. Pacaud. Exact converging
bounds for stochastic dual dynamic programming via fenchel duality. SIAM Journal on
Optimization, 30(2):1223–1250, 2020.

19. K. Linowsky and A. B. Philpott. On the convergence of sampling-based decomposition
algorithms for multistage stochastic programs. Journal of Optimization Theory and Ap-
plications, 125:349–366, 2005.

20. Y. E. Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer

Academic Publishers, Massachusetts, 2004.

21. M. Pereira and L. Pinto. Multi-stage stochastic optimization applied to energy planning.

Mathematical programming, 52(1-3):359–375, 1991.

22. A. Philpott, V. d. Matos, and E. Finardi. On solving multistage stochastic programs with

coherent risk measures. Operations Research, 61:957–970, 2013.

23. A. Philpott, F. Wahid, and F. Bonnans. Midas: A mixed integer dynamic approximation

scheme, 2016. PhD thesis, Inria Saclay Ile de France.

24. R Tyrrell Rockafellar and Roger J-B Wets. Scenarios and policy aggregation in optimiza-

tion under uncertainty. Mathematics of operations research, 16(1):119–147, 1991.

25. A. Ruszczy´nski. Decomposition methods.

In A. Ruszczy´nski and A. Shapiro, editors,

Stochastic Programming, pages 141–211. Elsevier, 2003.

26. A. Shapiro. On complexity of multistage stochastic programs. Operations Research Letters,

34:1–8, 2006.

27. A. Shapiro. Analysis of stochastic dual dynamic programming method. European Journal

of Operational Research, 209:63–72, 2011.

28. A. Shapiro, D. Dentcheva, and A. Ruszczy´nski. Lectures on Stochastic Programming:

Modeling and Theory. SIAM, Philadelphia, 2009.

29. A. Shapiro and A. Nemirovski. On complexity of stochastic programming problems. E-

print available at: http://www.optimization-online.org, 2004.

30. J. Zou, S. Ahmed, and X.A. Sun. Stochastic dual dynamic integer programming. Mathe-

matical Programming, 175 (1-2):461–502, 2019.

