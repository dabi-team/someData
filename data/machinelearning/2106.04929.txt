1
2
0
2

n
u
J

9

]
L
M

.
t
a
t
s
[

1
v
9
2
9
4
0
.
6
0
1
2
:
v
i
X
r
a

Fast and More Powerful Selective Inference for
Sparse High-order Interaction Model

Diptesh Das
Nagoya Institute of Technology
das.diptesh@nitech.ac.jp

Vo Nguyen Le Duy
Nagoya Institute of Technology / RIKEN
duy.mllab.nit@gmail.com

Hiroyuki Hanada
RIKEN
hiroyuki.hanada@riken.jp

Koji Tsuda
University of Tokyo / RIKEN
tsuda@k.u-tokyo.ac.jp

Ichiro Takeuchi
Nagoya Institute of Technology / RIKEN
takeuchi.ichiro@nitech.ac.jp

Abstract

Automated high-stake decision-making such as medical diagnosis requires models
with high interpretability and reliability. As one of the interpretable and reliable
models with good prediction ability, we consider Sparse High-order Interaction
Model (SHIM) in this study. However, ï¬nding statistically signiï¬cant high-order
interactions is challenging due to the intrinsic high dimensionality of the combi-
natorial effects. Another problem in data-driven modeling is the effect of "cherry-
picking" a.k.a. selection bias. Our main contribution is to extend the recently
developed parametric programming approach for selective inference to high-order
interaction models. Exhaustive search over the cherry tree (all possible interactions)
can be daunting and impractical even for a small-sized problem. We introduced
an efï¬cient pruning strategy and demonstrated the computational efï¬ciency and
statistical power of the proposed method using both synthetic and real data.

1

Introduction

Blackbox models such as deep neural network models generally have high predictive performance
but are difï¬cult to interpret and hence, often considered unreliable. Therefore, for tasks that require
high-stake decision-making, such as medical diagnosis and automated driving, models with higher
interpretability and reliability are required. As one of the interpretable and reliable models with good
prediction ability, we consider Sparse High-order Interaction Model (SHIM) in this study. Considering
a regression problem with a response y and m original covariates z1, . . . , zm, an example SHIM up
to 4th order interactions can be written as

y = Î²1z3 + Î²2z5 + Î²3z2z6 + Î²4z1z2z5z9.

(1)

where Î²1, Î²2, Î²3, Î²4 are the model parameters (or coefï¬cients). Such a SHIM has practical importance,
such as identifying complex genotypic features for HIV-1 drug resistance (Saigo et al., 2007). HIV-1
evolves in the human body and exposure to certain drugs causes mutations that leads to resistance
against the drugs. Structural biological studies show that it is the association of multiple mutations
along with some crucial single mutations that can best describe the complex biological phenomenon
of drug resistance (Vivet-Boudou et al., 2006; Iversen et al., 1996; Rhee et al., 2006).

Preprint. Under review.

 
 
 
 
 
 
The goal of this study is to ï¬t a SHIM such as (1) to the given data and subsequently perform
statistical signiï¬cance test to judge the reliability of the model parameters. However, unless the
original dimension and the order of interactions are small, ï¬tting a high-order interaction model can
be challenging and one would require some computational tricks to avoid the combinatorial effects.

Another challenge of data-driven modeling is understanding the reliability of ï¬ndings because the
model might have cherry-picked the strong associations given a particular realization of the data.
This is called "cherry-picking" effect a.k.a. selection bias (Taylor and Tibshirani, 2015). Traditional
statistical inference, which assumes that the statistical model and the target for which inferences are
conducted must be ï¬xed a priori, cannot be used for this problem. Any inference conducted after
model selection will suffer from the selection bias unless it is corrected.

Related works: Several approaches have been suggested in the literature to address the above
problem (Fithian et al. (2014, 2015), Choi et al. (2017), Tian and Taylor (2018), Chen and Bien
(2020), Hyun et al. (2018), Loftus and Taylor (2014, 2015), Panigrahi et al. (2016), Tibshirani et al.
(2016), Yang et al. (2016)). A particularly notable approach is conditional SI introduced in the
seminal paper by Lee et al. (2016). The basic idea of conditional SI is to make inference on a
data-driven hypothesis conditional on the selection event that the hypothesis is selected. Lee et al.
(2016) ï¬rst proposed conditional SI methods for the selected features by using Lasso. Their basic
idea is to characterize the selection event by a polytope, i.e., a set of linear inequalities, in the sample
space. When a selection event can be characterized by a polytope, practical computational methods
developed by these authors can be used for making inferences of the selected hypotheses conditional
on the selection events.

However, the conditional SI framework based on a polytope has a serious drawback called over-
conditioning issue, i.e., additional extra events must be introduced to characterize the selection event
by a single polytope, which is known to lead loss of statistical power or statistically sub-optimal
Fithian et al. (2014). The work by Suzumura et al. (2017), who ï¬rst applied polytope-based SI into
high-order interaction model when a high-order interaction feature is sequentially added to the model,
also suffers from this problem. As a solution in the case of LASSO Lee et al. (2016) proposed to take
the union of all possible signs of the selected features. However, unless the number of the selected
features is small, it is computationally expensive and, in the case of SHIM type problem, it will be
impractical due to the combinatorial effects.

Recently, Le Duy and Takeuchi (2021) introduced a homotopy method to resolve the over-conditioning
issue and realizes minimally-conditioned SI for Lasso. Our basic idea for identifying statistically
reliable high-order interaction features in sparse modeling framework is to employ exact homotopy-
based SI method for SHIM. Unfortunately, the computational cost for applying the exact homotopy
method to SHIM increases exponentially and intractable unless the size of the selected features and
the maximum order of interactions are fairly small. Several methods have already been proposed for
ï¬tting a SHIM (Saigo et al., 2009; Tsuda, 2007; Nakagawa et al., 2016).

Contribution: Our main contribution in this paper is to introduce a â€œhomotopy miningâ€ method by
exploting the best of both homotopy and (pattern) mining methods for conditional SI for SHIM. This
approach is motivated by the exact regularization path computation algorithm for graph data (Tsuda,
2007), which is considered as a homotopy method with respect to the regularization parameter. In the
algorithm of our proposed method, we use two types of homotopy mining methods, one for ï¬tting
a SHIM on the observed dataset (which is essentially the same as the approach in Tsuda (2007))
and, another for computing the sampling distribution of the test-statistic conditional on the selection
event. Interestingly, these two types of homotopy mining methods share many common properties
such as branch and bound techniques for pruning high-order interaction tree (see Fig.1). We applied
our proposed method on synthetic and real-world HIV1 drug resistance data and demonstrated in
Â§4 that we could quantify the statistical signiï¬cance of high-order interaction features in the forms
of p-values and conï¬dence intervals without any computational nor statistical approximations. In
an experimental study of the inference stage, we showed that a single traversal of a search space of
more than 1010 high-order interaction terms (sample size, n = 625) took less than 240 sec (worst
case) and 78 sec (best case) on average using Intel Xeon Gold 6230 CPU @ 2.10 GHZ. We extended
this framework to solve the Elastic Net optimization problem which was not trivial as we cannot
follow the common approach of data augmentation by stacking extra rows as this can be prohibitively
expensive due to the combinatorial effects.

2

2 Problem Statement

Consider a regression problem with a response vector y âˆˆ Rn and m original covariate vectors
z1, . . . , zm, where zj âˆˆ Rn and j âˆˆ [m] = {1, ..., m}. Then, a high-order interaction model up to
dth order is written as

(cid:88)

y =

Î±j1 zj1 +

(cid:88)

Î±j1,j2 zj1 â—¦ zj2 + Â· Â· Â· +

(cid:88)

Î±j1,...,jd zj1 â—¦ Â· Â· Â· â—¦ zjd ,

j1âˆˆ[m]

(j1,j2)âˆˆ[m]Ã—[m]
j1(cid:54)=j2

(j1,...,jd)âˆˆ[m]d
j1(cid:54)=...(cid:54)=jd

(2)

where â—¦ is the element-wise product and scalar Î±s are the coefï¬cients. In this paper, we consider each
element of the original covariate vector zj âˆˆ Rn, j âˆˆ [m], is deï¬ned in a domain [0, 1]. To simplify
notation, it is convenient to write the high-order interaction model in (2) by using the following
matrix of concatenated vectors of all high-order interactions:

, Â· Â· Â· , z1 . . . zd, . . . , zmâˆ’d+1 . . . zm
, z1z2, . . . , zmâˆ’1zm
X = [z1, . . . , zm
(cid:125)
(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
2ndorder

(cid:123)(cid:122)
dth order

(cid:123)(cid:122)
1st order

(cid:124)

(cid:124)

] âˆˆ RnÃ—p,

where p := (cid:80)d
Îº=1
interaction terms can be written as:

(cid:0)m
Îº

(cid:1). Similarly, the coefï¬cient vector associated with all possible high-order

, Î±1,2, . . . , Î±mâˆ’1,m
Î² := [Î±1, . . . , Î±m
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
2nd order

(cid:123)(cid:122)
1st order

(cid:124)

](cid:62) âˆˆ Rp.
, Â· Â· Â· , Î±1,...,d, . . . , Î±mâˆ’d+1,...,m
(cid:125)

(cid:124)

(cid:123)(cid:122)
dth order

The high-order interaction model (2) is then simply written as a linear model y = XÎ². Unfortunately,
p can be prohibitively large unless both m and d are fairly small. In SHIM, we consider a sparse
estimation of high-order interaction model. An example of SHIM looks like

y = Î±3z3 + Î±5z5 + Î±2,6z2z6 + Î±1,2,5,9z1z2z5z9.

(3)

The goal of this study is to ï¬t a SHIM such as (3) and test the statistical signiï¬cance of the coefï¬cients
of the selected model (in the above example, Î±3, Î±5, Î±2,6, Î±1,2,5,9) in order to quantify the reliability.
Unfortunately, both ï¬tting and testing a SHIM are non-trivial because, unless both m and d are
very small, a high-order interaction model will have an extremely large number of parameters to
be considered. Several algorithms for ï¬tting a sparse high-order interaction model were proposed
in the literature (see Â§1). A common approach taken in these existing works is to exploit the
hierarchical structure of high-order interaction features.
In other words, a tree structure as in
Fig. 1(a) is considered and a branch-and-bound strategy is employed in order to avoid handling all
the exponentially increasing number of high-order interaction features.

Here, we introduce an alogrithm for conditional SI in order to quantify the statistical signiï¬cance of
the ï¬tted coefï¬cients of SHIM such as Î±3, Î±5, Î±2,6, Î±1,2,5,9 in the forms of p-values or conï¬dence
intervals by using homotopy-based SI. However, due to the extremely large number of features in (2),
it is intractable to characterize the selection event for homotopy-based SI. In order to overcome this
challenge, we develop homotopy mining method which effectively combines the homotopy method
and branch-and-bound strategy in the cherry tree. Before delving into our proposed method, we
brieï¬‚y overview conditional SI.

2.1 Selective Inference and Homotopy Method

We present conditional selective inference (SI) which is introduced in Lee et al. (2016) and then
explain that optimal (i.e., minimally-conditioned) conditional SI can be conducted with a homotopy
method. In conditional SI framework, we assume that the design matrix X is ï¬xed, response vector y
is a realization of random response vector Y âˆ¼ N (Âµ, Î£), where Âµ âˆˆ Rn is unknown mean vector and
Î£ âˆˆ RnÃ—n is covariance matrix which is known or estimable from external data. In this framework,
we do not assume â€œtrueâ€ relationship between X and Âµ, but consider a case where the data analyst
adopts the SHIM as a reasonable approximation model to describe the relationship.

Let A be the set of selected features by solving the SHIM ï¬tting problem. With a slight abuse of
notation, we also write this set of features as A(y) in order to emphasize that the set of features A is
obtained when y is observed. This notation enables us to consider A(y(cid:48)) as the set of features which

3

Figure 1: (a) A cherry tree of patterns has been constructed by exploiting the hierarchical structure of
high-order interaction features. Not all nodes are traversed due to pruning. (b) The conditional data
space is restricted to a line. In the ï¬gure it is restricted along the horizontal "Ï„ -line" and we need to
ï¬nd the truncation points (Ï„t, Ï„t+1) along this line.

would be selected when a different response vector y(cid:48) is observed. Furthermore, A(Y ) represents the
â€œrandomâ€ set of features selected from the â€œrandomâ€ response vector Y .

Given the set of selected features A, consider the best linear approximation of Âµ with the selected
features. For j âˆˆ A, let

j := (X (cid:62)
Î²âˆ—

A XA)âˆ’1X (cid:62)
A Âµ

be the jth population coefï¬cient of the best linear approximation model ï¬tted only with the selected
features. In conditional SI framework, we consider the following hypothesis test:

H0 : Î²âˆ—

j = 0 v.s. H1 : Î²âˆ—

j (cid:54)= 0, j âˆˆ A.

j XA(Y )(X (cid:62)

(4)
A(Y )XA(Y ))âˆ’1 with ej âˆˆ Rn being the vector with 1 at
Noting that, by deï¬ning Î· := e(cid:62)
the jth component and 0 otherwise, we can write Î²âˆ—
j = Î·(cid:62)Âµ with Y = y. Therefore, it is reasonable
to use Î·(cid:62)Y as the test statistic for the test (4). The (unconditional) sampling distribution of Î·(cid:62)Y
is highly complicated and intractable because Î· also depends on the random response vector Y
through the selected features A(Y ). The basic idea of conditional SI is to consider the sampling
distribution of the test-statistic conditional on the selection event, i.e., Î·(cid:62)Y | {A(Y ) = A}. By
further conditioning on the nuisance component q(Y ) = (In âˆ’ bÎ·(cid:62))Y with b := Î£Î·(Î·(cid:62)Î£Î·)âˆ’1
which is independent of the test statistic Î·(cid:62)Y , Lee et al. (2016) showed that the conditional sampling
distribution of Î·(cid:62)Y | {A(Y ) = A, q(Y ) = q} follows a truncated Normal distribution
Î·(cid:62)Y | {A(Y ) = A, q(Y ) = q} âˆ¼ F T

(5)

Î·T Âµ,Î·T (cid:80) Î·,

where F T
ËœÂµ,ËœÏƒ2 is the c.d.f. of the truncated Normal distribution with mean ËœÂµ, variance ËœÏƒ2, the truncation
region T , and q is the observed nuisance component deï¬ned as q = (In âˆ’ bÎ·(cid:62))y. However,
identifying the conditional data space {A(Y ) = A, q(Y ) = q} is a challenging problem.

In Lee et al. (2016), the authors developed a practical algorithm to compute the truncated Normal
distribution by further conditioning on the signs of the selected features in A. Although the validity
of the inference can be maintained with this additional conditioning on the signs, it turns out that
the power of the inference is suboptimal with this over-conditioning (Fithian et al., 2014). Recently,
Le Duy and Takeuchi (2021) developed an algorithm to resolve this issue by using homotopy method.
In particular, they considered the parametrized response vector (see Fig. 1 (b))

y(Ï„ ) := q + bÏ„
(6)
for a scalar parameter Ï„ âˆˆ R, and solve the continuum of optimal solutions when the response vector
y is replaced with y(Ï„ ) by using homotopy method. Therefore, we can redeï¬ne the conditional data
space in (5) as

T = {Ï„ âˆˆ R | A(y(Ï„ )) = A(y)}.
(7)
It enables us to completely identify the truncation region of the truncated Normal sampling distribution
and compute the selective p-value

P selective

j

= 2 min{Ï€j, 1 âˆ’ Ï€j}, where, Ï€j = 1 âˆ’ F T

0,Î·T (cid:80) Î·(Î·(cid:62)y).

(8)

4

ğ‘§!ğ‘§"ğ‘§!ğ‘§"ğ‘§!ğ‘§#ğ‘§"ğ‘§$ğ‘§$ğ‘§#ğ‘§!ğ‘§"ğ‘§$ğ‘§$ğ‘§#{}ğ‘=âˆ‘&&!âˆ‘&ğ‘ğœğ‘(ğ‘¦)ğœğ‘ğœâ€™ğœâ€™(!y=q+ğ‘ğœ(a)(b)Similarly, one can obtain 1 âˆ’ Î± conï¬dence interval CÎ± for any Î± âˆˆ [0, 1] such that

P(Î²âˆ—

j âˆˆ CÎ±

(cid:12)
(cid:12){A(Y ) = A, q(Y ) = q}) = 1 âˆ’ Î±.

Unfortunately, in the case of SHIM, since the number of high-order interaction features are expo-
nentially large, we cannot use the same homotopy method. In the following section, we present the
homotopy mining algorithm which enables us to compute the conditional sampling distribution (5)
of the ï¬tted SHIM coefï¬cients by effectively combining homotopy method and branch-and-bound
method in pattern mining.

3 Proposed Method

In this study we propose a similar â€œhomotopy-miningâ€ approach for model selection and inference.
Homotopy method refers to an optimization framework for solving a sequence of parameterized
optimization problems. The basic idea of our homotopy mining approach is to consider the following
optimization problem with a parameterized response vector y(Ï„ ) in (6)

Î²(Î», Ï„ ) = arg min

Î²âˆˆRp

FÎ»,Ï„ (Î²) :=

1
2

(cid:107)y(Ï„ ) âˆ’ XÎ²(cid:107)2 + Î» (cid:107)Î²(cid:107)1 ,

(9)

where Ï„ âˆˆ R is a scalar parameter, Î» is the regularization parameter for L1-regularization, and the
objective function FÎ»,Ï„ (Î²) is parameterized by both Ï„ and Î». The homotopy mining enables us to
solve a sequence of parameterized optimization problems in the form of (9) by effectively combining
homotopy and mining method.

To extend the homotopy selective inference framework for SHIM, we ï¬rst need to solve (9) for a
ï¬xed Ï„ and target Î» using the observed data and obtain an active set A. Now, âˆ€j âˆˆ A, we need to
construct the exact solution path characterized by Ï„ and then identify the conditional data space in (7)
by identifying the intervals of Ï„ on the solution path. This exact solution path can be constructed
in a similar manner as the LARS-LASSO algorithm by an efï¬cient step size calculation. Here, we
deï¬ne the exact regularization paths Î» (cid:55)â†’ Î²(Î») for a ï¬xed Ï„ as the â€œÎ»-pathâ€ and Ï„ (cid:55)â†’ Î²(Ï„ ) for a
ï¬xed Î» as the â€œÏ„ -pathâ€, respectively. Then, both the selection and inference paths of the SHIM can
be constructed in a similar fashion as stated below:

â€¢ Model selection of SHIM can be done by using exact regularization path algorithm

Î»0 > Î»1 > Â· Â· Â· > Î»min â‡’ {Î²(Î»0), Î²(Î»1), Â· Â· Â· , Î²(Î»min)}.

â€¢ For inference, we can have similar path algorithm

Ï„0 > Ï„1 > Â· Â· Â· > Ï„min â‡’ {Î²(Ï„0), Î²(Ï„1), Â· Â· Â· , Î²(Ï„min)},

(10)

(11)

where sequences of Î» and Ï„ represent the breakpoints of homotopy method. The Equations (10)
and (11) have similar problem structure, the only difference is that in (10) we ï¬nd the solution
path characterized by the regularization parameter Î», whereas in (11) we ï¬nd the solution path
characterized by Ï„ . Basically, what we need to characterize the selection event is to ï¬nd those
breakpoints (e.g. Ï„0, Ï„3, Ï„8) along the Ï„ -line where the active set remains the same as the observed
one, i.e., A(y) = A(y(Ï„0)) = A(y(Ï„3)) = A(y(Ï„8)). However, computing the exact regularization
paths for such SHIM is a challenging task due to exponentially expanded feature space. Efï¬cient
computational methods are required both at the selection and inference stage. Therefore, we consid-
ered a tree structure (see Fig. 1 (a)) of the interaction terms (or patterns) and proposed a tree pruning
strategy both for the selection path (Î»-path) and inference path (Ï„ -path). In the next section, we
will present the main technical details of characterizing the conditional data space in (7) by using
homotopy-mining method.

3.1 Characterization of truncation region in SHIM

The optimal condition of (9) can be written as

X (cid:62)(cid:0)XÎ²(Î», Ï„ ) âˆ’ y(Ï„ )(cid:1) + Î»s(Î», Ï„ ) = 0 where sj(Î», Ï„ ) âˆˆ

(cid:26){âˆ’1, +1}
[âˆ’1, +1]

if Î²j(Î», Ï„ ) (cid:54)= 0,
if Î²j(Î», Ï„ ) = 0,

(12)

where j âˆˆ [p]. Let us deï¬ne the active set of features as A(y(Ï„ )) = {j âˆˆ [p] : Î²j(Î», Ï„ ) (cid:54)= 0}.

5

The Ï„ -path (Î» ï¬xed). Since Î» is ï¬xed we drop it from the notation. Now consider two real values
Ï„t and Ï„t+1 (Ï„t+1 > Ï„t) at which the active set does not change and their signs also remain the same.
For notational simplicity, we denote AÏ„t = A(y(Ï„t)). Then, one can write from (12)

Î²AÏ„t

Î»sAc
Ï„t

(Ï„ ) = (X (cid:62)

(Ï„t+1) âˆ’ Î²AÏ„t
(Ï„t+1) âˆ’ Î»sAc
Ï„t
where Î½AÏ„t
(Ï„ ) remain con-
b and Î³Ac
Ï„t
stant for all real values of Ï„ âˆˆ [Ï„t, Ï„t+1). Thus, Equations (13) and (14) state that Î²(Ï„ ) and Î»s(Ï„ ) are
(Ï„t) are given in Appendix
piecewise linear in Ï„ for a ï¬xed Î». The derivations of Î½AÏ„t
A. If Ï„t+1 > Ï„t is the next zero crossing point, then either of the following two events happens
â€¢ A zero variable becomes non-zero, i.e., âˆƒj âˆˆ Ac

(Ï„t) = Î½AÏ„t
(Ï„t) = Î³Ac
Ï„t
(Ï„ ) = X (cid:62)
Ac
Ï„t

(Ï„t) Ã— (Ï„t+1 âˆ’ Ï„t)
(Ï„t) Ã— (Ï„t+1 âˆ’ Ï„t)

(Ï„t) and Î³Ac
Ï„t

b âˆ’ X (cid:62)
Ac
Ï„t

(Ï„t+1))| = Î» or,

)âˆ’1X (cid:62)

XAÏ„t

XAÏ„t

Î½AÏ„t

j (y(Ï„t+1) âˆ’ XAÏ„t

Ï„t s.t. |x(cid:62)

Î²AÏ„t

(14)

(13)

AÏ„t

AÏ„t

â€¢ A non-zero variable becomes zero, i.e., âˆƒj âˆˆ AÏ„t s.t. Î²j(Ï„t) (cid:54)= 0 and Î²j(Ï„t+1) = 0 .
Overall, the next change of the active set happens at Ï„t+1 = Ï„t + âˆ†j, where
(cid:16)

(cid:18)

(cid:16)

(cid:17)

min
jâˆˆAc
Ï„t

Î»

sign(Î³j(Ï„t)) âˆ’ sj(Ï„t)
Î³j(Ï„t)

, min
jâˆˆAÏ„t

++

âˆ’

Î²j(Ï„t)
Î½j(Ï„t)

(cid:17)

(cid:19)

++

.

âˆ†j = min(âˆ†1

j , âˆ†2

j ) = min

(15)
Here, we use the convention that for any a âˆˆ R, (a)++ = a if a > 0 and âˆ otherwise. The derivation
of the step-size âˆ†j for the Ï„ -path is given in the Appendix A. However, solving the minimization
problem to determine the step-size of the Ï„ -path and the Î»-path (the details of Î»-path are given in
Appendix A) can be challenging for SHIM type problems. Hence, we need efï¬cient computational
methods to make it practically feasible. In the following section we present an efï¬cient tree pruning
strategy by considering a tree structure of the interaction terms (or patterns). Similar pruning strategy
already exists in the literature to solve the Î»-path of the LASSO in the context graph mining [Tsuda
(2007)]. In the next section we will show that the same pruning strategy can be applied for the Ï„ -path
of the SHIM.

3.2 Tree pruning

A tree is constructed in such a way that for any pair of nodes ((cid:96), (cid:96)(cid:48)), where (cid:96) is the ancestor of (cid:96)(cid:48), i.e.,
(cid:96) âŠ† (cid:96)(cid:48), the following conditions are satisï¬ed

xi(cid:96)(cid:48) = 1 =â‡’ xi(cid:96) = 1 and conversely, xi(cid:96) = 0 =â‡’ xi(cid:96)(cid:48) = 0 âˆ€i âˆˆ [n].
Now considering the Ï„ -path of the LASSO, the equicorrelation condition for any active feature
k âˆˆ AÏ„t+1 at a ï¬xed Î» can be written as

(cid:12)
(cid:12)x(cid:62)

k (y(Ï„t+1) âˆ’ XÎ²(Ï„t+1))(cid:12)

(cid:12) = Î».

(Î²AÏ„t

Ï„t becomes active at Ï„t+1 when the following

Therefore at a ï¬xed Î», any non-active feature (cid:96) âˆˆ Ac
condition is satisï¬ed
(cid:12)
(cid:0)y(Ï„t+1) âˆ’ XAÏ„t
(cid:12)x(cid:62)
(cid:96)
or
where the l.h.s. corresponds to (cid:96) âˆˆ Ac
(cid:16)
Ï(cid:96)(Ï„t, Ï„t+1) = x(cid:62)
Î²AÏ„t
(cid:96)
has a lower bound, i.e.,

y(Ï„t+1) âˆ’ XAÏ„t

(Ï„t) + âˆ†(cid:96)Î½AÏ„t

(cid:0)y(Ï„t+1) âˆ’ XAÏ„t

(cid:12) = (cid:12)
(Î²AÏ„t
|Ï(cid:96)(Ï„t, Ï„t+1) âˆ’ âˆ†(cid:96)Î·(cid:96)(Ï„t)| = |Ïk(Ï„t, Ï„t+1) âˆ’ âˆ†(cid:96)Î·k(Ï„t)|,

(Ï„t))(cid:1)(cid:12)
(cid:12)
(16)
Ï„t and the r.h.s. corresponds to k âˆˆ AÏ„t . Here, we deï¬ne
(Ï„t). The r.h.s. of (16)

and Î·(cid:96)(Ï„t) = x(cid:62)

(Ï„t) + âˆ†(cid:96)Î½AÏ„t

(Ï„t))(cid:1)(cid:12)

(cid:96) XAÏ„t

Î½AÏ„t

(cid:12)x(cid:62)
k

(Ï„t)

(cid:17)

|Ïk(Ï„t, Ï„t+1) âˆ’ âˆ†(cid:96)Î·k(Ï„t)| â‰¥ |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†(cid:96)|Î·k(Ï„t)|,

and the l.h.s. of (16) has an upper bound, i.e.,

|Ï(cid:96)(Ï„t, Ï„t+1) âˆ’ âˆ†(cid:96)Î·(cid:96)(Ï„t)| â‰¤ |Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†(cid:96)|Î·(cid:96)(Ï„t)|.

Therefore, for equation (16) to have a solution, the following condition needs to be satisï¬ed

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†(cid:96)|Î·(cid:96)(Ï„t)| â‰¥ |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†(cid:96)|Î·k(Ï„t)|.

(17)

If the above condition (17) is not satisï¬ed, then equation (16) will not have any solution, and that can
be used as a pruning condition. Therefore, the pruning condition can be written as

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†(cid:96)|Î·(cid:96)(Ï„t)| < |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†(cid:96)|Î·k(Ï„t)|.

(18)

6

Lemma 1 If âˆ†âˆ—

(cid:96) is the current minimum step-size, i.e. âˆ†âˆ—

(cid:96) = min

tâˆˆ{1,2,...,(cid:96)}

{âˆ†t}, (18) is equivalent to

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†âˆ—

(cid:96) |Î·(cid:96)(Ï„t)| < |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Ï„t)|.

Lemma 2 If Lemma 1 holds, then âˆ€(cid:96)(cid:48) âŠƒ (cid:96),

|Ï(cid:96)(cid:48)(Ï„t, Ï„t+1)| + âˆ†âˆ—

(cid:96) |Î·(cid:96)(cid:48)(Ï„t)| < |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Ï„t)|.

(19)

If the Lemma 2 holds, then âˆ€(cid:96)(cid:48) âŠƒ (cid:96), âˆ†(cid:96)(cid:48) > âˆ†âˆ—
(cid:96) . Therefore, we can use Lemma 2 as the pruning
criterion to prune the sub-tree with (cid:96)(cid:48) as the root node. The proofs of Lemmas 1 and 2 are deferred to
Appendix A. The complete algorithm for the inference path (Ï„ -path) is given in Algorithm 1.

Algorithm 1: Ï„ -path
1: Input: Z, Î», b, q, [Ï„min, Ï„max]
2: Initialization: t = 0, Ï„t = Ï„min, T = {Ï„t}, Î²(Ï„t) = 0
3: y(Ï„t) = q + bÏ„t, AÏ„t, Î²AÏ„k

Appendix A)

b,

AÏ„t

(Ï„t) = 0

Î½Ac
Ï„t

XAÏ„t

)âˆ’1X (cid:62)

(Ï„t) = (X (cid:62)
4: Î½AÏ„t
AÏ„t
5: while (Ï„t < Ï„max) do
6: Compute step-length âˆ†j â† Equation (15)
7: If âˆ†j = âˆ†1
j , add j into AÏ„t
8: If âˆ†j = âˆ†2
j , remove j from AÏ„t
9: update: Ï„t+1 â† Ï„t + âˆ†j, T = T âˆª {Ï„t+1}, Î²AÏ„t+1

y(Ï„t+1) = q + bÏ„t+1, Î½AÏ„t+1

10: end while
11: Output: T , {AÏ„t}Ï„tâˆˆT

(Ï„t+1) = (X (cid:62)

AÏ„t+1

3.3 Extension for Elastic Net

(Ï„t) â† Î»-path(Z, y(Ï„k), Î») (The algorithm of Î»-path is in

(cid:46) Inclusion
(cid:46) Deletion

(Ï„t) â† Î²AÏ„t
)âˆ’1X (cid:62)

XAÏ„t+1

(Ï„t) + âˆ†jÎ½AÏ„t

AÏ„t+1

b,

Î½Ac

Ï„t+1

(Ï„t),
(Ï„t+1) = 0

We extended our proposed method to solve the elastic net optimization problem. However, we could
not follow the general approach of solving the elastic net optimization problem as solving LASSO
with augmented data. Because, we cannot just simply augment the data by stacking extra rows as this
can be prohibitively expensive due to the combinatorial effects. In order to derive the step-size for
both Î»-path and Ï„ -path, we need a different approach as we construct the high-order interaction model
in a progressive manner. We have shown that using a simple trick, the step-size can be computed very
efï¬ciently. Similar trick is also used to derive the pruning condition. See Appendix B for the details.

4 Experiments

We only highlight the main results. The details of experimental setup and several additional experi-
mental results are deferred to Appendix C.

4.1 Comparison of statistical powers.

Synthetic data: We generated the i.i.d. random samples (zi, y) âˆˆ [0, 1]m Ã— R in such a way that
100m(1 âˆ’ Î¶)% of zi âˆˆ Rm contain 1son average. Here, Î¶ âˆˆ [0, 1] is the sparsity controlling
parameter. The response yi âˆˆ R is randomly generated from a normal distribution N (0, Ïƒ2). For the
comparison of false positive rates (FPRs), true positive rates (TPRs) and conï¬dence interval (CI)
across different methods, we generated the design matrix for a ï¬xed sparsity parameter Î¶ = 0.95.
In all experiments, the signiï¬cance level was set as Î± = 0.05. For the comparison of TPRs we
considered a true model of up to 3rd-order interactions deï¬ned as Âµ(xi) = 0.5z1 âˆ’ 2z2z3 + 3z4z5z6.
The response yi is accordingly generated from N (Âµ(X), Ïƒ2I). For the comparison of FPRs, we
set Î²j = 0, âˆ€j âˆˆ Rp. We compared both FPRs and TPRs across three different methods (ds: data
splitting, homo: homotopy, poly: polytope) for four different sample sizes n âˆˆ [100, 200, 400, 500].
We generated TPRs and FPRs over 100 trials for all three methods and repeated the experiments for 5

7

Figure 2: Demonstration of the statistical power of three selection bias correction methods (ds: data
splitting, homo: homotopy, poly: polytope) using synthetic data experiments. (a) and (b) show the
false positive rates and the true positive rates for different sample sizes and (c) shows the distribution
of the conï¬dence interval lengths.

Figure 3: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conï¬dence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conï¬dence interval lengths of the same experiments.
The numbers inside the brackets represent the average number of intervals along the Ï„ -line considered
for the homotopy method. Note that in case of polytope only one such interval is considered.

times. The results are shown in Fig. 2(a) and Fig. 2(b), respectively. It can be seen that all SI methods
can properly control the FPRs under Î± = 0.05. Regarding the TPRs comparison, it can be seen
that homotopy has the highest power which is obvious as it is minimally conditioned compared to
polytope which suffers from over conditioning. Comparing TPRs of data splitting (ds) and homotopy
(homo), it can be seen that TPRs of homo is always greater than that of ds. Note that in ds, only
half of the data is used for selection and the remaining half is used for the inference. Therefore,
compared to homo, ds has higher risk of failing to identify truly correlated features in selection stage
and similarly suffer from low statistical power in the inference stage. The result of CIs is shown in
Fig. 2(c). Here, we used the same true model of the TPR experiments and reported the average CIs
over 100 trials across different methods. The results of CIs are consistent with the ï¬ndings of TPRs.

Real data: We obtained HIV-1 sequence data from Stanford HIV Drug Resistance Database Rhee
et al. (2003). In our experiment we used 6 NRTIs, 1 NNRTIs and 3 PIs drugs. We only reported here
the results of 3 NRTIs drugs. Additional results are included in the Appendix C. To demonstrate the
statistical efï¬cacy of the proposed homotopy method over existing polytope method we generated
random sub-samples of those 10 drug data as follows. First, we created a dataset consisting of top 30
mutations from each of the 10 drug data. As most of the columns contain zeros we sorted the columns
based on the number of 1â€™s present in each column and picked the top 30 columns as our starting set.
Then, from this starting set we considered random sub-samples of ï¬ve features for three different

8

(c) Confidence interval lengths(b) True positive rate(a) False positive rate(b.1)(b.2)(b.3)DATA_NRTI_AZT_extop30ğ’P-value[# intervals]CI [# intervals]1000.60 [5.41]1.0 [5.41]2000.67 [8.55]1.0 [8.55]3000.73 [10.10]1.0 [10.10]DATA_NRTI_ABC_extop30ğ’P-value[# intervals]CI [# intervals]1000.74 [1.94]1.0 [1.94]2000.80 [2.25]1.0 [2.25]3000.76 [2.81]1.0 [2.81]DATA_NRTI_D4T_extop30ğ’P-value[# intervals]CI [# intervals]1000.85 [2.25]1.0 [2.25]2000.87 [1.99]1.0 [1.99]3000.82 [2.17]1.0 [2.17](a.1)(a.3)(a.2)Figure 4: Distribution of the fraction of total nodes traversed against different maximum pattern size
(d) constraints while applying the proposed pruning method during the construction of the Ï„ -path.
(a.1) - (a.3) demonstrate the results for 1st, 2nd and 3rd order interaction terms.

d

5
6
7
8
9
10
11
12
13
14
15
None

Search space
(# nodes)
174436
768211
2804011
8656936
8656936
53009101
107636401
194129626
313889476
459312151
614429671
1073741823

1st
14.56 Â± 6.05
34.80 Â± 16.10
68.19 Â± 33.17
110.25 Â± 55.55
151.31 Â± 76.81
188.26 Â± 95.91
212.34 Â± 105.54
226.98 Â± 115.71
233.88 Â± 117.25
240.36 Â± 124.79
238.0Â± 120.35
240.17 Â± 119.76

With pruning
2nd
6.58 Â± 2.05
13.56 Â± 5.75
24.15 Â± 11.83
37.70 Â± 19.39
51.08 Â± 27.09
63.66 Â± 34.71
69.26 Â± 38.49
74.36 Â± 41.20
76.86 Â± 43.10
78.127 Â± 43.44
79.67 Â± 44.72
78.08 Â± 43.62

3rd
6.78 Â± 3.92
13.99 Â± 10.08
25.19 Â± 20.50
39.45 Â± 33.37
54.06 Â± 47.34
65.49 Â± 58.42
74.54 Â± 66.42
78.97 Â± 70.33
83.09 Â± 75.13
82.98 Â± 74.13
83.31 Â± 75.16
82.98 Â± 74.33

1st
25.29 Â± 2.50
126.96 Â± 8.61
450.24 Â± 28.50
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

Without pruning
2nd
34.80 Â± 1.19
125.29 Â± 2.14
447.59 Â± 22.15
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

3rd
23.34 Â± 1.88
127.97 Â± 4.80
447.19 Â± 37.69
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

Table 1: Computation time (in sec) with and without puning for 1st, 2nd and 3rd order interactions.
Here, the computation time is measured against different maximum pattern size (d) constraints. The
last row corresponds to the case when "d" is not speciï¬ed and the whole search space is used for
exploration. All computation times are measured on Intel Xeon Gold 6230 CPU @ 2.10GHz.

sample sizes (n âˆˆ {100, 200, 300}). Here, we considered randomization without replacement for
both sample and features selection. We generated 100 samples and repeated the experiments for
ï¬ve times and hence, in total we generated 500 samples. Figure 3 demonstrates the percentage of
times homotopy produced smaller p-values and CI lengths than the polytope. This also depicts the
distributional difference of the CI lengths between homotopy and polytope. These results clearly
demonstrate that homotopy is statistically more powerful than existing polytope method.

4.2 Comparison of computational efï¬ciencies.

To demonstrate the computational efï¬ciency of the proposed pruning strategy for the Ï„ -path, we
applied our homotopy method with and without pruning on HIV NRTI D4T drug resistance data with
the same starting set of top 30 mutations as used to demonstrate the statistical power. Although we
varied the d from 5 to m, high-order interaction terms upto 3rd order appeared in A. We compared
both the number of nodes traversed (Fig.4) and the time taken (Table.1) against different maximum
interaction order d during the construction of the Ï„ -path of each test statistic direction. Empirically it
was found that the pruning was more effective for the Ï„ -path of high-order interaction terms compared
to that of singleton terms and the power of pruning increases as the order of interaction increases.
Therefore, we reported the average number of nodes and average time taken separately for 1st, 2nd
and 3rd order interaction terms. It can be observed that the pruning is more effective at the deeper
nodes of the tree and it saturates after certain depth of the tree. This is evident as the sparsity of the
data increases at the deeper nodes and the pruning exploits the monotonicity of high-order interaction
terms constructed as tree. In case of homotopy method without pruning we stopped the execution of
program if the Ï„ -path was not ï¬nished in one day. From Tab. 1, it can be observed that without the
pruning the construction of Ï„ -path is not practical owing to the generation of exponential number
of high-order interaction terms as we progress to the deeper nodes of the tree. The Ï„ -path without

9

(a.3) 3!"order interaction(a.2) 2#"order interaction(a.1) 1$%order interactionpruning took more than a day beyond d = 7, while the maximum time taken by the Ï„ -path with
pruning was around 240 sec on average, even when no d constraint was imposed.

5 Conclusions

In this paper, we presented an algorithm for testing a sparse high-order interaction model (SHIM) by
using the framework of conditional selective inference (SI). The algorithm is developed by effectively
combining the homotopy and branch-and-bound tree mining method to deal with the combinatorial
computational burden of the SHIM and also to improve the statistical power.

References

Shuxiao Chen and Jacob Bien. 2020. Valid inference corrected for outlier removal. Journal of

Computational and Graphical Statistics 29, 2 (2020), 323â€“334.

Yunjin Choi, Jonathan Taylor, and Robert Tibshirani. 2017. Selecting the number of principal
components: Estimation of the true rank of a noisy matrix. The Annals of Statistics (2017),
2590â€“2617.

William Fithian, Dennis Sun, and Jonathan Taylor. 2014. Optimal inference after model selection.

arXiv preprint arXiv:1410.2597 (2014).

William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan Tibshirani. 2015. Selective sequential

model selection. arXiv preprint arXiv:1512.02565 (2015).

Sangwon Hyun, Kevin Z Lin, Max Gâ€™Sell, and Ryan J Tibshirani. 2018. Post-selection inference
for changepoint detection algorithms with application to copy number variation data. Biometrics
(2018).

AK Iversen, Robert W Shafer, Kathy Wehrly, Mark A Winters, James I Mullins, Bruce Chesebro,
and Thomas C Merigan. 1996. Multidrug-resistant human immunodeï¬ciency virus type 1 strains
resulting from combination antiretroviral therapy. Journal of Virology 70, 2 (1996), 1086â€“1090.

Vo Nguyen Le Duy and Ichiro Takeuchi. 2021. Parametric programming approach for more powerful
and general lasso selective inference. In International Conference on Artiï¬cial Intelligence and
Statistics. PMLR, 901â€“909.

Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. 2016. Exact post-selection inference,

with application to the lasso. Annals of Statistics 44, 3 (2016), 907â€“927.

Joshua R Loftus and Jonathan E Taylor. 2014. A signiï¬cance test for forward stepwise model

selection. arXiv preprint arXiv:1405.3920 (2014).

Joshua R Loftus and Jonathan E Taylor. 2015. Selective inference in regression models with groups

of variables. arXiv preprint arXiv:1511.01478 (2015).

Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, and Ichiro Takeuchi. 2016.
Safe pattern pruning: An efï¬cient approach for predictive pattern mining. In Proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining. 1785â€“1794.

Snigdha Panigrahi, Jonathan Taylor, and Asaf Weinstein. 2016. Bayesian post-selection inference in

the linear model. arXiv preprint arXiv:1605.08824 28 (2016).

Soo-Yon Rhee, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W
Shafer. 2003. Human immunodeï¬ciency virus reverse transcriptase and protease sequence database.
Nucleic acids research 31, 1 (2003), 298â€“303.

Soo-Yon Rhee, Jonathan Taylor, Gauhar Wadhera, Asa Ben-Hur, Douglas L Brutlag, and Robert W
Shafer. 2006. Genotypic predictors of human immunodeï¬ciency virus type 1 drug resistance.
Proceedings of the National Academy of Sciences 103, 46 (2006), 17355â€“17360.

10

Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki, Taku Kudo, and Koji Tsuda. 2009. gBoost: a
mathematical programming approach to graph classiï¬cation and regression. Machine Learning 75,
1 (2009), 69â€“89.

Hiroto Saigo, Takeaki Uno, and Koji Tsuda. 2007. Mining complex genotypic features for predicting

HIV-1 drug resistance. Bioinformatics 23, 18 (2007), 2455â€“2462.

Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, and Ichiro Takeuchi. 2017. Selective
inference for sparse high-order interaction models. In International Conference on Machine
Learning. PMLR, 3338â€“3347.

Jonathan Taylor and Robert J Tibshirani. 2015. Statistical learning and selective inference. Proceed-

ings of the National Academy of Sciences 112, 25 (2015), 7629â€“7634.

Xiaoying Tian and Jonathan Taylor. 2018. Selective inference with a randomized response. The

Annals of Statistics 46, 2 (2018), 679â€“710.

Ryan J Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. 2016. Exact post-
selection inference for sequential regression procedures. J. Amer. Statist. Assoc. 111, 514 (2016),
600â€“620.

Koji Tsuda. 2007. Entire Regularization Paths for Graph Data. In Proceedings of the 24th International
Conference on Machine Learning (Corvalis, Oregon, USA) (ICML â€™07). Association for Computing
Machinery, New York, NY, USA, 919â€“926. https://doi.org/10.1145/1273496.1273612

V Vivet-Boudou, J Didierjean, C Isel, and R Marquet. 2006. Nucleoside and nucleotide inhibitors of

HIV-1 replication. Cellular and Molecular Life Sciences CMLS 63, 2 (2006), 163â€“186.

Fan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty. 2016. Selective inference for

group-sparse linear models. arXiv preprint arXiv:1607.08211 (2016).

Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of

the royal statistical society: series B (statistical methodology) 67, 2 (2005), 301â€“320.

11

A Appendix

A.1 LASSO Ï„ -path.

A.1.1 Derivations of Î½AÏ„t

(Ï„t) and Î³Ac
Ï„t

(Ï„t) in Equations (13) and (14).

From the optimality conditions (12) of the Lasso at Ï„t and Ï„t+1, we have following equations for the
active components

(y(Ï„t) âˆ’ XAÏ„t

âˆ’X (cid:62)
AÏ„t
(y(Ï„t+1) âˆ’ XAÏ„t

Î²AÏ„t

(Ï„t)) + Î»sAÏ„t
(Ï„t+1)) + Î»sAÏ„t

Î²AÏ„t

âˆ’X (cid:62)
AÏ„t

(Ï„t) = 0,

(Ï„t) = 0.

(20)

(21)

Note that AÏ„t = AÏ„t+1 and sAÏ„t
write

(Ï„t) = sAÏ„t

(Ï„t+1). Therefore, subtracting (20) from (21) we can

Î²AÏ„t

(Ï„t+1) âˆ’ Î²AÏ„t

(Ï„t) = (X (cid:62)
AÏ„t
= (X (cid:62)
AÏ„t
= (X (cid:62)
AÏ„t

XAÏ„t
XAÏ„t
XAÏ„t

)âˆ’1X (cid:62)
AÏ„t
)âˆ’1X (cid:62)
AÏ„t
)âˆ’1X (cid:62)
AÏ„t

= Î½AÏ„t

(Ï„t+1 âˆ’ Ï„t).

(y(Ï„t+1) âˆ’ y(Ï„t))

(q + bÏ„t+1 âˆ’ q âˆ’ bÏ„t) using Equation (6)

b(Ï„t+1 âˆ’ Ï„t)

where, we deï¬ned Î½AÏ„t
where Ac
Ï„t

= Ac

Ï„t+1 but, sAc
Ï„t

(Ï„t) = (X (cid:62)
AÏ„t

XAÏ„t

)âˆ’1X (cid:62)

AÏ„t

(Ï„t) (cid:54)= sAc
Ï„t

(Ï„t+1),

b. Similarly, for the non-active components,

âˆ’X T
Ac
Ï„t

(y(Ï„t) âˆ’ XAÏ„t
Î²AÏ„t

(y(Ï„t+1) âˆ’ XAÏ„t

Î²AÏ„t
(Ï„t+1)) + Î»sAc
Ï„t

(Ï„t)) + Î»sAc
Ï„t
(Ï„t+1) = 0

(Ï„t) = 0,

âˆ’X T
Ac
Ï„t

Therefore, subtracting (22) from (23) we can write

Î»sAc
Ï„t

(Ï„t+1) âˆ’ Î»sAc
Ï„t

(Ï„t)(Ï„t) = X T
(b âˆ’ XAÏ„t
Î½AÏ„t
Ac
Ï„t
(Ï„t)(Ï„t+1 âˆ’ Ï„t)

= Î³Ac
Ï„t

(Ï„t))(Ï„t+1 âˆ’ Ï„t)

where we deï¬ned Î³Ac
Ï„t

(Ï„t) = X T
Ac
Ï„t

(b âˆ’ XAÏ„t

Î½AÏ„t

(Ï„t)).

A.1.2 Derivation of step-size âˆ†j in Equation (15)

(22)

(23)

(24)

Step-size of inclusion âˆ†1
can rewrite Equation (12) for non-active components as

j : Letâ€™s deï¬ne âˆ€j âˆˆ Ac

Ï„t, cj(Ï„t) = x(cid:62)
j

(cid:0)y(Ï„t) âˆ’ XAÏ„t

Î²AÏ„t

(Ï„t)(cid:1), then we

âˆ’cj(Ï„t) + Î»sj(Ï„t) = 0
cj(Ï„t) = Î»sj(Ï„t).

âˆ´

(25)

Therefore, at any step (Ï„t+1 â†’ Ï„t + âˆ†j) any non-active feature (j âˆˆ Ac
Ï„t
following condition is satisï¬ed. i.e.

) becomes active when the

Now, letâ€™s consider a linear approximation of cj(Ï„t+1) by considering the value cj(Ï„t) at Ï„t i.e.

|cj(Ï„t+1)| = Î».

cj(Ï„t+1) = cj(Ï„t) + (Ï„t+1 âˆ’ Ï„t)

âˆ‚cj(Ï„t)
âˆ‚Ï„

= cj(Ï„t) + (Ï„t+1 âˆ’ Ï„t)gj(Ï„t).

12

(26)

(27)

. By plugging (27) into (26) and expanding (26) separately for positive and

where, gj(Ï„t) = âˆ‚cj (Ï„t)
negative terms we can write the step-size of inclusion as
(cid:33)
(cid:32)

âˆ‚Ï„

Î» âˆ’ cj(Ï„t)
gj(Ï„t)

,

âˆ’Î» âˆ’ cj(Ï„t)
gj(Ï„t)

âˆ†1

j = Ï„t+1 âˆ’ Ï„t = min
jâˆˆAc
Ï„t

= min
jâˆˆAc
Ï„t

= min
jâˆˆAc
Ï„t

= min
jâˆˆAc
Ï„t

(cid:32)

(cid:32)

(cid:33)

Â±Î» âˆ’ cj(Ï„t)
gj(Ï„t)

Î» sign(gj(Ï„t)) âˆ’ cj(Ï„t)
gj(Ï„t)

(cid:33)

(cid:32)

Î»

sign(Î³j(Ï„t)) âˆ’ sj(Ï„t)
Î³j(Ï„t)

(cid:33)

, using cj(Ï„t) = Î»sj(Ï„t) in Equation (25).

The last equality has been written considering the fact that gj(Ï„t) = Î³j(Ï„t). The proof of this is given
below.

Proof 1 We will now show that gj(Ï„t) = Î³j(Ï„t), âˆ€j âˆˆ Ac
Ï„t
Ac

Ï„t, is

. We know from (24) that Î³j(Ï„t), âˆ€j âˆˆ

Î³j(Ï„t) =

Î»sj(Ï„t+1) âˆ’ Î»sj(Ï„t)
Ï„t+1 âˆ’ Ï„1

,

=

=

cj(Ï„t+1) âˆ’ cj(Ï„t)
Ï„t+1 âˆ’ Ï„1

, using (25)

âˆ‚cj(Ï„t)
âˆ‚Ï„

,

= gj(Ï„t).

Step-size of deletion âˆ†2
0 and Î²j(Ï„t+1) = 0 .

j : A non zero variable becomes zero i.e. âˆƒj âˆˆ AÏ„t such that

: Î²j(Ï„t) (cid:54)=

âˆ´ Î²j(Ï„t+1) = Î²j(Ï„t) + âˆ†2

j Î½j(Ï„t) = 0 =â‡’ âˆ†2

j = min
jâˆˆAÏ„t

(cid:32)

âˆ’

Î²j(Ï„t)
Î½j(Ï„t)

(cid:33)

.

++

(28)

A.1.3 Proofs of Lemmas 1 and 2 in Â§3.2.

We ï¬rst prove Lemma 1. The pruning condition at any node (cid:96) in (18) is

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†(cid:96)|Î·(cid:96)(Ï„t)| < |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†(cid:96)|Î·k(Ï„t)|.

(29)

Let âˆ†âˆ—

(cid:96) is the current minimum step-size, i.e. âˆ†âˆ—

(cid:96) = min

tâˆˆ{1,2,...,(cid:96)}

{âˆ†t}. Now, if we consider the node (cid:96)

to ï¬nd the minimum step-size then, we are expecting that âˆ†(cid:96) â‰¤ âˆ†âˆ—
can write

(cid:96) . Therefore, by construction, we

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†(cid:96)|Î·(cid:96)(Ï„t)| â‰¤ |Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†âˆ—
|Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†âˆ—

(cid:96) |Î·(cid:96)(Ï„t)|
(cid:96) |Î·k(Ï„t)| â‰¤ |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†(cid:96)|Î·k(Ï„t)|.

and,

Therefore, (29) is equivalent to

|Ï(cid:96)(Ï„t, Ï„t+1)| + âˆ†âˆ—

(cid:96) |Î·(cid:96)(Ï„t)| < |Ïk(Ï„t, Ï„t+1)| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Ï„t)|.

(30)

This completes the proof of Lemma 1. Therefore, Lemma 1 is the new pruning condition using the
current minimum step-size, i.e âˆ†âˆ—
(cid:96) . Note that we can further simplify Lemma 1 as follows. We can
write

Ï(cid:96)(Ï„t, Ï„t+1) = |x(cid:62)
(cid:96)
= |x(cid:62)
(cid:96)
= |x(cid:62)
(cid:96)
= |Ï(cid:96)(Ï„t) + âˆ†âˆ—

(cid:0)y(Ï„t+1) âˆ’ XAÏ„t
(cid:0)y(Ï„t) + âˆ†âˆ—
(cid:0)y(Ï„t) âˆ’ XAÏ„t
(cid:96) Î¸(cid:96)|,

Î²AÏ„t
(cid:96) b) âˆ’ XAÏ„t
Î²AÏ„t

(Ï„t)(cid:1)|
Î²AÏ„t
(Ï„t)(cid:1) + âˆ†âˆ—

(Ï„t)(cid:1)| using (6)
(cid:96) x(cid:62)
(cid:96) b|

(31)

13

where Î¸(cid:96) = x(cid:62)

(cid:96) b and Ï(cid:96)(Ï„t) = x(cid:62)
(cid:96)

(cid:0)y(Ï„t) âˆ’ XAÏ„t

Î²AÏ„t
(cid:96) Î¸(cid:96)| â‰¥ |Ï(cid:96)(Ï„t)| âˆ’ âˆ†âˆ—
(cid:96) Î¸(cid:96)| â‰¤ |Ï(cid:96)(Ï„t)| + âˆ†âˆ—

(Ï„t)(cid:1). We know that
(cid:96) |Î¸(cid:96)|
(cid:96) |Î¸(cid:96)|.

and,

|Ï(cid:96)(Ï„t) + âˆ†âˆ—
|Ï(cid:96)(Ï„t) + âˆ†âˆ—

Now using (31) and (32) we can further write (30) as

|Ï(cid:96)(Ï„t)| + âˆ†âˆ—

(cid:96) |Î¸(cid:96)| + âˆ†âˆ—

(cid:96) |Î·(cid:96)(Ï„t)| < |Ïk(Ï„t)| âˆ’ âˆ†âˆ—

(cid:96) |Î¸k| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Ï„t)|.

(32)

(33)

Therefore, (33) serves as the simpliï¬ed expression of the Lemma 1. Next, we provide two propositions
which we use to prove Lemma 2.

Proposition 1 (Tree anti-monotonicity) A tree is constructed in such a way that for any pair of nodes
((cid:96), (cid:96)(cid:48)), where (cid:96) is the ancestor of (cid:96)(cid:48), i.e., (cid:96)(cid:48) âŠƒ (cid:96), the following conditions are satisï¬ed

xi(cid:96)(cid:48) = 1 =â‡’ xi(cid:96) = 1 and conversely, xi(cid:96) = 0 =â‡’ xi(cid:96)(cid:48) = 0 âˆ€i âˆˆ [n].

(34)

Proposition 2 If Proposition 1 holds, then âˆ€(cid:96)(cid:48) âŠƒ (cid:96), we have

|Ï(cid:96)(Ï„t)| â‰¥ |Ï(cid:96)(cid:48)(Ï„t)|,
|Î·(cid:96)(Ï„t)| â‰¥ |Î·(cid:96)(cid:48)(Ï„t)|,
|Î¸(cid:96)| â‰¥ |Î¸(cid:96)(cid:48)|.

Proof for Proposition 2:

If Proposition 1 holds, we have

Î²AÏ„t

(Ï„t)),

|Ï(cid:96)(Ï„t)| = |x(cid:62)
(cid:96) (y(Ï„t) âˆ’ XAÏ„t
= |x(cid:62)
(cid:96) w(Ï„t)|,
â‰¥ |x(cid:62)
(cid:96)(cid:48) w(Ï„t)|
=: |Ï(cid:96)(cid:48)(Ï„t)|,

if w(Ï„t) â‰¥ 0,

where w(Ï„t) = y(Ï„t) âˆ’ XAÏ„t

(Ï„t). Similarly, we also have

Î²AÏ„t
|Î·(cid:96)(Ï„t)| = |x(cid:62)
(cid:96) XAÏ„t
= |x(cid:62)
(cid:96) v(Ï„t)|,
â‰¥ |x(cid:62)
(cid:96)(cid:48) v(Ï„t)|
=: |Î·(cid:96)(cid:48)(Ï„t)|,

Î½AÏ„t

(Ï„t)|,

if v(Ï„t) â‰¥ 0,

where v(Ï„t) = XAÏ„t

Î½AÏ„t

(Ï„t), and

|Î¸(cid:96)| = |x(cid:62)
(cid:96) b|,
â‰¥ |x(cid:62)
(cid:96)(cid:48) b|
=: |Î¸(cid:96)(cid:48)|,

if b â‰¥ 0,

This completes the proof of Proposition 2.

Proposition 2 will be used to prove Lemma 2. We will prove Lemma 2 by contradiction i.e. we
assume that (33) holds and âˆ€(cid:96)(cid:48) s.t. (cid:96)(cid:48) âŠƒ (cid:96), âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l .
âˆ´ |Ïk(Ï„t)| âˆ’ âˆ†(cid:96)(cid:48)|Î¸k| âˆ’ âˆ†(cid:96)(cid:48)|Î·k(Ï„t)| > |Ïk(Ï„t)| âˆ’ âˆ†âˆ—
> |Ï(cid:96)(Ï„t)| + âˆ†âˆ—
> |Ï(cid:96)(cid:48)(Ï„t)| + âˆ†âˆ—
> |Ï(cid:96)(cid:48)(Ï„t)| + âˆ†(cid:96)(cid:48)|Î¸(cid:96)(cid:48)| + âˆ†(cid:96)(cid:48)|Î·(cid:96)(cid:48)(Ï„t)|, âˆµ âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l .

(cid:96) |Î·k(Ï„t)|, âˆµ âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l
(cid:96) |Î·(cid:96)(Ï„t)|, using (33)
(cid:96) |Î·(cid:96)(cid:48)(Ï„t)|, using Proposition 2,

(cid:96) |Î¸k| âˆ’ âˆ†âˆ—
(cid:96) |Î¸(cid:96)| + âˆ†âˆ—
(cid:96) |Î¸(cid:96)(cid:48)| + âˆ†âˆ—

Therefore, we got

âˆ´ |Ïk(Ï„t)| âˆ’ âˆ†(cid:96)(cid:48)|Î¸k| âˆ’ âˆ†(cid:96)(cid:48)|Î·k(Ï„t)| > |Ï(cid:96)(cid:48)(Ï„t)| + âˆ†(cid:96)(cid:48)|Î¸(cid:96)(cid:48)| + âˆ†(cid:96)(cid:48)|Î·(cid:96)(cid:48)(Ï„t)|
=â‡’ (cid:96)(cid:48) is infeasible (using (33)) =â‡’ âˆ†(cid:96)(cid:48) â‰® âˆ†âˆ—
(cid:96) .

This completes the proof of Lemma 2.

If any of w(Ï„t), v(Ï„t) and b in Proposition 2 contains at least one negative element, then we can
no longer use Lemma 2. Hence, using the following Proposition 3, we can propose Lemma 3 as a
general pruning condition.

14

Proposition 3 We can write

where

|Ï(cid:96)(Ï„t)| â‰¤ b(cid:96),w(Ï„t),
|Î·(cid:96)(Ï„t)| â‰¤ b(cid:96),v(Ï„t),
|Î¸(cid:96)| â‰¤ b(cid:96),Î¸,

b(cid:96),w(Ï„t) = max (cid:8) (cid:88)

|wi(Ï„t)|xi(cid:96),

(cid:88)

|wi(Ï„t)|xi(cid:96)

(cid:9)

wi(Ï„t)<0

b(cid:96),v(Ï„t) = max (cid:8) (cid:88)

|vi(Ï„t)|xi(cid:96),

wi(Ï„t)>0
(cid:88)

|vi(Ï„t)|xi(cid:96)

(cid:9)

vi(Ï„t)<0

b(cid:96),Î¸ = max (cid:8) (cid:88)

|bi|xi(cid:96),

(cid:88)

vi(Ï„t)>0
(cid:9).

|bi|xi(cid:96)

bi<0

bi>0

Proof of Proposition 3: We have

|Ï(cid:96)(Ï„t)| = |x(cid:62)
(cid:96) w(Ï„t)|
n
(cid:88)

wi(cid:96)xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

wi(cid:96)>0
(cid:40)

|wi(cid:96)|xi(cid:96) âˆ’

(cid:88)

wi(cid:96)<0

|wi(cid:96)|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

â‰¤ max

(cid:88)

|wi(cid:96)|xi(cid:96),

(cid:41)

|wi(cid:96)|xi(cid:96)

=: b(cid:96),w(Ï„t).

(cid:88)

wi(cid:96)<0

Similarly,

wi(cid:96)>0

|Î·(cid:96)(Ï„t)| = |x(cid:62)
(cid:96) v(Ï„t)|
n
(cid:88)

vi(cid:96)xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

vi(cid:96)>0
(cid:40)

|vi(cid:96)|xi(cid:96) âˆ’

(cid:88)

vi(cid:96)<0

|vi(cid:96)|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

vi(cid:96)>0

|vi(cid:96)|xi(cid:96),

(cid:88)

vi(cid:96)<0

(cid:41)

|vi(cid:96)|xi(cid:96)

=: b(cid:96),v(Ï„t)

â‰¤ max

and

bixi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Î¸(cid:96)| = |x(cid:62)
(cid:96) b|
n
(cid:88)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

bi>0

|bi|xi(cid:96) âˆ’

(cid:88)

bi<0

|bi|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

â‰¤ max

(cid:40)

(cid:88)

bi>0

|bi|xi(cid:96),

(cid:88)

bi<0

(cid:41)

|bi|xi(cid:96)

=: b(cid:96),Î¸.

This completes the proof of Proposition 3.

Lemma 3 Using Proposition 3 we can show that âˆ€(cid:96)(cid:48) âŠƒ (cid:96), if

b(cid:96),w(Ï„t) + âˆ†âˆ—

(cid:96) b(cid:96),Î¸ + âˆ†âˆ—

(cid:96) b(cid:96),v(Ï„t) < |Ïk(Ï„t)| âˆ’ âˆ†âˆ—

(cid:96) |Î¸k| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Ï„t)|,

(35)

then âˆ†(cid:96)(cid:48) > âˆ†âˆ—
(cid:96) .

15

Before proving Lemma 3, we introduce Proposition 4 which will be used to prove Lemma 3:

Proposition 4 If Proposition 1 holds, we have

b(cid:96),w(Ï„t) â‰¥ b(cid:96)(cid:48),w(Ï„t),
b(cid:96),v(Ï„t) â‰¥ b(cid:96)(cid:48),v(Ï„t),
b(cid:96),Î¸ â‰¥ b(cid:96)(cid:48),Î¸.

Proof of Proposition 4:

If Proposition 1 holds, we have

b(cid:96),w(Ï„t) = max (cid:8) (cid:88)

|wi(Ï„t)|xi(cid:96),

(cid:88)

|wi(Ï„t)|xi(cid:96)

(cid:9)

wi(Ï„t)<0

wi(Ï„t)>0
(cid:88)

â‰¥ max (cid:8) (cid:88)

|wi(Ï„t)|xi(cid:96)(cid:48),

|wi(Ï„t)|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),w(Ï„t).

Similarly, we also have

wi(Ï„t)<0

wi(Ï„t)>0

b(cid:96),v(Ï„t) = max (cid:8) (cid:88)

|vi(Ï„t)|xi(cid:96),

(cid:88)

|vi(Ï„t)|xi(cid:96)

(cid:9)

vi(Ï„t)<0

â‰¥ max (cid:8) (cid:88)

vi(Ï„t)>0
(cid:88)

|vi(Ï„t)|xi(cid:96)(cid:48),

vi(Ï„t)<0

vi(Ï„t)>0

|vi(Ï„t)|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),v(Ï„t).

and

b(cid:96),Î¸ = max (cid:8) (cid:88)

|bi|xi(cid:96),

(cid:88)

(cid:9)

|bi|xi(cid:96)

bi<0

â‰¥ max (cid:8) (cid:88)

bi>0
(cid:88)

|bi|xi(cid:96)(cid:48),

|bi|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),Î¸.

bi<0

bi>0

Proof of Lemma 3: We will prove Lemma 3 by contradiction i.e. we assume that (35) holds and
âˆ€(cid:96)(cid:48) s.t. (cid:96)(cid:48) âŠƒ (cid:96), âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l .
âˆ´ |Ïk(Ï„t)| âˆ’ âˆ†(cid:96)(cid:48)|Î¸k| âˆ’ âˆ†(cid:96)(cid:48)|Î·k(Ï„t)| > |Ïk(Ï„t)| âˆ’ âˆ†âˆ—
> b(cid:96),w(Ï„t) + âˆ†âˆ—
> b(cid:96)(cid:48),w(Ï„t) + âˆ†âˆ—
> b(cid:96)(cid:48),w(Ï„t) + âˆ†(cid:96)(cid:48)b(cid:96)(cid:48),Î¸ + âˆ†(cid:96)(cid:48)b(cid:96)(cid:48),v(Ï„t), âˆµ âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l .

(cid:96) |Î·k(Ï„t)|, âˆµ âˆ†(cid:96)(cid:48) < âˆ†âˆ—
l
(cid:96) b(cid:96),v(Ï„t), using (35)
(cid:96) b(cid:96)(cid:48),v(Ï„t), using Proposition 3,

(cid:96) |Î¸k| âˆ’ âˆ†âˆ—
(cid:96) b(cid:96),Î¸ + âˆ†âˆ—
(cid:96) b(cid:96)(cid:48),Î¸ + âˆ†âˆ—

Therefore, we got

|Ïk(Ï„t)| âˆ’ âˆ†(cid:96)(cid:48)|Î¸k| âˆ’ âˆ†(cid:96)(cid:48)|Î·k(Ï„t)| > b(cid:96)(cid:48),w(Ï„t) + âˆ†(cid:96)(cid:48)b(cid:96)(cid:48),Î¸ + âˆ†(cid:96)(cid:48)b(cid:96)(cid:48),v(Ï„t)
=â‡’ (cid:96)(cid:48) is infeasible (using (35)) =â‡’ âˆ†(cid:96)(cid:48) â‰® âˆ†âˆ—
(cid:96) .

This completes the proof of Lemma 3.

Hence, if the pruning condition in Lemma 3 holds, then we do not need to search the sub-tree with (cid:96)
as the root node, and hence increasing the efï¬ciency of the search procedure [Tsuda (2007)].

16

Î²AÎ»t

(Î»t+1) âˆ’ Î²AÎ»t
)âˆ’1sAÎ»t

A.2 LASSO: Î»-path

A.2.1 Î»-path: path w.r.t. to Î» (Ï„ ï¬xed)

Since Ï„ is ï¬xed, we drop it from the notation. The normal equation of the Î»-path can be written as
âˆ’X T (y âˆ’ XÎ²) + Î»s(Î») = 0

where, s(Î») is the sub-differential deï¬ned as

sj(Î») âˆˆ

(cid:26){âˆ’1, +1},
[âˆ’1, +1],

if Î²j(Î») (cid:54)= 0
if Î²j(Î») = 0.

Now, if we consider two Î» values (Î»t > Î»t+1) at which the active set does not change (i.e. AÎ»t =
AÎ»t+1) and the sign of the active coefï¬cients also remain the same (i.e. sAÎ»t
(Î»t+1)) ,
then we can write

(Î»t) = sAÎ»t+1

(Î»t) = âˆ’Î½AÎ»t

(Î»t)(Î»t+1 âˆ’ Î»t)

(36)

(Î»t) = (X T

AÎ»t

XAÎ»t

(Î»t). The derivation of Î½AÎ»t

where, Î½AÎ»t
(Î»t) is given in A.2.2. Note
that Î½AÎ»(Î») is constant for all real values of Î» âˆˆ [Î»t, Î»t+1) and thus, equation (36) states that Î²(Î»)
is piece-wise linear in Î» for a ï¬xed Ï„ . To draw the curve of solutions as a function of Î», we need to
check when the active set changes. If Î»t+1 is the next zero crossing point then either of the following
two events happens.
â€¢ A zero variable becomes non-zero i.e. âˆƒj âˆˆ Ac
Î»t

(Î»t+1))| = Î»t+1 or,

j (y âˆ’ XAÎ»t

Î²AÎ»t

: |x(cid:62)

s.t.

â€¢ A non zero variable becomes zero i.e. âˆƒj âˆˆ AÎ»t s.t. : Î²j(Î»t) (cid:54)= 0 but Î²j(Î»t+1) = 0 .
Overall, the next change of the active set happens at Î»t+1 = Î»t âˆ’ âˆ†j, where

âˆ†j = min(âˆ†1

j , âˆ†2

j ) = min

(cid:32)

min
jâˆˆAc
Î»t

(cid:16) (xk Â± xj)T w(Î»t)
(xk Â± xj)T v(Î»t)

(cid:17)

, min
jâˆˆAÏ„t

++

(cid:16)

âˆ’

Î²j(Î»t)
Î½j(Î»t)

(cid:17)

++

(cid:33)

.

(37)

where, w(Î»t) = y âˆ’ XAÎ»t
inclusion (âˆ†1

Î²AÎ»t
j ) is given in A.2.3.

(Î»t), and v(Î»t) = XAÎ»t

Î½AÎ»t

(Î»t). The derivation of the step-size of

A.2.2 Direction vector (Î»-path)

Letâ€™s consider the normal equation at Î»t and Î»t+1 for the active components

âˆ’X (cid:62)

AÎ»t

(y âˆ’ XAÎ»t

Î²AÎ»t

(Î»t)) + Î»tsAÎ»t

(Î»t) = 0,

âˆ’X (cid:62)

AÎ»t

(y âˆ’ XAÎ»t

Î²AÎ»t

(Î»t+1)) + Î»tsAÎ»t

(Î»t) = 0.

(38)

(39)

Note that AÎ»t = AÎ»t+1 and sAÎ»t
write

(Î»t) = sAÎ»t

(Î»t+1). Therefore, subtracting (38) from (39) one can

where, we deï¬ned Î½AÎ»t

Î²AÎ»t
(Î»t) = (X (cid:62)

(Î»t+1) âˆ’ Î²AÎ»t
XAÎ»t

AÎ»t

(Î»t)(Î»t+1 âˆ’ Î»t),

(Î»t) = âˆ’Î½AÎ»t
)âˆ’1sAÎ»t
(Î»t).

A.2.3 Step-size of inclusion (Î»-path)

The optimality condition for the active features j âˆˆ AÎ»t of the Î»-path of LASSO can be written as

(Î»t)(cid:1) + Î»s(Î²j) = 0,

âˆ’x(cid:62)
j

(cid:0)y âˆ’ XAÎ»t
x(cid:62)
j
(cid:12)
(cid:12)x(cid:62)
j

Î²AÎ»t
(cid:0)y âˆ’ XAÎ»t
(cid:0)y âˆ’ XAÎ»t

(Î»t)(cid:1) = Î»sj,
(Î»t)(cid:1)(cid:12)
(cid:12) = Î».
Therefore, at any step (Î»t â†’ Î»t âˆ’ âˆ†j) any non-active feature (j âˆˆ Ac
Î»t
following condition is satisï¬ed i.e.
(cid:12)
(cid:12)x(cid:62)
j

Î²AÎ»t
Î²AÎ»t

âˆ´

(Î²AÎ»t

(cid:0)y âˆ’ XAÎ»t
(cid:12)
(cid:12)x(cid:62)

(Î»t) + âˆ†jÎ½(Î»t))(cid:1)(cid:12)
v(Î»t))(cid:12)

(cid:12) = (cid:12)
(cid:12) = (cid:12)

(cid:0)y âˆ’ XAÎ»t
(cid:12)x(cid:62)
(Î²AÎ»t
k
(cid:12)x(cid:62)
k (w(Î»t) âˆ’ âˆ†jXAÎ»t

(Î»t) + âˆ†jÎ½(Î»t))(cid:1)(cid:12)
v(Î»t))(cid:12)
(cid:12) ,

j (w(Î»t) âˆ’ âˆ†jXAÎ»t

) becomes active when the

(cid:12) , âˆ€j âˆˆ Ac
Î»t

, âˆ€k âˆˆ AÎ»t

17

where w(Î»t) = y âˆ’ XAÎ»t
negative terms separately one can write the step-size of inclusion as -

(Î»t) and v(Î»t) = XAÎ»t

Î²AÎ»t

Î½AÎ»t

(Î»t). Now, considering the positive and

âˆ†1

j = min
jâˆˆAc
Î»t

(cid:32)

(xk Â± xj)(cid:62)w(Î»t)
(xk Â± xj)(cid:62)v(Î»t)

(cid:33)
.

A.2.4 Tree pruning (Î»-path)

The derivation of this pruning condition is also given in (Tsuda, 2007). However, here we provide the
same derivation in our notation to make it self-contained. Similar to (18), the pruning condition of
the Î»-path can be written as

|Ï(cid:96)(Î»t)|+âˆ†âˆ—

(cid:96) |Î·(cid:96)(Î»t)|< |Ïk(Î»t)|âˆ’âˆ†âˆ—
(cid:96) v(Î»t) âˆ€(cid:96) âˆˆ Ac
Î»t

(cid:96) |Î·k(Î»t)|,
, Ïk(Î»t) = x(cid:62)

(cid:96) w(Î»t) and Î·(cid:96)(Î»t) = x(cid:62)

âˆ€k âˆˆ AÎ»t . Now similar to the Proposition 3 we can also write

where Ï(cid:96)(Î»t) = x(cid:62)
x(cid:62)
k v(Î»t),

(40)

k w(Î»t) and Î·k(Î»t) =

Proposition 5 Using the tree anti-monotonicity property (34) we can easily show that âˆ€(cid:96)(cid:48) s.t. (cid:96)(cid:48) âŠƒ (cid:96),
the following conditions are satisï¬ed i.e.

|Ï(cid:96)(cid:48)(Î»t)| = |

(cid:88)

wi(Î»t)xi(cid:96)(cid:48)| â‰¤ max(cid:8) (cid:88)

|wi(Î»t)|xi(cid:96),

(cid:88)

|wi(Î»t)|xi(cid:96)

(cid:9) := bw(Î»t),

|Î·(cid:96)(cid:48)(Î»t)| = |

i
(cid:88)

i

vi(Î»t)xi(cid:96)(cid:48)| â‰¤ max(cid:8) (cid:88)

|vi(Î»t)|xi(cid:96),

wi(Î»t)<0

wi(Î»t)>0
(cid:88)

|vi(Î»t)|xi(cid:96)

(cid:9) := bv(Î»t).

vi(Î»t)<0

vi(Î»t)>0

Therefore, similar to the Ï„ -path (35) the pruning condition of the Î»-path (40) can be written as

bw(Î»t) + âˆ†âˆ—

(cid:96) bv(Î»t) < |Ïk(Î»t)| âˆ’ âˆ†âˆ—

(cid:96) |Î·k(Î»t)|.

(41)

The

complete

algorithm for

the

selection path (Î»-path)

is given in Algorithm2.

Algorithm 2: Î»-path
1: Input: Z, y, Î».
2: Initialization: Î»t = Î»max = ||X (cid:62)y||âˆ, AÎ»t = arg max

|X (cid:62)y|j,

Î²(Î»t) = 0,

(Î»t)), Î½Ac
Î»t

j
(Î»t) = 0.

AÎ»t

XAÎ»t

(Î»t) = (X (cid:62)

)âˆ’1sign(Î²AÎ»t

3: Î½AÎ»t
4: while Î»t â‰¥ Î»
do.
5: Compute step-length âˆ†j â† Equation (37).
6: If âˆ†j = âˆ†1
j , add j into AÎ»t.
7: If, âˆ†j = âˆ†2
j , remove j from AÎ»t.
8: Update: Î»t â† Î»t âˆ’ âˆ†j, Î²AÎ»t
(Î»t+1) = (X (cid:62)

(Î»t+1) â† Î²AÎ»t

XAÎ»t+1

AÎ»t+1

Î½AÎ»t+1
9: end while
10: Output: Î²(Î»), AÎ».

)âˆ’1sign(Î²AÎ»t+1

(Î»t) + âˆ†jÎ½AÎ»t
(Î»t+1)),

(cid:46) Inclusion
(cid:46) Deletion.

(Î»t),
Î½Ac

Î»t+1

(Î»t+1) = 0.

18

B Extension for Elastic Net (ElNet)

A common problem of the LASSO is that if the data has correlated features then, the LASSO picks
only one of them and ignores the rest, which leads to instability. To solve this problem Zou and
Hastie (2005) proposed the Elastic Net (ElNet). This feature correlation problem is very much evident
in SHIM type problem, and hence we extended our framework for the Elastic Net. To extend our
framework for the Elastic Net, we need to solve the following optimization problem.

Î²(Î», Ï„ ) âˆˆ arg min

Î²âˆˆRp

1
2

(cid:107)y(Ï„ ) âˆ’ XÎ²(cid:107)2

2 +

1
2

Î± (cid:107)Î²(cid:107)2

2 + Î» (cid:107)Î²(cid:107)1 .

(42)

B.1 Î»-path: path w.r.t. to Î» (Ï„ ï¬xed)

Similar to the LASSO, the normal equation can be written as

âˆ’X (cid:62) (y âˆ’ XÎ²(Î»)) + Î±Î²(Î») + Î»s(Î») = 0.

where, s(Î») is the sub-differential that can be deï¬ned in a similar fashion as done in the case of the
Î»-path for the LASSO (A.2.1). Now, if we consider two Î» values (Î»t > Î»t+1) at which the active set
does not change (i.e. AÎ»t = AÎ»t+1) and the sign of the active coefï¬cients also remain the same (i.e.
sAÎ»t

(Î»t+1)) , then we can write

(Î»t) = sAÎ»t

Î²AÎ»t

(Î»t+1) âˆ’ Î²AÎ»t
XAÎ»t

(Î»t) = âˆ’Î½AÎ»t

(Î»t)(Î»t+1 âˆ’ Î»t),

(43)

AÎ»t

(Î»t) = (X (cid:62)

+ Î±I|AÎ»t |)âˆ’1sAÎ»t

(Î»t). Now, similar to the LASSO we can derive the step-size of deletion (âˆ†2

where, Î½AÎ»t
(Î»t). Note that here the only change in the
direction vectors compared to the LASSO is the addition of an Î±I|AÎ»t | term to the expression of
Î½AÎ»t
j ) considering this
updated expression of the direction vector. However, to derive the step-size of inclusion (âˆ†1
j ), we
need a different approach. The elastic net optimization problem can actually be formulated as a
LASSO optimization problem using augmented data. If we consider an augmented data deï¬ned as
ËœX =

, then solving the elastic net optimization problem (42) for a ï¬xed Ï„ , is

and Ëœy =

(cid:19)

(cid:19)

(cid:18) Xâˆš
Î±Ip

(cid:18)y
0

equivalent to solving the following problem.

Î²(Î») âˆˆ arg min

Î²âˆˆRp

1
2

(cid:13)
(cid:13)Ëœy âˆ’ ËœXÎ²
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+ Î» (cid:107)Î²(cid:107)1 .

(44)

Now, similar to the LASSO we can write the step-size of inclusion (âˆ†1
the augmented data ( ËœX, Ëœy) as

j ) of the Î»-path of ElNet using

(cid:32)

âˆ†1

j = min
jâˆˆAc
Î»t

(Ëœxj âˆ’ Ëœxk)(cid:62) Ëœw(Î»t)
(Ëœxj âˆ’ Ëœxk)(cid:62)Ëœv(Î»t)

,

(Ëœxj + Ëœxk)(cid:62) Ëœw(Î»t)
(Ëœxj + Ëœxk)(cid:62)Ëœv(Î»t)

(cid:33)

.

(45)

However, we cannot just simply augment the data by stacking extra rows as this can be prohibitively
expensive due to the combinatorial effects. In order to derive the step-size of inclusion (âˆ†1
j ) we need
a different approach as we construct the high-order interaction model in a progressive manner. We
have shown that using the following approach the step-size of inclusion for the Î»-path of ElNet can
be computed very efï¬ciently, where the step-size of inclusion can be deï¬ned as

(cid:32)

âˆ†1

j = min
jâˆˆAc
Î»t

(xj âˆ’ xk)(cid:62)w(Î»t) + Î±Î²k
(xj âˆ’ xk)(cid:62)v(Î»t) âˆ’ Î±Î½k

,

(xj + xk)(cid:62)w(Î»t) âˆ’ Î±Î²k
(xj + xk)(cid:62)v(Î»t) + Î±Î½k

(cid:33)
.

(46)

The derivation of the above step-size (âˆ†1

j ) is given below.

Proof 2 Lets, consider Ëœw(Î»t) = Ëœyâˆ’ ËœXAÎ»t
where p = |AÎ»t| + |Ac
Î»t

|, then we can write

Î²AÎ»t

(Î»t) âˆˆ Rn+p and w(Î»t) = yâˆ’XAÎ»t

Î²AÎ»t

(Î»t) âˆˆ Rn,

Ëœwi(Î»t) =

ï£±
ï£²

wi(Î»t)
âˆš
âˆ’
Î±Î²j
ï£³
0

i â‰¤ n,

if
if n < i â‰¤ n + |AÎ»t|,
if n + |AÎ»t| < i â‰¤ n + p.

(47)

19

similarly considering Ëœv(Î»t) = ËœXÎ½(Î»t) âˆˆ Rn+p and v(Î»t) = XÎ½(Î»t) âˆˆ Rn, we can write

Ëœvi(Î»t) =

ï£±
ï£²

ï£³

vi(Î»t)
âˆš
Î±Î½j

0

i â‰¤ n,

if
if n < i â‰¤ n + |AÎ»t|,
if n + |AÎ»t| < i â‰¤ n + p.

and, considering ËœX âˆˆ Rn+p and X âˆˆ Rn we can write

Ëœxij =

ï£±
ï£²

xij
âˆš
Î±

ï£³

0

i â‰¤ n,
i > n and (i âˆ’ n) = j,

if
if
otherwise.

Therefore, in (45) we can write that âˆ€j âˆˆ Rp

(48)

(49)

Ëœx(cid:62)
j Ëœw(Î»t) =

=

n+p
(cid:88)

i=1

n
(cid:88)

i=1

Ëœwi(Î»t)Ëœxij,

Ëœwi(Î»t)Ëœxij +

n+|AÎ»t |
(cid:88)

i=n+1

Ëœwi(Î»t)Ëœxij +

n+p
(cid:88)

Ëœwi(Î»t)Ëœxij.

i=n+|AÎ»t |+1

Now, using (47) and (49) the second and the third quantity in the above expression can be written as

n+|AÎ»t |
(cid:88)

i=n+1

Ëœwi(Î»t)Ëœxij =

âˆš

Î±Î²j)(

Î±),

âˆš

(cid:26)(âˆ’
0

if (i âˆ’ n) = j,
otherwise.

and,

Therefore,

Ëœx(cid:62)
j Ëœw(Î»t) =

n
(cid:88)

i=1

n+p
(cid:88)

i=n+|AÎ»t |+1

Ëœwi(Î»t)Ëœxij = 0.

wi(Î»t)xij, âˆ€j âˆˆ Ac

Î»t and Ëœx(cid:62)

k Ëœw(Î»t) =

n
(cid:88)

i=1

wi(Î»t)xik âˆ’ Î±Î²k, âˆ€k âˆˆ AÎ»t.

Similarly, using (48) and (49) we can write

Ëœx(cid:62)
j Ëœv(Î»t) =

n
(cid:88)

i=1

vi(Î»t)xij, âˆ€j âˆˆ Ac
Î»t

and

Ëœx(cid:62)
k Ëœv(Î»t) =

n
(cid:88)

i=1

vi(Î»t)xik + Î±Î½k, âˆ€k âˆˆ AÎ»t.

Therefore the step-size of inclusion can be written as

(cid:32)

âˆ†1

j = min
jâˆˆAc
Î»t

(xj âˆ’ xk)(cid:62)w(Î»t) + Î±Î²k
(xj âˆ’ xk)(cid:62)v(Î»t) âˆ’ Î±Î½k

,

(xj + xk)(cid:62)w(Î»t) âˆ’ Î±Î²k
(xj + xk)(cid:62)v(Î»t) + Î±Î½k

(cid:33)
.

(50)

B.1.1 Tree pruning (Î»-path)

Similar to the LASSO (40) we can use the following inequality in augmented data ( ËœX, Ëœy) as the
pruning criteria for the Î»-path of ElNet.

|ËœÏ(cid:96)|+âˆ†âˆ—
(cid:96) Ëœv(Î»t) âˆ€(cid:96) âˆˆ Ac
where, ËœÏ(cid:96) = Ëœx(cid:62)
Î»t
AÎ»t . Now, using (47), (48) and (49) we can show that

(cid:96) Ëœw(Î»t) and ËœÎ·(cid:96) = Ëœx(cid:62)

(cid:96) |ËœÎ·(cid:96)|< |ËœÏk|âˆ’âˆ†âˆ—

(cid:96) |ËœÎ·k|,
, ËœÏk = Ëœx(cid:62)

k Ëœw(Î»t) and ËœÎ·k = Ëœx(cid:62)

k Ëœv(Î»t),

(51)

âˆ€k âˆˆ

ËœÏ(cid:96)(Î»t) =

n
(cid:88)

i=1

wi(Î»t)xi(cid:96), ËœÎ·(cid:96)(Î»t) =

n
(cid:88)

i=1

vi(Î»t)xi(cid:96) and,

ËœÏk(Î»t) =

n
(cid:88)

i=1

wi(Î»t)xik âˆ’ Î±Î²k, ËœÎ·k(Î»t) =

n
(cid:88)

i=1

vi(Î»t)xik + Î±Î½k.

20

Therefore, the pruning condition (51) can be redeï¬ned as -

n
(cid:88)

|

i=1

wi(Î»t)xi(cid:96)| + âˆ†âˆ—
(cid:96) |

n
(cid:88)

i=1

vi(Î»t)xi(cid:96)| < |

n
(cid:88)

i=1

wi(Î»t)xik âˆ’ Î±Î²k| âˆ’ âˆ†âˆ—
(cid:96) |

n
(cid:88)

i=1

vi(Î»t)xik + Î±Î½k|.

Now, similar to the LASSO (41) we can also write

bw(Î»t) + âˆ†âˆ—

(cid:96) bv(Î»t) < |Â¯Ïk(Î»t)| âˆ’ âˆ†âˆ—

(cid:96) |Â¯Î·k(Î»t)|,

(52)

where, Â¯Ïk(Î»t) = (cid:80)n

i=1 wi(Î»t)xik âˆ’ Î±Î²k, Â¯Î·k(Î»t) = (cid:80)n
bw(Î»t) = max(cid:8) (cid:88)

|wi(Î»t)|xi(cid:96),

i=1 vi(Î»t)xik + Î±Î½k, and
(cid:9),

|wi(Î»t)|xi(cid:96)

(cid:88)

wi(Î»t)<0

bv(Î»t) = max(cid:8) (cid:88)

|vi(Î»t)|xi(cid:96),

wi(Î»t)>0
(cid:88)

|vi(Î»t)|xi(cid:96)

(cid:9).

vi(Î»t)<0

vi(Î»t)>0

Therefore, (52) can be used as the pruning condition for the Î»-path of ElNet.

B.2 Ï„ -path: path w.r.t. to Ï„ (Î» ï¬xed)

If we consider two real values Ï„t and Ï„t+1 ( Ï„t+1 > Ï„t) at which the active set does not change and
their signs also remain the same, then we can write

Î²AÏ„t

(Ï„t+1) âˆ’ Î²AÏ„t
(Ï„t+1) âˆ’ Î»sAc
Ï„t

(Ï„t) = Î½AÏ„t
(Ï„t) = Î³Ac
Ï„t

(Ï„t)(Ï„t+1 âˆ’ Ï„t),
(Ï„t)(Ï„t+1 âˆ’ Ï„t).

Î»sAc
Ï„t

XAÏ„t

(Ï„t) = (X (cid:62)
AÏ„t

+ Î±I|AÏ„t |)âˆ’1X (cid:62)
where, Î½AÏ„t
(Ï„t).
AÏ„t
Note that here also the only change compared to the LASSO (A.1.1) is the addition of an Î±I|AÏ„t |
term to the expression of Î½AÏ„t
. Now, one can also derive a similar expression of step-size of inclusion
and deletion as done for the LASSO (A.1.2) by considering the updated expression of Î½AÏ„t
(Ï„t) and
Î³Ac
Ï„t

(Ï„t) = X (cid:62)
Ac
Ï„t

b âˆ’ X (cid:62)
Ac
Ï„t

b and Î³Ac
Ï„t

XAÏ„t

Î½AÏ„t

(Ï„t).

B.2.1 Tree pruning (Ï„ -path)

Similar to the LASSO (35), by using (47), (48) and (49) the pruning condition for the Ï„ -path of ElNet
can be written as

where Â¯Ïk(Ï„t) = (cid:80)n

b(cid:96),w(Ï„t) + âˆ†âˆ—
(cid:96) b(cid:96),Î¸ + âˆ†âˆ—
i=1 wi(Ï„t)xik âˆ’ Î±Î²k, Â¯Î·k(Ï„t) = (cid:80)n

i=1 vi(Ï„t)xik + Î±Î½k.

(cid:96) b(cid:96),v(Ï„t) < |Â¯Ïk(Ï„t)|âˆ’âˆ†âˆ—

(cid:96) |Î¸k|âˆ’âˆ†âˆ—

(cid:96) |Â¯Î·k(Ï„t)|,

(53)

21

C Additional Results

Here we report additional results using real world HIV-1 sequence data from Stanford HIV Drug
Resistance Database (Rhee et al., 2003). This dataset contains three classes of drug data: NRTIs,
NNRTIs and PIs consisting of 16 drugs. Finding virus induced mutations which leads to drug resis-
tance is crucial to drug development. However, drug resistance is a complex biological phenomenon
and it is often reported in the literature (Rhee et al., 2006; Tsuda, 2007; Suzumura et al., 2017), that
it is the association of multiple mutations along with some crucial single mutations that can best
describe the phenomenon. Hence, it is important to understand the association of multiple mutations
related to the drug resistance. In our experiment we used 6 NRTIs, 1 NNRTIs and 3 PIs drugs. We
reported the results on 3 NRTIs drugs in the main article and here we include the results on the
remaining 3 NRTIs (Fig. 5) and 3 PIs (Fig. 6) and 1 NNRTI (7) drugs. The continuous drug resistance
values corresponds to the response (y âˆˆ R) and the binary mutations corresponds to the original
features (z âˆˆ Rm) in our experimental settings.
In Table. 2 we demonstrated the computational

Figure 5: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conï¬dence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conï¬dence interval lengths of the same experiments.

Figure 6: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conï¬dence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conï¬dence interval lengths of the same experiments.

22

(b.1)(b.2)(b.3)(a.1)(a.3)(a.2)DATA_NRTI_TDF_extop30ğ’P-value[# intervals]CI [# intervals]1000.74 [2.22]1.0 [2.22]2000.63 [2.78]1.0 [2.78]3000.58 [3.76]1.0 [3.76]DATA_NRTI_DDI_extop30ğ’P-value[# intervals]CI [# intervals]1000.84 [2.23]1.0 [2.23]2000.90 [2.00]1.0 [2.00]3000.83 [2.10]1.0 [2.10]DATA_NRTI_X3TC_extop30ğ’P-value[# intervals]CI [# intervals]1000.70 [5.24]0.99 [5.24]2000.75 [7.58]1.0 [7.58]3000.80 [8.86]1.0 [8.86]DATA_PI_IDV_extop30ğ’P-value[# intervals]CI [# intervals]1000.60 [2.81]1.0 [2.81]2000.64 [4.68]1.0 [4.68]3000.67 [5.74]1.0 [5.74]DATA_PI_ATV_extop30ğ’P-value[# intervals]CI [# intervals]1000.66 [3.81]1.0 [3.81]2000.71 [6.35]1.0 [6.35]3000.75 [7.91]1.0 [7.91]DATA_PI_APV_extop30ğ’P-value[# intervals]CI [# intervals]1000.64 [2.43]1.0 [2.43]2000.66 [3.78]1.0 [3.78]3000.67 [5.29]1.0 [5.29](b.1)(b.2)(b.3)(a.1)(a.2)(a.3)Figure 7: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conï¬dence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conï¬dence interval lengths of the same experiments.

advantage of the proposed homotopy method over exiting method on conditioning on model (Lee
et al. (2016)). In this experiment the Î»-path was constructed until the active set (A) contains 20
features and subsequently that active set and the corresponding Î» value is used for the construction
of the Ï„ -path. The Lee et al. (2016) method needs to consider the union of all possible signs in the
observed active set (A) in order to condition on the model. However, our homotopy mining needs to
consider only âˆ¼ 120 polytopes (worst case) for the same task.

High-order interactions
1st
2nd
3rd

Homotopy
(# kinks)
104.15 Â± 10.73
101.0 Â± 4.64
78.33 Â± 24.69

Polytope
(# polytopes)
220
220
220

Table 2: Comparison of computational efï¬ciencies of the proposed homotopy method against existing
polytope method. The "# kinks" represents the average number of kinks encountered during the
construction of Ï„ -path for each test statistic direction, whereas the "# polytopes" represents the
number of all possible signs one needs to consider to condition on the model.

We note that theoretically, in the worst-case, the complexity of the homotopy method grows exponen-
tially. This is a common issue in homotopy-based methods such as computing regularization paths.
However, fortunately, it has been well-recognized (Le Duy and Takeuchi, 2021) that this worst case
rarely happens in practice, and this is also evident from our experimental results.

Similar to the pruning, empirical evidence also demonstrates that homotopy is more efï¬cient in case
of high-order interaction terms compared to that of singleton terms, and the efï¬ciency increases as
the order of interaction increases. We suspect that as the order of interaction increases the sparsity of
the data also increases which signiï¬cantly affects the construction of the Ï„ -path as evident from the
effectiveness of both pruning and the homotopy method. However, more theoretical investigations
are required to have a clear understanding of this phenomenon which we believe worth considering
in the future.

23

(b.1)(a.1)DATA_NNRTI_NVP_extop30ğ’P-value[# intervals]CI [# intervals]1000.70 [6.36]1.0 [6.36]2000.70 [9.05]1.0 [9.05]3000.74 [10.26]1.0 [10.26]