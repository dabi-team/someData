1
2
0
2

n
u
J

9

]
L
M

.
t
a
t
s
[

1
v
9
2
9
4
0
.
6
0
1
2
:
v
i
X
r
a

Fast and More Powerful Selective Inference for
Sparse High-order Interaction Model

Diptesh Das
Nagoya Institute of Technology
das.diptesh@nitech.ac.jp

Vo Nguyen Le Duy
Nagoya Institute of Technology / RIKEN
duy.mllab.nit@gmail.com

Hiroyuki Hanada
RIKEN
hiroyuki.hanada@riken.jp

Koji Tsuda
University of Tokyo / RIKEN
tsuda@k.u-tokyo.ac.jp

Ichiro Takeuchi
Nagoya Institute of Technology / RIKEN
takeuchi.ichiro@nitech.ac.jp

Abstract

Automated high-stake decision-making such as medical diagnosis requires models
with high interpretability and reliability. As one of the interpretable and reliable
models with good prediction ability, we consider Sparse High-order Interaction
Model (SHIM) in this study. However, ﬁnding statistically signiﬁcant high-order
interactions is challenging due to the intrinsic high dimensionality of the combi-
natorial effects. Another problem in data-driven modeling is the effect of "cherry-
picking" a.k.a. selection bias. Our main contribution is to extend the recently
developed parametric programming approach for selective inference to high-order
interaction models. Exhaustive search over the cherry tree (all possible interactions)
can be daunting and impractical even for a small-sized problem. We introduced
an efﬁcient pruning strategy and demonstrated the computational efﬁciency and
statistical power of the proposed method using both synthetic and real data.

1

Introduction

Blackbox models such as deep neural network models generally have high predictive performance
but are difﬁcult to interpret and hence, often considered unreliable. Therefore, for tasks that require
high-stake decision-making, such as medical diagnosis and automated driving, models with higher
interpretability and reliability are required. As one of the interpretable and reliable models with good
prediction ability, we consider Sparse High-order Interaction Model (SHIM) in this study. Considering
a regression problem with a response y and m original covariates z1, . . . , zm, an example SHIM up
to 4th order interactions can be written as

y = β1z3 + β2z5 + β3z2z6 + β4z1z2z5z9.

(1)

where β1, β2, β3, β4 are the model parameters (or coefﬁcients). Such a SHIM has practical importance,
such as identifying complex genotypic features for HIV-1 drug resistance (Saigo et al., 2007). HIV-1
evolves in the human body and exposure to certain drugs causes mutations that leads to resistance
against the drugs. Structural biological studies show that it is the association of multiple mutations
along with some crucial single mutations that can best describe the complex biological phenomenon
of drug resistance (Vivet-Boudou et al., 2006; Iversen et al., 1996; Rhee et al., 2006).

Preprint. Under review.

 
 
 
 
 
 
The goal of this study is to ﬁt a SHIM such as (1) to the given data and subsequently perform
statistical signiﬁcance test to judge the reliability of the model parameters. However, unless the
original dimension and the order of interactions are small, ﬁtting a high-order interaction model can
be challenging and one would require some computational tricks to avoid the combinatorial effects.

Another challenge of data-driven modeling is understanding the reliability of ﬁndings because the
model might have cherry-picked the strong associations given a particular realization of the data.
This is called "cherry-picking" effect a.k.a. selection bias (Taylor and Tibshirani, 2015). Traditional
statistical inference, which assumes that the statistical model and the target for which inferences are
conducted must be ﬁxed a priori, cannot be used for this problem. Any inference conducted after
model selection will suffer from the selection bias unless it is corrected.

Related works: Several approaches have been suggested in the literature to address the above
problem (Fithian et al. (2014, 2015), Choi et al. (2017), Tian and Taylor (2018), Chen and Bien
(2020), Hyun et al. (2018), Loftus and Taylor (2014, 2015), Panigrahi et al. (2016), Tibshirani et al.
(2016), Yang et al. (2016)). A particularly notable approach is conditional SI introduced in the
seminal paper by Lee et al. (2016). The basic idea of conditional SI is to make inference on a
data-driven hypothesis conditional on the selection event that the hypothesis is selected. Lee et al.
(2016) ﬁrst proposed conditional SI methods for the selected features by using Lasso. Their basic
idea is to characterize the selection event by a polytope, i.e., a set of linear inequalities, in the sample
space. When a selection event can be characterized by a polytope, practical computational methods
developed by these authors can be used for making inferences of the selected hypotheses conditional
on the selection events.

However, the conditional SI framework based on a polytope has a serious drawback called over-
conditioning issue, i.e., additional extra events must be introduced to characterize the selection event
by a single polytope, which is known to lead loss of statistical power or statistically sub-optimal
Fithian et al. (2014). The work by Suzumura et al. (2017), who ﬁrst applied polytope-based SI into
high-order interaction model when a high-order interaction feature is sequentially added to the model,
also suffers from this problem. As a solution in the case of LASSO Lee et al. (2016) proposed to take
the union of all possible signs of the selected features. However, unless the number of the selected
features is small, it is computationally expensive and, in the case of SHIM type problem, it will be
impractical due to the combinatorial effects.

Recently, Le Duy and Takeuchi (2021) introduced a homotopy method to resolve the over-conditioning
issue and realizes minimally-conditioned SI for Lasso. Our basic idea for identifying statistically
reliable high-order interaction features in sparse modeling framework is to employ exact homotopy-
based SI method for SHIM. Unfortunately, the computational cost for applying the exact homotopy
method to SHIM increases exponentially and intractable unless the size of the selected features and
the maximum order of interactions are fairly small. Several methods have already been proposed for
ﬁtting a SHIM (Saigo et al., 2009; Tsuda, 2007; Nakagawa et al., 2016).

Contribution: Our main contribution in this paper is to introduce a “homotopy mining” method by
exploting the best of both homotopy and (pattern) mining methods for conditional SI for SHIM. This
approach is motivated by the exact regularization path computation algorithm for graph data (Tsuda,
2007), which is considered as a homotopy method with respect to the regularization parameter. In the
algorithm of our proposed method, we use two types of homotopy mining methods, one for ﬁtting
a SHIM on the observed dataset (which is essentially the same as the approach in Tsuda (2007))
and, another for computing the sampling distribution of the test-statistic conditional on the selection
event. Interestingly, these two types of homotopy mining methods share many common properties
such as branch and bound techniques for pruning high-order interaction tree (see Fig.1). We applied
our proposed method on synthetic and real-world HIV1 drug resistance data and demonstrated in
§4 that we could quantify the statistical signiﬁcance of high-order interaction features in the forms
of p-values and conﬁdence intervals without any computational nor statistical approximations. In
an experimental study of the inference stage, we showed that a single traversal of a search space of
more than 1010 high-order interaction terms (sample size, n = 625) took less than 240 sec (worst
case) and 78 sec (best case) on average using Intel Xeon Gold 6230 CPU @ 2.10 GHZ. We extended
this framework to solve the Elastic Net optimization problem which was not trivial as we cannot
follow the common approach of data augmentation by stacking extra rows as this can be prohibitively
expensive due to the combinatorial effects.

2

2 Problem Statement

Consider a regression problem with a response vector y ∈ Rn and m original covariate vectors
z1, . . . , zm, where zj ∈ Rn and j ∈ [m] = {1, ..., m}. Then, a high-order interaction model up to
dth order is written as

(cid:88)

y =

αj1 zj1 +

(cid:88)

αj1,j2 zj1 ◦ zj2 + · · · +

(cid:88)

αj1,...,jd zj1 ◦ · · · ◦ zjd ,

j1∈[m]

(j1,j2)∈[m]×[m]
j1(cid:54)=j2

(j1,...,jd)∈[m]d
j1(cid:54)=...(cid:54)=jd

(2)

where ◦ is the element-wise product and scalar αs are the coefﬁcients. In this paper, we consider each
element of the original covariate vector zj ∈ Rn, j ∈ [m], is deﬁned in a domain [0, 1]. To simplify
notation, it is convenient to write the high-order interaction model in (2) by using the following
matrix of concatenated vectors of all high-order interactions:

, · · · , z1 . . . zd, . . . , zm−d+1 . . . zm
, z1z2, . . . , zm−1zm
X = [z1, . . . , zm
(cid:125)
(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
2ndorder

(cid:123)(cid:122)
dth order

(cid:123)(cid:122)
1st order

(cid:124)

(cid:124)

] ∈ Rn×p,

where p := (cid:80)d
κ=1
interaction terms can be written as:

(cid:0)m
κ

(cid:1). Similarly, the coefﬁcient vector associated with all possible high-order

, α1,2, . . . , αm−1,m
β := [α1, . . . , αm
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
2nd order

(cid:123)(cid:122)
1st order

(cid:124)

](cid:62) ∈ Rp.
, · · · , α1,...,d, . . . , αm−d+1,...,m
(cid:125)

(cid:124)

(cid:123)(cid:122)
dth order

The high-order interaction model (2) is then simply written as a linear model y = Xβ. Unfortunately,
p can be prohibitively large unless both m and d are fairly small. In SHIM, we consider a sparse
estimation of high-order interaction model. An example of SHIM looks like

y = α3z3 + α5z5 + α2,6z2z6 + α1,2,5,9z1z2z5z9.

(3)

The goal of this study is to ﬁt a SHIM such as (3) and test the statistical signiﬁcance of the coefﬁcients
of the selected model (in the above example, α3, α5, α2,6, α1,2,5,9) in order to quantify the reliability.
Unfortunately, both ﬁtting and testing a SHIM are non-trivial because, unless both m and d are
very small, a high-order interaction model will have an extremely large number of parameters to
be considered. Several algorithms for ﬁtting a sparse high-order interaction model were proposed
in the literature (see §1). A common approach taken in these existing works is to exploit the
hierarchical structure of high-order interaction features.
In other words, a tree structure as in
Fig. 1(a) is considered and a branch-and-bound strategy is employed in order to avoid handling all
the exponentially increasing number of high-order interaction features.

Here, we introduce an alogrithm for conditional SI in order to quantify the statistical signiﬁcance of
the ﬁtted coefﬁcients of SHIM such as α3, α5, α2,6, α1,2,5,9 in the forms of p-values or conﬁdence
intervals by using homotopy-based SI. However, due to the extremely large number of features in (2),
it is intractable to characterize the selection event for homotopy-based SI. In order to overcome this
challenge, we develop homotopy mining method which effectively combines the homotopy method
and branch-and-bound strategy in the cherry tree. Before delving into our proposed method, we
brieﬂy overview conditional SI.

2.1 Selective Inference and Homotopy Method

We present conditional selective inference (SI) which is introduced in Lee et al. (2016) and then
explain that optimal (i.e., minimally-conditioned) conditional SI can be conducted with a homotopy
method. In conditional SI framework, we assume that the design matrix X is ﬁxed, response vector y
is a realization of random response vector Y ∼ N (µ, Σ), where µ ∈ Rn is unknown mean vector and
Σ ∈ Rn×n is covariance matrix which is known or estimable from external data. In this framework,
we do not assume “true” relationship between X and µ, but consider a case where the data analyst
adopts the SHIM as a reasonable approximation model to describe the relationship.

Let A be the set of selected features by solving the SHIM ﬁtting problem. With a slight abuse of
notation, we also write this set of features as A(y) in order to emphasize that the set of features A is
obtained when y is observed. This notation enables us to consider A(y(cid:48)) as the set of features which

3

Figure 1: (a) A cherry tree of patterns has been constructed by exploiting the hierarchical structure of
high-order interaction features. Not all nodes are traversed due to pruning. (b) The conditional data
space is restricted to a line. In the ﬁgure it is restricted along the horizontal "τ -line" and we need to
ﬁnd the truncation points (τt, τt+1) along this line.

would be selected when a different response vector y(cid:48) is observed. Furthermore, A(Y ) represents the
“random” set of features selected from the “random” response vector Y .

Given the set of selected features A, consider the best linear approximation of µ with the selected
features. For j ∈ A, let

j := (X (cid:62)
β∗

A XA)−1X (cid:62)
A µ

be the jth population coefﬁcient of the best linear approximation model ﬁtted only with the selected
features. In conditional SI framework, we consider the following hypothesis test:

H0 : β∗

j = 0 v.s. H1 : β∗

j (cid:54)= 0, j ∈ A.

j XA(Y )(X (cid:62)

(4)
A(Y )XA(Y ))−1 with ej ∈ Rn being the vector with 1 at
Noting that, by deﬁning η := e(cid:62)
the jth component and 0 otherwise, we can write β∗
j = η(cid:62)µ with Y = y. Therefore, it is reasonable
to use η(cid:62)Y as the test statistic for the test (4). The (unconditional) sampling distribution of η(cid:62)Y
is highly complicated and intractable because η also depends on the random response vector Y
through the selected features A(Y ). The basic idea of conditional SI is to consider the sampling
distribution of the test-statistic conditional on the selection event, i.e., η(cid:62)Y | {A(Y ) = A}. By
further conditioning on the nuisance component q(Y ) = (In − bη(cid:62))Y with b := Ση(η(cid:62)Ση)−1
which is independent of the test statistic η(cid:62)Y , Lee et al. (2016) showed that the conditional sampling
distribution of η(cid:62)Y | {A(Y ) = A, q(Y ) = q} follows a truncated Normal distribution
η(cid:62)Y | {A(Y ) = A, q(Y ) = q} ∼ F T

(5)

ηT µ,ηT (cid:80) η,

where F T
˜µ,˜σ2 is the c.d.f. of the truncated Normal distribution with mean ˜µ, variance ˜σ2, the truncation
region T , and q is the observed nuisance component deﬁned as q = (In − bη(cid:62))y. However,
identifying the conditional data space {A(Y ) = A, q(Y ) = q} is a challenging problem.

In Lee et al. (2016), the authors developed a practical algorithm to compute the truncated Normal
distribution by further conditioning on the signs of the selected features in A. Although the validity
of the inference can be maintained with this additional conditioning on the signs, it turns out that
the power of the inference is suboptimal with this over-conditioning (Fithian et al., 2014). Recently,
Le Duy and Takeuchi (2021) developed an algorithm to resolve this issue by using homotopy method.
In particular, they considered the parametrized response vector (see Fig. 1 (b))

y(τ ) := q + bτ
(6)
for a scalar parameter τ ∈ R, and solve the continuum of optimal solutions when the response vector
y is replaced with y(τ ) by using homotopy method. Therefore, we can redeﬁne the conditional data
space in (5) as

T = {τ ∈ R | A(y(τ )) = A(y)}.
(7)
It enables us to completely identify the truncation region of the truncated Normal sampling distribution
and compute the selective p-value

P selective

j

= 2 min{πj, 1 − πj}, where, πj = 1 − F T

0,ηT (cid:80) η(η(cid:62)y).

(8)

4

𝑧!𝑧"𝑧!𝑧"𝑧!𝑧#𝑧"𝑧$𝑧$𝑧#𝑧!𝑧"𝑧$𝑧$𝑧#{}𝑏=∑&&!∑&𝑏𝜏𝑞(𝑦)𝜏𝑞𝜏’𝜏’(!y=q+𝑏𝜏(a)(b)Similarly, one can obtain 1 − α conﬁdence interval Cα for any α ∈ [0, 1] such that

P(β∗

j ∈ Cα

(cid:12)
(cid:12){A(Y ) = A, q(Y ) = q}) = 1 − α.

Unfortunately, in the case of SHIM, since the number of high-order interaction features are expo-
nentially large, we cannot use the same homotopy method. In the following section, we present the
homotopy mining algorithm which enables us to compute the conditional sampling distribution (5)
of the ﬁtted SHIM coefﬁcients by effectively combining homotopy method and branch-and-bound
method in pattern mining.

3 Proposed Method

In this study we propose a similar “homotopy-mining” approach for model selection and inference.
Homotopy method refers to an optimization framework for solving a sequence of parameterized
optimization problems. The basic idea of our homotopy mining approach is to consider the following
optimization problem with a parameterized response vector y(τ ) in (6)

β(λ, τ ) = arg min

β∈Rp

Fλ,τ (β) :=

1
2

(cid:107)y(τ ) − Xβ(cid:107)2 + λ (cid:107)β(cid:107)1 ,

(9)

where τ ∈ R is a scalar parameter, λ is the regularization parameter for L1-regularization, and the
objective function Fλ,τ (β) is parameterized by both τ and λ. The homotopy mining enables us to
solve a sequence of parameterized optimization problems in the form of (9) by effectively combining
homotopy and mining method.

To extend the homotopy selective inference framework for SHIM, we ﬁrst need to solve (9) for a
ﬁxed τ and target λ using the observed data and obtain an active set A. Now, ∀j ∈ A, we need to
construct the exact solution path characterized by τ and then identify the conditional data space in (7)
by identifying the intervals of τ on the solution path. This exact solution path can be constructed
in a similar manner as the LARS-LASSO algorithm by an efﬁcient step size calculation. Here, we
deﬁne the exact regularization paths λ (cid:55)→ β(λ) for a ﬁxed τ as the “λ-path” and τ (cid:55)→ β(τ ) for a
ﬁxed λ as the “τ -path”, respectively. Then, both the selection and inference paths of the SHIM can
be constructed in a similar fashion as stated below:

• Model selection of SHIM can be done by using exact regularization path algorithm

λ0 > λ1 > · · · > λmin ⇒ {β(λ0), β(λ1), · · · , β(λmin)}.

• For inference, we can have similar path algorithm

τ0 > τ1 > · · · > τmin ⇒ {β(τ0), β(τ1), · · · , β(τmin)},

(10)

(11)

where sequences of λ and τ represent the breakpoints of homotopy method. The Equations (10)
and (11) have similar problem structure, the only difference is that in (10) we ﬁnd the solution
path characterized by the regularization parameter λ, whereas in (11) we ﬁnd the solution path
characterized by τ . Basically, what we need to characterize the selection event is to ﬁnd those
breakpoints (e.g. τ0, τ3, τ8) along the τ -line where the active set remains the same as the observed
one, i.e., A(y) = A(y(τ0)) = A(y(τ3)) = A(y(τ8)). However, computing the exact regularization
paths for such SHIM is a challenging task due to exponentially expanded feature space. Efﬁcient
computational methods are required both at the selection and inference stage. Therefore, we consid-
ered a tree structure (see Fig. 1 (a)) of the interaction terms (or patterns) and proposed a tree pruning
strategy both for the selection path (λ-path) and inference path (τ -path). In the next section, we
will present the main technical details of characterizing the conditional data space in (7) by using
homotopy-mining method.

3.1 Characterization of truncation region in SHIM

The optimal condition of (9) can be written as

X (cid:62)(cid:0)Xβ(λ, τ ) − y(τ )(cid:1) + λs(λ, τ ) = 0 where sj(λ, τ ) ∈

(cid:26){−1, +1}
[−1, +1]

if βj(λ, τ ) (cid:54)= 0,
if βj(λ, τ ) = 0,

(12)

where j ∈ [p]. Let us deﬁne the active set of features as A(y(τ )) = {j ∈ [p] : βj(λ, τ ) (cid:54)= 0}.

5

The τ -path (λ ﬁxed). Since λ is ﬁxed we drop it from the notation. Now consider two real values
τt and τt+1 (τt+1 > τt) at which the active set does not change and their signs also remain the same.
For notational simplicity, we denote Aτt = A(y(τt)). Then, one can write from (12)

βAτt

λsAc
τt

(τ ) = (X (cid:62)

(τt+1) − βAτt
(τt+1) − λsAc
τt
where νAτt
(τ ) remain con-
b and γAc
τt
stant for all real values of τ ∈ [τt, τt+1). Thus, Equations (13) and (14) state that β(τ ) and λs(τ ) are
(τt) are given in Appendix
piecewise linear in τ for a ﬁxed λ. The derivations of νAτt
A. If τt+1 > τt is the next zero crossing point, then either of the following two events happens
• A zero variable becomes non-zero, i.e., ∃j ∈ Ac

(τt) = νAτt
(τt) = γAc
τt
(τ ) = X (cid:62)
Ac
τt

(τt) × (τt+1 − τt)
(τt) × (τt+1 − τt)

(τt) and γAc
τt

b − X (cid:62)
Ac
τt

(τt+1))| = λ or,

)−1X (cid:62)

XAτt

XAτt

νAτt

j (y(τt+1) − XAτt

τt s.t. |x(cid:62)

βAτt

(14)

(13)

Aτt

Aτt

• A non-zero variable becomes zero, i.e., ∃j ∈ Aτt s.t. βj(τt) (cid:54)= 0 and βj(τt+1) = 0 .
Overall, the next change of the active set happens at τt+1 = τt + ∆j, where
(cid:16)

(cid:18)

(cid:16)

(cid:17)

min
j∈Ac
τt

λ

sign(γj(τt)) − sj(τt)
γj(τt)

, min
j∈Aτt

++

−

βj(τt)
νj(τt)

(cid:17)

(cid:19)

++

.

∆j = min(∆1

j , ∆2

j ) = min

(15)
Here, we use the convention that for any a ∈ R, (a)++ = a if a > 0 and ∞ otherwise. The derivation
of the step-size ∆j for the τ -path is given in the Appendix A. However, solving the minimization
problem to determine the step-size of the τ -path and the λ-path (the details of λ-path are given in
Appendix A) can be challenging for SHIM type problems. Hence, we need efﬁcient computational
methods to make it practically feasible. In the following section we present an efﬁcient tree pruning
strategy by considering a tree structure of the interaction terms (or patterns). Similar pruning strategy
already exists in the literature to solve the λ-path of the LASSO in the context graph mining [Tsuda
(2007)]. In the next section we will show that the same pruning strategy can be applied for the τ -path
of the SHIM.

3.2 Tree pruning

A tree is constructed in such a way that for any pair of nodes ((cid:96), (cid:96)(cid:48)), where (cid:96) is the ancestor of (cid:96)(cid:48), i.e.,
(cid:96) ⊆ (cid:96)(cid:48), the following conditions are satisﬁed

xi(cid:96)(cid:48) = 1 =⇒ xi(cid:96) = 1 and conversely, xi(cid:96) = 0 =⇒ xi(cid:96)(cid:48) = 0 ∀i ∈ [n].
Now considering the τ -path of the LASSO, the equicorrelation condition for any active feature
k ∈ Aτt+1 at a ﬁxed λ can be written as

(cid:12)
(cid:12)x(cid:62)

k (y(τt+1) − Xβ(τt+1))(cid:12)

(cid:12) = λ.

(βAτt

τt becomes active at τt+1 when the following

Therefore at a ﬁxed λ, any non-active feature (cid:96) ∈ Ac
condition is satisﬁed
(cid:12)
(cid:0)y(τt+1) − XAτt
(cid:12)x(cid:62)
(cid:96)
or
where the l.h.s. corresponds to (cid:96) ∈ Ac
(cid:16)
ρ(cid:96)(τt, τt+1) = x(cid:62)
βAτt
(cid:96)
has a lower bound, i.e.,

y(τt+1) − XAτt

(τt) + ∆(cid:96)νAτt

(cid:0)y(τt+1) − XAτt

(cid:12) = (cid:12)
(βAτt
|ρ(cid:96)(τt, τt+1) − ∆(cid:96)η(cid:96)(τt)| = |ρk(τt, τt+1) − ∆(cid:96)ηk(τt)|,

(τt))(cid:1)(cid:12)
(cid:12)
(16)
τt and the r.h.s. corresponds to k ∈ Aτt . Here, we deﬁne
(τt). The r.h.s. of (16)

and η(cid:96)(τt) = x(cid:62)

(τt) + ∆(cid:96)νAτt

(τt))(cid:1)(cid:12)

(cid:96) XAτt

νAτt

(cid:12)x(cid:62)
k

(τt)

(cid:17)

|ρk(τt, τt+1) − ∆(cid:96)ηk(τt)| ≥ |ρk(τt, τt+1)| − ∆(cid:96)|ηk(τt)|,

and the l.h.s. of (16) has an upper bound, i.e.,

|ρ(cid:96)(τt, τt+1) − ∆(cid:96)η(cid:96)(τt)| ≤ |ρ(cid:96)(τt, τt+1)| + ∆(cid:96)|η(cid:96)(τt)|.

Therefore, for equation (16) to have a solution, the following condition needs to be satisﬁed

|ρ(cid:96)(τt, τt+1)| + ∆(cid:96)|η(cid:96)(τt)| ≥ |ρk(τt, τt+1)| − ∆(cid:96)|ηk(τt)|.

(17)

If the above condition (17) is not satisﬁed, then equation (16) will not have any solution, and that can
be used as a pruning condition. Therefore, the pruning condition can be written as

|ρ(cid:96)(τt, τt+1)| + ∆(cid:96)|η(cid:96)(τt)| < |ρk(τt, τt+1)| − ∆(cid:96)|ηk(τt)|.

(18)

6

Lemma 1 If ∆∗

(cid:96) is the current minimum step-size, i.e. ∆∗

(cid:96) = min

t∈{1,2,...,(cid:96)}

{∆t}, (18) is equivalent to

|ρ(cid:96)(τt, τt+1)| + ∆∗

(cid:96) |η(cid:96)(τt)| < |ρk(τt, τt+1)| − ∆∗

(cid:96) |ηk(τt)|.

Lemma 2 If Lemma 1 holds, then ∀(cid:96)(cid:48) ⊃ (cid:96),

|ρ(cid:96)(cid:48)(τt, τt+1)| + ∆∗

(cid:96) |η(cid:96)(cid:48)(τt)| < |ρk(τt, τt+1)| − ∆∗

(cid:96) |ηk(τt)|.

(19)

If the Lemma 2 holds, then ∀(cid:96)(cid:48) ⊃ (cid:96), ∆(cid:96)(cid:48) > ∆∗
(cid:96) . Therefore, we can use Lemma 2 as the pruning
criterion to prune the sub-tree with (cid:96)(cid:48) as the root node. The proofs of Lemmas 1 and 2 are deferred to
Appendix A. The complete algorithm for the inference path (τ -path) is given in Algorithm 1.

Algorithm 1: τ -path
1: Input: Z, λ, b, q, [τmin, τmax]
2: Initialization: t = 0, τt = τmin, T = {τt}, β(τt) = 0
3: y(τt) = q + bτt, Aτt, βAτk

Appendix A)

b,

Aτt

(τt) = 0

νAc
τt

XAτt

)−1X (cid:62)

(τt) = (X (cid:62)
4: νAτt
Aτt
5: while (τt < τmax) do
6: Compute step-length ∆j ← Equation (15)
7: If ∆j = ∆1
j , add j into Aτt
8: If ∆j = ∆2
j , remove j from Aτt
9: update: τt+1 ← τt + ∆j, T = T ∪ {τt+1}, βAτt+1

y(τt+1) = q + bτt+1, νAτt+1

10: end while
11: Output: T , {Aτt}τt∈T

(τt+1) = (X (cid:62)

Aτt+1

3.3 Extension for Elastic Net

(τt) ← λ-path(Z, y(τk), λ) (The algorithm of λ-path is in

(cid:46) Inclusion
(cid:46) Deletion

(τt) ← βAτt
)−1X (cid:62)

XAτt+1

(τt) + ∆jνAτt

Aτt+1

b,

νAc

τt+1

(τt),
(τt+1) = 0

We extended our proposed method to solve the elastic net optimization problem. However, we could
not follow the general approach of solving the elastic net optimization problem as solving LASSO
with augmented data. Because, we cannot just simply augment the data by stacking extra rows as this
can be prohibitively expensive due to the combinatorial effects. In order to derive the step-size for
both λ-path and τ -path, we need a different approach as we construct the high-order interaction model
in a progressive manner. We have shown that using a simple trick, the step-size can be computed very
efﬁciently. Similar trick is also used to derive the pruning condition. See Appendix B for the details.

4 Experiments

We only highlight the main results. The details of experimental setup and several additional experi-
mental results are deferred to Appendix C.

4.1 Comparison of statistical powers.

Synthetic data: We generated the i.i.d. random samples (zi, y) ∈ [0, 1]m × R in such a way that
100m(1 − ζ)% of zi ∈ Rm contain 1son average. Here, ζ ∈ [0, 1] is the sparsity controlling
parameter. The response yi ∈ R is randomly generated from a normal distribution N (0, σ2). For the
comparison of false positive rates (FPRs), true positive rates (TPRs) and conﬁdence interval (CI)
across different methods, we generated the design matrix for a ﬁxed sparsity parameter ζ = 0.95.
In all experiments, the signiﬁcance level was set as α = 0.05. For the comparison of TPRs we
considered a true model of up to 3rd-order interactions deﬁned as µ(xi) = 0.5z1 − 2z2z3 + 3z4z5z6.
The response yi is accordingly generated from N (µ(X), σ2I). For the comparison of FPRs, we
set βj = 0, ∀j ∈ Rp. We compared both FPRs and TPRs across three different methods (ds: data
splitting, homo: homotopy, poly: polytope) for four different sample sizes n ∈ [100, 200, 400, 500].
We generated TPRs and FPRs over 100 trials for all three methods and repeated the experiments for 5

7

Figure 2: Demonstration of the statistical power of three selection bias correction methods (ds: data
splitting, homo: homotopy, poly: polytope) using synthetic data experiments. (a) and (b) show the
false positive rates and the true positive rates for different sample sizes and (c) shows the distribution
of the conﬁdence interval lengths.

Figure 3: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conﬁdence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conﬁdence interval lengths of the same experiments.
The numbers inside the brackets represent the average number of intervals along the τ -line considered
for the homotopy method. Note that in case of polytope only one such interval is considered.

times. The results are shown in Fig. 2(a) and Fig. 2(b), respectively. It can be seen that all SI methods
can properly control the FPRs under α = 0.05. Regarding the TPRs comparison, it can be seen
that homotopy has the highest power which is obvious as it is minimally conditioned compared to
polytope which suffers from over conditioning. Comparing TPRs of data splitting (ds) and homotopy
(homo), it can be seen that TPRs of homo is always greater than that of ds. Note that in ds, only
half of the data is used for selection and the remaining half is used for the inference. Therefore,
compared to homo, ds has higher risk of failing to identify truly correlated features in selection stage
and similarly suffer from low statistical power in the inference stage. The result of CIs is shown in
Fig. 2(c). Here, we used the same true model of the TPR experiments and reported the average CIs
over 100 trials across different methods. The results of CIs are consistent with the ﬁndings of TPRs.

Real data: We obtained HIV-1 sequence data from Stanford HIV Drug Resistance Database Rhee
et al. (2003). In our experiment we used 6 NRTIs, 1 NNRTIs and 3 PIs drugs. We only reported here
the results of 3 NRTIs drugs. Additional results are included in the Appendix C. To demonstrate the
statistical efﬁcacy of the proposed homotopy method over existing polytope method we generated
random sub-samples of those 10 drug data as follows. First, we created a dataset consisting of top 30
mutations from each of the 10 drug data. As most of the columns contain zeros we sorted the columns
based on the number of 1’s present in each column and picked the top 30 columns as our starting set.
Then, from this starting set we considered random sub-samples of ﬁve features for three different

8

(c) Confidence interval lengths(b) True positive rate(a) False positive rate(b.1)(b.2)(b.3)DATA_NRTI_AZT_extop30𝒏P-value[# intervals]CI [# intervals]1000.60 [5.41]1.0 [5.41]2000.67 [8.55]1.0 [8.55]3000.73 [10.10]1.0 [10.10]DATA_NRTI_ABC_extop30𝒏P-value[# intervals]CI [# intervals]1000.74 [1.94]1.0 [1.94]2000.80 [2.25]1.0 [2.25]3000.76 [2.81]1.0 [2.81]DATA_NRTI_D4T_extop30𝒏P-value[# intervals]CI [# intervals]1000.85 [2.25]1.0 [2.25]2000.87 [1.99]1.0 [1.99]3000.82 [2.17]1.0 [2.17](a.1)(a.3)(a.2)Figure 4: Distribution of the fraction of total nodes traversed against different maximum pattern size
(d) constraints while applying the proposed pruning method during the construction of the τ -path.
(a.1) - (a.3) demonstrate the results for 1st, 2nd and 3rd order interaction terms.

d

5
6
7
8
9
10
11
12
13
14
15
None

Search space
(# nodes)
174436
768211
2804011
8656936
8656936
53009101
107636401
194129626
313889476
459312151
614429671
1073741823

1st
14.56 ± 6.05
34.80 ± 16.10
68.19 ± 33.17
110.25 ± 55.55
151.31 ± 76.81
188.26 ± 95.91
212.34 ± 105.54
226.98 ± 115.71
233.88 ± 117.25
240.36 ± 124.79
238.0± 120.35
240.17 ± 119.76

With pruning
2nd
6.58 ± 2.05
13.56 ± 5.75
24.15 ± 11.83
37.70 ± 19.39
51.08 ± 27.09
63.66 ± 34.71
69.26 ± 38.49
74.36 ± 41.20
76.86 ± 43.10
78.127 ± 43.44
79.67 ± 44.72
78.08 ± 43.62

3rd
6.78 ± 3.92
13.99 ± 10.08
25.19 ± 20.50
39.45 ± 33.37
54.06 ± 47.34
65.49 ± 58.42
74.54 ± 66.42
78.97 ± 70.33
83.09 ± 75.13
82.98 ± 74.13
83.31 ± 75.16
82.98 ± 74.33

1st
25.29 ± 2.50
126.96 ± 8.61
450.24 ± 28.50
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

Without pruning
2nd
34.80 ± 1.19
125.29 ± 2.14
447.59 ± 22.15
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

3rd
23.34 ± 1.88
127.97 ± 4.80
447.19 ± 37.69
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day
> 1 day

Table 1: Computation time (in sec) with and without puning for 1st, 2nd and 3rd order interactions.
Here, the computation time is measured against different maximum pattern size (d) constraints. The
last row corresponds to the case when "d" is not speciﬁed and the whole search space is used for
exploration. All computation times are measured on Intel Xeon Gold 6230 CPU @ 2.10GHz.

sample sizes (n ∈ {100, 200, 300}). Here, we considered randomization without replacement for
both sample and features selection. We generated 100 samples and repeated the experiments for
ﬁve times and hence, in total we generated 500 samples. Figure 3 demonstrates the percentage of
times homotopy produced smaller p-values and CI lengths than the polytope. This also depicts the
distributional difference of the CI lengths between homotopy and polytope. These results clearly
demonstrate that homotopy is statistically more powerful than existing polytope method.

4.2 Comparison of computational efﬁciencies.

To demonstrate the computational efﬁciency of the proposed pruning strategy for the τ -path, we
applied our homotopy method with and without pruning on HIV NRTI D4T drug resistance data with
the same starting set of top 30 mutations as used to demonstrate the statistical power. Although we
varied the d from 5 to m, high-order interaction terms upto 3rd order appeared in A. We compared
both the number of nodes traversed (Fig.4) and the time taken (Table.1) against different maximum
interaction order d during the construction of the τ -path of each test statistic direction. Empirically it
was found that the pruning was more effective for the τ -path of high-order interaction terms compared
to that of singleton terms and the power of pruning increases as the order of interaction increases.
Therefore, we reported the average number of nodes and average time taken separately for 1st, 2nd
and 3rd order interaction terms. It can be observed that the pruning is more effective at the deeper
nodes of the tree and it saturates after certain depth of the tree. This is evident as the sparsity of the
data increases at the deeper nodes and the pruning exploits the monotonicity of high-order interaction
terms constructed as tree. In case of homotopy method without pruning we stopped the execution of
program if the τ -path was not ﬁnished in one day. From Tab. 1, it can be observed that without the
pruning the construction of τ -path is not practical owing to the generation of exponential number
of high-order interaction terms as we progress to the deeper nodes of the tree. The τ -path without

9

(a.3) 3!"order interaction(a.2) 2#"order interaction(a.1) 1$%order interactionpruning took more than a day beyond d = 7, while the maximum time taken by the τ -path with
pruning was around 240 sec on average, even when no d constraint was imposed.

5 Conclusions

In this paper, we presented an algorithm for testing a sparse high-order interaction model (SHIM) by
using the framework of conditional selective inference (SI). The algorithm is developed by effectively
combining the homotopy and branch-and-bound tree mining method to deal with the combinatorial
computational burden of the SHIM and also to improve the statistical power.

References

Shuxiao Chen and Jacob Bien. 2020. Valid inference corrected for outlier removal. Journal of

Computational and Graphical Statistics 29, 2 (2020), 323–334.

Yunjin Choi, Jonathan Taylor, and Robert Tibshirani. 2017. Selecting the number of principal
components: Estimation of the true rank of a noisy matrix. The Annals of Statistics (2017),
2590–2617.

William Fithian, Dennis Sun, and Jonathan Taylor. 2014. Optimal inference after model selection.

arXiv preprint arXiv:1410.2597 (2014).

William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan Tibshirani. 2015. Selective sequential

model selection. arXiv preprint arXiv:1512.02565 (2015).

Sangwon Hyun, Kevin Z Lin, Max G’Sell, and Ryan J Tibshirani. 2018. Post-selection inference
for changepoint detection algorithms with application to copy number variation data. Biometrics
(2018).

AK Iversen, Robert W Shafer, Kathy Wehrly, Mark A Winters, James I Mullins, Bruce Chesebro,
and Thomas C Merigan. 1996. Multidrug-resistant human immunodeﬁciency virus type 1 strains
resulting from combination antiretroviral therapy. Journal of Virology 70, 2 (1996), 1086–1090.

Vo Nguyen Le Duy and Ichiro Takeuchi. 2021. Parametric programming approach for more powerful
and general lasso selective inference. In International Conference on Artiﬁcial Intelligence and
Statistics. PMLR, 901–909.

Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. 2016. Exact post-selection inference,

with application to the lasso. Annals of Statistics 44, 3 (2016), 907–927.

Joshua R Loftus and Jonathan E Taylor. 2014. A signiﬁcance test for forward stepwise model

selection. arXiv preprint arXiv:1405.3920 (2014).

Joshua R Loftus and Jonathan E Taylor. 2015. Selective inference in regression models with groups

of variables. arXiv preprint arXiv:1511.01478 (2015).

Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, and Ichiro Takeuchi. 2016.
Safe pattern pruning: An efﬁcient approach for predictive pattern mining. In Proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining. 1785–1794.

Snigdha Panigrahi, Jonathan Taylor, and Asaf Weinstein. 2016. Bayesian post-selection inference in

the linear model. arXiv preprint arXiv:1605.08824 28 (2016).

Soo-Yon Rhee, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W
Shafer. 2003. Human immunodeﬁciency virus reverse transcriptase and protease sequence database.
Nucleic acids research 31, 1 (2003), 298–303.

Soo-Yon Rhee, Jonathan Taylor, Gauhar Wadhera, Asa Ben-Hur, Douglas L Brutlag, and Robert W
Shafer. 2006. Genotypic predictors of human immunodeﬁciency virus type 1 drug resistance.
Proceedings of the National Academy of Sciences 103, 46 (2006), 17355–17360.

10

Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki, Taku Kudo, and Koji Tsuda. 2009. gBoost: a
mathematical programming approach to graph classiﬁcation and regression. Machine Learning 75,
1 (2009), 69–89.

Hiroto Saigo, Takeaki Uno, and Koji Tsuda. 2007. Mining complex genotypic features for predicting

HIV-1 drug resistance. Bioinformatics 23, 18 (2007), 2455–2462.

Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, and Ichiro Takeuchi. 2017. Selective
inference for sparse high-order interaction models. In International Conference on Machine
Learning. PMLR, 3338–3347.

Jonathan Taylor and Robert J Tibshirani. 2015. Statistical learning and selective inference. Proceed-

ings of the National Academy of Sciences 112, 25 (2015), 7629–7634.

Xiaoying Tian and Jonathan Taylor. 2018. Selective inference with a randomized response. The

Annals of Statistics 46, 2 (2018), 679–710.

Ryan J Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. 2016. Exact post-
selection inference for sequential regression procedures. J. Amer. Statist. Assoc. 111, 514 (2016),
600–620.

Koji Tsuda. 2007. Entire Regularization Paths for Graph Data. In Proceedings of the 24th International
Conference on Machine Learning (Corvalis, Oregon, USA) (ICML ’07). Association for Computing
Machinery, New York, NY, USA, 919–926. https://doi.org/10.1145/1273496.1273612

V Vivet-Boudou, J Didierjean, C Isel, and R Marquet. 2006. Nucleoside and nucleotide inhibitors of

HIV-1 replication. Cellular and Molecular Life Sciences CMLS 63, 2 (2006), 163–186.

Fan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty. 2016. Selective inference for

group-sparse linear models. arXiv preprint arXiv:1607.08211 (2016).

Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of

the royal statistical society: series B (statistical methodology) 67, 2 (2005), 301–320.

11

A Appendix

A.1 LASSO τ -path.

A.1.1 Derivations of νAτt

(τt) and γAc
τt

(τt) in Equations (13) and (14).

From the optimality conditions (12) of the Lasso at τt and τt+1, we have following equations for the
active components

(y(τt) − XAτt

−X (cid:62)
Aτt
(y(τt+1) − XAτt

βAτt

(τt)) + λsAτt
(τt+1)) + λsAτt

βAτt

−X (cid:62)
Aτt

(τt) = 0,

(τt) = 0.

(20)

(21)

Note that Aτt = Aτt+1 and sAτt
write

(τt) = sAτt

(τt+1). Therefore, subtracting (20) from (21) we can

βAτt

(τt+1) − βAτt

(τt) = (X (cid:62)
Aτt
= (X (cid:62)
Aτt
= (X (cid:62)
Aτt

XAτt
XAτt
XAτt

)−1X (cid:62)
Aτt
)−1X (cid:62)
Aτt
)−1X (cid:62)
Aτt

= νAτt

(τt+1 − τt).

(y(τt+1) − y(τt))

(q + bτt+1 − q − bτt) using Equation (6)

b(τt+1 − τt)

where, we deﬁned νAτt
where Ac
τt

= Ac

τt+1 but, sAc
τt

(τt) = (X (cid:62)
Aτt

XAτt

)−1X (cid:62)

Aτt

(τt) (cid:54)= sAc
τt

(τt+1),

b. Similarly, for the non-active components,

−X T
Ac
τt

(y(τt) − XAτt
βAτt

(y(τt+1) − XAτt

βAτt
(τt+1)) + λsAc
τt

(τt)) + λsAc
τt
(τt+1) = 0

(τt) = 0,

−X T
Ac
τt

Therefore, subtracting (22) from (23) we can write

λsAc
τt

(τt+1) − λsAc
τt

(τt)(τt) = X T
(b − XAτt
νAτt
Ac
τt
(τt)(τt+1 − τt)

= γAc
τt

(τt))(τt+1 − τt)

where we deﬁned γAc
τt

(τt) = X T
Ac
τt

(b − XAτt

νAτt

(τt)).

A.1.2 Derivation of step-size ∆j in Equation (15)

(22)

(23)

(24)

Step-size of inclusion ∆1
can rewrite Equation (12) for non-active components as

j : Let’s deﬁne ∀j ∈ Ac

τt, cj(τt) = x(cid:62)
j

(cid:0)y(τt) − XAτt

βAτt

(τt)(cid:1), then we

−cj(τt) + λsj(τt) = 0
cj(τt) = λsj(τt).

∴

(25)

Therefore, at any step (τt+1 → τt + ∆j) any non-active feature (j ∈ Ac
τt
following condition is satisﬁed. i.e.

) becomes active when the

Now, let’s consider a linear approximation of cj(τt+1) by considering the value cj(τt) at τt i.e.

|cj(τt+1)| = λ.

cj(τt+1) = cj(τt) + (τt+1 − τt)

∂cj(τt)
∂τ

= cj(τt) + (τt+1 − τt)gj(τt).

12

(26)

(27)

. By plugging (27) into (26) and expanding (26) separately for positive and

where, gj(τt) = ∂cj (τt)
negative terms we can write the step-size of inclusion as
(cid:33)
(cid:32)

∂τ

λ − cj(τt)
gj(τt)

,

−λ − cj(τt)
gj(τt)

∆1

j = τt+1 − τt = min
j∈Ac
τt

= min
j∈Ac
τt

= min
j∈Ac
τt

= min
j∈Ac
τt

(cid:32)

(cid:32)

(cid:33)

±λ − cj(τt)
gj(τt)

λ sign(gj(τt)) − cj(τt)
gj(τt)

(cid:33)

(cid:32)

λ

sign(γj(τt)) − sj(τt)
γj(τt)

(cid:33)

, using cj(τt) = λsj(τt) in Equation (25).

The last equality has been written considering the fact that gj(τt) = γj(τt). The proof of this is given
below.

Proof 1 We will now show that gj(τt) = γj(τt), ∀j ∈ Ac
τt
Ac

τt, is

. We know from (24) that γj(τt), ∀j ∈

γj(τt) =

λsj(τt+1) − λsj(τt)
τt+1 − τ1

,

=

=

cj(τt+1) − cj(τt)
τt+1 − τ1

, using (25)

∂cj(τt)
∂τ

,

= gj(τt).

Step-size of deletion ∆2
0 and βj(τt+1) = 0 .

j : A non zero variable becomes zero i.e. ∃j ∈ Aτt such that

: βj(τt) (cid:54)=

∴ βj(τt+1) = βj(τt) + ∆2

j νj(τt) = 0 =⇒ ∆2

j = min
j∈Aτt

(cid:32)

−

βj(τt)
νj(τt)

(cid:33)

.

++

(28)

A.1.3 Proofs of Lemmas 1 and 2 in §3.2.

We ﬁrst prove Lemma 1. The pruning condition at any node (cid:96) in (18) is

|ρ(cid:96)(τt, τt+1)| + ∆(cid:96)|η(cid:96)(τt)| < |ρk(τt, τt+1)| − ∆(cid:96)|ηk(τt)|.

(29)

Let ∆∗

(cid:96) is the current minimum step-size, i.e. ∆∗

(cid:96) = min

t∈{1,2,...,(cid:96)}

{∆t}. Now, if we consider the node (cid:96)

to ﬁnd the minimum step-size then, we are expecting that ∆(cid:96) ≤ ∆∗
can write

(cid:96) . Therefore, by construction, we

|ρ(cid:96)(τt, τt+1)| + ∆(cid:96)|η(cid:96)(τt)| ≤ |ρ(cid:96)(τt, τt+1)| + ∆∗
|ρk(τt, τt+1)| − ∆∗

(cid:96) |η(cid:96)(τt)|
(cid:96) |ηk(τt)| ≤ |ρk(τt, τt+1)| − ∆(cid:96)|ηk(τt)|.

and,

Therefore, (29) is equivalent to

|ρ(cid:96)(τt, τt+1)| + ∆∗

(cid:96) |η(cid:96)(τt)| < |ρk(τt, τt+1)| − ∆∗

(cid:96) |ηk(τt)|.

(30)

This completes the proof of Lemma 1. Therefore, Lemma 1 is the new pruning condition using the
current minimum step-size, i.e ∆∗
(cid:96) . Note that we can further simplify Lemma 1 as follows. We can
write

ρ(cid:96)(τt, τt+1) = |x(cid:62)
(cid:96)
= |x(cid:62)
(cid:96)
= |x(cid:62)
(cid:96)
= |ρ(cid:96)(τt) + ∆∗

(cid:0)y(τt+1) − XAτt
(cid:0)y(τt) + ∆∗
(cid:0)y(τt) − XAτt
(cid:96) θ(cid:96)|,

βAτt
(cid:96) b) − XAτt
βAτt

(τt)(cid:1)|
βAτt
(τt)(cid:1) + ∆∗

(τt)(cid:1)| using (6)
(cid:96) x(cid:62)
(cid:96) b|

(31)

13

where θ(cid:96) = x(cid:62)

(cid:96) b and ρ(cid:96)(τt) = x(cid:62)
(cid:96)

(cid:0)y(τt) − XAτt

βAτt
(cid:96) θ(cid:96)| ≥ |ρ(cid:96)(τt)| − ∆∗
(cid:96) θ(cid:96)| ≤ |ρ(cid:96)(τt)| + ∆∗

(τt)(cid:1). We know that
(cid:96) |θ(cid:96)|
(cid:96) |θ(cid:96)|.

and,

|ρ(cid:96)(τt) + ∆∗
|ρ(cid:96)(τt) + ∆∗

Now using (31) and (32) we can further write (30) as

|ρ(cid:96)(τt)| + ∆∗

(cid:96) |θ(cid:96)| + ∆∗

(cid:96) |η(cid:96)(τt)| < |ρk(τt)| − ∆∗

(cid:96) |θk| − ∆∗

(cid:96) |ηk(τt)|.

(32)

(33)

Therefore, (33) serves as the simpliﬁed expression of the Lemma 1. Next, we provide two propositions
which we use to prove Lemma 2.

Proposition 1 (Tree anti-monotonicity) A tree is constructed in such a way that for any pair of nodes
((cid:96), (cid:96)(cid:48)), where (cid:96) is the ancestor of (cid:96)(cid:48), i.e., (cid:96)(cid:48) ⊃ (cid:96), the following conditions are satisﬁed

xi(cid:96)(cid:48) = 1 =⇒ xi(cid:96) = 1 and conversely, xi(cid:96) = 0 =⇒ xi(cid:96)(cid:48) = 0 ∀i ∈ [n].

(34)

Proposition 2 If Proposition 1 holds, then ∀(cid:96)(cid:48) ⊃ (cid:96), we have

|ρ(cid:96)(τt)| ≥ |ρ(cid:96)(cid:48)(τt)|,
|η(cid:96)(τt)| ≥ |η(cid:96)(cid:48)(τt)|,
|θ(cid:96)| ≥ |θ(cid:96)(cid:48)|.

Proof for Proposition 2:

If Proposition 1 holds, we have

βAτt

(τt)),

|ρ(cid:96)(τt)| = |x(cid:62)
(cid:96) (y(τt) − XAτt
= |x(cid:62)
(cid:96) w(τt)|,
≥ |x(cid:62)
(cid:96)(cid:48) w(τt)|
=: |ρ(cid:96)(cid:48)(τt)|,

if w(τt) ≥ 0,

where w(τt) = y(τt) − XAτt

(τt). Similarly, we also have

βAτt
|η(cid:96)(τt)| = |x(cid:62)
(cid:96) XAτt
= |x(cid:62)
(cid:96) v(τt)|,
≥ |x(cid:62)
(cid:96)(cid:48) v(τt)|
=: |η(cid:96)(cid:48)(τt)|,

νAτt

(τt)|,

if v(τt) ≥ 0,

where v(τt) = XAτt

νAτt

(τt), and

|θ(cid:96)| = |x(cid:62)
(cid:96) b|,
≥ |x(cid:62)
(cid:96)(cid:48) b|
=: |θ(cid:96)(cid:48)|,

if b ≥ 0,

This completes the proof of Proposition 2.

Proposition 2 will be used to prove Lemma 2. We will prove Lemma 2 by contradiction i.e. we
assume that (33) holds and ∀(cid:96)(cid:48) s.t. (cid:96)(cid:48) ⊃ (cid:96), ∆(cid:96)(cid:48) < ∆∗
l .
∴ |ρk(τt)| − ∆(cid:96)(cid:48)|θk| − ∆(cid:96)(cid:48)|ηk(τt)| > |ρk(τt)| − ∆∗
> |ρ(cid:96)(τt)| + ∆∗
> |ρ(cid:96)(cid:48)(τt)| + ∆∗
> |ρ(cid:96)(cid:48)(τt)| + ∆(cid:96)(cid:48)|θ(cid:96)(cid:48)| + ∆(cid:96)(cid:48)|η(cid:96)(cid:48)(τt)|, ∵ ∆(cid:96)(cid:48) < ∆∗
l .

(cid:96) |ηk(τt)|, ∵ ∆(cid:96)(cid:48) < ∆∗
l
(cid:96) |η(cid:96)(τt)|, using (33)
(cid:96) |η(cid:96)(cid:48)(τt)|, using Proposition 2,

(cid:96) |θk| − ∆∗
(cid:96) |θ(cid:96)| + ∆∗
(cid:96) |θ(cid:96)(cid:48)| + ∆∗

Therefore, we got

∴ |ρk(τt)| − ∆(cid:96)(cid:48)|θk| − ∆(cid:96)(cid:48)|ηk(τt)| > |ρ(cid:96)(cid:48)(τt)| + ∆(cid:96)(cid:48)|θ(cid:96)(cid:48)| + ∆(cid:96)(cid:48)|η(cid:96)(cid:48)(τt)|
=⇒ (cid:96)(cid:48) is infeasible (using (33)) =⇒ ∆(cid:96)(cid:48) ≮ ∆∗
(cid:96) .

This completes the proof of Lemma 2.

If any of w(τt), v(τt) and b in Proposition 2 contains at least one negative element, then we can
no longer use Lemma 2. Hence, using the following Proposition 3, we can propose Lemma 3 as a
general pruning condition.

14

Proposition 3 We can write

where

|ρ(cid:96)(τt)| ≤ b(cid:96),w(τt),
|η(cid:96)(τt)| ≤ b(cid:96),v(τt),
|θ(cid:96)| ≤ b(cid:96),θ,

b(cid:96),w(τt) = max (cid:8) (cid:88)

|wi(τt)|xi(cid:96),

(cid:88)

|wi(τt)|xi(cid:96)

(cid:9)

wi(τt)<0

b(cid:96),v(τt) = max (cid:8) (cid:88)

|vi(τt)|xi(cid:96),

wi(τt)>0
(cid:88)

|vi(τt)|xi(cid:96)

(cid:9)

vi(τt)<0

b(cid:96),θ = max (cid:8) (cid:88)

|bi|xi(cid:96),

(cid:88)

vi(τt)>0
(cid:9).

|bi|xi(cid:96)

bi<0

bi>0

Proof of Proposition 3: We have

|ρ(cid:96)(τt)| = |x(cid:62)
(cid:96) w(τt)|
n
(cid:88)

wi(cid:96)xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

wi(cid:96)>0
(cid:40)

|wi(cid:96)|xi(cid:96) −

(cid:88)

wi(cid:96)<0

|wi(cid:96)|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max

(cid:88)

|wi(cid:96)|xi(cid:96),

(cid:41)

|wi(cid:96)|xi(cid:96)

=: b(cid:96),w(τt).

(cid:88)

wi(cid:96)<0

Similarly,

wi(cid:96)>0

|η(cid:96)(τt)| = |x(cid:62)
(cid:96) v(τt)|
n
(cid:88)

vi(cid:96)xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

vi(cid:96)>0
(cid:40)

|vi(cid:96)|xi(cid:96) −

(cid:88)

vi(cid:96)<0

|vi(cid:96)|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

vi(cid:96)>0

|vi(cid:96)|xi(cid:96),

(cid:88)

vi(cid:96)<0

(cid:41)

|vi(cid:96)|xi(cid:96)

=: b(cid:96),v(τt)

≤ max

and

bixi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|θ(cid:96)| = |x(cid:62)
(cid:96) b|
n
(cid:88)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

bi>0

|bi|xi(cid:96) −

(cid:88)

bi<0

|bi|xi(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max

(cid:40)

(cid:88)

bi>0

|bi|xi(cid:96),

(cid:88)

bi<0

(cid:41)

|bi|xi(cid:96)

=: b(cid:96),θ.

This completes the proof of Proposition 3.

Lemma 3 Using Proposition 3 we can show that ∀(cid:96)(cid:48) ⊃ (cid:96), if

b(cid:96),w(τt) + ∆∗

(cid:96) b(cid:96),θ + ∆∗

(cid:96) b(cid:96),v(τt) < |ρk(τt)| − ∆∗

(cid:96) |θk| − ∆∗

(cid:96) |ηk(τt)|,

(35)

then ∆(cid:96)(cid:48) > ∆∗
(cid:96) .

15

Before proving Lemma 3, we introduce Proposition 4 which will be used to prove Lemma 3:

Proposition 4 If Proposition 1 holds, we have

b(cid:96),w(τt) ≥ b(cid:96)(cid:48),w(τt),
b(cid:96),v(τt) ≥ b(cid:96)(cid:48),v(τt),
b(cid:96),θ ≥ b(cid:96)(cid:48),θ.

Proof of Proposition 4:

If Proposition 1 holds, we have

b(cid:96),w(τt) = max (cid:8) (cid:88)

|wi(τt)|xi(cid:96),

(cid:88)

|wi(τt)|xi(cid:96)

(cid:9)

wi(τt)<0

wi(τt)>0
(cid:88)

≥ max (cid:8) (cid:88)

|wi(τt)|xi(cid:96)(cid:48),

|wi(τt)|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),w(τt).

Similarly, we also have

wi(τt)<0

wi(τt)>0

b(cid:96),v(τt) = max (cid:8) (cid:88)

|vi(τt)|xi(cid:96),

(cid:88)

|vi(τt)|xi(cid:96)

(cid:9)

vi(τt)<0

≥ max (cid:8) (cid:88)

vi(τt)>0
(cid:88)

|vi(τt)|xi(cid:96)(cid:48),

vi(τt)<0

vi(τt)>0

|vi(τt)|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),v(τt).

and

b(cid:96),θ = max (cid:8) (cid:88)

|bi|xi(cid:96),

(cid:88)

(cid:9)

|bi|xi(cid:96)

bi<0

≥ max (cid:8) (cid:88)

bi>0
(cid:88)

|bi|xi(cid:96)(cid:48),

|bi|xi(cid:96)(cid:48)

(cid:9) =: b(cid:96)(cid:48),θ.

bi<0

bi>0

Proof of Lemma 3: We will prove Lemma 3 by contradiction i.e. we assume that (35) holds and
∀(cid:96)(cid:48) s.t. (cid:96)(cid:48) ⊃ (cid:96), ∆(cid:96)(cid:48) < ∆∗
l .
∴ |ρk(τt)| − ∆(cid:96)(cid:48)|θk| − ∆(cid:96)(cid:48)|ηk(τt)| > |ρk(τt)| − ∆∗
> b(cid:96),w(τt) + ∆∗
> b(cid:96)(cid:48),w(τt) + ∆∗
> b(cid:96)(cid:48),w(τt) + ∆(cid:96)(cid:48)b(cid:96)(cid:48),θ + ∆(cid:96)(cid:48)b(cid:96)(cid:48),v(τt), ∵ ∆(cid:96)(cid:48) < ∆∗
l .

(cid:96) |ηk(τt)|, ∵ ∆(cid:96)(cid:48) < ∆∗
l
(cid:96) b(cid:96),v(τt), using (35)
(cid:96) b(cid:96)(cid:48),v(τt), using Proposition 3,

(cid:96) |θk| − ∆∗
(cid:96) b(cid:96),θ + ∆∗
(cid:96) b(cid:96)(cid:48),θ + ∆∗

Therefore, we got

|ρk(τt)| − ∆(cid:96)(cid:48)|θk| − ∆(cid:96)(cid:48)|ηk(τt)| > b(cid:96)(cid:48),w(τt) + ∆(cid:96)(cid:48)b(cid:96)(cid:48),θ + ∆(cid:96)(cid:48)b(cid:96)(cid:48),v(τt)
=⇒ (cid:96)(cid:48) is infeasible (using (35)) =⇒ ∆(cid:96)(cid:48) ≮ ∆∗
(cid:96) .

This completes the proof of Lemma 3.

Hence, if the pruning condition in Lemma 3 holds, then we do not need to search the sub-tree with (cid:96)
as the root node, and hence increasing the efﬁciency of the search procedure [Tsuda (2007)].

16

βAλt

(λt+1) − βAλt
)−1sAλt

A.2 LASSO: λ-path

A.2.1 λ-path: path w.r.t. to λ (τ ﬁxed)

Since τ is ﬁxed, we drop it from the notation. The normal equation of the λ-path can be written as
−X T (y − Xβ) + λs(λ) = 0

where, s(λ) is the sub-differential deﬁned as

sj(λ) ∈

(cid:26){−1, +1},
[−1, +1],

if βj(λ) (cid:54)= 0
if βj(λ) = 0.

Now, if we consider two λ values (λt > λt+1) at which the active set does not change (i.e. Aλt =
Aλt+1) and the sign of the active coefﬁcients also remain the same (i.e. sAλt
(λt+1)) ,
then we can write

(λt) = sAλt+1

(λt) = −νAλt

(λt)(λt+1 − λt)

(36)

(λt) = (X T

Aλt

XAλt

(λt). The derivation of νAλt

where, νAλt
(λt) is given in A.2.2. Note
that νAλ(λ) is constant for all real values of λ ∈ [λt, λt+1) and thus, equation (36) states that β(λ)
is piece-wise linear in λ for a ﬁxed τ . To draw the curve of solutions as a function of λ, we need to
check when the active set changes. If λt+1 is the next zero crossing point then either of the following
two events happens.
• A zero variable becomes non-zero i.e. ∃j ∈ Ac
λt

(λt+1))| = λt+1 or,

j (y − XAλt

βAλt

: |x(cid:62)

s.t.

• A non zero variable becomes zero i.e. ∃j ∈ Aλt s.t. : βj(λt) (cid:54)= 0 but βj(λt+1) = 0 .
Overall, the next change of the active set happens at λt+1 = λt − ∆j, where

∆j = min(∆1

j , ∆2

j ) = min

(cid:32)

min
j∈Ac
λt

(cid:16) (xk ± xj)T w(λt)
(xk ± xj)T v(λt)

(cid:17)

, min
j∈Aτt

++

(cid:16)

−

βj(λt)
νj(λt)

(cid:17)

++

(cid:33)

.

(37)

where, w(λt) = y − XAλt
inclusion (∆1

βAλt
j ) is given in A.2.3.

(λt), and v(λt) = XAλt

νAλt

(λt). The derivation of the step-size of

A.2.2 Direction vector (λ-path)

Let’s consider the normal equation at λt and λt+1 for the active components

−X (cid:62)

Aλt

(y − XAλt

βAλt

(λt)) + λtsAλt

(λt) = 0,

−X (cid:62)

Aλt

(y − XAλt

βAλt

(λt+1)) + λtsAλt

(λt) = 0.

(38)

(39)

Note that Aλt = Aλt+1 and sAλt
write

(λt) = sAλt

(λt+1). Therefore, subtracting (38) from (39) one can

where, we deﬁned νAλt

βAλt
(λt) = (X (cid:62)

(λt+1) − βAλt
XAλt

Aλt

(λt)(λt+1 − λt),

(λt) = −νAλt
)−1sAλt
(λt).

A.2.3 Step-size of inclusion (λ-path)

The optimality condition for the active features j ∈ Aλt of the λ-path of LASSO can be written as

(λt)(cid:1) + λs(βj) = 0,

−x(cid:62)
j

(cid:0)y − XAλt
x(cid:62)
j
(cid:12)
(cid:12)x(cid:62)
j

βAλt
(cid:0)y − XAλt
(cid:0)y − XAλt

(λt)(cid:1) = λsj,
(λt)(cid:1)(cid:12)
(cid:12) = λ.
Therefore, at any step (λt → λt − ∆j) any non-active feature (j ∈ Ac
λt
following condition is satisﬁed i.e.
(cid:12)
(cid:12)x(cid:62)
j

βAλt
βAλt

∴

(βAλt

(cid:0)y − XAλt
(cid:12)
(cid:12)x(cid:62)

(λt) + ∆jν(λt))(cid:1)(cid:12)
v(λt))(cid:12)

(cid:12) = (cid:12)
(cid:12) = (cid:12)

(cid:0)y − XAλt
(cid:12)x(cid:62)
(βAλt
k
(cid:12)x(cid:62)
k (w(λt) − ∆jXAλt

(λt) + ∆jν(λt))(cid:1)(cid:12)
v(λt))(cid:12)
(cid:12) ,

j (w(λt) − ∆jXAλt

) becomes active when the

(cid:12) , ∀j ∈ Ac
λt

, ∀k ∈ Aλt

17

where w(λt) = y − XAλt
negative terms separately one can write the step-size of inclusion as -

(λt) and v(λt) = XAλt

βAλt

νAλt

(λt). Now, considering the positive and

∆1

j = min
j∈Ac
λt

(cid:32)

(xk ± xj)(cid:62)w(λt)
(xk ± xj)(cid:62)v(λt)

(cid:33)
.

A.2.4 Tree pruning (λ-path)

The derivation of this pruning condition is also given in (Tsuda, 2007). However, here we provide the
same derivation in our notation to make it self-contained. Similar to (18), the pruning condition of
the λ-path can be written as

|ρ(cid:96)(λt)|+∆∗

(cid:96) |η(cid:96)(λt)|< |ρk(λt)|−∆∗
(cid:96) v(λt) ∀(cid:96) ∈ Ac
λt

(cid:96) |ηk(λt)|,
, ρk(λt) = x(cid:62)

(cid:96) w(λt) and η(cid:96)(λt) = x(cid:62)

∀k ∈ Aλt . Now similar to the Proposition 3 we can also write

where ρ(cid:96)(λt) = x(cid:62)
x(cid:62)
k v(λt),

(40)

k w(λt) and ηk(λt) =

Proposition 5 Using the tree anti-monotonicity property (34) we can easily show that ∀(cid:96)(cid:48) s.t. (cid:96)(cid:48) ⊃ (cid:96),
the following conditions are satisﬁed i.e.

|ρ(cid:96)(cid:48)(λt)| = |

(cid:88)

wi(λt)xi(cid:96)(cid:48)| ≤ max(cid:8) (cid:88)

|wi(λt)|xi(cid:96),

(cid:88)

|wi(λt)|xi(cid:96)

(cid:9) := bw(λt),

|η(cid:96)(cid:48)(λt)| = |

i
(cid:88)

i

vi(λt)xi(cid:96)(cid:48)| ≤ max(cid:8) (cid:88)

|vi(λt)|xi(cid:96),

wi(λt)<0

wi(λt)>0
(cid:88)

|vi(λt)|xi(cid:96)

(cid:9) := bv(λt).

vi(λt)<0

vi(λt)>0

Therefore, similar to the τ -path (35) the pruning condition of the λ-path (40) can be written as

bw(λt) + ∆∗

(cid:96) bv(λt) < |ρk(λt)| − ∆∗

(cid:96) |ηk(λt)|.

(41)

The

complete

algorithm for

the

selection path (λ-path)

is given in Algorithm2.

Algorithm 2: λ-path
1: Input: Z, y, λ.
2: Initialization: λt = λmax = ||X (cid:62)y||∞, Aλt = arg max

|X (cid:62)y|j,

β(λt) = 0,

(λt)), νAc
λt

j
(λt) = 0.

Aλt

XAλt

(λt) = (X (cid:62)

)−1sign(βAλt

3: νAλt
4: while λt ≥ λ
do.
5: Compute step-length ∆j ← Equation (37).
6: If ∆j = ∆1
j , add j into Aλt.
7: If, ∆j = ∆2
j , remove j from Aλt.
8: Update: λt ← λt − ∆j, βAλt
(λt+1) = (X (cid:62)

(λt+1) ← βAλt

XAλt+1

Aλt+1

νAλt+1
9: end while
10: Output: β(λ), Aλ.

)−1sign(βAλt+1

(λt) + ∆jνAλt
(λt+1)),

(cid:46) Inclusion
(cid:46) Deletion.

(λt),
νAc

λt+1

(λt+1) = 0.

18

B Extension for Elastic Net (ElNet)

A common problem of the LASSO is that if the data has correlated features then, the LASSO picks
only one of them and ignores the rest, which leads to instability. To solve this problem Zou and
Hastie (2005) proposed the Elastic Net (ElNet). This feature correlation problem is very much evident
in SHIM type problem, and hence we extended our framework for the Elastic Net. To extend our
framework for the Elastic Net, we need to solve the following optimization problem.

β(λ, τ ) ∈ arg min

β∈Rp

1
2

(cid:107)y(τ ) − Xβ(cid:107)2

2 +

1
2

α (cid:107)β(cid:107)2

2 + λ (cid:107)β(cid:107)1 .

(42)

B.1 λ-path: path w.r.t. to λ (τ ﬁxed)

Similar to the LASSO, the normal equation can be written as

−X (cid:62) (y − Xβ(λ)) + αβ(λ) + λs(λ) = 0.

where, s(λ) is the sub-differential that can be deﬁned in a similar fashion as done in the case of the
λ-path for the LASSO (A.2.1). Now, if we consider two λ values (λt > λt+1) at which the active set
does not change (i.e. Aλt = Aλt+1) and the sign of the active coefﬁcients also remain the same (i.e.
sAλt

(λt+1)) , then we can write

(λt) = sAλt

βAλt

(λt+1) − βAλt
XAλt

(λt) = −νAλt

(λt)(λt+1 − λt),

(43)

Aλt

(λt) = (X (cid:62)

+ αI|Aλt |)−1sAλt

(λt). Now, similar to the LASSO we can derive the step-size of deletion (∆2

where, νAλt
(λt). Note that here the only change in the
direction vectors compared to the LASSO is the addition of an αI|Aλt | term to the expression of
νAλt
j ) considering this
updated expression of the direction vector. However, to derive the step-size of inclusion (∆1
j ), we
need a different approach. The elastic net optimization problem can actually be formulated as a
LASSO optimization problem using augmented data. If we consider an augmented data deﬁned as
˜X =

, then solving the elastic net optimization problem (42) for a ﬁxed τ , is

and ˜y =

(cid:19)

(cid:19)

(cid:18) X√
αIp

(cid:18)y
0

equivalent to solving the following problem.

β(λ) ∈ arg min

β∈Rp

1
2

(cid:13)
(cid:13)˜y − ˜Xβ
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+ λ (cid:107)β(cid:107)1 .

(44)

Now, similar to the LASSO we can write the step-size of inclusion (∆1
the augmented data ( ˜X, ˜y) as

j ) of the λ-path of ElNet using

(cid:32)

∆1

j = min
j∈Ac
λt

(˜xj − ˜xk)(cid:62) ˜w(λt)
(˜xj − ˜xk)(cid:62)˜v(λt)

,

(˜xj + ˜xk)(cid:62) ˜w(λt)
(˜xj + ˜xk)(cid:62)˜v(λt)

(cid:33)

.

(45)

However, we cannot just simply augment the data by stacking extra rows as this can be prohibitively
expensive due to the combinatorial effects. In order to derive the step-size of inclusion (∆1
j ) we need
a different approach as we construct the high-order interaction model in a progressive manner. We
have shown that using the following approach the step-size of inclusion for the λ-path of ElNet can
be computed very efﬁciently, where the step-size of inclusion can be deﬁned as

(cid:32)

∆1

j = min
j∈Ac
λt

(xj − xk)(cid:62)w(λt) + αβk
(xj − xk)(cid:62)v(λt) − ανk

,

(xj + xk)(cid:62)w(λt) − αβk
(xj + xk)(cid:62)v(λt) + ανk

(cid:33)
.

(46)

The derivation of the above step-size (∆1

j ) is given below.

Proof 2 Lets, consider ˜w(λt) = ˜y− ˜XAλt
where p = |Aλt| + |Ac
λt

|, then we can write

βAλt

(λt) ∈ Rn+p and w(λt) = y−XAλt

βAλt

(λt) ∈ Rn,

˜wi(λt) =




wi(λt)
√
−
αβj

0

i ≤ n,

if
if n < i ≤ n + |Aλt|,
if n + |Aλt| < i ≤ n + p.

(47)

19

similarly considering ˜v(λt) = ˜Xν(λt) ∈ Rn+p and v(λt) = Xν(λt) ∈ Rn, we can write

˜vi(λt) =






vi(λt)
√
ανj

0

i ≤ n,

if
if n < i ≤ n + |Aλt|,
if n + |Aλt| < i ≤ n + p.

and, considering ˜X ∈ Rn+p and X ∈ Rn we can write

˜xij =




xij
√
α



0

i ≤ n,
i > n and (i − n) = j,

if
if
otherwise.

Therefore, in (45) we can write that ∀j ∈ Rp

(48)

(49)

˜x(cid:62)
j ˜w(λt) =

=

n+p
(cid:88)

i=1

n
(cid:88)

i=1

˜wi(λt)˜xij,

˜wi(λt)˜xij +

n+|Aλt |
(cid:88)

i=n+1

˜wi(λt)˜xij +

n+p
(cid:88)

˜wi(λt)˜xij.

i=n+|Aλt |+1

Now, using (47) and (49) the second and the third quantity in the above expression can be written as

n+|Aλt |
(cid:88)

i=n+1

˜wi(λt)˜xij =

√

αβj)(

α),

√

(cid:26)(−
0

if (i − n) = j,
otherwise.

and,

Therefore,

˜x(cid:62)
j ˜w(λt) =

n
(cid:88)

i=1

n+p
(cid:88)

i=n+|Aλt |+1

˜wi(λt)˜xij = 0.

wi(λt)xij, ∀j ∈ Ac

λt and ˜x(cid:62)

k ˜w(λt) =

n
(cid:88)

i=1

wi(λt)xik − αβk, ∀k ∈ Aλt.

Similarly, using (48) and (49) we can write

˜x(cid:62)
j ˜v(λt) =

n
(cid:88)

i=1

vi(λt)xij, ∀j ∈ Ac
λt

and

˜x(cid:62)
k ˜v(λt) =

n
(cid:88)

i=1

vi(λt)xik + ανk, ∀k ∈ Aλt.

Therefore the step-size of inclusion can be written as

(cid:32)

∆1

j = min
j∈Ac
λt

(xj − xk)(cid:62)w(λt) + αβk
(xj − xk)(cid:62)v(λt) − ανk

,

(xj + xk)(cid:62)w(λt) − αβk
(xj + xk)(cid:62)v(λt) + ανk

(cid:33)
.

(50)

B.1.1 Tree pruning (λ-path)

Similar to the LASSO (40) we can use the following inequality in augmented data ( ˜X, ˜y) as the
pruning criteria for the λ-path of ElNet.

|˜ρ(cid:96)|+∆∗
(cid:96) ˜v(λt) ∀(cid:96) ∈ Ac
where, ˜ρ(cid:96) = ˜x(cid:62)
λt
Aλt . Now, using (47), (48) and (49) we can show that

(cid:96) ˜w(λt) and ˜η(cid:96) = ˜x(cid:62)

(cid:96) |˜η(cid:96)|< |˜ρk|−∆∗

(cid:96) |˜ηk|,
, ˜ρk = ˜x(cid:62)

k ˜w(λt) and ˜ηk = ˜x(cid:62)

k ˜v(λt),

(51)

∀k ∈

˜ρ(cid:96)(λt) =

n
(cid:88)

i=1

wi(λt)xi(cid:96), ˜η(cid:96)(λt) =

n
(cid:88)

i=1

vi(λt)xi(cid:96) and,

˜ρk(λt) =

n
(cid:88)

i=1

wi(λt)xik − αβk, ˜ηk(λt) =

n
(cid:88)

i=1

vi(λt)xik + ανk.

20

Therefore, the pruning condition (51) can be redeﬁned as -

n
(cid:88)

|

i=1

wi(λt)xi(cid:96)| + ∆∗
(cid:96) |

n
(cid:88)

i=1

vi(λt)xi(cid:96)| < |

n
(cid:88)

i=1

wi(λt)xik − αβk| − ∆∗
(cid:96) |

n
(cid:88)

i=1

vi(λt)xik + ανk|.

Now, similar to the LASSO (41) we can also write

bw(λt) + ∆∗

(cid:96) bv(λt) < |¯ρk(λt)| − ∆∗

(cid:96) |¯ηk(λt)|,

(52)

where, ¯ρk(λt) = (cid:80)n

i=1 wi(λt)xik − αβk, ¯ηk(λt) = (cid:80)n
bw(λt) = max(cid:8) (cid:88)

|wi(λt)|xi(cid:96),

i=1 vi(λt)xik + ανk, and
(cid:9),

|wi(λt)|xi(cid:96)

(cid:88)

wi(λt)<0

bv(λt) = max(cid:8) (cid:88)

|vi(λt)|xi(cid:96),

wi(λt)>0
(cid:88)

|vi(λt)|xi(cid:96)

(cid:9).

vi(λt)<0

vi(λt)>0

Therefore, (52) can be used as the pruning condition for the λ-path of ElNet.

B.2 τ -path: path w.r.t. to τ (λ ﬁxed)

If we consider two real values τt and τt+1 ( τt+1 > τt) at which the active set does not change and
their signs also remain the same, then we can write

βAτt

(τt+1) − βAτt
(τt+1) − λsAc
τt

(τt) = νAτt
(τt) = γAc
τt

(τt)(τt+1 − τt),
(τt)(τt+1 − τt).

λsAc
τt

XAτt

(τt) = (X (cid:62)
Aτt

+ αI|Aτt |)−1X (cid:62)
where, νAτt
(τt).
Aτt
Note that here also the only change compared to the LASSO (A.1.1) is the addition of an αI|Aτt |
term to the expression of νAτt
. Now, one can also derive a similar expression of step-size of inclusion
and deletion as done for the LASSO (A.1.2) by considering the updated expression of νAτt
(τt) and
γAc
τt

(τt) = X (cid:62)
Ac
τt

b − X (cid:62)
Ac
τt

b and γAc
τt

XAτt

νAτt

(τt).

B.2.1 Tree pruning (τ -path)

Similar to the LASSO (35), by using (47), (48) and (49) the pruning condition for the τ -path of ElNet
can be written as

where ¯ρk(τt) = (cid:80)n

b(cid:96),w(τt) + ∆∗
(cid:96) b(cid:96),θ + ∆∗
i=1 wi(τt)xik − αβk, ¯ηk(τt) = (cid:80)n

i=1 vi(τt)xik + ανk.

(cid:96) b(cid:96),v(τt) < |¯ρk(τt)|−∆∗

(cid:96) |θk|−∆∗

(cid:96) |¯ηk(τt)|,

(53)

21

C Additional Results

Here we report additional results using real world HIV-1 sequence data from Stanford HIV Drug
Resistance Database (Rhee et al., 2003). This dataset contains three classes of drug data: NRTIs,
NNRTIs and PIs consisting of 16 drugs. Finding virus induced mutations which leads to drug resis-
tance is crucial to drug development. However, drug resistance is a complex biological phenomenon
and it is often reported in the literature (Rhee et al., 2006; Tsuda, 2007; Suzumura et al., 2017), that
it is the association of multiple mutations along with some crucial single mutations that can best
describe the phenomenon. Hence, it is important to understand the association of multiple mutations
related to the drug resistance. In our experiment we used 6 NRTIs, 1 NNRTIs and 3 PIs drugs. We
reported the results on 3 NRTIs drugs in the main article and here we include the results on the
remaining 3 NRTIs (Fig. 5) and 3 PIs (Fig. 6) and 1 NNRTI (7) drugs. The continuous drug resistance
values corresponds to the response (y ∈ R) and the binary mutations corresponds to the original
features (z ∈ Rm) in our experimental settings.
In Table. 2 we demonstrated the computational

Figure 5: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conﬁdence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conﬁdence interval lengths of the same experiments.

Figure 6: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conﬁdence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conﬁdence interval lengths of the same experiments.

22

(b.1)(b.2)(b.3)(a.1)(a.3)(a.2)DATA_NRTI_TDF_extop30𝒏P-value[# intervals]CI [# intervals]1000.74 [2.22]1.0 [2.22]2000.63 [2.78]1.0 [2.78]3000.58 [3.76]1.0 [3.76]DATA_NRTI_DDI_extop30𝒏P-value[# intervals]CI [# intervals]1000.84 [2.23]1.0 [2.23]2000.90 [2.00]1.0 [2.00]3000.83 [2.10]1.0 [2.10]DATA_NRTI_X3TC_extop30𝒏P-value[# intervals]CI [# intervals]1000.70 [5.24]0.99 [5.24]2000.75 [7.58]1.0 [7.58]3000.80 [8.86]1.0 [8.86]DATA_PI_IDV_extop30𝒏P-value[# intervals]CI [# intervals]1000.60 [2.81]1.0 [2.81]2000.64 [4.68]1.0 [4.68]3000.67 [5.74]1.0 [5.74]DATA_PI_ATV_extop30𝒏P-value[# intervals]CI [# intervals]1000.66 [3.81]1.0 [3.81]2000.71 [6.35]1.0 [6.35]3000.75 [7.91]1.0 [7.91]DATA_PI_APV_extop30𝒏P-value[# intervals]CI [# intervals]1000.64 [2.43]1.0 [2.43]2000.66 [3.78]1.0 [3.78]3000.67 [5.29]1.0 [5.29](b.1)(b.2)(b.3)(a.1)(a.2)(a.3)Figure 7: Comparison of statistical powers (Homotopy vs Polytope). (a.1-a.3) show the percentage of
cases where selection bias corrected p-values and conﬁdence interval lengths of the proposed method
(Homotopy) was smaller than that of the existing method (Polytope) in random sub-sampling experi-
ments. (b.1-b.3) show the distributions of the conﬁdence interval lengths of the same experiments.

advantage of the proposed homotopy method over exiting method on conditioning on model (Lee
et al. (2016)). In this experiment the λ-path was constructed until the active set (A) contains 20
features and subsequently that active set and the corresponding λ value is used for the construction
of the τ -path. The Lee et al. (2016) method needs to consider the union of all possible signs in the
observed active set (A) in order to condition on the model. However, our homotopy mining needs to
consider only ∼ 120 polytopes (worst case) for the same task.

High-order interactions
1st
2nd
3rd

Homotopy
(# kinks)
104.15 ± 10.73
101.0 ± 4.64
78.33 ± 24.69

Polytope
(# polytopes)
220
220
220

Table 2: Comparison of computational efﬁciencies of the proposed homotopy method against existing
polytope method. The "# kinks" represents the average number of kinks encountered during the
construction of τ -path for each test statistic direction, whereas the "# polytopes" represents the
number of all possible signs one needs to consider to condition on the model.

We note that theoretically, in the worst-case, the complexity of the homotopy method grows exponen-
tially. This is a common issue in homotopy-based methods such as computing regularization paths.
However, fortunately, it has been well-recognized (Le Duy and Takeuchi, 2021) that this worst case
rarely happens in practice, and this is also evident from our experimental results.

Similar to the pruning, empirical evidence also demonstrates that homotopy is more efﬁcient in case
of high-order interaction terms compared to that of singleton terms, and the efﬁciency increases as
the order of interaction increases. We suspect that as the order of interaction increases the sparsity of
the data also increases which signiﬁcantly affects the construction of the τ -path as evident from the
effectiveness of both pruning and the homotopy method. However, more theoretical investigations
are required to have a clear understanding of this phenomenon which we believe worth considering
in the future.

23

(b.1)(a.1)DATA_NNRTI_NVP_extop30𝒏P-value[# intervals]CI [# intervals]1000.70 [6.36]1.0 [6.36]2000.70 [9.05]1.0 [9.05]3000.74 [10.26]1.0 [10.26]