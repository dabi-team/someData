Learning compositional programs with arguments and sampling

Giovanni De Toni1, Luca Erculiani1, Andrea Passerini1
1 Structured Machine Learning Group (SML),
Department of Information Engineering and Computer Science (DISI)
University of Trento, Italy
{giovanni.detoni, luca.erculiani, andrea.passerini}@unitn.it

Abstract

One of the most challenging goals in designing intelligent
systems is empowering them with the ability to synthesize
programs from data. Namely, given speciﬁc requirements in
the form of input/output pairs, the goal is to train a machine
learning model to discover a program that satisﬁes those re-
quirements. A recent class of methods exploits combinatorial
search procedures and deep learning to learn compositional
programs. However, they usually generate only toy programs
using a domain-speciﬁc language that does not provide any
high-level feature, such as function arguments, which reduces
their applicability in real-world settings. We extend upon a
state of the art model, AlphaNPI, by learning to generate
functions that can accept arguments. This improvement will
enable us to move closer to real computer programs. More-
over, we investigate employing an Approximate version of
Monte Carlo Tree Search (A-MCTS) to speed up conver-
gence. We showcase the potential of our approach by learning
the Quicksort algorithm, showing how the ability to deal with
arguments is crucial for learning and generalization.

Introduction
The ability to autonomously synthesize programs from data
is one of the main goals in developing intelligent systems.
Namely, given speciﬁc requirements, such as input/output
pairs or more formal speciﬁcations, the model must learn
a program that meets those demands. Several approaches
to try to solve this task are based on the neural controller
- interface framework, in which a neural network interacts
with an external structured environment by using primitives,
such as read/write heads or other atomic functions (Zaremba
et al. 2016; Reed and de Freitas 2016; Graves, Wayne, and
Danihelka 2014; Graves et al. 2016; Pierrot et al. 2019,
2020). Neural Turing Machines (Graves, Wayne, and Dani-
helka 2014), and Differentiable Neural Computers (Graves
et al. 2016) can learn simple procedures by training on in-
put/output pairs. These models use external memory, and
they are differentiable, which means that they can be trained
end-to-end by gradient descent. However, they are essen-
tially black boxes since it is non-trivial to inspect them to un-
derstand their discovered algorithm. More importantly, su-
pervision is provided at the level of full program outputs

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

only, making training strongly susceptible to overﬁtting as
shown for the priority sort task of Graves, Wayne, and Dani-
helka (2014).

Neural-Programmer interpreters (NPI) (Reed and de Fre-
itas 2016) use a reinforcement learning paradigm to learn
highly compositional programs by exploiting execution
traces. An execution trace is an ordered list of commands
that, if executed, solve a given task. Execution traces pro-
vide ﬁner-grain supervision during training, encouraging
convergence towards more accurate programs. NPI assumes
to have at hand a large dataset of execution traces for the
given problem, substantially limiting its applicability in real-
world scenarios, where an oracle capable of providing such
a dataset could be unavailable. A recent model, named Al-
phaNPI, shows how to train NPI without execution traces.
AlphaNPI can solve both discrete and continuous control
problems (Pierrot et al. (2019, 2020)), and it works by com-
bining NPI with Monte Carlo Tree Search (MCTS), which is
used to discover the execution traces. This approach showed
outstanding results, both in terms of generalization and in-
terpretability of the ﬁnal models. However, the spectrum of
tasks it can learn is still somewhat limited since it generates
programs that use an ad-hoc language that does not present
any high-level construct of a formal programming language
such as loops, variables or function arguments.

We can also induce programs from input/output exam-
ples by creating logic programs. Inductive Linear Program-
ming (ILP) Muggleton and de Raedt (1994); Cropper, Du-
mancic, and Muggleton (2020) is a classical framework to
learn logic programs from data. ILP is data-efﬁcient and also
supports predicate invention through the use of meta-rules.
Namely, ILP systems can invent new programs and func-
tions. However, these techniques require strong formaliza-
tion of the problem settings into logical form. They require
extensive domain knowledge, and it can be hard to provide
a formalization to learn more complex real-world programs.
Moreover, they are also sensitive to noisy programs since by
learning underperforming procedures, we can undermine the
performances of the ﬁnal learned algorithm.

This work presents some promising results towards gen-
eralized neural interpreters and discusses additional future
challenges that still need to be addressed. We also provide
a brief discussion comparing ILP and neural-based models.
The main contributions are the following:

Novel Setting
We change the original setting by making actions and pro-
grams accept arguments, a. Each action/program can take
an ordered list of at most three arguments. In our environ-
ment, these arguments are the pointers used to manipulate
the list. For example, the quicksort program accepts two
arguments, the pointer placed at the beginning of the list
and the pointer placed at the end of the list. Additionally,
an action/program can also accept no arguments, such as the
stop action, which terminates the execution of the simu-
lation. As for the pre-conditions, the maximum number of
arguments that a program can accept is given apriori. We
also extend the pre-conditions to consider the arguments
to decide if a program is feasible. For example, the swap
pointer action cannot be called by giving as argument the
same pointer twice.

Architecture
We start from the architecture developed by Pierrot et al.
(2019), and adapt it to the novel setting by introducing an
additional module, farguments. Thanks to this new mod-
ule, we learn compact programs that can adapt their be-
haviour by accepting arguments instead of learning a sep-
arate program for each possible value of the arguments.
For example, we can learn a move action to shift point-
ers that can operate on multiple pointers at
the same
time, move(pointer 1,pointer 3). With the original
method, we would have had to learn two separate actions,
move pointer 1 and move pointer 3.

The architecture is shown in Figure 1 and consists of six
modules: an encoder (fenc), a programs matrix (Mprog), an
LSTM core (flstm), a program network (fprogram), an argu-
ments network (farguments) and a value network (fvalue).
The encoder takes as input an environment et and encodes it
as a set of features st. The program matrix takes as input an
index it and returns the corresponding program embedding
pt. The LSTM core executes programs while conditioning
on pt, the feature set st and its internal state ht−1. The pro-
gram and arguments networks take the LSTM output, ht,
and return probabilities over the action space, πp, and argu-
ment space, πa, respectively. Lastly, the value network takes
instead the LSTM output to estimate the value function V .
The equation of the architecture are the following:

pt = Mprog[it]

st = fenc(et)
ht = flstm(st, pt, ht−1)
πp = fprogram(ht) πa = farguments(ht)
V = fvalue(ht)

(1)
(2)
(3)
(4)

Approximate Monte Carlo Tree Search
Monte Carlo Tree Search (Coulom 2006; Kocsis and
Szepesv´ari 2006) is an algorithm to search large combi-
natorial spaces represented by trees efﬁciently. It was suc-
cessfully used to solve many tasks, such as mastering com-
plex games and protein folding (Silver et al. 2017; Jumper
et al. 2021). The original method is made by four phases:

Figure 1: Complete Diagram of the Architecture

• We propose a new version of the AlphaNPI model, which
brings us closer to learning real-world programs by learn-
ing functions that also accept arguments.

• We propose an Approximate recursive Monte Carlo Tree
Search (A-MCTS) procedure to improve convergence.

• We developed a reﬁned training strategy featuring re-
training over failed tasks that improves robustness and
generalization.

Problem Statement and Original Setting
We consider the problem of learning a complex algorithm,
such as Quicksort, by having an agent interacting with an
environment e by choosing actions p. The initial actions are
called atomic actions. In the Quicksort case, the environ-
ment is a list of integers, and the atomic actions are element
swaps and one-step pointer moves. The agent learns to lever-
age and to combine these atomic actions to produce higher-
level programs. These learnt programs are then added to the
collection of available actions. Therefore, the agent can ben-
eﬁt from both atomic and more complex actions to produce
advanced algorithmic behaviour. Each action has assigned a
level. The level 0 actions are the atomic actions that do not
need to be learned. Discovered programs have positive lev-
els. Each program can only call lower-level action or lower-
level learned programs. The original work supported also
recursion, by letting programs calling themselves. However,
in this work, we do not experiment with recursive programs.
In addition, each program and atomic action has associated
pre-conditions, in the form of boolean constraints, condi-
tioned on the environment state. Pre-conditions ensures we
are consistently generating programs that are correct and
feasible. For instance, the pre-condition of the Quicksort
program is that all pointers must be at the beginning of the
list. Pre-conditions are not learned, but they are provided be-
forehand. The objective is to learn a library of programs or-
ganized into a hierarchy. In our case, each task corresponds
to learning a single program (e.g., the partition function
of Quicksort). The reward function returns 1 if the program
is correct, 0 otherwise. The goal of the agent is to maximize
the expected reward for all the tasks. This goal conﬁgures as
a multi-task reinforcement learning and as a discrete search
problem. The search space is sparse since there exist many
possible programs but very few viable ones. It also makes
the reward function sparse since we get a positive reward
and thus learn if and only if we obtain the correct program.

Figure 2: 1. Each simulation explores the tree by selecting those actions that maximize a target objective. 2. If we ﬁnd a new
node that is not an atomic function, we run A-MCTS recursively by resetting to zero the LSTM state. 3. Given the expanded
leaf node, we compute the program and actions policies (πmcts
). We sample from the two policies to add n new
child nodes (n << M where M is the total possible child nodes ). 4. We return the value V, and we propagate it back in the
tree.

and πmcts
a

p

selection, recursion, expansion and backup. Many different
ﬂavours of MCTS are available (for instance, Swiechowski
et al. (2021); Grill et al. (2020); Xiao, Mei, and M¨uller
(2018)). In this work, we build upon the recursive MCTS
developed by Pierrot et al. (2019). We propose an Approx-
imate MCTS (A-MCTS) by extending the expansion phase
not to expand all the possible nodes available. This improve-
ment is needed to account for larger search spaces that would
be prohibitive to explore thoroughly. In our setting, we have
discrete arguments, and the original MCTS requires creat-
ing a node for each program/argument pair available. If we
increase the number of available arguments, the number of
program/argument combinations grows signiﬁcantly.

In our setting, each node of the tree represents the en-
vironment’s state at time t and each transition represents
a program call. Figure 2 shows a description of the A-
MCTS algorithm. Given the policies πmcts
, we
add Dirichlet noise to foster exploration and we sample n
program/arguments pairs, (pi, ai), (n is a hyperparameter of
the model) and we add them as possible future available ac-
tions. The simulation phase will be shorter with fewer nodes.
By exploring fewer nodes, we reward immediately good so-
lutions. Thanks to the random perturbations, we will eventu-
ally explore all the available conﬁgurations if good solutions
are sparse.

and πmcts
a

p

Training

During a training iteration, the agent selects a program pt to
learn. It executes nep episodes by exploring the search space
with A-MCTS. The data gathered during the episodes are
aggregated to construct the tree policy vectors for both pro-
grams and arguments, πmcts
. After each success-
ful episode, we collect the observations, the hidden states,
the task indexes, the rewards, πmcts
into an ex-
ecution trace. More formally, an execution trace for a given
program pt is a tuple (eI , pt, ht, eO, rt, πmcts
). An
execution trace gives the exact sequence of actions that, if
applied to the input eI , produce the output eO. The ﬁnal dis-

and πmcts
a

and πmcts
a

, πmcts
a

p

p

p

covered execution traces are stored in a replay buffer. Then,
a mini-batch is sampled from the replay buffer, and the agent
is trained on this data to minimize this adjusted loss func-
tion:

X

l =

− (πmcts
p

batch

|

)T log πp
{z
}
lpolicy

− (πmcts
a

)T log πa
{z
}
larguments

+ (V − r)2
{z
}
lvalue

|

|

(5)
We want to push the network to reproduce the execu-
tion traces found by jointly minimizing the cross-entropy
between the policies discovered by MCTS and the poli-
cies generated by the network. lpolicy minimizes the cross-
entropy between the program policies, πmcts
and πp.
larguments minimizes the same cross-entropy but between
the two arguments policies, πmcts
and πa. Instead, lvalue
pushes the network to generate the correct value function V .
We also employ curriculum learning (Andreas, Klein, and
Levine 2017) to focus on learning programs by following the
hierarchy, such as to learn lower-level programs ﬁrst. More-
over, curriculum learning ensures choosing the next program
to learn by looking at the success rate of the single programs.
This rule means that programs that fail often are picked more
often for learning.

a

p

Given the environment state, the agent emits a program
policy πp and an argument policy πa. The policies are such
that P
M π(i) = 1, where M is the total number of available
programs or arguments. The best program/arguments pair
(p∗, a∗) is given by:

p∗ = arg max(πp)

a∗ = arg max(πa)

(6)

Re-train over failed environments
We noticed that a small fraction of training environments are
substantially harder to solve and are thus mostly neglected
when optimizing average performance during training. The
result is that sub-optimal programs are learned, and the per-
formance degradation becomes apparent when programs are
evaluated on the longer lists used for testing. To tackle this

problem and improve robustness and generalization, we im-
plemented a function g(pi) to re-train over previously failed
environments. Namely, given a task Ti to learn, if we get a 0
reward, we will store the used environment eI into a buffer
called ef ailed. Then, if we happen to try to learn the task Ti
again, we will run A-MCTS by sampling with a small prob-
ability (cid:15) an environment from ef ailed in which the model
had failed to solve in previous iterations. In the other case,
we sample the next state from a normal distribution over the
environment conditioned on the task T .

g(pi) =

(cid:26)eI ∼ ef ailed
eI ∼ N

(cid:15)
1 − (cid:15)

(7)

The buffer ef ailed implements some curriculum learning
since, given task Ti, if we use previously failed states, we
will sample the failed states by looking at the success rate. If
a state fails more often than others, then it will be sampled
more frequently.

Experiments
We tested our approach on a sorting task. It is a familiar set-
ting used by many others (Reed and de Freitas 2016; Pierrot
et al. 2019; Graves, Wayne, and Danihelka 2014). We fo-
cused on learning the hierarchy of programs needed to per-
form the Quicksort algorithm. Unlike the Bubblesort algo-
rithm used in previous works (Pierrot et al. 2019), Quicksort
cannot be easily written without functions that can accept ar-
guments. Therefore, we choose it as the target for our exper-
iments. Quicksort is a divide-and-conquer algorithm (Hoare
1962). It works by selecting a “pivot” value that partitions
the list into sub-lists which it then sorts. The algorithm has
an average time complexity of O(n log n), also presenting
good space complexity, O(log n) extra bits to sort n ele-
ments. The algorithm is more reﬁned to reach these perfor-
mances, especially the method used to select the pivot. The
code for the experiments is freely available1.

We consider an environment composed of three main
components: a list of n integers, three pointers that can refer-
ence the list’s elements and a stack that can save and retrieve
the pointers’ current positions. The programs can accept at
most three arguments. Each argument represents a reference
to one of the three pointers available in the environment.
We also added an empty value to signal arguments that the
given function has to ignore. Additionally, as in regular pro-
gramming languages, the arguments order count (e.g., func-
tion(a,b,c) has a different meaning from function(b,c,a)).

We trained the architecture with lists from 2 to 7 ele-
ments. The list’s elements were integers, and we constrained
them between 0 and 10. We trained four different models.
Malphanpi is the baseline, and it uses the vanilla architec-
ture from Pierrot et al. (2019). M s
alphanpi is again a baseline
model which uses our A-MCTS instead. Malphaargs and
M s
alphaargs employ our custom architecture presented in the
previous sections. The former uses the standard MCTS, and
the latter uses A-MCTS instead.

The higher-level programs that compose our hierarchy
and which will be learned are: partition update,

1github.com/geektoni/learning programs with arguments

partition, quicksort update and quicksort.
They correspond to certain pieces of the original Quicksort
algorithm. The complete lists and descriptions of learnable
programs and atomic actions can be found in the Appendix.
The Malphanpi and M S
alphanpi models learn functions that
do not accept arguments. Therefore, we provided them with
an augmented set of atomic actions, which we built by tak-
ing the cartesian product between the atomic actions and the
possible arguments they can accept.

Discussion
While training, we also recorded the total number of nodes
expanded by the MCTS and A-MCTS procedures after each
training step. The idea is to check if we can converge to a
solution by doing an approximate exploration of the search
space by expanding fewer nodes.

As further evaluation studies, we validated the trained
models on lists of up to 60 elements to investigate the gener-
alization capabilities. Table 1 presents the validation results
for all the learnable programs.

From the results, the Malphaargs is the only model which
can learn the Quicksort algorithm correctly, whereas the
other models fail. It also shows some generalization capac-
ities since it can sort lists with a different length than those
used for training. We can also see how the various models
struggled when learning the quicksort update func-
tion.

Interestingly, the models trained with the approximate
procedure A-MTCS show similar convergence rates to those
trained with MCTS. They also show good generalization
performances, albeit not reaching the level of the origi-
nal MCTS. From Figure 3, we can look at the total nodes
expanded by both the approximate and exact procedures.
For speciﬁc programs, such as partition update or
partition, the A-MCTS can converge to a solution by
exploring fewer nodes with respect to the counterpart. How-
ever, the A-MCTS behaviour seems to be less stable than
MCTS, and further investigations are required to be able to
exploit its exploration efﬁciency without losing robustness.

Comparison with ILP Currently, many common program
synthesis problems can be solved either by ILP or by neural-
interpreter methods. We argue that neural-based solutions
require less formalization and speciﬁcations to learn an al-
gorithm. However, they are more data-intensive than a stan-
dard ILP procedure. As we move closer to real-world pro-
grams, we think that neural-based models are easier to use
since they rely on looser speciﬁcations. Moreover, we think
the use of a differentiable model can speed up convergence
when used as a prior for the discrete search method. In our
case, a differentiable model implicitly learns a representa-
tion of the target program given the traces, thus leading to
faster trace discovery.

Conclusion
This work presents some initial results to improve program
synthesis by generating more human-like programs starting
from input/output examples. We propose an improved Al-
phaNPI architecture to learn programs that can accept ar-

Program

List Length Malphanpi M S

alphaargs

partition update 5
10
20
40
60

partition

quicksort update

quicksort

5
10
20
40
60

5
10
20
40
60

5
10
20
40
60

1.00
1.00
1.00
1.00
1.00

1.00
1.00
1.00
1.00
1.00

0.72
0.64
0.56
0.36
0.38

0.10
0
0
0
0

alphanpi Malphaargs M S
1.00
0.96
1.00
0.98
1.00
0.96
1.00
0.94
1.00
0.96

1.00
1.00
0.98
1.00
1.00

0.98
0.78
0.80
0.80
0.76

0.58
0.42
0.34
0.30
0.26

0.02
0
0
0
0

1.00
0.98
1.00
1.00
0.98

1.00
0.96
0.90
0.98
0.94

1.00
0.66
0.42
0.22
0.02

0.96
0.78
0.70
0.82
0.80

0.60
0.48
0.56
0.40
0.44

0.06
0
0
0
0

Table 1: Generalization accuracy of the learned programs by varying the length of the list. Each model was tested on 50 different
random lists for each given length. The accuracy gives the fraction of lists correctly sorted. The bold values indicate the best
results for that given list length.

guments to diversify their behaviour. The support for argu-
ments enables our revised architecture to produce richer pro-
grams and learn more complex algorithms with respect to
the original work. It also generates code that is more sim-
ilar to the one produced with current high-level program-
ming languages. Additionally, we present an Approximate
Monte Carlo Tree Search method that enables us to con-
verge by exploring only a fraction of the search space. We
benchmarked our method by learning a well-known sorting
algorithm, Quicksort, showing how our approach manages
to learn it effectively while AlphaNPI fails to converge. We
also show how the learned program generalizes when sort-
ing lists of increasing length. This work is a ﬁrst attempt
at enriching neural program synthesis algorithms with addi-
tional features of high-level programming languages. Many
open issues are still to be addressed. First, while the approx-
imate MCTS procedure effectively reduces the search space
and can potentially contribute to scale up the complexity of
learnable programs, further investigations are needed to im-
prove its robustness. Second, in this work only atomic ac-
tions can accept arguments, whereas higher-level programs
can accept only the “empty” argument. This behaviour hap-
pens because all methods operate in the same environment,
and there is no proper program scope. Additional studies are
required to add this notion of program space to better use the
new arguments.

References
Andreas, J.; Klein, D.; and Levine, S. 2017. Modular Mul-
titask Reinforcement Learning with Policy Sketches. In Pre-
cup, D.; and Teh, Y. W., eds., Proceedings of the 34th Interna-
tional Conference on Machine Learning, volume 70 of Pro-
ceedings of Machine Learning Research, 166–175. PMLR.
URL http://proceedings.mlr.press/v70/andreas17a.html.
Coulom, R. 2006. Efﬁcient Selectivity and Backup Op-
In Proceedings of
erators in Monte-Carlo Tree Search.
the 5th International Conference on Computers and Games,
ISBN
CG’06, 72–83. Berlin, Heidelberg: Springer-Verlag.
3540755373.
Cropper, A.; Dumancic, S.; and Muggleton, S. H. 2020. Turn-
ing 30: New Ideas in Inductive Logic Programming. CoRR
abs/2002.11002. URL https://arxiv.org/abs/2002.11002.
Graves, A.; Wayne, G.; and Danihelka, I. 2014. Neural Turing
Machines. CoRR abs/1410.5401. URL http://arxiv.org/abs/
1410.5401.
Graves, A.; Wayne, G.; Reynolds, M.; Harley, T.; Danihelka,
I.; Grabska-Barwi´nska, A.; Colmenarejo, S. G.; Grefenstette,
E.; Ramalho, T.; Agapiou, J.; Badia, A. P.; Hermann, K. M.;
Zwols, Y.; Ostrovski, G.; Cain, A.; King, H.; Summerﬁeld,
C.; Blunsom, P.; Kavukcuoglu, K.; and Hassabis, D. 2016.
Hybrid computing using a neural network with dynamic ex-
ternal memory. Nature 538(7626): 471–476. ISSN 00280836.
URL http://dx.doi.org/10.1038/nature20101.
Grill, J.-B.; Altch´e, F.; Tang, Y.; Hubert, T.; Valko, M.;
Antonoglou, I.; and Munos, R. 2020. Monte-Carlo Tree
Search as Regularized Policy Optimization. In III, H. D.; and
Singh, A., eds., Proceedings of the 37th International Con-
ference on Machine Learning, volume 119 of Proceedings

Figure 3: Total node expanded by models (top) and the corresponding accuracy (bottom) while training. The node count
is expressed with the log scale. In some cases, the Approximate MTCS shows to enable convergence by exploring fewer
nodes. Moreover, once the models learn a program, the node expansion slows down. Interestingly, it seems that the
quicksort update function is harder to learn. Unfortunately, only one model is able to learn the complete Quicksort
procedure.

Reed, S.; and de Freitas, N. 2016. Neural Programmer-
Interpreters.

Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;
Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,
A.; et al. 2017. Mastering the game of go without human
knowledge. nature 550(7676): 354–359.

Swiechowski, M.; Godlewski, K.; Sawicki, B.; and Mandz-
iuk, J. 2021. Monte Carlo Tree Search: A Review of Re-
cent Modiﬁcations and Applications. CoRR abs/2103.04931.
URL https://arxiv.org/abs/2103.04931.

Xiao, C.; Mei, J.; and M¨uller, M. 2018. Memory-augmented
monte carlo tree search. In Thirty-Second AAAI Conference
on Artiﬁcial Intelligence.

Zaremba, W.; Mikolov, T.; Joulin, A.; and Fergus, R. 2016.
Learning Simple Algorithms from Examples.
In Balcan,
M. F.; and Weinberger, K. Q., eds., Proceedings of The
33rd International Conference on Machine Learning, vol-
ume 48 of Proceedings of Machine Learning Research, 421–
429. New York, New York, USA: PMLR. URL https://
proceedings.mlr.press/v48/zaremba16.html.

of Machine Learning Research, 3769–3778. PMLR. URL
http://proceedings.mlr.press/v119/grill20a.html.
Hoare, C. A. R. 1962. Quicksort. The Computer Journal 5(1):
10–16. ISSN 0010-4620. doi:10.1093/comjnl/5.1.10. URL
https://doi.org/10.1093/comjnl/5.1.10.
Jumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.;
Ronneberger, O.; Tunyasuvunakool, K.; Bates, R.; ˇZ´ıdek, A.;
Potapenko, A.; Bridgland, A.; Meyer, C.; Kohl, S. A. A.; Bal-
lard, A. J.; Cowie, A.; Romera-Paredes, B.; Nikolov, S.; Jain,
R.; Adler, J.; Back, T.; Petersen, S.; Reiman, D.; Clancy, E.;
Zielinski, M.; Steinegger, M.; Pacholska, M.; Berghammer,
T.; Bodenstein, S.; Silver, D.; Vinyals, O.; Senior, A. W.;
Kavukcuoglu, K.; Kohli, P.; and Hassabis, D. 2021. Highly
accurate protein structure prediction with AlphaFold. Nature
doi:10.1038/s41586-021-03819-2. (Accelerated article pre-
view).
Kocsis, L.; and Szepesv´ari, C. 2006. Bandit based Monte-
In In: ECML-06. Number 4212 in LNCS,
Carlo Planning.
282–293. Springer.
Inductive
Muggleton, S.; and de Raedt, L. 1994.
The Jour-
Logic Programming: Theory and methods.
nal of Logic Programming 19-20: 629–679.
ISSN 0743-
1066.
doi:https://doi.org/10.1016/0743-1066(94)90035-
3. URL https://www.sciencedirect.com/science/article/pii/
0743106694900353. Special Issue: Ten Years of Logic Pro-
gramming.
Pierrot, T.; Ligner, G.; Reed, S.; Sigaud, O.; Perrin, N.; Lat-
erre, A.; Kas, D.; Beguir, K.; and de Freitas, N. 2019. Learn-
ing Compositional Neural Programs with Recursive Tree
Search and Planning.
Pierrot, T.; Perrin, N.; Behbahani, F.; Laterre, A.; Sigaud, O.;
Beguir, K.; and de Freitas, N. 2020. Learning Compositional
Neural Programs for Continuous Control.

Appendix

Table 2 and 3 show the complete hierarchy of programs. The former shows programs for the Malphanpi and M S
The latter shows programs for the Malphaargs and M S

alphaargs models. Table 4 shows the programs preconditions.

alphanpi models.

Table 2: Hierarchy of the programs for Quicksort. In this case, the programs cannot accept arguments. The level 0 programs are
not learned and they are the atomic actions.

Program
quicksort
quicksort update Execute partition call and populate the stack with the next pointers.
partition
Order a sub-array and return the pivot for the next operation.
partition update Swap the elements pointed by p3 (pivot) and p1 if p3 pointed value is less than p2 pointed

Description
Sort the list.

Level
5
4
2
1

save ptr 1
load ptr 1
push
pop
swap
swap pivot
ptr i left
ptr i right

value.
Save pointer 1 position into the temporary registry.
Load the temporary registry value into the pointer 1 position.
Push pointers values inside the stack.
Pop and restore pointers values from the stack.
Swap elements pointed by pointer 1 and by pointer 2.
Swap elements pointed by pointer 1 and by pointer 3 (pivot).
Move pointer i (where i ∈ {1, 2, 3}) one step left.
Move pointer i (where i ∈ {1, 2, 3}) one step right.

0
0
0
0
0
0
0
0

Table 3: Hierarchy of the programs for Quicksort. In this case, the atomic actions can accept arguments. The level 0 programs
are not learned and they are the atomic actions.

Description
Sort the list.

Program
quicksort
quicksort update Execute partition call and populate the stack with the next pointers.
partition
partition update
save ptr
load ptr
push
pop
swap
ptr left
ptr right

Order a sub-array and return the pivot for the next operation.
Swap the elements pointed by p3 (pivot) and p1 if p3 pointed value is less than p2 pointed value.
Save a given pointer position into the temporary registry.
Load the temporary registry value into the given pointer position.
Push pointers values inside the stack.
Pop and restore pointers values from the stack.
Given two pointers, swap the corresponding elements
Move pointer one, two or all three pointers one step left.
Move pointer one, two or all three pointers one step right.

Arguments Level
0
0
0
0
1
1
0
0
2
3
3

5
4
2
1
0
0
0
0
0
0
0

Table 4: Pre-conditions associated to each Quicksort program.

Program
quicksort

Pre-condition
Pointer 1 and 3 are at the extreme left of the list. Pointer 2 is at the extreme right of the
list. The stack is empty.

quicksort update The stack is not empty and the temporary registry is empty.
partition
partition update Pointer 1 position must be lower than Pointer 3 position. Pointer 3 position must be lower

The temporary registry must contain Pointer 1 position.

save ptr 1
load ptr 1
push
pop
swap
swap pivot
ptr i left
ptr i right

than Pointer 2 position. The temporary registry must be not empty.
No pre-condition.
The temporary stack must not be empty.
Check custom boolean condition (p1 + 1 < p2) ∨ (p1 − 1 > 0 ∧ p3 < p1 − 1).
The stack must not be empty.
Pointer 1 position must be different than pointer 2 position.
Pointer 1 position must be different than pointer 3 position.
Pointer i (where i ∈ {1, 2, 3}) is not at the extreme left of the list.
Pointer i (where i ∈ {1, 2, 3}) is not at the extreme right of the list.

