Probabilistic Programs with Stochastic Conditioning

David Tolpin 1 Yuan Zhou 2 Tom Rainforth 2 Hongseok Yang 3

1
2
0
2

r
a

M
8

]

G
L
.
s
c
[

3
v
2
8
2
0
0
.
0
1
0
2
:
v
i
X
r
a

Abstract

We tackle the problem of conditioning probabilis-
tic programs on distributions of observable vari-
ables. Probabilistic programs are usually condi-
tioned on samples from the joint data distribution,
which we refer to as deterministic conditioning.
However, in many real-life scenarios, the observa-
tions are given as marginal distributions, summary
statistics, or samplers. Conventional probabilistic
programming systems lack adequate means for
modeling and inference in such scenarios. We
propose a generalization of deterministic condi-
tioning to stochastic conditioning, that is, condi-
tioning on the marginal distribution of a variable
taking a particular form. To this end, we ﬁrst de-
ﬁne the formal notion of stochastic conditioning
and discuss its key properties. We then show how
to perform inference in the presence of stochas-
tic conditioning. We demonstrate potential usage
of stochastic conditioning on several case studies
which involve various kinds of stochastic condi-
tioning and are difﬁcult to solve otherwise. Al-
though we present stochastic conditioning in the
context of probabilistic programming, our formal-
ization is general and applicable to other settings.

1. Introduction

implement

statistical models.
Probabilistic programs
Mostly, probabilistic programming closely follows the
Bayesian approach (Kim & Pearl, 1983; Gelman et al.,
2013): a prior distribution is imposed on latent random
variables, and the posterior distribution is conditioned on
observations (data). The conditioning may take the form of
a hard constraint (a random variate must have a particular
value, e.g. as in Church (Goodman et al., 2008)), but a
more common practice is that conditioning is soft — the
observation is assumed to come from a distribution, and
the probability (mass or density) of the observation given

1Ben-Gurion University of the Negev 2University of Oxford
3School of Computing, KAIST. Correspondence to: David Tolpin
<david.tolpin@gmail.com>.

Copyright 2021 by the author(s).

the distribution is used in inference (Carpenter et al., 2017;
Goodman & Stuhlm¨uller, 2014; Tolpin et al., 2016; Ge et al.,
2018). In the standard setting, the conditioning is determin-
istic — observations are ﬁxed samples from the joint data
distribution. For example, in the model of an intensive care
unit patient, an observation may be a vector of vital sign
readings at a given time. This setting is well-researched,
widely applicable, and robust inference is possible (Hoff-
man & Gelman, 2011; Wood et al., 2014; TensorFlow, 2018;
Bingham et al., 2019).

However, instead of samples from the joint data distribution,
observations may be independent samples from marginal
data distributions of observable variables, summary statis-
tics, or even data distributions themselves, provided in
closed form or as samplers. These cases naturally appear
in real life scenarios: samples from marginal distributions
arise when different observations are collected by different
parties, summary statistics are often used to represent data
about a large population, and data distributions may express
uncertainty during inference about future states of the world,
e.g. in planning. Consider the following situations:

• A study is performed in a hospital on a group of pa-
tients carrying a certain disease. To preserve the pa-
tients’ privacy, the information is collected and pre-
sented as summary statistics of each of the monitored
symptoms, such that only marginal distributions of the
symptoms are approximately characterized. It would
be natural to condition the model on a combination of
symptoms, but such combinations are not observable.
• A traveller regularly drives between two cities and
wants to minimize the time this takes. However, some
road sections may be closed due to bad weather, which
can only be discovered at a crossing adjacent to the
road section. A policy that minimizes average travel
time, given the probabilities of each road closure, is
required, but ﬁnding this policy requires us to con-
dition on the distribution of states (Papadimitriou &
Yannakakis, 1989).

Most existing probabilistic programming systems, which
condition on samples from the joint data distribution, cannot
be directly applied to such scenarios for either model speci-
ﬁcation or performing inference. In principle, such models
can be expressed as nested probabilistic programs (Rain-

 
 
 
 
 
 
Probabilistic Programs with Stochastic Conditioning

forth, 2018). However, inference in such programs has lim-
ited choice of algorithms, is computationally expensive, and
is difﬁcult to implement correctly (Rainforth et al., 2018).

In some speciﬁc settings, models can be augmented with
additional auxiliary information, and custom inference tech-
niques can be used. For example, van de Meent et al. (2016)
employs black-box variational inference on augmented prob-
abilistic programs for policy search. But problem-speciﬁc
program augmentation and custom inference compromise
the core promise of probabilistic programming: programs
and algorithms should be separated, and off-the-shelf in-
ference methods should be applicable to a wide range of
programs in an automated, black-box manner (Wood et al.,
2014; Tolpin, 2019; Winn et al., 2019).

To address these issues and provide a general solution to
deﬁning models and running inference for such problems,
we propose a way to extend deterministic conditioning
p(x|y = y0), i.e. conditioning on some random variable
in our program y taking on a particular value y0, to stochas-
tic conditioning p(x|y∼D0), i.e. conditioning on y having
the marginal distribution D0. In the context of a higher-
order sampling process in which we ﬁrst sample a random
probability measure D ∼ p(D) and then sample a ran-
dom variable y ∼ D, stochastic conditioning p(x|y∼D0)
amounts to conditioning on the event D = D0, that is on
the random measure D itself taking the particular form D0.
Equivalently, we can think on conditioning on the event
y ∼ D0 (based on the ﬁrst step of our sampling process),
which says that the marginal distribution of y is given by
the distribution D0. We can develop intuition for this by
considering the special case of a discrete y, where y ∼ D0
means that the proportion of each possible instance of y
that occurs will be D0 if we conduct an inﬁnite number of
rollouts and sample a value of y for each.

To realize this intuition, we formalize stochastic condition-
ing and analyze its properties and usage in the context of
probabilistic programming, further showing how effective
automated inference engines can be set up for the resulting
models. We note that our results also address a basic concep-
tual problem in Bayesian modeling, and are thus applicable
to non-probabilistic programming settings as well.

We start with an informal introduction providing intuition
about stochastic conditioning (Section 2). Then, we deﬁne
the notion of stochastic conditioning formally and discuss
its key properties (Section 3), comparing our deﬁnition with
possible alternatives and related concepts. Following that,
we discuss efﬁcient inference for programs with stochastic
conditioning (Section 4). In case studies (Section 5), we
provide probabilistic programs for several problems of sta-
tistical inference which are difﬁcult to approach otherwise,
perform inference on the programs, and analyze the results.

2. Intuition

To get an intuition behind stochastic conditioning, we take
a fresh look at the Beta-Bernoulli generative model:

x ∼ Beta(α, β),

y ∼ Bernoulli(x).

(1)

The Beta prior on x has α and β parameters, which are inter-
preted as the belief about the number of times y=1 and y=0
seen before. Since Beta is the conjugate prior for Bernoulli,
belief updating in (1) can be performed analytically:

x|y ∼ Beta(α + y, β + 1 − y)

We can compose Bayesian belief updating. If after observ-
ing y we observed y(cid:48), then1

x|y(cid:48) ◦ y ∼ Beta(α + y + y(cid:48), β + 2 − y − y(cid:48)).

In general, if we observe y1:n = yn ◦ ... ◦ y2 ◦ y1, then
(cid:17)

(cid:16)

(cid:88)n

(cid:88)n

x|y1:n ∼ Beta

α +

yi, β + n −

yi

.

i=1

i=1

In (1) (also in many more general exchangeable settings)
belief updating is commutative — the posterior distribution
does not depend on the order of observations. One may view
y1:n as a multiset, rather than a sequence, of observations.

Let us now modify the procedure of presenting the evidence.
Instead of observing the value of each of yn ◦ .... ◦ y2 ◦ y1
in order, we just observe n variates, of which k = (cid:80)n
i=1 yi
variates have value 1 (but we are not told which ones). It
does not matter which of the observations are 1 and which
are 0. We can even stretch the notion of a single observa-
tion and say that it is, informally, a ‘combination’ of 1 with
probability θ = k
n and 0 with probability 1 − θ. In other
words, we can view each observation yi as an observation of
distribution Bernoulli(θ) itself; the posterior distribution of
x given n observations of Bernoulli(θ) should be the same
as the posterior distribution of x given y1:n. This extended
interpretation of belief updating based on observing distri-
butions lets us answer questions about the posterior of x
given that we observe the distribution Bernoulli(θ) of y:
x|(cid:0)y∼Bernoulli(θ)(cid:1) ∼ Beta(α + θ, β + 1 − θ).

Note that observing a distribution does not imply observing
its parametric representation. One may also observe a dis-
tribution through a random source of samples, a black-box
unnormalized density function, or summary statistics.

Commonly, probabilistic programming involves weighing
different assignments to x by the conditional probability of
y given x. For model (1),

p(y|x) = xy(1 − x)1−y.

1By y(cid:48) ◦ y we denote that y(cid:48) was observed after observing y

and updating the belief about the distribution of x.

Probabilistic Programs with Stochastic Conditioning

The conditional probability of observing a ﬁxed value ex-
tends naturally to observing a distribution:

An intuition behind the deﬁnition can be seen by rewrit-
ing (3) as a type II geometric integral:

p(y∼Bernoulli(θ)|x) = xθ(1 − x)1−θ
= exp (θ log x + (1 − θ) log(1 − x))

(2)

= exp

(cid:16) (cid:88)

y∈{0,1}

pBern(θ)(y) · log pBern(x)(y)

(cid:17)

where pBern(r)(y) is the probability mass function of the
distribution Bernoulli(r) evaluated at y. Note that pBern(x)
inside the log is precisely the conditional probability of y
in model (1). Equation (2) lets us specify a probabilistic
program for a version of model (1) with stochastic condi-
tioning — on a distribution rather than on a value. In the
next section, we introduce stochastic conditioning formally,
using a general form of (2).

3. Stochastic Conditioning

Let us deﬁne stochastic conditioning formally.
In what
follows, we mostly discuss the continuous case where the
observed distribution D has a density q. For the case that
D does not have a density, the notation q(y)dy in our dis-
cussion should be replaced with D(dy), which means the
Lebesgue integral with respect to the distribution (or prob-
ability measure) D. For the discrete case, probability den-
sities should be replaced with probability masses, and in-
tegrals with sums. Modulo these changes, all the theorem
and propositions in this section carry over to the discrete
case. A general measure-theoretic formalization of stochas-
tic conditioning, which covers all of these cases uniformly,
is described in Appendix A.

Deﬁnition 1. A probabilistic model with stochastic condi-
tioning is a tuple (p(x, y), D) where (i) p(x, y) is the joint
probability density of random variable x and observation
y, and it is factored into the product of the prior p(x) and
the likelihood p(y|x) (i.e., p(x, y) = p(x)p(y|x)); (ii) D
is the distribution from which observation y is marginally
sampled, and it has a density q(y).

Unlike in the usual setting, our objective is to infer
p(x|y∼D), the distribution of x given distribution D, rather
than an individual observation y. To accomplish this objec-
tive, we need to be able to compute p(x, y∼D), a possibly
unnormalized density on x and distribution D. We deﬁne
p(x, y∼D) = p(x)p(y∼D|x) where p(y∼D|x) is the fol-
lowing unnormalized conditional density:

Deﬁnition 2. The (unnormalized) conditional density
p(y∼D|x) of D given x is

p(y∼D|x) = exp

(log p(y|x)) q(y)dy

(3)

(cid:19)

(cid:18)(cid:90)

Y

where q is the density of D.

p(y∼D|x) =

(cid:89)
Y

p(y|x)q(y)dy.

Deﬁnition 2 hence can be interpreted as the probability of
observing all possible draws of y from D, each occurring
according to its probability q(y)dy.

At this point, the reader may wonder why we do not take the
following alternative, frequently coming up in discussions:

p1(y∼D|x) =

(cid:90)

Y

p(y|x) q(y)dy.

(4)

One may even see a connection between (4) and Jeffrey’s
soft evidence (Jeffrey, 1990)

p(x|ysoft∼D) =

(cid:90)

Y

q(y)p(x|y) dy,

(5)

although the latter addresses a different setting. In soft evi-
dence, an observation is a single value y, but the observer
does not know with certainty which of the values was ob-
served. Any value y from Y , the domain of D, can be
observed with probability q(y), but p(ysoft∼D|x) cannot be
generally deﬁned (Chan & Darwiche, 2003; Ben Mrad et al.,
2013). In our setting, distribution D is observed, and the
observation is certain.

We have two reasons to prefer (3) to (4). First, as Propo-
sition 1 will explain, our p(y∼D|x) is closely related to
the KL divergence between q(y) and p(y|x), while the al-
ternative p1(y∼D|x) in (4) lacks such connection. The
connection helps understand how p(y∼D|x) alters the prior
of x. Second, p(y∼D|x) treats all possible draws of y
more equally than p1(y∼D|x) in the following sense. Both
p(y∼D|x) and p1(y∼D|x) are instances of so called power
mean (Bullen, 2003) deﬁned by

pα(y∼D|x) =

(cid:18)(cid:90)

Y

p(y|x)α q(y)dy

(cid:19) 1

α

.

(6)

Setting α to 0 and 1 gives our p(y∼D|x) and the alternative
p1(y∼D|x), respectively. A general property of this power
mean is that as α tends to ∞, draws of y with large p(y|x)
contribute more to pα(y∼D|x), and the opposite situation
happens as α tends to −∞. Thus, the α = 0 case, which
gives our p(y∼D|x), can be regarded as the option that is
the least sensitive to p(y|x).

Proposition 1. For a given D with density q,

arg max

x

p(y∼D|x) = arg min

x

KL(q(y)||p(y|x)).

(7)

In particular, if there exists x∗ such that p(y|x∗) ≡ q(y),
then x∗ = arg maxx p(y∼D|x).

Probabilistic Programs with Stochastic Conditioning

Proof. We prove the proposition by re-expressing
log p(y∼D|x) in terms of the negative KL divergence:

(cid:90)

Y

log p(y∼D|x) =

(log p(y|x)) q(y)dy

=

=

(cid:18)(cid:90)

Y
(cid:18)(cid:90)

Y

(log q(y)) q(y)dy

−

(cid:19)

(cid:19)

(cid:90)

(cid:18)

Y

log

(cid:19)

q(y)
p(y|x)

q(y)dy

(log q(y)) q(y)dy

− KL(q(y)||p(y|x)).

Since the ﬁrst term in the last line does not depend on x, we
have the equation (7).

The proposition does not hold for the alternative def-
Even if there exists x∗
inition p1(y∼D|x) in (4).
such that p(y|x∗) ≡ q(y), all we can say about
p(y| arg maxx p1(y∼D|x)) is just
it maximizes
(cid:82)
Y p(y|x) q(y)dy, not that it is q(y). For example, consider
the ﬁnite discrete case and assume that there also exists x†
with p(y|x†) = Dirac(arg maxy q(y)). Then x†, not x∗,
maximizes p1(y∼D|x).

that

The next theorem explains our setting formally and shows
that p(y∼D|x) has a ﬁnite normalization constant.
Theorem 1. Assume that the distribution Dθ is parame-
terized by θ ∈ Θ ⊆ Rp. Let qθ be its density. Then,
p(y∼Dθ|x) has a ﬁnite normalization constant C over
D = {Dθ | θ ∈ Θ} if the following uniform-bound condi-
tion holds: supy∈Y
Θ qθ(y) dθ < ∞. Thus, in this case,
p(y∼Dθ|x)/C is a conditional probability density on D.

(cid:82)

Proof. Let C = (cid:82)

Θ p(y∼Dθ|x) dθ. Then,

(cid:90)

C =

exp

(cid:18)(cid:90)

Θ

Y

(log p(y|x)) qθ(y)dy

dθ.

(8)

(cid:19)

We compute a ﬁnite upper bound of C as follows:

(cid:90)

(cid:90)

C ≤1

exp(log p(y|x)) qθ(y)dydθ

Θ
(cid:90)

(cid:90)

=

Y

p(y|x) qθ(y)dydθ

Θ
(cid:90)

Y
(cid:18)(cid:90)

=2

(cid:19)

qθ(y) dθ

p(y|x)dy

Y

(cid:18)

≤

sup
y(cid:48)∈Y
(cid:90)

= sup
y(cid:48)∈Y

Θ

Θ
(cid:90)

Θ

qθ(y(cid:48)) dθ

(cid:18)(cid:90)

(cid:19)

·

(cid:19)

p(y|x)dy

Y

qθ(y(cid:48)) dθ <3 ∞.

Here ≤1 is by Jensen’s inequality, =2 follows from Fubini’s
theorem, and <3 uses the uniform-bound condition.

Let us illustrate the restriction on the set of distributions D
imposed by the uniform-bound condition in Theorem 1. The

set D = {Normal(θ, 1) | θ ∈ R} of normal distributions
with the ﬁxed variance 1 meets the condition, so that when
normalized, p(y∼D|x) becomes a probability density over
D. However, if we permit the variance to vary, the resulting
set D(cid:48) = {Normal(θ, σ2) | θ ∈ R, σ2 ∈ (0, ∞)} does not
satisfy the condition. Appendix A contains another set that
violates a measure-theoretic generalization of our condition
(described in Appendix A as well) and, furthermore, does
not have a normalization constant for some choice of p.

The next proposition shows that stochastic conditioning
generalizes conventional conditioning on a value. It uses a
non-density version of Deﬁnition 1 where the integral over
q(y)dy is understood as the integral over the distribution
(i.e., probability measure) D.
Proposition 2. When observing a distribution reduces to
observing a single value, D = Dirac(y), conditioning on
a distribution reduces to conventional conditioning on a
value: p(Dirac(y)|x) = p(y|x).

4. Inference

Algorithms for deterministic conditioning cannot be applied
without modiﬁcation to probabilistic programs with stochas-
tic conditioning. One approach is to rely on nested Monte
Carlo estimation (Rainforth et al., 2018). However, proba-
bilistic programs with stochastic conditioning constitute an
important special case of nested models, and it is possible to
leverage properties of such programs to apply a wider class
of inference algorithms. In our setting, log p(y∼D|x) is
easy to estimate, and p(y∼D|x) can be estimated in a bias-
adjusted manner, hence inference algorithms using these
estimates can be applied effectively.

(cid:88)N

log p(y∼D|x) ≈

In particular, Deﬁnition 2 implies that an unbiased Monte
Carlo estimate of log likelihood log p(y∼D|x) is available
based on samples yi ∼ D through which D is observed:
1
N
Another setting in which an unbiased Monte Carlo estimate
of log likelihood is available is subsampling for inference
in models with tall data (Korattikara et al., 2014; Bardenet
et al., 2014; 2017; Maclaurin & Adams, 2014; Quiroz et al.,
2018; 2019; Dang et al., 2019). In models considered for
subsampling, K observations y1, y2, ..., yK are condition-
ally independent given x:

log p(yi|x)

(9)

i=1

p(y1, y2, ..., yK|x) =

(cid:89)K

i=1

p(yi|x)

(10)

Most inference algorithms require evaluation of likelihood
p(y1, y2, ..., yK|x), which is expensive if K is large. For
example, in importance sampling, the likelihood is in-
volved in the computation of importance weights. In many
Markov chain Monte Carlo methods, the ratio of likeli-
hoods of the proposed and the current state is a factor in

Probabilistic Programs with Stochastic Conditioning

the Metropolis-Hastings acceptance rate. Subsampling re-
places p(y1, y2, ..., yK|x) by an estimate based on N sam-
ples yi1, yi2, ..., yiN , N < K, which results in an unbiased
Monte Carlo estimate of log likelihood:

log p(y1, y2, ..., yK|x) ≈

K
N

(cid:88)N

j=1

log p(yij |x)

(11)

The only difference between (9) and (11) is in factor K,
and inference algorithms for subsampling can be applied to
stochastic conditioning with minor modiﬁcations.

A simple bias-adjusted likelihood estimate ˆp(x, y∼D), re-
quired for the computation of the weights in importance
sampling as well as of the acceptance ratio in pseudo-
marginal Markov chain Monte Carlo (Andrieu & Roberts,
2009), can be computed based on (9) (Ceperley & Dewing,
1999; Nicholls et al., 2012; Quiroz et al., 2018). Stochastic
gradient-based inference algorithms (Chen et al., 2014; Ma
et al., 2015; Hoffman et al., 2013; Ranganath et al., 2014;
Kucukelbir et al., 2017) rely on an unbiased estimate of the
gradient of log likelihood, which is trivially obtained by
differentiating both sides of (9).

We implemented inference in probabilistic programs with
stochastic conditioning for Infergo (Tolpin, 2019). To facili-
tate support for stochastic conditioning in other probabilis-
tic programming systems, we provide details on likelihood
estimation and some possible adaptations of inference al-
gorithms to stochastic conditioning, as well as pointers to
alternative adaptations in the context of subsampling, in
Appendix B.

5. Case Studies

In the case studies, we explore several problems cast as
probabilistic programs with stochastic conditioning. We
place y ∼ D above a rule to denote that distribution D is
observed through y and is otherwise unknown to the model,
as in (12). Some models are more natural to express in terms
of the joint probability that they compute than in terms of
distributions from which x is drawn and y is observed. In
that case, we put the expression for the joint probability
p(x, y) under the rule, as in (13).

y ∼ D

x ∼ Prior

y|x ∼ Conditional(x)

(12)

y ∼ D

p(x, y) = ...

(13)

The code and data for the case studies are provided
in repository https://bitbucket.org/dtolpin/
stochastic-conditioning.

cycle ride takes 15 ± 2 minutes via a highway. If rain is
expected, the commuter takes a taxi, and the trip takes 30±4
minutes, because of crowded roads which slow down a four-
wheeled vehicle. Sometimes, however, the rain catches the
commuter in the saddle, and the commuter rides slowly and
carefully through rural roads, arriving at 60 ± 8 minutes.
Given weather observations and trip durations, we want to
estimate the accuracy of rain forecasts, that is, the probabil-
ity of the positive forecast on rainy days pt (true positive)
and on dry days pf (false positive).

The problem is represented by the following model:

pr, pt, pf ∼ Beta(1, 1)

rain|pr ∼ Bernoulli(pr)
(cid:40)

(14)

willRain|pt, pf , rain ∼

duration|rain, willRain ∼

Bernoulli(pt) if rain
Bernoulli(pf ) otherwise






Normal(30, 4) if willRain
Normal(15, 2) if ¬rain
Normal(60, 8) otherwise

Model (14) can be interpreted as either a simulator that
draws samples of (rain, duration) given pr, pt, and pf , or
as a procedure that computes the conditional probability of
(rain, duration) given pr, pt, and pf . We use the simulator
interpretation to generate synthetic observations for 30 days
and pr = 0.2, pt = 0.8, pf = 0.1. The conditional probabil-
ity interpretation lets us write down a probabilistic program
for posterior inference of pt and pf given observations.

If, instead of observing (rain, duration) simultaneously, we
observe weather conditions and trip durations separately
and do not know correspondence between them (a common
situation when measurements are collected by different par-
ties), we can still write a conventional probabilistic program
conditioned on the Cartesian product of weather conditions
and trip durations, but the number of observations and, thus,
time complexity of inference becomes quadratic in the num-
ber of days. In general, when a model is conditioned on
the Cartesian product of separately obtained observation
sets, inference complexity grows exponentially with the di-
mensionality of observations, and inference is infeasible in
problems with more than a couple of observed features.

Alternatively, we can draw rain and duration from the obser-
vation sets randomly and independently, and stochastically
condition on D = Rains × Durations:

rain, duration ∼ Rains × Durations

...

(15)

5.1. Inferring the Accuracy of Weather Forecast

A person commutes to work either by motorcycle or, on
rainy days, by taxi. When the weather is good, the motor-

One can argue that the probabilistic program for the case of
independent sets of observations of rain and duration can
still be implemented with linear complexity by noting that
the domain of rain contains only two values, true and false,

Probabilistic Programs with Stochastic Conditioning

(a) deterministic

(b) averaged

(c) stochastic

(d) intensity

Figure 1: Commute to work: posteriors of pt and pf for
each of the four models. Despite being exposed to partial
information only, models with stochastic conditioning let us
infer informative posteriors.

and analytically averaging the evidence over rain. How-
ever, such averaging is often impossible. Consider a variant
of the problem in which the duration of a motorcycle trip
in rain depends on rain intensity. Stochastic conditioning,
along with inference algorithms that use a small number of
samples to estimate the log likelihood (magnitude or gra-
dient), lets us preserve linear complexity in the number of
observations (Doucet et al., 2015; Bardenet et al., 2017).

We ﬁt the model using stochastic gradient Hamiltonian
Monte Carlo and used 10 000 samples to approximate the
posterior. Figure 1 shows marginal posteriors of pt and pf
for each of the four models, on the same simulated data set.
Posterior distributions should be the same for the analyt-
ically averaged and stochastic models. The deterministic
model is exposed to more information (correspondence be-
tween rain occurrence and trip duration). Hence, the poste-
rior distributions are more sharply peaked. The stochastic
model with observation of intensity should be less conﬁdent
about pt, since now the observation of a motorcycle trip
duration slowed down by rain is supposed to come from a
distribution conditioned on rain intensity.

5.2. Estimating the Population of New York State

This case study is inspired by Rubin (1983), also appearing
as Section 7.6 in Gelman et al. (2013). The original case
study evaluated Bayesian inference on the problem of es-
timating the total population of 804 municipalities of New
York state based on a sample of 100 municipalities. Two
samples were given, with different summary statistics, and
power-transformed normal model was ﬁt to the data to make
predictions consistent among the samples. The authors of
the original study apparently had access to the full data set
(population of each of 804 municipalities). However, only
summary description of the samples appears in the publica-
tion: mean, standard deviation, and quantiles (Table 1). We

Table 1: Summary statistics for populations of municipali-
ties in New York State in 1960; all 804 municipalities and
two random samples of 100. From Rubin (1983).

total
mean
sd
lowest
5%
25%
median
75%
95%
highest

Population
13,776,663
17,135
139,147
19
336
800
1,668
5,050
30,295
2,627,319

Sample 1
1,966,745
19,667
142,218
164
308
891
2,081
6,049
25,130
1,424,815

Sample 2
3,850,502
38,505
228,625
162
315
863
1,740
5,239
41,718
1809578

show how such summary description can be used to perform
Bayesian inference, with the help of stochastic conditioning.

The original case study in Rubin (1983) started with compar-
ing normal and log-normal models, and ﬁnally ﬁt a truncated
three-parameter power-transformed normal distribution to
the data, which helped reconcile conclusions based on each
of the samples while producing results consistent with the
total population. Here, we use a model with log-normal sam-
pling distribution, the normal prior on the mean, based on
the summary statistics, and the improper uniform prior on
the log of the variance. To complete the model, we stochas-
tically condition on the piecewise-uniform distribution D of
municipality populations according to the quantiles:

y1...n ∼ Quantiles

(cid:16)

√

(cid:17)

n

mean, sd /

m ∼ Normal
σ = (cid:112)log (s2/m2 + 1), µ = log m − σ2/2
y1...n|m, s2 ∼ LogNormal(µ, σ)

, log s2 ∼ Uniform(−∞,∞)

(16)

As in Section 5.1, we ﬁt the model using stochastic gradient
HMC and used 10 000 samples to approximate the poste-
rior. We then used 10 000 draws with replacement of 804-
element sample sets from the predictive posterior to estimate
the total population. The posterior predictive distributions of
the total population from both samples are shown in Figure 2.
The 95% intervals inferred from the summary statistics,
[9.6×106, 17.2×106] for sample 1, [12.1×106, 28.1×106]
for sample 2, cover the true total 13.8 × 106, and are tighter
than the best intervals based on the full samples reported
by Rubin (1983), [6 × 106, 20 × 106] for sample 1 and
[10 × 106, 34 × 106] for sample 2.

5.3. The Sailing Problem

The sailing problem (Figure 3) is a popular benchmark prob-
lem for search and planning (P´eret & Garcia, 2004; Kocsis
& Szepesv´ari, 2006; Tolpin & Shimony, 2012). A sailing

0.00.20.40.60.81.00246p_tp_f0.00.20.40.60.81.0024p_tp_f0.00.20.40.60.81.001234p_tp_f0.00.20.40.60.81.001234p_tp_fProbabilistic Programs with Stochastic Conditioning

Figure 2: Estimating the population of NY state. 95%
intervals inferred from the summary statistics include the
true total, and are tighter than Rubin’s results.

boat must travel between the opposite corners A and B of a
square lake of a given size. At each step, the boat can head
in 8 directions (legs) to adjacent squares (Figure 3a). The
unit distance cost of movement depends on the wind (Fig-
ure 3b), which can also blow in 8 directions. There are ﬁve
relative boat and wind directions and associated costs: into,
up, cross, down, and away. The cost of sailing into the wind
is prohibitively high, upwind is the highest feasible, and
away from the wind is the lowest. The side of the boat off
which the sail is hanging is called the tack. When the angle
between the boat and the wind changes sign, the sail must
be tacked to the opposite tack, which incurs an additional
tacking delay cost. The objective is to ﬁnd a policy that
minimizes the expected travel cost. The wind is assumed to
follow a random walk, either staying the same or switching
to an adjacent direction, with a known probability.

For any given lake size, the optimal policy can be found
using value iteration (Bellman, 1957). The optimal policy
is non-parametric: it tabulates the leg for each combination
of location, tack, and wind. In this case study, we learn a
simple parametric policy, which chooses a leg that maxi-
mizes the sum of the leg cost and the remaining travel cost
after the leg, estimated as the Euclidean distance to the goal
multiplied by the average unit distance cost:

leg = arg min leg

(cid:2)cost(tack , leg, wind )+

unit-cost · distance(next-location, goal )(cid:3)

(17)

The average unit distance cost is the policy variable which
we infer. Model (18) formalizes our setting. Stochastic con-
ditioning on D = RandomWalk models non-determinism
in wind directions.

wind -history ∼ RandomWalk

p(wind -history, unit-cost) =

1
Z

exp

(cid:16) −travel -cost(wind -history, unit-cost)
lake-size · temperature

(cid:17)

(18)

Under policy (17), the boat trajectory and the travel cost are
determined by the wind history and the unit cost. The joint
probability of the wind history and the unit cost is given
by the Boltzmann distribution of trajectories with the travel

(a) lake

(b) points of sail

Figure 3: The sailing problem

cost as the energy, a common physics-inspired choice in
stochastic control and policy search (Kappen, 2007; Wingate
et al., 2011a; van de Meent et al., 2016). The temperature is
a model parameter: the lower the temperature is, the tighter
is the concentration of policies around the optimal policy.
A uniform prior on the unit cost, within a feasible range, is
implicitly assumed. If desirable, an informative prior can be
added as a factor depending on the unit cost.

The model parameters (cost and wind change probabilities),
same as in Kocsis & Szepesv´ari (2006); Tolpin & Shimony
(2012), are shown in Table 2. We ﬁt the model using pseudo-
marginal Metropolis-Hastings (Andrieu & Roberts, 2009)
and used 10 000 samples to approximate the posterior. The
inferred unit and expected travel costs are shown in Figure 4.
Figure 4a shows the posterior distribution of the unit cost,
for two temperatures. For all lake sizes in the experiment
(25, 50, 100), the optimal unit cost, corresponding to the
mode of the posterior, is ≈ 3.5–3.9. Distributions for lower
temperatures are tighter around the mode. Figure 4b shows
the expected travel costs, with the expectations estimated
both over the unit cost and the wind. The 95% posterior
intervals are shaded. The inferred travel costs are compared
to the travel costs of the optimal policy (the dashed line of
the same color) and of the greedy policy (the dotted line of
the same color), according to which the boat always heads
in the direction of the steepest decrease of the distance to the
goal. One can see that the inferred policies attain a lower
expected travel cost than the greedy policy and become
closer to the optimal policy as the temperature decreases.

6. Related Work

Works related to this research belong to several intercon-
nected areas: non-determinism in probabilistic programs,
nesting of probabilistic programs, inference in nested statis-
tical models, and conditioning on distributions.

Stochastic conditioning can be viewed as an expression of
non-determinism with regard to the observed variate. The
problem of representing and handling non-determinism in
probabilistic programs was raised in Gordon et al. (2014),
as an avenue for future work. Non-determinism arises, in
particular, in application of probabilistic programming to

0.51.01.52.02.53.03.54.04.5population1e70.00.51.01.52.02.51e7true totalmean for sample 1mean for sample 2sample 195% post. for sample 1Rubin's 95% post. for 1sample 295% post. for sample 2Rubin's 95% post. for 2ABupcrossdownintoawaywindport  tackstarboard tackno sailProbabilistic Programs with Stochastic Conditioning

Table 2: Sailing problem parameters

cost

into up cross down away delay same left
∞ 4
0.3

3

2

1

4

wind probability
right
0.3

0.4

policy search in stochastic domains. van de Meent et al.
(2016) introduce a policy-search speciﬁc model speciﬁca-
tion and inference scheme based on black-box variational
inference. We suggest, and show in a case study, that policy
search in stochastic domains can be cast as inference in
probabilistic programs with stochastic conditioning.

It was noted that probabilistic programs, or queries, can be
nested, and that nested probabilistic programs are able to
represent models beyond those representable by ﬂat proba-
bilistic programs. Stuhlm¨uller & Goodman (2014) describe
how probabilistic programs can represent nested condition-
ing as a part of the model, with examples in diverse areas
of game theory, artiﬁcial intelligence, and linguistics. Sea-
man et al. (2018) apply nested probabilistic programs to
reasoning about autonomous agents. Some probabilistic
programming languages such as Church (Goodman et al.,
2008), WebPPL (Goodman & Stuhlm¨uller, 2014), Anglican
(Tolpin et al., 2016), and Gen (Cusumano-Towner et al.,
2019) support nesting of probabilistic programs. Stochas-
tic conditioning can be, in principle, represented through
nesting, however nesting in general incurs difﬁculties in in-
ference (Rainforth et al., 2018; Rainforth, 2018). Stochastic
conditioning, introduced in this work, allows both simpler
speciﬁcation and more efﬁcient inference, eliminating the
need for nesting in many important cases.

Conditioning of statistical models on distributions or dis-
tributional properties is broadly used in machine learn-
ing (Chen & Gopinath, 2001; Kingma & Welling, 2019;
Goodfellow et al., 2014; Makhzani et al., 2015; Bingham
et al., 2019). Conditioning on distributions represented by
samples is related to subsampling in deep probabilistic pro-
gramming (Tran et al., 2017; TensorFlow, 2018; Bingham
et al., 2019). Subsampling used with stochastic variational
inference (Ranganath et al., 2014) can be interpreted as a
special case of stochastic conditioning. Tavares et al. (2019)
approach the problem of conditioning on distributions by ex-
tending probabilistic programming language OMEGA with
constructs for conditioning on distributional properties such
as expectation or variance. This work takes a different ap-
proach by generalizing deterministic conditioning on values
to stochastic conditioning on distributions, without the need
to explicitly compute or estimate particular distributional
properties, and leverages inference algorithms developed in
the context of subsampling (Korattikara et al., 2014; Bar-
denet et al., 2014; 2017; Maclaurin & Adams, 2014; Quiroz
et al., 2018; 2019; Dang et al., 2019) for efﬁcient inference
in probabilistic programs with stochastic conditioning.

(a) unit cost

(b) expected travel cost

Figure 4: The sailing problem. The optimum unit cost is
≈ 3.5–3.9. The dashed lines are the expected travel costs
of the optimal policies, dotted — of the greedy policy.

There is a connection between stochastic conditioning and
Jeffrey’s soft evidence (Jeffrey, 1990). In soft evidence, the
observation is uncertain; any one out of a set of observations
could have been observed with a certain known probabil-
ity. A related concept in the context of belief networks is
virtual evidence (Pearl, 1988). Chan & Darwiche (2003)
demonstrate that Jeffrey’s soft evidence and Pearl’s virtual
evidence are different formulations of the same concept.
Ben Mrad et al. (2013); Dietrich et al. (2016); Jacobs (2018)
elaborate on connection between soft and virtual evidence
and their role in probabilistic inference. In probabilistic
programming, some cases of soft conditioning (Wood et al.,
2014; Goodman & Stuhlm¨uller, 2014) can be interpreted
as soft evidence. In this work, the setting is different: a
distribution is observed, and the observation is certain.

7. Discussion

In this work, we introduced the notion of stochastic condi-
tioning. We described kinds of problems for which deter-
ministic conditioning is insufﬁcient, and showed on case
studies how probabilistic programs with stochastic condi-
tioning can be used to represent and efﬁciently analyze such
problems. We believe that adoption of stochastic condition-
ing in probabilistic programming frameworks will facilitate
convenient modeling of new classes of problems, while
still supporting robust and efﬁcient inference. The idea of
stochastic conditioning is very general, and we believe our
work opens up a wide array of new research directions be-
cause of this. Support for stochastic conditioning in other
existing probabilistic programming languages and libraries
is a direction for future work. While we provide a reference
implementation, used in the case studies, we believe that

234567unit cost0.00.51.01.5densitymode rangesize:  25, log t: -1size:  25, log t: -5size:  50, log t: -1size:  50, log t: -5size: 100, log t: -1size: 100, log t: -5012345-log t4.14.24.3travel cost/sizesize:  25size:  50size: 100Probabilistic Programs with Stochastic Conditioning

stochastic conditioning should eventually become a part of
most probabilistic programming frameworks, just like other
common core concepts.

Probabilistic Programs with Stochastic Conditioning

References

Andrieu, C. and Roberts, G. O. The pseudo-marginal ap-
proach for efﬁcient Monte Carlo computations. The An-
nals of Statistics, 37(2):697–725, 2009.

Bardenet, R., Doucet, A., and Holmes, C. Towards scaling
up Markov chain Monte Carlo: an adaptive subsampling
approach. In Proceedings of the 31st International Con-
ference on Machine Learning, pp. 405–413, 2014.

Bardenet, R., Doucet, A., and Holmes, C. On Markov chain
Monte Carlo methods for tall data. Journal of Machine
Learning Research, 18(47):1–43, 2017.

Bellman, R. A Markovian decision process. Journal of

Mathematics and Mechanics, 6(5):679–684, 1957.

Ben Mrad, A., Delcroix, V., Piechowiak, S., Maalej, M. A.,
and Abid, M. Understanding soft evidence as probabilis-
tic evidence: Illustration with several use cases. In 2013
5th International Conference on Modeling, Simulation
and Applied Optimization (ICMSAO), pp. 1–6, 2013.

Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F.,
Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Hors-
fall, P., and Goodman, N. D. Pyro: deep universal prob-
abilistic programming. Journal of Machine Learning
Research, 20(28):1–6, 2019.

Bullen, P. S. Handbook of Means and Their Inequalities.

Springer Netherlands, 2003.

Carpenter, B., Gelman, A., Hoffman, M., Lee, D., Goodrich,
B., Betancourt, M., Brubaker, M., Guo, J., Li, P., and
Riddell, A. Stan: a probabilistic programming lan-
guage. Journal of Statistical Software, Articles, 76(1):
1–32, 2017.

Ceperley, D. M. and Dewing, M. The penalty method for
random walks with uncertain energies. The Journal of
Chemical Physics, 110(20):9812–9820, May 1999.

Chan, H. and Darwiche, A. On the revision of probabilistic
beliefs using uncertain evidence. Artiﬁcial Intelligence,
163:67–90, 2003.

Chen, S. S. and Gopinath, R. A. Gaussianization. In Ad-
vances in Neural Information Processing Systems, pp.
423–429. 2001.

Proceedings of the 40th ACM SIGPLAN Conference on
Programming Language Design and Implementation, pp.
221–236, 2019.

Dang, K.-D., Quiroz, M., Kohn, R., Tran, M.-N., and Villani,
M. Hamiltonian Monte Carlo with energy conserving
subsampling. Journal of Machine Learning Research, 20
(100):1–31, 2019.

Dietrich, F., List, C., and Bradley, R. Belief revision gen-
eralized: A joint characterization of Bayes’ and Jeffrey’s
rules. Journal of Economic Theory, 162:352–371, 2016.

Doucet, A., Pitt, M. K., Deligiannidis, G., and Kohn, R.
Efﬁcient implementation of Markov chain Monte Carlo
when using an unbiased likelihood estimator. Biometrika,
102(2):295–313, 03 2015.

Ge, H., Xu, K., and Ghahramani, Z. Turing: composable
In Proceed-
inference for probabilistic programming.
ings of the 21st Conference on International Conference
on Artiﬁcial Intelligence and Statistics, pp. 1682–1690,
2018.

Gelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A.,
and Rubin, D. Bayesian Data Analysis. Chapman &
Hall/CRC Texts in Statistical Science. CRC Press, 2013.

Ghosal, S. and van der Vaart, A. Fundamentals of Nonpara-
metric Bayesian Inference. Cambridge Series in Statisti-
cal and Probabilistic Mathematics. Cambridge University
Press, 2017. doi: 10.1017/9781139029834.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672–2680. 2014.

Goodman, N. D. and Stuhlm¨uller, A. The Design and Im-
plementation of Probabilistic Programming Languages.
2014. URL http://dippl.org/. electronic; re-
trieved 2019/3/29.

Goodman, N. D., Mansinghka, V. K., Roy, D. M., Bonawitz,
K., and Tenenbaum, J. B. Church: a language for genera-
tive models. In Proceedings of the 24th Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 220–229, 2008.

Chen, T., Fox, E. B., and Guestrin, C. Stochastic gradient
Hamiltonian Monte Carlo. In Proceedings of the 31st
International Conference on on Machine Learning, pp.
1683–1691, 2014.

Gordon, A. D., Henzinger, T. A., Nori, A. V., and Rajamani,
S. K. Probabilistic programming. In Proceedings of the
36th International Conference on Software Engineering
(ICSE, FOSE track), pp. 167–181, 2014.

Cusumano-Towner, M. F., Saad, F. A., Lew, A. K., and
Mansinghka, V. K. Gen: A general-purpose probabilistic
programming system with programmable inference. In

Hoffman, M. D. and Gelman, A. The No-U-Turn sampler:
adaptively setting path lengths in Hamiltonian Monte
Carlo. arXiv:1111.4246, 2011.

Probabilistic Programs with Stochastic Conditioning

Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
Stochastic variational inference. Journal of Machine
Learning Research, 14:1303–1347, 2013.

Pearl, J. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 1988.

Jacobs, B. The mathematics of changing one’s mind, via
Jeffrey’s or via Pearl’s update rule. arXiv:1807.05609,
2018.

Jeffrey, R. The Logic of Decision. McGraw-Hill series in
probability and statistics. University of Chicago Press,
1990.

Kappen, H. J. An introduction to stochastic control theory,
path integrals and reinforcement learning. In American
Institute of Physics Conference Series, volume 887, pp.
149–181, 2007.

Kim, J. and Pearl, J. A computational model for causal
and diagnostic reasoning in inference systems. In Pro-
ceedings of International Joint Conference on Artiﬁcial
Intelligence, pp. 190–193, 1983.

Kingma, D. P. and Welling, M. An introduction to varia-
tional autoencoders. Foundations and Trends in Machine
Learning, 12(4):307–392, 2019.

Kocsis, L. and Szepesv´ari, C. Bandit based Monte-Carlo
planning. In Proceedings of the European Conference on
Machine Learning, pp. 282–293, 2006.

Korattikara, A., Chen, Y., and Welling, M. Austerity in
MCMC land: Cutting the Metropolis-Hastings budget.
In Proceedings of the 31st International Conference on
Machine Learning, pp. 181–189, 2014.

Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and
Blei, D. M. Automatic differentiation variational infer-
ence. Journal of Machine Learning Research, 18(1):
430–474, January 2017.

P´eret, L. and Garcia, F. On-line search for solving Markov
decision processes via heuristic sampling. In Proceed-
ings of the 16th European Conference on Artiﬁcial Intel-
ligence, pp. 530–534, 2004.

Quiroz, M., Villani, M., Kohn, R., Tran, M.-N., and Dang,
K.-D. Subsampling mcmc — an introduction for the
survey statistician. Sankhya A, 80(1):33–69, Dec 2018.

Quiroz, M., Kohn, R., Villani, M., and Tran, M.-N. Speeding
up MCMC by efﬁcient data subsampling. Journal of
the American Statistical Association, 114(526):831–843,
2019.

Rainforth, T. Nesting probabilistic programs. In Proceed-
ings of the 34th Conference on Uncertainty in Artiﬁcial
Intelligence, pp. 249–258, 2018.

Rainforth, T., Cornish, R., Yang, H., Warrington, A., and
Wood, F. On nesting Monte Carlo estimators. In Proceed-
ings of the 35th International Conference on Machine
Learning, pp. 4267–4276, 2018.

Ranganath, R., Gerrish, S., and Blei, D. M. Black box
variational inference. In Proceedings of the 17th Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 814–822,
2014.

Rubin, D. B. A case study of the robustness of Bayesian
methods of inference: Estimating the total in a ﬁnite
population using transformations to normality. In Box,
G., Leonard, T., and Wu, C.-F. (eds.), Scientiﬁc Inference,
Data Analysis, and Robustness, pp. 213–244. Academic
Press, 1983.

Ma, Y.-A., Chen, T., and Fox, E. A complete recipe for
stochastic gradient MCMC. In Advances in Neural Infor-
mation Processing Systems, pp. 2917–2925. 2015.

Seaman, I. R., van de Meent, J.-W., and Wingate, D. Nested
reasoning about autonomous agents using probabilistic
programs. arXiv:1812.01569, 2018.

Maclaurin, D. and Adams, R. Fireﬂy Monte Carlo: Exact
MCMC with subsets of data. In Proceedings of the 30th
Conference on Uncertainty in Artiﬁcial Intelligence, pp.
543–552, 2014.

Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey,
B. Adversarial autoencoders. arXiv:1511.05644, 2015.

Nicholls, G. K., Fox, C., and Watt, A. M.

Cou-
pled MCMC with a randomized acceptance probability.
arXiv:1205.6857, 2012.

Staton, S., Yang, H., Wood, F., Heunen, C., and Kammar, O.
Semantics for probabilistic programming: Higher-order
functions, continuous distributions, and soft constraints.
In Proceedings of the 31st Annual ACM/IEEE Symposium
on Logic in Computer Science, pp. 525–534, 2016.

Stuhlm¨uller, A. and Goodman, N. Reasoning about rea-
soning by nested conditioning: Modeling theory of mind
with probabilistic programs. Cognitive Systems Research,
28:80–99, 2014. Special Issue on Mindreading.

Papadimitriou, C. H. and Yannakakis, M. Shortest paths
In Proceedings of 16th International
without a map.
Colloquium on Automata, Languages and Programming,
pp. 610–620, 1989.

Tavares, Z., Zhang, X., Minaysan, E., Burroni, J., Ran-
ganath, R., and Lezama, A. S. The random condi-
tional distribution for higher-order probabilistic inference.
arXiv:1903.10556, 2019.

Probabilistic Programs with Stochastic Conditioning

TensorFlow. TensorFlow probability. https://www.

tensorflow.org/probability/, 2018.

Tolpin, D. Deployable probabilistic programming. In Pro-
ceedings of the 2019 ACM SIGPLAN International Sym-
posium on New Ideas, New Paradigms, and Reﬂections
on Programming and Software, pp. 1–16, 2019.

Tolpin, D. and Shimony, S. E. MCTS based on simple regret.
In Proceedings of The 26th AAAI Conference on Artiﬁcial
Intelligence, pp. 570–576, 2012.

Tolpin, D., van de Meent, J.-W., Yang, H., and Wood, F. De-
sign and implementation of probabilistic programming
language Anglican. In Proceedings of the 28th Sympo-
sium on the Implementation and Application of Func-
tional Programming Languages, pp. 6:1–6:12, 2016.

Tran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Mur-
phy, K., and Blei, D. M. Deep probabilistic programming.
In Proceedings of the 5th International Conference on
Learning Representations, 2017.

van de Meent, J.-W., Paige, B., Tolpin, D., and Wood, F.
Black-box policy search with probabilistic programs. In
Proceedings of the 19th International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 1195–1204, 2016.

Wingate, D., Goodman, N. D., Roy, D. M., Kaelbling, L. P.,
and Tenenbaum, J. B. Bayesian policy search with policy
priors. In Proceedings of the 22nd International Joint
Conference on Artiﬁcial Intelligence, pp. 1565–1570,
2011a.

Wingate, D., Stuhlm¨uller, A., and Goodman, N. D.
Lightweight implementations of probabilistic program-
ming languages via transformational compilation. In Pro-
ceedings of the 14th Conference on Artiﬁcial Intelligence
and Statistics, pp. 770–778, 2011b.

Winn, J., Bishop, C. M., Diethe, T., and Zaykov, Y.
Model Based Machine Learning. 2019. URL http:
//mbmlbook.com/. electronic; retrieved 2019/4/21.

Wood, F., van de Meent, J.-W., and Mansinghka, V. A
new approach to probabilistic programming inference. In
Proceedings of the 17th International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 1024–1032, 2014.

Probabilistic Programs with Stochastic Conditioning

A. Measure-Theoretic Formalization of

Stochastic Conditioning

unnormalized distribution (i.e., measure) over Θ: for all
measurable subsets B of Θ,

Although stochastic conditioning is deﬁned in terms of the
density q of the distribution D, its key idea does not depend
on q. In fact, we have already explained informally how
stochastic conditioning and our results can be developed
even when the density q does not exist, as in the case of
Dirac distributions. Also, Proposition 2 assumes this general
development. In this section, we spell out this informal ex-
planation, and describe the measure-theoretic formalization
of stochastic conditioning.

We start by changing Deﬁnitions 1 and 2 such that D is
not required to have a density with respect to the Lebesgue
measure, and the conditional density p(y∼D|x) is deﬁned
for such D.

Deﬁnition 3. A probabilistic model with stochastic condi-
tioning is a tuple (p(x, y), D) where

• p(x, y) is the joint probability density of random vari-
able x and observation y, and it is factored into the
product of the prior p(x) and the conditional probabil-
ity p(y|x) (i.e., p(x, y) = p(x)p(y|x));

• D is the distribution (i.e., probability measure) from

which observation y is sampled.

Deﬁnition 4. The conditional density p(y∼D|x) of D given
x is

p(y∼D|x) = exp

(log p(y|x)) D(dy)

(19)

(cid:19)

(cid:18)(cid:90)

Y

where D(dy) indicates that the integral over Y is taken with
respect to the distribution D.

To explain where the term “density” in Deﬁnition 4 comes
from, we recall the standard setup of the work on random
distributions, which studies distributions over distributions.2
The setup over random distributions on Y ⊆ Rm is the
measurable space (D, Σ) where D is the set of distributions
over Y and Σ is the smallest σ-ﬁeld generated by the family

(cid:110)

{D | D(A) < r}

(cid:12)
(cid:12) measurable A ⊆ Y and r ∈ R
(cid:12)

(cid:111)
.

The next theorem generalizes Theorem 1. In a setting that
covers both continuous and discrete cases, with or without
densities, the theorem describes when p(y∼D|x) has a ﬁnite
normalization constant.

Theorem 2. Assume that we are given a distribution Dθ
parameterized by θ ∈ Θ ⊆ Rp such that D is a probability
kernel from Θ to Y , and the following µx is a well-deﬁned

µx(B) =

=

(cid:90)

B

(cid:90)

B

p(y∼Dθ|x) dθ

(cid:18)(cid:90)

exp

Y

(log p(y|x)) Dθ(dy)

dθ.

(cid:19)

Let νx be the push-forward of µx along the function θ (cid:55)−→
Dθ from Θ to D. The unnormalized distribution νx has a
ﬁnite normalization constant C (i.e., νx(D) = C < ∞) if
there exists C (cid:48) < ∞ such that for all measurable subsets A
of Y ,

(cid:90)

Dθ(A) dθ ≤

(cid:18)

(cid:90)

C (cid:48) ·

(cid:19)

dy

.

(20)

Θ

A

Before proving the theorem, we make two comments. First,
when Dθ is deﬁned in terms of a density qθ, the condition
(20) in the theorem is implied by the condition in Theorem 1:

(cid:90)

Θ

sup
y∈Y

qθ(y)dθ ≤ C (cid:48).

The implication is shown below:

(cid:90)

Θ

(cid:90)

(cid:90)

Dθ(A) dθ =

qθ(y)dydθ =

(cid:90)

(cid:90)

qθ(y)dθdy

(cid:90)

≤

A

Θ

sup
y(cid:48)∈Y

A
(cid:18)(cid:90)

Θ

A

Θ
(cid:18)

(cid:19)

dy ≤

(cid:90)

C (cid:48) ·

(cid:19)

dy

.

qθ(y(cid:48))dθ

A

Second, when λx is the push-forward of the Lebesgue mea-
sure along θ (cid:55)−→ Dθ, our p(y∼D|x) is the density of νx
with respect to λx. This is why we called p(y∼D|x) condi-
tional density.

Proof. The theorem claims that C = νx(D) is ﬁnite. But
C = µx(Θ) by the deﬁnition of the push-forward measure,
and so it sufﬁces to show the ﬁniteness of µx(Θ). Note

µx(Θ) =

(cid:90)

(cid:18)(cid:90)

exp

Θ

Y

(log p(y|x))Dθ(dy)

dθ.

(21)

(cid:19)

We compute a ﬁnite bound of C = µx(Θ) as follows:

(cid:90)

(cid:90)

(cid:16)

C ≤1

exp(log p(y|x))

(cid:17)

Dθ(dy)dθ

Θ
(cid:90)

(cid:90)

=

Y

p(y|x) Dθ(dy)dθ

(22)

Θ
(cid:18)

Y

C (cid:48) ·

(cid:90)

=2

Y

(cid:19)

p(y|x) dy

= C (cid:48) < ∞

2A brief yet good exposition on this topic can be found in

Appendix A of Ghosal & van der Vaart (2017).

where ≤1 is by Jensen’s inequality and =2 uses the assump-
tion of the theorem.

Probabilistic Programs with Stochastic Conditioning

Besides {Normal(θ, 1) | θ ∈ R} that we discussed already
after Theorem 1, the set {Dirac(θ) | θ ∈ R} satisﬁes the
condition (20) in Theorem 2. Thus, p(y∼D|x) can be nor-
malized to a distribution (i.e., a probability measure) in both
cases. However, p(y∼D|x) cannot be normalized over the
space {β · Dirac(0) + (1 − β) · Dirac(θ) | θ ∈ R} for
β ∈ (0, 1), which consists of the mixtures of two Dirac
distributions. The condition (20) in Theorem 2 does not
hold. In fact, if p(0|x) > 0, the normalization constant of
νx in the theorem is inﬁnite.

B. Inference algorithms

A simple bias-adjusted likelihood estimate ˆp(x, y∼D),
required for the computation of the weights in importance
sampling as well as of the acceptance ratio in pseudo-
marginal Markov chain Monte Carlo (Andrieu & Roberts,
2009), can be computed based on (9) as follows (Ceper-
ley & Dewing, 1999; Nicholls et al., 2012; Quiroz et al.,
2018). Under the conditions of the central limit theorem,
the distribution of

1
N

N
(cid:88)

j=1

log p(x, yj)

becomes similar to the normal distribution

Normal

(cid:16)

µ = E
y∼D

[log p(x, y)], σ2 =

1
N

Var
y∼D

[log p(x, y)]

(cid:17)

as N → ∞. Correspondingly, the distribution of



exp



1
N

N
(cid:88)

j=1



log p(x, yj)



and the log-normal distribution with the same parameters
become similar under the same asymptotics. But the mean
of the log-normal distribution is exp(µ + σ2
2 ). Thus, we can
construct a bias-adjusted estimate as

In importance sampling, xi’s are drawn from a proposal
distribution U with probability mass or density u(x) and
weighted by the joint probability mass or density of x and
observations. In the case of stochastic conditioning, the
weight wi of xi is approximated as ˆwi using an unbiased
estimate ˆp(xi, D) such as (23).

ˆwi =

ˆp(xi, D)
u(xi)

=

1
u(xi)

(cid:18)

exp

mi −

(cid:19)

.

s2
i
2N

(24)

Markov chain Monte Carlo algorithms are broadly
applied to inference in probabilistic programs, with
Lightweight Metropolis-Hastings (Wingate et al., 2011b)
as the simplest and universally applicable variant. Many
MCMC variants involve proposing a new state x(cid:48) from a
proposal distribution U with probability mass or density
u(x(cid:48)|x) and then either accepting x(cid:48) or retaining x, with
Metropolis-Hastings acceptance ratio α based on the joint
probability of x(cid:48) and observations:

(cid:26)

α = min

1,

u(x|x(cid:48))
u(x(cid:48)|x)

×

p(x(cid:48), y∼D)
p(x, y∼D)

(cid:27)

.

(25)

Just like with importance sampling, p(x, y∼D) cannot be
computed exactly for probabilistic programs with stochastic
conditioning. However, Andrieu & Roberts (2009) estab-
lish that the joint probability can be replaced with an unbi-
ased estimate without affecting the stationary distribution
of the Markov chain, resulting in pseudo-marginal MCMC.
Pseudo-marginal MCMC allows speeding up Monte Carlo
inference by subsampling (Bardenet et al., 2017; Quiroz
et al., 2019; Dang et al., 2019; Quiroz et al., 2018) and can
be applied to stochastic conditioning as well. The main
challenge in designing an efﬁcient MCMC algorithm, for
both subsampling and stochastic conditiong, is constructing
an unbiased low-variance estimate of the joint probability.
In a basic case, (23) can be used as a bias-adjusted estimate,
resulting in the acceptance ratio ˆα:

m =

1
N

N
(cid:88)

j=1

log p(x, yj),

s2 =

1
N −1

N
(cid:88)

(log p(x, yj)−m)2,

j=1

ˆp(x, y∼D) = exp(µ)

(cid:20)
≈ Ey1:N ∼Dn

exp

(cid:18) 1
N

(23)
(cid:19)(cid:21)

log p(x, yj)

(cid:88)N

j=1

(cid:18)

× exp

−

(cid:18)

≈ exp

m −

(cid:19)

σ2
2
s2
2N

(cid:19)

.

(cid:26)

ˆα = min

1,

(cid:26)

= min

1,

u(x|x(cid:48))
u(x(cid:48)|x)
u(x|x(cid:48))
u(x(cid:48)|x)

(cid:27)

×

ˆp(x(cid:48), D)
ˆp(x, y∼D)
(cid:18)

× exp

m(cid:48) − m −

s(cid:48)2 − s2
2N

(cid:19)(cid:27)

.

(26)
Note that the same samples y1, y2, ..., yN should be used
for estimating both m, s2 and m(cid:48), s(cid:48)2 (Andrieu & Roberts,
2009).

Stochastic gradient Markov chain Monte Carlo (sgM-
CMC) (Ma et al., 2015) can be used unmodiﬁed when the
log probability is differentiable with respect to x. sgM-
CMC uses an unbiased stochastic estimate of the gradient of
log probability density. Such estimate is trivially obtained
by drawing a single sample y1 from D and computing the

Probabilistic Programs with Stochastic Conditioning

gradient of the log joint density of x and y:

∇x log p(x, y∼D) = ∇x

= ∇x

= ∇x

(cid:90)

=

(cid:16)

(cid:16)

(cid:90)

(cid:16)

log

p(x)

(cid:89)

p(y|x)q(y)dy(cid:17)(cid:17)

y∈Y

(cid:16) (cid:89)

p(x, y)q(y)dy(cid:17)(cid:17)

log

y∈Y

q(y) log p(x, y)dy

y∈Y

(cid:16)

q(y)

∇x log p(x, y)

(cid:17)

dy

y∈Y

≈ ∇x log p(x, y1).

(27)

Stochastic variational inference (Hoffman et al., 2013;
Ranganath et al., 2014; Kucukelbir et al., 2017) requires a
noisy estimate of the gradient of the evidence lower bound
(ELBO) L. The most basic approach is to use the score
estimator that is derived from the following equation:

∇λL = Ex∼q(x|λ)

(cid:20)
(∇λ log q(x|λ))

(cid:18)

log

p(x, y∼D)
q(x|λ)

(cid:19)(cid:21)

.

(28)
As in the standard posterior inference setting, maximizing
ELBO is equivalent to minimizing the KL divergence from
q(x|λ) to p(x|D). Substituting (3) into (28), we obtain

∇λL = Ex∼q(x|λ)

(cid:20)
∇λ log q(x|λ)

(cid:16)

log p(x) +

q(y) log p(y|x)dy − log q(x|λ)

(cid:17)(cid:21)

(cid:90)

y∈Y

= Ex∼q(x|λ)

(cid:20) (cid:90)

y∈Y

∇λ log q(x|λ)

(cid:16)

log p(x) +

log p(y|x) − log q(x|λ)

(cid:17)

(cid:21)

q(y)dy

= E(x,y)∼q(x|λ)×D

(cid:20)
∇λ log q(x|λ)

(cid:16)

log p(x) +

log p(y|x) − log q(x|λ)

(cid:17)(cid:21)
.

(29)
Thus, ∇λL can be estimated using Monte Carlo samples
xs, ys ∼ q(x|λ) × D:

∇λL ≈

1
S

S
(cid:88)

s=1

∇λ log q(xs|λ)

(cid:16)

log p(xs) +

log p(ys|xs) − log q(xs|λ)

(cid:17)

(30)

,

and stochastic variational inference can be directly applied.
In fact, van de Meent et al. (2016) use black-box variational
inference (Ranganath et al., 2014) for a special case of
stochastic conditioning arising in policy search.

