1
2
0
2

p
e
S
8
2

]
L
M

.
t
a
t
s
[

1
v
1
9
8
3
1
.
9
0
1
2
:
v
i
X
r
a

Gaussian Processes to speed up MCMC with automatic
exploratory-exploitation effect

ALESSIO BENAVOLI, JASON WYSE, and ARTHUR WHITE, School of Computer Science and
Statistics, Trinity College Dublin, Ireland

1 INTRODUCTION
In this paper, we consider the problem of sampling from a posterior

𝜋 (𝜽 |𝐷) ∝ 𝑝 (𝐷 |𝜽 )𝑝 (𝜽 ),
where 𝐷 denotes data and 𝜽 ∈ Θ is a vector of unknown parameters, in the case where the likelihood
𝑝 (𝐷 |𝜽 ) is costly to evaluate. We discuss two-stage algorithms. In the first of these, we examine
an adaptive Metropolis-Hastings (MH) algorithm [Hastings 1970; Robert 2015] which employs
an adaptively tuned Gaussian Process (GP) surrogate model at the first stage to filter out poor
proposals. If a proposal is not filtered out, at the second stage a full (expensive) log-likelihood
evaluation is carried out and used to decide whether it is accepted as the next state. Introduction of
the first stage, constructed in this way, saves computation on poor proposals. A key contribution of
this work is in the form of the acceptance probability in the first stage obtained by marginalising
out the GP function. This makes the acceptance ratio dependent on the variance of the GP, which
naturally results in an exploration-exploitation trade-off similar to the one of Bayesian Optimisation
[Brochu et al. 2010], which allows us to sample while learning the GP. We demonstrate that using
this expectation serves as a useful filtering scheme. The second algorithm is a two-stage form of
Metropolis adjusted Langevin algorithm (MALA) [Neal 2011]. Here, we use GP as a surrogate for
the log-likelihood function again, but in this case the GP is also used to approximate the gradient
required for MALA updating, using a well known result that the gradient of a GP is also a GP [Solak
et al. 2002]. Marginalizing out of the GP can also be performed in this instance.

The approximation we use is

LL(𝐷 |𝜽 ) := ln 𝑝 (𝐷 |𝜽 ) ≈ (cid:102)LL𝑡 (𝐷 |𝜽 ) ∼ GP(𝜇 (𝜽 |I𝑡 ), 𝑘 (𝜽, 𝜽 ∗|I𝑡 ))
where I𝑡 denotes the set of 𝑡 full evaluations of the log-likelihood by the current iteration, and 𝜽 ∗
collectively denotes the parameter values at which these evaluations were made. Adaptive tuning
of the GP surrogate is accomplished through use of the collection I𝑡 of full evaluations of the log-
likelihood. We argue that the tuning schedule we suggest satisfies diminishing adaptation [Roberts
and Rosenthal 2007] and hence will ensure correct sampling from the true target 𝜋 (𝜽 |𝐷).

(1)

Within the Markov chain Monte Carlo (MCMC) literature, there has been much interest in
recent years, in the use of proxy quantities for the target measure evaluations from different
aspects. Approaches using noisy approximations to an invariant transition kernel [Alquier et al.
2016; Andrieu and Roberts 2009] have gained much interest. The work here assumes that the
log-likelihood, though maybe expensive, can be computed, and is thus more aligned to the work
of Bliznyuk et al. [2012]; Christen and Fox [2005]; Fielding et al. [2011]; Joseph [2012]; Li et al.
[2019]; Rasmussen [2003]; Sherlock et al. [2017], involving ideas from delayed acceptance MCMC.
The key difference is that we do not carry out pre-computation of the GP prior to running the
algorithm, investigating adaptation of the GP on the fly using key results from the adaptive MCMC
literature [Roberts and Rosenthal 2007] to ensure convergence to the true target.

Authors’ address: Alessio Benavoli; Jason Wyse; Arthur White, School of Computer Science and Statistics, Trinity College
Dublin, Ireland, alessio.benavoli@tcd.ie, wyseja@tcd.ie, arwhite@tcd.ie.

 
 
 
 
 
 
2

Alessio Benavoli, Jason Wyse, and Arthur White

We present the two stage MH algorithm in Section 2. Section 3 follows by introducing a the
two-stage MALA algorithm building on these ideas. Section 4 explores a range of examples and
demonstrates the merits of the filtering step, with a discussion of potential drawbacks.

2 TWO STAGE ADAPTIVE METROPOLIS-HASTINGS VIA GP APPROXIMATION
We combine the MH algorithm with a GP model which approximates the log-likelihood. In cases
where the log-likelihood is expensive to compute, the GP model can be used in a pre-filtering
step to determine proposals for which a full computation of the log-likelihood might well lead
to an acceptance [Christen and Fox 2005]. At each iteration of the algorithm, the first stage, uses
a GP to deliver an approximate log-likelihood evaluation. The GP is based on a collection I𝑡 of
previous full evaluations of the log-likelihood. A propsal is made from the current state and then
the usual MH acceptance probability is computed using the approximated log-likelihood (this
step is computationally inexpensive). If the proposal is accepted in this first stage, then it goes to
the second stage, where another acceptance probability is computed, but this time, based on the
full costly evaluation of the log-likelihood. The resulting evaluation of the log-likelihood is then
appended to I𝑡 , resulting in I𝑡 +1.

Before giving a full description of the algorithm, we introduce some notation and give an explicit

definition of I𝑡 :

• S𝑘 denotes the points sampled up to the iteration 𝑘 of the algorithm;
• 𝜽 (𝑘) denotes the most recent element in S𝑘 and 𝜽 ∗ denotes the proposed state
• I𝑡 = {(𝜽 (𝑖), LL(𝐷 |𝜽 (𝑖) )) :
performed up to iteration 𝑘.

𝑖 = 1, . . . , 𝑡 } denotes the 𝑡 ≤ 𝑘 exact likelihood evaluations

We use a noise free GP as a surrogate model for the log-likelihood and denote by GP𝑘 (𝜇 (𝜽 |I𝑡 ), 𝑘 (𝜽, 𝜽 |I𝑡 ))
the posterior GP at the iteration 𝑘 conditioned on the collection I𝑡 . We use (cid:102)LL𝑘 (𝜽 ) to denote the
GP-distributed log-likelihood. We choose the parameters of the GP to satisfy the following exact
interpolation property.

Assumption 1. The prior mean function and prior covariance function of the GP are selected to

guarantee exact interpolation:

𝜇 (𝜽 (𝑖) |I𝑡 ) = LL(𝐷 |𝜽 (𝑖) ),

𝑘 (𝜽 (𝑖), 𝜽 |I𝑡 ) = 0,

for all 𝜽 (𝑖) with a corresponding entry in I𝑡 and 𝜽 ∈ Θ.1

This means the predictions of the GP at the points 𝜽 (𝑖) ∈ I𝑡 are exact and certain (zero

(co)variance), which is a desirable property in a noise free regression problem.2.

The two stages of the MH algorithm are as follows.

Stage 1. Use the predictive posterior GP (conditioned on the collection I𝑡 ) to approximate the

log-likelihood. Define the first stage acceptance probability:

˜𝛼 (1) (𝜽 (𝑘), 𝜽 ∗) = 1 ∧

exp((cid:102)LL𝑡 (𝜽 ∗))𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗)
exp((cid:102)LL𝑡 (𝜽 (𝑘) ))𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) )

(2)

where (cid:102)LL𝑡 (·) ∼ GP𝑘 (𝜇 (𝜽 |I𝑡 ), 𝑘 (𝜽, 𝜽 |I𝑡 )) and we use the shorthand notation 𝑎 ∧ 𝑏 = min(𝑎, 𝑏).
Note that, because of the exact interpolation property in Assumption 1, it results that (cid:102)LL𝑡 (𝜽 (𝑘) ) =
LL𝑡 (𝜽 (𝑘) ).

1Any universal covariance function satisfies this property, for instance Squared Exponential.
2This also guarantees consistency between the two stages: the denominator of (2) and (6) is the same

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

3

The acceptance probability ˜𝛼 (1) (𝜽 (𝑘), 𝜽 ∗) (respectively, ˜𝛼 (1) (𝜽 ∗, 𝜽 (𝑘) )) depends on (cid:102)LL𝑡 (𝜽 ∗) −
(cid:102)LL𝑡 (𝜽 (𝑘) ) (respectively, −(cid:102)LL𝑡 (𝜽 ∗) + (cid:102)LL𝑡 (𝜽 (𝑘) ) ) which is GP distributed. A key part of our approach
involves marginalizing this dependence out by exploiting the following result.

Proposition 2.1. The distribution of 𝑒 (cid:102)LL𝑡 (𝜽 ) is Lognormal (𝜇 (𝜽 |I𝑡 ), 𝑘 (𝜽, 𝜽 |I𝑡 )) , and its mean is
(3)

𝑒 𝜇 (𝜽 |I𝑡 )+ 1

2 𝑘 (𝜽,𝜽 |I𝑡 ) .

The proofs of this and the other Propositions are given in Appendix B. By Assumption 1, we have
that 𝑘 (𝜽 (𝑘), 𝜽 (𝑘) |I𝑡 ) = 𝑘 (𝜽 ∗, 𝜽 (𝑘) |I𝑡 ) = 0 and, therefore, (cid:102)LL𝑡 (𝜽 ∗) and (cid:102)LL𝑡 (𝜽 (𝑘) ) are sampled inde-
pendently. By exploiting Proposition 2.1, we remove the dependence of the acceptance probability
on (cid:102)LL in (2) resulting in the acceptance probability:
𝑒 𝜇 (𝜽 ∗ |I𝑡 )+ 1

2 𝑘 (𝜽 ∗,𝜽 ∗ |I𝑡 )𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗)

,

(4)

𝛼 (1) (𝜽 (𝑘), 𝜽 ∗) = 1 ∧

𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) )

where 𝜇 (𝜽 (𝑘) |I𝑡 ) = LL(𝜽 (𝑘) ) is the exact log-likelihood (by Assumption 1).

It can be seen that 𝜇 (𝜽 ∗|I𝑡 ) + 1

2𝑘 (𝜽 ∗, 𝜽 ∗|I𝑡 ) depends on the GP variance and, therefore, the accep-
tance probability is larger in regions where the GP uncertainty is large. Similar to the acquisition
functions in Bayesian optimisation, this naturally results in an exploration-exploitation trade-off.
However, our goal here is different, we aim to sample from the target distribution.

Therefore, given (4), in Stage 1, we accept 𝜽 ∗ with probability 𝛼 (1) (𝜽 (𝑘), 𝜽 ∗), otherwise 𝜽 (𝑘+1) =

𝜽 (𝑘) . This defines the following transition kernel at Stage 1:

𝑘 (𝐴|𝜽 (𝑘) ) =
𝑄 ∗

∫

𝐴

𝛼 (1) (𝜽 (𝑘), 𝜽 ∗)𝑞(𝜽 ∗|𝜽 (𝑘) )𝑑𝜽 ∗ + 𝐼𝐴 (𝜽 )

∫

Θ

(1 − 𝛼 (1) (𝜽 (𝑘), 𝜽 ∗))𝑑𝜽 ∗.

(5)

One can show that the above transition kernel satisfies the detailed balance property for the
approximated target distribution 𝑒 𝜇 (𝜽 |I𝑡 )+ 1
2 𝑘 (𝜽,𝜽 |I𝑡 )𝑝 (𝜽 ).

Proposition 2.2. The transition kernel (5) satisfies detailed balance.
We are not interested in the approximated target distribution, this is the reason we perform the

second stage.

Stage 2. At Stage 2, we perform another MH acceptance step, evaluating the exact log-likelihood.
𝑘 (𝑑𝜽 ∗|𝜽 (𝑘) ). Note that, 𝜽 ∗ is either equal to

Let 𝜽 ∗ denote a point sampled from 𝑞∗
the point 𝜽 ∗ sampled at Stage 1 or to 𝜽 (𝑘) if 𝜽 ∗ was rejected at Stage 1.

𝑘 (𝜽 ∗|𝜽 (𝑘) ) := 𝑄 ∗

So, with probability

𝛼 (2) (𝜽 (𝑘), 𝜽 ∗) = 1 ∧

= 1 ∧

𝑘 (𝜽 (𝑘) |𝜽 ∗)
𝑘 (𝜽 ∗|𝜽 (𝑘) )

exp(LL(𝐷 |𝜽 ∗))𝑝 (𝜽 ∗)𝑞∗
exp(LL(𝐷 |𝜽 (𝑘) ))𝑝 (𝜽 (𝑘) )𝑞∗
exp(LL(𝐷 |𝜽 ∗))𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗)𝛼 (1) (𝜽 ∗, 𝜽 (𝑘) )
exp(LL(𝐷 |𝜽 (𝑘) ))𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) )𝛼 (1) (𝜽 (𝑘), 𝜽 ∗)

,

(6)

we accept 𝜽 ∗, otherwise 𝜽 (𝑘+1) = 𝜽 (𝑘) .

The definition of 𝑞∗
𝑘

means a rejection at Stage 1 always leads to a rejection at Stage 2, we do not
need to compute (6) when (2) has led to a rejection. When the sample 𝜽 ∗ is accepted a Stage 1, we
compute the full log-likelihood, update the set I𝑡 , and evaluate (6).

Overall the acceptance probability for a new point 𝜽 ∗ is 𝛼 (1) (𝜽 (𝑘), 𝜽 ∗)𝛼 (2) (𝜽 (𝑘), 𝜽 ∗). The overall
two-stage algorithm preserves detailed balance with respect to the posterior distribution and this fol-
lows directly by (6), which is a standard MH acceptance step with proposal 𝑞𝑘 (𝜽 (𝑘) |𝜽 ∗)𝛼 (1) (𝜽 ∗, 𝜽 (𝑘) ).

4

Alessio Benavoli, Jason Wyse, and Arthur White

(a)

(d)

(b)

(e)

(c)

(f)

Fig. 1. Convergence of the GP mean to the log-normal unnormalised density and of 𝛼1 to 𝛼2 with increasing
iterations (a)-(f).

Convergence analysis. To prove the convergence to the target distribution, it is enough to show
that the overall (two-stage) transition kernel 𝑃𝑡 (·|𝜽 ) satisfies the Diminishing Adaptation condition
in Roberts and Rosenthal [2007]:

lim
𝑡→∞

sup
𝜽 ∈Θ

||𝑃𝑡 (·|𝜽 ) − 𝑃𝑡 −1(·|𝜽 )|| = 0 in probability.

(7)

and the Bounded Convergence condition which is generally satisfied under some regularity conditions
of Θ and the target distribution. The adaptivity in our two-stage algorithm is due to the GP and
diminishing adaptation follows by this property of the posterior predictive variance.

Proposition 2.3. For fixed hypeprameters, the surrogated model satisfies this property: 𝑘 (𝜽 ∗, 𝜽 ∗|I𝑡 ) <

𝑘 (𝜽 ∗, 𝜽 ∗|I𝑡 −1).

For illustration, in Figure 1, we consider a 1D case with 𝜋 (𝜃 ) ∝ 𝑒− 𝑥 2
converges to 𝛼2 at the increase of the log-likelihood evaluations in I𝑡 .

2 . It can be noticed how 𝛼1

Proposition 3.1 holds under the assumption of fixed hyperparameters for the covariance function

of the GP. Therefore, in our algorithm, we update the hyperparameters only during burnin.
In the next section, we extend these results to Metropolis-adjusted Langevin method.

3 METROPOLIS-ADJUSTED LANGEVIN
The MALA takes one step (of step size 𝛿 > 0) in the direction of the gradient from the current point

𝜽 ∗ := 𝜽 (𝑘) +

𝛿Λ(cid:16)

1
2

∇𝐿𝐿(𝜽 (𝑘) ) + ∇ log 𝑝 (𝜽 (𝑘) )

(cid:17)

+

√

𝛿Λz

(8)

with z ∼ 𝑁 (0, 𝐼 ) and Λ is a preconditioning covariance matrix. Here
Λ denotes the matrix
square root. In this case, we assume that we can evaluate both the log-likelihood and its gradient
I𝑡 = {(𝜽 (𝑖), [LL(𝐷 |𝜽 (𝑖) ), ∇𝑇 LL(𝐷 |𝜽 (𝑖) )]) : 𝑖 = 1, . . . , 𝑡 }. We use a multiple-output joint GP [Solak
et al. 2002] as surrogate model for the log-likelihood and its gradient. The idea in this case is simply
to apply the previous two-stage algorithm using the proposal (8) with gradient replaced by (cid:103)∇𝐿𝐿.

√

3210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean12Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

˜𝛼 (1) (𝜽 (𝑘), 𝜽 ∗) = 1 ∧

exp((cid:102)LL𝑡 (𝜽 ∗))𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗ + 1
exp((cid:102)LL𝑡 (𝜽 (𝑘) ))𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) + 1

2𝛿Λ (cid:103)∇𝐿𝐿(𝜽 ∗) + 1
2𝛿Λ (cid:103)∇𝐿𝐿(𝜽 (𝑘) ) + 1

2𝛿Λ∇ log 𝑝 (𝜽 ∗))
2𝛿Λ∇ log 𝑝 (𝜽 (𝑘) ))

5

(9)

where 𝑞 is the Normal proposal with covariance 𝛿Λ. Note that, (cid:102)LL𝑡 (𝜽 (𝑘) )), (cid:103)∇𝐿𝐿(𝜽 (𝑘) ) are exact
evaluations because of Assumption 1. As before we can marginalise out (cid:102)𝐿𝐿, (cid:103)∇𝐿𝐿 computing the
expectation of 𝛼1 w.r.t. the GP. We use the following result.

Proposition 3.1. The expectation of

2 Λ∇𝑓 (𝜽 ∗) + 𝛿
w.r.t. the GP, where 𝑓 , ∇𝑓 denote the GP distributed log-likelihood and its gradient and 𝑝 (𝜽 ∗) is the
prior, is equal to

2 Λ∇ log 𝑝 (𝜽 ∗))

𝑒 𝑓 (𝜽 ∗)𝑞(𝜽 ′|𝜽 ∗ + 𝛿

(10)

(cid:35)

(cid:34) 𝜇
Λ𝜇 ∇

𝑉 𝑇

𝑒

2𝑉 𝑇 𝐾𝑉 + 1
+ 1
2

𝛿2
4 𝜇𝑇
∇

Λ𝜇∇

(cid:20)

(cid:16)

1,

with 𝑉 =
for 𝑓 , ∇𝑓 and 𝐾 is the relative covariance matrix.

2 Λ∇ log 𝑝 (𝜽 ∗) − 𝛿

𝜽 ′ − 𝜽 ∗ − 𝛿

4 (Λ𝜇 ∇)𝑇 )

𝑞(𝜽 ′|𝜽 ∗ + 𝛿
2 Λ∇ log 𝑝 (𝜽 ∗))
(cid:21)𝑇

(cid:17)𝑇

Λ−1 𝛿
2

, 𝜇, 𝜇 ∇ are the GP predictive means

Stage 2 uses the exact evaluation of the log-likelihood and its gradient. We omit the details. The
overall algorithm is similar to the one presented previously for MH with the only difference that
the GP is multi-output over the log-likelihood and its gradient.

4 NUMERICAL EXPERIMENTS
To model the log-likelihood (and its gradient for MALA), we use a GP with Square Exponential
covariance function. A zero mean is used with the value of LL(𝜽 (𝑘) ) (and its gradient for MALA)
subtracted. This is equivalent to defining a GP with prior mean equal to LL(𝜽 (𝑘) ); in this way,
far from the data, the acceptance probability 𝛼1 only depends on the variance of the GP. This
guarantees a high probability of acceptance in Stage 1 for samples in large-uncertainty regions.
The GP is initialised using 3 observations, before starting the two-stage sampler.

We consider five target distributions.
T1 The 2D posterior of the parameters 𝑎, 𝑏 of the banana shape distribution (true value set to

T2 The 3D posterior of the parameters 𝑎, 𝑏, 𝜎 of the nonlinear regression model 𝑦 = 𝑎 𝑥

𝑥+𝑏 + 𝜖,

𝜖 ∼ 𝑁 (0, 𝜎 2) (true value set to 𝑎 = 0.14, 𝑏 = 50, 𝜎 = 0.1).

T3 The 3D posterior of the parameters ℓ1, ℓ2, 𝜎 2 of the SE kernel for a GP-classifier.
T4 The 4D posterior of the parameters 𝛽, 𝛾, 𝜎1, 𝜎2 of a Susceptible, Infected, Recovery (SIR)

𝑎 = 0.2, 𝑏 = 2);

model.

T5 The 5D posterior of the parameters 𝛽0, . . . , 𝛽4 of a parametric logistic regression problem.
Appendix A gives further details on priors assumed for the parameters and selected proposal. Each
of these five posteriors has a specific feature, resulting in a diverse set of challenging targets, for
instance T1 is heavy tailed and T2 is heavily anisotropic. T4, the SIR problem, is a prototypical
example of the type of applications targeted by the proposed method. To compute the likelihood,
we need to solve numerically a system of ODEs and, in more complex biological and chemical
models, this can be computationally heavy.

Evaluating the likelihood in these five problems is very fast, this allows us to quickly perform
Monte Carlo simulations to assess the performance of the model by generating artificial data.

6

Alessio Benavoli, Jason Wyse, and Arthur White

T1

T3

T5

MH
GP-MH
MALA
GP-MALA
MH
GP-MH
MALA
GP-MALA
MH
GP-MH
MALA
GP-MALA

AR
0.37
0.36
0.26
0.26
0.42
0.42
0.44
0.43
0.29
0.29
0.67
0.67

ESS
90
113
73
75
137
135
133
134
98
102
339
368

ESJD
0.13
0.13
0.2
0.2
0.44
0.38
0.48
0.45
0.002
0.002
0.009
0.009

Eval%
100
41
100
35
100
42
100
45
100
35
100
68

SD
0.02
0.02
0.03
0.02
4.1
3.5
4.1
3.5
0.006
0.006
0.006
0.006

MH
GP-MH
MALA
GP-MALA
MH
GP-MH

T2

T4

AR
0.28
0.27
0.26
0.21
0.1
0.1

ESS
138
133
220
147
51
45

ESJD
32.6
31
51
29
0.003
0.003

Eval%
100
39
100
43
100
15

SD
339
339
316
255
0.009
0.009

Table 1. Comparison between the proposed GP-based samplers and standard MH and MALA in terms of
the acceptance rate (AR), Effective Sample Size (ESS), Expected Square Jumping Distance (ESJD), percentage
of likelihood evaluations in the 2000 iterations (EVAL), Square Distance (SD) between the true value of the
parameters and the estimated posterior mean. We have not run MALA for the SIR model.

We then evaluate the efficiency of the algorithms by simply counting the number of likelihood
evaluations.

We compare our two-stage algorithm with the standard implementations of MH and MALA. For
each target problem and in each simulation, we generate 2500 samples (including 500 for burnin).
We have deliberately selected a small number of samples to show that our approach converges
quickly, which is important in computationally expensive applications. We check for convergence
to the correct posterior distribution using the metrics described in the caption of Table 1.

Table 1 reports the value of the metrics averaged over the 30 simulations and over parameters.
Comparing the simulations’ results it can be noticed that the proposed GP-based samplers obtain
the same convergence metrics of the standard MH and MALA, but with a fraction of the number
of likelihood evaluations. It can also be noticed how the fraction of the number of full likelihood
evaluations required is problem dependent, ranging from 15% for T4 to 65% for T3. This demonstrates
that our approach automatically adapts to the complexity of the specific target distribution.

5 CONCLUSIONS
We have presented a two-stage Metropolis-Hastings algorithm for sampling probabilistic models,
whose log-likelihood is computationally expensive to evaluate, by using a surrogate GP model.
The key feature of the approach, and the difference w.r.t. previous works, is the ability to learn
the target distribution from scratch (while sampling), and so without the need of pre-training the
GP. This is fundamental for automatic and inference in Probabilistic Programming Languages In
particular, we have presented an alternative first stage acceptance scheme by marginalising out the
GP distributed function, which makes the acceptance ratio explicitly dependent on the variance of
the GP. This approach is extended to Metropolis-Adjusted Langevin algorithm (MALA). Numerical
experiments have demonstrated the effectiveness of the method, which can automatically adapt to
the complexity of the target distribution. In the numerical experiments, we have used a full GP
whose computational load grows cubically as the size of the training set increases. Sparse GPs can
be employed to address this issue [Bauer et al. 2016; Hensman et al. 2013; Hernández-Lobato and
Hernández-Lobato 2016; Quiñonero-Candela and Rasmussen 2005; Schuerch et al. 2020; Snelson
and Ghahramani 2006; Titsias 2009] when it is necessary to sample thousands of samples.

In future work, we plan to extend the approach we used for MALA to Hamiltonian Monte Carlo.
We also intend to investigate whether tailored covariance functions for log densities or ratios of
densities can provide any convergence advantage, but also investigate surrogate models alternative
to GPs.

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

7

ACKNOWLEDGMENTS

REFERENCES
P. Alquier, N. Friel, R. Everitt, and A. Boland. 2016. Noisy Monte Carlo: Convergence of Markov Chains with Approximate
Transition Kernels. Statistics and Computing 26, 1–2 (Jan. 2016), 29–47. https://doi.org/10.1007/s11222-014-
9521-x

Christophe Andrieu and Gareth O. Roberts. 2009. The pseudo-marginal approach for efficient Monte Carlo computations.

The Annals of Statistics 37, 2 (2009), 697 – 725. https://doi.org/10.1214/07-AOS574

Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. 2016. Understanding probabilistic sparse Gaussian process

approximations. In Advances in neural information processing systems. 1533–1541.

Nikolay Bliznyuk, David Ruppert, and Christine A Shoemaker. 2012. Local derivative-free approximation of computationally

expensive posterior densities. Journal of Computational and Graphical Statistics 21, 2 (2012), 476–495.

E. Brochu, V.M. Cora, and N. De Freitas. 2010. A tutorial on Bayesian optimization of expensive cost functions, with
application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599 (2010).
J. Andrés Christen and Colin Fox. 2005. Markov Chain Monte Carlo Using an Approximation. Journal of Computational and

Graphical Statistics 14, 4 (2005), 795–810. http://www.jstor.org/stable/27594150

Demetri Pananos. 2019. PyMC3 examples https://docs.pymc.io/notebooks/ODE_API_introduction.html.
Mark Fielding, David J Nott, and Shie-Yui Liong. 2011. Efficient MCMC schemes for computationally expensive posterior

distributions. Technometrics 53, 1 (2011), 16–28.

Leigh J Halliwell. 2015. The lognormal random multivariate. In Casualty Actuarial Society E-Forum, Spring, Vol. 5.
W. K. Hastings. 1970. Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57,
https://doi.org/10.1093/biomet/57.1.97 arXiv:https://academic.oup.com/biomet/article-

1 (04 1970), 97–109.
pdf/57/1/97/23940249/57-1-97.pdf

James Hensman, Nicolò Fusi, and Neil D. Lawrence. 2013. Gaussian Processes for Big Data. In Proceedings of the Twenty-Ninth

Conference on Uncertainty in Artificial Intelligence (UAI’13). AUAI Press, Arlington, Virginia, USA, 282–290.

Daniel Hernández-Lobato and José Miguel Hernández-Lobato. 2016. Scalable gaussian process classification via expectation

propagation. In Artificial Intelligence and Statistics. 168–176.

V Roshan Joseph. 2012. Bayesian computation using design of experiments-based interpolation technique. Technometrics

54, 3 (2012), 209–225.

Lingge Li, Andrew Holbrook, Babak Shahbaba, and Pierre Baldi. 2019. Neural network gradient hamiltonian monte carlo.

Computational statistics 34, 1 (2019), 281–299.

Radford M. Neal. 2011. MCMC Using Hamiltonian Dynamics. CRC Press, Chapter 5. https://doi.org/10.1201/b10905-7
Joaquin Quiñonero-Candela and Carl Edward Rasmussen. 2005. A unifying view of sparse approximate Gaussian process

regression. Journal of Machine Learning Research 6, Dec (2005), 1939–1959.

Carl Edward Rasmussen. 2003. Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. In

Seventh Valencia international meeting, dedicated to Dennis V. Lindley. Oxford University Press, 651–659.

Christian P. Robert. 2015. The Metropolis–Hastings Algorithm. American Cancer Society, 1–15. https://doi.org/10.
1002/9781118445112.stat07834 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07834
Gareth O. Roberts and Jeffrey S. Rosenthal. 2007. Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo

Algorithms. Journal of Applied Probability 44, 2 (2007), 458–475. https://doi.org/10.1239/jap/1183667414

Manuel Schuerch, Dario Azzimonti, Alessio Benavoli, and Marco Zaffalon. 2020. Recursive estimation for sparse Gaussian

process regression. Automatica 120 (2020), 109–127.

Chris Sherlock, Andrew Golightly, and Daniel A. Henderson. 2017. Adaptive, Delayed-Acceptance MCMC for Targets With
Expensive Likelihoods. Journal of Computational and Graphical Statistics 26, 2 (2017), 434–444. https://doi.org/10.
1080/10618600.2016.1231064 arXiv:https://doi.org/10.1080/10618600.2016.1231064

Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian processes using pseudo-inputs. In Advances in neural

information processing systems. 1257–1264.

E. Solak, R. Murray-Smith, W. E. Leithead, D. J. Leith, and C. E. Rasmussen. 2002. Derivative Observations in Gaussian
Process Models of Dynamic Systems. In Proceedings of the 15th International Conference on Neural Information Processing
Systems (NIPS’02). MIT Press, Cambridge, MA, USA, 1057–1064.

Michalis Titsias. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Proceedings of the Twelth
International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research), David van Dyk
and Max Welling (Eds.), Vol. 5. PMLR, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 567–574.

8

Alessio Benavoli, Jason Wyse, and Arthur White

A PRIORS FOR T1–T5
A zero prior is used for T1. In each simulation, we generate a different starting point in the interval
[−2, 2].

For T2, the prior is 𝑎 ∼ 𝑁 (3, 1), 𝑏 ∼ 𝑁 (30, 152), log 𝜎 ∼ 𝑁 (−2, 1) and 𝑥 = [28, 55, 83, 110, 138, 225, 375].

In each simulation, we generate different 𝑦 according to the likelihood reported in the main text.
For T3, the prior is log ℓ𝑖, log 𝜎 ∼ 𝑁 (0, 10). The GP classification dataset includes 1000 points
with 𝑥1, 𝑥2 ∼ 𝑁 (0, 1). In each simulation, the true two lengthscales are uniformly sampled from
[0.1, 1]. The true variance is fixed at 𝜎 2 = 5.

For T4, we use the probabilistic model described in [Demetri Pananos 2019].
For T5, the prior is 𝛽𝑖 ∼ 𝑁 (0, 100). The GP classification dataset includes 1000 points with

𝑥1, 𝑥2 ∼ 𝑁 (0, 1). In each simulation, the true 𝛽𝑖 are sampled from 𝑁 (0, 1).

In all cases, we have used Normal proposals for MH with diagonal covariance matrix.

B PROOFS

B.1 Proof of Proposition 2.1
This follows by the mean of the log-normal distribution.

B.2 Proof of Proposition 2.2
We prove detailed balance:

𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )+ 1

2 𝑘 (𝜽 (𝑘 ) ,𝜽 (𝑘 ) |I𝑡 )𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) )𝛼 (1) (𝜽 (𝑘), 𝜽 ∗)

= 𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )+ 1

=

=

(cid:16)
𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )+ 1

(cid:18) 𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )+
𝑒 𝜇 (𝜽 ∗ |I𝑡 )+

(cid:18)

1 ∧

2 𝑘 (𝜽 (𝑘 ) ,𝜽 (𝑘 ) |I𝑡 )𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) )

𝑒 𝜇 (𝜽 ∗ |I𝑡 )+
1
𝑒 𝜇 (𝜽 (𝑘 ) |I𝑡 )+
2 𝑘 (𝜽 (𝑘 ) ,𝜽 (𝑘 ) |I𝑡 )𝑝 (𝜽 (𝑘) )𝑞(𝜽 ∗|𝜽 (𝑘) ) ∧ 𝑒 𝜇 (𝜽 ∗ |I𝑡 )+ 1
2 𝑘 (𝜽 (𝑘 ) ,𝜽 (𝑘 ) |I𝑡 ) 𝑝 (𝜽 (𝑘 ) )𝑞 (𝜽 ∗ |𝜽 (𝑘 ) )
2 𝑘 (𝜽 ∗,𝜽 ∗ |I𝑡 ) 𝑝 (𝜽 ∗)𝑞 (𝜽 (𝑘 ) |𝜽 ∗)

𝑒 𝜇 (𝜽 ∗ |I𝑡 )+ 1

∧ 1

(cid:19)

1

1

(cid:19)

1

2 𝑘 (𝜽 ∗,𝜽 ∗ |I𝑡 ) 𝑝 (𝜽 ∗)𝑞 (𝜽 (𝑘 ) |𝜽 ∗)
2 𝑘 (𝜽 (𝑘 ) ,𝜽 (𝑘 ) |I𝑡 ) 𝑝 (𝜽 )𝑞 (𝜽 ∗ |𝜽 (𝑘 ) )
2 𝑘 (𝜽 ∗,𝜽 ∗ |I𝑡 )𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗)

(cid:17)

2 𝑘 (𝜽 ∗,𝜽 ∗ |I𝑡 )𝑝 (𝜽 ∗)𝑞(𝜽 (𝑘) |𝜽 ∗).

which ends the proof. The term in brackets in the last equation is 𝛼 (1) (𝜽 ∗, 𝜽 (𝑘) ). Because of
Assumption 1, we have that 𝑘 (𝜽 (𝑘), 𝜽 ∗|I𝑡 ) = 0 (independent). This is the reason we can assume
work on the numerator and denominator independently.

B.3 Proof of Proposition 2.3
Let 𝜽 ′ denote the new point in I𝑡 −1 and with 𝑅 all the points in I𝑡 −1, by definition of Kernel matrix:
(cid:21)

(cid:21)

𝐾𝑡 =

(cid:20) 𝐾𝑡 −1
𝑘𝑡 −1(𝜽 ′)
𝑘𝑡 −1(𝜽 ′)⊤ 𝑘 (𝜽 ′, 𝜽 ′)

:=

(cid:20) 𝑘 (𝑅, 𝑅)
𝑘 (𝜽 ′, 𝑅)
𝑘 (𝜽 ′, 𝑅)⊤ 𝑘 (𝜽 ′, 𝜽 ′)

.

(11)

The predicted variance at step 𝑡 at the point 𝜽 ∗ is

𝑘 (𝜽 ∗, 𝜽 ∗) − 𝑘𝑡 (𝜽 ∗)⊤𝐾 −1

𝑡 𝑘𝑡 (𝜽 ∗),

while the predicted variance at step 𝑡 − 1 at the point 𝜽 ∗ is

𝑘 (𝜽 ∗, 𝜽 ∗) − 𝑘𝑡 −1(𝜽 ∗)⊤𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗)

Therefore, we need to prove that

𝑘𝑡 (𝜽 ∗)⊤𝐾 −1

𝑡 𝑘𝑡 (𝜽 ∗) > 𝑘𝑡 −1(𝜽 ∗)⊤𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗).

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

The inverse of 𝐾𝑡 is:

𝐾 −1

𝑡 =

(cid:20)𝐾 −1

𝑡 −1 + 𝐾 −1

with 𝑀 = (𝑘 (𝜽 ′, 𝜽 ′) − 𝑘𝑡 −1(𝜽 ′)⊤𝐾 −1
𝑡 𝑘𝑡 (𝜽 ∗)

𝑘𝑡 (𝜽 ∗)⊤𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ′)𝑀𝑘𝑡 −1(𝜽 ′)⊤𝐾 −1
−𝑀𝑘𝑡 −1(𝜽 )⊤𝐾 −1
𝑡 −1
𝑡 −1𝑘𝑡 −1(𝜽 ′))−1. Now note that

𝑡 −1 −𝐾 −1

𝑡 −1𝑘𝑛−1(𝜽 )𝑀
𝑀

9

(12)

(cid:21)

,

= (cid:2)𝑘 ⊤

𝑡 −1 (𝜽 ∗), 𝑘 ′(𝜽 ∗)(cid:3)

(cid:20)𝐾 −1

𝑡 −1 + 𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ′)𝑀𝑘𝑡 −1(𝜽 ′)⊤𝐾 −1
−𝑀𝑘𝑡 −1(𝜽 )⊤𝐾 −1
𝑡 −1

𝑡 −1 −𝐾 −1

𝑡 −1𝑘𝑛−1(𝜽 )𝑀
𝑀

(cid:21)

(cid:21) (cid:20)𝑘𝑡 −1(𝜽 ∗)
𝑘 ′(𝜽 ∗)

which is equal to:
𝑘𝑡 (𝜽 ∗)⊤𝐾 −1

𝑡 𝑘𝑡 (𝜽 ∗) = 𝑘 ⊤

𝑡 −1 (𝜽 ∗)𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗) + 𝑘 ⊤

𝑡 −1(𝜽 ∗)𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ′)𝑀𝑘𝑡 −1 (𝜽 ′)⊤𝐾 −1

− 𝑘 ′(𝜽 ∗)𝑀𝑘𝑡 −1 (𝜽 ′)⊤𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗) − 𝑘𝑡 −1(𝜽 ∗)⊤𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗)
𝑡 −1𝑘𝑡 −1(𝜽 ′)𝑀𝑘 ′(𝜽 ∗) + 𝑘 ′(𝜽 ∗)𝑀𝑘 ′(𝜽 ∗).

where 𝑘 ′(𝜽 ∗) := 𝑘 (𝜽 ∗, 𝜽 ′). Therefore, we have that

𝑘𝑡 (𝜽 ∗)⊤𝐾 −1

𝑡 𝑘𝑡 (𝜽 ∗) = 𝑘 ⊤

𝑡 −1(𝜽 ∗)𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ∗) + 𝑁 .

with

𝑁 = 𝑀 (𝑘 ⊤

𝑡 −1(𝜽 ∗)𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ′) − 𝑘 ′(𝜽 ∗))2 = 𝑀 (𝑘 ⊤

𝑡 −1(𝜽 ∗)𝐾 −1

𝑡 −1𝑘𝑡 −1(𝜽 ′) − 𝑘 (𝜽 ∗, 𝜽 ′))2

which is strictly greater than zero whenever 𝜽 ∗, 𝜽 ′ ∉ I𝑡 −1. Note in fact that, under Assumption 1,
𝑁 = 0 only if either 𝜽 ∗ ∈ I𝑡 −1 or 𝜽 ′ ∈ I𝑡 −1 (exact interpolation property). If the proposal distribution
is absolutely continuous w.r.t. the Lebesgue measure on Θ, then 𝜽 ∗, 𝜽 ′ ∉ I𝑡 −1 holds with probability
1.

B.4 Proof of Proposition 3.1
We work in log-scale and omit the dependence of 𝑓 , ∇𝑓 on 𝜽 ∗ for notation simplification and so we
can rewrite the product of the two PDF in (10):

(cid:18)
𝜽 ′ − 𝜽 ∗ −

𝛿
2

Λ∇ log 𝑝 (𝜽 ∗)

(cid:19)𝑇

Λ−1(· · · ) +

2
2

(cid:18)
𝜽 ′ − 𝜽 ∗ −

Λ∇ log 𝑝 (𝜽 ∗)

(cid:19)𝑇

Λ−1(

𝛿
2

Λ∇𝑓 )

(cid:19)𝑇

Λ∇𝑓

Λ−1(· · · ) −

1
2

(cid:2)𝑓 − 𝜇, ∇𝑇 𝑓 − 𝜇𝑇
∇

(cid:3)

(cid:20) 𝐴 𝐵
𝐵𝑇 𝐶

𝛿
2
(cid:21) (cid:20) 𝑓 − 𝜇
∇𝑓 − 𝜇 ∇

(cid:21)

where Λ is the covariance matrix of the proposal, and the last term in the above sum is the GP
predictive posterior with mean [𝜇, 𝜇 ∇] and covariance 𝐾. We have expressed 𝐾 −1 as the block
matrix with blacks 𝐴, 𝐵, 𝐶, 𝐷. We can rewrite the above sum as

(cid:18)
𝜽 ′ − 𝜽 ∗ −

𝛿
2

Λ∇ log 𝑝 (𝜽 ∗)

(cid:19)𝑇

Λ−1(· · · ) +

2
2

(cid:18)
𝜽 ′ − 𝜽 ∗ −

𝛿
2

Λ∇ log 𝑝 (𝜽 ∗)

(cid:19)𝑇

Λ−1(

𝛿
2

Λ∇𝑓 )

𝜇𝑇
∇

Λ∇𝑓 +

1
2

𝛿 2
4

𝜇𝑇
∇

Λ𝜇 ∇ −

1
2

(cid:2)𝑓 − 𝜇, ∇𝑇 𝑓 − 𝜇𝑇
∇

(cid:3)

(cid:20) 𝐴
𝐵𝑇

𝐵
𝛿 2
4 Λ + 𝐶

(cid:21)

(cid:21) (cid:20) 𝑓 − 𝜇
∇𝑓 − 𝜇 ∇

We can now define the vector 𝑉 and prove the result using the moments of the multivariate
Lognormal distribution [Halliwell 2015].

1
2
(cid:18) 𝛿
2

𝑓 −

−

1
2

1
2
𝛿 2
4

𝑓 −

−

2
2

