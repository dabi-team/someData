1
2
0
2

p
e
S
8
2

]
L
M

.
t
a
t
s
[

1
v
1
9
8
3
1
.
9
0
1
2
:
v
i
X
r
a

Gaussian Processes to speed up MCMC with automatic
exploratory-exploitation effect

ALESSIO BENAVOLI, JASON WYSE, and ARTHUR WHITE, School of Computer Science and
Statistics, Trinity College Dublin, Ireland

1 INTRODUCTION
In this paper, we consider the problem of sampling from a posterior

ğœ‹ (ğœ½ |ğ·) âˆ ğ‘ (ğ· |ğœ½ )ğ‘ (ğœ½ ),
where ğ· denotes data and ğœ½ âˆˆ Î˜ is a vector of unknown parameters, in the case where the likelihood
ğ‘ (ğ· |ğœ½ ) is costly to evaluate. We discuss two-stage algorithms. In the first of these, we examine
an adaptive Metropolis-Hastings (MH) algorithm [Hastings 1970; Robert 2015] which employs
an adaptively tuned Gaussian Process (GP) surrogate model at the first stage to filter out poor
proposals. If a proposal is not filtered out, at the second stage a full (expensive) log-likelihood
evaluation is carried out and used to decide whether it is accepted as the next state. Introduction of
the first stage, constructed in this way, saves computation on poor proposals. A key contribution of
this work is in the form of the acceptance probability in the first stage obtained by marginalising
out the GP function. This makes the acceptance ratio dependent on the variance of the GP, which
naturally results in an exploration-exploitation trade-off similar to the one of Bayesian Optimisation
[Brochu et al. 2010], which allows us to sample while learning the GP. We demonstrate that using
this expectation serves as a useful filtering scheme. The second algorithm is a two-stage form of
Metropolis adjusted Langevin algorithm (MALA) [Neal 2011]. Here, we use GP as a surrogate for
the log-likelihood function again, but in this case the GP is also used to approximate the gradient
required for MALA updating, using a well known result that the gradient of a GP is also a GP [Solak
et al. 2002]. Marginalizing out of the GP can also be performed in this instance.

The approximation we use is

LL(ğ· |ğœ½ ) := ln ğ‘ (ğ· |ğœ½ ) â‰ˆ (cid:102)LLğ‘¡ (ğ· |ğœ½ ) âˆ¼ GP(ğœ‡ (ğœ½ |Iğ‘¡ ), ğ‘˜ (ğœ½, ğœ½ âˆ—|Iğ‘¡ ))
where Iğ‘¡ denotes the set of ğ‘¡ full evaluations of the log-likelihood by the current iteration, and ğœ½ âˆ—
collectively denotes the parameter values at which these evaluations were made. Adaptive tuning
of the GP surrogate is accomplished through use of the collection Iğ‘¡ of full evaluations of the log-
likelihood. We argue that the tuning schedule we suggest satisfies diminishing adaptation [Roberts
and Rosenthal 2007] and hence will ensure correct sampling from the true target ğœ‹ (ğœ½ |ğ·).

(1)

Within the Markov chain Monte Carlo (MCMC) literature, there has been much interest in
recent years, in the use of proxy quantities for the target measure evaluations from different
aspects. Approaches using noisy approximations to an invariant transition kernel [Alquier et al.
2016; Andrieu and Roberts 2009] have gained much interest. The work here assumes that the
log-likelihood, though maybe expensive, can be computed, and is thus more aligned to the work
of Bliznyuk et al. [2012]; Christen and Fox [2005]; Fielding et al. [2011]; Joseph [2012]; Li et al.
[2019]; Rasmussen [2003]; Sherlock et al. [2017], involving ideas from delayed acceptance MCMC.
The key difference is that we do not carry out pre-computation of the GP prior to running the
algorithm, investigating adaptation of the GP on the fly using key results from the adaptive MCMC
literature [Roberts and Rosenthal 2007] to ensure convergence to the true target.

Authorsâ€™ address: Alessio Benavoli; Jason Wyse; Arthur White, School of Computer Science and Statistics, Trinity College
Dublin, Ireland, alessio.benavoli@tcd.ie, wyseja@tcd.ie, arwhite@tcd.ie.

 
 
 
 
 
 
2

Alessio Benavoli, Jason Wyse, and Arthur White

We present the two stage MH algorithm in Section 2. Section 3 follows by introducing a the
two-stage MALA algorithm building on these ideas. Section 4 explores a range of examples and
demonstrates the merits of the filtering step, with a discussion of potential drawbacks.

2 TWO STAGE ADAPTIVE METROPOLIS-HASTINGS VIA GP APPROXIMATION
We combine the MH algorithm with a GP model which approximates the log-likelihood. In cases
where the log-likelihood is expensive to compute, the GP model can be used in a pre-filtering
step to determine proposals for which a full computation of the log-likelihood might well lead
to an acceptance [Christen and Fox 2005]. At each iteration of the algorithm, the first stage, uses
a GP to deliver an approximate log-likelihood evaluation. The GP is based on a collection Iğ‘¡ of
previous full evaluations of the log-likelihood. A propsal is made from the current state and then
the usual MH acceptance probability is computed using the approximated log-likelihood (this
step is computationally inexpensive). If the proposal is accepted in this first stage, then it goes to
the second stage, where another acceptance probability is computed, but this time, based on the
full costly evaluation of the log-likelihood. The resulting evaluation of the log-likelihood is then
appended to Iğ‘¡ , resulting in Iğ‘¡ +1.

Before giving a full description of the algorithm, we introduce some notation and give an explicit

definition of Iğ‘¡ :

â€¢ Sğ‘˜ denotes the points sampled up to the iteration ğ‘˜ of the algorithm;
â€¢ ğœ½ (ğ‘˜) denotes the most recent element in Sğ‘˜ and ğœ½ âˆ— denotes the proposed state
â€¢ Iğ‘¡ = {(ğœ½ (ğ‘–), LL(ğ· |ğœ½ (ğ‘–) )) :
performed up to iteration ğ‘˜.

ğ‘– = 1, . . . , ğ‘¡ } denotes the ğ‘¡ â‰¤ ğ‘˜ exact likelihood evaluations

We use a noise free GP as a surrogate model for the log-likelihood and denote by GPğ‘˜ (ğœ‡ (ğœ½ |Iğ‘¡ ), ğ‘˜ (ğœ½, ğœ½ |Iğ‘¡ ))
the posterior GP at the iteration ğ‘˜ conditioned on the collection Iğ‘¡ . We use (cid:102)LLğ‘˜ (ğœ½ ) to denote the
GP-distributed log-likelihood. We choose the parameters of the GP to satisfy the following exact
interpolation property.

Assumption 1. The prior mean function and prior covariance function of the GP are selected to

guarantee exact interpolation:

ğœ‡ (ğœ½ (ğ‘–) |Iğ‘¡ ) = LL(ğ· |ğœ½ (ğ‘–) ),

ğ‘˜ (ğœ½ (ğ‘–), ğœ½ |Iğ‘¡ ) = 0,

for all ğœ½ (ğ‘–) with a corresponding entry in Iğ‘¡ and ğœ½ âˆˆ Î˜.1

This means the predictions of the GP at the points ğœ½ (ğ‘–) âˆˆ Iğ‘¡ are exact and certain (zero

(co)variance), which is a desirable property in a noise free regression problem.2.

The two stages of the MH algorithm are as follows.

Stage 1. Use the predictive posterior GP (conditioned on the collection Iğ‘¡ ) to approximate the

log-likelihood. Define the first stage acceptance probability:

Ëœğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—) = 1 âˆ§

exp((cid:102)LLğ‘¡ (ğœ½ âˆ—))ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ—)
exp((cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ))ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )

(2)

where (cid:102)LLğ‘¡ (Â·) âˆ¼ GPğ‘˜ (ğœ‡ (ğœ½ |Iğ‘¡ ), ğ‘˜ (ğœ½, ğœ½ |Iğ‘¡ )) and we use the shorthand notation ğ‘ âˆ§ ğ‘ = min(ğ‘, ğ‘).
Note that, because of the exact interpolation property in Assumption 1, it results that (cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ) =
LLğ‘¡ (ğœ½ (ğ‘˜) ).

1Any universal covariance function satisfies this property, for instance Squared Exponential.
2This also guarantees consistency between the two stages: the denominator of (2) and (6) is the same

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

3

The acceptance probability Ëœğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—) (respectively, Ëœğ›¼ (1) (ğœ½ âˆ—, ğœ½ (ğ‘˜) )) depends on (cid:102)LLğ‘¡ (ğœ½ âˆ—) âˆ’
(cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ) (respectively, âˆ’(cid:102)LLğ‘¡ (ğœ½ âˆ—) + (cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ) ) which is GP distributed. A key part of our approach
involves marginalizing this dependence out by exploiting the following result.

Proposition 2.1. The distribution of ğ‘’ (cid:102)LLğ‘¡ (ğœ½ ) is Lognormal (ğœ‡ (ğœ½ |Iğ‘¡ ), ğ‘˜ (ğœ½, ğœ½ |Iğ‘¡ )) , and its mean is
(3)

ğ‘’ ğœ‡ (ğœ½ |Iğ‘¡ )+ 1

2 ğ‘˜ (ğœ½,ğœ½ |Iğ‘¡ ) .

The proofs of this and the other Propositions are given in Appendix B. By Assumption 1, we have
that ğ‘˜ (ğœ½ (ğ‘˜), ğœ½ (ğ‘˜) |Iğ‘¡ ) = ğ‘˜ (ğœ½ âˆ—, ğœ½ (ğ‘˜) |Iğ‘¡ ) = 0 and, therefore, (cid:102)LLğ‘¡ (ğœ½ âˆ—) and (cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ) are sampled inde-
pendently. By exploiting Proposition 2.1, we remove the dependence of the acceptance probability
on (cid:102)LL in (2) resulting in the acceptance probability:
ğ‘’ ğœ‡ (ğœ½ âˆ— |Iğ‘¡ )+ 1

2 ğ‘˜ (ğœ½ âˆ—,ğœ½ âˆ— |Iğ‘¡ )ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ—)

,

(4)

ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—) = 1 âˆ§

ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )

where ğœ‡ (ğœ½ (ğ‘˜) |Iğ‘¡ ) = LL(ğœ½ (ğ‘˜) ) is the exact log-likelihood (by Assumption 1).

It can be seen that ğœ‡ (ğœ½ âˆ—|Iğ‘¡ ) + 1

2ğ‘˜ (ğœ½ âˆ—, ğœ½ âˆ—|Iğ‘¡ ) depends on the GP variance and, therefore, the accep-
tance probability is larger in regions where the GP uncertainty is large. Similar to the acquisition
functions in Bayesian optimisation, this naturally results in an exploration-exploitation trade-off.
However, our goal here is different, we aim to sample from the target distribution.

Therefore, given (4), in Stage 1, we accept ğœ½ âˆ— with probability ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—), otherwise ğœ½ (ğ‘˜+1) =

ğœ½ (ğ‘˜) . This defines the following transition kernel at Stage 1:

ğ‘˜ (ğ´|ğœ½ (ğ‘˜) ) =
ğ‘„ âˆ—

âˆ«

ğ´

ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—)ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )ğ‘‘ğœ½ âˆ— + ğ¼ğ´ (ğœ½ )

âˆ«

Î˜

(1 âˆ’ ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—))ğ‘‘ğœ½ âˆ—.

(5)

One can show that the above transition kernel satisfies the detailed balance property for the
approximated target distribution ğ‘’ ğœ‡ (ğœ½ |Iğ‘¡ )+ 1
2 ğ‘˜ (ğœ½,ğœ½ |Iğ‘¡ )ğ‘ (ğœ½ ).

Proposition 2.2. The transition kernel (5) satisfies detailed balance.
We are not interested in the approximated target distribution, this is the reason we perform the

second stage.

Stage 2. At Stage 2, we perform another MH acceptance step, evaluating the exact log-likelihood.
ğ‘˜ (ğ‘‘ğœ½ âˆ—|ğœ½ (ğ‘˜) ). Note that, ğœ½ âˆ— is either equal to

Let ğœ½ âˆ— denote a point sampled from ğ‘âˆ—
the point ğœ½ âˆ— sampled at Stage 1 or to ğœ½ (ğ‘˜) if ğœ½ âˆ— was rejected at Stage 1.

ğ‘˜ (ğœ½ âˆ—|ğœ½ (ğ‘˜) ) := ğ‘„ âˆ—

So, with probability

ğ›¼ (2) (ğœ½ (ğ‘˜), ğœ½ âˆ—) = 1 âˆ§

= 1 âˆ§

ğ‘˜ (ğœ½ (ğ‘˜) |ğœ½ âˆ—)
ğ‘˜ (ğœ½ âˆ—|ğœ½ (ğ‘˜) )

exp(LL(ğ· |ğœ½ âˆ—))ğ‘ (ğœ½ âˆ—)ğ‘âˆ—
exp(LL(ğ· |ğœ½ (ğ‘˜) ))ğ‘ (ğœ½ (ğ‘˜) )ğ‘âˆ—
exp(LL(ğ· |ğœ½ âˆ—))ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ—)ğ›¼ (1) (ğœ½ âˆ—, ğœ½ (ğ‘˜) )
exp(LL(ğ· |ğœ½ (ğ‘˜) ))ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—)

,

(6)

we accept ğœ½ âˆ—, otherwise ğœ½ (ğ‘˜+1) = ğœ½ (ğ‘˜) .

The definition of ğ‘âˆ—
ğ‘˜

means a rejection at Stage 1 always leads to a rejection at Stage 2, we do not
need to compute (6) when (2) has led to a rejection. When the sample ğœ½ âˆ— is accepted a Stage 1, we
compute the full log-likelihood, update the set Iğ‘¡ , and evaluate (6).

Overall the acceptance probability for a new point ğœ½ âˆ— is ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—)ğ›¼ (2) (ğœ½ (ğ‘˜), ğœ½ âˆ—). The overall
two-stage algorithm preserves detailed balance with respect to the posterior distribution and this fol-
lows directly by (6), which is a standard MH acceptance step with proposal ğ‘ğ‘˜ (ğœ½ (ğ‘˜) |ğœ½ âˆ—)ğ›¼ (1) (ğœ½ âˆ—, ğœ½ (ğ‘˜) ).

4

Alessio Benavoli, Jason Wyse, and Arthur White

(a)

(d)

(b)

(e)

(c)

(f)

Fig. 1. Convergence of the GP mean to the log-normal unnormalised density and of ğ›¼1 to ğ›¼2 with increasing
iterations (a)-(f).

Convergence analysis. To prove the convergence to the target distribution, it is enough to show
that the overall (two-stage) transition kernel ğ‘ƒğ‘¡ (Â·|ğœ½ ) satisfies the Diminishing Adaptation condition
in Roberts and Rosenthal [2007]:

lim
ğ‘¡â†’âˆ

sup
ğœ½ âˆˆÎ˜

||ğ‘ƒğ‘¡ (Â·|ğœ½ ) âˆ’ ğ‘ƒğ‘¡ âˆ’1(Â·|ğœ½ )|| = 0 in probability.

(7)

and the Bounded Convergence condition which is generally satisfied under some regularity conditions
of Î˜ and the target distribution. The adaptivity in our two-stage algorithm is due to the GP and
diminishing adaptation follows by this property of the posterior predictive variance.

Proposition 2.3. For fixed hypeprameters, the surrogated model satisfies this property: ğ‘˜ (ğœ½ âˆ—, ğœ½ âˆ—|Iğ‘¡ ) <

ğ‘˜ (ğœ½ âˆ—, ğœ½ âˆ—|Iğ‘¡ âˆ’1).

For illustration, in Figure 1, we consider a 1D case with ğœ‹ (ğœƒ ) âˆ ğ‘’âˆ’ ğ‘¥ 2
converges to ğ›¼2 at the increase of the log-likelihood evaluations in Iğ‘¡ .

2 . It can be noticed how ğ›¼1

Proposition 3.1 holds under the assumption of fixed hyperparameters for the covariance function

of the GP. Therefore, in our algorithm, we update the hyperparameters only during burnin.
In the next section, we extend these results to Metropolis-adjusted Langevin method.

3 METROPOLIS-ADJUSTED LANGEVIN
The MALA takes one step (of step size ğ›¿ > 0) in the direction of the gradient from the current point

ğœ½ âˆ— := ğœ½ (ğ‘˜) +

ğ›¿Î›(cid:16)

1
2

âˆ‡ğ¿ğ¿(ğœ½ (ğ‘˜) ) + âˆ‡ log ğ‘ (ğœ½ (ğ‘˜) )

(cid:17)

+

âˆš

ğ›¿Î›z

(8)

with z âˆ¼ ğ‘ (0, ğ¼ ) and Î› is a preconditioning covariance matrix. Here
Î› denotes the matrix
square root. In this case, we assume that we can evaluate both the log-likelihood and its gradient
Iğ‘¡ = {(ğœ½ (ğ‘–), [LL(ğ· |ğœ½ (ğ‘–) ), âˆ‡ğ‘‡ LL(ğ· |ğœ½ (ğ‘–) )]) : ğ‘– = 1, . . . , ğ‘¡ }. We use a multiple-output joint GP [Solak
et al. 2002] as surrogate model for the log-likelihood and its gradient. The idea in this case is simply
to apply the previous two-stage algorithm using the proposal (8) with gradient replaced by (cid:103)âˆ‡ğ¿ğ¿.

âˆš

3210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean123210123x5432101logGP mean12Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

Ëœğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—) = 1 âˆ§

exp((cid:102)LLğ‘¡ (ğœ½ âˆ—))ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ— + 1
exp((cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) ))ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) + 1

2ğ›¿Î› (cid:103)âˆ‡ğ¿ğ¿(ğœ½ âˆ—) + 1
2ğ›¿Î› (cid:103)âˆ‡ğ¿ğ¿(ğœ½ (ğ‘˜) ) + 1

2ğ›¿Î›âˆ‡ log ğ‘ (ğœ½ âˆ—))
2ğ›¿Î›âˆ‡ log ğ‘ (ğœ½ (ğ‘˜) ))

5

(9)

where ğ‘ is the Normal proposal with covariance ğ›¿Î›. Note that, (cid:102)LLğ‘¡ (ğœ½ (ğ‘˜) )), (cid:103)âˆ‡ğ¿ğ¿(ğœ½ (ğ‘˜) ) are exact
evaluations because of Assumption 1. As before we can marginalise out (cid:102)ğ¿ğ¿, (cid:103)âˆ‡ğ¿ğ¿ computing the
expectation of ğ›¼1 w.r.t. the GP. We use the following result.

Proposition 3.1. The expectation of

2 Î›âˆ‡ğ‘“ (ğœ½ âˆ—) + ğ›¿
w.r.t. the GP, where ğ‘“ , âˆ‡ğ‘“ denote the GP distributed log-likelihood and its gradient and ğ‘ (ğœ½ âˆ—) is the
prior, is equal to

2 Î›âˆ‡ log ğ‘ (ğœ½ âˆ—))

ğ‘’ ğ‘“ (ğœ½ âˆ—)ğ‘(ğœ½ â€²|ğœ½ âˆ— + ğ›¿

(10)

(cid:35)

(cid:34) ğœ‡
Î›ğœ‡ âˆ‡

ğ‘‰ ğ‘‡

ğ‘’

2ğ‘‰ ğ‘‡ ğ¾ğ‘‰ + 1
+ 1
2

ğ›¿2
4 ğœ‡ğ‘‡
âˆ‡

Î›ğœ‡âˆ‡

(cid:20)

(cid:16)

1,

with ğ‘‰ =
for ğ‘“ , âˆ‡ğ‘“ and ğ¾ is the relative covariance matrix.

2 Î›âˆ‡ log ğ‘ (ğœ½ âˆ—) âˆ’ ğ›¿

ğœ½ â€² âˆ’ ğœ½ âˆ— âˆ’ ğ›¿

4 (Î›ğœ‡ âˆ‡)ğ‘‡ )

ğ‘(ğœ½ â€²|ğœ½ âˆ— + ğ›¿
2 Î›âˆ‡ log ğ‘ (ğœ½ âˆ—))
(cid:21)ğ‘‡

(cid:17)ğ‘‡

Î›âˆ’1 ğ›¿
2

, ğœ‡, ğœ‡ âˆ‡ are the GP predictive means

Stage 2 uses the exact evaluation of the log-likelihood and its gradient. We omit the details. The
overall algorithm is similar to the one presented previously for MH with the only difference that
the GP is multi-output over the log-likelihood and its gradient.

4 NUMERICAL EXPERIMENTS
To model the log-likelihood (and its gradient for MALA), we use a GP with Square Exponential
covariance function. A zero mean is used with the value of LL(ğœ½ (ğ‘˜) ) (and its gradient for MALA)
subtracted. This is equivalent to defining a GP with prior mean equal to LL(ğœ½ (ğ‘˜) ); in this way,
far from the data, the acceptance probability ğ›¼1 only depends on the variance of the GP. This
guarantees a high probability of acceptance in Stage 1 for samples in large-uncertainty regions.
The GP is initialised using 3 observations, before starting the two-stage sampler.

We consider five target distributions.
T1 The 2D posterior of the parameters ğ‘, ğ‘ of the banana shape distribution (true value set to

T2 The 3D posterior of the parameters ğ‘, ğ‘, ğœ of the nonlinear regression model ğ‘¦ = ğ‘ ğ‘¥

ğ‘¥+ğ‘ + ğœ–,

ğœ– âˆ¼ ğ‘ (0, ğœ 2) (true value set to ğ‘ = 0.14, ğ‘ = 50, ğœ = 0.1).

T3 The 3D posterior of the parameters â„“1, â„“2, ğœ 2 of the SE kernel for a GP-classifier.
T4 The 4D posterior of the parameters ğ›½, ğ›¾, ğœ1, ğœ2 of a Susceptible, Infected, Recovery (SIR)

ğ‘ = 0.2, ğ‘ = 2);

model.

T5 The 5D posterior of the parameters ğ›½0, . . . , ğ›½4 of a parametric logistic regression problem.
Appendix A gives further details on priors assumed for the parameters and selected proposal. Each
of these five posteriors has a specific feature, resulting in a diverse set of challenging targets, for
instance T1 is heavy tailed and T2 is heavily anisotropic. T4, the SIR problem, is a prototypical
example of the type of applications targeted by the proposed method. To compute the likelihood,
we need to solve numerically a system of ODEs and, in more complex biological and chemical
models, this can be computationally heavy.

Evaluating the likelihood in these five problems is very fast, this allows us to quickly perform
Monte Carlo simulations to assess the performance of the model by generating artificial data.

6

Alessio Benavoli, Jason Wyse, and Arthur White

T1

T3

T5

MH
GP-MH
MALA
GP-MALA
MH
GP-MH
MALA
GP-MALA
MH
GP-MH
MALA
GP-MALA

AR
0.37
0.36
0.26
0.26
0.42
0.42
0.44
0.43
0.29
0.29
0.67
0.67

ESS
90
113
73
75
137
135
133
134
98
102
339
368

ESJD
0.13
0.13
0.2
0.2
0.44
0.38
0.48
0.45
0.002
0.002
0.009
0.009

Eval%
100
41
100
35
100
42
100
45
100
35
100
68

SD
0.02
0.02
0.03
0.02
4.1
3.5
4.1
3.5
0.006
0.006
0.006
0.006

MH
GP-MH
MALA
GP-MALA
MH
GP-MH

T2

T4

AR
0.28
0.27
0.26
0.21
0.1
0.1

ESS
138
133
220
147
51
45

ESJD
32.6
31
51
29
0.003
0.003

Eval%
100
39
100
43
100
15

SD
339
339
316
255
0.009
0.009

Table 1. Comparison between the proposed GP-based samplers and standard MH and MALA in terms of
the acceptance rate (AR), Effective Sample Size (ESS), Expected Square Jumping Distance (ESJD), percentage
of likelihood evaluations in the 2000 iterations (EVAL), Square Distance (SD) between the true value of the
parameters and the estimated posterior mean. We have not run MALA for the SIR model.

We then evaluate the efficiency of the algorithms by simply counting the number of likelihood
evaluations.

We compare our two-stage algorithm with the standard implementations of MH and MALA. For
each target problem and in each simulation, we generate 2500 samples (including 500 for burnin).
We have deliberately selected a small number of samples to show that our approach converges
quickly, which is important in computationally expensive applications. We check for convergence
to the correct posterior distribution using the metrics described in the caption of Table 1.

Table 1 reports the value of the metrics averaged over the 30 simulations and over parameters.
Comparing the simulationsâ€™ results it can be noticed that the proposed GP-based samplers obtain
the same convergence metrics of the standard MH and MALA, but with a fraction of the number
of likelihood evaluations. It can also be noticed how the fraction of the number of full likelihood
evaluations required is problem dependent, ranging from 15% for T4 to 65% for T3. This demonstrates
that our approach automatically adapts to the complexity of the specific target distribution.

5 CONCLUSIONS
We have presented a two-stage Metropolis-Hastings algorithm for sampling probabilistic models,
whose log-likelihood is computationally expensive to evaluate, by using a surrogate GP model.
The key feature of the approach, and the difference w.r.t. previous works, is the ability to learn
the target distribution from scratch (while sampling), and so without the need of pre-training the
GP. This is fundamental for automatic and inference in Probabilistic Programming Languages In
particular, we have presented an alternative first stage acceptance scheme by marginalising out the
GP distributed function, which makes the acceptance ratio explicitly dependent on the variance of
the GP. This approach is extended to Metropolis-Adjusted Langevin algorithm (MALA). Numerical
experiments have demonstrated the effectiveness of the method, which can automatically adapt to
the complexity of the target distribution. In the numerical experiments, we have used a full GP
whose computational load grows cubically as the size of the training set increases. Sparse GPs can
be employed to address this issue [Bauer et al. 2016; Hensman et al. 2013; HernÃ¡ndez-Lobato and
HernÃ¡ndez-Lobato 2016; QuiÃ±onero-Candela and Rasmussen 2005; Schuerch et al. 2020; Snelson
and Ghahramani 2006; Titsias 2009] when it is necessary to sample thousands of samples.

In future work, we plan to extend the approach we used for MALA to Hamiltonian Monte Carlo.
We also intend to investigate whether tailored covariance functions for log densities or ratios of
densities can provide any convergence advantage, but also investigate surrogate models alternative
to GPs.

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

7

ACKNOWLEDGMENTS

REFERENCES
P. Alquier, N. Friel, R. Everitt, and A. Boland. 2016. Noisy Monte Carlo: Convergence of Markov Chains with Approximate
Transition Kernels. Statistics and Computing 26, 1â€“2 (Jan. 2016), 29â€“47. https://doi.org/10.1007/s11222-014-
9521-x

Christophe Andrieu and Gareth O. Roberts. 2009. The pseudo-marginal approach for efficient Monte Carlo computations.

The Annals of Statistics 37, 2 (2009), 697 â€“ 725. https://doi.org/10.1214/07-AOS574

Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. 2016. Understanding probabilistic sparse Gaussian process

approximations. In Advances in neural information processing systems. 1533â€“1541.

Nikolay Bliznyuk, David Ruppert, and Christine A Shoemaker. 2012. Local derivative-free approximation of computationally

expensive posterior densities. Journal of Computational and Graphical Statistics 21, 2 (2012), 476â€“495.

E. Brochu, V.M. Cora, and N. De Freitas. 2010. A tutorial on Bayesian optimization of expensive cost functions, with
application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599 (2010).
J. AndrÃ©s Christen and Colin Fox. 2005. Markov Chain Monte Carlo Using an Approximation. Journal of Computational and

Graphical Statistics 14, 4 (2005), 795â€“810. http://www.jstor.org/stable/27594150

Demetri Pananos. 2019. PyMC3 examples https://docs.pymc.io/notebooks/ODE_API_introduction.html.
Mark Fielding, David J Nott, and Shie-Yui Liong. 2011. Efficient MCMC schemes for computationally expensive posterior

distributions. Technometrics 53, 1 (2011), 16â€“28.

Leigh J Halliwell. 2015. The lognormal random multivariate. In Casualty Actuarial Society E-Forum, Spring, Vol. 5.
W. K. Hastings. 1970. Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57,
https://doi.org/10.1093/biomet/57.1.97 arXiv:https://academic.oup.com/biomet/article-

1 (04 1970), 97â€“109.
pdf/57/1/97/23940249/57-1-97.pdf

James Hensman, NicolÃ² Fusi, and Neil D. Lawrence. 2013. Gaussian Processes for Big Data. In Proceedings of the Twenty-Ninth

Conference on Uncertainty in Artificial Intelligence (UAIâ€™13). AUAI Press, Arlington, Virginia, USA, 282â€“290.

Daniel HernÃ¡ndez-Lobato and JosÃ© Miguel HernÃ¡ndez-Lobato. 2016. Scalable gaussian process classification via expectation

propagation. In Artificial Intelligence and Statistics. 168â€“176.

V Roshan Joseph. 2012. Bayesian computation using design of experiments-based interpolation technique. Technometrics

54, 3 (2012), 209â€“225.

Lingge Li, Andrew Holbrook, Babak Shahbaba, and Pierre Baldi. 2019. Neural network gradient hamiltonian monte carlo.

Computational statistics 34, 1 (2019), 281â€“299.

Radford M. Neal. 2011. MCMC Using Hamiltonian Dynamics. CRC Press, Chapter 5. https://doi.org/10.1201/b10905-7
Joaquin QuiÃ±onero-Candela and Carl Edward Rasmussen. 2005. A unifying view of sparse approximate Gaussian process

regression. Journal of Machine Learning Research 6, Dec (2005), 1939â€“1959.

Carl Edward Rasmussen. 2003. Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals. In

Seventh Valencia international meeting, dedicated to Dennis V. Lindley. Oxford University Press, 651â€“659.

Christian P. Robert. 2015. The Metropolisâ€“Hastings Algorithm. American Cancer Society, 1â€“15. https://doi.org/10.
1002/9781118445112.stat07834 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07834
Gareth O. Roberts and Jeffrey S. Rosenthal. 2007. Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo

Algorithms. Journal of Applied Probability 44, 2 (2007), 458â€“475. https://doi.org/10.1239/jap/1183667414

Manuel Schuerch, Dario Azzimonti, Alessio Benavoli, and Marco Zaffalon. 2020. Recursive estimation for sparse Gaussian

process regression. Automatica 120 (2020), 109â€“127.

Chris Sherlock, Andrew Golightly, and Daniel A. Henderson. 2017. Adaptive, Delayed-Acceptance MCMC for Targets With
Expensive Likelihoods. Journal of Computational and Graphical Statistics 26, 2 (2017), 434â€“444. https://doi.org/10.
1080/10618600.2016.1231064 arXiv:https://doi.org/10.1080/10618600.2016.1231064

Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian processes using pseudo-inputs. In Advances in neural

information processing systems. 1257â€“1264.

E. Solak, R. Murray-Smith, W. E. Leithead, D. J. Leith, and C. E. Rasmussen. 2002. Derivative Observations in Gaussian
Process Models of Dynamic Systems. In Proceedings of the 15th International Conference on Neural Information Processing
Systems (NIPSâ€™02). MIT Press, Cambridge, MA, USA, 1057â€“1064.

Michalis Titsias. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Proceedings of the Twelth
International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research), David van Dyk
and Max Welling (Eds.), Vol. 5. PMLR, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 567â€“574.

8

Alessio Benavoli, Jason Wyse, and Arthur White

A PRIORS FOR T1â€“T5
A zero prior is used for T1. In each simulation, we generate a different starting point in the interval
[âˆ’2, 2].

For T2, the prior is ğ‘ âˆ¼ ğ‘ (3, 1), ğ‘ âˆ¼ ğ‘ (30, 152), log ğœ âˆ¼ ğ‘ (âˆ’2, 1) and ğ‘¥ = [28, 55, 83, 110, 138, 225, 375].

In each simulation, we generate different ğ‘¦ according to the likelihood reported in the main text.
For T3, the prior is log â„“ğ‘–, log ğœ âˆ¼ ğ‘ (0, 10). The GP classification dataset includes 1000 points
with ğ‘¥1, ğ‘¥2 âˆ¼ ğ‘ (0, 1). In each simulation, the true two lengthscales are uniformly sampled from
[0.1, 1]. The true variance is fixed at ğœ 2 = 5.

For T4, we use the probabilistic model described in [Demetri Pananos 2019].
For T5, the prior is ğ›½ğ‘– âˆ¼ ğ‘ (0, 100). The GP classification dataset includes 1000 points with

ğ‘¥1, ğ‘¥2 âˆ¼ ğ‘ (0, 1). In each simulation, the true ğ›½ğ‘– are sampled from ğ‘ (0, 1).

In all cases, we have used Normal proposals for MH with diagonal covariance matrix.

B PROOFS

B.1 Proof of Proposition 2.1
This follows by the mean of the log-normal distribution.

B.2 Proof of Proposition 2.2
We prove detailed balance:

ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )+ 1

2 ğ‘˜ (ğœ½ (ğ‘˜ ) ,ğœ½ (ğ‘˜ ) |Iğ‘¡ )ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )ğ›¼ (1) (ğœ½ (ğ‘˜), ğœ½ âˆ—)

= ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )+ 1

=

=

(cid:16)
ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )+ 1

(cid:18) ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )+
ğ‘’ ğœ‡ (ğœ½ âˆ— |Iğ‘¡ )+

(cid:18)

1 âˆ§

2 ğ‘˜ (ğœ½ (ğ‘˜ ) ,ğœ½ (ğ‘˜ ) |Iğ‘¡ )ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) )

ğ‘’ ğœ‡ (ğœ½ âˆ— |Iğ‘¡ )+
1
ğ‘’ ğœ‡ (ğœ½ (ğ‘˜ ) |Iğ‘¡ )+
2 ğ‘˜ (ğœ½ (ğ‘˜ ) ,ğœ½ (ğ‘˜ ) |Iğ‘¡ )ğ‘ (ğœ½ (ğ‘˜) )ğ‘(ğœ½ âˆ—|ğœ½ (ğ‘˜) ) âˆ§ ğ‘’ ğœ‡ (ğœ½ âˆ— |Iğ‘¡ )+ 1
2 ğ‘˜ (ğœ½ (ğ‘˜ ) ,ğœ½ (ğ‘˜ ) |Iğ‘¡ ) ğ‘ (ğœ½ (ğ‘˜ ) )ğ‘ (ğœ½ âˆ— |ğœ½ (ğ‘˜ ) )
2 ğ‘˜ (ğœ½ âˆ—,ğœ½ âˆ— |Iğ‘¡ ) ğ‘ (ğœ½ âˆ—)ğ‘ (ğœ½ (ğ‘˜ ) |ğœ½ âˆ—)

ğ‘’ ğœ‡ (ğœ½ âˆ— |Iğ‘¡ )+ 1

âˆ§ 1

(cid:19)

1

1

(cid:19)

1

2 ğ‘˜ (ğœ½ âˆ—,ğœ½ âˆ— |Iğ‘¡ ) ğ‘ (ğœ½ âˆ—)ğ‘ (ğœ½ (ğ‘˜ ) |ğœ½ âˆ—)
2 ğ‘˜ (ğœ½ (ğ‘˜ ) ,ğœ½ (ğ‘˜ ) |Iğ‘¡ ) ğ‘ (ğœ½ )ğ‘ (ğœ½ âˆ— |ğœ½ (ğ‘˜ ) )
2 ğ‘˜ (ğœ½ âˆ—,ğœ½ âˆ— |Iğ‘¡ )ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ—)

(cid:17)

2 ğ‘˜ (ğœ½ âˆ—,ğœ½ âˆ— |Iğ‘¡ )ğ‘ (ğœ½ âˆ—)ğ‘(ğœ½ (ğ‘˜) |ğœ½ âˆ—).

which ends the proof. The term in brackets in the last equation is ğ›¼ (1) (ğœ½ âˆ—, ğœ½ (ğ‘˜) ). Because of
Assumption 1, we have that ğ‘˜ (ğœ½ (ğ‘˜), ğœ½ âˆ—|Iğ‘¡ ) = 0 (independent). This is the reason we can assume
work on the numerator and denominator independently.

B.3 Proof of Proposition 2.3
Let ğœ½ â€² denote the new point in Iğ‘¡ âˆ’1 and with ğ‘… all the points in Iğ‘¡ âˆ’1, by definition of Kernel matrix:
(cid:21)

(cid:21)

ğ¾ğ‘¡ =

(cid:20) ğ¾ğ‘¡ âˆ’1
ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)
ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)âŠ¤ ğ‘˜ (ğœ½ â€², ğœ½ â€²)

:=

(cid:20) ğ‘˜ (ğ‘…, ğ‘…)
ğ‘˜ (ğœ½ â€², ğ‘…)
ğ‘˜ (ğœ½ â€², ğ‘…)âŠ¤ ğ‘˜ (ğœ½ â€², ğœ½ â€²)

.

(11)

The predicted variance at step ğ‘¡ at the point ğœ½ âˆ— is

ğ‘˜ (ğœ½ âˆ—, ğœ½ âˆ—) âˆ’ ğ‘˜ğ‘¡ (ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ ğ‘˜ğ‘¡ (ğœ½ âˆ—),

while the predicted variance at step ğ‘¡ âˆ’ 1 at the point ğœ½ âˆ— is

ğ‘˜ (ğœ½ âˆ—, ğœ½ âˆ—) âˆ’ ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)

Therefore, we need to prove that

ğ‘˜ğ‘¡ (ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ ğ‘˜ğ‘¡ (ğœ½ âˆ—) > ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—).

Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect

The inverse of ğ¾ğ‘¡ is:

ğ¾ âˆ’1

ğ‘¡ =

(cid:20)ğ¾ âˆ’1

ğ‘¡ âˆ’1 + ğ¾ âˆ’1

with ğ‘€ = (ğ‘˜ (ğœ½ â€², ğœ½ â€²) âˆ’ ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)âŠ¤ğ¾ âˆ’1
ğ‘¡ ğ‘˜ğ‘¡ (ğœ½ âˆ—)

ğ‘˜ğ‘¡ (ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)ğ‘€ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)âŠ¤ğ¾ âˆ’1
âˆ’ğ‘€ğ‘˜ğ‘¡ âˆ’1(ğœ½ )âŠ¤ğ¾ âˆ’1
ğ‘¡ âˆ’1
ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²))âˆ’1. Now note that

ğ‘¡ âˆ’1 âˆ’ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘›âˆ’1(ğœ½ )ğ‘€
ğ‘€

9

(12)

(cid:21)

,

= (cid:2)ğ‘˜ âŠ¤

ğ‘¡ âˆ’1 (ğœ½ âˆ—), ğ‘˜ â€²(ğœ½ âˆ—)(cid:3)

(cid:20)ğ¾ âˆ’1

ğ‘¡ âˆ’1 + ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)ğ‘€ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)âŠ¤ğ¾ âˆ’1
âˆ’ğ‘€ğ‘˜ğ‘¡ âˆ’1(ğœ½ )âŠ¤ğ¾ âˆ’1
ğ‘¡ âˆ’1

ğ‘¡ âˆ’1 âˆ’ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘›âˆ’1(ğœ½ )ğ‘€
ğ‘€

(cid:21)

(cid:21) (cid:20)ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)
ğ‘˜ â€²(ğœ½ âˆ—)

which is equal to:
ğ‘˜ğ‘¡ (ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ ğ‘˜ğ‘¡ (ğœ½ âˆ—) = ğ‘˜ âŠ¤

ğ‘¡ âˆ’1 (ğœ½ âˆ—)ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—) + ğ‘˜ âŠ¤

ğ‘¡ âˆ’1(ğœ½ âˆ—)ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)ğ‘€ğ‘˜ğ‘¡ âˆ’1 (ğœ½ â€²)âŠ¤ğ¾ âˆ’1

âˆ’ ğ‘˜ â€²(ğœ½ âˆ—)ğ‘€ğ‘˜ğ‘¡ âˆ’1 (ğœ½ â€²)âŠ¤ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—) âˆ’ ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—)
ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²)ğ‘€ğ‘˜ â€²(ğœ½ âˆ—) + ğ‘˜ â€²(ğœ½ âˆ—)ğ‘€ğ‘˜ â€²(ğœ½ âˆ—).

where ğ‘˜ â€²(ğœ½ âˆ—) := ğ‘˜ (ğœ½ âˆ—, ğœ½ â€²). Therefore, we have that

ğ‘˜ğ‘¡ (ğœ½ âˆ—)âŠ¤ğ¾ âˆ’1

ğ‘¡ ğ‘˜ğ‘¡ (ğœ½ âˆ—) = ğ‘˜ âŠ¤

ğ‘¡ âˆ’1(ğœ½ âˆ—)ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ âˆ—) + ğ‘ .

with

ğ‘ = ğ‘€ (ğ‘˜ âŠ¤

ğ‘¡ âˆ’1(ğœ½ âˆ—)ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²) âˆ’ ğ‘˜ â€²(ğœ½ âˆ—))2 = ğ‘€ (ğ‘˜ âŠ¤

ğ‘¡ âˆ’1(ğœ½ âˆ—)ğ¾ âˆ’1

ğ‘¡ âˆ’1ğ‘˜ğ‘¡ âˆ’1(ğœ½ â€²) âˆ’ ğ‘˜ (ğœ½ âˆ—, ğœ½ â€²))2

which is strictly greater than zero whenever ğœ½ âˆ—, ğœ½ â€² âˆ‰ Iğ‘¡ âˆ’1. Note in fact that, under Assumption 1,
ğ‘ = 0 only if either ğœ½ âˆ— âˆˆ Iğ‘¡ âˆ’1 or ğœ½ â€² âˆˆ Iğ‘¡ âˆ’1 (exact interpolation property). If the proposal distribution
is absolutely continuous w.r.t. the Lebesgue measure on Î˜, then ğœ½ âˆ—, ğœ½ â€² âˆ‰ Iğ‘¡ âˆ’1 holds with probability
1.

B.4 Proof of Proposition 3.1
We work in log-scale and omit the dependence of ğ‘“ , âˆ‡ğ‘“ on ğœ½ âˆ— for notation simplification and so we
can rewrite the product of the two PDF in (10):

(cid:18)
ğœ½ â€² âˆ’ ğœ½ âˆ— âˆ’

ğ›¿
2

Î›âˆ‡ log ğ‘ (ğœ½ âˆ—)

(cid:19)ğ‘‡

Î›âˆ’1(Â· Â· Â· ) +

2
2

(cid:18)
ğœ½ â€² âˆ’ ğœ½ âˆ— âˆ’

Î›âˆ‡ log ğ‘ (ğœ½ âˆ—)

(cid:19)ğ‘‡

Î›âˆ’1(

ğ›¿
2

Î›âˆ‡ğ‘“ )

(cid:19)ğ‘‡

Î›âˆ‡ğ‘“

Î›âˆ’1(Â· Â· Â· ) âˆ’

1
2

(cid:2)ğ‘“ âˆ’ ğœ‡, âˆ‡ğ‘‡ ğ‘“ âˆ’ ğœ‡ğ‘‡
âˆ‡

(cid:3)

(cid:20) ğ´ ğµ
ğµğ‘‡ ğ¶

ğ›¿
2
(cid:21) (cid:20) ğ‘“ âˆ’ ğœ‡
âˆ‡ğ‘“ âˆ’ ğœ‡ âˆ‡

(cid:21)

where Î› is the covariance matrix of the proposal, and the last term in the above sum is the GP
predictive posterior with mean [ğœ‡, ğœ‡ âˆ‡] and covariance ğ¾. We have expressed ğ¾ âˆ’1 as the block
matrix with blacks ğ´, ğµ, ğ¶, ğ·. We can rewrite the above sum as

(cid:18)
ğœ½ â€² âˆ’ ğœ½ âˆ— âˆ’

ğ›¿
2

Î›âˆ‡ log ğ‘ (ğœ½ âˆ—)

(cid:19)ğ‘‡

Î›âˆ’1(Â· Â· Â· ) +

2
2

(cid:18)
ğœ½ â€² âˆ’ ğœ½ âˆ— âˆ’

ğ›¿
2

Î›âˆ‡ log ğ‘ (ğœ½ âˆ—)

(cid:19)ğ‘‡

Î›âˆ’1(

ğ›¿
2

Î›âˆ‡ğ‘“ )

ğœ‡ğ‘‡
âˆ‡

Î›âˆ‡ğ‘“ +

1
2

ğ›¿ 2
4

ğœ‡ğ‘‡
âˆ‡

Î›ğœ‡ âˆ‡ âˆ’

1
2

(cid:2)ğ‘“ âˆ’ ğœ‡, âˆ‡ğ‘‡ ğ‘“ âˆ’ ğœ‡ğ‘‡
âˆ‡

(cid:3)

(cid:20) ğ´
ğµğ‘‡

ğµ
ğ›¿ 2
4 Î› + ğ¶

(cid:21)

(cid:21) (cid:20) ğ‘“ âˆ’ ğœ‡
âˆ‡ğ‘“ âˆ’ ğœ‡ âˆ‡

We can now define the vector ğ‘‰ and prove the result using the moments of the multivariate
Lognormal distribution [Halliwell 2015].

1
2
(cid:18) ğ›¿
2

ğ‘“ âˆ’

âˆ’

1
2

1
2
ğ›¿ 2
4

ğ‘“ âˆ’

âˆ’

2
2

