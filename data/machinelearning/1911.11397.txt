2
2
0
2

r
p
A
8

]

Y
S
.
s
s
e
e
[

3
v
7
9
3
1
1
.
1
1
9
1
:
v
i
X
r
a

Adaptive dynamic programming for nonaï¬ƒne nonlinear optimal
control problem with state constraints

Jingliang Duana, Zhengyu Liua, Shengbo Eben Lia,âˆ—, Qi Suna, Zhenzhong Jiab and Bo Chenga

aState Key Lab of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, 100084, China.
bRobotics Institute at Carnegie Mellon University, Pittsburgh, PA 15213, USA.

A R T I C L E I N F O

A B S T R A C T

Keywords:
Adaptive dynamic programming
Optimal control
State constraint
Reinforcement learning

This paper presents a constrained adaptive dynamic programming (CADP) algorithm to solve general
nonlinear nonaï¬ƒne optimal control problems with known dynamics. Unlike previous ADP algo-
rithms, it can directly deal with problems with state constraints. Firstly, a constrained generalized
policy iteration (CGPI) framework is developed to handle state constraints by transforming the tradi-
tional policy improvement process into a constrained policy optimization problem. Next, we propose
an actor-critic variant of CGPI, called CADP, in which both policy and value functions are approx-
imated by multi-layer neural networks to directly map the system states to control inputs and value
function, respectively. CADP linearizes the constrained optimization problem locally into a quadrat-
ically constrained linear programming problem, and then obtains the optimal update of the policy
network by solving its dual problem. A trust region constraint is added to prevent excessive policy
update, thus ensuring linearization accuracy. We determine the feasibility of the policy optimization
problem by calculating the minimum trust region boundary and update the policy using two recovery
rules when infeasible. The vehicle control problem in the path-tracking task is used to demonstrate
the eï¬€ectiveness of this proposed method.

1. Introduction

Dynamic programming (DP) is a theoretical and eï¬€ec-
tive tool in solving discrete-time (DT) optimal control prob-
lems with known dynamics [1]. The optimal value function
(or cost-to-go) for DT systems is obtained by solving the
DT Hamilton-Jacobi-Bellman (HJB) equation, also known
as the Bellman optimality equation, which develops back-
ward in time [2]. However, due to the curse of dimension-
ality, running DP directly to get the optimal solution of DT
HJB is usually computationally untenable for complex non-
linear DT systems [3]. The adaptive dynamic programming
(ADP) methods were ï¬rst proposed by Werbos as a way to
overcome this diï¬ƒculty by solving an approximate solution
of DT HJB forward in time [4, 5]. In some studies, ADP is
also called approximate dynamic programming [6, 7].

ADP methods are usually implemented as an actor-critic
architecture which involves a critic parameterized function
for value function approximation and an actor parameter-
ized function for policy approximation [8, 7, 9, 10]. Neu-
ral networks (NNs) have been widely used as approximators
of both value function and policy due to their strong ï¬tting
ability, and have achieved good performance on many con-
trol tasks [7]. Most ADP methods adopt generalized policy
iteration (GPI) as a primary tool to adjust both value and pol-
icy networks by iteratively solving the DT HJB equation [7].
The well-known policy iteration and value iteration methods
can also be regarded as special cases of GPI [7, 11]. There
are two revolving iteration procedures for GPI framework:

âˆ—Corresponding author

duanjl15@163.com (J. Duan); liuzheng17@mails.tsinghua.edu.cn (Z.

Liu); lishbo@tsinghua.edu.cn (S.E. Li); qisun@tsinghua.edu.cn (Q. Sun);
zhenzhong.jia@gmail.com (Z. Jia); chengbo@tsinghua.edu.cn (B. Cheng)

ORCID(s): 0000-0003-4923-3633 (S.E. Li)

1) policy evaluation, which drives the value function towards
the true value function for the current policy, and 2) policy
improvement, which improves the policy to reduce the cur-
rent value function.

Over the last few decades, many ADP methods of ï¬nd-
ing the nearly optimal control solution for DT systems with
known dynamics have emerged. Chen and Jagannathan pro-
posed an ADP method to ï¬nd nearly optimal control state
feedback laws for input-aï¬ƒne nonlinear DT systems by it-
eratively solving the generalized HJB equation. The value
function was approximated by a linear combination of artiï¬-
cially designed basis functions, while the policy was directly
derived from the value function [12]. Both actor and critic
NNs were utilized by Al-Tamimi et al. to develop a value-
iteration-based algorithm for DT systems, and it was shown
that the algorithm could converge to the optimal value func-
tion and policy as long as the control coeï¬ƒcient matrix was
known [13]. Liu et al. proposed a GPI algorithm for DT
nonlinear systems, and the admissibility property of the pol-
icy network during learning could be guaranteed as long as
the initialized policy was admissible [14].

ADP is usually considered together with reinforcement
learning (RL) because both provide approximate solutions to
DP [11]. Compared with ADP, RL focuses more on solving
the nearly optimal policy of high-dimensional systems with
completely unknown models, such as Atari games [15] and
StarCraft [16]. In recent years, RL algorithms such as DSAC
(Distributional Soft Actor-Critic) [17, 18], DDPG (Deep De-
terministic Policy Gradient) [19], A3C (Asynchronous Ad-
vantage Actor-Critic) [20], SAC (Soft Actor-Critic) [21, 22],
TRPO (Trust Region Policy Optimization) [23], and PPO
(Proximal Policy Optimization) [24], have also been widely
used to solve DT optimal control problems.

It should be pointed out that the state constraints were

J. Duan et al.: Preprint submitted to Elsevier

Page 1 of 15

 
 
 
 
 
 
Preprint

not considered in these ADP techniques mentioned above.
For practical applications, however, most controlled systems
must be subject to some state restrictions. Taking vehicle
control in the path-tracking task as an example, in addition
to considering the tracking performance, certain state func-
tions of the vehicle must be constrained to the stability zone
to prevent vehicle instability problems [25]. Compared with
state constraints, input constraints can be easily confronted
by introducing a nonquadratic cost function or directly con-
straining the output range of actor NN using some saturated
functions, such as hyperbolic tangent function [26, 27, 28].
To cope with the state constraints, most related ADP re-
searches choose to transform the original problem into an
unconstrained one by constructing additional system states
or adding the state constraints to the objective function as
a penalty [29, 30, 31, 32]. However, the optimality of the
original system may not be guaranteed since the controller
of the transformed system has to spend extra eï¬€orts to ensure
the satisfaction of the state constraints, which will certainly
enlarge the cost function [29]. Also, the trade-oï¬€ between
performance and state constraints may lead to constraint vi-
olations in some cases. Model predictive control (MPC) is a
commonly used control method to solve control input online
while satisfying a set of constraints [33]. However, com-
pared with ADP, complex systems such as non input-aï¬ƒne
and nonlinear models are still big challenges for MPC. In
addition, Achiam et al. proposed a model-free RL algo-
rithm, constrained policy optimization (CPO), to maximize
rewards while enforcing constraints [34]. But the constraint
satisfaction of CPO, or its variant parallel CPO [35], cannot
ensure the policy safety, because its approximated constraint
function is an expected and cumulative value.

Besides, existing ADP algorithms usually rely on hand-
crafted features combined with linear value functions or pol-
icy representations, also called single NN in [36, 37]. Ob-
viously, the performance of these algorithms heavily relies
on the quality of the feature representation. However, it is
usually hard to design such features for high-dimensional
nonlinear systems. Compared with single NNs, NNs with
multiple hidden layers or deep NNs have the ï¬tting ability
to directly map the system states to optimal control inputs
or value function without reliance on hand-crafted features
[38]. In addition, theoretical analysis and experimental re-
sults show that the multi-layer NN can usually converge to
the nearly global minimum if it is over-parameterized [39,
40]. This is why deep RL has outperformed traditional RL
in many challenging domains, from games to robotic control
[11]. Hence, in this paper, multi-layer NNs are employed to
relax the need for hand-crafted features.

In this paper, we propose a new ADP algorithm, called
constrained ADP (CADP), to solve optimal control problems
with state constraints, which is applicable to general non-
linear systems with nonaï¬ƒne saturated control inputs. The
main contributions and advantages of this paper are summa-
rized as follows:

1. A constrained generalized policy iteration (CGPI) frame-
work is developed to handle state constraints, in which

the traditional policy improvement process is trans-
formed into a constrained policy optimization prob-
lem. Compared with most existing ADP algorithms
[12, 13, 14] that use the traditional policy improve-
ment, the proposed CADP method building on CGPI
is applicable to optimal control problems with state
constraints.

2. For the approximated policy with high-dimensional
parameters, directly solving the constrained policy op-
timization problem may be intractable due to the com-
putational cost and the nonlinear characteristics of dy-
namics and the policy function. Therefore, some ex-
isting ADP researches consider state constraints by
transforming the original problem into an unconstrained
one [29, 30, 31, 32]. However, the optimality may not
be guaranteed since the controller of the transformed
system has to spend extra eï¬€orts to ensure the satisfac-
tion of the state constraints, which will enlarge the cost
function [29]. The proposed CADP algorithm deals
with this situation by linearizing the constrained op-
timization problem locally into a quadratically con-
strained linear programming problem, and then ob-
tains the optimal update of the policy function by solv-
ing its dual problem. Meanwhile, a trust region con-
straint is added to prevent excessive policy update, thus
ensuring linearization accuracy. Besides, two recov-
ery rules are proposed to update the policy in case that
the primal problem is infeasible.

3. The proposed CADP algorithm employs the actor-critic
architecture to approximate both policy and value func-
tions by multi-layer NNs. According to the univer-
sal ï¬tting ability and global convergence property of
multi-layer NNs [39, 40], the value and policy net-
works of CADP directly map the system states to con-
trol inputs and value function, respectively. Therefore,
compared with ADP researches [12, 29, 36, 37, 41]
that use single NN, CADP relaxes the need for hand-
crafted features. Besides, diï¬€erent from [12, 29, 30,
31, 32, 36, 37, 41, 42] which are subject to input-aï¬ƒne
systems since the policy needs to be analytically rep-
resented by the value function, CADP is applicable to
arbitrary nonlinear nonaï¬ƒne dynamics by optimizing
the independent policy network.

The paper is organized as follows. In Section 2, we pro-
vide the formulation of the DT optimal control problem, fol-
lowed by the general description of GPI framework. Section
3 presents the constrained ADP algorithm. In Section 4, we
present a simulation example that shows the eï¬€ectiveness of
the CADP algorithm for DT system. Section 5 concludes
this paper.

2. Mathematical preliminaries
2.1. Notation

For ease of presentation, we summarize the abbrevia-
tions and mathematical notations in Table 1 and Table 2,
respectively.

J. Duan et al.: Preprint submitted to Elsevier

Page 2 of 15

Preprint

Table 1
Abbreviations

Abbreviation
ADP
CADP
CGPI
DT
GPI
HJB
NN
PI

Explanation
Adaptive dynamic programming
Constrained ADP
Constrained generalized policy iteration
Discrete-time
Generalized policy iteration
Hamilton-Jacobi-Bellman
Neural network
Policy iteration

Table 2
Mathematical Notations

Symbol
â„
â„•, â„•+
â„ğ‘›
ğ‘˜
ğ‘¥ âˆˆ â„ğ‘›, ğ‘¢ âˆˆ â„ğ‘š
ğ‘“ âˆ¶ â„ğ‘› Ã— â„ğ‘š â†’ â„ğ‘›
ğ›¾ âˆˆ (0, 1)
ğœ‹ âˆ¶ â„ğ‘› â†’ â„ğ‘š
ğ‘‰ âˆ¶ â„ğ‘› â†’ â„
ğœƒ âˆˆ â„ğ‘ , ğœ” âˆˆ â„ğ‘
ğ¾
âˆ‡ğ‘¥ğ¹ (â‹…)
â€– â‹… â€–2

Explanation
the set of real numbers
the set of natural numbers
ğ‘›-dimensional Euclidean space
time step index
state and control input vectors
system function
discount factor
policy function
value function
parameters vector of ğœ‹ and ğ‘‰
iteration index
the gradient of ğ¹ (â‹…) w.r.t. ğ‘¥
Euclidean norm

2.2. Discrete-time Hamilton-Jacobi-Bellman

Equation

Consider the discrete-time (DT) general time-invariant

dynamical system

ğ‘¥ğ‘˜+1 = ğ‘“ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜),

(1)

where ğ‘¥ğ‘˜ âˆˆ â„ğ‘› and ğ‘¢ğ‘˜ âˆˆ â„ğ‘š are the state vector and control
input vector at time ğ‘˜, respectively, and ğ‘“ âˆ¶ â„ğ‘› Ã— â„ğ‘š â†’ â„ğ‘›
is the system function. We assume that ğ‘“ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜) is Lipschitz
continuous on a compact set Î© that contains the origin, and
that the system is stabilizable on Î©, i.e., there exists a contin-
uous policy ğœ‹(ğ‘¥), where ğ‘¢ğ‘˜ = ğœ‹(ğ‘¥ğ‘˜), such that the system is
asymptotically stable on Î©. The system dynamics ğ‘“ (ğ‘¥ğ‘˜, ğ‘¢ğ‘˜)
is assumed to be known, which can be a nonlinear and in-
put nonaï¬ƒne analytic function only if ğœ•ğ‘“ (ğ‘¥,ğ‘¢)
are
available. The system input ğ‘¢ can be either constrained or
unconstrained. Given the policy ğœ‹(ğ‘¥ğ‘˜), we deï¬ne its associ-
ated inï¬nite-horizon value function as
âˆ
âˆ‘

and ğœ•ğ‘“ (ğ‘¥,ğ‘¢)

ğœ•ğ‘¥

ğœ•ğ‘¢

ğ‘‰ ğœ‹(ğ‘¥ğ‘˜) =

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–)),

(2)

ğ‘–=0

where ğ‘™(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜) âˆ¶ â„ğ‘› Ã— â„ğ‘š â†’ â„ is the utility function, and
ğ›¾ âˆˆ (0, 1) is the discount factor.

Furthermore, denoting the prediction horizon as ğ‘, (2)

can be rewritten as

ğ‘‰ ğœ‹(ğ‘¥ğ‘˜) =

ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–)) + ğ›¾ ğ‘ ğ‘‰ ğœ‹(ğ‘¥ğ‘˜+ğ‘ ), (3)

J. Duan et al.: Preprint submitted to Elsevier

which is the well-known Bellman equation. Then the op-
timal control problem can now be formulated as ï¬nding a
policy such that the value function associated with the sys-
tem in (1) is minimized for âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. The minimized value
function ğ‘‰ âˆ—(ğ‘¥ğ‘˜) deï¬ned by

ğ‘‰ âˆ—(ğ‘¥ğ‘˜) = min
ğœ‹

ğ‘‰ ğœ‹(ğ‘¥ğ‘˜)

(4)

satisï¬es the DT Hamilton-Jacobi-Bellman (HJB) equation or
Bellman optimality equation

ğ‘‰ âˆ—(ğ‘¥ğ‘˜) = min
ğœ‹

{ ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–)) + ğ›¾ ğ‘ ğ‘‰ âˆ—(ğ‘¥ğ‘˜+ğ‘ )

}

.

(5)

Meanwhile, the optimal control ğœ‹âˆ—(ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© can be
derived as

ğœ‹âˆ—(ğ‘¥ğ‘˜) = arg min
ğœ‹

{ ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–))+ğ›¾ ğ‘ ğ‘‰ âˆ—(ğ‘¥ğ‘˜+ğ‘ )

}
.

(6)

To ï¬nd the optimal control solution for the problem, one
only needs to solve (5) for the value function and then substi-
tute the solution into (6) to obtain the optimal control. How-
ever, due to the nonlinear nature of DT HJB, ï¬nding its so-
lution is generally diï¬ƒcult or intractable.

2.3. Adaptive Dynamic Programming

The proposed algorithm for DT optimal control prob-
lems used in this paper is motivated by generalized policy
iteration (GPI) techniques [11]. GPI is an iterative method
widely used in adaptive dynamic programming (ADP) and
reinforcement learning (RL) algorithms to ï¬nd the approxi-
mate solution of DT HJB. The ADP algorithms building on
GPI usually employ actor-critic architecture to approximate
both the value function and policy. In this study, both the
value function and policy are approximated by multi-layer
NNs, called value network (or critic network) ğ‘‰ (ğ‘¥; ğœ”) and
policy network (or actor network) ğœ‹(ğ‘¥; ğœƒ), where ğœ” âˆˆ â„ğ‘
and ğœƒ âˆˆ â„ğ‘  are network parameters. These two networks
directly build the map from the raw system states to the ap-
proximated value function and control inputs, respectively.
In this case, no hand-crafted features or basis functions are
needed.

GPI involves two interacting processes: 1) policy evalu-
ation, which drives the estimated value function towards the
true value function for current policy based on (3), and 2)
policy improvement, which improves the policy with respect
to current estimated value function based on (6).

Deï¬ning the accumulated future cost of state ğ‘¥ğ‘˜ under

policy ğœ‹ğœƒ and value ğ‘‰ğœ” as

ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”) =

ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ))+ğ›¾ ğ‘ ğ‘‰ (ğ‘¥ğ‘˜+ğ‘ ; ğœ”),

(7)

Page 3 of 15

Preprint

the policy evaluation process of GPI proceeds by iteratively
minimizing the following loss function:

ğ¿(ğœ”) = ğ”¼

ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

{ 1
2

(ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”) âˆ’ ğ‘‰ (ğ‘¥ğ‘˜; ğœ”)

)2}
,

(8)

where ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”) âˆ’ ğ‘‰ (ğ‘¥ğ‘˜; ğœ”) is usually called temporal
diï¬€erence (TD) error and ğ‘‘ğ‘¥ denotes the state distribution
over ğ‘¥ âˆˆ Î©. Under the assumption of the universal approxi-
mation theorem of NNs [43], ğ‘‘ğ‘¥ can be an arbitrary distribu-
tion as long as the probability density ğ‘(ğ‘¥) > 0 for âˆ€ğ‘¥ âˆˆ Î©,
such as uniform distribution. Therefore, the update gradient
for the value network is given by

Algorithm 1 GPI Framework

Initial with arbitrary ğœƒ0, ğœ”0, learning rates ğ›¼ğ‘ and ğ›¼ğ‘
Initialize iteration index ğ¾ = 0
repeat

Rollout ğ‘ steps from ğ‘¥ğ‘˜ âˆˆ Î© with policy ğœ‹ğœƒğ¾
Receive and store ğ‘¥ğ‘˜+ğ‘–, ğ‘– âˆˆ [1, ğ‘]
Policy evaluation:
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒğ¾
Update value function using

), âˆ‡ğœ”ğ¾

, ğ‘‰ğœ”ğ¾

ğ¿(ğœ”ğ¾ ) using (7), (9)

ğœ”ğ¾+1 = âˆ’ğ›¼ğ‘âˆ‡ğœ”ğ¾

ğ¿(ğœ”ğ¾ ) + ğœ”ğ¾

(12)

âˆ‡ğœ”ğ¿(ğœ”) = ğ”¼

ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

{(ğ‘‰ (ğ‘¥ğ‘˜; ğœ”)âˆ’ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”)

)

âˆ‡ğœ”ğ‘‰ (ğ‘¥ğ‘˜; ğœ”)

}
.

Policy improvement:
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒğ¾

, ğ‘‰ğœ”ğ¾+1

), âˆ‡ğœƒğ¾

ğ½ (ğœƒğ¾ ) using (7), (11)

(9)

Update policy using

In the policy improvement step, the parameters ğœƒ of the
policy network are updated to minimize the objective func-
tion

ğ½ (ğœƒ) = ğ”¼

ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

{

ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”)

}
.

Denoting the matrix

ğœ•ğ‘¢ğ‘˜+ğ‘–
ğœ•ğœƒ
as ğœ“ğ‘–, the update gradient for the policy network is

âˆˆ â„ğ‘ Ã—ğ‘› as ğœ™ğ‘–,

ğœ•ğ‘¥ğ‘˜+ğ‘–
ğœ•ğœƒ

(10)

âˆˆ â„ğ‘ Ã—ğ‘š

âˆ‡ğœƒğ½ (ğœƒ)

= ğ”¼

ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

= ğ”¼

ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

{ ğ‘âˆ’1
âˆ‘

ğ‘–=0
{ ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–âˆ‡ğœƒğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğ‘¢ğ‘˜+ğ‘–) + ğ›¾ ğ‘ âˆ‡ğœƒğ‘‰ (ğ‘¥ğ‘˜+ğ‘ ; ğœ”)

}

ğ›¾ ğ‘–[

ğœ™ğ‘–

ğœ•ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğ‘¢ğ‘˜+ğ‘–)
ğœ•ğ‘¥ğ‘˜+ğ‘–

+ ğœ“ğ‘–

]

ğœ•ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğ‘¢ğ‘˜+ğ‘–)
ğœ•ğ‘¢ğ‘˜+ğ‘–
ğœ•ğ‘‰ (ğ‘¥ğ‘˜+ğ‘ ; ğœ”)
ğœ•ğ‘¥ğ‘˜+ğ‘

}
,

+ ğ›¾ ğ‘ ğœ™ğ‘

(11)

where

ğœ™ğ‘– = ğœ™ğ‘–âˆ’1

ğœ•ğ‘“ (ğ‘¥ğ‘˜+ğ‘–âˆ’1, ğ‘¢ğ‘˜+ğ‘–âˆ’1)
ğœ•ğ‘¥ğ‘˜+ğ‘–âˆ’1

+ ğœ“ğ‘–âˆ’1

ğœ•ğ‘“ (ğ‘¥ğ‘˜+ğ‘–âˆ’1, ğ‘¢ğ‘˜+ğ‘–âˆ’1)
ğœ•ğ‘¢ğ‘˜+ğ‘–âˆ’1

,

with ğœ™0 = 0, and

ğœ“ğ‘– = ğœ™ğ‘–

ğœ•ğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ)
ğœ•ğ‘¥ğ‘˜+ğ‘–

+ âˆ‡ğœƒğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ).

In practice, âˆ‡ğœ”ğ¿(ğœ”) and âˆ‡ğœƒğ½ (ğœƒ) are usually approxi-
mated by the sample average. Any oï¬€-the-shelf NN opti-
mization methods can be used to update the value and pol-
icy networks, including stochastic gradient descent (SGD),
RMSProp, Adam [44]. Taking the SGD method as an ex-
ample, the pseudo-code of GPI can be summarized as Al-
gorithm 1. Algorithm 1 will iteratively converge to the op-
timal control policy ğœ‹(ğ‘¥ğ‘˜; ğœƒâˆ—) = ğœ‹âˆ—(ğ‘¥ğ‘˜) and value function
ğ‘‰ (ğ‘¥ğ‘˜; ğœ”âˆ—) = ğ‘‰ âˆ—(ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. Proofs of convergence
and optimality have been given in [7, 11].

ğœƒğ¾+1 = âˆ’ğ›¼ğ‘âˆ‡ğœƒğ¾

ğ½ (ğœƒğ¾ ) + ğœƒğ¾

(13)

ğ¾ = ğ¾ + 1
until Convergence

3. Constrained ADP
3.1. Constrained Policy Improvement

One drawback of the policy update rule in (13) is that it
is not suitable for optimal control problems with state con-
straints. However, for practical applications, most controlled
systems must be subject to some state restrictions, such as
vehicles [25], wave energy converters [31] and robots [45].
Although the state constraints can be added to the objec-
tive function as a penalty, it is often diï¬ƒcult to balance the
constraint requirements with the control objectives. The op-
timality may not be guaranteed since the controller has to
spend extra eï¬€orts to ensure the satisfaction of the state con-
straints. Besides, the trade-oï¬€ between performance and
state constraints may lead to constraint violations in some
cases.

In this paper, the state constraints of future ğ‘ steps are
introduced to transfer the policy improvement process into a
constrained optimization problem. Assuming there are ğœmax
kinds of state constraints, the ğœth state constraint can be for-
mulated as:

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1],

(ğ‘¥ğ‘˜+ğ‘–+1) â‰¤ ğ‘ğœ ,
ğ½ğ¶ğœ
(ğ‘¥ğ‘˜+ğ‘–+1) âˆ¶ â„ğ‘› â†’ â„ is the ğœth state function
where ğ½ğ¶ğœ
bounded above by boundary ğ‘ğœ . Therefore, the policy im-
provement process can be transformed into the following
constrained optimization problem:

(14)

ğœƒğ¾+1 = arg min
ğœƒ

ğ½ (ğœƒ)

ğ‘ .ğ‘¡. ğ‘¥ğ‘˜+ğ‘–+1 = ğ‘“ (ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ)),
(ğ‘¥ğ‘˜+ğ‘–+1) â‰¤ ğ‘ğœ ,

ğ½ğ¶ğœ

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1],
ğœ âˆˆ [1, ğœmax].

(15)

There are a total of ğ‘€ = ğ‘ Ã— ğœmax state constraints in (15).
In this paper, we refer to (15) as the constrained policy im-

J. Duan et al.: Preprint submitted to Elsevier

Page 4 of 15

Preprint

provement. Then we develop the constrained generalized
policy iteration (CGPI) framework, which builds on GPI by
replacing the policy improvement process in Algorithm 1
with (15). The pseudo-code of CGPI is shown in Algorithm
2.

Algorithm 2 CGPI Framework

Initial with arbitrary ğœƒ0, ğœ”0, learning rates ğ›¼ğ‘
Initialize iteration index ğ¾ = 0
repeat

, ğ‘‰ğœ”ğ¾

Rollout ğ‘ steps from ğ‘¥ğ‘˜ âˆˆ Î© with policy ğœ‹ğœƒğ¾
Receive and store ğ‘¥ğ‘˜+ğ‘–, ğ‘– âˆˆ [1, ğ‘]
Policy evaluation:
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒğ¾
), âˆ‡ğœ”ğ¾
Update value function using (12)
Constrained policy improvement:
Construct ğ½ (ğœƒ) under ğ‘‰ğœ”ğ¾+1
Update policy using (15)
ğ¾ = ğ¾ + 1
until Convergence

using (10)

ğ¿(ğœ”ğ¾ ) using (7), (9)

In Appendix A.1, we prove the convergence and global
optimality of a policy iteration variant of Algorithm 2 based
on tabular setting. Besides, as described in Appendix A.2
and A.3, the convergence results can be further extended to
the case of function approximation and Algorithm 2.

Remark 1. The state constraints need to be reasonable to
ensure (15) is feasible. For practical applications, the state
constraints usually come from the physical limitations or bound-
aries of controlled systems [25, 31, 45]. Taking vehicle con-
trol in the path-tracking task as an example, in addition to
considering the tracking performance, certain state func-
tions of the vehicle must be constrained to the stability zone
to prevent vehicle instability problems [25].

3.2. Approximate Solution

For policies with high-dimensional parameter spaces ğœƒ âˆˆ
â„ğ‘ , directly solving (15) may be intractable due to the com-
putational cost and the nonlinear characteristics of NNs and
dynamics. Local linearization is an eï¬€ective trick to deal
with this situation [23, 34, 35]. Firstly, we can linearize
the objective function and state constraints at the ğ¾th itera-
tion around current policy ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾ ) using Taylorâ€™s expan-
sion theorem. To ensure the approximation accuracy, policy
ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾+1) must be in a small neighborhood of ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾ ).
This means that we need to add a policy constraint to (15) to
avoid excessive policy update.

Inspired by [23], one eï¬€ective way to limit the policy
change is to constrain the diï¬€erence between the new policy
ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾+1) and the old policy ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾ ). Firstly, we deï¬ne
the following function

ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) â‰ ğ”¼ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

[â€–ğœ‹(ğ‘¥ğ‘˜; ğœƒ) âˆ’ ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾ )â€–

2
2]

to measure the diï¬€erence between ğœ‹(ğ‘¥ğ‘˜; ğœƒ) and ğœ‹(ğ‘¥ğ‘˜; ğœƒğ¾ ).
Then the following policy constraint, also known as the trust

region constraint, can be constructed:

ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) â‰¤ ğ›¿,

(16)

where ğ›¿ âˆˆ â„+ is the trust region boundary. In this case, the
policy update step is positively correlated with ğ›¿. Then, (15)
can be adapted to a trust region version:

ğœƒğ¾+1 = arg min
ğœƒ

ğ½ (ğœƒ)

ğ‘ .ğ‘¡. ğ‘¥ğ‘˜+ğ‘–+1 = ğ‘“ (ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ)),
(ğ‘¥ğ‘˜+ğ‘–+1) â‰¤ ğ‘ğœ ,

ğ½ğ¶ğœ
ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) â‰¤ ğ›¿.

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1],
ğœ âˆˆ [1, ğœmax],

(17)

For a small step size ğ›¿, the objective function ğ½ (ğœƒ) and

in the ğ¾th iteration can be well-approximated

state functions ğ½ğ¶ğœ
by linearizing around current policy ğœ‹(ğœƒğ¾ ) using Taylorâ€™s
expansion theorem. Denoting Î”ğœƒ = ğœƒ âˆ’ ğœƒğ¾ and ğ½ğ¶ğœ,ğ‘–
=
ğ½ğ¶ğœ

(ğ‘¥ğ‘˜+ğ‘–+1), it follows that:

ğ½ (ğœƒ) â‰ˆ ğ½ (ğœƒğ¾ ) + (âˆ‡ğœƒğ½ (ğœƒ)|
|
|ğœƒ=ğœƒğ¾

)âŠ¤Î”ğœƒ,

and

ğ½ğ¶ğœ

(ğ‘¥ğ‘˜+ğ‘–+1) â‰ˆ ğ½ğ¶ğœ

(ğ‘¥ğ‘˜+ğ‘–+1)|ğœƒ=ğœƒğ¾

+

(
âˆ‡ğœƒğ½ğ¶ğœ,ğ‘–

|
|
|ğœƒ=ğœƒğ¾

)âŠ¤

Î”ğœƒ,

where

âˆ‡ğœƒğ½ğ¶ğœ,ğ‘–

= ğœ™ğ‘–+1

ğœ•ğ½ğ¶ğœ

(ğ‘¥ğ‘˜+ğ‘–+1)

ğœ•ğ‘¥ğ‘˜+ğ‘–+1

,

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1].

In addition, since ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) and its gradient are both zero at
ğœƒ = ğœƒğ¾ , the trust region constraint is well-approximated by
second-order Taylor expansion:

ğ·ğœ‹(ğœƒ; ğœƒğ¾ )

â‰ˆ ğ·ğœ‹(ğœƒğ¾ ; ğœƒğ¾ ) +

=

1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒ,

(
âˆ‡ğœƒğ·ğœ‹(ğœƒ; ğœƒğ¾ )|
|
|ğœƒ=ğœƒğ¾

)âŠ¤

Î”ğœƒ +

1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒ

where ğ» âˆˆ â„ğ‘ Ã—ğ‘  is the Hessian of ğ·ğœ‹ with respect to ğœƒ, i.e.,
ğ»ğ‘–,ğ‘— = ğœ•2ğ·ğœ‹ (ğœƒ;ğœƒğ¾ )
. Since ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) â‰¥ 0 for âˆ€ğœƒ âˆˆ â„ğ‘ ,
ğ» is always positive semi-deï¬nite. In keeping with other
work in the literature [34, 35], we will assume it to be positive-
deï¬nite in the following.

|
|
|ğœƒ=ğœƒğ¾

ğœ•ğœƒğ‘–ğœ•ğœƒğ‘—

Denoting ğ‘” = âˆ‡ğœƒğ½ âˆ•â€–âˆ‡ğœƒğ½ â€–2, ğ‘ğ‘— = âˆ‡ğœƒğ½ğ¶ğœ,ğ‘–

âˆ•â€–âˆ‡ğœƒğ½ğ¶ğœ,ğ‘–â€–2
,
and ğ‘§ğ‘— = (ğ½ğ¶ğœ,ğ‘–|ğœƒ=ğœƒğ¾
, where ğ‘— = (ğœ âˆ’ 1) Ã—
âˆ’ ğ‘ğœ )âˆ•â€–âˆ‡ğœƒğ½ğ¶ğœ,ğ‘–â€–2
ğ‘ + ğ‘– + 1 âˆˆ [1, ğ‘€]. With ğ¶ â‰ [ğ‘1, ğ‘2, ..., ğ‘ğ‘€ ] âˆˆ â„ğ‘ Ã—ğ‘€ and
ğ‘§ â‰ [ğ‘§1, ğ‘§2, ..., ğ‘§ğ‘€ ]âŠ¤, the approximation to (17) is:

min
Î”ğœƒ
ğ‘ .ğ‘¡.

ğ‘”âŠ¤Î”ğœƒ

ğ‘§ + ğ¶ âŠ¤Î”ğœƒ â‰¤ 0,
1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒ â‰¤ ğ›¿.

(18)

Denoting the optimal solution of (18) as Î”ğœƒâˆ—, the update rule
for the constrained policy improvement process is

ğœƒğ¾+1 = Î”ğœƒâˆ— + ğœƒğ¾ .

J. Duan et al.: Preprint submitted to Elsevier

Page 5 of 15

Preprint

Although (18) is a convex constrained optimization prob-
lem, directly solving it will take lots of computation time and
resources because the dimension of variables Î”ğœƒ âˆˆ â„ğ‘  is
very large (usually over 10 thousand). Since (18) is convex,
it can also be solved using the dual method when feasible.
The Lagrange function of (18) can be expressed as

ğ¿ğ‘(Î”ğœƒ, ğœ†, ğœˆ) = ğ‘”âŠ¤Î”ğœƒ+ğœ†(

1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒâˆ’ğ›¿)+ğœˆâŠ¤(ğ‘§+ğ¶ âŠ¤Î”ğœƒ),

where ğœ† âˆˆ â„ and ğœˆ âˆˆ â„ğ‘€ . Then the dual to (18) is

max
ğœ†â‰¥0,ğœˆâ‰¥0

min
Î”ğœƒ

ğ¿ğ‘(Î”ğœƒ, ğœ†, ğœˆ).

(19)

The gradient of ğ¿ğ‘ with respect to parameters Î”ğœƒ can be
calculated as

âˆ‡Î”ğœƒğ¿ğ‘(Î”ğœƒ, ğœ†, ğœˆ) = ğ‘” + ğœ†ğ»Î”ğœƒ + ğ¶ğœˆ.

When âˆ‡Î”ğœƒğ¿ğ‘(Î”ğœƒ, ğœ†, ğœˆ) = 0, we have

Î”ğœƒ = âˆ’

ğ» âˆ’1(ğ‘” + ğ¶ğœˆ)
ğœ†

,

ğœ† > 0.

(20)

By taking (20) into (19), the dual to (18) can be expressed
as

max
ğœ†>0,ğœˆâ‰¥0

âˆ’

1
2ğœ†

(ğœ‡ + ğœˆâŠ¤ğ‘†ğœˆ + 2ğœˆâŠ¤ğ‘Ÿ) âˆ’ ğœ†ğ›¿ + ğœˆâŠ¤ğ‘§,

(21)

where ğœ‡ = ğ‘”âŠ¤ğ» âˆ’1ğ‘”, ğ‘† = ğ¶ âŠ¤ğ» âˆ’1ğ¶, ğ‘Ÿ = ğ¶ âŠ¤ğ» âˆ’1ğ‘”. Let

ğ¿(ğœ†, ğœˆ) â‰ âˆ’

1
2ğœ†

(ğœ‡ + ğœˆâŠ¤ğ‘†ğœˆ + 2ğœˆâŠ¤ğ‘Ÿ) âˆ’ ğœ†ğ›¿ + ğœˆâŠ¤ğ‘§,

then we can rewrite (21) with

min
ğœ†>0,ğœˆâ‰¥0

âˆ’ğ¿(ğœ†, ğœˆ).

Problem (22) is a bound-constrained convex optimiza-
tion problem with only ğ‘€ + 1 variables, which is also equal
to the number of constraints in (17) but much smaller than
the dimension of Î”ğœƒ âˆˆ â„ğ‘ . Therefore, compared with (17),
the optimal solution of (22) can be solved more easily and
eï¬ƒciently by using oï¬€-the-shelf methods such as L-BFGS-
B and truncated Newton method [46, 47]. Supposing ğœ†âˆ—, ğœˆâˆ—
are the optimal solutions to (22), ğœƒğ¾+1 can be updated as

ğœƒğ¾+1 = âˆ’

ğ» âˆ’1(ğ‘” + ğ¶ğœˆâˆ—)
ğœ†âˆ—

+ ğœƒğ¾ .

(23)

Note that for a high-dimensional policy ğœ‹(ğ‘¥ğ‘˜; ğœƒ), it is
prohibitively costly to compute ğ» and invert ğ» âˆ’1, which
poses a huge challenge for computing ğ» âˆ’1ğ‘” and ğ» âˆ’1ğ¶ in
(22). In this paper, we approximately compute them using
the conjugate gradient method without reliance on forming
the full matrix ğ» or ğ» âˆ’1 [23, 34].

3.3. Feasibility

On the one hand, the initial policy ğœ‹ğœƒ0

may be infeasi-
ble. On the other hand, due to the approximation errors in-
duced by linearization, the optimal solution Î”ğœƒâˆ— of (18) at
the ğ¾th iteration may be a bad update, and then a new pol-
icy ğœ‹ğœƒğ¾+1
that fails to satisfy state constraints may be pro-
duced. This may cause the optimization problem (18) of
the ğ¾ + 1th iteration to be infeasible. In other words, the
feasible region of (18) may be empty in some cases, i.e.,
Î˜ğ´ âˆ© Î˜ğµ = âˆ…, where Î˜ğ´ = {Î”ğœƒ âˆ¶ ğ‘§ + ğ¶ âŠ¤Î”ğœƒ â‰¤ 0}, and
Î˜ğµ = {Î”ğœƒ âˆ¶ 1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒ â‰¤ ğ›¿}.

Hence, before solving the dual problem (22), we check
whether the feasible region is empty by calculating the min-
imum trust region boundary, which makes the trust region
intersect with the intersection of half-planes of all linear con-
straints:

min
Î”ğœƒ
ğ‘ .ğ‘¡.

Î”ğœƒâŠ¤ğ»Î”ğœƒ

1
2
ğ‘§ + ğ¶ âŠ¤Î”ğœƒ â‰¤ 0.

(24)

Denoting the optimal solution to (24) as Î”ğœƒmin, then the
minimum trust region boundary that makes (18) feasible is
ğ›¿min = 1
âŠ¤ğ»Î”ğœƒmin, and it is clear that
2

Î”ğœƒmin

{Î˜ğ´ âˆ© Î˜ğµ = âˆ…,
â‰  âˆ…,
Î˜ğ´ âˆ© Î˜ğµ

ğ›¿min > ğ›¿,
â‰¤ ğ›¿.
ğ›¿min

(25)

The value ğ›¿min can be eï¬ƒciently obtained by solving the fol-
lowing dual problem:

max
ğœˆâ‰¥0

âˆ’

ğœˆâŠ¤ğ‘†ğœˆ
2

+ ğœˆâŠ¤ğ‘§.

(26)

ğ›¿min = âˆ’

ğœˆâ€ âŠ¤
ğ‘†ğœˆâ€ 
2

+ ğœˆâ€ âŠ¤

ğ‘§.

It is known from (25) that the magnitude of the value
ğ›¿ directly aï¬€ects the feasibility of (18). Denoting the ex-
â‰¤ ğ›¿ğ‘, we can
pected trust region boundary as ğ›¿ğ‘, if ğ›¿min
directly solve (22) with ğ›¿ = ğ›¿ğ‘. For the infeasible case, i.e.,
ğ›¿min > ğ›¿ğ‘, a recovery method is needed to calculate a rea-
sonable policy update. By introducing the recovery trust re-
gion boundary ğ›¿ğ‘, which is slightly greater than ğ›¿ğ‘, we pro-
pose two recovery rules according to the value of ğ›¿min: 1) If
â‰¥ ğ›¿min > ğ›¿ğ‘, we solve (22) with ğ›¿ = ğ›¿ğ‘ for ğœ†âˆ— and ğœˆâˆ—;
ğ›¿ğ‘
2) If ğ›¿min > ğ›¿ğ‘, we recover the policy by adding the state
constraints as a penalty to the original objective function:

min
Î”ğœƒ

ğ‘ .ğ‘¡.

(

(1 âˆ’ ğœ‚)ğ‘” + ğœ‚

)âŠ¤

Î”ğœƒ

ğ›¼ğ‘—ğ‘ğ‘—

ğ‘€
âˆ‘

ğ‘—=1

1
2

Î”ğœƒâŠ¤ğ»Î”ğœƒ â‰¤ ğ›¿ğ‘,

(27)

where ğœ‚ is the hyper-parameter that trades oï¬€ the importance
between the original objective function and the penalty term,

Suppose ğœˆâ€  is the optimal solution of (26), then

(22)

J. Duan et al.: Preprint submitted to Elsevier

Page 6 of 15

ğ›¼ğ‘— is the weight of the state constraint corresponding to ğ‘ğ‘—,
which is calculated by:

Preprint

ğ›¼ğ‘— =

ğ‘ğ‘—ğ‘’ğ‘§ğ‘—
ğ‘—=1 ğ‘ğ‘—ğ‘’ğ‘§ğ‘—

âˆ‘ğ‘€

,

where ğ‘ğ‘— = 5 if ğ‘§ğ‘— > 0, ğ‘ğ‘— = 1 otherwise, which penalizes
violations of the corresponding state constraint. Deï¬ning
ğ‘”ğ‘
ğ‘—=1 ğ›¼ğ‘—ğ‘ğ‘—, the dual to (27) can be expressed
as:

â‰ (1âˆ’ğœ‚)ğ‘”+ğœ‚ âˆ‘ğ‘€

max
ğœ†>0

âˆ’

ğœ‡ğ‘
2ğœ†

âˆ’ ğœ†ğ›¿ğ‘,

(28)

where ğœ‡ğ‘ = ğ‘”ğ‘
policy recovery rule:

âŠ¤ğ» âˆ’1ğ‘”ğ‘. In this case, we can easily ï¬nd the

âˆš

ğœƒğ¾+1 = âˆ’

2ğ›¿ğ‘
ğœ‡ğ‘

ğ» âˆ’1ğ‘”ğ‘ + ğœƒğ¾ .

(29)

3.4. Constrained Adaptive Dynamic Programming
By introducing the trust region constraint, approxima-
tion solution method, and recovery rules, we adapt Algo-
rithm 2 to Algorithm 3, called constrained adaptive dynamic
programming (CADP). Besides, we also incorporate the par-
allel exploring trick widely used in RL [20, 48, 17] to accel-
erate training and improve stability (See Fig. 1). In particu-
lar, we use multiple parallel agents to explore diï¬€erent state
spaces, thereby removing correlations in the training set. All
the state constraints of these parallel agents are stored in the
constraints buï¬€er. Due to the computational burden caused
by estimating the matrices ğ¶, ğ‘† and solving (22), the speed
of the policy optimization process will decrease as the num-
ber of state constraints increases. For each iteration, we only
consider ğ‘€ state constraints randomly selected from the con-
straints buï¬€er.

Remark 2. The performance of ADP approaches proposed
in [12, 29, 36, 37, 41] that use single NN heavily relies on
the quality of hand-crafted features. It is usually hard to de-
sign such features for high-dimensional nonlinear systems.
Nevertheless, the update rules (12) and (23) of the proposed
CADP algorithm are applicable to most approximate func-
tions, such as multi-layer NNs. Hence, according to Lemma
3 and 4, by employing multi-layer NNs to represent the pol-
icy and value function, CADP can directly learn a map from
system states to control inputs and value function, thereby
relaxing the need for hand-crafted features.

Remark 3. The proposed CADP method relies on the knowl-
edge of the system dynamics ğ‘“ (ğ‘¥, ğ‘¢). For controlled systems
with unknown dynamics, we can use supervised learning to
learn an approximated model from data, such as an NN-
based model. In recent years, many algorithms based on the
learned NN-dynamics have been proposed [49, 50]. Simi-
larly, given a real system with unknown dynamics, we can
ï¬rst learn an NN-based model, and then apply the CADP
algorithm to ï¬nd the nearly optimal policy.

Figure 1: CADP diagram. The value function and policy are
approximated by two NNs, called value network and policy net-
work, respectively. The value network is updated by minimiz-
ing loss function (8). If problem (18) is feasible, we update the
policy with its optimal solution; otherwise, we update with the
recovery rules. Multiple parallel agents are employed to explore
diï¬€erent parts of the state space. All the state constraints of
these parallel agents are stored in the constraints buï¬€er. For
each iteration, we only consider ğ‘€ state constraints randomly
selected from the constraints buï¬€er.

Algorithm 3 CADP Algorithm

Initial with arbitrary ğœƒ0, ğœ”0, learning rates ğ›¼ğ‘
Initialize iteration index ğ¾ = 0
repeat

ğ¿(ğœ”ğ¾ ) using (7), (9)

Rollout ğ‘ steps from ğ‘¥ğ‘˜ âˆˆ Î© with policy ğœ‹ğœƒğ¾
Receive and store ğ‘¥ğ‘˜+ğ‘–, ğ‘– âˆˆ [1, ğ‘]
Policy evaluation:
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒğ¾
), âˆ‡ğœ”ğ¾
Update value function using (12)
Constrained policy improvement:
Solve dual problem (26) for ğ›¿min
if ğ›¿min

, ğ‘‰ğœ”ğ¾

â‰¤ ğ›¿ğ‘ then
{ğ›¿ğ‘,
ğ›¿ğ‘,

ğ›¿ =

ğ›¿min

â‰¤ ğ›¿ğ‘
else

Solve dual problem (22) for ğœ†âˆ—, ğœˆâˆ—
Update policy with (23)

else

Update policy with (29)

end if
ğ¾ = ğ¾ + 1
until Convergence

Remark 4. For the proposed CADP algorithm, according
to (9) and (18), the system ğ‘“ (ğ‘¥, ğ‘¢) can be an arbitrary an-
alytic function only if it is diï¬€erentiable, i.e., ğœ•ğ‘“ (ğ‘¥,ğ‘¢)
and
ğœ•ğ‘“ (ğ‘¥,ğ‘¢)
ğœ•ğ‘¥

are available. Therefore, diï¬€erent from [12, 29, 30,
31, 32, 36, 37, 41, 42] that are subject to input-aï¬ƒne systems
since the policy needs to be analytically represented by the
value function, CADP is applicable to arbitrary nonlinear
systems with nonaï¬ƒne saturated inputs. Nonlinear nonaï¬ƒne
systems are very common in practical applications, such as
vehicle dynamics [25] and NN-based models learned from

ğœ•ğ‘¢

J. Duan et al.: Preprint submitted to Elsevier

Page 7 of 15

Value NetworkConstraints BufferOptimizerRecovery RuleFeasibility CheckParallel AgentsTD Error Policy NetworkUpdatexxlxxYNuUpdatePreprint

data [49, 50].

Remark 5. Given a practical optimal control problem with
known dynamics, we only need to formulate the policy opti-
mization process as (17). Then, CADP can be directly used
to ï¬nd the nearly optimal policy without reliance on hand-
crafted features. The learned oï¬„ine policy maps the states to
the corresponding nearly optimal control inputs, which can
be directly applied to the controlled system to realize online
control.

4. Simulation
4.1. Problem Description

To evaluate the performance of the CADP algorithm,
we choose the vehicle lateral and longitudinal control in the
path-tracking task as an example. It is a nonlinear and non-
aï¬ƒne system control problem with state constraints [25].
The control objective is to maximize the vehicle speed, while
maintaining a small tracking error and ensuring that the ve-
hicle stays within the stability region. The system states and
control inputs of this problem are listed in Table 3, and the
vehicle parameters are listed in Table 4.
In keeping with
other studies [25, 51, 52], we assume the states in Table 3 are
observable. Note that the system frequency used for simula-
tion is diï¬€erent from the sampling frequency ğ‘“ . The vehicle
is controlled by a saturated actuator, where Ì‡ğœ‰ âˆˆ [âˆ’0.35, 0.35]
and Ì‡ğ‘ âˆˆ [âˆ’2, 2]. According to the continuous-time vehicle
dynamics given in [25, 51, 52], the corresponding discrete-
time dynamics can be obtained using forward Euler methods
[51, 52], which can be described by:

ğ‘¥ =

ğ‘£ğ‘¦
â¡
ğ‘Ÿ
â¢
â¢
ğ‘£ğ‘¥
â¢
ğœ™
â¢
ğ‘¦
â¢
â¢
ğœ‰
â¢
ğ‘
â£

â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦

, ğ‘¢ =

]

[ Ì‡ğœ‰
Ì‡ğ‘

, ğ‘¥ğ‘˜+1 =

âˆ’ ğ‘£ğ‘¥ğ‘Ÿ

ğ¹ğ‘¦f cos ğœ‰+ğ¹ğ‘¦r
ğ‘š

â¤
â¡
ğ‘‘f ğ¹ğ‘¦f cos ğœ‰âˆ’ğ‘‘r ğ¹ğ‘¦r
â¥
â¢
â¥
â¢
ğ¼ğ‘§
â¥
â¢
ğ‘ + ğ‘£ğ‘¦ğ‘Ÿ
â¥
â¢
ğ‘Ÿ
â¥
â¢
ğ‘£ğ‘¥ sin ğœ™ + ğ‘£ğ‘¦ cos ğœ™
â¥
â¢
Ì‡ğœ‰
â¥
â¢
â¥
â¢
Ì‡ğ‘
â¦
â£

1
ğ‘“

+ğ‘¥ğ‘˜,

where ğ¹ğ‘¦f and ğ¹ğ‘¦r are the lateral tire forces of the front and
rear tires respectively. The lateral tire forces are approxi-
mated according to the Fiala tire model:

ğ¹ğ‘¦â€  = âˆ’sgn(ğ›¼â€ ) âˆ— min
{ |
|
ğ¶â€  tan ğ›¼â€ 
|
|
|
|

â€  (tan ğ›¼â€ )2
27(ğœ‡â€ ğ¹ğ‘§â€ )2

( ğ¶ 2

âˆ’

tan ğ›¼â€ |
ğ¶â€  |
|
|
3ğœ‡â€ ğ¹ğ‘§â€ 

+ 1

)|
|
|
|
|
|

}
,

ğœ‡â€ ğ¹ğ‘§â€ |
, |
|
|

by:

where ğ›¼â€  is the tire slip angle, ğ¹ğ‘§â€  is the tire load, ğœ‡â€  is the
lateral friction coeï¬ƒcient, and the subscript â€  âˆˆ {f, r} repre-
sents the front or rear tires. The slip angles can be calculated
from the geometric relationship between the front/rear axle
and the center of gravity (CG):

ğ›¼f = arctan(

ğ‘£ğ‘¦ + ğ‘‘f ğ‘Ÿ
ğ‘£ğ‘¥

) âˆ’ ğœ‰,

ğ›¼r = arctan(

ğ‘£ğ‘¦ âˆ’ ğ‘‘rğ‘Ÿ
ğ‘£ğ‘¥

).

J. Duan et al.: Preprint submitted to Elsevier

Table 3
State and Control Input

Explanation
Lateral velocity at center of gravity (CG)
Yaw rate
Longitudinal velocity at CG
Yaw angle between vehicle & trajectory
Distance between CG & trajectory
Front wheel angle
Longitudinal acceleration
Rate of change of ğ‘
Rate of change of ğœ‰

Symbol Unit
ğ‘£ğ‘¦
[m/s]
ğ‘Ÿ
[rad/s]
ğ‘£ğ‘¥
[m/s]
ğœ™
[rad]
ğ‘¦
[m]
ğœ‰
[rad]
[m/s2]
ğ‘
[m/s3]
Ì‡ğ‘
Ì‡ğœ‰
[rad/s]

Table 4
Vehicle Parameters

Explanation
Front wheel cornering stiï¬€ness
Rear wheel cornering stiï¬€ness
Distance from CG to front axle
Distance from CG to rear axle
Mass
Polar moment of inertia at CG
Tire-road friction coeï¬ƒcient
Sampling frequency
Simulation frequency

Symbol Value
ğ¶f
ğ¶r
ğ‘‘f
ğ‘‘r
ğ‘š
ğ¼ğ‘§
ğœ‡
ğ‘“

-88000 [N/rad]
-94000 [N/rad]
1.14 [m]
1.40 [m]
1500 [kg]
2420 [kgâ‹…m2]
1.0
40 [Hz]
200 [Hz]

Let ğ›¼max,â€  represent the tire slip angle when the tire fully-
sliding behavior occurs, calculated as:

ğ›¼max,â€  =

3ğœ‡â€ ğ¹ğ‘§â€ 
ğ¶â€ 

.

Assuming that the rolling resistance is negligible, the lat-

eral friction coeï¬ƒcient of the front/rear wheel is:

âˆš

(ğœ‡ğ¹ğ‘§â€ )2 âˆ’ (ğ¹ğ‘¥â€ )2
ğ¹ğ‘§â€ 

,

ğœ‡â€  =

where ğ¹ğ‘¥f and ğ¹ğ‘¥r are the longitudinal tire forces of the front
and rear tires respectively, calculated as:

{ğ¹ğ‘¥f , ğ¹ğ‘¥r} =

{{0, ğ‘šğ‘},
ğ‘šğ‘
ğ‘šğ‘
2
2

{

,

ğ‘ â‰¥ 0,

ğ‘ < 0.

},

The loads on the front and rear tires can be approximated

ğ¹ğ‘§f =

ğ‘‘r
ğ‘‘f + ğ‘‘r

ğ‘šğ‘”, ğ¹ğ‘§r =

ğ‘‘f
ğ‘‘f + ğ‘‘r

ğ‘šğ‘”.

To ensure vehicle stability, the yaw rate ğ‘Ÿ at the CG and
the slip angles ğ›¼â€  should be subject to the following con-
straints:

âˆ’ğ‘Ÿmax
âˆ’ğ›¼max,f
âˆ’ğ›¼max,r

â‰¤ğ‘Ÿ â‰¤ ğ‘Ÿmax,
â‰¤ğ›¼f
â‰¤ğ›¼r

â‰¤ ğ›¼max,f ,
â‰¤ ğ›¼max,r,

(30)

Page 8 of 15

Preprint

where ğ‘Ÿmax = ğœ‡r ğ‘”
ğ‘£ğ‘¥
The utility function is

.

ğ‘™(ğ‘¥, ğ‘¢) =
2(ğ‘£ğ‘¥ âˆ’ 30)2 + 80ğ‘¦2 + 40ğ‘Ÿ2 + 100(2ğœ™2 + ğœ‰2 + Ì‡ğœ‰2) + ğ‘2 + Ì‡ğ‘2
2000

.

Hence, the policy optimization problem of this example is
given by

{ ğ‘âˆ’1
âˆ‘

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğ‘¢ğ‘˜+ğ‘–) + ğ‘‰ (ğ‘¥ğ‘˜+ğ‘ ; ğœ”)

}

min
ğœƒ

ğ”¼
ğ‘¥ğ‘˜âˆ¼ğ‘‘ğ‘¥

ğ‘–=0
ğ‘ .ğ‘¡. ğ‘¥ğ‘˜+ğ‘–+1 = ğ‘“ (ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–; ğœƒ)),

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1],

|
|
|

|
|
|

â‰¤

â‰¤ ğ‘”,

|
|
|ğ‘˜+ğ‘–+1

|
|
|ğ‘˜+ğ‘–+1

ğ‘Ÿğ‘£ğ‘¥
ğœ‡r
3ğ¹ğ‘§f
ğ›¼f
ğ¶f
ğœ‡ğ‘“
3ğ¹ğ‘§r
ğ›¼r
|
|
|
|
ğ¶r
ğœ‡r
|ğ‘˜+ğ‘–+1
|
ğ·ğœ‹(ğœƒ; ğœƒğ¾ ) â‰¤ ğ›¿,

â‰¤

,

,

ğ›¼r
ğœ‡r

ğ›¼f
ğœ‡ğ‘“

ğ‘Ÿğ‘£ğ‘¥
ğœ‡r

, |
|
|

|
|
|ğ‘˜+ğ‘–+1

|
|
|ğ‘˜+ğ‘–+1

|
|
|ğ‘˜+ğ‘–+1

and |
where |
are the state con-
|
|
|
|
straint functions of state ğ‘¥ğ‘˜+ğ‘–+1 bounded above by ğ‘”, 3ğ¹ğ‘§f
ğ¶f
and 3ğ¹ğ‘§r
respectively. It is clear that the form of this prob-
ğ¶r
lem is the same as (17), which means that we can directly
train the vehicle control policy using the proposed CADP
algorithm.

4.2. Algorithm Details

In this paper, the value function and policy are repre-
sented by fully-connected NNs, which have the same archi-
tecture except for the output layers. For each network, the
input layer is composed of the states, followed by 5 hidden
layers using exponential linear units (ELUs) as activation
functions, with 32 units per layer. The output of the value
network is a linear unit, while the output layer of the policy
network is set as a ğ‘¡ğ‘ğ‘›â„ layer with two units, multiplied by
the matrix [0.35, 2] to confront bounded controls. We use
Adam method to update the value network ğ‘‰ (ğ‘¥; ğœ”) with the
learning rate of 8 Ã— 10âˆ’4. Other hyper-parameters of this
problem are shown in Table 5.

Table 5
Hyper-parameters

Parameters
agent number
prediction horizon
number of state constraints
discount factor
trust region boundary
recovery trust region boundary
penalty factor

Symbol Value

ğ‘
ğ‘€
ğ›¾
ğ›¿ğ‘
ğ›¿ğ‘
ğœ‚

256
30
10
0.98
0.0033
0.0063
0.8

We compare the CADP algorithm with four other al-
gorithms, namely GPI, TRADP (i.e., GPI with trust region

Figure 2: Training performance. Solid lines are average values
over 20 runs. Shaded regions correspond to 95% conï¬dence
interval.

constraint), penalty TRADP (P-TRADP, i.e., update policy
network by directly solving (27) with ğœ‚ = 0.2, 0.4, 0.6 re-
spectively), and CPO (constrained policy optimization [34]).
Note that TRADP can be considered as a special case of P-
TRADP, in which ğœ‚ = 0. The value ğœ‚ plays a diï¬€erent role
in CADP and P-TRADP algorithms. In CADP , ğœ‚ in (27)
works only when ğ›¿min > ğ›¿ğ‘. Therefore, we can take a rela-
tively large ğœ‚ to make (18) feasible as soon as possible.

4.3. Result Analysis

We train 20 diï¬€erent runs of each algorithm with dif-
ferent random seeds, with evaluations every 100 iterations.
Each evaluation measures the policy performance by calcu-
lating the undiscounted accumulated cost function of 1000
steps (25s) during the simulation period starting from a ran-
dom initialized state. The learning curves are shown in Fig.
2. Basically, all algorithms converge within 3000 iterations.
Fig. 3 compares the performance and constraint satisfac-
tion of each algorithm. As shown in Fig. 3a, CADP matches
or outperforms all other baseline ADP algorithms in policy
performance. Fig. 3b, 3c and 3d show the maximum value
of |ğ‘Ÿ| âˆ’ ğ‘Ÿmax, |ğ›¼r| âˆ’ ğ›¼max,r, and |ğ›¼f | âˆ’ ğ›¼max,f for each simu-
lation, respectively. Take Fig. 3b as an example, from (30),
|ğ‘Ÿ| âˆ’ ğ‘Ÿmax > 0 indicates the corresponding state constraint
is violated. Results show that only CADP meets all three
state constraints during the simulation. For TRADP and P-
TRADP algorithms, the cumulative cost increases with the
penalty factor ğœ‚. This is because the learned policy needs
to balance the cost term and constraint term. In particular,
when ğœ‚ = 0.6, the accumulated cost of the learned policy is
almost twice that of CADP because the constraint term ac-
counts for too much of the objective function in (27). Even
so, for the P-TRADP algorithm with ğœ‚ = 0.6, the situation
in which the controlled system violates the constraints still
exists. CPO considers the state constraint by limiting the ex-
pected discount cumulative state function, which is approx-
imated by an additional NN. The transformed constraints of

J. Duan et al.: Preprint submitted to Elsevier

Page 9 of 15

00.511.522.53(Ã—103)Iteration102103104105Accumulated CostMethodCADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPOPreprint

(a)

(b)

(c)

(d)

Figure 3: Algorithm comparison. Each box plot is drawn based on values of 20 runs. Values greater than 0 (marked as red
dashed lines) indicate violations of the corresponding state constraints. (a) Policy performance. (b) Maximum value of |ğ‘Ÿ| âˆ’ ğ‘Ÿmax
per simulation. (c) Maximum value of |ğ›¼r| âˆ’ ğ›¼max,r per simulation. (d) Maximum value of |ğ›¼f | âˆ’ ğ›¼max,f per simulation.

CPO are not equivalent to the original constraints given in
(14), so the satisfaction of state constraints are still not guar-
anteed. CADP deals with the state constraints by solving
(17), which enables it to ï¬nd the optimal policy from all fea-
sible policies that satisfy the state constraints.

The typical simulation results by applying GPI, P-TRADP
(ğœ‚ = 0.6) and CADP are presented in Fig. 4. As shown in
Fig. 4a, 4d and 4g, the policy learned by GPI is too aggres-
sive, resulting in large acceleration ğ‘, front wheel angle ğœ‰ and
velocity ğ‘£ğ‘¥, thus violating state constraints ğ‘Ÿmax and ğ›¼max,r.
In contrast, the policy learned by P-TRADP (ğœ‚ = 0.6) is
conservative in the control of ğ‘ and ğœ‰, leading to large accu-
mulated cost (see Fig. 4b, 4e and 4h). As a comparison, the
policy learned by CADP has a good balance between track-
ing performance and constraint satisfaction. This example
demonstrates the eï¬€ectiveness of CADP in solving the non-
aï¬ƒne nonlinear optimal control problems with multiple state
constraints.

5. Conclusions

This paper proposes a constrained adaptive dynamic pro-
gramming (CADP) algorithm to solve optimal control prob-

lems with multiple state constraints. The proposed algo-
rithm is applicable to general nonlinear systems with non-
aï¬ƒne and saturated control inputs. Firstly, a constrained
generalized policy iteration (CGPI) framework is developed
to handle state constraints by transforming the traditional
policy improvement process into a constrained policy op-
timization problem. The proposed CADP algorithm is an
actor-critic variant of CGPI, in which both policy and value
functions are approximated by multi-layer NNs to directly
map the system states to control inputs and value function re-
spectively, which relaxes the need for hand-crafted features.
Due to the computational cost and the nonlinear character-
istics of NNs, it may be intractable to directly solve the con-
strained policy optimization problem. For calculation con-
venience, the proposed CADP linearizes the constrained op-
timization problem locally into a quadratically constrained
linear programming problem, and then obtains the optimal
update of the policy network by solving its dual problem.
Meanwhile, a trust region constraint is added to prevent ex-
cessive policy update, thus ensuring linearization accuracy.
We determine the feasibility of the policy optimization prob-
lem by calculating the minimum trust region boundary and
update the policy using two recovery rules when infeasible.

J. Duan et al.: Preprint submitted to Elsevier

Page 10 of 15

CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO50100150200250300350Accumulated CostCADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.40.20.00.20.4|r|rmax [rad]CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.20.10.00.10.20.3|r|max,r [rad]CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.250.200.150.100.050.00|f|max,f [rad]Preprint

(b)

(e)

(h)

(a)

(d)

(g)

(c)

(f)

(i)

Figure 4: Simulation results. (a) State curves of GPI. (b) State curves of P-TRADP (ğœ‚ = 0.6). (c) State curves of CADP.
(d) Tire slip angles curves of GPI. (e) Tire slip angles curves of P-TRADP (ğœ‚ = 0.6). (f) Tire slip angles curves of CADP. (g)
Trajectory of GPI. (h) Trajectory of P-TRADP (ğœ‚ = 0.6). (i) Trajectory of CADP.

We apply CADP and six other baseline algorithms to
the vehicle control problem in the path-tracking task. Re-
sults show that CADP matches or outperforms all other base-
line algorithms in policy performance without violating any
In the future, we
state constraints during the simulation.
will extend the proposed algorithm to uncertain systems by
combining other techniques, such as robust control [53] and
fuzzy adaptive control [54, 55].

A. Appendix
A.1. Convergence Proofs of Constrained Policy

Iteration in the Tabular Case

We refer to the combination of policy iteration (PI), a
special case of GPI, and constrained policy improvement
(15) as constrained policy iteration (CPI). We ï¬rst discuss
the convergence and optimality of CPI. In a version of CPI
for a tabular setting, we maintain two tables to estimate pol-
icy ğœ‹ and the corresponding value function ğ‘‰ ğœ‹ respectively.
The pseudo-code of CPI in the tabular case can be summa-
rized as Algorithm 4.

Next, the convergence property of Algorithm 4 will be

established. As the iteration index ğ¾ â†’ âˆ, we will show
that the optimal value function and optimal policy can be
achieved using Algorithm 4. Before the main theorem, some
lemmas are necessary at this point. Lemma 1 shows the con-
vergence of policy evaluation, which has been proved and
described in previous studies.

Lemma 1. (Policy Evaluation [11]). For a ï¬xed policy ğœ‹ğ¾ ,
consider the Bellman backup rule in (31) and a mapping
ğ‘‰ğ¾,0(ğ‘¥ğ‘˜) âˆ¶ â„ğ‘› â†’ â„, then the sequence ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜) will con-
verge to the value function ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© as ğ‘ â†’ âˆ.

PROOF. Note that throughout our computation, the value func-
tion ğ‘‰ (ğ‘¥ğ‘˜) is always bounded for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© since ğ›¾ âˆˆ (0, 1)
and ğ‘™(ğ‘¥ğ‘˜, ğ‘¢ğ‘˜) is bounded. Deï¬ne a maximum norm on the
value function as

â€–ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜)âˆ’ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜)â€–âˆ

â‰ max

ğ‘¥ğ‘˜âˆˆÎ© |ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜)âˆ’ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜)|.

Suppose Î” = â€–ğ‘‰ğ¾,1(ğ‘¥) âˆ’ ğ‘‰ğ¾,0(ğ‘¥)â€–âˆ. According to (7) and

J. Duan et al.: Preprint submitted to Elsevier

Page 11 of 15

0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,Ï†,r,Î¾vx [m/s]Ï† [Â°]Î¾ [Â°]r [Â° / s]rmax [Â° / s]0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,Ï†,r,Î¾vx [m/s]Ï† [Â°]Î¾ [Â°]r [Â° / s]rmax [Â° / s]0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,Ï†,r,Î¾vx [m/s]Ï† [Â°]Î¾ [Â°]r [Â° / s]rmax [Â° / s]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]050100150200250300350400450500550600Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected Trajectory050100150200250300350400Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected Trajectory050100150200250300350400450Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected TrajectoryPreprint

Algorithm 4 CPI Algorithm: tabular case

Initial with arbitrary ğœ‹0, ğ‘‰0
Initialize iteration index ğ¾ = 0
Given an arbitrarily small positive ğœ–
repeat

Rollout ğ‘ steps from âˆ€ğ‘¥ğ‘˜ âˆˆ Î© with policy ğœ‹ğ¾
Receive and store ğ‘¥ğ‘˜+ğ‘–, ğ‘– âˆˆ [1, ğ‘]
Policy evaluation:
repeat
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾,ğ‘) using (7)
Update value function using

works for âˆ€ğ‘¥ğ‘˜ âˆˆ Î¨, it is deï¬ned as a feasible policy, denoted
by ğœ‹ âˆˆ Î .

Lemma 2. (Constrained Policy Improvement). Suppose Î© âŠ†
Î¨. Assume that if ğ‘¥ğ‘˜ âˆˆ Î¨ and ğœ‹ âˆˆ Î , then state ğ‘¥ğ‘˜+1 =
ğ‘“ (ğ‘¥ğ‘˜, ğœ‹(ğ‘¥ğ‘˜)) âˆˆ Î¨ with respect to system (1). Given the as-
sociated value function ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) of a policy ğœ‹ğ¾ (ğ‘¥ğ‘˜), and the
new policy ğœ‹ğ¾+1(ğ‘¥ğ‘˜) is obtained by solving (32) for âˆ€ğ‘¥ğ‘˜ âˆˆ
Î©. Then ğ‘‰ ğœ‹ğ¾+1 (ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© and âˆ€ğ¾ âˆˆ â„•+.

PROOF. Because Î© âŠ† Î¨, (32) is feasible for âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. Then,
from (32), one has

ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜) = ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾,ğ‘),

âˆ€ğ‘¥ğ‘˜ âˆˆ Î© (31)

ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾+1) â‰¥ ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾+1, ğ‘‰ğ¾+1), âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

until â€–ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜) âˆ’ ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜)â€–âˆ
ğ‘‰ğ¾+1(ğ‘¥ğ‘˜) = ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜),
Constrained policy improvement:

â‰¤ ğœ–
âˆ€ğ‘¥ğ‘˜ âˆˆ Î©

Update policy using:

ğº(ğ‘¥ğ‘˜, ğœ‹, ğ‘‰ğ¾+1)

ğœ‹ğ¾+1(ğ‘¥ğ‘˜) = arg min
ğœ‹(ğ‘¥ğ‘˜)
ğ‘ .ğ‘¡. ğ‘¥ğ‘˜+ğ‘–+1 = ğ‘“ (ğ‘¥ğ‘˜+ğ‘–, ğœ‹(ğ‘¥ğ‘˜+ğ‘–)),
(ğ‘¥ğ‘˜+ğ‘–+1) â‰¤ ğ‘ğœ ,

ğ½ğ¶ğœ

ğ‘– âˆˆ [0, ğ‘ âˆ’ 1]

ğœ âˆˆ [1, ğœmax]

(32)

ğ¾ = ğ¾ + 1
until Convergence

(31), it follows that

|ğ‘‰ğ¾,2(ğ‘¥ğ‘˜) âˆ’ ğ‘‰ğ¾,1(ğ‘¥ğ‘˜)|

= |ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾,1) âˆ’ ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾,0)|
= ğ›¾ ğ‘
|ğ‘‰ğ¾,1(ğ‘¥ğ‘˜+ğ‘ ) âˆ’ ğ‘‰ğ¾,0(ğ‘¥ğ‘˜+ğ‘ )|
â‰¤ ğ›¾ ğ‘ Î”,

âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

Extending this for all subsequent iteration steps ğ‘, one has

|ğ‘‰ğ¾,ğ‘+1(ğ‘¥ğ‘˜) âˆ’ ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜)|

|ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜+ğ‘ ) âˆ’ ğ‘‰ğ¾,ğ‘âˆ’1(ğ‘¥ğ‘˜+ğ‘ )|

= ğ›¾ ğ‘
â‰¤ ğ›¾ ğ‘ğ‘ Î”,

âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

Therefore, limğ‘â†’âˆ â€–ğ‘‰ğ¾,ğ‘+1(ğ‘¥) âˆ’ ğ‘‰ğ¾,ğ‘(ğ‘¥)â€–âˆ = 0, which also
means that ğ‘‰ğ¾,âˆ(ğ‘¥ğ‘˜) satisï¬es the Bellman equation (3) for
âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. So, the sequence ğ‘‰ğ¾,ğ‘(ğ‘¥ğ‘˜) converges to the value
function ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© as ğ‘ â†’ âˆ, i.e., ğ‘‰ğ¾+1 = ğ‘‰ ğœ‹ğ¾ .

The following lemma shows that, the new policy ğœ‹ğ¾+1
obtained by (32) has a lower value function ğ‘‰ ğœ‹ğ¾+1 than the
old policy ğ‘‰ ğœ‹ğ¾ . The proof borrows heavily from the policy
improvement theorem of Q-learning and soft Q-learning [11,
21, 22, 56].

Deï¬nition 1. (Feasible State and Policy). A state ğ‘¥ğ‘˜ is de-
ï¬ned as a feasible state, denoted by ğ‘¥ğ‘˜ âˆˆ Î¨ with respect to
system (1), if there is a policy that can ensure the system sat-
isï¬es all the state constraints described in (32). If a policy ğœ‹

By Lemma 1, one has ğ‘‰ğ¾+1 = ğ‘‰ ğœ‹ğ¾ . Furthermore, from (7),
it is clear that

ğº(ğ‘¥ğ‘˜, ğœ‹ğ¾ , ğ‘‰ğ¾+1) = ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜), âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

Therefore, we can show that

ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜)
ğ‘âˆ’1
âˆ‘

=

ğ›¾ ğ‘–ğ‘™(ğ‘¥â€²

ğ‘˜+ğ‘–, ğœ‹ğ¾ (ğ‘¥â€²

ğ‘˜+ğ‘–)) + ğ›¾ ğ‘ ğ‘‰ ğœ‹ğ¾ (ğ‘¥â€²

ğ‘˜+ğ‘ )

ğ‘–=0
ğ‘âˆ’1
âˆ‘

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹ğ¾+1(ğ‘¥ğ‘˜+ğ‘–)) + ğ›¾ ğ‘ ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜+ğ‘ )

ğ‘–=0
2ğ‘âˆ’1
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹ğ¾+1(ğ‘¥ğ‘˜+ğ‘–)) + ğ›¾ 2ğ‘ ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜+2ğ‘ )

(33)

âˆ
âˆ‘

ğ‘–=0

ğ›¾ ğ‘–ğ‘™(ğ‘¥ğ‘˜+ğ‘–, ğœ‹ğ¾+1(ğ‘¥ğ‘˜+ğ‘–))

â‰¥

â‰¥

â‹®

â‰¥

= ğ‘‰ ğœ‹ğ¾+1 (ğ‘¥ğ‘˜),

âˆ€ğ‘¥ğ‘˜ âˆˆ Î©,

where ğ‘¥â€²
proven that

ğ‘˜+ğ‘– âˆ¼ ğœ‹ğ¾ , ğ‘¥ğ‘˜+ğ‘– âˆ¼ ğœ‹ğ¾+1 and ğ‘¥â€²

ğ‘˜ = ğ‘¥ğ‘˜. We have thus

ğ‘‰ ğœ‹ğ¾+1 (ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜),

âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

Theorem 1. (Constrained Policy Iteration in Tabular Case).
Through Algorithm 4, any policy ğœ‹0 will converge to the
global optimal policy ğœ‹âˆ— âˆˆ Î , such that ğ‘‰ ğœ‹âˆ—
(ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹(ğ‘¥ğ‘˜)
for âˆ€ğœ‹ âˆˆ Î  and âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

PROOF. For the policy ğœ‹ğ¾ at iteration ğ¾, we can ï¬nd its
associated ğ‘‰ ğœ‹ğ¾ through policy evaluation process follows
from Lemma 1. By Lemma 2, ğ‘‰ ğœ‹ğ¾+1 (ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) for
âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. Besides, since ğ‘‰ ğœ‹ğ¾ (ğ‘¥ğ‘˜) is bounded below by
ğ‘‰ âˆ—(ğ‘¥ğ‘˜), ğœ‹ğ¾ and ğ‘‰ ğœ‹ğ¾ will converge to some ğœ‹âˆ and ğ‘‰ ğœ‹âˆ.
At convergence, for âˆ€ğœ‹ âˆˆ Î  and ğ‘¥ğ‘˜ âˆˆ Î©, it must follow that
ğº(ğ‘¥ğ‘˜, ğœ‹âˆ, ğ‘‰ ğœ‹âˆ ) â‰¤ ğº(ğ‘¥ğ‘˜, ğœ‹, ğ‘‰ ğœ‹âˆ). Using the same iterative
argument as in (33) of Lemma 2, it is clear that ğ‘‰ ğœ‹âˆ (ğ‘¥ğ‘˜) â‰¤
ğ‘‰ ğœ‹(ğ‘¥ğ‘˜) for âˆ€ğœ‹ âˆˆ Î  and ğ‘¥ğ‘˜ âˆˆ Î©. From (4), one has ğ‘‰ ğœ‹âˆ (ğ‘¥ğ‘˜) =
ğ‘‰ âˆ—(ğ‘¥ğ‘˜). Hence ğœ‹âˆ is optimal in Î , i.e., ğœ‹âˆ = ğœ‹âˆ—.

J. Duan et al.: Preprint submitted to Elsevier

Page 12 of 15

Preprint

A.2. Convergence Proofs of Constrained Policy

Iteration with Function Approximation

The pseudo-code of CPI with function approximation is
described as Algorithm 5. By Lemma 3 and 4, the conver-
gence results of tabular CPI (Algorithm 4) can be extended
to Algorithm 5.

Algorithm 5 CPI Algorithm: function approximation

Initial with arbitrary ğœƒ0, ğœ”0, learning rates ğ›¼ğ‘
Given an arbitrarily small positive ğœ–
Initialize iteration index ğ¾ = 0
repeat

Rollout ğ‘ steps from âˆ€ğ‘¥ğ‘˜ âˆˆ Î© with policy ğœ‹ğœƒğ¾
Receive and store ğ‘¥ğ‘˜+ğ‘–, ğ‘– âˆˆ [1, ğ‘]
Policy evaluation:

repeat

ğœ”ğ¾,ğ‘+1 = ğœ”ğ¾,ğ‘
Calculate ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒğ¾
repeat

, ğ‘‰ğœ”ğ¾,ğ‘+1

) using (7)

Calculate

dğ¿(ğœ”ğ¾,ğ‘+1)
dğœ”ğ¾,ğ‘+1
Update value function using

using (9)

ğœ”ğ¾,ğ‘+1 = âˆ’ğ›¼ğ‘

dğ¿(ğœ”ğ¾,ğ‘+1)
dğœ”ğ¾,ğ‘+1

+ ğœ”ğ¾,ğ‘+1

(34)

until ğ¿(ğœ”ğ¾,ğ‘+1) â‰¤ ğœ–

until â€–ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾,ğ‘+1) âˆ’ ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾,ğ‘)â€–âˆ
ğœ”ğ¾+1 = ğœ”ğ¾,ğ‘+1

Constrained policy improvement:
Construct ğ½ (ğœƒ) under ğ‘‰ğœ”ğ¾+1
Update policy using (15)

using (10)

â‰¤ ğœ–

ğ¾ = ğ¾ + 1
until Convergence

Lemma 3. (Universal Approximation Theorem [43]). For
any continuous function ğ¹ (ğ‘¥) âˆ¶ â„ğ‘› â†’ â„ğ‘‘ on a compact set
Î©, there exists a feed-forward NN, having only a single hid-
den layer, which uniformly approximates ğ¹ (ğ‘¥) and its gra-
dient to within arbitrarily small error ğœ– âˆˆ â„+ on Î©.

Lemma 4. (Global Minima of Over-Parameterized Neural
Networks [39, 40]). Consider the following optimization prob-
lem

min
ğœ“

îˆ¸(ğœ“) = ğ”¼
ğ‘‹ğ‘–âˆˆîˆ®

(îˆ² (ğ‘‹ğ‘–; ğœ“) âˆ’ ğ‘Œğ‘–)2}
,

{ 1
2

where ğ‘‹ğ‘– âˆˆ â„ğ‘› is the training input, ğ‘Œğ‘– âˆˆ â„ğ‘‘ is the associ-
ated label, îˆ® = {(ğ‘‹1, ğ‘Œ1), (ğ‘‹2, ğ‘Œ2), â€¦} is the dataset, ğœ“ is
the parameter to be optimized, and îˆ² âˆ¶ â„ğ‘› â†’ â„ğ‘‘ is an NN.
If the NN îˆ² (ğ‘‹; ğœ“) is over-parameterized (i.e., the number of
hidden neurons and layers is suï¬ƒciently large), simple al-
gorithms such as Gradient Descent (GD) or SGD can ï¬nd
global minima on the training objective îˆ¸(ğœ“) in polynomial
time, as long as the dataset îˆ® is non-degenerate. The dataset
is non-degenerate if the same inputs ğ‘‹1 = ğ‘‹2 have the same
labels ğ‘Œ1 = ğ‘Œ2.

Theorem 2. (Constrained Policy Iteration with Function Ap-
proximation). Suppose both ğ‘‰ (ğ‘¥ğ‘˜; ğœ”) and ğœ‹(ğ‘¥ğ‘˜; ğœƒ) are over-
parameterized. Through Algorithm 5, any initial parameters
ğœ”0 and ğœƒ0 will converge to ğœ”âˆ— and ğœƒâˆ—, such that ğ‘‰ (ğ‘¥ğ‘˜; ğœ”âˆ—) =
ğ‘‰ ğœ‹ğœƒâˆ— (ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹ğœƒ (ğ‘¥ğ‘˜) for âˆ€ğœ‹ğœƒ âˆˆ Î  and âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

PROOF. In the policy evaluation step of Algorithm 5, by
Lemma 3, there always âˆƒğœ”ğ¾,ğ‘+1 âˆˆ â„ğ‘, such that

ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾,ğ‘+1) = ğº(ğ‘¥ğ‘˜, ğœ‹ğœƒ, ğ‘‰ğœ”ğ¾,ğ‘

),

âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

(35)

In other words, âˆƒğœ”ğ¾,ğ‘+1 âˆˆ â„ğ‘, such that ğ¿(ğœ”ğ¾,ğ‘+1) â‰¤ ğœ–.
Since Algorithm 5 updates ğœ”ğ¾,ğ‘ using (34) to continuously
minimize ğ¿(ğœ”ğ¾,ğ‘), according to Lemma 4, we can ï¬nd ğœ”ğ¾,ğ‘+1
in polynomial time. From Lemma 1, as (35) and (31) are
equivalent, the sequence ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾,ğ‘+1) converges to the value
function ğ‘‰ ğœ‹ğœƒğ¾ (ğ‘¥ğ‘˜) for âˆ€ğ‘¥ğ‘˜ âˆˆ Î© as ğ‘ â†’ âˆ, i.e.,
ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾+1) = ğ‘‰ (ğ‘¥ğ‘˜; ğœ”ğ¾,âˆ) = ğ‘‰ ğœ‹ğœƒğ¾ (ğ‘¥ğ‘˜), âˆ€ğ‘¥ğ‘˜ âˆˆ Î©. (36)
For the policy improvement process, according to Lemma
3 and extending Lemma 2 to the function approximation
case, one has

ğ‘‰ ğœ‹ğœƒğ¾+1 (ğ‘¥ğ‘˜) â‰¤ ğ‘‰ ğœ‹ğœƒğ¾ (ğ‘¥ğ‘˜),

âˆ€ğ‘¥ âˆˆ Î©.

(37)

Finally, according to (36), (37) and Theorem 1, we can
get the conclusion that ğ‘‰ ğœ‹ğœƒâˆ (ğ‘¥ğ‘˜) = ğ‘‰ (ğ‘¥ğ‘˜; ğœ”âˆ) = ğ‘‰ âˆ—(ğ‘¥ğ‘˜)
for âˆ€ğ‘¥ğ‘˜ âˆˆ Î©, i.e., ğœƒâˆ = ğœƒâˆ— and ğœ”âˆ = ğœ”âˆ—.

A.3. The Generalized Policy Iteration Framework
Algorithms based on PI framework, such as Algorithm 4
and 5, proceed by alternately updating the value and policy
functions. Note that while one function is being updated, the
other remains unchanged. Besides, taking the policy evalua-
tion process of Algorithm 4 and 5 as an example, each func-
tion usually requires multiple updating iterations to meet the
terminal conditions, which is the so-called protracted iter-
ative computation problem [11]. This often leads to slow
learning. Therefore, for practical applications, almost all
ADP or RL algorithms build on the GPI framework, which
truncates the policy evaluation and policy improvement pro-
cesses into an arbitrary step or even one-step update [7, 11].
In recent years, many experimental results and theoretical
proofs have shown that almost all ADP or RL algorithms
based on PI can be adapted to a GPI version without losing
the convergence guarantees [8, 15, 17, 19, 20, 21, 22, 24, 56,
57]. Based on this fact and Theorem 2, we make the follow-
ing remark.

Remark 6. (Constrained Generalized Policy Iteration). Sup-
pose both ğ‘‰ (ğ‘¥ğ‘˜; ğœ”) and ğœ‹(ğ‘¥ğ‘˜; ğœƒ) are over-parameterized. Thr-
ough Algorithm 2, any initial parameters ğœ”0 and ğœƒ0 will
converge to ğœ”âˆ— and ğœƒâˆ—, such that ğ‘‰ (ğ‘¥ğ‘˜; ğœ”âˆ—) = ğ‘‰ ğœ‹ğœƒâˆ— (ğ‘¥ğ‘˜) â‰¤
ğ‘‰ ğœ‹ğœƒ (ğ‘¥ğ‘˜) for âˆ€ğœ‹ğœƒ âˆˆ Î  and âˆ€ğ‘¥ğ‘˜ âˆˆ Î©.

B. Acknowledgment

This study is supported by the Beijing Science and Tech-

nology Plan Project with Z191100007419008, Tsinghua University-
Didi Joint Research Center for Future Mobility, and NSF

J. Duan et al.: Preprint submitted to Elsevier

Page 13 of 15

Preprint

China with 51575293 and U20A20334. We would like to
acknowledge Jie Li, Hao Chen, Yang Zheng, Yiwen Liao,
Guofa Li, Ziyu Lin and Jiatong Xu for their valuable sug-
gestions. The authors are grateful to the Editor-in-Chief, the
Associate Editor, and anonymous reviewers for their valu-
able comments.

References
[1] D. P. Bertsekas, Dynamic programming and optimal control, 4th Edi-

tion, Athena Scientiï¬c, Belmont, 2017.

[2] F. L. Lewis, D. Vrabie, V. L. Syrmos, Optimal control, John Wiley &

Sons, New York, 2012. doi:10.1002/9781118122631.

[3] F. Y. Wang, H. Zhang, D. Liu, Adaptive dynamic programming: An
IEEE Computational Intelligence Magazine 4 (2009)

introduction,
39â€“47. doi:10.1109/MCI.2009.932261.

[4] P. Werbos, Beyond regression: New tools for prediction and analysis
in the behavioral sciences, Ph. D. dissertation, Harvard University
(1974).

[5] P. Werbos, Approximate dynamic programming for realtime con-
trol and neural modelling, Handbook of Intelligent Control: Neural,
Fuzzy and Adaptive Approaches (1992) 493â€“525.

[6] W. B. Powell, Approximate Dynamic Programming: Solving the
curses of dimensionality, John Wiley & Sons, New York, 2007.
doi:10.1002/9780470182963.

[7] D. Liu, Q. Wei, D. Wang, X. Yang, H. Li, Adaptive dynamic program-
ming with applications in optimal control, Springer, Berlin, 2017.
doi:10.1007/978-3-319-50815-3.

[8] J. Duan, S. E. Li, Z. Liu, M. Bujarbaruah, B. Cheng, Generalized
policy iteration for optimal control in continuous time, arXiv preprint
(2019). arXiv:1909.05402.

[9] K. G. Vamvoudakis, F. L. Lewis, Online actorâ€“critic algorithm to
solve the continuous-time inï¬nite horizon optimal control problem,
Automatica 46 (2010) 878â€“888. doi:10.1016/j.automatica.2010.02.
018.

[10] J. Li, H. Modares, T. Chai, F. L. Lewis, L. Xie, Oï¬€-policy reinforce-
ment learning for synchronization in multiagent graphical games,
IEEE transactions on neural networks and learning systems 28 (2017)
2434â€“2445. doi:10.1109/TNNLS.2016.2609500.

[11] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction,

MIT Press, Cambridge, 1998. doi:10.1109/TNN.1998.712192.

[12] Z. Chen, S. Jagannathan, Generalized Hamiltonâ€“Jacobiâ€“Bellman
formulation-based neural network control of aï¬ƒne nonlinear discrete-
time systems, IEEE Transactions on Neural Networks 19 (2008) 90â€“
106. doi:10.1109/TNN.2007.900227.

[13] A. Al-Tamimi, F. L. Lewis, M. Abu-Khalaf, Discrete-time nonlin-
ear HJB solution using approximate dynamic programming: Conver-
gence proof,
IEEE Transactions on Systems, Man, and Cybernet-
ics, Part B (Cybernetics) 38 (2008) 943â€“949. doi:10.1109/ADPRL.2007.
368167.

[14] D. Liu, Q. Wei, P. Yan, Generalized policy iteration adaptive dynamic
programming for discrete-time nonlinear systems, IEEE Transactions
on Systems, Man, and Cybernetics: Systems 45 (2015) 1577â€“1591.
doi:10.1109/TSMC.2015.2417510.

[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., Human-level control through deep reinforcement learning, Na-
ture 518 (2015) 529â€“533. doi:10.1038/nature14236.

[16] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., Grand-
master level in StarCraft II using multi-agent reinforcement learning,
Nature 575 (2019) 350â€“354. doi:10.1038/s41586-019-1724-z.
[17] J. Duan, Y. Guan, S. E. Li, Y. Ren, B. Cheng, Distributional soft
actor-critic: Oï¬€-policy reinforcement learning for addressing value
estimation errors, arXiv preprint (2020). arXiv:2001.02811.

[18] Y. Ren, J. Duan, S. E. Li, Y. Guan, Q. Sun, Improving generaliza-
tion of reinforcement learning with minimax distributional soft actor-

critic, in: 23rd IEEE International Conference on Intelligent Trans-
portation Systems, IEEE, 2020. doi:10.1109/ITSC45102.2020.9294300.
[19] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Sil-
ver, D. Wierstra, Continuous control with deep reinforcement learn-
ing, arXiv preprint (2015). arXiv:1509.02971.

[20] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, K. Kavukcuoglu, Asynchronous methods for deep rein-
in: Int. Conf. on Machine Learning, 2016, pp.
forcement learning,
1928â€“1937.

[21] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Oï¬€-
policy maximum entropy deep reinforcement learning with a stochas-
tic actor,
in: Proceedings of the 35th International Conference on
Machine Learning, PMLR, 2018, pp. 1861â€“1870.

[22] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-
mar, H. Zhu, A. Gupta, P. Abbeel, et al., Soft actor-critic algorithms
and applications, arXiv preprint (2018). arXiv:1812.05905.

[23] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region
in: Int. Conf. on Machine Learning, 2015, pp.

policy optimization,
1889â€“1897.

[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov,
arXiv preprint (2017).

Proximal policy optimization algorithms,
arXiv:1707.06347.

[25] S. E. Li, H. Chen, R. Li, Z. Liu, Z. Wang, Z. Xin, Predictive lat-
eral control to stabilise highly automated vehicles at tire-road friction
limits, Vehicle system dynamics 58 (2020) 768â€“786. doi:10.1080/
00423114.2020.1717553.

[26] M. Abu-Khalaf, F. L. Lewis, Nearly optimal control laws for non-
linear systems with saturating actuators using a neural network HJB
approach, Automatica 41 (2005) 779â€“791. doi:10.1016/j.automatica.
2004.11.034.

[27] L. Dong, X. Zhong, C. Sun, H. He, Event-triggered adaptive dy-
namic programming for continuous-time systems with control con-
straints, IEEE Transactions on Neural Networks and Learning Sys-
tems 28 (2016) 1941â€“1952. doi:10.1109/TNNLS.2016.2586303.

[28] B. Luo, H. N. Wu, T. Huang, D. Liu, Reinforcement learning solu-
tion for HJB equation arising in constrained optimal control problem,
Neural Networks 71 (2015) 150â€“158. doi:10.1016/j.neunet.2015.08.
007.

[29] B. Fan, Q. Yang, X. Tang, Y. Sun, Robust ADP design for continuous-
time nonlinear systems with output constraints, IEEE Transactions
on Neural Networks and Learning Systems 29 (2018) 2127â€“2138.
doi:10.1109/TNNLS.2018.2806347.

[30] T. Zhang, H. Xu, Adaptive optimal dynamic surface control of
strict-feedback nonlinear systems with output constraints,
Interna-
tional Journal of Robust and Nonlinear Control 30 (2020) 2059â€“2078.
doi:10.1002/rnc.4864.

[31] J. Na, B. Wang, G. Li, S. Zhan, W. He, Nonlinear constrained op-
timal control of wave energy converters with adaptive dynamic pro-
gramming,
IEEE Transactions on Industrial Electronics 66 (2019)
7904â€“7915. doi:10.1109/TIE.2018.2880728.

[32] J. Sun, C. Liu, Backstepping-based adaptive dynamic programming
for missile-target guidance systems with state and input constraints,
Journal of the Franklin Institute 355 (2018) 8412â€“8440. doi:10.1016/
j.jfranklin.2018.08.024.

[33] F. Borrelli, A. Bemporad, M. Morari, Predictive control for linear
and hybrid systems, Cambridge University Press, Cambridge, 2017.
doi:10.1017/9781139061759.

[34] J. Achiam, D. Held, A. Tamar, P. Abbeel, Constrained policy opti-
in: Proceedings of the 34th International Conference on

mization,
Machine Learning, JMLR, 2017, pp. 22â€“31.

[35] L. Wen, J. Duan, S. E. Li, S. X. Xu, H. Peng, Safe reinforcement
learning for autonomous vehicles through parallel constrained pol-
in: 23rd IEEE International Conference on Intel-
icy optimization,
ligent Transportation Systems, IEEE, 2020. doi:10.1109/ITSC45102.
2020.9294262.

[36] A. Heydari, S. N. Balakrishnan, Finite-horizon control-constrained
nonlinear optimal control using single network adaptive critics, IEEE
Transactions on Neural Networks and Learning Systems 24 (2012)

J. Duan et al.: Preprint submitted to Elsevier

Page 14 of 15

Preprint

tolerant control for strict-feedback nonlinear systems, IEEE Transac-
tions on Fuzzy Systems (2020). doi:10.1109/TFUZZ.2020.2965890.
[56] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning
with deep energy-based policies, in: Proc. of the 34th Int. Conf. on
Machine Learning, 2017, pp. 1352â€“1361.

[57] G. Barth-Maron, M. W. Hoï¬€man, D. Budden, W. Dabney, D. Horgan,
D. TB, A. Muldal, N. Heess, T. Lillicrap, Distributed distributional
deterministic policy gradients, in: Int. Conf. on Learning Represen-
tations, 2018.

145â€“157. doi:10.1109/TNNLS.2012.2227339.

[37] H. Zhang, L. Cui, Y. Luo, Near-optimal control for nonzero-sum dif-
ferential games of continuous-time nonlinear systems using single-
network ADP, IEEE Transactions on Cybernetics 43 (2012) 206â€“216.
doi:10.1109/TSMCB.2012.2203336.

[38] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015)

436â€“444. doi:10.1038/nature14539.

[39] Z. Allen-Zhu, Y. Li, Z. Song, A convergence theory for deep learning
via over-parameterization, in: Proceedings of the 36th International
Conference on Machine Learning, PMLR, 2019, pp. 242â€“252.
[40] S. Du, J. Lee, H. Li, L. Wang, X. Zhai, Gradient descent ï¬nds global
minima of deep neural networks, in: Proceedings of the 36th Inter-
national Conference on Machine Learning, PMLR, 2019, pp. 1675â€“
1685.

[41] T. Dierks, B. T. Thumati, S. Jagannathan, Optimal control of unknown
aï¬ƒne nonlinear discrete-time systems using oï¬„ine-trained neural net-
works with proof of convergence, Neural Networks 22 (2009) 851â€“
860. doi:10.1016/j.neunet.2009.06.014.

[42] H. Li, D. Liu, Optimal control for discrete-time aï¬ƒne non-linear sys-
tems using general value iteration, IET Control Theory & Applica-
tions 6 (2012) 2725â€“2736. doi:10.1049/iet-cta.2011.0783.

[43] K. Hornik, M. Stinchcombe, H. White, Universal approximation of
an unknown mapping and its derivatives using multilayer feedfor-
ward networks, Neural Networks 3 (1990) 551â€“560. doi:10.1016/
0893-6080(90)90005-6.

[44] S. Ruder, An overview of gradient descent optimization algorithms,

arXiv preprint (2016). arXiv:1609.04747.

[45] Z. Li, J. Deng, R. Lu, Y. Xu, J. Bai, C. Su, Trajectory-tracking
control of mobile robot systems incorporating neural-dynamic opti-
mized model predictive approach,
IEEE Transactions on Systems,
Man, and Cybernetics: Systems 46 (2015) 740â€“749. doi:10.1109/
TSMC.2015.2465352.

[46] C. Zhu, R. H. Byrd, P. Lu, J. Nocedal, Algorithm 778: L-BFGS-B:
Fortran subroutines for large-scale bound-constrained optimization,
ACM Transactions on Mathematical Software 23 (1997) 550â€“560.
doi:10.1145/279232.279236.

[47] J. Nocedal, S. Wright, Numerical optimization, Springer, Berlin,

2006. doi:10.1007/b98874.

[48] J. Duan, S. E. Li, Y. Guan, Q. Sun, B. Cheng, Hierarchical reinforce-
ment learning for self-driving decision-making without reliance on
labelled driving data,
IET Intelligent Transport Systems 14 (2020)
297â€“305. doi:10.1049/iet-its.2019.0317.

[49] A. Nagabandi, G. Kahn, R. S. Fearing, S. Levine, Neural network
dynamics for model-based deep reinforcement learning with model-
free ï¬ne-tuning, in: IEEE International Conference on Robotics and
Automation (ICRA), IEEE, 2018, pp. 7559â€“7566. doi:10.1109/ICRA.
2018.8463189.

[50] N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, Y. Tassa, Learn-
ing continuous control policies by stochastic value gradients, in: Ad-
vances in Neural Information Processing Systems, 2015, pp. 2944â€“
2952.

[51] R. Li, Y. Li, S. Li, E. Burdet, B. Cheng, Driver-automation indi-
rect shared control of highly automated vehicles with intention-aware
authority transition, in: IEEE Intelligent Vehicles Symposium (IV),
2017, pp. 26â€“32. doi:10.1109/IVS.2017.7995694.

[52] J. Kong, M. Pfeiï¬€er, G. Schildbach, F. Borrelli, Kinematic and dy-
namic vehicle models for autonomous driving control design,
in:
IEEE Intelligent Vehicles Symposium, IEEE, 2015, pp. 1094â€“1099.
doi:10.1109/IVS.2015.7225830.

[53] D. Liu, H. Li, D. Wang, Neural-network-based zero-sum game for
discrete-time nonlinear systems via iterative adaptive dynamic pro-
gramming algorithm, Neurocomputing 110 (2013) 92â€“100. doi:10.
1016/j.neucom.2012.11.021.

[54] K. Sun, Q. Jianbin, H. R. Karimi, Y. Fu, Event-triggered robust
fuzzy adaptive ï¬nite-time control of nonlinear systems with pre-
scribed performance, IEEE Transactions on Fuzzy Systems (2020).
doi:10.1109/TFUZZ.2020.2979129.

[55] K. Sun, L. Liu, J. Qiu, G. Feng, Fuzzy adaptive ï¬nite-time fault-

J. Duan et al.: Preprint submitted to Elsevier

Page 15 of 15

