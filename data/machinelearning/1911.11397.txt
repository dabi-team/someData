2
2
0
2

r
p
A
8

]

Y
S
.
s
s
e
e
[

3
v
7
9
3
1
1
.
1
1
9
1
:
v
i
X
r
a

Adaptive dynamic programming for nonaﬃne nonlinear optimal
control problem with state constraints

Jingliang Duana, Zhengyu Liua, Shengbo Eben Lia,∗, Qi Suna, Zhenzhong Jiab and Bo Chenga

aState Key Lab of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, 100084, China.
bRobotics Institute at Carnegie Mellon University, Pittsburgh, PA 15213, USA.

A R T I C L E I N F O

A B S T R A C T

Keywords:
Adaptive dynamic programming
Optimal control
State constraint
Reinforcement learning

This paper presents a constrained adaptive dynamic programming (CADP) algorithm to solve general
nonlinear nonaﬃne optimal control problems with known dynamics. Unlike previous ADP algo-
rithms, it can directly deal with problems with state constraints. Firstly, a constrained generalized
policy iteration (CGPI) framework is developed to handle state constraints by transforming the tradi-
tional policy improvement process into a constrained policy optimization problem. Next, we propose
an actor-critic variant of CGPI, called CADP, in which both policy and value functions are approx-
imated by multi-layer neural networks to directly map the system states to control inputs and value
function, respectively. CADP linearizes the constrained optimization problem locally into a quadrat-
ically constrained linear programming problem, and then obtains the optimal update of the policy
network by solving its dual problem. A trust region constraint is added to prevent excessive policy
update, thus ensuring linearization accuracy. We determine the feasibility of the policy optimization
problem by calculating the minimum trust region boundary and update the policy using two recovery
rules when infeasible. The vehicle control problem in the path-tracking task is used to demonstrate
the eﬀectiveness of this proposed method.

1. Introduction

Dynamic programming (DP) is a theoretical and eﬀec-
tive tool in solving discrete-time (DT) optimal control prob-
lems with known dynamics [1]. The optimal value function
(or cost-to-go) for DT systems is obtained by solving the
DT Hamilton-Jacobi-Bellman (HJB) equation, also known
as the Bellman optimality equation, which develops back-
ward in time [2]. However, due to the curse of dimension-
ality, running DP directly to get the optimal solution of DT
HJB is usually computationally untenable for complex non-
linear DT systems [3]. The adaptive dynamic programming
(ADP) methods were ﬁrst proposed by Werbos as a way to
overcome this diﬃculty by solving an approximate solution
of DT HJB forward in time [4, 5]. In some studies, ADP is
also called approximate dynamic programming [6, 7].

ADP methods are usually implemented as an actor-critic
architecture which involves a critic parameterized function
for value function approximation and an actor parameter-
ized function for policy approximation [8, 7, 9, 10]. Neu-
ral networks (NNs) have been widely used as approximators
of both value function and policy due to their strong ﬁtting
ability, and have achieved good performance on many con-
trol tasks [7]. Most ADP methods adopt generalized policy
iteration (GPI) as a primary tool to adjust both value and pol-
icy networks by iteratively solving the DT HJB equation [7].
The well-known policy iteration and value iteration methods
can also be regarded as special cases of GPI [7, 11]. There
are two revolving iteration procedures for GPI framework:

∗Corresponding author

duanjl15@163.com (J. Duan); liuzheng17@mails.tsinghua.edu.cn (Z.

Liu); lishbo@tsinghua.edu.cn (S.E. Li); qisun@tsinghua.edu.cn (Q. Sun);
zhenzhong.jia@gmail.com (Z. Jia); chengbo@tsinghua.edu.cn (B. Cheng)

ORCID(s): 0000-0003-4923-3633 (S.E. Li)

1) policy evaluation, which drives the value function towards
the true value function for the current policy, and 2) policy
improvement, which improves the policy to reduce the cur-
rent value function.

Over the last few decades, many ADP methods of ﬁnd-
ing the nearly optimal control solution for DT systems with
known dynamics have emerged. Chen and Jagannathan pro-
posed an ADP method to ﬁnd nearly optimal control state
feedback laws for input-aﬃne nonlinear DT systems by it-
eratively solving the generalized HJB equation. The value
function was approximated by a linear combination of artiﬁ-
cially designed basis functions, while the policy was directly
derived from the value function [12]. Both actor and critic
NNs were utilized by Al-Tamimi et al. to develop a value-
iteration-based algorithm for DT systems, and it was shown
that the algorithm could converge to the optimal value func-
tion and policy as long as the control coeﬃcient matrix was
known [13]. Liu et al. proposed a GPI algorithm for DT
nonlinear systems, and the admissibility property of the pol-
icy network during learning could be guaranteed as long as
the initialized policy was admissible [14].

ADP is usually considered together with reinforcement
learning (RL) because both provide approximate solutions to
DP [11]. Compared with ADP, RL focuses more on solving
the nearly optimal policy of high-dimensional systems with
completely unknown models, such as Atari games [15] and
StarCraft [16]. In recent years, RL algorithms such as DSAC
(Distributional Soft Actor-Critic) [17, 18], DDPG (Deep De-
terministic Policy Gradient) [19], A3C (Asynchronous Ad-
vantage Actor-Critic) [20], SAC (Soft Actor-Critic) [21, 22],
TRPO (Trust Region Policy Optimization) [23], and PPO
(Proximal Policy Optimization) [24], have also been widely
used to solve DT optimal control problems.

It should be pointed out that the state constraints were

J. Duan et al.: Preprint submitted to Elsevier

Page 1 of 15

 
 
 
 
 
 
Preprint

not considered in these ADP techniques mentioned above.
For practical applications, however, most controlled systems
must be subject to some state restrictions. Taking vehicle
control in the path-tracking task as an example, in addition
to considering the tracking performance, certain state func-
tions of the vehicle must be constrained to the stability zone
to prevent vehicle instability problems [25]. Compared with
state constraints, input constraints can be easily confronted
by introducing a nonquadratic cost function or directly con-
straining the output range of actor NN using some saturated
functions, such as hyperbolic tangent function [26, 27, 28].
To cope with the state constraints, most related ADP re-
searches choose to transform the original problem into an
unconstrained one by constructing additional system states
or adding the state constraints to the objective function as
a penalty [29, 30, 31, 32]. However, the optimality of the
original system may not be guaranteed since the controller
of the transformed system has to spend extra eﬀorts to ensure
the satisfaction of the state constraints, which will certainly
enlarge the cost function [29]. Also, the trade-oﬀ between
performance and state constraints may lead to constraint vi-
olations in some cases. Model predictive control (MPC) is a
commonly used control method to solve control input online
while satisfying a set of constraints [33]. However, com-
pared with ADP, complex systems such as non input-aﬃne
and nonlinear models are still big challenges for MPC. In
addition, Achiam et al. proposed a model-free RL algo-
rithm, constrained policy optimization (CPO), to maximize
rewards while enforcing constraints [34]. But the constraint
satisfaction of CPO, or its variant parallel CPO [35], cannot
ensure the policy safety, because its approximated constraint
function is an expected and cumulative value.

Besides, existing ADP algorithms usually rely on hand-
crafted features combined with linear value functions or pol-
icy representations, also called single NN in [36, 37]. Ob-
viously, the performance of these algorithms heavily relies
on the quality of the feature representation. However, it is
usually hard to design such features for high-dimensional
nonlinear systems. Compared with single NNs, NNs with
multiple hidden layers or deep NNs have the ﬁtting ability
to directly map the system states to optimal control inputs
or value function without reliance on hand-crafted features
[38]. In addition, theoretical analysis and experimental re-
sults show that the multi-layer NN can usually converge to
the nearly global minimum if it is over-parameterized [39,
40]. This is why deep RL has outperformed traditional RL
in many challenging domains, from games to robotic control
[11]. Hence, in this paper, multi-layer NNs are employed to
relax the need for hand-crafted features.

In this paper, we propose a new ADP algorithm, called
constrained ADP (CADP), to solve optimal control problems
with state constraints, which is applicable to general non-
linear systems with nonaﬃne saturated control inputs. The
main contributions and advantages of this paper are summa-
rized as follows:

1. A constrained generalized policy iteration (CGPI) frame-
work is developed to handle state constraints, in which

the traditional policy improvement process is trans-
formed into a constrained policy optimization prob-
lem. Compared with most existing ADP algorithms
[12, 13, 14] that use the traditional policy improve-
ment, the proposed CADP method building on CGPI
is applicable to optimal control problems with state
constraints.

2. For the approximated policy with high-dimensional
parameters, directly solving the constrained policy op-
timization problem may be intractable due to the com-
putational cost and the nonlinear characteristics of dy-
namics and the policy function. Therefore, some ex-
isting ADP researches consider state constraints by
transforming the original problem into an unconstrained
one [29, 30, 31, 32]. However, the optimality may not
be guaranteed since the controller of the transformed
system has to spend extra eﬀorts to ensure the satisfac-
tion of the state constraints, which will enlarge the cost
function [29]. The proposed CADP algorithm deals
with this situation by linearizing the constrained op-
timization problem locally into a quadratically con-
strained linear programming problem, and then ob-
tains the optimal update of the policy function by solv-
ing its dual problem. Meanwhile, a trust region con-
straint is added to prevent excessive policy update, thus
ensuring linearization accuracy. Besides, two recov-
ery rules are proposed to update the policy in case that
the primal problem is infeasible.

3. The proposed CADP algorithm employs the actor-critic
architecture to approximate both policy and value func-
tions by multi-layer NNs. According to the univer-
sal ﬁtting ability and global convergence property of
multi-layer NNs [39, 40], the value and policy net-
works of CADP directly map the system states to con-
trol inputs and value function, respectively. Therefore,
compared with ADP researches [12, 29, 36, 37, 41]
that use single NN, CADP relaxes the need for hand-
crafted features. Besides, diﬀerent from [12, 29, 30,
31, 32, 36, 37, 41, 42] which are subject to input-aﬃne
systems since the policy needs to be analytically rep-
resented by the value function, CADP is applicable to
arbitrary nonlinear nonaﬃne dynamics by optimizing
the independent policy network.

The paper is organized as follows. In Section 2, we pro-
vide the formulation of the DT optimal control problem, fol-
lowed by the general description of GPI framework. Section
3 presents the constrained ADP algorithm. In Section 4, we
present a simulation example that shows the eﬀectiveness of
the CADP algorithm for DT system. Section 5 concludes
this paper.

2. Mathematical preliminaries
2.1. Notation

For ease of presentation, we summarize the abbrevia-
tions and mathematical notations in Table 1 and Table 2,
respectively.

J. Duan et al.: Preprint submitted to Elsevier

Page 2 of 15

Preprint

Table 1
Abbreviations

Abbreviation
ADP
CADP
CGPI
DT
GPI
HJB
NN
PI

Explanation
Adaptive dynamic programming
Constrained ADP
Constrained generalized policy iteration
Discrete-time
Generalized policy iteration
Hamilton-Jacobi-Bellman
Neural network
Policy iteration

Table 2
Mathematical Notations

Symbol
ℝ
ℕ, ℕ+
ℝ𝑛
𝑘
𝑥 ∈ ℝ𝑛, 𝑢 ∈ ℝ𝑚
𝑓 ∶ ℝ𝑛 × ℝ𝑚 → ℝ𝑛
𝛾 ∈ (0, 1)
𝜋 ∶ ℝ𝑛 → ℝ𝑚
𝑉 ∶ ℝ𝑛 → ℝ
𝜃 ∈ ℝ𝑠, 𝜔 ∈ ℝ𝑝
𝐾
∇𝑥𝐹 (⋅)
‖ ⋅ ‖2

Explanation
the set of real numbers
the set of natural numbers
𝑛-dimensional Euclidean space
time step index
state and control input vectors
system function
discount factor
policy function
value function
parameters vector of 𝜋 and 𝑉
iteration index
the gradient of 𝐹 (⋅) w.r.t. 𝑥
Euclidean norm

2.2. Discrete-time Hamilton-Jacobi-Bellman

Equation

Consider the discrete-time (DT) general time-invariant

dynamical system

𝑥𝑘+1 = 𝑓 (𝑥𝑘, 𝑢𝑘),

(1)

where 𝑥𝑘 ∈ ℝ𝑛 and 𝑢𝑘 ∈ ℝ𝑚 are the state vector and control
input vector at time 𝑘, respectively, and 𝑓 ∶ ℝ𝑛 × ℝ𝑚 → ℝ𝑛
is the system function. We assume that 𝑓 (𝑥𝑘, 𝑢𝑘) is Lipschitz
continuous on a compact set Ω that contains the origin, and
that the system is stabilizable on Ω, i.e., there exists a contin-
uous policy 𝜋(𝑥), where 𝑢𝑘 = 𝜋(𝑥𝑘), such that the system is
asymptotically stable on Ω. The system dynamics 𝑓 (𝑥𝑘, 𝑢𝑘)
is assumed to be known, which can be a nonlinear and in-
put nonaﬃne analytic function only if 𝜕𝑓 (𝑥,𝑢)
are
available. The system input 𝑢 can be either constrained or
unconstrained. Given the policy 𝜋(𝑥𝑘), we deﬁne its associ-
ated inﬁnite-horizon value function as
∞
∑

and 𝜕𝑓 (𝑥,𝑢)

𝜕𝑥

𝜕𝑢

𝑉 𝜋(𝑥𝑘) =

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖)),

(2)

𝑖=0

where 𝑙(𝑥𝑘, 𝑢𝑘) ∶ ℝ𝑛 × ℝ𝑚 → ℝ is the utility function, and
𝛾 ∈ (0, 1) is the discount factor.

Furthermore, denoting the prediction horizon as 𝑁, (2)

can be rewritten as

𝑉 𝜋(𝑥𝑘) =

𝑁−1
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖)) + 𝛾 𝑁 𝑉 𝜋(𝑥𝑘+𝑁 ), (3)

J. Duan et al.: Preprint submitted to Elsevier

which is the well-known Bellman equation. Then the op-
timal control problem can now be formulated as ﬁnding a
policy such that the value function associated with the sys-
tem in (1) is minimized for ∀𝑥𝑘 ∈ Ω. The minimized value
function 𝑉 ∗(𝑥𝑘) deﬁned by

𝑉 ∗(𝑥𝑘) = min
𝜋

𝑉 𝜋(𝑥𝑘)

(4)

satisﬁes the DT Hamilton-Jacobi-Bellman (HJB) equation or
Bellman optimality equation

𝑉 ∗(𝑥𝑘) = min
𝜋

{ 𝑁−1
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖)) + 𝛾 𝑁 𝑉 ∗(𝑥𝑘+𝑁 )

}

.

(5)

Meanwhile, the optimal control 𝜋∗(𝑥𝑘) for ∀𝑥𝑘 ∈ Ω can be
derived as

𝜋∗(𝑥𝑘) = arg min
𝜋

{ 𝑁−1
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖))+𝛾 𝑁 𝑉 ∗(𝑥𝑘+𝑁 )

}
.

(6)

To ﬁnd the optimal control solution for the problem, one
only needs to solve (5) for the value function and then substi-
tute the solution into (6) to obtain the optimal control. How-
ever, due to the nonlinear nature of DT HJB, ﬁnding its so-
lution is generally diﬃcult or intractable.

2.3. Adaptive Dynamic Programming

The proposed algorithm for DT optimal control prob-
lems used in this paper is motivated by generalized policy
iteration (GPI) techniques [11]. GPI is an iterative method
widely used in adaptive dynamic programming (ADP) and
reinforcement learning (RL) algorithms to ﬁnd the approxi-
mate solution of DT HJB. The ADP algorithms building on
GPI usually employ actor-critic architecture to approximate
both the value function and policy. In this study, both the
value function and policy are approximated by multi-layer
NNs, called value network (or critic network) 𝑉 (𝑥; 𝜔) and
policy network (or actor network) 𝜋(𝑥; 𝜃), where 𝜔 ∈ ℝ𝑝
and 𝜃 ∈ ℝ𝑠 are network parameters. These two networks
directly build the map from the raw system states to the ap-
proximated value function and control inputs, respectively.
In this case, no hand-crafted features or basis functions are
needed.

GPI involves two interacting processes: 1) policy evalu-
ation, which drives the estimated value function towards the
true value function for current policy based on (3), and 2)
policy improvement, which improves the policy with respect
to current estimated value function based on (6).

Deﬁning the accumulated future cost of state 𝑥𝑘 under

policy 𝜋𝜃 and value 𝑉𝜔 as

𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔) =

𝑁−1
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖; 𝜃))+𝛾 𝑁 𝑉 (𝑥𝑘+𝑁 ; 𝜔),

(7)

Page 3 of 15

Preprint

the policy evaluation process of GPI proceeds by iteratively
minimizing the following loss function:

𝐿(𝜔) = 𝔼

𝑥𝑘∼𝑑𝑥

{ 1
2

(𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔) − 𝑉 (𝑥𝑘; 𝜔)

)2}
,

(8)

where 𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔) − 𝑉 (𝑥𝑘; 𝜔) is usually called temporal
diﬀerence (TD) error and 𝑑𝑥 denotes the state distribution
over 𝑥 ∈ Ω. Under the assumption of the universal approxi-
mation theorem of NNs [43], 𝑑𝑥 can be an arbitrary distribu-
tion as long as the probability density 𝑝(𝑥) > 0 for ∀𝑥 ∈ Ω,
such as uniform distribution. Therefore, the update gradient
for the value network is given by

Algorithm 1 GPI Framework

Initial with arbitrary 𝜃0, 𝜔0, learning rates 𝛼𝑐 and 𝛼𝑎
Initialize iteration index 𝐾 = 0
repeat

Rollout 𝑁 steps from 𝑥𝑘 ∈ Ω with policy 𝜋𝜃𝐾
Receive and store 𝑥𝑘+𝑖, 𝑖 ∈ [1, 𝑁]
Policy evaluation:
Calculate 𝐺(𝑥𝑘, 𝜋𝜃𝐾
Update value function using

), ∇𝜔𝐾

, 𝑉𝜔𝐾

𝐿(𝜔𝐾 ) using (7), (9)

𝜔𝐾+1 = −𝛼𝑐∇𝜔𝐾

𝐿(𝜔𝐾 ) + 𝜔𝐾

(12)

∇𝜔𝐿(𝜔) = 𝔼

𝑥𝑘∼𝑑𝑥

{(𝑉 (𝑥𝑘; 𝜔)−𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔)

)

∇𝜔𝑉 (𝑥𝑘; 𝜔)

}
.

Policy improvement:
Calculate 𝐺(𝑥𝑘, 𝜋𝜃𝐾

, 𝑉𝜔𝐾+1

), ∇𝜃𝐾

𝐽 (𝜃𝐾 ) using (7), (11)

(9)

Update policy using

In the policy improvement step, the parameters 𝜃 of the
policy network are updated to minimize the objective func-
tion

𝐽 (𝜃) = 𝔼

𝑥𝑘∼𝑑𝑥

{

𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔)

}
.

Denoting the matrix

𝜕𝑢𝑘+𝑖
𝜕𝜃
as 𝜓𝑖, the update gradient for the policy network is

∈ ℝ𝑠×𝑛 as 𝜙𝑖,

𝜕𝑥𝑘+𝑖
𝜕𝜃

(10)

∈ ℝ𝑠×𝑚

∇𝜃𝐽 (𝜃)

= 𝔼

𝑥𝑘∼𝑑𝑥

= 𝔼

𝑥𝑘∼𝑑𝑥

{ 𝑁−1
∑

𝑖=0
{ 𝑁−1
∑

𝑖=0

𝛾 𝑖∇𝜃𝑙(𝑥𝑘+𝑖, 𝑢𝑘+𝑖) + 𝛾 𝑁 ∇𝜃𝑉 (𝑥𝑘+𝑁 ; 𝜔)

}

𝛾 𝑖[

𝜙𝑖

𝜕𝑙(𝑥𝑘+𝑖, 𝑢𝑘+𝑖)
𝜕𝑥𝑘+𝑖

+ 𝜓𝑖

]

𝜕𝑙(𝑥𝑘+𝑖, 𝑢𝑘+𝑖)
𝜕𝑢𝑘+𝑖
𝜕𝑉 (𝑥𝑘+𝑁 ; 𝜔)
𝜕𝑥𝑘+𝑁

}
,

+ 𝛾 𝑁 𝜙𝑁

(11)

where

𝜙𝑖 = 𝜙𝑖−1

𝜕𝑓 (𝑥𝑘+𝑖−1, 𝑢𝑘+𝑖−1)
𝜕𝑥𝑘+𝑖−1

+ 𝜓𝑖−1

𝜕𝑓 (𝑥𝑘+𝑖−1, 𝑢𝑘+𝑖−1)
𝜕𝑢𝑘+𝑖−1

,

with 𝜙0 = 0, and

𝜓𝑖 = 𝜙𝑖

𝜕𝜋(𝑥𝑘+𝑖; 𝜃)
𝜕𝑥𝑘+𝑖

+ ∇𝜃𝜋(𝑥𝑘+𝑖; 𝜃).

In practice, ∇𝜔𝐿(𝜔) and ∇𝜃𝐽 (𝜃) are usually approxi-
mated by the sample average. Any oﬀ-the-shelf NN opti-
mization methods can be used to update the value and pol-
icy networks, including stochastic gradient descent (SGD),
RMSProp, Adam [44]. Taking the SGD method as an ex-
ample, the pseudo-code of GPI can be summarized as Al-
gorithm 1. Algorithm 1 will iteratively converge to the op-
timal control policy 𝜋(𝑥𝑘; 𝜃∗) = 𝜋∗(𝑥𝑘) and value function
𝑉 (𝑥𝑘; 𝜔∗) = 𝑉 ∗(𝑥𝑘) for ∀𝑥𝑘 ∈ Ω. Proofs of convergence
and optimality have been given in [7, 11].

𝜃𝐾+1 = −𝛼𝑎∇𝜃𝐾

𝐽 (𝜃𝐾 ) + 𝜃𝐾

(13)

𝐾 = 𝐾 + 1
until Convergence

3. Constrained ADP
3.1. Constrained Policy Improvement

One drawback of the policy update rule in (13) is that it
is not suitable for optimal control problems with state con-
straints. However, for practical applications, most controlled
systems must be subject to some state restrictions, such as
vehicles [25], wave energy converters [31] and robots [45].
Although the state constraints can be added to the objec-
tive function as a penalty, it is often diﬃcult to balance the
constraint requirements with the control objectives. The op-
timality may not be guaranteed since the controller has to
spend extra eﬀorts to ensure the satisfaction of the state con-
straints. Besides, the trade-oﬀ between performance and
state constraints may lead to constraint violations in some
cases.

In this paper, the state constraints of future 𝑁 steps are
introduced to transfer the policy improvement process into a
constrained optimization problem. Assuming there are 𝜏max
kinds of state constraints, the 𝜏th state constraint can be for-
mulated as:

𝑖 ∈ [0, 𝑁 − 1],

(𝑥𝑘+𝑖+1) ≤ 𝑏𝜏 ,
𝐽𝐶𝜏
(𝑥𝑘+𝑖+1) ∶ ℝ𝑛 → ℝ is the 𝜏th state function
where 𝐽𝐶𝜏
bounded above by boundary 𝑏𝜏 . Therefore, the policy im-
provement process can be transformed into the following
constrained optimization problem:

(14)

𝜃𝐾+1 = arg min
𝜃

𝐽 (𝜃)

𝑠.𝑡. 𝑥𝑘+𝑖+1 = 𝑓 (𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖; 𝜃)),
(𝑥𝑘+𝑖+1) ≤ 𝑏𝜏 ,

𝐽𝐶𝜏

𝑖 ∈ [0, 𝑁 − 1],
𝜏 ∈ [1, 𝜏max].

(15)

There are a total of 𝑀 = 𝑁 × 𝜏max state constraints in (15).
In this paper, we refer to (15) as the constrained policy im-

J. Duan et al.: Preprint submitted to Elsevier

Page 4 of 15

Preprint

provement. Then we develop the constrained generalized
policy iteration (CGPI) framework, which builds on GPI by
replacing the policy improvement process in Algorithm 1
with (15). The pseudo-code of CGPI is shown in Algorithm
2.

Algorithm 2 CGPI Framework

Initial with arbitrary 𝜃0, 𝜔0, learning rates 𝛼𝑐
Initialize iteration index 𝐾 = 0
repeat

, 𝑉𝜔𝐾

Rollout 𝑁 steps from 𝑥𝑘 ∈ Ω with policy 𝜋𝜃𝐾
Receive and store 𝑥𝑘+𝑖, 𝑖 ∈ [1, 𝑁]
Policy evaluation:
Calculate 𝐺(𝑥𝑘, 𝜋𝜃𝐾
), ∇𝜔𝐾
Update value function using (12)
Constrained policy improvement:
Construct 𝐽 (𝜃) under 𝑉𝜔𝐾+1
Update policy using (15)
𝐾 = 𝐾 + 1
until Convergence

using (10)

𝐿(𝜔𝐾 ) using (7), (9)

In Appendix A.1, we prove the convergence and global
optimality of a policy iteration variant of Algorithm 2 based
on tabular setting. Besides, as described in Appendix A.2
and A.3, the convergence results can be further extended to
the case of function approximation and Algorithm 2.

Remark 1. The state constraints need to be reasonable to
ensure (15) is feasible. For practical applications, the state
constraints usually come from the physical limitations or bound-
aries of controlled systems [25, 31, 45]. Taking vehicle con-
trol in the path-tracking task as an example, in addition to
considering the tracking performance, certain state func-
tions of the vehicle must be constrained to the stability zone
to prevent vehicle instability problems [25].

3.2. Approximate Solution

For policies with high-dimensional parameter spaces 𝜃 ∈
ℝ𝑠, directly solving (15) may be intractable due to the com-
putational cost and the nonlinear characteristics of NNs and
dynamics. Local linearization is an eﬀective trick to deal
with this situation [23, 34, 35]. Firstly, we can linearize
the objective function and state constraints at the 𝐾th itera-
tion around current policy 𝜋(𝑥𝑘; 𝜃𝐾 ) using Taylor’s expan-
sion theorem. To ensure the approximation accuracy, policy
𝜋(𝑥𝑘; 𝜃𝐾+1) must be in a small neighborhood of 𝜋(𝑥𝑘; 𝜃𝐾 ).
This means that we need to add a policy constraint to (15) to
avoid excessive policy update.

Inspired by [23], one eﬀective way to limit the policy
change is to constrain the diﬀerence between the new policy
𝜋(𝑥𝑘; 𝜃𝐾+1) and the old policy 𝜋(𝑥𝑘; 𝜃𝐾 ). Firstly, we deﬁne
the following function

𝐷𝜋(𝜃; 𝜃𝐾 ) ≐ 𝔼𝑥𝑘∼𝑑𝑥

[‖𝜋(𝑥𝑘; 𝜃) − 𝜋(𝑥𝑘; 𝜃𝐾 )‖

2
2]

to measure the diﬀerence between 𝜋(𝑥𝑘; 𝜃) and 𝜋(𝑥𝑘; 𝜃𝐾 ).
Then the following policy constraint, also known as the trust

region constraint, can be constructed:

𝐷𝜋(𝜃; 𝜃𝐾 ) ≤ 𝛿,

(16)

where 𝛿 ∈ ℝ+ is the trust region boundary. In this case, the
policy update step is positively correlated with 𝛿. Then, (15)
can be adapted to a trust region version:

𝜃𝐾+1 = arg min
𝜃

𝐽 (𝜃)

𝑠.𝑡. 𝑥𝑘+𝑖+1 = 𝑓 (𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖; 𝜃)),
(𝑥𝑘+𝑖+1) ≤ 𝑏𝜏 ,

𝐽𝐶𝜏
𝐷𝜋(𝜃; 𝜃𝐾 ) ≤ 𝛿.

𝑖 ∈ [0, 𝑁 − 1],
𝜏 ∈ [1, 𝜏max],

(17)

For a small step size 𝛿, the objective function 𝐽 (𝜃) and

in the 𝐾th iteration can be well-approximated

state functions 𝐽𝐶𝜏
by linearizing around current policy 𝜋(𝜃𝐾 ) using Taylor’s
expansion theorem. Denoting Δ𝜃 = 𝜃 − 𝜃𝐾 and 𝐽𝐶𝜏,𝑖
=
𝐽𝐶𝜏

(𝑥𝑘+𝑖+1), it follows that:

𝐽 (𝜃) ≈ 𝐽 (𝜃𝐾 ) + (∇𝜃𝐽 (𝜃)|
|
|𝜃=𝜃𝐾

)⊤Δ𝜃,

and

𝐽𝐶𝜏

(𝑥𝑘+𝑖+1) ≈ 𝐽𝐶𝜏

(𝑥𝑘+𝑖+1)|𝜃=𝜃𝐾

+

(
∇𝜃𝐽𝐶𝜏,𝑖

|
|
|𝜃=𝜃𝐾

)⊤

Δ𝜃,

where

∇𝜃𝐽𝐶𝜏,𝑖

= 𝜙𝑖+1

𝜕𝐽𝐶𝜏

(𝑥𝑘+𝑖+1)

𝜕𝑥𝑘+𝑖+1

,

𝑖 ∈ [0, 𝑁 − 1].

In addition, since 𝐷𝜋(𝜃; 𝜃𝐾 ) and its gradient are both zero at
𝜃 = 𝜃𝐾 , the trust region constraint is well-approximated by
second-order Taylor expansion:

𝐷𝜋(𝜃; 𝜃𝐾 )

≈ 𝐷𝜋(𝜃𝐾 ; 𝜃𝐾 ) +

=

1
2

Δ𝜃⊤𝐻Δ𝜃,

(
∇𝜃𝐷𝜋(𝜃; 𝜃𝐾 )|
|
|𝜃=𝜃𝐾

)⊤

Δ𝜃 +

1
2

Δ𝜃⊤𝐻Δ𝜃

where 𝐻 ∈ ℝ𝑠×𝑠 is the Hessian of 𝐷𝜋 with respect to 𝜃, i.e.,
𝐻𝑖,𝑗 = 𝜕2𝐷𝜋 (𝜃;𝜃𝐾 )
. Since 𝐷𝜋(𝜃; 𝜃𝐾 ) ≥ 0 for ∀𝜃 ∈ ℝ𝑠,
𝐻 is always positive semi-deﬁnite. In keeping with other
work in the literature [34, 35], we will assume it to be positive-
deﬁnite in the following.

|
|
|𝜃=𝜃𝐾

𝜕𝜃𝑖𝜕𝜃𝑗

Denoting 𝑔 = ∇𝜃𝐽 ∕‖∇𝜃𝐽 ‖2, 𝑐𝑗 = ∇𝜃𝐽𝐶𝜏,𝑖

∕‖∇𝜃𝐽𝐶𝜏,𝑖‖2
,
and 𝑧𝑗 = (𝐽𝐶𝜏,𝑖|𝜃=𝜃𝐾
, where 𝑗 = (𝜏 − 1) ×
− 𝑏𝜏 )∕‖∇𝜃𝐽𝐶𝜏,𝑖‖2
𝑁 + 𝑖 + 1 ∈ [1, 𝑀]. With 𝐶 ≐ [𝑐1, 𝑐2, ..., 𝑐𝑀 ] ∈ ℝ𝑠×𝑀 and
𝑧 ≐ [𝑧1, 𝑧2, ..., 𝑧𝑀 ]⊤, the approximation to (17) is:

min
Δ𝜃
𝑠.𝑡.

𝑔⊤Δ𝜃

𝑧 + 𝐶 ⊤Δ𝜃 ≤ 0,
1
2

Δ𝜃⊤𝐻Δ𝜃 ≤ 𝛿.

(18)

Denoting the optimal solution of (18) as Δ𝜃∗, the update rule
for the constrained policy improvement process is

𝜃𝐾+1 = Δ𝜃∗ + 𝜃𝐾 .

J. Duan et al.: Preprint submitted to Elsevier

Page 5 of 15

Preprint

Although (18) is a convex constrained optimization prob-
lem, directly solving it will take lots of computation time and
resources because the dimension of variables Δ𝜃 ∈ ℝ𝑠 is
very large (usually over 10 thousand). Since (18) is convex,
it can also be solved using the dual method when feasible.
The Lagrange function of (18) can be expressed as

𝐿𝑎(Δ𝜃, 𝜆, 𝜈) = 𝑔⊤Δ𝜃+𝜆(

1
2

Δ𝜃⊤𝐻Δ𝜃−𝛿)+𝜈⊤(𝑧+𝐶 ⊤Δ𝜃),

where 𝜆 ∈ ℝ and 𝜈 ∈ ℝ𝑀 . Then the dual to (18) is

max
𝜆≥0,𝜈≥0

min
Δ𝜃

𝐿𝑎(Δ𝜃, 𝜆, 𝜈).

(19)

The gradient of 𝐿𝑎 with respect to parameters Δ𝜃 can be
calculated as

∇Δ𝜃𝐿𝑎(Δ𝜃, 𝜆, 𝜈) = 𝑔 + 𝜆𝐻Δ𝜃 + 𝐶𝜈.

When ∇Δ𝜃𝐿𝑎(Δ𝜃, 𝜆, 𝜈) = 0, we have

Δ𝜃 = −

𝐻 −1(𝑔 + 𝐶𝜈)
𝜆

,

𝜆 > 0.

(20)

By taking (20) into (19), the dual to (18) can be expressed
as

max
𝜆>0,𝜈≥0

−

1
2𝜆

(𝜇 + 𝜈⊤𝑆𝜈 + 2𝜈⊤𝑟) − 𝜆𝛿 + 𝜈⊤𝑧,

(21)

where 𝜇 = 𝑔⊤𝐻 −1𝑔, 𝑆 = 𝐶 ⊤𝐻 −1𝐶, 𝑟 = 𝐶 ⊤𝐻 −1𝑔. Let

𝐿(𝜆, 𝜈) ≐ −

1
2𝜆

(𝜇 + 𝜈⊤𝑆𝜈 + 2𝜈⊤𝑟) − 𝜆𝛿 + 𝜈⊤𝑧,

then we can rewrite (21) with

min
𝜆>0,𝜈≥0

−𝐿(𝜆, 𝜈).

Problem (22) is a bound-constrained convex optimiza-
tion problem with only 𝑀 + 1 variables, which is also equal
to the number of constraints in (17) but much smaller than
the dimension of Δ𝜃 ∈ ℝ𝑠. Therefore, compared with (17),
the optimal solution of (22) can be solved more easily and
eﬃciently by using oﬀ-the-shelf methods such as L-BFGS-
B and truncated Newton method [46, 47]. Supposing 𝜆∗, 𝜈∗
are the optimal solutions to (22), 𝜃𝐾+1 can be updated as

𝜃𝐾+1 = −

𝐻 −1(𝑔 + 𝐶𝜈∗)
𝜆∗

+ 𝜃𝐾 .

(23)

Note that for a high-dimensional policy 𝜋(𝑥𝑘; 𝜃), it is
prohibitively costly to compute 𝐻 and invert 𝐻 −1, which
poses a huge challenge for computing 𝐻 −1𝑔 and 𝐻 −1𝐶 in
(22). In this paper, we approximately compute them using
the conjugate gradient method without reliance on forming
the full matrix 𝐻 or 𝐻 −1 [23, 34].

3.3. Feasibility

On the one hand, the initial policy 𝜋𝜃0

may be infeasi-
ble. On the other hand, due to the approximation errors in-
duced by linearization, the optimal solution Δ𝜃∗ of (18) at
the 𝐾th iteration may be a bad update, and then a new pol-
icy 𝜋𝜃𝐾+1
that fails to satisfy state constraints may be pro-
duced. This may cause the optimization problem (18) of
the 𝐾 + 1th iteration to be infeasible. In other words, the
feasible region of (18) may be empty in some cases, i.e.,
Θ𝐴 ∩ Θ𝐵 = ∅, where Θ𝐴 = {Δ𝜃 ∶ 𝑧 + 𝐶 ⊤Δ𝜃 ≤ 0}, and
Θ𝐵 = {Δ𝜃 ∶ 1
2

Δ𝜃⊤𝐻Δ𝜃 ≤ 𝛿}.

Hence, before solving the dual problem (22), we check
whether the feasible region is empty by calculating the min-
imum trust region boundary, which makes the trust region
intersect with the intersection of half-planes of all linear con-
straints:

min
Δ𝜃
𝑠.𝑡.

Δ𝜃⊤𝐻Δ𝜃

1
2
𝑧 + 𝐶 ⊤Δ𝜃 ≤ 0.

(24)

Denoting the optimal solution to (24) as Δ𝜃min, then the
minimum trust region boundary that makes (18) feasible is
𝛿min = 1
⊤𝐻Δ𝜃min, and it is clear that
2

Δ𝜃min

{Θ𝐴 ∩ Θ𝐵 = ∅,
≠ ∅,
Θ𝐴 ∩ Θ𝐵

𝛿min > 𝛿,
≤ 𝛿.
𝛿min

(25)

The value 𝛿min can be eﬃciently obtained by solving the fol-
lowing dual problem:

max
𝜈≥0

−

𝜈⊤𝑆𝜈
2

+ 𝜈⊤𝑧.

(26)

𝛿min = −

𝜈†⊤
𝑆𝜈†
2

+ 𝜈†⊤

𝑧.

It is known from (25) that the magnitude of the value
𝛿 directly aﬀects the feasibility of (18). Denoting the ex-
≤ 𝛿𝑎, we can
pected trust region boundary as 𝛿𝑎, if 𝛿min
directly solve (22) with 𝛿 = 𝛿𝑎. For the infeasible case, i.e.,
𝛿min > 𝛿𝑎, a recovery method is needed to calculate a rea-
sonable policy update. By introducing the recovery trust re-
gion boundary 𝛿𝑏, which is slightly greater than 𝛿𝑎, we pro-
pose two recovery rules according to the value of 𝛿min: 1) If
≥ 𝛿min > 𝛿𝑎, we solve (22) with 𝛿 = 𝛿𝑏 for 𝜆∗ and 𝜈∗;
𝛿𝑏
2) If 𝛿min > 𝛿𝑏, we recover the policy by adding the state
constraints as a penalty to the original objective function:

min
Δ𝜃

𝑠.𝑡.

(

(1 − 𝜂)𝑔 + 𝜂

)⊤

Δ𝜃

𝛼𝑗𝑐𝑗

𝑀
∑

𝑗=1

1
2

Δ𝜃⊤𝐻Δ𝜃 ≤ 𝛿𝑏,

(27)

where 𝜂 is the hyper-parameter that trades oﬀ the importance
between the original objective function and the penalty term,

Suppose 𝜈† is the optimal solution of (26), then

(22)

J. Duan et al.: Preprint submitted to Elsevier

Page 6 of 15

𝛼𝑗 is the weight of the state constraint corresponding to 𝑐𝑗,
which is calculated by:

Preprint

𝛼𝑗 =

𝑝𝑗𝑒𝑧𝑗
𝑗=1 𝑝𝑗𝑒𝑧𝑗

∑𝑀

,

where 𝑝𝑗 = 5 if 𝑧𝑗 > 0, 𝑝𝑗 = 1 otherwise, which penalizes
violations of the corresponding state constraint. Deﬁning
𝑔𝑝
𝑗=1 𝛼𝑗𝑐𝑗, the dual to (27) can be expressed
as:

≐ (1−𝜂)𝑔+𝜂 ∑𝑀

max
𝜆>0

−

𝜇𝑝
2𝜆

− 𝜆𝛿𝑏,

(28)

where 𝜇𝑝 = 𝑔𝑝
policy recovery rule:

⊤𝐻 −1𝑔𝑝. In this case, we can easily ﬁnd the

√

𝜃𝐾+1 = −

2𝛿𝑏
𝜇𝑝

𝐻 −1𝑔𝑝 + 𝜃𝐾 .

(29)

3.4. Constrained Adaptive Dynamic Programming
By introducing the trust region constraint, approxima-
tion solution method, and recovery rules, we adapt Algo-
rithm 2 to Algorithm 3, called constrained adaptive dynamic
programming (CADP). Besides, we also incorporate the par-
allel exploring trick widely used in RL [20, 48, 17] to accel-
erate training and improve stability (See Fig. 1). In particu-
lar, we use multiple parallel agents to explore diﬀerent state
spaces, thereby removing correlations in the training set. All
the state constraints of these parallel agents are stored in the
constraints buﬀer. Due to the computational burden caused
by estimating the matrices 𝐶, 𝑆 and solving (22), the speed
of the policy optimization process will decrease as the num-
ber of state constraints increases. For each iteration, we only
consider 𝑀 state constraints randomly selected from the con-
straints buﬀer.

Remark 2. The performance of ADP approaches proposed
in [12, 29, 36, 37, 41] that use single NN heavily relies on
the quality of hand-crafted features. It is usually hard to de-
sign such features for high-dimensional nonlinear systems.
Nevertheless, the update rules (12) and (23) of the proposed
CADP algorithm are applicable to most approximate func-
tions, such as multi-layer NNs. Hence, according to Lemma
3 and 4, by employing multi-layer NNs to represent the pol-
icy and value function, CADP can directly learn a map from
system states to control inputs and value function, thereby
relaxing the need for hand-crafted features.

Remark 3. The proposed CADP method relies on the knowl-
edge of the system dynamics 𝑓 (𝑥, 𝑢). For controlled systems
with unknown dynamics, we can use supervised learning to
learn an approximated model from data, such as an NN-
based model. In recent years, many algorithms based on the
learned NN-dynamics have been proposed [49, 50]. Simi-
larly, given a real system with unknown dynamics, we can
ﬁrst learn an NN-based model, and then apply the CADP
algorithm to ﬁnd the nearly optimal policy.

Figure 1: CADP diagram. The value function and policy are
approximated by two NNs, called value network and policy net-
work, respectively. The value network is updated by minimiz-
ing loss function (8). If problem (18) is feasible, we update the
policy with its optimal solution; otherwise, we update with the
recovery rules. Multiple parallel agents are employed to explore
diﬀerent parts of the state space. All the state constraints of
these parallel agents are stored in the constraints buﬀer. For
each iteration, we only consider 𝑀 state constraints randomly
selected from the constraints buﬀer.

Algorithm 3 CADP Algorithm

Initial with arbitrary 𝜃0, 𝜔0, learning rates 𝛼𝑐
Initialize iteration index 𝐾 = 0
repeat

𝐿(𝜔𝐾 ) using (7), (9)

Rollout 𝑁 steps from 𝑥𝑘 ∈ Ω with policy 𝜋𝜃𝐾
Receive and store 𝑥𝑘+𝑖, 𝑖 ∈ [1, 𝑁]
Policy evaluation:
Calculate 𝐺(𝑥𝑘, 𝜋𝜃𝐾
), ∇𝜔𝐾
Update value function using (12)
Constrained policy improvement:
Solve dual problem (26) for 𝛿min
if 𝛿min

, 𝑉𝜔𝐾

≤ 𝛿𝑏 then
{𝛿𝑎,
𝛿𝑏,

𝛿 =

𝛿min

≤ 𝛿𝑎
else

Solve dual problem (22) for 𝜆∗, 𝜈∗
Update policy with (23)

else

Update policy with (29)

end if
𝐾 = 𝐾 + 1
until Convergence

Remark 4. For the proposed CADP algorithm, according
to (9) and (18), the system 𝑓 (𝑥, 𝑢) can be an arbitrary an-
alytic function only if it is diﬀerentiable, i.e., 𝜕𝑓 (𝑥,𝑢)
and
𝜕𝑓 (𝑥,𝑢)
𝜕𝑥

are available. Therefore, diﬀerent from [12, 29, 30,
31, 32, 36, 37, 41, 42] that are subject to input-aﬃne systems
since the policy needs to be analytically represented by the
value function, CADP is applicable to arbitrary nonlinear
systems with nonaﬃne saturated inputs. Nonlinear nonaﬃne
systems are very common in practical applications, such as
vehicle dynamics [25] and NN-based models learned from

𝜕𝑢

J. Duan et al.: Preprint submitted to Elsevier

Page 7 of 15

Value NetworkConstraints BufferOptimizerRecovery RuleFeasibility CheckParallel AgentsTD Error Policy NetworkUpdatexxlxxYNuUpdatePreprint

data [49, 50].

Remark 5. Given a practical optimal control problem with
known dynamics, we only need to formulate the policy opti-
mization process as (17). Then, CADP can be directly used
to ﬁnd the nearly optimal policy without reliance on hand-
crafted features. The learned oﬄine policy maps the states to
the corresponding nearly optimal control inputs, which can
be directly applied to the controlled system to realize online
control.

4. Simulation
4.1. Problem Description

To evaluate the performance of the CADP algorithm,
we choose the vehicle lateral and longitudinal control in the
path-tracking task as an example. It is a nonlinear and non-
aﬃne system control problem with state constraints [25].
The control objective is to maximize the vehicle speed, while
maintaining a small tracking error and ensuring that the ve-
hicle stays within the stability region. The system states and
control inputs of this problem are listed in Table 3, and the
vehicle parameters are listed in Table 4.
In keeping with
other studies [25, 51, 52], we assume the states in Table 3 are
observable. Note that the system frequency used for simula-
tion is diﬀerent from the sampling frequency 𝑓 . The vehicle
is controlled by a saturated actuator, where ̇𝜉 ∈ [−0.35, 0.35]
and ̇𝑎 ∈ [−2, 2]. According to the continuous-time vehicle
dynamics given in [25, 51, 52], the corresponding discrete-
time dynamics can be obtained using forward Euler methods
[51, 52], which can be described by:

𝑥 =

𝑣𝑦
⎡
𝑟
⎢
⎢
𝑣𝑥
⎢
𝜙
⎢
𝑦
⎢
⎢
𝜉
⎢
𝑎
⎣

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

, 𝑢 =

]

[ ̇𝜉
̇𝑎

, 𝑥𝑘+1 =

− 𝑣𝑥𝑟

𝐹𝑦f cos 𝜉+𝐹𝑦r
𝑚

⎤
⎡
𝑑f 𝐹𝑦f cos 𝜉−𝑑r 𝐹𝑦r
⎥
⎢
⎥
⎢
𝐼𝑧
⎥
⎢
𝑎 + 𝑣𝑦𝑟
⎥
⎢
𝑟
⎥
⎢
𝑣𝑥 sin 𝜙 + 𝑣𝑦 cos 𝜙
⎥
⎢
̇𝜉
⎥
⎢
⎥
⎢
̇𝑎
⎦
⎣

1
𝑓

+𝑥𝑘,

where 𝐹𝑦f and 𝐹𝑦r are the lateral tire forces of the front and
rear tires respectively. The lateral tire forces are approxi-
mated according to the Fiala tire model:

𝐹𝑦† = −sgn(𝛼†) ∗ min
{ |
|
𝐶† tan 𝛼†
|
|
|
|

† (tan 𝛼†)2
27(𝜇†𝐹𝑧†)2

( 𝐶 2

−

tan 𝛼†|
𝐶† |
|
|
3𝜇†𝐹𝑧†

+ 1

)|
|
|
|
|
|

}
,

𝜇†𝐹𝑧†|
, |
|
|

by:

where 𝛼† is the tire slip angle, 𝐹𝑧† is the tire load, 𝜇† is the
lateral friction coeﬃcient, and the subscript † ∈ {f, r} repre-
sents the front or rear tires. The slip angles can be calculated
from the geometric relationship between the front/rear axle
and the center of gravity (CG):

𝛼f = arctan(

𝑣𝑦 + 𝑑f 𝑟
𝑣𝑥

) − 𝜉,

𝛼r = arctan(

𝑣𝑦 − 𝑑r𝑟
𝑣𝑥

).

J. Duan et al.: Preprint submitted to Elsevier

Table 3
State and Control Input

Explanation
Lateral velocity at center of gravity (CG)
Yaw rate
Longitudinal velocity at CG
Yaw angle between vehicle & trajectory
Distance between CG & trajectory
Front wheel angle
Longitudinal acceleration
Rate of change of 𝑎
Rate of change of 𝜉

Symbol Unit
𝑣𝑦
[m/s]
𝑟
[rad/s]
𝑣𝑥
[m/s]
𝜙
[rad]
𝑦
[m]
𝜉
[rad]
[m/s2]
𝑎
[m/s3]
̇𝑎
̇𝜉
[rad/s]

Table 4
Vehicle Parameters

Explanation
Front wheel cornering stiﬀness
Rear wheel cornering stiﬀness
Distance from CG to front axle
Distance from CG to rear axle
Mass
Polar moment of inertia at CG
Tire-road friction coeﬃcient
Sampling frequency
Simulation frequency

Symbol Value
𝐶f
𝐶r
𝑑f
𝑑r
𝑚
𝐼𝑧
𝜇
𝑓

-88000 [N/rad]
-94000 [N/rad]
1.14 [m]
1.40 [m]
1500 [kg]
2420 [kg⋅m2]
1.0
40 [Hz]
200 [Hz]

Let 𝛼max,† represent the tire slip angle when the tire fully-
sliding behavior occurs, calculated as:

𝛼max,† =

3𝜇†𝐹𝑧†
𝐶†

.

Assuming that the rolling resistance is negligible, the lat-

eral friction coeﬃcient of the front/rear wheel is:

√

(𝜇𝐹𝑧†)2 − (𝐹𝑥†)2
𝐹𝑧†

,

𝜇† =

where 𝐹𝑥f and 𝐹𝑥r are the longitudinal tire forces of the front
and rear tires respectively, calculated as:

{𝐹𝑥f , 𝐹𝑥r} =

{{0, 𝑚𝑎},
𝑚𝑎
𝑚𝑎
2
2

{

,

𝑎 ≥ 0,

𝑎 < 0.

},

The loads on the front and rear tires can be approximated

𝐹𝑧f =

𝑑r
𝑑f + 𝑑r

𝑚𝑔, 𝐹𝑧r =

𝑑f
𝑑f + 𝑑r

𝑚𝑔.

To ensure vehicle stability, the yaw rate 𝑟 at the CG and
the slip angles 𝛼† should be subject to the following con-
straints:

−𝑟max
−𝛼max,f
−𝛼max,r

≤𝑟 ≤ 𝑟max,
≤𝛼f
≤𝛼r

≤ 𝛼max,f ,
≤ 𝛼max,r,

(30)

Page 8 of 15

Preprint

where 𝑟max = 𝜇r 𝑔
𝑣𝑥
The utility function is

.

𝑙(𝑥, 𝑢) =
2(𝑣𝑥 − 30)2 + 80𝑦2 + 40𝑟2 + 100(2𝜙2 + 𝜉2 + ̇𝜉2) + 𝑎2 + ̇𝑎2
2000

.

Hence, the policy optimization problem of this example is
given by

{ 𝑁−1
∑

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝑢𝑘+𝑖) + 𝑉 (𝑥𝑘+𝑁 ; 𝜔)

}

min
𝜃

𝔼
𝑥𝑘∼𝑑𝑥

𝑖=0
𝑠.𝑡. 𝑥𝑘+𝑖+1 = 𝑓 (𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖; 𝜃)),

𝑖 ∈ [0, 𝑁 − 1],

|
|
|

|
|
|

≤

≤ 𝑔,

|
|
|𝑘+𝑖+1

|
|
|𝑘+𝑖+1

𝑟𝑣𝑥
𝜇r
3𝐹𝑧f
𝛼f
𝐶f
𝜇𝑓
3𝐹𝑧r
𝛼r
|
|
|
|
𝐶r
𝜇r
|𝑘+𝑖+1
|
𝐷𝜋(𝜃; 𝜃𝐾 ) ≤ 𝛿,

≤

,

,

𝛼r
𝜇r

𝛼f
𝜇𝑓

𝑟𝑣𝑥
𝜇r

, |
|
|

|
|
|𝑘+𝑖+1

|
|
|𝑘+𝑖+1

|
|
|𝑘+𝑖+1

and |
where |
are the state con-
|
|
|
|
straint functions of state 𝑥𝑘+𝑖+1 bounded above by 𝑔, 3𝐹𝑧f
𝐶f
and 3𝐹𝑧r
respectively. It is clear that the form of this prob-
𝐶r
lem is the same as (17), which means that we can directly
train the vehicle control policy using the proposed CADP
algorithm.

4.2. Algorithm Details

In this paper, the value function and policy are repre-
sented by fully-connected NNs, which have the same archi-
tecture except for the output layers. For each network, the
input layer is composed of the states, followed by 5 hidden
layers using exponential linear units (ELUs) as activation
functions, with 32 units per layer. The output of the value
network is a linear unit, while the output layer of the policy
network is set as a 𝑡𝑎𝑛ℎ layer with two units, multiplied by
the matrix [0.35, 2] to confront bounded controls. We use
Adam method to update the value network 𝑉 (𝑥; 𝜔) with the
learning rate of 8 × 10−4. Other hyper-parameters of this
problem are shown in Table 5.

Table 5
Hyper-parameters

Parameters
agent number
prediction horizon
number of state constraints
discount factor
trust region boundary
recovery trust region boundary
penalty factor

Symbol Value

𝑁
𝑀
𝛾
𝛿𝑎
𝛿𝑏
𝜂

256
30
10
0.98
0.0033
0.0063
0.8

We compare the CADP algorithm with four other al-
gorithms, namely GPI, TRADP (i.e., GPI with trust region

Figure 2: Training performance. Solid lines are average values
over 20 runs. Shaded regions correspond to 95% conﬁdence
interval.

constraint), penalty TRADP (P-TRADP, i.e., update policy
network by directly solving (27) with 𝜂 = 0.2, 0.4, 0.6 re-
spectively), and CPO (constrained policy optimization [34]).
Note that TRADP can be considered as a special case of P-
TRADP, in which 𝜂 = 0. The value 𝜂 plays a diﬀerent role
in CADP and P-TRADP algorithms. In CADP , 𝜂 in (27)
works only when 𝛿min > 𝛿𝑏. Therefore, we can take a rela-
tively large 𝜂 to make (18) feasible as soon as possible.

4.3. Result Analysis

We train 20 diﬀerent runs of each algorithm with dif-
ferent random seeds, with evaluations every 100 iterations.
Each evaluation measures the policy performance by calcu-
lating the undiscounted accumulated cost function of 1000
steps (25s) during the simulation period starting from a ran-
dom initialized state. The learning curves are shown in Fig.
2. Basically, all algorithms converge within 3000 iterations.
Fig. 3 compares the performance and constraint satisfac-
tion of each algorithm. As shown in Fig. 3a, CADP matches
or outperforms all other baseline ADP algorithms in policy
performance. Fig. 3b, 3c and 3d show the maximum value
of |𝑟| − 𝑟max, |𝛼r| − 𝛼max,r, and |𝛼f | − 𝛼max,f for each simu-
lation, respectively. Take Fig. 3b as an example, from (30),
|𝑟| − 𝑟max > 0 indicates the corresponding state constraint
is violated. Results show that only CADP meets all three
state constraints during the simulation. For TRADP and P-
TRADP algorithms, the cumulative cost increases with the
penalty factor 𝜂. This is because the learned policy needs
to balance the cost term and constraint term. In particular,
when 𝜂 = 0.6, the accumulated cost of the learned policy is
almost twice that of CADP because the constraint term ac-
counts for too much of the objective function in (27). Even
so, for the P-TRADP algorithm with 𝜂 = 0.6, the situation
in which the controlled system violates the constraints still
exists. CPO considers the state constraint by limiting the ex-
pected discount cumulative state function, which is approx-
imated by an additional NN. The transformed constraints of

J. Duan et al.: Preprint submitted to Elsevier

Page 9 of 15

00.511.522.53(×103)Iteration102103104105Accumulated CostMethodCADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPOPreprint

(a)

(b)

(c)

(d)

Figure 3: Algorithm comparison. Each box plot is drawn based on values of 20 runs. Values greater than 0 (marked as red
dashed lines) indicate violations of the corresponding state constraints. (a) Policy performance. (b) Maximum value of |𝑟| − 𝑟max
per simulation. (c) Maximum value of |𝛼r| − 𝛼max,r per simulation. (d) Maximum value of |𝛼f | − 𝛼max,f per simulation.

CPO are not equivalent to the original constraints given in
(14), so the satisfaction of state constraints are still not guar-
anteed. CADP deals with the state constraints by solving
(17), which enables it to ﬁnd the optimal policy from all fea-
sible policies that satisfy the state constraints.

The typical simulation results by applying GPI, P-TRADP
(𝜂 = 0.6) and CADP are presented in Fig. 4. As shown in
Fig. 4a, 4d and 4g, the policy learned by GPI is too aggres-
sive, resulting in large acceleration 𝑎, front wheel angle 𝜉 and
velocity 𝑣𝑥, thus violating state constraints 𝑟max and 𝛼max,r.
In contrast, the policy learned by P-TRADP (𝜂 = 0.6) is
conservative in the control of 𝑎 and 𝜉, leading to large accu-
mulated cost (see Fig. 4b, 4e and 4h). As a comparison, the
policy learned by CADP has a good balance between track-
ing performance and constraint satisfaction. This example
demonstrates the eﬀectiveness of CADP in solving the non-
aﬃne nonlinear optimal control problems with multiple state
constraints.

5. Conclusions

This paper proposes a constrained adaptive dynamic pro-
gramming (CADP) algorithm to solve optimal control prob-

lems with multiple state constraints. The proposed algo-
rithm is applicable to general nonlinear systems with non-
aﬃne and saturated control inputs. Firstly, a constrained
generalized policy iteration (CGPI) framework is developed
to handle state constraints by transforming the traditional
policy improvement process into a constrained policy op-
timization problem. The proposed CADP algorithm is an
actor-critic variant of CGPI, in which both policy and value
functions are approximated by multi-layer NNs to directly
map the system states to control inputs and value function re-
spectively, which relaxes the need for hand-crafted features.
Due to the computational cost and the nonlinear character-
istics of NNs, it may be intractable to directly solve the con-
strained policy optimization problem. For calculation con-
venience, the proposed CADP linearizes the constrained op-
timization problem locally into a quadratically constrained
linear programming problem, and then obtains the optimal
update of the policy network by solving its dual problem.
Meanwhile, a trust region constraint is added to prevent ex-
cessive policy update, thus ensuring linearization accuracy.
We determine the feasibility of the policy optimization prob-
lem by calculating the minimum trust region boundary and
update the policy using two recovery rules when infeasible.

J. Duan et al.: Preprint submitted to Elsevier

Page 10 of 15

CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO50100150200250300350Accumulated CostCADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.40.20.00.20.4|r|rmax [rad]CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.20.10.00.10.20.3|r|max,r [rad]CADPGPITRADPP-TRADP(=0.2)P-TRADP(=0.4)P-TRADP(=0.6)CPO0.250.200.150.100.050.00|f|max,f [rad]Preprint

(b)

(e)

(h)

(a)

(d)

(g)

(c)

(f)

(i)

Figure 4: Simulation results. (a) State curves of GPI. (b) State curves of P-TRADP (𝜂 = 0.6). (c) State curves of CADP.
(d) Tire slip angles curves of GPI. (e) Tire slip angles curves of P-TRADP (𝜂 = 0.6). (f) Tire slip angles curves of CADP. (g)
Trajectory of GPI. (h) Trajectory of P-TRADP (𝜂 = 0.6). (i) Trajectory of CADP.

We apply CADP and six other baseline algorithms to
the vehicle control problem in the path-tracking task. Re-
sults show that CADP matches or outperforms all other base-
line algorithms in policy performance without violating any
In the future, we
state constraints during the simulation.
will extend the proposed algorithm to uncertain systems by
combining other techniques, such as robust control [53] and
fuzzy adaptive control [54, 55].

A. Appendix
A.1. Convergence Proofs of Constrained Policy

Iteration in the Tabular Case

We refer to the combination of policy iteration (PI), a
special case of GPI, and constrained policy improvement
(15) as constrained policy iteration (CPI). We ﬁrst discuss
the convergence and optimality of CPI. In a version of CPI
for a tabular setting, we maintain two tables to estimate pol-
icy 𝜋 and the corresponding value function 𝑉 𝜋 respectively.
The pseudo-code of CPI in the tabular case can be summa-
rized as Algorithm 4.

Next, the convergence property of Algorithm 4 will be

established. As the iteration index 𝐾 → ∞, we will show
that the optimal value function and optimal policy can be
achieved using Algorithm 4. Before the main theorem, some
lemmas are necessary at this point. Lemma 1 shows the con-
vergence of policy evaluation, which has been proved and
described in previous studies.

Lemma 1. (Policy Evaluation [11]). For a ﬁxed policy 𝜋𝐾 ,
consider the Bellman backup rule in (31) and a mapping
𝑉𝐾,0(𝑥𝑘) ∶ ℝ𝑛 → ℝ, then the sequence 𝑉𝐾,𝑞(𝑥𝑘) will con-
verge to the value function 𝑉 𝜋𝐾 (𝑥𝑘) for ∀𝑥𝑘 ∈ Ω as 𝑞 → ∞.

PROOF. Note that throughout our computation, the value func-
tion 𝑉 (𝑥𝑘) is always bounded for ∀𝑥𝑘 ∈ Ω since 𝛾 ∈ (0, 1)
and 𝑙(𝑥𝑘, 𝑢𝑘) is bounded. Deﬁne a maximum norm on the
value function as

‖𝑉𝐾,𝑞+1(𝑥𝑘)−𝑉𝐾,𝑞(𝑥𝑘)‖∞

≐ max

𝑥𝑘∈Ω |𝑉𝐾,𝑞+1(𝑥𝑘)−𝑉𝐾,𝑞(𝑥𝑘)|.

Suppose Δ = ‖𝑉𝐾,1(𝑥) − 𝑉𝐾,0(𝑥)‖∞. According to (7) and

J. Duan et al.: Preprint submitted to Elsevier

Page 11 of 15

0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,φ,r,ξvx [m/s]φ [°]ξ [°]r [° / s]rmax [° / s]0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,φ,r,ξvx [m/s]φ [°]ξ [°]r [° / s]rmax [° / s]0510152025t [s]-4-2024y,a,vyy[m]a [m2/s]vy [m/s]-40-2002040vx,φ,r,ξvx [m/s]φ [°]ξ [°]r [° / s]rmax [° / s]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]0510152025t [s]-0.3-0.2-0.100.10.20.3Tire slip anglesaf [rad]ar [rad]amax,f [rad]amax,r [rad]050100150200250300350400450500550600Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected Trajectory050100150200250300350400Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected Trajectory050100150200250300350400450Longitudinal position [m]-15-10-5051015Lateral position [m]Actual TrajectoryExpected TrajectoryPreprint

Algorithm 4 CPI Algorithm: tabular case

Initial with arbitrary 𝜋0, 𝑉0
Initialize iteration index 𝐾 = 0
Given an arbitrarily small positive 𝜖
repeat

Rollout 𝑁 steps from ∀𝑥𝑘 ∈ Ω with policy 𝜋𝐾
Receive and store 𝑥𝑘+𝑖, 𝑖 ∈ [1, 𝑁]
Policy evaluation:
repeat
Calculate 𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾,𝑞) using (7)
Update value function using

works for ∀𝑥𝑘 ∈ Ψ, it is deﬁned as a feasible policy, denoted
by 𝜋 ∈ Π.

Lemma 2. (Constrained Policy Improvement). Suppose Ω ⊆
Ψ. Assume that if 𝑥𝑘 ∈ Ψ and 𝜋 ∈ Π, then state 𝑥𝑘+1 =
𝑓 (𝑥𝑘, 𝜋(𝑥𝑘)) ∈ Ψ with respect to system (1). Given the as-
sociated value function 𝑉 𝜋𝐾 (𝑥𝑘) of a policy 𝜋𝐾 (𝑥𝑘), and the
new policy 𝜋𝐾+1(𝑥𝑘) is obtained by solving (32) for ∀𝑥𝑘 ∈
Ω. Then 𝑉 𝜋𝐾+1 (𝑥𝑘) ≤ 𝑉 𝜋𝐾 (𝑥𝑘) for ∀𝑥𝑘 ∈ Ω and ∀𝐾 ∈ ℕ+.

PROOF. Because Ω ⊆ Ψ, (32) is feasible for ∀𝑥𝑘 ∈ Ω. Then,
from (32), one has

𝑉𝐾,𝑞+1(𝑥𝑘) = 𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾,𝑞),

∀𝑥𝑘 ∈ Ω (31)

𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾+1) ≥ 𝐺(𝑥𝑘, 𝜋𝐾+1, 𝑉𝐾+1), ∀𝑥𝑘 ∈ Ω.

until ‖𝑉𝐾,𝑞+1(𝑥𝑘) − 𝑉𝐾,𝑞(𝑥𝑘)‖∞
𝑉𝐾+1(𝑥𝑘) = 𝑉𝐾,𝑞+1(𝑥𝑘),
Constrained policy improvement:

≤ 𝜖
∀𝑥𝑘 ∈ Ω

Update policy using:

𝐺(𝑥𝑘, 𝜋, 𝑉𝐾+1)

𝜋𝐾+1(𝑥𝑘) = arg min
𝜋(𝑥𝑘)
𝑠.𝑡. 𝑥𝑘+𝑖+1 = 𝑓 (𝑥𝑘+𝑖, 𝜋(𝑥𝑘+𝑖)),
(𝑥𝑘+𝑖+1) ≤ 𝑏𝜏 ,

𝐽𝐶𝜏

𝑖 ∈ [0, 𝑁 − 1]

𝜏 ∈ [1, 𝜏max]

(32)

𝐾 = 𝐾 + 1
until Convergence

(31), it follows that

|𝑉𝐾,2(𝑥𝑘) − 𝑉𝐾,1(𝑥𝑘)|

= |𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾,1) − 𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾,0)|
= 𝛾 𝑁
|𝑉𝐾,1(𝑥𝑘+𝑁 ) − 𝑉𝐾,0(𝑥𝑘+𝑁 )|
≤ 𝛾 𝑁 Δ,

∀𝑥𝑘 ∈ Ω.

Extending this for all subsequent iteration steps 𝑞, one has

|𝑉𝐾,𝑞+1(𝑥𝑘) − 𝑉𝐾,𝑞(𝑥𝑘)|

|𝑉𝐾,𝑞(𝑥𝑘+𝑁 ) − 𝑉𝐾,𝑞−1(𝑥𝑘+𝑁 )|

= 𝛾 𝑁
≤ 𝛾 𝑞𝑁 Δ,

∀𝑥𝑘 ∈ Ω.

Therefore, lim𝑞→∞ ‖𝑉𝐾,𝑞+1(𝑥) − 𝑉𝐾,𝑞(𝑥)‖∞ = 0, which also
means that 𝑉𝐾,∞(𝑥𝑘) satisﬁes the Bellman equation (3) for
∀𝑥𝑘 ∈ Ω. So, the sequence 𝑉𝐾,𝑞(𝑥𝑘) converges to the value
function 𝑉 𝜋𝐾 (𝑥𝑘) for ∀𝑥𝑘 ∈ Ω as 𝑞 → ∞, i.e., 𝑉𝐾+1 = 𝑉 𝜋𝐾 .

The following lemma shows that, the new policy 𝜋𝐾+1
obtained by (32) has a lower value function 𝑉 𝜋𝐾+1 than the
old policy 𝑉 𝜋𝐾 . The proof borrows heavily from the policy
improvement theorem of Q-learning and soft Q-learning [11,
21, 22, 56].

Deﬁnition 1. (Feasible State and Policy). A state 𝑥𝑘 is de-
ﬁned as a feasible state, denoted by 𝑥𝑘 ∈ Ψ with respect to
system (1), if there is a policy that can ensure the system sat-
isﬁes all the state constraints described in (32). If a policy 𝜋

By Lemma 1, one has 𝑉𝐾+1 = 𝑉 𝜋𝐾 . Furthermore, from (7),
it is clear that

𝐺(𝑥𝑘, 𝜋𝐾 , 𝑉𝐾+1) = 𝑉 𝜋𝐾 (𝑥𝑘), ∀𝑥𝑘 ∈ Ω.

Therefore, we can show that

𝑉 𝜋𝐾 (𝑥𝑘)
𝑁−1
∑

=

𝛾 𝑖𝑙(𝑥′

𝑘+𝑖, 𝜋𝐾 (𝑥′

𝑘+𝑖)) + 𝛾 𝑁 𝑉 𝜋𝐾 (𝑥′

𝑘+𝑁 )

𝑖=0
𝑁−1
∑

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋𝐾+1(𝑥𝑘+𝑖)) + 𝛾 𝑁 𝑉 𝜋𝐾 (𝑥𝑘+𝑁 )

𝑖=0
2𝑁−1
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋𝐾+1(𝑥𝑘+𝑖)) + 𝛾 2𝑁 𝑉 𝜋𝐾 (𝑥𝑘+2𝑁 )

(33)

∞
∑

𝑖=0

𝛾 𝑖𝑙(𝑥𝑘+𝑖, 𝜋𝐾+1(𝑥𝑘+𝑖))

≥

≥

⋮

≥

= 𝑉 𝜋𝐾+1 (𝑥𝑘),

∀𝑥𝑘 ∈ Ω,

where 𝑥′
proven that

𝑘+𝑖 ∼ 𝜋𝐾 , 𝑥𝑘+𝑖 ∼ 𝜋𝐾+1 and 𝑥′

𝑘 = 𝑥𝑘. We have thus

𝑉 𝜋𝐾+1 (𝑥𝑘) ≤ 𝑉 𝜋𝐾 (𝑥𝑘),

∀𝑥𝑘 ∈ Ω.

Theorem 1. (Constrained Policy Iteration in Tabular Case).
Through Algorithm 4, any policy 𝜋0 will converge to the
global optimal policy 𝜋∗ ∈ Π, such that 𝑉 𝜋∗
(𝑥𝑘) ≤ 𝑉 𝜋(𝑥𝑘)
for ∀𝜋 ∈ Π and ∀𝑥𝑘 ∈ Ω.

PROOF. For the policy 𝜋𝐾 at iteration 𝐾, we can ﬁnd its
associated 𝑉 𝜋𝐾 through policy evaluation process follows
from Lemma 1. By Lemma 2, 𝑉 𝜋𝐾+1 (𝑥𝑘) ≤ 𝑉 𝜋𝐾 (𝑥𝑘) for
∀𝑥𝑘 ∈ Ω. Besides, since 𝑉 𝜋𝐾 (𝑥𝑘) is bounded below by
𝑉 ∗(𝑥𝑘), 𝜋𝐾 and 𝑉 𝜋𝐾 will converge to some 𝜋∞ and 𝑉 𝜋∞.
At convergence, for ∀𝜋 ∈ Π and 𝑥𝑘 ∈ Ω, it must follow that
𝐺(𝑥𝑘, 𝜋∞, 𝑉 𝜋∞ ) ≤ 𝐺(𝑥𝑘, 𝜋, 𝑉 𝜋∞). Using the same iterative
argument as in (33) of Lemma 2, it is clear that 𝑉 𝜋∞ (𝑥𝑘) ≤
𝑉 𝜋(𝑥𝑘) for ∀𝜋 ∈ Π and 𝑥𝑘 ∈ Ω. From (4), one has 𝑉 𝜋∞ (𝑥𝑘) =
𝑉 ∗(𝑥𝑘). Hence 𝜋∞ is optimal in Π, i.e., 𝜋∞ = 𝜋∗.

J. Duan et al.: Preprint submitted to Elsevier

Page 12 of 15

Preprint

A.2. Convergence Proofs of Constrained Policy

Iteration with Function Approximation

The pseudo-code of CPI with function approximation is
described as Algorithm 5. By Lemma 3 and 4, the conver-
gence results of tabular CPI (Algorithm 4) can be extended
to Algorithm 5.

Algorithm 5 CPI Algorithm: function approximation

Initial with arbitrary 𝜃0, 𝜔0, learning rates 𝛼𝑐
Given an arbitrarily small positive 𝜖
Initialize iteration index 𝐾 = 0
repeat

Rollout 𝑁 steps from ∀𝑥𝑘 ∈ Ω with policy 𝜋𝜃𝐾
Receive and store 𝑥𝑘+𝑖, 𝑖 ∈ [1, 𝑁]
Policy evaluation:

repeat

𝜔𝐾,𝑞+1 = 𝜔𝐾,𝑞
Calculate 𝐺(𝑥𝑘, 𝜋𝜃𝐾
repeat

, 𝑉𝜔𝐾,𝑞+1

) using (7)

Calculate

d𝐿(𝜔𝐾,𝑞+1)
d𝜔𝐾,𝑞+1
Update value function using

using (9)

𝜔𝐾,𝑞+1 = −𝛼𝑐

d𝐿(𝜔𝐾,𝑞+1)
d𝜔𝐾,𝑞+1

+ 𝜔𝐾,𝑞+1

(34)

until 𝐿(𝜔𝐾,𝑞+1) ≤ 𝜖

until ‖𝑉 (𝑥𝑘; 𝜔𝐾,𝑞+1) − 𝑉 (𝑥𝑘; 𝜔𝐾,𝑞)‖∞
𝜔𝐾+1 = 𝜔𝐾,𝑞+1

Constrained policy improvement:
Construct 𝐽 (𝜃) under 𝑉𝜔𝐾+1
Update policy using (15)

using (10)

≤ 𝜖

𝐾 = 𝐾 + 1
until Convergence

Lemma 3. (Universal Approximation Theorem [43]). For
any continuous function 𝐹 (𝑥) ∶ ℝ𝑛 → ℝ𝑑 on a compact set
Ω, there exists a feed-forward NN, having only a single hid-
den layer, which uniformly approximates 𝐹 (𝑥) and its gra-
dient to within arbitrarily small error 𝜖 ∈ ℝ+ on Ω.

Lemma 4. (Global Minima of Over-Parameterized Neural
Networks [39, 40]). Consider the following optimization prob-
lem

min
𝜓

(𝜓) = 𝔼
𝑋𝑖∈

( (𝑋𝑖; 𝜓) − 𝑌𝑖)2}
,

{ 1
2

where 𝑋𝑖 ∈ ℝ𝑛 is the training input, 𝑌𝑖 ∈ ℝ𝑑 is the associ-
ated label,  = {(𝑋1, 𝑌1), (𝑋2, 𝑌2), …} is the dataset, 𝜓 is
the parameter to be optimized, and  ∶ ℝ𝑛 → ℝ𝑑 is an NN.
If the NN  (𝑋; 𝜓) is over-parameterized (i.e., the number of
hidden neurons and layers is suﬃciently large), simple al-
gorithms such as Gradient Descent (GD) or SGD can ﬁnd
global minima on the training objective (𝜓) in polynomial
time, as long as the dataset  is non-degenerate. The dataset
is non-degenerate if the same inputs 𝑋1 = 𝑋2 have the same
labels 𝑌1 = 𝑌2.

Theorem 2. (Constrained Policy Iteration with Function Ap-
proximation). Suppose both 𝑉 (𝑥𝑘; 𝜔) and 𝜋(𝑥𝑘; 𝜃) are over-
parameterized. Through Algorithm 5, any initial parameters
𝜔0 and 𝜃0 will converge to 𝜔∗ and 𝜃∗, such that 𝑉 (𝑥𝑘; 𝜔∗) =
𝑉 𝜋𝜃∗ (𝑥𝑘) ≤ 𝑉 𝜋𝜃 (𝑥𝑘) for ∀𝜋𝜃 ∈ Π and ∀𝑥𝑘 ∈ Ω.

PROOF. In the policy evaluation step of Algorithm 5, by
Lemma 3, there always ∃𝜔𝐾,𝑞+1 ∈ ℝ𝑝, such that

𝑉 (𝑥𝑘; 𝜔𝐾,𝑞+1) = 𝐺(𝑥𝑘, 𝜋𝜃, 𝑉𝜔𝐾,𝑞

),

∀𝑥𝑘 ∈ Ω.

(35)

In other words, ∃𝜔𝐾,𝑞+1 ∈ ℝ𝑝, such that 𝐿(𝜔𝐾,𝑞+1) ≤ 𝜖.
Since Algorithm 5 updates 𝜔𝐾,𝑞 using (34) to continuously
minimize 𝐿(𝜔𝐾,𝑞), according to Lemma 4, we can ﬁnd 𝜔𝐾,𝑞+1
in polynomial time. From Lemma 1, as (35) and (31) are
equivalent, the sequence 𝑉 (𝑥𝑘; 𝜔𝐾,𝑞+1) converges to the value
function 𝑉 𝜋𝜃𝐾 (𝑥𝑘) for ∀𝑥𝑘 ∈ Ω as 𝑞 → ∞, i.e.,
𝑉 (𝑥𝑘; 𝜔𝐾+1) = 𝑉 (𝑥𝑘; 𝜔𝐾,∞) = 𝑉 𝜋𝜃𝐾 (𝑥𝑘), ∀𝑥𝑘 ∈ Ω. (36)
For the policy improvement process, according to Lemma
3 and extending Lemma 2 to the function approximation
case, one has

𝑉 𝜋𝜃𝐾+1 (𝑥𝑘) ≤ 𝑉 𝜋𝜃𝐾 (𝑥𝑘),

∀𝑥 ∈ Ω.

(37)

Finally, according to (36), (37) and Theorem 1, we can
get the conclusion that 𝑉 𝜋𝜃∞ (𝑥𝑘) = 𝑉 (𝑥𝑘; 𝜔∞) = 𝑉 ∗(𝑥𝑘)
for ∀𝑥𝑘 ∈ Ω, i.e., 𝜃∞ = 𝜃∗ and 𝜔∞ = 𝜔∗.

A.3. The Generalized Policy Iteration Framework
Algorithms based on PI framework, such as Algorithm 4
and 5, proceed by alternately updating the value and policy
functions. Note that while one function is being updated, the
other remains unchanged. Besides, taking the policy evalua-
tion process of Algorithm 4 and 5 as an example, each func-
tion usually requires multiple updating iterations to meet the
terminal conditions, which is the so-called protracted iter-
ative computation problem [11]. This often leads to slow
learning. Therefore, for practical applications, almost all
ADP or RL algorithms build on the GPI framework, which
truncates the policy evaluation and policy improvement pro-
cesses into an arbitrary step or even one-step update [7, 11].
In recent years, many experimental results and theoretical
proofs have shown that almost all ADP or RL algorithms
based on PI can be adapted to a GPI version without losing
the convergence guarantees [8, 15, 17, 19, 20, 21, 22, 24, 56,
57]. Based on this fact and Theorem 2, we make the follow-
ing remark.

Remark 6. (Constrained Generalized Policy Iteration). Sup-
pose both 𝑉 (𝑥𝑘; 𝜔) and 𝜋(𝑥𝑘; 𝜃) are over-parameterized. Thr-
ough Algorithm 2, any initial parameters 𝜔0 and 𝜃0 will
converge to 𝜔∗ and 𝜃∗, such that 𝑉 (𝑥𝑘; 𝜔∗) = 𝑉 𝜋𝜃∗ (𝑥𝑘) ≤
𝑉 𝜋𝜃 (𝑥𝑘) for ∀𝜋𝜃 ∈ Π and ∀𝑥𝑘 ∈ Ω.

B. Acknowledgment

This study is supported by the Beijing Science and Tech-

nology Plan Project with Z191100007419008, Tsinghua University-
Didi Joint Research Center for Future Mobility, and NSF

J. Duan et al.: Preprint submitted to Elsevier

Page 13 of 15

Preprint

China with 51575293 and U20A20334. We would like to
acknowledge Jie Li, Hao Chen, Yang Zheng, Yiwen Liao,
Guofa Li, Ziyu Lin and Jiatong Xu for their valuable sug-
gestions. The authors are grateful to the Editor-in-Chief, the
Associate Editor, and anonymous reviewers for their valu-
able comments.

References
[1] D. P. Bertsekas, Dynamic programming and optimal control, 4th Edi-

tion, Athena Scientiﬁc, Belmont, 2017.

[2] F. L. Lewis, D. Vrabie, V. L. Syrmos, Optimal control, John Wiley &

Sons, New York, 2012. doi:10.1002/9781118122631.

[3] F. Y. Wang, H. Zhang, D. Liu, Adaptive dynamic programming: An
IEEE Computational Intelligence Magazine 4 (2009)

introduction,
39–47. doi:10.1109/MCI.2009.932261.

[4] P. Werbos, Beyond regression: New tools for prediction and analysis
in the behavioral sciences, Ph. D. dissertation, Harvard University
(1974).

[5] P. Werbos, Approximate dynamic programming for realtime con-
trol and neural modelling, Handbook of Intelligent Control: Neural,
Fuzzy and Adaptive Approaches (1992) 493–525.

[6] W. B. Powell, Approximate Dynamic Programming: Solving the
curses of dimensionality, John Wiley & Sons, New York, 2007.
doi:10.1002/9780470182963.

[7] D. Liu, Q. Wei, D. Wang, X. Yang, H. Li, Adaptive dynamic program-
ming with applications in optimal control, Springer, Berlin, 2017.
doi:10.1007/978-3-319-50815-3.

[8] J. Duan, S. E. Li, Z. Liu, M. Bujarbaruah, B. Cheng, Generalized
policy iteration for optimal control in continuous time, arXiv preprint
(2019). arXiv:1909.05402.

[9] K. G. Vamvoudakis, F. L. Lewis, Online actor–critic algorithm to
solve the continuous-time inﬁnite horizon optimal control problem,
Automatica 46 (2010) 878–888. doi:10.1016/j.automatica.2010.02.
018.

[10] J. Li, H. Modares, T. Chai, F. L. Lewis, L. Xie, Oﬀ-policy reinforce-
ment learning for synchronization in multiagent graphical games,
IEEE transactions on neural networks and learning systems 28 (2017)
2434–2445. doi:10.1109/TNNLS.2016.2609500.

[11] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction,

MIT Press, Cambridge, 1998. doi:10.1109/TNN.1998.712192.

[12] Z. Chen, S. Jagannathan, Generalized Hamilton–Jacobi–Bellman
formulation-based neural network control of aﬃne nonlinear discrete-
time systems, IEEE Transactions on Neural Networks 19 (2008) 90–
106. doi:10.1109/TNN.2007.900227.

[13] A. Al-Tamimi, F. L. Lewis, M. Abu-Khalaf, Discrete-time nonlin-
ear HJB solution using approximate dynamic programming: Conver-
gence proof,
IEEE Transactions on Systems, Man, and Cybernet-
ics, Part B (Cybernetics) 38 (2008) 943–949. doi:10.1109/ADPRL.2007.
368167.

[14] D. Liu, Q. Wei, P. Yan, Generalized policy iteration adaptive dynamic
programming for discrete-time nonlinear systems, IEEE Transactions
on Systems, Man, and Cybernetics: Systems 45 (2015) 1577–1591.
doi:10.1109/TSMC.2015.2417510.

[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., Human-level control through deep reinforcement learning, Na-
ture 518 (2015) 529–533. doi:10.1038/nature14236.

[16] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., Grand-
master level in StarCraft II using multi-agent reinforcement learning,
Nature 575 (2019) 350–354. doi:10.1038/s41586-019-1724-z.
[17] J. Duan, Y. Guan, S. E. Li, Y. Ren, B. Cheng, Distributional soft
actor-critic: Oﬀ-policy reinforcement learning for addressing value
estimation errors, arXiv preprint (2020). arXiv:2001.02811.

[18] Y. Ren, J. Duan, S. E. Li, Y. Guan, Q. Sun, Improving generaliza-
tion of reinforcement learning with minimax distributional soft actor-

critic, in: 23rd IEEE International Conference on Intelligent Trans-
portation Systems, IEEE, 2020. doi:10.1109/ITSC45102.2020.9294300.
[19] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Sil-
ver, D. Wierstra, Continuous control with deep reinforcement learn-
ing, arXiv preprint (2015). arXiv:1509.02971.

[20] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, K. Kavukcuoglu, Asynchronous methods for deep rein-
in: Int. Conf. on Machine Learning, 2016, pp.
forcement learning,
1928–1937.

[21] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Oﬀ-
policy maximum entropy deep reinforcement learning with a stochas-
tic actor,
in: Proceedings of the 35th International Conference on
Machine Learning, PMLR, 2018, pp. 1861–1870.

[22] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-
mar, H. Zhu, A. Gupta, P. Abbeel, et al., Soft actor-critic algorithms
and applications, arXiv preprint (2018). arXiv:1812.05905.

[23] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region
in: Int. Conf. on Machine Learning, 2015, pp.

policy optimization,
1889–1897.

[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov,
arXiv preprint (2017).

Proximal policy optimization algorithms,
arXiv:1707.06347.

[25] S. E. Li, H. Chen, R. Li, Z. Liu, Z. Wang, Z. Xin, Predictive lat-
eral control to stabilise highly automated vehicles at tire-road friction
limits, Vehicle system dynamics 58 (2020) 768–786. doi:10.1080/
00423114.2020.1717553.

[26] M. Abu-Khalaf, F. L. Lewis, Nearly optimal control laws for non-
linear systems with saturating actuators using a neural network HJB
approach, Automatica 41 (2005) 779–791. doi:10.1016/j.automatica.
2004.11.034.

[27] L. Dong, X. Zhong, C. Sun, H. He, Event-triggered adaptive dy-
namic programming for continuous-time systems with control con-
straints, IEEE Transactions on Neural Networks and Learning Sys-
tems 28 (2016) 1941–1952. doi:10.1109/TNNLS.2016.2586303.

[28] B. Luo, H. N. Wu, T. Huang, D. Liu, Reinforcement learning solu-
tion for HJB equation arising in constrained optimal control problem,
Neural Networks 71 (2015) 150–158. doi:10.1016/j.neunet.2015.08.
007.

[29] B. Fan, Q. Yang, X. Tang, Y. Sun, Robust ADP design for continuous-
time nonlinear systems with output constraints, IEEE Transactions
on Neural Networks and Learning Systems 29 (2018) 2127–2138.
doi:10.1109/TNNLS.2018.2806347.

[30] T. Zhang, H. Xu, Adaptive optimal dynamic surface control of
strict-feedback nonlinear systems with output constraints,
Interna-
tional Journal of Robust and Nonlinear Control 30 (2020) 2059–2078.
doi:10.1002/rnc.4864.

[31] J. Na, B. Wang, G. Li, S. Zhan, W. He, Nonlinear constrained op-
timal control of wave energy converters with adaptive dynamic pro-
gramming,
IEEE Transactions on Industrial Electronics 66 (2019)
7904–7915. doi:10.1109/TIE.2018.2880728.

[32] J. Sun, C. Liu, Backstepping-based adaptive dynamic programming
for missile-target guidance systems with state and input constraints,
Journal of the Franklin Institute 355 (2018) 8412–8440. doi:10.1016/
j.jfranklin.2018.08.024.

[33] F. Borrelli, A. Bemporad, M. Morari, Predictive control for linear
and hybrid systems, Cambridge University Press, Cambridge, 2017.
doi:10.1017/9781139061759.

[34] J. Achiam, D. Held, A. Tamar, P. Abbeel, Constrained policy opti-
in: Proceedings of the 34th International Conference on

mization,
Machine Learning, JMLR, 2017, pp. 22–31.

[35] L. Wen, J. Duan, S. E. Li, S. X. Xu, H. Peng, Safe reinforcement
learning for autonomous vehicles through parallel constrained pol-
in: 23rd IEEE International Conference on Intel-
icy optimization,
ligent Transportation Systems, IEEE, 2020. doi:10.1109/ITSC45102.
2020.9294262.

[36] A. Heydari, S. N. Balakrishnan, Finite-horizon control-constrained
nonlinear optimal control using single network adaptive critics, IEEE
Transactions on Neural Networks and Learning Systems 24 (2012)

J. Duan et al.: Preprint submitted to Elsevier

Page 14 of 15

Preprint

tolerant control for strict-feedback nonlinear systems, IEEE Transac-
tions on Fuzzy Systems (2020). doi:10.1109/TFUZZ.2020.2965890.
[56] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning
with deep energy-based policies, in: Proc. of the 34th Int. Conf. on
Machine Learning, 2017, pp. 1352–1361.

[57] G. Barth-Maron, M. W. Hoﬀman, D. Budden, W. Dabney, D. Horgan,
D. TB, A. Muldal, N. Heess, T. Lillicrap, Distributed distributional
deterministic policy gradients, in: Int. Conf. on Learning Represen-
tations, 2018.

145–157. doi:10.1109/TNNLS.2012.2227339.

[37] H. Zhang, L. Cui, Y. Luo, Near-optimal control for nonzero-sum dif-
ferential games of continuous-time nonlinear systems using single-
network ADP, IEEE Transactions on Cybernetics 43 (2012) 206–216.
doi:10.1109/TSMCB.2012.2203336.

[38] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015)

436–444. doi:10.1038/nature14539.

[39] Z. Allen-Zhu, Y. Li, Z. Song, A convergence theory for deep learning
via over-parameterization, in: Proceedings of the 36th International
Conference on Machine Learning, PMLR, 2019, pp. 242–252.
[40] S. Du, J. Lee, H. Li, L. Wang, X. Zhai, Gradient descent ﬁnds global
minima of deep neural networks, in: Proceedings of the 36th Inter-
national Conference on Machine Learning, PMLR, 2019, pp. 1675–
1685.

[41] T. Dierks, B. T. Thumati, S. Jagannathan, Optimal control of unknown
aﬃne nonlinear discrete-time systems using oﬄine-trained neural net-
works with proof of convergence, Neural Networks 22 (2009) 851–
860. doi:10.1016/j.neunet.2009.06.014.

[42] H. Li, D. Liu, Optimal control for discrete-time aﬃne non-linear sys-
tems using general value iteration, IET Control Theory & Applica-
tions 6 (2012) 2725–2736. doi:10.1049/iet-cta.2011.0783.

[43] K. Hornik, M. Stinchcombe, H. White, Universal approximation of
an unknown mapping and its derivatives using multilayer feedfor-
ward networks, Neural Networks 3 (1990) 551–560. doi:10.1016/
0893-6080(90)90005-6.

[44] S. Ruder, An overview of gradient descent optimization algorithms,

arXiv preprint (2016). arXiv:1609.04747.

[45] Z. Li, J. Deng, R. Lu, Y. Xu, J. Bai, C. Su, Trajectory-tracking
control of mobile robot systems incorporating neural-dynamic opti-
mized model predictive approach,
IEEE Transactions on Systems,
Man, and Cybernetics: Systems 46 (2015) 740–749. doi:10.1109/
TSMC.2015.2465352.

[46] C. Zhu, R. H. Byrd, P. Lu, J. Nocedal, Algorithm 778: L-BFGS-B:
Fortran subroutines for large-scale bound-constrained optimization,
ACM Transactions on Mathematical Software 23 (1997) 550–560.
doi:10.1145/279232.279236.

[47] J. Nocedal, S. Wright, Numerical optimization, Springer, Berlin,

2006. doi:10.1007/b98874.

[48] J. Duan, S. E. Li, Y. Guan, Q. Sun, B. Cheng, Hierarchical reinforce-
ment learning for self-driving decision-making without reliance on
labelled driving data,
IET Intelligent Transport Systems 14 (2020)
297–305. doi:10.1049/iet-its.2019.0317.

[49] A. Nagabandi, G. Kahn, R. S. Fearing, S. Levine, Neural network
dynamics for model-based deep reinforcement learning with model-
free ﬁne-tuning, in: IEEE International Conference on Robotics and
Automation (ICRA), IEEE, 2018, pp. 7559–7566. doi:10.1109/ICRA.
2018.8463189.

[50] N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, Y. Tassa, Learn-
ing continuous control policies by stochastic value gradients, in: Ad-
vances in Neural Information Processing Systems, 2015, pp. 2944–
2952.

[51] R. Li, Y. Li, S. Li, E. Burdet, B. Cheng, Driver-automation indi-
rect shared control of highly automated vehicles with intention-aware
authority transition, in: IEEE Intelligent Vehicles Symposium (IV),
2017, pp. 26–32. doi:10.1109/IVS.2017.7995694.

[52] J. Kong, M. Pfeiﬀer, G. Schildbach, F. Borrelli, Kinematic and dy-
namic vehicle models for autonomous driving control design,
in:
IEEE Intelligent Vehicles Symposium, IEEE, 2015, pp. 1094–1099.
doi:10.1109/IVS.2015.7225830.

[53] D. Liu, H. Li, D. Wang, Neural-network-based zero-sum game for
discrete-time nonlinear systems via iterative adaptive dynamic pro-
gramming algorithm, Neurocomputing 110 (2013) 92–100. doi:10.
1016/j.neucom.2012.11.021.

[54] K. Sun, Q. Jianbin, H. R. Karimi, Y. Fu, Event-triggered robust
fuzzy adaptive ﬁnite-time control of nonlinear systems with pre-
scribed performance, IEEE Transactions on Fuzzy Systems (2020).
doi:10.1109/TFUZZ.2020.2979129.

[55] K. Sun, L. Liu, J. Qiu, G. Feng, Fuzzy adaptive ﬁnite-time fault-

J. Duan et al.: Preprint submitted to Elsevier

Page 15 of 15

