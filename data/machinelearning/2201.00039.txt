1
2
0
2

c
e
D
1
3

]

G
L
.
s
c
[

1
v
9
3
0
0
0
.
1
0
2
2
:
v
i
X
r
a

Stochastic convex optimization for provably efﬁcient
apprenticeship learning ∗

Angeliki Kamoutsi
ETH Zurich
kamoutsa@ethz.ch

Goran Banjac
ETH Zurich
gbanjac@ethz.ch

John Lygeros
ETH Zurich
jlygeros@ethz.ch

Abstract

We consider large-scale Markov decision processes (MDPs) with an unknown
cost function and employ stochastic convex optimization tools to address the prob-
lem of imitation learning, which consists of learning a policy from a ﬁnite set of
expert demonstrations. We adopt the apprenticeship learning formalism, which
carries the assumption that the true cost function can be represented as a linear
combination of some known features. Existing inverse reinforcement learning
algorithms come with strong theoretical guarantees, but are computationally ex-
pensive because they use reinforcement learning or planning algorithms as a sub-
routine. On the other hand, state-of-the-art policy gradient based algorithms (like
IM-REINFORCE, IM-TRPO, and GAIL), achieve signiﬁcant empirical success in
challenging benchmark tasks, but are not well understood in terms of theory. With
an emphasis on non-asymptotic guarantees of performance, we propose a method
that directly learns a policy from expert demonstrations, bypassing the interme-
diate step of learning the cost function, by formulating the problem as a single
convex optimization problem over occupancy measures. We develop a computa-
tionally efﬁcient algorithm and derive high conﬁdence regret bounds on the quality
of the extracted policy, utilizing results from stochastic convex optimization and
recent works in approximate linear programming for solving forward MDPs.

1 Introduction

The goal of apprenticeship learning (AL) in a Markov decision process (MDP) environment without
cost function is to learn a policy that achieves or even surpasses the performance of a policy demon-
strated by an expert. A usual assumption is that the unknown true cost function can be represented
as a weighted combination of some known basis functions, where the true unknown weights specify
how different desiderata should be traded off. An argument for this assumption is that in practice the
unknown cost function depends on just a few key properties, but the desirable weighting is unknown.

A lot of methods have been proposed to solve the AL problem. The most naive approach is behavior
cloning, which casts the problem as a supervised learning problem, in which the goal is to learn a
map from states to optimal actions. Although behavior cloning is simple and easy to implement,
the crucial i.i.d. assumption made in supervised learning is violated. As a result, the approach
suffers from the problem of cascading errors which is related to covariate shift [14]. Later works
like DAgger[20] eliminate this distribution mismatch by formulating the problem as a no-regret
algorithm in an online learning setting. However, these algorithms require interaction with the expert,
which is a different learning scenario from the one considered in this paper. Most importantly, their
sample and computational complexities scale polynomially with the horizon of the problem, which
in our case is inﬁnite.

∗This project has received funding from the European Research Council (ERC) under the European Union’s

Horizon 2020 research and innovation programme grant agreement OCAL, No. 787845.

Optimization Foundations for Reinforcement Learning Workshop at NeurIPS 2019, Vancouver, Canada.

 
 
 
 
 
 
Inverse reinforcement learning (IRL) [4] is a prevalent approach to AL. In this paradigm, the learner
ﬁrst infers the unknown cost function that the expert tries to minimize and then uses it to reproduce
the optimal behavior. IRL algorithms do not suffer from the problem of cascading errors because the
training takes place over entire expert trajectories, rather than individual actions. In addition, since
the recovered cost function “explains” the expert behavior, they can easily generalize to unseen
states or even new MDP environments. Note however, that most existing IRL algorithms [4, 24, 18,
3, 26, 19, 17, 15, 16] are computationally expensive because they use reinforcement learning as a
subroutine.

On the other hand, one can frame the problem as a single convex program [23], bypassing the
intermediate step of learning the cost function. Although the associated program can be solved
exactly for small-sized MDPs, the approach suffers from the curse of dimensionality, making it
intractable for large-scale problems, arising in, e.g., autonomous driving with increasing number
of sensors and decision aspects. Provably efﬁcient convex approximation schemes for the convex
formulation of AL [23] in the context of large-scale MDPs remain unexplored. However, it is worth
noting, that the formulations and reasoning in [23] formed the ground and inspired later state-of-the-
art algorithms [14, 13]. In particular, the authors in [14] developed a gradient-based optimization
formulation over parameterized policies for AL, and then presented algorithms which are parallel to
the policy gradient RL counterparts [25, 21]. The sequel paper [13] draws a connection between the
policy optimization formulation and generative adversarial networks [8], from which an analogous
imitation learning algorithm is derived. These approaches are model-free and scale to large and
continuous environments. However, in general the policy optimization problem is highly non-convex
and as a result remains hampered by limited theoretical understanding. In particular, these methods
provide no guarantees on the quality of the points they converge to.

With an emphasis on non-asymptotic guarantees of stability and performance, in this work we pro-
pose an approximation scheme for the convex formulation of AL [23]. In particular, the objective
is to minimize the ℓ1-distance between the feature expectation vector of the expert and the learner,
subject to linear constraints ensuring that the optimization variable is an occupancy measure induced
by a policy. Our AL algorithm and its theoretical analysis build upon recent innovations in approx-
imate linear programming (LP) for large-scale discounted MDPs [1, 2] and can be seen as the AL
analogue of their algorithhms. Similar to [1], we control the complexity by limiting our search to the
linear subspace deﬁned by a small number of features. We then convert the initial program to an un-
constrained convex optimization problem. To this end, we use a surrogate loss function by adding a
multiple of the total constraint violations to the initial objective. We then construct unbiased subgra-
dient estimators and apply the stochastic subgradient descent algorithm. In this way, by combining
bounds in the stochastic convex optimization literature and concentration inequalities, we are able
to give high conﬁdence regret bounds showing that the performance of our algorithm approaches
the best achievable by any policy in the comparison class. A salient feature of the algorithm is that
the iteration and sample complexities do not depend on the size of the state space but instead on the
number of approximation features.

It is worth mentioning that, since our methodology is based on the LP formulation of MDPs [10–
12, 6], it is naturally extensible to unconventional problems involving additional safety constraints
or secondary costs, where traditional dynamic programming techniques are not applicable [9, 7, 22].

To the best of our knowledge this is the ﬁrst time that a performance bound is derived for a policy-
optimization-based algorithm for AL. We hope that the techniques proposed in this work provide a
starting point for developing provably efﬁcient AL algorithms.

[1,

Notation. We denote by Ai,: and A:,j the i-th row and j-th column of a matrix A, respectively.
k · kp the p-norm in Rn. The corresponding induced matrix norm
For p
∞
∈
the usual
is deﬁned by
x, y
i
h
k
and
y denotes elementwise inequality. We deﬁne [x]+ = max
inner product. Moreover, x
0, x
}
{
[x]− =

kp. For vectors x and y, we denote by

. The set of probability measures on a set X is denoted by

kp = supk x kp≤1 k
≤

], we denote by
A

(X).

Ax

min
{

0, x
}

−

P

2 Preliminaries

Consider a ﬁnite MDP described by a tuple
is the state space,
=

Mc ,
is the action space, P :

A

,

, P, γ, ν0, c

A

a1, . . . , a|A|}

{

, where
(
X

X × A 7→ P

(cid:1)

X
(cid:0)

=

x1, . . . , x|X |}
X
) is the transition law,

{

2

γ
∈
and c :

(0, 1) is the discount factor, ν0 ∈ P

R is the one-stage cost function.

(
X

X × A 7→

) is the initial probability distribution of the system state,

The model
lution is described as follows. At time step t, if the system is in state xt = x
at = a
∈ A
xt+1, which is an
into the new state has occurred, a new action is chosen and the process is repeated.

Mc represents an inﬁnite horizon controlled discrete-time stochastic system whose evo-
, and the action
∈ X
is taken, then (i) the cost c(x, a) is incurred, and (ii) the system moves to the next state
x, a). Once transition

-valued random variable with probability distribution P (

X

·|

A stationary Markov policy is a map π :
X 7→ P
action a, while being in state x. We denote the space of stationary Markov policies by Π0.
Given a policy π
space Ω , (
Pπ
ν0

Π0, we denote by Pπ
ν0
)∞, i.e., Pπ
ν0 [
is denoted by Eπ
ν0

the induced probability measure on the canonical sample
ν0]. The expectation operator with respect to

x) denotes the probability of choosing

∈
X × A

π, x0 ∼

), and π(a

] = Prob[

(
A

· |

·

|

.

The optimal control problem is given by minπ∈Π0 ηc(π), where ηc(π) , Eπ
ν0
is the total expected discounted cost of a policy π.

∞
t=0 γtc(xt, at)
i
R+,
For every policy π
by µπ(x, a) ,
ν0 [xt = x, at = a] . The occupancy measure can be interpreted as the
(unnormalized) discounted visitation frequency of state-action pairs when acting according to policy
P
π. Moreover, it holds that ηc(π) =

h P
Π0, we deﬁne the γ-discounted occupancy measure µπ :

x,a µπ(x, a)c(s, a) = Eµπ [c(x, a)].

∈
∞
t=0 γt Pπ

X × A 7→

,

3 Apprenticeship Learning Framework

P

3.1 Problem statement

Consider now the Markov decision model without a cost function,
Assume that
instead, we have access to a ﬁnite number m of
1, . . . , xk
0, xk
0, ak
(xk
{
could also be history-dependent. We impose the following assumptions:
Assumption 1 (Apprenticeship learning).

.
,
M
i.i.d sample trajectories
(cid:1)
m
k=1 coming from an expert policy πE. Note that the expert policy

t , . . .)
}

, P, γ, ν0

X
(cid:0)

t , ak

1, ak

A

,

(A1) πE is a nearly optimal policy for the discounted MDP corresponding to the model

Mctrue =

,

, P, γ, ν0, ctrue

;

A
X
(cid:0)
(A2) ctrue
∈ Clin =
{
vectors, such that

nc
(cid:1)
i=1 wiψi | k
ψik∞ ≤
k
P

w

k∞ ≤
1 for all i = 1, . . . , nc.

}

1

, where

ψi}

nc
i=1 ⊂

{

R|X ||A| are ﬁxed basis

µπ, ctrue
The goal of AL is to ﬁnd a policy π, such that
h
ctrue is unknown, AL algorithms search for a policy π that satisﬁes
∈ Clin. Therefore, an AL algorithm seeks a policy that performs better than the expert across
c
by optimizing the objective
µπ, c
(
h

. Since the cost function
i
µπE , c
µπ, c
, for all
i
h
Clin,
(1)

µπE , ctrue

µπE , c

i ≤ h

min
π∈Π

i ≤ h

) .
i

i − h

sup
c∈Clin

nc

We highlight that one can consider other linearly parameterized cost classes, e.g.,
nc
i=1 wi = 1

i=1 wiψi | k

k2 ≤
{
The reasoning and the analysis are similar.

i=1 wiψi |

Cconvex =

wi ≥

[4], or

0,

w

nc

}

1

{

Clin,2 =
[24, 23].

}

P

P

P

3.2 The convex optimization view

In the remainder of the paper we will use the following vector notation borrowed from [1]. The
x′∈X P(x,a),x′ = 1, the initial probability
transition law is a matrix P
so that
+
∈
R|X |
R|X ||A|.
+ so that
distribution is a vector ν0 ∈
Finally, for a stationary Markov policy π
that encodes
π as M π
xi,(xj ,ak) = π(ak |

ν0k1 = 1, and the cost function is a vector c
k
R|X |×|X ||A|
Π0 we deﬁne the matrix M π
∈
xi,(xj ,ak) = 0 otherwise.

xi), if i = j, and M π

R|X ||A|×|X |

P

∈

∈

+

Next, we will characterize the set of occupancy measures in terms of linear constraint satisfaction.
|X ||A|×|X | is a
γP )T µ = ν0, µ
To this aim let

, where B

R|X ||A|

0, 1

(B

,

µ

0

F

∈

|

−

≥

∈ {

}

(cid:8)

(cid:9)

3

F

are also known as Bellman ﬂow constraints.

µπ
{
. Moreover, for every feasible solution µ
µ(x,a)

binary matrix deﬁned by B(xi,ak),xj = 1, if i = j, and B(xi,ak),xj = 0 otherwise. The constraints
that deﬁne the set
Proposition 1 ([23, Theorem 2]). It holds that,
have that µπ
policy πµ ∈
µπµ satisﬁes µπµ = µ.
Let Ψ , [ψ1|
feature expectation vector as

Π0, we
Π0}
, we can obtain a stationary Markov
Pa′ ∈A µ(x,a′) . Then, the corresponding induced occupancy measure

R|X ||A|×nc be the cost basis matrix. For a policy π

. Indeed, for every π

∈ F
Π0 by πµ(a

Π, we deﬁne its

|
∈
∈ F

x) ,

ψnc]

. . .

=

F

∈

∈

∈

π

|

|

In other words, for every i = 1, . . . , nc,
µπ, Ψ
h

ii =

µπ, ψii

h

Lemma 1. For every π

Π, the following holds

µπ, Ψ
i
h

, ΨT µπ

Rnc.

∈

= ηψi (π).

∈
sup
c∈Clin

(

µπ, c
h

µπE , c

i − h

) =
i

kh

µπ, Ψ

i − h

µπE , Ψ

ik1.

By Lemma 1, we get that (1) is equivalent to

µπ, Ψ

min
π∈Π kh

µπE , Ψ

ik1.

i − h
Note that although the objective function in (2) is convex in µπ, the whole program is non-convex in
= ηc(π) for every policy π, and every
π. However, combining Proposition 1 with the fact that
cost c, we conclude that the AL objective (2) can be stated equivalently as a convex optimization
program:

µπ, c
h

i

(2)

(3)

µ, Ψ

min
µ∈Fkh

i − h
linear constraints given by µ

µπE , Ψ

ik1.

Note that the
while the
generated by a stationary Markov policy.

|X ||A|
linear constraints given by (B

|X |

−

0 ensure that µ is a nonnegative measure,
γP )T µ = ν0 ensure that µ is an occupancy measure

≥

4 Algorithm and main result

In practice we do not have access to the whole policy πE, but instead can observe i.i.d. trajectory
samples distributed according to PπE
m
0, ak
ν0
k=1 ∼
Rnc of the expert feature expectation
(PπE
vector, i.e., for each i = 1, . . . , nc,

. For a multi-sample
\
µπE , Ψ
h

ν0 )m consider the Monte Carlo approximation

t , . . .)
}

1, . . . , xk

0, xk

t , ak

1, ak

(xk

i ∈

{

\
µπE , Ψ
h

ii =

\
µπE , ψii
h

, 1
m

∞

m

γtψi(xj

t , aj

t ).

t=0
X

j=1
X

Moreover, under Assumption (A2), the following is a pointwise bound on Ωm:

We are interested in optimizing the empirical convex objective for large-scale MDPs:

\
µπE , Ψ
h

ik∞ ≤

k

1/(1

γ).

−

(4)

µ, Ψ

min
µ∈Fkh

\
µπE , Ψ

ik1,

i −
h
which is a random convex program on (Ωm, (PπE
ν0 )m).
Our main aim is (i) to provide a computationally efﬁcient algorithm whose complexity does not
grow with the size of the state and action spaces, and (ii) to obtain explicit probabilistic performance
bounds on the quality of the extracted solution. To this end, we will design and analyze the AL
analogue of the algorithm proposed in [1] for the forward average-cost MDP problem. Most of the
tools from the forward MDP setting [1, 2] can be used for the AL formulation with the appropriate
modiﬁcations. We will present the main reasoning and results in this section with proofs presented
in the appendix.

(5)

4

As the ﬁrst step, instead of optimizing over the whole space R|X ||A|, we optimize over the linear
In this way, we reduce
hull of a small number of selected feature vectors
signiﬁcantly the number of optimization variables. Let Φ , [φ1|
R|X ||A|×d be the
feature matrix. The corresponding reduced program is

R|X ||A|.
. . .

d
i=1 ⊂

φi}

φd]

∈

{

|

min
θ∈Rd:Φθ∈Fk

ΨT Φθ

\
µπE , Ψ
h

ik1.

−

(6)

R|X ||A| by πu(a

R|X ||A|, which is not necessarily in
. If u(x, a)

, we can still deﬁne a
Note that for an arbitrary vector u
policy πu ∈
, we let πu(
x) =
x)
∈ A
be the uniform distribution [5]. Then, one has that µπu = u if and only if u
. In the general case,
the following lemma, which is the discounted cost analogue of [1, Lem. 2], quantiﬁes how close the
generated occupancy measure µπu is to u according to the degree of constraint violation.
2k[u]−k1+k(B−γP )T u−ν0k1
1−γ

∈
[u(x,a)]+
Pa′∈A[u(x,a′)]+

R|X ||A|, it holds that

Lemma 2. For any u

F
0, for all a

µπu

∈ F

· |

≤

u

|

.

k

−

k1 ≤

∈

∈

Rd, we deﬁne πθ , πΦθ and µθ , µπΦθ . As already discussed, µθ = Φθ if and only if
For any θ
θ is feasible for (6). In the general case, one can bound the distance between the occupancy measure
µθ and the vector Φθ by applying Lemma 2.
Let (ρ, λ) > 0 be positive constants, Θ ,
Rd
Θ the Euclidean
projection onto Θ. Consider the following surrogate loss function which is obtained by adding a
positive multiple of the constraint violations to the initial objective function:

, and ΠΘ : Rd

k2 ≤

| k

7→

∈

ρ

{

}

θ

θ

(θ) ,

L

k

ΨT Φθ

−

h

\
µπE , Ψ

ik1 + λ

[Φθ]−k1

k

:=V1(θ)

+λ

(B

k

−

γP )T (Φθ)

ν0k1

−

:=V2(θ)

=

nc

|

i=1
X

ΨT
:,i

Φθ

\
µπE , ψii|

|
+ λ

−

h

{z

}

{z
|
[Φ(x,a),:θ]− + λ

X(x,a)∈X ×A

Xx∈X

(B

|

}
−

γP )T
:,x

Φθ

ν0(x)
|

.

−

We are interested in the reduced unconstrained convex optimization program of the form
minθ∈Θ L
A subgradient of

at θ is given by

(θ).

L

(θ) =

∇θL

nc

(ΦT Ψ:,i) sign

ΨT
:,i

Φθ

\
µπE , ψii

−

h

i=1
X
+ λ

Xx∈X

λ

−

(ΦT (B

−

(cid:16)
γP ):,x) sign

(cid:17)
γP )T
:,x

(B

−

(ΦT

(x,a),:)1{Φ(x,a),:θ<0}.

(cid:0)

Φθ

−

ν0(x)

(cid:1)

Suppose that q1 ∈ P
(
X
We propose a method for AL shown in Algorithm 1. It uses an unbiased estimate of
ﬁxed expert trajectory samples, i.e.

) assign to each element a strictly positive probability.
(θ) for

(
X × A

∇θL

X(x,a)∈X ×A
) and q2 ∈ P

nc

gt(θ) =

(ΦT Ψ:,i) sign

ΨT
:,i

Φθ

\
µπE , ψii

−

h

i=1
X

+ λ

ΦT (B

(cid:16)
γP ):,y(t)

−
q2(y(t))

ΦT

(x(t),a(t)),:
q1(x(t), a(t))

λ

−

sign

(B

(cid:16)

−

1{Φ

(x(t) ,a(t) ),:

θ<0},

5

(cid:17)
:,y(t) Φθ
γP )T

−

ν0(y(t))
(cid:17)

(7)

Algorithm 1 Stochastic subgradient descent for apprenticeship learning (SGD-AL).
1: given cost matrix Ψ, feature matrix Φ, number of expert samples m, number of iterations T ,

learning rate η > 0, radius ρ > 0, regularization parameter λ > 0

1, . . . , xk

t , ak

t , . . .)
}

m
k=1 ∼

(PπE

ν0 )m

(xk

1, ak

0, ak

2: Set θ0 = 0
0, xk
3: Sample
{
4: for t = 1, . . . , T do
5:
6:
7:
8: ˆθT = 1
T
9: Return πˆθT
P

Sample (x(t), a(t))
∼
Compute gt(θt) via (7)
Update θt+1 = ΠΘ
θt −
T
t=1 θt
(cid:0)

q1 and y(t)

q2

∼

ηgt(θt)

(cid:1)

where (x(t), a(t))

q1 and y(t)

q2.

∼

∼

Regret bounds for Algorithm 1 can be obtained by using results from the stochastic convex optimiza-
tion literature and statistical learning theory.
Assumption 2. All entries of the feature matrix Φ are positive, i.e. every feature vector φi is a
measure which assigns a non-zero value to each pair (x, a). Moreover,

Φ

k1 = 1
1−γ .

k

We deﬁne the following constants:

C1 , max

(x,a)∈X ×A

k

ΦT

(x,a),:k2
q1(x, a)

,

C2 , max
x∈X

k

ΦT (B

γP ):,xk2

.

−
q2(x)

These constants appear in our performance bounds. We would like to choose appropriate distribu-
tions so that C1 and C2 are small, since they appear in the error bound. We refer the reader to [1]
for a thorough discussion on the choice of the distributions.

Observe from (7) that for all θ

∈

Θ we have the following bound:

gt(θ)

k2 ≤ k

k

Φ

k2

ψik2 + λ(C1 + C2) =: K.

k

(8)

Theorem 1. Let ε
2

2kΨk∞
4ρ2
λ(1−γ) + 1
ε2
with probability at least 1

(cid:16)

(cid:17)

(0, 1), δ

∈

∈
∆2 with ∆ , K +

(0, 1), ρ > 0, λ = 1/ε, m

10 log 2

δ +

5d log(1 + ρ2T

32n2

c log( 4nc
δ )
(1−γ)ε2
≥
d ), and η = ρ/(K√T ). Then,

, T

≥

δ, Algorithm 1 generates πˆθT

q

q

so that for all θ

Θ,

∈

−

ΨT µ ˆθT − h

k

µπE , Ψ

ik1 ≤ k

ΨT µθ − h
Ψ
2
k∞
k
γ
1

(cid:18)

−

+

µπE , Ψ

ik1 +

Ψ

k∞k

k
(cid:19) (cid:18)

Ψ

4
k∞
k
1
γ
(cid:18)
−
Φ
k1ρ√d +

+

1
ε
nc

(cid:19)

1

γ

(cid:19)

−

ε + ε.

(V1(θ) + V2(θ))

References

[1] Y. Abbasi-Yadkori, P. L. Bartlett, and A. Malek. Linear programming for large-scale Markov
decision problems. In International Conference on Machine Learning (ICML), pages 496–504,
2014.

[2] Y. Abbasi-Yadkori, P.L. Bartlett, X.D. Chen, and A. Malek. Large-scale markov decision

problems via the linear programming dual. ArXiv, abs/1901.01992, 2019.

[3] P. Abbeel, D. Dolgov, A. Y. Ng, and S. Thrun. Apprenticeship learning for motion planning
with application to parking lot navigation. In IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 1083–1090, 2008.

6

nc

i=1
X

[4] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Inter-

national Conference on Machine Learning (ICML), 2004.

[5] Ershad Banijamali, Yasin Abbasi-Yadkori, Mohammad Ghavamzadeh, and Nikos Vlassis. Op-
timizing over a restricted policy class in mdps. In Proceedings of the Twenty-Second Interna-
tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 3042–3050, 2019.

[6] V. S. Borkar. A convex analytic approach to Markov decision processes. Probability Theory

and Related Fields, 78(4):583–602, 1988.

[7] F. Dufour and T. Prieto-Rumeau. Finite linear programming approximations of constrained dis-
counted Markov decision processes. SIAM Journal on Control and Optimization, 51(2):1298–
1324, 2013.

[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems (NIPS), pages 2672–2680, 2014.

[9] O. Hernández-Lerma, J. González-Hernández, and R. López-Martínez. Constrained average
cost Markov control processes in Borel spaces. SIAM Journal on Control and Optimization,
42(2):442–468, 2003.

[10] O. Hernández-Lerma and J. B. Lasserre. Discrete-Time Markov Control Processes: Basic

Optimality Criteria. Springer-Verlag New York, 1996.

[11] O. Hernández-Lerma and J. B. Lasserre. Further Topics on Discrete-Time Markov Control

Processes. Springer-Verlag New York, 1999.

[12] O. Hernández-Lerma and J. B. Lasserre. The linear programming approach. In Handbook of

Markov Decision Processes: Methods and Applications, pages 377–407. Springer US, 2002.

[13] J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Infor-

mation Processing Systems (NIPS), pages 4565–4573, 2016.

[14] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In

International Conference on Machine Learning (ICML), pages 2760–2769, 2016.

[15] S. Levine, Z. Popovi´c, and V. Koltun. Feature construction for inverse reinforcement learning.
In Advances in Neural Information Processing Systems (NIPS), pages 1342–1350, 2010.

[16] S. Levine, Z. Popovi´c, and V. Koltun. Nonlinear inverse reinforcement learning with Gaussian
processes. In Advances in Neural Information Processing Systems (NIPS), pages 19–27, 2011.

[17] G. Neu and C. Szepesvári. Apprenticeship learning using inverse reinforcement learning and
gradient methods. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 295–
302, 2007.

[18] A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In International

Conference on Machine Learning (ICML), pages 663–670, 2000.

[19] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In International

Conference on Machine Learning (ICML), 2006.

[20] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction
to no-regret online learning. In Proceedings of the Fourteenth International Conference on
Artiﬁcial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research,
pages 627–635, Fort Lauderdale, FL, USA, 11–13 Apr 2011. PMLR.

[21] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and M. Moritz. Trust region policy optimization.

In International Conference on Machine Learning (ICML), pages 1889–1897, 2015.

[22] E. Shaﬁeepoorfard, M. Raginsky, and S. P. Meyn. Rational inattention in controlled Markov

processes. In American Control Conference (ACC), pages 6790–6797, 2013.

7

[23] U. Syed, M. Bowling, and R.E. Schapire. Apprenticeship learning using linear programming.
In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages
1032–1039, New York, NY, USA, 2008. ACM.

[24] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Ad-

vances in Neural Information Processing Systems (NIPS), pages 1449–1456, 2007.

[25] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine Learning, 8(3):229–256, 1992.

[26] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement

learning. In National Conference on Artiﬁcial Intelligence (AAAI), pages 1433–1438, 2008.

8

Π. Then, for every c =

nc
i=1 wiψi with

w

k∞ ≤

1, it holds that

A Proofs

Proof of Lemma 1. Fix a π

∈

µπ, c

h

µπE , c

i − h

=

i

nc

P
µπ, ψii − h
wi (
h
µπ, Ψ

i − h
µπE , Ψ

i − h

i=1
X
w
k∞kh
µπ, Ψ

≤ k
≤ kh

k
µπE , ψii
)
µπE , Ψ
ik1,
µπE , Ψ

ik1

ik1.

and thus,

Next, let ˜w

∈

sup
c∈Clin

(

µπ, c
h

µπE , c

i − h

)
i

Rnc be deﬁned by

µπ, Ψ

≤ kh

i − h

Then for ˜c ,

nc
i=1 ˜wiψi, we have ˜c

P

which proves that

h

˜wi = sign (

µπ, ψii − h
h

µπE , ψii

) .

∈ Clin and
µπE , ˜c
=
i

µπ, ˜c

i − h

µπ, Ψ

µπE , Ψ

kh

i − h

ik1,

sup
c∈Clin

µπ, c
(
h

µπE , c

i − h

)
i

This concludes the proof.

µπ, Ψ

µπE , Ψ

≥ kh

i − h

ik1.

In case of a stationary Markov policy, the induced discounted occupancy measure has the following
form.
Lemma 3. Let π
t

Π0 be a stationary Markov policy. Then for all x

and
(x,a) . In particular, (µπ)T =

ν0 [xt = x, at = a] =

0 M π(P M π)t
νT

∈ A

∈ X

, a

∈

N0, it holds that Pπ
0 M π(P M π)t.

∈
∞
t=0 γtνT

(cid:2)

(cid:3)

P
Proof. For t = 0 we have

Pπ

ν0[x0 = x, a0 = a] = ν0(x)π(a

x) = [ν⊤

0 M π](x,a).

|

Next, assume that the result holds for t

1. Then,

−

Pπ

ν0 [xt = x, at = a] = Pπ
= π(a

ν0 [at = a
x)

|

xt = x] Pπ

ν0 [xt = x]

P(x′,a′),x Pπ

ν0 [xt−1 = x′, at−1 = a′].

By the induction assumption, we conclude that

Pπ

ν0[xt = x, at = a] = π(a

x)

0 M π(P M π)t−1
ν⊤

(x′,a′) P(x′,a′),x
(cid:3)

Xx′∈X Xa′∈A

|

|

Xx′∈X Xa′∈A
(cid:2)
0 M π(P M π)t−1P
ν⊤
= π(a
|
0 M π(P M π)t
ν⊤

x)

=

(cid:2)

x

(cid:3)

(x,a) .
(cid:3)

(cid:2)

Proof of Lemma 2. We provide a reﬁned proof and bound similar to [2, Lemma 13]. We have

(B

k

−

γP )T [u]+ −

=:−w

ν0

k1 ≤ k

(B

γP )T [u]−k1 +

k

(B

−

−

γP )T u

ν0k1

−

|

{z

}

(1 + γ)
k

[u]−k1 +

k

(B

−

≤

γP )T u

ν0k1,

−

9

where we have used the fact that
Lemma 3, we have

k

BT

k1 =

B

k∞ = 1 and

k

k

P T

k1 =

P

k∞ = 1. By virtue of

k

(µπu )T =

=

=

=

∞

t=0
X
∞

t=0
X
∞

t=0
X
∞

t=0
X

γtνT

0 M πu(P M πu)t

γt(w + (B

−

γP )T [u]+)T M πu(P M πu)t

γtwT M πu(P M πu)t +

γt[u]T

+(P M πu)t

∞

t=0
X
γtwT M πu(P M πu)t + [u]T
+,

∞

−

t=0
X

γt+1[u]T

+(P M πu)t+1

where in the third equality we used [u]T

+BM πu = [u]T

+. Therefore,

µπu

k

[u]+k1 =

−

≤

k

t=0
X
∞
γt

t=0
X
1

≤

1

γ

−

∞

γt((M πu )T P T )t(M πu)T w

k1

M πu

t
∞k

k

P

t
∞k

k

M πu

k∞k

w

k1

k

(1 + γ)
k

[u]−k1 +

k

(B

−

γP )T u

ν0k1

−

,

where in the last inequality we used
inequality gives

k

(cid:0)
M πu

k∞ =

B

k∞ =

k

P

k

(cid:1)
k∞ = 1. Finally, the triangle

µπu

k

u

−

k1 ≤ k

µπu

[u]+k1 +

[u]−k1 ≤

k

−

2

[u]−k1 +

k

k

(B
1

γP )T u
−
γ
−

ν0k1

.

−

Proof of Theorem 1. The proof combines techniques presented in the proofs of [1, Theorem 2]
and [2, Lemma 14] and the Hoeffding’s bound.

(xk
ν0 )m.
We ﬁrst ﬁx an expert trajectory multi-sample
{
Then, by virtue of [1, Thm. 3] and by the uniform bound of the unbiased subgradient estimates
(8), we get that if the learning rate is η = ρ/(K√T ), then with probability at least 1
δ/2 (the
corresponding probability space is ((

m
k=1 ∼

t , . . .)
}

1, . . . , xk

T ), qT

0, xk

0, ak

1, ak

t , ak

)T

−

(PπE

X × A

× X

qT
2 ),

1 ⊗

(ˆθT )

L

min
θ∈Θ L

(θ)

−

≤

ρK
√T

1 + 4ρ2T
T 2

+

s

2
δ

2 log

(cid:18)

+ d log

1 +

(cid:18)

ρ2T
d

.
(cid:19)(cid:19)

(9)

T ), (PπE

Integrating over the whole probability space (Ωm, (PπE
δ/2, where the corresponding probability space is (Ωm
probability at least 1
−
qT
qT
ν0 )m
2 ).
1 ⊗
X
⊗
(ˆθT ) and
Substituting
obtain that with probability at least 1

(θ) by their deﬁnitions, and using the inequality √a + b

δ/2, for all θ

Θ,

L

L

×

≤

(
X × A

)T

×

√a + √b, we

ν0 )m), we conclude that (9) holds with

ΨT ΦˆθT −

k

\
µπE , Ψ
h

ik1+λV1(ˆθT )+λV2(ˆθT )

≤ k

−

∈
ΨT Φθ

\
µπE , Ψ
h

ik1+λV1(θ)+λV2(θ)+

−

ρ
√T

∆.

(10)

10

For all multi-samples that (10) holds, and for all θ
ΨT µ ˆθT −
Ψ

Θ,
∈
ΨT ΦˆθT k1 +
2V1(ˆθT ) + V2(ˆθT )

ΨT µ ˆθT −

\
µπE , Ψ

ik1 ≤ k

k

h

≤ k

k∞

1

γ

−

+ λV1(θ) + λV2(θ) +

k

ΨT ΦˆθT −
ΨT Φθ
+

\
µπE , Ψ
h

ik1
\
µπE , Ψ

k

−

h

ik1

ρ
√T

∆,

(11)

where we used the triangle inequality in the ﬁrst step, and Lemma 2 together with the bound (10) in
the second. Moreover, by (10),

V1(ˆθT ) + V2(ˆθT )

1
λ

≤

where we used that
γ).
nc/(1

θ

k

k1 ≤

−

nc

Φ

Ψ

k∞k

k1ρ√d +
k
(cid:18)
ρ√d, and the pointwise bound

−

(cid:19)

γ

1

+ V1(θ) + V2(θ) +

∆,

(12)

\
µπE , Ψ
h

ik1 ≤

k

\
µπE , Ψ
h

ik∞ ≤

ρ
λ√T
nc k

Once more, by the triangle inequality and Lemma 2, we get

ΨT Φθ

k

−

h

\
µπE , Ψ

ik1 ≤ k

ΨT Φθ

ΨT µθk1 +
2V1(θ) + V2(θ)

−

γ

1

−

Ψ

k∞

≤ k

ΨT µθ −
k
+

\
µπE , Ψ
h
ΨT µθ −

ik1
\
µπE , Ψ
h

k

ik1.

(13)

Therefore, by combining (11),(12) and (13), we get that with probability at least 1
θ

Θ,

∈

δ/2, for all

−

ΨT µ ˆθT −

k

\
µπE , Ψ
h

ik1 ≤ k

ΨT µθ −
Ψ
4
k∞
k
γ
1

(cid:18)

−

+

\
µπE , Ψ
h

ik1 +

Ψ

2
k
λ(1

k∞
γ)
−

k
(cid:18)
(V1(θ) + V2(θ)) +

+ λ

(cid:19)

Ψ

(cid:18)

k∞k
Ψ
2
k
λ(1

Φ

k1ρ√d +
k∞
γ)
−

+ 1

(cid:19)

nc

1
−
ρ
√T

γ

(cid:19)

∆.

For T
all θ

∈

4ρ2
ε2

≥
Θ,

(cid:16)

2kΨk∞
λ(1−γ) + 1

2

(cid:17)

∆2 and λ = 1/ε, it follows that with probability at least 1

δ/2, for

−

ΨT µ ˆθT −

k

\
µπE , Ψ
h

ik1 ≤ k

h

ΨT µθ −
Ψ
2
k∞
k
γ
1

+

\
µπE , Ψ

Ψ

4
k∞
ik1 +
k
1
γ
(cid:18)
−
Φ
Ψ
k1ρ√d +

k∞k

+

1
ε
nc

(cid:19)

(V1(θ) + V2(θ))

ε + ε/2.

k
(cid:19) (cid:18)
We conclude the proof by using Hoeffding’s inequality with conﬁdence δ/(2nc) and approximation
accuracy ε/(4nc). In particular, we have that for m

and for all i = 1, . . . , nc,

−

−

(cid:19)

(cid:18)

32nc log(4nc/δ)
2(1−γ)ε2

γ

1

ε/(4nc),

≤

δ/(2nc)). Note that under Assumption (A2), it holds that
and for all i = 1, . . . , nc.

with probability (PπE
∞
t=0 γtψi(xt, at)

ν0 )m at least (1

1/(1

−

≤

P
A union bound gives that for m

µπE , ψii −
h
−
γ) for all (xt, at)

(cid:12)
(cid:12)
(cid:12)

≥
\
µπE , ψii
h
(cid:12)
(cid:12)
(cid:12)

32nc log( 4nc
2(1−γ)ε2

δ )

≥

kh

µπE , Ψ
i −
least
(1

∈ X × A
,

\
µπE , Ψ
δ/2).

h
−

with probability (PπE
(
X × A
least (1
(cid:0)
−
Finally, a simple union bound concludes the proof. ˆδ

ν0 )m at
qT
2

)T
× X
δ/2).

T , qT

1 ⊗

(cid:1)

ε/4,

ik1 ≤

Integrating over

we have the same statement with probability (PπE

the whole
qT
ν0 )m
1 ⊗

⊗

space
qT
2 at

11

