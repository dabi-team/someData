Exact and Approximation Algorithms for Sparse
PCA

Yongchun Li
Department of Industrial & Systems Engineering, Virginia Tech, Blacksburg, VA 24061, liyc@vt.edu

Weijun Xie
Department of Industrial & Systems Engineering, Virginia Tech, Blacksburg, VA 24061, wxie@vt.edu

Sparse PCA (SPCA) is a fundamental model in machine learning and data analytics, which has witnessed a

variety of application areas such as ﬁnance, manufacturing, biology, healthcare. To select a prespeciﬁed-size

principal submatrix from a covariance matrix to maximize its largest eigenvalue for the better interpretability

purpose, SPCA advances the conventional PCA with both feature selection and dimensionality reduction.

Existing approaches often approximate SPCA as a semi-deﬁnite program (SDP) without strictly enforcing

the important cardinality constraint that restricts the number of selected features to be a constant. To ﬁll

this gap, we propose two exact mixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the

covariance matrix and the properties of the largest eigenvalues. We then analyze the theoretical optimality

gaps of their continuous relaxation values and prove that they are stronger than that of the state-of-art one.

We further show that the continuous relaxations of two MISDPs can be recast as saddle point problems

without involving semi-deﬁnite cones, and thus can be eﬀectively solved by ﬁrst-order methods such as the

subgradient method. Since oﬀ-the-shelf solvers, in general, have diﬃculty in solving MISDPs, we approximate

SPCA with arbitrary accuracy by a mixed-integer linear program (MILP) of a similar size as MISDPs. The

continuous relaxation values of two MISDPs can be leveraged to reduce the size of the proposed MILP

further. To be more scalable, we also analyze greedy and local search algorithms, prove their ﬁrst-known

approximation ratios, and show that the approximation ratios are tight. Our numerical study demonstrates

that the continuous relaxation values of the proposed MISDPs are quite close to optimality, the proposed

MILP model can solve small and medium-size instances to optimality, and the approximation algorithms

work very well for all the instances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD)

with non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple covariance matrices,

each corresponding to a protected group.

Key words : Sparse PCA, Largest Eigenvalue, Mixed-Integer Program, Semi-deﬁnite Program, Greedy,

Local Search, SVD, Fairness

0
2
0
2

g
u
A
8
2

]
L
M

.
t
a
t
s
[

1
v
8
3
4
2
1
.
8
0
0
2
:
v
i
X
r
a

1. Introduction This paper studies the sparse principal component analysis (SPCA) problem

of the form

(SPCA) w∗ := max
x∈Rn

(cid:8)x(cid:62)Ax : ||x||2 = 1, ||x||0 = k(cid:9) ,

(1)

1

 
 
 
 
 
 
2

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

where the symmetric positive semi-deﬁnite matrix A ∈ Rn×n denotes the sample covariance out of a

dataset with n features and the integer k ∈ [n] denotes the sparsity of its ﬁrst principal component

(PC). In SPCA (1), the objective is to select the best size-k principal submatrix from a covariance

matrix A with the maximum largest eigenvalue. Compared to the conventional PCA, the extra

zero-norm constraint ||x||0 = k in SPCA (1) restricts the number of features of the ﬁrst PC x to

be k most important ones. In this way, SPCA improves the interpretability of the obtained PC,

which has been shown as early as Jeﬀers [20] in 1967. It is also recognized that SPCA can be

more reliable for large-scale datasets than PCA, where the number of features is far more than

that of observations [41]. These advantages of SPCA have beneﬁted many application ﬁelds such

as biology, ﬁnance, cloud computing, and healthcare, which frequently deal with datasets with a

massive number of features (see, e.g., [8, 21, 25, 30]).

1.1. Relevant Literature Our paper contributes to relevant literature on SPCA from three

aspects: exact mixed-integer programs, convex relaxations, and approximation algorithms.

Exact Mixed-Integer Programs: As shown in formulation (1), SPCA is highly non-convex-

maximizing a convex function subject to two nonconvex constraints (i.e., an L2 equality constraint

and an L0 equality constraint). Albeit superior to traditional PCA, SPCA (1) is notoriously known

to be computationally expensive; see, e.g., the complexity analysis and inapproximability results

in Magdon-Ismail [27]. As a result, the equivalent formulations and algorithms for exactly solving

SPCA are quite limited in the literature (see, e.g., [5, 17, 29]). Moghaddam et al. [29] introduced a

branch and bound method to solve SPCA, and they pruned redundant nodes using the eigenvalue of

principal submatrices and a greedy algorithm. Recently, Berk and Bertsimas [5] embedded various

upper and lower bounds into this branch and bound framework, which could eﬃciently prune nodes

and quickly certiﬁcate the optimality for quite a few instances. It is worthy of mentioning that

Gally and Pfetsch [17] proposed a MISDP (MISDP) formulation for SPCA. Our second MISDP

formulation diﬀers from Gally and Pfetsch [17] by deriving two strong conic valid inequalities.

Another interesting work can be found in Dey et al. [15], where the authors developed approximate
convex integer programs for SPCA with an optimality gap of (1 + (cid:112)k/(k + 1))2. Quite diﬀerently,

we propose two exact MISDP formulations and one approximate mixed-integer linear program

(MILP) for SPCA from novel perspectives of analyzing the largest eigenvalue. Speciﬁcally, the

proposed MILP formulation can be arbitrarily close to the optimal value of SPCA, and it can be

directly solved by oﬀ-the-shelf solvers such as Gurobi.

Convex Relaxations: Besides solving exact SPCA, researchers have also actively sought to

explore eﬀective convex relaxations. A common approach in literature is to develop SDP relaxations

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

3

for SPCA (see e.g., [1, 13, 16, 12, 40]). Albeit convex, solvers often have diﬃculty in solving large-

scale instances of SDP formulations (e.g., n = Ω(100)). The computational challenge of these SDP

problems urgently calls for more eﬀective methods to compute the relaxation values for SPCA. From

a diﬀerent angle, this paper solves the continuous relaxations of the proposed MISDP formulations

as the maximin saddle point problem, where the subgradient method enjoys a O(1/T ) rate of

convergence [31] based on Euclidean projections. Surprisingly, we further show that the projection

oracle of the subgradient method is a second-order conic program rather than an SDP and thus

can be easily dealt with.

Approximation Algorithm: Another early thread of research on SPCA is the development of

high-quality heuristics for solving SPCA to near optimality such as greedy algorithm [16, 19],

truncation algorithm [9], power method [22], and variable neighborhood search method [7]. In

particular, the truncation algorithm in [9] so far provides the best-known approximation ratio

O(n−1/3), which can be easily implemented to generate a feasible solution for SPCA. This paper

investigates the greedy and local search algorithms and proves their ﬁrst-known approximation

ratios O(1/k) for SPCA.

1.2. Summary of Contributions We observe that when the support of x has been suc-

cessfully identiﬁed, SPCA (1) reduces to the conventional PCA ﬁnding the largest eigenvalue and

eigenvector of a size-k principal submatrix of A. This fact motivates us to derive two equiva-

lent MISDP formulations and an approximate MILP of SPCA. Below is a summary of the main

contributions in this paper.

(i) For each formulation, we derive the theoretical optimality gap between its continuous relax-

ation value and the optimal value of SPCA.

(ii) Our ﬁrst MISDP formulation inspires us to derive closed-form expressions of the coeﬃcients

of valid inequalities, which can be eﬃciently embedded into the branch and cut algorithms;

(iii) We show that the subgradient method can be adapted to ease the computational burden

of obtaining MISDP continuous relaxation values with O(1/T ) rate of convergence. These

continuous relaxations values can further help reduce the size of MILP;

(iv) The continuous relaxation of our second MISDP formulation is proven to be stronger than

the one proposed in d’Aspremont et al. [13];

(v) The proposed MILP formulation has a similar size as two MISDPs and can be directly solved

using many existing solvers;

(vi) We prove and demonstrate the tightness of the ﬁrst-known approximation ratios for the

greedy and local search algorithms;

4

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

(vii) Our analyses can be extended to the Rank-one Sparse SVD (R1-SSVD), which aims to com-

pute the largest singular value of the possibly non-symmetric matrix A with the sparsity

constraints on its left-singular and right-singular vectors separately; and

(viii) We extend the second MISDP formulation to Sparse Fair PCA (SFPCA), where the covariance

matrices are observed from multiple protected groups.

Our contributions have both theoretical and practical relevance. Theoretically, we contribute three

exact mixed-integer convex programs to SPCA. Practically, our MILP formulation can either attain

optimal solutions for SPCA, improve the continuous relaxations, or ﬁnd better-quality feasible

solutions for small and medium-size instances. We apply the computationally eﬃcient subgradi-

ent method to solving the continuous relaxations of the proposed MISDPs, as well as deriving

their theoretical optimality gaps. We also develop two scalable approximation algorithms to solve

SPCA to near optimality and prove their approximation ratios. Our proposed algorithms have been

demonstrated to be successfully applied to large-scale data analytics problems, such as identifying

key features for the drug abuse problem. We further extend the analyses to R1-SSVD and SFPCA.

All the theoretical contributions are summarized in Table 1.

Table 1. Summary of Theoretical Contributions

Problem Exact Mixed Integer Program

SPCA

R1-SSVD

MISDP (6)

MISDP (15)

MILP (22)

MISDP (34)

MISDP (35)

MILP (36)

Optimality Gap2

min{k, nk−1}

√

d/2 + 1/2)}

√

min{k(

d/2 + 1/2), nk−1

k, nk−1}
√

(cid:112)

√

d + (n − k)(
1 k−1
mnk−1
2
(cid:112)
1 k−1
mnk−1
2 }
√
d/2 + 1/2),

(cid:112)

mnk−1
√

k1k2,

min{
1 k−1
2 [min{(k1 + k2)(
d + (m + n − k1 − k2)(

√

mnk−1

1 k−1

2

d/2 + 1/2)} − 1]

SFPCA2

MISDP (40)

–

Problem Approximation Algorithm

Approximation Ratio3

SPCA

Greedy Algorithm 1

Local Search Algorithm 2

Truncation algorithm

R1-SSVD

Greedy Algorithm 3

Local Search Algorithm 4

max{

(cid:112)

k−1
1 ,

(cid:112)

k1k2m−1n−1}

k−1

k−1

√

k−1
2 ,
k−1
1 k−1
1 k−1
k−1

2

2

(cid:112)

(cid:112)

1 Optimality Gap is the ratio between the continuous relaxation value and the optimal one;
2 The formulation (40) provides an upper bound for general SFPCA and becomes exact when there
are only two groups;
3 Approximation Ratio denotes the ratio between the objective value of an approximation algorithm
and the optimal one.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

5

Organization: The remainder of this paper is organized as follows. Sections 2 and 3 develop two

MISDP formulations for SPCA and prove the optimality gaps of their continuous relaxation values.

Section 4 investigates an approximate MILP, which can be arbitrarily close to the optimal value of

SPCA, and proves the optimality gap of its continuous relaxation value. Section 5 introduces and

analyzes two approximation algorithms. Section 6 conducts a numerical study to demonstrate the

eﬃciency and the solution quality of our proposed formulations and algorithms. Sections 7 and 8

separately extend the analyses to the rank-one sparse SVD (R1-SSVD) and the sparse fair PCA

(SFPCA). Finally, conclusion and future directions are exhibited in Section 9.

Notation: The following notation is used throughout the paper. We let S n, S n

++ denote set of all
the n×n symmetric real matrices, set of all the n×n symmetric positive semi-deﬁnite matrices, and

+, S n

set of all the n × n symmetric positive deﬁnite matrices, respectively. We use bold lower-case letters

(e.g., x) and bold upper-case letters (e.g., X) to denote vectors and matrices, respectively, and use

corresponding non-bold letters (e.g., xi, Xij) to denote their components. We use 0 to denote the
zero vector and 1 to denote the all-ones vector. We use (cid:100)·(cid:101) as a ceil function. We let Rn
set of all the n dimensional nonnegative vectors and let Rn

+ denote the
++ denote the set of all the n dimensional
positive vectors. Given a positive integer n and an integer s ≤ n, we let [n] := {1, 2, · · · , n} and

let [s, n] := {s, s + 1, · · · , n}. We let In denote the n × n identity matrix and let ei denote its i-th
(cid:1) denote the
column vector. Given a set S and an integer k, we let |S| denote its cardinality and (cid:0)S

k

collection of all the size-k subsets out of S. Given an m × n matrix A and two sets S ∈ [m], T ∈ [n],

we let AS,T denote a submatrix of A with rows and columns indexed by sets S, T , respectively and
let AS denote a submatrix of A with columns from the set S. Given a vector x ∈ Rn, we let Diag(x)

denote the diagonal matrix with diagonal elements x1, · · · , xn, and let supp(x) denote the support

of x. Given a square symmetric matrix A, let diag(A) denote the vector of diagonal entries of A,

and let λmin(A), λmax(A) denote the smallest and largest eigenvalues of A, respectively. Given a

non-square matrix A, let σmax(A) denote the largest singular value. Additional notation will be

introduced later as needed.

2. Exact MISDP Formulation (I)

In this section, we derive an equivalent mixed-integer

semi-deﬁnite programming (MISDP) formulation for SPCA based on the spectral decomposition

and disjunctive programming techniques.

To begin with, for each i ∈ [n], we let the binary variable zi = 1 if the i-th feature is selected,

and 0, otherwise. Linearizing the zero-norm constraint using binary vector z, then SPCA (1) can

be equivalently formulated as a following nonconvex mixed-integer quadratic program:

(SPCA) w∗ := max

x∈Rn,z∈Z

(cid:26)

x(cid:62)Ax : ||x||2 = 1, |xi| ≤ zi, ∀i ∈ [n]

,

(2)

(cid:27)

6

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

where we let cardinality set Z denote the feasible region of z, i.e.,

(cid:26)

z ∈ {0, 1}n :

Z =

(cid:27)
.

zi = k

(cid:88)

i∈[n]

For SPCA (2), we note that (i) the binary vector z is of vital importance and its associated feasible

region Z will be used throughout this paper for two MISDPs and one MILP, and (ii) the derivations

of all the three mixed-integer formulations originate from the naive SPCA (2).

2.1. Spectral Reformulation We observe that given a size-k subset of features (i.e., the

support of the binary vector z in formulation (2) is speciﬁed), the SPCA (2) is equivalent to

ﬁnding the largest eigenvalue of the corresponding principal submatrix of A. This fact inspires

us to propose three equivalent mixed-integer convex programs for SPCA (2) . This observation is

summarized below.

Lemma 1 For a symmetric matrix A ∈ S n and a size-k set S ⊆ [n], the followings must hold:

(i) maxx∈Rn

(cid:8)x(cid:62)Ax : ||x||2 = 1, xi = 0, ∀i /∈ S(cid:9) = λmax(AS,S),
(ii) maxX∈Sk
{tr(AS,SX) : tr(X) = 1} = λmax(AS,S), and
(iii) If matrix A is positive semi-deﬁnite, then λmax(AS,S) = λmax((cid:80)

+

i ), where A = C (cid:62)C,
C ∈ Rd×n denotes the Cholesky factorization matrix of A, d is the rank of A, and ci ∈ Rd

i∈S cic(cid:62)

denotes i-th column vector of C for each i ∈ [n].

Proof. See Appendix A.1.

(cid:3)

The results in Lemma 1 are crucial to this paper and allow us to derive the exact mixed-integer

convex programs of SPCA. Speciﬁcally, we remark that: Part (i) of Lemma 1 reduces SPCA to

selecting the best size-k principal submatrix of A to achieve the maximum largest eigenvalue, which

establishes a combinatorial formulation of SPCA; Part (ii) of Lemma 1 shows that SDP relaxation

of the largest eigenvalue problem by dropping the rank-one constraint is exact and inspires us to

develop two MISDP formulations for SPCA; and since the covariance matrix used in SPCA is

always positive semi-deﬁnite, the identity in Part (iii) of Lemma 1 suggests an alternative way of

formulating SPCA using Cholesky decomposition, which motivates us to derive an exact MISDP

formulation in this section and an MILP in a later section.

According to Part (i) in Lemma 1, introducing a subset S, a natural combinatorial reformulation

of SPCA (1) is deﬁned as:

w∗ := max

S

{λmax(AS,S) : |S| = k, S ⊆ [n]} .

(3)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

7

By computing the Cholesky factorization of A = C (cid:62)C with C ∈ Rd×n and d denoting the rank of

A, then the identity in Part (iii) in Lemma 1 recasts the objective function of SPCA (3) as below:

w∗ := max

S

(cid:26)

(cid:18)(cid:88)

(cid:19)

cic(cid:62)
i

λmax

i∈S

: |S| = k, S ⊆ [n]

(cid:27)
.

(4)

Recall that for each i ∈ [n], binary variable zi = 1 if ith feature (i.e., column ci) is selected, and 0,

otherwise. Therefore, SPCA (4) can be further reformulated as

w∗ := max
z∈Z

(cid:26)

λmax

(cid:18) (cid:88)

(cid:19)(cid:27)
.

zicic(cid:62)
i

i∈[n]

(5)

The above formulation involves with concave objective function but it is a maximization problem,

which will cause much trouble. Fortunately, the result in Part (ii) of Lemma 1 and the reformulation

technique from disjunctive programming [2] motivate us to convert SPCA (5) to an equivalent

MISDP, which is shown as below.

Theorem 1 The SPCA (2) admits an equivalent MISDP formulation

(SPCA) w∗ :=

max
z∈Z,
X,W1,··· ,Wn∈Sd
+

(cid:40)

(cid:88)

i∈[n]

c(cid:62)
i Wici : tr(X) = 1, X (cid:23) Wi, tr(Wi) = zi, ∀i ∈ [n]

(cid:41)
.

(6)

Proof. According to Part (ii) in Lemma 1, the largest eigenvalue of a symmetric matrix can be

equivalently reformulated as an SDP, thus by introducing a positive semi-deﬁnite matrix variable

X ∈ S d

+, SPCA (5) can be represented as

w∗ := max

z∈Z,X∈Sd
+

(cid:40)

(cid:88)

i∈[n]

zic(cid:62)

i Xci : tr(X) = 1

(cid:41)
,

(7)

where the objective function comes from the identity tr(cic(cid:62)

i X) = c(cid:62)

i Xci for each i ∈ [n].

In SPCA (7), the objective function contains bilinear terms {ziX}i∈[n]. To further convexify

them, we create two copies of the matrix variable X, denoting by Wi1, Wi2 for each i ∈ [n] and

one of them will be equal to X depending on the value of binary variable zi. Speciﬁcally, SPCA

(7) now becomes

w∗ :=

max
z∈Z,X,Wi1,Wi2∈Sd
+

(cid:40)

(cid:88)

i∈[n]

c(cid:62)
i Wi1ci : X = Wi1 + Wi2, ∀i ∈ [n], tr(X) = 1,

tr(Wi1) = zi, tr(Wi2) = 1 − zi, ∀i ∈ [n]

(cid:41)
.

Above, the matrix variables {Wi2}i∈[n] are redundant and can be replaced by inequality X (cid:23) Wi
(cid:3)

for each i ∈ [n]. Thus, we arrive at the equivalent reformulation (4) for SPCA.

8

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Theorem 1 presents the ﬁrst equivalent MISDP formulation (6) to SPCA. The resulting formu-

lation (6) has several interesting properties: (i) it can be directly solved via exact MISDP solvers

such as YALMIP; (ii) matrix variables X and {Wi}i∈[n] have dimension of d × d, where d is the

rank of matrix A. Thus, the size of SPCA (6) can be further reduced if the covariance matrix

A is low-rank; and (iii) the binary variables z can be separated from the other variables, so one

can apply the Benders decomposition to solving the SPCA (6). This result will be elaborated with

more details in the next subsection.

For large-scale instances, computing the continuous relaxation values of the SPCA (6) provides

us an upper bound to the optimal value or can be useful to check the quality of diﬀerent heuristics.

In the following, we show that the continuous relaxation value of SPCA (6) is not too far away

from the optimal value w∗. First, let w1 denote the continuous relaxation value, i.e.,

w1 :=

max
z∈Z,
X,W1,··· ,Wn∈Sd
+

(cid:40)

(cid:88)

i∈[n]

c(cid:62)
i Wici : tr(X) = 1, X (cid:23) Wi, tr(Wi) = zi, ∀i ∈ [n]

(cid:41)
,

(8)

where we let Z denote the continuous relaxation of set Z, i.e.,

(cid:26)

z ∈ [0, 1]n :

Z =

(cid:27)
.

zi = k

(cid:88)

i∈[n]

Theorem 2 The continuous relaxation value w1 of formulation (6) achieves a min{k, n/k} opti-

mality gap of SPCA, i.e.,

w∗ ≤ w1 ≤ min{k, n/k}w∗.

Proof. It is obvious that w∗ ≤ w1 since the feasible region of continuous relaxation (8) includes the

original decision space. Thus, it remains to show that (i) w1 ≤ kw∗ and (ii) w1 ≤ n/kw∗.

Part (i) w1 ≤ kw∗. For any feasible solution (z, X, {Wi}i∈[n]) to problem (8), we must have

(cid:88)

i∈[n]

c(cid:62)
i Wici ≤

(cid:88)

i∈[n]

c(cid:62)
i ci tr(Wi) =

(cid:88)

i∈[n]

zic(cid:62)

i ci ≤

(cid:88)

i∈[n]

ziw∗ = kw∗,

where the ﬁrst inequality is due to the fact that the trace of the product of two symmetric positive

semi-deﬁnite matrices is no larger than the product of the traces of these two matrices [10], the

ﬁrst equality is from tr(Wi) = zi for each i ∈ [n], the second inequality is because

c(cid:62)
i ci = λmax

(cid:0)cic(cid:62)

i

(cid:1) ≤ max

S⊆[n]:|S|=k

λmax

(cid:18)(cid:88)

(cid:19)

cjc(cid:62)
j

:= w∗,

j∈S

and the second equality is due to (cid:80)

i∈[n] zi = k.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

9

Part (ii) w1 ≤ n/kw∗. Similarly, given any feasible solution (z, X, {Wi}i∈[n]) of continuous relax-

ation (8), we must have

(cid:88)

i∈[n]

c(cid:62)
i Wici ≤

(cid:88)

i∈[n]

c(cid:62)
i Xci =

1
(cid:1)
(cid:0)n−1
k−1

(cid:88)

(cid:88)

c(cid:62)
i Xci ≤

S∈([n]
k )

i∈S

(cid:1)
(cid:0)n
k
(cid:0)n−1
k−1

(cid:1) w∗ =

n
k

w∗,

where the ﬁrst inequality is because Wi (cid:23) X and the second one is from Part (ii) in Lemma 1. (cid:3)
Theorem 2 shows that the continuous relaxation value of formulation (8) is at most min{k, n/k}

away from the true optimal value of SPCA (6), implying that if k → 1 or k → n, then the continuous

relaxation value w1 is very close to the true optimal value w∗, which is consistent with the numerical

study in Section 6.

2.2. Solving SPCA (6) and SDP Relaxation (8): Benders Decomposition It has been

recognized that large-scale SDPs are challenging to solve, so is the MISDP (6). In this subsection,

we apply the Benders decomposition [4, 18] to the proposed MISDP (6), which can be further

integrated into the branch and cut framework. By relaxing the binary vector z to be continuous,

the Benders Decomposition recasts the continuous SDP relaxation (8) as a maximin saddle point

problem, which enables the adoption of the eﬃcient subgradient method.

The main idea of Benders decomposition is to decompose SPCA (6) into two stages: ﬁrst, the

master problem is a pure integer maximization problem over z, and second, given a feasible z ∈ Z,

the subproblem is to maximize over the remaining variables (X, {Wi}i∈[n]). Thus, by separating

the binary variables, we rewrite the SPCA (6) as

w∗ := max
z∈Z

H1(z) :=

max
X,W1,··· ,Wd∈Sd
+

(cid:40)

(cid:88)

i∈[n]

c(cid:62)
i Wici : tr(X) = 1, X (cid:23) Wi, tr(Wi) = zi, ∀i ∈ [n]

(cid:41)
.

(9)

Benders decomposition is of particular interest when the subproblem H1(z) for any z ∈ Z is easy

to compute, which is, unfortunately, not the case. Therefore, it is desirable if we can specify the

function H1(z) for any given z ∈ Z in an eﬃcient way. Surprisingly, invoking Part(ii) in Lemma 1,

the strong duality of inner SDP maximization problem in (9) holds and the obtained dual problem

admits a closed-form solution for any binary variables z ∈ Z, which enables the subproblem to

generate valid inequalities to the master problem eﬃciently. The results are shown below.

Proposition 1 For the function H1(z) deﬁned in (9), we have

(i) For any z ∈ Z, function H1(z) is equivalent to
(cid:26)

(cid:19)

(cid:18) (cid:88)

(cid:88)

Qi

+

H1(z) =

min
µ,Q1,··· ,Qn∈Sd
+

λmax

which is concave in z.

µizi : cic(cid:62)

i (cid:22) Qi + µiId, 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2, ∀i ∈ [n]

(cid:27)
,

i∈[n]

i∈[n]

(10)

10

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

(ii) For any binary z ∈ Z, an optimal solution to problem (10) is µ∗

i = 0 if zi = 1 and (cid:107)ci(cid:107)2
2,

otherwise, and Q∗

i := (1 − µ∗

i /(cid:107)ci(cid:107)2

2)cic(cid:62)
i

for each i ∈ [n].

Proof. See Appendix A.2.

(cid:3)

The Part (ii) of Proposition 1 shows that given a solution z ∈ Z with its support S, the optimal

value to (10) is equal to

H1(z) = λmax

(cid:18)(cid:88)

(cid:19)

+

cic(cid:62)
i

(cid:88)

||ci||2
2,

i∈S

i∈[n]\S

which leads to an equivalent reformulation of SPCA (9) as

(cid:26)

w∗ = max
z∈Z

w : w ≤ λmax(ASS) +

(cid:107)ci(cid:107)2

2zi, ∀S ⊆ [n] : |S| = k

(cid:27)
.

(cid:88)

i∈[n]\S

(11)

Above, for any mixed binary solution ((cid:98)z, (cid:98)w) ∈ Z × R, the most violated constraint is

w ≤ λmax(A (cid:98)S (cid:98)S) +

(cid:88)

(cid:107)ci(cid:107)2

2zi,

i∈[n]\ (cid:98)S

where set (cid:98)S := {i ∈ [n] : (cid:98)zi = 1} denotes the support of (cid:98)z. We remark that the exact branch and
cut approach to solve SPCA (11) using callback functions will beneﬁt from these closed-form valid

inequalities.

Note that by relaxing the binary variables to be continuous, the relaxed problem (9) is equivalent

to the SDP relaxation (8). However, given z ∈ Z, the dual representation of function H1(z) in (10)

is still a diﬃcult SDP. Motivated by Part (ii) in Proposition 1, we propose a more eﬃcient upper

bound H 1(z) than H1(z) by letting Qi := (1 − µi/(cid:107)ci(cid:107)2

2)cic(cid:62)
i

for each i ∈ [n] to problem (10). In

the next theorem, we show that the relaxed H 1(z) becomes exact for any binary vector z ∈ Z and

the resulting upper bound of SPCA also achieves a min{k, n/k} optimality gap.

Theorem 3 The following results hold for the relaxed function H 1(z):

(i) For any z ∈ Z, function H1(z) is upper bounded by

(cid:26)

H 1(z) = min

µ

λmax

(cid:18) (cid:88)

(1 − µi/(cid:107)ci(cid:107)2

2)cic(cid:62)

i

i∈[n]

(cid:19)

(cid:88)

+

i∈[n]

µizi : 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2, ∀i ∈ [n]

(cid:27)
;

(12)

(ii) If z ∈ Z, then H1(z) = H 1(z) = λmax((cid:80)
(iii) The continuous relaxation value of SPCA

i∈[n] zicic(cid:62)

i ); and

w2 = max
z∈Z

H 1(z)

(13)

achieves a min{k, n/k} optimality gap of SPCA, i.e., w∗ ≤ w1 ≤ w2 ≤ min{k, n/k}w∗, where

w1 is deﬁned in (8).

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

11

Proof.

(i) The conclusion follows by choosing a feasible Qi := (1 − µi/(cid:107)ci(cid:107)2

2)cic(cid:62)
i

for each i ∈ [n] in the

representation (10).

(ii) For any z ∈ Z, we derive from Part (ii) in Proposition 1 that H 1(z) ≥ λmax((cid:80)

i∈[n] zicic(cid:62)

i ).

Thus, it is suﬃcient to show that H 1(z) ≤ λmax((cid:80)
by letting µi = 0 if zi = 0, and ||ci||2

2, otherwise in (12).

i∈[n] zicic(cid:62)

i ). Indeed, this can be done simply

(iii) By the proof of Theorem 2, to obtain the same optimality gap for (13) as SDP (8), we need

to show that H 1(z) ≤ (cid:80)

i∈[n] zic(cid:62)

i ci and H 1(z) ≤ λmax(A) = λmax((cid:80)

We must have H 1(z) ≤ (cid:80)
We also have H 1(z) ≤ λmax(A) = λmax((cid:80)

i ci by by letting µi = c(cid:62)
i∈[n] cic(cid:62)

i∈[n] zic(cid:62)

i∈[n] cic(cid:62)
i ci for all i ∈ [n] in (12).

i ) for any z ∈ Z.

i ) by letting µi = 0 for all i ∈ [n] in (12).

Then the rest of the proof follows directly from that of Theorem 2 and is thus omitted.

(cid:3)

We remark that: (i) Compared to H1(z), function H 1(z) in (12) only involves an n-dimensional

variable µ. The resulting relaxation (13) of SPCA can be viewed as a conventional saddle problem

so we apply the subgradient method with convergence rate of O(1/T ) to the search for optimal

solutions (see, e.g., [31]), which oﬀers an eﬃcient way to generate an upper bound of SPCA in

Section 6; (ii) On the other hand, the continuous relaxation value w1 = maxz∈Z H1(z) tends to be

stronger than w2 in (13). Thus, it is a tradeoﬀ between computational eﬀort and a better upper

bound; (iii) Surprisingly, both bounds w1, w2 achieve the same optimality gap of SPCA. This

implies that there might be room to improve the analysis of optimality gap in Theorem 2. We leave

this to interested readers; and (iv) more importantly, when z ∈ Z is binary, both problems (10)

and (12) have closed-form results, which are very helpful for using the branch and cut method.

3. Exact MISDP Formulation (II) The MISDP formulation (6) developed for SPCA in

the previous section mainly are inspired from Part(ii) and Part(iii) in Lemma 1. In this section, we

will propose another exact MISDP reformulation of SPCA using Part(i) and Part(ii) in Lemma 1.

Similarly, we will present the optimality gap of the corresponding SDP relaxation to demonstrate

the strength of the second formulation. It is worthy of noting that the proposed MISDP (6) requires

the positive semi-deﬁniteness of matrix A as it is built on Cholesky decomposition of A, but the

result in this section is more general and holds even matrix A is not positive semi-deﬁnite.

3.1. A Naive Exact MISDP Formulation We ﬁrst establish a naive exact MISDP formu-

lation of SPCA (2) based on Part (ii) in Lemma 1, and the resulting continuous relaxation value

is equal to λmax(A).

12

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Proposition 2 The SPCA (2) admits the following MISDP formulation:

(SPCA) w∗ := max

z∈Z,X∈Sn
+

(cid:26)

tr(AX) : tr(X) = 1, Xii ≤ zi, ∀i ∈ [n]

(cid:27)
.

and its continuous relaxation value is equal to λmax(A).

Proof. See Appendix A.3.

(14)

(cid:3)

The SPCA formulation (14) can be also found in [17]. However, our proof is quite diﬀerent and

shorter, since it does not involve sophisticated extreme point characterization of SDPs. Although

the MISDP (14) is equivalent to SPCA (2), the fact that its continuous relaxation value is equal to

λmax(A) demonstrates that it might be a weak formulation. This motivates us to further strengthen

the formulation (14) by adding valid inequalities in the next subsection.

3.2. A Stronger Reformulation with Two Valid Inequalities

In this subsection, we

ﬁrst propose two valid inequalities for SPCA (14) and derive the optimality gap of its continuous

relaxation value of the improved formulation.

After examining diﬀerent types of valid inequalities, we propose the following two types of valid

inequalities for the SPCA formulation (14).

Lemma 2 The following two inequalities are valid to SPCA (14)

(i) (cid:80)
(cid:16)(cid:80)

(ii)

j∈[n] X 2
j∈[n] |Xij|

(cid:17)2

ij ≤ Xiizi for all i ∈ [n]; and

≤ kXiizi for all i ∈ [n].

Proof. See Appendix A.4.

We make the following remarks about Lemma 2.

(cid:3)

(i) Many other valid inequalities are dominated by the two types of valid inequalities in Lemma 2

such as

|Xij| ≤ zi, X 2

ij ≤ Xiizj, X 2

ij ≤ zizj, ∀i, j ∈ [n];

(ii) Note that the two types of valid inequalities are both second order conic (see e.g., [3]), and

thus can be embedded into SDP solvers such as MOSEK, SDPT3; and

(iii) We further observe that the inequality Xii ≤ zi in (14) is dominated by the ﬁrst type of
j∈[n]\{i} X 2
The results in Lemma 2 together with Proposition 2 give rise to a stronger MISDP of SPCA

ij ≤ Xiizi and Xii ≥ 0 for each i ∈ [n].

inequalities with the facts that X 2

ii + (cid:80)

than formulation (14), which is summarized below.

Theorem 4 The SPCA (2) can reduce to following stronger MISDP formulation:

(SPCA) w∗ := max

z∈Z,X∈Sn
+

(cid:26)

tr(AX) : tr(X) = 1,

(cid:88)

j∈[n]

X 2

ij ≤ Xiizi,

(cid:18) (cid:88)

j∈[n]

(cid:19)2

(cid:27)

|Xij|

≤ kXiizi, ∀i ∈ [n]

.

(15)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

13

Let w3 denote the continuous relaxation value of SPCA formulation (15), i.e.,

(cid:26)

w3 := max

z∈Z,X∈Sn
+

tr(AX) : tr(X) = 1,

(cid:88)

j∈[n]

X 2

ij ≤ Xiizi,

(cid:18) (cid:88)

(cid:19)2

|Xij|

j∈[n]

≤ kXiizi, ∀i ∈ [n]

(cid:27)
.

(16)

Clearly, we have λmax(A) ≥ w3. We are going to prove that the continuous relaxation value can

be even stronger than a well-known SDP upper bound for SPCA (2) introduced by d’Aspremont

et al. [13], denoted by w4, that has been widely used for solving SPCA in literature. The upper

bound from [13] comes to the following formulation

(cid:26)

w4 := max
X∈Sn
+

tr(AX) : tr(X) = 1,

|Xij| ≤ k

(cid:27)
.

(cid:88)

(cid:88)

i∈[n]

j∈[n]

(17)

The formal comparison result is shown below.

Proposition 3 The upper bounds w3, w4 of SPCA deﬁned in (16) and (17), respectively, satisfy

w4 ≥ w3, i.e., the continuous relaxations value of the stronger MISDP (15) is stronger than the

optimal value of the SDP formulation (17) from [13].

Proof. To show that w4 ≥ w3, it is suﬃcient to prove that any feasible solution (z, X) of the

continuous relaxation problem (16), will satisfy the constraints in the SDP formulation (17).

Clearly, we have X ∈ S n

+ and tr(X) = 1. It remains that (cid:80)
(cid:115)(cid:88)

(cid:88)

√

√

(cid:112)

(cid:88)

k

Xiizi ≤

k

|Xij| ≤

(cid:88)

(cid:115)(cid:88)

Xii

zi = k,

(cid:80)

j∈[n] |Xij| ≤ k. Indeed, we have

i∈[n]

i∈[n]

j∈[n]

i∈[n]

i∈[n]

i∈[n]

where the ﬁrst inequality results from type (ii) inequalities in Lemma 2, the second one is due to
CauchySchwartz inequality, and the equality is due to tr(X) = 1 and (cid:80)
(cid:3)

i∈[n] zi = k.

Next, we show that the continuous relaxations value of the stronger MISDP (15) is also quite

close to the true value. This phenomenon is more striking in the numerical study.

Theorem 5 The continuous relaxations value of the stronger MISDP formulation (15) yields a

min{k, n/k} optimality gap for SPCA, i..e,

Proof. The proof is separated into two parts: (i) w3 ≤ kw∗ and (ii) w3 ≤ n/kw∗.

w∗ ≤ w3 ≤ min{k, n/k}w∗.

(i) w3 ≤ kw∗. For any feasible solution X to problem (16), we have
|Aij||Xij| ≤ w∗ (cid:88)

tr(AX) =

AijXij ≤

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

|Xij| ≤ kw∗,

i∈[n]

j∈[n]

i∈[n]

j∈[n]

i∈[n]

j∈[n]

where the ﬁrst inequality is due to taking the absolute values, the second one is based on the
fact that maxi∈[n]{Ai,i} ≤ w∗ and |Ai,j| ≤ (cid:112)Ai,iAj,j ≤ w∗ for each pair i, j ∈ [n], and the third
one can be obtained from the proof of Proposition 3.

14

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

(ii) w3 ≤ n/kw∗. The proof is similar to the one of Theorem 2 since w3 ≤ λmax(A) ≤ n/kw∗. (cid:3)

In general, our two proposed MISDP formulations (6) and (15) are not comparable although their

continuous relaxations have the same theoretical approximation gap, which will be also illustrated

in the numerical study section. The continuous relaxation of the MISDP formulation (15) might be

diﬃcult to solve due to lager size of its matrix variables and higher complexity of its constraints. In

the next subsection, we will discuss Benders decomposition for SPCA (15), where the subproblem

reduces to a second order conic program rather than an SDP.

3.3. Benders Decomposition The decomposition method developed for SPCA (15) in this

subsection follows from Section 2.2. Therefore, many details will be omitted for brevity. Similarly,

we decompose the proposed MISDP formulation (15) by a master problem over binary variables

z ∈ Z and a subproblem over the matrix variable X ∈ S n

+. Also, we reformulate SPCA (15) as the

following equivalent two-stage optimization problem

w∗ = max
z∈Z

H2(z) := max
X∈Sn
+

(cid:26)

tr(AX) : tr(X) = 1,

(cid:88)

j∈[n]

X 2

ij ≤ Xiizi,

(cid:18) (cid:88)

j∈[n]

(cid:19)2

(cid:27)

|Xij|

≤ kXiizi, ∀i ∈ [n]

.

(18)

It is favorable to derive an eﬃcient dual formulation of H2(z) for any given z ∈ Z such that its

subgradient can be easily computed. Indeed, invoking Part(ii) in Lemma 1 and dualizing the second

order conic constraints, the strong duality of inner maximization over X in (18) still holds. The

proof is similar to Proposition 1 and is thus omitted.

Proposition 4 For any z ∈ Z, function H2(z) is equivalent to

H2(z) =

min
µ,ν1,ν2,Λ,W1,W2,β

λmax (A + Λ + 1/2 Diag(µ1 + µ2 + ν1 + ν2) − W1 + W2)

+ 1/2(−µ1 + µ2)(cid:62)z + k/2(−ν1 + ν2)(cid:62)z,

s.t. βi + (W1)ij + (W2)ij ≤ 0, ∀i ∈ [n], j ∈ [n],

(cid:88)

Λ2

ij + (µi1)2 ≤ (µi2)2, ∀i ∈ [n],

j∈[n]
i + (νi1)2 ≤ (νi2)2, ∀i ∈ [n],
β2

(W1)ij ≥ 0, (W2)ij ≥ 0, ∀i ∈ [n], ∀j ∈ [n],

ν1, ν2 ∈ Rn

+, Λ, W1, W2 ∈ S n,

(19)

which is concave in z.

For the equivalent function H2(z) derived in Proposition 4, we remark that: (i) Note that for any

given z ∈ Z, function H2(z) can be solved as an second order conic program and escape from the

SDP curse. More eﬀectively, it can be solved via many ﬁrst-order methods (e.g., the subgradient

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

15

method) since the subgradient is easy to obtain and the projection only involves second order conic

constraints; (ii) On the other hand, when we solve the continuous relaxation

w3 = max
z∈Z

H2(z),

(20)

the subgradient method is also applicable to solve the entire maximin saddle problem with O(1/T )

rate of convergence (see, e.g., [31]); (iii) We can warm start the exact branch and cut algorithm

by solving the continuous relaxation (20), and add all the subgradient inequalities into the root

relaxed problem.

4. A Mixed-Integer Linear Program (MILP) for SPCA with Arbitrary Accuracy

The formulations developed in the previous section for solving SPCA either rely on MISDP solvers

or customized branch and cut algorithms, which does not leverage existing computational powers

of solvers such as CPLEX, Gurobi. In this section, motivated by the SPCA formulation (5) and

the identity of eigenvalues, we further derive an approximate mixed-integer linear program (MILP)

for SPCA with arbitrary accuracy (cid:15) > 0 and O(n + d + log((cid:15)−1)) binary variables. We also prove

the optimality gap of its corresponding LP relaxation. The results in this section assume that A

is positive semi-deﬁnite.

4.1. An MILP Formulation for SPCA The diﬃculty of SPCA (5) lies in how to convexify

the objective function, i.e., the largest eigenvalue of a symmetric matrix A. In particular, our

proposed MISDP formulations stem from the fact that the largest eigenvalue can be formulated as

an equivalent SDP problem. Through a diﬀerent lens, we represent the largest eigenvalue function

based on the natural deﬁnition of eigenvalues of a matrix, i.e.,

(cid:26)

λmax(A) = max
w,x∈Rn

w : Ax = wx, x (cid:54)= 0

(cid:27)
,

where x denotes an eigenvector and the nonzero constraint rules out the trivial solution x = 0.

This motivates us to recast SPCA formulation (5) as the following nonconvex problem

w∗ = max

w,x∈Rd,z∈Z

(cid:26)

w :

(cid:88)

i∈[n]

zicic(cid:62)

i x = wx, (cid:107)x(cid:107)∞ = 1

(cid:27)
,

(21)

where (cid:107)x(cid:107)∞ = 1 also excludes the trivial solution x = 0.

For any given z ∈ Z, the nonconvexity of SPCA formulation (21) lies in three aspects: (i) Bilin-

ear terms {zix}i∈[n]. They can be easily linearized using the disjunctive programming techniques

since vector z is binary; (ii) Constraint (cid:107)x(cid:107)∞ = 1. The nonconvex constraint (cid:107)x(cid:107)∞ = 1 can be

equivalently written as a disjunction with 2d sets below

∪j∈[d]

(cid:8)x ∈ Rd : xj = 1, (cid:107)x(cid:107)∞ ≤ 1(cid:9) ∪j∈[d]

(cid:8)x ∈ Rd : xj = −1, (cid:107)x(cid:107)∞ ≤ 1(cid:9).

16

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Due to the equivalence of x and −x in SPCA (21), it suﬃces to only keep ﬁrst d sets, i.e.,
(cid:8)x ∈ Rd : xj = 1, (cid:107)x(cid:107)∞ ≤ 1(cid:9). This disjunction can be equivalently described as an MILP using
the results in [2]; and (iii) Bilinear term wx. We can ﬁrst approximate variable w using binary

∪j∈[d]

expansion and then linearize the obtained bilinear terms by the same disjunctive technique as part

(i). The resulting MILP formulation is summarized in the following theorem.

Theorem 6 Given a threshold (cid:15) > 0, the following MILP is O((cid:15))-approximate to SPCA (2), i.e.,
(cid:15) ≤ (cid:98)w((cid:15)) − w∗ ≤ (cid:15)

√

d

(cid:98)w((cid:15)) :=

max
w,z∈Z,y,α,x,,δ,µ,σ

w

s.t. x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [n],

x =

(cid:88)

j∈[d]

σj, ||σj||∞ ≤ yj, σjj = yj, ∀j ∈ [d],

(cid:88)

j∈[d]

yj = 1,

x = µ(cid:96)1 + µ(cid:96)2, ||µ(cid:96)1||∞ ≤ α(cid:96), ||µ(cid:96)2||∞ ≤ 1 − α(cid:96), ∀(cid:96) ∈ [m],

w = wU − (wU − wL)

(cid:18) (cid:88)

(cid:19)

,

2−iαi

i∈[m]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]

cic(cid:62)

i δi1 − wU x + (wU − wL)

(cid:88)

(cid:96)∈[m]

2−(cid:96)µ(cid:96)1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

≤ (cid:15),

α ∈ {0, 1}m, y ∈ {0, 1}d,

(22)

where wL, wU separately denote the lower and upper bounds of SPCA, m := (cid:100)log2((wU − wL)(cid:15)−1)(cid:101)
and the inﬁnite norm inequality constraints can be easily linearized.

Proof. See Appendix A.5.

For the proposed MILP formulation (22), we remark that

(cid:3)

(i) This is the ﬁrst-known MILP representation with arbitrary accuracy O((cid:15)) in literature of

SPCA;

(ii) The MILP formulation (22), although compact, involves O(n + d + log (cid:15)−1) binary variables,

O(nd + d log (cid:15)−1) continuous variables, and O(nd + n log (cid:15)−1) linear constraints;

(iii) In SPCA (21), one might be curious about the choice of inﬁnite norm. Unfortunately, as far

as we are concerned, this is the only norm that leads to a compact MILP formulation;

(iv) In the MILP formulation (22), one might consider replacing the inﬁnite norm in the constraint
i∈[m] 2−iµi1||∞ ≤ (cid:15) by other norms, which will lead to
diﬀerent formulations (either MILP or mixed-integer conic program) and slightly diﬀerent

i δi1 − wU x + (wU − wL) (cid:80)

i∈[n] cic(cid:62)

|| (cid:80)

approximation bounds;

(v) Strong lower and upper bounds of SPCA wL, wU can speed up the solution procedure; and

(vi) Instead of building a relatively large-scale MILP formulation (22), one might solve d number
(cid:8)x : xj = 1, (cid:107)x(cid:107)∞ ≤ 1(cid:9).

of smaller-scale MILPs by enumerating each set of a disjunction ∪j∈[d]

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

17

The last remark is summarized in the following corollary.

Corollary 1 Given a threshold (cid:15) > 0, the optimal value of MILP (22) is equal to (cid:98)w((cid:15)) =
maxj∈[d] (cid:98)wj((cid:15)), where for each j ∈ [d], (cid:98)wj((cid:15)) is deﬁned as

(cid:98)wj((cid:15)) :=

max
w,z∈Z,y,α,x,δ,µ

w

s.t. x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [n],

||x||∞ ≤ 1, xj = 1,

x = µ(cid:96)1 + µ(cid:96)2, ||µ(cid:96)1||∞ ≤ α(cid:96), ||µ(cid:96)2||∞ ≤ 1 − α(cid:96), ∀(cid:96) ∈ [m],

w = wU − (wU − wL)

(cid:18) (cid:88)

(cid:19)

,

2−iαi

i∈[m]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]

cic(cid:62)

i δi1 − wU x + (wU − wL)

(cid:88)

i∈[m]

2−iµi1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

≤ (cid:15),

α ∈ {0, 1}m,

(23)

where wL, wU separately denote the lower and upper bounds of SPCA, m := (cid:100)log2((wU − wL)(cid:15)−1)(cid:101)
and the inﬁnite norm inequality constraints can be easily linearized.

Albeit being smaller-size, some MILPs deﬁned in Corollary 1 might be infeasible. Since the optimal

value of an infeasible maximization problem is −∞ by default, the result in Corollary 1 still holds.

However, one might need to be cautious when using this result and be aware of infeasibilities.

4.2. Theoretical Optimality Gap Similar to other two exact formulations, we are also

interested in deriving theoretical approximation bound for MILP formulation (22) by relaxing

binary variables z. Particularly, we assume that other binary variables y, α can be enumerated

eﬀectively. Our results show that the theoretical optimality gap is, in general, worse than the other

two bounds.

Theorem 7 Given a threshold (cid:15) > 0, by enforcing the binary variables z to be continuous, let w5((cid:15))

denote the optimal value of the relaxed MILP formulation (22). Then we have

w5((cid:15)) ≤ min (cid:8)k(

√

d/2 + 1/2), n/k

√

√

d + (n − k)(

d/2 + 1/2)(cid:9)w∗ + (cid:15)

√

d.

Proof. See Appendix A.6.

(cid:3)

5. Approximation Algorithms

In this section, motivated by the equivalent combinatorial

formulation (4), we prove and demonstrate the tightness of the approximation ratios of the well-

known greedy and local search algorithms for solving SPCA.

18

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

5.1. Greedy Algorithm The greedy algorithm has been widely used in many combinatorial

problems with the cardinality constraint. The greedy algorithm in this subsection is particularly

based on the combinatorial formulation (4), which proceeds as follows: Given a subset (cid:98)SG ⊆ [n]
denoting the selected vectors, it aims to ﬁnd a new vector from {ci}i∈[n]\ (cid:98)SG
to maximize the largest
eigenvalue of the sum of rank-one matrices obtained so far including the new one. The detailed

implementation can be found in Algorithm 1.

Algorithm 1 Greedy Algorithm for SPCA (4)

1: Input: n × n matrix A (cid:23) 0 of rank d and integer k ∈ [n]
2: Let A = C (cid:62)C denote its Cholesky factorization where C ∈ Rd×n
3: Let ci ∈ Rd denote the i-th column vector of matrix C for each i ∈ [n]

4: Let (cid:98)SG := ∅ denote the chosen set
5: for (cid:96) = 1, · · · , k do

6:

7:

Compute j∗ ∈ arg maxj∈[n]\ (cid:98)S{λmax((cid:80)
Add j∗ to the set (cid:98)SG

i∈ (cid:98)SG∪{j} cic(cid:62)

i )}

8: end for

9: Output: (cid:98)SG

The following result show that the greedy Algorithm 1 yields 1/k-approximation ratio.

Theorem 8 The greedy Algorithm 1 yields a k−1-approximation ratio for SPCA (4), i.e., the

output (cid:98)SG of Algorithm 1 satisﬁes

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

λmax

i∈ (cid:98)SG

≥

1
k

w∗.

Proof. Suppose that the optimal set of SPCA (4) is S∗, then we have

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

λmax

i∈S∗

≤

(cid:88)

i∈S∗

λmax(cic(cid:62)

i ) ≤ k max
i∈[n]

λmax(cic(cid:62)

i ) ≤ kλmax

(cid:18) (cid:88)

(cid:19)

,

cic(cid:62)
i

i∈ (cid:98)SG

where the ﬁrst inequality results from the convexity of largest eigenvalue function and the last one
is because at the ﬁrst iteration, the greedy Algorithm 1 must choose the largest-length vector. (cid:3)

The approximation ratio k−1 of greedy Algorithm 1 is tight, since there exists an example whose

greedy optimum is no better than k−1. This example is presented as below.

Example 1 For any integer k ∈ [d], let d = k + 1, n = 2k, and the vectors {ci}i∈[n] ⊆ Rd be

(cid:40)

ci =

ei,
ek+1,

if i ∈ [k],
if i ∈ [k + 1, n],

∀i ∈ [n].

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

19

Proposition 5 In Example 1, the output value of greedy Algorithm 1 is k−1-away from the true

optimal value of SPCA. That is, approximation ratio k−1 of greedy Algorithm 1 is tight.

Proof.

In Example 1, according to the greedy Algorithm 1, it will select c1, c2, · · · , ck at each

iteration, i.e., the output set is (cid:98)SG = [k]. Thus, the resulting largest eigenvalue of greedy Algorithm 1

is equal to 1.

Apparently, the true optimal value of Example 1 is equal to

(cid:18) (cid:88)

λmax

(cid:19)

cic(cid:62)
i

i∈[k+1,n]

= λmax

(cid:0)kek+1e(cid:62)

k+1

(cid:1) = k.

This completes the proof.

(cid:3)

5.2. Local Search Algorithm The local search algorithm can improve the existing solutions

and has been successfully used to solve many interesting machine learning and data analytics

problems, such as experimental design [26] and maximum entropy sampling [24]. This subsection

investigates the local search algorithm for SPCA (4) and proves its approximation ratio.

In the local search algorithm, we start with a size-k subset, and in each iteration, swap an

element of chosen set with one of the unchosen set as long as it improves the largest eigenvalue.

The detailed implementation can be found in Algorithm 2.

Algorithm 2 Local Search Algorithm for SPCA (4)

1: Input: n × n matrix A (cid:23) 0 of rank d and integer k ∈ [n]
2: Let A = C (cid:62)C denote its Cholesky factorization where C ∈ Rd×n
3: Let ci ∈ Rd denote the i-th column vector of matrix C for each i ∈ [n]

4: Initialize a size-k subset (cid:98)SL ⊆ [n]
5: do

6:

7:

8:

9:

for each pair (i, j) ∈ (cid:98)SL × ([n] \ (cid:98)SL) do
> λmax

(cid:96)∈ (cid:98)SL∪{j}\{i} c(cid:96)c(cid:62)

if λmax

(cid:16)(cid:80)

(cid:17)

(cid:96)

(cid:16)(cid:80)

(cid:96)∈ (cid:98)SL

(cid:17)

c(cid:96)c(cid:62)
(cid:96)

then

Update (cid:98)SL := (cid:98)SL ∪ {j} \ {i}

end if

10:

end for

11: while there is still an improvement

12: Output: (cid:98)SL

20

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Theorem 9 The local search Algorithm 2 returns a k−1-approximation ratio of SPCA, i.e., the

output (cid:98)SL of the local search Algorithm 2 satisﬁes

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

λmax

i∈ (cid:98)SL

≥

1
k

w∗.

Proof. First, for each j ∈ [n], we will show that

(cid:18) (cid:88)

(cid:19)

c(cid:96)c(cid:62)
(cid:96)

λmax

≥ λmax(cjc(cid:62)

j ).

(cid:96)∈ (cid:98)SL

(24)

To prove it, there are two cases to be discussed: whether j belongs to (cid:98)SL or not. The monotonicity

of the largest eigenvalue of sum of positive semi-deﬁnite matrices implies that the inequality (24)

holds if j ∈ (cid:98)SL. If j ∈ [n] \ (cid:98)SL, then the local optimality condition implies that there exist i ∈ (cid:98)SL

such that

(cid:18) (cid:88)

(cid:19)

c(cid:96)c(cid:62)
(cid:96)

λmax

≥ λmax

(cid:18) (cid:88)

(cid:19)

c(cid:96)c(cid:62)
(cid:96)

≥ λmax(cjc(cid:62)

j ),

(cid:96)∈ (cid:98)SL

(cid:96)∈ (cid:98)SL∪{j}\{i}

where the second inequality is due to the monotonicity of the largest eigenvalue of sum of positive

semi-deﬁnite matrices.

Second, suppose S∗ to be the optimal solution to SPCA (4), by inequality (24), then we have

w∗ = λmax

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

i∈S∗

≤

(cid:88)

i∈S∗

λmax(cic(cid:62)

i ) ≤ kλmax

(cid:18) (cid:88)

(cid:19)

,

c(cid:96)c(cid:62)
(cid:96)

(cid:96)∈ (cid:98)SL

where the ﬁrst inequality is because of the convexity of function λmax(·).

(cid:3)

We remark that Example 1 also conﬁrms the tightness of our analysis for local search Algorithm 2.

Proposition 6 In Example 1, the output value of local search Algorithm 2 is k−1-away from opti-

mal value of SPCA. That is, approximation ratio k−1 of local search Algorithm 2 is tight.

Proof. In Example 1, we show that the initial subset (cid:98)SL = [k] already satisﬁes the local optimality

condition.

Indeed, for each pair (i, j) ∈ (cid:98)SL × ([n] \ (cid:98)SL), we have

(cid:18) (cid:88)

λmax

(cid:19)

c(cid:96)c(cid:62)
(cid:96)

(cid:96)∈ (cid:98)SL∪{j}\{i}

= λmax(Id − eie(cid:62)

i ) = 1 = λmax(Id − ede(cid:62)

d ) = λmax

(cid:18) (cid:88)

(cid:19)

,

c(cid:96)c(cid:62)
(cid:96)

(cid:96)∈ (cid:98)SL

where the identities follow the construction of {ci}i∈[n] in Example 1.

Therefore, the set (cid:98)SL achieves the local optimum with largest eigenvalue of 1. Since the optimal
(cid:3)

value of SPCA is w∗ = k, the approximation ratio of set (cid:98)SL is equal to k−1.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

21

As an improved heuristic, local search Algorithm 2 can use the output of the greedy Algorithm 1

as an initial solution. The results in Theorem 9 and Proposition 6 imply that the integrated

algorithm still yields a k−1-approximation ratio of SPCA, while for solving the practical instances,

our numerical study shows that the integrated algorithm in fact works very well. Since the greedy

Algorithm 1 and local search Algorithm 2 repeatedly require to compute the largest eigenvalues,

at each iteration, we can apply the power iteration method to eﬃciently calculate the largest

eigenvalues [35] and use the eigenvectors from the previous iterations as a warm-start.

Finally, we remark that there is only one swap in the local search Algorithm 2. We can improve

it by increasing the number of swapping elements at each iteration, termed s-swap local search

with s ∈ [k]. The following result shows that s-swap local search can indeed achieve a better

approximation ratio.

Corollary 2 The approximation ratio of s-swap local search is sk−1 for any s ∈ [k]. The approxi-

mation ratio is tight.

Proof. First, let set (cid:98)SL denote the indices of selected vectors by s-swap local search algorithm.
Then following the same proof as that in Theorem 9, for any size-s set T ⊆ [n], we have

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

λmax

≥ λmax

(cid:18)(cid:88)

(cid:19)

.

cic(cid:62)
i

i∈ (cid:98)SL

i∈T

(25)

Let S∗ denote the optimal solution to SPCA (4), using the result (25), the optimal value of

SPCA w∗ is upper bounded by

w∗ = λmax

(cid:18) (cid:88)

(cid:19)

cic(cid:62)
i

= λmax

i∈S∗

(cid:18) 1
(cid:0)k−1
(cid:1)
s−1

(cid:88)

(cid:88)

(cid:19)

≤

cic(cid:62)
i

T ⊆S∗,|T |=s

i∈T

(cid:1)

(cid:0)k
s
(cid:0)k−1
s−1

(cid:1) λmax

(cid:18) (cid:88)

i∈ (cid:98)SL

cic(cid:62)
i

(cid:19)

=

k
s

(cid:18) (cid:88)

(cid:19)

.

cic(cid:62)
i

i∈ (cid:98)SL

Second, to show the tightness, let us consider the following example.

Example 2 For any integer k ∈ [d], let d = k + 1, n = (s + 1)k, and the vectors {ci}i∈[n] ⊆ Rd be

ci =






ei,
...
ei−(s−1)k,
ek+1,

if i ∈ [k],

if i ∈ [(s − 1)k + 1, sk],
if i ∈ [sk + 1, n],

∀i ∈ [n].

In Example 2, we show that the subset (cid:98)SL = [k − s + 1] ∪ {(cid:96)k + 1}(cid:96)∈[s−1] satisﬁes the s-swap local
optimality condition.

Indeed, for each pair (T1, T2) such that T1 ⊆ (cid:98)SL, T2 ⊆ ([n] \ (cid:98)SL) with |T1| = |T2| = s, we have

(cid:18) (cid:88)

λmax

(cid:19)

c(cid:96)c(cid:62)
(cid:96)

≤ s.

(cid:96)∈ (cid:98)SL∪T2\T1

22

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Therefore, the set (cid:98)SL achieves s-swap local optimum with largest eigenvalue of s. Since the optimal
(cid:3)
value of SPCA is w∗ = k, the approximation ratio of set (cid:98)SL is equal to sk−1 for SPCA.
Albeit theoretically sound, s-swap local search with s ≥ 2 might not be practical since it involves

O(n2) swaps at each iteration. Therefore, in the numerical study, we use the simple local search

Algorithm 2, which already works very well.

6. Numerical Study In this section, we conduct numerical experiments on six datasets with

number of features n ranging from 13 to 2365 to demonstrate the computational eﬃciency and the

solution quality of the MISDP (6), MISDP (15), and MILP (22) for exactly solving SPCA, the

continuous relaxations (8), (16) and heuristic Algorithms 1, 2 for approximately solving SPCA. All

the methods in this section are coded in Python 3.6 with calls to Gurobi 9.0 and MOSEK 9.0 on

a personal PC with 2.3 GHz Intel Core i5 processor and 8G of memory. The codes and data are

available at https://github.com/yongchunli-13/Sparse-PCA.

6.1. Pitprops Dataset We ﬁrst test the proposed three exact SPCA formulations (6), (15),

(22) and their continuous relaxations to solve a commonly-used benchmark instance, Pitprops

dataset Jeﬀers [20], which consists of 13 features (i.e., n = 13). In this instance, the computational

results of seven diﬀerent cases with k chosen from {4, · · · , 10} are displayed in Table 2, Table 3,

and Table 4.

For each testing case, we solve two MISDP formulations (6) and (15) using the branch and cut

method. As for the MILP (22), it can be simply solved in Gurobi. Throughout the numerical study

of MILP (22), we set (cid:15) = 10−4, use the best SDP relaxation values as the upper bound wU , and use

the local search Algorithm 2 to compute the lower bound wL. As the newly released Gurobi 9.0

is able to solve the non-convex quadratic program, thus for the purpose of comparison, we further

use Gurobi to solve the following SPCA formulation

w∗ := max

z∈Z,x∈Rn

(cid:110)

x(cid:62)Ax : ||x||2 = 1, ||x||1 ≤

√

k, |xi| ≤ zi, ∀i ∈ [n]

(cid:111)

.

(26)

The computation results of the exact methods are shown in Table 2. In particular, we let time(s)

denote the running time in seconds of each case and let Gurobi denote the performance of Gurobi

for solving SPCA (26). In table 2, we see that all the SPCA formulations (6), (15), (22) can be solved

to optimality within seconds, which demonstrates the eﬃciency of the proposed formulations. We

also compare the numerical performance of the MILP formulation (22) with formulation (26) using

the Gurobi solver, and it is clear that MILP is more eﬃcient and stable. Especially for the case of

k = 10, Gurobi has trouble ﬁnding the optimal solution of SPCA (26).

Although the theoretical optimality gaps of the proposed SDP relaxations (8) and (16) are the

same, these gaps in practice can be much smaller and can be signiﬁcantly diﬀerent from each other.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Table 2. Computational results of exact values with Pitprops dataset

n=13 SPCA MISDP (6)

MISDP (15)

23

Gurobi

k
4
5
6
7
8
9
10

w∗

w∗

time(s) w∗

2.9375 2.9375
3.4062 3.4062
3.7710 3.7710
3.9962 3.9962
4.0686 4.0686
4.1386 4.1386
4.1726 4.1726

1 2.9375
1 3.4062
1 3.7710
1 3.9962
1 4.0686
1 4.1386
1 4.1726

time(s)

time(s) w∗

MILP (22)
(cid:98)w((cid:15))
2 2.9375
2 3.4062
2 3.7710
1 3.9962
2 4.0686
2 4.1386
1 4.1726

1 2.9375
1 3.4062
2 3.7710
1 3.9962
2 4.0686
1 4.1387
1 4.1441

time(s)
1
1
1
3
12
30
83

We use MOSEK to solve both SDP relaxations. The numerical results can be found in Table 3,

where the SDP relaxation (17) proposed by d’Aspremont et al. [13] is presented as a benchmark

comparison. In Table 3, we use gap(%) to denote the optimality gap, which is computed as

100 × (Upper Bound − w∗)/w∗. It can be seen that the second SDP relaxation (16) is superior to

the ﬁrst SDP relaxation (8) on the ﬁrst ﬁve cases. When k is close to n, the ﬁrst SDP relaxation (8)

can be better. This ﬁnding is consistent with remarks after Theorem 2. In addition, as proved in

Proposition 3, we see that the second SDP relaxation (16) always outperforms the bound (17) by

d’Aspremont et al. [13]. Finally, the second SDP relaxation (16) and the bound (17) by d’Aspremont

et al. [13] are also not comparable.

Table 3. Computational results of upper bounds with Pitprops dataset

n=13 SPCA Benchmark (17) SDP Relaxation (8)

w∗

w4

gap(%) w1

gap(%) time(s) w3

k
4
5
6
7
8
9
10

2.9375 3.0172
3.4062 3.4581
3.7710 3.8137
3.9962 4.0316
4.0686 4.1448
4.1386 4.2063
4.1726 4.2186

2.71 3.1065
1.52 3.4868
1.13 3.7859
0.89 3.9962
1.87 4.0805
1.64 4.1386
1.10 4.1763

5.75
2.37
0.39
0.00
0.29
0.00
0.09

SDP Relaxation (16)
gap(%) time(s)
0.13
0.18
0.15
0.15
0.17
0.15
0.16

0.41
0.18
0.15
0.00
0.26
0.03
0.12

0.51 2.9495
0.55 3.4124
0.52 3.7767
0.43 3.9962
0.29 4.0793
0.00 4.1398
0.09 4.1778

Table 4 presents the objective values and optimality gaps of the proposed approximation algo-

rithms for solving the Pitprops instance, where we let LB denote the lower bound and compute

gap(%) by 100 × (w∗ − LB)/w∗. Note that we initialize the local search Algorithm 2 by the output

of greedy Algorithm 1. To further improve the two algorithms, at each iteration, we employ the

power iteration method to eﬃciently compute the largest eigenvalues [35] and warm-start it with

24

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

the good-quality eigenvectors from the previous iterations. In Table 4, we see that greedy Algo-

rithm 1 and local search Algorithm 2 successfully ﬁnd the optimal solutions and outperforms the

truncation algorithm proposed by [9].

Table 4. Computational results of lower bounds with Pitprops dataset

n=13 SPCA Truncation algorithm [9] Greedy Algorithm 1 Local Search Algorithm 2

k
4
5
6
7
8
9
10

w∗

LB gap(%)
1.57
0.32
0.36
0.08
0.09
0.18
3.91

2.9375 2.8913
3.4062 3.3951
3.7710 3.7576
3.9962 3.9929
4.0686 4.0648
4.1386 4.1313
4.1726 4.0094

time(s)

LB gap(%) time(s) LB gap(%)
0.00
0.00
0.00
0.00
0.00
0.00
0.00

1e-3 2.9375
1e-3 3.4062
1e-2 3.7710
1e-2 3.9962
1e-2 4.0686
1e-2 4.1386
1e-2 4.1726

0.00
0.00
0.00
0.00
0.00
0.00
0.00

1e-3 2.9375
1e-3 3.4062
1e-3 3.7710
1e-3 3.9962
1e-3 4.0686
1e-3 4.1386
1e-3 4.1726

time(s)

1e-2
1e-2
1e-2
1e-2
1e-2
1e-2
1e-2

6.2. Four Larger-scale Datasets

In this subsection, we conduct experiments on four larger

instances from Dey et al. [15] to further testify the eﬃciency of our proposed methods for SPCA,

which are Eisen-1, Eisen-2, Colon and Reddit with n =79, 118, 500, and 2000. Since the MILP

formulation (22) consistently outperforms two MISDP formulations (6) and (15). Thus, in this set

of numerical experiments, we will stick to the MILP formulation (22).

We ﬁrst compare the performances of diﬀerent heuristic methods using the Reddit dataset with

n = 2000 and k ∈ {10, . . . , 70}. Thus, there are 7 cases in total. We implement the greedy Algo-

rithm 1 and the local search Algorithm 2 and compare them with the best-known truncation

algorithm proposed by [9]. The numerical results are shown in Table 5. We see that the local search

Algorithm 2 provides the highest-quality solution of the three. The greedy Algorithm 1 is almost

equally as good as the truncation algorithm. Although the local search Algorithm 2 takes the

longest running time, the running time is quite reasonable given the size of the testing cases. Hence,

our computation experiments show that the local search Algorithm 2 consistently outperforms the

other two methods within a reasonably short time. Thus, we recommend using this algorithm to

solve practical problems.

Next, we obtain the local search Algorithm 2, the continuous relaxation bounds and exact values

of SPCA on the four instances, i.e., Eisen-1, Eisen-2, Colon and Reddit. For these instances,

MOSEK fails to solve our proposed SDP relaxations (8) and (16). Thus, instead, we use the

subgradient method to solve the continuous relaxation formulations (13) and (20). For the MILP

formulation (22), we set the time limit of Gurobi to be an hour. The computational results are

presented in Table 6, where we let UB denote the upper bound of SPCA, let VAL denote the

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

25

Table 5. Computational results of lower bounds with Reddit dataset

time (s)

time (s)

n=2000

k
10
20
30
40
50
60
70

Truncation
algorithm [9]
LB
1482.3205
1666.2397
1953.3711
2203.1715
2311.2407
2427.2685
2475.9581

Greedy
Algorithm 1
LB
3 1521.3081
2 1670.4712
2 1856.2875
2 2123.5635
2 2289.0371
3 2402.8345
2 2488.8991

Local Search
Algorithm 2
LB
1 1521.3083
4 1684.3943
7 1953.7502
10 2208.2452
13 2322.8204
16 2441.7020
19 2494.6142

time (s)
9
59
92
208
207
202
193

best lower bound of MILP (22) found if the time limit is reached, and let MIPgap(%) denote

the percentage of output MIP Gap from Gurobi. For these instances, we see that the local search

Algorithm 2 still performs very well and the subgradient method is also eﬃcient to solve the

continuous relaxation (13). The continuous relaxation (20) turns out to be very diﬃcult to compute,

and even more diﬃcult than the MILP formulation (22). For the instance Eisen-1, we see that

both the MILP formulation (22) and local search Algorithm 2 can ﬁnd the optimal solutions. This

further demonstrates the eﬀectiveness of the local search Algorithm 2.

Table 6. Computational results of lower bounds, upper bounds and exact values with four larger instances

Continuous

Continuous

MILP (22)

n

Case

Data

k
Eisen-1 79 10
79 20
Eisen-2 118 10
118 20

Local Search
Algorithm 2
LB
17.3355
17.7195
11.7182
19.3228
Colon 500 10 2641.2289
500 20 4255.6941
Reddit 2000 10 1521.3083
2000 20 1684.3943

UB
time(s)
17.9144
1
18.1309
1
13.8732
1
1
22.9268
1 2901.1105
3 4833.1900
9 1867.9965
59 2184.2436

Relaxation (13) Relaxation (20)
VAL MIPgap(%) time(s)
time(s) UB time(s)
34
17.3355
126
125
17.7195
85
3600
11.7182
-
3600
-
19.3228
3600
- 2641.2289
3600
- 4255.6941
-
-
-
-
-
-

14 17.7571
13 18.0362
-
89
-
90
-
342
-
344
-
1198
-
1241

0.00
0.00
18.39
18.65
9.84
13.57
-
-

6.3. Drugabuse Dataset We ﬁnally apply the proposed local search Algorithm 2 to the

Drugabuse Dataset with n = 2365 features, where the dataset comes from a questionnaire collected

by the National Survey on Drug Use and Health (NSDUH) in 2018. It has been reported [33] that

with the growing illicit online sale of controlled substances, deaths attributable to opioid-related

drugs have been more than quadrupled in the U.S. since 1999. Thus, it is important to select a

handful of features that the researchers can focus on for further exploration. Indeed, SPCA is a good

tool to reduce the complexity and improve the interpretability of the machine learning algorithms

26

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

by selecting the most important features. Our numerical ﬁnding of the case of k = 10 is illustrated

in Figure 1, where the vertical values correspond to the selected features of the ﬁrst PC, which

are scaled by 100. We see that among 10 features, there are three categories (i.e., inhalants, drug

injection, drug treatment), which are important for analyzing drug abuse. In particular, SPCA

selects 6 features related to drug treatment, which is consistent with the literature [11, 39] that

the treatment records of drug abuse are informative and important. Three drug injection questions

have been designed to understand the injection experience of diﬀerent special drugs, and it is well

known that drug injection users are at high risk for HIV and other blood-borne infections [32, 38].

Inhalants feature, corresponding to various accessible products that can easily cause addictions,

signiﬁcantly contributes to the increase of drug abuse [6, 14].

Figure 1. 10 features selected by local search Algorithm 2 for Drugabuse dataset

7. Extension to the Rank-one Sparse Singular Value Decomposition (R1-SSVD)

In

this section, we extend the proposed formulations and theoretical results to the rank-one sparse

singular value decomposition (R1-SSVD). R1-SSVD has been successfully used to analyze the row-

column associations within high-dimensional data (see, e.g., [28, 23, 36]). The goal of R1-SSVD is

to ﬁnd the best submatrix (possibly non-square) of a particular size whose largest singular value

is maximized, from a given matrix.

Formally, R1-SSVD can be formulated as

(R1-SSVD) w∗

SVD := max

u∈Rm,v∈Rn

(cid:8)u(cid:62)Av : ||u||2 = 1, ||v||2 = 1, ||u||0 = k1, ||v||0 = k2

(cid:9) ,

(27)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

27

where the matrix A ∈ Rm×n is known, m, n, and k1 ∈ [m] and k2 ∈ [n] are positive integers.

Our reduction of R1-SSVD (27) to SPCA (1) follows from the development of an augmented

symmetric matrix A ∈ S m+n

A =

(cid:21)

(cid:20) 0 A
A(cid:62) 0

.

(28)

Let x := [u(cid:62), v(cid:62)](cid:62) denote an (m + n)-dimensional vector. According to the identity

x(cid:62)Ax = (cid:2)u(cid:62) v(cid:62)(cid:3)

(cid:20) 0 A
A(cid:62) 0

(cid:21)
(cid:21) (cid:20)u
v

= 2u(cid:62)Av,

then R1-SSVD (27) can be reformulated as

w∗

SVD :=

1
2

max
x∈Rm+n

(cid:8)x(cid:62)Ax : ||x1:m||2 = 1, ||xm+1:m+n||2 = 1, ||x1:m||0 = k1, ||xm+1:m+n||0 = k2

(cid:9) , (29)

where we let x1:m denote the collection of m entries of vector x from index set [m] and xm+1,m+n

denote the n entries of x from index set [m + 1, m + n]. In R1-SSVD (29), we enforce the sparse

restrictions on both x1:m and xm+1,m+n. Thus, the R1-SSVD (29) can be viewed as a special case

of the conventional SPCA (1), where A is symmetric but not positive semi-deﬁnite and there are

two sparsity constraints instead of one.

Similarly, introducing binary variable zi = 1 if ith column of matrix A is chosen, 0, otherwise,

we can linearize the zero-norm constraints and recast R1-SSVD (29) as

w∗

SVD :=

1
2

max
x∈Rm+n,z∈ZSVD

(cid:26)

x(cid:62)Ax : ||x1:m||2 = 1, ||xm+1:m+n||2 = 1, |x|i ≤ zi, ∀i ∈ [m + n]

(30)

(cid:27)
,

where set ZSVD is deﬁned as

ZSVD :=

(cid:26)

z ∈ {0, 1}m+n :

(cid:88)

zi = k1,

(cid:88)

(cid:27)
.

zi = k2

i∈[m]

i∈[m+1,m+n]

The following lemma inspires us three exact mixed-integer formulations for R1-SSVD (30).

Lemma 3 Given a matrix A ∈ Rm×n, consider its augmented counterpart A deﬁned in

(28),

two integers k1 ∈ [m] and k2 ∈ [n], and three subsets S, S1, S2 ⊆ [m + n] such that

S ⊆ [m + n], |S| = k1 + k2, S1 = S ∩ [m], |S1| = k1 and S2 = S ∩ [m + 1, m + n], |S2| = k2. Then the

following identities must hold:

(i) The eigenvalues of the augmented submatrix AS,S are the singular values of submatrix AS1,S2

and their negations;

(ii) σmax(AS1,S2) = λmax(AS,S) = 1/2 maxx∈Rk1+k2 {x(cid:62)Ax : ||x1:k1||2 = 1, ||xk1+1:k1+k2||2 = 1} =

1/2 max

X∈S

k1+k2
+

(cid:110)
tr(AS,SX) : (cid:80)

j∈[k1] Xjj = 1, (cid:80)

(cid:111)
.
i∈[k1+1,k1+k2] Xii = 1

Proof. See Appendix A.7.

(cid:3)

28

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Notably, Part (ii) in Lemma 3 shows that R1-SSVD is equivalent to the following combinatorial

optimization problem

w∗

SVD := max

S⊆[m+n]

(cid:26)

λmax(AS,S) : |S ∩ [m]| = k1, |S ∩ [m + 1, m + n]| = k2

(cid:27)
.

(31)

The next four subsections present MISDP formulations (I) and (II), a MILP formulation, and

approximation algorithms, respectively.

7.1. MISDP Formulation (I) The fact that matrix A is symmetric but not positive semi-

deﬁnite impedes us to directly apply the results in Section 2. Fortunately, a simple remedy by

adding a new matrix σmax(A)Im+n to A ﬁxes this issue. That is, let us deﬁne

#

A

:= A + σmax(A)Im+n,

(32)

which is indeed positive semi-deﬁnite according to Part (i) in Lemma 3. More importantly, the new

matrix A

#

preserves all the sparsity properties of the original one A.

Thus, the combinatorial optimization R1-SSVD (30) is equivalent to

(cid:26)

w∗

SVD := max

S⊆[m+n]

λmax(A

#
S,S) : |S ∩ [m] = k1, |S ∩ [m + 1, m + n] = k2

(cid:27)

− σmax(A).

(33)

Now all the results in Section 2 are directly applicable to R1-SSVD (33). We highlight two

important ones below.

Theorem 10 The R1-SSVD (33) admits an equivalent MISDP formulation:

w∗

SVD :=

max
z∈ZSVD,
X,W1,··· ,Wd∈Sd
+

(cid:40)

(cid:88)

i∈[m+n]

c(cid:62)
i Wici : tr(X) = 1, X (cid:23) Wi, tr(Wi) = zi, ∀i ∈ [m + n]

− σmax(A),

(cid:41)

(34)

where A

#

= C (cid:62)C denotes the Cholesky factorization of A

#

with C ∈ Rd×(m+n), d is the rank of

#

A

, and ci ∈ Rd denotes the i-th column vector of matrix C for each i ∈ [m + n].

Theorem 11 The continuous relaxation value wSVD1 of formulation (34) satisﬁes

w∗

SVD ≤ wSVD1 ≤

(cid:113)

mnk−1

1 k−1

2 w∗

SVD.

Proof. See Appendix A.8.

(cid:3)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

29

7.2. MISDP Formulation (II) Since the results in Section 3 do not rely on the positive

semi-deﬁniteness of matrix A, they can be directly extended to R1-SSVD (30).

We ﬁrst illustrate a naive MISDP for R1-SSVD (30) based on Part (ii) in Lemma 3.

Proposition 7 The R1-SSVD (30) is equivalent to the following MISDP formulation:

w∗

SVD :=

1
2

(cid:40)

max
z∈ZSVD,X∈Sm+n

+

tr(AX) :

(cid:88)

j∈[m]

Xjj = 1,

(cid:88)

j∈[m+1,m+n]

Xjj = 1, Xii ≤ zi, ∀i ∈ [m + n]

(cid:41)
.

(35)

The R1-SSVD formulation (35) is rather weak and its continuous relaxation value is equal to

σmax(A). Fortunately, we can derive two types of valid inequalities from strengthening it as below.

Lemma 4 For R1-SSVD (35), the following second-order conic inequalities are valid:

(i) (cid:80)
(ii) ((cid:80)

ij ≤ ziXii, (cid:80)
j∈[m] X 2
j∈[m] |Xij|)2 ≤ k1Xiizi, ((cid:80)

j∈[m+1,m+n] X 2

ij ≤ ziXii for all i ∈ [m + n]; and
j∈[m+1,m+n] |Xij|)2 ≤ k2Xiizi for all i ∈ [m + n].

Proof. See Appendix A.9.

(cid:3)

The MISDP formulation for R1-SSVD (35) can be strengthened by adding these valid inequalities.

Similar to Theorem 5, we provide the optimality gap of its continuous relaxation value as below.

Theorem 12 The continuous relaxation value wSVD2 of R1-SSVD (35) with the inequalities in
√

Lemma 4 yields an optimality gap at most min{
(cid:26)(cid:112)

w∗

SVD ≤ wSVD2 ≤ min

k1k2, mnk−1
(cid:113)

1 k−1

2 }, i.e.,
(cid:27)

k1k2,

mnk−1

1 k−1

2

w∗

SVD.

7.3. An MILP Formulation with Arbitrary Accuracy Similarly, we can develop an

MILP formulation with arbitrary accuracy based on the Cholesky decomposition of matrix A

#

in

R1-SSVD (33). The proofs are similar to Section 4 and are thus omitted.

Theorem 13 Given a threshold (cid:15) > 0 and lower and upper bounds of the optimal R1-SSVD,
wL, wU , the following MILP is O((cid:15))-approximate to R1-SSVD (33), i.e., (cid:15) ≤ (cid:98)w((cid:15)) − w∗ ≤ (cid:15)

d:

√

(cid:98)w((cid:15)) :=

max
w,z∈ZSVD,y,α,x,,δ,µ,σ

w − σmax(A)

s.t. x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [m + n],

x =

(cid:88)

j∈[d]

σj, ||σj||∞ ≤ yj, σjj = yj, ∀j ∈ [d],

(cid:88)

j∈[d]

yj = 1,

x = µ(cid:96)1 + µ(cid:96)2, ||µ(cid:96)1||∞ ≤ α(cid:96), ||µ(cid:96)2||∞ ≤ 1 − α(cid:96), ∀(cid:96) ∈ [L],

w = wU − (wU − wL)

(cid:18) (cid:88)

(cid:19)

,

2−iαi

i∈[L]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[m+n]

cic(cid:62)

i δi1 − wU x + (wU − wL)

2−iµi1

(cid:88)

i∈[L]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

≤ (cid:15),

α ∈ {0, 1}L, y ∈ {0, 1}d,

(36)

30

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

where L := (cid:100)log2((cid:15)/(wU − wL))(cid:101).

Theorem 14 Given a threshold (cid:15) > 0, let wSVD3((cid:15)) denote the optimal value of MILP formulation

(36) by relaxing the binary variables z to be continuous. Then we have

wSVD3((cid:15)) ≤

(cid:114) mn
k1k2
m + n
k1 + k2

(cid:20)

(cid:26)

min

(k1 + k2)

√

d + 1
2

√

d + (m + n − k1 − k2)

,
√

d + 1
2

(cid:27)

(cid:21)
w∗

− 1

√

SVD + (cid:15)

d.

7.4. Approximation Algorithms for R1-SSVD We will investigate three approximation

algorithms for R1-SSVD (27): truncation algorithm, greedy algorithm, and local search algorithm.

7.4.1. Truncation algorithm The approximation algorithm in [9] via truncation is known

so far with the best approximation ratio O(n−1/3) for SPCA. We show that a similar truncation

also works for R1-SSVD.

First, we deﬁne the truncation operator as below.

Deﬁnition 1 (Normalized Truncation) Given a vector x ∈ Rn and an integer s ∈ [n], vector (cid:98)x
is an s-truncation of x if

(cid:40)

|xi|,
0,

(cid:98)xi =

if |xi| is one of the s largest absolute entries of x
otherwise

for each i ∈ [n]. The normalized s-truncation of x is deﬁned as (cid:98)x := (cid:98)x/(cid:107)(cid:98)x(cid:107)2, which is normalized
to be of unit length.

Then the truncation algorithm for R1-SSVD has the following two steps:

(i) Truncation in the standard basis: For each i ∈ [n], let (cid:98)ui ∈ Rm be the normalized k1-
truncation on the i-th column vector of A, and for each j ∈ [m], let (cid:98)vj ∈ Rn be the normalized
k2-truncation on the j-th row vector of A. Clearly, (cid:98)ui and (cid:98)vj are feasible to R1-SSVD (27);
(ii) Truncation in the eigen-space basis: Let v1 and u1 denote the right and left eigenvectors
of A corresponding to the largest singular value. We then deﬁne the vector (cid:98)u1 as the normalized
k1-truncation on u1 and deﬁne (cid:98)v1 as the normalized k2-truncation of the vector A(cid:62)
(cid:98)u1. It is clear
that ((cid:98)u1, (cid:98)v1) is also feasible to R1-SSVD (27).

The approximation results of the truncation procedure are summarized below.

Theorem 15 For R1-SSVD (27), the truncation algorithm yields an approximation ratio
(cid:113)

(cid:26)(cid:113)

(cid:27)

max

k−1
1 ,

k−1
2 ,

k1k2m−1n−1

.

(cid:112)

In particular, the approximation ratio is O(n−1/3) when k1 ≈ k2 and m ≈ n.

Proof. See Appendix A.10.

(cid:3)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

31

7.4.2. Greedy and Local Search Algorithms We design the greedy and local search algo-

rithms according to the following equivalent combinatorial formulation of R1-SSVD (27)

w∗

SVD :=

max
S1⊆[m],S2⊆[n]

(cid:26)

σmax(AS1,S2) : |S1| = k1, |S2| = k2

(cid:27)
.

(37)

Diﬀerent from SPCA (3), the R1-SSVD (37) maximizes the largest singular value of any k1 × k2

submatrix rather than that of any size k-principal submatrix. Therefore, to solve R1-SSVD (37),

we adapt the greedy Algorithm 1 or the local search Algorithm 2 considering selecting a row and/or

a column at each iteration.

Speciﬁcally, for the greedy algorithm, let two subsets S1, S2 denote the index sets of the selected

columns and rows, respectively. We ﬁrst initialize the greedy algorithm by selecting the entry of A

that takes the largest absolute value. Then, we add one element into each subset at each iteration,

which maximizes the largest singular value of the obtained submatrix, unless we are not able to.

Next, we continue to selection one row (or one column) at each iteration, until we reach a k1 × k2

submatrix. The detailed implementation can be found in Algorithm 3.

Given an initial feasible solution (S1, S2) to R1-SSVD (37), the adapted local search algorithm

performs the swapping procedure on both S1 and S2 (see Algorithm 4 for details) simultaneously.

Algorithm 3 Greedy Algorithm for R1-SSVD (37)

1: Input: m × n matrix A (cid:23) 0, integers k1 ∈ [m], k2 ∈ [n]

2: Let (cid:98)S1 := ∅ and (cid:98)S2 := ∅ denote the selected rows and columns, separately
3: Compute j∗

(cid:8)|(A{j1},{j2}|(cid:9)

1 , j∗

4: Add j∗

1 , j∗

2 ∈ arg maxj1∈[m],j2∈[n]
2 to sets (cid:98)S1 and (cid:98)S2, separately

5: for (cid:96) = 2, · · · , max{k1, k2} do

6:

7:

8:

9:

10:

11:

12:

13:

if (cid:96) ≤ min{k1, k2} then

Compute j∗

Compute j∗

1 ∈ arg maxj1∈[m]\ (cid:98)S1
2 ∈ arg maxj2∈[n]\ (cid:98)S2

(cid:110)

σmax

(cid:16)

(cid:110)

σmax

(cid:16)

A (cid:98)S1∪{j1}, (cid:98)S2
A (cid:98)S1, (cid:98)S2∪{j2}

(cid:17)(cid:111)

(cid:17)(cid:111)

and add j∗

1 to set (cid:98)S1

and add j∗

2 to set (cid:98)S2

else if k1 ≤ k2 then

Compute j∗

2 ∈ arg maxj2∈[n]\ (cid:98)S2

(cid:8)σmax

(cid:0)AS1,S2∪{j2}

(cid:1)(cid:9) and add j∗

2 to set (cid:98)S2

else

Compute j∗

1 ∈ arg maxj1∈[m]\ (cid:98)S1

(cid:110)

σmax

(cid:16)

A (cid:98)S1∪{j1}, (cid:98)S2

(cid:17)(cid:111)

and add j∗

1 to set (cid:98)S1

end if

14: end for

15: Output: (cid:98)S1, (cid:98)S2

The following results illustrate the theoretical performance guarantees of the two algorithms for

R1-SSVD and show that the approximation ratios are both tight.

32

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Theorem 16 For the greedy Algorithm 3 and the local search Algorithm 4, we have (i) both algo-

rithms achieve a (

√

k1k2)−1-approximation ratio of R1-SSVD (37), and (ii) the ratio is tight.

Proof. See Appendix A.11.

(cid:3)

Algorithm 4 Local Search Algorithm for R1-SSVD (37)

1: Input: m × n matrix A (cid:23) 0 and integers k1 ∈ [m], k2 ∈ [n]

2: Initialize a size-k1 subset (cid:98)S1 ⊆ [m] and a size-k2 subset (cid:98)S2 ⊆ [n]
3: do

4:

5:

6:

7:

8:

for each pair (i1, j1, i2, j2) ∈ (cid:98)S1 × ([m] \ (cid:98)S1) × (cid:98)S2 × ([n] \ (cid:98)S2) do

if σmax

(cid:0)AS1∪{j1}\{i1},S2∪{j2}\{i2}

(cid:1) > σmax (AS1,S2) then
Update (cid:98)S1 := (cid:98)S1 ∪ {j1} \ {i1}, (cid:98)S2 := (cid:98)S2 ∪ {j2} \ {i2}

end if

end for

9: while there is still an improvement

10: Output: (cid:98)S1, (cid:98)S2

8. Extension to Sparse Fair PCA In this section, we study the Sparse Fair PCA (SFPCA)

and show its approximate MISDP formulation. The fair PCA has been recently studied in the

literature (see, e.g., [34, 37]). The goal of SFPCA is to seek the best principal submatrices of

multi-group covariance matrices to achieve the relatively similar objective values among diﬀerent

groups.

Suppose there are s groups and their corresponding covariance matrices are {Ai}i∈[s]. Then the

SFPCA can be formulated as

w∗

F := max

x

(cid:110)

min
i∈S

x(cid:62)Aix : ||x||2 = 1, ||x||0 ≤ k

(cid:111)

.

(38)

By introducing binary variables z and linearizing the objective function, we obtain

w∗

F := max
w,x,z∈Z

(cid:8)w : w ≤ x(cid:62)Aix, ∀i ∈ [s], ||x||2 = 1, −zi ≤ xi ≤ zi, ∀i ∈ [n](cid:9) .

(39)

As the SFPCA (39) is quite diﬀerent from SPCA, it is not surprising that the results in Section 2

and Section 4 do not apply to SFPCA (39). Fortunately, the results in Section 3 do provide an

interesting upper bound for SFPCA (39), which can be exact when there are s = 2 groups of

covariance matrices. Introducing a rank-one positive semi-deﬁnite matrix variable X ∈ S n

+ such

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

33

that X (cid:23) xx(cid:62), dropping the rank-one restriction, and adding the valid inequalities in Theorem 4,

the problem (39) can be upper bounded by

wF := max

w,X,z∈Z

(cid:26)

w : w ≤ tr(AiX), ∀i ∈ [s], tr(X) = 1,

(cid:88)

j∈[n]

X 2

ij ≤ Xiizi,

(cid:18) (cid:88)

(cid:19)2

|Xij|

j∈[n]

≤ kXiizi, ∀i ∈ [n]

(cid:27)
.

(40)

The following result shows that if s = 2, then the approximation (40) is exact, otherwise, it

provides an upper bound of SFPCA (39).

Proposition 8 For the MISDP formulation (40), we have

(i) The optimal value of MISDP formulation (40) provides an upper bound of SFPCA (39), i.e.,

wF ≥ w∗

F . Also, when s = 2, the formulation (40) becomes exact, i.e., wF = w∗
(ii) There exists an optimal solution (w∗, X ∗, z∗) of of MISDP (40) such that the rank of X ∗ is

F ; and

at most 1 + (cid:98)(cid:112)2s + 9/4 − 3/2(cid:99).

Proof.

(i) It is clear that wF ≥ w∗

F since we drop the rank-one restriction on X of MISDP formulation

(40). On the other hand, for the case of s = 2, theorem 1.1 in [37] shows that for any feasible

solution (w, X, z), there exists a rank-one semi-deﬁnite matrix (cid:99)X such that the new solution
(w, (cid:99)X, z) is also feasible and achieves the same objective value. Thus, we must have wF = w∗
F ;

(ii) Suppose (w, X, z) denotes an optimal solution of MISDP (40). Let S = {i ∈ [n] : zi = 1}. Then

according to theorem 1.7 in [37], there exists a semi-deﬁnite matrix (cid:99)X of the rank at most
1 + (cid:98)(cid:112)2s + 9/4 − 3/2(cid:99) such that the new solution (w, (cid:99)X, z) is also optimal.

(cid:3)

Proposition 8 shows that two-group SFPCA (39) admits an MISDP representation, while MISDP

formulation (40) provides a low-rank solution in general for SFPCA when s > 2. It is worthy of

mentioning that the results in Proposition 8 work for any convex fairness measure.

9. Conclusion In practice, to tune the parameter k via cross-validation, our developed greedy

and local search algorithms can be quickly warm started from solution procedure in the previous

iterations. We anticipate that the theoretical optimality gaps of three exact formulations for SPCA

and R1-SSVD are not tight and can be further strengthened. The analysis of the optimality gap

of sparse fair PCA requires new techniques, which can be an exciting research direction. Also, it

might be desirable to study robust sparse PCA when the datasets are noisy or contain outliers.

34

References

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

[1] Amini AA, Wainwright MJ (2008) High-dimensional analysis of semideﬁnite relaxations for sparse prin-

cipal components. 2008 IEEE International Symposium on Information Theory, 2454–2458 (IEEE).

[2] Balas E (1975) Disjunctive programming: cutting planes from logical conditions. Nonlinear Programming

2, 279–312 (Elsevier).

[3] Ben-Tal A, Nemirovski A (2001) Lectures on modern convex optimization: analysis, algorithms, and

engineering applications, volume 2 (Siam).

[4] Benders JF (1962) Partitioning procedures for solving mixed-variables programming problems. Numer.

Math. 4(1):238252, ISSN 0029-599X, URL http://dx.doi.org/10.1007/BF01386316.

[5] Berk L, Bertsimas D (2019) Certiﬁably optimal sparse principal component analysis. Mathematical

Programming Computation 11(3):381–420.

[6] Breakey WR, Goodell H, Lorenz PC, McHugh PR (1974) Hallucinogenic drugs as precipitants of

schizophrenia. Psychological Medicine 4(3):255–261.

[7] Carrizosa E, Guerrero V (2014) rs-sparse principal component analysis: A mixed integer nonlinear

programming approach with vns. Computers & operations research 52:349–354.

[8] Chaib S, Gu Y, Yao H (2015) An informative feature selection method based on sparse pca for vhr

scene classiﬁcation. IEEE Geoscience and Remote Sensing Letters 13(2):147–151.

[9] Chan SO, Papailliopoulos D, Rubinstein A (2016) On the approximability of sparse pca. Conference on

Learning Theory, 623–646.

[10] Coope I (1994) On matrix trace inequalities and related topics for products of hermitian matrices.

Journal of mathematical analysis and applications 188(3):999–1001.

[11] Coughlin LN, Tegge AN, Sheﬀer CE, Bickel WK (2020) A machine-learning approach to predicting

smoking cessation treatment outcomes. Nicotine and Tobacco Research 22(3):415–422.

[12] d’Aspremont A, Bach F, Ghaoui LE (2012) Approximation bounds for sparse principal component

analysis. arXiv preprint arXiv:1205.0121 .

[13] d’Aspremont A, Ghaoui LE, Jordan MI, Lanckriet GR (2005) A direct formulation for sparse pca using

semideﬁnite programming. Advances in neural information processing systems, 41–48.

[14] De Barona MS, Simpson DD (1984) Inhalant users in drug abuse prevention programs. The American

journal of drug and alcohol abuse 10(4):503–518.

[15] Dey SS, Mazumder R, Wang G (2018) A convex integer programming approach for optimal sparse pca.

arXiv preprint arXiv:1810.09062 .

[16] dAspremont A, Bach F, Ghaoui LE (2008) Optimal solutions for sparse principal component analysis.

Journal of Machine Learning Research 9(Jul):1269–1294.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

35

[17] Gally T, Pfetsch ME (2016) Computing restricted isometry constants via mixed-integer semideﬁnite

programming. preprint, submitted .

[18] Geoﬀrion AM (1972) Generalized benders decomposition. Journal of optimization theory and applica-

tions 10(4):237–260.

[19] He Y, Monteiro RD, Park H (2011) An algorithm for sparse pca based on a new sparsity control criterion.

Proceedings of the 2011 SIAM International Conference on Data Mining, 771–782 (SIAM).

[20] Jeﬀers J (1967) Two case studies in the application of principal component analysis. Journal of the

Royal Statistical Society: Series C (Applied Statistics) 16(3):225–236.

[21] Jiang R, Fei H, Huan J (2012) A family of joint sparse pca algorithms for anomaly localization in

network data streams. IEEE Transactions on Knowledge and Data Engineering 25(11):2421–2433.

[22] Journ´ee M, Nesterov Y, Richt´arik P, Sepulchre R (2010) Generalized power method for sparse principal

component analysis. Journal of Machine Learning Research 11(2).

[23] Lee M, Shen H, Huang JZ, Marron J (2010) Biclustering via sparse singular value decomposition.

Biometrics 66(4):1087–1095.

[24] Li Y, Xie W (2020) Best principal submatrix selection for the maximum entropy sampling problem:

Scalable algorithms and performance guarantees. arXiv preprint arXiv:2001.08537 .

[25] Luss R, dAspremont A (2010) Clustering and feature selection using sparse principal component analysis.

Optimization and Engineering 11(1):145–157.

[26] Madan V, Singh M, Tantipongpipat U, Xie W (2019) Combinatorial algorithms for optimal design.

Conference on Learning Theory, 2210–2258.

[27] Magdon-Ismail M (2017) Np-hardness and inapproximability of sparse pca. Information Processing

Letters 126:35–38.

[28] Min W, Liu J, Zhang S (2016) L0-norm sparse graph-regularized svd for biclustering. arXiv preprint

arXiv:1603.06035 .

[29] Moghaddam B, Weiss Y, Avidan S (2006) Spectral bounds for sparse pca: Exact and greedy algorithms.

Advances in neural information processing systems, 915–922.

[30] Naikal N, Yang AY, Sastry SS (2011) Informative feature selection for object recognition via sparse pca.

2011 International Conference on Computer Vision, 818–825 (IEEE).

[31] Nedi´c A, Ozdaglar A (2009) Subgradient methods for saddle-point problems. Journal of optimization

theory and applications 142(1):205–228.

[32] Ompad DC, Ikeda RM, Shah N, Fuller CM, Bailey S, Morse E, Kerndt P, Maslow C, Wu Y, Vlahov D,

et al. (2005) Childhood sexual abuse and age at initiation of injection drug use. American journal of

public health 95(4):703–709.

[33] Overdose O (2018) Understanding the epidemic. Atlanta, Centers for Disease Control and Prevention .

36

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

[34] Samadi S, Tantipongpipat U, Morgenstern JH, Singh M, Vempala S (2018) The price of fair pca: One

extra dimension. Advances in Neural Information Processing Systems, 10976–10987.

[35] Semlyen A, Angelidis G (1995) Eﬃcient calculation of critical eigenvalue clusters in the small signal

stability analysis of large power systems .

[36] Sill M, Kaiser S, Benner A, Kopp-Schneider A (2011) Robust biclustering by sparse singular value

decomposition incorporating stability selection. Bioinformatics 27(15):2089–2097.

[37] Tantipongpipat U, Samadi S, Singh M, Morgenstern JH, Vempala S (2019) Multi-criteria dimensionality

reduction with applications to fairness. Advances in Neural Information Processing Systems, 15135–

15145.

[38] Thomas DL, Vlahov D, Solomon L, Cohn S, Taylor E, Garfein R, Nelson KE (1995) Correlates of

hepatitis c virus infections among injection drug users. Medicine 74(4):212–220.

[39] Volkow ND, Fowler JS, Wang GJ, Swanson JM, Telang F (2007) Dopamine in drug abuse and addiction:

results of imaging studies and treatment implications. Archives of neurology 64(11):1575–1579.

[40] Zhang Y, dAspremont A, El Ghaoui L (2012) Sparse pca: Convex relaxations, algorithms and applica-

tions. Handbook on Semideﬁnite, Conic and Polynomial Optimization, 915–940 (Springer).

[41] Zhang Y, Ghaoui LE (2011) Large-scale sparse principal component analysis with application to text

data. Advances in Neural Information Processing Systems, 532–539.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

37

Appendix A. Proofs

A.1 Proof of Lemma 1

Lemma 1 For a symmetric matrix A ∈ S n and a size-k set S ⊆ [n], the followings must hold:

(i) maxx∈Rn

(cid:8)x(cid:62)Ax : ||x||2 = 1, xi = 0, ∀i /∈ S(cid:9) = λmax(AS,S),
(ii) maxX∈Sk
{tr(AS,SX) : tr(X) = 1} = λmax(AS,S), and
(iii) If matrix A is positive semi-deﬁnite, then λmax(AS,S) = λmax((cid:80)

+

i ), where A = C (cid:62)C,
C ∈ Rd×n denotes the Cholesky factorization matrix of A, d is the rank of A, and ci ∈ Rd
denotes i-th column vector of C for each i ∈ [n].

i∈S cic(cid:62)

Proof. Part (i) Given a size-k set S ⊆ [n], the maximization problem

reduces to

(cid:8)x(cid:62)Ax : ||x||2 = 1, xi = 0, ∀i /∈ S(cid:9)

max
x∈Rn

(cid:8)x(cid:62)AS,Sx : ||x||2 = 1(cid:9) ,

max
x∈Rk

which is exactly the deﬁnition of the largest eigenvalue of principal submatrix AS,S.
Part (ii) According to Part (i), it is suﬃcient to show that v∗ = (cid:98)v, where v∗, (cid:98)v are deﬁned as

v∗ : = max
X∈Sk
+
(cid:98)v : = max
x∈Rk

{tr(AS,SX) : tr(X) = 1} ,

(cid:8)x(cid:62)AS,Sx : ||x||2 = 1(cid:9) .

(41)

(42)

First, we must have v∗ ≥ (cid:98)v. Indeed, for any feasible x ∈ Rk to problem (42) such that ||x||2 = 1,
we can construct a positive semi-ﬁnite matrix by X = xx(cid:62), which is feasible to problem (41) and

yields the same objective value.

(cid:80)

Second, to prove (cid:98)v ≥ v∗, we let X ∗ ∈ S k
i∈[k] λiqiq(cid:62)
must satisfy (cid:80)

+ denote an optimal solution to problem (41) and X ∗ =
+, the eigenvalues
i∈[k] λi = 1 and λi ≥ 0 for each i ∈ [k]. Thus, the optimal value v∗ of problem (41) is

i denote its spectral decomposition. Since tr(X ∗) = 1 and X ∗ ∈ S k

equal to

v∗ = tr(AS,SX ∗) =

(cid:88)

i∈[k]

λiq(cid:62)

i AS,Sqi ≤ max
i∈[k]

q(cid:62)
i AS,Sqi ≤ (cid:98)v,

where the inequality is due to (cid:80)
Part (iii) For a positive semi-deﬁnite matrix A, let A = C (cid:62)C denote the Cholesky factorization
of A and C ∈ Rd×n, thus we have

i∈[k] λi = 1 and λi ≥ 0 for each i ∈ [k].

λmax(AS,S) = λmax(C (cid:62)

S CS) = λmax(CSC (cid:62)

S ),

where the second equality is because for any matrix, its largest singular value is equal to that of
(cid:3)

its transpose.

38

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

A.2 Proof of Proposition 1

Proposition 1 For the function H1(z) deﬁned in (9), we have

(i) For any z ∈ Z, function H1(z) is equivalent to

H1(z) =

min
µ,Q1,··· ,Qn∈Sd
+

(cid:26)

(cid:18) (cid:88)

(cid:19)

Qi

+

λmax

i∈[n]

(cid:88)

i∈[n]

which is concave in z.

µizi : cic(cid:62)

i (cid:22) Qi + µiId, 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2, ∀i ∈ [n]

(cid:27)
,

(10)

(ii) For any binary z ∈ Z, an optimal solution to problem (10) is µ∗

i = 0 if zi = 1 and (cid:107)ci(cid:107)2
2,

otherwise, and Q∗

i := (1 − µ∗

i /(cid:107)ci(cid:107)2

2)cic(cid:62)
i

for each i ∈ [n].

Proof. Part (i). We split the proof of strong duality into two cases depending on whether z is a

relative interior point of set Z or not.

Case a. We will ﬁrst prove the result by assuming that z is in the relative interior of set Z, i.e.,

0 < zi < 1 for each i ∈ [n]. For the inner maximization problem in (9), we dualize the

constraint X (cid:23) Wi, tr(Wi) = zi with Lagrangian multiplier Qi ∈ S d

+ and µi for each i ∈ [n].
Note that the constraints X (cid:23) Wi, tr(Wi) = zi for each i ∈ [n] and X, W1, · · · , Wn ∈ S d
+

can be always strictly satisﬁed since 0 < zi < 1. Thus, according to the strong duality of

general conic program (see, e.g., Theorem 1.4.4 in [3]), function H1(z) can be rewrite as

min
µ,Q1,··· ,Qn∈Sd
+

max
X,W1,··· ,Wn∈Sd
+

(cid:40)

(cid:88)

i∈[n]

c(cid:62)
i Wici +

(cid:88)

i∈[n]

tr (Qi(X − Wi)) +

(cid:88)

i∈[n]

µi (zi − tr(Wi)) : tr(X) = 1

(cid:41)
.

Then the inner maximization problem (43) over Wi for each i ∈ [n] and X yields

tr (cid:0)(cic(cid:62)

i − Qi − µiId)Wi

(cid:19)

(cid:19)

(cid:18)(cid:18) (cid:88)

tr

i∈[n]

Qi

X

: tr(X) = 1

= λmax

(cid:1) =

(cid:40)

0,
∞,
(cid:27)

i (cid:22) Qi + µiId,

cic(cid:62)
otherwise.
(cid:18) (cid:88)

(cid:19)

,

Qi

i∈[n]

max
Wi∈Sd
+
(cid:26)

max
X∈Sd
+

where the second identity is due to Part(ii) of Lemma 1.

Thus, problem (43) can be simpliﬁed as

H1(z) =

min
µ,Q1,··· ,Qn∈Sd
+

(cid:26)

λmax

(cid:18) (cid:88)

(cid:19)

Qi

+

i∈[n]

(cid:88)

i∈[n]

µizi : cic(cid:62)

i (cid:22) Qi + µiId, ∀i ∈ [n]

(cid:27)
.

(43)

(44)

We show that for the minimization problem (44), any optimal solution (µ, Q1, · · · , Qn)

must satisfy 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2 for each i ∈ [n]. We prove it by contradiction. Suppose that
there exits an optimal solution (µ, Q1, · · · , Qn) to the problem (44) such that µj < 0 for

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

39

some j ∈ [n]. Then, we can construct a new feasible solution (µ, Q1, · · · , Qn), which is
exactly equal to (µ, Q1, · · · , Qn) except

The new solution yields the objective value

µj = 0, Qj = Qj + µjId.

H1(z) + µj − µjzj = H1(z) + µj(1 − zj) < H1(z),

which is a contradiction to the optimality of (µ, Q1, · · · , Qn). Similarly, suppose that there
exits an optimal solution (µ, Q1, · · · , Qn) to the problem (44) such that µj > (cid:107)ci(cid:107)2

2 for
some j ∈ [n]. Similarly, we can arrive at a contradiction by deﬁning a new feasible solution
(µ, Q1, · · · , Qn), which is exactly equal to (µ, Q1, · · · , Qn) except µj = (cid:107)ci(cid:107)2
2.

Therefore, (44) can be reduced to (10).

Case b. Now we consider the case that z is not in the relative interior of Z and deﬁne two sets

T0 := {i ∈ [n] : zi = 0} and T1 := {i ∈ [n] : zi = 1}. Thus, at least one of the two sets is not

empty. In this case, we ﬁrst observe that H1(z) in (9) is equivalent to
(cid:40)

H1(z) :=

max
X,W1,··· ,Wd∈Sd
+

(cid:88)

c(cid:62)
i Wici +

c(cid:62)
i Xci : tr(X) = 1,

(cid:88)

i∈[n]\(T0∪T1)

i∈T1

X (cid:23) Wi, tr(Wi) = zi, ∀i ∈ [n] \ (T0 ∪ T1)

(cid:41)
.

(45)

Next, applying the same procedure as Case a., we have

H1(z) =

min
µ,{Qi}i∈[n]\(T0∪T1)⊆Sd

+

(cid:26)

λmax

(cid:18) (cid:88)

i∈[n]\(T0∪T1)

(cid:19)

+

cic(cid:62)
i

Qi +

(cid:88)

i∈T1

(cid:88)

µizi :

i∈[n]\(T0∪T1)

cic(cid:62)

i (cid:22) Qi + µiId, 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2, ∀i ∈ [n] \ (T0 ∪ T1)

To show the equivalence between (46) and (10), it remains to prove that
(cid:26)

(cid:19)

(cid:98)H1(z) =

min
µ,{Qi}i∈[n]⊆Sd
+

λmax

(cid:18) (cid:88)

(cid:88)

Qi

+

µizi :

i∈[n]

i∈[n]

cic(cid:62)

i (cid:22) Qi + µiId, 0 ≤ µi ≤ (cid:107)ci(cid:107)2

2, ∀i ∈ [n]

(cid:27)
.

(cid:27)
.

(46)

(47)

it by setting Qi = 0, µi = (cid:107)ci(cid:107)2

First, given any feasible solution (µ, {Qi}i∈[n]\(T0∪T1)) to the problem (46), let us augment
2 for each i ∈ T0 and Qi = cic(cid:62)
i , µi = 0 for each i ∈ T1. Then
(µ, {Qi}i∈[n]) is feasible to the problem (47) with the same objective value. Thus, we have

(cid:98)H1(z) ≤ H1(z).

On the other hand, given any feasible solution (µ, {Qi}i∈[n]) to the problem (47), then

(µ, {Qi}i∈[n]\(T0∪T1)) is feasible to the problem (46) a smaller objective value since cic(cid:62)
Qi + µi for each i ∈ T1. Thus, we have (cid:98)H1(z) ≥ H1(z). This completes the proof.

i (cid:22)

40

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Part (ii). For any z ∈ Z, let set S denote its support. We then construct a pair of the primal and

dual solutions to the maximization problem in (9) and its dual (10) as

X ∗ = q1q(cid:62)

1 , W ∗

i = X ∗, ∀i ∈ S, W ∗

i = 0, ∀i ∈ [n] \ S,

Q∗

i = cic(cid:62)

i , µi = 0, ∀i ∈ S, Q∗

i = 0, µi = ||ci||2

2, ∀i ∈ [n] \ S,

where q1 denote the eigenvector for the largest eigenvalue of matrix (cid:80)

i∈S cic(cid:62)
i .

According to the results in Lemma 1, the above solutions return the same objective value for

primal and dual problems, which is λmax((cid:80)
dual solution.

i∈S cic(cid:62)

i ). This proves the optimality of the proposed
(cid:3)

A.3 Proof of Proposition 2

Proposition 2 The SPCA (2) admits the following MISDP formulation:

(SPCA) w∗ := max

z∈Z,X∈Sn
+

(cid:26)

tr(AX) : tr(X) = 1, Xii ≤ zi, ∀i ∈ [n]

(cid:27)
.

(14)

and its continuous relaxation value is equal to λmax(A).

Proof.

(i) To show the equivalence of problem (14) and SPCA (2), we only need to show that for any

feasible z ∈ Z with its support S = {i : zi = 1}, we must have

(cid:26)

max
X∈Sn
+

tr(AX) : tr(X) = 1, Xii ≤ zi, ∀i ∈ [n]

= λmax(ASS).

(48)

(cid:27)

Indeed, since X is a positive semi-deﬁnite matrix, thus Xii = 0 for each i ∈ [n] \ S implies

Xij = 0, ∀(i, j) /∈ S × S.

The left-hand side of (48) is equivalent to

(cid:26)

max
X∈Sn
+

tr(AX) : tr(X) = 1, Xii ≤ zi, ∀i ∈ [n]

(cid:27)

= max
X∈Sk
+

{tr(AS,SX) : tr(X) = 1} = λmax(ASS),

where the second equality is due to Part (ii) in Lemma 1.

(ii) The continuous relaxation value of problem (14) is

(cid:26)

w3 = max

z∈Z,X∈Sn
+

tr(AX) : tr(X) = 1, Xii ≤ zi, ∀i ∈ [n]

(cid:27)
.

Since tr(X) = 1, thus the linking constraint Xii ≤ zi is redundant for each i ∈ [n]. Hence,

(cid:26)

w3 = max
X∈Sn
+

tr(AX) : tr(X) = 1

= λmax(A),

(cid:27)

where the equality is due to Part (ii) in Lemma 1.

(cid:3)

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

41

A.4 Proof of Lemma 2

Lemma 2 The following two inequalities are valid to SPCA (14)

(i) (cid:80)
(cid:16)(cid:80)

(ii)

j∈[n] X 2
j∈[n] |Xij|

(cid:17)2

ij ≤ Xiizi for all i ∈ [n]; and

≤ kXiizi for all i ∈ [n].

Proof. From the proof of Proposition 2, there must exists an optimal solution (z∗, X ∗) of SPCA

(14) such that X ∗ must be rank-one. Thus, without loss of generality, for any feasible solution

(z, X) of SPCA (14), we can assume that X = xx(cid:62), where (x, z) is also feasible to SPCA (2).

Next, we split the proof into two parts.

(i) Since X = xx(cid:62), thus

(cid:88)

X 2

ij =

(cid:88)

j∈[n]

j∈[n]

i x2
x2

j = x2

i ≤ ziXii, ∀i ∈ [n],

where the last inequality follows from the facts that Xii = x2

i ≤ zi and zi is binary for each

i ∈ [n].

(ii) It is known (see, e.g., [15]) that ||x||1 ≤

√

k. Thus,

(cid:88)

j∈[n]

|Xij| =

(cid:88)

j∈[n]

|xi||xj| ≤

√

k|xi| ≤

√

k

(cid:112)

Xiizi,

where the second inequality is due to the facts that Xii = x2

i ≤ zi and zi is binary for each
(cid:3)

i ∈ [n].

A.5 Proof of Theorem 6

Theorem 6 Given a threshold (cid:15) > 0, the following MILP is O((cid:15))-approximate to SPCA (2), i.e.,
(cid:15) ≤ (cid:98)w((cid:15)) − w∗ ≤ (cid:15)

√

d

(cid:98)w((cid:15)) :=

max
w,z∈Z,y,α,x,,δ,µ,σ

w

s.t. x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [n],

x =

(cid:88)

j∈[d]

σj, ||σj||∞ ≤ yj, σjj = yj, ∀j ∈ [d],

(cid:88)

j∈[d]

yj = 1,

x = µ(cid:96)1 + µ(cid:96)2, ||µ(cid:96)1||∞ ≤ α(cid:96), ||µ(cid:96)2||∞ ≤ 1 − α(cid:96), ∀(cid:96) ∈ [m],

w = wU − (wU − wL)

(cid:18) (cid:88)

(cid:19)

,

2−iαi

i∈[m]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]

cic(cid:62)

i δi1 − wU x + (wU − wL)

(cid:88)

(cid:96)∈[m]

2−(cid:96)µ(cid:96)1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

≤ (cid:15),

α ∈ {0, 1}m, y ∈ {0, 1}d,

(22)

where wL, wU separately denote the lower and upper bounds of SPCA, m := (cid:100)log2((wU − wL)(cid:15)−1)(cid:101)
and the inﬁnite norm inequality constraints can be easily linearized.

42

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

Proof. Throughout the proof, we use indices i ∈ [n], j ∈ [d], and (cid:96) ∈ [m] to denote the elements of

three diﬀerent dimensional vectors, respectively. To construct the MILP by SPCA (21) and show

the approximation accuracy, we split the proof into four steps.

Step 1. Linearize the bilinear terms {zix}i∈[n] in (21). This can be done by introducing two copies

δi1, δi2 of vector x for each i ∈ [n] such that

x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [n],

(cid:88)

i∈[n]

zicic(cid:62)

i x =

(cid:88)

i∈[n]

cic(cid:62)

i δi1.

Step 2. Linearize the nonconvex constraint (cid:107)x(cid:107)∞ = 1. We ﬁrst observe that due to symmetry,

(cid:107)x(cid:107)∞ = 1 can be equivalently written as a disjunction with d sets as below

∪j∈[d]

(cid:8)x ∈ Rd : xj = 1, (cid:107)x(cid:107)∞ ≤ 1(cid:9).

Next, for each j ∈ d, we introduce a binary variable yj = 1 indicating the j-th set is active
and 0, otherwise, and then create a copy σj ∈ Rd of variable x such that

x =

(cid:88)

j∈[d]

σj, ||σj||∞ ≤ yj, σjj = yj, ∀j ∈ [d],

yj = 1, y ∈ {0, 1}d.

(cid:88)

j∈[d]

Step 3. Approximate and linearize bilinear term wx. We ﬁrst approximate variable w using m

binary variables α ∈ Rm with m := (cid:100)log2((wU − wL)/(cid:15))(cid:101). Thus, we have

w ≈ wU − (wU − wL)

(cid:18) (cid:88)

(cid:19)

2−(cid:96)α(cid:96)

(cid:96)∈[m]

with approximation accuracy at most (wU − wL)/2m ≤ (cid:15). The bilinear term wx is now

approximated by

wx ≈ wU x − (wU − wL)

(cid:18) (cid:88)

2−(cid:96)α(cid:96)x

(cid:19)

.

(cid:96)∈[m]

(49)

With binary variables α, the resulting bilinear terms {α(cid:96)x}(cid:96)∈[m] can be further linearized

following the same arguments as Step 2, i.e.,

x = µ(cid:96)1 + µ(cid:96)2, ||µ(cid:96)1||∞ ≤ α(cid:96), ||µ(cid:96)2||∞ ≤ 1 − α(cid:96), ∀(cid:96) ∈ [m],

wU x − (wU − wL)

(cid:18) (cid:88)

(cid:19)

2−(cid:96)α(cid:96)x

(cid:96)∈[m]

= wU x − (wU − wL)

(cid:88)

(cid:96)∈[m]

2−(cid:96)µ(cid:96)1.

Step 4. Finally, following the approximation and linearization results in Step 3, the equality con-

straint (cid:80)

i∈[n] cic(cid:62)
by the following inequality

i σi1 = wx in (21) might not hold exactly. Thus we replace the equality

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]

cic(cid:62)

i δi1 − wU x + (wU − wL)

(cid:88)

i∈[m]

2−iµi1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

43

(cid:88)

i∈[n]

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

cic(cid:62)

i zix − wU x + (wU − wL)

(cid:88)

2−iαix

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

i∈[m]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

2−iαix

(cid:88)

i∈[m]

wx − wU x + (wU − wL)

≤ (wU − wL)/2m ≤ (cid:15),

which holds for any feasible solution of formulation (21).

First, we have (cid:98)w((cid:15)) ≥ w∗ − (cid:15) since w := w∗ − (cid:15) is feasible to the MILP (22).
Moreover, given an optimal solution ((cid:98)x, (cid:98)z, (cid:98)w((cid:15))) to the MILP (22), we must have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]

i (cid:98)x − (cid:98)w((cid:15))(cid:98)x

≤ (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

(cid:98)zicic(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(⇒)

min
x:(cid:107)x(cid:107)∞=1

i x − (cid:98)w((cid:15))x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

≤ (cid:15)

(cid:88)

i∈[n]

(cid:88)

(cid:98)zicic(cid:62)
i∈[n]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i∈[n]

i∈[n]

(⇒) d−1/2 min

x:(cid:107)x(cid:107)∞=1

(cid:98)zicic(cid:62)

i x − (cid:98)w((cid:15))x

≤ (cid:15)

(⇒) d−1/2 min

x:(cid:107)x(cid:107)2≥1

(cid:98)zicic(cid:62)

i x − (cid:98)w((cid:15))x

≤ (cid:15)

(⇔) d−1/2 min

x:(cid:107)x(cid:107)2=1

(cid:98)zicic(cid:62)

i x − (cid:98)w((cid:15))x

≤ (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

where the ﬁrst implication is due to (cid:107)(cid:98)x(cid:107)∞ = 1, the second one is due to (cid:107)x(cid:107)∞ ≥ d−1/2(cid:107)x(cid:107)2
since x ∈ Rd, the third one is because (cid:107)x(cid:107)∞ = 1 implies (cid:107)x(cid:107)2 ≥ 1, and the equivalence is
because of monotonicity and positive homogeneity of the objective function. According to
the last inequality, there exists an eigenvalue w of matrix (cid:80)
i such that | (cid:98)w((cid:15)) −
(cid:3)
d, which further implies that (cid:98)w((cid:15)) − w∗ ≤ (cid:15)

d since w ≤ w∗.

i∈[n] (cid:98)zicic(cid:62)

w| ≤ (cid:15)

√

√

A.6 Proof of Theorem 7

Theorem 7 Given a threshold (cid:15) > 0, by enforcing the binary variables z to be continuous, let w5((cid:15))

denote the optimal value of the relaxed MILP formulation (22). Then we have

w5((cid:15)) ≤ min (cid:8)k(

√

d/2 + 1/2), n/k

√

√

d + (n − k)(

d/2 + 1/2)(cid:9)w∗ + (cid:15)
√

√

d.

Proof. From the proof of Theorem 6, we know that w5((cid:15)) ≤ w5(0) + (cid:15)

d. Thus, it is suﬃcient to

show that

√

w5(0) ≤ k(

d/2 + 1/2)w∗.

We observe that when (cid:15) = 0, the resulting formulation by relaxing binary variables z to be

continuous becomes:

w5(0) =

max
w,z∈Z,x,
{δi1}i∈[n],{δi2}i∈[n]

(cid:26)

w :

(cid:88)

i∈[n]

cic(cid:62)

i δi1 = wx, (cid:107)x(cid:107)∞ = 1,

44

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

x = δi1 + δi2, ||δi1||∞ ≤ zi, ||δi2||∞ ≤ 1 − zi, ∀i ∈ [n]

(cid:27)
,

(50)

Next, we split the proof into three steps.

Step 1. For any feasible solution to problem (50), we have

w =

|| (cid:80)

i δi1||∞

i∈[n] cic(cid:62)
||x||∞

= ||

(cid:88)

i∈[n]

cic(cid:62)

i δi1||∞ ≤

(cid:88)

i∈[n]

||cic(cid:62)

i δi1||∞ =

(cid:88)

i∈[n]

||ci||∞|c(cid:62)

i δi1|

(cid:88)

≤

i∈[n]

||ci||∞||ci||1||δi1||∞ ≤

(cid:88)

i∈[n]

||ci||∞||ci||1zi ≤ k max
i∈[n]

||ci||∞||ci||1,

where the ﬁrst inequality is due to triangle inequality, the second one is because of Holder’s

inequality, the third one is because ||δi1||∞ ≤ zi, and the last one is due to ||ci||∞||ci||1 ≤
maxj∈[n] ||cj||∞||cj||1 for each i ∈ [n] and (cid:80)
Step 2. Now it remains to show that for each i ∈ [n]

i∈[n] zi = k.

||ci||∞||ci||1 ≤

√

d + 1
2

w∗.

Let ς be a permutation of index set [d] such that ci,ς(1), · · · , ci,ς(d) are sorted in an

ascending order. Then we have
(cid:18) (cid:88)

c2
i,ς(1) +

1
d − 1

j∈[2,d]

(cid:19)2

|ci,ς(j)|

≤ c2

i,ς(1) + · · · + c2

i,ς(d) = ||ci||2

2 ≤ w∗,

where the ﬁrst inequality is from the arithmetic and quadratic mean inequality and the

second inequality follows from ||ci||2

2 = λmax(cic(cid:62)

i ) ≤ w∗.

For ease of exposition, let us introduce v1 = |ci,ς(1)| and v2 = (cid:80)

j∈[2,d] |ci,ς(j)|. Next, let

us consider an optimization problem

(cid:26)

v1(v1 + v2) : v2

1 + 1/(d − 1)v2

2 ≤ w∗

(cid:27)

,

(51)

ν = max
v∈R2
+

whose optimal value clearly provides an upper bound of ||ci||∞||ci||1.

To solve (51), we ﬁrst rewrite v1, v2 as

v1 = r sin(θ)r, v2 = r

√

d − 1 cos(θ), θ ∈ [0, π/2], r ≤

√

w∗.

In this way, the objective function (51) is equal to

v1(v1 + v2) = v2

1 + v1v2 = r2 sin2(θ) + r2

√

d − 1 sin(θ) cos(θ) = r2 1 − cos(2θ)
√

√

+ r2

d − 1

sin(2θ)
2

=

r2
2

−

r2
2

cos(2θ) +

1
2

r2

√

d − 1 sin(2θ) ≤

r2 +

r2 ≤

w∗,

1
2

d
2

2
√
d + 1
2

where the ﬁrst inequality is due to Cauchy-Schwartz inequality and the second one is

because r2 ≤ w∗. Thus, we must have

||ci||∞||ci||1 ≤

√

d + 1
2

w∗.

This proves the ﬁrst bound k(

√

d/2 + 1/2) together with Step 1.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

45

Step 3. We now prove the second bound. Plugging the equations δi1 = x − δi2 for all i ∈ [n], we

rewrite the continuous relaxation value as

|| (cid:80)

i∈[n] cic(cid:62)

i (x − δi2)||∞

|| (cid:80)

w =

≤

||x||∞

|| (cid:80)

i x||∞

i∈[n] cic(cid:62)
||x||∞

≤

√

+ (n − k)

i x||∞

i∈[n] cic(cid:62)
||x||∞

+

|| (cid:80)

i δi2||∞

i∈[n] cic(cid:62)
||x||∞

d + 1
2

w∗ ≤ max
i∈[d]

(cid:88)

j∈[d]

|C ij| + (n − k)

√

d + 1
2

w∗,

where C := CC (cid:62) = (cid:80)

i∈[n] cic(cid:62)

i and the ﬁrst inequality is from the triangle inequality, the

second one follows from the derivations in Steps 1 and 2, and the third one is due to xi ≤ 1

for each i ∈ [d].

Next, the ﬁrst term of the right-hand side above can be upper bounded by

max
i∈[d]

(cid:88)

j∈[d]

|C ij| = ||C||1 ≤

√

d||C||2 =

√

dλmax(C) ≤

√

n
k

dw∗,

where the equations are from the deﬁnition of (cid:96)1-norm and (cid:96)2-norm of a matrix and the
(cid:3)

second inequality is due to λmax(C) = λmax(A) ≤ n/kw∗.

A.7 Proof of Lemma 3

Lemma 3 Given a matrix A ∈ Rm×n, consider its augmented counterpart A deﬁned in

(28),

two integers k1 ∈ [m] and k2 ∈ [n], and three subsets S, S1, S2 ⊆ [m + n] such that

S ⊆ [m + n], |S| = k1 + k2, S1 = S ∩ [m], |S1| = k1 and S2 = S ∩ [m + 1, m + n], |S2| = k2. Then the

following identities must hold:

(i) The eigenvalues of the augmented submatrix AS,S are the singular values of submatrix AS1,S2

and their negations;

(ii) σmax(AS1,S2) = λmax(AS,S) = 1/2 maxx∈Rk1+k2 {x(cid:62)Ax : ||x1:k1||2 = 1, ||xk1+1:k1+k2||2 = 1} =

1/2 max

X∈S

k1+k2
+

(cid:110)
tr(AS,SX) : (cid:80)

j∈[k1] Xjj = 1, (cid:80)

(cid:111)
.
i∈[k1+1,k1+k2] Xii = 1

Proof. The proof includes two parts.

(i) By the deﬁnition of augmented matrix A in (28), for its submatrix AS,S, we observe that

AS,S =

(cid:20) 0 AS1,S2
A(cid:62)

0

S1,S2

(cid:21)

.

Then the statement in Part (i) directly follows from the result in Ben-Tal and Nemirovski

[3], which shows that the eigenvalues of an augmented symmetric matrix exactly are equal to

the singular values and negative ones of the original matrix.

46

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

(ii) The ﬁrst equality λmax(AS,S) = σmax(AS1,S2) is obtained from Part (i).

For the largest singular value of AS1,S2, we have

σmax(AS1,S2) = max

(cid:8)u(cid:62)AS1,S2v : ||u||2 = 1, ||u||2 = 1(cid:9)

u∈Rk1 ,v∈Rk2
1
2

max
x∈Rk1+k2

=

(cid:8)x(cid:62)AS,Sx : ||x1:k1||2 = 1, ||xk1+1:k1+k2||2 = 1(cid:9) ,

(52)

which proves the second equality of Part (ii).

As for the last equality of Part (ii), we let (cid:98)w∗
SVD denote the optimal value of the right-
hand side SDP problem. Then we must have (cid:98)w∗
SVD ≥ σmax(AS1,S2) as the SDP problem is
exactly a SDP relaxation of the maximization problem over x in (52) by relaxing the rank-one

constraint. On the other hand, summing up two constraints in the SDP problem, we obtain
an upper bound of (cid:98)w∗

SVD, i.e.,

(cid:98)w∗
SVD ≤

1
2

max

X∈S

k1+k2
+

(cid:8)tr(AS,SX) : tr(X) = 2(cid:9) = λmax(AS,S) = σmax(AS1,S2),

where the ﬁrst equality is due to Part (ii) in Lemma 1.

(cid:3)

A.8 Proof of Theorem 11

Theorem 11 The continuous relaxation value wSVD1 of formulation (34) satisﬁes

w∗

SVD ≤ wSVD1 ≤

(cid:113)

mnk−1

1 k−1

2 w∗

SVD.

Proof. For the matrix A

#

deﬁned in (32), using Part (i) in Lemma 3, we can derive that its

largest eigenvalue is equal to 2σmax(A). Let ((cid:98)z, (cid:99)X, (cid:99)W1, · · · , (cid:99)Wm+n) denote an optimal solution to
the continuous SDP relaxation of problem (34). We now have

2σmax(A) = λmax

#(cid:17)

(cid:16)

A

= max

X(cid:23)0,tr(X)=1

(cid:26) (cid:88)

(cid:27)

≥

c(cid:62)
i Xci

(cid:88)

c(cid:62)
i (cid:99)Xci ≥

(cid:88)

c(cid:62)
i (cid:99)Wici,

i∈[m+n]

i∈[m+n]

i∈[m+n]

where the last inequality is because (cid:99)X (cid:23) (cid:99)Wi for each i ∈ [m + n]. Note that the right-hand side

above is equal to wSVD1 + σmax(A) and the inequalities above lead to

wSVD1 =

(cid:88)

i∈[m+n]

c(cid:62)
i (cid:99)Wici − σmax(A) ≤ 2σmax(A) − σmax(A) = σmax(A).

Now it remains to show that

Claim 1 σmax(A) ≤

(cid:112)mnk−1

1 k−1

2 w∗

SVD.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

47

Proof. Let u1, v1 denote the top right and left eigenvectors of A, i.e., u(cid:62)

1 Av1 = σmax(A), Av1 =
1 . We tailor u1, v1 to meet the feasibility of R1-SSVD (27) as below

σmax(A)v1, u(cid:62)

1 A = σmax(A)u(cid:62)
(cid:40)

(cid:98)uj1 =

uj1,
0,

if uj1 is one of the k1 largest entries of u1
otherwise

, ∀j ∈ [n],

(cid:40)

(A(cid:62)
0,

(cid:98)u)j,

(cid:98)vj1 =

if |(A(cid:62)
otherwise

(cid:98)u)j| is one of the k2 largest entries of |A(cid:62)

(cid:98)u|

, ∀j ∈ [m].

Let us normalize (cid:98)u1 = (cid:98)u1
|| (cid:98)u1||2

and (cid:98)v1 = (cid:98)v1
||(cid:98)v1||2

. Clearly, ((cid:98)u1, (cid:98)v1) is feasible R1-SSVD (27). Then we

have

(cid:114)

k1
n

σmax(A) ≤ σmax(A)(cid:98)u(cid:62)

1 u1 = (cid:98)u(cid:62)

1 Av1 ≤ (cid:107)(cid:98)u(cid:62)

1 A(cid:107)2 ≤

(cid:114) m
k2

(cid:98)u(cid:62)
1 A(cid:98)v1 ≤

(cid:114) m
k2

w∗

SVD,

where the ﬁrst inequality is due to the deﬁnition of (cid:98)u1, the equality is because of the deﬁnition of
v1, the second inequality is due to the Cauchy-Schwartz inequality, the third one is based on the
choice of (cid:98)v1, and the last one is due to the feasibility of ((cid:98)u1, (cid:98)v1). This completes the proof.

(cid:5)

(cid:3)

A.9 Proof of Lemma 4

Lemma 4 For R1-SSVD (35), the following second-order conic inequalities are valid:

(i) (cid:80)
(ii) ((cid:80)

ij ≤ ziXii, (cid:80)
j∈[m] X 2
j∈[m] |Xij|)2 ≤ k1Xiizi, ((cid:80)

j∈[m+1,m+n] X 2

ij ≤ ziXii for all i ∈ [m + n]; and
j∈[m+1,m+n] |Xij|)2 ≤ k2Xiizi for all i ∈ [m + n].

Proof. According to Proposition 7, there must exist an optimal solution (z∗, X ∗) to MISDP (35)

such that X ∗ is rank-one. Thus, without loss of generality, for any feasible solution (z, X) of SPCA

(14), we can assume that X =

, where vectors (u, v) thus satisfy

(cid:21)(cid:62)

(cid:20)u
v

(cid:21) (cid:20)u
v

||u||2 = ||v||2 = 1, ||u||1 ≤

(cid:112)

k1, ||v||1 ≤

(cid:112)

k2.

Then the rest of the proof is almost identical to that of Lemma 2 and is thus omitted for brevity.

A.10 Proof of Theorem 15

Theorem 15 For R1-SSVD (27), the truncation algorithm yields an approximation ratio

(cid:26)(cid:113)

(cid:113)

(cid:112)

k−1
2 ,

k−1
1 ,

max

(cid:27)

k1k2m−1n−1

.

In particular, the approximation ratio is O(n−1/3) when k1 ≈ k2 and m ≈ n.

Proof. We derive the three approximation ratios of the truncation algorithm below.

48

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

(i) According to the truncation in the standard basis, the obtained vector (cid:98)ui is feasible to the

R1-SSVD problem for each i ∈ [n] and is also optimal to the following problem

(cid:98)ui ∈ arg

max
||ui||2=1,||ui||0=k1

(cid:8)u(cid:62)

i Aei

(cid:9) , ∀i ∈ [n].

Suppose the optimal solution of the R1-SSVD (27) to be u∗ and v∗, let S∗

1 , S∗

2 denote their

supports, respectively. We then rewrite v∗ = (cid:80)

i∈S∗
2

v∗
i ei and we have

w∗

SVD = (u∗)(cid:62)Av∗ =

i (u∗)(cid:62)Aei ≤
v∗

(cid:88)

i∈S∗
2

(cid:115)(cid:88)

(cid:115)(cid:88)

(v∗

i )2

i∈S∗
2

i∈S∗
2

[(u∗)(cid:62)Aei]2 ≤

(cid:112)

k2 max

i∈[n] (cid:98)u(cid:62)

i Aei,

where the ﬁrst inequality is due to Cauchy-Schwartz and the second one is because of maxi-
mality of maxi∈[n] (cid:98)u(cid:62)

i Aei.

Since (1 − (k2 − 1)(cid:15))ei + (cid:15) (cid:80)

j∈[k2]∪{i}\{i} ej with suﬃciently small (cid:15) > 0 is feasible to R1-
SSVD (27), thus the right-hand side above is an lower bound of R1-SSVD according to the
continuity by letting (cid:15) → 0. This prove the approximation ratio (cid:112)k−1
2 .

Similarly, we can derive

w∗

SVD = (u∗)(cid:62)Av∗ ≤

(cid:112)

k1 max
j∈[m]

e(cid:62)
j A(cid:98)vj,

which prove the approximation ratio (cid:112)k−1
1 .

(ii) Following the proof of Claim 1, for the truncation in the eigen-space basis, we have

(cid:114)

k1
n

w∗

SVD ≤

(cid:114)

k1
n

σmax(A) ≤ σmax(A)(cid:98)u(cid:62)

1 u1 = (cid:98)u(cid:62)

1 Av1 ≤ (cid:107)(cid:98)u(cid:62)

1 A(cid:107)2 ≤

which proves the approximation ratio of

√

k1k2m−1n−1.

A.11 Proof of Theorem 16

(cid:114) m
k2

(cid:98)u(cid:62)
1 A(cid:98)v1,

(cid:3)

Theorem 16 For the greedy Algorithm 3 and the local search Algorithm 4, we have (i) both algo-

rithms achieve a (

k1k2)−1-approximation ratio of R1-SSVD (37), and (ii) the ratio is tight.

√

Proof. The proof is split into two parts.

(i) In R1-SSVD (37), according to the part (i) of the proof of Theorem 15, we have

w∗

SVD ≤

(cid:112)

k2 max

j∈[n] (cid:98)u(cid:62)

j Aej ≤

(cid:112)

k1k2 max

i∈[m],j∈[n]

Aij,

where vectors {(cid:98)ui}i∈[n] ⊆ Rm are obtained by the normalized k1-truncation in the standard
basis of A. Then, following the similar analyses of Theorem 8 and Theorem 9, the largest

singular value from greedy Algorithm 3 and local search Algorithm 4 must be lower bounded

by maxi∈[m],j∈[n] Aij.

Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA

49

(ii) We next show an example in which the ratio (cid:112)k−1

1 k−1

2

can be achieved. Suppose that, without

loss of generality, k1 ≤ k2. Then, consider m = 2k2, n = 2k2, and matrix A ∈ Rm×n as

A :=

(cid:20) Ik2 0k2×k2
0k2×k2 1k2×k2

(cid:21)

.

Above, the submatrix A[k1],[k2] satisﬁes greedy and local optimality conditions with the objec-
tive value equal to 1, while the best size k1 × k2 submatrix is A[k2+1,k2+k1],[k2+1,2k2] with the
(cid:3)
optimal value

√

k1k2.

