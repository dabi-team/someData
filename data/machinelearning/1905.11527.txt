9
1
0
2

t
c
O
1
3

]

G
L
.
s
c
[

2
v
7
2
5
1
1
.
5
0
9
1
:
v
i
X
r
a

Tight Regret Bounds for Model-Based Reinforcement
Learning with Greedy Policies

Yonathan Efroni∗
Technion, Israel

Nadav Merlis ∗
Technion, Israel

Mohammad Ghavamzadeh
Facebook AI Research

Shie Mannor
Technion, Israel

Abstract

State-of-the-art efﬁcient model-based Reinforcement Learning (RL) algorithms typ-
ically act by iteratively solving empirical models, i.e., by performing full-planning
on Markov Decision Processes (MDPs) built by the gathered experience. In this
paper, we focus on model-based RL in the ﬁnite-state ﬁnite-horizon undiscounted
MDP setting and establish that exploring with greedy policies – act by 1-step plan-
ning – can achieve tight minimax performance in terms of regret, ˜O(
HSAT ).
Thus, full-planning in model-based RL can be avoided altogether without any per-
formance degradation, and, by doing so, the computational complexity decreases
by a factor of S. The results are based on a novel analysis of real-time dynamic
programming, then extended to model-based RL. Speciﬁcally, we generalize ex-
isting algorithms that perform full-planning to act by 1-step planning. For these
generalizations, we prove regret bounds with the same rate as their full-planning
counterparts.

√

1

Introduction

Reinforcement learning (RL) [Sutton and Barto, 2018] is a ﬁeld of machine learning that tackles the
problem of learning how to act in an unknown dynamic environment. An agent interacts with the
environment, and receives feedback on its actions in the form of a state-dependent reward signal.
Using this experience, the agent’s goal is then to ﬁnd a policy that maximizes the long-term reward.

There are two main approaches for learning such a policy: model-based and model-free. The model-
based approach estimates the system’s model and uses it to assess the long-term effects of actions via
full-planning (e.g., Jaksch et al. 2010). Model-based RL algorithms usually enjoy good performance
guarantees in terms of the regret – the difference between the sum of rewards gained by playing
an optimal policy and the sum of rewards that the agent accumulates [Jaksch et al., 2010, Bartlett
and Tewari, 2009]. Nevertheless, model-based algorithms suffer from high space and computation
complexity. The former is caused by the need for storing a model. The latter is due to the frequent
full-planning, which requires a full solution of the estimated model. Alternatively, model-free RL
algorithms directly estimate quantities that take into account the long-term effect of an action, thus,
avoiding model estimation and planning operations altogether [Jin et al., 2018]. These algorithms
usually enjoy better computational and space complexity, but seem to have worse performance
guarantees.

In many applications, the high computational complexity of model-based RL makes them infeasible.
Thus, practical model-based approaches alleviate this computational burden by using short-term
planning e.g., Dyna [Sutton, 1991], instead of full-planning. To the best of our knowledge, there are
no regret guarantees for such algorithms, even in the tabular setting. This raises the following question:
Can a model-based approach coupled with short-term planning enjoy the favorable performance of
model-based RL?

∗equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
Algorithm

UCRL22
[Jaksch et al., 2010]
UCBVI [Azar et al., 2017]
EULER [Zanette and Brunskill, 2019]
UCRL2-GP
EULER-GP
Q-v2 [Jin et al., 2018]

Lower bounds

˜O(

H 2T )

√

√

Regret
H 2S2AT )

˜O(
√
HSAT +
√
˜O(
HSAT )
√
˜O(
H 2S2AT )
√
˜O(
HSAT )
√
˜O(
H 3SAT
(cid:17)

(cid:16)√

Ω

HSAT

Time Complexity
˜O(N SAH)
˜O(N SAH)
˜O(N SAH)
˜O(N AH)
˜O(N AH)
˜O(AH)

Space Complexity
˜O(HS + N SA)
˜O(HS + N SA)
˜O(HS + N SA)
˜O(HS + N SA)
˜O(HS + N SA)
˜O(HSA)

–

–

Table 1: Comparison of our bounds with several state-of-the-art bounds for RL in tabular ﬁnite-
horizon MDPs. The time complexity of the algorithms is per episode; S and A are the sizes of the
state and action sets, respectively; H is the horizon of the MDP; T is the total number of samples that
the algorithm gathers; N ≤ S is the maximum number of non-zero transition probabilities across the
entire state-action pairs. The algorithms proposed in this paper are highlighted in gray.

In this work, we show that model-based algorithms that use 1-step planning can achieve the same
performance as algorithms that perform full-planning, thus, answering afﬁrmatively to the above
question. To this end, we study Real-Time Dynamic-Programming (RTDP) [Barto et al., 1995]
that ﬁnds the optimal policy of a known model by acting greedily based on 1-step planning, and
establish new and sharper ﬁnite sample guarantees. We demonstrate how the new analysis of RTDP
can be incorporated into two model-based RL algorithms, and prove that the regret of the resulting
algorithms remains unchanged, while their computational complexity drastically decreases. As Table
1 shows, this reduces the computational complexity of model-based RL methods by a factor of S.

The contributions of our paper are as follows: we ﬁrst prove regret bounds for RTDP when the
model is known. To do so, we establish concentration results on Decreasing Bounded Processes,
which are of independent interest. We then show that the regret bound translates into a Uniform
Probably Approximately Correct (PAC) [Dann et al., 2017] bound for RTDP that greatly improves
existing PAC results [Strehl et al., 2006]. Next, we move to the learning problem, where the model
is unknown. Based on the analysis developed for RTDP we adapt UCRL2 [Jaksch et al., 2010] and
EULER [Zanette and Brunskill, 2019], both act by full-planning, to UCRL2 with Greedy Policies
(UCRL2-GP) and EULER with Greedy Policies (EULER-GP); model-based algorithms that act by
1-step planning. The adapted versions are shown to preserve the performance guarantees, while
improve in terms of computational complexity.

2 Notations and Deﬁnitions

We consider ﬁnite-horizon MDPs with time-independent dynamics [Bertsekas and Tsitsiklis, 1996].
A ﬁnite-horizon MDP is deﬁned by the tuple M = (S, A, R, p, H), where S and A are the state and
action spaces with cardinalities S and A, respectively. The immediate reward for taking an action a
at state s is a random variable R(s, a) ∈ [0, 1] with expectation ER(s, a) = r(s, a). The transition
probability is p(s(cid:48) | s, a), the probability of transitioning to state s(cid:48) upon taking action a at state s.
Furthermore, N := maxs,a |{s(cid:48) : p(s(cid:48) | s, a) > 0}| is the maximum number of non-zero transition
probabilities across the entire state-action pairs. If this number is unknown to the designer of the
algorithm in advanced, then we set N = S. The initial state in each episode is arbitrarily chosen and
H ∈ N is the horizon, i.e., the number of time-steps in each episode. We deﬁne [N ] := {1, . . . , N },
for all N ∈ N, and throughout the paper use t ∈ [H] and k ∈ [K] to denote time-step inside an
episode and the index of an episode, respectively.

A deterministic policy π : S × [H] → A is a mapping from states and time-step indices to actions.
We denote by at := π(st, t), the action taken at time t at state st according to a policy π. The quality

2Similarly to previous work in the ﬁnite horizon setting, we state the regret in terms of the horizon H. The

√

regret in the inﬁnite horizon setting is DS

AT , where D is the diameter of the MDP.

2

of a policy π from state s at time t is measured by its value function, which is deﬁned as

(cid:34) H
(cid:88)

t (s) := E
V π

(cid:35)
r(st(cid:48), π(st(cid:48), t(cid:48))) | st = s

,

t(cid:48)=t

where the expectation is over the environment’s randomness. An optimal policy maximizes this
value for all states s and time-steps t, and the corresponding optimal value is denoted by V ∗
t (s) :=
maxπ V π
t (s), for all t ∈ [H]. The optimal value satisﬁes the optimal Bellman equation, i.e.,

t (s) = T ∗V ∗
V ∗

t+1(s) := max

a

(cid:8)r(s, a) + p(· | s, a)T V ∗

t+1

(cid:9).

(1)

We consider an agent that repeatedly interacts with an MDP in a sequence of episodes [K]. The perfor-
1)(cid:1).
mance of the agent is measured by its regret, deﬁned as Regret(K) := (cid:80)K
Throughout this work, the policy πk is computed by a 1-step planning operation with respect to the
value function estimated by the algorithm at the end of episode k − 1, denoted by ¯V k−1. We also
call such policy a greedy policy. Moreover, sk
t and ak
t stand, respectively, for the state and the action
taken at the tth time-step of the kth episode.

1) − V πk

1 (sk

1 (sk

(cid:0)V ∗

k=1

Next, we deﬁne the ﬁltration Fk that includes all events (states, actions, and rewards) until the end
of the kth episode, as well as the initial state of the episode k + 1. We denote by T = KH, the
total number of time-steps (samples). Moreover, we denote by nk(s, a), the number of times that
the agent has visited state-action pair (s, a), and by ˆXk, the empirical average of a random variable
X. Both quantities are based on experience gathered until the end of the kth episode and are Fk
measurable. We also deﬁne the probability to visit the state-action pair (s, a) at the kth episode at
(cid:1). We note that πk is Fk−1 measurable, and
time-step t by wtk(s, a) = Pr(cid:0)sk
thus, wtk(s, a) = Pr(cid:0)sk
t = s, ak
We use ˜O(X) to refer to a quantity that depends on X up to poly-log expression of a quantity at most
δ . Similarly, (cid:46) represents ≤ up to numerical constants or poly-log
polynomial in S, A, T , K, H, and 1
factors. We deﬁne (cid:107)X(cid:107)2,p := (cid:112)EpX 2, where p is a probability distribution over the domain of X,
and use X ∨ Y := max{X, Y }. Lastly, P(S) is the set of probability distributions over the state
space S.

(cid:1). Also denote wk(s, a) = (cid:80)H

t = s, ak
t = a | Fk−1

t=1 wtk(s, a).

t = a | sk

0, πk

3 Real-Time Dynamic Programming

Algorithm 1 Real-Time Dynamic Programming

Initialize: ∀s ∈ S, ∀t ∈ [H], ¯V 0
for k = 1, 2, . . . do
Initialize sk
1
for t = 1, . . . , H do

t (s) = H − (t − 1).

t ∈ arg maxa r(sk
ak
¯V k
t , ak
t ) = r(sk
t (sk
Act with ak

t , a) + p(· | sk
t ) + p(· | sk
t , ak
t+1.

t and observe sk

t , a)T ¯V k−1
t )T ¯V k−1

t+1

t+1

end for

end for

RTDP [Barto et al., 1995] is a well-known algorithm that solves an MDP when a model of the
environment is given. Unlike, e.g., Value Iteration (VI) [Bertsekas and Tsitsiklis, 1996] that solves an
MDP by ofﬂine calculations, RTDP solves an MDP in a real-time manner. As mentioned in Barto
et al. [1995], RTDP can be interpreted as an asynchronous VI adjusted to a real-time algorithm.

Algorithm 1 contains the pseudocode of RTDP for ﬁnite-horizon MDPs. The value function is
initialized with an optimistic value, i.e., an upper bound of the optimal value. At each time-step t and
episode k, the agent acts from the current state sk
t greedily with respect to the current value at the
next time step, ¯V k−1
t+1 . It then updates the value of sk
t according to the optimal Bellman operator. We
denote by ¯V , the value function, and as we show in the following, it always upper bounds V ∗. Note
that since the action at a ﬁxed state is chosen according to ¯V k−1, then πk is Fk−1 measurable.

3

Since RTDP is an online algorithm, i.e., it updates its value estimates through interactions with the
environment, it is natural to measure its performance in terms of the regret. The rest of this section is
devoted to supplying expected and high-probability bounds on the regret of RTDP, which will also
lead to PAC bounds for this algorithm. In Section 4, based on the observations from this section, we
will establish minimax regret bounds for 1-step greedy model-based RL.

We start by stating two basic properties of RTDP in the following lemma: the value is always
optimistic and decreases in k (see proof in Appendix B). Although the ﬁrst property is known [Barto
et al., 1995], to the best of our knowledge, the second one has not been proven in previous work.
Lemma 1. For all s, t, and k, it holds that (i) V ∗

t (s) and (ii) ¯V k

t (s) ≤ ¯V k−1

t (s) ≤ ¯V k

(s).

t

(sk

1) and the real value V πk

The following lemma, that we believe is new, relates the difference between the optimistic value
¯V k−1
1) to the expected cumulative update of the value function at the
1
end of the kth episode (see proof in Appendix B).
Lemma 2 (Value Update for Exact Model). The expected cumulative value update of RTDP at the
kth episode satisﬁes

1 (sk

¯V k−1
1

(sk

1) − V πk

1 (sk

1) =

H
(cid:88)

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1].

The result relates the difference of the optimistic value ¯V k−1 and the value of the greedy policy
V πk to the expected update along the trajectory, created by following πk. Thus, for example, if the
optimistic value is overestimated, then the value update throughout this episode is expected to be
large.

3.1 Regret and PAC Analysis

1) ≥ V πk

1) ≥ ¯V ∗(sk

Using Lemma 1, we observe that the sequence of values is decreasing and bounded from below. Thus,
intuitively, the decrements of the values cannot be indeﬁnitely large. Importantly, Lemma 2 states
that when the expected decrements of the values are small, then V πk
1), and
thus, to V ∗, since ¯V k−1(sk

1).
Building on this reasoning, we are led to establish a general result on a decreasing process. This
result will allow us to formally justify the aforementioned reasoning and derive regret bounds for
RTDP. The proof utilizes self-normalized concentration bounds [de la Peña et al., 2007], applied on
martingales, and can be found in Appendix A.
Deﬁnition 1 (Decreasing Bounded Process). We call a random process {Xk, Fk}k≥0, where
{Fk}k≥0 is a ﬁltration and {Xk}k≥0 is adapted to this ﬁltration, a Decreasing Bounded Process, if
it satisﬁes the following properties:

1) is close to ¯V k−1(sk

1 (sk

1 (sk

1. {Xk}k≥0 decreases, i.e., Xk+1 ≤ Xk a.s. .

2. X0 = C ≥ 0, and for all k, Xk ≥ 0 a.s. .

Theorem 3 (Regret Bound of a Decreasing Bounded Process). Let {Xk, Fk}k≥0 be a Decreasing
Bounded Process and RK = (cid:80)K

k=1 Xk−1 − E[Xk | Fk−1] be its K-round regret. Then,

(cid:26)

Pr

∃K > 0 : RK ≥ C

(cid:16)

1 + 2(cid:112)ln(2/δ)

(cid:17)2(cid:27)

≤ δ.

Speciﬁcally, it holds that Pr{∃K > 0 : RK ≥ 9C ln(3/δ)} ≤ δ.

We are now ready to prove the central result of this section, the expected and high-probability regret
bounds on RTDP (see full proof in Appendix B).
Theorem 4 (Regret Bounds for RTDP). The following regret bounds hold for RTDP:

1. E[Regret(K)] ≤ SH 2.

2. For any δ > 0, with probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH 2 ln(3SH/δ).

4

Proof Sketch. We give a sketch of the proof of the second claim. Applying Lemmas 1 and then 2,

Regret(K) :=

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≤

K
(cid:88)

k=1

¯V k−1
1

(sk

1) − V πk (sk
1)

≤

K
(cid:88)

H
(cid:88)

k=1

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1].

(2)

We then establish (see Lemma 34) that RHS of (2) is, in fact, a sum of SH Decreasing Bounded
Processes, i.e.,

(2) =

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s∈S

k=1

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1].

(3)

Since for any ﬁxed s, t, (cid:8) ¯V k
for a ﬁxed s, t, and conclude the proof by applying the union bound on all SH terms in (3).

k≥0 is a decreasing process by Lemma 1, we can use Theorem 3,

t (s)(cid:9)

Theorem 4 exhibits a regret bound that does not depend on T = KH. While it is expected that RTDP,
that has access to the exact model, would achieve better performance than an RL algorithm with no
such access, a regret bound independent of T is a noteworthy result. Indeed, it leads to the following
Uniform PAC (see Dann et al. 2017 for the deﬁnition) and (0, δ) PAC guarantees for RTDP (see
proofs in Appendix B). To the best of our knowledge, both are the ﬁrst PAC guarantees for RTDP.3
Corollary 5 (RTDP is Uniform PAC). Let δ > 0 and N(cid:15) be the number of episodes in which RTDP
outputs a policy with V ∗

1 (sk

1) > (cid:15). Then,

1 (sk

1) − V πk
(cid:26)

Pr

∃(cid:15) > 0 : N(cid:15) ≥

9SH 2 ln(3SH/δ)
(cid:15)

(cid:27)

≤ δ.

Corollary 6 (RTDP is (0, δ) PAC). Let δ > 0 and N be the number of episodes in which
RTDP outputs a non optimal policy. Deﬁne the (unknown) gap of the MDP, ∆(M) =
mins minπ:V π

1 (s)(cid:54)=V ∗

1 (s) V ∗

1 (s) − V π
1 (s) > 0. Then,
(cid:26)

Pr

N ≥

9SH 2 ln(3SH/δ)
∆(M)

(cid:27)

≤ δ.

4 Exploration in Model-based RL: Greedy Policy Achieves Minimax Regret

We start this section by formulating a general optimistic RL scheme that acts by 1-step planning (see
Algorithm 2). Then, we establish Lemma 7, which generalizes Lemma 2 to the case where a non-exact
model is used for the value updates. Using this lemma, we offer a novel regret decomposition for
algorithms which follow Algorithm 2. Based on the decomposition, we analyze generalizations of
UCRL2 [Jaksch et al., 2010] (for ﬁnite horizon MDPs) and EULER [Zanette and Brunskill, 2019],
that use greedy policies instead of solving an MDP (full planning) at the beginning of each episode.
Surprisingly, we ﬁnd that both generalized algorithms do not suffer from performance degradation, up
to numerical constants and logarithmic factors. Thus, we conclude that there exists an RL algorithm
that achieves the minimax regret bound, while acting according to greedy policies.

Consider the general RL scheme that explores by greedy policies as depicted in Algorithm 2. The
value ¯V is initialized optimistically and the algorithm interacts with the unknown environment in
an episodic manner. At each time-step t, a greedy policy from the current state, sk
t , is calculated
optimistically based on the empirical model (ˆrk−1, ˆpk−1, nk−1) and the current value at the next
time-step ¯V k−1
t+1 . This is done in a subroutine called ‘ModelBasedOptimisticQ’.4 We further assume
the optimistic Q-function has the form ¯Q(sk
t+1 and refer to

t , a) + ˜pk−1(· | sk

t , a) = ˜rk−1(sk

t , a)T ¯V k−1

3Existing PAC results on RTDP analyze variations of RTDP in which (cid:15) is an input parameter of the algorithm.
4We also allow the subroutine to use O(S) internal memory for auxiliary calculations, which does not change

the overall space complexity.

5

t (s) = H − (t − 1).

Algorithm 2 Model-based RL with Greedy Policies
1: Initialize: ∀s ∈ S, ∀t ∈ [H], ¯V 0
2: for k = 1, 2, . . . do
Initialize sk
3:
1
for t = 1, . . . , H do
4:
5:
6:
7:
8:
9:
10:
11: end for

∀a, ¯Q(sk
t ∈ arg maxa ¯Q(sk
ak
t ) = min(cid:8) ¯V k−1
¯V k
t (sk
Act with ak

t
t and observe sk

t ), ¯Q(sk
t+1.

t , a)
(sk

t , ak

t )(cid:9)

end for
Update ˆrk, ˆpk, nk with all experience gathered in episode.

t , a) = ModelBasedOptimisticQ(cid:0)ˆrk−1, ˆpk−1, nk−1, ¯V k−1

t+1

(cid:1)

(˜rk−1, ˜pk−1) as the optimistic model. The agent interacts with the environment based on the greedy
policy with respect to ¯Q and uses the gathered experience to update the empirical model at the end of
the episode.

By construction of the update rule (see Line 7), the value is a decreasing function of k, for all
(s, t) ∈ S × [H]. Thus, property (ii) in Lemma 1 holds for Algorithm 2. Furthermore, the algorithms
analyzed in this section will also be optimistic with high probability, i.e., property (i) in Lemma 1
also holds. Finally, since the value update uses the empirical quantities ˆrk−1, ˆpk−1, nk−1 and ¯V k−1
t+1
from the previous episode, policy πk is still Fk−1 measurable.

The following lemma generalizes Lemma 2 to the case where, unlike in RTDP, the update rule does
not use the exact model (see proof in Appendix C).
Lemma 7 (Value Update for Optimistic Model). The expected cumulative value update of Algorithm
2 in the kth episode is bounded by

¯V k−1
1

(sk

1) − V πk

1 (sk

1) ≤

H
(cid:88)

t=1

E(cid:2) ¯V k−1

t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1

(cid:3)

+

H
(cid:88)

t=1

E(cid:2)(˜rk−1 − r)(sk

t , ak

t ) + (˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3) .

In the rest of the section, we consider two instantiations of the subroutine ‘ModelBasedOptimisticQ’
in Algorithm 2. We use the bonus terms of UCRL2 and of EULER to acquire an optimistic Q-
function, ¯Q. These two options then lead to UCRL2 with Greedy Policies (UCRL2-GP) and EULER
with Greedy Policies (EULER-GP) algorithms.

4.1 UCRL2 with Greedy Policies for Finite-Horizon MDPs

Algorithm 3 UCRL2 with Greedy Policies (UCRL2-GP)

1: ˜rk−1(sk

t , a) = ˆrk−1(sk
(cid:26)

t , a) +

(cid:114)

2 ln 8SAT

nk−1(sk

δ
t ,a)∨1

2: CI(sk

t , a) =

P (cid:48) ∈ P(S) : (cid:107)P (cid:48)(·) − ˆpk−1(· | sk

t , a) = arg maxP (cid:48)∈CI(sk

t , a) = ˜rk−1(sk

t , a) + ˜pk−1(· | sk

t ,a) P (cid:48)(· | sk
t , a)T ¯V k−1

t+1

3: ˜pk−1(· | sk
4: ¯Q(sk
5: Return ¯Q(sk

t , a)

t , a)(cid:107)1 ≤
t , a)T ¯V k−1

t+1

(cid:114)

(cid:27)

4S ln 12SAT
nk−1(sk

δ
t ,a)∨1

We form the optimistic local model based on the conﬁdence set of UCRL2 [Jaksch et al., 2010].
This amounts to use Algorithm 3 as the subroutine ‘ModelBasedOptimisticQ’ in Algorithm 2. The
maximization problem on Line 3 of Algorithm 3 is common, when using bonus based on an optimistic
model [Jaksch et al., 2010], and it can be solved efﬁciently in ˜O(N ) operations (e.g., Strehl and
Littman 2008, Section 3.1.5). A full version of the algorithm can be found in Appendix D.

6

Thus, Algorithm 3 performs N AH operations per episode. This saves the need to perform Extended
Value Iteration [Jaksch et al., 2010], that costs N SAH operations per episode (an extra factor of S).
Despite the signiﬁcant improvement in terms of computational complexity, the regret of UCRL2-GP
is similar to the one of UCRL2 [Jaksch et al., 2010] as the following theorem formalizes (see proof in
Appendix D).
Theorem 8 (Regret Bound of UCRL2-GP). For any time T ≤ KH, with probability at least 1 − δ,
the regret of UCRL2-GP is bounded by ˜O

AT + H 2

SSA

HS

√

√

(cid:16)

(cid:17)

.

Proof Sketch. Using the optimism of the value function (see Section D.2) and by applying Lemma 7,
we bound the regret as follows:

Regret(K) =

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≤

K
(cid:88)

k=1

¯V k−1
1

(sk

1) − V πk

1 (sk
1)

K
(cid:88)

H
(cid:88)

≤

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1]

k=1

t=1

K
(cid:88)

H
(cid:88)

+

k=1

t=1

E (cid:2)(˜rk−1 − r)(sk

t , ak

t ) + (˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3) .

(4)

Thus, the regret is upper bounded by two terms. As in Theorem 4, by applying Lemma 11 (Ap-
pendix A), the ﬁrst term in (4) is a sum of SH Decreasing Bounded Processes, and can thus be
bounded by ˜O(cid:0)SH 2(cid:1). The presence of the second term in (4) is common in recent regret analyses
(e.g., Dann et al. 2017). Using standard techniques [Jaksch et al., 2010, Dann et al., 2017, Zanette
and Brunskill, 2019], this term can be bounded (up to additive constant factors) with high probability
by (cid:46) H

≤ ˜O(HS

S (cid:80)K

AT ).

(cid:80)H

(cid:104)(cid:113)

√

√

E

(cid:105)

1

k=1

t=1

nk−1(sk

t ,ak

t ) | Fk−1

4.2 EULER with Greedy Policies

In this section, we use bonus terms as in EULER [Zanette and Brunskill, 2019]. Similar to the
previous section, this amounts to replacing the subroutine ‘ModelBasedOptimisticQ’ in Algorithm 2
with a subroutine based on the bonus terms from [Zanette and Brunskill, 2019]. Algorithm 5 in
Appendix E contains the pseudocode of the algorithm. The bonus terms in EULER are based on the
empirical Bernstein inequality and tracking both an upper bound ¯Vt and a lower-bound V t on V ∗
t .
Using these, EULER achieves both minimax optimal and problem dependent regret bounds.

EULER [Zanette and Brunskill, 2019] performs O(N SAH) computations per episode (same as the
VI algorithm), while EULER-GP requires only O(N AH). Despite this advantage in computational
complexity, EULER-GP exhibits similar minimax regret bounds to EULER (see proof in Appendix E),
much like the equivalent performance of UCRL2 and UCRL2-GP proved in Section 4.1.
Theorem 9 (Regret Bound of EULER-GP). Let G be an upper bound on the total reward
t+1(s)(cid:1) and
collected within an episode. Deﬁne Q∗ := maxs,a,t
Heﬀ := min(cid:8)Q∗, G2/H(cid:9). With probability 1 − δ, for any time T ≤ KH jointly on all episodes
k ∈ [K], the regret of EULER-GP is bounded by ˜O
. Thus,
√
(cid:16)√

(cid:0)VarR(s, a) + Vars(cid:48)∼p(·|s,a)V ∗

Heﬀ SAT +
√
(cid:17)

SSAH 2(

S +

(cid:16)√

H)

√

√

√

√

(cid:17)

it is also bounded by ˜O

HSAT +

SSAH 2(

S +

H)

.

Note that Theorem 9 exhibits similar problem-dependent regret-bounds as in Theorem 1 of [Zanette
and Brunskill, 2019]. Thus, the same corollaries derived in [Zanette and Brunskill, 2019] for EULER
can also be applied to EULER-GP.

5 Experiments

In this section, we present an empirical evaluation of both UCRL2 and EULER, and compare their
performance to the proposed variants, which use greedy policy updates, UCRL2-GP and EULER-GP,

7

(a) Chain environment with N = 25 states

(b) 2D chain environment with 5 × 5 grid

Figure 1: A comparison UCRL2 and EULER with their greedy counterpart. Results are averaged
over 5 random seeds and are shown alongside error bars (±3std).

respectively. We evaluated the algorithms on two environments. (i) Chain environment [Osband
and Van Roy, 2017]: In this MDP, there are N states, which are connected in a chain. The agent
starts at the left side of the chain and can move either to the left or try moving to the right, which
succeeds w.p. 1 − 1/N , and results with movement to the left otherwise. The agent goal is to reach
the right side of the chain and try moving to the right, which results with a reward r ∼ N (1, 1).
Moving backwards from the initials state also results with r ∼ N (0, 1), and otherwise, the reward is
r = 0. Furthermore, the horizon is set to H = N , so that the agent must always move to the right
to have a chance to receive a reward. (ii) 2D chain: A generalization of the chain environment, in
which the agent starts at the upper-left corner of a N × N grid and aims to reach the lower-right
corner and move towards this corner, in H = 2N − 1 steps. Similarly to the chain environment, there
is a probability 1/H to move backwards (up or left), and the agent must always move toward the
corner to observe a reward r ∼ N (1, 1). Moving into the starting corner results with r ∼ N (0, 1),
and otherwise r = 0. This environment is more challenging for greedy updates, since there are many
possible trajectories that lead to reward.

The simulation results can be found in Figure 1, and clearly indicate that using greedy planning
leads to negligible degradation in the performance. Thus, the simulations verify our claim that
greedy policy updates greatly improve the efﬁciency of the algorithm while maintaining the same
performance.

6 Related Work

SA/(cid:15)2(1 − γ)4(cid:17)
(cid:16)

Real-Time Dynamic Programming: RTDP [Barto et al., 1995] has been extensively used and has
many variants that exhibit superior empirical performance (e.g., [Bonet and Geffner, 2003, McMahan
et al., 2005, Smith and Simmons, 2006]). For discounted MDPs, Strehl et al. [2006] proved ((cid:15), δ)-PAC
bounds of ˜O
, for a modiﬁed version of RTDP in which the value updates occur
only if the decrease in value is larger than (cid:15)(1 − γ). I.e., their algorithm explicitly use (cid:15) to mark
states with accurate value estimate. We prove that RTDP converges in a rate of ˜O(cid:0)SH 2/(cid:15)(cid:1) without
knowing (cid:15). Indeed, Strehl et al. [2006] posed whether the original RTDP is PAC as an open problem.
Furthermore, no regret bound for RTDP has been reported in the literature.

Regret bounds for RL: The most renowned algorithms with regret guarantees for undiscounted
inﬁnite-horizon MDPs are UCRL2 [Jaksch et al., 2010] and REGAL [Bartlett and Tewari, 2009],
which have been extended throughout the years (e.g., by Fruit et al. 2018, Talebi and Maillard
2018). Recently, there is an increasing interest in regret bounds for MDPs with ﬁnite horizon H and
stationary dynamics. In this scenario, UCRL2 enjoys a regret bound of order HS
AT . Azar et al.
HSAT , which is also asymptotically
[2017] proposed UCBVI, with improved regret bound of order
tight [Osband and Van Roy, 2016]. Dann et al. [2018] presented ORLC that achieves tight regret
bounds and (nearly) tight PAC guarantees for non-stationary MDPs. Finally, Zanette and Brunskill
[2019] proposed EULER, an algorithm that enjoys tight minimax regret bounds and has additional

√

√

8

problem-dependent bounds that encapsulate the MDP’s complexity. All of these algorithms are
model-based and require full-planning. Model-free RL was analyzed by [Jin et al., 2018]. There,
the authors exhibit regret bounds that are worse by a factor of H relatively to the lower-bound. To
the best of our knowledge, there are no model-based algorithms with regret guarantees that avoid
full-planning. It is worth noting that while all the above algorithms, and the ones in this work, rely on
the Optimism in the Face of Uncertainty principle [Lai and Robbins, 1985], Thompson Sampling
model-based RL algorithms exist [Osband et al., 2013, Gopalan and Mannor, 2015, Agrawal and Jia,
2017, Osband and Van Roy, 2017]. There, a model is sampled from a distribution over models, on
which full-planning takes place.

Greedy policies in model-based RL: By adjusting RTDP to the case where the model is unknown,
Strehl et al. [2012] formulated model-based RL algorithms that act using a greedy policy. They
proved a ˜O
sample complexity bound for discounted MDPs. To the best of our
knowledge, there are no regret bounds for model-based RL algorithms that act by greedy policies.

S2A/(cid:15)3(1 − γ)6(cid:17)

(cid:16)

Practical model-based RL: Due to the high computational complexity of planning in model-based
RL, most of the practical algorithms are model-free (e.g., Mnih et al. 2015). Algorithms that do use a
model usually only take advantage of local information. For example, Dyna [Sutton, 1991, Peng et al.,
2018] selects state-action pairs, either randomly or via prioritized sweeping [Moore and Atkeson,
1993, Van Seijen and Sutton, 2013], and updates them according to a local model. Other papers use
the local model to plan for a short horizon from the current state [Tamar et al., 2016, Hafner et al.,
2018]. The performance of such algorithms depends heavily on the planning horizon, that in turn
dramatically increases the computational complexity.

7 Conclusions and Future Work

In this work, we established that tabular model-based RL algorithms can explore by 1-step planning
instead of full-planning, without suffering from performance degradation. Speciﬁcally, exploring
with model-based greedy policies can be minimax optimal in terms of regret. Differently put, the
variance caused by exploring with greedy policies is smaller than the variance caused by learning
a sufﬁciently good model. Indeed, the extra term which appears due to the greedy exploration is
˜O(SH 2) (e.g., the ﬁrst term in (4)); a constant term, smaller than the existing constant terms of
UCRL2 and EULER.

This work raises and highlights some interesting research questions. The obvious ones are extensions
to average and discounted MDPs, as well as to Thompson sampling based RL algorithms. Although
these scenarios are harder or different in terms of analysis, we believe this work introduces the
relevant approach to tackle this question. Another interesting question is the applicability of the
results in large-scale problems, when tabular representation is infeasible and approximation must be
used. There, algorithms that act using lookahead policies, instead of 1-step planning, are expected
to yield better performance, as they are less sensitive to value approximation errors (e.g., Bertsekas
and Tsitsiklis 1996, Jiang et al. 2018, Efroni et al. 2018b,a). Even then, full-planning, as opposed to
using a short-horizon planning, might be unnecessary. Lastly, establishing whether the model-based
approach is or is not provably better than the model-free approach, as the current state of the literature
suggests, is yet an important and unsolved open problem.

Acknowledgments

We thank Oren Louidor for illuminating discussions relating the Decreasing Bounded Process, and
Esther Derman for the very helpful comments. This work was partially funded by the Israel Science
Foundation under ISF grant number 1380/16.

References

Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case
regret bounds. In Advances in Neural Information Processing Systems, pages 1184–1194, 2017.

9

Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 263–272. JMLR. org, 2017.

Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence, pages 35–42. AUAI Press, 2009.

Andrew G Barto, Steven J Bradtke, and Satinder P Singh. Learning to act using real-time dynamic

programming. Artiﬁcial intelligence, 72(1-2):81–138, 1995.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientiﬁc

Belmont, MA, 1996.

Blai Bonet and Hector Geffner. Labeled rtdp: Improving the convergence of real-time dynamic

programming. In ICAPS, volume 3, pages 12–21, 2003.

Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, pages
5713–5723, 2017.

Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certiﬁcates: Towards accountable

reinforcement learning. arXiv preprint arXiv:1811.03056, 2018.

Victor H de la Peña, Michael J Klass, Tze Leung Lai, et al. Pseudo-maximization and self-normalized

processes. Probability Surveys, 4:172–192, 2007.

Victor H de la Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and

Statistical Applications. Springer Science & Business Media, 2008.

Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. How to combine tree-search methods

in reinforcement learning. arXiv preprint arXiv:1809.01843, 2018a.

Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. Multiple-step greedy policies in
approximate and online reinforcement learning. In Advances in Neural Information Processing
Systems, pages 5238–5247, 2018b.

Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efﬁcient bias-span-constrained

exploration-exploitation in reinforcement learning. arXiv preprint arXiv:1802.04020, 2018.

Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized markov decision

processes. In Conference on Learning Theory, pages 861–898, 2015.

Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.

Daniel R Jiang, Emmanuel Ekwedike, and Han Liu. Feedback-based tree search for reinforcement

learning. arXiv preprint arXiv:1805.05935, 2018.

Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efﬁcient?

In Advances in Neural Information Processing Systems, pages 4863–4873, 2018.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

applied mathematics, 6(1):4–22, 1985.

Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penaliza-

tion. arXiv preprint arXiv:0907.3740, 2009.

H Brendan McMahan, Maxim Likhachev, and Geoffrey J Gordon. Bounded real-time dynamic
programming: Rtdp with monotone upper bounds and performance guarantees. In Proceedings of
the 22nd international conference on Machine learning, pages 569–576. ACM, 2005.

10

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.

Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning with

less data and less time. Machine learning, 13(1):103–130, 1993.

Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv

preprint arXiv:1608.02732, 2016.

Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement
learning? In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 2701–2710. JMLR. org, 2017.

Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efﬁcient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems, pages 3003–3011,
2013.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su. Deep
dyna-q: Integrating planning for task-completion dialogue policy learning. arXiv preprint
arXiv:1801.06176, 2018.

Trey Smith and Reid Simmons. Focused real-time dynamic programming for mdps: Squeezing more

out of a heuristic. In AAAI, pages 1227–1232, 2006.

Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.

Alexander L Strehl, Lihong Li, and Michael L Littman. Pac reinforcement learning bounds for rtdp

and rand-rtdp. In Proceedings of AAAI workshop on learning for search, 2006.

Alexander L Strehl, Lihong Li, and Michael L Littman. Incremental model-based learners with

formal learning-time guarantees. arXiv preprint arXiv:1206.6870, 2012.

Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART

Bulletin, 2(4):160–163, 1991.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undis-

counted reinforcement learning in mdps. arXiv preprint arXiv:1803.01626, 2018.

Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In

Advances in Neural Information Processing Systems, pages 2154–2162, 2016.

Harm Van Seijen and Richard S Sutton. Planning by prioritized sweeping with small backups. arXiv

preprint arXiv:1301.2343, 2013.

Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger.
Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.

Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. arXiv preprint arXiv:1901.00210,
2019.

11

A Proofs on Decreasing Bounded Processes

In this section, we state and prove useful results on Decreasing Bounded Processes (see Deﬁnition 1).
These results will be in use in proofs of the central theorems of this work.
Theorem 3 (Regret Bound of a Decreasing Bounded Process). Let {Xk, Fk}k≥0 be a Decreasing
Bounded Process and RK = (cid:80)K

k=1 Xk−1 − E[Xk | Fk−1] be its K-round regret. Then,

(cid:26)

Pr

∃K > 0 : RK ≥ C

(cid:16)

1 + 2(cid:112)ln(2/δ)

(cid:17)2(cid:27)

≤ δ.

Speciﬁcally, it holds that Pr{∃K > 0 : RK ≥ 9C ln(3/δ)} ≤ δ.

Proof. Without loss of generality, assume C > 0, since otherwise the results are trivial. We start
by remarking that RK is almost surely monotonically increasing, since Xk ≤ Xk−1. Deﬁne the
martingale difference process

ξk = Xk − E[Xk | Fk−1] = Xk − Xk−1 − E[Xk − Xk−1 | Fk−1]

and the martingale process MK = (cid:80)K
k=1 ξk. Since Xk ≥ 0 almost surely, RK can be bounded
by RK = MK + X0 − XK ≤ X0 + MK. Also deﬁne the quadratic variations as (cid:104)M (cid:105)K =
(cid:80)K
k. Next, recall Theorem 2.7 of [de la Peña et al., 2007]:

(cid:3) and [M ]K = (cid:80)K

k | Fk−1

k=1 ξ2

E(cid:2)ξ2

k=1

Theorem 10. Let A and B be two random variables, such that for all λ ∈ R, we have

Then, ∀x > 0,

(cid:104)
eλA− λ2B2
E

2

(cid:105)

≤ 1 .

(cid:40)

Pr

|A|
(cid:112)B2 + E[B2]

(cid:41)

> x

≤

√

2e−x2/4.

(5)

(6)

Condition (5) holds for AK = MK and B2
et al., 2008]. AK can be easily bounded by |AK| ≥ RK − X0 ≥ RK − C. To bound B2
calculate ξ2
k = (Xk − Xk−1)2 − 2(Xk − Xk−1)E[Xk − Xk−1 | Fk−1] + (E[Xk − Xk−1 | Fk−1])2,
ξ2
E(cid:2)ξ2

K = (cid:104)M (cid:105)K + [M ]K, due to Theorem 9.21 of [de la Peña
K, we ﬁrst

− (E[Xk − Xk−1 | Fk−1])2.

(Xk − Xk−1)2 | Fk−1

k and E(cid:2)ξ2

(cid:104)
(cid:3) = E

k | Fk−1

(cid:3):

(cid:105)

k | Fk−1

Thus,
k + E(cid:2)ξ2
ξ2
k | Fk−1
(cid:104)
= (Xk − Xk−1)2 + E

(cid:3)

(Xk − Xk−1)2 | Fk−1

(cid:105)

− 2(Xk − Xk−1)E[Xk − Xk−1 | Fk−1]

(cid:104)
(∗)
(Xk − Xk−1)2 | Fk−1
≤ (Xk − Xk−1)2 + E

(cid:105)

(∗∗)
≤ (Xk − Xk−1)2 + CE[Xk−1 − Xk | Fk−1] .

In (∗) we used the fact that Xk−1−Xk ≥ 0 a.s., which allows us to conclude that the cross-term is non-
positive. In (**), we bounded 0 ≤ Xk−1 − Xk ≤ C. We can also bound (cid:80)K
k=1(Xk−1 − Xk)2 ≤ C 2,
since each of the summands is a.s. non-negative, and thus,
(cid:33)2

K
(cid:88)

(cid:32) K
(cid:88)

(Xk−1 − Xk)2 ≤

Xk−1 − Xk

= (XK − X0)2 ≤ C 2.

k=1

k=1

Combining all of the above bounds yields

B2

K ≤

K
(cid:88)

(cid:16)

k=1

(Xk − Xk−1)2 + CE[Xk−1 − Xk | Fk−1]

(cid:17)

≤ C 2 + C

K
(cid:88)

k=1

E[Xk−1 − Xk | Fk−1] = C 2 + CRK.

12

Finally, we can bound E(cid:2)B2

K

(cid:3) by

E(cid:2)B2

K

(cid:3) =

K
(cid:88)

k=1

E(cid:2)ξ2

k + E(cid:2)ξ2

k | Fk−1

(cid:3)(cid:3) = 2

K
(cid:88)

k=1

E(cid:2)E(cid:2)ξ2

k | Fk−1

(cid:3)(cid:3)

= 2

≤ 2

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:104)
(cid:104)
E
E

(Xk − Xk−1)2 | Fk−1

(cid:105)

− (E[Xk − Xk−1 | Fk−1])2(cid:105)

(cid:104)
(cid:104)
E
E

(Xk − Xk−1)2 | Fk−1

(cid:105)(cid:105)

= 2

K
(cid:88)

(Xk − Xk−1)2 ≤ 2C 2.

k=1

Combining everything we obtain

|A|
(cid:112)B2 + E[B2]

≥

√

RK − C
C 2 + CRK + 2C 2

=

√

RK − C
3C 2 + CRK

.

Or, substituting in (6), we have

(cid:26) RK − C

Pr

√

3C 2 + CRK

(cid:27)

(cid:40)

> x

≤ Pr

|A|
(cid:112)B2 + E[B2]

(cid:41)

> x

≤

√

2e−x2/4.

Next, notice that for C > 0, the function f (y) =

y−C√

3C2+Cy

is monotonically increasing for any

y > 0:

f (cid:48)(y) =

(cid:112)3C 2 + Cy − C(y−C)
3C2+Cy

√
2

3C 2 + Cy

=

2(3C 2 + Cy) − Cy + C 2
2(3C 2 + Cy)3/2

=

7C 2 + Cy
2(3C 2 + Cy)3/2

> 0

Moreover, for y = C(1 + x)2,

f (cid:0)C(1 + x)2(cid:1) =

>

C(1 + x)2 − C
(cid:112)3C 2 + C 2(1 + x)2
Cx2 + 2Cx
4C 2 + 4C 2x + C 2x2

√

=

√

=

Cx2 + 2Cx
4C 2 + 2C 2x + C 2x2
Cx2 + 2Cx
Cx + 2C

= x ,

where the inequality holds since x > 0. Thus, if RK ≥ C(1 + x)2, then f (RK) > x, and we can
bound the probability that RK ≥ C(1 + x)2 by

Pr(cid:8)RK ≥ C(1 + x)2(cid:9) ≤ Pr

√

(cid:26) RK − C

3C 2 + CRK

(cid:27)

> x

≤

√

2e−x2/4 ,

and setting x = 2

(cid:113)

ln 2

δ > 0, we obtain






Pr

RK ≥ C

(cid:32)

(cid:114)

1 + 2

ln

(cid:33)2




2
δ

≤ δ .

We remark that since RK is monotonically increasing a.s., this bound also implies that




Pr



∃N : 1 ≤ N ≤ K, RN ≥ C

1 + 2

ln

(cid:32)

(cid:114)

(cid:33)2




2
δ

≤ δ.

13

To obtain a uniform bound, that is, bound that holds for all K > 0, note that the random sequence
ZK = 1

is monotonically increasing in K and bounded.

∃1 ≤ N ≤ K : RN ≥ C

(cid:17)2(cid:27)

1 + 2

(cid:113)

(cid:26)

(cid:16)

ln 2
δ

Thus, due to monotone convergence




Pr



∃K > 0 : RK ≥ C

1 + 2

ln

(cid:32)

(cid:114)

2
δ

(cid:33)2




(cid:32)

= lim
K→∞




Pr



∃1 ≤ N ≤ K : RN ≥ C

1 + 2

ln

(cid:114)

(cid:33)2




2
δ

≤ δ.

(cid:104)
= E

lim
K→∞

(cid:105)

ZK

= lim
K→∞

E[ZK]

To conclude the proof, note that δ ≤ 1, and thus, ln 3

(cid:32)

(cid:114)

C

1 + 2

ln

2
δ

(cid:33)2

(cid:32)

(cid:114)

≤ C

1 + 2

ln

3
δ

which yields the second bound.

δ ≥ 1. Therefore, we can bound
(cid:33)2

(cid:33)2

(cid:114)

(cid:32)
3

≤ C

ln

3
δ

= 9C ln

,

3
δ

Lemma 11. Let {X k
of the sum of processes is deﬁned as R(K) = (cid:80)N
δ > 0, we have

n}k≥1 be a Bounded Decreasing Process in [0, C] for any n ∈ [N ]. The regret
n | Fk−1]. Then, for any

n − E[X k

k=1 X k−1

(cid:80)K

n=1

(cid:26)

Pr

∃K > 0 : R(K) ≥ 9CN ln

(cid:27)

3N
δ

≤ δ.

Proof. We ﬁrst remark that if X 0
so we assume w.l.o.g. that X 0

n = C. Deﬁne

n < C, we can replace it to X 0

n = C, which only increases the regret,

Rn(K) :=

K
(cid:88)

k=1

X k−1
n

(s) − E[X k

n(s) | Fk−1].

Deﬁne the event An := (cid:8)∃K > 0 : Rn(K) ≥ 9CN ln 3N
ity δ

N , it holds that for a ﬁxed n ∈ [N ]

δ

(cid:9). By applying Theorem 3, with probabil-

(cid:26)

Pr

∃K > 0 : Rn(K) ≥ 9C ln

(cid:27)

3N
δ

= Pr{An} ≤

δ
N

.

(7)

Finally, we obtain

(cid:26)

Pr

∃K > 0 : R(K) ≥ 9N C ln

(cid:27)

3N
δ

(cid:40)

= Pr

∃K > 0 :

N
(cid:88)

Rn(K) ≥ 9N C ln

(cid:40) N
(cid:91)

(1)
≤ Pr

An

n=1
(cid:41) (2)
≤

N
(cid:88)

n=1

n=1

Pr{An}

(3)
≤ δ.

(cid:41)

3N
δ

Relation (1) holds since

(cid:40)

∃K > 0 :

N
(cid:88)

n=1

Rn(K) ≥ 9N C ln

(cid:41)

3N
δ

⊆

N
(cid:91)

n=1

An.

In (2) we use the union bound and (3) holds by (7).

14

B Proof of Real-Time Dynamic Programming Bounds

Lemma 1. For all s, t, and k, it holds that (i) V ∗

t (s) ≤ ¯V k

t (s) and (ii) ¯V k

t (s) ≤ ¯V k−1

t

(s).

Proof. Both claims are proven using induction.

(i) By the initialization, ∀s, t, V ∗
t (s). Assume the claim holds for k − 1 episodes. Let
sk
t be the state the algorithm is at in the tth time-step of the kth episode. By the value update of
Algorithm 1,

t (s) ≤ V 0

¯V k
t (sk

t ) = max

a

r(sk

t , a) +

≥ max

a

r(sk

t , a) +

p(s(cid:48) | sk

t , a) ¯V k−1

t+1 (s(cid:48))

p(s(cid:48) | sk

t , a) ¯V ∗

t+1(s(cid:48)) = V ∗(sk

t ).

(cid:88)

s(cid:48)
(cid:88)

s(cid:48)

The second relation holds by the induction hypothesis and the monotonicity of the optimal Bellman
operator [Bertsekas and Tsitsiklis, 1996]. The third relation holds by the recursion satisﬁed by the
optimal value function (see Section 2). Thus, the induction step is proven for the ﬁrst claim.

(ii) To prove the base case of the second claim we use the optimistic initialization. Let s1
state the algorithm is at in the tth time-step of the ﬁrst episode. By the update rule,

t be the

¯V 1
t (s1

t ) = max

a

r(s1

t , a) +

(cid:88)

s(cid:48)

p(s(cid:48) | s1

t , a) ¯V 0

t+1(s(cid:48))

(1)
= max

a

r(s1

t , a) + H − t

(2)
≤ 1 + H − t = H − (t − 1)

(3)
= ¯V 0

t (s1

t ).

Relation (1) holds by the initialization of the values, (2) holds since r(s, a) ∈ [0, 1] and (3) is by the
initialization. States that were not visited on the ﬁrst episode were not update, and thus the inequality
trivially holds.
Assume the second claim holds for k − 1 episodes. Let sk
tth time-step of the kth episode. By the value update of Algorithm 1, we have

t be the state that the algorithm is at in the

¯V k
t (sk

t ) = max

a

r(sk

t , a) +

(cid:88)

p(s(cid:48) | sk

t , a) ¯V k−1

t+1 (s(cid:48)).

s(cid:48)
t was previously updated, let ¯k be the previous episode in which the update occured. By the
(s). Using the monotonicity of the Bellman

If sk
induction hypothesis, we have that ∀s, t, ¯V ¯k
t (s) ≥ ¯V k−1
operator [Bertsekas and Tsitsiklis, 1996], we may write

t

max
a

r(sk

t , a) +

(cid:88)

s(cid:48)

p(s(cid:48) | sk

t , a) ¯V k−1

t+1 (s(cid:48))

≤ max

a

r(sk

t , a) +

(cid:88)

s(cid:48)

p(s(cid:48) | sk

t , a) ¯V

¯k−1
t+1 (s(cid:48)) = ¯V k−1(sk

t ).

Thus, ¯V k
¯V k−1
(sk
t
and the result can be proven similarly to the base case.

t ) ≤ ¯V k−1(sk
t (sk

t (sk
t ) = ¯V 0

t ) and the induction step is proved. If sk

t ). In this case, the induction hypothesis implies that ∀s(cid:48), ¯V k−1

t was not previously updated, then
t+1(s(cid:48))

t+1 (s(cid:48)) ≤ ¯V 0

15

Lemma 2 (Value Update for Exact Model). The expected cumulative value update of RTDP at the
kth episode satisﬁes

¯V k−1
1

(sk

1) − V πk

1 (sk

1) =

H
(cid:88)

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1].

Proof. By the deﬁnition of ak

t and the update rule, the following holds:

t )T ¯V k−1
t , ak

(cid:3) + E


(cid:88)

E(cid:2) ¯V k

t (sk

t ) | Fk−1

(cid:3) = E(cid:2)r(sk

t , ak

t ) + p(· | sk

t+1 | Fk−1

(cid:3)



.

= E(cid:2)r(sk

t , ak

t ) | Fk−1

p(¯st+1 | st, πk) ¯V k−1

t+1 (¯st+1) | Fk−1

¯st+1

Furthermore,



E



(cid:88)

p(¯st+1 | sk

t , πk) ¯V k−1

t+1 (¯st+1) | Fk−1





=

=

¯st+1
(cid:88)

Pr(sk

t | sk

1, πk)

(cid:88)

p(¯st+1 | sk

t , πk) ¯V k−1

t+1 (¯st+1)

Pr(sk

t+1 | sk

¯st+1∈S
1, πk) ¯V k−1

t+1 (sk

t+1) = E(cid:2) ¯V k−1

t+1 (sk

t+1) | Fk−1

(cid:3).

(8)

sk
t
(cid:88)

sk
t+1∈S

The ﬁrst relation holds by deﬁnition and the second one holds by the Markovian property of the
dynamics. Substituting back and summing both side from t = 1, . . . , H, we obtain

(cid:34) H
(cid:88)

E

t=1

¯V k
t (sk

t ) | Fk−1

(cid:35)

(cid:34) H
(cid:88)

= E

t=1
(cid:34) H
(cid:88)

= E

t=1

r(sk

t , ak

t ) | Fk−1

r(sk

t , ak

t ) | Fk−1

(cid:35)

(cid:35)

(cid:34) H
(cid:88)

+ E

t=1
(cid:34) H
(cid:88)

+ E

¯V k−1
t+1 (sk

t+1) | Fk−1

(cid:35)

(cid:35)

¯V k−1
t

(sk

t ) | Fk−1

− ¯V k−1
1

(sk
1)

= V πk

1 (sk

1) + E

(cid:34) H
(cid:88)

t=1

(cid:35)

¯V k−1
t

(sk

t ) | Fk−1

− ¯V k−1
1

(sk
1)

The second line hold by shifting the index of the sum and using the fact that ∀s, ¯V k
third line holds by the deﬁnition of the value function,

H+1(s) = 0. The

t=1

H
(cid:88)

t=1

E[r(sk

t , ak

t ) | Fk−1] = E[

H
(cid:88)

t=1

r(sk

t , ak

t ) | s1 = sk

1] = V πk

1 (sk

1).

Reorganizing the equation yields the desired result.

16

Theorem 4 (Regret Bounds for RTDP). The following regret bounds hold for RTDP:

1. E[Regret(K)] ≤ SH 2.

2. For any δ > 0, with probability 1 − δ, for all K > 0, Regret(K) ≤ 9SH 2 ln(3SH/δ).

Proof. The following bounds on the regret hold.

Regret(K) :=

K
(cid:88)

k=1

V ∗(sk

1) − V πk (sk

1) ≤

K
(cid:88)

k=1

¯V k−1
1

(sk

1) − V πk (sk
1)

≤

K
(cid:88)

H
(cid:88)

k=1

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1].

(9)

The second relation is by the optimism of the value function (Lemma 1), and the third relation is by
Lemma 2.

To prove the bound on the expected regret, we take expectation on both sides of (9). Thus,

E[Regret(K)] ≤

K
(cid:88)

k=1

E[E[

H
(cid:88)

t=1

¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1]] = E[

K
(cid:88)

H
(cid:88)

¯V k−1
t

k=1

t=1

(sk

t ) − ¯V k

t (sk

t )].

Where the second relation holds by the tower property and linearity of expectation. Finally, for any
run of RTDP, we have that

K
(cid:88)

H
(cid:88)

k=1

t=1

¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) =

(cid:88)

H
(cid:88)

s

t=1

t (s) − ¯V K
¯V 0

t (s) ≤

(cid:88)

H
(cid:88)

s

t=1

¯V 0
t (s) − V ∗

t (s) ≤ SH 2.

The ﬁrst relation holds since per s, the sum is telescopic, thus, only the ﬁrst and last term exist in the
sum. Due to the update rule, on the ﬁrst time a state appears, its value will be ¯V 0
t (s). From the last
time it appears, its value will not be updated and thus the last value of a state is ¯V K
t (s). The second
relation holds by Lemma 1. The third relation holds since ∀s, t, ¯V 0
t (s) ∈ [0, H], summing
on SH such terms yields the result.

t (s) − V ∗

To prove the high-probability bound we apply Lemma 34 by which,

(9) =

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s

k=1

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1].

t (s)(cid:9)

For a ﬁxed s, t, (cid:8) ¯V k
k≥0 is a Decreasing Bounded Process by Lemma 1, and its initial value
is less than H. Thus, (9) is a sum of SH Decreasing Bounded Processes. We apply Lemma 11
which provides a high-probability bound on a sum of Decreasing Bounded Processes to conclude the
proof.

17

Corollary 5 (RTDP is Uniform PAC). Let δ > 0 and N(cid:15) be the number of episodes in which RTDP
outputs a policy with V ∗

1 (sk

1) > (cid:15). Then,

1 (sk

1) − V πk
(cid:26)

Pr

∃(cid:15) > 0 : N(cid:15) ≥

9SH 2 ln(3SH/δ)
(cid:15)

(cid:27)

≤ δ.

Proof. Let KN(cid:15) be an episode index such that there are N(cid:15) previous episodes k ≤ KN(cid:15) in which
RTDP outputs a policy with V ∗

1) > (cid:15). The following relation holds,

1) − V πk

1 (sk

1 (sk

∀(cid:15) > 0 : kkvbvuvdhf irinhblbkudchurbknbulrN(cid:15)(cid:15) ≤ Regret(KN(cid:15) ).

Thus,

(cid:26)

∃(cid:15) > 0 : N(cid:15)(cid:15) ≥ 9SH 2 ln

3SH
δ

(cid:27)

(cid:26)

⊆

⊆

(cid:26)

Regret(KN(cid:15)) ≥ 9SH 2 ln

(cid:27)

3SH
δ

∃K > 0 : Regret(K) ≥ 9SH 2 ln

Which results in

(cid:26)

Pr

∃(cid:15) > 0 : N(cid:15)(cid:15) ≥ 9SH 2 ln

(cid:27)

(cid:26)

≤ Pr

3SH
δ

∃K > 0 : Regret(K) ≥ 9SH 2 ln

where the third relation holds by Theorem 4.

(cid:27)

.

3SH
δ

(cid:27)

3SH
δ

≤ δ.

Corollary 6 (RTDP is (0, δ) PAC). Let δ > 0 and N be the number of episodes in which
RTDP outputs a non optimal policy. Deﬁne the (unknown) gap of the MDP, ∆(M) =
mins minπ:V π

1 (s)(cid:54)=V ∗

1 (s) V ∗

1 (s) − V π
1 (s) > 0. Then,
(cid:26)

Pr

N ≥

9SH 2 ln(3SH/δ)
∆(M)

(cid:27)

≤ δ.

Proof. We have that N = N∆(M) since ∆(M) is the minimal gap; in all rest of episodes in which
the gap is smaller than ∆(M), the policy πk is necessarily the optimal one. Based on Corollary 5 we
conclude that,

(cid:40)

Pr

N ≥

9SH 2 ln 3SH
∆(M)

δ

(cid:41)

(cid:40)

= Pr

N∆(M) ≥

9SH 2 ln 3SH
∆(M)

δ

(cid:41)

≤ δ.

18

C Proofs of Section 4

Lemma 7 (Value Update for Optimistic Model). The expected cumulative value update of Algorithm
2 in the kth episode is bounded by

¯V k−1
1

(sk

1) − V πk

1 (sk

1) ≤

H
(cid:88)

t=1

E(cid:2) ¯V k−1

t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1

(cid:3)

+

H
(cid:88)

t=1

E(cid:2)(˜rk−1 − r)(sk

t , ak

t ) + (˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3) .

We prove a more general, Lemma 12, of which Lemma 7 is a direct corollay (by setting t = 1).

Lemma 12. The expected value update of Algorithm 2 in the kth episode at the state tth is bounded
by

¯V k−1
t

(sk

t ) − V πk

t

(sk

t ) ≤

H
(cid:88)

t(cid:48)=t

E(cid:2) ¯V k−1
t(cid:48)

(sk

t(cid:48)) − ¯V k

t(cid:48) (sk

t(cid:48)) | Fk−1, sk
t

(cid:3)

+

H
(cid:88)

t(cid:48)=t

E(cid:2)(˜rk−1 − r)(sk

t(cid:48), ak

t(cid:48)) + (˜pk−1 − p)(· | sk

t(cid:48), ak

t(cid:48))T ¯V k−1

t(cid:48)+1 | Fk−1, sk

t

(cid:3) .

Proof. We closely follow the proof of Lemma 2. By the deﬁnition of ak
t(cid:48) ≥ t, the following holds.

t and the update rule, for

t(cid:48)) | Fk−1, sk
t

(cid:3)

E(cid:2) ¯V k
t(cid:48) (sk

˜rk−1(sk

(1)
≤ E

t(cid:48), ak

t(cid:48)) +

˜pk−1(¯st(cid:48)+1 | sk

t(cid:48), at(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t





(cid:88)

¯st(cid:48)+1∈S

(2)

= E(cid:2)r(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t(cid:48)


(cid:3) + E


(cid:88)

¯st(cid:48)+1∈S

p(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t






(˜rk−1 − r)(sk

+ E

t(cid:48), ak

t(cid:48)) +

(cid:88)

(˜pk−1 − p)(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t

¯st(cid:48)+1∈S
(cid:3) + E(cid:2) ¯V k−1

t(cid:48)+1 (sk

t(cid:48)+1) | Fk−1, sk
t

(cid:3)

(3)

= E(cid:2)r(sk

t(cid:48), ak

t(cid:48)) | Fk−1, , sk
t


(˜rk−1 − r)(sk

+ E

t(cid:48), ak

t(cid:48)) +

(cid:88)

¯st(cid:48)+1∈S

(˜pk−1 − p)(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t









Relation (1) holds by the update rule for ¯V k
t . Next, (2) holds by adding an subtracting the real
reward and dynamics and using linearity of expectation. In (3), we used the same reasoning as in
Equation (8).

19

Summing both side from t(cid:48) = t, . . . , H, we obtain:

(cid:34) H
(cid:88)

E

t(cid:48)=t

¯V k
t(cid:48) (sk

t(cid:48)) | Fk−1, sk
t

(cid:35)

(cid:34) H
(cid:88)

≤ E

t(cid:48)=t

r(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:35)

(cid:34) H
(cid:88)

+ E

¯V k−1
t(cid:48)+1 (sk

t(cid:48)+1) | Fk−1, sk
t

(cid:35)


E
(˜rk−1 − r)(sk

t(cid:48), ak

t(cid:48)) +

+

H
(cid:88)

t(cid:48)=t

t(cid:48)=t

(cid:88)

¯st(cid:48)+1∈S

(˜pk−1 − p)(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t

(1)
= V πk
t

(sk

t ) + E

¯V k−1
t(cid:48)+1 (sk

t(cid:48)+1) | Fk−1, sk
t

(cid:35)

(cid:34) H
(cid:88)

t(cid:48)=t


E
(˜rk−1 − r)(sk

t(cid:48), ak

t(cid:48)) +

+

H
(cid:88)

t(cid:48)=t

(cid:88)

¯st(cid:48)+1∈S

(˜pk−1 − p)(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t

(2)
= V πk
t

(cid:34) H
(cid:88)

(sk

t ) + E

¯V k−1
t(cid:48)

(sk

t(cid:48)) | Fk−1, sk
t

− ¯V k−1
t

(sk
t )

(cid:35)

t(cid:48)=t


E
(˜rk−1 − r)(sk

t(cid:48), ak

t(cid:48)) +

(cid:88)

(˜pk−1 − p)(¯st(cid:48)+1 | sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t(cid:48)+1 (¯st(cid:48)+1) | Fk−1, sk

t













+

H
(cid:88)

t(cid:48)=t

In (1) we used the fact that V πk
t(cid:48)=t r(sk
shifting the index of the sum and using ∀s, k, ¯V k−1
desired result.

(sk

t

t(cid:48), πk(sk

t ]. Relation (2) holds by
H+1(s) = 0. Reorganizing the equation yields the

t(cid:48))) | Fk−1, sk

¯st(cid:48)+1∈S
t ) = E[(cid:80)H

20

D Proof of Theorem 8

Algorithm 4 UCRL2 with Greedy Policies
4 ∀s ∈ S, t ∈ [H], ¯V 0
1: Initialize: δ, δ(cid:48) = δ
2: for k = 1, 2, .. do
Initialize sk
3:
1
for t = 1, .., H do
4:
5:
6:

#Update Upper Bound on V ∗
for a ∈ A do
˜rk−1(sk

t , a) +

(cid:114)

t , a) = ˆrk−1(sk
(cid:40)

t (s) = H − (t − 1).

2 ln 2SAT

nk−1(sk

δ(cid:48)
t ,a)∨1

(cid:114)

4S ln 3SAT

nk−1(sk

δ(cid:48)
t ,a)∨1

(cid:41)

7:

8:

9:

10:
11:
12:
13:
14:
15:
16:
17:
18: end for

CI(sk

t , a) =

P (cid:48) ∈ P(S) : (cid:107)P (cid:48)(·) − ˆpk−1(· | sk

t , a)(cid:107)1 ≤

t , a) = arg maxP (cid:48)∈CI(sk

t , a) + ˜pk−1(· | sk

t , a)T ¯V k−1

t+1

t ,a) P (cid:48)(· | sk
t , a)T ¯V k−1

t+1

t , a) = ˜rk−1(sk

˜pk−1(· | sk
¯Q(sk
end for
t ∈ arg maxa ¯Q(sk
ak
t ) = min(cid:8) ¯V k−1
¯V k
t (sk
#Act by the Greedy Policy
Apply ak
t+1.

t and observe sk

t , a)
(sk

t

1), ¯Q(sk

t , ak

t )(cid:9)

end for
Update ˆrk, ˆpk, nk with all experience gathered in the episode.

We provide the full proof of Theorem 8 which establishes a regret bound for UCRL2 with Greedy
Policies (UCRL2-GP) in ﬁnite horizon MDPs. In the following, we present the structure of this
section.

We deﬁne the failure events for UCRL2-GP in Section D.1. Most of the events are standard low-
probability failure events, derived using, e.g., Hoeffding’s inequality. We add to the standard set of
events a failure event which holds when a sum of Decreasing Bounded Processes is large in its value.
Using uniform bounds, the failure events are shown to hold jointly. When all failure events do not
occur for all time-steps we say the algorithm is outside the failure event. In Section D.2 we establish
that UCRL2-GP is optimistic, and, more speciﬁcally, that for all s, t, k ¯V k
t (s), outside the
failure event. Lastly, in Section D.3 we give the full proof of Theorem 8, based on a new regret
decomposition using on Lemma 7, the new results on Decreasing Bounded Processes (see Appendix
A), and existing techniques (e.g., [Dann et al., 2017, Zanette and Brunskill, 2019]).

t (s) ≥ V ∗

D.1 Failure Events for UCRL2 with Greedy Policies

Deﬁne the following failure events.

F r

k =

F p

k =

F N

k =














∃s, a : |r(s, a) − ˆrk−1(s, a)| ≥

(cid:115)

2 ln 2SAT
nk−1(s, a) ∨ 1

δ(cid:48)






(cid:115)

4S ln 3SAT
nk−1(s, a) ∨ 1

δ(cid:48)






∃s, a : (cid:107)p(· | s, a) − ˆpk−1(· | s, a)(cid:107)1 ≥

∃s, a : nk−1(s, a) ≤

1
2

(cid:88)

j<k

wj(s, a) − H ln

SAH
δ(cid:48)



.


(cid:40)

F DBP =

∃k > 0 :

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1] ≥ 9SH 2 ln

(cid:41)

3SH
δ(cid:48)

Furthermore, the following relations hold.

21

• Let F r = (cid:83)K

k=1 F r

k . Then Pr{F r} ≤ δ(cid:48), by Hoeffding’s inequality, and using a union
bound argument on all s, a, possible values of nk(s, a) and k. Furthermore, for n(s, a) = 0
the bound holds trivially since R ∈ [0, 1].

• Let F P = (cid:83)K

k=1 F p

k . Then Pr{F p} ≤ δ(cid:48), holds by [Weissman et al., 2003] while applying
union bound on all s, a, nk−1(s, a) and possible values of k (e.g., Azar et al. 2017, Zanette
and Brunskill 2019). Furthermore, for n(s, a) = 0 the bound holds trivially.

• Let F N = (cid:83)K

k=1 F N

k . Then, Pr(cid:8)F N (cid:9) ≤ δ(cid:48). The proof is given in [Dann et al., 2017]

Corollary E.4 (and is used in Zanette and Brunskill 2019 Appendix D.4).

• By construction of Algorithm 2, ∀s, t, ¯V k

Furthermore, since ˆrk−1(s, a) and ˆpk−1(· | s, a) are non-negative, and ¯V 0
induction allows us to conclude that ∀s, t, ¯V k
Pr(cid:8)F DBP (cid:9) ≤ δ(cid:48).
Lemma 13. Setting δ(cid:48) = δ
not hold we say the algorithm is outside the failure event.

4 then Pr{F r (cid:83) F p (cid:83) F N (cid:83) F DBP } ≤ δ. When the failure events does

t (s) is a decreasing function of k, with ¯V 0

t (s) = H.
t (s) > 0, a simple
t (s) ≥ 0. Thus, by applying Lemma 11,

D.2 UCRL2 with Greedy Policies is Optimistic

Lemma 14. Outside the failure event UCRL2-GP is Optimistic,

∀s, t, k ¯V k

t (s) ≥ V ∗

t (s).

Proof. We prove by induction. The base case holds by the initialization of the algorithm, ¯V 0
t (s) =
H − (t − 1) ≥ V ∗
t (s). Assume the induction hypothesis holds for k − 1 episodes. At the kth episode,
states there were not visited at step t will not be updated, and thus by the induction hypothesis, the
result hold for these states. For states that were visited, if the minimum at the update stage equals
to ¯V k−1
t be a state that was updated according to the
t
optimistic model, and let

(s), then the result similarly holds. Let sk

˜a∗ ∈ arg max

a

a∗ ∈ arg max

a

Then,

˜rk−1(sk

t , a) + ˜pk−1(· | sk
t , a)V ∗

t , a) + p(· | sk

t+1.

r(sk

t , a)vk−1
t+1

¯V k
t (sk

t ) = max

a

˜rk−1(sk

t , a) + ˜pk−1(· | sk

t , a) ¯V k−1

t+1

(1)
= ˜rk−1(sk
(2)
≥ ˜rk−1(sk
(3)
≥ r(sk

t , ˜a∗) + ˜pk−1(· | sk

t , ˜a∗) ¯V k−1

t+1

t , a∗) + ˜pk−1(· | sk

t , a∗) ¯V k−1

t+1

t , a∗) + p(· | sk

t , a∗) ¯V k−1

t+1

(4)
≥ r(sk

t , a∗) + p(· | sk

t , a∗)V ∗

t+1

(5)
= V ∗

t (sk

t ).

Relations (1) and (2) are by the deﬁnition and optimality of ˜a∗, respectively. (3) holds since outside
failure event F r
k , the real transition
probabilities p(· | s, a∗) ∈ CI(sk

t , a∗). Furthermore, outside failure event F p

t , a∗) ≥ r(sk

k , ˜rk−1(sk

t , a∗), and thus

max
P (cid:48)∈CI(sk

t ,a∗)

P (cid:48)(· | sk

t , a∗) ¯V k−1

t+1 = ˜pk−1(· | sk

t , a∗) ¯V k−1

t+1 ≥ pk−1(· | sk

t , a∗) ¯V k−1
t+1 .

Finally, (4) holds by the induction hypothesis ∀s, a, t, ¯V k−1
Bellman recursion.

t

(s) ≥ V ∗

t (s) and (5) holds by the

22

D.3 Proof of Theorem 8

Theorem 8 (Regret Bound of UCRL2-GP). For any time T ≤ KH, with probability at least 1 − δ,
the regret of UCRL2-GP is bounded by ˜O

AT + H 2

SSA

HS

√

√

(cid:17)

(cid:16)

.

Proof. By the optimism of the value (Lemma 14), we have that

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≤

K
(cid:88)

k=1

¯V k−1
1

(sk

1) − V πk

1 (sk
1)

K
(cid:88)

H
(cid:88)

≤

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1]

t=1

k=1
(cid:124)

K
(cid:88)

H
(cid:88)

+

t=1

k=1
(cid:124)

(cid:123)(cid:122)
(A)

(cid:125)

E (cid:2)(˜rk−1 − r)(sk

t , ak

t ) + (˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:123)(cid:122)
(B)

.

(10)

(cid:3)

(cid:125)

The ﬁrst relation is by the optimism of the value, and the second relation is by Lemma 7. We now
bound the two terms outside the failure event.

Bounding (A). By Lemma 34 (Appendix F),

(A) =

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1].

Outside the failure event, the sum is bounded by 9SH 2 ln 3SH
δ(cid:48)

(see event F DBP ). Thus,

(A) ≤ ˜O(SH 2).

Bounding (B). Outside failure event F r

k the following inequality holds:

K
(cid:88)

H
(cid:88)

k=1

t=1

E (cid:2)(˜rk−1 − r)(sk

t , πk(sk

t ) | Fk−1

(cid:3)

K
(cid:88)

H
(cid:88)

(cid:46)

(cid:34)(cid:115)

E

k=1

t=1

1
t , πk(sk

t )) ∨ 1

nk−1(sk

(cid:35)

| Fk−1

(cid:46) ˜O(

√

SAT + SAH),

(11)

where the second inequality is by Lemma 38. It is worth noting that Lemma 38 is proven by deﬁning
Lk, the set of ’good’ state-action pairs, that contains pairs that were visited sufﬁciently often in the
past [Dann et al., 2017, Zanette and Brunskill, 2019]. The term we bound is then analyzed separately
for state-action pairs inside and outside Lk. The deﬁnition of Lk can be found in Deﬁnition 2, and its
properties (including Lemma) are analyzed in Appendix F.1.

23

Furthermore, outside the failure event,

K
(cid:88)

H
(cid:88)

E (cid:2)(˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3)

k=1

t=1

=

K
(cid:88)

H
(cid:88)

k=1

t=1

E (cid:2)(˜pk−1 − ˆpk−1)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3)

+ E (cid:2)(ˆpk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:3)

K
(cid:88)

H
(cid:88)

(1)
≤

k=1

t=1

E (cid:2)(cid:107)(˜pk−1 − ˆpk−1)(· | sk

t , ak

t )(cid:107)1(cid:107) ¯V k−1

t+1 (cid:107)∞ | Fk−1

(cid:3)

+ E (cid:2)(cid:107)(ˆpk−1 − p)(· | sk

t , ak

t )(cid:107)(cid:107) ¯V k−1

t+1 (cid:107)∞ | Fk−1

(cid:3)

K
(cid:88)

H
(cid:88)

(2)
≤ H

E (cid:2)(cid:107)(ˆpk−1 − p)(· | sk

t , ak

t )(cid:107)1 + (cid:107)(˜pk−1 − ˆpk−1)(· | sk

t , ak

t )(cid:107)1 | Fk−1

k=1
√

S

(3)
(cid:46) H

t=1

(cid:34)(cid:115)

K
(cid:88)

H
(cid:88)

E

t=1

k=1
√

(4)
(cid:46) ˜O(HS

AT + H 2

√

SSA).

1
t , ak
nk−1(sk

t ) ∨ 1

(cid:35)

| Fk−1

(cid:3)

(12)

Relation (1) holds by Hölder’s inequality. Next, (2) holds since ∀s, t, k, 0 ≤ ¯V k
t (s) ≤ H. The lower
bounds holds by Lemma 14 and since V ∗
t ≥ 0. The upper bound is since the value can only decrease
by Algorithm 2 and the inequality holds for the initialized value. Finally, (3) holds outside failure
event F p (Lemma 13), and (4) holds by Lemma 38.

Combining (11), (12) we conclude that,

(B) ≤ ˜O(HS

√

AT + H 2

√

SSA)

Combining the bounds on (A) and (B) in (10) concludes the proof.

24

(cid:114)

9 , ∀s ∈ S, t ∈ [H], ¯V 0
2 ˆVar ˆpk−1(s,a)( ¯V k−1
nk−1(s,a)

t+1 ) ln 4SAT

δ(cid:48)

t (s) = H − (t − 1), V 0

t (s) = 0,
(cid:113)

ln 4SAT

,

δ(cid:48)

+

δ(cid:48)

2H ln 4SAT
3nk−1(s,a) , L = 2
2 ln 4SAT

(cid:113)

.

δ(cid:48)

2H ln 4SAT
3

δ(cid:48)

, Bv =

(cid:113)

2 ln 4SAT

δ(cid:48)

, Bp = H

E Proof of Theorem 9

Algorithm 5 EULER with Greedy Policies
1: Initialize: δ, δ(cid:48) = δ

2:

φ(s, a) =

J =

3:
4: for k = 1, 2, .. do
Initialize sk
5:
1
for t = 1, .., H do
6:
7:
8:

9:

10:

11:
12:
13:
14:
15:

16:

17:

18:

#Update Upper Bound on V ∗
for a ∈ A do
k(sk
br

2 ˆVar(R(sk

(cid:114)

δ(cid:48)

+

nk−1(sk

14 ln 4SAT

t , a) =

t , a) + br

t ,a)) ln 4SAT
t ,a)∨1
t , a), ¯V k−1
k(sk

δ(cid:48)
3nk−1(sk
t ,a)∨1
t+1 ) + 4J+Bp
nk−1(sk
t , a) + ˆpk−1(· | sk

t , a) = φ(ˆpk−1(· | sk
t , a) = ˆrk−1(sk

bpv
k (sk
¯Q(sk
end for
t ∈ arg maxa ¯Q(sk
ak
t ) = min(cid:8) ¯V k−1
¯V k
t (sk
#Update Lower Bound on V ∗
4J+Bp
bpv
t ), V k−1
t , ak
k (sk
t+1 ) +
t ,ak
t ) + ˆpk−1(· | sk
t , ak
t ) − br
t , ak
Q(sk
k(sk
(cid:111)
V k
(sk
t (sk
t , ak
t ), Q(sk
t )
#Act by the Greedy Policy
Apply ak
t+1.

t ) = φ(ˆpk−1(· | sk
t ) = ˆrk−1(sk
V k−1
t

t , ak
t , ak
t ) = max

t and observe sk

t , a)
(sk

t ), ¯Q(sk

t , ak

nk−1(sk

t )(cid:9)

(cid:110)

t

end for
Update ˆrk, ˆpk, nk with all experience gathered in episode.

19:
20:
21:
22:
23: end for

t ,a)∨1 +
t , a)T ¯V k−1

Bv(cid:107) ¯V k−1
√

t+1 (cid:107)2, ˆp

t+1 −V k−1
nk−1(sk
k (sk

t ,a)∨1
t , a)

t+1 + bpv

√

t )∨1 +
t , a)T V k−1

Bv(cid:107) ¯V k−1

t+1 −V k−1
t ,ak
nk−1(sk
t+1 − bpv
k (sk

t+1 (cid:107)2, ˆp
t )∨1
t , ak
t )

Remark 1. Note that the algorithm does not explicitly deﬁne ˜rk−1(s, a) and ˜pk−1(· | s, a). While we
can directly set ˜rk−1(s, a) = ˆrk−1(s, a) + br
t , a), the optimistic transition kernel is only implicitly
deﬁned as

k(sk

˜pk−1(· | s, a)T ¯V k−1

t

= ˆpk−1(· | s, a)T ¯V k−1

t+1 + bpv

k (s, a)

Nevertheless, throughout the proofs we are only interested in the above quantity, and thus, except for
some abuse of notation, all of the proofs hold. We use this notation since it is common in previous
works (e.g., Zanette and Brunskill 2019, Dann et al. 2017) and for brevity.

In this section, we provide the full proof of Theorem 9 which establishes a regret bound for EULER
with Greedy Policies (EULER-GP). In Zanette and Brunskill [2019] the authors prove their results
using a general conﬁdence interval, which they refer as admissible conﬁdence interval. In Section E.1
we state there deﬁnition and state some useful properties. In Section E.2 we deﬁne the set of
failure events and show that with high-probability the failure events do not occur. The set of failure
events includes high-probability events derived using empirical Bernstein inequalities [Maurer and
Pontil, 2009], as well as high probability events on Decreasing Bounded Process, as we establish
in Appendix A. In Section E.3 we analyze the optimism EULER-GP and prove it satisﬁes the same
optimism and pessimism as in Zanette and Brunskill [2019], outside the failure event for all s, t, k
V k

t (s) ≤ ¯V k

t (s).

t (s) ≤ V ∗

In Section E.4, using the above, we give the full proof of Theorem 9. As for the proof of UCRL2-GP,
we apply the new suggested regret decomposition, based on Lemma 7, and use the new results on
Decreasing Bounded Processes. In section E.5 we modify some results of [Zanette and Brunskill,
2019] to our setting, and utilize the new results to bound each term in the regret decomposition in
section E.6.

25

E.1 Properties of Conﬁdence Intervals

In this section, we cite the important properties of the conﬁdence intervals of EULER, as was stated
in [Zanette and Brunskill, 2019]. We start from their deﬁnition of an admissible conﬁdence interval:
Deﬁnition 1. A conﬁdence interval φ is called admissible for EULER if the following properties
hold:

1. φ(p, V ) takes the following functional form:

φ(p, V ) =

g(p, V )
(cid:112)nk−1(s, a) ∨ 1

+

j(p, V )
nk−1(s, a) ∨ 1

,

for some functions j(p, V ) ≤ J ∈ R, and

|g(p, V1) − g(p, V2)| ≤ Bv(cid:107)V1 − V2(cid:107)2,p

If the value function is uniform then:

g(p, α1) = 0,

∀α ∈ R.

2. With probability at least 1 − δ(cid:48) it holds that:

|(ˆpk−1(· | s, a) − p(· | s, a))T V ∗

t+1| ≤ φ(p(· | s, a), V ∗

t+1)

jointly for all timesteps t, episodes k, states s and actions a.

3. With probability at least 1 − δ(cid:48) it holds that:

(cid:12)
(cid:12)g(ˆpk−1(· | s, a), V ∗

t+1) − g(p(· | s, a), V ∗

t+1)(cid:12)

(cid:12) ≤

Bp
(cid:112)nk−1(s, a) ∨ 1

jointly for all episodes k, timesteps t, states s, actions a and some constant Bp that does not
depend on (cid:112)nk−1(s, a) ∨ 1.

An admissible conﬁdence interval enjoys many properties, which are summarized in the following
lemma:
Lemma 15. If φ is admissible for EULER, and under the events that properties 2,3 of Deﬁnition 1
hold, then:

1. For any V ∈ RS with (cid:107)V (cid:107)∞ ≤ H, it holds that |g(p, V )| ≤ BvH.
2. For any V ∈ RS,

|φ(ˆpk−1(· | s, a), V ) − φ(p(· | s, a), V ∗

t+1)| ≤

Bv(cid:107)V − V ∗
t+1(cid:107)2, ˆp
(cid:112)nk−1(s, a) ∨ 1

+

Bp + 4J
nk−1(s, a) ∨ 1

3. Let bpv

k be the transition bonus, which is deﬁned as

bpv
k (ˆpk−1(· | s, a), V1, V2) = φ(ˆpk−1(· | s, a), V1) +

Bp + 4J
nk−1(s, a) ∨ 1

+

Bv(cid:107)V2 − V1(cid:107)2, ˆp
(cid:112)nk−1(s, a) ∨ 1

.

For any V1, V2 ∈ RS such that V1 ≤ V ∗ ≤ V2 pointwise, it holds that
bpv
k (ˆpk−1(· | s, a), V1, V2) ≥ φ(p(· | s, a), V ∗)
bpv
k (ˆpk−1(· | s, a), V2, V1) ≥ φ(p(· | s, a), V ∗)

Proof. The ﬁrst property is due to Corollary 1.3 of [Zanette and Brunskill, 2019], and the second one
is Lemma 4 of their paper. The third property is equivalent to Proposition 3 in [Zanette and Brunskill,
2019], but since we allow general value functions V1, V2, we write the full proof for completeness.

26

we start by proving that if V1 ≤ V ∗ ≤ V2, then for any transition probability vector p,

(cid:107)V2 − V ∗(cid:107)2,p ≤ (cid:107)V2 − V1(cid:107)2,p
(cid:107)V1 − V ∗(cid:107)2,p ≤ (cid:107)V2 − V1(cid:107)2,p

(13)

To this end, notice that ∀s

and since all of the quantities are non-negative, it also holds that

0 ≤ V2(s) − V ∗(s) ≤ V2(s) − V1(s) ,

0 ≤ (V2(s) − V ∗(s))2 ≤ (V2(s) − V1(s))2 .

The inequality holds pointwise, and therefore holds for any linear combination with non-negative
constants:

(cid:88)

0 ≤

s

p(s)(V2(s) − V ∗(s))2 ≤

p(s)(V2(s) − V1(s))2 .

(cid:88)

s

Taking the root of this inequality yields Inequality (13). Substituting in the deﬁnition of
bpv
k (ˆpk−1(· | s, a), V2, V1) yields:

bpv
k (ˆpk−1(· | s, a), V2, V1) = φ(ˆpk−1(· | s, a), V2) +

≥ φ(ˆpk−1(· | s, a), V2) +

≥ φ(p(· | s, a), V ∗)

Bp + 4J
nk−1(s, a) ∨ 1

Bp + 4J
nk−1(s, a) ∨ 1

+

+

Bv(cid:107)V2 − V1(cid:107)2, ˆp
(cid:112)nk−1(s, a) ∨ 1
Bv(cid:107)V2 − V ∗(cid:107)2, ˆp
(cid:112)nk−1(s, a) ∨ 1

The ﬁrst inequality is due to (13), and the second is due to the second part of the Lemma. The result
for bpv

k (ˆpk−1(· | s, a), V1, V2) can be proven similarly, and thus omitted.

Another property that will be useful throughout the proof is the following upper bound on
bpv
k (ˆpk−1(· | s, a), V1, V2)
Lemma 16. For any V1, V2 such that for all s, V1(s), V2(s) ∈ [0, H]

bpv
k (ˆpk−1(· | s, a), V2, V1) ≤

2BvH + 5J + Bp
(cid:112)nk−1(s, a) ∨ 1

,

Proof. We bound bpv

k (ˆpk−1(· | s, a), V2, V1) as follows:

bpv
k (ˆpk−1(· | s, a), V2, V1) = φ(ˆpk−1(· | s, a), V2) +

(1)
≤

(2)
≤

(3)
≤

+

+

g(p, V )
(cid:112)nk−1(s, a) ∨ 1
BvH
(cid:112)nk−1(s, a) ∨ 1
2BvH + 5J + Bp
(cid:112)nk−1(s, a) ∨ 1

j(p, V )
nk−1(s, a) ∨ 1

J
nk−1(s, a) ∨ 1

+

+

Bp + 4J
nk−1(s, a) ∨ 1

Bp + 4J
nk−1(s, a) ∨ 1

+

+

Bp + 4J
nk−1(s, a) ∨ 1

+

Bv(cid:107)V2 − V1(cid:107)2, ˆp
(cid:112)nk−1(s, a) ∨ 1
BvH
(cid:112)nk−1(s, a) ∨ 1
BvH
(cid:112)nk−1(s, a) ∨ 1

In (1), we substituted φ and bounded (cid:107)V2 − V1(cid:107)2, ˆp ≤ H. (2) is by Lemma 15 and Deﬁnition 1, and
(3) is by noting that n ≥

n for n ≥ 1.

√

27

We end this section by stating that Bernstein’s inequality induces an admissible φ. The proof can be
found in [Zanette and Brunskill, 2019], Proposition 2.
Lemma 17. Bernstein inequality induces an admissible conﬁdence interval with g(p, V ) =
(cid:113)

2Vars(cid:48)∼p(·|s,a)V ln 2SAT

δ(cid:48) and j(p, V ) = 2H ln 2SAT
δ(cid:48)

, or explicitly:

(cid:115)

φ(p(· | s, a), V ) =

2Vars(cid:48)∼p(·|s,a)V ln 2SAT
nk−1(s, a) ∨ 1

δ(cid:48)

+

2H ln 2SAT
3nk−1(s, a) ∨ 1

δ(cid:48)

(cid:113)

with the constants J =

= ˜O(1) and Bp =
δ(cid:48) = ˜O(H). Using lemma 16, it also implies that for any V1, V2 such that for all
k (ˆpk−1(· | s, a), V2, V1) (cid:46) ˜O(H).

H
s, V1(s), V2(s) ∈ [0, H], it holds that bpv

= ˜O(H), Bv =

2 ln 2SAT

2 ln 2SAT

δ(cid:48)

2H ln 2SAT
3

δ(cid:48)

(cid:113)

E.2 Failure Events

E.2.1 Failure Events of EULER

We start by recalling the failure events as stated in [Zanette and Brunskill, 2019], Appendix D. These
events are high probability bounds that are based on the Empirical Bernstein Inequality [Maurer and
Pontil, 2009] and leads to the bonus terms of the algorithm. Importantly, these events depend on the
state-action visitation counter, and, thus, are indifferent to the greedy exploration scheme which we
consider.

Deﬁne the following failure events.




F r =

∃s, a, k : |r(s, a) − ˆrk−1(s, a)| ≥

(cid:115)

2 ˆVark−1R(s, a) ln 4SAT
nk−1(s, a) ∨ 1

δ(cid:48)

+

14 ln 4SAT
3(nk−1(s, a) ∨ 1)

δ(cid:48)






∃s, a, k :

(cid:113)

ˆVark−1 R(s, a) − (cid:112)Var R(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:115)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

4 ln 2SAT
nk−1(s, a) ∨ 1

δ(cid:48)










F vr =





F pv =







F pn1 =






∃s, a, t, k :





F pv2 =

∃s, a, t, k :

(cid:12)
(cid:12)(ˆpk−1(· | s, a) − p(· | s, a))T V ∗
(cid:12)

t+1

(cid:12)
(cid:12)
(cid:12) ≥

(cid:115)

2Vars(cid:48)∼p(·|s,a)V ∗

t+1 ln 4SAT

δ(cid:48)

nk−1(s, a) ∨ 1

+

2H ln 2SAT
3(nk−1(s, a) ∨ 1)

δ(cid:48)






(cid:12)
(cid:12)(cid:107)V ∗
(cid:12)

t (cid:107)2, ˆp − (cid:107)V ∗

t (cid:107)2,p

(cid:115)

(cid:12)
(cid:12)
(cid:12) ≥ H

4 ln 2SAT
nk−1(s, a) ∨ 1

δ(cid:48)






F ps =

∃s, s(cid:48), a, k : |ˆpk−1(s(cid:48) | s, a) − pk−1(s(cid:48) | s, a)| ≥

(cid:115)

p(s(cid:48) | s, a)(1 − p(s(cid:48) | s, a)) ln 2T S2A
nk−1(s, a) ∨ 1

δ(cid:48)

+

2 ln 2T S2A
3(nk−1(s, a) ∨ 1)

δ(cid:48)






∃s, a, k : (cid:107)ˆpk−1(· | s, a) − p(· | s, a)(cid:107)1 ≥

(cid:115)

4S ln 3SAT
nk−1(s, a) ∨ 1

δ(cid:48)






F N

k =






∃s, a, k : nk−1(s, a) ≤

1
2

(cid:88)

j<k

wj(s, a) − H ln

SAH
δ(cid:48)






.

where wj(s, a) := (cid:80)H
events hold individually with probability at most δ(cid:48).

t=1 wtj(s, a). In [Zanette and Brunskill, 2019], Appendix D, it is shown these

28

E.2.2 Failure Events of Decreasing Bounded Processes

In this section, we add another failure events to the total set of failure events. This set of failure event
is not present in previous analysis of regret in optimistic RL algorithms (e.g., in Azar et al. 2017,
Dann et al. 2017, 2018, Zanette and Brunskill 2019).

We deﬁne the following failure events.

(cid:40)

F vDP =

∃K ≥ 0 :

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1] ≥ 9SH 2 ln

(cid:41)

3SH
δ(cid:48)

(cid:40)

F vsDP =

∃K ≥ 0 :

K
(cid:88)

H
(cid:88)

(cid:88)

( ¯V k−1
t

(s) − V k−1

t

k=1

t=1

s

(s))2 − E[( ¯V k

t (s) − V k

t (s))2 | Fk−1] ≥ 9SH 3 ln

(cid:41)

3SH
δ(cid:48)

In this section, we prove that both of these failure events occur with low probability δ(cid:48).
We start by proving that (cid:8) ¯V k

t (s)(cid:9) is a decreasing processes, independently to the previously deﬁned
(cid:111)2
starts as a decreasing process and

(cid:110) ¯V k

t (s) − V k

t (s)

failure events. We continue and prove that
then becomes and increasing process.
Lemma 18. The following claims hold.

1. For every s, t, (cid:8) ¯V k

t (s)(cid:9)
k is a decreasing process and is bounded by [0, H − (t − 1)].
(cid:111)

is an increasing process and is bounded by [0, H − (t − 1)].

(cid:110)

2. For every s, t,

k

V k
t (s)
(cid:26)(cid:16) ¯V k

t (s) − V k

t (s)

(cid:17)2(cid:27)

3. For every s, t,

starts as a decreasing process bounded by

[0, (H − (t − 1)2)] and then, possibly, becomes an increasing process.

k

t (s) = H − (t − 1). By construction of the update rule ¯V k

Proof. We start by proving the ﬁrst claim.The following holds. By the initialization of the algorithm
∀s, t, ¯V 0
t (s) can only decrease (see
Line 14).
We now prove that for every s, t, k, (cid:8) ¯V k
k is bounded from below by 0. By assumption
r(s, a) ∈ [0, 1], and thus ˆrk−1(s, a) ≥ 0 a.s. . By induction, this implies ¯V k−1
≥ 0. The base case
t
holds by initialization, and the induction step by the fact ˆrk−1 ≥ 0 and that the bonus terms are
positive.

t (s)(cid:9)

Proving the second claim is done with similar argument, while using ˆrk−1(s, a) ≤ 1 a.s.. By the
is an Increasing Bounded Process in [0, H − (t − 1)] (similar
update rule (see Line 18),
deﬁnition as in 1 with opposite inequality).

t (s)

V k

(cid:110)

(cid:111)

k

To prove the third claim we combine the two claims. Thus,

(cid:26)(cid:16) ¯V k

t (s) − V k

t (s)

(cid:17)2(cid:27)

starts as a

decreasing process. Then, if the upper and lower value function crosses one another, the process
becomes an increasing process.

k

Remark 2. Notice that the upper bound and lower bound of the optimal value crosses one another
only inside the failure events deﬁned in Section E.2.1). Yet, the analysis in the following will be
indifferent to whether the failure event takes place or not.
Lemma 19. Pr(cid:8)F vDP (cid:9) ≤ δ(cid:48).

Proof. We wish to bound

Pr{∃K ≥ 0 :

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

¯V k−1
h

(s) − E[ ¯V k

h (s) | Fk−1] ≥ 9SH 2 ln

3SH
δ(cid:48) }.

According to Lemma 18, for every s, t, (cid:8) ¯V k
t (s)(cid:9)
(Appendix A) which bounds the sum of Decreasing Bounded Processes we conclude the proof.

k≥1 is a decreasing process. Applying Lemma 11

29

Lemma 20. Pr(cid:8)F vsDP (cid:9) ≤ δ(cid:48).

Proof. We wish to bound

(cid:40)

Pr

∃K ≥ 0 :

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

( ¯V k−1
t

(s) − V k−1

t

(s))2 − E[( ¯V k

t (s) − V k

t (s))2 | Fk−1] ≥ 9SH 3 ln

(cid:41)
.

3SH
δ(cid:48)

Consider a ﬁxed s, t. Furthermore, deﬁne the following event

Ak−1 =

(cid:110) ¯V k−1

t

(s) > V k−1

t

(cid:111)
.

(s)

We have that

K
(cid:88)

( ¯V k−1
t

(s) − V k−1

t

k=1

(s))2 − E[( ¯V k

t (s) − V k

t (s))2 | Fk−1]

≤

=

=

K
(cid:88)

(cid:16)

k=1

( ¯V k−1
t

(s) − V k−1

t

(s))2 − E[( ¯V k

t (s) − V k

(cid:17)
t (s))2 | Fk−1]

1{Ak−1}

K
(cid:88)

( ¯V k−1
t

(s) − V k−1

t

k=1

K
(cid:88)

( ¯V k−1
t

(s) − V k−1

t

(s))21{Ak−1} − E[( ¯V k

t (s) − V k

t (s))21{Ak−1} | Fk−1]

(s))21{Ak−1} − E[( ¯V k

t (s) − V k

t (s))21{Ak} | Fk−1]

k=1

−

K
(cid:88)

k=1

E[( ¯V k

t (s) − V k

t (s))2(1{Ak−1} − 1{Ak}) | Fk−1]

K
(cid:88)

≤

( ¯V k−1
t

(s) − V k−1

t

k=1

(s))21{Ak−1} − E[( ¯V k

t (s) − V k

t (s))21{Ak} | Fk−1].

The ﬁrst relation holds by deﬁnition, if the event Ak−1 is false then the term is negative, since
the process becomes increasing, and only decreases the sum. The second relation holds since
1{Ak−1} is FK−1 measureable. The forth relation holds since ( ¯V k−1
(s))2 ≥ 0 and
(1{Ak−1} − 1{Ak}) ≥ 0. Where the latter holds since 1{Ak} = 1 → 1{Ak−1} = 1, i.e.,

(s) − V k−1

t

t

¯V k
t (s) > V k
Differently put, if at the kth episode ¯V k
t (s − 1), as the process (cid:8) ¯V k
¯V k−1
t

(s) > V k

(s) > V k−1

t (s) → ¯V k−1
t
t (s) > V k
t (s) then it also holds for the k − 1th episode,
t (s)(cid:9)
t (s)
is decreasing by

k≥0 is increasing and

(s).

V k

(cid:111)

(cid:110)

t

k≥0

Lemma 18.

Furthermore, by Lemma 18,

(cid:110)

( ¯V k

t (s) − V k

(cid:111)
t (s))21{Ak}

[0, H 2]. Initially, it decreases since 1{Ak}k = 1 and
Furthermore, when 1{Ak} = 0 it cannot increase. Lastly, ( ¯V 0
Applying Theorem 3 we get that for a ﬁxed s, t, with probability δ(cid:48)
SH

t (s) − V 0

(cid:110)

k
( ¯V k
t (s) − V k

is a Decreasing Bounded Process in

t (s))2(cid:111)
t (s))21{A0} ≤ H 2.

is initially decreasing.

K
(cid:88)

k=1

( ¯V k−1
t

(s) − V k

t (s))2 − E[( ¯V k

t (s) − V k−1

t

(s))2 | Fk−1] ≥ 9SH 3 ln

3SH
δ(cid:48)

.

By applying Lemma 11 (Appendix A), which extends this bound to the sum on s, t we conclude the
proof.

Lemma 21. (All Failure Events) If δ(cid:48) = δ

9 , then

F := F r (cid:91)

F pv2 (cid:91)
holds with probability at most δ. If the event F does not hold we say the algorithm is outside the
failure event.

F vDP (cid:91)

F pn1 (cid:91)

F pv (cid:91)

F vr (cid:91)

F pr (cid:91)

F ps (cid:91)

F vsDP

30

Proof. Applying a union bound on all events, which hold individually with probability at most δ(cid:48)
yield the result.

E.3 EULER with Greedy Policies is Optimistic

Our algorithm modiﬁes the exploration bonus of [Zanette and Brunskill, 2019] by using ¯Vk−1, V k−1
instead of ¯Vk, V k, and uses the following bonus (with some abuse of notation):

k (s, a) = bpv
bpv

k

(cid:0)ˆpk−1(· | s, a), ¯Vk−1, V k−1

(cid:1) .

We now show that the modiﬁed bonus retains the optimism of the algorithm:
Lemma 22. Outside the failure event of the estimation (see Lemma 21), if the conﬁdence interval is
admissible, then the relation

t ≤ V ∗
holds pointwise for all timesteps t and episodes k.

V k−1

t ≤ ¯V k−1

t

Proof. We follow the proof of [Zanette and Brunskill, 2019], Proposition 4, and prove by induction.
We ﬁrst prove that for all k, V ∗

t ≤ ¯V k
t .

The claim trivially holds for k = 0, due to the initialization of the value. Suppose that the result holds
for any state s and timestep t in the k − 1th episode. If

ˆrk−1(sk

t , ak

t ) + br

k−1(sk

t , ak

t ) + ˆpk−1(· | s, ak

t )T ¯V k−1

t+1 + bpv

k−1(sk

t , ak

t ) ≥ ¯V k−1

t

(st) ,

then by the induction’s assumption we are done. Otherwise, denote the optimal action in the real
MDP at state sk

t . The value is updated as follows:

t by a∗

¯Vk(sk

t ) = ˆrk−1(sk
≥ ˆrk−1(sk
t , a∗
≥ r(sk

k−1(sk
k−1(sk

t ) + br
t ) + br

t , ak
t , ak
t a∗
t , a∗
t ) + ˆpk−1(· | s, a∗

t ) + ˆpk−1(· | s, ak
t ) + ˆpk−1(· | s, a∗
t )T ¯V k−1
t+1 + bpv
k−1(sk

t )T ¯V k−1
t+1 + bpv
t )T ¯V k−1
t+1 + bpv
t , a∗
t )

k−1(sk
k−1(sk

t , ak
t )
t , a∗
t )

The ﬁrst inequality is since ak
t is the action that maximizes the greedy value and the second inequality
is due to the optimism of the reward when the reward bonus is added, outside the failure events
(Lemma 21). Next, using the inductive hypothesis (V ∗

t+1 element-wise), we get

t+1 ≤ ¯V k−1

¯V k
t (sk

t ) ≥ r(sk

t , a∗

t ) + ˆpk−1(· | s, a∗

t )T V ∗

t+1 + bpv

k−1(sk

t , a∗
t )

We now apply Lemma 15, which implies that

bpv
k−1(sk

t , a∗

t ) ≥ φ(p(· | sk

t , a∗

t ), V ∗) ,

and thus

¯V k
t (sk

t ) ≥ r(sk

t , a∗

t ) + ˆpk−1(· | s, a∗

t )T V ∗

t+1 + φ(p(· | sk

t , a∗

t ), v∗)

Finally, since φ is admissible, we get the desired result from property (2) of Deﬁnition 1:

¯V k
t (sk

t ) ≥ r(sk

t , a∗

t ) + p(· | s, a∗

t )T V ∗

t+1 = V ∗

t+1(sk
t )

The proof for V k−1

t ≤ V ∗
t

is almost identical, and thus omitted from this paper.

31

E.4 Proof of Theorem 9

Proof. Throughout the proof, we assume that we are outside the failure events that were deﬁned in
Section E.2, which happens with probability of at least 1 − δ (Lemma 21). Speciﬁcally, it implies
that the value function is optimistic, namely V ∗
1 (s) (Lemma 22), and we can bound the
regret by,

1 (s) ≤ V k

Regret(K) =

K
(cid:88)

k=1

1 (sk
V ∗

1) − V πk

1 (sk

1) ≤

K
(cid:88)

k=1

¯V k−1
1

Next, by applying Lemma 7 the following bound holds,

(sk

1) − V πk

1 (sk

1).

K
(cid:88)

H
(cid:88)

≤

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1]

t=1

k=1
(cid:124)

K
(cid:88)

H
(cid:88)

+

t=1

k=1
(cid:124)

(cid:123)(cid:122)
(A)

(cid:125)

E(cid:2)(˜rk−1 − r)(sk

t , ak

t ) + (˜pk−1 − p)(· | sk

t , ak

t )T ¯V k−1

t+1 | Fk−1

(cid:123)(cid:122)
(B)

(14)

(cid:3)
.

(cid:125)

The regret is thus upper bounded by two terms. The ﬁrst term (A) also appears in the analysis of
RTDP (Theorem 4). Speciﬁcally, by Lemma 34 (Appendix F), we can express this term as a sum of
SH Decreasing Bounded Process in [0, H]:

(A) =

(cid:88)

H
(cid:88)

K
(cid:88)

s

t=1

k=1

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1].

Bounding (A). Outside failure event F vDP , this term is bounded by 9SH 2 ln 3SH
δ(cid:48)

. Thus,

(A) (cid:46) ˜O(SH 2)

Bounding (B). The term (B) is almost the same term that is bounded in [Zanette and Brunskill, 2019],
and its presence is common in recent literature on exploration in RL (e.g., Dann et al. 2017, 2018,
Zanette and Brunskill 2019). The only difference between (B) and the term bounded in [Zanette
and Brunskill, 2019] is the presence of ¯V k−1, the value before the update, instead of ¯V k, the value
after applying the update rule. This is since existing algorithms perform planning from the end of an
episode and backwards. Thus, when choosing an action at some timestep t, these algorithms have
access to the updated value of step t + 1. In contrast, we avoid the planning stage, and therefore must
rely on the previous value ¯V k−1. We will later see that we can overcome this without affecting the
regret.

Next, let Lk be the set of ’good’ state-action pairs, which is deﬁned in Deﬁnition 2 and analyzed
thoroughly in Appendix F.1. We now decompose the sum of (B) to state actions in and outside Lk.
We also note that except for the sk
t , all of the variables in (B) are Fk−1 measurable, which allows
us to explicitly write the conditional expectation using wtk(s, a), as follows:

t , ak

32

K
(cid:88)

H
(cid:88)

k=1

t=1

K
(cid:88)

H
(cid:88)

(B) =

=

(cid:16)

wtk(s, a)

(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1
t+1

(cid:17)

(cid:88)

wtk(s, a)

(cid:16)
(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1
t+1

(cid:17)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+

k=1

t=1

(s,a) /∈Lk

(cid:16)

wtk(s, a)

(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1
t+1

(cid:17)

K
(cid:88)

H
(cid:88)

(1)
(cid:46)

(cid:88)

(cid:16)

wtk(s, a)

(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1
t+1

(cid:17)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+ H

k=1

t=1

(s,a) /∈Lk

wtk(s, a)

K
(cid:88)

H
(cid:88)

(2)
(cid:46)

(cid:88)

(cid:16)

wtk(s, a)

(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1
t+1

(cid:17)

+ ˜O(SAH 2)

k=1

t=1

(s,a)∈Lk

For (1), we bound (˜rk−1 − r)(s, a) ≤ ˜rk−1(s, a) and (˜pk−1 − p)(· | s, a)T ¯V k−1
s, a)T ¯V k−1
reward ˜rk−1(s, a) is ˜O(1). Due to Lemma 22, the optimistic value ¯V k−1
s, a)T ≤ H. The transition bonus is bpv
second term is ˜O(H). Together, both terms are ˜O(H). (2) is due to Lemma 36 of Appendix F.1.

t+1 ≤ ˜pk−1(· |
t+1 . The estimated reward is in [0, 1] and it’s bonus is at most ˜O(1), and thus the optimistic
t+1 ≤ H, and thus ˆpk−1(· |
k (s, a) = ˜O(H) due to Lemma 17, which implies that the

As in [Zanette and Brunskill, 2019], we continue the decomposition of the remaining term by adding
and subtracting cross-terms that depends on ˆpk−1(· | s, a)

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(cid:0)(˜rk−1 − r)(s, a) + (˜pk−1 − p)(· | s, a)T ¯V k−1

t+1

(cid:1)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

=

k=1

t=1

(s,a)∈Lk

wtk(s, a)(˜rk−1 − r)(s, a)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(1)

+ wtk(s, a)(˜pk−1 − ˆpk−1)T (· | s, a) ¯V k−1
t+1
(cid:123)(cid:122)
(cid:125)
(2)

(cid:124)

+ wtk(s, a)(ˆpk−1 − p)(· | s, a)T V ∗

+ wtk(s, a)(ˆpk−1 − p)(· | s, a)T ( ¯V k−1

t+1 − V ∗

.

(15)

(cid:124)

(cid:123)(cid:122)
(3)

t+1
(cid:125)

(cid:124)

(cid:123)(cid:122)
(4)

t+1)
(cid:125)

Recall that we use Bernstein’s inequality as the admissible conﬁdence interval. Thus, by Lemma 26,
(cid:113)
δ(cid:48) =

= ˜O(H), Bv =
it holds that J =
˜O(H). Also let F, D be the constants deﬁned in Lemma 23, and speciﬁcally

δ(cid:48) = ˜O(1) and Bp = H

2H ln 2SAT
3

2 ln 2SAT

2 ln 2SAT

(cid:113)

δ(cid:48)

√

F := 2L + LH

S + 6BvH = ˜O
D := 18J + 4Bp + 4L2 = ˜O(H)

(cid:16)

√

(cid:17)

S

H

Substituting these constants, terms (1) − (4) are bounded in Lemmas 30, 33, 31 and 32 respectively
as follows:

(1) (cid:46) (cid:112)C∗
(2) (cid:46) min

rSAT + SA
(cid:110)√

C∗SAT + S

√

SAH 2 + SAH

5
2 ,

√

CπSAT + S

√

(cid:111)

5
2

SAH

(3) (cid:46) min

(cid:110)√

C∗SAT + SAH,

√

CπSAT + S

√

(cid:111)

5
2

SAH

(4) (cid:46) S2AH 2 + S

√

SAH

5
2

33

Thus, term (B) of the regret is bounded by:

(cid:110)√

(B) (cid:46) min

C∗SAT ,
(cid:46) (cid:112)min{C∗ + C∗

√

(cid:111)

CπSAT

+ (cid:112)C∗

r, Cπ + C∗

r}SAT + S

rSAT + S2AH 2 + S
SAH 2(cid:16)√
√
√

S +

H

√

5
2

SAH
(cid:17)

Finally, using Lemma 28, we can bound this term by

(cid:115)

(B) (cid:46)

min

(cid:26)

Q∗,

(cid:27)

G2
H

SAT + S

√

SAH 2(cid:16)√

S +

√

(cid:17)

H

and noticing that (A) is negligible compared to (B), we get

(cid:115)

Regret(K) (cid:46)

min

(cid:26)

Q∗,

(cid:27)

G2
H

SAT + S

√

SAH 2(cid:16)√

S +

√

(cid:17)

H

To derive the problem independent bound, we use the fact that the maximal reward in a trajectory is
bounded by G ≤ H, which yields

Regret(K) (cid:46)

√

HSAT + S

√

SAH 2(cid:16)√

S +

√

(cid:17)

H

E.5 Cumulative Squared Value Difference

In this section, we aim to bound the expected cumulative squared value difference. Speciﬁcally, we
are interested in a bound for the following quantities:

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2

.

wtk(s, a)p(· | s, a)T (cid:0) ¯V k−1

t+1 − V πk

t+1

(cid:1)2

(16)

(17)

The ﬁrst quantity allows us to replace Lemma 12 [Zanette and Brunskill, 2019], and the second
allows us to prove Lemma 14 of the same paper. Together, they enable us to use the same analysis
of [Zanette and Brunskill, 2019]. The ﬁnal results are stated in Lemmas 26 and 27 by the end of
this section. Most of the section will focus on bounding (16), which requires a much more delicate
analysis than the bound of [Zanette and Brunskill, 2019].

t+1 − V k−1
In order to bound (16), we start by bounding
t+1
corresponds to Proposition 5 of [Zanette and Brunskill, 2019]:

(cid:16) ¯V k−1

(cid:17)2

Lemma 23. Outside the failure event, the following holds:

in the following lemma, which

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ E[ ¯V k−1

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1) | Fk−1, sk

t ] + min

(cid:40)

F + D
t , ak

(cid:112)nk−1(sk

t ) ∨ 1

(cid:41)

, H

,

where F := 2L + LH

√

in Deﬁnition 1 and L := 2

(cid:113)

ln 4SAT

δ(cid:48)

.

S + 6BvH, D := 18J + 4Bp + 4L2, the constants J, Bv, Bp are deﬁned

Proof. The proof is similar to [Zanette and Brunskill, 2019] Proposition 5, which is presented here
with the needed adaptation.

34

If the state sk
updated according to the update rule. Thus,

t is encountered in the kth episode at the tth time-step, then ¯V k

t (sk

t ), V k

t (sk

t ) will be

¯V k
t (sk
t (sk
V k

t ) ≤ ˆrk−1(sk
t ) ≥ ˆrk−1(sk

t , ak
t , ak

t ) + br
t ) − br

k−1(sk
k−1(sk

t , ak
t , ak

t ) + ˆpk−1(· | sk
t ) + ˆpk−1(· | sk

t , ak
t , ak

t )T ¯V k−1
t )T V k−1

t+1 + bpv
t+1 − bpv

k (ˆpk−1(· | sk
k (ˆpk−1(· | sk

t , ak
t , ak

t ), ¯V k−1
t ), V k−1

t+1 , V k−1
t+1 )
t+1 , ¯V k−1
t+1 ).

Subtraction yields:

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ 2br

k−1(sk
t , ak
k (ˆpk−1(· | sk

t ) + ˆpk−1(· | sk
t ), ¯V k−1
t , ak

t , ak
t+1 , V k−1

t )T ( ¯V k−1
t+1 ) + bpv

t+1 − V k−1
t+1 )
k (ˆpk−1(· | sk

+ bpv

t , ak

t ), V k−1

t+1 , ¯V k−1
t+1 ).

Next, we substitute the deﬁnition of the conﬁdence bonus, which yields

¯V k
t (sk

t ) − V k

t (sk

t ) ≤2br

k−1(sk

t , ak

t ) + ˆpk−1(· | sk

t , a)T ( ¯V k−1

+ φ(ˆpk−1(· | sk

t , ak

t ), ¯V k−1

t+1 ) +

+ φ(ˆpk−1(· | sk

t , ak

t ), V k−1

t+1 ) +

t+1 − V k−1
t+1 )
4J + Bp
t , ak

t ) ∨ 1

nk−1(sk

4J + Bp
t , ak

nk−1(sk

t ) ∨ 1

+

+

Bv(cid:107) ¯V k−1
(cid:112)nk−1(sk
Bv(cid:107) ¯V k−1
(cid:112)nk−1(sk

t+1 − V k−1
t , ak
t+1 − V k−1
t , ak

t+1 (cid:107)2, ˆp
t ) ∨ 1
t+1 (cid:107)2, ˆp
t ) ∨ 1

.

Using Lemma 15, property (2), and Inequalities (13), we get,

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ 2br

k−1(sk

t , ak

t ) + ˆpk−1(· | sk

+ 2φ(p(· | sk

t , ak

t ), V ∗

t+1) + 4

= 2br

+ 2

t ) + p(· | sk
t , ak
k−1(sk
t , ak
+ (ˆpk−1(· | sk
t , ak
g(p(· | sk
(cid:112)nk−1(sk
(cid:32)

t ) − p(· | sk
t ), V ∗
t+1)
t , ak
t ) ∨ 1
4J + Bp
t , ak

nk−1(sk

t ) ∨ 1

+ 4

+

+ 2

t , a)T ( ¯V k−1
t , ak

+

(cid:32)

t ) ∨ 1

t , a)T ( ¯V k−1

t+1 − V k−1
t+1 )
4J + Bp
nk−1(sk
t , ak
t+1 − V k−1
t+1 )
t ))T ( ¯V k−1
t+1 − V k−1
t+1 )
J
nk−1(sk
t , ak
t ) ∨ 1
Bv(cid:107) ¯V k−1
t+1 − V k−1
t+1 (cid:107)2, ˆp
(cid:112)nk−1(sk
t , ak
t ) ∨ 1

(cid:33)
,

Bv(cid:107) ¯V k−1
(cid:112)nk−1(sk

t+1 − V k−1
t , ak

t+1 (cid:107)2, ˆp
t ) ∨ 1

(cid:33)

where in the last relation we substituted φ and added and subtracted p(· | sk

t , a)T ( ¯V k−1
(cid:13)
(cid:13)
¯V k−1
t+1 − V k−1
(cid:13)
(cid:13)
(cid:13) ≤ H, which also
(cid:13)
t+1 (cid:107)2, ˆp ≤ H. In addition, using Hölder’s inequality, and outside failure

t+1 ∈ [0, H]. Thus,

t+1 − V k−1
t+1 ).

t+1 , V k−1

t+1

By Lemma 18, we know that ¯V k−1
implies that (cid:107) ¯V k−1
event F pn1, we can bound

t+1 − V k−1

(ˆpk−1(· | sk

t , ak

t , ak

t ) − p(· | sk
≤ (cid:13)

(cid:13)ˆpk−1(· | sk

t ))T ( ¯V k−1
t , ak

t+1 − V k−1
t+1 )
t , ak
t ) − p(· | sk
(cid:115)

(cid:115)

≤ H

4S ln 2SAT
nk−1(s, a) ∨ 1

δ(cid:48)

= LH

S
t , ak
nk−1(sk

t ) ∨ 1

t )(cid:13)
(cid:13)1

(cid:13)
¯V k−1
t+1 − V k−1
(cid:13)
(cid:13)

t+1

(cid:13)
(cid:13)
(cid:13)∞

Substituting both of these bounds, we get

35

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ 2br

k−1(sk

t , ak

t ) + p(· | sk

t , a)T ( ¯V k−1

t+1 − V k−1

t+1 ) + LH

(cid:115)

S
t , ak
nk−1(sk

t ) ∨ 1

+ 2

+ 4

t+1)
t ) ∨ 1

t , ak
g(p(· | sk
(cid:112)nk−1(sk
(cid:32)

t ), V ∗
t , ak
4J + Bp
t , ak

nk−1(sk

t ) ∨ 1

+

+ 2

J
t , ak
nk−1(sk

t ) ∨ 1

BvH
(cid:112)nk−1(sk

t , ak

t ) ∨ 1

(cid:33)

(18)

We now bound the remaining terms. First, using Lemma 15, property (1), we can bound g(p, V ∗
BvH. Second, notice that
t , a))T ( ¯V k−1

t , a))T ( ¯V k−1

t+1) − V k−1

t+1 − V k−1

t+1 | sk

t+1 (sk

t+1 (sk

t+1 ) =

p(· | sk

t+1))

p(sk

(cid:88)

t+1) ≤

sk
t+1

= E

(cid:104) ¯V k−1

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1) | Fk−1, sk
t

(cid:105)

.

Finally, outside failure event F r, the reward bonus can be bounded by

k(sk
br

t , ak

t ) =

≤

(cid:115)

2 ˆVar(R(sk
nk−1(sk
L
(cid:112)nk−1(sk
t , ak

t , ak
t , ak

t )) ln 4SAT
δ(cid:48)
t ) ∨ 1

+

14 ln 4SAT
δ(cid:48)
t , ak
t ) ∨ 1

3nk−1(sk

+

2L2
t , ak

nk−1(sk

t ) ∨ 1

t ) ∨ 1

where we used the fact that for variables in [0, 1], ˆVar(R(sk

t , ak

t )) ≤ 1.

Putting it all together in (18), we get

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ E

(cid:104) ¯V k−1

t+1 (sk

t+1) − V k−1
√

t+1 (sk

t+1) | Fk−1, sk
t

(cid:105)

+

2L + LH
(cid:112)nk−1(sk
t+1 (sk

S + 6BvH
t , ak
t ) ∨ 1
t+1) − V k−1

(cid:104) ¯V k−1

t+1 (sk

+

18J + 4Bp + 4L2
t , ak
nk−1(sk
t ) ∨ 1
(cid:105)

t+1) | Fk−1, sk
t

+

≤ E

F + D
t , ak

nk−1(sk

t ) ∨ 1

√

n ≤ n for n ≥ 1.

t (s) ≤ ¯V k

t (s), and the ﬁrst term is therefore

where in the last relation we substituted F and D and used
To ﬁnalize the proof note that outside the failure event, V k
positive. combined with ¯V k
t (sk
(cid:40)

t ) ≤ H yields

t ) − V k

t (sk

¯V k
t (sk

t ) − V k

t (sk

t ) ≤ min

(cid:104) ¯V k−1
E

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1) | Fk−1, sk
t

≤ E

(cid:104) ¯V k−1

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1) | Fk−1, sk
t

(cid:40)

(cid:105)

+ min

F + D
t , ak

(cid:112)nk−1(sk

t ) ∨ 1

(cid:105)

+

F + D
t , ak

(cid:112)nk−1(sk

t ) ∨ 1

, H

(cid:41)

(cid:41)

, H

.

Remark 3. See that the ﬁrst term in Equation Lemma 23 does not appear in the analysis of [Zanette
and Brunskill, 2019]. Its existence is a direct consequence of the fact we use 1-step greedy policies,
and not solving the approximate model at the beginning of each episode. Remarkably, we will later
see that this term is comparable to the other previously existing terms.

36

We now move to bounding the expected squared value difference, as formally stated in as follows:
(cid:17)

(cid:17)

(cid:16) ¯V k−1

t

(sk

t ) − V k−1

t

(sk
t )

(cid:16) ¯V k

−

t (sk

t ) − V k

t (sk
t )

. Then, outside the failure

Lemma 24. Let ∆k
event,

t :=

(cid:16) ¯V k−1

t

E[

(sk

t ) − V k−1

t

(cid:17)2

(sk
t )

| Fk−1]

≤ 2H

H−1
(cid:88)

t(cid:48)=t

(cid:20)
∆k

E

t(cid:48)(sk
t(cid:48))

(cid:26)

2

+ min

(F + D)2
t(cid:48), ak

nk−1(sk

t(cid:48)) ∨ 1

(cid:27)

, H 2

(cid:21)
,

| Fk−1

where F + D is deﬁned in Lemma 23.

Proof. Before proving the bound, we express the bound of Lemma 23 in terms of ∆k

t . For brevity,

we denote Yk(s, a) := min

, H

, which is Fk measurable.

(cid:26)

F +D√

nk(s,a)∨1

(cid:27)

Assume the state sk

¯V k−1
t

(sk

t is visited in the kth episode at the tth time-step. Then, by Lemma 23,
t ) − V k−1
t ) = ∆k
6
t + Yk−1(sk
≤ ∆k

t ) − V k
(cid:104) ¯V k−1
t+1 (sk

t+1) | Fk−1, sk
t

(sk

(cid:105)

.

t + ¯V k
t (sk
t ) + E
t , ak
(cid:124)

t (sk
t )
t+1) − V k−1
t+1 (sk
(cid:123)(cid:122)
(∗)

Next, by substituting Equation (19) in (*), we get
(cid:104) ¯V k−1

t+1 + Yk−1(sk

t+1) + E

t+1, ak

∆k

t+2 (sk

t+2) − V k−1

t+2 (sk

t+1 + Yk−1(sk

t+1, ak

t+1) + ¯V k−1

t+2 (sk

t+2) − V k−1

t+2 (sk

(cid:104)
(∗) ≤ E
(cid:104)
= E
∆k

t+2) | Fk−1, sk
(cid:105)
,

t+2) | Fk−1, sk
t

t+1

(19)

(cid:125)

(cid:105)

(cid:105)

| Fk−1, sk
t

where the last relation holds by the tower property.
Iterating using this technique until t = H, and using ¯VH+1 = V H+1 = 0, we conclude the following
bound:

¯V k−1
t

(sk

t ) − V k−1

t

(sk

t ) ≤

H
(cid:88)

t(cid:48)=t

E[∆k

t(cid:48)(sk

t(cid:48)) + Yk−1(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t ],

With this bound at hand, we can derive the desired result as follows:

(cid:16) ¯V k−1

t

(sk

t ) − V k−1

t

(cid:17)2

(sk
t )

≤

(cid:32) H
(cid:88)

t(cid:48)=t

E(cid:2)∆k

t(cid:48)(sk

t(cid:48)) + Yk−1(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:33)2
(cid:3)

(CS)
≤ (H − t + 1)

H
(cid:88)

t(cid:48)=t

E(cid:2)∆k

t(cid:48)(sk

t(cid:48)) + Yk−1(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:3)2

(J)
≤ (H − t + 1)

H
(cid:88)

t(cid:48)=t

(cid:104)(cid:0)∆k
E

t(cid:48)(sk

t(cid:48)) + Yk−1(sk

t(cid:48), ak

t(cid:48))(cid:1)2

(cid:105)

| Fk−1, sk
t

(CS)
≤ 2(H − t + 1)

H
(cid:88)

t(cid:48)=t

E(cid:2)∆k

t(cid:48)(sk

t(cid:48))2 + Y 2

k−1(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:3)

≤ 2H

H
(cid:88)

t(cid:48)=t

E(cid:2)∆k

t(cid:48)(sk

t(cid:48))2 + Y 2

k−1(sk

t(cid:48), ak

t(cid:48)) | Fk−1, sk
t

(cid:3)

(CS) denotes Cauchy-Schwarz inequality, and speciﬁcally ((cid:80)n
i . (J) is Jensen’s
inequality. Taking the conditional expectation E[· | Fk−1], using the tower property and substituting
Yk(s, a) gives the desired result.

i=1 ai)2 ≤ n (cid:80)n

i=1 a2

37

After bounding the expected squared value difference in a single state, we now move to bounding its
sum over different time-steps and episode. The main difﬁculty is in bounding the sum over the ﬁrst
term, which we bound in the following lemma:
Lemma 25. Outside the failure event,

K
(cid:88)

H
(cid:88)

H−1
(cid:88)

k=1

t=1

t(cid:48)=t

E[∆k

t(cid:48)(sk
t(cid:48))

2

| Fk−1] ≤ ˜O(SH 4),

where ∆k

t (sk

t ) is deﬁned in Lemma 24.

Proof. We have that

H
(cid:88)

H
(cid:88)

t=1

t(cid:48)=t

E[∆k

2
t(cid:48)(sk
t(cid:48))

| Fk−1] =

H
(cid:88)

t=1

tE[∆k

2
t (sk
t )

| Fk−1] ≤ H

H
(cid:88)

t=1

E[∆k

2
t (sk
t )

| Fk−1].

Furthermore,

(cid:0)∆k

t (sk

t )(cid:1)2

=

=

≤

=

t

(cid:16)(cid:16) ¯V k−1
(cid:16) ¯V k−1

t

(sk
(cid:16) ¯V k−1

t

− 2
(cid:16) ¯V k−1
(cid:16) ¯V k−1

t

t

(sk

t ) − V k−1

t

t (sk

t ) − V k

t (sk
t )

(cid:17)(cid:17)2

t ) − V k−1

t

(sk
t )

t (sk

t ) − V k

(sk

t ) − V k−1

t

t (sk

t ) − V k

t (sk
t )

(cid:17)2

t (sk
t )
(cid:17)

(cid:17)

(sk
t )
(cid:17)2

(sk
t )
(cid:17)2

−

(cid:16) ¯V k
(cid:16) ¯V k
+
(cid:17)(cid:16) ¯V k
(cid:16) ¯V k
(cid:16) ¯V k

+

−

(sk

t ) − V k−1

t

(cid:17)2

(sk
t )

t (sk

t ) − V k

t (sk
t )

(cid:17)2

,

(sk

t ) − V k−1

t

(sk
t )

t (sk

t ) − V k

t (sk
t )

(cid:17)2

(cid:16) ¯V k

− 2

t (sk

t ) − V k

t (sk
t )

(cid:17)2

where the third relation holds since ¯V k(s), V k(s) decreases and increases, respectively, by Lemma
18, and since outside of the failure event ¯V k(s) ≥ V k(s), ∀k (Lemma 22). Another implication these
properties is that

(cid:16) ¯V k−1

t

Thus,

(sk

t ) − V k−1

t

(cid:17)2

(sk
t )

≥

(cid:16) ¯V k

t (sk

t ) − V k

t (sk
t )

(cid:17)2

,

H

K
(cid:88)

H
(cid:88)

k=1

t=1

E[∆k

t (sk
t )

2

| Fk−1]

≤ H

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:16) ¯V k−1

t

E[

(sk

t ) − V k−1

t

(cid:16) ¯V k

t (sk

t ) − V k

t (sk
t )

(cid:17)2

| Fk−1] .

(20)

(cid:17)2

(sk
t )

−

38

For brevity, we deﬁne ∆V k
(Appendix F),

t (s) = ¯V k

t (s) − V k

t (s). Similarly to the technique used in Lemma 34

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:16) ¯V k−1

t

E[

(sk

t ) − V k−1

t

(cid:17)2

(sk
t )

−

(cid:16) ¯V k

t (sk

t ) − V k

t (sk
t )

(cid:17)2

| Fk−1]

=

(1)
=

(2)
=

K
(cid:88)

H
(cid:88)

k=1

t=1

E[∆V k−1
t

(sk

t )2 − ∆V k

t (sk

t )2 | Fk−1]

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

E[1(cid:8)sk

t = s(cid:9)∆V k−1

t

E[1(cid:8)sk

t = s(cid:9)∆V k−1

t

(s)2 − 1(cid:8)sk

t = s(cid:9)∆V k

t (s)2 | Fk−1]

(s)2 + 1(cid:8)sk

t (cid:54)= s(cid:9)∆V k−1

t

(s)2 | Fk−1]

− E[1(cid:8)sk

t = s(cid:9)∆V k

t (s)2 + 1(cid:8)sk

t (cid:54)= s(cid:9)∆V k−1

t

(s)2 | Fk−1]

K
(cid:88)

H
(cid:88)

(cid:88)

(3)
=

k=1

t=1

s

∆V k−1
t

(s)2 − E[∆V k

t (s)2 | Fk−1]

Relation (1) holds by adding and subtracting 1(cid:8)s (cid:54)= sk
(s) while using the linearity of expec-
tation. (2) holds since for any event 1{A} + 1{Ac} = 1 and since ∆V k−1
is Fk−1 measurable. (3)
holds by the deﬁnition of the update rule. If state s is visited in the kth episode at time-step t, then
both ¯V k
t (s) are updated. If not, their value remains as in the k − 1 iteration.

t (s), V k

(cid:9) ¯V k−1

t

t

t

Lastly,

K
(cid:88)

H
(cid:88)

(cid:88)

∆V k−1
t

(s)2 − E[∆V k

t (s)2 | Fk−1]

k=1

t=1

s

K
(cid:88)

H
(cid:88)

=

(cid:88)

(cid:16) ¯V k−1

t

(s) − V k−1

t

(cid:17)2

(s)

− E[

(cid:16) ¯V k

t (s) − V k

t (s)

(cid:17)2

| Fk−1] ≤ ˜O(SH 3),

k=1

t=1

s

where the inequality holds outside the failure event F vsDP , which is deﬁned in Appendix E.2.2.
Plugging this into (20) concludes the proof.

39

We are now ready to prove the main results of this section and bound (16) and (17):
Lemma 26. Outside the failure event.

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2

≤ ˜O(SAH 2(F + D)2 + SAH 5).

where F + D is deﬁned in Lemma 23

Proof. Recall that wtk(s, a) = Pr(sk
t
MDP the state-action in the kth episode at the tth time-step is (sk
relation holds.

1, πk) is the probability when following πk in the true
t ) = (s, a). Thus, the following

t , ak

| sk

wtk(s, a)p(· | s, a)T ( ¯V k−1

t+1 − V k−1

t+1 )2

(cid:88)

s,a

(cid:88)

st
(cid:88)

=

=

Pr(sk

t | sk

1, πk)

(cid:88)

p(sk

t+1 | sk

t , ak

t )( ¯V k−1

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1))2

Pr(sk

t+1 | sk

st+1
1, πk)( ¯V k−1

t+1 (sk

t+1) − V k−1

t+1 (sk

t+1))2

st+1
= E[( ¯V k−1

t+1 (st+1) − V k−1

t+1 (st+1))2 | Fk−1].

Since ¯V k−1

H+1(st+1) = V k−1

H+1(st+1 = 0, we obtain,

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)( ¯V k−1

t+1 − V k−1

t+1 )2

=

≤

K
(cid:88)

H
(cid:88)

k=1

t=1

K
(cid:88)

H
(cid:88)

k=1

t=1

E[( ¯V k−1

t+1 (sk

t ) − V k−1

t+1 (sk

t )2 | Fk−1]

E[( ¯V k−1
t

(sk

t ) − V k−1

t

(sk

t )2 | Fk−1].

Thus,
K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)p(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

E[

≤

k=1

t=1

(cid:16) ¯V k−1

t+1 (sk

t ) − V k−1

t+1 (sk
t )

(cid:17)2

| Fk−1]

(∗)
≤ 2H

K
(cid:88)

H
(cid:88)

H
(cid:88)

k=1

t=1

t(cid:48)=t

E[∆k

t(cid:48) (sk
t(cid:48) )

2

| Fk−1] + 2H

K
(cid:88)

H
(cid:88)

H
(cid:88)

(cid:20)

E

(cid:26)

min

= 2H

K
(cid:88)

H
(cid:88)

k=1

t=1

tE[∆k

t (sk
t )

2

| Fk−1] + 2H

K
(cid:88)

H
(cid:88)

k=1

t=1

k=1

t=1
(cid:20)
tE

t(cid:48)=t

(cid:26)

min

(F + D)2
t(cid:48) , ak

nk−1(sk

(cid:27)

, H 2

(cid:21)

| Fk−1

t(cid:48) ) ∨ 1
(cid:27)

, H 2

| Fk−1

(F + D)2
t , ak

nk−1(sk

t ) ∨ 1

(cid:21)

(cid:21)

≤ 2H 2

K
(cid:88)

H
(cid:88)

k=1

t=1

E[∆k

2
t (sk
t )

| Fk−1] + 2H 2

K
(cid:88)

H
(cid:88)

(cid:20)

(cid:26)

min

E

k=1

t=1

(F + D)2
t , ak

nk−1(sk

t ) ∨ 1

(cid:27)

, H 2

| Fk−1

(21)

where (∗) last relation holds by Lemma 24, in which ∆k
t ) is deﬁned. The ﬁrst term is bounded in
Lemma 25 by ˜O(SH 5). The second term is bounded outside the failure event Using the ’Good Set’
Lk, which is deﬁned and analyzed in Appendix F.1. The bound for this term can be found in Lemma
39. Combining both of the results and substituting into (21) yields

t (sk

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2

≤ ˜O(SH 5) + ˜O(SAH 2(F + D)2 + SAH 5)

= ˜O(SAH 2(F + D)2 + SAH 5)

40

Lemma 27. Outside the failure event.

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)T (cid:0) ¯V k−1

t+1 − V πk

t+1

(cid:1)2

≤ ˜O(SAH 3(F + D)2 + SAH 5)

where F + D is deﬁned in Lemma 23

Proof. Similarly to Lemma 26, we have that

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)p(· | s, a)T (cid:0) ¯V k−1

t+1 − V πk

t+1

(cid:1)2

≤

≤

K
(cid:88)

H
(cid:88)

k=1

t=1

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0) ¯V k−1
E

t+1 (sk

t ) − V πk

t+1(sk

t )(cid:1)2

(cid:105)

| Fk−1

(cid:104)(cid:0) ¯V k−1
E

t

(sk

t ) − V πk

t

(sk

t )(cid:1)2

| Fk−1

(cid:105)

.

(22)

where the last inequality is since ¯V k−1

H+1(st+1) = V πk

H+1(st+1 = 0. Applying Lemma 7, we get,

(sk

t ) − V πk

t

(cid:17)2

(sk
t )

| Fk−1]

E[

(cid:16) ¯V k−1
t


(1)
≤ E



(cid:32) H
(cid:88)

(cid:104) ¯V k−1(sk
E

t(cid:48) ) − ¯V k(sk

t(cid:48) ) + (˜rk−1 − r)(sk

t(cid:48) , ak

t(cid:48) ) + (˜pk−1 − p)(sk

t(cid:48) , ak

t(cid:48) ) ¯V k−1

t+1 | Fk−1, sk

t

(cid:33)2
(cid:105)





| Fk−1

t(cid:48)=t
(cid:34) H
(cid:88)

E

(2)
≤ 3HE

(cid:20)(cid:16) ¯V k−1(sk

t(cid:48) ) − ¯V k(sk
t(cid:48) )

(cid:17)2

| Fk−1, sk
t

(cid:21)

| Fk−1

(cid:35)

t(cid:48)=t
(cid:34) H
(cid:88)

+ 3HE

t(cid:48)=t

(cid:20)(cid:16)

E

(˜rk−1 − r)(sk

t(cid:48) , ak
t(cid:48) )

(cid:17)2

+

(cid:16)
(˜pk−1 − p)(sk

t(cid:48) , ak

t(cid:48) ) ¯V k−1

t+1

(cid:17)2

| Fk−1, sk
t

(cid:21)

| Fk−1

(cid:35)

(3)
= 3H

H
(cid:88)

E

t(cid:48)=t

(cid:20)(cid:16) ¯V k−1(sk

t(cid:48) ) − ¯V k(sk
t(cid:48) )

(cid:17)2

| Fk−1

(cid:21)

+ 3H

H
(cid:88)

(cid:20)(cid:16)

E

t(cid:48)=t

(˜rk−1 − r)(sk

t(cid:48) , ak
t(cid:48) )

(cid:17)2

(cid:16)

+

(˜pk−1 − p)(sk

t(cid:48) , ak

t(cid:48) ) ¯V k−1

t+1

(cid:17)2

(cid:21)
.

| Fk−1

Inequality (1) is by Lemma 12.
((cid:80)n
Plugging this back into (22),

i=1 ai)2 ≤ n (cid:80)n

i=1 a2

i , and (3) is by the tower property.

(2) is due to Jensen’s inequality, and using the inequality

(22) ≤ 3H

+ 3H

K
(cid:88)

H
(cid:88)

H
(cid:88)

k=1

t=1

t(cid:48)=t

K
(cid:88)

H
(cid:88)

H
(cid:88)

k=1

t=1

t(cid:48)=t

(cid:104)(cid:0) ¯V k−1(sk
E

t(cid:48)) − ¯V k(sk

t(cid:48))(cid:1)2

(cid:105)

| Fk−1

(cid:104)(cid:0)(˜rk−1 − r)(sk
E

t(cid:48), ak

t(cid:48))(cid:1)2

+ (cid:0)(˜pk−1 − p)(sk

t(cid:48), ak

t(cid:48)) ¯V k−1

t+1

(cid:1)2

| Fk−1

(cid:105)

K
(cid:88)

H
(cid:88)

≤ 3H 2

(cid:104)(cid:0) ¯V k−1(sk
E

k=1

t=1

(cid:124)

t )(cid:1)2

t ) − ¯V k(sk
(cid:123)(cid:122)
(∗)

| Fk−1

(cid:105)

(cid:125)

+ 3H 2

K
(cid:88)

H
(cid:88)

k=1

t=1

E


(cid:0)(˜rk−1 − r)(sk


(cid:124)

(cid:123)(cid:122)
(∗∗)

t , ak

t )(cid:1)2
(cid:125)

+ (cid:0)(˜pk−1 − p)(sk
t , ak
(cid:123)(cid:122)
(∗∗∗)

(cid:124)

t ) ¯V k−1

t+1




.

| Fk−1

(cid:1)2

(cid:125)

(23)

41

We now bound each term of the above. First, we have that

(∗) =

H
(cid:88)

H
(cid:88)

t=1

t=1

(cid:104)(cid:0) ¯V k−1(sk
E

t ) − ¯V k(sk

t )(cid:1)2

(cid:105)

| Fk−1

=

(1)
≤

=

(2)
=

H
(cid:88)

H
(cid:88)

(cid:104)(cid:0) ¯V k−1(sk
E

t )(cid:1)2

+ (cid:0) ¯V k(sk

t )(cid:1)2

− 2 ¯V k(sk

t ) ¯V k−1(sk

t ) | Fk−1

(cid:105)

t=1

t=1

H
(cid:88)

H
(cid:88)

t=1

t=1

(cid:104)(cid:0) ¯V k−1(sk
E

t )(cid:1)2

+ (cid:0) ¯V k(sk

t )(cid:1)2

− 2(cid:0) ¯V k(sk

t )(cid:1)2

| Fk−1

(cid:105)

H
(cid:88)

H
(cid:88)

(cid:104)(cid:0) ¯V k−1(sk
E

t )(cid:1)2

− (cid:0) ¯V k(sk

t )(cid:1)2

| Fk−1

(cid:105)

t=1

t=1

H
(cid:88)

H
(cid:88)

(cid:88)

(cid:0) ¯V k−1(s)(cid:1)2

− E

(cid:104)(cid:0) ¯V k(s(cid:1)2

| Fk−1

(cid:105)

.

t=1

t=1

s

Relation (1) holds since 0 ≤ ¯V k ≤ ¯V k−1 (see Lemma 18). (2) is proven similarly to Lemma 34
(Appendix F), as follows

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0) ¯V k−1(sk
E

t )(cid:1)2

− (cid:0) ¯V k(sk

t )(cid:1)2

| Fk−1

(cid:105)

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

K
(cid:88)

H
(cid:88)

(cid:88)

(1)
=

(2)
=

k=1

t=1

s

E[1(cid:8)sk

t = s(cid:9)(cid:0) ¯V k−1(s)(cid:1)2

− 1(cid:8)sk

t = s(cid:9)(cid:0) ¯V k(s)(cid:1)2

| Fk−1]

E[1(cid:8)sk

t = s(cid:9)(cid:0) ¯V k−1(s)(cid:1)2

+ 1(cid:8)sk

t (cid:54)= s(cid:9)(cid:0) ¯V k−1(s)(cid:1)2

| Fk−1]

− E[1(cid:8)sk

t = s(cid:9)(cid:0) ¯V k(s)(cid:1)2

+ 1(cid:8)sk

t (cid:54)= s(cid:9)(cid:0) ¯V k−1(s)(cid:1)2

| Fk−1]

K
(cid:88)

H
(cid:88)

(cid:88)

(3)
=

k=1

t=1

s

(cid:0) ¯V k−1(s)(cid:1)2

− E[(cid:0) ¯V k(s)(cid:1)2

| Fk−1]

(cid:9) ¯V k−1

(1) holds by adding and subtracting 1(cid:8)s (cid:54)= sk
(s) while using the linearity of expectation. (2)
holds since for any event 1{A} + 1{Ac} = 1 and since ∆V k−1
is Fk−1 measurable. (3) holds
by the deﬁnition of the update rule. If state s is visited in the kth episode at time-step t, then both
¯V k
t (s), V k
Next, by Lemma 18 for a ﬁxed s, t, (cid:8) ¯V k
Applying Lemma 11 we conclude that

t (s) are updated. If not, their value remains as in the k − 1 iteration.
t (s)(cid:9)

k≥0 is a Decreasing Bounded Process in [0, H 2].

t

t

t

(∗) ≤

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s

(cid:0) ¯V k−1(s)(cid:1)2

− E[(cid:0) ¯V k(s)(cid:1)2

| Fk−1] (cid:46) ˜O(H 3S).

42

We now turn to bound (∗∗).

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0)(˜rk−1 − r)(sk
E

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

K
(cid:88)

H
(cid:88)

(1)
≤ 2

(2)
≤ 4

k=1

t=1

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0)(ˆrk−1 − r)(sk
E

t , ak

t )(cid:1)2

| Fk−1

(cid:105)

+ 2

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0)br
E

k(sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

(cid:104)(cid:0)br
E

k(sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

= 4

K
(cid:88)

H
(cid:88)

k=1

t=1





(cid:115)

E






2 ˆVar(R(sk
nk−1(sk

t , ak
t , ak

t )) ln 4SAT
δ(cid:48)
t ) ∨ 1

+

14 ln 4SAT
δ(cid:48)
t , ak
t ) ∨ 1

3nk−1(sk



2



| Fk−1






1
t , ak
nk−1(sk

t ) ∨ 1

(cid:21)

| Fk−1

(3)
(cid:46)

K
(cid:88)

H
(cid:88)

(cid:20)

E

k=1

t=1

(4)
(cid:46) ˜O(SAH).

In (1), we used the deﬁnition of ˜rk−1 and the inequality (a + b)2 ≤ 2a2 + 2b2. (2) is since outside
the failure event F r, (ˆrk−1 − r)(s, a) ≤ br
k(s, a). (3) uses the fact that R(s, a) ∈ [0, 1], and thus
√
ˆVar(R(s, a) ≤ 1, and
Lastly, we bound (∗ ∗ ∗).

n ≤ n for n ≥ 1. Finally, (4) is due to Lemma 39.

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0)(˜pk−1 − p)(sk
E

t , ak

t )T ¯V k−1

t+1

(cid:1)2

| Fk−1

(cid:105)

=

K
(cid:88)

H
(cid:88)

k=1

t=1

(cid:104)(cid:0)(ˆpk−1 − p)(sk
E

t , ak

t )T ¯V k−1

t+1 + bpv

k (sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

K
(cid:88)

H
(cid:88)

(1)
≤ 2

k=1

t=1

K
(cid:88)

H
(cid:88)

k=1

t=1

K
(cid:88)

H
(cid:88)

(2)
≤ 2

(3)
≤ 2

(cid:104)(cid:0)(ˆpk−1 − p)(sk
E

t , ak

t )T ¯V k−1

t+1

(cid:1)2

+ (cid:0)bpv

k (sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

(cid:104)(cid:0)(cid:107)ˆpk−1 − p(cid:107)1(cid:107) ¯V k−1
E

t+1 (cid:107)∞

(cid:1)2

+ (cid:0)bpv

k (sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

(cid:104)
H 2(cid:107)ˆpk−1 − p(cid:107)2
E

1 + (cid:0)bpv

k (sk

t , ak

t )(cid:1)2

(cid:105)

| Fk−1

k=1

t=1

K
(cid:88)

H
(cid:88)

(4)
(cid:46)

(cid:34)

E

k=1

t=1

H 2S
nk−1(sk

t , ak
t )

+

(2BvH + 5J + Bp)2

nk−1(sk

t , ak
t )

(cid:35)

| Fk−1

.

(5)
(cid:46) ˜O(SAH(F + D)2)

Similarly to the bound on the reward, (1) uses the inequality (a + b)2 ≤ 2a2 + 2b2. Inequality (2) is
due to Hölder’s inequality, and (3) bounds (cid:107) ¯V k−1
t+1 (cid:107)∞ ≤ H, which is due to Lemma 18. Next, (4)
bounds the transition error outside to failure event F pn1 and bpv
k according to Lemma 16. Finally, (5)
is by Lemma 39 and noting that H 2S + (2BvH + 5J + Bp)2 (cid:46) (F + D)2.
Substituting all of the results into (23), and remembering the H 2 factor in this equation, gives the
desired result.

43

E.6 Bounding Different Terms in the Regret Decomposition

In this section, we bound each of the individual terms of the regret decomposition (Equation 15),
relaying on results from [Zanette and Brunskill, 2019], as well as on the new lemmas derived in
Section E.5, Lemma 26 and Lemma 27. First, we present the problem dependent constants of
[Zanette and Brunskill, 2019] for general admissible conﬁdence intervals, and their relation to
problem dependent constants with Bernstein’s inequality
Lemma 28. Let C∗ and Cπ be upper dependent bounds on the following qualities:

C∗ ≥

Cπ ≥

1
T

1
T

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)g(p, V ∗

t+1)2

wtk(s, a)g(p, V πk

t+1)2 ,

with g(p, V ) =

(cid:113)

2Vars(cid:48)∼p(·|s,a)V (s(cid:48)) ln 2SAT

δ(cid:48)

, and let

C∗

r =

1
T





K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk



wtk(s, a)VarR(s, a)

 ,

where Lk is deﬁned in Deﬁnition 2. Finally, let Q∗ := maxs,a,t
Then,

(cid:0)VarR(s, a) + Vars(cid:48)∼p(·|s,a)V ∗

t+1(s(cid:48))(cid:1).

C∗
r + C∗ (cid:46) Q∗
Cπ (cid:46) G2
H
G2
H

r ≤

C∗

Proof. We follow proposition 6 of [Zanette and Brunskill, 2019], and start by substituting g(p, V )
into C∗

r + C∗

r + C∗ (cid:46) 1
C∗
T





K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)VarR(s, a)





+

1
T

1
T

≤

≤

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

(cid:88)

1
T





k=1

t=1

(s,a)





K
(cid:88)

H
(cid:88)

(cid:88)

t=1

(s,a)

wtk(s, a)Vars(cid:48)∼p(·|s,a)V ∗

t+1(s(cid:48))



wtk(s, a)(VarR(s, a)) + Vars(cid:48)∼p(·|s,a)V ∗

t+1(s(cid:48))





wtk(s, a) max
s,a,t

{VarR(s, a)} + Vars(cid:48)∼p(·|s,a)V ∗

t+1(s(cid:48))



k=1


K
(cid:88)



H
(cid:88)

(cid:88)



wtk(s, a)



k=1

t=1

(s,a)

=

Q∗

T

= Q∗

where the last equality is since (cid:80)

(s,a) wtk(s, a) = 1 and T = HK.

44

Next, we bound Cπ:

Cπ (cid:46) 1
T

(1)
=

1
T

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)Vars(cid:48)∼p(·|s,a)V πk

t+1(s(cid:48))

k=1

K
(cid:88)

s,a
(cid:32) H
(cid:88)

t=1


E



k=1

t=1

(cid:33)2



r(sk

t , ak

t ) − V πk

1 (sk
1)

| Fk−1





E



(cid:32) H
(cid:88)

(cid:33)2



r(sk

t , ak
t )

| Fk−1



≤

1
T

K
(cid:88)

k=1

(2)
≤

1
T

KG2 =

t=1

G2
H

,

where (1) is due to the Law of Total Variance (LTV), which was used in [Azar et al., 2017], and was
stated formally in Lemma 15 of [Zanette and Brunskill, 2019]. In (2), we bound the reward in an
episode by G.
Finally, the bound on C∗
this proof.

r is proven in Lemma 8 of [Zanette and Brunskill, 2019], which concludes

We also prove the following lemma that helps translating bounds that depend on C∗ to bounds that
depends on Cπ. This lemma is equivalent to lemma 14 of [Zanette and Brunskill, 2019], but the
prove requires Lemma 27, that was not proved in their paper. This is since they rely on the inequality
V k−1
Lemma 29 (Bound Translation Lemma). Outside the failure event, it holds that

t ≤ V πk , which does not seem to hold.

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)

t+1)

g(p, V ∗
(cid:112)nk−1(s, a) ∨ 1

−

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)

g(p, V πk
(cid:112)nk−1(s, a) ∨ 1

t+1)

(cid:16)

= ˜O

BvSAH

3

2 (F + D) + BvSAH

(cid:17)

5
2

where F, D are deﬁned in Lemma 23.

Proof. We start as in the original Lemma 14 of [Zanette and Brunskill, 2019]:

wtk(s, a)

g(p, V πk
(cid:112)nk−1(s, a) ∨ 1

t+1)

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)

(cid:88)

K
(cid:88)

H
(cid:88)

−

(cid:88)

t+1)

g(p, V ∗
(cid:112)nk−1(s, a) ∨ 1
t=1
k=1
(s,a)∈Lk
(cid:13)
(cid:13)
t+1 − V πk
(cid:13)V ∗
(cid:13)2,p
t+1
(cid:112)nk−1(s, a) ∨ 1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

wtk(s, a)
nk−1(s, a) ∨ 1

wtk(s, a)

H
(cid:88)

K
(cid:88)

(cid:88)

K
(cid:88)

H
(cid:88)

(1)
≤ Bv

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

H
(cid:88)

(2)
≤ Bv

t=1

(s,a)∈Lk

k=1

(s,a)∈Lk

k=1

t=1

(s,a)∈Lk

t=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

√

(3)
(cid:46) Bv

SA

H
(cid:88)

(cid:88)

wtk(s, a)p(· | s, a)T (cid:0) ¯V k−1

t+1 − V πk

t+1

(cid:1)2

(cid:88)

wtk(s, a)(cid:13)

(cid:13)V ∗

t+1 − V πk

t+1

(cid:13)
2
(cid:13)
2,p

k=1

t=1

(s,a)

where in (1) we use property 1 of Deﬁnition 1, and (2) is due to Cauchy-Schwarz in-
In (3) we used Lemma 37. Next, we apply Lemma 27 to bound the re-
equality.
and bound (cid:112)SAH 3(F + D)2 + SAH 5 ≤
maining term by ˜O
(cid:112)SAH 3(F + D)2 +

(cid:16)(cid:112)SAH 3(F + D)2 + SAH 5
√

SAH 5, which yields the desired result

(cid:17)

45

We are now ready to bound each of the terms of the regret. To bound the ﬁrst term, we cite Lemma 8
of [Zanette and Brunskill, 2019]:
Lemma 30 (Optimistic Reward Bound). Outside the failure event, it holds that

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)(˜rk−1 − r)(sk

t , ak

t ) = ˜O

(cid:16)(cid:112)C∗

rSAT + SA

(cid:17)

The next three lemmas correspond to the remaining terms, and follow Lemmas 9,10 and 11 of
[Zanette and Brunskill, 2019], with slight modiﬁcations:
Lemma 31 (Empirical Transition Bound). Outside the failure event, it holds that

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)(ˆpk−1 − pk−1)(· | s, a)T V ∗

t+1 = ˜O

(cid:16)√

C∗SAT + JSA

(cid:17)

The following bound also holds:

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(ˆpk−1 − pk−1)(· | s, a)T V ∗

t+1

k=1

= ˜O

t=1
(cid:16)√

(s,a)∈Lk
CπSAT + JSA + BvSAH

3

2 (F + D) + BvSAH

(cid:17)

5
2

where F, D are deﬁned in Lemma 23.

Proof. Similarly to Lemma 9 of [Zanette and Brunskill, 2019], by the deﬁnition of φ (Deﬁnition 1),
and outside failure event F pv,

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)(ˆpk−1 − pk−1)(· | s, a)T V ∗

t+1

K
(cid:88)

H
(cid:88)

(cid:88)

(cid:32)

wtk(s, a)

t=1

(s,a)∈Lk

t+1)

g(p, V ∗
(cid:112)nk−1(s, a) ∨ 1

+

J
nk−1(s, a) ∨ 1

(cid:33)

(24)

≤

(∗)
≤

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)g(p, V ∗

t+1)2

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+ J

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

where the last inequality is by Cauchy-Schwarz Inequality. Substituting the deﬁnition of C∗, and
using Lemma 37, we get

√

(cid:46)

T C∗

√

SA + JSA ,

which concludes the ﬁrst statement of the lemma. To get the second statement, we apply Lemma
29 before inequality (∗) and only then use Cauchy-Schwarz Inequality. This creates the additional
constant term of ˜O
. Then, by applying Lemma 37, we get the
bound with Cπ.

2 (F + D) + BvSAH 5

BvSAH 3

(cid:16)

(cid:17)

2

46

Lemma 32 (Lower Order Term). Let F, D be the constants deﬁned in Lemma 23. Outside the failure
event, it holds that

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)(cid:12)

(cid:12)(ˆpk−1 − p)(· | s, a)T ( ¯V k−1

t+1 − V ∗

t+1)(cid:12)
(cid:12)

(cid:16)

= ˜O

3
2 AH(F + D + H

3

2 ) + S2AH

S

(cid:17)

Proof. Similarly to Lemma 11 of [Zanette and Brunskill, 2019], by the deﬁnition of φ (Deﬁnition 1),
and outside failure event F ps,

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(cid:12)

(cid:12)(ˆpk−1 − p)(· | s, a)T ( ¯V k−1

t+1 − V ∗

t+1)(cid:12)
(cid:12)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

(cid:46)

wtk(s, a)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+

k=1

t=1

(s,a)∈Lk

wtk(s, a)

(cid:88)

K
(cid:88)

H
(cid:88)

(cid:88)

≤

wtk(s, a)

(cid:115)

(cid:88)

s(cid:48)

s(cid:48)
(cid:115)

(cid:88)

s(cid:48)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+

k=1

t=1

(s,a)∈Lk

wtk(s, a)

HS
nk−1(s, a) ∨ 1

,

p(s(cid:48) | s, a)(1 − p(s(cid:48) | s, a))
nk−1(s, a) ∨ 1
t+1(s(cid:48))(cid:12)
(cid:12)

t+1 (s(cid:48)) − V ∗

(cid:12)
(cid:12) ¯V k−1

nk−1(s, a) ∨ 1

p(s(cid:48) | s, a)(1 − p(s(cid:48) | s, a))
nk−1(s, a) ∨ 1

(cid:12)
(cid:12) ¯V k−1

t+1 (s(cid:48)) − V ∗

t+1(s(cid:48))(cid:12)
(cid:12)

(cid:12)
(cid:12) ¯V k−1

t+1 (s(cid:48)) − V ∗

t+1(s(cid:48))(cid:12)
(cid:12)

where in the last inequality we used the fact that V ∗
using the optimism of the value V k−1
p ∈ [0, 1], we can bound

t+1 ≤ V ∗

t+1 and ¯V k−1

t+1 are in [0, H], by Lemma 18. Next,
t+1 (Lemma 22), and since (1 − p) ≤ 1 for

t+1 ≤ ¯V k−1

K
(cid:88)

H
(cid:88)

(cid:88)

≤

k=1

t=1

(s,a)∈Lk

wtk(s, a)

(cid:115)

(cid:88)

s(cid:48)

p(s(cid:48) | s, a)
nk−1(s, a) ∨ 1

(cid:12)
(cid:12)
(cid:12)

¯V k−1
t+1 (s(cid:48)) − V k−1

t+1 (s(cid:48))

(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

H
(cid:88)

(cid:88)

+ HS

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

K
(cid:88)

H
(cid:88)

(cid:88)

(CS)
≤

(cid:115)

wtk(s, a)

Sp(· | s, a)T (cid:0) ¯V k−1

t+1 − V k−1

t+1

(cid:1)2

nk−1(s, a) ∨ 1

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+ HS

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(CS)
≤

√

S

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

wtk(s, a)
nk−1(s, a) ∨ 1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)p(· | s, a)T (cid:0) ¯V k−1

t+1 − V k−1

t+1

(cid:1)2

K
(cid:88)

H
(cid:88)

(cid:88)

+ HS

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

√

√

(∗)
(cid:46)

S
(cid:16)

SA(cid:112)SAH 2(F + D)2 + SAH 5 + SH · SA
2 ) + S2AH

3
2 AH(F + D + H

(cid:17)

3

= ˜O

S

(CS) denotes Cauchy-Schwarz. Speciﬁcally, the ﬁrst inequality uses (cid:80)n
In (∗), we used Lemmas 37 and 26.

i=1 aibi ≤ (cid:112)n (cid:80)n

i=1 a2

i b2
i .

47

Lemma 33 (Optimistic Transition Bound). Let F, D be the constants deﬁned in Lemma 23. Outside
the failure event, it holds that

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(˜pk−1 − ˆpk−1)(· | s, a)T ¯V k−1
t+1

k=1

(s,a)∈Lk

t=1
(cid:18)√

= ˜O

C∗SAT + (J + Bp)SA + BvSAH

(cid:16)

F + D + H

(cid:17)

3
2

(cid:113)

+ BvSA

S

The following bound also holds:

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(ˆpk−1 − ˆpk−1)(· | s, a)T V ∗

t+1

k=1

(s,a)∈Lk

t=1
(cid:18)√

= ˜O

CπSAT + (J + Bp)SA + BvSAH

3
2 (F + D + H) + BvSA

(cid:113)

S

1
2 H(F + D + H

5
2 ) + SH 2

(cid:19)

1
2 H(F + D + H

5
2 ) + SH 2

(cid:19)

Proof. Similarly to Lemma 10 of [Zanette and Brunskill, 2019], by the deﬁnition of the bonus,

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)(˜pk−1 − ˆpk−1)(· | s, a)T ¯V k−1
t+1

k=1

t=1

(s,a)∈Lk

=

=

≤

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)bpv

k (s, a)

(cid:32)

wtk(s, a)

φ(ˆpk−1(· | s, a), V k−1

t+1 ) +

Bv(cid:107) ¯V k−1

t+1 − V k−1
(cid:112)nk−1(s, a) ∨ 1

t+1 (cid:107)2, ˆp

(cid:33)

+

4J + Bp
nk−1(s, a) ∨ 1

(cid:32)

wtk(s, a)

φ(p(· | s, a), V ∗) + 2

Bv(cid:107) ¯V k−1

t+1 − V k−1
(cid:112)nk−1(s, a) ∨ 1

t+1 (cid:107)2, ˆp

+ 2

4J + Bp
nk−1(s, a) ∨ 1

(cid:33)

.

In the last inequality, we applied Lemma 15, Property(3), and used Equation (13) together with the
optimism of the value function, that is V k−1
t+1 (Lemma 18). Next, we substitute the
deﬁnition of φ (Deﬁnition 1), and get

t+1 ≤ ¯V k−1

t+1 ≤ V ∗

K
(cid:88)

H
(cid:88)

(cid:88)

(cid:46)

wtk(s, a)

(cid:18) g(p, V ∗

t+1)

nk−1(s, a) ∨ 1

+

J + Bp
nk−1(s, a) ∨ 1

(cid:19)

k=1

t=1

(s,a)∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

+

k=1

t=1

(s,a)∈Lk

wtk(s, a)

Bv(cid:107) ¯V k−1

t+1 − V k−1
(cid:112)nk−1(s, a) ∨ 1

t+1 (cid:107)2, ˆp

.

(25)

(26)

The term in Equation (25) is almost identical to Equation (24) of Lemma 31, and can be similarly
(cid:17)
bounded by replacing J with J + Bp. This yields a bound of either ˜O
C∗SAT + (J + Bp)SA
(cid:17)
or ˜O
2 (F + D) + BvSAH 5
the second term. Notice that

CπSAT + (J + Bp)SA + BvSAH 3

. We now move to bounding

(cid:16)√

(cid:16)√

2

(cid:107) ¯V k−1

t+1 − V k−1

t+1 (cid:107)2

2, ˆp = ˆpk−1(· | s, a)T (cid:16) ¯V k−1
= p(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1
t+1
(cid:17)2

t+1 − V k−1

t+1

(cid:17)2

+ (ˆpk−1 − p)(· | s, a)T (cid:16) ¯V k−1
(cid:17)2

2,p + (ˆpk−1 − p)(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

t+1 − V k−1

t+1

(cid:17)2

(27)

= (cid:107) ¯V k−1

t+1 − V k−1

t+1 (cid:107)2

48

t=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

Next, applying Cauchy-Schwartz Inequality on (26), we get

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(26) ≤ Bv

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a) ∨ 1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)(cid:107) ¯V k−1

t+1 − V k−1

t+1 (cid:107)2

2, ˆp

√

(cid:46) Bv

SA

H
(cid:88)

(cid:88)

t=1

(s,a)∈Lk

wtk(s, a)(cid:107) ¯V k−1

t+1 − V k−1

t+1 (cid:107)2

2,p

√

+ Bv

SA

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

(cid:12)
(cid:12)
wtk(s, a)
(cid:12)
(cid:12)

(ˆpk−1 − p)(· | s, a)T

(cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where the last inequality is by Lemma 37, substituting (27) and using the inequality
√

a + b ≤
b. The ﬁrst term can be directly bounded by Lemma 26. The second term can be bounded

a +

√

√

using Lemma 32 as follows:

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

(cid:12)
(cid:12)
wtk(s, a)
(cid:12)
(cid:12)

(ˆpk−1 − p)(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)2(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

H
(cid:88)

(cid:88)

≤ H

wtk(s, a)

(cid:12)
(cid:12)

(cid:12)(ˆpk−1 − p)(· | s, a)T (cid:16) ¯V k−1

t+1 − V k−1

t+1

(cid:17)(cid:12)
(cid:12)
(cid:12)

t=1

(s,a)∈Lk

k=1
(cid:16)

= ˜O

3
2 AH(F + D + H

S

5

2 ) + S2AH 2(cid:17)

,

where we trivially bounded the value difference by H at the ﬁrst inequality (due to Lemma 18) and
used Lemma 32 at the second one. Summing both terms yields

(cid:18)

(cid:18)

(26) = ˜O

= ˜O

√

Bv

SA(cid:112)SAH 2(F + D)2 + SAH 5 + Bv

√

SA

(cid:113)

S 3

2 AH(F + D + H 5

2 ) + S2AH 2

(cid:19)

(cid:16)

BvSAH

F + D + H

(cid:17)

3
2

(cid:113)

+ BvSA

S 1

2 H(F + D + H 5

2 ) + SH 2

(cid:19)

Combining both bounds on (25) and (26) concludes the proof.

49

F General Lemmas

Lemma 34 (On Trajectory Regret to Sum of Decreasing Bounded Processes Regret). For Algorithm 1
and Algorithm 2 it holds that,

K
(cid:88)

H
(cid:88)

k=1

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1] =

Proof. The following relations hold.

K
(cid:88)

H
(cid:88)

k=1

t=1

E[ ¯V k−1
t

(sk

t ) − ¯V k

t (sk

t ) | Fk−1]

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s

k=1

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1]

(28)

=

(1)
=

(2)
=

(3)
=

K
(cid:88)

H
(cid:88)

(cid:88)

E[1(cid:8)s = sk

t

(cid:9) ¯V k−1

t

(s) − 1(cid:8)s = sk

t

(cid:9) ¯V k

t (s) | Fk−1]

k=1

t=1

s

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s

k=1

E[1(cid:8)s = sk

t

(cid:9) ¯V k−1

t

(s) + 1(cid:8)s (cid:54)= sk

t

(cid:9) ¯V k−1

t

(s) | Fk−1]

− E[1(cid:8)s = sk

t

(cid:9) ¯V k

t (s) + 1(cid:8)s (cid:54)= sk

t

(cid:9) ¯V k−1

t

(s) | Fk−1]

H
(cid:88)

(cid:88)

K
(cid:88)

t=1

s

H
(cid:88)

(cid:88)

k=1

K
(cid:88)

t=1

s

k=1

¯V k−1
t

(s) − E[1(cid:8)s = sk

t

(cid:9) ¯V k

t (s) + 1(cid:8)s (cid:54)= sk

t

(cid:9) ¯V k−1

t

(s) | Fk−1]

¯V k−1
t

(s) − E[ ¯V k

t (s) | Fk−1].

(29)

Relation (1) holds by adding and subtracting 1(cid:8)s (cid:54)= sk
(s) while using the linearity of expec-
tation. (2) holds since for any event 1{A} + 1{Ac} = 1 and since ∆V k−1
is Fk−1 measurable. (3)
holds by the deﬁnition of the update rule. If state s is visited in the kth episode at time-step t, then
both ¯V k
t (s) are updated. If not, their value remains as in the k − 1 iteration.

t (s), V k

(cid:9) ¯V k−1

t

t

t

F.1 The Good Set Lk and Few Lemmas

We introduce that set Lk. The construction is similar to [Dann et al., 2017] and we follow the one
formulated in [Zanette and Brunskill, 2019]. The idea is to partition the state-action space at each
episode to two sets, the set of state-action pairs that have been visited sufﬁciently often, and the ones
that were not.
Deﬁnition 2. The set Lk is deﬁned as follows.

Lk :=






(s, a) ∈ S × A :

1
4

(cid:88)

j<k

wj(s, a) ≥ H ln

SAH
δ(cid:48) + H






where wj(s, a) := (cid:80)H

t=1 wtj(s, a)

We now state some useful lemmas. See proofs in [Zanette and Brunskill, 2019], Lemma 6, Lemma 7,
Lemma 13.
Lemma 35. Outside the failure event, it holds that if (s, a) ∈ Lk, then

nk−1(s, a) ≥

1
4

(cid:88)

j≥k

wj(s, a) ,

which also implies that nk−1(s, a) ≥ H ln SAH
Lemma 36. Outside the failure event, it holds that

δ(cid:48) + H ≥ 1

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a) /∈Lk

wtk(s, a) ≤ ˜O(SAH).

50

Lemma 37. Outside the failure event, it holds that

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

wtk(s, a)
nk−1(s, a)

≤ ˜O(SA).

Combining these lemmas we conclude the following one.
Lemma 38. Outside the failure event, it holds that

K
(cid:88)

H
(cid:88)

(cid:34)(cid:115)

E

k=1

t=1

1
t , πk(sk

t )) ∨ 1

nk−1(sk

Proof. The following holds relations hold.

(cid:35)

| Fk−1

≤ ˜O(

√

SAT + SAH)

(cid:35)

| Fk−1

K
(cid:88)

H
(cid:88)

(cid:34)(cid:115)

E

k=1

t=1

K
(cid:88)

H
(cid:88)

(cid:88)

nk−1(sk

t )) ∨ 1

1
t , πk(sk
(cid:115)

wtk(s, a)

=

≤

≤

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a∈Lk

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a∈Lk

wtk(s, a)

(cid:115)

wtk(s, a)

1
nk−1(s, a) ∨ 1
(cid:115)

1
nk−1(s, a)

+

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a /∈∈Lk

wtk(s, a)

1
nk−1(s, a)

+ SAH.

(30)

The ﬁrst relation holds by deﬁnition. The second relation holds by the following argument. For the
ﬁrst term, if (s, a) ∈ Lk then by Lemma 35, nk−1(s, a) ≥ 1, and thus nk−1(s, a) ∨ 1 = nk−1(s, a).
The second term is bounded by taking the worst case for the fraction, which is nk−1(s, a) ∨ 1 ≥ 1.
The third relation holds by Lemma 36.

Consider the ﬁrst term in (30).

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a∈Lk

(cid:115)

wtk(s, a)

1
nk−1(s, a)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

K
(cid:88)

H
(cid:88)

(cid:88)

wtk(s, a)

H
(cid:88)

(cid:88)

wtk(s, a)
nk−1(s, a)

k=1

t=1

s,a∈Lk

k=1

t=1

s,a∈Lk

(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤

K
(cid:88)

H
(cid:88)

(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

wtk(s, a)

K
(cid:88)

H
(cid:88)

(cid:88)

t=1

s,a

k=1

t=1

s,a∈Lk

wtk(s, a)
nk−1(s, a)

k=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

T

√

=

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

s,a

wtk(s, a)
nk−1(s, a)

√

(cid:46) ˜O(

SAT ).

The ﬁrst relation holds by Cauchy-Schartz inequality. In the second relation, we replaced the sum in
the ﬁrst term to cover all of the state-action pairs, thus adding positive quantities. The third relation
holds since by deﬁnition (cid:80)H
s,a wtk(s, a) = H and T = KH. The last relation holds by
t=1
Lemma 37.

(cid:80)

Combining the result in (30) concludes the proof.

51

Lemma 39. Let u, v ≥ 0 be some non-negative constants. Outside the failure event,

K
(cid:88)

H
(cid:88)

k=1

t=1

and speciﬁcally,

(cid:20)

E

min

(cid:26)

u
t(cid:48), ak
nk−1(sk

t(cid:48)) ∨ 1

(cid:27)

(cid:21)

, v

| Fk−1

≤ ˜O(SAu + SAHv) ,

K
(cid:88)

H
(cid:88)

(cid:20)

E

k=1

t=1

u
nk−1(sk
t(cid:48), ak

t(cid:48)) ∨ 1

(cid:21)

| Fk−1

≤ ˜O(SAHu) .

Proof. The proof partially follows [Zanette and Brunskill, 2019], Lemma 12:

(cid:26)

(cid:20)

E

min

K
(cid:88)

H
(cid:88)

k=1

t=1

K
(cid:88)

H
(cid:88)

(cid:88)

u
t(cid:48), ak
nk−1(sk
t(cid:48)) ∨ 1
(cid:26)

wtk(s, a) min

(cid:27)

(cid:21)

, v

| Fk−1

u
nk−1(s, a) ∨ 1

(cid:27)

, v

(1)
=

(2)
≤

k=1

t=1

s,a

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a)∈Lk

(3)
(cid:46) SAu + SAHv

wtk(s, a)

u
nk−1(s, a)

+ v

K
(cid:88)

H
(cid:88)

(cid:88)

k=1

t=1

(s,a) /∈Lk

wtk(s, a)

(1) is from the deﬁnition of wtk(s, a) and the fact that nk−1(s, a) is Fk−1 measurable. In (2) we
divided the sum into state-actions in and outside Lk. For state-actions in Lk, we bounded the
minimum by the ﬁrst term, and otherwise we bounded it by H 2. Note that for any (s, a) ∈ Lk,
nk−1(s, a) ≥ 1, from Lemma 35. (3) is due to Lemmas 36 and 37.

The second part of the lemma is a direct result of ﬁxing v = u.

52

