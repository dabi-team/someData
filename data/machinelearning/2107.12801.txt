1
2
0
2

l
u
J

7
2

]

G
L
.
s
c
[

1
v
1
0
8
2
1
.
7
0
1
2
:
v
i
X
r
a

Robust Optimization Framework for Training Shallow Neural Networks
Using Reachability Method

Yejiang Yang and Weiming Xiang, Senior Member, IEEE

Abstract— In this paper, a robust optimization framework is
developed to train shallow neural networks based on reachabil-
ity analysis of neural networks. To characterize noises of input
data, the input training data is disturbed in the description
of interval sets. Interval-based reachability analysis is then
performed for the hidden layer. With the reachability analysis
results, a robust optimization training method is developed
in the framework of robust least-square problems. Then, the
developed robust least-square problem is relaxed to a semi-
deﬁnite programming problem. It has been shown that the
developed robust learning method can provide better robustness
against perturbations at the price of loss of training accuracy
to some extent. At last, the proposed method is evaluated on a
robot arm model learning example.

I. INTRODUCTION

Neural networks are extensively used in machine learn-
ing systems for their effectiveness in controlling complex
systems in a variety of research activities such as focusing
on the promise of neural networks in controlling nonlinear
systems in [1], providing a neural network control method
for a general type of nonlinear system in [2], controlling
industrial processes using adaptive neural network in [3],
investigating the problem of sampled data stabilization for
neural network control systems with a guaranteed cost in [4],
controlling autonomous vehicles using deep neural networks
in [5], etc. However, small perturbations in the input can
signiﬁcantly distort the output of the neural network.

The robustness and reliability of neural networks have
received particular attention in the machine learning com-
munity. For instance, adding Lyapunov constraints in the
training neural networks to enhance stabilization of learning
nonlinear system in [6], introducing performance-driven BP
learning for edge decision-making in [7], studying the adver-
sarial robustness of neural networks using robust optimiza-
tion [8], adversarial neural pruning and suppressing latent
vulnerability by proposing a Bayesian framework in [9],
studying the method to learn deep ReLU-based classiﬁers
that are provably robust against norm-bounded adversarial
perturbations in [10], identifying a trade-off between ro-
bustness and accuracy in the design of defenses against
adversarial examples in [11], an intelligent bearing fault diag-
nosis method based on stacked inverted residual convolution
neural networks is proposed in [12], etc. Yet by adding
adversarial examples in the training data may not improve
the worst-case performance of neural networks directly, and

Yejiang Yang is with Department of Electrical Engineering, Southwest

Jiaotong University, China yangyejiang0316@163.com

Weiming Xiang is with School of Computer and Cyber Sciences, Augusta

University, Augusta GA 30912, USA wxiang@augusta.edu

existing robust optimization frameworks for training neural
networks are focusing on training classiﬁers. There’s an
increasing demand for neural networks with a certain degree
of immunity to perturbations and the method for guaranteed
approximation error estimation between the system and its
neural network approximation in safety-critical scenarios.
The key to training robust neural networks is to mitigate
perturbations. Based on the methods proposed in [13]–[17],
the output reachability analysis of neural networks can be
performed in presence of perturbations in input data .

According to the Universal Approximation Theorem, a
shallow neural network could be able to solve any nonlinear
approximation problems [18]. In this work, a shallow neural
network is considered for saving computing resources with
its simple structure and the less conservative output of which
can be obtained using reachability methods. This paper
aims to train a robust neural network by mitigating the
effects caused by perturbations using reachable set methods.
Speciﬁcally, we form the output reachable set from the input
disturbed data described by the interval-based reachability
method. After forming the reachable set of the hidden layer,
the training of shallow neural network is performed by
least-square problems as an extension of
solving robust
the Extreme Learning Machine (ELM) method proposed in
[18]. The neural network trained by the proposed robust
optimization framework is then compared with ELM in terms
of reachable set estimation and the mean square error in
presence of perturbations in input data, as shown in a robot
modeling example. The results indicate that the robustness
using the proposed method has been improved and the trade-
off between accuracy and robustness is tolerable when it
comes to robust data-driven modeling problems.

The rest of the paper is organized as follows: preliminaries
and problem formulation are given in Section II. The main
result, robust optimization training for shallow neural net-
works using reachability method is presented in Section III.
In Section IV, a robot arm modeling example is provided to
evaluate our method. Conclusions are given in Section V.

Notations: N denotes the set of natural numbers, R repre-
sents the ﬁeld of real numbers, and Rn is the vector space
of all n-tuples of real numbers, Rn×n is the space of n × n
matrices with real entries. Given a matrix X ∈ Rn×n, the
notation X ≻ 0 means X is real symmetric and positive
deﬁnite. Given a matrix A , [ai,j] ∈ Rn×m, vec(A) =
[a1,1, · · · , an,m]⊤ is the vectorization of A. k·k2 and k·k∞
stand for Euclidean norm and inﬁnity norm, respectively. In
addition, in symmetric block matrices, we use * as an ellipsis
for the terms that are induced by symmetry.

 
 
 
 
 
 
II. PRELIMINARIES AND PROBLEM FORMULATION

A. Shallow Feedforward Neural Networks

A neural network consists of hidden layers with informa-
tion ﬂow from input layer to output layer. Each layer consists
of processing neurons which respond to the weighted input
receiving from other neurons in the form of:

Speciﬁcally, the following trivial assumption, which is
satisﬁed by most of the activation functions, is given for
the computation of set X (k+1).

Assumption 1: Assume that the following inequality for

activation function

φk(x1) ≤ φk(x2)

(7)

n

xi = φ(

wi,jxj + bi)

(1)

X

j=1
where xj ∈ R is the jth input of the ith neuron, n ∈ N is the
number of inputs for the ith neuron, xi ∈ R is the output of
the ith neuron, wi,j ∈ R is the weight from jth neuron to ith
neuron, bi ∈ R is the bias of the ith neuron, and φ : R → R
is the activation function.

The feedforward neural network considered in this paper
is a class of neural networks with no cycles and loops. Each
layer is connected with nearby layer by the weight matrix
and with a bias vector in the form of



(2)

w(k)
w(k)
1,2
1,1
w(k)
w(k)
2,2
2,1
...
...
w(k)
nk,1 w(k)
nk,2
1 , · · · , b(k)

W (k) = 






1,nk−1

2,nk−1

· · · w(k)
· · · w(k)
...
. . .
· · · w(k)

nk,nk−1







b(k) = [b(k)

nk ]⊤
(3)
in which nk ∈ N denotes the number of neurons in layer k,
thus the output of kth layer x(k) can be described by
x(k) = φk(W (k)x(k−1) + b(k)), ∀k = 1, · · · , L

(4)

where x(k) ∈ Rnk is the output vector of layer k.

Speciﬁcally, we consider a shallow feedforward neural
network with one hidden layer φ1 and the output layer φ2,
the mapping from the input vector x(0) of input layer to the
output vector x(2) can be expressed in the form of

x(2) = Φ(x(0))

(5)

where Φ(·) = φ2 ◦ φ1.

Remark 1: Compared with deep neural networks with
multiple hidden layers, shallow neural networks normally
consist of one hidden layer. Shallow neural networks rep-
resent a signiﬁcant class of neural networks in the machine
learning ﬁeld, e.g., Extreme Learning Machine (ELM) [18],
Broad Learning Systems (BLS) [19]. Notably, these shallow
neural networks are learners with universal approximation
and classiﬁcation capabilities provided with sufﬁcient neu-
rons and data, e.g., as shown in [18], [19].

B. Reachability of Neural Networks

In this paper, we introduce the concept of reachable set
of neural networks for robust training. Some critical notions
and results related to the reachability of neural networks are
presented as follows.

Given interval set X (k) = [x(k), x(k)] of kth layer,
in which x(k), x(k) ∈ Rnk are the lower- and upper-
bond of the kth layer’s output. The interval set X (k+1) =
[x(k+1), x(k+1)] for layer k + 1 of is deﬁned by

X (k+1) = [φk](X (k)).

(6)

holds for any two scalars x1 ≤ x2.
With Assumption 1, x(k+1)

in X (k+1) =
[x(k+1), x(k+1)] can be computed by the following lemma,
which is inspired by [14], [20].

and x(k+1)
i

i

Lemma 1: Given a shallow feedforward neural network
(5) and an input set X (0) = [x(0), x(0)], then the output
reachable set of X (1) and X (2) can be computed by

x(k)
i = φk(
x(k)
i = φk(

X

nk

j=1
nk

j=1

p(k)
i,j + bi)
p(k)
i,j + bi)

where p(k)

i,j and p(k)

i,j are

X

p(k)
i,j =

p(k)
i,j =

j

j

i,j x(k−1)
w(k)
i,j x(k−1)
w(k)
i,j x(k−1)
w(k)
i,j x(k−1)
w(k)

j

j

(

(

w(k)
w(k)
w(k)
w(k)

i,j ≥ 0
i,j < 0

i,j ≥ 0
i,j < 0

(8)

(9)

(10)

(11)

where k = 1, 2.

Proof: The proof is given in Appendix A.

Remark 2: Lemma 1 provides a formula to compute the
reachable set for both the hidden layer as well as output
layer. As indicated in [20], the interval arithmetic compu-
tation framework might lead to overly conservative over-
approximation as the number of hidden layers grows large.
However, for shallow neural networks considered in this
paper, there is only one hidden layer so that this interval
arithmetic computation framework proposed in Lemma 1
performs well in practice.

C. Problem Formulation

This paper aims to provide the method of training neural
networks to enhance their robustness and reliability using the
reachable set.

Given N arbitrary distinct input-output samples {ui, yi}
with ui ∈ Rn0 and yi ∈ Rn2 , shallow neural network training
aims to ﬁnd weights and biases W (k), b(k), k = 1, 2 for the
following optimization problem of

min
W (k),b(k),k=1,2

kvec(Φ(U ) − Y )k2

U = 

where U and Y are
u1
...
uN
y⊤
1
...
y⊤
N

Y = 

















u1,1
...
uN,1
y1,1
...
yN,1

= 




= 




...

· · · u1,n(0)
. . .
· · · uN,n(0)
y1,n(2)
· · ·
...
. . .
yN,n(2)
· · ·






(12)

(13)






.

(14)

In this paper, we utilize the ELM proposed in [18] to
train shallow neural networks. Unlike the most common
understanding that all the parameters of neural networks, i.e.,
W (k), b(k), k = 1, 2, need to be adjusted, weights W (1), and
the hidden layer biases b(1) are in fact not necessarily tuned
and can actually remain unchanged once random values
have been assigned to these parameters in the beginning of
training. By further letting b(2) = 0 and linear functions
as output activation functions as in ELM training, one can
rewrite Φ(U ) = W (2)H(U ) where

H(U ) = 

h(u1)
...
h(uN )









⊤

h1,1(u1)
...
hN,1(uN )

= 




h1,n(1) (u1)
...

· · ·
.. .
· · · hN,n(1) (uN )

⊤




(15)


in which h(ui) = φ1(W (1)ui + b(1)). Then, the training
process is then can be formulated in the form of

.

(16)

min
W (2)

vec(W (2)H(U ) − Y )
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

To incorporate robustness in the training process in (16),
the neural network is expected to be robust to the dis-
turbances injected in the input. Therefore, input data are
generalized from points to intervals containing perturbations
in the data, i.e., input data ui are purposefully crafted to
[ui, ui] where ui = ui − δi, ui = ui + δi with δi > 0
representing perturbations. The interval data set is denoted
by U. Moreover, the trained neural network is expected to be
capable of mitigating the changes caused by perturbations as
small as possible, thus the target data set is expected to stay
the same, i.e., Y .

With the interval data set U, the robust training problem
can be stated as follows, which is the main problem to be
addressed in this paper.

Problem 1: Given N arbitrary distinct input-output sam-
ples {ui, yi} with ui ∈ Rn0 and yi ∈ Rn2, and also
considering perturbations δi, how does one compute weights
and biases W (k), b(k), k = 1, 2 for the following robust
optimization problem of

min
W (k),b(k),k=1,2

max
δi,i=1,...N

kvec(Φ(U) − Y )k2

(17)

where Y is deﬁned by (14) and U are

[u⊤

1 , u⊤
1 ]
...
N , u⊤
N ]

[u⊤

U = 




[u1,1, u1,1]
...
[uN,1, uN,1]

· · ·
. . .
· · ·

= 









[u1,n(0) , u1,n(0) ]
...
[uN,n(0), uN,n(0)]




(18)

in which ui = ui − δi, ui = ui + δi with δi > 0.

Remark 3: From (17), the weights and biases in neural
networks are optimized to mitigate the negative effects
brought in by perturbations in the training process. Moreover,
in this paper, we will utilize ELM for shallow neural network
training, thus the robust optimization problem (17) can be
converted to the following optimization problem

min
W (2)

max
δi=1,...,N

vec(W (2)H(U) − Y )
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(19)

where

H(U) = 

[h]([u1, u1])
...
[h]([uN , uN ])

[h1,1, h1,1]
...
[hN,1, hN,1]

⊤





· · ·
. . .
· · ·

[h1,n(1) , h1,n(1) ]
...
[hN,n(1) , hN,n(1) ]

= 







⊤

.

(20)






Compared with optimization problem (16) for ELM, the
above robust optimization problem (19) can be viewed as
an extension for ELM training that involves perturbations in
input data.

III. ROBUST OPTIMIZATION TRAINING FRAMEWORK
In this section, a robust optimization-based training
method for shallow neural networks will be presented. As we
utilize the ELM training framework as proposed in [18], the
hidden layer weights W (1), and the hidden layer biases b(1)
can be randomly assigned. With the random assignment of
these parameters and using the reachability results proposed
in Lemma 1, H(U) can be obtained.

Proposition 1: Given a shallow feedforward neural net-
work (5) and a disturbed input data set U described by (18),
then the interval matrix H(U) in the form of (20) can be
computed by

hi,j = φ1(

hi,j = φ1(

X

n(1)

k=1
n1

k=1

p(1)
i,j,k + bj)
p(1)
i,j,k + bj)

where p(1)

i,j,k and p(1)

i,j,k are

X

p(1)
i,j,k =

p(1)
i,j,k =

(

(

w(1)
j,k ui,j w(1)
j,k ui,j w(1)
w(1)
j,k ui,j w(1)
w(1)
j,k ui,j w(1)
w(1)

j,k ≥ 0
j,k < 0

j,k ≥ 0
j,k < 0

(21)

(22)

(23)

(24)

and φ1 is the activation function satisfying Assumption 1.

Proof: Using the results in Lemma 1, (21)–(24) can be
obtained straightforwardly by letting k = 1, X (0) = U and
X (1) = H(U). The proof is complete.

With the interval matrix H(U), the next critical step in
robust training of shallow neural networks is solving robust
optimization problem (19) to compute weights W (2) of
output layer.

Proposition 2: Given a shallow feedforward neural net-
work (5), a disturbed input data set U described by (18)
then the interval matrix H(U)
and a target data set Y ,
for hidden layer is given in the form of (20), there exist
scalars γ > 0, λi,j > 0 and matrix W (2) such that
vec(W (2)H(U) − Y )

2 ≤ γ where γ is computed by

(cid:13)
(cid:13)

s.t.

min γ
(cid:13)
(cid:13)
γ − Ω1,1
∗
∗





0
Ω1,3
Ω2,2 Z ⊤
∗

I 


(cid:23) 0

(25)

in which Ω1,1 =
N
Y ), Ω2,2 =
i=1
Y ), · · · , vec(W (2)HN,n(1) − Y )].

P

P

n(1)
N
j=1 λi,j , Ω1,3 = vec⊤(W (2)H0 −
i=1
n(1)
j=1 λi,j Qi,j and Z = [vec(W (2)H1,1 −

Proof: In order to develop a tractable algorithm to solve
(19), we propose the following equivalent representation for
interval matrix H(U) in (20), i.e.,

P

P

N

n(1)

H(U) = H0 +

τi,j Hi,j

(26)

where τi,j ∈ [−1, 1] and H0, Hi,j are deﬁned by

i=1
X

j=1
X

h

2

1,n(1) +h
...
N,n(1) +h

1,n(1)

N,n(1)

h

2

⊤








h1,1+h1,1
2

...

· · ·
. . .

· · ·

hN,1+hN,1
2
0(i−1)×(j−1)
01×(j−1)

H0 = 





Hi,j = 




0(i−1)×1
hi,j −hi,j
2

0

(i−1)×n(1)−j
0

1×(n(1)−j)
(N −i)×n(1)−j

0(N −i)×(j−1) 0(N −i)×1 0

(27)

⊤

.

(28)






Based on (26)–(28),

vec(W (2)H(U) − Y

k2 can be ex-

pressed as

(cid:13)
(cid:13)

=

vec(W (2)H(U) − Y )
2
(cid:13)
(cid:13)
N
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
=ξ⊤
(cid:13)
(cid:13)
(cid:13)

H0 +

W (2)

i=1
X

vec

Θξ









(cid:1)





− Y

τi,j Hi,j

n(1)

j=1
X

where ξ = [1, τ ]⊤, τ = [τ1,1, · · · , τN,n(1) ] and





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(29)

(30)

Θ =

Θ1,1 Θ1,2
Θ2,2(cid:21)

∗
in which Θ1,1 = vec⊤(W (2)H0 − Y )vec(W (2)H0 − Y ),
Θ1,2 = vec⊤(W (2)H0 − Y )Z with Z = [vec(W (2)H1,1 −
Y ), · · · , vec(W (2)HN,n(1) − Y )], and Θ2,2 = Z ⊤Z.

(cid:20)

Therefore, the robust optimization problem (19) can be

rewritten to

ξ⊤Θξ

min
W (2)
s.t. kτ k∞ ≤ 1

(31)

where Θ and ξ are deﬁned by (29) and (30).
Moreover, letting Qi,j = diag{01,i×j−1, 1, 0

N ×n(1)−i×j },
we can see that τ ⊤Qi,jτ ≤ 1 will deduce kτ k∞ ≤ 1.
Furthermore, τ ⊤Qi,jτ ≤ 1 equals to

ξ⊤

0

1
∗ −Qi,j
(cid:20)

(cid:21)

ξ ≥ 0

(32)

Thus, we can formulate the following optimization problem
ξ⊤Θξ

min
W (2)
1
∗ −Qi,j

0

(cid:20)

s.t. ξ⊤

ξ ≥ 0

(cid:21)

(33)

for all i = 1, . . . , N , j = 1, . . . , n(1). It is noted that the
solution of (33) also satisﬁes optimization problem (31).

Using S-procedure and letting γ = ξ⊤Θξ, we can formu-

late an optimization problem with λi,j > 0 as follows

min γ

s.t.

γ − Θ1,1 −Θ1,2
−Θ2,2(cid:21)

∗

(cid:20)

−

N

n(1)

i=1
X

j=1
X

0

1
∗ −Qi,j

λi,j

(cid:20)

(cid:23) 0

(cid:21)

(34)

which ensures (33) holds.

Based on Schur Complement formula, it is equivalent to

min γ

(cid:23) 0

(35)

s.t.

γ − Ω1,1
∗
∗

Ω1,3
0
Ω2,2 Z ⊤
∗



P

P


N
i=1
N
i=1

P
k2. The proof is complete.

I 

n(1)
j=1 λi,j, Ω1,3 = vec⊤(W (2)H0 −
where Ω1,1 =
n(1)
Y ) and Ω2,2 =
j=1 λi,jQi,j. Therefore, we have
that the optimized γ ≥ ξ⊤Θξ, which implies that γ ≥
P
vec(W (2)H(U) − Y
Remark 4: Proposition 2 suggests that robust optimization
(cid:13)
problem (19) can be formulated in the form of Semi-Deﬁnite
(cid:13)
Programming (SDP) problem, so that the weights W (2) of
the output layer can be efﬁciently solved with the help of
existing SDP tools. By solving the optimization problem
(25), the obtained weights W (2) are designed to make the
approximation error γ between disturbed input data set U
and target set Y as small as possible, which implies a robust
training performance of shallow neural network (5).

(cid:1)

Remark 5: Since the robust

training process considers
perturbations imposed on the input data set and optimizes
weights to minimize the approximation error, the neural
network tends to be able to tolerate noises better than those
trained by the traditional training process. On the other hand,
also due to the consideration of perturbations which are
purposefully crafted in the input data set and is a player
that is always playing against weights in robust optimization
training, the approximation error would increase compared
with traditional training. This is the trade-off between ro-
bustness and accuracy in neural network training, and it will
be illustrated in a training example later.

In summary, the robust optimization training algorithm for
shallow neural networks is presented in Algorithm 1, which
consists of three major components.
Initialization: Since we employ ELM to train shallow neural
networks, the weights W (1) and biases b(1) of the hidden
layer are randomly assigned. In addition, the biases of output
layer is set to b(2) = 0. According to [18], W (1), b(1) and
b(2) will remain unchanged in the rest of training process.
Reachable Set Computation: The reachability analysis
comes into play for the computation of H(U),
the
reachable set of hidden layer. The computation is carried
out based on (21)–(24) in Proposition 1.
Robust Optimization: This is the key step to achieve
robustness in training shallow neural networks. Based on

i.e.,

Algorithm 1: Robust Optimization Training of Shal-
low Neural Networks
: Input data set U , output data set Y .
Input
Output: Weights and biases W k, b(k), k = 1, 2 for

Shallow neural network Φ.

/* Initialization

*/

1 Generate perturbed input data interval set U;
2 Randomly assign weights W (1) and biases b(1), and

b(2) = 0;
/* Reachable Set Computation

3 Compute the hidden layer output set H(U) using

(21)–(24);
/* Robust optimization

4 Solve SDP problem (25) to obtain output layer

*/

*/

weights W (2).

(cid:5) (cid:6)
(cid:2) (cid:3)

(cid:7)

(cid:1) =

(cid:1)

(cid:1)(cid:3)

(cid:1) =

(cid:2)

(cid:4)

θ

(cid:2)

θ

(cid:1)

Fig. 1. Robotic arm with two joints. The normal working zone of (θ1, θ2)
12 , 7π
is colored in green θ1, θ2 ∈ [
12 ]. The buffering zone is in yellow
3 , 5π
θ1, θ2 ∈ [
3 ] ∪
2π
3 , 2π].
[

3 ]. The forbidden zone is θ1, θ2 ∈ [0, π

12 , 2π

12 ] ∪ [

7π

5π

π

Proposition 2, the robust optimization training process is
converted to an SDP problem in the form of (25), which
can be solved by various SDP tools.

Remark 6: As shown in SDP problem (25), the computa-
tional cost of Algorithm 1 heavily depends on the number
of decision variables λi,j which is N n(1). The value of
N n(1) is normally dominated by the number of input data
N which usually is a large number as sufﬁcient input data
is normally required for desired training performance. To
reduce the computational cost in practice, we can modify
λi,j such as particularly letting τ1,j = · · · = τN,j to relax
the computational burden caused by a large number of input
data. However, the price to pay here is that the result is a
sub-optimal solution instead of the optimal solution to (25).

IV. EVALUATION

In this section, a learning forward kinematics of a robotic
arm model with two joints proposed in [14] is used to
evaluate our developed robust optimization training method.
The robotic arm model is shown in Fig. 1.

The learning task is using a feedforward neural network to
predict the position (x, y) of the end with knowing the joint
angles (θ1, θ2). The input space [0, 2π]×[0, 2π] for (θ1, θ2) is
classiﬁed into three zones for its operations: Normal working
3 , 5π
zone θ1, θ2 ∈ [ 5π
12 ] ∪
12 , 2π
[ 7π
3 , 2π]. The

3 ] and forbidden zone θ1, θ2 ∈ [0, π

12 ], buffering zone θ1, θ2 ∈ [ π

3 ] ∪ [ 2π

12 , 7π

Fig. 2.
The reachable sets of the neural network trained by traditional
ELM method for robot arm model. Maximal radius of output sets subject
to disturbed input data set is 3.106.

TABLE I
RADIUS OF OUTPUT REACHABLE SETS AND MEAN SQUARE ERROR

Method

Radius
Traditional ELM 3.1060
1.9494

Algorithm 1

MSE
0.0076
0.0643

detailed formulation for this robotic arm model and neural
network training can be found in [14].

testing output data,

To show the advantage of robust learning, we ﬁrst train a
shallow neural network using the traditional ELM method.
Then, assuming the injected disturbances are δi = 0.01.
By using Lemma 1 and choosing the maximal deviation
of outputs as the radius for all
the
output reachable set for all perturbed inputs are shown in
Fig. 2. Moreover, we train a shallow neural network using
Algorithm 1, i.e., the robust optimization training method.
The output sets for perturbed inputs are shown in Fig. 3. It
can be explicitly observed that the neural network trained
by the robust optimization method has tighter reachable
sets which means the neural network is less sensitive to
disturbances. Therefore, we can conclude that the neural
network trained by the robust optimization method is more
robust to noises injected in input data. On the other hand,
comparing Figs. 2 and 3, the deviation of neural network
output from output data is increased by observation, i.e, the
training accuracy is sacriﬁced for improving robustness.

Furthermore, the trade-off between robustness and accu-
racy mentioned in Remark 5 are elucidated in Table I. It can
be seen that robust learning provides a better tolerance in
input data noises but yields less accuracy than the traditional
learning process, i.e., a larger Mean Square Error (MSE).

V. CONCLUSIONS

A robust optimization learning framework is proposed
in this paper for shallow neural networks. First, the input
set data are generalized to interval sets to characterize
injected noises. Then based on the layer-by-layer reachability

[9] D. Madaan, J. Shin, and S. J. Hwang, “Adversarial neural pruning
with latent vulnerability suppression,” in International Conference on
Machine Learning. PMLR, 2020, pp. 6575–6585.

[10] E. Wong and Z. Kolter, “Provable defenses against adversarial ex-
amples via the convex outer adversarial polytope,” in International
Conference on Machine Learning. PMLR, 2018, pp. 5286–5295.

[11] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan,
“Theoretically principled trade-off between robustness and accuracy,”
in International Conference on Machine Learning. PMLR, 2019, pp.
7472–7482.

[12] D. Yao, H. Liu, J. Yang, and X. Li, “A lightweight neural network
with strong robustness for bearing fault diagnosis,” Measurement, vol.
159, p. 107756, 2020.

[13] W. Xiang, H.-D. Tran, and T. T. Johnson, “Reachable set computation
and safety veriﬁcation for neural networks with ReLU activations,”
arXiv preprint arXiv:1712.08163, 2017.

[14] ——, “Output reachable set estimation and veriﬁcation for multilayer
neural networks,” IEEE Transactions on Neural Networks and Learn-
ing Systems, vol. 29, no. 11, pp. 5777–5783, 2018.

[15] W. Xiang, H.-D. Tran, J. A. Rosenfeld, and T. T. Johnson, “Reachable
set estimation and safety veriﬁcation for piecewise linear systems
with neural network controllers,” in 2018 Annual American Control
Conference (ACC).
IEEE, 2018, pp. 1574–1579.

[16] H.-D. Tran, D. M. Lopez, P. Musau, X. Yang, L. V. Nguyen, W. Xiang,
and T. T. Johnson, “Star-based reachability analysis of deep neural
networks,” in International Symposium on Formal Methods. Springer,
2019, pp. 670–686.

[17] W. Xiang and T. T. Johnson, “Reachability analysis and safety
veriﬁcation for neural network control systems,” arXiv preprint
arXiv:1805.09944, 2018.

[18] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, “Extreme learning machine:
theory and applications,” Neurocomputing, vol. 70, no. 1-3, pp. 489–
501, 2006.

[19] C. P. Chen and Z. Liu, “Broad learning system: An effective and
efﬁcient
the need for deep
learning system without
architecture,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 29, no. 1, pp. 10–24, 2017.

incremental

[20] W. Xiang, H.-D. Tran, X. Yang, and T. T. Johnson, “Reachable set
estimation for neural network control systems: a simulation-guided
approach,” IEEE Transactions on Neural Networks and Learning
Systems, 2020, DOI: 10.1109/TNNLS.2020.2991090.

A. Proof of Lemma 1

APPENDIX

We shall focus on Xk ⊆ [x(k), x(k)], ∀k = 1, 2 . Consid-

ering the kth layer, the following inequalities
i,j x(k−1)
w(k)
i,j x(k−1)
w(k)
hold for any input of this layer x(k−1) ∈ [x(k−1), x(k−1)].

i,j
≤ pi,j

≥ p

j

j

Using the monotonic property (7) of activation function

φk in Assumption 1, it leads to

x(k+1)
i

≥ φk(

x(k+1)
i

≤ φk(

X

nk

j=1
nk

j=1

p(k)
i,j + bi)
p(k)
i,j + bi)

(36)

(37)

X
for any input x(k−1) ∈ [x(k−1), x(k−1)]. Thus, by iterat-
ing the above process from k = 1, 2, given any x(0) ∈
[x(0), x(0)], the output of the neural network (4) satisﬁes

x(k) ∈ [x(k), x(k)], k = 1, 2

(38)

where [x(k), x(k)] covers the outputs of reachable set com-
putation (6) for the kth layer, respectively. Therefore, for
any input set X0 ⊆ [x(0), x(0)],
the output set Xk ⊆
[x(k), x(k)], ∀k = 1, 2. The proof is complete.

Fig. 3.
The reachable sets of the neural network trained by robust
optimization learning method for robot arm model. The maximal radius
of output sets subject to disturbed input data set is 1.9494 which means the
robustness has been improved. On the other hand, the deviation of neural
network outputs from output data is increased by our observation, which
means the training accuracy has been decreased. Therefore, the trade-off
between robustness and training accuracy exists.

analysis for neural networks, the output sets of the hidden
layer are computed, which play a critical role in the robust
optimization training process. The robust training problem is
formulated in terms of robust least-squares problems, which
can be then converted to an SDP problem. The trade-off
between robustness and training accuracy is observed in
the proposed framework. A robot arm modeling example is
provided to evaluate our method.

REFERENCES

[1] K. J. Hunt, D. Sbarbaro, R. ˙Zbikowski, and P. J. Gawthrop, “Neural
networks for control systems—a survey,” Automatica, vol. 28, no. 6,
pp. 1083–1112, 1992.

[2] S. S. Ge, C. C. Hang, and T. Zhang, “Adaptive neural network control
of nonlinear systems by state and output feedback,” IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 29, no. 6,
pp. 818–828, 1999.

[3] T. Wang, H. Gao, and J. Qiu, “A combined adaptive neural network and
nonlinear model predictive control for multirate networked industrial
process control,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 27, no. 2, pp. 416–425, 2015.

[4] Z.-G. Wu, P. Shi, H. Su, and J. Chu, “Exponential stabilization for
sampled-data neural-network-based control systems,” IEEE Transac-
tions on Neural Networks and Learning Systems, vol. 25, no. 12, pp.
2180–2190, 2014.

[5] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of
the 40th International Conference on Software Engineering, 2018, pp.
303–314.

[6] K. Neumann, A. Lemme, and J. J. Steil, “Neural learning of stable
dynamical systems based on data-driven lyapunov candidates,” in 2013
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, 2013, pp. 1216–1222.

[7] H. He, M. Chen, G. Xu, Z. Zhu, and Z. Zhu, “Learnability and
robustness of shallow neural networks learned with a performance-
driven bp and a variant pso for edge decision-making,” arXiv preprint
arXiv:2008.06135, 2020.

[8] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,” arXiv
preprint arXiv:1706.06083, 2017.

