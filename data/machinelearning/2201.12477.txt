2
2
0
2

n
u
J

1

]
T
I
.
s
c
[

2
v
7
7
4
2
1
.
1
0
2
2
:
v
i
X
r
a

An Indirect Rate-Distortion Characterization

for Semantic Sources: General Model and the

1

Case of Gaussian Observation

Jiakun Liu, Shuo Shao, Wenyi Zhang, and H. Vincent Poor

Abstract

A new source model, which consists of an intrinsic state part and an extrinsic observation part, is

proposed and its information-theoretic characterization, namely its rate-distortion function, is deﬁned

and analyzed. Such a source model is motivated by the recent surge of interest in the semantic aspect of

information: the intrinsic state corresponds to the semantic feature of the source, which in general is not

observable but can only be inferred from the extrinsic observation. There are two distortion measures,

one between the intrinsic state and its reproduction, and the other between the extrinsic observation

and its reproduction. Under a given code rate, the tradeoff between these two distortion measures is

characterized by the rate-distortion function, which is solved via the indirect rate-distortion theory and

is termed as the semantic rate-distortion function of the source. As an application of the general model

and its analysis, the case of Gaussian extrinsic observation is studied, assuming a linear relationship

between the intrinsic state and the extrinsic observation, under a quadratic distortion structure. The

semantic rate-distortion function is shown to be the solution of a convex programming problem with

respect to an error covariance matrix, and a reverse water-ﬁlling type of solution is provided when the

model further satisﬁes a diagonalizability condition.

J. Liu and W. Zhang are with Department of Electronic Engineering and Information Science, University of Science and

Technology of China, Hefei, China (liujk@mail.ustc.edu.cn, wenyizha@ustc.edu.cn), S. Shao is with Department of Electronic

Engineering, Shanghai Jiaotong University, Shanghai, China (shuoshao@sjtu.edu.cn), and H. Vincent Poor is with Department

of Electrical Engineering, Princeton University, Princeton, NJ, USA (poor@princeton.edu). (Co-ﬁrst authors: J. Liu and S. Shao;

Corresponding authors: W. Zhang and H. V. Poor)

Preliminary results of this work have been presented in part at the IEEE International Symposium on Information Theory

(ISIT), 2021 [1].

June 2, 2022

DRAFT

 
 
 
 
 
 
A standard approach to describe an information source is to model a source as a stochastic

I. INTRODUCTION

2

process

, and when the stochastic process is memoryless, it sufﬁces to model a source as

a random variable1 X with a given probability distribution p(x) [2] [3]. In this paper, we study

Xi

{

}

a new source model, which consists of an intrinsic state process and an extrinsic observation

process. In the memoryless case, we can describe such a source model as a pair of random

variables (S, X), with a given joint probability distribution p(s, x), deﬁned over an appropriate

product alphabet

.

S × X

In order to characterize the information-theoretic aspect of such a source, consider the problem
of compressing the source (S, X) so as to reproduce, in a lossy sense, a reproduction ( ˆS, ˆX) over
a reproduction product alphabet ˆ
R

. Of course, a pair of distortion measures, ds :

ˆ
S 7→

S ×

ˆ
X

S ×

R, are introduced correspondingly. Here, the subscript s stands for “state” and

and do :

ˆ
X 7→

X ×

the subscript o stands for “observation”. A key point of the problem is that the compressor only

has access to X, the extrinsic observation; — while S, the intrinsic state, remains unrevealed.

The situation is illustrated in Figure 1.

p(s)

ds(s, ˆs)

S

ˆS

s)

p(x
|

X

Encoder

do(x, ˆx)

ˆX

W

Decoder

Fig. 1.

Illustration of a semantic source and its lossy compression.

Our source model, termed as a semantic source in the sequel, is motivated by the recent surge

of interest in the semantic aspect of information. In a number of applications that may beneﬁt

from taking into account the “semantic” feature of information, it is adequate to adopt a goal-

oriented perspective; that is, the destination’s interest in obtaining a piece of information is to

accomplish a certain goal. Furthermore, it is customary to adopt an inference-theoretic problem

formulation, which casts the accomplishment of the said goal as solving a statistical inference

1In this paper, random variables can be drawn from general alphabets, so random vectors are vector-valued random variables.

June 2, 2022

DRAFT

3

problem. The reproduction of the intrinsic state S corresponds to the semantic inference part of

the source, and the reproduction of the extrinsic observation X corresponds to the conventional

lossy compression part of the source.

We give two examples of the above consideration:

• Systems that support MPEG Video Coding for Machines (VCM) are becoming popular in

applications. In VCM, both the video itself and its features are reproduced: the video signal

is for human vision, and the features are for machine vision tasks [4] [5] [6]. Treating the

video as a semantic source, the video signal itself corresponds to its extrinsic observation,

and the underlying features correspond to its intrinsic state, so as to embody the semantic

aspect of the video. Usually the code rate required for reproducing features can be drastically

lower than that required for reproducing the video signal itself. Intuitively, features typically

have much smaller rate distortion functions and hence can be described with many fewer

bits, compared with video signals. For instance, previous works have shown that neural

network-based learning techniques can extract a very small amount of data from video

signals to satisfy the need of action recognition, target classiﬁcation and many other tasks

[7] [8]. In contrast, traditional video coding schemes such as H.264/AVC/MPEG-4 and

H.265/HEVC/MPEG-H Part 2 only target at reproducing the video signal with high ﬁdelity,

but may perform poorly for machine vision purposes [9].

• In coding of speech signals, the semantic aspect is embodied as a sequence of text words,

which, of course, can only be inferred from the speech signal itself. Treating the speech

as a semantic source, the words correspond to its intrinsic state and the speech signal

corresponds to its extrinsic observation. It is the usual case that both the words and the

speech signal are desirable, because the words carry the meaning of speech, and the speech

signal waveform may help us infer the stress and emotion of the speaker [10], and may

further help us accomplish tasks like speaker recognition and speaker veriﬁcation [11].

Our main contributions include:

• We propose a theoretical framework based on rate distortion theory for characterizing

semantic information.

• We deﬁne and derive a single-letter expression for the semantic rate distortion function.

• When the extrinsic observation is Gaussian and satisﬁes a linear relationship with the

June 2, 2022

DRAFT

4

intrinsic state, we reduce the calculation of the semantic rate distortion function to a con-

vex programming problem, which is tractable with standard scientiﬁc computing software.

Furthermore, under a diagonalizability condition, we obtain a weighted reverse water-ﬁlling

solution for the semantic rate distortion function.

We give a brief overview of related works in the remaining part of this section. Then we

provide a formal mathematical description of the semantic source model and the corresponding

semantic rate-distortion problem formulation in Section II, for which we establish the semantic

rate-distortion function in general form in Section III. As an application of the general results,

in Section IV we turn to a case study of Gaussian extrinsic observation, assuming a linear

relationship between the intrinsic state and the extrinsic observation, under a quadratic distortion

structure. Therein, we formulate a convex programming to solve the semantic rate-distortion

function. When the Gaussian observation model further satisﬁes a diagonalizability condition,

we develop a reverse water-ﬁlling type of solution in Section V. Finally we conclude this paper

in Section VI.

A. Related Works

The ﬁrst formulation in Shannon’s information theory is lossless source coding, wherein a

sequence of symbols obeying a certain probabilistic law is represented as a bit string (i.e., a

codeword) by an encoder, and the decoder reproduces, based upon the codeword, the original

sequence of symbols, with success probability exactly one or asymptotically approaching one.

Hence, the coding is solely determined by the probabilistic model of the source, and there is

certainly no role of the semantic aspect of the source. This is also consistent with Shannon’s

remark in his landmark paper [2], saying “these semantic aspects of communication are irrelevant

to the engineering problem.”

In a broad sense, however, the lossy source coding formulation in Shannon’s information

theory, namely, the rate-distortion theory [12], has provided a means of studying the semantic

aspects of a source. This is because the coding is not solely determined by the probabilistic

model of a source, but is also affected by a distortion measure, which may be deﬁned in a rather

versatile way so as to capture the “utility” when the source is reproduced at the decoder.

Our present work goes one step further, by endowing a source with a state-observation structure

and studying the rate distortion function of such a source model. This model captures the fact that

June 2, 2022

DRAFT

5

the semantic aspects of a source are generally embedded as intrinsic features, and hence should

be characterized by studying the reproduction of the intrinsic state, in addition to the reproduction

of the extrinsic observation. Our treatment of semantic aspects of sources is also in line with the

recently heightened interest in the development of 5G and beyond wireless systems [13] [14]

[15], where for many applications the semantic aspects correspond to the accomplishment of

certain inference goals. Hence, if we consider an information theoretic characterization of such

a “semantic” source, the task of coding is to efﬁciently encode the extrinsic observation so that

the decoder can infer both the intrinsic state and the extrinsic observation, subject to ﬁdelity

criteria on both, simultaneously. Our problem formulation and approach are closely related to two

variants of the standard rate distortion theory, namely, indirect rate distortion function and rate

distortion function under multiple distortion measures; see our discussion following Theorem 1

in Section III.

The inference-theoretic goal-oriented approach adopted in our problem formulation does not

seek a task-independent universal deﬁnition of semantic information, which is outside the scope

of the present paper; for some attempts in that regard, see, e.g., [16] [17] [18] [19] for a few

representative works that undertake drastically different approaches.

As related topics, the information bottleneck [20] [21] and the privacy funnel [22] [23] are, in

a certain sense, dual concepts, and both place constraints in terms of mutual information. The

underlying idea of the information bottleneck is, in a broad sense, similar to ours. Speciﬁcally,

there one generates a reproduction based upon the extrinsic observation, minimizing the mutual

information between the extrinsic observation and the reproduction, while maintaining a level

of mutual information between the intrinsic state and the reproduction. But for the information

bottleneck problem formulation, there is neither explicit distortion measure, nor operational

deﬁnition of lossy compression.

Task-based compression has been approached mainly from the perspective of quantizer design

[24]. It has been demonstrated that steering the design goal according to the task leads to

performance beneﬁts compared with conventional task-agnostic approach, a conclusion in line

with what we advocate in our work. The perception-distortion tradeoff [25] imposes an additional

constraint on the probability distribution of the reproduction. None of these related works

proposes to decompose the information source into intrinsic and extrinsic parts as in our work, let

alone investigate the joint behavior of them. In [26], a similar intrinsic state-extrinsic observation

June 2, 2022

DRAFT

6

model is studied, but the encoder is designed based on the marginal distribution of the extrinsic

observation only.

II. SYSTEM MODEL AND PROBLEM FORMULATION

As already outlined in the introduction, we model a memoryless semantic source as a pair

of random variables (S, X) that are correlated with joint probability distribution p(s, x). The

semantic aspect is embodied in the intrinsic state S, which is not observable but can only be

inferred from the extrinsic observation X. In order to characterize the rate-distortion behavior

of the semantic source, we consider a sequence of independent and identically distributed (i.i.d.)

samples of (S, X), denoted as (Si, Xi)i∈N, and denote its length-n block as (Sn, X n).

The i.i.d. source model is an idealistic scenario for our information-theoretic study. Real-

world data generally exhibit sophisticated memory structures. A particularly interesting scenario

is when the intrinsic state is a Markov chain, and the extrinsic observation obeys a hidden Markov

model (HMM) [27]. Extensions of our approach for semantic source models with memory are

left for future research.

The lossy compression of a semantic source has been illustrated in Figure 1. The encoder

only has access to a length-n block of the extrinsic observation sequence X n, and the decoder
has two tasks: reproducing the intrinsic state block as ˆSn under a state distortion measure ds,
and reproducing the extrinsic observation block as ˆX n under an observation distortion measure

do. The encoder and the decoder are connected via a bit pipe in which the codeword W of nR

bits is transferred from the encoder to the decoder, where R is thus the code rate of the lossy

compression system.

Below we provide a formal description of the lossy compression problem of a semantic source.
R+ be two given distortion measures, deﬁned over
. The extended

and the reproduction product alphabet ˆ

S ×
the source product alphabet

R+ and do :

ˆ
X →

ˆ
S →

Let ds :

X ×

ˆ
X

S ×

S × X

block-wise distortion measures are as follows:

ds(sn, ˆsn) =

do(xn, ˆxn) =

1
n

1
n

ds(si, ˆsi),

n

i=1
X
n

do(xi, ˆxi).

i=1
X

June 2, 2022

(1)

(2)

DRAFT

7

We claim a tuple (R, Ds, Do) to be achievable, if for any ǫ > 0 and all sufﬁciently large n,

there exist the following functions:

• Encoding function f :

W = f (X n);

n

X

7→ {

1, 2, . . . , 2⌊n(R+ǫ)⌋

}

which generates the codeword W as

• State decoding function gs :

1, 2, . . . , 2⌊n(R+ǫ)⌋

{

ˆ
n, such that
S

} 7→

where ˆSn = gs(f (X n));

E

ds(Sn, ˆSn)
h

i

≤

Ds + ǫ,

• Observation decoding function go :

1, 2, . . . , 2⌊n(R+ǫ)⌋

ˆ
X

} 7→

n, such that

E

where ˆX n = go(f (X n)).

{
do(X n, ˆX n)
h

Do + ǫ,

≤

i

(3)

(4)

It is clear that the state decoding function gs and the observation decoding function go together

constitute the decoder illustrated in Figure 1.

Our goal is to characterize the region of all achievable (R, Ds, Do) tuples. Hence, we deﬁne

the semantic rate distortion function as follows2:

R(Ds, Do) = inf

R : (R, Ds, Do) is achievable
}

.

{

(5)

Clearly, characterizing the semantic rate distortion function R(Ds, Do) is equivalent to charac-

terizing the achievable region of (R, Ds, Do).

We will also consider a variant of the distortion constraint; that is, the state distortion and the

observation distortion are linearly combined to yield a single overall distortion. Hence, instead

of (3) and (4), the decoding functions are required to satisfy the following weighted distortion

constraint:

wsds(Sn, ˆSn) + wodo(X n, ˆX n)
h
where ws and wo are non-negative weighting coefﬁcients.

E

i

≤

¯D + ǫ,

(6)

It is also natural to generalize the system model to include several intrinsic state variables

each associated with a speciﬁed reproduction and a distortion. Such a semantic source is de-

scribed by a tuple of random variables, (S0, S1, . . . , Sk−1, X), with joint probability distribution

2This is the operational deﬁnition of a rate distortion function, which has been widely used (see, for example, [3] [28] [29]).

June 2, 2022

DRAFT

p(s0, s1, . . . , sk−1, x) over

S0 × S1 ×

. . .

× S

k−1 × X

, where each Sj

is an intrinsic state

reﬂecting a certain semantic aspect of the source. The decoder now consists of an observation

8

decoding function and k state decoding functions, among which gs,j maps the codeword W
1, 2, . . . , 2⌊n(R+ǫ)⌋

into a reproduction sequence ˆSn
j

to satisfy

∈

{

}

The notion of achievability can be deﬁned in a similar fashion with respect to the tuple (R, Ds,0,

j , ˆSn
j )

E

ds,j(Sn
h

i

Ds,j + ǫ.

≤

(7)

Ds,1, . . ., Ds,k−1, Do), and the semantic rate distortion function is consequently deﬁned as

R(Ds,0, Ds,1,

· · ·

, Ds,k−1, Do) = inf

R : (R, Ds,0, Ds,1,

{

, Ds,k−1, Do) is achievable
}

.

· · ·

(8)

Examples of such semantic sources with multiple semantic aspects can be found in [5] [9],

which consider a hierarchy of image or video features, each feature associated with a quality

metric.

III. SEMANTIC RATE DISTORTION FUNCTION

In this section, we establish in the following theorem a single-letter characterization of the

semantic rate distortion function R(Ds, Do) deﬁned in Section II.

Theorem 1: For a given semantic source (S, X) with p(s, x) over

, reproduction alphabet

S ×X

, and distortion measures ds and do, the semantic rate distortion function R(Ds, Do) is as

ˆ
X

ˆ
S ×
follows:

R(Ds, Do) = min
p(ˆs,ˆx|x)

I(X; ˆS, ˆX)

s.t. E

E

where

do(X, ˆX)
h
ˆds(X, ˆS)
h

i

≤

i

≤

Do,

Ds,

ˆds(x, ˆs) = E [ds(S, ˆs)

x] =

|

x)ds(s, ˆs),

p(s

|

and S, X, ˆS, ˆX constitute a Markov chain S

Proof: See Appendix I. (cid:3)

s∈S
X

X

↔

↔

( ˆS, ˆX).

(9)

(10)

(11)

(12)

Here we brieﬂy discuss the basic idea of the proof of Theorem 1. There are two main

ingredients in the problem formulation: an indirect rate distortion problem which has been studied

June 2, 2022

DRAFT

9

in [30] [31] [32, Chap. 3, Sec. 5] [33], and a rate distortion problem with several distortion

constraints which has been studied in [34, Sec. VII] [3, Prob. 10.19] [35, Prob. 7.14]. A key is
to recognize reproducing ˆS as an indirect rate distortion problem, for which the state distortion
between S and ˆS can be equivalently converted to a distortion between X and ˆS. Indeed, the

converted distortion is nothing but the conditional expectation of the original state distortion

ds(S, ˆs), over p(s

x). This conversion hence circumvents the difﬁculty due to the absence of

|

access to S at the encoder. The detailed derivation, which is based on a uniﬁed treatment in

[33], is given in Appendix I.

We note that the semantic rate distortion function can be non-trivial even for the special case

where S is a deterministic function of X, because from a lossy reproduction of X it is generally

impossible to reproduce S in a lossless fashion. Speciﬁcally, suppose that S = g(X). Then
ˆds(x, ˆs) can be simpliﬁed into

ˆds(x, ˆs) =

s∈S
X

x)ds(s, ˆs) = ds(g(x), ˆs).

p(s

|

(13)

Similar to standard rate distortion functions, a corollary of the semantic rate distortion function

as given by Theorem 1 is the following regarding monotonicity and convexity.

Corollary 1: The semantic rate distortion function R(Ds, Do) in Theorem 1 has the following

properties:

• R(Ds, Do) is monotonically nonincreasing with Ds and Do.

• R(Ds, Do) is jointly convex with respect to (Ds, Do).

• The contour set

(Ds, Do) : R(Ds, Do)

R

is convex for any R

0.

}
Proof: The proof of the ﬁrst two properties is exactly the same as that for standard rate

≤

≥

{

distortion functions; see, e.g., [3]. The third property is then an immediate corollary of the
second property. (cid:3)

Corollary 1 implies a trade-off between the two distortions: for a given code rate, the smaller

the state distortion, the larger the observation distortion, and vice versa. Concrete numerical

examples can be found in Section IV, where Figures 2 and 4 plot the achievable regions of

(R, Ds, Do) and their projections under different values of R, for two experimental setups,

respectively. These plots demonstrate that for ﬁxed R, the achievable (Ds, Do) pairs form a

convex region, whose boundary exhibits a trade-off between Ds and Do. Hence a sensible coding

scheme of a semantic source should exhibit such behavior.

June 2, 2022

DRAFT

Now consider the weighted distortion constraint (6). We have the following corollary.

Corollary 2: For a given semantic source under the weighted distortion constraint (6), the

rate distortion function is as follows:

10

R( ¯D) = min

R(Ds, Do)

wsDs + woDo

¯D

.

(14)

|
Proof: Given the semantic rate distortion function R(Ds, Do) in Theorem 1, we have that
any coding scheme that achieves (R, ¯D) should achieve a (R, Ds, Do) tuple for the semantic

≤

(cid:9)

(cid:8)

rate distortion problem under distortion constraints (3) and (4), for some Ds and Do satisfying

wsDs + woDo

¯D, and vice versa. (cid:3)

≤

We end this section with the semantic rate distortion function (8) for semantic sources with

several intrinsic states, as given by the following corollary. Its proof is essentially identical to

that of Theorem 1.

Corollary 3: For a semantic source (S0, S1, . . . , Sk−1, X) with p(s0, s1, . . . , sk−1, x) over

× S

k−1 × X

, reproduction alphabet ˆ

S0 ×

ˆ
S1 ×

. . .

ˆ
k−1 ×
S

ˆ
X

×

j=0,1,...,k−1 and do, the semantic rate distortion function R(Ds0, Ds1, . . . , Dsk−1, Do) is as

S0 ×
, and distortion measures

. . .

S1 ×
dsj }
{
follows:

R(Ds0, Ds1, . . . , Dsk−1, Do) =

min
p(ˆs0,ˆs1,...,ˆsk−1,ˆx|x)

I(X; ˆS0, ˆS1, . . . , ˆSk−1, ˆX)

Do,

≤

s.t. E

E

do(X, ˆX)
h
i
ˆdsj (X, ˆSj)
h

i

Dsj ,

j = 0, 1, . . . , k

1,

−

≤

ˆdsj (x, ˆsj) = E

dsj (Sj, ˆsj)

x

=

|

sj∈Sj
X
(cid:2)
j=0,1,...,k−1, ˆX constitute a Markov chain S

(cid:3)

p(sj

|

x)dsj (sj, ˆsj),

X

↔

↔

( ˆS0, ˆS1, . . . , ˆSk−1, ˆX).

where

and S, X,

ˆSj

{

}

(15)

(16)

(17)

(18)

IV. GAUSSIAN OBSERVATION WITH LINEAR STATE-OBSERVATION RELATIONSHIP

Theorem 1 establishes the general form of the semantic rate distortion function, which comes

with an optimization problem, extending its counterpart in a standard rate distortion problem.

In this section, we specialize the general result to a case where the extrinsic observation X is

Gaussian and the intrinsic state-extrinsic observation pair (S, X) satisﬁes a linear relationship,

under quadratic distortion measures.

June 2, 2022

DRAFT

The extrinsic observation X obeys a multivariate Gaussian distribution

(0, KX),3 where KX

N

is an m

×

m positive semi-deﬁnite matrix. The intrinsic state S is given by

S = HX + Z,

(19)

11

×

where H is an l

m matrix, and Z is a random vector independent of X, with zero mean and

covariance matrix KZ. Note that we neither restrict Z to be Gaussian nor require H or KZ to be

full-rank. According to (19), the intrinsic state S is a linear transformation of X, further disturbed

by an independent component Z. This linear assumption holds for jointly Gaussian intrinsic state

S and extrinsic observation X, and can usually be extended to non-Gaussian models as well,

either precisely or approximately, for example, when a linear estimator of S conditioned upon

X can be obtained by traditional statistical methods, or by multilayer perceptron (MLP) neural

networks alternatively [36]. On the other hand, note that the linear assumption no longer holds

when one invokes nonlinear mappings, and deriving an analytical form of the corresponding

semantic rate distortion function will generally be an extremely difﬁcult task.

This model covers the special case where (S, X) are jointly Gaussian. In fact, if (S, X) are

jointly Gaussian with zero mean and covariance matrix

we can represent S according to

KT

KS KSX
SX KX 






,

S = KSXK−1

X X + Z,

(20)

(21)

where Z

(0, KS

KSXK−1

X KT

SX); that is, H = KSXK−1

X and KZ = KS

∼ N

−

We consider quadratic distortion measures, deﬁned as

ds(s, ˆs) =

k

s

ˆs
k
ˆx

2
2 = tr(s

−
2
2 = tr(x

ˆs)(s

−
ˆx)(x

−
x

−

k

−

−

ˆs)T ,

ˆx)T .

do(x, ˆx) =

k

Consequently, we have

E

ds(S, ˆS)

= tr(KS− ˆS),

h

i

3We use KV to denote the covariance matrix of a random column vector V .

June 2, 2022

KSXK−1

X KT

SX.

−

(22)

(23)

(24)

DRAFT

E

do(X, ˆX)

= tr(KX− ˆX ).

12

(25)

For the considered model (19), we can derive its semantic rate distortion function, given by

h

i

the following theorem.

Theorem 2: The semantic rate distortion function for the semantic source with Gaussian

extrinsic observation and linear state-observation relationship (19), under quadratic distortion

measures (22) and (23), is given by:

RG(Ds, Do) = min
∆∈Sm

1
2

s.t. O

∆

(cid:22)

≺

det(KX)
det(∆)

(cid:19)

log

(cid:18)
KX,

tr(H∆HT )

Ds

−

≤

tr(KZ),

tr(∆)

Do.

≤

(26)

(27)

(28)

(29)

where

S
subscript

G

m denotes the set of all m

m positive deﬁnite matrices. Note that here we use a

×

to emphasize that the extrinsic observation is Gaussian.

Proof: See Appendix II. (cid:3)

From (28), when Z is sufﬁciently strong so that tr(KZ) > Ds, the optimization (26) is no

longer feasible and hence RG(Ds, Do) =

. Otherwise, there is no further restriction on KZ.

∞

For example, even if Z = 0, i.e., the relationship between S and X is deterministic as S = HX,

the optimization problem in Theorem 2 is still non-trivial.

A simpliﬁed case arises when H is an orthogonal matrix satisfying HT H = I. In this case,

(28) becomes

tr(H∆HT ) = tr(∆HT H) = tr(∆)

Ds

−

≤

tr(KZ),

which can then be combined with (29) leading to a single distortion constraint

tr(∆)

min

{

≤

Do, Ds

−

tr(KZ)

.

}

(30)

(31)

In Theorem 2, the matrix ∆ which we optimize corresponds to the mean squared error (MSE)
of estimating X based upon ˆX at the decoder. The key to the proof of Theorem 2 is to show that

the semantic rate distortion function is achieved by a Gaussian reproduction. This is similar to

situations in several Gaussian lossy compression problems, including the standard Gaussian rate

distortion problem [12] and the Gaussian quadratic CEO problem [37]. Existing techniques based

on the entropy power inequality (EPI), extremal inequalities, and Fisher information inequalities

June 2, 2022

DRAFT

13

may also be interpreted as the optimality of Gaussian reproduction for the minimum mean

squared error (MMSE) estimation under a given MSE constraint. In our analysis, we further

need to accommodate with two MSE constraints, corresponding to the intrinsic state and the

extrinsic observation, respectively.

Compared with the general form of semantic rate distortion function in Theorem 1, Theorem

2 involves only one matrix-valued optimization variable ∆, which, as remarked in the previous
paragraph, is the MSE of estimating X based upon ˆX alone. In fact, the solution exhibits a
ˆS. To help understand the optimality of the Markov
Markov structure, i.e., S
chain solution, supposing that an alternative solution ( ˆX ′, ˆS′) is given which does not satisfy the
ˆX ′, ˆS′),
Markov structure, consequently one can form an improved reproduction as ˆX = E(X
satisfying the Markov structure and achieving the same code rate I(X; ˆX, ˆS′) = I(X; ˆX ′, ˆS′).

ˆX

↔

↔

↔

X

|

The Markov chain solution further suggests a “two-stage” coding interpretation which is in
fact extensively adopted in practice: the decoder ﬁrst generates a reproduction for X as ˆX,
and then uses that reproduction to further generate a reproduction for S as ˆS. Similar to the
standard Gaussian rate distortion problem, the optimal ˆX can be constructed with the aid of a
“test channel”, for which ˆX as the channel input is Gaussian and the additive Gaussian noise of

the test channel has a covariance matrix ∆, thereby producing X as the desired channel output.
To generate ˆS based upon ˆX, it sufﬁces to adopt a linear transform ˆS = H ˆX. On the other

hand, the Markov chain solution does not mean that the reproduction of S is trivial, because the

ﬁdelity criterion on X still needs to be adjusted according to Ds. The detailed arguments are

given in the proof in Appendix II.

An interesting property of the semantic rate distortion function derived in Theorem 2 is that

it is in fact an upper bound for all semantic sources with the same covariance structure under

the quadratic distortion measure. This essentially indicates that a semantic source with Gaussian

extrinsic observation is the hardest to describe, analogous to its counterpart in conventional source

coding problems (see, e.g., [3, Exercise 10.8]). Formally, we have the following corollary.

Corollary 4: For a semantic source (S, X) with general probability density function, whose

covariance matrix is given by (20), its semantic rate distortion function subject to quadratic

distortion constraints (22) and (23) satisﬁes R(Ds, Do)
the semantic rate distortion function given in Theorem 2, with H = KSXK−1

RG(Ds, Do), where RG(Ds, Do) is

X and KZ =

≤

KS

−

KSXK−1

X KT

SX.

June 2, 2022

DRAFT

14

Proof: See Appendix III. (cid:3)

A. Computation of the Semantic Rate Distortion Function

We remark that the optimization problem in Theorem 2 is convex, and hence can be numer-

ically solved by software like CVX in an efﬁcient and stable fashion. In this subsection we

present some illustrative numerical examples.

Our ﬁrst example is a small-scale toy model, given by

11

0

0.5

0

0.5

3

2
−
2 2.35

−








KX = 





, H =

0.0701

0.0305

−





0.457

0.305
0.220 0.671


−

, KZ =

0.701

0.305

−





.

0.305
−
0.220 


The resulting semantic rate distortion function is computed as displayed in Figure 2. The dotted

region in Figure 2(b) indicates that both constraints (28) and (29) are active. The trade-off

between the two distortions are clear: the smaller the state distortion, the larger the observation

distortion, and vice versa.

R

D

s,

D

o)

(

15.0

12.5

5.0

10.0

2.5

o
D

7.5

5.0

0

5

1.0

1.5

2.0

s

D

2.5

10

D

o

15

3.0

2.5

3

0.0

6

5

7

R
(

D

s
,

D
o
)

6

4

2

0

2

1

4

(a) Surface plot of RG(Ds, Do)

(b) Contour plot of RG(Ds, Do)

Fig. 2. Surface and contour plots of the semantic rate distortion function RG(Ds, Do) for the toy example.

1.0

1.5

2.0

2.5

3.0

D

s

Our second example captures a sparse state-observation relationship, as follows. The extrinsic

observation is a length-64 vector X = [X1,

, X64]T consisting of i.i.d.

(0, 2) random

· · ·

N

variables. The transformation matrix H is a randomly masked 16

64 Rademacher matrix;

×

that is, we ﬁrst generate a Rademacher matrix whose elements are i.i.d. taking values

}
with equal probability 1/2, and then independently reset these elements to zero with probability

1
−

1,

{

June 2, 2022

DRAFT

0.95. A realization of H is shown in Figure 3. The noise vector Z = [Z1,

, Z16]T consists of

· · ·

i.i.d.

N

(0, 1) random variables.

15

0

4

8

r
e
b
m
u
n
w
o
R

12

1.0

0.5

0.0

−0.5

−1.0

0

8

16 24 32 40 48 56

Column number

Fig. 3. A 16 × 64 transformation matrix H shown as a two-dimensional grid. Elements are shown as cells with different colors
corresponding to their values: blue for −1, red for 1, and gray for 0.

We numerically solve the semantic rate distortion function according to Theorem 2, and a

typical surface of RG(Ds, Do) is illustrated in Figure 4(a). More details of RG(Ds, Do) can be

seen from the contour plot in Figure 4(b), wherein the dotted region indicate that both constraints

(28) and (29) are active. From Figure 4(b), it is evident that describing the extrinsic observation

X tends to be much more costly than describing the intrinsic state S: at the same code rate, the

achieved Ds is generally much lower than the achieved Do.

Another interesting fact regarding RG(Ds, Do) can be inferred from the dotted region in the

contour plot Figure 4(b), and is more clearly revealed by plotting the trends of RG(Ds, Do)

as a function of Do (for ﬁxed Ds) or Ds (for ﬁxed Do), shown in Figures 5(a) and 5(b),

respectively. We ﬁnd that, the code rate RG(Ds, Do) as a function of Do does not seem to

be sensitive to the choice of Ds. This fact has an important consequence for designing lossy

compression schemes for semantic sources: although several different codes may have similar

performance in terms of reproducing the extrinsic observation, they can differ considerably in

terms of reproducing the intrinsic state. A heuristic explanation is as follows: since X is a

high-dimensional vector, describing it along several different directions may lead to similar

quadratic distortion performance; but since S corresponds to a low-dimensional feature of X,

June 2, 2022

DRAFT

 
16

D

o)

R

D

(

s,

R
(

D

s
,

D
o
)

150

100

150

100

50

0

50

25

50

75

s

D

50

100

D

o

100

125

120

100

o
D

80

60

40

20

0

25

50

75

100

125

150

175

20

40

60

80

100

120

D

s

(a) Surface plot of RG(Ds, Do)

(b) Contour plot of RG(Ds, Do)

Fig. 4.

Surface and contour plots of the semantic rate distortion function RG(Ds, Do) for the example of a sparse state-

observation relationship.

its reproduction only favors the direction of describing X that retains the feature of S the best.

150

100

)
o
D

,
s

D

(

R

50

D

s= 44.41

D

o= 31.35

80

D

s= 72.82

D

o= 62.69

D

s= 101.22

60

D

o= 94.04

D

s= 132.00

)
o
D

,
s

D

(

R

40

20

D

o= 128.00

0

0

0

20

40

60

80

100

120

20

40

60

80

100

120

D

o

D

s

(a) RG (Ds, Do) versus Do

(b) RG(Ds, Do) versus Ds

Fig. 5. The semantic rate distortion function RG(Ds, Do) as a function of Do or Ds.

B. Generalizations of Theorem 2

We can derive from Theorem 2 several corollaries corresponding to the variants of the problem

formulation in Section II.

First, let us consider replacing the quadratic distortion measures by the positive semi-deﬁnite

distortion constraints. Following the same arguments in the proof of Theorem 2, we again arrive

June 2, 2022

DRAFT

at the optimality of Gaussian descriptions under positive semi-deﬁnite distortion constraints, and

hence the following corollary characterizes the semantic rate distortion function.

Corollary 5: Consider the positive semi-deﬁnite distortion measures as

17

ds(s, ˆs) = (s

ˆs)(s

−

−

ˆs)T ,

do(x, ˆx) = (x

ˆx)(x

−

−

ˆx)T .

The semantic rate distortion function is given by

R(Ds, Do) = min
∆∈Sm

1
2

det(KX)
det(∆)

(cid:19)

log

(cid:18)
KX ,

Ds

−

KZ,

(32)

(33)

(34)

(35)

s.t. O

∆

(cid:22)

≺
H∆HT

(cid:22)
Do.

∆

(cid:22)

This is a semi-deﬁnite programming problem and can be readily solved by software.

Now consider the weighted distortion constraint, where the distortion measure is deﬁned as a

weighted sum of two individual distortion measures, i.e.

¯d = wsds(s, ˆs) + wodo(x, ˆx) = ws

s

k

−

ˆs
k

2
2 + wo

x

k

−

2
2.

ˆx

k

(36)

Applying Corollary 2, we obtain the semantic rate distortion function in the following corollary.
Corollary 6: For the weighted distortion measure ¯d, the semantic rate distortion function

R( ¯D) is given by

R( ¯D) = min
∆∈Sm

1
2

s.t. O

∆

(cid:22)

≺

log

(cid:18)
KX,

det(KX)
det(∆)

(cid:19)

Finally, consider the case of k intrinsic states. The extrinsic observation X is still

tr((wsHT H + woIm)∆)

¯D

≤

−

ws tr(KZ).

For each j

0, 1,

, k

1

}

−

· · ·

∈ {

, the j-th intrinsic state is generated according to

Sj = HjX + Zj,

(37)

(38)

(39)

(0, KX).

N

where Hj is an lj

m matrix, and Zj is a random vector independent of X, with zero mean

×

and covariance matrix KZj . We consider quadratic distortion measures, as

June 2, 2022

dsj (sj, ˆsj) =

sj

k

−

ˆsj

2
2,

k

j = 0, 1, . . . , k

1,

−

(40)

DRAFT

do(x, ˆx) =

x

k

−

2
2.

ˆx
k

18

(41)

The semantic rate distortion function is given by the following corollary.

Corollary 7: For the semantic source with a Gaussian extrinsic observation and k intrinsic

states, the semantic rate distortion function under distortion measures ds0, ds1,

, dsk−1, do is

· · ·

R(Ds0, Ds1,

, Dsk−1, Do) = min
∆∈Sm

· · ·

1
2

det(KX )
det(∆)

(cid:19)

log

(cid:18)
KX,

s.t. O

∆

≺

(cid:22)
tr(Hj∆HT
j )

Dsj −

≤

tr(KZj ),

j

0, 1,

, k

1

,

}

−

· · ·

∈ {

tr(∆)

Do.

≤

V. WEIGHTED REVERSE WATER-FILLING

Analogous to the standard Gaussian rate distortion problem wherein (after appropriate linear

transformation) the solution can be interpreted as a reverse water-ﬁlling type of rate allocation,

for the semantic rate distortion function in Theorem 2, under a diagonalizability condition, the

solution can also be interpreted as reverse water-ﬁlling, but with appropriately weighted water

levels.

For the model of Gaussian observation with linear state-observation relationship in Section IV,

we further assume that the following diagonalizability condition is satisﬁed: there exists an

unitary matrix Q such that

• Q†KX Q = diag(σ1, σ2,
· · ·
• Q†HT HQ = diag(α1, α2,

, σm),

, αm)

· · ·

simultaneously hold. Here it loses no generality to order

αi

}
m, then αq > 0 and αq+1 =

{

≤

m

i=1 so that α1 ≥
= αm = 0.
· · ·

α2 ≥ · · · ≥

αm.

Denoting the rank of HT H as q

Lemma 1: Under the diagonalizability condition, the resulting optimal ∆ takes the form

and the semantic rate distortion function in Theorem 2 can be further written in terms of the

∆ = Q diag(δ1, δ2,

, δm)Q†,

· · ·

(42)

following optimization problem:

RG(Ds, Do) = min

δ1,δ2,··· ,δm

1
2

m

j=1
X

log

σj
δj (cid:19)

(cid:18)

June 2, 2022

(43)

DRAFT

s.t.

0 < δj

σj,

≤

j
∀

1, 2,

,

, m
}

· · ·

∈ {

m

j=1
X
m

j=1
X

Proof: See Appendix IV. (cid:3)

αjδj

Ds

−

≤

tr(KZ),

δj

≤

Do.

19

(44)

(45)

(46)

In order to describe the weighted reverse water-ﬁlling solution, we ﬁrst introduce the following

curves.

• Curve Cs:

m

Cs =

(  

j=1
X

αj min

σj,

(cid:18)

1
λ

(cid:19)

m

+ tr(KZ),

min

σj,

j=1
X

(cid:18)

λ > 0

,

(47)

)

1
λ

(cid:19)!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

which starts from (tr(HKXHT + KZ), tr(KX)) and ends at (tr(KZ), 0).

• Curve Co:

Co =

q

( 

j=1
X

αj min

σj,

(cid:18)

1
µαj (cid:19)

q

+ tr(KZ),

min

σj,

j=1
X

(cid:18)

1
µαj (cid:19)

+

m

σj

j=q+1
X

which starts from (tr(HKXHT + KZ), tr(KX)) and ends at (tr(KZ),
j=1+1 σj is interpreted as 0 if HT H is full-rank and thus q = m.

m

P

µ > 0

,

)

(48)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
j=q+1 σj). Here,

We then introduce the following partitioning of the (Ds, Do) plane, based upon the curves Cs

P

and Co:

• A0 =

(Ds, Do)

Ds

tr(HKXHT + KZ), Do

tr(KX)

;

{

|

≥

≥

}

• A1: on the right of the curve Cs, and between the two horizontal lines Do = 0 and Do =

tr(KX);

• A2: above the curve Co, and between the two vertical lines Ds = tr(KZ) and Ds =

tr(HKXHT + KZ);

• A3: surrounded by the curves Cs and Co, and the vertical line Ds = tr(KZ).

An example of the partitioning above is plotted in Figure 6.

The following theorem describes the weighted reverse water-ﬁlling solution.

Theorem 3: For the model of Gaussian observation with linear state-observation relationship

in Section IV, under the diagonalizability condition, the optimal ∆ = Q diag(δ1, δ2,

is given by

June 2, 2022

δm)Q†

· · ·

DRAFT

20

(49)

(50)

TABLE I

ACTIVITY OF CONSTRAINTS (45) AND (46) IN A0, A1, A2 AND A3.

(45) active

(45) inactive

(46) inactive

(46) active

A2

A3

A0

A1

• If (Ds, Do)

A0:

∈

• If (Ds, Do)

A1:

∈

δ∗
j = σj,

j
∀

1, 2,

.

, m
}

· · ·

∈ {

where λ is chosen to satisfy

δ∗
j = min

σj,

1
λ

(cid:18)
m
j=1 δ∗

(cid:19)
j = Do.

,

j
∀

1, 2,

,

, m
}

· · ·

∈ {

• If (Ds, Do)

A2:

∈

P

min

σj,

(cid:18)

σj,

δ∗
j = 


1
µαj (cid:19)

, αj > 0

αj = 0

,

j
∀

1, 2,

, q

,

}

· · ·

∈ {

(51)

where µ is chosen to satisfy



q
j=1 αjδ∗

j = Ds

tr(KZ).

−

• If (Ds, Do)

A3:

∈

P

δ∗
j = min

σj,

(cid:18)
where λ, µ are chosen to satisfy

1
λ + µαj (cid:19)
m
j=1 δ∗

j = Do and

,

j
∀

1, 2,

∈ {
· · ·
q
j=1 αjδ∗

,

, m
}

(52)

j = Ds

tr(KZ).

−

Proof: See Appendix IV. (cid:3)

P

P

The partitioning

A0, A1, A2, A3}

{

is closely related to activity of the constraints (45) and (46),

as summarized in Table I. In A0, both constraints are inactive, and hence the optimization is

unconstrained yielding the trivial solution (49). In A1, only the observation distortion constraint

is active, and the solution (50) is a standard reverse water-ﬁlling with water level 1/λ. In A2, only

the state distortion is active, and the solution (51) essentially makes the weighted eigenvalues

α1δ1, α2δ2,

, αmδm fulﬁll a reverse water-ﬁlling structure, with water level 1/µ. Alternatively,

· · ·

we may view the term 1/(µαj) in (51) as a water level with weight 1/αj. In A3, both constraints

are active, and the solution (52) also fulﬁlls a reverse water-ﬁlling structure with unequal water

levels.

June 2, 2022

DRAFT

21

A. Case Study: Circulant KX and H and Weighted Reverse Water-ﬁlling in Frequency Domain

A case of special interest is where KX and H are both circulant matrices [38]. As the

dimension of X grows large, this models the scenario where X is a circularly stationary Gaussian

process,4 and S is obtained via passing X through a time-invariant linear ﬁlter whose response

is given by the ﬁrst row of H. For a circulant matrix, the corresponding unitary matrix Q is

the well known discrete Fourier transform (DFT) matrix, and its eigenvalues are the DFT of the

ﬁrst row of the matrix. Hence the weighted reverse water-ﬁlling may be interpreted as exercised

in the frequency domain, similar to its counterpart for the standard rate distortion function of

stationary Gaussian processes.

In the illustrative example below, consider KX as a 128

row

128 circulant matrix with the ﬁrst

×

H as a 128

×

128 circulant matrix with the ﬁrst row

[1, 0.4, 0,

, 0, 0.4],

· · ·

[0.3, 0.3, 0.3, 0.3, 0,

, 0],

· · ·

and KZ as a 128

128 zero matrix (i.e., no noise in the state-observation relationship). Therefore,

Q is the 128

×

×
128 DFT matrix whose (i, j)-th element is

1
√128

e−i

2π

128 ij,

i, j = 0, 1,

, 127.

· · ·

The diagonal elements α0, α1,

, α127 of Q†HT HQ are shown in Figure 7, and the diagonal
, σ127 of Q†KXQ are shown as the blue solid curve in Figure 8. Figure 6

· · ·

elements σ0, σ1,

· · ·

shows the four regions A0, A1, A2, A3 and the two curves Cs, Co. It also displays ﬁve points

on the contour of RG(Ds, Do) = 50, marked with colors varying from purple to yellow. The

weighted reverse water-ﬁlling solution (δ∗

127) for these points are depicted in Figure 8.
· · ·
For (Ds,1, Do,1), the optimal solution degenerates into a standard reverse water-ﬁlling form, as

0, δ∗
1,

, δ∗

indicated by the purple line. When we go from (Ds,1, Do,1) to (Ds,2, Do,2), the water level

begins to “ripple”. Note that this weighted reverse water-ﬁlling can be viewed as exercised in

the frequency domain, and the angular frequencies are marked on the top of Figure 8.

4If we remove the circulant restriction and consider a stationary Gaussian process, then we encounter a Toeplitz KX , for

which our solution still approximately applies; see, e.g., [38].

June 2, 2022

DRAFT

22

A

2

A

0

1.4

(tr(

HK

X

H

+

K

Z), tr(

K

X))

T

1.2

175

150

125

o
D

100

75

50

25

D

(

s

, 2,

D

o

, 2)

A

3

A

1

D

(

s

, 1,

D

o

, 1)

D

(

s

, 3,

D

o

, 3)

C

s

C

o

(tr(

K

Z), 0)

0

0

20

40

60

80

100

1.0

0.8

j

α

0.6

0.4

0.2

0.0

D

s

0

20

40

60

80

100

120

j

Fig. 6. The (Ds, Do) plane is divided into four regions A0,

Fig. 7. Diagonal elements α0, α1, · · · , α127 of Q†HT HQ.

A1, A2, A3, which determine the form of the optimal ∆.

Five points on the contour RG(Ds, Do) = 50 are marked

with colors varying from purple to yellow.

0

π

/2

π

π

3

/2

π

2

Angular frequency

(

D

s,

D

o) = (

D

s

, 1,

D

o

, 1)

(

D

s,

D

o) = (

D

s

, 3,

D

o

, 3)

(

D

s,

D

o) = (

D

s

, 2,

D

o

, 2)

*

δ

j

=

σ

j

1.8

1.6

1.4

1.2

*

j

δ

1.0

0.8

0.6

0.4

0.2

0

20

40

60

80

100

120

j

Fig. 8. Optimal diagonal elements (δ∗

1 , δ∗

2 , δ∗

3 ) of Q∆QT for the marked points in Figure 6, plotted with the colors in Figure 6.

June 2, 2022

DRAFT

23

VI. CONCLUSION

We have provided a general source model to describe information sources that have semantic

aspects, and proposed a corresponding rate distortion problem formulation for characterizing the

amount of information content of such semantic sources. We have studied the case of Gaussian

extrinsic observation subject to a linear state-observation relationship and a quadratic distortion

structure. There are a variety of issues that we have not touched upon in the present work. First,

calculating and bounding the semantic rate distortion functions for other interesting cases would

make further use of our proposed framework, for example, when the intrinsic state is a discrete

categorical random variable, corresponding to the important problem of classiﬁcation; see [1]

for some preliminary results. Second, a more challenging problem is to estimate the semantic

rate distortion function, and more importantly, to develop effective lossy compression methods

when the joint probability distribution of the intrinsic state and the extrinsic observation is not

perfectly known, say, when only ﬁnite training data of the state-observation pair are available.

APPENDIX I

PROOF OF THEOREM 1

The key to proving Theorem 1 is converting the semantic rate distortion problem into an

equivalent standard rate distortion problem, with an indirect (state) distortion constraint and a

direct (observation) distortion constraint. More precisely, we need to show that the constraint

with respect to the state distortion measure ds(s, ˆs) is equivalent to a constraint on a converted
distortion measure ˆds(x, ˆs); that is, as long as a reproduction ˆS satisﬁes the constraint on ˆds(x, ˆs),

it will satisfy the constraint on ds(s, ˆs), and vice versa.

A general and uniﬁed approach to the indirect rate-distortion function put forward in [33] is

ﬁrst showing that the one-shot expected distortion E

is equivalent to E

ds(S, ˆS)
h

ˆds(X, ˆS)
h

,

i
and then invoking a tensorization argument to extend the one-shot equivalence to block codes.
( ˆSn, ˆX n) generated

Here we directly illustrate how this can be accomplished for Sn

X n

i

↔

↔

by an arbitrary encoder-decoder pair, as follows:

E

ds(Sn, ˆSn)
h

i

=

p(sn, ˆsn)ds(sn, ˆsn)

sn,ˆsn
X

June 2, 2022

DRAFT

p(sn, xn, ˆsn)ds(sn, ˆsn)

24

=

(a)
=

=

(b)
=

=

(c)
=

=

=

=

(d)
=

=

sn,xn,ˆsn
X

p(sn, xn)p(ˆsn

sn,xn,ˆsn
X

p(ˆsn

xn)ds(sn, ˆsn)

|

xn)

p(sn, xn)ds(sn, ˆsn)

sn
X

xn)

p(sn, xn)

1
n

n

i=1
X

ds(si, ˆsi)

p(sn, xn)ds(si, ˆsi)

sn
X

sn
X
n
1
n

i=1
X
n

|

|

|

|

|

|

|

p(ˆsn

p(ˆsn

p(ˆsn

p(ˆsn

p(ˆsn

p(ˆsn

xn)

xn)

xn)

xn)

xn)

1
n

1
n

1
n

1
n

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

xn,ˆsn
X

p(xn, ˆsn)

1
n

i=1
X

p(xn, ˆsn) ˆds(xn, ˆsn)

p(¯si, ¯xi)p(si, xi)ds(si, ˆsi)

i=1
X
n

¯si X
si∈S
X

p(¯si, ¯xi)

p(si, xi)ds(si, ˆsi)

i=1
X
n

¯si
X

si∈S
X

p(¯xi)

p(si, xi)ds(si, ˆsi)

i=1
X
n

i=1
X
n

si∈S
X

p(xn)

si∈S
X
ˆds(xi, ˆsi)

xi)ds(si, ˆsi)

p(si

|

= E

ˆds(X n, ˆSn)
h

,

(53)

i
where ¯xi = (x1, . . . , xi−1, xi+1, . . . , xn), ¯si = (s1, . . . , si−1, si+1, . . . , sn), (a) is due to the
xn, ˆsn), (b) follows
existence of the Markov chain Sn

ˆSn and hence p(sn

xn) = p(sn

X n

↔

↔

|

|

from the deﬁnition of block-wise distortion measure in (1), (c) is by the fact that (Si, Xi)i∈N
is an i.i.d. sequence, and (d) is by the deﬁnition of ˆds(x, ˆs) in (12). Subsequently, the problem

is reduced into a standard lossy source coding problem with two distortion constraints, one on
do(x, ˆx) and the other on ˆds(x, ˆs). The semantic rate distortion function hence follows from

standard achievability and converse proofs [34, Sec. VII] [35, Prob. 7.14] [3, Prob. 10.19].

June 2, 2022

DRAFT

25

APPENDIX II

PROOF OF THEOREM 2

The proof of Theorem 2 involves two steps. First we prove that the semantic rate distortion
function can be achieved by jointly Gaussian ˆX and ˆS. Then we show that we can further endow
a Markov chain structure on X, ˆX and ˆS, so that we only need to optimize with one variable,
i.e., ˆX, while generating ˆS from ˆX subsequently.

A. Optimality of Jointly Gaussian Reproduction

By the deﬁnition of ˆds(X; ˆS) in (12), E

E

ˆds(X; ˆS)
h

i

=

p(x, ˆs)

p(s

|

ˆds(X; ˆS)
h

i
x)ds(s, ˆs)ds

can be written as follows:

dxdˆs

Z

Z

=

(a)
=

p(x, ˆs)

(cid:18)Z

(cid:18)Z

p(Hx + z

|

(cid:19)
x)(Hx + z

ˆs)(Hx + z

−

−

ˆs)T dz

dxdˆs

(cid:19)

p(x, ˆs)

p(z) tr(HxxT HT + HxzT

Hxˆs

−

(cid:18) Z

Z
+ zxT H + zzT

zˆsT

−

(b)
=

p(x, ˆs) tr(HxxT HT

−

−

Z

= tr(HKXHT

(c)
= tr(HKXHT

−

−

ˆsxT HT

−

ˆszT + ˆsˆsT )dz

dxdˆs

(cid:19)
ˆsxT HT + ˆsˆsT )dxdˆs

Hxˆs + KZ

−

HKX ˆS + KZ

−

K ˆSX HT + K ˆS)

2HKX ˆS + KZ + K ˆS),

(54)

where (a) is due to independence between Z and X, (b) is according to the problem setup
that E(Z) = 0, and (c) is due to the fact that tr(HKX ˆS) = tr(K ˆSXHT ). From this chain
of identities, we see that for any two reproductions of the intrinsic state, ˆS and ˆS′, we have
E

dS(S; ˆS)
h
Therefore, by Theorem 1, the semantic rate distortion function R(Ds, Do) can be further

as long as K ˆS = K ˆS′ and KX ˆS = KX ˆS′.

dS(S; ˆS′)
h

= E

i

i

written as

June 2, 2022

R(Ds, Do) = min I(X; ˆS, ˆX) = h(X)

max h(X

ˆS, ˆX)

|

−
Do

s.t.

tr(KX

−

2KX ˆX + K ˆX)

tr(HKXHT

−

≤
2HKX ˆS + KZ + K ˆS)

Ds.

≤

(55)

(56)

(57)

DRAFT

Notice that, by denoting T , ( ˆS, ˆX) for convenience, h(X

ˆS, ˆX) can be upper bounded as

|

26

h(X

= h(X

= h(X

h(X

ˆS, ˆX)

T )

|

|

KXT K−1

T T

T )

|

KXT K−1

T T )

−

−

(a)

≤
(b)

≤

=

1
2
1
2

log det(2πeKX−KXT K−1

T T )

log det(2πeKX

KXT K−1

T KT X),

−

(58)

where (a) is by the fact that conditioning reduces entropy, and equality holds when X
KXT K−1

−
T T is independent of T ; (b) is due to the fact that Gaussian distribution maximizes
differential entropy with given second central moment. Overall, we can see that this upper

bound of h(X

ˆS, ˆX) is achieved when X and T are jointly Gaussian.

|

Based on the argument above, for an arbitrary T = ( ˆS, ˆX), we can generate T ′ = ( ˆS′, ˆX ′)

according to a linear relationship

( ˆS′, ˆX ′) = KT XK−1

X X + N,

(59)

where N is a multivariate Gaussian random variable following

X KXT ) and
is independent of X. Clearly it holds that KT ′ = KT and KXT = KXT ′. According to (58), we
h( ˆS′, ˆX ′). That is to say, for any ( ˆS, ˆX) that satisﬁes the distortion
can see that h(X
constraints, there always exists a Gaussian ( ˆS′, ˆX ′) which also satisﬁes the distortion constraints,
but achieving a lower code rate. We thus establish that jointly Gaussian reproduction ( ˆS, ˆX)

ˆS, ˆX)

(0, KT

N

≤

−

KT XK−1

|

achieves the semantic rate distortion function.

B. Reduction to One Optimization Variable

In fact, it is unnecessary to optimize with two random variables ( ˆS, ˆX) simultaneously, and

in the following we reduce the number of optimization variables to only one. We choose the

new optimization variable as cov(X

ˆX, ˆS), deﬁned as

|

cov(X

|

ˆX, ˆS) = E

X

−

ˆX, ˆS

|

E

X

h

(cid:20)(cid:16)

i(cid:17) (cid:16)

X

−

ˆX, ˆS

|

E

X

h

T

,

(cid:21)

i(cid:17)

June 2, 2022

DRAFT

27

(61)

(62)

i.e., the error covariance matrix of MMSE estimating X by ( ˆX, ˆS). By denoting cov(X
ˆX, ˆS) as
∆ for short, we can write I(X; ˆX, ˆS) as (26). Therefore, now the key point is to show that the
R1) is the same as the feasible region deﬁned

feasible region deﬁned by (56)-(57) (denoted as

|

by (27)-(29) (denoted as

R2).

∆

cov(X

First we show that
ˆX) and ∆

R1 ⊆ R2. For any K( ˆS, ˆX) ∈ R1, with ∆ = cov(X
ˆS), and correspondingly tr(∆)
tr(cov(X
(cid:22)
|
ˆS))
tr(H∆HT + KZ)

ˆS)HT + KZ) = tr(cov(HX + Z

tr(Hcov(X

cov(X

(cid:22)

≤

|

|

ˆS, ˆX), we have
|
ˆX))

Do and

≤

Ds.

(60)

≤

|

That is to say, for any K( ˆS, ˆX) ∈ R1, we can ﬁnd a corresponding ∆
R2 ⊆ R1. For any ∆

R1 ⊆ R2.
∈ R2, we consider a test channel with X = ˆX + N
(0, ∆). Hence we have

and let ˆS = H ˆX, where N obeys Gaussian distribution

Then we show that

≤

|
∈ R2, and hence

N

E

E

do(X, ˆX)
h
ds(S, ˆS)
h

= tr(∆)

Do,

≤

i
= tr(Hcov(X

i
That is to say, for any ∆
R2 ⊆ R1.

hence

ˆS)HT + KZ) = tr(H∆HT + KZ)

|

Ds.

≤

∈ R2, we can also ﬁnd a corresponding tuple of K( ˆS, ˆX) ∈ R1, and

Now, we can conclude that, under the setting of Theorem 2, Theorems 1 and 2 deﬁne two

optimization problems with the same objective function and the same feasible region. This

therefore completes the proof.

APPENDIX III

PROOF OF COROLLARY 4

By Theorem 2 and the identities H = KSXK−1

SX in (21), the
semantic rate distortion function of a jointly Gaussian semantic source with covariance matrix

X and KZ = KS

KSXK−1

X KT

−

(20) is given by

RG(Ds, Do) = min
∆∈Sm

1
2

s.t. O

∆

≺

(cid:22)

det(KX)
det(∆)

(cid:19)

log

(cid:18)
KX,

tr(KSXK−1

X ∆K−1

X KT

SX)

Ds

−

≤

tr(KS

−

KSXK−1

X KT

SX),

tr(∆)

Do.

≤

June 2, 2022

(63)

(64)

(65)

(66)

DRAFT

We will prove

R(Ds, Do)

1
2

≤

log

(cid:18)

det(KX )
det(∆)

(cid:19)

28

(67)

for an arbitrary symmetric matrix ∆ that satisﬁes (64), (65) and (66), by constructing a test

channel. This implies that R(Ds, Do) is no greater than (63).

In order to construct the test channel, let U be a Gaussian vector with zero mean and covariance

∆K−1

−

matrix ∆
Lemma 2 at the end of this subsection. Deﬁne ˆX = (Im

X ∆, independent of (S, X). That ∆

X ∆ is semi-deﬁnite will be proved in
X )X + U and ˆS = KSXK−1
ˆX.
∆K−1
ˆS is a Markov chain. We will verify in the next paragraphs that

ˆX

−

−

X

∆K−1

Thus S
↔
E[ ˆds(X, ˆS)]

X

≤

↔

↔
Ds, where ˆds(x, ˆs) = E[
k
I(X; ˆS, ˆX)

S

X

ˆs
k
1
2

−

≤

2
2|

X = x], E[
k
det(KX)
det(∆)

log

(cid:18)

(cid:19)

ˆX

2
2]

k

≤

Do, and

−

.

(68)

These leads to (67), and thus proves Corollary 4.
By the deﬁnitions of ˆX and ˆS, we have S

X U, where L =
X . Noticing E[SU T ] = Ol×m and E[XU T ] = Om×m, we can obtain, after some

KSXLX

KSXK−1

−

−

−

ˆS = S

K−1

K−1

X −

X ∆K−1
algebraic manipulations,

E

(S

ˆS)(S

ˆS)T

= KS

KSXK−1

X KT

SX + KSXK−1

X ∆K−1

X KT

SX.

i

h

−

−

−
Taking the trace in this equation and using (65), we get E[
S
k
Rm and every ˆs
lead to E[
k
ˆS

−
X = x, ˆS = ˆs] = E[
k

Do. For every x

E[
k

ˆs
k

2
2|

2
2|

ˆX

2
2]

X

−

≤

−

∈

S

S

k

k

∈

where the second equality is due to S
immediately leads to E[ ˆds(X, ˆS)] = E[
k

↔
S

X

−

It remains to verify (68). We have

I(X; ˆS, ˆX)

(a)
= I(X; ˆX)

↔
ˆS

2
2]

≤

Ds.

k

Ds. Similar calculations

ˆS
2
2]
−
≤
Rl, we have

k

X = x, ˆS = ˆs] = E[
k

ˆs
k
ˆS. An application of the law of total expectation

X = x] = ˆds(x, ˆs),

2
2|

−

S

log((2πe)m det(∆

∆K−1

X ∆))

= h( ˆX)

(b)
= h( ˆX)

−

−

h( ˆX
1
2

X)

|

−
1
2

(c)

≤

=

1
2
1
2

log((2πe)m(KX

det(KX)
det(∆)

log

(cid:18)

(cid:19)

∆))

−

−

,

log((2πe)m det(∆

∆K−1

X ∆))

−

June 2, 2022

DRAFT

where (a) is by X

ˆX

↔

↔

ˆS, (b) is because after translation h( ˆX

X) = h(U), and (c) is because

|

the Gaussian distribution maximizes the differential entropy subject to a covariance constraint.

Finally let us verify the existence of the auxiliary random vector U.

29

Lemma 2: For any ∆, K

m, ∆

K, ∆

∆K−1∆ is semi-deﬁnite.

∈ S
Proof: Because ∆ is positive deﬁnite, there exists an m

−

(cid:22)

m matrix Q such that ∆ = QT Q.

×

For every λ < 0,

det(λIm

(Im

−

−

QK−1QT )) = (λ

= (λ

−

−

1)m det

Im +

1)m det

(cid:18)

(cid:18)

Im +

K−1

= (λ

1)m det

−
det((λ

=

−
det(K)

(cid:18)
1)K + ∆)

(cid:18)

QK−1QT

(cid:19)

K−1QT Q

(cid:19)

∆

(cid:19)(cid:19)

1

−

1

λ

λ

1

−
1

1

λ

1

−
K +

= 0,

because (λ

1)K + ∆ = λK

(K

−
any negative eigenvalue. Therefore Im

−

−

∆) is negative deﬁnite. So Im

QK−1QT does not have

−
QK−1QT is positive semi-deﬁnite, and consequently

∆K−1∆ = QT (Im

∆

−

−

−

QK−1QT )Q is also positive semi-deﬁnite. (cid:3)

DERIVATION OF THE WEIGHTED REVERSE WATER-FILLING SOLUTION

APPENDIX IV

We ﬁrst rewrite (26) with a variable substitution ∆ = QDQ†. This leads to

RG(Ds, Do) = min

D∈B(Ds,Do)

1
2

log

(cid:18)

det(KX)
det(D)

,

(cid:19)

where B(Ds, Do) is the set of positive deﬁnite real matrices D that satisfy

D

tr(Q†HT HQD)

tr(D)

Q†KXQ,

tr(KZ),

Ds

−

Do.

(cid:22)

≤

≤

Any optimal D in this minimization is diagonal. To see this, consider a non-diagonal D

∈
B(Ds, Do). Replacing the non-diagonal elements in D with zeros, we get a new matrix D′ =

diag(δ1, δ2,

· · ·

, δm). Because

Om

D

(cid:22)

≺

Q†KXQ = diag(σ1, σ2,

, σm),

· · ·

June 2, 2022

DRAFT

6
30

≺

Ds

D′

(cid:22)

Q†KX Q. Moreover,

tr(KZ),

−

we have 0 < δj

σj for each j

≤

∈ {

1, 2,

m

, m
}

· · ·

, which impies Om

tr(Q†HT HQD′) =

αjδj = tr(Q†HT HQD)

tr(D′) =

j=1
X
m

j=1
X

δj = tr(D)

Do.

≤

B(Ds, Do). By Hadamard’s inequality,

≤

So D′

∈

1
2

log

(cid:18)

det(KX)
det(D′)

(cid:19)

<

1
2

log

(cid:18)

det(KX )
det(D)

.

(cid:19)

Therefore, any non-diagonal D

B(Ds, Do) is suboptimal, and (43) is veriﬁed.

∈

By the Karush-Kuhn-Tucker (KKT) optimality conditions, there exist non-negative numbers

λ, µ, ν1, ν2,

· · ·

, νm that satisfy

λ

m

j=1
X

δ∗
j −

Do

!

= 0,

µ

m

j=1
X

αjδ∗

j −

Ds + tr(KZ)

= 0,

!

νj(δ∗

σj) = 0,

j −
+ λ + µαj + νj = 0,

j
∀

j
∀

1, 2,

1, 2,

,

, m
}

.

, m
}

· · ·

· · ·

∈ {

∈ {

1
δ∗
j

−

Suppose λ = 0 and µ = 0. For each j

1, 2,

, m
}

· · ·

∈ {

, we have νj = 1/δ∗

j > 0, so δ∗

j = σj.

Because δ∗

1, δ∗
2,

· · ·

, δ∗

m satisfy (45) and (46), we have

Ds

≥

Do

≥

m

j=1
X
m

j=1
X

αjσj + tr(KZ) = tr(HKXHT + KZ),

σj = tr(KX),

i.e. (Ds, Do)

A0.

∈

Suppose λ > 0 and µ = 0. The problem now reduces to the one involved in the rate distortion

problem of parallel Gaussian sources [3], because the constraint (46) is active and (45) is not.

Thus (50) holds, and

m

m

Do =

δ∗
j =

min

σj,

j=1
X

j=1
X

(cid:18)

1
λ

,

(cid:19)

June 2, 2022

DRAFT

 
 
Ds

≥

m

j=1
X

A1.

αjδ∗

j + tr(KZ) =

m

j=1
X

αj min

σj,

(cid:18)

1
λ

(cid:19)

+ tr(KZ),

i.e. (Ds, Do)

∈

Similarly, the conditions λ = 0 and µ > 0 imply (51) leading to (Ds, Do)

conditions λ > 0 and µ > 0 imply (52) leading to (Ds, Do)

A3.

∈

REFERENCES

31

A2, and the

∈

[1] J. Liu, W. Zhang, and H. V. Poor, “A rate-distortion framework for characterizing semantic information,” Proc. IEEE Int.

Symp. Inf. Theory (ISIT), 2021.

[2] C. E. Shannon, “A mathematical theory of communication,” Bell Syst. Tech. J., vol. 27, no. 3, pp. 379–423, 1948.

[3] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. Hoboken, New Jersey, USA: John Wiley &

Sons, Ltd, 2006.

[4] S. Ma, X. Zhang, S. Wang, X. Zhang, C. Jia, and S. Wang, “Joint feature and texture coding: Toward smart video

representation via front-end intelligence,” IEEE Trans. Circuits Syst. Video Technol., vol. 29, no. 10, pp. 3095–3105, 2019.

[5] L. Duan, J. Liu, W. Yang, T. Huang, and W. Gao, “Video coding for machines: A paradigm of collaborative compression

and intelligent analytics,” IEEE Trans. Image Process., vol. 29, pp. 8680–8695, 2020.

[6] S. Yang, Y. Hu, W. Yang, L. Duan, and J. Liu, “Towards coding for human and machine vision: Scalable face image

coding,” IEEE Trans. Multimedia, vol. 23, pp. 2957–2971, 2021.

[7] Y. Yang, G. Shu, and M. Shah, “Semi-supervised learning of feature hierarchies for object detection in a video,” in Proc.

IEEE Conf. Comput. Vis. Pattern Recognit (CVPR), 2013, pp. 1650–1657.

[8] Y. Wu, K. Zhang, D. Wu, C. Wang, C.-A. Yuan, X. Qin, T. Zhu, Y.-C. Du, H.-L. Wang, and D.-S. Huang, “Person

reidentiﬁcation by multiscale feature representation learning with random batch feature mask,” IEEE Trans. Cogn. Develop.

Syst., vol. 13, no. 4, pp. 865–874, 2021.

[9] K. Liu, D. Liu, N. Yan, and H. Li, “Semantics-to-signal scalable image compression with learned revertible representations,”

Int. J. Comput. Vis., vol. 129, p. 2605–2621, 2021.

[10] L. R. Rabiner and R. W. Schafer, “Introduction to digital speech processing,” Found. Trends Signal Process., vol. 1, no.

1-2, pp. 1–194, 2007. [Online]. Available: http://dx.doi.org/10.1561/2000000001

[11] S. Furui, “Cepstral analysis technique for automatic speaker veriﬁcation,” IEEE Trans. Acoust., Speech, Signal Process.,

vol. 29, no. 2, pp. 254–272, 1981.

[12] C. E. Shannon, “Coding theorems for a discrete source with a ﬁdelity criterion,” IRE Conv. Rec., pp. 142–163, 1959.

[13] P. Popovski, O. Simeone, F. Boccardi, D. Gunduz, and O. Sahin, “Semantic-effectiveness ﬁltering and control for post-5G

wireless connectivity,” 2019. [Online]. Available: https://arxiv.org/abs/1907.02441

[14] M. Kountouris and N. Pappas, “Semantics-empowered communication for networked intelligent systems,” 2021. [Online].

Available: https://arxiv.org/abs/2007.11579v1

[15] H. Seo, J. Park, M. Bennis, and M. Debbah, “Semantics-native communication with contextual reasoning,” 2021.

[Online]. Available: https://arxiv.org/abs/2108.05681

[16] Y. Bar-Hillel and R. Carnap, “Semantic information,” British J. Philosophy Science, vol. 4, no. 14, pp. 147–157, 1953.

[17] L. Floridi, “Outline of a theory of strongly semantic information,” Minds and Machines, pp. 197–221, 2004.

June 2, 2022

DRAFT

32

[18] J. Bao, P. Basu, M. Dean, C. Partridge, A. Swami, W. Leland, and J. A. Hendler, “Towards a theory of semantic

communication,” in Proc. IEEE Network Science Workshop (NSW), 2011, pp. 110–117.

[19] B. Juba, Universal Semantic Communication. Springer, 2011.

[20] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck method,” in Proc. Allerton Conf. Commun. Control

Comput., Monticello, IL, USA, Sep 1999, pp. 368–377.

[21] Z. Goldfeld and Y. Polyanskiy, “The information bottleneck problem and its applications in machine learning,” IEEE J.

Select. Area. Inf. Theory, vol. 1, no. 1, pp. 19–38, 2020.

[22] A. Makhdoumi, S. Salamatian, N. Fawaz, and M. M´edard, “From the information bottleneck to the privacy funnel,” in

Proc. IEEE Inf. Theory Workshop (ITW), 2014, pp. 501–505.

[23] Y. Y. Shkel, R. S. Blum, and H. V. Poor, “Secrecy by design with applications to privacy and compression,” IEEE Trans.

Inf. Theory, vol. 67, no. 2, pp. 824–843, 2021.

[24] N. Shlezinger, Y. C. Eldar, and M. R. D. Rodrigues, “Hardware-limited task-based quantization,” IEEE Trans. Signal

Process., vol. 67, no. 20, pp. 5223–5238, 2019.

[25] Y. Blau and T. Michaeli, “Rethinking lossy compression: The rate-distortion-perception tradeoff,” in Proc. Int. Conf.

Machine Learning (ICML), 09–15 Jun 2019, pp. 675–685.

[26] A. Kipnis, S. Rini, and A. J. Goldsmith, “The rate-distortion risk in estimation from compressed data,” IEEE Trans. Inf.

Theory, vol. 67, no. 5, pp. 2910–2924, 2021.

[27] L. Rabiner, “A tutorial on hidden Markov models and selected applications in speech recognition,” Proc. IEEE, vol. 77,

no. 2, pp. 257–286, 1989.

[28] A. El Gamal and Y.-H. Kim, Network Information Theory. Cambridge University Press, 2011.

[29] R. W. Yeung, Information Theory and Network Coding, ser. Information Technology: Transmission, Processing and Storage.

New York: Springer, 2008.

[30] R. Dobrushin and B. Tsybakov, “Information transmission with additional noise,” IRE Trans. Inf. Theory, vol. 8, no. 5,

pp. 293–304, 1962.

[31] J. Wolf and J. Ziv, “Transmission of noisy information to a noisy receiver with minimum distortion,” IEEE Trans. Inf.

Theory, vol. 16, no. 4, pp. 406–411, 1970.

[32] T. Berger, Rate Distortion Theory. Englewood Cliffs, NJ, USA: Prentice-Hall, 1971.

[33] H. Witsenhausen, “Indirect rate distortion problems,” IEEE Trans. Inf. Theory, vol. 26, no. 5, pp. 518–521, 1980.

[34] A. El Gamal and T. Cover, “Achievable rates for multiple descriptions,” IEEE Trans. Inf. Theory, vol. 28, no. 6, pp.

851–857, 1982.

[35]

I. Csisz´ar and J. K¨orner, Information Theory: Coding Theorems for Discrete Memoryless Systems, 2nd ed. Cambridge

University Press, 2011.

[36] Y. Xia, C. Sun, and W. X. Zheng, “Discrete-time neural network for fast solving large linear L1 estimation problems and

its application to image restoration,” IEEE Trans. Neural Netw. Learn. Syst, vol. 23, no. 5, pp. 812–820, 2012.

[37] Y. Oohama, “The rate-distortion function for the quadratic Gaussian CEO problem,” IEEE Trans. Inf. Theory, vol. 44,

no. 3, pp. 1057–1070, 1998.

[38] R. M. Gray, Toeplitz and Circulant Matrices: A Review. NOW Publishers, 2009.

June 2, 2022

DRAFT

