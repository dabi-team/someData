Sample Complexity of Robust Reinforcement Learning
with a Generative Model

Kishan Panaganti
Texas A&M University

Dileep Kalathil
Texas A&M University

2
2
0
2

y
a
M
4
1

]

G
L
.
s
c
[

3
v
6
0
5
1
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

The Robust Markov Decision Process
(RMDP) framework focuses on designing
control policies that are robust against the
parameter uncertainties due to the mis-
matches between the simulator model and
real-world settings. An RMDP problem is
typically formulated as a max-min problem,
where the objective is to ﬁnd the policy that
maximizes the value function for the worst
possible model that lies in an uncertainty
set around a nominal model. The standard
approach
robust dynamic programming
requires the knowledge of the nominal model
for computing the optimal robust policy.
In this work, we propose a model-based
reinforcement learning (RL) algorithm for
learning an ε-optimal robust policy when
the nominal model is unknown. We consider
three diﬀerent forms of uncertainty sets,
characterized by the total variation distance,
chi-square divergence, and KL divergence.
For each of these uncertainty sets, we give
a precise characterization of the sample
In
complexity of our proposed algorithm.
addition to the sample complexity results,
we also present a formal analytical argument
on the beneﬁt of using robust policies.
Finally, we demonstrate the performance of
our algorithm on two benchmark problems.

1

Introduction

Reinforcement Learning (RL) algorithms typically re-
quire a large number of data samples to learn a control
policy, which makes the training of RL algorithms di-

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

rectly on the real-world systems expensive and poten-
tially dangerous. This problem is typically avoided by
training the RL algorithm on a simulator and transfer-
ring the trained policy to the real-world system. How-
ever, due to multiple reasons such as the approxima-
tion errors incurred while modeling, changes in the
real-world parameters over time and possible adver-
sarial disturbances in the real-world, there will be in-
evitable mismatches between the simulator model and
the real-world system. For example, the standard sim-
ulator settings of the sensor noise, action delays, fric-
tion, and mass of a mobile robot can be diﬀerent from
that of the actual real-world robot. This mismatch
between the simulator and real-world model parame-
ters, often called ‘simulation-to-reality gap’, can sig-
niﬁcantly degrade the real-world performance of the
RL algorithms trained on a simulator model.

Robust Markov Decision Process (RMDP) addresses
the planning problem of computing the optimal pol-
icy that is robust against the parameter mismatch
between the simulator and real-world system. The
RMDP framework was ﬁrst introduced in (Iyengar,
2005; Nilim and El Ghaoui, 2005). The RMDP prob-
lem has been analyzed extensively in the literature
(Xu and Mannor, 2010; Wiesemann et al., 2013; Yu
and Xu, 2015; Mannor et al., 2016; Russel and Petrik,
2019), considering diﬀerent types of uncertainty set
and computationally eﬃcient algorithms. However,
these works are limited to the planning problem, which
assumes the knowledge of the system. Robust RL al-
gorithms for learning the optimal robust policy have
also been proposed (Roy et al., 2017; Panaganti and
Kalathil, 2021), but they only provide asymptotic con-
vergence guarantees. Robust RL problem has also
been addressed using deep RL methods (Pinto et al.,
2017; Derman et al., 2018, 2020; Mankowitz et al.,
2020; Zhang et al., 2020a). However, these works are
empirical in nature and do not provide any theoret-
In particu-
ical guarantees for the learned policies.
lar, there are few works that provide robust RL al-
gorithms with provable (non-asymptotic) ﬁnite-sample
performance guarantees.

 
 
 
 
 
 
Sample Complexity of Robust Reinforcement Learning with a Generative Model

In this work, we address the problem of developing a
model-based robust RL algorithm with provable ﬁnite-
sample guarantees on its performance, characterized
by the metric of sample complexity in a PAC (proba-
bly approximately correct) sense. The RMDP frame-
work assumes that the real-world model lies within
some uncertainty set P around a nominal (simulator)
model P o. The goal is to learn a policy that performs
the best under the worst possible model in this un-
certainty set. We do not assume that the algorithm
knows the exact simulator model (and hence the ex-
act uncertainty set). Instead, similar to the standard
(non-robust) RL setting (Singh and Yee, 1994; Azar
et al., 2013; Haskell et al., 2016; Sidford et al., 2018;
Agarwal et al., 2020; Li et al., 2020; Kalathil et al.,
2021), we assume that the algorithm has access to
a generative sampling model that can generate next-
state samples for all state-action pairs according to the
nominal simulator model. In this context, we answer
the following important question: How many samples
from the nominal simulator model do we need to learn
an ε-optimal robust policy with high probability?

Our contributions: The main contributions of our
work are as follows:

(1) We propose a model-based robust RL algorithm,
which we call the robust empirical value iteration algo-
rithm (REVI), for learning an approximately optimal
robust policy. We consider three diﬀerent classes of
RMDPs with three diﬀerent uncertainty sets: (i) Total
Variation (TV) uncertainty set, (ii) Chi-square uncer-
tainty set, and (iii) Kullback-Leibler (KL) uncertainty
set, each characterized by its namesake distance mea-
sure. Robust RL problem is much more challenging
than the standard (non-robust) RL problems due to
the inherent nonlinearity associated with the robust
dynamic programming and the resulting unbiasedness
of the empirical estimates. We overcome this challenge
analytically by developing a series of upperbounds that
are amenable to using concentration inequality results
(which are typically useful only in the unbiased set-
ting), where we exploit a uniform concentration bound
with a covering number argument. We rigorously char-
acterize the sample complexity of the proposed algo-
rithm for each of these uncertainty sets. We also make
a precise comparison with the sample complexity of
non-robust RL.

(2) We give a formal argument for the need for using
a robust policy when the simulator model is diﬀerent
from the real-world model. More precisely, we ana-
lytically address the question ‘why do we need robust
policies?’, by showing that the worst case performance
of a non-robust policy can be arbitrarily bad (as bad
as a random policy) when compared to that of a ro-
bust policy. While the need for robust policies have

been discussed in the literature qualitatively, to the
best of our knowledge, this is the ﬁrst work that gives
an analytical answer to the above question.

(3) Finally, we demonstrate the performance of our
REVI algorithm in two experiment settings and for
two diﬀerent uncertainty sets.
In each setting, we
show that the policy learned by our proposed REVI
algorithm is indeed robust against the changes in the
model parameters. We also illustrate the convergence
of our algorithm with respect to the number of samples
and the number of iterations.

1.1 Related Work

Robust RL: An RMDP setting where some state-
action pairs are adversarial and the others are station-
ary was considered by (Lim et al., 2013), who pro-
posed an online algorithm to address this problem. An
approximate robust dynamic programming approach
with linear function approximation was proposed in
(Tamar et al., 2014). State aggregation and kernel-
based function approximation for robust RL were stud-
ied in (Petrik and Subramanian, 2014; Lim and Autef,
2019). (Roy et al., 2017) proposed a robust version
of the Q-learning algorithm. (Panaganti and Kalathil,
2021) developed a least squares policy iteration ap-
proach to learn the optimal robust policy using linear
function approximation with provable guarantees. A
soft robust RL algorithm was proposed in (Derman
et al., 2018) and a maximum a posteriori policy op-
timization approach was used in (Mankowitz et al.,
2020). While the above mentioned works make inter-
esting contributions to the area of robust RL, they
focus either on giving asymptotic performance guar-
antees or on the empirical performance without giving
provable guarantees. In particular, they do not provide
provable guarantees on the ﬁnite-sample performance
of the robust RL algorithms.

The closest to our work is (Zhou et al., 2021), which
analyzed the sample complexity with a KL uncertainty
set. Our work is diﬀerent in two signiﬁcant aspects:
Firstly, we consider the total variation uncertainty set
and chi-square uncertainty set, in addition to the KL
uncertainty set. The analysis for these uncertainty sets
are very challenging and signiﬁcantly diﬀerent from
that of the KL uncertainty set. Secondly, we give a
more precise characterization of the sample complexity
bound for the KL uncertainty set by clearly specifying
the exponential dependence on (1 − γ)−1, where γ is
the discount factor, which was left unspeciﬁed in (Zhou
et al., 2021).

While this paper was under review, we were notiﬁed
of a concurrent work (Yang et al., 2021), which also
provides similar sample complexity bounds for robust

Kishan Panaganti, Dileep Kalathil

Table 1: Comparison of the sample complexities of diﬀerent uncertainty sets and the best known result in the
non-robust setting (Li et al., 2020). Here |S| and |A| are the cardinality of the state and action spaces, cr is the
robust RL problem parameter, and γ is the discount factor. We consider the optimality gap ε ∈ (0, c/(1 − γ)),
where c > 0 is a constant. We refer to Section 3.2 for further details.

Uncertainty set

TV

Chi-square

KL

Non-robust

Sample Complexity O( |S|2|A|

(1−γ)4ε2 ) O( cr|S|2|A|

(1−γ)4ε2 ) O( |S|2|A| exp(1/(1−γ))

r(1−γ)4ε2
c2

)

O( |S||A|

(1−γ)3ε2 )

RL. Our proof technique is signiﬁcantly diﬀerent from
their work. Moreover, we also provide open-source ex-
perimental results that illustrate the performance of
our robust RL algorithm.

Other related works: Robust control
is a well-
studied area (Zhou et al., 1996; Dullerud and Paganini,
2013) in the classical control theory. Recently, there
are some interesting works that address the robust RL
problem using this framework, especially focusing on
the linear quadratic regulator setting (Zhang et al.,
2020b). Our framework of robust MDP is signiﬁcantly
diﬀerent from this line of work. Risk sensitive RL algo-
rithms (Borkar, 2002) and adversarial RL algorithms
(Pinto et al., 2017) also address the robustness prob-
lem implicitly. Our approach is diﬀerent from these
works also.

Notations: For any set X , |X | denotes its cardinality.
For any vector x, (cid:107)x(cid:107) denotes its inﬁnity norm (cid:107)x(cid:107)∞.

2 Preliminaries: Robust Markov

Decision Process

(MDP)

is a tuple
A Markov Decision Process
(S, A, r, P, γ), where S is the state space, A is the ac-
tion space, r : S × A → R is the reward function,
and γ ∈ (0, 1) is the discount factor. The transition
probability function Ps,a(s(cid:48)) represents the probability
of transitioning to state s(cid:48) when action a is taken at
state s. P is also called the the model of the system.
We consider a ﬁnite MDP setting where |S| and |A|
are ﬁnite. We will also assume that r(s, a) ∈ [0, 1], for
all (s, a) ∈ S × A, without loss of generality.

Uncertainty set: Unlike the standard MDP which
considers a single model (transition probability func-
tion), the RMDP formulation considers a set of mod-
els. We call this set as the uncertainty set and denote it
as P. We assume that the set P satisﬁes the standard
rectangularity condition (Iyengar, 2005). We note that
a similar uncertainty set can be considered for the re-
ward function at the expense of additional notations.
However, since the analysis will be similar and the
samples complexity guarantee will be identical upto
a constant, without loss of generality, we assume that
the reward function is known and deterministic. We
specify a robust MDP as a tuple M = (S, A, r, P, γ).

The uncertainty set P is typically deﬁned as

P = ⊗ Ps,a, where, Ps,a = {Ps,a ∈ [0, 1]|S|

:

D(Ps,a, P o

s,a) ≤ cr,

(cid:88)

s(cid:48)∈S

Ps,a(s(cid:48)) = 1},

(3)

where P o = (P o
s,a, (s, a) ∈ S × A) is the nominal
transition probability function, cr > 0 indicates the
level of robustness, and D(·, ·) is a distance metric be-
tween two probability distributions. In the following,
we call P o as the nominal model. In other words, P
is the set of all valid transition probability functions
in the neighborhood of the nominal model P o, where
the neighborhood is deﬁned using the distance metric
D(·, ·). We note that the radius cr can depend on the
state-action pair (s, a). We omit this to reduce the
notation complexity. We also note that for cr ↓ 0, we
recover the non-robust regime.

We consider three diﬀerent uncertainty sets corre-
sponding to three diﬀerent distance metrics D(·, ·).

A (deterministic) policy π maps each state to an ac-
tion. The value of a policy π for an MDP with model
P , evaluated at state s is given by

1. Total Variation (TV) uncertainty set (P tv): We
deﬁne P tv = ⊗P tv
s,a, where P tv
s,a is deﬁned as in (3)
using the total variation distance

Vπ,P (s) = Eπ,P [

∞
(cid:88)

t=0

γtr(st, at) | s0 = s],

(1)

where at = π(st), st+1 ∼ Pst,at(·). The optimal value
function V ∗
P and the optimal policy π∗
P of an MDP
with the model P are deﬁned as

V ∗
P = max

π

Vπ,P ,

π∗
P = arg max

π

Vπ,P .

(2)

Dtv(Ps,a, P o

s,a) = (1/2)(cid:107)Ps,a − P o

s,a(cid:107)1.

(4)

2. Chi-square uncertainty set (P c): We deﬁne P c =
⊗P c
s,a, where P c
s,a is deﬁned as in (3) using the Chi-
square distance

Dc(Ps,a, P o

s,a) =

(cid:88)

s(cid:48)∈S

(Ps,a(s(cid:48)) − P o
s,a(s(cid:48))
P o

s,a(s(cid:48)))2

.

(5)

Sample Complexity of Robust Reinforcement Learning with a Generative Model

3. Kullback-Leibler (KL) uncertainty set (P kl): We
deﬁne P kl = ⊗P kl
s,a is deﬁned as in (3)
using the Kullback-Leibler (KL) distance

s,a, where P kl

Dkl(Ps,a, P o

s,a) =

Ps,a(s(cid:48)) log

(cid:88)

s(cid:48)

Ps,a(s(cid:48))
s,a(s(cid:48))
P o

.

(6)

We note that the sample complexity and its analysis
will depend on the speciﬁc form of the uncertainty set.

Robust value iteration: The goal of the RMDP
problem is to compute the optimal robust policy which
maximizes the value even under the worst model in the
uncertainty set. Formally, the robust value function
Vπ corresponding to a policy π and the optimal robust
value function V ∗ are deﬁned as (Iyengar, 2005; Nilim
and El Ghaoui, 2005)

V π = inf
P ∈P

Vπ,P ,

V ∗ = sup
π

inf
P ∈P

Vπ,P .

(7)

The optimal robust policy π∗ is such that the robust
value function corresponding to it matches the optimal
robust value function, i.e., V π∗
= V ∗. It is known that
there exists a deterministic optimal policy (Iyengar,
2005) for the RMDP problem. So, we will restrict our
attention to the class of deterministic policies.

For any set B and a vector v, let

σB(v) = inf{u(cid:62)v : u ∈ B}.

Using this notation, we can deﬁne the robust Bellman
operator (Iyengar, 2005) as T (V )(s) = maxa (r(s, a) +
γσPs,a (V )). It is known that T is a contraction map-
ping in inﬁnity norm and the V ∗ is the unique ﬁxed
point of T (Iyengar, 2005). Since T is a contrac-
tion, robust value iteration can be used to compute
V ∗, similar to the non-robust MDP setting (Iyengar,
2005). More precisely, the robust value iteration, de-
ﬁned as Vk+1 = T Vk, converges to V ∗, i.e., Vk → V ∗.
Similar to the optimal robust value function, we can
also deﬁne the optimal robust action-value function
as Q∗(s, a) = r(s, a) + γσPs,a (V ∗). Similar to the
non-robust setting, it is straight forward to show that
π∗(s) = arg maxa Q∗(s, a) and V ∗(s) = maxa Q∗(s, a).

3 Algorithm and Sample Complexity

The robust value iteration requires the knowledge of
the nominal model P o and the radius of the uncer-
tainty set cr to compute V ∗ and π∗. While cr may be
available as design parameter, the form of the nominal
model may not be available in most practical problems.
So, we do not assume the knowledge of the nominal
model P o. Instead, similar to the non-robust RL set-
ting, we assume only to have access to the samples
from a generative model, which can generate samples

Algorithm 1 Robust Empirical Value Iteration
(REVI) Algorithm

1: Input: Loop termination number K
2: Initialize: Q0 = 0
3: Compute the empirical uncertainty set (cid:98)P accord-

ing to (8)

Vk(s) = maxa Qk(s, a), ∀s

4: for k = 0, · · · , K − 1 do
5:
6: Qk+1(s, a) = r(s, a) + γσ
7: end for
8: Output: πK(s) = arg maxa QK(s, a), ∀s ∈ S

(Vk), ∀(s, a)

(cid:98)Ps,a

of the next state s(cid:48) according to P o
s,a(·), given the state-
action pair (s, a) as the input. We propose a model-
based robust RL algorithm that uses these samples to
estimate the nominal model and uncertainty set.

3.1 Robust Empirical Value Iteration

(REVI) Algorithm

We ﬁrst get a maximum likelihood estimate (cid:98)P o of the
nominal model P o by following the standard approach
(Azar et al., 2013, Algorithm 3). More precisely, we
generate N next-state samples corresponding to each
state-action pairs. Then, the maximum likelihood es-
s,a(s(cid:48)) = N (s, a, s(cid:48))/N , where
timate (cid:98)P o is given by (cid:98)P o
N (s, a, s(cid:48)) is the number of times the state s(cid:48) is re-
alized out of the total N transitions from the state-
action pair (s, a). Given (cid:98)P o, we can get an empirical
estimate (cid:98)P of the uncertainty set P as,

(cid:98)P = ⊗ (cid:98)Ps,a, where, (cid:98)Ps,a = {P ∈ [0, 1]S :

D(Ps,a, (cid:98)Ps,a) ≤ cr,

(cid:88)

s(cid:48)∈S

Ps,a(s(cid:48)) = 1},

(8)

where D is one of the metrics speciﬁed in (4) - (6).

For ﬁnding an approximately optimal robust policy, we
now consider the empirical RMDP (cid:99)M = (S, A, r, (cid:98)P, γ)
and perform robust value iteration using (cid:98)P. This is
indeed our approach, which we call the Robust Empir-
ical Value Iteration (REVI) Algorithm. The optimal
robust policy and value function of (cid:99)M are denoted as
(cid:98)π(cid:63), (cid:98)V (cid:63), respectively.

3.2 Sample Complexity

In this section we give the sample complexity guaran-
tee of the REVI algorithm for the three uncertainty
sets. We ﬁrst consider the TV uncertainty set.

Theorem 1 (TV Uncertainty Set). Consider an
RMDP with a total variation uncertainty set P tv. Fix
δ ∈ (0, 1) and ε ∈ (0, 24γ/(1−γ)). Consider the REVI

Kishan Panaganti, Dileep Kalathil

algorithm with K ≥ K0 and N ≥ N tv, where

Finally, we consider the KL uncertainty set.

K0 =

N tv =

8γ

log(

1
log(1/γ)
72γ2|S|
(1 − γ)4ε2 log(

ε(1 − γ)2 ) and
144γ|S||A|
(δε(1 − γ)2)

).

(9)

(10)

Theorem 3 (KL Uncertainty Set). Consider an
RMDP with a KL uncertainty set P kl. Fix δ ∈ (0, 1)
and ε ∈ (0, 1/(1 − γ)). Consider the REVI algorithm
with K ≥ K0 and N ≥ N kl, where K0 is as in (9) and

Then, (cid:107)V ∗ − V πK (cid:107) ≤ ε with probability at least 1 − 2δ.

N kl=

(1−γ)4ε2 ).

Remark 1. The total number of samples needed in the
REVI algorithm is Ntotal = N |S||A|. So the sample
complexity of the REVI algorithm with the TV uncer-
tainty set is O( |S|2|A|
Remark 2 (Comparison with the sample complexity
of the non-robust RL). For the non-robust setting,
the lowerbound for the total number of samples from
the generative sampling device is Ω( |S||A|
ε2(1−γ)3 log |S||A|
)
(Azar et al., 2013, Theorem 3). The variance re-
duced value iteration algorithm proposed in (Sid-
ford et al., 2018) achieves a sample complexity
of O( |S||A|
), matching the lower bound.
However, this work is restricted to ε ∈ (0, 1), whereas
ε can be considered upto the value 1/(1 − γ) for the
MDP problems. Recently, this result has been fur-
ther improved recently by (Agarwal et al., 2020) and
(Li et al., 2020), which considered ε ∈ (0, 1/(cid:112)(1 − γ))
and ε ∈ (0, 1/(1 − γ)), respectively.

ε2(1−γ)3 log |S||A|

δε

δ

Theorem 1 for the robust RL setting also considers ε
upto O(1/(1 − γ)). However, the sample complexity
obtained is worse by a factor of |S| and 1/(1 − γ) when
compared to the non-robust setting. These additional
terms are appearing in our result due to a covering
number argument we used in the proof, which seems
necessary for getting a tractable bound. However, it
is not clear if this is fundamental to the robust RL
problem with TV uncertainty set. We leave this inves-
tigation for our future work.

We next consider the chi-square uncertainty set.

Theorem 2 (Chi-square Uncertainty Set). Consider
an RMDP with a Chi-square uncertainty set P c. Fix
δ ∈ (0, 1) and ε ∈ (0, 16γ/(1 − γ)), for an absolute
constant c1 > 1. Consider the REVI algorithm with
K ≥ K0 and N ≥ N c, where K0 is as given in (9)
and

N c =

64γ2(2cr + 1)|S|
(1 − γ)4ε2

log(

192|S||A|γ
(δε(1 − γ)2)

).

(11)

Then, (cid:107)V ∗ − V πK (cid:107) ≤ ε with probability at least 1 − 2δ.

Remark 3. The sample complexity of the algorithm
with the chi-square uncertainty set is O( |S|2|A|cr
(1−γ)4ε2 ). The
order of sample complexity remains the same com-
pared to that of the TV uncertainty set given in The-
orem 1.

8γ2|S|
r(1 − γ)4ε2 exp(
c2

2λkl + 4
λkl(1 − γ)

) log(

9|S||A|
δλkl(1 − γ)

),

(12)

and λkl is a problem dependent parameter but indepen-
dent of N kl. Then, (cid:107)V ∗ − V πK (cid:107) ≤ ε with probability
at least 1 − 2δ.

1

exp(

Remark 4. The sample complexity with the KL un-
|S|2|A|
certainty set is O(
(1−γ) )). We note that
(1−γ)4ε2c2
r
(Zhou et al., 2021) also considered the robust RL prob-
lem with KL uncertainty set. They provided a sample
complexity bound of the form O( C|S|2|A|
), However
(1−γ)4ε2c2
r
the exponential dependence on 1/(1 − γ) was hidden
inside the constant C. In this work, we clearly specify
the depends on the factor 1/(1 − γ).

4 Why Do We Need Robust Policies?

In the introduction, we have given a qualitative de-
scription about the need for ﬁnding a robust policy.
In this section, we give a formal argument to show
that the worst case performance of a non-robust pol-
icy can be arbitrarily bad (as bad as a random policy)
when compared to that of a robust policy.

We consider a simple setting with an uncertainty set
that contains only two models, i.e., P = {P o, P (cid:48)}. Let
π∗ be the optimal robust policy. Following the nota-
tion in (2), let πo = πP o and π(cid:48) = πP (cid:48) be the non-
robust optimal policies when the model is P o and P (cid:48),
respectively. Assume that nominal model is P o and we
decide to employ the non-robust policy πo. The worst
case performance of πo is characterized by its robust
value function V πo

which is min{Vπo,P o, Vπo,P (cid:48)}.

We now state the following result.

Theorem 4 (Robustness Gap). There exists a robust
MDP M with uncertainty set P = {P o, P (cid:48)}, discount
factor γ ∈ (γo, 1], and state s1 ∈ S such that

V πo

(s1) ≤ V π∗

(s1) − c/(1 − γ),

where c is a positive constant, π∗ is the optimal robust
policy, and πo = πP o is the non-robust optimal policy
when the model is P o.

Theorem 4 states that the worst case performance of
the non-robust policy πo is lower than that of the op-
timal robust policy π∗, and this performance gap is

Sample Complexity of Robust Reinforcement Learning with a Generative Model

Ω(1/(1 − γ)). Since |r(s, a)| ≤ 1, ∀(s, a) ∈ S × A by
assumption, (cid:107)Vπ,P (cid:107) ≤ 1/(1 − γ) for any policy π and
any model P . Therefore, the diﬀerence between the
optimal (robust) value function and the (robust) value
function of an arbitrary policy cannot be greater than
O(1/(1 − γ)). Thus the worst-case performance of the
non-robust policy πo can be as bad as an arbitrary
policy in an order sense.

5 Sample Complexity Analysis

In this section we explain the key ideas used in the
analysis of the REVI algorithm for obtaining the sam-
ple complexity bound for each of the uncertainty sets.
Recall that we consider an RMDP M and its empirical
estimate version as (cid:99)M .
To bound (cid:107)V ∗ − V πK (cid:107), we split it into three terms as
(cid:107)V ∗−V πK (cid:107) ≤ (cid:107)V ∗− (cid:98)V ∗(cid:107)+(cid:107) (cid:98)V ∗− (cid:98)V πK (cid:107)+(cid:107) (cid:98)V πK −V πK (cid:107),
and analyze each term separately.

Analyzing the second term, (cid:107) (cid:98)V ∗ − (cid:98)V πK (cid:107), is similar to
that of non-robust algorithms. Due to the contraction
property of the robust Bellman operator, it is straight
forward to show that (cid:107) (cid:98)V ∗ − (cid:98)V πk+1 (cid:107) ≤ γ(cid:107) (cid:98)V ∗ − (cid:98)V πk (cid:107)
for any k. This exponential convergence, with some
additional results from the MDP theory, enables us to
get a bound (cid:107) (cid:98)V ∗ − (cid:98)V πK (cid:107) ≤ 2γK+1/(1 − γ)2.

The analysis of terms (cid:107)V ∗ − (cid:98)V ∗(cid:107) and (cid:107) (cid:98)V πK −V πK (cid:107) are
however non-trivial and signiﬁcantly more challenging
compared to the non-robust setting. We will focus on
the latter, and the analysis of the former is similar.

For any policy π and for any state s, and denoting
a = π(s), we have

V π(s) − (cid:98)V π(s) = γσPs,a (V π) − γσ
= γ(σPs,a (V π) − σPs,a ( (cid:98)V π)) + γ(σPs,a ( (cid:98)V π) − σ

( (cid:98)V π)

(cid:98)Ps,a

(cid:98)Ps,a

( (cid:98)V π))
(13)

To bound the ﬁrst term in (13), we present a result
that shows that σPs,a is 1-Lipschitz in the sup-norm.
Lemma 1. For any (s, a) ∈ S×A and for any V1, V2 ∈
R|S|, we have |σPs,a (V1) − σPs,a (V2)| ≤ (cid:107)V1 − V2(cid:107) and
(V2)| ≤ (cid:107)V1 − V2(cid:107).
|σ

(V1) − σ

(cid:98)Ps,a

(cid:98)Ps,a

Using the above lemma, the ﬁrst term in (13) will be
bounded by γ(cid:107)V π − (cid:98)V π(cid:107) and the discount factor makes
this term amenable to getting a closed form bound.

( (cid:98)V π) is the
Obtaining a bound for σPs,a ( (cid:98)V π) − σ
most challenging part of our analysis.
In the non-
robust setting, this will be equivalent to the error term
P o
s,aV − (cid:98)Ps,aV , which is unbiased and can be easily
bounded using concentration inequalities. In the ro-
bust setting, however, because of the nonlinear nature

(cid:98)Ps,a

of the function σ(·), E[σ
( (cid:98)V π)] (cid:54)= σPs,a ( (cid:98)V π). So,
(cid:98)Ps,a
using concentration inequalities to get a bound is not
immediate. Our strategy is to ﬁnd appropriate upper-
bound for this term that is amenable to using concen-
tration inequalities. To that end, we will analyze this
term separately for each of the three uncertainty set.

5.1 Total variation uncertainty set

We will ﬁrst get following upperbound:
Lemma 2 (TV uncertainty set). Let V = {V ∈ R|S| :
(cid:107)V (cid:107) ≤ 1/(1 − γ)}. For any (s, a) ∈ S × A and for any
V ∈ V,

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )| ≤ 2 max
µ∈V

| (cid:98)Ps,aµ − P o

s,aµ|.

(14)

While the term | (cid:98)Ps,aµ − P o
s,aµ| in (14) can be up-
perbounded using the standard Hoeﬀding’s inequality,
bounding maxµ∈V | (cid:98)Ps,aµ − P o
s,aµ| is more challenging
as it requires a uniform bound. Since µ can take a
continuum of values, a simple union bound argument
will also not work. We overcome this issue by using
a covering number argument and obtain the following
bound.
Lemma 3. Let V ∈ R|S| with (cid:107)V (cid:107) ≤ 1/(1 − γ). For
any η, δ ∈ (0, 1),

max
µ:0≤µ≤V

max
s,a

1
1 − γ

| (cid:98)Ps,aµ − P o
(cid:115)

s,aµ| ≤

|S|
2N

log(

12|S||A|
(δη(1 − γ))

+ 2η,

with probability at least 1 − δ/2.

We note that this uniform bound adds an additional
(cid:112)|S| factor compared to the non-robust setting, which
results in an additional |S| in the sample complexity.
Combining these, we ﬁnally get the following result.
Proposition 1. Let V = {V ∈ R|S| : (cid:107)V (cid:107) ≤ 1/(1 −
γ)}. For any η, δ ∈ (0, 1), with probability at least 1−δ,

(V )| ≤ C tv

u (N, η, δ), where,

max
V ∈V

|σ

(cid:98)P tv
s,a

(V ) − σP tv

max
s,a
C tv
u (N, η, δ) = 4η +
(cid:114)

s,a

2
1 − γ

|S| log(6|S||A|/(δη(1 − γ)))
2N

.

(15)

Tracing back the steps to (13), we can get an arbitrary
small bound for (cid:107)V π − (cid:98)V π(cid:107) by selecting N appropri-
ately, as speciﬁed in Theorem 1.

5.2 Chi-square uncertainty set

We will ﬁrst get the following upperbound:

Kishan Panaganti, Dileep Kalathil

Lemma 4 (Chi-square uncertainty set). For any
(s, a) ∈ S × A and for any V ∈ R|S|, (cid:107)V (cid:107) ≤ 1/(1 − γ),

(V )| ≤

|σ

(cid:98)P c

s,a

max
µ:0≤µ≤V

(V ) − σP c
(cid:113)
|

crVar

s,a

(V − µ) −

(cid:113)

crVarP o

s,a

(V − µ)|

(cid:98)Ps,a

+ max

µ:0≤µ≤V

| (cid:98)Ps,a(V − µ) − P o

s,a(V − µ)|.

(16)

The second term of (16) can be bounded using Lemma
3. However, the ﬁrst term, which involves the square-
root of the variance is more challenging. We use a con-
centration inequality that is applicable for variance to
overcome this challenge. Finally, we get the following
result.
Proposition 2. Let V = {V ∈ R|S| : (cid:107)V (cid:107) ≤ 1/(1 −
γ)}. For any η, δ ∈ (0, 1), with probability at least
(1 − δ),

max
V ∈V

max
s,a

|σ

(cid:98)P c

s,a

(V ) − σP c

s,a

(V )| ≤ C c

u(N, η, δ), where,

C c

u(N, η, δ) ≤
(cid:114)

(cid:114) 32ηcr
1 − γ

+ 2η+

1
1 − γ

(2cr + 1)|S| log(12|S||A|/(δη(1 − γ)))
N

, (17)

Now, by selecting appropriate N as speciﬁed in Theo-
rem 2, we can show the ε-optimality of πK.

The details on the KL uncertainty set analysis is in-
cluded in the appendix.

6 Experiments

In this section we demonstrate the convergence be-
havior and robust performance of our REVI algorithm
using numerical experiments. We consider two dif-
ferent settings, namely, the Gambler’s Problem envi-
ronment (Sutton and Barto, 2018, Example 4.3) and
FrozenLake8x8 environment in OpenAI Gym (Brock-
man et al., 2016). We also consider the TV uncertainty
set and chi-square uncertainty set. We solve the opti-
mization problem σ
(cid:98)P and σP using the Scipy (Virtanen
et al., 2020) optimization library.

We illustrate the following important characteristics of
the REVI algorithm:

(1) Rate of convergence with respect to the number
of iterations: To demonstrate this, we plot (cid:107)Vk − V ∗(cid:107)
against the iteration number k, where Vk is the value
at the kth step of the REVI algorithm with N = 5000.
We compute V ∗ using the full knowledge of the un-
certainty set for benchmarking the performance of the
REVI algorithm.

(2) Rate of convergence with respect to the number of
samples: To show this, we plot (cid:107)VK(N ) − V ∗(cid:107) against
the number of samples N , where VK(N ) is ﬁnal value
obtained from the REVI algorithm using N samples.
(3) Robustness of the learned policy: To demonstrate
this, we plot the number of times the robust policy
πK (obtained from the REVI algorithm) successfully
completed the task as a function of the change in an
environment parameter. We perform 1000 trials for
each environment and each uncertainty set, and plot
the fraction of the success.

Gambler’s Problem: In gambler’s problem, a gam-
bler starts with a random balance in her account
and makes bets on a sequence of coin ﬂips, win-
ning her stake with heads and losing with tails, un-
til she wins $100 or loses all money. This problem
can be formulated as a chain MDP with states in
{1, · · · , 99} and when in state s the available actions
are in {0, 1, · · · , min(s, 100 − s)}. The agent is re-
warded 1 after reaching a goal and rewarded 0 in every
other timestep. The biased coin probability is ﬁxed
throughout the game. We denote its heads-up proba-
bility as ph and use 0.6 as a nominal model for training
our algorithm. We also ﬁx cr = 0.2 for the chi-square
uncertainty set experiments and cr = 0.4 for the TV
uncertainty set experiments.

The red curves with square markers in the ﬁrst two
plots in Fig. 1 show the rate of convergence with
respect to the number of iterations for TV and chi-
square uncertainty sets respectively. As expected, con-
vergence is fast due to the contraction property of the
robust Bellman operator.

The blue curves with triangle markers in the ﬁrst two
plots in Fig. 1 show the rate of convergence with re-
spect to the number of samples for TV and chi-square
uncertainty sets. We generated these curves for 10 dif-
ferent seed runs. The bold line depicts the mean of
these runs and the error bar is the standard deviation.
As expected, the plots show that VK(N ) converges to
V ∗ as N increases.

We then demonstrate the robustness of the ap-
proximate robust policy πK (obtained with N =
100, 500, 3000) by evaluating its performance on en-
vironments with diﬀerent values of ph. We plot the
fraction of the wins out of 1000 trails. We also plot the
performance the optimal robust policy π∗ as a bench-
mark. The third and fourth plot in Fig. 1 show the
results with TV and chi-square uncertainty sets respec-
tively. We note that the performance of the non-robust
policy decays drastically as we decrease the parameter
ph from its nominal value 0.6. On the other hand, the
optimal robust policy performs consistently better un-
der this change in the environment. We also note that

Sample Complexity of Robust Reinforcement Learning with a Generative Model

Figure 1: Experiment results for the Gambler’s problem. The ﬁrst two plots shows the rate of convergence with
respect to the number of iterations (k) and the rate of convergence with respect to the number of samples (N )
for the TV and chi-square uncertainty set, respectively. The third and fourth plots shows the robustness of the
learned policy against changes in the model parameter (heads-up probability).

Figure 2: Experiment results for the FrozenLake8x8 environment. The ﬁrst two plots shows the rate of con-
vergence with respect to the number of iterations (k) and the rate of convergence with respect to the number
of samples (N ) for the TV and chi-square uncertainty set, respectively. The third and fourth plots shows the
robustness of the learned policy against changes in the model parameter (probability of picking a random action).

πK(N ) closely follows the performance of π∗ for large
enough N .

Frozen Lake environment: FrozenLake8x8 is a
gridworld environment of size 8×8. It consists of some
ﬂimsy tiles which makes the agent fall into the water.
The agent is rewarded 1 after reaching a goal tile with-
out falling and rewarded 0 in every other timestep. We
use the FrozenLake8x8 environment with default de-
sign as our nominal model except that we make the
probability of transitioning to a state in the intended
direction to be 0.4 (the default value is 1/3). We also
set cr = 0.35 for the chi-square uncertainty set ex-
periments and cr = 0.7 for the TV uncertainty set
experiments.

The red curves in the ﬁrst two plots in Fig. 2 show
the rate of convergence with respect to the number
of iterations for TV and chi-square uncertainty sets
respectively. The blue curves in the ﬁrst two plots in
Fig. 2 show the rate of convergence with respect to the
number of samples for TV and chi-square uncertainty
sets respectively. The behavior is similar to the one
observed in the case of gambler’s problem.

We demonstrate the robustness of the learned policy
by evaluating it on FrozenLake test environments with
action perturbations. In the real-world settings, due

to model mismatch or noise in the environments, the
resulting action can be diﬀerent from the intended ac-
tion. We model this by picking a random action with
some probability at each time step.
In addition, we
change the probability of transitioning to a state in
the intended direction to be 0.2 for these test environ-
ments. We observe that the performance of the robust
RL policy is consistently better than the non-robust
policy as we introduce model mismatch in terms of
the probability of picking random actions. We also
note that πK(N ) closely follows the performance of π∗
for large enough N .

We note that we have included our code for ex-
periments in this GitHub page. We note that we
can employ a hyperparameter learning strategy to
ﬁnd the best value of cr. We demonstrate this on
the FrozenLake environment for the TV uncertainty
set. We computed the optimal robust policy for
each cr ∈ {0.1, 0.2, . . . , 1.6}. We tested these policies
across 1000 games with random action probabilities
{0.1, 0.2, . . . , 0.5} on the test environment. We found
that the policy for cr = 1.2 has the best winning ratio
across all the random action probabilities. We do not
exhibit exhaustive experiments on this hyperparame-
ter learning strategy as it is out of scope of the intent
of this manuscript.

0.20.30.40.50.60.77est heads-up prRbability0.00.20.40.60.81.05atiR Rf winning in 1000 gameschi-sTuare uncertainty setnRn-rRbust RptimalrRbust RptimalrRbust, N 100rRbust, N 500rRbust, N 30001234567iteration k■0.10.20.30.40.5||Vk−V*||■102103N samples▾0.00.10.20.30.40.50.6||VK−V*||▾TV uncertainty set123456iteration k■0.000.050.100.150.200.250.300.35||Vk−V*||■102103N samples▾0.050.100.150.20||VK−V*||▾chi-square uncertainty set0.20.30.40.50.60.7Test heads-up probability0.00.20.40.60.81.0Ratio of winning in 1000 gamesTV uncertainty set0.20.30.40.50.60.7Test heads-up probability0.00.20.40.60.81.0Ratio of winning in 1000 gameschi-square uncertainty set0.00.10.20.30.40.53rRbability Rf picking randRP actiRn0.00.20.40.60.81.05atiR Rf winning in 1000 gaPesℓ1 uncertainty setnRn-rRbust RptiPalrRbust RptiPalrRbust, N 50rRbust, N 1000rRbust, N 30000255075100125150175iteration k■0.10.20.30.40.50.60.7||Vk−V*||■102103N samples▾0.100.120.140.160.18||VK−V*||▾TV uncertainty set024681012iteration k■0.20.30.40.50.60.7||Vk−V*||■102103N samples▾0.090.100.110.120.130.140.150.16||VK−V*||▾chi-square uncertainty set0.00.10.20.30.40.5Probability of picking random action0.00.20.40.60.81.0Ratio of winning in 1000 gamesTV uncertainty set0.00.10.20.30.40.5Probability of picking random action0.00.20.40.60.81.0Ratio of winning in 1000 gameschi-square uncertainty setKishan Panaganti, Dileep Kalathil

7 Conclusion and Future Work

We presented a model-based robust reinforcement
learning algorithm called Robust Empirical Value Iter-
ation algorithm, where we used an approximate robust
Bellman updates in the vanilla robust value iteration
algorithm. We provided a ﬁnite sample performance
characterization of the learned policy with respect to
the optimal robust policy for three diﬀerent uncer-
tainty sets, namely, the total variation uncertainty
set, the chi-square uncertainty set, and the Kullback-
Leibler uncertainty set. We also demonstrated the per-
formance of REVI algorithm on two diﬀerent environ-
ments showcasing its theoretical properties of conver-
gence. We also showcased the REVI algorithm based
policy being robust to the changes in the environment
as opposed to the non-robust policies.

The goal of this work was to develop the fundamental
theoretical results for the ﬁnite state space and action
space regime. As mentioned earlier, the sub-optimality
of the sample complexity of our REVI algorithm in fac-
tors |S| and 1/(1 − γ) needs more investigation and re-
ﬁnements in the analyses. In the future, we will extend
this idea to robust RL with linear and nonlinear func-
tion approximation architectures and for more general
models in deep RL.

Acknowledgments

Dileep Kalathil gratefully acknowledges funding from
the U.S. National Science Foundation (NSF) grants
NSF-EAGER-1839616, NSF-CRII-CPS-1850206 and
NSF-CAREER-EPCN-2045783.

References

Agarwal, A., Kakade, S., and Yang, L. F. (2020).
Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on
Learning Theory, pages 67–83. 2, 5

Azar, M. G., Munos, R., and Kappen, H. J. (2013).
Minimax PAC bounds on the sample complexity
of reinforcement learning with a generative model.
Mach. Learn., 91(3):325–349. 2, 4, 5

Borkar, V. S. (2002). Q-learning for risk-sensitive con-
trol. Mathematics of operations research, 27(2):294–
311. 3

Brockman, G., Cheung, V., Pettersson, L., Schneider,
J., Schulman, J., Tang, J., and Zaremba, W. (2016).
Openai gym. arXiv preprint arXiv:1606.01540. 7

Derman, E., Mankowitz, D., Mann, T., and Mannor,
S. (2020). A bayesian approach to robust reinforce-
ment learning. In Uncertainty in Artiﬁcial Intelli-
gence, pages 648–658. 1

Derman, E., Mankowitz, D. J., Mann, T. A., and
Mannor, S. (2018). Soft-robust actor-critic policy-
gradient. In AUAI press for Association for Uncer-
tainty in Artiﬁcial Intelligence, pages 208–218. 1,
2

Dullerud, G. E. and Paganini, F. (2013). A course in
robust control theory: a convex approach, volume 36.
Springer Science & Business Media. 3

Haskell, W. B., Jain, R., and Kalathil, D. (2016). Em-
pirical dynamic programming. Mathematics of Op-
erations Research, 41(2):402–429. 2

Iyengar, G. N. (2005). Robust dynamic programming.
Mathematics of Operations Research, 30(2):257–280.
1, 3, 4, 13, 14, 15, 18

Kalathil, D., Borkar, V. S., and Jain, R. (2021).
Empirical Q-Value Iteration. Stochastic Systems,
11(1):1–18. 2

Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020).
Breaking the sample size barrier in model-based re-
In
inforcement learning with a generative model.
Advances in Neural Information Processing Sys-
tems, volume 33, pages 12861–12872. 2, 3, 5

Lim, S. H. and Autef, A. (2019). Kernel-based rein-
forcement learning in robust Markov decision pro-
cesses.
In International Conference on Machine
Learning, pages 3973–3981. 2

Lim, S. H., Xu, H., and Mannor, S. (2013). Reinforce-
ment learning in robust Markov decision processes.
In Advances in Neural Information Processing Sys-
tems, pages 701–709. 2

Mankowitz, D. J., Levine, N., Jeong, R., Abdolmaleki,
A., Springenberg, J. T., Shi, Y., Kay, J., Hester,
T., Mann, T., and Riedmiller, M. (2020). Robust
reinforcement learning for continuous control with
model misspeciﬁcation. In International Conference
on Learning Representations. 1, 2

Mannor, S., Mebel, O., and Xu, H. (2016). Robust
mdps with k-rectangular uncertainty. Mathematics
of Operations Research, 41(4):1484–1509. 1

Maurer, A. and Pontil, M. (2009). Empirical bernstein
bounds and sample-variance penalization. In COLT
2009 - The 22nd Conference on Learning Theory,
Montreal, Quebec, Canada, June 18-21, 2009. 11

Nilim, A. and El Ghaoui, L. (2005). Robust control of
Markov decision processes with uncertain transition
matrices. Operations Research, 53(5):780–798. 1, 4,
18, 21

Panaganti, K. and Kalathil, D. (2021). Robust re-
inforcement learning using least squares policy it-
eration with provable performance guarantees.
In
Proceedings of the 38th International Conference on
Machine Learning, pages 511–520. 1, 2

Sample Complexity of Robust Reinforcement Learning with a Generative Model

Petrik, M. and Subramanian, D. (2014). Raam: The
beneﬁts of robustness in approximating aggregated
mdps in reinforcement learning.
In NIPS, pages
1979–1987. 2

Xu, H. and Mannor, S. (2010). Distributionally robust
Markov decision processes. In Advances in Neural
Information Processing Systems, pages 2505–2513.
1

Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
(2017). Robust adversarial reinforcement learning.
In International Conference on Machine Learning,
pages 2817–2826. 1, 3

Yang, W., Zhang, L., and Zhang, Z. (2021). Towards
theoretical understandings of robust markov deci-
sion processes: Sample complexity and asymptotics.
arXiv preprint arXiv:2105.03863. 2

Roy, A., Xu, H., and Pokutta, S. (2017). Reinforce-
ment learning under model mismatch. In Advances
in Neural Information Processing Systems, pages
3043–3052. 1, 2

Yu, P. and Xu, H. (2015). Distributionally robust
counterpart in Markov decision processes.
IEEE
Transactions on Automatic Control, 61(9):2538–
2543. 1

Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning,
D., and Hsieh, C.-J. (2020a). Robust deep reinforce-
ment learning against adversarial perturbations on
state observations. Advances in Neural Information
Processing Systems, 33:21024–21037. 1

Zhang, K., Hu, B., and Basar, T. (2020b). Policy
optimization for H2 linear control with H∞ robust-
ness guarantee: Implicit regularization and global
In Proceedings of the 2nd Annual
convergence.
Conference on Learning for Dynamics and Control,
L4DC 2020, Online Event, Berkeley, CA, USA, 11-
12 June 2020, volume 120 of Proceedings of Machine
Learning Research, pages 179–190. 3

Zhou, K., Doyle, J. C., Glover, K., et al. (1996). Robust
and optimal control, volume 40. Prentice hall New
Jersey. 3

Zhou, Z., Bai, Q., Zhou, Z., Qiu, L., Blanchet, J., and
Glynn, P. (2021). Finite-sample regret bound for
distributionally robust oﬄine tabular reinforcement
learning. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 3331–3339. 2, 5,
17

Russel, R. H. and Petrik, M. (2019). Beyond con-
ﬁdence regions: Tight bayesian ambiguity sets for
robust mdps. Advances in Neural Information Pro-
cessing Systems. 1

Sidford, A., Wang, M., Wu, X., Yang, L. F., and Ye, Y.
(2018). Near-optimal time and sample complexities
for solving markov decision processes with a genera-
tive model. In Proceedings of the 32nd International
Conference on Neural Information Processing Sys-
tems, pages 5192–5202. 2, 5

Singh, S. P. and Yee, R. C. (1994). An upper bound on
the loss from approximate optimal-value functions.
Machine Learning, 16(3):227–233. 2, 14

Sutton, R. S. and Barto, A. G. (2018). Reinforcement

learning: An introduction. MIT press. 7

Tamar, A., Mannor, S., and Xu, H. (2014). Scaling up
robust mdps using function approximation. In In-
ternational Conference on Machine Learning, pages
181–189. 2

van Handel, R. (2014). Probability in High Dimension.

Technical report, Princeton University NJ. 11

Vershynin, R. (2018). High-Dimensional Probability:
An Introduction with Applications in Data Science,
volume 47. Cambridge University press. 11

Virtanen, P., Gommers, R., Oliphant, T. E., Haber-
land, M., Reddy, T., Cournapeau, D., Burovski,
E., Peterson, P., Weckesser, W., Bright, J., van der
Walt, S. J., Brett, M., Wilson, J., Millman, K. J.,
Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R.,
Larson, E., Carey, C. J., Polat, ˙I., Feng, Y., Moore,
E. W., VanderPlas, J., Laxalde, D., Perktold, J.,
Cimrman, R., Henriksen, I., Quintero, E. A., Harris,
C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,
F., van Mulbregt, P., and SciPy 1.0 Contributors
(2020). SciPy 1.0: Fundamental Algorithms for
Scientiﬁc Computing in Python. Nature Methods,
17:261–272. 7

Wiesemann, W., Kuhn, D., and Rustem, B. (2013).
Robust Markov decision processes. Mathematics of
Operations Research, 38(1):153–183. 1

Kishan Panaganti, Dileep Kalathil

Appendix

A Useful Technical Results

In this section we state some existing results that are useful in our analysis.
Lemma 5 (Hoeﬀding’s inequality (Vershynin, 2018, Theorem 2.2.6)). Let X1, · · · , XT be independent random
variables. Assume that Xt ∈ [mt, Mt] for every t with Mt > mt. Then, for any ε > 0, we have

(cid:33)

(cid:32)

(Xt − E[Xt]) ≥ ε

≤ exp

−

(cid:32) T

(cid:88)

P

t=1

2ε2
t=1(Mt − mt)2

(cid:80)T

(cid:33)

.

Lemma 6 (Self-bounding variance inequality (Maurer and Pontil, 2009, Theorem 10)). Let X1, · · · , XT be in-
dependent and identically distributed random variables with ﬁnite variance, that is, Var(X1) < ∞. Assume that
Xt ∈ [0, M ] for every t with M > 0, and let S2

t=1 Xt)2. Then, for any ε > 0, we have

t=1 X 2

t − ( 1
T

T = 1
T

(cid:80)T

(cid:80)T

P

(cid:16)

(cid:17)
|ST − (cid:112)Var(X1)| ≥ ε

(cid:18)

≤ 2 exp

−

(cid:19)

.

T ε2
2M 2

Proof. The proof of this lemma directly follows from (Maurer and Pontil, 2009, Theorem 10) by noting that we
can rewrite S2

T as follows

T
T − 1

S2

T =

1
T (T − 1)

T
(cid:88)

(Xi − Xj)2.

i,j=1

Also, note that we apply (Maurer and Pontil, 2009, Theorem 10) for the scaled random variables Xt/M ∈
[0, 1].

We now provide a covering number result that is useful to get high probability concentration bounds for value
function classes. We ﬁrst deﬁne minimal η-cover of a set.
Deﬁnition 1 (Minimal η-cover; (van Handel, 2014, Deﬁnition 5.5)). A set NV (η) is called an η-cover for a
metric space (V, d) if for every V ∈ V, there exists a V (cid:48) ∈ N such that d(V, V (cid:48)) ≤ η. Furthermore, NV (η) with
the minimal cardinality (|NV (η)|) is called a minimal η-cover.

From (van Handel, 2014, Exercise 5.5 and Lemma 5.13) we have the following result.
Lemma 7 (Covering Number). Let V = {V ∈ R|S|
respect to the distance metric d(V, V (cid:48)) = (cid:107)V − V (cid:48)(cid:107) for some ﬁxed η ∈ (0, 1). Then we have

: (cid:107)V (cid:107) ≤ Vmax}. Let NV (η) be a minimal η-cover of V with

log |NV (η)| ≤ |S| · log

(cid:18) 3 Vmax
η

(cid:19)

.

Proof. We will consider the normalized metric space (Vn, dn), where

Vn := V/Vmax = {V ∈ R|S|

: (cid:107)V (cid:107) ≤ 1}

and dn := d/Vmax to make use of the fact that the covering number is invariant to the scaling of a metric space.
Let ηn := η/Vmax. Then, it follows from (van Handel, 2014, Exercise 5.5 and Lemma 5.13) that

log |NV (η)| = log |NVn (ηn)| ≤ |S| · log

(cid:19)

(cid:18) 3
ηn

= |S| · log

(cid:18) 3 Vmax
η

(cid:19)

.

This completes the proof.

Here we present another covering number result, with a similar proof as Lemma 7, that is useful to get our
upperbound for the KL uncertainty set.
Lemma 8 (Covering Number of a bounded real line). Let Θ ⊂ R with Θ = [l, u] for some real numbers u > l. Let
NΘ(η) be a minimal η-cover of Θ with respect to the distance metric d(θ, θ(cid:48)) = |θ − θ(cid:48)| for some ﬁxed η ∈ (0, 1).
Then we have |NΘ(η)| ≤ 3(u − l)/η.

Sample Complexity of Robust Reinforcement Learning with a Generative Model

B Proof of the Theorems

B.1 Concentration Results

Here, we prove Lemma 3. We state the following result ﬁrst.
Lemma 9. For any V ∈ R|S| with (cid:107)V (cid:107) ≤ Vmax, with probability at least 1 − δ,

max
(s,a)

|P o

s,aV − (cid:98)Ps,aV | ≤ Vmax

(cid:114)

log(2|S||A|/δ)
2N

Proof. Fix any (s, a) pair. Consider a discrete random variable X taking value V (j) with probability P o
all j ∈ {1, 2, · · · , |S|}. From Hoeﬀding’s inequality (Lemma 5), we have

s,a(j) for

P(P o

s,aV − (cid:98)Ps,aV ≥ ε) ≤ exp(−2N ε2/V 2

max), P( (cid:98)Ps,aV − P o

s,aV ≥ ε) ≤ exp(−2N ε2/V 2

max).

Choosing ε = Vmax
bound, we get

(cid:113) log(2|S||A|/δ)
2N

, we get P(|P o

s,aV − (cid:98)Ps,aV | ≥ Vmax

(cid:113) log(2|S||A|/δ)
2N

) ≤ δ

|S||A| . Now, using union

P(max
(s,a)

|P o

s,aV − (cid:98)Ps,aV | ≥ Vmax

(cid:114)

log(2|S||A|/δ)
2N

) ≤

(cid:88)

s,a

P(|P o

s,aV − (cid:98)Ps,aV | ≥ Vmax

(cid:114)

log(2|S||A|/δ)
2N

) ≤ δ.

This completes the proof.

Proof of Lemma 3: Let V = {V ∈ R|S| : (cid:107)V (cid:107)∞ ≤ 1/(1 − γ)}. Let NV (η) be a minimal η-cover of V. Fix a
µ ≤ V . By the deﬁnition of NV (η), there exists a µ(cid:48) ∈ NV (η) such that (cid:107)µ − µ(cid:48)(cid:107) ≤ η. Now, for these particular
µ and µ(cid:48), we get

| (cid:98)Ps,aµ − P o

s,aµ| ≤ | (cid:98)Ps,aµ − (cid:98)Ps,aµ(cid:48)| + | (cid:98)Ps,aµ(cid:48) − P o

s,aµ(cid:48)| + |P o

s,aµ(cid:48) − P o

s,aµ|

(a)
≤ (cid:107) (cid:98)Ps,a(cid:107)1(cid:107)µ − µ(cid:48)(cid:107)∞ + | (cid:98)Ps,aµ(cid:48) − P o
s,aµ(cid:48)| + 2η
≤ sup

| (cid:98)Ps,aµ(cid:48) − P o

max
s,a

µ(cid:48)∈NV (η)

s,aµ(cid:48)| + (cid:107)P o

s,a(cid:107)1(cid:107)µ(cid:48) − µ(cid:107)∞

where (a) follows from H¨older’s inequality. Now, taking max on both sides with respect to µ and (s, a) we get

sup
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ| ≤ sup

max
s,a

| (cid:98)Ps,aµ(cid:48) − P o

s,aµ(cid:48)| + 2η

µ(cid:48)∈NV (η)
(cid:114)

1
1 − γ

(b)
≤

(c)
≤

log(4|S||A||NV (η)|/δ)
2N

+ 2η

(cid:114)

1
1 − γ

|S| log(12|S||A|/(δη(1 − γ)))
2N

+ 2η,

with probability at least 1 − δ/2. Here, (b) follows from Lemma 9 and the union bound and (c) from Lemma
7.

B.2 Proof of Theorem 1

Proof of Lemma 1. We only prove the ﬁrst inequality since the proof is analogous for the other inequality.
For any (s, a) ∈ S × A we have

σPs,a (V2) − σPs,a (V1) = inf

q∈Ps,a

q(cid:62)V2 − inf

˜q∈Ps,a

˜q(cid:62)V1 = inf

q∈Ps,a

sup
˜q∈Ps,a

q(cid:62)V2 − ˜q(cid:62)V1

≥ inf

q∈Ps,a

q(cid:62)(V2 − V1) = σPs,a (V2 − V1).

(18)

Kishan Panaganti, Dileep Kalathil

By deﬁnition, for any arbitrary ε > 0, there exists a Ps,a ∈ Ps,a such that

P (cid:62)

s,a(V2 − V1) − ε ≤ σPs,a (V2 − V1).

(19)

Using (19) and (18),

σPs,a (V1) − σPs,a (V2) ≤ P (cid:62)

s,a(V1 − V2) + ε

(a)
≤ (cid:107)Ps,a(cid:107)1(cid:107)V1 − V2(cid:107) + ε = (cid:107)V1 − V2(cid:107) + ε

where (a) follows from Holder’s inequality. Since ε is arbitrary, we get, σPs,a (V1) − σPs,a (V2) ≤ (cid:107)V1 − V2(cid:107).
Exchanging the roles of V1 and V2 completes the proof.

Proof of Lemma 2. Fix any (s, a) pair. From (Iyengar, 2005, Lemma 4.3) we have that

σP tv

s,a

(V ) = P o

s,aV + max

µ:0≤µ≤V

(cid:18)

(cid:18)

− P o

s,aµ − cr max

s

(Vµ(s)) + cr min

s

(Vµ(s))

(cid:19)

(cid:19)

σ

(cid:98)P tv
s,a

(V ) = (cid:98)Ps,aV + max

µ:0≤µ≤V

− (cid:98)Ps,aµ − cr max

s

(Vµ(s)) + cr min

s

(Vµ(s))

,

where 0 ≤ µ ≤ V is an elementwise inequality and Vµ(s) = V (s) − µ(s) for all s ∈ S.

Using the fact that | maxx f (x) − maxx g(x)| ≤ maxx |f (x) − g(x)|, it directly follows that

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

Further simplifying we get

(V )| ≤ | (cid:98)Ps,aV − P o

s,aV | + max

µ:0≤µ≤V

| (cid:98)Ps,aµ − P o

s,aµ|.

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )| ≤ | (cid:98)Ps,aV − P o

s,aV | + max

µ:0≤µ≤V

| (cid:98)Ps,aµ − P o

s,aµ|

≤ max
µ∈V

| (cid:98)Ps,aµ − P o

s,aµ| + max

µ:0≤µ≤V

| (cid:98)Ps,aµ − P o

s,aµ| ≤ 2 max
µ∈V

| (cid:98)Ps,aµ − P o

s,aµ|.

This completes the proof.

We are now ready to prove Proposition 1.

Proof of Proposition 1. For any given V ∈ V and (s, a), from Lemma 2, we have

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )| ≤ 2 max
µ∈V

| (cid:98)Ps,aµ − P o

s,aµ| ≤ 2 max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ|.

Taking the maximum over V and (s, a) on both sides, we get

max
V ∈V

max
s,a

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )| ≤ 2 max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ|.

Now, from the proof of Lemma 3, for any η, δ ∈ (0, 1), we get

max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ| ≤

(cid:114)

1
1 − γ

|S| log(6|S||A|/(δη(1 − γ)))
2N

+ 2η,

with probability greater than 1 − δ. Using (23) in (22), we get the desired result.

(20)

(21)

(22)

(23)

We also need the following result that speciﬁes the ampliﬁcation when replacing the algorithm iterate value
function with the value function of the policy towards approximating the optimal value.
Lemma 10. Let Vk and Qk be as given in the REVI algorithm for k ≥ 1. Also, let πk = arg maxa Qk(s, a).
Then,

Furthermore,

(cid:107) (cid:98)V ∗ − (cid:98)V πk (cid:107) ≤

2γ
1 − γ

(cid:107)Vk − (cid:98)V ∗(cid:107).

(cid:107)V ∗ − V πk (cid:107) ≤

2
1 − γ

(cid:107)Qk − Q∗(cid:107).

Sample Complexity of Robust Reinforcement Learning with a Generative Model

Proof. The proof is similar to the proof in (Singh and Yee, 1994, Main Theorem, Corollary 2). A straight forward
modiﬁcation to this proof, using the fact that σ
and σPs,a are 1-Lipschitz functions as shown in Lemma 1,
will give the desired result.

(cid:98)Ps,a

Proof of Theorem 1. Recall the empirical RMDP (cid:99)M = (S, A, r, (cid:98)P tv, γ). For any policy π, let (cid:98)V π be robust
value function of policy π with respect to the RMDP (cid:99)M . The optimal robust policy, value function, and
state-action value function of (cid:99)M are denoted as (cid:98)π(cid:63), (cid:98)V (cid:63) and (cid:98)Q(cid:63), respectively. Also, for any policy π, we have
(cid:98)Qπ(s, a) = r(s, a) + γσ

( (cid:98)V π) and Qπ(s, a) = r(s, a) + γσP tv

(V π).

s,a

(cid:98)P tv
s,a

Let Vk and Qk be as given in the REVI algorithm for k ≥ 1. Also, let πk(s) = arg maxa Qk(s, a). Now,

(cid:107)V ∗ − V πk (cid:107) ≤ (cid:107)V ∗ − (cid:98)V ∗(cid:107) + (cid:107) (cid:98)V ∗ − (cid:98)V πk (cid:107) + (cid:107) (cid:98)V πk − V πk (cid:107).

(24)

1) Bounding the ﬁrst term in (24): Let V = {V ∈ R|S| : (cid:107)V (cid:107) ≤ 1/(1 − γ)}. For any s ∈ S,

V ∗(s) − (cid:98)V ∗(s) = Q∗(s, π∗(s)) − (cid:98)Q∗(s, ˆπ∗(s))

(a)
≤ Q∗(s, π∗(s)) − (cid:98)Q∗(s, π∗(s))

(b)
= γσP tv

s,π∗ (s)

(V ∗) − γσ

(cid:98)P tv

s,π∗ (s)

( (cid:98)V ∗)

= γ(σP tv

s,π∗ (s)

(V ∗) − σ

(cid:98)P tv

s,π∗(s)

(V ∗)) + γ(σ

(cid:98)P tv

s,π∗ (s)

(V ∗) − σ

( (cid:98)V ∗))

(cid:98)P tv

s,π∗ (s)

(c)
≤ γ(σP tv

s,π∗ (s)

(V ∗) − σ

(cid:98)P tv

s,π∗ (s)

(V ∗)) + γ(cid:107)V ∗ − (cid:98)V ∗(cid:107)

≤ γ max
V ∈V

max
s,a

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )| + γ(cid:107)V ∗ − (cid:98)V ∗(cid:107)

where (a) follows since ˆπ∗ is the robust optimal policy for (cid:99)M , (b) follows from the deﬁnitions of Q∗ and (cid:98)Q∗, (c)
follows from Lemma 1. Similarly analyzing for (cid:98)V ∗(s) − V ∗(s), we get

(cid:107)V ∗ − (cid:98)V ∗(cid:107) ≤

γ
(1 − γ)

max
V ∈V

max
s,a

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )|.

Now, using Proposition 1, with probability greater than 1 − δ, we get

(cid:107)V ∗ − (cid:98)V ∗(cid:107) ≤

γ
(1 − γ)

C tv

u (N, η, δ),

(25)

(26)

where C tv

u (N, η, δ) is given in equation (15) in the statement of Proposition 1.

2) Bounding the second term in (24): Let (cid:98)T be the robust Bellman operator corresponding to (cid:99)M . So, (cid:98)T is
a γ-contraction mapping and (cid:98)V ∗ is its unique ﬁxed point (Iyengar, 2005). The REVI iterates Vk, k ≥ 0, with
V0 = 0, can now be expressed as Vk+1 = (cid:98)T Vk. Using the properties of (cid:98)T , we get

(cid:107)Vk − (cid:98)V ∗(cid:107) = (cid:107) (cid:98)T Vk−1 − (cid:98)T (cid:98)V ∗(cid:107) ≤ γ(cid:107)Vk−1 − (cid:98)V ∗(cid:107) ≤ · · · ≤ γk(cid:107)V0 − (cid:98)V ∗(cid:107) ≤ γk/(1 − γ).

Now, using Lemma 10, we get

(cid:107) (cid:98)V πk − (cid:98)V ∗(cid:107) ≤

2γk+1
(1 − γ)2 .

(27)

(28)

3) Bounding the third term in (24): This is similar to bounding the ﬁrst term. For any s ∈ S,

V πk (s) − (cid:98)V πk (s) = Qπk (s, πk(s)) − (cid:98)Qπk (s, πk(s)) = γσPs,πk (s) (V πk ) − γσ

(cid:98)Ps,πk (s)

( (cid:98)V πk )

= γ(σPs,πk (s) (V πk ) − σPs,πk (s)( (cid:98)V πk )) + γ(σPs,πk (s) ( (cid:98)V πk ) − σ

(cid:98)Ps,πk (s)

( (cid:98)V πk ))

(d)

≤ γ(cid:107)V πk − (cid:98)V πk (cid:107) + γ(σPs,πk (s) ( (cid:98)V πk ) − σ
≤ γ(cid:107)V πk − (cid:98)V πk (cid:107) + γ max
V ∈V

max
s,a

(cid:98)P tv
s,a

|σ

(cid:98)Ps,πk (s)
(V ) − σP tv

s,a

( (cid:98)V πk ))

(V )|

Kishan Panaganti, Dileep Kalathil

where (d) follows from Lemma 1. Similarly analyzing for (cid:98)V πk (s) − V πk (s), we get,

(cid:107)V πk − (cid:98)V πk (cid:107) ≤

γ
(1 − γ)

max
V ∈V

max
s,a

|σ

(cid:98)P tv
s,a

(V ) − σP tv

s,a

(V )|.

Now, using Proposition 1, with probability greater than 1 − δ, we get

(cid:107)V πk − (cid:98)V πk (cid:107) ≤

γ
(1 − γ)

C tv

u (N, η, δ).

Using (26) - (30) in (24), we get, with probability at least 1 − 2δ,

(cid:107)V ∗ − V πk (cid:107) ≤

2γk+1
(1 − γ)2 +

2γ
(1 − γ)

C tv

u (N, η, δ).

Using the value of C tv

u (N, η, δ) as given in Proposition 1, we get

(cid:107)V ∗ − V πk (cid:107) ≤

2γk+1
(1 − γ)2 +

4γ
(1 − γ)2

(cid:114)

|S| log(6|S||A|/(δη(1 − γ)))
2N

+

8γη
(1 − γ)

with probability at least 1 − 2δ.

Now, choose η = ε(1 − γ)/(24γ). Since ε ∈ (0, 24γ/(1 − γ)), this particular η is in (0, 1). Now, choosing

k ≥ K0 =

N ≥ N tv =

1
log(1/γ)
72γ2
(1 − γ)4

log(

6γ
ε(1 − γ)2 ),

|S| log(144γ|S||A|/(δε(1 − γ)2))
ε2

,

we get (cid:107)V ∗ − V πk (cid:107) ≤ ε with probability at least 1 − 2δ.

B.3 Proof of Theorem 2

(29)

(30)

(31)

(32)

(33)

(34)

Proof of Lemma 4. Fix an (s, a) pair. From (Iyengar, 2005, Lemma 4.2), we have
(cid:19)

(cid:18)

σP c

s,a

(V ) = max

µ:0≤µ≤V

P o

s,a(V − µ) −

(cid:113)

crVarP o

s,a

(V − µ)

,

(35)

(V − µ) = P o

where VarP o
(V ). Using these
expressions, with the additional facts that | maxx f (x)−maxx g(x)| ≤ maxx |f (x)−g(x)| and maxx(f (x)+g(x)) ≤
maxx f (x) + maxx g(x), we get the desired result.

s,a(V − µ))2. We get a similar expression for σ

s,a(V − µ)2 − (P o

(cid:98)P c

s,a

s,a

We state the following concentration result that is useful for the proof of Proposition 2.
Lemma 11. For any V ∈ R|S|

+ with (cid:107)V (cid:107) ≤ Vmax, with probability at least 1 − δ,

(cid:113)

VarP o

s,a

max
(s,a)

|

(cid:113)

V −

Var

(cid:98)Ps,a

V | ≤ Vmax

(cid:114)

2 log(2|S||A|/δ)
N

Proof. Fix any (s, a) pair. Consider a discrete random variable X taking value V (j) with probability P o
all j ∈ {1, 2, · · · , |S|}. From the Self-bounding variance inequality (Lemma 6), we have
(cid:113)

(cid:113)

P(|

VarP o

s,a

V −

Var

(cid:98)Ps,a

V | ≥ ε) ≤ 2 exp(−N ε2/(2V 2

max)).

s,a(j) for

Choosing ε = Vmax
bound, we get

(cid:113) 2 log(2|S||A|/δ)
N

, we get P(|P o

s,aV − (cid:98)Ps,aV | ≥ Vmax

(cid:113) 2 log(2|S||A|/δ)
N

) ≤ δ

|S||A| . Now, using union

(cid:113)
|

P(max
(s,a)

VarP o

s,a

V −

(cid:113)

Var

(cid:98)Ps,a

(cid:114)

V | ≥ Vmax

2 log(2|S||A|/δ)
N
(cid:114)

)

(cid:88)

(cid:113)

P(|

≤

VarP o

s,a

V −

(cid:113)

s,a

This completes the proof.

Var

(cid:98)Ps,a

V | ≥ Vmax

2 log(2|S||A|/δ)
N

) ≤ δ.

Sample Complexity of Robust Reinforcement Learning with a Generative Model

We are now ready to prove Proposition 2.

Proof of Proposition 2. Fix an (s, a) pair. From Lemma 4, for any given V ∈ V, we have

|σ

(cid:98)P c

s,a

(V ) − σP c

s,a (V )| ≤ max

µ:0≤µ≤V

(cid:113)

crVar

|

(V − µ) −

(cid:98)Ps,a

(cid:113)

crVarP o

s,a (V − µ)| + max

µ:0≤µ≤V

| (cid:98)Ps,a(V − µ) − P o

s,a(V − µ)|.

By a simple variable substitution, we get

|σ

(cid:98)P c

s,a

(V ) − σP c

s,a

(V )| ≤ max
µ∈V+

max
s,a

(cid:113)
|

crVar

(cid:98)Ps,a

µ −

(cid:113)

crVarP o

s,a

µ| + max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ|,

which will give

max
V ∈V

max
s,a

|σ

(cid:98)P c

s,a

(V ) − σP c

s,a

(V )| ≤ max
µ∈V+

max
s,a

|

where V+ = {V ∈ R|S|

+ : (cid:107)V (cid:107) ≤ 1/(1 − γ)}.

(cid:113)

crVar

(cid:98)Ps,a

µ −

(cid:113)

crVarP o

s,a

µ| + max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ|,

(36)

We will ﬁrst bound the second term on the RHS of (36). From the proof of Lemma 3, for any η, δ ∈ (0, 1), we
get

max
µ∈V

max
s,a

| (cid:98)Ps,aµ − P o

s,aµ| ≤

(cid:114)

1
1 − γ

|S| log(12|S||A|/(δη(1 − γ)))
2N

+ 2η,

(37)

with probability greater than 1 − δ/2.

Now, we will focus on the ﬁrst term on the RHS of (36). Fix a µ ∈ V+. Consider a minimal η-cover NV+(η) of
the set V+. By deﬁnition, there exists µ(cid:48) ∈ NV+(η) such that (cid:107)µ − µ(cid:48)(cid:107) ≤ η. Now, following the same step as in
the proof of Lemma 3, we get

(cid:113)
|

Var

(cid:98)Ps,a

µ −

(cid:113)

VarP o

s,a

µ| ≤ |

(cid:113)

Var

(cid:98)Ps,a

(cid:113)

µ −

Var

(cid:98)Ps,a

(cid:113)

µ(cid:48)| + |

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

(cid:113)

µ(cid:48)| + |

VarP o

s,a

µ −

(cid:113)

VarP o

s,a

µ(cid:48)|

(cid:113)

(a)
≤ |

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

µ(cid:48)| +

(cid:113)

|Var

(cid:98)Ps,a

µ − Var

(cid:98)Ps,a

µ(cid:48)| +

(cid:113)

|VarP o

s,a

µ − VarP o

s,a

µ(cid:48)|

(cid:113)

(b)
≤ |

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

µ(cid:48)| +

(cid:113)

| (cid:98)Ps,a(µ2 − µ(cid:48)2)| +

(cid:113)

(cid:113)

(c)
≤ |

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

µ(cid:48)| +

(cid:114) 32η
1 − γ

(cid:113)

|( (cid:98)Ps,aµ)2 − ( (cid:98)Ps,aµ(cid:48))2)|+
(cid:113)

|P o

s,a(µ2 − µ(cid:48)2)| +

|(P o

s,aµ)2 − (P o

s,aµ(cid:48))2)|

≤

sup
µ(cid:48)∈NV+ (η)

max
s,a

(cid:113)
|

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

µ(cid:48)| +

(cid:114) 32η
1 − γ

where (a) follows from the fact |
√

x + y| ≤
y for all x, y ∈ R+, and (c) follows by using the fact x2 − y2 = (x + y)(x − y), (cid:107)µ(cid:107) ≤ 1/(1 − γ), and

y| ≤ (cid:112)|x − y| for all x, y ∈ R+, (b) follows from the fact |

x −

x +

√

(cid:107)µ(cid:48)(cid:107) ≤ 1/(1 − γ) with H¨older’s inequality. Now, taking max on both sides with respect to µ and (s, a) we get

√

√

√

sup
µ∈V+

max
s,a

(cid:113)
|

Var

(cid:98)Ps,a

µ −

(cid:113)

VarP o

s,a

µ| ≤

(d)
≤

(e)
≤

sup
µ(cid:48)∈NV+ (η)
(cid:114)

1
1 − γ

(cid:113)
|

max
s,a

Var

(cid:98)Ps,a

µ(cid:48) −

(cid:113)

VarP o

s,a

µ(cid:48)| +

(cid:114) 32η
1 − γ

2 log(4|S||A||NV+(η)|/δ)
N

(cid:114) 32η
1 − γ

+

(cid:114)

1
1 − γ

2|S| log(12|S||A|/(δη(1 − γ)))
N

+

(cid:114) 32η
1 − γ

,

(38)

with probability at least 1 − δ/2. Here, (d) follows from Lemma 11 and the union bound and (e) from Lemma 7.

Kishan Panaganti, Dileep Kalathil

Applying (37) and (38) in (36), we get

max
V ∈V

max
s,a

|σ

(cid:98)P c

s,a

(V ) − σP c

s,a

(V )| ≤

(cid:114)

1
1 − γ

2cr|S| log(12|S||A|/(δη(1 − γ)))
N

+

(cid:114) 32ηcr
1 − γ

(cid:114)

+

1
1 − γ

|S| log(12|S||A|/(δη(1 − γ)))
2N

+ 2η,

with probability greater than 1 − δ. This completes the proof.

Proof of Theorem 2. The basic steps of the proof is similar to that of Theorem 1. So, we present only the
important steps.

Following the same steps as given before (26) and using Proposition 2, we get, with probability greater than
1 − δ,

Similarly, following the steps as given before (28), we get

(cid:107)V ∗ − (cid:98)V ∗(cid:107) ≤

γ
(1 − γ)

C c

u(N, η, δ)

(cid:107) (cid:98)V πk − (cid:98)V ∗(cid:107) ≤

2γk+1
(1 − γ)2 .

(39)

(40)

In the same vein, following the steps as given before (30) and using Proposition 2, we get, with probability
greater than 1 − δ,

(cid:107)V πk − (cid:98)V πk (cid:107) ≤

γ
(1 − γ)

C c

u(N, η, δ).

Using (39) - (41), similar to (31), we get, with probability greater than 1 − 2δ,

(cid:107)V ∗ − V πk (cid:107) ≤

2γk+1
(1 − γ)2 +

2γ
(1 − γ)

C c

u(N, η, δ).

(41)

(42)

Using the value of C c

u(N, η, δ) as given in Proposition 2, we get, with probability greater than 1 − 2δ,

(cid:107)V ∗ − V πk (cid:107) ≤

2γk+1
(1 − γ)2 +

√

8γ
2ηcr
(1 − γ)3/2

+

4γη
1 − γ

+

2γ
(1 − γ)2

(cid:114)

(2cr + 1)|S| log(12|S||A|/(δη(1 − γ)))
N

.

We can now choose k, ε, η to make each of the term on the RHS of the above inequality small. In particular, we
2cr/(1 − γ)3/2}) and η = min{ε(1 − γ)/(16γ), ε2(1 − γ)3/(2048crγ2)}. Note
select ε ∈ (0, min{16γ/(1 − γ), 32γ
that this choice also ensure η ∈ (0, 1). Now, by choosing

√

k ≥ K0 =

N ≥ N c =

1
log(1/γ)
64γ2
(1 − γ)4 ·

· log(

8γ
ε(1 − γ)2 ),

(2cr + 1)|S| log(12|S||A|/(δη(1 − γ)))
ε2

,

(43)

(44)

we will get (cid:107)V ∗ − V πk (cid:107) ≤ ε with probability at least 1 − 2δ.

B.4 Proof of Theorem 3

We state a result from (Zhou et al., 2021) that will be useful in the proof of Theorem 3.

Lemma 12 ((Zhou et al., 2021, Lemma 4) ). Fix any δ ∈ (0, 1). Let X ∼ P be a bounded random variable with
X ∈ [0, M ] and let PN denote the empirical distribution of P with N samples. For t > 0, for any

λ∗ ∈ arg max

λ≥0

{−λ log(EP [exp(−X/λ)]) − λt} ,

Sample Complexity of Robust Reinforcement Learning with a Generative Model

(1) λ∗ = 0. Furthermore, let the support of X be ﬁnite. Then there exists a problem dependent constant

N (cid:48)(δ, t, P ) := max{log(2/δ)/ log(1/(1 − min

x∈supp(X)

P (X = x))) , 2M 2 log(4/δ)/(P (X = ess inf X) − exp(−t))2},

such that for N ≥ N (cid:48)(δ, t, P ) we have, with probability at least 1 − δ,

0 ∈ arg max

λ≥0

{−λ log(EPN [exp(−X/λ)]) − λt} .

(2) λ∗ > 0. Then there exists a problem dependent constant

N (cid:48)(cid:48)(δ, t, P ) :=

max
λ∈{λ,λ∗,M/t}

8M 2 exp(2M/λ)
τ 2

log(6/δ),

where λ = λ∗/2 > 0 (independent of N ) and

τ = min{λ log(EP [exp(−X/λ)]) + λt, (M/t) log(EP [exp(−tX/M )]) + M }

− (λ∗ log(EP [exp(−X/λ∗)]) + λ∗t) > 0,

such that for N ≥ N (cid:48)(cid:48)(δ, t, P ), with probability at least 1 − δ, there exists a

(cid:98)λ∗ ∈ arg max

λ≥0

{−λ log(EPN [exp(−X/λ)]) − λt} ,

such that λ∗, (cid:98)λ∗ ∈ [λ, M/t].

We now prove the following result.

Lemma 13. For any (s, a) ∈ S × A and for any V ∈ R|S| with (cid:107)V (cid:107) ≤ 1/(1 − γ),

|σ

(cid:98)P kl
s,a

(V ) − σP kl

s,a

(V )| ≤

exp(1/λkl(1 − γ))
cr(1 − γ)

max

λ∈[λkl,

1

cr (1−γ) ]

|(P o

s,a − (cid:98)Ps,a) exp(−V /λ)|

(45)

holds with probability at least 1 − δ/(2|S||A|) for N ≥ max{N (cid:48)(δ/(4|S||A|), cr, P o
where both N (cid:48), N (cid:48)(cid:48) are deﬁned as in Lemma 12.

s,a), N (cid:48)(cid:48)(δ/(4|S||A|), cr, P o

s,a)},

Proof. Fix any (s, a) pair. From (Iyengar, 2005, Lemma 4.1), we have

σP kl

s,a

(V ) = max
λ≥0

(−crλ − λ log(P o

s,a exp(−V /λ))),

σ

(cid:98)P kl
s,a

(V ) = max
λ≥0

(−crλ − λ log( (cid:98)Ps,a exp(−V /λ))),

(46)

where exp(−V /λ) is an element-wise exponential function.
λ log(P o
be the optimal solution of the second problem above.

It is straight forward to show that (−crλ −
s,a exp(−V /λ))) is a concave function in λ. So, there exists an optimal solution λ∗. Similarly, let (cid:98)λ∗

We can now give an upperbound for λ∗, (cid:98)λ∗ as follows: Since σP kl

s,a

(V ) ≥ 0, we have

0 ≤ −crλ∗ − λ∗ log(P o

s,a exp(−V /λ∗))

(a)
≤ −crλ∗ − λ∗ log(exp(−1/(λ∗(1 − γ)))) ≤ −crλ∗ + 1/(1 − γ),

from which we can conclude that λ∗ ≤ 1/(cr(1 − γ)). Same argument applies for the case of (cid:98)λ∗.

From (Nilim and El Ghaoui, 2005, Appendix C) it follows that whenever the maximizer λ∗ is 0 ( (cid:98)λ∗ is 0), we have

Kishan Panaganti, Dileep Kalathil

σP kl

s,a

(V ) = Vmin ( σ

(cid:98)P kl
s,a

(V ) = Vmin) where Vmin = minj∈S V (j). We include this part in detail for completeness.

−crλ − λ log(P o

lim
λ↓0

s,a exp(−V /λ)) = lim
λ↓0

−crλ − λ log(exp(−Vmin/λ)

(cid:88)

s,a(s(cid:48)) exp((Vmin − V (s(cid:48)))/λ))
P o

Vmin − crλ − λ log(

(cid:88)

s(cid:48)
s,a(s(cid:48)) exp((Vmin − V (s(cid:48)))/λ))
P o

Vmin − crλ − λ log(

s(cid:48)

(cid:88)

s,a(s(cid:48)) +
P o

(cid:88)

s,a(s(cid:48)) exp((Vmin − V (s(cid:48)))/λ))
P o

= lim
λ↓0

= lim
λ↓0

(a)
= lim
λ↓0

(b)
= lim
λ↓0

(c)
= lim
λ↓0

Vmin − crλ − λ log(

s(cid:48):V (s(cid:48))=Vmin
(cid:88)

s(cid:48):V (s(cid:48))>Vmin

s,a(s(cid:48)) + O(exp(−t/λ)))
P o

Vmin − crλ − λ log(

s(cid:48):V (s(cid:48))=Vmin
(cid:88)

s,a(s(cid:48))) − λ log(1 + O(exp(−t/λ)))
P o

Vmin − λ(cr + log(

s(cid:48):V (s(cid:48))=Vmin
(cid:88)

s,a(s(cid:48)))) − O(λ exp(−t/λ)) = Vmin,
P o

s(cid:48):V (s(cid:48))=Vmin

where (a) follows by taking t = mins(cid:48):V (s(cid:48))>Vmin V (s(cid:48)) − Vmin > 0, and (b) and (c) follows from the Taylor series
expansion. Thus when λ∗ is 0, we have σP kl

(V ) = Vmin. A similar argument applies for σ

(V ).

s,a

(cid:98)P kl
s,a

Now consider the case when λ∗ = 0. From Lemma 12, it follows that, with probability at least 1 − δ/(4|S||A|),
s,a), where N (cid:48) is deﬁned in Lemma 12. Thus, whenever λ∗ = 0, we have
(cid:98)λ∗ = 0 for N ≥ N (cid:48)(δ/(4|S||A|), cr, P o
|σ
(V ) − σPs,a (V )| = |Vmin − Vmin| = 0, with probability at least 1 − δ/(4|S||A|). Thus having resolving this
trivial case, we now focus on the case when λ∗ > 0.

(cid:98)Ps,a

Consider the case when λ∗ > 0. Let λkl
s,a, V, and cr but independent of
N ). Again from Lemma 12, if λ∗ ∈ [λkl, 1/(cr(1 − γ))], then with probability at least 1 − δ/(4|S||A|) we have
(cid:98)λ∗ ∈ [λkl, 1/(cr(1 − γ))] for N ≥ N (cid:48)(cid:48)(δ/(4|S||A|), cr, P o

s,a), where N (cid:48)(cid:48) is deﬁned in Lemma 12.

:= λ∗/2 > 0 (dependent on P o

From these arguments, it is clear that we can restrict the optimization problem (46) to the set λ ∈ [λkl, 1/(cr(1 −
γ)). Using this, with the additional fact that | maxx f (x) − maxx g(x)| ≤ maxx |f (x) − g(x)|, we get

|σ

(cid:98)P kl
s,a

(V ) − σP kl

s,a

(V )| ≤

max

λ∈[λkl,

1

cr (1−γ) ]

|λ log(

(cid:98)Ps,a exp(−V /λ)
P o
s,a exp(−V /λ)

)|.

(47)

Now,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

log(

(cid:98)Ps,a exp(−V /λ)
P o
s,a exp(−V /λ)

(cid:12)
(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
log(1 +
(cid:12)
(cid:12)

s,a) exp(−V /λ)

( (cid:98)Ps,a − P o
P o

s,a exp(−V /λ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

≤

(d)
≤

|(P o

s,a − (cid:98)Ps,a) exp(−V /λ)|
|P o
s,a exp(−V /λ)|

|(P o

s,a − (cid:98)Ps,a) exp(−V /λ)|
λkl(1−γ) )

exp( −1

,

(48)

where (d) follows since λ ≥ λkl and (cid:107)V (cid:107) ≤ 1/(1−γ). Using (48) in (47) along with the fact that λ ≤ 1/(cr(1−γ)),
we get the desired result.

Proof of Theorem 3. The basic steps of the proof is similar to that of Theorem 1. So, we present only the
important steps.

Following the same steps as given before (25) and (29) , we get

(cid:107)V ∗ − (cid:98)V ∗(cid:107) + (cid:107)V πk − (cid:98)V πk (cid:107) ≤

2γ
(1 − γ)

max
V ∈V

max
s,a

|σ

(cid:98)P kl
s,a

(V ) − σP kl

s,a

(V )|.

Similarly, following the steps as given before (28), we get

(cid:107) (cid:98)V πk − (cid:98)V ∗(cid:107) ≤

2γk+1
(1 − γ)2 .

(49)

(50)

Sample Complexity of Robust Reinforcement Learning with a Generative Model

Using Lemma 13 in (49), we get

(cid:107)V ∗ − (cid:98)V ∗(cid:107) + (cid:107)V πk − (cid:98)V πk (cid:107) ≤

2γ
(1 − γ)

exp(1/(λkl(1 − γ)))
cr(1 − γ)

max
s,a

max
V ∈V

max

λ∈[λkl,

1

cr (1−γ) ]

|(P o

s,a − (cid:98)Ps,a) exp(−V /λ)|.

(51)

We now bound the max term in (51). We reparameterize 1/λ as θ and consider the set Θ = [cr(1 − γ), 1
]. Also,
λkl
consider the minimal η-cover NΘ(η) of Θ and ﬁx a V ∈ V. Then, for any given θ ∈ Θ, there exits a θ(cid:48) ∈ NΘ(η)
such that |θ − θ(cid:48)| ≤ η. Now, for this particular θ, θ(cid:48),

|(P o

s,a − (cid:98)Ps,a) exp(−V θ)| = |( (cid:98)Ps,a − P o

s,a)(exp(−V θ(cid:48)) ◦ exp(−V (θ − θ(cid:48)))|

(c)
≤ |( (cid:98)Ps,a − P o

s,a) exp(−V θ(cid:48))| exp(η/(1 − γ)) ≤ max
s,a

max
θ(cid:48)∈NΘ(η)

|( (cid:98)Ps,a − P o

s,a) exp(−V θ(cid:48))| exp(η/(1 − γ)),

where (c) follows because V is non-negative and (cid:107)V (cid:107) ≤ 1/(1 − γ). Now consider a minimal η-cover NV (η) of the
set V. By deﬁnition, there exists V (cid:48) ∈ NV (η) such that (cid:107)V − V (cid:48)(cid:107) ≤ η. So, we get

|(P o

s,a − (cid:98)Ps,a) exp(−V θ)| ≤ |( (cid:98)Ps,a − P o
= |( (cid:98)Ps,a − P o
(d)
≤ |( (cid:98)Ps,a − P o
max
≤ max
s,a
V (cid:48)∈V

s,a) exp(−V θ(cid:48))| exp(η/(1 − γ))
s,a)(exp(−V (cid:48)θ(cid:48)) ◦ exp(θ(cid:48)(V (cid:48) − V )))| exp(η/(1 − γ))

s,a)(exp(−V (cid:48)θ(cid:48)))| exp(η/(1 − γ)) exp(η/λkl)
max
θ(cid:48)∈NΘ(η)

|( (cid:98)Ps,a − P o

s,a)(exp(−V (cid:48)θ(cid:48)))| exp(η/(1 − γ)) exp(η/λkl)

where (d) follows because θ(cid:48) ∈ NΘ(η) ⊆ Θ. Now, taking maximum on both sides with respect to (s, a), θ, and
V , we get

max
s,a

max
θ∈Θ

max
V ∈V

|( (cid:98)Ps,a − P o

s,a) exp(−V θ)| ≤ exp(η/(1 − γ)) exp(η/λkl) max
s,a
(cid:114)

(e)
≤ exp(η/(1 − γ)) exp(η/λkl)

max
V (cid:48)∈V

max
θ(cid:48)∈NΘ(η)

|( (cid:98)Ps,a − P o

s,a) exp(−V (cid:48)θ(cid:48))|

log(2|S||A||NΘ(η)||NV (η)|/δ)
2N

(f )
≤ exp(η/(1 − γ)) exp(η/λkl)

(cid:114)

|S| log(18|S||A|/(δη2(1 − γ)λkl))
2N

(52)

with probability greater than 1 − δ. Here, (e) follows from Lemma 9 with a union bound accounting for |NΘ(η)|,
|NV (η)| and the fact that (cid:107) exp(−V (cid:48)θ(cid:48))(cid:107) ≤ 1, and (f ) follows from Lemmas 7 and 8.

Using (49) - (52), we get, with probability greater than 1 − δ,

(cid:107)V ∗ − V πk (cid:107) ≤

2γk+1
(1 − γ)2 +
2γ
(1 − γ)

exp(1/(λkl(1 − γ)))
cr(1 − γ)

exp(η/(1 − γ)) exp(η/λkl)

(cid:114)

|S| log(18|S||A|/(δη2(1 − γ)λkl))
2N

.

We can now choose k, ε, η to make each of the term on the RHS of the above inequality small. In particular,
choosing η = 1, ε ∈ (0, 1/(1 − γ)), and k, N satisfying the conditions

k ≥ K0 =

1
log(1/γ)
(cid:26)

· log(

4
ε(1 − γ)2 )

and

N ≥ N kl = max

max
s,a

N (cid:48)(δ/(4|S||A|), cr, P o

s,a), max
s,a

N (cid:48)(cid:48)(δ/(4|S||A|), cr, P o

s,a),

8γ2|S|
r(1 − γ)4ε2 exp(
c2
we get (cid:107)V ∗ − V πk (cid:107) ≤ ε with probability greater than 1 − δ.

4 + 2λkl
λkl(1 − γ)

) log(

18|S||A|
δλkl(1 − γ)

(cid:27)
)

,

Kishan Panaganti, Dileep Kalathil

B.5 Proof of Theorem 4

Proof. We consider the deterministic MDP (S, A, r, P o, γ) shown in Fig.3 to be the nominal model. We ﬁx
γ ∈ (0.01, 1] and s1 = 0. The state space is S = {0, 1} and action space is A = {al, ar}, where al denotes
‘move left’ and ar denotes ‘move right’ action. Reward for state 1 and action ar pair is r(1, ar) = 1, for state
0 and action ar pair is r(0, ar) = −100γ/99, and the reward is 0 for all other (s, a). Transition function P o is
deterministic, as indicated by the arrows.

Figure 3: Transitions and rewards corresponding to
the nominal model P o. The states {0, 1} are given
inside the circles, and the actions {al, ar} and asso-
ciated rewards are given on the corresponding tran-
sitions.

Figure 4: Transitions and rewards corresponding to
the model P (cid:48).

Similarly, we consider another deterministic model P (cid:48), as shown in Fig.4. We consider the set P = {P o, P (cid:48)}.

It is straight forward to show that taking action ar in any state is the optimal non-robust policy πo corresponding
to the nominal model P o. This is obvious if for state s = 1. For s = 0, notice that taking action al will give a
value zero and taking action ar will give a value
99 . Since γ > 0.01, taking action ar will give a positive
value and hence is optimal. So, we get

1−γ − 100γ

γ

Vπo,P o(0) =

γ
1 − γ

−

100γ
99

.

We can now compute Vπo,P (cid:48)(0) using the recursive equation

Vπo,P (cid:48)(0) = −

100γ
99

+ γ + γ2Vπo,P (cid:48)(0).

Solving this, we get Vπo,P (cid:48)(0) = −γ/(99(1 − γ2)).
Now the robust value of πo is given by

V πo

(0) = min{Vπo,P o(0), Vπo,P (cid:48)(0)} = −γ/(99(1 − γ2)).

We will now compute the optimal non-robust value from state 0 of model P (cid:48).

max
π

Vπ,P (cid:48)(0) = max{V(π(0)=ar,π(1)=ar),P (cid:48)(0), V(π(0)=al,π(1)=al),P (cid:48)(0),

= max{ −

V(π(0)=ar,π(1)=al),P (cid:48)(0), V(π(0)=al,π(1)=ar),P (cid:48)(0)}
100γ
99(1 + γ2)

γ
99(1 − γ2)

, 0} = 0.

, 0, −

Now, we ﬁnd the optimal robust value V ∗(0). From the perfect duality result of robust MDP (Nilim and
El Ghaoui, 2005, Theorem 1), we have

V ∗(0) = min{max

π

Vπ,P o(0), max

π

Vπ,P (cid:48)(0)} = min{Vπo,P o(0), max

π

Vπ,P (cid:48)(0)} = 0.

We ﬁnally have

V ∗(0) − V πo

(0) =

γ
99(1 − γ2)

≥

γ
198(1 − γ)

,

where the inequality follows since 1 + γ ≤ 2. Thus, setting c = γ/198 and γo = 0.01, completes the proof of this
theorem.

10$%,−100*/99$(,0$%,1$(,010$%,−100*/99$(,0$%,1$(,0