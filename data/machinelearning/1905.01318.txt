9
1
0
2

y
a
M
3

]
h
p
-
t
n
a
u
q
[

1
v
8
1
3
1
0
.
5
0
9
1
:
v
i
X
r
a

Optimization and learning of quantum programs

Leonardo Banchi,1, 2 Jason Pereira,3 Seth Lloyd,4, 5 and Stefano Pirandola3, 5
1Department of Physics and Astronomy, University of Florence,
via G. Sansone 1, I-50019 Sesto Fiorentino (FI), Italy
2INFN Sezione di Firenze, via G.Sansone 1, I-50019 Sesto Fiorentino (FI), Italy
3Department of Computer Science, University of York, York YO10 5GH, UK
4Department of Mechanical Engineering, Massachusetts Institute of Technology (MIT), Cambridge MA 02139, USA
5Research Laboratory of Electronics, Massachusetts Institute of Technology (MIT), Cambridge MA 02139, USA

A programmable quantum processor is a fundamental model of quantum computation. In this
model, any quantum channel can be approximated by applying a ﬁxed universal quantum operation
onto an input state and a quantum “program” state, whose role is to condition the operation
performed by the processor.
It is known that perfect channel simulation is only possible in the
limit of inﬁnitely large program states, so that ﬁnding the best program state represents an open
problem in the presence of realistic ﬁnite-dimensional resources. Here we prove that the search for
the optimal quantum program is a convex optimization problem. This can be solved either exactly,
by minimizing a diamond distance cost function via semi-deﬁnite programming, or approximately,
by minimizing other cost functions via gradient-based machine learning methods. We apply this
general result to a number of diﬀerent designs for the programmable quantum processor, from the
shallow protocol of quantum teleportation, to deeper schemes relying on port-based teleportation
and parametric quantum circuits. We benchmark the various designs by investigating their optimal
performance in simulating arbitrary unitaries, Pauli and amplitude damping channels.

I.

INTRODUCTION

Today the ﬁeld of quantum computing [1] is becom-
ing more and more mature, also thanks to the combined
eﬀorts of academic and industrial researchers. From a
theoretical point of view, this endeavour is supported by
increasing interconnections with other rapidly-advancing
ﬁelds, such as machine learning [2]. For instance, we have
recently witnessed the development of new hybrid ar-
eas of investigation, such as quantum-enhanced machine
learning [3–7] (e.g., quantum neural networks, quantum
annealing etc.), protocols of quantum-inspired machine
learning (e.g., for recommendation systems [8] or compo-
nent analysis and supervised clustering [9]) and classical
learning methods applied to quantum computers, as ex-
plored here in this manuscript.

In quantum computing, a fundamental model is the
programmable quantum gate array or programmable
quantum processor [10]. This is a quantum processor
where a ﬁxed quantum operation is applied to an input
state and a program state. The role of the program state
is to condition the quantum operation in such a way to
apply some target quantum gate or channel to the in-
put state. This is a very ﬂexible scheme but not actually
universal: an arbitrary quantum channel cannot be pro-
grammed exactly, unless the program state is allowed to
have an inﬁnite number of qubits. For instance, a possi-
ble design relies on port-based teleportation (PBT) [11–
13], where an input state is subject to certain local op-
erations and classical communication (LOCCs) that are
programmed by a tensor product of N bipartite states.
For inﬁnite N , any quantum channel can be simulated
by copies of its Choi matrix [14] but, for any ﬁnite N ,
this simulation is not perfect.

Despite this fundamental model of quantum computa-

tion is known since 1997, a quantitative characterization
of its actual performance in terms of gate implementa-
tion or channel simulation is still missing. Given a target
quantum gate or channel, it is not yet known what de-
gree of approximation can be reached and what kind of
optimization procedure must be employed to choose the
program state. After more than 20 years, the solution to
these open problems comes from a suitable application of
techniques of semideﬁnite programming (SDP) and ma-
chine learning (ML).

In our work, we quantify the error between an arbi-
trary target channel and its programmable simulation in
terms of the diamond distance and other suitable cost
functions, including the trace distance and the quantum
ﬁdelity. For all the considered cost functions, we are able
to show that the minimization of the simulation error is
a convex optimization problem in the space of the pro-
gram states. This already solves an outstanding prob-
lem which aﬀects various models of quantum computers
(e.g., variational quantum circuits) where the optimiza-
tion over classical parameters is non-convex and therefore
not guaranteed to converge to a global optimum. By con-
trast, because our problem is proven to be convex, we can
use SDP to minimize the diamond distance and always
ﬁnd the optimal program state for the simulation of a
target channel, therefore optimizing the programmable
quantum processor. Similarly, we may ﬁnd suboptimal
solutions by minimizing the trace distance or the quan-
tum ﬁdelity by means of gradient-based ML techniques,
such as the projected subgradient method [15] and the
conjugate gradient method [16, 17]. We note indeed that
the minimization of the (cid:96)1-norm, mathematically related
to the quantum trace distance, is widely employed in
many ML tasks [18, 19], so many of those techniques can
be adapted for learning program states.

 
 
 
 
 
 
With these general results in our hands, we ﬁrst discuss
the optimal learning of arbitrary unitaries with a generic
programmable quantum processor. Then, we consider
speciﬁc designs of the processor, from a shallow scheme
based on the teleportation protocol, to higher-depth de-
signs based on PBT [11–13] and parametric quantum cir-
cuits (PQCs) [20], introducing a suitable convex reformu-
lation of the latter. In the various cases, we benchmark
the processors for the simulation of basic unitary gates
(qubit rotations) and various basic channels, including
the amplitude damping channel which is known to be
the most diﬃcult to simulate [21, 22]. For the deeper
designs, we ﬁnd that the optimal program states do not
correspond to the Choi matrices of the target channels,
which is rather counter-intuitive and unexpected.

The paper is structured as follows. In Sec. II we discuss
the general notion of programmable channel simulation,
the various cost functions and a suitable Choi-reduction
of the problem. In Sec. III we then show that the opti-
mization of a generic programmable quantum processor
is convex in the space of the program states. In Sec. IV
we consider the optimization of the diamond distance via
SDP and the minimization of the other cost functions via
gradient descent. In particular, in Sec. V we provide the
details of the gradient-based ML algorithms to be used,
together with a discussion of smoothing techniques. In
Sec. VI, we discuss the optimal learning of arbitrary uni-
taries. We then move to discuss the various speciﬁc de-
signs based on teleportation (Sec. VII), PBT (Sec. VIII)
and PQC (Sec. IX). Sec. X is for conclusions.

II. PROGRAMMABLE SIMULATION

A. General problem

E

Consider an arbitrary but known quantum channel
from dimension d to dimension d(cid:48) [1, 23]. We want
E
to simulate
using a programmable quantum proces-
sor [10] that we simply call “quantum processor” (see
Fig. 1). This is represented by a completely positive
trace-preserving (CPTP) universal map Q which is as-
sumed to be ﬁxed and applied to the arbitrary input ρ of
the channel together with a program state π (which may
be varied). In this way, the quantum processor generates
Eπ as
an approximate channel

Eπ(ρ) = Tr2 [Q(ρ

⊗

π)] .

(1)

Our goal is to ﬁnd the program state π for which the
, i.e., so that we minimize

simulation
the following cost function

Eπ is the closest to

E

2

FIG. 1. Arbitrary quantum channel E and its simulation Eπ
via a quantum processor Q applied to a program state π.

= 0 for arbitrary

From theory [10, 26] we know that we cannot achieve
C
unless π and Q have inﬁnite di-
(cid:5)
mensions. As a result, for any ﬁnite-dimensional realistic
design of the quantum processor, ﬁnding the optimal pro-
gram state ˜π is an open problem.

E

Recall that the diamond distance is deﬁned by the fol-

lowing maximization

(cid:107)E − Eπ(cid:107)(cid:5)

= max

ϕ (cid:107)I ⊗ E

(ϕ)

− I ⊗ Eπ(ϕ)

(cid:107)1 ,

(4)

O
(cid:107)

(cid:107)1 := Tr√O†O is the trace norm [23]. Because
where
the trace norm is convex over mixed states, one may re-
duce the maximization in Eq. (4) to bipartite pure states
ϕ =
. In general, we therefore need to consider a
|
min-max optimization, i.e., ﬁnd ˜π and (pure) ˜ϕ such that

ϕ
|

(cid:105) (cid:104)

ϕ

(cid:107)I ⊗ E
= min

π

( ˜ϕ)
max

− I ⊗ E˜π( ˜ϕ)
(ϕ)

ϕ (cid:107)I ⊗ E

(cid:107)1
− I ⊗ Eπ(ϕ)

(cid:107)1 .

(5)

Also recall that the diamond distance can be computed
using SDP [27]. In particular, due to strong duality, it
may be computed via a minimization rather than a max-
imization, so that the min-max optimization problem in
Eq. (5) can be transformed into a more convenient min-
imization problem (more details in Sec. IV A). An alter-
native solution is to reduce the general problem into a
weaker one which is expressed in terms of the Choi ma-
trix of the channel (see following section). In this way, we
also avoid the maximization in ϕ but with the downside
of using a larger cost function, the trace distance.

B. Processor map and Choi reduction

It is known that a quantum channel

E
with its Choi matrix χ
(Φ), where Φ :=
E
is d-dimensional maximally-entangled state, i.e.,

I ⊗ E

is one-to-one
Φ

Φ
|

:=

(cid:105)(cid:104)

|

:= d−

Φ
|

(cid:105)

1/2 (cid:88)

i

.

i, i
(cid:105)

|

(6)

(π) :=

C

(cid:5)

(cid:107)E − Eπ(cid:107)(cid:5) ≤

2,

(2)

Using the channel deﬁnition of Eq. (1), we may write

in terms of the diamond distance [24, 25]. In other words,

Find ˜π such that C
(cid:5)

(˜π) = min

π

(π).

C
(cid:5)

(3)

χ

π =
E

I ⊗ Eπ(Φ)
1(cid:80)
i
ij |
(cid:105)(cid:104)

= d−

j

Tr2 [Q(
|

i
(cid:105)(cid:104)

j

| ⊗

| ⊗

π)] .

(7)

Qππ (cid:1)From this expression, it is clear that the Choi matrix
χ
π is linear in the program state π. More precisely, the
E
π at the output of the processor Q can be
Choi matrix χ
E
directly written as a CPTP linear map Λ acting on the
space of the program states π, i.e.,

χπ := χ

π = Λ(π).
E

(8)

This map is also depicted in Fig. 2.

We may connect the minimization of the diamond dis-

tance C

(cid:5)

(π) to the minimization of the trace distance

C1(π) :=

χ
(cid:107)

E −

χπ(cid:107)1 ,

(9)

between the Choi matrices χ
E
write the sandwich relation [23]

and χπ. In fact, we may

C1(π)

(π)

C
(cid:5)

≤

≤

d C1(π).

(10)

While the lower bound is immediate from the deﬁnition
of Eq. (4), the upper bound can be proven using the
following equivalent form of the diamond distance

(cid:107)E −Eπ(cid:107)(cid:5)

= sup
ρ0,ρ1

d

(√ρ0 ⊗
(cid:107)

11)(χ

E −

χπ)(√ρ1 ⊗

11)

(cid:107)1, (11)

where the optimization is done over the density matrices
ρ0 and ρ1 [27, Theorem 3.1]. In fact, consider the Frobe-
nius norm

Tr[A†A] and the spectral norm

(cid:112)

A
(cid:107)

(cid:107)2 :=
:= max

A
(cid:107)

(cid:107)∞

(cid:107) ≤
{(cid:107)
which satisfy the following properties [23]

∈

: u

Au
(cid:107)

Cd,

u
(cid:107)

ABC
11

⊗

(cid:107)1 ≤ (cid:107)
=
(cid:107)
(cid:107)∞

A
A

B

(cid:107)1(cid:107)
A

(cid:107)∞(cid:107)
(cid:107)∞ ≤ (cid:107)

C
(cid:107)2.

(cid:107)∞

(cid:107)
A
(cid:107)

Then, from Eqs. (11), (13) and (14), one gets

(cid:112)

d

χ

E −

E −

(15)

χπ(cid:107)1

= d
(cid:107)

sup
ρ0,ρ1
χ

(cid:107)E − Eπ(cid:107)(cid:5) ≤

Trρ0Trρ1(cid:107)
χπ(cid:107)1.
Thanks to Eq. (10), we may avoid the maximization
step in the deﬁnition of the diamond distance and sim-
plify the original problem to approximating the Choi ma-
trix χ
of the channel by varying the program state π.
E
This is a process of learning Choi matrices as depicted
in Fig. 2. Because the simpler cost function C1(π) is
an upper bound, its minimization generally provides a
sub-optimal solution for the program state.

C. Other cost functions

Besides C
(cid:5)

and C1 we can introduce other cost func-
tions. First of all, using the Fuchs-van de Graaf inequal-
ity [28], we may write

C1(π)

≤

(cid:112)
2

CF (π), CF (π) = 1

F (π)2,

−

(16)

,

1
}

,

(12)

(13)
(14)

3

FIG. 2. Map of the processor and learning of Choi matrices.
Consider an arbitrary (but known) quantum channel E and
its associated Choi matrix χE , generated by propagating part
of a maximally-entangled state Φ. Then, consider a quantum
processor Q with program state π which generates the simu-
lated channel Eπ and, therefore, the corresponding Choi ma-
trix χπ := χEπ upon propagating part of Φ as input state. The
map of the processor is the CPTP map Λ from the program
state π to the output Choi matrix χπ. In a simpliﬁed version
of our problem, we may optimize the program π in such a way
to minimize the trace distance C1(π) := (cid:107)χE − χπ(cid:107)1.

where F (π) is Bures’ ﬁdelity between the two Choi ma-
trices χ
E

and χπ, i.e.,

F (π) :=

√χ
E

√χπ(cid:107)1 = Tr

(cid:107)

(cid:113)

√χ

χπ√χ
E

.

E

(17)

Another possible upper bound can be written using the
quantum Pinsker’s inequality [29, 30]. In fact, we may
write C1(π)

CR(π), where

(2 ln √2)

(cid:112)

≤

CR(π) := min

S(χ

{

)

χ
χπ), S(χπ||
E ||
E
log2 σ)] is the quantum rel-

(18)

}

,

and S(ρ
ative entropy between ρ and σ.

σ) := Tr[ρ(log2 ρ
||

−

Finally we may consider other cost functions in terms
of any Shatten p-norm Cp(π) :=
χπ(cid:107)p, even
though this option provides lower bounds instead of up-
per bounds for the trace distance. Recall that, given an
1, we may deﬁne its
operator O and a real number p
Schatten p-norm as [23]

χ
(cid:107)

E −

≥

O
(cid:107)

(cid:107)p = (Tr

O
|

p)1/p,
|

(19)

= √O†O. For any 1

where
O
|
|
has the monotony
O
(cid:107)
of operators A and B, and each pair of parameters p, q
[1,

O
≤
(cid:107)1. An important property is duality. For each pair
∈

p
≤
≤
(cid:107)q, so that
(cid:107)

1 = 1, we may write [23]

≤ ∞
(cid:107)∞ ≤

] such that p−

, one
. . .

(cid:107)p ≥ (cid:107)

1 + q−

q
O

O

(cid:107)

∞

A
(cid:107)

(cid:107)p = sup

B

(cid:107)

(cid:107)q≤

1 |(cid:104)

B, A

(cid:105)| ≡

sup
B
(cid:107)q≤

(cid:107)

B, A
(cid:105)

1 (cid:104)

,

(20)

B, A
(cid:105)

where
= Tr(B†A) is the Hilbert-Schmidt product,
and the second inequality follows since we can arbitrarily
change the sign of B.

(cid:104)

Q  Φ ℰ𝜋 Φ 𝜒ℰ 𝜒𝜋 ℰ Λ III. CONVEXITY

and we may write

(cid:5)

In this section, we show that the minimization of the
main cost functions C
, C1 and CF is a convex opti-
mization problem in the space of the program states π.
This means that we can ﬁnd the optimal program state ˜π
by minimizing C
or, alternatively, sub-optimal program
(cid:5)
states can be found by minimizing either C1 or CF . For
the sake of generality we prove the result for all the cost
functions discussed in the previous section.

(cid:5)

Theorem 1 The minimization of the generic cost func-
tion C = C
, C1, CF , CR or Cp for any p > 1 is a convex
optimization problem in the space of program states. In
particular, the global minimum ˜π can always be found as
a local minimum of C
. Alternatively, this optimal pro-
gram state can be approximated by minimizing C1 or CF .

(cid:5)

Proof. Let us start to show the result for the diamond
distance C

. In this case, we can write the following

(cid:5)

p)π(cid:48)]

(cid:5)

−

[pπ + (1

C
:= (cid:13)
(cid:13)
E − Epπ+(1
−
(p+1
(cid:107)

(1)
=

p)

E −

−

p

p)π(cid:48)

(cid:13)
(cid:13)
(cid:5)
Eπ −
(1
(cid:107)

−

p)

(1

−

(cid:107)(cid:5)

Eπ(cid:48)
(1

(2)

≤ (cid:107)
(3)

p
≤
= pC

p

p

Eπ(cid:107)(cid:5)

+

E −

p)

E −

p)

Eπ(cid:48)

(cid:107)(cid:5)

−

(cid:107)E − Eπ(cid:107)(cid:5)
(π) + (1
(cid:5)

+ (1

−
p)C

(cid:5)

−

(cid:107)E − Eπ(cid:48)

p)
(π(cid:48)),

(cid:107)(cid:5)

(21)

where we use (1) the linearity of
, (2)
E
inequality and (3) the property
(cid:107)1 =
xA
(cid:107)
for any operator A and coeﬃcient x.

x
|

the triangle
(cid:107)1, valid

|(cid:107)

A

For any Schatten p-norm Cp with p

1, we may
exploit the dual representation in Eq. (20) with A =
χ

Λ(π), so that

≥

E −

Cp(π) = sup
(cid:107)q≤

B

(cid:107)

(cid:12)
(cid:12)Tr
{

1

B†[χ

E −

(cid:12)
Λ(π)]
(cid:12) .
}

(22)

4

(cid:12)
(cid:12)

Cp(¯π)

= sup
B
(cid:107)q≤

(cid:107)

1

(cid:12)
(cid:12)Tr
{

B†[p0χ
E

+ p1χ

E −

p0Λ(π0)

p1Λ(π1)]
}

−

(cid:12)
p0B†[χ
(cid:12)Tr
{

E −

Λ(π0)] + p1B†[χ

Λ(π1)]
}

E −

(cid:12)
(cid:12)

(cid:12)
(cid:12)p0Tr
{

B†[χ

E −

Λ(π0)]
}

+

+p1Tr
{

B†[χ

E −

(cid:12)
(cid:12)p0Tr
{

B†[χ

E −

(cid:12)
Λ(π1)]
(cid:12)
}
(cid:12)
(cid:12) +
Λ(π0)]
}

+ (cid:12)

B†[χ
(cid:12)p1Tr
{

(cid:12)
Λ(π1)]
(cid:12)
}

E −

(cid:12)
B†[χ
(cid:12)Tr
{

Λ(π0)]
}

E −

(cid:12)
(cid:12) +

(1)
= sup
B
(cid:107)q≤

(cid:107)

1

(2)
= sup
B
(cid:107)q≤

(cid:107)

1

(3)

≤

(4)

≤

sup
B
(cid:107)q≤

1

(cid:107)

sup
1
B
(cid:107)q≤

p0

(cid:107)
+ p1

sup
1
C
(cid:107)q≤
= p0C(π0) + p1C(π1),

C †[χ
Tr
{

(cid:107)

Λ(π1)]
}

E −

(23)

where we use: (1) linearity of the operator B, (2) lin-
earity of the trace, (3) triangle inequality, (4) and the
inequality supB f (B) + g(B)
supB f (B) + supC g(C)
for the optimization of two functions.

≤

To show the convexity of CF , deﬁned in Eq. (16), we
note that the ﬁdelity function F (ρ, σ) satisﬁes the follow-
ing concavity relation [31]

(cid:32)

(cid:88)

F

k

(cid:33)2

pkρk, σ

(cid:88)

≥

k

pkF (ρk, σ)2 .

(24)

Due to the linearity of χπ = Λ(π), the ﬁdelity in Eq. (17)
πk for ¯π := (cid:80)
satisﬁes F 2
k pkπk. Accordingly,
we get the following convexity result

k pkF 2

¯π ≥

(cid:80)

CF

(cid:33)

pkπk

(cid:32)

(cid:88)

k

(cid:88)

≤

k

pkCF (πk) .

(25)

For the cost function CR, the result comes from the lin-
earity of Λ(π) and the joint convexity of the relative en-
tropy. In fact, for ¯π := p0π0 + p1π1, we may write

S[Λ(¯π)

χ
E

||

] = S[p0Λ(π0) + p1Λ(π1)
= S[p0Λ(π0) + p1Λ(π1)

χ
]
E
p0χ

p0S[Λ(π0), χ
E

≤
with symmetric proof for S[χ
E ||
convexity of CR(π) in Eq. (18). (cid:4)

||
||
] + p1S[Λ(π1), χ
E

+ p1χ
],

E

E

]
(26)

Λ(¯π)]. This implies the

For any convex combination ¯π := p0π0 + p1π1, with p0 +
p1 = 1, we have Λ(¯π) = p0Λ(π0) + p1Λ(π1) by linearity,

The result of the theorem can certainly be extended
to any convex parametrization of program states. For

A. Convex classical parametrizations

instance, assume that π = π(λ), where λ =
λi}
{
probability distribution. This means that, for 0
p
≤
≤
and any two parametrizations, λ and λ(cid:48), we may write

is a
1

π[pλ + (1

−

p)λ(cid:48)] = pπ(λ) + (1

−

p)π(λ(cid:48)).

(27)

Then the problem remains convex in λ and we may there-
fore ﬁnd the global minimum in these parameters. It is
clear that this global minimum ˜λ identiﬁes a program
state π(˜λ) which is not generally the optimal state ˜π in
the entire program space
, even though the solution may
be a convenient solution for experimental applications.

S

Note that a possible classical parametrization consists

of using classical program states, of the form

π(λ) =

(cid:88)

i

λi |

ϕi(cid:105) (cid:104)

ϕi|

,

(28)

ϕi(cid:105)}

where
is an orthonormal basis in the program
space. Convex combinations of probability distributions
therefore deﬁne a convex set of classical program states

{|

Moreover, because χΩπ is Hermitian, the above SDP

can be simpliﬁed into

5

Minimize 2

Subject to Z

≥

Tr2Z
(cid:107)
0 and Z

(cid:107)∞
≥

,
d χΩπ .

(32)

|

(cid:5)

(cid:107)

d

≤

(π) but also pro-
Not only this procedure computes C
χ
vides the upper bound C
[33].
χπ|(cid:107)∞
Tr2 |
E −
(cid:5)
In fact, it is suﬃcient to choose Z = d χ+
Ωπ , where χ+ =
)/2 is the positive part of χ. Using Tr2χΩπ = 0,
(χ +
Ωπ = d
we may write Tr2Z

dTr2χ+

χ
|

(π)

The SDP form in Eq. (32) is particularly convenient
for ﬁnding the optimal program. In fact, suppose now
that π is not ﬁxed but we want to optimize on this state
too, so as to compute the optimal program state ˜π such
(π). The problem is therefore
C
that C
(cid:5)
mapped into the following unique minimization

(˜π) = minπ

.
χΩπ |

2 Tr2|

≤

∈S

(cid:5)

Subject to Z

≥

Minimize 2
0, π

Tr2Z
(cid:107)
0, Tr(π) = 1, Z

(cid:107)∞

,

≥

d χΩπ . (33)

≥

Sclass =

{

π : π =

(cid:88)

i

λi |

ϕi(cid:105) (cid:104)

ϕi|

,

ϕi|

(cid:104)

ϕj(cid:105)

= δij}

. (29)

This algorithm can be used to optimize the performance
of any programmable quantum processor.

Optimizing over this speciﬁc subspace corresponds to op-
timizing the programmable quantum processor over clas-
sical programs. It is clear that global minima in
Sclass
are expected to be very diﬀerent. For instance,
and
Sclass cannot certainly include Choi matrices which are
usually very good quantum programs.

S

IV. CONVEX OPTIMIZATION

A. SDP minimization

Once we have Theorem 1 in our hands, we can success-
fully minimize the various cost functions in the search of
the optimal program state. In other words, for a generic
cost function C we want to solve minπ
C(π). The solu-
tion is exact if we directly use the diamond-distance cost
C

and we minimize it via SDP.

(π) =
(cid:5)
Let us introduce the linear map Ωπ :=

(cid:107)E − Eπ(cid:107)(cid:5)
corresponding Choi matrix

E − Eπ with

∈S

χΩπ = χ

χπ = χ

E −

E −

Λ(π).

(30)

Thanks to the property of strong duality of the diamond
norm, for any program π we can compute the cost func-
tion C
(cid:5)

via the following SDP [32]

(π) =

Ωπ(cid:107)(cid:5)
(cid:107)
1
2

Minimize

Subject to

(

Tr2M0(cid:107)∞
(cid:107)
(cid:18) M0

+

Tr2M1(cid:107)∞
(cid:19)

(cid:107)
d χΩπ

) ,

−

d χ†Ωπ M1

≥

0,

(31)

−
0 and M1 ≥
equals the maximum singular value of O.

0 in Cd

, and the spectral

d(cid:48)

×

where M0 ≥
O
norm
(cid:107)∞
(cid:107)

B. Gradient descent

An alternative approach (useful for deeper processors)
consists in the optimization of the larger but easier-to-
compute cost function C = C1 (trace distance) or CF
(inﬁdelity). According to Theorem 1, the cost function
R is convex over the program space
C :
and,
S
C(π) by
therefore, we can solve the optimization minπ
using gradient-based ML algorithms. This means that
we need to compute the derivatives of C and use gradient
descent in order to converge to a local (global) minimum.
is

The sub-diﬀerential of C at the generic point π

S →

∈S

∈ S

deﬁned as

∂C(π) =

Z : C(σ)

C(π)

Tr[Z(σ

π)],

σ

{

∈ S}(34)
where Z is Hermitian [34, 35]. In the points where C is
not only convex but also diﬀerentiable, then

−

−

≥

∀

∂C(π) =

,
C(π)
}

{∇

(35)

∇

namely the subgradient contains a single element, the
gradient
C, that can be obtained as the Fr´echet deriva-
tive of C (for more details see Appendix A). In the points
where C is not diﬀerentiable, then the gradient still pro-
vides an element of the subgradient to be used in the
gradient-based minimization process.
In order to compute the gradient

C, it is convenient
to consider the Kraus decomposition of the processor
map Λ. Let us write

∇

Λ(π) =

(cid:88)

k

AkπA†k,

(36)

with Kraus operators Ak. We then deﬁne the dual
map Λ∗ of the processor as the one (generally non-trace-
preserving) which is given by the following decomposition

Λ∗(ρ) =

(cid:88)

k

A†kρAk.

(37)

composition

With these deﬁnitions in hands, we prove the following.

Theorem 2 Suppose we use a quantum processor Q with
map Λ(π) = χπ in order to approximate the Choi matrix
. Then, the gradients of the
χ
E
trace distance C1(π) and the inﬁdelity CF (π) are given
by the following analytical formulas

of an arbitrary channel

E

C1(π) =

∇

CF (π) =

∇

F (π) =

∇

(cid:88)

k

sign(λk)Λ∗(Pk),

(cid:112)
2

Λ∗

1
−
(cid:104)
√χ

−
1
2

CF (π)

(√χ

E

E

F (π),

∇
Λ(π) √χ

(38)

(39)

(cid:105)

)−

E

1

2 √χ
E

,

(40)

where λk (Pk) are the eigenvalues (eigenprojectors) of the
. When C1(π) or CF (π) are
Hermitian operator χπ −
E
not diﬀerentiable at π, then the above expressions provide
an element of the subgradient ∂C(π).

χ

Proof. We prove the above theorem assuming that the
functions are diﬀerentiable for program π. For non-
diﬀerentiable points, the only diﬀerence is that the above
analytical expressions are not unique and provide only
one of the possibly inﬁnite elements of the subgradient.
Further details of this mathematical proof are given in
Appendix A. Following matrix diﬀerentiation (see Ap-
pendix A 1), for any function f (A) = Tr[g(A)] of a matrix
A, we may write

6

Exploiting this expression in Eq. (41) we get the gradient
F (π) as in Eq. (40). The other Eq. (39) simply follows

∇
from applying the deﬁnition in Eq. (16).

For the trace distance, let us write the eigenvalue de-

χπ −

χ
E

=

(cid:88)

k

λkPk .

(46)

Then using linearity of Eq. (42), the deﬁnition of proces-
sor map of Eq. (8) and diﬀerential calculations of the
trace distance (see Appendix A 3 for details), we can
write

dC1(π) =

=

(cid:88)

k
(cid:88)

sign(λk)Tr[PkΛ(dπ)]

sign(λk)Tr[Λ∗(Pk)dπ]

k
= Tr

Λ∗[sign(χπ −
{
From the deﬁnition of the gradient in Eq. (41), we ﬁnally
get

(47)

)]dπ

χ
E

}

.

E
which leads to the result in Eq. (38). (cid:4)

C1(π) = Λ∗[sign(χπ −

∇

χ

)],

(48)

The above results in Eqs. (39) and (38) can be used
together with the projected subgradient method [15]
or conjugate gradient algorithm [16, 17] to iteratively
ﬁnd the optimal program state in the minimization of
minπ
C(π) for C = C1 or CF . In the following section
we present the details of the two mentioned gradient-
based ML algorithms and how they can be adapted for
the learning of program states.

∈S

dTr[g(A)] = Tr[g(cid:48)(A)dA],

(41)

V. GRADIENT-BASED CONVEX
OPTIMIZATION TECHNIQUES

f (A) = g(cid:48)(A). Both the trace-
and the gradient is
distance and ﬁdelity cost functions can be written in this
form. To ﬁnd the explicit gradient of the ﬁdelity function
we ﬁrst note that, by linearity, we may write

∇

Λ(π + δπ) = Λ(π) + Λ(δπ) ,

(42)

and therefore the following expansion

√χ
E
Λ(π)√χ

Λ(π + δπ)√χ
+ √χ

E

=
Λ(δπ)√χ
E

E

E

√χ
E

.

(43)

From this equation and diﬀerential calculations of the
ﬁdelity (see Appendix A 2 for details), we ﬁnd

dF =

(cid:104)

Tr

1
2

(√χ

Λ(π)√χ
E

E

)−

1

2 √χ

E

Λ(δπ)√χ

(cid:105)

E

,

(44)

where dF = F (π + δπ)
property of the trace, we get

−

F (π). Then, using the cyclic

dF =

(cid:104)

Λ∗

(cid:104)
√χ

Tr

1
2

(√χ
E

E

Λ(π)√χ

E

)−

1

2 √χ

E

(cid:105)

(cid:105)

δπ

. (45)

Gradient-based convex optimization is at the heart of
many popular ML techniques such as, online learning in
a high-dimensional feature space [18], missing value esti-
mation problems [19], text classiﬁcation, image ranking,
and optical character recognition [36], to name a few.
In all the above applications, “learning” corresponds to
the following minimization problem minx
f (x), where
is a convex set. Quan-
f (x) is a convex function and
tum learning falls into this category, as the space of pro-
gram states is convex due to the linearity of quantum
mechanics and cost functions are typically convex in this
space (see Theorem 1). Gradient-based approaches are
among the most applied methods for convex optimiza-
tion of non-linear, possibly non-smooth functions [34].
Here we present two algorithms, the projected subgra-
dient method and the conjugate gradient method, and
show how that can be adapted to our problem.

∈S

S

Projected subgradient methods have the advantage of
simplicity and the ability to optimize non-smooth func-
2(cid:1)
tions, but can be slower, with a convergence rate

(cid:0)(cid:15)−

O

O

(cid:0)(cid:15)−

for a desired accuracy (cid:15). Conjugate gradient meth-
1(cid:1), pro-
ods [16, 17] have a faster convergence rate
vided that the cost function is smooth. This conver-
1/2(cid:1)
gence rate can be improved even further to
for strongly convex functions [37] or using Nesterov’s ac-
celerated gradient method [38]. The technical diﬃculty
in the adaptation of these methods for learning program
states comes because the latter is a constrained optimiza-
tion problem, namely at each iteration step the optimal
program must be a proper quantum state, and the cost
functions coming from quantum information theory are,
generally, non-smooth.

(cid:0)(cid:15)−

O

A. Projected subgradient method

Given the space
onto

projection

PS

of program states, let us deﬁne the
as

S
S

PS

(X) = argmin

S (cid:107)

π

∈

X

π

(cid:107)2 ,

−

(49)

where argmin is the argument of the minimum, namely
to the operator X. Then, a
the closest state π
C(π) is to apply the
ﬁrst order algorithm to solve minπ
projected subgradient method [15, 34], which iteratively
applies the following steps

∈ S

∈S

1) Select an operator gi from ∂C(πi),
2) Update πi+1 =

αigi) ,

(πi −

PS

(50)

where i is the iteration index and αi a learning rate.

The above algorithm diﬀers from standard gradient
i) the update rule is based on
methods in two aspects:
the subgradient, which is deﬁned even for non-smooth
αigi is generally not a
functions; ii) the operator πi −
quantum state, so the algorithm ﬁxes this issue by pro-
jecting that operator back to the closest quantum state,
via Eq. (49). The algorithm converges to the optimal so-
(approximating the optimal program ˜π) as [15]
lution π

∗

C(πi)

C(π

)

∗

−

≤

e1 + G (cid:80)i
2 (cid:80)i

k=1 αk

k=1 α2
k

=: (cid:15),

(51)

(cid:107)

π

∈

2
2 is the initial error (in Frobenius
where e1 =
π1 −
∗(cid:107)
(cid:107)
2
∂C. Pop-
G for any g
norm) and G is such that
g
2 ≤
(cid:107)
ular choices for the learning rate that assure convergence
1/√k and αk = a/(b + k) for some a, b > 0.
are αk ∝
In general, the projection step is the major drawback,
which often limits the applicability of the projected sub-
gradient method to practical problems. Indeed, projec-
tions like Eq. (49) require another full optimization at
each iteration that might be computationally intensive.
Nonetheless, we show in the following theorem that this
issue does not occur in learning quantum states, because
the resulting optimization can be solved analytically.

Theorem 3 Let X be a Hermitian operator in a d-
dimensional Hilbert space with spectral decomposition

7

X = U xU †, where the eigenvalues xj are ordered in de-
creasing order. Then

(X) of Eq. (49) is given by

PS
where θ = 1
s

PS
(X) = U λU †, λi = max
{

(cid:80)s

j=1 (xj −

1) and

xi −

θ, 0

,
}

(52)

s = max






k

[1, ..., d] : xk >

∈

1
k

k
(cid:88)

j=1

(xj −






1)

.

(53)

Proof. Any quantum (program) state can be written
in the diagonal form π = V λV † where V is a unitary
matrix, and λ is the vector of eigenvalues in decreasing
j λj = 1. To ﬁnd the optimal
order, with λj ≥
state, it is required to ﬁnd both the optimal unitary V
and the optimal eigenvalues λ with the above property,
i.e.,

0 and (cid:80)

(X) = argmin

V,λ

X

(cid:107)

−

V λV †

(cid:107)2 .

PS

(54)

For any unitarily-invariant norm, the following inequality
holds [39, Eq. IV.64]

π

−

−

(55)

X
(cid:107)

(cid:107)2 ,
λ

x
(cid:107)2 ≥ (cid:107)
with equality when U = V , where X = U xU † is a spec-
tral decomposition of X such that the xj’s are in de-
creasing order. This shows that the optimal unitary in
Eq. (54) is the diagonalization matrix of the operator X.
The eigenvalues of any density operator form a probabil-
ity simplex. The optimal eigenvalues λ are then obtained
thanks to Algorithm 1 from Ref. [18]. (cid:4)

In the following section we present an alternative al-
gorithm with faster convergence rates, but stronger re-
quirements on the function to be optimized.

B. Conjugate gradient method

The conjugate gradient method [16, 34], sometimes
called Frank-Wolfe algorithm, has been developed to pro-
vide better convergence speed and to avoid the projection
step at each iteration. Although the latter can be explic-
itly computed for quantum states (thanks to our The-
orem 3), having a faster convergence rate is important,
especially with higher dimensional Hilbert spaces. The
downside of this method is that it necessarily requires a
diﬀerentiable cost function C, with gradient

C.

In its standard form, the conjugate gradient method to
C(π) is deﬁned by

approximate the solution of argminπ
the following iterative rule

∈S

∇

1) Find argminσ
2) πi+1 = πi + 2

∈S
i+2 (σ

Tr[σ

C(πi)],
∇
πi) = i

i+2 πi + 2

i+2 σ.

−

(56)

The ﬁrst step in the above iteration rule is solved by ﬁnd-
C(πi). Indeed, since
ing the smallest eigenvector
C is
π is an operator and C(π) a scalar, the gradient

∇

of

σ

(cid:105)

|

∇

an operator with the same dimension of π. Therefore, for
learning quantum programs we ﬁnd the following itera-
tionfollowing

The previous deﬁnition of the trace distance, C1 in
Eq. (9), is recovered for µ
0 and, for any non-zero
→
µ, the Cµ bounds C1 as follows

8

|

of

∇

i+2 |

σi(cid:105)

C(πi),

1) Find the smallest eigenvalue
i+2 πi + 2
2) πi+1 = i
.
σi|

σi(cid:105) (cid:104)
When the gradient of C is Lipschitz continuous with con-
stant L, the conjugate gradient method converges after
(L/(cid:15)) steps [17, 38]. The following iteration with adap-
O
tive learning rate αi has even faster convergence rates,
provided that C is strongly convex [37]:

(57)

∇

(58)

∇
(cid:105)

C(πi),

τi(cid:107)
−

[0,1] α
∈

+ α2 βC
2 (cid:107)
3) πi+1 = (1

of
σi(cid:105)
1) Find the smallest eigenvalue
|
C(πi)
τi,
2) Find αi = argminα
∇
(cid:104)
2
σi| −
C, for τi =
σi(cid:105)(cid:104)
πi,
|
σi|
.
σi(cid:105) (cid:104)
αi)πi + αi |
(cid:107) · (cid:107)C depend on C [37].
where the constant βC and norm
In spite of the faster convergence rate, conjugate gra-
dient methods require smooth cost functions (so that the
gradient
C is well deﬁned at every point). However,
cost functions based on trace distance (9) are not smooth.
For instance, the trace distance in one-dimensional spaces
reduces to the absolute value function
that is non-
analytic at x = 0. When some eigenvalues are close
to zero, conjugate gradient methods may display unex-
pected behaviors, though we have numerically observed
that convergence is always obtained with a careful choice
of the learning rate. Moreover, in the next section we will
show how to formally justify the applicability of the con-
jugate gradient method, following Nesterov’s smoothing
prescription [38].

x
|

|

Cµ(π)

C1(π)

≤

≤

Cµ(π) +

µd
2

,

(61)

where d is the dimension of the program state π.
Appendix B 2 we then prove the following result

In

Theorem 4 The smooth cost function Cµ(π) is a convex
function over program states and its gradient is given by

Cµ(π) = Λ∗[h(cid:48)µ(χπ −
where h(cid:48)µ is the derivative of hµ. Moreover, the gradient
is L-Lipschitz continuous with

(62)

)],

∇

χ

E

L =

d
µ

,

(63)

where d is the dimension of the program state.

O

Being Lipschitz continuous, the conjugate gradient al-
gorithm and its variants [37, 38] converge up to an ac-
(L/(cid:15)) steps. In some applications, it is
curacy (cid:15) after
desirable to analyze the convergence in trace distance in
the limit of large program states, namely for d
.
→ ∞
The parameter µ can be chosen such that the smooth
trace distance converges to the trace distance, namely
. Indeed, given the inequality (61),
Cµ →
(1+η)) for some η > 0 so
a possibility is to set µ =
O
that, from Eq. (63), the convergence to the trace norm is
(d2+η) steps.
achieved after

C1 for d

→ ∞

(d−

O

C. Smooth trace distance

VI. LEARNING OF ARBITRARY UNITARIES

O

(cid:0) L
(cid:15)

The conjugate gradient method converges to the global
(cid:1) steps, provided that the gradient of
optimum after
C is L-Lipschitz continuous [38]. However, the constant
L can diverge for non-smooth functions like the trace
distance (9) so the convergence of the algorithm cannot
be formally stated, although it may still be observed in
numerical simulations, as we will show. To solidify the
convergence proof (see also Appendix B 2) we introduce a
smooth approximation to the trace distance. This is de-
ﬁned by the following cost function that is diﬀerentiable
at every point

Cµ(π) = Tr [hµ (χπ −

χ

E

)] =

(cid:88)

j

hµ(λj) ,

(59)

where λj are the eigenvalues of χπ −
so-called Huber penalty function

χ

E

and hµ is the

The simulation of quantum gates or, more generally,
unitary transformations is crucial for quantum comput-
ing applications [20] so ML techniques have been de-
veloped for this purpose [40–43]. Here we consider the
more general setting of simulating an arbitrary ﬁnite-
dimensional unitary U by means of a programmable
quantum processor with map Λ. For a unitary U the
Choi matrix is a maximally-entangled pure state χ
=
χU (cid:105)(cid:104)
is a one-dimensional
|
projector and Eq. (40) is drastically simpliﬁed to

. Therefore, √χ

χU |

= χ

E

E

E

∇

F (π) =

χU |
χU (cid:105)(cid:104)
Λ∗ [
|
(cid:112)
Λ(π)
χU |
|
(cid:104)
Therefore the gradient (39) of the convex cost function
CF ,

]
χU (cid:105)

(64)

2

.

CF (π) =

Λ∗ [

χU (cid:105)(cid:104)
|

χU |

] ,

−

∇

(65)

hµ(x) :=

(cid:40) x2
2µ
x
|

| −

µ
2

if
if

x
x

< µ ,
µ .

|
| ≤

|
|

(60)

is independent of π. When we employ the conjugate gra-
dient method, the state
is the same for each iteration
step. This implies that conjugate gradient is converging

σk(cid:105)
|

χU |

χU (cid:105)(cid:104)
Λ∗ [
|

towards one eigenvector of
] with minimum
−
eigenvalue. In other terms, the ﬁxed point of the itera-
tion in Eq. (57), namely the optimal program state ˜πF
(according to the ﬁdelity cost function) is pure and equal
to the eigenvector of Λ∗ [
] with maximum eigen-
χU (cid:105)(cid:104)
|
value.
The above result can be proven as follows. Let π1 be
the initial guess for the program state. After k iterations
of Eq. (57), we ﬁnd the following approximation to the
optimal program state

χU |

πk =

2

k + k2 π1 +

(cid:18)

2
k + k2

1

−

(cid:19)

˜πF ,

(66)

where
πk →

2

k+k2 = (cid:81)k
˜πF for k

1
−
j=1

→ ∞

j
j+2 . The above equation shows that
, with error in trace distance

2
k + k2 (cid:107)

2) .

O

((cid:15)−

(k−

(67)

πk −
(cid:107)

˜πF (cid:107)1 =

˜πF (cid:107)1 =

π1 −
For learning arbitrary unitaries, the ﬁdelity cost func-
tion provides a convenient choice where the optimal pro-
gram can be found analytically. Moreover, this example
1) of the conjugate
shows that the convergence rate
method provides a worst case instance that can be beaten
in some applications with some suitable cost functions.
2 for learning arbitrary
From Eq. (67) we see that (cid:15) = k−
unitaries via the minimization of CF , meaning that con-
1/2). On
vergence is obtained with the faster rate
the other hand, there are no obvious simpliﬁcations for
the optimization of the trace distance, since the latter
still requires the diagonalization of Eq. (46). For the
trace distance, or its smooth version, only numerical ap-
proaches are feasible.

((cid:15)−

O

O

VII. TELEPORTATION PROCESSOR

×

One possible (shallow) design for the quantum proces-
sor Q is the teleportation protocol [44] which has to be
applied to a generic program state π instead of a maxi-
mally entangled state. In dimension d, the program πAB
is a d
d state. The teleportation protocol involves a ba-
sis of d2 maximally entangled states
Ui}
of teleportation unitary such that Tr(U †i Uj) = dδij [45].
In the protocol, an input d-dimensional state ρS and the
A part of the program πAB are subject to the projector
. The classical outcome i is communicated to the
Φi(cid:105)(cid:104)
|
1
B part of πAB where the correction U −
is applied. In
i
this way, we implement the following teleportation chan-
nel

and a basis

Φi(cid:105)

Φi|

{

|

Eπ from qudit S to qudit B
U B
Eπ(ρ) =
i (cid:104)

ΦSA
i

ρS
|

(cid:88)

⊗

i

πAB

ΦSA
i
|

U B
†i
(cid:105)

.

(68)

The Choi matrix of the teleportation channel

Eπ can
be written as χπ = Λtele(π), where the map of the tele-
portation processor is equal to

Λtele(π) =

1
d2

(cid:88)

i

(U ∗i ⊗

Ui) π (U ∗i ⊗

Ui)† .

(69)

9

FIG. 3. Optimization of program states for simulating
the rotation R(θ) = eiθX with a teleportation processor.
The optimization is via the minimization of trace distance
C1 of Eq. (9) with the projected subgradient method in
Eq. (50). The dashed lines correspond to the upper bound
(cid:112)1 − F [Λ(˜πF ), χE ]2 of the trace distance, where ˜πF is the op-
timal program that maximizes the ﬁdelity, namely the eigen-
vector of Eq. (64) with maximum eigenvalue.

Note that, if the program π is teleportation covariant [21],
Ui] = 0, then π is automatically a
namely if [π, U ∗i ⊗
ﬁxed point of the map, i.e., we have χπ := Λtele(π) = π.
Also note that, the channel in Eq. (69) is self-dual, i.e.,
Λ∗ = Λ. As a result, for any operator ˆO, we may write

Λ∗tele( ˆO) =

1
d2

(cid:88)

i

(U ∗i ⊗

Ui) ˆO (U ∗i ⊗

Ui)† .

(70)

As an example, assume that the target channel is a
χU |
χU (cid:105)(cid:104)
|
is maximally entangled.
, we may
(cid:105)

unitary U , so that its Choi matrix is χU :=
Φ
with
= 11
(cid:105)
|
Φ
11
By using Eq. (70) and U ∗
|
⊗
write the dual processor map

χU (cid:105)
|

= 11

and

U †

⊗

⊗

Φ

Φ

U

(cid:105)

(cid:105)

|

|

Λ∗tele[
|
1
d2

=

]
χU |
χU (cid:105)(cid:104)
(cid:88)
(cid:0)11

i

(cid:1)

V U
i

Φ
|

Φ

(cid:105)(cid:104)

|

(cid:0)11

⊗

V U
i

(cid:1)† ,

(71)

⊗

χU |

where V U
i = UiU U †i . The maximum eigenvector of
] represents the optimal program state ˜πF
χU (cid:105)(cid:104)
Λ∗tele[
|
for simulating the unitary U via the teleportation pro-
cessor (according to the ﬁdelity cost function). In some
cases, the solution is immediate. For instance, this hap-
pens when V U
U is independent of i. This is the case
when U is a teleportation unitary, because it satisﬁes the
Weyl-Heysenberg algebra [21]. For a teleportation uni-
tary U , we simply have

i ∝

χU (cid:105)(cid:104)
so that the unique optimal program is ˜πF =

χU (cid:105)(cid:104)

Λ∗[
|

χU |

χU |

] =

|

,

(72)

χU (cid:105)(cid:104)
|

.
χU |

In Fig. 3 we show the convergence of the projected
subgradient algorithm using the teleportation processor

and target unitaries R(θ) = eiθX , for diﬀerent values of
θ. When θ is a multiple of π/2, then the above unitary
is teleportation covariant and the Frank-Wolfe algorithm
converges to zero trace distance. For other values of θ
perfect simulation is impossible, and we notice that the
algorithm converges to a non zero value of the trace dis-
tance (9). For comparison, in Fig. 3 we also plot the value
]2, where
of the ﬁdelity upper bound
˜πF is the optimal program that maximizes the ﬁdelity of
Eq. (17), namely the eigenvector of Eq. (71) with max-
imum eigenvalue. We note that for θ = π/2(cid:96) the trace
is
distance decreases for larger θ. The limit case (cid:96)
→ ∞
perfectly simulable as R(0) is teleportation covariant.

F [Λ(˜πF ), χ
E

(cid:112)
1

−

A. Pauli channel simulation

Pauli channels are deﬁned as [1]

(ρ) =

P

(cid:88)

i

piUiρU †i

,

(73)

where Ui are generalized Pauli operators and pi some
probabilities. For d = 2 the Pauli operators are the four
Pauli matrices I, X, Y, Z and in any dimension they form
the Weyl-Heisenberg group [1]. These operators are ex-
actly the teleportation unitaries Uj deﬁned in the previ-
ous section. The Choi matrix χ
is
diagonal in the Bell basis, i.e., we have

of a Pauli channel

P

P

=

χ

P

(cid:88)

i

pi|

Φi(cid:105)(cid:104)

Φi|

,

(74)

and

Φ
Ui|

where Φi = 11

Φ
(cid:105)
|
We now consider the simulation of a Pauli channel with
the teleportation quantum processor introduced in the
previous section. Let

j=1 |

jj

⊗

(cid:105)

(cid:105)

/√d.

= (cid:80)d

10

FIG. 4. PBT scheme. Two distant parties, Alice and Bob,
share N maximally entangled pairs {Ak, Bk}N
k=1. Alice also
has another system C in the state |ψ(cid:105). To teleport C,
Alice performs the POVM ΠAC
on all her local systems
A = {Ak}N
k=1 and C. She then communicates the outcome i
to Bob. Bob discards all his systems B = {Bk}N
k=1 with the
exception of Bi. After these steps, the state |ψ(cid:105) is approx-
imately teleported to Bi. Similarly, an arbitrary channel E
is simulated with N copies of the Choi matrix χAkBk
. The
ﬁgure shows an example with N = 5, where i = 4 is selected.

E

i

proven [46, 47] that these are the only channels we can
perfectly simulate. This is true even if we apply the Pauli
corrections in a probabilistic way, i.e., we assume a classi-
cal channel from the Bell outcomes to the corresponding
label of the Pauli correction operator [47].

π =

(cid:88)

ij

πij|

Φi(cid:105)(cid:104)

Φj|

,

(75)

be an arbitrary program state expanded in the Bell ba-
sis. For any program state, the Choi matrix of the
teleportation-simulated channel is given by Eq. (69). Us-
ing standard properties of the Pauli matrices we ﬁnd

χπ ≡

Λ(π) =

(cid:88)

i

πii|

Φi(cid:105)(cid:104)

Φi|

,

(76)

namely a generic state is transformed into a Bell diagonal
state. Therefore, the cost function

VIII. PORT-BASED TELEPORTATION

We now study a design of programmable quantum pro-
cessor that can potentially simulate any target quantum
channel in the asymptotic limit of an arbitrarily large
program state. This design is PBT [11–13], a general-
ization of the standard teleportation scheme. For ﬁnite-
dimensional programs, a PBT processor cannot achieve
a perfect deterministic simulation of an arbitrary chan-
nel [10]. In this realistic ﬁnite-dimensional setting, our
study ﬁnally establishes the optimal performance achiev-
able by this type of quantum processor.

C Pauli
1

=

χP −
can be minimized analytically for any Pauli channel by
choosing πij = piδij. With this choice we ﬁnd C Pauli
= 0,
meaning that the simulation is perfect.

χπ(cid:107)1 ,

(77)

(cid:107)

1

From theory [46–48] we know that only Pauli chan-
nels can be perfectly simulated in this way. No mat-
ter how more general we can make the states π, it is

A. Basics of PBT

The overall protocol of PBT is illustrated in Fig. 4. Un-
like standard teleportation protocol, PBT requires that
Alice and Bob share N entangled pairs for the simulation
of the identity channel [11]. The protocol is based on a re-
source state (the program) given by πAB = (cid:78)N
k=1 ΦAkBk ,

AliceBobdiscardBkk6=i|ψiC≈|ψiA1B1A2B2A3B3A4B4A5B5(cid:7)iΠACi|

ΦAkBk (cid:105)

are Bell states for Alice’s N qudits A =
where
(A1, . . . , AN ) and Bob’s N qudits B = (B1, . . . , BN ). Af-
ter preparing such a state, Alice performs a joint positive-
on her A-half of
operator value measure (POVM)
πAB and an input state
(cid:105)C that she wishes to teleport.
She communicates the outcome i to Bob, who discards all
“ports” B except Bi = Bout. The resulting PBT channel
Pπ :

Πi}
{

ψ
|

HC (cid:55)→ HBout is then
N
(cid:88)

Pπ(ρ) =

Tr
A ¯BiC

[Πi(πAB

ρC)]Bi

⊗

Bout

→

(78)

i=1

N
(cid:88)

i=1

=

Tr
A ¯BiC

(cid:104)(cid:112)

Πi(πAB

(cid:105)

(cid:112)

Πi

ρC)

⊗

,

Bi

→

Bout

where ¯Bi = B
Bi =
\
PBT approximates an identity channel

Bk : k
{

. In the limit N
= i
}
ρ.
Pπ(ρ)

≈

In the standard PBT protocol [11, 12] the following

,
→ ∞

POVM is used

Πi = ˜Πi +

(cid:32)

11

−

1
N

(cid:88)

k

(cid:33)

˜Πk

,

where

1/2

˜Πi = σ−
AC ΦAiCσ−
N
(cid:88)

1/2
AC ,

σAC :=

ΦAiC,

(79)

(80)

(81)

i=1

1/2 is an operator deﬁned only on the support of
and σ−
σ. The PBT protocol is formulated for N
2 ports.
However, we also include here the trivial case for N = 1,
corresponding to the process where Alice’s input is traced
out and the output is the reduced state of Bob’s port, i.e.,
a maximally mixed state.

≥

With the choice of the POVM in Eq. (79), the identity
can be simulated with ﬁdelity [11, 13]

channel

I

Fπ = 1

− O

(cid:19)

(cid:18) 1
N

,

(82)

so perfect simulation is possible only in the limit N
.
→ ∞
More generally, it has been shown [14] that simulation
error in diamond norm scales as

(cid:107)I − Pπ(cid:107)(cid:5) ≤

2d(d
N

−

1)

.

(83)

B. Channel simulation via PBT

E

E

between

Any generic channel

can be written as a composition
. Channel sim-
and the identity channel
E ◦I
ulation can be achieved by replacing the identity channel
to Bi.
I
However, since Bob does not perform any post-processing
on its systems B, aside from discarding all ports Bk with
N to all his
k

= i, he can also apply ﬁrst the channel

Pπ, and then applying

with its PBT simulation

I

E

⊗

E

11

ports and then discard all the ports Bk with k
doing so, he changes the program state to

= i. In

πAB = 11A ⊗ E

⊗
B

(cid:34) N
(cid:79)

N

k=1

(cid:35)

ΦAkBk

=

N
(cid:79)

k=1

χAkBk
E

.

(84)

E

In other terms, any channel
can be PBT-approximated
as program state. How-
by N copies of its Choi matrix χ
E
ever, while such a program state is optimal when N
,
→ ∞
for ﬁnite N there may be better alternatives. In general,
for any ﬁnite N , ﬁnding the optimal program state πAB
with PBT is an open problem,
simulating a channel
and no explicit solutions or procedures are known.

E

We employ our convex optimization procedures to ﬁnd
the optimal program state. This can be done either ex-
actly by minimizing the diamond distance cost function
C
via SDP, or approximately, by determining the opti-
(cid:5)
mal program state via the minimization of the trace dis-
tance cost function C1 via the gradient-based ML tech-
niques discussed above. For this second approach, we
need to derive the map Λ of the PBT processor, between
the program state π to output Choi matrix as in Eq. (8).
From the deﬁnition in Eq. (78) we ﬁnd the following op-
erator sum decomposition

Λ(π) = χ

π = 11D ⊗ Pπ[ΦDC]
(cid:104)(cid:112)

P
N
(cid:88)

Πi(πAB

Tr
A ¯BiC

(cid:112)

ΦDC)

Πi

(cid:105)

Bi

→

Bout

⊗

KikπK †ik ,

(85)

=

=

i=1
(cid:88)

ik

where the corresponding Kraus operators are

K AB
ik

→

DBout

=

(cid:112)

e(i)
k |
(cid:104)

Πi ⊗

11BD|

ΦDC(cid:105)

,

(86)

and

e(i)
k (cid:105)
|

span a basis of A ¯BiC.

C. Program state compression

The program state grows exponentially with the num-
ber of ports N as d2N where d is the dimension of the
Hilbert space. However, as also discussed in the origi-
nal proposal [11, 12] and more recently in Ref. [49], the
resource state of PBT can be chosen with extra sym-
metries, so as to reduce the number of free parameters.
In particular, we may consider the set of program states
that are symmetric under the exchange of ports, i.e., such
that rearranging any A modes and the corresponding B
modes leaves the program state unchanged.

Let Ps be the permutation operator swapping labels 1
to N for the labels in the sequence s, which contains all
the numbers 1 to N once each in some permuted order.
Namely Ps exchanges all ports according to the rule i
(cid:55)→
si. Since PBT is symmetric under exchange of ports, we
may write

PPsπP †

s

(ρ) =

Pπ (ρ) for any s.

(87)

(cid:54)
(cid:54)
(cid:54)
Consider then an arbitrary permutation-symmetric re-
source state πsym as

When the program state π = χ⊗

N is directly used in

Eq. (85) we ﬁnd

12

πsym =

1
N !

(cid:88)

s

PsπP †s ,

where the sum is over all possible sequences s that deﬁne
independent permutations and N ! is the total number
of possible permutations. Clearly
Pπ, so any
program state gives the same PBT channel as some sym-
metric program state. It therefore suﬃces to consider the
set of symmetric program states. This is a convex set:
any linear combination of symmetric states is a symmet-
ric state.

Pπsym =

|

x

To construct a basis of the symmetric space, we note
that each element of a density matrix is the coeﬃcient
). If permutation of labels
y
of a dyadic (of the form
|
(cid:105) (cid:104)
maps one dyadic to another, the coeﬃcients must be the
same. This allows us to constrain our density matrix
using fewer global parameters. For instance, for d = 2 we
can deﬁne the 16 parameters n00,00, n00,01, n00,10, etc.,
corresponding to the number of ports in the dyadic of the
form
, etc.
Each element of a symmetric density matrix can then
be deﬁned solely in terms of these parameters, i.e., all
elements corresponding to dyadics with the same values
of these parameters have the same value.

0A0B(cid:105) (cid:104)
|

0A0B(cid:105) (cid:104)
|

0A0B(cid:105) (cid:104)

,
0A0B|

1A0B|

0A1B|

,

|

cA, dB|

aA, bB(cid:105)(cid:104)
|

For the general qudit case, in which our program state
consists of N ports, each composed of two d-dimensional
qudits, we can ﬁnd the number of independent parame-
ters from the number of independent dyadics. Each port
in a dyadic can be written as
where the
extra indices A and B describe whether those states are
modeling either qudit A or B. There are d4 diﬀerent
combinations of
, so we can place each qudit
a, b, c, d
}
{
into one of d4 categories based on these values. If two el-
ements in the density matrix correspond to dyadics with
the same number of ports in each category, they must
take the same value. Hence, the number of independent
coeﬃcients is given by the number of ways of placing
N (identical) ports into d4 (distinguishable) categories.
This is exactly the binomial coeﬃcient
(cid:18)N + d4
d4

(N d4

(88)

1) .

(cid:19)
1

−
1

=

−

O

−

Consequently, exploiting permutation symmetry of the
PBT protocol, we can exponentially reduce the number
of parameters for the optimization over program states.
The number of parameters can be reduced even further
by considering products of Choi matrices. We may focus
indeed on the Choi set

(cid:40)

CN =

π : π =

(cid:41)

N

pkχ⊗
k

,

(cid:88)

k

where each χk = χk
AB is a generic Choi matrix, therefore
111, and pk form a probability dis-
satisfying TrBχk = d−
tribution. Clearly
is a convex set. We now show that
this set can be further reduced to just considering N = 1.

C

Λ(π) =

N
(cid:88)

i=1

TrA ¯BiC

(cid:2)Πi

(cid:0)χ⊗

N
AB ⊗

ΦDC

(cid:1)(cid:3)

Bi

→

Bout

=

1
dN

−

1

N
(cid:88)

i=1

:= ˜Λ(χ) ,

TrAiC [Πi (χAiBout ⊗

ΦDC)]

(90)

(91)

(92)

namely that the optimization can be reduced to the
(d4) dimensional space of Choi matrices χ. Note that,

O
in the above equation, we used the identity

N

Tr ¯Biχ⊗

AB = χAiBi ⊗

11 ¯Ai
dN

−

1 ,

(93)

where ¯Ai = A

Ai.

\

Now let π be a linear combination of tensor products
, each with probability pk as

of Choi matrix states, χ⊗
k
in Eq. (89). Then we can write

N

Tr ¯BiπAB = Tr ¯Bi

(cid:88)

N

pkχ⊗
k

k
(cid:18)

(cid:88)

=

pk

k

χk

AiBi ⊗

(cid:19)

.

11 ¯Ai
1
dN

−

(94)

(95)

N of some other Choi matrix χ(cid:48) = (cid:80)

However, this is precisely the partial trace over the tensor
k pkχk.
product χ(cid:48)⊗
Hence, the program state π = (cid:80)
simulates the
same channel as the resource state π(cid:48) = ((cid:80)
k pkχk)⊗

N .
CN can
be reduced to the optimization over products of Choi
N . From Eq. (92) this can be further reduced
matrices χ⊗
to the optimization of the quantum channel ˜Λ over the
convex set of single-copy Choi matrices χ

Therefore, the optimization over the convex set

k pkχ⊗
k

N

π : π = χAB, TrBχAB = 11/2
C1 =
{
}
(d4). Using

O

C1 drastically reduces the diﬃculty
which is
of numerical simulations, thus allowing the exploration of
signiﬁcantly larger values of N . Details on how to explic-
itly construct ˜Λ for d = 2 are presented in Appendix C.

,

(96)

D. Numerical examples

We ﬁrst consider the simulation of an amplitude damp-
i K AD
, which is deﬁned

i ρK AD

ing channel
by the Kraus operators

EAD(ρ) = (cid:80)

†

i

(89)

K AD

0 =

(cid:18)1

0
0 √1

−

(cid:19)

p

, K AD

1 =

(cid:19)
(cid:18)0 √p
0
0

.

(97)

In Fig. 5 we study the performance of the PBT simula-
tion of the amplitude damping channel
EAD for diﬀerent
choices of p. For p = 0 the amplitude damping is equal to

13

FIG. 5. PBT Simulation of the amplitude damping channel
EAD for various damping rates p. Minimization of the trace
distance C1(EAD, π) = (cid:107)χEAD −χπ(cid:107)1 between the target chan-
nel’s Choi matrix and its PBT simulation with program state
π, for diﬀerent number of ports N . We consider N = 1, 2, 3
and two kinds of programs: copies of the channel’s Choi ma-
trix χ⊗N
and the state ˜π1 obtained from the minimization
EAD
of C1 via the projected subgradient (PS) method after 200
iterations. Note that the simulation error C1 is maximum for
the identity channel (p = 0) and goes to zero for p → 1.

FIG. 6. PBT Simulation of the amplitude damping channel
EAD for various damping rates p. We plot the diamond dis-
tance cost function C(cid:5)(EAD, π) = (cid:107)EAD −EAD,π(cid:107)(cid:5) between the
target channel EAD and its PBT simulation EAD,π with pro-
gram state π. In particular, for the program state we compare
the naive choice of the channel’s Choi matrix π = χ⊗N
(dot-
EAD
ted lines) with the SDP minimization over the set of generic
Choi matrices π = χ⊗N (solid lines). Diﬀerent values of
N = 2, . . . , 6 and N = 20 are shown.

0
|

the identity channel, while for p = 1 it is a “reset” chan-
nel sending all states to
. We compare the simulation
(cid:105)
error with program states π either made by products of
N
the channel’s Choi matrix χ⊗
AD as in Eq. (84) or obtained
E
from the minimization of the trace distance cost func-
tion of Eq. (9) with the projected subgradient iteration
in Eq. (50). Alternative methods, like the conjugated
gradient algorithm, perform similarly for this channel.
We observe that, surprisingly, the optimal program ˜π1
obtained by minimizing the trace distance C1 is always
N
AD.
better than the natural choice χ⊗
E

In Fig. 6 we study the PBT simulation of the ampli-
tude damping channel by considering the subset of pro-
N which is made of tensor products
gram states π = χ⊗
of the 4
4 generic Choi matrices χ (therefore satisfy-
ing Tr2χ = 11/2). As discussed in previous Sec. VIII C,
this is equivalent to optimizing over the Choi set
CN and
it practically reduces to the convex optimization of the
channel ˜Λ over the generic single-copy Choi matrix χ.
Moreover, ˜Λ itself can be simpliﬁed, as shown in Ap-
pendix C, so the all operations depend polynomially on
the number N of ports. This allows us to numerically ex-
plore much larger values of N , even for the minimization
. In Fig. 6 the dotted lines correspond to the value
of C
(cid:5)
N
AD is employed, where
of C
when the program π = χ⊗
(cid:5)
E
χ
AD is the channel’s Choi matrix. As Fig. 6 shows, the
E
may be signiﬁcantly smaller with an optimal χ,
cost C
(cid:5)
thus showing that the optimal program may be diﬀerent
from the channel’s Choi matrix, especially when p is far
from the two boundaries p = 0 and p = 1.

×

As an another example, we consider the simulation of

the depolarizing channel deﬁned by

Edep(ρ) = (1

−

p)ρ +

p
d

11.

(98)

In Fig. 7 we study the performance of PBT simulation
of the depolarizing channel in terms of p. For p = 0
the depolarizing channel is equal to the identity chan-
nel, while for p = 1 it sends all states to the maximally
mixed state. Again we compare the simulation error with
program states either made copies of the channel’s Choi
N
matrices χ⊗
dep or obtained from the minimization of C1
E
with the conjugate gradient method of Eq. (57), which
performs signiﬁcantly better than the projected subgra-
dient for this channel. Also for the depolarizing channel
we observe that, for any ﬁnite N , we obtain a lower er-
ror by optimizing over the program states instead of the
N
dep .
naive choice χ⊗
E

Finally, in Fig. 8 we study the PBT simulation of a uni-
tary gate Uθ = eiθX for diﬀerent values of θ. Unlike the
previous non-unitary channels, in Fig. 8 we observe a ﬂat
error where diﬀerent unitaries have the same simulation
error of the identity channel θ = 0. This is expected be-
cause both the trace distance and the diamond distance
are invariant under unitary transformations. In general,
we have the following.

Proposition 5 Given a unitary
PBT simulation

U
Uπ with program π we may write

(ρ) = U ρU † and its

min
π ||U − Uπ||(cid:5)

= min

π ||I − Iπ||(cid:5)

,

(99)

0.00.20.40.60.8C1(EAD,π)0.00.20.40.60.81.0pChoiN=1ChoiN=2ChoiN=3PS200N=1PS200N=2PS200N=30.00.20.40.60.81.01.2C(cid:5)(EAD,χ⊗N)0.00.20.40.60.81.0pN=2N=3N=4N=5N=6N=2014

FIG. 8. PBT Simulation of the unitary gate Uθ = eiθX for
diﬀerent angles θ, where X is the bit-ﬂip Pauli matrix. Trace
distance C1(Uθ, π) = (cid:107)χUθ − χπ(cid:107)1 between the target Choi
matrix of the unitary and its PBT simulation with program
state π, for diﬀerent number of ports N . We consider N =
1, 2, 3 and two kinds of programs: copies of the Choi matrix
of the unitary χ⊗N
and the program state ˜π1 obtained from
Uθ
the minimization of C1 via the projected subgradient (PS)
method after 200 iterations.

FIG. 7. PBT Simulation of the qubit depolarizing chan-
nel versus probability of depolarizing p. Trace distance
C1(Edep, π) = (cid:107)χEdep − χπ(cid:107)1 between the target channel’s
Choi matrix and its PBT simulation with program state π,
for diﬀerent number of ports N . We consider N = 1, 2, 3
and two kinds of programs: copies of the channel’s Choi ma-
trix π = χ⊗N
and the optimal program state ˜π1 obtained
Edep
from the minimization of C1 via the conjugate gradient (CG)
method after 200 iterations. Note that the simulation error
C1 is maximum for the identity channel (p = 0) and even-
tually goes to zero for a ﬁnite value of p that decreases for
increasing N .

where

Iπ is the PBT simulation of the identity channel.

Proof. In fact, we simultaneously prove

min
π ||I − Iπ||(cid:5)

(1)

≤

min
π ||U − Uπ||(cid:5)

(2)

≤

min
π ||I − Iπ||(cid:5)

,

(100)
1

1

1

−

−

−

−

−

I

U

=

=

1)⊗

Uπ||(cid:5)

Uπ||(cid:5)
(

where (1) comes from the fact that
and

||I − U
U
PBT simulation of the identity
N (π) once

||U −Uπ||(cid:5)
||U
U −
1
Uπ is a possible
−
U
with program state
1 is swapped with the ﬁlter-
I ⊗
ing of the ports; then (2) comes from the fact that the
composition
U ◦ Iπ is a possible simulation of the uni-
N (π) and we have the
tary
U
inequality

. (cid:4)
for diﬀerent values of N is
plotted in Fig. 9 where numerical values are obtained
from SDP, while the upper bound is given by Eq. (83).

||U ◦ I − U ◦ Iπ||(cid:5) ≤ ||I − Iπ||(cid:5)

with program state

The scaling of

||I − Iπ||(cid:5)

I ⊗ U

U

⊗

FIG. 9. PBT Simulation of the identity channel for diﬀerent
number of ports N . For the identity channel the optimal
Choi matrix coincides with the channel’s Choi matrix χI.
The optimal π has been obtained by minimising C(cid:5) via SDP.
The upper bound corresponds to Eq. (83).

IX. PARAMETRIC QUANTUM CIRCUITS

A. Basic idea

We now study another design of universal quantum
processor that can simulate any target quantum channel
in the asymptotic limit of an arbitrarily large program
state. This is based on a suitable reformulation of the
PQCs, which are known to simulate any quantum com-
putation with a limited set of quantum gates [20, 50].

A PQC is composed of a sequence of unitary matrices
Uj(θj), each depending on a classical parameter θ. The
resulting unitary operation is then

U (θ) = UN (θN ) . . . U2(θ2)U1(θ1).

(101)

0.00.20.40.60.8C1(EDepo,π)0.00.20.40.60.81.0pChoiN=1ChoiN=2ChoiN=3CG200N=1CG200N=2CG200N=30.00.20.40.60.8C1(Uθ,π)0π2π3π22πθChoiN=1ChoiN=2ChoiN=3PS200N=1PS200N=2PS200N=30.00.40.81.21.62.0C(cid:5)(I,π)48121620Nπ=χ⊗NIbound2d(d−1)N−1optimalπ15

where U (θ) is deﬁned in Eq. (101). The parametric quan-
tum processor Qπ in Eq. (103) is capable of simulating
any parametric quantum channels, but it is more gen-
eral, as it allows entangled quantum parameters and also
parameters in quantum superposition.

An equivalent measurement-based protocol is obtained
by performing the trace in Eq. (109) over the basis
θ1, . . . , θN (cid:105)
|
Qπ(θ) =

U (θ)ρU (θ)†

, so that

(cid:88)

,

π
θ1, . . . , θN |
(cid:104)

θ1, . . . , θN (cid:105)

|

(105)
where U (θ) is deﬁned in Eq. (101). In this alternative,
yet equivalent formulation, at a certain iteration j, the
processor measures the qubit register Rj. Depending
on the measurement outcome θj, the processor then ap-
plies a diﬀerent unitary U (θj) on the system. However,
in this formulation the program state
is destroyed
after each channel use. From Eq. (105) we note that
Qπ depends on π only via the probability distribution
. As such any advantage in us-
π
θ1, . . . , θN |
(cid:104)
ing quantum states can only come from the capability of
quantum systems to model computationally hard proba-
bility distributions [52].

θ1, . . . , θN (cid:105)
|

π

(cid:105)

|

θj

{

}

C. Universal channel simulation via PQCs

The universality of PQCs can be employed for univer-
sal channel simulation. Indeed, thanks to Stinespring’s
dilation theorem, any channel can be written as a unitary
evolution on a bigger space, where the system is paired
to an extra register R0

(ρA) = TrR0 [U (ρA ⊗

E

θ0)U †],

(106)

where θ0 belongs to R0, and U acts on system A and
register R0. In Ref. [50] it was shown that two quantum
gates are universal for quantum computation. Speciﬁ-
cally, given U0 = eit0H0 and UB = eit1H1 for ﬁxed times
ti and Hamiltonians Hj, it is possible to write any uni-
tary as

FIG. 10. Convex reformulation of a PQC as a coherent pro-
grammable quantum processor that applies a sequence of con-
ditional gates as in Eq. (102) depending on the program state
|π(cid:105) = |θ1, . . . , θN (cid:105). The program state is not destroyed and
can be reused.

A convenient choice is via Uj(θj) = exp(iθjHj), where
each elementary gate corresponds to a Schr¨odinger evo-
lution with Hamiltonian Hj for a certain time inter-
val θj. For certain choices of Hj and suitably large N
the above circuit is universal [20], namely any unitary
can be obtained with U (θ) and a suitable choice of θj.
The optimal parameters can be found with numerical
algorithms [51], e.g. by minimizing the cost function
C(θ) =
. However, the above cost func-
Tr[U †targetU (θ)]
|
tion is not convex, so the numerical algorithms are not
guaranteed to converge to the global optimum.

|

As a ﬁrst step, we show that the task of learning the
optimal parameters in a PQC can be transformed into
a convex optimization problem by using a quantum pro-
gram. This allows us to use SDP and gradient-based ML
methods for ﬁnding the global optimum solution.

B. Convex reformulation

Consider a program state

composed
θ1, . . . , θN (cid:105)
by N registers Rj, each in a separable state
. We
θj(cid:105)
|
can transform the classical parameters in Eq. (101) into
quantum parameters via the conditional gates

π
|

=

(cid:105)

|

ˆUj = exp


iHj ⊗

(cid:88)

θj



that acts non-trivially on system and register Rj. If the
parameters θj are continuous, then we can replace the
sum with an integral. With the above gates we deﬁne
the parametric quantum channel

θj|

θj(cid:105)(cid:104)

 ,

θj|

(102)

U

≈ · · ·

1 U m3
U m4

0 U m2

1 U m1

0

,

(107)

for some integers mj. Under suitable conditions, it was
shown that with M = (cid:80)
d) it is possible
j mj =
to approximate any unitary U with a precision (cid:15). More
precisely, the conditions are the following

(d2(cid:15)−

O

Qπ(ρ) = TrR





N
(cid:89)

j=1

ˆUj (ρ

π)

⊗



ˆUj

†

 ,

N
(cid:89)

j=1

(103)

i) The Hamiltonians H0 and H1 are generators of the
full Lie algebra, namely H0, H1 and their repeated
commutators generate all the elements of su(d).

ψ
whose action on a generic state
(cid:105)
|
For a pure separable program
=
π
(cid:105)
|
tain the standard result, i.e.,

is shown in Fig. 10.
, we ob-
θ1, . . . , θN (cid:105)
|

Q
θ1,...,θN
|

(cid:105)

(ρ) = U (θ)ρU (θ)†,

(104)

ii) The eigenvalues of U0 and U1 have phases that are

irrationally related to π.

The decomposition in Eq. (107) is a particular case of
Eq. (101) where θj can only take binary values θj = 0, 1.

|ψ(cid:105)U(θ1)U(θ2)U(θ3)U(θ4)U(θ5)|θ1(cid:105)•|θ2(cid:105)•|θ3(cid:105)•|θ4(cid:105)•|θ5(cid:105)•16

following Theorem 2. When we are interested in simu-
lating a unitary channel U via the quantum ﬁdelity, then
following the results of Section VI, the corresponding op-
timal program ˜πF is simply the eigenvector Λ∗[
]
χU |
χU (cid:105)(cid:104)
|
with maximum eigenvalue, where
. Note
Φ
U
(cid:105)
|
χU |
χU (cid:105)(cid:104)
also that Λ∗[
|
χU |BA ⊗
Z = (
(cid:104)

χU (cid:105)
] = Z †Z where

11R) ˆUAR (
|

ΦBA(cid:105) ⊗

(114)

11R) ,

= 11

⊗

|

so the optimal program ˜πF is the principal component of
Z. Since there are quantum algorithms for principal com-
ponent analysis [53], the optimization may be eﬃciently
performed on a quantum computer.

D. Numerical examples

As an example we study the simulation of an amplitude
damping channel, with Kraus operators in Eq. (97). A
possible Stinespring dilation for this channel is obtained
with

and

=

θ0(cid:105)
|

U =

0
|


(cid:105)
1
0
0 √1


0
0

−

0
p √p

−
√p √1
−
0
0



 = eiHAD,

0
0
p 0
1

(115)

FIG. 11. Simulation of a quantum channel via Stinespring
decomposition together with unitary simulation as in Fig. 10.

As such we can write the conditional gates of Eq. (102)
as

0
(cid:105)j j(cid:104)

+ it1H1 ⊗ |

ˆUj = exp (it0H0 ⊗ |

0
|
for some times tj. Channel simulation is then obtained
by replacing the unitary evolution U of Eq. (106) with
the approximate form in Eq. (107) and its simulation
in Eq. (109). The result is illustrated in Fig. 11 and
described by the following channel

1
(cid:105)j j(cid:104)

) ,
1
|

(108)

Qπ(ρ) = TrR





N
(cid:89)

j=1

ˆUj A,R0,Rj

(ρA ⊗

π)



ˆUj

†
A,R0,Rj

 ,

N
(cid:89)

j=1

(109)
where the program state π is deﬁned over R =
(R0, . . . , RN ) and each ˆHj acts on the input system A
and two ancillary qubits R0 and Rj. The decomposition
of Eq. (107) assures that, with the program

π
|

(cid:105)

=

θ0(cid:105) ⊗ · · · ⊗ |
|

1
(cid:105)

m2

⊗

0

⊗
(cid:105)

⊗ |

m1 ,

(110)

the product of unitaries approximates U in Eq. (106) with
precision (cid:15). This is possible in general, provided that the
d). However, the
program state has dimension
channel (109) is more general, as it allows both quantum
superposition and entanglement.

(d2(cid:15)−

O

The processor map Λ is then simply obtained as

Λ(π) = TrR

(cid:104)

ˆUAR (ΦBA ⊗

πR) ˆU †AR

(cid:105)

,

(111)

where

ˆUAR = 11B ⊗

N
(cid:89)

j=1

ˆUj A,R0,Rj

,

(112)

while the (non-trace-preserving) dual channel may be
written as

Λ∗(X) =

ΦBA|

ˆU †AR (XBA ⊗

(cid:104)

11R) ˆUAR

ΦBA(cid:105)
|

.

(113)

This channel requires 2N quantum gates at each itera-
tion and can be employed for the calculation of gradients,

where the Hamiltonian is given by

HAD =

arcsin(√p)
2

(Y

X

X

−

⊗

⊗

Y ),

(116)

with X and Y being Pauli operators. We may construct
a PQC simulation by taking

U0 = eiα(Y

X

X

Y ),

⊗

−

⊗

(117)

for some α and taking U1 to be a diﬀerent unitary that
makes the pair U0, U1 universal. Here we may choose
α = √2 and U1 = eiH1 with

H1 = (√2Z + √3Y + √5X)

(Y + √2Z).

(118)

⊗

Results are shown in Fig. 12. Compared with the sim-
ilar PBT simulation of Fig. 5, we observe that PQC sim-
ulation displays a non-monotonic behavior as a function
of N . PBT with N pairs requires a register of 2N qubits,
while PQC requires N + 1 qubits, namely N qubits from
the conditional gates and an extra one coming from Stine-
spring decomposition (see Fig. 11). We observe that,
with a comparable yet ﬁnite register size, PQC can out-
perform PBT in simulating the amplitude damping chan-
nel. In Fig. 13 we also study the PQC simulation of the
depolarizing channel for diﬀerent values of p. Although
the gates U0 and U1 were chosen with inspiration from
the Stinespring decomposition of the amplitude damping
channel, those gates are universal and capable of sim-
ulating other channels.
Indeed, we observe in Fig. 13
that a depolarizing channel is already well simulated with
N = 4 for all values of p.

|ψiUθ1Uθ2Uθ3Uθ4|θ0i|θ1i•|θ2i•|θ3i•|θ4i•17

optimization problem that can always be solved.

In particular, by minimizing the diamond distance via
SDP, we can always determine the optimal program state
for the simulation of an arbitrary channel. Alternatively,
we may minimize the simpler but larger cost functions in
terms of trace distance and quantum ﬁdelity via gradient-
based ML methods, so as to provide a very good ap-
proximation of the optimal program state. This other
approach can also provide closed analytical solutions, as
is the case for the simulation of arbitrary unitaries, for
which the minimization of the ﬁdelity cost function cor-
responds to compute an eigenvector.

We have then applied our results to various de-
signs of programmable quantum processor,
from a
to deeper and
shallow teleportation-based scheme
asymptotically-universal designs that are based on PBT
and PQCs. We have explicitly benchmarked the per-
formances of these quantum processors by considering
the simulation of unitary gates, depolarizing and ampli-
tude damping channels, showing that the optimal pro-
gram states may diﬀer from the naive choice based on
the Choi matrix of the target channel.

An immediate application of our work may be the de-
velopment of a model of “programmable” blind quantum
computation, where a client has an input state to be
processed by a quantum server which is equipped with a
programmable quantum processor. The client classically
informs the server about what type of computation it
needs (e.g., some speciﬁed quantum algorithm) and the
server generates an optimal program state which closely
approximates the overall quantum channel to be applied
to the input. The server then accepts the input from
the client, processes it, and returns the output together
with the value of a cost function quantifying how close
the computation was with respect to the client’s request.

Our results may also be useful in areas beyond quan-
tum computing, wherever channel simulation is a basic
problem. For instance this is the case of quantum com-
munication, for the derivation of quantum and private
communication capacities, and quantum metrology and
hypothesis testing, for the simpliﬁcation of adaptive pro-
tocols and the analysis of the ultimate discrimination and
estimation performance with quantum channels.

FIG. 12. PQC simulation of the amplitude damping channel.
Trace distance C1(EAD, π) = (cid:107)χEAD −χπ(cid:107)1 between the target
channel’s Choi matrix and its PQC simulation with program
state π, for diﬀerent numbers of register qubits N . The opti-
mal program is obtained from the minimization of C1 via the
projected subgradient (PS) method after 200 iterations.

FIG. 13. PQC simulation of the depolarizing channel. Trace
distance C1(EDep, π) = (cid:107)χEDep − χπ(cid:107)1 between the target
channel’s Choi matrix and its UPQC simulation with pro-
gram state π, for diﬀerent numbers of register qubits N . The
optimal program is obtained from the minimization of C1 via
the projected subgradient (PS) method after 200 iterations.

X. CONCLUSIONS

In this work we have considered a general, ﬁnite-
dimensional, model of programmable quantum processor,
which is a fundamental scheme for quantum computing
and also a primitive tool for other areas of quantum in-
formation. By introducing suitable cost functions, based
on the diamond distance, trace distance and quantum
ﬁdelity, we have shown how to characterize the optimal
performance of this processor in the simulation of an ar-
bitrary quantum gate or channel. In fact, we have shown
that the minimization of these cost functions is a convex

Acknowledgements.
L.B. acknowledges support by
the program “Rita Levi Montalcini” for your young
researchers.
S.P. acknowledges support by the EP-
SRC via the ‘UK Quantum Communications Hub’
(EP/M013472/1) and the European Union via the
project ‘Continuous Variable Quantum Communications’
(CiViQ, no 820466). S.P. would like to thank George
Zweig, Jacques Carolan, John Watrous, and Dirk En-
glund for discussions and feedback.

0.00.20.40.60.8C1(EAD,π)0.00.20.40.60.81.0pPS200N=1PS200N=2PS200N=3PS200N=4PS200N=5PS200N=60.00.20.40.60.8C1(EDep,π)0.00.20.40.60.81.0pPS200N=1PS200N=2PS200N=3PS200N=4PS200N=5PS200N=6Appendix A: Matrix calculus

3. Diﬀerential of the trace distance

1. Matrix diﬀerentiation

The trace norm for a Hermitian operator X is deﬁned

18

For a general overview of these techniques, the reader
may consult Ref. [54]. Thanks to Cauchy’s theorem, a
matrix function can be written as

f (A) =

1
2πi

(cid:90)

Γ

dλ f (λ)(λ11

A)−

1 .

−

(A1)

For the same reason

f (cid:48)(A) =

1
2πi

(cid:90)

Γ

dλ f (λ)(λ11

A)−

2 .

−

(A2)

Applying a basic rule of matrix diﬀerentiation, d(A−

1) =

A−

1(dA)A−

1 we obtain

−

df (A) =

1
2πi

(cid:90)

Γ

dλ f (λ)(λ11

−

A)−

1dA(λ11

−

A)−

1 . (A3)

Clearly, df (A) = f (cid:48)(A)dA only when [A, dA] = 0.
In
general df (A) is a superoperator that depends on A and
is applied to dA. The explicit form is easily computed
using the eigenvalue decomposition or other techniques
[54]. Note that in some cases the expressions are simple.
Indeed, using the cyclic invariance of the trace, we have

dTr[f (A)] = Tr[f (cid:48)(A)dA],

(A4)

while in general dTr[Bf (A)]

=Tr[Bf (cid:48)(A)dA].

2. Diﬀerential of the quantum ﬁdelity

The quantum ﬁdelity can be expanded as

as

t(X) =

=

(cid:107)1 := Tr√X †X = Tr[√X 2]
(cid:90)
1] ,
XX)−

dλ √λTr[(λ11

X
(cid:107)
1
2πi

Γ

−

(A7)

j |

λj|

where in the second line we applied Eq. (A1). From
the spectral decomposition X = U λU † we ﬁnd t(X) =
(cid:80)
, so the trace distance reduces to the absolute
value function for one-dimensional Hilbert spaces. The
is diﬀerentiable at every
absolute value function
λ
|
= 0 the
points, except λ = 0. Therefore, for any λ
subgradient of the absolute value function is made by its
gradient, namely

|

∂

λ
|

|

=

sign(λ)
}
{

for λ

= 0 .

(A8)

For λ = 0 we can use the deﬁnition (34) to write

z :
{

σ
|

| ≥

zσ for all σ

,

}

(A9)

∂

|

|λ=0 =
λ
1

which is true iﬀ

z

1. Therefore,

|

∂

−

≤

1, 1] .

≤
|λ=0 = [
λ
−
The sign function in (A8) can be extended to λ = 0 in
multiple ways (common choices are sign(0) =
1, 0, 1).
From the above equation, it appears that for any exten-
sion of the sign function, provided that sign(0)
1, 1]
we may write the general form

(A10)

[
−

−

∈

sign(λ)

,

∂

λ
|

|

∈

(A11)

F (X, Y ) = Tr
1
2πi

=

(cid:113)

√XY √X
(cid:90)

dλ √λTr[(λ11

Γ

(A5)

√XY √X)−

1] ,

−

which is true for any value of λ.

With the same spirit we extend the above argument to
any matrix dimension, starting from the case where X is
an invertible operator (no zero eigenvalues). Taking the
diﬀerential with respect to X we ﬁnd

where in the second line we have applied Eq. (A1). Tak-
ing the diﬀerential with respect to Y and using the cyclic
property of the trace we get

dt(X) := t(X + dX)

t(X) =

−
dλ √λTr[(λ11

F (X, Y )

−

dλ √λTr[(λ11

√XY √X)−

2√XdY √X]

−

Tr[(√XY √X)−

1

2 √XdY √X]

(cid:90)

(1)
=

dY F := F (X, Y + dY )
1
2πi
1
2
1
2

(2)
=

(3)
=

Γ

Tr[√X(√XY √X)−

1

2 √X dY ] ,

(A6)

where in (1) we use Eq. (A3) and the cyclic property of
the trace; in (2) we use Eq. (A2) with f (λ) = √λ, so
f (cid:48)(λ) = 1
1/2; and in (3) we use the cyclic property of
the trace. See also Lemma 11 in [35].

2 λ−

(cid:90)

Γ

(1)
=

(2)
=

1
2πi
1
2

X 2)−

2(X(dX) + (dX)X)]

−

Tr[(X 2)−

1
2 (X(dX) + (dX)X)]

(3)
= Tr[(X 2)−

1
2 X (dX)]

(A12)

where in (1) we use Eq. (A3), the cyclic property of the
trace and the identity dX 2 = X(dX) + (dX)X; in (2)
we use Eq. (A2) with f (λ) = √λ, so f (cid:48)(λ) = 1
1/2;
and in (3) we use the cyclic property of the trace and the
commutation of X and √X 2. Let

2 λ−

X =

(cid:88)

k

λkPk ,

(A13)

(cid:54)
(cid:54)
(cid:54)
be the eigenvalue decomposition of X with eigenvalues
λk and eigenprojectors Pk. For non-zero eigenvalues we
may write

(X 2)−

1
2 X =

(cid:88)

k

and accordingly

sign(λk)Pk =: sign(X) ,

(A14)

dt(X) :=

X + dX
(cid:107)
(cid:88)

(cid:107)1 − (cid:107)
sign(λk)Tr[Pk dX] .

(cid:107)1

X

=

(A15)

k

Therefore, for invertible operators we may write

∂t(X) =

t(X)

,

∇

}

{∇

t(X) = sign(X) .

We now consider the general case where some eigenvalues
of X may be zero. We do this by generalizing Eq. (A11),
namely we show that even if ∂t(X) may contain multiple
∂t, provided that
elements, it is always true that
∈
11. Following (34) we may write, for

sign(X)

∇

11

t

−
ﬁxed X and arbitrary Y ,

≤

≤

Tr[

t(X)(Y

X)]

∇
Tr[

−
t(X)Y ] + t(X)

t(Y )

t(X)

−

−

(1)
= t(Y )

(2)

≥

t(Y )

−

t(X)

−

Tr[Y ] =

−
(cid:88)

∇
λj| −

(
|

j

λj)

0 ,

≥

(A16)

where in (1) we use the property
X
(cid:107)1 = Tr[sign(X)X]
(cid:107)
and in (2) we use the assumption
11.
sign(X)
11
−
From the deﬁnition of the subgradient (34), the above
equation shows that sign(X)
∂t(X), so we may always
∈
use
t(X) = sign(X) in the projected subgradient algo-
∇
rithm (50).

≤

≤

Appendix B: Smoothing techniques

1. Stochastic smoothing

The conjugate gradient algorithm converges after
(c/(cid:15)) steps [16, 17], where (cid:15) is the desired precision and
O
c is a curvature constant that depends on the function.
However, it is known that c could diverge for non-smooth
functions. This is the case for the trace norm, as shown
in Example 0.1 in [55].

A general solution, valid for arbitrary functions, is via
In this approach the non-

stochastic smoothing [56].
smooth function C(π) is replaced by the average

19

so that Cη(π) provides a good approximation for C(π).
Moreover, Cη is diﬀerentiable at any point, so we may
apply the conjugate gradient algorithm. A modiﬁed con-
jugate gradient algorithm with adaptive stochastic ap-
proximation was presented in Ref. [57], At each iteration
k the algorithm reads

k−

(cid:80)k

1/2,

k+2 |

of ¯gk,

j=1 g(πk + ηkσj) for ηk ∝
σk(cid:105)
|
.

1) Sample some operators σ1, . . . , σk,
2) Evaluate ¯gk = 1
k
3) Find the smallest eigenvalue
k+2 πk + 2
4) πk+1 = k
σk|
σk(cid:105) (cid:104)
where g denotes any element of the subgradient ∂C. The
((cid:15)2) iterations. Since
above algorithm converges after
Eqs. (40) and (38) provide an element of the subgradi-
ent, the above algorithm can be applied to both ﬁdelity
and trace distance. However, this algorithm requires k
evaluation of the subgradient to perform the averages, so
it may be impractical when the number of iterations get
larger. In the following we study an alternative that does
not require any average.

O

2. Nesterov’s smoothing

An alternative smoothing scheme is based on Nes-
terov’s dual formulation [38]. Suppose that the non-
smooth objective function f admits a dual representation
as follows

[
f (x) = sup

y

x, y
(cid:104)

(cid:105) −

g(y)],

(B3)

for some inner product
. Nesterov’s approximation
·(cid:105)
consists in adding a strongly convex function d to the
dual

,
(cid:104)·

fµ(x) =

(cid:88)
[

y

x, y
(cid:104)

(cid:105) −

g(y)

−

µd(y)].

(B4)

The resulting µ-approximation is smooth and satisﬁes

fµ(x)

f (x)

≤

≤

fµ(x) + µ sup

y

d(y).

(B5)

The trace norm admits the dual representation [25]
(cid:107)1 = sup

t(X) =

Y, X

,
(cid:105)

X

(cid:107)

1(cid:104)

Y

∞

(cid:107)

(cid:107)

Y, X

≤
is the Hilbert Schmidt product. This can
where
be regularized with any strongly convex function d. A
convenient choice [19] that enables an analytic solution
is via d(X) = 1

X, X

so

X

(cid:105)

(cid:104)

(B6)

2 (cid:107)

2 := 1
2
2 (cid:104)
(cid:107)
(cid:104)

(cid:105)

tµ(X) = max
≤

∞

Y

(cid:107)

(cid:107)

1

Y, X
(cid:104)

(cid:105) −

µ
2 (cid:107)

Y

2
2
(cid:107)

(cid:105)

.

(B7)

Cη(π) = Eσ[C(π + ησ)] .

(B1)

This function is smooth and its gradient is given by [19]

where σ is such that
M

, then

y

x
(cid:107)

−

(cid:107)∞

σ

(cid:107)

(cid:107)∞ ≤

1.

If

C(x)
|

−

C(y)

| ≤

∇

C(π)

≤

Cη(π)

≤

C(π) + M η ,

(B2)

tµ(X) = argmax
1
≤
(cid:107)
= argmin
Y

∞

Y

(cid:107)

∞

(cid:107)

(cid:107)

≤

(cid:104)

Y, X
(cid:104)

(cid:105) −

(cid:105)

µ
2 (cid:107)

Y

2
2
(cid:107)

µY

1 (cid:107)

X

2
2 = U ΣµV †,
(cid:107)

−

where X = U ΣV † is the singular value decomposition
of X and Σµ is a diagonal matrix with diagonal entries
. Plugging this into Eq. (B7) we
Σi/µ, 1
(Σµ)i = min
}
get

{

tµ(X) = Tr

(cid:104)

Σµ

(cid:16)

Σ

(cid:17)(cid:105)

.

Σµ

µ
2

−

(B8)

For a diagonalizable matrix X with spectral decompo-
sition X = U λU †, the singular value decomposition is
obtained with Σ =
Inserting
these expressions in (B8) we ﬁnd

and V = U sign(λ).

λ
|

|

tµ(X) =

(cid:88)

j

hµ(λj) = Tr[hµ(X)],

(B9)

20

Theorem 7 The gradient of the smooth trace norm is
Lipschitz continuous with Lipschitz constant

L =

d
µ

.

(B16)

In particular, being the gradient Lipschitz continuous,
the smooth trace norm satisﬁes the following inequality
for any state π, σ

Cµ(σ)

≤

Cµ(π) +

Cµ(π), σ

(cid:104)∇

π

(cid:105)

−

+

L
2 (cid:107)

σ

π

2
2. (B17)
(cid:107)

−

Proof. Given the linearity of the quantum channel Λ, we
can apply theorem 1 from [38] to ﬁnd

hµ(x) =

where hµ is the so called Huber penalty function
(cid:40) x2
2µ
x
|
tµ is then h(cid:48)µ(X)
(cid:40) x
µ
sign(x)

x
|
x
|
U h(cid:48)(λ)U †, where

The gradient

< µ,
µ.

< µ,
µ.

h(cid:48)µ(x) =

|
| ≥

if
if

if
if

| −

∇

≡

µ
2

x
|
x
|

|
| ≥

(B10)

(B11)

We ﬁnd then that via the smooth trace norm tµ we
can deﬁne the smooth trace distance of Eq. (59) that is
diﬀerentiable at every point

Cµ(π) = Tr [hµ (χπ −

χ
E

)] .

(B12)

Thanks to the inequalities in (B5), the smooth trace dis-
tance bounds the cost C1 as

Cµ(π)

C1(π)

≤

≤

Cµ(π) +

µd
2

,

(B13)

where we employed the identity sup
get the upper bound. Moreover, we ﬁnd the following

Y
1 (cid:107)

(cid:107)

≤

∞

Y

(cid:107)

(cid:107)

2
2 ≤

d to

Lemma 6 The
Eq. (59), is a convex function of π.

smooth trace distance,

deﬁned in

Proof. From the deﬁnition and Eq. (B7) we ﬁnd

Cµ(π) = tµ [Λ(π)
−
(cid:104)
(cid:104)

= max
Y
∞
≤

(cid:107)

(cid:107)

1

χ

]

E
Y, Λ(π)

χ

−

E (cid:105) −

µ
2 (cid:107)

Y

2
2
(cid:107)

(cid:105)

.

(B14)

p)π2 linearity implies f (¯π) :=
p)f (π2). Therefore

= pf (π1) + (1

−
(cid:104)
pf (π1) + (1

−

1

Y

∞

E (cid:105)

Now for ¯π = pπ1 + (1
χ
Y, Λ(¯π)
(cid:104)

−
Cµ(¯π) = max
(cid:107)
(cid:107)
≤
p max
Y
∞
(cid:107)
≤
p) max
Z
∞
≤
(cid:107)
= pCµ(π1) + (1
showing the convexity. (cid:4)

(cid:107)
+ (1

≤

−

(cid:104)

(cid:107)

1

Y, Λ(π1)
(cid:104)

Z, Λ(π2)

(cid:104)
(cid:104)
p)Cµ(π2),

1

−

p)f (π2)

−

−

χ

E (cid:105) −

−

µ
2 (cid:107)

Y

Y

(cid:105)

2
2
(cid:107)

µ
2 (cid:107)
(cid:105)

2
2
(cid:107)
µ
2 (cid:107)

χ

−

E (cid:105) −

Z

Then, using the deﬁnitions from [38], the following the-

orem bounds on the growth of the gradient

L =

1
µ

x

(cid:107)

(cid:107)

sup
y

2=1,

(cid:107)

y, Λ(x)
(cid:105)

.

2=1(cid:104)

(cid:107)

(B18)

Since all eigenvalues of y are smaller or equal to 1, we
can write y

1 and as such

≤

L

≤

1
µ

(cid:107)

sup
x

2=1

(cid:107)

Tr[Λ(x)] =

1
µ

(cid:107)

sup
x

2=1

(cid:107)

Tr[x]

d
µ

.

≤

(B19)

(cid:4)

Appendix C: PBT reduced channel

Here we provide an explicit expression for the reduced
map ˜Λ of Eq. (92) in the case of qubits. For d = 2 we
can rewrite PBT in a language that can be more easily
formulated from representations of SU(2). For simplicity
of notation, here we do not use bold letters for vectorial
quantities.

Let us modify the POVM in Eq. (78) as

1/2

˜Πi = σ−
AC Ψ−AiCσ−
N
(cid:88)

1/2
AC ,

σAC =

Ψ−AiC,

i=1
Πi = ˜Πi + ∆,

∆ =



11

1
N

(cid:88)

−

j



˜Πj

 ,

(C1)

(C2)

(C3)

(C4)

)/√2 is a singlet state. For
Ψ−
(cid:105)
n the quantum channel is simpliﬁed.
In fact,

where
|
π = χ⊗
since TrB χ = 11/2, we may write

= (
|

(cid:105) − |

01

10

(cid:105)

Pπ =

=

N
(cid:88)

i=1
(cid:88)

(cid:96)

(cid:105)

2
2
(cid:107)
(B15)

1
2N

1 TrAC
−

(cid:104)(cid:112)

Πi

K 0
(cid:96) (ρC ⊗

χ)K 0

(cid:96) † +

(cid:0)ρC ⊗
χAiB ⊗
K 1
(cid:96) (ρC ⊗

(cid:88)

(cid:96)(cid:48)

(cid:1) (cid:112)

(cid:105)

Πi

11 ¯Ai

χ)K 1

(cid:96) †, (C5)

where (cid:96) and (cid:96)(cid:48) are multi-indices and,
in deﬁning the
Kraus operators, we have separated the contributions
from ˜Πi and ∆ (see below).

In order to express these operators, we write

are Clebsch-Gordan coeﬃcients.

21

ψ−CAi(cid:105)(cid:104)
|

ψ−CAi|

=

11

−

(cid:126)σC ·
4

(cid:126)σAi

,

(C6)

so that

σAC =

N
(cid:88)

i=1

ψ−CAi(cid:105)(cid:104)
|
(cid:126)S2
tot −

ψ−CAi|
(cid:126)S2
C −
2

=

N
4 −

=

N
4 −

(cid:126)SC ·

(cid:126)SA

(cid:126)S2
A

,

(C7)

where (cid:126)S = (cid:126)σ/2 is a vector of spin operators, (cid:126)SA =
(cid:80)
(cid:126)SAj and (cid:126)Stot = (cid:126)SC + (cid:126)SA. The eigenvalues of σAC
j
are then obtained from the eigenvalues of the three com-
muting Casimir operators

λ(sA) =

N
4 −

Stot(Stot + 1)

−

sA(sA + 1)
2

−

3/4

, (C8)

where Stot = sA ±
of eigenvalues

1/2.

Substituting the deﬁnition of Stot, we ﬁnd two classes

λ+(sA) =

2sA

N

−
4

, λ−(sA) =

N + 2sA + 2
4

,

(C9)

with corresponding eigenvectors

, sA, M, α

|±

=

(cid:105)

(cid:88)

k,m

ΓM,m,k
sA
±

1

2 ,sA|

sA, m, α

k

(cid:105)C|

(cid:105)A ,

(C10)

N +1

2 , α = 1, . . . , g[N ](s) describes
where
the degeneracy, g[N ](s) is the size of the degenerate sub-
space, and

2 ≤

M

−

≤

N +1

ΓM,m,k
S,s

=

1/2, 1/2
S, M ; s, 1/2
|
(cid:104)

−

k; s, m
(cid:105)

(C11)

Note that

the Clebsch-Gordan coeﬃcients deﬁne
two bases
. From the orthogonal-

a unitary transformation between the
S, M ; s1, s2(cid:105)
s1, m1; s2; m2(cid:105)
|
|
ity relations of these coeﬃcients we ﬁnd the equalities

and

(cid:88)

S,s ΓM,m(cid:48),i(cid:48)
ΓM,m,i

S,s

= δi,i(cid:48)δm,m(cid:48),

S,M
(cid:88)

m,i

S,s ΓM (cid:48),m,i
ΓM,m,i

S(cid:48),s = δM,M (cid:48)δ(S, S(cid:48), s),

(C12)

(C13)

where δ(S, S(cid:48), s) = 1 iﬀ S = S(cid:48) and
s +
1/2. The eigenvalues in Eq. (C9) are zero iﬀ Stot = SA +
1/2 and SA = N/2. These eigenvalues have degeneracy
2Stot + 1 = N + 2 and the corresponding eigenvectors are

1/2

| ≤

s
|

−

≤

S

, M, α

| ⊥

=

+, N/2, M, α
|

(cid:105)

.

(cid:105)

(C14)

Thus, the operator ∆ from Eq. (C4) may be written as

∆ =

1
N

N +1

2(cid:88)

(cid:88)

M =

−

N +1
2

α

, M, α

| ⊥

, M, α

.

|

(cid:105)(cid:104)⊥

(C15)

To ﬁnish the calculation we need to perform the partial
trace over all spins except those in port i. We use s ¯Ai,
m ¯Ai and αi to model the state of the total spin in ports
Aj with j
= i. These refer to the value of total spin and
the projection along the z axis, as well as the degeneracy.
Moreover, since S ¯Ai commutes with both S2
A and Sz
A,
we may select a basis for the degeneracy that explicitly
contains s ¯Ai. We may write then α = (s ¯Ai, ˜αi) where ˜αi
represents some other degrees of freedom.

With the above deﬁnitions, when we insert several res-
olutions of the identity in Eq. (C5), we may write the
Kraus operators as

K 0

i,s ¯Ai

,m ¯Ai

,αi,s(cid:48)

¯Ai

,m(cid:48)

¯Ai

,α(cid:48)
i

= 2−

N −1
2

= 2−

N −1
2

s ¯Ai , m ¯Ai, αi| ⊗ (cid:104)
(cid:104)
(cid:88)
(sA)−
λ

,sA,M,α

±

ψ−AiC|

1/2

(cid:104)

1/2
σ−
AC |
ψ−AiC|(cid:104)

s(cid:48)¯Ai

, m(cid:48)¯Ai

, α(cid:48)i(cid:105)
s ¯Ai, m ¯Ai, αi|±

, sA, M, α

K 1

i,M,α,s(cid:48)

,m(cid:48)

¯Ai

,α(cid:48)
i

= 2−

¯Ai

1/2

+, N/2, M, α
(cid:104)

|

s(cid:48)¯Ai

, m(cid:48)¯Ai

,
, α(cid:48)i(cid:105)

N −1

±
2 N −

, sA, M, α

s(cid:48)¯Ai
|

, m(cid:48)¯Ai

,
, α(cid:48)i(cid:105)

(cid:105)(cid:104)±

(C16)

where each set of states
of the space corresponding to all ports j with j

s ¯Ai, m ¯Ai, αi(cid:105)
|

represent a basis
= i. To

simplify the Kraus operators we study the overlap

s¯ı, m¯ı, αi|±
(cid:104)
(cid:88)
k
=
(cid:105)C(cid:104)
|

, S, M, α
(cid:105)
s¯ı, m¯ı, αi|

ΓM,m,k
2 ,S|
S
±

1

S, m, α

k,m
(cid:88)

|

k,m
(cid:88)

k,(cid:96),m

=

=

k

(cid:105)C(cid:104)

s¯ı, m¯ı, αi|
(cid:105)Ai ΓM,m,k

S

±

ΓM,m,k
1
2 ,S
S
±

(cid:88)

(cid:96)

(cid:96)
|

2 ,SΓm,m¯ı,(cid:96)

S,s¯ı ≡

1

k

|

(cid:96)
(cid:105)C|

(cid:105)A
s(cid:48)¯ı, m(cid:48)¯ı, α(cid:48)i(cid:105)¯ıΓm,m(cid:48)
(cid:105)i|
ˆQs¯ı,m¯ı
±

,s,M .

S,s(cid:48)
¯ı

¯ı,k

(C17)

(cid:54)
(cid:54)
In the last line we ﬁnd that the overlap is independent
on α and αi, though with constraints α = (s¯ı, αi), which
requires αi = α(cid:48)i. Therefore, diﬀerent Kraus operators
provide exactly the same operation and, accordingly, we
can sum over these equivalent Kraus operators to reduce

the number of indices. After this process we get

22

(cid:113)

(sA)−

1/2

λ

±

g[N

1](s¯ı)

−

×

,sA,M

±
ˆQs¯ı,m¯ı
±

,sA,M

ˆQs¯ı,m(cid:48)

¯ı
,sA,M †

±

(cid:17)

11B,

⊗

K 0

(cid:96) ≡

K 0

s¯ı,m¯ı,m(cid:48)
¯ı

= 2−

N −1

2 √N

(cid:88)

(cid:16)

ψ−AC|
(cid:104)
K 1
(cid:114)

g[N

M,s¯ı,m¯ı

×

K 1

(cid:96) ≡

=

1](s¯ı)
1

−

−
2N

ˆQs¯ı,m(cid:48)

¯ı
+,N/2,M †

11B.

⊗

(C18)

(C19)

The Kraus operators of the reduced channel ˜Λ are ob-
tained as (K u
11AB). It is simple to check
that the above operators deﬁne a CPTP-map.

Ψ−CD(cid:105) ⊗
|

11D)(

(cid:96) ⊗

[1] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information (Cambridge University Press,
Cambridge, 2000).

[2] C. M. Bishop, Pattern Recognition and Machine Learning

(Springer, 2006).

[3] P. Wittek, Quantum Machine Learning: What Quantum
Computing Means to Data Mining (Academic Press, El-
sevier, 2014).

[4] J Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, “Quantum machine learning,”
Nature (London) 549, 195 (2017).

[5] V. Dunjko and H. J. Briegel, “Machine learning & ar-
tiﬁcial intelligence in the quantum domain: a review of
recent progress,” Rep. Prog. Phys. 81, 074001 (2018).
[6] M. Schuld, I. Sinayskiy, and F. Petruccione, “An intro-
duction to quantum machine learning,” Contemporary
Physics 56, 172–185 (2015).

[7] C. Ciliberto, M. Herbster, Alessandro D.

Ialongo,
M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig,
“Quantum machine learning: a classical perspective,”
Proceedings of the Royal Society A: Mathematical, Phys-
ical and Engineering Sciences 474, 20170551 (2018).
[8] E Tang, “A quantum-inspired classical algorithm for
recommendation systems,” preprint arXiv:1807.04271
(2018).

[9] E Tang, “Quantum-inspired classical algorithms for prin-
cipal component analysis and supervised clustering,”
preprint arXiv:1811.00414 (2018).

[10] M. A. Nielsen and I. L. Chuang, “Programmable quan-
tum gate arrays,” Phys. Rev. Lett. 79, 321 (1997).
[11] S. Ishizaka and T. Hiroshima, “Asymptotic teleportation
scheme as a universal programmable quantum proces-
sor,” Phys. Rev. Lett. 101, 240501 (2008).

[12] S. Ishizaka and T. Hiroshima, “Quantum teleportation
scheme by selecting one of multiple output ports,” Phys.
Rev. A 79, 042306 (2009).

[13] S. Ishizaka, “Some remarks on port-based teleportation,”

preprint arXiv:1506.01555 (2015).

[14] S. Pirandola, R. Laurenza, and C. Lupo, “Fundamen-
tal limits to quantum channel discrimination,” preprint
arXiv:1803.02834 (2018).

[15] S. Boyd, L. Xiao, and A. Mutapcic, Subgradient methods

(2003).

[16] M. Jaggi, “Convex optimization without projection

steps,” preprint arXiv:1108.1170 (2011).

[17] M. Jaggi, “Revisiting frank-wolfe: projection-free sparse
convex optimization,” in Proceedings of the 30th Inter-
national Conference on International Conference on Ma-
chine Learning-Volume 28 (JMLR. org, 2013) pp. 1–427.
[18] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chan-
dra, “Eﬃcient projections onto the l 1-ball for learning
in high dimensions,” in Proceedings of the 25th interna-
tional conference on Machine learning (ACM, 2008) pp.
272–279.

[19] J. Liu, P. Musialski, P. Wonka,

and J. Ye, “Tensor
completion for estimating missing values in visual data,”
IEEE transactions on pattern analysis and machine in-
telligence 35, 208–220 (2013).

[20] S Lloyd, “Universal quantum simulators,” Science 273,

1073–1078 (1996).

[21] S. Pirandola, R. Laurenza, C. Ottaviani, and L. Banchi,
“Fundamental limits of repeaterless quantum communi-
cations,” Nat. Commun. 8, 15043 (2017).

[22] S. Pirandola, S. L. Braunstein, R. Laurenza, C. Otta-
viani, T. P. W. Cope, G. Spedalieri,
and L. Banchi,
“Theory of channel simulation and bounds for private
communication,” Quant. Sci. Tech. 3, 035009 (2018).
[23] J. Watrous, The theory of quantum information (Cam-
bridge Univ. Press, 2018) freely available at https://cs.
uwaterloo.ca/~watrous/TQI/.

[24] A. Y. Kitaev, A. Shen, and M. N. Vyalyi, Classical and
quantum computation, 47 (American Mathematical Soci-
ety, Providence, Rhode Island, 2002) sec. 11.

[25] J. Watrous, Advanced Topics in Quantum Information

Processing (Lecture notes, 2004).

[26] E. Knill, R. Laﬂamme, and G. J Milburn, “A scheme
for eﬃcient quantum computation with linear optics,”
Nature 409, 46 (2001).

[27] J. Watrous, “Simpler semideﬁnite programs for com-
pletely bounded norms,” Chicago Journal of Theoretical
Computer Science 8, 1–19 (2013).

[28] C. A. Fuchs and J. van de Graaf, “Cryptographic distin-
guishability measures for quantum-mechanical states,”
IEEE Trans. Info. Theory 45, 1216–1227 (1999).

[29] M. S. Pinsker, Information and information stability of
random variables and processes (Holden-Day, San Fran-
cisco, 1964).

[30] E. A. Carlen and E. H. Lieb, “Bounds for entanglement
via an extension of strong subadditivity of entropy,” Lett.
Math. Phys. 101, 1–11 (2012).

[31] A. Uhlmann, “The transition probability...” Rep. Math.

Phys. 9, 273–279 (1976).

[32] John Watrous, “Semideﬁnite programs for completely
bounded norms,” Theory of Computing 5, 217–238
(2009).

[33] I. Nechita, Z. Pucha(cid:32)la, (cid:32)L. Pawela, and K.

˙Zyczkowski,
“Almost all quantum channels are equidistant,” J. Math.
Phys. 59, 052201 (2018).

[34] Y. Nesterov, Introductory lectures on convex optimiza-
tion: A basic course, Vol. 87 (Springer Science & Busi-
ness Media, New York, 2013).

[35] B. Coutts, M. Girard, and J. Watrous, “Certifying op-
timality for convex quantum channel optimization prob-
lems,” preprint arXiv:1810.13295 (2018).

[36] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradi-
ent methods for online learning and stochastic optimiza-
tion,” Journal of Machine Learning Research 12, 2121–
2159 (2011).

[37] D. Garber and E. Hazan, “Faster rates for the frank-wolfe
method over strongly-convex sets,” in Proceedings of the
32nd International Conference on International Confer-
ence on Machine Learning-Volume 37 (JMLR. org, 2015)
pp. 541–549.

[38] Y. Nesterov, “Smooth minimization of non-smooth func-
tions,” Mathematical programming 103, 127–152 (2005).
[39] R. Bhatia, Matrix analysis, Vol. 169 (Springer Science &

Business Media, New York, 2013).

[40] L. Banchi, N. Pancotti, and S. Bose, “Quantum gate
learning in qubit networks: Toﬀoli gate without time-
dependent control,” npj Quantum Inf. 2, 16019 (2016).
[41] L. Innocenti, L. Banchi, A. Ferraro, S. Bose, and M. Pa-
ternostro, “Supervised learning of
time-independent
hamiltonians for gate design,” preprint arXiv:1803.07119
(2018).

[42] K. Mitarai, M. Negoro, M. Kitagawa,

and K. Fujii,
“Quantum circuit learning,” Phys. Rev. A 98, 032309
(2018).

[43] J. M. Arrazola, T. R. Bromley, J. Izaac, C. R. Myers,
K. Br´adler, and N. Killoran, “Machine learning method
for state preparation and gate synthesis on photonic
quantum computers,” Quantum Sci. Technol. 4, 024004

23

(2019).

[44] C. H. Bennett, G. Brassard, C. Cr´epeau, R. Jozsa,
A. Peres, and W. K. Wootters, “Teleporting an unknown
quantum state via dual classical and einstein-podolsky-
rosen channels,” Phys. Rev. Lett. 70, 1895 (1993).
[45] S. Pirandola, J. Eisert, C. Weedbrook, A. Furusawa, and
S. L. Braunstein, “Advances in quantum teleportation,”
Nat. Photon. 9, 641–652 (2015).

[46] G. Bowen and S. Bose, “Teleportation as a depolarizing
quantum channel, relative entropy, and classical capac-
ity,” Phys. Rev. Lett. 87, 267901 (2001).

[47] T. P. W. Cope, L. Hetzel, L. Banchi, and S. Pirandola,
“Simulation of non-pauli channels,” Phys. Rev. A 96,
022323 (2017).

[48] C. H. Bennett, D. P. DiVincenzo, J. A. Smolin, and
W. K. Wootters, “Mixed-state entanglement and quan-
tum error correction,” Phys. Rev. A 58, 3824 (1996).
[49] M. Christandl, F. Leditzky, C. Majenz, G. Smith,
F. Speelman, and M. Walter, “Asymptotic performance
of port-based teleportation,” preprint arXiv:1809.10751
(2018).

[50] S. Lloyd, “Almost any quantum logic gate is universal,”

Phys. Rev. Lett. 75, 346 (1995).

[51] N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbr¨uggen,
and S. J. Glaser, “Optimal control of coupled spin dy-
namics: design of nmr pulse sequences by gradient ascent
algorithms,” J. Magn. Reson. 172, 296–305 (2005).
[52] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush,
N. Ding, Z. Jiang, M. J. Bremner, J. M. Martinis, and
H. Neven, “Characterizing quantum supremacy in near-
term devices,” Nat. Phys. 14, 595 (2018).

[53] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost,
“Quantum principal component analysis,” Nat. Phys. 10,
631 (2014).

[54] E. Stickel, “On the fr´echet derivative of matrix func-
tions,” Linear Algebra and its Applications 91, 83–88
(1987).

[55] S. N. Ravi, M. D. Collins, and V. Singh, “A determinis-
tic nonsmooth frank wolfe algorithm with coreset guar-
antees,” preprint arXiv:1708.06714 (2017).

[56] F. Youseﬁan, A. Nedi´c,

and U. V. Shanbhag, “On
stochastic gradient and subgradient methods with adap-
tive steplength sequences,” Automatica 48, 56–67 (2012).
large-scale convex pro-
gramming under a linear optimization oracle,” preprint
arXiv:1309.5550 (2013).

[57] G. Lan, “The complexity of

