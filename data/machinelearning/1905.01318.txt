9
1
0
2

y
a
M
3

]
h
p
-
t
n
a
u
q
[

1
v
8
1
3
1
0
.
5
0
9
1
:
v
i
X
r
a

Optimization and learning of quantum programs

Leonardo Banchi,1, 2 Jason Pereira,3 Seth Lloyd,4, 5 and Stefano Pirandola3, 5
1Department of Physics and Astronomy, University of Florence,
via G. Sansone 1, I-50019 Sesto Fiorentino (FI), Italy
2INFN Sezione di Firenze, via G.Sansone 1, I-50019 Sesto Fiorentino (FI), Italy
3Department of Computer Science, University of York, York YO10 5GH, UK
4Department of Mechanical Engineering, Massachusetts Institute of Technology (MIT), Cambridge MA 02139, USA
5Research Laboratory of Electronics, Massachusetts Institute of Technology (MIT), Cambridge MA 02139, USA

A programmable quantum processor is a fundamental model of quantum computation. In this
model, any quantum channel can be approximated by applying a ï¬xed universal quantum operation
onto an input state and a quantum â€œprogramâ€ state, whose role is to condition the operation
performed by the processor.
It is known that perfect channel simulation is only possible in the
limit of inï¬nitely large program states, so that ï¬nding the best program state represents an open
problem in the presence of realistic ï¬nite-dimensional resources. Here we prove that the search for
the optimal quantum program is a convex optimization problem. This can be solved either exactly,
by minimizing a diamond distance cost function via semi-deï¬nite programming, or approximately,
by minimizing other cost functions via gradient-based machine learning methods. We apply this
general result to a number of diï¬€erent designs for the programmable quantum processor, from the
shallow protocol of quantum teleportation, to deeper schemes relying on port-based teleportation
and parametric quantum circuits. We benchmark the various designs by investigating their optimal
performance in simulating arbitrary unitaries, Pauli and amplitude damping channels.

I.

INTRODUCTION

Today the ï¬eld of quantum computing [1] is becom-
ing more and more mature, also thanks to the combined
eï¬€orts of academic and industrial researchers. From a
theoretical point of view, this endeavour is supported by
increasing interconnections with other rapidly-advancing
ï¬elds, such as machine learning [2]. For instance, we have
recently witnessed the development of new hybrid ar-
eas of investigation, such as quantum-enhanced machine
learning [3â€“7] (e.g., quantum neural networks, quantum
annealing etc.), protocols of quantum-inspired machine
learning (e.g., for recommendation systems [8] or compo-
nent analysis and supervised clustering [9]) and classical
learning methods applied to quantum computers, as ex-
plored here in this manuscript.

In quantum computing, a fundamental model is the
programmable quantum gate array or programmable
quantum processor [10]. This is a quantum processor
where a ï¬xed quantum operation is applied to an input
state and a program state. The role of the program state
is to condition the quantum operation in such a way to
apply some target quantum gate or channel to the in-
put state. This is a very ï¬‚exible scheme but not actually
universal: an arbitrary quantum channel cannot be pro-
grammed exactly, unless the program state is allowed to
have an inï¬nite number of qubits. For instance, a possi-
ble design relies on port-based teleportation (PBT) [11â€“
13], where an input state is subject to certain local op-
erations and classical communication (LOCCs) that are
programmed by a tensor product of N bipartite states.
For inï¬nite N , any quantum channel can be simulated
by copies of its Choi matrix [14] but, for any ï¬nite N ,
this simulation is not perfect.

Despite this fundamental model of quantum computa-

tion is known since 1997, a quantitative characterization
of its actual performance in terms of gate implementa-
tion or channel simulation is still missing. Given a target
quantum gate or channel, it is not yet known what de-
gree of approximation can be reached and what kind of
optimization procedure must be employed to choose the
program state. After more than 20 years, the solution to
these open problems comes from a suitable application of
techniques of semideï¬nite programming (SDP) and ma-
chine learning (ML).

In our work, we quantify the error between an arbi-
trary target channel and its programmable simulation in
terms of the diamond distance and other suitable cost
functions, including the trace distance and the quantum
ï¬delity. For all the considered cost functions, we are able
to show that the minimization of the simulation error is
a convex optimization problem in the space of the pro-
gram states. This already solves an outstanding prob-
lem which aï¬€ects various models of quantum computers
(e.g., variational quantum circuits) where the optimiza-
tion over classical parameters is non-convex and therefore
not guaranteed to converge to a global optimum. By con-
trast, because our problem is proven to be convex, we can
use SDP to minimize the diamond distance and always
ï¬nd the optimal program state for the simulation of a
target channel, therefore optimizing the programmable
quantum processor. Similarly, we may ï¬nd suboptimal
solutions by minimizing the trace distance or the quan-
tum ï¬delity by means of gradient-based ML techniques,
such as the projected subgradient method [15] and the
conjugate gradient method [16, 17]. We note indeed that
the minimization of the (cid:96)1-norm, mathematically related
to the quantum trace distance, is widely employed in
many ML tasks [18, 19], so many of those techniques can
be adapted for learning program states.

 
 
 
 
 
 
With these general results in our hands, we ï¬rst discuss
the optimal learning of arbitrary unitaries with a generic
programmable quantum processor. Then, we consider
speciï¬c designs of the processor, from a shallow scheme
based on the teleportation protocol, to higher-depth de-
signs based on PBT [11â€“13] and parametric quantum cir-
cuits (PQCs) [20], introducing a suitable convex reformu-
lation of the latter. In the various cases, we benchmark
the processors for the simulation of basic unitary gates
(qubit rotations) and various basic channels, including
the amplitude damping channel which is known to be
the most diï¬ƒcult to simulate [21, 22]. For the deeper
designs, we ï¬nd that the optimal program states do not
correspond to the Choi matrices of the target channels,
which is rather counter-intuitive and unexpected.

The paper is structured as follows. In Sec. II we discuss
the general notion of programmable channel simulation,
the various cost functions and a suitable Choi-reduction
of the problem. In Sec. III we then show that the opti-
mization of a generic programmable quantum processor
is convex in the space of the program states. In Sec. IV
we consider the optimization of the diamond distance via
SDP and the minimization of the other cost functions via
gradient descent. In particular, in Sec. V we provide the
details of the gradient-based ML algorithms to be used,
together with a discussion of smoothing techniques. In
Sec. VI, we discuss the optimal learning of arbitrary uni-
taries. We then move to discuss the various speciï¬c de-
signs based on teleportation (Sec. VII), PBT (Sec. VIII)
and PQC (Sec. IX). Sec. X is for conclusions.

II. PROGRAMMABLE SIMULATION

A. General problem

E

Consider an arbitrary but known quantum channel
from dimension d to dimension d(cid:48) [1, 23]. We want
E
to simulate
using a programmable quantum proces-
sor [10] that we simply call â€œquantum processorâ€ (see
Fig. 1). This is represented by a completely positive
trace-preserving (CPTP) universal map Q which is as-
sumed to be ï¬xed and applied to the arbitrary input Ï of
the channel together with a program state Ï€ (which may
be varied). In this way, the quantum processor generates
EÏ€ as
an approximate channel

EÏ€(Ï) = Tr2 [Q(Ï

âŠ—

Ï€)] .

(1)

Our goal is to ï¬nd the program state Ï€ for which the
, i.e., so that we minimize

simulation
the following cost function

EÏ€ is the closest to

E

2

FIG. 1. Arbitrary quantum channel E and its simulation EÏ€
via a quantum processor Q applied to a program state Ï€.

= 0 for arbitrary

From theory [10, 26] we know that we cannot achieve
C
unless Ï€ and Q have inï¬nite di-
(cid:5)
mensions. As a result, for any ï¬nite-dimensional realistic
design of the quantum processor, ï¬nding the optimal pro-
gram state ËœÏ€ is an open problem.

E

Recall that the diamond distance is deï¬ned by the fol-

lowing maximization

(cid:107)E âˆ’ EÏ€(cid:107)(cid:5)

= max

Ï• (cid:107)I âŠ— E

(Ï•)

âˆ’ I âŠ— EÏ€(Ï•)

(cid:107)1 ,

(4)

O
(cid:107)

(cid:107)1 := TrâˆšOâ€ O is the trace norm [23]. Because
where
the trace norm is convex over mixed states, one may re-
duce the maximization in Eq. (4) to bipartite pure states
Ï• =
. In general, we therefore need to consider a
|
min-max optimization, i.e., ï¬nd ËœÏ€ and (pure) ËœÏ• such that

Ï•
|

(cid:105) (cid:104)

Ï•

(cid:107)I âŠ— E
= min

Ï€

( ËœÏ•)
max

âˆ’ I âŠ— EËœÏ€( ËœÏ•)
(Ï•)

Ï• (cid:107)I âŠ— E

(cid:107)1
âˆ’ I âŠ— EÏ€(Ï•)

(cid:107)1 .

(5)

Also recall that the diamond distance can be computed
using SDP [27]. In particular, due to strong duality, it
may be computed via a minimization rather than a max-
imization, so that the min-max optimization problem in
Eq. (5) can be transformed into a more convenient min-
imization problem (more details in Sec. IV A). An alter-
native solution is to reduce the general problem into a
weaker one which is expressed in terms of the Choi ma-
trix of the channel (see following section). In this way, we
also avoid the maximization in Ï• but with the downside
of using a larger cost function, the trace distance.

B. Processor map and Choi reduction

It is known that a quantum channel

E
with its Choi matrix Ï‡
(Î¦), where Î¦ :=
E
is d-dimensional maximally-entangled state, i.e.,

I âŠ— E

is one-to-one
Î¦

Î¦
|

:=

(cid:105)(cid:104)

|

:= dâˆ’

Î¦
|

(cid:105)

1/2 (cid:88)

i

.

i, i
(cid:105)

|

(6)

(Ï€) :=

C

(cid:5)

(cid:107)E âˆ’ EÏ€(cid:107)(cid:5) â‰¤

2,

(2)

Using the channel deï¬nition of Eq. (1), we may write

in terms of the diamond distance [24, 25]. In other words,

Find ËœÏ€ such that C
(cid:5)

(ËœÏ€) = min

Ï€

(Ï€).

C
(cid:5)

(3)

Ï‡

Ï€ =
E

I âŠ— EÏ€(Î¦)
1(cid:80)
i
ij |
(cid:105)(cid:104)

= dâˆ’

j

Tr2 [Q(
|

i
(cid:105)(cid:104)

j

| âŠ—

| âŠ—

Ï€)] .

(7)

QÏ€Ï€ (cid:1)From this expression, it is clear that the Choi matrix
Ï‡
Ï€ is linear in the program state Ï€. More precisely, the
E
Ï€ at the output of the processor Q can be
Choi matrix Ï‡
E
directly written as a CPTP linear map Î› acting on the
space of the program states Ï€, i.e.,

Ï‡Ï€ := Ï‡

Ï€ = Î›(Ï€).
E

(8)

This map is also depicted in Fig. 2.

We may connect the minimization of the diamond dis-

tance C

(cid:5)

(Ï€) to the minimization of the trace distance

C1(Ï€) :=

Ï‡
(cid:107)

E âˆ’

Ï‡Ï€(cid:107)1 ,

(9)

between the Choi matrices Ï‡
E
write the sandwich relation [23]

and Ï‡Ï€. In fact, we may

C1(Ï€)

(Ï€)

C
(cid:5)

â‰¤

â‰¤

d C1(Ï€).

(10)

While the lower bound is immediate from the deï¬nition
of Eq. (4), the upper bound can be proven using the
following equivalent form of the diamond distance

(cid:107)E âˆ’EÏ€(cid:107)(cid:5)

= sup
Ï0,Ï1

d

(âˆšÏ0 âŠ—
(cid:107)

11)(Ï‡

E âˆ’

Ï‡Ï€)(âˆšÏ1 âŠ—

11)

(cid:107)1, (11)

where the optimization is done over the density matrices
Ï0 and Ï1 [27, Theorem 3.1]. In fact, consider the Frobe-
nius norm

Tr[Aâ€ A] and the spectral norm

(cid:112)

A
(cid:107)

(cid:107)2 :=
:= max

A
(cid:107)

(cid:107)âˆ

(cid:107) â‰¤
{(cid:107)
which satisfy the following properties [23]

âˆˆ

: u

Au
(cid:107)

Cd,

u
(cid:107)

ABC
11

âŠ—

(cid:107)1 â‰¤ (cid:107)
=
(cid:107)
(cid:107)âˆ

A
A

B

(cid:107)1(cid:107)
A

(cid:107)âˆ(cid:107)
(cid:107)âˆ â‰¤ (cid:107)

C
(cid:107)2.

(cid:107)âˆ

(cid:107)
A
(cid:107)

Then, from Eqs. (11), (13) and (14), one gets

(cid:112)

d

Ï‡

E âˆ’

E âˆ’

(15)

Ï‡Ï€(cid:107)1

= d
(cid:107)

sup
Ï0,Ï1
Ï‡

(cid:107)E âˆ’ EÏ€(cid:107)(cid:5) â‰¤

TrÏ0TrÏ1(cid:107)
Ï‡Ï€(cid:107)1.
Thanks to Eq. (10), we may avoid the maximization
step in the deï¬nition of the diamond distance and sim-
plify the original problem to approximating the Choi ma-
trix Ï‡
of the channel by varying the program state Ï€.
E
This is a process of learning Choi matrices as depicted
in Fig. 2. Because the simpler cost function C1(Ï€) is
an upper bound, its minimization generally provides a
sub-optimal solution for the program state.

C. Other cost functions

Besides C
(cid:5)

and C1 we can introduce other cost func-
tions. First of all, using the Fuchs-van de Graaf inequal-
ity [28], we may write

C1(Ï€)

â‰¤

(cid:112)
2

CF (Ï€), CF (Ï€) = 1

F (Ï€)2,

âˆ’

(16)

,

1
}

,

(12)

(13)
(14)

3

FIG. 2. Map of the processor and learning of Choi matrices.
Consider an arbitrary (but known) quantum channel E and
its associated Choi matrix Ï‡E , generated by propagating part
of a maximally-entangled state Î¦. Then, consider a quantum
processor Q with program state Ï€ which generates the simu-
lated channel EÏ€ and, therefore, the corresponding Choi ma-
trix Ï‡Ï€ := Ï‡EÏ€ upon propagating part of Î¦ as input state. The
map of the processor is the CPTP map Î› from the program
state Ï€ to the output Choi matrix Ï‡Ï€. In a simpliï¬ed version
of our problem, we may optimize the program Ï€ in such a way
to minimize the trace distance C1(Ï€) := (cid:107)Ï‡E âˆ’ Ï‡Ï€(cid:107)1.

where F (Ï€) is Buresâ€™ ï¬delity between the two Choi ma-
trices Ï‡
E

and Ï‡Ï€, i.e.,

F (Ï€) :=

âˆšÏ‡
E

âˆšÏ‡Ï€(cid:107)1 = Tr

(cid:107)

(cid:113)

âˆšÏ‡

Ï‡Ï€âˆšÏ‡
E

.

E

(17)

Another possible upper bound can be written using the
quantum Pinskerâ€™s inequality [29, 30]. In fact, we may
write C1(Ï€)

CR(Ï€), where

(2 ln âˆš2)

(cid:112)

â‰¤

CR(Ï€) := min

S(Ï‡

{

)

Ï‡
Ï‡Ï€), S(Ï‡Ï€||
E ||
E
log2 Ïƒ)] is the quantum rel-

(18)

}

,

and S(Ï
ative entropy between Ï and Ïƒ.

Ïƒ) := Tr[Ï(log2 Ï
||

âˆ’

Finally we may consider other cost functions in terms
of any Shatten p-norm Cp(Ï€) :=
Ï‡Ï€(cid:107)p, even
though this option provides lower bounds instead of up-
per bounds for the trace distance. Recall that, given an
1, we may deï¬ne its
operator O and a real number p
Schatten p-norm as [23]

Ï‡
(cid:107)

E âˆ’

â‰¥

O
(cid:107)

(cid:107)p = (Tr

O
|

p)1/p,
|

(19)

= âˆšOâ€ O. For any 1

where
O
|
|
has the monotony
O
(cid:107)
of operators A and B, and each pair of parameters p, q
[1,

O
â‰¤
(cid:107)1. An important property is duality. For each pair
âˆˆ

p
â‰¤
â‰¤
(cid:107)q, so that
(cid:107)

1 = 1, we may write [23]

â‰¤ âˆ
(cid:107)âˆ â‰¤

] such that pâˆ’

, one
. . .

(cid:107)p â‰¥ (cid:107)

1 + qâˆ’

q
O

O

(cid:107)

âˆ

A
(cid:107)

(cid:107)p = sup

B

(cid:107)

(cid:107)qâ‰¤

1 |(cid:104)

B, A

(cid:105)| â‰¡

sup
B
(cid:107)qâ‰¤

(cid:107)

B, A
(cid:105)

1 (cid:104)

,

(20)

B, A
(cid:105)

where
= Tr(Bâ€ A) is the Hilbert-Schmidt product,
and the second inequality follows since we can arbitrarily
change the sign of B.

(cid:104)

Q ï° Î¦ â„°ğœ‹ Î¦ ğœ’â„° ğœ’ğœ‹ â„° Î› III. CONVEXITY

and we may write

(cid:5)

In this section, we show that the minimization of the
main cost functions C
, C1 and CF is a convex opti-
mization problem in the space of the program states Ï€.
This means that we can ï¬nd the optimal program state ËœÏ€
by minimizing C
or, alternatively, sub-optimal program
(cid:5)
states can be found by minimizing either C1 or CF . For
the sake of generality we prove the result for all the cost
functions discussed in the previous section.

(cid:5)

Theorem 1 The minimization of the generic cost func-
tion C = C
, C1, CF , CR or Cp for any p > 1 is a convex
optimization problem in the space of program states. In
particular, the global minimum ËœÏ€ can always be found as
a local minimum of C
. Alternatively, this optimal pro-
gram state can be approximated by minimizing C1 or CF .

(cid:5)

Proof. Let us start to show the result for the diamond
distance C

. In this case, we can write the following

(cid:5)

p)Ï€(cid:48)]

(cid:5)

âˆ’

[pÏ€ + (1

C
:= (cid:13)
(cid:13)
E âˆ’ EpÏ€+(1
âˆ’
(p+1
(cid:107)

(1)
=

p)

E âˆ’

âˆ’

p

p)Ï€(cid:48)

(cid:13)
(cid:13)
(cid:5)
EÏ€ âˆ’
(1
(cid:107)

âˆ’

p)

(1

âˆ’

(cid:107)(cid:5)

EÏ€(cid:48)
(1

(2)

â‰¤ (cid:107)
(3)

p
â‰¤
= pC

p

p

EÏ€(cid:107)(cid:5)

+

E âˆ’

p)

E âˆ’

p)

EÏ€(cid:48)

(cid:107)(cid:5)

âˆ’

(cid:107)E âˆ’ EÏ€(cid:107)(cid:5)
(Ï€) + (1
(cid:5)

+ (1

âˆ’
p)C

(cid:5)

âˆ’

(cid:107)E âˆ’ EÏ€(cid:48)

p)
(Ï€(cid:48)),

(cid:107)(cid:5)

(21)

where we use (1) the linearity of
, (2)
E
inequality and (3) the property
(cid:107)1 =
xA
(cid:107)
for any operator A and coeï¬ƒcient x.

x
|

the triangle
(cid:107)1, valid

|(cid:107)

A

For any Schatten p-norm Cp with p

1, we may
exploit the dual representation in Eq. (20) with A =
Ï‡

Î›(Ï€), so that

â‰¥

E âˆ’

Cp(Ï€) = sup
(cid:107)qâ‰¤

B

(cid:107)

(cid:12)
(cid:12)Tr
{

1

Bâ€ [Ï‡

E âˆ’

(cid:12)
Î›(Ï€)]
(cid:12) .
}

(22)

4

(cid:12)
(cid:12)

Cp(Â¯Ï€)

= sup
B
(cid:107)qâ‰¤

(cid:107)

1

(cid:12)
(cid:12)Tr
{

Bâ€ [p0Ï‡
E

+ p1Ï‡

E âˆ’

p0Î›(Ï€0)

p1Î›(Ï€1)]
}

âˆ’

(cid:12)
p0Bâ€ [Ï‡
(cid:12)Tr
{

E âˆ’

Î›(Ï€0)] + p1Bâ€ [Ï‡

Î›(Ï€1)]
}

E âˆ’

(cid:12)
(cid:12)

(cid:12)
(cid:12)p0Tr
{

Bâ€ [Ï‡

E âˆ’

Î›(Ï€0)]
}

+

+p1Tr
{

Bâ€ [Ï‡

E âˆ’

(cid:12)
(cid:12)p0Tr
{

Bâ€ [Ï‡

E âˆ’

(cid:12)
Î›(Ï€1)]
(cid:12)
}
(cid:12)
(cid:12) +
Î›(Ï€0)]
}

+ (cid:12)

Bâ€ [Ï‡
(cid:12)p1Tr
{

(cid:12)
Î›(Ï€1)]
(cid:12)
}

E âˆ’

(cid:12)
Bâ€ [Ï‡
(cid:12)Tr
{

Î›(Ï€0)]
}

E âˆ’

(cid:12)
(cid:12) +

(1)
= sup
B
(cid:107)qâ‰¤

(cid:107)

1

(2)
= sup
B
(cid:107)qâ‰¤

(cid:107)

1

(3)

â‰¤

(4)

â‰¤

sup
B
(cid:107)qâ‰¤

1

(cid:107)

sup
1
B
(cid:107)qâ‰¤

p0

(cid:107)
+ p1

sup
1
C
(cid:107)qâ‰¤
= p0C(Ï€0) + p1C(Ï€1),

C â€ [Ï‡
Tr
{

(cid:107)

Î›(Ï€1)]
}

E âˆ’

(23)

where we use: (1) linearity of the operator B, (2) lin-
earity of the trace, (3) triangle inequality, (4) and the
inequality supB f (B) + g(B)
supB f (B) + supC g(C)
for the optimization of two functions.

â‰¤

To show the convexity of CF , deï¬ned in Eq. (16), we
note that the ï¬delity function F (Ï, Ïƒ) satisï¬es the follow-
ing concavity relation [31]

(cid:32)

(cid:88)

F

k

(cid:33)2

pkÏk, Ïƒ

(cid:88)

â‰¥

k

pkF (Ïk, Ïƒ)2 .

(24)

Due to the linearity of Ï‡Ï€ = Î›(Ï€), the ï¬delity in Eq. (17)
Ï€k for Â¯Ï€ := (cid:80)
satisï¬es F 2
k pkÏ€k. Accordingly,
we get the following convexity result

k pkF 2

Â¯Ï€ â‰¥

(cid:80)

CF

(cid:33)

pkÏ€k

(cid:32)

(cid:88)

k

(cid:88)

â‰¤

k

pkCF (Ï€k) .

(25)

For the cost function CR, the result comes from the lin-
earity of Î›(Ï€) and the joint convexity of the relative en-
tropy. In fact, for Â¯Ï€ := p0Ï€0 + p1Ï€1, we may write

S[Î›(Â¯Ï€)

Ï‡
E

||

] = S[p0Î›(Ï€0) + p1Î›(Ï€1)
= S[p0Î›(Ï€0) + p1Î›(Ï€1)

Ï‡
]
E
p0Ï‡

p0S[Î›(Ï€0), Ï‡
E

â‰¤
with symmetric proof for S[Ï‡
E ||
convexity of CR(Ï€) in Eq. (18). (cid:4)

||
||
] + p1S[Î›(Ï€1), Ï‡
E

+ p1Ï‡
],

E

E

]
(26)

Î›(Â¯Ï€)]. This implies the

For any convex combination Â¯Ï€ := p0Ï€0 + p1Ï€1, with p0 +
p1 = 1, we have Î›(Â¯Ï€) = p0Î›(Ï€0) + p1Î›(Ï€1) by linearity,

The result of the theorem can certainly be extended
to any convex parametrization of program states. For

A. Convex classical parametrizations

instance, assume that Ï€ = Ï€(Î»), where Î» =
Î»i}
{
probability distribution. This means that, for 0
p
â‰¤
â‰¤
and any two parametrizations, Î» and Î»(cid:48), we may write

is a
1

Ï€[pÎ» + (1

âˆ’

p)Î»(cid:48)] = pÏ€(Î») + (1

âˆ’

p)Ï€(Î»(cid:48)).

(27)

Then the problem remains convex in Î» and we may there-
fore ï¬nd the global minimum in these parameters. It is
clear that this global minimum ËœÎ» identiï¬es a program
state Ï€(ËœÎ») which is not generally the optimal state ËœÏ€ in
the entire program space
, even though the solution may
be a convenient solution for experimental applications.

S

Note that a possible classical parametrization consists

of using classical program states, of the form

Ï€(Î») =

(cid:88)

i

Î»i |

Ï•i(cid:105) (cid:104)

Ï•i|

,

(28)

Ï•i(cid:105)}

where
is an orthonormal basis in the program
space. Convex combinations of probability distributions
therefore deï¬ne a convex set of classical program states

{|

Moreover, because Ï‡â„¦Ï€ is Hermitian, the above SDP

can be simpliï¬ed into

5

Minimize 2

Subject to Z

â‰¥

Tr2Z
(cid:107)
0 and Z

(cid:107)âˆ
â‰¥

,
d Ï‡â„¦Ï€ .

(32)

|

(cid:5)

(cid:107)

d

â‰¤

(Ï€) but also pro-
Not only this procedure computes C
Ï‡
vides the upper bound C
[33].
Ï‡Ï€|(cid:107)âˆ
Tr2 |
E âˆ’
(cid:5)
In fact, it is suï¬ƒcient to choose Z = d Ï‡+
â„¦Ï€ , where Ï‡+ =
)/2 is the positive part of Ï‡. Using Tr2Ï‡â„¦Ï€ = 0,
(Ï‡ +
â„¦Ï€ = d
we may write Tr2Z

dTr2Ï‡+

Ï‡
|

(Ï€)

The SDP form in Eq. (32) is particularly convenient
for ï¬nding the optimal program. In fact, suppose now
that Ï€ is not ï¬xed but we want to optimize on this state
too, so as to compute the optimal program state ËœÏ€ such
(Ï€). The problem is therefore
C
that C
(cid:5)
mapped into the following unique minimization

(ËœÏ€) = minÏ€

.
Ï‡â„¦Ï€ |

2 Tr2|

â‰¤

âˆˆS

(cid:5)

Subject to Z

â‰¥

Minimize 2
0, Ï€

Tr2Z
(cid:107)
0, Tr(Ï€) = 1, Z

(cid:107)âˆ

,

â‰¥

d Ï‡â„¦Ï€ . (33)

â‰¥

Sclass =

{

Ï€ : Ï€ =

(cid:88)

i

Î»i |

Ï•i(cid:105) (cid:104)

Ï•i|

,

Ï•i|

(cid:104)

Ï•j(cid:105)

= Î´ij}

. (29)

This algorithm can be used to optimize the performance
of any programmable quantum processor.

Optimizing over this speciï¬c subspace corresponds to op-
timizing the programmable quantum processor over clas-
sical programs. It is clear that global minima in
Sclass
are expected to be very diï¬€erent. For instance,
and
Sclass cannot certainly include Choi matrices which are
usually very good quantum programs.

S

IV. CONVEX OPTIMIZATION

A. SDP minimization

Once we have Theorem 1 in our hands, we can success-
fully minimize the various cost functions in the search of
the optimal program state. In other words, for a generic
cost function C we want to solve minÏ€
C(Ï€). The solu-
tion is exact if we directly use the diamond-distance cost
C

and we minimize it via SDP.

(Ï€) =
(cid:5)
Let us introduce the linear map â„¦Ï€ :=

(cid:107)E âˆ’ EÏ€(cid:107)(cid:5)
corresponding Choi matrix

E âˆ’ EÏ€ with

âˆˆS

Ï‡â„¦Ï€ = Ï‡

Ï‡Ï€ = Ï‡

E âˆ’

E âˆ’

Î›(Ï€).

(30)

Thanks to the property of strong duality of the diamond
norm, for any program Ï€ we can compute the cost func-
tion C
(cid:5)

via the following SDP [32]

(Ï€) =

â„¦Ï€(cid:107)(cid:5)
(cid:107)
1
2

Minimize

Subject to

(

Tr2M0(cid:107)âˆ
(cid:107)
(cid:18) M0

+

Tr2M1(cid:107)âˆ
(cid:19)

(cid:107)
d Ï‡â„¦Ï€

) ,

âˆ’

d Ï‡â€ â„¦Ï€ M1

â‰¥

0,

(31)

âˆ’
0 and M1 â‰¥
equals the maximum singular value of O.

0 in Cd

, and the spectral

d(cid:48)

Ã—

where M0 â‰¥
O
norm
(cid:107)âˆ
(cid:107)

B. Gradient descent

An alternative approach (useful for deeper processors)
consists in the optimization of the larger but easier-to-
compute cost function C = C1 (trace distance) or CF
(inï¬delity). According to Theorem 1, the cost function
R is convex over the program space
C :
and,
S
C(Ï€) by
therefore, we can solve the optimization minÏ€
using gradient-based ML algorithms. This means that
we need to compute the derivatives of C and use gradient
descent in order to converge to a local (global) minimum.
is

The sub-diï¬€erential of C at the generic point Ï€

S â†’

âˆˆS

âˆˆ S

deï¬ned as

âˆ‚C(Ï€) =

Z : C(Ïƒ)

C(Ï€)

Tr[Z(Ïƒ

Ï€)],

Ïƒ

{

âˆˆ S}(34)
where Z is Hermitian [34, 35]. In the points where C is
not only convex but also diï¬€erentiable, then

âˆ’

âˆ’

â‰¥

âˆ€

âˆ‚C(Ï€) =

,
C(Ï€)
}

{âˆ‡

(35)

âˆ‡

namely the subgradient contains a single element, the
gradient
C, that can be obtained as the FrÂ´echet deriva-
tive of C (for more details see Appendix A). In the points
where C is not diï¬€erentiable, then the gradient still pro-
vides an element of the subgradient to be used in the
gradient-based minimization process.
In order to compute the gradient

C, it is convenient
to consider the Kraus decomposition of the processor
map Î›. Let us write

âˆ‡

Î›(Ï€) =

(cid:88)

k

AkÏ€Aâ€ k,

(36)

with Kraus operators Ak. We then deï¬ne the dual
map Î›âˆ— of the processor as the one (generally non-trace-
preserving) which is given by the following decomposition

Î›âˆ—(Ï) =

(cid:88)

k

Aâ€ kÏAk.

(37)

composition

With these deï¬nitions in hands, we prove the following.

Theorem 2 Suppose we use a quantum processor Q with
map Î›(Ï€) = Ï‡Ï€ in order to approximate the Choi matrix
. Then, the gradients of the
Ï‡
E
trace distance C1(Ï€) and the inï¬delity CF (Ï€) are given
by the following analytical formulas

of an arbitrary channel

E

C1(Ï€) =

âˆ‡

CF (Ï€) =

âˆ‡

F (Ï€) =

âˆ‡

(cid:88)

k

sign(Î»k)Î›âˆ—(Pk),

(cid:112)
2

Î›âˆ—

1
âˆ’
(cid:104)
âˆšÏ‡

âˆ’
1
2

CF (Ï€)

(âˆšÏ‡

E

E

F (Ï€),

âˆ‡
Î›(Ï€) âˆšÏ‡

(38)

(39)

(cid:105)

)âˆ’

E

1

2 âˆšÏ‡
E

,

(40)

where Î»k (Pk) are the eigenvalues (eigenprojectors) of the
. When C1(Ï€) or CF (Ï€) are
Hermitian operator Ï‡Ï€ âˆ’
E
not diï¬€erentiable at Ï€, then the above expressions provide
an element of the subgradient âˆ‚C(Ï€).

Ï‡

Proof. We prove the above theorem assuming that the
functions are diï¬€erentiable for program Ï€. For non-
diï¬€erentiable points, the only diï¬€erence is that the above
analytical expressions are not unique and provide only
one of the possibly inï¬nite elements of the subgradient.
Further details of this mathematical proof are given in
Appendix A. Following matrix diï¬€erentiation (see Ap-
pendix A 1), for any function f (A) = Tr[g(A)] of a matrix
A, we may write

6

Exploiting this expression in Eq. (41) we get the gradient
F (Ï€) as in Eq. (40). The other Eq. (39) simply follows

âˆ‡
from applying the deï¬nition in Eq. (16).

For the trace distance, let us write the eigenvalue de-

Ï‡Ï€ âˆ’

Ï‡
E

=

(cid:88)

k

Î»kPk .

(46)

Then using linearity of Eq. (42), the deï¬nition of proces-
sor map of Eq. (8) and diï¬€erential calculations of the
trace distance (see Appendix A 3 for details), we can
write

dC1(Ï€) =

=

(cid:88)

k
(cid:88)

sign(Î»k)Tr[PkÎ›(dÏ€)]

sign(Î»k)Tr[Î›âˆ—(Pk)dÏ€]

k
= Tr

Î›âˆ—[sign(Ï‡Ï€ âˆ’
{
From the deï¬nition of the gradient in Eq. (41), we ï¬nally
get

(47)

)]dÏ€

Ï‡
E

}

.

E
which leads to the result in Eq. (38). (cid:4)

C1(Ï€) = Î›âˆ—[sign(Ï‡Ï€ âˆ’

âˆ‡

Ï‡

)],

(48)

The above results in Eqs. (39) and (38) can be used
together with the projected subgradient method [15]
or conjugate gradient algorithm [16, 17] to iteratively
ï¬nd the optimal program state in the minimization of
minÏ€
C(Ï€) for C = C1 or CF . In the following section
we present the details of the two mentioned gradient-
based ML algorithms and how they can be adapted for
the learning of program states.

âˆˆS

dTr[g(A)] = Tr[g(cid:48)(A)dA],

(41)

V. GRADIENT-BASED CONVEX
OPTIMIZATION TECHNIQUES

f (A) = g(cid:48)(A). Both the trace-
and the gradient is
distance and ï¬delity cost functions can be written in this
form. To ï¬nd the explicit gradient of the ï¬delity function
we ï¬rst note that, by linearity, we may write

âˆ‡

Î›(Ï€ + Î´Ï€) = Î›(Ï€) + Î›(Î´Ï€) ,

(42)

and therefore the following expansion

âˆšÏ‡
E
Î›(Ï€)âˆšÏ‡

Î›(Ï€ + Î´Ï€)âˆšÏ‡
+ âˆšÏ‡

E

=
Î›(Î´Ï€)âˆšÏ‡
E

E

E

âˆšÏ‡
E

.

(43)

From this equation and diï¬€erential calculations of the
ï¬delity (see Appendix A 2 for details), we ï¬nd

dF =

(cid:104)

Tr

1
2

(âˆšÏ‡

Î›(Ï€)âˆšÏ‡
E

E

)âˆ’

1

2 âˆšÏ‡

E

Î›(Î´Ï€)âˆšÏ‡

(cid:105)

E

,

(44)

where dF = F (Ï€ + Î´Ï€)
property of the trace, we get

âˆ’

F (Ï€). Then, using the cyclic

dF =

(cid:104)

Î›âˆ—

(cid:104)
âˆšÏ‡

Tr

1
2

(âˆšÏ‡
E

E

Î›(Ï€)âˆšÏ‡

E

)âˆ’

1

2 âˆšÏ‡

E

(cid:105)

(cid:105)

Î´Ï€

. (45)

Gradient-based convex optimization is at the heart of
many popular ML techniques such as, online learning in
a high-dimensional feature space [18], missing value esti-
mation problems [19], text classiï¬cation, image ranking,
and optical character recognition [36], to name a few.
In all the above applications, â€œlearningâ€ corresponds to
the following minimization problem minx
f (x), where
is a convex set. Quan-
f (x) is a convex function and
tum learning falls into this category, as the space of pro-
gram states is convex due to the linearity of quantum
mechanics and cost functions are typically convex in this
space (see Theorem 1). Gradient-based approaches are
among the most applied methods for convex optimiza-
tion of non-linear, possibly non-smooth functions [34].
Here we present two algorithms, the projected subgra-
dient method and the conjugate gradient method, and
show how that can be adapted to our problem.

âˆˆS

S

Projected subgradient methods have the advantage of
simplicity and the ability to optimize non-smooth func-
2(cid:1)
tions, but can be slower, with a convergence rate

(cid:0)(cid:15)âˆ’

O

O

(cid:0)(cid:15)âˆ’

for a desired accuracy (cid:15). Conjugate gradient meth-
1(cid:1), pro-
ods [16, 17] have a faster convergence rate
vided that the cost function is smooth. This conver-
1/2(cid:1)
gence rate can be improved even further to
for strongly convex functions [37] or using Nesterovâ€™s ac-
celerated gradient method [38]. The technical diï¬ƒculty
in the adaptation of these methods for learning program
states comes because the latter is a constrained optimiza-
tion problem, namely at each iteration step the optimal
program must be a proper quantum state, and the cost
functions coming from quantum information theory are,
generally, non-smooth.

(cid:0)(cid:15)âˆ’

O

A. Projected subgradient method

Given the space
onto

projection

PS

of program states, let us deï¬ne the
as

S
S

PS

(X) = argmin

S (cid:107)

Ï€

âˆˆ

X

Ï€

(cid:107)2 ,

âˆ’

(49)

where argmin is the argument of the minimum, namely
to the operator X. Then, a
the closest state Ï€
C(Ï€) is to apply the
ï¬rst order algorithm to solve minÏ€
projected subgradient method [15, 34], which iteratively
applies the following steps

âˆˆ S

âˆˆS

1) Select an operator gi from âˆ‚C(Ï€i),
2) Update Ï€i+1 =

Î±igi) ,

(Ï€i âˆ’

PS

(50)

where i is the iteration index and Î±i a learning rate.

The above algorithm diï¬€ers from standard gradient
i) the update rule is based on
methods in two aspects:
the subgradient, which is deï¬ned even for non-smooth
Î±igi is generally not a
functions; ii) the operator Ï€i âˆ’
quantum state, so the algorithm ï¬xes this issue by pro-
jecting that operator back to the closest quantum state,
via Eq. (49). The algorithm converges to the optimal so-
(approximating the optimal program ËœÏ€) as [15]
lution Ï€

âˆ—

C(Ï€i)

C(Ï€

)

âˆ—

âˆ’

â‰¤

e1 + G (cid:80)i
2 (cid:80)i

k=1 Î±k

k=1 Î±2
k

=: (cid:15),

(51)

(cid:107)

Ï€

âˆˆ

2
2 is the initial error (in Frobenius
where e1 =
Ï€1 âˆ’
âˆ—(cid:107)
(cid:107)
2
âˆ‚C. Pop-
G for any g
norm) and G is such that
g
2 â‰¤
(cid:107)
ular choices for the learning rate that assure convergence
1/âˆšk and Î±k = a/(b + k) for some a, b > 0.
are Î±k âˆ
In general, the projection step is the major drawback,
which often limits the applicability of the projected sub-
gradient method to practical problems. Indeed, projec-
tions like Eq. (49) require another full optimization at
each iteration that might be computationally intensive.
Nonetheless, we show in the following theorem that this
issue does not occur in learning quantum states, because
the resulting optimization can be solved analytically.

Theorem 3 Let X be a Hermitian operator in a d-
dimensional Hilbert space with spectral decomposition

7

X = U xU â€ , where the eigenvalues xj are ordered in de-
creasing order. Then

(X) of Eq. (49) is given by

PS
where Î¸ = 1
s

PS
(X) = U Î»U â€ , Î»i = max
{

(cid:80)s

j=1 (xj âˆ’

1) and

xi âˆ’

Î¸, 0

,
}

(52)

s = max

ï£±
ï£²

ï£³

k

[1, ..., d] : xk >

âˆˆ

1
k

k
(cid:88)

j=1

(xj âˆ’

ï£¼
ï£½

ï£¾

1)

.

(53)

Proof. Any quantum (program) state can be written
in the diagonal form Ï€ = V Î»V â€  where V is a unitary
matrix, and Î» is the vector of eigenvalues in decreasing
j Î»j = 1. To ï¬nd the optimal
order, with Î»j â‰¥
state, it is required to ï¬nd both the optimal unitary V
and the optimal eigenvalues Î» with the above property,
i.e.,

0 and (cid:80)

(X) = argmin

V,Î»

X

(cid:107)

âˆ’

V Î»V â€ 

(cid:107)2 .

PS

(54)

For any unitarily-invariant norm, the following inequality
holds [39, Eq. IV.64]

Ï€

âˆ’

âˆ’

(55)

X
(cid:107)

(cid:107)2 ,
Î»

x
(cid:107)2 â‰¥ (cid:107)
with equality when U = V , where X = U xU â€  is a spec-
tral decomposition of X such that the xjâ€™s are in de-
creasing order. This shows that the optimal unitary in
Eq. (54) is the diagonalization matrix of the operator X.
The eigenvalues of any density operator form a probabil-
ity simplex. The optimal eigenvalues Î» are then obtained
thanks to Algorithm 1 from Ref. [18]. (cid:4)

In the following section we present an alternative al-
gorithm with faster convergence rates, but stronger re-
quirements on the function to be optimized.

B. Conjugate gradient method

The conjugate gradient method [16, 34], sometimes
called Frank-Wolfe algorithm, has been developed to pro-
vide better convergence speed and to avoid the projection
step at each iteration. Although the latter can be explic-
itly computed for quantum states (thanks to our The-
orem 3), having a faster convergence rate is important,
especially with higher dimensional Hilbert spaces. The
downside of this method is that it necessarily requires a
diï¬€erentiable cost function C, with gradient

C.

In its standard form, the conjugate gradient method to
C(Ï€) is deï¬ned by

approximate the solution of argminÏ€
the following iterative rule

âˆˆS

âˆ‡

1) Find argminÏƒ
2) Ï€i+1 = Ï€i + 2

âˆˆS
i+2 (Ïƒ

Tr[Ïƒ

C(Ï€i)],
âˆ‡
Ï€i) = i

i+2 Ï€i + 2

i+2 Ïƒ.

âˆ’

(56)

The ï¬rst step in the above iteration rule is solved by ï¬nd-
C(Ï€i). Indeed, since
ing the smallest eigenvector
C is
Ï€ is an operator and C(Ï€) a scalar, the gradient

âˆ‡

of

Ïƒ

(cid:105)

|

âˆ‡

an operator with the same dimension of Ï€. Therefore, for
learning quantum programs we ï¬nd the following itera-
tionfollowing

The previous deï¬nition of the trace distance, C1 in
Eq. (9), is recovered for Âµ
0 and, for any non-zero
â†’
Âµ, the CÂµ bounds C1 as follows

8

|

of

âˆ‡

i+2 |

Ïƒi(cid:105)

C(Ï€i),

1) Find the smallest eigenvalue
i+2 Ï€i + 2
2) Ï€i+1 = i
.
Ïƒi|

Ïƒi(cid:105) (cid:104)
When the gradient of C is Lipschitz continuous with con-
stant L, the conjugate gradient method converges after
(L/(cid:15)) steps [17, 38]. The following iteration with adap-
O
tive learning rate Î±i has even faster convergence rates,
provided that C is strongly convex [37]:

(57)

âˆ‡

(58)

âˆ‡
(cid:105)

C(Ï€i),

Ï„i(cid:107)
âˆ’

[0,1] Î±
âˆˆ

+ Î±2 Î²C
2 (cid:107)
3) Ï€i+1 = (1

of
Ïƒi(cid:105)
1) Find the smallest eigenvalue
|
C(Ï€i)
Ï„i,
2) Find Î±i = argminÎ±
âˆ‡
(cid:104)
2
Ïƒi| âˆ’
C, for Ï„i =
Ïƒi(cid:105)(cid:104)
Ï€i,
|
Ïƒi|
.
Ïƒi(cid:105) (cid:104)
Î±i)Ï€i + Î±i |
(cid:107) Â· (cid:107)C depend on C [37].
where the constant Î²C and norm
In spite of the faster convergence rate, conjugate gra-
dient methods require smooth cost functions (so that the
gradient
C is well deï¬ned at every point). However,
cost functions based on trace distance (9) are not smooth.
For instance, the trace distance in one-dimensional spaces
reduces to the absolute value function
that is non-
analytic at x = 0. When some eigenvalues are close
to zero, conjugate gradient methods may display unex-
pected behaviors, though we have numerically observed
that convergence is always obtained with a careful choice
of the learning rate. Moreover, in the next section we will
show how to formally justify the applicability of the con-
jugate gradient method, following Nesterovâ€™s smoothing
prescription [38].

x
|

|

CÂµ(Ï€)

C1(Ï€)

â‰¤

â‰¤

CÂµ(Ï€) +

Âµd
2

,

(61)

where d is the dimension of the program state Ï€.
Appendix B 2 we then prove the following result

In

Theorem 4 The smooth cost function CÂµ(Ï€) is a convex
function over program states and its gradient is given by

CÂµ(Ï€) = Î›âˆ—[h(cid:48)Âµ(Ï‡Ï€ âˆ’
where h(cid:48)Âµ is the derivative of hÂµ. Moreover, the gradient
is L-Lipschitz continuous with

(62)

)],

âˆ‡

Ï‡

E

L =

d
Âµ

,

(63)

where d is the dimension of the program state.

O

Being Lipschitz continuous, the conjugate gradient al-
gorithm and its variants [37, 38] converge up to an ac-
(L/(cid:15)) steps. In some applications, it is
curacy (cid:15) after
desirable to analyze the convergence in trace distance in
the limit of large program states, namely for d
.
â†’ âˆ
The parameter Âµ can be chosen such that the smooth
trace distance converges to the trace distance, namely
. Indeed, given the inequality (61),
CÂµ â†’
(1+Î·)) for some Î· > 0 so
a possibility is to set Âµ =
O
that, from Eq. (63), the convergence to the trace norm is
(d2+Î·) steps.
achieved after

C1 for d

â†’ âˆ

(dâˆ’

O

C. Smooth trace distance

VI. LEARNING OF ARBITRARY UNITARIES

O

(cid:0) L
(cid:15)

The conjugate gradient method converges to the global
(cid:1) steps, provided that the gradient of
optimum after
C is L-Lipschitz continuous [38]. However, the constant
L can diverge for non-smooth functions like the trace
distance (9) so the convergence of the algorithm cannot
be formally stated, although it may still be observed in
numerical simulations, as we will show. To solidify the
convergence proof (see also Appendix B 2) we introduce a
smooth approximation to the trace distance. This is de-
ï¬ned by the following cost function that is diï¬€erentiable
at every point

CÂµ(Ï€) = Tr [hÂµ (Ï‡Ï€ âˆ’

Ï‡

E

)] =

(cid:88)

j

hÂµ(Î»j) ,

(59)

where Î»j are the eigenvalues of Ï‡Ï€ âˆ’
so-called Huber penalty function

Ï‡

E

and hÂµ is the

The simulation of quantum gates or, more generally,
unitary transformations is crucial for quantum comput-
ing applications [20] so ML techniques have been de-
veloped for this purpose [40â€“43]. Here we consider the
more general setting of simulating an arbitrary ï¬nite-
dimensional unitary U by means of a programmable
quantum processor with map Î›. For a unitary U the
Choi matrix is a maximally-entangled pure state Ï‡
=
Ï‡U (cid:105)(cid:104)
is a one-dimensional
|
projector and Eq. (40) is drastically simpliï¬ed to

. Therefore, âˆšÏ‡

Ï‡U |

= Ï‡

E

E

E

âˆ‡

F (Ï€) =

Ï‡U |
Ï‡U (cid:105)(cid:104)
Î›âˆ— [
|
(cid:112)
Î›(Ï€)
Ï‡U |
|
(cid:104)
Therefore the gradient (39) of the convex cost function
CF ,

]
Ï‡U (cid:105)

(64)

2

.

CF (Ï€) =

Î›âˆ— [

Ï‡U (cid:105)(cid:104)
|

Ï‡U |

] ,

âˆ’

âˆ‡

(65)

hÂµ(x) :=

(cid:40) x2
2Âµ
x
|

| âˆ’

Âµ
2

if
if

x
x

< Âµ ,
Âµ .

|
| â‰¤

|
|

(60)

is independent of Ï€. When we employ the conjugate gra-
dient method, the state
is the same for each iteration
step. This implies that conjugate gradient is converging

Ïƒk(cid:105)
|

Ï‡U |

Ï‡U (cid:105)(cid:104)
Î›âˆ— [
|

towards one eigenvector of
] with minimum
âˆ’
eigenvalue. In other terms, the ï¬xed point of the itera-
tion in Eq. (57), namely the optimal program state ËœÏ€F
(according to the ï¬delity cost function) is pure and equal
to the eigenvector of Î›âˆ— [
] with maximum eigen-
Ï‡U (cid:105)(cid:104)
|
value.
The above result can be proven as follows. Let Ï€1 be
the initial guess for the program state. After k iterations
of Eq. (57), we ï¬nd the following approximation to the
optimal program state

Ï‡U |

Ï€k =

2

k + k2 Ï€1 +

(cid:18)

2
k + k2

1

âˆ’

(cid:19)

ËœÏ€F ,

(66)

where
Ï€k â†’

2

k+k2 = (cid:81)k
ËœÏ€F for k

1
âˆ’
j=1

â†’ âˆ

j
j+2 . The above equation shows that
, with error in trace distance

2
k + k2 (cid:107)

2) .

O

((cid:15)âˆ’

(kâˆ’

(67)

Ï€k âˆ’
(cid:107)

ËœÏ€F (cid:107)1 =

ËœÏ€F (cid:107)1 =

Ï€1 âˆ’
For learning arbitrary unitaries, the ï¬delity cost func-
tion provides a convenient choice where the optimal pro-
gram can be found analytically. Moreover, this example
1) of the conjugate
shows that the convergence rate
method provides a worst case instance that can be beaten
in some applications with some suitable cost functions.
2 for learning arbitrary
From Eq. (67) we see that (cid:15) = kâˆ’
unitaries via the minimization of CF , meaning that con-
1/2). On
vergence is obtained with the faster rate
the other hand, there are no obvious simpliï¬cations for
the optimization of the trace distance, since the latter
still requires the diagonalization of Eq. (46). For the
trace distance, or its smooth version, only numerical ap-
proaches are feasible.

((cid:15)âˆ’

O

O

VII. TELEPORTATION PROCESSOR

Ã—

One possible (shallow) design for the quantum proces-
sor Q is the teleportation protocol [44] which has to be
applied to a generic program state Ï€ instead of a maxi-
mally entangled state. In dimension d, the program Ï€AB
is a d
d state. The teleportation protocol involves a ba-
sis of d2 maximally entangled states
Ui}
of teleportation unitary such that Tr(U â€ i Uj) = dÎ´ij [45].
In the protocol, an input d-dimensional state ÏS and the
A part of the program Ï€AB are subject to the projector
. The classical outcome i is communicated to the
Î¦i(cid:105)(cid:104)
|
1
B part of Ï€AB where the correction U âˆ’
is applied. In
i
this way, we implement the following teleportation chan-
nel

and a basis

Î¦i(cid:105)

Î¦i|

{

|

EÏ€ from qudit S to qudit B
U B
EÏ€(Ï) =
i (cid:104)

Î¦SA
i

ÏS
|

(cid:88)

âŠ—

i

Ï€AB

Î¦SA
i
|

U B
â€ i
(cid:105)

.

(68)

The Choi matrix of the teleportation channel

EÏ€ can
be written as Ï‡Ï€ = Î›tele(Ï€), where the map of the tele-
portation processor is equal to

Î›tele(Ï€) =

1
d2

(cid:88)

i

(U âˆ—i âŠ—

Ui) Ï€ (U âˆ—i âŠ—

Ui)â€  .

(69)

9

FIG. 3. Optimization of program states for simulating
the rotation R(Î¸) = eiÎ¸X with a teleportation processor.
The optimization is via the minimization of trace distance
C1 of Eq. (9) with the projected subgradient method in
Eq. (50). The dashed lines correspond to the upper bound
(cid:112)1 âˆ’ F [Î›(ËœÏ€F ), Ï‡E ]2 of the trace distance, where ËœÏ€F is the op-
timal program that maximizes the ï¬delity, namely the eigen-
vector of Eq. (64) with maximum eigenvalue.

Note that, if the program Ï€ is teleportation covariant [21],
Ui] = 0, then Ï€ is automatically a
namely if [Ï€, U âˆ—i âŠ—
ï¬xed point of the map, i.e., we have Ï‡Ï€ := Î›tele(Ï€) = Ï€.
Also note that, the channel in Eq. (69) is self-dual, i.e.,
Î›âˆ— = Î›. As a result, for any operator Ë†O, we may write

Î›âˆ—tele( Ë†O) =

1
d2

(cid:88)

i

(U âˆ—i âŠ—

Ui) Ë†O (U âˆ—i âŠ—

Ui)â€  .

(70)

As an example, assume that the target channel is a
Ï‡U |
Ï‡U (cid:105)(cid:104)
|
is maximally entangled.
, we may
(cid:105)

unitary U , so that its Choi matrix is Ï‡U :=
Î¦
with
= 11
(cid:105)
|
Î¦
11
By using Eq. (70) and U âˆ—
|
âŠ—
write the dual processor map

Ï‡U (cid:105)
|

= 11

and

U â€ 

âŠ—

âŠ—

Î¦

Î¦

U

(cid:105)

(cid:105)

|

|

Î›âˆ—tele[
|
1
d2

=

]
Ï‡U |
Ï‡U (cid:105)(cid:104)
(cid:88)
(cid:0)11

i

(cid:1)

V U
i

Î¦
|

Î¦

(cid:105)(cid:104)

|

(cid:0)11

âŠ—

V U
i

(cid:1)â€  ,

(71)

âŠ—

Ï‡U |

where V U
i = UiU U â€ i . The maximum eigenvector of
] represents the optimal program state ËœÏ€F
Ï‡U (cid:105)(cid:104)
Î›âˆ—tele[
|
for simulating the unitary U via the teleportation pro-
cessor (according to the ï¬delity cost function). In some
cases, the solution is immediate. For instance, this hap-
pens when V U
U is independent of i. This is the case
when U is a teleportation unitary, because it satisï¬es the
Weyl-Heysenberg algebra [21]. For a teleportation uni-
tary U , we simply have

i âˆ

Ï‡U (cid:105)(cid:104)
so that the unique optimal program is ËœÏ€F =

Ï‡U (cid:105)(cid:104)

Î›âˆ—[
|

Ï‡U |

Ï‡U |

] =

|

,

(72)

Ï‡U (cid:105)(cid:104)
|

.
Ï‡U |

In Fig. 3 we show the convergence of the projected
subgradient algorithm using the teleportation processor

and target unitaries R(Î¸) = eiÎ¸X , for diï¬€erent values of
Î¸. When Î¸ is a multiple of Ï€/2, then the above unitary
is teleportation covariant and the Frank-Wolfe algorithm
converges to zero trace distance. For other values of Î¸
perfect simulation is impossible, and we notice that the
algorithm converges to a non zero value of the trace dis-
tance (9). For comparison, in Fig. 3 we also plot the value
]2, where
of the ï¬delity upper bound
ËœÏ€F is the optimal program that maximizes the ï¬delity of
Eq. (17), namely the eigenvector of Eq. (71) with max-
imum eigenvalue. We note that for Î¸ = Ï€/2(cid:96) the trace
is
distance decreases for larger Î¸. The limit case (cid:96)
â†’ âˆ
perfectly simulable as R(0) is teleportation covariant.

F [Î›(ËœÏ€F ), Ï‡
E

(cid:112)
1

âˆ’

A. Pauli channel simulation

Pauli channels are deï¬ned as [1]

(Ï) =

P

(cid:88)

i

piUiÏU â€ i

,

(73)

where Ui are generalized Pauli operators and pi some
probabilities. For d = 2 the Pauli operators are the four
Pauli matrices I, X, Y, Z and in any dimension they form
the Weyl-Heisenberg group [1]. These operators are ex-
actly the teleportation unitaries Uj deï¬ned in the previ-
ous section. The Choi matrix Ï‡
is
diagonal in the Bell basis, i.e., we have

of a Pauli channel

P

P

=

Ï‡

P

(cid:88)

i

pi|

Î¦i(cid:105)(cid:104)

Î¦i|

,

(74)

and

Î¦
Ui|

where Î¦i = 11

Î¦
(cid:105)
|
We now consider the simulation of a Pauli channel with
the teleportation quantum processor introduced in the
previous section. Let

j=1 |

jj

âŠ—

(cid:105)

(cid:105)

/âˆšd.

= (cid:80)d

10

FIG. 4. PBT scheme. Two distant parties, Alice and Bob,
share N maximally entangled pairs {Ak, Bk}N
k=1. Alice also
has another system C in the state |Ïˆ(cid:105). To teleport C,
Alice performs the POVM Î AC
on all her local systems
A = {Ak}N
k=1 and C. She then communicates the outcome i
to Bob. Bob discards all his systems B = {Bk}N
k=1 with the
exception of Bi. After these steps, the state |Ïˆ(cid:105) is approx-
imately teleported to Bi. Similarly, an arbitrary channel E
is simulated with N copies of the Choi matrix Ï‡AkBk
. The
ï¬gure shows an example with N = 5, where i = 4 is selected.

E

i

proven [46, 47] that these are the only channels we can
perfectly simulate. This is true even if we apply the Pauli
corrections in a probabilistic way, i.e., we assume a classi-
cal channel from the Bell outcomes to the corresponding
label of the Pauli correction operator [47].

Ï€ =

(cid:88)

ij

Ï€ij|

Î¦i(cid:105)(cid:104)

Î¦j|

,

(75)

be an arbitrary program state expanded in the Bell ba-
sis. For any program state, the Choi matrix of the
teleportation-simulated channel is given by Eq. (69). Us-
ing standard properties of the Pauli matrices we ï¬nd

Ï‡Ï€ â‰¡

Î›(Ï€) =

(cid:88)

i

Ï€ii|

Î¦i(cid:105)(cid:104)

Î¦i|

,

(76)

namely a generic state is transformed into a Bell diagonal
state. Therefore, the cost function

VIII. PORT-BASED TELEPORTATION

We now study a design of programmable quantum pro-
cessor that can potentially simulate any target quantum
channel in the asymptotic limit of an arbitrarily large
program state. This design is PBT [11â€“13], a general-
ization of the standard teleportation scheme. For ï¬nite-
dimensional programs, a PBT processor cannot achieve
a perfect deterministic simulation of an arbitrary chan-
nel [10]. In this realistic ï¬nite-dimensional setting, our
study ï¬nally establishes the optimal performance achiev-
able by this type of quantum processor.

C Pauli
1

=

Ï‡P âˆ’
can be minimized analytically for any Pauli channel by
choosing Ï€ij = piÎ´ij. With this choice we ï¬nd C Pauli
= 0,
meaning that the simulation is perfect.

Ï‡Ï€(cid:107)1 ,

(77)

(cid:107)

1

From theory [46â€“48] we know that only Pauli chan-
nels can be perfectly simulated in this way. No mat-
ter how more general we can make the states Ï€, it is

A. Basics of PBT

The overall protocol of PBT is illustrated in Fig. 4. Un-
like standard teleportation protocol, PBT requires that
Alice and Bob share N entangled pairs for the simulation
of the identity channel [11]. The protocol is based on a re-
source state (the program) given by Ï€AB = (cid:78)N
k=1 Î¦AkBk ,

AliceBobdiscardBkk6=i|ÏˆiCâ‰ˆ|ÏˆiA1B1A2B2A3B3A4B4A5B5(cid:7)iÎ ACi|

Î¦AkBk (cid:105)

are Bell states for Aliceâ€™s N qudits A =
where
(A1, . . . , AN ) and Bobâ€™s N qudits B = (B1, . . . , BN ). Af-
ter preparing such a state, Alice performs a joint positive-
on her A-half of
operator value measure (POVM)
Ï€AB and an input state
(cid:105)C that she wishes to teleport.
She communicates the outcome i to Bob, who discards all
â€œportsâ€ B except Bi = Bout. The resulting PBT channel
PÏ€ :

Î i}
{

Ïˆ
|

HC (cid:55)â†’ HBout is then
N
(cid:88)

PÏ€(Ï) =

Tr
A Â¯BiC

[Î i(Ï€AB

ÏC)]Bi

âŠ—

Bout

â†’

(78)

i=1

N
(cid:88)

i=1

=

Tr
A Â¯BiC

(cid:104)(cid:112)

Î i(Ï€AB

(cid:105)

(cid:112)

Î i

ÏC)

âŠ—

,

Bi

â†’

Bout

where Â¯Bi = B
Bi =
\
PBT approximates an identity channel

Bk : k
{

. In the limit N
= i
}
Ï.
PÏ€(Ï)

â‰ˆ

In the standard PBT protocol [11, 12] the following

,
â†’ âˆ

POVM is used

Î i = ËœÎ i +

(cid:32)

11

âˆ’

1
N

(cid:88)

k

(cid:33)

ËœÎ k

,

where

1/2

ËœÎ i = Ïƒâˆ’
AC Î¦AiCÏƒâˆ’
N
(cid:88)

1/2
AC ,

ÏƒAC :=

Î¦AiC,

(79)

(80)

(81)

i=1

1/2 is an operator deï¬ned only on the support of
and Ïƒâˆ’
Ïƒ. The PBT protocol is formulated for N
2 ports.
However, we also include here the trivial case for N = 1,
corresponding to the process where Aliceâ€™s input is traced
out and the output is the reduced state of Bobâ€™s port, i.e.,
a maximally mixed state.

â‰¥

With the choice of the POVM in Eq. (79), the identity
can be simulated with ï¬delity [11, 13]

channel

I

FÏ€ = 1

âˆ’ O

(cid:19)

(cid:18) 1
N

,

(82)

so perfect simulation is possible only in the limit N
.
â†’ âˆ
More generally, it has been shown [14] that simulation
error in diamond norm scales as

(cid:107)I âˆ’ PÏ€(cid:107)(cid:5) â‰¤

2d(d
N

âˆ’

1)

.

(83)

B. Channel simulation via PBT

E

E

between

Any generic channel

can be written as a composition
. Channel sim-
and the identity channel
E â—¦I
ulation can be achieved by replacing the identity channel
to Bi.
I
However, since Bob does not perform any post-processing
on its systems B, aside from discarding all ports Bk with
N to all his
k

= i, he can also apply ï¬rst the channel

PÏ€, and then applying

with its PBT simulation

I

E

âŠ—

E

11

ports and then discard all the ports Bk with k
doing so, he changes the program state to

= i. In

Ï€AB = 11A âŠ— E

âŠ—
B

(cid:34) N
(cid:79)

N

k=1

(cid:35)

Î¦AkBk

=

N
(cid:79)

k=1

Ï‡AkBk
E

.

(84)

E

In other terms, any channel
can be PBT-approximated
as program state. How-
by N copies of its Choi matrix Ï‡
E
ever, while such a program state is optimal when N
,
â†’ âˆ
for ï¬nite N there may be better alternatives. In general,
for any ï¬nite N , ï¬nding the optimal program state Ï€AB
with PBT is an open problem,
simulating a channel
and no explicit solutions or procedures are known.

E

We employ our convex optimization procedures to ï¬nd
the optimal program state. This can be done either ex-
actly by minimizing the diamond distance cost function
C
via SDP, or approximately, by determining the opti-
(cid:5)
mal program state via the minimization of the trace dis-
tance cost function C1 via the gradient-based ML tech-
niques discussed above. For this second approach, we
need to derive the map Î› of the PBT processor, between
the program state Ï€ to output Choi matrix as in Eq. (8).
From the deï¬nition in Eq. (78) we ï¬nd the following op-
erator sum decomposition

Î›(Ï€) = Ï‡

Ï€ = 11D âŠ— PÏ€[Î¦DC]
(cid:104)(cid:112)

P
N
(cid:88)

Î i(Ï€AB

Tr
A Â¯BiC

(cid:112)

Î¦DC)

Î i

(cid:105)

Bi

â†’

Bout

âŠ—

KikÏ€K â€ ik ,

(85)

=

=

i=1
(cid:88)

ik

where the corresponding Kraus operators are

K AB
ik

â†’

DBout

=

(cid:112)

e(i)
k |
(cid:104)

Î i âŠ—

11BD|

Î¦DC(cid:105)

,

(86)

and

e(i)
k (cid:105)
|

span a basis of A Â¯BiC.

C. Program state compression

The program state grows exponentially with the num-
ber of ports N as d2N where d is the dimension of the
Hilbert space. However, as also discussed in the origi-
nal proposal [11, 12] and more recently in Ref. [49], the
resource state of PBT can be chosen with extra sym-
metries, so as to reduce the number of free parameters.
In particular, we may consider the set of program states
that are symmetric under the exchange of ports, i.e., such
that rearranging any A modes and the corresponding B
modes leaves the program state unchanged.

Let Ps be the permutation operator swapping labels 1
to N for the labels in the sequence s, which contains all
the numbers 1 to N once each in some permuted order.
Namely Ps exchanges all ports according to the rule i
(cid:55)â†’
si. Since PBT is symmetric under exchange of ports, we
may write

PPsÏ€P â€ 

s

(Ï) =

PÏ€ (Ï) for any s.

(87)

(cid:54)
(cid:54)
(cid:54)
Consider then an arbitrary permutation-symmetric re-
source state Ï€sym as

When the program state Ï€ = Ï‡âŠ—

N is directly used in

Eq. (85) we ï¬nd

12

Ï€sym =

1
N !

(cid:88)

s

PsÏ€P â€ s ,

where the sum is over all possible sequences s that deï¬ne
independent permutations and N ! is the total number
of possible permutations. Clearly
PÏ€, so any
program state gives the same PBT channel as some sym-
metric program state. It therefore suï¬ƒces to consider the
set of symmetric program states. This is a convex set:
any linear combination of symmetric states is a symmet-
ric state.

PÏ€sym =

|

x

To construct a basis of the symmetric space, we note
that each element of a density matrix is the coeï¬ƒcient
). If permutation of labels
y
of a dyadic (of the form
|
(cid:105) (cid:104)
maps one dyadic to another, the coeï¬ƒcients must be the
same. This allows us to constrain our density matrix
using fewer global parameters. For instance, for d = 2 we
can deï¬ne the 16 parameters n00,00, n00,01, n00,10, etc.,
corresponding to the number of ports in the dyadic of the
form
, etc.
Each element of a symmetric density matrix can then
be deï¬ned solely in terms of these parameters, i.e., all
elements corresponding to dyadics with the same values
of these parameters have the same value.

0A0B(cid:105) (cid:104)
|

0A0B(cid:105) (cid:104)
|

0A0B(cid:105) (cid:104)

,
0A0B|

1A0B|

0A1B|

,

|

cA, dB|

aA, bB(cid:105)(cid:104)
|

For the general qudit case, in which our program state
consists of N ports, each composed of two d-dimensional
qudits, we can ï¬nd the number of independent parame-
ters from the number of independent dyadics. Each port
in a dyadic can be written as
where the
extra indices A and B describe whether those states are
modeling either qudit A or B. There are d4 diï¬€erent
combinations of
, so we can place each qudit
a, b, c, d
}
{
into one of d4 categories based on these values. If two el-
ements in the density matrix correspond to dyadics with
the same number of ports in each category, they must
take the same value. Hence, the number of independent
coeï¬ƒcients is given by the number of ways of placing
N (identical) ports into d4 (distinguishable) categories.
This is exactly the binomial coeï¬ƒcient
(cid:18)N + d4
d4

(N d4

(88)

1) .

(cid:19)
1

âˆ’
1

=

âˆ’

O

âˆ’

Consequently, exploiting permutation symmetry of the
PBT protocol, we can exponentially reduce the number
of parameters for the optimization over program states.
The number of parameters can be reduced even further
by considering products of Choi matrices. We may focus
indeed on the Choi set

(cid:40)

CN =

Ï€ : Ï€ =

(cid:41)

N

pkÏ‡âŠ—
k

,

(cid:88)

k

where each Ï‡k = Ï‡k
AB is a generic Choi matrix, therefore
111, and pk form a probability dis-
satisfying TrBÏ‡k = dâˆ’
tribution. Clearly
is a convex set. We now show that
this set can be further reduced to just considering N = 1.

C

Î›(Ï€) =

N
(cid:88)

i=1

TrA Â¯BiC

(cid:2)Î i

(cid:0)Ï‡âŠ—

N
AB âŠ—

Î¦DC

(cid:1)(cid:3)

Bi

â†’

Bout

=

1
dN

âˆ’

1

N
(cid:88)

i=1

:= ËœÎ›(Ï‡) ,

TrAiC [Î i (Ï‡AiBout âŠ—

Î¦DC)]

(90)

(91)

(92)

namely that the optimization can be reduced to the
(d4) dimensional space of Choi matrices Ï‡. Note that,

O
in the above equation, we used the identity

N

Tr Â¯BiÏ‡âŠ—

AB = Ï‡AiBi âŠ—

11 Â¯Ai
dN

âˆ’

1 ,

(93)

where Â¯Ai = A

Ai.

\

Now let Ï€ be a linear combination of tensor products
, each with probability pk as

of Choi matrix states, Ï‡âŠ—
k
in Eq. (89). Then we can write

N

Tr Â¯BiÏ€AB = Tr Â¯Bi

(cid:88)

N

pkÏ‡âŠ—
k

k
(cid:18)

(cid:88)

=

pk

k

Ï‡k

AiBi âŠ—

(cid:19)

.

11 Â¯Ai
1
dN

âˆ’

(94)

(95)

N of some other Choi matrix Ï‡(cid:48) = (cid:80)

However, this is precisely the partial trace over the tensor
k pkÏ‡k.
product Ï‡(cid:48)âŠ—
Hence, the program state Ï€ = (cid:80)
simulates the
same channel as the resource state Ï€(cid:48) = ((cid:80)
k pkÏ‡k)âŠ—

N .
CN can
be reduced to the optimization over products of Choi
N . From Eq. (92) this can be further reduced
matrices Ï‡âŠ—
to the optimization of the quantum channel ËœÎ› over the
convex set of single-copy Choi matrices Ï‡

Therefore, the optimization over the convex set

k pkÏ‡âŠ—
k

N

Ï€ : Ï€ = Ï‡AB, TrBÏ‡AB = 11/2
C1 =
{
}
(d4). Using

O

C1 drastically reduces the diï¬ƒculty
which is
of numerical simulations, thus allowing the exploration of
signiï¬cantly larger values of N . Details on how to explic-
itly construct ËœÎ› for d = 2 are presented in Appendix C.

,

(96)

D. Numerical examples

We ï¬rst consider the simulation of an amplitude damp-
i K AD
, which is deï¬ned

i ÏK AD

ing channel
by the Kraus operators

EAD(Ï) = (cid:80)

â€ 

i

(89)

K AD

0 =

(cid:18)1

0
0 âˆš1

âˆ’

(cid:19)

p

, K AD

1 =

(cid:19)
(cid:18)0 âˆšp
0
0

.

(97)

In Fig. 5 we study the performance of the PBT simula-
tion of the amplitude damping channel
EAD for diï¬€erent
choices of p. For p = 0 the amplitude damping is equal to

13

FIG. 5. PBT Simulation of the amplitude damping channel
EAD for various damping rates p. Minimization of the trace
distance C1(EAD, Ï€) = (cid:107)Ï‡EAD âˆ’Ï‡Ï€(cid:107)1 between the target chan-
nelâ€™s Choi matrix and its PBT simulation with program state
Ï€, for diï¬€erent number of ports N . We consider N = 1, 2, 3
and two kinds of programs: copies of the channelâ€™s Choi ma-
trix Ï‡âŠ—N
and the state ËœÏ€1 obtained from the minimization
EAD
of C1 via the projected subgradient (PS) method after 200
iterations. Note that the simulation error C1 is maximum for
the identity channel (p = 0) and goes to zero for p â†’ 1.

FIG. 6. PBT Simulation of the amplitude damping channel
EAD for various damping rates p. We plot the diamond dis-
tance cost function C(cid:5)(EAD, Ï€) = (cid:107)EAD âˆ’EAD,Ï€(cid:107)(cid:5) between the
target channel EAD and its PBT simulation EAD,Ï€ with pro-
gram state Ï€. In particular, for the program state we compare
the naive choice of the channelâ€™s Choi matrix Ï€ = Ï‡âŠ—N
(dot-
EAD
ted lines) with the SDP minimization over the set of generic
Choi matrices Ï€ = Ï‡âŠ—N (solid lines). Diï¬€erent values of
N = 2, . . . , 6 and N = 20 are shown.

0
|

the identity channel, while for p = 1 it is a â€œresetâ€ chan-
nel sending all states to
. We compare the simulation
(cid:105)
error with program states Ï€ either made by products of
N
the channelâ€™s Choi matrix Ï‡âŠ—
AD as in Eq. (84) or obtained
E
from the minimization of the trace distance cost func-
tion of Eq. (9) with the projected subgradient iteration
in Eq. (50). Alternative methods, like the conjugated
gradient algorithm, perform similarly for this channel.
We observe that, surprisingly, the optimal program ËœÏ€1
obtained by minimizing the trace distance C1 is always
N
AD.
better than the natural choice Ï‡âŠ—
E

In Fig. 6 we study the PBT simulation of the ampli-
tude damping channel by considering the subset of pro-
N which is made of tensor products
gram states Ï€ = Ï‡âŠ—
of the 4
4 generic Choi matrices Ï‡ (therefore satisfy-
ing Tr2Ï‡ = 11/2). As discussed in previous Sec. VIII C,
this is equivalent to optimizing over the Choi set
CN and
it practically reduces to the convex optimization of the
channel ËœÎ› over the generic single-copy Choi matrix Ï‡.
Moreover, ËœÎ› itself can be simpliï¬ed, as shown in Ap-
pendix C, so the all operations depend polynomially on
the number N of ports. This allows us to numerically ex-
plore much larger values of N , even for the minimization
. In Fig. 6 the dotted lines correspond to the value
of C
(cid:5)
N
AD is employed, where
of C
when the program Ï€ = Ï‡âŠ—
(cid:5)
E
Ï‡
AD is the channelâ€™s Choi matrix. As Fig. 6 shows, the
E
may be signiï¬cantly smaller with an optimal Ï‡,
cost C
(cid:5)
thus showing that the optimal program may be diï¬€erent
from the channelâ€™s Choi matrix, especially when p is far
from the two boundaries p = 0 and p = 1.

Ã—

As an another example, we consider the simulation of

the depolarizing channel deï¬ned by

Edep(Ï) = (1

âˆ’

p)Ï +

p
d

11.

(98)

In Fig. 7 we study the performance of PBT simulation
of the depolarizing channel in terms of p. For p = 0
the depolarizing channel is equal to the identity chan-
nel, while for p = 1 it sends all states to the maximally
mixed state. Again we compare the simulation error with
program states either made copies of the channelâ€™s Choi
N
matrices Ï‡âŠ—
dep or obtained from the minimization of C1
E
with the conjugate gradient method of Eq. (57), which
performs signiï¬cantly better than the projected subgra-
dient for this channel. Also for the depolarizing channel
we observe that, for any ï¬nite N , we obtain a lower er-
ror by optimizing over the program states instead of the
N
dep .
naive choice Ï‡âŠ—
E

Finally, in Fig. 8 we study the PBT simulation of a uni-
tary gate UÎ¸ = eiÎ¸X for diï¬€erent values of Î¸. Unlike the
previous non-unitary channels, in Fig. 8 we observe a ï¬‚at
error where diï¬€erent unitaries have the same simulation
error of the identity channel Î¸ = 0. This is expected be-
cause both the trace distance and the diamond distance
are invariant under unitary transformations. In general,
we have the following.

Proposition 5 Given a unitary
PBT simulation

U
UÏ€ with program Ï€ we may write

(Ï) = U ÏU â€  and its

min
Ï€ ||U âˆ’ UÏ€||(cid:5)

= min

Ï€ ||I âˆ’ IÏ€||(cid:5)

,

(99)

0.00.20.40.60.8C1(EAD,Ï€)0.00.20.40.60.81.0pChoiN=1ChoiN=2ChoiN=3PS200N=1PS200N=2PS200N=30.00.20.40.60.81.01.2C(cid:5)(EAD,Ï‡âŠ—N)0.00.20.40.60.81.0pN=2N=3N=4N=5N=6N=2014

FIG. 8. PBT Simulation of the unitary gate UÎ¸ = eiÎ¸X for
diï¬€erent angles Î¸, where X is the bit-ï¬‚ip Pauli matrix. Trace
distance C1(UÎ¸, Ï€) = (cid:107)Ï‡UÎ¸ âˆ’ Ï‡Ï€(cid:107)1 between the target Choi
matrix of the unitary and its PBT simulation with program
state Ï€, for diï¬€erent number of ports N . We consider N =
1, 2, 3 and two kinds of programs: copies of the Choi matrix
of the unitary Ï‡âŠ—N
and the program state ËœÏ€1 obtained from
UÎ¸
the minimization of C1 via the projected subgradient (PS)
method after 200 iterations.

FIG. 7. PBT Simulation of the qubit depolarizing chan-
nel versus probability of depolarizing p. Trace distance
C1(Edep, Ï€) = (cid:107)Ï‡Edep âˆ’ Ï‡Ï€(cid:107)1 between the target channelâ€™s
Choi matrix and its PBT simulation with program state Ï€,
for diï¬€erent number of ports N . We consider N = 1, 2, 3
and two kinds of programs: copies of the channelâ€™s Choi ma-
trix Ï€ = Ï‡âŠ—N
and the optimal program state ËœÏ€1 obtained
Edep
from the minimization of C1 via the conjugate gradient (CG)
method after 200 iterations. Note that the simulation error
C1 is maximum for the identity channel (p = 0) and even-
tually goes to zero for a ï¬nite value of p that decreases for
increasing N .

where

IÏ€ is the PBT simulation of the identity channel.

Proof. In fact, we simultaneously prove

min
Ï€ ||I âˆ’ IÏ€||(cid:5)

(1)

â‰¤

min
Ï€ ||U âˆ’ UÏ€||(cid:5)

(2)

â‰¤

min
Ï€ ||I âˆ’ IÏ€||(cid:5)

,

(100)
1

1

1

âˆ’

âˆ’

âˆ’

âˆ’

âˆ’

I

U

=

=

1)âŠ—

UÏ€||(cid:5)

UÏ€||(cid:5)
(

where (1) comes from the fact that
and

||I âˆ’ U
U
PBT simulation of the identity
N (Ï€) once

||U âˆ’UÏ€||(cid:5)
||U
U âˆ’
1
UÏ€ is a possible
âˆ’
U
with program state
1 is swapped with the ï¬lter-
I âŠ—
ing of the ports; then (2) comes from the fact that the
composition
U â—¦ IÏ€ is a possible simulation of the uni-
N (Ï€) and we have the
tary
U
inequality

. (cid:4)
for diï¬€erent values of N is
plotted in Fig. 9 where numerical values are obtained
from SDP, while the upper bound is given by Eq. (83).

||U â—¦ I âˆ’ U â—¦ IÏ€||(cid:5) â‰¤ ||I âˆ’ IÏ€||(cid:5)

with program state

The scaling of

||I âˆ’ IÏ€||(cid:5)

I âŠ— U

U

âŠ—

FIG. 9. PBT Simulation of the identity channel for diï¬€erent
number of ports N . For the identity channel the optimal
Choi matrix coincides with the channelâ€™s Choi matrix Ï‡I.
The optimal Ï€ has been obtained by minimising C(cid:5) via SDP.
The upper bound corresponds to Eq. (83).

IX. PARAMETRIC QUANTUM CIRCUITS

A. Basic idea

We now study another design of universal quantum
processor that can simulate any target quantum channel
in the asymptotic limit of an arbitrarily large program
state. This is based on a suitable reformulation of the
PQCs, which are known to simulate any quantum com-
putation with a limited set of quantum gates [20, 50].

A PQC is composed of a sequence of unitary matrices
Uj(Î¸j), each depending on a classical parameter Î¸. The
resulting unitary operation is then

U (Î¸) = UN (Î¸N ) . . . U2(Î¸2)U1(Î¸1).

(101)

0.00.20.40.60.8C1(EDepo,Ï€)0.00.20.40.60.81.0pChoiN=1ChoiN=2ChoiN=3CG200N=1CG200N=2CG200N=30.00.20.40.60.8C1(UÎ¸,Ï€)0Ï€2Ï€3Ï€22Ï€Î¸ChoiN=1ChoiN=2ChoiN=3PS200N=1PS200N=2PS200N=30.00.40.81.21.62.0C(cid:5)(I,Ï€)48121620NÏ€=Ï‡âŠ—NIbound2d(dâˆ’1)Nâˆ’1optimalÏ€15

where U (Î¸) is deï¬ned in Eq. (101). The parametric quan-
tum processor QÏ€ in Eq. (103) is capable of simulating
any parametric quantum channels, but it is more gen-
eral, as it allows entangled quantum parameters and also
parameters in quantum superposition.

An equivalent measurement-based protocol is obtained
by performing the trace in Eq. (109) over the basis
Î¸1, . . . , Î¸N (cid:105)
|
QÏ€(Î¸) =

U (Î¸)ÏU (Î¸)â€ 

, so that

(cid:88)

,

Ï€
Î¸1, . . . , Î¸N |
(cid:104)

Î¸1, . . . , Î¸N (cid:105)

|

(105)
where U (Î¸) is deï¬ned in Eq. (101). In this alternative,
yet equivalent formulation, at a certain iteration j, the
processor measures the qubit register Rj. Depending
on the measurement outcome Î¸j, the processor then ap-
plies a diï¬€erent unitary U (Î¸j) on the system. However,
in this formulation the program state
is destroyed
after each channel use. From Eq. (105) we note that
QÏ€ depends on Ï€ only via the probability distribution
. As such any advantage in us-
Ï€
Î¸1, . . . , Î¸N |
(cid:104)
ing quantum states can only come from the capability of
quantum systems to model computationally hard proba-
bility distributions [52].

Î¸1, . . . , Î¸N (cid:105)
|

Ï€

(cid:105)

|

Î¸j

{

}

C. Universal channel simulation via PQCs

The universality of PQCs can be employed for univer-
sal channel simulation. Indeed, thanks to Stinespringâ€™s
dilation theorem, any channel can be written as a unitary
evolution on a bigger space, where the system is paired
to an extra register R0

(ÏA) = TrR0 [U (ÏA âŠ—

E

Î¸0)U â€ ],

(106)

where Î¸0 belongs to R0, and U acts on system A and
register R0. In Ref. [50] it was shown that two quantum
gates are universal for quantum computation. Speciï¬-
cally, given U0 = eit0H0 and UB = eit1H1 for ï¬xed times
ti and Hamiltonians Hj, it is possible to write any uni-
tary as

FIG. 10. Convex reformulation of a PQC as a coherent pro-
grammable quantum processor that applies a sequence of con-
ditional gates as in Eq. (102) depending on the program state
|Ï€(cid:105) = |Î¸1, . . . , Î¸N (cid:105). The program state is not destroyed and
can be reused.

A convenient choice is via Uj(Î¸j) = exp(iÎ¸jHj), where
each elementary gate corresponds to a SchrÂ¨odinger evo-
lution with Hamiltonian Hj for a certain time inter-
val Î¸j. For certain choices of Hj and suitably large N
the above circuit is universal [20], namely any unitary
can be obtained with U (Î¸) and a suitable choice of Î¸j.
The optimal parameters can be found with numerical
algorithms [51], e.g. by minimizing the cost function
C(Î¸) =
. However, the above cost func-
Tr[U â€ targetU (Î¸)]
|
tion is not convex, so the numerical algorithms are not
guaranteed to converge to the global optimum.

|

As a ï¬rst step, we show that the task of learning the
optimal parameters in a PQC can be transformed into
a convex optimization problem by using a quantum pro-
gram. This allows us to use SDP and gradient-based ML
methods for ï¬nding the global optimum solution.

B. Convex reformulation

Consider a program state

composed
Î¸1, . . . , Î¸N (cid:105)
by N registers Rj, each in a separable state
. We
Î¸j(cid:105)
|
can transform the classical parameters in Eq. (101) into
quantum parameters via the conditional gates

Ï€
|

=

(cid:105)

|

Ë†Uj = exp

ï£«
ï£­iHj âŠ—

(cid:88)

Î¸j

ï£¶

that acts non-trivially on system and register Rj. If the
parameters Î¸j are continuous, then we can replace the
sum with an integral. With the above gates we deï¬ne
the parametric quantum channel

Î¸j|

Î¸j(cid:105)(cid:104)

ï£¸ ,

Î¸j|

(102)

U

â‰ˆ Â· Â· Â·

1 U m3
U m4

0 U m2

1 U m1

0

,

(107)

for some integers mj. Under suitable conditions, it was
shown that with M = (cid:80)
d) it is possible
j mj =
to approximate any unitary U with a precision (cid:15). More
precisely, the conditions are the following

(d2(cid:15)âˆ’

O

QÏ€(Ï) = TrR

ï£®

ï£°

N
(cid:89)

j=1

Ë†Uj (Ï

Ï€)

âŠ—

ï£¹

Ë†Uj

â€ 

ï£» ,

N
(cid:89)

j=1

(103)

i) The Hamiltonians H0 and H1 are generators of the
full Lie algebra, namely H0, H1 and their repeated
commutators generate all the elements of su(d).

Ïˆ
whose action on a generic state
(cid:105)
|
For a pure separable program
=
Ï€
(cid:105)
|
tain the standard result, i.e.,

is shown in Fig. 10.
, we ob-
Î¸1, . . . , Î¸N (cid:105)
|

Q
Î¸1,...,Î¸N
|

(cid:105)

(Ï) = U (Î¸)ÏU (Î¸)â€ ,

(104)

ii) The eigenvalues of U0 and U1 have phases that are

irrationally related to Ï€.

The decomposition in Eq. (107) is a particular case of
Eq. (101) where Î¸j can only take binary values Î¸j = 0, 1.

|Ïˆ(cid:105)U(Î¸1)U(Î¸2)U(Î¸3)U(Î¸4)U(Î¸5)|Î¸1(cid:105)â€¢|Î¸2(cid:105)â€¢|Î¸3(cid:105)â€¢|Î¸4(cid:105)â€¢|Î¸5(cid:105)â€¢16

following Theorem 2. When we are interested in simu-
lating a unitary channel U via the quantum ï¬delity, then
following the results of Section VI, the corresponding op-
timal program ËœÏ€F is simply the eigenvector Î›âˆ—[
]
Ï‡U |
Ï‡U (cid:105)(cid:104)
|
with maximum eigenvalue, where
. Note
Î¦
U
(cid:105)
|
Ï‡U |
Ï‡U (cid:105)(cid:104)
also that Î›âˆ—[
|
Ï‡U |BA âŠ—
Z = (
(cid:104)

Ï‡U (cid:105)
] = Z â€ Z where

11R) Ë†UAR (
|

Î¦BA(cid:105) âŠ—

(114)

11R) ,

= 11

âŠ—

|

so the optimal program ËœÏ€F is the principal component of
Z. Since there are quantum algorithms for principal com-
ponent analysis [53], the optimization may be eï¬ƒciently
performed on a quantum computer.

D. Numerical examples

As an example we study the simulation of an amplitude
damping channel, with Kraus operators in Eq. (97). A
possible Stinespring dilation for this channel is obtained
with

and

=

Î¸0(cid:105)
|

U =

0
|
ï£«

(cid:105)
1
0
0 âˆš1
ï£¬
ï£­
0
0

âˆ’

0
p âˆšp

âˆ’
âˆšp âˆš1
âˆ’
0
0

ï£¶
ï£·
ï£¸ = eiHAD,

0
0
p 0
1

(115)

FIG. 11. Simulation of a quantum channel via Stinespring
decomposition together with unitary simulation as in Fig. 10.

As such we can write the conditional gates of Eq. (102)
as

0
(cid:105)j j(cid:104)

+ it1H1 âŠ— |

Ë†Uj = exp (it0H0 âŠ— |

0
|
for some times tj. Channel simulation is then obtained
by replacing the unitary evolution U of Eq. (106) with
the approximate form in Eq. (107) and its simulation
in Eq. (109). The result is illustrated in Fig. 11 and
described by the following channel

1
(cid:105)j j(cid:104)

) ,
1
|

(108)

QÏ€(Ï) = TrR

ï£®

ï£°

N
(cid:89)

j=1

Ë†Uj A,R0,Rj

(ÏA âŠ—

Ï€)

ï£¹

Ë†Uj

â€ 
A,R0,Rj

ï£» ,

N
(cid:89)

j=1

(109)
where the program state Ï€ is deï¬ned over R =
(R0, . . . , RN ) and each Ë†Hj acts on the input system A
and two ancillary qubits R0 and Rj. The decomposition
of Eq. (107) assures that, with the program

Ï€
|

(cid:105)

=

Î¸0(cid:105) âŠ— Â· Â· Â· âŠ— |
|

1
(cid:105)

m2

âŠ—

0

âŠ—
(cid:105)

âŠ— |

m1 ,

(110)

the product of unitaries approximates U in Eq. (106) with
precision (cid:15). This is possible in general, provided that the
d). However, the
program state has dimension
channel (109) is more general, as it allows both quantum
superposition and entanglement.

(d2(cid:15)âˆ’

O

The processor map Î› is then simply obtained as

Î›(Ï€) = TrR

(cid:104)

Ë†UAR (Î¦BA âŠ—

Ï€R) Ë†U â€ AR

(cid:105)

,

(111)

where

Ë†UAR = 11B âŠ—

N
(cid:89)

j=1

Ë†Uj A,R0,Rj

,

(112)

while the (non-trace-preserving) dual channel may be
written as

Î›âˆ—(X) =

Î¦BA|

Ë†U â€ AR (XBA âŠ—

(cid:104)

11R) Ë†UAR

Î¦BA(cid:105)
|

.

(113)

This channel requires 2N quantum gates at each itera-
tion and can be employed for the calculation of gradients,

where the Hamiltonian is given by

HAD =

arcsin(âˆšp)
2

(Y

X

X

âˆ’

âŠ—

âŠ—

Y ),

(116)

with X and Y being Pauli operators. We may construct
a PQC simulation by taking

U0 = eiÎ±(Y

X

X

Y ),

âŠ—

âˆ’

âŠ—

(117)

for some Î± and taking U1 to be a diï¬€erent unitary that
makes the pair U0, U1 universal. Here we may choose
Î± = âˆš2 and U1 = eiH1 with

H1 = (âˆš2Z + âˆš3Y + âˆš5X)

(Y + âˆš2Z).

(118)

âŠ—

Results are shown in Fig. 12. Compared with the sim-
ilar PBT simulation of Fig. 5, we observe that PQC sim-
ulation displays a non-monotonic behavior as a function
of N . PBT with N pairs requires a register of 2N qubits,
while PQC requires N + 1 qubits, namely N qubits from
the conditional gates and an extra one coming from Stine-
spring decomposition (see Fig. 11). We observe that,
with a comparable yet ï¬nite register size, PQC can out-
perform PBT in simulating the amplitude damping chan-
nel. In Fig. 13 we also study the PQC simulation of the
depolarizing channel for diï¬€erent values of p. Although
the gates U0 and U1 were chosen with inspiration from
the Stinespring decomposition of the amplitude damping
channel, those gates are universal and capable of sim-
ulating other channels.
Indeed, we observe in Fig. 13
that a depolarizing channel is already well simulated with
N = 4 for all values of p.

|ÏˆiUÎ¸1UÎ¸2UÎ¸3UÎ¸4|Î¸0i|Î¸1iâ€¢|Î¸2iâ€¢|Î¸3iâ€¢|Î¸4iâ€¢17

optimization problem that can always be solved.

In particular, by minimizing the diamond distance via
SDP, we can always determine the optimal program state
for the simulation of an arbitrary channel. Alternatively,
we may minimize the simpler but larger cost functions in
terms of trace distance and quantum ï¬delity via gradient-
based ML methods, so as to provide a very good ap-
proximation of the optimal program state. This other
approach can also provide closed analytical solutions, as
is the case for the simulation of arbitrary unitaries, for
which the minimization of the ï¬delity cost function cor-
responds to compute an eigenvector.

We have then applied our results to various de-
signs of programmable quantum processor,
from a
to deeper and
shallow teleportation-based scheme
asymptotically-universal designs that are based on PBT
and PQCs. We have explicitly benchmarked the per-
formances of these quantum processors by considering
the simulation of unitary gates, depolarizing and ampli-
tude damping channels, showing that the optimal pro-
gram states may diï¬€er from the naive choice based on
the Choi matrix of the target channel.

An immediate application of our work may be the de-
velopment of a model of â€œprogrammableâ€ blind quantum
computation, where a client has an input state to be
processed by a quantum server which is equipped with a
programmable quantum processor. The client classically
informs the server about what type of computation it
needs (e.g., some speciï¬ed quantum algorithm) and the
server generates an optimal program state which closely
approximates the overall quantum channel to be applied
to the input. The server then accepts the input from
the client, processes it, and returns the output together
with the value of a cost function quantifying how close
the computation was with respect to the clientâ€™s request.

Our results may also be useful in areas beyond quan-
tum computing, wherever channel simulation is a basic
problem. For instance this is the case of quantum com-
munication, for the derivation of quantum and private
communication capacities, and quantum metrology and
hypothesis testing, for the simpliï¬cation of adaptive pro-
tocols and the analysis of the ultimate discrimination and
estimation performance with quantum channels.

FIG. 12. PQC simulation of the amplitude damping channel.
Trace distance C1(EAD, Ï€) = (cid:107)Ï‡EAD âˆ’Ï‡Ï€(cid:107)1 between the target
channelâ€™s Choi matrix and its PQC simulation with program
state Ï€, for diï¬€erent numbers of register qubits N . The opti-
mal program is obtained from the minimization of C1 via the
projected subgradient (PS) method after 200 iterations.

FIG. 13. PQC simulation of the depolarizing channel. Trace
distance C1(EDep, Ï€) = (cid:107)Ï‡EDep âˆ’ Ï‡Ï€(cid:107)1 between the target
channelâ€™s Choi matrix and its UPQC simulation with pro-
gram state Ï€, for diï¬€erent numbers of register qubits N . The
optimal program is obtained from the minimization of C1 via
the projected subgradient (PS) method after 200 iterations.

X. CONCLUSIONS

In this work we have considered a general, ï¬nite-
dimensional, model of programmable quantum processor,
which is a fundamental scheme for quantum computing
and also a primitive tool for other areas of quantum in-
formation. By introducing suitable cost functions, based
on the diamond distance, trace distance and quantum
ï¬delity, we have shown how to characterize the optimal
performance of this processor in the simulation of an ar-
bitrary quantum gate or channel. In fact, we have shown
that the minimization of these cost functions is a convex

Acknowledgements.
L.B. acknowledges support by
the program â€œRita Levi Montalciniâ€ for your young
researchers.
S.P. acknowledges support by the EP-
SRC via the â€˜UK Quantum Communications Hubâ€™
(EP/M013472/1) and the European Union via the
project â€˜Continuous Variable Quantum Communicationsâ€™
(CiViQ, no 820466). S.P. would like to thank George
Zweig, Jacques Carolan, John Watrous, and Dirk En-
glund for discussions and feedback.

0.00.20.40.60.8C1(EAD,Ï€)0.00.20.40.60.81.0pPS200N=1PS200N=2PS200N=3PS200N=4PS200N=5PS200N=60.00.20.40.60.8C1(EDep,Ï€)0.00.20.40.60.81.0pPS200N=1PS200N=2PS200N=3PS200N=4PS200N=5PS200N=6Appendix A: Matrix calculus

3. Diï¬€erential of the trace distance

1. Matrix diï¬€erentiation

The trace norm for a Hermitian operator X is deï¬ned

18

For a general overview of these techniques, the reader
may consult Ref. [54]. Thanks to Cauchyâ€™s theorem, a
matrix function can be written as

f (A) =

1
2Ï€i

(cid:90)

Î“

dÎ» f (Î»)(Î»11

A)âˆ’

1 .

âˆ’

(A1)

For the same reason

f (cid:48)(A) =

1
2Ï€i

(cid:90)

Î“

dÎ» f (Î»)(Î»11

A)âˆ’

2 .

âˆ’

(A2)

Applying a basic rule of matrix diï¬€erentiation, d(Aâˆ’

1) =

Aâˆ’

1(dA)Aâˆ’

1 we obtain

âˆ’

df (A) =

1
2Ï€i

(cid:90)

Î“

dÎ» f (Î»)(Î»11

âˆ’

A)âˆ’

1dA(Î»11

âˆ’

A)âˆ’

1 . (A3)

Clearly, df (A) = f (cid:48)(A)dA only when [A, dA] = 0.
In
general df (A) is a superoperator that depends on A and
is applied to dA. The explicit form is easily computed
using the eigenvalue decomposition or other techniques
[54]. Note that in some cases the expressions are simple.
Indeed, using the cyclic invariance of the trace, we have

dTr[f (A)] = Tr[f (cid:48)(A)dA],

(A4)

while in general dTr[Bf (A)]

=Tr[Bf (cid:48)(A)dA].

2. Diï¬€erential of the quantum ï¬delity

The quantum ï¬delity can be expanded as

as

t(X) =

=

(cid:107)1 := TrâˆšX â€ X = Tr[âˆšX 2]
(cid:90)
1] ,
XX)âˆ’

dÎ» âˆšÎ»Tr[(Î»11

X
(cid:107)
1
2Ï€i

Î“

âˆ’

(A7)

j |

Î»j|

where in the second line we applied Eq. (A1). From
the spectral decomposition X = U Î»U â€  we ï¬nd t(X) =
(cid:80)
, so the trace distance reduces to the absolute
value function for one-dimensional Hilbert spaces. The
is diï¬€erentiable at every
absolute value function
Î»
|
= 0 the
points, except Î» = 0. Therefore, for any Î»
subgradient of the absolute value function is made by its
gradient, namely

|

âˆ‚

Î»
|

|

=

sign(Î»)
}
{

for Î»

= 0 .

(A8)

For Î» = 0 we can use the deï¬nition (34) to write

z :
{

Ïƒ
|

| â‰¥

zÏƒ for all Ïƒ

,

}

(A9)

âˆ‚

|

|Î»=0 =
Î»
1

which is true iï¬€

z

1. Therefore,

|

âˆ‚

âˆ’

â‰¤

1, 1] .

â‰¤
|Î»=0 = [
Î»
âˆ’
The sign function in (A8) can be extended to Î» = 0 in
multiple ways (common choices are sign(0) =
1, 0, 1).
From the above equation, it appears that for any exten-
sion of the sign function, provided that sign(0)
1, 1]
we may write the general form

(A10)

[
âˆ’

âˆ’

âˆˆ

sign(Î»)

,

âˆ‚

Î»
|

|

âˆˆ

(A11)

F (X, Y ) = Tr
1
2Ï€i

=

(cid:113)

âˆšXY âˆšX
(cid:90)

dÎ» âˆšÎ»Tr[(Î»11

Î“

(A5)

âˆšXY âˆšX)âˆ’

1] ,

âˆ’

which is true for any value of Î».

With the same spirit we extend the above argument to
any matrix dimension, starting from the case where X is
an invertible operator (no zero eigenvalues). Taking the
diï¬€erential with respect to X we ï¬nd

where in the second line we have applied Eq. (A1). Tak-
ing the diï¬€erential with respect to Y and using the cyclic
property of the trace we get

dt(X) := t(X + dX)

t(X) =

âˆ’
dÎ» âˆšÎ»Tr[(Î»11

F (X, Y )

âˆ’

dÎ» âˆšÎ»Tr[(Î»11

âˆšXY âˆšX)âˆ’

2âˆšXdY âˆšX]

âˆ’

Tr[(âˆšXY âˆšX)âˆ’

1

2 âˆšXdY âˆšX]

(cid:90)

(1)
=

dY F := F (X, Y + dY )
1
2Ï€i
1
2
1
2

(2)
=

(3)
=

Î“

Tr[âˆšX(âˆšXY âˆšX)âˆ’

1

2 âˆšX dY ] ,

(A6)

where in (1) we use Eq. (A3) and the cyclic property of
the trace; in (2) we use Eq. (A2) with f (Î») = âˆšÎ», so
f (cid:48)(Î») = 1
1/2; and in (3) we use the cyclic property of
the trace. See also Lemma 11 in [35].

2 Î»âˆ’

(cid:90)

Î“

(1)
=

(2)
=

1
2Ï€i
1
2

X 2)âˆ’

2(X(dX) + (dX)X)]

âˆ’

Tr[(X 2)âˆ’

1
2 (X(dX) + (dX)X)]

(3)
= Tr[(X 2)âˆ’

1
2 X (dX)]

(A12)

where in (1) we use Eq. (A3), the cyclic property of the
trace and the identity dX 2 = X(dX) + (dX)X; in (2)
we use Eq. (A2) with f (Î») = âˆšÎ», so f (cid:48)(Î») = 1
1/2;
and in (3) we use the cyclic property of the trace and the
commutation of X and âˆšX 2. Let

2 Î»âˆ’

X =

(cid:88)

k

Î»kPk ,

(A13)

(cid:54)
(cid:54)
(cid:54)
be the eigenvalue decomposition of X with eigenvalues
Î»k and eigenprojectors Pk. For non-zero eigenvalues we
may write

(X 2)âˆ’

1
2 X =

(cid:88)

k

and accordingly

sign(Î»k)Pk =: sign(X) ,

(A14)

dt(X) :=

X + dX
(cid:107)
(cid:88)

(cid:107)1 âˆ’ (cid:107)
sign(Î»k)Tr[Pk dX] .

(cid:107)1

X

=

(A15)

k

Therefore, for invertible operators we may write

âˆ‚t(X) =

t(X)

,

âˆ‡

}

{âˆ‡

t(X) = sign(X) .

We now consider the general case where some eigenvalues
of X may be zero. We do this by generalizing Eq. (A11),
namely we show that even if âˆ‚t(X) may contain multiple
âˆ‚t, provided that
elements, it is always true that
âˆˆ
11. Following (34) we may write, for

sign(X)

âˆ‡

11

t

âˆ’
ï¬xed X and arbitrary Y ,

â‰¤

â‰¤

Tr[

t(X)(Y

X)]

âˆ‡
Tr[

âˆ’
t(X)Y ] + t(X)

t(Y )

t(X)

âˆ’

âˆ’

(1)
= t(Y )

(2)

â‰¥

t(Y )

âˆ’

t(X)

âˆ’

Tr[Y ] =

âˆ’
(cid:88)

âˆ‡
Î»j| âˆ’

(
|

j

Î»j)

0 ,

â‰¥

(A16)

where in (1) we use the property
X
(cid:107)1 = Tr[sign(X)X]
(cid:107)
and in (2) we use the assumption
11.
sign(X)
11
âˆ’
From the deï¬nition of the subgradient (34), the above
equation shows that sign(X)
âˆ‚t(X), so we may always
âˆˆ
use
t(X) = sign(X) in the projected subgradient algo-
âˆ‡
rithm (50).

â‰¤

â‰¤

Appendix B: Smoothing techniques

1. Stochastic smoothing

The conjugate gradient algorithm converges after
(c/(cid:15)) steps [16, 17], where (cid:15) is the desired precision and
O
c is a curvature constant that depends on the function.
However, it is known that c could diverge for non-smooth
functions. This is the case for the trace norm, as shown
in Example 0.1 in [55].

A general solution, valid for arbitrary functions, is via
In this approach the non-

stochastic smoothing [56].
smooth function C(Ï€) is replaced by the average

19

so that CÎ·(Ï€) provides a good approximation for C(Ï€).
Moreover, CÎ· is diï¬€erentiable at any point, so we may
apply the conjugate gradient algorithm. A modiï¬ed con-
jugate gradient algorithm with adaptive stochastic ap-
proximation was presented in Ref. [57], At each iteration
k the algorithm reads

kâˆ’

(cid:80)k

1/2,

k+2 |

of Â¯gk,

j=1 g(Ï€k + Î·kÏƒj) for Î·k âˆ
Ïƒk(cid:105)
|
.

1) Sample some operators Ïƒ1, . . . , Ïƒk,
2) Evaluate Â¯gk = 1
k
3) Find the smallest eigenvalue
k+2 Ï€k + 2
4) Ï€k+1 = k
Ïƒk|
Ïƒk(cid:105) (cid:104)
where g denotes any element of the subgradient âˆ‚C. The
((cid:15)2) iterations. Since
above algorithm converges after
Eqs. (40) and (38) provide an element of the subgradi-
ent, the above algorithm can be applied to both ï¬delity
and trace distance. However, this algorithm requires k
evaluation of the subgradient to perform the averages, so
it may be impractical when the number of iterations get
larger. In the following we study an alternative that does
not require any average.

O

2. Nesterovâ€™s smoothing

An alternative smoothing scheme is based on Nes-
terovâ€™s dual formulation [38]. Suppose that the non-
smooth objective function f admits a dual representation
as follows

[
f (x) = sup

y

x, y
(cid:104)

(cid:105) âˆ’

g(y)],

(B3)

for some inner product
. Nesterovâ€™s approximation
Â·(cid:105)
consists in adding a strongly convex function d to the
dual

,
(cid:104)Â·

fÂµ(x) =

(cid:88)
[

y

x, y
(cid:104)

(cid:105) âˆ’

g(y)

âˆ’

Âµd(y)].

(B4)

The resulting Âµ-approximation is smooth and satisï¬es

fÂµ(x)

f (x)

â‰¤

â‰¤

fÂµ(x) + Âµ sup

y

d(y).

(B5)

The trace norm admits the dual representation [25]
(cid:107)1 = sup

t(X) =

Y, X

,
(cid:105)

X

(cid:107)

1(cid:104)

Y

âˆ

(cid:107)

(cid:107)

Y, X

â‰¤
is the Hilbert Schmidt product. This can
where
be regularized with any strongly convex function d. A
convenient choice [19] that enables an analytic solution
is via d(X) = 1

X, X

so

X

(cid:105)

(cid:104)

(B6)

2 (cid:107)

2 := 1
2
2 (cid:104)
(cid:107)
(cid:104)

(cid:105)

tÂµ(X) = max
â‰¤

âˆ

Y

(cid:107)

(cid:107)

1

Y, X
(cid:104)

(cid:105) âˆ’

Âµ
2 (cid:107)

Y

2
2
(cid:107)

(cid:105)

.

(B7)

CÎ·(Ï€) = EÏƒ[C(Ï€ + Î·Ïƒ)] .

(B1)

This function is smooth and its gradient is given by [19]

where Ïƒ is such that
M

, then

y

x
(cid:107)

âˆ’

(cid:107)âˆ

Ïƒ

(cid:107)

(cid:107)âˆ â‰¤

1.

If

C(x)
|

âˆ’

C(y)

| â‰¤

âˆ‡

C(Ï€)

â‰¤

CÎ·(Ï€)

â‰¤

C(Ï€) + M Î· ,

(B2)

tÂµ(X) = argmax
1
â‰¤
(cid:107)
= argmin
Y

âˆ

Y

(cid:107)

âˆ

(cid:107)

(cid:107)

â‰¤

(cid:104)

Y, X
(cid:104)

(cid:105) âˆ’

(cid:105)

Âµ
2 (cid:107)

Y

2
2
(cid:107)

ÂµY

1 (cid:107)

X

2
2 = U Î£ÂµV â€ ,
(cid:107)

âˆ’

where X = U Î£V â€  is the singular value decomposition
of X and Î£Âµ is a diagonal matrix with diagonal entries
. Plugging this into Eq. (B7) we
Î£i/Âµ, 1
(Î£Âµ)i = min
}
get

{

tÂµ(X) = Tr

(cid:104)

Î£Âµ

(cid:16)

Î£

(cid:17)(cid:105)

.

Î£Âµ

Âµ
2

âˆ’

(B8)

For a diagonalizable matrix X with spectral decompo-
sition X = U Î»U â€ , the singular value decomposition is
obtained with Î£ =
Inserting
these expressions in (B8) we ï¬nd

and V = U sign(Î»).

Î»
|

|

tÂµ(X) =

(cid:88)

j

hÂµ(Î»j) = Tr[hÂµ(X)],

(B9)

20

Theorem 7 The gradient of the smooth trace norm is
Lipschitz continuous with Lipschitz constant

L =

d
Âµ

.

(B16)

In particular, being the gradient Lipschitz continuous,
the smooth trace norm satisï¬es the following inequality
for any state Ï€, Ïƒ

CÂµ(Ïƒ)

â‰¤

CÂµ(Ï€) +

CÂµ(Ï€), Ïƒ

(cid:104)âˆ‡

Ï€

(cid:105)

âˆ’

+

L
2 (cid:107)

Ïƒ

Ï€

2
2. (B17)
(cid:107)

âˆ’

Proof. Given the linearity of the quantum channel Î›, we
can apply theorem 1 from [38] to ï¬nd

hÂµ(x) =

where hÂµ is the so called Huber penalty function
(cid:40) x2
2Âµ
x
|
tÂµ is then h(cid:48)Âµ(X)
(cid:40) x
Âµ
sign(x)

x
|
x
|
U h(cid:48)(Î»)U â€ , where

The gradient

< Âµ,
Âµ.

< Âµ,
Âµ.

h(cid:48)Âµ(x) =

|
| â‰¥

if
if

if
if

| âˆ’

âˆ‡

â‰¡

Âµ
2

x
|
x
|

|
| â‰¥

(B10)

(B11)

We ï¬nd then that via the smooth trace norm tÂµ we
can deï¬ne the smooth trace distance of Eq. (59) that is
diï¬€erentiable at every point

CÂµ(Ï€) = Tr [hÂµ (Ï‡Ï€ âˆ’

Ï‡
E

)] .

(B12)

Thanks to the inequalities in (B5), the smooth trace dis-
tance bounds the cost C1 as

CÂµ(Ï€)

C1(Ï€)

â‰¤

â‰¤

CÂµ(Ï€) +

Âµd
2

,

(B13)

where we employed the identity sup
get the upper bound. Moreover, we ï¬nd the following

Y
1 (cid:107)

(cid:107)

â‰¤

âˆ

Y

(cid:107)

(cid:107)

2
2 â‰¤

d to

Lemma 6 The
Eq. (59), is a convex function of Ï€.

smooth trace distance,

deï¬ned in

Proof. From the deï¬nition and Eq. (B7) we ï¬nd

CÂµ(Ï€) = tÂµ [Î›(Ï€)
âˆ’
(cid:104)
(cid:104)

= max
Y
âˆ
â‰¤

(cid:107)

(cid:107)

1

Ï‡

]

E
Y, Î›(Ï€)

Ï‡

âˆ’

E (cid:105) âˆ’

Âµ
2 (cid:107)

Y

2
2
(cid:107)

(cid:105)

.

(B14)

p)Ï€2 linearity implies f (Â¯Ï€) :=
p)f (Ï€2). Therefore

= pf (Ï€1) + (1

âˆ’
(cid:104)
pf (Ï€1) + (1

âˆ’

1

Y

âˆ

E (cid:105)

Now for Â¯Ï€ = pÏ€1 + (1
Ï‡
Y, Î›(Â¯Ï€)
(cid:104)

âˆ’
CÂµ(Â¯Ï€) = max
(cid:107)
(cid:107)
â‰¤
p max
Y
âˆ
(cid:107)
â‰¤
p) max
Z
âˆ
â‰¤
(cid:107)
= pCÂµ(Ï€1) + (1
showing the convexity. (cid:4)

(cid:107)
+ (1

â‰¤

âˆ’

(cid:104)

(cid:107)

1

Y, Î›(Ï€1)
(cid:104)

Z, Î›(Ï€2)

(cid:104)
(cid:104)
p)CÂµ(Ï€2),

1

âˆ’

p)f (Ï€2)

âˆ’

âˆ’

Ï‡

E (cid:105) âˆ’

âˆ’

Âµ
2 (cid:107)

Y

Y

(cid:105)

2
2
(cid:107)

Âµ
2 (cid:107)
(cid:105)

2
2
(cid:107)
Âµ
2 (cid:107)

Ï‡

âˆ’

E (cid:105) âˆ’

Z

Then, using the deï¬nitions from [38], the following the-

orem bounds on the growth of the gradient

L =

1
Âµ

x

(cid:107)

(cid:107)

sup
y

2=1,

(cid:107)

y, Î›(x)
(cid:105)

.

2=1(cid:104)

(cid:107)

(B18)

Since all eigenvalues of y are smaller or equal to 1, we
can write y

1 and as such

â‰¤

L

â‰¤

1
Âµ

(cid:107)

sup
x

2=1

(cid:107)

Tr[Î›(x)] =

1
Âµ

(cid:107)

sup
x

2=1

(cid:107)

Tr[x]

d
Âµ

.

â‰¤

(B19)

(cid:4)

Appendix C: PBT reduced channel

Here we provide an explicit expression for the reduced
map ËœÎ› of Eq. (92) in the case of qubits. For d = 2 we
can rewrite PBT in a language that can be more easily
formulated from representations of SU(2). For simplicity
of notation, here we do not use bold letters for vectorial
quantities.

Let us modify the POVM in Eq. (78) as

1/2

ËœÎ i = Ïƒâˆ’
AC Î¨âˆ’AiCÏƒâˆ’
N
(cid:88)

1/2
AC ,

ÏƒAC =

Î¨âˆ’AiC,

i=1
Î i = ËœÎ i + âˆ†,

âˆ† =

ï£«

ï£­11

1
N

(cid:88)

âˆ’

j

ï£¶

ËœÎ j

ï£¸ ,

(C1)

(C2)

(C3)

(C4)

)/âˆš2 is a singlet state. For
Î¨âˆ’
(cid:105)
n the quantum channel is simpliï¬ed.
In fact,

where
|
Ï€ = Ï‡âŠ—
since TrB Ï‡ = 11/2, we may write

= (
|

(cid:105) âˆ’ |

01

10

(cid:105)

PÏ€ =

=

N
(cid:88)

i=1
(cid:88)

(cid:96)

(cid:105)

2
2
(cid:107)
(B15)

1
2N

1 TrAC
âˆ’

(cid:104)(cid:112)

Î i

K 0
(cid:96) (ÏC âŠ—

Ï‡)K 0

(cid:96) â€  +

(cid:0)ÏC âŠ—
Ï‡AiB âŠ—
K 1
(cid:96) (ÏC âŠ—

(cid:88)

(cid:96)(cid:48)

(cid:1) (cid:112)

(cid:105)

Î i

11 Â¯Ai

Ï‡)K 1

(cid:96) â€ , (C5)

where (cid:96) and (cid:96)(cid:48) are multi-indices and,
in deï¬ning the
Kraus operators, we have separated the contributions
from ËœÎ i and âˆ† (see below).

In order to express these operators, we write

are Clebsch-Gordan coeï¬ƒcients.

21

Ïˆâˆ’CAi(cid:105)(cid:104)
|

Ïˆâˆ’CAi|

=

11

âˆ’

(cid:126)ÏƒC Â·
4

(cid:126)ÏƒAi

,

(C6)

so that

ÏƒAC =

N
(cid:88)

i=1

Ïˆâˆ’CAi(cid:105)(cid:104)
|
(cid:126)S2
tot âˆ’

Ïˆâˆ’CAi|
(cid:126)S2
C âˆ’
2

=

N
4 âˆ’

=

N
4 âˆ’

(cid:126)SC Â·

(cid:126)SA

(cid:126)S2
A

,

(C7)

where (cid:126)S = (cid:126)Ïƒ/2 is a vector of spin operators, (cid:126)SA =
(cid:80)
(cid:126)SAj and (cid:126)Stot = (cid:126)SC + (cid:126)SA. The eigenvalues of ÏƒAC
j
are then obtained from the eigenvalues of the three com-
muting Casimir operators

Î»(sA) =

N
4 âˆ’

Stot(Stot + 1)

âˆ’

sA(sA + 1)
2

âˆ’

3/4

, (C8)

where Stot = sA Â±
of eigenvalues

1/2.

Substituting the deï¬nition of Stot, we ï¬nd two classes

Î»+(sA) =

2sA

N

âˆ’
4

, Î»âˆ’(sA) =

N + 2sA + 2
4

,

(C9)

with corresponding eigenvectors

, sA, M, Î±

|Â±

=

(cid:105)

(cid:88)

k,m

Î“M,m,k
sA
Â±

1

2 ,sA|

sA, m, Î±

k

(cid:105)C|

(cid:105)A ,

(C10)

N +1

2 , Î± = 1, . . . , g[N ](s) describes
where
the degeneracy, g[N ](s) is the size of the degenerate sub-
space, and

2 â‰¤

M

âˆ’

â‰¤

N +1

Î“M,m,k
S,s

=

1/2, 1/2
S, M ; s, 1/2
|
(cid:104)

âˆ’

k; s, m
(cid:105)

(C11)

Note that

the Clebsch-Gordan coeï¬ƒcients deï¬ne
two bases
. From the orthogonal-

a unitary transformation between the
S, M ; s1, s2(cid:105)
s1, m1; s2; m2(cid:105)
|
|
ity relations of these coeï¬ƒcients we ï¬nd the equalities

and

(cid:88)

S,s Î“M,m(cid:48),i(cid:48)
Î“M,m,i

S,s

= Î´i,i(cid:48)Î´m,m(cid:48),

S,M
(cid:88)

m,i

S,s Î“M (cid:48),m,i
Î“M,m,i

S(cid:48),s = Î´M,M (cid:48)Î´(S, S(cid:48), s),

(C12)

(C13)

where Î´(S, S(cid:48), s) = 1 iï¬€ S = S(cid:48) and
s +
1/2. The eigenvalues in Eq. (C9) are zero iï¬€ Stot = SA +
1/2 and SA = N/2. These eigenvalues have degeneracy
2Stot + 1 = N + 2 and the corresponding eigenvectors are

1/2

| â‰¤

s
|

âˆ’

â‰¤

S

, M, Î±

| âŠ¥

=

+, N/2, M, Î±
|

(cid:105)

.

(cid:105)

(C14)

Thus, the operator âˆ† from Eq. (C4) may be written as

âˆ† =

1
N

N +1

2(cid:88)

(cid:88)

M =

âˆ’

N +1
2

Î±

, M, Î±

| âŠ¥

, M, Î±

.

|

(cid:105)(cid:104)âŠ¥

(C15)

To ï¬nish the calculation we need to perform the partial
trace over all spins except those in port i. We use s Â¯Ai,
m Â¯Ai and Î±i to model the state of the total spin in ports
Aj with j
= i. These refer to the value of total spin and
the projection along the z axis, as well as the degeneracy.
Moreover, since S Â¯Ai commutes with both S2
A and Sz
A,
we may select a basis for the degeneracy that explicitly
contains s Â¯Ai. We may write then Î± = (s Â¯Ai, ËœÎ±i) where ËœÎ±i
represents some other degrees of freedom.

With the above deï¬nitions, when we insert several res-
olutions of the identity in Eq. (C5), we may write the
Kraus operators as

K 0

i,s Â¯Ai

,m Â¯Ai

,Î±i,s(cid:48)

Â¯Ai

,m(cid:48)

Â¯Ai

,Î±(cid:48)
i

= 2âˆ’

N âˆ’1
2

= 2âˆ’

N âˆ’1
2

s Â¯Ai , m Â¯Ai, Î±i| âŠ— (cid:104)
(cid:104)
(cid:88)
(sA)âˆ’
Î»

,sA,M,Î±

Â±

Ïˆâˆ’AiC|

1/2

(cid:104)

1/2
Ïƒâˆ’
AC |
Ïˆâˆ’AiC|(cid:104)

s(cid:48)Â¯Ai

, m(cid:48)Â¯Ai

, Î±(cid:48)i(cid:105)
s Â¯Ai, m Â¯Ai, Î±i|Â±

, sA, M, Î±

K 1

i,M,Î±,s(cid:48)

,m(cid:48)

Â¯Ai

,Î±(cid:48)
i

= 2âˆ’

Â¯Ai

1/2

+, N/2, M, Î±
(cid:104)

|

s(cid:48)Â¯Ai

, m(cid:48)Â¯Ai

,
, Î±(cid:48)i(cid:105)

N âˆ’1

Â±
2 N âˆ’

, sA, M, Î±

s(cid:48)Â¯Ai
|

, m(cid:48)Â¯Ai

,
, Î±(cid:48)i(cid:105)

(cid:105)(cid:104)Â±

(C16)

where each set of states
of the space corresponding to all ports j with j

s Â¯Ai, m Â¯Ai, Î±i(cid:105)
|

represent a basis
= i. To

simplify the Kraus operators we study the overlap

sÂ¯Ä±, mÂ¯Ä±, Î±i|Â±
(cid:104)
(cid:88)
k
=
(cid:105)C(cid:104)
|

, S, M, Î±
(cid:105)
sÂ¯Ä±, mÂ¯Ä±, Î±i|

Î“M,m,k
2 ,S|
S
Â±

1

S, m, Î±

k,m
(cid:88)

|

k,m
(cid:88)

k,(cid:96),m

=

=

k

(cid:105)C(cid:104)

sÂ¯Ä±, mÂ¯Ä±, Î±i|
(cid:105)Ai Î“M,m,k

S

Â±

Î“M,m,k
1
2 ,S
S
Â±

(cid:88)

(cid:96)

(cid:96)
|

2 ,SÎ“m,mÂ¯Ä±,(cid:96)

S,sÂ¯Ä± â‰¡

1

k

|

(cid:96)
(cid:105)C|

(cid:105)A
s(cid:48)Â¯Ä±, m(cid:48)Â¯Ä±, Î±(cid:48)i(cid:105)Â¯Ä±Î“m,m(cid:48)
(cid:105)i|
Ë†QsÂ¯Ä±,mÂ¯Ä±
Â±

,s,M .

S,s(cid:48)
Â¯Ä±

Â¯Ä±,k

(C17)

(cid:54)
(cid:54)
In the last line we ï¬nd that the overlap is independent
on Î± and Î±i, though with constraints Î± = (sÂ¯Ä±, Î±i), which
requires Î±i = Î±(cid:48)i. Therefore, diï¬€erent Kraus operators
provide exactly the same operation and, accordingly, we
can sum over these equivalent Kraus operators to reduce

the number of indices. After this process we get

22

(cid:113)

(sA)âˆ’

1/2

Î»

Â±

g[N

1](sÂ¯Ä±)

âˆ’

Ã—

,sA,M

Â±
Ë†QsÂ¯Ä±,mÂ¯Ä±
Â±

,sA,M

Ë†QsÂ¯Ä±,m(cid:48)

Â¯Ä±
,sA,M â€ 

Â±

(cid:17)

11B,

âŠ—

K 0

(cid:96) â‰¡

K 0

sÂ¯Ä±,mÂ¯Ä±,m(cid:48)
Â¯Ä±

= 2âˆ’

N âˆ’1

2 âˆšN

(cid:88)

(cid:16)

Ïˆâˆ’AC|
(cid:104)
K 1
(cid:114)

g[N

M,sÂ¯Ä±,mÂ¯Ä±

Ã—

K 1

(cid:96) â‰¡

=

1](sÂ¯Ä±)
1

âˆ’

âˆ’
2N

Ë†QsÂ¯Ä±,m(cid:48)

Â¯Ä±
+,N/2,M â€ 

11B.

âŠ—

(C18)

(C19)

The Kraus operators of the reduced channel ËœÎ› are ob-
tained as (K u
11AB). It is simple to check
that the above operators deï¬ne a CPTP-map.

Î¨âˆ’CD(cid:105) âŠ—
|

11D)(

(cid:96) âŠ—

[1] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information (Cambridge University Press,
Cambridge, 2000).

[2] C. M. Bishop, Pattern Recognition and Machine Learning

(Springer, 2006).

[3] P. Wittek, Quantum Machine Learning: What Quantum
Computing Means to Data Mining (Academic Press, El-
sevier, 2014).

[4] J Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, â€œQuantum machine learning,â€
Nature (London) 549, 195 (2017).

[5] V. Dunjko and H. J. Briegel, â€œMachine learning & ar-
tiï¬cial intelligence in the quantum domain: a review of
recent progress,â€ Rep. Prog. Phys. 81, 074001 (2018).
[6] M. Schuld, I. Sinayskiy, and F. Petruccione, â€œAn intro-
duction to quantum machine learning,â€ Contemporary
Physics 56, 172â€“185 (2015).

[7] C. Ciliberto, M. Herbster, Alessandro D.

Ialongo,
M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig,
â€œQuantum machine learning: a classical perspective,â€
Proceedings of the Royal Society A: Mathematical, Phys-
ical and Engineering Sciences 474, 20170551 (2018).
[8] E Tang, â€œA quantum-inspired classical algorithm for
recommendation systems,â€ preprint arXiv:1807.04271
(2018).

[9] E Tang, â€œQuantum-inspired classical algorithms for prin-
cipal component analysis and supervised clustering,â€
preprint arXiv:1811.00414 (2018).

[10] M. A. Nielsen and I. L. Chuang, â€œProgrammable quan-
tum gate arrays,â€ Phys. Rev. Lett. 79, 321 (1997).
[11] S. Ishizaka and T. Hiroshima, â€œAsymptotic teleportation
scheme as a universal programmable quantum proces-
sor,â€ Phys. Rev. Lett. 101, 240501 (2008).

[12] S. Ishizaka and T. Hiroshima, â€œQuantum teleportation
scheme by selecting one of multiple output ports,â€ Phys.
Rev. A 79, 042306 (2009).

[13] S. Ishizaka, â€œSome remarks on port-based teleportation,â€

preprint arXiv:1506.01555 (2015).

[14] S. Pirandola, R. Laurenza, and C. Lupo, â€œFundamen-
tal limits to quantum channel discrimination,â€ preprint
arXiv:1803.02834 (2018).

[15] S. Boyd, L. Xiao, and A. Mutapcic, Subgradient methods

(2003).

[16] M. Jaggi, â€œConvex optimization without projection

steps,â€ preprint arXiv:1108.1170 (2011).

[17] M. Jaggi, â€œRevisiting frank-wolfe: projection-free sparse
convex optimization,â€ in Proceedings of the 30th Inter-
national Conference on International Conference on Ma-
chine Learning-Volume 28 (JMLR. org, 2013) pp. 1â€“427.
[18] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chan-
dra, â€œEï¬ƒcient projections onto the l 1-ball for learning
in high dimensions,â€ in Proceedings of the 25th interna-
tional conference on Machine learning (ACM, 2008) pp.
272â€“279.

[19] J. Liu, P. Musialski, P. Wonka,

and J. Ye, â€œTensor
completion for estimating missing values in visual data,â€
IEEE transactions on pattern analysis and machine in-
telligence 35, 208â€“220 (2013).

[20] S Lloyd, â€œUniversal quantum simulators,â€ Science 273,

1073â€“1078 (1996).

[21] S. Pirandola, R. Laurenza, C. Ottaviani, and L. Banchi,
â€œFundamental limits of repeaterless quantum communi-
cations,â€ Nat. Commun. 8, 15043 (2017).

[22] S. Pirandola, S. L. Braunstein, R. Laurenza, C. Otta-
viani, T. P. W. Cope, G. Spedalieri,
and L. Banchi,
â€œTheory of channel simulation and bounds for private
communication,â€ Quant. Sci. Tech. 3, 035009 (2018).
[23] J. Watrous, The theory of quantum information (Cam-
bridge Univ. Press, 2018) freely available at https://cs.
uwaterloo.ca/~watrous/TQI/.

[24] A. Y. Kitaev, A. Shen, and M. N. Vyalyi, Classical and
quantum computation, 47 (American Mathematical Soci-
ety, Providence, Rhode Island, 2002) sec. 11.

[25] J. Watrous, Advanced Topics in Quantum Information

Processing (Lecture notes, 2004).

[26] E. Knill, R. Laï¬‚amme, and G. J Milburn, â€œA scheme
for eï¬ƒcient quantum computation with linear optics,â€
Nature 409, 46 (2001).

[27] J. Watrous, â€œSimpler semideï¬nite programs for com-
pletely bounded norms,â€ Chicago Journal of Theoretical
Computer Science 8, 1â€“19 (2013).

[28] C. A. Fuchs and J. van de Graaf, â€œCryptographic distin-
guishability measures for quantum-mechanical states,â€
IEEE Trans. Info. Theory 45, 1216â€“1227 (1999).

[29] M. S. Pinsker, Information and information stability of
random variables and processes (Holden-Day, San Fran-
cisco, 1964).

[30] E. A. Carlen and E. H. Lieb, â€œBounds for entanglement
via an extension of strong subadditivity of entropy,â€ Lett.
Math. Phys. 101, 1â€“11 (2012).

[31] A. Uhlmann, â€œThe transition probability...â€ Rep. Math.

Phys. 9, 273â€“279 (1976).

[32] John Watrous, â€œSemideï¬nite programs for completely
bounded norms,â€ Theory of Computing 5, 217â€“238
(2009).

[33] I. Nechita, Z. Pucha(cid:32)la, (cid:32)L. Pawela, and K.

Ë™Zyczkowski,
â€œAlmost all quantum channels are equidistant,â€ J. Math.
Phys. 59, 052201 (2018).

[34] Y. Nesterov, Introductory lectures on convex optimiza-
tion: A basic course, Vol. 87 (Springer Science & Busi-
ness Media, New York, 2013).

[35] B. Coutts, M. Girard, and J. Watrous, â€œCertifying op-
timality for convex quantum channel optimization prob-
lems,â€ preprint arXiv:1810.13295 (2018).

[36] J. Duchi, E. Hazan, and Y. Singer, â€œAdaptive subgradi-
ent methods for online learning and stochastic optimiza-
tion,â€ Journal of Machine Learning Research 12, 2121â€“
2159 (2011).

[37] D. Garber and E. Hazan, â€œFaster rates for the frank-wolfe
method over strongly-convex sets,â€ in Proceedings of the
32nd International Conference on International Confer-
ence on Machine Learning-Volume 37 (JMLR. org, 2015)
pp. 541â€“549.

[38] Y. Nesterov, â€œSmooth minimization of non-smooth func-
tions,â€ Mathematical programming 103, 127â€“152 (2005).
[39] R. Bhatia, Matrix analysis, Vol. 169 (Springer Science &

Business Media, New York, 2013).

[40] L. Banchi, N. Pancotti, and S. Bose, â€œQuantum gate
learning in qubit networks: Toï¬€oli gate without time-
dependent control,â€ npj Quantum Inf. 2, 16019 (2016).
[41] L. Innocenti, L. Banchi, A. Ferraro, S. Bose, and M. Pa-
ternostro, â€œSupervised learning of
time-independent
hamiltonians for gate design,â€ preprint arXiv:1803.07119
(2018).

[42] K. Mitarai, M. Negoro, M. Kitagawa,

and K. Fujii,
â€œQuantum circuit learning,â€ Phys. Rev. A 98, 032309
(2018).

[43] J. M. Arrazola, T. R. Bromley, J. Izaac, C. R. Myers,
K. BrÂ´adler, and N. Killoran, â€œMachine learning method
for state preparation and gate synthesis on photonic
quantum computers,â€ Quantum Sci. Technol. 4, 024004

23

(2019).

[44] C. H. Bennett, G. Brassard, C. CrÂ´epeau, R. Jozsa,
A. Peres, and W. K. Wootters, â€œTeleporting an unknown
quantum state via dual classical and einstein-podolsky-
rosen channels,â€ Phys. Rev. Lett. 70, 1895 (1993).
[45] S. Pirandola, J. Eisert, C. Weedbrook, A. Furusawa, and
S. L. Braunstein, â€œAdvances in quantum teleportation,â€
Nat. Photon. 9, 641â€“652 (2015).

[46] G. Bowen and S. Bose, â€œTeleportation as a depolarizing
quantum channel, relative entropy, and classical capac-
ity,â€ Phys. Rev. Lett. 87, 267901 (2001).

[47] T. P. W. Cope, L. Hetzel, L. Banchi, and S. Pirandola,
â€œSimulation of non-pauli channels,â€ Phys. Rev. A 96,
022323 (2017).

[48] C. H. Bennett, D. P. DiVincenzo, J. A. Smolin, and
W. K. Wootters, â€œMixed-state entanglement and quan-
tum error correction,â€ Phys. Rev. A 58, 3824 (1996).
[49] M. Christandl, F. Leditzky, C. Majenz, G. Smith,
F. Speelman, and M. Walter, â€œAsymptotic performance
of port-based teleportation,â€ preprint arXiv:1809.10751
(2018).

[50] S. Lloyd, â€œAlmost any quantum logic gate is universal,â€

Phys. Rev. Lett. 75, 346 (1995).

[51] N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-HerbrÂ¨uggen,
and S. J. Glaser, â€œOptimal control of coupled spin dy-
namics: design of nmr pulse sequences by gradient ascent
algorithms,â€ J. Magn. Reson. 172, 296â€“305 (2005).
[52] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush,
N. Ding, Z. Jiang, M. J. Bremner, J. M. Martinis, and
H. Neven, â€œCharacterizing quantum supremacy in near-
term devices,â€ Nat. Phys. 14, 595 (2018).

[53] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost,
â€œQuantum principal component analysis,â€ Nat. Phys. 10,
631 (2014).

[54] E. Stickel, â€œOn the frÂ´echet derivative of matrix func-
tions,â€ Linear Algebra and its Applications 91, 83â€“88
(1987).

[55] S. N. Ravi, M. D. Collins, and V. Singh, â€œA determinis-
tic nonsmooth frank wolfe algorithm with coreset guar-
antees,â€ preprint arXiv:1708.06714 (2017).

[56] F. Youseï¬an, A. NediÂ´c,

and U. V. Shanbhag, â€œOn
stochastic gradient and subgradient methods with adap-
tive steplength sequences,â€ Automatica 48, 56â€“67 (2012).
large-scale convex pro-
gramming under a linear optimization oracle,â€ preprint
arXiv:1309.5550 (2013).

[57] G. Lan, â€œThe complexity of

