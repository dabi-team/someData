9
1
0
2

n
u
J

6
2

]

Y
S
.
s
s
e
e
[

1
v
9
6
3
1
1
.
6
0
9
1
:
v
i
X
r
a

Approximate Dynamic Programming For Linear
Systems with State and Input Constraints

[Full version of paper presented at European Control Conference 2019]

Ankush Chakrabarty∗, Rien Quirynen, Claus Danielson, Weinan Gao†

June 26, 2019

Abstract

Enforcing state and input constraints during reinforcement learning (RL) in continuous state
spaces is an open but crucial problem which remains a roadblock to using RL in safety-critical
applications. This paper leverages invariant sets to update control policies within an approx-
imate dynamic programming (ADP) framework that guarantees constraint satisfaction for all
time and converges to the optimal policy (in a linear quadratic regulator sense) asymptoti-
cally. An algorithm for implementing the proposed constrained ADP approach in a data-driven
manner is provided. The potential of this formalism is demonstrated via numerical examples.

1 Introduction

Combining optimal control theory and reinforcement learning (RL) has yielded many excellent algo-
rithms for generating control policies that imbue the closed-loop system with a desired level of per-
formance in spite of unmodeled dynamics or modeling uncertainties [1,2]. Speciﬁcally, approximate
dynamic programming (ADP) (sometimes also referred to as adaptive dynamic programming), a
modern embodiment of RL [3, 4] applied to continuous state and action spaces has gained traction
for its ability to provide tractable solutions (in spite of the curse of dimensionality) to optimal
control problems via function approximation and iterative updates of control policies and value
functions [5, 6].

There are two main classes of ADP algorithms: policy iteration and value iteration [7]. A policy
iteration algorithm for discrete-time linear systems was formulated in [8] that leverages Q-functions
proposed in [9, 10], enabling control policy design without a complete system description. This
methodology has been extended to continuous-time systems [11], H2 and H∞ formulations [12,
13], tracking [14], output regulation [15], and game-theoretic settings [16, 17]. To reiterate, a
particularly beneﬁcial feature of this class of iterative methods is that control policies generated by
policy iteration converge to the optimal control policy with data obtained by exciting the system
dynamics, in spite of incomplete model knowledge [18, 19]. While optimality is important for
certifying performance in a control system, often times the more critical concern is safety. A key
aspect of safe control design is the ability of the system to respect both state and input constraints.

∗Corresponding author. Email: chakrabarty@merl.com. Phone: +1 (617) 758-6175.
†A. Chakrabarty, R. Quirynen, and C. Danielson are aﬃliated with the Control and Dynamical (CD) Systems
Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA. W. Gao is with the Department of Electrical
and Computer Engineering, Allen E. Paulson College of Engineering and Computing, Georgia Southern University,
Statesboro, GA 30460, USA.

1

 
 
 
 
 
 
To the best of our knowledge, this critical problem remains an open challenge in the context of
ADP (and RL at large) in continuous state and action spaces.

In this paper, we modify the classical policy iteration algorithm to incorporate safety through
constraint satisfaction. The key idea is to compute control policies and associated constraint
admissible invariant sets that ensure the system states and control inputs never violate design
constraints. In the spirit of ADP, these policies and invariant sets are computed iteratively, and the
sequence of policies are guaranteed to converge asymptotically to the optimal constraint-satisfying
policy, provided that the system is suﬃciently excited. The use of invariant sets to incorporate
safety in learning/adaptive control algorithms via constraint handling has been done in model-based
control design, such as model predictive control (MPC) [20–23], but its application to data-driven
or model-free RL methods is relatively unexplored. A recent paper [24] is a noteworthy exception,
although our method is distinct from this work in that we do not compute a model of the system
using the data obtained during operation; that is, our method is a direct data-driven approach, as
deﬁned in [25].

The main contributions of this paper are: (i) we extend classical policy iteration in continuous
state-action spaces to enforce state and input constraints; (ii) we provide a data-driven variant of
this constrained policy iteration algorithm with unknown state matrix; and, (iii) we provide new
suﬃcient conditions for safety (via constraint satisfaction), stability, and convergence of the policies
generated by our proposed algorithm to the optimal constrained control policy.

The rest of the paper is organized as follows. In Section 2 and 3, we present our notation and
describe the problem statement in formal terms. We also discuss the standard (unconstrained)
policy iteration algorithm. Our proposed alterations to the standard policy iteration algorithm for
enabling state and input constraint satisfaction is discussed in Section 4. Theoretical performance
certiﬁcates such as safety, stability, and algorithm convergence are provided in Section 5. Numerical
examples including a 2D illustrative example and a 5D example are provided in Section 6 to
illustrate the potential of this method. Conclusions are drawn in Section 7.

2 Notation

√

We denote by R the set of real numbers, R+ as the set of positive reals, and N as the set of
natural numbers. For every v ∈ Rn, we denote (cid:107)v(cid:107) =
v(cid:62)v, where v(cid:62) is the transpose of v. The
sup-norm is deﬁned as (cid:107)v(cid:107)∞ (cid:44) supt∈R (cid:107)v(t)(cid:107). We denote by σ(P ) and σ(P ) as the smallest and
largest singular value of a square, symmetric matrix P , respectively. The symbol (cid:31) (≺) indicates
positive (negative) deﬁniteness and A (cid:31) B implies A − B (cid:31) 0 for A, B of appropriate dimensions.
Similarly, (cid:23) ((cid:22)) implies positive (negative) semi-deﬁniteness. The operator norm is denoted (cid:107)P (cid:107)
and is deﬁned as the maximum singular value of P , vec(P ) denotes the column-wise vectorization
of P , and ⊗ denotes the Kronecker product. We parameterize an ellipsoid E ρ
= {x : x(cid:62)P0x ≤ ρ}
P0
using a scalar ρ > 0 and a matrix P0 (cid:31) 0.

3 Motivation

In this section, we begin by describing a general approximate dynamic programming formulation
for solving the unconstrained discrete-time LQR problem.

2

3.1 Problem Statement

We consider discrete-time linear systems of the form

xt+1 = Axt + But,

(1)

where t ∈ R is the time index, x ∈ X ⊂ Rn is the state of the system, u ∈ U ⊂ Rm is the control
input, and xt0 is a known initial state of the system. We assume the admissible state and input
constraints sets X and U are polytopic, and therefore, can be represented as

X (cid:48) =

(cid:21)

(cid:26)(cid:20)x
u

∈ Rn+m : c(cid:62)

i x + d(cid:62)

i u ≤ 1

(cid:27)

,

(2)

for i = 1, . . . , r, where r is the total number of state and input constraints and ci ∈ Rn and di ∈ Rm.
The sets X ⊂ Rn and U ⊂ Rm are known, compact, convex, and contain the origin in their interiors.
Note that with any ﬁxed control policy K, the constraint set described in (2) is equivalent to the
set

(cid:110)

x ∈ Rn : (c(cid:62)

i + d(cid:62)

i K)x ≤ 1

(cid:111)

,

(3)

for i = 1, . . . , r.

X(cid:48) =

Remark 1. The inequalities (2) deﬁne a polytopic admissible state and input constraint set. Note
that ci = 0 implies that the ith constraint is an input constraint, and di = 0 implies that it is a
state constraint.

The system matrix A and input matrix B have appropriate dimensions. We make the following
assumption on our knowledge of the system; these are standard assumptions in policy iteration
methods.

Assumption 1. The matrix A is unknown, the matrix B is known, and the pair (A, B) is stabiliz-
able. Furthermore, there exists a known control gain K0 such that u = K0x is a stabilizing control
policy for the system (1).

While the knowledge of the input matrix B is not needed in approaches like Q-learning [13], it
is fairly standard for policy improvement in policy iteration methods, even with function approxi-
mators [2]. From a practical perspective, it is not uncommon for a designer to have knowledge of
input channels and channel gains that represent the elements of the B matrix.

Our objective is to design an optimal control policy K∞ such that the state-feedback controller

u = K∞x stabilizes the partially known system (1) while minimizing a cost functional

V :=

∞
(cid:88)

t=0

t Qxt + u(cid:62)
x(cid:62)

t Rut

(4)

where Q (cid:23) 0 and R (cid:31) 0 are user-deﬁned symmetric matrices, with the pair (A, Q1/2) being
observable. The main contribution of this paper is to derive controller gains that stabilize the
system (1) while strictly enforcing state and input constraints.

3.2 Overview of optimal control for discrete-time LQR

Let the value function be deﬁned as

Vt(xt, ut) :=

∞
(cid:88)

k=t

k Qxk + u(cid:62)
x(cid:62)

k Ruk.

3

Clearly, Vt satisﬁes the recurrence relation

Vt(xt, ut) = x(cid:62)

t Qxt + u(cid:62)

t Rut + Vt+1(xt+1, ut+1).

We know from optimal control theory that the optimization problem

is solved in order to obtain the optimal control action

V∞(xt) := min

u

Vt(xt, ut)

u∞ := arg min

u

Vt(xt, ut)

(5)

(6)

(7)

for each time instant t ≥ t0. For discrete-time linear time-invariant systems of the form (1), we
know that the value function Vt is quadratic in the state [2]. Therefore, solving (6) is equivalent to
ﬁnding a symmetric matrix P∞ (cid:31) 0 that satisﬁes the equation

A(cid:62)P∞A − P∞ + Q − A(cid:62)P∞B

(cid:16)

R + B(cid:62)P∞B

(cid:17)−1

B(cid:62)P∞A = 0.

(8)

Upon solving for P∞, the optimal unconstrained discrete-time LQR policy generated by solving (7)
is given by

K∞ = −(R + B(cid:62)P∞B)−1B(cid:62)P∞A.

(9)

Since by assumption, the model A is unknown, one cannot directly compute P∞ from (8) or K∞
from (9). Instead, we resort to ADP, an iterative method for ‘learning’ the optimal control policy (9)
by using on-line data without knowing a full model of the system (1). A popular embodiment
of ADP is policy iteration, wherein an initial stabilizing control policy K0 is iteratively improved
using operational data, that is, without full model information. The sequence of control policies
converges asymptotically to the optimal control policy K∞ deﬁned in (9). The key steps of policy
iteration without constraints are described next.

3.3 Unconstrained policy iteration

Let Kt be the t-th policy iterate, where t ∈ N. Policy iteration has two key steps: policy evaluation
and policy improvement. We begin by describing the steps in model-based policy iteration and
subsequently demonstrate how to perform the same steps in a data-driven manner.

3.3.1 Model-based policy evaluation

In the policy evaluation step, the value function parameter Pt+1 (cid:31) 0 is estimated with the control
gain Kt using the relation

(A + BKt)(cid:62)Pt+1(A + BKt) − Pt+1 + Q + K(cid:62)

t RKt = 0.

(10)

Note that (10) can be derived from (5) when Vt = x(cid:62)
(A + BKt)xt.

t Ptxt and replacing ut = Ktxt and xt+1 =

4

3.3.2 Model-based policy improvement

Upon updating the value function via (10), one needs to update the corresponding control policy.
This is done by computing the new controller gain via

Kt+1 = −

(cid:16)

R + B(cid:62)Pt+1B

(cid:17)−1

B(cid:62)Pt+1A.

(11)

This equation is reminiscent of the optimal control policy equation (9); in fact, the unique stationary
point of the system of equations (10)–(11) is at Pt = P∞ and Kt = K∞ as demonstrated in [6].

This model-based implementation can be performed in a data-driven manner, described next.

3.3.3 Data-driven policy evaluation

We assume that policy iteration is performed a discrete-time instances ti where

T = {ti}∞
i=1

(12)

denotes the set of all policy iteration times. The minimum number of data-points obtain between
policy iterations [ti, ti+1] is given by

N = inf
i∈N

{ti+1 − ti|ti, ti+1 ∈ T },

(13)

that is, N denotes the minimum number of data points contained within any learning cycle. In a
model based implementation, T = N.

At each learning time instant ti ∈ T , one can rewrite (10) as

t P +xt = x(cid:62)
x(cid:62)

t Qxt + u(cid:62)

t Rut + x(cid:62)

t+1P +xt+1,

(14)

for every t ∈ {ti + 1, ti + 2, . . . , ti+1}, with P + representing the updated value function matrix.
Assuming that the state and input data is available to us, and that Q and R are known, we can
rewrite (14) as

ti+1Qxti+1 + u(cid:62)
x(cid:62)
ti+2Qxti+2 + u(cid:62)
x(cid:62)

ti+1Ruti+1
ti+2Ruti+2

∆xxvec(P +) =

...












,








ti+1Qxti+1 + u(cid:62)
x(cid:62)

ti+1Ruti+1

where

∆xx =






xti ⊗ xti − xti+1 ⊗ xti+1
...
xti+1 ⊗ xti+1 − xti+1+1 ⊗ xti+1+1




 .

Under well-known persistence of excitation conditions [2], one can solve (15) as a (regularized)
least squares problem subject to the constraint that P + (cid:31) 0 to obtain P + without knowing A or B.
For the time instants t ∈ T when the learning occurs, the new value function matrix Pt+1 is set to
P + obtained by solving (15). For other time instants between learning time instants, that is t /∈ T ,
the value function matrix obtained in the previous learning cycle is utilized, that is, Pt+1 := Pt.

5

(15)

(16)

3.3.4 Data-driven policy improvement

Since the control policy is restricted to be linear in this paper, ﬁnding an optimal policy is tanta-
mount to ﬁnding the minimizer Kt+1 of the optimization problem

min
K

ti+1
(cid:88)

(cid:16)

t=ti+1

t K(cid:62)RKxt + x(cid:62)
x(cid:62)

t Qxt + x(cid:62)

t (A + BK)(cid:62)Pt+1(A + BK)xt

(cid:17)

,

(17)

where ti, ti+1 ∈ T . This is a quadratic optimization problem in K because {xt}, Q, R, and Pt+1
are all known quantities in the window {ti + 1, ti + 2, . . . , ti+1}. Note that Kt+1 can be updated
recursively within each learning window ti ≤ t ≤ ti+1 using Pt+1 for these time instants. Since (17)
is a quadratic problem, using Newton-type iterative solvers are expected to yield quick convergence;
in this case, in one step.

4 Constrained ADP

In this section, we elucidate upon how to use invariant sets to generate new control policies that
are both stabilizing and constraint satisfying. We also propose an algorithm for implementing a
constrained ADP in a data-driven manner.
We begin with the following deﬁnition.

Deﬁnition 1 (CAIS). A non-empty set E within the admissible state space X is a constraint
admissible invariant set (CAIS) for the closed-loop system (1) under a control law u = Kx if, for
every initial condition xt0 ∈ E, all subsequent states xt ∈ E and inputs Kxt ∈ U for all t ≥ t0.

According to Assumption 1, the ADP iteration is initialized with a stabilizing linear controller
K0. This stabilizing controller renders a subset of the state-space invariant while satisfying state
and input constraints. In particular, there exists an ellipsoidal region

such that E ρ
P0
initial CAIS ellipsoid E ρ
P0

⊂ X and K0E ρ
P0

E ρ
P0

= {x : x(cid:62)P0x ≤ ρ},

⊂ U. We assume that the value function matrix P0 deﬁning the

is known. This is encapsulated formally herein.

Assumption 2 (Constrained ADP). There exists a symmetric positive deﬁnite matrix P0 such
that E ρ
⊂ X is a CAIS for the closed-loop system (1) under the initial control policy u = K0x, and
P0
K0x ∈ U for all x ∈ E ρ
P0

.

4.1 Model-based constrained policy iteration

4.1.1 Model-based constrained policy evaluation

Let

Jt(P ) := (A + BKt)(cid:62)P (A + BKt) − P + Q + K(cid:62)

t RKt.

6

In order to implement constrained model-based policy evaluation (that is, obtain Pt+1 from Kt and
Pt), we need to solve the following semi-deﬁnite programming problem:

Pt+1, ρt+1 = arg min

P,ρ

(cid:107)Jt(P )(cid:107)

subject to:

(A + BKt)(cid:62)P (A + BKt) − λP (cid:22) 0
x(cid:62)
t P xt ≤ ρ
(c(cid:62)
k + d(cid:62)
α1I (cid:22) P (cid:22) α2I
ρ > 0

k Kt)(cid:62)ρ (c(cid:62)

k Kt) (cid:22) P

k + d(cid:62)

for some α1, α2 > 0 and

λ <

(cid:19)2/N

.

(cid:18) α1
α2

(18a)

(18b)

(18c)

(18d)

(18e)

(18f)

(19)

Here, k ∈ {1, . . . , r}. Note that ensuring this problem is convex involves ﬁxing the scalars α1 and
α2, and pre-computing λ using (19).

The rationale behind (18) can be explained as follows. Since (18e) ensures that P (cid:31) 0, this
constraint, along with the objective (18a), is equivalent to (10), which is identical to the uncon-
strained policy evaluation step. Therefore, constraint satisfaction is made possible by equipping
the constraints (18b)–(18d) and (18f).

The inequality (18b) ensures that the value function is contractive, and therefore, non-increasing
for every t ≥ t0. To see this, we multiply (18b) by x(cid:62) and x from the left and right, respectively,
which yields

t+1P xt+1 − x(cid:62)
x(cid:62)

t P xt ≤ −(1 − λ) x(cid:62)

t P xt < 0,

for any t, since 0 < λ < 1. This is a key ingredient to ensure that the updated control policies
will provide stability certiﬁcates for the closed-loop system. The two inequalities (18c) and (18d)
enforce that the state and input constraints with the current policy are satisﬁed in spite of the value
function update, given the current state xt. The condition (18e) ensures that the value function
matrix P is positive deﬁnite, and the positive scalar ρ allows the selection of sub- and super-
level sets of the Lyapunov function. More details about how these conditions relate to theoretical
properties of the proposed constrained ADP algorithm are provided in Section 5.

4.1.2 Model-based constrained policy improvement

Unlike unconstrained policy iteration, adding state and input constraints could result in nonlinear
optimal control policies. In this paper, we restrict ourselves to design linear control policies of the
form u = Kx, and hence, our optimal policy improvement step is analogous to the unconstrained
case (11), that is,

K(cid:63)

t+1 = −

R + B(cid:62)Pt+1B

B(cid:62)Pt+1A.

(20)

(cid:16)

(cid:17)−1

Remark 2. In spite of parameterizing via linear control policies, our controller is actually nonlinear
since Kt depends on Pt which depends on the states through (18).

We adopt a backtracking strategy in order to update the current constrained policy Kt to a new
t+1 in (20) that

constrained policy Kt+1 that is as close as possible to the unconstrained policy K(cid:63)

7

enforces state and input constraints. A simpliﬁed version of this backtracking strategy is outlined
in Algorithm 1.

Algorithm 1 Constrained Policy Improvement: Backtracking
Input: Desired policy K(cid:63)
t+1 and current constrained policy Kt.
1: Kt+1 ← K(cid:63)
t+1, α ← 1.
2: while (c(cid:62)
3:

i Kt+1) (cid:14) Pt+1 do

i Kt+1)(cid:62)ρ (c(cid:62)

i + d(cid:62)

i + d(cid:62)

α ← βα, where 0 < β < 1.
Kt+1 ← Kt + α (cid:0)K(cid:63)
t+1 − Kt

(cid:1).

4:

Remark 3. Note that the conditional statement in step 2 can be implemented eﬃciently based on
a Cholesky factorization to check whether this particular symmetric matrix is positive deﬁnite.

A particular beneﬁt of our proposed method is that it enables both expansion, contraction,
and rotation of the constraint admissible invariant sets. This is important in reference tracking for
instance where a more aggressive controller is required when the state is near the boundary of the
state constraints. This could also be useful for applying this approach to nonlinear systems where
(A, B) is a local linear approximation of the globally nonlinear dynamics. Our approach allows the
ellipsoidal invariant sets to adapt its size and shape based on the local vector ﬁeld. For example,
suppose EP∞ denote the CAIS that is associated with the constrained optimal control policy K∞
and optimal value function deﬁned by P∞. Also suppose that we have an initial admissible policy
K0 whose associated CAIS E ρ
is contained within EP∞. Then our proposed method will generate
P0
a sequence of EPt such that these invariant sets will expand, contract, and rotate as necessary until
the sequence of invariant sets {EPt} converges to the optimal EP∞.

4.2 Data-driven constrained policy iteration

In order to obtain a data-driven implementation of the constrained ADP method, one needs to
gather a sequence of state-input data points {¯xt, ¯ut, ¯xt+1} and control policies { ¯Kt} which will be
used to update the value function matrix and control policies at the learning time instants deﬁned
by T in (12). Given the discrete-time system dynamics in (1), the relation between these data
points is given by

¯xt+1 = A¯xt + B ¯ut = A¯xt + B (cid:0) ¯Kt ¯xt + νt
where νt represents a known exploration noise signal that ensures the system (21) is persistently
excited; see [2]. To arrive at a more compact notation, let us deﬁne

(21)

(cid:1) ,

˜xt+1 := ¯xt+1 − Bνt and ˜ut := ¯Kt ¯xt

such that

˜xt+1 = A¯xt + B ˜ut = (A + B ¯Kt)¯xt.

(22)

4.2.1 Data-driven constrained policy evaluation

Consider the i-th learning cycle, occuring at the time instant ti ∈ T . Let

¯Jt(P ) := ˜x(cid:62)

t+1P ˜xt+1 − ¯x(cid:62)

t P ¯xt + ¯xtQ¯xt + ˜u(cid:62)

t R ˜ut.

8

The data-driven analogue of the constrained policy evaluation step discussed in the previous section
is given by the following semi-deﬁnite program (SDP) with α1 and α2 ﬁxed:

¯Pt+1, ρt+1 := arg min
ρ,P

1
2

ti+1−1
(cid:88)

t=0

(cid:0) ¯Jt(P )(cid:1)2 − λρρ

subject to:

t P ¯xt ≤ 0

t+1P ˜xt+1 − λ¯x(cid:62)
˜x(cid:62)
x(cid:62)
ti+1P xti+1 ≤ ρ
(c(cid:62)
k + d(cid:62)
k
α1I (cid:22) P (cid:22) α2I
ρ > 0,

¯Kt)(cid:62)ρ (c(cid:62)

k + d(cid:62)
k

¯Kt) (cid:22) P

(23a)

(23b)

(23c)

(23d)

(23e)

(23f)

for t ∈ {ti + 1, ti + 2, . . . , ti+1} and k ∈ {1, . . . , r}. Note that the ﬁnal four inequalities in (23) are
exactly the set of inequalities presented in (18) with the model information replaced by state and
input data. Also, replacing ˜xt+1 in (23b) with (A + B ¯Kt)¯xt using equality (22) shows that it is
equivalent to the inequality (18b).

4.2.2 Data-driven constrained policy improvement

Once a value function is found whose sub-level set is constraint admissible, the corresponding policy
Kt+1 is to be computed. If A and B are known, this step would be easy: indeed, one could utilize
Eq. (20) to this end. However, since only B is known (by assumption), we resort to a data-driven
iterative update methodology for generating the new policy.

Given the current policy Kt, we gather another batch of measurements {¯xt, ¯ut, ¯Kt, ¯xt+1}t=ti+1,...,ti+1

where a new policy ¯Kt is the optimizer of the least squares problem

min
K

1
2

ti+1
(cid:88)

(cid:16)

¯x(cid:62)
t

t=ti+1

K(cid:62)RK + (A + BK)(cid:62) ¯Pt+1(A + BK)

(cid:17)

¯xt.

(24)

The problem (24) can be solved in a data-driven manner eﬃciently using a real-time recursive least
squares (RLS) implementation [26]

t ⊗ (R + B(cid:62) ¯Pt+1B),
Ht+1 = Ht + ¯xt ¯x(cid:62)
gt+1 = ¯xt ⊗ (R ¯Kt ¯xt + B(cid:62) ¯Pt+1 ˜xt+1),

vec( ¯Kt+1) = vec( ¯Kt) − βt H −1

t+1 gt+1,

(25a)

(25b)

(25c)

for t = ti + 1, . . . , ti+1 − 1. Note that (25) is solved without knowledge of A using the updates.
Also, the starting value for the Hessian matrix is chosen as the identity matrix ρ I and ρ > 0
to ensure non-singularity. The step size βt is typically equal to one, even though a smaller step
βt ≤ 1 can be chosen, e.g., based on the backtracking procedure in Algorithm 1 in order to impose
the aﬃne state and input constraints in (23d) for each updated control policy ¯Kt+1. The Hessian
matrix in (25) can be reset to H = q I (cid:31) 0 whenever a new value function is obtained from solving
the SDP in (23). Note that (25a) corresponds to a rank-m matrix update, where m denotes the
number of control inputs. Therefore, its matrix inverse H −1
t+1 can be updated eﬃciently using the
Sherman-Morrison formula, for example, in the form of m rank-one updates.

9

4.2.3 Algorithm Implementation: Pseudocode

Algorithm 2 provides a detailed description of our proposed approach for data-driven constrained
adaptive dynamic programming for linear systems. The general procedure corresponds to the
sequence of high-level steps:

(i) We require an initial stabilizing policy ¯K0 and a corresponding constraint admissible invariant

set (CAIS) E ρ
P0

; see Assumptions 1 and 2.

(ii) Obtain a sequence of at least ti+1 data points {¯xt, ¯ut, ¯Kt, ¯xt+1} while the system is persistently
excited and compute a new ellipsoidal set deﬁned by the matrix ¯Pt+1 by solving the least
squares SDP in (23).

(iii) At each time step, perform the policy improvement step to compute ¯Kt+1 based on the real-
time recursive least squares method as described in (25), in combination with the backtracking
procedure of Algorithm 1 to enforce state and input constraints.

(iv) If the policy improvement has converged based on the condition (cid:107)gt(cid:107) ≤ (cid:15), return to step (ii).

(see Assumption 2), initial state value ¯x0 and (cid:15) > 0.

Algorithm 2 Data-driven constrained ADP
Input: Initial policy ¯K0 and CAIS E ρ
P0
1: D ← {}.
2: for t = 0, 1, . . . do
3:

Apply control input ¯ut = νt + ¯Kt ¯xt.
Obtain new state estimate ¯xt+1.
if (cid:107)gt(cid:107) ≤ (cid:15) then:
D ← {D, t}.

/* Policy evaluation step (SDP) */
if PE condition holds with data ∀ t ∈ D then:

Compute ρ+, P + by solving SDP (23) based on
stored buﬀer of data points {¯xt, ¯ut, ¯Kt, ¯xt+1}t∈D.
¯Pt+1, ρt+1 ← P +, ρ+.
Reset buﬀer D ← {}.

else

¯Pt+1, ρt+1 ← ¯Pt, ρt.

/* Policy improvement step (RLS) */
Compute new policy ¯Kt+1 as in (25), using Alg. 1,
given new measurements (¯xt, ¯ut, ¯Kt, ¯xt+1) and ¯Pt+1.

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

(cid:46) to system

(cid:46) from system

(cid:46) convergence check
(cid:46) add data time stamp

(cid:46) deﬁne new ellipsoid

4.3 Remarks on computational complexity

Semideﬁnite programs (SDP) of the form (23) are convex optimization problems that can be solved
in polynomial time, for example, using interior point methods (IPMs). However, in general, stan-
dard implementations of IPMs for solving SDPs have a computational complexity O(n6) when
solving for n × n matrix variables and a memory complexity of O(n4) [27, 28]. Instead, the per
iteration complexity and memory requirements for ﬁrst order optimization algorithms such as, e.g.,
the alternating direction method of multipliers (ADMM) can be much smaller, even though they

10

typically require more iterations in practice [29, 30]. Note that, instead, the policy improvement
steps are computationally cheap because both the low-rank update techniques for the Hessian
matrix (25a) and the matrix-vector multiplication in (25c) can be performed with a complexity
O(n2m2) that scales quadratically with the dimensions of the policy matrix K.

The policy evaluation step, based on the SDP solution in (23), could be computed also using
a recursive least squares type implementation or in a receding horizon or sliding window manner.
However, given the computational complexity of treating linear matrix inequalities in the SDP
formulation, a batch-type approach as in Algorithm 2 would typically be preferred for real-time
feasible control applications under strict timing requirements. Additionally, it is important to note
that the SDP solution in Algorithm 2 is not necessarily required to be real-time feasible, unlike the
recursive least squares based policy improvement step in (25), which is computationally cheap.

5 Constraint Satisfaction, Stability, and Algorithm Convergence

We present theoretical guarantees for our proposed constrained policy iteration. For the data-driven
case, we adhere to the standard assumption that the system is persistently excited. The following
theorem demonstrates constraint enforcement and stability guarantees of the closed-loop system.

Theorem 1. Suppose Assumptions 1 and 2 hold. Then the system (1) in closed-loop with the
time-varying controller ut = Ktxt has the following properties:

(i) The constraints xt ∈ X and ut ∈ U are satisﬁed for all t ∈ N.

(ii) The closed-loop system is asymptotically stable.

Proof. (i) Consider the ellipsoid E ρ

Pt+1

. The inequality (18d) yields

1
ρ

Pt+1 (cid:23) (c(cid:62)

i + d(cid:62)

i K)(cid:62)(c(cid:62)

i + d(cid:62)

i K).

For x ∈ E ρ

Pt+1

we have x(cid:62)P x ≤ ρ. Thus,

x(cid:62)(c(cid:62)

i + d(cid:62)

i K)(cid:62)(c(cid:62)

i + d(cid:62)

i K)x ≤ 1

which implies x ∈ X(cid:48) since (c(cid:62)
constraints are satisﬁed for all states inside the ellipsoid E ρ

i K)x ≤ 1. Thus, E ρ

i + d(cid:62)

Pt+1

.

Pt+1

⊆ X(cid:48), which implies that state and input

Note that the state xt is contained in this ellipsoid while the t-th controller ut = Ktxt is active.
This can be inferred from (18b) and (18c), which (respectively) imply that the ellipsoid E ρ
is
positive-invariant and that the initial state is contained in the ellipsoid when the controller is ﬁrst
engaged.

Pt+1

(ii) Since the closed-loop system is a switched system, we will use the concept of multiple

Lyapunov functions to prove stability. Consider the set of Lyapunov functions

VP (x) = x(cid:62)P x

for all P that satisﬁes (18). We will show that the t-th controller ut = Ktxt decreases all of these
Lyapunov functions1 over the time period t ∈ {ti + 1, ti + 2, . . . , ti+1} for the i-th learning cycle in

1The Lyapunov functions do not necessarily decrease monotonically for all t as long as the function values are

decreasing at each learning instant ti.

11

which it was engaged (recall ti, ti+1 ∈ T deﬁned in (12)). Note

VP

(cid:0)xti+1

(cid:1) ≤

≤

<

α2
α1
α2
α1
α1
α2

VPt+1

(cid:0)xti+1

(cid:1)

λN VPt+1 (xti)

VPt+1 (xti)

≤ VP (xti) ,

where the ﬁrst and last inequalities are a consequence of (18e). The second inequality is a con-
sequence of (13) and (18b), along with Algorithm 1 which uses a convex combination of Kt and
K(cid:63)
t+1, both of which are guaranteed to contract the value function by λ. The third inequality is
(cid:1) < VP (xti), each Lyapunov function VP (x)
(cid:0)xti+1
a consequence of the condition (19). Since VP
converges to zero. As (18e) ensures that these Lyapunov functions are positive-deﬁnite, we get
xt → 0 as t → ∞, which concludes the proof.

Previous stability results for approximate dynamic programming rely on the tacit assumption
that the learning converges after a ﬁnite number of batch iterations (typically one). In other words,
the adaptive controller only works because it stops adapting. In contrast, for constraint satisfaction,
the controller may need to continually adapt since the set of active constraints will change as the
state evolves. This necessitates the development of a more involved set of conditions to ensure that
feedback control loop and the learning loop do not destabilize each other.

Theorem 2. Suppose Assumptions 1 and 2 hold. Let α1 ≤ σ(P∞), α2 ≥ σ(P∞), and

(cid:16)

λ ≥ σ

I − P −1/2

∞ (Q + K(cid:62)

∞RK∞)P −1/2

∞

(cid:17)

.

Under the iteration (18) and (20), the value Pt and policy Kt converge to the LQR cost-to-go P∞
and controller gain K∞. That is,

lim
t→∞

Pt = P∞ and

lim
t→∞

Kt = K∞.

(26)

Proof. If feasible, the LQR cost-to-go P∞ with controller gain K∞ will be the optimal solution
of (18). By the assumptions on α1, α2, the LQR value P∞ satisﬁes (18e). Note that

σ(cid:0)I − P −1/2

∞ (Q + K(cid:62)

(cid:1)

∞RK∞)P −1/2
∞ (Q + K(cid:62)

∞

z(cid:62)(I − P −1/2

= sup

z

= sup

x

z(cid:62)z
∞RK∞))x

x(cid:62)(P∞ − (Q + K(cid:62)
x(cid:62)P∞x

∞RK∞)P −1/2

∞ )z

where z = P −1/2

∞ x. Thus, the LQR satisﬁes

(A + BK∞)(cid:62)P∞(A + BK∞)
= P∞ − (Q + K(cid:62)
∞RK∞)
≤ σ(cid:0)I − P −1/2
∞ (Q + K(cid:62)
≤ λP∞.

∞RK∞)P −1/2

∞

(cid:1)P∞

12

Thus, the LQR value P∞ and policy K∞ satisfy (18b).

P∞

Finally, we show that (18d) and (18c) are satisﬁed at some ﬁnite time T ≥ t0. Since, the state
X and input U constraints contain the origin in their interiors, there exists ρ > 0 such that the
ellipsoidal region E ρ

of the LQR cost-to-go x(cid:62)P∞x satisﬁes (18d).

Furthermore, since the closed-loop system is asymptotically stable and E ρ
Pt

is a CAIS for every
t ≥ t0, there exists a ﬁnite time T ≥ t0 such that xt ∈ E ρ
for all t ≥ T . Thus, (18c) will be
Pt
satisﬁed by the LQR controller after time T . As for all t ≥ T , the state and input constraints are
automatically satisﬁed (and are therefore, inactive), one can use the same arguments as in classical
model-based policy iteration [5] to conclude the proof.

Remark 4. In order to solve SDP (23) using least squares methods, one need to collect a sequence
of states such that the matrix ∆¯x¯x (obtained by replacing x in (16) with ¯x) has full column rank.
This full rank condition is like the condition of persistence excitation (PE) in adaptive control
theory. In order to satisfy this full rank condition, we add an exploration noise νt into the input to
excite the system as in [2, 8, 13]. As the exploration noise νt goes to zero, the solution to (23) will
converge to the solution to model-based constrained policy evaluation problem (18).

6 Numerical Example

6.1 Linear system with two states, one control input

(cid:3) , B = (cid:2) −0.5507
0.0758

We randomly generate controllable systems of the form (1) to test the proposed algorithm. A partic-
ular realization of these randomly generated systems, A = (cid:2) 1.1387 0.0491
(cid:3) is inves-
−0.8680 0.9679
tigated to illustrate constraint satisfaction and stability of the algorithm. Of course, A is unknown
(and unstable), B is known, and it is veriﬁed that (A, B) is a controllable pair. The admissible state
space is given by X = {x ∈ R2 : (cid:107)x(cid:107)∞ ≤ 1}, and the operational cost is parameterized by Q = I2
and R = 0.5. For learning, the window length is ﬁxed at N = 8 samples (T = {8, 16, 24, . . .}), and
the regularization parameter for policy updating is given by ρK = 10−4. Persistence excitation is
ensured by generating uniformly distributed noise bounded within [−0.02, 0.02]. An initial policy is
generated that satisﬁes state constraints using the randomly chosen cost matrices that are distinct
from Q and R, and an initial condition is generated randomly on the boundary of the initial domain
of attraction. Therefore, the initial state is ensured to be within X but suﬃciently far from the
origin to require non-trivial control for stabilization.

The results of the constrained policy iteration algorithm are illustrated in Fig. 1. In Fig. 1[A],
a sequence of ellipsoids generated by our proposed algorithm is presented. Note that the ellipsoids
generated in subsequent learning cycles after the ﬁrst (the orange elongated ellipsoid) are not mere
sub- or super-level sets of the initial ellipsoid; instead, the policy iterator allows for contractions
and expansions on both x1 and x2 axes until the true policy is learned. As evident from subplots
[B] and [C], state constraints are not violated throughout the learning procedure. The subplots
[D, E] demonstrate the convergence of a sub-optimal initial control policy at t = 0 to the true and
optimal LQR policy at around t = 24, after three learning cycles.

6.2 Higher-dimensional linear system

In addition, let us illustrate the performance for a dynamic system with 5 states and 2 control
inputs. For this purpose, we have randomly generated 50 unique dynamic systems (1) that are
constructed to be (slightly) unstable but controllable, such that an initial stabilizing policy can be
obtained. Similar to before, the system matrix A is unknown but the input matrix B is known.

13

Figure 1: Results of constrained ADP for 2-state dynamic system: [A] Sequence of invariant sets
learned on-line. Each set is labeled with the time iteration t when it was learned.
[B] State
evolution (x1: blue, x2: red) with constraints (black, dashed). [C] Control input (blue) evolution
with constraints (black, dashed). [D, E] Convergence of learned LQR policy to the true LQR policy.

[A] State evolution
Figure 2: Results of constrained ADP for 5-state, 2-input dynamic system:
with constraints (black, dashed). [B] Control input evolution with constraints (black, dashed). [C]
Convergence of learned LQR policy to the true LQR policy, using 2-norm of the error.

The admissible state and input space, respectively, is given by X = {x ∈ R5 : (cid:107)x(cid:107)∞ ≤ 1} and
U = {u ∈ R2 : (cid:107)u(cid:107)∞ ≤ 1}, and the cost matrices read as Q = I5 and R = 0.5I2. Figure 2

14

[A][B][C][D][E]presents the closed-loop state and input trajectories when applying the proposed constrained ADP
implementation (see Algorithm 2) to each of these generated test problems. Note that the learning
window has been chosen to be equal to N = 30 samples; indicating that T = {30, 60, . . .}.

From Figure 2, it can be observed that the 5-dimensional dynamic system is stabilized (the
small perturbations in subplot [A] and [B] are due to the exploratory noise) in all of the generated
test cases and both the state and input constraints are respected at all time. In addition, in most
of the cases, the optimal policy is obtained relatively quickly in an amount of time that corresponds
to 2 learning window lengths, i.e., 60 time steps in Figure 2. The policy error, computed using the
matrix 2-norm of the diﬀerence between the current and optimal policy, generally decreases over
time for all cases, under the necessary condition for persistence of excitation.

7 Concluding Remarks

In this paper, we provide a methodology for implementing constraint satisfying policy iteration
for continuous-time, continuous-state systems via invariant sets. Beneﬁts of our approach include
computational tractability, and safety guarantees through constraint satisfaction. In future work,
we will extend this framework to nonlinear systems.

References

[1] F. L. Lewis and D. Vrabie, “Reinforcement learning and adaptive dynamic programming for

feedback control,” IEEE Circuits and Systems Magazine, vol. 9, no. 3, pp. 32–50, 2009.

[2] F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, “Reinforcement learning and feedback control:
Using natural decision methods to design optimal adaptive controllers,” IEEE Control Systems,
vol. 32, no. 6, pp. 76–105, 2012.

[3] D. P. Bertsekas, Dynamic Programming and Optimal Control, 2nd ed. Athena Scientiﬁc,

2000.

[4] B. Kiumarsi, K. G. Vamvoudakis, H. Modares, and F. L. Lewis, “Optimal and Autonomous
Control Using Reinforcement Learning: A Survey,” IEEE Transactions on Neural Networks
and Learning Systems, vol. 29, no. 6, pp. 2042–2062, 2018.

[5] G. Hewer, “An iterative technique for the computation of the steady state gains for the
discrete optimal regulator,” IEEE Transactions on Automatic Control, vol. 16, no. 4, pp.
382–384, aug 1971. [Online]. Available: http://ieeexplore.ieee.org/document/1099755/

[6] D. L. Kleinman, “On an iterative technique for riccati equation computations,” IEEE Trans.

on Automatic Control, vol. 13, no. 1, pp. 114–115, 1968.

[7] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 2011.

[8] S. J. Bradtke, B. E. Ydstie, and A. G. Barto, “Adaptive linear quadratic control using policy
iteration,” in Proc. of the American Control Conference, vol. 3. Citeseer, 1994, pp. 3475–3475.

[9] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3-4, pp. 279–292,

1992.

[10] P. J. Werbos, “Neural networks for control and system identiﬁcation,” in Proc. of the 28th
IEEE, 1989, pp. 260–265.

IEEE Conf. Dec. and Control.

15

[11] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. L. Lewis, “Adaptive optimal control for
continuous-time linear systems based on policy iteration,” Automatica, vol. 45, no. 2, pp.
477–484, 2009.

[12] T. Landelius, “Reinforcement learning and distributed local model synthesis,” Ph.D. disserta-

tion, Link¨oping University Electronic Press, 1997.

[13] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, “Model-free Q-learning designs for linear
discrete-time zero-sum games with application to H∞ control,” Automatica, vol. 43, no. 3, pp.
473–481, 2007.

[14] R. Kamalapurkar, H. Dinh, S. Bhasin, and W. E. Dixon, “Approximate optimal trajectory
tracking for continuous-time nonlinear systems,” Automatica, vol. 51, pp. 40–48, 2015.

[15] W. Gao and Z.-P. Jiang, “Adaptive dynamic programming and adaptive optimal output reg-
ulation of linear systems,” IEEE Transactions on Automatic Control, vol. 61, no. 12, pp.
4164–4169, 2016.

[16] Q. Zhang, D. Zhao, and Y. Zhu, “Data-driven adaptive dynamic programming for continuous-
time fully cooperative games with partially constrained inputs,” Neurocomputing, vol. 238, pp.
377–386, 2017.

[17] H. Zhang, H. Jiang, C. Luo, and G. Xiao, “Discrete-time nonzero-sum games for multiplayer
using policy-iteration-based adaptive dynamic programming algorithms,” IEEE transactions
on cybernetics, vol. 47, no. 10, pp. 3331–3340, 2017.

[18] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, “Discrete-time nonlinear hjb solution using
approximate dynamic programming: Convergence proof,” IEEE Transactions on Systems,
Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 4, pp. 943–949, 2008.

[19] A. Heydari, “Revisiting Approximate Dynamic Programming and its Convergence,” IEEE

Transactions on Cybernetics, vol. 44, no. 12, pp. 2733–2743, 2014.

[20] M. V. Kothare, V. Balakrishnan, and M. Morari, “Robust constrained model predictive control

using linear matrix inequalities,” Automatica, vol. 32, no. 10, pp. 1361–1379, 1996.

[21] A. Chakrabarty, V. C. Dinh, M. J. Corless, A. E. Rundell, S. H. Zak, G. T. Buzzard et al., “Sup-
port vector machine informed explicit nonlinear model predictive control using low-discrepancy
sequences.” IEEE Trans. Automat. Contr., vol. 62, no. 1, pp. 135–148, 2017.

[22] K. Berntorp, A. Weiss, C. Danielson, I. V. Kolmanovsky, and S. Di Cairano, “Automated
driving: Safe motion planning using positively invariant sets,” in Proc. of the 20th Int. Conf.
on Intelligent Transportation Sys. (ITSC).

IEEE, 2017, pp. 1–6.

[23] F. Berkenkamp, R. Moriconi, A. P. Schoellig, and A. Krause, “Safe learning of regions of
attraction for uncertain, nonlinear systems with gaussian processes,” Proc. of the IEEE Conf.
Decision and Control, pp. 4661–4666, 2016.

[24] Z. Li, U. Kalabi´c, and T. Chu, “Safe reinforcement learning: Learning with supervision using
IEEE,

a constraint-admissible set,” in Proc. of the American Control Conference (ACC).
2018, pp. 6390–6395.

16

[25] D. Piga, S. Formentin, and A. Bemporad, “Direct data-driven control of constrained systems,”
IEEE Transactions on Control Systems Technology, vol. 26, no. 4, pp. 1422–1429, 2018.

[26] L. Ljung, System identiﬁcation: Theory for the User. Upper Saddle River, N.J.: Prentice

Hall, 1999.

[27] M. J. Todd, “Semideﬁnite optimization,” Acta Numerica, vol. 10, p. 515560, 2001.

[28] L. Vandenberghe and S. Boyd, “Semideﬁnite programming,” SIAM Review, vol. 38, no. 1, pp.

49–95, 1996.

[29] Z. Wen, D. Goldfarb, and W. Yin, “Alternating direction augmented lagrangian methods
for semideﬁnite programming,” Mathematical Programming Computation, vol. 2, no. 3, pp.
203–230, Dec 2010.

[30] Y. Zheng, G. Fantuzzi, A. Papachristodoulou, P. Goulart, and A. Wynn, “Fast ADMM for
semideﬁnite programs with chordal sparsity,” in 2017 American Control Conference (ACC),
May 2017, pp. 3335–3340.

17

