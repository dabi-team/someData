1
2
0
2

c
e
D
1

]

G
L
.
s
c
[

1
v
9
7
5
0
0
.
2
1
1
2
:
v
i
X
r
a

CONDITIONAL EXPECTATION BASED
VALUE DECOMPOSITION FOR
SCALABLE ON-DEMAND RIDE POOLING

Avinandan Bose1, Pradeep Varakantham2
IIT Kanpur1, Singapore Management University2
avibose@iitk.ac.in,pradeepv@smu.edu.sg

ABSTRACT

Owing to the beneﬁts for customers (lower prices), drivers (higher revenues), ag-
gregation companies (higher revenues) and the environment (fewer vehicles), on-
demand ride pooling (e.g., Uber pool, Grab Share) has become quite popular. The
signiﬁcant computational complexity of matching vehicles to combinations of re-
quests has meant that traditional ride pooling approaches are myopic in that they
do not consider the impact of current matches on future value for vehicles/drivers.
Recently, Neural Approximate Dynamic Programming (NeurADP) has employed
value decomposition with Approximate Dynamic Programming (ADP) to outper-
form leading approaches by considering the impact of an individual agent’s (vehi-
cle) chosen actions on the future value of that agent. However, in order to ensure
scalability and facilitate city-scale ride pooling, NeurADP completely ignores the
impact of other agents actions on individual agent/vehicle value. As demonstrated
in our experimental results, ignoring the impact of other agents actions on indi-
vidual value can have a signiﬁcant impact on the overall performance when there
is increased competition among vehicles for demand. Our key contribution is a
novel mechanism based on computing conditional expectations through joint con-
ditional probabilities for capturing dependencies on other agents actions without
increasing the complexity of training or decision making. We show that our new
approach, Conditional Expectation based Value Decomposition (CEVD) outper-
forms NeurADP by up to 9.76% in terms of overall requests served, which is a
signiﬁcant improvement on a city wide benchmark taxi dataset.

1

INTRODUCTION

Taxi/car on Demand (ToD) services (e.g., UberX, Lyft, Grab) not only provide a comfortable means
of transport for customers, but also are good for the environment by enabling sharing of vehicles over
time (while being used to serve one request at any one point in time). A further improvement of ToD
is on-demand ride pooling (e.g., UberPool, LyftLine, GrabShare etc.), where vehicles are shared not
only over time but also in space (on the taxi/car). On-demand ride pooling reduces the number
of vehicles required, thereby reducing emissions and trafﬁc congestion compared to Taxi/car on-
Demand (ToD) services. This is achieved while providing beneﬁts to all the stakeholders involved:
(a) Individual passengers have reduced costs due to sharing of space; (b) Drivers make more money
per trip as multiple passengers (or passenger groups) are present; (c) For the aggregation company
more customer requests can be satisﬁed with the same number of vehicles.

In this paper, we focus on this on-demand ride pooling problem at city scale, referred to as Ride-
Pool Matching Problem (RMP) [Alonso-Mora et al. (2017); Bei & Zhang (2018); Lowalekar et al.
(2019)]. The goal in an RMP is to assign combinations of user requests to vehicles (of arbitrary
capacity) online such that quality constraints (e.g., delay in reaching destination due to sharing is not
more than 10 minutes) and matching constraints (one request can be assigned at most one vehicle,
one vehicle must be assigned at most one request combination) are satisﬁed while maximizing an
overall objective (e.g., number of requests, revenue). Unlike the ToD problem that requires solving a
bipartite matching problem between vehicles and customers, RMP requires effective matching on a
tripartite graph of requests, trips (combinations of requests) and vehicles. This matching on tripartite

1

 
 
 
 
 
 
graph signiﬁcantly increases the complexity of solving RMP online, especially at city scale where
there are hundreds or thousands of vehicles, hundreds of requests arriving every minute and request
combinations have to be computed for each vehicle.

Due to this complexity and the need to make decisions online, most existing work related to solving
RMP has focused on computing best greedy assignments [Ma et al. (2013); Tong et al. (2018); Huang
et al. (2014); Lowalekar et al. (2019); Alonso-Mora et al. (2017)]. While these scale well, they are
myopic and, as a result, do not consider the impact of a given assignment on future assignments.
The closest works of relevance to this paper are by Shah et al. [Shah et al. (2020)] and Lowalekar
et al. [Lowalekar et al. (2021)]. We speciﬁcally focus on the work by Shah et al., as it has the best
performance, while being scalable. That work considers future impact of current assignment from
an individual agents’ perspective without sacriﬁcing on scalability (to city scale). However, a key
limitation of that work is that they do not consider the impact of other agents (vehicles) actions on
an agents’(vehicle) future impact, which as we demonstrate in our experiments can have a major
effect (primarily because vehicles are competing for the common demand).

To that end, we develop a conditional expectation based value decomposition approach that not only
considers future impact of current assignments but also of other agents state and actions through the
use of conditional probabilities and tighter estimates of individual impact. Due to these conditional
probability based tighter estimates of individual value functions, we can scale the work by Guestrin
et al. [Guestrin et al. (2002)] and Li et al. [Li et al. (2021)] to solve problems with no explicit
coordination graphs and hundreds/thousands of homogeneous agents. Unlike value decomposition
approaches [Rashid et al. (2018); Sunehag et al. (2018)] developed for solving cooperative Multi-
Agent Reinforcement Learning (MARL) with tens of agents and under centralized training and
decentralized execution set up, we focus on problems with hundreds or thousands of agents with
centralized training and centralized execution (e.g., Uber, Lyft, Grab).

In this application domain of taxi on demand services, where improving 0.5%-1% is a major achieve-
ment [Lin et al. (2018)], we demonstrate that our approach easily outperforms the existing best ap-
proach, NeurADP [Shah et al. (2020)] by at least 3.8% and up to 9.76% on a wide variety of settings
for the benchmark real world taxi dataset [NYYellowTaxi (2016)].

2 BACKGROUND

In this section, we formally describe the RMP problem and also provide details of an existing ap-
proach for on-demand ride pooling called NeurADP, which we improve over.

Ride-pool Matching Problem (RMP) : We consider a ﬂeet of vehicles/resources R with random
initial locations, travelling on a predeﬁned road network G with intersections : L as nodes, road
segments : E as edges and weights on edges indicate the travel time on the road segment. Passengers
that want to travel from one location to another send requests to a central entity that collects these
requests over a time-window called the decision epoch ∆. The goal of the RMP is to match these
collected requests U t to empty or partially ﬁlled vehicles that can serve them such that an objective
J is maximised subject to constraints on the delay D.

We upperbound D and consider the objective J to be the number of requests served. Thus, RMP is
deﬁned using the tuple [G, U, R, D, ∆, J ]1. Please refer Appendix A.1 for a detailed description.

Delay constraints : D consider two delays, {τ, 2τ }. τ denotes the maximum allowed pick-up delay
which is the difference between the arrival time of a request and the time at which a vehicle picks
the user up. 2τ denotes the maximum allowed detour delay which is the difference between the time
at which the user arrived at their destination in a shared cab and the time at which they would have
arrived if they had taken a single-passenger cab.

Neural Approximate Dynamic Programming (NeurADP) for Solving RMP: Figure 1 provides
the overall approach. In this paper, there are two NeurADP [Shah et al. (2020)] contributions of
relevance:

1Everywhere in the paper [, ] is used as the concatenation operator

2

Figure 1: NeurADP approach [Shah et al. (2020)]

FV: To estimate Future Value of current actions, a method for solving the underlying Approx-
imate Dynamic Program (ADP) [Powell (2007)] by considering neural network represen-
tations of value functions.

DJV: To ensure scalability, Decomposing the Joint Value function into individual vehicle value

functions by extending on the work of Russell et al. [Russell & Zimdars (2003)].

Future Value (FV): ADP is similar to a Markov Decision Problem (MDP) with the key difference
that the transition uncertainty is extrinsic to the system and not dependent on the action. The ADP
problem for RMP is formulated using the tuple (cid:104)S, A, ξ, T, J (cid:105), where :
S : The state of the system is represented as st = (rt, ut) where rt is the state of all vehicles and ut

contains all the requests waiting to be served. The state is obtained in Step A of Figure 1.

A : At each time step there are a large number of requests arriving to the taxi service provider,
however for an individual vehicle only a small number of such requests are reachable. The feasible
set of request combinations for each vehicle i at time t, F i

t is computed in Step B of Figure 1:

t = {f i|f i ∈ ∪ci
F i

c(cid:48)=1[U]c(cid:48)

, PickUpDelay(f i, i) ≤ τ, DetourDelay(f i, i) ≤ 2τ }

(1)

ai,f
is the decision variable that indicates whether vehicle i takes action f (a combination of
t
requests) at a decision epoch t. Joint actions across vehicles have to satisfy matching constraints:
(i) each vehicle, i can only be assigned at most one request combination, f ; (ii) at most one
vehicle, i can be assigned to a request j; and (iii) a vehicle, i can be either assigned or not
assigned to a request combination.

ai,f
t = 1 ::: ∀i ∈ R

(cid:88)

f ∈F i
t

(cid:88)

(cid:88)

i∈R

f ∈F i

t ;j∈f

ai,f
t ≤ 1 ::: ∀j ∈ Ut

ai,f
t ∈ {0, 1} ::: ∀i, f

(2)

ξ : denotes the exogenous information – the source of randomness in the system. This would

correspond to the user requests or demand. ξt denotes the exogenous information at time t.

T : denotes the transitions of system state.

In an ADP, the system evolution happens as
t , · · · ), where st denotes the pre-decision state at decision
t denotes the post-decision state [Powell (2007)]. The transition from state st to

(s0, a0, sa
epoch t and sa
st+1 depends on the action vector at and the exogenous information ξt+1. Therefore,

1, · · · , st, at, sa

0, ξ1, s1, a1, sa

st+1 = T (st, at, ξt+1); sa

t = T a(st, at); st+1 = T ξ(sa

t , ξt+1)

It should be noted that T a(., .) is deterministic as uncertainty is extrinsic to the system.

J : denotes the reward function and in RMP, this will be the revenue from a trip.

3

Let V (st) denotes the value of being in state st at decision epoch t, then using Bellman equation:

V (st) = max
at∈At

(J (st, at) + γE[V (st+1)|st, at, ξt+1])

where γ is the discount factor. Using post-decision state, this expression breaks down nicely:

V (st) = max
at∈At

(J (st, at) + γV a(sa

t ));

V a(sa

t ) = E[V (st+1)|sa

t , ξt+1]

(3)

(4)

The advantage of this two step value estimation is that the maximization problem in Equation 4
can be solved using a Integer Linear Program (ILP) with matching constraints indicated in ex-
pression 2. Step D of Figure 1) provides this aspect of the overall algorithm. The value function
approximation around post-decision state, V a(sa
t ) is a neural network and is updated (Step E of
Figure 1) by stepping forward through time using sample realizations of exogenous information
(i.e. demand observed in data). However, as we describe next, maintaining a joint value function
is not scalable and hence we decompose and maintain individual value functions.
Decomposing Joint Value (DJV): Non-linear value functions, unlike their linear counterparts,
cannot be directly integrated into the ILP mentioned above. One way to incorporate them is
to evaluate the value function for all possible post-decision states and then add these values
as constants. However, the number of post-decision states is exponential in the number of re-
sources/vehicles.
[Shah et al. (2020)] introduced a two-step decomposition of the joint value function that converts
it into a linear combination over individual value functions associated with each vehicle. In the
ﬁrst step, following [Russell & Zimdars (2003)], the joint value function is written as the sum
over individual value functions : V (sa
i V i(sa
In the second step, the individual vehicles’ value functions are approximated. They assumed
that the long-term expected reward of a given vehicle is not signiﬁcantly affected by the speciﬁc
actions another vehicle makes in the current decision epoch and thereby completely neglect the
impact of the actions taken by other vehicles at the current time step. Thus they model the value
function using the pre-decision, rather than post-decision, state of other vehicles which gives :

t ) = (cid:80)

t ).

V i(sa

t ) = V i([si,a

t

, s-i,a
t

]) ≈ V i([si,a

t

, s-i

t ])

t ) = (cid:80)

where -i refers to all vehicles except vehicle i. This allows NeurADP to get around the combi-
natorial explosion of the post-decision state of all vehicles. NeurADP thus has the joint value
function : V (sa
They then evaluate these individual V i values (Step C of Figure 1) for all possible si,a
(from
the individual value neural network) and then integrate the overall value function into the ILP
as a linear function over these individual values. This reduces the number of evaluations of the
non-linear value function from exponential to linear in the number of vehicles.

i V i([si,a

t ]).

, s-i

t

t

3 CONDITIONAL EXPECTATION BASED VALUE DECOMPOSITION, CEVD

One of the fundamental drawbacks in NeurADP is that each agent/vehicle2 to a large extent is kept
in oblivion about the values of the feasible actions for other agents/vehicles. Since our problem
execution (assignment of requests to agents) is centralized, this independence of individual agents
(as shown in experimental results) leads to sub-optimal actions for the entire system.

While there are dependencies between agents, not all agents are dependent on each other and one
mechanism typically employed to represent sparsely connected multi-agent systems is through the
use of a coordination graph [ Guestrin et al. (2002); Li et al. (2021)], CG = (X, E). The joint value
of the system with joint state s and joint action a in the context of a coordination graph is given by:

QCG(s, a) =

(cid:88)

i∈X

QCG
i

(s, a);

QCG
i

(s, a) = f i(ai|s) +

(cid:88)

f ij(ai, aj|s)

(5)

j|(i,j)∈E

where f i(.|.) represents the value of agent i and f ij(., .|.) represents the impact of agent j’s actions
on agent i’s value. Such an approach is scalable if there are a few agents. However, when considering
thousands of agents and a central ILP which requires values for all different joint action pairs,

2We will use agent and vehicle interchangeably.

4

Figure 2: Schematic outlining CEVD’s neighbour aware scoring mechanism

there is a combinatorial explosion making the model non deployable in real time. To put things
into perspective, assuming each agent has |A| feasible actions (request combinations) and there
are N (typically 1000) agents, the number of value evaluations jumps from N · |A| in DJV to
N · |A|2 while using a coordination graph. It should be further noted the A corresponds to request
combinations and hence can increase combinatorially.

Thus, we need a mechanism that is scalable while considering the impact of i on other agents. In the
well known Expectation Maximization algorithm [Dempster (1977)] for identifying missing data,
the likelihood is calculated by introducing a conditional probability of unknown data given known
data. In a similar vein, our method to deal with the unknown impact of other agents is by considering
conditional probability of agent j taking action aj given agent i takes action ai in state s. This will
ensure the overall value is dependent on individual agent values and not on joint values. More
speciﬁcally, the expected value of agent i is:

QCG
i

(s, a) = f i(ai|s) +

(cid:88)

P (aj|ai, s)f j(aj|s)

j|(i,j)∈E,aj ∈Aj

To make this broad idea of conditional expectation operational in case of RMP, we have to address
multiple key challenges. We describe these key challenges and our ways of addressing them below.
Figure 2 provides the overall method, with step (II) outlining the conditional expectation idea and
the key difference from NeurADP described in Figure 1.

3.1 NO EXPLICIT/STATIC COORDINATION GRAPH

While it is clear that agents that are very far apart will not have any dependency, there is no explicit
coordination graph that is present in RMP. However, RMP has two characteristics that make it easier
to identify neighbouring agents for any given agent:
• Agents that are nearby spatially are more probable to compete over the same set of requests and

hence would have a dependency.

• Agents/vehicles do not have identity, i.e., they are all homogenous.
Due to these characteristics, we can cluster the intersections in the road network (to capture spatial
dependencies) and consider agents at a time step in an intersection cluster as neighboring agents.
Due to homogeneity of agents, the only aspect of importance is whether there are agents (and not
which speciﬁc agents) competing for the same requests. Unlike previous works, the coordination
graph keeps changing at each time step, as agents move between clusters. At time t, an agent placed
at an intersection belonging to cluster Ck will coordinate with all other agents in cluster Ck.

5

[Expected Value of  conditioned on ][Expected Value of  conditioned on ][Neighbour aware value for action  for ][Neighbour aware value for action  for ]Integer LinearProgram (ILP)Optimal  Actions  Chosen(I) Get Requests, Compute FeasibleActions, Find Individual Values (II) Score Values considering consequences on other vehicles(III)Solve ILP using Neighbour aware scores, assign optimal actions, simulate motionWe deﬁne function M : L −→ [K] to map each intersection of the road network into one of the
K clusters : {C1, C2, . . . , CK} based on the average travel times between intersections. Because of
clustering locations and assigning agents to location clusters, the total number of agents becomes
less of an issue with respect to scalability.

3.2 HOW TO CONSIDER IMPACT OF OTHER AGENTS IN THE CLUSTER?

Let us consider agent i present in cluster Ck at time t. The other agents in cluster Ck are agents
j1, j2, . . . , jn and are termed its neigbours. In NeurADP, agent i credited action f ∈ F i
t with value
V i(si,f
t ) which is oblivious to the presence of neighbour agents. Since the execution is centralized,
agent i can however weigh the losses/gains its action f has on the cluster by getting useful feed-
back from the individual values of other agents. Let us take a neighbour agent j ∈ Ck (j (cid:54)= i)
having feasible actions g1, g2, . . . , gkj ∈ F j
t . From agent i’s perspective a conditional probability
distribution

P (Agent j takes action g|Agent i takes action f) = P j(g|si

t, f )
t, f )V j(sj,g

t ).

P j(g|si

is formed and the feedback term from agent j is written as (cid:80)

g∈F t
j

We do this for all the neighbours and take an average :

1
|Ck| − 1

(cid:88)

(cid:88)

P j(g|si

t, f )V j(sj,g
t )

j∈Ck,j(cid:54)=i

g∈F t
j

Now to calculate the value of agent i on taking action f , after getting this feedback, we take an
afﬁne combination and write the new individual value as

ˆV i(si,f

t ) =

(cid:20)

1
1 + λ

V (si,f

t ) + λ

1
|Ck| − 1

(cid:88)

(cid:88)

P j(g|si

t, f )V j(sj,g
t )

(cid:21)

j∈Ck,j(cid:54)=i

g∈F t
j

where λ is a learnable parameter. It should be noted that this individual value not only considers
the future impact of current action, f , but also considers the impact on other agents.

Our overall ILP objective thus becomes :

(cid:88)

(cid:88)

(cid:18)

max

J i(si

t, f ) + γ

(cid:20)

1
λ + 1

V i(si,f

t ) + λ

(cid:88)

j∈Ck,j(cid:54)=i

1
|Ck| − 1

(cid:88)

g∈F t
j

P j(g|si

t, f )V j(sj,g
t )

(cid:21)(cid:19)

× ai,f
t

i

f ∈F t
i

(cid:88)

(cid:88)

= max

i

f ∈F t
i

(cid:18)

J i(si

t, f ) + γ ˆV i(si,f
t )

(cid:19)

× ai,f
t

subject to feasibility constraints in expression 2.

What should be the functional form for conditional probabilities? Each request ui
pickup location oi
Every action f is associated with some user request ui

t ∈ Ut has a
t ∈ L. Recall function M : L −→ [K] which maps each intersection to its cluster.
t, we deﬁne

At : Ft −→ L

which maps each action f to the corresponding request (ui
composition function

t)’s pickup intersection oi

t. Deﬁne the

Ct = M ◦ Ut : Ft −→ [K]

that maps each action to a cluster by the pickup location of the action’s corresponding user request.
d : [K] × [K] −→ R+ is deﬁned as the average travel time between 2 clusters. We model the
conditional probability of agent j taking action g given agent i takes action f as :

P j(g|si

t, f ) ∝ e(α·d(Ct(g),Ct(f )))

where α is a learnable parameter. The normalizing constant is computed by summing over actions
in F j
t .

6

Figure 3: Comparison of estimated value and discounted ”real” value for NeurADP, NeurADP+ and
CEVD.

3.3 OVER/UNDER ESTIMATION OF INDIVIDUAL VALUES IN FV AND DJV:

NeurADP makes an optimistic assumption that individual agent will get to take the best action in
the next time step. Since central ILP decides the joint action, this can result in an overestimation
or underestimation of individual agent value. Due to this and other issues, in our experiments, we
found that NeurADP values can have signiﬁcant errors compared to the discounted future rewards
as shown in Figure 3. We ﬁx this problem through two key enhancements:
• Controlling large Variance of exogeneous information : Recall from Equation 4, to calculate

values of post action states, NeurADP employs

V a(sa

t ) = E[V (st+1)|sa
Here the exogenous information ξt+1 is the global demand (gt) at time t. Even within a small
number of consecutive epochs, the global demand displays a signiﬁcant variance. This results
in individual values showing a large variance, instead of varying smoothly over time. We thus
consider expected discounted future demand,

t , ξt+1]

Ft+1 = E[

T
(cid:88)

t(cid:48) =0

(cid:48)

γt

gt+t(cid:48) ]

(where T is large but ﬁnite horizon) which varies smoothly over time unlike the current demand.
In our approach, we consider exogenous information as ξt+1 = [gt, Ft].

• Enforcing values to be positive : NeurADP models the value function as a shared parameter Neu-
ral Network. The ﬁnal layer of this Neural Network is a fully connected multi layer perceptron
(MLP) having range as the entire real line. However, as our objective function (number of re-
quests served) is non-negative, negative values (admissible by NeurADP) are not reasonable. We
thus use a SoftPlus activation after the ﬁnal MLP to ensure that the computed values are strictly
positive.
To evaluate the impact of the above modiﬁcations (calling the model NeurADP+), in Figure
3 we plot the following : (cid:80)
t is the action chosen for vehicle i at time t) vs
(cid:80)T
Rt+t(cid:48) where Rt is the total number of requests served at time t for NeurADP, Neu-
rADP+ and CEVD. Notice how the gap between the Estimated Value curve and the Discounted
Reward curve is very small in NeurADP+ and CEVD (as it should be by the Bellman Equation 3),
whereas the gap is quite signiﬁcant for NeurADP. Note that the height of the graphs is different
and while NeurADP+ improves the quality of the estimation, CEVD is responsible for the bulk of
the performance gain.

i V i(sf i
t ,i

t(cid:48) =0 γt

) (f i

t

(cid:48)

4 ALGORITHM

Given a post decision state si,f
t ). Our
t
joint function ˆVθ,λ,α has 3 parameters :
(i) θ : parameters of a Neural Network Based Indi-
vidual Agent Value Function Estimator (as in NeurADP)3, (ii) λ : parameter to control the im-

, we need a paremterized function to compute ˆV (si,f

3In this section by NeurADP we mean NeurADP updated with the proposals in 3.3

7

portance given to an agent’s neighbours while taking an afﬁne combination, (iii) α : parame-
ter to control conditional probabilities P which controls the relative importance given to differ-
ent feasible actions of neighbours. We infer the parameters step by step. Setting λ = 0, re-
duces this function to NeurADP ( ˆVθ,0,0). We ﬁrst estimate optimal θ∗ (following the alogrithm
in NeurADP). This gives us the NeurADP parameters, which are a good starting point to esti-
mate values at an individual level. Now to estimate λ, we set α = 0 (this corresponds to uni-
form distribution over actions), and do a linear search on a set of sampled points on the real

line to ﬁnd the optimal λ∗ = maxλ

(cid:80)t=T
t=0

(cid:80)|R|
i=0

J i(si

t, f ) + γ ˆV (si,f
t )

× ai,f
t

subject to con-

(cid:18)

(cid:19)

straints in expression 2. At this stage, we haven’t changed the preference over actions from an
individual perspective, however for the central executor the values are now much more reﬁned
as the individual over/under estimates have been smoothened by considering the neigbours. Fi-
nally we estimate α by linear search on a set of sampled points on the real line to get optimal
(cid:19)

(cid:18)

subject to constraints in expression 2. We

(cid:80)t=T
t=0

× ai,f
α∗ = maxα
t
can now compute values on unseen data using ˆVθ∗,λ∗,α∗ .

t, f ) + γ ˆV (si,f
t )

J i(si

(cid:80)|R|
i=0

5 EXPERIMENTS

The goal of the experiments is to compare the performance of our approach CEVD to Neu-
rADP[Shah et al. (2020)](henceforth referred to as baseline), which is the current best approach
for solving the RMP. We make this comparison on a real-world dataset [NYYellowTaxi (2016)]
across different RMP parameter settings. We quantitatively justify our performance by comparing
the service rate, i.e., the percentage improvement on the total requests served. We vary the following
parameters: the maximum allowed waiting time τ from 90 seconds to 150 seconds, the number of
vehicles |R| from 500 to 1000 and the capacity C from 4 to 5. The value of maximum allowable
detour delay λ is taken as 2 ∗ τ . The decision epoch duration ∆ is taken as 60 seconds.

Setup: We perform our experiments on the demand distribution from the publicly available New
York Yellow Taxi Dataset [NYYellowTaxi (2016)]. The experimental setup is similar to the setup
used by [Shah et al. (2020)]. Street intersections are used as the set of locations L. They are
identiﬁed by taking the street network of the city from openstreetmap using osmnx with ’drive’
network type [Boeing (2017)]. Nodes that do not have outgoing edges are removed, i.e., we take
the largest strongly connected component of the network. The resulting network has 4373 locations
(street intersections) and 9540 edges. The travel time on each road segment of the street network
is taken as the daily mean travel time estimate computed using the method proposed in [Santi et al.
(2014)]. We further cluster these intersections L into K clusters using K-Means Clustering based
on the average travel times between different intersections. We choose the value of K based on the
number of vehicles. K is chosen to be 100,150 and 200 for 500,750 and 1000 vehicles respectively.
Similar to previous work, we only consider the street network of Manhattan as a majority (∼75%)
of requests have both pickup and drop-off locations within it. The dataset contains data about past
customer requests for taxis at different times of the day and different days of the week. From
this dataset, we take the following ﬁelds: (1) Pickup and drop-off locations (latitude and longitude
coordinates) - These locations are mapped to the nearest street intersection. (2) Pickup time - This
time is converted to appropriate decision epoch based on the value of ∆. The dataset contains on an
average 322714 requests in a day (on weekdays) and 19820 requests during peak hour.

We evaluate the approaches over 24 hours on different days starting at midnight and take the average
value over 5 weekdays (4 - 8 April 2016) by running them with a single instance of initial random
location of taxis 4. CEVD is trained using the data for 8 weekdays (23 March - 1 April 2016) and
it is validated on 22 March 2016. For the experimental analysis, we consider that all vehicles have
identical capacities.

Results : We compare CEVD to NeurADP (referred to as baseline). Table 1 gives a detailed
performance analysis for the service rates of CEVD and baseline. Here are some key observations:
Effect of changing tolerance to delay, τ : CEVD obtains a 9.37% improvement over the baseline

4All experiments are run on 60 core - 3.8GHz Intel Xeon C2 processor and 240GB RAM. The algorithms

are implemented in python and optimisation models are solved using CPLEX 20.1

8

Varying

Pickup

Delay
Number
of
Vehicles

Capacity

Number of
Vehicles
500
500
500
500
750
1000
500
500

Parameters
Pickup
Delay
90
120
150
90
90
90
90
90

Capacity

4
4
4
4
4
4
4
5

Baseline
Requests
Served
90286.8±2108.78
103933.0±2604.05
113051.8±2771.45
90286.8±2108.78
129791.6±3516.83
165110.2±4677.10
90286.8±2108.78
91509.4±2144.54

Our Approach

Requests
Served
98748.2±2449.38
113184.0±2774.00
117351.6±2818.49
98748.2±2449.38
139365.6±3853.98
175453.4±5173.82
98748.2±2449.38
100443.8±2502.34

Percentage
Improvement
9.37±0.59
8.90±0.23
3.80±0.24
9.37±0.59
7.38±0.38
6.26±0.18
9.37±0.59
9.76±0.38

Table 1: Detailed Quantitative Results

Figure 4: This graph compares the number of requests served as a function of time. The bold lines
represent a moving average for different conﬁgurations averaged over requests from 4-8 April 2016.

approach for τ = 90 seconds. The difference between the baseline and CEVD decreases as τ
increases. The lower value of τ makes it difﬁcult for vehicles to accept new requests while satisfying
the constraints for already accepted requests. The neighbouring vehicles’ interactions in CEVD
prevents a vehicle from picking up requests it values highly however which would have been more
suitable given the delay constraints for some other vehicle and instead picks up requests which it
might value less but still is feasible. Thus the overall requests served increases.
Effect of changing the capacity, C : CEVD obtains a 9.76% gain over baseline for capacity 5. The
difference between the baseline and CEVD increases as the capacity increases as for higher capacity
vehicles, there is a larger scope for improvement if vehicles cooperate well.
Effect of changing the number of vehicles, |R|: CEVD obtains a 9.37% improvement over the
baseline for capacity |R| = 500. The difference between the baseline and CEVD decreases as the
number of vehicles increase as in the presence of a large number of vehicles, there will always be a
vehicle that can serve the request. As a result, the quality of assignments plays a smaller role.

We further analyse the improvements obtained by CEVD over baseline by comparing the number of
requests served by both approaches at each decision epoch throughout the day. Figure 4 shows the
number of requests served by the baseline and CEVD at different decision epochs5. As shown in the
ﬁgure, initially at night time when the demand is low both approaches serve all available demand.
During the transition period from low demand to high demand period, the baseline algorithm starts
to choose suboptimal actions without considering the impact of each vehicle’s action on the whole
system while CEVD is able to capitalize on the joint action values and serve much more requests
than the baseline.

The approach can be executed in real-time settings. The average time taken to compute each batch
assignment using CEVD is less than 60 seconds (for all cases) 6. These results indicate that using
our approach can help ride-pooling platforms to better meet customer demand.

5Results for other settings shown in appendix
660 seconds is the decision epoch duration considered in the experiments

9

6 CONCLUSION

Due to the matching required on a tri-partite graph between user requests, trips (combination of
user requests) and vehicles, on-demand ride pooling is challenging. Improving on existing methods,
we provide a scalable novel value decomposition method based on conditional probabilities, where
individual value is not only able to consider future impact of current matches but also impact on other
agent values. This new approach is able to outperform the best existing method in all settings of the
benchmark taxi data set employed for on-demand ride pooling by margins of up to 9.79%. To put
this result in perspective, typically, an improvement of 1% is considered a signiﬁcant improvement
on ToD for an entire city [Xu et al. (2018); Lowalekar et al. (2019)].

10

REFERENCES

Javier Alonso-Mora, Samitha Samaranayake, Alex Wallar, Emilio Frazzoli, and Daniela Rus. On-
demand high-capacity ride-sharing via dynamic trip-vehicle assignment. Proceedings of the Na-
tional Academy of Sciences, pp. 201611675, 2017.

Xiaohui Bei and Shengyu Zhang. Algorithms for trip-vehicle assignment in ride-sharing. 2018.

Geoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing com-

plex street networks. Computers, Environment and Urban Systems, 65:126–139, 2017.

et al. Dempster, A. P. Maximum likelihood from incomplete data via the em algorithm. Journal of

the Royal Statistical Society. Series B (Methodological), 1977.

C. Guestrin, D. Koller, and R. Parr. Multiagent planning with factored mdps. In Neural Information

Processing Systems, 2002.

Yan Huang, Favyen Bastani, Ruoming Jin, and Xiaoyang Sean Wang. Large scale real-time rideshar-
ing with service guarantee on road networks. Proceedings of the VLDB Endowment, 7(14):2017–
2028, 2014.

S. Li, J.K. Gupta, P. Morales, R. Allen, and M.J. Kochenderfer. Deep implicit coordination graphs
for multi-agent reinforcement learning. In International Conference on Autonomous Agents and
Multi-Agent Systems, AAMAS, 2021.

Kaixiang Lin, Renyu Zhao, Zhe Xu, and Jiayu Zhou. Efﬁcient large-scale ﬂeet management via
multi-agent deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery &#38; Data Mining, pp. 1774–1783. ACM, 2018.

Meghna Lowalekar, Pradeep Varakantham, and Patrick Jaillet. ZAC: A zone path construction
approach for effective real-time ridesharing. In Proceedings of the Twenty-Ninth International
Conference on Automated Planning and Scheduling, ICAPS 2018, Berkeley, CA, USA, July 11-
15, 2019., pp. 528–538, 2019.

Meghna Lowalekar, Pradeep Varakantham, and Patrick Jaillet. Zone path construction (zac) based
approaches for effective real-time ridesharing. Journal of Artiﬁcial Intelligence Research, JAIR,
2021.

Shuo Ma, Yu Zheng, and Ouri Wolfson. T-share: A large-scale dynamic taxi ridesharing service.
In Data Engineering (ICDE), 2013 IEEE 29th International Conference on, pp. 410–421. IEEE,
2013.

NYYellowTaxi. New york yellow taxi dataset. http://www.nyc.gov/html/tlc/html/

about/trip_record_data.shtml, 2016.

Sophie N Parragh, Karl F Doerner, and Richard F Hartl. A survey on pickup and delivery problems.

Journal f¨ur Betriebswirtschaft, 58(1):21–51, 2008.

Warren B Powell. Approximate Dynamic Programming: Solving the curses of dimensionality, vol-

ume 703. John Wiley & Sons, 2007.

T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, , and S Whiteson. Qmix: Mono-
tonic value function factorisation for deep multi-agent reinforcement learning. In International
Conference on Machine Learning, ICML, 2018.

Ulrike Ritzinger, Jakob Puchinger, and Richard F Hartl. A survey on dynamic and stochastic vehicle

routing problems. International Journal of Production Research, 54(1):215–231, 2016.

Stefan Ropke and Jean-Franc¸ois Cordeau. Branch and cut and price for the pickup and delivery

problem with time windows. Transportation Science, 43(3):267–286, 2009.

Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents.

In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 656–663,
2003.

11

Paolo Santi, Giovanni Resta, Michael Szell, Stanislav Sobolevsky, Steven H Strogatz, and Carlo
Ratti. Quantifying the beneﬁts of vehicle pooling with shareability networks. Proceedings of the
National Academy of Sciences, 111(37):13290–13294, 2014.

Sanket Shah, Meghna Lowalekar, and Pradeep Varakantham. Neural approximate dynamic pro-
In Proceedings of the AAAI Conference on Artiﬁcial

gramming for on-demand ride-pooling.
Intelligence, volume 34, pp. 507–515, 2020.

P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg, M. Lanctot,
N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value- decomposition networks for cooperative
multi-agent learning based on team reward. In International Conference on Autonomous Agents
and Multi-Agent Systems, AAMAS, 2018.

Yongxin Tong, Yuxiang Zeng, Zimu Zhou, Lei Chen, Jieping Ye, and Ke Xu. A uniﬁed approach
to route planning for shared mobility. Proceedings of the VLDB Endowment, 11(11):1633–1646,
2018.

Zhe Xu, Zhixin Li, Qingwen Guan, Dingshui Zhang, Qiang Li, Junxiao Nan, Chunyang Liu, Wei
Bian, and Jieping Ye. Large-scale order dispatch in on-demand ride-hailing platforms: A learning
and planning approach. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 905–913. ACM, 2018.

12

A APPENDIX

A.1 RIDE-POOL MATCHING PROBLEM, RMP

Here, we provide speciﬁc details of the RMP problem.
G: Following Alonso-Mora et al. (2017), the road network is represented by a weighted graph (L, E)
where L denotes the set of street intersections and E deﬁnes the adjacency of these intersections
which captures the travel time for a road segment. We assume that vehicles only pick up and drop
people off at intersections.

U: = ×t Ut , is the combination of requests that we observe at each decision epoch t. Each re-
(cid:68)
t , ej
oj
t ∈ L denote the origin and

t ∈ Ut is represented by the tuple:

quest uj
destination and t denotes the arrival epoch of the request.

(cid:69)
, where oj

t , ej

t , t

R: The set of resources/vehicles where each element i ∈ R is represented by the tuple

ci, pi,(cid:126)li(cid:69)
(cid:68)
.
ci denotes the capacity of the vehicle, i.e., the maximum number of passengers it can carry simul-
taneously, pi its current position and (cid:126)li the ordered list of locations that the vehicle should visit
next to satisfy the requests currently assigned to it.

D: {τ, λ} denotes the set of constraints on delay. τ denotes the maximum allowed pick-up delay
which is the difference between the arrival time of a request and the time at which a vehicle picks
the user up. λ denotes the maximum allowed detour delay which is the difference between the
time at which the user arrived at their destination in a shared cab and the time at which they would
have arrived if they had taken a single-passenger cab.

∆: denotes the decision epoch duration.
J : represents the objective, with J i

t denoting the value obtained by serving request i at decision
epoch t. The goal of the online assignment problem is to maximize the overall objective over a
given time horizon, T .

A.2 RELATED WORK

There are three main threads of existing work in solving RMP problems:
(i) The ﬁrst set of approaches are traditional planning approaches that model RMP as an optimiza-
tion problem Ropke & Cordeau (2009); Ritzinger et al. (2016); Parragh et al. (2008). The problem
with this class of approaches is that they don’t scale to on-demand city-scale scenarios.
(ii) The second set of approaches are focused on making the best greedy assignments Ma et al.
(2013); Tong et al. (2018); Huang et al. (2014); Lowalekar et al. (2019); Alonso-Mora et al. (2017).
While these scale well, they are myopic and, as a result, do not consider the impact of a given as-
signment on future assignments.
(iii) The third thread of methods Shah et al. (2020); Lowalekar et al. (2021) are focussed on use of
Reinforcement Learning (RL) or online Multi-Stage Stochastic Optimization to address the myopia
associated with approaches from the second category. These set of approaches consider future im-
pact of current matches through the use of individual value function, they achieve this by ignoring
the impact of other agents on the value (e.g., future revenue) of a vehicle.

With respect to our technical contributions on value decomposition, we improve the work of Guestrin
et al. [Guestrin et al. (2002)] and Li et al. [Li et al. (2021)] that was previously applicable to tens
of agents with coordination graphs, to scale to hundreds/thousands of homogeneous agents with no
explicit coordination graphs. The key difference is with regards to the use of conditional probability
based dependencies amongst neighboring agents.

Another technical contribution that is of relevance is the value decomposition approaches [Rashid
et al. (2018); Sunehag et al. (2018)] developed for solving cooperative Multi-Agent Reinforcement
Learning (MARL). These approaches have been developed to solve problems with tens of agents and
under centralized training and decentralized execution set up, we focus on problems with hundreds
or thousands of agents with centralized training and centralized execution (e.g., Uber, Lyft, Grab).

A.3 PSEUDOCODE

13

Algorithm 1: CEVD Execution
1: Input : Neural Network Value Function V , parameters λ (controlling relative importance of neighbours)
and α (controlling conditional probabilities), Number of Clusters : K, Road Network : G, Decision Epoch
: ∆, Delay Parameter : τ , Capacity : C, Objective Function : J

2: Compute K clusters on road network G based on average travel times between intersections.
3: Precompute conditional probabilities (upto a proportionality constant) and store in a K × K matrix
4: Initialize the state s0 by randomly positioning vehicles.
5: for each step 0 ≤ t ≤ T do
6:
7:

Fetch all user requests Ut in decision epoch ∆
Compute the feasible action set Ft based on current state : st and the user requests.

F i

t = {f i|f i ∈ ∪C

c(cid:48)=1[U]c(cid:48)

, PickUpDelay(f i, i) ≤ τ, DetourDelay(f i, i) ≤ 2 ∗ τ }

8:
9:
10:

Compute individual values using the Neural Network Value Function : V i(sf
Assign each agent its cluster based on current location.
Compute CEVD Values ∀f ∈ F i
t , ∀i ∈ R, ∀k ∈ [K] :

t ); ∀f ∈ F i

t , ∀i ∈ R

ˆV i(si,f

t ) =

(cid:20)

1
1 + λ

V (si,f

t ) + λ

1
|Ck| − 1

(cid:88)

(cid:88)

P j(g|si

t, f )V j(sj,g
t )

(cid:21)

j∈Ck,j(cid:54)=i

g∈F t
j

11:

Solve the MIP :

12:

Subject to constraints :

(cid:88)

(cid:88)

max

i

f ∈F t
i

(cid:18)

J i(si

t, f ) + γ ˆV i(si,f
t )

(cid:19)

× ai,f
t

ai,f
t = 1 ::: ∀i ∈ R

(cid:88)

f ∈F i
t

(cid:88)

(cid:88)

i∈R

f ∈F i

t ;j∈f

ai,f
t ≤ 1 ::: ∀j ∈ U

ai,f
t ∈ {0, 1} ::: ∀i, f

13:
14:

Assign actions based on the solution of MIP.
Simulate agents to pick up requests.

A.4 NEURADP ALGORITHM

Algorithm 2: NeurADP (N, T )
1: Initialize: replay memory M , Neural value function V

(with random weights θ)

0 by randomly positioning vehicles.

2: for each episode 1 ≤ n < N do
Initialize the state sn
3:
Choose a sample path ξn
4:
for each step 0 ≤ t ≤ T do
5:
6:
7:

Compute the feasible action set Ft based on sn
t .
Solve the ILP to get best action an
t .
(Add the Gaussian noise for exploration.)
Store (rn
if t % updateFrequency == 0 then

t , Ft) as an experience in M .

Sample a random mini-batch of experiences
from M
for each experience e do

8:
9:
10:

11:
12:

13:
14:

Solve the ILP with the information
from experience e to get the objective value ye
for each vehicle i do

Perform a gradient descent step on (ye,i − V (ri,n
the network parameters θ
t ), sn

t+1 = T ξ(sa,n

t = T a(sn

t , an

t+1)

, ξn

t

t

))2 with respect to

Update: sa,n

15:
16: return θ

14

Figure 5: This graph compares the number of requests served as a function of time. The bold lines
represent a moving average for different conﬁgurations averaged over requests from 4-8 April 2016.

A.5 ADDITIONAL PLOTS

We provide additional graphs of number of requests served in Figure 5 with different settings.

A.6 PARAMETER RANGES FOR CEVD

To estimate λ we uniformly sample points in the range (−1.0, 1.0) and choose the optimal λ∗ as the
value giving the largest rewards over a horizon. Similarly, to estimate α we uniformly sample points
in the range [−10.0, 10.0] and choose the optimal α∗ as the value giving the largest rewards over a
horizon after choosing λ∗.

15

