Symbolic Regression using Mixed-Integer Nonlinear Optimization

Vernon Austel 1 Cristina Cornelio 1 Sanjeeb Dash 1 João Gonçalves 1 Lior Horesh 1 Tyler Josephson 2
Nimrod Megiddo 3

0
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

1
v
3
1
8
6
0
.
6
0
0
2
:
v
i
X
r
a

Abstract

The Symbolic Regression (SR) problem, where
the goal is to ﬁnd a regression function that does
not have a pre-speciﬁed form, but is any function
that can be composed of a list of operators, is a
hard problem in machine learning, both theoreti-
cally and computationally. Genetic programming
based methods, that heuristically search over a
very large space of functions, are the most com-
monly used methods to tackle SR problems. An
alternative mathematical programming approach,
proposed in the last decade, is to express the op-
timal symbolic expression as the solution of a
system of nonlinear equations over continuous
and discrete variables that minimizes a certain ob-
jective, and to solve this system via a global solver
for mixed-integer nonlinear programming prob-
lems. Algorithms based on the latter approach are
often very slow. We propose a hybrid algorithm
that combines mixed-integer nonlinear optimiza-
tion with explicit enumeration and incorporates
constraints from dimensional analysis. We show
that our algorithm is competitive, for some syn-
thetic data sets, with a state-of-the-art SR software
and a recent physics-inspired method called AI
Feynman.

1. Introduction

Regression methods have numerous applications in science
and economics. In traditional regression methods, such as
linear or logistic regression, the relationship between a de-
pendent variable – say y – and a number of independent
variables – say x1, . . . , xn – is assumed to have a ﬁxed,
parametric, functional form, and the parameters are cal-
culated from data to minimize a speciﬁc “loss” function.
For example, in linear regression, the functional form is

1IBM Research AI, 1101 Kitchawan Road, Yorktown Heights,
NY 10598 2Department of Chemistry, University of Minnesota
3IBM Research - Almaden, 650 Harry Rd, San Jose, CA 95120.
Correspondence to: Sanjeeb Dash <sanjeebd@us.ibm.com>.

assumed to be a linear function of the independent variables:
y = a1x1 + · · · + anxn. The unknown parameters ai are
the coefﬁcients that are calculated so as to minimize, say,
i(yi − (cid:80)
the least-square error: (cid:80)
j)2, where yi and
xi
j are the values of y and xj, respectively, in the ith data
point.

j ajxi

In symbolic regression (SR), both the functional form of
of a regression function, and the associated parameters or
coefﬁcients are calculated from data so as to minimize some
loss function. The functional form is assumed to be anything
that can be composed from a given list of basic functions or
operators applied to the independent variables and arbitrary
constants. For example, if the operators are +, − and ×,
then the space of all possible functions is the set of all
polynomials of arbitrary degree.The usefulness of SR was
demonstrated in (Connor & Taylor, 1977; Langley, 1981;
Willis et al., 1997; Davidson et al., 2003; Schmidt & Lipson,
2009; 2010).

Given its generality, symbolic regression is clearly a dif-
ﬁcult problem and there has been a lot of research into
designing computationally effective algorithms. Starting
with (Koza, 1992), SR problems were solved with genetic
programming (GP) (Augusto & Barbosa, 2000; Banzhaf
et al., 1998), and SR is often treated as being synonymous
with GP (Stephens, 2019). In (Korns, 2011), the accuracy
of SR solutions achieved by genetic algorithms was claimed
to be poor. "Bloat" is another common issue in GP based
methods (obtaining functions with high description length).

Some recent research on improving symbolic regression
techniques is aimed at speeding them up (Luo et al., 2017),
ﬁnding accurate constants in the derived symbolic math-
ematical expression (Izzo et al., 2017), and searching for
implicit functional relationships (Schmidt & Lipson, 2010).
These approaches are primarily based on genetic algorithms.
Another approach explicitly populates a large hypothesis
space of functions, on which sparse selection is applied
(Brunton et al., 2016). In (Udrescu & Tegmark, 2019), a
non-GP SR algorithm is developed , potentially well-suited
for physics problems, that combines neural-network ﬁtting
with several techniques inspired by physics, for example,
dimensional analysis, and also complete enumeration of
symbolic expressions of a certain limited size.

 
 
 
 
 
 
Symbolic Regression using MINO

A major challenge in symbolic regression is the difﬁculty of
ﬁnding scientiﬁcally meaningful models out of the large
number of possible models that may ﬁt given data.
In
(Schmidt & Lipson, 2009; 2010), partial derivatives are
matched, in addition to ﬁtting data, to discover meaning-
ful functions. In AI Feynman (Udrescu & Tegmark, 2019),
dimensional analysis is used to both speed up the solution
process, and to generate meaningful physical models.

Some recent papers proposed mathematical optimization
based approaches for obtaining regression models: a model
is obtained as a solution of a constrained optimization prob-
lem that selects the kind of model and optimizes its pa-
rameters. Various Mixed-Integer Programming (MIP) for-
mulations were proposed in (Bertsimas & Shioda, 2007)
for ﬁnding classiﬁcation and regression models. Mixed-
Integer Nonlinear Programming (MINLP) formulations for
SR were introduced in (Cozad, 2014), (Avron et al., 2015)
and (Horesh et al., 2016). Some computational results were
presented in (Cozad, 2014; Austel et al., 2017; Cozad &
Sahinidis, 2018). In these three papers, the BARON (Tawar-
malani & Sahinidis, 2005) solver was used to obtain globally
optimal (see the next section) solutions, though other similar
solvers can be used. An advantage of the MINLP frame-
work is that, in principle, globally optimal symbolic expres-
sions can be obtained along with certiﬁcates of optimality.
However, existing MINLP models for SR, and the solvers
for such problems, have limited scalability. In addition, the
MINLP framework allows one to incorporate domain knowl-
edge that can be encoded via nonlinear constraints: adding
constraints to enforce dimensional consistency is proposed
in (Gunawan et al., 2017).

1.1. Our contribution

In this paper, we develop a symbolic regression algorithm
based on solving MINLPs inspired by and extending the
free-form SR discovery framework (Austel et al., 2017) that
was tested on a few toy problems, such as relations found
by Kepler and Galileo. Our work is different from the work
in the previous paragraph in multiple ways. Firstly, we sac-
riﬁce global optimality to get a faster algorithm (thus our
algorithm can be viewed as a heuristic, though we can, in
principle obtain globally optimal solutions given enough
time). Secondly, we choose a different MINLP formulation
of an expression tree than in previous papers, and also em-
ploy a different search strategy. Finally, we add constraints
to enforce dimensional consistency.

We compare our method with AI Feynman (Udrescu &
Tegmark, 2019) on the synthetic, physics-inspired data set
in the associated paper, and show that our MINLP-based
approach is still competitive for a number of problems, and
obtains solutions to some problems that are not solved by
Eureqa (but solved by AI Feynman). We can also obtain

Figure 1. Expression tree for 1
0), expressed with full
arithmetic notation, and as a more compact tree of L-monomials.

4 mx2(ω2 + ω2

solutions to some problems from less data.

2. Related work

A symbolic regression scheme consists of a space of valid
mathematical expressions composable from a basic list of
operators (we assume these to be unary or binary), and a
mechanism for exploring the space. Each valid mathemat-
ical expression can be represented by an expression tree:
a rooted binary tree where each non-leaf node has an as-
, log, etc.),
sociated binary or unary operator (+, −, ×,
and each leaf nodes has an associated constant or indepen-
dent variable. An example of an expression tree for the
expression

√

1
4

mx2(ω2 + ω2
0)

is presented in Figure 1. The second tree in the ﬁgure is
explained later.

A typical genetic programming (GP) solver searches the
space of expressions using a genetic algorithm The Eureqa
package (Schmidt & Lipson, 2014), based on the article
(Schmidt & Lipson, 2009), is a state-of-the-art GP-based
solver. Another popular solver is gplearn (Stephens,
2019). AI Feynman (Udrescu & Tegmark, 2019) ﬁrst trains
a neural network (NN) on the input data, so that the function
encoded by the NN is an accurate ﬁt for the input data. It
then uses tests if the true function has various properties
such as separability, symmetry etc, by evaluating the NN
at points not originally in the data set. In addition, it uses
dimensional analysis to speed up the search for the best

Symbolic Regression using MINO

symbolic expression. In (Udrescu & Tegmark, 2019), the
authors applied their algorithm to one-hundred physics equa-
tions, and claim to rediscover all of them. They observe that
Eureqa obtained solutions to only 71 of these instances.

following form:

min

s.t.

2.1. Globally optimal symbolic regression

In a series of works (Cozad, 2014; Cozad & Sahinidis, 2018;
Avron et al., 2015; Horesh et al., 2016; Austel et al., 2017),
symbolic regression (SR) methods based on combined dis-
crete and continuous optimization were developed, and the
symbolic regression problem was formulated in various
ways as a Mixed-Integer Nonlinear Programming (MINLP)
problem. The MINLP problem is solved to global optimal-
ity using an off-the-shelf MINLP solver such as BARON
(Tawarmalani & Sahinidis, 2005), COUENNE or SCIP (Ma-
her et al., 2017). These solvers solve problems of the form

min

s.t.

f (x, y)

g(x, y) ≤ b
x ∈ Rm, y ∈ Zm

(1)

(2)

(3)

where x is a vector of m continuous variables, y is a vec-
tor of n discrete variables, f (x, y) is a real-valued func-
tion that can be composed using a ﬁnite list of operators
such as +, −, ×, /, exp(·) etc.
(this list will vary with
each solver), and g(x, y) is a vector-valued function cre-
ated using the same operators. These solvers use various
convex-relaxation schemes to obtain lower bounds on the
objective-function value, and utilize these bounds in branch-
and-bound schemes to obtain globally optimal solutions.

This approach produces a globally-optimal mathematical
expression (along with a certiﬁcate of optimality) while
avoiding an exhaustive search of the solution space. An-
other advantage is that it directly produces correct, within
a tolerance, real-valued constants; most other methods use
specialized algorithms to reﬁne constants (Izzo et al., 2017)
and cannot guarantee global optimality.

In these MINLPs, the set of valid binary expression trees is
speciﬁed by a set of constraints over discrete and continuous
variables. The discrete variables of the formulation are used
to deﬁne the structure of the expression tree including the
assignment of operators to non-leaf nodes and whether a
leaf node is assigned a constant or a speciﬁc independent
variable. The continuous variables are used for the undeter-
mined constants, and also to evaluate the resulting symbolic
expression for speciﬁc numerical values associated with
individual data points. The objective functions vary from
accuracy (measured as sum of squared deviations) to model
complexity in (Austel et al., 2017).

In (Austel et al., 2017), for example, the MINLP has the

C(fθ)
θ ∈ T
vi = fθ(x(i)), ∀i ∈ I
D(fθ(x), y) ≤ ε

complexity

grammar

prediction

error

where fθ represents an expression tree deﬁned by the struc-
tural and continuous decision variables collectively desig-
nated as θ; T is the universe of valid expression trees; C
measures the description complexity; D measures error of
the predicted values v; y is the vector of observations for
inputs {x(i)}i∈I .

3. System Description

We ﬁrst describe the basics of our system (without dimen-
sional analysis), and then later explain how we incorporate
dimensional analysis.

√

We take as input a list of operators (for this discussion,
assume the input operators are +, −, ×, /, and
), an upper
bound d on the tree depth, an upper bound k on the number
of constants, and a domain for the constants [−Ω, Ω]. In
Figure 1, there is a single constant (distinct from 1) with
value 1/4 in the expression tree and in the L-monomial tree.
In the prior work on globally optimal symbolic regression,
the MINLP formulations typically have discrete variables
that (1) determine the placement of the operators in nodes
of the expression tree and (2) the mapping of independent
variables to leaf nodes, and (3) determine whether to map
an independent variable or a constant to a leaf node.

In our formulation, we simply do not have discrete variables
for (1). We explicitly enumerate all possible assignments
of the operators in expression trees up to depth d. More
precisely, if a“partial” expression tree is one where the leaf
nodes are undetermined, but the operator assignments to
non-leaf nodes are determined, then we enumerate all pos-
sible partial expression trees up to a certain depth. Our
assignment of variables/constants to leaf nodes is also differ-
ent from prior work. Let x1, . . . , xn be all the independent
variables. Instead of assuming that each leaf node is either
a constant h or one of x1, . . . , xn as in the prior work, we
assume each leaf node is a one-term multivariable Laurent
polynomial of the form

hxa1

1 xa2

2 · · · xan
n ,

(4)

where a1, . . . , an are (undetermined) integers (for computa-
tional efﬁciency, we limit these integers to lie in the range
[−δ, δ] for some input constant δ), and h represents an (un-
determined) constant in the ﬁnal expression. We refer to an
expression of the type (4) an L-monomial. In other words,
rather than assigning a single variable or constant to a leaf

Symbolic Regression using MINO

node, we potentially assign both variables and constants,
and also multiple variables (with positive or negative pow-
ers to a leaf node). We call the resulting trees generalized
expression trees, and gentrees for convenience.

Each gentree T (with depth say d) corresponds to a symbolic
expression: each non-leaf node with height 1 corresponds
to the expression formed by applying the operator at the
node to the expressions (i.e., L-monomials) in the children
nodes, and non-leaf nodes at greater heights are handled
in the same manner in order of height. The only gentree
with depth 0 corresponds to the L-monomial L1, whereas
L,
the gentrees of depth 1 correspond to the expressions
L1 +L2, L1 ×L2, L1/L2 and L1 −L2, respectively, where
L1 and L2 are L-monomials.

√

3.1. Pruning the list of gentrees

We try to reduce the number of relevant partial expression
trees/gentrees by removing a number of "redundant" trees
using the fact that the set of nonzero (the constant h in
4 is nonzero) L-monomials is closed under multiplication
and division. Notice that both L1 × L2 and L1/L2 are L-
monomials and thus can be represented by a single expres-
sion L1. In other words, if there is a symbolic expression
f of the form f = L1 × L2 that ﬁts our data, then there is
one of the form f = L1. Accordingly our ﬁrst few pruning
rules are: remove a tree T from the list T of all gentrees
with depth up to d if T has the subexpression

[R1]

[R2a]

[R2b]

[R3]

L1 × L2 or L1/L2.
L1 × (L3 ± L4).
(L1 ± L2) ∗ (L3 ± L4).
(L1 ± L2)/L3.

1 + L(cid:48)

1 + L(cid:48)

1 and L(cid:48)

2, for some L-monomials L(cid:48)

The ﬁrst rule was explained above. The second follows by
associativity: L1 ∗ (L3 + L4) = L1 × L3 + L1 × L4 =
L(cid:48)
2. If a T with a
subexpression L1 ∗ (L3 + L4) appears in T , then replacing
2 results in another gentree T (cid:48)
this subexpression by L(cid:48)
of the same depth, which must therefore be in T (cid:48). The same
idea can be applied to the subexpression L1 ∗ (L3 − L4).
We can apply associativity twice to justify rule R2b, as
(L1 + L2) ∗ (L3 + L4) = L(cid:48)
4, for some
L(cid:48)
i (the same argument holds when either + is replied by a
− in R2b). Once again, the second expression has the same
depth as the ﬁrst, and therefore there must be a tree in T
containing it. R3 can be explained similarly.

2 + L(cid:48)

1 + L(cid:48)

3 + L(cid:48)

3.2. MINLP formulation for a gentree

Let the data points be x(i) for i ∈ I, where I is an index
set, and let the features/independent variables be x1, . . . , xn.
Let y stand for the observations/dependent variable, with
y(i) standing for the ith observation (of the dependent vari-

able). Let T be a given gentree with m leaf nodes, and let
p be the vector of all integer variables corresponding to the
powers of the independent variables in the different leaf
nodes. Then p ∈ Zmn. Let h1, . . . hm be the variables cor-
responding to the constants in leaf nodes 1, . . . , m. Finally,
assume we have a 0-1 variable zi that determines whether or
not the ith leaf node has a constant hi that is different from
one. Thus the (vector) variables in our model are p, z and
h. Let fh,p,z,T stand for the symbolic expression deﬁned by
ﬁxing the values of these variables. Then the MINLP we
solve can be framed as

min

s.t.

(cid:80)

i∈I (y(i) − fh,p,z,T (x(i))2
−δ ≤ pi ≤ δ for i = 1, . . . , mn
−Ωzi + (1 − zi) ≤ hi ≤ Ωzi + (1 − zi)

for i = 1, . . . , m
(cid:80)m
i=1 zi ≤ k
z ∈ {0, 1}m, p ∈ Zmn

(5)

(6)

(7)

(8)

(9)

The constraints (9) and (6) force zi to take on 0-1 values,
and pi to take on values in the range {−δ, −δ + 1, . . . , δ}.
The constraint (7) restricts hi to lie in the range [−Ω, Ω]
if zi has value 1, and forces hi to take on value 1, when
zi has value 0. The constraints (8) allow at most k of the
zi variables to have value 1, and therefore at most k of
the hi values to be different from 1. The function fh,p,z,T
is composed of the operators in the non-leaf nodes of T
from the L-monomials in the leaf nodes. The objective
function is the sum of squares of differences between the
symbolic expression values for each input data point and
the corresponding dependent variable value (call it the least-
square error).

We use BARON to solve the MINLPs we generate, and thus
T and fh,p,z,T are limited by the operators that BARON
can handle (+, −, ×, /, exp(), log()). We will illustrate f
for a few examples, rather than specify it formally. Sup-
pose we are trying to derive the formula F = G m1m2
,
where m1, m2 and r are independent variables, and G is an
unknown constant, and we have multiple data points (for
i ∈ I) with values for the independent variables, and for as-
sociated F . If T is a depth 0 gentree, then T is just a single
L-monomial, and fh,p,z,T = hma
i where a, b, c are
undetermined integers in the range [−δ, δ], and h is an un-
determined real number in the range [−Ω, Ω]. The objective
function is then

1mb

2rc

r2

(cid:88)

(Fi − hma

1imb

2irc

i )2 ,

i∈I

where Fi, m1i, m2i and ri are the values of F, m1, m2, r in
the ith data point. If the gentree can be written as L1 + L2,
then the objective function becomes

(cid:88)

(Fi − (h1ma1

1i mb1

2i rc1

i + h2ma2

1i mb2

2i rc2

i ))2;

i∈I

here ai, bi, ci and hi are undetermined, and we solve for
these values.

and the constant of proportionality is G, the gravitational
constant. Therefore, we have

Symbolic Regression using MINO

As depicted in Figure 1, an expression tree of a certain depth
can sometimes be represented by a gentree of smaller depth.
Therefore, by enumerating gentrees of a certain depth, one
obtains a much richer class of functions than can be obtained
by expression trees of the same depth.

3.3. Enumeration and parallel processing

By enumerating the generalized expression trees and solv-
ing them separately, the problem is divided into multiple,
easier-to-solve sub-problems (operator placement in non-
leaf nodes does not have to be determined any more) that
can be solved much more quickly. This "divide and con-
quer" formulation can obtain solutions to problems that were
intractable for the formulation in (Austel et al., 2017).

When available, we exploit parallelism, and run multiple
threads, with one MINLP corresponding to a single gentree
in a single thread. If we obtain a solution that ﬁts the data
within a prescribed tolerance from a gentree with depth d(cid:48),
then we stop all processes/threads containing gentrees with
depth > d(cid:48). We terminate a gentree if the lower bound on the
least-square error exceeds the least-square error found from
other gentrees of the same or lower depth. Thus we execute
a branch-and-bound type search, and implicitly search for
the least depth gentree that ﬁts the data.

If we have more gentrees than available cores, we process
them in a round robin fashion; if t is the number of cores,
we start solving for t gentrees in parallel, and after a ﬁxed
amount of time (10 seconds), we pause the ﬁrst t gentrees,
and start solving t more, till we either ﬁnd a solution or
run out of gentrees in which we case we start from the ﬁrst
gentree. The gentrees are sorted by a measure of complexity
(roughly equal to the number of nodes). Of course, the
gentree enumeration (and thus our overall algorithm) grows
exponentially with gentree depth d(cid:48): the number of gentrees
for d(cid:48) = 2, 3, 4, 5 are, respectively, 7, 60, 4485 and over
100,000. Our gentree enumeration approach is unlikely to
be tractable for d(cid:48) >= 6 and is already hard for d(cid:48) = 4
if we include more operators than listed earlier. But each
L-monomial can represent large expression trees.

F = G

m1m2
r2

.

The units of G are chosen so that units of the right-hand-side
expression equal the units of force (mass × distance / time-
squared). Suppose one is given a data set for this example,
where each data item has the masses of two bodies and the
distance and gravitational force between them. Dimensional
analysis would rule out F = m1m2/r2 or F = m1/m2 +
m2r, for example, as possible solutions.

If constants can have units and every L-monomial can have
a such a constant, then dimensional analysis conveys essen-
tially no information. For example, we can choose constants
h1, h2, h3 with appropriate dimensions so that both the ex-
pressions F = h1m1m2/r2 and F = h2m1/m2 + h3m2r
satisfy all dimensional requirements. In the AI Feynman ex-
periments, the authors do not allow constants to have units,
and also ensure that for all required universal constants that
have units (such as the Gravitational constant), both numeri-
cal values and units are given as input. We allow constants
to have units or not based on an input ﬂag.

We next explain the constraints we add to our formulation to
enforce dimensional consistency. Assume that all constants
have no units, and that k = 2, and δ = 2, and d = 1. For
the gravitation example (without the gravitational constant
G as an input), each L-monomial L has the form

L = hma

1mb

1rc,

(10)

where a, b and c are bounded integer variables with an
input range of [−2, 2], and h is a variable representing a
constant. We add to the system of constraints (6) - (9) linear
constraints that equate the units of the symbolic expression
to those of the dependent variable. For the case our gentree
consists of a single node (as in the ﬁrst tree in Figure 1),
our symbolic expression has the form (10), and we add the
linear constraints







1
0

0

a

 + b



1
0
 + c

0



0
1
 =

0

1
1
−2



 .

3.4. Dimensional analysis

Consider Newton’s law of universal gravitation which says
that the gravitational force between two bodies is propor-
tional to the product of their masses and inversely propor-
tional to the square of the distance between their centers (of
gravity):

Here, each component of a column vector corresponds to
a unit (mass, distance, time, respectively). The column on
the right-hand side represents the units of force, i.e., mass
times distance divided by squared time. The columns on
the left-hand side represent the dimensionalities of mass
and distance, respectively. The third of the above linear
equations does not have a solution.

F ∝

m1m2
r2

Next, assume the symbolic expression is L1 + L2 where
L1 = h1ma1
2 rc2, and the un-
kowns are ai, bi, ci and hi. The units of L1 and L2 have

2 rc1 and L2 = ma2

1 mb2

1 mb1

Symbolic Regression using MINO

to match, and must also equal the units of F . The linear
constraints we add are



1
0
 + b1

0


1
0
 + b2

0


1
0

0

1
0

0



 + c1



 + c2



0
1
 =

0


0
1
 =

0

a1

a2















1
1
−2

1
1
−2

 .

(11)

(12)

√

√

L where L is an L-monomial (we allow

i = 1, . . . , n. Secondly, we add another pruning rule. We
remove any gentree which contains a subexpression of the
L1 + L2,
form
for example). Finally, we drop the operator − from our set.
However, as we allow positive and negative constants, we
can simulate a − operator. But because we only allow at
most one constant, this means that we effectively allow a
single − operator in our generated solutions. All constants
are allowed to have units.

Finally, for the symbolic expression L1 × L2, to compute
the units of this expression we need to add up the units of L1
and of L2. Thus we sum the left hand sides of (11) and (12)
and equate this sum to the righ hand side of (11). We can
similarly deal with dimension matching in the remaining
gentrees of depth 1 via linear constraints. For greater depth
gentrees, we apply the ideas above to depth 1 (non-leaf
nodes), and then to depth 2 nodes and so on.

4. Results

4.1. Feynman Database for Symbolic Regression

In our ﬁrst experiment, we choose a subset of the 100 prob-
lems from the Feynman Database for Symbolic Regres-
sion (FSReD), created in (Udrescu & Tegmark, 2019). We
ﬁrst eliminate the 19 problems which involve operators our
system cannot handle because these operators cannot be
handled by the underlying MINLP solver we use (namely
sin, cos, arcsin, tanh). We give detailed results on 32 out of
the remaining 81 instances with varied properties in Table 1:
some can easily be solved by AI Feynman and Eureqa, and
some are hard for these solvers, and some need many data
points or limited noise.

We set the depth limit d to 3, and the number of constants k
to 1. We terminate when we ﬁnd an expression with objec-
tive value (squared error) less than 10−4 (error tolerance).
The operators we use are described in Section 3. We set Ω
to 100, so all constants are in the range [−100, 100]. We set
δ = 2 for computational efﬁciency. This means that each
power in an L-monomial lies in the range [−2, 2]. Under this
setting, the pruning rules – R1, R2,R3 – remove potentially
non-redundant gentrees, and our algorithm does not explore
all possible symbolic expressions that are representatable as
a gentree of depth up to 3. Though L-monomials are closed
under multiplication, L-monomials with bounded powers
1x2
are not. For example, if there is a solution of the form x3
2,
we may not ﬁnd it.

We constrain the search in three more ways. We bound
the sum of the variable powers in an L-monomial by an
input number τ = 6. Thus, for each term of the type
(4), we add the constraint (cid:80)n
i=1 |ai| ≤ τ , which is rep-
resentable by a linear constraint using n auxiliary variables
i : (cid:80)n
a(cid:48)
i for

i ≥ 0, −ai ≤ a(cid:48)

i and ai ≤ a(cid:48)

i ≤ τ and a(cid:48)

i=1 a(cid:48)

We give our results in Table 1. The ﬁrst column contains
the equation label from FSReD, the second shows the ac-
tual function. The third column gives the running time (in
seconds) of our code on a machine with 30 cores, and with
τ = 6. A ’f’ in this column implies our code did not produce
the correct answer within the time limit of 600 seconds. The
fourth column gives the outcome of our experiment with
noise. The “data used” column contains the number of data
points we use, which is (the ﬁrst) 10. Our method does
not scale well with the number of data points: equation (5)
becomes more complex, and the resulting MINLPs harder.
We can solve for 20-30 points for most instances, but the
running time increasing signiﬁcantly for some problems. In
columns 6-8 we give the time taken by AI Feynman, the
min number of points it needs to get the correct answer, and
the maximum noise level under which it obtains the correct
answer, all taken from the associated paper, as is the Eureqa
solution status in the last column.

We fail on 5 instances, but on the remaining instances, our
results are mixed in comparison to AI Feynman. We are of-
ten slower, but sometimes faster (II.24.17, II.38.3, III.10.19),
even if we scale our times by 30 (to account for the paral-
lelism used by our code). We note that AI Feynman searches
over more operators, and thus a potentially larger search
space. For three instances – I.48.20, II.11.27, III.10.19 (with
the number of data points in bold) – we obtain the correct
answer with one-tenth the number of data points needed by
AI Feynman.

√

To solve I.6.20a, indicated by a ’*’, we ﬁrst spend the time
limit of 600 seconds using the default set of operators, and
replaced by exp(), and then take another
then restart with
100 seconds. This is why we report a running time of 700.
While using exp(), we also decrease the error tolerance to
10−8. (We note that AI Feynman also cycles through differ-
ent sets of operators). Out of the remaining 49 problems,
we are able to solve 36 instances, and fail on 13. Changing
some of our parameters will allow additional problems to
be solved at the cost of making the code slower. For exam-
ple, increasing the number of constants k to 2 allows our
code to solve I.15.3t. Finally, BARON can handle the log()
operator, but we have not experimented with it.

In a second experiment, we set the relative noise level to
10−2 (as described in the AI Feynman paper), and report

Eqn.
label
I.6.20a*
I.9.18
I.10.7

I.12.2
I.12.4
I.13.4
I.13.12

I.15.3t

I.15.10

I.16.6
I.18.4
I.24.6
I.25.13
I.27.6

I.32.5
I.34.10
I.34.14
I.39.11
I.43.43

I.48.20

II.2.42
II.11.3

II.11.20
II.11.27
II.11.28
II.21.32

II.24.17
II.34.11
II.36.38
II.37.1
II.38.3
III.10.19

Eqn.

√

e−θ2/2/

2π
Gm1m2
(x2−x1)2+(y2−y1)2+(z2−z1)2
m0√

1−v2/c2

q1q2
4πεr2
q1
4πεr2
2 m(v2 + u2 + w2)
1
Gm1m2( 1
)
r2
t−ux/c2
√
1−u2/c2

− 1
r1

m0v√

1

1−v2/c2
u+v
1+uv/c2
m1r1+m2r2
m1+m2
4 m(ω2 + ω2
q/C
1
+ n
1
d1
d2
q2a2/(6πεc3)

0)x2

ω0
1−v/c
1+v/c
1−v2/c2 ω0
1
γ−1 pF V
kbv
(γ−1)A
mc2√

1−v2/c2

k(T2 − T1)A/d

qEf
0 −ω2)
dEf

m(ω2
nρp2

3kbT
nα
1−nα/3 (cid:15)Ef
1 + nα
q
4πεr(1−v/c)
(cid:113) ω2

1−nα/3

d2

c2 − π2
g_qB/(2m)
kbT + µmαM
µmB
εc2kbT
µM (1 + χ)B
Y Ax
d
µM

(cid:112)B2

x + B2

y + B2
z

Symbolic Regression using MINO

exp 1

noise
exp

700
f
11

data
used
10
10
10

1
1
6
13

f

2

10
8
3
1
5

f
4
7
3
5

8

13
8

9
36
f
13

2
1
f
1
1
6

yes

yes

yes

yes
yes

yes
yes

yes

yes
yes

10
10
10
10

10

10

10
10
10
10
10

10
10
10
10
10

10

10
10

10
10
10
10

10
10
10
10
10
10

17
12
22
20

AIF AIF min AIF max
noise
data
time
10−2
10
16
10−5
106
5975
10−4
10
14
10−2
10−2
10−4
10−4
10−4
10−4
10−3
10−2
10−4
10−2
10−2

10
10
10
10
102

18
17
22
10
14

10
10
10
10
10

20

13

10

13
13
14
13
16

108

54
25

18
337
1708
21

62
16
77
15
47
410

10
10
10
10
10
102

10
10

10
102
102
10

10
10
10
10
10
102

10−2
10−3
10−3
10−3
10−3
10−5
10−3
10−3

10−3
10−3
10−4
10−3

10−5
10−4
10−2
10−3
10−3
10−3

Eureqa
solved
no
no
no

yes
yes
yes
yes

no

no

no
yes
yes
yes
yes

yes
no
no
yes
yes

no

yes
yes

yes
no
no
yes

no
yes
yes
yes
yes
yes

Table 1. Results on a subset of problems from the Feynman Database for Symbolic Regression

a “yes” if one of the gentrees returns the expected formula
as a solution and has low error. When the “yes” is in bold,
AI Feynman needs a lower noise level. However, as we
generate arbitrary constants, for some problems we ﬁnd
solutions that have lower squared error than the true formula
(on the 10 data points we chose), and one should apply AIC
or BIC to choose from alternate formulae.

A major difference between our code and AI Feynman is
that our code ﬁnds arbitrary constants, whereas AI Feynman
ﬁnds combinations of ﬁxed symbols (such as small integers
and π and other known physical constants). For example, in
I.12.2, we ﬁnd 1/(4π) as 0.07957747. For problem I.13.12,
AI Feynman uses the known value of Newton’s Gravitational
constant and cannot discover it, whereas our code can.

5. Conclusions

A few MINLP based SR methods have been proposed ear-
lier, but these are not very scalable. These methods obtain
an “optimal” solution and a certiﬁcate of optimality. We
proposed an MINLP based approach that enumerates expres-
sion trees with ﬁxed operators, and uses MINLP to solve
for the form of the leaves. Our approach is competitive with
state-of-the-art SR packages on some difﬁcult problems. It
incorporates dimensional analysis (a ﬁrst for an MINLP
based implementation) but can also ﬁnd real-valued con-
stants (as can gplearn). A number of drawbacks – such as
limitations on number of data points, and high variability
with respect to parameters – of our method suggest future
directions of research. Even though our overall method is a

Symbolic Regression using MINO

heuristic, the MINLP solves for individual gentrees return
certiﬁcates of optimality. One can speedup the method by
developing heuristics to ﬁnd good solutions for a gentree
and terminating before proving optimality.

Our code combines a number of useful features: a) it
infers results from limited data, b) discovers complex
constants, and c) deals with some noise as in real data,
and d) is reasonably fast. AI Feynman cannot handle
arbitrary constants or much noise, and Eureqa cannot
handle explicit equations (such as dimensional equations,
though dimensional equations can be handled implicitly by
an application of the the Buckingham-Pi and the implied
data preprocessing).

Acknowledgements Tyler Josephson was primarily sup-
ported by the U.S. Department of Energy (DOE), Of-
ﬁce of Basic Energy Sciences, Division of Chemical Sci-
ences, Geosciences and Biosciences under Award DE-FG02-
17ER16362. Tyler Josephson and Lior Horesh gratefully
acknowledge the support of the Institute for Mathematics
and its Applications (IMA), where a part of this work was
initiated.

References

Augusto, D. A. and Barbosa, H. J. Symbolic regression
via genetic programming. In Proceedings of the Sixth
Brazilian Symposium on Neural Networks, pp. 173–178.
IEEE, 2000.

Austel, V., Dash, S., Gunluk, O., Horesh, L., Liberti, L.,
Nannicini, G., and Schieber, B. Globally optimal sym-
bolic regression. NIPS Symposium on Interpretable Ma-
chine Learning, 2017.

Avron, H., Horesh, L., Liberti, L., and Nahamoo, D.
Globally convergent system and method for automated
model discovery, June 30 2015. U.S. Patent Application
20170004231, application No. 14/755,942 ﬁled June 30,
2015.

Banzhaf, W., Nordin, P., Keller, R. E., and Francone, F. D.
Genetic programming: an introduction, volume 1. Mor-
gan Kaufmann, San Francisco, 1998.

Bertsimas, D. and Shioda, R. Classiﬁcation and regression
via integer optimization. Operations Research, 55:252–
271, 2007.

Brunton, S. L., Proctor, J. L., and Kutz, J. N. Discovering
governing equations from data by sparse identiﬁcation
of nonlinear dynamical systems. Proceedings of the Na-
tional Academy of Sciences, 113(15):3932–3937, 2016.

Connor, J. W. and Taylor, J. B. Scaling laws for plasma

conﬁnement. Nuclear Fusion, 17(5):1047, 1977.

Cozad, A. Data- and theory-driven techniques for surrogate-
based optimization. PhD thesis, Carnegie Mellon, Pitts-
burgh, PA, 2014.

Cozad, A. and Sahinidis, N. V. A global minlp approach
to symbolic regression. Math. Program., Ser. B, 170:
97–119, 2018.

Davidson, J. W., Savic, D. A., and Walters, G. A. Symbolic
and numerical regression: experiments and applications.
Information Sciences, 150(1):95–117, 2003.

Gunawan, O., Horesh, L., Nannicini, G., and Zhou, W.
Symbolic regression embedding dimensionality analysis,
2017. U. S. Patent Application 20190188256A1, applica-
tion no. 15/843,118, ﬁled Dec 15, 2017.

Horesh, L., Liberti, L., and Avron, H. Globally optimal
mixed integer non-linear programming (MINLP) formu-
lation for symbolic regression. Technical Report 219095,
IBM, 2016.

Izzo, D., Biscani, F., and Mereta, A. Differentiable genetic
programming. In et al., J. M. (ed.), EuroGP 2017, LNCS
10196, pp. 35––51, 2017.

Korns, M. F. Accuracy in symbolic regression. In Riolo,
R., Vladislavleva, E., and Moore, J. H. (eds.), Genetic
Programming Theory and Practice IX, pp. 129—151.
Springer, Berlin, 2011.

Koza, J. R. Genetic Programming: On the Programming
of Computers by Means of Natural Selection. MIT Press,
Cambridge, 1992.

Langley, P. Data-driven discovery of physical laws. Cogni-

tive Science, 5(1):31–54, 1981.

Langmuir, I. The adsorption of gases on plane surfaces
of glass, mica and platinum. Journal of the American
Chemical Society, 40(9):1361–1403, 1918. doi: 10.1021/
ja02242a004.

Luo, C., Chen, C., and Jiang, Z. A divide and conquer
method for symbolic regression. arXiv:1705.08061v2,
2017.

Maher, S. J., Fischer, T., Gally, T., Gamrath, G., Gleixner,
A., Gottwald, R. L., Hendel, G., Koch, T., Lübbecke,
M. E., Miltenberger, M., Müller, B., Pfetsch, M. E.,
Puchert, C., Rehfeldt, D., Schenker, S., Schwarz, R.,
Serrano, F., Shinano, Y., Weninger, D., Witt, J. T., and
Witzig, J. The SCIP optimization suite 4.0. Technical
Report 17-12, ZIB, Takustr.7, 14195 Berlin, 2017.

Schmidt, M. and Lipson, H. Distilling free-form natural
laws from experimental data. Science, 324(5923):81–85,
2009.

Symbolic Regression using MINO

Schmidt, M. and Lipson, H. Symbolic regression of implicit
equations. In Genetic Programming Theory and Practice
VII, pp. 73–85. Springer, Berlin, 2010.

Schmidt, M. and Lipson, H. Eureqa (Version 0.98 beta)
[Software]. 2014. Available from www.nutonian.com.

Stephens, T. Genetic programming in python, with a
scikit-learn inspired api: gplearn. version 0.41, 2019.
URL https://gplearn.readthedocs.io/en/
stable/.

Tawarmalani, M. and Sahinidis, N. V. A polyhedral branch-
and-cut approach to global optimization. Mathematical
Programming, 103(2):225–249, 2005.

Udrescu, S.-M. and Tegmark, M. AI Feynman: A physics-
inspired method for symbolic regression. Sci. Adv. 6:
eaay2631, 2020.

Veeramachaneni, K., Arnaldo, I., Derby, O., and O’Reilly,
U.-M. Flexgp: Cloud-based ensemble learning with ge-
netic programming for large regression problems. Journal
Of Grid Computing, 13(3):391–407, 2015.

Willis, M. J., Hiden, H. G., Marenbach, P., McKay, B., and
Montague, G. A. Genetic programming: An introduction
and survey of applications. In Genetic Algorithms in En-
gineering Systems: Innovations and Applications, 1997.
GALESIA 97. Second International Conference On (Conf.
Publ. No. 446), pp. 314–319. IET, 1997.

