FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

2
2
0
2

p
e
S
3
1

]

G
L
.
s
c
[

3
v
5
1
2
2
0
.
5
0
2
2
:
v
i
X
r
a

Davoud Ataee Tarzanagh 1 Mingchen Li 2 Christos Thrampoulidis 3 Samet Oymak 2

Abstract

Standard federated optimization methods success-
fully apply to stochastic problems with single-
level structure. However, many contemporary
ML problems – including adversarial robustness,
hyperparameter tuning, actor-critic – fall under
nested bilevel programming that subsumes min-
imax and compositional optimization.
In this
work, we propose FEDNEST: A federated al-
ternating stochastic gradient method to address
general nested problems. We establish provable
convergence rates for FEDNEST in the presence
of heterogeneous data and introduce variations
for bilevel, minimax, and compositional opti-
mization. FEDNEST introduces multiple inno-
vations including federated hypergradient com-
putation and variance reduction to address inner-
level heterogeneity. We complement our theory
with experiments on hyperparameter & hyper-
representation learning and minimax optimiza-
tion that demonstrate the beneﬁts of our method
in practice. Code is available at https://
github.com/ucr-optml/FedNest.

1. Introduction

In the federated learning (FL) paradigm, multiple clients
cooperate to learn a model under the orchestration of a
central server (McMahan et al., 2017) without directly ex-
changing local client data with the server or other clients.
The locality of data distinguishes FL from traditional dis-
tributed optimization and also motivates new methodologies
to address heterogeneous data across clients. Additionally,
cross-device FL across many edge devices presents addi-
tional challenges since only a small fraction of clients partic-
ipate in each round, and clients cannot maintain state across
rounds (Kairouz et al., 2019).

1Email: tarzanaq@umich.edu, University of Michigan
2Emails: {mli176@,oymak@ece.}ucr.edu, University of
California, Riverside 3Email: cthrampo@ece.ubc.ca, Uni-
versity of British Columbia.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Traditional distributed SGD methods are often unsuitable in
FL and incur high communication costs. To overcome this
issue, popular FL methods, such as FEDAVG (McMahan
et al., 2017), use local client updates, i.e. clients update their
models multiple times before communicating with the server
(aka, local SGD). Although FEDAVG has seen great success,
recent works have exposed convergence issues in certain
settings (Karimireddy et al., 2020; Hsu et al., 2019). This
is due to a variety of factors, including client drift, where
local models move away from globally optimal models due
to objective and/or systems heterogeneity.

Existing FL methods, such as FEDAVG, are widely applied
to stochastic problems with single-level structure. Instead,
many machine learning tasks – such as adversarial learning
(Madry et al., 2017), meta learning (Bertinetto et al., 2018),
hyperparameter optimization (Franceschi et al., 2018), rein-
forcement/imitation learning (Wu et al., 2020; Arora et al.,
2020), and neural architecture search (Liu et al., 2018) – ad-
mit nested formulations that go beyond the standard single-
level structure. Towards addressing such nested problems,
bilevel optimization has received signiﬁcant attention in
the recent literature (Ghadimi & Wang, 2018; Hong et al.,
2020; Ji et al., 2021); albeit in non-FL settings. On the
other hand, federated versions have been elusive perhaps
due to the additional challenges surrounding heterogeneity,
communication, and inverse Hessian approximation.

Contributions: This paper addresses these challenges and
develops FEDNEST: A federated machinery for nested
problems with provable convergence and lightweight com-
munication. FEDNEST is composed of FEDINN: a feder-
ated stochastic variance reduction algorithm (FEDSVRG)
to solve the inner problem while avoiding client drift, and
FEDOUT: a communication-efﬁcient federated hypergradi-
ent algorithm for solving the outer problem. Importantly,
we allow both inner & outer objectives to be ﬁnite sums over
heterogeneous client functions. FEDNEST runs a variant
of FEDSVRG on inner & outer variables in an alternating
fashion as outlined in Algorithm 1. We make multiple algo-
rithmic and theoretical contributions summarized below.
• The variance reduction of FEDINN enables robustness
in the sense that local models converge to the globally
optimal inner model despite client drift/heterogeneity
unlike FEDAVG. While FEDINN is similar
to
FEDSVRG (Koneˇcn`y et al., 2018) and FEDLIN (Mi-

 
 
 
 
 
 
FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Communication-efﬁciency:

(cid:3)(cid:51) FEDIHGP avoids explicit Hessian
(cid:3)(cid:51) LFEDNEST for local hypergradients

Client heterogeneity:

Finite sample bilevel theory:

(cid:3)(cid:51) FEDINN avoids client drift
(cid:3)(cid:51) Stochastic inner & outer analysis
Speciﬁc nested optimization problems:
(cid:3)(cid:51) Bilevel
(cid:3)(cid:51) Minimax
(cid:3)(cid:51) Compositional

Figure 1: Depiction of federated bilevel nested optimization and high-level summary of FEDNEST (Algorithm 1). At
outer loop, FEDIHGP uses multiple rounds of matrix-vector products to facilitate hypergradient computation while only
communicating vectors. At inner loop, FEDINN uses FEDSVRG to avoid client drift and ﬁnd the unique global minima.
Both are crucial for establishing provable convergence of FEDNEST.

tra et al., 2021), we make two key contributions: (i) We
leverage the global convergence of FEDINN to ensure
accurate hypergradient computation which is crucial for
our bilevel proof. (ii) We establish new convergence guar-
antees for single-level stochastic non-convex FEDSVRG,
which are then integrated within our FEDOUT.

• Communication efﬁcient bilevel optimization: Within
FEDOUT, we develop an efﬁcient federated method for
hypergradient estimation that bypass Hessian compu-
tation. Our approach approximates the global Inverse
Hessian-Gradient-Product (IHGP) via computation of
matrix-vector products over few communication rounds.
• LFEDNEST: To further improve communication efﬁ-
ciency, we additionally propose a Light-FEDNEST algo-
rithm, which computes hypergradients locally and only
needs a single communication round for the outer update.
Experiments reveal that LFEDNEST becomes very com-
petitive as client functions become more homogeneous.
• Uniﬁed federated nested theory: We specialize our
bilevel results to minimax and compositional optimiza-
tion with emphasis on the former. For these, FEDNEST
signiﬁcantly simpliﬁes and leads to faster convergence.
Importantly, our results are on par with the state-of-the-
art non-federated guarantees for nested optimization lit-
erature without additional assumptions (Table 1).

• We provide extensive numerical experiments on bilevel
and minimax optimization problems. These demonstrate
the beneﬁts of FEDNEST, efﬁciency of LFEDNEST, and
shed light on tradeoffs surrounding communication, com-
putation, and heterogeneity.

2. Federated Nested Problems & FEDNEST

Stochastic Bilevel Optimization

FEDNEST

ALSET

Non-Federated
BSA

TTSA

O(1)

O(κ5
O(κ9

g(cid:15)−2)
g(cid:15)−2)

O(κ5
O(κ9

g(cid:15)−2)
g(cid:15)−2)

O(κ6
O(κ9

g(cid:15)−2)
g(cid:15)−3)

O(κp
O(κp

g(cid:15)−2.5)
g(cid:15)−2.5)

batch size
samples in ξ
samples in ζ

Stochastic Minimax Optimization

FEDNEST
O(1)

ALSET
O(1)

Non-Federated
SGDA
O((cid:15)−1)

SMD
N.A.

O(κ3

f (cid:15)−2)

batch size
samples

Stochastic Compositional Optimization
Non-Federated
SCGD

FEDNEST

ALSET

NASA

batch size
samples

O((cid:15)−2)

O((cid:15)−2)

O((cid:15)−4)

O((cid:15)−2)

O(1)

Table 1: Sample complexity of FEDNEST and comparable non-FL
methods to ﬁnd an (cid:15)-stationary point of f : κg := (cid:96)g,1/µg and
κf := (cid:96)f,1/µf . κp
g denotes a polynomial function of κg. ALSET
(Chen et al., 2021a), BSA (Ghadimi & Wang, 2018), TTSA (Hong
et al., 2020), SGDA (Lin et al., 2020), SMD (Raﬁque et al., 2021),
SCGD (Wang et al., 2017), and NASA (Ghadimi et al., 2020).

bers, respectively. For a differentiable function h(x, y) :
Rd1 × Rd2 → R in which y = y(x) : Rd1 → Rd2, we
denote ∇h ∈ Rd1 the gradient of h as a function of x and
∇xh, ∇yh the partial derivatives of h with respect to x and
y, respectively. We let ∇2
xyh and ∇2
yh denote the Jacobian
and Hessian of h, respectively. We consider FL optimization
over m clients and we denote S = {1, . . . , m}. For vectors
v ∈ Rd and matrix M ∈ Rd×d, we denote (cid:107)v(cid:107) and (cid:107)M (cid:107)
the respective Euclidean and spectral norms.

2.1. Preliminaries on Federated Nested Optimization

In federated bilevel learning, we consider the following
nested optimization problem as depicted in Figure 1:

We will ﬁrst provide the background on bilevel nested prob-
lems and then introduce our general federated method.
Notation. N and R denotes the set of natural and real num-

min
x∈Rd1
subj. to

(cid:80)m

f (x) = 1
m
y∗(x) ∈ argmin
y∈Rd2

i=1 fi (x, y∗(x))
(cid:80)m

1
m

i=1 gi (x, y) .

(1a)

Federated Bilevel LearningServerClient 1Client 2Client 3Client !2!!!"!#!$"!"""#"$"!,$!"",$""#,$#"$,$$"=1!'%&!$"%$=1!'%&!$$%InnerOuter(∗(*)FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

FEDOUT with stepsizes {(αk, βk)}K−1
k=0

FEDNEST
Algorithm 1 FEDNEST
FEDNEST
1: Inputs: K, T ∈ N; (x0, y0) ∈ Rd1+d2; FEDINN,
2:
3: for k = 0, · · · , K − 1 do
4:
5:
6:
7:
8:
9:
10: end for

yk,0 = yk
for t = 0, · · · , T − 1 do
FEDINN (cid:0)xk, yk,t, βk(cid:1)
yk,t+1 = FEDINN
FEDINN

end for
yk+1 = yk,T
FEDOUT (cid:0)xk, yk+1, αk(cid:1)
xk+1 = FEDOUT
FEDOUT

Recall that m is the number of clients. Here, to model
objective heterogeneity, each client i is allowed to have its
own individual outer & inner functions (fi, gi). Moreover,
we consider a general stochastic oracle model, access to
local functions (fi, gi) is via stochastic sampling as follows:

fi(x, y∗(x)) := Eξ∼Ci [fi(x, y∗(x); ξ)] ,
gi(x, y) := Eζ∼Di [gi(x, y; ζ)] ,

(1b)

where (ξ, ζ) ∼ (Ci, Di) are outer/inner sampling distribu-
tions for the ith client. We emphasize that for i (cid:54)= j, the
tuples (fi, gi, Ci, Di) and (fj, gj, Cj, Dj) can be different.
Example 1 (Hyperparameter tuning). Each client has lo-
cal validation and training datasets associated with objec-
tives (fi, gi)m
i=1 corresponding to validation and training
losses, respectively. The goal is ﬁnding hyper-parameters x
that lead to learning model parameters y that minimize the
(global) validation loss.

The stochastic bilevel problem (1) subsumes two popular
problem classes with the nested structure: Stochastic Mini-
Max & Stochastic Compositional. Therefore, results on the
general nested problem (1) also imply the results in these
special cases. Below, we brieﬂy describe them.

Minimax optimization. If gi(x, y; ζ) := −fi(x, y; ξ) for
all i ∈ S, the stochastic bilevel problem (1) reduces to the
stochastic minimax problem

min
x∈Rd1

f (x) :=

1
m

max
y∈Rd2

m
(cid:88)

i=1

E[fi (x, y; ξ)].

(2)

Motivated by applications in fair beamforming, training
generative-adversarial networks (GANs) and robust ma-
chine learning, signiﬁcant efforts have been made for solv-
ing (2) including (Daskalakis & Panageas, 2018; Gidel et al.,
2018; Mokhtari et al., 2020; Thekumparampil et al., 2019).

Example 2 (GANs). We train a generative model gx(·) and
an adversarial model ay(·) using client datasets Ci. The
local functions may for example take the form fi(x, y) =
Es∼Ci{log ay(s)} + Ez∼Dnoise {log[1 − ay(gx(z))]}.

Compositional optimization. Suppose fi(x, y; ξ) :=
fi(y; ξ) and gi is quadratic in y given as gi(x, y; ζ) :=
(cid:107)y − ri(x; ζ)(cid:107)2. Then, the bilevel problem (1) reduces to
(cid:80)m

i=1 fi (y∗(x))

min
x∈Rd1
subj. to

f (x) = 1
m
y∗(x) = argmin
y∈Rd2

1
m

(cid:80)m

i=1 gi (x, y)

(3)

with fi(y∗(x)) := Eξ∼Ci [fi(y∗(x); ξ)] and gi(x, y) :=
Eζ∼Di[gi(x, y; ζ)]. Optimization problems in the form of
(3) occur for example in model agnostic meta-learning and
policy evaluation in reinforcement learning (Finn et al.,
2017; Ji et al., 2020b; Dai et al., 2017; Wang et al., 2017).
Assumptions. Let z = (x, y) ∈ Rd1+d2. Throughout, we
make the following assumptions on inner/outer objectives.

Assumption A (Well-behaved objectives). For all i ∈ [m]:
(A1) fi(z), ∇fi(z), ∇gi(z), ∇2gi(z) are

(cid:96)f,0,(cid:96)f,1,(cid:96)g,1,

(cid:96)g,2-Lipschitz continuous, respectively; and
(A2) gi(x, y) is µg-strongly convex in y for all x ∈ Rd1.

Throughout, we use κg = (cid:96)g,1/µg to denote the condition
number of the inner function g.
Assumption B (Stochastic samples). For all i ∈ [m]:
(B1) ∇fi(z; ξ), ∇gi(z; ζ), ∇2gi(z; ζ) are unbiased estima-

tors of ∇fi(z), ∇gi(z), ∇2gi(z), respectively; and

(B2) Their variances are bounded, i.e., Eξ[(cid:107)∇fi(z; ξ) −
g,1,
g,2 for some

∇fi(z)(cid:107)2] ≤ σ2
and Eζ[(cid:107)∇2gi(z; ζ) − ∇2gi(z)(cid:107)2] ≤ σ2
f , σ2
σ2

f , Eζ[(cid:107)∇gi(z; ζ) − ∇gi(z)(cid:107)2] ≤ σ2

g,1, and σ2

g,2.

These assumptions are common in the bilevel optimization
literature (Ghadimi & Wang, 2018; Chen et al., 2021a; Ji
et al., 2021). Assumption A requires that the inner and outer
functions are well-behaved. Speciﬁcally, strong-convexity
of the inner objective is a recurring assumption in bilevel
optimization theory implying a unique solution to the inner
minimization in (1).

2.2. Proposed Algorithm: FEDNEST

In this section, we develop FEDNEST, which is formally pre-
sented in Algorithm 1. The algorithm operates in two nested
loops. The outer loop operates in rounds k ∈ {1, . . . , K}.
Within each round, an inner loop operating for T iterations
is executed. Given estimates xk and yk, each iteration
t ∈ {1, . . . , T } of the inner loop produces a new global
model yk,t+1 of the inner optimization variable y∗(xk) as
the output of an optimizer FEDINN. The ﬁnal estimate
yk+1 = yk,T of the inner variable is then used by an opti-
mizer FEDOUT to update the outer global model xk+1.

The subroutines FEDINN and FEDOUT are gradient-based
optimizers. Each subroutine involves a certain number of

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

local training steps indexed by ν ∈ {0, . . . , τi − 1} that
are performed at the ith client. The local steps of FEDINN
iterate over local models yi,ν of the inner variable. Ac-
cordingly, FEDOUT iterates over local models xi,ν of the
global variable. A critical component of FEDOUT is a
communication-efﬁcient federated hypergradient estimation
routine, which we call FEDIHGP. The implementation of
FEDINN, FEDOUT and FEDIHGP is critical to circumvent
the algorithmic challenges of federated bilevel optimization.
In the remaining of this section, we detail the challenges and
motivate our proposed implementations. Later, in Section 3,
we provide a formal convergence analysis of FEDNEST.

2.3. Key Challenge: Federated Hypergradient

Estimation

FEDOUT is a gradient-based optimizer for the outer min-
imization in (1); thus each iteration involves computing
∇f (x) = (1/m) (cid:80)m
i=1 ∇fi(x, y∗(x)). Unlike single-
level FL, the fact that the outer objective f depends ex-
plicitly on the inner minimizer y∗(x) introduces a new
challenge. A good starting point to understand the challenge
is the following evaluation of ∇f (x) in terms of partial
derivatives. The result is well-known from properties of
implicit functions.
Lemma 2.1. Under Assumption A, for all i ∈ [m]:
∇fi(x, y∗(x)) = ∇Dfi (x, y∗(x)) + ∇Ifi (x, y∗(x)) ,

where the direct and indirect gradient components are:

∇Dfi(x, y∗(x)) := ∇xfi (x, y∗(x)) ,
∇Ifi(x, y∗(x)) := −∇2
xyg(x, y∗(x))

(4a)

· (cid:2)∇2

yg(x, y∗(x))(cid:3)−1

∇yfi (x, y∗(x)) .

(4b)

We now use the above formula to describe the two core
challenges of bilevel FL optimization.

First, evaluation of any of the terms in (4) requires access
to the minimizer y∗(x) of the inner problem. On the other
hand, one may at best hope for a good approximation to
y∗(x) produced by the inner optimization subroutine. Of
course, this challenge is inherent in any bilevel optimization
setting, but is exacerbated in the FL setting because of client
drift. Speciﬁcally, when clients optimize their individual
(possibly different) local inner objectives, the global esti-
mate of the inner variable produced by SGD-type methods
may drift far from (a good approximation to) y∗(x). We
explain in Section 2.5 how FEDINN solves that issue.

The second challenge comes from the stochastic nature of
the problem. Observe that the indirect component in (4b)
is nonlinear in the Hessian ∇2
yg(x, y∗(x)), complicating
an unbiased stochastic approximation of ∇fi(x, y∗(x)).
As we expose here, solutions to this complication devel-
oped in the non-federated bilevel optimization literature, are

not directly applicable in the FL setting. Indeed, existing
stochastic bilevel algorithms, e.g. (Ghadimi & Wang, 2018),
deﬁne ¯∇f (x, y) := ¯∇Df (x, y) + ¯∇If (x, y) as a surro-
gate of ∇f (x, y∗(x)) by replacing y∗(x) in deﬁnition (4)
with an approximation y and using the following stochastic
approximations:

¯∇Df (x, y) ≈ ∇xf (x, y; ˙ξ),
¯∇If (x, y) ≈ −∇2

xyg(x, y; ζN (cid:48)+1)
yg(x, y; ζn)(cid:1)(cid:105)
∇2

(cid:104) N
(cid:96)g,1

N (cid:48)
(cid:89)

n=1

(cid:0)I −

1
(cid:96)g,1

(5a)

∇yf (x, y; ˙ξ). (5b)

Here, N (cid:48) is drawn from {0, . . . , N −1} uniformly at random
(UAR) and { ˙ξ, ζ1, . . . , ζN (cid:48)+1} are i.i.d. samples. Ghadimi
& Wang (2018); Hong et al. (2020) have shown that us-
ing (5), the inverse Hessian estimation bias exponentially
decreases with the number of samples N .

One might hope to directly leverage the above approach in
a local computation fashion by replacing the global outer
function f with the individual function fi. However, note
from (4b) and (5b) that the proposed stochastic approxima-
tion of the indirect gradient involves in a nonlinear way
the global Hessian, which is not available at the client 1.
Communication efﬁciency is one of the core objectives of
FL making the idea of communicating Hessians between
clients and server prohibitive. Is it then possible, in a FL
setting, to obtain an accurate stochastic estimate of the in-
direct gradient while retaining communication efﬁciency?
In Section 2.4, we show how FEDOUT and its subroutine
FEDIHGP, a matrix-vector products-based (thus, communi-
cation efﬁcient) federated hypergradient estimator, answer
this question afﬁrmatively.

2.4. Outer Optimizer: FEDOUT

This section presents the outer optimizer FEDOUT, formally
described in Algorithm 2. As a subroutine of FEDNEST
(see Line 9, Algorithm 1), at each round k = 0, . . . , K − 1,
FEDOUT takes the most recent global outer model xk to-
gether the updated (by FEDINN) global inner model yk+1
and produces an update xk+1. To lighten notation, for a
round k, denote the function’s input as (x, y+) (instead
of (xk, yk+1)) and the output as x+ (instead of xk+1).
For each client i ∈ S, FEDOUT uses stochastic approxi-
mations of ¯∇Ifi(x, y+) and ¯∇Dfi(x, y+), which we call
i (x, y+) and hD
hI
i (x, y+), respectively. The speciﬁc choice
of these approximations (see Line 5) is critical and is dis-
cussed in detail later in this section. Before that, we explain

1We note that the approximation in (5) is not the only construc-
tion, and bilevel optimization can accommodate other forms of
gradient surrogates (Ji et al., 2021). Yet, all these approximations
require access (in a nonlinear fashion) to the global Hessian; thus,
they suffer from the same challenge in FL setting.

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

FEDOUT (x, y+, α) for stochastic
Algorithm 2 x+ = FEDOUT
FEDOUT
bilevel and minimax problems
1: Fi(·) ← ∇xfi(·, y+; ·)
2: xi,0 = x and αi ∈ (0, α]
3: Choose N ∈ N and set pN (cid:48) = FEDIHGP
FEDIHGP (x, y+, N )
FEDIHGP
4: for i ∈ S in parallel do
hi = Fi(x; ξi) − ∇2
5:
hi = Fi(x; ξi)

xygi(x, y+; ζi)pN (cid:48)

i∈S hi

6:
7: end for
8: h = |S|−1 (cid:80)
9: for i ∈ S in parallel do
10:
11:
12:
13:
14: end for
15: x+ = |S|−1 (cid:80)

end for

i∈S xi,τi

for ν = 0, . . . , τi − 1 do

hi,ν = Fi(xi,ν; ξi,ν) − Fi(x; ξi,ν) + h
xi,ν+1 = xi,ν − αihi,ν

pi,0 = ∇yfi(x, y+; ξi,0)

FEDIHGP (x, y+, N ): Federated
FEDIHGP
Algorithm 3 pN (cid:48) = FEDIHGP
approximation of inverse-Hessian-gradient product
1: Select N (cid:48) ∈ {0, . . . , N − 1} UAR.
2: Select S0 ∈ S UAR.
3: for i ∈ S0 in parallel do
4:
5: end for
6: p0 = N
(cid:96)g,1
7: if N (cid:48) = 0 then
Return pN (cid:48)
8:
9: end if
10: Select S1, . . . , SN (cid:48) ∈ S UAR.
11: for n = 1, . . . , N (cid:48) do
12:

|S0|−1 (cid:80)

pi,0

i∈S0

for i ∈ Sn in parallel do
(cid:16)
∇2

pi,n =

I − 1
(cid:96)g,1

ygi(x, y+; ζi,n)

pn−1

(cid:17)

13:

end for
pn = |Sn|−1 (cid:80)

14:
15:
16: end for

pi,n

i∈Sn

how each client uses these proxies to form local updates of
the outer variable. In each round, starting from a common
global model xi,0 = x, each client i performs τi local steps
(in parallel):

xi,ν+1 = xi,ν − αihi,ν,

(6)

and then the server aggregates local models via x+ =
|S|−1 (cid:80)

i∈S xi,τi. Here, αi ∈ (0, α] is the local stepsize,

hi,ν :=hI(x, y+) + hD(x, y+)

− hD

i (x, y+) + hD

i (xi,ν, y+) ,

hI(x, y)
|S|−1 (cid:80)

:= |S|−1 (cid:80)
i (x, y).

i∈S hD

i∈S hI

i (x, y), and hD(x, y)

(7)

:=

The key features of updates (6)–(7) are exploiting past gra-
dients (variance reduction) to account for objective het-
erogeneity. Indeed, the ideal update in FEDOUT would
(cid:0)hI(xi,ν, y+) +
perform the update xi,ν+1 = xi,ν − αi
hD(xi,ν, y+)(cid:1) using the global gradient estimates. But this
requires each client i to have access to both direct and indi-
rect gradients of all other clients–which it does not, since
clients do not communicate between rounds. To overcome
this issue, each client i uses global gradient estimates, i.e.,
hI(x, y+) + hD(x, y+) from the beginning of each round
as a guiding direction in its local update rule. However,
since both hD and hI are computed at a previous (x, y+),
client i makes a correction by subtracting off the stale di-
rect gradient estimate hD
i (x, y+) and adding its own local
estimate hD
i (xi,ν, y+). Our local update rule in Step 11
of Algorithm 2 is precisely of this form, i.e., hi,ν approx-
imates hI(xi,ν, y+) + hD(xi,ν, y+) via (7). Note here
that the described local correction of FEDOUT only applies

to the direct gradient component (the indirect component
would require global Hessian information). An alterantive
approach leading to LFEDNEST is discussed in Section 2.6.

FEDOUT applied to special nested problems. Algo-
rithm 2 naturally allows the use of other optimizers for
minimax & compositional optimization. For example,
in the minimax problem (2), the bilevel gradient com-
ponents are ∇Dfi(x, y∗(x)) = ∇xfi (x, y∗(x)) and
∇Ifi(x, y∗(x)) = 0 for all i ∈ S. Hence, the hyper-
gradient estimate (7) reduces to

hi,ν = hD(x, y+) − hD

i (x, y+) + hD

i (xi,ν, y+).

(8)

For the compositional problem (3), Hessian becomes the
identity matrix, the direct gradient is the zero vector, and
∇xyg(x, y) = −(1/m) (cid:80)m
i=1 ∇ri(x)(cid:62). Hence, hi =
(cid:96)g,1∇ri(x)(cid:62)p0 for all i ∈ S.

More details on these special cases are provided in Appen-
dices D and E.

Indirect gradient estimation & FEDIHGP. Here, we aim
to address one of the key challenges in nested FL: inverse
Hessian gradient product. Note from (5b) that the proposed
stochastic approximation of the indirect gradient involves in
a nonlinear way the global Hessian, which is not available at
the client. To get around this, we use a client sampling strat-
egy and recursive reformulation of (5b) so that ¯∇Ifi(x, y)
can be estimated in an efﬁcient federated manner. In par-
ticular, given N ∈ N, we select N (cid:48) ∈ {0 . . . , N − 1} and
S0, . . . , SN (cid:48) ∈ S UAR. For all i ∈ S, we then deﬁne

hI
i (x, y) = −∇2

xygi(x, y; ζi)pN (cid:48),

(9a)

where pN (cid:48) = |S0|−1 (cid:99)Hy

(cid:80)

i∈S0

∇yfi(x, y; ξi,0) and (cid:99)Hy

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

is the approximate inverse Hessian:

Algorithm 4 y+ = FEDINN
FEDINN
FEDINN (x, y, β)

N
(cid:96)g,1

N (cid:48)
(cid:89)

(cid:16)

n=1

I −

1
(cid:96)g,1|Sn|

|Sn|
(cid:88)

i=1

∇2

ygi(x, y; ζi,n)

(cid:17)

.

(9b)

The subroutine FEDIHGP provides a recursive strategy to
compute pN (cid:48) and FEDOUT multiplies pN (cid:48) with the Jaco-
bian to drive an indirect gradient estimate.
Importantly,
these approximations require only matrix-vector products
and vector communications.

Lemma 2.2. Under Assumptions A and B, the approximate
inverse Hessian (cid:99)Hy deﬁned in (9b) satisﬁes the following
for any x and y:

1: Gi(·) ← ∇ygi(x, ·) (bilevel) , −∇yfi(x, ·) (minimax)
2: yi,0 = y and βi ∈ (0, β]
3: for i ∈ S in parallel do
qi = Gi(y; ζi)
4:
5: end for
6: q = |S|−1 (cid:80)
7: for i ∈ S in parallel do
8:
9:
10:
11:
12: end for
13: y+ = |S|−1 (cid:80)

qi,ν = Gi(yi,ν; ζi,ν) − Gi(y; ζi,ν) + q
yi,ν+1 = yi,ν − βiqi,ν

for ν = 0, . . . , τi − 1 do

end for

i∈S qi

i∈S yi,τi

(cid:13)
(cid:2)∇2
(cid:13)
(cid:13)

yg(x, y)(cid:3)−1
(cid:104)(cid:13)
(cid:2)∇2
(cid:13)
(cid:13)

yg(x, y)(cid:3)−1

EW

− EW [(cid:99)Hy]

− (cid:99)Hy

(cid:13)
(cid:13)
(cid:13) ≤
(cid:13)
(cid:105)
(cid:13)
(cid:13)

≤

1
µg
2
µg

(cid:18) κg − 1
κg

(cid:19)N

,

2.6. Light-FEDNEST: Communication Efﬁciency via

.

(10)

Local Hypergradients

Here, W := {Sn, ξi, ζi, ξi,0, ζi,n | i ∈ Sn, 0 ≤ n ≤ N (cid:48)}.
Further, for all i ∈ S, hI

i (x, y) deﬁned in (9a) satisﬁes

(cid:13)
(cid:13)EW [hI

i (x, y)] − ¯∇Ifi(x, y)(cid:13)

(cid:13) ≤ b,

(11)

where b := κg(cid:96)f,1

(cid:0)(κg − 1)/κg

(cid:1)N

.

2.5. Inner Optimizer: FEDINN

In FL, each client performs multiple local training steps in
isolation on its own data (using for example SGD) before
communicating with the server. Due to such local steps,
FEDAVG suffers from a client-drift effect under objective
heterogeneity; that is, the local iterates of each client drift-
off towards the minimum of their own local function. In
turn, this can lead to convergence to a point different from
the global optimum y∗(x) of the inner problem; e.g., see
(Mitra et al., 2021). This behavior is particularly undesirable
in a nested optimization setting since it directly affects the
outer optimization; see, e.g. (Liu et al., 2021, Section 7).

In light of this observation, we build on the recently
proposed FEDLIN (Mitra et al., 2021) which improves
FEDSVRG (Koneˇcn`y et al., 2018) to solve the inner prob-
lem; see Algorithm 4. For each i ∈ S, let qi(x, y) de-
note an unbiased estimate of the gradient ∇ygi(x, y). In
each round, starting from a common global model y, each
client i performs τi local SVRG-type training steps in par-
allel: yi,ν+1 = yi,ν − βiqi,ν, where qi,ν := qi(x, yi,ν) −
qi(x, y) + q(x, y), βi ∈ (0, β] is the local inner step-
size, and q(x, y) := |S|−1 (cid:80)
i∈S qi(x, y). We note that
for the optimization problems (1), (2), and (3), qi(x, yi,ν)
is equal to ∇ygi(x, yi,ν; ζi,ν), −∇yfi(x, yi,ν; ξi,ν), and
yi,ν − ri(x; ζi,ν), respectively; see Appendices C–E.

Each FEDNEST epoch k requires 2T + N + 3 communica-
tion rounds as follows: 2T rounds for SVRG of FEDINN, N
iterations for inverse Hessian approximation within FEDI-
HGP and 3 additional aggregations. Note that, these are
vector communications and we fully avoid Hessian com-
munication. In Appendix A, we also propose simpliﬁed
variants of FEDOUT and FEDIHGP, which are tailored to
homogeneous or high-dimensional FL settings. These algo-
rithms can then either use local Jacobian / inverse Hessian
or their approximation, and can use either SVRG or SGD.

Light-FEDNEST: Speciﬁcally, we propose LFEDNEST
where each client runs IHGP locally. This reduces the num-
ber of rounds to T + 1, saving T + N + 2 rounds (see
experiments in Section 4 for performance comparison and
Appendix A for further discussion.)

3. Convergence Analysis for FEDNEST

In this section, we present convergence results for FEDNEST.
All proofs are relegated to Appendices C–E.

Theorem 3.1. Suppose Assumptions A and B hold. Further,
assume αk

i = βk/τi for all i ∈ S, where

i = αk/τi and βk
¯βαk
T

, αk = min

βk =

(cid:26)

¯α1, ¯α2, ¯α3,

(cid:27)

¯α
√
K

(12)

for some positive constants ¯α1, ¯α2, ¯α3, ¯α, and ¯β indepen-
dent of K. Then, for any T ≥ 1, the iterates {(xk, yk)}k≥0
generated by FEDNEST satisfy

1
K

K
(cid:88)

k=1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

= O

g,1, σ2
(cid:16) ¯α max(σ2
√
K

g,2, σ2
f )

+

1
min(¯α1, ¯α2, ¯α3)K

+ b2(cid:17)

,

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

where b = κg(cid:96)f,1
rameter to FEDIHGP.

(cid:0)(κg − 1)/κg

(cid:1)N

and N is the input pa-

Corollary 3.1 (Bilevel). Under the same conditions as in
Theorem 3.1, if N = O(κg log K) and T = O(κ4

g), then

Corrollary 3.3 implies that for the compositional problem
(3), the convergence rate of FEDNEST to the stationary point
of f is O(1/
K). This matches the convergence rate of
non-federated stochastic algorithms such as SCGD (Wang
et al., 2017) and NASA (Ghadimi et al., 2020) (Table 1).

√

1
K

K
(cid:88)

k=1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

= O

(cid:32)

κ4
g
K

+

κ2.5
g√

K

(cid:33)

.

For (cid:15)-accurate stationary point, we need K = O(κ5

g(cid:15)−2).
√

Above, we choose N ∝ κg log K to guarantee b2 (cid:46) 1/
K.
In contrast, we use T (cid:38) κ4
g inner SVRG epochs. From
Section 2.6, this would imply the communication cost is
dominated by SVRG epochs N and O(κ4

g) rounds.

From Corollary 3.1, we remark that FEDNEST matches
the guarantees of non-federated alternating SGD methods,
such as ALSET (Chen et al., 2021a) and BSA (Ghadimi &
Wang, 2018), despite federated setting, i.e. communication
challenge, heterogeneity in the client objectives, and device
heterogeneity.

3.1. Minimax Federated Learning

We focus on special features of federated minimax prob-
lems and customize the general results to yield improved
convergence results for this special case. Recall from (2)
that gi(x, y) = −fi(x, y) which implies that b = 0 and
following Assumption A, fi(x, y) is µf –strongly concave
in y for all x.

Corollary 3.2 (Minimax). Denote κf = (cid:96)f,1/µf . Assume
same conditions as in Theorem 3.1 and T = O(κf ). Then,

1
K

K
(cid:88)

k=1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

= O

(cid:32) κ2
f
K

(cid:33)

.

+

κf√
K

3.3. Single-Level Federated Learning

Building upon the general results for stochastic nonconvex
nested problems, we establish new convergence guaran-
tees for single-level stochastic non-convex federated SVRG
which is integrated within our FEDOUT. Note that in the
single-level setting, the optimization problem (1) reduces to

min
x∈Rd1

f (x) =

1
m

m
(cid:88)

i=1

fi (x)

(13)

with fi(x) := Eξ∼Ci[fi(x; ξ)], where ξ ∼ Ci is sampling
distribution for the ith client.

We make the following assumptions on (13) that are coun-
terparts of Assumptions A and B.
Assumption C (Lipschitz continuity). For all i ∈ [m],
∇fi(x) is Lf -Lipschitz continuous.
Assumption D (Stochastic samples). For all i ∈ [m],
∇fi(x; ξ) is an unbiased estimator of ∇fi(x) and its vari-
ance is bounded, i.e., Eξ[(cid:107)∇fi(x; ξ) − ∇fi(x)(cid:107)2] ≤ σ2
f .
Theorem 3.2 (Single-Level). Suppose Assumptions C and
D hold. Further, assume αk
i = αk/τi for all i ∈ S, where

(cid:26)

αk = min

¯α1,

(cid:27)

¯α
√
K

(14)

for some positive ¯α1 and ¯α. Then,
(cid:32)

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

= O

1
K

K
(cid:88)

k=1

∆f
¯α1K

∆f
¯α + ¯ασ2
√
K

f

(cid:33)

,

+

√

Corrollary 3.2 implies that for the minimax problem, the
convergence rate of FEDNEST to the stationary point of f
is O(1/
K). Again, we note this matches the convergence
rate of non-FL algorithms (see also Table 1) such as SGDA
(Lin et al., 2020) and SMD (Raﬁque et al., 2021).

where ∆f := f (x0) − E[f (xK)].

Theorem 3.2 extends recent results by (Mitra et al., 2021)
from the stochastic strongly convex to the stochastic noncon-
vex setting. The above rate is also consistent with existing
single-level non-FL guarantees (Ghadimi & Lan, 2013).

3.2. Compositional Federated Learning

4. Numerical Experiments

Observe that in the compositional problem (3), the outer
function is fi(x, y; ξ) = fi(y; ξ) and the inner function
is gi(x, y; ζ) = 1
2 (cid:107)y − ri(x; ζ)(cid:107)2, for all i ∈ S. Hence,
b = 0 and κg = 1.

Corollary 3.3 (Compositional). Under the same condi-
tions as in Theorem 3.1, if we select T = 1 in (12). Then,

1
K

K
(cid:88)

k=1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

= O

(cid:18) 1
√
K

(cid:19)

.

In this section, we numerically investigate the impact of sev-
eral attributes of our algorithms on a hyper-representation
problem (Franceschi et al., 2018), a hyper-parameter opti-
mization problem for loss function tuning (Li et al., 2021),
and a federated minimax optimization problem.

4.1. Hyper-Representation Learning

Modern approaches in meta learning such as MAML (Finn
et al., 2017) and reptile (Nichol & Schulman, 2018) learn

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

y
c
a
r
u
c
c
a

t
s
e
T

FEDNEST epochs

(a) Comparison between FEDNEST and
LFEDNEST.

Communication rounds
(b) SVRG in FEDINN provides better con-
vergence and stability.

Communication rounds
(c) Larger τ in FEDOUT provides better
performance.

Figure 2: Hyper-representation experiments on a 2-layer MLP and MNIST dataset.

representations (that are shared across all tasks) in a bilevel
manner. Similarly, the hyper-representation problem opti-
mizes a classiﬁcation model in a two-phased process. The
outer objective optimizes the model backbone to obtain
better feature representation on validation data. The inner
problem optimizes a header for downstream classiﬁcation
tasks on training data. In this experiment, we use a 2-layer
multilayer perceptron (MLP) with 200 hidden units. The
outer problem optimizes the hidden layer with 157,000 pa-
rameters, and the inner problem optimizes the output layer
with 2,010 parameters. We study both i.i.d and non-i.i.d.
ways of partitioning the MNIST data exactly following FE-
DAVG (McMahan et al., 2017), and split each client’s data
evenly to train and validation datasets. Thus, each client has
300 train and 300 validation samples.

Figure 2 demonstrates the impact on test accuracy of several
important components of FEDNEST. Figure 2a compares
FEDNEST and LFEDNEST. Both algorithms perform well
on the i.i.d. setup, while on the non-i.i.d. setup, FEDNEST
achieves i.i.d. performance, signiﬁcantly outperforming
LFEDNEST. These ﬁndings are in line with our discussions
in Section 2.6. LFEDNEST saves on communication rounds
compared to FEDNEST and performs well on homogeneous
clients. However, for heterogeneous clients, the isolation
of local Hessian in LFEDNEST (see Algorithm 5 in Ap-
pendix A) degrades the test performance. Next, Figure 2b
demonstrates the importance of SVRG in FEDINN algo-
rithm for heterogeneous data (as predicted by our theoretical
considerations in Section 2.5). To further clarify the algo-
rithm difference in Figures 2b and 3a, we use FEDNESTSGD
to denote the FEDNEST algorithm where SGD is used in
FEDINN. Finally, Figure 2c elucidates the role of local
epoch τ in FEDOUT: larger τ saves on communication and
improves test performance by enabling faster convergence.

4.2. Loss Function Tuning on Imbalanced Dataset

We use bilevel optimization to tune a loss function for learn-
ing an imbalanced MNIST dataset. We aim to maximize
the class-balanced validation accuracy (which helps mi-

y
c
a
r
u
c
c
A

t
s
e
T
d
e
c
n
a
l
a
B

FEDNEST epochs

Communication rounds

(a) FEDNEST achieves similar
performance as non-federated
bilevel loss function tuning.

(b) SVRG in FEDINN provides
better convergence and stabil-
ity especially in non-iid setup.

Figure 3: Loss function tuning on a 3-layer MLP and im-
balanced MNIST dataset to maximize class-balanced test
accuracy. The brown dashed line is the accuracy on non-
federated bilevel optimization (Li et al., 2021), and the black
dashed line is the accuracy without tuning the loss function.

nority/tail classes). Following the problem formulation in
(Li et al., 2021), we tune the so-called VS-loss function
(Kini et al., 2021) in a federated setting. In particular, we
ﬁrst create a long-tail imbalanced MNIST dataset by ex-
ponentially decreasing the number of examples per class
(e.g. class 0 has 6,000 samples, class 1 has 3,597 samples
and ﬁnally, class 9 has only 60 samples). We partition the
dataset to 100 clients following again FEDAVG (McMahan
et al., 2017) on both i.i.d. and non-i.i.d. setups. Differ-
ent from the hyper-representation experiment, we employ
80%-20% train-validation on each client and use a 3-layer
MLP model with 200, 100 hidden units, respectively. It
is worth noting that, in this problem, the outer objective f
(aka validation cost) only depends on the hyperparameter
x through the optimal model parameters y∗(x); thus, the
direct gradient ∇Dfi(x, y∗(x)) is zero for all i ∈ S.

Figure 3 displays test accuracy vs epochs/rounds for our
federated bilevel algorithms. The horizontal dashed lines
serve as non-FL baselines: brown depicts accuracy reached
by bilevel optimization in non-FL setting, and, black depicts
accuracy without any loss tuning. Compared to these, Fig-
ure 3a shows that FEDNEST achieves near non-federated
performance. In Figure 3b, we investigate the key role of
SVRG in FEDINN by comparing it with possible alternative

01002003004005006065707580859095100FedNest, non-iidLFedNest, non-iidFedNest, iidLFedNest, iid010002000300040006065707580859095100FedNest, non-iidFedNestSGD, non-iidFedNest, iidFedNestSGD, iid010002000300040006065707580859095100=1, non-iid=5, non-iid=1, iid=5, iid020040060080010006065707580859095100FedNestLFedNest01000200030004000500060006065707580859095100FedNest, non-iidFedNestSGD, non-iidFedNest, iidFedNestSGD, iidFEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

2
(cid:107)
∗
y
−
y
(cid:107)

2
(cid:107)
∗
x
−
x
(cid:107)

Epoch

Epoch

Figure 4: FEDNEST converges linearly despite heterogene-
ity. LFEDNEST slightly outperforms FEDAVG-S.

implementation that uses SGD-type updates. The ﬁgure
conﬁrms our discussion in Section 2.5: SVRG offers sig-
niﬁcant performance gains that are pronounced by client
heterogeneity.

4.3. Federated Minimax Problem

We conduct experiments on the minimax problem (2) with

fi(x, y) := −

(cid:20) 1
2

(cid:107)y(cid:107)2 − b(cid:62)

i y + y(cid:62)Aix

(cid:21)

+

λ
2

(cid:107)x(cid:107)2,

to compare standard FEDAVG saddle-point (FEDAVG-
S) method updating (x, y) simultaneously (Hou et al.,
2021) and our alternative approaches (LFEDNEST and
This is a saddle-point
FEDNEST).
formulation of
(cid:80)m
2 (cid:107) 1
1
i=1 Aix − bi(cid:107)2. We set λ = 10, bi =
minx∈Rd1
m
(cid:80)m
i − 1
i ∼ N (0, s2Id),
i and Ai = tiI, where b(cid:48)
b(cid:48)
i=1 b(cid:48)
m
and ti is drawn UAR over (0, 0.1). Figure 4 shows that
LFEDNEST and FEDNEST outperform FEDAVG-S thanks
to their alternating nature. FEDNEST signiﬁcantly improves
the convergence of LFEDNEST due to controlling client-
drift. To our knowledge, FEDNEST is the only alternating
federated SVRG for minimax problems.

5. Related Work

Federated learning. FEDAVG was ﬁrst introduced by
McMahan et al. (2017), who showed it can dramatically
reduce communication costs. For identical clients, FEDAVG
coincides with local SGD (Zinkevich et al., 2010) which has
been analyzed by many works (Stich, 2019; Yu et al., 2019;
Wang & Joshi, 2018). Recently, many variants of FEDAVG
have been proposed to tackle issues such as convergence and
client drift. Examples include FEDPROX (Li et al., 2020b),
SCAFFOLD (Karimireddy et al., 2020), FEDSPLIT (Pathak
& Wainwright, 2020), FEDNOVA (Wang et al., 2020), and,
the most closely relevant to us FEDLIN (Mitra et al., 2021).
A few recent studies are also devoted to the extension of
FEDAVG to the minimax optimization (Rasouli et al., 2020;
Deng et al., 2020) and compositional optimization (Huang
et al., 2021). In contrast to these methods, FEDNEST makes
alternating SVRG updates between the global variables x
and y, and yields sample complexity bounds and batch size
choices that are on par with the non-FL guarantees (Table 1).
Evaluations in the Appendix H.1 reveal that both alternating

updates and SVRG provides a performance boost over these
prior approaches.

Bilevel optimization. This class of problems was ﬁrst in-
troduced by (Bracken & McGill, 1973), and since then, dif-
ferent types of approaches have been proposed. See (Sinha
et al., 2017; Liu et al., 2021) for surveys. Earlier works
in (Aiyoshi & Shimizu, 1984; Lv et al., 2007) reduced
the bilevel problem to a single-level optimization prob-
lem. However, the reduced problem is still difﬁcult to
solve due to for example a large number of constraints.
Recently, more efﬁcient gradient-based algorithms have
been proposed by estimating the hypergradient of ∇f (x)
through iterative updates (Maclaurin et al., 2015; Franceschi
et al., 2017; Domke, 2012; Pedregosa, 2016). The asymp-
totic and non-asymptotic analysis of bilevel optimization
has been provided in (Franceschi et al., 2018; Shaban
et al., 2019; Liu et al., 2020) and (Ghadimi & Wang, 2018;
Hong et al., 2020), respectively. There is also a line of
work focusing on minimax optimization (Nemirovski, 2004;
Daskalakis & Panageas, 2018) and compositional optimiza-
tion (Wang et al., 2017). Closely related to our work are
(Lin et al., 2020; Raﬁque et al., 2021; Chen et al., 2021a)
and (Ghadimi et al., 2020; Chen et al., 2021a) which provide
non-asymptotic analysis of SGD-type methods for minimax
and compositional problems with outer nonconvex objective,
respectively.

A more in-depth discussion of related work is given in Ap-
pendix B. We summarize the complexities of different meth-
ods for FL/non-FL bilevel optimization in Table 1.

6. Conclusions

We presented a new class of federated algorithms for solv-
ing general nested stochastic optimization spanning bilevel
and minimax problems. FEDNEST runs a variant of fed-
erated SVRG on inner & outer variables in an alternating
fashion. We established provable convergence rates for
FEDNEST under arbitrary client heterogeneity and intro-
duced variations for min-max and compositional problems
and for improved communication efﬁciency (LFEDNEST).
We showed that, to achieve an (cid:15)-stationary point of the
nested problem, FEDNEST requires O((cid:15)−2) samples in to-
tal, which matches the complexity of the non-federated
nested algorithms in the literature.

Acknowledgements
Davoud Ataee Tarzanagh was supported by ARO YIP award
W911NF1910027 and NSF CAREER award CCF-1845076.
Christos Thrampoulidis was supported by NSF Grant Num-
bers CCF-2009030 and HDR-1934641, and an NSERC
Discovery Grant. Mingchen Li and Samet Oymak were sup-
ported by the NSF CAREER award CCF-2046816, Google
Research Scholar award, and ARO grant W911NF2110312.

02550751001251501752001022101810141010106102102FedAvg-S, s=1FedAvg-S, s=10LFedNest, s=1LFedNest, s=10FedNest, s=1FedNest, s=1002550751001251501752001022101810141010106102102FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

References

Acar, D. A. E., Zhao, Y., Navarro, R. M., Mattina, M.,
Whatmough, P. N., and Saligrama, V. Federated learn-
ing based on dynamic regularization. arXiv preprint
arXiv:2111.04263, 2021.

Aiyoshi, E. and Shimizu, K. A solution method for the static
constrained stackelberg problem via penalty method.
IEEE Transactions on Automatic Control, 29(12):1111–
1114, 1984.

Al-Khayyal, F. A., Horst, R., and Pardalos, P. M. Global
optimization of concave functions subject to quadratic
constraints: an application in nonlinear bilevel program-
ming. Annals of Operations Research, 34(1):125–147,
1992.

Arora, S., Du, S., Kakade, S., Luo, Y., and Saunshi, N.
Provable representation learning for imitation learning
via bi-level optimization. In International Conference on
Machine Learning, pp. 367–376. PMLR, 2020.

Barazandeh, B., Huang, T., and Michailidis, G. A decentral-
ized adaptive momentum method for solving a class of
min-max optimization problems. Signal Processing, 189:
108245, 2021a.

Barazandeh, B., Tarzanagh, D. A., and Michailidis, G. Solv-
ing a class of non-convex min-max games using adaptive
momentum methods. In ICASSP 2021-2021 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3625–3629. IEEE, 2021b.

Basu, D., Data, D., Karakus, C., and Diggavi, S. Qsparse-
local-SGD: Distributed SGD with quantization, sparsiﬁ-
cation and local computations. In Advances in Neural
Information Processing Systems, pp. 14668–14679, 2019.

Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi,
A. Meta-learning with differentiable closed-form solvers.
arXiv preprint arXiv:1805.08136, 2018.

Bracken, J. and McGill, J. T. Mathematical programs with
optimization problems in the constraints. Operations
Research, 21(1):37–44, 1973.

Brown, G. W. Iterative solution of games by ﬁctitious play.
Activity analysis of production and allocation, 13(1):374–
376, 1951.

Chen, T., Sun, Y., and Yin, W. Closing the gap: Tighter anal-
ysis of alternating stochastic gradient methods for bilevel
problems. Advances in Neural Information Processing
Systems, 34, 2021a.

Dagréou, M., Ablin, P., Vaiter, S., and Moreau, T. A frame-
work for bilevel optimization that enables stochastic and
global variance reduction algorithms. arXiv preprint
arXiv:2201.13409, 2022.

Dai, B., He, N., Pan, Y., Boots, B., and Song, L. Learn-
ing from conditional distributions via dual embeddings.
In Artiﬁcial Intelligence and Statistics, pp. 1458–1467.
PMLR, 2017.

Daskalakis, C. and Panageas, I. The limit points of (opti-
mistic) gradient descent in min-max optimization. arXiv
preprint arXiv:1807.03907, 2018.

Deng, Y. and Mahdavi, M. Local stochastic gradient de-
scent ascent: Convergence analysis and communication
efﬁciency. In International Conference on Artiﬁcial Intel-
ligence and Statistics, pp. 1387–1395. PMLR, 2021.

Deng, Y., Kamani, M. M., and Mahdavi, M. Distributionally
robust federated averaging. Advances in Neural Informa-
tion Processing Systems, 33:15111–15122, 2020.

Diakonikolas, J., Daskalakis, C., and Jordan, M. Efﬁcient
methods for structured nonconvex-nonconcave min-max
optimization. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 2746–2754. PMLR, 2021.

Domke, J. Generic methods for optimization-based model-
ing. In Artiﬁcial Intelligence and Statistics, pp. 318–326.
PMLR, 2012.

Edmunds, T. A. and Bard, J. F. Algorithms for nonlinear
bilevel mathematical programs. IEEE transactions on
Systems, Man, and Cybernetics, 21(1):83–89, 1991.

Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In Interna-
tional Conference on Machine Learning, pp. 1126–1135.
PMLR, 2017.

Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
Forward and reverse gradient-based hyperparameter opti-
mization. In International Conference on Machine Learn-
ing, pp. 1165–1173. PMLR, 2017.

Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil,
M. Bilevel programming for hyperparameter optimiza-
tion and meta-learning. In International Conference on
Machine Learning, pp. 1568–1577. PMLR, 2018.

Ghadimi, S. and Lan, G. Stochastic ﬁrst-and zeroth-order
methods for nonconvex stochastic programming. SIAM
Journal on Optimization, 23(4):2341–2368, 2013.

Chen, T., Sun, Y., and Yin, W. A single-timescale
stochastic bilevel optimization method. arXiv preprint
arXiv:2102.04671, 2021b.

Ghadimi, S. and Wang, M. Approximation methods for
bilevel programming. arXiv preprint arXiv:1802.02246,
2018.

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Ghadimi, S., Ruszczynski, A., and Wang, M. A single
timescale stochastic approximation method for nested
stochastic optimization. SIAM Journal on Optimization,
30(1):960–979, 2020.

Ji, K., Yang, J., and Liang, Y. Bilevel optimization: Con-
In Interna-
vergence analysis and enhanced design.
tional Conference on Machine Learning, pp. 4882–4892.
PMLR, 2021.

Gidel, G., Berard, H., Vignoud, G., Vincent, P., and
Lacoste-Julien, S. A variational inequality perspec-
tive on generative adversarial networks. arXiv preprint
arXiv:1802.10551, 2018.

Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S. On
the iteration complexity of hypergradient computation.
In International Conference on Machine Learning, pp.
3748–3758. PMLR, 2020.

Guo, Z., Xu, Y., Yin, W., Jin, R., and Yang, T. On stochastic
moving-average estimators for non-convex optimization.
arXiv preprint arXiv:2104.14840, 2021.

Hansen, P., Jaumard, B., and Savard, G. New branch-and-
bound rules for linear bilevel programming. SIAM Jour-
nal on scientiﬁc and Statistical Computing, 13(5):1194–
1217, 1992.

Hong, M., Wai, H.-T., Wang, Z., and Yang, Z. A two-
timescale framework for bilevel optimization: Complex-
ity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.

Hou, C., Thekumparampil, K. K., Fanti, G., and Oh, S. Efﬁ-
cient algorithms for federated saddle point optimization.
arXiv preprint arXiv:2102.06333, 2021.

Hsu, T.-M. H., Qi, H., and Brown, M. Measuring the effects
of non-identical data distribution for federated visual clas-
siﬁcation. arXiv preprint arXiv:1909.06335, 2019.

Huang, F. and Huang, H. Biadam: Fast adaptive bilevel
optimization methods. arXiv preprint arXiv:2106.11396,
2021.

Huang, F., Li, J., and Huang, H. Compositional federated
learning: Applications in distributionally robust averag-
ing and meta learning. arXiv preprint arXiv:2106.11264,
2021.

Ji, K. and Liang, Y. Lower bounds and accelerated algo-
rithms for bilevel optimization. ArXiv, abs/2102.03926,
2021.

Ji, K., Yang, J., and Liang, Y. Provably faster algorithms
for bilevel optimization and applications to meta-learning.
ArXiv, abs/2010.07962, 2020a.

Ji, K., Yang, J., and Liang, Y. Theoretical convergence of
multi-step model-agnostic meta-learning. arXiv preprint
arXiv:2002.07836, 2020b.

Ji, S. A pytorch implementation of federated learning. Mar

2018. doi: 10.5281/zenodo.4321561.

Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis,
M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode,
G., Cummings, R., et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977,
2019.

Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S.,
and Suresh, A. T. Scaffold: Stochastic controlled averag-
ing for federated learning. In International Conference
on Machine Learning, pp. 5132–5143. PMLR, 2020.

Khaled, A., Mishchenko, K., and Richtárik, P. First anal-
ysis of local GD on heterogeneous data. arXiv preprint
arXiv:1909.04715, 2019.

Khanduri, P., Zeng, S., Hong, M., Wai, H.-T., Wang, Z.,
and Yang, Z. A near-optimal algorithm for stochas-
tic bilevel optimization via double-momentum. arXiv
preprint arXiv:2102.07367, 2021.

Khodak, M., Tu, R., Li, T., Li, L., Balcan, M.-F. F., Smith,
V., and Talwalkar, A. Federated hyperparameter tuning:
Challenges, baselines, and connections to weight-sharing.
Advances in Neural Information Processing Systems, 34,
2021.

Kini, G. R., Paraskevas, O., Oymak, S., and Thram-
poulidis, C. Label-imbalanced and group-sensitive clas-
siﬁcation under overparameterization. arXiv preprint
arXiv:2103.01550, 2021.

Koneˇcn`y, J., McMahan, H. B., Ramage, D., and Richtárik,
P. Federated optimization: Distributed machine learning
for on-device intelligence. International Conference on
Learning Representations, 2018.

Li, J., Gu, B., and Huang, H. Improved bilevel model: Fast
and optimal algorithm with theoretical guarantee. arXiv
preprint arXiv:2009.00690, 2020a.

Li, M., Zhang, X., Thrampoulidis, C., Chen, J., and Oymak,
S. Autobalance: Optimized loss functions for imbal-
anced data. Advances in Neural Information Processing
Systems, 34, 2021.

Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A.,
and Smith, V. Federated optimization in heterogeneous
networks. Proceedings of Machine Learning and Systems,
2:429–450, 2020b.

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the
convergence of FedAvg on non-IID data. arXiv preprint
arXiv:1907.02189, 2019.

Lin, T., Jin, C., and Jordan, M. On gradient descent ascent
for nonconvex-concave minimax problems. In Interna-
tional Conference on Machine Learning, pp. 6083–6093.
PMLR, 2020.

Liu, H., Simonyan, K., and Yang, Y. Darts: Differentiable
architecture search. arXiv preprint arXiv:1806.09055,
2018.

Liu, M., Mroueh, Y., Ross, J., Zhang, W., Cui, X., Das, P.,
and Yang, T. Towards better understanding of adaptive
gradient algorithms in generative adversarial nets. arXiv
preprint arXiv:1912.11940, 2019.

Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J. A generic
ﬁrst-order algorithmic framework for bi-level program-
ming beyond lower-level singleton. In International Con-
ference on Machine Learning, pp. 6305–6315. PMLR,
2020.

Liu, R., Gao, J., Zhang, J., Meng, D., and Lin, Z. Investigat-
ing bi-level optimization for learning and vision from a
uniﬁed perspective: A survey and beyond. arXiv preprint
arXiv:2101.11517, 2021.

Luo, L., Ye, H., Huang, Z., and Zhang, T. Stochastic re-
cursive gradient descent ascent for stochastic nonconvex-
strongly-concave minimax problems. Advances in Neural
Information Processing Systems, 33:20566–20577, 2020.

Lv, Y., Hu, T., Wang, G., and Wan, Z. A penalty func-
tion method based on kuhn–tucker condition for solving
linear bilevel programming. Applied Mathematics and
Computation, 188(1):808–813, 2007.

Mitra, A., Jaafar, R., Pappas, G., and Hassani, H. Linear
convergence in federated learning: Tackling client het-
erogeneity and sparse gradients. Advances in Neural
Information Processing Systems, 34, 2021.

Mohri, M., Sivek, G., and Suresh, A. T. Agnostic feder-
ated learning. In International Conference on Machine
Learning, pp. 4615–4625. PMLR, 2019.

Mokhtari, A., Ozdaglar, A., and Pattathil, S. A uniﬁed
analysis of extra-gradient and optimistic gradient methods
for saddle point problems: Proximal point approach. In
International Conference on Artiﬁcial Intelligence and
Statistics, pp. 1497–1507. PMLR, 2020.

Moore, G. M. Bilevel programming algorithms for ma-
chine learning model selection. Rensselaer Polytechnic
Institute, 2010.

Nazari, P., Tarzanagh, D. A., and Michailidis, G. Dadam: A
consensus-based distributed adaptive gradient method for
online optimization. arXiv preprint arXiv:1901.09109,
2019.

Nedi´c, A. and Ozdaglar, A. Subgradient methods for saddle-
point problems. Journal of optimization theory and ap-
plications, 142(1):205–228, 2009.

Nemirovski, A. Prox-method with rate of convergence
o(1/t) for variational inequalities with lipschitz contin-
uous monotone operators and smooth convex-concave
saddle point problems. SIAM Journal on Optimization,
15(1):229–251, 2004.

Nichol, A. and Schulman, J. Reptile: a scalable metalearn-
ing algorithm. arXiv preprint arXiv:1803.02999, 2(3):4,
2018.

Maclaurin, D., Duvenaud, D., and Adams, R. Gradient-
based hyperparameter optimization through reversible
learning. In International conference on machine learn-
ing, pp. 2113–2122. PMLR, 2015.

Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Raza-
viyayn, M. Solving a class of non-convex min-max games
using iterative ﬁrst order methods. Advances in Neural
Information Processing Systems, 32, 2019.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.

Pathak, R. and Wainwright, M. J. Fedsplit: an algorithmic
framework for fast federated optimization. Advances in
Neural Information Processing Systems, 33:7057–7066,
2020.

McMahan, B., Moore, E., Ramage, D., Hampson, S.,
and y Arcas, B. A. Communication-efﬁcient learn-
ing of deep networks from decentralized data. In Pro-
ceedings of the 20th International Conference on Arti-
ﬁcial Intelligence and Statistics, AISTATS 2017, 20-22
April 2017, Fort Lauderdale, FL, USA, pp. 1273–1282,
2017. URL http://proceedings.mlr.press/
v54/mcmahan17a.html.

Pedregosa, F. Hyperparameter optimization with approxi-
mate gradient. In International conference on machine
learning, pp. 737–746. PMLR, 2016.

Raﬁque, H., Liu, M., Lin, Q., and Yang, T. Weakly-convex–
concave min–max optimization: provable algorithms and
applications in machine learning. Optimization Methods
and Software, pp. 1–35, 2021.

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Rasouli, M., Sun, T., and Rajagopal, R. Fedgan: Federated
generative adversarial networks for distributed data. arXiv
preprint arXiv:2006.07228, 2020.

Reddi, S. J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
Koneˇcn`y, J., Kumar, S., and McMahan, H. B. Adaptive
federated optimization. In International Conference on
Learning Representations, 2020.

Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.
Robust federated learning: The case of afﬁne distribu-
tion shifts. Advances in Neural Information Processing
Systems, 33:21554–21565, 2020.

Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B. Trun-
cated back-propagation for bilevel optimization. In The
22nd International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1723–1732. PMLR, 2019.

Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. Tack-
ling the objective inconsistency problem in heterogeneous
federated optimization. Advances in neural information
processing systems, 2020.

Wang, M., Liu, J., and Fang, E. Accelerating stochastic com-
position optimization. Advances in Neural Information
Processing Systems, 29, 2016.

Wang, M., Fang, E. X., and Liu, H. Stochastic composi-
tional gradient descent: algorithms for minimizing com-
positions of expected-value functions. Mathematical Pro-
gramming, 161(1-2):419–449, 2017.

Wang, S., Tuor, T., Salonidis, T., Leung, K. K., Makaya,
C., He, T., and Chan, K. Adaptive federated learning
in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 37(6):
1205–1221, 2019.

Shen, Y., Du, J., Zhao, H., Zhang, B., Ji, Z., and Gao, M.
Fedmm: Saddle point optimization for federated adversar-
ial domain adaptation. arXiv preprint arXiv:2110.08477,
2021.

Wu, Y. F., Zhang, W., Xu, P., and Gu, Q. A ﬁnite-time
analysis of two time-scale actor-critic methods. Advances
in Neural Information Processing Systems, 33:17617–
17628, 2020.

Shi, C., Lu, J., and Zhang, G. An extended kuhn–tucker
approach for linear bilevel programming. Applied Mathe-
matics and Computation, 162(1):51–63, 2005.

Xie, J., Zhang, C., Zhang, Y., Shen, Z., and Qian, H. A
federated learning framework for nonconvex-pl minimax
problems. arXiv preprint arXiv:2105.14216, 2021.

Sinha, A., Malo, P., and Deb, K. A review on bilevel opti-
mization: from classical to evolutionary approaches and
applications. IEEE Transactions on Evolutionary Com-
putation, 22(2):276–295, 2017.

Yan, Y., Xu, Y., Lin, Q., Liu, W., and Yang, T. Optimal
epoch stochastic gradient descent ascent methods for min-
max optimization. Advances in Neural Information Pro-
cessing Systems, 33:5789–5800, 2020.

Stich, S. U. Local SGD converges fast and communicates
little. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/
forum?id=S1g2JnRcFX.

Stich, S. U. and Karimireddy, S. P. The error-feedback
framework: Better rates for SGD with delayed gradi-
ents and compressed communication. arXiv preprint
arXiv:1909.05350, 2019.

Thekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S.
Efﬁcient algorithms for smooth minimax optimization.
arXiv preprint arXiv:1907.01543, 2019.

Tran Dinh, Q., Liu, D., and Nguyen, L. Hybrid variance-
reduced sgd algorithms for minimax problems with
nonconvex-linear function. Advances in Neural Infor-
mation Processing Systems, 33:11096–11107, 2020.

Yang, J., Kiyavash, N., and He, N. Global convergence and
variance reduction for a class of nonconvex-nonconcave
minimax problems. Advances in Neural Information
Processing Systems, 33:1153–1165, 2020.

Yoon, T. and Ryu, E. K. Accelerated algorithms for smooth
convex-concave minimax problems with o (1/kˆ 2) rate
on squared gradient norm. In International Conference
on Machine Learning, pp. 12098–12109. PMLR, 2021.

Yu, H., Yang, S., and Zhu, S. Parallel restarted SGD with
faster convergence and less communication: Demystify-
ing why model averaging works for deep learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 33, pp. 5693–5700, 2019.

Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. Paral-
lelized stochastic gradient descent. In Advances in neural
information processing systems, pp. 2595–2603, 2010.

Wang,

J. and Joshi, G.

Cooperative SGD: A
uniﬁed framework for the design and analysis of
communication-efﬁcient SGD algorithms. arXiv preprint
arXiv:1808.07576, 2018.

APPENDIX
FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

The appendix is organized as follows: Section A introduces the LFEDNEST algorithm. Section B discusses the related
work. We provide all details for the proof of the main theorems in Sections C, D, E, and F for federated bilevel, minimax,
compositional, and single-level optimization, respectively. In Section G, we state a few auxiliary technical lemmas. Finally,
in Section H, we provide the detailed parameters of our numerical experiments (Section 4) and then introduce further
experiments.

A. LFEDNEST

Implementing FEDINN and FEDOUT naively by using the global direct and indirect gradients and sending the local
information to the server that would then calculate the global gradients leads to a communication and space complexity of
which can be prohibitive for large-sized d1 and d2. One can consider possible local variants of FEDINN and FEDOUT tailore
to such scenarios. Each of the possible algorithms (See Table 2) can then either use the global gradient or only the local
gradient, either use a SVRG or SGD.

Algorithm 5 x+ = LFEDOUT
LFEDOUT
LFEDOUT (x, y, α) for stochastic bilevel , minimax , and compositional problems

1: xi,0 = x and αi ∈ (0, α] for each i ∈ S.
2: Choose N ∈ N (the number of terms of Neumann series).
3: for i ∈ S in parallel do
4:
5:

Select N (cid:48) ∈ {0, . . . , N − 1} UAR.

for ν = 0, . . . , τi − 1 do

6:

7:

hi,ν = ∇xfi(xi,ν , y; ξi,ν ) − N
(cid:96)g,1

∇2

xygi(xi,ν , y; ζi,ν )

N (cid:48)
(cid:81)
n=1

(cid:0)I − 1
(cid:96)g,1

∇2

ygi(xi,ν , y; ζi,n)(cid:1)∇xfi(yi,ν , y, ξi,ν )

hi,ν = ∇xfi(xi,ν , y; ξi,ν )

hi,ν = ∇ri(xi,ν ; ζi,ν )(cid:62)∇fi(yi,ν ; ξi,ν )
xi,ν+1 = xi,ν − αihi,ν

8:
9:
10:
11: end for
12: x+ = |S|−1 (cid:80)

end for

i∈S xi,τi

Algorithm 6 y+ = LFEDINN
LFEDINN
LFEDINN (x, y, β) for stochastic bilevel , minimax , and compositional problems

for ν = 0, . . . , τi − 1 do

1: yi,0 = y and βi ∈ (0, β] for each i ∈ S.
2: for i ∈ S in parallel do
3:
4:
5:
6:
7: end for
8: y+ = |S|−1 (cid:80)

end for

i∈S yi,τi

qi,ν = ∇ygi(x, yi,ν ; ζi,ν ) qi,ν = −∇yfi(x, yi,ν ; ξi,ν ) qi,ν = yi,ν − ri(x; ζi,ν )
yi,ν+1 = yi,ν − βiqi,ν

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

deﬁnition

properties

outer
optimizer

inner
optimizer

global
outer gradient

global
IHGP

global
inner gradient

# communication
rounds

FEDNEST
FEDNEST
FEDNEST

LFEDNEST
LFEDNEST
LFEDNEST

FEDNEST
FEDNESTSGD
FEDNEST

LFEDNEST
LFEDNESTSVRG
LFEDNEST

Algorithm 2
(SVRG on x)
Algorithm 5
(SGD on x)
Algorithm 2
(SVRG on x)
Algorithm 5
(SGD on x)

Algorithm 4
(SVRG on y)
Algorithm 6
(SGD on y)
Algorithm 6
(SGD on y)
Algorithm 4
(SVRG on y)

yes

no

yes

no

yes

no

yes

no

yes

no

no

yes

2T + N + 3

T + 1

T + N + 3

2T + 1

Table 2: Deﬁnition of studied algorithms by using inner/outer optimization algorithms and server updates and resulting
properties of these algorithms. T and N denote the number of inner iterations and terms of Neumann series, respectively.

B. Related Work

We provide an overview of the current literature on non-federated nested (bilevel, minmimax, and compositional) optimiza-
tion and federated learning.

B.1. Bilevel Optimization

A broad collection of algorithms have been proposed to solve bilevel nonlinear programming problems. Aiyoshi & Shimizu
(1984); Edmunds & Bard (1991); Al-Khayyal et al. (1992); Hansen et al. (1992); Shi et al. (2005); Lv et al. (2007); Moore
(2010) reduce the bilevel problem to a single-level optimization problem using for example the Karush-Kuhn-Tucker (KKT)
conditions or penalty function methods. A similar idea was also explored in Khodak et al. (2021) where the authors provide
a reformulation of the hyperparameter optimization (bilevel objective) into a single-level objective and develop a federated
online method to solve it. However, the reduced single-level problem is usually difﬁcult to solve (Sinha et al., 2017).

In comparison, alternating gradient-based approaches designed for the bilevel problems are more attractive due to their
simplicity and effectiveness. This type of approaches estimate the hypergradient ∇f (x) for iterative updates, and are
generally divided to approximate implicit differentiation (AID) and iterative differentiation (ITD) categories. ITD-based
approaches (Maclaurin et al., 2015; Franceschi et al., 2017; Finn et al., 2017; Grazzi et al., 2020) estimate the hypergradient
∇f (x) in either a reverse (automatic differentiation) or forward manner. AID-based approaches (Pedregosa, 2016; Grazzi
et al., 2020; Ghadimi & Wang, 2018) estimate the hypergradient via implicit differentiation which involves solving a linear
system. Our algorithms follow the latter approach.

Theoretically, bilevel optimization has been studied via both asymptotic and non-asymptotic analysis (Franceschi et al.,
2018; Liu et al., 2020; Li et al., 2020a; Shaban et al., 2019; Ghadimi & Wang, 2018; Ji et al., 2021; Hong et al., 2020). In
particular, (Franceschi et al., 2018) provided the asymptotic convergence of a backpropagation-based approach as one of
ITD-based algorithms by assuming the inner problem is strongly convex. (Shaban et al., 2019) gave a similar analysis for a
truncated backpropagation approach. Non-asymptotic complexity analysis for bilevel optimization has also been explored.
Ghadimi & Wang (2018) provided a ﬁnite-time convergence analysis for an AID-based algorithm under three different
loss geometries, where f (·) is either strongly convex, convex or nonconvex, and g(x, ·) is strongly convex. (Ji et al., 2021)
provided an improved non-asymptotic analysis for AID- and ITD-based algorithms under the nonconvex-strongly-convex
geometry. (Ji & Liang, 2021) provided the ﬁrst-known lower bounds on complexity as well as tighter upper bounds. When
the objective functions can be expressed in an expected or ﬁnite-time form, (Ghadimi & Wang, 2018; Ji et al., 2021; Hong
et al., 2020) developed stochastic bilevel algorithms and provided the non-asymptotic analysis. (Chen et al., 2021a) provided
a tighter analysis of SGD for stochastic bilevel problems. (Chen et al., 2021b; Guo et al., 2021; Khanduri et al., 2021; Ji
et al., 2020a; Huang & Huang, 2021; Dagréou et al., 2022) studied accelerated SGD, SAGA, momentum, and adaptive-type
bilevel optimization methods. More results can be found in the recent review paper (Liu et al., 2021) and references therein.

B.1.1. MINIMAX OPTIMIZATION

Minimax optimization has a long history dating back to (Brown, 1951). Earlier works focused on the deterministic convex-
concave regime (Nemirovski, 2004; Nedi´c & Ozdaglar, 2009). Recently, there has emerged a surge of studies of stochastic

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

minimax problems. The alternating version of the gradient descent ascent (SGDA) has been studied by incorporating the
idea of optimism (Daskalakis & Panageas, 2018; Gidel et al., 2018; Mokhtari et al., 2020; Yoon & Ryu, 2021). (Raﬁque
et al., 2021; Thekumparampil et al., 2019; Nouiehed et al., 2019; Lin et al., 2020) studied SGDA in the nonconvex-strongly
concave setting. Speciﬁcally, the O((cid:15)−2) sample complexity has been established in (Lin et al., 2020) under an increasing
batch size O((cid:15)−1). Chen et al. (2021a) provided the O((cid:15)−2) sample complexity under an O(1) constant batch size. In the
same setting, accelerated GDA algorithms have been developed in (Luo et al., 2020; Yan et al., 2020; Tran Dinh et al.,
2020). Going beyond the one-side concave settings, algorithms and their convergence analysis have been studied for
nonconvex-nonconcave minimax problems with certain benign structure; see e.g., (Gidel et al., 2018; Liu et al., 2019; Yang
et al., 2020; Diakonikolas et al., 2021; Barazandeh et al., 2021b). A comparison of our results with prior work can be found
in Table 1.

B.1.2. COMPOSITIONAL OPTIMIZATION

Stochastic compositional gradient algorithms (Wang et al., 2017; 2016) can be viewed as an alternating SGD for the special
compositional problem. However, to ensure convergence, the algorithms in (Wang et al., 2017; 2016) use two sequences of
variables being updated in two different time scales, and thus the iteration complexity of (Wang et al., 2017) and (Wang
et al., 2016) is worse than O((cid:15)−2) of the standard SGD. Our work is closely related to ALSET (Chen et al., 2021a), where
an O((cid:15)−2) sample complexity has been established in a non-FL setting.

B.2. Federated Learning

FL involves learning a centralized model from distributed client data. Although this centralized model beneﬁts from all
client data, it raises several types of issues such as generalization, fairness, communication efﬁciency, and privacy (Mohri
et al., 2019; Stich, 2019; Yu et al., 2019; Wang & Joshi, 2018; Stich & Karimireddy, 2019; Basu et al., 2019; Nazari
et al., 2019; Barazandeh et al., 2021a). FEDAVG (McMahan et al., 2017) can tackle some of these issues such as high
communication costs. Many variants of FEDAVG have been proposed to tackle other emerging issues such as convergence
and client drift. Examples include adding a regularization term in the client objectives towards the broadcast model (Li et al.,
2020b), proximal splitting (Pathak & Wainwright, 2020; Mitra et al., 2021), variance reduction (Karimireddy et al., 2020;
Mitra et al., 2021), dynamic regularization (Acar et al., 2021), and adaptive updates (Reddi et al., 2020). When clients are
homogeneous, FEDAVG is closely related to local SGD (Zinkevich et al., 2010), which has been analyzed by many works
(Stich, 2019; Yu et al., 2019; Wang & Joshi, 2018; Stich & Karimireddy, 2019; Basu et al., 2019).

In order to analyze FEDAVG in heterogeneous settings, (Li et al., 2020b; Wang et al., 2019; Khaled et al., 2019; Li et al., 2019)
derive convergence rates depending on the amount of heterogeneity. They showed that the convergence rate of FEDAVG gets
worse with client heterogeneity. By using control variates to reduce client drift, the SCAFFOLD method (Karimireddy
et al., 2020) achieves convergence rates that are independent of the amount of heterogeneity. Relatedly, FEDNOVA (Wang
et al., 2020) and FEDLIN (Mitra et al., 2021) provided the convegence of their methods despite arbitrary local objective and
systems heterogeneity. In particular, (Mitra et al., 2021) showed that FEDLIN guarantees linear convergence to the global
minimum of deterministic objective, despite arbitrary objective and systems heterogeneity. As explained in the main body,
our algorithms critically leverage these ideas after identifying the additional challenges that client drift brings to federated
bilevel settings.

B.2.1. FEDERATED MINIMAX LEARNING

A few recent studies are devoted to federated minimax optimization (Rasouli et al., 2020; Reisizadeh et al., 2020; Deng
et al., 2020; Hou et al., 2021). In particular, (Reisizadeh et al., 2020) consider minimax problem with inner problem
satisfying PL condition and the outer one being either nonconvex or satisfying PL. However, the proposed algorithm
only communicates x to the server. Xie et al. (2021) consider a general class of nonconvex-PL minimax problems in the
cross-device federated learning setting. Their algorithm performs multiple local update steps on a subset of active clients
in each round and leverages global gradient estimates to correct the bias in local update directions. Deng & Mahdavi
(2021) studied federated optimization for a family of smooth nonconvex minimax functions. Shen et al. (2021) proposed a
distributed minimax optimizer called FEDMM, designed speciﬁcally for the federated adversary domain adaptation problem.
Hou et al. (2021) proposed a SCAFFOLD saddle point algorithm (SCAFFOLD-S) for solving strongly convex-concave
minimax problems in the federated setting. To the best of our knowledge, all the aforementioned developments require a
bound on the heterogeneity of the local functions, and do not account for the effects of systems heterogeneity which is also
a key challenge in FL. In addition, our work proposes the ﬁrst alternating federated SVRG-type algorithm for minimax

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

problems with iteration complexity that matches to the non-federated setting (see, Table 1).

C. Proof for Federated Bilevel Optimization

Throughout the proof, we will use F k,t
i,ν to denote the ﬁltration that captures all the randomness up to the ν-th local step
of client i in inner round t and outer round k. With a slight abuse of notation, F k,t
i,−1 is to be interpreted as F k,t, ∀i ∈ S.
For simplicity, we remove subscripts k and t from the deﬁnition of stepsize and model parameters. For example, x and x+
denote xk and xk+1, respectively.

We analyze the convergence of FEDNEST for general setting: starting from a common global model xi,0 = x, each client i
performs τi local steps (in parallel):

xi,ν+1 = xi,ν − αi

(cid:0)h(x, y+) − hi(x, y+) + hi(xi,ν, y+)(cid:1) ,

(15)

and then the server aggregates local models via x+ = |S|−1 (cid:80)
h(x, y) := |S|−1 (cid:80)
i (x, y) + hD
direction follows similarly. Note that from (15), we have

i∈S hI

i∈S xi,τi. Here, αi ∈ (0, α] is the local stepsize, and
i (x, y). The proof for the special case (6)–(7) where hi,ν is used as a search

x+ = x −

= x −

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

αi

αi

τi−1
(cid:88)

ν=0
τi−1
(cid:88)

i=1

ν=0

(cid:0)h(x, y+) − hi(x, y+) + hi(xi,ν, y+)(cid:1)

hi(xi,ν, y+).

(cid:104)
¯hi(xi,ν, y+) := E

hi(xi,ν, y+)|Fi,ν−1

(cid:105)
.

(16)

(17)

We further set

Proof of Lemma 2.1

Proof. Given x ∈ Rd1, the optimality condition of the inner problem in (1) is ∇yg(x, y) = 0. Now, since
∇x (∇yg(x, y)) = 0, we obtain

0 =

m
(cid:88)

j=1

(cid:0)∇2

xygj (x, y∗(x)) + ∇y∗(x)∇2

ygj (x, y∗(x))(cid:1) ,

which implies

∇y∗(x) = −

(cid:18) m
(cid:88)

i=1

∇2

xygi (x, y∗(x))

(cid:19)(cid:18) m
(cid:88)

i=1

∇2

ygj(x, y∗(x))

(cid:19)−1

.

The results follows from a simple application of the chain rule to f as follows:

∇f (x, y∗(x)) = ∇xf (x, y∗(x)) + ∇y∗(x)∇yf (x, y∗(x)) .

Proof of Lemma 2.2

Proof. By independency of N (cid:48), ζi,n, and Sn, and under Assumption B, we have

EW

(cid:105)

(cid:104)
(cid:99)Hy

= EW





N
(cid:96)g,1

N (cid:48)
(cid:89)



I −

1
(cid:96)g,1|Sn|

|Sn|
(cid:88)

i=1





∇2

ygi(x, y; ζi,n)





n=1

Eζ


ES1:N (cid:48)

= EN (cid:48)





N
(cid:96)g,1

N (cid:48)
(cid:89)

n=1



I −

1
(cid:96)g,1|Sn|

|Sn|
(cid:88)

i=1

∇2

ygi(x, y; ζi,n)

















= 1
(cid:96)g,1

N −1
(cid:88)

n=0

(cid:104)
I − 1
(cid:96)g,1

∇2

yg(x, y)

(cid:105)n

,

(18)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

where the last equality follows from the uniform distribution of N (cid:48).

Note that since I (cid:23) 1
(cid:96)g,1

∇2

ygi (cid:23) µg
(cid:96)g,1

for all i ∈ [m] due to Assumption A, we have

EW

(cid:105)
(cid:104)
(cid:107)(cid:99)Hy(cid:107)

≤

≤

N (cid:48)
(cid:89)





n=1

(cid:20)

1 −

N
(cid:96)g,1

N
(cid:96)g,1

EW

EN (cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I −

1
(cid:96)g,1|Sn|

|Sn|
(cid:88)

i=1

∇2

ygi(x, y; ζi,n)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:21)N (cid:48)

µg
(cid:96)g,1

=

1
(cid:96)g,1

N −1
(cid:88)

n=0

(cid:20)

1 −

(cid:21)n

µg
(cid:96)g,1

≤

1
µg

.

The reminder of the proof is similar to (Ghadimi & Wang, 2018).

The following lemma extends (Ghadimi & Wang, 2018, Lemma 2.2) and (Chen et al., 2021a, Lemma 2) to the ﬁnite-sum
problem (1). Proofs follow similarly by applying their analysis to the inner & outer functions (fi, gi), ∀i ∈ S.

Lemma C.1. Under Assumptions A and B, for all x1, x2:

(cid:107)∇f (x1) − ∇f (x2)(cid:107) ≤ Lf (cid:107)x1 − x2(cid:107),
(cid:107)y∗(x1) − y∗(x2)(cid:107) ≤ Ly(cid:107)x1 − x2(cid:107),
(cid:107)∇y∗(x1) − ∇y∗(x2)(cid:107) ≤ Lyx(cid:107)x1 − x2(cid:107).

Also, for all i ∈ S, ν ∈ {0, . . . , τi − 1}, x1, x2, and y, we have:

(cid:107) ¯∇fi(x1, y) − ¯∇fi(x1, y∗(x1))(cid:107) ≤ Mf (cid:107)y∗(x1) − y(cid:107),
(cid:107) ¯∇fi(x2, y) − ¯∇fi(x1, y)(cid:107) ≤ Mf (cid:107)x2 − x1(cid:107),

E (cid:2)(cid:107)¯hi(xi,ν, y) − hi(xi,ν, y)(cid:107)2(cid:3) ≤ ˜σ2
f ,
(cid:3) ≤ ˜D2
E (cid:2)(cid:107)hi(xi,ν, y+)(cid:107)2|Fi,ν−1
f .

Here,

Ly :=

Lyx :=

= O(κg),

(cid:96)g,1
µg
(cid:96)g,2 + (cid:96)g,2Ly
µg
(cid:96)g,1(cid:96)f,1
µg

+

+

(cid:96)g,1
µ2
g
(cid:96)f,0
µg

Mf := (cid:96)f,1 +

Lf := (cid:96)f,1 +

(cid:96)g,1((cid:96)f,1 + Mf )
µg

+

(cid:96)f,0
µg

(cid:16)

(cid:96)g,2 + (cid:96)g,2Ly

(cid:17)

= O(κ3

g),

(cid:18)

(cid:96)g,2 +

(cid:96)g,1(cid:96)g,2
µg

(cid:18)

(cid:96)g,2 +

(cid:19)

= O(κ2

g),

(cid:19)

(cid:96)g,1(cid:96)g,2
µg

= O(κ3

g),

(19a)

(19b)

(19c)

(19d)

(19e)

(19f)

(19g)

(20)

f := σ2
˜σ2

f +

(cid:16)

3
µ2
g

(σ2

f + (cid:96)2

f,0)(σ2

g,2 + 2(cid:96)2

g,1) + σ2

f (cid:96)2
g,1

(cid:17)

,

˜D2

f :=

(cid:18)

(cid:96)f,0 +

(cid:96)g,1
µg

(cid:96)f,1 + (cid:96)g,1(cid:96)f,1

(cid:19)2

1
µg

+ ˜σ2

f = O(κ2

g),

where the other constants are provided in Assumptions A and B.

C.1. Descent of Outer Objective

The following lemma characterizes the descent of the outer objective.

Lemma C.2 (Descent Lemma). Suppose Assumptions A and B hold. Further, assume τi ≥ 1 and αi = α/τi, ∀i ∈ S for

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

some positive constant α. Then, FEDOUT guarantees:

E (cid:2)f (x+)(cid:3) − E [f (x)] ≤ −

−

α
2

α
2

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) +

(1 − αLf ) E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

α2Lf
2
m
(cid:88)

i=1

˜σ2
f

1
τi

τi−1
(cid:88)

ν=0

2

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)



(21)

(cid:32)

b2 + M 2
f

+

3α
2

E (cid:2)(cid:107)y+ − y∗(x)(cid:107)2(cid:3) +

M 2
f
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

(cid:33)

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3)

.

Proof. Using (16) and the Lipschitz property of ∇f in Lemma C.1, we have

E (cid:2)f (x+)(cid:3) − E [f (x)] ≤ E (cid:2)(cid:104)x+ − x, ∇f (x)(cid:105)(cid:3) +

Lf
2

E (cid:2)(cid:107)x+ − x(cid:107)2(cid:3)

(cid:43)(cid:35)

(cid:34)(cid:42)

= −E

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

+

Lf
2

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

(cid:13)
(cid:13)
hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2

 .

hi(xi,ν, y+), ∇f (x)

(22)

In the following, we bound each term on the right hand side (RHS) of (22). For the ﬁrst term, we have

(cid:34)(cid:42)

−E

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

hi(xi,ν, y+), ∇f (x)

= − E

(cid:43)(cid:35)

(cid:34)

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

E (cid:2)(cid:10)hi(xi,ν, y+), ∇f (x)(cid:11) | Fi,ν−1

(cid:35)
(cid:3)

(cid:34)(cid:42)

= − E

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

¯hi(xi,ν, y+), ∇f (x)

(cid:43)(cid:35)

= −

+

E

E

α
2

α
2









(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

i=1

1
τi

1
τi

τi−1
(cid:88)

ν=0

τi−1
(cid:88)

ν=0

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2

 −

(23)

(cid:104)

(cid:107)∇f (x)(cid:107)2(cid:105)

E

α
2

(cid:13)
(cid:13)
¯hi(xi,ν, y+) − ∇f (x)
(cid:13)
(cid:13)
(cid:13)

2

 ,

where the ﬁrst equality follows from the law of total expectation; the second equality uses the fact that ¯hi(xi,ν, y+) =
E [hi(xi,ν, y+)|Fi,ν−1]; and the last equality is obtained from our assumption αi = α/τi, ∀i ∈ S.

Next, we bound the last term in (23). Note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

¯hi(xi,ν, y+) − ∇f (x)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

(¯hi(xi,ν, y+) − ¯∇fi(x, y+))

ν=0

+

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

¯∇fi(x, y+) − ∇f (x)

(cid:13)
2
(cid:13)
(cid:13)

i=1

ν=0

≤ 3

1
τi

1
m

m
(cid:88)

τi−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
m
(cid:13)
(cid:13) ¯∇f (x, y+) − ∇f (x)(cid:13)
+ 3 (cid:13)
2
(cid:13)

τi−1
(cid:88)

m
(cid:88)

1
τi

+ 3

ν=0

i=1

,

(¯hi(xi,ν, y+) − ¯∇fi(xi,ν, y+))

( ¯∇fi(xi,ν, y+) − ¯∇fi(x, y+))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

where the inequality uses Lemma G.1.

Hence,

(cid:34)

E

(cid:107)

1
m

≤ 3b2 +

m
(cid:88)

i=1
3M 2
f
m

1
τi

¯hi(xi,ν, y+) − ∇f (x)(cid:107)2

(cid:35)

τi−1
(cid:88)

ν=0

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 3M 2

f

E (cid:2)(cid:107)y+ − y∗(x)(cid:107)2(cid:3) ,

where the inequality uses Lemmas 2.2 and C.1.

Substituting (24) into (23) yields

(cid:34)(cid:42)

− E

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

hi(xi,ν, y+), ∇f (x)

(cid:43)(cid:35)

1
m

(cid:34)

≤ −

α
2

E

(cid:107)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

(cid:35)

¯hi(xi,ν, y+)(cid:107)2

−

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3)

α
2

+

3α
2

(cid:16)

b2 + M 2
f

E (cid:2)(cid:107)y+ − y∗(x)(cid:107)2(cid:3) +

M 2
f
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) (cid:17)
.

Next, we bound the second term on the RHS of (22). Observe that

(cid:13)
(cid:13)
hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2



τi−1
(cid:88)

ν=0

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

= α2E

≤ α2E

m
(cid:88)

αi

i=1
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:34)

(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

m
(cid:88)

i=1

1
τi

1
τi

τi−1
(cid:88)

ν=0
τi−1
(cid:88)

ν=0

(cid:0)hi(xi,ν, y+) − ¯hi(xi,ν, y+) + ¯hi(xi,ν, y+)(cid:1)

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯hi(xi,ν, y+)(cid:13)
2
(cid:13)

(cid:35)

+ α2 ˜σ2
f ,

(24)

(25)

(26)

where the inequality follows from Lemmas G.3 and C.1.

Plugging (26) and (25) into (22) completes the proof.

C.2. Error of FEDINN

The following lemma establishes the progress of FEDINN. It should be mentioned that the assumption on βi, ∀i ∈ S is
identical to the one listed in (Mitra et al., 2021, Theorem 4).

Lemma C.3 (Error of FEDINN). Suppose Assumptions A and B hold. Further, assume

where 0 < β < min (cid:0)1/(6(cid:96)g,1), 1(cid:1) and α is some positive constant. Then, FEDINN guarantees:

τi ≥ 1, αi =

α
τi

,

βi =

β
τi

,

∀i ∈ S,

E

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

2(cid:105)

(cid:18)

≤

1 −

(cid:19)T

(cid:107)y − y(cid:63)(x)(cid:107)2(cid:105)
(cid:104)

E

+ 25T β2σ2

g,1,

and

βµg
2


E

(cid:104)(cid:13)
(cid:13)y+ − y(cid:63)(x+)(cid:13)
(cid:13)

2(cid:105)

≤ a1(α)E

+ a2(α)E

τi−1
(cid:88)



1
τi

1
m

m
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

i=1

ν=0
2(cid:105)

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2



+ a3(α)˜σ2
f .

(27a)

(27b)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Here,

for any η > 0.

Proof. Note that

a1(α) := L2

yα2 +

Lyα
4Mf

+

a2(α) := 1 + 4Mf Lyα +

a3(α) := α2L2

y +

Lyxα2
2η

,

,

Lyxα2
2η
ηLyx ˜D2
2

f α2

,

E (cid:2)(cid:107)y+ − y∗(x+)(cid:107)2(cid:3) =E (cid:2)(cid:107)y+ − y∗(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)y∗(x+) − y∗(x)(cid:107)2(cid:3)

+2E (cid:2)(cid:104)y+ − y∗(x), y∗(x) − y∗(x+)(cid:105)(cid:3) .

Next, we upper bound each term on the RHS of (29).
Bounding the ﬁrst term in (29):
From (Mitra et al., 2021, Theorem 4), for all t ∈ {0, . . . , T − 1}, we obtain

E[(cid:107)yt+1 − y(cid:63)(x)(cid:107)

2

] ≤

(cid:18)

1 −

(cid:19)

βµg
2

2
E[(cid:107)yt − y(cid:63)(x)(cid:107)

] + 25β2σ2

g,1,

which together with our setting y+ = yT implies

E[(cid:107)y+ − y∗(x)(cid:107)2] ≤

(cid:19)T

(cid:18)

1 −

βµg
2

E[(cid:107)y − y(x∗)(cid:107)2] + 25T β2σ2

g,1.

Bounding the second term in (29):
By similar steps as in (26), we have

E (cid:2)(cid:107)y∗(x+) − y∗(x)(cid:107)2(cid:3) ≤ L2

y

E

≤ L2
y

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

αi

αi

τi−1
(cid:88)

ν=0

τi−1
(cid:88)

i=1

ν=0

2

(cid:13)
(cid:13)
hi(x, y+)
(cid:13)
(cid:13)
(cid:13)

¯hi(xi,ν, y+)



2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ α2L2

y ˜σ2
f ,

where the inequalities are obtained from Lemmas C.1 and G.3.

Bounding the third term in (29):
Observe that

E (cid:2)(cid:104)y+ − y∗(x), y∗(x) − y∗(x+)(cid:105)(cid:3) = −E (cid:2)(cid:104)y+− y(cid:63)(x), ∇y(cid:63)(x)(x+− x)(cid:105)(cid:3)

− E (cid:2)(cid:104)y+− y(cid:63)(x), y(cid:63)(x+) − y(cid:63)(x) − ∇y(cid:63)(x)(x+− x)(cid:105)(cid:3) .

For the ﬁrst term on the R.H.S. of the above equality, we have

−E[(cid:104)y+− y(cid:63)(x), ∇y(cid:63)(x)(x+− x)(cid:105)] = − E

(cid:104)y+− y(cid:63)(x),

(cid:34)

1
m

(cid:34)
(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

≤E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)
(cid:13)

≤LyE

1
m
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇y(cid:63)(x)

1
m

m
(cid:88)

i=1

αi

≤2γE

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

2(cid:105)

+

L2

yα2
8γ

∇y(cid:63)(x)

m
(cid:88)

αi

τi−1
(cid:88)

¯hi(xi,ν, y+)(cid:105)

(cid:35)

i=1

m
(cid:88)

αi

ν=0
τi−1
(cid:88)

i=1

ν=0

(cid:35)

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

τi−1
(cid:88)

ν=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



E

(cid:35)

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2

 ,

(28)

(29)

(30)

(31)

(32)

(33)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

where the ﬁrst equality uses the fact that ¯hi(xi,ν, y+) = E [hi(xi,ν, y+)|Fi,ν−1]; the second inequality follows from
Lemma C.1; and the last inequality is obtained from the Young’s inequality such that ab ≤ 2γa2 + b2
8γ .

Further, using Lemma C.1, we have

−E[(cid:104)y+− y(cid:63)(x), y(cid:63)(x+) − y(cid:63)(x) − ∇y(cid:63)(x)(x+− x)(cid:105)]

≤ E (cid:2)(cid:13)
Lyx
2

≤

(cid:13)y+− y(cid:63)(x)(cid:107)(cid:107)y(cid:63)(x+) − y(cid:63)(x) − ∇y(cid:63)(x)(x+− x)(cid:13)
(cid:3)
(cid:13)
E

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

(cid:13)x+− x(cid:13)
(cid:13)
(cid:13)

2(cid:105)

,

(34)

where the inequality follows from Lemma C.1.

Note that F0 = Fi,0 for all i ∈ S. This together with (16) implies that

E

≤

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

(cid:13)x+− x(cid:13)
2 (cid:13)
(cid:13)

2(cid:105)

m
(cid:88)

1
m

i=1
≤ α2 ˜D2
f

E

E

τi−1
(cid:88)

α2
τi
(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

ν=0

2(cid:105)

,

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
2 E
(cid:13)

(cid:104)(cid:13)
(cid:13)hi(xi,ν, y+)(cid:13)
2
(cid:13)

| Fi,τi−1

(cid:105)(cid:105)

(35)

where the last inequality uses Lemma C.1.
Note also that for any η > 0, we have 1 ≤ η

2 + 1

2η . Combining this inequality with (34) and using (35) give

−E[(cid:104)y+− y(cid:63)(x), y(cid:63)(x+) − y(cid:63)(x) − ∇y(cid:63)(x)(x+− x)(cid:105)]

≤

≤

≤

E

E

Lyx
2
ηLyx
4
ηLyx ˜D2
4

(cid:13)x+− x(cid:13)
(cid:13)
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)
(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

(cid:13)x+− x(cid:13)
(cid:13)
(cid:13)

2(cid:105)

+

Lyx
4η

E

(cid:104)(cid:13)
(cid:13)x+− x(cid:13)
(cid:13)

2(cid:105)

f α2

E

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

2(cid:105)

+

Lyxα2
4η

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

(cid:13)
(cid:13)
¯hi(xi,ν, y+)
(cid:13)
(cid:13)
(cid:13)

2

 +

(36)

Lyxα2
4η

˜σ2
f ,

where the last inequality uses (35) and Lemma 2.2.

Let γ = Mf Lyα. Plugging (36) and (33) into (32), we have

E[(cid:104)y+ − y∗(x), y∗(x) − y∗(x+)(cid:105)] ≤

2γ +

(cid:32)

(cid:33)

ηLyx ˜D2
f
4

α2

E

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

2(cid:105)

L2

yα2
8γ

+

Lyxα2
4η

(cid:33)

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

+

(cid:32)

=

2Mf Lyα +

ηLyx ˜D2
f
4

α2

1
m

(cid:33)

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

¯hi(xi,ν, y+)

E

(cid:104)(cid:13)
(cid:13)y+− y(cid:63)(x)(cid:13)
(cid:13)

2(cid:105)

2

 +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Lyxα2
4η

˜σ2
f

(37)

+

(cid:18) Lyα
8Mf

+

Lyxα2
4η

(cid:19)

E

Substituting (37), (31), and (30) into (29) completes the proof.

C.3. Drifting Errors of FEDOUT





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

¯hi(xi,ν, y+)

2

 +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Lyxα2
4η

˜σ2
f .

The following lemma provides a bound on the drift of each xi,ν from x for stochastic nonconvex bilevel problems.

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Lemma C.4 (Drifting Error of FEDOUT). Suppose Assumptions A and B hold. Further, assume τi ≥ 1 and αi ≤
1/(5Mf τi), ∀i ∈ S. Then, for each i ∈ S and ∀ν ∈ {0, . . . , τi − 1}, FEDOUT guarantees:

E

(cid:107)xi,ν − x(cid:107)2(cid:105)
(cid:104)

(cid:0)M 2

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1)

≤ 36τ 2

i α2
i
i ˜σ2
+ 27τiα2
f .

f

Proof. The result trivially holds for τi = 1. Let τi > 1 and deﬁne

vi,ν := ¯hi(xi,ν, y+) − ¯∇fi(xi,ν, y+) − ¯hi(x, y+)
+ ¯∇fi(x, y+) + ¯h(x, y+) − ¯∇f (x, y+),
wi,ν := hi(xi,ν, y+) − ¯hi(xi,ν, y+) + ¯hi(x, y+)
− hi(x, y+) + h(x, y+) − ¯h(x, y+),

zi,ν := ¯∇fi(xi,ν, y+) − ¯∇fi(x, y+) + ¯∇f (x, y+) − ∇f (x) + ∇f (x).

One will notice that

vi,ν + wi,ν + zi,ν = hi(xi,ν, y+) − hi(x, y+) + h(x, y+).

Hence, from Algorithm 2, for each i ∈ S, and ∀ν ∈ {0, . . . , τi − 1}, we have

xi,ν+1 − x = xi,ν − x − αi(vi,ν + wi,ν + zi,ν),

which implies that

E (cid:2)(cid:107)xi,ν+1 − x(cid:107)2(cid:3) = E (cid:2)(cid:107)xi,ν − x − αi(vi,ν + zi,ν)(cid:107)2(cid:3) + α2

E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3)
− 2E [E [(cid:104)xi,ν − x − αi(vi,ν + zi,ν), αiwi,ν(cid:105) | Fi,ν−1]]
E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3) .
= E (cid:2)(cid:107)xi,ν − x − αi(vi,ν + zi,ν)(cid:107)2(cid:3) + α2

i

i

Here, the last equality uses Lemma G.3 since E[wi,ν|Fi,ν−1] = 0, by deﬁnition.

From Lemmas G.1, 2.2, and C.1, for vi,ν, wi,ν, and zi,ν deﬁned in (39), we have

(cid:107)vi,ν(cid:107)2(cid:105)
(cid:104)
E

≤ 3E

(cid:104) (cid:13)
(cid:13) ¯∇fi(xi,ν, y+) − ¯hi(xi,ν, y+)(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)¯hi(x, y+) − ¯∇fi(x, y+)(cid:13)
2
(cid:13)

≤ 9b2,

+ (cid:13)

(cid:13) ¯∇f (x, y+) − ¯h(x, y+)(cid:13)
(cid:13)

2 (cid:105)

(cid:104)
E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3) ≤ 3E
(cid:107)hi(xi,ν, y+) − ¯hi(xi,ν, y+)(cid:107)2

+ (cid:107)¯hi(x, y+) − hi(x, y+)(cid:107)2 + (cid:107)h(x, y+) − ¯h(x, y+)(cid:107)2(cid:105)
≤ 9˜σ2
f ,

and

(cid:104)
E (cid:2)(cid:107)zi,ν(cid:107)2(cid:3) ≤ 3E

(cid:107) ¯∇fi(xi,ν, y+) − ¯∇fi(x, y+)(cid:107)2
+ (cid:107) ¯∇f (x, y+) − ∇f (x)(cid:107)2 + (cid:107)∇f (x)(cid:107)2(cid:105)
E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + M 2
≤ 3 (cid:0)M 2

f

f

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3)(cid:1) .

(38)

(39)

(40)

(41)

(42a)

(42b)

(42c)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Now, the ﬁrst term in the RHS of (41) can be bounded as follows:

E (cid:2)(cid:107)xi,ν − x − αi(vi,ν + zi,ν)(cid:107)2(cid:3) ≤

1 +

(cid:18)

(cid:18)

≤

1 +

(cid:18)

≤

1 +

(cid:18)

≤

1 +

(cid:19)

(cid:19)

(cid:19)

1
2τi − 1
1
2τi − 1
1
2τi − 1
1
2τi − 1
(cid:0)M 2

f

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 2τiE (cid:2)(cid:107)αi(vi,ν + zi,ν)(cid:107)2(cid:3)

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 4τiα2

i

(cid:0)E (cid:2)(cid:107)zi,ν(cid:107)2(cid:3) + E (cid:2)(cid:107)vi,ν(cid:107)2(cid:3)(cid:1)

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 4τiα2
(cid:19)

i

+ 12τiα2

i M 2
f

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3)

(cid:0)E (cid:2)(cid:107)zi,ν(cid:107)2(cid:3) + 9b2(cid:1)

(43)

+ 12τiα2
i

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1) .

Here, the ﬁrst inequality follows from Lemma G.2; the second inequality uses Lemma G.1; and the third and last inequalities
follow from (42a) and (42c).

Substituting (43) into (41) gives

E

(cid:107)xi,ν+1 − x(cid:107)2(cid:105)
(cid:104)

(cid:18)

≤

1 +

f

1
2τi − 1
(cid:0)M 2
1
τi − 1
(cid:0)M 2

f

+ 12τiα2
i
(cid:18)

≤

1 +

+ 12τiα2
i

(cid:19)

i M 2
f

+ 12τiα2

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3)
E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1) + 9α2
(cid:19)

i ˜σ2
f

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3)

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1) + 9α2

i ˜σ2
f .

Here, the ﬁrst inequality uses (42b) and the last inequality follows by noting αi ≤ 1/(5Mf τi).

For all τi > 1, we have

ν−1
(cid:88)

(cid:18)

1 +

j=0

(cid:19)j

=

1
τi − 1

(cid:17)ν

(cid:17)

− 1

− 1

(cid:16)

τi−1

1 + 1
(cid:16)

1 + 1
(cid:18)

τi−1

≤ τi

1 +

(cid:19)ν

(cid:18)

≤ τi

1 +

(cid:19)τi

1
τi

≤ exp (1)τi < 3τi.

1
τi

(44)

(45)

Now, iterating equation (44) and using xi,0 = x, ∀i ∈ S, we obtain

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) ≤ (cid:0)12τiα2

i

(cid:0)M 2

f

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1) + 9α2

i ˜σ2
f

ν−1
(cid:88)

(cid:18)

(cid:1)

1 +

j=0

(cid:19)j

1
τi − 1

≤ 3τi

(cid:0)12τiα2

i

(cid:0)M 2

f

E (cid:2)(cid:107)y+ − y(cid:63)(x)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 3b2(cid:1) + 9α2

i ˜σ2
f

(cid:1) ,

where the second inequality uses (45). This completes the proof.

Remark C.1. Lemma C.4 shows that the bound on the client-drift scales linearly with τi and the inner error (cid:107)y+ − y(cid:63)(x)(cid:107)2
in general nested FL. We aim to control such a drift by selecting αi = O(1/τi) for all i ∈ S and using the inner error
bound provided in Lemma C.3.

Next, we provide the proof of our main result which can be adapted to general nested problems (bilevel, min-max,
compositional).

C.4. Proof of Theorem 3.1

Proof. We deﬁne the following Lyapunov function

Wk := f (xk) +

Mf
Ly

(cid:107)yk − y(cid:63)(xk)(cid:107)2.

(46)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Motivated by (Chen et al., 2021a), we bound the difference between two Lyapunov functions. That is,

Wk+1 − Wk =f (xk+1) − f (xk) +

Mf
Ly

(cid:0)(cid:107)yk+1 − y∗(xk+1)(cid:107)2 − (cid:107)yk − y(cid:63)(xk)(cid:107)2(cid:1) .

(47)

The ﬁrst two terms on the RHS of (47) quantiﬁes the descent of outer objective f and the reminding terms measure the
descent of the inner errors.

From our assumption, we have αk
Lemmas C.2 and C.3, and using (47), we get

i = αk/τi, βk

i = βk/τi, ∀i ∈ S. Substituting these stepsizes into the bounds provided in

E[Wk+1] − E[Wk] ≤

(cid:18) Lf α2
2

k

+

Mf
Ly

(cid:19)

a3(αk)

˜σ2
f +

−

αk
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

3M 2
f αk
2m

b2,

1
τi

τi−1
(cid:88)

ν=0

E (cid:2)(cid:107)xk

i,ν − xk(cid:107)2(cid:3)

3αk
2
m
(cid:88)

i=1


E



(cid:18) αk
2

−

Lf α2
k
2

−

Mf
Ly

(cid:19)

a1(αk)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

¯hi(xk

i,ν, yk+1)

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Mf
Ly

(cid:18) 3Mf Lyαk
2

(cid:19)

+ a2(αk)

E (cid:2)(cid:107)yk+1 − y∗(xk)(cid:107)2(cid:3) −

Mf
Ly

E (cid:2)(cid:107)yk − y∗(xk)(cid:107)2(cid:3) ,

(48c)

−

+

where a1(α) − a3(α) are deﬁned in (28).

Let

αk

i =

αk
τi

, ∀i ∈ S where αk ≤

The above choice of αk satisﬁes the condition of Lemma C.4 and we have 54M 2
get

k ≤ α2

k/4. Hence, from Lemma C.4, we

1
216M 2
f + 5Mf
f α3

.

(49)

E (cid:2)(cid:107)yk+1 − y(cid:63)(xk)(cid:107)2(cid:3) + E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) + 3b2(cid:1)

(cid:0)M 2

E (cid:2)(cid:107)yk+1 − y(cid:63)(xk)(cid:107)2(cid:3) + 3b2(cid:1)

f

(48a) ≤ −

αk
2
+ 54M 2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +
(cid:0)M 2

f α3
k
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

f

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

≤ −

≤ −

+

αk
4
αk
4
25Mf
Ly

81
2

kM 2
α3

f ˜σ2
f

α2
k
4
α2
k
4

˜σ2
f +

˜σ2
f +

α2
k
4
3α2
k
4
Mf
Ly

b2

(cid:18) Mf Ly
4

(cid:19)

α2
k

T β2

kσ2

g,1 +

(cid:18) Mf Ly
4

α2
k

(cid:19) (cid:18)

1 −

(cid:19)T

,

βkµg
2

where the ﬁrst inequlaity uses (49) and the last inequality follows from (27a).
To guarantee the descent of Wk, the following constraints need to be satisﬁed

(48b) ≤ 0,

=⇒

αk
2

−

=⇒ αk ≤

where the second line uses (28).

Further, substituting (27) in (48c) gives

(cid:18)

−

L2

yα2

Lf α2
k
2

Mf
Ly
1
2Lf + 4Mf Ly + 2Mf Lyx

k +

Lyη

,

Lyαk
4Mf

(cid:19)

+

Lyxα2
k
2η

≥ 0,

(48c) ≤

25Mf
Ly

(cid:18) 3Mf Lyαk
2

+ a2(αk)

(cid:19)

T β2

kσ2
g,1

+

Mf
Ly

(cid:32)(cid:18) 3Mf Lyαk
2

(cid:19) (cid:18)

+ a2(αk)

1 −

(cid:19)T

βkµg
2

(cid:33)

− 1

E[(cid:107)yk − y∗(xk)(cid:107)2].

(48a)

(48b)

(50)

(51)

(52)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Substituting (50)–(52) into (48) gives

E[Wk+1] − E[Wk] ≤ −

+

+

+

b2

E[(cid:107)∇f (xk)(cid:107)2]

αk
4
(cid:18) 3
2
(cid:18)(cid:18) Lf
2
25Mf
Ly

αk +

(cid:19)

(cid:19)

+

α2
k

3
4
1
4
(cid:18) Mf Ly
4
(cid:32)(cid:18) Mf Ly

α2

k +

(cid:19)

a3(αk)

˜σ2
f

Mf
Ly

α2

k +

3Mf Lyαk
2

+ a2(αk)

(cid:19)

T β2

kσ2
g,1

3Mf Lyαk
2
Let βk < min (cid:0)1/(6(cid:96)g,1), 1(cid:1). Then, we have βkµg/2 < 1. This together with (28) implies that for any αk > 0

βkµg
2

+ a2(αk)

Mf
Ly

k +

− 1

1 −

α2

+

4

E[(cid:107)yk − y∗(xk)(cid:107)2]. (53a)

(cid:19) (cid:18)

(cid:19)T

(cid:33)

(53a) ≤ 0,

(cid:32)

=⇒

1 +

Mf Ly
4

α2

k +

=⇒ exp

(cid:32)

Mf Ly
4

α2

k +

11Mf Lyαk
2

11Mf Ly + ηLyx ˜D2

µg

11Mf Lyαk
2

+

ηLyx ˜D2
2
ηLyx ˜D2
2
f αk + Mf Lyαk

+

2

f α2
k

·

αk
T

.

f α2
k

(cid:33) (cid:18)

1 −

(cid:33)

(cid:18)

exp

−

(cid:19)T

βkµg
2

(cid:19)

T βkµg
2

− 1 ≤ 0,

− 1 ≤ 0,

=⇒ βk ≥

From (49), (51) and (54), we select

where

αk = min{¯α1, ¯α2, ¯α3,

¯α
√
K

},

βk =

¯βαk
T

,

¯β :=

(cid:18)

1
µg

11Mf Ly + ηLyx ˜D2

f ¯α1 +

Mf Ly ¯α1
2

(cid:19)

,

¯α1 :=

1
2Lf + 4Mf Ly + 2Mf Lyx

Lyη

,

¯α2 :=

T
8(cid:96)g,1

,

¯β

¯α3 :=

1
216M 2
f + 5Mf

,

With the above choice of stepsizes, (53) can be simpliﬁed as

αk
E[Wk+1] − E[Wk] ≤ −
4
(cid:18) 3
2

+

E[(cid:107)∇f (xk)(cid:107)2]

αk +

(cid:19)

b2

3
4

α2
k

α2

k +
(cid:18) Mf Ly
4

a3(αk)

(cid:19)

˜σ2
f

Mf
Ly

α2

k +

3Mf Ly
2

(cid:19)

αk + a2(αk)

+

+

(cid:18) Lf + 1
2
2
25Mf
Ly
αk
4

T β2

kσ2
g,1
(cid:19)

(cid:18) 3
2

αk +

3
4

≤ −

E[(cid:107)∇f (xk)(cid:107)2] + c1α2

kσ2

g,1 +

α2
k

b2 + c2α2

k ˜σ2
f ,

where the constants c1 and c2 are deﬁned as

(cid:32)

1 +

11Mf Ly
2

¯α1 +

(cid:32) Mf Ly + 2ηLyx ˜D2
4

f

(cid:33)

(cid:33)

¯α2
1

¯β2 1
T

,

c1 =

c2 =

25Mf
Ly
Lf + 1
2
2

+ Mf Ly +

LyxMf
4ηLy

.

(54)

(55)

(56)

(57)

(58)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Then telescoping gives

1
K

K−1
(cid:88)

k=0

E[(cid:107)∇f (xk)(cid:107)2] ≤

4
(cid:80)K−1
k=0 αk

(cid:32)

∆W +

(cid:18)

K−1
(cid:88)

k=0

3
2

αk +

(cid:19)

α2
k
2

b2 + c1α2

kσ2

g,1 + c2α2

k ˜σ2
f

(cid:33)

≤

4∆W
min{¯α1, ¯α2, ¯α3}K

+

(cid:32)

= O

1
min{¯α1, ¯α2, ¯α3}K

(cid:19)

b2 +

(cid:18)

1 +

+ 6

¯α
4∆W
√
√
2
K
¯α
g,1, σ2
¯α max(σ2
√
K

+

K
g,2, σ2
f )

+ b2

,

σ2
g,1 +

4c2 ¯α
√
K

˜σ2
f

4c1 ¯α
√
K

(cid:33)

(59)

where ∆W := W0 − E[WK].

C.5. Proof of Corollary 3.1
Proof. Let η = Mf
Ly

= O(κg) in (58). It follows from (20), (56), and (58) that

¯α1 = O(κ−3

g ), ¯α2 = O(T κ−3

g ), ¯α3 = O(κ−4

g ), c1 = O(κ9

g/T ), c2 = O(κ3

g).

(60)

Further, N = O(κg log K) gives b = 1

K1/4 . Now, if we select ¯α = O(κ−2.5

g

) and T = O(κ4

g), Eq. (59) gives

1
K

K−1
(cid:88)

k=0

E[(cid:107)∇f (xk)(cid:107)2] = O

(cid:32)

κ4
g
K

+

κ2.5
g√

K

(cid:33)

.

To achieve ε-optimal solution, we need K = O(κ5
respectively.

gε−2), and the samples in ξ and ζ are O(κ5

gε−2) and O(κ9

gε−2),

D. Proof for Federated Minimax Optimization

Note that the minimax optimization problem (2) has the following bilevel form

min
x∈Rd1
subj. to

Here,

is the loss functions of the ith client.

In this case, the hypergradient of (61) is

(cid:80)m

f (x) = 1
m
y∗(x) = argmin
y∈Rd2

i=1 fi (x, y∗(x))

− 1
m

(cid:80)m

i=1 fi (x, y) .

fi(x, y) = Eξ∼Ci [fi(x, y; ξ)]

(61a)

(61b)

∇fi(x) = ∇xfi

(cid:0)x, y∗(x)(cid:1) + ∇xy∗(x)(cid:62)∇yfi

(cid:0)x, y∗(x)(cid:1) = ∇xfi

(cid:0)x, y∗(x)(cid:1),

(62)

where the second equality follows from the optimality condition of the inner problem, i.e., ∇yf (x, y∗(x)) = 0.
(cid:0)x, y(cid:1).
For each i ∈ S, we can approximate ∇fi(x) on a vector y in place of y∗(x), denoted as ∇fi(x, y) := ∇xfi
We also note that in the minimax case hi is an unbiased estimator of ¯∇fi(x, y). Thus, b = 0. Therefore, we can apply
FEDNEST using

qi,ν = −∇yfi(x, yi,ν; ξi,ν) + ∇yfi(x, y; ξi,ν) −

1
m

m
(cid:88)

∇yfi(x, y; ξi),

hi,ν = ∇xfi(xi,ν, y+; ξi,ν) − ∇xfi(x, y+; ξi,ν) +

i=1
m
(cid:88)

i=1

1
m

∇xfi(x, y+; ξi).

(63)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

D.1. Supporting Lemmas

Let z = (x, y) ∈ Rd1+d2. We make the following assumptions that are counterparts of Assumptions A and B.

Assumption E. For all i ∈ [m]:

(E1) fi(z), ∇fi(z), ∇2fi(z) are respectively (cid:96)f,0, (cid:96)f,1, (cid:96)f,2-Lipschitz continuous; and

(E2) fi(x, y) is µf -strongly convex in y for any ﬁxed x ∈ Rd1.

We use κf = (cid:96)f,1/µf to denote the condition number of the inner objective with respect to y.
Assumption F. For all i ∈ [m]:

(F1) ∇fi(z; ξ) is unbiased estimators of ∇fi(z); and

(F2) Its variance is bounded, i.e., Eξ[(cid:107)∇fi(z; ξ) − ∇fi(z)(cid:107)2] ≤ σ2

f , for some σ2
f .

In the following, we re-derive Lemma C.1 for the ﬁnite-sum minimax problem (61).
Lemma D.1. Under Assumptions E and F, we have ¯hi(x, y) = ¯∇fi(x, y) for all i ∈ S and (19a)–(19g) hold with

Lyx =

(cid:96)f,2 + (cid:96)f,2Ly
µf

+

(cid:96)f,1((cid:96)f,2 + (cid:96)f,2Ly)
µ2
f

= O(κ3

f ),

Mf = (cid:96)f,1 = O(1), Lf = ((cid:96)f,1 +

(cid:96)2
f,1
µf

) = O(κf ),

Ly =

(cid:96)f,1
µf

= O(κf ), ˜σ2

f = σ2
f ,

˜D2

f = (cid:96)2

l,0 + σ2
f ,

where (cid:96)f,0, (cid:96)f,1, (cid:96)f,2, µf , and σf are given in Assumptions E and F.

D.2. Proof of Corollary 3.2

Proof. Let η = 1. From (55) and (56), we have

(cid:26)

αk = min

¯α1, ¯α2, ¯α3,

(cid:27)

,

¯α
√
K

βk =

¯βαk
T

,

where

¯β =

(cid:18)

1
µg

11(cid:96)f,1Ly + Lyx ˜D2

f ¯α1 +

(cid:19)

(cid:96)f,1Ly ¯α1
2

¯α1 =

1
2Lf + 4(cid:96)f,1Ly + 2(cid:96)f,1Lyx

Ly

,

¯α2 =

T
8(cid:96)g,1

¯β

,

,

¯α3 =

1

216(cid:96)2

f,1 + 5(cid:96)f,1

.

Using the above choice of stepsizes, (59) reduces to

1
K

K−1
(cid:88)

k=0

E[(cid:107)∇f (xk)(cid:107)2] ≤

4∆W
K min{¯α1, ¯α2, ¯α3}

+

4∆W
√
K
¯α

+

4(c1 + c2)¯α
√
K

σ2
f ,

where ∆W = W0 − E[WK],

c1 =

c2 =

25(cid:96)f,1
Ly
Lf + 1
2
2

(cid:32)

1 +

11(cid:96)f,1Ly
2

¯α1 +

(cid:32) (cid:96)f,1Ly + 2Lyx ˜D2
4

f

(cid:33)

(cid:33)

¯α2
1

¯β2 1
T

,

+ (cid:96)f,1Ly +

Lyx(cid:96)f,1
4Ly

.

(64)

(65a)

(65b)

(66)

(67)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Let ¯α = O(κ−1

f ). Since by our assumption, T = O(κf ), it follows from (64) and (73) that

¯α1 = O(κ−2

f ), ¯α2 = O(κ−1

f ), ¯α3 = O(1), c1 = O(κ2

f ), c2 = O(κ2

f ).

Substituting (68) in (66) and (67) gives

1
K

K−1
(cid:88)

k=0

E[(cid:107)∇f (xk)(cid:107)2] = O

(cid:32) κ2
f
K

(cid:33)

.

+

κf√
K

To achieve ε-accuracy, we need K = O(κ2

f ε−2).

E. Proof for Federated Compositional Optimization

(68)

(69)

Note that in the stochastic compositional problem (3), the inner function fi(x, y; ξ) = fi(y; ξ) for all i ∈ S, and the outer
function is gi(x, y; ζ) = 1

2 (cid:107)y − ri(x; ζ)(cid:107)2, for all i ∈ S. In this case, we have

∇ygi (x, y) = yi − ri(x; ζ), ∇yyg(x, y; ζ) = Id2×d2, and ∇xyg(x, y; ζ) = −

1
m

m
(cid:88)

i=1

∇ri(x; ζ)(cid:62).

(70)

Hence, the hypergradient of (3) has the following form

∇fi(x) = ∇xfi (y∗(x))
− ∇2

yg(x, y∗(x))]−1∇yfi (y∗(x))

xyg(x, y∗(x))[∇2
1
m

m
(cid:88)

i=1

= (

∇ri(x))(cid:62)∇yfi(y∗(x)).

(71)

¯∇fi(x, y) =
i=1 ∇ri(x))(cid:62)∇yfi(y). It should be mentioned that in the compositional case b = 0. Thus, we can apply FEDNEST

We can obtain an approximate gradient ∇fi(x) by replacing y(cid:63)(x) with y;
( 1
m
and LFEDNEST using the above gradient approximations.

(cid:80)m

that

is

E.1. Supporting Lemmas

Let z = (x, y) ∈ Rd1+d2. We make the following assumptions that are counterparts of Assumptions A and B.

Assumption G. For all i ∈ [m], fi(z), ∇fi(z), ri(z), ∇ri(z) are respectively (cid:96)f,0, (cid:96)f,1, (cid:96)r,0, (cid:96)r,1-Lipschitz continuous.
Assumption H. For all i ∈ [m]:

(H1) ∇fi(z; ξ), ri(x; ζ), ∇ri(x; ζ) are unbiased estimators of ∇fi(z), ri(x), and ∇ri(x).

(H2) Their variances are bounded, i.e., Eξ[(cid:107)∇fi(z; ξ) − ∇fi(z)(cid:107)2] ≤ σ2

f , Eζ[(cid:107)ri(z; ζ) − ri(z)(cid:107)2] ≤ σ2

r,0, and

Eζ[(cid:107)∇ri(z; ζ) − ∇ri(z)(cid:107)2] ≤ σ2

r,1 for some σ2

f , σ2

r,0, and σ2

r,1.

The following lemma is the counterpart of Lemma C.1. The proof is similar to (Chen et al., 2021a, Lemma 7).
Lemma E.1. Under Assumptions G and H, we have ¯hi(x, y) = ¯∇fi(x, y) for all i ∈ S, and (19a)–(19g) hold with

Mf = (cid:96)r,0(cid:96)f,1, Ly = (cid:96)r,0, Lf = (cid:96)2
f = (cid:96)2
˜σ2

f,0 + σ2

f + ((cid:96)2

r,0σ2

f )σ2

r,1,

˜D2

r,0(cid:96)f,1 + (cid:96)f,0(cid:96)r,1, Lyx = (cid:96)r,1,
f )((cid:96)2

r,0 + σ2

f,0 + σ2

r,1).

f = ((cid:96)2

E.2. Proof of Corrollary 3.3

Proof. By our assumption T = 1. Let ¯α = 1 and η = 1/Lyx. From (55) and (56), we obtain

(cid:26)

αk = min

¯α1, ¯α2, ¯α3,

(cid:27)

,

1
√
K

βk = ¯βαk,

(72)

(73a)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

where

¯β =

1
µg

(cid:32)

11(cid:96)f,1(cid:96)2

r,0 + ˜D2

f ¯α1 +

(cid:96)f,1(cid:96)2
r,0 ¯α1
2

(cid:33)

,

¯α1 =

1

2(cid:96)f,0(cid:96)r,1 + 6(cid:96)f,1(cid:96)2

r,0 + 2(cid:96)f,1(cid:96)2

r,1

,

¯α2 =

1
8(cid:96)r,0

,

¯β

¯α3 =

1
216((cid:96)r,0(cid:96)f,1)2 + 5(cid:96)r,0(cid:96)f,1

.

Then, using (59), we obtain

1
K

K−1
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) = O

(cid:18) 1
√
K

(cid:19)

.

This completes the proof.

F. Proof for Federated Single-Level Optimization

(73b)

(74)

Next, we re-derive Lemmas C.2 and C.4 for single-level nonconvex FL under Assumptions C and D.

Lemma F.1 (Counterpart of Lemma C.2). Suppose Assumptions C and D hold. Further, assume τi ≥ 1 and αi = α/τi, for
all i ∈ S and some positive constant α. Then, FEDOUT guarantees:

(75)

E (cid:2)f (x+)(cid:3) − E [f (x)] ≤ −

α
2

(1 − αLf ) E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

∇fi(xi,ν)

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

α
2

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) +

αL2
f
2m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) +

α2Lf
2

σ2
f .

Proof. By applying Algorithm 2 to the single-level optimization problem (13), we get

xi,0 = x ∀i ∈ S, x+ = x −

= x −

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

αi

αi

τi−1
(cid:88)

ν=0
τi−1
(cid:88)

i=1

ν=0

hi(xi,ν) − hi(x) + h(x)

hi(xi,ν),

where

hi(xi,ν) = ∇fi(xi,ν; ξi,ν), hi(x) = ∇fi(x; ξi,ν),

and h(x) = 1/m

m
(cid:88)

i=1

∇fi(x; ξi).

This together with Assumption C implies that

E (cid:2)f (x+)(cid:3) − E [f (x)] ≤E (cid:2)(cid:104)x+ − x, ∇f (x)(cid:105)(cid:3) +

Lf
2

E (cid:2)(cid:107)x+ − x(cid:107)2(cid:3)
(cid:43)(cid:35)

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

(cid:34)(cid:42)

= − E

+

Lf
2

E

1
m




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

hi(xi,ν)

 .

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

hi(xi,ν), ∇f (x)

(76)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

For the ﬁrst term on the RHS of (76), we obtain

(cid:34)(cid:42)

−E

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

(cid:43)(cid:35)

(cid:34)

hi(xi,ν), ∇f (x)

= − E

1
m


m
(cid:88)

i=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

1
m

= −

+

≤ −

α
2

α
2

α
2

E

E

E











m
(cid:88)

i=1

m
(cid:88)

i=1

m
(cid:88)

i=1

1
τi

1
τi

1
τi

τi−1
(cid:88)

ν=0

τi−1
(cid:88)

ν=0

τi−1
(cid:88)

ν=0

(cid:35)
E [(cid:104)hi(xi,ν), ∇f (x)(cid:105) | Fi,ν−1]

α
τi

τi−1
(cid:88)

ν=0

∇fi(xi,ν)

 −

(cid:107)∇f (x)(cid:107)2(cid:105)
(cid:104)

E

α
2

2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇fi(xi,ν) − ∇f (x)

 ,

(77)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇fi(xi,ν)

 −

(cid:107)∇f (x)(cid:107)2(cid:105)
(cid:104)

E

α
2

+

αL2
f
2m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

E

(cid:104)

(cid:107)xi,ν − x(cid:107)2(cid:105)

.

the ﬁrst equality follows from the law of total expectation;

the second equality uses E [∇fi(xi,ν)] =
Here,
E [E [hi(xi,ν)|Fi,ν−1]] and the fact that 2 a(cid:62)b = (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2; and the last inequality is obtained from
Assumption C.

For the second term on the RHS of (76), Assumption D together with Lemma G.1 gives

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

αi

τi−1
(cid:88)

i=1

ν=0

hi(xi,ν)

2
 = α2E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ α2E









(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

i=1

1
τi

1
τi

τi−1
(cid:88)

ν=0

τi−1
(cid:88)

ν=0

Plugging (78) and (77) into (76) gives the desired result.

(hi(xi,ν) − ∇fi(xi,ν) + ∇fi(xi,ν))

∇fi(xi,ν)

2
 + α2σ2
f .

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



(78)

The following lemma provides a bound on the drift of each xi,ν from x for stochastic nonconvex singl-level problems. It
should be mentioned that similar drifting bounds are provided in the literature under either strong convexity (Mitra et al.,
2021) and/or bounded dissimilarity assumptions (Wang et al., 2020; Reddi et al., 2020; Li et al., 2020b).
Lemma F.2 (Counterpart of Lemma C.4). Suppose Assumptions C and D hold. Further, assume τi ≥ 1 and αi = α/τi, ∀i ∈
S, where α ≤ 1/(3Lf ). Then, for all ν ∈ {0, . . . , τi − 1}, FEDOUT gives
(cid:107)∇f (x)(cid:107)2(cid:105)

(cid:107)xi,ν − x(cid:107)2(cid:105)

+ 27τiα2

≤ 12τ 2

(79)

E

E

(cid:104)

(cid:104)

i σ2
f .

i α2
i

Proof. The result trivially holds for τi = 1. Similar to what is done in the proof of Lemma C.4, let τi > 1 and deﬁne

vi,ν := ∇fi(xi,ν) − ∇fi(x) + ∇f (x),
wi,ν := hi(xi,ν) − ∇fi(xi,ν) + ∇fi(x) − hi(x) + h(x) − ∇f (x).

From Algorithm 2, for each i ∈ S, and ∀ν ∈ {0, . . . , τi − 1}, we obtain

xi,ν+1 − x = xi,ν − x − αi (hi(xi,ν) − hi(x) + h(x))
= xi,ν − x − αi (vi,ν + wi,ν) ,

which implies that

E (cid:2)(cid:107)xi,ν+1 − x(cid:107)2(cid:3) = E (cid:2)(cid:107)xi,ν − x − αivi,ν(cid:107)2(cid:3) + α2

E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3)
− 2E [E [(cid:104)xi,ν − x − αivi,ν, αiwi,ν(cid:105) | Fi,ν−1]]
E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3) .
= E (cid:2)(cid:107)xi,ν − x − αivi,ν(cid:107)2(cid:3) + α2

i

i

(80)

(81)

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

Here, the last equality uses Lemma G.3 since E[wi,ν|Fi,ν−1] = 0.

From Assumption D and Lemma G.1, for wi,ν deﬁned in (80), we have

E (cid:2)(cid:107)wi,ν(cid:107)2(cid:3) ≤ 3E (cid:2)(cid:107)hi(xi,ν) − ∇fi(xi,ν)(cid:107)2 + (cid:107)∇fi(x) − hi(x)(cid:107)2 + (cid:107)h(x) − ∇f (x)(cid:107)2(cid:3)

(82)

≤ 9σ2
f .
Substituting (82) into (81), we get

E (cid:2)(cid:107)xi,ν − x − αivi,ν(cid:107)2(cid:3) ≤

(cid:18)

1 +

(cid:18)

≤

1 +

(cid:18)

≤

1 +

(cid:19)

1
2τi − 1
1
2τi − 1
(cid:19)
1
τi − 1

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 2τiα2

i

E (cid:2)(cid:107)vi,ν(cid:107)2(cid:3) + 9α2

i σ2
f

+ 4τiα2

i L2
f

(cid:19)

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 4τiα2

i

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 9α2

i σ2
f

(83)

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) + 4τiα2

i

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 9α2

i σ2
f .

Here, the ﬁrst inequality follows from Lemma G.2; the second inequality uses Assumption C and Lemma G.1; and the last
inequality follows by noting αi = α/τi, ∀i ∈ S and α ≤ 1/(3Lf ).

Now, iterating equation (83) and using xi,0 = x, ∀i ∈ S, we obtain

E (cid:2)(cid:107)xi,ν − x(cid:107)2(cid:3) ≤ (cid:0)4τiα2

i

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 9α2

i σ2
f

ν−1
(cid:88)

(cid:1)

(cid:18)

1 +

≤ 12τ 2

i α2
i

E (cid:2)(cid:107)∇f (x)(cid:107)2(cid:3) + 27τiα2

j=0
i σ2
f ,

(cid:19)j

1
τi − 1

(84)

where the second inequality uses (45). This completes the proof.

F.1. Proof of Theorem 3.2

Proof. Let ¯α1 := 1/(3Lf (1 + 8Lf )). Note that by our assumption αk ≤ ¯α1. Hence, the stepsize αk satisﬁes the condition
of Lemma F.2, and we also have 6L2

k/4 ≤ αk/4. This together with Lemmas F.1 and F.2 gives

k ≤ α2

f α3

E (cid:2)f (xk+1)(cid:3) − E (cid:2)f (xk)(cid:3) ≤ −

αk
2

E[(cid:107)∇f (xk)(cid:107)2] +

L2
f αk
2m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

(cid:104)
E

ν=0

−

αk
2

(1 − αkLf )E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

∇fi(xk

i,ν)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)xk

i,ν − xk(cid:107)2(cid:105)
2

 +

α2
kLf
2

σ2
f

≤ −

≤ −

≤ −

αk
2

αk
2
αk
4

E[(cid:107)∇f (xk)(cid:107)2] +

L2
f αk
2m

m
(cid:88)

i=1

1
τi

τi−1
(cid:88)

ν=0

(cid:104)
E

(cid:107)xk

i,ν − xk(cid:107)2(cid:105)

+

E[(cid:107)∇f (xk)(cid:107)2] + 6L2

f α3
k

E[(cid:107)∇f (xk)(cid:107)2] +

(cid:18) 27
2

kL2
α3

f +

(85)

α2
kLf
2

σ2
f

α2
kLf
2

(cid:19)

σ2
f

E[(cid:107)∇f (xk)(cid:107)2] + (1 + Lf )α2

kσ2
f ,

where the second and last inequalities follow from (14).

Summing (85) over k and using our choice of stepsize in (14), we obtain

1
K

K−1
(cid:88)

k=0

E[(cid:107)∇f (xk)(cid:107)2] ≤

≤

(cid:40)

√

(cid:41)

4∆f
K
(cid:18) 4∆f
¯α1
(cid:32)

· min

(cid:19) 1
K

+

∆f
¯α1K

+

,

1
K
¯α
¯α1
(cid:18) 4∆f
¯α
∆f
¯α + ¯ασ2
√
K

f

(cid:33)

,

+ 4(1 + Lf )σ2
f ·

¯α
√
K

+ 4(1 + Lf )¯ασ2
f

(cid:19) 1
√
K

,

(86)

where ∆f = f (x0) − E[f (xK)].

≤ O

FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

G. Other Technical Lemmas

We collect additional technical lemmas in this section.
Lemma G.1. For any set of vectors {xi}m

i=1 with xi ∈ Rd, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

xi

≤ m

m
(cid:88)

i=1

(cid:107)xi(cid:107)2.

Lemma G.2. For any x, y ∈ Rd, the following holds for any c > 0:

(cid:107)x + y(cid:107)2 ≤ (1 + c)(cid:107)x(cid:107)2 +

(cid:18)

1 +

(cid:19)

1
c

(cid:107)y(cid:107)2.

Lemma G.3. For any set of independent, mean zero random variables {xi}m

i=1 with xi ∈ Rd, we have

E

H. Additional Experimental Results

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1

2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

xi

=

m
(cid:88)

i=1

E

(cid:107)xi(cid:107)2(cid:105)
(cid:104)

.

(87)

(88)

(89)

In this section, we ﬁrst provide the detailed parameters in Section 4 and then discuss more experiments. In Section 4, our
federated algorithm implementation is based on (Ji, 2018), both hyper-representation and loss function tuning use batch
size 64 and Neumann series parameter N = 5. We conduct 5 SGD/SVRG epoch of local updates in FEDINN and τ = 1 in
FEDOUT. In FEDNEST, we use T = 1, have 100 clients in total, and 10 clients are selected in each FEDNEST epoch.

y
c
a
r
u
c
c
a

t
s
e
T

Epoch

# of communications

(a) The test accuracy w.r.t to the algorithm epochs.

(b) The test accuracy w.r.t to the number of communications.

Figure 5: Hyper representation experiment comparing LFEDNEST, LFEDNESTSVRG and LFEDNEST-NONALT on non-i.i.d
dataset. The number in parentheses corresponds to communication rounds shown in Table 2.

H.1. The effect of the alternating between inner and outer global variables

In our addition experiments, we investigate the effect of the alternating between inner and outer global variables x and y.
We use LFEDNEST-NONALT to denote the training where each client updates their local yi and then update local xi w.r.t.
local yi for all i ∈ S. Hence, the nested optimization is performed locally (within the clients) and the joint variable [xi, yi]
is communicated with the server. One can notice that only one communication is conducted when server update global x
and y by aggregating all xi and yi.

02000400060008000100006065707580859095100LFedNest-NonAlt (1)LFedNest (T+1)LFedNestSVRG (2T+1)02000400060008000100006065707580859095100LFedNest-NonAlt (1)LFedNest (T+1)LFedNestSVRG (2T+1)FEDNEST: Federated Bilevel, Minimax, and Compositional Optimization

# of communications

(a) α = 0.0075

# of communications

(b) α = 0.005

y
c
a
r
u
c
c
a

t
s
e
T

y
c
a
r
u
c
c
a

t
s
e
T

# of communications

(c) α = 0.0025

# of communications

(d) α = 0.001

Figure 6: Learning rate analysis on non-i.i.d. data with respect to # of communications.

As illustrated in Figure 5, the test accuracy of LFEDNEST-NONALT remains around 80%, but both standard LFEDNEST
and LFEDNESTSVRG achieves better performance. Here, the number of inner iterations is set to T = 1. The performance
boost reveals the necessity of both averaging and SVRG in FEDINN, where the extra communication makes clients more
consistent.

H.2. The effect of the learning rate and the global inverse Hessian

Figure 6 shows that on non-i.i.d. dataset, both SVRG and FEDOUT have the effect of stabilizing the training. Here, we set
T = 1 and N = 5. As we observe in (a)-(d), where the learning rate decreases, the algorithms with more communications
are easier to achieve convergence. We note that LFEDNEST successfully converges in (d) with a very small learning rate. In
contrast, in (a), FEDNEST (using the global inverse Hessian) achieves better test accuracy in the same communication round
with a larger learning rate.

010002000300040005000102030405060708090100LFedNest (T+1)LFedNestSVRG (2T+1)FedNest (2T+N+3)010002000300040005000102030405060708090100LFedNest (T+1)LFedNestSVRG (2T+1)FedNest (2T+N+3)010002000300040005000102030405060708090100LFedNest (T+1)LFedNestSVRG (2T+1)FedNest (2T+N+3)010002000300040005000102030405060708090100LFedNest (T+1)LFedNestSVRG (2T+1)FedNest (2T+N+3)