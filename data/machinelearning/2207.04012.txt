2
2
0
2

g
u
A
0
1

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

2
v
2
1
0
4
0
.
7
0
2
2
:
v
i
X
r
a

Lagrangian Large Eddy Simulations
via Physics Informed Machine Learning

Yifeng Tiana, Michael Woodwardb,c, Mikhail Stepanovb, Chris Fryerc,
Criston Hyettb,c, Daniel Livescuc, and Michael Chertkovb

aInformation Sciences Group, Computer, Computational and Statistical
Sciences Division (CCS-3), Los Alamos National Laboratory, Los Alamos,
NM, 87545, USA.
bGraduate Interdisciplinary Program in Applied Mathematics and
Department of Mathematics, University of Arizona, Tucson, AZ, 85721,
USA.
cComputational Physics and Methods Group, Computer, Computational
and Statistical Sciences Division (CCS-2), Los Alamos National Laboratory,
Los Alamos, NM, 87545, USA.

August 11, 2022

Abstract

High Reynolds Homogeneous Isotropic Turbulence is fully described within the
Navier-Stokes (NS) equations, which are notoriously diﬃcult to solve numerically. En-
gineers, interested primarily in describing turbulence at a reduced range of resolved
scales, have designed heuristics, known as Large Eddy Simulation (LES). LES is de-
scribed in terms of the temporally evolving Eulerian velocity ﬁeld deﬁned over a spatial
grid with the mean-spacing correspondent to the resolved scale. This classic Eulerian
LES depends on assumptions about eﬀects of sub-grid scales on the resolved scales.

Here, we take an alternative approach and design novel LES heuristics stated in
terms of Lagrangian particles moving with the ﬂow. Our Lagrangian LES, thus L-LES,
is described by equations generalizing the weakly compressible Smoothed Particle Hy-
drodynamics formulation with extended parametric and functional freedom, which is
then resolved via Machine Learning training on Lagrangian data from Direct Numerical
Simulations of the NS equations. The L-LES model includes physics-informed param-
eterization and functional form, by combining physics-based parameters and physics-
inspired Neural Networks to describe the evolution of turbulence within the resolved
range of scales. The sub-grid scale contributions are modeled separately with physical
constraints to account for the eﬀects from un-resolved scales. We build the resulting

1

 
 
 
 
 
 
model under the Diﬀerentiable Programming framework to facilitate eﬃcient training.
We experiment with loss functions of diﬀerent types, including physics-informed ones
accounting for statistics of Lagrangian particles. We show that our Lagrangian LES
model is capable of reproducing Eulerian and unique Lagrangian turbulence structures
and statistics over a range of turbulent Mach numbers.

Keywords: Physics Informed Machine Learning
Simulations

Deep Learning

|

Lagrangian Particles

Large Eddy

|

|

Accurate Direct Numerical Simulations (DNS) of turbulent ﬂows in physical sciences and
engineering applications are, in general, prohibitively expensive due to the existence of a
wide range of length and time scales. This challenge has motivated the development of the
Reduced Order Models (ROM) which achieve fast and eﬃcient solutions in practical appli-
cations. In the ﬁeld of turbulence simulations, Reynolds-Averaged Navier-Stokes (RANS)
and Large Eddy Simulations (LES) have been widely adopted as alternatives for the DNS
of practical turbulent ﬂows [56]. In these traditional methods, turbulent ﬂows are viewed
from the Eulerian frame where the computational domain is discretized into small elements
or units via meshes. In LES, a spatio-temporal low-pass ﬁlter is applied to the Navier-Stokes
(NS) equations, therefore removing small-scale ﬂuctuations from the consideration. LES
results in the reduction of the computational complexity, simply because the range of scales
that need to be resolved [56] is reduced.

Eulerian LES has achieved great success in scientiﬁc and engineering applications. How-
ever, when solving turbulent problems with complex geometries, moving interfaces, and
multi-phase materials, the Eulerian description suﬀers from deteriorating numerical accu-
racy due to deforming mesh, ﬂuid-particle coupling, etc. To resolve these issues, resulted
from Eulerian description of the ﬂow, mesh-free methods have been proposed [29, 31], such as
Smoothed Particle Hydrodynamics, vortex method, and mesh-free Galerkin method. Simu-
lating turbulent ﬂows using the mesh-free, Lagrangian approximation of Navier-Stokes equa-
tions is appealing because it provides a unique perspective for understanding the transport
processes, such as the mixing and dispersion of passive scalars. Such processes are dominated
by the advective motion of velocity ﬂuctuations in time and space, which is naturally a part
of the Lagrangian representation of the ﬂuid via trajectories of representative ﬂuid particles
[71]. Among the mesh-free methods for solving ﬂuids problems, Smoothed Particle Hydro-
dynamics (SPH) [17, 39, 41] has been adopted in a wide range of scientiﬁc applications, e.g.
astrophysics, computer graphics, free-surface ﬂows, ﬂuid-structure interaction, bio-ﬂuids, ge-
ological ﬂows, magnetohydrodynamics [19, 5, 4, 54, 14, 7, 60, 61, 49, 59, 27, 70, 53]. Recently,
there is a number of studies adopting SPH to model turbulence [40, 34, 9, 48, 35, 50, 52].
SPH framework was also integrated into RANS modeling, and speciﬁcally into RANS two-
equation models, such as the k
(cid:15) type [66, 10, 26]. A series of studies explored similarity of
the ﬁltering procedure of LES and the smoothing procedure of SPH to develop LES models
in the SPH framework [6, 12, 2]. These papers have helped to establish SPH as a promising
framework for modeling turbulence which accounts for advection explicitly, though inte-
gration over Lagrangian trajectories. However, the papers have also highlighted signiﬁcant
challenges of the SPH modeling related to resolving the stress tensor term in the Lagrangian
frame and also the multi-particle nature of the formulation, making derivation and tuning
parameters within the SPH models time-consuming and diﬃcult [12].

−

2

This manuscript addresses the challenge of adopting the Lagrangian approach, and specif-
ically generalizing, i.e adApting – NOT adOpting, the SPH approach, to reduce-order mod-
eling of NS turbulence directly. Here, we mention several works that attempt to adapt the
SPH approach based on diﬀerent numerical procedures, such as mesh-less ﬁnite volume [20]
and adaptive smoothing kernel [43] and diﬀerentiate them from the more general formulation
proposed in the work, with physics-informed and interpretable parameterization that can be
learned from data. We achieve this goal by utilizing the power of Machine Learning (ML),
and more generally of the Artiﬁcial Intelligence (AI).

Modern ML and speciﬁcally Deep Learning, leveraging eﬃcient computational tools such
as automatic diﬀerentiation and sensitivity analysis of forward and backward propagation,
has resulted in multiple success stories in classic AI disciplines, such as computer vision [58],
speech recognition [11] and natural language processing [18]. However, the biggest achieve-
ments of AI and ML in sciences so far had been limited, until recently, to approaches that
are data-driven but agnostic to traditional scientiﬁc modeling of the underlying physics.
Integrating the breakthrough of the modern AI and ML with physical modeling, and specif-
ically for this manuscript with Lagrangian modeling of turbulence, is the major challenge
of what we call today Physics-Informed Machine Learning (PIML) 1. In the ﬁeld of tur-
bulence modeling, there has been a surge of PIML activity in recent years. In particular,
major eﬀorts have been devoted to the development of closure models for Reynolds Aver-
aged Navier-Stokes (RANS) and Large Eddy Simulations (LES) in the Eulerian frame using
innovative Neural Network (NN) architectures. Some of the contributions, prioritized here
due to their signiﬁcance for the methodology of this manuscript, are: Tensor Basis Neural
Network (TBNN) embedding physical constraints, such as Galilean invariance and rotational
invariance, into the closure model [28]; and PIML models infusing physical constraints into
the neural networks [22, 67, 37]. A comprehensive overview of these and other related con-
tributions to the ﬁeld of turbulence closure modeling in Eulerian frame can be found in [13].
Other than developing closure models for RANS and LES, researchers have been experiment-
ing with novel ML approaches to learn turbulence dynamics. In this regard, and just to name
a few, we mention [36, 38], where a Convolutional Long Short Term Memory (ConvLSTM)
Neural Network was developed to learn spatial-temporal turbulence dynamics; studies of
super-resolution allowing to reconstruct turbulence ﬁeld using under-resolved data [16]; and
Neural Ordinary Diﬀerential Equation (Neural ODE) for turbulence forecasting [46]. PIML
models for Lagrangian description of turbulence have also been actively developed. Authors
of [23] have used a regression forest to approximate the behavior of particles in Lagrangian
ﬂuid simulations. A diﬀerentiable ﬂuid solver accounting for Lagrangian dynamics has been
[57]. A continuous convolution network was constructed to learn SPH-
developed in Ref.

1It may be appropriate to mention here that PIML, as a term, was coined in 2016 in the name of the
LANL workshop in Santa Fe, NM, which later became a bi-annual event with the 4-th PIML taken place
in May of 2022 [1]. The workshops brought together ML/AI experts and statistical physicists to present
their work where either principal ideas from statistical physics were utilized in the design and training of ML
models, including Neural Networks, or ML techniques were integrated into physical models to better describe
applications in various quantitative disciplines of sciences and engineering. However, the term was also used
later by the authors of [21] to describe Neural Network modiﬁcation of the classic collocation method (for
the numerical solution of ODEs and PDEs, see [25]), originally introduced in [24] and then popularized in
[51] as the Physics Informed Neural Networks (PINN).

3

inspired Lagrangian simulation in [65]. Three of us (YT, DL, and MC) have reported in
[64] development of a PIML approach to design a closure model for the Lagrangian (single
particle) dynamics of the Velocity Gradient Tensor (VGT).

We have also developed in [69] a hierarchy of reduced Lagrangian multi-particle models
of the PIML/SPH type trained on ground truth data of two types, as described below.
First, we train the SPH model on synthetic multi-particle simulations imitating multi-scale
Lagrangian turbulence. Then, we also extend it to the Lagrangian ground truth data derived
from the fully-resolved Eulerian DNS of homogeneous isotropic turbulence.

In this manuscript, we further develop the ideas from the Lagrangian modeling of the
PIML/SPH type we have started to explore in [69].
In [69], we took advantage of the
degrees of freedom in the deﬁnition of SPH and then adOpted SPH to the ground truth data,
i.e. adjusted the degrees of freedom within SPH to ﬁt the data. On the contrary, in this
manuscript we generalize SPH, i.e. adApt SPH. This generalization allows us to introduce
a general Lagrangian LES (L-LES) model, simulating turbulence at the resolved scales, and
to train this model on the Lagrangian data derived from the fully-resolved Eulerian DNS.

We describe the L-LES framework in the remainder of the manuscript in steps. The stage
is set in Section 1 where we introduce (ﬁctitious) particles, and describe their Lagrangian
dynamics, i.e. eﬀective advection by a collective velocity ﬁeld, and eﬀective interaction im-
posed by the collective contribution of the particles to the inter-particle forces, reconstructed
from the velocity ﬁeld. Parametric, as well as functional, degrees of freedom in the resulting
non-linear system of multi-particle ODEs, stated in terms of coordinates and velocities of the
particles, are discussed both in intuitive physical but also formal mathematical terms. We
argue about why and in which sense the system of equations governing dynamics of interact-
ing Lagrangian particles is a good LES heuristics. In Section 2, the connection between the
Lagrangian and Eulerian descriptions of turbulence is established through Smoothing Kernel
(SK). We introduce parameterization of the SK via a Neural Network (NN), and then set
an SK- Loss Function (LF), based on the turbulence statistics (connecting both Lagrangian
and Eulerian descriptions) to train the SK-NN. We provide a comprehensive description of
how we ”learn” the PIML / L-LES model in Section 3. We discuss the choice of the L-LES
loss function, respective L-LES NN architecture, and relevant technical details. Section 4 is
devoted to details of the training and validation experiments. We explain the ground truth
DNS model, process of data generation, and signiﬁcance of validating/ testing the underlying
physics in detail. We summarize results and discuss path forward, respectively, in Section 5
and Section 6.

1 Lagrangian Large Eddy Simulation

In this Section we describe the main principles of the proposed Lagrangian Large Eddy
Simulation (L-LES) methodology for modeling turbulent ﬂows using a system of interacting
Lagrangian particles, whose evolution is governed by a set of parameterized equations with
embedded physical constraints.

In the classic mesh-free Lagrangian models of turbulence [29, 31], the dynamics of ac-
celeration, density, and pressure ﬁelds evaluated at the particle positions, are approximated
following the Lagrangian version of the Navier-Stokes equations. This system of interacting

4

particles provides an excellent platform for simulating turbulent ﬂows in the Lagrangian
frame, especially for resolving large-scale structures in the context of Large Eddy Simulation
(LES). This is because dynamics of a Lagrangian particle cloud represent accurately sweep-
ing of an eddy associated with the mean particle distance by larger eddies. The interactions
among particles in the cloud span over a wide range of scales, which enables the system
to capture the multi-scale turbulence dynamics even at the scales which are smaller than
the ﬁltering length (sub-grid scale) because the distances between non-uniform Lagrangian
particles may become smaller than the ﬁltering length. However, to formally derive a set of
dynamical equations that describe Lagrangian particles in the spatially-ﬁltered turbulence
ﬁeld, additional closure models for the emerged sub-ﬁlter contributions are needed. In the
classic LES, which are to the best of our knowledge exclusively Eulerian, these closure mod-
els have been historically formulated based on heuristics and hand-tuned parameters. Very
recently, the PIML framework has been started to be used to improve the parameter tuning,
or even to discover new functional forms of the closure for Eulerian LES [13, 47]. In this
study, we aim to leverage the PIML framework in the Lagrangian, multi-particle framework
to learn Lagrangian LES (L-LES) models from ﬁltered DNS data, which alleviates us from
the exhaustive process of model tuning.

1.1 Pair-Wise Particle Interaction

A straightforward phenomenological model for the evolution of a system of Lagrangian par-
In this work, we utilize the main
ticles is through the interactions among the particles.
physics idea of the particle-based methodology – i.e., express the ﬂow, e.g., velocity and
density ﬁelds evolving in time – in terms of particles. Speciﬁcally, we consider particles
”interacting” with each other in a pair-wise way, where the force exerted on a given particle
by the ﬂow is expressed as a sum of terms, each dependent on the current position of the
particle and another particle (thus a pair), but also dependent on the velocities and densi-
ties evaluated at positions of the two particles. The pair-wise expression for the force acting
on any particle in the volume is common for all Particle-Based Methods (PBM), including
arguably the most popular example of the PBM – the Smoothed Particle Hydrodynamics.
Other PBM examples include the family of the mesh-free Galerkin methods and Molecular
Dynamics (MD) methods, see [30, 29, 31] and references therein.

Notice that the prime use of the particle-based, Lagrangian methods, is for either de-
scription of sub-viscous scale phenomena in materials (largely MD methods) and ﬂows or
for descriptions of the large scale (energy-containing) part of the ﬂows. In the sub-viscous
setting, particles are associated with the actual physical particles or small patches of ﬂows
resolved at the scales which are of the order or smaller than the viscous (Kolmogorov) scale
of the actual ﬂow. In the case of large-scale ﬂow modeling, the particle-based methods (pri-
marily SPH) have been used for a sketchy (imprecise) description of the underlying physics
at the largest (often astronomical) scales. The main goal of this manuscript is to bridge the
gap between the smallest (viscous) and the largest (energy-containing) scales in a continuum
description of the ﬂow. We focus here on building L-LES by resolving the range of scales
that covers a signiﬁcant portion of the large-scale part of the inertial range of scales in a
turbulent ﬂow.

The traditional approach of the particle-based methods describing continuum level ﬂows

5

consists of an intuitive derivation of the inter-particle interaction by analogy with the Navier-
Stokes equations. In this work, we choose to generalize the approach. We do not derive the
interactions by analogy with the Navier-Stokes equations, but rather model them in the
most general way. This general way of modeling is inclusive in the sense that it allows us to
include many other modeling ideas used in the LES and particle-based methods so far.

For an N particles system in three-dimensional domain Ω

R3, we introduce a vector,
φi = [vi, ρi]T , built from velocity and density ﬁelds evaluated at the particle locations,
xi ∈
Ω. (Given that our focus here is on the weekly compressible limit of fully compressible
ﬂows, we do not account for explicit dependence on the pressure ﬁeld, assumed expressed
via density according to a barotropic equations of state.) Then, the most general equations
we consider describing the evolution of the system of particles are:

⊂

(cid:21)

(cid:20)xi
φi

d
dt

=






N
(cid:88)

j=1,j

=i

vi

f (xi, xj, φi, φj)




 ,

i

∀

∈

1, ..., N,

(1)

where f is (yet to be learned/discovered) function expressing the pair-wise force imposed on
a particle i by a particle j and dependent on the positions of the two functions as well as
the vector φ evaluated at the positions of the two particles.

In general, we have the freedom to choose the functional form of f in Eq. (1) for model-
ing, by either using heuristic physical/mathematical arguments or inferring it from data. For
example, in the speciﬁc choice made within the context of Smoothed Particle Hydrodynamic
modeling, f is modeled by approximating the right-hand side (RHS) of the Lagrangian NS
equations using the reconstructed ﬁeld by a parameterized smoothing kernel. SPH is formu-
lated to ensure that the physical constraints, such as conservation of momentum, Galilean
and rotational invariance, are satisﬁed. Readers interested in the details on the SPH model-
ing and learning of NS turbulence are advised to check [69]. In this manuscript, we do account
for the relevant physical constraints, however, we do not follow the PIML/SPH learning path
of [69]. Instead, we take here a more general approach, described in the following.

1.2 Physics-Informed Choice of the Pair-Wise Interaction

Despite the existence of a wide range of functions that might describe the data, a Neural
Network, as a general function approximator with over-parameterization, provides an excel-
lent data-driven platform for modeling pair-particle interaction. In the PIML framework,
which aims to leverage the capabilities of NNs in advancing scientiﬁc applications, the math-
ematical and physical laws and constraints are either built into the architectures of NN or
integrated into the loss functions. Various studies have shown that the PIML framework is
advantageous in improving robustness, eﬃciency, and interpretability over a physics-blind
one [38, 64]. In this Section, we describe how the physics constraints are included in (other-
wise generic) pair-wise function f in Eq. (1).

We need to enforce invariance of the dynamics (1) under the Galilean, translational, and
rotational transformations of the underlying frame of reference. Let us denote the operator,
. Then, we require
representing an element of the union of the physical transformations, as

T

6

(cid:54)
that the pair-wise force is

-invariant, i.e. formally,

T
f (xi, xj, φi, φj) = f (

T

xi,

T

T

xj,

T

φi,

T

φj).

(2)

T

T

T

If

T
xi = xi + x(cid:48), whereas φi stays invariant, i.e.

is limited to the translational invariance, then transformation of xi can be expressed
φi = φi. Therefore, a straightforward
as
approach to enforce translational invariance of f is to replace the general dependence on xi
xj. Similarly, the Galilean
and xj on dependence on only the diﬀerence of the two, xi −
vi =
invariance, which is associated with the transformation to a diﬀerent inertial frame,
xi = xi+v(cid:48)t, can be enforced by replacing the general dependence of the two particle
vi+v(cid:48),
vj. To enforce the rotational invariance,
velocities, vi and vj, by the relative velocity, vi −
symmetry transformations of the scalar (density) and the vector (velocity) components of φ,
vi, should be dealt with diﬀerently. Let us decompose f into its scalar, fs, and vector, fv,
components. Then rotational invariance of the scalar component of the force requires that,
fs = fs. The respective constraint which needs to be imposed on the transformed vector
T
component of fv,
fv, is that it becomes a linear combination of the rotation-aware vector
bases and scalar functions of rotational invariant scalars (we follow here the so-called Tensor
Basis (TB) approach of [28, 64]).

T

T

Another important aspect in developing our PIML model is the capability of generalizing
over a wider range of parameters that have not been used in the model training.
In the
context of weakly compressible Navier-Stokes turbulence, we are interested in developing a
Lagrangian LES model that can describe turbulence with diﬀerent turbulent Mach numbers,
Mt

2, at ﬁxed Reynolds number 3.
Our next key step is to leverage the enormous progress achieved during recent decade in
using NNs as universal function approximators and represent the remaining freedom left in
the functions on the right-hand-side of Eqs. (1) via NNs. This path leads us to the following
NN version of Eqs. (1) consistent with the physical symmetries (expressed according to the
TB approach):

dφi
dt

=

(cid:21)

(cid:20)ρi
vi

d
dt

=

N
(cid:88)

j=1



2
(cid:88)




k=1

N N ρ(Iij,m, m

∈

1, ..., 5; λρ)

N N v,k(Iij,m, m

∈

1, ..., 5; λv)bij,k + Πijbij,1




 +

(cid:21)

(cid:20) 0
Fi

,

(3)

≈

M 2

t ρ0 [33], so that M 2
t

ρ0 + ρ1, where ρ1 ∼

2Mt is the turbulent Mach number, which is estimated as the ratio of the typical velocity at the energy-
containing scale to the speed of sound, Mt = vL/cs. In the limit of low Mach number, for single component
is proportional to the
ﬂows, density can be expanded as ρ
ﬂuid density deviation from the uniform distribution. Note that any particle-based modeling of turbulence
requires introducing, discussing, and analyzing compressibility simply because any distribution of particles
translates into ﬂuid density which is always spatially non-uniform, even if slightly. Therefore, even if we
model fully incompressible turbulence, we should still introduce an eﬀective turbulent Mach number when
discussing a particle-based approximation, as M 2
ρ
t ∼ |

−
3Re number measures a relative strength of the self-advection term to the viscous term evaluated at the
integral scale of turbulence. It is thus estimated as Re = vLL/ν = ε1/3L4/3/ν [15]. However, as a part of G.I.
Taylor legacy, we often measure the strength of turbulence in terms of the so-called, Reλ = vLλ/ν, where
15/Re is the so-called Taylor micro-scale, which is sandwiched in between the energy-containing
λ = L
scale, L, and the Kolmogorov (viscous) scale, η, i.e. η < λ < L [62].

ρ0|

/ρ0.

(cid:112)

7

where N is the number of particles placed inside of the computational domain, Ω. In all
the numerical experiments, reported in this manuscript, Ω is a three dimensional cube of
the volume, (2π)3, i.e. Ω = [0, 2π]3. Here in Eq. (3) I and b denote, respectively, the scalar
invariants and vector bases of the bases expansion approach of [28, 64] and are deﬁned using
pair-particle information:

xj)/d, vij = (vi −

xij = (xi −
Iij,1 = ρi/ρrms, Iij,2 = ρj/ρrms, Iij,3 =
bij,1 = xij, bij,2 = vij.

vj)/vrms, ρij =
xij|
|

1
2

(ρi + ρj)/ρrms,

, Iij,4 =

vij|
|

, Iij,5 = xij ·

vij,

To ensure the generalizability of the L-LES model, we formulate it in non-dimensional
form, as shown above. In this work, we use the ﬁltered RMS velocity vrms and average pair-
particle distance d as the reference velocity and length scales, while the density ﬂuctuations
are normalized by ρrms. In particular, the latter makes the features independent of Mt to
leading order. The reference length scale d, which represents the resolved turbulence ﬁeld,
is chosen to be within the inertial subrange. With this normalization of features, we expect
the learned dynamics can be generalized over diﬀerent d values within the inertial range,
as this is assumed to be universal [62]. On the other hand, the forcing term is used in the
ground-truth data generation to control Mt and Reλ and contains an explicit dependency
on Mt. Thus, through the non-dimensionalization of the input features of NNs, we separate
the modeling of pair-particle interaction into Mt independent (represented by NN) and Mt
dependent (reference scales) parts. When generalizing over diﬀerent Mt, the same NN can
be employed, while the overall magnitude of the acceleration learned from the ground truth
data can be used to re-scale the NN output to the correct magnitude.

Let us discuss other details of the parameters and terms contributing to Eqs. (3):
• Neural Networks: We introduce two distinct NNs,

N N v, to approximate
respective components of the function f . As customary, the NNs will over-parameterize
to ﬁt the data. We will utilize in the following, speciﬁcally in Section 3 where numerical
validation of the L-LES approach is discussed, the same neural network for both
N N ρ
and
N N v – containing 5 scalar features as input and 4 hidden layers, each with 100
N N ρ is a scalar that accounts for the RHS of the continuity
neurons. The output of
N N v are the scalar coeﬃcients of the two vector bases.
equation. The outputs of

N N ρ and

• Eddy Viscosity: We make sure that the eddy diﬀusivity, Π-dependent, term in Eq. (3)
satisﬁes physical symmetries (see discussion above) and otherwise we use the following
form introduced in [42], which has been used as the standard artiﬁcial viscosity in SPH
literature.

Πij =

µij =

(cid:40)

−

αcµij +βµ2
ij
ρij

0,
dvij ·
xij|
|

xij
2 + (cid:15)d2 ,

, vij ·
vij ·

xij < 0,
xij ≥
0

The eddy viscosity term associated with a pair of particles activates (becomes sig-

8

niﬁcant) when the two particles are separated from each other at the distance which
is smaller than the resolved scale, d. We naturally link the resolved scale, d, to the
number of particles, N , thus setting d = 2π/N 1/3 in our L-LES model over the [0, 2π]3
cube. Therefore the eddy-viscosity term leads, eﬀectively, to repulsion elongated with
bij,1, and it can thus be interpreted (according to the name) as modeling eﬀective en-
ergy transfer from the resolved scales to smaller scales. Also, and consistently with
the common reasoning, well documented in the SPH literature (see e.g. [70]), the eddy
viscosity term prevents particles from clustering and collisions, therefore, providing
overall numerical stability of the scheme. The coeﬃcients,
, contribut-
ing to the eddy viscosity term are not ﬁxed (as custom in the classic particle-based
method) but are learnable, i.e. subject to optimization together with the parameters
of the aforementioned NN. The eddy viscosity term could be absorbed into the above-
mentioned NN term since NN can be used as a universal function approximator, but
here we separate the parameterization of the eddy viscosity term to inject physical in-
tuition and mathematical structures. This may also alleviate the burdens on learning
NN parameters so that the learning process becomes easier and smoother.

λρ, λv, α, β

{

}

• External Force: The last term Fi in (3) models the external force, which is large-scale,
i.e. it injects energy into the system largely at the scales comparable to the size of the
box, L. We choose the forcing term in the L-LES model (3) to be linear in vi

Fi = αF

N
(cid:88)

j

χF (xi −

xj)vi,

(4)

consistently with the structure of the forcing term used in our “ground-truth” DNS
data to ensure that the DNS (and thus our L-LES model too) reaches a statistically-
steady state. Here in Eq. (4), αF is a parameter, which is set to match the energy
injection rate from external forcing with the dissipation rate of the turbulent ﬂow; and
xj) is the forcing weight function, dependent on the inter-particle radius-vector
χF (xi −
in the form prescribed by the DNS. In all of our experiments the large-scale forcing
term is ﬁxed, i.e.
it is the only term on the RHS of Eq. (3) which is not subject to
ﬁtting/learning.

A number of comments about the future use of the L-LES model (3) in the remainder of

the manuscript are in order.

First, it is important to emphasize that all the parameters contributing to the L-LES
model are subject to learning, i.e. optimization minimizing the so-called Loss Function (LF)
expressing the mismatch between DNS data and the L-LES model. There are many plausible
and also physics-informed choices for the LF, e.g. based on Lagrangian trajectories, Eulerian
ﬁeld, and statistics, which we discuss in detail in Section 4.

Second, let us recall that the L-LES model is set as a phenomenology/heuristics which is
stated in terms of the particles that are not representing exactly the actual particles advected
by the velocity ﬁelds from the DNS (our ground truth data). Indeed, we aim to construct
L-LES as a statistical model – i.e. model reproducing correctly only important statistical
features of turbulence but not exact (deterministic) trajectories. To achieve this statistical

9

goal, we will introduce in the next Section a mapping between Lagrangian particles and the
Eulerian ﬁeld, which will then be used to formulate the learning problem and then perform
training, validation, and statistical analysis. This mapping from the Lagrangian trajectories
to the Eulerian ﬁelds has also been adopted in other mesh-free methods, such as the ﬁeld
re-construction in SPH using the smoothing kernel. Notice, however, that this particles-to-
ﬁeld mapping was used in SPH in a much more targeted way (than in how we use it here).
Speciﬁcally, the SPH-mapping was tuned to approximate the spatial derivatives of the ﬁeld
quantities for approximating RHS of NS equations.

Finally and third, if all the functions (represented by NNs) and constants are ﬁxed by
ﬁtting Eqs. (3) from the Lagrangian DNS data, the equations are closed. Notice that this
simple closure is in a contrast with what is built into the methodology of the SPH and of
some other mesh-free methods. Indeed, the SPH scheme also includes reconstruction of the
pressure ﬁeld from the density ﬁeld, according to the equation of state, then followed by
reconstruction of the velocity ﬁeld from pressure by solving (in the incompressible case) the
Poisson equation. Even though we do not have the density-to-pressure-to-velocity recon-
struction step explicitly embedded into the L-LES Eqs. (3), we still reconstruct the ﬁelds
from particles, because the reconstructed ﬁelds will be used in the following: (1) to build the
Eulerian ﬁeld-based loss function to learn parameters of the L-LES model, and (2) to test
the results via statistical a-posteriori analysis.

2 From Lagrangian Trajectories to Eulerian Fields with

Smoothing Kernel

Reconstruction of the ﬁelds (e.g. of velocity, density, and pressure) deﬁned at an Eulerian
grid, which is not changing in time, from the Lagrangian trajectories is one of the key
elements of the SPH methodology where the temporally-evolving positions and velocities
of the particles are considered as interpolation points for the ﬁelds evaluated at any other
points of the domain. We follow the same basic approach and utilize the smoothing kernel
method to reconstruct ﬁelds from the positions and velocities of the particles.

Consider an exemplary scalar ﬁeld, A(x), which may be one (of the three) components
of the velocity ﬁeld, the density ﬁeld, or the pressure ﬁeld. Image of the ﬁeld convoluted
against the smoothing kernel, Wθ(x), parameterized by the vector of parameters, θ is

A(x)
(cid:104)

(cid:105)

=

(cid:90)

Ω

A(x(cid:48))Wθ(x

x(cid:48))dx(cid:48).

−

(5)

The smoothing kernel, Wθ(x), is assumed to be a continuous and suﬃciently smooth function
h = cd, where c is a suﬃciently large, c > 1,
of compact support — non-zero if
constant. This assumption, that the kernel is compactly supported, allows a more eﬃcient
implementation. Notice that the smoothing kernel also depends on the resolved scale, d.
However, to simplify notations, here and below we will not show this dependence explicitly.
The smoothing kernel is also normalized

x
|

| ≤

x(cid:48)

−

(cid:90)

Ω

Wθ(x

−

x(cid:48))dx(cid:48) = 1,

10

(6)

to guarantee the zeroth order consistency of the integral representation of continuum func-
tions [31].

Eq. (5) allows us to reconstruct
evaluated at the particle locations xi

A
(cid:105)
(cid:104)

at an ﬁeld grid position x from the values of A

A(x)
(cid:105)

(cid:104)

=

N
(cid:88)

j=1

mj
ρj

A(xj)Wθ(x

xj),

−

(7)

where mj/ρj stands for a volume element associated with the particle j. A schematic illustra-
tion of the reconstruction of the ﬁeld
is shown in Fig. 1 (a). We can also reconstruct
the ﬁeld of gradient of A,

(Fig. 1 (b))using the following equation [42]:

A(x)
(cid:104)

(cid:105)

A(x)
(cid:105)

∇(cid:104)

A(x)
(cid:105)

(cid:104)∇

=

(cid:34) N
(cid:88)

j=1

1
ρi

mj [A(xj)

A(x)]

∇rWθ

−

(cid:35)

,

(8a)

, of the ﬁeld, A,
Figure 1: Schematic illustration of (a) the reconstruction of the image,
deﬁned as a convolution of the kernel, Wθ(
), with the ﬁeld evaluated at the positions of the
·
. Width of the kernel, h, shown in the Figure,
A
Lagrangian particles and (b) its gradient
(cid:105)
∇(cid:104)
should be viewed as one of the components of the (tunable) vector of the kernel parameters,
θ.

A
(cid:105)
(cid:104)

A classic SPH approach consists of postulating the form of the kernel function, Wθ(
).
·
On the contrary, the approach of this manuscript (which was also developed in our earlier
paper [69]) aims at learning the kernel. Speciﬁcally, we model the parameterized kernel,
subject to the ﬁnite-support and normalization constraints described above, using a neural
network as follows

11

Wθ(r) = αn

N N θ(r/h)
1 + exp(C(r/h
−

.

1))

(9)

N N θ(r) stands for a Neural Network (NN) with scalar input, r =
, and scalar
output; and αn is the normalization coeﬃcient enforcing Eq. (6). This speciﬁc choice of the
functional form of Wθ(
) ensures that the kernel is of compact support, by multiplying a
·
sigmoid function, and that it is properly normalized. C is a parameter that determines the
sharpness of the decay to zero within the compact support domain.

xij|
|

r
|

=

|

The process of learning the smoothing kernel, Wθ(
), i.e. training NN entering (9), de-
·
pends on two additional constructs described below. First, in Section 2.1, we introduce the
ﬁnite neighborhood list approximation, which is a technique that allows to reduce compu-
tational complexity of the convolution involving the smoothing kernel and thus to make the
learning problem tractable. Then, in Section 2.2 we discuss the loss function used to train
the smoothing kernel.

2.1 Finite Neighborhood List Approximation

Formally, the sum over particle pairs contributing the RHS of Eq. (7) evaluated at the i-th
particle location, x = xi, contains N terms. However, for any x there will be only O(Nb)
terms contributing signiﬁcantly, where Nb is the number of particles that are O(h) close to the
i-th particle. To improve the eﬃciency of this evaluation, we prepare and update dynamically
the ﬁnite neighborhood list for each particle in the system (i.e. the list consisting of the ﬁnite
O(1) number of neighbors for each particle), and then use it to truncate the number of terms
contributing Eq. (7) to O(1).

2.2 Smoothing Kernel Loss Function

· · ·

To describe the reconstruction of the velocity ﬁeld in the Eulerian frame we introduce a
constant, i.e. frozen in time, grid deﬁned in terms of NE locations, xa, a = 1,
, NE which
we index by the letters from the ﬁrst three letters of the English alphabet, a, b, c (that is
in contrast with the Lagrangian particles, changing locations in time, which are indexed by,
i, j, k = 1,

N ).

· · ·

To learn the smoothing kernel (SK) that maps Lagrangian particles to Eulerian ﬁelds,

we build the SK Loss Function (LF), consisting of three terms,

LSK = cvLv + cgLg + cnLn.

(10)

The ﬁrst two terms in the SK-LF represent a mismatch between Lagrangian (particle)
data and Eulerian (ﬁeld) data for the velocities, Lv, and the velocity gradients, Lg, respec-
tively:

12

Lv =

1
NE

NE(cid:88)

a=1

v(xa)

|(cid:104)

v(xa)

2 ,
|

(cid:105) −

v(xa)
(cid:105)
(cid:104)

=

(cid:90)

Ω

Lg =

vα(xa)
(cid:105)

(cid:104)∇

=

≈

1
NE
(cid:90)

Ω

Nb(cid:88)

i=1

v(x)Wθ(xa −
NE(cid:88)

3
(cid:88)

a=1

α=1

|(cid:104)∇

x)dx

Nb(cid:88)

i=1

≈

viWθ(xa −

xi)∆Vi,

vα(xa)

2 ,
vα(xa)
|

(cid:105) − ∇

(vα(x)

vα(xa))

∇rWθ(xa −

−

x)dx

mi
∆Vi

(vα

i −

vα(xa))

∇raiWθ(xa −

xi)

(xa −
rai

xi)

,

(11)

(12)

(13)

(14)

where ∆Vi = mi/((cid:80)N
xj)) is the volume element associated with the (La-
j=1 mjWθ(xi −
1, 2, 3 denotes the component of the velocity vector;
grangian) particle i [41]; superscript α
and values of v, evaluated at the Lagrangian positions and locations of the Eulerian grid, are
assumed taken from the ground truth (GT) data extracted from properly ﬁltered Lagrangian
DNS (see Section 4 for details).

∈

There are, obviously, many possible choices one can make in designing the loss function.
Speciﬁc choices of Lv and Lg in Eqs. (11,13,15) were guided by consideration of simplicity
and also accepted practices of the SPH community. In particular, we choose to work with
the l2 norm and we utilize the ﬁnite-particle asymmetric approximation for gradients from
[39], which is, according to [30, 32], helps to improve numerical accuracy.

The role of the third contribution to the SK-LF, Ln, is to enforce the SK normalization
condition (6), and thus learn the normalization coeﬃcient α in Eq. (9). Since the SK
is modeled via NN, whose integral over Ω can not be computed analytically, we consider
spherically symmetric SK, split the compact spherical domain of the integration described
by Eq. 5 into Nr shells, of radii rk, k = 0, 1..., Nr and then approximate the remaining
one-dimensional integral according to the trapezoidal rule, thus arriving at the following
expression for the normalization enforcing component of LSK:

Ln = (I

−

1)2, I =

(cid:90)

Ω

Wθ(r)dr

4π∆r

≈

Nr
1
(cid:88)
−

k=1

Wθ(rk)r2
k

(15)

The Neural Network (NN) contributing the SK (9) is constructed using PyTorch [44]
open-source ML library. We build the neural network from four fully-connected layers with 20
nodes each, and with the hyperbolic tangent activation functions. The parameters (weights
and biases of the NN layers) are randomly initialized using the Glorot normal initialization
method. The parameters are updated by minimizing LSK using ”Adam” (one of the most
3 and it is then reduced
popular optimization methods). The initial learning rate is set to 10−
6 throughout the training process. Training is terminated when both training
gradually to 10−
and testing losses are saturated.

13

3 Loss Function Enforcing Multi-Physics in L-LES

To train the PIML L-LES model (3) and verify that the model can reproduce realistic
turbulence ﬁelds and statistics at the resolved scales, we need to introduce a Loss Function
(LF) whose minimization embodies the training. There are a number of options (for the
LF) to choose from. We present below some of these options and then rely on their linear
combination in accomplishing the training task.

Since the L-LES model (3) is principally Lagrangian, it is natural to consider a loss
function stated in terms of particles. Therefore, let us, ﬁrst of all, construct a trajectory-
based LF, Lt, which compares prediction of the L-LES model (3) and respective GT data
evaluated along the Lagrangian trajectories:

Lt =

1
N M

N
(cid:88)

M
(cid:88)

i=1

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

φm+1

i −
∆

φm
i

(cid:18) right-hand-side of Eq. (3)

−

evaluated at φm
i

2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(16)

i stands for φi(t) observed (as a Lagrangian part of the GT data) at the discretized
, M ; and we use a simple Euler method with time step

tm to ﬁnd components of φ integrating the L-LES model (3) in discrete time.

were φm
moments of time tm, m = 0, 1,
∆ = tm+1 −
predicted values of the Eulerian ﬁeld, thus resulting in

· · ·

However, we may also build a loss function based on a comparison of the observed and

Lf =

1
NEM

NE(cid:88)

M
(cid:88)

a=1

m=1
N
(cid:88)

i=1

φm+1
a
(cid:104)

(cid:105)

=

(cid:12)
(cid:12)

φm+1
a
(cid:104)

(cid:105) −

φm+1
a

2

(cid:12)
(cid:12)

,

mi
ρi

φm+1

i Wθ(xm+1

a −

xm+1
i

), xm+1

i

(17)

,

= xm

i + ∆vm+1

i

i , xm
where (φm
the GT data.

i , vm

i |∀

i = 1,

, N,

· · ·

m = 1,

∀

· · ·

, M ) is available as the Lagrangian part of

Finally, we construct the LF which aims to match the Lagrangian particle statistics –
predicted and observed. We choose to work with statistics of the single-particle Lagrangian
velocities and accelerations – which are the simplest possible Lagrangian characteristics
testing the resolved scale. Therefore we aim to compare respective Probability Distribution
Functions (PDFs) represented via histograms.

To build a histogram of a single-particle instantaneous velocity, vα

i (tn), we map the GT
, N , all the directions, α = 1, 2, 3,
, M , to a histogram Pv(µk) build of

i (tm), aggregated over all the particles, i = 1,
m = 1,

data vα
and all the available moments of time, tm,
K bins, k = 1,

, K, [µk, µk+1 = µk + w], each of size, w:

· · ·

· · ·

∀

· · ·

k : Pv(µk) =

∀

1
3N M K

N
(cid:88)

(cid:88)

M
(cid:88)

i=1

α=1,2,3

m=1

1 (vα

i (tm)

∈

[µk, µk+1]) .

(18)

Histogram of the particle acceleration, Pdv/dt(
) is built similarly. Notice that the samples-
· · ·
to-histogram functions, Pv(µk) and Pdv/dt(µk), allow eﬃcient representation via a Convolu-
tional Neural Network (CNN), with some ﬁxed weights and biases [68], which is designed

14

speciﬁcally to enable eﬃcient backpropagation over parameters (entering Eq. (18) via vi).
To compare statistics predicted by the model and statistics of the corresponding GT data we
construct the Kullback-Leibler (KL) loss function for velocity (and similarly for acceleration):

LKL;v =

m
(cid:88)

k=1

PGT (µk)log

(cid:20) PGT ;v(µk)
Ppred;v(µk)

(cid:21)

.

(19)

We combine the loss functions, representing diﬀerent physical inputs into the structure

of L-LES, in one expression,

L = ctLt + cf Lf + cKL;vLKL;v + cKL;dv/dtLKL;dv/dt,

(20)

where ct, cf , cKL;v and cKL;dv/dt are constants which can be adjusted (e.g. during training
for faster convergence).

4 Ground Truth Data

Our “ground truth” (GT) Lagrangian data are generated from the Eulerian Navier-Stokes
(NS) Direct Numerical Simulation (DNS) of weakly compressible, thus low Mach number,
stationary Homogeneous Isotropic Turbulence (HIT). Our numerical implementation of the
Eulerian DNS is on a 2563 mesh over the three-dimensional box, Ω = [0, 2π]3. We use sixth-
order compact ﬁnite diﬀerences for spatial discretization and the 4th-order Runge-Kutta
scheme for time advancement, also imposing triply-periodic boundary conditions over the
box. The velocity ﬁeld is initialized with 3D Gaussian spectral density enforcing zero mean
condition for all components. A large-scale quasi-solenoidal linear forcing term is applied to
< 2 to prevent turbulence from decaying [45]. The forcing
the simulation at wavenumber
method allows the speciﬁcation of the Kolmogorov scale at the onset and ensures that it
remains close to the speciﬁed value. The simulations presented here have η/∆x = 0.8, where
∆x is the grid spacing. Compared to a standard (well-resolved) spectral simulation with
ηkmax = 1.5, where kmax is the maximum resolved wavenumber, which has η/∆x = 1.5/π,
0.6 [45, 3] and the maximum diﬀerentiation error at the grid
the contraction factor is
(Nyquist) scale is less than 3.5%. Compared to a spectral method with ηkmax = 1, which
has η/∆x = 1/π, the contraction factor is
0.4 [45, 3] and the maximum diﬀerentiation
≈
error at the Nyquist scale is less than 0.2%. The initial temperature ﬁeld is set to be uniform
and the initial pressure ﬁeld is calculated by solving the Poisson equation. More details about
the numerical method and setup can be found in Refs. [45, 55]. The simulation is conducted
until the turbulence becomes statistically stationary, which is veriﬁed based on the evolution
of the kinetic energy and dissipation [45, 55].

k
|

≈

|

Once a statistically-steady state of HIT is achieved, we remove the forcing term in the
Eulerian DNS scheme and apply a Gaussian ﬁlter to the spatio-temporal Eulerian data to
obtain the velocity ﬁeld at the resolved scale, d, and then inject in the ﬁltered ﬂow 643
non-inertial Lagrangian ﬂuid particles. We remove forcing to make sure that the ﬂow, and
consequently Lagrangian dynamics of passive particles, are not masked by non-universal
details of the large-scale forcing. We use a Gaussian ﬁlter, which is commonly used in LES,
with a ﬁltering width of the order or larger than the scale d that can be resolved for the

15

Table 1: Three use cases considered in training and validation of the L-LES model.

Case number
Turbulent Mach number Mt
Taylor Reynolds number Reλ
Kolmogorov timescale tη
Usage

3
0.04
80
4.7
training & validation validation validation

2
0.16
80
1.2

1
0.08
80
2.3

262144 = 643 particles. (In the dimensionless units, where the energy-containing scale, L,
which is also the size of the box, is L = 2π, the smallest d we can resolve with this number
of particles is π/32, i.e. 64 times smaller than the size of the domain.)

The particles are placed in the computational domain at random, and then we follow
trajectories of the passively advected particles for the time, τ , which is of the order of (or
longer) than the turbulence turnover time of an eddy of size comparable to the resolved scale,
d, i.e. τ = O(d2/3/ε1/3), where ε is the estimate of the energy ﬂux transferred downscale
within the inertial range of turbulence 4.

In this work, we consider three turbulence cases for training and testing the model with
comparable Reynolds numbers, Reλ ≈
80, and the turbulent Mach number estimated as
Mt = 0.04, 0.08, and 0.16, respectively. (See also Table 1.) For fare comparison of diﬀerent
approaches to learning L-LES, we extract the ﬁltered Eulerian velocity and velocity gradient
snapshot at the same time as the Lagrangian particles. Particle positions and velocities
xi, vi}
are then used to reconstruct velocities at the points of the Eulerian grid according
{
to (7). We follow Lagrangian trajectories for as long as the (largest) eddy turnover time
(2π)2/3/ε1/3, and utilize the data to compute and optimize the particle-based loss.
τeddy ∼
The Eulerian ﬁelds are also recorded to compute and optimize the ﬁeld-based loss.

5 Results and Analyses

This Section presents results testing the quality of training of the Smoothing Kernel (SK),
described by Eq. (9), and of the L-LES model, described by Eq. (3). We present the SK
results ﬁrst in Section 5.1. Here the focus is on analyzing the shape of the SK an testing the
quality of the Eulerian ﬁeld predictions for diﬀerent choices of the coeﬃcients in the SK-LF
(10). We then analyze in Section 5.2 the L-LES model by investigating the generalization
errors of the trained L-LES model and test the ability to reproduce turbulence statistics
collected by simulating the trained L-LES model.

5.1 Smoothing Kernel

The learning of the SK (9) is discussed ﬁrst because it is a prerequisite for learning the
L-LES model (3) and for the following statistical analyses. We experiment with training
LF correspondent to diﬀerent choices of the coeﬃcients in Eq. (10). Figure (2) shows the

4We remind that d is bounded from above by the size of the box, i.e. L = 2π in the dimensionless units
of our DNS setting, and from below by the Kolmogorov (viscous) scale, η = O(ν3/4/ε1/4), where ν is the
(kinematic) viscosity coeﬃcient.

16

resulting shape of SK and its derivative as a function of r/h for two diﬀerent choices of
LF. We also show as a reference a cubic spline shape of SK popular in the SPH studies
[42]. We observe that the shape of the SK depends strongly on whether the gradient loss,
Lg, is included in the LF. Without Lg, the learned SK does not show the decay property
(that is the future empirically established, and thus widely adopted in the SPH studies [31])
and it shows a peak at r/h
0.4. On the other hand, when Lg is included in the LF, the
anticipated decay of the SK with increase in r is observed. This observation is signiﬁcant
because it explains that decay of the SK is instrumental for an accurate prediction of the
gradient ﬁeld.

≈

Figure 2: The learned smoothing kernels (a) and their derivatives to r/h for diﬀerent com-
binations of loss functions.

(cid:105)

(cid:105)

To examine qualitatively the reconstructed ﬁelds using the learned SK, we show in Fig.
vx
and its spatial derivative
(3) the two-dimensional contours of the ﬁrst velocity component
(cid:104)
(cid:104)∇xvx
along the same direction,
using out-of-sample testing data. Comparing Fig. 3 (a)-
(c), we observe that the large-scale features of the ﬂow can be reproduced using models
trained with or without the gradient-based loss functions. On the other hand, the qualities
of the reconstructed gradient ﬁeld vary dependent on the LF. As shown in Fig. 3 (d)-(e),
the gradient ﬁeld can not be correctly reproduced by models without accounting for the
gradient term in the LF. In general, we observe (not shown in the ﬁgures) that even though
the proﬁle of the SK has a weak eﬀect on the reconstruction quality of the ﬁeld it inﬂuences
very strongly the proﬁle of higher derivatives. This observation emphasizes the signiﬁcance
of choosing correct SK. A naive, i.e. not properly trained, choice of the smoothing kernel
has a signiﬁcant deterioration eﬀect on predicting coarse-grained ﬂows and their statistics.

5.2 Training and Testing L-LES model

5.2.1 Optimal Loss Function: A-priory Test

In this Section we follow the discussion of Section 3 and test the eﬀects on the training
results of the diﬀerent L-LES model Loss Function contributions (diﬀerent terms in Eq.

17

0.00.51.01.52.0r/h0.000.050.100.150.20Wθ(r)(a)0.00.51.01.52.0r/h−15−10−5051015∇rWθ(r)(b)Lf+Lg,cfcg=1Lf+Lg,cfcg=10Lf+Lg,cfcg=0.1Lfcubic B-splineFigure 3: Two-dimensional contours of the three-dimensional velocity and velocity gradient
ﬁeld at z = π reconstructed with the trained SK. Top row shows the x-component of the
velocity vector and bottom row shows spatial derivative of vx in x-direction. The recon-
structed ﬁeld using diﬀerent combinations of loss functions (b , c, e, f) are compared with
ground truth DNS data (a,d).

(20)). An exemplary a-priori test is set as follows: (a) We start with the trajectory-based
LF, i.e. with all coeﬃcients in Eq. (20) but ct set to zero. This choice of the starting point
is motivated by our empirical observation that the trajectory-based LF Lt shows a faster
training (decay rate of the LF). (b) When the decay rate saturates, we add to the LF all the
other ﬁeld and statistics-based contributions, Lf and LKLs. Notice that convergence of the
ﬁeld-based LF is signiﬁcantly slower (than that observed initially with the trajectory-based
loss). We attribute this observation to the additional layer of spatial averaging (summation
over multiple particles) in (17). (c) The training is stopped once the decay of the combined
LF is saturated.

We run a series of diagnostic tests of the L-LES model (3). As custom in ML, we
test the quality of the model training on the data not used in training. Speciﬁcally, we
test if the trained model is capable to predict evolution from unseen sections of the ﬂow
ﬁeld (interpolation) or ﬂows with diﬀerent characteristics (extrapolation). Fig. 4 (a) shows
comparison of the normalized L2 errors across diﬀerent cases.

To test the interpolation abilities of the model we apply it to the same turbulent model
0.08) however predicting ﬂows at times that are delayed to a diﬀerent degree with
(Mt ≈
respect to the ﬂow segment used for training. We observe that prediction errors of the model,
when applied with a delay, are very similar to the training errors. This indicates that the
models do not over-ﬁt and we can use it to interpolate in time.

18

02460246Y-axisvx, GT (a)02460246⟨vx⟩⟩Lf⟨Lg(b)02460246Lf(c)0246X-axis0246Y-axis∇xvx⟩GT(d)0246X-axis0246⟨∇xvx⟩⟩Lf⟨Lg(e)0246X-axis0246Lf(f)-0.18-0.14-0.11-0.07-0.040.000.040.070.110.14-0.39-0.29-0.19-0.090.010.110.210.310.410.51Figure 4: Performance on the trained L-LES model in interpolation and extrapolation errors
over delayed intervals and diﬀerent level of turbulent Mach number, respectively. We show
(a) Normalized L2 error; and (b) PDFs of the prediction.

Our second test is more demanding. We test if the model allows extrapolation, i.e.
generalizes well to new regimes with diﬀerent Mt. Note that the L-LES model in the form
of Eq. (3) is dimensionless. After selecting the proper velocity, density, and acceleration
scales based on the corresponding Mt, we are able to generalize the dynamics to diﬀerent
turbulence intensities and resolved scales. Results, testing diﬀerent Mt, correspondent to
diﬀerent values of the resolved scale are also shown in Fig. 4 (a). We observe that the model
extrapolates very well to the regimes with diﬀerent Mts.

Fig. 4 (b) reports a comprehensive comparison of the acceleration statistics for the afore-
mentioned interpolation and extrapolation experiments. We observe that the PDFs predicted
by the trained L-LES model match the ground truth data very well.

5.2.2 Ultimate Test of the L-LES

Once our choice of the Loss Function is validated (see preceding Subsection) we naturally
turn to an in-depth, robust, and thus ultimate (we may also call it a-posteriori ) test of the
Lagrangian features and prediction capabilities of the L-LES model.

Consider the following setup: 1) A random snapshot from the ﬁltered Ground Truth
(GT) DNS simulation, which has not been used in training, is selected. In this ultimate test,
we choose Mt = 0.16, a turbulent Mach number never used in the training of the L-LES
model. 2) Trained L-LES model (3) is initialized with 643 particles dispersed (at random) at
the locations xi with their properties φi interpolated using the respective Eulerian portion
of the GT data. 3) The external force in the L-LES model ((3)) is chosen similar to the one
implemented in the GT DNS Eulerian frame – the energy injection rate is set to match the
dissipation rate extracted from the decaying turbulence simulation. 4) We utilize the Varlet
integration method to calculate the properties and positions of the Lagrangian particles at
the next step, (xn+1
i =
1,
, N ). 5) This dynamic multi-particle simulation is run for teddy, which is the turnover
time of the turbulence energy containing scale. Then, we stop the simulation and perform

, N ) , given input from the previous step, (xn

i , φn
i |

i = 1,
|

, φn+1
i

· · ·

· · ·

i

19

Training,Mt=0.08tηteddyMt=0.16Mt=0.04Cases0.00.20.40.60.81.0NormalizedL2error(a)−0.2−0.10.00.10.20.3dφ/dt10−410−310−210−1100101102PDF(b)Training,Mt=0.08tηteddyMt=0.16Mt=0.04Figure 5: Two-dimensional contours of the three-dimensional velocity ﬁeld at plane z =
π and turbulent Mach number Mt = 0.16. The top row shows the evolution of the x
component of the velocity vector from the initial snapshot (a) to diﬀerent times scales,
including Kolmogorov time scale (b) and eddy turnover time (c, d). The bottom row shows
the reconstructed ﬁeld by the learned SK using predictions from the L-LES model from the
same initial condition.

dynamical and statistical analysis, some requiring interpolation of the Lagrangian particles
to the Eulerian grid (according to the previously learned Smoothing Kernel).

Fig. (5) shows the comparison of the two-dimensional contours at z = π of predicted and
GT velocity ﬁelds throughout the dynamic process just explained: the GT Eulerian ﬁelds at
diﬀerent moments of time are shown in the ﬁrst row and the reconstructed ﬁelds are shown
in the second row. We observe that early in the process, e.g. at the times comparable to
turnover time at the Kolmogorov scale, the contours match each other perfectly. Then, at the
times corresponding to roughly a half of the eddy turnover time the Lagrangian simulations
start to deviate from the exact trajectory of the GT ﬁeld at the smaller scales. However, the
large-scale structures of the two ﬂows (predicted vs GT) still match each other reasonably
well. When the time stamp approaches the eddy turnover time (right-most panel in Fig. (5)
we start to observe a mismatch.
In fact, the mismatch at the longest time stamp is not
surprising for the following two reasons. First, our L-LES is a reduced-order model and as
such it is not supposed to reproduce the dynamics of the truly chaotic system one-on-one.
We do expect deviation growth in time eventually reaching the magnitude comparable to
the ﬂow itself (just as seen in Fig. (5)). Second, the linear forcing is used in L-LES and is
an approximation of the DNS forcing, but not exactly the same, therefore causing not very
fast but still accumulation of the mismatch with time.

Results of the statistical analysis of the ultimate test setting are shown in Fig. (6). Here,
the two-dimensional contours of the vorticity and the turbulent kinetic energy (TKE) derived

20

01234560123456Ground Truthvx, Initial(a)01234560123456t(b)012345601234560.5teddy(c)01234560123456teddy(d)0123456X-axis0123456L-LESvx, Initial(e)0123456X-axis0123456t(f)0123456X-axis01234560.5teddy(g)0123456X-axis0123456teddy(h)-3.0-2.4-1.8-1.2-0.60.10.71.31.92.5Figure 6: Two-dimensional contours of the three-dimensional vorticity (a,c) and turbulent
kinetic energy (b,c) ﬁeld at the plane z = π and turbulent Mach number Mt = 0.16. The top
row shows the ground truth DNS ﬁeld at the resolved scale d at eddy turnover time and the
bottom row shows the ﬁelds constructed by the learned smoothing kernel using predictions
of the L-LES model from the same initial condition.

21

01234560123456Ground Truthz, teddy(a)01234560123456TKE, teddy(b)01234560123456L-LESz, teddy(c)01234560123456TKE, teddy(d)-2.9-2.2-1.4-0.70.00.71.42.22.90.00.61.21.82.43.03.64.24.8Figure 7: Normalized PDFs of the Lagrangian particle velocity and acceleration at diﬀerent
time scales. A reference Gaussian distribution (blue dotted line) is used for comparison.

from the test are juxtaposed with these extracted from the GT data. We observe a reasonable
qualitative agreement between the two.

Qualitative analysis of the velocity contours, reported in Fig. (5), shows the principal
capability of the L-LES approach to predict ﬂows fatefully. However, such capabilities are
also readily oﬀered by the classic Eulerian LES. To demonstrate that L-LES can capture
more physics, we turn now to a more quantitative analysis also emphasizing the Lagrangian
perspective of the approach.

Fig. (7) reports the results of the analysis capturing Lagrangian statistics of the turbulent
ﬂow. We show here the Probability Distribution Functions (PDFs) of the particle velocity
and acceleration as captured by L-LES (vs ground truth) at the diﬀerent time stamps (past
the period where the GT data was available for training). We observe in Fig. 7 (a) that the
PDF of the particle velocity (coarse-grained at the resolved scale) is close to Gaussian. We
also observe that L-LES predictions of the particle velocity at diﬀerent time stamps match
the respective GT data very well even if checked with a delay time (up to an eddy turnover
time later). Fig. 7 (b) shows similar comparison for the particle acceleration. Here again,
we report a very good quantitative reproduction of the acceleration statistics by L-LES even
when checked after evolving for one eddy turnover time.

Our statistical analysis so far was limited to relatively simple objects, e.g. velocity and
acceleration statistics at the resolved scale. Let us now turn to a more advanced diagnostics
and test the so-called Q-R plane statistics of the coarse-grained velocity gradient [8, 64].
R plane, where Q and R are second and third order invariants of the
Statistics in the Q
velocity gradient tensor coarse-grained at the resolved scale, d, is known to determine a
variety of important characteristics of turbulence, such as the ﬂow topology, deformation of
material volume, energy cascade, and intermittency.

−

To test L-LES ability to reconstruct the joint PDF of Q and R, we extract respective
statistics following these steps: 1) We get vi by running L-LES model (3) and then compute
vi applying the Smoothing Kernel (5,9). 2) We calculate the anisotropic part of the velocity
1
vi). Then the second and third invariants
3Tr(

∇
gradient tensor according to,
of the anisotropic part of the velocity tensor are computed from

vi −
∇

v∗i =

∇

∇

22

−4−2024v/σ(v)10−510−410−310−210−1PDF(a)−10−5051015a/σ(a)10−610−510−410−310−210−1100(b)GTModel,tηModel,0.5teddyModel,teddyFigure 8: Statistical geometry of turbulence, as revealed by the joint PDF of the invariants
of the coarse-grained velocity gradient (Q and R). Subﬁgure (a) shows the ground truth
(DNS). Subﬁgure (b) shows the PDF reconstructed for the moment equal to the beginning
of the training window. Subﬁgures (c-d) show the PDF reconstructed by the L-LES model
at the time stamps separated from the train window by τeddy/2 and τeddy, respectively. The
VGT statistics is computed using the learned smoothing kernel (see text for details).

Figure 9: Vector ﬁeld plot of the mean (dQ/dt, dR/dt) conditioned to the values of the VGT
invariants, Q, R, and averaged over Lagrangian particles, as extracted from the DNS data
coarse-grained over, d (a), and predicted by L-LES model (b).

23

−3−2−10123−3−2−10123Q∗GT(a)−3−2−10123−3−2−10123t0(b)−3−2−10123R∗−3−2−10123Q∗0.5teddy(c)−3−2−10123R∗−3−2−10123teddy(d)−3−2−101−0.6−0.4−0.20.00.20.40.6R∗−1.0−0.50.00.51.0Q∗GT(a)−0.6−0.4−0.20.00.20.40.6R∗−1.0−0.50.00.51.0L-LES(b)where

×

Qi =

Ri =

1
2
1
3

−

−

Tr(

Tr(

v∗i × ∇
v∗i × ∇

∇

∇

v∗i ),

v∗i × ∇

v∗i ),

i
∀

∈

1, ..., N

(21)

(22)

denotes matrix multiplication.

We calculate the second and third invariants for the particles at diﬀerent time stamps
from the L-LES predictions and aggregate them in a histogram/PDF, summing over particles.
The results are shown in Fig. (8) in the form of the iso-lines of the joint PDF of the invariants.
We observe the distinctive “tear-drop” shape, expressing complex dynamics of turbulence,
in all the subﬁgures of Fig. (8), consistent with what is referred to in the literature as
the statistical geometry of turbulence (see [8] and references therein). Speciﬁcally, Fig. 8
(a) shows the (Q, R) PDF from the ground truth (GT), DNS results. Fig. 8 (b) shows the
(Q, R) joint PDF reconstructed by L-LES at the moment of time corresponding to the initial
condition of L-LES simulation. Figs. 8 (c,d) show the joint (Q,R) PDFs predicted by L-LES
for the time stamp equal to half and one eddy turnover time (of the energy-containing scale),
respectively. It is evident that the tear-drop shape of the joint (Q,R) PDF is reproduced
by L-LES very well even at much later times, indicating the L-LES model is capable of
reproducing detailed long-time statistics of Lagrangian turbulence.

The ability of L-LES to predict Lagrangian trajectories allows for the study of much more
intimate features of turbulence, in particular statistics of the Velocity Gradient Tensor (VGT)
as seen from the Lagrangian frame, i.e. PDF of the VGT accumulated over Lagrangian
particles [64, 63]. The Lagrangian dynamics of the VGT projected to the (Q,R) plane is
shown in Fig. (9). Speciﬁcally, we show in Fig. (9) the mean (over Lagrangian particles)
vector of the time derivatives of the VGT invariants, ( dR
dt ), conditioned to the values of
the invariants. We observe that the vector ﬁeld prediction of the L-LES, shown in Fig. 9
(b), agrees very well with the results extracted from the GT/DNS data coarse-grained at
the resolved scale, d, shown in Fig. 9 (a). The agreement is especially impressive given that
we did not enforce the vector ﬁeld matching in the loss function used to train the L-LES.
In other words, the agreement between L-LES and DNS conﬁrms that the former learns
detailed Lagrangian features of turbulence from the latter accurately.

dt , dQ

Fig. 10 shows evolution of the energy spectra for two diﬀerent settings correspondent
to Mt = 0.08 and 0.16. Following our general logic, we train L-LES on the DNS/GT data
correspondent to Mt = 0.08 and then compare the energy spectra predictions at both Mt =
0.08 and Mt = 0.16 with the results extracted from the DNS/GT. The L-LES energy spectra
results are extracted from the Lagrangian (particles) data in two steps. First, we apply
the Smoothing Kernel (SK) to Lagrangian (particle) data to reconstruct the snapshot of the
velocity over the Eulerian grid, and then we compute the energy spectra (via standard Fourier
transform), also truncating it at the wave vector correspondent to the resolved scale, d. We
observe that at smaller scales, the energy spectra predicted by L-LES are slightly more intense
than the energy spectrum extracted from the DNS/GT. We explain this overestimation of
the turbulence intensity at the smaller scales by the fact that the inter-particle distance
ﬂuctuates, therefore leading to an additional energy stored at scales smaller than d. This is
in contrast with the Eulerian DNS, which is ﬁltered more precisely at the resolved scale, d.

24

Figure 10: The energy spectrum of turbulence ﬁeld as extracted from the Eulerian DNS/GT
data vs reconstructed by L-LES at diﬀerent time stamp. Subﬁgures (a) and (b) show com-
parisons at Mt = 0.16 and Mt = 0.08, respectively, while the training for both case is done
at Mt = 0.08.

Looking at the large-scale part of the spectra we observe that as we increase the prediction
time to teddy the most energetic mode decreases. We attribute this to the fact that the forcing
is not exactly the same in DNS and L-LES, i.e. the forcing term is applied in the spectral
domain in DNS, but in L-LES it is applied in the spatial domain.

6 Conclusions

Representing turbulent ﬂows with particles is advantageous because it potentially oﬀers
the ultimate model reduction — underlying physics becomes transparent and mathematics
simpler. In the case of passive scalar turbulence, it allows to express n-th order correlation
functions via dispersion of n particles. In the case of ”Burgulence”, i.e. turbulence described
by Burgers equations, it allows to focus attention on the dynamics and interaction of shocks.
This manuscript presents an attempt to carry over this Lagrangian logic to the much
more challenging case of Navier-Stokes turbulence. We show how to build a reduced model of
Homogeneous Isotropic Turbulence (HIT) in terms of a system of N co-evolving Lagrangian
markers/particles. Speciﬁcally, we claim fateful representation of HIT within the larger
scale portion of the inertial range, which extends upscale from the resolved scale, d, to the
energy-containing scale, L, in terms of at least N particles, where N = O((L/d)3)

1.

We call this new approach Lagrangian Large Eddy Simulation (L-LES), because it can
be viewed as a Lagrangian version of the classic and highly popular LES approach, which
was thus far solely Eulerian.

(cid:29)

One may as well argue that L-LES extends the growing list of particle-based approaches
to modeling complex dynamics of ﬂuids and materials. Insofar the Smooth Particle Hydro-
dynamics (SPH), which is arguably one of the most popular particle-based methods in ﬂuid
mechanics, is the L-LES closest ancestor.

L-LES is also an advanced instance of the Physics Informed Machine Learning (PIML)

25

100101Wavenumberk10−510−410−310−210−1E(k)(a)100101Wavenumberk10−510−410−310−210−1(b)GTReconstruced,t=0t=tη0.5teddyteddyapproach because it ﬁts the ground truth data with the dynamic model which is suﬃciently
loose but also consistent with our current physical interpretation of the geometry and dy-
namics of HIT.

L-LES, considered as a learning problem, is split in two parts. First, we learn/train the
Smoothing Kernel (SK), represented by Eqs. (9), which maps positions and velocities of the
Lagrangian particles to the velocity and density ﬁelds evaluated over the Eulerian grid. Then,
we ﬁt the L-LES model itself, represented by Eqs. (3). In both cases, the training is done
on the DNS data and we rely on Neural Networks to provide the best ﬁt to the degrees of
freedom remaining after all the physical considerations and mathematical constraints, such
as normalization of the SK, translational and rotational invariance, are taken into account.
Physics also enters into the design of the Loss Function (LF).

The SK-LF is built from a combination of terms enforcing SK normalization and also
matching the velocity ﬁeld and the velocity gradient ﬁeld with the SK applied to the respec-
tive ground truth (GT) Lagrangian data we obtained by advecting particles placed in the
ﬁltered (at the resolved scale) DNS. Our experiments show that combining all three terms
is critical for learning the SK.

The L-LES Eqs. (3), describing Lagrangian dynamics of the probe particles at the resolved
scale consistent with the ﬁltered NS equations and thus accounting for the ﬁltered stress
tensor, sub-ﬁlter, viscous, and forcing contributions, are at the core of our modeling. We
show that given the capability of a deep NN used to represent the remaining (and suﬃciently
large) freedom in the L-LES equations, the L-LES model is more inclusive than all previously
considered particle-based models of turbulence, including SPH. Three types of LFs were
introduced in order to train the L-LES model – trajectory-based, ﬁeld-based, and statistics-
based. The model is trained on the Eulerian and Lagrangian GT data extracted from DNS
simulations at turbulent Mach number Mt = 0.08. Various a-priori tests of the L-LES model
all show that the model is capable to predict accurately multi-particle dynamics in unseen
portions of the data at Mt = 0.08 and it also generalizes (extrapolates) well to regimes with
other values of Mt within the weakly compressible limit.

We also conducted a-posteriori tests of advanced Lagrangian physics of turbulence where
the model was not enforced directly. These tests/experiments included seeding Lagrangian
particles into unseen (in training) parts of the GT data and collecting advanced statistics,
e.g. on the energy spectra and invariants of the ﬁltered velocity gradients. All of the tests
have reported a quality agreement between the model and the GT.

Acknowledgement

This work was performed under the auspices of DOE. Financial support comes from Los
Alamos National Laboratory (LANL), Laboratory Directed Research and Development (LDRD)
project ”Machine Learning for Turbulence,” 20180059DR. LANL, an aﬃrmative action/equal
opportunity employer, is managed by Triad National Security, LLC, for the National Nuclear
Security Administration of the U.S. Department of Energy under contract 89233218CNA000001.

26

References

[1] See online links and materials from the series of LANL conferences, in Santa Fe, NM,

in 2018, 2020, 2022 on ”Physics Informed Machine Learning”.

[2] M. Antuono, S. Marrone, A. Di Mascio, and A. Colagrossi. Smoothed particle hydro-
dynamics method from a large eddy simulation perspective. generalization to a quasi-
lagrangian model. Phys. Fluids, 33(1):015102, 2021.

[3] J. R. Baltzer and D. Livescu. Variable-density eﬀects in incompressible non-buoyant

shear-driven turbulent mixing layers,. J. Fluid Mech., 900:A16, 2020.

[4] M. R. Bate, I. A. Bonnell, and N. M. Price. Modelling accretion in protobinary systems.

MNRAS, 277(2):362–376, Nov. 1995.

[5] W. Benz and E. Asphaug. Catastrophic disruptions revisited. Icarus, 142(1):5–20, Nov.

1999.

[6] G. Bicknell. The equations of motion of particles in smoothed particle hydrodynamics.

SIAM J. Sci. Stat. Comput., 12(5):1198–1206, 1991.

[7] V. Bromm, P. S. Coppi, and R. B. Larson. Forming the ﬁrst stars in the universe: The

fragmentation of primordial gas. ApJ Letters, 527(1):L5–L8, Dec. 1999.

[8] M. Chertkov, A. Pumir, and B. I. Shraiman. Lagrangian tetrad dynamics and the

phenomenology of turbulence. Phys. Fluids, 11(8):2394–2410, 1999.

[9] R. A. Dalrymple and B. Rogers. Numerical modeling of water waves with the SPH

method. Coast. Eng., 53(2-3):141–147, 2006.

[10] D. De Padova, M. Mossa, and S. Sibilla. SPH numerical investigation of the velocity
ﬁeld and vorticity generation within a hydrofoil-induced spilling breaker. Environ. Fluid
Mech., 16(1):267–287, 2016.

[11] L. Deng and X. Li. Machine learning paradigms for speech recognition: An overview.

IEEE Trans. Audio Speech Lang. Process., 21(5):1060–1089, 2013.

[12] A. Di Mascio, M. Antuono, A. Colagrossi, and S. Marrone. Smoothed particle hydrody-
namics method from a large eddy simulation perspective. Phys. Fluids, 29(3):035102,
2017.

[13] K. Duraisamy, G. Iaccarino, and H. Xiao. Turbulence modeling in the age of data.

Annu. Rev. Fluid Mech., 51:357–377, 2019.

[14] C. S. Frenk, S. D. M. White, P. Bode, J. R. Bond, G. L. Bryan, R. Cen, H. M. P. Couch-
man, A. E. Evrard, N. Gnedin, A. Jenkins, A. M. Khokhlov, A. Klypin, J. F. Navarro,
M. L. Norman, J. P. Ostriker, J. M. Owen, F. R. Pearce, U. L. Pen, M. Steinmetz, P. A.
Thomas, J. V. Villumsen, J. W. Wadsley, M. S. Warren, G. Xu, and G. Yepes. The
santa barbara cluster comparison project: A comparison of cosmological hydrodynamics
solutions. ApJ, 525(2):554–582, Nov. 1999.

27

[15] U. Frisch and A. Kolmogorov. Turbulence: The legacy of A. N. Kolmogorov. Cambridge

University Press, 1995.

[16] K. Fukami, K. Fukagata, and K. Taira. Super-resolution reconstruction of turbulent

ﬂows with machine learning. J. Fluid. Mech., 870:106–120, 2019.

[17] R. A. Gingold and J. J. Monaghan. Smoothed particle hydrodynamics: theory and

application to non-spherical stars. MNRAS, 181:375–389, Nov. 1977.

[18] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.

[19] M. Herant, W. Benz, W. R. Hix, C. L. Fryer, and S. A. Colgate. Inside the supernova:

A powerful convective engine. ApJ, 435:339, Nov. 1994.

[20] P. F. Hopkins. A new class of accurate, mesh-free hydrodynamic simulation methods.

MNRAS, 450(1):53–110, 2015.

[21] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-

informed machine learning. Nat. Rev. Phys., 3(6):422–440, 2021.

[22] R. King, O. Hennigh, A. Mohan, and M. Chertkov. From deep to physics-informed

learning of turbulence: Diagnostics. APS/DFD and arXiv:1810.07785, 2018.

[23] L. Ladick`y, S. Jeong, B. Solenthaler, M. Pollefeys, and M. Gross. Data-driven ﬂuid

simulations using regression forests. ACM Trans. Graph, 34(6):1–9, 2015.

[24] I. Lagaris, A. Likas, and D. Fotiadis. Artiﬁcial neural networks for solving ordinary and

partial diﬀerential equations. IEEE Trans. Neural Netw., 9(5):987–1000, 1998.

[25] L. Lapidus and G. F. Pinder. Numerical solution of partial diﬀerential equations in

science and engineering. John Wiley & Sons, 2011.

[26] A. Leroy, D. Violeau, M. Ferrand, and C. Kassiotis. Uniﬁed semi-analytical wall bound-
ary conditions applied to 2-D incompressible sph. J. Comput. Phys., 261:106–129, 2014.

[27] S. J. Lind, B. D. Rogers, and P. K. Stansby. Review of smoothed particle hydrodynamics:
towards converged lagrangian ﬂow modelling. Proc. Royal Soc. A, 476(2241):20190801,
2020.

[28] J. Ling, A. Kurzawski, and J. Templeton. Reynolds averaged turbulence modelling
using deep neural networks with embedded invariance. J. Fluid Mech., 807:155–166,
2016.

[29] G.-R. Liu and Y.-T. Gu. An introduction to meshfree methods and their programming.

Springer Science & Business Media, 2005.

[30] G.-R. Liu and M. B. Liu. Smoothed particle hydrodynamics: A meshfree particle method.

World scientiﬁc, 2003.

28

[31] M. Liu and G. Liu. Smoothed particle hydrodynamics (SPH): an overview and recent

developments. Arch. Comput. Methods Eng., 17(1):25–76, 2010.

[32] M. Liu and G.-R. Liu. Restoring particle consistency in smoothed particle hydrody-

namics. Appl. Numer. Math., 56(1):19–36, 2006.

[33] D. Livescu. Turbulence with large thermal and compositional density variations. Annu.

Rev. Fluid Mech., 52:309–341, 2020.

[34] E. Y. Lo and S. Shao. Simulation of near-shore solitary wave mechanics by an incom-

pressible SPH method. Appl. Ocean Res., 24(5):275–286, 2002.

[35] A. Mayrhofer, D. Laurence, B. Rogers, and D. Violeau. DNS and LES of 3-D wall-
bounded turbulence using smoothed particle hydrodynamics. Comput. Fluids, 115:86–
97, 2015.

[36] A. T. Mohan, D. Daniel, M. Chertkov, and D. Livescu. Compressed convolutional
lstm: An eﬃcient deep learning framework to model high ﬁdelity 3d turbulence.
arXiv:1903.00033, 2019.

[37] A. T. Mohan, N. Lubbers, D. Livescu, and M. Chertkov. Embedding hard physical
constraints in neural network coarse-graining of 3d turbulence. arXiv:2002.00021, 2020.

[38] A. T. Mohan, D. Tretiak, M. Chertkov, and D. Livescu. Spatio-temporal deep learning
models of 3d turbulence with physics informed diagnostics. J. Turbul., 21:484–524, 2020.

[39] J. Monaghan. Smoothed particle hydrodynamics. Annu. Rev. Astron. Astrophys.,

30(1):543–574, 1992.

[40] J. J. Monaghan. SPH compressible turbulence. MNRAS, 335(3):843–852, Sept. 2002.

[41] J. J. Monaghan. Smoothed particle hydrodynamics and its diverse applications. Annu.

Rev. Fluid Mech., 44:323–346, 2012.

[42] J. J. Monaghan and J. C. Lattanzio. A reﬁned particle method for astrophysical prob-

lems. Astron. Astrophys., 149:135–143, 1985.

[43] J. M. Owen, J. V. Villumsen, P. R. Shapiro, and H. Martel. Adaptive smoothed par-
ticle hydrodynamics: Methodology. ii. The Astrophysical Journal Supplement Series,
116(2):155, 1998.

[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,
A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An
imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

[45] M. Petersen and D. Livescu. Forcing for statistically stationary compressible isotropic

turbulence. Phys. Fluids, 22(11):116101, 2010.

29

[46] G. D. Portwood, P. P. Mitra, M. D. Ribeiro, T. M. Nguyen, B. T. Nadiga, J. A. Saenz,
M. Chertkov, A. Garg, A. Anandkumar, A. Dengel, et al. Turbulence forecasting via
neural ode. arXiv:1911.05180, 2019.

[47] G. D. Portwood, B. T. Nadiga, J. A. Saenz, and D. Livescu. Interpreting neural network

models of residual scalar ﬂux. J. Fluid Mech., 907, 2021.

[48] D. J. Price. Resolving high Reynolds numbers in smoothed particle hydrodynamics

simulations of subsonic turbulence. MNRAS: Letters, 420(1):L33–L37, 2012.

[49] D. J. Price. Smoothed particle hydrodynamics and magnetohydrodynamics. J. Comput.

Phys., 231(3):759–794, Feb. 2012.

[50] D. J. Price, J. Wurster, T. S. Tricco, C. Nixon, S. Toupin, A. Pettitt, C. Chan, D. Men-
tiplay, G. Laibe, S. Glover, C. Dobbs, R. Nealon, D. Liptai, H. Worpel, C. Bonnerot,
G. Dipierro, G. Ballabio, E. Ragusa, C. Federrath, R. Iaconi, T. Reichardt, D. For-
gan, M. Hutchison, T. Constantino, B. Ayliﬀe, K. Hirsh, and G. Lodato. Phantom:
a smoothed particle hydrodynamics and magnetohydrodynamics code for astrophysics.
Publications of the Astronomy Society of Australia, 35:e031, Sept. 2018.

[51] M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial
diﬀerential equations. J. Comput. Phys., 378:686–707, 2019.

[52] D. Rennehan. Mixing matters. MNRAS, 506(2):2836–2852, Sept. 2021.

[53] S. Rosswog. Modelling astrophysical ﬂuids with particles.

arXiv e-prints, page

arXiv:2201.05896, Jan. 2022.

[54] S. Rosswog, M. Liebend¨orfer, F. K. Thielemann, M. B. Davies, W. Benz, and T. Piran.
Mass ejection in neutron star mergers. Astronomy and Astrophysics, 341:499–526, Jan.
1999.

[55] J. Ryu and D. Livescu. Turbulence structure behind the shock in canonical shock–

vortical turbulence interaction. J. Fluid Mech., 756:R1, 2014.

[56] P. Sagaut. Large eddy simulation for incompressible ﬂows: an introduction. Springer

Science & Business Media, 2006.

[57] C. Schenck and D. Fox. Spnets: Diﬀerentiable ﬂuid dynamics for deep neural networks.

In Conference on Robot Learning, pages 317–335. PMLR, 2018.

[58] N. Sebe, I. Cohen, A. Garg, and T. S. Huang. Machine learning in computer vision,

volume 29. Springer Science & Business Media, 2005.

[59] M. S. Shadloo, G. Oger, and D. Le Touz´e. Smoothed particle hydrodynamics method for
ﬂuid ﬂows, towards industrial applications: Motivations, current state, and challenges.
Comput. Fluids, 136:11–34, 2016.

30

[60] V. Springel. The cosmological simulation code GADGET-2. MNRAS, 364(4):1105–1134,

Dec. 2005.

[61] G. Stinson, A. Seth, N. Katz, J. Wadsley, F. Governato, and T. Quinn. Star forma-
tion and feedback in smoothed particle hydrodynamic simulations - I. Isolated galaxies.
MNRAS, 373(3):1074–1090, Dec. 2006.

[62] H. Tennekes and J. Lumley. A ﬁrst course in turbulence. MIT PRESS, 1978.

[63] Y. Tian, F. A. Jaberi, and D. Livescu. Density eﬀects on post-shock turbulence structure

and dynamics. J. Fluid Mech., 880:935–968, 2019.

[64] Y. Tian, D. Livescu, and M. Chertkov. Physics-informed machine learning of the la-
grangian dynamics of velocity gradient tensor. Phys. Rev. Fluids, 6(9):094607, 2021.

[65] B. Ummenhofer, L. Prantl, N. Thuerey, and V. Koltun. Lagrangian ﬂuid simulation
with continuous convolutions. In International Conference on Learning Representations
(ICLR), 2019.

[66] D. Violeau and R. Issa. Numerical modelling of complex turbulent free-surface ﬂows
with the SPH method: An overview. Int. J. Numer. Methods Fluids, 53(2):277–304,
2007.

[67] J.-X. Wang, J.-L. Wu, and H. Xiao. Physics-informed machine learning approach for
reconstructing reynolds stress modeling discrepancies based on DNS data. Phys. Rev.
Fluids, 2(3):034603, 2017.

[68] Z. Wang, H. Li, W. Ouyang, and X. Wang. Learnable histogram: Statistical context
features for deep neural networks. In European Conference on Computer Vision, pages
246–262. Springer, 2016.

[69] M. Woodward, Y. Tian, C. Hyett, C. Fryer, D. Livescu, M. Stepanov, and M. Chertkov.
Physics informed machine learning of SPH: Machine learning lagrangian turbulence.
arXiv:2110.13311, 2021.

[70] T. Ye, D. Pan, C. Huang, and M. Liu. Smoothed particle hydrodynamics (SPH) for
complex ﬂuid ﬂows: Recent developments in methodology and applications. Phys.
Fluids, 31(1):011301, 2019.

[71] P. K. Yeung. Lagrangian investigations of turbulence. Annu. Rev. Fluid Mech.,

34(1):115–142, 2002.

31

