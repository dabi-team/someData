1
2
0
2

l
u
J

6
1

]

C
H
.
s
c
[

1
v
1
4
1
8
0
.
7
0
1
2
:
v
i
X
r
a

© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

An Automated Approach to Reasoning About Task-Oriented
Insights in Responsive Visualization

Hyeok Kim, Ryan Rossi, Abhraneel Sarma, Dominik Moritz, and Jessica Hullman

Abstract— Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques
when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter
relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve
while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for
task-oriented visualization insights (identiﬁcation, comparison, and trend) in responsive transformation of a source visualization. We
operationalize identiﬁcation, comparison, and trend loss as objective functions calculated by comparing properties of the rendered
source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine
learning models on human ranked small screen alternative visualizations across a set of source visualizations. We ﬁnd that our
approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype
responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates
the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of
automated and semi-automated responsive visualization recommendation.

Index Terms—Task-oriented insight preservation, responsive visualization

1 INTRODUCTION

Visualization authors often transform their designs to accommodate
different audience, styles, or display types. For example, authors might
simplify charts for audiences with different graphical literacy, altering
information conveyed in the new design [14]. Authors of responsive
visualizations create multiple designs for different screen sizes and
interactivity [30, 43]. However, transforming visualizations may invoke
trade-offs between desirable design criteria. For instance, in Fig. 1,
proportionate rescaling (a) makes it harder to compare values along the
y-axis due to the reduced absolute height, while increasing the relative
height (b) or transposing (c) can distort the shape of the represented
distribution. Increasing the bin size (d) reduces possible comparisons
and a viewer’s ability to see distributional detail.

Unfortunately, design trade-offs make it difﬁcult to reason about pre-
serving takeaways or “insights” under visualization design transforma-
tions. Designers may need to iteratively try out different combinations
of strategies like those in Fig. 1 and compare them with the original
design [30]. Kim et al. [43] identify trade-offs in designing responsive
visualization where authors try to strike a balance between maintaining
graphical density (i.e., an appropriate number of marks per unit screen
size) and the preservation of a user’s ability to arrive at certain insights.
For example, authors need to decide either to preserve distributional
characteristics with higher visual density by rescaling proportionately
(a) or to adjust visual density while losing distributional details by
changing the bin size (d). Currently these decisions are made manually.
We contribute an automated approach to approximating the amount
of change to task-oriented insights—insights that viewers are likely
to be able to obtain from a visualization by performing visual tasks—
under design transformation. We deﬁne measures that approximate
a visualization’s support for three low-level visual analytic tasks dis-
cussed in the literature [2, 7]: the viewer’s ability to identify a datum, to

• Hyeok Kim, Abhraneel Sarma, and Jessica Hullman are with Northwestern

University. E-mail: hyeokkim2024@u.northwestern.edu,
abhraneelsarma2024 @u.northwestern.edu, jhullman@northwestern.edu.

• Ryan Rossi is with Adobe Research. E-mail: ryrossi@adobe.com.
• Dominik Moritz is with Carnegie Mellon University. E-mail:

domoritz@cmu.edu.

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx

Fig. 1. Example responsive transformations for small screen generated
from a large screen design: (a) proportionate rescaling, (b) dispropor-
tionate rescaling, (c) transposing axes, and (d) increasing bin size.

compare pairs of data points, and to perceive a multivariate trend. We
demonstrate the use of our measures for choosing between alternative
transformations of a source large screen visualization in a responsive
design context. We provide a prototype automated design recommender
for responsive visualization that enumerates responsive design trans-
formations based on an input source view and reasons about changes
in task-oriented insights from the source design to each transformed
design. Our recommender supports scatterplots, bar charts, line graphs,
and heatmaps with position, color, size, and shape encoding channels.
We train and test different machine learning (ML) models based on
our measures to evaluate their utility for automatically ranking small
screen visualization design alternatives given a large screen view. We
achieve up to 84% accuracy (via a random forest model) in ranking
a set of responsive transformations across a set of six source large
screen views spanning different encoding channels. Models trained
with our measures outperform two baseline models based on simple
heuristics related to chart size changes (59%) and transposing of axes
(63%). We discuss implications of our work for future research, in-
cluding recommender-driven responsive visualization authoring and
generalizing our approach to further visualization design domains such
as simpliﬁcation and style transfer.

2 RELATED WORK

2.1 Responsive Visualization and Design Transformations

We use visualization design transformation to refer to the transforma-
tion of a source visualization speciﬁcation to a new visualization speci-
ﬁcation intended to better achieve certain context-speciﬁc constraints.
These might be screen size limitations for responsive visualization,
audience-related constraints in visualization simpliﬁcation or audience
retargeting (e.g., [5, 40]), or style constraints in style transfer [24, 25],
for example. Visualization design transformation differs from creating

1

Large screen designResposive transformations for small screen(a) (d) (b) (c)  
 
 
 
 
 
multiple different views from the same dataset (e.g., a visualization
sequence or dashboard) in that transformations of an original source
view typically are intended to preserve many properties of the source
while changing select properties.

Prior work on responsive visualization, which tends to focus on Web-
based communicative visualization, or scalable visualization [13] more
broadly, emphasizes the importance of maintaining intended takeaways
between source and transformed views. Analyzing 378 responsive visu-
alization pairs on desktop and mobile devices, Kim et al. [43] identify
density-message trade-offs in responsive visualization where authors
need to balance adjusting visual density or complexity for different
screen types while maintaining patterns, trends or other important in-
formation conveyed in the source view. Focusing on maintaining key
information at different scales, earlier work on visualization resizing
introduces algorithms that repeatedly remove the pixels determined to
be least important [18] and iteratively minimize scaling in more salient
regions [76], for example. We extend prior approaches by proposing
approximation methods for task-oriented visualization insights.

Defaulting to simpler views over complex, over-encoded plots is
often recommended when exploring or publicizing complex data [41].
Authors accomplish this through data-level transformations, such as
data abstraction, clutter reduction, ﬁltering, or clustering. For example,
data abstraction studies have attempted to enhance the simplicity of a
view while preserving original structure or insights (e.g., aggregating
a large movement dataset [1], using interactive dimensionality reduc-
tion [39], using hierarchical aggregation [19], and measuring the quality
of an abstraction [14]).

2.2 Visualization Recommendation

We discuss two approaches in visualization recommendation—insight-
based and similarity-based—that are relevant to our goal of approxi-
mating changes in task-oriented insights. Prior work on visualization
recommendation employs statistical calculations to characterize proper-
ties of a visualization thought to relate to the insights a user can draw
from it. Often these ‘insights’ are intended to capture how well a user
can perform analytic tasks, such as recognizing trends or identifying
and comparing data points. Tang et al. [65] suggest detecting ‘top-k
insights’ from data using statistical signiﬁcance testing (e.g., low p-
value of a linear regression coefﬁcient for slope insight). Similarly,
Foresight [17], DataSite [15], and Voder [61] use statistics calculated
on the data, such as correlation coefﬁcient and interquartile range, and
recommend visualization types predicted to better support extracting
such information. However, statistics on data are invariant for views
sharing the same data set and hence of limited use for comparing dif-
ferent ways of visualizing the same underlying data. Our work instead
considers statistics calculated on the rendered visualization.

Several prior visualization recommenders model similarity between
views, but assume a scenario where the underlying dataset changes.
GraphScape [45] offers a view similarity model that assigns costs to
visualization pairs that are intended to approximate the cognitive cost
of transitioning from one view to another in a visualization sequence.
GraphScape applies an a priori cost model in which data transformation
(e.g., binning, modifying scales) is always less costly than changes
in encoding. Hence, ﬁltering data has a lower cost than transposing
axes. However, ﬁltering operations like removing a bar from a bar
chart or rescaling a y-axis can signiﬁcantly change the presumed “take-
aways” of a chart (e.g., [23, 31]). The space of transformations covered
by GraphScape also does not include view size transformations, so it
cannot assign costs to changes in aspect ratio common to responsive
visualization. Although Dziban [47] extends GraphScape to suggest
a view that is ‘anchored’ to the previous view for an exploratory data
analysis process, it also assumes different subsets of data between the
previous and current views and focuses more on similar chart encodings
than on preserving task-oriented insights.

2.3 Comparing Visual Structure by Processing Signal

Signal processing-based approaches analyze the underlying visual or
perceptual structure of a visualization to enable multi-scale visual-
izations (i.e., providing different insights at different scales) and to

2

Fig. 2. A pipeline for a responsive visualization recommender

enhance visualization effectiveness. Prior work has attempted to enable
multi-scale views through perceptual organization analysis of a informa-
tion graphic at each scale [72, 73] and hybrid-image visualization that
displays different aggregation levels at different viewing distances [35],
for example. Signal processing approaches have also been applied to
improve the effectiveness of a visualization, for instance, by measur-
ing the difference between the visual salience of a representation and
salience of signals in data [37, 46], comparing kernel density estima-
tions between a LOESS curve and different representations [71], and
extending a structural similarity index for image compression to data
visualization [68]. Signal processing-based approaches have typically
been applied to single views, and are generally conﬁned to a prede-
ﬁned set of marks and visual variables (e.g., a line chart, a scatterplot),
restricting their applicability for settings like ours.

3 PROBLEM FORMULATION

We propose formulating responsive visualization as a search problem
from an input source view to transformed target views, following the
characterization proposed by Kim et al. [43]. Consider a recommender
that takes a source desktop view as input and returns a ranked set of
targets as illustrated in Fig. 2. The ﬁrst step in creating such a rec-
ommender is to deﬁne a search space that can enumerate well-formed
responsive targets. To generate useful target views from a source (large
screen) visualization, a search space should cover common transforma-
tion strategies in responsive visualization, such as rescaling, aggregat-
ing, binning, and transposing [30, 43].

After enumerating target views, a responsive visualization recom-
mender should evaluate how well each target preserves certain infor-
mation or “insights.” While the term insight can be overloaded [77],
a relatively robust way to deﬁne insights comes from typologies for
describing visualization judgments or patterns [2, 7]. These typologies
suggest deﬁning insights around common low-level visual analysis
tasks like identifying and comparing data. In an automated design
recommendation scenario, these task-oriented insights can be approxi-
mated by objective functions (i.e., loss measures) that capture support
for common tasks, applied to both the source and target view. Finally,
the recommender returns the set of target designs based on how well
they minimize these loss measures. We formalize this problem and mo-
tivate and deﬁne three loss measures that we call task-oriented insight
preservation measures. In Sect. 5, we describe a prototype visualization
recommender in which we implemented the approach.

3.1 Notation

We deﬁne a visualization (or a view), V , as a three tuple

V = [DV ,CV , EV ],

(1)

where DV is the data used in V , CV is a visualization speciﬁcation
(deﬁning encodings, chart size, mark type, etc.), and EV is a set of ren-
dered values that we compute our measures on. For example, suppose a
bivariate data set with GDP and GNI ﬁelds (i.e., DV = {x1, x2, . . . , xn},
where xi = (xi.GDP, xi.GNI)). CV maps GDP and GNI to x and y
positions of point marks, respectively, producing a scatterplot. The
corresponding set of rendered values is a set of Cartesian coordinates
on the XY-plane (i.e., EV = {e1, e2, . . . , en}, where ei = (ei.x, ei.y) is
the tuple of rendered values for xi). Similarly, for a data set containing
a ﬁeld CO2 (emission) that is mapped to color, ei.color would corre-
spond to the rendered value of xi.CO2. For brevity, we deﬁne DV .ﬁeld

Input: a source (large screen view)Enumerate well-formedalternativesSearchspaceObjectivefunctionsTargets(small screen views)Evaluate task-orientedinsight preservationOutput: ranked targets© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

Fig. 3. Our notation for a visualization. Rendered values are deﬁned in
the space implied by the visual variable (e.g., pixel space for position or
size, color space for color).

as a vector of ﬁeld values and EV .channel as a vector of rendered values
in channel. Our notation is also illustrated in Fig. 3

Given a source view S and a transformation (or target) T, we repre-

sent the loss of insight type M from S to T as below:

outcome space, where the probability of US.c taking x is deﬁned as the
relative frequency of x in ES.c, formalized as

P(US.c = x) = Counti(ei.c = x)/n

H(ES.c) = −∑
x

P(US.c = x) log2 P(US.c = x)

(3)

(4)

We can similarly compute the probabilities of rendered values,
P(UT.c), and the entropy of an encoding channel, H(ET.c), for a target
view T. Finally, we can calculate the identiﬁcation loss for the channel
as the absolute difference in entropy (i.e., |H(ES.c) − H(ET.c)|), where
0 difference is the identity. The ﬁnal identiﬁcation loss from S to T is
the sum of absolute differences in entropy for each encoding channel c
between the two views:

Loss(S → T; Identiﬁcation) = ∑
c

|H(ES.c) − H(ET.c)|,

(5)

Loss(S → T; M)

(2)

4.2 Comparison Loss

For example, Loss(S → T; Trend) indicates trend loss from S to T.

4 TASK-ORIENTED INSIGHT PRESERVATION MEASURES

High level criteria for preserving task-oriented insights of a visualiza-
tion include preserving datum-level information, maintaining compa-
rability of data points, and preserving the aggregate features [7]. We
use these distinct classes of information to deﬁne task-oriented insight
loss measures for approximating how well a responsive transformation
preserves support for low-level tasks of identifying data, comparing
data, and identifying trend. Our goal is to deﬁne a small set of measures
that capture important types of low-level tasks a designer might wish to
preserve in responsive transformation. Each measure should be distinct
(i.e., mostly independent of the others) and should improve accuracy
when combined with the others (such as through regression or ML
modeling) to predict human judgments about how visualization trans-
formations rank. Together, the measures should outperform reasonable
baseline approaches based on simple heuristics. While chosen to cover
three important classes of low-level analytic task, the measures we
describe are not meant to be exhaustive, as there are many ways one
could approximate support for task-oriented insights.

4.1 Identiﬁcation Loss

Responsive visualization strategies often alter the number of visual
attributes of marks that viewers can identify (affecting a low-level
identiﬁcation task [2, 7]). As illustrated in Fig. 4, when the number of
bin buckets of a histogram is decreased in a mobile view (a), each bar
encodes more information on average than in the desktop view, such that
some information about the distribution is lost. Similarly, strategies to
adjust graphical density, like aggregating distributions (b) and ﬁltering
certain data (c), also reduce the number of identiﬁable attributes. We use
identiﬁcation loss to refer to changes to the identiﬁability of rendered
values between a source view and a target.

Information theory, and in particular Shannon Entropy (entropy, here-
after) captures the information in a signal by measuring the minimum
number of bits needed to encode it [59]. Given a random variable X,
entropy is deﬁned as H(X) = − ∑x∈X P(x) log2 P(x). Applying this to
visualization, suppose that for a source visualization S, a vector for
data ﬁeld f , DS.f = {x1.f, . . . , xn.f}, is mapped to an encoding channel
c. The corresponding rendered values ES.c = {e1.c, . . . , en.c} compose
a random variable US.c that takes the set of unique values of ES.c as its

Responsive transformations like resizing or scaling a view or aggregat-
ing data can alter the number of possible data comparisons that a user
can make and how perceptually difﬁcult they are (affecting a low-level
comparison task [2, 7]). For instance, in Fig. 5, resizing (a) diminishes
the magnitude of difference between two highlighted data points in the
small screen design. In a mobile design with aggregation (b), viewers
are no longer able to make each comparison that is available in the
large screen view. This motivates estimating how similarly viewers are
able to discriminate between pairs of points in a target view compared
to the source view, which we refer to as comparison loss.

Empirical visualization studies (e.g., [44, 64]) often operationalizes
accuracy as the viewer’s ability to perceive relationships between pairs
of values. While simpler scalar statistics like a sum or mean might
sufﬁce under some transformations, a method that preserves the distri-
bution of distances will be more robust to transformations that change
the number of data points or scales (e.g., log-scale). We operationalize
comparison loss as the difference in pairwise discriminability, mea-
sured using Earth Mover’s Distance (EMD), between the source and a
target in each encoding channel used in a visualization:

Loss(S → T; Comparison) = ∑
c

EMD(BS.c, BT.c(cid:48)),

(6)

where BS.c and BS.c(cid:48) are the discriminability distributions of the source
and target views in encoding channel c and c(cid:48), respectively, that encode
the same data ﬁeld.

Given a source visualization S, we deﬁne the discriminability dis-
tribution BS.c, of an encoding channel c for a view S, as the set of
distances between each pair of rendered values (ES.c) of S in terms of
c. This is formalized as

BS.c = {dc(ei.c, e j.c) : ei.c, e j.c ∈ ES.c},

(7)

where dc(·, ·) is a distance metric for the encoding channel c.
Distance metrics: Ideally, comparison loss should account for differ-
ences in how well visual channels support perception of numerical
values. Informed by visual perception models, we select several dis-
tance metrics intended to provide a rough proxy of the perceptual
difference between two visual signals. While visual variables can have
interaction effects [9, 60, 64], for simplicity in demonstrating our ap-
proach, we limit our use of perceptual distance metrics to encoding

Fig. 4. Responsive transformations that may cause identiﬁcation loss.

Fig. 5. Responsive transformations that may cause comparison loss.

3

CV: speciﬁcationGDPGNIEV: rendered valuesDV: data valuesV: a visualization=++xi3.6B3.5Bx12.4B2.5Bx22.9B2.8Bx3.........xi .GNIxi .GDPei .yei .xei8987e16567e27573e3.........Size: 100×100Mark: pointx: GDPy: GNI...DV.GDP = {3.5B, 2.5B, 2.8B, ...}    EV.x = {87, 67, 73, ...}x=73y=75e3(c) Filtering datan=19n=13(a) Increasing bin sizen=20n=7(b) Aggregatingn=23n=7LargeSmall(c) RescalingNumber of comparisonsMagnitude of differencecomparable(b) Aggregating(a) ResizingMagnitude of differenceLargeSmall∆=112∆=10350px15methods (e.g., the difference between regression coefﬁcients) might
ignore. We deﬁne trend models for the quantitative encoding channels
in our scope (position, color and size):

• ey ∼ ex: a 2D trend of y on x as appears in a simple scatterplot,

line chart, or bar graph.

• ecolor ∼ ex + ey: a 3D trend of color on x and y like a heatmap or

a scatterplot with a continuous color channel

• esize ∼ ex + ey: a 3D trend of size on x and y (e.g., a scatterplot

with a continuous size encoding)

After calculating trend models for a source and target, we can deﬁne
trend loss as the sum of the relative area between curves (or volume
between surfaces) of the estimated trends in each trend model (m). This
is formalized as:

Loss(S → T; Trend) = ∑
m

A(LOESS(mS), LOESS(mT))

(12)

where A stands for the relative area between curves (ABC) between
the source and target trends (mS and mT), normalized by dividing by
the area under the curve of the source trend for a 2D model. For a 3D
model, A is the relative volume between surfaces (VBS), which is the
VBS of the source and target trends divided by the volume under the
surface of the source trend.

We estimate the trend models using LOESS regression [12] as it is
non-parametric. We use uniform weights and bandwidth of 0.5 [12].
LOESS regression returns an estimate at each observed value of the
independent variable(s) (as an array of coordinates): an estimated
curve for a 2D model and an estimated surface for a 3D model. Thus,
when source and target views have different chart sizes or different
sets of rendered values for the independent variable(s), it is difﬁcult
to directly compare the LOESS estimations. As shown in Fig. 7a, we

channel speciﬁc measures. However, as the state-of-the-art in predict-
ing effects of visual variable interactions develops, our approach could
be amended to consider combinations.

For position channels, we use the absolute difference between two
position values (in pixel space), as human vision is highly accurate in
discriminating positions according to Stevens’ power law [62, 63] and
empirical studies [27, 28]:

dposition(ei.position, e j.position) = |ei.position − e j.position|

(8)

We measure distance in a size channel using the absolute difference
between two size values (in pixel) raised to the estimated Stevens’
exponent of 0.7 [62, 63]:

dsize(ei.size, e j.size) = |ei.size − e j.size|0.7

(9)

We calculate the Euclidean distance in the perceptual color space

CIELAB [20] (CIELAB 2002):

dcolor(ei.color, e j.color) =
(cid:113)

(ei.L − e j.L)2 + (ei.a − e j.a)2 + (ei.b − e j.b)2,

(10)

where L, a, and b represent L∗, a∗, and b∗ in CIELAB space.

Lastly, for shape encodings, we employ a perceptual kernel [16],
a (symmetric) matrix of pairwise distances between visual attributes.
The i, j-th element in the perceptual kernel for shape is the empirical
probability of discriminating shape i from shape j based on an online
crowdsourced experiment in which workers completed a triplet discrim-
ination task where they chose the most dissimilar shape out of three
shapes. Formally, our shape distance metric can be stated as:

dshape(ei.shape, e j.shape) =

P(ei.shape is discriminated from e j.shape)

(11)

Comparing discriminability distributions: To quantify the discrep-
ancy between the discriminability distributions of encoding channel c
and c(cid:48) (mapping the same ﬁeld) for the source S and target T (i.e., BS.c
and BT.c(cid:48), respectively), we compute Earth Mover’s Distance [69]
(EMD or Wasserstein distance). We use EMD, which measures the
minimum cost to transform a distribution to another distribution, be-
cause it is non-parametric, symmetric, and unbounded. An EMD of 0
is the identity, and the greater the EMD is, the more different the two
distributions are. Thus, the comparison loss between the source view S
and a target view T is the sum of the EMD between their discriminabil-
ity distributions in each encoding channel, formalized in Equation 6.

4.3 Trend Loss
Responsive transformations like disproportionate rescaling and changes
to binning may impact the implied relationship (or trend) between two
or more variables represented in a target view compared to the source
view (affecting low-level trend identiﬁcation [7]). As shown in Fig. 6,
different aspect ratios can alter the magnitude of the slope of a trend,
and modifying bin size affect the amount of distributional information
available. We use trend loss to refer to changes in the implied trend
from the source to a target.

To capture representative data patterns while avoiding inﬂuences of
noise, our trend loss ﬁrst estimates trend models between the source and
target views using LOESS. We then compare the area (or volume) of
the estimated trends because it is more sensitive to details that simpler

Fig. 6. Responsive transformations motivating trend loss.

Fig. 7. Components of computing trend loss.
(a) Calculating area
between curves by standardizing chart size and interpolating break
points. (b) Dividing and matching subgroups. (c) Linearizing color scale.
LS is large screen, and SS is small screen.

4

Large screena1 + b2(a) Changing aspect ratioSmall screen(b) Changing       bin buckeks12312SSLSDifferent absolute sizesbetween LS and SS viewsStandardized to havethe same width(a) Computing relative area between curves (A(LOESS(mS), LOESS(mT))Step 1. Standardize (rescale chart width)Step 2. InterpolateOriginal estimatesInterpolated pointsInterpolated area between curvesArea under curveof source trendArea between curvesLOESS curvesLOESSCompute areaInterpolateLOESS estimates do not result in equidistant points along the x axis, so we interpolate to obtain equidistant points.(b) SubgroupingMatchedUnmatchedMatchedLSSSLSSS(c) Color scale linearization012.8326.6348.0668.6789.38110.68131.21153.41180.87206Linearized0102030405060708090100Data value (46.84, -15.95, -18.83) = ei-1.colorei.color = (54.50, -30.90, -6.71)∆=20.71When a nominal encoding divides points into subgroups, we match those subgroups in source and target views.We recursively accumulate the distances between each consecutive pair of rendered values.© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

Fig. 8. Prototype pipeline. (1) The full speciﬁcation of an input source view in ASP. (2) Enumerating targets by extracting a partial speciﬁcation of the
source view and generating a search space using an ASP solver. (3) Evaluating targets by computing our loss measures and ranking them using a
model trained on human-produced rankings. (4) Ranked targets.

ﬁrst standardize the chart sizes of two views by rescaling an estimated
LOESS curve or surface in a target view to have the same chart width
with the source. Then, we interpolate the LOESS curve to have equal
distances between two consecutive coordinates for a 2D model (Fig. 7b).
We interpolate on 300 breakpoints in a 2D model by default, where
one breakpoint corresponds to one to three pixels in many Web-based
visualizations. For a 3D model, we interpolate 300 × 300 breakpoints
from a LOESS surface in a similar way. Given these interpolations for
the LOESS curves (or surfaces) in the source and target, we obtain the
ABC (or VBS) segment at each breakpoint.
Subgroups: When a nominal variable encoded by color or shape di-
vides the data set into subgroups, viewers might naturally consider
each subgroup’s trend independently. To distinguish trends implied by
subgroups, we ﬁrst identify and match subgroups which occur in both
the source and target views by looking at their nominal data values, as
depicted in Fig. 7b. Then, we compute the relative ABC (or VBS) of
each subgroup and combine them by taking their average.
Color scale linearization: Although a continuous color scale encodes
a unidimensional vector, color is often modeled on a multi-dimensional
space (e.g., RGB, CIELAB), which makes it complex to estimate a
LOESS surface. Similar to how common color schemes such as viridis
or magma are designed to be perceptually uniform by keeping equi-
distance in a perceptual color space between two consecutive color
points [67], we can make use of the Euclidean distance between ren-
dered color values in CIELAB to linearize a 3D color scheme. Speciﬁ-
cally, we recursively accumulate the distances between each consecu-
tive pair of rendered values to create a unidimensional vector. In Fig. 7c,
we show how the linear value of i-th color point is computed from that
of i − 1-th point; we take the calculated value of the i − 1-th point and
add to it the distance between the i − 1-th and i-th points. The ﬁrst color
point is assigned as zero.

5 PROTOTYPE RESPONSIVE VISUALIZATION RECOMMENDER

To implement our task-oriented insight preservation measures, we
developed a prototype responsive visualization recommender that enu-
merates and evaluates responsive designs (or targets). As shown in
Fig. 8, given an input source (large screen) view, our recommender ﬁrst
converts it to a partial speciﬁcation, and then generates a search space
of small screen targets based on the partial speciﬁcation. We adopt
the desktop-ﬁrst approach that visualization authors have described
using [30, 43]. Finally, the recommender computes our measures be-
tween the source view and each target to rank those targets using an
ML model trained on human-labeled rankings.

5.1 Enumerating Target Views

To enumerate target views, we need a formal grammar for representing
visualization speciﬁcations and formulating a search space. We use An-
swer Set Programming (ASP) [8], particularly by modifying Draco [50].
ASP is a declarative programming language for complex search prob-
lems (e.g., satisﬁability problems) that encodes knowledge as facts,
rules, and constraints. Rules generate further facts, and constraints
prevent certain combinations of facts. Formalized in ASP, for exam-
ple, Draco has a rule that if an encoding is binned, then it is discrete,

and a constraint that disallows logarithmic scale on a discrete encod-
ing [50]. A constraint solver then solves an ASP program (the partial
speciﬁcation of a source view and our search space), returning stable
sets of non-conﬂicting facts (enumerated target views with different
transformation strategies). We use Clingo [21, 22] as our solver.

Converting to a partial speciﬁcation: Our recommender converts the
full speciﬁcation of an input source view to a partial speciﬁcation to
allow applying responsive transformation strategies. We maintain the
data speciﬁcation (data ﬁle, data ﬁeld deﬁnitions, and the number of
rows) and encoding information (e.g., count aggregation, association of
data ﬁeld) that are not changed under transformation. We indicate the
rest of the speciﬁcation (mark type, chart size, and encoding channels)
as information about the source view to constrain responsive transfor-
mation strategies (e.g., constraining possible mark type replacement,
allowing for swapping position encodings for axis-transpose).

Generating a search space: Our goal in generating a search space is
to produce a set of reasonable targets that a responsive visualization
author might consider given a source view. We generate a search space
by automatically applying responsive visualization transformations
recently observed in an empirical study of common responsive visual-
ization design strategies [43] to a source visualization. Our prototype
implements rescaling, aggregation, binning, transposing, and select
changes to marks and encodings. For rescaling, we ﬁx the width of tar-
get views and vary heights, in the range from the height resulting from
proportionate rescaling to the height that forms the inverse aspect ratio
with an increment of 50 px. For example, if the source view has a width
of 600 px and a height of 300 px (an aspect ratio of 2:1) and the width
of target views is ﬁxed at 300 px, then the height varies from 150 px
(2:1) to 600 (1:2) by 50 px (i.e., 150, 200, . . . , 550, 600 px). Given a dis-
aggregated source view, we generate alternatives by applying binning
(max bin buckets of 25, 15, and 5) and aggregation (count, mean, me-
dian, sum) as graphical density adjustment strategies. We also generate
alternatives by transposing axes (i.e., swapping x and y position chan-
nels). Finally, in line with the observation of prior work that responsive
visualization authors occasionally substituted mark types when adding
an encoding channel for aggregation, we allow a mark type change in
scatterplots from a point mark to a rectangle (heatmap). We formulate
these strategies in ASP format and add them to Draco [50].

5.2 Evaluating and Ranking Targets

To evaluate enumerated targets, we calculate our loss measures on
rendered values after rendering source and target views using Vega-
Lite [56]. Then, we obtain rendered values, EV , of a visualization
V by gleaning Vega [57] states (a set of raw rendered values [66]).
We implemented the loss measures in Python using SciPy [70]’s
stats.entropy and stats.wasserstein distance methods for
entropy and EMD, respectively. To compute LOESS regression, we use
the LOESS package [11]. Finally, to rank the enumerated targets, we
combine the computed loss values by training ML models, which we
detail in Sect. 6. We use ML models for ranking instead of formalizing
them in ASP because our measures are not declarative (not rule-based).

5

data(”economy.csv”)fieldtype(gdpc,number).mark(bar).width(600).height(300).encoding(e0).field(e0,gdpc).bin(e0,10).channel(e0,x).encoding(e1).aggregate(e1,count).Source (large screen)1Enumerate targets (small screen)2Evaluate targets3Outcome4Draco speciﬁcationdata(”economy.csv”).fieldtype(gdpc,number).encoding(e0).field(e0,gdpc).encoding(e1).aggregate(e1,count).mark_source(bar).bin_source(e0,10).channel_source(e0,x).channel_source(e1,y).Partial speciﬁcationGenerating search spaceTarget viewsTask-oriented insight preservation measuresRank targetsVega-Lite RenderingRanked targetsRendered targets(Vega-Lite)Transpose axesChange markChange bin sizeAggregateResize chartIdentiﬁcationComparisonTrend0.50.30.70.60.50.80.60.41.2(not actual values)is betterorA model trained onhuman-producedrankings(x(1), x(2))Data speciﬁcationMaintained informationSource view informationASPSolverFig. 9. (a, b) Example target transformations enumerated by our prototype responsive visualization recommender (total size of search space per
source given as #Targets).
indicates rankings of each ﬁve targets per source view predicted by our best model (see Sect. 6.3). (c) Source
visualizations for our user study (also includes a and b). Sources views have width of 600px and height of 300px. The width of targets is ﬁxed as
300px. Data sets are from Our World in Data [26, 53, 54]. Continuous, Nominal, Temporal, Identiﬁcation loss, Comparison loss, and Trend loss.

5.3 Examples
We introduce two example cases of transformations generated by our
prototype and describe how our measures distinguish target views.

5.3.1 Case 1: Simple scatterplot
In the source scatterplot (Fig. 9a), each point mark represents a country,
and x and y positions encode Gini coefﬁcients and annual growth rate
of GDP per capita of different countries, respectively. The ﬁrst example
transformation (Ta1) is simple resizing. The second target view (Ta2) is
transposed from the source view while keeping the size. The third and
fourth target views (Ta3 and Ta4) are resized, binned in x and y scales,
and aggregated by count, so the size of each dot represents the number
of data points in the corresponding bin bucket. In the ﬁfth target (Ta5),
the mark type is changed from point to rectangle in addition to resizing,
binning, and aggregating, and the color of each rectangle encodes the
number of data points in that cell.

Because Ta1 and Ta2 perfectly preserve the number of identiﬁable
rendered values, identiﬁcation loss is zero. Ta4 and Ta5 have more
identiﬁable points than Ta3 (due to their smaller bin size), so they
have smaller identiﬁcation loss. While Ta1 has disaggregated values,
Ta4 better preserves the distances between points in terms of position
encoding, so it has smaller comparison loss. Compared to the source
view, the implied trend given x and y positions in Ta1 has a more similar
slope and hence smaller trend loss than Ta2, whereas Ta2 preserves
the differences in the position encodings, resulting in zero comparison
loss. Similarly, Ta3 has a smaller trend loss than Ta4 because Ta3 better
preserves the visual shape of the distribution in the source view.

5.3.2 Case 2: Histogram
The source histogram in Fig. 9b shows the distribution of GDP per
capita of different countries. There are 23 bins along the x axis and
each bar height (y position) represents the number of countries in the
corresponding bin. The ﬁrst target view (Tb1) is resized. The second
and third target views (Tb2 and Tb3) are transposed with different
resizing. In the fourth and ﬁfth target views (Tb4 and Tb5) bin sizes
are changed from (23 to 10 and 5, respectively), with Tb5 transposed.
As Tb1, Tb2, and Tb3 have no changes in binning, they have zero
identiﬁcation loss, whereas Tb4 and Tb5 has greater identiﬁcation loss
proportional to their bin sizes. While Tb1, Tb2, and Tb3 have the
same binning, Tb3 has the most similar differences between bar heights

and bar intervals in pixel space, so it has the smallest comparison loss
among them. Transposing axes (Tb3) better preserves the resolution
for comparison (i.e., chart height and width), often resulting in the
smaller comparison loss than other similarly transformed targets. Tb5
has smaller trend loss than Tb4 as it shows a similar aspect ratio to the
source view, though inverted, as implied by x and y positions.

6 MODEL TRAINING AND EVALUATION
A responsive visualization recommender should combine loss measures
to rank a set of targets by how well they preserve task-oriented insights.
For our prototype recommender, we train machine learning models to
efﬁciently combine our loss measures and rank enumerated targets. We
describe training data collection, model speciﬁcation, and results.

6.1 Labeling
We obtained training and test data consisting of ranked target views
for a set of source views using a Web-based task completed by nine
visualization experts. As shown in Fig. 10a, each labeler was assigned
one out of three trial sets and performed 36 trials, with each trial asking
them to rank ﬁve target transformations (small screen) given a source
visualization (large screen).
Task materials: To create instances for labeling, we selected six desk-
top visualizations (source views) as shown in Fig. 9c. Our goal was to
include different chart types, multiple encoding channels for identiﬁca-
tion and comparison losses, and different types of examples for trend
loss (e.g., 2D/3D models, subgroups, color scale linearization). Our
prototype generates 60 to 620 target transformations (2,120 in total)
for these six source views. We generated three sets of 30 target views
per source view for labeling, using quintile sampling per preservation
(loss) measure, to ensure relatively diverse sets of targets. After sorting
targets in terms of each of our three measures, we sampled two targets
from each quintile of the top 100 targets per measure, as depicted in
Fig. 10c. We took the top 100 targets after inspecting the best ranked
views per measure for each source view, to avoid labeling examples that
might be obviously inferior. Because identiﬁcation loss is measured
using entropy and is primarily affected by how data are binned, certain
source views had fewer than ﬁve unique discrete values within top 100
targets. In this case, we proportionately sampled each discrete value.

After sampling 30 targets for a source view in a trial set, we randomly
divided them into six trials (but ﬁxed these trials between labeler in the

6

Source viewTarget transformations(a) Example case 1(b) Example case 2(c) Source visualizations for user study (also includes (a) and (b))Ta1      1Ta3      4Ta4      3Ta2      2Ta5      5Tb1      3Tb3      2Tb4      5Tb2      1Tb5      4x: C Gini coef.  y: C GDP per capitagrowth rate#Targets: 560Scatterplotx: C past Gini coefﬁcienty: C current Gini coefﬁcientcolor: C GDP per capitaScatterplot (#Targets: 620)x: C GDP growth ratey: C Covid-19 death (per million)size: C Covid-19 death (total)Scatterplot (#Targets: 620)x: C GDP per     capita  (binned)y: C countHistogram#Targets: 60x: T yeary: C productivity  per hour workedcolor: N countryLine graph (#Targets: 80)x: C life expectancy (binned)y: C average years of schooling (binned)color: C countHeatmap  (#Targets: 180)600px300px300pxResizeICT0.000.731.33ResizeBinAgg.ICT4.420.631.52ResizeBinAgg.Change markICT4.420.991.28ResizeBinAgg.ICT5.210.970.69TransposeICT0.000.002.39ResizeICT0.000.910.34ResizeChange binICT1.101.311.58ResizeTransposeChange binICT2.361.351.07ResizeTransposeICT0.000.481.58ResizeTransposeICT0.000.680.89🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆🏆© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

Fig. 10. (a) Study design. (b) Task interface. (c) Quintile sampling of
targets for task materials.

same trial set), so we had 1,080 pairs (1,077 unique pairs1) labeled by
three people each. We randomly assigned each trial set to labeler (3
trial sets (between) × 6 source views × 30 targets (within), Fig. 10a).
Labelers: All ﬁve authors, who have considerable background in visu-
alization design and evaluation, and an additional convenience sample
of four visualization experts (representing postdoctoral researchers and
graduate students in visualization) participated in labeling. All labelers
worked independently.
Labeling task: Each labeler was asked to imagine that they were a
visualization designer for a responsive visualization project, tasked with
ranking a set of small screen design alternatives created by transforming
the source. Their goal was to consider what would be an appropriate
small screen design that would also preserve insights or takeaways
conveyed in the desktop version as much as possible.

The study interface is shown in Fig. 10b. Each labeler completed
36 trials (6 desktop visualizations × 6 sets of 5 smartphone design
candidates). In each trial, the desktop visualization and ﬁve smartphone
design candidates were shown, and labeler ranked the candidates by
dragging and dropping them into an order. Trial order was randomized.
Aggregating labels: From the task, we collected human-judged rank-
ings of 1,080 pairs each of which was ranked by three labelers. To
produce our training data set, we aggregated the three labels obtained
from the three labelers of each pair into a single label representing the
majority opinion, such that that for the i-th pair xi = (x(1)
), the
label yi is 1 if x(1)
is more likely to appear higher than x(2)
otherwise.

, x(2)
i
, and −1

i

i

i

yi =

(cid:40)

1,
−1,

i more often appears higher than x(2)

i

if x(1)
otherwise

(13)

To avoid a biased distribution of training data as well as minimize the
ordering effect within each pair, we randomized the order of pairs so
that half of the pairs are labeled as 1 and the other half as −1, which
naturally sets the baseline training accuracy of 50%.

6.2 Model Description

Table 1. The set of features for our ML models by each chart type. These
features are either concatenated or differentiated for each pair of targets.
Aggregated features are the sum of the corresponding Disaggregated
features. Pink, bold-bordered circles represent required features, and
yellow, light-bordered circles optional encoding-speciﬁc features.

x(2)), and returns their orders (i.e., either x(1) or x(2) ranks higher).

f (g(x(1), x(2))) =

(cid:40)

1,
−1,

if x(1) appears higher in the ranking
if x(2) appears higher in the ranking

,

(14)
where g(·, ·) is a mapping function that combines the features from a
pair of objects. We consider vector difference and concatenation for g.
Our models take two target views representing transformations of the
same source view and return the one with higher predicted ranking, as
depicted in Fig. 8.3.
Features: We deﬁne the feature matrix X ∈ Rn×d where each row
corresponds to a pair of target visualizations and columns represent
the features (converted by g). We use our proposed loss measures as
the features (Table 1). Aggregated features (A) refer to our three loss
measures: identiﬁcation, comparison, and trend loss, as described in
Equations 5, 6, and 12 (Sect. 4). Disaggregated features (D) refer to
the components of the aggregated features (e.g., the EMD value in each
encoding channels for comparison loss). We standardized all features.
Model training: We train SVM with a linear kernel, K-nearest neigh-
borhood (KNN) with k = 1, 10, logistic regression, decision tree (DT),
and a Multilayer Perceptron (MLP) with four layers and 128 percep-
trons per layer, similar to other recent applications of ML in data
visualization (e.g., Hu et al. [32], Luo et al. [48]). We also train ensem-
ble models of DTs: random forest (RF) with 50, and 100 estimators,
Adaptive Boosting (AB), and gradient boosting (GB). Given the moder-
ate number of observations (1,067) in our data set, we use leave-one-out
(LOO) as a cross validation iterator to obtain robust training results.
We used Scikit-Learn [51] for training.
Baselines: In addition to the natural baseline of 50% (random), we in-
clude two simple heuristic-based baselines to evaluate the performance
of our models. The ﬁrst baseline (B1) includes the changes in chart
width and height between a target and its source, capturing an intuition
about maintaining size and aspect ratio. The second baseline (B2) is
whether x and y axes are transposed, capturing an intuition that, of the
strategies in our search space, transposing is the most drastic change.

6.3 Results

All the experimental materials, and ﬁles used for analysis are included
in the supplementary materials, available at https://osf.io/jcvbx.

Prior approaches to visualization ranking problems (e.g., Draco-
Learn [50], DeepEye [48]) utilize ML methods that convert the ranking
problem to a pairwise ordering problem, such as RankSVM (Support
Vector Machine) [29] and the learning-to-rank model [10]; we adopt a
similar approach. A model, f , takes as input a pair of objects, x = (x(1),

1Each source view had 180 pairs, but the histogram source view with 60

transformations has 177 unique pairs.

6.3.1 Rank correlation between loss measures

To ensure that our loss measures capture different information about
transformations, we compute and inspect rank correlations between
each pair of aggregated, and each pair of disaggregated measures. If
two different loss measures produce highly similar rankings of target
views, then one of them might be redundant. Our measures tend to be
orthogonal to each other (see Fig. 11), with Kendall rank correlation

7

(a) Study design6 Desktop visualizations36 Trials(within)6 Random sets of 5 smartphone designs63 Trial sets(between)30 targets per desktop vis.1,080 labeled pairs1,077 unique pairs(b) Task interfaceDesktop visualizationSmartphone version candidates InstructionBetterWorseDrag and drop to change ranking(in a randomized order)(c) Sampling targetsSort by eachloss measureSample two targets from each quintleQuintileSampledIdentiﬁcationComparisonTrend12Features  Chart typesAggregated Disaggregated  Insight type Enc./Model Scatterplot Bar graph Line chart HeatmapIdentiﬁcation  x      Required y size      Optional color shapeComparison x y size color shapeTrend y~x size~x+y color~x+yTable 2. (a) Prediction accuracy of our models, averaged over LOO cross validation. Other performance measures (AUC score and F1-score)
appeared similarly to accuracy. (b) Average importance of Disaggregated features (g = difference) measured by impurity-based importance from
training a random forest model (e = 50) 10 times.

that trial violates the monotonicity assumption. 102 out of 108 trials
(94.44%) in our data set had fully monotonic orderings. Of the six
orderings which are not fully monotonic, ﬁve are partially monotonic
with only one misaligned pair each (out of the ten ordered pairs). The
other non-monotonic ordering (a trial with line chart as the source view)
had multiple conﬂicts; we dropped this ordering from our training data,
resulting in 1,070 training pairs (1,067 unique training pairs).

6.3.3 Training results

Model performance: Overall, our models with disaggregated (D) and
aggregated features (A) achieved prediction accuracy greater than 75%
(Table 2a), showing the utility of our measures in ranking responsive
design transformations. Ensemble models (RF, AB, and GB) with D
features resulted in the highest overall accuracy (above 81%) because
they iterate over multiple different models and we have a relatively
small number of features. In particular, RF with D and 100 estimators
showed highest accuracy of 84%. Our neural network model (MLP)
also provided comparable performance to the ensemble models.

Models with D features in general obtained higher accuracy than A
features, and combining them (D + A) did not provide signiﬁcant gain
in accuracy. Although they had only three features, our models with A
features showed reasonable accuracy of up to 77.4% (g = concatenate)
and 76.4% (g = difference). For mapping functions, concatenation
performed slightly better than difference for our best performing models
(RF).

Our models all outperformed those with both baselines features (B1
and B2), indicating that our loss measures capture information that
simple heuristics, such as changes in chart size or axes transposition,
are unable to capture. When we trained the best performing model
(RF) with the features of only a single loss criterion (e.g., only trend),
accuracy ranged from 52.6% to 79.7%, implying that our measures are
more useful when combined than when used individually. As hypo-
thetical upper bounds for accuracy, training and testing the model on
the same data set resulted in accuracy from 84% (KNN) to 100% (RF).
Feature importance: To understand how the different loss measures
function in our models, we inspected the importance of each disag-
gregated feature (mapping function g = difference) using the impurity-
based importance measure (average information gain) by training a
random forest model with 50 estimators (average over 10 training iter-
ations). As shown in Table 2b, features related to position encodings
(x, y, ex ∼ ey) in general seem to have higher importance, which makes
sense given their ubiquity in our sample.
Predicted rankings of example cases: Using the best prediction
model (RF with 100 estimators), we predicted the rankings of example
cases described in Sect. 5.3. Transformations from the simple scatter-
plot example (Fig. 9a) are ranked as: Ta1 (resizing), Ta2 (transposing
axes), Ta4, Ta3 (binning, resizing, aggregation), and Ta5 (binning, re-
sizing, aggregation, mark type change). Ta1 appears higher in ranking
than Ta2 because Ta2 has higher trend loss, while Ta1 slightly sacriﬁces
comparison loss. Ta4 is ranked in a higher position than Ta5 because
Ta5 has higher comparison and trend loss. Responsive transformations

Fig. 11. (a) Joint distributions of rankings of target views in each pair of
aggregated features (the source visualization is shown in Fig. 9a). (b)
Kendall rank correlation coefﬁcients for targets of our source views in
Fig. 9. Cmp (comparison).

coefﬁcients [42] between −0.41 and 0.47. The same pattern is observed
for the disaggregated measures with overall correlation coefﬁcients
mostly between −0.5 and 0.5 (see supplementary material).

When the chart type of a source view allows a few, limited re-
sponsive transformation strategies due to its own design constraints
(e.g., line chart, heatmap), the correlation between measures appear
slightly higher than the other chart types. For example, it is often
impossible to add a new encoding channel through aggregation or bin-
ning in a line chart. This makes the line chart more sensitive to chart
size changes, resulting in relatively higher negative rank correlation
between comparison and trend loss. Similarly, different binning levels
in a heatmap can affect both one’s ability to identify data points in
different encoding channels and to recognize a trend implied by x and
y on color channels (i.e., ecolor ex + ey), leading to a slightly higher
positive rank correlation between identiﬁcation and trend loss.

6.3.2 Monotonicity
Ranking problems through pairwise comparison assume that the partial
rankings used as input are consistent with the full ranking (monotonicity
of rankings) [29, 36]. In other words, we need to ensure that the partial
pairwise rankings that we calculate based on aggregated expert labels
can yield a monotonic full ranking. Comparison sorting algorithms
can be used to determine whether a monotonic full ranking can be
obtained from pairwise rankings, as a comparison sort will only result
in a monotonic ranking if the principle of transitivity (a > b ∧ b >
c =⇒ a > c) and connexity (∀a and b, a ≤ b ∨ b ≤ a) hold.

To conﬁrm whether our expert labels satisfy the monotonicity as-
sumption, we ﬁrst sort the ﬁve target views in each of our 108 trials,
using the ten aggregated pairwise rankings as a comparison function.
Next, we check whether each consecutive pair in the reproduced or-
dering conﬂicts with the aggregated expert labels, because if a pair
in the reproduced ordering is not aligned with the aggregated label,

8

78.7376.4878.6378.3576.1983.0484.0781.1681.8281.7277.8877.7978.0778.8275.9182.3882.2979.0180.8882.1071.6072.7375.9175.3570.2076.6677.4172.7377.3277.2367.4872.2676.3876.0167.9574.7074.3273.0175.2676.2977.7976.0178.6378.4476.1982.1982.7679.9482.9480.9777.1377.3277.9878.3574.7082.5782.3878.7380.0481.0751.0855.1153.7055.0150.0550.8951.5556.4253.5154.9249.0250.4254.0859.0451.0850.6150.9853.0553.0553.6162.3262.6162.4262.8963.1761.2062.0462.4263.1761.9559.7062.3262.4263.1763.1762.8963.0763.1763.1762.04concatenateconcatenateconcatenateconcatenateconcatenatedifferencedifferencedifferencedifferencedifferenceDisaggregated ModelsMappingsFeaturesAggregatedDisagg. + Agg.B1 (chart size change)B2 (axes-transpose)1-Nearest NeighborhoodK-Nearest NeighborhoodLogistic RegressionSVM LinearDecision TreeRrandom Forest (e=50)Random Forest (e=100)Adaptive BoostingGradient BoostingMultilayer Perceptron(a) Prediction accuracy(b) Feature importancesFeatures  Importance* Ident. y .159  x .153  color .067  size .026 Cmp.  y .141  x .114   color .092  size .038 Trend  y~x .136  color~x+y .055  size~x+y .020(a) Example case: Simple scatterplot (Fig. 9a)(b) Rank correlationIdentiﬁcation/Trend0200400Comparison/Trend02004000200400Identiﬁcation/Cmp.0200400Identiﬁcation/TrendComparison/TrendIdentiﬁcation/Cmp..27– .12– .14Scatter.19– .22– .14Scatter+color.13.08– .18Scatter+size.37.21– .03Histogram.16.08– .41Line.30.47– .01Heatmap© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

from the histogram example (Fig. 9b) are ranked as: Tb2 (transposing
axes, resizing), Tb3 (transposing axes, resizing), Tb1 (resizing), Tb5
(resizing, changing bin size), and Tb4 (resizing, changing bin size).
The transposed views (Tb2 and Tb3) are ranked higher than Tb1 proba-
bly because the model has more emphasis on comparison loss as the
feature importance (Table 2b) shows. The ordering between Tb5 and
Tb4 can be backed by the smaller trend loss of Tb5 while the difference
in comparison loss between them appears subtle.

7 DISCUSSION AND FUTURE WORK
7.1 Extending and Validating Our Preservation Measures
We devised a small set of measures for three common low-level tasks
in visualization use and found that they can be used to build reasonably
well-performing ML models for ranking small screen design alterna-
tives given a large screen source view. Our measures are not strongly
correlated, and removing some of the measures results in lower pre-
dictive accuracy. However, there are other forms of prominent task-
oriented insights that could extend our approach if approximated well,
such as clustering data points or identifying outliers. As our measures
lose information by processing rendered values, future work could esti-
mate task-oriented insights with different methods, such as extracting
and directly comparing image features from rendered visualizations.

There are also opportunities to strengthen and extend our measures
through human subject studies. These include more formative research
with mixed methods to understand heuristics and other strategies that
visualization authors and users employ to reason about how well a
design transformation preserves important takeaways.
In addition,
future work could conduct perceptual experiments that more precisely
estimate human baselines for identiﬁcation, comparison, and trend
losses. We also used simple approximations of perceptual differences
in position, size, and color channels which could be improved through
new experiments speciﬁcally designed to understand how perception
is affected on smaller screen sizes, adding to work like examining
task performance on smaller screens by different chart types [6] and
comparing task performance between small and large screens [3, 4]. A
limitation is that our experiment was conducted on desktop devices.
Future work could test on mobile devices, as well as explore mobile-
ﬁrst design contexts, as our measures are designed to be symmetric.

7.2 Responsive Visualization Authoring Tools
Our work demonstrates how task-oriented insight preservation can be
used to rank design alternatives in responsive visualization. To do
so, we formulated and evaluated our insight preservation measures
on a search space representing common responsive visualization de-
sign strategies and mark-encoding combinations. However, our work
should be extended in several important ways to support a responsive
visualization authoring use case.

First, while more drastic encoding changes than those supported by
our generator are rare in practice [43], this might be because respon-
sive visualization authoring is currently a tedious process and authors
satisﬁce by exploring smaller regions of the design space. There are
many strategies that could be added to a search space like the one we
deﬁned, and used to evaluate our measures as well as to learn more
about how authors react when confronted with more diverse sets of
design alternatives. For example, while we mainly consider single-
view, static visualizations, many communicative visualizations employ
multiple views and interactivity [33, 34, 58]. Ideally a responsive visu-
alization recommender should be able to formulate related strategies
(e.g., rearranging the layout of multiple views, omitting an interaction
feature, editing non-data ink like legends). Recommenders may need to
consider further conditions such as consistency constraints for multiple
views [52], effectiveness of visualization sequence [45], semantics of
composite visualizations [38], and effectiveness of interactive graphical
encoding [55]. As indicated in Kim et al. [43], loss measures should
be able to address concurrency of information because rearranging
multiples views (e.g., serializing) can make it difﬁcult to recognize
data points at the same time on small screen devices. In addition, they
should also account for loss of information that can only be obtained
via user interaction (e.g., trend implied by ﬁltered marks).

We envision our measures, and similar measures motivated to cap-
ture other task-oriented insights, being surfaced for an author to specify
preferences on in a semi-automated responsive visualization design
context. Because what our measures capture is relatively interpretable,
authors may ﬁnd it useful to customize them for certain design tasks,
such as prioritizing one measure or changing how information is com-
bined to capture identiﬁcation, comparison, or trend loss. This is a
strength of our approach relative to using a more “black-box” approach
where model predictions might be difﬁcult to explain.

7.2.1 Extending ML-based approaches

The human labelers in our experiment, including the authors, seemed to
at times use strategies or heuristics such as preferring non-transposed
views in their rankings or trying to minimize changes to aspect ratio for
some chart types. However, models with our loss measures as features
perform better than heuristic approaches like detecting axes-transpose
and chart size changes, implying that task-oriented insights may be the
right level at which to model rankings. As an extension, future work
might learn pre-deﬁned costs for different transformation strategies to
reduce the time complexity of evaluating task-oriented insights preser-
vation, similar to the approach adopted by Draco-Learn [50] which
obtained costs for constraint violation. Learning such pre-deﬁned costs
may also enable better understanding how each responsive design strat-
egy contributes to changes in task-oriented insights. An alternative
approach could be to use our loss measures as cost functions and opti-
mize different strategies to reduce them as MobileVisFixer [74] ﬁxes a
non-mobile-friendly visualizations for a mobile screen by minimizing
heuristic-based costs. As recent deep learning models [49, 75] have
performed well in visualization ranking problems, future work may
further elaborate on those models. In doing so, one could combine our
measures with image features (e.g., ScatterNet [49]) or chart parameters
(e.g., aspect ratio, orientation [75]).

As noted in Sect. 6.3.2, there were a few partial and not fully mono-
tonic orderings in our data set. A better model might ignore this
assumption and try to identify highly recommendable transformations
or classify them into multiple ordinal classes, yet this might come up
with lower interpretability about recommendations due to a lack of
explicit ordinal relationship between transformations.

7.3 Generalizing Our Measures to Other Design Domains

Our approach to task-oriented insight preservation is likely to be useful
in visualization design domains beyond responsive visualization, like
style transfer and visualization simpliﬁcation, although other domains
may also require different transformation strategies. Style transfer, for
instance, may involve techniques like aggregation but is more likely
to change visual attributes of marks or references. While our loss
measures are designed to be low-level enough to apply relatively gener-
ically to visualizations, their precise formulation and the combination
strategy might warrant changes in other domains. For example, in
visualization simpliﬁcation, minimizing trend loss is likely to be more
important than preserving identiﬁcation and comparison of individual
data points. Style transfer often focuses on altering color schemes, size
scales, or mark types [24] which can result in different discriminability
distributions, so it might put more emphasis on comparison loss.

8 CONCLUSION

Responsive visualization transformations often alter task-oriented in-
sights obtainable from a transformed view relative to a source view.
To enable automated recommenders for iterative responsive visualiza-
tion design, we suggest loss measures for identiﬁcation, comparison,
and trend insights. We developed a prototype responsive visualization
recommender that enumerates transformations and evaluates them us-
ing our measures. To evaluate the utility of our measures, we trained
ML models on human-produced orderings that we collected, achieving
accuracy of up to 84.1% with a random forest model.

ACKNOWLEDGMENTS

Jessica Hullman thanks NSF (#1907941) and Adobe.

9

REFERENCES

3694.

[1] N. Adrienko and G. Adrienko. Spatial generalization and aggregation
IEEE Transactions on Visualization and

of massive movement data.
Computer Graphics, 2011. doi: 10.1109/TVCG.2010.44

[2] R. Amar, J. Eagan, and J. Stasko. Low-level components of analytic
activity in information visualization. In IEEE Symposium on Information
Visualization, 2005, InfoVis ’05. IEEE Computer Society, 2005. doi: 10.
1109/INFVIS.2005.1532136

[3] T. Blascheck, L. Besanc¸on, A. Bezerianos, B. Lee, and P. Isenberg. Glance-
able visualization: Studies of data comparison performance on smart-
watches. IEEE Transactions on Visualization and Computer Graphics,
2019. doi: 10.1109/TVCG.2018.2865142

[4] T. Blascheck. and P. Isenberg. A replication study on glanceable visual-
izations: Comparing different stimulus sizes on a laptop computer. In
Proceedings of the 16th International Joint Conference on Computer Vi-
sion, Imaging and Computer Graphics Theory and Applications - Volume
3: IVAPP,. INSTICC, SciTePress, 2021. doi: 10.5220/0010328501330143
[5] M. B¨ottinger. Reaching Broad Audiences from a Research Institute Setting.
Springer International Publishing, 2020. doi: 10.1007/978-3-030-34444-3 17
[6] M. Brehmer, B. Lee, P. Isenberg, and E. K. Choe. A comparative evaluation
of animation and small multiples for trend visualization on mobile phones.
IEEE Transactions on Visualization and Computer Graphics, 2020. doi:
10.1109/TVCG.2019.2934397

[7] M. Brehmer and T. Munzner. A multi-level typology of abstract visualiza-
tion tasks. IEEE Transactions on Visualization and Computer Graphics,
2013. doi: 10.1109/TVCG.2013.124

[8] G. Brewka, T. Eiter, and M. Truszczy´nski. Answer set programming at a

glance. Commun. ACM, 2011. doi: 10.1145/2043174.2043195

[9] A. Brychtov´a and A. C¸ ¨oltekin. The effect of spatial distance on the dis-
criminability of colors in maps. Cartography and Geographic Information
Science, 2017. doi: 10.1080/15230406.2016.1140074

[10] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and
G. Hullender. Learning to rank using gradient descent. In Proceedings
of the 22nd International Conference on Machine Learning, ICML ’05.
ACM, 2005. doi: 10.1145/1102351.1102363

[11] M. Cappellari, R. M. McDermid, K. Alatalo, L. Blitz, M. Bois, F. Bour-
naud, M. Bureau, A. F. Crocker, R. L. Davies, T. A. Davis, P. T. de
Zeeuw, P.-A. Duc, E. Emsellem, S. Khochfar, D. Krajnovi´c, H. Kuntschner,
R. Morganti, T. Naab, T. Oosterloo, M. Sarzi, N. Scott, P. Serra, A.-M.
Weijmans, and L. M. Young. The atlas3D project - xx. mass-size and mass-
σ distributions of early-type galaxies: bulge fraction drives kinematics,
mass-to-light ratio, molecular gas fraction and stellar initial mass function.
MNRAS, 2013. doi: 10.1093/mnras/stt644

[12] W. S. Cleveland. Robust locally weighted regression and smoothing
scatterplots. Journal of the American Statistical Association, 1979. doi:
10.1080/01621459.1979.10481038

[13] K. A. Cook and J. J. Thomas. Illuminating the path: The research and
development agenda for visual analytics. Technical report, National
Visualization and Analytic Center, 2005. https://www.hsdl.org/
?abstract&did=485291.

[14] Q. Cui, M. Ward, E. Rundensteiner, and J. Yang. Measuring data ab-
straction quality in multiresolution visualizations. IEEE Transactions on
Visualization and Computer Graphics, 2006. doi: 10.1109/TVCG.2006.161

[15] Z. Cui, S. K. Badam, M. A. Yalc¸in, and N. Elmqvist. Datasite: Proactive vi-
sual data exploration with computation of insight-based recommendations.
Information Visualization, 2019. doi: 10.1177/1473871618806555

[16] C¸ . Demiralp, M. S. Bernstein, and J. Heer. Learning perceptual kernels for
visualization design. IEEE Transactions on Visualization and Computer
Graphics, 2014. doi: 10.1109/TVCG.2014.2346978

[17] C¸ . Demiralp, P. J. Haas, S. Parthasarathy, and T. Pedapati. Foresight:
Recommending visual insights. Proceedings of the VLDB Endowment,
2017. doi: 10.14778/3137765.3137813

[18] E. Di Giacomo, W. Didimo, G. Liotta, and F. Montecchiani. Network visu-
alization retargeting. In 2015 6th International Conference on Information,
Intelligence, Systems and Applications (IISA), 2015. doi: 10.1109/IISA.2015.
7388095

[19] N. Elmqvist and J. Fekete. Hierarchical aggregation for information visual-
ization: Overview, techniques, and design guidelines. IEEE Transactions
on Visualization and Computer Graphics, 2010. doi: 10.1109/TVCG.2009.84

[20] M. D. Fairchild. Color appearance models. Wiley Blackwell, 2004.
[21] M. Gebser, R. Kaminski, B. Kaufmann, and T. Schaub. Clingo = ASP
+ control: Preliminary report. 2014. https://arxiv.org/abs/1405.

[22] M. Gebser, B. Kaufmann, R. Kaminski, M. Ostrowski, T. Schaub, and
M. Schneider. Potassco: The potsdam answer set solving collection. AI
Commun., 2011. doi: 10.3233/AIC-2011-0491

[23] A. Gelman and A. Guzey. Statistics as squid ink: How prominent re-
searchers can get away with misrepresenting data. CHANCE, 2020. doi:
10.1080/09332480.2020.1754069

[24] J. Harper and M. Agrawala. Deconstructing and restyling d3 visualizations.
In Proceedings of the 27th Annual ACM Symposium on User Interface
Software and Technology, UIST ’14. ACM, 2014. doi: 10.1145/2642918.
2647411

[25] J. Harper and M. Agrawala. Converting basic d3 charts into reusable style
templates. IEEE Transactions on Visualization and Computer Graphics,
2017. doi: 10.1109/TVCG.2017.2659744

[26] J. Hasell. Which countries have protected both health and the
economy in the pandemic?, 2020. https://ourworldindata.org/
covid-health-economy Last accessed: March 1, 2021.

[27] J. Heer and M. Bostock. Crowdsourcing graphical perception: Using
mechanical turk to assess visualization design. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, CHI ’10.
ACM, 2010. doi: 10.1145/1753326.1753357

[28] J. Heer, N. Kong, and M. Agrawala. Sizing the horizon: The effects of
chart size and layering on the graphical perception of time series visual-
izations. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, CHI ’09. ACM, 2009. doi: 10.1145/1518701.1518897
[29] R. Herbrich. Support vector learning for ordinal regression. In Proceedings
of the 9th International Conference on Artiﬁcial Neural Networks, ICANN
’99. Institution of Engineering and Technology, 1999. doi: 10.1049/cp:
19991091

[30] J. Hoffswell, W. Li, and L. Zhicheng. Techniques for ﬂexible responsive
In Proceedings of the 2020 CHI Conference on
visualization design.
Human Factors in Computing Systems, CHI ’20. ACM, 2020. doi: 10.
1145/3313831.3376777

[31] J. M. Hofman, D. G. Goldstein, and J. Hullman. How visualizing inferen-
tial uncertainty can mislead readers about treatment effects in scientiﬁc
results. In Proceedings of the 2020 CHI Conference on Human Factors in
Computing Systems, CHI ’20. ACM, 2020. doi: 10.1145/3313831.3376454
[32] K. Hu, M. A. Bakker, S. Li, T. Kraska, and C. Hidalgo. Vizml: A machine
learning approach to visualization recommendation. In Proceedings of the
2019 CHI Conference on Human Factors in Computing Systems, CHI ’19.
ACM, 2019. doi: 10.1145/3290605.3300358

[33] J. Hullman and N. Diakopoulos. Visualization rhetoric: Framing effects in
narrative visualization. IEEE Transactions on Visualization and Computer
Graphics, 2011. doi: 10.1109/TVCG.2011.255

[34] J. Hullman, S. Drucker, N. H. Riche, B. Lee, D. Fisher, and E. Adar.
IEEE
A deeper understanding of sequence in narrative visualization.
Transactions on visualization and computer graphics, 2013. doi: 10.1109/
TVCG.2013.119

[35] P. Isenberg, P. Dragicevic, W. Willett, A. Bezerianos, and J. Fekete. Hybrid-
image visualization for large viewing environments. IEEE Transactions
on Visualization & Computer Graphics, 2013. doi: 10.1109/TVCG.2013.163
[36] K. G. Jamieson and R. D. Nowak. Active ranking using pairwise compar-
isons. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Wein-
berger, eds., Advances in Neural Information Processing Systems. Curran
Associates, Inc., 2011. https://proceedings.neurips.cc/paper/
2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf.
[37] H. J¨anicke and M. Chen. A salience-based quality metric for visualization.
Computer Graphics Forum, 2010. doi: 10.1111/j.1467-8659.2009.01667.x

[38] W. Javed and N. Elmqvist. Exploring the design space of composite
visualization. In 2012 IEEE Paciﬁc Visualization Symposium, PaciﬁcVis
’12. IEEE Computer Society, 2012. doi: 10.1109/PaciﬁcVis.2012.6183556

[39] S. Johansson and J. Johansson.

Interactive dimensionality reduction
IEEE Transac-
through user-deﬁned combinations of quality metrics.
tions on Visualization and Computer Graphics, 2009. doi: 10.1109/TVCG.
2009.153

[40] G. T. Johnson and S. Hertig. A guide to the visual analysis and commu-
nication of biomolecular structural data. Nature Reviews Molecular Cell
Biology, 2014. doi: 10.1038/nrm3874

[41] C. Kelleher and T. Wagener. Ten guidelines for effective data visualization
in scientiﬁc publications. Environmental Modelling & Software, 2011. doi:
10.1016/j.envsoft.2010.12.006

[42] M. G. Kendall. Rank correlation methods. Grifﬁn, 1948.
[43] H. Kim, D. Moritz, and J. Hullman. Design patterns and trade-offs in

10

© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and
Computer Graphics. The ﬁnal version of this record is available at: xx.xxxx/TVCG.201x.xxxxxxx/

1109/TVCG.2017.2744359

[65] B. Tang, S. Han, M. L. Yiu, R. Ding, and D. Zhang. Extracting top-k
insights from multi-dimensional data. In Proceedings of the 2017 ACM
International Conference on Management of Data, SIGMOD ’17. ACM,
2017. doi: 10.1145/3035918.3035922

[66] UW Interactive Data Lab. Vega: View api, 2017. https://vega.
github.io/vega/docs/api/view/#view_getState. Last accessed:
June 18, 2021.

[67] S. van der Walt and N. Smith. mpl colormaps, 2015. https://bids.

github.io/colormap/. Last accessed: March 1, 2021.

[68] R. Veras and C. Collins. Discriminability tests for visualization effective-
ness and scalability. IEEE Transactions on Visualization and Computer
Graphics, 2020. doi: 10.1109/TVCG.2019.2934432

[69] C. Villani. The Wasserstein distances. Springer Berlin Heidelberg, 2009.

doi: 10.1007/978-3-540-71050-9 6

[70] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cour-
napeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der
Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson,
E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y. Feng, E. W. Moore,
J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A.
Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa,
P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental
Algorithms for Scientiﬁc Computing in Python. Nature Methods, 2020.
doi: 10.1038/s41592-019-0686-2

[71] Y. Wang, F. Han, L. Zhu, O. Deussen, and B. Chen. Line graph or scatter
plot? automatic selection of methods for visualizing trends in time series.
IEEE Transactions on Visualization and Computer Graphics, 2018. doi:
10.1109/TVCG.2017.2653106

[72] M. Wattenberg and D. Fisher. A model of multi-scale perceptual orga-
nization in information graphics. In IEEE Symposium on Information
Visualization 2003, InfoVis ’03. IEEE Computer Society, 2003. doi: 10.
1109/INFVIS.2003.1249005

[73] M. Wattenberg and D. Fisher. Analyzing perceptual organization in infor-
mation graphics. Information Visualization, 2004. doi: 10.1057/palgrave.ivs.
9500070

[74] A. Wu, W. Tong, T. Dwyer, B. Lee, P. Isenberg, and H. Qu. Mobilevisﬁxer:
Tailoring web visualizations for mobile phones leveraging an explainable
reinforcement learning framework. IEEE Transactions on Visualization
and Computer Graphics, 2020. doi: 10.1109/TVCG.2020.3030423

[75] A. Wu, L. Xie, B. Lee, Y. Wang, W. Cui, and H. Qu. Learning to automate
chart layout conﬁgurations using crowdsourced paired comparison. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing
Systems, CHI ’21. ACM, 2021. doi: 10.1145/3411764.3445179

[76] Y. Wu, X. Liu, S. Liu, and K. Ma. Visizer: A visualization resizing
framework. IEEE Transactions on Visualization and Computer Graphics,
2013. doi: 10.1109/TVCG.2012.114

[77] E. Zgraggen, Z. Zhao, R. Zeleznik, and T. Kraska. Investigating the effect
of the multiple comparisons problem in visual analysis. In Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems, CHI
’18. ACM, 2018. doi: 10.1145/3173574.3174053

authoring responsive visualization. Computer Graphics Forum, 2021. doi:
10.1111/cgf.14321

[44] Y. Kim and J. Heer. Assessing effects of task and data distribution on the
effectiveness of visual encodings. Computer Graphics Forum, 2018. doi:
10.1111/cgf.13409

[45] Y. Kim, K. Wongsuphasawat, J. Hullman, and J. Heer. Graphscape: A
model for automated reasoning about visualization similarity and sequenc-
ing. In Proceedings of the 2017 CHI Conference on Human Factors in
Computing Systems, CHI ’17. ACM, 2017. doi: 10.1145/3025453.3025866
[46] G. Kindlmann and C. Scheidegger. An algebraic process for visualization
design. IEEE Transactions on Visualization and Computer Graphics, 2014.
doi: 10.1109/TVCG.2014.2346325

[47] H. Lin, D. Moritz, and J. Heer. Dziban: Balancing agency & automation
in visualization design via anchored recommendations. In Proceedings of
the 2020 CHI Conference on Human Factors in Computing Systems, CHI
’20. ACM, 2020. doi: 10.1145/3313831.3376880

[48] Y. Luo, X. Qin, N. Tang, and G. Li. Deepeye: Towards automatic data
visualization. In 2018 IEEE 34th International Conference on Data Engi-
neering, ICDE ’18. IEEE Computer Society, 2018. doi: 10.1109/ICDE.2018.
00019

[49] Y. Ma, A. K. H. Tung, W. Wang, X. Gao, Z. Pan, and W. Chen. Scatternet:
A deep subjective similarity model for visual analysis of scatterplots. IEEE
Transactions on Visualization and Computer Graphics, pp. 1562–1576,
2020. doi: 10.1109/TVCG.2018.2875702

[50] D. Moritz, C. Wang, G. L. Nelson, H. Lin, A. M. Smith, B. Howe, and
J. Heer. Formalizing visualization design knowledge as constraints: Ac-
tionable and extensible models in draco. IEEE Transactions on Visualiza-
tion and Computer Graphics, 2018. doi: 10.1109/TVCG.2018.2865240
[51] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and ´Edouard Duchesnay.
Scikit-learn: Machine learning in python. Journal of Machine Learning Re-
search, 2011. http://jmlr.org/papers/v12/pedregosa11a.html.
[52] Z. Qu and J. Hullman. Evaluating visualization sets: Trade-offs between
local effectiveness and global consistency. In Proceedings of the Sixth
Workshop on Beyond Time and Errors on Novel Evaluation Methods for
Visualization, BELIV ’16. ACM, 2016. doi: 10.1145/2993901.2993910

[53] M. Roser.

Human development

https://
ourworldindata.org/human-development-index. Last accessed:
March 1, 2021.

index (hdi), 2014.

[54] M. Roser and E. Ortiz-Ospina.

Income inequality, 2013. https://
ourworldindata.org/income-inequality Last accessed: March 1,
2021.

[55] B. Saket, A. Srinivasan, E. D. Ragan, and A. Endert. Evaluating inter-
IEEE Transactions
active graphical encodings for data visualization.
on Visualization and Computer Graphics, 2018. doi: 10.1109/TVCG.2017.
2680452

[56] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-lite:
A grammar of interactive graphics. IEEE Transactions on Visualization
and Computer Graphics, 2017. doi: 10.1109/TVCG.2016.2599030

[57] A. Satyanarayan, R. Russell, J. Hoffswell, and J. Heer. Reactive vega: A
streaming dataﬂow architecture for declarative interactive visualization.
IEEE Transactions on Visualization and Computer Graphics, 2016. doi:
10.1109/TVCG.2015.2467091

[58] E. Segel and J. Heer. Narrative visualization: Telling stories with data.
IEEE Transactions on Visualization and Computer Graphics, 2010. doi:
10.1109/TVCG.2010.179

[59] C. E. Shannon. A mathematical theory of communication. The Bell System

Technical Journal, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x

[60] S. Smart and D. A. Szaﬁr. Measuring the separability of shape, size,
and color in scatterplots. In Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems, CHI ’19. ACM, 2019. doi: 10.
1145/3290605.3300899

[61] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko. Augmenting
visualizations with interactive data facts to facilitate interpretation and
communication. IEEE Transactions on Visualization and Computer Graph-
ics, 2019. doi: 10.1109/TVCG.2018.2865145

[62] S. S. Stevens. On the psychophysical law. Psychological review, 1957.

doi: 10.1037/h0046162

[63] S. S. Stevens. Psychophysics: Introduction to its perceptual, neural and

social prospects. Routledge, 2 ed., 2017.

[64] D. A. Szaﬁr. Modeling color difference for visualization design. IEEE
Transactions on Visualization and Computer Graphics, 2018. doi: 10.

11

