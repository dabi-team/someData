1
2
0
2

v
o
N
0
1

]

C
O
.
h
t
a
m

[

1
v
0
8
6
5
0
.
1
1
1
2
:
v
i
X
r
a

Stability for Constrained Minimax Optimization

Yu-Hong Dai*† and

Liwei Zhang ‡

Abstract

Minimax optimization problems are an important class of optimization problems arising
from both modern machine learning and from traditional research areas. We focus on the sta-
bility of constrained minimax optimization problems based on the notion of local minimax point
by Dai and Zhang (2020). Firstly, we extend the classical Jacobian uniqueness conditions of
nonlinear programming to the constrained minimax problem and prove that this set of properties
2 perturbation. Secondly, we provide a set of conditions, called
is stable with respect to small
Property A, which does not require the strict complementarity condition for the upper level
constraints. Finally, we prove that Property A is a suﬃcient condition for the strong regularity
of the Kurash-Kuhn-Tucker (KKT) system at the KKT point, and it is also a suﬃcient condition
for the local Lipschitzian homeomorphism of the Kojima mapping near the KKT point.

C

Key words: constrained minimax optimization, Jacobian uniqueness conditions, strong regu-
larity, strong suﬃcient optimality condition, Kojima mapping, local Lipschitzian homeomor-
phism.

AMS subject classiﬁcation: 90C30

1 Introduction

Let m, n, m1, m2, n1 and n2 be positive integers and f :

n

m

g :
ℜ
the constrained minimax optimization problem of the form

→ ℜ

→ ℜ

→ ℜ

× ℜ

n1 and G :

m2, H :

ℜ

ℜ

n

n

n

m

m1,
, h :
n2 be given functions. We are interested in

→ ℜ

→ ℜ

× ℜ

× ℜ

ℜ

ℜ

m

n

min
Φ
x
∈

max
Y(x)
y
∈

f (x, y),

where Φ

⊂ ℜ

n is a feasible set of decision variable x deﬁned by

Φ =

x

{

∈ ℜ

n : H(x) = 0, G(x)

0
}

≤

(1.1)

(1.2)

*LSEC, ICMSEC, AMSS, Chinese Academy of Sciences, Beijing 100190, China. Email: dyh@lsec.cc.ac.cn. This
author was supported by the Natural Science Foundation of China (Nos. 11991020, 11631013, 11971372 and 11991021)
and the Strategic Priority Research Program of Chinese Academy of Sciences (No. XDA27000000).

†School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China.
‡Corresponding author. School of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China.
Email: lwzhang@dlut.edu.cn. This author was supported by the Natural Science Foundation of China (Nos. 11971089
and 11731013).

1

 
 
 
 
 
 
and Y :

n ⇒

ℜ

ℜ

m is a set-valued mapping deﬁned by

Y(x) =

y

{

∈ ℜ

m : h(x, y) = 0, g(x, y)

.

0
}

≤

(1.3)

For unconstrained nonconvex-nonconcave minimax optimization, Jin et al. [7] proposed a
proper deﬁnition of local minimax point. This deﬁnition of local minimax point is extended in
[5] for the constrained minimax optimization problem (1.1).

∈

Deﬁnition 1.1 A point (x∗, y∗)
× ℜ
there exist δ0 > 0 and a function η : (0, δ0]
Φ]
δ

∈ ℜ
[Bδ(x∗)

(0, δ0] and any (x, y)

n

m is said to be a local minimax point of Problem (1.1) if
0 such that for any

+ satisfying η(δ)

0 as δ

→ ℜ
[Y(x∗)

→
Bδ(y∗)], we have

→

∈
f (x∗, y)

∩
f (x∗, y∗)

×

≤

max
z

≤

n

∩
f (x, z) : z

Y(x)

∩

∈

Bη(δ)(y∗)
o

.

(1.4)

In [5], we established the ﬁrst-order optimality, the second-order necessary and suﬃcient optimality
conditions for Problem (1.1) when the Jacobian uniqueness conditions are satisﬁed for the lower
level problem and the ﬁrst-order necessary optimality conditions when the strong second-order suf-
ﬁcient optimality condition and the linear independence constraint qualiﬁcation are satisﬁed for the
lower level problem.

It is well known that, for nonlinear programming, the Jacobian uniqueness condition can be
2-perturbation (see for instance [6]) and prove that the strong
used to establish the stability of the
second-order suﬃcient optimality condition and the linear independence constraint qualiﬁcation
are equivalent to the strong regularity of the Kurash-Kuhn-Tucker (KKT) system (see [11] and
[8]). The question naturally arises: What are the counterparts of these two stability properties for
the constrained minimax optimization problem? The purpose of this paper is to answer this basic
question.

C

The rest of this paper is organized as follows. In Section 2, we develop a simpliﬁed version
for second-order optimality conditions for the constrained minimax optimization problem, which
is suitable for the study of stability properties. In Section 3, we prove that the proposed Jacobian
2-perturbation of the original problem
uniqueness conditions for Problem (1.1) are kept when a
occurs. In Section 4, we prove that the proposed Property A, which does not require the strict
complementarity for the upper level problem, is a suﬃcient condition for the strong regularity of
the KKT system at the KKT point. Finally, we draw a conclusion in Section 5.

C

Notation. Scalars and vectors are expressed in lower case letters and matrices are expressed in
p, a
b denotes
p, a > 0, denote
k, we use ΠD(w) to stand for the projection of
⊂ ℜ
p, and a
m
× ℜ

upper case letters. For a vector x, denote Bδ(x) =
the Hadamard product of a and b; namely, a
◦
√a = Diag( √a1, . . . , √ap). For a convex set D
n
w onto D. For simplicity, for a function F :
mapping y :

. For a, b
x
k ≤
b = (a1b1, . . . , apbp)T . For a

, a mapping g :

m, we denote

∈ ℜ
∈ ℜ

x′ −

→ ℜ

→ ℜ

× ℜ

x′ :

δ
}

ℜ

ℜ

◦

k

m

{

n

n

ℜ

→ ℜ
xF(x, y(x)) =
∇
∇
xxF(x, y(x)) =
2
∇
xg(x, y(x)) =

y=y(x),
yF(x, y)
|
2
y=y(x),
xyF(x, y)
|
∇
y=y(x).
yg(x, y)
|
J
m be a locally Lipschitz continuous mapping over an open set

y=y(x),
xF(x, y)
|
2
y=y(x),
xxF(x, y)
|
∇
y=y(x),
xg(x, y)
|

yF(x, y(x)) =
∇
∇
xyF(x, y(x)) =
2
∇
yg(x, y(x)) =

J

J

J

n

Let G :
→ ℜ
diﬀerentiable almost everywhere in
O
, the B-subdiﬀerential of G at x is deﬁned by
a point x

. Let

ℜ

D

∈ O

G denote the set of diﬀerentiable points of G in

O

. Then G is
. For

O

∂BG(x) =

xk

∃

V :
n

G, xk

∈ D

x,

J

→

G(xk)

→

V

o

2

and the Clarke subdiﬀerential of G at x is deﬁned by

∂G(x) = conv ∂BG(x).

For diﬀerential properties of Lipschitz mappings, see the famous book [4].

2 Simpliﬁed Second-order Optimality Conditions

Consider the case when the Jacobian uniqueness conditions hold at some point (x∗, y∗, µ∗, λ∗)

n

m

q
ℜ
diﬀerentiable. For a point x

×ℜ

×ℜ

×ℜ

p, where (x∗, y∗)

n

∈
m is a point around which f, h, g are twice continuously

∈ ℜ

×ℜ

n around x∗, we use (Px) to denote the following problem

∈ ℜ

maxz
s.t.

m

∈ℜ

f (x, z)
h(x, z) = 0,

g(x, z)

0.

≤

(2.1)

The Lagrangian of Problem (Px) is deﬁned by

(x, z, µ, λ) = f (x, z) + µT h(x, z)

L

λT g(x, z).

−

Deﬁnition 2.1 Let (µ∗, λ∗)
Problem (Px∗) are satisﬁed at (y∗, µ∗, λ∗) if

∈ ℜ

× ℜ

m1

m2 be a point. We say that Jacobian uniqueness conditions of

(a) The point (y∗, µ∗, λ∗) is a Karush-Kuhn-Tucker point of Problem (Px∗ ); namely,

(x∗, y∗, µ∗, λ∗) = 0,

y
∇
h(x∗, y∗) = 0,
λ∗ ⊥
0
(b) The linear independence constraint qualiﬁcation holds at y∗; namely, the set of vectors

g(x∗, y∗)

0.

L

≤

≤

are linearly independent, where Ix∗ (y∗) =

yh1(x∗, y∗), . . . ,

yhm1(x∗, y∗)

∇

n∇

o ∪ n∇

ygi(x∗, y∗) : i
Ix∗ (y∗)
o
i : gi(x∗, y∗) = 0, i = 1, . . . , m2}
.

∈

{

(c) The strict complementarity condition holds at y∗ for λ∗; namely,

gi(x∗, y∗) > 0,

i = 1, . . . , m2.

λ∗i −

(d) The second-order suﬃcient optimality condition holds at (y∗, µ∗, λ∗),

where

C
x∗ (y∗) =

C

(x∗, y∗, µ∗, λ∗)dy, dy

< 0

∀
x∗ (y∗) is the critical cone of Problem (Px∗) at y∗,

i

2
yyL
h∇

dy

x∗ (y∗),

∈ C

m :

∈ ℜ

dy

n

yh(x∗, y∗)dy = 0;

J

ygi(x∗, y∗)dy

∇

0, i

∈

≤

Ix∗ (y∗);

y f (x∗, y∗)dy

∇

≤

.

0
o

3

Let us denote

α =

, αc =
i : gi(x∗, y∗) = 0, i = 1, . . . , p
o
n

i : gi(x∗, y∗) < 0, i = 1, . . . , p
.
o
n

(2.2)

n

∈ ℜ
m1

m be a point around which f, h, g are twice continuously diﬀer-
Lemma 2.1 Let (x∗, y∗)
m2 such that Jacobian uniqueness conditions of Problem (Px∗ ) are
entiable. Let (µ∗, λ∗)
∈ ℜ
satisﬁed at (y∗, µ∗, λ∗). Then there exist δ0 > 0 and ε0 > 0, and a twice continuously diﬀerentiable
Bε0(λ∗) such that Jacobian uniqueness conditions
mapping (y, µ, λ) : Bδ0(x∗)
→
of Problem (Px) are satisﬁed at (y(x), µ(x), λ(x)) when x

Bδ0(x∗). Moreover, for x

× ℜ
Bε0(y∗)

Bδ0(x∗),

Bε0(µ∗)

× ℜ

×

×

∈

gi(x, y(x)) = 0, λi(x) > 0, i
gi(x, y(x)) < 0, λi(x) = 0, i

α,
αc.

∈

∈

For (y(x), µ(x), λ(x)) given in Lemma 2.1, deﬁne the optimal value function

ϕ(x) = f (x, y(x)),

Bδ0(x∗)

x

∈

and

Kα(x) = 

2
yyL
∇

(x, y(x), µ(x), λ(x))



J

yh(x, y(x))
ygα(x, y(x))

−J
n

J

yh(x, y(x))T
0

0

ygα(x, y(x))T

−J

0

0

Lemma 2.2 Let (x∗, y∗)
entiable. Let (µ∗, λ∗)
satisﬁed at (x∗, µ∗, λ∗). Then Kα(x∗) is nonsingular and Kα(x) is nonsingular when x
small δ0 > 0.

m be a point around which f, h, g are twice continuously diﬀer-
m2 such that Jacobian uniqueness conditions of Problem (Px∗ ) are
Bδ0(x∗) for

∈ ℜ
m1

× ℜ

× ℜ

∈ ℜ

∈

Basing on (2.3), we may simplify the formula in Proposition 2.1 of [5] for the second-order

derivative of ϕ(x).

Proposition 2.1 If the assumptions of Lemma 2.1 are satisﬁed and ϕ is deﬁned by (2.4), then

(2.3)

(2.4)

.

(2.5)

∈





xϕ(x) =

∇

x

∇

(x, y(x), µ(x), λ(x))

L

2ϕ(x) =

∇

2
xxL
∇

(x, y(x), µ(x), λ(x))

Nα(x)T Kα(x)−

1Nα(x),

−

and

where

Nα(x) = 


2
x,yL
∇

(x, y(x)µ(x), λ(x))

xh(x, y(x))
xgα(x, y(x))

J
J

.





It is easy to check that K(x) is nonsingular when x

Proof.
Proposition 2.1 of [5], we only need to check

∈

Bδ0(x∗) for small δ0 > 0. From

N(x)T K(x)−

1N(x) = Nα(x)T Kα(x)−

1Nα(x),

(2.9)

4

(2.6)

(2.7)

(2.8)

where K(x) and N(x) are deﬁned in [5] with the following expressions

2
yyL
∇

(x, y(x), µ(x), λ(x))

0

yh(x, y(x))

yg(x, y(x))

J

J

0

−

2Diag(λ(x))

0

2Diag

g(x, y(x))

(cid:17)

−

(cid:16) p

yh(x, y(x))T

J

J
2Diag

yg(x, y(x))T

g(x, y(x))

−
0

(cid:16) p

0

(cid:17)









K(x) =

and

Deﬁne

0

0

0

.





2
x,yL
∇

(x, y(x)µ(x), λ(x))

0
xh(x, y(x))
xg(x, y(x))

N(x) = 



P =

In
n
×
0

0

0





J
J

0

0

0
Ip
×

p

p

0
Ip
×
0

0

0

0
Iq
q
×
0

.





Then PT P = PPT = In+q+2p and

K(x)−

1 = [PT PK(x)PT P]−

1 = PT [PK(x)PT ]−

1P.

Thus

N(x)T K(x)−

1N(x) = N(x)T PT [PK(x)PT ]−

1PN(x) = [PN(x)]T [PK(x)PT ]−

1[PN(x)].

Let G(x) =
have that

(x, y(x), µ(x), λ(x)), D(x) = 2Diag

2
yyL
∇

−

(cid:16) p

gα(x, y(x))

and E(x) =

(cid:17)

2Diag(λα(x)). We

−

PK(x)PT

G(x)

yh(x, y(x))

yg(x, y(x))

J

J

0
G(x)

J

yh(x, y(x))
J
ygα(x, y(x))
ygαc(x, y(x))
0

J

0

=

=








0

0

J

yh(x, y(x))T
0

0

0

0

0

J

yh(x, y(x))T
0

J

yg(x, y(x))T
0

0

0

0

2Diag

g(x, y(x))





(cid:17)

−
(cid:16) p
2Diag(λ(x))
0

g(x, y(x))
(cid:17)
0

2Diag

−
yg(x, y(x))T
(cid:16) p
0

J

−
0

0

0

0

E(x)

0

0

0

0

D(x)

0

0

0

D(x)

0

0





0

0

0

0

5

Also let

Q =

In
×
0

0

0

0

0

0
Iq
q
×
0

0

0

0

0

0

I
α
|
|×|
0

α
|

0

0

0

0

0

I
αc
|

αc

|×|
0

|

0





0

0

0

0

0

I
α
|

α
|

|×|

0

0

0

0

I
αc
|

αc

|×|
0

|

.





Then QQT = QT Q = In+q+2p. Obviously, we have that

QPK(x)PT QT

G(x)

=





and

J

yh(x, y(x))
J
ygα(x, y(x))
ygαc(x, y(x))
0

J

0

J

yh(x, y(x))T
0

J

yg(x, y(x))T
0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

D(x)

D(x)

0

0

0

0

0

0

0

0

E(x)





QPN(x) = 



Therefore we obtain

2
x,yL
∇

(x, y(x)µ(x), λ(x))

J
J

xh(x, y(x))
xg(x, y(x))
0

.





N(x)T K(x)−

1N(x) = [QPN(x)]T [QPK(x)PT QT ]−
1
−

T

0

0

1[QPN(x)]

Nα(x)
xgαc (x, y(x))
0αc
Nα(x)
xgαc (x, y(x))
0αc

J

J

= 

= 




= Nα(x)T Kα(x)−

1Nα(x) +

= Nα(x)T Kα(x)−

1Nα(x).

T















Kα(x)
0

0

D(x)

0
Kα(x)−

D(x)
1 0

0

0









Nα(x)
xgαc (x, y(x))
0αc

J





0

D(x)

xgαc(x, y(x))
0αc

0
0

J




D(x)

1 
−



0
D(x)

0

T







Nα(x)
xgαc(x, y(x))
0αc
1
−











J

D(x)

J

xgαc (x, y(x))
0αc




0







Namely, (2.9) holds. The proof is completed.

2
Φ, the Mangasarian-Fromovitz constraint qualiﬁcation is said to hold at x∗ the con-

For x∗ ∈
straint set Φ if

(a) The set of vectors

∇

H j(x∗), j = 1, . . . , n1 are linearly independent.

6

(b) There exists a vector ¯d

n such that

∈ ℜ

H j(x∗)T ¯d = 0, j = 1, . . . , n1,

∇

Gi(x∗)T ¯d < 0, i
∇

∈

I(x∗),

where I(x∗) =

i : Gi(x∗) = 0, i = 1, . . . , n2}

.

{

Deﬁne the critical cone at x∗ by

(x∗) =

dx

{

∈ ℜ

n :

J

C

H(x∗)dx = 0;

Gi(x∗)T dx
∇

≤

0, i

∈

I(x∗); ϕ′(x∗; dx)

.

0
}

≤

(2.10)

In this case, the critical cone

(x∗) can be expressed as

C
H(x∗)dx = 0;

(x∗) =

dx

{

∈ ℜ

n :

J

C

Gi(x∗)T dx
∇

≤

0, i

∈

I(x∗);

x

∇

(x∗, y∗, µ∗, λ∗)T dx

L

. (2.11)

0
}

≤

Based on (2.9) we may simplify Theorems 3.1 and 3.2 in [5] as follows.

m be a point around which
Theorem 2.1 (Necessary Optimality Conditions) Let (x∗, y∗)
f , h, g are twice continuously diﬀerentiable and H, G are twice continuously diﬀerentiable around
x∗. Let (x∗, y∗) be a local minimax point of Problem (1.1). Assume that the linear independence
constraint qualiﬁcation holds at y∗ for constraint set Y(x∗). Then there exists a unique vector
(µ∗, λ∗)

m2 such that

× ℜ

∈ ℜ

m1

n

∈ ℜ

× ℜ

For any dy

∈ C

x∗ (y∗), we have that

(x∗, y∗, µ∗, λ∗) = 0,

L

y
∇
h(x∗, y∗) = 0,
λ∗ ⊥
0

≥

g(x∗, y∗)

0.

≤

2
yyL
h∇

(x∗, y∗, µ∗, λ∗)dy, dy

0.

i ≤

(2.12)

(2.13)

Assume further that Problem (Px∗) satisﬁes Jacobian uniqueness conditions at (y∗, µ∗, λ∗) and the
Mangasarian-Fromovitz constraint qualiﬁcation holds at x∗ for the constraint set Φ. Then there
exists (u∗, v∗)

n2 such that

n1

∈ ℜ

× ℜ

(x∗, y∗, µ∗, λ∗) +

x

L

∇
H(x∗) = 0,
v∗ ⊥
0

≤

G(x∗)

0.

≤

H(x∗)T u∗ +

J

J

G(x∗)T v∗ = 0,

(2.14)

The set of all (u∗, v∗) satisfying (2.14), denoted by Λ(x∗), is nonempty compact convex set. Further-
(x∗) is deﬁned by (2.11),
more, for every dx

(x∗), where

∈ C

C

n1

Xj=1

ui

2
xxH j(x∗) +
∇

n2

Xi=1

vi

max
∈

(u,v)

Λ(x∗) 
*



+
Dh∇

2
xxL
where Kα(x) is deﬁned by (2.5) and Nα(x) is deﬁned by (2.8).

(x∗, y∗, µ∗, λ∗)

Nα(x∗)T Kα(x∗)−

−

2
xxGi(x∗)
∇


dx, dx

+


1Nα(x∗)
i

(2.15)

dx, dx

0,

E ≥

We name the ﬁrst-order necessary optimality conditions (2.14) and (2.12) as KKT conditions of

Problem (1.1) at (x∗, u∗, v∗, y∗, µ∗, λ∗).

7

m be a point
Theorem 2.2 (Second-order Suﬃcient Optimality Conditions) Let (x∗, y∗)
around which f, h, g are twice continuously diﬀerentiable and H, G are twice continuously diﬀer-
m1
m2. Suppose
entiable around x∗. Assume that x∗ ∈
× ℜ
that Problem (Px∗) satisﬁes Jacobian uniqueness conditions at (y∗, µ∗, λ∗), Λ(x∗) ,
, and for every
∅
(where
dx

(x∗) is deﬁned by (2.11)),

Φ and y∗ ∈

Y(x∗). Let (µ∗, λ∗)

∈ ℜ

∈ ℜ

× ℜ

(x∗)

n

∈ C

\ ∅

C

sup

(u,v)

∈

Λ(x∗)


*



+
Dh∇

n1

Xj=1

ui

2
xxH j(x∗) +
∇

n2

Xi=1

vi

2
xxL

(x∗, y∗, µ∗, λ∗)

dx, dx

2
xxGi(x∗)
∇


Nα(x∗)T Kα(x∗)−

+


1Nα(x∗)
i

−

dx, dx

> 0,

E

where Kα(x) is deﬁned by (2.5) and Nα(x) is deﬁned by (2.8). Then there exist δ1 ∈
(0, ε0) (where δ0 and ε0 are given by Lemma 2.1) and γ1 > 0,γ2 > 0 such that for x
∈
and y

Bε1(y∗)

(2.16)

(0, δ0), ε1 ∈
Bδ1(x∗)
Φ

∩

y∗

2/2
k

−

≤

f (x∗, y∗)

≤

z

∈

sup

Y(x)

Bε0 (y∗)
∩

f (x, z)

x

γ2k

−

−

x∗

2/2,
k

(2.17)

which implies that (x∗, y∗) is a local minimax point of Problem (1.1).

3 Stability under Jacobian Uniqueness Condition

For convenience in stating the stability result about

2 perturbation of Problem (1.1) when the

conditions in Theorem 2.2 are satisﬁed, we introduce the following deﬁnition.

C

Deﬁnition 3.1 If the following conditions are satisﬁed, we say that Problem (1.1) satisﬁes Jacobian
n
uniqueness condition at (x∗, u∗, v∗, y∗, µ∗, λ∗)

m2.

m1

n2

n1

m

× ℜ
Φ and conditions in (2.14) are satisﬁed at (x∗, u∗, v∗, y∗, µ∗, λ∗).

× ℜ

× ℜ

× ℜ

∈ ℜ

× ℜ

∈

∩

Y(x∗),
f (x∗, y) + γ1k
y

Gi(x∗) : i

∪ n∇

∈

I(x∗)
o

are linearly independent, where

(i) x∗ ∈
(ii) The set vectors

I(x∗) =

{

H1(x∗), . . . ,
Hn1
i : Gi(x∗) = 0, i = 1. . . . , n2}
.
(cid:9)

∇

∇

(cid:8)

Gi(x∗) > 0 for i

I(x∗).

∈

(iii) v∗i −
(iv) y∗ ∈
(v) For every dx

(x∗)
C
optimality condition (2.16) is satisﬁed.

(where

∈ C

0
}

\ {

Y(x∗) and Problem (Px∗) satisﬁes Jacobian uniqueness conditions at (y∗, µ∗, λ∗).

(x∗) is deﬁned by (2.11)), the second-order suﬃcient

Following Kojima (1980) [9], we deﬁne the so-called Kojima mapping for Problem (1.1),

F(x, u, w, y, µ, ξ) =





(x, y, µ, ξ+) +

L

x

∇

H(x)T u +

J
H(x)

G(x)T w+

J

y
∇

L

G(x)

w−
−
(x, y, µ, ξ+)
h(x, y)
g(x, y) + ξ−

−

8

(3.1)

,





where w+
i
= min
ξ−i
λ = ξ+, (x, y, u, v, µ, λ) satisﬁes the ﬁrst-order necessary optimality conditions of Problem (1.1).

, i = 1, . . . , n2 for w
0, ξi
,
∈ ℜ
m2. If F(x, u, w, y, µ, ξ) = 0, then, letting v = w+ and
(cid:9)
(cid:9)

= min
, ξ−i
, i = 1, . . . , m2 for ξ

= max
0, ξi

n2 and ξ+
i

0, wi
(cid:8)

= max

(cid:8)
∈ ℜ

0, wi

(cid:9)

(cid:8)

(cid:8)

(cid:9)

n

n1

Lemma 3.1 Suppose that the Jacobian uniqueness condition of Problem (1.1) in Deﬁnition 3.1 is
m1
m2. Then F is diﬀerentiable
satisﬁed at (x∗, u∗, v∗, y∗, µ∗, λ∗)
F(x∗, u∗, w∗, y∗, µ∗, ξ∗) is nonsingular for w∗ = v∗ + G(x∗) and ξ∗ =
at (x∗, u∗, w∗, y∗, µ∗, ξ∗) and
λ∗ + g(x∗, y∗).
G(x∗) > 0, we know that ξ+ and ξ− are diﬀerentiable at ξ∗,
Proof. Since λ∗ −
and w+ and w− are diﬀerentiable at w∗. Thus F is diﬀerentiable at (x∗, y∗, u∗, w∗, µ∗, ξ∗). Without
loss of generality, we assume that

g(x∗, y∗) > 0 and v∗ −

∈ ℜ

× ℜ

× ℜ

× ℜ

× ℜ

× ℜ

J

n2

m

where

β := I(x∗) =

1, . . . , r

{

}

, α =

1, . . . , s
}

,

{

I(x∗) =

{
1, . . . , n2} \

i : Gi(x∗) = 0, i = 1, . . . , n2}
β and αc =
βc =

{
r + 1, . . . , n2}

{

1, . . . , m2} \

, α =

i : gi(x∗, y∗) = 0, i = 1, . . . , m2}

{

α, we get that

, αc =

s + 1, . . . , m2}

.

{

.

(3.2)

Then, for βc =

{

Thus we obtain

Denote

,

=

=

Ir 0
0
Is 0
0






0 

0 

(x∗, y∗, µ∗, λ∗) +

,

w−|

w=w∗

J

ξ−|

ξ=ξ∗

J

=

=






n2

ui

xxH j(x∗) +
2
∇

Xi=1

n1

Xj=1

,

0
0
r 
0 In2

−
0
0
s 
0 Im2

2
xxGi(x∗),
∇

vi

−

.

w+

J

w=w∗
|

ξ+

ξ=ξ∗
|

J

=

=

=

G∗11

G∗12
G∗22

2
xxL
∇
2
xyL
∇
2
yyL
∇

(x∗, y∗, µ∗, λ∗),
(x∗, y∗, µ∗, λ∗).
xh∗ and

For simplicity, we use notations
tively. The same notations are also applied to gα and gαc. Then the Jacobian of F at (x∗, u∗, w∗, y∗, µ∗, ξ∗)
can be expressed as

yh(x∗, y∗), respec-

yh∗ to represent

xh(x∗, y∗) and

J

J

J

J

F(x∗, u∗, w∗, y∗, µ∗, ξ∗)
H(x∗)T

J

J

=





G∗11
H(x∗)

J

Gβ(x∗)

J

J

Gβc(x∗)
T
G∗
12
xh∗

J

xg∗α
xg∗αc

−J

−J

0

0

0

0

0

0

0

Gβ(x∗)T

J

0

0

0

0

0

0

0

0

0

0

In2

r
−

−

0

0

0

0

9

G∗12
0

0

0

G∗22
yh∗

J

yg∗α
yg∗αc

−J

−J

T
xh∗

J

0

0

0

T
yh∗

J

0

0

0

T
xg∗
α

−J
0

0

0

T
yg∗
α

−J
0

0

0

0

0

0

0

0

0

0

Im2

s

−





(3.3)

.

The nonsingularity of
matrix

J

F(x∗, u∗, w∗, y∗, µ∗, ξ∗) is equivalent to the nonsingularity of the following

G∗11
H(x∗)

J

J

Gβ(x∗)
T
G∗
12
xh∗

J

xg∗α

−J
Gβc(x∗)

J

xg∗αc

−J

H(x∗)T

J

J

Gβ(x∗)T

0

0

0

0

0

0

0

0

0

0

0

0

0

0

G∗12
0

0

G∗22
yh∗

J

yg∗α

−J
0

yg∗αc

−J

T
xh∗

J

0

0

T
yh∗

J

0

0

0

0

T
xg∗
α

−J
0

0

T
yg∗
α

−J
0

0

0

0

0

0

0

0

0

0

In2

r
−

−

0





which is equivalent to the nonsingularity of the following matrix

0

0

0

0

0

0

0

Im2

s

−

,





G∗11
H(x∗)

J

J

Gβ(x∗)
T
G∗
12
xh∗

J

xg∗α

−J

H(x∗)T

J

J

Gβ(x∗)T

0

0

0

0

0

0

0

0

0

0

G∗12
0

0

G∗22
yh∗

J

yg∗α

−J

T
xh∗

J

0

0

T
yh∗

J

0

0

T
xg∗
α

−J
0

0

T
yg∗
α

−J
0

0





H =





.

(3.4)

Therefore, we only need to prove that the matrix H is nonsingular. From Lemma 2.2, we obtain that
Kα(x∗) is nonsingular, where

Kα(x∗) = 

G∗22
yh∗

J

yg∗α

−J

T
yh∗

J

0

0

T
yg∗
α

−J
0

0



.





So it suﬃces to prove that H/Kα(x∗) is nonsingular. Noticing that

G∗11
H(x∗)

J

J

Gβ(x∗)
G∗12 J
0





0
G∗11 −

H/Kα(x∗) = 



−

= 



H(x∗)T

J

J

Gβ(x∗)T

0

0

0

0





T
xh∗

0

0

T
xg∗
α

−J
0

0

Nα(x∗)T Kα(x∗)Nα(x∗)

Kα(x∗)−

1 

T
G∗
12
xh∗

J

0 0

0 0

xg∗α 0 0
Gβ(x∗)T

J



−J

H(x∗)T

J





H(x∗)

J

Gβ(x∗)

J

10

0

0

0

0

(3.5)

,
















*


we have from (iii) that

C

(x∗) is reduced to the following subspace

(x∗) = Ker

C

J

J

H(x∗)
Gβ(x∗) 






.

(3.6)

Now we prove that H/Kα(x∗) is nonsingular via the formula (3.5). Let a
satisfy

n, b

∈ ℜ

∈ ℜ

n1 and c

r

∈ ℜ

G∗11 −

Nα(x∗)T Kα(x∗)Nα(x∗)

H(x∗)T

J

J

Gβ(x∗)T

H(x∗)

J

Gβ(x∗)

J

Nα(x∗)T Kα(x∗)Nα(x∗)a +

0

0

H(x∗)T b +

J

0

0

J

= 0

a

b

c













Gβ(x∗)T c = 0,

or

G∗11 −
J

H(x∗)a = 0,
Gβ(x∗)a = 0.
J
H(x∗)a = 0 and

It follows from
equation in (3.7), we obtain

J

Gβ(x∗)a = 0 that a

J

∈ C

(x∗). Premultiplying aT to the ﬁrst

(3.7)

n1

Xj=1

xxH j(x∗) +
2

u∗i ∇

n2

Xi=1

v∗i ∇

a, a
+

2
xxGi(x∗)


−
which implies a = 0 from the condition (2.16). From the ﬁrst equation in (3.7) again, we obtain

Dh

+

2
xxL
∇

(x∗, y∗, µ∗, λ∗)

Nα(x∗)T Kα(x∗)−

1Nα(x∗)
i

a, a
E

= 0,

H(x∗)T b +

Gβ(x∗)T c = 0,

J

J
from which we obtain b = 0 and c = 0 from (iii). Therefore H/Kα(x∗) is nonsingular. The proof is
2
completed.
2 perturbation of Problem (1.1)

Basing on Lemma 3.1, we may establish the stability on the
under the Jacobian uniqueness condition at (x∗, u∗, v∗, y∗, µ∗, λ∗).

C

Now consider the parameterized constrained minimax optimization problem of the form

(Pϑ)

min
Φ(ϑ)
x
∈

max
¯Y(x,ϑ)

y
∈

¯f (x, y, ϑ),

, Φ

n is a feasible set of decision variable x deﬁned by

where ¯f :

n

m

l

ℜ

× ℜ

× ℜ

and Y :

n

ℜ

× ℜ

l ⇒

ℜ

l be a vector such that

Let ϑ0 ∈ ℜ

and

→ ℜ

Φ(ϑ) =

⊂ ℜ
x

∈ ℜ
m is a set-valued mapping deﬁned by

{

n : ¯H(x, ϑ) = 0, ¯G(x, ϑ)

0
}

≤

¯Y(x, ϑ) =

m : ¯h(x, y, ϑ) = 0, ¯g(x, y, ϑ)

y

{

∈ ℜ

.

0
}

≤

f (x, y) = ¯f (x, y, ϑ0),

h(x, y) = ¯h(x, y, ϑ0),

g(x, y) = ¯g(x, y, ϑ0)

H(x) = ¯H(x, ϑ0), G(x) = ¯G(x, ϑ0).

11

(3.8)

(3.9)

(3.10)

Deﬁnition 3.2 We say Problem (3.8) is a local
m and Θ
there exist open sets
O1 ⊂ ℜ
¯g are twicely smooth over

O2 ⊂ ℜ

n,

O1 × O2 ×
The Kojima mapping for Problem (Pϑ) is the following function

Θ, and ¯H, ¯G are twicely smooth over

2 perturbation of Problem (1.1) around (x∗, y∗) if
Θ, x∗ ∈ O1, y∗ ∈ O2 and ¯f , ¯h,

l satisfying ϑ0 ∈

C
⊂ ℜ

Θ.

O1 ×

¯F(x, u, w, y, µ, ξ; ϑ) =





where

(x, y, µ, ξ+; ϑ) +

x ¯
L

∇

H(x, ϑ)T u +

J
H(x, ϑ)

G(x, ϑ)T w+

J

∇

w−

G(x, ϑ)
y ¯
L

−
(x, y, µ, ξ+; ϑ)
h(x, y, ϑ)
g(x, y, ϑ) + ξ−

−

(3.11)

,





(x, y, µ, λ; ϑ) = ¯f (x, y, ϑ) +

¯
L

µ, ¯h(x, y, ϑ)

i − h

h

λ, ¯g(x, y, ϑ)
i

.

n

n1

Theorem 3.1 Suppose that the Jacobian uniqueness condition of Problem (1.1) is satisﬁed at (x∗,
2 perturbation of Problem
u∗, v∗, y∗, µ∗, λ∗)
×ℜ
×ℜ
C
(1.1) around (x∗, y∗). Then there exist ε > 0 and δ > 0 such that B(ϑ0, δ)
⊂ O1 and
⊂
B(y∗, ε)
B(y∗, ε)
)) : B(ϑ0, δ)
⊂ O2, and there is a mapping (x(
×
×
→
·
B(µ∗, ε)
B(v∗, ε)
B(u∗, ε)
×

Θ,B(x∗, ε)
B(x∗, ε)
), µ(
·
B(λ∗, ε) such that, for v(ϑ) = w(ϑ)+ and λ(ϑ) = ξ(ϑ)+,

m2 and (Pϑ) is a local

), λ(
·

), u(
·

), y(
·

), v(
·

∈ ℜ

×ℜ

×ℜ

×ℜ

m1

×

×

n2

m

(1) (x(ϑ0), y(ϑ0), u(ϑ0), v(ϑ0), µ(ϑ0), λ(ϑ0)) = (x∗, y∗, u∗, v∗, µ∗, λ∗).

(2) For any ϑ

B(ϑ0, δ), (x(
·

∈

), y(
·

), u(
·

), v(
·

), µ(
·

), λ(
·

)) is continuously diﬀerentiable at ϑ.

(3) For any ϑ

B(ϑ0, δ), Problem (Pϑ) satisﬁes the Jacobian uniqueness condition at (x(ϑ), y(ϑ),

∈
u(ϑ), v(ϑ), µ(ϑ), λ(ϑ)).

Proof. Let w∗ = v∗ + G(x∗) and ξ∗ = λ∗ + g(x∗, y∗). From the deﬁnitions of ¯F and F in (3.1), we
have

¯F(x, u, w, y, µ, ξ; ϑ0) = F(x, u, w, y, µ, ξ).

Thus we get that

¯F(x∗, u∗, w∗, y∗, µ∗, ξ∗; ϑ0) = 0,

J(x,u,w,y,µ,ξ) ¯F(x∗, u∗, w∗, y∗, µ∗, ξ∗; ϑ0) =

F(x∗, u∗, w∗, y∗, µ∗, ξ∗),
J(x,u,w,y,µ,ξ) ¯F(x∗, u∗, w∗, y∗, µ∗, ξ∗; ϑ0) is nonsingular from Lemma 3.1. From the classical
Θ,B(x∗, ε)
⊂
B(x∗, ε)
), w(
), u(
×
·
·
B(ξ∗, ε) such that

and in turn
implicit function theorem, we get that there exist ε > 0 and δ > 0 such that B(ϑ0, δ)
O1 and B(y∗, ε)
B(u∗, ε)
×

⊂ O2, and there is a mapping (x(
·

⊂
)) : B(ϑ0, δ)

B(w∗, ε)

B(µ∗, ε)

B(y∗, ε)

), µ(
·

), ξ(
·

), y(
·

→

J

×

×

×

(x(ϑ0), u(ϑ0), w(ϑ0), y(ϑ0), µ(ϑ0), ξ(ϑ0)) = (x∗, u∗, w∗, y∗, µ∗, ξ∗);

meanwhile, for any ϑ
and

∈

B(ϑ0, δ), (x(
·

), u(
), ξ(
·
·
¯F(x(ϑ), u(ϑ), w(ϑ), y(ϑ), µ(ϑ), ξ(ϑ); ϑ) = 0,

), w(
·

), µ(
·

), y(
·

)) is continuously diﬀerentiable at ϑ,

ϑ

∀

∈

B(ϑ0, δ).

(3.12)

12

From the continuity of (x(ϑ), u(ϑ), w(ϑ), y(ϑ), µ(ϑ), ξ(ϑ)) for ϑ
wi(ϑ)− = 0 in (3.12) that

B(ϑ0, δ), we have from Gi(x(ϑ))

−

Gi(x(ϑ)) < 0, w+(ϑ)i = 0, i

∈
βc

∈

and

Therefore we have that

w+
i (ϑ) > 0, Gi(x(ϑ)) = 0, i

β.

∈

β(ϑ) :=

{

i : Gi(x(ϑ)) = 0, i = 1, . . . , n2}

= β,

and that w+
i (
·
diﬀerentiable over B(ϑ0, δ). Using the same arguments as the above, we obtain

) is diﬀerentiable at ϑ for i

≡

∈

∈

)

i : w+

i (x(ϑ)) = 0, i = 1, . . . , n2}
0 for i

βc. In turn v(ϑ) = w+(ϑ) is

= βc

{
β and w+
i (
·

α(ϑ) :=

i (x(ϑ)) = 0, i = 1, . . . , m2}
and λ(ϑ) = ξ(ϑ)+ is also diﬀerentiable over B(ϑ0, δ). Hence the assertions (1) and (2) hold.

i : gi(x(ϑ), y(ϑ)) = 0, i = 1, . . . , m2}

= α,

{

{

= αc

i : ξ+

Now we prove the assertion (3). From the ﬁrst three equations in (3.12) and the deﬁnition of

(v(ϑ), λ(ϑ)), we obtain

(x(ϑ), y(ϑ), µ(ϑ), λ(ϑ)) +

H(x(ϑ))T u(ϑ) +

J

J

G(x(ϑ))T v(ϑ) = 0,

x

L

∇
H(x(ϑ)) = 0,
G(x(ϑ))

−

[G(x(ϑ) + v(ϑ)]− = 0,

H1(x(ϑ)), . . . ,

which are exactly the conditions in (i) of the Jacobian uniqueness condition of (Pϑ) from Deﬁnition
3.1. From the continuity of (x(ϑ), v(ϑ)) and β(ϑ) = β claimed just now, we have that the set vectors
Hn1(x(ϑ))
i :
∇
∪ n∇
Gi(x(ϑ)) = 0, i = 1. . . . , n2}
, which implies (ii) of the Jacobian uniqueness condition of (Pϑ). And
(cid:9)
(cid:8)
β(ϑ); namely, (iii) of the Jacobian uniqueness condition of
Gi(x(ϑ)) > 0 for i
we have that vi(ϑ)
(Pϑ) holds.

are linearly independent, where β(ϑ) =

Gi(x(ϑ)) : i

β(ϑ)
o

∇

−

∈

∈

{

Now we check (iv) of the Jacobian uniqueness condition of (Px(ϑ)); namely, (Px(ϑ)) satisﬁes the
Jacobian uniqueness condition at (y(ϑ), µ(ϑ), λ(ϑ)). From the last three equations in (3.12) and the
deﬁnition of (v(ϑ), λ(ϑ)), we obtain

(x(ϑ), y(ϑ), µ(ϑ), λ(ϑ)) = 0,

L

y
∇
h(x(ϑ), y(ϑ)) = 0,
g(x(ϑ), y(ϑ))

−

[g(x(ϑ), y(ϑ)) + λ(ϑ)]− = 0,

which are just KKT conditions of (Px(ϑ)) at (y(ϑ), µ(ϑ), λ(ϑ)). Since the continuity of (x(ϑ), y(ϑ),
µ(ϑ), λ(ϑ)) and α(ϑ) = α, we have that the set vectors

yh1(x(ϑ), y(ϑ)), . . . ,

∇

∇

yhn1 (x(ϑ), y(ϑ))
(cid:9)

ygi(x(ϑ), y(ϑ)) : i

∪ n∇

∈

α(ϑ)
o

(cid:8)

are linearly independent when δ > 0 is small enough; namely, the linear independence constraint
qualiﬁcation of (Px(ϑ)) at y(ϑ) is satisﬁed. We also have λi(ϑ)
gi(x(θ), y(ϑ)) > 0; namely, the strict
complementarity condition of (Px(ϑ)) holds at (y(ϑ), λ(ϑ)). Until now, for the Jacobian uniqueness
condition of (Px(ϑ)), only the second-order suﬃcient optimality condition is left to prove. It can be

−

13

proved in the same way as that for (v) of the Jacobian uniqueness condition of (Pϑ) from Deﬁnition
3.1. We omit it here.

Finally, we prove (v) of the Jacobian uniqueness condition of (Pϑ) at (x(ϑ), y(ϑ), u(ϑ), v(ϑ),

µ(ϑ), λ(ϑ)) from Deﬁnition 3.1. From α(ϑ) = α, we have that

x(ϑ)(y(ϑ)) = ker

yh(x(ϑ), y(ϑ))

ker

ygα(x(ϑ), y(ϑ))

J

∩

J

C

(3.13)

is a subspace of
u(ϑ), v(ϑ), µ(ϑ), λ(ϑ)) from Deﬁnition 3.1, we only need to construct a matrix Z(ϑ)
such that

m. For proving (v) of the Jacobian uniqueness condition of (Pϑ) at (x(ϑ), y(ϑ),
α
|
−|

∈ ℜ

m,m

ℜ

m1

−

and

where

Range Z(ϑ) = ker

yh(x(ϑ), y(ϑ))

J

Z(ϑ)T Ψ(ϑ)Z(ϑ)

ker

∩

0,

≻

ygα(x(ϑ), y(ϑ))

J

(3.14)

(3.15)

Ψ(ϑ) =

n1

Xj=1

ui(ϑ)

xx H j(x(ϑ)) +
2
∇

n2

Xi=1

vi(ϑ)

2
xxGi(x(ϑ))
∇

To do this, deﬁne A

m

∈ ℜ

+

2
xxL
∇
m by
×

(x(ϑ), y(ϑ), µ(ϑ), λ(ϑ))

Nα(x(ϑ))T Kα(x(ϑ))−

1Nα(x(ϑ)).

−

yh(x(ϑ), y(ϑ))
ygα(x(ϑ), y(ϑ))

J

J
¯A

,





A(ϑ) = 



−

m

−|

m1

∈ ℜ

where ¯A
α
| is chosen such that A(ϑ0) is nonsingular. Then if ϑ is close to ϑ0 enough, we
have that A(ϑ) is nonsingular as well. By applying the standard Gram-Schmidt orthogonalization
procedure to the columns of A(ϑ), we obtain an orthogonal matrix P(ϑ) = [P1(ϑ) P2(ϑ)]
m
α
|. Then P(ϑ) is a continuous function over B(ϑ0, δ). Let
with P1(ϑ)
Z(ϑ) = P2(ϑ) satisfy (3.14) and Z(ϑ) be continuous over B(ϑ0, δ). Then Z(ϑ) satisﬁes (3.14) and
(3.15) when ϑ

B(ϑ0, δ) for small δ > 0, which comes from the fact that

m1+
α
|, P2(ϑ)
|
×

∈ ℜ

∈ ℜ

∈ ℜ

m1

−|

m

m

m

−

×

∈

from (2.16). The proof is completed.

Z(ϑ0)T Ψ(ϑ0)Z(ϑ0)

0,

≻

2

4 Strong Regularity without Strict Complementarity

In the Jacobian uniqueness condition of Problem (1.1) by Deﬁnition 3.1, a critical condition is
the strict complementarity for the upper level problem. In this section, we consider the case when
this condition does not hold.

Let (x∗, u∗, v∗, y∗, µ∗, λ∗) be a KKT point of Problem (1.1); namely, it satisﬁes the following

conditions

H(x)T u +
G(x)

≤

G(x)T v = 0,

J
0,

(4.1)

(x, y, µ, λ) +

x

∇
L
H(x) = 0,

J
v
0
⊥
(x, y, µ, λ) = 0,
λ

y
∇
h(x, y) = 0,

L

≤

0

g(x, y)

0.

≤

⊥

≥

14

Let z := (x, u, v, y, µ, λ) and deﬁne

=

K

n

n1

ℜ

× ℜ

× ℜ

n2
+

m

m1

× ×ℜ

× ℜ

× ℜ

m2
+

.

(4.2)

and

(z) =

H

(x, y, µ, λ) +

L

x

∇

H(x)T u +

J
H(x)

G(x)T v

J





G(x)
−
(x, y, µ, λ)

y
∇

L

h(x, y)

g(x, y)

−

(z) + N

(z).

(z) + N

(z)

0

η





(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

The KKT conditions above can be expressed as the following generalized equation

∈ H
For η = (ηx; ηH; ηG; ηy; ηh; ηg), it is easy to see that the perturbed generalized equation

K

∈ H
represents the KKT conditions for the following canonical perturbation of Problem (1.1),

K

n

y
n

η

where f :

n

m

ℜ

× ℜ

x
∈
, Φ :

min
Φ(ηH ,ηG)

max
Y(x,ηh,ηg)

y
∈

f (x, y)

ηx, x

i − h

,

ηy, y
i

− h

n1

n2 ⇒

n is a set-valued mapping deﬁned by

→ ℜ

ℜ
Φ(ηH, ηG) =

× ℜ
x

ℜ
n : H(x)

∈ ℜ

ηH = 0, G(x) + ηG

−

≤

0
o

m2 ⇒

m is a set-valued mapping deﬁned by

and Y :

n

m1

ℜ

× ℜ

× ℜ
Y(x, ηh, ηg) =

ℜ

m : h(x, y)

−

∈ ℜ

ηh = 0, g(x, y) + ηg

≤

.

0
o

Robinson [11] introduced the concept of strong regularity for a solution of the generalized equation
(4.3).

Deﬁnition 4.1 Let z∗ be a solution of the generalized equation (4.3). We say that z∗ is a strongly
regular solution of the generalized equation (4.3) if there exist positive numbers δ and ε > 0 such
that for every η

B(0, δ), the following linearized generalized equation

∈

∈ JH
has a unique solution in B(z∗, ε), denoted by
continuous.

(z∗)(z

z∗) + N

(z)

K

−

z(η), and the mapping

(4.8)

z : B(0, δ)

→

B(z∗, ε) is Lipschitz

b

b

It follows from [11] or [1] that if z∗ is a strongly regular solution of the generalized equation (4.3),
B(0, δ), the following general-
then there exist positive numbers δ and ε > 0 such that for every η
ized equation

∈

has a unique solution in B(z∗, ε), denoted by z(η), and the mapping z : B(0, δ)
continuous over B(0, δ).

η

∈ H

(z) + N

(z)

K

(4.9)

B(z∗, ε) is Lipschitz

→

To study the strong regularity of the KKT system at (x∗, u∗, v∗, y∗, µ∗, λ∗), we introduce the

following deﬁnition.

15

(4.10)

(4.11)

Deﬁnition 4.2 We say that Problem (1.1) satisﬁes Property A at (x∗, u∗, v∗, y∗, µ∗, λ∗)

n

n1

∈ ℜ

× ℜ

×

× ℜ

× ℜ
Φ and conditions in (2.14) are satisﬁed at (x∗, u∗, v∗, y∗, µ∗, λ∗).

I(x∗) =

{

H1(x∗), . . . ,
Hn1
i : Gi(x∗) = 0, i = 1. . . . , n2}
.
(cid:9)

∇

∇

(cid:8)

Gi(x∗) : i

∪ n∇

∈

I(x∗)
o

are linearly independent, where

Y(x∗) and Problem (Px∗) satisﬁes Jacobian uniqueness conditions at (y∗, µ∗, λ∗).

m

m1

m2 if

n2

ℜ

× ℜ
(i) x∗ ∈
(ii) The set vectors

(iv) y∗ ∈
(v) For every dx

Aﬀ

∈

(x∗)

0
}

\ {

C

(where

C

(x∗) is deﬁned by (2.11)),

sup

(u,v)

∈

Λ(x∗)


*



+
Dh∇

n1

Xj=1

ui

2
xxH j(x∗) +
∇

n2

Xi=1

vi

2
xxL

(x∗, y∗, µ∗, λ∗)

dx, dx

2
xxGi(x∗)
∇


Nα(x∗)T Kα(x∗)−

+


1Nα(x∗)
i

−

dx, dx

> 0,

E

where Kα(x) is deﬁned by (2.5) and Nα(x) is deﬁned by (2.8).

It can be checked that the perturbed Kojima mapping of the form

F(x, u, w, y, µ, ξ) =

(x, y, µ, ξ+) +

L

x

∇

H(x)T u +

J
H(x)

G(x)T w+

J

L

G(x)

w−
−
(x, y, µ, ξ+)
h(x, y)
g(x, y) + ξ−

y
∇

−





ηx
ηH
ηG
−
ηy
ηh
ηg









−





is the Kojima mapping of the canonical perturbation Problem (4.5), where Φ :
m1
is deﬁned by (4.6) and Y :
× ℜ
F(x, u, w, y, µ, ξ) = η with η = (ηx; ηH;
(4.5).

n
ℜ
m is deﬁned by (4.7). Thus we have that
× ℜ
ηG; ηy; ηh; ηg) corresponds to KKT conditions for Problem

m2 ⇒

n2 ⇒

× ℜ

ℜ

ℜ

ℜ

−

n1

n

Lemma 4.1 Suppose that Property A of Problem (1.1) is satisﬁed at (x∗, u∗, v∗, y∗, µ∗, λ∗)

n

×
m2. Then for w∗ = v∗ + G(x∗) and ξ∗ = λ∗ + g(x∗, y∗), any element of

∈ ℜ

n1

n2

m

m1

ℜ
× ℜ
∂F(x∗, u∗, w∗, y∗, µ∗, ξ∗) is nonsingular.

× ℜ

× ℜ

× ℜ

g(x∗, y∗) > 0, we know that ξ+ and ξ− are diﬀerentiable at ξ∗, and w+ and w− are
Proof. Since λ∗ −
diﬀerentiable at w∗. Since w+ and w− are strongly semi-smooth in the sense of [10], F is strongly
semi-smooth at (x∗, y∗, u∗, w∗, µ∗, ξ∗). Without loss of generality, we assume that

β := I(x∗) =

1, . . . , r

{

}

, α =

1, . . . , s
}

,

{

where

Let

I(x∗) =

i : Gi(x∗) = 0, i = 1, . . . , n2}

{

, α =

i : gi(x∗, y∗) = 0, i = 1, . . . , m2}

.

{

β+ =

i
n

∈

β : v∗i > 0
o

,

β0 =

β : v∗i

i
n

∈

= 0
o

16

and assume

Then, for βc =

1, . . . , n2} \
,
o

{
1, . . . , r1
n

β and αc =

{

β0 =

r1 + 1, . . . , r
n

β+ =

.
o

β+ =
1, . . . , r1
n
1, . . . , m2} \
βc =
,
o

{

{

α, we get that

r + 1, . . . , n2}

, αc =

s + 1, . . . , m2}

.

{

Thus we obtain

and

∂w+

w=w∗
|

=

w=w∗

∂w−|
Denote

n

ξ+

J

ξ=ξ∗
|

=

Is 0
0

0 





,

ξ−

ξ=ξ∗
|

J

=

0
0
0 Im2

−

s 





: ωβ0

= Diag[ωr1+1,

= 


Ir1
0
0 ωβ0


0
0

In2 −

ω : ω

∈

0

0





0
∂w+

w=w∗
|

.

o

, ωr], ωi

·

∈

[0, 1], i

∈

,

β0



n2

=

=

G∗11

G∗12

2
xxL
∇
2
xyL
∇

(x∗, y∗, µ∗, λ∗) +

n1

ui

Xj=1
(x∗, y∗, µ∗, λ∗), G∗22
xh∗ and

=

2
yyL
∇
yh∗ to represent

xxH j(x∗) +
2
∇

2
xxGi(x∗),
∇
(x∗, y∗, µ∗, λ∗).

Xi=1

vi

(4.12)

(4.13)

For simplicity, we use notations
tively. The same notations are also applied to gα and gαc. Let V be an element of ∂F(x∗, u∗, w∗, y∗, µ∗, ξ∗).
Then there exists an element ω

yh(x∗, y∗), respec-

xh(x∗, y∗) and

J
∂w+

J

J

J
w=w∗ such that
∈
|
Gβ+ (x∗)T ωβ0J

Gβ0(x∗)T

H(x∗)T

J

J

0

0

0

0
In2−
0

r

−

0

0

0

G∗12
0

0

0

0

T
xh∗

J

0

0

0

0

T
xg∗
α

−J
0

0

0

0

G∗22
yh∗

J

yg∗α

−J

yg∗αc

−J

T
yh∗

J

0

0

0

T
yg∗
α

−J
0

0

0

0

0

0

0

0

0

0

.





0
Im2−
s
(4.14)

G∗11
H(x∗)

J
Gβ+ (x∗)

J

J

J

Gβ0(x∗)

Gβc (x∗)

T
G∗
12
xh∗

J

xg∗α

−J

xg∗αc

−J

V =





0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
+ ωβ0
0

I

β0|

|

−

0

0

0

0

17

The nonsingularity of V is equivalent to the nonsingularity of the following matrix

G∗11
H(x∗)

J
Gβ+(x∗)

J

J

Gβ0 (x∗)
T
G∗
12
xh∗

J

xg∗α

−J
Gβc(x∗)

J

xg∗αc

−J

H(x∗)T

J

Gβ+ (x∗)T ωβ0J

J

Gβ0(x∗)T

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
+ ωβ0
0

I

β0|

|

−

0

0

0

0

G∗12
0

0

0

G∗22
yh∗

J

yg∗α

−J
0

yg∗αc

−J

T
xh∗

J

0

0

0

T
yh∗

J

0

0

0

0

T
xg∗
α

−J
0

0

0

T
yg∗
α

−J
0

0

0

0

0

0

0

0

0

0

0
In2−
0

r

−

0

0

0

0

0

0

0

0
Im2−

s





which is equivalent to the nonsingularity of the following matrix

H(x∗)T

J

Gβ+(x∗)T ωβ0J

J

Gβ0(x∗)T
0

H(ω) =

G∗11
H(x∗)

J
Gβ+(x∗)

J

J

Gβ0(x∗)
T
G∗
12
xh∗

J

xg∗α

−J





0

0

0

0

0

0

0

0

0

0

0

0

G∗12
0

0

0

G∗22
yh∗

J

yg∗α

−J

T
xh∗

J

0

0

0

T
xg∗
α

−J
0

0

0

T
yh∗

J

0

0

T
yg∗
α

−J
0

0

0
+ ωβ0
0

I
β0
|

|

−

0

0

(4.15)
Therefore, we only need to prove that the matrix H(ω) is nonsingular. From Lemma 2.2, we obtain
that Kα(x∗) is nonsingular, where

,

.








G∗22
yh∗

J

yg∗α

−J

T
yh∗

J

0

0

T
yg∗
α

−J
0

0

.





Kα(x∗) = 



18

So we only need to prove that H(ω)/Kα(x∗) is nonsingular. Notice that

H(ω)/Kα(x∗) =





−

G∗11
H(x∗)

J
Gβ+(x∗)

J

Gβ0(x∗)
J
G∗12 J
0





0

0
G∗11 −

H(x∗)T

J

Gβ+(x∗)T ωβ0J

J

0

0

0

0

0

0

T
xh∗
0

T
xg∗
α
−J
0

0

0

0

0





Kα(x∗)−

1 



Gβ0 (x∗)T
0

0
+ ωβ0





0 0 0

0 0 0

|

I
β0
|

−
T
G∗
12
xh∗

J





−J

xg∗α 0 0 0
Gβ+(x∗)T ωβ0J

J

=





Nα(x∗)T Kα(x∗)Nα(x∗)

H(x∗)T

J

H(x∗)

J
Gβ+(x∗)

J

Gβ0(x∗)

J

0

0

0

0

0

0

(x∗) is of the following subspace

C

It is easy to check that Aﬀ

Gβ0(x∗)T
0

0
+ ωβ0

I
β0
|

|

−

.





(4.16)

Aﬀ

C

(x∗) = Ker

J

J

H(x∗)
Gβ+(x∗) 






.

(4.17)

r1 and a4 ∈ ℜ
G∗11 −

Now we prove that H(ω)/Kα(x∗) is nonsingular via the formula (4.16). Let a1 ∈ ℜ
a3 ∈ ℜ

r1 satisfy
−

r

n, a2 ∈ ℜ

n1,

Nα(x∗)T Kα(x∗)Nα(x∗)

H(x∗)T

J

Gβ+(x∗)T ωβ0J

J

H(x∗)

J
Gβ+(x∗)

J

Gβ0(x∗)

J

0

0

0

0

0

0

Gβ0(x∗)T
0

0
+ ωβ0

I
β0
|

|

−

= 0n+n1+r

a1

a2

a3

a4

















or

[G∗11 −

Nα(x∗)T Kα(x∗)Nα(x∗)]a1 +

H(x∗)T a2 +

Gβ+(x∗)T a3 + ωβ0J

J

J

Gβ0(x∗)T a4 = 0,

(4.18)

H(x∗)a1 = 0,

Gβ+(x∗)a1 = 0,

J

J

|

J

I
β0
|

+ ωβ0]a4 = 0.

Gβ0(x∗)a1 + [
−
It follows from (4.19) and (4.20) that a1 ∈
aT
1 [G∗11 −

(4.19)

(4.20)

(4.21)

Aﬀ

C

(x∗). Premultiplying aT

1 to (4.18), we obtain

Nα(x∗)T Kα(x∗)Nα(x∗)]a1 + aT

Gβ0(x∗)T a4 = 0.

(4.22)

1 ωβ0J

19

From the relation (4.21), we get that

aT
1 ωβ0J

Gβ0(x∗)T a4 =

ωi

ωi

1

−

β0:0<wi<1

Xi
∈

Gi(x∗)T a1]2
[
∇

≥

0.

(4.23)

If follows from (4.18) and (4.23) that

n1

Xj=1

xxH j(x∗) +
2

u∗i ∇

n2

Xi=1

v∗i ∇

*


a1, a1

+

2
xxGi(x∗)


+

2
xxL
which implies a1 = 0 from the condition (2.16). Let ¯β0 =
from (4.21), a1 = 0 and (4.18), we can get that

(x∗, y∗, µ∗, λ∗)

Dh∇

−

Nα(x∗)T Kα(x∗)−

i

{

∈

1Nα(x∗)
i
β0 : ωi = 1
}

a1, a1

E ≤
and ¯βc
0

and

[a4] ¯βc

0

= 0

0,

= β0 \

¯β0. Then

(4.24)

H(x∗)T a2 +

J
From (iii), we obtain a2 = 0,a3 = 0 and [a4] ¯β0
a3 = 0 and a4 = 0. Therefore H(ω)/Kα(x∗) is nonsingular. The proof is completed.

= 0.
(4.25)
= 0. Combining with (4.24), we have that a2 = 0,
2
Now we are in a position to establish the main result about the strong regularity of the KKT

G ¯β0(x∗)T [a4] ¯β0

Gβ+(x∗)T a3 +

J

J

system for Problem (1.1).

Theorem 4.1 Let (x∗, y∗)
tiable, and H and G are twice continuously diﬀerentiable. Assume that there exists (u∗, v∗, µ∗, λ∗)

m be a point around which f , h and g are twice diﬀeren-

∈ ℜ

× ℜ

∈
m2 such that (x∗, u∗, v∗, y∗, µ∗, λ∗) satisﬁes Karush-Kuhn-Tucker conditions for

m1

n2

n1

n

ℜ
Problem (1.1). Consider the following four statements

× ℜ

× ℜ

× ℜ

(a) Property A holds at (x∗, u∗, v∗, y∗, µ∗, λ∗).

(b) For w∗ = v∗ + G(x∗) and ξ∗ = λ∗ + g(x∗, y∗), any element of ∂F(x∗, u∗, w∗, y∗, µ∗, ξ∗) is

nonsingular.

(c) F is a locally Lipschitz homeomorphism near (x∗, u∗, w∗, y∗, µ∗, ξ∗).

(d) The point (x∗, u∗, v∗, y∗, µ∗, λ∗) is a strongly regular solution of the generalized equation (4.3).

Then it holds that (a) =

⇒

(b) =

⇒

(c)

⇐⇒

(d).

Proof. From Lemma 4.1, we obtain (a)=
[3, 4]), F is a locally Lipschitz homeomorphism near (x∗, u∗, w∗, y∗, µ∗, ξ∗) and hence we get (b)=
(c). Noting that the generalized equation

(b). By Clarke’s inverse function theorem (Clarke

⇒

⇒

η

∈ H

(z) + N

(z)

K

represents the KKT conditions for Problem (4.5) and F(x, u, w, y, µ, ξ) = η with η = (ηx; ηH;
ηG;
ηy; ηh; ηg) corresponds to KKT conditions for Problem (4.5), we obtain the equivalence between (c)
2
and (d). The proof is completed.

−

20

5 Some Concluding Remarks

In this paper, we have analyzed the stability properties of the Kurash-Kuhn-Tucker (KKT) system
for Problem (1.1). Firstly, we proposed the deﬁnition of Jacobian uniqueness condition of Problem
2-perturbation. Secondly,
(1.1) and proved that this property is stable with respect to a small
comparing with the Jacobian uniqueness condition, we proposed Property A by eliminating the
strict complementarity condition for the outer level constraints and adopting the strong second-
order suﬃciency optimality condition. We proved that the strong regularity of the KKT system at
the KKT point is equivalent to the local Lipschitz homeomorphism of the Kojima mapping near the
KKT point. Finally, we proved that Property A is a suﬃcient condition for the strong regularity of
the KKT system at the KKT point.

C

There are many problems about the stability of constrained minimax optimization left to us. For
instance, in our analysis, even Property A requires the Jacobian uniqueness condition for the inner
level problem. Is it possible to weaken this condition? A closely related problem is how to obtain
the second-order optimality conditions for the constrained minimax problem when the Jacobian
uniqueness condition for the inner level problem fails. This might be a diﬃcult problem. Another
question is, when the constraints are linear and the objective function is even convex-concave, can
we have a sharp theoretical result like the result for linear semideﬁnte programming in [2]?

References

[1] Bonnans J. F. and Shapiro A., Perturbation Analysis of Optimization Problems, Springer-

Verlag, New York, 2000.

[2] Chan Z. X. and Sun D., Constraint nondegeneracy, strong regularity and nonsingularity in

semideﬁnite programming, SIAM Journal on Optimization, 19 (2008), pp. 370-396.

[3] Clarke F. H., On the inverse function theorem, Paciﬁc Journal of Mathematics 64 (1976), pp.

97-102.

[4] Clarke F. H., Optimization and Nonsmooth Analysis, John Wiley and Sons, New York, 1983.

[5] Dai Y. H. and Zhang L. W., Optimality conditions for constrained minimax optimization,

CSIAM Trans. Appl. Math., 1:2 (2020), pp. 296-315.

[6] Fiacco A. V. and McCormick G. P., Nonlinear Programming: Sequential Unconstrained Min-

imization Techniques, SIAM Philadelphia, 1990.

[7] Jin C., Netrapalli P. and Jordan M. I., What is local optimality in nonconvex-nonconcave

minimaxoptimization? arXiv:1902.00618v2 [cs.LG] 3 Jun 2019.

[8] Jongen H. Th., Ruckmann J., and Tammer K., Implicit functions and sensitivity of stationary

points, Mathematical Programming 49 (1990), pp. 123-138.

[9] Kojima M., Stronglystablestationarysolutionsonnonlinearprograms, In: Analysis and Com-
putation of Fixed Points (ed. Robinson S. M.), Academic Press, New York, 1980, pp. 93-138.

21

[10] Qi L., Convergence analysisofsomealgorithmsforsolving nonsmooth equations, Mathemat-

ics of Operations Research, 18:1 (1993), pp. 227-244.

[11] Robinson, S. M., Strongly regular generalized equations, Mathematics of Operations Re-

search. 5 (1980), pp. 43-62.

22

