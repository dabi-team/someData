1
2
0
2

g
u
A
4

]
E
N
.
s
c
[

1
v
3
2
0
2
0
.
8
0
1
2
:
v
i
X
r
a

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural
Networks to Neuromorphic Hardware

SHIHAO SONG, HARRY CHONG, ADARSHA BALAJI, ANUP DAS, JAMES SHACKLE-
FORD, and NAGARAJAN KANDASAMY, Drexel University, USA

Spiking Neural Networks (SNN) are an emerging computation model, which uses event-driven activation
and bio-inspired learning algorithms. SNN-based machine-learning programs are typically executed on tile-
based neuromorphic hardware platforms, where each tile consists of a computation unit called crossbar,
which maps neurons and synapses of the program. However, synthesizing such programs on an off-the-shelf
neuromorphic hardware is challenging. This is because of the inherent resource and latency limitations of the
hardware, which impact both model performance, e.g., accuracy, and hardware performance, e.g., throughput.
We propose DFSynthesizer, an end-to-end framework for synthesizing SNN-based machine learning programs
to neuromorphic hardware. The proposed framework works in four steps. First, it analyzes a machine-learning
program and generates SNN workload using representative data. Second, it partitions the SNN workload and
generates clusters that fit on crossbars of the target neuromorphic hardware. Third, it exploits the rich semantics
of Synchronous Dataflow Graph (SDFG) to represent a clustered SNN program, allowing for performance
analysis in terms of key hardware constraints such as number of crossbars, dimension of each crossbar, buffer
space on tiles, and tile communication bandwidth. Finally, it uses a novel scheduling algorithm to execute
clusters on crossbars of the hardware, guaranteeing hardware performance. We evaluate DFSynthesizer with
10 commonly used machine-learning programs. Our results demonstrate that DFSynthesizer provides much
tighter performance guarantee compared to current mapping approaches.

CCS Concepts: • Hardware → Neural systems; Emerging languages and compilers; Emerging tools
and methodologies; • Computer systems organization → Data flow architectures; Neural networks.

Additional Key Words and Phrases: Neuromorphic Computing, Synchronous Dataflow Graph (SDFG), Machine
Learning, Spiking Neural Networks (SNN), Compiler, Mapping

ACM Reference Format:
Shihao Song, Harry Chong, Adarsha Balaji, Anup Das, James Shackleford, and Nagarajan Kandasamy. 2020.
DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware. ACM
Trans. Embedd. Comput. Syst. 0, 0, Article 0 (January 2020), 33 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Spiking Neural Network (SNN) is an emerging computing model that uses spike-based computa-
tions and bio-inspired learning algorithms [71]. In an SNN, pre-synaptic neurons communicate
information encoded in spike trains to post-synaptic neurons, via synapses (see Fig. 1). Performance,
e.g., accuracy of an SNN model, is assessed in terms of the inter-spike interval (ISI), which is defined
as inverse of the mean firing rate of the neurons.

Authors’ address: Shihao Song; Harry Chong; Adarsha Balaji; Anup Das; James Shackleford; Nagarajan Kandasamy,
anup.das@drexel.edu, Drexel University, 3141 Chestnut Street, Philadelphia, PA, USA, 19104.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2020 Association for Computing Machinery.
1539-9087/2020/1-ART0 $15.00
https://doi.org/10.1145/1122445.1122456

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

 
 
 
 
 
 
0:2

Song, et al.

Fig. 1. Integration of spike trains at the post-synaptic neuron from four pre-synaptic neurons in a Spiking
Neural Network (SNN). Each spike is a voltage waveform of time duration to the order of ms.

SNNs are typically executed on neuromorphic hardware platforms such as DYNAP-SE [73],
TrueNorth [47], and Loihi [45]. These hardware platforms are designed as a tile-based architecture
with a shared, hierarchical interconnect to facilitate inter-tile communication (see Fig. 2) [25]. Each
tile consists of a crossbar for mapping neurons and synapses, and input and output buffer space for
communicating spikes over the interconnect. A crossbar is a 2D organization of horizontal and
vertical wires, where the horizontal wires are connected to pre-synaptic neurons while the vertical
wires are connected to post-synaptic neurons. Non-Volatile Memory (NVM) cells are placed at the
crosspoints of each crossbar to implement storage of synaptic weights [24, 72].1

Fig. 2. A tile-based neuromorphic architecture [25], which is representative of many neuromorphic platforms
such as DYNAP-SE [73], TrueNorth [47], and Loihi [45].

Energy consumed by neuromorphic hardware can be several orders of magnitude lower than a
conventional machine-learning accelerator such as Eyeriss [26]. This is due to low-power VLSI im-
plementation of analog neurons [62], low-power and high-density NVM-based synaptic storage [24],
as well as distributed computing and storage architecture using crossbars. Given these advantages,
a neuromorphic hardware can implement machine-learning tasks for power-constrained platforms
such as embedded systems and edge nodes of the Internet-of-Things (IoT) [5].

Unlike conventional von-Neumann computing systems, where CPUs compute by exchanging
data centrally from the main memory, synthesizing, i.e., compiling and mapping a machine-learning
program on a neuromorphic hardware is challenging. This is because in a neuromorphic hardware,
computation units (i.e., the neurons) and storage units (i.e., the synapses) are distributed within the
hardware as crossbars. It is therefore important to properly partition a large SNN model such that
it can be mapped efficiently to the underlying resources. Additionally, each crossbar also presents
limitations on how many pre-synaptic connections are allowed per post-synaptic neuron, and how
much buffer space is available to send and receive spikes over the interconnect. These hardware
limitations impact both model accuracy and hardware performance such as throughput, latency,
and energy consumption.

1Beyond neuromorphic computing, NVMs are also used as main memory for conventional computing using shared-memory
computers [83, 85, 87, 89, 90].

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

X4____,__,__L__J__---1...__,.. 17J...-f--t--:-_-___.......J_L--binary events post-synaptic neuron inter-spike interval (ISi) InterconnectTileTileTileTileTileTileInterconnectTileTileTileTileTileTileInterconnectTileTileTileTileTileTileBufferBufferDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:3

We develop DFSynthesizer, a systematic and end-to-end framework to analyze and map
machine-learning programs to state-of-the-art neuromorphic hardware, while guaranteeing perfor-
mance. Following are our key contributions.2

• Contribution 1. We present an approach to analyze machine-learning programs and generate
SNN workload using representative data. Our framework allows workload generation with
only a modest impact on model performance.

• Contribution 2. We present an approach to decompose and partition complex SNN workloads
and generate clusters of neurons and synapses such that each cluster can fit onto the resources
of a crossbar in the hardware.

• Contribution 3. We exploit the rich semantics of Synchronous Dataflow Graphs (SDFGs) [69]
to represent clustered SNN programs. This allows for the SNN’s performance, e.g., throughput,
to be estimated on the hardware as a function of key properties such as number of crossbars,
dimension of crossbars, buffer space on tiles, and tile communication bandwidth.

• Contribution 4. We develop a novel scheduling algorithm based on Self-Timed Execution
for executing clusters on crossbars of a neuromorphic hardware, providing performance
guarantee in scenarios with dynamic resource availability.

• Contribution 5. We propose a design-space exploration framework incorporating DFSynthesizer
that allows the Pareto-space of different SNN mappings to hardware to be explored while
considering other hardware metrics such as energy, latency, and reliability.

• Contribution 6. We evaluate DFSynthesizer using 10 machine learning programs that are
representative of the three most commonly used neural network classes — convolutional
neural network (CNN), multi-layer perceptron (MLP), and recurrent neural network (RNN).

2 SCOPE AND HIGH-LEVEL OVERVIEW OF DFSYNTHESIZER

DFSynthesizer is developed for supervised machine learning approaches, where a machine-
learning model is first trained using representative data from the field. Machine learning inference
refers to generating output from the trained model by feeding live data. To improve energy efficiency,
the inference is performed on a neuromorphic hardware. Once deployed on the hardware, the
model is expected to perform inference in real-time on a continuous basis from data collected using
sensors.3 Therefore, a key performance metric for neuromorphic hardware performing real-time
inference is throughput, defined as the number of frames processed per unit time, where a frame is
defined as an individual image (for image-based models) or a window of time-series data.4

Figure 3 illustrates the proposed end-to-end framework of DFSynthesizer, which synthesizes, i.e.,
compiles and maps a machine learning program to a neuromorphic hardware in four steps. First, it
analyzes a machine learning program written in a high-level language such as Python and C/C++
to generate SNN workload (Section 3). Second, it compiles SNN workloads to an intermediate
representation format (h5 and json), performing spatial decomposition and clustering to fit onto the
resources of a crossbar (Section 4). Third, it uses Synchronous Dataflow Graph (SDF) to represent
clustered SNN (in XML representation), allocating resources to the clusters considering hardware

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:4

Song, et al.

Fig. 3. High-level overview of DFSynthesizer. A machine learning program is analyzed and mapped to the
hardware using the proposed 4-step methodology.

resource constraints (Section 5). Finally, it schedules the SDF representation of a clustered SNN to
the hardware crossbars, guaranteeing performance (Section 6).
3 PROGRAM ANALYSIS AND WORKLOAD GENERATION
In this step, a machine-learning program is analyzed to generate its workload. In the following, we
discuss the steps involved in the workload generation.

3.1 Workflow for Workload Generation
Figure 4 summarizes the workflow of the workload generation step of DFSynthesizer, where a
machine-learning program is analyzed to generate its workload which is then used to map the
application to a neuromorphic hardware.

Fig. 4. Workflow of the workload generation step of DFSynthesizer.

DFSynthesizer can incorporate both Artificial Neural Networks (ANNs) and Spiking Neural
Networks (SNNs) in its workflow. At a high level, the proposed workflow consists of a model
training component followed by model analysis. In the following, we elaborate on these components.

2Contributions 2, 3, and 4 appeared in our prior work [86]. This work introduces the contributions 1, 5, and 6.
3Camera sensors are used for image classification models, e.g., LeNet, AlexNet, and VGG16, while electrocardiogram sensors
are used for heart-rate classification and estimation models. See our evaluation setup in Section 7.
4By maximizing the throughput, DFSynthesizer minimizes the time to process individual frame using the neuromorphic
inference hardware, which makes DFSynthesizer applicable to both real-time and non real-time applications.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

Proposed Solution25Workload GenerationHLLC++PythonCompilationh5 jsonIntermediateResource AllocationSDFBackendSchedulinghardware specificBackendDATE’18ISVLSI’19LCPC’20EDCC’20MWCAS’20JSPS’20http://www.pages.drexel.edu/~ad3639/publications.htmlSNNSimulationDecomposition and ClusteringLoad BalancingPerformance GuaranteeModel AnalysisModel TrainingSNN ConversionFunctional Simulation (CARLSim) ANN ModelSNN ModelModel Parsing*.json, *.h5KerasPyCARLDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:5

3.2 Model Training
3.2.1 Training Artificial Neural Networks. DFSynthesizer’s frontend is integrated with Keras [57],
which is used to define a model and train it on a database. Keras utilizes Tensorflow backend [1].
DFSynthesizer also supports other frameworks such as PyTorch [76]. To demonstrate the capabilities
of DFSynthesizer, we evaluate it with three Convolutional Neural Network (CNN) architectures – 1)
LeNet [68], trained on MNIST handwritten digit dataset [49], 2) AlexNet [66], trained on ImageNet
dataset [48], and 3) VGGNet [82], trained on ImageNet dataset. These models are derived from the
MLPerf [78] dataset and instantiated in Keras. We use a Lambda workstation with two GPUs (see
our evaluation setup in Section 7) to train these models.

3.2.2 Training Spiking Neural Networks. DFSynthesizer’s frontend supports training SNN models
using PyCARL [13], a Python frontend to CARLsim [28]. CARLsim facilitates SNN simulations
using CPUs and multi-GPUs. PyCARL is designed to integrate with PyNN [46], which provides
a common frontend to different SNN simulators with various degrees of neurobiological details.
We use CARLsim for model training. CARLsim’s support for built-in biologically realistic neuron,
synapse, current and emerging learning models and continuous integration and testing, make it an
easy-to-use and powerful simulator of biologically-plausible SNN models. DFSynthesizer can also
utilize other SNN simulators such as Brian [55], NEST [51], and NEURON [59] for model training.

3.3 Model Analysis
3.3.1 Model Parsing and Conversion. Unfortunately, ANN models cannot be executed directly on
event-driven neuromorphic hardware platforms such as DYNAP-SE [73], TrueNorth [47], and
Loihi [45]. Recently, many tools have been proposed to convert ANN operations to SNNs. Examples
include Nengo [19], N2D2 [22], and SNNToolBox [79]. A common limitation of these toolboxes
is that they are open-loop converters, meaning that the conversion is performed considering per-
formance degradation only. In our prior work [9], we have proposed a closed-loop conversion
mechanism, where the conversion of analog operations to spiking equivalent is performed consid-
ering the energy consumption on hardware. These conversion steps are briefly discussed below.5

(1) ReLU Activation Functions: This is implemented as the approximate firing rate of a leaky

integrate and fire (LIF) neuron.

(2) Bias: A bias is represented as a constant input current to a neuron, the value of which is

proportional to the bias of the neuron in the corresponding analog model.

(3) Weight Normalization: This is achieved by setting a factor 𝜆 to control the firing rate of spiking

neurons.

(4) Softmax: To implement softmax, an external Poisson spike generator is used to generate

spikes proportional to the weighted sum accumulated at each neuron.

(5) Max and Average Pooling: To implement max pooling, the neuron which fires first is considered
to be the winning neuron, and therefore, its responses are forwarded to the next layer,
suppressing the responses from other neurons in the pooling function. To implement average
pooling, the average firing rate (obtained from total spike count) of the pooling neurons are
forwarded to the next layer.

5The conversion framework was introduced in [9] for converting CNN-based HeartClass application to its equivalent
SNN representation. We used this application to evaluate DFSynthesizer. Additionally, we have extended the conversion
framework to add other key functionalities such as Layer Flattening, Concatenation, Binary Weight Activation, and Non-
Zero Biases. These new functionalities allowed the conversion framework to convert state-of-the-art CNN architectures
such as LeNet, AlexNet, and VGG16, which are used to evaluate DFSynthesizer.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:6

Song, et al.

We have extended our framework with the following new functionalities to allow for the
conversion of CNN architectures such as LeNet, AlexNet, and VGGNet to their spiking counterparts.
(1) 1-D Convolution: The 1-D convolution is implemented to extract patterns from inputs in a
single spatial dimension. A 1xn filter, called a kernel, slides over the input while computing
the element-wise dot-product between the input and the kernel at each step.

(2) Residual Connections: Residual connections are implemented to convert the residual block
used in CNN models such as ResNet. Typically, the residual connection connects the input of
the residual block directly to the output neurons of the block, with a synaptic weight of ‘1’.
This allows for the input to be directly propagated to the output of the residual block while
skipping the operations performed within the block.

(3) Flattening: The flatten operation converts the 2-D output of the final pooling operation into a
1-D array. This allows for the output of the pooling operation to be fed as individual features
into the decision-making fully connected layers of the CNN model.

(4) Concatenation: The concatenation operation, also known as a merging operation, is used as a
channel-wise integration of the features extracted from 2 or more layers into a single output.
Table 1 reports the accuracy impact due to the SNN conversion of three state-of-the-art supervised
CNN models. These accuracy numbers are obtained from CARLsim [28], which allows functional
simulation and performance estimation of SNN-based applications. We use these three converted
CNN models to evaluate DFSynthesizer (See Section 7).

Application

Top-1 Accuracy (%)

Original

SNN

Application

Top-1 Accuracy (%)

Original

LeNet

94.98%

94.08%

AlexNet

74.1%

Application

Top-1 Accuracy (%)

Original

SNN

VGG16

93.56%

91.62%

SNN

71.7%

Table 1. Accuracy impact due to conversion of three state-of-the-art CNN models to their SNN equivalent.
The original accuracy numbers are obtained by simulating these architectures in Keras [57] with Tensorflow
backend [1]. The converted accuracy numbers reported in the columns marked “SNN” are obtained from
CARLsim [28]. We use a multi-GPU machine to simulate these architectures using both Keras and CARLsim.
See our evaluation framework in Section 7.

3.3.2 Workload Generation. The SNN model (or the converted ANN model) is analyzed in CARLsim
to generate the following information.

• Spike Data: the exact spike times of all neurons in the SNN model. We let 𝑠𝑝𝑘 (𝑖) represents a

list of spike times of the 𝑖th neuron in the model.

• Weight Data: the synaptic strength of all synapses in the SNN model. We let 𝑤 (𝑖, 𝑗) represents
the synaptic weight of the connection between the 𝑖th and 𝑗 th neurons in the SNN model.
The spike and weight data of a trained SNN form the SNN workload. Formally, an SNN workload

is defined as

Definition 1. (SNN Workload) An SNN Workload GSNN = (N, S, W) is a directed graph consisting

of a finite set 𝑁 of neurons, a set 𝑆 of spikes, and a set 𝑊 of synapses between the neurons.

4 PROGRAM COMPILATION AND PERFORMANCE ESTIMATION
In this step, DFSynthesizer clusters a given machine-learning model to map onto the crossbars of a
neuromorphic hardware. To do so, we first introduce the system architecture and then discuss the
clustering step needed to map applications to this architecture.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:7

4.1 System Architecture
Figure 5 illustrates our system architecture. DFSynthesizer is designed for crossbar-based neuro-
morphic hardware designs as shown in Figure 2. This is representative of many recent neuromorphic
designs [3, 25, 56, 61]. A machine learning model (ANN or SNN) is first analyzed to generate its
workload (Section 3). This workload is then partitioned to generate clusters, where each cluster
consists of a fraction of the neurons and synapses of the original machine learning model. The
cluster workload is stored in a disk along with other machine learning workloads. To execute a
specific workload on the neuromorphic hardware, it is first loaded into the host memory and then
the clusters are programmed on to the crossbars of the hardware via the PCIe interface.6

Fig. 5. Our system architecture, integrating a neuromorphic hardware. DFSynthesizer is designed for crossbar-
based neuromorphic hardware [3, 25, 56, 61]. This is representative of many recent neuromorphic designs. To
evaluate DFSynthesizer, we have configured our evaluation setup to model the DYNAP-SE hardware [73].

In the remainder of this section, we describe the workload compilation step of DFSynthesizer,
which consists of the following two design components – Workload Decomposition and Workload
Clustering. We conclude this section by providing a dataflow modeling approach for clustered
workloads and performance estimation using such model.

4.2 Workload Decomposition
We note that each 𝑁 × 𝑁 crossbar in a neuromorphic hardware can accommodate up to 𝑁 pre-
synaptic connections per post-synaptic neuron, with typical value of 𝑁 set between 128 (in DYNAP-
SE) and 256 (in TrueNorth). Figure 6 illustrates an example of mapping a) one 4-input, b) one
3-input, and c) two 2-input neurons on a 4 × 4 crossbar. Unfortunately, neurons with more than 4
pre-synaptic connections per post-synaptic neuron cannot be mapped to the crossbar. In fact, in
many complex machine learning models such as AlexNet and VGG16, the number of pre-synaptic
connections per post-synaptic neuron is much higher than 128. Therefore, these neurons cannot be
mapped to a 128 × 128 crossbar in DYNAP-SE.

To address the above limitation, we have previously proposed a spatial decomposition technique
which exploits the firing principle of LIF neurons, decomposing each neuron with many pre-synaptic
connections into a sequence of homogeneous fanin-of-two (FIT) neural units [16].

6Although we illustrate the crossbars to be interconnected in a mesh-based architecture such as Networks-on-Chip
(NoC) [20], DFSynthesizer can work with other interconnect types such as Segmented Bus [12].

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

ANN/SNN User 1 f-----+ Model Disk ML 3 data+ parameters ML 1 data+ parameters ML 2 data+ parameters Workload f-----+ Generation Host Memory Kernel ML1 ML2 PCIe Data 1 Data 2 Data N ···················------cParameters_l Parameters_2 Parameters _ N Cluster 1 Cluster 2 Cluster N Data_l s Parameter 1 • • • • • s s Data_N Neuromorphic Hardware Parameter N PCIe 0:8

Song, et al.

Fig. 6. Example mapping of a) one 4-input, b) one 3-input, and c) two 2-input neurons on a 4 × 4 crossbar.

Figure 7 illustrates the spatial decomposition using a small example of a 3-input neuron shown
in Figure 7(a). We consider the mapping of this neuron to 2x2 crossbars. Since each crossbar can
accommodate a maximum of two pre-synaptic connections per neuron, the example 3-input neuron
cannot be mapped to the crossbar directly. The most common solution is to eliminate a synaptic
connection, which may lead to accuracy loss. Figure 7(b) illustrates the decomposition mechanism,
where the 3-input neuron is implemented using two FIT neural units connected in sequence as
shown in Figure 7(b). Each FIT unit is similar to a 2-input neuron and it exploits the leaky integrate
behavior in hardware to maintain the functional equivalence between Figures 7(a) and 7(b).

Fig. 7. Illustrating the decomposition of a 3-input neuron (a) to a sequence of FIT neural units (b). The
mapping of the FIT units to two 2x2 crossbars is shown in (c).

For the sake of completeness, Figure 7(c) illustrates the mapping of the decomposed neuron
utilizing two 2x2 crossbars. The functionality of the FIT neural units is implemented using the
Non-Volatile Memory (NVM) cells of the two crossbars.

To describe the decomposition Algorithm, we introduce the following notations. Let 𝑛1

be the 𝑚𝑖 pre-synaptic connections of the neuron 𝑁𝑖. Let 𝐹 1
units that are generated by spatially decomposing this neuron. The input of unit 𝐹 𝑗
𝐼𝑛 (𝐹 𝑗

𝑖 ) can be represented as

𝑖 , 𝐹 2

𝑖

𝑖 , · · · , 𝐹 𝑚𝑖 −1

𝑖 , 𝑛2

𝑖 , · · · , 𝑛𝑚𝑖
be the (𝑚𝑖 − 1) FIT neural
𝑖 denoted as

𝑖

𝐼𝑛 (𝐹 𝑗

𝑖 ) =

(cid:40)

{𝑛1
𝑖 , 𝑛2
𝑖 }
, 𝑂𝑢𝑡 (𝐹 𝑗 −1
{𝑛 𝑗 +1
𝑖

𝑖

) }

for j = 1
otherwise

∀𝑗 ∈ {1, 2, · · · , 𝑚𝑖 − 1}

(1)

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

(a)(b)(c)FITFITabcabcSpike DecoderSpike EncoderSpike DecoderSpike EncoderdooabddcoInterconnectd(a) Original Neuron(b) Decomposed Neuron(c) Mapping the decomposed neuron to two crossbarsDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:9

𝑖 ) is the output of the unit 𝐹 𝑗

where 𝑂𝑢𝑡 (𝐹 𝑗
𝑖 . When decomposing a neuron, we note that the first FIT
unit uses two of the original inputs of the original neuron. Subsequently, all other FIT units use
one of the original inputs and the output of the preceding FIT units as shown in Figure 7(b).

Formally, a decomposed SNN graph is defined as follows.
Definition 2. (Decomposed SNN Graph) A decomposed SNN graph GDSNN = (F, L) is a directed

graph consisting of a finite set F of FIT neural units and a finite set L of links between these units.

Algorithm 1 shows the pseudo-code of the spatial decomposition technique, which performs the
graph transformation 𝐺𝑆𝑁 𝑁 → 𝐺𝐷𝑆𝑁 𝑁 . For each neuron 𝑁𝑖 (line 1), a set of inputs to this neuron is
obtained (line 2). The first FIT unit is formed using two input inputs (line 3). This is in accordance
with Equation 1 and Figure 7(b). The FIT unit is inserted into the decomposed graph 𝐺𝐷𝑆𝑁 𝑁 (line
4). The algorithm then creates the other FIT units iteratively (lines 5-8) using Equation 1 and stores
those units in 𝐺𝐷𝑆𝑁 𝑁 . Finally, the graph 𝐺𝐷𝑆𝑁 𝑁 is returned (line 10).

The overall complexity of this algorithm is calculated as follows. The Out for loop (lines 1-9)
is executed for the neurons in the original graph 𝐺𝑆𝑁 𝑁 , i.e., for |𝑁 | times. Within each iteration,
the algorithm creates a total of ( |𝐼𝑛 (𝑁𝑖 ) | − 1) FIT units, where 𝐼𝑛 (𝑁𝑖 ) is the set of input of neuron 𝑁𝑖.
Therefore, the algorithmic complexity is

Complexity = O (cid:169)
(cid:173)
(cid:171)

(cid:19)

|𝐼𝑛 (𝑁𝑖 | − 1

|𝑁 |
∑︁

(cid:18)

𝑖=1

(cid:170)
(cid:174)
(cid:172)

≈ O ( |𝑊 |)

(2)

In deriving the final expression, we note that the input connections of all the neurons in the graph
𝐺𝑆𝑁 𝑁 are the edges 𝑊 in the graph.

Algorithm 1: Spatial decomposition of SNN graph 𝐺𝑆𝑁 𝑁 .
Input: 𝐺𝑆𝑁 𝑁 = (N, W)
Output: 𝐺𝐷𝑆𝑁 𝑁 = (F, L)

} = In(𝑁𝑖 ) ;

3

𝑖 , 𝑛2

𝑖 , · · · , 𝑛𝑚𝑖

1 for 𝑁𝑖 ∈ N do
{𝑛1
2
𝑖
𝑖 with In(𝐹 1
Create node 𝐹 1
𝐺𝐷𝑆𝑁 𝑁 .insert(𝐹 1
𝑖 );
for 𝑗 = 2; 𝑗 < 𝑚𝑖 ; 𝑗 + + do
Create node 𝐹 𝑗
𝐺𝐷𝑆𝑁 𝑁 .insert(𝐹 𝑗

7

5

6

4

𝑖 with in(𝐹 𝑗
𝑖 );

𝑖 ) = {𝑛1, 𝑛2 } ;

𝑖 ) = {𝑛 𝑗 +1

𝑖

, 𝐹 𝑗 −1
𝑖

};

/* insert the FIT neural unit 𝑢𝑖

/* for each node of 𝐺𝑆𝑁 𝑁 */
/* input links of 𝑁𝑖 */
/* first FIT unit */
1 in 𝐺𝐷𝑆𝑁 𝑁 */
/* remaining FIT units */

end

8
9 end
10 Return 𝐺𝐷𝑆𝑁 𝑁

4.3 Workload Clustering
The decomposed SNN graph is clustered such that each cluster is able to fit onto a crossbar. Figure 8
illustrates the concept using an example of a decomposed SNN graph shown in (❶). The nodes are
the FIT neural units and the links are the synaptic connections. The number on a link represents
the average number of spikes communicated between the source and destination FIT units for
the representative training data. We consider the mapping of this decomposed SNN graph to a
hardware with 2 × 2 crossbars. Since a crossbar in this hardware can only accommodate a maximum
of 2 pre-synaptic connections, we partition the graph of (❶) into two partitions (shown in two
different colors) in (❷). These partitions can then be mapped to the two crossbars as shown in
(❸), with an average 8 spikes communicated between the crossbars due to the mapping of the
link between neuron d and e on the shared interconnect of the hardware. Finally, the two clusters
generated from the SNN graph are shown in (❹) along with the inter-cluster communication.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:10

Song, et al.

Fig. 8. Illustration of SNN graph clustering. (❶) is the original decomposed SNN graph with FIT neural units
shown as the nodes and average spikes communicated between them shown on the links. (❷) shows the
partitioning of this graph. (❸) shows the mapping of the partitions to the two crossbars. (❹) shows the two
clusters generated from the SNN graph of (❶) considering the constraints of the crossbar.

Formally, a clustered SNN graph is defined as follows.

Definition 3. (Clustered SNN Graph) A clustered SNN graph GCSNN = (A, C) is a directed graph

consisting of a finite set A of clusters and a finite set C of connections between these clusters.

Recently, different approaches have been proposed for clustering SNNs. Examples include
SpiNeMap [14] for energy minimization and NEUTRAMS [63] for performance. See Section 9
for a comprehensive overview of other state-of-the-art SNN clustering approaches.

We formulate SNN clustering as a graph transformation problem and introduce an efficient
algorithm to improve resource utilization. This objective is essential to provide tighter guarantee
on performance of SNNs in hardware as we demonstrate in Section 8.

The graph transformation 𝐺𝐷𝑆𝑁 𝑁 → 𝐺𝐶𝑆𝑁 𝑁 is a classical graph partitioning problem [65], and
has been applied in many contexts, including task mapping on multiprocessor systems [38]. We
propose a greedy approach to pack the FIT neural units and synapses of the decomposed SNN graph
𝐺𝐷𝑆𝑁 𝑁 into clusters, improving cluster resource utilization. Algorithm 2 provides the pseudo-code
of the clustering algorithm. For each node of the unrolled graph, the algorithm tries to see if the
node can be merged into one of the existing clusters (line 3), before creating a new one (lines
4–8). In this algorithm, clusters in 𝐺𝐶𝑆𝑁 𝑁 are sorted in descending order of neuron and synapse
utilization (line 12), so that the heavily utilized clusters are first considered for packing neurons
and synapses, further improving their utilization.

4.4 Dataflow Modeling of Clustered Workload
We model a clustered SNN as a Synchronous Data Flow Graph (SDFG) for predictable performance
analysis [69]. SDFGs are commonly used to model streaming applications that are implemented
on a multi-processor system-on-chip [94]. These graphs are used to analyze a system in terms
of key performance properties such as throughput, execution time, communication bandwidth,
and buffer requirements [96]. Nodes of an SDFG are called actors. Each node is a cluster of the
clustered SNN graph GCSNN = (A, C). Actors are computed by reading tokens, i.e., spikes from their
input ports and writing the results of the computation as tokens on the output ports. The number
of tokens produced or consumed in one execution of an actor is called the port rate. They represent
the number of spikes per unit time at the input and output of different clusters in the SNN. Port

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

adbecabddceCluster 1Cluster 2BufferBufferBufferBuffer94813894813212834adbec948131DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:11

Algorithm 2: Utilization-aware SNN clustering.

find 𝐶 𝑗 ∈ cluster_list such that 𝐹𝑖 can be packed in 𝐶 𝑗 while improving neuron and synapse utilization of 𝐶 𝑗 ;
if 𝐶 𝑗 = ∅ then

Input: 𝐺𝐷𝑆𝑁 𝑁 = (F, L)
Output: 𝐺𝐶𝑆𝑁 𝑁 = (A, C)

1 𝐺𝐶𝑆𝑁 𝑁 = {} and cluster_list = {};
2 foreach 𝐹𝑖 ∈ F do
3

4

5

6

7

8

9

10

11

12
13 end

Create new cluster 𝐶new;
Assign 𝐹𝑖 and its synaptic connections to 𝐶new;
𝐺𝐶𝑆𝑁 𝑁 .push(𝐶new);

end
else

Assign 𝐹𝑖 and its synaptic connections to 𝐶 𝑗 ;

end
sort 𝐺𝐶𝑆𝑁 𝑁 in descending order of neuron and synapse utilizations;

rates are visualized as annotations on edges. Actor execution is also called firing, and it requires
a fixed amount of time to execute on a crossbar. Edges in the graph are called channels and they
represent dependencies among actors. An actor is said to be ready when it has sufficient input
tokens on all its input channels and sufficient buffer space on all its output channels; an actor can
only fire when it is ready. A set 𝑃𝑜𝑟𝑡𝑠 of ports is assumed, and with each port 𝑝 ∈ 𝑃𝑜𝑟𝑡𝑠, a finite
rate 𝑅𝑎𝑡𝑒 (𝑝) ∈ N \ {0} is associated. Formally, an actor is defined as follows.

Definition 4. (Actor) An actor a𝑖 is a tuple (𝐼𝑖, 𝑂𝑖, 𝜏𝑖, 𝜇𝑖 ) consisting of a set 𝐼𝑖 (⊆ 𝑃𝑜𝑟𝑡𝑠) of input
ports, a set 𝑂𝑖 (⊆ 𝑃𝑜𝑟𝑡𝑠) of output ports with 𝐼𝑖 ∩ 𝑂𝑖 = ∅, 𝜏𝑖 is the execution time of a𝑖 and 𝜇𝑖 is its
state space, i.e., buffer space needed for communicating spikes on all of its channels.

The source of channel 𝑐ℎ 𝑗

𝑖 ∈ 𝐶 is an output port of actor a𝑖 , the destination is an input port
of actor a𝑗 . All ports of all actors are connected to precisely one channel, and all channels are
connected to ports of some actors. The source and the destination port of channel 𝑐ℎ 𝑗
𝑖 are denoted
by 𝑆𝑟𝑐𝑃 (𝑐ℎ 𝑗
𝑖 ) respectively. Channels connected to the input and output ports of an
actor a𝑖 are denoted by 𝐼𝑛𝐶 (a𝑖 ) and 𝑂𝑢𝑡𝐶 (a𝑖 ) respectively.

𝑖 ) and 𝐷𝑠𝑡𝑃 (𝑐ℎ 𝑗

Before an actor a𝑖 starts its firing, it requires 𝑅𝑎𝑡𝑒 (𝑞𝑖 ) tokens from all (𝑝, 𝑞𝑖 ) ∈ 𝐼𝑛𝐶 (a𝑖 ). When the
actor completes execution, it produces 𝑅𝑎𝑡𝑒 (𝑝𝑖 ) tokens on every (𝑝𝑖, 𝑞) ∈ 𝑂𝑢𝑡𝐶 (a𝑖 ). One important
property of an SDFG is throughput, which is defined as the inverse of its long-term period. A period
is the average time needed for one iteration of the SDFG. An iteration is defined as the minimum
non-zero execution such that the original state of the SDFG is obtained. This is the performance
parameter used in this paper. Following definitions are introduced to formulate throughput.

Definition 5. (Repetition Vector) The Repetition Vector RptV of an SDFG is defined as the vector

specifying the number of times actors in the SDFG are executed in one iteration.

For the SDFG representation of a clustered SNN, all spikes generated on a channel are consumed
by the destination actor. This means that all actors are fired exactly once during one iteration of
the application. So, 𝑅𝑝𝑡𝑉 = [1111111].

4.5 Cyclic Dependency and Deadlock Avoidance
The clustering approach may lead to cyclic dependency among actors. Figure 9(a) illustrates a simple
feedforward network of 3 neurons (A, B, & C). Figure 9(b) illustrates a scenario where neurons
A and C are placed in cluster 1 (actor 1) and neuron B in cluster 2 (actor 2) during partitioning.
Due to the connectivity of the neurons in Figure 9(a), there is a cyclic dependency between the

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:12

Song, et al.

two actors: actor_1→actor_2→actor_1. SDF graphs allow representing such cyclic dependency
among actors, justifying our choice of using them for modeling clustered SNNs.

Fig. 9. An example cycle generated during clustering of SNNs.
However, presence of cycles complicates the scheduling problem because cyclic dependences
can lead to deadlocks. To address this, a cyclic SDF graph is decomposed into hierarchies of acyclic
subgraphs. To describe this, we introduce the following definition.

Definition 6. (Strongly Connected Subgraph) A subgraph 𝑍 of a directed (cyclic or acyclic)
graph is called a strongly-connected subgraph, iff for every pair of vertices 𝑎 and 𝑏 of 𝑍, there is a path
from 𝑎 to 𝑏 and a path from 𝑏 to 𝑎.

Fig. 10. Cycle breaking for deadlock avoidance of cyclic SDF graphs [18].
Figure 10 shows the flowchart for cycle breaking, also known as sub-independence partitioning,
which is the process of decomposition of strongly connected SDF graphs into hierarchies of acyclic
graphs. This is roughly based on the Loose Interdependence Algorithms Framework (LIAF) [18]. A
cyclic SDF graph is first decomposed into a series of strongly connected subgraphs 𝑍1, 𝑍2, · · · , 𝑍𝑁 .
For each strongly connected subgraph 𝑍𝑖, the LIAF algorithm tries to break cycles by properly
removing edges that have sufficient delays. Let 𝑍𝑖 (𝑉𝑖, 𝐸𝑖 ) be the strongly-connected subgraph of the
SDF Graph. An edge 𝑒 𝑗 ∈ 𝐸𝑖 can be removed if it has enough initial tokens to satisfy the consumption
requirements of its sink actor for a complete iteration of 𝑍𝑖 and scheduling 𝑍𝑖 without 𝑒 𝑗 does
not lead to deadlock. The edge 𝑒 𝑗 is called inter-iteration edge. The inter-iteration edge removal is
performed iteratively until the new subgraph with the inter-iteration edges removed is no longer a
strongly connected subgraph (i.e., it becomes a loosely connected subgraph). The subgraph is pushed
into a ready list for scheduling purposes. The algorithm is repeated for all the strongly-connected
subgraphs. At the end, all deadlock-free subgraphs are scheduled.

4.6 Performance Estimation
We present an approach to compute the application period of an SDFG by analyzing its maximum
cycle mean (MCM) and assuming infinite hardware resources. For this, we use Max-Plus Algebra
[29, 58, 107]. The Max-Plus semiring Rmax is the set R ∪ {−∞} defined with two basic operations
⊕ and ⊗, which are related to linear algebra as

𝑎 ⊕ 𝑏 = max(𝑎, 𝑏) and 𝑎 ⊗ 𝑏 = 𝑎 + 𝑏.

(3)

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

ABCACBCluster 1 (actor 1)Cluster 2 (actor 2)(a)(b)Generated Strongly Connected SubgraphsRemove Inter-Iteration EdgesSubgraph strongly connected?YesSubgraphs presentt?YesNoSchedule Deadlock-free  subgraphsCyclic SDF GraphReady to ScheduleNoDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:13

The identity element 0 for the addition ⊕ is −∞ in linear algebra, i.e., 𝑎 ⊕ 0 = 𝑎. The identity

element 1 for the multiplication ⊗ is 0 in linear algebra, i.e., 𝑎 ⊗ 1 = 𝑎.

To use Max-Plus Algebra to analyze an SDFG, it is customary to express the time at which an
actor fires in terms of preceding firings in linear algebra and then use standard analysis techniques
for Max-Plus Algebra to estimate timing performance. We use the running example of the SDFG
in Figure 11(a), which is obtained by clustering EdgeDet [28], an application used to evaluate
DFSynthesizer (see Section 7). The clustering is performed considering 1024x1024 crossbars.7 The
firing end times of all 9 actors in the 𝑘 th iteration (in linear algebra) are

𝑡0 (𝑘) ≥ 𝑡0 (𝑘 − 1) + 𝜏0

𝑡1 (𝑘) ≥ 𝑡0 (𝑘) + 𝜏1

𝑡2 (𝑘) ≥ 𝑡1 (𝑘) + 𝜏2

𝑡3 (𝑘) ≥ max

𝑡4 (𝑘) ≥ max

(cid:104)
𝑡2 (𝑘), 𝑡5 (𝑘)
(cid:104)
𝑡1 (𝑘), 𝑡0 (𝑘)

(cid:105)

(cid:105)

+ 𝜏3

+ 𝜏4

𝑡5 (𝑘) ≥ max

𝑡6 (𝑘) ≥ max

𝑡7 (𝑘) ≥ max

𝑡8 (𝑘) ≥ max

(cid:105)

(cid:104)
𝑡2 (𝑘), 𝑡1 (𝑘), 𝑡4 (𝑘)
(cid:104)
𝑡2 (𝑘), 𝑡0 (𝑘)
(cid:104)
𝑡1 (𝑘), 𝑡0 (𝑘)
(cid:104)
𝑡2 (𝑘), 𝑡3 (𝑘), 𝑡6 (𝑘)

+ 𝜏6

+ 𝜏7

(cid:105)

(4)

(cid:105)

+ 𝜏5

(cid:105)

+ 𝜏8

Fig. 11. (a) An example of SDFG obtained from clustering of the EdgeDet application [28]. (b) Mapping of
the SDFG to a neuromorphic hardware with 4 tiles.

Observe that the firing end time of actor 𝐴0 in the 𝑘 th iteration is after its firing end time in the
(𝑘 − 1)th iteration. Furthermore, the production and consumption rates are the same for every
channel in the SDFG. Using previously introduced Max-Plus semantics, firing end times for every
actor in the SDFG can be expressed as

where T is a matrix in R8×8
The following definitions are introduced to estimate latency.

(5)
max that captures the actor execution times 𝜏𝑛 and tk = {𝑡0 (𝑘), 𝑡1 (𝑘), · · · , 𝑡8 (𝑘) }.

tk = ⊕T ⊗ tk−1

Definition 7. (Digraph) The digraph Γ(𝑇 ) of a 𝑛 × 𝑛 matrix 𝑇 with entries defined in Rmax is the
tuple ⟨𝐴, 𝐸⟩, where 𝐴 is the set of vertices, i.e., 𝐴 = {1, 2, · · · 𝑛} and 𝐸 is the set of connected ordered
arcs between vertices i.e., 𝐸 = {(𝑖, 𝑗) | 𝑇𝑖,𝑗 ≠ −∞}.

To give an example, the matrix 𝑇 =

(cid:21)

(cid:20)−∞ 6
3
1

corresponds to the digraph shown in Figure 12.

7We evaluate DFSynthesizer primarily for DYNAP-SE neuromorphic hardware with 128 × 128 crossbars [73]. Here we
configure 1024 × 1024 crossbars to generate fewer clusters from EdgeDet for illustration purposes.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

13889900610457024106634788363775652844688274682133808134565328550681388990061045702410663478836377565284468827468213380813456532855068tile 05068tile 131323132tile 2tile 3(a)(b)31320:14

Song, et al.

Fig. 12. An example digraph.

Definition 8. (Walk) A walk 𝑤 in digraph Γ(𝑇 ) is the sequence of arcs (𝑥1, 𝑥2)(𝑥2, 𝑥3) · · · (𝑥𝑘−1, 𝑥𝑘 );
head of an arc in the sequence is either the start vertex of the walk or tail vertex of a preceding arc;
and the tail vertex of an arc in the sequence is either the end vertex of the walk or head vertex of a
succeeding arc. Weight of the walk is given by

(6)
Definition 9. (Cycle) A cycle 𝑐 in digraph Γ(𝑇 ) is the walk (𝑥1, 𝑥2)(𝑥2, 𝑥3) · · · (𝑥𝑘−1, 𝑥𝑘 ), such

|𝑤 |𝑇 = 𝑇𝑥1𝑥2 + · · ·𝑇𝑥𝑘−1

𝑥𝑘

that 𝑥𝑘 = 𝑥1.

Definition 10. (Maximum Cycle Mean) The maximum cycle mean, 𝜌max (𝑇 ) is the maximum of

the weight-to-length ratio of all cycles 𝑐 in Γ(𝑇 ) i.e.,

𝜌max (𝑇 ) = max

∀𝑐 in Γ (𝑇 )

|𝑐 |𝑇
|𝑐 |

= max
𝑘 >1

max
𝑥1,··· ,𝑥𝑘−1

𝑇𝑥1𝑥2 + · · ·𝑇𝑥𝑘−1
𝑘 − 1

𝑥𝑘

(7)

In this paper, performance of an SNN is defined in terms of throughput of the equivalent

SDFG, measured as the inverse of its maximum cycle mean (Equation 7), i.e.,

Performance (throughput) =

1
𝜌max (𝑇 )

(8)

In Equation 8, the performance is computed using the worst-case execution time of an actor on a
crossbar. This is obtained from the propagation delay of current through the synaptic elements in
the crossbar. As shown in many recent works [99, 100, 102], the current propagation delay within
a crossbar depends on the specific synaptic elements that are being activated in the crossbar. This
is due to the difference in the amount of parasitic components on the bitlines and wordlines of a
crossbar along the different current paths. For performance guarantee purposes, we assume the
worst-case propagation delay in the crossbar, and use the same to represent the execution time of
actors on the crossbars of a neuromorphic hardware.

The performance metric defined in Equation 8 provides the maximum throughput, considering
only the worst-case execution time of actors. However, a neuromorphic hardware introduces
constraints such as limited buffer space on the crossbars and non-zero latency on the interconnect,
which can lower the throughput significantly. Therefore,

Throughput(cid:12)

(cid:12)𝑆𝑁 𝑁

≤ Throughput(cid:12)

(cid:12)max

=

1
𝜌max (𝑇 )

(9)

In this work, we show that performance is impacted by
(1) how hardware resources are allocated to actors of a clustered SNN (Section 5), and
(2) how actors mapped to the same crossbar are time-multiplexed and scheduled (Section 6).
We seek to find the lower bound on performance (Throughput(cid:12)

) such that

(cid:12)bound

Throughput(cid:12)

(cid:12)bound

≤ Throughput(cid:12)

(cid:12)𝑆𝑁 𝑁

≤ Throughput(cid:12)

(cid:12)max

(10)

By making Throughput(cid:12)

(cid:12)bound

close to Throughput(cid:12)

(cid:12)max

, we provide a tighter bound on performance.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

12613DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:15

5 RESOURCE ALLOCATION AND HARDWARE MAPPING
The performance obtained using Equation 7 defines the maximum throughput obtained when the
clustered SNN is mapped to a hardware with infinite resources, i.e., a hardware with as many
crossbars as the number of actors (clusters) in the clustered SNN graph. Additionally, each crossbar
is assumed to have sufficient buffer space to send and receive spikes over the shared interconnect.
However, state-of-the-art neuromorphic hardware platforms present the following three critical
limitations. First, the number of crossbars in a neuromorphic hardware is limited. Therefore, the
available crossbars need to be time-multiplexed amongst the clusters of an SNN. Second, the input
and output buffer space on each crossbar are limited. Therefore, no more than one cluster can be
executed on a crossbar concurrently. Third, the communication bandwidth of each tile is limited.
Therefore, only a few spikes can be sent or received from the interconnect at once. Formally, a
neuromorphic hardware is defined as follows.

Definition 11. (Neuromorphic Hardware Graph) A neuromorphic hardware graph 𝐺𝑁 𝐻 = (T, I)

is a directed graph consisting of a finite set T of tiles and a finite set I of interconnect links.

Each tile consists of a crossbar to map neurons and synapses, and input and output buffers to re-
ceive and send tokens (spikes) over the interconnect, respectively. A tile 𝑇𝑖 is a tuple ⟨𝑁 , 𝑖𝑛𝐵𝑖, 𝑜𝑢𝑡𝐵𝑖 ⟩,
where 𝑁𝑖 is the dimension of the crossbar on the tile, i.e., the tile 𝑇𝑖 can accommodate 𝑁𝑖 pre-synaptic
𝑖 synaptic connections, 𝑖𝑛𝐵𝑖 is the input buffer size on the
neurons, 𝑁𝑖 post-synaptic neurons, and 𝑁 2
tile, and 𝑜𝑢𝑡𝐵𝑖 is its output buffer size. Each interconnect link is bidirectional, representing two-way
communication between the source and destination tiles with a fixed bandwidth 𝐵𝑊 .

The mapping M : 𝐺𝐶𝑆𝑁 𝑁 → 𝐺𝑁 𝐻 is specified by matrix (𝑚𝑖 𝑗 ) ∈ {0, 1}|A|×|T|, where 𝑚𝑖 𝑗 is defined as

𝑚𝑖 𝑗 =

(cid:40)

1

if actor 𝐴𝑖 ∈ A is mapped to tile 𝑇𝑗 ∈ T

0 otherwise

The mapping constraint is that a cluster can be mapped to only one tile, i.e.,

𝑚𝑖 𝑗 = 1 ∀𝑖

∑︁

𝑗

(11)

(12)

The throughput of the clustered SNN graph 𝐺𝐶𝑆𝑁 𝑁 on the neuromorphic hardware 𝐺𝑁 𝐻 for

mapping M is computed as

𝜏M = DFSynthesizer(𝐺𝐶𝑆𝑁 𝑁 , 𝐺𝑁 𝐻 , M),
where DFSynthesizer is the extended Max-Plus formulation of Equation 7 incorporating platform
constraints. The following three steps describe DFSynthesizer. Without loss of generality, we use
Equation 14 as a running mapping example, where the 9 actors of Figure 11 are mapped to 4 tiles.

(13)

tile_0 : 𝐴3, 𝐴8, tile_2 : 𝐴1, 𝐴2, 𝐴4
tile_1 : 𝐴5, 𝐴6 tile_3 : 𝐴0, 𝐴7

The mapping corresponding to Equation 14 is therefore M =

0
0
0
1










0
0
1
0

0
0
1
0

1
0
0
0

0
0
1
0

0
1
0
0

0
1
0
0

0
0
0
1

(14)

1
0
0
0

𝑇

.










5.1 Step 1: Modeling Limited Buffer Sizes of Crossbars
Limited input and output buffer sizes of a tile are modeled as back-edges with initial tokens
indicating the buffer size available on the tile. This is illustrated in Figure 11(b) with the back-edge
from 𝐴8 to 𝐴3, both of which are mapped to tile 0. When an actor generates spikes on a channel, the
available size reduces; when the receiving actor consumes the spike, the available buffer is released.
In the example, before 𝐴3 can be executed, it has to check if enough buffer space is available. This is

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:16

Song, et al.

modeled by requiring tokens from the back-edge to be consumed. Since it produces 5068 spikes per
firing, 5068 tokens from the back-edge are consumed, indicating reservation of the buffer spaces. On
the consumption side, when 𝐴8 is executed, it frees 5068 buffer spaces, indicated by a release of these
tokens on the back-edge. We assume atomic execution of actors on a crossbar, i.e., a crossbar reads
input tokens and produces output tokens in the output buffer for no more than one actor at any
given instance of time. To prevent other actors mapped to the same tile from firing simultaneously,
the output buffer space is claimed at the start of execution and released only at the end of firing.

5.2 Step 2: Actor Ordering on Crossbars
The number of crossbars in a neuromorphic hardware is limited. Therefore they may have to be
shared between actors of an SNN. However, on a tile, only one instance of an actor can be executing
at the same moment in time. We use time-division multiple-access (TDMA) to allocate time slices to
actors mapped to the same tile. During its allocated time slice, an actor is executed on the crossbar
of the tile and generates spikes, which are stored in the output buffer for communication on the
interconnect. Next, we generate the order in which the actors bound to a tile are fired to provide
performance guarantee, i.e., throughput. For this, we apply our Max-Plus Algebra formulation
(Eq. 7) on the SDFG of Fig. 11(b). This is our static-order schedule, and is constructed at design time.

5.3 Step 3: Actor Execution on Crossbars
Once the static-order schedule is constructed for all tiles of the hardware, we use a self-timed
execution strategy [74] to execute these actors at run time. Here, the exact firing times of actors
are discarded, retaining only the assignment and ordering of actors on each tile as obtained from
the design-time analysis (step 2). At run time, ready actors are inserted into a list and fired in the
same order previously determined during design time.

5.4 Mapping Exploration
Sections 5.1 through 5.3 extend the Max-Plus formulation to incorporate platform constraints.
Using these constraints and the new formulation, one can estimate the throughput of a clustered
SNN on a neuromorphic hardware for a specific actor-to-tile mapping. In the following, we explain
the mapping scenario where the number of tiles in the hardware is less than the number of actors
in the clustered SNN. Therefore, each tile needs to be time-multiplexed between multiple actors.
Figure 13 conceptually illustrates the mapping exploration using DFSynthesizer compared to
state-of-the-art solutions and the selection of lower bound on throughput. ❶ represents the
throughput obtained using SpiNeMap [14], which optimizes energy consumption for a hardware
platform where the number of tiles is higher than the number of actors. When SpiNeMap is applied
to the case where the tiles need to be time-multiplexed, it randomly distributes the actors to the
tiles and schedules them arbitrarily, without considering throughput. Therefore, the throughput
represented by ❶ (SpiNeMap) is significantly lower than the maximum throughput (i.e., the upper
bound) represented using ❻. Therefore, the throughput variation is 𝑇❻ − 𝑇❶.

In Figure 13, ❷ represents the throughput obtained using a solution such as PyCARL [13], which
balances the load on each tile for a scenario where actors need to be time-multiplexed on the
tiles. However, the actors mapped to a tile are scheduled in an arbitrary order without considering
throughput. By balancing the tile load, PyCARL reduces the number of clusters mapped per tile,
which improves throughput. Therefore, the throughput represented by ❷ is higher than ❶, but
lower than the maximum throughput ❻. Therefore, the throughput variation is 𝑇❻ − 𝑇❷.

In Figure 13, ❸ represents the throughput obtained using our previous work SDFSNN [86],
which first balances the load of each tile by distributing the actors evenly, and then uses a dataflow

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:17

Fig. 13. Different mapping explorations and choices for the lower bound of throughput (see Equation 10).

approach to schedule the actors on each tile, improving throughput. The throughput represented by
❸ is therefore higher than both ❶ and ❷, but lower than the maximum throughput ❻. Therefore,
the throughput variation is 𝑇❻ − 𝑇❸.

In Figure 13, ❹ represents the throughput obtained using a mapping exploration framework,
which explores a combination of actor-to-tile mapping and dataflow-based scheduling of actors
on each tile to maximize the throughput. This throughput is higher than ❶-❸, and is closer to
the maximum throughput ❻. Finally, ❺ represents the throughput obtained using an actor-to-
tile mapping that jointly optimizes energy and throughput, and uses dataflow-based scheduling
of actors on each tile to further improve the throughput. Since this solution takes energy into
consideration in the mapping step, the throughput can be somewhat lower than ❹ as illustrated in
the figure. In Section 8, we evaluate all these approaches and show that ❺ is still higher than ❶-❸.
To conclude, the design-space exploration of DFSynthesizer can generate mappings representing
two minimum throughput solutions – ❹ and ❺. Although the maximum throughput remains
the same for DFSynthesizer and other state-of-the-art approaches, the minimum throughput of
DFSynthesizer (i.e, ❺) is higher than the minimum throughput obtained using all state-of-the-
art mapping solutions (i.e., ❶-❸). Therefore, the difference between maximum and minimum
throughput is the least in DFSynthesizer compared to all state-of-the-art solutions, meaning that
DFSynthesizer provides stricter performance guarantee, which is critical for real-time systems.
We now describe DFSynthesizer.

We integrate the extended Max-Plus formulation inside a design-space exploration framework to
obtain cluster mappings that are Pareto optimal in terms of hardware metrics such as throughput,
latency, energy, and reliability. In the following, we describe our mapping explorations considering
energy and throughput. Such formulations can be trivially extended to consider other metrics.

The energy consumption 𝐸M of the mapping M is measured considering the number of spikes
that are generated inside each tile and the number of spikes that are routed on the interconnect [101].
The energy parameters are reported in Table 3. Using these parameters, the energy consumption is
𝐸M = 𝐸𝑠𝑝𝑘 + 𝐸comm,

(15)

where 𝐸𝑠𝑝𝑘 is the energy consumed in generating the spikes and propagating the spike current
via the synapses, and 𝐸𝑐𝑜𝑚𝑚 is the energy consumed in communicating spikes via the shared
interconnect. where 𝑆 (𝑇𝑖 ) is the number of spikes generated inside tile 𝑇𝑖 ∈ 𝑇 and 𝑆 (𝐼𝑖,𝑗 ) is the
number of spikes communicated on the link 𝐼𝑖,𝑗 between tiles 𝑇𝑖 and 𝑇𝑗 in the hardware.

Our objective is to maximize throughput of a given machine-learning model on hardware (Eq. 7)
and minimize the hardware energy consumption (Eq. 15). We formulate a joint metric 𝜆 = 𝐸/𝜏, and
minimize it during our mapping explorations. To this end, we propose an iterative approach, which

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

1256Throughput431Random Allocation + Random Schedule (SpiNeMap)2Load Balance + Random Schedule (PyCARL)3Load Balance + Proposed Schedule (SDFSNN)4Proposed Allocation + Proposed Schedule5Pareto Mapping + Proposed Schedule (DFSynthesizer)6Highest ThroughputChoice of lower bound of throughputMax throughput0:18

Song, et al.

explores different mapping alternatives, satisfying the cluster mapping constraint (Eq. 12). For each
mapping alternative, we evaluate throughput and energy consumption. Finally, Pareto-optimal
mappings are retained and returned.

Algorithm 3 provides the pseudo-code of our proposed mapping exploration. We start by ran-
domly distributing clusters to the tiles (line 3). We evaluate throughput and energy consumption of
this mapping and compute the joint metric 𝜆 (lines 4–5). For each cluster, we do the following. We
move the cluster from its current tile to every other tile and recalculate 𝜆 (lines 6–10). If 𝜆 reduces,
the new mapping is retained (lines 11–13), and the algorithm proceeds to analyze the next cluster.
In this way, a local minimum is reached, starting from the initial random allocation of clusters. We
re-execute the algorithm 𝜂 times, starting with a different random allocation of the clusters each
time. In this way, many mappings are explored. Finally, mappings that are Pareto-optimal in terms
of throughput and energy consumption are retained.

/* This set holds all the mappings */
/* Run for 𝜂 times */

/* For each cluster in the graph 𝐺𝑐𝑙 */
𝑇𝐶𝑖 = GetTileOfCluster( M, C⟩); /* Get the tile to which the cluster 𝐶𝑖 is mapped in the mapping M
*/
for 𝑇𝑗 ∈ 𝑇 \ 𝑇𝐶𝑖 do

*/
/* Update the mapping to reflect the movement of cluster

/* Move the cluster to every other tile

Algorithm 3: Mapping of the clustered graph 𝐺𝑐𝑙 .
Input: 𝐺𝑐𝑙 = (𝐶, 𝐴), 𝐺𝑛ℎ = (𝑇 , 𝐼 )
Output: Mmax

1 M = { };
2 for 𝑟 = 0; 𝑟 < 𝜂; 𝑟 ++ do
3

Allocate clusters randomly to tiles. Call this mapping M;
Calculate 𝜏M using (7) and energy consumption 𝐸M using (15);
Calculate the joint metric 𝜆 = 𝜏M · 𝐸M ;
for 𝐶𝑖 ∈ 𝐶 do

4

5

6

7

8

9

10

11

12

13

14

M 𝑗 = MoveClusterToTile( M, 𝐶𝑖,𝑇𝑗 ) ;
𝐶𝑖 to tile 𝑇𝑗 */
Calculate 𝜏M 𝑗 , 𝐸M 𝑗 , and 𝜆𝑗 ;
if 𝜆 𝑗 < 𝜆 then
M = M 𝑗 ;

end

end
M.insert( M)

15
16 end
17 M𝑃𝑂 = ParetoOptimization(M);
18 Return Mmax, the mapping with minimum execution time.

/* If the joint metric improves */
/* Retain the new mapping */

/* Retain only the Pareto-Optimal Mappings */

The complexity of this algorithm is as follows. The unit function GetTileofCluster is essentially an
argmax function with a complexity of 𝑂 ( |𝑇 |). The unit function MoveClusterToTile is an update of
matrix and can be performed in 𝑂 (1). Therefore, the complexity of the algorithm is 𝜂 × |𝐶 | × |𝑇 |. Here,
𝜂 is a user-defined parameter and controls the compilation time with a trade-off on the solution
quality, i.e., execution time and energy consumption of the application on hardware.

6 SCHEDULING AND PERFORMANCE GUARANTEE
Self-timed execution is widely used to schedule SDFGs [54]. Static schedules are constructed using
worst-case actor execution times determined during design time. Actor ordering on each tile is
retained while discarding the timing information. At run time, actors are fired while maintaining
the same order as determined during design time. In this regard, the following lemmas are stated [34,
38, 54].

Lemma 1. For a consistent and strongly connected SDFG, the self-timed execution consists of a

transient phase followed by a periodic phase.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:19

Lemma 2. For a consistent and strongly connected SDFG, the throughput of an actor is given by the

average firing of the actor per unit time in the periodic phase of the self-timed execution.

Figure 14 shows an example self-timed execution of 3 actors – 𝐴1, 𝐴2 and 𝐴4 of Figure 11(b) on

tile 2.

Fig. 14. Self-timed execution consisting of transient phase followed by periodic phase

A modern neuromorphic hardware is expected to execute many SNN applications simultane-
ously. When a new application is to be admitted to a hardware, which is currently running other
applications, the incoming application needs to be compiled and mapped to the hardware within a
short time window, based on resources currently available on the hardware. Furthermore, when
an existing application finishes execution, its hardware resources are freed, meaning that such
resources can now be allocated to other running applications to improve their performance. For
such dynamic scenarios, SDFG schedules must be constructed for every allocation scenario. If the
run-time schedule is different from that used for analysis at design time, the throughput obtained
will be significantly different than what is guaranteed at design time. There are therefore two
approaches to generating run-time schedules.

• Store the actor mapping and scheduling for all resource-allocation scenarios and for all

applications from design time (storage-based solution).

• Construct the schedule at run time based on the mappings stored from the design-time

(construction-based solution)

The former is associated with high storage overhead and the latter with longer execution time.
Both storage and schedule construction time are crucial for machine-learning systems deployed
in resource- and power-constrained environments. Therefore, we propose a modification of the
self-timed execution scheduling as follows. First, we construct the static-order schedule for all actors
of an SNN on a single tile at design time. This is achieved using the Max-Plus Algebra formulation of
Equation 7. Next, we discard the exact timing information, retaining only the actor firing orders for
run-time use. At run time, we first construct the cluster mapping to tiles (Section 5.4), considering
the available tiles. Next, we use the single-tile static-order schedule to derive the actor schedules
on each tile, without having to construct them from scratch.

Figure 15 illustrates the construction of per-tile schedules for an SNN application with 9 actors,
and with two different mappings of actors to tiles from the same single-tile static order schedule.
We illustrate two scenarios in this example. In the first scenario (left), the application uses two tiles
of the hardware. In the second scenario (right), the application uses three tiles of the hardware. In
both scenarios, actor orders on each tile are the same as those on the single tile. Since tile schedules
are not constructed from scratch, the schedule construction time is much lower.

However, performance obtained using this single-tile schedule can be lower than the maximum
performance of a multi-tile schedule constructed independently. As long as this performance
deviation is bounded, the actor schedule for any tile can be easily derived from the binding of actors
to this tile and a given single-tile static-order schedule. See Section 8 for performance evaluation.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:20

Song, et al.

Fig. 15. Schedules constructed from the same single-tile static order schedule using 2 and 3 tiles, respectively.

7 EVALUATION METHODOLOGY
We conduct all simulations on a Lambda workstation, which has AMD Threadripper 3960X with 24
cores, 128 MB cache, 128 GB RAM, and 2 RTX3090 GPUs. Keras [57] and CARLsim [28] use the
two GPUs to accelerate model training and SNN function simulation, respectively.

Figure 16 illustrates our evaluation setup using the cycle-accurate NeuroXplorer [17] framework.
This framework is validated extensively against the DYNAP-SE neuromorphic hardware [9, 13,
14, 31, 44], and can model the architecture of other neuromorphic hardware platforms such as
Loihi [45] and TrueNorth [47]. NeuroXplorer can simulate multi-compartment neuron models and
9-parameter Izhikevich and leaky integrate-and-fire (LIF) spiking neuron models. Additionally,
NeuroXplorer can model Non-Volatile Memory (NVM) synapses such as Phase Change Memory
(PCM) and Oxide-based Resistive Random Access Memory (OxRRAM). NeuroXplorer also models
the spike delay on the shared interconnect as well as the delay in propagating spikes through the
synapses of a crossbar [17]. The mapping and scheduling results obtained using DFSynthesizer are
used in NeuroXplorer to estimate energy, accuracy, and throughput.

Fig. 16. Our evaluation setup based on NeuroXplorer [17].

7.1 Evaluated Applications
We evaluate 10 machine learning programs which are representative of three most commonly-used
neural network classes: convolutional neural network (CNN), multi-layer perceptron (MLP), and
recurrent neural network (RNN). These applications are 1) LeNet based handwritten digit recogni-
tion with 28 × 28 images of handwritten digits from the MNIST dataset; 2) AlexNet for ImageNet
classification; 3) VGG16, also for ImageNet classification; 4) ECG-based heart-beat classification
(HeartClass) [9, 43] using electrocardiogram (ECG) data; 5) image smoothing (ImgSmooth) [28] on
64 × 64 images; 6) edge detection (EdgeDet) [28] on 64 × 64 images using difference-of-Gaussian; 7)
multi-layer perceptron (MLP)-based handwritten digit recognition (DigitRecogMLP) [50] using the
MNIST database; 8) heart-rate estimation (HeartEstm) [31] using ECG data; 9) RNN-based predictive

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

Single-tile Static Order Scheduletile_0tile_1tile_0tile_1tile_2MappingMapping1  1  1  0  1  0  0  0  10  0  0  1  0  1  1  1  00  0  0  0  0  0  0  0  00  0  0  0  0  0  0  0  0T0  1  1  0  1  0  0  0  00  0  0  1  0  0  1  1  01  0  0  0  0  1  0  0  10  0  0  0  0  0  0  0  0TMappingcycle-accuratesimulatorSchedulerThroughputCARLSim AccuracyEnergySNNDFSynthesizerDYNAP-SEDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:21

visual pursuit (VisualPursuit) [64]; and 10) recurrent digit recognition (DigitRecogSTDP) [50]. To
demonstrate the potential of DFSynthesizer, we consider a real-time neuromorphic system, where
these machine learning programs are executed continuously in a streaming fashion. Therefore, by
optimizing throughput, DFSynthesizer improves real-time performance.

Table 2 summarizes the topology, the number of neurons and synapses of these applications,
and their baseline accuracy on the DYNAP-SE neuromorphic hardware using the SpiNeMap [14]
mapping framework. As reported in many recent works [13, 14, 44], spike latency on the shared
interconnect of a neuromorphic hardware can lead to inter-spike interval (ISI) distortion and spike
disorder. Since the performance of an SNN is a function of ISI, such non-idealities can lead to
accuracy loss. Therefore, the accuracy of the three CNN architectures – LeNet, AlexNet, and VGG16
in Table 2 is somewhat lower than that reported via functional simulation in Table 1.

Table 2. Applications used to evaluate DFSynthesizer.

Class

CNN

MLP

RNN

Applications
LeNet
AlexNet
VGG16
HeartClass [9]
ImgSmooth [28]
EdgeDet [28]
DigitRecogMLP
HeartEstm [31]
VisualPursuit [64]

282,936

Dataset
MNIST

Synapses Neurons Topology
20,602
CNN
230,443 CNN
554,059 CNN
153,730 CNN
4,096
6,120
884
166
205
567

ImageNet 38,730,222
ImageNet 99,080,704
Physionet 1,049,249
CARLsim
9,025
CARLsim 114,057
79,400
MNIST
66,406
Physionet
163,880
[64]
11,442

DigitRecogSTDP [50] MNIST

FeedForward (4096, 1024)
FeedForward (4096, 1024, 1024, 1024)
FeedForward (784, 100, 10)
Recurrent Reservoir
Recurrent Reservoir
Recurrent Reservoir

Top-1 Accuracy (%)
85.1%
69.8%
90.7 %
63.7%
100%
100%
91.6%
100%
47.3%
83.6%

7.2 Hardware Parameters
We model the DYNAP-SE neuromorphic hardware [73] with 1024 tiles organized in a 32 × 32 mesh.
Each tile has one 128 × 128 crossbar. To test the scalability of DFSynthesizer, we also evaluate other
crossbar configurations, e.g., 256 × 256, 512 × 512, and 1024 × 1024. Table 3 reports the relevant
hardware parameters.

Table 3. Major simulation parameters extracted from [73].

Neuron technology

28nm FD-SOI

Synapse technology

Supply voltage

HfO2 -based OxRAM
1.0V

Energy per spike

50pJ at 30Hz spike frequency

Energy per routing

147pJ

Switch bandwidth

1.8G. Events/s

The additional overhead in time multiplexing the tiles among multiple crossbars is incorporated
in computing the throughput using NeuroXplorer. Specifically, once the cluster mapping to tiles are
generated using DFSynthesizer, the synaptic weights of all clusters mapped to a tile are pre-loaded
into the tile’s local memory (see our system architecture in Figure 5). In this way, DFSynthesizer
reduces the overhead of transferring synaptic weights at run-time from the shared main memory.
Additionally, since the loading of clusters (context switching) in crossbars happen concurrently
from their respective private memory, the time-multiplexing overhead is minimal.

7.3 Evaluated Metrics
We evaluate the following performance metrics.

• Performance. This is the throughput of each application on the hardware.
• Resource Utilization. This is the neuron, synapse, buffer, connection, and input and output

bandwidth utilization on the hardware for each application.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:22

Song, et al.

• Energy Consumption. This is the energy consumed on the hardware for each application.
This is the total energy consumed to generate spikes on each tile and communicate spike
between tiles via the shared interconnect.

• Cluster Connection. This is the average degree of the SDFG as percentage of the total

number of nodes, obtained using the clustering technique for each application.

• Spike Communication. This is the total number of spikes communicated on the shared

interconnect of the neuromorphic hardware.

• Synthesis Time. This is the time to compile and map each application on the hardware.

7.4 Evaluated Approaches
We evaluate the following approaches.

• SpiNeMap [14]. This approach first partitions an SNN into clusters of neurons and synapses
by incorporating its workload. The objective is to minimize inter-cluster communication.
Clusters are then mapped to tiles while minimizing spike communication on the shared
interconnect and reducing energy consumption. When mapping SNNs to neuromorphic
hardware with fewer tiles than the number of actors, 1) SpiNeMap allocates actors to tiles
randomly and 2) SpiNeMap schedules the actors on each tile arbitrarily. Therefore, SpiNeMap
does not consider throughput.

• PyCARL [13]. This approach maps neurons and synapses to tiles of a neuromorphic hardware,
balancing the number of neurons and synapses on each tile. PyCARL does not incorporate
SNN workload, i.e., spikes generated by neurons in the SNN. Therefore, some tiles may end
up communicating more spikes than others, i.e., those tiles become the energy bottleneck.
• SDFSNN [86]. This approach uses the load-balancing mapping of PyCARL to allocate actors

to tiles. It uses dataflow scheduling to improve the throughput.

• DFSynthesizer. The proposed approach first clusters an SNN, considering its workload.
The objective is to improve cluster utilization. This is done by first decomposing the SNN
into homogeneous neural units with fanin-of-two. The clusters are then mapped to tiles,
jointly optimizing throughput and energy consumption. DFSynthesizer uses dataflow-based
scheduling of actors to tiles to further improve the throughput.

8 RESULTS AND DISCUSSIONS

8.1 Throughput
Figure 17 reports the throughput on DYNAP-SE for the evaluated approaches, for each application
normalized to SpiNeMap. For reference, we have reported the maximum throughput in frames-
per-second obtained with unlimited hardware resources for each application. For image-based
applications (LeNet, AlexNet, VGGNet, EdgeDet, ImgSmooth, and DigitSTDP), a frame corresponds
to an individual image. For other time-series applications (HeartClass, HeartEstm, and VisualPur-
suit), a frame corresponds to a window of 500ms. We make the following four key observations.
First, although the number of neurons and synapses of larger applications such as AlexNet and
VGG16 is significantly higher than LeNet, the throughput of LeNet on a hardware with unlimited
resources,8 i.e., without time-multiplexing of crossbars is only 1.5x higher than AlexNet and 2x
higher than VGG16. This is because with no time-multiplexing of crossbars, computations in
a machine learning program take place concurrently on the crossbars, the basic philosophy of
distributed computing, which is enabled using neuromorphic platforms. Therefore, the overhead
due to time-multiplexing of crossbars is no longer the throughput bottleneck. Rather, the bottleneck

8In the context of this work, unlimited resources refer to a neuromorphic hardware that has at least the same number of
crossbars as there are clusters in the machine learning program.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:23

Fig. 17. Throughput on DYNAP-SE for each evaluated application normalized to SpiNeMap. The throughput
in frames-per-second is reported for the maximum throughput approach for each application assuming
unlimited hardware resources.

shifts to spike delay between the clusters. Additionally, in our framework we cluster machine
learning programs to minimize inter-cluster spikes. Therefore, even though Alexnet has significantly
higher number of neurons and synapses than LeNet, its number of inter-cluster spikes is not
significantly higher. The throughput of AlexNet is only 33% lower than LeNet. Similarly, VGG16,
which has higher inter-cluster spikes than AlexNet, has 25% lower throughput.

Second, the throughput obtained using SpiNeMap is the least because SpiNeMap does not
guarantee throughput during actor-to-tile mapping and actor scheduling on tiles. The throughput
of PyCARL is on average 4% higher than SpiNeMap. This is because PyCARL balances the load on
the tiles and therefore, the average number of actors mapped to each tile is lower than SpiNeMap,
which results in higher throughput. The throughput of SDFSNN is on average 9.7% higher than
PyCARL. This improvement is because of the use of dataflow-based scheduling, which maximizes
the throughput. DFSynthesizer improves throughput by an average of 17% compared SDFSNN. This
improvement is because unlike SDFSNN, which maps actors to tiles balancing the tile load without
considering the throughput, DFSynthesizer performs throughput- and energy-aware mapping of
actors to tiles and then uses dataflow-based scheduling to further improve the throughput. We
have analyzed such throughput differences in Section 5.4.

Third, the throughput using DFSynthesizer is only 16% lower on average than the maximum
throughput obtained with unlimited hardware resources. Finally, the throughput of DigitMLP is a
very small application. All the techniques generate the same number of clusters for this application,
resulting in similar throughput.

8.2 Workload Energy
Figure 18 reports the workload energy estimated on DYNAP-SE of the evaluated approaches for
each application normalized to SpiNeMap. For reference, we have reported the workload energy
in 𝜇 𝐽 obtained using the maximum throughput approach, which assumes unlimited hardware
resources. We make the following observation.

Fig. 18. Workload energy on DYNAP-SE for each evaluated application normalized to SpiNeMap. The workload
energy in 𝜇 𝐽 is reported for the maximum throughput approach for each application assuming
unlimited hardware resources.

The energy consumption of SpiNeMap is the least because this approach partitions SNNs into
clusters to explicitly minimize the number of inter-cluster spikes. Therefore, when the clusters are

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0.00.51.01.52.02.53.0ThroughputNormalizedtoSpiNeMap6431.33810122210SpiNeMapPyCARLSDFSNNDFSynthesizerMaxThroughputLeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0.81.01.21.41.6EnergyNormalizedtoSpiNeMap186411811226323251822SpiNeMapPyCARLSDFSNNDFSynthesizerMaxThroughput0:24

Song, et al.

mapped to hardware, the energy consumption on the shared interconnect is reduced.9 Second, the
energy consumption of PyCARL is on average 15% higher than SpiNeMap. This is because PyCARL
balances the tile load without incorporating energy consumption. Therefore, clusters with high
volume of spike communication between them may get placed on different tiles, increasing the
communication energy. SpiNeMap places those tiles on the same tile lowering the communication
energy. The energy consumption of SDFSNN is the same as PyCARL because the cluster-to-tile
mapping of these two approaches is the same. SDFSNN gains over PyCARL in terms of throughput
due to its dataflow-based cluster scheduling on tiles. We analyzed this in Section 8.1. The energy
consumption of DFSynthesizer is lower than SDFSNN by an average of 8%. This reduction is due
to the cluster-to-tile mapping of DFSynthesizer, which incorporates energy consumption.

8.3 Scheduling
Figure 19 reports throughput of each of our applications for our proposed approach normalized
to PyCARL. We compare throughput obtained using DFSynthesizer where schedules are indepen-
dently constructed for each tile against the throughput obtained using our proposed single-tile
based schedule (DFSynthesizer+STS). We make the following three observations.

Fig. 19. Throughput normalized to PyCARL.
First, throughput obtained from a single-tile static-order schedule is on average 15% lower than
the case when schedules are constructed independently — that is, by using DFSynthesizer. This
verifies our Lemma 2. Second, for some applications such as HeartEstm and HeratClass, throughput
obtained using DFSynthesizer+STS is exactly the same as that obtained using DFSynthesizer. Third,
throughput using DFSynthesizer+STS is still higher than PyCARL by an average of 41%.

8.4 Resource Utilization
Table 4 reports the utilization of hardware resources (tile resources, buffer size, connections, and
input and output bandwidth) on the DYNAP-SE neuromorphic hardware for each application.
The average utilization of hardware resources is 92.5% for the crossbar IOs on each tile, 9.0% for
buffer space, 42.6% for connections, and 15% for input and output tile bandwidth. Since we perform
hardware-aware analysis, resource utilization never exceeds 100%.

These results illustrate that DFSynthesizer can be used to design neuromorphic hardware while
considering key hardware parameters such as number of tiles, but all other resources such as buffer
space, connections, and input and output bandwidth.

To give more insight on the utilization within each tile, Figure 20 reports the average synapse
utilization on tiles of the evaluated approaches for each application normalized to PyCARL. We
make the following two key observations.

9The mapping exploration only impacts the communication energy on the shared interconnect. The spike generation energy
remains the same for all approaches.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE01234ThroughputNormalizedtoPyCARLPyCARLDFSynthesizerDFSynthesizer+STSDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:25

Application

Tile Buffer Connections

Utilization (%)

100
LeNet
100
AlexNet
100
VGG16
100
HeartClass
81.25
DigitMLP
87.5
EdgeDet
87.5
ImgSmooth
HeartEstm
96.87
VisualPursuit 90.12
89.33
DigitSTDP

87.8
91.8
94.2
79.1
9.67
11.23
8.39
9.61
21.2
20.13

37.5
46.87
15.62
25
46.87
68.75
37.5
62.5
25.04
22.19

Bandwidth
Input Output
20.34
17.09
6.51
9.76
22.78
22.78
17.08
4.7
12.11
11.94

20.34
17.09
6.51
9.76
22.78
22.78
17.08
4.7
16.6
11.7

Table 4. Resource utilization on DYNAP-SE.

Fig. 20. Average synapse utilization on tiles for each evaluated application normalized to PyCARL.

First, the synapse utilization on tiles using SpiNeMap is the least of all three evaluated approaches.
This is because SpiNeMap produces the highest number of clusters (Sec. 8.5) and therefore, the
average number of synapses per cluster is the least. Subsequently, when these clusters are mapped
to tiles, the average synapse utilization on tiles reduces. Second, DFSynthesizer generates fewer
clusters than both SpiNeMap and PyCARL due to its dense packing of synapses using Algorithm 2.
Therefore, the average number of synapses per cluster is higher, which increases synapse utilization
on tiles when the clusters are mapped to tiles. On average, the average synapse utilization of
DFSynthesizer is 2x higher than PyCARL and 2.2x higher than SpiNeMap.

8.5 Number of Clusters
Figure 21 reports the total number of clusters of the evaluated approaches for each application
normalized to PyCARL. We make the following two key observations.

First, the number of clusters of SpiNeMap is the highest of all three evaluated approaches. This is
because SpiNeMap minimizes the number of inter-cluster communication during clustering of an
SNN. Therefore, neurons that spike the most are placed within individual clusters along with their
fanins. Since SpiNeMap does not consider cluster utilization, it results in creating more clusters than
PyCARL. Second, DFSynthesizer clusters an SNN to maximize the resource utilization on each tile.
Therefore, the number of clusters generated by DFSynthesizer is the lowest. Overall, the number
of clusters of DFSynthesizer is 41% lower than SpiNeMap and 47% lower than PyCARL. Lower the
number of clusters, lower is the size of hardware needed to achieve highest throughput (Sec. 8.1).
Therefore, DFSynthesizer reduces the hardware requirement for machine learning applications.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE012345SynaptseUtilizationNormalizedtoPyCARLPyCARLSpiNeMapDFSynthesizer0:26

Song, et al.

Fig. 21. Number of clusters for each evaluated application normalized to PyCARL.

8.6 Cluster Connections
Figure 22 reports the cluster connections of the evaluated approaches for each application normal-
ized to PyCARL. We make the following two key observations.

Fig. 22. Cluster connections for each evaluated application normalized to PyCARL.

First, the number of inter-cluster connections of SpiNeMap is the least of all three evaluated
approaches. This is because SpiNeMap minimizes the number of inter-cluster communication while
clustering an SNN, which indirectly reduces the cluster connectivity. Second, DFSynthesizer clusters
an SNN to maximize the resource utilization on each tile. Therefore, the number of connections
between the clusters is higher in DFSynthesizer because of the higher number of post-synaptic
neurons mapped to each cluster. Overall, the average cluster connections of DFSynthesizer is 3.1x
higher than SpiNeMap and 3.9x higher than PyCARL.

8.7 Architecture Exploration
Figure 23 reports the number of clusters generated using DFSynthesizer for neuromorphic hardware
with 128 × 128, 256 × 256, and 1024 × 1024 crossbars, normalized to a DYNAP-SE configuration with
128 × 128 crossbars. We observe that the number of clusters generated using DFSynthesizer reduces
by 60% and 92% when the size of a crossbar increases to 256 × 256 and 1024 × 1024, respectively.
Fewer number of clusters increases throughput. To illustrate this, Figure 24 reports the throughput
using DFSynthesizer for different crossbar sizes normalized to throughput on DYNAP-SE with four
128 × 128 crossbars. We make the following two observations.

First, throughput increases by 18% and 30% when using 256 × 256 and 1024 × 1024 crossbars,
respectively. This improvement is because with larger size crossbars, there are fewer clusters
generated by DFSynthesizer (Fig. 23). Therefore, the number of clusters per tile reduces, which
reduces the bottleneck of time-multiplexing clusters on tiles. This increases throughput. Second, for
applications such as DigitMLP, EdgeDet, and HeartEstm, there is no throughput improvement when

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0.00.51.01.52.0NumberofClusterNormalizedtoPyCARLPyCARLSpiNeMapDFSynthesizerLeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0246810ClusterConnectionsNormalizedtoPyCARLPyCARLSpiNeMapDFSynthesizerDFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:27

Fig. 23. Number of clusters generated using DFSynthesizer for 128 × 128, 256 × 256, and 1024 × 1204 crossbars,
normalized to the configuration of DYNAP-SE with 128 × 128 crossbars.

Fig. 24. Throughput achieved using DFSynthesizer for 128 × 128, 256 × 256, and 1024 × 1204 crossbars,
normalized to throughput on DYNAP-SE with 128 × 128 crossbars.

the crossbar size increased from 512 × 512 to 1024 × 1024. This is because for these applications,
256 × 256 crossbar configuration is sufficient to achieve the highest throughput. For all other
applications, the throughput increases by 11% when going from 256 × 256 to 1024 × 1024 crossbars.

8.8 Synthesis Time
Figure 25 reports the synthesis time on DYNAP-SE for the evaluated approaches, for each application
normalized to PyCARL. We make the following three key observations.

Fig. 25. Synthesis time for each application normalized to PyCARL.

First, the synthesis time of SpiNeMap is on average 61.6% higher than PyCARL. The higher
synthesis time of SpiNeMap is due to the analysis it performs with the workload to obtain the
minimum energy mapping. Second, the synthesis time of DFSynthesizer is the highest. On average,
the synthesis time of DFSynthesizer is 35x higher than PyCARL and 25x higher than SpiNeMap.
This higher synthesis time is due to 1) DFSynthesizer’s mapping explorations using Algorithm 3,

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0.00.51.0NumberofClustersNormalizedtoPyCARL128x128256x2561024x1024LeNetAlexNetVGG16HeartClassDigitMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitSTDPAVERAGE0.00.51.01.52.02.5ThroughputNormalizedtoPyCARL128x128256x2561024x1024LeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE0.02.55.07.510.0SynthesisTimeNormalizedtoPyCARL25.5272.336.135.4PyCARLSpiNeMapDFSynthesizer0:28

Song, et al.

and 2) DFSynthesizer’s SDFG analysis mechanism using the proposed Max Plus formulation.
Third, the synthesis time of DFSynthesizer increases with model complexity. The synthesis time of
DFSynthesizer is higher than PyCARL by 3.1x for LeNet, 25.5x for AlexNet, and 272.3x for VGG16.

8.9 Model Quality
DFSynthesizer does not alter synaptic connections. Therefore, the model quality, e.g., accuracy is not
impacted by the analysis technique of DFSynthesizer. The only impact DFSynthesizer introduces
is in converting CNNs. The accuracy impact is reported in Table 1. For all other applications,
DFSynthesizer’s accuracy is the same as the baseline accuracy reported in Table 2.

9 RELATED WORKS
Recently, many approaches are proposed to map machine learning workloads to neuromorphic
hardware. Corelet [2] is used to map SNNs to TrueNorth [47]. PACMAN [53] is used to map SNNs to
SpiNNaker [52]. PyNN [13] is used to map SNNs on Loihi [45], BrainScaleS [80], and Neurogrid [21]
by balancing the load on each tile. PyCARL [13] is used to map SNNs to DYNAP-SE [73]. The
primary objective of these approaches is to balance the workload on each tile by distributing the
neurons and synapses evenly.

Beyond load balancing, recent techniques have also explored other objectives. PSOPART [44] is
used to map SNNs to neuromorphic hardware, reducing the energy consumption on the shared
interconnect. SpiNeMap [14] performs energy-aware clustering of SNNs and then maps the clus-
ters to tiles, reducing the communication energy. DecomposeSNN [16] decomposes an SNN to
improve the cluster utilization. There are also performance-oriented SNN mapping approaches
such as [7, 11, 15, 86], energy-aware SNN mapping approaches such as [101], circuit aging-aware
SNN mapping approaches such as [10, 67, 84, 88, 91], endurance-aware SNN mapping approaches
such as [93, 99, 102], and thermal-aware SNN mapping approaches such as [100]. These approaches
are evaluated with emerging SNN based applications [9, 31, 43, 50, 64, 75], which we also use to
evaluate DFSynthesizer.

There are also other mapping approaches such as [4, 70, 77, 103–106]. We compare DFSynthesizer

against PyCARL and SpiNeMap, and found it to perform significantly better.

Similar Concept in Related Domain
SDFGs are widely used for predictable mapping of applications to multiprocessor systems. Numerous
approaches to throughput analysis of SDFGs have been previously proposed [30, 41, 81, 81, 97,
98, 108]. Bonfietti et al. evaluated mappings of SDFG to multiprocessor system, maximizing the
throughput [23]. Stemmer et al. propose to use probabilistic analysis to allocate and schedule
SDFGs on multiprocessor systems [95]. Das et al. evaluated the fault-tolerant mapping of SDFGs
to multiprocessor systems [32, 34–40, 42]. Recently, SDFG-based analysis is also proposed for
analyzing machine learning applications [6, 8, 27, 33, 60, 92]. However, none of these approaches
address application analysis with limited hardware resources, both at design-time and at run-time.

10 CONCLUSIONS
We introduce DFSynthesizer for predictable synthesis of SNN-based applications on state-of-the-
art neuromorphic hardware. Prior works have only addressed design-time mapping, considering
unlimited resources in the underlying hardware. These approaches present significant limitations
when used to compile and map machine-learning applications to a resource-constrained hardware.
DFSynthesizer makes five key contributions. First, we present an approach to analyze machine-
learning programs and generate SNN workload using representative data. Second, we present an
approach to decompose and partition complex SNN workloads to generate clusters of neurons

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:29

and synapses such that each cluster can fit onto a crossbar of the hardware. Third, we exploit the
rich semantics of Synchronous Dataflow Graphs (SDFGs) to represent clustered SNN programs.
This allows for the SNN’s performance, e.g., throughput, to be estimated on the hardware as a
function of key properties such as number of crossbars, dimension of crossbars, buffer space on
tiles, and tile communication bandwidth. Four, we develop a novel scheduling algorithm based on
Self-Timed Execution for executing clusters on crossbars of a neuromorphic hardware, providing
performance guarantee in scenarios with dynamic resource availability. Five, we propose a design-
space exploration framework incorporating DFSynthesizer that allows the Pareto-space of different
SNN mappings to hardware to be explored while considering other hardware metrics such as
energy, latency, and reliability.

We evaluate DFSynthesizer using 10 machine learning programs that are representative of
the three most commonly used neural network classes — convolutional neural network (CNN),
multi-layer perceptron (MLP), and recurrent neural network (RNN). Our results demonstrate that
DFSynthesizer provides much tighter performance guarantee compared to current practices.

ACKNOWLEDGMENTS
This work is supported by 1) the National Science Foundation Award CCF-1937419 (RTML: Small:
Design of System Software to Facilitate Real-Time Neuromorphic Computing) and 2) the National
Science Foundation Faculty Early Career Development Award CCF-1942697 (CAREER: Facilitating
Dependable Neuromorphic Computing: Vision, Architecture, and Impact on Programmability).

REFERENCES

[1] M. Abadi, P. Barham, J. Chen et al., “Tensorflow: A system for large-scale machine learning,” in OSDI, 2016.
[2] A. Amir, P. Datta, W. P. Risk et al., “Cognitive computing programming paradigm: A corelet language for composing

networks of neurosynaptic cores,” in IJCNN, 2013.

[3] A. Ankit, A. Sengupta, and K. Roy, “TraNNsformer: Neural network transformation for memristive crossbar based

neuromorphic system design,” in ICCAD, 2017.

[4] A. Ankit, A. Sengupta, and K. Roy, “Neuromorphic computing across the stack: Devices, circuits and architectures,” in

SIPS, 2018.

[5] L. Atzori, A. Iera, and G. Morabito, “The internet of things: A survey,” Computer Networks, 2010.
[6] M. Bacis, G. Natale, E. Del Sozzo et al., “A pipelined and scalable dataflow implementation of convolutional neural

networks on FPGA,” in IPDPSW, 2017.

[7] A. Balaji and A. Das, “Compiling spiking neural networks to mitigate neuromorphic hardware constraints,” in IGSC

Workshops, 2020.

[8] A. Balaji and A. Das, “A framework for the analysis of throughput-constraints of SNNs on neuromorphic hardware,”

in ISVLSI, 2019.

[9] A. Balaji, F. Corradi, A. Das et al., “Power-accuracy trade-offs for heartbeat classification on neural networks hardware,”

JOLPE, 2018.

[10] A. Balaji, S. Song, A. Das et al., “A framework to explore workload-specific performance and lifetime trade-offs in

neuromorphic computing,” CAL, 2019.

[11] A. Balaji, S. Ullah, A. Das et al., “Design methodology for embedded approximate artificial neural networks,” in

GLSVLSI, 2019.

[12] A. Balaji, Y. Wu, A. Das et al., “Exploration of segmented bus as scalable global interconnect for neuromorphic

computing,” in GLSVLSI, 2019.

[13] A. Balaji, P. Adiraju, H. J. Kashyap et al., “PyCARL: A PyNN interface for hardware-software co-simulation of spiking

neural network,” in IJCNN, 2020.

[14] A. Balaji, A. Das, Y. Wu et al., “Mapping spiking neural networks to neuromorphic hardware,” TVLSI, 2020.
[15] A. Balaji, T. Marty, A. Das et al., “Run-time mapping of spiking neural networks to neuromorphic hardware,” JSPS,

2020.

[16] A. Balaji, S. Song, A. Das et al., “Enabling resource-aware mapping of spiking neural networks via spatial decomposi-

tion,” ESL, 2020.

[17] A. Balaji, S. Song, T. Titirsha et al., “NeuroXplorer 1.0: An extensible framework for architectural exploration with

spiking neural networks,” in ICONS, 2021.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:30

Song, et al.

[18] S. S. Battacharyya, P. K. Murthy, and E. A. Lee, “Loose interdependence algorithms,” in Software Synthesis from

Dataflow Graphs, 1996.

[19] T. Bekolay, J. Bergstra, E. Hunsberger et al., “Nengo: A python tool for building large-scale functional brain models,”

Frontiers in Neuroinformatics, 2014.

[20] L. Benini and G. De Micheli, “Networks on chip: A new paradigm for systems on chip design,” in DATE, 2002.
[21] B. V. Benjamin, P. Gao, E. McQuinn et al., “Neurogrid: A mixed-analog-digital multichip system for large-scale neural

simulations,” Proceedings of the IEEE, 2014.

[22] O. Bichler, D. Briand, V. Gacoin et al., “N2D2: Neural network design & deployment,” https://github.com/CEA-LIST/N2D2,

2017.

[23] A. Bonfietti, M. Lombardi, M. Milano et al., “Maximum-throughput mapping of SDFGs on multi-core SoC platforms,”

JPDC, 2013.

[24] G. W. Burr, R. M. Shelby et al., “Neuromorphic computing using non-volatile memory,” Advances in Physics: X, 2017.
[25] F. Catthoor, S. Mitra, A. Das et al., “Very large-scale neuromorphic systems for biological signal processing,” in CMOS

Circuits for Biological Sensing and Processing, 2018.

[26] Y.-H. Chen, T. Krishna, J. S. Emer et al., “Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional

neural networks,” JSSC, 2016.

[27] Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne, “Using dataflow to optimize energy efficiency of deep neural network

accelerators,” IEEE Micro, 2017.

[28] T.-S. Chou, H. J. Kashyap, J. Xing et al., “CARLsim 4: An open source library for large scale, biologically detailed

spiking neural network simulation using heterogeneous clusters,” in IJCNN, 2018.

[29] J. Cong and Z. Zhang, “An efficient and versatile scheduling algorithm based on SDC formulation,” in DAC, 2006.
[30] M. Damavandpeyma, S. Stuijk, T. Basten et al., “Modeling static-order schedules in synchronous dataflow graphs,” in

DATE, 2012.

[31] A. Das, P. Pradhapan, W. Groenendaal et al., “Unsupervised heart-rate estimation in wearables with Liquid states and

a probabilistic readout,” Neural Networks, 2018.

[32] A. Das and A. Kumar, “Fault-aware task re-mapping for throughput constrained multimedia applications on NoC-based

MPSoCs,” in RSP, 2012.

[33] A. Das and A. Kumar, “Dataflow-based mapping of spiking neural networks on neuromorphic hardware,” in GLSVLSI,

2018.

[34] A. Das, A. Kumar, and B. Veeravalli, “Energy-aware communication and remapping of tasks for reliable multimedia

multiprocessor systems,” in ICPADS, 2012.

[35] A. Das, A. Kumar, and B. Veeravalli, “Aging-aware hardware-software task partitioning for reliable reconfigurable

multiprocessor systems,” in CASES, 2013.

[36] A. Das, A. Kumar, and B. Veeravalli, “Communication and migration energy aware design space exploration for

multicore systems with intermittent faults,” in DATE, 2013.

[37] A. Das, A. K. Singh, and A. Kumar, “Energy-aware dynamic reconfiguration of communication-centric applications

for reliable MPSoCs,” in ReCoSoC, 2013.

[38] A. Das, A. Kumar, and B. Veeravalli, “Communication and migration energy aware task mapping for reliable multi-

processor systems,” FGCS, 2014.

[39] A. Das, A. Kumar, and B. Veeravalli, “Energy-aware task mapping and scheduling for reliable embedded computing

systems,” TECS, 2014.

[40] A. Das, A. Kumar, and B. Veeravalli, “Reliability and energy-aware mapping and scheduling of multimedia applications

on multiprocessor systems,” TPDS, 2015.

[41] A. Das, M. J. Walker, A. Hansson et al., “Hardware-software interaction for run-time power optimization: A case

study of embedded linux on multicore smartphones,” in Proceedings of ISLPED, 2015.

[42] A. Das, B. M. Al-Hashimi, and G. V. Merrett, “Adaptive and hierarchical runtime manager for energy-aware thermal

management of embedded systems,” TECS, 2016.

[43] A. Das, F. Catthoor, and S. Schaafsma, “Heartbeat classification in wearables using multi-layer perceptron and

time-frequency joint distribution of ECG,” in CHASE, 2018.

[44] A. Das, Y. Wu, K. Huynh et al., “Mapping of local and global synapses on spiking neuromorphic hardware,” in DATE,

2018.

[45] M. Davies, N. Srinivasa, T.-H. Lin et al., “Loihi: A neuromorphic manycore processor with on-chip learning,” IEEE

Micro, 2018.

[46] A. P. Davison, D. Brüderle, J. M. Eppler et al., “PyNN: a common interface for neuronal network simulators,” Frontiers

in Neuroinformatics, 2009.

[47] M. V. DeBole, B. Taba, A. Amir et al., “TrueNorth: Accelerating from zero to 64 million neurons in 10 years,” Computer,

2019.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:31

[48] J. Deng, W. Dong, R. Socher et al., “Imagenet: A large-scale hierarchical image database,” in CVPR, 2009.
[49] L. Deng, “The MNIST database of handwritten digit images for machine learning research [best of the web],” Signal

Processing Magazine, 2012.

[50] P. U. Diehl and M. Cook, “Unsupervised learning of digit recognition using spike-timing-dependent plasticity,”

Frontiers in Computational Neuroscience, 2015.

[51] J. M. Eppler, M. Helias, E. Muller et al., “PyNEST: A convenient interface to the NEST simulator,” Frontiers in

Neuroinformatics, 2009.

[52] S. B. Furber, F. Galluppi, S. Temple et al., “The SpiNNaker project,” Proceedings of the IEEE, 2014.
[53] F. Galluppi, X. Lagorce, E. Stromatias et al., “A framework for plasticity implementation on the spinnaker neural

architecture,” Frontiers in Neuroscience, 2015.

[54] A. H. Ghamarian, M. C. Geilen, S. Stuijk et al., “Throughput analysis of synchronous data flow graphs,” in ACSD, 2006.
[55] D. F. Goodman and R. Brette, “The brian simulator,” Frontiers in Neuroscience, 2009.
[56] R. Gopalakrishnan, Y. Chua, P. Sun et al., “HFNet: A CNN architecture co-designed for neuromorphic hardware with

a crossbar array of synapses,” Frontiers in Neuroscience, 2020.

[57] A. Gulli and S. Pal, Deep learning with Keras, 2017.
[58] B. Heidergott, G. J. Olsder, and J. Van Der Woude, Max Plus at work: Modeling and analysis of synchronized systems: a

course on Max-Plus algebra and its applications, 2014.

[59] M. L. Hines and N. T. Carnevale, “The NEURON simulation environment,” Neural Computation, 1997.
[60] H. Hong, H. Oh, and S. Ha, “Hierarchical dataflow modeling of iterative applications,” in DAC, 2017.
[61] M. Hu, J. P. Strachan, Z. Li et al., “Dot-product engine for neuromorphic computing: Programming 1T1M crossbar to

accelerate matrix-vector multiplication,” in DAC, 2016.

[62] G. Indiveri, “A low-power adaptive integrate-and-fire neuron circuit,” in ISCAS, 2003.
[63] Y. Ji, Y. Zhang, S. Li et al., “NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware

constraints,” in MICRO, 2016.

[64] H. J. Kashyap, G. Detorakis, N. Dutt et al., “A recurrent neural network based model of predictive smooth pursuit eye

movement in primates,” in IJCNN, 2018.

[65] B. W. Kernighan and S. Lin, “An efficient heuristic procedure for partitioning graphs,” Bell System Technical Journal,

1970.

[66] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,”

NeurIPS, 2012.

[67] S. Kundu, K. Basu, M. Sadi et al., “Special Session: Reliability analysis for ML/AI hardware,” in VTS, 2021.
[68] Y. LeCun et al., “Lenet-5, convolutional neural networks,” URL: http://yann. lecun. com/exdb/lenet, 2015.
[69] E. Lee and D. Messerschmitt, “Synchronous data flow,” Proceedings of the IEEE, 1987.
[70] M. K. F. Lee, Y. Cui, T. Somu et al., “A system-level simulator for RRAM-based neuromorphic computing chips,” TACO,

2019.

[71] W. Maass, “Networks of spiking neurons: the third generation of neural network models,” Neural Networks, 1997.
[72] A. Mallik, D. Garbin, A. Fantini et al., “Design-technology co-optimization for oxrram-based synaptic processing

unit,” in VLSIT, 2017.

[73] S. Moradi, N. Qiao, F. Stefanini et al., “A scalable multicore architecture with heterogeneous memory structures for

dynamic neuromorphic asynchronous processors (DYNAPs),” TBCAS, 2017.

[74] O. M. Moreira and M. J. Bekooij, “Self-timed scheduling analysis for real-time applications,” JASP, 2007.
[75] E. J. Moyer, A. Das et al., “Machine learning applications to dna subsequence and restriction site analysis,” in SPMB,

2020.

[76] A. Paszke, S. Gross, F. Massa et al., “PyTorch: An imperative style, high-performance deep learning library,” arXiv,

2019.

[77] S. G. Ramasubramanian, R. Venkatesan, M. Sharad et al., “SPINDLE: SPINtronic deep learning engine for large-scale

neuromorphic computing,” in ISLPED, 2014.

[78] V. J. Reddi, C. Cheng, D. Kanter et al., “Mlperf inference benchmark,” in ISCA, 2020.
[79] B. Rueckauer, I.-A. Lungu, Y. Hu et al., “Theory and tools for the conversion of analog to spiking convolutional neural

networks,” arXiv, 2016.

[80] J. Schemmel, A. Grübl, S. Hartmann et al., “Live demonstration: A scaled-down version of the brainscales wafer-scale

neuromorphic system,” in ISCAS, 2012.

[81] R. A. Shafik, A. Das, S. Yang et al., “Adaptive energy minimization of openmp parallel applications on many-core

systems,” in PARMA-DITAM, 2015.

[82] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv, 2014.
[83] S. Song and A. Das, “Design methodologies for reliable and energy-efficient PCM systems,” in IGSC Workshops, 2020.
[84] S. Song and A. Das, “A case for lifetime reliability-aware neuromorphic computing,” in MWSCAS, 2020.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

0:32

Song, et al.

[85] S. Song, A. Das, O. Mutlu et al., “Enabling and exploiting partition-level parallelism (PALP) in phase change memories,”

TECS, 2019.

[86] S. Song, A. Balaji, A. Das et al., “Compiling spiking neural networks to neuromorphic hardware,” in LCTES, 2020.
[87] S. Song, A. Das, and N. Kandasamy, “Exploiting inter- and intra-memory asymmetries for data mapping in hybrid

tiered-memories,” in ISMM, 2020.

[88] S. Song, A. Das, and N. Kandasamy, “Improving dependability of neuromorphic computing with non-volatile memory,”

in EDCC, 2020.

[89] S. Song, A. Das, O. Mutlu et al., “Improving phase change memory performance with data content aware access,” in

ISMM, 2020.

[90] S. Song, A. Das, O. Mutlu et al., “Aging-aware request scheduling for non-volatile main memory,” in ASP-DAC, 2021.
[91] S. Song, J. Hanamshet, A. Balaji et al., “Dynamic reliability management in neuromorphic computing,” JETC, 2021.
[92] S. Song, A. Paul, L. V. Mirtinti et al., “A design flow for mapping spiking neural networks to many-core neuromorphic

hardware,” arXiv, 2021.

[93] S. Song, T. Titirsha, and A. Das, “Improving inference lifetime of neuromorphic systems via intelligent synapse

mapping,” in ASAP, 2021.

[94] S. Sriram and S. Bhattacharyya, Embedded Multiprocessors; Scheduling and Synchronization, 2000.
[95] R. Stemmer, H.-D. Vu, K. Grüttner et al., “Towards probabilistic timing analysis for SDFGs on tile based heterogeneous

MPSoCs,” in ECRTS, 2020.

[96] S. Stuijk, M. Geilen, and T. Basten, “Exploring trade-offs in buffer requirements and throughput constraints for

synchronous dataflow graphs,” in DAC, 2006.

[97] S. Stuijk, M. Geilen, and T. Basten, “Exploring trade-offs in buffer requirements and throughput constraints for

synchronous dataflow graphs,” in DAC, 2006.

[98] S. Stuijk, T. Basten, M. Geilen et al., “Multiprocessor resource allocation for throughput-constrained synchronous

dataflow graphs,” in DAC, 2007.

[99] T. Titirsha and A. Das, “Reliability-performance trade-offs in neuromorphic computing,” in IGSC Workshops, 2020.
[100] T. Titirsha and A. Das, “Thermal-aware compilation of spiking neural networks to neuromorphic hardware,” in LCPC,

2020.

[101] T. Titirsha, S. Song, A. Balaji et al., “On the role of system software in energy management of neuromorphic computing,”

in CF, 2021.

[102] T. Titirsha, S. Song, A. Das et al., “Endurance-aware mapping of spiking neural networks to neuromorphic hardware,”

TPDS, 2021.

[103] W. Wen, C.-R. Wu, X. Hu et al., “An EDA framework for large scale hybrid neuromorphic computing systems,” in

DAC, 2015.

[104] P. Wijesinghe, A. Ankit, A. Sengupta et al., “An all-memristor deep spiking neural computing system: A step toward

realizing the low-power stochastic brain,” TETCI, 2018.

[105] Q. Xia and J. J. Yang, “Memristive crossbar arrays for brain-inspired computing,” Nature Materials, 2019.
[106] X. Zhang, A. Huang, Q. Hu et al., “Neuromorphic computing with memristor crossbar,” Physica Status Solidi (a), 2018.
[107] Z. Zhang and B. Liu, “SDC-based modulo scheduling for pipeline synthesis,” in ICCAD, 2013.
[108] X.-Y. Zhu, M. Geilen, T. Basten et al., “Static rate-optimal scheduling of multirate DSP algorithms via retiming and

unfolding,” in RTAS, 2012.

A CONVERTING ANALOG OPERATIONS TO SPIKING EQUIVALENT
In this section, we briefly elaborate how an analog operation such as Rectified Linear Unit (ReLU)
is implemented using Spiking Neural Network (SNN). The output 𝑌 of a ReLU activation function
is given by

𝑌 = max 0, ∑︁

𝑤𝑖 ∗ 𝑥𝑖,

(16)

𝑖
where 𝑤𝑖 is the weight and 𝑥𝑖 is the activation on the 𝑖th synapse of the neuron. To map the ReLU
activation function, we consider a particular type of spiking neuron model known as an Integrate
and Fire (IF) neuron model. The IF spiking neuron’s transfer function can be represented as

𝑣𝑚 (𝑡 + 1) = 𝑣𝑚 (𝑡 ) +

𝑤𝑖 ∗ 𝑥𝑖 (𝑡 ),

∑︁

𝑖

(17)

where 𝑣𝑚 (𝑡 ) is the membrane potential of the IF neuron at time 𝑡, 𝑤𝑖 is the weight, and 𝑥𝑖 (𝑡 ) is the
activation on the 𝑖th synapse of the neuron at time 𝑡. The IF spiking neuron integrates incoming

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware

0:33

spikes (𝑋𝑖) and generates an output spike (𝑌spike) when the membrane potential (𝑣𝑚) exceeds the
threshold voltage (𝑣th) of the IF neuron. Therefore, by ensuring that the output spiking rate 𝑌spike is
proportional to the ReLU activation 𝑌 , i.e., 𝑌spike ∝ 𝑌 , we accurately convert the ReLU activation to
the spike-based model. To further illustrate this, we consider the multi-layer perceptron (MLP) of
Figure 26a and its SNN conversion using rate-based encoding (Figure 26b) and inter-spike interval
(ISI) encoding (Figure 26c).

Fig. 26. Example of converting an analog MLP to its spiking equivalent.

In Figure 26a, neurons 1,2 and 3 are the input neurons and neurons 4 and 5 are the output neurons.
To keep the model simple, let us consider the case where the activations of the input neurons 1,2
and 3 are equal to 1. Using Equation 16, we know that the output of neurons 4 and 5 are 0.6 and 0.3,
respectively. Figures 26b and 26c show the mapped SNN model, using rate-based and inter-spike
interval encoding schemes, respectively. In the rate-based model in Figure 26b, the rate of spikes
generated is expected to be proportional to the output of neurons 4 and 5 in the MLP. In the case of
the ISI-based SNN model, the inter-spike interval of the spikes generated by neurons 4 and 5 is
expected to be proportional to the output generated in the MLP, as shown in Figure 26c.

We note that non-linear activation functions such as sigmoid and tanh cannot be accurately
mapped to a spike-based model. This can be attributed to the transfer function of a biological spiking
neuron (neuron response curve) closely resembling a ReLU and not sigmoid and tanh activation
functions. While approximate implementations of the sigmoid and tanh operators using spiking
neurons can be found in literature, they induce significant inaccuracies into the conversion process
and require more resources (neurons) to implement. The tanh activation function, for instance,
generates output values ranging between -1.0 to 1.0. In order to represent the tanh function in a
spike-based model, both excitatory and inhibitory spiking neurons will be required to represent the
positive and negative output values, respectively. This will require doubling the number of spiking
neurons needed to represent the tanh activation function.

ACM Trans. Embedd. Comput. Syst., Vol. 0, No. 0, Article 0. Publication date: January 2020.

(a) MLP in analog domain(b) MLP in spiking domain (rate coding)(b) MLP in spiking domain (ISI coding)