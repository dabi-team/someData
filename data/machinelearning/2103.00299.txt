Primal-Dual Stochastic Mirror Descent for MDPs

Daniil Tiapkin
HSE University, Russia

Alexander Gasnikov
Moscow Institute of Physics and Technology, Russia
HSE Univeristy, Russia

2
2
0
2

b
e
F
7
2

]

C
O
.
h
t
a
m

[

4
v
9
9
2
0
0
.
3
0
1
2
:
v
i
X
r
a

Abstract

We consider the problem of learning the op-
timal policy for inﬁnite-horizon Markov de-
cision processes (MDPs). For this purpose,
some variant of Stochastic Mirror Descent is
proposed for convex programming problems
with Lipschitz-continuous functionals. An
important detail is the ability to use inexact
values of functional constraints and compute
the value of dual variables. We analyze this
algorithm in a general case and obtain an es-
timate of the convergence rate that does not
accumulate errors during the operation of the
method. Using this algorithm, we get the
ﬁrst parallel algorithm for mixing average-
reward MDPs with a generative model with-
out reduction to discounted MDP. One of the
main features of the presented method is low
communication costs in a distributed central-
ized setting, even with very large networks.

1 INTRODUCTION

We consider the following nonsmooth convex optimiza-
tion problem over a simple closed convex set Q ⊆ E,
where E is a ﬁnite-dimensional normed space, with
additional functional constraints

f (x),

min
x∈Q
s.t. g(l)(x) ≤ 0, ∀l ∈ [m].

In the context of modern large-scale optimization, the
number of constraints m could be huge, so we are in-
terested in ﬁrst-order algorithms in which a number
of iterations does not depend on m. In book by Ne-
mirovski and Yudin (1983) it was noticed that the

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

usual (sub)gradient method could handle functional
constraints without additional price in terms of a to-
tal number of iterations. The proposed scheme in the
simple form could be written as follows

xk+1 =






xk − η∇f (xk),

max
l∈[m]
xk − η∇g(l(k))(xk), max
l∈[m]

g(l)(xk) ≤ ε;

g(l)(xk) > ε,

where l(k) = argmaxl∈[m] g(l)(xk) and ε is a desired ac-
curacy of constraint satisfaction. We emphasize that
this scheme could be paralleled very eﬃciently: dif-
ferent threads or nodes of the network could handle
the computation of diﬀerent constraints. The ability
to parallel computation is crucial for any large-scale
application.

Next there were two main directions in the develop-
ment of this scheme:

• use of stochastic (sub)gradients;

• computation of dual variables for the Lagrange

dual problem (primal-duality).

The ﬁrst direction is essential for large-scale applica-
tions because the computation of exact gradients could
be impossible or computationally heavy. The value of
the second type of development highly depends on the
particular application but always gives a possibility to
use stopping criteria based on the duality gap.

The development of the stochastic case was initiated
in the paper (Nemirovski et al., 2009) for Mirror De-
scent without functional constraint and developed for
high-probability deviations bounds in the paper (Lan
et al., 2012). The work (Bayandina et al., 2018) makes
possible application of these results to the scheme with
functional constraints.

The ﬁrst step to the primal-duality of subgradient
method was done in the paper (Nesterov, 2009) but in
a diﬀerent direction: the author solves the dual prob-
lem using subgradient method and reconstructs the
primal variables. In the work (Nesterov and Shpirko,
2014) the authors propose the scheme that solves the

 
 
 
 
 
 
Primal-Dual Stochastic Mirror Descent for MDPs

primal conic problem using subgradient method with
functional constraints and afterward computes dual
variables without any computational price. This ap-
proach was generalized to arbitrary deterministic con-
vex optimization problems in the paper (Bayandina
et al., 2018).

However, a natural question appears: Is it possible to
combine these two properties and propose a stochastic
subgradient algorithm that computes the dual variables
without additional computational price? In this paper,
we give a positive answer. Additionally, we propose a
bright application that requires combining both traits
with additional inexactness in constraint computation.

Markov Decision Process. We apply the pro-
posed primal-dual stochastic Mirror Descent to the
problem of mixing average-reward Markov Decision
Process.

Markov Decision Process (MDP) is a mathematical
model for the reinforcement learning (RL) problem,
the rapidly developing branch of modern machine
learning (Sutton and Barto, 2018; Szepesvari, 2010).
We consider the inﬁnite horizon average-reward set-
ting of this problem. For complete notations we refer
to Section 3.

The solution to the average-reward MDP (AMDP)
with S states and Atot state-action pairs could be de-
scribed through the linear program that obtained from
Bellman equations (Bertsekas, 2005)

¯v

min
¯v,h
s.t. ¯v1 + (ˆI − P)h − r ≥ 0,

where ˆI(i,ai),j = Ii,j, P ∈ RAtot×S is a transition prob-
ability matrix, r ∈ RAtot
is a vector of rewards for
state-action pairs, ¯v is an average reward value, and
h is a bias vector. For another setting of discounted
MDP we refer to the line of the previous work (Azar
et al., 2012; Sidford et al., 2018a,b; Agarwal et al.,
2020; Li et al., 2020) and references within.

Let us list important properties of the problem (6): 1)
it has a huge number of constraints Atot; 2) it is im-
possible to compute neither constraints nor its gradi-
ent since the transition probability matrix P is usually
unknown; 3) a policy that corresponds to the optimal
average reward ¯v can be computed from the solution
to the dual problem of (6).

To handle the second problem we consider solving
AMDP with generative model or sampler (Azar et al.,
2012; Jin and Sidford, 2020; Agarwal et al., 2020):
a stochastic oracle generates a transition state from
a given state-action pair according to probabilities
P. This assumption makes applying of the proposed

stochastic primal-dual Mirror Descent possible to the
problem (6) after a suitable approximation of con-
straint functions. We underline that it is required to
use a combination of all properties of our algorithm
to solve this problem. Additionally, we notice that
the work (Lan and Zhou, 2020) can handle stochastic
constraints without additional approximation of con-
straint functions but at the price of much worse com-
plexity ∼ ε−4.

However, it is impossible to obtain rates for AMDP
solving without additional assumptions. We consider
the mixing assumption that the Markov chain that
corresponds to the choice of any policy converges to
the stationary distribution suﬃciently fast. In papers
(Wang, 2017; Jin and Sidford, 2020) authors showed
that under mixing assumption, the search space for the
bias vector h could be bounded using mixing time tmix.
A similar assumption was studied in the paper (Kearns
and Singh, 2002) and an alternative one of communi-
cating MDP with a ﬁnite diameter in works (Bartlett
and Tewari, 2009; Jaksch et al., 2010; Agrawal and Jia,
2017).

To the authors’ best knowledge, there are only three
works that consider the inﬁnite-horizon mixing AMDP
with a generative model – (Wang, 2017; Jin and Sid-
ford, 2020, 2021).
In the ﬁrst two papers the same
general convex optimization algorithm was used –
Stochastic Mirror Descent for saddle-point problems,
and both of the presented algorithms are not designed
for parallel computations. In the last paper authors
perform reduction to the discounted problem, and the
price of this reduction is sample complexity of order
O(ε−3).
In this paper, we present the ﬁrst paral-
lel algorithm for this problem without reductions to
discounted MDP with very low communication costs.
The parallelism gives a possibility to handle very large
setups of MDPs that cannot be stored in the memory
of one machine. In the case of simultaneous working
of Atot workers, our algorithm works in ˜O(t2
mix|S|ε−2)
real time and outperforms approach of Jin and Sidford
(2020) which works in ˜O(t2

mixAtotε−2).

Our contribution.

• The ﬁrst (sub)gradient-based algorithm for opti-
mization with functional constraints that 1) al-
lows the use of stochastic gradients, 2) computes
the value of Lagrange dual variables, 3) allows
inexact computation of constraints at the same
time.

• The ﬁrst parallel algorithm for solving mixing
average-reward MDP without reduction to the
discounted problem. Additionally, this algorithm
has very low communication costs and thus could

Daniil Tiapkin, Alexander Gasnikov

work on very large centralized networks eﬀec-
tively.

Now we are going to introduce deﬁnitions that will be
useful in the algorithm description.

Paper organization. Section 2 is devoted to the
proposed primal-dual stochastic variant of the Mirror
Descent algorithm. All proofs are presented in the sup-
plementary material. Section 3 describes how to apply
the results of Section 2 to the mixing AMDP problem.
Finally, Section 4 contains a numerical comparison to
the approach described in the paper of Jin and Sidford
(2020).

i=1 xi = 1} and Bn

Notation. For a matrix A ∈ Rn×m we deﬁne its i-th
row as A(i). We denote by 1 = (1, . . . ,1)(cid:62) the vec-
tor ﬁlled with ones. By ei we deﬁne a standard basis
vector. Also we deﬁne ∆n = {x ∈ Rn | ∀i : xi ≥
0, (cid:80)n
c = [−c,c]n. I is an identity
matrix of size deducible from the context. Inner prod-
uct (cid:104)·, ·(cid:105) : E∗ × E → R is deﬁned on pairs of vectors
from dual and primal spaces. In the case of Euclidean
spaces, it coincides with the standard inner product.
For a normed space (E, (cid:107) · (cid:107)) we deﬁne a dual norm
on a space E∗ as follows: (cid:107)v(cid:107)∗ = supx:(cid:107)x(cid:107)=1(cid:104)v, x(cid:105).
Also we deﬁne [m] = {1, . . . ,m}. By I{A} we de-
ﬁne an indicator of a set A. For i ∈ [m] deﬁne
ei ∈ Rm : ei[j] = I{i = j}.

2 PRIMAL-DUAL STOCHASTIC

MIRROR DESCENT

In this section we develop techniques of Bayandina
et al. (2018). Firstly, we introduce a basic notation
that will be used further. Then we propose a new
algorithm for the constrained convex stochastic opti-
mization problem in the model of inexact computation
of constraint functions. Finally, we prove convergence
of the algorithm in terms of the duality gap between
primal and (Lagrange) dual problems. The last part
is crucial for an application on average-reward MDPs.

2.1 Notation

We consider the constrained convex optimization prob-
lem over a convex compact set Q ⊆ E, where E is a
ﬁnite-dimensional normed space

Deﬁnition 1 Function hδ : Q → R is called a δ-
approximation of h : Q → R if |hδ(x) − h(x)| ≤ δ for
all x ∈ Q.

For our problem, we suggest that we have an oracle
not for computation of constraints g(l) but their δ-
approximations g(l)
δ . It is the important diﬀerence with
the setup of Bayandina et al. (2018): in our assump-
tion we cannot consider only one constraint of form
g(x) = maxl∈[m] g(l)(x) because it does not seem pos-
sible to compute a subgradient of g given subgradients
of g(l) when index on which the maximal value attains
is unknown.

Now we are ready to list all our assumptions on prob-
lem setup (1) for our algorithm:

(A1) f and all g(l) are Lipschitz continuous with con-
stant M for objective function and for all con-
straints;

(A2) Stochastic

subgradients

are

unbiased:

E∇f (x,ξ) = ∇f (x), E∇g(l)(x,ξ(l)) = ∇g(l)(x);

(A3) Stochastic

subgradiens
(cid:107)∇f (x,ξ)(cid:107)∗ ≤ M, (cid:107)∇g(l)(x,ξ(l))(cid:107)∗ ≤ M a.s.;

are

bounded:

Since our algorithm is Mirror-Descent based, the next
step is to deﬁne the proximal step and basic properties
of Mirror Descent.

Firstly, we deﬁne a prox-function d : Q → R as a con-
tinuous 1-strongly convex function d with respect to
the norm (cid:107) · (cid:107) on E that admits continous selection
of subgradients ∇d(x) where they exist. Bregman di-
vergence that corresponds to a prox-function d is a
function V (x,y) = d(y) − d(x) − (cid:104)∇d(x), y − x(cid:105).

Given vectors x ∈ E and v ∈ E∗, the mirror step is
deﬁned as

x+ = Mirr(x, v) = argmin

{(cid:104)v, y(cid:105) + V (x,y)} .

y∈X

We assume that the mirror step can be easily com-
puted.

f (x),

min
x∈Q
s.t. g(l)(x) ≤ 0, ∀l ∈ [m],

2.2 Primal Problem

(1)

and f : Q → R, g(l) : Q → R are convex functions. We
assume that subgradients of these functions exist for
each x ∈ Q for simplicity. We call ∇f (x), ∇g(l)(x) any
subgradients of corresponding functions. However, in
our algorithm we have an access only to stochastic
subgradient oracles ∇f (x, ξ), ∇g(l)(x, ξ(l)).

In this subsection, we consider problem (1) in terms
of convergence of the objective function in conﬁdence
region. Formally speaking, a vector ˆx is called an
(εf , εg, σ)-solution to the primal problem (1), if

f (ˆx) − f (x∗) ≤ εf ,

g(l)(ˆx) ≤ εg ∀l ∈ {1, . . . ,m} w.p. ≥ 1 − σ,

(2)

Primal-Dual Stochastic Mirror Descent for MDPs

where x∗ is a true minimizer of the problem (1). We
assume that an algorithm have access only to stochas-
tic subgradient oracles of functions f, g(l) and to δ-
approximations g(l)

δ of constraint functions.

Algorithm 1: Stochastic Mirror Descent
with noisy constraints

Input: accuracy ε > 0, number of steps N ,

stepsize η = ε/M 2

1 x0 = argminx∈Q d(x);
2 I = ∅, J = ∅;
3 for k = 0,1,2, . . . ,N − 1 do

4

5

6

7

8

9

10

if g(l)

δ (xk) ≤ ε + δ ∀l ∈ [m] then
xk+1 = Mirr(xk, η∇f (xk, ξk)) ;
// "productive" steps
Add k to I

else

l(k) = argmax

l∈[m]

g(l)
δ (xk);

xk+1 = Mirr(xk, η∇g(l(k))(xk, ξk
// "non-productive" steps
Add k to J;

(l(k)))) ;

11 return ˆx = 1
|I|

(cid:80)

k∈I xk;

Denote ˆ∇kf = ∇f (xk, ξk), ∇kf = ∇f (xk) and
ˆ∇kg(l) = ∇g(l)(xk, ξk
(l)), ∇kg(l) = ∇g(l)(xk). Addi-
tionally, we deﬁne Θ2
0 = d(x∗) − d(x0). In these terms
we could provide the main theorem.

Theorem 1 Algorithm 1 with a constant stepsize η =
ε/M 2 outputs (ε, ε + 2δ, σ)-solution for any ε > 0, σ ∈
(0,1), δ ≥ 0 in sense of (2) after

N ≥ N0 =

280 · Θ2

0M 2 log(1/σ)

ε2

.

The proof of this theorem is given in supplementary
material.

Remark 1. Notice that from theoretical point of view
selection of the maximum in line 9 of Algorithm 1 could
not be avoided. However, in case of non-stochastic
constraint computation one of used heuristic is to set
l(k) to any index of violated constraint and we suggest
that such heuristic could work in the noisy setting too.

Remark 2. Notice that a quantity Θ2
0 is not used in the
pseudocode of Algorithm 1. Thus, it is possible to use
another initial x0 and have a warm start such that our
algorithm converges faster. This warm start could be
obtained by running an algorithm several times with
a decreasing value of ε.

dina et al. (2018); Stonyakin et al. (2019) to use adap-
tive stepsizes that does not rely on knowledge of Lip-
schitz constant M .

2.3 Primal-Dual Convergence

In this subsection, we extend properties of the previous
algorithm and prove its primal-duality. First of all, let
us deﬁne the (Lagrange) dual optimization problem
associated with the problem (1)

(cid:40)

max
λ∈Rm
+

φ(λ) := min
x∈Q

{f (x) +

(cid:41)

λlg(l)(x)}

.

(3)

n
(cid:88)

i=1

Call λ∗ a solution to this dual problem (if it exists).
We refer to (Boyd and Vandenberghe, 2004) for an
additional background and examples.

It is well-known that for any x ∈ Q : g(l)(x) ≤
0 ∀l ∈ {1, . . . ,m} and λ ∈ Rm
+ the weak duality holds:
∆(x,λ) = f (x) − φ(λ) ≥ 0, where ∆ is so-called the
duality gap. We assume that for our primal problem
(1) the Slater’s condition holds, i.e. ∃x ∈ Q : ∀l ∈
{1, . . . , m} : g(l)(x) < 0.
It implies that the dual
problem has a solution and there is the strong dual-
ity: ∆(x∗, λ∗) = 0 for any x∗ and λ∗ are solutions to
the primal and the dual problems respectively.

It gives us a natural way to measure a quality of the
pair (ˆx, ˆλ) by the value of the duality gap ∆(ˆx, ˆλ). Let
us call the pair (ˆx, ˆλ) a primal-dual (ε∆, εg, σ)-solution
to (1) if the following holds with probability at least
1 − σ

∆(ˆx, ˆλ) ≤ ε∆,
g(l)(ˆx) ≤ εg ∀l ∈ [m].

(4)

Notice that since ˆx is not a feasible solution to the
primal problem (1), we do not have the weak duality
inequality ∆(ˆx, ˆλ) ≥ 0. However, the value of duality
gap could be controlled from below because of con-
trolled unfeasibility.

The most powerful property of Algorithm 1 is a pos-
sibility to generate a pair of primal-dual solutions in
sense of (4): we could control the value of the dual-
ity gap without the explicit access to the constraint
functions.

Following (Nesterov and Shpirko, 2014; Bayandina
et al., 2018), we choose the following ˆλ ∈ Rm
+ as an
estimate of dual variables

ˆλl =

1
|I|

(cid:88)

k∈J

I{l = l(k)}

(5)

Remark 3. We used a constant stepsize for the sake of
simplicity. It is possible to adapt techniques of Bayan-

in terms of Algorithm 1. Additionally, we deﬁne useful
2
0 = supy∈Q(d(y) − d(x0)).
constant Θ

Daniil Tiapkin, Alexander Gasnikov

Using ˆλ, we could provide primal-dual properties of
Algorithm 1.

Theorem 2 Let us choose ˆλ ∈ Rm
+ as deﬁned in (5)
and ˆx is an output of Algorithm 1 with a constant step-
size η = ε/M 2. Then the pair (ˆx, ˆλ) is an (ε, ε+2δ, σ)-
solution in sense of (4) for any ε > 0, δ ≥ 0, σ ∈
(0, 1/2) after

N ≥ N (cid:48)

0 =

128Θ

2
0M 2(17 log(2/σ) + 2κ(E∗))

ε2

,

where κ(E∗) is a constant of Nemirovski’s inequality
(Boucheron et al., 2013) for the dual space.

Remark 1.
If E has a ﬁnite dimension d, then we
always have κ(E∗) ≤ d. Additionally, if E is endowed
with (cid:96)p norm, then E∗ is endowed with (cid:96)q norm, where
1/p + 1/q = 1, and there is a more precise bound,
according to (D¨umbgen et al., 2010)

κ(E∗) ≤ K

(cid:18) p

p − 1

(cid:19)

, d

=

(cid:40)

2

p −1, p ∈ [1,2]
d
d1− 2

p , p ∈ (2, + ∞]

In particular, if E has (cid:96)2 norm, κ(E∗) = 1. For p ∈
[2, + ∞] this bound is tight, however, in the case p ∈
[1,2] and d ≥ 3 it could be improved (Boucheron et al.,
2013) to, for instance, a logarithmic bound κ(E∗) ≤
2e log(d)−e, that could be useful in the case of (cid:96)1-norm
and an entropy prox-function.

We can write bound on N (cid:48)

0 using O-notation as follows

N = O

(cid:32)

2
0M 2(log(1/σ) + κ(E∗))
Θ
ε2

(cid:33)

.

The only diﬀerence between primal and dual case is
connected to the constant in Nemirovski’s inequality.

Remark 2. As in the primal case, in the complexity
2
bounds we have a constant Θ
0 that does not appear in
the algorithm description. This fact gives us a chance
to work much better in practice than using worst-case
constant. The same situation with constant κ(E∗).

3.1 Markov Decision Process

An instance of MDP is a tuple M = (S,A,P, r), where
S is a ﬁnite set of states; A = (cid:70)
i∈S Ai is a ﬁnite
state of actions, each set Ai contains actions from the
state i. P is the collection of state-to-state transition
probabilities given actions: P = {pij(ai) | i,j ∈ S, ai ∈
A} where pij(ai) is a probability of transition from a
state i to a state j given an action ai. Also, we deﬁne
r ∈ [0,1]|A| as the state-action reward vector, ri,ai is
the instant reward received when taking the action ai
at the state i ∈ S. For consistency of notation with
work of Jin and Sidford (2020), let (i,ai) ∈ A denote
an action ai at a state i. Atot = |A| = (cid:80)
i∈S |Ai|
denotes the total number of state-action pairs. Also
we denote by P the action-state transition probability
matrix of size Atot × |S|, where P(i,ai),j = pij(ai) in
terms of P.

The goal is to ﬁnd a stationary (randomized) policy
that speciﬁes actions to choose in the ﬁxed state. For-
mally, a policy π is a block vector such that i-th block
corresponds to a probability distribution over Ai. De-
ﬁne as Pπ, rπ the transition matrix and the cost vector
under the ﬁxed policy π.

Now we are going to deﬁne optimality of the policy.
In this paper we consider the inﬁnite-horizon average-
reward MDP with the following objective to maximize

¯vπ = lim
T →∞

Eπ

(cid:34)

1
T

T
(cid:88)

t=1

(cid:35)

rit,at | i1 ∼ q

.

Here {i1,a1, . . . ,it,at} are state-actions transitions gen-
erated by MDP under a policy π, q is an initial dis-
tribution, and an expectation Eπ[·] is taken over tra-
jectories. In our case we interested in the case then
the Markov chain generated by an AMDP under a
ﬁxed policy π has a unique stationary distribution
νπ : νπ · Pπ = νπ. Notice that in this case the value of
¯vπ does not depend on an initial distribution q. Then
the objective simpliﬁes a lot:

¯vπ = (cid:104)νπ, rπ(cid:105).

3 MIXING AMDP

In this section, we discuss the application of the devel-
oped algorithm to the problem of approximate solv-
ing mixing average-reward Markov Decision Processes
(MDP). Firstly, we propose basic deﬁnitions connected
to MDPs. Next, we discuss some technical nuances
that will appear in the algorithm and, ﬁnally, we out-
line the complete algorithm and its parallel implemen-
tation.

Next, we deﬁne Bellman equations for an AMDP
(Bertsekas, 2005): ¯v∗ is the optimal average reward
if and only if there exists a vector h∗ ∈ R|S| satisfying
the following

¯v∗ + h∗

i = max
ai∈Ai




(cid:88)



j∈S

pij(ai)h∗

j + ri,ai






, ∀i ∈ S.

We focus on study of the primal LP which solution is

Primal-Dual Stochastic Mirror Descent for MDPs

equivalent to the solution to Bellman equation

¯v

min
¯v,h
s.t. ¯v1 + (ˆI − P)h − r ≥ 0,

(6)

where ˆI(i,ai),j = Ii,j.
However, without additional assumptions it is hard to
analyze problem. Following (Jin and Sidford, 2020;
Wang, 2017), we introduce one important assumption
on an AMDP instance.

Assumption 1 (Mixing AMDP) We
an
AMDP instance mixing if its so-called mixing time
(deﬁned below) is bounded

call

(cid:20)

(cid:26)

tmix := max

π

argmin
t≥1

max
q

(cid:107)(Pπ (cid:62))tq − νπ(cid:107)1 ≤

(cid:27)(cid:21)

.

1
2

The most powerful corollary of this result is a possi-
bility to make the search space a compact convex set.
Formally speaking, in (Jin and Sidford, 2020) it was
proven that the search space for primal variables could
be reduced to X = [0,1] × B|S|
2R = [0,1] × [−2R,2R]|S|,
where R = 2tmix. We choose bounds of size 2R instead
of R because of the same reasons as (Jin and Sidford,
2020) that will be described in Section 3.3.

Overall, we have a (linear) optimization problem on a
compact with a large number of constraints

¯v

min
¯v,h∈X
s.t. ¯v1 + (ˆI − P)h − r ≥ 0.

(7)

If we knew the matrix P, we could apply Stochas-
tic Mirror Descent with constraints (Bayandina et al.,
2018) and get an approximate solution to this LP prob-
lem. However, it is not the case: we have only sam-
pling access to the transition probability matrix. An-
other problem we face is computing an optimal policy
by an approximate solution to this linear program. It
is known that there is a strong connection between the
optimal policy and the dual LP but not the primal one.
Therefore, we use primal-duality of Algorithm 1 and
construct a policy using dual variables.

3.2 Preprocessing

In this subsection we aim to describe complexity of the
preprocessing connected to the estimate of the transi-
tion probability matrix. We take the estimate of the
form

(cid:101)P(i,ai) =

1
n

n
(cid:88)

j=1

eXj ,

where Xj are sampled from categorical distribution
P(i,ai). Choosing appropriate N and compute these

quantities for each state-action pair in parallel, we ob-
tain the following proposition.

Proposition 1 For each δ(cid:48),σ(cid:48) > 0, the estimate (cid:101)P
of P, such that for each a ∈ A, h ∈ B|S|
2R : |(cid:104)P(a) −
(cid:101)P(a), h(cid:105)| ≤ δ(cid:48) with probability at least 1 − σ(cid:48), could be
computed in O
total sam-
(cid:17)

mixAtot · |S|+log(Atot/σ(cid:48))
t2

δ(cid:48)2
(cid:16)

(cid:17)

(cid:16)

ples, O(1) parallel depth and O
samples proceed by each single node.
case of m ≤ Atot available workers,
|S|+log(Atot/σ(cid:48))
O
δ(cid:48)2

real time.

m · t2

(cid:16) Atot

mix

(cid:17)

t2
mix

|S|+log(Atot/σ(cid:48))
δ(cid:48)2

In the
it works in

The proof of this proposition is given in supplemen-
tary.

3.3 Rounding to Optimal Policy

In this subsection, we prove the result that give us
a possibility to obtain an approximate optimal policy
from the dual variables produced by (1).

Proposition 2 Suppose that primal (¯vε, hε) and dual
µε variables are (εf , εg, σ) -approximate solution to
(7) in terms of (expected) primal-dual convergence (4).
Deﬁne the policy π: πi,ai = µi,ai
+ is
λi
deﬁned as λi = (cid:80)
µi,ai . Then π is an 4(εf + εg)-
optimal policy with probability at least 1 − σ.

, where λ ∈ R|S|

ai∈Ai

The proof is given in supplementary material, and it
is very similar to the proof of (Jin and Sidford, 2020).
There are two diﬀerences. Firstly, we have guar-
antees on our primal-dual solution, not the solution
to a saddle-point problem, and this slightly changes
the structure of the proof. Secondly, we have high-
probability bounds instead of bounds in expectation.

3.4 Parallel Algorithm

In this subsection, we describe a ﬁnal algorithm to
approximate solving AMDP in parallel. In our setup,
each single node of a centralized network corresponds
to a single state-action pair.

First of all, we describe linear program corresponding
to AMDP (7) in terms of (1),

f (¯v, h) = ¯v,

min
¯v,h∈X
s.t. g(i,ai)(¯v, h) = r(i,ai) − ¯v

(8)

+ ((cid:104)P(i,ai), h(cid:105) − hi) ≤ 0 ∀(i,ai) ∈ A.

4tmix

where X = [0,1] × B|S|
. We set standard Euclidean
prox-structure on X : (cid:96)2-norm (cid:107)·(cid:107)2 and a prox-function
d(x) = 1
2 (cid:107) · (cid:107)2
2. In these terms, we have the following
mix|S|).
constants: M = 2 and Θ

2
0 = (4R)2|S|+1 = O(t2

Daniil Tiapkin, Alexander Gasnikov

However, in our case we cannot compute constraints
since there is no access to the true model. To over-
come this, we run preprocessing described in Section
3.2 to derive approximate model (cid:101)P and obtain δ-
approximation of constraints g(i,ai)

δ

g(i,ai)
δ

(¯v,h) = r(i,ai) − ¯v + ((cid:104) (cid:101)P(i,ai), h(cid:105) − hi).

(9)

Additionally, we are going to use stochastic subgradi-
ents for g by using samples of next state s ∼ P(i,ai):

∇¯vg(i,ai)(¯v, h) = −1,

ˆ∇hg(i,ai)(¯v, h, s) = es − ei,

(10)

with constant M = 2. In this case, we can use Al-
gorithm 1 with the following update rules for primal
variables ¯vk and hk. For productive steps (k ∈ I)

¯vk+1 = ¯vk − η,

hk+1 = hk,

(11)

and for non-productive steps (k ∈ J)

¯vk+1 = ¯vk + η,

hk+1 = hk − η(es − ei),

(12)

a

is

(i, ai)
δ (¯vk, hk) attains, and s ∼ P(i,ai)

pair where
where
maxa∈A ga
is a
transition sample. We note that in this form this
algorithm is sequential.

state-action

To design a parallel version of Mirror Descent that pre-
sented in Algorithm 2, we are going to use a separate
node for each state-action pair (j,aj). The aim of node
corresponds to state-action pair (j,aj) is 1) compute
and update value of ck
and 2) sample
transitions of MDP.

(j,aj ) := g(j,aj )

δ

To compute ck
(j,aj ) faster than in O(|S|) operations, we
note that updates of ¯vk and hk are sparse and we can
store the previous value ck−1
(j,aj ).
In the case of productive steps (k ∈ I) we update as
follows:

(j,aj ) = ck−1
ck

(j,aj ) + η.

(13)

This update rule is correct by the update rule (11) and
equation (9).

In the case of non-productive steps (k ∈ J) we have a
more complicated update

(j,aj ) = ck−1
ck

(j,aj ) − η(1 + (cid:101)P(j,aj ),s − (cid:101)P(j,aj ),i

+ I{j = i} − I{j = s}).

(14)

The correctness of the procedure is guaranteed by the
update rule (12) and deﬁnition (9). Update rules (13)
and (14) gives us an opportunity to update constraints
in O(1) time per each node.

Theorem 3 Let ε > 0 and σ ∈ (0, 1/2). The policy
ˆπ generated by Algorithm 2 with a constant stepsize

η = ε/64 and preprocessing described in Section 3.2
performed with parameters δ(cid:48) = ε/16, σ(cid:48) = σ/2 is an
ε-approximate optimal policy with probability at least
1 − σ if

N = O

(cid:18) t2

mix|S| log(1/σ)
ε2

(cid:19)

.

The described algorithm has O(1) parallel depth,
O(Atot·N ) sample and running time complexity. In the
case of m ≤ Atot available workers, algorithm works in
mix|S| log(1/σ) · ε−2(cid:1) real time. The full de-
O (cid:0) Atot
scription is presented in Algorithm 2.

m · t2

Remark 1. Notice that complexity of preprocessing
and Algorithm 2 matches up to logarithmic factors.

Remark 2. Messages ”Productive step” and ”Non-
productive step” ensure that nodes update their values
to actual ones.

Remark 3. The communication costs on each round
of communication are low: each text message could be
send using O(1) bits, and each message with a sample
could be sent using only O(log |S|) bits.

Remark 4. Additional advantage of the algorithm is
a sparsity of updates: there are at most 2 values in
the vector h updated each iteration. From the point
of view of external memory algorithms, it gives us a
possibility to make only 2 requests to the memory if
the state-space is too large to store a vector h in RAM.

Remark 5. We use a very coarse bound on Θ
practice we might expect that Θ
thus, reduce our dependence on |S| to logarithmic.

∼ |S|. In
∼ poly log |S| and,

2

2

Proof. By Proposition 2, we can produce ε-optimal
policy with probability at least 1 − σ/2 by running
Algorithm 1 on problem (8) with an accuracy ε(cid:48) = ε/8
because 4(ε∆ + εg) = 4(ε(cid:48) + 2δ) = 4ε(cid:48) + 8δ = 8ε(cid:48).
Performing union bound for probability of failure for
a preprocessing and an algorithm, we have required
probability of success of whole scheme.

Since from the point of view of the head node
Algorithm 2 is essentially Algorithm 1, we have
needed guarantees on number of Mirror Descent it-
2
erations by computed constants M = O(1), Θ
0 =
O(t2
mix|S|), κ(E∗) = 1 and Theorem 2. Notice that
the parallel depth of the algorithm is equal to 1.

Total running time complexity forms by additional
Atot computations on constraints on each iterations
and each computation spends O(1) time by using pre-
vious values of computed constraints. The last obser-
vation connected to the fact that update of ¯vk and ¯hk
(cid:3)
also spends O(1) time by sparsity.

Primal-Dual Stochastic Mirror Descent for MDPs

Algorithm 2: Parallel average-reward MDP
Input: accuracy ε > 0, number of steps N ,

approximation accuracy δ = ε/16,
Mirror Descent accuracy ˜ε = ε/16,
stepsize η = ˜ε/4, conﬁdence level σ.

1 Procedure MirrorDescent():
2

¯v0 = 0; h0 = 0;
for k = 0,1,2 . . . , N − 1 do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

(¯vk, hk) ∀(i, ai) ∈ A;
(¯vk, hk) ≤ ˜ε + δ then

Send ”Check constraints” to all nodes;
Receive g(i,ai)
δ
g(i,ai)
if max
δ
a∈A
¯vk+1 = ¯vk − η, hk+1 = hk;
Send ”Productive step” to all nodes;
Add k to I;

end
else

δ

(¯vk, hk);

(i,ai)k = argmaxa∈A g(i,ai)
Send ”Sample” to node (i,ai)k;
Receive state s;
¯vk+1 = ¯vk + η,
hk+1 = hk − η · (es − ei);
Send ”Non-productive step”, i and

s to all nodes;

Add k to J;

end

19

20

end
(cid:80)
ˆµ(i,ai) = 1
|I|
return πi,ai = µi,ai/ (cid:0)(cid:80)
21
22 Procedure WorkerNode(j, aj):
23

k∈J I{(i,ai)k = (i,ai)};
(cid:1);

µi,ai

ai∈Ai

Compute (cid:101)P(j,aj ) with precision δ and
conﬁdence level σ/2;
c0
(j,aj ) = rj,aj ,k = 0;
while is not ﬁnished do

Wait message;
if ”Compute constraints” then
(j,aj ) as g(j,aj )
(¯vk, hk);

Send ck

δ

end
else if ”Sample” then

Sample s ∼ P(j,aj ) and send it;

end
else if ”Productive step” then

k = k + 1;
(j,aj ) = ck−1
ck

(j,aj ) + η;

end
else if ”Non-productive step” then

Receive s and i;
k = k + 1;
(j,aj ) = ck−1
ck
(j,aj ) − η(1 + (cid:101)P(j,aj ),s −
(cid:101)P(j,aj ),i + I{j = i} − I{j = s});

end

end

Figure 1: Comparison between Stochastic Mirror De-
scent (Jin and Sidford, 2020) and Algorithm 2 on
RiverSwim environment with 6 states and 2 actions.

4 NUMERICAL EXPERIMENTS

In this section we perform numerical comparison of
Algorithm 2 in sequential setting with algorithm de-
scribed in paper (Jin and Sidford, 2020).

At ﬁrst, we highlight technical features. In both men-
tioned algorithms, the value of mixing time tmix is
needed. However, computation of maximum over poli-
cies of discrete-valued function seems to be a very
hard problem to be computed precisely. We replace
the value of tmix by its estimate (cid:100)tmix computed over
1000 randomly generated policies. Additionally, to
make precise comparison we compute optimal average-
reward value v∗ using LP-representation of the prob-
lem (6) for known model, and the average-reward value
vπ of the given policy π by computing a stationary dis-
tribution using known model.

For our comparison, we apply algorithms to two dif-
ferent environments: RiverSwim (Strehl and Littman,
2008), Access-Control Queuing task (Sutton and
Barto, 2018). We choose these environments because
we have an exact model for them, and they require
non-trivial exploration.

RiverSwim. We start with the environment de-
scription. RiverSwim is an environment with 6 states
in a row with 2 actions for each state: swim to the left
or swim to the right. Swimming to the left is always
successful. For the ﬁrst state, this action returns to
itself and gives 0.005 reward. Swimming to the right
move the agent to the right state with probability 0.35,
make the agent stay in the current state with proba-
bility 0.6, and move to the left with probability 0.05.
For the last state moving to the right state returns the
agent to this last state and gives 1 reward. Thus, the
optimal average-reward policy is always swimming to
the right.

Estimate value of (cid:100)tmix is equal to 155 and it makes this
environment hard for our algorithm 2 and Stochastic

Daniil Tiapkin, Alexander Gasnikov

a possibility to work with a very large state space.

Another contribution is the development of Mirror
Descent with constraints algorithms. We provide
an algorithm that works with inexact computation
of constraints and prove its primal-dual properties.
The setting of inexact computation of constraints was
developed in (Lan and Zhou, 2020) but results on
primal-dual convergence of such algorithms appeared
in known literature only in a deterministic exact case
(Bayandina et al., 2018).

Turning to possible extensions, there arise natural
questions.

Could a preprocessing step be avoided and make the
algorithm model-free?
In the current version, we
needed to do a required number of preprocessing it-
erations to guarantee the condition on constraints. It
seems possible to use an unbiased stochastic oracle for
constraint evaluation.

Another question is connected to the total work com-
plexity. The cost of a high level of parallelism is
worse total running time and sample complexity in
comparison to algorithms based on saddle-point for-
mulations.
It is connected to two theoretical issues:
1) (cid:96)1-approximation of the model and 2) performing
Mirror Descent with box constraints. The ﬁrst issue
could be resolved using another approximation metric
that respects MDP structure as it was done in (Agar-
wal et al., 2020). The second issue is fundamental for
non-linear optimization due to existing lower bounds
(Guzm´an and Nemirovski, 2015). The only way to
avoid it is to use the linear structure of mixing AMDP
problem. Could the total work time and sample com-
plexity be reduced without losing the possibility to run
in parallel?

The last question is about further generalizations of
our approach. One of the interesting directions is to
use our algorithm for solving constrained MDP as it is
done in the paper (Jin and Sidford, 2020). However,
we note that it is possible to use non-linear constraints
in our case.

Figure 2: Comparison between Stochastic Mirror De-
scent (Jin and Sidford, 2020) and Algorithm 2 on
Access-Control Queuing task with 10 servers.

Mirror Descent (SMD) (Jin and Sidford, 2020). We
compare these algorithm with ε = 10−2 and use 1000
samples for preprocessing, the result is presented on
Figure 1. Signiﬁcant superior of our approach could
be explained by independence of stepsize of Algorithm
2 to the value of tmix, whereas stepsize in algorithm of
(Jin and Sidford, 2020) depends on this quanitity as
1/t2
mix. In the case of this MDP, stepsize of SMD is
smaller in 2.5 · 104 times. Additionally, notice that we
generated a very little number of samples during pre-
processing, and therefore these computation does not
aﬀect the total running time. Additionally, high spar-
sity of rewards explains the high value of the objective
function during ﬁrst iterates.

Access-Control Queuing task. This environment
is modelling very practical situation for using average-
reward MDP model. For the description we refer to
(Sutton and Barto, 2018). The only diﬀerence is nor-
malization of rewards to make them in the interval
[0,1].

Estimated value of mixing time (cid:100)tmix = 44 and has
dense rewards. On Figure 2 we present comparison
between Algorithm 2 and SMD with ε = 10−2 and
500 samples for preprocessing. Again, since the value
of tmix is relatively large for this MDP, worse conver-
gence of SMD is explained by smaller stepsize that is
required by theoretical analysis. Our algorithm is al-
most agnostic to the value of tmix and it makes it much
more eﬀective for large values of tmix.

5 CONCLUSION

Acknowledgements

In this work, we proposed a parallel algorithm for solv-
ing an average-reward MDP. As far as we know, it is
the ﬁrst parallel algorithm in the generative model set-
ting without reduction to the discounted problem. The
interesting properties of the provided method are very
low communication costs between the head node and
all other nodes and the sparsity of updates that oﬀer

The work of D. Tiapkin was supported by the grant
for research centers in the ﬁeld of AI provided by
the Analytical Center for the Government of the Rus-
sian Federation (ACRF) in accordance with the agree-
ment on the provision of subsidies (identiﬁer of the
agreement 000000D730321P5Q0002) and the agree-
ment with HSE University No. 70-2021-00139.

Primal-Dual Stochastic Mirror Descent for MDPs

References

Agarwal, A., Kakade, S., and Yang, L. F. (2020).
Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on
Learning Theory, pages 67–83. PMLR.

Agrawal, S. and Jia, R. (2017). Optimistic posterior
sampling for reinforcement learning: worst-case re-
gret bounds. In Guyon, I., Luxburg, U. V., Bengio,
S., Wallach, H., Fergus, R., Vishwanathan, S., and
Garnett, R., editors, Advances in Neural Informa-
tion Processing Systems, volume 30. Curran Asso-
ciates, Inc.

Azar, M. G., Munos, R., and Kappen, B. (2012). On
the Sample Complexity of Reinforcement Learning
with a Generative Model. Appears in Proceedings
of the 29th International Conference on Machine
Learning (ICML 2012).

Bartlett, P. L. and Tewari, A. (2009). Regal: A reg-
ularization based algorithm for reinforcement learn-
ing in weakly communicating mdps. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI ’09, page 35–42, Arling-
ton, Virginia, USA. AUAI Press.

Bayandina, A., Dvurechensky, P., Gasnikov, A.,
Stonyakin, F., and Titov, A. (2018). Mirror
Descent and Convex Optimization Problems with
Non-smooth Inequality Constraints, pages 181–213.
Springer International Publishing, Cham.

Bertsekas, D. (2005). Dynamic Programming and Op-
timal Control. Number 1 in Athena Scientiﬁc opti-
mization and computation series. Athena Scientiﬁc.

Boucheron, S., Lugosi, G., and Massart, P. (2013).
Concentration Inequalities: A Nonasymptotic The-
ory of Independence. Oxford University Press.

Boyd, S. and Vandenberghe, L. (2004). Convex Opti-

mization. NY Cambridge University Press.

D¨umbgen, L., Geer, S., Veraar, M., and Wellner, J.
(2010). Nemirovski’s inequalities revisited. The
American mathematical monthly : the oﬃcial jour-
nal of the Mathematical Association of America,
117:138–160.

Guzm´an, C. and Nemirovski, A. (2015). On lower com-
plexity bounds for large-scale smooth convex opti-
mization. J. Complex., 31:1–14.

Jaksch, T., Ortner, R., and Auer, P. (2010). Near-
optimal regret bounds for reinforcement learn-
ing.
Journal of Machine Learning Research,
11(51):1563–1600.

Jin, Y. and Sidford, A. (2020). Eﬃciently solving
MDPs with stochastic mirror descent. In III, H. D.
and Singh, A., editors, Proceedings of the 37th Inter-
national Conference on Machine Learning, volume

119 of Proceedings of Machine Learning Research,
pages 4890–4900. PMLR.

Jin, Y. and Sidford, A. (2021). Towards tight bounds
on the sample complexity of average-reward mdps.
In Meila, M. and Zhang, T., editors, Proceedings
of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine
Learning Research, pages 5055–5064. PMLR.

Kearns, M. and Singh, S. (2002). Near-optimal re-
inforcement learning in polynomial time. Machine
Learning, 49(2):209–232.

Lan, G., Nemirovski, A., and Shapiro, A. (2012).
Validation analysis of mirror descent stochastic ap-
proximation method. Mathematical programming,
134(2):425–458.

Lan, G. and Zhou, Z. (2020). Algorithms for stochastic
optimization with expectation constraints. Compu-
tational Optimization and Applications, 76.

Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020).
Breaking the sample size barrier in model-based re-
inforcement learning with a generative model.
In
Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M. F., and Lin, H., editors, Advances in Neural
Information Processing Systems, volume 33, pages
12861–12872. Curran Associates, Inc.

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
(2009). Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Op-
timization, 19(4):1574–1609.

Nemirovski, A. and Yudin, D. (1983). Problem Com-
plexity and Method Eﬃciency in Optimization. A
Wiley-Interscience publication. Wiley.

Nesterov, Y. (2009). Primal-dual subgradient methods
for convex problems. Mathematical Programming,
120(1):221–259. First appeared in 2005 as CORE
discussion paper 2005/67.

Nesterov, Y. and Shpirko, S. (2014). Primal-dual sub-
gradient method for huge-scale linear conic prob-
lems. SIAM Journal on Optimization, 24(3):1444–
1457.

Sidford, A., Wang, M., Wu, X., Yang, L. F., and Ye, Y.
(2018a). Near-optimal time and sample complexities
for solving markov decision processes with a genera-
tive model. In Proceedings of the 32nd International
Conference on Neural Information Processing Sys-
tems, pages 5192–5202.

Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b).
Variance reduced value iteration and faster algo-
rithms for solving markov decision processes. In Pro-
ceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 770–787.
SIAM.

Daniil Tiapkin, Alexander Gasnikov

Stonyakin, F., Alkousa, M., Stepanov, A., and Tytov,
A. (2019). Adaptive mirror descent algorithms for
convex and strongly convex optimization problems
with functional constraints. Journal of Applied and
Industrial Mathematics, 13:557–574.

Strehl, A. L. and Littman, M. L. (2008). An analysis
of model-based interval estimation for markov de-
cision processes. Journal of Computer and System
Sciences, 74(8):1309–1331. Learning Theory 2005.

Sutton, R. S. and Barto, A. G. (2018). Reinforcement
Learning: An Introduction. The MIT Press, second
edition.

Szepesvari, C. (2010). Algorithms for Reinforcement

Learning. Morgan and Claypool Publishers.

Wang, M. (2017). Primal-dual π learning: Sam-
ple complexity and sublinear run time for ergodic
markov decision problems.

Supplementary Material:
Primal-Dual Stochastic Mirror Descent for MDPs

A MISSING PROOFS

A.1 Proof of Theorem 1

To simplify the notation, we introduce the stochastic gradient noise function

γk(y) =

(cid:40)

η(cid:104) ˆ∇kf − ∇kf, y − xk(cid:105),
k ∈ I;
η(cid:104) ˆ∇kg(l(k)) − ∇kg(l(k)), y − xk(cid:105), k ∈ J.

(15)

The main property of this quantity is that it forms a martingale-diﬀerence sequence.

Now we are going to provide some useful properties of Algorithm 1 in terms of bounds of error in objective
function f and constraint satisﬁability. Afterwards, we combine all properties to a convergence analysis of the
algorithm. But at ﬁrst, we state one useful technical result. We will actively use the following lemma:

Lemma 1 (Bayandina et al. (2018)) Let h be some convex function over a set Q, η > 0 is a stepsize, x ∈ Q.
Let the point x+ = Mirr(x, η(∇h(x) + ∆)), where ∆ is some vector from the dual space. Then, for any y ∈ Q

η(h(x) − h(y) + (cid:104)∆, x − y(cid:105)) ≤ η(cid:104)∇h(x) + ∆, x − y(cid:105)

≤

η2
2

(cid:107)∇h(x) + ∆(cid:107)2

∗ + V (x,y) − V (x+, y).

Lemma 2 For a point ˆx produced by Algorithm 1 and any y ∈ Q the following holds

η|I| · (f (ˆx) − f (y)) ≤

+

η2M 2
2
N −1
(cid:88)

k=0

N + [d(y) − d(x0)] − |J|η · ε

γk(y) + η

g(l(k))(y).

(cid:88)

k∈J

(16)

Proof. By the construction of ”productive” and ”non-productive” steps in Algorithm 1 and Lemma 1, we have
for all y ∈ Q

η(f (xk) − f (y)) ≤

η2M 2
2

+ V (xk, y) − V (xk+1, y)

+ η(cid:104) ˆ∇kf − ∇kf, y − xk(cid:105),

η(g(l(k))(xk) − g(l(k))(y)) ≤

η2M 2
2

+ V (xk, y) − V (xk+1, y)

+ η(cid:104) ˆ∇kg(l(k)) − ∇kg(l(k)), y − xk(cid:105)

k ∈ I;

k ∈ J.

By deﬁnition of δ-approximation, we have the following inequalities for ”productive” and ”non-productive” steps
respectively

η(f (xk) − f (y)) ≤

η(g(l(k))
δ

(xk) − g(l(k))(y)) ≤

η2M 2
2
η2M 2
2

+ V (xk, y) − V (xk+1, y) + γk(y),

+ V (xk, y) − V (xk+1, y) + γk(y) + ηδ.

Daniil Tiapkin, Alexander Gasnikov

Sum all these inequalities over all k ∈ I and k ∈ J and use the fact that I ∪ J = {0, . . . ,N − 1}

η(f (xk) − f (y)) +

(cid:88)

k∈I

(cid:88)

k∈J

η(g(l(k))
δ

(xk) − g(l(k))(y))

≤

η2M 2
2

|I| +

η2M 2
2

|J| +

N −1
(cid:88)

k=0

[V (xk,y) − V (xk+1,y)] +

N −1
(cid:88)

k=0

γk(y) + |J|ηδ.

(17)

By the choice of x0 = argminx∈Q d(x), we have

N −1
(cid:88)

[V (xk,y) − V (xk+1,y)] ≤ V (x0,y) = d(y) − d(x0) − (cid:104)∇d(x0), y − x0(cid:105) = d(y) − d(x0).

k=0

Using deﬁnition of ”non-productive” steps g(l(k))

δ

(xk) > ε + δ and convexity of f

η(f (xk) − f (y)) +

(cid:88)

k∈I

(cid:88)

k∈J

η(g(l(k))
δ

(xk) − g(l(k))(y))

≥ η|I|(f (ˆx) − f (y)) + η|J|(ε + δ) − η

g(l(k))(y).

(cid:88)

k∈J

By application of inequality (17) and simple regrouping of terms, we ﬁnish the proof.

(cid:3)

During this section we set y = x∗. In this case d(x∗) − d(x0) ≤ Θ2
at ˆx for each separate function g(l):

0. Further we are going to derive bound on g(l)

Lemma 3 For ˆx produced by Algorithm 1 the following holds

g(l)(ˆx) ≤ ε + 2δ.

Proof. By convexity and deﬁnition of a δ-approximation

g(l)(ˆx) ≤

1
|I|

(cid:88)

k∈I

g(l)(xk) ≤

1
|I|

(cid:88)

(g(l)

δ (xk) + δ).

k∈I

We ﬁnish the proof by deﬁnition of ”productive” steps and a set I.

Before obtaining ﬁnal bounds on f we prove some technical fact:

Lemma 4 For any y ∈ Q: (cid:107)y − x0(cid:107)2 ≤ 2(d(y) − d(x0)).

(18)

(cid:3)

Proof. Follows from strongly convexity of d with respect to norm (cid:107) · (cid:107) and the fact that x0 is a minimum of d. (cid:3)

Now we derive bounds on sums of our stochastic noise function with high probability.

Lemma 5 Deﬁne event E such that the following inequalities holds

N −1
(cid:88)

γk(x∗) <

k=0

2Θ0
M

· (cid:112)2N ε2 log(1/σ).

Then Pr[E] ≥ 1 − σ.

Proof. Deﬁne ﬁltration of σ-algebras Fk = σ
sequence adapted to Fk.

(cid:16)

{ξi, ξi

(l)}i≤k

(cid:17)

. Notice that γk(x∗) is a martingale-diﬀerence

Let us derive bound on the sum. Notice that by Holder inequality and Lemma 4

|γk(x∗)| ≤ 2ηM (cid:107)x∗ − xk(cid:107) ≤

4εΘ0
M

,

Primal-Dual Stochastic Mirror Descent for MDPs

Then by Azuma-Hoeﬀding inequality

Pr

(cid:34)N −1
(cid:88)

k=0

(cid:35)

γk(x∗) ≥ t1

≤ exp

(cid:18)

−t2
1
2N · (16ε2Θ2

0)/(M 2)

(cid:19)

;

By setting t1 = 4Θ0M −1 · (cid:112)2N ε2 log(1/σ) we ﬁnish the proof.
One could apply Lemma 5 to inequalities in Lemmas 2 and 3:

(cid:3)

Corollary 1 Under event E deﬁned in Lemma 5, the following inequalities holds for all l ∈ [m]

η|I|(f (ˆx) − f (x∗)) < η|I|ε −

g(l)(ˆx) < ε + 2δ.

ε2N
2M 2 + Θ2

0 +

4Θ0

(cid:112)2N ε2 log(1/σ)

M

;

Proof. The second inequality follows directly from Lemma 5. The ﬁrst one uses deﬁnition η = ε/M 2, inequalities
(cid:3)
g(l))(x∗) ≤ 0 and, ﬁnally, Lemma 5.

Now we are going to prove last technical result before the statement of the main theorem of this subsection:

Lemma 6 If σ < 1 and

then

N ≥ N0 =

280 · Θ2

0M 2 log(1/σ)

ε2

ε2N
2M 2 − Θ2

0 −

√

2N ε2

(cid:32)

4Θ0

(cid:112)log(1/σ)

(cid:33)

M

≥ 0.

Proof. Fix variable t = 2N ε2. Then we have quadratic inequality of type at2 −(b1 +b2)t−c ≥ 0. By exact formula
for quadrativ equation and Taylor approximation we know that inequality holds if t > (b1 +b2)/a+c/(b1 +b2). By
numeric inequality 1/(b1 +b2) ≤ 1/b1 for nonnegative b1,b2, we have that enough to choose t ≥ b1/a+b2/a+c/b1.
Using our choice of N and numeric inequality 2a2 + 2b2 ≥ (a + b)2

√

2N ε2 ≥

√

280 · Θ0M (cid:112)log(1/σ) ≥ 16Θ0

(cid:112)log(3/σ) · M +

Θ2
0M
(cid:112)log(1/σ)

.

2Θ0

Here we use that log(1/σ) ≥ 1 and 16 + 1/2 ≤

√

280.

(cid:3)

Finally, we are ready to state theorem that describe convergence properties of Algorithm 1.

Proof of Theorem 1. To guarantee that f (ˆx)−f (x∗) ≤ ε with probability at least 1−σ, we use the ﬁrst inequality
(cid:3)
in Corollary 1 and Lemma 6. To guarantee satisfaction of constraints, we simply use Corollary 1.

A.2 Proof of Theorem 2

Recall our estimate of a dual variables

ˆλl =

1
|I|

(cid:88)

k∈J

I{l = l(k)}.

Lemma 7 Suppose ˆx is an output of Algorithm 1 and ˆλ is deﬁned as in (5) and Θ

2
0 ≥ supy∈Q(d(y) − d(x0)).

Then the following holds

η|I|∆(ˆx, ˆλ) ≤

+

η2M 2
2
N −1
(cid:88)

k=0

N + Θ

γk(x0) + Θ0

2
0 − |J|ηε
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 ·

√

N −1
(cid:88)

k=0

∆k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

,

(19)

Daniil Tiapkin, Alexander Gasnikov

where ∆k is deﬁned as follows

∆k =


η


η


(cid:17)

(cid:16) ˆ∇kf − ∇kf
(cid:16) ˆ∇kg(l(k)) − ∇kg(l(k))(cid:17)

,

k ∈ I

, k ∈ J.

Proof. We start from Lemma 2. Here we move all terms consist of y to the right-hand side and minimize over y

η|I|f (ˆx) ≤

η2M 2
2

(cid:40)

N − |J|ηε

+ min
y∈Q

d(y) − d(x0) +

N −1
(cid:88)

k=0

γk(y) + η|I|f (y) + η

(cid:41)

g(l(k))(y)

.

(cid:88)

k∈J

Notice that by deﬁnition of ˆλ we have

(cid:40)

η|I|φ(ˆλ) = min
y∈Q

η|I|f (y) + η

(cid:41)

g(l(k))

.

(cid:88)

k∈J

Thus, we have to upper bound d(y) − d(x0) and (cid:80)N −1
The ﬁrst upper bound is trivial: d(y) − d(x0) ≤ Θ0

k=0 γk(y) without dependence on y to obtain required result.
2

2

by deﬁnition of Θ0

.

To analyse the second term, we use the deﬁnition of γk in terms of ∆k and Holder inequality

N −1
(cid:88)

k=0

γk(y) =

N −1
(cid:88)

(cid:104)∆k, y − x0 + x0 − xk(cid:105) ≤ (cid:107)y − x0(cid:107)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N −1
(cid:88)

k=0

∆k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

+

N −1
(cid:88)

k=0

γk(x0).

The last step is to apply Lemma 4 and deﬁnition of Θ

2
0 to obtain uniform bound on (cid:107)y − x0(cid:107).

(cid:3)

Our next goal is to derive bound on the right-hand side of (19). It is possible using concentration of measure
techniques as in Lemma 5.

Lemma 8 Deﬁne event E (cid:48) such that the following inequalities holds

N −1
(cid:88)

γk(x0) <

N −1
(cid:88)

k=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=0

∆k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

<

· (cid:112)2N ε2 log(2/σ)

2Θ0
M
(cid:112)2κ(E∗) + (cid:112)4 log (2/σ)
M

√

2N ε2,

where ∆k is deﬁned in 7 and κ(E∗) is a constant of Nemirovski’s inequality (Boucheron et al., 2013) for the dual
space. Then Pr[E (cid:48)] ≥ 1 − σ.

Remark. If E has a ﬁnite dimension d, then we always have κ(E∗) ≤ d. Additionally, if E is endowed with (cid:96)p
norm, then E∗ is endowed with (cid:96)q norm, where 1/p + 1/q = 1, and there is a more precise bound, according to
(D¨umbgen et al., 2010)

κ(E∗) ≤ K

(cid:18) p

p − 1

(cid:19)

, d

=

2

p −1, p ∈ [1,2]
d
d1− 2

p , p ∈ (2, + ∞]

(cid:40)

In particular, if E has (cid:96)2 norm, κ(E∗) = 1. For p ∈ [2, + ∞] this bound is tight, however, in the case p ∈ [1,2]
and d ≥ 3 it could be improved to, for instance, a logarithmic bound κ(E∗) ≤ 2e log(d) − e, that could be useful
in the case of (cid:96)1-norm and an entropy prox-function. Proof. The case of ﬁrst inequality is identical to Lemma 5
by rescaling σ to σ/2.

To ensure the last inequality, we apply bounded diﬀerence inequality (Boucheron et al., 2013). This follows by
observing that Z = (cid:107) (cid:80)N −1
k=0 ∆k(cid:107)∗ satisﬁes bounded diﬀerence condition with a constant 4 · εM −1 that is greater
than 2 · (cid:107)∆k(cid:107)∗ a.s.. Thus

Pr[Z − EZ > t2] ≤ exp

(cid:18)

−t2
2
2 · N ε2 · 4M −2

(cid:19)

.

Primal-Dual Stochastic Mirror Descent for MDPs

√

Take t2 = 2M −1
Nemirovski’s inequality

2N ε2 · (cid:112)log(4/σ) and the last we have to do is to bound expectation of Z. Here we apply

(cid:32)

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N −1
(cid:88)

k=0

∆k

(cid:33)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

≤ E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N −1
(cid:88)

k=0


 ≤ κ(E∗)

∆k

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∗

N −1
(cid:88)

k=0

(cid:104)

E

(cid:107)∆k(cid:107)2
∗

(cid:105)

≤ κ(E∗)

4N ε2
M 2 .

By application of the union bound we ﬁnish the proof.

(cid:3)

Corollary 2 Under event E (cid:48) deﬁned in Lemma 8, the following inequalities holds for all l ∈ [m]

η|I|∆(ˆx, ˆλ) < η|I|ε −

g(l)(ˆx) < ε + 2δ.

ε2N
2M 2 + Θ

2
0 +

√

2N ε2

(cid:32)

2Θ0(4(cid:112)log(2/σ) + (cid:112)2κ(E∗))
M

(cid:33)

;

Proof. Inequalities on g(l)(ˆx) is equivalent to the same in 1. Inequality on ∆(ˆx, ˆλ) follows from a combination of
(cid:3)
Lemma 7 and Lemma 8.

Now we are ready to state a technical lemma that is similar to Lemma 6.

Lemma 9 If σ ≤ 1/2 and

then

N ≥ N (cid:48)

0 =

128Θ

2
0M 2(17 log(2/σ) + 2κ(E∗))

ε2

ε2N
2M 2 − Θ

2
0 −

√

2N ε2

(cid:32)

2Θ0(4(cid:112)log(2/σ) + (cid:112)2κ(E∗))
M

(cid:33)

≥ 0.

Proof. Using the same reasoning about solving quadratic inequality in terms of t =
suﬃcient to show that

√

2N ε2 as in Lemma 6 it is

√

2N ε2 ≥ 4

(cid:16)

2Θ0M (4(cid:112)log(2/σ) + (cid:112)2κ(E∗))

(cid:17)

+

2
0M
Θ
2Θ0(4(cid:112)log(2/σ) + (cid:112)2κ(E∗))

.

Since σ ≤ 1/2 ⇒ log(2/σ) ≥ 1 and (cid:112)2κ(E∗) ≥ 1, we have that it is suﬃcient to show that

√

2N ε2 ≥ 4

(cid:16)
2Θ0M ((4 + 1/20)(cid:112)log(2/σ) + (cid:112)2κ(E∗))

(cid:17)

.

By numeric inequality 2a2 + 2b2 ≥ (a + b)2 and (4 + 1/20)2 ≤ 17, it is satisﬁed with our choice of N ≥ N (cid:48)

0. (cid:3)

Now we are ready to prove our main result.

Proof of Theorem 2. The inequality on g(ˆx) is satisﬁes by Corollary 2. We have to satisfy inequality on duality
gap. The inequality on duality gap follows directly from Corollary 2 and Lemma 9 since N ≥ N (cid:48)
0. Notice that a
(cid:3)
probability 1 − σ appears from the event E (cid:48) deﬁned in Lemma 8.

A.3 Proof of Proposition 1

At ﬁrst, we prove one technical lemma.

samples
i=1, Xi ∈ {1, . . . ,d} drawn from categorical distribution with a parameter s ∈ ∆d. Deﬁne an empirical

Lemma 10 (Estimation of parameters of categorical distribution) Suppose
{Xi}N
estimate ˆs = 1
N

i=1 eXi ∈ ∆d, where ej is an j-th standard basis vector.
Then, for any δ(cid:48),σ(cid:48) > 0, the inequality (cid:107)s − ˆs(cid:107)1 ≤ δ(cid:48) holds with probability at least 1 − σ(cid:48) if

that we have

(cid:80)N

N ≥

8d + 4 log(1/σ(cid:48))
δ(cid:48)2

.

Daniil Tiapkin, Alexander Gasnikov

Proof. First of all, notice that EeXi = s. Denote by ∆ a centred random variable ˆs − s = 1
i=1(eXi − s).
N
Then, by Nemirovski’s inequality and bound on the (cid:96)1 norm of elements of a simplex, we might estimate the
mean of the square of (cid:96)1 norm of this random variable: E(cid:107)∆(cid:107)2

1 ≤ 4d/N .

(cid:80)n

Let us deﬁne function f (X1, . . . ,XN ) = (cid:107)∆(cid:107)1. It might be checked that this function satisﬁes conditions of the
bounded diﬀerence inequality (Boucheron et al., 2013) with constant 2/N . Thus, for all t > 2(cid:112)d/N ≥ E(cid:107)∆(cid:107)1

Pr[(cid:107)∆(cid:107)1 > t] = Pr[(cid:107)∆(cid:107)1 − E(cid:107)∆(cid:107)1 > t − E(cid:107)∆(cid:107)1] ≤ exp

(cid:32)

−(t − 2(cid:112)d/N )2
2N −1

(cid:33)

.

Taking N ≥ (8d + 4 log(1/σ(cid:48))) · (δ(cid:48))−2 and t = δ(cid:48), we ﬁnish the proof.

(cid:3)

Using this simple lemma, we can easily obtain required sample complexity for the preprocessing even in parallel
sampling setting. Remember that we are going to use Algorithm 1, hence, we want to approximate each constraint
function.

Proof of Proposition 1. Notice that each P(a) ∈ ∆|S| is a parameters of a categorical distribution we sampling
from, and the estimator from the previous lemma could be applied.

By Holder’s inequality and the deﬁnition of the search space for h we have that |(cid:104)P(a) − (cid:101)P(a), h(cid:105)| ≤ 2M · (cid:107)P(a) −
(cid:101)P(a)(cid:107)1. Hence, to make it less than δ(cid:48), we required to make (cid:96)1 norm of diﬀerence less than δ(cid:48)/(2M ). To make
all conditions work simultaneously, it is enough to make the probability in the terms of the previous lemma less
than σ(cid:48)/Atot and apply the union bound over all Atot conditions. The last observation that ﬁnishes the proof is
(cid:3)
that sampling for an estimation of each (cid:101)Pa could be done separately and independent.

A.4 Proof of Proposition 2

Proof. Conditions of the proposition give us the following guarantees in terms of the duality gap and constraint
satisfaction with needed probability ≥ 1 − σ

¯vε − min
¯v,h

(cid:16)

¯v + (µε)(cid:62) (cid:16)

−¯v1 − (ˆI − P)h + r

(cid:17)(cid:17)

≤ εf ,

¯vε1 + (ˆI − P)hε − r ≥ −εg1.

We can rewrite the ﬁrst condition in more suitable terms of λε

∀¯v, ∀h ∈ X :εf ≥ (¯vε − ¯v) + ¯v · (λε)(cid:62)1 + (λε)(cid:62)([I − Pπ]h − rπ).

(20)

(21)

Now we have all required instruments and we can bound the expectation of an average value of our policy

vπ = (νπ)(cid:62)rπ = (νπ − λε)(cid:62)([Pπ − I]hε + rπ) + (λε)(cid:62)([Pπ − I]hε + rπ).

Here we used the stationary of our policy: (νπ)(cid:62)(Pπ − I) = 0. For simplicity, we remind that µε, λε ≥ 0, and,
hence, (cid:104)µε, 1(cid:105) = (cid:104)λε, 1(cid:105) = (cid:107)λε(cid:107)1.

Firstly, bound the second term by 21

To bound the ﬁrst term we will use Lemma 7 from (Jin and Sidford, 2020):

(λε)(cid:62)([Pπ − I]hε + rπ) ≥ ¯vε − ¯v(1 − (cid:107)λε(cid:107)1) − εf .

(cid:107)(I − Pπ + 1(νπ)(cid:62))−1(cid:107)∞ ≤ M.

Also we have for all µ ≥ 0 by primal feasibility: µ(cid:62)([ˆI − P]h∗ − r + ¯v∗1) ≥ 0. We can combine it with our
condition (21) for arbitrary h and ¯v = ¯v∗, using the fact that ¯vε ≥ ¯v∗ − εg

εf + εg ≥ ¯v∗ − ¯v∗ + (µε)(cid:62)([ˆI − P]h − r + ¯v∗1)

≥ (µε)(cid:62)([ˆI − P]h − r + ¯v∗1) − (µε)(cid:62)([ˆI − P]h∗ − r + ¯v∗1)
= (µε)(cid:62)([ˆI − P](h − h∗)) = (λε)(cid:62)[I − Pπ](h − h∗).

Primal-Dual Stochastic Mirror Descent for MDPs

Hence, we have

2M (cid:107)(λε)(cid:62)[I − Pπ](cid:107)1 = max
h∈B|S|
2M
= max
h∈B|S|
2M

(λε)(cid:62)[I − Pπ]h

(λε)(cid:62)[I − Pπ](h − h∗) + (λε)(cid:62)[I − Pπ]h∗

≤ εf + εg + M (cid:107)(λε)(cid:62)[I − Pπ](cid:107)1.

Combining with the fact that νπ(I − Pπ) = 0, we obtain(cid:107)(νπ − λε)(cid:62)[I − Pπ](cid:107)1 ≤ εf +εg
M . By almost the same
argument as in (Jin and Sidford, 2020), we also have |(cid:104)νπ − λε, rπ(cid:105)| ≤ εf + εg. Hence, there is a bound on the
required ﬁrst term:

(νπ − λε)(cid:62)([Pπ − I]hε + rπ) ≥ −2M ·

(εf + εg)
M

− (εf + εg).

Overall, we obtain the required inequality by taking ¯v = 0 and by feasibility of the pair (¯vε + εg, hε)

¯vπ ≥ ¯vε − ¯v(1 − (cid:107)λε(cid:107)1) − εf − 3(εf + εg) ≥ ¯v∗ − 4(εf + εg).

(cid:3)

