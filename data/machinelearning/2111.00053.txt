1
2
0
2

v
o
N
7
1

]
E
N
.
s
c
[

2
v
3
5
0
0
0
.
1
1
1
2
:
v
i
X
r
a

Symbolic Regression via Neural-Guided Genetic
Programming Population Seeding

T. Nathan Mundhenk
mundhenk1@llnl.gov

Mikel Landajuela
landajuelala1@llnl.gov

Ruben Glatt
glatt1@llnl.gov

Claudio P. Santiago
prata@llnl.gov

Daniel M. Faissol
faissol1@llnl.gov

Brenden K. Petersen
bp@llnl.gov

Computational Engineering Division
Lawrence Livermore National Laboratory
Livermore, CA 94550

Abstract

Symbolic regression is the process of identifying mathematical expressions that ﬁt
observed output from a black-box process. It is a discrete optimization problem
generally believed to be NP-hard. Prior approaches to solving the problem include
neural-guided search (e.g. using reinforcement learning) and genetic program-
ming. In this work, we introduce a hybrid neural-guided/genetic programming
approach to symbolic regression and other combinatorial optimization problems.
We propose a neural-guided component used to seed the starting population of a
random restart genetic programming component, gradually learning better starting
populations. On a number of common benchmark tasks to recover underlying
expressions from a dataset, our method recovers 65% more expressions than
a recently published top-performing model using the same experimental setup.
We demonstrate that running many genetic programming generations without
interdependence on the neural-guided component performs better for symbolic
regression than alternative formulations where the two are more strongly coupled.
Finally, we introduce a new set of 22 symbolic regression benchmark problems
with increased difﬁculty over existing benchmarks. Source code is provided at
www.github.com/brendenpetersen/deep-symbolic-optimization.

1

Introduction

Symbolic regression involves searching the space of mathematical expressions to ﬁt a dataset using
equations which are potentially easier to interpret than, for example, neural networks. A key difference
compared to polynomial or neural network-based regression is that we seek to illuminate the true
underlying process that generated the data. Thus, the process of symbolic regression is analogous
to how a physicist may derive a set of fundamental expressions to describe a natural process. For
example, Tycho Brahe meticulously mapped the motion of planets through the sky, but it was Johannes
Kepler who later created the expressions for the laws that described their motion. Given a dataset
(X, y), where each point has inputs Xi ∈ Rn and response yi ∈ R, symbolic regression aims to
identify a function f : Rn → R that best ﬁts the dataset, where the functional form of f is a short
closed-form mathematical expression.

The space of mathematical expressions is structurally discrete (in functional form) but continuous
in parameter space (e.g. ﬂoating-point constants). The search space grows exponentially with the
length of the expression, rendering symbolic regression a challenging machine learning problem. It
is generally believed to be NP-hard [Lu et al., 2016]; however, no formal proof exists. Given the

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

 
 
 
 
 
 
Figure 1: Method overview. A parameterized sequence generator (e.g. RNN) generates N samples,
i.e. expressions for symbolic regression. These samples are used as the starting population for a GP
component. GP then runs S generations. The top M samples from GP are extracted, combined with
the N samples from the RNN, and used to train the RNN, e.g. using VPG, RSPG, or PQT. Since GP
is stateless, it runs in a random restart-like fashion each time the RNN is sampled.

large, combinatorial search space, traditional approaches to symbolic regression commonly utilize
evolutionary algorithms, especially genetic programming (GP) [Koza, 1992, Schmidt and Lipson,
2009, Fortin et al., 2012, Bäck et al., 2018]. GP-based symbolic regression operates by maintaining a
population of mathematical expression “individuals” that are “evolved” using evolutionary operations
like selection, crossover, and mutation. A ﬁtness function acts to improve the population over many
generations.

Neural networks can also be leveraged for symbolic regression [Kusner et al., 2017, Sahoo et al.,
2018, Udrescu and Tegmark, 2020]. Recently, it has been proposed to solve symbolic regression
using neural-guided search [Petersen et al., 2021, Landajuela et al., 2021b]. This approach works by
using a recurrent neural network (RNN) to stochastically emit batches of expressions as a sequence
of mathematical operators or “tokens.” Expressions are evaluated for goodness of ﬁt and a training
strategy is used to improve the quality of generated formulas. Petersen et al. [2021] propose a
risk-seeking policy gradient strategy, which ﬁlters out the lesser performers and returns an “elite
set” of expressions to the RNN each step of training. Constraints can be employed to prune the
search space, preventing nonsensical or extraneous statements in generated expressions. For instance,
inversions (e.g. log (ex)) and nested trigonometric functions (e.g. sin(1 + cos(x)) can be avoided.
The performance attained using neural-guided search outperformed GP-based approaches, including
commonly used commercial software.

Genetic programming and neural-guided search are mechanistically dissimilar, yet both have proven
to be effective solutions to symbolic regression. Might it be possible to combine the two approaches
to leverage each of their strengths? The population of individual solutions in GP is structurally the
same as a batch of samples emitted by the RNN in neural-guided search. In principle, they can
formally be interfaced by allowing GP individuals to ﬂow to the RNN training and vice versa.

For reinforcement learning-based training objectives like the one used in Petersen et al. [2021], this
way of coupling creates an out-of-distribution problem on the RNN side. Standard off-policy methods,
like importance sampling [Glynn and Iglehart, 1989], do not apply here since the GP distribution is
intractable. Note, however, that the mechanistic interpretation of the policy gradient as maximizing
the log likelihood of individuals proportional to their ﬁtness is still valid. On the other hand, for
training alternatives based on maximum likelihood estimation over a selected subset of samples, like
the cross-entropy method [De Boer et al., 2005] or priority queue training [Abolaﬁa et al., 2018],
there are no assumptions about samples being “on-policy,” and thus they can be applied seamlessly.
In this work, we explore three different ways of training the RNN: two reinforcement learning-based
training methods (without off-policy correction) and the priority queue training method.

2

2 Related Work

Deep learning for symbolic regression. Several recent approaches leverage deep learning for
symbolic regression. AI Feynman [Udrescu and Tegmark, 2020] proposes a problem-simpliﬁcation
tool for symbolic regression. They use neural networks to identify simplifying properties in a
dataset (e.g. multiplicative separability, translational symmetry), which they exploit to recursively
deﬁne simpliﬁed sub-problems that can then be tackled using any symbolic regression algorithm. In
GrammarVAE, Kusner et al. [2017] develop a generative model for discrete objects that adhere to a
pre-speciﬁed grammar, then optimize them in latent space. They demonstrate this can be used for
symbolic regression; however, the method struggles to exactly recover expressions, and the generated
expressions are not always syntactically valid. Sahoo et al. [2018] develop a symbolic regression
framework using neural networks whose activation functions are symbolic operators. While this
approach enables an end-to-end differentiable system, back-propagation through activation functions
like division or logarithm requires the authors to make several simpliﬁcations to the search space,
ultimately precluding learning certain simple classes of expressions like

x or sin(x/y).

√

Genetic programming/policy gradient hybrids. The overall idea of combining GP with gradient-
based methods in general predates the deep learning era [Igel and Kreutz, 1999, Topchy and Punch,
2001, Zhang and Smart, 2004, Montastruc et al., 2004, Wierstra et al., 2008] and dissatisfaction
with deep reinforcement learning (DRL) is not entirely new [Kurenkov, 2018]. Recently, several
approaches have combined GP and reinforcement learning (RL). In these works, RL is used to alter or
augment the GP process, e.g. by adjusting the probabilities of performing different genetic operations.
Alternatively, GP is used to augment the creation or operation of a neural network [Such et al., 2018,
Chang et al., 2018, Gangwani and Peng, 2018, Chen et al., 2019, Stanley et al., 2019, Real et al.,
2019, Miikkulainen1 et al., 2019, Sehgal et al., 2019, Tian et al., 2020, Chen et al., 2020a,b].

Sample population interchanging methods. We identify a broad class of existing methods that can
be characterized by using samples from a sequence generator and interchanging those samples with
a GP population. The sequence generator may be any discrete distribution or generative process
that creates a sequence of tokens, e.g. a recurrent neural network or transformer. Samples from the
sequence generator are then treated as interchangeable with a population generated by GP. Thus,
sequence generator samples can be inserted into the population of GP, and GP samples can be used
to update the sequence generator. In some cases the sequence generator may not have learnable
parameters.

Within this class of methods, Pourchot and Sigaud [2019] and Khadka and Tumer [2018] solve
physical continuous control problems like the inverted pendulum or moon rover with a fungible
neural network controller. Their approaches are similar to neuro-evolution [Stanley and Miikkulainen,
2002, Floreano et al., 2008, Lüders et al., 2017, Risi and Togelius, 2017], as both techniques are
based on using deep deterministic policy gradients (DDPG) [Lillicrap et al., 2016] and utilize a
synchronous shared pool of actors between GP and RL components. Khadka and Tumer [2018]
introduced the ERL method that pools RL and GP actor samples into a cyclic replay buffer. This
single buffer is drawn to either seed a GP step or help update the actor-critic model. Both GP and RL
work synchronously, and the population is interchangeable. Pourchot and Sigaud [2019] introduce
CEM-RL which solves the same family of problems in a similar way. The most notable difference is
that the shared population is not fed directly into a GP, but rather it is used to update a distribution
from which a population is drawn.

Most closely related to our work, Ahn et al. [2020] develop genetic expert-guided learning (GEGL).
In GEGL, samples are generated using a stochastic RNN-based sequence generator and added to a
maximum reward priority queue (MRPQ). A GP component then applies mutation and/or crossover
on each item in the MRPQ. Notably, only one generation of evolutionary operators is applied to each
sample in the MRPQ. As a consequence, there is no notion of a selection operator. In contrast, we
demonstrate that the selection operator and performing multiple generations of evolution is crucial
to maximize the beneﬁts of GP. After performing one generation of GP, the resulting population is
then added to a second MRPQ. Samples from the union of the GP-based MRPQ and RNN-based
MRPQ are then used to train the RNN. In contrast, we ﬁnd that priority queues are not necessary and
may prevent sufﬁcient exploration. As with ERL and CEM-RL, GEGL’s evolutionary and training
steps are one-to-one, meaning that each training step is followed by exactly one GP generation.
Additionally, GP pulls from a persistent memory of sample populations, which it shares with the
neural network component. This strong coupling makes the evolutionary component much more
interdependent on the neural network.

3

3 Methods

Our overall algorithm comprises two main components: (1) a sequence generator (with learnable
parameters) and (2) a genetic programming component. In the sections below, we ﬁrst discuss pre-
liminaries, then describe each component individually, and ﬁnally describe how the two components
interact.

Preliminaries. Any mathematical expression f can be represented by an algebraic expression tree,
where internal nodes are operators (e.g. ×, sin) and terminal nodes are input variables (e.g. x) or
constants [Petersen et al., 2021]. We refer to τ = [τ1, . . . , τ|τ |] as the pre-order traversal of such
an expression tree. Notably, there is a one-to-one correspondence between an expression tree and
its pre-order traversal (see Petersen et al. [2021] for details). Each τi is an operator, input variable,
or constant selected from a library of possible tokens, e.g. [+, −, ×, ÷, sin, cos, exp, log, x]. A pre-
order traversal τ can be instantiated into a corresponding mathematical expression f and evaluated
based on its ﬁtness to a dataset. Speciﬁcally, we consider the metric normalized root-mean-square
error (NRMSE), deﬁned as follows. Given a pre-order traversal τ and a dataset of (X, y) pairs of
i=1 (yi − f (Xi))2, where
size N , with X ∈ Rn and y ∈ R, we deﬁne NRMSE(τ ) = 1
σy
f : Rn → R is the instantiated mathematical expression represented by τ , and σy is the standard
deviation of y. Finally, the ﬁtness or reward function is given by R(τ ) = 1/(1 + NRMSE(τ )). We
use the terms “ﬁtness” and “reward” interchangeably, with the former being more typical in the
context of GP and the latter more common in the context of RL.

(cid:80)N

(cid:113)

1
N

Sequence generator. The sequence generator is a parameterized distribution over mathematical
expressions, p(τ |θ). Typically, a model is chosen such that the likelihood of an expression is tractable
with respect to parameters θ, allowing backpropagation of a differentiable loss function. A common
choice of model is an autoregressive RNN, in which the likelihood of the ith token (denoted τi)
is conditionally independent given the preceding tokens τ1, . . . , τ(i−1). That is, p(τi|τj(cid:54)=i, θ) =
p(τi|τj<i, θ). We follow the sequence generator used in Petersen et al. [2021]: an autoregressive
RNN comprising a single-layer LSTM with 32 hidden nodes. For notational simplicity, hereafter we
assume the use of an RNN as the sequence generator.

The sequence generator is typically trained using reinforcement learning or a related approach. Under
this perspective, the sequence generator can be viewed as a reinforcement learning policy, which we
seek to optimize by sampling a batch of N expressions T , evaluating each expression under a reward
function R(τ ), and performing gradient descent on a loss function. In this work, we explore three
methods for training the RNN:

• Vanilla policy gradient (VPG): Using the well-known REINFORCE rule [Williams,
1992], training is performed over the batch T , yielding the loss function: L(θ) =
τ ∈T (R(τ ) − b)∇θ log p(τ |θ), where b is a baseline term or control variate, e.g. an

1
|T |
exponentially-weighted moving average (EWMA) of rewards.

(cid:80)

• Risk-seeking policy gradient (RSPG): Petersen et al. [2021] develop an alternative to VPG
τ ∈T (R(τ ) −
, where ε is a hyperparameter controlling the degree of risk-

intended to optimize for best-case instead of average reward: L(θ) = 1
ε|T |
˜Rε)∇θ log p(τ |θ)1R(τ )> ˜Rε
seeking and ˜R(cid:15) is the empirical (1 − ε) quantile of the rewards of T .

(cid:80)

• Priority queue training (PQT): Abolaﬁa et al. [2018] introduce a non-RL approach also
intended to focus on optimizing best-case performance. Samples from each batch are added
to a persistent maximum reward priority queue (MRPQ), and training is performed over sam-
ples in the MRPQ using a supervised learning objective: L(θ) = 1
τ ∈MRPQ ∇θ log p(τ |θ),
k
where k is the size of the MRPQ.

(cid:80)

Note that our method is agnostic to the training procedure; additional procedures may be considered,
for example the cross-entropy method [De Boer et al., 2005], which is closely related to PQT. In our
formulation, we include, as is common, an additional term in the loss function proportional to the
entropy of the distribution at each position along the sequence [Bello et al., 2016, Abolaﬁa et al.,
2018, Petersen et al., 2021, Landajuela et al., 2021b].

Genetic programming. Genetic programming (GP) begins with a set (“population”) of expression
trees (“individuals”), denoted TGP. In standard GP, these individuals are randomly generated; how-
ever, we discuss in the subsequent section how this differs in our algorithm. A single iteration or

4

Algorithm 1 Neural-guided genetic programming population seeding
input batch/population size N ; number of GP generations per RNN training update S; constraints
Ω; crossover probability Pc; mutation probability Pm; tournament size k; GP sample selection
size M (must be ≤ N )

input Loss function L(θ) for training the RNN, including corresponding hyperparameters (e.g.

EWMA coefﬁcient for VPG, risk factor for RSPG, priority queue size for PQT)

7:
8:

output Best sample τ (cid:63)
1: Initialize reward function R : τ → R
2: Initialize RNN distribution over expressions, p(·|θ, Ω)
3: Initialize GP operation, GP(Pc, Pm, k, R) = Γ : T → T
4: τ (cid:63) ← null
5: while total samples below budget do
TRNN ← {τ (i) ∼ p(·|θ, Ω)}N
6:
T (0)
GP ← TRNN
for s = 1, . . . , S do
(cid:16)
T (s−1)
GP
(cid:16)
T (0)
GP ∪ T (1)
Ttrain ← Top-M
Ttrain ← Ttrain ∪ TRNN
11:
12: R ← {R(τ ) ∀ τ ∈ Ttrain}
θ ← θ + ∇θL(θ)
13:
if max R > R(τ (cid:63)) then τ (cid:63) ← τ (arg max R)
14:
15: return τ (cid:63)

GP ∪ · · · ∪ T (S)

T (s)
GP ← Γ

10:

i=1

9:

GP

(cid:17)

(cid:17)

(cid:46) Sample batch of size N
(cid:46) Seed GP starting population with RNN batch

(cid:46) Apply GP operations

(cid:46) Filter best M samples from GP
(cid:46) Join RNN and best GP samples
(cid:46) Compute rewards
(cid:46) Train the RNN (e.g. using PQT)
(cid:46) Update best sample

“generation” of a GP algorithm consists of several “evolutionary operations” that directly alter the
current population. A mutation operator introduces random variations to an individual, for example
by replacing one subtree with another randomly generated subtree. A crossover operator involves
exchanging content between two individuals, for example by swapping one random subtree of one
individual with another random subtree of another individual. A selection operator is used to select
which individuals from the current population persist onto the next population. A common selection
operator is tournament selection [Koza, 1992], in which a set of k candidate individuals are randomly
sampled from the population, and the individual with the highest ﬁtness is selected. In each generation
of GP, each individual has a probability of undergoing mutation and a probability of undergoing
crossover; selection is performed until the new generation’s population has the same size as the
current generation’s population. GP does not have an explicit objective function as does the sequence
generator, but it does tend to move the population toward higher ﬁtness [Such et al., 2018].

For the GP component, we begin with a standard formulation from DEAP [Fortin et al., 2012] and
introduce several key changes:

1. Typically only uniform mutation is used. Instead, we select among uniform, node replace-

ment, insertion, or shrink mutation with equal probability.

2. We incorporate constraints from Petersen et al. [2021] (for example, constraining nested
trigonometric functions, e.g. sin(1 + cos(x))). If a genetic operation would result in an
individual that violates any constraint, that genetic operation is instead reverted. That is, the
child expression instead becomes a copy of the parent expression. This procedure ensures
that all individuals satisfy all constraints for all generations.

3. The sample population is never initialized randomly. Rather, the initial population is always

seeded by samples from the RNN. (See next section for details.)

Neural-guided genetic programming population seeding. While both a fully RNN-based ap-
proach (i.e. deep symbolic regression [Petersen et al., 2021]) and a fully GP-based approach (i.e.
GP-based symbolic regression [Koza, 1992]) involve generating “batches” or “populations” of expres-
sions, they arrive at their expressions very differently. Namely, we observe that the RNN can generate
expressions “from scratch” given only parameters θ; in contrast, GP requires an extant population
to operate on. More speciﬁcally, given parameters θ, the RNN can be used to sample a batch of
expressions, TRNN; in contrast, generation i of GP begins with a population of expressions T (i)
GP and
application of one generation of GP produces a new, augmented population T (i+1)

.

GP

5

Table 1: Recovery rate of several algorithms on the Nguyen benchmark problem set across 100
independent training runs. Results of our algorithm are obtained using PQT; slightly lower recovery
rates were obtained using VPG and RSPG training (see Table 3 for comparisons).
Recovery rate (%)

Benchmark

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12

Expression
x3 + x2 + x
x4 + x3 + x2 + x
x5 + x4 + x3 + x2 + x
x6 + x5 + x4 + x3 + x2 + x
sin(x2) cos(x) − 1
sin(x) + sin(x + x2)
log(x + 1) + log(x2 + 1)
√
x
sin(x) + sin(y2)
2 sin(x) cos(y)
xy

x4 − x3 + 1

2 y2 − y

Ours DSR PQT VPG GP

Eureqa

100
100
100
100
100
100
97
100
100
100
100
0

100
100
100
100
72
100
35
96
100
100
100
0

100
99
86
93
73
98
41
21
100
91
100
0

96
47
4
1
5
100
3
5
100
99
100
0

100
97
100
100
45
91
0
5
100
76
7
0

100
100
95
70
73
100
85
0
100
64
100
0

Average

91.4

83.6

75.2

46.7

60.1

73.9

Thus, we propose that a natural point at which to interface neural-guided search and GP is the starting
population of GP, T (0)
GP . Speciﬁcally, we propose to use the most recent batch of samples from
the RNN directly as the starting population for GP: T (0)
GP = TRNN. From there, we can perform S
generations of GP, resulting in a ﬁnal GP population, T (S)
GP . Finally, we can sub-select an “elite
set” of the top-performing GP samples, and include these samples in the gradient update for neural
guided search (e.g. VPG, RSPG, PQT). This process constitutes one step of our algorithm, and is
repeated until a maximum number of total expression evaluations is reached. Thus, GP acts as an
inner optimization loop, within an outer gradient-based loop of neural-guided search.

Note that the GP process is restarted for each new batch of samples from the RNN; thus, the process
is similar to GP with random restarts; the key difference for our proposed algorithm is that the GP
starting population upon each restart changes as the RNN learns via an objective function. Thus,
from the perspective of GP, the RNN provides increasingly better starting populations. Empirically,
we found that this hybrid approach also allows for larger learning rates relative to pure neural-guided
search.

We hypothesize that this hybrid approach of neural-guided search and GP will leverage the strengths
of each individual approach. Whereas GP is stateless and there is no learning step, neural-guided
search is stateful (via RNN parameters θ) and learns from data via a well-deﬁned, differentiable loss
function. Whereas neural-guided search is known to easily get stuck in local optima, GP can produce
large variations in the population, resulting in “fresh” samples that are essentially out-of-distribution
from the RNN-induced distribution. We discuss this further in Discussion.

We provide pseudocode for our algorithm in Algorithm 1 and illustrate it in Figure 1. While this work
focuses on the task of symbolic regression, our approach applies to any symbolic optimization task
with a black-box reward function.

4 Results

We used two popular benchmark problem sets to compare our technique to other methods: Nguyen
[Uy et al., 2014] and the R rationals [Krawiec and Pawlak, 2013]. A symbolic regression benchmark
problem is deﬁned by a ground truth expression, a set of sampled points from the expression, and a set
of allowable tokens. Additionally, we introduce a new benchmark problem set with this work, which
we call Livermore. The impetus for introducing a new benchmark problem set was that our algorithm
achieves nearly perfect scores on Nguyen, so we designed a benchmark problem set with a large
range of problem difﬁculty. Finally, we include variants Nguyen-12(cid:63), R-1(cid:63), R-2(cid:63), and R-3(cid:63), which
use the same expression and set of tokens, but increase the domain over which sampled points are
taken. All benchmarks are described in Appendix Table 9. Hyperparameters are shown in Appendix

6

Table 2: Recovery rate of several algorithms on the Nguyen, R, and Livermore benchmark problem
sets across 25 independent training runs. 95% conﬁdence intervals are obtained from the standard
error between mean recovery on 37 unique benchmark problems. Recovery rates on individual
benchmark problems are shown in Appendix Table 5.

Recovery rate (%)

All

Nguyen

R

Livermore

Ours
GEGL [Ahn et al., 2020]
Random Restart GP (i.e. GP only)
DSR (i.e. RNN only) [Petersen et al., 2021]

74.92
64.11
63.57
45.19

92.33
86.00
88.67
83.58

33.33
33.33
2.67
0.00

71.09
56.36
58.18
30.41

95% conﬁdence interval

±1.54 ±1.76 ±2.81

±1.32

Table 11. For all algorithms, we tuned hyperparameters using Nguyen-7 and R-3(cid:63). None of the
Livermore benchmarks were used for hyperparameter tuning.

We follow the experimental procedure of Petersen et al. [2021] unless otherwise noted. For all
benchmark problems, we run each algorithm multiple times using a different random number seed.
Experiments were conducted on 36 core, 2.1 GHz, Intel Xeon E5-2695 workstations. We run each
benchmark on a single core, which take an average of 4.4 minutes per run on the Nguyen benchmarks.
Runtimes on individual Nguyen benchmarks are shown in Appendix Table 6.

Our primary empirical performance metric is “recovery rate,” deﬁned as the fraction of independent
training runs in which an algorithm discovers an expression that is symbolically equivalent to the
ground truth expression within a maximum of 2 million candidate expressions. The ground truth
expression is used to determine whether the best found candidate was correctly recovered. Table 1
shows recovery rates on each of the Nguyen benchmark problems compared with DSR, PQT, VPG,
GP, and Eureqa [Schmidt and Lipson, 2009]. We note that DSR stands as the prior top performer on
this set [Petersen et al., 2021]. As we can see, the recovery rate for our algorithm is 9.3% higher than
the previous leader, DSR.

We next compare against a recent method called GEGL [Ahn et al., 2020] using the Nguyen, R, and
Livermore benchmark problem sets. GEGL is a hybrid RNN/GP algorithm originally demonstrated
for designing small molecules that ﬁt a set of desired parameters. Many details of the open-source
implementation are tied to the particular problem of molecular design (e.g. the mutation operators
are speciﬁc to molecules), so we adapted their method to symbolic regression (i.e. using the same
genetic operators as our method). In addition to GEGL, we compare against a “GP only” and “RNN
only” version of our method, which are the most critical ablations. “GP only” does not use an RNN,
and each GP population is seeded using randomly generated expressions; this is essentially GP with
random restarts. “RNN only” does not use GP, essentially reducing to DSR [Petersen et al., 2021].
For these experiments, we use the variations Nguyen-12(cid:63), R-1(cid:63), R-2(cid:63), and R-3(cid:63). Table 2 shows that
our method ties with GEGL on the R benchmark set and outperforms the other three methods overall.
It also outperforms all other approaches on the other benchmark sets. It is interesting to note how
well random restart GP does on its own, outperforming DSR and on par with GEGL. Results for
individual benchmarks are shown in Appendix Table 5.

Results on additional benchmark problem sets are found in Appendix Tables 7 and 8. Finally, in
Appendix Table 13 we see an additional improvement in recovery rates by repeating our experiments
for the Nguyen, R, and Livermore benchmarks using the soft length prior and hierarchical entropy
regularizer from Landajuela et al. [2021a], which were developed concurrently with this work.

5 Discussion

Intuition. We provide a two-sided perspective as to why we believe our hybrid method outperforms
its constituent components so strongly. First, from the RNN perspective, we believe GP helps by
providing "fresh" new samples that help escape local optima. NN-based discrete distributions often
concentrate their probability mass on a relatively small portion of the search space, resulting in
premature convergence to locally optimal solutions. In contrast, the evolutionary operations of GP
generate new individuals that may fall well outside the RNN’s concentrated region of the search
space. Second, from the perspective of GP, the RNN helps by learning good starting populations.
With a good starting set, GP can easily identify excellent solutions within a few generations. Further,

7

Figure 2: Recovery rate of our method from 25 independent runs on the Nguyen, R, and Livermore
benchmark problem sets as a function of number of GP generations per RNN training step. Error
bars represent the 95% conﬁdence interval. The zero generations case reduces to DSR.

Table 3: Recovery rates for various ablations of our algorithm (sorted by overall performance) from
25 independent runs on the Nguyen, R, and Livermore benchmark problem sets.

Recovery rate (%)

All

Nguyen

R

Livermore

Trainer = PQT
No RL samples to train RNN (all off-policy)
Trainer = VPG
Entropy weight = 0
Trainer = RSPG
(A): Uniform mutation only
(B): No inv/trig constraint
(A) and (B)
No GP samples to train RNN (all on-policy)
(C): Trainer learning rate = 0
Random Restart (GP only)
(D): No RL seeds to GP
(C) and (D)
DSR (RL only) [Petersen et al., 2021]

74.92
74.81
74.27
73.95
73.95
72.65
71.35
68.11
66.27
65.95
63.57
63.24
63.14
45.19

92.33
93.00
93.67
92.00
92.67
91.67
91.33
91.33
89.00
90.67
88.67
89.00
88.67
83.58

33.33
29.33
28.00
30.67
39.33
25.33
32.00
21.33
24.00
17.33
2.67
2.67
2.67
0.00

71.09
71.09
70.00
70.00
69.82
68.73
65.82
61.82
59.64
59.09
58.18
57.45
57.45
30.41

95% Conﬁdence Interval

±1.54 ±1.76 ±2.81

±1.32

the RNN may act to keep GP from moving too quickly in a suboptimal direction and may have an
effect analogous to using a trust region. Each time GP restarts, it falls back to the current probability
mass of the training RNN.

GP generations per RNN training step. An important hyperparameter in our method is S, the
number of GP generations to perform per RNN training step. We hypothesized that S be proportional
to expression length. That is, the larger an expression is, the more evolutionary operations should be
required to transform any arbitrary seed individual into the target expression. Our initial estimate was
that 25 GP steps would be needed per each RNN training step. Figure 2 shows a post-hoc analysis
of how performance varies depending on how many GP steps we do between each RNN training
step. The optimal number of steps is between 10 and 25. We note that the hard limit set by our
implementation is 30 tokens. However, each GP step can make many changes at a time, so we expect
the number of needed GP steps to be less than the maximum possible hamming distance. As such,
we can see the similarity between the optimal number of GP steps and the length of expressions.

Ablations. We ran several post-hoc ablations to determine the contribution of our various design
choices to our method’s performance. Table 3 shows the recovery rate for several ablations. We ﬁrst
note that the choice of RNN training procedure (i.e. VPG, RSPG, or PQT) does not make a large
difference; while PQT outperforms VPG and RSPG, the difference falls within the 95% conﬁdence
interval. Interestingly, removing the entropy regularizer by setting the weight to zero made a small
difference in performance. Other works have found the entropy regularizer to be extremely important
[Abolaﬁa et al., 2018, Petersen et al., 2021, Landajuela et al., 2021a]. We hypothesize that entropy
is less important in our hybrid approach because the GP-produced samples provide the RNN with

8

Table 4: Minimum normalized mean-square error (NMSE) from 25 independent runs on three
challenging benchmark problems.

Minimum NMSE

Ours
1.41 × 10−4
3.55 × 10−5
2.83 × 10−4

DSR
2.01 × 10−2
2.74 × 10−3
5.31 × 10−3

Nguyen-12
R-1
R-2

sufﬁcient exploration. Using different types of mutation in the GP component with equal probability
slightly improves performance; we hypothesize that this increases sample diversity. Including the
constraints proposed by Petersen et al. [2021] also improves performance.

We performed several ablations designed to determine the relative importance of the RNN versus
GP components. Completely removing the RNN leads to a standalone GP algorithm running with
random restarts. This actually performs quite well compared to GP without random restarts. This
is not surprising since random-restarts has been shown to help GP by others [Houck et al., 1996,
Ghannadian et al., 1996, Fukunaga, 2010]. Similarly, we can set the RNN learning rate to zero. This
ablation essentially reduces to a version of GP with random restarts, in which the starting population
is based on a randomly initialized RNN; this performs similarly to GP with random restarts.

We also explore a fully “off-policy” ablation, in which only GP samples are considered in the training
step (RNN samples are excluded). Notably, this does not signiﬁcantly affect performance, as the
GP samples (having undergone many generations of GP reﬁnement) are generally superior to the
RNN samples. Similarly, we explore a fully “on-policy” version, in which only RNN samples are
considered in the training step (GP samples are excluded). In this case, there is no feedback between
GP and the RNN; the only role of GP is to provide strong candidates that may be selected as the best
ﬁnal expression. This ablation results in a large drop in performance, suggesting that it is important
that the RNN be trained on samples from the GP.

Finally, in Appendix Table 12 we show additional results when various hyperparameters from Petersen
et al. [2021] or Fortin et al. [2012] are restored to their original values.

Limitations. The primary limiting factor to our approach is that there are still some expressions which
cannot be completely recovered. While it may be a matter of ﬁnding the right set of hyperparameters,
we have not as yet recovered benchmarks Nguyen-12, R-1, or R-2. While we have not yet solved
three benchmarks, Table 4 shows that we still signiﬁcantly outperform DSR in terms of NMSE. Other
challenging benchmarks include R-3, which we recover about 1 in 25 runs, and Livermore-7 and
Livermore-8, which we recover about once in every few hundred runs.

6 Conclusion and Future Work

We introduce a hybrid approach to solve symbolic regression and other symbolic optimization
problems using neural-guided search to generate starting populations for a generic programming
component running in a random restart-like fashion. The results are state-of-the-art on a series of
benchmark symbolic regression tasks.

Our future research will focus on methods to mitigate or resolve the off-policy issue of policy
gradient-based training methods, by either correcting the weights on the gradient computation or
considering alternative optimization objectives. We will also further examine why Nguyen-12, R-1,
and R-2 seem intractable and see if we can come to a solution. We also plan on doing in-depth
analysis of our algorithm with noisy data.

Acknowledgements

We thank the LLNL Hypothesis Testing team, Kate Svyatets, and Miles the cat for their helpful
comments, feedback, and support. This work was performed under the auspices of the U.S. Depart-
ment of Energy by Lawrence Livermore National Laboratory under contract DE-AC52-07NA27344.
Lawrence Livermore National Security, LLC. LLNL-CONF-820015 and was supported by the LLNL-
LDRD Program under Projects 19-DR-003 and 21-SI-001. The authors have no competing interests
to report.

9

References

Daniel A Abolaﬁa, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. Neural program

synthesis with priority queue training. arXiv preprint arXiv:1801.03526, 2018.

Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization
with genetic exploration. In Proceeding of the Conference on Neural Information Processing
Systems (NeurIPS), 2020.

Thomas Bäck, David B Fogel, and Zbigniew Michalewicz. Evolutionary Computation 1: Basic

Algorithms and Operators. CRC press, 2018.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial

optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Simyung Chang, John Yang, Jaeseok Choi, and Nojun Kwak. Genetic-gated networks for deep
In Proceeding of the Conference on Neural Information Processing

reinforcement learning.
Systems (NeurIPS), 2018.

Diqi Chen, Yizhou Wang, and Wen Gao. Combining a gradient-based method and an evolution

strategy for multi-objective reinforcement learning. Applied Intelligence, 50, 2020a.

Qiong Chen, Mengxing Huang, Qiannan Xu, Hao Wang, and Jinghui Wang. Reinforcement learning-
based genetic algorithm in optimizing multidimensional data discretization scheme. Mathematical
Problems in Engineering, 2020b.

Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang, Lisen Mu, and Xinggang
Wang. Renas: Reinforced evolutionary neural architecture search. In Proceeding of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the

cross-entropy method. Annals of operations research, 134(1):19–67, 2005.

Dario Floreano, Peter Dürr, and Claudio Mattiussi. Neuroevolution: from architectures to learning.

Evolutionary Intelligence, 1:47–62, 2008.

Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, and
Christian Gagné. DEAP: Evolutionary algorithms made easy. Journal of Machine Learning
Research, 13:2171–2175, jul 2012.

Alex S. Fukunaga. Restart scheduling for genetic algorithms. In Proceeding of the 5th International

Conference on Parallel Problem Solving from Nature, 2010.

Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. In Proceeding of the

International Conference on Learning Representations (ICLR), 2018.

Farzad Ghannadian, Cecil Alford, and Ron Shonkwiler. Application of random restart to genetic

algorithms. Intelligent Systems, 95:81–102, 1996.

Peter W Glynn and Donald L Iglehart. Importance sampling for stochastic simulations. Management

science, 35(11):1367–1392, 1989.

Christopher R. Houck, Jeffrey A. Joines, and Michael G. Kay. Comparison of genetic algorithms,
random restart, and two-opt switching for solving large location-allocation problems. Computers
and Operations Research, 23:587–596, 1996.

Christian Igel and Martin Kreutz. Using ﬁtness distributions to improve the evolution of learning
structures. In Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No.
99TH8406), volume 3, pages 1902–1909. IEEE, 1999.

Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. Bayesian symbolic regression. arXiv

preprint arXiv:1910.08892, 2019.

Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning.
In Proceeding of the Conference on Neural Information Processing Systems (NeurIPS), 2018.

10

John R Koza. Genetic Programming: On the Programming of Computers by Means of Natural

Selection, volume 1. MIT press, 1992.

Krzysztof Krawiec and Tomasz Pawlak. Approximating geometric crossover by semantic backpropa-
gation. In Proceeding of the ﬁfteenth annual conference on Genetic and evolutionary computation
(GECCO), 2013.

Andrey Kurenkov. Reinforcement learning’s foundational ﬂaw. The Gradient, 2018.

Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder.
In Proceedings of the 34th Proceeding of the International Conference on Machine Learning
(ICML)-Volume 70, pages 1945–1954. JMLR. org, 2017.

Mikel Landajuela, Brenden K Petersen, Soo K Kim, Claudio P Santiago, Ruben Glatt, T Nathan
Mundhenk, Jacob F Pettit, and Daniel M Faissol. Improving exploration in policy gradient search:
Application to symbolic optimization. arXiv preprint arXiv:2107.09158, 2021a.

Mikel Landajuela, Brenden K Petersen, Sookyung Kim, Claudio P Santiago, Ruben Glatt, Nathan
Mundhenk, Jacob F Pettit, and Daniel Faissol. Discovering symbolic policies with deep rein-
forcement learning. In International Conference on Machine Learning, pages 5979–5989. PMLR,
2021b.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
In
Proceeding of the International Conference on Learning Representations (ICLR), 2016.

Qiang Lu, Jun Ren, and Zhiguang Wang. Using genetic programming with prior formula knowledge

to solve symbolic regression problem. Computational intelligence and neuroscience, 2016.

Benno Lüders, Mikkel Schläger, Aleksandra Korach, and Sebastian Risi. Continual and one-shot
learning through neural networks with dynamic external memory. In In European Conference on
the Applications of Evolutionary Computation, pages 886–901, 2017.

Risto Miikkulainen1, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, OlivierFrancon, Bala
Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural
networks. Artiﬁcial Intelligence in the Age of Neural Networks and Brain Computing, 2019.

Ludovic Montastruc, Catherine Azzaro-Pantel, Luc Pibouleau, and Serge Domenech. Use of ge-
netic algorithms and gradient based optimization techniques for calcium phosphate precipitation.
Chemical Engineering and Processing: Process Intensiﬁcation, 43(10):1289–1298, 2004. ISSN
0255-2701. doi: https://doi.org/10.1016/j.cep.2003.12.002. URL https://www.sciencedirect.
com/science/article/pii/S0255270103002733.

Brenden K Petersen, Mikel Landajuela, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and
Joanne T Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-
seeking policy gradients. Proceeding of the International Conference on Learning Representations
(ICLR), 2021.

Aloïs Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based methods for
policy search. In Proceeding of the International Conference on Learning Representations (ICLR),
2019.

Esteban Real, Chen Liang, David R. So, and Quoc V. Leg. Automl-zero: Evolving machine learning
algorithms from scratch. In Proceeding of the International Conference on Machine Learning
(ICML), 2019.

Sebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges.

IEEE Transactions on Computational Intelligence and AI in Games, 9:25–41, 2017.

Subham S Sahoo, Christoph H Lampert, and Georg Martius. Learning equations for extrapolation
and control. In Proceedings of the 35th International Conference on Machine Learning (ICML),
2018.

Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. science,

324(5923):81–85, 2009.

11

Adarsh Sehgal, Hung La, Sushil Louis, and Hai Nguyen. Deep reinforcement learning using genetic
algorithm for parameter optimization. In Proceeding of the IEEE International Conference on
Robotic Computing (ICRC), 2019.

Kenneth O. Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topolo-

gies. Evolutionary computation, 10:99–127, 2002.

Kenneth O. Stanley, Jeff Clune, Joel Lehman1, and Risto Miikkulainen. Designing neural networks

through neuroevolution. Nature Machine Intelligence, 1, 2019.

Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff
Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep
neural networks for reinforcement learning. In arXiv:1712.06567, 2018.

Yuan Tian, Qin Wang1, Zhiwu Huang, Wen Li, Dengxin Dai, Minghao Yang, Jun Wang, , and Olga
Fink. Off-policy reinforcement learning for efﬁcient and effective gan architecture search. In
Proceeding of the European Computer Vision Conference (ECCV), 2020.

Alexander Topchy and William Punch. Faster genetic programming based on local gradient search of
numeric leaf values. In Proceeding of the third annual conference on Genetic and evolutionary
computation (GECCO), 2001.

Leonardo Trujillo, Luis Muñoz, Edgar Galván-López, and Sara Silva. neat genetic programming:

Controlling bloat naturally. Information Sciences, 333:21–43, 2016.

Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic

regression. Science Advances, 6(16), 2020.

Nguyen Quang Uy, Nguyen Xuan Hoai, Michael O’Neill, R.I. McKay, and Edgar Galvan-Lopez.
Semantically-based crossover in genetic programming: Application to realvalued symbolic regres-
sion. Genetic Programming and Evolvable Machines, 12:91–119, 2014.

Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In

Proceeding of the IEEE Congress on Evolutionary Computation, 2008.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

Mengjie Zhang and Will Smart. Genetic programming with gradient descent search for multiclass
object classiﬁcation. In Maarten Keijzer, Una-May O’Reilly, Simon Lucas, Ernesto Costa, and
Terence Soule, editors, Genetic Programming, pages 399–408, Berlin, Heidelberg, 2004. Springer
Berlin Heidelberg. ISBN 978-3-540-24650-3.

12

Appendix

Table 5: Recovery rates for individual benchmark problems.

Recovery rate (%)

DSR

Random Restart GP GEGL Ours

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12(cid:63)

Nguyen average
R-1(cid:63)
R-2(cid:63)
R-3(cid:63)

R average

Livermore-1
Livermore-2
Livermore-3
Livermore-4
Livermore-5
Livermore-6
Livermore-7
Livermore-8
Livermore-9
Livermore-10
Livermore-11
Livermore-12
Livermore-13
Livermore-14
Livermore-15
Livermore-16
Livermore-17
Livermore-18
Livermore-19
Livermore-20
Livermore-21
Livermore-22

100
100
100
100
72
100
35
96
100
100
100
0

100
100
100
100
100
100
64
100
100
100
100
0

100
100
100
100
92
100
48
100
100
92
100
0

100
100
100
100
100
100
96
100
100
100
100
12

83.58

88.67

86.00

92.33

0
0
0

4
0
4

0
0
100

4
4
92

0.00

2.67

33.33

33.33

3
87
66
76
0
97
0
0
0
0
17
61
55
0
0
4
0
0
100
98
2
3

100
92
100
100
0
4
0
0
0
0
100
100
96
96
92
92
64
32
100
100
12
0

100
44
100
100
0
64
0
0
12
0
92
100
84
100
96
12
4
0
100
100
64
68

100
100
100
100
4
88
0
0
24
24
100
100
100
100
100
92
68
56
100
100
24
84

Livermore average

30.41

All average

45.19

58.18

63.57

56.36

64.11

71.09

74.92

13

Table 6: Single-core runtimes of our algorithm on the Nguyen benchmark problem set.

Benchmark

Runtime (sec)

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12(cid:63)

Average

27.05
59.79
151.06
268.88
501.65
43.96
752.32
123.21
31.17
103.72
66.50
1057.11

265.54

Table 7: Comparison of mean root-mean-square error (RMSE) for our method to literature-reported
values from DSR [Petersen et al., 2021] and Bayesian symbolic regression (BSR) [Jin et al., 2019] on
the Jin benchmark problem set. Note that these benchmarks include ﬂoating-point constant values
that are optimized as part of the reward function computation, as in Petersen et al. [2021]. Table 10
shows the formulas for these benchmarks.

Ours

DSR

BSR Recovered by Ours

Mean RMSE

Jin-1
Jin-2
Jin-3
Jin-4
Jin-5
Jin-6

Average

0
0
0
0
0
0

0

0.46
0
0.00052
0.00014
0
2.23

0.45

2.04
6.84
0.21
0.16
0.66
4.63

2.42

Yes
Yes
Yes
Yes
Yes
Yes

Table 8: Comparison of median root-mean-square error (RMSE) for our method to literature-reported
values from DSR [Petersen et al., 2021] and neat genetic programming (Neat-GP) [Trujillo et al.,
2016] on the Neat benchmark problem set. Note that Neat-6, Neat-7, and Neat-8 are not fully
recoverable given the function set prescribed by the benchmark. Table 10 shows the formulas for
these benchmarks.

Ours

DSR

Neat-GP Recovered by Ours

Median RMSE

Neat-1
Neat-2
Neat-3
Neat-4
Neat-5
Neat-6
Neat-7
Neat-8
Neat-9

0
0
0
0
0
6.1 × 10−6
1.0028
0.0228
0

0
0
0.0041
0.0189
0
0.2378
1.0606
0.1076
0.1511

0.0779
0.0576
0.0065
0.0253
0.0023
0.2855
1.0541
0.1498
0.1202

Average

0.1139

0.1756

0.1977

Yes
Yes
Yes
Yes
Yes
—
—
—
Yes

14

Table 9: Benchmark symbolic regression problem speciﬁcations. Input variables are denoted by x
and/or y. U (a, b, c) denotes c random points uniformly sampled between a and b for each input
variable; training and test datasets use different random seeds. E (a, b, c) denotes c points evenly
spaced between a and b for each input variable; training and test datasets use the same points. All
benchmark problems use the following set of allowable tokens: {+, −, ×, ÷, sin, cos, exp, log, x, y}
(y is excluded for single-dimensional datasets).

Name

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12
Nguyen-12(cid:63)

R-1
R-2
R-3
R-1(cid:63)
R-2(cid:63)
R-3(cid:63)

Livermore-1
Livermore-2
Livermore-3
Livermore-4
Livermore-5
Livermore-6
Livermore-7
Livermore-8
Livermore-9
Livermore-10
Livermore-11
Livermore-12
Livermore-13
Livermore-14
Livermore-15
Livermore-16
Livermore-17
Livermore-18
Livermore-19
Livermore-20
Livermore-21
Livermore-22

Expression
x3 + x2 + x
x4 + x3 + x2 + x
x5 + x4 + x3 + x2 + x
x6 + x5 + x4 + x3 + x2 + x
sin(x2) cos(x) − 1
sin(x) + sin(x + x2)
log(x + 1) + log(x2 + 1)
√

x

sin(x) + sin(y2)
2 sin(x) cos(y)
xy
x4 − x3 + 1
x4 − x3 + 1

2 y2 − y
2 y2 − y

(x+1)3
x2−x+1
x5−3x3+1
x2+1

x6+x5
x4+x3+x2+x+1
(x+1)3
x2−x+1
x5−3x3+1
x2+1

x6+x5
x4+x3+x2+x+1
1

Dataset

U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (0, 2, 20)
U (0, 4, 20)
U (0, 1, 20)
U (0, 1, 20)
U (0, 1, 20)
U (0, 1, 20)
U (0, 10, 20)

E (−1, 1, 20)
E (−1, 1, 20)
E (−1, 1, 20)

E (−10, 10, 20)
E (−10, 10, 20)
E (−10, 10, 20)

3 + x + sin (cid:0)x2(cid:1)
U (−10, 10, 1000)
sin (cid:0)x2(cid:1) cos (x) − 2
U (−1, 1, 20)
sin (cid:0)x3(cid:1) cos (cid:0)x2(cid:1) − 1
U (−1, 1, 20)
log(x + 1) + log(x2 + 1) + log(x)
U (0, 2, 20)
x4 − x3 + x2 − y
U (0, 1, 20)
4x4 + 3x3 + 2x2 + x
U (−1, 1, 20)
U (−1, 1, 20)
sinh(x)
cosh(x)
U (−1, 1, 20)
x9 + x8 + x7 + x6 + x5 + x4 + x3 + x2 + x U (−1, 1, 20)
6 sin (x) cos (y)
x2x2
x+y
x5
y3
x 1
x3 + x2 + x + sin (x) + sin (cid:0)x2(cid:1)
x 1
x 2
4 sin (x) cos (y)
sin (cid:0)x2(cid:1) cos (x) − 5
x5 + x4 + x2 + x
exp (cid:0)−x2(cid:1)
x8 + x7 + x6 + x5 + x4 + x3 + x2 + x
exp (cid:0)−0.5x2(cid:1)

U (0, 1, 20)
U (−1, 1, 50)
U (−1, 1, 50)
U (0, 4, 20)
U (−1, 1, 20)
U (0, 4, 20)
U (0, 4, 20)
U (0, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)
U (−1, 1, 20)

5

5

3

15

Table 10: Benchmark symbolic regression problems that include unknown constants. Input variables
are denoted by x and/or y. U(a, b, c) denotes c random points uniformly sampled between a and
b for each input variable; training and test datasets use different random seeds. E(a, b, c) denotes
c points evenly spaced between a and b for each input variable; training and test datasets use the
same points (except Neat-6, which uses E(1, 120, 120) as test data, and the Jin tests, which use
U(−3, 3, 30) as test data). To simplify notation, libraries are deﬁned relative to a “base” library
L0 = {+, −, ×, ÷, sin, cos, exp, log, x}. Placeholder operands are denoted by •, e.g. •2 corresponds
to the square operator.

Name

Jin-1
Jin-2
Jin-3
Jin-4
Jin-5
Jin-6

Neat-1
Neat-2
Neat-3
Neat-4
Neat-5
Neat-6
Neat-7

Neat-8
Neat-9

Expression

2.5x4 − 1.3x3 + 0.5y2 − 1.7y
8.0x2 + 8.0y3 − 15.0
0.2x3 + 0.5y3 − 1.2y − 0.5x
1.5 exp(x) + 5.0 cos(y)
6.0 sin(x) cos(y)
1.35xy + 5.5 sin((x − 1.0)(y − 1.0))

x4 + x3 + x2 + x
x5 + x4 + x3 + x2 + x
sin(x2) cos(x) − 1
log(x + 1) + log(x2 + 1)
2 sin(x) cos(y)

(cid:80)x

1
k

k=1
2 − 2.1 cos(9.8x) sin(1.3y)
e−(x−1)2
1.2+(y−2.5)2
1+x−4 + 1
1

1+y−4

Dataset

U(−3, 3, 100)
U(−3, 3, 100)
U(−3, 3, 100)
U(−3, 3, 100)
U(−3, 3, 100)
U(−3, 3, 100)

U(−1, 1, 20)
U(−1, 1, 20)
U(−1, 1, 20)
U(0, 2, 20)
U(−1, 1, 100)
E(1, 50, 50)

Library
L0 − {log} ∪ {•2, •3, y, const}
L0 − {log} ∪ {•2, •3, y, const}
L0 − {log} ∪ {•2, •3, y, const}
L0 − {log} ∪ {•2, •3, y, const}
L0 − {log} ∪ {•2, •3, y, const}
L0 − {log} ∪ {•2, •3, y, const}

L0 ∪ {1}
L0 ∪ {1}
L0 ∪ {1}
L0 ∪ {1}
L0 ∪ {y}
{+, ×, ÷, •−1, −•,

√

•, x}
√

E(−50, 50, 105) L0 ∪ {tan, tanh, •2, •3,
U(0.3, 4, 100)
E(−5, 5, 21)

•, y}
{+, −, ×, ÷, exp, e−•, •2, x, y}
L0 ∪ {y}

16

Table 11: Hyperparameter values for all experiments, unless otherwise noted. If an applicable
hyperparameter value differs from one of the baseline methods, it is noted.

Hyperparameter

Symbol

Value

Comment

Shared Parameters

Batch/population size

Minimum expression length
Maximum expression length
Maximum expressions
Maximum constants

Reward/ﬁtness function

RNN Parameters

Optimizer
RNN cell type
RNN cell layers
RNN cell size
Training method
PQT queue size
Sample selection size
Learning rate
Entropy weight

GP Parameters

Generations per iteration
Crossover operator
Crossover probability
Mutation operator
Mutation probability
Selection operator
Tournament size
Mutate tree maximum

N

–
–
–
–

R

–
–
–
–
–
–
M
α
–

S
–
Pc
–
Pm
–
k
–

500

4
30
2,000,000
∞

Petersen et al. [2021]: 1000
Fortin et al. [2012]: 300
–
–
–
Petersen et al. [2021]: 3
(only used for Jin benchmarks)

Inverse NRMSE –

Adam
LSTM
1
32
PQT
10
1
0.0025
0.005

–
–
–
–
–
–
–
Petersen et al. [2021]: 0.0005
–

25
One Point
0.5
Multiple
0.5
Tournament
5
3

–
–
–
Fortin et al. [2012]: Uniform
Fortin et al. [2012]: 0.1
–
Fortin et al. [2012]: 3
Fortin et al. [2012]: 2

Table 12: Additional ablations when using original hyperparameter values from Petersen et al. [2021]
and/or Fortin et al. [2012] rather than the values in Table 11.

Recovery rate (%)

All

Nguyen

R

Livermore

Mutation probability Pm = 0.1
Baseline (Table 11 values)
Tournament size k = 3
Batch size N = 1000
RNN learning rate α = 0.0005

75.24
74.92
73.62
72.11
71.03

90.33
92.33
92.00
93.33
91.67

32.00
33.33
29.33
25.33
22.67

72.91
71.09
69.64
66.91
66.36

95% conﬁdence interval

±1.54 ±1.76 ±2.81

±1.32

17

Table 13: Recovery rates when using the soft length prior (SLP) and hierarchical entropy regularizer
(HER) introduced in Landajuela et al. [2021a]2021a], and increasing maximum length from 30 to
100. These results are post-hoc, as Landajuela et al. [2021a] was performed concurrently.

Recovery rate (%)
Ours Ours + SLP/HER

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12(cid:63)

Nguyen average
R-1(cid:63)
R-2(cid:63)
R-3(cid:63)

R average

Livermore-1
Livermore-2
Livermore-3
Livermore-4
Livermore-5
Livermore-6
Livermore-7
Livermore-8
Livermore-9
Livermore-10
Livermore-11
Livermore-12
Livermore-13
Livermore-14
Livermore-15
Livermore-16
Livermore-17
Livermore-18
Livermore-19
Livermore-20
Livermore-21
Livermore-22

100
100
100
100
100
100
96
100
100
100
100
12

92.33

4.00
4.00
92.00

33.33

100
100
100
100
4
88
0
0
24
24
100
100
100
100
100
92
68
56
100
100
24
84

100
100
100
100
100
100
100
100
100
100
100
4

92.00

100.00
100.00
96.00

98.67

100
100
100
100
40
100
4
0
88
8
100
100
100
100
100
100
36
48
100
100
88
92

Livermore average

71.09

All average

74.92

77.45

83.89

18

