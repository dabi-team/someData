Empirical Study of Transformers for Source Code

Nadezhda Chirkova
HSE University
Moscow, Russia
nchirkova@hse.ru

Sergey Troshin
HSE University
Moscow, Russia
stroshin@hse.ru

1
2
0
2

n
u
J

4
2

]

G
L
.
s
c
[

2
v
7
8
9
7
0
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT
Initially developed for natural language processing (NLP), Trans-
formers are now widely used for source code processing, due to
the format similarity between source code and text. In contrast to
natural language, source code is strictly structured, i.e., it follows
the syntax of the programming language. Several recent works
develop Transformer modifications for capturing syntactic infor-
mation in source code. The drawback of these works is that they
do not compare to each other and consider different tasks. In this
work, we conduct a thorough empirical study of the capabilities of
Transformers to utilize syntactic information in different tasks. We
consider three tasks (code completion, function naming and bug
fixing) and re-implement different syntax-capturing modifications
in a unified framework. We show that Transformers are able to
make meaningful predictions based purely on syntactic information
and underline the best practices of taking the syntactic information
into account for improving the performance of the model.

CCS CONCEPTS
• Computing methodologies → Neural networks.

KEYWORDS
neural networks, transformer, variable misuse detection, function
naming, code completion

ACM Reference Format:
Nadezhda Chirkova and Sergey Troshin. 2021. Empirical Study of Trans-
formers for Source Code. In Proceedings of the 29th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering (ESEC/FSE ’21), August 23–28, 2021, Athens, Greece. ACM,
New York, NY, USA, 19 pages. https://doi.org/10.1145/3468264.3468611

1 INTRODUCTION
Transformer [37] is currently a state-of-the-art architecture in a lot
of source code processing tasks, including code completion [20],
code translation [21, 32], and bug fixing [14]. Particularly, Trans-
formers were shown to outperform classic deep learning architec-
tures, e.g., recurrent (RNNs), recursive and convolutional neural
networks in the mentioned tasks. These architectures focus on
local connections between input elements, while Transformer pro-
cesses all input elements in parallel and focuses on capturing global

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.3468611

dependencies in data, producing more meaningful code representa-
tions [14]. This parallelism also speeds up training and prediction.
Transformer is often applied to source code directly, treating
code as a sequence of language keywords, punctuation marks, and
identifiers. In this case, a neural network mostly relies on identifiers,
e. g. variable names, to make predictions [1, 22]. High-quality vari-
able names can be a rich source of information about the semantics
of the code; however, this is only an indirect, secondary source of
information. The primary source of information of what the code
implements is its syntactic structure.

Transformer architecture relies on the self-attention mechanism
that is not aware of the order or structure of input elements and
treats the input as an unordered bag of elements. To account for the
particular structure of the input, additional mechanisms are usually
used, e.g. positional encoding for processing sequential structure.
In recent years, a line of research has developed mechanisms for
utilizing tree structure of code in Transformer [14, 20, 32]. However,
the most effective way of utilizing syntactic information in Trans-
former is still unclear for three reasons. First, the mechanisms were
developed concurrently, so they were not compared to each other by
their authors. Moreover, different works test the proposed mecha-
nisms on different code processing tasks, making it hard to align the
empirical results reported in the papers. Secondly, the mentioned
works used standard Transformer with positional encodings as a
baseline, while modern practice uses more advanced modifications
of Transformer, e.g., equipping it with relative attention [31]. As
a result, it is unclear whether using sophisticated mechanisms for
utilizing syntactic information is needed at all. Thirdly, most of the
works focus on utilizing tree structure in Transformer and do not
investigate the effect of processing other syntax components, e. g.
the syntactic units of the programming language.

In this work, we conduct an empirical study of using Transformer
for processing source code. Firstly, we would like to answer the
question, what is the best way of utilizing syntactic information
in Transformer, and to provide practical recommendations for the
use of Transformers in software engineering tasks. Secondly, we
aim at understanding whether Transformer is generally suitable
for capturing code syntax, to ground the future development of
Transformers for code. Our contributions are as follows:

• We re-implement several approaches for capturing syntactic
structure in Transformer and investigate their effectiveness
in three code processing tasks on two datasets. We underline
the importance of evaluating code processing models on
several different tasks and believe that our work will help to
establish standard benchmarks in neural code processing.
• We introduce an anonymized setting in which all user-defined
identifiers are replaced with placeholders, and show that
Transformer is capable of making meaningful predictions
based purely on syntactic information, in all three tasks. We

 
 
 
 
 
 
ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Figure 1: Illustration of mechanisms for processing AST structure in Transformer.

also show that using the proposed anonymization can im-
prove the quality of the model, either a single Transformer
or an ensemble of Transformers.

• We conduct an ablation study of different syntax-capturing
components in Transformer, underlining which ones are
essential for achieving high quality and analysing the results
obtained for the anonymized setting.

Our source code is available at https://github.com/bayesgroup/
code_transformers.

The rest of the work is organized as follows. In Section 2 we
review the existing approaches for utilizing syntactic information
in Transformer. In Section 3 we describe our methodology for the
empirical evaluation of Transformer capabilities to utilize syntactic
information. The following Sections 6–8 describe our empirical find-
ings. In Section 10, we give the review of the literature connected
to our research. Finally, Section 11 discusses threats to validity and
Section 12 concludes the work.

2 REVIEW OF TRANSFORMERS FOR SOURCE

CODE

Abstract syntax tree. A syntactic structure of code is usually
represented in the form of an abstract syntax tree (AST). Each node
of the tree contains a type, which represents the syntactic unit of the
programming language (e.g. "Assign", "Index", "NameLoad"), some
nodes also contain a value (e.g. "idx", "elem"). Values store user-
defined variable names, reserved language names, integers, strings
etc. An example AST for a code snippet is shown in Figure 1(b).

being passed to the Transformer blocks, the input sequence is firstly
mapped into a sequence of embeddings 𝑥1, . . . , 𝑥𝐿, 𝑥𝑖 ∈ R𝑑𝑚𝑜𝑑𝑒𝑙 .

A key ingredient of a Transformer block is a self-attention layer
that maps input sequence 𝑥1, . . . 𝑥𝐿, 𝑥𝑖 ∈ R𝑑𝑚𝑜𝑑𝑒𝑙 to a sequence of
the same length: 𝑧1, . . . , 𝑧𝐿, 𝑧𝑖 ∈ R𝑑𝑧 . Self-attention first computes
𝑗 = 𝑥 𝑗𝑊 𝐾 ,
key, query, and value vectors from each input vector: 𝑥𝑘
𝑥𝑞
𝑗 = 𝑥 𝑗𝑊 𝑉 . Each output 𝑧𝑖 is computed as a
𝑗 = 𝑥 𝑗𝑊 𝑄 and 𝑥 𝑣
weighted combination of inputs:

𝑧𝑖 =

∑︁

˜𝛼𝑖 𝑗 𝑥 𝑣
𝑗 ,

˜𝛼𝑖 𝑗 =

𝑗

exp(𝑎𝑖 𝑗 )
(cid:205)𝑗 exp(𝑎𝑖 𝑗 )

,

𝑎𝑖 𝑗 =

𝑇

𝑥𝑞
𝑖 𝑥𝑘
𝑗
√
𝑑𝑧

(1)

ℎ , 𝑊 𝑄

𝑖 , . . . , 𝑧𝐻

Attention weights ˜𝛼𝑖 𝑗 ⩾ 0, (cid:205)𝐿
𝛼𝑖 𝑗 = 1 are computed based
𝑗=1
on query-key similarities. Several attention layers (heads) are ap-
plied in parallel with different projection matrices 𝑊 𝑉
ℎ , 𝑊 𝐾
ℎ ,
ℎ = 1, . . . , 𝐻 . The outputs are concatenated and projected to ob-
𝑖 ]𝑊 𝑂 , 𝑊 𝑂 ∈ R𝐻𝑑𝑧 ×𝑑𝑚𝑜𝑑𝑒𝑙 . A Transformer
tain ˆ𝑥𝑖 = [𝑧1
block includes the described multi-head attention, a residual con-
nection, a layer normalization, and a position-wise fully-connected
layer. The overall Transformer architecture is composed by the
consequent stacking of the described blocks. When applying Trans-
former to generation tasks, future elements (𝑖 > 𝑗) are masked in
self-attention (Transformer decoder). Without this masking, the
stack of the layers is called Transformer encoder. In the sequence-
to-sequence task, when both encoder and decoder are used, the
attention from decoder to encoder is also incorporated into the
model.

2.1 Transformer architecture and
self-attention mechanism

We describe Transformer architecture for a task of mapping input
sequence 𝑐1, . . . 𝑐𝐿, 𝑐𝑖 ∈ {1, . . . , 𝑀 } (𝑀 is a vocabulary size) to a
sequence of 𝑑𝑚𝑜𝑑𝑒𝑙 -dimensional representations 𝑦1, . . . , 𝑦𝐿 that can
be used for making task-specific predictions in various tasks. Before

2.2 Passing ASTs to Transformers
For our study, we select two commonly used NLP approaches for uti-
lizing sequential structure and three approaches developed specifi-
cally for utilizing source code structure in Transformer.

Sequential positional encodings and embeddings. Transformers
were initially developed for NLP and therefore were augmented

elem=lst[idx]<latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit>AssignNameStoreelemSubscriptLoadNameLoadlstIndexNameLoadidx123456UDD46123456//1/2/2/1/2/2/2/2/1<latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit>/2/2/16AssignNameStoreSubscriptLoad...hemptyielemhemptyi...<latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit>...NameLoadIndexNameLoad...lsthemptyiidx<latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit>1234561IDDDDDDDDD2UIUDUDDUDDUDDD3UUDIDDDD4UUUUDUIUDUDD5UUUUDUUDID6UUUUUUDUUUUDUI<latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit>(a) Code:(b) Abstract syntax     tree (AST):(c) AST depth-ﬁrst traversal:(d) Tree positional encodings:(e) Tree relative attention:(f) GGNN Sandwich: Types of edges:•Parent (P)•Child (C)•Left (L)•Right (R)1234561P,LP2C,RL3CRP,LP4C,RL5CRP,L6C,R<latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit>1: 000 000 0002: 100 000 0003: 010 000 0004: 100 010 0005: 010 010 0006: 100 010 010TransGGNNGGNNTransINPUTOUTPUTstack-like enc.SSSSSS•Self (S)Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

with sequence-capturing mechanisms to account for sequential
input structure. As a result, the simplest way of applying Trans-
formers to AST is to traverse AST in some order, e.g., in depth-first
order (see Figure 1(c)), and use standard sequence-capturing mech-
anisms.

To account for the sequential nature of the input, standard Trans-
former is augmented with positional encodings or positional em-
beddings. Namely, the input embeddings 𝑥𝑖 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 are summed
up with positional representations 𝑝𝑖 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 : ˆ𝑥𝑖 = 𝑥𝑖 + 𝑝𝑖 . For
example, positional embeddings imply learning the embedding vec-
tor of each position 𝑖 ∈ 1 . . . 𝐿: 𝑝𝑖 = 𝑒𝑖, 𝑒𝑖 ∈ 𝑅𝑑𝑚𝑜𝑑𝑒𝑙 . Positional
encoding implies computing 𝑝𝑖 based on sine and cosine functions.
We include positional embeddings in our comparisons and add the
prefix “sequential” to the title of this mechanism. This approach
was used as a baseline in several of works [14, 32].

Sequential relative attention. Shaw et al. [31] proposed relative at-
tention for capturing the order of the input elements. They augment
self-attention with relative embeddings:

𝑧𝑖 =

∑︁

𝑗

˜𝛼𝑖 𝑗 (𝑥 𝑣

𝑗 + 𝑒𝑣

𝑖−𝑗 ),

˜𝛼𝑖 𝑗 =

exp(𝑎𝑖 𝑗 )
(cid:205)𝑗 exp(𝑎𝑖 𝑗 )

, 𝑎𝑖 𝑗 =

𝑥𝑞
𝑗 + 𝑒𝑘
𝑖 (𝑥𝑘
√
𝑑𝑧

𝑖−𝑗 )𝑇

,

𝑖−𝑗 , 𝑒𝑘

(2)
where 𝑒𝑣
𝑖−𝑗 ∈ R𝑑𝑧 are learned embeddings for each relative
position 𝑖 − 𝑗, e. g. one token is located two tokens to the left from
another token. This mechanism, that we call sequential relative
attention, was shown to substantially outperform sequential posi-
tional embeddings and encodings in sequence-based text processing
tasks. Ahmad et al. [1] reach the same conclusion evaluating se-
quential relative attention in a task of code summarization, i.e.,
generating natural language summaries for code snippets.

Tree positional encodings. Inspired by previously discussed works,
several authors developed mechanisms for processing trees. Shiv
and Quirk [32] develop positional encodings for tree-structured
data, assuming that the maximum number 𝑛𝑤 of node children and
the maximum depth 𝑛𝑑 of the tree are relatively small. Example
encodings are given in Figure 1(d). The position of each node in a
tree is defined by its path from the root, and each child number
in the path is encoded using 𝑛𝑤-sized one-hot vector. The overall
representation of a node is obtained by concatenating these one-hot
vectors in reverse order and padding short paths with zeros from
the right. The authors also introduce the learnable parameters of
the encoding, their number equals 𝑑model/(𝑛𝑤 · 𝑛𝑑 ). Paths longer
than 𝑛𝑑 are clipped (the root node is clipped first). The authors
binarize ASTs to achieve 𝑛𝑤 = 2. To avoid this binarization, we
replace all child numbers greater than 𝑛𝑤 with 𝑛𝑤, and select the
best hyperparameters 𝑛𝑤 and 𝑛𝑑 using grid search, see details in
section 4.

Shiv and Quirk [32] tested the approach on the task of code
translation (code-to-code) and semantic parsing (text-to-code). The
Transformer with tree positional encodings outperformed standard
Transformer with sequential positional encodings and TreeLSTM
[34].

Tree relative attention. An extension of sequential relative atten-
tion for trees was proposed by Kim et al. [20]. In a sequence, the
distance between two input positions is defined as the number of

positions between them. Similarly, in a tree, the distance between
two nodes can be defined as the shortest path between nodes, con-
sisting of 𝑛𝑈 ⩾ 0 steps up and 𝑛𝐷 ⩾ 0 steps down, see example
in Figure 1(e). Now, similarly to sequential relative attention, we
can learn embeddings for the described distances and plug them
into self-attention. Learning multidimensional embeddings for the
tree input requires much more memory than for sequential input,
since distances in the tree are object-specific, while distances in the
sequence are the same for all objects in a mini-batch. As a result,
the authors use scalar embedding 𝑟𝑖 𝑗 ∈ R for the distance between
nodes 𝑖 and 𝑗 and plug it into the attention mechanism as follows
(other formulas stay the same):

˜𝛼𝑖 𝑗 =

exp(𝑎𝑖 𝑗 · 𝑟𝑖 𝑗 )
(cid:205)𝑗 exp(𝑎𝑖 𝑗 · 𝑟𝑖 𝑗 )

.

(3)

Our preliminary experiments suggested that using summation 𝛼𝑖 𝑗 +
𝑟𝑖 𝑗 instead of multiplication leads to a higher final score. The authors
tested the approach on the task of code completion, i.e., predicting
the next token, and showed that using modified attention improves
quality when applying Transformer to AST traversal and to code
as text.

GGNN Sandwich. Due to the graph nature of AST, source codes
are often processed using graph gated neural networks (GGNN) [4].
To add more inductive bias, AST is augmented with edges of sev-
eral additional types, e.g., reflecting data- and control-flow in the
program. Such a model captures local dependencies in data well
but lacks a global view of the input program, that is the Trans-
former’s forte. Inspired by this reasoning, Hellendoorn et al. [14]
propose alternating Transformer and GGNN layers as illustrated in
Figure 1(f), to combine the strengths of both models. GGNN layer
relies on passing messages through edges for a fixed number of iter-
ations (number of passes). The model is called GGNN Sandwich by
the authors, and the details can be found in [14]. GGNN Sandwich
was shown to be effective in the variable misuse detection task,
i.e., predicting the location of a bug and the location used to fix
the bug (copy variable). GGNN Sandwich outperformed standard
Transformer with sequential positional encodings.

Our work focuses of processing syntactic information in Trans-
former, thus we do not use data- and control-flow edges. Data- or
control-flow edges are hard to incorporate in other mechanisms ex-
cept GGNN Sandwich. In our GGNN Sandwich, we use AST edges,
edges connecting the neighbouring nodes in the AST depth-first tra-
versal, and edges connecting nodes to themselves, see illustration
in fig. 1(f).

Hellendoorn et al. [14] also propose a model called GREAT that is
inspired by relative attention and incorporates 1-dimensional edge
embeddings into the attention mechanism. This model is concep-
tually similar to the tree relative attention, thus we do not include
GREAT in our comparison.

3 LIMITATIONS OF EXISTING APPROACHES
AND METHODOLOGY OF THE WORK

As shown in Section 2 several approaches for processing ASTs in
Transformers have been proposed. However, it is still unclear which
approaches perform better than others and what mechanisms to
use in practice. First, all the works discussed in Section 2 conduct

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

experiments with different tasks making it hard to align the results.
Moreover, almost all the listed works compare their approaches with
the vanilla Transformer, i.e., Transformer with sequential positional
encodings or embeddings, while modern practices use advanced
mechanisms, like sequential relative attention, by default. Even
works that propose tree-processing approaches inspired by sequen-
tial relative attention do not include this mechanism as a baseline.
That is, it is unclear whether using advanced tree-processing mech-
anisms is beneficial at all. Secondly, the existing approaches focus
on capturing tree structure and do not investigate the influence of
other components of AST, i.e., types and values.

In this work, we conduct a thorough empirical study on utilizing
AST in Transformers. We consider three code processing tasks: vari-
able misuse (VM) detection, function naming (FN), and code com-
pletion (CC), and two source code datasets: Python150k [38] and
JavaScript150k [30]. We selected tasks that are often used as bench-
marks in the literature and on which the compared approaches were
tested by their authors. Our selection also covers various Trans-
former configurations, i.e., encoder only (VM), decoder only (CC)
and encoder-decoder (FN). We selected the Python150k dataset be-
cause it is often used in the literature, and JavaScript150k because
it is distributed by the same authors and has the same format.

We re-implement all mechanisms described in Section 2 in a
unified framework and investigate the most effective approach for
processing ASTs in Transformer in different tasks. We answer the
following research questions:

• What is the most effective approach for utilizing AST struc-

ture in Transformer?

• Is Transformer generally capable of utilizing syntactic infor-

mation represented via AST?

• What components of AST (structure, node types and values)

does Transformer use in different tasks?

There is no common practice of preprocessing ASTs, particularly,
processing values. Each node in AST is associated with a type, but
not all nodes have associated values. Kim et al. [20] and Shiv and
Quirk [32] attach values as separate child nodes so that each node
stores only one item (type or value), while Hellendoorn et al. [14]
propose omitting types. The former approach increases input length
and thus makes code processing significantly slower, while the
latter approach loses type information. We choose an in-between
strategy inspired by the approach of Li et al. [23] used for RNNs: we
associate the <empty> value with nodes that do note have values, so
that each node 𝑖 in AST has both type 𝑡𝑖 and value 𝑣𝑖 , see Figure 1(c).
This setup preserves the initial AST structure and allows us to easily
ablate types or values, leaving the other item in each node present.
Some works show that splitting values based on snake_case or
CamelCase, or using splitting techniques such as byte-pair encod-
ing may improve the quality [6, 19]. We do not use splitting into
subtokens for two reasons. Firstly, splitting makes sequences much
longer, resulting in a substantial slow down of training procedure
because of quadratic Transformer complexity w.r.t. the input length.
Secondly, splitting breaks the one-to-one correspondence between
AST nodes and values, i.e., several values belong to one AST node.
There are different ways of adapting AST-based Transformers to
the described problem: one option is to average embeddings over
subtokens [14], another option is to assign a chain of subtokens

as a child of a node and then directly apply tree-processing mech-
anisms. A third option is to modify tree-processing mechanisms,
e.g., duplicate paths for all subtokens in tree positional encoding or
duplicate tree relations for all pairs of subtokens of two tokens in
tree relative attention. As a result, the question of how splitting into
subtokens affects syntax-capturing mechanisms requires a separate
study which we leave for the future work.

An important part of our methodology is conducting experi-
ments in two settings, namely anonymized and full-data. The full-
data setting corresponds to the conventional training of Trans-
former on ASTs parsed from code. In this case, Transformer has two
sources of information about input code snippets: syntactic informa-
tion and user-defined identifiers (stored in node values). Identifiers
usually give much additional information about the semantics of
the code, however, their presence is not necessary for correct code
execution: renaming all user-defined identifiers with placeholders
var1, var2, var3 etc. will lead to the same result of code execution
and will not change the semantics of the algorithm the code im-
plements. Here we mean that all occurrences of an identifier are
replaced with the same placeholder, thus, important information
about identifier repetition is saved. We call this renaming identifiers
with placeholders as anonymization. In the anonymized setting, the
input code is represented purely with syntax structure and the only
way Transformer can make meaningful predictions is to capture
information from AST. In this way, using the anonymized setting
allows a better understanding of the capabilities of Transformer to
utilize syntactic information. More details on the anonymization
procedure are given in Appendix A.

Another important part of our methodology is a thoughtful
splitting of the dataset into training and testing sets, which includes
splitting by repository and removing code duplicates. Alon et al.
[6], LeClair et al. [22] notice that code files inside one repository
usually share variable names and code patterns, thus splitting files
from one repository between training and testing sets simplifies
predictions for the testing set and leads to a data leak. To avoid
this, one should put all files from one repository into one set, either
training or testing (this strategy is called splitting by repository).
Even using this strategy, duplicate code can still occur in the testing
set, since developers often copy code from other projects or fork
other repositories. Allamanis [3] underline that in commonly used
datasets up to 20% of testing objects can be repeated in the training
set, biasing evaluation results. As a result, the deduplication step is
needed after data splitting.

4 EXPERIMENTAL SETUP

Data. In all tasks, we use the Python150k (PY) dataset [28] (redis-
tributable version) and JavaScript150k (JS) dataset [30] downloaded
from the official repository at https://eth-sri.github.io. Both datasets
consist of code files downloaded from Github and are commonly
used to evaluate code processing models.

Most research use the train-test split provided by the authors
of the dataset, however, this split does not follow best practices
described in Section 3 and produce biased results [3], so we release
a new split of the dataset. We remove duplicate files from both
datasets using the list of duplicates provided by Allamanis [3]. We
also filter out absolutely identical code files, and when selecting

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

functions from code files, we additionally filter out absolutely iden-
tical functions. We split data into training / validation / testing sets
in proportion 60% / 6.7% / 33.3% based on Github usernames (each
repository is assigned to one username).

Preprocessing details for each task are given below. We release
our data split and our source code including scripts for downloading
data, deterministic code for data preprocessing, models, training
etc. We largely rely on the implementations of other research, and
compare the quality of our baseline models to the results reported
in other papers, when possible; see details in Section 9.

Variable misuse task (VM). For the variable misuse task, we use
the setup and evaluation strategy of Hellendoorn et al. [14]. Given
the code of a function, the task is to identify two positions (using two
pointers): one in which position a wrong variable is used, and one in
which position a correct variable can be copied from (any such posi-
tion is accepted). If a snippet is non-buggy, the first pointer should
select a special no-bug position. We obtain two pointers, by apply-
ing two position-wise fully-connected layers, and softmax over po-
sitions on top of Transformer outputs. For example, the first pointer
selects position as argmax1⩽𝑖 ⩽𝐿softmax([𝑢𝑇 𝑦1, . . . , 𝑢𝑇 𝑦𝐿, 𝑏]), 𝑦𝑖 ∈
R𝑑model, 𝑢 ∈ R𝑑model , 𝑏 ∈ R (𝑏 is a learnable scalar corresponding
to the no-bug position), [. . . ] denotes the concatenation of the el-
ements into a vector of scalars. The second pointer is computed
in a similar way but without 𝑏. The model is trained using the
cross-entropy loss.

To process the original dataset for the variable misuse task, we
select all top-level functions, including functions inside classes,
from all (filtered) 150K files, and filter out functions: longer than
250 nodes (to avoid very long functions); and functions with less
than three positions containing user-defined variables or less than
three distinct user-defined variables (to avoid trivial bug fixes). We
select a function with a root node type FunctionDef for PY, and
FunctionDeclaration or Function Expression for JS. The result-
ing training / validation / testing set consists of 417K / 48K / 231K
functions for PY and 202K / 29K / 108K for JS. One function may
occur in the dataset up to 6 times, 3 times with a synthetically gen-
erated bug and 3 times without a bug. Following [14], we use this
strategy to avoid biasing towards long functions with a lot of differ-
ent variables. The buggy examples are generated synthetically by
choosing random bug and fix positions from positions containing
user-defined variables.

We use the joint localization and repair accuracy metric of [14] to
assess the quality of the model. This metric estimates the portion of
buggy samples for which the model correctly localizes and repairs
the bug. We also measured localization accuracy and repair accuracy
independently and found that all three metrics correlate well with
each other.

Function naming task (FN). In this task, given the code of a func-
tion, the task is to predict the name of the function. To solve this task,
we use the classic sequence-to-sequence Transformer architecture
that outputs the function name word by word. A particular imple-
mentation is borrowed from [1] (the paper used another dataset).
Firstly, we pass function code to the Transformer encoder to obtain
code representations 𝑦1, . . . , 𝑦𝐿. Then, the Transformer decoder
generates the method name word by word, and during each word

generation, decoder attends to 𝑦1, . . . , 𝑦𝐿 (using encoder-decoder
attention) and to previously generated tokens (using masked de-
coder attention). To account for the sequential order of the function
name, we use sequential positional embeddings in the Transformer
decoder. In the encoder, we consider different structure-capturing
mechanisms. We use greedy decoding. We train the whole encoder-
decoder model end-to-end, optimizing the cross-entropy loss.

To obtain the processed dataset for the function name task, we
select all top-level functions, including functions inside classes,
from all (filtered) 150K files, and filter out functions longer than
250 AST nodes (to avoid very long functions), functions for which
the name could not be extracted (a lot of FunctionExpressions in
JS are anonymous), and functions with names consisting of only
underscore characters and names containing rare words (less than
5 / 3 occurrences in the training set for PY / JS). To extract functions,
we use the same root node types as in the VM task. The resulting
dataset consists of 523K / 56K / 264K training/validation/testing
functions for PY and 186K / 23K / 93K for JS. We replace func-
tion name in the AST with a special <fun_name> token. To ex-
tract target function names, we remove extra underscores and
split each function name based on CamelCase or snake_case, e.g.,
name _get_feature_names becomes [get, feature, names]. A
mean±std length of function name is 2.42±1.46 words for PY and
2.22±1.23 for JS.

We assess the quality of the generated function names using the
F1-metric. If 𝑔𝑡𝑛 is a set of words in ground-truth function name
and 𝑝𝑛 is a set of words in predicted function name, the F1-metric
is computed as 2𝑃𝑅/(𝑃 + 𝑅) ∈ [0, 1], where 𝑃 = |𝑔𝑡𝑛 ∩ 𝑝𝑛|/|𝑝𝑛|,
𝑅 = |𝑔𝑡𝑛 ∩ 𝑝𝑛|/|𝑔𝑡𝑛|, | · | denotes the number of elements. F1 is
averaged over functions. We choose the F1 metric following Alon
et al. [6] who solved a similar task with another dataset and model.

Code completion task (CC). For the task of code completion, we
use the setup, metrics and Transformer implementation of Kim
et al. [20]. The task is to predict the next node (𝑡𝑖, 𝑣𝑖 ) in the depth-
first traversal of AST [(𝑡1, 𝑣1), . . . , (𝑡𝑖−1, 𝑣𝑖−1)]. We predict type 𝑡𝑖
and value 𝑣𝑖 using two fully connected layers with softmax on
top of the prefix representation 𝑦𝑖 : 𝑃 (𝑡𝑖 ) = softmax(𝑊 𝑡𝑦𝑖 ), 𝑊 𝑡 ∈
R#types×𝑑model , 𝑃 (𝑣𝑖 ) = softmax(𝑊 𝑣𝑦𝑖 ), 𝑊 𝑣 ∈ R#values×dmodel .

To obtain the dataset for the code completion task, we use full
ASTs from (filtered) 150k files, removing sequences with length less
than 2. If the number of AST nodes is larger than 𝑛𝑐𝑡𝑥 = 500, we split
AST into overlapping chunks of length 𝑛𝑐𝑡𝑥 with a shift 1
𝑛𝑐𝑡𝑥 . The
2
overlap provides a context for the model. For example, if the length
of AST is 800, we select the following samples: 𝐴𝑆𝑇 [: 500), 𝐴𝑆𝑇 [250 :
750), 𝐴𝑆𝑇 [300 : 800]. We do not calculate loss or metrics over the
intersection twice. For the previous example, the quality of predic-
tions is measured only on 𝐴𝑆𝑇 [: 500), 𝐴𝑆𝑇 [500 : 750), 𝐴𝑆𝑇 [750 : 800].
The overlapping splitting procedure is borrowed from [20] and is
needed since processing extremely long sequences is too slow in
Transformer because of its quadratic complexity w.r.t. input length.
The resulting dataset consists of 186K / 20K / 100K training / vali-
dation / testing chunks for PY and 270K / 32K / 220K for JS.

We mostly focus on value prediction, since it is the more complex
task, and present results for type prediction where they significantly
differ from other tasks. We optimize the sum of cross-entropy losses
for types and values.

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Table 1: Selected hyperparameters for different structure-
capturing mechanisms, tasks and datasets. The details on
selecting hyperparameters are given in Appendix A.

Model (hypers.)

Lang.

Seq. rel. attn.
(max. dist.)
Tree pos. enc.
(max width, depth)
Tree rel. attn.
(rel. vocab. size)
GGNN Sandwich
(num. layers,
num. edge types,
is GGNN first?)

PY
JS
PY
JS
PY
JS
PY
JS

VM

8
8
8, 16
4, 8
100
600

FN

250
250
16, 8
2, 64
600
100

12, 3, N 6, 2, Y
12, 3, N 6, 2, Y

CC

32
32
16, 8
16, 32
1000
1000
N/A
N/A

(cid:205)𝑁

We use mean reciprocal rank (MRR) to measure the model quality
since it reflects the practical application of code completion: MRR =
1
𝑖=1 1/𝑟𝑎𝑛𝑘𝑖 , where 𝑟𝑎𝑛𝑘𝑖 is a position of the 𝑖-th true token
𝑁
in the model ranking, 𝑁 is the total number of target tokens in a
dataset, excluding <padding> and <empty> tokens. As in [20], we
assign zero score if the true token is out of top 10 predicted tokens.

Hyperparameters. We list general hyperparameters for the vari-
able misuse / function naming / code completion tasks using slashes.
Our Transformer models have 6 layers, 6 / 6 / 8 heads, 𝑑𝑚𝑜𝑑𝑒𝑙 = 512.
We limit vocabulary sizes for values up to 50K / 50K / 100K tokens
and preserve all types. As discussed in Section 3, we do not split
values into subtokens. We train all Transformers using Adam with
a starting learning rate of 0.00001 / 0.0001 / 0.0001 and a batch size
of 32 for 25 / 15 / 20 epochs for PY and 40 / 25 / 20 epochs for JS (the
number of functions in the JS dataset is smaller than in PY dataset,
thus more epochs are needed). In the code completion task, we use
the cosine learning rate schedule [25] with 2000 warm-up steps
and a zero minimal learning rate, and a gradient clipping of 0.2. In
the variable misuse task and in the function naming task for JS, we
use a constant learning rate. In the function naming task for PY,
we decay the learning rate by 0.9 after each epoch. In the function
naming task, we also use a gradient clipping of 5. We use resid-
ual, embedding and attention dropout with 𝑝 = 0.2 / 0.2 / 0.1. All
models were trained three times, to estimate the standard deviation
of the quality (except hyperparameter tuning). In all experiments
we report the quality on the test set, except hyperparameter tun-
ing where we report the quality on the validation set. We train all
models on one GPU (NVIDIA Tesla P40 or V100).

The hyperparameters for different structure-capturing mecha-
nisms were tuned using grid search, based on the quality on the
the validation set, for each dataset–task combination individually.
For sequential relative attention, we tune the maximum relative
distance between the elements of the sequence. For tree positional
encoding, we tune the maximum path width and the maximum path
depth. For tree relative attention, we tune the size of the relation vo-
cabulary. For the GGNN Sandwich model, we consider 6-layer and
12-layer configurations of alternating Transformer (T) and GGNN
(G) layers, we also consider placing both types of layers first i.e.,
[T, G, T, G, T, G] or [G, T, G, T, G, T] (and similarly for 12 layers).

GGNN layers include 4 message passes. We also consider omitting
edges of types Left and Right. Sequential positional embeddings
do not have hyperparameters. The number of parameters in all
AST-based modifications of Transformer are approximately the
same, except GGNN Sandwiches: 12-layer Sandwich incorporates
slightly more parameters than vanilla Transformer, while 6-layer
incorporates slightly fewer parameters. The details and the Tables
on hyperparameter search are given in Appendix A, the resulting
hyperparameters are listed in Table 1.

5 COMPARISON OF APPROACHES FOR

UTILIZING SYNTACTIC STRUCTURE IN
TRANSFORMER

We begin with investigating which of the mechanisms for utilizing
AST structure in Transformer is the most effective one. We obtain
the trees storing a (type, value) pair in each node using the approach
described in Section 3 and pass these trees to Transformer, equipped
with one of the mechanisms described in Section 2. GGNN Sand-
wich is not applicable to code completion because message-passing
involves all nodes and prohibits using masking in the decoder.

The results are presented in Figure 2. In function naming, most
structure-capturing mechanisms perform similarly. In Section 7, we
show that in this task quality is not affected much even if we com-
pletely ablate structure information, i.e., do not use any structure-
capturing mechanism and treat input as a set of (type, value) pairs.
That is, Transformer hardly utilizes syntactic structure when pre-
dicting function names. However, in other tasks, this is not the case
and there is more variability in different mechanisms performance.
Utilizing structure information in the input embeddings is not
effective: sequential positional embeddings and tree positional en-
codings do not achieve highest score in any task, except function
naming where tree positional encodings perform on par with other
mechanisms.

Utilizing structure in the self-attention mechanism is much more
effective: in all tasks, at least one of sequential relative attention
and tree relative attention is the best performing model. Sequential
relative attention achieves the highest score in variable misuse and
value prediction tasks, while tree relative attention outperforms
others by a high margin in type prediction task (this model was
developed for code completion task). The last result is interpretable
since tree relative attention helps to find relatives in AST tree, e.g.,
parent and siblings, which is important in type prediction. The
advantage of sequential relative attention is that it can use the
multidimensional embeddings of relations: the sequential relations
are shared across objects, leading to affordable 3-dimensional em-
bedding tensors of shape (length, length, embedding dimension).
In contrast, tree relative attention can only afford one-dimensional
embeddings of relations, because tree-based relations are not shared
between objects in a mini-batch, and extracting them for a mini-
batch would already lead to a 3-dimensional tensor: (batch, length,
length).

GGNN Sandwich achieves high results in the variable misuse
task for which this model was developed. The reason is that in
variable misuse detection, the goal is to choose two variables, and
local message passing informs each variable of its role in a program
and makes variable representations more meaningful. The original

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Python:

Variable Misuse

Function Naming

Code Completion (values) Code Completion (types)

JavaScript:

Variable Misuse

Function Naming

Code Completion (values) Code Completion (types)

Figure 2: A comparison of different mechanisms for processing AST structure in Transformer, in the full-data setting. The
numeric data for barplots is given in Appendix C.

Table 2: Time- and storage-consumption of different
structure-capturing mechanisms for the variable misuse
task on the Python dataset.

Model

Seq. pos. emb.
Seq. rel. att.
Tree pos. enc.
Tree rel. attn.
GGNN Sandwich

Train time
(h/epoch)

Preprocess
time (ms/func.)

Add. train
data (GB)

2.3
2.7
2.5
3.9
7.2

0
0
0.4
16.7
0.3

0
0
0.3
18
0.35

work on GGNN Sandwiches also uses additional types of edges
which would improve the performance of this model further. Using
these types of edges is out of scope of this work, since we focus
on utilizing syntactic information, thus we only use syntax-based
edges.

In Appendix B, we visualize the progress of test metrics dur-
ing training, for different structure-capturing mechanisms in the
full-data setting. This Appendix also presents the comparison of
structure-capturing mechanisms in the anonymized setting that is
described in Section 3 and implies replacing values in ASTs with
unique placeholders. The leading mechanisms are the same in all
tasks as in the full data setting, considered above. An interested
reader may also find examples of attention maps for different mech-
anisms in Appendix D.

In Table 2, we list training time and the size of auxiliary data
needed for different structure-capturing mechanisms. GGNN Sand-
wich model requires twice the time for training (and prediction)
compared to other models, because of the time-consuming mes-
sage passing mechanism. Tree relative attention requires dozens
of gigabytes for storing pairwise relation matrices for all training
objects that could be replaced with slow on-the-fly relation matrix
generation. Tree positional encodings and GGNN Sandwich models
also require additional disk space for storing preprocessed graph

Table 3: Comparison of combinations of sequential relative
attention (SRA) with other structure-capturing approaches.
All numbers in percent, standard deviations: VM: 0.5%, FN:
0.4%, CC: 0.1%. Bold emphasizes combinations that signifi-
cantly outperform SRA. *In the VM task, SRA+GGNN Sand-
wich significantly outperforms SRA during the first half of
epochs, but loses superiority at the last epochs, for both
datasets. On the Python dataset, SRA+GGNN Sandwich out-
performs SRA by one standard deviation at the last epoch.

Model

VM

FN

CC (val.)

PY

PY

JS

JS

SRA
SRA + Seq. pos. emb.
SRA + Tree pos. enc.
SRA + Tree rel. attn.
SRA + GGNN sand.
SRA
SRA + Seq. pos. emb.
SRA + Tree pos. enc.
SRA + Tree rel. attn.
SRA + GGNN sand.

81.42
80.77
81.73
81.58
82.00*
76.52
73.17
74.73
76.34
75.33*

35.73
33.99
34.71
35.41
33.39
24.62
23.09
23.70
24.71
21.44

54.53
54.37
54.63
54.91
N/A
64.11
63.97
64.49
64.79
N/A

representations, but the sizes of these files are relatively small. Se-
quential positional embeddings and relative attention are the most
efficient models, in both time- and disk-consumption aspects.

To sum up, we emphasise sequential relative attention as the most
effective and efficient approach for capturing AST structure in Trans-
former.

Combining structure-capturing mechanisms. In Table 3, we show
that using sophisticated structure-capturing mechanisms may be use-
ful for further improving sequential relative attention if we com-
bine two mechanisms. We find that tree relative attention (for both
datasets) and tree positional encoding (for JS) improve the score in

607080Joint accuracySeq. pos. emb.Seq. rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich3233343536F14050607080Joint accuracySeq. pos. emb.Seq. rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich22232425F1ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Table 4: Illustration of different kinds of models used in the
experiments. The code snippet and its AST used in the illus-
tration may be found in Figure 1 (a, b).

Model

Input representation

Syntax+Text

[(Assign, <empty>), (NameStore, elem), ...

..., (Index, <empty>), (NameLoad, idx)]

Syntax

[(Assign, <empty>), (NameStore, <var1>), ...

Text
Constant

..., (Index, <empty>), (NameLoad, <var3>)]

[elem, lst, idx]
Predicts the most frequent target for any input

the value prediction task, while GGNN Sandwich may improve the
score in the variable misuse task, especially at earlier epochs.

6 CAPABILITY OF TRANSFORMER TO
UTILIZE SYNTACTIC INFORMATION

When developing approaches for utilizing syntactic information
in Transformer, the majority of works mostly focus on the tree
structure. We discussed the utilization of structure in the previous
section, but we also would like to investigate the influence of other
AST components, namely types and values. In the next section, we
conduct an ablation study of the mentioned components, and in this
section, we investigate whether Transformer is generally capable
of utilizing the syntactic information in source code, i. e. does pro-
cessing AST components improve performance. This conceptual
experiment tests the overall suitability of Transformer for source
code processing. We formalize the specified question in full-data
and anonymized settings as follows:

• Syntax+Text vs. Text: First, we test whether using syntactic
information in addition to the textual information is benefi-
cial, compared to using pure textual information. To do so,
we compare the quality of Transformer trained on full AST
data (Syntax+Text) with the quality of Transformer trained
on a sequence of non-empty values (Text), see Table 4 for
illustrations. The Text model relies only on textual informa-
tion stored in values and does not have access to any other
kind of information.

• Syntax vs. Constant: Secondly, we test whether Transformer
is able to make meaningful predictions given only syntac-
tic information, without textual information. To do this,
we test whether the quality of Transformer trained on the
anonymized AST data (Syntax) is better than the quality of a
simple constant baseline (Constant). Anonymization removes
identifiers, i.e., textual information, but preserves informa-
tion about identifiers’ repetition, which may be essential for
understanding the program. The Constant model outputs the
most frequent target, e.g., no-bug in VM and name init for
PY and exports for JS in FN. Since anonymized AST data
contains only syntactic information and does not contain
textual information, the only way the Transformer can out-
perform the Constant baseline on this data is to capture some
information from AST.

All the described models are trained with sequential relative atten-
tion. Deduplicating the dataset is very important in this experiment
to avoid overestimating the Syntax baseline.

The results for three tasks are presented in Figure 3. In all cases,
Syntax+Text outperforms Text, and Syntax outperforms Constant. In
Figure 4, we present example predictions for code completion and
function naming tasks for Python language with all four models.
In code completion, syntax-based models capture frequent coding
patterns well, for instance, in example (b), Syntax model correctly
chooses (anonymized) value argmnents and not argv or parse
because argmnents goes before assignment. In example (a), Syn-
tax+Text correctly predicts create_bucket because it goes inside
the if-statement checking whether the bucket exists, while Text
model outputs frequent tokens associated with buckets.

In the function naming task, the Text model captures code se-
mantics based on variable names and sometimes selects wrong
“anchor” variables, e.g., split in example (c), while the Syntax+Text
model utilizes AST information and outputs correct word dict. The
Syntax model does not have access to variable names as it processes
data with placeholders, and outputs overly general predictions, e.g.,
get all in example (c). Nevertheless, the Syntax model is capable
of distinguishing general code purpose, e.g., model uses word get in
example (c) and word read in example (d). To sum up, Transformer
is indeed capable of utilizing syntactic information.

Interestingly, in code completion (value prediction, PY) and vari-
able misuse detection tasks (JS), the Syntax model, trained on the
anonymized data outperforms the Syntax+Text model trained on
full data, though the latter uses more data than the former. The
reason is that the values vocabulary on full data is limited, so ap-
prox. 25% of values are replaced with the UNK token and cannot
be predicted correctly. On the other hand, this is not a complica-
tion for the Syntax model, which anonymizes both frequent and
rare identifiers in the same way. For example, in Figure 4(b), the
Syntax model correctly predicts the misspelled token argmnents
while for the Syntax+Text model, this token is out-of-vocabulary
and so model outputs frequently used strings for printing. One
more advantage of the Syntax model in the value prediction task
is that it is twice faster in training because of the small output
softmax dimension. In the function naming task, the Syntax model
performs substantially worse than Syntax+Text, because variable
names provide much natural language information needed to make
natural language predictions. To sum up, anonymization may lead
to a higher quality than using full data.

7 ABLATION STUDY OF MECHANISMS FOR
UTILIZING SYNTACTIC INFORMATION IN
TRANSFORMER

In this section, we investigate the effect of ablating different AST
components on the performance in three tasks. This ablation study
is important for both providing practical recommendations and un-
derstanding what mechanisms are essential for making reasonable
predictions in the anonymized setting, discussed in the previous
section. We consider three AST components (comments in items
regard the usual scenario without ablation): (types): the types of
nodes are passed as one of the Transformer inputs; (values): the
(anonymized) values are passed as one of the Transformer inputs;

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Variable Misuse – Python

Function Naming – Python

Code Completion (values) – Python

Variable Misuse – JavaScript

Function Naming – JavaScript

Code Completion (values) – JavaScript

Figure 3: Comparison of syntax-based Transformer models with text-only and constant baselines.

Figure 4: Example predictions in code completion (a, b; top 3 predictions) and function naming (c, d) tasks. S: Syntax, T: Text.

(structure): AST structure is processed using one of the mechanisms
discussed in Section 2.

role in achieving high quality in the variable misuse task, with
absent values from the data.

As in Section 6, we consider both anonymized and full-data set-
tings. We ablate AST components one by one in both models Syntax
and Syntax+Text and check whether the quality drops. Ablating
types for Syntax+Text was in fact performed in Section 6, but we
repeat the results in this experiment’s table and also report this ab-
lation for the anonymized setting. Ablating structure means turning
off all syntax-capturing mechanisms so that the input of the Syn-
tax+Text / Syntax model will be viewed as an unordered set of (type,
value) / (type, anonymized value) pairs. Ablating values means us-
ing only types as the input to the model — the numbers are the same
for both full-data and anonymized settings. We skip this ablation in
the code completion task, since, in this case, (anonymized) values
are the target of the model.

The results are presented in Table 5. In variable misuse and code
completion, all AST components are essential for achieving high qual-
ity results. Particularly, in variable misuse, all ablations result in a
large quality drop, in both settings, and in code completion, ablating
types results in a large quality drop and ablating structure — in
substantial drop. Interestingly, anonymization plays an important

However, the observations differ for function naming. In this
task, (1) ablating types results in a substantial quality drop in both
settings, (2) ablating structure results in a small (but significant)
quality drop in both settings, (3) ablating values in the full-data
setting results in the large quality drop, and (4) ablating anonymized
values does not affect the performance in the anonymized setting.
The first and the third observations underline the importance of
using both types and values in practice. The second and the fourth
observations show that Transformer is now far from utilizing all the
information stored in AST when predicting function names. Partic-
ularly, in the anonymized setting, Transformer predicts function
names mostly based on types. It hardly uses syntactic structure, and
does not use information about value repetition which is stored in
anonymized values and is essential for understanding the algorithm
that the code implements. Overcoming this issue is an interesting
direction for future research.

0510152025Epoch020406080Joint accuracySyntax+TextTextSyntaxConstant02468101214Epoch101520253035F1Syntax+TextTextSyntaxConstant0.02.55.07.510.012.515.017.5Epoch102030405060MRRSyntax+TextTextSyntaxDummy0510152025303540Epoch01020304050607080Joint accuracySyntax+TextTextSyntaxConstant0510152025Epoch510152025F1Syntax+TextTextSyntaxConstant0.02.55.07.510.012.515.017.5Epoch102030405060MRRSyntax+TextTextSyntaxDummydefget_or_create_bucket(s3_connection):bucket=s3_connection.get_bucket(settings.S3_BUCKET_NAME)ifbucketisNone:bucket=s3_connection.importsysfromutils.parsingimportparseif__name__=='__main__':argmnents=parse(sys.argv[1:])print(def<fun_name>(seqs):dt={}forseqinseqs:forwordinseq.split():ifnotwordindt:dt[word]=1else:dt[word]+=1returndtdef<fun_name>(filename):withopen(filename)asfin:returnlen(fin.read().split("\n"))(a)S+T:[createbucketgetbucketgetkey](b)S+T:[Usage:done.\n](c)S+T:getdict(d)S+T:readﬁleS:[getbucketS3BUCKETNAMEvar107]S:[argmnentsvar440var289]S:getallS:readT:[bucketgetbucketsettings]T:[sysoutfnlen]T:getsplitT:readﬁleDummy:[self0None]Dummy:[self0None]Dummy:initDummy:initGold:createbucketGold:argmnentsGold:getdictionaryGold:countlines1ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Table 5: Ablation study of processing different AST components in Transformer. Bold emphasises best models and ablations
that do not hurt the performance. AST w/o struct.: Transformer treats input as a bag without structure; AST w/o types: only
values or anonymized values are passed to Transformer; AST w/o an.val.: only types are passed to Transformer. N/A – not
applicable.

Full data

Anonymized data

Python

Full AST
AST w/o struct.
AST w/o types
AST w/o an.val.
Full AST

JavaScript AST w/o struct.
AST w/o types
AST w/o an.val.

Var. misuse

Fun. naming Comp. (val.)

Var. misuse
81.59±0.50% 35.73±0.19% 54.59±0.2% 81.71±0.41% 25.26±0.15%
23.29±0.18%
12.41±0.58%
53.1±0.1%
26.81±0.47%
34.80±0.24%
33.60±0.23% 42.01±0.05% 58.55±0.51 %
71.55±0.28%
12.50±1.5%
32.44±0.35% 25.25±0.06%
32.44±0.35% 25.25 ±0.06%
N/A
78.47±0.26% 13.66±0.30% 60.82±0.07%
75.60±0.15% 24.62±0.14%
64.2±0.05
58.59±0.1%
11.25±0.08%
5.37±0.97%
23.40±0.12% 61.53±0.15%
17.25±0.83%
42.91±0.1%
43.53±0.92 %
53.4±0.1 %
23.09±0.09%
60.33±0.50%
8.10±1.4%
42.56±0.24% 13.64±0.07%
13.64±0.07%
42.56±0.24%
N/A
N/A

Fun. naming Comp. (val.)
58.76±0.2%
57.75±0.05%
41.26±0.05%
N/A

Table 6: Comparison of ensembles. Notation: ST – Syn-
tax+Text, S – Syntax, & denotes ensembling. All models are
trained with sequential relative attention. All numbers in
percent, standard deviations: VM: 0.5%, FN: 0.4%, CC: 0.1%.

Models

VM

FN

CC (types) CC (values)

PY

JS

ST
ST & ST
S
S & S
ST & S
ST
ST & ST
S
S & S
ST & S

81.42
82.80
81.83
82.57
86.72
76.52
77.25
78.53
79.65
82.29

35.73
35.61
25.26
25.46
32.15
24.62
24.53
13.66
13.19
19.33

89.22
89.39
88.42
88.65
89.49
90.14
90.56
88.22
88.52
90.32

54.53
56.35
58.6
59.29
61.84
64.11
65.68
60.71
61.42
68.33

8 ENSEMBLING OF SYNTAX-BASED MODELS
As was shown in Figure 4, Syntax+Text and Syntax models capture
dependencies of different nature and are orthogonal in a sense
of handling missing values and first time seen tokens. This allows
hypothesizing that ensembling two mentioned models can boost the
performance of the Transformer. We use the standard ensembling
approach that implies training networks from different random
initializations and averaging their predictions after softmax [8]. We
use sequential relative attention in this experiment.

In Table 6, we compare an ensemble of Syntax+Text and Syntax
models with ensembles of two Syntax+Text and of two Syntax mod-
els. We observe that in variable misuse and value prediction tasks,
ensembling models that view input data in two completely differ-
ent formats is much more effective than ensembling two similar
models. This is the way how using anonymized data may boost the
Transformer’s performance.

9 VALIDATING OUR IMPLEMENTATIONS
AND COMPARING TO OTHER WORKS

We ensure the validity of our results in two ways: by relying on the
code of already published works, and by comparing our numbers

achieved for the commonly used data split to the numbers in the
corresponding papers. Particularly, we use the model / loss / metrics
/ overlapping chunks code of Kim et al. [20] as the baseline for the
CC task, we rewrite (line by line) the main parts of the model /
loss / metrics code of Hellendoorn et al. [14] in PyTorch, as the
baseline for the VM task, and we use the model / loss / metrics code
of Ahmad et al. [1] as the baseline for the FN task.

For VM, the vanilla Transformer of Hellendoorn et al. [14] achieve
67.7% joint accuracy and we achieve 64.4%: the results are close to
each other. Here the performance is given for our model, closest to
the model of Hellendoorn et al. [14]: the Text model of similar size
and with similar number of training updates; using our Syntax+Text
model achieves higher quality. The performance of GGNN Sand-
wich is high on VM, as in [14]. For CC, with tree relative attention,
we achieve 59.79 / 91.65 MRR (values / types) while Kim et al. [20]
achieved 58.8 / 91.9 (their “TravTrans variant”); and for standard
Transformer (no structure information), we achieve 59.66 / 89.16
MRR while Kim et al. [20] achieved 58.0 / 87.3 (their “TravTrans”)
respectively, again the results are close. For FN, we used the code
of Ahmad et al. [1] with our custom data and targets, so the results
are not comparable, but we checked that their code produces the
same numbers on their data as in the paper.

The results given in our paper are for our custom data split and
thus are not directly comparable to the numbers in other works. We
argue that data resplitting is crucial for achieving correct results, see
details in Section 3. At the same time, the remaining experimental
setup (e. g. architecture, metrics) is the same as in recent works.

10 RELATED WORK

Variable misuse. The field of automated program repair includes
a lot of different tasks, see [27] for a review, we focus on a particular
variable misuse detection task. This task was introduced by Alla-
manis et al. [4] who proposed using GGNN with different types of
edges to predict the true variable name for each name placeholder.
Vasic et al. [36] enhances the VM task by learning to jointly classify,
localize bug and repair the code snippet. They use an RNN equipped
with two pointers that locate and fix the bug. Hellendoorn et al.
[14] improved the performance on VM task, using Transformers,
GREAT model, and GGNN Sandwich model.

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Code summarization. The task of code summarization is formal-
ized in literature in different ways: given a code snippet, predict
the docstring [16], the function name [5], or the accompanying
comment [17]. Allamanis et al. [5] propose using convolutional
neural networks for generating human readable function names,
while Iyer et al. [17] proposed using LSTM [15] with attention
to generate natural language summaries. Alon et al. [7] proposed
sampling random AST paths and encoding them with bidirectional
LSTM to produce natural method names and summaries of the code.
Fernandes et al. [11] proposed combining RNNs/Transformers with
GGNN. Ahmad et al. [1] empirically investigate Transformers for
code summarization, showing that Transformer with sequential rel-
ative attention outperforms Transformer equipped with positional
encodings as well as a wide range of other models, e, g.RNNs.

Code completion. Early works on code generation built proba-
bilistic models over the grammar rules. Maddison and Tarlow [26]
learned Markov Decision Process over free context grammars, uti-
lizing AST. Raychev et al. [29] learned decision trees predicting AST
nodes. Sun et al. [33] generated code by expanding AST nodes, us-
ing natural language comments as additional source of information.
Li et al. [24] used LSTM with a pointer, this model either generates
the next token from vocabulary or copies the token from a previ-
ously seen position. Kim et al. [20] proposed using Transformers
for code generation, enhanced them with tree relative attention
and showed that the resulting model significantly outperforms
RNN-based models as well as other models [29].

Recent advances in neural source code processing. The recent line
of work is dedicated to learning contextual embeddings for code
on the basis of Bidirectional Encoder Representations from Trans-
formers (BERT) [9]. Such models are firstly pretrained on large
datasets providing high-quality embeddings of code, and then fine-
tuned on small datasets for downstream tasks [10, 12, 18]. All these
Transformer-based models treat code as text and can potentially
benefit from the further utilization of the syntactic information.
Another line of research regards making Transformers more time-
and memory-efficient [35]. Investigating the applicability of such
methods to syntax-based Transformers is an interesting direction
for future research.

Investigating neural networks for code with omitted variable names.
A few of previous works considered training neural networks with
omitted variable names: Gupta et al. [13], Xu et al. [39] trained RNNs
on the data with anonymized variables, Ahmed et al. [2] replaced
variables with their types. LeClair et al. [22] investigated the effect
of replacing all values in the AST traversal with <unk> value in
the code summarization task, and concluded that the quality of
an RNN trained on such data is extremely low. Their result aligns
with ours, while we consider a more general procedure of value
anonymization (that saves information about value repetition) and
investigate the effect of using anonymization in a wider set of tasks
for a Transformer architecture.

11 THREATS TO VALIDITY
We did our best to make out comparison of different AST pro-
cessing mechanisms as fair as possible. However, the following
factors could potentially affect the validity of our results: using the

same training hyperparameters for all models, not using subtok-
enization, and not using data- and control-flow edges in GGNN
Sandwich. The decision not to use subtokenization was explained
in Section 3. Moreover, we underline that sequential relative atten-
tion, our best performing mechanism, allows for easy combination
with any subtokenization technique which will result in further
quality improvement. On the contrary, this is not the case for more
complex considered AST-processing mechanisms. The decision not
use control- and data-flow edges was explained in Section 2. We
note that adding data- and control-flow edges to the GGNN Sand-
wich equipped with sequential relative attention would increase
the quality of this combined model even further. As for the training
hyperparameters, tuning them for each model individually would
be very expensive given our limited computational resources. How-
ever, we note that our models differ only in the AST-processing
mechanism that is a relatively small change to the architecture.
Thus we assume that using the same training hyperparameters for
different models is permissible in our work.

12 CONCLUSION
In this work, we investigated the capabilities of Transformer to
utilize syntactic information in source code processing. Our study
underlined the following practical conclusions:

• sequential relative attention is a simple, fast and not con-
sidered as the baseline in previous works mechanism that
performs best in 3 out of 4 tasks (in some cases, similarly to
other slower mechanisms);

• combining sequential relative attention with GGNN Sand-
wich in the variable misuse task and with tree relative atten-
tion or tree positional encoding in the code completion task
may further improve quality;

• omitting types, values or edges in ASTs hurts performance;
• ensembling Transformer trained on the full-data with Trans-
former trained on the anonymized data outperforms the
ensemble of Transformers trained on the same kind of data.
Further, our study highlighted two conceptual insights. On the
one hand, Transformers are generally capable of utilizing syntactic
information in source code, despite they were initially developed for
NLP, i. e. processing sequences. On the other hand, Transformers
utilize syntactic information fully not in all tasks: in variable misuse
and code completion, Transformer uses all AST components, while
in function naming, Transformer mostly relies on a set of types and
values used in the program, hardly utilizing syntactic structure.

ACKNOWLEDGMENTS
We would like to thank Ildus Sadrtdinov, Ivan Rubachev and the
anonymous reviewers for the valuable feedback. The results pre-
sented in Sections 5 and 8 were supported by the Russian Science
Foundation grant №19-71-30020. The results presented in Sections 6
and 7 were supported by Samsung Research, Samsung Electronics.
The research was supported in part through the computational
resources of HPC facilities at NRU HSE.

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).
[2] Umair Z. Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit
Gulwani. 2018. Compilation Error Repair: For the Student Programs, from the
Student Programs. In Proceedings of the 40th International Conference on Software
Engineering: Software Engineering Education and Training (Gothenburg, Sweden)
(ICSE-SEET ’18). Association for Computing Machinery, New York, NY, USA,
78–87. https://doi.org/10.1145/3183377.3183383

[3] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (2019).

[4] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Con-
ference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=
BJOFETxR-

[5] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
Attention Network for Extreme Summarization of Source Code. In International
Conference on Machine Learning (ICML).

[6] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net. https://openreview.net/forum?id=H1gKYo09tX

[7] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations. https://openreview.net/forum?id=H1gKYo09tX
[8] Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov.
2020. Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep
Learning. In International Conference on Learning Representations, ICLR 2020.
https://openreview.net/forum?id=BJxI5gHKDr

[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,
4171–4186. https://doi.org/10.18653/v1/N19-1423

[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP 2020. Association for Computa-
tional Linguistics, Online, 1536–1547. https://doi.org/10.18653/v1/2020.findings-
emnlp.139

[11] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured
Neural Summarization. In International Conference on Learning Representations.
https://openreview.net/forum?id=H1ersoRqtm

[12] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations
with Data Flow. In International Conference on Learning Representations. https:
//openreview.net/forum?id=jLoC4ez43PZ

[13] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence (San Francisco, California,
USA) (AAAI’17). AAAI Press, 1345–1351.

[14] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and
David Bieber. 2020. Global Relational Models of Source Code. In International
Conference on Learning Representations, ICLR 2020.
https://openreview.net/
forum?id=B1lnbRNtwr

[15] Sepp Hochreiter and JÃŒrgen Schmidhuber. 1997. Long Short-term Memory.
Neural computation 9 (12 1997), 1735–80. https://doi.org/10.1162/neco.1997.9.8.
1735

[16] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep Code Comment
Generation. In Proceedings of the 26th Conference on Program Comprehension
(Gothenburg, Sweden) (ICPC ’18). Association for Computing Machinery, New
York, NY, USA, 200–210. https://doi.org/10.1145/3196321.3196334

[17] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Berlin, Germany,
2073–2083. https://doi.org/10.18653/v1/P16-1195

[18] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning and evaluating contextual embedding of source code. In Proceedings
of the 37th International Conference on Machine Learning, ICML 2020, 12-18 July
2020 (Proceedings of Machine Learning Research). PMLR.

[19] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary Models for
Source Code. In Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering (Seoul, South Korea) (ICSE 20). Association for Computing
Machinery, New York, NY, USA, 1073–1085. https://doi.org/10.1145/3377811.
3380342

[20] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2020. Code Predic-

tion by Feeding Trees to Transformers. arXiv:2003.13848 [cs.SE]

[21] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample.
2020. Unsupervised Translation of Programming Languages. In arXiv preprint
arXiv:2006.03511. arXiv:2006.03511 [cs.CL]

[22] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A Neural Model for
Generating Natural Language Summaries of Program Subroutines. In Proceedings
of the 41st International Conference on Software Engineering (Montreal, Quebec,
Canada) (ICSE ’19). IEEE Press, 795–806. https://doi.org/10.1109/ICSE.2019.00087
[23] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
Neural Attention and Pointer Networks. In Proceedings of the 27th International
Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAI’18). AAAI
Press, 4159–25.

[24] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
Neural Attention and Pointer Networks. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intelligence, IJCAI-18. International
Joint Conferences on Artificial Intelligence Organization, 4159–4165. https:
//doi.org/10.24963/ijcai.2018/578

[25] I. Loshchilov and F. Hutter. 2017. SGDR: Stochastic Gradient Descent with Warm

Restarts. In ICLR.

[26] Chris Maddison and Daniel Tarlow. 2014. Structured Generative Models of Natu-
ral Source Code. In Proceedings of the 31st International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 32), Eric P. Xing and
Tony Jebara (Eds.). PMLR, Bejing, China, 649–657. http://proceedings.mlr.press/
v32/maddison14.html

[27] Martin Monperrus. 2020. The Living Review on Automated Program Repair.
(Dec. 2020). https://hal.archives-ouvertes.fr/hal-01956501 working paper or
preprint.

[28] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model for
Code with Decision Trees. In Proceedings of the 2016 ACM SIGPLAN International
Conference on Object-Oriented Programming, Systems, Languages, and Applications
(Amsterdam, Netherlands) (OOPSLA 2016). Association for Computing Machinery,
New York, NY, USA, 731–747. https://doi.org/10.1145/2983990.2984041
[29] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model for
Code with Decision Trees. SIGPLAN Not. 51, 10 (Oct. 2016), 731–747. https:
//doi.org/10.1145/3022671.2984041

[30] Veselin Raychev, Pavol Bielik, Martin Vechev, and Andreas Krause. 2016. Learning
Programs from Noisy Data. In Proceedings of the 43rd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages (St. Petersburg, FL,
USA) (POPL ’16). Association for Computing Machinery, New York, NY, USA,
761–774. https://doi.org/10.1145/2837614.2837671

[31] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers). Association for Computational
Linguistics, New Orleans, Louisiana, 464–468. https://doi.org/10.18653/v1/N18-
2074

[32] Vighnesh Shiv and Chris Quirk. 2019. Novel positional encodings to enable
tree-based transformers. In Advances in Neural Information Processing Systems 32,
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(Eds.). Curran Associates, Inc., 12081–12091. http://papers.nips.cc/paper/9376-
novel-positional-encodings-to-enable-tree-based-transformers.pdf

[33] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020.
TreeGen: A Tree-Based Transformer Architecture for Code Generation. Proceed-
ings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 8984–8991.
https://doi.org/10.1609/aaai.v34i05.6430

[34] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Association for Computational Linguistics,
Beijing, China, 1556–1566. https://doi.org/10.3115/v1/P15-1150

[35] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient

Transformers: A Survey. arXiv:2009.06732 [cs.LG]

[36] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.
2019. Neural Program Repair by Jointly Learning to Localize and Repair. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=
ByloJ20qtm

[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V.

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.).
Curran Associates, Inc., 5998–6008. http://papers.nips.cc/paper/7181-attention-
is-all-you-need.pdf

[38] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018.
Improving Automatic Source Code Summarization via
Deep Reinforcement Learning. In Proceedings of the 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineering (Montpellier, France) (ASE

2018). Association for Computing Machinery, New York, NY, USA, 397–407.
https://doi.org/10.1145/3238147.3238206

[39] Shengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, and Jian Lu. 2019.
Commit Message Generation for Source Code Changes. In Proceedings of the
Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19.
International Joint Conferences on Artificial Intelligence Organization, 3975–
3981. https://doi.org/10.24963/ijcai.2019/552

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

A ADDITIONAL EXPERIMENTAL DETAILS
A.1 Hyperparameter tuning
To ensure the fairness of comparing different mechanisms for pro-
cessing AST in Transformer, we tune their hyperparameters indi-
vidually for each task–dataset combination.

Sequential positional embeddings do not have hyperparameters.
For sequential relative attention, we tune the maximum relative
distance between the elements of the sequence. We consider options
[8, 32, 128, 250]: from small distance to the maximum input length.
For tree positional encoding, we tune the maximum path width
𝑛𝑤 and maximum path depth 𝑛𝑑 . Since the embedding size is fixed
and equal to 𝑑model = 512, and the number of learnable parameters
in the tree positional encoding equals 𝑑model/(𝑛𝑤 · 𝑛𝑑 ) (see details
in [32]), the greater values of 𝑛𝑤 and 𝑛𝑑 we use, the less number
of the parameters in the encoding layer we have. We selected op-
tions to cover both cases when we have wide and deep paths but
only one parameter as well as narrow and shallow paths but more
parameters.

For tree relative attention, we tune the size of the relations vo-
cabulary. All relations follow the pattern “𝑘𝑢 nodes up and 𝑘𝑑 nodes
down”, 𝑘𝑢, 𝑘𝑑 = 0, 1, 2, . . . . We select the top of the most frequent
relations in the dataset. We consider options from relatively small
vocabulary to the full vocabulary of relations.

For GGNN Sandwich model, we consider 6-layer and 12-layer
configurations of alternating Transformer (T) and GGNN (G) layers,
we also consider placing both types of layers first i. e. [T, G, T, G, T,
G] or [G, T, G, T, G, T] (and similarly for 12 layers). GGNN layers
include 4 message passes. We also consider omitting edges of types
Left and Right (see Figure 1(f)).

The results for hyperparameter tuning are given in Table 7. Each
model was trained once, but since we have several choices for
hyperparameters, we could analyse whether there are stable de-
pendencies or the difference in quality is a result of noise. The
choice of the maximum relative distance is stable across datasets
in all tasks: in the VM task, distance equal to 8 is chosen, which is
interpretable since in this task, local dependencies matter much;
in contrast, in the FN task, small values of maximum distance per-
form poorly, and in this task, global dependencies are important.
The similar reasoning applies to tree positional encoding: in the
“local” VM task, small values of (𝑛𝑤, 𝑛𝑑 ) are chosen, while in the
“global” FN task, large values outperform (4, 8) combination. For
tree relative attention, changing the relation vocabulary size does
not affect quality much: rare relations’ embeddings are updated
only several times during training, so including them does not im-
prove the performance. For the GGNN Sandwich, the choice of the
optimal hyperparameters is again stable across datasets. In the VM
task, the bigger the model, the higher the quality: deep 12-layer
models substantially outperform 6-layers ones, and 3-edge-type
models outperform 3-edge-type models. In contrast, in the FN task,

the 12-layer model fails to train properly, while the 6-layer model
which achieves good quality.

A.2 Anonymization procedure.
We anonymize values so that inside one code snippet, different val-
ues are mapped to different placeholders var1, var2, var3 etc. We
use random strategy for assigning placeholders: we fix the vocabu-
lary of 1000 placeholders and for unique each value in each code
snippet, choose a placeholder randomly. For example, the code snip-
pet total_cnts[i][w] = title_cnt[i][w] + text_cnt[i][w]
may be anonymized as follows: var341[var30][var89] =
var785[var30][var89] + var453[var30][var89]. Anonymiza-
tion of different code snippets is independent: one value may be
replaced with different placeholders in different code snippets. For
example, value total_cnt may be replaced with var341 in one
code snippet and with var110 — in another one.

B COMPARISON OF APPROACHES FOR

UTILIZING SYNTACTIC STRUCTURE IN
TRANSFORMER IN THE ANONYMIZED
SETTING

In this section, we compare mechanisms for utilizing syntactic struc-
ture in Transformer in the anonymized setting, i. e. when mecha-
nisms are incorporated in the Syntax model described in Section 6.
Due to the high computational cost of hyperparameter tuning,
in this experiment, we use the same hyperparameters as in the
full-data setting. In Section A we discussed that the selected hyper-
parameters are quite interpretable in different tasks, which allows
to hypothesise that in the anonymized setting, the optimal hyper-
parameters would be the same.

The results are shown in Figure 5. The leading approaches are
the same as in the full data setting: sequential relative attention in
the VM and CC (value prediction) tasks, tree relative attention in
the CC (type prediction) task, and almost similar performance of
different mechanisms in the FN task.

C TABLES AND ADDITIONAL PLOTS FOR

COMPARING DIFFERENT
STRUCTURE-CAPTURING MECHANISMS
Table 8 lists numerical data for barplots presented in Figure 2 in the
main part of the paper (comparing AST-processing mechanisms in
the full-data setting). Figure 6 visualizes the progress of test metrics
during training for different AST-processing mechanisms in the
full-data setting. This plots shows that the number of epochs we use
is sufficient, i. e. the ordering of methods would not change and all
models converged or almost converged. Table 8 lists numerical data
for barplots presented in Figure 5 (comparing structure-capturing
mechanisms in the anonymized setting).

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Table 7: The results of hyperparameter tuning for different structure-capturing mechanisms, tasks and datasets. The quality
is measured over the validation dataset ( 10% of the training data). Variable Misuse: joint localization and repair accuracy set,
Function Naming: F1-measure; Code completion (predicting values): MRR, all numbers in percent. Datasets: PY — Python150k,
JS – JavaScript150k. Notation: Y – Yes, N – No, all – the full vocabulary of relations is used. Bold font emphasizes the maximum
among values in a column.

Variable misuse

Function naming

Code completion

Model (hyperparams.)
Sequential
relative attention
(max. relative distance)

Tree positional
encoding
(max. path width,
max. path depth)

Tree relative
attention
(relations vocabulary size)

GGNN Sandwich
(the number of layers,
the number of edge types,
is GGNN the first layer?)

PY
81.69
8
81.56
32
80.94
128
81.05
250
64.74
2, 64
73.77
4, 8
74.53
8, 16
74.29
16, 8
66.78
16, 32
71.69
100
71.31
600
71.52
1500
71.36
all
70.39
6, 2, N
70.25
6, 2, Y
78.16
6, 3, N
6, 3, Y
78.33
12, 2, N 71.96
72.60
12, 2, Y
12, 3, N 80.71
80.61
12, 3, Y

JS
76.98
75.83
75.52
74.75
55.06
62.14
60.19
59.21
45.67
65.55
66.61
64.38
66.20
65.25
65.87
68.74
71.06
68.99
68.30
73.88
73.09

PY
32.63
8
33.05
32
33.43
128
33.54
250
32.96
2, 64
30.75
4, 8
33.06
8, 16
33.48
16, 8
32.85
16, 32
33.38
100
33.71
600
33.45
1500
32.18
all
33.04
6, 2, N
33.61
6, 2, Y
32.72
6, 3, N
31.79
6, 3, Y
5.37
12, 2, N
9.85
12, 2, Y
12, 3, N 21.26
20.36
12, 3, Y

JS
22.36
23.43
23.43
23.76
23.72
21.00
22.29
22.41
23.02
24.25
23.78
23.67
23.95
23.36
24.05
21.40
21.49
5.11
5.11
3.40
6.27

JS
61.36
61.73
61.66
61.64
54.49
59.3
60.23
60.32
60.42
60.32
60.94
61.19
61.05

8
32
128
250
2, 64
4, 8
8, 16
16, 8
16, 32
10
100
1000
all

PY
52.68
53.04
53.04
52.95
50.89
52.22
52.16
52.38
52.26
51.90
52.54
52.66
52.46

N/A

Python:

Variable Misuse

Function Naming

Code Completion (values)

Code Completion (types)

JavaScript:

Variable Misuse

Function Naming

Code Completion (values)

Code Completion (types)

Figure 5: A comparison of different mechanisms for processing AST structure in Transformer, in the anonymized setting. The
numeric data for barplots is given in Table 9. Please mind that the x-axis limits are different from Figure 2.

2224F10.560.570.580.590.60MRR0.750.800.850.90MRR1011121314F10.580.600.62MRR0.750.800.850.90MRRESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Table 8: The numerical data for the comparison of different mechanisms for processing AST structure in Transformer, in the
full-data setting.

Variable misuse

Function naming

Code completion (Values) Code completion (Types)

Model
Seq. pos. emb.
Seq. rel. attn.
Tree pos. enc.
Tree rel. attn.
GGNN Sandwich

PY
74.38±0.56
81.42±0.16
74.65±0.99
71.33±0.54
80.23±0.15

JS
55.12±0.19
76.52±0.34
61.29±0.41
65.17±0.09
72.58±0.14

PY
33.79±0.20
35.49±0.35
34.95±0.40
35.19±0.17
35.40±0.21

JS
23.11±0.27
24.67±0.18
24.45±0.21
24.24±0.11
23.57±0.50

PY
53.50±0.02
54.53±0.07
53.82±0.007
54.00±0.10
N/A

JS
62.7±0.18
64.11±0.03
62.70±0.45
63.39±0.11
N/A

PY
88.90±0.02
89.22±0.02
88.96±0.005
91.36±0.01
N/A

JS
89.59±0.07
90.14±0.002
89.71±0.21
91.2±0.03
N/A

Table 9: The numerical data for the comparison of different mechanisms for processing AST structure in Transformer, in the
anonymized setting.

Variable misuse

Function naming

Code completion (Values) Code completion (Types)

Model
Seq. pos. emb.
Seq. rel. attn.
Tree pos. enc.
Tree rel. attn.
GGNN Sandwich

PY
77.21±0.02
81.83±0.12
74.86±0.19
67.35±0.03
77.22±0.08

JS
66.19±0.65
78.53±0.17
61.07±1.82
65.47±0.44
69.64±0.11

PY
23.40±0.51
24.79±0.15
24.52±0.25
23.94±0.20
24.91±0.11

JS
12.24±0.12
13.70±0.19
13.43±0.05
12.91±0.12
13.11±0.22

PY
58.18±0.08
58.78±0.05
58.27±0.09
58.45±0.16
N/A

JS
59.71±0.01
60.83±0.08
60.31±0.01
60.45±0.05
N/A

PY
88.10±0.04
88.44±0.03
88.23±0.05
90.72±0.04
N/A

JS
87.64±0.01
88.25±0.06
88.19±0.01
89.64±0.03
N/A

Python:

Variable Misuse

Function Naming

Code Completion (values)

Code Completion (types)

JavaScript:

Variable Misuse

Function Naming

Code Completion (values)

Code Completion (types)

Figure 6: A comparison of different mechanisms for processing AST structure in Transformer: test metrics by epochs.

0510152025Epoch304050607080Joint accuracySeq. pos. emb.Seq. rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich02468101214Epoch262830323436F1Seq. pos. emb.Seq rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich012345678Epoch0.480.490.500.510.520.530.54MRRCC PY Value predictionseq. pos. emb.seq. rel. att.tree pos. enc.tree rel. att.0.02.55.07.510.012.515.017.5Epoch0.860.870.880.890.900.91MRRCC PY Type predictionseq. pos. emb.seq. rel. att.tree pos. enc.tree rel. att.0510152025303540Epoch1020304050607080Joint accuracySeq. pos. emb.Seq. rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich0510152025Epoch7.510.012.515.017.520.022.525.0Joint accuracySeq. pos. emb.Seq rel. attn.Tree pos. enc.Tree rel. attn.GGNN Sandwich0.02.55.07.510.012.515.017.5Epoch0.540.560.580.600.62MRRCC JS Value predictionseq. pos. emb.seq. rel. att.tree pos. enc.tree rel. att.0.02.55.07.510.012.515.017.5Epoch0.870.880.890.900.91MRRCC JS Type predictionseq. pos. emb.seq. rel. att.tree pos. enc.tree rel. att.Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

D ATTENTION MAPS FOR DIFFERENT

STRUCTURE-CAPTURING MECHANISMS
IN TRANSFORMER

In Figures 8, 9, 10, 11 we present attention maps for different
structure-capturing mechanisms on the code completion task in
the anonymized setting. We visualize attention maps for the first
layer since low level interactions should reflect the differences in
considered mechanisms. We observe that all mechanisms except
tree relative attention mostly attend to the last predicted tokens.
Transformer with tree positional encodings (figure 9) always pays
significant attention to the root node, that is easy to find because
of zero tree encoding, or to some “anchor” nodes, e. g. For node in
the first map of Figure 9. In other words, tree positional encodings
allow emphasizing important nodes in the tree. Also this model is
able to distinguish children numbers. Transformer with tree rela-
tive attention (figure 11) often watches at siblings of the node to
be predicted, but this model cannot differentiate child numbers,
by construction. The attention maps for this model are smoother
than for other models, because this model introduces multiplicative
coefficients to the weights after softmax in self attention.

Figure 7: Code snippet (and its AST) used to visualize atten-
tion maps.

count=0foriinrange(len(seq)):count+=seq[i]1ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

N. Chirkova and S. Troshin

Figure 8: Attention maps for the 1st layer of Syntax model, sequential positional embeddings, code completion task. Y axis:
what the model predicts, X axis: which token the model attends to.

Figure 9: Attention maps for the 1st layer of Syntax model, tree positional encodings, code completion task. Y axis: what the
model predicts, X axis: which token the model attends to.

Empirical Study of Transformers for Source Code

ESEC/FSE ’21, August 23–28, 2021, Athens, Greece

Figure 10: Attention maps for the 1st layer of Syntax model, sequential relative attention, code completion task. Y axis: what
the model predicts, X axis: which token the model attends to.

Figure 11: Attention maps for the 1st layer of Syntax model, tree relative attention, code completion task. Y axis: what the
model predicts, X axis: which token the model attends to.

